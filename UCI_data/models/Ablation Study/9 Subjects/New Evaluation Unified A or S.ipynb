{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Activity_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Activity_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class Subject_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Subject_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_train = GAN_data.iloc[:,:-2].values\n",
    "    y_train = GAN_data[[y_label]].values\n",
    "    \n",
    "    return X_train, y_train.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)):\n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Activity_Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.533818006515503, Final Batch Loss: 1.1105163097381592\n",
      "Epoch 2, Loss: 5.5187541246414185, Final Batch Loss: 1.1048098802566528\n",
      "Epoch 3, Loss: 5.497993350028992, Final Batch Loss: 1.0971593856811523\n",
      "Epoch 4, Loss: 5.480450630187988, Final Batch Loss: 1.0935215950012207\n",
      "Epoch 5, Loss: 5.4549185037612915, Final Batch Loss: 1.088227391242981\n",
      "Epoch 6, Loss: 5.4179171323776245, Final Batch Loss: 1.08084237575531\n",
      "Epoch 7, Loss: 5.36445689201355, Final Batch Loss: 1.0678369998931885\n",
      "Epoch 8, Loss: 5.288394927978516, Final Batch Loss: 1.0519883632659912\n",
      "Epoch 9, Loss: 5.188693046569824, Final Batch Loss: 1.0305815935134888\n",
      "Epoch 10, Loss: 5.045647144317627, Final Batch Loss: 1.0012670755386353\n",
      "Epoch 11, Loss: 4.865965366363525, Final Batch Loss: 0.9596382975578308\n",
      "Epoch 12, Loss: 4.637374579906464, Final Batch Loss: 0.907181441783905\n",
      "Epoch 13, Loss: 4.35869699716568, Final Batch Loss: 0.8254823088645935\n",
      "Epoch 14, Loss: 4.1308006048202515, Final Batch Loss: 0.8330798149108887\n",
      "Epoch 15, Loss: 3.874618709087372, Final Batch Loss: 0.7437912225723267\n",
      "Epoch 16, Loss: 3.6859402656555176, Final Batch Loss: 0.7127642631530762\n",
      "Epoch 17, Loss: 3.5065165162086487, Final Batch Loss: 0.6454089283943176\n",
      "Epoch 18, Loss: 3.3946914672851562, Final Batch Loss: 0.6942360401153564\n",
      "Epoch 19, Loss: 3.248072564601898, Final Batch Loss: 0.6630883812904358\n",
      "Epoch 20, Loss: 3.1102797389030457, Final Batch Loss: 0.6417940855026245\n",
      "Epoch 21, Loss: 2.862297296524048, Final Batch Loss: 0.5422183275222778\n",
      "Epoch 22, Loss: 2.6361953020095825, Final Batch Loss: 0.5082249045372009\n",
      "Epoch 23, Loss: 2.397385895252228, Final Batch Loss: 0.4867524206638336\n",
      "Epoch 24, Loss: 2.084158718585968, Final Batch Loss: 0.4382576644420624\n",
      "Epoch 25, Loss: 1.8281047344207764, Final Batch Loss: 0.3835103213787079\n",
      "Epoch 26, Loss: 1.5503301322460175, Final Batch Loss: 0.28955817222595215\n",
      "Epoch 27, Loss: 1.2990076392889023, Final Batch Loss: 0.23086503148078918\n",
      "Epoch 28, Loss: 1.170545071363449, Final Batch Loss: 0.2323639988899231\n",
      "Epoch 29, Loss: 1.0800902247428894, Final Batch Loss: 0.2229456901550293\n",
      "Epoch 30, Loss: 1.015990361571312, Final Batch Loss: 0.1939862221479416\n",
      "Epoch 31, Loss: 1.089271828532219, Final Batch Loss: 0.24004343152046204\n",
      "Epoch 32, Loss: 0.8418733924627304, Final Batch Loss: 0.09356151521205902\n",
      "Epoch 33, Loss: 1.0231579691171646, Final Batch Loss: 0.1750922054052353\n",
      "Epoch 34, Loss: 0.9590407162904739, Final Batch Loss: 0.1849628984928131\n",
      "Epoch 35, Loss: 0.9424082785844803, Final Batch Loss: 0.18764586746692657\n",
      "Epoch 36, Loss: 0.8793091177940369, Final Batch Loss: 0.13817153871059418\n",
      "Epoch 37, Loss: 0.8927763402462006, Final Batch Loss: 0.1910100281238556\n",
      "Epoch 38, Loss: 0.8697520345449448, Final Batch Loss: 0.15788422524929047\n",
      "Epoch 39, Loss: 0.8946679830551147, Final Batch Loss: 0.1584625095129013\n",
      "Epoch 40, Loss: 0.8643518760800362, Final Batch Loss: 0.10804330557584763\n",
      "Epoch 41, Loss: 0.8998362421989441, Final Batch Loss: 0.19713087379932404\n",
      "Epoch 42, Loss: 0.7855065017938614, Final Batch Loss: 0.19196254014968872\n",
      "Epoch 43, Loss: 0.7769427597522736, Final Batch Loss: 0.1707562506198883\n",
      "Epoch 44, Loss: 0.7476959526538849, Final Batch Loss: 0.15364788472652435\n",
      "Epoch 45, Loss: 0.7977155148983002, Final Batch Loss: 0.14832958579063416\n",
      "Epoch 46, Loss: 0.8379108160734177, Final Batch Loss: 0.2110384702682495\n",
      "Epoch 47, Loss: 0.7613895088434219, Final Batch Loss: 0.13162191212177277\n",
      "Epoch 48, Loss: 0.7581896707415581, Final Batch Loss: 0.11550824344158173\n",
      "Epoch 49, Loss: 0.7309937477111816, Final Batch Loss: 0.13603907823562622\n",
      "Epoch 50, Loss: 0.7892191782593727, Final Batch Loss: 0.14293695986270905\n",
      "Epoch 51, Loss: 0.7269514128565788, Final Batch Loss: 0.10586870461702347\n",
      "Epoch 52, Loss: 0.7261464223265648, Final Batch Loss: 0.17003965377807617\n",
      "Epoch 53, Loss: 0.7553401663899422, Final Batch Loss: 0.13683681190013885\n",
      "Epoch 54, Loss: 0.7170642241835594, Final Batch Loss: 0.14125479757785797\n",
      "Epoch 55, Loss: 0.6783665642142296, Final Batch Loss: 0.1069340780377388\n",
      "Epoch 56, Loss: 0.7770834043622017, Final Batch Loss: 0.20482951402664185\n",
      "Epoch 57, Loss: 0.7062418386340141, Final Batch Loss: 0.17799261212348938\n",
      "Epoch 58, Loss: 0.6770103275775909, Final Batch Loss: 0.1039312556385994\n",
      "Epoch 59, Loss: 0.6523637771606445, Final Batch Loss: 0.09752235561609268\n",
      "Epoch 60, Loss: 0.6645380333065987, Final Batch Loss: 0.09197939187288284\n",
      "Epoch 61, Loss: 0.6711057648062706, Final Batch Loss: 0.09191009402275085\n",
      "Epoch 62, Loss: 0.7342341393232346, Final Batch Loss: 0.22201362252235413\n",
      "Epoch 63, Loss: 0.6524641811847687, Final Batch Loss: 0.1430182158946991\n",
      "Epoch 64, Loss: 0.6760823726654053, Final Batch Loss: 0.1556878238916397\n",
      "Epoch 65, Loss: 0.6754092425107956, Final Batch Loss: 0.10610868781805038\n",
      "Epoch 66, Loss: 0.698354534804821, Final Batch Loss: 0.09967068582773209\n",
      "Epoch 67, Loss: 0.6525109112262726, Final Batch Loss: 0.14562268555164337\n",
      "Epoch 68, Loss: 0.6782875582575798, Final Batch Loss: 0.1242491751909256\n",
      "Epoch 69, Loss: 0.6481674462556839, Final Batch Loss: 0.1276705265045166\n",
      "Epoch 70, Loss: 0.638681598007679, Final Batch Loss: 0.12464069575071335\n",
      "Epoch 71, Loss: 0.5813418328762054, Final Batch Loss: 0.10053899139165878\n",
      "Epoch 72, Loss: 0.6533316299319267, Final Batch Loss: 0.13454970717430115\n",
      "Epoch 73, Loss: 0.6667682975530624, Final Batch Loss: 0.18473364412784576\n",
      "Epoch 74, Loss: 0.6211505085229874, Final Batch Loss: 0.16772004961967468\n",
      "Epoch 75, Loss: 0.5782341659069061, Final Batch Loss: 0.12210450321435928\n",
      "Epoch 76, Loss: 0.6748239398002625, Final Batch Loss: 0.12763455510139465\n",
      "Epoch 77, Loss: 0.6161574088037014, Final Batch Loss: 0.0562405101954937\n",
      "Epoch 78, Loss: 0.6170536875724792, Final Batch Loss: 0.18140719830989838\n",
      "Epoch 79, Loss: 0.6460231393575668, Final Batch Loss: 0.0919288769364357\n",
      "Epoch 80, Loss: 0.5931684523820877, Final Batch Loss: 0.08820448815822601\n",
      "Epoch 81, Loss: 0.6346048340201378, Final Batch Loss: 0.10974480956792831\n",
      "Epoch 82, Loss: 0.5720457583665848, Final Batch Loss: 0.15374799072742462\n",
      "Epoch 83, Loss: 0.6150910928845406, Final Batch Loss: 0.11176202446222305\n",
      "Epoch 84, Loss: 0.6047743409872055, Final Batch Loss: 0.06936421990394592\n",
      "Epoch 85, Loss: 0.5920524075627327, Final Batch Loss: 0.12619471549987793\n",
      "Epoch 86, Loss: 0.6086549535393715, Final Batch Loss: 0.14924481511116028\n",
      "Epoch 87, Loss: 0.584937185049057, Final Batch Loss: 0.10244859755039215\n",
      "Epoch 88, Loss: 0.5562601387500763, Final Batch Loss: 0.06992856413125992\n",
      "Epoch 89, Loss: 0.5219686180353165, Final Batch Loss: 0.09744168817996979\n",
      "Epoch 90, Loss: 0.6038819700479507, Final Batch Loss: 0.19028781354427338\n",
      "Epoch 91, Loss: 0.5757561102509499, Final Batch Loss: 0.11057843267917633\n",
      "Epoch 92, Loss: 0.5664845183491707, Final Batch Loss: 0.10859692841768265\n",
      "Epoch 93, Loss: 0.5632689669728279, Final Batch Loss: 0.13758471608161926\n",
      "Epoch 94, Loss: 0.6103538125753403, Final Batch Loss: 0.12871715426445007\n",
      "Epoch 95, Loss: 0.5725247636437416, Final Batch Loss: 0.121732197701931\n",
      "Epoch 96, Loss: 0.5552282556891441, Final Batch Loss: 0.10238686203956604\n",
      "Epoch 97, Loss: 0.5534342750906944, Final Batch Loss: 0.10278211534023285\n",
      "Epoch 98, Loss: 0.5525924190878868, Final Batch Loss: 0.12191937118768692\n",
      "Epoch 99, Loss: 0.5879719108343124, Final Batch Loss: 0.11127415299415588\n",
      "Epoch 100, Loss: 0.6131681725382805, Final Batch Loss: 0.09613890200853348\n",
      "Epoch 101, Loss: 0.5701478384435177, Final Batch Loss: 0.09947714954614639\n",
      "Epoch 102, Loss: 0.5897427499294281, Final Batch Loss: 0.09706611186265945\n",
      "Epoch 103, Loss: 0.5559070780873299, Final Batch Loss: 0.08675743639469147\n",
      "Epoch 104, Loss: 0.554052010178566, Final Batch Loss: 0.11325930804014206\n",
      "Epoch 105, Loss: 0.5385400950908661, Final Batch Loss: 0.06898166239261627\n",
      "Epoch 106, Loss: 0.5117104463279247, Final Batch Loss: 0.10206574946641922\n",
      "Epoch 107, Loss: 0.5344094410538673, Final Batch Loss: 0.09887922555208206\n",
      "Epoch 108, Loss: 0.5693807154893875, Final Batch Loss: 0.17189201712608337\n",
      "Epoch 109, Loss: 0.4746972434222698, Final Batch Loss: 0.1217426136136055\n",
      "Epoch 110, Loss: 0.5648885071277618, Final Batch Loss: 0.05755755305290222\n",
      "Epoch 111, Loss: 0.5102110654115677, Final Batch Loss: 0.09564951807260513\n",
      "Epoch 112, Loss: 0.600958988070488, Final Batch Loss: 0.20756840705871582\n",
      "Epoch 113, Loss: 0.5481917932629585, Final Batch Loss: 0.14000709354877472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114, Loss: 0.5575242713093758, Final Batch Loss: 0.14619365334510803\n",
      "Epoch 115, Loss: 0.5632417872548103, Final Batch Loss: 0.13942445814609528\n",
      "Epoch 116, Loss: 0.5982112139463425, Final Batch Loss: 0.11325942724943161\n",
      "Epoch 117, Loss: 0.51945810765028, Final Batch Loss: 0.07414615154266357\n",
      "Epoch 118, Loss: 0.4659692049026489, Final Batch Loss: 0.06664687395095825\n",
      "Epoch 119, Loss: 0.5122368969023228, Final Batch Loss: 0.0607488639652729\n",
      "Epoch 120, Loss: 0.4578244611620903, Final Batch Loss: 0.06279721111059189\n",
      "Epoch 121, Loss: 0.527247741818428, Final Batch Loss: 0.13407929241657257\n",
      "Epoch 122, Loss: 0.561200350522995, Final Batch Loss: 0.12968429923057556\n",
      "Epoch 123, Loss: 0.5113345235586166, Final Batch Loss: 0.09089231491088867\n",
      "Epoch 124, Loss: 0.4973267875611782, Final Batch Loss: 0.06100388243794441\n",
      "Epoch 125, Loss: 0.5116461589932442, Final Batch Loss: 0.09080193936824799\n",
      "Epoch 126, Loss: 0.49680835753679276, Final Batch Loss: 0.1402357518672943\n",
      "Epoch 127, Loss: 0.45785152167081833, Final Batch Loss: 0.11552634090185165\n",
      "Epoch 128, Loss: 0.5147479698061943, Final Batch Loss: 0.12791872024536133\n",
      "Epoch 129, Loss: 0.5208944454789162, Final Batch Loss: 0.12435060739517212\n",
      "Epoch 130, Loss: 0.5098731778562069, Final Batch Loss: 0.11235862970352173\n",
      "Epoch 131, Loss: 0.5066774263978004, Final Batch Loss: 0.09441933035850525\n",
      "Epoch 132, Loss: 0.4815031513571739, Final Batch Loss: 0.08661416918039322\n",
      "Epoch 133, Loss: 0.4784163013100624, Final Batch Loss: 0.06787057965993881\n",
      "Epoch 134, Loss: 0.47985949367284775, Final Batch Loss: 0.08442382514476776\n",
      "Epoch 135, Loss: 0.5006266236305237, Final Batch Loss: 0.1596594601869583\n",
      "Epoch 136, Loss: 0.47621846944093704, Final Batch Loss: 0.07547589391469955\n",
      "Epoch 137, Loss: 0.502772867679596, Final Batch Loss: 0.06782332807779312\n",
      "Epoch 138, Loss: 0.46573031693696976, Final Batch Loss: 0.12601713836193085\n",
      "Epoch 139, Loss: 0.41071726754307747, Final Batch Loss: 0.03910409286618233\n",
      "Epoch 140, Loss: 0.43179433792829514, Final Batch Loss: 0.09179048240184784\n",
      "Epoch 141, Loss: 0.47505049407482147, Final Batch Loss: 0.03802265226840973\n",
      "Epoch 142, Loss: 0.5199063941836357, Final Batch Loss: 0.13205738365650177\n",
      "Epoch 143, Loss: 0.4273713007569313, Final Batch Loss: 0.07949505746364594\n",
      "Epoch 144, Loss: 0.5025914832949638, Final Batch Loss: 0.0950026586651802\n",
      "Epoch 145, Loss: 0.48718514293432236, Final Batch Loss: 0.11060678213834763\n",
      "Epoch 146, Loss: 0.48672834783792496, Final Batch Loss: 0.08006250113248825\n",
      "Epoch 147, Loss: 0.44908954948186874, Final Batch Loss: 0.09830943495035172\n",
      "Epoch 148, Loss: 0.48566804081201553, Final Batch Loss: 0.09369668364524841\n",
      "Epoch 149, Loss: 0.44506286829710007, Final Batch Loss: 0.07764894515275955\n",
      "Epoch 150, Loss: 0.4818265810608864, Final Batch Loss: 0.16968010365962982\n",
      "Epoch 151, Loss: 0.46939678117632866, Final Batch Loss: 0.1000119149684906\n",
      "Epoch 152, Loss: 0.544422946870327, Final Batch Loss: 0.1554931253194809\n",
      "Epoch 153, Loss: 0.46723275631666183, Final Batch Loss: 0.106556236743927\n",
      "Epoch 154, Loss: 0.4637237787246704, Final Batch Loss: 0.09060903638601303\n",
      "Epoch 155, Loss: 0.4810474291443825, Final Batch Loss: 0.14988447725772858\n",
      "Epoch 156, Loss: 0.4121066555380821, Final Batch Loss: 0.08586341887712479\n",
      "Epoch 157, Loss: 0.4433278329670429, Final Batch Loss: 0.05759282782673836\n",
      "Epoch 158, Loss: 0.4709579274058342, Final Batch Loss: 0.11637949198484421\n",
      "Epoch 159, Loss: 0.4798867926001549, Final Batch Loss: 0.10768876224756241\n",
      "Epoch 160, Loss: 0.444266851991415, Final Batch Loss: 0.1035052016377449\n",
      "Epoch 161, Loss: 0.4250878170132637, Final Batch Loss: 0.04853908717632294\n",
      "Epoch 162, Loss: 0.38914546743035316, Final Batch Loss: 0.060640912503004074\n",
      "Epoch 163, Loss: 0.45820360630750656, Final Batch Loss: 0.07426346093416214\n",
      "Epoch 164, Loss: 0.37185806781053543, Final Batch Loss: 0.04851335659623146\n",
      "Epoch 165, Loss: 0.4086993783712387, Final Batch Loss: 0.102933369576931\n",
      "Epoch 166, Loss: 0.4538920745253563, Final Batch Loss: 0.09445583820343018\n",
      "Epoch 167, Loss: 0.39017975330352783, Final Batch Loss: 0.059774018824100494\n",
      "Epoch 168, Loss: 0.3945406302809715, Final Batch Loss: 0.07561995089054108\n",
      "Epoch 169, Loss: 0.4821387529373169, Final Batch Loss: 0.08511084318161011\n",
      "Epoch 170, Loss: 0.4618799760937691, Final Batch Loss: 0.090519979596138\n",
      "Epoch 171, Loss: 0.37989436835050583, Final Batch Loss: 0.051127105951309204\n",
      "Epoch 172, Loss: 0.38847118616104126, Final Batch Loss: 0.0990593284368515\n",
      "Epoch 173, Loss: 0.43118803203105927, Final Batch Loss: 0.08990104496479034\n",
      "Epoch 174, Loss: 0.42366328462958336, Final Batch Loss: 0.11307668685913086\n",
      "Epoch 175, Loss: 0.41988392174243927, Final Batch Loss: 0.06253290176391602\n",
      "Epoch 176, Loss: 0.3998296558856964, Final Batch Loss: 0.05395576357841492\n",
      "Epoch 177, Loss: 0.43298083916306496, Final Batch Loss: 0.07809125632047653\n",
      "Epoch 178, Loss: 0.3937624841928482, Final Batch Loss: 0.049037300050258636\n",
      "Epoch 179, Loss: 0.384384848177433, Final Batch Loss: 0.06513486057519913\n",
      "Epoch 180, Loss: 0.4058467634022236, Final Batch Loss: 0.10022371262311935\n",
      "Epoch 181, Loss: 0.43306444957852364, Final Batch Loss: 0.10546790808439255\n",
      "Epoch 182, Loss: 0.4202512837946415, Final Batch Loss: 0.09770068526268005\n",
      "Epoch 183, Loss: 0.4332943595945835, Final Batch Loss: 0.054352838546037674\n",
      "Epoch 184, Loss: 0.3885425738990307, Final Batch Loss: 0.061640720814466476\n",
      "Epoch 185, Loss: 0.4301949068903923, Final Batch Loss: 0.08097436279058456\n",
      "Epoch 186, Loss: 0.39641764014959335, Final Batch Loss: 0.0749279260635376\n",
      "Epoch 187, Loss: 0.41071707010269165, Final Batch Loss: 0.08732612431049347\n",
      "Epoch 188, Loss: 0.40638842433691025, Final Batch Loss: 0.13457787036895752\n",
      "Epoch 189, Loss: 0.37759802117943764, Final Batch Loss: 0.07741319388151169\n",
      "Epoch 190, Loss: 0.4187422841787338, Final Batch Loss: 0.07730929553508759\n",
      "Epoch 191, Loss: 0.4054819084703922, Final Batch Loss: 0.1272365003824234\n",
      "Epoch 192, Loss: 0.4226733297109604, Final Batch Loss: 0.09126158058643341\n",
      "Epoch 193, Loss: 0.42344047129154205, Final Batch Loss: 0.1508481651544571\n",
      "Epoch 194, Loss: 0.4376709684729576, Final Batch Loss: 0.1081976369023323\n",
      "Epoch 195, Loss: 0.35345424711704254, Final Batch Loss: 0.0599115714430809\n",
      "Epoch 196, Loss: 0.4098856672644615, Final Batch Loss: 0.10862480849027634\n",
      "Epoch 197, Loss: 0.38083625584840775, Final Batch Loss: 0.0672697126865387\n",
      "Epoch 198, Loss: 0.390222005546093, Final Batch Loss: 0.09035996347665787\n",
      "Epoch 199, Loss: 0.3817853145301342, Final Batch Loss: 0.1278051733970642\n",
      "Epoch 200, Loss: 0.41486863791942596, Final Batch Loss: 0.16058732569217682\n",
      "Epoch 201, Loss: 0.36119233816862106, Final Batch Loss: 0.052157871425151825\n",
      "Epoch 202, Loss: 0.35904165729880333, Final Batch Loss: 0.037833113223314285\n",
      "Epoch 203, Loss: 0.3652275428175926, Final Batch Loss: 0.10528771579265594\n",
      "Epoch 204, Loss: 0.3406624533236027, Final Batch Loss: 0.05134692043066025\n",
      "Epoch 205, Loss: 0.319166611880064, Final Batch Loss: 0.020460613071918488\n",
      "Epoch 206, Loss: 0.3519224151968956, Final Batch Loss: 0.051952823996543884\n",
      "Epoch 207, Loss: 0.3841363415122032, Final Batch Loss: 0.04363716393709183\n",
      "Epoch 208, Loss: 0.36368652805685997, Final Batch Loss: 0.07344675809144974\n",
      "Epoch 209, Loss: 0.3683243654668331, Final Batch Loss: 0.09471524506807327\n",
      "Epoch 210, Loss: 0.3428601399064064, Final Batch Loss: 0.046601440757513046\n",
      "Epoch 211, Loss: 0.3622254841029644, Final Batch Loss: 0.0677795484662056\n",
      "Epoch 212, Loss: 0.33898913115262985, Final Batch Loss: 0.05919460952281952\n",
      "Epoch 213, Loss: 0.34454960376024246, Final Batch Loss: 0.06473953276872635\n",
      "Epoch 214, Loss: 0.34074899554252625, Final Batch Loss: 0.0434417650103569\n",
      "Epoch 215, Loss: 0.3895394839346409, Final Batch Loss: 0.05951355770230293\n",
      "Epoch 216, Loss: 0.3773573003709316, Final Batch Loss: 0.08507009595632553\n",
      "Epoch 217, Loss: 0.39802418276667595, Final Batch Loss: 0.07879820466041565\n",
      "Epoch 218, Loss: 0.3618936762213707, Final Batch Loss: 0.08444656431674957\n",
      "Epoch 219, Loss: 0.3612721785902977, Final Batch Loss: 0.08425398916006088\n",
      "Epoch 220, Loss: 0.39150119572877884, Final Batch Loss: 0.1273191273212433\n",
      "Epoch 221, Loss: 0.35116564109921455, Final Batch Loss: 0.08630795031785965\n",
      "Epoch 222, Loss: 0.3695959933102131, Final Batch Loss: 0.08276009559631348\n",
      "Epoch 223, Loss: 0.3075263872742653, Final Batch Loss: 0.06183762475848198\n",
      "Epoch 224, Loss: 0.36722589656710625, Final Batch Loss: 0.02883061021566391\n",
      "Epoch 225, Loss: 0.36948386952281, Final Batch Loss: 0.056064870208501816\n",
      "Epoch 226, Loss: 0.31136389449238777, Final Batch Loss: 0.08293218910694122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227, Loss: 0.3687324710190296, Final Batch Loss: 0.06734095513820648\n",
      "Epoch 228, Loss: 0.3545900508761406, Final Batch Loss: 0.0641435906291008\n",
      "Epoch 229, Loss: 0.3388477936387062, Final Batch Loss: 0.061276718974113464\n",
      "Epoch 230, Loss: 0.2863023355603218, Final Batch Loss: 0.04311457648873329\n",
      "Epoch 231, Loss: 0.35367535054683685, Final Batch Loss: 0.05220350995659828\n",
      "Epoch 232, Loss: 0.35898149013519287, Final Batch Loss: 0.08822786062955856\n",
      "Epoch 233, Loss: 0.3333469033241272, Final Batch Loss: 0.041659049689769745\n",
      "Epoch 234, Loss: 0.362533301115036, Final Batch Loss: 0.07965131849050522\n",
      "Epoch 235, Loss: 0.32908032834529877, Final Batch Loss: 0.06945431977510452\n",
      "Epoch 236, Loss: 0.37125492468476295, Final Batch Loss: 0.07178544998168945\n",
      "Epoch 237, Loss: 0.3761991448700428, Final Batch Loss: 0.11048595607280731\n",
      "Epoch 238, Loss: 0.37429559230804443, Final Batch Loss: 0.14579340815544128\n",
      "Epoch 239, Loss: 0.3008005805313587, Final Batch Loss: 0.06329396367073059\n",
      "Epoch 240, Loss: 0.3738529123365879, Final Batch Loss: 0.08133094012737274\n",
      "Epoch 241, Loss: 0.3443913795053959, Final Batch Loss: 0.06450109928846359\n",
      "Epoch 242, Loss: 0.33876635879278183, Final Batch Loss: 0.056802794337272644\n",
      "Epoch 243, Loss: 0.3333781063556671, Final Batch Loss: 0.059906262904405594\n",
      "Epoch 244, Loss: 0.329052172601223, Final Batch Loss: 0.03281191736459732\n",
      "Epoch 245, Loss: 0.3576311096549034, Final Batch Loss: 0.035514481365680695\n",
      "Epoch 246, Loss: 0.3050899188965559, Final Batch Loss: 0.03023822046816349\n",
      "Epoch 247, Loss: 0.32337474077939987, Final Batch Loss: 0.08267489820718765\n",
      "Epoch 248, Loss: 0.30492186173796654, Final Batch Loss: 0.03374001011252403\n",
      "Epoch 249, Loss: 0.28065669536590576, Final Batch Loss: 0.044458262622356415\n",
      "Epoch 250, Loss: 0.3456116020679474, Final Batch Loss: 0.04330413043498993\n",
      "Epoch 251, Loss: 0.3278369940817356, Final Batch Loss: 0.0742269903421402\n",
      "Epoch 252, Loss: 0.32008959725499153, Final Batch Loss: 0.07398597151041031\n",
      "Epoch 253, Loss: 0.3213949613273144, Final Batch Loss: 0.06502770632505417\n",
      "Epoch 254, Loss: 0.31380628421902657, Final Batch Loss: 0.09239938855171204\n",
      "Epoch 255, Loss: 0.3386033661663532, Final Batch Loss: 0.0567965991795063\n",
      "Epoch 256, Loss: 0.3205742761492729, Final Batch Loss: 0.043505094945430756\n",
      "Epoch 257, Loss: 0.32057399675250053, Final Batch Loss: 0.06075669080018997\n",
      "Epoch 258, Loss: 0.3087463043630123, Final Batch Loss: 0.06028275191783905\n",
      "Epoch 259, Loss: 0.31447170302271843, Final Batch Loss: 0.037886057049036026\n",
      "Epoch 260, Loss: 0.31636684015393257, Final Batch Loss: 0.10798858851194382\n",
      "Epoch 261, Loss: 0.3260640688240528, Final Batch Loss: 0.0771210715174675\n",
      "Epoch 262, Loss: 0.2801433466374874, Final Batch Loss: 0.04861706867814064\n",
      "Epoch 263, Loss: 0.2801775857806206, Final Batch Loss: 0.01695910096168518\n",
      "Epoch 264, Loss: 0.2837711237370968, Final Batch Loss: 0.05641588196158409\n",
      "Epoch 265, Loss: 0.37350820377469063, Final Batch Loss: 0.15246854722499847\n",
      "Epoch 266, Loss: 0.3082694485783577, Final Batch Loss: 0.07946145534515381\n",
      "Epoch 267, Loss: 0.2800487205386162, Final Batch Loss: 0.06299442797899246\n",
      "Epoch 268, Loss: 0.3178965263068676, Final Batch Loss: 0.06349407881498337\n",
      "Epoch 269, Loss: 0.2975584827363491, Final Batch Loss: 0.058803874999284744\n",
      "Epoch 270, Loss: 0.2632613517343998, Final Batch Loss: 0.020296722650527954\n",
      "Epoch 271, Loss: 0.3170732669532299, Final Batch Loss: 0.059861958026885986\n",
      "Epoch 272, Loss: 0.301833376288414, Final Batch Loss: 0.05497325584292412\n",
      "Epoch 273, Loss: 0.339275524020195, Final Batch Loss: 0.15451829135417938\n",
      "Epoch 274, Loss: 0.29100964590907097, Final Batch Loss: 0.04730531573295593\n",
      "Epoch 275, Loss: 0.30800724402070045, Final Batch Loss: 0.07252005487680435\n",
      "Epoch 276, Loss: 0.3175303712487221, Final Batch Loss: 0.07090312987565994\n",
      "Epoch 277, Loss: 0.29985252022743225, Final Batch Loss: 0.05324501171708107\n",
      "Epoch 278, Loss: 0.2898348793387413, Final Batch Loss: 0.05233452096581459\n",
      "Epoch 279, Loss: 0.30756473541259766, Final Batch Loss: 0.042119793593883514\n",
      "Epoch 280, Loss: 0.2632837388664484, Final Batch Loss: 0.023488057777285576\n",
      "Epoch 281, Loss: 0.29488990642130375, Final Batch Loss: 0.07144618034362793\n",
      "Epoch 282, Loss: 0.28899384289979935, Final Batch Loss: 0.06385447084903717\n",
      "Epoch 283, Loss: 0.31024985387921333, Final Batch Loss: 0.0753229483962059\n",
      "Epoch 284, Loss: 0.3320055566728115, Final Batch Loss: 0.10427162051200867\n",
      "Epoch 285, Loss: 0.2783542312681675, Final Batch Loss: 0.054910093545913696\n",
      "Epoch 286, Loss: 0.2985284570604563, Final Batch Loss: 0.0662401020526886\n",
      "Epoch 287, Loss: 0.3598024398088455, Final Batch Loss: 0.08614294230937958\n",
      "Epoch 288, Loss: 0.2917209602892399, Final Batch Loss: 0.04929845407605171\n",
      "Epoch 289, Loss: 0.31191639974713326, Final Batch Loss: 0.07970278710126877\n",
      "Epoch 290, Loss: 0.3012196682393551, Final Batch Loss: 0.03943054378032684\n",
      "Epoch 291, Loss: 0.24892213009297848, Final Batch Loss: 0.016824306920170784\n",
      "Epoch 292, Loss: 0.2578032948076725, Final Batch Loss: 0.03393494337797165\n",
      "Epoch 293, Loss: 0.2617355193942785, Final Batch Loss: 0.019354207441210747\n",
      "Epoch 294, Loss: 0.30980877205729485, Final Batch Loss: 0.06557577103376389\n",
      "Epoch 295, Loss: 0.2763052135705948, Final Batch Loss: 0.060943860560655594\n",
      "Epoch 296, Loss: 0.3375651612877846, Final Batch Loss: 0.12972494959831238\n",
      "Epoch 297, Loss: 0.3020220845937729, Final Batch Loss: 0.034213725477457047\n",
      "Epoch 298, Loss: 0.28283657133579254, Final Batch Loss: 0.032377734780311584\n",
      "Epoch 299, Loss: 0.2691008523106575, Final Batch Loss: 0.0414278507232666\n",
      "Epoch 300, Loss: 0.287816546857357, Final Batch Loss: 0.04193952679634094\n",
      "Epoch 301, Loss: 0.28704826161265373, Final Batch Loss: 0.05063122510910034\n",
      "Epoch 302, Loss: 0.31324881315231323, Final Batch Loss: 0.09388680756092072\n",
      "Epoch 303, Loss: 0.2714550346136093, Final Batch Loss: 0.060860902070999146\n",
      "Epoch 304, Loss: 0.3182222321629524, Final Batch Loss: 0.044144827872514725\n",
      "Epoch 305, Loss: 0.2591969184577465, Final Batch Loss: 0.053850527852773666\n",
      "Epoch 306, Loss: 0.3023827914148569, Final Batch Loss: 0.017386047169566154\n",
      "Epoch 307, Loss: 0.25907679460942745, Final Batch Loss: 0.02303454838693142\n",
      "Epoch 308, Loss: 0.27008383348584175, Final Batch Loss: 0.0477621927857399\n",
      "Epoch 309, Loss: 0.28357137739658356, Final Batch Loss: 0.05202070251107216\n",
      "Epoch 310, Loss: 0.25597459077835083, Final Batch Loss: 0.04286423325538635\n",
      "Epoch 311, Loss: 0.2915448322892189, Final Batch Loss: 0.06598645448684692\n",
      "Epoch 312, Loss: 0.27772853523492813, Final Batch Loss: 0.06780464202165604\n",
      "Epoch 313, Loss: 0.2435027938336134, Final Batch Loss: 0.04801289364695549\n",
      "Epoch 314, Loss: 0.3090336285531521, Final Batch Loss: 0.06878629326820374\n",
      "Epoch 315, Loss: 0.2904856465756893, Final Batch Loss: 0.058854274451732635\n",
      "Epoch 316, Loss: 0.2890552021563053, Final Batch Loss: 0.05681736394762993\n",
      "Epoch 317, Loss: 0.27360762655735016, Final Batch Loss: 0.046896714717149734\n",
      "Epoch 318, Loss: 0.28703033924102783, Final Batch Loss: 0.09922919422388077\n",
      "Epoch 319, Loss: 0.27026357129216194, Final Batch Loss: 0.06202911585569382\n",
      "Epoch 320, Loss: 0.319168034940958, Final Batch Loss: 0.08392049372196198\n",
      "Epoch 321, Loss: 0.2562039792537689, Final Batch Loss: 0.03901917114853859\n",
      "Epoch 322, Loss: 0.2355177365243435, Final Batch Loss: 0.05296545848250389\n",
      "Epoch 323, Loss: 0.279416523873806, Final Batch Loss: 0.07668913155794144\n",
      "Epoch 324, Loss: 0.2590555511415005, Final Batch Loss: 0.03365178406238556\n",
      "Epoch 325, Loss: 0.296513918787241, Final Batch Loss: 0.1265525221824646\n",
      "Epoch 326, Loss: 0.22429281659424305, Final Batch Loss: 0.04594894498586655\n",
      "Epoch 327, Loss: 0.284106083214283, Final Batch Loss: 0.050300490111112595\n",
      "Epoch 328, Loss: 0.27532775327563286, Final Batch Loss: 0.06799958646297455\n",
      "Epoch 329, Loss: 0.2779937610030174, Final Batch Loss: 0.06703601777553558\n",
      "Epoch 330, Loss: 0.29863234981894493, Final Batch Loss: 0.10560400784015656\n",
      "Epoch 331, Loss: 0.30286553502082825, Final Batch Loss: 0.05543912202119827\n",
      "Epoch 332, Loss: 0.2599654644727707, Final Batch Loss: 0.041742824018001556\n",
      "Epoch 333, Loss: 0.3019764609634876, Final Batch Loss: 0.13233539462089539\n",
      "Epoch 334, Loss: 0.31343765556812286, Final Batch Loss: 0.05895450711250305\n",
      "Epoch 335, Loss: 0.30423901975154877, Final Batch Loss: 0.12698133289813995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336, Loss: 0.2492009550333023, Final Batch Loss: 0.045588232576847076\n",
      "Epoch 337, Loss: 0.25377921387553215, Final Batch Loss: 0.03965665400028229\n",
      "Epoch 338, Loss: 0.2658086158335209, Final Batch Loss: 0.05555933713912964\n",
      "Epoch 339, Loss: 0.2528484873473644, Final Batch Loss: 0.03517730161547661\n",
      "Epoch 340, Loss: 0.2665396109223366, Final Batch Loss: 0.07165929675102234\n",
      "Epoch 341, Loss: 0.2476720940321684, Final Batch Loss: 0.028350530192255974\n",
      "Epoch 342, Loss: 0.2554362379014492, Final Batch Loss: 0.05680759623646736\n",
      "Epoch 343, Loss: 0.25100160762667656, Final Batch Loss: 0.009861074388027191\n",
      "Epoch 344, Loss: 0.2638317905366421, Final Batch Loss: 0.06936422735452652\n",
      "Epoch 345, Loss: 0.22700433060526848, Final Batch Loss: 0.016410496085882187\n",
      "Epoch 346, Loss: 0.24839998595416546, Final Batch Loss: 0.048788271844387054\n",
      "Epoch 347, Loss: 0.264798853546381, Final Batch Loss: 0.0371277891099453\n",
      "Epoch 348, Loss: 0.2689625062048435, Final Batch Loss: 0.04557320848107338\n",
      "Epoch 349, Loss: 0.2757069319486618, Final Batch Loss: 0.058573853224515915\n",
      "Epoch 350, Loss: 0.24449383839964867, Final Batch Loss: 0.03659499064087868\n",
      "Epoch 351, Loss: 0.26430726423859596, Final Batch Loss: 0.026593323796987534\n",
      "Epoch 352, Loss: 0.23549019172787666, Final Batch Loss: 0.05010591447353363\n",
      "Epoch 353, Loss: 0.27315106615424156, Final Batch Loss: 0.0370168462395668\n",
      "Epoch 354, Loss: 0.308888491243124, Final Batch Loss: 0.10781017690896988\n",
      "Epoch 355, Loss: 0.23909375071525574, Final Batch Loss: 0.013851646333932877\n",
      "Epoch 356, Loss: 0.2980072423815727, Final Batch Loss: 0.11557450145483017\n",
      "Epoch 357, Loss: 0.236740380525589, Final Batch Loss: 0.01929635927081108\n",
      "Epoch 358, Loss: 0.2447926327586174, Final Batch Loss: 0.059127453714609146\n",
      "Epoch 359, Loss: 0.26676213927567005, Final Batch Loss: 0.06285415589809418\n",
      "Epoch 360, Loss: 0.2730028498917818, Final Batch Loss: 0.08999796956777573\n",
      "Epoch 361, Loss: 0.2676572762429714, Final Batch Loss: 0.10846574604511261\n",
      "Epoch 362, Loss: 0.2612535171210766, Final Batch Loss: 0.046063218265771866\n",
      "Epoch 363, Loss: 0.21382357366383076, Final Batch Loss: 0.019277315586805344\n",
      "Epoch 364, Loss: 0.26532136276364326, Final Batch Loss: 0.03632159158587456\n",
      "Epoch 365, Loss: 0.2437195498496294, Final Batch Loss: 0.04281316325068474\n",
      "Epoch 366, Loss: 0.25438930094242096, Final Batch Loss: 0.050018344074487686\n",
      "Epoch 367, Loss: 0.2387822438031435, Final Batch Loss: 0.028560569509863853\n",
      "Epoch 368, Loss: 0.24473146721720695, Final Batch Loss: 0.039114706218242645\n",
      "Epoch 369, Loss: 0.21287013590335846, Final Batch Loss: 0.015314972028136253\n",
      "Epoch 370, Loss: 0.2592516541481018, Final Batch Loss: 0.04959522560238838\n",
      "Epoch 371, Loss: 0.2490791231393814, Final Batch Loss: 0.05929975211620331\n",
      "Epoch 372, Loss: 0.23492272570729256, Final Batch Loss: 0.05820649117231369\n",
      "Epoch 373, Loss: 0.2604391612112522, Final Batch Loss: 0.03728943318128586\n",
      "Epoch 374, Loss: 0.2951718792319298, Final Batch Loss: 0.08073574304580688\n",
      "Epoch 375, Loss: 0.21100440248847008, Final Batch Loss: 0.04691002890467644\n",
      "Epoch 376, Loss: 0.23687315732240677, Final Batch Loss: 0.04006476327776909\n",
      "Epoch 377, Loss: 0.2523234002292156, Final Batch Loss: 0.04003199189901352\n",
      "Epoch 378, Loss: 0.2234873827546835, Final Batch Loss: 0.023829391226172447\n",
      "Epoch 379, Loss: 0.2478106301277876, Final Batch Loss: 0.02782507613301277\n",
      "Epoch 380, Loss: 0.22293199971318245, Final Batch Loss: 0.07002776861190796\n",
      "Epoch 381, Loss: 0.22585059888660908, Final Batch Loss: 0.035808660089969635\n",
      "Epoch 382, Loss: 0.2160738781094551, Final Batch Loss: 0.021779261529445648\n",
      "Epoch 383, Loss: 0.20911786146461964, Final Batch Loss: 0.017265940085053444\n",
      "Epoch 384, Loss: 0.25033770501613617, Final Batch Loss: 0.042498767375946045\n",
      "Epoch 385, Loss: 0.20064276456832886, Final Batch Loss: 0.02252453751862049\n",
      "Epoch 386, Loss: 0.24749578535556793, Final Batch Loss: 0.018058650195598602\n",
      "Epoch 387, Loss: 0.24963529407978058, Final Batch Loss: 0.04225129261612892\n",
      "Epoch 388, Loss: 0.2192807551473379, Final Batch Loss: 0.03749335929751396\n",
      "Epoch 389, Loss: 0.25645752623677254, Final Batch Loss: 0.04967375844717026\n",
      "Epoch 390, Loss: 0.2160380370914936, Final Batch Loss: 0.05763515457510948\n",
      "Epoch 391, Loss: 0.22843267023563385, Final Batch Loss: 0.033303242176771164\n",
      "Epoch 392, Loss: 0.23406622931361198, Final Batch Loss: 0.05997241288423538\n",
      "Epoch 393, Loss: 0.24029136076569557, Final Batch Loss: 0.048076990991830826\n",
      "Epoch 394, Loss: 0.26246151328086853, Final Batch Loss: 0.04190004989504814\n",
      "Epoch 395, Loss: 0.2494063936173916, Final Batch Loss: 0.048722878098487854\n",
      "Epoch 396, Loss: 0.24894962087273598, Final Batch Loss: 0.025261271744966507\n",
      "Epoch 397, Loss: 0.21179693937301636, Final Batch Loss: 0.01702292636036873\n",
      "Epoch 398, Loss: 0.22462928108870983, Final Batch Loss: 0.021230077371001244\n",
      "Epoch 399, Loss: 0.23986255377531052, Final Batch Loss: 0.04955674707889557\n",
      "Epoch 400, Loss: 0.2340195383876562, Final Batch Loss: 0.03779676929116249\n",
      "Epoch 401, Loss: 0.2401789017021656, Final Batch Loss: 0.08691655844449997\n",
      "Epoch 402, Loss: 0.22525851987302303, Final Batch Loss: 0.029242897406220436\n",
      "Epoch 403, Loss: 0.2204714361578226, Final Batch Loss: 0.03496960550546646\n",
      "Epoch 404, Loss: 0.226652842015028, Final Batch Loss: 0.04020221158862114\n",
      "Epoch 405, Loss: 0.26410972513258457, Final Batch Loss: 0.0995173454284668\n",
      "Epoch 406, Loss: 0.22889654338359833, Final Batch Loss: 0.03330013155937195\n",
      "Epoch 407, Loss: 0.22364523448050022, Final Batch Loss: 0.021302061155438423\n",
      "Epoch 408, Loss: 0.22477025352418423, Final Batch Loss: 0.03706134483218193\n",
      "Epoch 409, Loss: 0.19445161148905754, Final Batch Loss: 0.03451910614967346\n",
      "Epoch 410, Loss: 0.23640776053071022, Final Batch Loss: 0.04374874755740166\n",
      "Epoch 411, Loss: 0.24157599918544292, Final Batch Loss: 0.05917415767908096\n",
      "Epoch 412, Loss: 0.2082773968577385, Final Batch Loss: 0.05443374067544937\n",
      "Epoch 413, Loss: 0.24709131196141243, Final Batch Loss: 0.07176005095243454\n",
      "Epoch 414, Loss: 0.2113810870796442, Final Batch Loss: 0.030841587111353874\n",
      "Epoch 415, Loss: 0.18484554067254066, Final Batch Loss: 0.011995721608400345\n",
      "Epoch 416, Loss: 0.23496275767683983, Final Batch Loss: 0.06359373033046722\n",
      "Epoch 417, Loss: 0.2152267824858427, Final Batch Loss: 0.028562946245074272\n",
      "Epoch 418, Loss: 0.22339872643351555, Final Batch Loss: 0.058537114411592484\n",
      "Epoch 419, Loss: 0.2505986951291561, Final Batch Loss: 0.06945783644914627\n",
      "Epoch 420, Loss: 0.2163746040314436, Final Batch Loss: 0.040164411067962646\n",
      "Epoch 421, Loss: 0.2870789822191, Final Batch Loss: 0.04041210189461708\n",
      "Epoch 422, Loss: 0.2550301942974329, Final Batch Loss: 0.08534364402294159\n",
      "Epoch 423, Loss: 0.18268312700092793, Final Batch Loss: 0.012070884928107262\n",
      "Epoch 424, Loss: 0.1986477468162775, Final Batch Loss: 0.03779437392950058\n",
      "Epoch 425, Loss: 0.23684420436620712, Final Batch Loss: 0.0607268363237381\n",
      "Epoch 426, Loss: 0.24359995499253273, Final Batch Loss: 0.054082632064819336\n",
      "Epoch 427, Loss: 0.24499471299350262, Final Batch Loss: 0.05000770092010498\n",
      "Epoch 428, Loss: 0.20802828297019005, Final Batch Loss: 0.03512321040034294\n",
      "Epoch 429, Loss: 0.20328690111637115, Final Batch Loss: 0.06930112838745117\n",
      "Epoch 430, Loss: 0.1982748843729496, Final Batch Loss: 0.050940804183483124\n",
      "Epoch 431, Loss: 0.19137209746986628, Final Batch Loss: 0.011264410801231861\n",
      "Epoch 432, Loss: 0.22715146839618683, Final Batch Loss: 0.04946715012192726\n",
      "Epoch 433, Loss: 0.21404529735445976, Final Batch Loss: 0.07428524643182755\n",
      "Epoch 434, Loss: 0.201532619073987, Final Batch Loss: 0.04446631297469139\n",
      "Epoch 435, Loss: 0.19369239173829556, Final Batch Loss: 0.0541333369910717\n",
      "Epoch 436, Loss: 0.22741785272955894, Final Batch Loss: 0.03609476238489151\n",
      "Epoch 437, Loss: 0.19293317385017872, Final Batch Loss: 0.034874867647886276\n",
      "Epoch 438, Loss: 0.21559436433017254, Final Batch Loss: 0.03142092004418373\n",
      "Epoch 439, Loss: 0.23913428001105785, Final Batch Loss: 0.050537753850221634\n",
      "Epoch 440, Loss: 0.2302487324923277, Final Batch Loss: 0.06245974823832512\n",
      "Epoch 441, Loss: 0.16712909750640392, Final Batch Loss: 0.022997679188847542\n",
      "Epoch 442, Loss: 0.2559904232621193, Final Batch Loss: 0.07446392625570297\n",
      "Epoch 443, Loss: 0.23956511914730072, Final Batch Loss: 0.1109442338347435\n",
      "Epoch 444, Loss: 0.2151972148567438, Final Batch Loss: 0.06141434237360954\n",
      "Epoch 445, Loss: 0.19437871128320694, Final Batch Loss: 0.036452483385801315\n",
      "Epoch 446, Loss: 0.18574046157300472, Final Batch Loss: 0.014831149950623512\n",
      "Epoch 447, Loss: 0.21509451046586037, Final Batch Loss: 0.06747982650995255\n",
      "Epoch 448, Loss: 0.18275059200823307, Final Batch Loss: 0.031431008130311966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449, Loss: 0.20442124642431736, Final Batch Loss: 0.0513463020324707\n",
      "Epoch 450, Loss: 0.23284713551402092, Final Batch Loss: 0.06829481571912766\n",
      "Epoch 451, Loss: 0.17957697808742523, Final Batch Loss: 0.01626797951757908\n",
      "Epoch 452, Loss: 0.1720428615808487, Final Batch Loss: 0.028430573642253876\n",
      "Epoch 453, Loss: 0.2097055036574602, Final Batch Loss: 0.0388512909412384\n",
      "Epoch 454, Loss: 0.19332911632955074, Final Batch Loss: 0.04892611503601074\n",
      "Epoch 455, Loss: 0.20896460115909576, Final Batch Loss: 0.04972468689084053\n",
      "Epoch 456, Loss: 0.19703618623316288, Final Batch Loss: 0.013098282739520073\n",
      "Epoch 457, Loss: 0.22275926172733307, Final Batch Loss: 0.05670606717467308\n",
      "Epoch 458, Loss: 0.20456665009260178, Final Batch Loss: 0.032137785106897354\n",
      "Epoch 459, Loss: 0.21903721801936626, Final Batch Loss: 0.0414656326174736\n",
      "Epoch 460, Loss: 0.17808975279331207, Final Batch Loss: 0.030976174399256706\n",
      "Epoch 461, Loss: 0.17752198688685894, Final Batch Loss: 0.023175235837697983\n",
      "Epoch 462, Loss: 0.1732902256771922, Final Batch Loss: 0.013007658533751965\n",
      "Epoch 463, Loss: 0.18053786270320415, Final Batch Loss: 0.027724437415599823\n",
      "Epoch 464, Loss: 0.1809147698804736, Final Batch Loss: 0.046693675220012665\n",
      "Epoch 465, Loss: 0.19536669738590717, Final Batch Loss: 0.03706565126776695\n",
      "Epoch 466, Loss: 0.1774482410401106, Final Batch Loss: 0.04654179513454437\n",
      "Epoch 467, Loss: 0.17422938346862793, Final Batch Loss: 0.02750094048678875\n",
      "Epoch 468, Loss: 0.19091465324163437, Final Batch Loss: 0.06133392080664635\n",
      "Epoch 469, Loss: 0.19895506650209427, Final Batch Loss: 0.043112628161907196\n",
      "Epoch 470, Loss: 0.20645719952881336, Final Batch Loss: 0.024138009175658226\n",
      "Epoch 471, Loss: 0.18997478298842907, Final Batch Loss: 0.0392996110022068\n",
      "Epoch 472, Loss: 0.20611821673810482, Final Batch Loss: 0.06843347102403641\n",
      "Epoch 473, Loss: 0.2021178901195526, Final Batch Loss: 0.06066856533288956\n",
      "Epoch 474, Loss: 0.1735150497406721, Final Batch Loss: 0.02505175769329071\n",
      "Epoch 475, Loss: 0.2171742245554924, Final Batch Loss: 0.03658941760659218\n",
      "Epoch 476, Loss: 0.16016638930886984, Final Batch Loss: 0.028009014204144478\n",
      "Epoch 477, Loss: 0.20357126742601395, Final Batch Loss: 0.05124753341078758\n",
      "Epoch 478, Loss: 0.17711795307695866, Final Batch Loss: 0.0717480331659317\n",
      "Epoch 479, Loss: 0.1909580808132887, Final Batch Loss: 0.04012012109160423\n",
      "Epoch 480, Loss: 0.19664321281015873, Final Batch Loss: 0.02657969295978546\n",
      "Epoch 481, Loss: 0.18507317639887333, Final Batch Loss: 0.027086280286312103\n",
      "Epoch 482, Loss: 0.1866931188851595, Final Batch Loss: 0.03630521520972252\n",
      "Epoch 483, Loss: 0.17539953626692295, Final Batch Loss: 0.035425152629613876\n",
      "Epoch 484, Loss: 0.20427174493670464, Final Batch Loss: 0.027232646942138672\n",
      "Epoch 485, Loss: 0.187265420332551, Final Batch Loss: 0.02821558341383934\n",
      "Epoch 486, Loss: 0.20476057473570108, Final Batch Loss: 0.060833293944597244\n",
      "Epoch 487, Loss: 0.17497320100665092, Final Batch Loss: 0.03989114612340927\n",
      "Epoch 488, Loss: 0.16779229417443275, Final Batch Loss: 0.02043239027261734\n",
      "Epoch 489, Loss: 0.16171303391456604, Final Batch Loss: 0.02684134989976883\n",
      "Epoch 490, Loss: 0.1587495943531394, Final Batch Loss: 0.006094439886510372\n",
      "Epoch 491, Loss: 0.18220075964927673, Final Batch Loss: 0.03939563408493996\n",
      "Epoch 492, Loss: 0.14566382206976414, Final Batch Loss: 0.017364507541060448\n",
      "Epoch 493, Loss: 0.15942866634577513, Final Batch Loss: 0.011055105365812778\n",
      "Epoch 494, Loss: 0.21725599840283394, Final Batch Loss: 0.028810981661081314\n",
      "Epoch 495, Loss: 0.19756853207945824, Final Batch Loss: 0.05505847558379173\n",
      "Epoch 496, Loss: 0.1429900610819459, Final Batch Loss: 0.008774003945291042\n",
      "Epoch 497, Loss: 0.17723337560892105, Final Batch Loss: 0.07356561720371246\n",
      "Epoch 498, Loss: 0.20532722398638725, Final Batch Loss: 0.062001682817935944\n",
      "Epoch 499, Loss: 0.1596094537526369, Final Batch Loss: 0.009048344567418098\n",
      "Epoch 500, Loss: 0.15904295071959496, Final Batch Loss: 0.0317235104739666\n",
      "Epoch 501, Loss: 0.1967296376824379, Final Batch Loss: 0.00702742300927639\n",
      "Epoch 502, Loss: 0.16334134340286255, Final Batch Loss: 0.013853097334504128\n",
      "Epoch 503, Loss: 0.1812243452295661, Final Batch Loss: 0.07101982086896896\n",
      "Epoch 504, Loss: 0.1584656871855259, Final Batch Loss: 0.02413344196975231\n",
      "Epoch 505, Loss: 0.15825711749494076, Final Batch Loss: 0.027504900470376015\n",
      "Epoch 506, Loss: 0.1704868208616972, Final Batch Loss: 0.04939704388380051\n",
      "Epoch 507, Loss: 0.14320708950981498, Final Batch Loss: 0.023194825276732445\n",
      "Epoch 508, Loss: 0.17079215217381716, Final Batch Loss: 0.03926981985569\n",
      "Epoch 509, Loss: 0.13710480649024248, Final Batch Loss: 0.009904487989842892\n",
      "Epoch 510, Loss: 0.14993464387953281, Final Batch Loss: 0.03775596618652344\n",
      "Epoch 511, Loss: 0.16696914192289114, Final Batch Loss: 0.032498594373464584\n",
      "Epoch 512, Loss: 0.14087381958961487, Final Batch Loss: 0.026384880766272545\n",
      "Epoch 513, Loss: 0.17246908508241177, Final Batch Loss: 0.04476693272590637\n",
      "Epoch 514, Loss: 0.15286620520055294, Final Batch Loss: 0.014506859704852104\n",
      "Epoch 515, Loss: 0.1450612503103912, Final Batch Loss: 0.021775947883725166\n",
      "Epoch 516, Loss: 0.15156171098351479, Final Batch Loss: 0.053533706814050674\n",
      "Epoch 517, Loss: 0.16708395583555102, Final Batch Loss: 0.010744759812951088\n",
      "Epoch 518, Loss: 0.17163255531340837, Final Batch Loss: 0.04238877072930336\n",
      "Epoch 519, Loss: 0.15520169772207737, Final Batch Loss: 0.03635667636990547\n",
      "Epoch 520, Loss: 0.14523622393608093, Final Batch Loss: 0.03554597496986389\n",
      "Epoch 521, Loss: 0.14198122639209032, Final Batch Loss: 0.03502965718507767\n",
      "Epoch 522, Loss: 0.17372758872807026, Final Batch Loss: 0.02777424268424511\n",
      "Epoch 523, Loss: 0.14890834409743547, Final Batch Loss: 0.03534821793437004\n",
      "Epoch 524, Loss: 0.16401460487395525, Final Batch Loss: 0.015298346988856792\n",
      "Epoch 525, Loss: 0.1514291251078248, Final Batch Loss: 0.010237005539238453\n",
      "Epoch 526, Loss: 0.13227700907737017, Final Batch Loss: 0.01078647281974554\n",
      "Epoch 527, Loss: 0.14332326594740152, Final Batch Loss: 0.013891642913222313\n",
      "Epoch 528, Loss: 0.1466535609215498, Final Batch Loss: 0.02367141842842102\n",
      "Epoch 529, Loss: 0.1520697120577097, Final Batch Loss: 0.03371105343103409\n",
      "Epoch 530, Loss: 0.13952909782528877, Final Batch Loss: 0.033499110490083694\n",
      "Epoch 531, Loss: 0.16926042549312115, Final Batch Loss: 0.03792991489171982\n",
      "Epoch 532, Loss: 0.13514534197747707, Final Batch Loss: 0.0377298928797245\n",
      "Epoch 533, Loss: 0.14312550239264965, Final Batch Loss: 0.01831257902085781\n",
      "Epoch 534, Loss: 0.14021717943251133, Final Batch Loss: 0.035600677132606506\n",
      "Epoch 535, Loss: 0.17112278565764427, Final Batch Loss: 0.03129136934876442\n",
      "Epoch 536, Loss: 0.1643689051270485, Final Batch Loss: 0.028745871037244797\n",
      "Epoch 537, Loss: 0.11659068800508976, Final Batch Loss: 0.027551637962460518\n",
      "Epoch 538, Loss: 0.16253050602972507, Final Batch Loss: 0.03610823675990105\n",
      "Epoch 539, Loss: 0.17783545423299074, Final Batch Loss: 0.00982391182333231\n",
      "Epoch 540, Loss: 0.1295902207493782, Final Batch Loss: 0.013196544721722603\n",
      "Epoch 541, Loss: 0.17484083585441113, Final Batch Loss: 0.07084538787603378\n",
      "Epoch 542, Loss: 0.12696549482643604, Final Batch Loss: 0.0149648142978549\n",
      "Epoch 543, Loss: 0.13063507713377476, Final Batch Loss: 0.025569796562194824\n",
      "Epoch 544, Loss: 0.1185715738683939, Final Batch Loss: 0.012456689029932022\n",
      "Epoch 545, Loss: 0.13563773967325687, Final Batch Loss: 0.007813503034412861\n",
      "Epoch 546, Loss: 0.14083168096840382, Final Batch Loss: 0.032561615109443665\n",
      "Epoch 547, Loss: 0.11558924056589603, Final Batch Loss: 0.03221675008535385\n",
      "Epoch 548, Loss: 0.14097713772207499, Final Batch Loss: 0.03699304908514023\n",
      "Epoch 549, Loss: 0.16470994800329208, Final Batch Loss: 0.074802465736866\n",
      "Epoch 550, Loss: 0.12239020690321922, Final Batch Loss: 0.031825706362724304\n",
      "Epoch 551, Loss: 0.1320644672960043, Final Batch Loss: 0.02032109722495079\n",
      "Epoch 552, Loss: 0.10641356185078621, Final Batch Loss: 0.01636500470340252\n",
      "Epoch 553, Loss: 0.1130368746817112, Final Batch Loss: 0.031302038580179214\n",
      "Epoch 554, Loss: 0.10789933986961842, Final Batch Loss: 0.015629397705197334\n",
      "Epoch 555, Loss: 0.1734680151566863, Final Batch Loss: 0.056218139827251434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 556, Loss: 0.13016772083938122, Final Batch Loss: 0.028841182589530945\n",
      "Epoch 557, Loss: 0.1494901478290558, Final Batch Loss: 0.011913623660802841\n",
      "Epoch 558, Loss: 0.10426677204668522, Final Batch Loss: 0.03708655759692192\n",
      "Epoch 559, Loss: 0.12774399854242802, Final Batch Loss: 0.020831838250160217\n",
      "Epoch 560, Loss: 0.1207524985074997, Final Batch Loss: 0.03212787210941315\n",
      "Epoch 561, Loss: 0.14473071787506342, Final Batch Loss: 0.014771397225558758\n",
      "Epoch 562, Loss: 0.10851742699742317, Final Batch Loss: 0.017472008243203163\n",
      "Epoch 563, Loss: 0.11236197780817747, Final Batch Loss: 0.01853722147643566\n",
      "Epoch 564, Loss: 0.1384896459057927, Final Batch Loss: 0.0685410425066948\n",
      "Epoch 565, Loss: 0.1201275628991425, Final Batch Loss: 0.06232574209570885\n",
      "Epoch 566, Loss: 0.12640618532896042, Final Batch Loss: 0.015268373303115368\n",
      "Epoch 567, Loss: 0.1357872635126114, Final Batch Loss: 0.02673421800136566\n",
      "Epoch 568, Loss: 0.1297029433771968, Final Batch Loss: 0.040990278124809265\n",
      "Epoch 569, Loss: 0.13933950290083885, Final Batch Loss: 0.018449876457452774\n",
      "Epoch 570, Loss: 0.10636665672063828, Final Batch Loss: 0.012126640416681767\n",
      "Epoch 571, Loss: 0.15278753265738487, Final Batch Loss: 0.027318358421325684\n",
      "Epoch 572, Loss: 0.10948330909013748, Final Batch Loss: 0.016154401004314423\n",
      "Epoch 573, Loss: 0.12152490764856339, Final Batch Loss: 0.018386442214250565\n",
      "Epoch 574, Loss: 0.10966773144900799, Final Batch Loss: 0.018081864342093468\n",
      "Epoch 575, Loss: 0.11345201125368476, Final Batch Loss: 0.03233563154935837\n",
      "Epoch 576, Loss: 0.1058765766210854, Final Batch Loss: 0.012763742357492447\n",
      "Epoch 577, Loss: 0.11189805064350367, Final Batch Loss: 0.011366705410182476\n",
      "Epoch 578, Loss: 0.0838446980342269, Final Batch Loss: 0.02582484669983387\n",
      "Epoch 579, Loss: 0.11582164000719786, Final Batch Loss: 0.016407327726483345\n",
      "Epoch 580, Loss: 0.11197168007493019, Final Batch Loss: 0.023759538307785988\n",
      "Epoch 581, Loss: 0.11280143354088068, Final Batch Loss: 0.025122182443737984\n",
      "Epoch 582, Loss: 0.12930136639624834, Final Batch Loss: 0.011715789325535297\n",
      "Epoch 583, Loss: 0.09468554332852364, Final Batch Loss: 0.021551305428147316\n",
      "Epoch 584, Loss: 0.10002509225159883, Final Batch Loss: 0.0277496799826622\n",
      "Epoch 585, Loss: 0.08399165794253349, Final Batch Loss: 0.013143053278326988\n",
      "Epoch 586, Loss: 0.09223937336355448, Final Batch Loss: 0.012691180221736431\n",
      "Epoch 587, Loss: 0.11388018680736423, Final Batch Loss: 0.048324745148420334\n",
      "Epoch 588, Loss: 0.13784563168883324, Final Batch Loss: 0.02349509857594967\n",
      "Epoch 589, Loss: 0.1347016436047852, Final Batch Loss: 0.004756385926157236\n",
      "Epoch 590, Loss: 0.09389943210408092, Final Batch Loss: 0.053880490362644196\n",
      "Epoch 591, Loss: 0.08540050406008959, Final Batch Loss: 0.008264333941042423\n",
      "Epoch 592, Loss: 0.07682210765779018, Final Batch Loss: 0.011527528055012226\n",
      "Epoch 593, Loss: 0.08602183498442173, Final Batch Loss: 0.021610047668218613\n",
      "Epoch 594, Loss: 0.08270192239433527, Final Batch Loss: 0.011603331193327904\n",
      "Epoch 595, Loss: 0.08688814472407103, Final Batch Loss: 0.019817087799310684\n",
      "Epoch 596, Loss: 0.06508691143244505, Final Batch Loss: 0.008254812099039555\n",
      "Epoch 597, Loss: 0.0873519629240036, Final Batch Loss: 0.0233142226934433\n",
      "Epoch 598, Loss: 0.11741057131439447, Final Batch Loss: 0.012720058672130108\n",
      "Epoch 599, Loss: 0.07781877089291811, Final Batch Loss: 0.009007611311972141\n",
      "Epoch 600, Loss: 0.11320830741897225, Final Batch Loss: 0.023878159001469612\n",
      "Epoch 601, Loss: 0.12974793557077646, Final Batch Loss: 0.026130007579922676\n",
      "Epoch 602, Loss: 0.10631762119010091, Final Batch Loss: 0.005392852239310741\n",
      "Epoch 603, Loss: 0.10779799427837133, Final Batch Loss: 0.015591205097734928\n",
      "Epoch 604, Loss: 0.0936763109639287, Final Batch Loss: 0.033504825085401535\n",
      "Epoch 605, Loss: 0.12884333077818155, Final Batch Loss: 0.012578072026371956\n",
      "Epoch 606, Loss: 0.09832739178091288, Final Batch Loss: 0.01597738452255726\n",
      "Epoch 607, Loss: 0.07718754839152098, Final Batch Loss: 0.0084745604544878\n",
      "Epoch 608, Loss: 0.13574842736124992, Final Batch Loss: 0.05357510969042778\n",
      "Epoch 609, Loss: 0.09846425615251064, Final Batch Loss: 0.019987182691693306\n",
      "Epoch 610, Loss: 0.1308764759451151, Final Batch Loss: 0.011022600345313549\n",
      "Epoch 611, Loss: 0.1601591152139008, Final Batch Loss: 0.0031087822280824184\n",
      "Epoch 612, Loss: 0.08458069572225213, Final Batch Loss: 0.01719318889081478\n",
      "Epoch 613, Loss: 0.1370239187963307, Final Batch Loss: 0.007368399295955896\n",
      "Epoch 614, Loss: 0.0891729649156332, Final Batch Loss: 0.007759773172438145\n",
      "Epoch 615, Loss: 0.0843142089433968, Final Batch Loss: 0.016220860183238983\n",
      "Epoch 616, Loss: 0.08421789947897196, Final Batch Loss: 0.012632891535758972\n",
      "Epoch 617, Loss: 0.07474481407552958, Final Batch Loss: 0.012481529265642166\n",
      "Epoch 618, Loss: 0.08156292838975787, Final Batch Loss: 0.04351631551980972\n",
      "Epoch 619, Loss: 0.08064580336213112, Final Batch Loss: 0.01698143407702446\n",
      "Epoch 620, Loss: 0.07873555226251483, Final Batch Loss: 0.00783775094896555\n",
      "Epoch 621, Loss: 0.1243599196895957, Final Batch Loss: 0.00982433371245861\n",
      "Epoch 622, Loss: 0.12815937586128712, Final Batch Loss: 0.025406058877706528\n",
      "Epoch 623, Loss: 0.09669048432260752, Final Batch Loss: 0.011309380643069744\n",
      "Epoch 624, Loss: 0.10794748179614544, Final Batch Loss: 0.04521454870700836\n",
      "Epoch 625, Loss: 0.155307000502944, Final Batch Loss: 0.023471252992749214\n",
      "Epoch 626, Loss: 0.10713530983775854, Final Batch Loss: 0.008977261371910572\n",
      "Epoch 627, Loss: 0.10647066030651331, Final Batch Loss: 0.018012773245573044\n",
      "Epoch 628, Loss: 0.09725742880254984, Final Batch Loss: 0.04096592962741852\n",
      "Epoch 629, Loss: 0.10808391915634274, Final Batch Loss: 0.005298266652971506\n",
      "Epoch 630, Loss: 0.09992603864520788, Final Batch Loss: 0.03544096276164055\n",
      "Epoch 631, Loss: 0.12840625923126936, Final Batch Loss: 0.024789346382021904\n",
      "Epoch 632, Loss: 0.10063222236931324, Final Batch Loss: 0.016889536753296852\n",
      "Epoch 633, Loss: 0.1415043268352747, Final Batch Loss: 0.00864042341709137\n",
      "Epoch 634, Loss: 0.08376705925911665, Final Batch Loss: 0.007669778540730476\n",
      "Epoch 635, Loss: 0.08302194345742464, Final Batch Loss: 0.011047743260860443\n",
      "Epoch 636, Loss: 0.12131518684327602, Final Batch Loss: 0.016724832355976105\n",
      "Epoch 637, Loss: 0.09962311200797558, Final Batch Loss: 0.03195000812411308\n",
      "Epoch 638, Loss: 0.09214640315622091, Final Batch Loss: 0.027599988505244255\n",
      "Epoch 639, Loss: 0.07853638008236885, Final Batch Loss: 0.005808927118778229\n",
      "Epoch 640, Loss: 0.09915624745190144, Final Batch Loss: 0.014091264456510544\n",
      "Epoch 641, Loss: 0.08531556092202663, Final Batch Loss: 0.00571443373337388\n",
      "Epoch 642, Loss: 0.10253444314002991, Final Batch Loss: 0.00875065941363573\n",
      "Epoch 643, Loss: 0.07984600914642215, Final Batch Loss: 0.02399471402168274\n",
      "Epoch 644, Loss: 0.059818176086992025, Final Batch Loss: 0.0010905121453106403\n",
      "Epoch 645, Loss: 0.09013079525902867, Final Batch Loss: 0.020746760070323944\n",
      "Epoch 646, Loss: 0.09921304788440466, Final Batch Loss: 0.0238984152674675\n",
      "Epoch 647, Loss: 0.10999628249555826, Final Batch Loss: 0.008194323629140854\n",
      "Epoch 648, Loss: 0.07477508392184973, Final Batch Loss: 0.012740819714963436\n",
      "Epoch 649, Loss: 0.07214097771793604, Final Batch Loss: 0.004524832591414452\n",
      "Epoch 650, Loss: 0.1042188466526568, Final Batch Loss: 0.02505658194422722\n",
      "Epoch 651, Loss: 0.15199807204771787, Final Batch Loss: 0.05884402617812157\n",
      "Epoch 652, Loss: 0.10439846618101001, Final Batch Loss: 0.022583486512303352\n",
      "Epoch 653, Loss: 0.10194646939635277, Final Batch Loss: 0.03544450178742409\n",
      "Epoch 654, Loss: 0.12273477390408516, Final Batch Loss: 0.020448407158255577\n",
      "Epoch 655, Loss: 0.10151237435638905, Final Batch Loss: 0.005504339002072811\n",
      "Epoch 656, Loss: 0.0633263224735856, Final Batch Loss: 0.009643805213272572\n",
      "Epoch 657, Loss: 0.1171182906255126, Final Batch Loss: 0.06455186754465103\n",
      "Epoch 658, Loss: 0.11148845683783293, Final Batch Loss: 0.012476669624447823\n",
      "Epoch 659, Loss: 0.07567537762224674, Final Batch Loss: 0.018225548788905144\n",
      "Epoch 660, Loss: 0.09367362316697836, Final Batch Loss: 0.022169077768921852\n",
      "Epoch 661, Loss: 0.0758539461530745, Final Batch Loss: 0.007526570465415716\n",
      "Epoch 662, Loss: 0.0607339998241514, Final Batch Loss: 0.002911262447014451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 663, Loss: 0.08867102488875389, Final Batch Loss: 0.0265145655721426\n",
      "Epoch 664, Loss: 0.058209977112710476, Final Batch Loss: 0.02217349410057068\n",
      "Epoch 665, Loss: 0.1295573841780424, Final Batch Loss: 0.018239302560687065\n",
      "Epoch 666, Loss: 0.08651100611314178, Final Batch Loss: 0.005329425912350416\n",
      "Epoch 667, Loss: 0.07772317714989185, Final Batch Loss: 0.018872397020459175\n",
      "Epoch 668, Loss: 0.11617976613342762, Final Batch Loss: 0.039280783385038376\n",
      "Epoch 669, Loss: 0.10956285893917084, Final Batch Loss: 0.025655202567577362\n",
      "Epoch 670, Loss: 0.0964540084823966, Final Batch Loss: 0.013364827260375023\n",
      "Epoch 671, Loss: 0.08649999555200338, Final Batch Loss: 0.01487634889781475\n",
      "Epoch 672, Loss: 0.07849898701533675, Final Batch Loss: 0.014310258440673351\n",
      "Epoch 673, Loss: 0.12062300741672516, Final Batch Loss: 0.03415222465991974\n",
      "Epoch 674, Loss: 0.07299437839537859, Final Batch Loss: 0.012280438095331192\n",
      "Epoch 675, Loss: 0.0838007964193821, Final Batch Loss: 0.02370774745941162\n",
      "Epoch 676, Loss: 0.07968909665942192, Final Batch Loss: 0.041887521743774414\n",
      "Epoch 677, Loss: 0.09218070097267628, Final Batch Loss: 0.03752351924777031\n",
      "Epoch 678, Loss: 0.09611590160056949, Final Batch Loss: 0.037411026656627655\n",
      "Epoch 679, Loss: 0.09098593797534704, Final Batch Loss: 0.0073491549119353294\n",
      "Epoch 680, Loss: 0.05673403898254037, Final Batch Loss: 0.012715022079646587\n",
      "Epoch 681, Loss: 0.09215045720338821, Final Batch Loss: 0.012616419233381748\n",
      "Epoch 682, Loss: 0.11757753510028124, Final Batch Loss: 0.013731231912970543\n",
      "Epoch 683, Loss: 0.07014308730140328, Final Batch Loss: 0.03184424713253975\n",
      "Epoch 684, Loss: 0.07674384210258722, Final Batch Loss: 0.016150692477822304\n",
      "Epoch 685, Loss: 0.08716545719653368, Final Batch Loss: 0.023899665102362633\n",
      "Epoch 686, Loss: 0.08611154370009899, Final Batch Loss: 0.010067319497466087\n",
      "Epoch 687, Loss: 0.08440340170636773, Final Batch Loss: 0.025425584986805916\n",
      "Epoch 688, Loss: 0.09068137733265758, Final Batch Loss: 0.021371090784668922\n",
      "Epoch 689, Loss: 0.10036432836204767, Final Batch Loss: 0.035967424511909485\n",
      "Epoch 690, Loss: 0.07000129902735353, Final Batch Loss: 0.004728660453110933\n",
      "Epoch 691, Loss: 0.1543223150074482, Final Batch Loss: 0.04511459916830063\n",
      "Epoch 692, Loss: 0.07667317241430283, Final Batch Loss: 0.022097602486610413\n",
      "Epoch 693, Loss: 0.10852316487580538, Final Batch Loss: 0.018446754664182663\n",
      "Epoch 694, Loss: 0.08993218280375004, Final Batch Loss: 0.02666063979268074\n",
      "Epoch 695, Loss: 0.08863446535542607, Final Batch Loss: 0.007770901080220938\n",
      "Epoch 696, Loss: 0.07172025460749865, Final Batch Loss: 0.013498559594154358\n",
      "Epoch 697, Loss: 0.07296131225302815, Final Batch Loss: 0.019103823229670525\n",
      "Epoch 698, Loss: 0.10384751064702868, Final Batch Loss: 0.04398377984762192\n",
      "Epoch 699, Loss: 0.05159179586917162, Final Batch Loss: 0.008943491615355015\n",
      "Epoch 700, Loss: 0.08578954730182886, Final Batch Loss: 0.010580199770629406\n",
      "Epoch 701, Loss: 0.0718325653579086, Final Batch Loss: 0.002852426143363118\n",
      "Epoch 702, Loss: 0.1031443877145648, Final Batch Loss: 0.04064486175775528\n",
      "Epoch 703, Loss: 0.06429835129529238, Final Batch Loss: 0.010435153730213642\n",
      "Epoch 704, Loss: 0.14159655198454857, Final Batch Loss: 0.004472177475690842\n",
      "Epoch 705, Loss: 0.07138804998248816, Final Batch Loss: 0.012915545143187046\n",
      "Epoch 706, Loss: 0.056431491393595934, Final Batch Loss: 0.014406610280275345\n",
      "Epoch 707, Loss: 0.07860881835222244, Final Batch Loss: 0.01241827942430973\n",
      "Epoch 708, Loss: 0.09121387591585517, Final Batch Loss: 0.005667755845934153\n",
      "Epoch 709, Loss: 0.07407184666953981, Final Batch Loss: 0.003750757547095418\n",
      "Epoch 710, Loss: 0.07951781246811152, Final Batch Loss: 0.030650030821561813\n",
      "Epoch 711, Loss: 0.07630877010524273, Final Batch Loss: 0.005770236253738403\n",
      "Epoch 712, Loss: 0.08846995467320085, Final Batch Loss: 0.0362265482544899\n",
      "Epoch 713, Loss: 0.0765839945524931, Final Batch Loss: 0.0051097748801112175\n",
      "Epoch 714, Loss: 0.09018051438033581, Final Batch Loss: 0.024521008133888245\n",
      "Epoch 715, Loss: 0.052898467518389225, Final Batch Loss: 0.004631879273802042\n",
      "Epoch 716, Loss: 0.07371210446581244, Final Batch Loss: 0.003513114992529154\n",
      "Epoch 717, Loss: 0.06266375817358494, Final Batch Loss: 0.0033640731126070023\n",
      "Epoch 718, Loss: 0.08426102250814438, Final Batch Loss: 0.014787156134843826\n",
      "Epoch 719, Loss: 0.06130285910330713, Final Batch Loss: 0.0133893471211195\n",
      "Epoch 720, Loss: 0.07714761700481176, Final Batch Loss: 0.02052711322903633\n",
      "Epoch 721, Loss: 0.10144778713583946, Final Batch Loss: 0.03446777164936066\n",
      "Epoch 722, Loss: 0.04819689365103841, Final Batch Loss: 0.006088983733206987\n",
      "Epoch 723, Loss: 0.09039804805070162, Final Batch Loss: 0.004618730861693621\n",
      "Epoch 724, Loss: 0.06675544090103358, Final Batch Loss: 0.001782449777238071\n",
      "Epoch 725, Loss: 0.07901993440464139, Final Batch Loss: 0.00917857512831688\n",
      "Epoch 726, Loss: 0.10793440230190754, Final Batch Loss: 0.05485028773546219\n",
      "Epoch 727, Loss: 0.0827829132322222, Final Batch Loss: 0.020462829619646072\n",
      "Epoch 728, Loss: 0.04969744710251689, Final Batch Loss: 0.005987443961203098\n",
      "Epoch 729, Loss: 0.08942218963056803, Final Batch Loss: 0.013013148680329323\n",
      "Epoch 730, Loss: 0.06677964655682445, Final Batch Loss: 0.009201744571328163\n",
      "Epoch 731, Loss: 0.08634308027103543, Final Batch Loss: 0.023629741743206978\n",
      "Epoch 732, Loss: 0.06430259393528104, Final Batch Loss: 0.015649467706680298\n",
      "Epoch 733, Loss: 0.08417505491524935, Final Batch Loss: 0.035291511565446854\n",
      "Epoch 734, Loss: 0.06544425943866372, Final Batch Loss: 0.008070169948041439\n",
      "Epoch 735, Loss: 0.07939184550195932, Final Batch Loss: 0.011243543587625027\n",
      "Epoch 736, Loss: 0.1170219061896205, Final Batch Loss: 0.04841125011444092\n",
      "Epoch 737, Loss: 0.06276913918554783, Final Batch Loss: 0.02041621133685112\n",
      "Epoch 738, Loss: 0.08605171367526054, Final Batch Loss: 0.015208035707473755\n",
      "Epoch 739, Loss: 0.04516013199463487, Final Batch Loss: 0.002520865760743618\n",
      "Epoch 740, Loss: 0.060444111470133066, Final Batch Loss: 0.01042275968939066\n",
      "Epoch 741, Loss: 0.09918881207704544, Final Batch Loss: 0.009505403228104115\n",
      "Epoch 742, Loss: 0.06117856106720865, Final Batch Loss: 0.0038509320002049208\n",
      "Epoch 743, Loss: 0.06780384806916118, Final Batch Loss: 0.028183581307530403\n",
      "Epoch 744, Loss: 0.07636148482561111, Final Batch Loss: 0.025116201490163803\n",
      "Epoch 745, Loss: 0.12367767374962568, Final Batch Loss: 0.009112067520618439\n",
      "Epoch 746, Loss: 0.06665616715326905, Final Batch Loss: 0.019929178059101105\n",
      "Epoch 747, Loss: 0.08990825177170336, Final Batch Loss: 0.03487532213330269\n",
      "Epoch 748, Loss: 0.06235726410523057, Final Batch Loss: 0.0029843575321137905\n",
      "Epoch 749, Loss: 0.06869892589747906, Final Batch Loss: 0.01588572934269905\n",
      "Epoch 750, Loss: 0.04873034916818142, Final Batch Loss: 0.002233701292425394\n",
      "Epoch 751, Loss: 0.06294956407509744, Final Batch Loss: 0.025925287976861\n",
      "Epoch 752, Loss: 0.0395732163451612, Final Batch Loss: 0.008481336757540703\n",
      "Epoch 753, Loss: 0.060219862731173635, Final Batch Loss: 0.0014987026806920767\n",
      "Epoch 754, Loss: 0.08249532780610025, Final Batch Loss: 0.04085993021726608\n",
      "Epoch 755, Loss: 0.061453410889953375, Final Batch Loss: 0.005227386485785246\n",
      "Epoch 756, Loss: 0.07306038215756416, Final Batch Loss: 0.018573472276329994\n",
      "Epoch 757, Loss: 0.09970708331093192, Final Batch Loss: 0.020935440436005592\n",
      "Epoch 758, Loss: 0.07335650315508246, Final Batch Loss: 0.004114856477826834\n",
      "Epoch 759, Loss: 0.11065382277593017, Final Batch Loss: 0.0498514361679554\n",
      "Epoch 760, Loss: 0.06511357799172401, Final Batch Loss: 0.012387160211801529\n",
      "Epoch 761, Loss: 0.07239473890513182, Final Batch Loss: 0.03518231585621834\n",
      "Epoch 762, Loss: 0.11056556552648544, Final Batch Loss: 0.005977170541882515\n",
      "Epoch 763, Loss: 0.07687056995928288, Final Batch Loss: 0.023619282990694046\n",
      "Epoch 764, Loss: 0.08020411850884557, Final Batch Loss: 0.024984518066048622\n",
      "Epoch 765, Loss: 0.11779617611318827, Final Batch Loss: 0.01479320228099823\n",
      "Epoch 766, Loss: 0.07955008465796709, Final Batch Loss: 0.005798354744911194\n",
      "Epoch 767, Loss: 0.07249718671664596, Final Batch Loss: 0.018573997542262077\n",
      "Epoch 768, Loss: 0.09161620959639549, Final Batch Loss: 0.007762367371469736\n",
      "Epoch 769, Loss: 0.07875894429162145, Final Batch Loss: 0.009758357889950275\n",
      "Epoch 770, Loss: 0.07465998106636107, Final Batch Loss: 0.002986112842336297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771, Loss: 0.1194775914773345, Final Batch Loss: 0.010622973553836346\n",
      "Epoch 772, Loss: 0.05435763578861952, Final Batch Loss: 0.008656074292957783\n",
      "Epoch 773, Loss: 0.05820632912218571, Final Batch Loss: 0.004057751502841711\n",
      "Epoch 774, Loss: 0.10005558654665947, Final Batch Loss: 0.013961005955934525\n",
      "Epoch 775, Loss: 0.07054849434643984, Final Batch Loss: 0.005486601032316685\n",
      "Epoch 776, Loss: 0.08245542948134243, Final Batch Loss: 0.0030327935237437487\n",
      "Epoch 777, Loss: 0.10834691952914, Final Batch Loss: 0.023087169975042343\n",
      "Epoch 778, Loss: 0.10960786417126656, Final Batch Loss: 0.028708694502711296\n",
      "Epoch 779, Loss: 0.0654680363368243, Final Batch Loss: 0.010240414179861546\n",
      "Epoch 780, Loss: 0.15877362480387092, Final Batch Loss: 0.0073737516067922115\n",
      "Epoch 781, Loss: 0.0770157827064395, Final Batch Loss: 0.024471206590533257\n",
      "Epoch 782, Loss: 0.06532294163480401, Final Batch Loss: 0.00865151546895504\n",
      "Epoch 783, Loss: 0.09427129477262497, Final Batch Loss: 0.012899774126708508\n",
      "Epoch 784, Loss: 0.06917510950006545, Final Batch Loss: 0.010364808142185211\n",
      "Epoch 785, Loss: 0.06879684748128057, Final Batch Loss: 0.010191843844950199\n",
      "Epoch 786, Loss: 0.0874128108844161, Final Batch Loss: 0.02023931033909321\n",
      "Epoch 787, Loss: 0.08375857025384903, Final Batch Loss: 0.0037734615616500378\n",
      "Epoch 788, Loss: 0.07496155938133597, Final Batch Loss: 0.013307672925293446\n",
      "Epoch 789, Loss: 0.049256673315539956, Final Batch Loss: 0.004236637614667416\n",
      "Epoch 790, Loss: 0.0716840650420636, Final Batch Loss: 0.024202333763241768\n",
      "Epoch 791, Loss: 0.06204824382439256, Final Batch Loss: 0.03186951205134392\n",
      "Epoch 792, Loss: 0.09869120013900101, Final Batch Loss: 0.030618809163570404\n",
      "Epoch 793, Loss: 0.06129443016834557, Final Batch Loss: 0.012126236222684383\n",
      "Epoch 794, Loss: 0.04659698833711445, Final Batch Loss: 0.002428939798846841\n",
      "Epoch 795, Loss: 0.0994906984269619, Final Batch Loss: 0.02724614366889\n",
      "Epoch 796, Loss: 0.055143436416983604, Final Batch Loss: 0.00895653571933508\n",
      "Epoch 797, Loss: 0.1083780787885189, Final Batch Loss: 0.04758995771408081\n",
      "Epoch 798, Loss: 0.05112874647602439, Final Batch Loss: 0.009013195522129536\n",
      "Epoch 799, Loss: 0.06682370649650693, Final Batch Loss: 0.004284888971596956\n",
      "Epoch 800, Loss: 0.05858249869197607, Final Batch Loss: 0.016405833885073662\n",
      "Epoch 801, Loss: 0.05925306328572333, Final Batch Loss: 0.003058667527511716\n",
      "Epoch 802, Loss: 0.06253664556425065, Final Batch Loss: 0.025827817618846893\n",
      "Epoch 803, Loss: 0.0675565148703754, Final Batch Loss: 0.010998318903148174\n",
      "Epoch 804, Loss: 0.09860302787274122, Final Batch Loss: 0.045727748423814774\n",
      "Epoch 805, Loss: 0.06860600411891937, Final Batch Loss: 0.006630648858845234\n",
      "Epoch 806, Loss: 0.10374914575368166, Final Batch Loss: 0.04789325222373009\n",
      "Epoch 807, Loss: 0.04796122177504003, Final Batch Loss: 0.002522122347727418\n",
      "Epoch 808, Loss: 0.11830269638448954, Final Batch Loss: 0.04648628085851669\n",
      "Epoch 809, Loss: 0.047508895047940314, Final Batch Loss: 0.0017772751161828637\n",
      "Epoch 810, Loss: 0.12655991595238447, Final Batch Loss: 0.015946408733725548\n",
      "Epoch 811, Loss: 0.07006110367365181, Final Batch Loss: 0.007064441218972206\n",
      "Epoch 812, Loss: 0.08235826576128602, Final Batch Loss: 0.017779991030693054\n",
      "Epoch 813, Loss: 0.05470064887776971, Final Batch Loss: 0.016333933919668198\n",
      "Epoch 814, Loss: 0.08813958708196878, Final Batch Loss: 0.021521268412470818\n",
      "Epoch 815, Loss: 0.07778469799086452, Final Batch Loss: 0.008966853842139244\n",
      "Epoch 816, Loss: 0.10539881186559796, Final Batch Loss: 0.0231026578694582\n",
      "Epoch 817, Loss: 0.07869473961181939, Final Batch Loss: 0.01632562093436718\n",
      "Epoch 818, Loss: 0.058731177472509444, Final Batch Loss: 0.001166155911050737\n",
      "Epoch 819, Loss: 0.06347128609195352, Final Batch Loss: 0.010673836804926395\n",
      "Epoch 820, Loss: 0.09173799678683281, Final Batch Loss: 0.02646578848361969\n",
      "Epoch 821, Loss: 0.04033771390095353, Final Batch Loss: 0.00251995911821723\n",
      "Epoch 822, Loss: 0.062241537030786276, Final Batch Loss: 0.02306937985122204\n",
      "Epoch 823, Loss: 0.05683182715438306, Final Batch Loss: 0.01647697575390339\n",
      "Epoch 824, Loss: 0.06527250097133219, Final Batch Loss: 0.0062070973217487335\n",
      "Epoch 825, Loss: 0.08552556950598955, Final Batch Loss: 0.016437239944934845\n",
      "Epoch 826, Loss: 0.05704294424504042, Final Batch Loss: 0.004791676998138428\n",
      "Epoch 827, Loss: 0.07030268269591033, Final Batch Loss: 0.0038060599472373724\n",
      "Epoch 828, Loss: 0.0735459104180336, Final Batch Loss: 0.01702767051756382\n",
      "Epoch 829, Loss: 0.07601771969348192, Final Batch Loss: 0.0315057672560215\n",
      "Epoch 830, Loss: 0.0859685237519443, Final Batch Loss: 0.007036814000457525\n",
      "Epoch 831, Loss: 0.05756240733899176, Final Batch Loss: 0.0027580077294260263\n",
      "Epoch 832, Loss: 0.05880954675376415, Final Batch Loss: 0.004208475351333618\n",
      "Epoch 833, Loss: 0.09016988635994494, Final Batch Loss: 0.0038222374860197306\n",
      "Epoch 834, Loss: 0.06047022761777043, Final Batch Loss: 0.015913810580968857\n",
      "Epoch 835, Loss: 0.05856303358450532, Final Batch Loss: 0.012396339327096939\n",
      "Epoch 836, Loss: 0.07817553833592683, Final Batch Loss: 0.001737547921948135\n",
      "Epoch 837, Loss: 0.06272142776288092, Final Batch Loss: 0.0027125917840749025\n",
      "Epoch 838, Loss: 0.08261798415333033, Final Batch Loss: 0.007577105425298214\n",
      "Epoch 839, Loss: 0.07426834106445312, Final Batch Loss: 0.010982370004057884\n",
      "Epoch 840, Loss: 0.07880240492522717, Final Batch Loss: 0.029659563675522804\n",
      "Epoch 841, Loss: 0.05368818831630051, Final Batch Loss: 0.0028512736316770315\n",
      "Epoch 842, Loss: 0.06879500683862716, Final Batch Loss: 0.028222132474184036\n",
      "Epoch 843, Loss: 0.07318244804628193, Final Batch Loss: 0.03331555798649788\n",
      "Epoch 844, Loss: 0.05974604282528162, Final Batch Loss: 0.013848899863660336\n",
      "Epoch 845, Loss: 0.05589827802032232, Final Batch Loss: 0.005845374893397093\n",
      "Epoch 846, Loss: 0.05362602195236832, Final Batch Loss: 0.013476361520588398\n",
      "Epoch 847, Loss: 0.05666959239169955, Final Batch Loss: 0.0028005267959088087\n",
      "Epoch 848, Loss: 0.07815474923700094, Final Batch Loss: 0.011492298915982246\n",
      "Epoch 849, Loss: 0.09393107937648892, Final Batch Loss: 0.0237995944917202\n",
      "Epoch 850, Loss: 0.06572688277810812, Final Batch Loss: 0.017327547073364258\n",
      "Epoch 851, Loss: 0.05326514830812812, Final Batch Loss: 0.005993889179080725\n",
      "Epoch 852, Loss: 0.0607063767965883, Final Batch Loss: 0.003481894498690963\n",
      "Epoch 853, Loss: 0.06439963867887855, Final Batch Loss: 0.005435403902083635\n",
      "Epoch 854, Loss: 0.07145823864266276, Final Batch Loss: 0.006033068988472223\n",
      "Epoch 855, Loss: 0.05239102989435196, Final Batch Loss: 0.003002115059643984\n",
      "Epoch 856, Loss: 0.056982255540788174, Final Batch Loss: 0.010110992938280106\n",
      "Epoch 857, Loss: 0.07816768204793334, Final Batch Loss: 0.02747246064245701\n",
      "Epoch 858, Loss: 0.05044308118522167, Final Batch Loss: 0.009151100181043148\n",
      "Epoch 859, Loss: 0.07309581991285086, Final Batch Loss: 0.03923891484737396\n",
      "Epoch 860, Loss: 0.06736049754545093, Final Batch Loss: 0.01600637100636959\n",
      "Epoch 861, Loss: 0.0612427587620914, Final Batch Loss: 0.010490894317626953\n",
      "Epoch 862, Loss: 0.05640045693144202, Final Batch Loss: 0.0036599943414330482\n",
      "Epoch 863, Loss: 0.05404739489313215, Final Batch Loss: 0.004775950685143471\n",
      "Epoch 864, Loss: 0.06940351030789316, Final Batch Loss: 0.0014623117167502642\n",
      "Epoch 865, Loss: 0.07545000780373812, Final Batch Loss: 0.018330516293644905\n",
      "Epoch 866, Loss: 0.08330138679593801, Final Batch Loss: 0.013548293150961399\n",
      "Epoch 867, Loss: 0.07397938892245293, Final Batch Loss: 0.008580444380640984\n",
      "Epoch 868, Loss: 0.05990059534087777, Final Batch Loss: 0.01572742871940136\n",
      "Epoch 869, Loss: 0.09324235934764147, Final Batch Loss: 0.036040693521499634\n",
      "Epoch 870, Loss: 0.07891050726175308, Final Batch Loss: 0.008732887916266918\n",
      "Epoch 871, Loss: 0.05802415357902646, Final Batch Loss: 0.009334873408079147\n",
      "Epoch 872, Loss: 0.05777717940509319, Final Batch Loss: 0.00710727833211422\n",
      "Epoch 873, Loss: 0.05164913716726005, Final Batch Loss: 0.008999299257993698\n",
      "Epoch 874, Loss: 0.05066827533300966, Final Batch Loss: 0.003631249302998185\n",
      "Epoch 875, Loss: 0.057657632045447826, Final Batch Loss: 0.020458167418837547\n",
      "Epoch 876, Loss: 0.0694342884235084, Final Batch Loss: 0.016279416158795357\n",
      "Epoch 877, Loss: 0.14695214107632637, Final Batch Loss: 0.019455939531326294\n",
      "Epoch 878, Loss: 0.06854109838604927, Final Batch Loss: 0.0156498234719038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 879, Loss: 0.06670967070385814, Final Batch Loss: 0.005423063412308693\n",
      "Epoch 880, Loss: 0.07833645987557247, Final Batch Loss: 0.0009350411710329354\n",
      "Epoch 881, Loss: 0.05858092405833304, Final Batch Loss: 0.01866256073117256\n",
      "Epoch 882, Loss: 0.0495322966016829, Final Batch Loss: 0.014904120936989784\n",
      "Epoch 883, Loss: 0.08617424394469708, Final Batch Loss: 0.0012298783985897899\n",
      "Epoch 884, Loss: 0.06242788629606366, Final Batch Loss: 0.009016660042107105\n",
      "Epoch 885, Loss: 0.06753427907824516, Final Batch Loss: 0.014445734210312366\n",
      "Epoch 886, Loss: 0.04745656927116215, Final Batch Loss: 0.011556676588952541\n",
      "Epoch 887, Loss: 0.06436109775677323, Final Batch Loss: 0.0049457005225121975\n",
      "Epoch 888, Loss: 0.05709937051869929, Final Batch Loss: 0.007143134251236916\n",
      "Epoch 889, Loss: 0.09546259371563792, Final Batch Loss: 0.017528392374515533\n",
      "Epoch 890, Loss: 0.04891728004440665, Final Batch Loss: 0.0015380752738565207\n",
      "Epoch 891, Loss: 0.044171258923597634, Final Batch Loss: 0.0014431033050641418\n",
      "Epoch 892, Loss: 0.06880192225798965, Final Batch Loss: 0.011412997730076313\n",
      "Epoch 893, Loss: 0.07684017252177, Final Batch Loss: 0.03347007557749748\n",
      "Epoch 894, Loss: 0.10932515899185091, Final Batch Loss: 0.01269109733402729\n",
      "Epoch 895, Loss: 0.12756788718979806, Final Batch Loss: 0.0012015531538054347\n",
      "Epoch 896, Loss: 0.06987576698884368, Final Batch Loss: 0.01665751449763775\n",
      "Epoch 897, Loss: 0.05917305778712034, Final Batch Loss: 0.0018600327894091606\n",
      "Epoch 898, Loss: 0.05546944634988904, Final Batch Loss: 0.014690788462758064\n",
      "Epoch 899, Loss: 0.06365398643538356, Final Batch Loss: 0.001171494135633111\n",
      "Epoch 900, Loss: 0.0715672594960779, Final Batch Loss: 0.019508499652147293\n",
      "Epoch 901, Loss: 0.05499537754803896, Final Batch Loss: 0.007420768495649099\n",
      "Epoch 902, Loss: 0.07324125757440925, Final Batch Loss: 0.004274414386600256\n",
      "Epoch 903, Loss: 0.06253826385363936, Final Batch Loss: 0.004993978422135115\n",
      "Epoch 904, Loss: 0.07145901583135128, Final Batch Loss: 0.002034030854701996\n",
      "Epoch 905, Loss: 0.0539063181495294, Final Batch Loss: 0.008287040516734123\n",
      "Epoch 906, Loss: 0.062454273691400886, Final Batch Loss: 0.012393035925924778\n",
      "Epoch 907, Loss: 0.06853688322007656, Final Batch Loss: 0.01732577756047249\n",
      "Epoch 908, Loss: 0.05905442778021097, Final Batch Loss: 0.009447986260056496\n",
      "Epoch 909, Loss: 0.07899850560352206, Final Batch Loss: 0.043820787221193314\n",
      "Epoch 910, Loss: 0.06924954755231738, Final Batch Loss: 0.0097349863499403\n",
      "Epoch 911, Loss: 0.0877487463876605, Final Batch Loss: 0.008681428618729115\n",
      "Epoch 912, Loss: 0.051931394496932626, Final Batch Loss: 0.0029467528220266104\n",
      "Epoch 913, Loss: 0.07286155736073852, Final Batch Loss: 0.00976371206343174\n",
      "Epoch 914, Loss: 0.057978213764727116, Final Batch Loss: 0.008864589035511017\n",
      "Epoch 915, Loss: 0.04204118112102151, Final Batch Loss: 0.006322565022855997\n",
      "Epoch 916, Loss: 0.0602267284411937, Final Batch Loss: 0.0032674178946763277\n",
      "Epoch 917, Loss: 0.08696163538843393, Final Batch Loss: 0.023452643305063248\n",
      "Epoch 918, Loss: 0.043836178723722696, Final Batch Loss: 0.007603026926517487\n",
      "Epoch 919, Loss: 0.046636815182864666, Final Batch Loss: 0.010541480034589767\n",
      "Epoch 920, Loss: 0.03772348118945956, Final Batch Loss: 0.018415674567222595\n",
      "Epoch 921, Loss: 0.05004995409399271, Final Batch Loss: 0.0025071504060178995\n",
      "Epoch 922, Loss: 0.0452887867577374, Final Batch Loss: 0.00978051032871008\n",
      "Epoch 923, Loss: 0.04977347981184721, Final Batch Loss: 0.0014211004599928856\n",
      "Epoch 924, Loss: 0.05352373648202047, Final Batch Loss: 0.0008671478717587888\n",
      "Epoch 925, Loss: 0.11661715799709782, Final Batch Loss: 0.0007200809777714312\n",
      "Epoch 926, Loss: 0.04531410802155733, Final Batch Loss: 0.005925359204411507\n",
      "Epoch 927, Loss: 0.12283431831747293, Final Batch Loss: 0.013940143398940563\n",
      "Epoch 928, Loss: 0.06057510199025273, Final Batch Loss: 0.005088341888040304\n",
      "Epoch 929, Loss: 0.07280343305319548, Final Batch Loss: 0.02646278776228428\n",
      "Epoch 930, Loss: 0.058616476599127054, Final Batch Loss: 0.014522848650813103\n",
      "Epoch 931, Loss: 0.0756174074485898, Final Batch Loss: 0.015822896733880043\n",
      "Epoch 932, Loss: 0.09439406148158014, Final Batch Loss: 0.030175840482115746\n",
      "Epoch 933, Loss: 0.052075850777328014, Final Batch Loss: 0.01081416942179203\n",
      "Epoch 934, Loss: 0.07237814133986831, Final Batch Loss: 0.008298791013658047\n",
      "Epoch 935, Loss: 0.07516670972108841, Final Batch Loss: 0.019645756110548973\n",
      "Epoch 936, Loss: 0.08400485082529485, Final Batch Loss: 0.003007652470842004\n",
      "Epoch 937, Loss: 0.07002000464126468, Final Batch Loss: 0.007696039974689484\n",
      "Epoch 938, Loss: 0.08791384845972061, Final Batch Loss: 0.03773525357246399\n",
      "Epoch 939, Loss: 0.06323124864138663, Final Batch Loss: 0.023468684405088425\n",
      "Epoch 940, Loss: 0.058143127243965864, Final Batch Loss: 0.01763756200671196\n",
      "Epoch 941, Loss: 0.040536966640502214, Final Batch Loss: 0.004566347226500511\n",
      "Epoch 942, Loss: 0.0709982980042696, Final Batch Loss: 0.03198838606476784\n",
      "Epoch 943, Loss: 0.05602706060744822, Final Batch Loss: 0.0034947579260915518\n",
      "Epoch 944, Loss: 0.06298004230484366, Final Batch Loss: 0.01433029305189848\n",
      "Epoch 945, Loss: 0.05502967559732497, Final Batch Loss: 0.0023603739682585\n",
      "Epoch 946, Loss: 0.06887824775185436, Final Batch Loss: 0.01294277049601078\n",
      "Epoch 947, Loss: 0.06219586357474327, Final Batch Loss: 0.008685342967510223\n",
      "Epoch 948, Loss: 0.07627320569008589, Final Batch Loss: 0.03652575984597206\n",
      "Epoch 949, Loss: 0.05861143255606294, Final Batch Loss: 0.006805269047617912\n",
      "Epoch 950, Loss: 0.07336457166820765, Final Batch Loss: 0.024647651240229607\n",
      "Epoch 951, Loss: 0.06083923648111522, Final Batch Loss: 0.01612231321632862\n",
      "Epoch 952, Loss: 0.0825130888260901, Final Batch Loss: 0.002016593236476183\n",
      "Epoch 953, Loss: 0.1275624695699662, Final Batch Loss: 0.002317040925845504\n",
      "Epoch 954, Loss: 0.07732271729037166, Final Batch Loss: 0.009261087514460087\n",
      "Epoch 955, Loss: 0.04890339728444815, Final Batch Loss: 0.011470269411802292\n",
      "Epoch 956, Loss: 0.06568077462725341, Final Batch Loss: 0.008626244962215424\n",
      "Epoch 957, Loss: 0.04840746335685253, Final Batch Loss: 0.015012945048511028\n",
      "Epoch 958, Loss: 0.04586996149737388, Final Batch Loss: 0.0019501774804666638\n",
      "Epoch 959, Loss: 0.06960710929706693, Final Batch Loss: 0.013284560292959213\n",
      "Epoch 960, Loss: 0.05303130159154534, Final Batch Loss: 0.003897033166140318\n",
      "Epoch 961, Loss: 0.0779920038767159, Final Batch Loss: 0.007728731259703636\n",
      "Epoch 962, Loss: 0.08577995887026191, Final Batch Loss: 0.006475570145994425\n",
      "Epoch 963, Loss: 0.04067665943875909, Final Batch Loss: 0.005180398467928171\n",
      "Epoch 964, Loss: 0.0957670439966023, Final Batch Loss: 0.01962675154209137\n",
      "Epoch 965, Loss: 0.0716168568469584, Final Batch Loss: 0.023543916642665863\n",
      "Epoch 966, Loss: 0.05973922717384994, Final Batch Loss: 0.0026170106139034033\n",
      "Epoch 967, Loss: 0.05211600300390273, Final Batch Loss: 0.00184055941645056\n",
      "Epoch 968, Loss: 0.04498526360839605, Final Batch Loss: 0.006112255156040192\n",
      "Epoch 969, Loss: 0.047267323127016425, Final Batch Loss: 0.0028844799380749464\n",
      "Epoch 970, Loss: 0.06506821629591286, Final Batch Loss: 0.006269553676247597\n",
      "Epoch 971, Loss: 0.0740291615948081, Final Batch Loss: 0.0008115535601973534\n",
      "Epoch 972, Loss: 0.05624861776595935, Final Batch Loss: 0.0007536610937677324\n",
      "Epoch 973, Loss: 0.07413100288249552, Final Batch Loss: 0.008569345809519291\n",
      "Epoch 974, Loss: 0.04820565995760262, Final Batch Loss: 0.011802208609879017\n",
      "Epoch 975, Loss: 0.04530376195907593, Final Batch Loss: 0.003974646329879761\n",
      "Epoch 976, Loss: 0.0646527165081352, Final Batch Loss: 0.002411623252555728\n",
      "Epoch 977, Loss: 0.06389659573324025, Final Batch Loss: 0.0025765353348106146\n",
      "Epoch 978, Loss: 0.04668007930740714, Final Batch Loss: 0.011878748424351215\n",
      "Epoch 979, Loss: 0.05479581211693585, Final Batch Loss: 0.004476180765777826\n",
      "Epoch 980, Loss: 0.05834170081652701, Final Batch Loss: 0.01310801599174738\n",
      "Epoch 981, Loss: 0.08261559903621674, Final Batch Loss: 0.04377157241106033\n",
      "Epoch 982, Loss: 0.042685003485530615, Final Batch Loss: 0.0022761845029890537\n",
      "Epoch 983, Loss: 0.07047214265912771, Final Batch Loss: 0.014693595468997955\n",
      "Epoch 984, Loss: 0.05008988967165351, Final Batch Loss: 0.02433648146688938\n",
      "Epoch 985, Loss: 0.0722666447982192, Final Batch Loss: 0.00980816874653101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986, Loss: 0.072010419331491, Final Batch Loss: 0.018788108602166176\n",
      "Epoch 987, Loss: 0.05372481234371662, Final Batch Loss: 0.01750367507338524\n",
      "Epoch 988, Loss: 0.06657873373478651, Final Batch Loss: 0.00480047008022666\n",
      "Epoch 989, Loss: 0.04063499392941594, Final Batch Loss: 0.0010506059043109417\n",
      "Epoch 990, Loss: 0.04513898794539273, Final Batch Loss: 0.015143465250730515\n",
      "Epoch 991, Loss: 0.050667787436395884, Final Batch Loss: 0.005414985120296478\n",
      "Epoch 992, Loss: 0.088777219876647, Final Batch Loss: 0.010802306234836578\n",
      "Epoch 993, Loss: 0.06435183621942997, Final Batch Loss: 0.030024802312254906\n",
      "Epoch 994, Loss: 0.09032534761354327, Final Batch Loss: 0.012109328992664814\n",
      "Epoch 995, Loss: 0.05886608152650297, Final Batch Loss: 0.00388729409314692\n",
      "Epoch 996, Loss: 0.052464385284110904, Final Batch Loss: 0.003865644335746765\n",
      "Epoch 997, Loss: 0.068974276073277, Final Batch Loss: 0.016169650480151176\n",
      "Epoch 998, Loss: 0.07137341797351837, Final Batch Loss: 0.022581253200769424\n",
      "Epoch 999, Loss: 0.07540983846411109, Final Batch Loss: 0.010218774899840355\n",
      "Epoch 1000, Loss: 0.04063927219249308, Final Batch Loss: 0.0032732675317674875\n",
      "Epoch 1001, Loss: 0.06522635230794549, Final Batch Loss: 0.018070349469780922\n",
      "Epoch 1002, Loss: 0.06033950624987483, Final Batch Loss: 0.004749348387122154\n",
      "Epoch 1003, Loss: 0.04951579903718084, Final Batch Loss: 0.005697420332580805\n",
      "Epoch 1004, Loss: 0.08054848993197083, Final Batch Loss: 0.004048267845064402\n",
      "Epoch 1005, Loss: 0.049193844257388264, Final Batch Loss: 0.0007836442091502249\n",
      "Epoch 1006, Loss: 0.08030356105882674, Final Batch Loss: 0.020809754729270935\n",
      "Epoch 1007, Loss: 0.0712108442094177, Final Batch Loss: 0.024754773825407028\n",
      "Epoch 1008, Loss: 0.03754634608048946, Final Batch Loss: 0.0017403039382770658\n",
      "Epoch 1009, Loss: 0.03709748387336731, Final Batch Loss: 0.0017784246010705829\n",
      "Epoch 1010, Loss: 0.05687101557850838, Final Batch Loss: 0.005892402492463589\n",
      "Epoch 1011, Loss: 0.050959532614797354, Final Batch Loss: 0.01788000948727131\n",
      "Epoch 1012, Loss: 0.05468522012233734, Final Batch Loss: 0.0014484415296465158\n",
      "Epoch 1013, Loss: 0.0652624499052763, Final Batch Loss: 0.006902820430696011\n",
      "Epoch 1014, Loss: 0.04534214921295643, Final Batch Loss: 0.003451891243457794\n",
      "Epoch 1015, Loss: 0.05670712236315012, Final Batch Loss: 0.020896822214126587\n",
      "Epoch 1016, Loss: 0.05917664151638746, Final Batch Loss: 0.02694690227508545\n",
      "Epoch 1017, Loss: 0.07062539458274841, Final Batch Loss: 0.02027486264705658\n",
      "Epoch 1018, Loss: 0.047401230316609144, Final Batch Loss: 0.008980179205536842\n",
      "Epoch 1019, Loss: 0.046839163172990084, Final Batch Loss: 0.015491204336285591\n",
      "Epoch 1020, Loss: 0.09163093427196145, Final Batch Loss: 0.002233642153441906\n",
      "Epoch 1021, Loss: 0.04283380275592208, Final Batch Loss: 0.004829011391848326\n",
      "Epoch 1022, Loss: 0.05656763445585966, Final Batch Loss: 0.013072538189589977\n",
      "Epoch 1023, Loss: 0.06594188907183707, Final Batch Loss: 0.010881553404033184\n",
      "Epoch 1024, Loss: 0.12629490438848734, Final Batch Loss: 0.026148786768317223\n",
      "Epoch 1025, Loss: 0.055099880089983344, Final Batch Loss: 0.003581854747608304\n",
      "Epoch 1026, Loss: 0.0418875397881493, Final Batch Loss: 0.005052562337368727\n",
      "Epoch 1027, Loss: 0.04249748261645436, Final Batch Loss: 0.0040703727863729\n",
      "Epoch 1028, Loss: 0.0527941610198468, Final Batch Loss: 0.0033150252420455217\n",
      "Epoch 1029, Loss: 0.055107953608967364, Final Batch Loss: 0.0006233210442587733\n",
      "Epoch 1030, Loss: 0.05613928916864097, Final Batch Loss: 0.007146588061004877\n",
      "Epoch 1031, Loss: 0.09536757599562407, Final Batch Loss: 0.015782508999109268\n",
      "Epoch 1032, Loss: 0.07822344242595136, Final Batch Loss: 0.016809409484267235\n",
      "Epoch 1033, Loss: 0.07302236219402403, Final Batch Loss: 0.049371931701898575\n",
      "Epoch 1034, Loss: 0.06934598926454782, Final Batch Loss: 0.025981925427913666\n",
      "Epoch 1035, Loss: 0.060511555057018995, Final Batch Loss: 0.00381399504840374\n",
      "Epoch 1036, Loss: 0.043701284332200885, Final Batch Loss: 0.008556688204407692\n",
      "Epoch 1037, Loss: 0.06799819925799966, Final Batch Loss: 0.022357625886797905\n",
      "Epoch 1038, Loss: 0.05849562562070787, Final Batch Loss: 0.0225896704941988\n",
      "Epoch 1039, Loss: 0.050344150979071856, Final Batch Loss: 0.004467834252864122\n",
      "Epoch 1040, Loss: 0.07915751705877483, Final Batch Loss: 0.025155531242489815\n",
      "Epoch 1041, Loss: 0.03160058555658907, Final Batch Loss: 0.0054587856866419315\n",
      "Epoch 1042, Loss: 0.052123092813417315, Final Batch Loss: 0.003953610081225634\n",
      "Epoch 1043, Loss: 0.06149680190719664, Final Batch Loss: 0.039401426911354065\n",
      "Epoch 1044, Loss: 0.03130028955638409, Final Batch Loss: 0.012775307521224022\n",
      "Epoch 1045, Loss: 0.04824746632948518, Final Batch Loss: 0.005251617170870304\n",
      "Epoch 1046, Loss: 0.08275391277857125, Final Batch Loss: 0.04660448059439659\n",
      "Epoch 1047, Loss: 0.0466274144127965, Final Batch Loss: 0.003444329369813204\n",
      "Epoch 1048, Loss: 0.05494957393966615, Final Batch Loss: 0.017564542591571808\n",
      "Epoch 1049, Loss: 0.0661995462141931, Final Batch Loss: 0.011412634514272213\n",
      "Epoch 1050, Loss: 0.06795119401067495, Final Batch Loss: 0.00450598681345582\n",
      "Epoch 1051, Loss: 0.04741400294005871, Final Batch Loss: 0.003990598022937775\n",
      "Epoch 1052, Loss: 0.06130048935301602, Final Batch Loss: 0.004730406682938337\n",
      "Epoch 1053, Loss: 0.08885578718036413, Final Batch Loss: 0.01167718879878521\n",
      "Epoch 1054, Loss: 0.07179301232099533, Final Batch Loss: 0.006956971250474453\n",
      "Epoch 1055, Loss: 0.08767446223646402, Final Batch Loss: 0.03498512879014015\n",
      "Epoch 1056, Loss: 0.052438730956055224, Final Batch Loss: 0.001660525449551642\n",
      "Epoch 1057, Loss: 0.045685607008636, Final Batch Loss: 0.009226196445524693\n",
      "Epoch 1058, Loss: 0.06306217820383608, Final Batch Loss: 0.03318668529391289\n",
      "Epoch 1059, Loss: 0.0530959730676841, Final Batch Loss: 0.0004515768669079989\n",
      "Epoch 1060, Loss: 0.04081947368104011, Final Batch Loss: 0.0010373025434091687\n",
      "Epoch 1061, Loss: 0.0665697765070945, Final Batch Loss: 0.01595178432762623\n",
      "Epoch 1062, Loss: 0.0603977395221591, Final Batch Loss: 0.020761119201779366\n",
      "Epoch 1063, Loss: 0.03285972727462649, Final Batch Loss: 0.0039795925840735435\n",
      "Epoch 1064, Loss: 0.047443289775401354, Final Batch Loss: 0.011160430498421192\n",
      "Epoch 1065, Loss: 0.06409090524539351, Final Batch Loss: 0.0318966805934906\n",
      "Epoch 1066, Loss: 0.08350574038922787, Final Batch Loss: 0.011953306384384632\n",
      "Epoch 1067, Loss: 0.03347027802374214, Final Batch Loss: 0.0018781711114570498\n",
      "Epoch 1068, Loss: 0.07768819318152964, Final Batch Loss: 0.02530098706483841\n",
      "Epoch 1069, Loss: 0.05261514545418322, Final Batch Loss: 0.0027908729389309883\n",
      "Epoch 1070, Loss: 0.040565666276961565, Final Batch Loss: 0.007188883144408464\n",
      "Epoch 1071, Loss: 0.0691339853219688, Final Batch Loss: 0.02017476223409176\n",
      "Epoch 1072, Loss: 0.028504199057351798, Final Batch Loss: 0.0005272423732094467\n",
      "Epoch 1073, Loss: 0.04726523719727993, Final Batch Loss: 0.006648841314017773\n",
      "Epoch 1074, Loss: 0.05621956288814545, Final Batch Loss: 0.012514026835560799\n",
      "Epoch 1075, Loss: 0.055664377519860864, Final Batch Loss: 0.0238186027854681\n",
      "Epoch 1076, Loss: 0.04054963658563793, Final Batch Loss: 0.001251530135050416\n",
      "Epoch 1077, Loss: 0.055372505565173924, Final Batch Loss: 0.015627501532435417\n",
      "Epoch 1078, Loss: 0.04049605247564614, Final Batch Loss: 0.01065912563353777\n",
      "Epoch 1079, Loss: 0.05869653541594744, Final Batch Loss: 0.005511808674782515\n",
      "Epoch 1080, Loss: 0.08993226219899952, Final Batch Loss: 0.03287162631750107\n",
      "Epoch 1081, Loss: 0.06371496617794037, Final Batch Loss: 0.010594509541988373\n",
      "Epoch 1082, Loss: 0.09181987214833498, Final Batch Loss: 0.05145983025431633\n",
      "Epoch 1083, Loss: 0.0368459903402254, Final Batch Loss: 0.002868840005248785\n",
      "Epoch 1084, Loss: 0.04265616997145116, Final Batch Loss: 0.004769057501107454\n",
      "Epoch 1085, Loss: 0.08571390714496374, Final Batch Loss: 0.01098888274282217\n",
      "Epoch 1086, Loss: 0.07262622006237507, Final Batch Loss: 0.0032252520322799683\n",
      "Epoch 1087, Loss: 0.07810875587165356, Final Batch Loss: 0.04374147579073906\n",
      "Epoch 1088, Loss: 0.06184902414679527, Final Batch Loss: 0.008450571447610855\n",
      "Epoch 1089, Loss: 0.07755077094770968, Final Batch Loss: 0.003998220432549715\n",
      "Epoch 1090, Loss: 0.049312641262076795, Final Batch Loss: 0.00286923348903656\n",
      "Epoch 1091, Loss: 0.044281961396336555, Final Batch Loss: 0.004830744117498398\n",
      "Epoch 1092, Loss: 0.061244877288118005, Final Batch Loss: 0.02890583500266075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1093, Loss: 0.07309044944122434, Final Batch Loss: 0.022794460877776146\n",
      "Epoch 1094, Loss: 0.06869399035349488, Final Batch Loss: 0.009147407487034798\n",
      "Epoch 1095, Loss: 0.055811598198488355, Final Batch Loss: 0.008948047645390034\n",
      "Epoch 1096, Loss: 0.06198919774033129, Final Batch Loss: 0.003402498783543706\n",
      "Epoch 1097, Loss: 0.05017954693175852, Final Batch Loss: 0.003851138288155198\n",
      "Epoch 1098, Loss: 0.07388079026713967, Final Batch Loss: 0.004054730292409658\n",
      "Epoch 1099, Loss: 0.031919515458866954, Final Batch Loss: 0.0071441782638430595\n",
      "Epoch 1100, Loss: 0.054000800591893494, Final Batch Loss: 0.002678116550669074\n",
      "Epoch 1101, Loss: 0.0532303883228451, Final Batch Loss: 0.001930529484525323\n",
      "Epoch 1102, Loss: 0.06027289177291095, Final Batch Loss: 0.006543466355651617\n",
      "Epoch 1103, Loss: 0.06977491127327085, Final Batch Loss: 0.027118923142552376\n",
      "Epoch 1104, Loss: 0.053221311420202255, Final Batch Loss: 0.01649508625268936\n",
      "Epoch 1105, Loss: 0.06115461187437177, Final Batch Loss: 0.019709037616848946\n",
      "Epoch 1106, Loss: 0.044648291543126106, Final Batch Loss: 0.017604539170861244\n",
      "Epoch 1107, Loss: 0.05873108236119151, Final Batch Loss: 0.017784198746085167\n",
      "Epoch 1108, Loss: 0.05767247290350497, Final Batch Loss: 0.034280601888895035\n",
      "Epoch 1109, Loss: 0.03817069553770125, Final Batch Loss: 0.014275843277573586\n",
      "Epoch 1110, Loss: 0.0362280763220042, Final Batch Loss: 0.002091354690492153\n",
      "Epoch 1111, Loss: 0.050075937528163195, Final Batch Loss: 0.003126897383481264\n",
      "Epoch 1112, Loss: 0.08197122672572732, Final Batch Loss: 0.016513003036379814\n",
      "Epoch 1113, Loss: 0.06745676044374704, Final Batch Loss: 0.004244531039148569\n",
      "Epoch 1114, Loss: 0.06482429290190339, Final Batch Loss: 0.007551511283963919\n",
      "Epoch 1115, Loss: 0.09283107984811068, Final Batch Loss: 0.014770877547562122\n",
      "Epoch 1116, Loss: 0.04708739393390715, Final Batch Loss: 0.02573266439139843\n",
      "Epoch 1117, Loss: 0.04487977456301451, Final Batch Loss: 0.0072559514082968235\n",
      "Epoch 1118, Loss: 0.055117846466600895, Final Batch Loss: 0.01872950978577137\n",
      "Epoch 1119, Loss: 0.034556442871689796, Final Batch Loss: 0.0012675344478338957\n",
      "Epoch 1120, Loss: 0.06418875465169549, Final Batch Loss: 0.0031311153434216976\n",
      "Epoch 1121, Loss: 0.06601517950184643, Final Batch Loss: 0.03386950120329857\n",
      "Epoch 1122, Loss: 0.057582508539780974, Final Batch Loss: 0.02799568884074688\n",
      "Epoch 1123, Loss: 0.07865238096565008, Final Batch Loss: 0.019252881407737732\n",
      "Epoch 1124, Loss: 0.05516589665785432, Final Batch Loss: 0.0025777413975447416\n",
      "Epoch 1125, Loss: 0.06892004772089422, Final Batch Loss: 0.005979441571980715\n",
      "Epoch 1126, Loss: 0.04154928750358522, Final Batch Loss: 0.0011571652721613646\n",
      "Epoch 1127, Loss: 0.06385040190070868, Final Batch Loss: 0.015543432906270027\n",
      "Epoch 1128, Loss: 0.03535062458831817, Final Batch Loss: 0.00806763581931591\n",
      "Epoch 1129, Loss: 0.0597551129758358, Final Batch Loss: 0.006961490493267775\n",
      "Epoch 1130, Loss: 0.054826403385959566, Final Batch Loss: 0.011913958936929703\n",
      "Epoch 1131, Loss: 0.044059948064386845, Final Batch Loss: 0.0037004174664616585\n",
      "Epoch 1132, Loss: 0.04103830421809107, Final Batch Loss: 0.005447286646813154\n",
      "Epoch 1133, Loss: 0.05810730578377843, Final Batch Loss: 0.02003236673772335\n",
      "Epoch 1134, Loss: 0.07678789837518707, Final Batch Loss: 0.0008330667042173445\n",
      "Epoch 1135, Loss: 0.08247069723438472, Final Batch Loss: 0.058983225375413895\n",
      "Epoch 1136, Loss: 0.04213852039538324, Final Batch Loss: 0.01884353719651699\n",
      "Epoch 1137, Loss: 0.05614461563527584, Final Batch Loss: 0.006979421246796846\n",
      "Epoch 1138, Loss: 0.05076986108906567, Final Batch Loss: 0.014864749275147915\n",
      "Epoch 1139, Loss: 0.048259061528369784, Final Batch Loss: 0.02445230260491371\n",
      "Epoch 1140, Loss: 0.04014203976839781, Final Batch Loss: 0.006049007177352905\n",
      "Epoch 1141, Loss: 0.034920867532491684, Final Batch Loss: 0.0009869374334812164\n",
      "Epoch 1142, Loss: 0.0624063964933157, Final Batch Loss: 0.030146293342113495\n",
      "Epoch 1143, Loss: 0.027138821315020323, Final Batch Loss: 0.0008417703211307526\n",
      "Epoch 1144, Loss: 0.04378010192885995, Final Batch Loss: 0.004704833962023258\n",
      "Epoch 1145, Loss: 0.04117553122341633, Final Batch Loss: 0.0032554748468101025\n",
      "Epoch 1146, Loss: 0.033070209494326264, Final Batch Loss: 0.0007086241967044771\n",
      "Epoch 1147, Loss: 0.05278994981199503, Final Batch Loss: 0.01594417542219162\n",
      "Epoch 1148, Loss: 0.04797113034874201, Final Batch Loss: 0.005958581808954477\n",
      "Epoch 1149, Loss: 0.03749024437274784, Final Batch Loss: 0.001941343885846436\n",
      "Epoch 1150, Loss: 0.04153150552883744, Final Batch Loss: 0.0018603438511490822\n",
      "Epoch 1151, Loss: 0.06543124956078827, Final Batch Loss: 0.04334856569766998\n",
      "Epoch 1152, Loss: 0.06250529293902218, Final Batch Loss: 0.030824236571788788\n",
      "Epoch 1153, Loss: 0.04828114062547684, Final Batch Loss: 0.009471583180129528\n",
      "Epoch 1154, Loss: 0.04776656883768737, Final Batch Loss: 0.0020210284274071455\n",
      "Epoch 1155, Loss: 0.035503795836120844, Final Batch Loss: 0.0070638046599924564\n",
      "Epoch 1156, Loss: 0.06500591756775975, Final Batch Loss: 0.0025392617098987103\n",
      "Epoch 1157, Loss: 0.05341616598889232, Final Batch Loss: 0.015204530209302902\n",
      "Epoch 1158, Loss: 0.04659988428466022, Final Batch Loss: 0.009794593788683414\n",
      "Epoch 1159, Loss: 0.033214890863746405, Final Batch Loss: 0.004227040335536003\n",
      "Epoch 1160, Loss: 0.04502084688283503, Final Batch Loss: 0.02274446189403534\n",
      "Epoch 1161, Loss: 0.03776566102169454, Final Batch Loss: 0.0018144517671316862\n",
      "Epoch 1162, Loss: 0.04825431969948113, Final Batch Loss: 0.015414213761687279\n",
      "Epoch 1163, Loss: 0.05132989143021405, Final Batch Loss: 0.016228148713707924\n",
      "Epoch 1164, Loss: 0.05846994370222092, Final Batch Loss: 0.0013221390545368195\n",
      "Epoch 1165, Loss: 0.053463960299268365, Final Batch Loss: 0.007848870940506458\n",
      "Epoch 1166, Loss: 0.03681777021847665, Final Batch Loss: 0.0049230861477553844\n",
      "Epoch 1167, Loss: 0.043952491134405136, Final Batch Loss: 0.015044508501887321\n",
      "Epoch 1168, Loss: 0.07669680030085146, Final Batch Loss: 0.0008907115552574396\n",
      "Epoch 1169, Loss: 0.04572175955399871, Final Batch Loss: 0.0050637065432965755\n",
      "Epoch 1170, Loss: 0.12336084898561239, Final Batch Loss: 0.007018460892140865\n",
      "Epoch 1171, Loss: 0.07936640316620469, Final Batch Loss: 0.006900069769471884\n",
      "Epoch 1172, Loss: 0.09685651329346001, Final Batch Loss: 0.051791492849588394\n",
      "Epoch 1173, Loss: 0.06430902658030391, Final Batch Loss: 0.01336582936346531\n",
      "Epoch 1174, Loss: 0.03858331171795726, Final Batch Loss: 0.009827603586018085\n",
      "Epoch 1175, Loss: 0.03924314375035465, Final Batch Loss: 0.008606026880443096\n",
      "Epoch 1176, Loss: 0.04673457588069141, Final Batch Loss: 0.006961426232010126\n",
      "Epoch 1177, Loss: 0.06325376132735983, Final Batch Loss: 0.03779054433107376\n",
      "Epoch 1178, Loss: 0.038601554930210114, Final Batch Loss: 0.010052390396595001\n",
      "Epoch 1179, Loss: 0.04034973029047251, Final Batch Loss: 0.008642694912850857\n",
      "Epoch 1180, Loss: 0.03169031170546077, Final Batch Loss: 0.00046718711382709444\n",
      "Epoch 1181, Loss: 0.058859776239842176, Final Batch Loss: 0.0028768456541001797\n",
      "Epoch 1182, Loss: 0.04287256859242916, Final Batch Loss: 0.005725855007767677\n",
      "Epoch 1183, Loss: 0.05240818718448281, Final Batch Loss: 0.006171547807753086\n",
      "Epoch 1184, Loss: 0.07366255531087518, Final Batch Loss: 0.037714097648859024\n",
      "Epoch 1185, Loss: 0.08784918859601021, Final Batch Loss: 0.009306742809712887\n",
      "Epoch 1186, Loss: 0.05278252437710762, Final Batch Loss: 0.002773178741335869\n",
      "Epoch 1187, Loss: 0.06115789507748559, Final Batch Loss: 0.0008898142841644585\n",
      "Epoch 1188, Loss: 0.05132772121578455, Final Batch Loss: 0.0035919900983572006\n",
      "Epoch 1189, Loss: 0.0488341964664869, Final Batch Loss: 0.017409684136509895\n",
      "Epoch 1190, Loss: 0.043972892919555306, Final Batch Loss: 0.009444634430110455\n",
      "Epoch 1191, Loss: 0.06561048049479723, Final Batch Loss: 0.027146216481924057\n",
      "Epoch 1192, Loss: 0.049995882669463754, Final Batch Loss: 0.0020222049206495285\n",
      "Epoch 1193, Loss: 0.05247282423079014, Final Batch Loss: 0.011361174285411835\n",
      "Epoch 1194, Loss: 0.0448705229209736, Final Batch Loss: 0.0018769103335216641\n",
      "Epoch 1195, Loss: 0.06490053725428879, Final Batch Loss: 0.009618258103728294\n",
      "Epoch 1196, Loss: 0.03851839271374047, Final Batch Loss: 0.00811693910509348\n",
      "Epoch 1197, Loss: 0.06443045358173549, Final Batch Loss: 0.013384443707764149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1198, Loss: 0.031225240149069577, Final Batch Loss: 0.0006843575392849743\n",
      "Epoch 1199, Loss: 0.06591372971888632, Final Batch Loss: 0.012500024400651455\n",
      "Epoch 1200, Loss: 0.03861147444695234, Final Batch Loss: 0.004105926025658846\n",
      "Epoch 1201, Loss: 0.030127463047392666, Final Batch Loss: 0.0026331478729844093\n",
      "Epoch 1202, Loss: 0.0559109658934176, Final Batch Loss: 0.007873311638832092\n",
      "Epoch 1203, Loss: 0.04810713394545019, Final Batch Loss: 0.001846143277361989\n",
      "Epoch 1204, Loss: 0.03904017619788647, Final Batch Loss: 0.0005199001170694828\n",
      "Epoch 1205, Loss: 0.04936236282810569, Final Batch Loss: 0.004027005285024643\n",
      "Epoch 1206, Loss: 0.05763240111991763, Final Batch Loss: 0.004510125610977411\n",
      "Epoch 1207, Loss: 0.06654882105067372, Final Batch Loss: 0.004776268731802702\n",
      "Epoch 1208, Loss: 0.048764808918349445, Final Batch Loss: 0.003136470215395093\n",
      "Epoch 1209, Loss: 0.06248618196696043, Final Batch Loss: 0.011092761531472206\n",
      "Epoch 1210, Loss: 0.057075624354183674, Final Batch Loss: 0.019223416224122047\n",
      "Epoch 1211, Loss: 0.05059436243027449, Final Batch Loss: 0.003103216178715229\n",
      "Epoch 1212, Loss: 0.10438192822039127, Final Batch Loss: 0.07336287945508957\n",
      "Epoch 1213, Loss: 0.03861759859137237, Final Batch Loss: 0.0039053859654814005\n",
      "Epoch 1214, Loss: 0.05172015377320349, Final Batch Loss: 0.002157609211280942\n",
      "Epoch 1215, Loss: 0.05117980926297605, Final Batch Loss: 0.004183650948107243\n",
      "Epoch 1216, Loss: 0.048041151370853186, Final Batch Loss: 0.011583724990487099\n",
      "Epoch 1217, Loss: 0.057290312834084034, Final Batch Loss: 0.004600629210472107\n",
      "Epoch 1218, Loss: 0.04625068118912168, Final Batch Loss: 0.013865683227777481\n",
      "Epoch 1219, Loss: 0.04351849411614239, Final Batch Loss: 0.0024116726126521826\n",
      "Epoch 1220, Loss: 0.06245071173179895, Final Batch Loss: 0.015784457325935364\n",
      "Epoch 1221, Loss: 0.03229967155493796, Final Batch Loss: 0.003316033398732543\n",
      "Epoch 1222, Loss: 0.02777290827361867, Final Batch Loss: 0.00025301723508164287\n",
      "Epoch 1223, Loss: 0.03541908459737897, Final Batch Loss: 0.002455482492223382\n",
      "Epoch 1224, Loss: 0.04174719215370715, Final Batch Loss: 0.0020328008104115725\n",
      "Epoch 1225, Loss: 0.03119417035486549, Final Batch Loss: 0.003421807661652565\n",
      "Epoch 1226, Loss: 0.060627039056271315, Final Batch Loss: 0.02048446424305439\n",
      "Epoch 1227, Loss: 0.062105115270242095, Final Batch Loss: 0.01815078593790531\n",
      "Epoch 1228, Loss: 0.04029447655193508, Final Batch Loss: 0.004446004517376423\n",
      "Epoch 1229, Loss: 0.03235281375236809, Final Batch Loss: 0.004170851316303015\n",
      "Epoch 1230, Loss: 0.06205906183458865, Final Batch Loss: 0.0037885077763348818\n",
      "Epoch 1231, Loss: 0.050535995862446725, Final Batch Loss: 0.013476431369781494\n",
      "Epoch 1232, Loss: 0.04728503432124853, Final Batch Loss: 0.002652252558618784\n",
      "Epoch 1233, Loss: 0.03664656460750848, Final Batch Loss: 0.023096082732081413\n",
      "Epoch 1234, Loss: 0.04665922105778009, Final Batch Loss: 0.015370908193290234\n",
      "Epoch 1235, Loss: 0.05134615418501198, Final Batch Loss: 0.005302049685269594\n",
      "Epoch 1236, Loss: 0.06206314079463482, Final Batch Loss: 0.015699727460741997\n",
      "Epoch 1237, Loss: 0.04036090767476708, Final Batch Loss: 0.0036444293800741434\n",
      "Epoch 1238, Loss: 0.07369780912995338, Final Batch Loss: 0.005516550969332457\n",
      "Epoch 1239, Loss: 0.05118617136031389, Final Batch Loss: 0.003455820959061384\n",
      "Epoch 1240, Loss: 0.05389910563826561, Final Batch Loss: 0.006197917740792036\n",
      "Epoch 1241, Loss: 0.06855721771717072, Final Batch Loss: 0.010211358778178692\n",
      "Epoch 1242, Loss: 0.033344034221954644, Final Batch Loss: 0.002826939569786191\n",
      "Epoch 1243, Loss: 0.059137416537851095, Final Batch Loss: 0.014498216100037098\n",
      "Epoch 1244, Loss: 0.05125152162509039, Final Batch Loss: 0.000615874130744487\n",
      "Epoch 1245, Loss: 0.05170420045033097, Final Batch Loss: 0.002623927779495716\n",
      "Epoch 1246, Loss: 0.05069253628607839, Final Batch Loss: 0.002751820720732212\n",
      "Epoch 1247, Loss: 0.061760010197758675, Final Batch Loss: 0.00876654963940382\n",
      "Epoch 1248, Loss: 0.030477899010293186, Final Batch Loss: 0.0027906300965696573\n",
      "Epoch 1249, Loss: 0.07399720838293433, Final Batch Loss: 0.007551314774900675\n",
      "Epoch 1250, Loss: 0.06414020014926791, Final Batch Loss: 0.005128609947860241\n",
      "Epoch 1251, Loss: 0.052472853916697204, Final Batch Loss: 0.016007471829652786\n",
      "Epoch 1252, Loss: 0.05520963575690985, Final Batch Loss: 0.002169268671423197\n",
      "Epoch 1253, Loss: 0.03395159216597676, Final Batch Loss: 0.004971212241798639\n",
      "Epoch 1254, Loss: 0.05054636066779494, Final Batch Loss: 0.005637831520289183\n",
      "Epoch 1255, Loss: 0.06202946789562702, Final Batch Loss: 0.003997324034571648\n",
      "Epoch 1256, Loss: 0.048070157412439585, Final Batch Loss: 0.006371898110955954\n",
      "Epoch 1257, Loss: 0.04918337462004274, Final Batch Loss: 0.007766358554363251\n",
      "Epoch 1258, Loss: 0.03597763879224658, Final Batch Loss: 0.0013244857545942068\n",
      "Epoch 1259, Loss: 0.03249112010234967, Final Batch Loss: 0.0009453844395466149\n",
      "Epoch 1260, Loss: 0.033168374095112085, Final Batch Loss: 0.0027808540035039186\n",
      "Epoch 1261, Loss: 0.04496963124256581, Final Batch Loss: 0.017956214025616646\n",
      "Epoch 1262, Loss: 0.04505326505750418, Final Batch Loss: 0.009584516286849976\n",
      "Epoch 1263, Loss: 0.055589472060091794, Final Batch Loss: 0.031386930495500565\n",
      "Epoch 1264, Loss: 0.03845429152715951, Final Batch Loss: 0.0020101198460906744\n",
      "Epoch 1265, Loss: 0.045961381983943284, Final Batch Loss: 0.019107790663838387\n",
      "Epoch 1266, Loss: 0.06405795132741332, Final Batch Loss: 0.024817852303385735\n",
      "Epoch 1267, Loss: 0.050392447505146265, Final Batch Loss: 0.0020660238806158304\n",
      "Epoch 1268, Loss: 0.067733203060925, Final Batch Loss: 0.004376629367470741\n",
      "Epoch 1269, Loss: 0.06907309126108885, Final Batch Loss: 0.023888887837529182\n",
      "Epoch 1270, Loss: 0.044939928222447634, Final Batch Loss: 0.001091256272047758\n",
      "Epoch 1271, Loss: 0.04720036336220801, Final Batch Loss: 0.0011392749147489667\n",
      "Epoch 1272, Loss: 0.04527960671111941, Final Batch Loss: 0.003109414828941226\n",
      "Epoch 1273, Loss: 0.05550042659160681, Final Batch Loss: 0.00040072263800539076\n",
      "Epoch 1274, Loss: 0.061085655353963375, Final Batch Loss: 0.024049045518040657\n",
      "Epoch 1275, Loss: 0.04146467614918947, Final Batch Loss: 0.0037264549173414707\n",
      "Epoch 1276, Loss: 0.050327128963544965, Final Batch Loss: 0.017121315002441406\n",
      "Epoch 1277, Loss: 0.03936149994842708, Final Batch Loss: 0.005389561876654625\n",
      "Epoch 1278, Loss: 0.04027966782450676, Final Batch Loss: 0.006531968247145414\n",
      "Epoch 1279, Loss: 0.05003102356567979, Final Batch Loss: 0.0138154998421669\n",
      "Epoch 1280, Loss: 0.028934549656696618, Final Batch Loss: 0.007647580001503229\n",
      "Epoch 1281, Loss: 0.0359161994419992, Final Batch Loss: 0.005268324166536331\n",
      "Epoch 1282, Loss: 0.049393942521419376, Final Batch Loss: 0.00046581815695390105\n",
      "Epoch 1283, Loss: 0.056063447846099734, Final Batch Loss: 0.002783390460535884\n",
      "Epoch 1284, Loss: 0.032184421783313155, Final Batch Loss: 0.012373579666018486\n",
      "Epoch 1285, Loss: 0.04015402472577989, Final Batch Loss: 0.006297963671386242\n",
      "Epoch 1286, Loss: 0.06969853397458792, Final Batch Loss: 0.006904200185090303\n",
      "Epoch 1287, Loss: 0.08082802407443523, Final Batch Loss: 0.03932143747806549\n",
      "Epoch 1288, Loss: 0.03348131477832794, Final Batch Loss: 0.005720650777220726\n",
      "Epoch 1289, Loss: 0.05406453274190426, Final Batch Loss: 0.0023146974854171276\n",
      "Epoch 1290, Loss: 0.07369266427122056, Final Batch Loss: 0.017087766900658607\n",
      "Epoch 1291, Loss: 0.049067532643675804, Final Batch Loss: 0.00924945529550314\n",
      "Epoch 1292, Loss: 0.0627572382800281, Final Batch Loss: 0.013981802389025688\n",
      "Epoch 1293, Loss: 0.05484431516379118, Final Batch Loss: 0.0031834952533245087\n",
      "Epoch 1294, Loss: 0.08969755831640214, Final Batch Loss: 0.0006862693699076772\n",
      "Epoch 1295, Loss: 0.05407667439430952, Final Batch Loss: 0.0074227480217814445\n",
      "Epoch 1296, Loss: 0.05828413343988359, Final Batch Loss: 0.005652547348290682\n",
      "Epoch 1297, Loss: 0.11995049356482923, Final Batch Loss: 0.08068394660949707\n",
      "Epoch 1298, Loss: 0.04892078856937587, Final Batch Loss: 0.021787742152810097\n",
      "Epoch 1299, Loss: 0.03416067291982472, Final Batch Loss: 0.0011462788097560406\n",
      "Epoch 1300, Loss: 0.08432897593593225, Final Batch Loss: 0.0008059394895099103\n",
      "Epoch 1301, Loss: 0.035392550053074956, Final Batch Loss: 0.004965849220752716\n",
      "Epoch 1302, Loss: 0.0505817529046908, Final Batch Loss: 0.010922149755060673\n",
      "Epoch 1303, Loss: 0.0324649210087955, Final Batch Loss: 0.011467640288174152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1304, Loss: 0.046712087700143456, Final Batch Loss: 0.004959647078067064\n",
      "Epoch 1305, Loss: 0.03694820776581764, Final Batch Loss: 0.01639745756983757\n",
      "Epoch 1306, Loss: 0.03300327225588262, Final Batch Loss: 0.010699520818889141\n",
      "Epoch 1307, Loss: 0.07119165733456612, Final Batch Loss: 0.04062269628047943\n",
      "Epoch 1308, Loss: 0.03935717139393091, Final Batch Loss: 0.01027096901088953\n",
      "Epoch 1309, Loss: 0.06409072177484632, Final Batch Loss: 0.012245941907167435\n",
      "Epoch 1310, Loss: 0.03636369900777936, Final Batch Loss: 0.006851898040622473\n",
      "Epoch 1311, Loss: 0.057684475439600646, Final Batch Loss: 0.0011893723858520389\n",
      "Epoch 1312, Loss: 0.08242397278081626, Final Batch Loss: 0.0007600117241963744\n",
      "Epoch 1313, Loss: 0.0524577657924965, Final Batch Loss: 0.015564796514809132\n",
      "Epoch 1314, Loss: 0.052678070147521794, Final Batch Loss: 0.0019004677888005972\n",
      "Epoch 1315, Loss: 0.04860592097975314, Final Batch Loss: 0.02416873164474964\n",
      "Epoch 1316, Loss: 0.040230473794508725, Final Batch Loss: 0.010127323679625988\n",
      "Epoch 1317, Loss: 0.04722109669819474, Final Batch Loss: 0.0008945595473051071\n",
      "Epoch 1318, Loss: 0.08155006752349436, Final Batch Loss: 0.032785575836896896\n",
      "Epoch 1319, Loss: 0.04807728511514142, Final Batch Loss: 0.00022443366469815373\n",
      "Epoch 1320, Loss: 0.03091578441672027, Final Batch Loss: 0.005592856090515852\n",
      "Epoch 1321, Loss: 0.05259752168785781, Final Batch Loss: 0.0014019262744113803\n",
      "Epoch 1322, Loss: 0.047934035770595074, Final Batch Loss: 0.01023893989622593\n",
      "Epoch 1323, Loss: 0.05512475688010454, Final Batch Loss: 0.004272785037755966\n",
      "Epoch 1324, Loss: 0.029360304353758693, Final Batch Loss: 0.0019243663409724832\n",
      "Epoch 1325, Loss: 0.0420039800228551, Final Batch Loss: 0.014419332146644592\n",
      "Epoch 1326, Loss: 0.042295105289667845, Final Batch Loss: 0.0047125923447310925\n",
      "Epoch 1327, Loss: 0.02280585072003305, Final Batch Loss: 0.002493475563824177\n",
      "Epoch 1328, Loss: 0.09452844643965364, Final Batch Loss: 0.0603666715323925\n",
      "Epoch 1329, Loss: 0.07493563927710056, Final Batch Loss: 0.004508438520133495\n",
      "Epoch 1330, Loss: 0.07016006344929338, Final Batch Loss: 0.021267861127853394\n",
      "Epoch 1331, Loss: 0.05054417485371232, Final Batch Loss: 0.011743859387934208\n",
      "Epoch 1332, Loss: 0.04902142734499648, Final Batch Loss: 0.0051573216915130615\n",
      "Epoch 1333, Loss: 0.07397437305189669, Final Batch Loss: 0.004755106288939714\n",
      "Epoch 1334, Loss: 0.053783010749612004, Final Batch Loss: 0.009974604472517967\n",
      "Epoch 1335, Loss: 0.03911982267163694, Final Batch Loss: 0.007746960502117872\n",
      "Epoch 1336, Loss: 0.040142552461475134, Final Batch Loss: 0.004002957604825497\n",
      "Epoch 1337, Loss: 0.04211712465621531, Final Batch Loss: 0.0049667805433273315\n",
      "Epoch 1338, Loss: 0.048643798218108714, Final Batch Loss: 0.004572787322103977\n",
      "Epoch 1339, Loss: 0.06425357260741293, Final Batch Loss: 0.026374775916337967\n",
      "Epoch 1340, Loss: 0.054661443806253374, Final Batch Loss: 0.024950187653303146\n",
      "Epoch 1341, Loss: 0.03223338024690747, Final Batch Loss: 0.005216467659920454\n",
      "Epoch 1342, Loss: 0.050993529497645795, Final Batch Loss: 0.009857562370598316\n",
      "Epoch 1343, Loss: 0.054139032145030797, Final Batch Loss: 0.0008981744758784771\n",
      "Epoch 1344, Loss: 0.06310965097509325, Final Batch Loss: 0.031912583857774734\n",
      "Epoch 1345, Loss: 0.06388787692412734, Final Batch Loss: 0.015695316717028618\n",
      "Epoch 1346, Loss: 0.02637453458737582, Final Batch Loss: 0.006309330929070711\n",
      "Epoch 1347, Loss: 0.07130524329841137, Final Batch Loss: 0.0042832884937524796\n",
      "Epoch 1348, Loss: 0.05359935713931918, Final Batch Loss: 0.00881311483681202\n",
      "Epoch 1349, Loss: 0.0385324046947062, Final Batch Loss: 0.004557271022349596\n",
      "Epoch 1350, Loss: 0.03548763087019324, Final Batch Loss: 0.01470800582319498\n",
      "Epoch 1351, Loss: 0.07033541146665812, Final Batch Loss: 0.022593310102820396\n",
      "Epoch 1352, Loss: 0.05079403845593333, Final Batch Loss: 0.011086815968155861\n",
      "Epoch 1353, Loss: 0.04868656862527132, Final Batch Loss: 0.0144434105604887\n",
      "Epoch 1354, Loss: 0.0324609917588532, Final Batch Loss: 0.019579991698265076\n",
      "Epoch 1355, Loss: 0.03277278761379421, Final Batch Loss: 0.005759395658969879\n",
      "Epoch 1356, Loss: 0.058810806134715676, Final Batch Loss: 0.0030061155557632446\n",
      "Epoch 1357, Loss: 0.057067059678956866, Final Batch Loss: 0.027206948027014732\n",
      "Epoch 1358, Loss: 0.04765780223533511, Final Batch Loss: 0.007122641429305077\n",
      "Epoch 1359, Loss: 0.04539734101854265, Final Batch Loss: 0.0010771413799375296\n",
      "Epoch 1360, Loss: 0.02694135019555688, Final Batch Loss: 0.00472149346023798\n",
      "Epoch 1361, Loss: 0.023408008739352226, Final Batch Loss: 0.002086714841425419\n",
      "Epoch 1362, Loss: 0.047925938735716045, Final Batch Loss: 0.004544164519757032\n",
      "Epoch 1363, Loss: 0.050090740143787116, Final Batch Loss: 0.01649773307144642\n",
      "Epoch 1364, Loss: 0.05910128937102854, Final Batch Loss: 0.014226933009922504\n",
      "Epoch 1365, Loss: 0.06166661699535325, Final Batch Loss: 0.04303312301635742\n",
      "Epoch 1366, Loss: 0.03754247818142176, Final Batch Loss: 0.010589909739792347\n",
      "Epoch 1367, Loss: 0.04103117808699608, Final Batch Loss: 0.002231969963759184\n",
      "Epoch 1368, Loss: 0.06202404701616615, Final Batch Loss: 0.04163661599159241\n",
      "Epoch 1369, Loss: 0.04190810746513307, Final Batch Loss: 0.010659931227564812\n",
      "Epoch 1370, Loss: 0.08287609042599797, Final Batch Loss: 0.004768159706145525\n",
      "Epoch 1371, Loss: 0.036262381938286126, Final Batch Loss: 0.013974207453429699\n",
      "Epoch 1372, Loss: 0.038372390205040574, Final Batch Loss: 0.0072549632750451565\n",
      "Epoch 1373, Loss: 0.05188469309359789, Final Batch Loss: 0.0126457829028368\n",
      "Epoch 1374, Loss: 0.04331123433075845, Final Batch Loss: 0.018267672508955002\n",
      "Epoch 1375, Loss: 0.1283639167668298, Final Batch Loss: 0.06875614076852798\n",
      "Epoch 1376, Loss: 0.05327835807111114, Final Batch Loss: 0.0009931250242516398\n",
      "Epoch 1377, Loss: 0.060700325993821025, Final Batch Loss: 0.032375819981098175\n",
      "Epoch 1378, Loss: 0.035726738162338734, Final Batch Loss: 0.01843833550810814\n",
      "Epoch 1379, Loss: 0.036172436201013625, Final Batch Loss: 0.009186726063489914\n",
      "Epoch 1380, Loss: 0.04910390137229115, Final Batch Loss: 0.009236239828169346\n",
      "Epoch 1381, Loss: 0.05379102937877178, Final Batch Loss: 0.007710644509643316\n",
      "Epoch 1382, Loss: 0.06581551150884479, Final Batch Loss: 0.0019859361927956343\n",
      "Epoch 1383, Loss: 0.08754616579972208, Final Batch Loss: 0.0018120685126632452\n",
      "Epoch 1384, Loss: 0.0706747320946306, Final Batch Loss: 0.005249197129160166\n",
      "Epoch 1385, Loss: 0.04791038949042559, Final Batch Loss: 0.005991926416754723\n",
      "Epoch 1386, Loss: 0.054777913726866245, Final Batch Loss: 0.0012360219843685627\n",
      "Epoch 1387, Loss: 0.03699836472515017, Final Batch Loss: 0.001551796798594296\n",
      "Epoch 1388, Loss: 0.04780133394524455, Final Batch Loss: 0.0021242303773760796\n",
      "Epoch 1389, Loss: 0.04434694512747228, Final Batch Loss: 0.002072429284453392\n",
      "Epoch 1390, Loss: 0.032217081636190414, Final Batch Loss: 0.0028629200533032417\n",
      "Epoch 1391, Loss: 0.0487996281881351, Final Batch Loss: 0.0016409920062869787\n",
      "Epoch 1392, Loss: 0.06434201402589679, Final Batch Loss: 0.001650185789912939\n",
      "Epoch 1393, Loss: 0.03470378974452615, Final Batch Loss: 0.005580064374953508\n",
      "Epoch 1394, Loss: 0.05752998101525009, Final Batch Loss: 0.02560042403638363\n",
      "Epoch 1395, Loss: 0.048575861379504204, Final Batch Loss: 0.0075980294495821\n",
      "Epoch 1396, Loss: 0.05848265276290476, Final Batch Loss: 0.011643357574939728\n",
      "Epoch 1397, Loss: 0.031230726424837485, Final Batch Loss: 0.015124284662306309\n",
      "Epoch 1398, Loss: 0.07052764017134905, Final Batch Loss: 0.031109368428587914\n",
      "Epoch 1399, Loss: 0.03493509558029473, Final Batch Loss: 0.020956121385097504\n",
      "Epoch 1400, Loss: 0.03712763520888984, Final Batch Loss: 0.0031647200230509043\n",
      "Epoch 1401, Loss: 0.034854851895943284, Final Batch Loss: 0.007790763396769762\n",
      "Epoch 1402, Loss: 0.08648788649588823, Final Batch Loss: 0.002794997300952673\n",
      "Epoch 1403, Loss: 0.028555622091516852, Final Batch Loss: 0.006314999423921108\n",
      "Epoch 1404, Loss: 0.057995976181700826, Final Batch Loss: 0.03080957941710949\n",
      "Epoch 1405, Loss: 0.04420543531887233, Final Batch Loss: 0.0039022762794047594\n",
      "Epoch 1406, Loss: 0.04075677553191781, Final Batch Loss: 0.011852147988975048\n",
      "Epoch 1407, Loss: 0.02896611567120999, Final Batch Loss: 0.0055449483916163445\n",
      "Epoch 1408, Loss: 0.0720773774664849, Final Batch Loss: 0.020870288833975792\n",
      "Epoch 1409, Loss: 0.051956961397081614, Final Batch Loss: 0.004355554934591055\n",
      "Epoch 1410, Loss: 0.04636186105199158, Final Batch Loss: 0.0022723570000380278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1411, Loss: 0.037445029825903475, Final Batch Loss: 0.0013127840356901288\n",
      "Epoch 1412, Loss: 0.037634748383425176, Final Batch Loss: 0.003960158675909042\n",
      "Epoch 1413, Loss: 0.06235763290897012, Final Batch Loss: 0.0297282375395298\n",
      "Epoch 1414, Loss: 0.05110985296778381, Final Batch Loss: 0.0056632570922374725\n",
      "Epoch 1415, Loss: 0.06676900282036513, Final Batch Loss: 0.03549263998866081\n",
      "Epoch 1416, Loss: 0.04038930870592594, Final Batch Loss: 0.005951994564384222\n",
      "Epoch 1417, Loss: 0.07270788191817701, Final Batch Loss: 0.004162788391113281\n",
      "Epoch 1418, Loss: 0.05155308754183352, Final Batch Loss: 0.0006742691621184349\n",
      "Epoch 1419, Loss: 0.038423081394284964, Final Batch Loss: 0.017777573317289352\n",
      "Epoch 1420, Loss: 0.02502202958567068, Final Batch Loss: 0.005009938962757587\n",
      "Epoch 1421, Loss: 0.09321931330487132, Final Batch Loss: 0.03166797757148743\n",
      "Epoch 1422, Loss: 0.0662246816791594, Final Batch Loss: 0.017653748393058777\n",
      "Epoch 1423, Loss: 0.10630060359835625, Final Batch Loss: 0.05596935749053955\n",
      "Epoch 1424, Loss: 0.043782024877145886, Final Batch Loss: 0.01585356891155243\n",
      "Epoch 1425, Loss: 0.06031761923804879, Final Batch Loss: 0.012062127701938152\n",
      "Epoch 1426, Loss: 0.06918727699667215, Final Batch Loss: 0.04433126002550125\n",
      "Epoch 1427, Loss: 0.07373028958681971, Final Batch Loss: 0.001011503511108458\n",
      "Epoch 1428, Loss: 0.055195942521095276, Final Batch Loss: 0.022661784663796425\n",
      "Epoch 1429, Loss: 0.06622860720381141, Final Batch Loss: 0.035841699689626694\n",
      "Epoch 1430, Loss: 0.07672802451997995, Final Batch Loss: 0.01922938972711563\n",
      "Epoch 1431, Loss: 0.05939668579958379, Final Batch Loss: 0.027098888531327248\n",
      "Epoch 1432, Loss: 0.05086285690777004, Final Batch Loss: 0.008268809877336025\n",
      "Epoch 1433, Loss: 0.04291497031226754, Final Batch Loss: 0.0058022732846438885\n",
      "Epoch 1434, Loss: 0.05286374827846885, Final Batch Loss: 0.017057567834854126\n",
      "Epoch 1435, Loss: 0.05527235986664891, Final Batch Loss: 0.002778192749246955\n",
      "Epoch 1436, Loss: 0.03749824967235327, Final Batch Loss: 0.009710320271551609\n",
      "Epoch 1437, Loss: 0.07024212670512497, Final Batch Loss: 0.0008400676306337118\n",
      "Epoch 1438, Loss: 0.0637782453559339, Final Batch Loss: 0.011197904124855995\n",
      "Epoch 1439, Loss: 0.04353698494378477, Final Batch Loss: 0.022892383858561516\n",
      "Epoch 1440, Loss: 0.04801784548908472, Final Batch Loss: 0.006470519583672285\n",
      "Epoch 1441, Loss: 0.03633323579560965, Final Batch Loss: 0.0009642989607527852\n",
      "Epoch 1442, Loss: 0.0606650672852993, Final Batch Loss: 0.003962887451052666\n",
      "Epoch 1443, Loss: 0.03673729614820331, Final Batch Loss: 0.0018166628433391452\n",
      "Epoch 1444, Loss: 0.030698993941769004, Final Batch Loss: 0.004070018418133259\n",
      "Epoch 1445, Loss: 0.05049065826460719, Final Batch Loss: 0.007418781518936157\n",
      "Epoch 1446, Loss: 0.07446736481506377, Final Batch Loss: 0.0017592773074284196\n",
      "Epoch 1447, Loss: 0.03069369838340208, Final Batch Loss: 0.006556730717420578\n",
      "Epoch 1448, Loss: 0.041808909038081765, Final Batch Loss: 0.008835398592054844\n",
      "Epoch 1449, Loss: 0.023960181686561555, Final Batch Loss: 0.0009078415459953249\n",
      "Epoch 1450, Loss: 0.04006871802266687, Final Batch Loss: 0.02073797397315502\n",
      "Epoch 1451, Loss: 0.04046440217643976, Final Batch Loss: 0.023404546082019806\n",
      "Epoch 1452, Loss: 0.036398582393303514, Final Batch Loss: 0.002741920528933406\n",
      "Epoch 1453, Loss: 0.049107386730611324, Final Batch Loss: 0.007143679074943066\n",
      "Epoch 1454, Loss: 0.04505587136372924, Final Batch Loss: 0.008062286302447319\n",
      "Epoch 1455, Loss: 0.04564746282994747, Final Batch Loss: 0.014060664921998978\n",
      "Epoch 1456, Loss: 0.04577594617148861, Final Batch Loss: 0.0013042913051322103\n",
      "Epoch 1457, Loss: 0.04559898190200329, Final Batch Loss: 0.014561977237462997\n",
      "Epoch 1458, Loss: 0.03131156449671835, Final Batch Loss: 0.0028987748082727194\n",
      "Epoch 1459, Loss: 0.036675744398962706, Final Batch Loss: 0.0006408912013284862\n",
      "Epoch 1460, Loss: 0.03487974498420954, Final Batch Loss: 0.006662443280220032\n",
      "Epoch 1461, Loss: 0.031042916874866933, Final Batch Loss: 0.007621458265930414\n",
      "Epoch 1462, Loss: 0.07216186891309917, Final Batch Loss: 0.03350304067134857\n",
      "Epoch 1463, Loss: 0.02336973766796291, Final Batch Loss: 0.002367576351389289\n",
      "Epoch 1464, Loss: 0.05417293682694435, Final Batch Loss: 0.014723330736160278\n",
      "Epoch 1465, Loss: 0.029857954126782715, Final Batch Loss: 0.004077737685292959\n",
      "Epoch 1466, Loss: 0.034975323476828635, Final Batch Loss: 0.0014960506232455373\n",
      "Epoch 1467, Loss: 0.045249931688886136, Final Batch Loss: 0.022791706025600433\n",
      "Epoch 1468, Loss: 0.032220823457464576, Final Batch Loss: 0.006954680662602186\n",
      "Epoch 1469, Loss: 0.039030855521559715, Final Batch Loss: 0.014546866528689861\n",
      "Epoch 1470, Loss: 0.04726561438292265, Final Batch Loss: 0.004087759647518396\n",
      "Epoch 1471, Loss: 0.08950424171052873, Final Batch Loss: 0.03807776793837547\n",
      "Epoch 1472, Loss: 0.03975700796581805, Final Batch Loss: 0.008519945666193962\n",
      "Epoch 1473, Loss: 0.0407408366445452, Final Batch Loss: 0.002132631139829755\n",
      "Epoch 1474, Loss: 0.042751703877002, Final Batch Loss: 0.016622118651866913\n",
      "Epoch 1475, Loss: 0.03720765153411776, Final Batch Loss: 0.0064894878305494785\n",
      "Epoch 1476, Loss: 0.052704336965689436, Final Batch Loss: 0.00032013506279326975\n",
      "Epoch 1477, Loss: 0.04323605168610811, Final Batch Loss: 0.011111162602901459\n",
      "Epoch 1478, Loss: 0.03248135425383225, Final Batch Loss: 0.0007479568594135344\n",
      "Epoch 1479, Loss: 0.06952374801039696, Final Batch Loss: 0.03215112164616585\n",
      "Epoch 1480, Loss: 0.03779824380762875, Final Batch Loss: 0.007498978637158871\n",
      "Epoch 1481, Loss: 0.05974839138798416, Final Batch Loss: 0.007654356770217419\n",
      "Epoch 1482, Loss: 0.03572347706358414, Final Batch Loss: 0.0002433605695841834\n",
      "Epoch 1483, Loss: 0.058101608883589506, Final Batch Loss: 0.026962652802467346\n",
      "Epoch 1484, Loss: 0.08115132083185017, Final Batch Loss: 0.027597462758421898\n",
      "Epoch 1485, Loss: 0.051561747677624226, Final Batch Loss: 0.0022021715994924307\n",
      "Epoch 1486, Loss: 0.030978742288425565, Final Batch Loss: 0.01719593070447445\n",
      "Epoch 1487, Loss: 0.046154170762747526, Final Batch Loss: 0.004766663536429405\n",
      "Epoch 1488, Loss: 0.040773664019070566, Final Batch Loss: 0.017934734001755714\n",
      "Epoch 1489, Loss: 0.04762831919651944, Final Batch Loss: 0.011891096830368042\n",
      "Epoch 1490, Loss: 0.06194677297025919, Final Batch Loss: 0.017036281526088715\n",
      "Epoch 1491, Loss: 0.04872619779780507, Final Batch Loss: 0.004056882578879595\n",
      "Epoch 1492, Loss: 0.028872316470369697, Final Batch Loss: 0.0023374322336167097\n",
      "Epoch 1493, Loss: 0.05800647963769734, Final Batch Loss: 0.002442504046484828\n",
      "Epoch 1494, Loss: 0.03683618258219212, Final Batch Loss: 0.0006686876295134425\n",
      "Epoch 1495, Loss: 0.045871983165852726, Final Batch Loss: 0.020699476823210716\n",
      "Epoch 1496, Loss: 0.043254418298602104, Final Batch Loss: 0.013766941614449024\n",
      "Epoch 1497, Loss: 0.03626189171336591, Final Batch Loss: 0.002333340235054493\n",
      "Epoch 1498, Loss: 0.04163974500261247, Final Batch Loss: 0.009119401685893536\n",
      "Epoch 1499, Loss: 0.06162463501095772, Final Batch Loss: 0.012328972108662128\n",
      "Epoch 1500, Loss: 0.060637939255684614, Final Batch Loss: 0.013378557749092579\n",
      "Epoch 1501, Loss: 0.03210002765990794, Final Batch Loss: 0.0030717605259269476\n",
      "Epoch 1502, Loss: 0.04668935853987932, Final Batch Loss: 0.002204792108386755\n",
      "Epoch 1503, Loss: 0.06452603591606021, Final Batch Loss: 0.02002640627324581\n",
      "Epoch 1504, Loss: 0.042061503045260906, Final Batch Loss: 0.009879433549940586\n",
      "Epoch 1505, Loss: 0.06546640302985907, Final Batch Loss: 0.023474955931305885\n",
      "Epoch 1506, Loss: 0.06520982540678233, Final Batch Loss: 0.043109290301799774\n",
      "Epoch 1507, Loss: 0.06791481841355562, Final Batch Loss: 0.035224344581365585\n",
      "Epoch 1508, Loss: 0.047922279220074415, Final Batch Loss: 0.022697506472468376\n",
      "Epoch 1509, Loss: 0.06201458105351776, Final Batch Loss: 0.011354501359164715\n",
      "Epoch 1510, Loss: 0.037482612766325474, Final Batch Loss: 0.005788643844425678\n",
      "Epoch 1511, Loss: 0.05486653512343764, Final Batch Loss: 0.009408877231180668\n",
      "Epoch 1512, Loss: 0.049046983011066914, Final Batch Loss: 0.015499688684940338\n",
      "Epoch 1513, Loss: 0.027542026713490486, Final Batch Loss: 0.002958238823339343\n",
      "Epoch 1514, Loss: 0.03739829221740365, Final Batch Loss: 0.016765903681516647\n",
      "Epoch 1515, Loss: 0.03951506037265062, Final Batch Loss: 0.010659344494342804\n",
      "Epoch 1516, Loss: 0.06938022992108017, Final Batch Loss: 0.0016794392140582204\n",
      "Epoch 1517, Loss: 0.06128745945170522, Final Batch Loss: 0.0203829575330019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1518, Loss: 0.06700350530445576, Final Batch Loss: 0.00969091709703207\n",
      "Epoch 1519, Loss: 0.07003974937833846, Final Batch Loss: 0.04488431662321091\n",
      "Epoch 1520, Loss: 0.03674816561397165, Final Batch Loss: 0.014894186519086361\n",
      "Epoch 1521, Loss: 0.06953722797334194, Final Batch Loss: 0.004983206279575825\n",
      "Epoch 1522, Loss: 0.08150453213602304, Final Batch Loss: 0.018484143540263176\n",
      "Epoch 1523, Loss: 0.0365549698472023, Final Batch Loss: 0.006453922484070063\n",
      "Epoch 1524, Loss: 0.04589559440501034, Final Batch Loss: 0.006327020935714245\n",
      "Epoch 1525, Loss: 0.03035474952775985, Final Batch Loss: 0.008708242326974869\n",
      "Epoch 1526, Loss: 0.03305485687451437, Final Batch Loss: 0.0008270962280221283\n",
      "Epoch 1527, Loss: 0.05106943193823099, Final Batch Loss: 0.0065856678411364555\n",
      "Epoch 1528, Loss: 0.0643245279788971, Final Batch Loss: 0.010668288916349411\n",
      "Epoch 1529, Loss: 0.058079057023860514, Final Batch Loss: 0.0013431297848001122\n",
      "Epoch 1530, Loss: 0.07837997609749436, Final Batch Loss: 0.03152875229716301\n",
      "Epoch 1531, Loss: 0.060491977259516716, Final Batch Loss: 0.02755054086446762\n",
      "Epoch 1532, Loss: 0.029599913861602545, Final Batch Loss: 0.01102631539106369\n",
      "Epoch 1533, Loss: 0.037425627757329494, Final Batch Loss: 0.0063453842885792255\n",
      "Epoch 1534, Loss: 0.044381521991454065, Final Batch Loss: 0.015966208651661873\n",
      "Epoch 1535, Loss: 0.055710549000650644, Final Batch Loss: 0.007611264009028673\n",
      "Epoch 1536, Loss: 0.04057101113721728, Final Batch Loss: 0.004945240914821625\n",
      "Epoch 1537, Loss: 0.03491649148054421, Final Batch Loss: 0.004671785980463028\n",
      "Epoch 1538, Loss: 0.038143029320053756, Final Batch Loss: 0.013411223888397217\n",
      "Epoch 1539, Loss: 0.06468753423541784, Final Batch Loss: 0.0025261808186769485\n",
      "Epoch 1540, Loss: 0.03774801269173622, Final Batch Loss: 0.003241848200559616\n",
      "Epoch 1541, Loss: 0.058814266696572304, Final Batch Loss: 0.008905020542442799\n",
      "Epoch 1542, Loss: 0.03229577196179889, Final Batch Loss: 0.00566361378878355\n",
      "Epoch 1543, Loss: 0.03082407273177523, Final Batch Loss: 0.006881615146994591\n",
      "Epoch 1544, Loss: 0.024322657380253077, Final Batch Loss: 0.0019714203663170338\n",
      "Epoch 1545, Loss: 0.07892482448369265, Final Batch Loss: 0.0187162347137928\n",
      "Epoch 1546, Loss: 0.027491248911246657, Final Batch Loss: 0.0029822473879903555\n",
      "Epoch 1547, Loss: 0.03875385271385312, Final Batch Loss: 0.007287043612450361\n",
      "Epoch 1548, Loss: 0.02792585373390466, Final Batch Loss: 0.0011363670928403735\n",
      "Epoch 1549, Loss: 0.04047979414463043, Final Batch Loss: 0.00427157711237669\n",
      "Epoch 1550, Loss: 0.030835738754831254, Final Batch Loss: 0.003682951210066676\n",
      "Epoch 1551, Loss: 0.09190546971512958, Final Batch Loss: 0.047316744923591614\n",
      "Epoch 1552, Loss: 0.03490723064169288, Final Batch Loss: 0.02384261228144169\n",
      "Epoch 1553, Loss: 0.04537853493820876, Final Batch Loss: 0.010385462082922459\n",
      "Epoch 1554, Loss: 0.0309552070684731, Final Batch Loss: 0.005029008258134127\n",
      "Epoch 1555, Loss: 0.042017874075099826, Final Batch Loss: 0.0039314329624176025\n",
      "Epoch 1556, Loss: 0.04171500541269779, Final Batch Loss: 0.00199745106510818\n",
      "Epoch 1557, Loss: 0.03495509643107653, Final Batch Loss: 0.012629305943846703\n",
      "Epoch 1558, Loss: 0.040826765121892095, Final Batch Loss: 0.0020732751581817865\n",
      "Epoch 1559, Loss: 0.0588993348646909, Final Batch Loss: 0.0035284922923892736\n",
      "Epoch 1560, Loss: 0.024090633844025433, Final Batch Loss: 0.006982028018683195\n",
      "Epoch 1561, Loss: 0.07163196452893317, Final Batch Loss: 0.027341341599822044\n",
      "Epoch 1562, Loss: 0.05171380715910345, Final Batch Loss: 0.024714648723602295\n",
      "Epoch 1563, Loss: 0.09308271668851376, Final Batch Loss: 0.07248584181070328\n",
      "Epoch 1564, Loss: 0.05999495950527489, Final Batch Loss: 0.002208353253081441\n",
      "Epoch 1565, Loss: 0.03344671311788261, Final Batch Loss: 0.0003212436567991972\n",
      "Epoch 1566, Loss: 0.07373342709615827, Final Batch Loss: 0.042634982615709305\n",
      "Epoch 1567, Loss: 0.06671711150556803, Final Batch Loss: 0.027702653780579567\n",
      "Epoch 1568, Loss: 0.04421977116726339, Final Batch Loss: 0.0034535119775682688\n",
      "Epoch 1569, Loss: 0.07167467288672924, Final Batch Loss: 0.025892557576298714\n",
      "Epoch 1570, Loss: 0.06992858741432428, Final Batch Loss: 0.028861768543720245\n",
      "Epoch 1571, Loss: 0.06917497050017118, Final Batch Loss: 0.01870383322238922\n",
      "Epoch 1572, Loss: 0.04951164685189724, Final Batch Loss: 0.020188044756650925\n",
      "Epoch 1573, Loss: 0.028480679029598832, Final Batch Loss: 0.014181315898895264\n",
      "Epoch 1574, Loss: 0.031071411474840716, Final Batch Loss: 0.0004302207671571523\n",
      "Epoch 1575, Loss: 0.04308805405162275, Final Batch Loss: 0.019071176648139954\n",
      "Epoch 1576, Loss: 0.05291055561974645, Final Batch Loss: 0.01612328179180622\n",
      "Epoch 1577, Loss: 0.04183137230575085, Final Batch Loss: 0.00852359738200903\n",
      "Epoch 1578, Loss: 0.0493240401847288, Final Batch Loss: 0.0007244310108944774\n",
      "Epoch 1579, Loss: 0.047949553932994604, Final Batch Loss: 0.012002642266452312\n",
      "Epoch 1580, Loss: 0.0516752596013248, Final Batch Loss: 0.022432997822761536\n",
      "Epoch 1581, Loss: 0.058910060906782746, Final Batch Loss: 0.01002701185643673\n",
      "Epoch 1582, Loss: 0.045230976305902004, Final Batch Loss: 0.01268458366394043\n",
      "Epoch 1583, Loss: 0.06868785084225237, Final Batch Loss: 0.020168263465166092\n",
      "Epoch 1584, Loss: 0.045816716738045216, Final Batch Loss: 0.0015862226719036698\n",
      "Epoch 1585, Loss: 0.04647319228388369, Final Batch Loss: 0.029189856722950935\n",
      "Epoch 1586, Loss: 0.029736007563769817, Final Batch Loss: 0.0037521645426750183\n",
      "Epoch 1587, Loss: 0.038123965030536056, Final Batch Loss: 0.0059880586341023445\n",
      "Epoch 1588, Loss: 0.026569962152279913, Final Batch Loss: 0.008309916593134403\n",
      "Epoch 1589, Loss: 0.04448066430632025, Final Batch Loss: 0.01217004656791687\n",
      "Epoch 1590, Loss: 0.041619706084020436, Final Batch Loss: 0.012456078082323074\n",
      "Epoch 1591, Loss: 0.030290941358543932, Final Batch Loss: 0.006281060632318258\n",
      "Epoch 1592, Loss: 0.04291402501985431, Final Batch Loss: 0.005263481754809618\n",
      "Epoch 1593, Loss: 0.07135999947786331, Final Batch Loss: 0.03827008232474327\n",
      "Epoch 1594, Loss: 0.02753079673857428, Final Batch Loss: 0.00034089191467501223\n",
      "Epoch 1595, Loss: 0.061784553807228804, Final Batch Loss: 0.007014740724116564\n",
      "Epoch 1596, Loss: 0.040245432406663895, Final Batch Loss: 0.015108861960470676\n",
      "Epoch 1597, Loss: 0.03466127463616431, Final Batch Loss: 0.014371130615472794\n",
      "Epoch 1598, Loss: 0.0255442108027637, Final Batch Loss: 0.0013872740091755986\n",
      "Epoch 1599, Loss: 0.0355080432491377, Final Batch Loss: 0.0038943833205848932\n",
      "Epoch 1600, Loss: 0.03055268235038966, Final Batch Loss: 0.004694007337093353\n",
      "Epoch 1601, Loss: 0.05083293095231056, Final Batch Loss: 0.015566576272249222\n",
      "Epoch 1602, Loss: 0.02358223605551757, Final Batch Loss: 0.00040488518425263464\n",
      "Epoch 1603, Loss: 0.0664642674382776, Final Batch Loss: 0.002128402004018426\n",
      "Epoch 1604, Loss: 0.05645588063634932, Final Batch Loss: 0.024968275800347328\n",
      "Epoch 1605, Loss: 0.03940464771585539, Final Batch Loss: 0.010442185215651989\n",
      "Epoch 1606, Loss: 0.03191750403493643, Final Batch Loss: 0.018821056932210922\n",
      "Epoch 1607, Loss: 0.05283671757206321, Final Batch Loss: 0.020227210596203804\n",
      "Epoch 1608, Loss: 0.034587758884299546, Final Batch Loss: 0.0023092059418559074\n",
      "Epoch 1609, Loss: 0.02335834677796811, Final Batch Loss: 0.0017438321374356747\n",
      "Epoch 1610, Loss: 0.036360594094730914, Final Batch Loss: 0.012349792756140232\n",
      "Epoch 1611, Loss: 0.044683909334708005, Final Batch Loss: 0.0029542415868490934\n",
      "Epoch 1612, Loss: 0.023828908102586865, Final Batch Loss: 0.007889476604759693\n",
      "Epoch 1613, Loss: 0.020861759898252785, Final Batch Loss: 0.0031179790385067463\n",
      "Epoch 1614, Loss: 0.026533749420195818, Final Batch Loss: 0.003927712794393301\n",
      "Epoch 1615, Loss: 0.04021417023614049, Final Batch Loss: 0.01763729564845562\n",
      "Epoch 1616, Loss: 0.02680784626863897, Final Batch Loss: 0.003701845183968544\n",
      "Epoch 1617, Loss: 0.025041838875040412, Final Batch Loss: 0.0014687429647892714\n",
      "Epoch 1618, Loss: 0.03348428732715547, Final Batch Loss: 0.017069093883037567\n",
      "Epoch 1619, Loss: 0.025103668798692524, Final Batch Loss: 0.0089411037042737\n",
      "Epoch 1620, Loss: 0.03319807176012546, Final Batch Loss: 0.0028633468318730593\n",
      "Epoch 1621, Loss: 0.02313861856237054, Final Batch Loss: 0.001672346144914627\n",
      "Epoch 1622, Loss: 0.035514136077836156, Final Batch Loss: 0.005119442939758301\n",
      "Epoch 1623, Loss: 0.039418799336999655, Final Batch Loss: 0.011011525057256222\n",
      "Epoch 1624, Loss: 0.031850640662014484, Final Batch Loss: 0.01277038175612688\n",
      "Epoch 1625, Loss: 0.025590389035642147, Final Batch Loss: 0.006506357342004776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1626, Loss: 0.026489524345379323, Final Batch Loss: 0.0018816200317814946\n",
      "Epoch 1627, Loss: 0.03528987045865506, Final Batch Loss: 0.0012938875006511807\n",
      "Epoch 1628, Loss: 0.032375378999859095, Final Batch Loss: 0.0065437364391982555\n",
      "Epoch 1629, Loss: 0.03125651052687317, Final Batch Loss: 0.0015126714715734124\n",
      "Epoch 1630, Loss: 0.04915892763528973, Final Batch Loss: 0.001065314863808453\n",
      "Epoch 1631, Loss: 0.029908556229202077, Final Batch Loss: 0.00041391068953089416\n",
      "Epoch 1632, Loss: 0.04327094496693462, Final Batch Loss: 0.018124522641301155\n",
      "Epoch 1633, Loss: 0.03986648377031088, Final Batch Loss: 0.0033392570912837982\n",
      "Epoch 1634, Loss: 0.054591999331023544, Final Batch Loss: 0.0008766846149228513\n",
      "Epoch 1635, Loss: 0.03109022759599611, Final Batch Loss: 0.0005827219574712217\n",
      "Epoch 1636, Loss: 0.030467181466519833, Final Batch Loss: 0.0019199708476662636\n",
      "Epoch 1637, Loss: 0.027883586008101702, Final Batch Loss: 0.008801452815532684\n",
      "Epoch 1638, Loss: 0.024700670153833926, Final Batch Loss: 0.0012453729286789894\n",
      "Epoch 1639, Loss: 0.029965141555294394, Final Batch Loss: 0.012307231314480305\n",
      "Epoch 1640, Loss: 0.040154737303964794, Final Batch Loss: 0.0010048332624137402\n",
      "Epoch 1641, Loss: 0.04182253126055002, Final Batch Loss: 0.016238724812865257\n",
      "Epoch 1642, Loss: 0.04102583430358209, Final Batch Loss: 0.0004143236728850752\n",
      "Epoch 1643, Loss: 0.05153517331928015, Final Batch Loss: 0.006077180150896311\n",
      "Epoch 1644, Loss: 0.02074048580834642, Final Batch Loss: 0.007174435071647167\n",
      "Epoch 1645, Loss: 0.05014496319927275, Final Batch Loss: 0.020352419465780258\n",
      "Epoch 1646, Loss: 0.05730105843394995, Final Batch Loss: 0.00821683555841446\n",
      "Epoch 1647, Loss: 0.04289183160290122, Final Batch Loss: 0.014812824316322803\n",
      "Epoch 1648, Loss: 0.040707382606342435, Final Batch Loss: 0.0011828651186078787\n",
      "Epoch 1649, Loss: 0.04784772219136357, Final Batch Loss: 0.005362827796489\n",
      "Epoch 1650, Loss: 0.09294395812321454, Final Batch Loss: 0.06627602130174637\n",
      "Epoch 1651, Loss: 0.053618295409251004, Final Batch Loss: 0.0070478906854987144\n",
      "Epoch 1652, Loss: 0.04981587268412113, Final Batch Loss: 0.029298309236764908\n",
      "Epoch 1653, Loss: 0.03691869997419417, Final Batch Loss: 0.005815277807414532\n",
      "Epoch 1654, Loss: 0.06569344829767942, Final Batch Loss: 0.040580883622169495\n",
      "Epoch 1655, Loss: 0.04656732967123389, Final Batch Loss: 0.0027216041926294565\n",
      "Epoch 1656, Loss: 0.027496760012581944, Final Batch Loss: 0.004936561919748783\n",
      "Epoch 1657, Loss: 0.0707251998828724, Final Batch Loss: 0.02324490062892437\n",
      "Epoch 1658, Loss: 0.03139318618923426, Final Batch Loss: 0.0034788218326866627\n",
      "Epoch 1659, Loss: 0.02476027316879481, Final Batch Loss: 0.0026226977352052927\n",
      "Epoch 1660, Loss: 0.029105114692356437, Final Batch Loss: 0.000740561808925122\n",
      "Epoch 1661, Loss: 0.05104703875258565, Final Batch Loss: 0.01604052446782589\n",
      "Epoch 1662, Loss: 0.09380878042429686, Final Batch Loss: 0.026111137121915817\n",
      "Epoch 1663, Loss: 0.024799422943033278, Final Batch Loss: 0.002572365803644061\n",
      "Epoch 1664, Loss: 0.016866899910382926, Final Batch Loss: 0.003761130850762129\n",
      "Epoch 1665, Loss: 0.0735634338343516, Final Batch Loss: 0.02082766406238079\n",
      "Epoch 1666, Loss: 0.034166114521212876, Final Batch Loss: 0.0008987976470962167\n",
      "Epoch 1667, Loss: 0.033322792034596205, Final Batch Loss: 0.00116012804210186\n",
      "Epoch 1668, Loss: 0.04718295997008681, Final Batch Loss: 0.006627415772527456\n",
      "Epoch 1669, Loss: 0.020194058190099895, Final Batch Loss: 0.0011659822193905711\n",
      "Epoch 1670, Loss: 0.029611024539917707, Final Batch Loss: 0.01849844492971897\n",
      "Epoch 1671, Loss: 0.06469105987343937, Final Batch Loss: 0.049207016825675964\n",
      "Epoch 1672, Loss: 0.03896464570425451, Final Batch Loss: 0.0019557520281523466\n",
      "Epoch 1673, Loss: 0.04827558691613376, Final Batch Loss: 0.011923964135348797\n",
      "Epoch 1674, Loss: 0.032998602953739464, Final Batch Loss: 0.007451716810464859\n",
      "Epoch 1675, Loss: 0.03761884989216924, Final Batch Loss: 0.01415472012013197\n",
      "Epoch 1676, Loss: 0.028646826511248946, Final Batch Loss: 0.0017459808150306344\n",
      "Epoch 1677, Loss: 0.03387558460235596, Final Batch Loss: 0.0068369535729289055\n",
      "Epoch 1678, Loss: 0.05029202741570771, Final Batch Loss: 0.013721062801778316\n",
      "Epoch 1679, Loss: 0.07009688549442217, Final Batch Loss: 0.035135287791490555\n",
      "Epoch 1680, Loss: 0.03716246038675308, Final Batch Loss: 0.009388738311827183\n",
      "Epoch 1681, Loss: 0.03992045356426388, Final Batch Loss: 0.015299302525818348\n",
      "Epoch 1682, Loss: 0.035117484629154205, Final Batch Loss: 0.0051345969550311565\n",
      "Epoch 1683, Loss: 0.04816200805362314, Final Batch Loss: 0.008063561283051968\n",
      "Epoch 1684, Loss: 0.05107451934600249, Final Batch Loss: 0.0005327523103915155\n",
      "Epoch 1685, Loss: 0.05837277485989034, Final Batch Loss: 0.01957813650369644\n",
      "Epoch 1686, Loss: 0.019667996908538043, Final Batch Loss: 0.0022087099496275187\n",
      "Epoch 1687, Loss: 0.04572445654775947, Final Batch Loss: 0.00991045217961073\n",
      "Epoch 1688, Loss: 0.04594409395940602, Final Batch Loss: 0.016392704099416733\n",
      "Epoch 1689, Loss: 0.026173614780418575, Final Batch Loss: 0.001875561662018299\n",
      "Epoch 1690, Loss: 0.05342483147978783, Final Batch Loss: 0.006162692792713642\n",
      "Epoch 1691, Loss: 0.03203587792813778, Final Batch Loss: 0.0070945327170193195\n",
      "Epoch 1692, Loss: 0.03887909511104226, Final Batch Loss: 0.005750688724219799\n",
      "Epoch 1693, Loss: 0.06841299869120121, Final Batch Loss: 0.0209269430488348\n",
      "Epoch 1694, Loss: 0.03891533042769879, Final Batch Loss: 0.0011659605661407113\n",
      "Epoch 1695, Loss: 0.03660526033490896, Final Batch Loss: 0.006714753340929747\n",
      "Epoch 1696, Loss: 0.03279149695299566, Final Batch Loss: 0.01316132303327322\n",
      "Epoch 1697, Loss: 0.05355681828223169, Final Batch Loss: 0.014143450185656548\n",
      "Epoch 1698, Loss: 0.019529554119799286, Final Batch Loss: 0.00041340640746057034\n",
      "Epoch 1699, Loss: 0.05097004002891481, Final Batch Loss: 0.024669505655765533\n",
      "Epoch 1700, Loss: 0.04061019455548376, Final Batch Loss: 0.006051006726920605\n",
      "Epoch 1701, Loss: 0.020099980232771486, Final Batch Loss: 0.0002461444237269461\n",
      "Epoch 1702, Loss: 0.04866557469358668, Final Batch Loss: 0.012291512452065945\n",
      "Epoch 1703, Loss: 0.025941679487004876, Final Batch Loss: 0.0012635200982913375\n",
      "Epoch 1704, Loss: 0.041736990271601826, Final Batch Loss: 0.018409371376037598\n",
      "Epoch 1705, Loss: 0.038998033036477864, Final Batch Loss: 0.007476877421140671\n",
      "Epoch 1706, Loss: 0.024024401558563113, Final Batch Loss: 0.0012157699093222618\n",
      "Epoch 1707, Loss: 0.02758514613378793, Final Batch Loss: 0.015235868282616138\n",
      "Epoch 1708, Loss: 0.02955257229041308, Final Batch Loss: 0.004802462179213762\n",
      "Epoch 1709, Loss: 0.017203391005750746, Final Batch Loss: 0.006418387871235609\n",
      "Epoch 1710, Loss: 0.08074055332690477, Final Batch Loss: 0.012642594054341316\n",
      "Epoch 1711, Loss: 0.020426064351340756, Final Batch Loss: 0.00045771346776746213\n",
      "Epoch 1712, Loss: 0.02040465921163559, Final Batch Loss: 0.007160149049013853\n",
      "Epoch 1713, Loss: 0.024136405671015382, Final Batch Loss: 0.0006680814549326897\n",
      "Epoch 1714, Loss: 0.02405710774473846, Final Batch Loss: 0.01090236846357584\n",
      "Epoch 1715, Loss: 0.019658160977996886, Final Batch Loss: 0.0011266787769272923\n",
      "Epoch 1716, Loss: 0.030135484877973795, Final Batch Loss: 0.0020191436633467674\n",
      "Epoch 1717, Loss: 0.038818719011032954, Final Batch Loss: 0.014400388114154339\n",
      "Epoch 1718, Loss: 0.03191083751153201, Final Batch Loss: 0.013880502432584763\n",
      "Epoch 1719, Loss: 0.05939200706779957, Final Batch Loss: 0.019495293498039246\n",
      "Epoch 1720, Loss: 0.02459942363202572, Final Batch Loss: 0.0018966456409543753\n",
      "Epoch 1721, Loss: 0.028941389406099916, Final Batch Loss: 0.0034560319036245346\n",
      "Epoch 1722, Loss: 0.01808774401433766, Final Batch Loss: 0.0018990514799952507\n",
      "Epoch 1723, Loss: 0.032855399171239696, Final Batch Loss: 0.0003896965063177049\n",
      "Epoch 1724, Loss: 0.03482321940828115, Final Batch Loss: 0.00100962002761662\n",
      "Epoch 1725, Loss: 0.04102609143592417, Final Batch Loss: 0.00508358608931303\n",
      "Epoch 1726, Loss: 0.05317493109032512, Final Batch Loss: 0.004482369404286146\n",
      "Epoch 1727, Loss: 0.02665049023926258, Final Batch Loss: 0.0069007850252091885\n",
      "Epoch 1728, Loss: 0.02996053989045322, Final Batch Loss: 0.005560355726629496\n",
      "Epoch 1729, Loss: 0.10926320508588105, Final Batch Loss: 0.08232848346233368\n",
      "Epoch 1730, Loss: 0.050776741001755, Final Batch Loss: 0.024419361725449562\n",
      "Epoch 1731, Loss: 0.03563703747931868, Final Batch Loss: 0.0024780193343758583\n",
      "Epoch 1732, Loss: 0.03238491533556953, Final Batch Loss: 0.0009613927104510367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1733, Loss: 0.05130195803940296, Final Batch Loss: 0.003992142155766487\n",
      "Epoch 1734, Loss: 0.034426157537382096, Final Batch Loss: 0.00979455467313528\n",
      "Epoch 1735, Loss: 0.03301847935654223, Final Batch Loss: 0.00184122403152287\n",
      "Epoch 1736, Loss: 0.11758965940680355, Final Batch Loss: 0.014015947468578815\n",
      "Epoch 1737, Loss: 0.032773055718280375, Final Batch Loss: 0.0010110625298693776\n",
      "Epoch 1738, Loss: 0.029804258374497294, Final Batch Loss: 0.00964172650128603\n",
      "Epoch 1739, Loss: 0.03140494448598474, Final Batch Loss: 0.002177800051867962\n",
      "Epoch 1740, Loss: 0.027376483660191298, Final Batch Loss: 0.0067182546481490135\n",
      "Epoch 1741, Loss: 0.06850781920365989, Final Batch Loss: 0.019311588257551193\n",
      "Epoch 1742, Loss: 0.025936554331565276, Final Batch Loss: 0.001734471064992249\n",
      "Epoch 1743, Loss: 0.022640826035058126, Final Batch Loss: 0.00037428803625516593\n",
      "Epoch 1744, Loss: 0.02988831780385226, Final Batch Loss: 0.0013664301950484514\n",
      "Epoch 1745, Loss: 0.0236185981775634, Final Batch Loss: 0.01063442975282669\n",
      "Epoch 1746, Loss: 0.03235029266215861, Final Batch Loss: 0.0007772769313305616\n",
      "Epoch 1747, Loss: 0.018147532071452588, Final Batch Loss: 0.006704788189381361\n",
      "Epoch 1748, Loss: 0.03125973942223936, Final Batch Loss: 0.010271798819303513\n",
      "Epoch 1749, Loss: 0.07152679655700922, Final Batch Loss: 0.014045354910194874\n",
      "Epoch 1750, Loss: 0.08593906194437295, Final Batch Loss: 0.0032831153366714716\n",
      "Epoch 1751, Loss: 0.03744186705444008, Final Batch Loss: 0.0017173722153529525\n",
      "Epoch 1752, Loss: 0.03218780888710171, Final Batch Loss: 0.0013171049067750573\n",
      "Epoch 1753, Loss: 0.05586492328438908, Final Batch Loss: 0.0011059209937229753\n",
      "Epoch 1754, Loss: 0.048461908707395196, Final Batch Loss: 0.0021131793037056923\n",
      "Epoch 1755, Loss: 0.035316453548148274, Final Batch Loss: 0.010195423848927021\n",
      "Epoch 1756, Loss: 0.0347723433515057, Final Batch Loss: 0.007213941775262356\n",
      "Epoch 1757, Loss: 0.0671660132938996, Final Batch Loss: 0.008121020160615444\n",
      "Epoch 1758, Loss: 0.024409469566307962, Final Batch Loss: 0.0015619377372786403\n",
      "Epoch 1759, Loss: 0.09604228241369128, Final Batch Loss: 0.00563778867945075\n",
      "Epoch 1760, Loss: 0.02389077501720749, Final Batch Loss: 0.0001442416978534311\n",
      "Epoch 1761, Loss: 0.0169588421122171, Final Batch Loss: 0.001194598968140781\n",
      "Epoch 1762, Loss: 0.037417571409605443, Final Batch Loss: 0.006361066363751888\n",
      "Epoch 1763, Loss: 0.03087288129609078, Final Batch Loss: 0.005730857606977224\n",
      "Epoch 1764, Loss: 0.03618856635876, Final Batch Loss: 0.014603547751903534\n",
      "Epoch 1765, Loss: 0.053931063041090965, Final Batch Loss: 0.0022403395269066095\n",
      "Epoch 1766, Loss: 0.037118286883924156, Final Batch Loss: 0.0016438720049336553\n",
      "Epoch 1767, Loss: 0.06540084793232381, Final Batch Loss: 0.017120812088251114\n",
      "Epoch 1768, Loss: 0.01825531155918725, Final Batch Loss: 0.00016126659465953708\n",
      "Epoch 1769, Loss: 0.018107117532053962, Final Batch Loss: 0.00031745570595376194\n",
      "Epoch 1770, Loss: 0.01611611491534859, Final Batch Loss: 0.0017589885974302888\n",
      "Epoch 1771, Loss: 0.020797278033569455, Final Batch Loss: 0.0017216221895068884\n",
      "Epoch 1772, Loss: 0.02219697821419686, Final Batch Loss: 0.010543560609221458\n",
      "Epoch 1773, Loss: 0.039318852592259645, Final Batch Loss: 0.008446265012025833\n",
      "Epoch 1774, Loss: 0.017387235500791576, Final Batch Loss: 0.00010632603516569361\n",
      "Epoch 1775, Loss: 0.04117579135345295, Final Batch Loss: 0.000851391872856766\n",
      "Epoch 1776, Loss: 0.022973099548835307, Final Batch Loss: 0.0005782754742540419\n",
      "Epoch 1777, Loss: 0.022409147233702242, Final Batch Loss: 0.0008648802759125829\n",
      "Epoch 1778, Loss: 0.020940414513461292, Final Batch Loss: 0.002004205249249935\n",
      "Epoch 1779, Loss: 0.028541877632960677, Final Batch Loss: 0.0029357634484767914\n",
      "Epoch 1780, Loss: 0.043947040336206555, Final Batch Loss: 0.0007725267205387354\n",
      "Epoch 1781, Loss: 0.03145242687605787, Final Batch Loss: 0.00021223137446213514\n",
      "Epoch 1782, Loss: 0.019843222835334018, Final Batch Loss: 0.0004682389844674617\n",
      "Epoch 1783, Loss: 0.039468883944209665, Final Batch Loss: 0.0007190885371528566\n",
      "Epoch 1784, Loss: 0.02797415084205568, Final Batch Loss: 0.002655307063832879\n",
      "Epoch 1785, Loss: 0.03954335063463077, Final Batch Loss: 0.010556874796748161\n",
      "Epoch 1786, Loss: 0.05935827596113086, Final Batch Loss: 0.003343656426295638\n",
      "Epoch 1787, Loss: 0.023552358616143465, Final Batch Loss: 0.001482035149820149\n",
      "Epoch 1788, Loss: 0.029617672320455313, Final Batch Loss: 0.0031003684271126986\n",
      "Epoch 1789, Loss: 0.027344925794750452, Final Batch Loss: 0.0008462893310934305\n",
      "Epoch 1790, Loss: 0.012457189266569912, Final Batch Loss: 0.001344489399343729\n",
      "Epoch 1791, Loss: 0.024696517852135003, Final Batch Loss: 0.0049496181309223175\n",
      "Epoch 1792, Loss: 0.02089911056100391, Final Batch Loss: 0.001844532904215157\n",
      "Epoch 1793, Loss: 0.01820309495087713, Final Batch Loss: 0.0029263030737638474\n",
      "Epoch 1794, Loss: 0.03188745747320354, Final Batch Loss: 0.004758035764098167\n",
      "Epoch 1795, Loss: 0.010994424053933471, Final Batch Loss: 0.0015142959309741855\n",
      "Epoch 1796, Loss: 0.03165723732672632, Final Batch Loss: 0.007988478988409042\n",
      "Epoch 1797, Loss: 0.021653722069459036, Final Batch Loss: 0.012422137893736362\n",
      "Epoch 1798, Loss: 0.021464499295689166, Final Batch Loss: 0.004923462867736816\n",
      "Epoch 1799, Loss: 0.018124165595509112, Final Batch Loss: 0.002593482844531536\n",
      "Epoch 1800, Loss: 0.04241457988973707, Final Batch Loss: 0.0046237194910645485\n",
      "Epoch 1801, Loss: 0.03755396243650466, Final Batch Loss: 0.0025842811446636915\n",
      "Epoch 1802, Loss: 0.035784433712251484, Final Batch Loss: 0.007838643155992031\n",
      "Epoch 1803, Loss: 0.058090282836928964, Final Batch Loss: 0.005255347117781639\n",
      "Epoch 1804, Loss: 0.02412479411577806, Final Batch Loss: 0.0014631381491199136\n",
      "Epoch 1805, Loss: 0.032568520240602084, Final Batch Loss: 0.00024108363140840083\n",
      "Epoch 1806, Loss: 0.04161279898835346, Final Batch Loss: 0.0006134655559435487\n",
      "Epoch 1807, Loss: 0.03923051757737994, Final Batch Loss: 0.0022333937231451273\n",
      "Epoch 1808, Loss: 0.05245053151156753, Final Batch Loss: 0.005303694400936365\n",
      "Epoch 1809, Loss: 0.04564915387891233, Final Batch Loss: 0.0015267801936715841\n",
      "Epoch 1810, Loss: 0.018707421753788367, Final Batch Loss: 0.00479787727817893\n",
      "Epoch 1811, Loss: 0.03574398616910912, Final Batch Loss: 0.02077474631369114\n",
      "Epoch 1812, Loss: 0.03628868446685374, Final Batch Loss: 0.0018104722257703543\n",
      "Epoch 1813, Loss: 0.02486710756784305, Final Batch Loss: 0.002294703619554639\n",
      "Epoch 1814, Loss: 0.046317441781866364, Final Batch Loss: 0.02957267127931118\n",
      "Epoch 1815, Loss: 0.052779541234485805, Final Batch Loss: 0.021409815177321434\n",
      "Epoch 1816, Loss: 0.030802302761003375, Final Batch Loss: 0.004624590743333101\n",
      "Epoch 1817, Loss: 0.022990924771875143, Final Batch Loss: 0.0003131832927465439\n",
      "Epoch 1818, Loss: 0.022120645153336227, Final Batch Loss: 0.002401004545390606\n",
      "Epoch 1819, Loss: 0.02373568224720657, Final Batch Loss: 0.004937916994094849\n",
      "Epoch 1820, Loss: 0.014052500104298815, Final Batch Loss: 0.0002646124630700797\n",
      "Epoch 1821, Loss: 0.021473631204571575, Final Batch Loss: 0.010748055763542652\n",
      "Epoch 1822, Loss: 0.03588408912764862, Final Batch Loss: 0.0006635990575887263\n",
      "Epoch 1823, Loss: 0.017007467686198652, Final Batch Loss: 0.0023458017967641354\n",
      "Epoch 1824, Loss: 0.08361534908181056, Final Batch Loss: 0.024178270250558853\n",
      "Epoch 1825, Loss: 0.023550785903353244, Final Batch Loss: 0.0027970534283667803\n",
      "Epoch 1826, Loss: 0.023699987854342908, Final Batch Loss: 0.0011456758948042989\n",
      "Epoch 1827, Loss: 0.020144305774010718, Final Batch Loss: 0.0014599806163460016\n",
      "Epoch 1828, Loss: 0.03876409400254488, Final Batch Loss: 0.015729358419775963\n",
      "Epoch 1829, Loss: 0.024885861028451473, Final Batch Loss: 0.000481344701256603\n",
      "Epoch 1830, Loss: 0.035889264196157455, Final Batch Loss: 0.005205994006246328\n",
      "Epoch 1831, Loss: 0.031378926534671336, Final Batch Loss: 0.0009056811686605215\n",
      "Epoch 1832, Loss: 0.07116252090781927, Final Batch Loss: 0.00771930068731308\n",
      "Epoch 1833, Loss: 0.04873003356624395, Final Batch Loss: 0.006374529097229242\n",
      "Epoch 1834, Loss: 0.021838052896782756, Final Batch Loss: 0.002352308016270399\n",
      "Epoch 1835, Loss: 0.026396787143312395, Final Batch Loss: 0.0028162680100649595\n",
      "Epoch 1836, Loss: 0.07733712933259085, Final Batch Loss: 0.011492772959172726\n",
      "Epoch 1837, Loss: 0.025893355021253228, Final Batch Loss: 0.0013589360751211643\n",
      "Epoch 1838, Loss: 0.015895141987130046, Final Batch Loss: 0.004112363327294588\n",
      "Epoch 1839, Loss: 0.01930200537026394, Final Batch Loss: 0.0001601221301825717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1840, Loss: 0.030602058628574014, Final Batch Loss: 0.013073445297777653\n",
      "Epoch 1841, Loss: 0.01912518683820963, Final Batch Loss: 0.0008361674263142049\n",
      "Epoch 1842, Loss: 0.03401002800092101, Final Batch Loss: 0.0022863014601171017\n",
      "Epoch 1843, Loss: 0.03579384693875909, Final Batch Loss: 0.000974499445874244\n",
      "Epoch 1844, Loss: 0.01785517216194421, Final Batch Loss: 0.0007995160995051265\n",
      "Epoch 1845, Loss: 0.052671650890260935, Final Batch Loss: 0.0072366599924862385\n",
      "Epoch 1846, Loss: 0.03232977737206966, Final Batch Loss: 0.0020236081909388304\n",
      "Epoch 1847, Loss: 0.051872706797439605, Final Batch Loss: 0.005185022950172424\n",
      "Epoch 1848, Loss: 0.03020876774098724, Final Batch Loss: 0.0037552977446466684\n",
      "Epoch 1849, Loss: 0.03531672351527959, Final Batch Loss: 0.003847150132060051\n",
      "Epoch 1850, Loss: 0.016731975076254457, Final Batch Loss: 0.0006765787838958204\n",
      "Epoch 1851, Loss: 0.017052604554919526, Final Batch Loss: 0.0004470920830499381\n",
      "Epoch 1852, Loss: 0.03517491987440735, Final Batch Loss: 0.011168494820594788\n",
      "Epoch 1853, Loss: 0.03783136763377115, Final Batch Loss: 0.00317214778624475\n",
      "Epoch 1854, Loss: 0.06004644325003028, Final Batch Loss: 0.04057804122567177\n",
      "Epoch 1855, Loss: 0.020427556708455086, Final Batch Loss: 0.004103794693946838\n",
      "Epoch 1856, Loss: 0.02259119600057602, Final Batch Loss: 0.004277496598660946\n",
      "Epoch 1857, Loss: 0.04949229140765965, Final Batch Loss: 0.007227956783026457\n",
      "Epoch 1858, Loss: 0.03922333405353129, Final Batch Loss: 0.019707433879375458\n",
      "Epoch 1859, Loss: 0.022473809134680778, Final Batch Loss: 0.013906723819673061\n",
      "Epoch 1860, Loss: 0.11084940773434937, Final Batch Loss: 0.026130778715014458\n",
      "Epoch 1861, Loss: 0.059796299232402816, Final Batch Loss: 0.00028565843240357935\n",
      "Epoch 1862, Loss: 0.02381226487341337, Final Batch Loss: 0.00036210889811627567\n",
      "Epoch 1863, Loss: 0.022720095701515675, Final Batch Loss: 0.0038960480596870184\n",
      "Epoch 1864, Loss: 0.017109422013163567, Final Batch Loss: 0.007786520756781101\n",
      "Epoch 1865, Loss: 0.046902278205379844, Final Batch Loss: 0.006835875101387501\n",
      "Epoch 1866, Loss: 0.04384662047959864, Final Batch Loss: 0.0016704178415238857\n",
      "Epoch 1867, Loss: 0.049681575736030936, Final Batch Loss: 0.006776889786124229\n",
      "Epoch 1868, Loss: 0.04096965165808797, Final Batch Loss: 0.013814720325171947\n",
      "Epoch 1869, Loss: 0.03356242831796408, Final Batch Loss: 0.00041396915912628174\n",
      "Epoch 1870, Loss: 0.019974632072262466, Final Batch Loss: 0.0014590647770091891\n",
      "Epoch 1871, Loss: 0.07590260420693085, Final Batch Loss: 0.005212504416704178\n",
      "Epoch 1872, Loss: 0.02022639033384621, Final Batch Loss: 0.0036249160766601562\n",
      "Epoch 1873, Loss: 0.11670128587866202, Final Batch Loss: 0.10219186544418335\n",
      "Epoch 1874, Loss: 0.055496583110652864, Final Batch Loss: 0.03914332762360573\n",
      "Epoch 1875, Loss: 0.03249484082334675, Final Batch Loss: 0.018758054822683334\n",
      "Epoch 1876, Loss: 0.051240423461422324, Final Batch Loss: 0.01181673351675272\n",
      "Epoch 1877, Loss: 0.05610221310053021, Final Batch Loss: 0.01961967721581459\n",
      "Epoch 1878, Loss: 0.04445273522287607, Final Batch Loss: 0.018724462017416954\n",
      "Epoch 1879, Loss: 0.039764637826010585, Final Batch Loss: 0.0025369178038090467\n",
      "Epoch 1880, Loss: 0.03234782419167459, Final Batch Loss: 0.0011664596386253834\n",
      "Epoch 1881, Loss: 0.01559104595798999, Final Batch Loss: 0.0011402335949242115\n",
      "Epoch 1882, Loss: 0.03172547300346196, Final Batch Loss: 0.00449005700647831\n",
      "Epoch 1883, Loss: 0.0548804672434926, Final Batch Loss: 0.01948622055351734\n",
      "Epoch 1884, Loss: 0.06194977668928914, Final Batch Loss: 0.0003834392118733376\n",
      "Epoch 1885, Loss: 0.016879238726687618, Final Batch Loss: 0.00011261606414336711\n",
      "Epoch 1886, Loss: 0.055304839537711814, Final Batch Loss: 0.006386991124600172\n",
      "Epoch 1887, Loss: 0.09948581416392699, Final Batch Loss: 0.008711033500730991\n",
      "Epoch 1888, Loss: 0.03590114993858151, Final Batch Loss: 0.00022559610079042614\n",
      "Epoch 1889, Loss: 0.020200190600007772, Final Batch Loss: 0.002514877123758197\n",
      "Epoch 1890, Loss: 0.017121198965469375, Final Batch Loss: 0.0004369387461338192\n",
      "Epoch 1891, Loss: 0.02883062488399446, Final Batch Loss: 0.013975768350064754\n",
      "Epoch 1892, Loss: 0.01373277889797464, Final Batch Loss: 0.0015928487991914153\n",
      "Epoch 1893, Loss: 0.03370335738873109, Final Batch Loss: 0.00021680089412257075\n",
      "Epoch 1894, Loss: 0.04207837185822427, Final Batch Loss: 0.008349679410457611\n",
      "Epoch 1895, Loss: 0.020771877490915358, Final Batch Loss: 0.008027150295674801\n",
      "Epoch 1896, Loss: 0.029350224416702986, Final Batch Loss: 0.004975852556526661\n",
      "Epoch 1897, Loss: 0.016537241055630147, Final Batch Loss: 0.0028733424842357635\n",
      "Epoch 1898, Loss: 0.03069721208885312, Final Batch Loss: 0.0021254548337310553\n",
      "Epoch 1899, Loss: 0.025236202345695347, Final Batch Loss: 0.009472837671637535\n",
      "Epoch 1900, Loss: 0.02953623194480315, Final Batch Loss: 0.0007613706984557211\n",
      "Epoch 1901, Loss: 0.0373609462112654, Final Batch Loss: 0.016757480800151825\n",
      "Epoch 1902, Loss: 0.012803584337234497, Final Batch Loss: 0.002703010803088546\n",
      "Epoch 1903, Loss: 0.030720663256943226, Final Batch Loss: 0.0240792203694582\n",
      "Epoch 1904, Loss: 0.13025310169905424, Final Batch Loss: 0.002478858921676874\n",
      "Epoch 1905, Loss: 0.044354949379339814, Final Batch Loss: 0.020377179607748985\n",
      "Epoch 1906, Loss: 0.04539860691875219, Final Batch Loss: 0.003773311385884881\n",
      "Epoch 1907, Loss: 0.040966644301079214, Final Batch Loss: 0.010514015331864357\n",
      "Epoch 1908, Loss: 0.05416777334176004, Final Batch Loss: 0.012724529020488262\n",
      "Epoch 1909, Loss: 0.05409534543287009, Final Batch Loss: 0.0026912125758826733\n",
      "Epoch 1910, Loss: 0.04080114664975554, Final Batch Loss: 0.007399824447929859\n",
      "Epoch 1911, Loss: 0.06292198330629617, Final Batch Loss: 0.000884855689946562\n",
      "Epoch 1912, Loss: 0.044373519835062325, Final Batch Loss: 0.020467398688197136\n",
      "Epoch 1913, Loss: 0.034271803800947964, Final Batch Loss: 0.004237248096615076\n",
      "Epoch 1914, Loss: 0.06750733056105673, Final Batch Loss: 0.006734674330800772\n",
      "Epoch 1915, Loss: 0.03163394029252231, Final Batch Loss: 0.004590817727148533\n",
      "Epoch 1916, Loss: 0.028404771233908832, Final Batch Loss: 0.010093853808939457\n",
      "Epoch 1917, Loss: 0.022114176768809557, Final Batch Loss: 0.0016472497954964638\n",
      "Epoch 1918, Loss: 0.048361049266532063, Final Batch Loss: 0.00211709993891418\n",
      "Epoch 1919, Loss: 0.03089403104968369, Final Batch Loss: 0.0019528164993971586\n",
      "Epoch 1920, Loss: 0.03652226831763983, Final Batch Loss: 0.013579268008470535\n",
      "Epoch 1921, Loss: 0.025900851178448647, Final Batch Loss: 0.0008558919653296471\n",
      "Epoch 1922, Loss: 0.02917923463974148, Final Batch Loss: 0.0013373574474826455\n",
      "Epoch 1923, Loss: 0.047041436773724854, Final Batch Loss: 0.026525769382715225\n",
      "Epoch 1924, Loss: 0.027420403668656945, Final Batch Loss: 0.0053671193309128284\n",
      "Epoch 1925, Loss: 0.04494954040274024, Final Batch Loss: 0.0023916829377412796\n",
      "Epoch 1926, Loss: 0.034486227232264355, Final Batch Loss: 0.0002946246531791985\n",
      "Epoch 1927, Loss: 0.020216291188262403, Final Batch Loss: 0.0025054120924323797\n",
      "Epoch 1928, Loss: 0.01944467768771574, Final Batch Loss: 0.0005756819155067205\n",
      "Epoch 1929, Loss: 0.014986996306106448, Final Batch Loss: 0.0017738551832735538\n",
      "Epoch 1930, Loss: 0.016163070176844485, Final Batch Loss: 0.00014108426694292575\n",
      "Epoch 1931, Loss: 0.021380670834332705, Final Batch Loss: 0.0018935049884021282\n",
      "Epoch 1932, Loss: 0.04569962224923074, Final Batch Loss: 0.01763732358813286\n",
      "Epoch 1933, Loss: 0.028148597571998835, Final Batch Loss: 0.007544107269495726\n",
      "Epoch 1934, Loss: 0.009399741655215621, Final Batch Loss: 0.00034909334499388933\n",
      "Epoch 1935, Loss: 0.030280434992164373, Final Batch Loss: 0.0009507718496024609\n",
      "Epoch 1936, Loss: 0.019989447260741144, Final Batch Loss: 0.00723192049190402\n",
      "Epoch 1937, Loss: 0.03853477421216667, Final Batch Loss: 0.007052332162857056\n",
      "Epoch 1938, Loss: 0.06311197928152978, Final Batch Loss: 0.04259099066257477\n",
      "Epoch 1939, Loss: 0.05925055034458637, Final Batch Loss: 0.004055571276694536\n",
      "Epoch 1940, Loss: 0.026569535897579044, Final Batch Loss: 0.01704390160739422\n",
      "Epoch 1941, Loss: 0.03173060738481581, Final Batch Loss: 0.0028667314909398556\n",
      "Epoch 1942, Loss: 0.025516101624816656, Final Batch Loss: 0.010642707347869873\n",
      "Epoch 1943, Loss: 0.032154092798009515, Final Batch Loss: 0.002181323943659663\n",
      "Epoch 1944, Loss: 0.017631887196330354, Final Batch Loss: 0.00027857450186274946\n",
      "Epoch 1945, Loss: 0.039129930664785206, Final Batch Loss: 0.001107755582779646\n",
      "Epoch 1946, Loss: 0.019238454406149685, Final Batch Loss: 0.010601058602333069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1947, Loss: 0.059312594414222986, Final Batch Loss: 0.0029788310639560223\n",
      "Epoch 1948, Loss: 0.01772621099371463, Final Batch Loss: 0.002991684479638934\n",
      "Epoch 1949, Loss: 0.05160165624693036, Final Batch Loss: 0.004034104291349649\n",
      "Epoch 1950, Loss: 0.0550431240699254, Final Batch Loss: 0.004902616608887911\n",
      "Epoch 1951, Loss: 0.019491948769427836, Final Batch Loss: 0.0011294361902400851\n",
      "Epoch 1952, Loss: 0.04903459805063903, Final Batch Loss: 0.0008320135530084372\n",
      "Epoch 1953, Loss: 0.027727495413273573, Final Batch Loss: 0.00046779122203588486\n",
      "Epoch 1954, Loss: 0.019033683085581288, Final Batch Loss: 0.001246292726136744\n",
      "Epoch 1955, Loss: 0.048889536410570145, Final Batch Loss: 0.00022698845714330673\n",
      "Epoch 1956, Loss: 0.025616749946493655, Final Batch Loss: 0.0074144634418189526\n",
      "Epoch 1957, Loss: 0.02454021549783647, Final Batch Loss: 0.003822257276624441\n",
      "Epoch 1958, Loss: 0.027613572136033326, Final Batch Loss: 0.013201341964304447\n",
      "Epoch 1959, Loss: 0.013481518777552992, Final Batch Loss: 0.0005219936720095575\n",
      "Epoch 1960, Loss: 0.019130851243971847, Final Batch Loss: 0.00017030593880917877\n",
      "Epoch 1961, Loss: 0.042371968913357705, Final Batch Loss: 0.021582772955298424\n",
      "Epoch 1962, Loss: 0.008740962715819478, Final Batch Loss: 0.0011086065787822008\n",
      "Epoch 1963, Loss: 0.06652588001452386, Final Batch Loss: 0.0023935989011079073\n",
      "Epoch 1964, Loss: 0.05593855178449303, Final Batch Loss: 0.0010218886891379952\n",
      "Epoch 1965, Loss: 0.028355483897030354, Final Batch Loss: 0.007898668758571148\n",
      "Epoch 1966, Loss: 0.03510288905818015, Final Batch Loss: 0.0013486318057402968\n",
      "Epoch 1967, Loss: 0.019862003799062222, Final Batch Loss: 0.0008309922413900495\n",
      "Epoch 1968, Loss: 0.07755312719382346, Final Batch Loss: 0.03910180926322937\n",
      "Epoch 1969, Loss: 0.031020896276459098, Final Batch Loss: 0.0075600831769406796\n",
      "Epoch 1970, Loss: 0.0262232581153512, Final Batch Loss: 0.0004663335857912898\n",
      "Epoch 1971, Loss: 0.07795319519937038, Final Batch Loss: 0.003604582976549864\n",
      "Epoch 1972, Loss: 0.016294190601911396, Final Batch Loss: 0.0007840307080186903\n",
      "Epoch 1973, Loss: 0.022060029208660126, Final Batch Loss: 0.007161910645663738\n",
      "Epoch 1974, Loss: 0.03231508005410433, Final Batch Loss: 0.01350455917418003\n",
      "Epoch 1975, Loss: 0.047765991650521755, Final Batch Loss: 0.020995257422327995\n",
      "Epoch 1976, Loss: 0.01866723905550316, Final Batch Loss: 0.006994978990405798\n",
      "Epoch 1977, Loss: 0.037667236756533384, Final Batch Loss: 0.0027093139942735434\n",
      "Epoch 1978, Loss: 0.08012981968931854, Final Batch Loss: 0.049493592232465744\n",
      "Epoch 1979, Loss: 0.031289497623220086, Final Batch Loss: 0.001761884312145412\n",
      "Epoch 1980, Loss: 0.04896643292158842, Final Batch Loss: 0.027563638985157013\n",
      "Epoch 1981, Loss: 0.04665014112833887, Final Batch Loss: 0.0010917781619355083\n",
      "Epoch 1982, Loss: 0.03310366836376488, Final Batch Loss: 0.01007524412125349\n",
      "Epoch 1983, Loss: 0.07782917679287493, Final Batch Loss: 0.0031367898918688297\n",
      "Epoch 1984, Loss: 0.025033612502738833, Final Batch Loss: 0.0032817937899380922\n",
      "Epoch 1985, Loss: 0.024008381296880543, Final Batch Loss: 0.006450004409998655\n",
      "Epoch 1986, Loss: 0.03473809693241492, Final Batch Loss: 0.0005434700869955122\n",
      "Epoch 1987, Loss: 0.03366042242851108, Final Batch Loss: 0.0017353446455672383\n",
      "Epoch 1988, Loss: 0.01978841662639752, Final Batch Loss: 0.004603450186550617\n",
      "Epoch 1989, Loss: 0.008796781010460109, Final Batch Loss: 0.001499344129115343\n",
      "Epoch 1990, Loss: 0.012144292748416774, Final Batch Loss: 0.00017704190395306796\n",
      "Epoch 1991, Loss: 0.04451479180715978, Final Batch Loss: 0.003487651003524661\n",
      "Epoch 1992, Loss: 0.017824178357841447, Final Batch Loss: 0.002601180924102664\n",
      "Epoch 1993, Loss: 0.03605841059470549, Final Batch Loss: 0.0005944790900684893\n",
      "Epoch 1994, Loss: 0.019630607625003904, Final Batch Loss: 0.0004594842321239412\n",
      "Epoch 1995, Loss: 0.020225826068781316, Final Batch Loss: 0.010370693169534206\n",
      "Epoch 1996, Loss: 0.027644047630019486, Final Batch Loss: 0.014771859161555767\n",
      "Epoch 1997, Loss: 0.020172191550955176, Final Batch Loss: 0.007533203344792128\n",
      "Epoch 1998, Loss: 0.017990893684327602, Final Batch Loss: 0.0010656885569915175\n",
      "Epoch 1999, Loss: 0.03129727875057142, Final Batch Loss: 0.009946190752089024\n",
      "Epoch 2000, Loss: 0.034415192960295826, Final Batch Loss: 0.005411837715655565\n",
      "Epoch 2001, Loss: 0.020561116456519812, Final Batch Loss: 0.0005234062555246055\n",
      "Epoch 2002, Loss: 0.023429852968547493, Final Batch Loss: 0.013500308617949486\n",
      "Epoch 2003, Loss: 0.01862074516247958, Final Batch Loss: 0.0020366301760077477\n",
      "Epoch 2004, Loss: 0.008338394167367369, Final Batch Loss: 0.00020485970890149474\n",
      "Epoch 2005, Loss: 0.01062949257902801, Final Batch Loss: 0.0008510485058650374\n",
      "Epoch 2006, Loss: 0.013034161063842475, Final Batch Loss: 0.0012063508620485663\n",
      "Epoch 2007, Loss: 0.04017877619480714, Final Batch Loss: 0.0005680103786289692\n",
      "Epoch 2008, Loss: 0.01762707531452179, Final Batch Loss: 0.0005982613656669855\n",
      "Epoch 2009, Loss: 0.04029481881298125, Final Batch Loss: 0.004587358795106411\n",
      "Epoch 2010, Loss: 0.042410633293911815, Final Batch Loss: 0.02462209202349186\n",
      "Epoch 2011, Loss: 0.038527207681909204, Final Batch Loss: 0.008106539025902748\n",
      "Epoch 2012, Loss: 0.03464450954925269, Final Batch Loss: 0.003472236916422844\n",
      "Epoch 2013, Loss: 0.009342830569949001, Final Batch Loss: 0.003962223883718252\n",
      "Epoch 2014, Loss: 0.06873553723562509, Final Batch Loss: 0.0074280789121985435\n",
      "Epoch 2015, Loss: 0.01672661886550486, Final Batch Loss: 0.001655852422118187\n",
      "Epoch 2016, Loss: 0.03264355700230226, Final Batch Loss: 0.0012545993085950613\n",
      "Epoch 2017, Loss: 0.07151994155719876, Final Batch Loss: 0.0013512300793081522\n",
      "Epoch 2018, Loss: 0.022886157588800415, Final Batch Loss: 0.0008619423024356365\n",
      "Epoch 2019, Loss: 0.03474656480830163, Final Batch Loss: 0.0018213099101558328\n",
      "Epoch 2020, Loss: 0.021013010060414672, Final Batch Loss: 0.001613259082660079\n",
      "Epoch 2021, Loss: 0.023166327155195177, Final Batch Loss: 0.001824022619985044\n",
      "Epoch 2022, Loss: 0.016300410010444466, Final Batch Loss: 6.779950490454212e-05\n",
      "Epoch 2023, Loss: 0.01357207482215017, Final Batch Loss: 0.0004759623552672565\n",
      "Epoch 2024, Loss: 0.029409416543785483, Final Batch Loss: 0.0007494496530853212\n",
      "Epoch 2025, Loss: 0.021364461863413453, Final Batch Loss: 0.004472387954592705\n",
      "Epoch 2026, Loss: 0.014617935696151108, Final Batch Loss: 0.00024428736651316285\n",
      "Epoch 2027, Loss: 0.01698595061316155, Final Batch Loss: 0.005910444539040327\n",
      "Epoch 2028, Loss: 0.01985594443976879, Final Batch Loss: 0.000986270373687148\n",
      "Epoch 2029, Loss: 0.012966090631380212, Final Batch Loss: 0.00010110239236382768\n",
      "Epoch 2030, Loss: 0.04750859948399011, Final Batch Loss: 0.0002404438710073009\n",
      "Epoch 2031, Loss: 0.04113414813764393, Final Batch Loss: 0.01928466558456421\n",
      "Epoch 2032, Loss: 0.02200210085720755, Final Batch Loss: 8.864901610650122e-05\n",
      "Epoch 2033, Loss: 0.030314684147015214, Final Batch Loss: 0.0016405051574110985\n",
      "Epoch 2034, Loss: 0.0556484037078917, Final Batch Loss: 0.007569299079477787\n",
      "Epoch 2035, Loss: 0.016219725337577984, Final Batch Loss: 0.001180982100777328\n",
      "Epoch 2036, Loss: 0.027869268495123833, Final Batch Loss: 0.00046187167754396796\n",
      "Epoch 2037, Loss: 0.028689889470115304, Final Batch Loss: 0.003495157929137349\n",
      "Epoch 2038, Loss: 0.020230459049344063, Final Batch Loss: 0.0001430688425898552\n",
      "Epoch 2039, Loss: 0.029674808029085398, Final Batch Loss: 0.0031409738585352898\n",
      "Epoch 2040, Loss: 0.011444728705100715, Final Batch Loss: 0.0023362317588180304\n",
      "Epoch 2041, Loss: 0.042070643743500113, Final Batch Loss: 0.019482655450701714\n",
      "Epoch 2042, Loss: 0.06764667679090053, Final Batch Loss: 0.004145744256675243\n",
      "Epoch 2043, Loss: 0.039971408317796886, Final Batch Loss: 0.001180923660285771\n",
      "Epoch 2044, Loss: 0.031037391629070044, Final Batch Loss: 0.002064622938632965\n",
      "Epoch 2045, Loss: 0.049820183077827096, Final Batch Loss: 0.0216127447783947\n",
      "Epoch 2046, Loss: 0.031997470068745315, Final Batch Loss: 0.0025685876607894897\n",
      "Epoch 2047, Loss: 0.013490268756868318, Final Batch Loss: 0.001801195670850575\n",
      "Epoch 2048, Loss: 0.020120671717450023, Final Batch Loss: 0.008023519068956375\n",
      "Epoch 2049, Loss: 0.04261105018667877, Final Batch Loss: 0.003676643129438162\n",
      "Epoch 2050, Loss: 0.03052275936352089, Final Batch Loss: 0.004918075632303953\n",
      "Epoch 2051, Loss: 0.010750639368779957, Final Batch Loss: 0.001038257614709437\n",
      "Epoch 2052, Loss: 0.03567634895443916, Final Batch Loss: 0.016631251201033592\n",
      "Epoch 2053, Loss: 0.012770626693964005, Final Batch Loss: 0.006082612089812756\n",
      "Epoch 2054, Loss: 0.010816603782586753, Final Batch Loss: 0.005263337399810553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2055, Loss: 0.014543237630277872, Final Batch Loss: 0.0010041187051683664\n",
      "Epoch 2056, Loss: 0.01063421054277569, Final Batch Loss: 0.003689882345497608\n",
      "Epoch 2057, Loss: 0.0385236288420856, Final Batch Loss: 0.0011823485838249326\n",
      "Epoch 2058, Loss: 0.020520592108368874, Final Batch Loss: 0.006906815338879824\n",
      "Epoch 2059, Loss: 0.02079808755661361, Final Batch Loss: 0.0028120933566242456\n",
      "Epoch 2060, Loss: 0.01693813083693385, Final Batch Loss: 0.0006281237583607435\n",
      "Epoch 2061, Loss: 0.013926688639912754, Final Batch Loss: 0.0006450010696426034\n",
      "Epoch 2062, Loss: 0.012565708486363292, Final Batch Loss: 0.0032122652046382427\n",
      "Epoch 2063, Loss: 0.057576548773795366, Final Batch Loss: 0.033189233392477036\n",
      "Epoch 2064, Loss: 0.023514657113992143, Final Batch Loss: 0.0013932035071775317\n",
      "Epoch 2065, Loss: 0.021044107590569183, Final Batch Loss: 0.01409099344164133\n",
      "Epoch 2066, Loss: 0.011602599908655975, Final Batch Loss: 0.0016567453276365995\n",
      "Epoch 2067, Loss: 0.03418375988258049, Final Batch Loss: 0.00045681645860895514\n",
      "Epoch 2068, Loss: 0.030512058205204085, Final Batch Loss: 0.00039156366256065667\n",
      "Epoch 2069, Loss: 0.03214738445240073, Final Batch Loss: 0.0031578796915709972\n",
      "Epoch 2070, Loss: 0.043001315905712545, Final Batch Loss: 0.0034350266214460135\n",
      "Epoch 2071, Loss: 0.02067621366586536, Final Batch Loss: 0.0069081224501132965\n",
      "Epoch 2072, Loss: 0.04677986056776717, Final Batch Loss: 0.01572062261402607\n",
      "Epoch 2073, Loss: 0.03893423135741614, Final Batch Loss: 0.0004376819415483624\n",
      "Epoch 2074, Loss: 0.028151458129286766, Final Batch Loss: 0.0005187843926250935\n",
      "Epoch 2075, Loss: 0.060148665579617955, Final Batch Loss: 0.016031978651881218\n",
      "Epoch 2076, Loss: 0.04078186466358602, Final Batch Loss: 0.0019923290237784386\n",
      "Epoch 2077, Loss: 0.013770763063803315, Final Batch Loss: 0.0044067432172596455\n",
      "Epoch 2078, Loss: 0.016538832802325487, Final Batch Loss: 0.001924813026562333\n",
      "Epoch 2079, Loss: 0.025431917019886896, Final Batch Loss: 0.0001367155637126416\n",
      "Epoch 2080, Loss: 0.028211571625433862, Final Batch Loss: 0.00037178013008087873\n",
      "Epoch 2081, Loss: 0.030640899552963674, Final Batch Loss: 0.019100947305560112\n",
      "Epoch 2082, Loss: 0.028955291432794183, Final Batch Loss: 0.006218743976205587\n",
      "Epoch 2083, Loss: 0.06127900682622567, Final Batch Loss: 0.0005506993620656431\n",
      "Epoch 2084, Loss: 0.08680601965170354, Final Batch Loss: 0.06655341386795044\n",
      "Epoch 2085, Loss: 0.010776340554002672, Final Batch Loss: 0.0035591560881584883\n",
      "Epoch 2086, Loss: 0.02852747164433822, Final Batch Loss: 0.0007443905924446881\n",
      "Epoch 2087, Loss: 0.019981340039521456, Final Batch Loss: 0.0077283489517867565\n",
      "Epoch 2088, Loss: 0.04145371785853058, Final Batch Loss: 0.004809113219380379\n",
      "Epoch 2089, Loss: 0.08833160513313487, Final Batch Loss: 0.0459994338452816\n",
      "Epoch 2090, Loss: 0.04214378970209509, Final Batch Loss: 0.03004673309624195\n",
      "Epoch 2091, Loss: 0.0905199064873159, Final Batch Loss: 0.005845654755830765\n",
      "Epoch 2092, Loss: 0.020160686050076038, Final Batch Loss: 0.0027546226046979427\n",
      "Epoch 2093, Loss: 0.042680021142587066, Final Batch Loss: 0.0036905715242028236\n",
      "Epoch 2094, Loss: 0.040265930350869894, Final Batch Loss: 0.016647454351186752\n",
      "Epoch 2095, Loss: 0.01926493109203875, Final Batch Loss: 0.0009149961406365037\n",
      "Epoch 2096, Loss: 0.08508208207786083, Final Batch Loss: 0.0028570136055350304\n",
      "Epoch 2097, Loss: 0.01304277329472825, Final Batch Loss: 0.0005669945385307074\n",
      "Epoch 2098, Loss: 0.016741060244385153, Final Batch Loss: 0.008180479519069195\n",
      "Epoch 2099, Loss: 0.026390663580968976, Final Batch Loss: 0.001070528756827116\n",
      "Epoch 2100, Loss: 0.026384247990790755, Final Batch Loss: 0.010415521450340748\n",
      "Epoch 2101, Loss: 0.031132379779592156, Final Batch Loss: 0.001041666604578495\n",
      "Epoch 2102, Loss: 0.019970729190390557, Final Batch Loss: 7.867050589993596e-05\n",
      "Epoch 2103, Loss: 0.03323552364599891, Final Batch Loss: 0.0026351558044552803\n",
      "Epoch 2104, Loss: 0.04455443980987184, Final Batch Loss: 0.013694483786821365\n",
      "Epoch 2105, Loss: 0.01626352232415229, Final Batch Loss: 0.004109837114810944\n",
      "Epoch 2106, Loss: 0.01805319666163996, Final Batch Loss: 0.0004833367420360446\n",
      "Epoch 2107, Loss: 0.06589756108587608, Final Batch Loss: 0.04065592586994171\n",
      "Epoch 2108, Loss: 0.018628654186613858, Final Batch Loss: 0.0034829964861273766\n",
      "Epoch 2109, Loss: 0.065662419074215, Final Batch Loss: 0.03357144817709923\n",
      "Epoch 2110, Loss: 0.02249156651669182, Final Batch Loss: 0.0005611586384475231\n",
      "Epoch 2111, Loss: 0.016202007420361042, Final Batch Loss: 0.00026114494539797306\n",
      "Epoch 2112, Loss: 0.02467271556815831, Final Batch Loss: 0.0028536797035485506\n",
      "Epoch 2113, Loss: 0.04179097828455269, Final Batch Loss: 0.022257061675190926\n",
      "Epoch 2114, Loss: 0.011001659644534811, Final Batch Loss: 0.0003250970912631601\n",
      "Epoch 2115, Loss: 0.018372683349298313, Final Batch Loss: 0.00732613168656826\n",
      "Epoch 2116, Loss: 0.010213929315796122, Final Batch Loss: 0.003766283392906189\n",
      "Epoch 2117, Loss: 0.023746661696350202, Final Batch Loss: 0.0013639143435284495\n",
      "Epoch 2118, Loss: 0.01876760515733622, Final Batch Loss: 0.0019458298338577151\n",
      "Epoch 2119, Loss: 0.02683135319966823, Final Batch Loss: 0.0017848178977146745\n",
      "Epoch 2120, Loss: 0.023768762534018606, Final Batch Loss: 0.000815142469946295\n",
      "Epoch 2121, Loss: 0.01808362500742078, Final Batch Loss: 0.00024800054961815476\n",
      "Epoch 2122, Loss: 0.05445768916979432, Final Batch Loss: 0.021931059658527374\n",
      "Epoch 2123, Loss: 0.022659313981421292, Final Batch Loss: 0.0014783411752432585\n",
      "Epoch 2124, Loss: 0.03425095829879865, Final Batch Loss: 0.014926474541425705\n",
      "Epoch 2125, Loss: 0.024547452718252316, Final Batch Loss: 0.003438785206526518\n",
      "Epoch 2126, Loss: 0.013372979999985546, Final Batch Loss: 0.0054190149530768394\n",
      "Epoch 2127, Loss: 0.03751378049491905, Final Batch Loss: 5.1025854190811515e-05\n",
      "Epoch 2128, Loss: 0.021897923448705114, Final Batch Loss: 0.01515931636095047\n",
      "Epoch 2129, Loss: 0.022074122971389443, Final Batch Loss: 0.005150716286152601\n",
      "Epoch 2130, Loss: 0.036192356841638684, Final Batch Loss: 0.004363390617072582\n",
      "Epoch 2131, Loss: 0.009487898816587403, Final Batch Loss: 0.0019487395184114575\n",
      "Epoch 2132, Loss: 0.011615436611464247, Final Batch Loss: 0.00044013853766955435\n",
      "Epoch 2133, Loss: 0.008827168960124254, Final Batch Loss: 0.001710198470391333\n",
      "Epoch 2134, Loss: 0.010800156160257757, Final Batch Loss: 0.0014662290923297405\n",
      "Epoch 2135, Loss: 0.023321844695601612, Final Batch Loss: 0.0011359485797584057\n",
      "Epoch 2136, Loss: 0.01344722742214799, Final Batch Loss: 0.0012083706678822637\n",
      "Epoch 2137, Loss: 0.012176420190371573, Final Batch Loss: 0.0030088627245277166\n",
      "Epoch 2138, Loss: 0.048283556825481355, Final Batch Loss: 0.00729356799274683\n",
      "Epoch 2139, Loss: 0.011534680845215917, Final Batch Loss: 0.0038059703074395657\n",
      "Epoch 2140, Loss: 0.011715492699295282, Final Batch Loss: 0.0010941202053800225\n",
      "Epoch 2141, Loss: 0.011872981791384518, Final Batch Loss: 0.0007324606413021684\n",
      "Epoch 2142, Loss: 0.02445275365607813, Final Batch Loss: 0.009820282459259033\n",
      "Epoch 2143, Loss: 0.010505983955226839, Final Batch Loss: 0.0009645282407291234\n",
      "Epoch 2144, Loss: 0.010830581028130837, Final Batch Loss: 9.674810280557722e-05\n",
      "Epoch 2145, Loss: 0.02296486683189869, Final Batch Loss: 0.005148554686456919\n",
      "Epoch 2146, Loss: 0.04617527883965522, Final Batch Loss: 0.02763965167105198\n",
      "Epoch 2147, Loss: 0.024789748014882207, Final Batch Loss: 0.002390035893768072\n",
      "Epoch 2148, Loss: 0.04044990994952968, Final Batch Loss: 5.6065469834720716e-05\n",
      "Epoch 2149, Loss: 0.015334682422690094, Final Batch Loss: 0.0013146452838554978\n",
      "Epoch 2150, Loss: 0.02046105923363939, Final Batch Loss: 0.0008329869597218931\n",
      "Epoch 2151, Loss: 0.06189682043623179, Final Batch Loss: 0.001933282590471208\n",
      "Epoch 2152, Loss: 0.011044675688026473, Final Batch Loss: 0.0004634517536032945\n",
      "Epoch 2153, Loss: 0.02117153225117363, Final Batch Loss: 0.008071987889707088\n",
      "Epoch 2154, Loss: 0.06892218091525137, Final Batch Loss: 0.030760031193494797\n",
      "Epoch 2155, Loss: 0.025325673865154386, Final Batch Loss: 0.0006693612085655332\n",
      "Epoch 2156, Loss: 0.03129152982728556, Final Batch Loss: 0.0007012716378085315\n",
      "Epoch 2157, Loss: 0.05415071058087051, Final Batch Loss: 0.0048078917898237705\n",
      "Epoch 2158, Loss: 0.0176162623683922, Final Batch Loss: 0.005962860770523548\n",
      "Epoch 2159, Loss: 0.05287110659992322, Final Batch Loss: 0.0009341058903373778\n",
      "Epoch 2160, Loss: 0.09427757456433028, Final Batch Loss: 0.0016962796216830611\n",
      "Epoch 2161, Loss: 0.04826510592829436, Final Batch Loss: 0.010069763287901878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2162, Loss: 0.010752636182587594, Final Batch Loss: 0.002791025210171938\n",
      "Epoch 2163, Loss: 0.02571164577966556, Final Batch Loss: 0.0005681993789039552\n",
      "Epoch 2164, Loss: 0.03228937787935138, Final Batch Loss: 0.0019067574758082628\n",
      "Epoch 2165, Loss: 0.02041829889640212, Final Batch Loss: 0.0008801105432212353\n",
      "Epoch 2166, Loss: 0.012166495900601149, Final Batch Loss: 0.0033367290161550045\n",
      "Epoch 2167, Loss: 0.016872892359970137, Final Batch Loss: 0.0005101371789351106\n",
      "Epoch 2168, Loss: 0.012540897849248722, Final Batch Loss: 0.0004135892086196691\n",
      "Epoch 2169, Loss: 0.013283073552884161, Final Batch Loss: 0.002004220150411129\n",
      "Epoch 2170, Loss: 0.017000112537061796, Final Batch Loss: 0.00016445931396447122\n",
      "Epoch 2171, Loss: 0.015823204535990953, Final Batch Loss: 0.002769805258139968\n",
      "Epoch 2172, Loss: 0.006799772789236158, Final Batch Loss: 0.000873129814863205\n",
      "Epoch 2173, Loss: 0.01875883765751496, Final Batch Loss: 0.009099780581891537\n",
      "Epoch 2174, Loss: 0.04072120221098885, Final Batch Loss: 0.013384707272052765\n",
      "Epoch 2175, Loss: 0.011076457391027361, Final Batch Loss: 0.0004117191710975021\n",
      "Epoch 2176, Loss: 0.02758240542607382, Final Batch Loss: 0.017781700938940048\n",
      "Epoch 2177, Loss: 0.024029416585108265, Final Batch Loss: 0.001717958366498351\n",
      "Epoch 2178, Loss: 0.05060487205628306, Final Batch Loss: 0.0004113636678084731\n",
      "Epoch 2179, Loss: 0.048678891151212156, Final Batch Loss: 0.026205500587821007\n",
      "Epoch 2180, Loss: 0.0543764301110059, Final Batch Loss: 0.01342304889112711\n",
      "Epoch 2181, Loss: 0.044360351050272584, Final Batch Loss: 0.009699502028524876\n",
      "Epoch 2182, Loss: 0.04327051536529325, Final Batch Loss: 0.0054257879965007305\n",
      "Epoch 2183, Loss: 0.030928005930036306, Final Batch Loss: 0.0009240004583261907\n",
      "Epoch 2184, Loss: 0.022074177279137075, Final Batch Loss: 0.002671742346137762\n",
      "Epoch 2185, Loss: 0.05192116787657142, Final Batch Loss: 0.009532948024570942\n",
      "Epoch 2186, Loss: 0.03958023781888187, Final Batch Loss: 0.000782740768045187\n",
      "Epoch 2187, Loss: 0.037858993047848344, Final Batch Loss: 0.002008514478802681\n",
      "Epoch 2188, Loss: 0.04324145498685539, Final Batch Loss: 0.024629291146993637\n",
      "Epoch 2189, Loss: 0.02363705256721005, Final Batch Loss: 0.003884472418576479\n",
      "Epoch 2190, Loss: 0.01543756271712482, Final Batch Loss: 0.001076113199815154\n",
      "Epoch 2191, Loss: 0.03199836704879999, Final Batch Loss: 0.015395237132906914\n",
      "Epoch 2192, Loss: 0.01591298310086131, Final Batch Loss: 0.0007847367087379098\n",
      "Epoch 2193, Loss: 0.05340436432743445, Final Batch Loss: 0.03535819798707962\n",
      "Epoch 2194, Loss: 0.02422097435919568, Final Batch Loss: 0.0017377713229507208\n",
      "Epoch 2195, Loss: 0.035665646428242326, Final Batch Loss: 0.00986417569220066\n",
      "Epoch 2196, Loss: 0.054839100455865264, Final Batch Loss: 0.0058038607239723206\n",
      "Epoch 2197, Loss: 0.021119937300682068, Final Batch Loss: 0.0033384361304342747\n",
      "Epoch 2198, Loss: 0.040258218767121434, Final Batch Loss: 0.002837162697687745\n",
      "Epoch 2199, Loss: 0.022441734676249325, Final Batch Loss: 0.0013033724389970303\n",
      "Epoch 2200, Loss: 0.018205100437626243, Final Batch Loss: 0.002843199297785759\n",
      "Epoch 2201, Loss: 0.008656084071844816, Final Batch Loss: 0.005496137775480747\n",
      "Epoch 2202, Loss: 0.023159525546361692, Final Batch Loss: 0.0015747144352644682\n",
      "Epoch 2203, Loss: 0.041270243120379746, Final Batch Loss: 0.001764089334756136\n",
      "Epoch 2204, Loss: 0.010491661145351827, Final Batch Loss: 0.0019961462821811438\n",
      "Epoch 2205, Loss: 0.028655582980718464, Final Batch Loss: 0.011430004611611366\n",
      "Epoch 2206, Loss: 0.018082794354995713, Final Batch Loss: 0.003473154501989484\n",
      "Epoch 2207, Loss: 0.01584433499374427, Final Batch Loss: 0.00047328780055977404\n",
      "Epoch 2208, Loss: 0.050228875945322216, Final Batch Loss: 0.0006165489321574569\n",
      "Epoch 2209, Loss: 0.00917154861963354, Final Batch Loss: 0.0009619506308808923\n",
      "Epoch 2210, Loss: 0.05253339733462781, Final Batch Loss: 0.01663353480398655\n",
      "Epoch 2211, Loss: 0.02271389552333858, Final Batch Loss: 0.00014138246478978544\n",
      "Epoch 2212, Loss: 0.03253753609897103, Final Batch Loss: 0.0024772980250418186\n",
      "Epoch 2213, Loss: 0.008083199558313936, Final Batch Loss: 0.00275835650973022\n",
      "Epoch 2214, Loss: 0.016332213417626917, Final Batch Loss: 0.002002179156988859\n",
      "Epoch 2215, Loss: 0.018946636701002717, Final Batch Loss: 0.001444097375497222\n",
      "Epoch 2216, Loss: 0.026731063553597778, Final Batch Loss: 0.0023040147498250008\n",
      "Epoch 2217, Loss: 0.034510305791627616, Final Batch Loss: 0.00020411331206560135\n",
      "Epoch 2218, Loss: 0.037895369983743876, Final Batch Loss: 0.00517725944519043\n",
      "Epoch 2219, Loss: 0.02497920300811529, Final Batch Loss: 0.0011854649055749178\n",
      "Epoch 2220, Loss: 0.02270007086917758, Final Batch Loss: 0.008528681471943855\n",
      "Epoch 2221, Loss: 0.03792236768640578, Final Batch Loss: 0.008397954516112804\n",
      "Epoch 2222, Loss: 0.011048958753235638, Final Batch Loss: 0.0007071283180266619\n",
      "Epoch 2223, Loss: 0.03020173334516585, Final Batch Loss: 0.011448158882558346\n",
      "Epoch 2224, Loss: 0.030374645022675395, Final Batch Loss: 0.0011319743935018778\n",
      "Epoch 2225, Loss: 0.03610151179600507, Final Batch Loss: 0.0023728522937744856\n",
      "Epoch 2226, Loss: 0.012671841817791574, Final Batch Loss: 0.00019885254732798785\n",
      "Epoch 2227, Loss: 0.008673153351992369, Final Batch Loss: 0.0002579295542091131\n",
      "Epoch 2228, Loss: 0.014572683838196099, Final Batch Loss: 0.0014580624410882592\n",
      "Epoch 2229, Loss: 0.011636709910817444, Final Batch Loss: 0.0014321187045425177\n",
      "Epoch 2230, Loss: 0.011472398589830846, Final Batch Loss: 0.001938080182299018\n",
      "Epoch 2231, Loss: 0.005726054005208425, Final Batch Loss: 0.000191061437362805\n",
      "Epoch 2232, Loss: 0.016699891224561725, Final Batch Loss: 0.0005870464956387877\n",
      "Epoch 2233, Loss: 0.01224886170530226, Final Batch Loss: 0.004419634118676186\n",
      "Epoch 2234, Loss: 0.04691704624565318, Final Batch Loss: 0.02509554848074913\n",
      "Epoch 2235, Loss: 0.021746239755884744, Final Batch Loss: 0.0024816531222313643\n",
      "Epoch 2236, Loss: 0.02605722806765698, Final Batch Loss: 0.00033583492040634155\n",
      "Epoch 2237, Loss: 0.03648858773522079, Final Batch Loss: 0.019073553383350372\n",
      "Epoch 2238, Loss: 0.014762263977900147, Final Batch Loss: 0.0013964215759187937\n",
      "Epoch 2239, Loss: 0.03597909910604358, Final Batch Loss: 0.02241232991218567\n",
      "Epoch 2240, Loss: 0.02240670775063336, Final Batch Loss: 0.014394204132258892\n",
      "Epoch 2241, Loss: 0.02270202964427881, Final Batch Loss: 0.0027022624853998423\n",
      "Epoch 2242, Loss: 0.020942503469996154, Final Batch Loss: 0.00227704388089478\n",
      "Epoch 2243, Loss: 0.010343989124521613, Final Batch Loss: 0.0034459554590284824\n",
      "Epoch 2244, Loss: 0.023392930103000253, Final Batch Loss: 0.0020427098497748375\n",
      "Epoch 2245, Loss: 0.04687113349791616, Final Batch Loss: 0.0007534255273640156\n",
      "Epoch 2246, Loss: 0.033436810117564164, Final Batch Loss: 0.014273781329393387\n",
      "Epoch 2247, Loss: 0.009776636376045644, Final Batch Loss: 0.0026731796097010374\n",
      "Epoch 2248, Loss: 0.01648001023568213, Final Batch Loss: 0.005717349238693714\n",
      "Epoch 2249, Loss: 0.05041996878571808, Final Batch Loss: 0.03370021656155586\n",
      "Epoch 2250, Loss: 0.008432246395386755, Final Batch Loss: 0.003070989390835166\n",
      "Epoch 2251, Loss: 0.03890773036982864, Final Batch Loss: 0.0027321185916662216\n",
      "Epoch 2252, Loss: 0.016657882602885365, Final Batch Loss: 0.0017992171924561262\n",
      "Epoch 2253, Loss: 0.01642963133053854, Final Batch Loss: 0.0018150467658415437\n",
      "Epoch 2254, Loss: 0.01429121193359606, Final Batch Loss: 0.0023242991883307695\n",
      "Epoch 2255, Loss: 0.05019616842037067, Final Batch Loss: 0.03632499277591705\n",
      "Epoch 2256, Loss: 0.031493302580202, Final Batch Loss: 0.0017681583994999528\n",
      "Epoch 2257, Loss: 0.027063147048465908, Final Batch Loss: 0.012995420955121517\n",
      "Epoch 2258, Loss: 0.01671368139795959, Final Batch Loss: 0.006128349341452122\n",
      "Epoch 2259, Loss: 0.05145310773514211, Final Batch Loss: 0.01574520394206047\n",
      "Epoch 2260, Loss: 0.010177042669965886, Final Batch Loss: 0.0002123830927303061\n",
      "Epoch 2261, Loss: 0.026467534946277738, Final Batch Loss: 0.0021449406631290913\n",
      "Epoch 2262, Loss: 0.00978696889069397, Final Batch Loss: 0.0021712956950068474\n",
      "Epoch 2263, Loss: 0.021659551915945485, Final Batch Loss: 0.0004770941159222275\n",
      "Epoch 2264, Loss: 0.06866792106302455, Final Batch Loss: 0.023941295221447945\n",
      "Epoch 2265, Loss: 0.024463852168992162, Final Batch Loss: 0.0004556708736345172\n",
      "Epoch 2266, Loss: 0.055375534706399776, Final Batch Loss: 0.0036459011025726795\n",
      "Epoch 2267, Loss: 0.04302682349953102, Final Batch Loss: 8.503226126777008e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2268, Loss: 0.013759559631580487, Final Batch Loss: 0.00442128162831068\n",
      "Epoch 2269, Loss: 0.03558679507113993, Final Batch Loss: 0.0003971913829445839\n",
      "Epoch 2270, Loss: 0.01296036911662668, Final Batch Loss: 0.0007696737302467227\n",
      "Epoch 2271, Loss: 0.040064458502456546, Final Batch Loss: 0.010061835870146751\n",
      "Epoch 2272, Loss: 0.013075432856567204, Final Batch Loss: 0.000341187696903944\n",
      "Epoch 2273, Loss: 0.028501442749984562, Final Batch Loss: 0.0016172905452549458\n",
      "Epoch 2274, Loss: 0.021361358056310564, Final Batch Loss: 0.0016689597396180034\n",
      "Epoch 2275, Loss: 0.014364981674589217, Final Batch Loss: 0.0056358701549470425\n",
      "Epoch 2276, Loss: 0.022502688807435334, Final Batch Loss: 0.005153264384716749\n",
      "Epoch 2277, Loss: 0.04864845678093843, Final Batch Loss: 0.02514900453388691\n",
      "Epoch 2278, Loss: 0.01168041949858889, Final Batch Loss: 0.0026582006830722094\n",
      "Epoch 2279, Loss: 0.01904008473502472, Final Batch Loss: 0.0006910093943588436\n",
      "Epoch 2280, Loss: 0.011957335722399876, Final Batch Loss: 0.00035041439696215093\n",
      "Epoch 2281, Loss: 0.006032211444107816, Final Batch Loss: 0.00038055118056945503\n",
      "Epoch 2282, Loss: 0.008360849635209888, Final Batch Loss: 0.003709659446030855\n",
      "Epoch 2283, Loss: 0.033255612244829535, Final Batch Loss: 0.007169579621404409\n",
      "Epoch 2284, Loss: 0.011479124281322584, Final Batch Loss: 0.007191497832536697\n",
      "Epoch 2285, Loss: 0.009086927166208625, Final Batch Loss: 0.001180519931949675\n",
      "Epoch 2286, Loss: 0.01770925530581735, Final Batch Loss: 0.013440326787531376\n",
      "Epoch 2287, Loss: 0.018699055653996766, Final Batch Loss: 0.015507147647440434\n",
      "Epoch 2288, Loss: 0.01994790992466733, Final Batch Loss: 0.0005686184740625322\n",
      "Epoch 2289, Loss: 0.04131711687659845, Final Batch Loss: 0.0004061990766786039\n",
      "Epoch 2290, Loss: 0.025172767404001206, Final Batch Loss: 0.01884395442903042\n",
      "Epoch 2291, Loss: 0.018688177136937156, Final Batch Loss: 0.0020509411115199327\n",
      "Epoch 2292, Loss: 0.03710874496027827, Final Batch Loss: 0.025314560160040855\n",
      "Epoch 2293, Loss: 0.013890768110286444, Final Batch Loss: 0.001467142952606082\n",
      "Epoch 2294, Loss: 0.031185633648419753, Final Batch Loss: 0.007986566983163357\n",
      "Epoch 2295, Loss: 0.012012724881060421, Final Batch Loss: 0.00024280243087559938\n",
      "Epoch 2296, Loss: 0.011148329067509621, Final Batch Loss: 0.00137234921567142\n",
      "Epoch 2297, Loss: 0.00941119238268584, Final Batch Loss: 0.0002645737840794027\n",
      "Epoch 2298, Loss: 0.015152039748500101, Final Batch Loss: 0.00733662536367774\n",
      "Epoch 2299, Loss: 0.040458539297105744, Final Batch Loss: 0.001821235753595829\n",
      "Epoch 2300, Loss: 0.011653017078060657, Final Batch Loss: 0.001743061002343893\n",
      "Epoch 2301, Loss: 0.01896925613982603, Final Batch Loss: 0.003429829142987728\n",
      "Epoch 2302, Loss: 0.008508698156219907, Final Batch Loss: 0.00023970134498085827\n",
      "Epoch 2303, Loss: 0.015487054828554392, Final Batch Loss: 0.0012924328912049532\n",
      "Epoch 2304, Loss: 0.04337260022293776, Final Batch Loss: 0.00048591906670480967\n",
      "Epoch 2305, Loss: 0.013051416957750916, Final Batch Loss: 0.0038796078879386187\n",
      "Epoch 2306, Loss: 0.037260846816934645, Final Batch Loss: 0.00047279533464461565\n",
      "Epoch 2307, Loss: 0.07405318625387736, Final Batch Loss: 0.03962988778948784\n",
      "Epoch 2308, Loss: 0.005214856792008504, Final Batch Loss: 0.000991437234915793\n",
      "Epoch 2309, Loss: 0.02459777817421127, Final Batch Loss: 0.00012857031833846122\n",
      "Epoch 2310, Loss: 0.020434406615095213, Final Batch Loss: 0.00021487960475496948\n",
      "Epoch 2311, Loss: 0.026994359795935452, Final Batch Loss: 0.005558898206800222\n",
      "Epoch 2312, Loss: 0.006342499051243067, Final Batch Loss: 0.001674893544986844\n",
      "Epoch 2313, Loss: 0.020392255391925573, Final Batch Loss: 0.006761457771062851\n",
      "Epoch 2314, Loss: 0.014330334575788584, Final Batch Loss: 0.001761231105774641\n",
      "Epoch 2315, Loss: 0.0151649487670511, Final Batch Loss: 0.0004663422587327659\n",
      "Epoch 2316, Loss: 0.008412829891312867, Final Batch Loss: 0.0008091561612673104\n",
      "Epoch 2317, Loss: 0.06479569571092725, Final Batch Loss: 0.0020130579359829426\n",
      "Epoch 2318, Loss: 0.028734683262882754, Final Batch Loss: 0.003760235384106636\n",
      "Epoch 2319, Loss: 0.032259132829494774, Final Batch Loss: 0.0008076942758634686\n",
      "Epoch 2320, Loss: 0.030717006040504202, Final Batch Loss: 0.01763548143208027\n",
      "Epoch 2321, Loss: 0.027498613519128412, Final Batch Loss: 0.00164601334836334\n",
      "Epoch 2322, Loss: 0.038671750749927014, Final Batch Loss: 0.0005492738564498723\n",
      "Epoch 2323, Loss: 0.01785819639917463, Final Batch Loss: 0.0066566625609993935\n",
      "Epoch 2324, Loss: 0.039468970644520596, Final Batch Loss: 0.00016746335313655436\n",
      "Epoch 2325, Loss: 0.006319066655123606, Final Batch Loss: 0.0004512821324169636\n",
      "Epoch 2326, Loss: 0.03397405776195228, Final Batch Loss: 0.0004430681001394987\n",
      "Epoch 2327, Loss: 0.02589182490191888, Final Batch Loss: 0.010682797059416771\n",
      "Epoch 2328, Loss: 0.021151922032004222, Final Batch Loss: 0.013538416475057602\n",
      "Epoch 2329, Loss: 0.01546971988864243, Final Batch Loss: 0.0016238931566476822\n",
      "Epoch 2330, Loss: 0.007523806183598936, Final Batch Loss: 0.0010410641552880406\n",
      "Epoch 2331, Loss: 0.04432244674535468, Final Batch Loss: 0.00064187339739874\n",
      "Epoch 2332, Loss: 0.11030846531502903, Final Batch Loss: 0.033198703080415726\n",
      "Epoch 2333, Loss: 0.014500284567475319, Final Batch Loss: 0.0010742482263594866\n",
      "Epoch 2334, Loss: 0.013871993054635823, Final Batch Loss: 0.0017228414071723819\n",
      "Epoch 2335, Loss: 0.025601908680982888, Final Batch Loss: 0.002187560312449932\n",
      "Epoch 2336, Loss: 0.011857776175020263, Final Batch Loss: 0.002529701916500926\n",
      "Epoch 2337, Loss: 0.03181825127103366, Final Batch Loss: 0.0117160240188241\n",
      "Epoch 2338, Loss: 0.022780267521739006, Final Batch Loss: 0.0033970451913774014\n",
      "Epoch 2339, Loss: 0.1038486072793603, Final Batch Loss: 0.01815026067197323\n",
      "Epoch 2340, Loss: 0.026069678366184235, Final Batch Loss: 0.002434029709547758\n",
      "Epoch 2341, Loss: 0.04814601264661178, Final Batch Loss: 0.0008856248459778726\n",
      "Epoch 2342, Loss: 0.058559350509312935, Final Batch Loss: 0.0001838531024986878\n",
      "Epoch 2343, Loss: 0.05105912883300334, Final Batch Loss: 0.01820419169962406\n",
      "Epoch 2344, Loss: 0.03716617508325726, Final Batch Loss: 0.013768795877695084\n",
      "Epoch 2345, Loss: 0.04241315764375031, Final Batch Loss: 0.018958643078804016\n",
      "Epoch 2346, Loss: 0.024661261239089072, Final Batch Loss: 0.002054495271295309\n",
      "Epoch 2347, Loss: 0.05643435969250277, Final Batch Loss: 0.0010545908007770777\n",
      "Epoch 2348, Loss: 0.07301021576859057, Final Batch Loss: 0.00013302545994520187\n",
      "Epoch 2349, Loss: 0.02341227198485285, Final Batch Loss: 0.0016044190851971507\n",
      "Epoch 2350, Loss: 0.032430070103146136, Final Batch Loss: 0.003346857847645879\n",
      "Epoch 2351, Loss: 0.047591046430170536, Final Batch Loss: 0.030688980594277382\n",
      "Epoch 2352, Loss: 0.034507364965975285, Final Batch Loss: 0.0020950904581695795\n",
      "Epoch 2353, Loss: 0.01980374073900748, Final Batch Loss: 0.0013635099167004228\n",
      "Epoch 2354, Loss: 0.019914761534892023, Final Batch Loss: 0.005015891510993242\n",
      "Epoch 2355, Loss: 0.016464600630570203, Final Batch Loss: 0.0008520880364812911\n",
      "Epoch 2356, Loss: 0.008394294825848192, Final Batch Loss: 0.0004180995747447014\n",
      "Epoch 2357, Loss: 0.03256873122882098, Final Batch Loss: 0.001540374825708568\n",
      "Epoch 2358, Loss: 0.02210989606101066, Final Batch Loss: 0.008332562632858753\n",
      "Epoch 2359, Loss: 0.03263248087023385, Final Batch Loss: 0.0021802829578518867\n",
      "Epoch 2360, Loss: 0.025419687619432807, Final Batch Loss: 0.0005007535219192505\n",
      "Epoch 2361, Loss: 0.039774491684511304, Final Batch Loss: 0.025092976167798042\n",
      "Epoch 2362, Loss: 0.03297763445880264, Final Batch Loss: 0.016044512391090393\n",
      "Epoch 2363, Loss: 0.0089693998452276, Final Batch Loss: 0.0018507092026993632\n",
      "Epoch 2364, Loss: 0.033387356030289084, Final Batch Loss: 0.018381420522928238\n",
      "Epoch 2365, Loss: 0.03815762838348746, Final Batch Loss: 0.0034623725805431604\n",
      "Epoch 2366, Loss: 0.006510409264592454, Final Batch Loss: 0.00024158283486030996\n",
      "Epoch 2367, Loss: 0.04947341384831816, Final Batch Loss: 0.010263359174132347\n",
      "Epoch 2368, Loss: 0.028141205839347094, Final Batch Loss: 0.00033313367748633027\n",
      "Epoch 2369, Loss: 0.023231507744640112, Final Batch Loss: 0.0072012245655059814\n",
      "Epoch 2370, Loss: 0.017425988451577723, Final Batch Loss: 0.0016389806987717748\n",
      "Epoch 2371, Loss: 0.009135072163189761, Final Batch Loss: 0.0001796145661501214\n",
      "Epoch 2372, Loss: 0.008842381415888667, Final Batch Loss: 0.0034849767107516527\n",
      "Epoch 2373, Loss: 0.026564518339000642, Final Batch Loss: 0.0005617827409878373\n",
      "Epoch 2374, Loss: 0.05822119704680517, Final Batch Loss: 0.011360782198607922\n",
      "Epoch 2375, Loss: 0.013512825011275709, Final Batch Loss: 0.003320296062156558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2376, Loss: 0.019056697434280068, Final Batch Loss: 0.0024444623850286007\n",
      "Epoch 2377, Loss: 0.009766263945493847, Final Batch Loss: 0.0005472705815918744\n",
      "Epoch 2378, Loss: 0.0408499576151371, Final Batch Loss: 0.003247868735343218\n",
      "Epoch 2379, Loss: 0.05225222760782344, Final Batch Loss: 8.91985182533972e-05\n",
      "Epoch 2380, Loss: 0.009400301380082965, Final Batch Loss: 0.0006650248542428017\n",
      "Epoch 2381, Loss: 0.03025702794548124, Final Batch Loss: 0.01525292918086052\n",
      "Epoch 2382, Loss: 0.011716869805241004, Final Batch Loss: 0.00041558631346561015\n",
      "Epoch 2383, Loss: 0.034321404760703444, Final Batch Loss: 0.008539904840290546\n",
      "Epoch 2384, Loss: 0.06456117081688717, Final Batch Loss: 0.05519998446106911\n",
      "Epoch 2385, Loss: 0.011401737108826637, Final Batch Loss: 0.001325259800069034\n",
      "Epoch 2386, Loss: 0.024640161427669227, Final Batch Loss: 0.014725206419825554\n",
      "Epoch 2387, Loss: 0.05691179045243189, Final Batch Loss: 0.00040373712545260787\n",
      "Epoch 2388, Loss: 0.022365089447703212, Final Batch Loss: 0.00030107825295999646\n",
      "Epoch 2389, Loss: 0.025512064341455698, Final Batch Loss: 0.005418215878307819\n",
      "Epoch 2390, Loss: 0.05779140640515834, Final Batch Loss: 0.021965162828564644\n",
      "Epoch 2391, Loss: 0.024939639028161764, Final Batch Loss: 0.003402445465326309\n",
      "Epoch 2392, Loss: 0.035363640839932486, Final Batch Loss: 0.0002599727304186672\n",
      "Epoch 2393, Loss: 0.06795426597818732, Final Batch Loss: 0.03354286774992943\n",
      "Epoch 2394, Loss: 0.028386008707457222, Final Batch Loss: 0.0023480441886931658\n",
      "Epoch 2395, Loss: 0.040936128178145736, Final Batch Loss: 0.0019201944814994931\n",
      "Epoch 2396, Loss: 0.027812586864456534, Final Batch Loss: 0.006956472992897034\n",
      "Epoch 2397, Loss: 0.015065926185343415, Final Batch Loss: 0.006933519151061773\n",
      "Epoch 2398, Loss: 0.010001954389736056, Final Batch Loss: 0.0026085232384502888\n",
      "Epoch 2399, Loss: 0.022865240462124348, Final Batch Loss: 0.0018877548864111304\n",
      "Epoch 2400, Loss: 0.02673204694292508, Final Batch Loss: 0.00048535826499573886\n",
      "Epoch 2401, Loss: 0.01226588268764317, Final Batch Loss: 0.0015609023394063115\n",
      "Epoch 2402, Loss: 0.00537522992817685, Final Batch Loss: 0.0002970044151879847\n",
      "Epoch 2403, Loss: 0.013449404912535101, Final Batch Loss: 0.0011753016151487827\n",
      "Epoch 2404, Loss: 0.021374410658609122, Final Batch Loss: 0.0007573479670099914\n",
      "Epoch 2405, Loss: 0.014055830659344792, Final Batch Loss: 0.00014904607087373734\n",
      "Epoch 2406, Loss: 0.03676838253159076, Final Batch Loss: 0.0015373056521639228\n",
      "Epoch 2407, Loss: 0.06946417829021811, Final Batch Loss: 0.002454115077853203\n",
      "Epoch 2408, Loss: 0.007149688724894077, Final Batch Loss: 0.0009998927125707269\n",
      "Epoch 2409, Loss: 0.023210730054415762, Final Batch Loss: 0.001044691656716168\n",
      "Epoch 2410, Loss: 0.007881558834924363, Final Batch Loss: 0.00018109170196112245\n",
      "Epoch 2411, Loss: 0.06101068895077333, Final Batch Loss: 0.008157632313668728\n",
      "Epoch 2412, Loss: 0.01155304879648611, Final Batch Loss: 0.00037941927439533174\n",
      "Epoch 2413, Loss: 0.013328767498023808, Final Batch Loss: 0.002610655501484871\n",
      "Epoch 2414, Loss: 0.006893840269185603, Final Batch Loss: 0.0014510252512991428\n",
      "Epoch 2415, Loss: 0.06875241990201175, Final Batch Loss: 0.0335502065718174\n",
      "Epoch 2416, Loss: 0.06645085720811039, Final Batch Loss: 0.018693121150135994\n",
      "Epoch 2417, Loss: 0.006542504881508648, Final Batch Loss: 0.0012730252929031849\n",
      "Epoch 2418, Loss: 0.011168434168212116, Final Batch Loss: 0.0008750184788368642\n",
      "Epoch 2419, Loss: 0.0340635534375906, Final Batch Loss: 0.0018577215960249305\n",
      "Epoch 2420, Loss: 0.022872988745803013, Final Batch Loss: 0.005100671201944351\n",
      "Epoch 2421, Loss: 0.011635756440227851, Final Batch Loss: 0.0051050372421741486\n",
      "Epoch 2422, Loss: 0.037261025165207684, Final Batch Loss: 0.007591735105961561\n",
      "Epoch 2423, Loss: 0.02622550819069147, Final Batch Loss: 0.014788204804062843\n",
      "Epoch 2424, Loss: 0.016003455908503383, Final Batch Loss: 0.0008819396025501192\n",
      "Epoch 2425, Loss: 0.012175227981060743, Final Batch Loss: 0.003258239943534136\n",
      "Epoch 2426, Loss: 0.015056279487907887, Final Batch Loss: 0.005687172058969736\n",
      "Epoch 2427, Loss: 0.02091934601776302, Final Batch Loss: 0.00406067818403244\n",
      "Epoch 2428, Loss: 0.0231765866628848, Final Batch Loss: 0.019960295408964157\n",
      "Epoch 2429, Loss: 0.00616467246436514, Final Batch Loss: 0.0003883516474161297\n",
      "Epoch 2430, Loss: 0.031148803973337635, Final Batch Loss: 0.0003974450228270143\n",
      "Epoch 2431, Loss: 0.015798587162862532, Final Batch Loss: 0.0001956099149538204\n",
      "Epoch 2432, Loss: 0.049055869167204946, Final Batch Loss: 0.0015510530211031437\n",
      "Epoch 2433, Loss: 0.027120244296384044, Final Batch Loss: 0.00019239222456235439\n",
      "Epoch 2434, Loss: 0.01431885123020038, Final Batch Loss: 0.0005754196899943054\n",
      "Epoch 2435, Loss: 0.015081548364832997, Final Batch Loss: 0.0015340510290116072\n",
      "Epoch 2436, Loss: 0.007939844159409404, Final Batch Loss: 0.0019846372306346893\n",
      "Epoch 2437, Loss: 0.017931235051946715, Final Batch Loss: 0.0007396702421829104\n",
      "Epoch 2438, Loss: 0.013569068105425686, Final Batch Loss: 0.0005228868685662746\n",
      "Epoch 2439, Loss: 0.015580923995003104, Final Batch Loss: 0.0007525637047365308\n",
      "Epoch 2440, Loss: 0.023563868482597172, Final Batch Loss: 0.012787483632564545\n",
      "Epoch 2441, Loss: 0.017851229524239898, Final Batch Loss: 0.00031021388713270426\n",
      "Epoch 2442, Loss: 0.062379920622333884, Final Batch Loss: 0.001176434918306768\n",
      "Epoch 2443, Loss: 0.03894846641924232, Final Batch Loss: 0.00041346324724145234\n",
      "Epoch 2444, Loss: 0.07374864013399929, Final Batch Loss: 0.06811122596263885\n",
      "Epoch 2445, Loss: 0.007139994093449786, Final Batch Loss: 0.001700356020592153\n",
      "Epoch 2446, Loss: 0.011183419381268322, Final Batch Loss: 0.0007974137552082539\n",
      "Epoch 2447, Loss: 0.017501769762020558, Final Batch Loss: 0.0008049907628446817\n",
      "Epoch 2448, Loss: 0.009739815257489681, Final Batch Loss: 0.002552767749875784\n",
      "Epoch 2449, Loss: 0.03173613059334457, Final Batch Loss: 0.0016536146868020296\n",
      "Epoch 2450, Loss: 0.013088416832033545, Final Batch Loss: 0.001582417986355722\n",
      "Epoch 2451, Loss: 0.016941278241574764, Final Batch Loss: 0.0039544543251395226\n",
      "Epoch 2452, Loss: 0.011544646986294538, Final Batch Loss: 0.0002556107356213033\n",
      "Epoch 2453, Loss: 0.03233320458093658, Final Batch Loss: 0.0009600286721251905\n",
      "Epoch 2454, Loss: 0.033191554961376823, Final Batch Loss: 0.002935840282589197\n",
      "Epoch 2455, Loss: 0.029731790724326856, Final Batch Loss: 0.00016325929027516395\n",
      "Epoch 2456, Loss: 0.03372019319795072, Final Batch Loss: 0.02428537607192993\n",
      "Epoch 2457, Loss: 0.0830146157531999, Final Batch Loss: 0.04218510165810585\n",
      "Epoch 2458, Loss: 0.09254646769113606, Final Batch Loss: 9.622543439036235e-05\n",
      "Epoch 2459, Loss: 0.058880017371848226, Final Batch Loss: 0.034796733409166336\n",
      "Epoch 2460, Loss: 0.01245545782148838, Final Batch Loss: 0.0033534348476678133\n",
      "Epoch 2461, Loss: 0.05012317653745413, Final Batch Loss: 0.013749232515692711\n",
      "Epoch 2462, Loss: 0.0645272167166695, Final Batch Loss: 0.001635972992517054\n",
      "Epoch 2463, Loss: 0.051204435352701694, Final Batch Loss: 0.0004686769680120051\n",
      "Epoch 2464, Loss: 0.01795129245147109, Final Batch Loss: 0.002476391149684787\n",
      "Epoch 2465, Loss: 0.018532505579059944, Final Batch Loss: 0.0002573932579252869\n",
      "Epoch 2466, Loss: 0.03376402967842296, Final Batch Loss: 0.0026185368187725544\n",
      "Epoch 2467, Loss: 0.02613977837609127, Final Batch Loss: 0.014266451820731163\n",
      "Epoch 2468, Loss: 0.029878803936298937, Final Batch Loss: 0.0007774272817187011\n",
      "Epoch 2469, Loss: 0.028381177922710776, Final Batch Loss: 0.0013055657036602497\n",
      "Epoch 2470, Loss: 0.042621630476787686, Final Batch Loss: 0.006675264798104763\n",
      "Epoch 2471, Loss: 0.017391431611031294, Final Batch Loss: 0.0012229490093886852\n",
      "Epoch 2472, Loss: 0.01599690323928371, Final Batch Loss: 0.0006142123020254076\n",
      "Epoch 2473, Loss: 0.019696386851137504, Final Batch Loss: 0.007156721316277981\n",
      "Epoch 2474, Loss: 0.03166260709986091, Final Batch Loss: 0.01541741844266653\n",
      "Epoch 2475, Loss: 0.016685537877492607, Final Batch Loss: 0.0031699305400252342\n",
      "Epoch 2476, Loss: 0.032392897323006764, Final Batch Loss: 0.00604053121060133\n",
      "Epoch 2477, Loss: 0.012028137332890765, Final Batch Loss: 1.635297121538315e-05\n",
      "Epoch 2478, Loss: 0.009904015591018833, Final Batch Loss: 6.92868052283302e-05\n",
      "Epoch 2479, Loss: 0.10884640912991017, Final Batch Loss: 0.008576514199376106\n",
      "Epoch 2480, Loss: 0.017992137232795358, Final Batch Loss: 0.0011508328607305884\n",
      "Epoch 2481, Loss: 0.0921358201012481, Final Batch Loss: 0.020672300830483437\n",
      "Epoch 2482, Loss: 0.02226530401094351, Final Batch Loss: 0.00015207928663585335\n",
      "Epoch 2483, Loss: 0.024919150047935545, Final Batch Loss: 0.007401341572403908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2484, Loss: 0.011912578920600936, Final Batch Loss: 0.0002461026015225798\n",
      "Epoch 2485, Loss: 0.025546669377945364, Final Batch Loss: 0.0012864720774814487\n",
      "Epoch 2486, Loss: 0.023277795655303635, Final Batch Loss: 0.012474858202040195\n",
      "Epoch 2487, Loss: 0.018058899004245177, Final Batch Loss: 0.002557809464633465\n",
      "Epoch 2488, Loss: 0.02575260098092258, Final Batch Loss: 0.015299741178750992\n",
      "Epoch 2489, Loss: 0.03583122370764613, Final Batch Loss: 0.0013142311945557594\n",
      "Epoch 2490, Loss: 0.037326368619687855, Final Batch Loss: 0.003933172207325697\n",
      "Epoch 2491, Loss: 0.012229532701894641, Final Batch Loss: 0.0032391948625445366\n",
      "Epoch 2492, Loss: 0.036986235209042206, Final Batch Loss: 0.0006221549701876938\n",
      "Epoch 2493, Loss: 0.020329648221377283, Final Batch Loss: 0.0038620857521891594\n",
      "Epoch 2494, Loss: 0.05208519421285018, Final Batch Loss: 0.0006278286455199122\n",
      "Epoch 2495, Loss: 0.030913018039427698, Final Batch Loss: 0.0009526110952720046\n",
      "Epoch 2496, Loss: 0.020497246063314378, Final Batch Loss: 0.0006489785155281425\n",
      "Epoch 2497, Loss: 0.022608976578339934, Final Batch Loss: 0.002011326840147376\n",
      "Epoch 2498, Loss: 0.01589023627457209, Final Batch Loss: 0.0033715954050421715\n",
      "Epoch 2499, Loss: 0.018789066118188202, Final Batch Loss: 0.0026641979347914457\n",
      "Epoch 2500, Loss: 0.06162327207857743, Final Batch Loss: 0.055973801761865616\n",
      "Epoch 2501, Loss: 0.024034271016716957, Final Batch Loss: 0.007356169167906046\n",
      "Epoch 2502, Loss: 0.004472220214665867, Final Batch Loss: 0.000515552528668195\n",
      "Epoch 2503, Loss: 0.050192488299217075, Final Batch Loss: 0.008661719970405102\n",
      "Epoch 2504, Loss: 0.029842798074241728, Final Batch Loss: 0.01681527867913246\n",
      "Epoch 2505, Loss: 0.03717350907390937, Final Batch Loss: 0.0003608916304074228\n",
      "Epoch 2506, Loss: 0.018596444744616747, Final Batch Loss: 0.011714151129126549\n",
      "Epoch 2507, Loss: 0.007330085296416655, Final Batch Loss: 0.003154715523123741\n",
      "Epoch 2508, Loss: 0.010675481345970184, Final Batch Loss: 0.0025679203681647778\n",
      "Epoch 2509, Loss: 0.025402381521416828, Final Batch Loss: 0.00036919364356435835\n",
      "Epoch 2510, Loss: 0.021619011342409067, Final Batch Loss: 0.00020059796224813908\n",
      "Epoch 2511, Loss: 0.03945408697472885, Final Batch Loss: 0.003635810688138008\n",
      "Epoch 2512, Loss: 0.009944753313902766, Final Batch Loss: 0.0004739928408525884\n",
      "Epoch 2513, Loss: 0.03725093780667521, Final Batch Loss: 0.0005691549158655107\n",
      "Epoch 2514, Loss: 0.02918575529474765, Final Batch Loss: 0.003921546507626772\n",
      "Epoch 2515, Loss: 0.013260070452815853, Final Batch Loss: 0.004953206516802311\n",
      "Epoch 2516, Loss: 0.017347691173199564, Final Batch Loss: 0.009903800673782825\n",
      "Epoch 2517, Loss: 0.00800468257511966, Final Batch Loss: 0.0003619275230448693\n",
      "Epoch 2518, Loss: 0.01937249008915387, Final Batch Loss: 0.002487091813236475\n",
      "Epoch 2519, Loss: 0.01849749992834404, Final Batch Loss: 0.00023890921147540212\n",
      "Epoch 2520, Loss: 0.022383014278602786, Final Batch Loss: 0.01395364012569189\n",
      "Epoch 2521, Loss: 0.007726799551164731, Final Batch Loss: 0.00014033951447345316\n",
      "Epoch 2522, Loss: 0.014205614013917511, Final Batch Loss: 4.322689710534178e-05\n",
      "Epoch 2523, Loss: 0.01713541737990454, Final Batch Loss: 0.0001790414098650217\n",
      "Epoch 2524, Loss: 0.062364948535105214, Final Batch Loss: 0.020288489758968353\n",
      "Epoch 2525, Loss: 0.03003923362120986, Final Batch Loss: 0.011014540679752827\n",
      "Epoch 2526, Loss: 0.04749860498122871, Final Batch Loss: 0.0029528667218983173\n",
      "Epoch 2527, Loss: 0.017288214497966692, Final Batch Loss: 0.006494860630482435\n",
      "Epoch 2528, Loss: 0.032042030536103994, Final Batch Loss: 0.00531392078846693\n",
      "Epoch 2529, Loss: 0.09631661442108452, Final Batch Loss: 0.02446414716541767\n",
      "Epoch 2530, Loss: 0.053349835390690714, Final Batch Loss: 0.0009311915491707623\n",
      "Epoch 2531, Loss: 0.011503727990202606, Final Batch Loss: 0.0028211791068315506\n",
      "Epoch 2532, Loss: 0.027398038597311825, Final Batch Loss: 0.00044523278484120965\n",
      "Epoch 2533, Loss: 0.03480196511372924, Final Batch Loss: 0.0047422475181519985\n",
      "Epoch 2534, Loss: 0.01614230254199356, Final Batch Loss: 0.012727134861052036\n",
      "Epoch 2535, Loss: 0.023855820763856173, Final Batch Loss: 0.0034264607820659876\n",
      "Epoch 2536, Loss: 0.020038609218318015, Final Batch Loss: 0.008457736112177372\n",
      "Epoch 2537, Loss: 0.020831049347179942, Final Batch Loss: 0.00021818354434799403\n",
      "Epoch 2538, Loss: 0.04646072746254504, Final Batch Loss: 0.007334741298109293\n",
      "Epoch 2539, Loss: 0.02087477359600598, Final Batch Loss: 0.00042498044786043465\n",
      "Epoch 2540, Loss: 0.009485258371569216, Final Batch Loss: 0.001529667410068214\n",
      "Epoch 2541, Loss: 0.06713894486892968, Final Batch Loss: 0.005836602300405502\n",
      "Epoch 2542, Loss: 0.014926418400136754, Final Batch Loss: 0.0024736335035413504\n",
      "Epoch 2543, Loss: 0.011066076927818358, Final Batch Loss: 0.005836499854922295\n",
      "Epoch 2544, Loss: 0.02405996245215647, Final Batch Loss: 0.00028932522400282323\n",
      "Epoch 2545, Loss: 0.01894274918595329, Final Batch Loss: 0.00421789288520813\n",
      "Epoch 2546, Loss: 0.012185368512291461, Final Batch Loss: 0.0031557432375848293\n",
      "Epoch 2547, Loss: 0.02007082590716891, Final Batch Loss: 0.0010144462576135993\n",
      "Epoch 2548, Loss: 0.04940921429079026, Final Batch Loss: 0.008622425608336926\n",
      "Epoch 2549, Loss: 0.016172755626030266, Final Batch Loss: 0.0010710733477026224\n",
      "Epoch 2550, Loss: 0.0334330186014995, Final Batch Loss: 0.0009101477335207164\n",
      "Epoch 2551, Loss: 0.019499811198329553, Final Batch Loss: 0.0006980207981541753\n",
      "Epoch 2552, Loss: 0.014459248748607934, Final Batch Loss: 0.0019910060800611973\n",
      "Epoch 2553, Loss: 0.010344966693082824, Final Batch Loss: 0.0017371070571243763\n",
      "Epoch 2554, Loss: 0.020630795508623123, Final Batch Loss: 0.002712515415623784\n",
      "Epoch 2555, Loss: 0.005953802465228364, Final Batch Loss: 0.001393968123011291\n",
      "Epoch 2556, Loss: 0.011820446234196424, Final Batch Loss: 0.006879070773720741\n",
      "Epoch 2557, Loss: 0.006701928214170039, Final Batch Loss: 0.001072628889232874\n",
      "Epoch 2558, Loss: 0.032308081979863346, Final Batch Loss: 0.011447974480688572\n",
      "Epoch 2559, Loss: 0.021846290008397773, Final Batch Loss: 0.004457635805010796\n",
      "Epoch 2560, Loss: 0.01174957788316533, Final Batch Loss: 0.00037748331669718027\n",
      "Epoch 2561, Loss: 0.024483815068379045, Final Batch Loss: 0.012917103245854378\n",
      "Epoch 2562, Loss: 0.0217516906268429, Final Batch Loss: 0.00030201164190657437\n",
      "Epoch 2563, Loss: 0.039654811000218615, Final Batch Loss: 0.0004086789267603308\n",
      "Epoch 2564, Loss: 0.014460746780969203, Final Batch Loss: 0.0015974986599758267\n",
      "Epoch 2565, Loss: 0.009372905013151467, Final Batch Loss: 0.0012670481810346246\n",
      "Epoch 2566, Loss: 0.025078261940507218, Final Batch Loss: 0.0023645670153200626\n",
      "Epoch 2567, Loss: 0.04236603685421869, Final Batch Loss: 0.00034350104397162795\n",
      "Epoch 2568, Loss: 0.013101847842335701, Final Batch Loss: 0.0018046485492959619\n",
      "Epoch 2569, Loss: 0.08171813079388812, Final Batch Loss: 0.000955937139224261\n",
      "Epoch 2570, Loss: 0.027488403487950563, Final Batch Loss: 0.009905926883220673\n",
      "Epoch 2571, Loss: 0.026515440549701452, Final Batch Loss: 0.0005019617383368313\n",
      "Epoch 2572, Loss: 0.01942247168335598, Final Batch Loss: 0.002427796833217144\n",
      "Epoch 2573, Loss: 0.018395411389064975, Final Batch Loss: 0.00019444474310148507\n",
      "Epoch 2574, Loss: 0.0170956565416418, Final Batch Loss: 0.00988127663731575\n",
      "Epoch 2575, Loss: 0.05977330368477851, Final Batch Loss: 0.034728944301605225\n",
      "Epoch 2576, Loss: 0.019784889998845756, Final Batch Loss: 0.0014413661556318402\n",
      "Epoch 2577, Loss: 0.017706223530694842, Final Batch Loss: 0.009685376659035683\n",
      "Epoch 2578, Loss: 0.022203203610843047, Final Batch Loss: 0.0002009194577112794\n",
      "Epoch 2579, Loss: 0.02528032468399033, Final Batch Loss: 0.0028658632654696703\n",
      "Epoch 2580, Loss: 0.028475334227550775, Final Batch Loss: 0.010733217000961304\n",
      "Epoch 2581, Loss: 0.0317180217243731, Final Batch Loss: 0.006090859416872263\n",
      "Epoch 2582, Loss: 0.0209247637540102, Final Batch Loss: 0.0014688855735585093\n",
      "Epoch 2583, Loss: 0.05132867000065744, Final Batch Loss: 0.017423927783966064\n",
      "Epoch 2584, Loss: 0.020880179828964174, Final Batch Loss: 0.01081220805644989\n",
      "Epoch 2585, Loss: 0.024842606188030913, Final Batch Loss: 0.007745790295302868\n",
      "Epoch 2586, Loss: 0.020486225315835327, Final Batch Loss: 0.0008580576977692544\n",
      "Epoch 2587, Loss: 0.018426044727675617, Final Batch Loss: 0.0026722836773842573\n",
      "Epoch 2588, Loss: 0.01826399384299293, Final Batch Loss: 0.003434317884966731\n",
      "Epoch 2589, Loss: 0.02820817768224515, Final Batch Loss: 0.0004574623017106205\n",
      "Epoch 2590, Loss: 0.007602689671330154, Final Batch Loss: 0.0013994112377986312\n",
      "Epoch 2591, Loss: 0.016885954362805933, Final Batch Loss: 0.008059217594563961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2592, Loss: 0.003088604047661647, Final Batch Loss: 0.000942450889851898\n",
      "Epoch 2593, Loss: 0.016777092561824247, Final Batch Loss: 0.004736434202641249\n",
      "Epoch 2594, Loss: 0.014079066051635891, Final Batch Loss: 0.0012401146814227104\n",
      "Epoch 2595, Loss: 0.012604264222318307, Final Batch Loss: 0.0008155775722116232\n",
      "Epoch 2596, Loss: 0.02663165888225194, Final Batch Loss: 0.00010740825382526964\n",
      "Epoch 2597, Loss: 0.03703581425361335, Final Batch Loss: 0.0015937953721731901\n",
      "Epoch 2598, Loss: 0.00861339713446796, Final Batch Loss: 0.0016358847497031093\n",
      "Epoch 2599, Loss: 0.017233566963113844, Final Batch Loss: 0.0030104017350822687\n",
      "Epoch 2600, Loss: 0.008378783022635616, Final Batch Loss: 0.00017981963173951954\n",
      "Epoch 2601, Loss: 0.017889504437334836, Final Batch Loss: 0.010127496905624866\n",
      "Epoch 2602, Loss: 0.00750786712160334, Final Batch Loss: 0.000937780539970845\n",
      "Epoch 2603, Loss: 0.03637384681496769, Final Batch Loss: 0.0007053801091387868\n",
      "Epoch 2604, Loss: 0.012389211769914255, Final Batch Loss: 0.00030497947591356933\n",
      "Epoch 2605, Loss: 0.006746316415956244, Final Batch Loss: 0.0010943635134026408\n",
      "Epoch 2606, Loss: 0.003160884720273316, Final Batch Loss: 0.0009350830223411322\n",
      "Epoch 2607, Loss: 0.014992847354733385, Final Batch Loss: 0.0004503749660216272\n",
      "Epoch 2608, Loss: 0.01567770778638078, Final Batch Loss: 0.010794220492243767\n",
      "Epoch 2609, Loss: 0.005427486918051727, Final Batch Loss: 0.0033126100897789\n",
      "Epoch 2610, Loss: 0.004279122789739631, Final Batch Loss: 0.00012507180508691818\n",
      "Epoch 2611, Loss: 0.004903640885459026, Final Batch Loss: 3.96097129851114e-05\n",
      "Epoch 2612, Loss: 0.023139673750847578, Final Batch Loss: 0.011459019966423512\n",
      "Epoch 2613, Loss: 0.03986093233106658, Final Batch Loss: 0.008505825884640217\n",
      "Epoch 2614, Loss: 0.01568322704406455, Final Batch Loss: 0.003904889337718487\n",
      "Epoch 2615, Loss: 0.03619675827212632, Final Batch Loss: 0.002340904902666807\n",
      "Epoch 2616, Loss: 0.06896599475294352, Final Batch Loss: 0.0457463264465332\n",
      "Epoch 2617, Loss: 0.041005255334312096, Final Batch Loss: 0.005638353526592255\n",
      "Epoch 2618, Loss: 0.008280684894998558, Final Batch Loss: 0.00012700437218882143\n",
      "Epoch 2619, Loss: 0.018789682362694293, Final Batch Loss: 0.00018972993711940944\n",
      "Epoch 2620, Loss: 0.017358320474158973, Final Batch Loss: 0.0007430341793224216\n",
      "Epoch 2621, Loss: 0.03896520403213799, Final Batch Loss: 0.0015125522622838616\n",
      "Epoch 2622, Loss: 0.00818994437577203, Final Batch Loss: 0.0006108293891884387\n",
      "Epoch 2623, Loss: 0.02813891692494508, Final Batch Loss: 0.00022341786825563759\n",
      "Epoch 2624, Loss: 0.024650365696288645, Final Batch Loss: 0.001383293536491692\n",
      "Epoch 2625, Loss: 0.035640687216073275, Final Batch Loss: 0.002683441387489438\n",
      "Epoch 2626, Loss: 0.022849482658784837, Final Batch Loss: 0.000801422051154077\n",
      "Epoch 2627, Loss: 0.013217435916885734, Final Batch Loss: 0.0023289043456315994\n",
      "Epoch 2628, Loss: 0.04068535410624463, Final Batch Loss: 0.00014356589235831052\n",
      "Epoch 2629, Loss: 0.014087850198848173, Final Batch Loss: 0.0007508285925723612\n",
      "Epoch 2630, Loss: 0.024202576954849064, Final Batch Loss: 0.00984749011695385\n",
      "Epoch 2631, Loss: 0.012945156573550776, Final Batch Loss: 0.00037828602944500744\n",
      "Epoch 2632, Loss: 0.026217903767246753, Final Batch Loss: 0.001333458349108696\n",
      "Epoch 2633, Loss: 0.04585773282451555, Final Batch Loss: 0.007864252664148808\n",
      "Epoch 2634, Loss: 0.0335765519994311, Final Batch Loss: 0.00025594577891752124\n",
      "Epoch 2635, Loss: 0.016675517079420388, Final Batch Loss: 0.005336437840014696\n",
      "Epoch 2636, Loss: 0.019442026779870503, Final Batch Loss: 0.00014917941007297486\n",
      "Epoch 2637, Loss: 0.010511871165363118, Final Batch Loss: 0.0004680475394707173\n",
      "Epoch 2638, Loss: 0.01693054905626923, Final Batch Loss: 0.00022976953187026083\n",
      "Epoch 2639, Loss: 0.056241690763272345, Final Batch Loss: 0.0037947280798107386\n",
      "Epoch 2640, Loss: 0.049918063305085525, Final Batch Loss: 0.0028691014740616083\n",
      "Epoch 2641, Loss: 0.058892734043183737, Final Batch Loss: 0.00022787197667639703\n",
      "Epoch 2642, Loss: 0.02436151955043897, Final Batch Loss: 0.0008329667034558952\n",
      "Epoch 2643, Loss: 0.021212129155173898, Final Batch Loss: 0.0031244184356182814\n",
      "Epoch 2644, Loss: 0.01914517139084637, Final Batch Loss: 0.0009175502927973866\n",
      "Epoch 2645, Loss: 0.025235492357751355, Final Batch Loss: 0.00022932860883884132\n",
      "Epoch 2646, Loss: 0.039539214223623276, Final Batch Loss: 0.01733550615608692\n",
      "Epoch 2647, Loss: 0.012555179084301926, Final Batch Loss: 0.0016109057469293475\n",
      "Epoch 2648, Loss: 0.0061825813027098775, Final Batch Loss: 0.00336611014790833\n",
      "Epoch 2649, Loss: 0.00920814236451406, Final Batch Loss: 0.00016921917267609388\n",
      "Epoch 2650, Loss: 0.017805209616199136, Final Batch Loss: 0.009603125974535942\n",
      "Epoch 2651, Loss: 0.033190057263709605, Final Batch Loss: 0.0010898192413151264\n",
      "Epoch 2652, Loss: 0.03115429895115085, Final Batch Loss: 0.0003432844823691994\n",
      "Epoch 2653, Loss: 0.010364900372223929, Final Batch Loss: 0.001633407431654632\n",
      "Epoch 2654, Loss: 0.024521839659428224, Final Batch Loss: 0.0006569236284121871\n",
      "Epoch 2655, Loss: 0.021163854864425957, Final Batch Loss: 0.0004568357835523784\n",
      "Epoch 2656, Loss: 0.03320406696002465, Final Batch Loss: 0.0002838289365172386\n",
      "Epoch 2657, Loss: 0.009994596781325527, Final Batch Loss: 0.0027228679973632097\n",
      "Epoch 2658, Loss: 0.03690877713961527, Final Batch Loss: 0.026724018156528473\n",
      "Epoch 2659, Loss: 0.07517333643045276, Final Batch Loss: 0.001352669089101255\n",
      "Epoch 2660, Loss: 0.011691888212226331, Final Batch Loss: 0.0017798529006540775\n",
      "Epoch 2661, Loss: 0.011961448806687258, Final Batch Loss: 0.008121060207486153\n",
      "Epoch 2662, Loss: 0.029169025481678545, Final Batch Loss: 0.0020210498478263617\n",
      "Epoch 2663, Loss: 0.02765799663029611, Final Batch Loss: 0.015373601578176022\n",
      "Epoch 2664, Loss: 0.012425645545590669, Final Batch Loss: 0.0005681966431438923\n",
      "Epoch 2665, Loss: 0.04976269125472754, Final Batch Loss: 0.0008360062493011355\n",
      "Epoch 2666, Loss: 0.04078644677065313, Final Batch Loss: 0.012259680777788162\n",
      "Epoch 2667, Loss: 0.01085751825303305, Final Batch Loss: 0.002141450298950076\n",
      "Epoch 2668, Loss: 0.05737848428543657, Final Batch Loss: 0.019097814336419106\n",
      "Epoch 2669, Loss: 0.06279057313804515, Final Batch Loss: 0.028437452390789986\n",
      "Epoch 2670, Loss: 0.025895685888826847, Final Batch Loss: 0.0006500412710011005\n",
      "Epoch 2671, Loss: 0.03306398476706818, Final Batch Loss: 0.0015255552716553211\n",
      "Epoch 2672, Loss: 0.012107597925933078, Final Batch Loss: 0.00479605607688427\n",
      "Epoch 2673, Loss: 0.018111210389179178, Final Batch Loss: 0.00020820276404265314\n",
      "Epoch 2674, Loss: 0.02545736380852759, Final Batch Loss: 0.0005102664581499994\n",
      "Epoch 2675, Loss: 0.02682426737737842, Final Batch Loss: 0.00027073061210103333\n",
      "Epoch 2676, Loss: 0.006156177172670141, Final Batch Loss: 0.000702033459674567\n",
      "Epoch 2677, Loss: 0.037912630912614986, Final Batch Loss: 0.02250766195356846\n",
      "Epoch 2678, Loss: 0.04561013600323349, Final Batch Loss: 0.0022193538025021553\n",
      "Epoch 2679, Loss: 0.011102618998847902, Final Batch Loss: 0.005194046068936586\n",
      "Epoch 2680, Loss: 0.014948632713640109, Final Batch Loss: 0.0021578073501586914\n",
      "Epoch 2681, Loss: 0.011444906383985654, Final Batch Loss: 0.006009107921272516\n",
      "Epoch 2682, Loss: 0.014264397730585188, Final Batch Loss: 0.003218460362404585\n",
      "Epoch 2683, Loss: 0.018267187610035762, Final Batch Loss: 0.005341771524399519\n",
      "Epoch 2684, Loss: 0.010355192032875493, Final Batch Loss: 0.0003659362264443189\n",
      "Epoch 2685, Loss: 0.014733905903995037, Final Batch Loss: 0.006067087408155203\n",
      "Epoch 2686, Loss: 0.026849588786717504, Final Batch Loss: 0.0010926274117082357\n",
      "Epoch 2687, Loss: 0.014017754845554009, Final Batch Loss: 0.00924097839742899\n",
      "Epoch 2688, Loss: 0.018323850526940078, Final Batch Loss: 0.0029081078246235847\n",
      "Epoch 2689, Loss: 0.023542200404335745, Final Batch Loss: 0.00014603872841689736\n",
      "Epoch 2690, Loss: 0.011276771139819175, Final Batch Loss: 0.0031670427415519953\n",
      "Epoch 2691, Loss: 0.011289823683910072, Final Batch Loss: 0.003786657238379121\n",
      "Epoch 2692, Loss: 0.011952165514230728, Final Batch Loss: 0.0007891814457252622\n",
      "Epoch 2693, Loss: 0.007791327341692522, Final Batch Loss: 0.005433025304228067\n",
      "Epoch 2694, Loss: 0.02464207317098044, Final Batch Loss: 0.0008567902259528637\n",
      "Epoch 2695, Loss: 0.00932556354382541, Final Batch Loss: 0.0005880672251805663\n",
      "Epoch 2696, Loss: 0.02911911642877385, Final Batch Loss: 0.003549265908077359\n",
      "Epoch 2697, Loss: 0.006988509645452723, Final Batch Loss: 0.0005476780934259295\n",
      "Epoch 2698, Loss: 0.010573407605988905, Final Batch Loss: 0.0002188114740420133\n",
      "Epoch 2699, Loss: 0.007153231679694727, Final Batch Loss: 0.00132305221632123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2700, Loss: 0.006529987076646648, Final Batch Loss: 0.003468940267339349\n",
      "Epoch 2701, Loss: 0.004576059975079261, Final Batch Loss: 0.0005557611584663391\n",
      "Epoch 2702, Loss: 0.011497761239297688, Final Batch Loss: 0.004483462776988745\n",
      "Epoch 2703, Loss: 0.023313362849876285, Final Batch Loss: 0.01100719440728426\n",
      "Epoch 2704, Loss: 0.03738673347106669, Final Batch Loss: 0.00021913241653237492\n",
      "Epoch 2705, Loss: 0.03127677796874195, Final Batch Loss: 0.0076343100517988205\n",
      "Epoch 2706, Loss: 0.02066469402052462, Final Batch Loss: 0.002222924493253231\n",
      "Epoch 2707, Loss: 0.030100931762717664, Final Batch Loss: 0.01883215643465519\n",
      "Epoch 2708, Loss: 0.01797792353318073, Final Batch Loss: 0.00046302974806167185\n",
      "Epoch 2709, Loss: 0.05013930826680735, Final Batch Loss: 0.04271876439452171\n",
      "Epoch 2710, Loss: 0.015714412555098534, Final Batch Loss: 0.00117075617890805\n",
      "Epoch 2711, Loss: 0.010519630392082036, Final Batch Loss: 0.001532552414573729\n",
      "Epoch 2712, Loss: 0.01837124940357171, Final Batch Loss: 0.010808735154569149\n",
      "Epoch 2713, Loss: 0.008760042954236269, Final Batch Loss: 0.0029681732412427664\n",
      "Epoch 2714, Loss: 0.012129174618166871, Final Batch Loss: 0.009159548208117485\n",
      "Epoch 2715, Loss: 0.0143788591340126, Final Batch Loss: 1.666354000917636e-05\n",
      "Epoch 2716, Loss: 0.02296795742586255, Final Batch Loss: 0.005340105388313532\n",
      "Epoch 2717, Loss: 0.018901573319453746, Final Batch Loss: 0.003840798046439886\n",
      "Epoch 2718, Loss: 0.012979229635675438, Final Batch Loss: 0.000213300998439081\n",
      "Epoch 2719, Loss: 0.023697832104517147, Final Batch Loss: 0.019547538831830025\n",
      "Epoch 2720, Loss: 0.029200944118201733, Final Batch Loss: 0.00038544778362847865\n",
      "Epoch 2721, Loss: 0.028844954096712172, Final Batch Loss: 0.0005893438355997205\n",
      "Epoch 2722, Loss: 0.015946213359711692, Final Batch Loss: 0.0050928001292049885\n",
      "Epoch 2723, Loss: 0.0345724261860596, Final Batch Loss: 0.0002166378399124369\n",
      "Epoch 2724, Loss: 0.011517024831846356, Final Batch Loss: 0.006106031127274036\n",
      "Epoch 2725, Loss: 0.03330567367083859, Final Batch Loss: 0.0001243734877789393\n",
      "Epoch 2726, Loss: 0.00781248277053237, Final Batch Loss: 0.002255160128697753\n",
      "Epoch 2727, Loss: 0.02948029903927818, Final Batch Loss: 0.014596927911043167\n",
      "Epoch 2728, Loss: 0.07341484777862206, Final Batch Loss: 0.023486396297812462\n",
      "Epoch 2729, Loss: 0.029381598287727684, Final Batch Loss: 0.010205489583313465\n",
      "Epoch 2730, Loss: 0.009793599834665656, Final Batch Loss: 0.003307906212285161\n",
      "Epoch 2731, Loss: 0.02972976362798363, Final Batch Loss: 0.02121601067483425\n",
      "Epoch 2732, Loss: 0.03740129916695878, Final Batch Loss: 0.0005008940934203565\n",
      "Epoch 2733, Loss: 0.010737407093984075, Final Batch Loss: 0.0011843328829854727\n",
      "Epoch 2734, Loss: 0.08124594215769321, Final Batch Loss: 0.033833373337984085\n",
      "Epoch 2735, Loss: 0.05682917288504541, Final Batch Loss: 0.039847660809755325\n",
      "Epoch 2736, Loss: 0.023872320423834026, Final Batch Loss: 0.0016329408390447497\n",
      "Epoch 2737, Loss: 0.008322949899593368, Final Batch Loss: 0.0022581678349524736\n",
      "Epoch 2738, Loss: 0.01857417414430529, Final Batch Loss: 0.0009835485834628344\n",
      "Epoch 2739, Loss: 0.02523016044870019, Final Batch Loss: 0.0036134133115410805\n",
      "Epoch 2740, Loss: 0.02225238410755992, Final Batch Loss: 0.005792451091110706\n",
      "Epoch 2741, Loss: 0.03694975306279957, Final Batch Loss: 0.016422774642705917\n",
      "Epoch 2742, Loss: 0.028199515159940347, Final Batch Loss: 0.01269112154841423\n",
      "Epoch 2743, Loss: 0.013029523892328143, Final Batch Loss: 0.00862486008554697\n",
      "Epoch 2744, Loss: 0.015841094660572708, Final Batch Loss: 0.001366007374599576\n",
      "Epoch 2745, Loss: 0.01374443352688104, Final Batch Loss: 0.0027888843324035406\n",
      "Epoch 2746, Loss: 0.0634044548496604, Final Batch Loss: 0.0013351067900657654\n",
      "Epoch 2747, Loss: 0.02849876624532044, Final Batch Loss: 0.0006818449473939836\n",
      "Epoch 2748, Loss: 0.011535709461895749, Final Batch Loss: 0.0004190057225059718\n",
      "Epoch 2749, Loss: 0.0308627832273487, Final Batch Loss: 0.0037059590686112642\n",
      "Epoch 2750, Loss: 0.06009651336353272, Final Batch Loss: 0.04039842262864113\n",
      "Epoch 2751, Loss: 0.07829205272719264, Final Batch Loss: 0.03487938642501831\n",
      "Epoch 2752, Loss: 0.03785818605683744, Final Batch Loss: 0.002272089710459113\n",
      "Epoch 2753, Loss: 0.0575351407751441, Final Batch Loss: 0.019701311364769936\n",
      "Epoch 2754, Loss: 0.029437536955811083, Final Batch Loss: 0.009942680597305298\n",
      "Epoch 2755, Loss: 0.028021857026033103, Final Batch Loss: 0.0033382291439920664\n",
      "Epoch 2756, Loss: 0.010545146360527724, Final Batch Loss: 0.0012150541879236698\n",
      "Epoch 2757, Loss: 0.027362486551282927, Final Batch Loss: 0.0022073460277169943\n",
      "Epoch 2758, Loss: 0.02012539844145067, Final Batch Loss: 0.005953325890004635\n",
      "Epoch 2759, Loss: 0.0320427508559078, Final Batch Loss: 0.00458027608692646\n",
      "Epoch 2760, Loss: 0.02826286619529128, Final Batch Loss: 0.00547059578821063\n",
      "Epoch 2761, Loss: 0.019634790427517146, Final Batch Loss: 0.0006402719882316887\n",
      "Epoch 2762, Loss: 0.013790012162644416, Final Batch Loss: 0.0012831669300794601\n",
      "Epoch 2763, Loss: 0.028999410569667816, Final Batch Loss: 0.005529399495571852\n",
      "Epoch 2764, Loss: 0.013099716961733066, Final Batch Loss: 9.792983473744243e-05\n",
      "Epoch 2765, Loss: 0.020410412951605394, Final Batch Loss: 0.00021168356761336327\n",
      "Epoch 2766, Loss: 0.010951934033073485, Final Batch Loss: 0.0011231722310185432\n",
      "Epoch 2767, Loss: 0.012052605510689318, Final Batch Loss: 0.0014558861730620265\n",
      "Epoch 2768, Loss: 0.01016738242469728, Final Batch Loss: 0.000814234372228384\n",
      "Epoch 2769, Loss: 0.00990401633316651, Final Batch Loss: 0.0004898866754956543\n",
      "Epoch 2770, Loss: 0.013512261684809346, Final Batch Loss: 0.006676346529275179\n",
      "Epoch 2771, Loss: 0.031107073358725756, Final Batch Loss: 0.00047543738037347794\n",
      "Epoch 2772, Loss: 0.022029709623893723, Final Batch Loss: 0.0002479905670043081\n",
      "Epoch 2773, Loss: 0.04534084157785401, Final Batch Loss: 0.0003408232587389648\n",
      "Epoch 2774, Loss: 0.04547559283673763, Final Batch Loss: 0.023863209411501884\n",
      "Epoch 2775, Loss: 0.02811376377940178, Final Batch Loss: 0.0005007606232538819\n",
      "Epoch 2776, Loss: 0.03549981993273832, Final Batch Loss: 0.002293021883815527\n",
      "Epoch 2777, Loss: 0.0496344055281952, Final Batch Loss: 0.001105459756217897\n",
      "Epoch 2778, Loss: 0.026588909153360873, Final Batch Loss: 0.000559791864361614\n",
      "Epoch 2779, Loss: 0.020913455402478576, Final Batch Loss: 0.007461313623934984\n",
      "Epoch 2780, Loss: 0.02207518636714667, Final Batch Loss: 0.012529841624200344\n",
      "Epoch 2781, Loss: 0.06134560954524204, Final Batch Loss: 0.024679657071828842\n",
      "Epoch 2782, Loss: 0.033996832091361284, Final Batch Loss: 0.0040388405323028564\n",
      "Epoch 2783, Loss: 0.02419761393684894, Final Batch Loss: 0.01052467804402113\n",
      "Epoch 2784, Loss: 0.04235447244718671, Final Batch Loss: 0.007384189870208502\n",
      "Epoch 2785, Loss: 0.01278707166784443, Final Batch Loss: 0.0006348729366436601\n",
      "Epoch 2786, Loss: 0.013022362720221281, Final Batch Loss: 0.003226742148399353\n",
      "Epoch 2787, Loss: 0.00701919365383219, Final Batch Loss: 0.00013233131903689355\n",
      "Epoch 2788, Loss: 0.03290181019110605, Final Batch Loss: 0.0028602750971913338\n",
      "Epoch 2789, Loss: 0.006524243683088571, Final Batch Loss: 0.0004163365811109543\n",
      "Epoch 2790, Loss: 0.01061654050135985, Final Batch Loss: 0.0037211577873677015\n",
      "Epoch 2791, Loss: 0.012188083361252211, Final Batch Loss: 0.001198662561364472\n",
      "Epoch 2792, Loss: 0.043362094234908, Final Batch Loss: 0.00031420556479133666\n",
      "Epoch 2793, Loss: 0.06335474539082497, Final Batch Loss: 0.0018169641261920333\n",
      "Epoch 2794, Loss: 0.032551603158935905, Final Batch Loss: 0.015982937067747116\n",
      "Epoch 2795, Loss: 0.016350242105545476, Final Batch Loss: 0.0004128583532292396\n",
      "Epoch 2796, Loss: 0.0311396035249345, Final Batch Loss: 0.022128237411379814\n",
      "Epoch 2797, Loss: 0.028430403326638043, Final Batch Loss: 0.002808662597090006\n",
      "Epoch 2798, Loss: 0.042023702582810074, Final Batch Loss: 0.0005975762032903731\n",
      "Epoch 2799, Loss: 0.008272481354651973, Final Batch Loss: 0.0016460483893752098\n",
      "Epoch 2800, Loss: 0.010427739442093298, Final Batch Loss: 0.0002724347577895969\n",
      "Epoch 2801, Loss: 0.023315953789278865, Final Batch Loss: 0.0007230627816170454\n",
      "Epoch 2802, Loss: 0.02096089260885492, Final Batch Loss: 0.006061377003788948\n",
      "Epoch 2803, Loss: 0.03408914164174348, Final Batch Loss: 0.003348739119246602\n",
      "Epoch 2804, Loss: 0.0084646307877847, Final Batch Loss: 7.478669431293383e-05\n",
      "Epoch 2805, Loss: 0.029235715905088, Final Batch Loss: 0.00011860851373057812\n",
      "Epoch 2806, Loss: 0.0037841268931515515, Final Batch Loss: 0.00029735572752542794\n",
      "Epoch 2807, Loss: 0.017484150564996526, Final Batch Loss: 0.007650969084352255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2808, Loss: 0.012585450531332754, Final Batch Loss: 0.0008429630543105304\n",
      "Epoch 2809, Loss: 0.018234060029499233, Final Batch Loss: 0.007689799182116985\n",
      "Epoch 2810, Loss: 0.031778287375345826, Final Batch Loss: 0.001750980969518423\n",
      "Epoch 2811, Loss: 0.0833735775668174, Final Batch Loss: 0.07679901272058487\n",
      "Epoch 2812, Loss: 0.03852555135381408, Final Batch Loss: 0.00015780681860633194\n",
      "Epoch 2813, Loss: 0.04162323917262256, Final Batch Loss: 0.0018109144875779748\n",
      "Epoch 2814, Loss: 0.049998727976344526, Final Batch Loss: 0.0015957624418660998\n",
      "Epoch 2815, Loss: 0.023477763927076012, Final Batch Loss: 0.0008278466411866248\n",
      "Epoch 2816, Loss: 0.018132446275558323, Final Batch Loss: 0.0056153289042413235\n",
      "Epoch 2817, Loss: 0.029438514029607177, Final Batch Loss: 0.0025909242685884237\n",
      "Epoch 2818, Loss: 0.025860057445243, Final Batch Loss: 0.009286526590585709\n",
      "Epoch 2819, Loss: 0.02342462242813781, Final Batch Loss: 0.004214259330183268\n",
      "Epoch 2820, Loss: 0.011324324703309685, Final Batch Loss: 0.003032833570614457\n",
      "Epoch 2821, Loss: 0.039869065629318357, Final Batch Loss: 0.026947716251015663\n",
      "Epoch 2822, Loss: 0.02632253654883243, Final Batch Loss: 0.00046499192831106484\n",
      "Epoch 2823, Loss: 0.013481098168995231, Final Batch Loss: 0.0022969727870076895\n",
      "Epoch 2824, Loss: 0.022657693552901037, Final Batch Loss: 0.00013545785623136908\n",
      "Epoch 2825, Loss: 0.008465515042189509, Final Batch Loss: 0.0010082601802423596\n",
      "Epoch 2826, Loss: 0.007940719311591238, Final Batch Loss: 0.0020994474180042744\n",
      "Epoch 2827, Loss: 0.017070547910407186, Final Batch Loss: 0.002030909527093172\n",
      "Epoch 2828, Loss: 0.007783473382005468, Final Batch Loss: 0.00025836806162260473\n",
      "Epoch 2829, Loss: 0.01743793406058103, Final Batch Loss: 0.00040385883767157793\n",
      "Epoch 2830, Loss: 0.005771814758190885, Final Batch Loss: 0.00024161109467968345\n",
      "Epoch 2831, Loss: 0.02993876556865871, Final Batch Loss: 0.0010355323320254683\n",
      "Epoch 2832, Loss: 0.013510258984752, Final Batch Loss: 0.00472781527787447\n",
      "Epoch 2833, Loss: 0.022391535283531994, Final Batch Loss: 0.00045406381832435727\n",
      "Epoch 2834, Loss: 0.027852731058374047, Final Batch Loss: 0.0015528594376519322\n",
      "Epoch 2835, Loss: 0.007899411037215032, Final Batch Loss: 0.0001757996215019375\n",
      "Epoch 2836, Loss: 0.011470585654024035, Final Batch Loss: 0.0011504428694024682\n",
      "Epoch 2837, Loss: 0.006660698621999472, Final Batch Loss: 0.0006015894468873739\n",
      "Epoch 2838, Loss: 0.02119187615608098, Final Batch Loss: 8.93038886715658e-05\n",
      "Epoch 2839, Loss: 0.014474103110842407, Final Batch Loss: 0.0007831144612282515\n",
      "Epoch 2840, Loss: 0.0296253347187303, Final Batch Loss: 0.011413084343075752\n",
      "Epoch 2841, Loss: 0.00844383763615042, Final Batch Loss: 0.0019922128412872553\n",
      "Epoch 2842, Loss: 0.0703889571595937, Final Batch Loss: 0.035686541348695755\n",
      "Epoch 2843, Loss: 0.03830953047145158, Final Batch Loss: 0.0006765850121155381\n",
      "Epoch 2844, Loss: 0.021342015272239223, Final Batch Loss: 0.0002475414949003607\n",
      "Epoch 2845, Loss: 0.024321333621628582, Final Batch Loss: 0.000568840594496578\n",
      "Epoch 2846, Loss: 0.01259243258391507, Final Batch Loss: 0.0016110099386423826\n",
      "Epoch 2847, Loss: 0.005350795152480714, Final Batch Loss: 0.0008539292030036449\n",
      "Epoch 2848, Loss: 0.016759278223617002, Final Batch Loss: 0.002497253241017461\n",
      "Epoch 2849, Loss: 0.027713257062714547, Final Batch Loss: 7.470842683687806e-05\n",
      "Epoch 2850, Loss: 0.018881333802710287, Final Batch Loss: 0.00036761784576810896\n",
      "Epoch 2851, Loss: 0.019909557711798698, Final Batch Loss: 0.009044468402862549\n",
      "Epoch 2852, Loss: 0.004366666224086657, Final Batch Loss: 0.00026641544536687434\n",
      "Epoch 2853, Loss: 0.021746737329522148, Final Batch Loss: 0.0003637495974544436\n",
      "Epoch 2854, Loss: 0.0033652250131126493, Final Batch Loss: 0.0006573869613930583\n",
      "Epoch 2855, Loss: 0.004995827446691692, Final Batch Loss: 0.0007010287372395396\n",
      "Epoch 2856, Loss: 0.0029462906095432118, Final Batch Loss: 0.0003198060439899564\n",
      "Epoch 2857, Loss: 0.011968198050453793, Final Batch Loss: 0.00011248594819335267\n",
      "Epoch 2858, Loss: 0.009271447488572448, Final Batch Loss: 0.000954506394919008\n",
      "Epoch 2859, Loss: 0.029747293985565193, Final Batch Loss: 0.0076001412235200405\n",
      "Epoch 2860, Loss: 0.1021476772002643, Final Batch Loss: 0.041130416095256805\n",
      "Epoch 2861, Loss: 0.05203541641822085, Final Batch Loss: 0.017678119242191315\n",
      "Epoch 2862, Loss: 0.019718799681868404, Final Batch Loss: 0.0060880170203745365\n",
      "Epoch 2863, Loss: 0.019879148370819166, Final Batch Loss: 0.00029371734126470983\n",
      "Epoch 2864, Loss: 0.027623921749182045, Final Batch Loss: 0.0030412187334150076\n",
      "Epoch 2865, Loss: 0.07404137623962015, Final Batch Loss: 0.0016418364830315113\n",
      "Epoch 2866, Loss: 0.008867852680850774, Final Batch Loss: 0.0004434122529346496\n",
      "Epoch 2867, Loss: 0.011822177009889856, Final Batch Loss: 0.0007429617689922452\n",
      "Epoch 2868, Loss: 0.01624158746562898, Final Batch Loss: 0.0030856598168611526\n",
      "Epoch 2869, Loss: 0.02790074417134747, Final Batch Loss: 0.014449180103838444\n",
      "Epoch 2870, Loss: 0.011013834300683811, Final Batch Loss: 0.0006493470282293856\n",
      "Epoch 2871, Loss: 0.005958833724434953, Final Batch Loss: 6.487289647338912e-05\n",
      "Epoch 2872, Loss: 0.007390321814455092, Final Batch Loss: 0.0013314498355612159\n",
      "Epoch 2873, Loss: 0.034432210959494114, Final Batch Loss: 0.029088273644447327\n",
      "Epoch 2874, Loss: 0.008108834474114701, Final Batch Loss: 0.000897368707228452\n",
      "Epoch 2875, Loss: 0.007822565297828987, Final Batch Loss: 0.0026811209972947836\n",
      "Epoch 2876, Loss: 0.012893132661702111, Final Batch Loss: 0.00034384243190288544\n",
      "Epoch 2877, Loss: 0.055895036275614984, Final Batch Loss: 0.0001823416823754087\n",
      "Epoch 2878, Loss: 0.03370008472120389, Final Batch Loss: 0.00274702743627131\n",
      "Epoch 2879, Loss: 0.011197599422303028, Final Batch Loss: 0.000127705410704948\n",
      "Epoch 2880, Loss: 0.013391518412390724, Final Batch Loss: 0.0004842679190915078\n",
      "Epoch 2881, Loss: 0.0475293323979713, Final Batch Loss: 0.007906514219939709\n",
      "Epoch 2882, Loss: 0.004887782997684553, Final Batch Loss: 0.00024553999537602067\n",
      "Epoch 2883, Loss: 0.037227745895506814, Final Batch Loss: 0.013193421065807343\n",
      "Epoch 2884, Loss: 0.01170373969944194, Final Batch Loss: 0.00044132451876066625\n",
      "Epoch 2885, Loss: 0.007128936878871173, Final Batch Loss: 0.0016823667101562023\n",
      "Epoch 2886, Loss: 0.06309843063354492, Final Batch Loss: 0.006436419673264027\n",
      "Epoch 2887, Loss: 0.009272551367757842, Final Batch Loss: 0.00040953196003101766\n",
      "Epoch 2888, Loss: 0.03368442255305126, Final Batch Loss: 0.0015415026573464274\n",
      "Epoch 2889, Loss: 0.046838169917464256, Final Batch Loss: 0.004800266120582819\n",
      "Epoch 2890, Loss: 0.020691491954494268, Final Batch Loss: 0.0006562158232554793\n",
      "Epoch 2891, Loss: 0.005236774290096946, Final Batch Loss: 0.0005408970173448324\n",
      "Epoch 2892, Loss: 0.00635102491651196, Final Batch Loss: 0.00013390132517088205\n",
      "Epoch 2893, Loss: 0.017130930034909397, Final Batch Loss: 0.00047441833885386586\n",
      "Epoch 2894, Loss: 0.012176231684861705, Final Batch Loss: 0.0013318326091393828\n",
      "Epoch 2895, Loss: 0.011346014041919261, Final Batch Loss: 0.00034188112476840615\n",
      "Epoch 2896, Loss: 0.0045559478749055415, Final Batch Loss: 0.0007324570906348526\n",
      "Epoch 2897, Loss: 0.005862749909283593, Final Batch Loss: 0.0004729285428766161\n",
      "Epoch 2898, Loss: 0.010266026045428589, Final Batch Loss: 0.003125565592199564\n",
      "Epoch 2899, Loss: 0.012403213258949108, Final Batch Loss: 0.0003318781382404268\n",
      "Epoch 2900, Loss: 0.04382101497321855, Final Batch Loss: 0.0030637530144304037\n",
      "Epoch 2901, Loss: 0.01209062695124885, Final Batch Loss: 6.200433563208207e-05\n",
      "Epoch 2902, Loss: 0.02702233975287527, Final Batch Loss: 0.01579156145453453\n",
      "Epoch 2903, Loss: 0.017512665886897594, Final Batch Loss: 0.00022717105457559228\n",
      "Epoch 2904, Loss: 0.020068832232936984, Final Batch Loss: 6.0349670093273744e-05\n",
      "Epoch 2905, Loss: 0.03613253639196046, Final Batch Loss: 0.00010974056203849614\n",
      "Epoch 2906, Loss: 0.012971934891538695, Final Batch Loss: 0.0005786472465842962\n",
      "Epoch 2907, Loss: 0.015551896125543863, Final Batch Loss: 0.00015417279792018235\n",
      "Epoch 2908, Loss: 0.05017730561667122, Final Batch Loss: 0.000467985199065879\n",
      "Epoch 2909, Loss: 0.01799103443045169, Final Batch Loss: 0.002517296001315117\n",
      "Epoch 2910, Loss: 0.022110553210950457, Final Batch Loss: 0.008618132211267948\n",
      "Epoch 2911, Loss: 0.01225442748000205, Final Batch Loss: 2.5730290872161277e-05\n",
      "Epoch 2912, Loss: 0.0452272848051507, Final Batch Loss: 0.0004992548492737114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2913, Loss: 0.0145982544781873, Final Batch Loss: 0.0005842737737111747\n",
      "Epoch 2914, Loss: 0.014866337282001041, Final Batch Loss: 0.0002145321195712313\n",
      "Epoch 2915, Loss: 0.012928993790410459, Final Batch Loss: 0.007567586377263069\n",
      "Epoch 2916, Loss: 0.016034743806812912, Final Batch Loss: 0.0009949833620339632\n",
      "Epoch 2917, Loss: 0.03485750430263579, Final Batch Loss: 0.0004753634857479483\n",
      "Epoch 2918, Loss: 0.013156211090972647, Final Batch Loss: 0.00752244470641017\n",
      "Epoch 2919, Loss: 0.013406587167992257, Final Batch Loss: 0.0002876778307836503\n",
      "Epoch 2920, Loss: 0.013033205934334546, Final Batch Loss: 0.00459533603861928\n",
      "Epoch 2921, Loss: 0.0564582404185785, Final Batch Loss: 0.0005989751080051064\n",
      "Epoch 2922, Loss: 0.01917839067755267, Final Batch Loss: 0.008506343699991703\n",
      "Epoch 2923, Loss: 0.007464658672688529, Final Batch Loss: 0.00010081029904540628\n",
      "Epoch 2924, Loss: 0.022364290300174616, Final Batch Loss: 0.0012859473936259747\n",
      "Epoch 2925, Loss: 0.018861862627090886, Final Batch Loss: 0.010090993717312813\n",
      "Epoch 2926, Loss: 0.02701134467497468, Final Batch Loss: 0.00031217484502121806\n",
      "Epoch 2927, Loss: 0.0665586938848719, Final Batch Loss: 0.0014754257863387465\n",
      "Epoch 2928, Loss: 0.014024941163370386, Final Batch Loss: 0.0002807738783303648\n",
      "Epoch 2929, Loss: 0.036933410447090864, Final Batch Loss: 0.00432581314817071\n",
      "Epoch 2930, Loss: 0.01801813079509884, Final Batch Loss: 0.001558452844619751\n",
      "Epoch 2931, Loss: 0.07490219862665981, Final Batch Loss: 0.04839629307389259\n",
      "Epoch 2932, Loss: 0.042205413250485435, Final Batch Loss: 0.00025953154545277357\n",
      "Epoch 2933, Loss: 0.032429097744170576, Final Batch Loss: 0.0007631092448718846\n",
      "Epoch 2934, Loss: 0.013623338541947305, Final Batch Loss: 0.0008971182396635413\n",
      "Epoch 2935, Loss: 0.00975539698265493, Final Batch Loss: 0.0032052830792963505\n",
      "Epoch 2936, Loss: 0.05992095288820565, Final Batch Loss: 0.04462447017431259\n",
      "Epoch 2937, Loss: 0.01996447762940079, Final Batch Loss: 0.014686510898172855\n",
      "Epoch 2938, Loss: 0.022561577206943184, Final Batch Loss: 0.013500942848622799\n",
      "Epoch 2939, Loss: 0.01971897117618937, Final Batch Loss: 0.0002938721445389092\n",
      "Epoch 2940, Loss: 0.03401140961796045, Final Batch Loss: 0.0011898140655830503\n",
      "Epoch 2941, Loss: 0.031730491667985916, Final Batch Loss: 0.0020011013839393854\n",
      "Epoch 2942, Loss: 0.008205404214095324, Final Batch Loss: 0.0019687723834067583\n",
      "Epoch 2943, Loss: 0.00807097990764305, Final Batch Loss: 0.0009037907584570348\n",
      "Epoch 2944, Loss: 0.005245284177362919, Final Batch Loss: 0.0015845909947529435\n",
      "Epoch 2945, Loss: 0.011661029770039022, Final Batch Loss: 0.004869184922426939\n",
      "Epoch 2946, Loss: 0.01713256922084838, Final Batch Loss: 0.0014695061836391687\n",
      "Epoch 2947, Loss: 0.015999400638975203, Final Batch Loss: 0.0005552676157094538\n",
      "Epoch 2948, Loss: 0.023157841060310602, Final Batch Loss: 0.0009093338157981634\n",
      "Epoch 2949, Loss: 0.040567727090092376, Final Batch Loss: 0.0001616253866814077\n",
      "Epoch 2950, Loss: 0.0152911213808693, Final Batch Loss: 0.005299030337482691\n",
      "Epoch 2951, Loss: 0.006744409911334515, Final Batch Loss: 0.001566613675095141\n",
      "Epoch 2952, Loss: 0.014287306141341105, Final Batch Loss: 0.00012262057862244546\n",
      "Epoch 2953, Loss: 0.04769050712639, Final Batch Loss: 0.0001569421001477167\n",
      "Epoch 2954, Loss: 0.03467506142624188, Final Batch Loss: 6.567240052390844e-05\n",
      "Epoch 2955, Loss: 0.04221333819441497, Final Batch Loss: 0.001683432376012206\n",
      "Epoch 2956, Loss: 0.016745056927902624, Final Batch Loss: 0.0010020544286817312\n",
      "Epoch 2957, Loss: 0.03575143311172724, Final Batch Loss: 0.0018928389763459563\n",
      "Epoch 2958, Loss: 0.034078318160027266, Final Batch Loss: 0.0005927556776441634\n",
      "Epoch 2959, Loss: 0.02538628625916317, Final Batch Loss: 0.011395430192351341\n",
      "Epoch 2960, Loss: 0.03845604485832155, Final Batch Loss: 0.023847762495279312\n",
      "Epoch 2961, Loss: 0.0056721978762652725, Final Batch Loss: 0.00011753718717955053\n",
      "Epoch 2962, Loss: 0.013180001522414386, Final Batch Loss: 0.0011408861028030515\n",
      "Epoch 2963, Loss: 0.03352701426774729, Final Batch Loss: 0.00023686869826633483\n",
      "Epoch 2964, Loss: 0.10191812772245612, Final Batch Loss: 0.08697699010372162\n",
      "Epoch 2965, Loss: 0.010145562744583003, Final Batch Loss: 0.0009736951906234026\n",
      "Epoch 2966, Loss: 0.034278511884622276, Final Batch Loss: 0.002653866307809949\n",
      "Epoch 2967, Loss: 0.03250900731654838, Final Batch Loss: 0.0005261797341518104\n",
      "Epoch 2968, Loss: 0.025642215012339875, Final Batch Loss: 0.0003009661741089076\n",
      "Epoch 2969, Loss: 0.014279463270213455, Final Batch Loss: 0.0007866845116950572\n",
      "Epoch 2970, Loss: 0.014549039420671761, Final Batch Loss: 0.004694762639701366\n",
      "Epoch 2971, Loss: 0.03106741697411053, Final Batch Loss: 0.00038589685573242605\n",
      "Epoch 2972, Loss: 0.022912619053386152, Final Batch Loss: 0.001062222640030086\n",
      "Epoch 2973, Loss: 0.006786470941733569, Final Batch Loss: 0.0019359436118975282\n",
      "Epoch 2974, Loss: 0.010471541027072817, Final Batch Loss: 0.0006240666843950748\n",
      "Epoch 2975, Loss: 0.02362811117200181, Final Batch Loss: 0.001179738319478929\n",
      "Epoch 2976, Loss: 0.032197990862187, Final Batch Loss: 0.0007699448615312576\n",
      "Epoch 2977, Loss: 0.021515437663765624, Final Batch Loss: 0.00031731711351312697\n",
      "Epoch 2978, Loss: 0.011004729749402031, Final Batch Loss: 0.0005173585959710181\n",
      "Epoch 2979, Loss: 0.006244840566068888, Final Batch Loss: 0.0014506838051602244\n",
      "Epoch 2980, Loss: 0.008864964998792857, Final Batch Loss: 0.001918336609378457\n",
      "Epoch 2981, Loss: 0.023930970928631723, Final Batch Loss: 0.0009060819866135716\n",
      "Epoch 2982, Loss: 0.02378945259260945, Final Batch Loss: 0.0017287100199609995\n",
      "Epoch 2983, Loss: 0.008277992368675768, Final Batch Loss: 0.0010774069232866168\n",
      "Epoch 2984, Loss: 0.011272249947069213, Final Batch Loss: 0.001573923509567976\n",
      "Epoch 2985, Loss: 0.006767271785065532, Final Batch Loss: 0.0010165326530113816\n",
      "Epoch 2986, Loss: 0.015765258824103512, Final Batch Loss: 0.00030283728847280145\n",
      "Epoch 2987, Loss: 0.014815526650636457, Final Batch Loss: 0.0002412211470073089\n",
      "Epoch 2988, Loss: 0.004413961520185694, Final Batch Loss: 0.0001480360806453973\n",
      "Epoch 2989, Loss: 0.01707307882315945, Final Batch Loss: 0.00012117657752241939\n",
      "Epoch 2990, Loss: 0.008736741205211729, Final Batch Loss: 0.0014947172021493316\n",
      "Epoch 2991, Loss: 0.013208147487603128, Final Batch Loss: 0.0010134383337572217\n",
      "Epoch 2992, Loss: 0.005933532433118671, Final Batch Loss: 0.0008191560627892613\n",
      "Epoch 2993, Loss: 0.07452616500813747, Final Batch Loss: 0.00762434396892786\n",
      "Epoch 2994, Loss: 0.012997547775739804, Final Batch Loss: 0.0010941101936623454\n",
      "Epoch 2995, Loss: 0.030062208184972405, Final Batch Loss: 0.0023662291932851076\n",
      "Epoch 2996, Loss: 0.018813908551237546, Final Batch Loss: 0.00017598549311514944\n",
      "Epoch 2997, Loss: 0.023009223077679053, Final Batch Loss: 0.002318869810551405\n",
      "Epoch 2998, Loss: 0.018054051382932812, Final Batch Loss: 0.0013085674727335572\n",
      "Epoch 2999, Loss: 0.008372042619157583, Final Batch Loss: 0.0007184510468505323\n",
      "Epoch 3000, Loss: 0.02241448048152961, Final Batch Loss: 0.00033574012923054397\n",
      "Epoch 3001, Loss: 0.00660909159341827, Final Batch Loss: 0.0013334417017176747\n",
      "Epoch 3002, Loss: 0.014806074264924973, Final Batch Loss: 0.01265728659927845\n",
      "Epoch 3003, Loss: 0.014607693738071248, Final Batch Loss: 0.011396137997508049\n",
      "Epoch 3004, Loss: 0.005035358481109142, Final Batch Loss: 0.0008466059807687998\n",
      "Epoch 3005, Loss: 0.037331487954361364, Final Batch Loss: 0.0002933054056484252\n",
      "Epoch 3006, Loss: 0.01234100031433627, Final Batch Loss: 0.0028713021893054247\n",
      "Epoch 3007, Loss: 0.061040249187499285, Final Batch Loss: 0.024469995871186256\n",
      "Epoch 3008, Loss: 0.023207834805361927, Final Batch Loss: 0.016861379146575928\n",
      "Epoch 3009, Loss: 0.007748666335828602, Final Batch Loss: 0.003063914831727743\n",
      "Epoch 3010, Loss: 0.05776943592354655, Final Batch Loss: 0.034367337822914124\n",
      "Epoch 3011, Loss: 0.03794378659222275, Final Batch Loss: 0.0014130930649116635\n",
      "Epoch 3012, Loss: 0.02276122255716473, Final Batch Loss: 0.0010429694084450603\n",
      "Epoch 3013, Loss: 0.018097867025062442, Final Batch Loss: 0.0011067753657698631\n",
      "Epoch 3014, Loss: 0.013893735595047474, Final Batch Loss: 0.0003118604654446244\n",
      "Epoch 3015, Loss: 0.02667265327181667, Final Batch Loss: 0.0016688696341589093\n",
      "Epoch 3016, Loss: 0.02052727050613612, Final Batch Loss: 0.0018145242938771844\n",
      "Epoch 3017, Loss: 0.011339651624439284, Final Batch Loss: 0.0002836005005519837\n",
      "Epoch 3018, Loss: 0.00939045671839267, Final Batch Loss: 0.005206441041082144\n",
      "Epoch 3019, Loss: 0.026010325003881007, Final Batch Loss: 0.0007693994557484984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3020, Loss: 0.007812225027009845, Final Batch Loss: 0.0014668740332126617\n",
      "Epoch 3021, Loss: 0.007890758744906634, Final Batch Loss: 0.0005619772127829492\n",
      "Epoch 3022, Loss: 0.008548145735403523, Final Batch Loss: 0.0013131842715665698\n",
      "Epoch 3023, Loss: 0.010846829507499933, Final Batch Loss: 0.00014852517051622272\n",
      "Epoch 3024, Loss: 0.024360004754271358, Final Batch Loss: 0.00044187717139720917\n",
      "Epoch 3025, Loss: 0.025479442003415897, Final Batch Loss: 0.0009971538092941046\n",
      "Epoch 3026, Loss: 0.010263263051456306, Final Batch Loss: 0.0057275500148534775\n",
      "Epoch 3027, Loss: 0.026190684555331245, Final Batch Loss: 0.02219415083527565\n",
      "Epoch 3028, Loss: 0.01824581460095942, Final Batch Loss: 0.0026698631700128317\n",
      "Epoch 3029, Loss: 0.024080434945062734, Final Batch Loss: 0.016151627525687218\n",
      "Epoch 3030, Loss: 0.010166904277866706, Final Batch Loss: 0.0012589877005666494\n",
      "Epoch 3031, Loss: 0.010544561155256815, Final Batch Loss: 0.00011768761032726616\n",
      "Epoch 3032, Loss: 0.015103041427209973, Final Batch Loss: 0.0004374844138510525\n",
      "Epoch 3033, Loss: 0.03441399383882526, Final Batch Loss: 0.0003651730949059129\n",
      "Epoch 3034, Loss: 0.01599661260843277, Final Batch Loss: 0.0057974462397396564\n",
      "Epoch 3035, Loss: 0.0066771815763786435, Final Batch Loss: 0.0003022484597750008\n",
      "Epoch 3036, Loss: 0.00927133351797238, Final Batch Loss: 0.003042197320610285\n",
      "Epoch 3037, Loss: 0.006814489082898945, Final Batch Loss: 0.0013401416363194585\n",
      "Epoch 3038, Loss: 0.04440564988180995, Final Batch Loss: 0.0010697842808440328\n",
      "Epoch 3039, Loss: 0.05583390488754958, Final Batch Loss: 0.0024561844766139984\n",
      "Epoch 3040, Loss: 0.01570253138197586, Final Batch Loss: 0.0006390991038642824\n",
      "Epoch 3041, Loss: 0.012781619851011783, Final Batch Loss: 0.0011815032921731472\n",
      "Epoch 3042, Loss: 0.022216621262487024, Final Batch Loss: 0.00037875823909416795\n",
      "Epoch 3043, Loss: 0.0252494624000974, Final Batch Loss: 0.00018078129505738616\n",
      "Epoch 3044, Loss: 0.01424059292912716, Final Batch Loss: 9.7567135526333e-05\n",
      "Epoch 3045, Loss: 0.025625651789596304, Final Batch Loss: 0.0001837075687944889\n",
      "Epoch 3046, Loss: 0.05484032329695765, Final Batch Loss: 0.04519221559166908\n",
      "Epoch 3047, Loss: 0.03617994036176242, Final Batch Loss: 0.01276353932917118\n",
      "Epoch 3048, Loss: 0.02618044709379319, Final Batch Loss: 0.0005789116257801652\n",
      "Epoch 3049, Loss: 0.021675629424862564, Final Batch Loss: 0.0036433269269764423\n",
      "Epoch 3050, Loss: 0.02604348410386592, Final Batch Loss: 0.0003432903904467821\n",
      "Epoch 3051, Loss: 0.027310755278449506, Final Batch Loss: 0.0008272460545413196\n",
      "Epoch 3052, Loss: 0.055269017786486074, Final Batch Loss: 0.0031432239338755608\n",
      "Epoch 3053, Loss: 0.029916170868091285, Final Batch Loss: 0.0022192620672285557\n",
      "Epoch 3054, Loss: 0.029030191071797162, Final Batch Loss: 0.0008420319645665586\n",
      "Epoch 3055, Loss: 0.011434507061494514, Final Batch Loss: 0.0009339373791590333\n",
      "Epoch 3056, Loss: 0.021981932572089136, Final Batch Loss: 0.00309262634254992\n",
      "Epoch 3057, Loss: 0.018902928626630455, Final Batch Loss: 0.0034855189733207226\n",
      "Epoch 3058, Loss: 0.022119122862932272, Final Batch Loss: 0.0005041311378590763\n",
      "Epoch 3059, Loss: 0.014688989012938691, Final Batch Loss: 4.234064181218855e-05\n",
      "Epoch 3060, Loss: 0.012756505573634058, Final Batch Loss: 0.0005126094329170883\n",
      "Epoch 3061, Loss: 0.15192364975519013, Final Batch Loss: 0.13347744941711426\n",
      "Epoch 3062, Loss: 0.01360586768714711, Final Batch Loss: 0.0003127622767351568\n",
      "Epoch 3063, Loss: 0.01915160809585359, Final Batch Loss: 0.01429958175867796\n",
      "Epoch 3064, Loss: 0.014935629558749497, Final Batch Loss: 0.0036088479682803154\n",
      "Epoch 3065, Loss: 0.019401725236093625, Final Batch Loss: 0.0012222236255183816\n",
      "Epoch 3066, Loss: 0.013737158675212413, Final Batch Loss: 0.009209223091602325\n",
      "Epoch 3067, Loss: 0.0565551980107557, Final Batch Loss: 0.005640339106321335\n",
      "Epoch 3068, Loss: 0.035012934589758515, Final Batch Loss: 0.006156581919640303\n",
      "Epoch 3069, Loss: 0.041962905728723854, Final Batch Loss: 0.0004636451485566795\n",
      "Epoch 3070, Loss: 0.022016507282387465, Final Batch Loss: 0.0022628200240433216\n",
      "Epoch 3071, Loss: 0.009462027461268008, Final Batch Loss: 0.0018393357750028372\n",
      "Epoch 3072, Loss: 0.028010225971229374, Final Batch Loss: 0.001779564074240625\n",
      "Epoch 3073, Loss: 0.04736002936260775, Final Batch Loss: 0.007002231199294329\n",
      "Epoch 3074, Loss: 0.032405432022642344, Final Batch Loss: 0.017631815746426582\n",
      "Epoch 3075, Loss: 0.00507633329834789, Final Batch Loss: 0.0010152438189834356\n",
      "Epoch 3076, Loss: 0.026179497013799846, Final Batch Loss: 0.009354396723210812\n",
      "Epoch 3077, Loss: 0.015012248055427335, Final Batch Loss: 0.0012152137933298945\n",
      "Epoch 3078, Loss: 0.013139605725882575, Final Batch Loss: 0.0003849014174193144\n",
      "Epoch 3079, Loss: 0.01825392508180812, Final Batch Loss: 0.0019230785546824336\n",
      "Epoch 3080, Loss: 0.04283397692779545, Final Batch Loss: 0.00014196151460055262\n",
      "Epoch 3081, Loss: 0.02957530668209074, Final Batch Loss: 0.017960987985134125\n",
      "Epoch 3082, Loss: 0.040658979560248554, Final Batch Loss: 0.029720032587647438\n",
      "Epoch 3083, Loss: 0.01665484404657036, Final Batch Loss: 0.0011915978975594044\n",
      "Epoch 3084, Loss: 0.020614585489965975, Final Batch Loss: 0.0018907308112829924\n",
      "Epoch 3085, Loss: 0.012563330121338367, Final Batch Loss: 0.0012693451717495918\n",
      "Epoch 3086, Loss: 0.040640118124429137, Final Batch Loss: 0.0007812215480953455\n",
      "Epoch 3087, Loss: 0.010900993715040386, Final Batch Loss: 0.0021452195942401886\n",
      "Epoch 3088, Loss: 0.01133472757646814, Final Batch Loss: 0.0006835668464191258\n",
      "Epoch 3089, Loss: 0.05114409499219619, Final Batch Loss: 0.000207795383175835\n",
      "Epoch 3090, Loss: 0.008216567803174257, Final Batch Loss: 0.0019586128182709217\n",
      "Epoch 3091, Loss: 0.016596013796515763, Final Batch Loss: 0.0026472769677639008\n",
      "Epoch 3092, Loss: 0.12636548723094165, Final Batch Loss: 0.0006279568187892437\n",
      "Epoch 3093, Loss: 0.018725884816376492, Final Batch Loss: 0.0004503035743255168\n",
      "Epoch 3094, Loss: 0.01204485769267194, Final Batch Loss: 0.0019561948720365763\n",
      "Epoch 3095, Loss: 0.04026702686678618, Final Batch Loss: 0.017620474100112915\n",
      "Epoch 3096, Loss: 0.08472554624313489, Final Batch Loss: 0.0006720363744534552\n",
      "Epoch 3097, Loss: 0.014977437444031239, Final Batch Loss: 0.007477731443941593\n",
      "Epoch 3098, Loss: 0.04010445537278429, Final Batch Loss: 0.02750966139137745\n",
      "Epoch 3099, Loss: 0.02147780149243772, Final Batch Loss: 0.00030136643908917904\n",
      "Epoch 3100, Loss: 0.013012185343541205, Final Batch Loss: 0.004525996278971434\n",
      "Epoch 3101, Loss: 0.033711151547322515, Final Batch Loss: 9.595910523785278e-05\n",
      "Epoch 3102, Loss: 0.01440494431881234, Final Batch Loss: 0.0005053913337178528\n",
      "Epoch 3103, Loss: 0.011048441810999066, Final Batch Loss: 0.0066886115819215775\n",
      "Epoch 3104, Loss: 0.006101396691519767, Final Batch Loss: 0.0006925264024175704\n",
      "Epoch 3105, Loss: 0.0037830075016245246, Final Batch Loss: 0.0005787527188658714\n",
      "Epoch 3106, Loss: 0.028842228814028203, Final Batch Loss: 0.00123631174210459\n",
      "Epoch 3107, Loss: 0.035472822142764926, Final Batch Loss: 0.0008332136203534901\n",
      "Epoch 3108, Loss: 0.06688866496551782, Final Batch Loss: 0.028934752568602562\n",
      "Epoch 3109, Loss: 0.006854787847260013, Final Batch Loss: 0.00029665662441402674\n",
      "Epoch 3110, Loss: 0.07122746587265283, Final Batch Loss: 0.0019444377394393086\n",
      "Epoch 3111, Loss: 0.02737711602821946, Final Batch Loss: 0.003057710360735655\n",
      "Epoch 3112, Loss: 0.028941697150003165, Final Batch Loss: 0.0056480467319488525\n",
      "Epoch 3113, Loss: 0.031931691846693866, Final Batch Loss: 0.011704878881573677\n",
      "Epoch 3114, Loss: 0.009908279695082456, Final Batch Loss: 0.0007120376103557646\n",
      "Epoch 3115, Loss: 0.023845417017582804, Final Batch Loss: 0.0016812538960948586\n",
      "Epoch 3116, Loss: 0.04983229091158137, Final Batch Loss: 0.006438081618398428\n",
      "Epoch 3117, Loss: 0.00759823783300817, Final Batch Loss: 0.0002845639828592539\n",
      "Epoch 3118, Loss: 0.00823996908729896, Final Batch Loss: 0.002230693120509386\n",
      "Epoch 3119, Loss: 0.0062767468043603, Final Batch Loss: 0.0005823753890581429\n",
      "Epoch 3120, Loss: 0.016116991289891303, Final Batch Loss: 0.0004348045331425965\n",
      "Epoch 3121, Loss: 0.014221934776287526, Final Batch Loss: 0.002046676119789481\n",
      "Epoch 3122, Loss: 0.037656802785932086, Final Batch Loss: 0.015566512010991573\n",
      "Epoch 3123, Loss: 0.007448199670761824, Final Batch Loss: 0.00035034556640312076\n",
      "Epoch 3124, Loss: 0.03673008346231654, Final Batch Loss: 0.0019783528987318277\n",
      "Epoch 3125, Loss: 0.03228741543716751, Final Batch Loss: 0.000128032494103536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3126, Loss: 0.012580928698298521, Final Batch Loss: 0.00806101318448782\n",
      "Epoch 3127, Loss: 0.028183117974549532, Final Batch Loss: 0.0036351599264889956\n",
      "Epoch 3128, Loss: 0.007710053090704605, Final Batch Loss: 0.0002033958153333515\n",
      "Epoch 3129, Loss: 0.029741004604147747, Final Batch Loss: 0.002256642561405897\n",
      "Epoch 3130, Loss: 0.004570076387608424, Final Batch Loss: 0.0011332562426105142\n",
      "Epoch 3131, Loss: 0.008419021149165928, Final Batch Loss: 0.00019671680638566613\n",
      "Epoch 3132, Loss: 0.011085130623541772, Final Batch Loss: 0.002972522983327508\n",
      "Epoch 3133, Loss: 0.006717105803545564, Final Batch Loss: 0.0004266889882273972\n",
      "Epoch 3134, Loss: 0.007910236075986177, Final Batch Loss: 0.003987517207860947\n",
      "Epoch 3135, Loss: 0.010445775871630758, Final Batch Loss: 0.0009630146669223905\n",
      "Epoch 3136, Loss: 0.006815648259362206, Final Batch Loss: 0.0006800423725508153\n",
      "Epoch 3137, Loss: 0.017890621558763087, Final Batch Loss: 0.0014489683089777827\n",
      "Epoch 3138, Loss: 0.033704377186950296, Final Batch Loss: 0.0024708076380193233\n",
      "Epoch 3139, Loss: 0.013638286502100527, Final Batch Loss: 0.002936572302132845\n",
      "Epoch 3140, Loss: 0.0043582806683843955, Final Batch Loss: 0.0013783047907054424\n",
      "Epoch 3141, Loss: 0.014669811513158493, Final Batch Loss: 0.00019228858582209796\n",
      "Epoch 3142, Loss: 0.011542809879756533, Final Batch Loss: 0.0036746966652572155\n",
      "Epoch 3143, Loss: 0.014853793429210782, Final Batch Loss: 0.0005023644771426916\n",
      "Epoch 3144, Loss: 0.015279576604370959, Final Batch Loss: 0.0001298279530601576\n",
      "Epoch 3145, Loss: 0.01624017108406406, Final Batch Loss: 0.0001488319830968976\n",
      "Epoch 3146, Loss: 0.045231598312966526, Final Batch Loss: 0.001784373540431261\n",
      "Epoch 3147, Loss: 0.04380091578059364, Final Batch Loss: 0.0001417275780113414\n",
      "Epoch 3148, Loss: 0.05197480472270399, Final Batch Loss: 0.04038713127374649\n",
      "Epoch 3149, Loss: 0.009082349191885442, Final Batch Loss: 0.0004908909904770553\n",
      "Epoch 3150, Loss: 0.01122148975264281, Final Batch Loss: 0.0008335296297445893\n",
      "Epoch 3151, Loss: 0.04509162440081127, Final Batch Loss: 0.0012469064677134156\n",
      "Epoch 3152, Loss: 0.015791987476404756, Final Batch Loss: 0.00031075935112312436\n",
      "Epoch 3153, Loss: 0.015114285342860967, Final Batch Loss: 0.005680954549461603\n",
      "Epoch 3154, Loss: 0.0280728204670595, Final Batch Loss: 0.02182101085782051\n",
      "Epoch 3155, Loss: 0.024891840119380504, Final Batch Loss: 0.0007867911481298506\n",
      "Epoch 3156, Loss: 0.013038416509516537, Final Batch Loss: 0.002123062266036868\n",
      "Epoch 3157, Loss: 0.0062470660341205075, Final Batch Loss: 0.0008462535333819687\n",
      "Epoch 3158, Loss: 0.03033273294568062, Final Batch Loss: 0.0015951949171721935\n",
      "Epoch 3159, Loss: 0.021090275491587818, Final Batch Loss: 0.006777454633265734\n",
      "Epoch 3160, Loss: 0.02113685235963203, Final Batch Loss: 0.001002595410682261\n",
      "Epoch 3161, Loss: 0.003043369186343625, Final Batch Loss: 0.00037692891783080995\n",
      "Epoch 3162, Loss: 0.028768141753971577, Final Batch Loss: 0.00032471970189362764\n",
      "Epoch 3163, Loss: 0.0049300159298582, Final Batch Loss: 0.00012738238729070872\n",
      "Epoch 3164, Loss: 0.009691759041743353, Final Batch Loss: 0.003237065626308322\n",
      "Epoch 3165, Loss: 0.028232990414835513, Final Batch Loss: 0.0019041678169742227\n",
      "Epoch 3166, Loss: 0.006174115231260657, Final Batch Loss: 0.0003886332269757986\n",
      "Epoch 3167, Loss: 0.010546935285674408, Final Batch Loss: 0.004483610391616821\n",
      "Epoch 3168, Loss: 0.013122716482030228, Final Batch Loss: 0.0005938558606430888\n",
      "Epoch 3169, Loss: 0.0369620615238091, Final Batch Loss: 0.00024062958254944533\n",
      "Epoch 3170, Loss: 0.03717812098329887, Final Batch Loss: 0.0008470015018247068\n",
      "Epoch 3171, Loss: 0.03950945381075144, Final Batch Loss: 0.011568558402359486\n",
      "Epoch 3172, Loss: 0.004863997077336535, Final Batch Loss: 0.0004285603354219347\n",
      "Epoch 3173, Loss: 0.017223663875483908, Final Batch Loss: 6.851066427771002e-05\n",
      "Epoch 3174, Loss: 0.015662613790482283, Final Batch Loss: 0.0002950215130113065\n",
      "Epoch 3175, Loss: 0.013291921612108126, Final Batch Loss: 0.0005646214121952653\n",
      "Epoch 3176, Loss: 0.010748416461865418, Final Batch Loss: 0.006181551609188318\n",
      "Epoch 3177, Loss: 0.01986797951394692, Final Batch Loss: 0.000929705856833607\n",
      "Epoch 3178, Loss: 0.012555978988530114, Final Batch Loss: 0.003908941056579351\n",
      "Epoch 3179, Loss: 0.009431505983229727, Final Batch Loss: 0.0012145474320277572\n",
      "Epoch 3180, Loss: 0.04429067189630587, Final Batch Loss: 4.32296801591292e-05\n",
      "Epoch 3181, Loss: 0.010632908888510428, Final Batch Loss: 0.006897293962538242\n",
      "Epoch 3182, Loss: 0.03439967081794748, Final Batch Loss: 0.011777934618294239\n",
      "Epoch 3183, Loss: 0.012799063231796026, Final Batch Loss: 0.0008405678090639412\n",
      "Epoch 3184, Loss: 0.03924750769510865, Final Batch Loss: 0.013155287131667137\n",
      "Epoch 3185, Loss: 0.007557857752544805, Final Batch Loss: 0.00044132646871730685\n",
      "Epoch 3186, Loss: 0.005779144907137379, Final Batch Loss: 0.00278207054361701\n",
      "Epoch 3187, Loss: 0.011374390363926068, Final Batch Loss: 0.0016097562620416284\n",
      "Epoch 3188, Loss: 0.006890209537232295, Final Batch Loss: 0.0035190784838050604\n",
      "Epoch 3189, Loss: 0.04078089847462252, Final Batch Loss: 0.0004088433925062418\n",
      "Epoch 3190, Loss: 0.009328129926871043, Final Batch Loss: 0.0013654540525749326\n",
      "Epoch 3191, Loss: 0.010711244831327349, Final Batch Loss: 0.0010320995934307575\n",
      "Epoch 3192, Loss: 0.008658251841552556, Final Batch Loss: 0.003970649093389511\n",
      "Epoch 3193, Loss: 0.00623960854136385, Final Batch Loss: 0.0020902068354189396\n",
      "Epoch 3194, Loss: 0.006797912006732076, Final Batch Loss: 0.0018826250452548265\n",
      "Epoch 3195, Loss: 0.06318328599445522, Final Batch Loss: 0.002464870223775506\n",
      "Epoch 3196, Loss: 0.006917171238455921, Final Batch Loss: 0.0014723221538588405\n",
      "Epoch 3197, Loss: 0.022726869879988953, Final Batch Loss: 0.00026483050896786153\n",
      "Epoch 3198, Loss: 0.06782806292176247, Final Batch Loss: 0.015714827924966812\n",
      "Epoch 3199, Loss: 0.032917549106059596, Final Batch Loss: 0.003118704305961728\n",
      "Epoch 3200, Loss: 0.0072430375148542225, Final Batch Loss: 0.0005759905907325447\n",
      "Epoch 3201, Loss: 0.005880819007870741, Final Batch Loss: 0.00011865621490869671\n",
      "Epoch 3202, Loss: 0.008553875290090218, Final Batch Loss: 0.0007750018849037588\n",
      "Epoch 3203, Loss: 0.0050058047054335475, Final Batch Loss: 0.0020277630537748337\n",
      "Epoch 3204, Loss: 0.01166630774969235, Final Batch Loss: 0.0020274054259061813\n",
      "Epoch 3205, Loss: 0.017926130458363332, Final Batch Loss: 0.00020926703291479498\n",
      "Epoch 3206, Loss: 0.022540075471624732, Final Batch Loss: 7.467425893992186e-05\n",
      "Epoch 3207, Loss: 0.01617071077635046, Final Batch Loss: 0.000350436894223094\n",
      "Epoch 3208, Loss: 0.03602410003077239, Final Batch Loss: 0.0009826848981902003\n",
      "Epoch 3209, Loss: 0.0034383901220280677, Final Batch Loss: 0.0003882675082422793\n",
      "Epoch 3210, Loss: 0.005184602574445307, Final Batch Loss: 0.0020146132446825504\n",
      "Epoch 3211, Loss: 0.009945454148692079, Final Batch Loss: 0.0018041182775050402\n",
      "Epoch 3212, Loss: 0.01761212811106816, Final Batch Loss: 0.00044048650306649506\n",
      "Epoch 3213, Loss: 0.024794236465822905, Final Batch Loss: 0.007313767913728952\n",
      "Epoch 3214, Loss: 0.04193462754483335, Final Batch Loss: 0.03912491723895073\n",
      "Epoch 3215, Loss: 0.034559536550659686, Final Batch Loss: 0.009365477599203587\n",
      "Epoch 3216, Loss: 0.022287695785053074, Final Batch Loss: 0.012045477516949177\n",
      "Epoch 3217, Loss: 0.0651590209454298, Final Batch Loss: 0.03479995205998421\n",
      "Epoch 3218, Loss: 0.07408471568487585, Final Batch Loss: 0.03168272227048874\n",
      "Epoch 3219, Loss: 0.026766244089230895, Final Batch Loss: 0.0018755148630589247\n",
      "Epoch 3220, Loss: 0.01599395356606692, Final Batch Loss: 0.00047065524267964065\n",
      "Epoch 3221, Loss: 0.021733666071668267, Final Batch Loss: 0.00036024959990754724\n",
      "Epoch 3222, Loss: 0.030688037018990144, Final Batch Loss: 0.001374071347527206\n",
      "Epoch 3223, Loss: 0.040145584964193404, Final Batch Loss: 0.019561240449547768\n",
      "Epoch 3224, Loss: 0.026747980387881398, Final Batch Loss: 0.0017469034064561129\n",
      "Epoch 3225, Loss: 0.051791503559798, Final Batch Loss: 0.009797927923500538\n",
      "Epoch 3226, Loss: 0.07032773713581264, Final Batch Loss: 0.0036956872791051865\n",
      "Epoch 3227, Loss: 0.018663813709281385, Final Batch Loss: 0.006907808594405651\n",
      "Epoch 3228, Loss: 0.009503789304289967, Final Batch Loss: 0.00028762719011865556\n",
      "Epoch 3229, Loss: 0.022536206670338288, Final Batch Loss: 0.00039476636447943747\n",
      "Epoch 3230, Loss: 0.014639273518696427, Final Batch Loss: 0.004023828078061342\n",
      "Epoch 3231, Loss: 0.012115249963244423, Final Batch Loss: 0.002536540850996971\n",
      "Epoch 3232, Loss: 0.034267101145815104, Final Batch Loss: 0.00047122983960434794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3233, Loss: 0.007911196269560605, Final Batch Loss: 0.0005700573092326522\n",
      "Epoch 3234, Loss: 0.021203839933150448, Final Batch Loss: 0.00014557236863765866\n",
      "Epoch 3235, Loss: 0.007123619900085032, Final Batch Loss: 0.002012680284678936\n",
      "Epoch 3236, Loss: 0.04229022451909259, Final Batch Loss: 0.0007510468130931258\n",
      "Epoch 3237, Loss: 0.029038966953521594, Final Batch Loss: 0.004806660581380129\n",
      "Epoch 3238, Loss: 0.040877439081668854, Final Batch Loss: 0.01528750266879797\n",
      "Epoch 3239, Loss: 0.027698951278580353, Final Batch Loss: 0.0003813976945821196\n",
      "Epoch 3240, Loss: 0.04633482149802148, Final Batch Loss: 0.0025141541846096516\n",
      "Epoch 3241, Loss: 0.032800695626065135, Final Batch Loss: 0.002113538794219494\n",
      "Epoch 3242, Loss: 0.014773416391108185, Final Batch Loss: 0.0005383453681133687\n",
      "Epoch 3243, Loss: 0.03373217908665538, Final Batch Loss: 0.008426849730312824\n",
      "Epoch 3244, Loss: 0.007092962856404483, Final Batch Loss: 0.0007081761723384261\n",
      "Epoch 3245, Loss: 0.005517760815564543, Final Batch Loss: 0.0017507709562778473\n",
      "Epoch 3246, Loss: 0.021115239709615707, Final Batch Loss: 0.016490871086716652\n",
      "Epoch 3247, Loss: 0.005609787200228311, Final Batch Loss: 0.0010957963531836867\n",
      "Epoch 3248, Loss: 0.00862950345617719, Final Batch Loss: 0.0021195150911808014\n",
      "Epoch 3249, Loss: 0.006918001250596717, Final Batch Loss: 0.0009026911575347185\n",
      "Epoch 3250, Loss: 0.026524438071646728, Final Batch Loss: 0.0011136316461488605\n",
      "Epoch 3251, Loss: 0.009854381751210894, Final Batch Loss: 9.383231372339651e-05\n",
      "Epoch 3252, Loss: 0.018332331310375594, Final Batch Loss: 0.00019321749277878553\n",
      "Epoch 3253, Loss: 0.006594235048396513, Final Batch Loss: 0.00035103553091175854\n",
      "Epoch 3254, Loss: 0.012849065504269674, Final Batch Loss: 0.0007064068922773004\n",
      "Epoch 3255, Loss: 0.03384961967822164, Final Batch Loss: 0.0008931237971410155\n",
      "Epoch 3256, Loss: 0.0089985426311614, Final Batch Loss: 0.00015809854085091501\n",
      "Epoch 3257, Loss: 0.018633695355674718, Final Batch Loss: 0.00011174758401466534\n",
      "Epoch 3258, Loss: 0.02390872992691584, Final Batch Loss: 0.0004945083637721837\n",
      "Epoch 3259, Loss: 0.005600797449005768, Final Batch Loss: 0.0009358776151202619\n",
      "Epoch 3260, Loss: 0.037518569675739855, Final Batch Loss: 0.0008139163837768137\n",
      "Epoch 3261, Loss: 0.024119954672642052, Final Batch Loss: 0.014786950312554836\n",
      "Epoch 3262, Loss: 0.01415254021412693, Final Batch Loss: 0.00021498351998161525\n",
      "Epoch 3263, Loss: 0.014900759211741388, Final Batch Loss: 0.001691003912128508\n",
      "Epoch 3264, Loss: 0.0305664628976956, Final Batch Loss: 0.02354361116886139\n",
      "Epoch 3265, Loss: 0.019885556566805462, Final Batch Loss: 7.5959960668114945e-06\n",
      "Epoch 3266, Loss: 0.04205578280379996, Final Batch Loss: 0.020604582503437996\n",
      "Epoch 3267, Loss: 0.02243895106948912, Final Batch Loss: 0.0012921927263960242\n",
      "Epoch 3268, Loss: 0.07912893494358286, Final Batch Loss: 0.043392013758420944\n",
      "Epoch 3269, Loss: 0.0077197589562274516, Final Batch Loss: 0.0003271540626883507\n",
      "Epoch 3270, Loss: 0.009648629406001419, Final Batch Loss: 0.00016889319522306323\n",
      "Epoch 3271, Loss: 0.026720215566456318, Final Batch Loss: 0.017338911071419716\n",
      "Epoch 3272, Loss: 0.007734792772680521, Final Batch Loss: 0.001085823168978095\n",
      "Epoch 3273, Loss: 0.017324774264125153, Final Batch Loss: 0.006670895963907242\n",
      "Epoch 3274, Loss: 0.007236747449496761, Final Batch Loss: 0.00033238829928450286\n",
      "Epoch 3275, Loss: 0.007784142915625125, Final Batch Loss: 0.00034657184733077884\n",
      "Epoch 3276, Loss: 0.030544618901330978, Final Batch Loss: 0.01860748790204525\n",
      "Epoch 3277, Loss: 0.009043165249750018, Final Batch Loss: 0.0006720381206832826\n",
      "Epoch 3278, Loss: 0.00854872929630801, Final Batch Loss: 0.0016242702258750796\n",
      "Epoch 3279, Loss: 0.008977171324659139, Final Batch Loss: 0.0005115651874803007\n",
      "Epoch 3280, Loss: 0.008761986857280135, Final Batch Loss: 0.0032475078478455544\n",
      "Epoch 3281, Loss: 0.025587016920326278, Final Batch Loss: 0.004506140481680632\n",
      "Epoch 3282, Loss: 0.00497616536449641, Final Batch Loss: 0.0005397414788603783\n",
      "Epoch 3283, Loss: 0.04572160724637797, Final Batch Loss: 7.291134534170851e-05\n",
      "Epoch 3284, Loss: 0.013425480618025176, Final Batch Loss: 0.0017937931697815657\n",
      "Epoch 3285, Loss: 0.010613431717501953, Final Batch Loss: 0.00022994857863523066\n",
      "Epoch 3286, Loss: 0.004912357835564762, Final Batch Loss: 0.0004305673064664006\n",
      "Epoch 3287, Loss: 0.022129286800918635, Final Batch Loss: 0.00046304427087306976\n",
      "Epoch 3288, Loss: 0.049649331471300684, Final Batch Loss: 0.0005936795496381819\n",
      "Epoch 3289, Loss: 0.017294815319473855, Final Batch Loss: 0.007057212293148041\n",
      "Epoch 3290, Loss: 0.029287642741110176, Final Batch Loss: 0.01799989677965641\n",
      "Epoch 3291, Loss: 0.03025324991904199, Final Batch Loss: 0.01981113851070404\n",
      "Epoch 3292, Loss: 0.008376353944186121, Final Batch Loss: 0.0008127031615003943\n",
      "Epoch 3293, Loss: 0.013871154820662923, Final Batch Loss: 0.00019524067465681583\n",
      "Epoch 3294, Loss: 0.0073658590408740565, Final Batch Loss: 0.00014978022954892367\n",
      "Epoch 3295, Loss: 0.006167248182464391, Final Batch Loss: 0.0008347434340976179\n",
      "Epoch 3296, Loss: 0.013399209943599999, Final Batch Loss: 0.0008526245364919305\n",
      "Epoch 3297, Loss: 0.044860031572170556, Final Batch Loss: 0.0006765802972950041\n",
      "Epoch 3298, Loss: 0.022647950521786697, Final Batch Loss: 0.0002141338336514309\n",
      "Epoch 3299, Loss: 0.027511244115885347, Final Batch Loss: 0.007328354753553867\n",
      "Epoch 3300, Loss: 0.006388825771864504, Final Batch Loss: 0.002100608544424176\n",
      "Epoch 3301, Loss: 0.011666262114886194, Final Batch Loss: 0.0005095945089124143\n",
      "Epoch 3302, Loss: 0.021431182729429565, Final Batch Loss: 0.00170991662889719\n",
      "Epoch 3303, Loss: 0.004595456062816083, Final Batch Loss: 0.0014875887427479029\n",
      "Epoch 3304, Loss: 0.015131840918911621, Final Batch Loss: 0.00033327043638564646\n",
      "Epoch 3305, Loss: 0.04858517652610317, Final Batch Loss: 0.0007453762227669358\n",
      "Epoch 3306, Loss: 0.014046932512428612, Final Batch Loss: 0.0010053800651803613\n",
      "Epoch 3307, Loss: 0.02024965512100607, Final Batch Loss: 0.0005214243428781629\n",
      "Epoch 3308, Loss: 0.009204159490764141, Final Batch Loss: 0.001957033760845661\n",
      "Epoch 3309, Loss: 0.006056851059838664, Final Batch Loss: 0.0028897379525005817\n",
      "Epoch 3310, Loss: 0.00390984516707249, Final Batch Loss: 6.552459672093391e-05\n",
      "Epoch 3311, Loss: 0.013946344508440234, Final Batch Loss: 9.331577166449279e-05\n",
      "Epoch 3312, Loss: 0.003992961632320657, Final Batch Loss: 0.0004702397854998708\n",
      "Epoch 3313, Loss: 0.018863049161154777, Final Batch Loss: 0.00178715237416327\n",
      "Epoch 3314, Loss: 0.010878159147978295, Final Batch Loss: 0.002932000206783414\n",
      "Epoch 3315, Loss: 0.01825777364138048, Final Batch Loss: 0.00032925253617577255\n",
      "Epoch 3316, Loss: 0.033604920259676874, Final Batch Loss: 0.030675973743200302\n",
      "Epoch 3317, Loss: 0.02757414767984301, Final Batch Loss: 0.00021563004702329636\n",
      "Epoch 3318, Loss: 0.018515876377932727, Final Batch Loss: 0.0005725075607188046\n",
      "Epoch 3319, Loss: 0.03601481190708, Final Batch Loss: 2.1816420485265553e-05\n",
      "Epoch 3320, Loss: 0.00927566789323464, Final Batch Loss: 0.0017597200348973274\n",
      "Epoch 3321, Loss: 0.02076439087977633, Final Batch Loss: 0.004289712756872177\n",
      "Epoch 3322, Loss: 0.03362988732260419, Final Batch Loss: 0.0026672317180782557\n",
      "Epoch 3323, Loss: 0.0163510094571393, Final Batch Loss: 0.002536741318181157\n",
      "Epoch 3324, Loss: 0.038308858580421656, Final Batch Loss: 0.0010268579935654998\n",
      "Epoch 3325, Loss: 0.05116894468665123, Final Batch Loss: 0.03673504292964935\n",
      "Epoch 3326, Loss: 0.01971044417587109, Final Batch Loss: 0.0001656023960094899\n",
      "Epoch 3327, Loss: 0.014340930385515094, Final Batch Loss: 0.0007638026145286858\n",
      "Epoch 3328, Loss: 0.009967757563572377, Final Batch Loss: 0.0002876323997043073\n",
      "Epoch 3329, Loss: 0.007855884323362261, Final Batch Loss: 0.00030136044370010495\n",
      "Epoch 3330, Loss: 0.019721181321074255, Final Batch Loss: 0.00015259151405189186\n",
      "Epoch 3331, Loss: 0.01752044860040769, Final Batch Loss: 0.0019442420452833176\n",
      "Epoch 3332, Loss: 0.025732892914675176, Final Batch Loss: 0.001607852173037827\n",
      "Epoch 3333, Loss: 0.005812372983200476, Final Batch Loss: 0.0029928204603493214\n",
      "Epoch 3334, Loss: 0.010808443534187973, Final Batch Loss: 0.0015858565457165241\n",
      "Epoch 3335, Loss: 0.05808912494103424, Final Batch Loss: 0.0023648282513022423\n",
      "Epoch 3336, Loss: 0.023094964293704834, Final Batch Loss: 8.749652624828741e-05\n",
      "Epoch 3337, Loss: 0.020670291734859347, Final Batch Loss: 0.01061315182596445\n",
      "Epoch 3338, Loss: 0.01876828633248806, Final Batch Loss: 0.005180620588362217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3339, Loss: 0.01807429938344285, Final Batch Loss: 0.00076004903530702\n",
      "Epoch 3340, Loss: 0.012419436214258894, Final Batch Loss: 0.0006045105401426554\n",
      "Epoch 3341, Loss: 0.017135814647190273, Final Batch Loss: 0.0006634249584749341\n",
      "Epoch 3342, Loss: 0.010217127244686708, Final Batch Loss: 0.000591077608987689\n",
      "Epoch 3343, Loss: 0.004212185267533641, Final Batch Loss: 7.375296991085634e-05\n",
      "Epoch 3344, Loss: 0.02169627812691033, Final Batch Loss: 0.0007443195208907127\n",
      "Epoch 3345, Loss: 0.025058396975509822, Final Batch Loss: 0.005578287411481142\n",
      "Epoch 3346, Loss: 0.013673919427674264, Final Batch Loss: 0.0014204030157998204\n",
      "Epoch 3347, Loss: 0.003934559703338891, Final Batch Loss: 0.0004996057832613587\n",
      "Epoch 3348, Loss: 0.01500340536586009, Final Batch Loss: 0.001053328043781221\n",
      "Epoch 3349, Loss: 0.004320449821534567, Final Batch Loss: 9.443001181352884e-05\n",
      "Epoch 3350, Loss: 0.004899866704363376, Final Batch Loss: 0.00020863302052021027\n",
      "Epoch 3351, Loss: 0.004607181006576866, Final Batch Loss: 0.0006065729539841413\n",
      "Epoch 3352, Loss: 0.0029598963556054514, Final Batch Loss: 4.346764399087988e-05\n",
      "Epoch 3353, Loss: 0.019415413349634036, Final Batch Loss: 0.00010573820327408612\n",
      "Epoch 3354, Loss: 0.026174962065852014, Final Batch Loss: 3.710521195898764e-05\n",
      "Epoch 3355, Loss: 0.011412883512093686, Final Batch Loss: 0.00030396165675483644\n",
      "Epoch 3356, Loss: 0.003082315713982098, Final Batch Loss: 0.00016279415285680443\n",
      "Epoch 3357, Loss: 0.03693998442031443, Final Batch Loss: 0.0022768413182348013\n",
      "Epoch 3358, Loss: 0.021036315039964393, Final Batch Loss: 0.005928244907408953\n",
      "Epoch 3359, Loss: 0.015673495534429094, Final Batch Loss: 0.01075778715312481\n",
      "Epoch 3360, Loss: 0.010714741045376286, Final Batch Loss: 0.0007146435673348606\n",
      "Epoch 3361, Loss: 0.020713043748401105, Final Batch Loss: 0.0006094009149819613\n",
      "Epoch 3362, Loss: 0.034238240026752464, Final Batch Loss: 0.02408781088888645\n",
      "Epoch 3363, Loss: 0.03640647526481189, Final Batch Loss: 0.029801806434988976\n",
      "Epoch 3364, Loss: 0.008336311679158825, Final Batch Loss: 0.0001546839630464092\n",
      "Epoch 3365, Loss: 0.004495803921599872, Final Batch Loss: 0.00020054516789969057\n",
      "Epoch 3366, Loss: 0.009639366806368344, Final Batch Loss: 0.0008055376820266247\n",
      "Epoch 3367, Loss: 0.02773380458529573, Final Batch Loss: 0.0006449457723647356\n",
      "Epoch 3368, Loss: 0.0168021358531405, Final Batch Loss: 2.448915438435506e-05\n",
      "Epoch 3369, Loss: 0.006993264134507626, Final Batch Loss: 0.0011660880409181118\n",
      "Epoch 3370, Loss: 0.03511032814276405, Final Batch Loss: 0.030792957171797752\n",
      "Epoch 3371, Loss: 0.004529406418441795, Final Batch Loss: 0.00030901978607289493\n",
      "Epoch 3372, Loss: 0.01850411095074378, Final Batch Loss: 0.0014411881566047668\n",
      "Epoch 3373, Loss: 0.005765323221567087, Final Batch Loss: 8.064955181907862e-05\n",
      "Epoch 3374, Loss: 0.005649971484672278, Final Batch Loss: 0.0006988822715356946\n",
      "Epoch 3375, Loss: 0.003907097387127578, Final Batch Loss: 0.00033418566454201937\n",
      "Epoch 3376, Loss: 0.018320096016395837, Final Batch Loss: 0.0005855164490640163\n",
      "Epoch 3377, Loss: 0.006543546071043238, Final Batch Loss: 0.00483296625316143\n",
      "Epoch 3378, Loss: 0.009161409660009667, Final Batch Loss: 0.00033764643012546003\n",
      "Epoch 3379, Loss: 0.010401709441794083, Final Batch Loss: 0.0063146729953587055\n",
      "Epoch 3380, Loss: 0.010811181433382444, Final Batch Loss: 0.00012997926387470216\n",
      "Epoch 3381, Loss: 0.006544548592501087, Final Batch Loss: 0.001400335575453937\n",
      "Epoch 3382, Loss: 0.0048171742164413445, Final Batch Loss: 0.0007209494360722601\n",
      "Epoch 3383, Loss: 0.007866090119932778, Final Batch Loss: 0.005080877803266048\n",
      "Epoch 3384, Loss: 0.005667751946020871, Final Batch Loss: 0.003426924580708146\n",
      "Epoch 3385, Loss: 0.019334415468620136, Final Batch Loss: 0.01654824987053871\n",
      "Epoch 3386, Loss: 0.005025017526349984, Final Batch Loss: 0.0015773284249007702\n",
      "Epoch 3387, Loss: 0.004940869068377651, Final Batch Loss: 0.0009465193725191057\n",
      "Epoch 3388, Loss: 0.04880032368237153, Final Batch Loss: 0.008828061632812023\n",
      "Epoch 3389, Loss: 0.014091656892560422, Final Batch Loss: 0.0015731448074802756\n",
      "Epoch 3390, Loss: 0.00939514506899286, Final Batch Loss: 0.0006027310737408698\n",
      "Epoch 3391, Loss: 0.012979476450709626, Final Batch Loss: 0.001969329547137022\n",
      "Epoch 3392, Loss: 0.002095734133035876, Final Batch Loss: 0.00018892815569415689\n",
      "Epoch 3393, Loss: 0.0027944153116550297, Final Batch Loss: 0.0006279367953538895\n",
      "Epoch 3394, Loss: 0.028689767379546538, Final Batch Loss: 0.0002847592404577881\n",
      "Epoch 3395, Loss: 0.008798068563919514, Final Batch Loss: 0.0020450428128242493\n",
      "Epoch 3396, Loss: 0.013636854346259497, Final Batch Loss: 9.888799104373902e-05\n",
      "Epoch 3397, Loss: 0.005174061341676861, Final Batch Loss: 0.0003718127845786512\n",
      "Epoch 3398, Loss: 0.055783227042411454, Final Batch Loss: 0.0006762022967450321\n",
      "Epoch 3399, Loss: 0.037401483015855774, Final Batch Loss: 0.007363352458924055\n",
      "Epoch 3400, Loss: 0.007619906013133004, Final Batch Loss: 0.001213732291944325\n",
      "Epoch 3401, Loss: 0.005341721458535176, Final Batch Loss: 0.00031756708631291986\n",
      "Epoch 3402, Loss: 0.006437467032810673, Final Batch Loss: 0.0004637049569282681\n",
      "Epoch 3403, Loss: 0.00796274488675408, Final Batch Loss: 0.0035156002268195152\n",
      "Epoch 3404, Loss: 0.0069136315723881125, Final Batch Loss: 0.001294904388487339\n",
      "Epoch 3405, Loss: 0.010428313820739277, Final Batch Loss: 0.0003432997327763587\n",
      "Epoch 3406, Loss: 0.00890885668923147, Final Batch Loss: 0.0014405754627659917\n",
      "Epoch 3407, Loss: 0.018899326416430995, Final Batch Loss: 0.00015837300452403724\n",
      "Epoch 3408, Loss: 0.007436907995725051, Final Batch Loss: 6.199799827300012e-05\n",
      "Epoch 3409, Loss: 0.006270728597883135, Final Batch Loss: 0.0004558499495033175\n",
      "Epoch 3410, Loss: 0.005309956133714877, Final Batch Loss: 0.0018112151883542538\n",
      "Epoch 3411, Loss: 0.015015282377135009, Final Batch Loss: 0.0017822752706706524\n",
      "Epoch 3412, Loss: 0.0037832437592442147, Final Batch Loss: 0.0023675174452364445\n",
      "Epoch 3413, Loss: 0.021068278634629678, Final Batch Loss: 0.0009927981300279498\n",
      "Epoch 3414, Loss: 0.009795536936508142, Final Batch Loss: 7.592378096887842e-05\n",
      "Epoch 3415, Loss: 0.0027748292923206463, Final Batch Loss: 0.00037801649887114763\n",
      "Epoch 3416, Loss: 0.039004113990813494, Final Batch Loss: 0.032873913645744324\n",
      "Epoch 3417, Loss: 0.002701511955820024, Final Batch Loss: 0.0002563728194218129\n",
      "Epoch 3418, Loss: 0.027149597779498436, Final Batch Loss: 0.00022381068265531212\n",
      "Epoch 3419, Loss: 0.004262486181687564, Final Batch Loss: 0.0014283758355304599\n",
      "Epoch 3420, Loss: 0.004719409756944515, Final Batch Loss: 0.0007275840034708381\n",
      "Epoch 3421, Loss: 0.00444878768757917, Final Batch Loss: 0.00020071145263500512\n",
      "Epoch 3422, Loss: 0.005689549769158475, Final Batch Loss: 0.0035796440206468105\n",
      "Epoch 3423, Loss: 0.02777458626223961, Final Batch Loss: 7.127231947379187e-05\n",
      "Epoch 3424, Loss: 0.016981281936750747, Final Batch Loss: 2.3000073269940913e-05\n",
      "Epoch 3425, Loss: 0.0058474116085562855, Final Batch Loss: 0.0024562699254602194\n",
      "Epoch 3426, Loss: 0.003248664303100668, Final Batch Loss: 0.0006818877882324159\n",
      "Epoch 3427, Loss: 0.0189339052849391, Final Batch Loss: 1.2707194400718436e-05\n",
      "Epoch 3428, Loss: 0.005674711268511601, Final Batch Loss: 0.003064175136387348\n",
      "Epoch 3429, Loss: 0.010254400163830724, Final Batch Loss: 0.00018735813500825316\n",
      "Epoch 3430, Loss: 0.0409324795473367, Final Batch Loss: 0.01386936753988266\n",
      "Epoch 3431, Loss: 0.019586167356465012, Final Batch Loss: 0.002942966530099511\n",
      "Epoch 3432, Loss: 0.02376457376522012, Final Batch Loss: 0.02068965882062912\n",
      "Epoch 3433, Loss: 0.0025589505676180124, Final Batch Loss: 0.0007990995072759688\n",
      "Epoch 3434, Loss: 0.019596900659962557, Final Batch Loss: 0.0005196909769438207\n",
      "Epoch 3435, Loss: 0.029984507273184136, Final Batch Loss: 0.0005874802591279149\n",
      "Epoch 3436, Loss: 0.015149954473599792, Final Batch Loss: 0.004644481930881739\n",
      "Epoch 3437, Loss: 0.007481181412003934, Final Batch Loss: 0.00042033317731693387\n",
      "Epoch 3438, Loss: 0.0092705019924324, Final Batch Loss: 0.001663052593357861\n",
      "Epoch 3439, Loss: 0.004938930855132639, Final Batch Loss: 5.984114250168204e-05\n",
      "Epoch 3440, Loss: 0.002642248844495043, Final Batch Loss: 0.0006694358889944851\n",
      "Epoch 3441, Loss: 0.01691169460536912, Final Batch Loss: 0.000975248753093183\n",
      "Epoch 3442, Loss: 0.051904171057685744, Final Batch Loss: 0.0012289169244468212\n",
      "Epoch 3443, Loss: 0.0032912407550611533, Final Batch Loss: 9.690620208857581e-05\n",
      "Epoch 3444, Loss: 0.010862230032216758, Final Batch Loss: 0.00040459836600348353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3445, Loss: 0.04016594751738012, Final Batch Loss: 0.0007113740430213511\n",
      "Epoch 3446, Loss: 0.04820700595155358, Final Batch Loss: 0.0005231872200965881\n",
      "Epoch 3447, Loss: 0.030959862187955878, Final Batch Loss: 2.4284197934321128e-05\n",
      "Epoch 3448, Loss: 0.012071177246980369, Final Batch Loss: 0.0010693283984437585\n",
      "Epoch 3449, Loss: 0.031584106021909975, Final Batch Loss: 0.00011164703755639493\n",
      "Epoch 3450, Loss: 0.010036039602709934, Final Batch Loss: 0.0010775425471365452\n",
      "Epoch 3451, Loss: 0.008280806927359663, Final Batch Loss: 0.005483673885464668\n",
      "Epoch 3452, Loss: 0.011507862014696002, Final Batch Loss: 0.0008421022212132812\n",
      "Epoch 3453, Loss: 0.016177569690626115, Final Batch Loss: 0.004208328202366829\n",
      "Epoch 3454, Loss: 0.045363409037236124, Final Batch Loss: 0.013982800766825676\n",
      "Epoch 3455, Loss: 0.008889652730431408, Final Batch Loss: 0.0004841941117774695\n",
      "Epoch 3456, Loss: 0.007042938319500536, Final Batch Loss: 0.004352231975644827\n",
      "Epoch 3457, Loss: 0.03671805397607386, Final Batch Loss: 0.0018840278498828411\n",
      "Epoch 3458, Loss: 0.02819692250341177, Final Batch Loss: 0.001050319755449891\n",
      "Epoch 3459, Loss: 0.008240263028710615, Final Batch Loss: 0.001854780362918973\n",
      "Epoch 3460, Loss: 0.0101821543648839, Final Batch Loss: 0.0024570911191403866\n",
      "Epoch 3461, Loss: 0.007432818340021186, Final Batch Loss: 0.00039771245792508125\n",
      "Epoch 3462, Loss: 0.007735921739367768, Final Batch Loss: 0.004376883618533611\n",
      "Epoch 3463, Loss: 0.005817545621539466, Final Batch Loss: 0.0017086361767724156\n",
      "Epoch 3464, Loss: 0.014033661020221189, Final Batch Loss: 0.0038171776104718447\n",
      "Epoch 3465, Loss: 0.0067816456576110795, Final Batch Loss: 0.0001548355066915974\n",
      "Epoch 3466, Loss: 0.003971305632148869, Final Batch Loss: 0.0012391676427796483\n",
      "Epoch 3467, Loss: 0.0469233451076434, Final Batch Loss: 0.00011904925486305729\n",
      "Epoch 3468, Loss: 0.014403891429537907, Final Batch Loss: 0.001147970906458795\n",
      "Epoch 3469, Loss: 0.036189916980220005, Final Batch Loss: 0.0030679330229759216\n",
      "Epoch 3470, Loss: 0.00899829517584294, Final Batch Loss: 0.0002185766352340579\n",
      "Epoch 3471, Loss: 0.014384136040462181, Final Batch Loss: 0.0005364301614463329\n",
      "Epoch 3472, Loss: 0.015898016175924568, Final Batch Loss: 5.4845655540702865e-05\n",
      "Epoch 3473, Loss: 0.06835665680409875, Final Batch Loss: 7.915262540336698e-05\n",
      "Epoch 3474, Loss: 0.006058901897631586, Final Batch Loss: 0.0007633724599145353\n",
      "Epoch 3475, Loss: 0.022608716710237786, Final Batch Loss: 0.0022408301010727882\n",
      "Epoch 3476, Loss: 0.016578679846134037, Final Batch Loss: 0.011779427528381348\n",
      "Epoch 3477, Loss: 0.01571313015301712, Final Batch Loss: 0.00028828205540776253\n",
      "Epoch 3478, Loss: 0.01847974416159559, Final Batch Loss: 0.0001718898565741256\n",
      "Epoch 3479, Loss: 0.01126793771982193, Final Batch Loss: 0.0005130717763677239\n",
      "Epoch 3480, Loss: 0.056712808611337095, Final Batch Loss: 0.00033946678740903735\n",
      "Epoch 3481, Loss: 0.005994565348373726, Final Batch Loss: 0.00020659601432271302\n",
      "Epoch 3482, Loss: 0.00433326925849542, Final Batch Loss: 0.00048179266741499305\n",
      "Epoch 3483, Loss: 0.023793253698386252, Final Batch Loss: 0.0014384426176548004\n",
      "Epoch 3484, Loss: 0.046086976159131154, Final Batch Loss: 0.00021412657224573195\n",
      "Epoch 3485, Loss: 0.007263512321515009, Final Batch Loss: 0.0008967195171862841\n",
      "Epoch 3486, Loss: 0.009008546330733225, Final Batch Loss: 0.0008303652284666896\n",
      "Epoch 3487, Loss: 0.025984020088799298, Final Batch Loss: 0.0011340499622747302\n",
      "Epoch 3488, Loss: 0.0208449077908881, Final Batch Loss: 0.0013905289815738797\n",
      "Epoch 3489, Loss: 0.03995311603648588, Final Batch Loss: 0.0016589047154411674\n",
      "Epoch 3490, Loss: 0.004742752644233406, Final Batch Loss: 0.0003338258829899132\n",
      "Epoch 3491, Loss: 0.018055307591566816, Final Batch Loss: 0.0014325932133942842\n",
      "Epoch 3492, Loss: 0.029459771263645962, Final Batch Loss: 0.0004607396258506924\n",
      "Epoch 3493, Loss: 0.009512143966276199, Final Batch Loss: 0.0027904154267162085\n",
      "Epoch 3494, Loss: 0.03163456975016743, Final Batch Loss: 0.018120359629392624\n",
      "Epoch 3495, Loss: 0.0072552119818283245, Final Batch Loss: 0.00024048004706855863\n",
      "Epoch 3496, Loss: 0.003902835218468681, Final Batch Loss: 0.0002456207585055381\n",
      "Epoch 3497, Loss: 0.011863718536915258, Final Batch Loss: 0.0002611665695440024\n",
      "Epoch 3498, Loss: 0.006381331826560199, Final Batch Loss: 0.0009716468630358577\n",
      "Epoch 3499, Loss: 0.0036785574338864535, Final Batch Loss: 6.71384041197598e-05\n",
      "Epoch 3500, Loss: 0.04051598039222881, Final Batch Loss: 0.0009514717967249453\n",
      "Epoch 3501, Loss: 0.014002450800035149, Final Batch Loss: 0.010854172520339489\n",
      "Epoch 3502, Loss: 0.03172248011105694, Final Batch Loss: 0.000660675170365721\n",
      "Epoch 3503, Loss: 0.07118334789993241, Final Batch Loss: 0.00029999049729667604\n",
      "Epoch 3504, Loss: 0.027730373141821474, Final Batch Loss: 0.00127118150703609\n",
      "Epoch 3505, Loss: 0.007675718050450087, Final Batch Loss: 0.0005097950343042612\n",
      "Epoch 3506, Loss: 0.031471478985622525, Final Batch Loss: 0.02533002942800522\n",
      "Epoch 3507, Loss: 0.06764428652240895, Final Batch Loss: 0.00025789623032324016\n",
      "Epoch 3508, Loss: 0.01785584955359809, Final Batch Loss: 0.00044948895811103284\n",
      "Epoch 3509, Loss: 0.005395534273702651, Final Batch Loss: 0.0003424076421651989\n",
      "Epoch 3510, Loss: 0.016393873909692047, Final Batch Loss: 3.6336550692794845e-05\n",
      "Epoch 3511, Loss: 0.01217184956476558, Final Batch Loss: 0.00023171916836872697\n",
      "Epoch 3512, Loss: 0.016958973719738424, Final Batch Loss: 0.001628292491659522\n",
      "Epoch 3513, Loss: 0.01834820135263726, Final Batch Loss: 0.00018244789680466056\n",
      "Epoch 3514, Loss: 0.006286300136707723, Final Batch Loss: 0.004051961470395327\n",
      "Epoch 3515, Loss: 0.02895399066619575, Final Batch Loss: 0.0002955149975605309\n",
      "Epoch 3516, Loss: 0.03414298850111663, Final Batch Loss: 0.0010614997008815408\n",
      "Epoch 3517, Loss: 0.030645593389635906, Final Batch Loss: 0.0005572066875174642\n",
      "Epoch 3518, Loss: 0.009264131556847133, Final Batch Loss: 0.0002234211569884792\n",
      "Epoch 3519, Loss: 0.029393182834610343, Final Batch Loss: 0.0001372703700326383\n",
      "Epoch 3520, Loss: 0.013083797544823028, Final Batch Loss: 0.0003687035641632974\n",
      "Epoch 3521, Loss: 0.0036909772461513057, Final Batch Loss: 0.0001492128794780001\n",
      "Epoch 3522, Loss: 0.003450451942626387, Final Batch Loss: 0.0019161676755174994\n",
      "Epoch 3523, Loss: 0.021223745396127924, Final Batch Loss: 0.017364835366606712\n",
      "Epoch 3524, Loss: 0.05115284220664762, Final Batch Loss: 0.022444339469075203\n",
      "Epoch 3525, Loss: 0.003690691140946001, Final Batch Loss: 0.0007189544849097729\n",
      "Epoch 3526, Loss: 0.018690061042434536, Final Batch Loss: 0.00017564672452863306\n",
      "Epoch 3527, Loss: 0.04486699355766177, Final Batch Loss: 0.00030010927002876997\n",
      "Epoch 3528, Loss: 0.004869996046181768, Final Batch Loss: 0.0009365412406623363\n",
      "Epoch 3529, Loss: 0.007215229677967727, Final Batch Loss: 0.0016864142380654812\n",
      "Epoch 3530, Loss: 0.010071711090859026, Final Batch Loss: 0.0005930357147008181\n",
      "Epoch 3531, Loss: 0.02424168650759384, Final Batch Loss: 0.018244190141558647\n",
      "Epoch 3532, Loss: 0.06600258106482215, Final Batch Loss: 0.006080904044210911\n",
      "Epoch 3533, Loss: 0.004706962659838609, Final Batch Loss: 0.00015636843454558402\n",
      "Epoch 3534, Loss: 0.017921253136591986, Final Batch Loss: 0.0016096674371510744\n",
      "Epoch 3535, Loss: 0.03457978440565057, Final Batch Loss: 0.02214762195944786\n",
      "Epoch 3536, Loss: 0.015533978992607445, Final Batch Loss: 0.0004937577177770436\n",
      "Epoch 3537, Loss: 0.08214125223457813, Final Batch Loss: 0.006258947309106588\n",
      "Epoch 3538, Loss: 0.05525192664936185, Final Batch Loss: 0.02977709285914898\n",
      "Epoch 3539, Loss: 0.09875656943768263, Final Batch Loss: 0.01930207386612892\n",
      "Epoch 3540, Loss: 0.028697745874524117, Final Batch Loss: 0.00033668510150164366\n",
      "Epoch 3541, Loss: 0.05348752764984965, Final Batch Loss: 0.001107409130781889\n",
      "Epoch 3542, Loss: 0.015628992216079496, Final Batch Loss: 0.004187310580164194\n",
      "Epoch 3543, Loss: 0.00903646508231759, Final Batch Loss: 0.0002621216408442706\n",
      "Epoch 3544, Loss: 0.02063688385533169, Final Batch Loss: 0.0023445782717317343\n",
      "Epoch 3545, Loss: 0.015714556822786108, Final Batch Loss: 0.0006674678297713399\n",
      "Epoch 3546, Loss: 0.012514125555753708, Final Batch Loss: 0.006054719444364309\n",
      "Epoch 3547, Loss: 0.09097704442683607, Final Batch Loss: 0.06418360024690628\n",
      "Epoch 3548, Loss: 0.012809930893126875, Final Batch Loss: 0.002226804615929723\n",
      "Epoch 3549, Loss: 0.024968471960164607, Final Batch Loss: 0.00035007495898753405\n",
      "Epoch 3550, Loss: 0.008867702912539244, Final Batch Loss: 0.001553023117594421\n",
      "Epoch 3551, Loss: 0.027168011583853513, Final Batch Loss: 0.0037675790954381227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3552, Loss: 0.01791229151422158, Final Batch Loss: 0.0008635335834696889\n",
      "Epoch 3553, Loss: 0.022018091985955834, Final Batch Loss: 0.0011737269815057516\n",
      "Epoch 3554, Loss: 0.008348812611075118, Final Batch Loss: 0.0011075260117650032\n",
      "Epoch 3555, Loss: 0.019273141399025917, Final Batch Loss: 0.00038032326847314835\n",
      "Epoch 3556, Loss: 0.011280319391516969, Final Batch Loss: 0.0028917097952216864\n",
      "Epoch 3557, Loss: 0.025749275577254593, Final Batch Loss: 0.01428926270455122\n",
      "Epoch 3558, Loss: 0.0130613399669528, Final Batch Loss: 0.002784928772598505\n",
      "Epoch 3559, Loss: 0.014482457641861401, Final Batch Loss: 0.00014747421664651483\n",
      "Epoch 3560, Loss: 0.013222684938227758, Final Batch Loss: 0.00025987389381043613\n",
      "Epoch 3561, Loss: 0.013948668958619237, Final Batch Loss: 0.000851488090120256\n",
      "Epoch 3562, Loss: 0.011023220547940582, Final Batch Loss: 0.0024535679258406162\n",
      "Epoch 3563, Loss: 0.002819721499690786, Final Batch Loss: 0.00038470866275019944\n",
      "Epoch 3564, Loss: 0.029765670362394303, Final Batch Loss: 0.0026582470163702965\n",
      "Epoch 3565, Loss: 0.00746268144575879, Final Batch Loss: 0.0009512057877145708\n",
      "Epoch 3566, Loss: 0.012078820072929375, Final Batch Loss: 0.00016557557682972401\n",
      "Epoch 3567, Loss: 0.0025923714274540544, Final Batch Loss: 0.0005040474934503436\n",
      "Epoch 3568, Loss: 0.006995827367063612, Final Batch Loss: 0.000689020729623735\n",
      "Epoch 3569, Loss: 0.008110673574265093, Final Batch Loss: 0.0011552147334441543\n",
      "Epoch 3570, Loss: 0.0072252411628142, Final Batch Loss: 0.00031346664763987064\n",
      "Epoch 3571, Loss: 0.006452108849771321, Final Batch Loss: 0.004368484485894442\n",
      "Epoch 3572, Loss: 0.009269853035220876, Final Batch Loss: 0.0036679725162684917\n",
      "Epoch 3573, Loss: 0.011968593811616302, Final Batch Loss: 0.0003006893675774336\n",
      "Epoch 3574, Loss: 0.005924967612372711, Final Batch Loss: 0.001313460641540587\n",
      "Epoch 3575, Loss: 0.016698593681212515, Final Batch Loss: 0.002337796613574028\n",
      "Epoch 3576, Loss: 0.034876394260209054, Final Batch Loss: 0.0005538388504646719\n",
      "Epoch 3577, Loss: 0.024071302672382444, Final Batch Loss: 0.01755605638027191\n",
      "Epoch 3578, Loss: 0.06502100068610162, Final Batch Loss: 0.03900016099214554\n",
      "Epoch 3579, Loss: 0.011960399016970769, Final Batch Loss: 0.0025426947977393866\n",
      "Epoch 3580, Loss: 0.005583498772466555, Final Batch Loss: 0.00024613074492663145\n",
      "Epoch 3581, Loss: 0.00310330132197123, Final Batch Loss: 0.0005929035251028836\n",
      "Epoch 3582, Loss: 0.0071522600774187595, Final Batch Loss: 0.0033733800519257784\n",
      "Epoch 3583, Loss: 0.02157944397185929, Final Batch Loss: 0.0012742955004796386\n",
      "Epoch 3584, Loss: 0.0072845574613893405, Final Batch Loss: 0.00016032489656936377\n",
      "Epoch 3585, Loss: 0.01543832418974489, Final Batch Loss: 0.00031188828870654106\n",
      "Epoch 3586, Loss: 0.014794005270232446, Final Batch Loss: 0.0007712648366577923\n",
      "Epoch 3587, Loss: 0.011561885185074061, Final Batch Loss: 0.0015717179048806429\n",
      "Epoch 3588, Loss: 0.011157311877468601, Final Batch Loss: 0.0004193669883534312\n",
      "Epoch 3589, Loss: 0.027376068450394087, Final Batch Loss: 0.000150195715832524\n",
      "Epoch 3590, Loss: 0.029239136434625834, Final Batch Loss: 0.00019269069889560342\n",
      "Epoch 3591, Loss: 0.010457121825311333, Final Batch Loss: 0.000894082710146904\n",
      "Epoch 3592, Loss: 0.027506375627126545, Final Batch Loss: 0.00035884720273315907\n",
      "Epoch 3593, Loss: 0.013185780691856053, Final Batch Loss: 0.0001209770780405961\n",
      "Epoch 3594, Loss: 0.010410605900688097, Final Batch Loss: 0.0027957588899880648\n",
      "Epoch 3595, Loss: 0.011895576259121299, Final Batch Loss: 0.0007351957610808313\n",
      "Epoch 3596, Loss: 0.004478796923649497, Final Batch Loss: 0.00023747178784105927\n",
      "Epoch 3597, Loss: 0.005547001608647406, Final Batch Loss: 0.0018308156868442893\n",
      "Epoch 3598, Loss: 0.006878638785565272, Final Batch Loss: 0.0010026977397501469\n",
      "Epoch 3599, Loss: 0.005157328021596186, Final Batch Loss: 7.104327960405499e-05\n",
      "Epoch 3600, Loss: 0.016009249433409423, Final Batch Loss: 0.0004214544896967709\n",
      "Epoch 3601, Loss: 0.007887341431342065, Final Batch Loss: 0.0010092424927279353\n",
      "Epoch 3602, Loss: 0.0033410193427698687, Final Batch Loss: 0.0003642371448222548\n",
      "Epoch 3603, Loss: 0.00799326048581861, Final Batch Loss: 0.0025499213952571154\n",
      "Epoch 3604, Loss: 0.015134293586015701, Final Batch Loss: 0.007904762402176857\n",
      "Epoch 3605, Loss: 0.007838119636289775, Final Batch Loss: 0.0003933127736672759\n",
      "Epoch 3606, Loss: 0.011992828556685708, Final Batch Loss: 0.00018328540318179876\n",
      "Epoch 3607, Loss: 0.03356494175386615, Final Batch Loss: 0.00014249258674681187\n",
      "Epoch 3608, Loss: 0.028462044123443775, Final Batch Loss: 0.0017070702742785215\n",
      "Epoch 3609, Loss: 0.04598199216707144, Final Batch Loss: 6.900310108903795e-05\n",
      "Epoch 3610, Loss: 0.0280032242735615, Final Batch Loss: 0.00020533915085252374\n",
      "Epoch 3611, Loss: 0.010359667066950351, Final Batch Loss: 0.001803252613171935\n",
      "Epoch 3612, Loss: 0.0102843887580093, Final Batch Loss: 0.002250240184366703\n",
      "Epoch 3613, Loss: 0.02748564613284543, Final Batch Loss: 0.0008066101581789553\n",
      "Epoch 3614, Loss: 0.00784653183654882, Final Batch Loss: 0.0043089683167636395\n",
      "Epoch 3615, Loss: 0.008559042486012913, Final Batch Loss: 0.00013961680815555155\n",
      "Epoch 3616, Loss: 0.023935222399813938, Final Batch Loss: 2.9953122066217475e-05\n",
      "Epoch 3617, Loss: 0.02995731108239852, Final Batch Loss: 0.002991880290210247\n",
      "Epoch 3618, Loss: 0.0036996137350797653, Final Batch Loss: 0.0002101001446135342\n",
      "Epoch 3619, Loss: 0.029211940855020657, Final Batch Loss: 0.0019249779870733619\n",
      "Epoch 3620, Loss: 0.007393359330308158, Final Batch Loss: 0.00010323228343622759\n",
      "Epoch 3621, Loss: 0.01508734547678614, Final Batch Loss: 8.71589218149893e-05\n",
      "Epoch 3622, Loss: 0.0495518067618832, Final Batch Loss: 0.011118989437818527\n",
      "Epoch 3623, Loss: 0.01486183874658309, Final Batch Loss: 0.00044964285916648805\n",
      "Epoch 3624, Loss: 0.015020660765003413, Final Batch Loss: 0.0036869391333311796\n",
      "Epoch 3625, Loss: 0.009898349468130618, Final Batch Loss: 0.0009195212624035776\n",
      "Epoch 3626, Loss: 0.020092012535315007, Final Batch Loss: 0.0007682053255848587\n",
      "Epoch 3627, Loss: 0.03746178587607574, Final Batch Loss: 0.0001230130874319002\n",
      "Epoch 3628, Loss: 0.045814601704478264, Final Batch Loss: 0.02463642880320549\n",
      "Epoch 3629, Loss: 0.029073305515339598, Final Batch Loss: 0.0008356817997992039\n",
      "Epoch 3630, Loss: 0.03703478001989424, Final Batch Loss: 0.00171061756554991\n",
      "Epoch 3631, Loss: 0.004481851268792525, Final Batch Loss: 0.0004141517565585673\n",
      "Epoch 3632, Loss: 0.012984775908989832, Final Batch Loss: 0.006833180319517851\n",
      "Epoch 3633, Loss: 0.021312483673682436, Final Batch Loss: 0.007177258376032114\n",
      "Epoch 3634, Loss: 0.013009398608119227, Final Batch Loss: 0.001472333911806345\n",
      "Epoch 3635, Loss: 0.022856075258459896, Final Batch Loss: 0.006500779651105404\n",
      "Epoch 3636, Loss: 0.013813847144774627, Final Batch Loss: 0.00011009739682776853\n",
      "Epoch 3637, Loss: 0.041800643608439714, Final Batch Loss: 0.00021134811686351895\n",
      "Epoch 3638, Loss: 0.02304245271079708, Final Batch Loss: 0.012180378660559654\n",
      "Epoch 3639, Loss: 0.018130907730665058, Final Batch Loss: 0.002211623592302203\n",
      "Epoch 3640, Loss: 0.05549364449689165, Final Batch Loss: 0.0006283945986069739\n",
      "Epoch 3641, Loss: 0.023575086088385433, Final Batch Loss: 0.0021851470228284597\n",
      "Epoch 3642, Loss: 0.01658670602773782, Final Batch Loss: 0.0010252869687974453\n",
      "Epoch 3643, Loss: 0.032368586003940436, Final Batch Loss: 2.734224653977435e-05\n",
      "Epoch 3644, Loss: 0.01466602724394761, Final Batch Loss: 0.00046585581731051207\n",
      "Epoch 3645, Loss: 0.04435427718271967, Final Batch Loss: 0.02039993181824684\n",
      "Epoch 3646, Loss: 0.01177745880704606, Final Batch Loss: 0.00010596685024211183\n",
      "Epoch 3647, Loss: 0.04447200789581984, Final Batch Loss: 0.0034446788486093283\n",
      "Epoch 3648, Loss: 0.006167997664306313, Final Batch Loss: 0.0007418409804813564\n",
      "Epoch 3649, Loss: 0.0162560321186902, Final Batch Loss: 0.0019176304340362549\n",
      "Epoch 3650, Loss: 0.004961531682056375, Final Batch Loss: 7.466763781849295e-05\n",
      "Epoch 3651, Loss: 0.011283169616945088, Final Batch Loss: 0.00042435058276169\n",
      "Epoch 3652, Loss: 0.06246614319388755, Final Batch Loss: 0.000478740461403504\n",
      "Epoch 3653, Loss: 0.0072086679574567825, Final Batch Loss: 0.00036189157981425524\n",
      "Epoch 3654, Loss: 0.049923923856113106, Final Batch Loss: 0.020326944068074226\n",
      "Epoch 3655, Loss: 0.018916753033408895, Final Batch Loss: 0.00033523570164106786\n",
      "Epoch 3656, Loss: 0.01092664452153258, Final Batch Loss: 0.0012055494589731097\n",
      "Epoch 3657, Loss: 0.010982144478475675, Final Batch Loss: 0.00037699288805015385\n",
      "Epoch 3658, Loss: 0.019547814008546993, Final Batch Loss: 0.0002470347099006176\n",
      "Epoch 3659, Loss: 0.0035755977805820294, Final Batch Loss: 4.5521250285673887e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3660, Loss: 0.006566184543771669, Final Batch Loss: 0.0020626168698072433\n",
      "Epoch 3661, Loss: 0.011788775096647441, Final Batch Loss: 0.000678837881423533\n",
      "Epoch 3662, Loss: 0.008274233085103333, Final Batch Loss: 0.0027936354745179415\n",
      "Epoch 3663, Loss: 0.006450743807363324, Final Batch Loss: 0.002512919483706355\n",
      "Epoch 3664, Loss: 0.004927649919409305, Final Batch Loss: 0.0016054159495979548\n",
      "Epoch 3665, Loss: 0.0042238977330271155, Final Batch Loss: 0.0014922270784154534\n",
      "Epoch 3666, Loss: 0.005107729048177134, Final Batch Loss: 0.00021881093562114984\n",
      "Epoch 3667, Loss: 0.011714305612258613, Final Batch Loss: 0.0009592680144123733\n",
      "Epoch 3668, Loss: 0.0038918061181902885, Final Batch Loss: 0.0007332511013373733\n",
      "Epoch 3669, Loss: 0.01989814336411655, Final Batch Loss: 0.00026227638591080904\n",
      "Epoch 3670, Loss: 0.006959564489079639, Final Batch Loss: 0.0003378911642357707\n",
      "Epoch 3671, Loss: 0.01261658439761959, Final Batch Loss: 0.0030400939285755157\n",
      "Epoch 3672, Loss: 0.05342868028674275, Final Batch Loss: 0.002400924451649189\n",
      "Epoch 3673, Loss: 0.059512977430131286, Final Batch Loss: 0.002252701437100768\n",
      "Epoch 3674, Loss: 0.015666927458369173, Final Batch Loss: 1.9769140635617077e-05\n",
      "Epoch 3675, Loss: 0.014269154620706104, Final Batch Loss: 0.0009116138680838048\n",
      "Epoch 3676, Loss: 0.02469369450409431, Final Batch Loss: 0.00024335157650057226\n",
      "Epoch 3677, Loss: 0.01560163042449858, Final Batch Loss: 0.00019860190514009446\n",
      "Epoch 3678, Loss: 0.03428758433437906, Final Batch Loss: 0.026006998494267464\n",
      "Epoch 3679, Loss: 0.008807912236079574, Final Batch Loss: 0.0014402022352442145\n",
      "Epoch 3680, Loss: 0.012697994672635105, Final Batch Loss: 9.520981257082894e-05\n",
      "Epoch 3681, Loss: 0.006802764488384128, Final Batch Loss: 0.0029088915325701237\n",
      "Epoch 3682, Loss: 0.0098477877327241, Final Batch Loss: 0.00043189001735299826\n",
      "Epoch 3683, Loss: 0.005238285011728294, Final Batch Loss: 0.001840200973674655\n",
      "Epoch 3684, Loss: 0.013305151427630335, Final Batch Loss: 0.00036566174821928144\n",
      "Epoch 3685, Loss: 0.011932112800423056, Final Batch Loss: 0.0001266022736672312\n",
      "Epoch 3686, Loss: 0.02085585254826583, Final Batch Loss: 0.006764846853911877\n",
      "Epoch 3687, Loss: 0.0029614778322866186, Final Batch Loss: 0.0005536449607461691\n",
      "Epoch 3688, Loss: 0.023608330404385924, Final Batch Loss: 0.000908415880985558\n",
      "Epoch 3689, Loss: 0.009039338532602414, Final Batch Loss: 0.002197747118771076\n",
      "Epoch 3690, Loss: 0.018364171832217835, Final Batch Loss: 5.1463204727042466e-05\n",
      "Epoch 3691, Loss: 0.0536587139358744, Final Batch Loss: 0.02963794767856598\n",
      "Epoch 3692, Loss: 0.03396201446594205, Final Batch Loss: 0.02815077267587185\n",
      "Epoch 3693, Loss: 0.023362874169833958, Final Batch Loss: 0.0011109075276181102\n",
      "Epoch 3694, Loss: 0.008145669766236097, Final Batch Loss: 0.00045945675810799\n",
      "Epoch 3695, Loss: 0.024096878827549517, Final Batch Loss: 0.002233480103313923\n",
      "Epoch 3696, Loss: 0.017640515899984166, Final Batch Loss: 0.0003496795252431184\n",
      "Epoch 3697, Loss: 0.05412737631195341, Final Batch Loss: 3.797132012550719e-05\n",
      "Epoch 3698, Loss: 0.08453111020207871, Final Batch Loss: 0.037600770592689514\n",
      "Epoch 3699, Loss: 0.08938647927425336, Final Batch Loss: 7.584916602354497e-05\n",
      "Epoch 3700, Loss: 0.04050805140286684, Final Batch Loss: 0.02730736881494522\n",
      "Epoch 3701, Loss: 0.0223471296048956, Final Batch Loss: 0.00014929119788575917\n",
      "Epoch 3702, Loss: 0.03343002137262374, Final Batch Loss: 0.001024021883495152\n",
      "Epoch 3703, Loss: 0.03046217223163694, Final Batch Loss: 0.012797226198017597\n",
      "Epoch 3704, Loss: 0.013590950635261834, Final Batch Loss: 0.0034393409732729197\n",
      "Epoch 3705, Loss: 0.023462756595108658, Final Batch Loss: 0.000837248342577368\n",
      "Epoch 3706, Loss: 0.02852956752758473, Final Batch Loss: 0.01733221486210823\n",
      "Epoch 3707, Loss: 0.01191490284691099, Final Batch Loss: 0.0016727414913475513\n",
      "Epoch 3708, Loss: 0.01221068698214367, Final Batch Loss: 0.002016048412770033\n",
      "Epoch 3709, Loss: 0.017393392510712147, Final Batch Loss: 0.008401169441640377\n",
      "Epoch 3710, Loss: 0.008311900892294943, Final Batch Loss: 0.0011467961594462395\n",
      "Epoch 3711, Loss: 0.005013108893763274, Final Batch Loss: 0.0012838066322728992\n",
      "Epoch 3712, Loss: 0.011755885483580641, Final Batch Loss: 0.00010617072985041887\n",
      "Epoch 3713, Loss: 0.00539400614798069, Final Batch Loss: 0.0008377304184250534\n",
      "Epoch 3714, Loss: 0.010686483496101573, Final Batch Loss: 0.0009157047024928033\n",
      "Epoch 3715, Loss: 0.005393685853050556, Final Batch Loss: 0.0014803501544520259\n",
      "Epoch 3716, Loss: 0.005645579629344866, Final Batch Loss: 0.0017733359709382057\n",
      "Epoch 3717, Loss: 0.005457781153381802, Final Batch Loss: 0.00041727512143552303\n",
      "Epoch 3718, Loss: 0.014227153616957366, Final Batch Loss: 0.0005759602063335478\n",
      "Epoch 3719, Loss: 0.00466978654731065, Final Batch Loss: 0.0002910550101660192\n",
      "Epoch 3720, Loss: 0.009931606939062476, Final Batch Loss: 0.0015733182663097978\n",
      "Epoch 3721, Loss: 0.020293060108087957, Final Batch Loss: 0.002006265101954341\n",
      "Epoch 3722, Loss: 0.0036910013295710087, Final Batch Loss: 0.0008931017364375293\n",
      "Epoch 3723, Loss: 0.004406116000609472, Final Batch Loss: 0.0004697418480645865\n",
      "Epoch 3724, Loss: 0.003408892938750796, Final Batch Loss: 0.0002379323123022914\n",
      "Epoch 3725, Loss: 0.013740733527811244, Final Batch Loss: 0.00027721282094717026\n",
      "Epoch 3726, Loss: 0.006048804600141011, Final Batch Loss: 0.0021322572138160467\n",
      "Epoch 3727, Loss: 0.011168162585818209, Final Batch Loss: 0.00017114759248215705\n",
      "Epoch 3728, Loss: 0.009515745812677778, Final Batch Loss: 0.00026359918410889804\n",
      "Epoch 3729, Loss: 0.02941972139524296, Final Batch Loss: 0.000411585730034858\n",
      "Epoch 3730, Loss: 0.005923710981733166, Final Batch Loss: 0.0004596116195898503\n",
      "Epoch 3731, Loss: 0.004935253877192736, Final Batch Loss: 0.00014959432883188128\n",
      "Epoch 3732, Loss: 0.0022287674983090255, Final Batch Loss: 4.554039696813561e-05\n",
      "Epoch 3733, Loss: 0.007215472276584478, Final Batch Loss: 5.241243707132526e-05\n",
      "Epoch 3734, Loss: 0.006437264186388347, Final Batch Loss: 0.0004420474579092115\n",
      "Epoch 3735, Loss: 0.01597434808354592, Final Batch Loss: 0.00012151966075180098\n",
      "Epoch 3736, Loss: 0.003727542993146926, Final Batch Loss: 0.0006366190500557423\n",
      "Epoch 3737, Loss: 0.005593217923888005, Final Batch Loss: 0.00026182286092080176\n",
      "Epoch 3738, Loss: 0.005287505060550757, Final Batch Loss: 0.0015428403858095407\n",
      "Epoch 3739, Loss: 0.08943071911926381, Final Batch Loss: 0.00044063484529033303\n",
      "Epoch 3740, Loss: 0.01693030184833333, Final Batch Loss: 0.0006823657313361764\n",
      "Epoch 3741, Loss: 0.0037275205977493897, Final Batch Loss: 0.00012571184197440743\n",
      "Epoch 3742, Loss: 0.003103145612840308, Final Batch Loss: 0.00019996303308289498\n",
      "Epoch 3743, Loss: 0.018976129813381704, Final Batch Loss: 4.325366535340436e-05\n",
      "Epoch 3744, Loss: 0.006117424607509747, Final Batch Loss: 0.0001806626096367836\n",
      "Epoch 3745, Loss: 0.007461845310899662, Final Batch Loss: 0.00018153783457819372\n",
      "Epoch 3746, Loss: 0.02588114663376473, Final Batch Loss: 0.00013081140059512109\n",
      "Epoch 3747, Loss: 0.020642857532948256, Final Batch Loss: 0.0009532087715342641\n",
      "Epoch 3748, Loss: 0.004270072437066119, Final Batch Loss: 0.00011269722745055333\n",
      "Epoch 3749, Loss: 0.014282007497968152, Final Batch Loss: 0.00015967560466378927\n",
      "Epoch 3750, Loss: 0.002916896999522578, Final Batch Loss: 0.0004163296543993056\n",
      "Epoch 3751, Loss: 0.013988706799864303, Final Batch Loss: 0.00011162991722812876\n",
      "Epoch 3752, Loss: 0.016175016746274196, Final Batch Loss: 0.012877590022981167\n",
      "Epoch 3753, Loss: 0.004780145434779115, Final Batch Loss: 0.0027085752226412296\n",
      "Epoch 3754, Loss: 0.006884785019792616, Final Batch Loss: 4.921725485473871e-05\n",
      "Epoch 3755, Loss: 0.007274154166225344, Final Batch Loss: 0.0028068101964890957\n",
      "Epoch 3756, Loss: 0.006974616975639947, Final Batch Loss: 0.0011247420916333795\n",
      "Epoch 3757, Loss: 0.08077450326527469, Final Batch Loss: 0.07268054038286209\n",
      "Epoch 3758, Loss: 0.01592033702036133, Final Batch Loss: 0.0022679155226796865\n",
      "Epoch 3759, Loss: 0.0038594780053244904, Final Batch Loss: 0.0002415433555142954\n",
      "Epoch 3760, Loss: 0.034084699203958735, Final Batch Loss: 0.0028471793048083782\n",
      "Epoch 3761, Loss: 0.004310378106310964, Final Batch Loss: 0.0008576111285947263\n",
      "Epoch 3762, Loss: 0.015051422436954454, Final Batch Loss: 0.00019731305656023324\n",
      "Epoch 3763, Loss: 0.022088885656557977, Final Batch Loss: 0.0010962110245600343\n",
      "Epoch 3764, Loss: 0.0032556907244725153, Final Batch Loss: 0.00031425926135852933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3765, Loss: 0.041333609566208906, Final Batch Loss: 0.002450590254738927\n",
      "Epoch 3766, Loss: 0.004753243760205805, Final Batch Loss: 0.000941468111705035\n",
      "Epoch 3767, Loss: 0.021266394178383052, Final Batch Loss: 0.0006723684491589665\n",
      "Epoch 3768, Loss: 0.0030376312060980126, Final Batch Loss: 0.0004775987472385168\n",
      "Epoch 3769, Loss: 0.03945627645589411, Final Batch Loss: 0.0005748128169216216\n",
      "Epoch 3770, Loss: 0.027114989396068268, Final Batch Loss: 0.021210823208093643\n",
      "Epoch 3771, Loss: 0.003798482270212844, Final Batch Loss: 0.00016123647219501436\n",
      "Epoch 3772, Loss: 0.004871554265264422, Final Batch Loss: 0.0004471339634619653\n",
      "Epoch 3773, Loss: 0.004485398141696351, Final Batch Loss: 4.627147063729353e-05\n",
      "Epoch 3774, Loss: 0.004212968618958257, Final Batch Loss: 9.847023466136307e-05\n",
      "Epoch 3775, Loss: 0.029938709078123793, Final Batch Loss: 0.001353090745396912\n",
      "Epoch 3776, Loss: 0.0027582606417126954, Final Batch Loss: 0.0008184367325156927\n",
      "Epoch 3777, Loss: 0.013345491519430652, Final Batch Loss: 0.00035563731216825545\n",
      "Epoch 3778, Loss: 0.04619938542600721, Final Batch Loss: 0.019394826143980026\n",
      "Epoch 3779, Loss: 0.005424554547062144, Final Batch Loss: 0.0006730033783242106\n",
      "Epoch 3780, Loss: 0.005950212449533865, Final Batch Loss: 0.0002992055960930884\n",
      "Epoch 3781, Loss: 0.0048225909122265875, Final Batch Loss: 0.00013726280303671956\n",
      "Epoch 3782, Loss: 0.012532804306829348, Final Batch Loss: 0.0004699859709944576\n",
      "Epoch 3783, Loss: 0.010900126537308097, Final Batch Loss: 0.0005089084152132273\n",
      "Epoch 3784, Loss: 0.030440386894042604, Final Batch Loss: 0.011231772601604462\n",
      "Epoch 3785, Loss: 0.03339949157088995, Final Batch Loss: 0.001412559300661087\n",
      "Epoch 3786, Loss: 0.009406978992046788, Final Batch Loss: 0.00209673005156219\n",
      "Epoch 3787, Loss: 0.025677560071926564, Final Batch Loss: 0.0021641491912305355\n",
      "Epoch 3788, Loss: 0.016748991280110204, Final Batch Loss: 1.4362924048327841e-05\n",
      "Epoch 3789, Loss: 0.025416205578949302, Final Batch Loss: 0.0004006321541965008\n",
      "Epoch 3790, Loss: 0.008669608796481043, Final Batch Loss: 0.001628466765396297\n",
      "Epoch 3791, Loss: 0.02884045339305885, Final Batch Loss: 0.0003219204081688076\n",
      "Epoch 3792, Loss: 0.017587830167030916, Final Batch Loss: 0.00029752086265943944\n",
      "Epoch 3793, Loss: 0.016549247840885073, Final Batch Loss: 0.0008086831658147275\n",
      "Epoch 3794, Loss: 0.014281748444773257, Final Batch Loss: 0.0033386745490133762\n",
      "Epoch 3795, Loss: 0.043996100586809916, Final Batch Loss: 4.686925603891723e-05\n",
      "Epoch 3796, Loss: 0.0070082516758702695, Final Batch Loss: 0.0034252479672431946\n",
      "Epoch 3797, Loss: 0.021818217064719647, Final Batch Loss: 0.003144745947793126\n",
      "Epoch 3798, Loss: 0.008960815765021835, Final Batch Loss: 9.335665527032688e-05\n",
      "Epoch 3799, Loss: 0.005204859669902362, Final Batch Loss: 8.626923954579979e-05\n",
      "Epoch 3800, Loss: 0.015430940919031855, Final Batch Loss: 0.00011284558422630653\n",
      "Epoch 3801, Loss: 0.029334432736504823, Final Batch Loss: 0.014407189562916756\n",
      "Epoch 3802, Loss: 0.013053487386059714, Final Batch Loss: 5.856390271219425e-05\n",
      "Epoch 3803, Loss: 0.018875548208598047, Final Batch Loss: 0.003310376312583685\n",
      "Epoch 3804, Loss: 0.029524185927584767, Final Batch Loss: 0.009862756356596947\n",
      "Epoch 3805, Loss: 0.005916239169891924, Final Batch Loss: 0.0010915062157437205\n",
      "Epoch 3806, Loss: 0.016294462839141488, Final Batch Loss: 0.0006868373020552099\n",
      "Epoch 3807, Loss: 0.008007073658518493, Final Batch Loss: 0.004610896576195955\n",
      "Epoch 3808, Loss: 0.0046574880834668875, Final Batch Loss: 0.00023854125174693763\n",
      "Epoch 3809, Loss: 0.032939313212409616, Final Batch Loss: 0.006195093505084515\n",
      "Epoch 3810, Loss: 0.005181931977858767, Final Batch Loss: 0.0020188477355986834\n",
      "Epoch 3811, Loss: 0.024346154648810625, Final Batch Loss: 0.0015694629400968552\n",
      "Epoch 3812, Loss: 0.02323618793161586, Final Batch Loss: 0.006822282448410988\n",
      "Epoch 3813, Loss: 0.03684598031031783, Final Batch Loss: 0.030857332050800323\n",
      "Epoch 3814, Loss: 0.018313096137717366, Final Batch Loss: 0.00016891100676730275\n",
      "Epoch 3815, Loss: 0.02189915964845568, Final Batch Loss: 0.0008017351501621306\n",
      "Epoch 3816, Loss: 0.01413642459010589, Final Batch Loss: 3.93000191252213e-05\n",
      "Epoch 3817, Loss: 0.0391747493413277, Final Batch Loss: 0.00020950321049895138\n",
      "Epoch 3818, Loss: 0.011617449839832261, Final Batch Loss: 0.00010430210386402905\n",
      "Epoch 3819, Loss: 0.005528432608116418, Final Batch Loss: 0.0007976805791258812\n",
      "Epoch 3820, Loss: 0.015124324840144254, Final Batch Loss: 0.003647679230198264\n",
      "Epoch 3821, Loss: 0.04539335856679827, Final Batch Loss: 0.040233030915260315\n",
      "Epoch 3822, Loss: 0.030655391630716622, Final Batch Loss: 0.002904803492128849\n",
      "Epoch 3823, Loss: 0.016371949517633766, Final Batch Loss: 0.00013576698256656528\n",
      "Epoch 3824, Loss: 0.005682094633812085, Final Batch Loss: 0.0010051720310002565\n",
      "Epoch 3825, Loss: 0.009705555668915622, Final Batch Loss: 0.00588619289919734\n",
      "Epoch 3826, Loss: 0.006767603103071451, Final Batch Loss: 0.00015589593385811895\n",
      "Epoch 3827, Loss: 0.01055191084742546, Final Batch Loss: 0.0005530858761630952\n",
      "Epoch 3828, Loss: 0.004349559221736854, Final Batch Loss: 0.002360488520935178\n",
      "Epoch 3829, Loss: 0.01001392415491864, Final Batch Loss: 0.0003386167227290571\n",
      "Epoch 3830, Loss: 0.009892864036373794, Final Batch Loss: 0.006319752428680658\n",
      "Epoch 3831, Loss: 0.029065110720694065, Final Batch Loss: 0.002415150636807084\n",
      "Epoch 3832, Loss: 0.028884484490845352, Final Batch Loss: 0.005949231795966625\n",
      "Epoch 3833, Loss: 0.01968488487182185, Final Batch Loss: 0.00445147231221199\n",
      "Epoch 3834, Loss: 0.006637432263232768, Final Batch Loss: 0.00044890266144648194\n",
      "Epoch 3835, Loss: 0.01362212622188963, Final Batch Loss: 0.0002320069179404527\n",
      "Epoch 3836, Loss: 0.03569248956046067, Final Batch Loss: 0.02530881203711033\n",
      "Epoch 3837, Loss: 0.00525954453041777, Final Batch Loss: 0.0009154516155831516\n",
      "Epoch 3838, Loss: 0.008035806124098599, Final Batch Loss: 0.00015383699792437255\n",
      "Epoch 3839, Loss: 0.00829604800674133, Final Batch Loss: 0.0050139580853283405\n",
      "Epoch 3840, Loss: 0.038291354139801115, Final Batch Loss: 0.018934130668640137\n",
      "Epoch 3841, Loss: 0.0022759756902814843, Final Batch Loss: 9.681814844952896e-05\n",
      "Epoch 3842, Loss: 0.010790037864353508, Final Batch Loss: 0.0006066960631869733\n",
      "Epoch 3843, Loss: 0.03463265355458134, Final Batch Loss: 0.032186370342969894\n",
      "Epoch 3844, Loss: 0.007491362717701122, Final Batch Loss: 0.0022553494200110435\n",
      "Epoch 3845, Loss: 0.028767319541657344, Final Batch Loss: 0.0002841350215021521\n",
      "Epoch 3846, Loss: 0.018983912275871262, Final Batch Loss: 0.0013866607332602143\n",
      "Epoch 3847, Loss: 0.0069880333612672985, Final Batch Loss: 0.0014025961281731725\n",
      "Epoch 3848, Loss: 0.006274772596952971, Final Batch Loss: 0.0005870852037332952\n",
      "Epoch 3849, Loss: 0.006692701383144595, Final Batch Loss: 0.0010208552703261375\n",
      "Epoch 3850, Loss: 0.024506280780769885, Final Batch Loss: 0.000705825281329453\n",
      "Epoch 3851, Loss: 0.034427555845468305, Final Batch Loss: 0.011353873647749424\n",
      "Epoch 3852, Loss: 0.007940420386148617, Final Batch Loss: 0.0003461359883658588\n",
      "Epoch 3853, Loss: 0.005823398532811552, Final Batch Loss: 9.168521501123905e-05\n",
      "Epoch 3854, Loss: 0.003411231402424164, Final Batch Loss: 0.0001357985456706956\n",
      "Epoch 3855, Loss: 0.02294635641737841, Final Batch Loss: 0.012381035834550858\n",
      "Epoch 3856, Loss: 0.0027907283511012793, Final Batch Loss: 0.00010797852883115411\n",
      "Epoch 3857, Loss: 0.016399028434534557, Final Batch Loss: 0.0012863570591434836\n",
      "Epoch 3858, Loss: 0.011348592699505389, Final Batch Loss: 0.0014788525877520442\n",
      "Epoch 3859, Loss: 0.014786018524318933, Final Batch Loss: 0.005133748985826969\n",
      "Epoch 3860, Loss: 0.032347510423278436, Final Batch Loss: 0.004055927507579327\n",
      "Epoch 3861, Loss: 0.035572393506299704, Final Batch Loss: 0.0010121283121407032\n",
      "Epoch 3862, Loss: 0.009994260326493531, Final Batch Loss: 0.0035286261700093746\n",
      "Epoch 3863, Loss: 0.04701323976041749, Final Batch Loss: 0.0443049781024456\n",
      "Epoch 3864, Loss: 0.013811038181302138, Final Batch Loss: 0.00013222747656982392\n",
      "Epoch 3865, Loss: 0.004462724624318071, Final Batch Loss: 0.00014938219101168215\n",
      "Epoch 3866, Loss: 0.0665679145604372, Final Batch Loss: 0.031932491809129715\n",
      "Epoch 3867, Loss: 0.005799582082545385, Final Batch Loss: 0.00027595649589784443\n",
      "Epoch 3868, Loss: 0.017126938211731613, Final Batch Loss: 0.001929630059748888\n",
      "Epoch 3869, Loss: 0.021153895038878545, Final Batch Loss: 0.0012089675292372704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3870, Loss: 0.0035955135608674027, Final Batch Loss: 0.0006192530272528529\n",
      "Epoch 3871, Loss: 0.00846059582545422, Final Batch Loss: 0.0034869227092713118\n",
      "Epoch 3872, Loss: 0.0058833142393268645, Final Batch Loss: 0.0005980132846161723\n",
      "Epoch 3873, Loss: 0.0081770287360996, Final Batch Loss: 0.00034668328589759767\n",
      "Epoch 3874, Loss: 0.003936215594876558, Final Batch Loss: 0.000272879347903654\n",
      "Epoch 3875, Loss: 0.00900462461868301, Final Batch Loss: 0.0018521974561735988\n",
      "Epoch 3876, Loss: 0.013476594212988857, Final Batch Loss: 0.0002449066087137908\n",
      "Epoch 3877, Loss: 0.006000960056553595, Final Batch Loss: 0.003989619202911854\n",
      "Epoch 3878, Loss: 0.021315673016943038, Final Batch Loss: 0.0013229141477495432\n",
      "Epoch 3879, Loss: 0.02922211747500114, Final Batch Loss: 0.00021443446166813374\n",
      "Epoch 3880, Loss: 0.019552895217202604, Final Batch Loss: 0.002680732635781169\n",
      "Epoch 3881, Loss: 0.004926729277940467, Final Batch Loss: 0.002066695364192128\n",
      "Epoch 3882, Loss: 0.11456511670257896, Final Batch Loss: 0.01385951042175293\n",
      "Epoch 3883, Loss: 0.00855224922997877, Final Batch Loss: 0.0025247614830732346\n",
      "Epoch 3884, Loss: 0.015355703842942603, Final Batch Loss: 0.0003072700637858361\n",
      "Epoch 3885, Loss: 0.01892205240437761, Final Batch Loss: 0.00031541299540549517\n",
      "Epoch 3886, Loss: 0.004221234034048393, Final Batch Loss: 0.00048038584645837545\n",
      "Epoch 3887, Loss: 0.008970572263933718, Final Batch Loss: 0.00027602369664236903\n",
      "Epoch 3888, Loss: 0.012804890284314752, Final Batch Loss: 0.00037117613828741014\n",
      "Epoch 3889, Loss: 0.028632696590648266, Final Batch Loss: 3.843695230898447e-05\n",
      "Epoch 3890, Loss: 0.005111887978273444, Final Batch Loss: 0.0031503522768616676\n",
      "Epoch 3891, Loss: 0.010933892568573356, Final Batch Loss: 0.0003762452397495508\n",
      "Epoch 3892, Loss: 0.0076627285743597895, Final Batch Loss: 0.00035268074134364724\n",
      "Epoch 3893, Loss: 0.004591782271745615, Final Batch Loss: 0.0001807970693334937\n",
      "Epoch 3894, Loss: 0.006132257636636496, Final Batch Loss: 0.00024745240807533264\n",
      "Epoch 3895, Loss: 0.004461310156330001, Final Batch Loss: 0.00011783523223130032\n",
      "Epoch 3896, Loss: 0.004965791129507124, Final Batch Loss: 0.0005762287182733417\n",
      "Epoch 3897, Loss: 0.026796932448633015, Final Batch Loss: 0.021001972258090973\n",
      "Epoch 3898, Loss: 0.026085824698384386, Final Batch Loss: 0.0008262918563559651\n",
      "Epoch 3899, Loss: 0.007676866895053536, Final Batch Loss: 0.0004780791059602052\n",
      "Epoch 3900, Loss: 0.0028289851325098425, Final Batch Loss: 0.0002821716188918799\n",
      "Epoch 3901, Loss: 0.010153678347705863, Final Batch Loss: 0.007081508636474609\n",
      "Epoch 3902, Loss: 0.03534538293024525, Final Batch Loss: 0.00115956028457731\n",
      "Epoch 3903, Loss: 0.08002033625962213, Final Batch Loss: 0.00020125845912843943\n",
      "Epoch 3904, Loss: 0.022636366717051715, Final Batch Loss: 0.0008450859459117055\n",
      "Epoch 3905, Loss: 0.011226662158151157, Final Batch Loss: 0.002288817660883069\n",
      "Epoch 3906, Loss: 0.0335384797945153, Final Batch Loss: 0.0008469042368233204\n",
      "Epoch 3907, Loss: 0.04927243400015868, Final Batch Loss: 0.01690438948571682\n",
      "Epoch 3908, Loss: 0.012161250022472814, Final Batch Loss: 0.0002876614744309336\n",
      "Epoch 3909, Loss: 0.008666775363963097, Final Batch Loss: 0.001364581286907196\n",
      "Epoch 3910, Loss: 0.007388003868982196, Final Batch Loss: 0.002532119629904628\n",
      "Epoch 3911, Loss: 0.02560913740308024, Final Batch Loss: 0.001517542521469295\n",
      "Epoch 3912, Loss: 0.007781743071973324, Final Batch Loss: 0.0003395907988306135\n",
      "Epoch 3913, Loss: 0.02973730003577657, Final Batch Loss: 0.006560194306075573\n",
      "Epoch 3914, Loss: 0.0036726664984598756, Final Batch Loss: 0.0002325375535292551\n",
      "Epoch 3915, Loss: 0.005212630203459412, Final Batch Loss: 0.0011320915073156357\n",
      "Epoch 3916, Loss: 0.017886069443193264, Final Batch Loss: 0.00021762058895546943\n",
      "Epoch 3917, Loss: 0.017954496666789055, Final Batch Loss: 0.0005116077372804284\n",
      "Epoch 3918, Loss: 0.011706934659741819, Final Batch Loss: 0.0007022179779596627\n",
      "Epoch 3919, Loss: 0.005833495320985094, Final Batch Loss: 0.001101688016206026\n",
      "Epoch 3920, Loss: 0.013976038142573088, Final Batch Loss: 0.003729879390448332\n",
      "Epoch 3921, Loss: 0.010947417235001922, Final Batch Loss: 0.0045520537532866\n",
      "Epoch 3922, Loss: 0.030362450634129345, Final Batch Loss: 0.02623019739985466\n",
      "Epoch 3923, Loss: 0.006644807261181995, Final Batch Loss: 0.0011233171680942178\n",
      "Epoch 3924, Loss: 0.035471951254294254, Final Batch Loss: 0.00011741438356693834\n",
      "Epoch 3925, Loss: 0.030466234806226566, Final Batch Loss: 0.01567857339978218\n",
      "Epoch 3926, Loss: 0.014445272041484714, Final Batch Loss: 0.004271837417036295\n",
      "Epoch 3927, Loss: 0.0030151661121635698, Final Batch Loss: 7.408073724946007e-05\n",
      "Epoch 3928, Loss: 0.0028479268148657866, Final Batch Loss: 0.00010268556798109785\n",
      "Epoch 3929, Loss: 0.0019293714285595343, Final Batch Loss: 0.0004305928305257112\n",
      "Epoch 3930, Loss: 0.042139177094213665, Final Batch Loss: 0.0021374074276536703\n",
      "Epoch 3931, Loss: 0.043661169707775116, Final Batch Loss: 0.02530510164797306\n",
      "Epoch 3932, Loss: 0.01644079743709881, Final Batch Loss: 0.0001612475170986727\n",
      "Epoch 3933, Loss: 0.017734694731188938, Final Batch Loss: 0.0011065948056057096\n",
      "Epoch 3934, Loss: 0.005677351975464262, Final Batch Loss: 0.0002310914424015209\n",
      "Epoch 3935, Loss: 0.013028793327976018, Final Batch Loss: 0.0004962274106219411\n",
      "Epoch 3936, Loss: 0.022123132250271738, Final Batch Loss: 0.00064547557849437\n",
      "Epoch 3937, Loss: 0.0046657164275529794, Final Batch Loss: 7.282947626663372e-05\n",
      "Epoch 3938, Loss: 0.022765009009162895, Final Batch Loss: 0.004047842230647802\n",
      "Epoch 3939, Loss: 0.005283442005747929, Final Batch Loss: 0.00042109822970815003\n",
      "Epoch 3940, Loss: 0.015584426902933046, Final Batch Loss: 0.0001741169107845053\n",
      "Epoch 3941, Loss: 0.021749858395196497, Final Batch Loss: 0.0013880690094083548\n",
      "Epoch 3942, Loss: 0.004994013663235819, Final Batch Loss: 0.0010306781623512506\n",
      "Epoch 3943, Loss: 0.0059317467457731254, Final Batch Loss: 0.0029530595056712627\n",
      "Epoch 3944, Loss: 0.03628668139572255, Final Batch Loss: 0.030017990618944168\n",
      "Epoch 3945, Loss: 0.025790106985368766, Final Batch Loss: 0.00015957627329044044\n",
      "Epoch 3946, Loss: 0.019166313984896988, Final Batch Loss: 0.0007656842935830355\n",
      "Epoch 3947, Loss: 0.007273822004208341, Final Batch Loss: 0.0006543891504406929\n",
      "Epoch 3948, Loss: 0.004620038467692211, Final Batch Loss: 0.00033849620376713574\n",
      "Epoch 3949, Loss: 0.010853686530026607, Final Batch Loss: 0.0005666805664077401\n",
      "Epoch 3950, Loss: 0.0038907292764633894, Final Batch Loss: 0.0014806328108534217\n",
      "Epoch 3951, Loss: 0.01710644418199081, Final Batch Loss: 0.00018000260752160102\n",
      "Epoch 3952, Loss: 0.004398780438350514, Final Batch Loss: 0.0016873598797246814\n",
      "Epoch 3953, Loss: 0.005984287041428615, Final Batch Loss: 0.0006720976089127362\n",
      "Epoch 3954, Loss: 0.01438151329421089, Final Batch Loss: 4.7853569412836805e-05\n",
      "Epoch 3955, Loss: 0.004286726281861775, Final Batch Loss: 0.0014397120103240013\n",
      "Epoch 3956, Loss: 0.026298504701117054, Final Batch Loss: 0.017085852101445198\n",
      "Epoch 3957, Loss: 0.0357877571877907, Final Batch Loss: 5.3590549214277416e-05\n",
      "Epoch 3958, Loss: 0.006188725383253768, Final Batch Loss: 0.0008762692450545728\n",
      "Epoch 3959, Loss: 0.018067327669996303, Final Batch Loss: 0.010431252419948578\n",
      "Epoch 3960, Loss: 0.014884206117130816, Final Batch Loss: 0.0011476305080577731\n",
      "Epoch 3961, Loss: 0.014278415008448064, Final Batch Loss: 0.007864209823310375\n",
      "Epoch 3962, Loss: 0.005738664884120226, Final Batch Loss: 0.0008707095985300839\n",
      "Epoch 3963, Loss: 0.026346809325332288, Final Batch Loss: 0.015936480835080147\n",
      "Epoch 3964, Loss: 0.0205856567772571, Final Batch Loss: 0.005367372650653124\n",
      "Epoch 3965, Loss: 0.007882103978772648, Final Batch Loss: 0.005591399036347866\n",
      "Epoch 3966, Loss: 0.030263341206591576, Final Batch Loss: 0.01914154551923275\n",
      "Epoch 3967, Loss: 0.021075528318760917, Final Batch Loss: 0.00018557196017354727\n",
      "Epoch 3968, Loss: 0.003468807233730331, Final Batch Loss: 0.00035269721411168575\n",
      "Epoch 3969, Loss: 0.010705751366913319, Final Batch Loss: 0.0017676542047411203\n",
      "Epoch 3970, Loss: 0.009809914641664363, Final Batch Loss: 0.00028613783069886267\n",
      "Epoch 3971, Loss: 0.09139235271140933, Final Batch Loss: 9.765330469235778e-05\n",
      "Epoch 3972, Loss: 0.010577379900496453, Final Batch Loss: 0.003520905040204525\n",
      "Epoch 3973, Loss: 0.006746909173671156, Final Batch Loss: 0.00030606050859205425\n",
      "Epoch 3974, Loss: 0.0033343607210554183, Final Batch Loss: 0.0001414247672073543\n",
      "Epoch 3975, Loss: 0.03906412834476214, Final Batch Loss: 0.0016622284892946482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3976, Loss: 0.015117692673811689, Final Batch Loss: 0.0002249687968287617\n",
      "Epoch 3977, Loss: 0.017399239950464107, Final Batch Loss: 0.00013262576248962432\n",
      "Epoch 3978, Loss: 0.010555413042311557, Final Batch Loss: 0.0014923596754670143\n",
      "Epoch 3979, Loss: 0.02605313219828531, Final Batch Loss: 0.009899475611746311\n",
      "Epoch 3980, Loss: 0.01603537710616365, Final Batch Loss: 0.0005686561344191432\n",
      "Epoch 3981, Loss: 0.0699535414314596, Final Batch Loss: 0.00012745008280035108\n",
      "Epoch 3982, Loss: 0.007622211676789448, Final Batch Loss: 0.0008852045866660774\n",
      "Epoch 3983, Loss: 0.00904371467186138, Final Batch Loss: 0.0026738885790109634\n",
      "Epoch 3984, Loss: 0.010898159525822848, Final Batch Loss: 0.0006127554806880653\n",
      "Epoch 3985, Loss: 0.014960870539653115, Final Batch Loss: 0.009346211329102516\n",
      "Epoch 3986, Loss: 0.04663422048906796, Final Batch Loss: 0.0009198948973789811\n",
      "Epoch 3987, Loss: 0.014350286917760968, Final Batch Loss: 0.00024778314400464296\n",
      "Epoch 3988, Loss: 0.01407391089014709, Final Batch Loss: 0.003814299823716283\n",
      "Epoch 3989, Loss: 0.027401987441407982, Final Batch Loss: 0.00034218039945699275\n",
      "Epoch 3990, Loss: 0.024250121554359794, Final Batch Loss: 0.004474536515772343\n",
      "Epoch 3991, Loss: 0.10090397724707145, Final Batch Loss: 0.06800441443920135\n",
      "Epoch 3992, Loss: 0.01291004462109413, Final Batch Loss: 0.0017127853352576494\n",
      "Epoch 3993, Loss: 0.019046011002501473, Final Batch Loss: 0.0036410088650882244\n",
      "Epoch 3994, Loss: 0.03648235439322889, Final Batch Loss: 0.00023306801449507475\n",
      "Epoch 3995, Loss: 0.008997810684377328, Final Batch Loss: 0.0005504635628312826\n",
      "Epoch 3996, Loss: 0.01577057657414116, Final Batch Loss: 0.0004631514602806419\n",
      "Epoch 3997, Loss: 0.00776502194639761, Final Batch Loss: 0.003652885789051652\n",
      "Epoch 3998, Loss: 0.04145413771038875, Final Batch Loss: 0.009828553535044193\n",
      "Epoch 3999, Loss: 0.02411169116385281, Final Batch Loss: 0.0015220681671053171\n",
      "Epoch 4000, Loss: 0.0044164044738863595, Final Batch Loss: 0.0009625320672057569\n",
      "Epoch 4001, Loss: 0.00569109249045141, Final Batch Loss: 0.0024248019326478243\n",
      "Epoch 4002, Loss: 0.030306587257655337, Final Batch Loss: 0.0011147897457703948\n",
      "Epoch 4003, Loss: 0.034808440192136914, Final Batch Loss: 0.006639412138611078\n",
      "Epoch 4004, Loss: 0.016155562014319003, Final Batch Loss: 0.0013307709014043212\n",
      "Epoch 4005, Loss: 0.015992606553481892, Final Batch Loss: 0.0023938051890581846\n",
      "Epoch 4006, Loss: 0.05389039620058611, Final Batch Loss: 0.00017300742911174893\n",
      "Epoch 4007, Loss: 0.013897011944209225, Final Batch Loss: 0.006659577134996653\n",
      "Epoch 4008, Loss: 0.023183025768958032, Final Batch Loss: 9.131486876867712e-05\n",
      "Epoch 4009, Loss: 0.023483236516767647, Final Batch Loss: 0.0034406990744173527\n",
      "Epoch 4010, Loss: 0.020531191490590572, Final Batch Loss: 0.000762779382057488\n",
      "Epoch 4011, Loss: 0.014524367987178266, Final Batch Loss: 0.001149861142039299\n",
      "Epoch 4012, Loss: 0.016401994973421097, Final Batch Loss: 0.0037668272852897644\n",
      "Epoch 4013, Loss: 0.005230456074059475, Final Batch Loss: 0.00010175910574616864\n",
      "Epoch 4014, Loss: 0.011993165244348347, Final Batch Loss: 0.0016369762597605586\n",
      "Epoch 4015, Loss: 0.016904105112189427, Final Batch Loss: 0.00029477200587280095\n",
      "Epoch 4016, Loss: 0.02548709191614762, Final Batch Loss: 6.123777711763978e-05\n",
      "Epoch 4017, Loss: 0.03211911738617346, Final Batch Loss: 0.00699137058109045\n",
      "Epoch 4018, Loss: 0.06702775205485523, Final Batch Loss: 0.014096545986831188\n",
      "Epoch 4019, Loss: 0.07992252486292273, Final Batch Loss: 0.030495308339595795\n",
      "Epoch 4020, Loss: 0.02380206636735238, Final Batch Loss: 0.009904724545776844\n",
      "Epoch 4021, Loss: 0.009197089326335117, Final Batch Loss: 0.0001449054980184883\n",
      "Epoch 4022, Loss: 0.012784190577804111, Final Batch Loss: 0.0031317437533289194\n",
      "Epoch 4023, Loss: 0.02224780645337887, Final Batch Loss: 0.0006658600177615881\n",
      "Epoch 4024, Loss: 0.014593355997931212, Final Batch Loss: 0.005866806488484144\n",
      "Epoch 4025, Loss: 0.02184310320444638, Final Batch Loss: 5.606775084743276e-05\n",
      "Epoch 4026, Loss: 0.008010744175408036, Final Batch Loss: 0.0012422142317518592\n",
      "Epoch 4027, Loss: 0.009091708954656497, Final Batch Loss: 0.00028107711113989353\n",
      "Epoch 4028, Loss: 0.00663424379308708, Final Batch Loss: 0.0002775948669295758\n",
      "Epoch 4029, Loss: 0.00543913125875406, Final Batch Loss: 0.0005261023179627955\n",
      "Epoch 4030, Loss: 0.009549490307108499, Final Batch Loss: 9.160181798506528e-05\n",
      "Epoch 4031, Loss: 0.0514415132929571, Final Batch Loss: 0.046630874276161194\n",
      "Epoch 4032, Loss: 0.029625747178215533, Final Batch Loss: 0.012880777940154076\n",
      "Epoch 4033, Loss: 0.0101792431451031, Final Batch Loss: 0.00010116917110281065\n",
      "Epoch 4034, Loss: 0.016419448103988543, Final Batch Loss: 0.00014427831047214568\n",
      "Epoch 4035, Loss: 0.04527569189667702, Final Batch Loss: 0.021949416026473045\n",
      "Epoch 4036, Loss: 0.010624712595017627, Final Batch Loss: 0.006136924494057894\n",
      "Epoch 4037, Loss: 0.018515112722525373, Final Batch Loss: 0.014772860333323479\n",
      "Epoch 4038, Loss: 0.03391024371376261, Final Batch Loss: 0.004408723674714565\n",
      "Epoch 4039, Loss: 0.023118148383218795, Final Batch Loss: 0.014079950749874115\n",
      "Epoch 4040, Loss: 0.03499918604211416, Final Batch Loss: 0.002956908429041505\n",
      "Epoch 4041, Loss: 0.021219070942606777, Final Batch Loss: 0.0006798031972721219\n",
      "Epoch 4042, Loss: 0.015539373758656438, Final Batch Loss: 8.512569911545143e-05\n",
      "Epoch 4043, Loss: 0.007076697103912011, Final Batch Loss: 0.0016466428060084581\n",
      "Epoch 4044, Loss: 0.038680874393321574, Final Batch Loss: 0.0011693575652316213\n",
      "Epoch 4045, Loss: 0.012429845271981321, Final Batch Loss: 4.902679938822985e-05\n",
      "Epoch 4046, Loss: 0.010908159601967782, Final Batch Loss: 0.0023786218371242285\n",
      "Epoch 4047, Loss: 0.013614807568956167, Final Batch Loss: 0.0007743354653939605\n",
      "Epoch 4048, Loss: 0.044634365825913846, Final Batch Loss: 0.001794475712813437\n",
      "Epoch 4049, Loss: 0.0051007843285333365, Final Batch Loss: 0.0003395971725694835\n",
      "Epoch 4050, Loss: 0.012843685573898256, Final Batch Loss: 0.0015154257416725159\n",
      "Epoch 4051, Loss: 0.02118365820206236, Final Batch Loss: 0.018512701615691185\n",
      "Epoch 4052, Loss: 0.022687571356073022, Final Batch Loss: 0.00015064660692587495\n",
      "Epoch 4053, Loss: 0.007156527630286291, Final Batch Loss: 0.0013714944943785667\n",
      "Epoch 4054, Loss: 0.007897079412941821, Final Batch Loss: 0.00014791215653531253\n",
      "Epoch 4055, Loss: 0.017389940010616556, Final Batch Loss: 0.0006522590992972255\n",
      "Epoch 4056, Loss: 0.0052850655920337886, Final Batch Loss: 0.00041913686436600983\n",
      "Epoch 4057, Loss: 0.010679545113816857, Final Batch Loss: 0.0006108375382609665\n",
      "Epoch 4058, Loss: 0.020094718580367044, Final Batch Loss: 0.0016019395552575588\n",
      "Epoch 4059, Loss: 0.02480971597833559, Final Batch Loss: 0.0004595234349835664\n",
      "Epoch 4060, Loss: 0.005844048399012536, Final Batch Loss: 0.0017145115416496992\n",
      "Epoch 4061, Loss: 0.04008591268211603, Final Batch Loss: 0.00028821316664107144\n",
      "Epoch 4062, Loss: 0.020295337482821196, Final Batch Loss: 0.009867937304079533\n",
      "Epoch 4063, Loss: 0.0032265146874124184, Final Batch Loss: 0.00011896256182808429\n",
      "Epoch 4064, Loss: 0.013077462164801545, Final Batch Loss: 0.00011229912342969328\n",
      "Epoch 4065, Loss: 0.008190837106667459, Final Batch Loss: 0.0005683948402293026\n",
      "Epoch 4066, Loss: 0.01657203314243816, Final Batch Loss: 0.0016214607749134302\n",
      "Epoch 4067, Loss: 0.008124065992888063, Final Batch Loss: 0.0008062387350946665\n",
      "Epoch 4068, Loss: 0.009384859375131782, Final Batch Loss: 0.00010974852921208367\n",
      "Epoch 4069, Loss: 0.012204975919303251, Final Batch Loss: 0.0015457692788913846\n",
      "Epoch 4070, Loss: 0.009120484442973975, Final Batch Loss: 0.0004972917377017438\n",
      "Epoch 4071, Loss: 0.006023085516062565, Final Batch Loss: 0.0005928011960349977\n",
      "Epoch 4072, Loss: 0.004219924710923806, Final Batch Loss: 0.00042424086132086813\n",
      "Epoch 4073, Loss: 0.0549398519215174, Final Batch Loss: 0.00047890894347801805\n",
      "Epoch 4074, Loss: 0.031103221102966927, Final Batch Loss: 0.0037874251138418913\n",
      "Epoch 4075, Loss: 0.007395153428660706, Final Batch Loss: 0.0023617774713784456\n",
      "Epoch 4076, Loss: 0.014735794742591679, Final Batch Loss: 0.0006049356306903064\n",
      "Epoch 4077, Loss: 0.0038430826680269092, Final Batch Loss: 0.0018061508890241385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4078, Loss: 0.005397835637268145, Final Batch Loss: 0.0004896199679933488\n",
      "Epoch 4079, Loss: 0.01705235351982992, Final Batch Loss: 0.001019410789012909\n",
      "Epoch 4080, Loss: 0.007189676980488002, Final Batch Loss: 0.0017523017013445497\n",
      "Epoch 4081, Loss: 0.009901052428176627, Final Batch Loss: 0.00047850553528405726\n",
      "Epoch 4082, Loss: 0.01308404267183505, Final Batch Loss: 0.00881662406027317\n",
      "Epoch 4083, Loss: 0.020297801645938307, Final Batch Loss: 0.0003563720965757966\n",
      "Epoch 4084, Loss: 0.014303103409474716, Final Batch Loss: 0.0001574688940308988\n",
      "Epoch 4085, Loss: 0.03375660636811517, Final Batch Loss: 0.0012660013744607568\n",
      "Epoch 4086, Loss: 0.009553988056723028, Final Batch Loss: 0.005735763814300299\n",
      "Epoch 4087, Loss: 0.04467757456586696, Final Batch Loss: 0.011357931420207024\n",
      "Epoch 4088, Loss: 0.011698330519720912, Final Batch Loss: 0.0024683335795998573\n",
      "Epoch 4089, Loss: 0.006721512618241832, Final Batch Loss: 0.0016771581722423434\n",
      "Epoch 4090, Loss: 0.0075590359047055244, Final Batch Loss: 0.0002327085821889341\n",
      "Epoch 4091, Loss: 0.01139011891791597, Final Batch Loss: 0.004636500030755997\n",
      "Epoch 4092, Loss: 0.004106719468836673, Final Batch Loss: 0.0010687308385968208\n",
      "Epoch 4093, Loss: 0.042239791073370725, Final Batch Loss: 0.03293474763631821\n",
      "Epoch 4094, Loss: 0.037390890833194135, Final Batch Loss: 3.7524918298004195e-05\n",
      "Epoch 4095, Loss: 0.015112655906705186, Final Batch Loss: 0.00014133306103758514\n",
      "Epoch 4096, Loss: 0.00672967653372325, Final Batch Loss: 0.0032418915070593357\n",
      "Epoch 4097, Loss: 0.022288619045866653, Final Batch Loss: 0.0007976480410434306\n",
      "Epoch 4098, Loss: 0.012640317203477025, Final Batch Loss: 0.0006837957771494985\n",
      "Epoch 4099, Loss: 0.004356698518677149, Final Batch Loss: 0.0024579036980867386\n",
      "Epoch 4100, Loss: 0.03646710887551308, Final Batch Loss: 0.0004546003183349967\n",
      "Epoch 4101, Loss: 0.0322082511120243, Final Batch Loss: 0.0001672368816798553\n",
      "Epoch 4102, Loss: 0.022582379635423422, Final Batch Loss: 0.01321448665112257\n",
      "Epoch 4103, Loss: 0.009200550324749202, Final Batch Loss: 0.003513112897053361\n",
      "Epoch 4104, Loss: 0.0038632046489510685, Final Batch Loss: 0.0004902662476524711\n",
      "Epoch 4105, Loss: 0.03263846621848643, Final Batch Loss: 0.012074392288923264\n",
      "Epoch 4106, Loss: 0.07975735342188273, Final Batch Loss: 0.003809202928096056\n",
      "Epoch 4107, Loss: 0.035729747149161994, Final Batch Loss: 0.0010107889538630843\n",
      "Epoch 4108, Loss: 0.029488254862371832, Final Batch Loss: 0.0013300362043082714\n",
      "Epoch 4109, Loss: 0.021990885463310406, Final Batch Loss: 0.01074629183858633\n",
      "Epoch 4110, Loss: 0.028106464946176857, Final Batch Loss: 0.0013559241779148579\n",
      "Epoch 4111, Loss: 0.019540222943760455, Final Batch Loss: 0.0003503863699734211\n",
      "Epoch 4112, Loss: 0.06575436081038788, Final Batch Loss: 0.0011978513794019818\n",
      "Epoch 4113, Loss: 0.046000497648492455, Final Batch Loss: 0.027433639392256737\n",
      "Epoch 4114, Loss: 0.02920711919432506, Final Batch Loss: 0.0023869830183684826\n",
      "Epoch 4115, Loss: 0.05268211085058283, Final Batch Loss: 0.0001307876518694684\n",
      "Epoch 4116, Loss: 0.02804470236878842, Final Batch Loss: 0.0026916186325252056\n",
      "Epoch 4117, Loss: 0.02470043837092817, Final Batch Loss: 0.009352047927677631\n",
      "Epoch 4118, Loss: 0.04480667086318135, Final Batch Loss: 0.03366214409470558\n",
      "Epoch 4119, Loss: 0.014438057056395337, Final Batch Loss: 0.0019261017441749573\n",
      "Epoch 4120, Loss: 0.012543149117846042, Final Batch Loss: 0.0001768532965797931\n",
      "Epoch 4121, Loss: 0.009512437041848898, Final Batch Loss: 0.0042528072372078896\n",
      "Epoch 4122, Loss: 0.02198945207055658, Final Batch Loss: 0.0012982598273083568\n",
      "Epoch 4123, Loss: 0.01698878430761397, Final Batch Loss: 0.003921331837773323\n",
      "Epoch 4124, Loss: 0.007745064154732972, Final Batch Loss: 0.0030271236319094896\n",
      "Epoch 4125, Loss: 0.04878233012277633, Final Batch Loss: 0.042218707501888275\n",
      "Epoch 4126, Loss: 0.019601349194999784, Final Batch Loss: 0.00236094044521451\n",
      "Epoch 4127, Loss: 0.01951217366149649, Final Batch Loss: 0.012207742780447006\n",
      "Epoch 4128, Loss: 0.005004813778214157, Final Batch Loss: 0.0013319741701707244\n",
      "Epoch 4129, Loss: 0.01240599845732504, Final Batch Loss: 1.8206808817922138e-05\n",
      "Epoch 4130, Loss: 0.015019268175819889, Final Batch Loss: 0.0007237405516207218\n",
      "Epoch 4131, Loss: 0.007377171263215132, Final Batch Loss: 7.14139750925824e-05\n",
      "Epoch 4132, Loss: 0.022240434773266315, Final Batch Loss: 0.00012603704817593098\n",
      "Epoch 4133, Loss: 0.0032632685324642807, Final Batch Loss: 0.000584242632612586\n",
      "Epoch 4134, Loss: 0.02293568608001806, Final Batch Loss: 0.00038075188058428466\n",
      "Epoch 4135, Loss: 0.004473244538530707, Final Batch Loss: 0.0004770251107402146\n",
      "Epoch 4136, Loss: 0.003545035171555355, Final Batch Loss: 0.0008830091101117432\n",
      "Epoch 4137, Loss: 0.00454368838109076, Final Batch Loss: 0.0006469848449341953\n",
      "Epoch 4138, Loss: 0.004529159399680793, Final Batch Loss: 0.0006810937775298953\n",
      "Epoch 4139, Loss: 0.02419467133586295, Final Batch Loss: 0.006195924244821072\n",
      "Epoch 4140, Loss: 0.01013450845493935, Final Batch Loss: 0.0001050578139256686\n",
      "Epoch 4141, Loss: 0.027799154573585838, Final Batch Loss: 0.00038422882789745927\n",
      "Epoch 4142, Loss: 0.012495456729084253, Final Batch Loss: 0.0006269592558965087\n",
      "Epoch 4143, Loss: 0.006356300080369692, Final Batch Loss: 0.00011201512097613886\n",
      "Epoch 4144, Loss: 0.0173125308501767, Final Batch Loss: 3.509684756863862e-05\n",
      "Epoch 4145, Loss: 0.009626065788324922, Final Batch Loss: 0.00020472839241847396\n",
      "Epoch 4146, Loss: 0.010797571259899996, Final Batch Loss: 0.0004178025119472295\n",
      "Epoch 4147, Loss: 0.010226686004898511, Final Batch Loss: 0.0029243354219943285\n",
      "Epoch 4148, Loss: 0.015693612564064097, Final Batch Loss: 0.011099187657237053\n",
      "Epoch 4149, Loss: 0.022811665694462135, Final Batch Loss: 0.00026472072931937873\n",
      "Epoch 4150, Loss: 0.0026128190220333636, Final Batch Loss: 0.00047310677473433316\n",
      "Epoch 4151, Loss: 0.0049838481936603785, Final Batch Loss: 0.00011407454439904541\n",
      "Epoch 4152, Loss: 0.003694232764246408, Final Batch Loss: 0.0015886283945292234\n",
      "Epoch 4153, Loss: 0.016657788975862786, Final Batch Loss: 0.0015188013203442097\n",
      "Epoch 4154, Loss: 0.05479685869067907, Final Batch Loss: 0.04930643364787102\n",
      "Epoch 4155, Loss: 0.005478227540152147, Final Batch Loss: 0.00012576536391861737\n",
      "Epoch 4156, Loss: 0.023371745279291645, Final Batch Loss: 0.0025025447830557823\n",
      "Epoch 4157, Loss: 0.005124368952238001, Final Batch Loss: 0.0001300734729738906\n",
      "Epoch 4158, Loss: 0.03019689415668836, Final Batch Loss: 0.003498851088806987\n",
      "Epoch 4159, Loss: 0.007618584961164743, Final Batch Loss: 0.005417380481958389\n",
      "Epoch 4160, Loss: 0.004757514499942772, Final Batch Loss: 0.0002303197979927063\n",
      "Epoch 4161, Loss: 0.00373065379972104, Final Batch Loss: 0.0008217921713367105\n",
      "Epoch 4162, Loss: 0.007181687622505706, Final Batch Loss: 0.004932180047035217\n",
      "Epoch 4163, Loss: 0.0113476037804503, Final Batch Loss: 0.002008233917877078\n",
      "Epoch 4164, Loss: 0.004920264531392604, Final Batch Loss: 7.815985009074211e-05\n",
      "Epoch 4165, Loss: 0.019632667070254683, Final Batch Loss: 0.00032746014767326415\n",
      "Epoch 4166, Loss: 0.03075217653531581, Final Batch Loss: 0.00026602111756801605\n",
      "Epoch 4167, Loss: 0.012134562406572513, Final Batch Loss: 0.0008780295611359179\n",
      "Epoch 4168, Loss: 0.004195305693428963, Final Batch Loss: 0.0009575057774782181\n",
      "Epoch 4169, Loss: 0.03161206858931109, Final Batch Loss: 0.0008935178047977388\n",
      "Epoch 4170, Loss: 0.004741149838082492, Final Batch Loss: 0.002817007480189204\n",
      "Epoch 4171, Loss: 0.009701371491246391, Final Batch Loss: 0.0003076446591876447\n",
      "Epoch 4172, Loss: 0.005154991144081578, Final Batch Loss: 0.0001545442792121321\n",
      "Epoch 4173, Loss: 0.0466285374131985, Final Batch Loss: 0.0012360579567030072\n",
      "Epoch 4174, Loss: 0.0020556185190798715, Final Batch Loss: 0.00012968717783223838\n",
      "Epoch 4175, Loss: 0.008363159198779613, Final Batch Loss: 0.0008239888702519238\n",
      "Epoch 4176, Loss: 0.006443101643526461, Final Batch Loss: 6.544388452311978e-05\n",
      "Epoch 4177, Loss: 0.014690951698867138, Final Batch Loss: 9.946047066478059e-05\n",
      "Epoch 4178, Loss: 0.005529263697098941, Final Batch Loss: 0.00010442035272717476\n",
      "Epoch 4179, Loss: 0.005250025889836252, Final Batch Loss: 0.0004355952551122755\n",
      "Epoch 4180, Loss: 0.02891016060311813, Final Batch Loss: 0.0003319648967590183\n",
      "Epoch 4181, Loss: 0.002880439002183266, Final Batch Loss: 0.00013839421444572508\n",
      "Epoch 4182, Loss: 0.014602047012886032, Final Batch Loss: 0.0010800431482493877\n",
      "Epoch 4183, Loss: 0.024217451151343994, Final Batch Loss: 0.0013746314216405153\n",
      "Epoch 4184, Loss: 0.011692931177094579, Final Batch Loss: 0.0007532061426900327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4185, Loss: 0.015567346330499277, Final Batch Loss: 0.0004283749149180949\n",
      "Epoch 4186, Loss: 0.024416846226813504, Final Batch Loss: 3.3012682251865044e-05\n",
      "Epoch 4187, Loss: 0.006249868281884119, Final Batch Loss: 0.0017324906075373292\n",
      "Epoch 4188, Loss: 0.014932933438103646, Final Batch Loss: 0.0027643167413771152\n",
      "Epoch 4189, Loss: 0.03549834002478747, Final Batch Loss: 0.00011113663640571758\n",
      "Epoch 4190, Loss: 0.015921203121251892, Final Batch Loss: 0.00010815334826475009\n",
      "Epoch 4191, Loss: 0.012863560434198007, Final Batch Loss: 5.788300768472254e-05\n",
      "Epoch 4192, Loss: 0.006052230659406632, Final Batch Loss: 0.0008844718104228377\n",
      "Epoch 4193, Loss: 0.019738973471248755, Final Batch Loss: 5.458330633700825e-05\n",
      "Epoch 4194, Loss: 0.015635026415111497, Final Batch Loss: 0.0003618675109464675\n",
      "Epoch 4195, Loss: 0.023316264239838347, Final Batch Loss: 0.0005637863068841398\n",
      "Epoch 4196, Loss: 0.005717140418710187, Final Batch Loss: 0.0041034044697880745\n",
      "Epoch 4197, Loss: 0.02210496208863333, Final Batch Loss: 0.0006737277144566178\n",
      "Epoch 4198, Loss: 0.003388815835933201, Final Batch Loss: 0.00023025674454402179\n",
      "Epoch 4199, Loss: 0.021798691916046664, Final Batch Loss: 0.014584257267415524\n",
      "Epoch 4200, Loss: 0.005345221521565691, Final Batch Loss: 0.003350188722833991\n",
      "Epoch 4201, Loss: 0.0034149481944041327, Final Batch Loss: 0.00020192489319015294\n",
      "Epoch 4202, Loss: 0.013082051737001166, Final Batch Loss: 0.007944348268210888\n",
      "Epoch 4203, Loss: 0.009330926230177283, Final Batch Loss: 0.0037038992159068584\n",
      "Epoch 4204, Loss: 0.03792959301790688, Final Batch Loss: 0.0020105228759348392\n",
      "Epoch 4205, Loss: 0.015405916259624064, Final Batch Loss: 0.0008111427305266261\n",
      "Epoch 4206, Loss: 0.02137260910240002, Final Batch Loss: 0.012923488393425941\n",
      "Epoch 4207, Loss: 0.007701650523813441, Final Batch Loss: 0.00154833123087883\n",
      "Epoch 4208, Loss: 0.010151686044991948, Final Batch Loss: 0.00020461071108002216\n",
      "Epoch 4209, Loss: 0.018326664445339702, Final Batch Loss: 8.172175148501992e-05\n",
      "Epoch 4210, Loss: 0.008164522863808088, Final Batch Loss: 0.00038285707705654204\n",
      "Epoch 4211, Loss: 0.008893981474102475, Final Batch Loss: 0.00026486263959668577\n",
      "Epoch 4212, Loss: 0.005915993853705004, Final Batch Loss: 0.004095920827239752\n",
      "Epoch 4213, Loss: 0.014446374610997736, Final Batch Loss: 0.0006056175916455686\n",
      "Epoch 4214, Loss: 0.05157232191413641, Final Batch Loss: 0.0010146477725356817\n",
      "Epoch 4215, Loss: 0.030745608470169827, Final Batch Loss: 0.003428447525948286\n",
      "Epoch 4216, Loss: 0.01864233158994466, Final Batch Loss: 0.0008411887683905661\n",
      "Epoch 4217, Loss: 0.005229400208918378, Final Batch Loss: 0.0005485588335432112\n",
      "Epoch 4218, Loss: 0.0093925372348167, Final Batch Loss: 0.0021345973946154118\n",
      "Epoch 4219, Loss: 0.0244305928790709, Final Batch Loss: 0.008416801691055298\n",
      "Epoch 4220, Loss: 0.047165603027679026, Final Batch Loss: 0.033767059445381165\n",
      "Epoch 4221, Loss: 0.033736036566551775, Final Batch Loss: 0.000854889047332108\n",
      "Epoch 4222, Loss: 0.04650796660280321, Final Batch Loss: 0.00012765366409439594\n",
      "Epoch 4223, Loss: 0.003875590075040236, Final Batch Loss: 8.260499453172088e-05\n",
      "Epoch 4224, Loss: 0.026512496086070314, Final Batch Loss: 0.00018645732779987156\n",
      "Epoch 4225, Loss: 0.041409621844650246, Final Batch Loss: 0.02793028950691223\n",
      "Epoch 4226, Loss: 0.007013756243395619, Final Batch Loss: 0.0025646414142102003\n",
      "Epoch 4227, Loss: 0.015228832984576002, Final Batch Loss: 0.0037612684536725283\n",
      "Epoch 4228, Loss: 0.07037363562267274, Final Batch Loss: 0.000808355282060802\n",
      "Epoch 4229, Loss: 0.007258523401105776, Final Batch Loss: 0.0003326423466205597\n",
      "Epoch 4230, Loss: 0.03975088067818433, Final Batch Loss: 0.01440486405044794\n",
      "Epoch 4231, Loss: 0.010040405264589936, Final Batch Loss: 0.0005678703309968114\n",
      "Epoch 4232, Loss: 0.02941136614390416, Final Batch Loss: 0.000662842532619834\n",
      "Epoch 4233, Loss: 0.023691961221629754, Final Batch Loss: 0.011848320253193378\n",
      "Epoch 4234, Loss: 0.030365715487278067, Final Batch Loss: 5.1485185394994915e-05\n",
      "Epoch 4235, Loss: 0.034298700105864555, Final Batch Loss: 0.0004947333363816142\n",
      "Epoch 4236, Loss: 0.019399261625949293, Final Batch Loss: 0.005812148563563824\n",
      "Epoch 4237, Loss: 0.004577400890411809, Final Batch Loss: 6.361532723531127e-05\n",
      "Epoch 4238, Loss: 0.02161205890297424, Final Batch Loss: 0.0011236112331971526\n",
      "Epoch 4239, Loss: 0.02372424496570602, Final Batch Loss: 0.0018043910386040807\n",
      "Epoch 4240, Loss: 0.06365502648986876, Final Batch Loss: 0.01372202392667532\n",
      "Epoch 4241, Loss: 0.03143846121383831, Final Batch Loss: 0.0005483765853568912\n",
      "Epoch 4242, Loss: 0.013880696771593648, Final Batch Loss: 0.00043757795356214046\n",
      "Epoch 4243, Loss: 0.027113841264508665, Final Batch Loss: 0.006719017866998911\n",
      "Epoch 4244, Loss: 0.01216557071893476, Final Batch Loss: 0.0008423790568485856\n",
      "Epoch 4245, Loss: 0.03671248132013716, Final Batch Loss: 0.0028346891049295664\n",
      "Epoch 4246, Loss: 0.007837581069907174, Final Batch Loss: 0.0035560978576540947\n",
      "Epoch 4247, Loss: 0.026250881317537278, Final Batch Loss: 0.003707204246893525\n",
      "Epoch 4248, Loss: 0.02899558877106756, Final Batch Loss: 0.0017849525902420282\n",
      "Epoch 4249, Loss: 0.0044230195780983195, Final Batch Loss: 0.00020039411901962012\n",
      "Epoch 4250, Loss: 0.0055791593476897106, Final Batch Loss: 0.001408762182109058\n",
      "Epoch 4251, Loss: 0.005520704056834802, Final Batch Loss: 0.0006735914503224194\n",
      "Epoch 4252, Loss: 0.006884126632940024, Final Batch Loss: 0.00248158467002213\n",
      "Epoch 4253, Loss: 0.006334618461551145, Final Batch Loss: 0.0002787864359561354\n",
      "Epoch 4254, Loss: 0.01297925210383255, Final Batch Loss: 0.003215634496882558\n",
      "Epoch 4255, Loss: 0.00428300813655369, Final Batch Loss: 0.0014396068872883916\n",
      "Epoch 4256, Loss: 0.0035908028585254215, Final Batch Loss: 0.0001198723548441194\n",
      "Epoch 4257, Loss: 0.0381304607471975, Final Batch Loss: 0.03570808842778206\n",
      "Epoch 4258, Loss: 0.004836241059820168, Final Batch Loss: 0.0016056044260039926\n",
      "Epoch 4259, Loss: 0.01259811817726586, Final Batch Loss: 0.00031711035990156233\n",
      "Epoch 4260, Loss: 0.03529045589675661, Final Batch Loss: 0.00017815640603657812\n",
      "Epoch 4261, Loss: 0.02316582306229975, Final Batch Loss: 0.0012461651349440217\n",
      "Epoch 4262, Loss: 0.013526891212677583, Final Batch Loss: 0.008477512747049332\n",
      "Epoch 4263, Loss: 0.028030525718349963, Final Batch Loss: 0.00012668463750742376\n",
      "Epoch 4264, Loss: 0.01736609637737274, Final Batch Loss: 0.0002757680194918066\n",
      "Epoch 4265, Loss: 0.00770464472589083, Final Batch Loss: 0.0012360098771750927\n",
      "Epoch 4266, Loss: 0.0035859772469848394, Final Batch Loss: 0.0007393831037916243\n",
      "Epoch 4267, Loss: 0.04021019642823376, Final Batch Loss: 0.000907275069039315\n",
      "Epoch 4268, Loss: 0.005543240418774076, Final Batch Loss: 0.0030819911044090986\n",
      "Epoch 4269, Loss: 0.05058517138240859, Final Batch Loss: 0.0063591632060706615\n",
      "Epoch 4270, Loss: 0.02009997064305935, Final Batch Loss: 0.008043691515922546\n",
      "Epoch 4271, Loss: 0.005409855773905292, Final Batch Loss: 0.003689607372507453\n",
      "Epoch 4272, Loss: 0.020133916405029595, Final Batch Loss: 0.0008925717556849122\n",
      "Epoch 4273, Loss: 0.04349202150478959, Final Batch Loss: 0.02412458509206772\n",
      "Epoch 4274, Loss: 0.03552318656875286, Final Batch Loss: 0.0009046669583767653\n",
      "Epoch 4275, Loss: 0.015715995454229414, Final Batch Loss: 0.012982605025172234\n",
      "Epoch 4276, Loss: 0.0037004894693382084, Final Batch Loss: 0.0007492078584618866\n",
      "Epoch 4277, Loss: 0.019054193486226723, Final Batch Loss: 0.00019783785683102906\n",
      "Epoch 4278, Loss: 0.03637236526992638, Final Batch Loss: 0.003901713527739048\n",
      "Epoch 4279, Loss: 0.0071677840169286355, Final Batch Loss: 0.0004906756803393364\n",
      "Epoch 4280, Loss: 0.02939492584846448, Final Batch Loss: 0.001703879446722567\n",
      "Epoch 4281, Loss: 0.009658089227741584, Final Batch Loss: 0.0017350225243717432\n",
      "Epoch 4282, Loss: 0.01765616802731529, Final Batch Loss: 0.001698588370345533\n",
      "Epoch 4283, Loss: 0.006239646987523884, Final Batch Loss: 0.0002191718085668981\n",
      "Epoch 4284, Loss: 0.006241006689378992, Final Batch Loss: 0.0017613709205761552\n",
      "Epoch 4285, Loss: 0.02184210778796114, Final Batch Loss: 0.018269667401909828\n",
      "Epoch 4286, Loss: 0.006801394891226664, Final Batch Loss: 0.000703207915648818\n",
      "Epoch 4287, Loss: 0.05450505005137529, Final Batch Loss: 0.0001570103777339682\n",
      "Epoch 4288, Loss: 0.005785194902273361, Final Batch Loss: 0.000660119520034641\n",
      "Epoch 4289, Loss: 0.007117176282918081, Final Batch Loss: 0.0003303368575870991\n",
      "Epoch 4290, Loss: 0.007303254154976457, Final Batch Loss: 0.0008269309182651341\n",
      "Epoch 4291, Loss: 0.019246371390181594, Final Batch Loss: 0.0005831925664097071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4292, Loss: 0.012731548500596546, Final Batch Loss: 0.009258488193154335\n",
      "Epoch 4293, Loss: 0.016233106216532178, Final Batch Loss: 0.000451597006758675\n",
      "Epoch 4294, Loss: 0.003438060830376344, Final Batch Loss: 1.1454150808276609e-05\n",
      "Epoch 4295, Loss: 0.00289901731594, Final Batch Loss: 0.00022842026373837143\n",
      "Epoch 4296, Loss: 0.021712048233894166, Final Batch Loss: 0.0008968593901954591\n",
      "Epoch 4297, Loss: 0.01909404657635605, Final Batch Loss: 0.0007221041014418006\n",
      "Epoch 4298, Loss: 0.005743551620980725, Final Batch Loss: 0.0007533307070843875\n",
      "Epoch 4299, Loss: 0.006258799527131487, Final Batch Loss: 0.002382942708209157\n",
      "Epoch 4300, Loss: 0.0037171664662309922, Final Batch Loss: 6.275648047449067e-05\n",
      "Epoch 4301, Loss: 0.004314547935791779, Final Batch Loss: 0.0007068297127261758\n",
      "Epoch 4302, Loss: 0.00795421059592627, Final Batch Loss: 0.001966778887435794\n",
      "Epoch 4303, Loss: 0.003444076981395483, Final Batch Loss: 0.0009580040932632983\n",
      "Epoch 4304, Loss: 0.002261520152387675, Final Batch Loss: 0.0001651883649174124\n",
      "Epoch 4305, Loss: 0.002318174476386048, Final Batch Loss: 0.0005174372927285731\n",
      "Epoch 4306, Loss: 0.021981879843224306, Final Batch Loss: 6.444170867325738e-05\n",
      "Epoch 4307, Loss: 0.0149375367545872, Final Batch Loss: 9.058960858965293e-05\n",
      "Epoch 4308, Loss: 0.004159464500844479, Final Batch Loss: 0.0009479231666773558\n",
      "Epoch 4309, Loss: 0.006731121058692224, Final Batch Loss: 5.026978033129126e-05\n",
      "Epoch 4310, Loss: 0.00491683494328754, Final Batch Loss: 0.0019717172253876925\n",
      "Epoch 4311, Loss: 0.01435973253683187, Final Batch Loss: 0.0004891808493994176\n",
      "Epoch 4312, Loss: 0.004667240778871928, Final Batch Loss: 0.0010550774168223143\n",
      "Epoch 4313, Loss: 0.003454092759056948, Final Batch Loss: 0.0001591252366779372\n",
      "Epoch 4314, Loss: 0.01748520284309052, Final Batch Loss: 0.0026656300760805607\n",
      "Epoch 4315, Loss: 0.0030333088216139004, Final Batch Loss: 0.0009268573485314846\n",
      "Epoch 4316, Loss: 0.004997510113753378, Final Batch Loss: 0.0007475470774807036\n",
      "Epoch 4317, Loss: 0.0026461518718861043, Final Batch Loss: 0.00011929950414923951\n",
      "Epoch 4318, Loss: 0.003863564408675302, Final Batch Loss: 0.0006307832663878798\n",
      "Epoch 4319, Loss: 0.02501761311577866, Final Batch Loss: 0.0021500266157090664\n",
      "Epoch 4320, Loss: 0.02488944845390506, Final Batch Loss: 0.012315305881202221\n",
      "Epoch 4321, Loss: 0.006972861505346373, Final Batch Loss: 0.00031914602732285857\n",
      "Epoch 4322, Loss: 0.0052364852163009346, Final Batch Loss: 0.0002772878797259182\n",
      "Epoch 4323, Loss: 0.023323283428908326, Final Batch Loss: 0.004325682297348976\n",
      "Epoch 4324, Loss: 0.03348650655243546, Final Batch Loss: 0.0006984536303207278\n",
      "Epoch 4325, Loss: 0.06841114116832614, Final Batch Loss: 0.0007258703117258847\n",
      "Epoch 4326, Loss: 0.06403639609925449, Final Batch Loss: 0.014006366021931171\n",
      "Epoch 4327, Loss: 0.03789801197126508, Final Batch Loss: 0.001350305275991559\n",
      "Epoch 4328, Loss: 0.05833458877168596, Final Batch Loss: 0.001261178869754076\n",
      "Epoch 4329, Loss: 0.009607452346244827, Final Batch Loss: 0.0008377421763725579\n",
      "Epoch 4330, Loss: 0.02468580036656931, Final Batch Loss: 0.0009203442023135722\n",
      "Epoch 4331, Loss: 0.01578084627726639, Final Batch Loss: 0.0016859888564795256\n",
      "Epoch 4332, Loss: 0.017604550695978105, Final Batch Loss: 0.002831848803907633\n",
      "Epoch 4333, Loss: 0.0028684366843663156, Final Batch Loss: 0.0006257810746319592\n",
      "Epoch 4334, Loss: 0.011715664353687316, Final Batch Loss: 0.002358905505388975\n",
      "Epoch 4335, Loss: 0.00352886832843069, Final Batch Loss: 0.0001358569716103375\n",
      "Epoch 4336, Loss: 0.0032550648611504585, Final Batch Loss: 0.0003126106457784772\n",
      "Epoch 4337, Loss: 0.027160202036611736, Final Batch Loss: 0.0037838940043002367\n",
      "Epoch 4338, Loss: 0.027346673246938735, Final Batch Loss: 0.003098653396591544\n",
      "Epoch 4339, Loss: 0.01708484966366086, Final Batch Loss: 6.53580209473148e-05\n",
      "Epoch 4340, Loss: 0.03317949337360915, Final Batch Loss: 0.00021957400895189494\n",
      "Epoch 4341, Loss: 0.014829309366177768, Final Batch Loss: 0.0007037564646452665\n",
      "Epoch 4342, Loss: 0.02120061135792639, Final Batch Loss: 0.0009280666126869619\n",
      "Epoch 4343, Loss: 0.004209541148156859, Final Batch Loss: 0.0005621368181891739\n",
      "Epoch 4344, Loss: 0.011189188611751888, Final Batch Loss: 0.009063350036740303\n",
      "Epoch 4345, Loss: 0.016414948811871, Final Batch Loss: 3.986399678979069e-05\n",
      "Epoch 4346, Loss: 0.033955537553993054, Final Batch Loss: 0.0008379148202948272\n",
      "Epoch 4347, Loss: 0.03646824331372045, Final Batch Loss: 0.00024211299023590982\n",
      "Epoch 4348, Loss: 0.017867134185507894, Final Batch Loss: 0.0007864562212489545\n",
      "Epoch 4349, Loss: 0.0187456211715471, Final Batch Loss: 0.0003962521150242537\n",
      "Epoch 4350, Loss: 0.024246636516181752, Final Batch Loss: 0.00019644523854367435\n",
      "Epoch 4351, Loss: 0.023424301121849567, Final Batch Loss: 0.000633921183180064\n",
      "Epoch 4352, Loss: 0.02035829972010106, Final Batch Loss: 0.0008019849192351103\n",
      "Epoch 4353, Loss: 0.005458733532577753, Final Batch Loss: 0.000781131093390286\n",
      "Epoch 4354, Loss: 0.007212219075881876, Final Batch Loss: 0.00012834773224312812\n",
      "Epoch 4355, Loss: 0.005873378526302986, Final Batch Loss: 0.0005051381886005402\n",
      "Epoch 4356, Loss: 0.019777619454544038, Final Batch Loss: 0.015385000966489315\n",
      "Epoch 4357, Loss: 0.02654240099946037, Final Batch Loss: 0.001328174606896937\n",
      "Epoch 4358, Loss: 0.05230346362804994, Final Batch Loss: 0.0009010554640553892\n",
      "Epoch 4359, Loss: 0.007846654043532908, Final Batch Loss: 0.0028139539062976837\n",
      "Epoch 4360, Loss: 0.009962873125914484, Final Batch Loss: 0.0005064947763457894\n",
      "Epoch 4361, Loss: 0.018154363562643994, Final Batch Loss: 7.244353037094697e-05\n",
      "Epoch 4362, Loss: 0.01842585231861449, Final Batch Loss: 5.320796844898723e-05\n",
      "Epoch 4363, Loss: 0.03220264776609838, Final Batch Loss: 0.023093657568097115\n",
      "Epoch 4364, Loss: 0.031943291309289634, Final Batch Loss: 0.0006934086559340358\n",
      "Epoch 4365, Loss: 0.020499101607128978, Final Batch Loss: 0.0009310559835284948\n",
      "Epoch 4366, Loss: 0.01740340137621388, Final Batch Loss: 0.0003043023752979934\n",
      "Epoch 4367, Loss: 0.014568434598913882, Final Batch Loss: 3.809735790127888e-05\n",
      "Epoch 4368, Loss: 0.012557682523038238, Final Batch Loss: 0.0006731071625836194\n",
      "Epoch 4369, Loss: 0.008379478415008634, Final Batch Loss: 0.0026412790175527334\n",
      "Epoch 4370, Loss: 0.02915889729047194, Final Batch Loss: 0.00103942456189543\n",
      "Epoch 4371, Loss: 0.008753097135922872, Final Batch Loss: 0.000544148322660476\n",
      "Epoch 4372, Loss: 0.048123020998900756, Final Batch Loss: 0.00036912268842570484\n",
      "Epoch 4373, Loss: 0.029038008469797205, Final Batch Loss: 0.00012060224980814382\n",
      "Epoch 4374, Loss: 0.06593665614491329, Final Batch Loss: 0.030322765931487083\n",
      "Epoch 4375, Loss: 0.01651800115359947, Final Batch Loss: 0.0021343491971492767\n",
      "Epoch 4376, Loss: 0.00901662657270208, Final Batch Loss: 0.0006389921181835234\n",
      "Epoch 4377, Loss: 0.05737182236043736, Final Batch Loss: 0.0004102892125956714\n",
      "Epoch 4378, Loss: 0.005276134703308344, Final Batch Loss: 0.0005271408590488136\n",
      "Epoch 4379, Loss: 0.007950606406666338, Final Batch Loss: 0.0005447459407150745\n",
      "Epoch 4380, Loss: 0.018167046073358506, Final Batch Loss: 0.0017908746376633644\n",
      "Epoch 4381, Loss: 0.012684169632848352, Final Batch Loss: 0.0005442312103696167\n",
      "Epoch 4382, Loss: 0.004492971347644925, Final Batch Loss: 0.0005788553971797228\n",
      "Epoch 4383, Loss: 0.013208218151703477, Final Batch Loss: 0.0023038615472614765\n",
      "Epoch 4384, Loss: 0.008699617290403694, Final Batch Loss: 0.0016047504032030702\n",
      "Epoch 4385, Loss: 0.014214834664016962, Final Batch Loss: 0.0009935868438333273\n",
      "Epoch 4386, Loss: 0.01712073061207775, Final Batch Loss: 0.00047118894872255623\n",
      "Epoch 4387, Loss: 0.06998644594568759, Final Batch Loss: 0.008036860264837742\n",
      "Epoch 4388, Loss: 0.04760617910505971, Final Batch Loss: 0.006813548970967531\n",
      "Epoch 4389, Loss: 0.012016993714496493, Final Batch Loss: 9.526940993964672e-05\n",
      "Epoch 4390, Loss: 0.022501869359984994, Final Batch Loss: 0.001449421513825655\n",
      "Epoch 4391, Loss: 0.01037279426236637, Final Batch Loss: 0.00017371957073919475\n",
      "Epoch 4392, Loss: 0.020820698322495446, Final Batch Loss: 0.0031494551803916693\n",
      "Epoch 4393, Loss: 0.028373832639772445, Final Batch Loss: 0.0017018317012116313\n",
      "Epoch 4394, Loss: 0.02247492829337716, Final Batch Loss: 0.004277552478015423\n",
      "Epoch 4395, Loss: 0.019903509004507214, Final Batch Loss: 0.010832708328962326\n",
      "Epoch 4396, Loss: 0.007897943396528717, Final Batch Loss: 0.0040804194286465645\n",
      "Epoch 4397, Loss: 0.04541193967452273, Final Batch Loss: 0.0015899845166131854\n",
      "Epoch 4398, Loss: 0.008620943495770916, Final Batch Loss: 0.0035192747600376606\n",
      "Epoch 4399, Loss: 0.0061790104664396495, Final Batch Loss: 0.0010566076962277293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4400, Loss: 0.026776756800245494, Final Batch Loss: 0.0013866148656234145\n",
      "Epoch 4401, Loss: 0.008543630785425194, Final Batch Loss: 0.002292939927428961\n",
      "Epoch 4402, Loss: 0.010736579773947597, Final Batch Loss: 0.0010541428346186876\n",
      "Epoch 4403, Loss: 0.008837501336529385, Final Batch Loss: 2.5530571292620152e-05\n",
      "Epoch 4404, Loss: 0.005839421137352474, Final Batch Loss: 0.00018889758212026209\n",
      "Epoch 4405, Loss: 0.011166883981786668, Final Batch Loss: 0.00020843581296503544\n",
      "Epoch 4406, Loss: 0.0039501445717178285, Final Batch Loss: 0.0008872887119650841\n",
      "Epoch 4407, Loss: 0.033913514867890626, Final Batch Loss: 0.0009501839522272348\n",
      "Epoch 4408, Loss: 0.00602824566885829, Final Batch Loss: 0.00156170385889709\n",
      "Epoch 4409, Loss: 0.005918862851103768, Final Batch Loss: 0.0009060277952812612\n",
      "Epoch 4410, Loss: 0.006612511278945021, Final Batch Loss: 0.0022422983311116695\n",
      "Epoch 4411, Loss: 0.008402860985370353, Final Batch Loss: 0.0004399084427859634\n",
      "Epoch 4412, Loss: 0.020423785492312163, Final Batch Loss: 0.0014630650402978063\n",
      "Epoch 4413, Loss: 0.016537089497433044, Final Batch Loss: 0.00043307311716489494\n",
      "Epoch 4414, Loss: 0.003773279720917344, Final Batch Loss: 0.0020226319320499897\n",
      "Epoch 4415, Loss: 0.030518115439917892, Final Batch Loss: 0.00026340706972405314\n",
      "Epoch 4416, Loss: 0.005517184006748721, Final Batch Loss: 0.0035599255934357643\n",
      "Epoch 4417, Loss: 0.00542706142005045, Final Batch Loss: 0.00013853867130819708\n",
      "Epoch 4418, Loss: 0.038899797858903185, Final Batch Loss: 0.029335686936974525\n",
      "Epoch 4419, Loss: 0.004106089909328148, Final Batch Loss: 0.0012259026989340782\n",
      "Epoch 4420, Loss: 0.008702778795850463, Final Batch Loss: 0.0012866120086982846\n",
      "Epoch 4421, Loss: 0.004237211018335074, Final Batch Loss: 0.001320201437920332\n",
      "Epoch 4422, Loss: 0.013442480107187293, Final Batch Loss: 0.00020613883680198342\n",
      "Epoch 4423, Loss: 0.009936141781508923, Final Batch Loss: 0.005001231096684933\n",
      "Epoch 4424, Loss: 0.006960744154639542, Final Batch Loss: 0.001343860407359898\n",
      "Epoch 4425, Loss: 0.005281496421957854, Final Batch Loss: 6.607074465136975e-05\n",
      "Epoch 4426, Loss: 0.014706386369653046, Final Batch Loss: 0.0012314453488215804\n",
      "Epoch 4427, Loss: 0.03565106566748, Final Batch Loss: 0.0007672404171898961\n",
      "Epoch 4428, Loss: 0.002303186283825198, Final Batch Loss: 5.641325333272107e-05\n",
      "Epoch 4429, Loss: 0.03656161710387096, Final Batch Loss: 0.025590917095541954\n",
      "Epoch 4430, Loss: 0.04710936898482032, Final Batch Loss: 0.02714604325592518\n",
      "Epoch 4431, Loss: 0.0030132679094094783, Final Batch Loss: 0.00034206276177428663\n",
      "Epoch 4432, Loss: 0.004237671248120023, Final Batch Loss: 5.993414015392773e-05\n",
      "Epoch 4433, Loss: 0.00557466282043606, Final Batch Loss: 0.0007492845179513097\n",
      "Epoch 4434, Loss: 0.005897774623008445, Final Batch Loss: 0.0021138209849596024\n",
      "Epoch 4435, Loss: 0.022934146642000997, Final Batch Loss: 0.0013369014486670494\n",
      "Epoch 4436, Loss: 0.0036569938238244504, Final Batch Loss: 4.008809628430754e-05\n",
      "Epoch 4437, Loss: 0.012522367760539055, Final Batch Loss: 0.006325500085949898\n",
      "Epoch 4438, Loss: 0.0038139312528073788, Final Batch Loss: 5.754292942583561e-05\n",
      "Epoch 4439, Loss: 0.0016731067807995714, Final Batch Loss: 0.0005104031297378242\n",
      "Epoch 4440, Loss: 0.011564503874978982, Final Batch Loss: 0.004593576304614544\n",
      "Epoch 4441, Loss: 0.00434443425911013, Final Batch Loss: 0.0001662469730945304\n",
      "Epoch 4442, Loss: 0.014613209859817289, Final Batch Loss: 0.0001896453759400174\n",
      "Epoch 4443, Loss: 0.008283072122139856, Final Batch Loss: 0.0005969293415546417\n",
      "Epoch 4444, Loss: 0.005253707495285198, Final Batch Loss: 0.00047148126759566367\n",
      "Epoch 4445, Loss: 0.017762276926077902, Final Batch Loss: 0.0014488217420876026\n",
      "Epoch 4446, Loss: 0.011319686091155745, Final Batch Loss: 0.0013464143266901374\n",
      "Epoch 4447, Loss: 0.00809574763479759, Final Batch Loss: 0.00014256741269491613\n",
      "Epoch 4448, Loss: 0.010161419762880541, Final Batch Loss: 0.0033481072168797255\n",
      "Epoch 4449, Loss: 0.046536355701391585, Final Batch Loss: 0.000191343788173981\n",
      "Epoch 4450, Loss: 0.0042190606909571216, Final Batch Loss: 0.000510271405801177\n",
      "Epoch 4451, Loss: 0.007869730994571, Final Batch Loss: 0.004863062407821417\n",
      "Epoch 4452, Loss: 0.01195063792692963, Final Batch Loss: 0.0003433705714996904\n",
      "Epoch 4453, Loss: 0.010324440023396164, Final Batch Loss: 0.0004118529031984508\n",
      "Epoch 4454, Loss: 0.013109377934597433, Final Batch Loss: 0.011201875284314156\n",
      "Epoch 4455, Loss: 0.003664963092887774, Final Batch Loss: 0.0012724361149594188\n",
      "Epoch 4456, Loss: 0.008892827274394222, Final Batch Loss: 0.00045078006223775446\n",
      "Epoch 4457, Loss: 0.020800498401513323, Final Batch Loss: 0.0007444998482242227\n",
      "Epoch 4458, Loss: 0.026202422130154446, Final Batch Loss: 0.019343825057148933\n",
      "Epoch 4459, Loss: 0.005267831031233072, Final Batch Loss: 0.00017826168914325535\n",
      "Epoch 4460, Loss: 0.03893635654821992, Final Batch Loss: 0.009299668483436108\n",
      "Epoch 4461, Loss: 0.04630716724204831, Final Batch Loss: 0.034811846911907196\n",
      "Epoch 4462, Loss: 0.07017819985048845, Final Batch Loss: 0.0005270583205856383\n",
      "Epoch 4463, Loss: 0.048178110795561224, Final Batch Loss: 0.005610299296677113\n",
      "Epoch 4464, Loss: 0.024778430757578462, Final Batch Loss: 0.0026503312401473522\n",
      "Epoch 4465, Loss: 0.009626333790947683, Final Batch Loss: 0.00020507394219748676\n",
      "Epoch 4466, Loss: 0.0691360654309392, Final Batch Loss: 0.0014313891297206283\n",
      "Epoch 4467, Loss: 0.006817701418185607, Final Batch Loss: 0.0016466813394799829\n",
      "Epoch 4468, Loss: 0.011549832386663184, Final Batch Loss: 0.0026461035013198853\n",
      "Epoch 4469, Loss: 0.015665192826418206, Final Batch Loss: 0.00020405979012139142\n",
      "Epoch 4470, Loss: 0.14391760784201324, Final Batch Loss: 0.0006011586519889534\n",
      "Epoch 4471, Loss: 0.011764385417336598, Final Batch Loss: 0.0002638727892190218\n",
      "Epoch 4472, Loss: 0.005967919540125877, Final Batch Loss: 0.00021044668392278254\n",
      "Epoch 4473, Loss: 0.0034619295329321176, Final Batch Loss: 0.0002455662179272622\n",
      "Epoch 4474, Loss: 0.009884305647574365, Final Batch Loss: 0.00014937290688976645\n",
      "Epoch 4475, Loss: 0.028348108869977295, Final Batch Loss: 0.0007156281499192119\n",
      "Epoch 4476, Loss: 0.0169748313492164, Final Batch Loss: 0.0015268824063241482\n",
      "Epoch 4477, Loss: 0.003781691106269136, Final Batch Loss: 0.00032005688990466297\n",
      "Epoch 4478, Loss: 0.007282865350134671, Final Batch Loss: 0.00023041115491650999\n",
      "Epoch 4479, Loss: 0.006973680719966069, Final Batch Loss: 0.00029700403683818877\n",
      "Epoch 4480, Loss: 0.01658924508956261, Final Batch Loss: 0.0006257607601583004\n",
      "Epoch 4481, Loss: 0.002796833257889375, Final Batch Loss: 0.00011189832002855837\n",
      "Epoch 4482, Loss: 0.0039461569976992905, Final Batch Loss: 0.0008323946967720985\n",
      "Epoch 4483, Loss: 0.014870521234115586, Final Batch Loss: 0.00021636727615259588\n",
      "Epoch 4484, Loss: 0.05957575670618098, Final Batch Loss: 3.470899537205696e-05\n",
      "Epoch 4485, Loss: 0.05014382101944648, Final Batch Loss: 0.002360031008720398\n",
      "Epoch 4486, Loss: 0.01318638572411146, Final Batch Loss: 0.0008029952878132463\n",
      "Epoch 4487, Loss: 0.003969752986449748, Final Batch Loss: 0.0002681827754713595\n",
      "Epoch 4488, Loss: 0.008925614551117178, Final Batch Loss: 0.0009436254040338099\n",
      "Epoch 4489, Loss: 0.03384670169907622, Final Batch Loss: 0.02623744122684002\n",
      "Epoch 4490, Loss: 0.005335277484846301, Final Batch Loss: 0.00018580381583888084\n",
      "Epoch 4491, Loss: 0.008133911709592212, Final Batch Loss: 9.15308264666237e-05\n",
      "Epoch 4492, Loss: 0.02056076133158058, Final Batch Loss: 0.019266098737716675\n",
      "Epoch 4493, Loss: 0.01247772887290921, Final Batch Loss: 0.00012640094792004675\n",
      "Epoch 4494, Loss: 0.07122931111371145, Final Batch Loss: 0.031623564660549164\n",
      "Epoch 4495, Loss: 0.00301305518951267, Final Batch Loss: 0.00044204643927514553\n",
      "Epoch 4496, Loss: 0.024927521299105138, Final Batch Loss: 0.0004198258393444121\n",
      "Epoch 4497, Loss: 0.05173684470355511, Final Batch Loss: 0.001967740710824728\n",
      "Epoch 4498, Loss: 0.025165270315483212, Final Batch Loss: 0.007197291124612093\n",
      "Epoch 4499, Loss: 0.030158075969666243, Final Batch Loss: 0.0014702072367072105\n",
      "Epoch 4500, Loss: 0.037234188755974174, Final Batch Loss: 0.01525136549025774\n",
      "Epoch 4501, Loss: 0.009323563106590882, Final Batch Loss: 0.00031649976153858006\n",
      "Epoch 4502, Loss: 0.011676858470309526, Final Batch Loss: 0.0010022222995758057\n",
      "Epoch 4503, Loss: 0.036534961152938195, Final Batch Loss: 0.015060055069625378\n",
      "Epoch 4504, Loss: 0.01655823935288936, Final Batch Loss: 0.002452002139762044\n",
      "Epoch 4505, Loss: 0.018308659142348915, Final Batch Loss: 0.012789301574230194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4506, Loss: 0.01705340837361291, Final Batch Loss: 0.00029334594728425145\n",
      "Epoch 4507, Loss: 0.010161804035305977, Final Batch Loss: 0.0008599112625233829\n",
      "Epoch 4508, Loss: 0.01914882767596282, Final Batch Loss: 0.0013593633193522692\n",
      "Epoch 4509, Loss: 0.018437484133755788, Final Batch Loss: 0.00036810003803111613\n",
      "Epoch 4510, Loss: 0.013628861648612656, Final Batch Loss: 0.0002113114605890587\n",
      "Epoch 4511, Loss: 0.0039678515458945185, Final Batch Loss: 0.000998537871055305\n",
      "Epoch 4512, Loss: 0.007648509461432695, Final Batch Loss: 0.0005147759220562875\n",
      "Epoch 4513, Loss: 0.006182747834827751, Final Batch Loss: 0.002015078440308571\n",
      "Epoch 4514, Loss: 0.017528568700072356, Final Batch Loss: 0.008181380107998848\n",
      "Epoch 4515, Loss: 0.01723967178259045, Final Batch Loss: 0.001184925902634859\n",
      "Epoch 4516, Loss: 0.014200350939063355, Final Batch Loss: 0.0004118393117096275\n",
      "Epoch 4517, Loss: 0.0730795490089804, Final Batch Loss: 0.044516921043395996\n",
      "Epoch 4518, Loss: 0.016289773571770638, Final Batch Loss: 0.0011540277628228068\n",
      "Epoch 4519, Loss: 0.007537776720710099, Final Batch Loss: 0.0007172939949668944\n",
      "Epoch 4520, Loss: 0.017783690098440275, Final Batch Loss: 0.0003760069084819406\n",
      "Epoch 4521, Loss: 0.005911934946198016, Final Batch Loss: 0.0003087208024226129\n",
      "Epoch 4522, Loss: 0.0078079026425257325, Final Batch Loss: 0.0004488794947974384\n",
      "Epoch 4523, Loss: 0.017979470925638452, Final Batch Loss: 0.0002972045913338661\n",
      "Epoch 4524, Loss: 0.007494115241570398, Final Batch Loss: 0.0016857318114489317\n",
      "Epoch 4525, Loss: 0.0351686846697703, Final Batch Loss: 0.00012872304068878293\n",
      "Epoch 4526, Loss: 0.016788377659395337, Final Batch Loss: 0.014093504287302494\n",
      "Epoch 4527, Loss: 0.019128894695313647, Final Batch Loss: 0.0004796197754330933\n",
      "Epoch 4528, Loss: 0.00636337938340148, Final Batch Loss: 0.002826994052156806\n",
      "Epoch 4529, Loss: 0.012505595732363872, Final Batch Loss: 0.0007448869291692972\n",
      "Epoch 4530, Loss: 0.022719408181728795, Final Batch Loss: 0.0003946438373532146\n",
      "Epoch 4531, Loss: 0.024419020570348948, Final Batch Loss: 0.0013853309210389853\n",
      "Epoch 4532, Loss: 0.012040361732942984, Final Batch Loss: 0.003018804360181093\n",
      "Epoch 4533, Loss: 0.006935665354831144, Final Batch Loss: 0.002103364560753107\n",
      "Epoch 4534, Loss: 0.006699465826386586, Final Batch Loss: 0.0012043798342347145\n",
      "Epoch 4535, Loss: 0.00515607868146617, Final Batch Loss: 0.00024063080491032451\n",
      "Epoch 4536, Loss: 0.019198915077140555, Final Batch Loss: 0.003932725638151169\n",
      "Epoch 4537, Loss: 0.004175701658823527, Final Batch Loss: 0.003103704424574971\n",
      "Epoch 4538, Loss: 0.02977273057331331, Final Batch Loss: 0.00030430516926571727\n",
      "Epoch 4539, Loss: 0.023694372190220747, Final Batch Loss: 9.108099766308442e-05\n",
      "Epoch 4540, Loss: 0.015170663216849789, Final Batch Loss: 0.00737000722438097\n",
      "Epoch 4541, Loss: 0.03302146717032883, Final Batch Loss: 0.01904458738863468\n",
      "Epoch 4542, Loss: 0.020646206190576777, Final Batch Loss: 0.0019287961767986417\n",
      "Epoch 4543, Loss: 0.01710582809027983, Final Batch Loss: 0.0019125302787870169\n",
      "Epoch 4544, Loss: 0.0060020069940947, Final Batch Loss: 0.001629477133974433\n",
      "Epoch 4545, Loss: 0.001977589097805321, Final Batch Loss: 0.00012187290121801198\n",
      "Epoch 4546, Loss: 0.012683212538831867, Final Batch Loss: 0.0014334649313241243\n",
      "Epoch 4547, Loss: 0.004947293411532883, Final Batch Loss: 0.001971024554222822\n",
      "Epoch 4548, Loss: 0.018788205459713936, Final Batch Loss: 0.0020111133344471455\n",
      "Epoch 4549, Loss: 0.008029190154047683, Final Batch Loss: 0.0003383257135283202\n",
      "Epoch 4550, Loss: 0.004631038158549927, Final Batch Loss: 0.00013593041512649506\n",
      "Epoch 4551, Loss: 0.009508971284958534, Final Batch Loss: 0.004385881591588259\n",
      "Epoch 4552, Loss: 0.00880428399250377, Final Batch Loss: 0.0007136273197829723\n",
      "Epoch 4553, Loss: 0.012452113500330597, Final Batch Loss: 0.0008636401616968215\n",
      "Epoch 4554, Loss: 0.015190917940344661, Final Batch Loss: 0.00037427563802339137\n",
      "Epoch 4555, Loss: 0.013149458914995193, Final Batch Loss: 0.0008326240349560976\n",
      "Epoch 4556, Loss: 0.012190690657007508, Final Batch Loss: 0.00021405897859949619\n",
      "Epoch 4557, Loss: 0.0031710681651020423, Final Batch Loss: 0.0005425068084150553\n",
      "Epoch 4558, Loss: 0.03773793583968654, Final Batch Loss: 0.013912748545408249\n",
      "Epoch 4559, Loss: 0.027983775245957077, Final Batch Loss: 8.889005403034389e-05\n",
      "Epoch 4560, Loss: 0.003698224783875048, Final Batch Loss: 0.00021536642452701926\n",
      "Epoch 4561, Loss: 0.0031905325304251164, Final Batch Loss: 0.0016256347298622131\n",
      "Epoch 4562, Loss: 0.016872936626896262, Final Batch Loss: 0.010275804437696934\n",
      "Epoch 4563, Loss: 0.014322148694191128, Final Batch Loss: 0.00041328492807224393\n",
      "Epoch 4564, Loss: 0.009603634171071462, Final Batch Loss: 5.5591968703083694e-05\n",
      "Epoch 4565, Loss: 0.011127507226774469, Final Batch Loss: 0.0013080734061077237\n",
      "Epoch 4566, Loss: 0.03032875611097552, Final Batch Loss: 0.028250280767679214\n",
      "Epoch 4567, Loss: 0.007932379478006624, Final Batch Loss: 0.0007228858303278685\n",
      "Epoch 4568, Loss: 0.012914736042148434, Final Batch Loss: 0.0011379412608221173\n",
      "Epoch 4569, Loss: 0.013685961224837229, Final Batch Loss: 0.0028559837955981493\n",
      "Epoch 4570, Loss: 0.007130265024898108, Final Batch Loss: 0.0009577964665368199\n",
      "Epoch 4571, Loss: 0.0045772791127092205, Final Batch Loss: 0.00036233841092325747\n",
      "Epoch 4572, Loss: 0.0041061668016482145, Final Batch Loss: 0.00020816733012907207\n",
      "Epoch 4573, Loss: 0.002802494549541734, Final Batch Loss: 0.0015479420544579625\n",
      "Epoch 4574, Loss: 0.00289274295209907, Final Batch Loss: 8.03865259513259e-05\n",
      "Epoch 4575, Loss: 0.004191142797935754, Final Batch Loss: 0.0017484385753050447\n",
      "Epoch 4576, Loss: 0.004610118034179322, Final Batch Loss: 0.00016845132631715387\n",
      "Epoch 4577, Loss: 0.0029873368330299854, Final Batch Loss: 0.0014244763879105449\n",
      "Epoch 4578, Loss: 0.005233330128248781, Final Batch Loss: 0.0007929904386401176\n",
      "Epoch 4579, Loss: 0.050318390436586924, Final Batch Loss: 0.00020043841504957527\n",
      "Epoch 4580, Loss: 0.042556685279123485, Final Batch Loss: 0.0007405619253404438\n",
      "Epoch 4581, Loss: 0.009923536563292146, Final Batch Loss: 0.0017216859851032495\n",
      "Epoch 4582, Loss: 0.014242617355193943, Final Batch Loss: 0.008015613071620464\n",
      "Epoch 4583, Loss: 0.004939154408930335, Final Batch Loss: 0.0014186560874804854\n",
      "Epoch 4584, Loss: 0.05087338387966156, Final Batch Loss: 0.041063856333494186\n",
      "Epoch 4585, Loss: 0.03302778134820983, Final Batch Loss: 0.02878214418888092\n",
      "Epoch 4586, Loss: 0.014619423018302768, Final Batch Loss: 0.00022660812828689814\n",
      "Epoch 4587, Loss: 0.007970171107444912, Final Batch Loss: 0.00026176011306233704\n",
      "Epoch 4588, Loss: 0.006800326867960393, Final Batch Loss: 0.0018500173464417458\n",
      "Epoch 4589, Loss: 0.008623645699117333, Final Batch Loss: 0.0009159630862995982\n",
      "Epoch 4590, Loss: 0.016664891416439787, Final Batch Loss: 0.0007880012271925807\n",
      "Epoch 4591, Loss: 0.0076520391667145304, Final Batch Loss: 6.382015271810815e-05\n",
      "Epoch 4592, Loss: 0.0047937361523509026, Final Batch Loss: 0.00025788811035454273\n",
      "Epoch 4593, Loss: 0.019466758269118145, Final Batch Loss: 0.0005899841198697686\n",
      "Epoch 4594, Loss: 0.01218485159915872, Final Batch Loss: 0.007431515958160162\n",
      "Epoch 4595, Loss: 0.031054969576871372, Final Batch Loss: 2.873570156225469e-05\n",
      "Epoch 4596, Loss: 0.005569619417656213, Final Batch Loss: 0.00012759558740071952\n",
      "Epoch 4597, Loss: 0.007557249831734225, Final Batch Loss: 0.0026037590578198433\n",
      "Epoch 4598, Loss: 0.012786445440724492, Final Batch Loss: 0.0021209209226071835\n",
      "Epoch 4599, Loss: 0.007013946611550637, Final Batch Loss: 0.0010252344654873013\n",
      "Epoch 4600, Loss: 0.008708106935955584, Final Batch Loss: 0.004245460499078035\n",
      "Epoch 4601, Loss: 0.03192524690530263, Final Batch Loss: 0.000791686587035656\n",
      "Epoch 4602, Loss: 0.01119052225840278, Final Batch Loss: 6.56626361887902e-05\n",
      "Epoch 4603, Loss: 0.02409111116139684, Final Batch Loss: 0.00019929088011849672\n",
      "Epoch 4604, Loss: 0.013467981305439025, Final Batch Loss: 0.011066785082221031\n",
      "Epoch 4605, Loss: 0.0033239736876566894, Final Batch Loss: 0.0011548273032531142\n",
      "Epoch 4606, Loss: 0.002488954683940392, Final Batch Loss: 0.0004522284434642643\n",
      "Epoch 4607, Loss: 0.004707126448920462, Final Batch Loss: 0.0010475229937583208\n",
      "Epoch 4608, Loss: 0.008471922934404574, Final Batch Loss: 0.00019928370602428913\n",
      "Epoch 4609, Loss: 0.002522213035263121, Final Batch Loss: 0.00021724715770687908\n",
      "Epoch 4610, Loss: 0.0021010898344684392, Final Batch Loss: 0.00018640447524376214\n",
      "Epoch 4611, Loss: 0.004315255107940175, Final Batch Loss: 0.0023680527228862047\n",
      "Epoch 4612, Loss: 0.0029654700483661145, Final Batch Loss: 0.0008533875807188451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4613, Loss: 0.027174650167580694, Final Batch Loss: 0.0008349506533704698\n",
      "Epoch 4614, Loss: 0.013532352822949179, Final Batch Loss: 0.006824206560850143\n",
      "Epoch 4615, Loss: 0.0041112763865385205, Final Batch Loss: 0.0003943510237149894\n",
      "Epoch 4616, Loss: 0.0021269182325340807, Final Batch Loss: 0.00025981536600738764\n",
      "Epoch 4617, Loss: 0.022620039293542504, Final Batch Loss: 0.0009073358378373086\n",
      "Epoch 4618, Loss: 0.034637610748177394, Final Batch Loss: 0.004005146212875843\n",
      "Epoch 4619, Loss: 0.014096498605795205, Final Batch Loss: 0.001181615050882101\n",
      "Epoch 4620, Loss: 0.0028486184382927604, Final Batch Loss: 4.480242932913825e-05\n",
      "Epoch 4621, Loss: 0.004794409702299163, Final Batch Loss: 0.0007692027138546109\n",
      "Epoch 4622, Loss: 0.03568794755847193, Final Batch Loss: 0.0008145595784299076\n",
      "Epoch 4623, Loss: 0.022794093754782807, Final Batch Loss: 9.87979510682635e-05\n",
      "Epoch 4624, Loss: 0.038031734206015244, Final Batch Loss: 0.0003467438800726086\n",
      "Epoch 4625, Loss: 0.00481117391746011, Final Batch Loss: 4.6182262849470135e-06\n",
      "Epoch 4626, Loss: 0.04382467208779417, Final Batch Loss: 0.0006152800633572042\n",
      "Epoch 4627, Loss: 0.028429579106159508, Final Batch Loss: 0.0059058223851025105\n",
      "Epoch 4628, Loss: 0.039355635817628354, Final Batch Loss: 0.00025920988991856575\n",
      "Epoch 4629, Loss: 0.030697352602146566, Final Batch Loss: 0.01886570453643799\n",
      "Epoch 4630, Loss: 0.010462092366651632, Final Batch Loss: 0.0037124210502952337\n",
      "Epoch 4631, Loss: 0.047113858090597205, Final Batch Loss: 0.00015802939014974982\n",
      "Epoch 4632, Loss: 0.04348545614629984, Final Batch Loss: 0.01587291620671749\n",
      "Epoch 4633, Loss: 0.034166835219366476, Final Batch Loss: 0.0002530386846046895\n",
      "Epoch 4634, Loss: 0.021195239096414298, Final Batch Loss: 0.00025597738567739725\n",
      "Epoch 4635, Loss: 0.0036445347795961425, Final Batch Loss: 0.00019572976452764124\n",
      "Epoch 4636, Loss: 0.05703689780784771, Final Batch Loss: 0.0002741871285252273\n",
      "Epoch 4637, Loss: 0.009926737344358116, Final Batch Loss: 0.0025018828455358744\n",
      "Epoch 4638, Loss: 0.008405792410485446, Final Batch Loss: 0.0009054869296960533\n",
      "Epoch 4639, Loss: 0.010988797643221915, Final Batch Loss: 0.003407490672543645\n",
      "Epoch 4640, Loss: 0.011447280790889636, Final Batch Loss: 0.00010949725401587784\n",
      "Epoch 4641, Loss: 0.045609243359649554, Final Batch Loss: 0.0010118200443685055\n",
      "Epoch 4642, Loss: 0.03240473355981521, Final Batch Loss: 0.00208351225592196\n",
      "Epoch 4643, Loss: 0.0409625651082024, Final Batch Loss: 0.02839675173163414\n",
      "Epoch 4644, Loss: 0.024943870666902512, Final Batch Loss: 0.0012807687744498253\n",
      "Epoch 4645, Loss: 0.016053489118348807, Final Batch Loss: 0.00023250302183441818\n",
      "Epoch 4646, Loss: 0.029498003190383315, Final Batch Loss: 0.0006254996987991035\n",
      "Epoch 4647, Loss: 0.003875791429891251, Final Batch Loss: 0.0019683074206113815\n",
      "Epoch 4648, Loss: 0.008261341543402523, Final Batch Loss: 0.0021798566449433565\n",
      "Epoch 4649, Loss: 0.003504031599732116, Final Batch Loss: 0.0002909761678893119\n",
      "Epoch 4650, Loss: 0.0033413753262721, Final Batch Loss: 0.00040946947410702705\n",
      "Epoch 4651, Loss: 0.022291461005806923, Final Batch Loss: 7.10432359483093e-05\n",
      "Epoch 4652, Loss: 0.006035651131242048, Final Batch Loss: 0.0020360916387289762\n",
      "Epoch 4653, Loss: 0.0074765984900295734, Final Batch Loss: 0.001082829898223281\n",
      "Epoch 4654, Loss: 0.009976880479371175, Final Batch Loss: 0.0023570903576910496\n",
      "Epoch 4655, Loss: 0.018610879313200712, Final Batch Loss: 0.0007490823627449572\n",
      "Epoch 4656, Loss: 0.025332457153126597, Final Batch Loss: 0.007340777199715376\n",
      "Epoch 4657, Loss: 0.0057137501644319855, Final Batch Loss: 0.00011872420873260126\n",
      "Epoch 4658, Loss: 0.012112139485907392, Final Batch Loss: 2.8171949452371337e-05\n",
      "Epoch 4659, Loss: 0.007696695567574352, Final Batch Loss: 0.0025492168497294188\n",
      "Epoch 4660, Loss: 0.005638173795887269, Final Batch Loss: 0.00012609643454197794\n",
      "Epoch 4661, Loss: 0.012012521590804681, Final Batch Loss: 0.00440990412607789\n",
      "Epoch 4662, Loss: 0.009927142440574244, Final Batch Loss: 0.003360367612913251\n",
      "Epoch 4663, Loss: 0.003241625367081724, Final Batch Loss: 6.880311411805451e-05\n",
      "Epoch 4664, Loss: 0.0062269198533613235, Final Batch Loss: 0.00034342744038440287\n",
      "Epoch 4665, Loss: 0.02949651359085692, Final Batch Loss: 0.02109953574836254\n",
      "Epoch 4666, Loss: 0.00369755772408098, Final Batch Loss: 9.211021824739873e-05\n",
      "Epoch 4667, Loss: 0.021686644744477235, Final Batch Loss: 0.00015325001731980592\n",
      "Epoch 4668, Loss: 0.02480150260089431, Final Batch Loss: 0.0012214838061481714\n",
      "Epoch 4669, Loss: 0.0034133754052163567, Final Batch Loss: 1.5655878087272868e-05\n",
      "Epoch 4670, Loss: 0.04619269313116092, Final Batch Loss: 0.00018320076924283057\n",
      "Epoch 4671, Loss: 0.00923863664502278, Final Batch Loss: 0.003300790674984455\n",
      "Epoch 4672, Loss: 0.014331366681290092, Final Batch Loss: 0.000192869410966523\n",
      "Epoch 4673, Loss: 0.011468044598586857, Final Batch Loss: 0.007939521223306656\n",
      "Epoch 4674, Loss: 0.0033781520614866167, Final Batch Loss: 0.00023543392308056355\n",
      "Epoch 4675, Loss: 0.009719082590891048, Final Batch Loss: 0.0005144124734215438\n",
      "Epoch 4676, Loss: 0.10211576172150671, Final Batch Loss: 0.09085560590028763\n",
      "Epoch 4677, Loss: 0.004110940790269524, Final Batch Loss: 0.000315741024678573\n",
      "Epoch 4678, Loss: 0.06275898544117808, Final Batch Loss: 0.0022217966616153717\n",
      "Epoch 4679, Loss: 0.020548447049804963, Final Batch Loss: 0.003574842121452093\n",
      "Epoch 4680, Loss: 0.010386785572336521, Final Batch Loss: 0.00012046671326970682\n",
      "Epoch 4681, Loss: 0.004880536696873605, Final Batch Loss: 0.0010749326320365071\n",
      "Epoch 4682, Loss: 0.01192731354967691, Final Batch Loss: 0.0008096087258309126\n",
      "Epoch 4683, Loss: 0.05424814875004813, Final Batch Loss: 0.015801377594470978\n",
      "Epoch 4684, Loss: 0.0371777948239469, Final Batch Loss: 0.001136214123107493\n",
      "Epoch 4685, Loss: 0.02768087509321049, Final Batch Loss: 0.0017076126532629132\n",
      "Epoch 4686, Loss: 0.03288419230375439, Final Batch Loss: 0.009293622337281704\n",
      "Epoch 4687, Loss: 0.03437262855004519, Final Batch Loss: 0.0014060362009331584\n",
      "Epoch 4688, Loss: 0.04180662822909653, Final Batch Loss: 0.004027960356324911\n",
      "Epoch 4689, Loss: 0.02265220563276671, Final Batch Loss: 0.0004204299475532025\n",
      "Epoch 4690, Loss: 0.023449557047570124, Final Batch Loss: 6.29444548394531e-05\n",
      "Epoch 4691, Loss: 0.035994465462863445, Final Batch Loss: 0.010449644178152084\n",
      "Epoch 4692, Loss: 0.027147777087520808, Final Batch Loss: 0.00048785394756123424\n",
      "Epoch 4693, Loss: 0.018688103184103966, Final Batch Loss: 0.001619139569811523\n",
      "Epoch 4694, Loss: 0.004125312945689075, Final Batch Loss: 0.0013506297254934907\n",
      "Epoch 4695, Loss: 0.012645960290683433, Final Batch Loss: 0.0005404108669608831\n",
      "Epoch 4696, Loss: 0.008441901998594403, Final Batch Loss: 0.0009167038952000439\n",
      "Epoch 4697, Loss: 0.01022107522294391, Final Batch Loss: 0.003169843927025795\n",
      "Epoch 4698, Loss: 0.0058198924525640905, Final Batch Loss: 0.0014177403645589948\n",
      "Epoch 4699, Loss: 0.022974978550337255, Final Batch Loss: 0.0019798297435045242\n",
      "Epoch 4700, Loss: 0.015664780978113413, Final Batch Loss: 0.0003372070495970547\n",
      "Epoch 4701, Loss: 0.025064483226742595, Final Batch Loss: 0.0030154951382428408\n",
      "Epoch 4702, Loss: 0.006892563600558788, Final Batch Loss: 0.0014335296582430601\n",
      "Epoch 4703, Loss: 0.03635310256504454, Final Batch Loss: 0.032848600298166275\n",
      "Epoch 4704, Loss: 0.01525883621070534, Final Batch Loss: 0.007758784107863903\n",
      "Epoch 4705, Loss: 0.034533700585598126, Final Batch Loss: 0.022373812273144722\n",
      "Epoch 4706, Loss: 0.016342147486284375, Final Batch Loss: 0.0003302802797406912\n",
      "Epoch 4707, Loss: 0.05247827642597258, Final Batch Loss: 0.001503906911239028\n",
      "Epoch 4708, Loss: 0.014004186319652945, Final Batch Loss: 0.0057483636774122715\n",
      "Epoch 4709, Loss: 0.024650150968227535, Final Batch Loss: 0.0003215013421140611\n",
      "Epoch 4710, Loss: 0.007397714514809195, Final Batch Loss: 0.00010741767619038001\n",
      "Epoch 4711, Loss: 0.028543633496155962, Final Batch Loss: 0.0003338181122671813\n",
      "Epoch 4712, Loss: 0.005576316965743899, Final Batch Loss: 0.0013301922008395195\n",
      "Epoch 4713, Loss: 0.011316121817799285, Final Batch Loss: 0.00023570287157781422\n",
      "Epoch 4714, Loss: 0.013688823783013504, Final Batch Loss: 7.515527977375314e-05\n",
      "Epoch 4715, Loss: 0.018633039784617722, Final Batch Loss: 0.015015737153589725\n",
      "Epoch 4716, Loss: 0.029337748361285776, Final Batch Loss: 0.00018622871721163392\n",
      "Epoch 4717, Loss: 0.01589539353881264, Final Batch Loss: 7.140226807678118e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4718, Loss: 0.014335269486764446, Final Batch Loss: 0.0024268224369734526\n",
      "Epoch 4719, Loss: 0.003871758708555717, Final Batch Loss: 9.212294389726594e-05\n",
      "Epoch 4720, Loss: 0.004759821167681366, Final Batch Loss: 0.00032353115966543555\n",
      "Epoch 4721, Loss: 0.0033552004460943863, Final Batch Loss: 0.00018980987078975886\n",
      "Epoch 4722, Loss: 0.04153123599826358, Final Batch Loss: 0.03229701891541481\n",
      "Epoch 4723, Loss: 0.003815122734522447, Final Batch Loss: 0.00025096142780967057\n",
      "Epoch 4724, Loss: 0.006620177024160512, Final Batch Loss: 0.0007702562143094838\n",
      "Epoch 4725, Loss: 0.00645937625085935, Final Batch Loss: 0.0039618173614144325\n",
      "Epoch 4726, Loss: 0.019625080458354205, Final Batch Loss: 0.003636843990534544\n",
      "Epoch 4727, Loss: 0.013420890863926616, Final Batch Loss: 0.0005522822611965239\n",
      "Epoch 4728, Loss: 0.010260729846777394, Final Batch Loss: 0.0003172342258039862\n",
      "Epoch 4729, Loss: 0.007926562895590905, Final Batch Loss: 7.930221181595698e-05\n",
      "Epoch 4730, Loss: 0.025269449353800155, Final Batch Loss: 0.010529709048569202\n",
      "Epoch 4731, Loss: 0.004225424840115011, Final Batch Loss: 0.0004817211301997304\n",
      "Epoch 4732, Loss: 0.042405661777593195, Final Batch Loss: 0.0017913172487169504\n",
      "Epoch 4733, Loss: 0.03072837385116145, Final Batch Loss: 0.00016349897487089038\n",
      "Epoch 4734, Loss: 0.03444996994949179, Final Batch Loss: 0.011702015064656734\n",
      "Epoch 4735, Loss: 0.034684357699006796, Final Batch Loss: 0.013688626699149609\n",
      "Epoch 4736, Loss: 0.03070922219194472, Final Batch Loss: 0.000723885023035109\n",
      "Epoch 4737, Loss: 0.013677178707439452, Final Batch Loss: 0.000217415887163952\n",
      "Epoch 4738, Loss: 0.021834653583937325, Final Batch Loss: 0.0013667840976268053\n",
      "Epoch 4739, Loss: 0.008408111069002189, Final Batch Loss: 0.000496513384860009\n",
      "Epoch 4740, Loss: 0.029607450094772503, Final Batch Loss: 0.0021481658332049847\n",
      "Epoch 4741, Loss: 0.016247113526333123, Final Batch Loss: 0.004494750406593084\n",
      "Epoch 4742, Loss: 0.01182229319238104, Final Batch Loss: 0.002048227470368147\n",
      "Epoch 4743, Loss: 0.028823899454437196, Final Batch Loss: 0.0002368733985349536\n",
      "Epoch 4744, Loss: 0.019246327690780163, Final Batch Loss: 0.008093638345599174\n",
      "Epoch 4745, Loss: 0.006893876299727708, Final Batch Loss: 0.0015621966449543834\n",
      "Epoch 4746, Loss: 0.005769239432993345, Final Batch Loss: 0.002177770948037505\n",
      "Epoch 4747, Loss: 0.031264538440154865, Final Batch Loss: 0.026903409510850906\n",
      "Epoch 4748, Loss: 0.010326314950361848, Final Batch Loss: 0.0004734994436148554\n",
      "Epoch 4749, Loss: 0.004875311424257234, Final Batch Loss: 0.0009392813080921769\n",
      "Epoch 4750, Loss: 0.014248227424104698, Final Batch Loss: 0.00012525056081358343\n",
      "Epoch 4751, Loss: 0.005801001854706556, Final Batch Loss: 0.0036328965798020363\n",
      "Epoch 4752, Loss: 0.008577542168495711, Final Batch Loss: 9.944260091288015e-05\n",
      "Epoch 4753, Loss: 0.003936479042749852, Final Batch Loss: 0.0003265826089773327\n",
      "Epoch 4754, Loss: 0.003574846952687949, Final Batch Loss: 0.000698514049872756\n",
      "Epoch 4755, Loss: 0.007889173255534843, Final Batch Loss: 0.000594276178162545\n",
      "Epoch 4756, Loss: 0.007505365530960262, Final Batch Loss: 0.0041768657974898815\n",
      "Epoch 4757, Loss: 0.004596448048687307, Final Batch Loss: 3.4400331060169265e-05\n",
      "Epoch 4758, Loss: 0.0033143991604447365, Final Batch Loss: 0.0009770767064765096\n",
      "Epoch 4759, Loss: 0.00706395939050708, Final Batch Loss: 0.000205401869607158\n",
      "Epoch 4760, Loss: 0.00533258133509662, Final Batch Loss: 0.00015592199633829296\n",
      "Epoch 4761, Loss: 0.016235422692261636, Final Batch Loss: 0.0006714942865073681\n",
      "Epoch 4762, Loss: 0.004409680987009779, Final Batch Loss: 0.0008328472613357008\n",
      "Epoch 4763, Loss: 0.04757438617525622, Final Batch Loss: 0.0005615080590359867\n",
      "Epoch 4764, Loss: 0.0021516231245186646, Final Batch Loss: 0.0003304241690784693\n",
      "Epoch 4765, Loss: 0.05542287940625101, Final Batch Loss: 0.042513567954301834\n",
      "Epoch 4766, Loss: 0.012355727434623986, Final Batch Loss: 0.0008334473823197186\n",
      "Epoch 4767, Loss: 0.00859910152212251, Final Batch Loss: 0.0001535431802039966\n",
      "Epoch 4768, Loss: 0.009899668097204994, Final Batch Loss: 0.0018500100122764707\n",
      "Epoch 4769, Loss: 0.0068393303954508156, Final Batch Loss: 9.965136996470392e-05\n",
      "Epoch 4770, Loss: 0.016391947800002526, Final Batch Loss: 0.00011125881428597495\n",
      "Epoch 4771, Loss: 0.008781827695202082, Final Batch Loss: 0.00012663396773859859\n",
      "Epoch 4772, Loss: 0.005347184633137658, Final Batch Loss: 0.0016618259251117706\n",
      "Epoch 4773, Loss: 0.0037216839264146984, Final Batch Loss: 0.0013292379444465041\n",
      "Epoch 4774, Loss: 0.016810232293209992, Final Batch Loss: 0.0010860698530450463\n",
      "Epoch 4775, Loss: 0.01587584322260227, Final Batch Loss: 8.934373909141868e-05\n",
      "Epoch 4776, Loss: 0.010204054386122152, Final Batch Loss: 0.00022770880605094135\n",
      "Epoch 4777, Loss: 0.00395510156522505, Final Batch Loss: 4.9091147957369685e-05\n",
      "Epoch 4778, Loss: 0.012725510365271475, Final Batch Loss: 0.00021376324002631009\n",
      "Epoch 4779, Loss: 0.009136338761891238, Final Batch Loss: 0.0007420292822644114\n",
      "Epoch 4780, Loss: 0.0033300467330263928, Final Batch Loss: 0.0005897789378650486\n",
      "Epoch 4781, Loss: 0.004329670569859445, Final Batch Loss: 0.0015034039970487356\n",
      "Epoch 4782, Loss: 0.03870387881761417, Final Batch Loss: 0.036101385951042175\n",
      "Epoch 4783, Loss: 0.05316094565205276, Final Batch Loss: 0.04504331573843956\n",
      "Epoch 4784, Loss: 0.006093566749768797, Final Batch Loss: 0.0013139051152393222\n",
      "Epoch 4785, Loss: 0.0040748850733507425, Final Batch Loss: 0.0010417794110253453\n",
      "Epoch 4786, Loss: 0.02429075950931292, Final Batch Loss: 0.010236797854304314\n",
      "Epoch 4787, Loss: 0.0037827176856808364, Final Batch Loss: 0.00010140993254026398\n",
      "Epoch 4788, Loss: 0.005419846595032141, Final Batch Loss: 0.0006274692714214325\n",
      "Epoch 4789, Loss: 0.005739202766562812, Final Batch Loss: 0.00010330022632842883\n",
      "Epoch 4790, Loss: 0.009519431594526395, Final Batch Loss: 0.0020065084099769592\n",
      "Epoch 4791, Loss: 0.02255253808107227, Final Batch Loss: 0.00012836838141083717\n",
      "Epoch 4792, Loss: 0.0034011218704108614, Final Batch Loss: 5.340198185876943e-05\n",
      "Epoch 4793, Loss: 0.018422316570649855, Final Batch Loss: 0.0005375657347030938\n",
      "Epoch 4794, Loss: 0.005887827399419621, Final Batch Loss: 0.00019997646450065076\n",
      "Epoch 4795, Loss: 0.0044595091239898466, Final Batch Loss: 0.00012027390039293095\n",
      "Epoch 4796, Loss: 0.0033028002362698317, Final Batch Loss: 0.0007088739657774568\n",
      "Epoch 4797, Loss: 0.01110349478040007, Final Batch Loss: 0.00023012846941128373\n",
      "Epoch 4798, Loss: 0.0031536711321678013, Final Batch Loss: 0.0001824961946113035\n",
      "Epoch 4799, Loss: 0.03079016563424375, Final Batch Loss: 0.0001952867751242593\n",
      "Epoch 4800, Loss: 0.039379085195832886, Final Batch Loss: 0.00020493440388236195\n",
      "Epoch 4801, Loss: 0.002143238940334413, Final Batch Loss: 8.174905815394595e-05\n",
      "Epoch 4802, Loss: 0.013689899695236818, Final Batch Loss: 0.001526211271993816\n",
      "Epoch 4803, Loss: 0.009846605855273083, Final Batch Loss: 0.00032800858025439084\n",
      "Epoch 4804, Loss: 0.013728501959121786, Final Batch Loss: 0.0014443426625803113\n",
      "Epoch 4805, Loss: 0.014204740669811144, Final Batch Loss: 0.00046722390106879175\n",
      "Epoch 4806, Loss: 0.008630556556454394, Final Batch Loss: 9.50676403590478e-05\n",
      "Epoch 4807, Loss: 0.010028754104496329, Final Batch Loss: 1.3100105206831358e-05\n",
      "Epoch 4808, Loss: 0.02221685414406238, Final Batch Loss: 0.00019817335123661906\n",
      "Epoch 4809, Loss: 0.007660117480554618, Final Batch Loss: 0.0013577992795035243\n",
      "Epoch 4810, Loss: 0.0053478019835893065, Final Batch Loss: 0.003001259174197912\n",
      "Epoch 4811, Loss: 0.010183022488490678, Final Batch Loss: 0.00840388610959053\n",
      "Epoch 4812, Loss: 0.008562339149648324, Final Batch Loss: 0.005892944522202015\n",
      "Epoch 4813, Loss: 0.027063971720053814, Final Batch Loss: 0.0003838084521703422\n",
      "Epoch 4814, Loss: 0.005002296165912412, Final Batch Loss: 9.099549788516015e-05\n",
      "Epoch 4815, Loss: 0.002303068496985361, Final Batch Loss: 0.000661835540086031\n",
      "Epoch 4816, Loss: 0.01586091867648065, Final Batch Loss: 0.0002911644405685365\n",
      "Epoch 4817, Loss: 0.015862021129578352, Final Batch Loss: 0.0016662012785673141\n",
      "Epoch 4818, Loss: 0.005840098776388913, Final Batch Loss: 0.0011557044927030802\n",
      "Epoch 4819, Loss: 0.012123474400141276, Final Batch Loss: 0.0009863453451544046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4820, Loss: 0.017677696188911796, Final Batch Loss: 0.0008375411271117628\n",
      "Epoch 4821, Loss: 0.0028041667392244563, Final Batch Loss: 0.0002065045846393332\n",
      "Epoch 4822, Loss: 0.005807823734357953, Final Batch Loss: 0.0001918045454658568\n",
      "Epoch 4823, Loss: 0.012259137234650552, Final Batch Loss: 0.00013601270620711148\n",
      "Epoch 4824, Loss: 0.00783834044705145, Final Batch Loss: 0.005275893025100231\n",
      "Epoch 4825, Loss: 0.007292620422958862, Final Batch Loss: 0.0015596130397170782\n",
      "Epoch 4826, Loss: 0.00815995383891277, Final Batch Loss: 0.002721584402024746\n",
      "Epoch 4827, Loss: 0.007479259104002267, Final Batch Loss: 0.0002813012106344104\n",
      "Epoch 4828, Loss: 0.020805168052902445, Final Batch Loss: 0.006872239056974649\n",
      "Epoch 4829, Loss: 0.028254280630790163, Final Batch Loss: 7.778649887768552e-05\n",
      "Epoch 4830, Loss: 0.03700379688598332, Final Batch Loss: 0.002482474548742175\n",
      "Epoch 4831, Loss: 0.004824027168069733, Final Batch Loss: 2.137994670192711e-05\n",
      "Epoch 4832, Loss: 0.036213280145602766, Final Batch Loss: 7.620833639521152e-05\n",
      "Epoch 4833, Loss: 0.0060169681964907795, Final Batch Loss: 0.0010357163846492767\n",
      "Epoch 4834, Loss: 0.006019082982675172, Final Batch Loss: 0.00013658184616360813\n",
      "Epoch 4835, Loss: 0.017554890699102543, Final Batch Loss: 8.209476072806865e-05\n",
      "Epoch 4836, Loss: 0.011423517986258958, Final Batch Loss: 9.51935289776884e-05\n",
      "Epoch 4837, Loss: 0.022428873933677096, Final Batch Loss: 4.0855615225154907e-05\n",
      "Epoch 4838, Loss: 0.012939214590005577, Final Batch Loss: 0.00010967296839226037\n",
      "Epoch 4839, Loss: 0.00759514086166746, Final Batch Loss: 0.0007776633137837052\n",
      "Epoch 4840, Loss: 0.006168570700538112, Final Batch Loss: 0.003608093596994877\n",
      "Epoch 4841, Loss: 0.0330533543456113, Final Batch Loss: 0.02100951038300991\n",
      "Epoch 4842, Loss: 0.004079346726939548, Final Batch Loss: 0.0010332724777981639\n",
      "Epoch 4843, Loss: 0.01106708636507392, Final Batch Loss: 0.0014311744598671794\n",
      "Epoch 4844, Loss: 0.024635001027490944, Final Batch Loss: 0.00041079713264480233\n",
      "Epoch 4845, Loss: 0.012572710256790742, Final Batch Loss: 0.00022435776190832257\n",
      "Epoch 4846, Loss: 0.0061999852187000215, Final Batch Loss: 0.0015503654722124338\n",
      "Epoch 4847, Loss: 0.0030645027582067996, Final Batch Loss: 0.0004351692332420498\n",
      "Epoch 4848, Loss: 0.006562389815371716, Final Batch Loss: 3.034113979083486e-05\n",
      "Epoch 4849, Loss: 0.002662691866134992, Final Batch Loss: 3.922541145584546e-05\n",
      "Epoch 4850, Loss: 0.0080903318739729, Final Batch Loss: 0.0017830170691013336\n",
      "Epoch 4851, Loss: 0.019124985672533512, Final Batch Loss: 0.0009050691151060164\n",
      "Epoch 4852, Loss: 0.005950156803010032, Final Batch Loss: 0.00013455914449878037\n",
      "Epoch 4853, Loss: 0.025022853631526232, Final Batch Loss: 0.00010693541844375432\n",
      "Epoch 4854, Loss: 0.009435520158149302, Final Batch Loss: 0.00046574859879910946\n",
      "Epoch 4855, Loss: 0.006466978902608389, Final Batch Loss: 0.0005230302340351045\n",
      "Epoch 4856, Loss: 0.023636761217858293, Final Batch Loss: 4.6490087697748095e-05\n",
      "Epoch 4857, Loss: 0.0037049633101560175, Final Batch Loss: 0.0006009062053635716\n",
      "Epoch 4858, Loss: 0.025062432687263936, Final Batch Loss: 0.00040391882066614926\n",
      "Epoch 4859, Loss: 0.028932956272910815, Final Batch Loss: 7.685698074055836e-05\n",
      "Epoch 4860, Loss: 0.005162375018699095, Final Batch Loss: 0.00035077103530056775\n",
      "Epoch 4861, Loss: 0.0034165427496191114, Final Batch Loss: 0.0007625631405971944\n",
      "Epoch 4862, Loss: 0.006476681708591059, Final Batch Loss: 0.000999287236481905\n",
      "Epoch 4863, Loss: 0.029626316478243098, Final Batch Loss: 0.010787772946059704\n",
      "Epoch 4864, Loss: 0.0023963507264852524, Final Batch Loss: 0.0002230810932815075\n",
      "Epoch 4865, Loss: 0.004565850700601004, Final Batch Loss: 0.00021941553859505802\n",
      "Epoch 4866, Loss: 0.01535185650573112, Final Batch Loss: 9.504098852630705e-05\n",
      "Epoch 4867, Loss: 0.04008852242259309, Final Batch Loss: 0.00014649878721684217\n",
      "Epoch 4868, Loss: 0.016193538191146217, Final Batch Loss: 0.0025165495462715626\n",
      "Epoch 4869, Loss: 0.016892850369913504, Final Batch Loss: 0.0046906969510018826\n",
      "Epoch 4870, Loss: 0.002692025460419245, Final Batch Loss: 0.00028907207888551056\n",
      "Epoch 4871, Loss: 0.0036465551747824065, Final Batch Loss: 0.00011966865713475272\n",
      "Epoch 4872, Loss: 0.020540647441521287, Final Batch Loss: 0.0002968914050143212\n",
      "Epoch 4873, Loss: 0.02745961447362788, Final Batch Loss: 0.0019957064650952816\n",
      "Epoch 4874, Loss: 0.026544795575318858, Final Batch Loss: 0.009879798628389835\n",
      "Epoch 4875, Loss: 0.005897060103961849, Final Batch Loss: 0.004101140424609184\n",
      "Epoch 4876, Loss: 0.022962652408750728, Final Batch Loss: 2.169546132790856e-05\n",
      "Epoch 4877, Loss: 0.00784694924368523, Final Batch Loss: 0.00013915271847508848\n",
      "Epoch 4878, Loss: 0.03132711873695371, Final Batch Loss: 0.0001152987897512503\n",
      "Epoch 4879, Loss: 0.04094084131065756, Final Batch Loss: 0.02244706079363823\n",
      "Epoch 4880, Loss: 0.035068332334049046, Final Batch Loss: 0.026770401746034622\n",
      "Epoch 4881, Loss: 0.009005102750961669, Final Batch Loss: 0.0005434906925074756\n",
      "Epoch 4882, Loss: 0.010674585133529035, Final Batch Loss: 3.309153180452995e-05\n",
      "Epoch 4883, Loss: 0.03662082547089085, Final Batch Loss: 0.00025692180497571826\n",
      "Epoch 4884, Loss: 0.03915346886788029, Final Batch Loss: 0.017783764749765396\n",
      "Epoch 4885, Loss: 0.011718549852957949, Final Batch Loss: 0.0006786277517676353\n",
      "Epoch 4886, Loss: 0.0183055143643287, Final Batch Loss: 8.74567631399259e-05\n",
      "Epoch 4887, Loss: 0.045773680292768404, Final Batch Loss: 0.02563096582889557\n",
      "Epoch 4888, Loss: 0.0062356059497687966, Final Batch Loss: 0.0004375078424345702\n",
      "Epoch 4889, Loss: 0.03616531120496802, Final Batch Loss: 0.00028673416818492115\n",
      "Epoch 4890, Loss: 0.01614062048611231, Final Batch Loss: 0.0026686342898756266\n",
      "Epoch 4891, Loss: 0.005197965481784195, Final Batch Loss: 0.00030294881435111165\n",
      "Epoch 4892, Loss: 0.024700176698388532, Final Batch Loss: 0.00015430817438755184\n",
      "Epoch 4893, Loss: 0.006435104209231213, Final Batch Loss: 0.004754097666591406\n",
      "Epoch 4894, Loss: 0.007067849212035071, Final Batch Loss: 0.001760370098054409\n",
      "Epoch 4895, Loss: 0.004485818513785489, Final Batch Loss: 0.0008191619417630136\n",
      "Epoch 4896, Loss: 0.03321076711290516, Final Batch Loss: 0.0013986033154651523\n",
      "Epoch 4897, Loss: 0.006165057246107608, Final Batch Loss: 0.0011836426565423608\n",
      "Epoch 4898, Loss: 0.018406239862088114, Final Batch Loss: 0.00021894853853154927\n",
      "Epoch 4899, Loss: 0.018072470593324397, Final Batch Loss: 0.010661824606359005\n",
      "Epoch 4900, Loss: 0.003227831191907171, Final Batch Loss: 0.0001331600797129795\n",
      "Epoch 4901, Loss: 0.04013785719871521, Final Batch Loss: 0.0010849125683307648\n",
      "Epoch 4902, Loss: 0.009312785841757432, Final Batch Loss: 0.0003458149149082601\n",
      "Epoch 4903, Loss: 0.022805834363680333, Final Batch Loss: 0.0008310293778777122\n",
      "Epoch 4904, Loss: 0.007082914140482899, Final Batch Loss: 0.0008012804901227355\n",
      "Epoch 4905, Loss: 0.004356517049018294, Final Batch Loss: 0.00018318153161089867\n",
      "Epoch 4906, Loss: 0.005591397908574436, Final Batch Loss: 9.965072240447626e-05\n",
      "Epoch 4907, Loss: 0.003467409100267105, Final Batch Loss: 0.0002406646526651457\n",
      "Epoch 4908, Loss: 0.01638054847717285, Final Batch Loss: 0.0103539964184165\n",
      "Epoch 4909, Loss: 0.006853415121440776, Final Batch Loss: 3.8566504372283816e-05\n",
      "Epoch 4910, Loss: 0.04591654660180211, Final Batch Loss: 0.010239104740321636\n",
      "Epoch 4911, Loss: 0.06001969485078007, Final Batch Loss: 0.0069654532708227634\n",
      "Epoch 4912, Loss: 0.031155200362263713, Final Batch Loss: 7.708083285251632e-05\n",
      "Epoch 4913, Loss: 0.03767143748700619, Final Batch Loss: 0.011805647984147072\n",
      "Epoch 4914, Loss: 0.008286235126433894, Final Batch Loss: 0.00017687914078123868\n",
      "Epoch 4915, Loss: 0.014019283815287054, Final Batch Loss: 0.0006660786457359791\n",
      "Epoch 4916, Loss: 0.012453167058993131, Final Batch Loss: 0.00043606985127553344\n",
      "Epoch 4917, Loss: 0.031985285473638214, Final Batch Loss: 0.019498644396662712\n",
      "Epoch 4918, Loss: 0.006562045920873061, Final Batch Loss: 0.0011293190764263272\n",
      "Epoch 4919, Loss: 0.03475553187308833, Final Batch Loss: 0.0032866597175598145\n",
      "Epoch 4920, Loss: 0.02683198020531563, Final Batch Loss: 8.36696635815315e-05\n",
      "Epoch 4921, Loss: 0.012146834123996086, Final Batch Loss: 0.0021449155174195766\n",
      "Epoch 4922, Loss: 0.006059590319637209, Final Batch Loss: 0.0007172820623964071\n",
      "Epoch 4923, Loss: 0.007696286716964096, Final Batch Loss: 0.0014673657715320587\n",
      "Epoch 4924, Loss: 0.002757237700279802, Final Batch Loss: 0.0006648508133366704\n",
      "Epoch 4925, Loss: 0.010701455757953227, Final Batch Loss: 0.007439439184963703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4926, Loss: 0.005724636605009437, Final Batch Loss: 0.0027632536366581917\n",
      "Epoch 4927, Loss: 0.002339673083042726, Final Batch Loss: 0.0006953366100788116\n",
      "Epoch 4928, Loss: 0.004820185466087423, Final Batch Loss: 0.0001631144987186417\n",
      "Epoch 4929, Loss: 0.0056720404827501625, Final Batch Loss: 0.0006872215890325606\n",
      "Epoch 4930, Loss: 0.012873263188339479, Final Batch Loss: 5.006303581467364e-06\n",
      "Epoch 4931, Loss: 0.01851156321936287, Final Batch Loss: 0.0005308067193254828\n",
      "Epoch 4932, Loss: 0.02228632911464956, Final Batch Loss: 1.6443564163637348e-05\n",
      "Epoch 4933, Loss: 0.008566825985326432, Final Batch Loss: 0.00025289563927799463\n",
      "Epoch 4934, Loss: 0.005048715771408752, Final Batch Loss: 0.0017303770873695612\n",
      "Epoch 4935, Loss: 0.0034740335249807686, Final Batch Loss: 0.0004859394975937903\n",
      "Epoch 4936, Loss: 0.01692804893536959, Final Batch Loss: 8.215494744945318e-05\n",
      "Epoch 4937, Loss: 0.02720799126836937, Final Batch Loss: 5.373639578465372e-05\n",
      "Epoch 4938, Loss: 0.0023598492643941427, Final Batch Loss: 0.0010724124731495976\n",
      "Epoch 4939, Loss: 0.02047000817128719, Final Batch Loss: 8.512707609042991e-06\n",
      "Epoch 4940, Loss: 0.04815768200205639, Final Batch Loss: 1.9258674001321197e-05\n",
      "Epoch 4941, Loss: 0.01180021399341058, Final Batch Loss: 0.0015406544553115964\n",
      "Epoch 4942, Loss: 0.04567996825790033, Final Batch Loss: 0.043580006808042526\n",
      "Epoch 4943, Loss: 0.0022715351296938024, Final Batch Loss: 0.0003049620718229562\n",
      "Epoch 4944, Loss: 0.004327492060838267, Final Batch Loss: 0.00017929283785633743\n",
      "Epoch 4945, Loss: 0.006686873472062871, Final Batch Loss: 0.0034195885527879\n",
      "Epoch 4946, Loss: 0.007319403433939442, Final Batch Loss: 0.0015379332471638918\n",
      "Epoch 4947, Loss: 0.012203983598737977, Final Batch Loss: 7.0156020228751e-05\n",
      "Epoch 4948, Loss: 0.013396918016951531, Final Batch Loss: 0.001982705434784293\n",
      "Epoch 4949, Loss: 0.0044172282359795645, Final Batch Loss: 0.0004821504116989672\n",
      "Epoch 4950, Loss: 0.0029426995752146468, Final Batch Loss: 0.00021453849331010133\n",
      "Epoch 4951, Loss: 0.0023252587125170976, Final Batch Loss: 0.00017488688172306865\n",
      "Epoch 4952, Loss: 0.0031628483193344437, Final Batch Loss: 5.288686224957928e-05\n",
      "Epoch 4953, Loss: 0.023345927067566663, Final Batch Loss: 0.013595777563750744\n",
      "Epoch 4954, Loss: 0.005399911548011005, Final Batch Loss: 0.00024094810942187905\n",
      "Epoch 4955, Loss: 0.002342468869755976, Final Batch Loss: 0.00037292265915311873\n",
      "Epoch 4956, Loss: 0.0044325538037810475, Final Batch Loss: 0.00010371176176704466\n",
      "Epoch 4957, Loss: 0.003916271416528616, Final Batch Loss: 7.855210424168035e-05\n",
      "Epoch 4958, Loss: 0.007646176680282224, Final Batch Loss: 0.00011086828453699127\n",
      "Epoch 4959, Loss: 0.011990940874966327, Final Batch Loss: 0.01032313983887434\n",
      "Epoch 4960, Loss: 0.01902646670350805, Final Batch Loss: 0.0012584978248924017\n",
      "Epoch 4961, Loss: 0.027900312663405202, Final Batch Loss: 0.0012644901871681213\n",
      "Epoch 4962, Loss: 0.020322492229752243, Final Batch Loss: 0.00016739859711378813\n",
      "Epoch 4963, Loss: 0.005549070148845203, Final Batch Loss: 0.0004002116620540619\n",
      "Epoch 4964, Loss: 0.002286428847583011, Final Batch Loss: 0.00012121020699851215\n",
      "Epoch 4965, Loss: 0.003355476525030099, Final Batch Loss: 0.0014125251909717917\n",
      "Epoch 4966, Loss: 0.02016984143119771, Final Batch Loss: 0.0028392369858920574\n",
      "Epoch 4967, Loss: 0.027978906815405935, Final Batch Loss: 0.0007794956909492612\n",
      "Epoch 4968, Loss: 0.022346918893163092, Final Batch Loss: 0.010846204124391079\n",
      "Epoch 4969, Loss: 0.019238246692111716, Final Batch Loss: 0.0008614549296908081\n",
      "Epoch 4970, Loss: 0.026646998667274602, Final Batch Loss: 0.0199164766818285\n",
      "Epoch 4971, Loss: 0.03754656079399865, Final Batch Loss: 0.0034433670807629824\n",
      "Epoch 4972, Loss: 0.01580052163626533, Final Batch Loss: 4.2101703002117574e-05\n",
      "Epoch 4973, Loss: 0.008471768560411874, Final Batch Loss: 0.0005287290550768375\n",
      "Epoch 4974, Loss: 0.011062790232244879, Final Batch Loss: 0.0008859118679538369\n",
      "Epoch 4975, Loss: 0.00780870919697918, Final Batch Loss: 0.005064962897449732\n",
      "Epoch 4976, Loss: 0.021223176911007613, Final Batch Loss: 0.000243065194808878\n",
      "Epoch 4977, Loss: 0.004298852276406251, Final Batch Loss: 0.0001543597609270364\n",
      "Epoch 4978, Loss: 0.012879181886091828, Final Batch Loss: 0.00023559236433357\n",
      "Epoch 4979, Loss: 0.03398457221919671, Final Batch Loss: 0.016833025962114334\n",
      "Epoch 4980, Loss: 0.003550305453245528, Final Batch Loss: 0.0013593401527032256\n",
      "Epoch 4981, Loss: 0.01952435987186618, Final Batch Loss: 0.017538001760840416\n",
      "Epoch 4982, Loss: 0.011355856839145417, Final Batch Loss: 0.000696286209858954\n",
      "Epoch 4983, Loss: 0.0358373820054112, Final Batch Loss: 0.00011174466635566205\n",
      "Epoch 4984, Loss: 0.004237341338011902, Final Batch Loss: 4.3937274313066155e-05\n",
      "Epoch 4985, Loss: 0.007718679953541141, Final Batch Loss: 0.00010681931598810479\n",
      "Epoch 4986, Loss: 0.008835969863866922, Final Batch Loss: 7.47619997127913e-05\n",
      "Epoch 4987, Loss: 0.014862984826322645, Final Batch Loss: 0.0006749384920112789\n",
      "Epoch 4988, Loss: 0.0505000677658245, Final Batch Loss: 0.009972590021789074\n",
      "Epoch 4989, Loss: 0.005781533822300844, Final Batch Loss: 9.137517190538347e-05\n",
      "Epoch 4990, Loss: 0.05887992342468351, Final Batch Loss: 0.0014889161102473736\n",
      "Epoch 4991, Loss: 0.08927153916738462, Final Batch Loss: 7.879261102061719e-05\n",
      "Epoch 4992, Loss: 0.018773777559545124, Final Batch Loss: 4.48533428425435e-05\n",
      "Epoch 4993, Loss: 0.037200045539066195, Final Batch Loss: 0.003375382162630558\n",
      "Epoch 4994, Loss: 0.005098750829347409, Final Batch Loss: 0.0017620768630877137\n",
      "Epoch 4995, Loss: 0.032928778004134074, Final Batch Loss: 0.0016626212745904922\n",
      "Epoch 4996, Loss: 0.0046935685677453876, Final Batch Loss: 0.000266812538029626\n",
      "Epoch 4997, Loss: 0.020333460997790098, Final Batch Loss: 0.0024005474988371134\n",
      "Epoch 4998, Loss: 0.025965126988012344, Final Batch Loss: 0.0009181092609651387\n",
      "Epoch 4999, Loss: 0.03116005362244323, Final Batch Loss: 0.0008745173108763993\n",
      "Epoch 5000, Loss: 0.053961562807671726, Final Batch Loss: 0.03027777560055256\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103   5   0]\n",
      " [  4  74   0]\n",
      " [  0   0 101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96262   0.95370   0.95814       108\n",
      "           1    0.93671   0.94872   0.94268        78\n",
      "           2    1.00000   1.00000   1.00000       101\n",
      "\n",
      "    accuracy                        0.96864       287\n",
      "   macro avg    0.96644   0.96747   0.96694       287\n",
      "weighted avg    0.96873   0.96864   0.96867       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=112, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 112)\n",
    "load_model(gen, \"3 Label 9 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 9)\n",
    "\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 98   0   0]\n",
      " [  0 102   0]\n",
      " [  0   0  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        98\n",
      "           1    1.00000   1.00000   1.00000       102\n",
      "           2    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       287\n",
      "   macro avg    1.00000   1.00000   1.00000       287\n",
      "weighted avg    1.00000   1.00000   1.00000       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix(act_vectors[0], preds.cpu()))\n",
    "print(metrics.classification_report(act_vectors[0], preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Subject\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)):\n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    elif y[k] == 5:\n",
    "        y[k] = 2\n",
    "    elif y[k] == 7:\n",
    "        y[k] = 3\n",
    "    elif y[k] == 8:\n",
    "        y[k] = 4\n",
    "    elif y[k] == 11:\n",
    "        y[k] = 5\n",
    "    elif y[k] == 14:\n",
    "        y[k] = 6\n",
    "    elif y[k] == 17:\n",
    "        y[k] = 7\n",
    "    else:\n",
    "        y[k] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model_subject = Subject_Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_subject.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.029923677444458, Final Batch Loss: 2.2061479091644287\n",
      "Epoch 2, Loss: 11.026373386383057, Final Batch Loss: 2.212064743041992\n",
      "Epoch 3, Loss: 11.011316061019897, Final Batch Loss: 2.205458879470825\n",
      "Epoch 4, Loss: 11.000563859939575, Final Batch Loss: 2.201493740081787\n",
      "Epoch 5, Loss: 10.99707555770874, Final Batch Loss: 2.2067110538482666\n",
      "Epoch 6, Loss: 10.98584532737732, Final Batch Loss: 2.2040576934814453\n",
      "Epoch 7, Loss: 10.973849058151245, Final Batch Loss: 2.192683458328247\n",
      "Epoch 8, Loss: 10.96964955329895, Final Batch Loss: 2.1993355751037598\n",
      "Epoch 9, Loss: 10.948185205459595, Final Batch Loss: 2.1899614334106445\n",
      "Epoch 10, Loss: 10.928164005279541, Final Batch Loss: 2.1937594413757324\n",
      "Epoch 11, Loss: 10.900380849838257, Final Batch Loss: 2.1675636768341064\n",
      "Epoch 12, Loss: 10.896092414855957, Final Batch Loss: 2.187126398086548\n",
      "Epoch 13, Loss: 10.855258703231812, Final Batch Loss: 2.1820740699768066\n",
      "Epoch 14, Loss: 10.828618049621582, Final Batch Loss: 2.1737141609191895\n",
      "Epoch 15, Loss: 10.764058828353882, Final Batch Loss: 2.1355230808258057\n",
      "Epoch 16, Loss: 10.725198984146118, Final Batch Loss: 2.1391303539276123\n",
      "Epoch 17, Loss: 10.664380550384521, Final Batch Loss: 2.1266961097717285\n",
      "Epoch 18, Loss: 10.589385271072388, Final Batch Loss: 2.0983266830444336\n",
      "Epoch 19, Loss: 10.54092264175415, Final Batch Loss: 2.1129894256591797\n",
      "Epoch 20, Loss: 10.471168041229248, Final Batch Loss: 2.1063365936279297\n",
      "Epoch 21, Loss: 10.348315715789795, Final Batch Loss: 2.086609363555908\n",
      "Epoch 22, Loss: 10.206669330596924, Final Batch Loss: 2.003141403198242\n",
      "Epoch 23, Loss: 10.153248071670532, Final Batch Loss: 2.0075979232788086\n",
      "Epoch 24, Loss: 10.080048322677612, Final Batch Loss: 2.0429933071136475\n",
      "Epoch 25, Loss: 9.926857471466064, Final Batch Loss: 1.9661582708358765\n",
      "Epoch 26, Loss: 9.822978496551514, Final Batch Loss: 1.9479490518569946\n",
      "Epoch 27, Loss: 9.658382534980774, Final Batch Loss: 1.8578945398330688\n",
      "Epoch 28, Loss: 9.629891395568848, Final Batch Loss: 1.928184986114502\n",
      "Epoch 29, Loss: 9.551017761230469, Final Batch Loss: 1.9097449779510498\n",
      "Epoch 30, Loss: 9.477046608924866, Final Batch Loss: 1.9092305898666382\n",
      "Epoch 31, Loss: 9.388375759124756, Final Batch Loss: 1.9037803411483765\n",
      "Epoch 32, Loss: 9.271621108055115, Final Batch Loss: 1.7812557220458984\n",
      "Epoch 33, Loss: 9.198097467422485, Final Batch Loss: 1.858040690422058\n",
      "Epoch 34, Loss: 9.09514319896698, Final Batch Loss: 1.8121442794799805\n",
      "Epoch 35, Loss: 9.019981861114502, Final Batch Loss: 1.7777901887893677\n",
      "Epoch 36, Loss: 8.910263657569885, Final Batch Loss: 1.7860227823257446\n",
      "Epoch 37, Loss: 8.806069612503052, Final Batch Loss: 1.7655003070831299\n",
      "Epoch 38, Loss: 8.767548084259033, Final Batch Loss: 1.7263195514678955\n",
      "Epoch 39, Loss: 8.81772494316101, Final Batch Loss: 1.8734328746795654\n",
      "Epoch 40, Loss: 8.657849907875061, Final Batch Loss: 1.741433024406433\n",
      "Epoch 41, Loss: 8.515146970748901, Final Batch Loss: 1.671333909034729\n",
      "Epoch 42, Loss: 8.554292798042297, Final Batch Loss: 1.761989951133728\n",
      "Epoch 43, Loss: 8.511890649795532, Final Batch Loss: 1.7282854318618774\n",
      "Epoch 44, Loss: 8.48298168182373, Final Batch Loss: 1.753124713897705\n",
      "Epoch 45, Loss: 8.351794838905334, Final Batch Loss: 1.6651833057403564\n",
      "Epoch 46, Loss: 8.241900205612183, Final Batch Loss: 1.5694971084594727\n",
      "Epoch 47, Loss: 8.290149927139282, Final Batch Loss: 1.6988521814346313\n",
      "Epoch 48, Loss: 8.280431151390076, Final Batch Loss: 1.6575706005096436\n",
      "Epoch 49, Loss: 8.248761773109436, Final Batch Loss: 1.634942889213562\n",
      "Epoch 50, Loss: 8.18239450454712, Final Batch Loss: 1.6901930570602417\n",
      "Epoch 51, Loss: 8.162735104560852, Final Batch Loss: 1.6120262145996094\n",
      "Epoch 52, Loss: 8.066409349441528, Final Batch Loss: 1.6130603551864624\n",
      "Epoch 53, Loss: 8.016796946525574, Final Batch Loss: 1.6262222528457642\n",
      "Epoch 54, Loss: 8.020063877105713, Final Batch Loss: 1.6354187726974487\n",
      "Epoch 55, Loss: 7.909500241279602, Final Batch Loss: 1.6027326583862305\n",
      "Epoch 56, Loss: 7.959166049957275, Final Batch Loss: 1.6768697500228882\n",
      "Epoch 57, Loss: 7.916893839836121, Final Batch Loss: 1.6573083400726318\n",
      "Epoch 58, Loss: 7.736992597579956, Final Batch Loss: 1.507794737815857\n",
      "Epoch 59, Loss: 7.945369839668274, Final Batch Loss: 1.4926490783691406\n",
      "Epoch 60, Loss: 7.851696729660034, Final Batch Loss: 1.5670121908187866\n",
      "Epoch 61, Loss: 7.64564061164856, Final Batch Loss: 1.4464890956878662\n",
      "Epoch 62, Loss: 7.771298885345459, Final Batch Loss: 1.5185527801513672\n",
      "Epoch 63, Loss: 7.685273766517639, Final Batch Loss: 1.600274682044983\n",
      "Epoch 64, Loss: 7.745776534080505, Final Batch Loss: 1.492272138595581\n",
      "Epoch 65, Loss: 7.65233588218689, Final Batch Loss: 1.4688886404037476\n",
      "Epoch 66, Loss: 7.514536142349243, Final Batch Loss: 1.5352548360824585\n",
      "Epoch 67, Loss: 7.538681983947754, Final Batch Loss: 1.5038042068481445\n",
      "Epoch 68, Loss: 7.498725891113281, Final Batch Loss: 1.5463244915008545\n",
      "Epoch 69, Loss: 7.614892840385437, Final Batch Loss: 1.5857317447662354\n",
      "Epoch 70, Loss: 7.425835728645325, Final Batch Loss: 1.4100077152252197\n",
      "Epoch 71, Loss: 7.464313387870789, Final Batch Loss: 1.4819610118865967\n",
      "Epoch 72, Loss: 7.458871126174927, Final Batch Loss: 1.5491338968276978\n",
      "Epoch 73, Loss: 7.377714991569519, Final Batch Loss: 1.4601572751998901\n",
      "Epoch 74, Loss: 7.326828837394714, Final Batch Loss: 1.447361946105957\n",
      "Epoch 75, Loss: 7.514601707458496, Final Batch Loss: 1.537670612335205\n",
      "Epoch 76, Loss: 7.243334770202637, Final Batch Loss: 1.4569551944732666\n",
      "Epoch 77, Loss: 7.208070993423462, Final Batch Loss: 1.3441957235336304\n",
      "Epoch 78, Loss: 7.1367881298065186, Final Batch Loss: 1.3624637126922607\n",
      "Epoch 79, Loss: 7.212673544883728, Final Batch Loss: 1.3770421743392944\n",
      "Epoch 80, Loss: 7.240792632102966, Final Batch Loss: 1.4304232597351074\n",
      "Epoch 81, Loss: 7.073507189750671, Final Batch Loss: 1.3751670122146606\n",
      "Epoch 82, Loss: 7.177858948707581, Final Batch Loss: 1.389987826347351\n",
      "Epoch 83, Loss: 7.2538676261901855, Final Batch Loss: 1.448072075843811\n",
      "Epoch 84, Loss: 7.017922759056091, Final Batch Loss: 1.3753435611724854\n",
      "Epoch 85, Loss: 7.041558504104614, Final Batch Loss: 1.393336534500122\n",
      "Epoch 86, Loss: 7.085016250610352, Final Batch Loss: 1.4755265712738037\n",
      "Epoch 87, Loss: 6.921355366706848, Final Batch Loss: 1.443830132484436\n",
      "Epoch 88, Loss: 6.957022547721863, Final Batch Loss: 1.4286692142486572\n",
      "Epoch 89, Loss: 6.97063684463501, Final Batch Loss: 1.2949461936950684\n",
      "Epoch 90, Loss: 6.874771475791931, Final Batch Loss: 1.4253958463668823\n",
      "Epoch 91, Loss: 6.925502181053162, Final Batch Loss: 1.397633671760559\n",
      "Epoch 92, Loss: 6.782251715660095, Final Batch Loss: 1.3039196729660034\n",
      "Epoch 93, Loss: 6.758202910423279, Final Batch Loss: 1.4047075510025024\n",
      "Epoch 94, Loss: 6.613357782363892, Final Batch Loss: 1.2440764904022217\n",
      "Epoch 95, Loss: 6.72159206867218, Final Batch Loss: 1.4219105243682861\n",
      "Epoch 96, Loss: 6.733079671859741, Final Batch Loss: 1.4362237453460693\n",
      "Epoch 97, Loss: 6.744251132011414, Final Batch Loss: 1.3906925916671753\n",
      "Epoch 98, Loss: 6.59947395324707, Final Batch Loss: 1.3397247791290283\n",
      "Epoch 99, Loss: 6.69143807888031, Final Batch Loss: 1.3574450016021729\n",
      "Epoch 100, Loss: 6.620751976966858, Final Batch Loss: 1.4051940441131592\n",
      "Epoch 101, Loss: 6.605694890022278, Final Batch Loss: 1.260679841041565\n",
      "Epoch 102, Loss: 6.569584846496582, Final Batch Loss: 1.2997806072235107\n",
      "Epoch 103, Loss: 6.648686051368713, Final Batch Loss: 1.410576343536377\n",
      "Epoch 104, Loss: 6.552336692810059, Final Batch Loss: 1.354345440864563\n",
      "Epoch 105, Loss: 6.457289934158325, Final Batch Loss: 1.323954701423645\n",
      "Epoch 106, Loss: 6.386466145515442, Final Batch Loss: 1.2163867950439453\n",
      "Epoch 107, Loss: 6.436655521392822, Final Batch Loss: 1.2390056848526\n",
      "Epoch 108, Loss: 6.45159113407135, Final Batch Loss: 1.3080631494522095\n",
      "Epoch 109, Loss: 6.452121019363403, Final Batch Loss: 1.2764315605163574\n",
      "Epoch 110, Loss: 6.4179394245147705, Final Batch Loss: 1.2233145236968994\n",
      "Epoch 111, Loss: 6.329926133155823, Final Batch Loss: 1.2706700563430786\n",
      "Epoch 112, Loss: 6.204695224761963, Final Batch Loss: 1.120458960533142\n",
      "Epoch 113, Loss: 6.307678937911987, Final Batch Loss: 1.3057823181152344\n",
      "Epoch 114, Loss: 6.31015682220459, Final Batch Loss: 1.1688954830169678\n",
      "Epoch 115, Loss: 6.174683690071106, Final Batch Loss: 1.2616209983825684\n",
      "Epoch 116, Loss: 6.315993428230286, Final Batch Loss: 1.3066118955612183\n",
      "Epoch 117, Loss: 6.182917356491089, Final Batch Loss: 1.2578775882720947\n",
      "Epoch 118, Loss: 6.210555076599121, Final Batch Loss: 1.1371374130249023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119, Loss: 6.148095965385437, Final Batch Loss: 1.2417149543762207\n",
      "Epoch 120, Loss: 5.8907310962677, Final Batch Loss: 1.033727765083313\n",
      "Epoch 121, Loss: 6.255754113197327, Final Batch Loss: 1.245781660079956\n",
      "Epoch 122, Loss: 6.166225552558899, Final Batch Loss: 1.3318817615509033\n",
      "Epoch 123, Loss: 6.093024134635925, Final Batch Loss: 1.2739191055297852\n",
      "Epoch 124, Loss: 6.117876291275024, Final Batch Loss: 1.2194877862930298\n",
      "Epoch 125, Loss: 6.210393190383911, Final Batch Loss: 1.2598100900650024\n",
      "Epoch 126, Loss: 6.088076949119568, Final Batch Loss: 1.3355823755264282\n",
      "Epoch 127, Loss: 6.020406723022461, Final Batch Loss: 1.2863435745239258\n",
      "Epoch 128, Loss: 5.876379013061523, Final Batch Loss: 1.1141349077224731\n",
      "Epoch 129, Loss: 5.8776432275772095, Final Batch Loss: 1.108117938041687\n",
      "Epoch 130, Loss: 6.010250926017761, Final Batch Loss: 1.2694780826568604\n",
      "Epoch 131, Loss: 5.813632607460022, Final Batch Loss: 1.1337063312530518\n",
      "Epoch 132, Loss: 5.863118290901184, Final Batch Loss: 1.215782880783081\n",
      "Epoch 133, Loss: 6.104582905769348, Final Batch Loss: 1.354395866394043\n",
      "Epoch 134, Loss: 5.898091554641724, Final Batch Loss: 1.2284812927246094\n",
      "Epoch 135, Loss: 5.756945490837097, Final Batch Loss: 1.1579867601394653\n",
      "Epoch 136, Loss: 5.8734705448150635, Final Batch Loss: 1.1242254972457886\n",
      "Epoch 137, Loss: 5.99893593788147, Final Batch Loss: 1.249098300933838\n",
      "Epoch 138, Loss: 5.926338791847229, Final Batch Loss: 1.113771915435791\n",
      "Epoch 139, Loss: 5.669742047786713, Final Batch Loss: 0.932830274105072\n",
      "Epoch 140, Loss: 5.726025700569153, Final Batch Loss: 1.101884365081787\n",
      "Epoch 141, Loss: 5.769559144973755, Final Batch Loss: 1.1588833332061768\n",
      "Epoch 142, Loss: 5.7259968519210815, Final Batch Loss: 1.1888924837112427\n",
      "Epoch 143, Loss: 5.843780517578125, Final Batch Loss: 1.2121604681015015\n",
      "Epoch 144, Loss: 5.754171013832092, Final Batch Loss: 1.218454122543335\n",
      "Epoch 145, Loss: 5.734877347946167, Final Batch Loss: 1.1180474758148193\n",
      "Epoch 146, Loss: 5.789749026298523, Final Batch Loss: 1.1781988143920898\n",
      "Epoch 147, Loss: 5.716837048530579, Final Batch Loss: 1.2081298828125\n",
      "Epoch 148, Loss: 5.630818843841553, Final Batch Loss: 1.1029775142669678\n",
      "Epoch 149, Loss: 5.722625732421875, Final Batch Loss: 1.1379598379135132\n",
      "Epoch 150, Loss: 5.618424415588379, Final Batch Loss: 1.051311731338501\n",
      "Epoch 151, Loss: 5.589250206947327, Final Batch Loss: 1.1545202732086182\n",
      "Epoch 152, Loss: 5.621806859970093, Final Batch Loss: 1.227691650390625\n",
      "Epoch 153, Loss: 5.718824982643127, Final Batch Loss: 1.2058826684951782\n",
      "Epoch 154, Loss: 5.6981648206710815, Final Batch Loss: 1.1144514083862305\n",
      "Epoch 155, Loss: 5.730415105819702, Final Batch Loss: 1.170688509941101\n",
      "Epoch 156, Loss: 5.624608159065247, Final Batch Loss: 1.115517258644104\n",
      "Epoch 157, Loss: 5.448444366455078, Final Batch Loss: 1.0867574214935303\n",
      "Epoch 158, Loss: 5.530061721801758, Final Batch Loss: 1.0486875772476196\n",
      "Epoch 159, Loss: 5.574219584465027, Final Batch Loss: 1.1213141679763794\n",
      "Epoch 160, Loss: 5.733066439628601, Final Batch Loss: 1.1908947229385376\n",
      "Epoch 161, Loss: 5.640155792236328, Final Batch Loss: 1.239955186843872\n",
      "Epoch 162, Loss: 5.466818690299988, Final Batch Loss: 1.1923471689224243\n",
      "Epoch 163, Loss: 5.397450685501099, Final Batch Loss: 1.006996989250183\n",
      "Epoch 164, Loss: 5.521705806255341, Final Batch Loss: 1.225122094154358\n",
      "Epoch 165, Loss: 5.553408622741699, Final Batch Loss: 1.0932652950286865\n",
      "Epoch 166, Loss: 5.602852940559387, Final Batch Loss: 1.0682793855667114\n",
      "Epoch 167, Loss: 5.56930410861969, Final Batch Loss: 1.0953940153121948\n",
      "Epoch 168, Loss: 5.319804549217224, Final Batch Loss: 0.9682139158248901\n",
      "Epoch 169, Loss: 5.428093671798706, Final Batch Loss: 1.1011509895324707\n",
      "Epoch 170, Loss: 5.498680830001831, Final Batch Loss: 1.0707156658172607\n",
      "Epoch 171, Loss: 5.371345639228821, Final Batch Loss: 1.0009362697601318\n",
      "Epoch 172, Loss: 5.570647835731506, Final Batch Loss: 1.1703921556472778\n",
      "Epoch 173, Loss: 5.4227253794670105, Final Batch Loss: 1.08315908908844\n",
      "Epoch 174, Loss: 5.499170422554016, Final Batch Loss: 1.1129014492034912\n",
      "Epoch 175, Loss: 5.376934051513672, Final Batch Loss: 1.2007272243499756\n",
      "Epoch 176, Loss: 5.3184069991111755, Final Batch Loss: 1.0487029552459717\n",
      "Epoch 177, Loss: 5.513532996177673, Final Batch Loss: 1.0800964832305908\n",
      "Epoch 178, Loss: 5.366192281246185, Final Batch Loss: 1.0519976615905762\n",
      "Epoch 179, Loss: 5.4444204568862915, Final Batch Loss: 1.0950891971588135\n",
      "Epoch 180, Loss: 5.306918561458588, Final Batch Loss: 0.9548467993736267\n",
      "Epoch 181, Loss: 5.376610636711121, Final Batch Loss: 1.120930790901184\n",
      "Epoch 182, Loss: 5.445802092552185, Final Batch Loss: 1.1314324140548706\n",
      "Epoch 183, Loss: 5.37078070640564, Final Batch Loss: 1.0567086935043335\n",
      "Epoch 184, Loss: 5.182776689529419, Final Batch Loss: 0.9383140802383423\n",
      "Epoch 185, Loss: 5.429033398628235, Final Batch Loss: 1.0695056915283203\n",
      "Epoch 186, Loss: 5.181518495082855, Final Batch Loss: 0.923923671245575\n",
      "Epoch 187, Loss: 5.4178608655929565, Final Batch Loss: 1.0955272912979126\n",
      "Epoch 188, Loss: 5.341215133666992, Final Batch Loss: 1.1207014322280884\n",
      "Epoch 189, Loss: 5.377818584442139, Final Batch Loss: 1.1015214920043945\n",
      "Epoch 190, Loss: 5.227284729480743, Final Batch Loss: 0.9999144673347473\n",
      "Epoch 191, Loss: 5.372165322303772, Final Batch Loss: 1.0851186513900757\n",
      "Epoch 192, Loss: 5.367988705635071, Final Batch Loss: 0.9861924648284912\n",
      "Epoch 193, Loss: 5.23426741361618, Final Batch Loss: 0.9933648705482483\n",
      "Epoch 194, Loss: 5.267180919647217, Final Batch Loss: 0.8951156139373779\n",
      "Epoch 195, Loss: 5.370071232318878, Final Batch Loss: 1.1266411542892456\n",
      "Epoch 196, Loss: 5.196178138256073, Final Batch Loss: 0.9092493653297424\n",
      "Epoch 197, Loss: 5.266535997390747, Final Batch Loss: 1.015608310699463\n",
      "Epoch 198, Loss: 5.184460163116455, Final Batch Loss: 1.1319506168365479\n",
      "Epoch 199, Loss: 5.157133102416992, Final Batch Loss: 1.0022151470184326\n",
      "Epoch 200, Loss: 5.352806985378265, Final Batch Loss: 1.043171763420105\n",
      "Epoch 201, Loss: 5.185807824134827, Final Batch Loss: 1.1024436950683594\n",
      "Epoch 202, Loss: 5.314133524894714, Final Batch Loss: 1.062803864479065\n",
      "Epoch 203, Loss: 5.263457536697388, Final Batch Loss: 1.048693299293518\n",
      "Epoch 204, Loss: 5.097753286361694, Final Batch Loss: 0.9498085975646973\n",
      "Epoch 205, Loss: 5.407466173171997, Final Batch Loss: 1.1568920612335205\n",
      "Epoch 206, Loss: 5.348921060562134, Final Batch Loss: 1.1430860757827759\n",
      "Epoch 207, Loss: 5.219464898109436, Final Batch Loss: 1.0125404596328735\n",
      "Epoch 208, Loss: 5.143166780471802, Final Batch Loss: 1.0819134712219238\n",
      "Epoch 209, Loss: 5.148403227329254, Final Batch Loss: 0.9280686974525452\n",
      "Epoch 210, Loss: 5.169784665107727, Final Batch Loss: 1.0464751720428467\n",
      "Epoch 211, Loss: 5.313423156738281, Final Batch Loss: 1.1318668127059937\n",
      "Epoch 212, Loss: 5.2575294971466064, Final Batch Loss: 1.170942783355713\n",
      "Epoch 213, Loss: 4.998377084732056, Final Batch Loss: 0.8750404119491577\n",
      "Epoch 214, Loss: 5.07074761390686, Final Batch Loss: 1.0690828561782837\n",
      "Epoch 215, Loss: 5.071209669113159, Final Batch Loss: 0.9950921535491943\n",
      "Epoch 216, Loss: 5.113621413707733, Final Batch Loss: 1.0056878328323364\n",
      "Epoch 217, Loss: 5.12534773349762, Final Batch Loss: 1.0122637748718262\n",
      "Epoch 218, Loss: 5.22961962223053, Final Batch Loss: 1.0104535818099976\n",
      "Epoch 219, Loss: 5.008642733097076, Final Batch Loss: 1.0214513540267944\n",
      "Epoch 220, Loss: 5.180081725120544, Final Batch Loss: 1.0940470695495605\n",
      "Epoch 221, Loss: 5.066404819488525, Final Batch Loss: 1.0592336654663086\n",
      "Epoch 222, Loss: 5.066692233085632, Final Batch Loss: 1.0840827226638794\n",
      "Epoch 223, Loss: 5.060157358646393, Final Batch Loss: 0.9945067167282104\n",
      "Epoch 224, Loss: 5.060113608837128, Final Batch Loss: 1.015504002571106\n",
      "Epoch 225, Loss: 4.995359301567078, Final Batch Loss: 1.0276862382888794\n",
      "Epoch 226, Loss: 5.014124155044556, Final Batch Loss: 0.994864821434021\n",
      "Epoch 227, Loss: 5.111299395561218, Final Batch Loss: 1.0578089952468872\n",
      "Epoch 228, Loss: 4.915282309055328, Final Batch Loss: 1.0168755054473877\n",
      "Epoch 229, Loss: 4.927045583724976, Final Batch Loss: 0.9432858228683472\n",
      "Epoch 230, Loss: 5.019000589847565, Final Batch Loss: 1.0350803136825562\n",
      "Epoch 231, Loss: 5.039967954158783, Final Batch Loss: 1.1103488206863403\n",
      "Epoch 232, Loss: 4.883832156658173, Final Batch Loss: 0.9198084473609924\n",
      "Epoch 233, Loss: 4.907814145088196, Final Batch Loss: 0.8813263177871704\n",
      "Epoch 234, Loss: 4.854023575782776, Final Batch Loss: 0.9148576259613037\n",
      "Epoch 235, Loss: 4.950231730937958, Final Batch Loss: 1.0588568449020386\n",
      "Epoch 236, Loss: 4.975448727607727, Final Batch Loss: 1.0487781763076782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237, Loss: 4.96855366230011, Final Batch Loss: 1.0254898071289062\n",
      "Epoch 238, Loss: 5.044667363166809, Final Batch Loss: 0.925557017326355\n",
      "Epoch 239, Loss: 4.921505630016327, Final Batch Loss: 1.0204198360443115\n",
      "Epoch 240, Loss: 4.898784637451172, Final Batch Loss: 0.9598411917686462\n",
      "Epoch 241, Loss: 4.95870840549469, Final Batch Loss: 1.0821268558502197\n",
      "Epoch 242, Loss: 4.833668768405914, Final Batch Loss: 1.0033615827560425\n",
      "Epoch 243, Loss: 5.021805047988892, Final Batch Loss: 1.1048790216445923\n",
      "Epoch 244, Loss: 4.9979424476623535, Final Batch Loss: 1.0822222232818604\n",
      "Epoch 245, Loss: 5.009774148464203, Final Batch Loss: 0.996688187122345\n",
      "Epoch 246, Loss: 4.952150464057922, Final Batch Loss: 0.9759770631790161\n",
      "Epoch 247, Loss: 5.0021472573280334, Final Batch Loss: 0.9287590384483337\n",
      "Epoch 248, Loss: 4.842446029186249, Final Batch Loss: 1.1224747896194458\n",
      "Epoch 249, Loss: 4.961311638355255, Final Batch Loss: 1.002474069595337\n",
      "Epoch 250, Loss: 4.926486134529114, Final Batch Loss: 0.9744153022766113\n",
      "Epoch 251, Loss: 4.779389202594757, Final Batch Loss: 0.9175652861595154\n",
      "Epoch 252, Loss: 4.8055466413497925, Final Batch Loss: 0.9446180462837219\n",
      "Epoch 253, Loss: 4.910006463527679, Final Batch Loss: 0.9738650321960449\n",
      "Epoch 254, Loss: 5.018835544586182, Final Batch Loss: 1.0165865421295166\n",
      "Epoch 255, Loss: 4.971559643745422, Final Batch Loss: 0.8966839909553528\n",
      "Epoch 256, Loss: 4.993730068206787, Final Batch Loss: 0.9604714512825012\n",
      "Epoch 257, Loss: 4.794389605522156, Final Batch Loss: 0.9849006533622742\n",
      "Epoch 258, Loss: 4.894684851169586, Final Batch Loss: 0.9618553519248962\n",
      "Epoch 259, Loss: 4.889437317848206, Final Batch Loss: 0.93321293592453\n",
      "Epoch 260, Loss: 4.763421416282654, Final Batch Loss: 0.909004271030426\n",
      "Epoch 261, Loss: 4.816931068897247, Final Batch Loss: 0.9137361645698547\n",
      "Epoch 262, Loss: 4.899701178073883, Final Batch Loss: 0.9183123111724854\n",
      "Epoch 263, Loss: 4.916063725948334, Final Batch Loss: 0.9753690361976624\n",
      "Epoch 264, Loss: 4.802178740501404, Final Batch Loss: 0.9415230751037598\n",
      "Epoch 265, Loss: 4.722431659698486, Final Batch Loss: 0.8301283717155457\n",
      "Epoch 266, Loss: 4.7733635902404785, Final Batch Loss: 1.002504825592041\n",
      "Epoch 267, Loss: 4.701494514942169, Final Batch Loss: 0.8382186889648438\n",
      "Epoch 268, Loss: 4.686440050601959, Final Batch Loss: 0.8819571137428284\n",
      "Epoch 269, Loss: 4.742809951305389, Final Batch Loss: 0.9259811639785767\n",
      "Epoch 270, Loss: 4.803907573223114, Final Batch Loss: 0.8139848113059998\n",
      "Epoch 271, Loss: 4.77325838804245, Final Batch Loss: 0.9250966310501099\n",
      "Epoch 272, Loss: 4.82579642534256, Final Batch Loss: 0.9915080070495605\n",
      "Epoch 273, Loss: 4.761289894580841, Final Batch Loss: 0.8806139826774597\n",
      "Epoch 274, Loss: 4.703307926654816, Final Batch Loss: 0.8833041191101074\n",
      "Epoch 275, Loss: 4.8037150502204895, Final Batch Loss: 0.9931633472442627\n",
      "Epoch 276, Loss: 4.8731818199157715, Final Batch Loss: 0.9766032099723816\n",
      "Epoch 277, Loss: 4.666914939880371, Final Batch Loss: 1.0099079608917236\n",
      "Epoch 278, Loss: 4.818185210227966, Final Batch Loss: 0.9480597972869873\n",
      "Epoch 279, Loss: 4.777458548545837, Final Batch Loss: 0.9360383152961731\n",
      "Epoch 280, Loss: 4.932905077934265, Final Batch Loss: 0.9003767371177673\n",
      "Epoch 281, Loss: 4.873827040195465, Final Batch Loss: 0.9444122314453125\n",
      "Epoch 282, Loss: 4.639766216278076, Final Batch Loss: 0.9680531620979309\n",
      "Epoch 283, Loss: 4.592650651931763, Final Batch Loss: 0.7959598302841187\n",
      "Epoch 284, Loss: 4.825677216053009, Final Batch Loss: 0.9610148668289185\n",
      "Epoch 285, Loss: 4.58096307516098, Final Batch Loss: 0.9431880712509155\n",
      "Epoch 286, Loss: 4.751708388328552, Final Batch Loss: 0.9939588904380798\n",
      "Epoch 287, Loss: 4.757564187049866, Final Batch Loss: 0.8896710872650146\n",
      "Epoch 288, Loss: 4.772281169891357, Final Batch Loss: 0.9602099061012268\n",
      "Epoch 289, Loss: 4.79000997543335, Final Batch Loss: 1.0556813478469849\n",
      "Epoch 290, Loss: 4.8292595744133, Final Batch Loss: 0.9977513551712036\n",
      "Epoch 291, Loss: 4.754705131053925, Final Batch Loss: 0.8888911008834839\n",
      "Epoch 292, Loss: 4.6985689997673035, Final Batch Loss: 0.9282143712043762\n",
      "Epoch 293, Loss: 4.647056341171265, Final Batch Loss: 0.8779319524765015\n",
      "Epoch 294, Loss: 4.734376788139343, Final Batch Loss: 0.9725802540779114\n",
      "Epoch 295, Loss: 4.647538542747498, Final Batch Loss: 0.8425412178039551\n",
      "Epoch 296, Loss: 4.655845999717712, Final Batch Loss: 0.9028633832931519\n",
      "Epoch 297, Loss: 4.604250133037567, Final Batch Loss: 0.9719954133033752\n",
      "Epoch 298, Loss: 4.615947723388672, Final Batch Loss: 0.9142197370529175\n",
      "Epoch 299, Loss: 4.822246611118317, Final Batch Loss: 1.053206443786621\n",
      "Epoch 300, Loss: 4.535555422306061, Final Batch Loss: 0.9164978265762329\n",
      "Epoch 301, Loss: 4.861986577510834, Final Batch Loss: 1.0212326049804688\n",
      "Epoch 302, Loss: 4.65942919254303, Final Batch Loss: 0.9132631421089172\n",
      "Epoch 303, Loss: 4.673756182193756, Final Batch Loss: 0.965707004070282\n",
      "Epoch 304, Loss: 4.597307026386261, Final Batch Loss: 0.9645999073982239\n",
      "Epoch 305, Loss: 4.603016197681427, Final Batch Loss: 0.9848442077636719\n",
      "Epoch 306, Loss: 4.655572056770325, Final Batch Loss: 1.0107742547988892\n",
      "Epoch 307, Loss: 4.811174988746643, Final Batch Loss: 1.0004709959030151\n",
      "Epoch 308, Loss: 4.757974922657013, Final Batch Loss: 1.0106911659240723\n",
      "Epoch 309, Loss: 4.70100200176239, Final Batch Loss: 0.9763594269752502\n",
      "Epoch 310, Loss: 4.626389563083649, Final Batch Loss: 0.9998404383659363\n",
      "Epoch 311, Loss: 4.562318861484528, Final Batch Loss: 0.8846136927604675\n",
      "Epoch 312, Loss: 4.527369797229767, Final Batch Loss: 0.774369478225708\n",
      "Epoch 313, Loss: 4.632767617702484, Final Batch Loss: 0.9772228598594666\n",
      "Epoch 314, Loss: 4.755742609500885, Final Batch Loss: 1.075658917427063\n",
      "Epoch 315, Loss: 4.486407935619354, Final Batch Loss: 1.0032895803451538\n",
      "Epoch 316, Loss: 4.580017566680908, Final Batch Loss: 0.940528392791748\n",
      "Epoch 317, Loss: 4.628737092018127, Final Batch Loss: 0.8859156370162964\n",
      "Epoch 318, Loss: 4.563912451267242, Final Batch Loss: 0.9109395146369934\n",
      "Epoch 319, Loss: 4.519316077232361, Final Batch Loss: 0.8240613341331482\n",
      "Epoch 320, Loss: 4.370933473110199, Final Batch Loss: 0.7208155393600464\n",
      "Epoch 321, Loss: 4.568670332431793, Final Batch Loss: 0.8302661776542664\n",
      "Epoch 322, Loss: 4.656557619571686, Final Batch Loss: 0.9382579922676086\n",
      "Epoch 323, Loss: 4.561448276042938, Final Batch Loss: 0.9783685207366943\n",
      "Epoch 324, Loss: 4.427648067474365, Final Batch Loss: 0.9652998447418213\n",
      "Epoch 325, Loss: 4.565319895744324, Final Batch Loss: 0.8978223204612732\n",
      "Epoch 326, Loss: 4.599070370197296, Final Batch Loss: 0.9570804834365845\n",
      "Epoch 327, Loss: 4.475727379322052, Final Batch Loss: 0.8331199884414673\n",
      "Epoch 328, Loss: 4.574248135089874, Final Batch Loss: 0.9135628342628479\n",
      "Epoch 329, Loss: 4.6151538491249084, Final Batch Loss: 0.9639164209365845\n",
      "Epoch 330, Loss: 4.443939387798309, Final Batch Loss: 0.8686144351959229\n",
      "Epoch 331, Loss: 4.601336419582367, Final Batch Loss: 1.0240631103515625\n",
      "Epoch 332, Loss: 4.615465879440308, Final Batch Loss: 1.062848448753357\n",
      "Epoch 333, Loss: 4.434252917766571, Final Batch Loss: 0.8673788905143738\n",
      "Epoch 334, Loss: 4.520766496658325, Final Batch Loss: 0.7633830308914185\n",
      "Epoch 335, Loss: 4.6713438630104065, Final Batch Loss: 1.0057390928268433\n",
      "Epoch 336, Loss: 4.520003378391266, Final Batch Loss: 0.8279767632484436\n",
      "Epoch 337, Loss: 4.501157283782959, Final Batch Loss: 0.8682194352149963\n",
      "Epoch 338, Loss: 4.6304104924201965, Final Batch Loss: 1.1295034885406494\n",
      "Epoch 339, Loss: 4.394677639007568, Final Batch Loss: 0.7848153710365295\n",
      "Epoch 340, Loss: 4.500899851322174, Final Batch Loss: 0.9916420578956604\n",
      "Epoch 341, Loss: 4.42570298910141, Final Batch Loss: 0.8537149429321289\n",
      "Epoch 342, Loss: 4.51867538690567, Final Batch Loss: 0.7669568657875061\n",
      "Epoch 343, Loss: 4.46081268787384, Final Batch Loss: 0.7760528922080994\n",
      "Epoch 344, Loss: 4.384140968322754, Final Batch Loss: 0.8540534973144531\n",
      "Epoch 345, Loss: 4.461038827896118, Final Batch Loss: 0.9271572828292847\n",
      "Epoch 346, Loss: 4.545717656612396, Final Batch Loss: 0.7853701114654541\n",
      "Epoch 347, Loss: 4.536014556884766, Final Batch Loss: 0.7980183959007263\n",
      "Epoch 348, Loss: 4.479512810707092, Final Batch Loss: 0.827707827091217\n",
      "Epoch 349, Loss: 4.427420377731323, Final Batch Loss: 0.7847665548324585\n",
      "Epoch 350, Loss: 4.608613312244415, Final Batch Loss: 0.8216670155525208\n",
      "Epoch 351, Loss: 4.500993072986603, Final Batch Loss: 0.8388786315917969\n",
      "Epoch 352, Loss: 4.640222072601318, Final Batch Loss: 0.9648532271385193\n",
      "Epoch 353, Loss: 4.372086763381958, Final Batch Loss: 0.8680829405784607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354, Loss: 4.449796915054321, Final Batch Loss: 0.8988725543022156\n",
      "Epoch 355, Loss: 4.541412055492401, Final Batch Loss: 1.1225837469100952\n",
      "Epoch 356, Loss: 4.331856191158295, Final Batch Loss: 0.8218727111816406\n",
      "Epoch 357, Loss: 4.49927294254303, Final Batch Loss: 0.8433527946472168\n",
      "Epoch 358, Loss: 4.453995406627655, Final Batch Loss: 0.79062420129776\n",
      "Epoch 359, Loss: 4.42147421836853, Final Batch Loss: 0.8790539503097534\n",
      "Epoch 360, Loss: 4.560124337673187, Final Batch Loss: 0.9830459356307983\n",
      "Epoch 361, Loss: 4.48860102891922, Final Batch Loss: 0.9694207310676575\n",
      "Epoch 362, Loss: 4.378101289272308, Final Batch Loss: 0.9230786561965942\n",
      "Epoch 363, Loss: 4.416515529155731, Final Batch Loss: 0.8380051851272583\n",
      "Epoch 364, Loss: 4.568311154842377, Final Batch Loss: 1.0252679586410522\n",
      "Epoch 365, Loss: 4.572549879550934, Final Batch Loss: 1.0370041131973267\n",
      "Epoch 366, Loss: 4.4405728578567505, Final Batch Loss: 0.8322241902351379\n",
      "Epoch 367, Loss: 4.385668933391571, Final Batch Loss: 0.8946751952171326\n",
      "Epoch 368, Loss: 4.299704849720001, Final Batch Loss: 0.7953892946243286\n",
      "Epoch 369, Loss: 4.2910173535346985, Final Batch Loss: 0.8388438820838928\n",
      "Epoch 370, Loss: 4.448835015296936, Final Batch Loss: 0.8948534727096558\n",
      "Epoch 371, Loss: 4.165215075016022, Final Batch Loss: 0.8416589498519897\n",
      "Epoch 372, Loss: 4.202781677246094, Final Batch Loss: 0.894740641117096\n",
      "Epoch 373, Loss: 4.255027711391449, Final Batch Loss: 0.8343167901039124\n",
      "Epoch 374, Loss: 4.4162986278533936, Final Batch Loss: 0.817550003528595\n",
      "Epoch 375, Loss: 4.375748097896576, Final Batch Loss: 0.8315727114677429\n",
      "Epoch 376, Loss: 4.2978702783584595, Final Batch Loss: 0.8753583431243896\n",
      "Epoch 377, Loss: 4.551189303398132, Final Batch Loss: 0.9368104934692383\n",
      "Epoch 378, Loss: 4.395912766456604, Final Batch Loss: 0.9101033210754395\n",
      "Epoch 379, Loss: 4.345558822154999, Final Batch Loss: 0.9651129245758057\n",
      "Epoch 380, Loss: 4.4187814593315125, Final Batch Loss: 0.967979907989502\n",
      "Epoch 381, Loss: 4.514177203178406, Final Batch Loss: 0.9943557381629944\n",
      "Epoch 382, Loss: 4.448435723781586, Final Batch Loss: 0.932961642742157\n",
      "Epoch 383, Loss: 4.329213976860046, Final Batch Loss: 0.7406828999519348\n",
      "Epoch 384, Loss: 4.1772602796554565, Final Batch Loss: 0.7273937463760376\n",
      "Epoch 385, Loss: 4.38085263967514, Final Batch Loss: 0.9802690148353577\n",
      "Epoch 386, Loss: 4.560858368873596, Final Batch Loss: 0.9519690275192261\n",
      "Epoch 387, Loss: 4.415162026882172, Final Batch Loss: 0.9311857223510742\n",
      "Epoch 388, Loss: 4.381474554538727, Final Batch Loss: 0.9141533970832825\n",
      "Epoch 389, Loss: 4.284217536449432, Final Batch Loss: 0.8063020706176758\n",
      "Epoch 390, Loss: 4.356988608837128, Final Batch Loss: 0.7606821656227112\n",
      "Epoch 391, Loss: 4.26078474521637, Final Batch Loss: 0.7833095192909241\n",
      "Epoch 392, Loss: 4.256115019321442, Final Batch Loss: 0.8827937841415405\n",
      "Epoch 393, Loss: 4.295916497707367, Final Batch Loss: 0.8604541420936584\n",
      "Epoch 394, Loss: 4.182806313037872, Final Batch Loss: 0.8467804193496704\n",
      "Epoch 395, Loss: 4.402666747570038, Final Batch Loss: 1.0559247732162476\n",
      "Epoch 396, Loss: 4.253493547439575, Final Batch Loss: 0.9148328900337219\n",
      "Epoch 397, Loss: 4.243215918540955, Final Batch Loss: 0.8186817765235901\n",
      "Epoch 398, Loss: 4.230098009109497, Final Batch Loss: 0.8428642749786377\n",
      "Epoch 399, Loss: 4.3554046750068665, Final Batch Loss: 0.8622182011604309\n",
      "Epoch 400, Loss: 4.260293066501617, Final Batch Loss: 0.8494192361831665\n",
      "Epoch 401, Loss: 4.318500876426697, Final Batch Loss: 0.8376684188842773\n",
      "Epoch 402, Loss: 4.329756796360016, Final Batch Loss: 0.8742336630821228\n",
      "Epoch 403, Loss: 4.347291827201843, Final Batch Loss: 1.0587329864501953\n",
      "Epoch 404, Loss: 4.356853902339935, Final Batch Loss: 0.7806217670440674\n",
      "Epoch 405, Loss: 4.3555832505226135, Final Batch Loss: 0.88203364610672\n",
      "Epoch 406, Loss: 4.2029324769973755, Final Batch Loss: 0.8211795091629028\n",
      "Epoch 407, Loss: 4.131807267665863, Final Batch Loss: 0.8343629240989685\n",
      "Epoch 408, Loss: 4.28925621509552, Final Batch Loss: 0.8435542583465576\n",
      "Epoch 409, Loss: 4.202454388141632, Final Batch Loss: 0.8353830575942993\n",
      "Epoch 410, Loss: 4.332318961620331, Final Batch Loss: 0.842272162437439\n",
      "Epoch 411, Loss: 4.24431449174881, Final Batch Loss: 0.8268725872039795\n",
      "Epoch 412, Loss: 4.239224016666412, Final Batch Loss: 0.9073460102081299\n",
      "Epoch 413, Loss: 4.223982870578766, Final Batch Loss: 0.8037724494934082\n",
      "Epoch 414, Loss: 4.327175319194794, Final Batch Loss: 0.8175588846206665\n",
      "Epoch 415, Loss: 4.0950122475624084, Final Batch Loss: 0.7594528198242188\n",
      "Epoch 416, Loss: 4.277847170829773, Final Batch Loss: 0.8465849757194519\n",
      "Epoch 417, Loss: 4.1600621342659, Final Batch Loss: 0.8339547514915466\n",
      "Epoch 418, Loss: 4.217322587966919, Final Batch Loss: 0.8503149747848511\n",
      "Epoch 419, Loss: 4.251327335834503, Final Batch Loss: 0.9330950379371643\n",
      "Epoch 420, Loss: 4.34024977684021, Final Batch Loss: 0.8951089978218079\n",
      "Epoch 421, Loss: 4.1311463713645935, Final Batch Loss: 0.8620628118515015\n",
      "Epoch 422, Loss: 4.176163077354431, Final Batch Loss: 0.8699509501457214\n",
      "Epoch 423, Loss: 4.258461117744446, Final Batch Loss: 0.8370984792709351\n",
      "Epoch 424, Loss: 4.253829061985016, Final Batch Loss: 0.8624566793441772\n",
      "Epoch 425, Loss: 4.1507954597473145, Final Batch Loss: 0.8158742785453796\n",
      "Epoch 426, Loss: 4.2292503118515015, Final Batch Loss: 0.8640640377998352\n",
      "Epoch 427, Loss: 4.284005105495453, Final Batch Loss: 0.9162076711654663\n",
      "Epoch 428, Loss: 4.266773700714111, Final Batch Loss: 0.8034324645996094\n",
      "Epoch 429, Loss: 4.247920572757721, Final Batch Loss: 0.9762705564498901\n",
      "Epoch 430, Loss: 4.488468110561371, Final Batch Loss: 0.8770407438278198\n",
      "Epoch 431, Loss: 4.320461094379425, Final Batch Loss: 0.8857176899909973\n",
      "Epoch 432, Loss: 4.276015520095825, Final Batch Loss: 0.8613312840461731\n",
      "Epoch 433, Loss: 4.230855107307434, Final Batch Loss: 0.9392040371894836\n",
      "Epoch 434, Loss: 4.21072244644165, Final Batch Loss: 0.9010953903198242\n",
      "Epoch 435, Loss: 4.27716201543808, Final Batch Loss: 0.7401494383811951\n",
      "Epoch 436, Loss: 4.315861940383911, Final Batch Loss: 0.7616955041885376\n",
      "Epoch 437, Loss: 4.154947221279144, Final Batch Loss: 0.8517104387283325\n",
      "Epoch 438, Loss: 4.201820552349091, Final Batch Loss: 0.7706774473190308\n",
      "Epoch 439, Loss: 4.051070868968964, Final Batch Loss: 0.75422602891922\n",
      "Epoch 440, Loss: 4.07940000295639, Final Batch Loss: 0.7661773562431335\n",
      "Epoch 441, Loss: 3.945110261440277, Final Batch Loss: 0.7041691541671753\n",
      "Epoch 442, Loss: 4.203249633312225, Final Batch Loss: 0.9030819535255432\n",
      "Epoch 443, Loss: 4.348400354385376, Final Batch Loss: 1.0487693548202515\n",
      "Epoch 444, Loss: 4.054969608783722, Final Batch Loss: 0.7358263731002808\n",
      "Epoch 445, Loss: 4.224471211433411, Final Batch Loss: 0.7668452262878418\n",
      "Epoch 446, Loss: 4.298940539360046, Final Batch Loss: 0.8916786909103394\n",
      "Epoch 447, Loss: 4.044378280639648, Final Batch Loss: 0.7297186851501465\n",
      "Epoch 448, Loss: 4.2325319647789, Final Batch Loss: 0.7131516933441162\n",
      "Epoch 449, Loss: 4.255350589752197, Final Batch Loss: 0.9514630436897278\n",
      "Epoch 450, Loss: 4.213672876358032, Final Batch Loss: 0.7742903232574463\n",
      "Epoch 451, Loss: 4.0030882358551025, Final Batch Loss: 0.8523096442222595\n",
      "Epoch 452, Loss: 4.0165510177612305, Final Batch Loss: 0.8142890930175781\n",
      "Epoch 453, Loss: 4.176962733268738, Final Batch Loss: 0.8571508526802063\n",
      "Epoch 454, Loss: 3.982331335544586, Final Batch Loss: 0.7001591920852661\n",
      "Epoch 455, Loss: 4.175659000873566, Final Batch Loss: 0.8477566838264465\n",
      "Epoch 456, Loss: 4.154385209083557, Final Batch Loss: 0.9137992858886719\n",
      "Epoch 457, Loss: 4.089241147041321, Final Batch Loss: 0.8606534004211426\n",
      "Epoch 458, Loss: 4.047671020030975, Final Batch Loss: 0.8477044701576233\n",
      "Epoch 459, Loss: 4.149731636047363, Final Batch Loss: 0.8354455828666687\n",
      "Epoch 460, Loss: 4.01388543844223, Final Batch Loss: 0.8419009447097778\n",
      "Epoch 461, Loss: 4.171214520931244, Final Batch Loss: 0.7463173270225525\n",
      "Epoch 462, Loss: 4.007252931594849, Final Batch Loss: 0.7733304500579834\n",
      "Epoch 463, Loss: 3.9331202507019043, Final Batch Loss: 0.8678368926048279\n",
      "Epoch 464, Loss: 4.194311141967773, Final Batch Loss: 0.8322153687477112\n",
      "Epoch 465, Loss: 4.017513394355774, Final Batch Loss: 0.7514915466308594\n",
      "Epoch 466, Loss: 4.123084008693695, Final Batch Loss: 0.7347066402435303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 467, Loss: 4.105818569660187, Final Batch Loss: 0.7852829098701477\n",
      "Epoch 468, Loss: 4.090888679027557, Final Batch Loss: 0.8224353194236755\n",
      "Epoch 469, Loss: 4.066317200660706, Final Batch Loss: 0.8760179281234741\n",
      "Epoch 470, Loss: 3.963324010372162, Final Batch Loss: 0.7879936099052429\n",
      "Epoch 471, Loss: 4.038911998271942, Final Batch Loss: 0.8607873320579529\n",
      "Epoch 472, Loss: 4.137686252593994, Final Batch Loss: 0.9060627818107605\n",
      "Epoch 473, Loss: 4.230302453041077, Final Batch Loss: 0.9667975306510925\n",
      "Epoch 474, Loss: 4.25651878118515, Final Batch Loss: 0.9416847229003906\n",
      "Epoch 475, Loss: 3.9910059571266174, Final Batch Loss: 0.622494637966156\n",
      "Epoch 476, Loss: 4.1971213817596436, Final Batch Loss: 0.8921018242835999\n",
      "Epoch 477, Loss: 4.089692294597626, Final Batch Loss: 0.878900408744812\n",
      "Epoch 478, Loss: 4.038232862949371, Final Batch Loss: 0.8389902114868164\n",
      "Epoch 479, Loss: 3.960995614528656, Final Batch Loss: 0.7792999148368835\n",
      "Epoch 480, Loss: 4.187286734580994, Final Batch Loss: 0.8308129906654358\n",
      "Epoch 481, Loss: 4.157773971557617, Final Batch Loss: 0.8822840452194214\n",
      "Epoch 482, Loss: 4.05658882856369, Final Batch Loss: 0.7276968359947205\n",
      "Epoch 483, Loss: 4.065931439399719, Final Batch Loss: 0.8590287566184998\n",
      "Epoch 484, Loss: 4.109870254993439, Final Batch Loss: 0.7725409269332886\n",
      "Epoch 485, Loss: 4.088340580463409, Final Batch Loss: 0.8389772772789001\n",
      "Epoch 486, Loss: 4.135114133358002, Final Batch Loss: 0.8455402851104736\n",
      "Epoch 487, Loss: 3.9375513195991516, Final Batch Loss: 0.8475200533866882\n",
      "Epoch 488, Loss: 4.083006143569946, Final Batch Loss: 0.7700484395027161\n",
      "Epoch 489, Loss: 3.982703983783722, Final Batch Loss: 0.7074517011642456\n",
      "Epoch 490, Loss: 4.013559997081757, Final Batch Loss: 0.8352932929992676\n",
      "Epoch 491, Loss: 4.043684899806976, Final Batch Loss: 0.6357613205909729\n",
      "Epoch 492, Loss: 4.199402689933777, Final Batch Loss: 0.8820483088493347\n",
      "Epoch 493, Loss: 3.9287184476852417, Final Batch Loss: 0.787604033946991\n",
      "Epoch 494, Loss: 4.08034735918045, Final Batch Loss: 0.7980290651321411\n",
      "Epoch 495, Loss: 4.00918310880661, Final Batch Loss: 0.8011057376861572\n",
      "Epoch 496, Loss: 4.02448570728302, Final Batch Loss: 0.7919558882713318\n",
      "Epoch 497, Loss: 3.9450486302375793, Final Batch Loss: 0.6905122399330139\n",
      "Epoch 498, Loss: 4.073534607887268, Final Batch Loss: 0.8601288795471191\n",
      "Epoch 499, Loss: 3.9747868180274963, Final Batch Loss: 0.8480852246284485\n",
      "Epoch 500, Loss: 4.071811020374298, Final Batch Loss: 0.8352952599525452\n",
      "Epoch 501, Loss: 4.040261924266815, Final Batch Loss: 0.8034390807151794\n",
      "Epoch 502, Loss: 4.1296563148498535, Final Batch Loss: 0.8920880556106567\n",
      "Epoch 503, Loss: 3.9487337470054626, Final Batch Loss: 0.7269536256790161\n",
      "Epoch 504, Loss: 3.9531922936439514, Final Batch Loss: 0.781354546546936\n",
      "Epoch 505, Loss: 3.9797887802124023, Final Batch Loss: 0.7248409986495972\n",
      "Epoch 506, Loss: 3.929650664329529, Final Batch Loss: 0.752148449420929\n",
      "Epoch 507, Loss: 3.9292401671409607, Final Batch Loss: 0.7738944292068481\n",
      "Epoch 508, Loss: 4.142664790153503, Final Batch Loss: 0.8415946364402771\n",
      "Epoch 509, Loss: 3.90475732088089, Final Batch Loss: 0.7929718494415283\n",
      "Epoch 510, Loss: 3.93776935338974, Final Batch Loss: 0.800491452217102\n",
      "Epoch 511, Loss: 3.990029990673065, Final Batch Loss: 0.7546667456626892\n",
      "Epoch 512, Loss: 3.8659873008728027, Final Batch Loss: 0.6998723149299622\n",
      "Epoch 513, Loss: 4.1063618659973145, Final Batch Loss: 0.7892816662788391\n",
      "Epoch 514, Loss: 4.004601061344147, Final Batch Loss: 0.8107494711875916\n",
      "Epoch 515, Loss: 3.9681605100631714, Final Batch Loss: 0.7600647211074829\n",
      "Epoch 516, Loss: 4.005131244659424, Final Batch Loss: 0.8114879727363586\n",
      "Epoch 517, Loss: 3.9823166131973267, Final Batch Loss: 0.7926918268203735\n",
      "Epoch 518, Loss: 3.9731357097625732, Final Batch Loss: 0.8242684602737427\n",
      "Epoch 519, Loss: 4.058902859687805, Final Batch Loss: 0.8985117077827454\n",
      "Epoch 520, Loss: 4.002301931381226, Final Batch Loss: 0.8635046482086182\n",
      "Epoch 521, Loss: 3.9087917804718018, Final Batch Loss: 0.7804842591285706\n",
      "Epoch 522, Loss: 3.781396508216858, Final Batch Loss: 0.7563384771347046\n",
      "Epoch 523, Loss: 3.8764490485191345, Final Batch Loss: 0.7537505626678467\n",
      "Epoch 524, Loss: 3.9817089438438416, Final Batch Loss: 0.717251718044281\n",
      "Epoch 525, Loss: 3.813405752182007, Final Batch Loss: 0.6922622919082642\n",
      "Epoch 526, Loss: 3.890126943588257, Final Batch Loss: 0.8059800267219543\n",
      "Epoch 527, Loss: 3.8998286724090576, Final Batch Loss: 0.7171059250831604\n",
      "Epoch 528, Loss: 3.8298982977867126, Final Batch Loss: 0.7945237755775452\n",
      "Epoch 529, Loss: 4.161285221576691, Final Batch Loss: 0.7623719573020935\n",
      "Epoch 530, Loss: 3.8597214818000793, Final Batch Loss: 0.7568156123161316\n",
      "Epoch 531, Loss: 3.796534776687622, Final Batch Loss: 0.6625014543533325\n",
      "Epoch 532, Loss: 3.9196935296058655, Final Batch Loss: 0.8303857445716858\n",
      "Epoch 533, Loss: 3.8970625400543213, Final Batch Loss: 0.921223521232605\n",
      "Epoch 534, Loss: 4.039622008800507, Final Batch Loss: 0.8113337159156799\n",
      "Epoch 535, Loss: 3.9261218309402466, Final Batch Loss: 0.7246899604797363\n",
      "Epoch 536, Loss: 4.03321897983551, Final Batch Loss: 0.8375362753868103\n",
      "Epoch 537, Loss: 3.8343909978866577, Final Batch Loss: 0.7090003490447998\n",
      "Epoch 538, Loss: 3.9670618772506714, Final Batch Loss: 0.7952985763549805\n",
      "Epoch 539, Loss: 3.9079391956329346, Final Batch Loss: 0.793061375617981\n",
      "Epoch 540, Loss: 3.883992552757263, Final Batch Loss: 0.7447075843811035\n",
      "Epoch 541, Loss: 4.0994150042533875, Final Batch Loss: 0.7485884428024292\n",
      "Epoch 542, Loss: 3.757533848285675, Final Batch Loss: 0.6211855411529541\n",
      "Epoch 543, Loss: 3.9628299474716187, Final Batch Loss: 0.7799862027168274\n",
      "Epoch 544, Loss: 3.8227017521858215, Final Batch Loss: 0.7426177859306335\n",
      "Epoch 545, Loss: 3.9205615520477295, Final Batch Loss: 0.9016205668449402\n",
      "Epoch 546, Loss: 4.032606720924377, Final Batch Loss: 0.8911322355270386\n",
      "Epoch 547, Loss: 3.9976152777671814, Final Batch Loss: 0.7892017364501953\n",
      "Epoch 548, Loss: 3.9784621596336365, Final Batch Loss: 0.6454620361328125\n",
      "Epoch 549, Loss: 3.9588071703910828, Final Batch Loss: 0.8150242567062378\n",
      "Epoch 550, Loss: 3.7978886365890503, Final Batch Loss: 0.8566483855247498\n",
      "Epoch 551, Loss: 3.9123117923736572, Final Batch Loss: 0.7662500739097595\n",
      "Epoch 552, Loss: 3.7871596813201904, Final Batch Loss: 0.7343882322311401\n",
      "Epoch 553, Loss: 3.844643533229828, Final Batch Loss: 0.8551286458969116\n",
      "Epoch 554, Loss: 3.9514617323875427, Final Batch Loss: 0.8859071731567383\n",
      "Epoch 555, Loss: 3.9882195591926575, Final Batch Loss: 0.8895700573921204\n",
      "Epoch 556, Loss: 3.8375070095062256, Final Batch Loss: 0.8195955753326416\n",
      "Epoch 557, Loss: 3.8048330545425415, Final Batch Loss: 0.758693277835846\n",
      "Epoch 558, Loss: 3.835832417011261, Final Batch Loss: 0.8260910511016846\n",
      "Epoch 559, Loss: 3.9574219584465027, Final Batch Loss: 0.8214665651321411\n",
      "Epoch 560, Loss: 3.6802427768707275, Final Batch Loss: 0.6967860460281372\n",
      "Epoch 561, Loss: 3.846811294555664, Final Batch Loss: 0.7256491780281067\n",
      "Epoch 562, Loss: 3.866187036037445, Final Batch Loss: 0.7805456519126892\n",
      "Epoch 563, Loss: 3.952636420726776, Final Batch Loss: 0.8976126909255981\n",
      "Epoch 564, Loss: 3.754372477531433, Final Batch Loss: 0.811499834060669\n",
      "Epoch 565, Loss: 4.085771381855011, Final Batch Loss: 0.9540395736694336\n",
      "Epoch 566, Loss: 3.860994517803192, Final Batch Loss: 0.7035561203956604\n",
      "Epoch 567, Loss: 3.707904636859894, Final Batch Loss: 0.7867060899734497\n",
      "Epoch 568, Loss: 3.880709707736969, Final Batch Loss: 0.7605258822441101\n",
      "Epoch 569, Loss: 4.050506114959717, Final Batch Loss: 1.027353048324585\n",
      "Epoch 570, Loss: 3.7947981357574463, Final Batch Loss: 0.8177381753921509\n",
      "Epoch 571, Loss: 3.7518520951271057, Final Batch Loss: 0.8234662413597107\n",
      "Epoch 572, Loss: 3.856375217437744, Final Batch Loss: 0.7903628945350647\n",
      "Epoch 573, Loss: 3.933424413204193, Final Batch Loss: 0.67844158411026\n",
      "Epoch 574, Loss: 3.9140608310699463, Final Batch Loss: 0.6819400787353516\n",
      "Epoch 575, Loss: 3.733201563358307, Final Batch Loss: 0.6258506774902344\n",
      "Epoch 576, Loss: 3.8091684579849243, Final Batch Loss: 0.8048701882362366\n",
      "Epoch 577, Loss: 3.8279935717582703, Final Batch Loss: 0.7388288378715515\n",
      "Epoch 578, Loss: 3.8579960465431213, Final Batch Loss: 0.7648833990097046\n",
      "Epoch 579, Loss: 3.8870174884796143, Final Batch Loss: 0.6962029337882996\n",
      "Epoch 580, Loss: 3.8165822625160217, Final Batch Loss: 0.7511361241340637\n",
      "Epoch 581, Loss: 3.7884634733200073, Final Batch Loss: 0.7064681649208069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 582, Loss: 3.758259356021881, Final Batch Loss: 0.6433242559432983\n",
      "Epoch 583, Loss: 3.7597216367721558, Final Batch Loss: 0.6882336735725403\n",
      "Epoch 584, Loss: 3.6445918679237366, Final Batch Loss: 0.6759129762649536\n",
      "Epoch 585, Loss: 3.7926459908485413, Final Batch Loss: 0.7215155363082886\n",
      "Epoch 586, Loss: 3.795544743537903, Final Batch Loss: 0.665739119052887\n",
      "Epoch 587, Loss: 3.8898735642433167, Final Batch Loss: 0.7667106986045837\n",
      "Epoch 588, Loss: 3.8809198141098022, Final Batch Loss: 0.9191322326660156\n",
      "Epoch 589, Loss: 3.7059819102287292, Final Batch Loss: 0.7861784100532532\n",
      "Epoch 590, Loss: 3.7959617972373962, Final Batch Loss: 0.7054225206375122\n",
      "Epoch 591, Loss: 3.643562078475952, Final Batch Loss: 0.6315184831619263\n",
      "Epoch 592, Loss: 3.9657721519470215, Final Batch Loss: 0.8064768314361572\n",
      "Epoch 593, Loss: 3.8949716687202454, Final Batch Loss: 0.856461226940155\n",
      "Epoch 594, Loss: 3.7704572081565857, Final Batch Loss: 0.7256974577903748\n",
      "Epoch 595, Loss: 3.801947832107544, Final Batch Loss: 0.8036078810691833\n",
      "Epoch 596, Loss: 3.7391477823257446, Final Batch Loss: 0.7139483690261841\n",
      "Epoch 597, Loss: 3.7512069940567017, Final Batch Loss: 0.6898888349533081\n",
      "Epoch 598, Loss: 3.6483426094055176, Final Batch Loss: 0.7437629699707031\n",
      "Epoch 599, Loss: 3.790800452232361, Final Batch Loss: 0.7273925542831421\n",
      "Epoch 600, Loss: 3.815752625465393, Final Batch Loss: 0.891689658164978\n",
      "Epoch 601, Loss: 3.943795680999756, Final Batch Loss: 0.7817752957344055\n",
      "Epoch 602, Loss: 3.8380748629570007, Final Batch Loss: 0.8604921698570251\n",
      "Epoch 603, Loss: 3.755413830280304, Final Batch Loss: 0.7821617126464844\n",
      "Epoch 604, Loss: 3.9302675127983093, Final Batch Loss: 0.8629308938980103\n",
      "Epoch 605, Loss: 3.7816523909568787, Final Batch Loss: 0.7682793736457825\n",
      "Epoch 606, Loss: 3.7966795563697815, Final Batch Loss: 0.8212172389030457\n",
      "Epoch 607, Loss: 3.8373982310295105, Final Batch Loss: 0.7143515944480896\n",
      "Epoch 608, Loss: 3.806982159614563, Final Batch Loss: 0.6567233204841614\n",
      "Epoch 609, Loss: 3.782529830932617, Final Batch Loss: 0.7275182008743286\n",
      "Epoch 610, Loss: 3.8055384159088135, Final Batch Loss: 0.6719270348548889\n",
      "Epoch 611, Loss: 3.741778612136841, Final Batch Loss: 0.8150257468223572\n",
      "Epoch 612, Loss: 3.642844021320343, Final Batch Loss: 0.726692795753479\n",
      "Epoch 613, Loss: 3.810053586959839, Final Batch Loss: 0.8087559342384338\n",
      "Epoch 614, Loss: 3.609334349632263, Final Batch Loss: 0.7246555089950562\n",
      "Epoch 615, Loss: 3.776978373527527, Final Batch Loss: 0.7434670925140381\n",
      "Epoch 616, Loss: 3.827370345592499, Final Batch Loss: 0.8179912567138672\n",
      "Epoch 617, Loss: 3.563052177429199, Final Batch Loss: 0.7265705466270447\n",
      "Epoch 618, Loss: 3.8148910999298096, Final Batch Loss: 0.8474527597427368\n",
      "Epoch 619, Loss: 3.8669323325157166, Final Batch Loss: 0.8986833095550537\n",
      "Epoch 620, Loss: 3.894205391407013, Final Batch Loss: 0.80776447057724\n",
      "Epoch 621, Loss: 3.684039354324341, Final Batch Loss: 0.8110072016716003\n",
      "Epoch 622, Loss: 3.7480549216270447, Final Batch Loss: 0.5678117871284485\n",
      "Epoch 623, Loss: 3.842391014099121, Final Batch Loss: 0.7982404232025146\n",
      "Epoch 624, Loss: 3.780099034309387, Final Batch Loss: 0.8564856052398682\n",
      "Epoch 625, Loss: 3.519343852996826, Final Batch Loss: 0.6373602747917175\n",
      "Epoch 626, Loss: 3.6827752590179443, Final Batch Loss: 0.7649635672569275\n",
      "Epoch 627, Loss: 3.6517967581748962, Final Batch Loss: 0.6580999493598938\n",
      "Epoch 628, Loss: 3.7728822231292725, Final Batch Loss: 0.7754302024841309\n",
      "Epoch 629, Loss: 3.6627359986305237, Final Batch Loss: 0.7705853581428528\n",
      "Epoch 630, Loss: 3.721843123435974, Final Batch Loss: 0.7907426953315735\n",
      "Epoch 631, Loss: 3.7085545659065247, Final Batch Loss: 0.7040964365005493\n",
      "Epoch 632, Loss: 3.7347083687782288, Final Batch Loss: 0.8275185227394104\n",
      "Epoch 633, Loss: 3.5832216143608093, Final Batch Loss: 0.718980610370636\n",
      "Epoch 634, Loss: 3.662782371044159, Final Batch Loss: 0.737326443195343\n",
      "Epoch 635, Loss: 3.7299577593803406, Final Batch Loss: 0.7387917637825012\n",
      "Epoch 636, Loss: 3.5300069451332092, Final Batch Loss: 0.6506479382514954\n",
      "Epoch 637, Loss: 3.6011871695518494, Final Batch Loss: 0.6917535662651062\n",
      "Epoch 638, Loss: 3.639669418334961, Final Batch Loss: 0.7648325562477112\n",
      "Epoch 639, Loss: 3.6235698461532593, Final Batch Loss: 0.7257343530654907\n",
      "Epoch 640, Loss: 3.767440915107727, Final Batch Loss: 0.643797755241394\n",
      "Epoch 641, Loss: 3.773772716522217, Final Batch Loss: 0.7489771246910095\n",
      "Epoch 642, Loss: 3.585302233695984, Final Batch Loss: 0.687213122844696\n",
      "Epoch 643, Loss: 3.7186959981918335, Final Batch Loss: 0.7714270353317261\n",
      "Epoch 644, Loss: 3.7049014568328857, Final Batch Loss: 0.766810953617096\n",
      "Epoch 645, Loss: 3.7056580185890198, Final Batch Loss: 0.8099232912063599\n",
      "Epoch 646, Loss: 3.627757489681244, Final Batch Loss: 0.7920352220535278\n",
      "Epoch 647, Loss: 3.7365283966064453, Final Batch Loss: 0.7380549907684326\n",
      "Epoch 648, Loss: 3.703088402748108, Final Batch Loss: 0.7755206823348999\n",
      "Epoch 649, Loss: 3.7377949953079224, Final Batch Loss: 0.7655818462371826\n",
      "Epoch 650, Loss: 3.670346736907959, Final Batch Loss: 0.6957064270973206\n",
      "Epoch 651, Loss: 3.743137776851654, Final Batch Loss: 0.8027490973472595\n",
      "Epoch 652, Loss: 3.502268075942993, Final Batch Loss: 0.634752631187439\n",
      "Epoch 653, Loss: 3.5331432223320007, Final Batch Loss: 0.6909633278846741\n",
      "Epoch 654, Loss: 3.8197742104530334, Final Batch Loss: 0.8062435984611511\n",
      "Epoch 655, Loss: 3.7519405484199524, Final Batch Loss: 0.8117502927780151\n",
      "Epoch 656, Loss: 3.7234750986099243, Final Batch Loss: 0.8157497644424438\n",
      "Epoch 657, Loss: 3.797668933868408, Final Batch Loss: 0.9025408625602722\n",
      "Epoch 658, Loss: 3.584913671016693, Final Batch Loss: 0.6976233720779419\n",
      "Epoch 659, Loss: 3.751581132411957, Final Batch Loss: 0.8106638193130493\n",
      "Epoch 660, Loss: 3.591272473335266, Final Batch Loss: 0.6917198300361633\n",
      "Epoch 661, Loss: 3.5933201909065247, Final Batch Loss: 0.6819244027137756\n",
      "Epoch 662, Loss: 3.8262219429016113, Final Batch Loss: 0.8582959771156311\n",
      "Epoch 663, Loss: 3.712122321128845, Final Batch Loss: 0.6653648614883423\n",
      "Epoch 664, Loss: 3.525207757949829, Final Batch Loss: 0.680108904838562\n",
      "Epoch 665, Loss: 3.6139869689941406, Final Batch Loss: 0.6529437303543091\n",
      "Epoch 666, Loss: 3.645612120628357, Final Batch Loss: 0.656290590763092\n",
      "Epoch 667, Loss: 3.649742066860199, Final Batch Loss: 0.729580283164978\n",
      "Epoch 668, Loss: 3.502919614315033, Final Batch Loss: 0.5943501591682434\n",
      "Epoch 669, Loss: 3.747318923473358, Final Batch Loss: 0.8631194233894348\n",
      "Epoch 670, Loss: 3.631272315979004, Final Batch Loss: 0.6815528869628906\n",
      "Epoch 671, Loss: 3.6068937182426453, Final Batch Loss: 0.6718028783798218\n",
      "Epoch 672, Loss: 3.546535611152649, Final Batch Loss: 0.7274726033210754\n",
      "Epoch 673, Loss: 3.6538467407226562, Final Batch Loss: 0.6408316493034363\n",
      "Epoch 674, Loss: 3.567605972290039, Final Batch Loss: 0.8533202409744263\n",
      "Epoch 675, Loss: 3.524975597858429, Final Batch Loss: 0.6930955052375793\n",
      "Epoch 676, Loss: 3.517053008079529, Final Batch Loss: 0.6885409355163574\n",
      "Epoch 677, Loss: 3.648993968963623, Final Batch Loss: 0.7483586072921753\n",
      "Epoch 678, Loss: 3.635079324245453, Final Batch Loss: 0.7012880444526672\n",
      "Epoch 679, Loss: 3.571259081363678, Final Batch Loss: 0.7678898572921753\n",
      "Epoch 680, Loss: 3.6142871379852295, Final Batch Loss: 0.7873308658599854\n",
      "Epoch 681, Loss: 3.5208073258399963, Final Batch Loss: 0.6857689023017883\n",
      "Epoch 682, Loss: 3.4858304858207703, Final Batch Loss: 0.6621454358100891\n",
      "Epoch 683, Loss: 3.7147955894470215, Final Batch Loss: 0.8302449584007263\n",
      "Epoch 684, Loss: 3.7876036167144775, Final Batch Loss: 0.7703878879547119\n",
      "Epoch 685, Loss: 3.7522993683815002, Final Batch Loss: 0.7798084020614624\n",
      "Epoch 686, Loss: 3.5890197157859802, Final Batch Loss: 0.8222512602806091\n",
      "Epoch 687, Loss: 3.660373866558075, Final Batch Loss: 0.7612654566764832\n",
      "Epoch 688, Loss: 3.6523120403289795, Final Batch Loss: 0.9440688490867615\n",
      "Epoch 689, Loss: 3.5135651230812073, Final Batch Loss: 0.6552406549453735\n",
      "Epoch 690, Loss: 3.583239257335663, Final Batch Loss: 0.6722086071968079\n",
      "Epoch 691, Loss: 3.745449721813202, Final Batch Loss: 0.7617393732070923\n",
      "Epoch 692, Loss: 3.46768456697464, Final Batch Loss: 0.7311695218086243\n",
      "Epoch 693, Loss: 3.5942063331604004, Final Batch Loss: 0.7922702431678772\n",
      "Epoch 694, Loss: 3.6445192098617554, Final Batch Loss: 0.8245456218719482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 695, Loss: 3.565545439720154, Final Batch Loss: 0.645394504070282\n",
      "Epoch 696, Loss: 3.553880214691162, Final Batch Loss: 0.6829892992973328\n",
      "Epoch 697, Loss: 3.5063503980636597, Final Batch Loss: 0.625104546546936\n",
      "Epoch 698, Loss: 3.538689970970154, Final Batch Loss: 0.5991018414497375\n",
      "Epoch 699, Loss: 3.558895170688629, Final Batch Loss: 0.6479937434196472\n",
      "Epoch 700, Loss: 3.6834517121315002, Final Batch Loss: 0.7020241618156433\n",
      "Epoch 701, Loss: 3.750483751296997, Final Batch Loss: 0.7505894899368286\n",
      "Epoch 702, Loss: 3.5033663511276245, Final Batch Loss: 0.7064651250839233\n",
      "Epoch 703, Loss: 3.6873123049736023, Final Batch Loss: 0.7893474102020264\n",
      "Epoch 704, Loss: 3.5021417140960693, Final Batch Loss: 0.7382908463478088\n",
      "Epoch 705, Loss: 3.582381546497345, Final Batch Loss: 0.7313194870948792\n",
      "Epoch 706, Loss: 3.6441308856010437, Final Batch Loss: 0.7770320177078247\n",
      "Epoch 707, Loss: 3.511423110961914, Final Batch Loss: 0.6589784026145935\n",
      "Epoch 708, Loss: 3.5729158520698547, Final Batch Loss: 0.7045862674713135\n",
      "Epoch 709, Loss: 3.6503430604934692, Final Batch Loss: 0.730836033821106\n",
      "Epoch 710, Loss: 3.678523540496826, Final Batch Loss: 0.6761534214019775\n",
      "Epoch 711, Loss: 3.5566808581352234, Final Batch Loss: 0.7221185564994812\n",
      "Epoch 712, Loss: 3.596310615539551, Final Batch Loss: 0.7594454884529114\n",
      "Epoch 713, Loss: 3.603949010372162, Final Batch Loss: 0.755571722984314\n",
      "Epoch 714, Loss: 3.5533335208892822, Final Batch Loss: 0.6353844404220581\n",
      "Epoch 715, Loss: 3.636745810508728, Final Batch Loss: 0.8679083585739136\n",
      "Epoch 716, Loss: 3.538289964199066, Final Batch Loss: 0.6821277737617493\n",
      "Epoch 717, Loss: 3.4252662658691406, Final Batch Loss: 0.6420494318008423\n",
      "Epoch 718, Loss: 3.5434433817863464, Final Batch Loss: 0.7762519717216492\n",
      "Epoch 719, Loss: 3.5437225699424744, Final Batch Loss: 0.6418840885162354\n",
      "Epoch 720, Loss: 3.409747898578644, Final Batch Loss: 0.6981800198554993\n",
      "Epoch 721, Loss: 3.6128705739974976, Final Batch Loss: 0.6663304567337036\n",
      "Epoch 722, Loss: 3.5101194977760315, Final Batch Loss: 0.7152419686317444\n",
      "Epoch 723, Loss: 3.4785500168800354, Final Batch Loss: 0.6281418800354004\n",
      "Epoch 724, Loss: 3.6295732855796814, Final Batch Loss: 0.738709568977356\n",
      "Epoch 725, Loss: 3.510453522205353, Final Batch Loss: 0.6394751071929932\n",
      "Epoch 726, Loss: 3.5447264909744263, Final Batch Loss: 0.6589483022689819\n",
      "Epoch 727, Loss: 3.607071816921234, Final Batch Loss: 0.8247289657592773\n",
      "Epoch 728, Loss: 3.5791642665863037, Final Batch Loss: 0.7302621603012085\n",
      "Epoch 729, Loss: 3.5178455114364624, Final Batch Loss: 0.6424316167831421\n",
      "Epoch 730, Loss: 3.562886118888855, Final Batch Loss: 0.7801709175109863\n",
      "Epoch 731, Loss: 3.4786832332611084, Final Batch Loss: 0.6894875168800354\n",
      "Epoch 732, Loss: 3.4896798133850098, Final Batch Loss: 0.6871986985206604\n",
      "Epoch 733, Loss: 3.794087052345276, Final Batch Loss: 0.913856565952301\n",
      "Epoch 734, Loss: 3.430172026157379, Final Batch Loss: 0.6550617218017578\n",
      "Epoch 735, Loss: 3.6142001152038574, Final Batch Loss: 0.815902054309845\n",
      "Epoch 736, Loss: 3.495523989200592, Final Batch Loss: 0.6710240840911865\n",
      "Epoch 737, Loss: 3.552273690700531, Final Batch Loss: 0.7467132210731506\n",
      "Epoch 738, Loss: 3.5019614696502686, Final Batch Loss: 0.7230934500694275\n",
      "Epoch 739, Loss: 3.5240865349769592, Final Batch Loss: 0.6333849430084229\n",
      "Epoch 740, Loss: 3.687322437763214, Final Batch Loss: 0.6911497116088867\n",
      "Epoch 741, Loss: 3.489551842212677, Final Batch Loss: 0.6541810631752014\n",
      "Epoch 742, Loss: 3.4750271439552307, Final Batch Loss: 0.6365179419517517\n",
      "Epoch 743, Loss: 3.506292998790741, Final Batch Loss: 0.7110802531242371\n",
      "Epoch 744, Loss: 3.532608687877655, Final Batch Loss: 0.7164435386657715\n",
      "Epoch 745, Loss: 3.2359049916267395, Final Batch Loss: 0.6054851412773132\n",
      "Epoch 746, Loss: 3.502251088619232, Final Batch Loss: 0.6854715943336487\n",
      "Epoch 747, Loss: 3.4321390986442566, Final Batch Loss: 0.6797983050346375\n",
      "Epoch 748, Loss: 3.3667746782302856, Final Batch Loss: 0.6058306694030762\n",
      "Epoch 749, Loss: 3.4231404066085815, Final Batch Loss: 0.6789934635162354\n",
      "Epoch 750, Loss: 3.5977628231048584, Final Batch Loss: 0.8543546199798584\n",
      "Epoch 751, Loss: 3.468943953514099, Final Batch Loss: 0.6858751773834229\n",
      "Epoch 752, Loss: 3.477468490600586, Final Batch Loss: 0.6988818049430847\n",
      "Epoch 753, Loss: 3.5362924933433533, Final Batch Loss: 0.7075642943382263\n",
      "Epoch 754, Loss: 3.5272586941719055, Final Batch Loss: 0.7589792013168335\n",
      "Epoch 755, Loss: 3.609440505504608, Final Batch Loss: 0.7047561407089233\n",
      "Epoch 756, Loss: 3.433449447154999, Final Batch Loss: 0.70240318775177\n",
      "Epoch 757, Loss: 3.5348936319351196, Final Batch Loss: 0.743493378162384\n",
      "Epoch 758, Loss: 3.507794141769409, Final Batch Loss: 0.6829522848129272\n",
      "Epoch 759, Loss: 3.445624053478241, Final Batch Loss: 0.6443024277687073\n",
      "Epoch 760, Loss: 3.3558149337768555, Final Batch Loss: 0.5410971641540527\n",
      "Epoch 761, Loss: 3.561806857585907, Final Batch Loss: 0.820219099521637\n",
      "Epoch 762, Loss: 3.4432522654533386, Final Batch Loss: 0.7858089804649353\n",
      "Epoch 763, Loss: 3.535984694957733, Final Batch Loss: 0.7200389504432678\n",
      "Epoch 764, Loss: 3.4618061184883118, Final Batch Loss: 0.6627436876296997\n",
      "Epoch 765, Loss: 3.3560920357704163, Final Batch Loss: 0.6535359621047974\n",
      "Epoch 766, Loss: 3.33880615234375, Final Batch Loss: 0.7080856561660767\n",
      "Epoch 767, Loss: 3.4071882367134094, Final Batch Loss: 0.7857861518859863\n",
      "Epoch 768, Loss: 3.41510009765625, Final Batch Loss: 0.6551206111907959\n",
      "Epoch 769, Loss: 3.586673140525818, Final Batch Loss: 0.6960898041725159\n",
      "Epoch 770, Loss: 3.619983673095703, Final Batch Loss: 0.7215772867202759\n",
      "Epoch 771, Loss: 3.425961911678314, Final Batch Loss: 0.7168591022491455\n",
      "Epoch 772, Loss: 3.4015485048294067, Final Batch Loss: 0.7757096886634827\n",
      "Epoch 773, Loss: 3.4533550143241882, Final Batch Loss: 0.6443297266960144\n",
      "Epoch 774, Loss: 3.4319828748703003, Final Batch Loss: 0.6778961420059204\n",
      "Epoch 775, Loss: 3.4167152643203735, Final Batch Loss: 0.5892545580863953\n",
      "Epoch 776, Loss: 3.3651975989341736, Final Batch Loss: 0.6093066930770874\n",
      "Epoch 777, Loss: 3.495515525341034, Final Batch Loss: 0.6438735723495483\n",
      "Epoch 778, Loss: 3.6255006194114685, Final Batch Loss: 0.7409638166427612\n",
      "Epoch 779, Loss: 3.437884747982025, Final Batch Loss: 0.7084181904792786\n",
      "Epoch 780, Loss: 3.350580930709839, Final Batch Loss: 0.6700411438941956\n",
      "Epoch 781, Loss: 3.5960991382598877, Final Batch Loss: 0.8053067922592163\n",
      "Epoch 782, Loss: 3.3131662011146545, Final Batch Loss: 0.627673327922821\n",
      "Epoch 783, Loss: 3.417092204093933, Final Batch Loss: 0.7090469598770142\n",
      "Epoch 784, Loss: 3.360898017883301, Final Batch Loss: 0.6451975703239441\n",
      "Epoch 785, Loss: 3.51410710811615, Final Batch Loss: 0.8133938908576965\n",
      "Epoch 786, Loss: 3.37520831823349, Final Batch Loss: 0.6438931226730347\n",
      "Epoch 787, Loss: 3.4441418051719666, Final Batch Loss: 0.6627435684204102\n",
      "Epoch 788, Loss: 3.386463940143585, Final Batch Loss: 0.6820984482765198\n",
      "Epoch 789, Loss: 3.502159833908081, Final Batch Loss: 0.7682856917381287\n",
      "Epoch 790, Loss: 3.5864003896713257, Final Batch Loss: 0.7509705424308777\n",
      "Epoch 791, Loss: 3.3970717787742615, Final Batch Loss: 0.7475229501724243\n",
      "Epoch 792, Loss: 3.3611605167388916, Final Batch Loss: 0.5908281803131104\n",
      "Epoch 793, Loss: 3.532304286956787, Final Batch Loss: 0.6267437934875488\n",
      "Epoch 794, Loss: 3.4494476914405823, Final Batch Loss: 0.6692952513694763\n",
      "Epoch 795, Loss: 3.274435818195343, Final Batch Loss: 0.5792404413223267\n",
      "Epoch 796, Loss: 3.359149217605591, Final Batch Loss: 0.6548071503639221\n",
      "Epoch 797, Loss: 3.4182952642440796, Final Batch Loss: 0.7500280737876892\n",
      "Epoch 798, Loss: 3.5512237548828125, Final Batch Loss: 0.8304932117462158\n",
      "Epoch 799, Loss: 3.5077927112579346, Final Batch Loss: 0.7335605621337891\n",
      "Epoch 800, Loss: 3.4424245953559875, Final Batch Loss: 0.7266788482666016\n",
      "Epoch 801, Loss: 3.345444977283478, Final Batch Loss: 0.6606990694999695\n",
      "Epoch 802, Loss: 3.4439204931259155, Final Batch Loss: 0.763785183429718\n",
      "Epoch 803, Loss: 3.5468796491622925, Final Batch Loss: 0.8047751188278198\n",
      "Epoch 804, Loss: 3.4444252252578735, Final Batch Loss: 0.7715540528297424\n",
      "Epoch 805, Loss: 3.4204530119895935, Final Batch Loss: 0.7521207332611084\n",
      "Epoch 806, Loss: 3.5407649874687195, Final Batch Loss: 0.6439269781112671\n",
      "Epoch 807, Loss: 3.559522032737732, Final Batch Loss: 0.815285861492157\n",
      "Epoch 808, Loss: 3.308646500110626, Final Batch Loss: 0.7037304043769836\n",
      "Epoch 809, Loss: 3.378729820251465, Final Batch Loss: 0.8336061835289001\n",
      "Epoch 810, Loss: 3.4614651799201965, Final Batch Loss: 0.599624752998352\n",
      "Epoch 811, Loss: 3.4392480850219727, Final Batch Loss: 0.5925878882408142\n",
      "Epoch 812, Loss: 3.391706705093384, Final Batch Loss: 0.6623528599739075\n",
      "Epoch 813, Loss: 3.2440677285194397, Final Batch Loss: 0.5843518972396851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 814, Loss: 3.3660236597061157, Final Batch Loss: 0.718944251537323\n",
      "Epoch 815, Loss: 3.3718783259391785, Final Batch Loss: 0.7250803709030151\n",
      "Epoch 816, Loss: 3.348372459411621, Final Batch Loss: 0.6302060484886169\n",
      "Epoch 817, Loss: 3.3633522987365723, Final Batch Loss: 0.7086469531059265\n",
      "Epoch 818, Loss: 3.295492708683014, Final Batch Loss: 0.6066632866859436\n",
      "Epoch 819, Loss: 3.5016040802001953, Final Batch Loss: 0.7500841617584229\n",
      "Epoch 820, Loss: 3.440825045108795, Final Batch Loss: 0.6897838711738586\n",
      "Epoch 821, Loss: 3.3765665888786316, Final Batch Loss: 0.7028982043266296\n",
      "Epoch 822, Loss: 3.409221351146698, Final Batch Loss: 0.6146811246871948\n",
      "Epoch 823, Loss: 3.4082903265953064, Final Batch Loss: 0.704106330871582\n",
      "Epoch 824, Loss: 3.357040524482727, Final Batch Loss: 0.6319291591644287\n",
      "Epoch 825, Loss: 3.311844825744629, Final Batch Loss: 0.6686189770698547\n",
      "Epoch 826, Loss: 3.4045441150665283, Final Batch Loss: 0.796608567237854\n",
      "Epoch 827, Loss: 3.4250600337982178, Final Batch Loss: 0.6450932621955872\n",
      "Epoch 828, Loss: 3.276194930076599, Final Batch Loss: 0.8233693242073059\n",
      "Epoch 829, Loss: 3.35803759098053, Final Batch Loss: 0.5922861695289612\n",
      "Epoch 830, Loss: 3.4303826093673706, Final Batch Loss: 0.7674476504325867\n",
      "Epoch 831, Loss: 3.5041959285736084, Final Batch Loss: 0.8208729028701782\n",
      "Epoch 832, Loss: 3.4119794964790344, Final Batch Loss: 0.7162160277366638\n",
      "Epoch 833, Loss: 3.3827741146087646, Final Batch Loss: 0.6799389123916626\n",
      "Epoch 834, Loss: 3.387795329093933, Final Batch Loss: 0.6542498469352722\n",
      "Epoch 835, Loss: 3.388218343257904, Final Batch Loss: 0.7317126989364624\n",
      "Epoch 836, Loss: 3.5374847650527954, Final Batch Loss: 0.6671929955482483\n",
      "Epoch 837, Loss: 3.4815195202827454, Final Batch Loss: 0.6927356719970703\n",
      "Epoch 838, Loss: 3.3365501165390015, Final Batch Loss: 0.7142419219017029\n",
      "Epoch 839, Loss: 3.3107579946517944, Final Batch Loss: 0.5987036228179932\n",
      "Epoch 840, Loss: 3.2726516127586365, Final Batch Loss: 0.7229654788970947\n",
      "Epoch 841, Loss: 3.2710379362106323, Final Batch Loss: 0.563649594783783\n",
      "Epoch 842, Loss: 3.3330901861190796, Final Batch Loss: 0.5768769383430481\n",
      "Epoch 843, Loss: 3.3711318373680115, Final Batch Loss: 0.7230163812637329\n",
      "Epoch 844, Loss: 3.3696128726005554, Final Batch Loss: 0.6554709672927856\n",
      "Epoch 845, Loss: 3.397735118865967, Final Batch Loss: 0.692301332950592\n",
      "Epoch 846, Loss: 3.3152924180030823, Final Batch Loss: 0.6896545886993408\n",
      "Epoch 847, Loss: 3.468980848789215, Final Batch Loss: 0.752685010433197\n",
      "Epoch 848, Loss: 3.4554684162139893, Final Batch Loss: 0.7526571154594421\n",
      "Epoch 849, Loss: 3.2735222578048706, Final Batch Loss: 0.6947805285453796\n",
      "Epoch 850, Loss: 3.3169385194778442, Final Batch Loss: 0.6767865419387817\n",
      "Epoch 851, Loss: 3.186622589826584, Final Batch Loss: 0.47661110758781433\n",
      "Epoch 852, Loss: 3.1885271072387695, Final Batch Loss: 0.6306409239768982\n",
      "Epoch 853, Loss: 3.291068732738495, Final Batch Loss: 0.6244887113571167\n",
      "Epoch 854, Loss: 3.4333282113075256, Final Batch Loss: 0.7217773199081421\n",
      "Epoch 855, Loss: 3.388267993927002, Final Batch Loss: 0.7240844964981079\n",
      "Epoch 856, Loss: 3.359924793243408, Final Batch Loss: 0.6815479397773743\n",
      "Epoch 857, Loss: 3.3227389454841614, Final Batch Loss: 0.6109292507171631\n",
      "Epoch 858, Loss: 3.337703824043274, Final Batch Loss: 0.6152892708778381\n",
      "Epoch 859, Loss: 3.312216281890869, Final Batch Loss: 0.5929864048957825\n",
      "Epoch 860, Loss: 3.2271231412887573, Final Batch Loss: 0.598783552646637\n",
      "Epoch 861, Loss: 3.502050042152405, Final Batch Loss: 0.7769270539283752\n",
      "Epoch 862, Loss: 3.4621885418891907, Final Batch Loss: 0.630488932132721\n",
      "Epoch 863, Loss: 3.293465554714203, Final Batch Loss: 0.7663576006889343\n",
      "Epoch 864, Loss: 3.4572632908821106, Final Batch Loss: 0.748073399066925\n",
      "Epoch 865, Loss: 3.5961623191833496, Final Batch Loss: 0.6659179329872131\n",
      "Epoch 866, Loss: 3.5188422203063965, Final Batch Loss: 0.8012295365333557\n",
      "Epoch 867, Loss: 3.3341225385665894, Final Batch Loss: 0.7840414643287659\n",
      "Epoch 868, Loss: 3.3539775013923645, Final Batch Loss: 0.6189830303192139\n",
      "Epoch 869, Loss: 3.2802486419677734, Final Batch Loss: 0.6660889387130737\n",
      "Epoch 870, Loss: 3.3602977991104126, Final Batch Loss: 0.7095969319343567\n",
      "Epoch 871, Loss: 3.1528689861297607, Final Batch Loss: 0.6610642671585083\n",
      "Epoch 872, Loss: 3.271187961101532, Final Batch Loss: 0.6289503574371338\n",
      "Epoch 873, Loss: 3.3573983311653137, Final Batch Loss: 0.6230290532112122\n",
      "Epoch 874, Loss: 3.4295324087142944, Final Batch Loss: 0.7380297183990479\n",
      "Epoch 875, Loss: 3.2092504501342773, Final Batch Loss: 0.6348863244056702\n",
      "Epoch 876, Loss: 3.1889630556106567, Final Batch Loss: 0.6913319230079651\n",
      "Epoch 877, Loss: 3.1752063632011414, Final Batch Loss: 0.5399752855300903\n",
      "Epoch 878, Loss: 3.2718908190727234, Final Batch Loss: 0.7141788601875305\n",
      "Epoch 879, Loss: 3.44233101606369, Final Batch Loss: 0.6604430675506592\n",
      "Epoch 880, Loss: 3.516077220439911, Final Batch Loss: 0.8232205510139465\n",
      "Epoch 881, Loss: 3.3348429203033447, Final Batch Loss: 0.7319483160972595\n",
      "Epoch 882, Loss: 3.3387744426727295, Final Batch Loss: 0.6132803559303284\n",
      "Epoch 883, Loss: 3.2660387754440308, Final Batch Loss: 0.5824778079986572\n",
      "Epoch 884, Loss: 3.410760521888733, Final Batch Loss: 0.716304361820221\n",
      "Epoch 885, Loss: 3.3159635066986084, Final Batch Loss: 0.7840517163276672\n",
      "Epoch 886, Loss: 3.3717122673988342, Final Batch Loss: 0.8606273531913757\n",
      "Epoch 887, Loss: 3.284509778022766, Final Batch Loss: 0.6261922121047974\n",
      "Epoch 888, Loss: 3.4778284430503845, Final Batch Loss: 0.6912186145782471\n",
      "Epoch 889, Loss: 3.4127930402755737, Final Batch Loss: 0.6975178718566895\n",
      "Epoch 890, Loss: 3.240623414516449, Final Batch Loss: 0.6520732641220093\n",
      "Epoch 891, Loss: 3.2046159505844116, Final Batch Loss: 0.5770055055618286\n",
      "Epoch 892, Loss: 3.407968521118164, Final Batch Loss: 0.6151951551437378\n",
      "Epoch 893, Loss: 3.3183135986328125, Final Batch Loss: 0.5732163786888123\n",
      "Epoch 894, Loss: 3.242626428604126, Final Batch Loss: 0.60725337266922\n",
      "Epoch 895, Loss: 3.3404964804649353, Final Batch Loss: 0.6808242797851562\n",
      "Epoch 896, Loss: 3.375269591808319, Final Batch Loss: 0.7073332667350769\n",
      "Epoch 897, Loss: 3.3531693816184998, Final Batch Loss: 0.6580916047096252\n",
      "Epoch 898, Loss: 3.4859509468078613, Final Batch Loss: 0.7723214030265808\n",
      "Epoch 899, Loss: 3.3482187390327454, Final Batch Loss: 0.7464166283607483\n",
      "Epoch 900, Loss: 3.5163188576698303, Final Batch Loss: 0.6065985560417175\n",
      "Epoch 901, Loss: 3.176385283470154, Final Batch Loss: 0.5786236524581909\n",
      "Epoch 902, Loss: 3.3816307187080383, Final Batch Loss: 0.7072320580482483\n",
      "Epoch 903, Loss: 3.259646773338318, Final Batch Loss: 0.6092559695243835\n",
      "Epoch 904, Loss: 3.2991687655448914, Final Batch Loss: 0.6527597904205322\n",
      "Epoch 905, Loss: 3.3343511819839478, Final Batch Loss: 0.6722989082336426\n",
      "Epoch 906, Loss: 3.272247016429901, Final Batch Loss: 0.6934982538223267\n",
      "Epoch 907, Loss: 3.29035484790802, Final Batch Loss: 0.6663093566894531\n",
      "Epoch 908, Loss: 3.3194910883903503, Final Batch Loss: 0.6775261163711548\n",
      "Epoch 909, Loss: 3.2175878286361694, Final Batch Loss: 0.5983789563179016\n",
      "Epoch 910, Loss: 3.301839828491211, Final Batch Loss: 0.6973429918289185\n",
      "Epoch 911, Loss: 3.3622377514839172, Final Batch Loss: 0.7187304496765137\n",
      "Epoch 912, Loss: 3.2732688784599304, Final Batch Loss: 0.674046516418457\n",
      "Epoch 913, Loss: 3.2639602422714233, Final Batch Loss: 0.5318795442581177\n",
      "Epoch 914, Loss: 3.3326512575149536, Final Batch Loss: 0.6610293388366699\n",
      "Epoch 915, Loss: 3.256411075592041, Final Batch Loss: 0.6278868317604065\n",
      "Epoch 916, Loss: 3.3369640111923218, Final Batch Loss: 0.6865847706794739\n",
      "Epoch 917, Loss: 3.257877826690674, Final Batch Loss: 0.6290391683578491\n",
      "Epoch 918, Loss: 3.3030501008033752, Final Batch Loss: 0.612520158290863\n",
      "Epoch 919, Loss: 3.287561297416687, Final Batch Loss: 0.6544655561447144\n",
      "Epoch 920, Loss: 3.3659698367118835, Final Batch Loss: 0.6859689354896545\n",
      "Epoch 921, Loss: 3.2327314019203186, Final Batch Loss: 0.690687358379364\n",
      "Epoch 922, Loss: 3.495859742164612, Final Batch Loss: 0.7653201818466187\n",
      "Epoch 923, Loss: 3.2626092433929443, Final Batch Loss: 0.6888899803161621\n",
      "Epoch 924, Loss: 3.3572909235954285, Final Batch Loss: 0.6676993370056152\n",
      "Epoch 925, Loss: 3.432010591030121, Final Batch Loss: 0.7893316745758057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926, Loss: 3.242860972881317, Final Batch Loss: 0.6892778873443604\n",
      "Epoch 927, Loss: 3.2676165103912354, Final Batch Loss: 0.5919984579086304\n",
      "Epoch 928, Loss: 3.311362862586975, Final Batch Loss: 0.7408905029296875\n",
      "Epoch 929, Loss: 3.0530342757701874, Final Batch Loss: 0.4679522216320038\n",
      "Epoch 930, Loss: 3.4780696034431458, Final Batch Loss: 0.7156922221183777\n",
      "Epoch 931, Loss: 3.410006284713745, Final Batch Loss: 0.7660462260246277\n",
      "Epoch 932, Loss: 3.2089239358901978, Final Batch Loss: 0.6990513801574707\n",
      "Epoch 933, Loss: 3.360042631626129, Final Batch Loss: 0.675242006778717\n",
      "Epoch 934, Loss: 3.1898592710494995, Final Batch Loss: 0.6056545376777649\n",
      "Epoch 935, Loss: 3.3100779056549072, Final Batch Loss: 0.6762204170227051\n",
      "Epoch 936, Loss: 3.125117063522339, Final Batch Loss: 0.648847222328186\n",
      "Epoch 937, Loss: 3.165288031101227, Final Batch Loss: 0.5435106754302979\n",
      "Epoch 938, Loss: 3.3906933069229126, Final Batch Loss: 0.7048795819282532\n",
      "Epoch 939, Loss: 3.2089723944664, Final Batch Loss: 0.6037697792053223\n",
      "Epoch 940, Loss: 3.274859458208084, Final Batch Loss: 0.7643226981163025\n",
      "Epoch 941, Loss: 3.408652424812317, Final Batch Loss: 0.6762672662734985\n",
      "Epoch 942, Loss: 3.2646748423576355, Final Batch Loss: 0.6118909120559692\n",
      "Epoch 943, Loss: 3.107316792011261, Final Batch Loss: 0.5727460980415344\n",
      "Epoch 944, Loss: 3.4549769163131714, Final Batch Loss: 0.7030606865882874\n",
      "Epoch 945, Loss: 3.4299562573432922, Final Batch Loss: 0.5726377964019775\n",
      "Epoch 946, Loss: 3.294509530067444, Final Batch Loss: 0.5743324160575867\n",
      "Epoch 947, Loss: 3.0840349197387695, Final Batch Loss: 0.6587077975273132\n",
      "Epoch 948, Loss: 3.2694050669670105, Final Batch Loss: 0.6417756676673889\n",
      "Epoch 949, Loss: 3.310088634490967, Final Batch Loss: 0.6447343230247498\n",
      "Epoch 950, Loss: 3.2131162881851196, Final Batch Loss: 0.5655235648155212\n",
      "Epoch 951, Loss: 3.048131048679352, Final Batch Loss: 0.5546842813491821\n",
      "Epoch 952, Loss: 3.350507915019989, Final Batch Loss: 0.6242067217826843\n",
      "Epoch 953, Loss: 3.176218271255493, Final Batch Loss: 0.6736936569213867\n",
      "Epoch 954, Loss: 3.2174482345581055, Final Batch Loss: 0.6601293683052063\n",
      "Epoch 955, Loss: 3.255696177482605, Final Batch Loss: 0.6439172625541687\n",
      "Epoch 956, Loss: 3.299438953399658, Final Batch Loss: 0.729009211063385\n",
      "Epoch 957, Loss: 3.2051952481269836, Final Batch Loss: 0.6443672776222229\n",
      "Epoch 958, Loss: 3.2405829429626465, Final Batch Loss: 0.6728119254112244\n",
      "Epoch 959, Loss: 3.142277777194977, Final Batch Loss: 0.5925847291946411\n",
      "Epoch 960, Loss: 3.4378843903541565, Final Batch Loss: 0.7861311435699463\n",
      "Epoch 961, Loss: 3.2807124853134155, Final Batch Loss: 0.5804800987243652\n",
      "Epoch 962, Loss: 3.063592493534088, Final Batch Loss: 0.5293819904327393\n",
      "Epoch 963, Loss: 3.1831194162368774, Final Batch Loss: 0.5530624985694885\n",
      "Epoch 964, Loss: 3.317717134952545, Final Batch Loss: 0.703902006149292\n",
      "Epoch 965, Loss: 3.4382012486457825, Final Batch Loss: 0.7889769673347473\n",
      "Epoch 966, Loss: 3.487625479698181, Final Batch Loss: 0.6197598576545715\n",
      "Epoch 967, Loss: 3.1725922226905823, Final Batch Loss: 0.5332013964653015\n",
      "Epoch 968, Loss: 3.2362712621688843, Final Batch Loss: 0.664344847202301\n",
      "Epoch 969, Loss: 3.315330982208252, Final Batch Loss: 0.7012311816215515\n",
      "Epoch 970, Loss: 3.358245074748993, Final Batch Loss: 0.7649610042572021\n",
      "Epoch 971, Loss: 3.2859275341033936, Final Batch Loss: 0.6830185651779175\n",
      "Epoch 972, Loss: 3.137652814388275, Final Batch Loss: 0.537447988986969\n",
      "Epoch 973, Loss: 3.3631067276000977, Final Batch Loss: 0.6831358671188354\n",
      "Epoch 974, Loss: 3.37017160654068, Final Batch Loss: 0.6677013039588928\n",
      "Epoch 975, Loss: 3.3249586820602417, Final Batch Loss: 0.718325138092041\n",
      "Epoch 976, Loss: 3.3939713835716248, Final Batch Loss: 0.654261589050293\n",
      "Epoch 977, Loss: 3.348782777786255, Final Batch Loss: 0.5574729442596436\n",
      "Epoch 978, Loss: 3.205143988132477, Final Batch Loss: 0.6767041683197021\n",
      "Epoch 979, Loss: 3.2695987820625305, Final Batch Loss: 0.6224804520606995\n",
      "Epoch 980, Loss: 3.1220195293426514, Final Batch Loss: 0.5863444805145264\n",
      "Epoch 981, Loss: 3.1709747314453125, Final Batch Loss: 0.6032619476318359\n",
      "Epoch 982, Loss: 3.3256067037582397, Final Batch Loss: 0.7353705167770386\n",
      "Epoch 983, Loss: 3.268769860267639, Final Batch Loss: 0.6575977206230164\n",
      "Epoch 984, Loss: 3.2960551381111145, Final Batch Loss: 0.5609205365180969\n",
      "Epoch 985, Loss: 3.318251669406891, Final Batch Loss: 0.7841377258300781\n",
      "Epoch 986, Loss: 3.23361337184906, Final Batch Loss: 0.6326252818107605\n",
      "Epoch 987, Loss: 3.4957053661346436, Final Batch Loss: 0.7379922270774841\n",
      "Epoch 988, Loss: 3.3027427196502686, Final Batch Loss: 0.719756543636322\n",
      "Epoch 989, Loss: 3.298163652420044, Final Batch Loss: 0.6489885449409485\n",
      "Epoch 990, Loss: 3.345325529575348, Final Batch Loss: 0.646896481513977\n",
      "Epoch 991, Loss: 3.168788194656372, Final Batch Loss: 0.6024074554443359\n",
      "Epoch 992, Loss: 3.25264710187912, Final Batch Loss: 0.6469117999076843\n",
      "Epoch 993, Loss: 3.1578879356384277, Final Batch Loss: 0.7115110754966736\n",
      "Epoch 994, Loss: 3.226472318172455, Final Batch Loss: 0.6116169691085815\n",
      "Epoch 995, Loss: 3.31106036901474, Final Batch Loss: 0.747805655002594\n",
      "Epoch 996, Loss: 3.23343288898468, Final Batch Loss: 0.5543785691261292\n",
      "Epoch 997, Loss: 3.1132118105888367, Final Batch Loss: 0.6588117480278015\n",
      "Epoch 998, Loss: 3.2885448932647705, Final Batch Loss: 0.675858736038208\n",
      "Epoch 999, Loss: 3.1826024651527405, Final Batch Loss: 0.7289800047874451\n",
      "Epoch 1000, Loss: 3.001735180616379, Final Batch Loss: 0.48836490511894226\n",
      "Epoch 1001, Loss: 3.1738837361335754, Final Batch Loss: 0.6455997228622437\n",
      "Epoch 1002, Loss: 3.1180431842803955, Final Batch Loss: 0.6229829788208008\n",
      "Epoch 1003, Loss: 3.19573712348938, Final Batch Loss: 0.5125402212142944\n",
      "Epoch 1004, Loss: 3.114186078310013, Final Batch Loss: 0.4736923277378082\n",
      "Epoch 1005, Loss: 3.2735025882720947, Final Batch Loss: 0.5959124565124512\n",
      "Epoch 1006, Loss: 3.2634880542755127, Final Batch Loss: 0.6204048991203308\n",
      "Epoch 1007, Loss: 3.2878714203834534, Final Batch Loss: 0.5855199098587036\n",
      "Epoch 1008, Loss: 3.594949424266815, Final Batch Loss: 0.7849398255348206\n",
      "Epoch 1009, Loss: 3.2766894698143005, Final Batch Loss: 0.643779456615448\n",
      "Epoch 1010, Loss: 3.2575994729995728, Final Batch Loss: 0.6705811023712158\n",
      "Epoch 1011, Loss: 3.2408225536346436, Final Batch Loss: 0.6141229867935181\n",
      "Epoch 1012, Loss: 3.354142725467682, Final Batch Loss: 0.6956204175949097\n",
      "Epoch 1013, Loss: 3.409248471260071, Final Batch Loss: 0.6887996196746826\n",
      "Epoch 1014, Loss: 3.3246355056762695, Final Batch Loss: 0.7479037046432495\n",
      "Epoch 1015, Loss: 3.362815499305725, Final Batch Loss: 0.63328617811203\n",
      "Epoch 1016, Loss: 3.2539817094802856, Final Batch Loss: 0.6459640860557556\n",
      "Epoch 1017, Loss: 3.433492124080658, Final Batch Loss: 0.6845570802688599\n",
      "Epoch 1018, Loss: 3.3653897643089294, Final Batch Loss: 0.7643185257911682\n",
      "Epoch 1019, Loss: 3.104861259460449, Final Batch Loss: 0.6122714877128601\n",
      "Epoch 1020, Loss: 3.3000709414482117, Final Batch Loss: 0.6208407282829285\n",
      "Epoch 1021, Loss: 3.34128600358963, Final Batch Loss: 0.6415570378303528\n",
      "Epoch 1022, Loss: 3.4050574898719788, Final Batch Loss: 0.8613722324371338\n",
      "Epoch 1023, Loss: 3.35120952129364, Final Batch Loss: 0.7046999931335449\n",
      "Epoch 1024, Loss: 3.437496840953827, Final Batch Loss: 0.8149039149284363\n",
      "Epoch 1025, Loss: 3.3282952308654785, Final Batch Loss: 0.7012543082237244\n",
      "Epoch 1026, Loss: 3.1177533864974976, Final Batch Loss: 0.5614262819290161\n",
      "Epoch 1027, Loss: 3.2625668048858643, Final Batch Loss: 0.5508836507797241\n",
      "Epoch 1028, Loss: 3.4763259291648865, Final Batch Loss: 0.7714938521385193\n",
      "Epoch 1029, Loss: 3.2791961431503296, Final Batch Loss: 0.6886500716209412\n",
      "Epoch 1030, Loss: 3.137787103652954, Final Batch Loss: 0.5994844436645508\n",
      "Epoch 1031, Loss: 3.3994836807250977, Final Batch Loss: 0.7087440490722656\n",
      "Epoch 1032, Loss: 3.1825506687164307, Final Batch Loss: 0.6029118895530701\n",
      "Epoch 1033, Loss: 3.371464192867279, Final Batch Loss: 0.7417744398117065\n",
      "Epoch 1034, Loss: 3.048932135105133, Final Batch Loss: 0.449737548828125\n",
      "Epoch 1035, Loss: 2.9542428851127625, Final Batch Loss: 0.6160193085670471\n",
      "Epoch 1036, Loss: 3.2784553170204163, Final Batch Loss: 0.6990285515785217\n",
      "Epoch 1037, Loss: 3.306727945804596, Final Batch Loss: 0.6973986625671387\n",
      "Epoch 1038, Loss: 3.0405156016349792, Final Batch Loss: 0.5973255634307861\n",
      "Epoch 1039, Loss: 3.305763304233551, Final Batch Loss: 0.7699313163757324\n",
      "Epoch 1040, Loss: 3.2153123021125793, Final Batch Loss: 0.6581203937530518\n",
      "Epoch 1041, Loss: 3.111528217792511, Final Batch Loss: 0.5735116600990295\n",
      "Epoch 1042, Loss: 3.239887058734894, Final Batch Loss: 0.7811225056648254\n",
      "Epoch 1043, Loss: 3.1329955458641052, Final Batch Loss: 0.679345428943634\n",
      "Epoch 1044, Loss: 3.190583288669586, Final Batch Loss: 0.6093300580978394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045, Loss: 3.247048497200012, Final Batch Loss: 0.601662278175354\n",
      "Epoch 1046, Loss: 3.370275139808655, Final Batch Loss: 0.7282291054725647\n",
      "Epoch 1047, Loss: 3.2002170085906982, Final Batch Loss: 0.6332025527954102\n",
      "Epoch 1048, Loss: 2.9520262479782104, Final Batch Loss: 0.5616549849510193\n",
      "Epoch 1049, Loss: 3.2197946310043335, Final Batch Loss: 0.5956940054893494\n",
      "Epoch 1050, Loss: 3.3738386034965515, Final Batch Loss: 0.7549155354499817\n",
      "Epoch 1051, Loss: 3.133076846599579, Final Batch Loss: 0.5066009759902954\n",
      "Epoch 1052, Loss: 3.172830104827881, Final Batch Loss: 0.5865371227264404\n",
      "Epoch 1053, Loss: 3.1187922954559326, Final Batch Loss: 0.5733197331428528\n",
      "Epoch 1054, Loss: 3.2140374183654785, Final Batch Loss: 0.6043888330459595\n",
      "Epoch 1055, Loss: 3.039524018764496, Final Batch Loss: 0.5923495888710022\n",
      "Epoch 1056, Loss: 3.1326690912246704, Final Batch Loss: 0.5884765982627869\n",
      "Epoch 1057, Loss: 3.3332390189170837, Final Batch Loss: 0.5963205695152283\n",
      "Epoch 1058, Loss: 3.144639015197754, Final Batch Loss: 0.5386912822723389\n",
      "Epoch 1059, Loss: 3.115535020828247, Final Batch Loss: 0.5433751344680786\n",
      "Epoch 1060, Loss: 3.1246917843818665, Final Batch Loss: 0.7321020364761353\n",
      "Epoch 1061, Loss: 3.0968538522720337, Final Batch Loss: 0.7321277260780334\n",
      "Epoch 1062, Loss: 3.20443457365036, Final Batch Loss: 0.6411597728729248\n",
      "Epoch 1063, Loss: 3.3519745469093323, Final Batch Loss: 0.6952086091041565\n",
      "Epoch 1064, Loss: 3.3177680373191833, Final Batch Loss: 0.6580480933189392\n",
      "Epoch 1065, Loss: 3.1941863298416138, Final Batch Loss: 0.6300021409988403\n",
      "Epoch 1066, Loss: 3.199080765247345, Final Batch Loss: 0.529106616973877\n",
      "Epoch 1067, Loss: 3.329983174800873, Final Batch Loss: 0.7733442187309265\n",
      "Epoch 1068, Loss: 3.210171699523926, Final Batch Loss: 0.622072696685791\n",
      "Epoch 1069, Loss: 3.2888049483299255, Final Batch Loss: 0.6296964883804321\n",
      "Epoch 1070, Loss: 3.139007806777954, Final Batch Loss: 0.6840565800666809\n",
      "Epoch 1071, Loss: 3.1031869649887085, Final Batch Loss: 0.6293125152587891\n",
      "Epoch 1072, Loss: 3.1576226949691772, Final Batch Loss: 0.6992034316062927\n",
      "Epoch 1073, Loss: 3.183198571205139, Final Batch Loss: 0.6731077432632446\n",
      "Epoch 1074, Loss: 3.017256736755371, Final Batch Loss: 0.5719337463378906\n",
      "Epoch 1075, Loss: 3.2295221090316772, Final Batch Loss: 0.6612003445625305\n",
      "Epoch 1076, Loss: 3.2008877396583557, Final Batch Loss: 0.6864649057388306\n",
      "Epoch 1077, Loss: 3.109188199043274, Final Batch Loss: 0.6248747110366821\n",
      "Epoch 1078, Loss: 3.3078964948654175, Final Batch Loss: 0.7069579362869263\n",
      "Epoch 1079, Loss: 2.9874133467674255, Final Batch Loss: 0.562740683555603\n",
      "Epoch 1080, Loss: 3.152087152004242, Final Batch Loss: 0.6944904923439026\n",
      "Epoch 1081, Loss: 3.2169915437698364, Final Batch Loss: 0.6884390711784363\n",
      "Epoch 1082, Loss: 3.4261037707328796, Final Batch Loss: 0.8792471289634705\n",
      "Epoch 1083, Loss: 3.3379480838775635, Final Batch Loss: 0.7129347920417786\n",
      "Epoch 1084, Loss: 3.165271580219269, Final Batch Loss: 0.7066277265548706\n",
      "Epoch 1085, Loss: 3.2189828157424927, Final Batch Loss: 0.7551682591438293\n",
      "Epoch 1086, Loss: 3.189298391342163, Final Batch Loss: 0.666845977306366\n",
      "Epoch 1087, Loss: 3.197497546672821, Final Batch Loss: 0.6999465227127075\n",
      "Epoch 1088, Loss: 3.1956136226654053, Final Batch Loss: 0.6155335307121277\n",
      "Epoch 1089, Loss: 3.2031806111335754, Final Batch Loss: 0.6335652470588684\n",
      "Epoch 1090, Loss: 3.027166724205017, Final Batch Loss: 0.6283623576164246\n",
      "Epoch 1091, Loss: 3.0843722820281982, Final Batch Loss: 0.5630536079406738\n",
      "Epoch 1092, Loss: 3.1219722628593445, Final Batch Loss: 0.6190863251686096\n",
      "Epoch 1093, Loss: 3.180568516254425, Final Batch Loss: 0.6320905089378357\n",
      "Epoch 1094, Loss: 3.123063385486603, Final Batch Loss: 0.6071037650108337\n",
      "Epoch 1095, Loss: 3.07491934299469, Final Batch Loss: 0.6296595335006714\n",
      "Epoch 1096, Loss: 3.2595693469047546, Final Batch Loss: 0.7039622068405151\n",
      "Epoch 1097, Loss: 3.1457417607307434, Final Batch Loss: 0.5411713719367981\n",
      "Epoch 1098, Loss: 3.1814074516296387, Final Batch Loss: 0.6193023324012756\n",
      "Epoch 1099, Loss: 3.278118848800659, Final Batch Loss: 0.6006763577461243\n",
      "Epoch 1100, Loss: 3.120492458343506, Final Batch Loss: 0.6669328808784485\n",
      "Epoch 1101, Loss: 3.1341423988342285, Final Batch Loss: 0.7285096645355225\n",
      "Epoch 1102, Loss: 3.2871747612953186, Final Batch Loss: 0.7162687182426453\n",
      "Epoch 1103, Loss: 3.340965688228607, Final Batch Loss: 0.5868563652038574\n",
      "Epoch 1104, Loss: 3.323727548122406, Final Batch Loss: 0.7277318239212036\n",
      "Epoch 1105, Loss: 3.175608456134796, Final Batch Loss: 0.6344438791275024\n",
      "Epoch 1106, Loss: 2.9951908588409424, Final Batch Loss: 0.5817849636077881\n",
      "Epoch 1107, Loss: 3.0136302709579468, Final Batch Loss: 0.585801362991333\n",
      "Epoch 1108, Loss: 3.1836345195770264, Final Batch Loss: 0.7347843050956726\n",
      "Epoch 1109, Loss: 3.1230230927467346, Final Batch Loss: 0.5061825513839722\n",
      "Epoch 1110, Loss: 2.9705770611763, Final Batch Loss: 0.5322701334953308\n",
      "Epoch 1111, Loss: 3.12443608045578, Final Batch Loss: 0.6147110462188721\n",
      "Epoch 1112, Loss: 3.1163777709007263, Final Batch Loss: 0.6032043695449829\n",
      "Epoch 1113, Loss: 3.1965177059173584, Final Batch Loss: 0.6453810930252075\n",
      "Epoch 1114, Loss: 3.367062270641327, Final Batch Loss: 0.640986442565918\n",
      "Epoch 1115, Loss: 3.12515127658844, Final Batch Loss: 0.5756893754005432\n",
      "Epoch 1116, Loss: 3.239876925945282, Final Batch Loss: 0.5802000164985657\n",
      "Epoch 1117, Loss: 3.192019522190094, Final Batch Loss: 0.6525204181671143\n",
      "Epoch 1118, Loss: 3.2513065338134766, Final Batch Loss: 0.8262262344360352\n",
      "Epoch 1119, Loss: 3.3369645476341248, Final Batch Loss: 0.6374269127845764\n",
      "Epoch 1120, Loss: 3.2304659485816956, Final Batch Loss: 0.7305000424385071\n",
      "Epoch 1121, Loss: 3.0952712893486023, Final Batch Loss: 0.5803486704826355\n",
      "Epoch 1122, Loss: 3.11296546459198, Final Batch Loss: 0.6243611574172974\n",
      "Epoch 1123, Loss: 3.0528870820999146, Final Batch Loss: 0.6542887091636658\n",
      "Epoch 1124, Loss: 3.1254902482032776, Final Batch Loss: 0.6922142505645752\n",
      "Epoch 1125, Loss: 3.219650983810425, Final Batch Loss: 0.7105808258056641\n",
      "Epoch 1126, Loss: 3.286427319049835, Final Batch Loss: 0.754060685634613\n",
      "Epoch 1127, Loss: 3.029206156730652, Final Batch Loss: 0.6863510012626648\n",
      "Epoch 1128, Loss: 2.9658623039722443, Final Batch Loss: 0.4771740138530731\n",
      "Epoch 1129, Loss: 3.2312877774238586, Final Batch Loss: 0.6372891664505005\n",
      "Epoch 1130, Loss: 3.1326498985290527, Final Batch Loss: 0.5701720118522644\n",
      "Epoch 1131, Loss: 3.115880012512207, Final Batch Loss: 0.7877479195594788\n",
      "Epoch 1132, Loss: 3.0301041305065155, Final Batch Loss: 0.6489081382751465\n",
      "Epoch 1133, Loss: 3.140827000141144, Final Batch Loss: 0.51410973072052\n",
      "Epoch 1134, Loss: 3.053716778755188, Final Batch Loss: 0.544552206993103\n",
      "Epoch 1135, Loss: 3.211736261844635, Final Batch Loss: 0.6425005793571472\n",
      "Epoch 1136, Loss: 3.110637903213501, Final Batch Loss: 0.6491484642028809\n",
      "Epoch 1137, Loss: 3.1913498640060425, Final Batch Loss: 0.5335190892219543\n",
      "Epoch 1138, Loss: 3.159150242805481, Final Batch Loss: 0.5632690191268921\n",
      "Epoch 1139, Loss: 3.044301152229309, Final Batch Loss: 0.6270156502723694\n",
      "Epoch 1140, Loss: 3.0899681448936462, Final Batch Loss: 0.6710684299468994\n",
      "Epoch 1141, Loss: 3.205862879753113, Final Batch Loss: 0.6460997462272644\n",
      "Epoch 1142, Loss: 3.1061328649520874, Final Batch Loss: 0.7937391996383667\n",
      "Epoch 1143, Loss: 3.329002618789673, Final Batch Loss: 0.7539627552032471\n",
      "Epoch 1144, Loss: 3.152548909187317, Final Batch Loss: 0.6629248261451721\n",
      "Epoch 1145, Loss: 3.1373185515403748, Final Batch Loss: 0.5558581352233887\n",
      "Epoch 1146, Loss: 3.0566096901893616, Final Batch Loss: 0.5594935417175293\n",
      "Epoch 1147, Loss: 3.108277976512909, Final Batch Loss: 0.7289944291114807\n",
      "Epoch 1148, Loss: 3.0629022121429443, Final Batch Loss: 0.6736204028129578\n",
      "Epoch 1149, Loss: 3.192882716655731, Final Batch Loss: 0.6869483590126038\n",
      "Epoch 1150, Loss: 3.20145320892334, Final Batch Loss: 0.6248854994773865\n",
      "Epoch 1151, Loss: 3.088652789592743, Final Batch Loss: 0.5807822942733765\n",
      "Epoch 1152, Loss: 3.0141372680664062, Final Batch Loss: 0.5748387575149536\n",
      "Epoch 1153, Loss: 3.179944396018982, Final Batch Loss: 0.5780006647109985\n",
      "Epoch 1154, Loss: 2.9883331060409546, Final Batch Loss: 0.5135509371757507\n",
      "Epoch 1155, Loss: 3.0724501609802246, Final Batch Loss: 0.5821881294250488\n",
      "Epoch 1156, Loss: 3.060725688934326, Final Batch Loss: 0.6494240164756775\n",
      "Epoch 1157, Loss: 3.0123881101608276, Final Batch Loss: 0.6128737330436707\n",
      "Epoch 1158, Loss: 3.2521438598632812, Final Batch Loss: 0.6151207089424133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1159, Loss: 3.080141603946686, Final Batch Loss: 0.5939503908157349\n",
      "Epoch 1160, Loss: 3.096251904964447, Final Batch Loss: 0.5733873844146729\n",
      "Epoch 1161, Loss: 3.0090537667274475, Final Batch Loss: 0.5242862701416016\n",
      "Epoch 1162, Loss: 3.0377994775772095, Final Batch Loss: 0.5839905738830566\n",
      "Epoch 1163, Loss: 3.0288278460502625, Final Batch Loss: 0.5680884718894958\n",
      "Epoch 1164, Loss: 2.9795899391174316, Final Batch Loss: 0.5686209797859192\n",
      "Epoch 1165, Loss: 2.8963552713394165, Final Batch Loss: 0.5121784210205078\n",
      "Epoch 1166, Loss: 3.143420100212097, Final Batch Loss: 0.6960244178771973\n",
      "Epoch 1167, Loss: 3.0081896781921387, Final Batch Loss: 0.6900632381439209\n",
      "Epoch 1168, Loss: 2.9568450450897217, Final Batch Loss: 0.6579938530921936\n",
      "Epoch 1169, Loss: 3.1635488867759705, Final Batch Loss: 0.5145447254180908\n",
      "Epoch 1170, Loss: 3.2038521766662598, Final Batch Loss: 0.6374359726905823\n",
      "Epoch 1171, Loss: 3.0399805307388306, Final Batch Loss: 0.6721243858337402\n",
      "Epoch 1172, Loss: 3.1700058579444885, Final Batch Loss: 0.7310822606086731\n",
      "Epoch 1173, Loss: 2.996568262577057, Final Batch Loss: 0.5570966601371765\n",
      "Epoch 1174, Loss: 3.160927891731262, Final Batch Loss: 0.6296491622924805\n",
      "Epoch 1175, Loss: 2.915607213973999, Final Batch Loss: 0.5540418028831482\n",
      "Epoch 1176, Loss: 3.0460572838783264, Final Batch Loss: 0.6670849323272705\n",
      "Epoch 1177, Loss: 2.9548999965190887, Final Batch Loss: 0.5964415073394775\n",
      "Epoch 1178, Loss: 3.191994369029999, Final Batch Loss: 0.6645732522010803\n",
      "Epoch 1179, Loss: 3.0159380435943604, Final Batch Loss: 0.6072606444358826\n",
      "Epoch 1180, Loss: 2.8403390645980835, Final Batch Loss: 0.6113346219062805\n",
      "Epoch 1181, Loss: 3.127002775669098, Final Batch Loss: 0.788889467716217\n",
      "Epoch 1182, Loss: 3.07756769657135, Final Batch Loss: 0.6580304503440857\n",
      "Epoch 1183, Loss: 3.2278544306755066, Final Batch Loss: 0.5982033610343933\n",
      "Epoch 1184, Loss: 3.0660004019737244, Final Batch Loss: 0.5559445023536682\n",
      "Epoch 1185, Loss: 2.9160489439964294, Final Batch Loss: 0.5838728547096252\n",
      "Epoch 1186, Loss: 3.161478340625763, Final Batch Loss: 0.5395680069923401\n",
      "Epoch 1187, Loss: 3.108791172504425, Final Batch Loss: 0.6694413423538208\n",
      "Epoch 1188, Loss: 2.9419866800308228, Final Batch Loss: 0.6023223400115967\n",
      "Epoch 1189, Loss: 3.1359347701072693, Final Batch Loss: 0.6783430576324463\n",
      "Epoch 1190, Loss: 3.0724818110466003, Final Batch Loss: 0.5105502605438232\n",
      "Epoch 1191, Loss: 2.9449554085731506, Final Batch Loss: 0.5830399394035339\n",
      "Epoch 1192, Loss: 3.31627357006073, Final Batch Loss: 0.7207494378089905\n",
      "Epoch 1193, Loss: 3.150413930416107, Final Batch Loss: 0.6972790956497192\n",
      "Epoch 1194, Loss: 2.9131693840026855, Final Batch Loss: 0.5682684779167175\n",
      "Epoch 1195, Loss: 3.0680565237998962, Final Batch Loss: 0.58183354139328\n",
      "Epoch 1196, Loss: 3.1619738936424255, Final Batch Loss: 0.6433659791946411\n",
      "Epoch 1197, Loss: 3.354770839214325, Final Batch Loss: 0.867108941078186\n",
      "Epoch 1198, Loss: 3.0468982458114624, Final Batch Loss: 0.5370074510574341\n",
      "Epoch 1199, Loss: 3.2980681657791138, Final Batch Loss: 0.7247939705848694\n",
      "Epoch 1200, Loss: 3.307541072368622, Final Batch Loss: 0.8509343862533569\n",
      "Epoch 1201, Loss: 2.990459442138672, Final Batch Loss: 0.6972762942314148\n",
      "Epoch 1202, Loss: 3.0016528367996216, Final Batch Loss: 0.6064302325248718\n",
      "Epoch 1203, Loss: 3.123687207698822, Final Batch Loss: 0.6594982743263245\n",
      "Epoch 1204, Loss: 3.232088804244995, Final Batch Loss: 0.7076038718223572\n",
      "Epoch 1205, Loss: 3.1049916744232178, Final Batch Loss: 0.6283636689186096\n",
      "Epoch 1206, Loss: 3.258679986000061, Final Batch Loss: 0.856357991695404\n",
      "Epoch 1207, Loss: 3.174862802028656, Final Batch Loss: 0.693671703338623\n",
      "Epoch 1208, Loss: 2.9854419231414795, Final Batch Loss: 0.5705751776695251\n",
      "Epoch 1209, Loss: 3.166479229927063, Final Batch Loss: 0.6188204884529114\n",
      "Epoch 1210, Loss: 3.099212110042572, Final Batch Loss: 0.5212474465370178\n",
      "Epoch 1211, Loss: 3.019236385822296, Final Batch Loss: 0.5813376307487488\n",
      "Epoch 1212, Loss: 3.094618320465088, Final Batch Loss: 0.638310432434082\n",
      "Epoch 1213, Loss: 3.0440419912338257, Final Batch Loss: 0.5591751337051392\n",
      "Epoch 1214, Loss: 3.08502596616745, Final Batch Loss: 0.6063414812088013\n",
      "Epoch 1215, Loss: 3.0749374628067017, Final Batch Loss: 0.6031757593154907\n",
      "Epoch 1216, Loss: 3.0831885933876038, Final Batch Loss: 0.6742177605628967\n",
      "Epoch 1217, Loss: 3.0174863934516907, Final Batch Loss: 0.4902231693267822\n",
      "Epoch 1218, Loss: 3.0610878467559814, Final Batch Loss: 0.6589308381080627\n",
      "Epoch 1219, Loss: 3.0378799736499786, Final Batch Loss: 0.4807700216770172\n",
      "Epoch 1220, Loss: 3.196146845817566, Final Batch Loss: 0.7054324150085449\n",
      "Epoch 1221, Loss: 3.072978138923645, Final Batch Loss: 0.5497628450393677\n",
      "Epoch 1222, Loss: 2.9871939420700073, Final Batch Loss: 0.631892204284668\n",
      "Epoch 1223, Loss: 3.1849461793899536, Final Batch Loss: 0.6882591247558594\n",
      "Epoch 1224, Loss: 3.1140971183776855, Final Batch Loss: 0.7012478113174438\n",
      "Epoch 1225, Loss: 3.0617271065711975, Final Batch Loss: 0.6648619174957275\n",
      "Epoch 1226, Loss: 3.0543899536132812, Final Batch Loss: 0.6321824193000793\n",
      "Epoch 1227, Loss: 3.1079643964767456, Final Batch Loss: 0.5082696676254272\n",
      "Epoch 1228, Loss: 3.2566652297973633, Final Batch Loss: 0.642882764339447\n",
      "Epoch 1229, Loss: 3.0097159147262573, Final Batch Loss: 0.6027151346206665\n",
      "Epoch 1230, Loss: 3.0769218802452087, Final Batch Loss: 0.6997125148773193\n",
      "Epoch 1231, Loss: 3.130014955997467, Final Batch Loss: 0.6553261280059814\n",
      "Epoch 1232, Loss: 3.0758707523345947, Final Batch Loss: 0.6194262504577637\n",
      "Epoch 1233, Loss: 3.0113450288772583, Final Batch Loss: 0.6177801489830017\n",
      "Epoch 1234, Loss: 2.8929882049560547, Final Batch Loss: 0.5010749101638794\n",
      "Epoch 1235, Loss: 3.003409206867218, Final Batch Loss: 0.6321147084236145\n",
      "Epoch 1236, Loss: 2.921097993850708, Final Batch Loss: 0.5445998907089233\n",
      "Epoch 1237, Loss: 2.95622980594635, Final Batch Loss: 0.5380614995956421\n",
      "Epoch 1238, Loss: 3.2173181772232056, Final Batch Loss: 0.5590737462043762\n",
      "Epoch 1239, Loss: 3.0703312754631042, Final Batch Loss: 0.5789128541946411\n",
      "Epoch 1240, Loss: 2.9971172213554382, Final Batch Loss: 0.6346471309661865\n",
      "Epoch 1241, Loss: 2.8538549542427063, Final Batch Loss: 0.6426681280136108\n",
      "Epoch 1242, Loss: 3.1566933393478394, Final Batch Loss: 0.7056271433830261\n",
      "Epoch 1243, Loss: 2.941240906715393, Final Batch Loss: 0.5938286185264587\n",
      "Epoch 1244, Loss: 2.944830060005188, Final Batch Loss: 0.656588613986969\n",
      "Epoch 1245, Loss: 3.006849765777588, Final Batch Loss: 0.6035547256469727\n",
      "Epoch 1246, Loss: 3.0964523553848267, Final Batch Loss: 0.6189191341400146\n",
      "Epoch 1247, Loss: 3.1007919907569885, Final Batch Loss: 0.6578235030174255\n",
      "Epoch 1248, Loss: 2.9974706768989563, Final Batch Loss: 0.609326958656311\n",
      "Epoch 1249, Loss: 3.0806884169578552, Final Batch Loss: 0.5473442077636719\n",
      "Epoch 1250, Loss: 2.9083409905433655, Final Batch Loss: 0.5609405636787415\n",
      "Epoch 1251, Loss: 2.9942721128463745, Final Batch Loss: 0.7002449035644531\n",
      "Epoch 1252, Loss: 3.035378098487854, Final Batch Loss: 0.5925877690315247\n",
      "Epoch 1253, Loss: 3.1012962460517883, Final Batch Loss: 0.574524462223053\n",
      "Epoch 1254, Loss: 3.0917277336120605, Final Batch Loss: 0.7712677121162415\n",
      "Epoch 1255, Loss: 2.9937813878059387, Final Batch Loss: 0.6353402137756348\n",
      "Epoch 1256, Loss: 3.0581758320331573, Final Batch Loss: 0.7091951966285706\n",
      "Epoch 1257, Loss: 3.0269776582717896, Final Batch Loss: 0.5468065738677979\n",
      "Epoch 1258, Loss: 3.0974971652030945, Final Batch Loss: 0.6829946041107178\n",
      "Epoch 1259, Loss: 3.217836916446686, Final Batch Loss: 0.6335659027099609\n",
      "Epoch 1260, Loss: 2.9677006900310516, Final Batch Loss: 0.4970575273036957\n",
      "Epoch 1261, Loss: 3.086264908313751, Final Batch Loss: 0.6988275647163391\n",
      "Epoch 1262, Loss: 3.0614519119262695, Final Batch Loss: 0.6301935911178589\n",
      "Epoch 1263, Loss: 2.9453205466270447, Final Batch Loss: 0.5774897336959839\n",
      "Epoch 1264, Loss: 3.0559824109077454, Final Batch Loss: 0.6227352023124695\n",
      "Epoch 1265, Loss: 3.2058791518211365, Final Batch Loss: 0.6303566694259644\n",
      "Epoch 1266, Loss: 3.048125445842743, Final Batch Loss: 0.71232008934021\n",
      "Epoch 1267, Loss: 3.1102230548858643, Final Batch Loss: 0.6832097172737122\n",
      "Epoch 1268, Loss: 3.003836214542389, Final Batch Loss: 0.5411176085472107\n",
      "Epoch 1269, Loss: 2.9534971117973328, Final Batch Loss: 0.6289637088775635\n",
      "Epoch 1270, Loss: 2.892469346523285, Final Batch Loss: 0.5490949749946594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1271, Loss: 3.052123486995697, Final Batch Loss: 0.5258373022079468\n",
      "Epoch 1272, Loss: 2.984253227710724, Final Batch Loss: 0.6241698861122131\n",
      "Epoch 1273, Loss: 2.969121754169464, Final Batch Loss: 0.5054040551185608\n",
      "Epoch 1274, Loss: 3.1741207242012024, Final Batch Loss: 0.6069180965423584\n",
      "Epoch 1275, Loss: 3.064134120941162, Final Batch Loss: 0.6381357908248901\n",
      "Epoch 1276, Loss: 2.8611824214458466, Final Batch Loss: 0.45277300477027893\n",
      "Epoch 1277, Loss: 3.0540207028388977, Final Batch Loss: 0.6235728859901428\n",
      "Epoch 1278, Loss: 3.0971524715423584, Final Batch Loss: 0.6249649524688721\n",
      "Epoch 1279, Loss: 3.0719639658927917, Final Batch Loss: 0.6511301398277283\n",
      "Epoch 1280, Loss: 2.9277742505073547, Final Batch Loss: 0.586285412311554\n",
      "Epoch 1281, Loss: 2.844690680503845, Final Batch Loss: 0.49992287158966064\n",
      "Epoch 1282, Loss: 3.2693651914596558, Final Batch Loss: 0.7526760101318359\n",
      "Epoch 1283, Loss: 3.106670558452606, Final Batch Loss: 0.6444646120071411\n",
      "Epoch 1284, Loss: 3.0788456201553345, Final Batch Loss: 0.5575096607208252\n",
      "Epoch 1285, Loss: 3.156262755393982, Final Batch Loss: 0.7297390103340149\n",
      "Epoch 1286, Loss: 3.0250266194343567, Final Batch Loss: 0.6404739618301392\n",
      "Epoch 1287, Loss: 3.004879117012024, Final Batch Loss: 0.6303175091743469\n",
      "Epoch 1288, Loss: 2.9693744778633118, Final Batch Loss: 0.6845163702964783\n",
      "Epoch 1289, Loss: 2.9404039680957794, Final Batch Loss: 0.5915020108222961\n",
      "Epoch 1290, Loss: 2.833684116601944, Final Batch Loss: 0.4738099277019501\n",
      "Epoch 1291, Loss: 2.9616076946258545, Final Batch Loss: 0.5026113390922546\n",
      "Epoch 1292, Loss: 3.0657402873039246, Final Batch Loss: 0.6444180607795715\n",
      "Epoch 1293, Loss: 3.0552209615707397, Final Batch Loss: 0.6859420537948608\n",
      "Epoch 1294, Loss: 3.1414774656295776, Final Batch Loss: 0.6932617425918579\n",
      "Epoch 1295, Loss: 3.0273728370666504, Final Batch Loss: 0.6142758131027222\n",
      "Epoch 1296, Loss: 3.1071842908859253, Final Batch Loss: 0.5619929432868958\n",
      "Epoch 1297, Loss: 3.0766993165016174, Final Batch Loss: 0.543168306350708\n",
      "Epoch 1298, Loss: 3.031536877155304, Final Batch Loss: 0.5756009221076965\n",
      "Epoch 1299, Loss: 3.0213921070098877, Final Batch Loss: 0.5820690393447876\n",
      "Epoch 1300, Loss: 2.9383895993232727, Final Batch Loss: 0.5799055695533752\n",
      "Epoch 1301, Loss: 3.06399405002594, Final Batch Loss: 0.5634766221046448\n",
      "Epoch 1302, Loss: 3.010182201862335, Final Batch Loss: 0.5183401107788086\n",
      "Epoch 1303, Loss: 3.0276745557785034, Final Batch Loss: 0.5509671568870544\n",
      "Epoch 1304, Loss: 3.076305389404297, Final Batch Loss: 0.6382379531860352\n",
      "Epoch 1305, Loss: 2.9136162996292114, Final Batch Loss: 0.6416653394699097\n",
      "Epoch 1306, Loss: 3.0072560906410217, Final Batch Loss: 0.647936999797821\n",
      "Epoch 1307, Loss: 2.8036359548568726, Final Batch Loss: 0.5611673593521118\n",
      "Epoch 1308, Loss: 3.1781816482543945, Final Batch Loss: 0.7590550184249878\n",
      "Epoch 1309, Loss: 3.037061870098114, Final Batch Loss: 0.5721110701560974\n",
      "Epoch 1310, Loss: 3.079660952091217, Final Batch Loss: 0.6386510729789734\n",
      "Epoch 1311, Loss: 3.0944275856018066, Final Batch Loss: 0.5956885814666748\n",
      "Epoch 1312, Loss: 2.8572587966918945, Final Batch Loss: 0.5090057849884033\n",
      "Epoch 1313, Loss: 3.100013792514801, Final Batch Loss: 0.6382004618644714\n",
      "Epoch 1314, Loss: 3.2284330129623413, Final Batch Loss: 0.7168258428573608\n",
      "Epoch 1315, Loss: 2.961456775665283, Final Batch Loss: 0.5171219706535339\n",
      "Epoch 1316, Loss: 3.0991597771644592, Final Batch Loss: 0.6419217586517334\n",
      "Epoch 1317, Loss: 3.0404958724975586, Final Batch Loss: 0.6905863285064697\n",
      "Epoch 1318, Loss: 3.173540771007538, Final Batch Loss: 0.7183741927146912\n",
      "Epoch 1319, Loss: 3.0228846073150635, Final Batch Loss: 0.5283439755439758\n",
      "Epoch 1320, Loss: 3.107673168182373, Final Batch Loss: 0.5308545827865601\n",
      "Epoch 1321, Loss: 2.916193127632141, Final Batch Loss: 0.5608460307121277\n",
      "Epoch 1322, Loss: 2.9777639508247375, Final Batch Loss: 0.6265000700950623\n",
      "Epoch 1323, Loss: 2.897476077079773, Final Batch Loss: 0.5100271701812744\n",
      "Epoch 1324, Loss: 2.917189300060272, Final Batch Loss: 0.5526906847953796\n",
      "Epoch 1325, Loss: 3.1266002655029297, Final Batch Loss: 0.6762459874153137\n",
      "Epoch 1326, Loss: 2.9179142713546753, Final Batch Loss: 0.5973867774009705\n",
      "Epoch 1327, Loss: 2.8559221625328064, Final Batch Loss: 0.6055997610092163\n",
      "Epoch 1328, Loss: 3.0604363083839417, Final Batch Loss: 0.627884566783905\n",
      "Epoch 1329, Loss: 2.917251169681549, Final Batch Loss: 0.5529769659042358\n",
      "Epoch 1330, Loss: 2.9552448391914368, Final Batch Loss: 0.5586308836936951\n",
      "Epoch 1331, Loss: 2.8660157918930054, Final Batch Loss: 0.5620280504226685\n",
      "Epoch 1332, Loss: 3.0161536931991577, Final Batch Loss: 0.5938546061515808\n",
      "Epoch 1333, Loss: 3.2964011430740356, Final Batch Loss: 0.7439793348312378\n",
      "Epoch 1334, Loss: 2.8375329971313477, Final Batch Loss: 0.5586333870887756\n",
      "Epoch 1335, Loss: 2.986942946910858, Final Batch Loss: 0.5496191382408142\n",
      "Epoch 1336, Loss: 2.936700463294983, Final Batch Loss: 0.6051103472709656\n",
      "Epoch 1337, Loss: 3.128750264644623, Final Batch Loss: 0.7697098851203918\n",
      "Epoch 1338, Loss: 2.9740160703659058, Final Batch Loss: 0.5562697052955627\n",
      "Epoch 1339, Loss: 2.9810123443603516, Final Batch Loss: 0.6156944036483765\n",
      "Epoch 1340, Loss: 2.9906458854675293, Final Batch Loss: 0.5889509916305542\n",
      "Epoch 1341, Loss: 2.9635506868362427, Final Batch Loss: 0.5245084762573242\n",
      "Epoch 1342, Loss: 3.0149335861206055, Final Batch Loss: 0.542919933795929\n",
      "Epoch 1343, Loss: 3.011383891105652, Final Batch Loss: 0.6826601028442383\n",
      "Epoch 1344, Loss: 2.927003264427185, Final Batch Loss: 0.5403231382369995\n",
      "Epoch 1345, Loss: 2.9633457362651825, Final Batch Loss: 0.48943671584129333\n",
      "Epoch 1346, Loss: 2.989168345928192, Final Batch Loss: 0.6115676760673523\n",
      "Epoch 1347, Loss: 3.1164233684539795, Final Batch Loss: 0.608837902545929\n",
      "Epoch 1348, Loss: 3.156669318675995, Final Batch Loss: 0.6798089146614075\n",
      "Epoch 1349, Loss: 3.1826822757720947, Final Batch Loss: 0.6366756558418274\n",
      "Epoch 1350, Loss: 2.9518874883651733, Final Batch Loss: 0.5757277607917786\n",
      "Epoch 1351, Loss: 2.9079995155334473, Final Batch Loss: 0.5937768220901489\n",
      "Epoch 1352, Loss: 2.9601582884788513, Final Batch Loss: 0.6330340504646301\n",
      "Epoch 1353, Loss: 2.927516669034958, Final Batch Loss: 0.5458771586418152\n",
      "Epoch 1354, Loss: 2.986585319042206, Final Batch Loss: 0.5939633846282959\n",
      "Epoch 1355, Loss: 2.889630675315857, Final Batch Loss: 0.5884948968887329\n",
      "Epoch 1356, Loss: 3.0039010643959045, Final Batch Loss: 0.57695072889328\n",
      "Epoch 1357, Loss: 3.0604167580604553, Final Batch Loss: 0.593976616859436\n",
      "Epoch 1358, Loss: 3.129403054714203, Final Batch Loss: 0.580075740814209\n",
      "Epoch 1359, Loss: 2.9019524455070496, Final Batch Loss: 0.6280924677848816\n",
      "Epoch 1360, Loss: 2.933639407157898, Final Batch Loss: 0.5530624985694885\n",
      "Epoch 1361, Loss: 2.950048804283142, Final Batch Loss: 0.5712373852729797\n",
      "Epoch 1362, Loss: 3.0699458718299866, Final Batch Loss: 0.6169776320457458\n",
      "Epoch 1363, Loss: 2.9453200101852417, Final Batch Loss: 0.6833211183547974\n",
      "Epoch 1364, Loss: 3.0067664980888367, Final Batch Loss: 0.6161515712738037\n",
      "Epoch 1365, Loss: 2.8481112718582153, Final Batch Loss: 0.5023457407951355\n",
      "Epoch 1366, Loss: 3.051235854625702, Final Batch Loss: 0.5830199718475342\n",
      "Epoch 1367, Loss: 2.9276285469532013, Final Batch Loss: 0.452394038438797\n",
      "Epoch 1368, Loss: 2.937950760126114, Final Batch Loss: 0.6252859234809875\n",
      "Epoch 1369, Loss: 3.0638304352760315, Final Batch Loss: 0.650711715221405\n",
      "Epoch 1370, Loss: 3.0341915488243103, Final Batch Loss: 0.586783766746521\n",
      "Epoch 1371, Loss: 2.909127801656723, Final Batch Loss: 0.45960161089897156\n",
      "Epoch 1372, Loss: 3.006801187992096, Final Batch Loss: 0.605635941028595\n",
      "Epoch 1373, Loss: 2.947783052921295, Final Batch Loss: 0.5905285477638245\n",
      "Epoch 1374, Loss: 3.11442106962204, Final Batch Loss: 0.6324987411499023\n",
      "Epoch 1375, Loss: 2.974036455154419, Final Batch Loss: 0.5736932754516602\n",
      "Epoch 1376, Loss: 3.1520193815231323, Final Batch Loss: 0.5758438110351562\n",
      "Epoch 1377, Loss: 2.9880946278572083, Final Batch Loss: 0.5250669121742249\n",
      "Epoch 1378, Loss: 3.120965600013733, Final Batch Loss: 0.6614262461662292\n",
      "Epoch 1379, Loss: 2.917693257331848, Final Batch Loss: 0.5888413786888123\n",
      "Epoch 1380, Loss: 2.9533661603927612, Final Batch Loss: 0.5078461766242981\n",
      "Epoch 1381, Loss: 2.9791616201400757, Final Batch Loss: 0.6359871029853821\n",
      "Epoch 1382, Loss: 2.875017911195755, Final Batch Loss: 0.483694463968277\n",
      "Epoch 1383, Loss: 2.9370608031749725, Final Batch Loss: 0.47674360871315\n",
      "Epoch 1384, Loss: 3.0032286643981934, Final Batch Loss: 0.6475258469581604\n",
      "Epoch 1385, Loss: 2.9048081040382385, Final Batch Loss: 0.525773286819458\n",
      "Epoch 1386, Loss: 3.0971794724464417, Final Batch Loss: 0.6142436861991882\n",
      "Epoch 1387, Loss: 3.0782368779182434, Final Batch Loss: 0.6558170318603516\n",
      "Epoch 1388, Loss: 3.087160050868988, Final Batch Loss: 0.6620928049087524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1389, Loss: 2.892207980155945, Final Batch Loss: 0.5146052837371826\n",
      "Epoch 1390, Loss: 3.1564469933509827, Final Batch Loss: 0.5967764854431152\n",
      "Epoch 1391, Loss: 2.9167532324790955, Final Batch Loss: 0.6164648532867432\n",
      "Epoch 1392, Loss: 2.9777947664260864, Final Batch Loss: 0.6651707887649536\n",
      "Epoch 1393, Loss: 2.9473700523376465, Final Batch Loss: 0.5456033945083618\n",
      "Epoch 1394, Loss: 3.047348737716675, Final Batch Loss: 0.5874819159507751\n",
      "Epoch 1395, Loss: 2.9804775714874268, Final Batch Loss: 0.530968189239502\n",
      "Epoch 1396, Loss: 3.2424817085266113, Final Batch Loss: 0.7501211762428284\n",
      "Epoch 1397, Loss: 2.9230777621269226, Final Batch Loss: 0.6318094730377197\n",
      "Epoch 1398, Loss: 2.859391361474991, Final Batch Loss: 0.47406479716300964\n",
      "Epoch 1399, Loss: 3.034391164779663, Final Batch Loss: 0.6699417233467102\n",
      "Epoch 1400, Loss: 2.816489666700363, Final Batch Loss: 0.4874127209186554\n",
      "Epoch 1401, Loss: 2.9357057213783264, Final Batch Loss: 0.573258638381958\n",
      "Epoch 1402, Loss: 3.004321277141571, Final Batch Loss: 0.5939948558807373\n",
      "Epoch 1403, Loss: 3.0835278630256653, Final Batch Loss: 0.600281298160553\n",
      "Epoch 1404, Loss: 2.7874048948287964, Final Batch Loss: 0.4530315399169922\n",
      "Epoch 1405, Loss: 3.011649250984192, Final Batch Loss: 0.5557610988616943\n",
      "Epoch 1406, Loss: 2.8737861812114716, Final Batch Loss: 0.6768227815628052\n",
      "Epoch 1407, Loss: 3.0129625499248505, Final Batch Loss: 0.6446494460105896\n",
      "Epoch 1408, Loss: 2.9000500440597534, Final Batch Loss: 0.5988435745239258\n",
      "Epoch 1409, Loss: 3.171671450138092, Final Batch Loss: 0.6966397762298584\n",
      "Epoch 1410, Loss: 2.802748054265976, Final Batch Loss: 0.6600790619850159\n",
      "Epoch 1411, Loss: 2.984014868736267, Final Batch Loss: 0.6530376672744751\n",
      "Epoch 1412, Loss: 2.8095297515392303, Final Batch Loss: 0.5521270036697388\n",
      "Epoch 1413, Loss: 2.98058158159256, Final Batch Loss: 0.650961697101593\n",
      "Epoch 1414, Loss: 2.8813949823379517, Final Batch Loss: 0.5763920545578003\n",
      "Epoch 1415, Loss: 3.043666422367096, Final Batch Loss: 0.67991703748703\n",
      "Epoch 1416, Loss: 2.868560016155243, Final Batch Loss: 0.5224772691726685\n",
      "Epoch 1417, Loss: 2.8345876932144165, Final Batch Loss: 0.5593814849853516\n",
      "Epoch 1418, Loss: 2.876240372657776, Final Batch Loss: 0.5779686570167542\n",
      "Epoch 1419, Loss: 2.9341747760772705, Final Batch Loss: 0.6648389101028442\n",
      "Epoch 1420, Loss: 2.9694955945014954, Final Batch Loss: 0.5980679392814636\n",
      "Epoch 1421, Loss: 3.2423989176750183, Final Batch Loss: 0.5305959582328796\n",
      "Epoch 1422, Loss: 2.9725697934627533, Final Batch Loss: 0.46416500210762024\n",
      "Epoch 1423, Loss: 3.033489406108856, Final Batch Loss: 0.6241507530212402\n",
      "Epoch 1424, Loss: 2.9862714409828186, Final Batch Loss: 0.6171045303344727\n",
      "Epoch 1425, Loss: 2.970744550228119, Final Batch Loss: 0.6528782844543457\n",
      "Epoch 1426, Loss: 2.8119006752967834, Final Batch Loss: 0.44209742546081543\n",
      "Epoch 1427, Loss: 2.807710647583008, Final Batch Loss: 0.513709306716919\n",
      "Epoch 1428, Loss: 2.8528963327407837, Final Batch Loss: 0.5440362095832825\n",
      "Epoch 1429, Loss: 2.863119751214981, Final Batch Loss: 0.6075193881988525\n",
      "Epoch 1430, Loss: 3.076961398124695, Final Batch Loss: 0.6196208000183105\n",
      "Epoch 1431, Loss: 2.897319734096527, Final Batch Loss: 0.6179365515708923\n",
      "Epoch 1432, Loss: 3.018306076526642, Final Batch Loss: 0.5914598107337952\n",
      "Epoch 1433, Loss: 2.99867582321167, Final Batch Loss: 0.6013445854187012\n",
      "Epoch 1434, Loss: 3.000996172428131, Final Batch Loss: 0.599846601486206\n",
      "Epoch 1435, Loss: 2.943339467048645, Final Batch Loss: 0.5645069479942322\n",
      "Epoch 1436, Loss: 3.053610622882843, Final Batch Loss: 0.7466477751731873\n",
      "Epoch 1437, Loss: 2.8912190198898315, Final Batch Loss: 0.4826655387878418\n",
      "Epoch 1438, Loss: 3.2458767890930176, Final Batch Loss: 0.7334577441215515\n",
      "Epoch 1439, Loss: 3.1568546891212463, Final Batch Loss: 0.6046223044395447\n",
      "Epoch 1440, Loss: 3.005659580230713, Final Batch Loss: 0.6741300821304321\n",
      "Epoch 1441, Loss: 2.9924196004867554, Final Batch Loss: 0.6550668478012085\n",
      "Epoch 1442, Loss: 3.062613010406494, Final Batch Loss: 0.6298181414604187\n",
      "Epoch 1443, Loss: 2.8907023668289185, Final Batch Loss: 0.5588153600692749\n",
      "Epoch 1444, Loss: 2.8514784574508667, Final Batch Loss: 0.5044851303100586\n",
      "Epoch 1445, Loss: 3.0245324969291687, Final Batch Loss: 0.6535234451293945\n",
      "Epoch 1446, Loss: 2.9770542979240417, Final Batch Loss: 0.6071249842643738\n",
      "Epoch 1447, Loss: 2.898763597011566, Final Batch Loss: 0.5666924715042114\n",
      "Epoch 1448, Loss: 2.750857949256897, Final Batch Loss: 0.5263850688934326\n",
      "Epoch 1449, Loss: 2.9206345081329346, Final Batch Loss: 0.6216033101081848\n",
      "Epoch 1450, Loss: 2.7950552105903625, Final Batch Loss: 0.6083781719207764\n",
      "Epoch 1451, Loss: 2.906861901283264, Final Batch Loss: 0.5634244680404663\n",
      "Epoch 1452, Loss: 3.0804901719093323, Final Batch Loss: 0.6007272005081177\n",
      "Epoch 1453, Loss: 3.180223822593689, Final Batch Loss: 0.6855577230453491\n",
      "Epoch 1454, Loss: 2.9471855759620667, Final Batch Loss: 0.5445900559425354\n",
      "Epoch 1455, Loss: 2.8866778016090393, Final Batch Loss: 0.518831729888916\n",
      "Epoch 1456, Loss: 2.9771711230278015, Final Batch Loss: 0.6583756804466248\n",
      "Epoch 1457, Loss: 2.8692076206207275, Final Batch Loss: 0.549859344959259\n",
      "Epoch 1458, Loss: 2.9651233553886414, Final Batch Loss: 0.7005708813667297\n",
      "Epoch 1459, Loss: 2.9314505457878113, Final Batch Loss: 0.583267867565155\n",
      "Epoch 1460, Loss: 3.011837661266327, Final Batch Loss: 0.5306345820426941\n",
      "Epoch 1461, Loss: 2.868521600961685, Final Batch Loss: 0.48816969990730286\n",
      "Epoch 1462, Loss: 2.878710627555847, Final Batch Loss: 0.5503630638122559\n",
      "Epoch 1463, Loss: 2.8215513229370117, Final Batch Loss: 0.5352233648300171\n",
      "Epoch 1464, Loss: 2.9072330594062805, Final Batch Loss: 0.4901275038719177\n",
      "Epoch 1465, Loss: 2.9575816988945007, Final Batch Loss: 0.6264845728874207\n",
      "Epoch 1466, Loss: 2.7987982630729675, Final Batch Loss: 0.5077850222587585\n",
      "Epoch 1467, Loss: 3.0278674960136414, Final Batch Loss: 0.5967265963554382\n",
      "Epoch 1468, Loss: 3.0889087915420532, Final Batch Loss: 0.7197637557983398\n",
      "Epoch 1469, Loss: 2.953040659427643, Final Batch Loss: 0.7141184210777283\n",
      "Epoch 1470, Loss: 2.9501627683639526, Final Batch Loss: 0.6178663969039917\n",
      "Epoch 1471, Loss: 3.0113441944122314, Final Batch Loss: 0.5706494450569153\n",
      "Epoch 1472, Loss: 2.9020529985427856, Final Batch Loss: 0.6778339743614197\n",
      "Epoch 1473, Loss: 2.832562029361725, Final Batch Loss: 0.4486119747161865\n",
      "Epoch 1474, Loss: 3.3363940119743347, Final Batch Loss: 0.770453691482544\n",
      "Epoch 1475, Loss: 3.164036273956299, Final Batch Loss: 0.7036139369010925\n",
      "Epoch 1476, Loss: 2.887688159942627, Final Batch Loss: 0.5048101544380188\n",
      "Epoch 1477, Loss: 3.0242327451705933, Final Batch Loss: 0.6061401963233948\n",
      "Epoch 1478, Loss: 2.890519082546234, Final Batch Loss: 0.6144559383392334\n",
      "Epoch 1479, Loss: 2.9793220162391663, Final Batch Loss: 0.5792412161827087\n",
      "Epoch 1480, Loss: 2.8579511046409607, Final Batch Loss: 0.5146580934524536\n",
      "Epoch 1481, Loss: 3.001738727092743, Final Batch Loss: 0.5700132846832275\n",
      "Epoch 1482, Loss: 2.8463876843452454, Final Batch Loss: 0.6137189269065857\n",
      "Epoch 1483, Loss: 2.738060474395752, Final Batch Loss: 0.5059058666229248\n",
      "Epoch 1484, Loss: 2.98236083984375, Final Batch Loss: 0.6027044653892517\n",
      "Epoch 1485, Loss: 2.82524773478508, Final Batch Loss: 0.6326025128364563\n",
      "Epoch 1486, Loss: 2.9040293097496033, Final Batch Loss: 0.5850642323493958\n",
      "Epoch 1487, Loss: 2.8230658769607544, Final Batch Loss: 0.5872531533241272\n",
      "Epoch 1488, Loss: 2.815189242362976, Final Batch Loss: 0.6139545440673828\n",
      "Epoch 1489, Loss: 2.954562246799469, Final Batch Loss: 0.5885306000709534\n",
      "Epoch 1490, Loss: 2.8120940923690796, Final Batch Loss: 0.5018061399459839\n",
      "Epoch 1491, Loss: 2.9349023699760437, Final Batch Loss: 0.5745909810066223\n",
      "Epoch 1492, Loss: 2.927408665418625, Final Batch Loss: 0.5964943170547485\n",
      "Epoch 1493, Loss: 2.9746392965316772, Final Batch Loss: 0.644713819026947\n",
      "Epoch 1494, Loss: 3.0239319801330566, Final Batch Loss: 0.5739609003067017\n",
      "Epoch 1495, Loss: 3.2279382944107056, Final Batch Loss: 0.6251955628395081\n",
      "Epoch 1496, Loss: 2.925285905599594, Final Batch Loss: 0.4978099465370178\n",
      "Epoch 1497, Loss: 2.826024651527405, Final Batch Loss: 0.5281513333320618\n",
      "Epoch 1498, Loss: 2.8753734827041626, Final Batch Loss: 0.6325990557670593\n",
      "Epoch 1499, Loss: 2.9637221097946167, Final Batch Loss: 0.6057883501052856\n",
      "Epoch 1500, Loss: 2.9232961535453796, Final Batch Loss: 0.5994051098823547\n",
      "Epoch 1501, Loss: 2.757389396429062, Final Batch Loss: 0.5880772471427917\n",
      "Epoch 1502, Loss: 2.75798237323761, Final Batch Loss: 0.5425834655761719\n",
      "Epoch 1503, Loss: 2.8160692751407623, Final Batch Loss: 0.6794458627700806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1504, Loss: 2.920065462589264, Final Batch Loss: 0.5794956088066101\n",
      "Epoch 1505, Loss: 2.8352256417274475, Final Batch Loss: 0.6355317831039429\n",
      "Epoch 1506, Loss: 2.849448323249817, Final Batch Loss: 0.5890980958938599\n",
      "Epoch 1507, Loss: 3.108314573764801, Final Batch Loss: 0.6923201084136963\n",
      "Epoch 1508, Loss: 2.9635114073753357, Final Batch Loss: 0.5749222040176392\n",
      "Epoch 1509, Loss: 2.824521839618683, Final Batch Loss: 0.628440797328949\n",
      "Epoch 1510, Loss: 2.9248971343040466, Final Batch Loss: 0.5043917298316956\n",
      "Epoch 1511, Loss: 3.063271552324295, Final Batch Loss: 0.6720267534255981\n",
      "Epoch 1512, Loss: 2.9160383343696594, Final Batch Loss: 0.5140207409858704\n",
      "Epoch 1513, Loss: 2.8542979061603546, Final Batch Loss: 0.6092499494552612\n",
      "Epoch 1514, Loss: 2.732264816761017, Final Batch Loss: 0.5721831917762756\n",
      "Epoch 1515, Loss: 3.0204213857650757, Final Batch Loss: 0.6971096396446228\n",
      "Epoch 1516, Loss: 3.104091167449951, Final Batch Loss: 0.6771190762519836\n",
      "Epoch 1517, Loss: 2.953985571861267, Final Batch Loss: 0.5460386872291565\n",
      "Epoch 1518, Loss: 2.92470845580101, Final Batch Loss: 0.4356183111667633\n",
      "Epoch 1519, Loss: 2.892508625984192, Final Batch Loss: 0.6279850602149963\n",
      "Epoch 1520, Loss: 3.053683280944824, Final Batch Loss: 0.6215261220932007\n",
      "Epoch 1521, Loss: 2.780218720436096, Final Batch Loss: 0.4409922957420349\n",
      "Epoch 1522, Loss: 2.96050488948822, Final Batch Loss: 0.7142060399055481\n",
      "Epoch 1523, Loss: 2.9445701241493225, Final Batch Loss: 0.7287246584892273\n",
      "Epoch 1524, Loss: 2.821168154478073, Final Batch Loss: 0.5305922627449036\n",
      "Epoch 1525, Loss: 2.8633983731269836, Final Batch Loss: 0.6441208124160767\n",
      "Epoch 1526, Loss: 2.8253349661827087, Final Batch Loss: 0.6734840869903564\n",
      "Epoch 1527, Loss: 2.834019035100937, Final Batch Loss: 0.4585264027118683\n",
      "Epoch 1528, Loss: 2.8172294199466705, Final Batch Loss: 0.4665413200855255\n",
      "Epoch 1529, Loss: 2.9348482489585876, Final Batch Loss: 0.5909923315048218\n",
      "Epoch 1530, Loss: 2.9465667009353638, Final Batch Loss: 0.7362637519836426\n",
      "Epoch 1531, Loss: 2.9523779153823853, Final Batch Loss: 0.5637801885604858\n",
      "Epoch 1532, Loss: 2.9572583436965942, Final Batch Loss: 0.5310010313987732\n",
      "Epoch 1533, Loss: 2.969054162502289, Final Batch Loss: 0.6082653403282166\n",
      "Epoch 1534, Loss: 2.8956356942653656, Final Batch Loss: 0.476701945066452\n",
      "Epoch 1535, Loss: 2.864626854658127, Final Batch Loss: 0.6080669164657593\n",
      "Epoch 1536, Loss: 2.9031859636306763, Final Batch Loss: 0.6069369316101074\n",
      "Epoch 1537, Loss: 2.8830647468566895, Final Batch Loss: 0.6121993660926819\n",
      "Epoch 1538, Loss: 2.9418896436691284, Final Batch Loss: 0.5433134436607361\n",
      "Epoch 1539, Loss: 2.6925664842128754, Final Batch Loss: 0.4567774832248688\n",
      "Epoch 1540, Loss: 2.9187827110290527, Final Batch Loss: 0.6273002028465271\n",
      "Epoch 1541, Loss: 2.945859909057617, Final Batch Loss: 0.6613287329673767\n",
      "Epoch 1542, Loss: 2.8725943565368652, Final Batch Loss: 0.5783956050872803\n",
      "Epoch 1543, Loss: 2.948588252067566, Final Batch Loss: 0.6114353537559509\n",
      "Epoch 1544, Loss: 2.7253676652908325, Final Batch Loss: 0.5323716402053833\n",
      "Epoch 1545, Loss: 2.705155372619629, Final Batch Loss: 0.42143714427948\n",
      "Epoch 1546, Loss: 2.833118587732315, Final Batch Loss: 0.5450751185417175\n",
      "Epoch 1547, Loss: 2.6980133056640625, Final Batch Loss: 0.5305189490318298\n",
      "Epoch 1548, Loss: 2.9182881116867065, Final Batch Loss: 0.5722657442092896\n",
      "Epoch 1549, Loss: 3.018140494823456, Final Batch Loss: 0.6502931714057922\n",
      "Epoch 1550, Loss: 3.073466181755066, Final Batch Loss: 0.664332926273346\n",
      "Epoch 1551, Loss: 3.038984179496765, Final Batch Loss: 0.5428500175476074\n",
      "Epoch 1552, Loss: 2.8387798070907593, Final Batch Loss: 0.5668368935585022\n",
      "Epoch 1553, Loss: 2.953109622001648, Final Batch Loss: 0.6439056992530823\n",
      "Epoch 1554, Loss: 3.0375977158546448, Final Batch Loss: 0.6739347577095032\n",
      "Epoch 1555, Loss: 2.948506534099579, Final Batch Loss: 0.5836455821990967\n",
      "Epoch 1556, Loss: 2.84655100107193, Final Batch Loss: 0.5708048939704895\n",
      "Epoch 1557, Loss: 2.8319132030010223, Final Batch Loss: 0.6268631219863892\n",
      "Epoch 1558, Loss: 2.937713086605072, Final Batch Loss: 0.5550147294998169\n",
      "Epoch 1559, Loss: 2.7450545132160187, Final Batch Loss: 0.43450692296028137\n",
      "Epoch 1560, Loss: 2.8012939989566803, Final Batch Loss: 0.4878034293651581\n",
      "Epoch 1561, Loss: 2.883013427257538, Final Batch Loss: 0.6331118941307068\n",
      "Epoch 1562, Loss: 2.775878608226776, Final Batch Loss: 0.6088185906410217\n",
      "Epoch 1563, Loss: 2.7601501047611237, Final Batch Loss: 0.5389552712440491\n",
      "Epoch 1564, Loss: 3.052495241165161, Final Batch Loss: 0.5942201018333435\n",
      "Epoch 1565, Loss: 2.918379247188568, Final Batch Loss: 0.7332481741905212\n",
      "Epoch 1566, Loss: 2.837991952896118, Final Batch Loss: 0.5391663908958435\n",
      "Epoch 1567, Loss: 2.8131392300128937, Final Batch Loss: 0.4494653642177582\n",
      "Epoch 1568, Loss: 2.7065031230449677, Final Batch Loss: 0.44508031010627747\n",
      "Epoch 1569, Loss: 2.9459841549396515, Final Batch Loss: 0.4741683900356293\n",
      "Epoch 1570, Loss: 2.957368403673172, Final Batch Loss: 0.6921578049659729\n",
      "Epoch 1571, Loss: 2.793197512626648, Final Batch Loss: 0.49907156825065613\n",
      "Epoch 1572, Loss: 3.025364935398102, Final Batch Loss: 0.6681590676307678\n",
      "Epoch 1573, Loss: 2.9667959213256836, Final Batch Loss: 0.6539012789726257\n",
      "Epoch 1574, Loss: 2.8112666308879852, Final Batch Loss: 0.559779167175293\n",
      "Epoch 1575, Loss: 2.908417522907257, Final Batch Loss: 0.5270043611526489\n",
      "Epoch 1576, Loss: 3.142904281616211, Final Batch Loss: 0.6708829402923584\n",
      "Epoch 1577, Loss: 2.991646945476532, Final Batch Loss: 0.5116889476776123\n",
      "Epoch 1578, Loss: 2.9370182752609253, Final Batch Loss: 0.5609532594680786\n",
      "Epoch 1579, Loss: 3.1008870601654053, Final Batch Loss: 0.6740469932556152\n",
      "Epoch 1580, Loss: 2.8116748332977295, Final Batch Loss: 0.5690010786056519\n",
      "Epoch 1581, Loss: 2.953374147415161, Final Batch Loss: 0.533469021320343\n",
      "Epoch 1582, Loss: 2.935151219367981, Final Batch Loss: 0.5634565353393555\n",
      "Epoch 1583, Loss: 2.874777227640152, Final Batch Loss: 0.6169799566268921\n",
      "Epoch 1584, Loss: 2.927251160144806, Final Batch Loss: 0.609415590763092\n",
      "Epoch 1585, Loss: 2.9466195106506348, Final Batch Loss: 0.6417846083641052\n",
      "Epoch 1586, Loss: 2.901249945163727, Final Batch Loss: 0.546539843082428\n",
      "Epoch 1587, Loss: 2.826559752225876, Final Batch Loss: 0.4835047721862793\n",
      "Epoch 1588, Loss: 2.9863263368606567, Final Batch Loss: 0.6267617344856262\n",
      "Epoch 1589, Loss: 2.89537650346756, Final Batch Loss: 0.5836637020111084\n",
      "Epoch 1590, Loss: 2.833757758140564, Final Batch Loss: 0.5590248107910156\n",
      "Epoch 1591, Loss: 2.988373816013336, Final Batch Loss: 0.6692547798156738\n",
      "Epoch 1592, Loss: 2.8345735669136047, Final Batch Loss: 0.585205078125\n",
      "Epoch 1593, Loss: 2.797685921192169, Final Batch Loss: 0.5541096925735474\n",
      "Epoch 1594, Loss: 2.747867703437805, Final Batch Loss: 0.5580865144729614\n",
      "Epoch 1595, Loss: 2.9144018292427063, Final Batch Loss: 0.5904535055160522\n",
      "Epoch 1596, Loss: 2.882401794195175, Final Batch Loss: 0.6076894998550415\n",
      "Epoch 1597, Loss: 2.953208714723587, Final Batch Loss: 0.8732042908668518\n",
      "Epoch 1598, Loss: 2.9896347522735596, Final Batch Loss: 0.4965707063674927\n",
      "Epoch 1599, Loss: 2.8782655000686646, Final Batch Loss: 0.5564514398574829\n",
      "Epoch 1600, Loss: 2.798123598098755, Final Batch Loss: 0.5678954124450684\n",
      "Epoch 1601, Loss: 2.946801543235779, Final Batch Loss: 0.6209783554077148\n",
      "Epoch 1602, Loss: 2.9457415342330933, Final Batch Loss: 0.6075172424316406\n",
      "Epoch 1603, Loss: 2.8740430772304535, Final Batch Loss: 0.4950500428676605\n",
      "Epoch 1604, Loss: 2.7636712789535522, Final Batch Loss: 0.5428687334060669\n",
      "Epoch 1605, Loss: 3.045169770717621, Final Batch Loss: 0.67915278673172\n",
      "Epoch 1606, Loss: 2.7649530470371246, Final Batch Loss: 0.6377096772193909\n",
      "Epoch 1607, Loss: 2.784379690885544, Final Batch Loss: 0.48780038952827454\n",
      "Epoch 1608, Loss: 3.0130995512008667, Final Batch Loss: 0.702228844165802\n",
      "Epoch 1609, Loss: 2.867366909980774, Final Batch Loss: 0.6068643927574158\n",
      "Epoch 1610, Loss: 2.9555117785930634, Final Batch Loss: 0.5946218371391296\n",
      "Epoch 1611, Loss: 2.922829508781433, Final Batch Loss: 0.5804540514945984\n",
      "Epoch 1612, Loss: 2.854092538356781, Final Batch Loss: 0.50717693567276\n",
      "Epoch 1613, Loss: 2.771558701992035, Final Batch Loss: 0.5007640719413757\n",
      "Epoch 1614, Loss: 2.838107794523239, Final Batch Loss: 0.6877708435058594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1615, Loss: 2.8558411598205566, Final Batch Loss: 0.5672690868377686\n",
      "Epoch 1616, Loss: 2.8352889120578766, Final Batch Loss: 0.4598614275455475\n",
      "Epoch 1617, Loss: 2.8128907680511475, Final Batch Loss: 0.513042151927948\n",
      "Epoch 1618, Loss: 2.814507931470871, Final Batch Loss: 0.44000574946403503\n",
      "Epoch 1619, Loss: 2.8049744367599487, Final Batch Loss: 0.5647380352020264\n",
      "Epoch 1620, Loss: 2.883077085018158, Final Batch Loss: 0.6202653050422668\n",
      "Epoch 1621, Loss: 2.8642826974391937, Final Batch Loss: 0.48023781180381775\n",
      "Epoch 1622, Loss: 2.929383337497711, Final Batch Loss: 0.6582386493682861\n",
      "Epoch 1623, Loss: 2.758120536804199, Final Batch Loss: 0.6142784953117371\n",
      "Epoch 1624, Loss: 2.910853326320648, Final Batch Loss: 0.5678094029426575\n",
      "Epoch 1625, Loss: 3.0618095993995667, Final Batch Loss: 0.5841614007949829\n",
      "Epoch 1626, Loss: 2.9881519079208374, Final Batch Loss: 0.6050639748573303\n",
      "Epoch 1627, Loss: 2.749530702829361, Final Batch Loss: 0.4921598732471466\n",
      "Epoch 1628, Loss: 2.719679296016693, Final Batch Loss: 0.5295715928077698\n",
      "Epoch 1629, Loss: 2.9158029556274414, Final Batch Loss: 0.6170921921730042\n",
      "Epoch 1630, Loss: 2.9536728262901306, Final Batch Loss: 0.7087978720664978\n",
      "Epoch 1631, Loss: 2.7196260392665863, Final Batch Loss: 0.4648517370223999\n",
      "Epoch 1632, Loss: 2.7791300415992737, Final Batch Loss: 0.5808940529823303\n",
      "Epoch 1633, Loss: 2.8165987133979797, Final Batch Loss: 0.579811692237854\n",
      "Epoch 1634, Loss: 2.8670774102211, Final Batch Loss: 0.6192178726196289\n",
      "Epoch 1635, Loss: 2.722680002450943, Final Batch Loss: 0.5768159627914429\n",
      "Epoch 1636, Loss: 2.9343483448028564, Final Batch Loss: 0.604174792766571\n",
      "Epoch 1637, Loss: 2.8219297528266907, Final Batch Loss: 0.5088440179824829\n",
      "Epoch 1638, Loss: 2.9014543294906616, Final Batch Loss: 0.6615098714828491\n",
      "Epoch 1639, Loss: 2.9920319318771362, Final Batch Loss: 0.6308441758155823\n",
      "Epoch 1640, Loss: 2.8084288239479065, Final Batch Loss: 0.5555861592292786\n",
      "Epoch 1641, Loss: 2.766427755355835, Final Batch Loss: 0.41941773891448975\n",
      "Epoch 1642, Loss: 2.6701208651065826, Final Batch Loss: 0.5029792785644531\n",
      "Epoch 1643, Loss: 2.773960530757904, Final Batch Loss: 0.5123571753501892\n",
      "Epoch 1644, Loss: 2.7302917540073395, Final Batch Loss: 0.41439083218574524\n",
      "Epoch 1645, Loss: 2.897463858127594, Final Batch Loss: 0.597350537776947\n",
      "Epoch 1646, Loss: 3.0747777819633484, Final Batch Loss: 0.7114160060882568\n",
      "Epoch 1647, Loss: 2.8752893209457397, Final Batch Loss: 0.6189188361167908\n",
      "Epoch 1648, Loss: 2.717478632926941, Final Batch Loss: 0.511258065700531\n",
      "Epoch 1649, Loss: 2.904665172100067, Final Batch Loss: 0.5433527231216431\n",
      "Epoch 1650, Loss: 2.8443775177001953, Final Batch Loss: 0.5141143202781677\n",
      "Epoch 1651, Loss: 2.6630150377750397, Final Batch Loss: 0.4566977620124817\n",
      "Epoch 1652, Loss: 2.9575677514076233, Final Batch Loss: 0.6080783009529114\n",
      "Epoch 1653, Loss: 2.901808977127075, Final Batch Loss: 0.6596347689628601\n",
      "Epoch 1654, Loss: 2.8908697366714478, Final Batch Loss: 0.5489552617073059\n",
      "Epoch 1655, Loss: 2.9382284283638, Final Batch Loss: 0.6257249116897583\n",
      "Epoch 1656, Loss: 2.7677738964557648, Final Batch Loss: 0.5505813956260681\n",
      "Epoch 1657, Loss: 2.8638737499713898, Final Batch Loss: 0.5446122288703918\n",
      "Epoch 1658, Loss: 2.8118591904640198, Final Batch Loss: 0.5552668571472168\n",
      "Epoch 1659, Loss: 2.820370137691498, Final Batch Loss: 0.5253674387931824\n",
      "Epoch 1660, Loss: 2.857578784227371, Final Batch Loss: 0.4786235988140106\n",
      "Epoch 1661, Loss: 2.939542829990387, Final Batch Loss: 0.6520067453384399\n",
      "Epoch 1662, Loss: 2.710421711206436, Final Batch Loss: 0.49341055750846863\n",
      "Epoch 1663, Loss: 2.900851845741272, Final Batch Loss: 0.5445151329040527\n",
      "Epoch 1664, Loss: 2.99521267414093, Final Batch Loss: 0.7471463084220886\n",
      "Epoch 1665, Loss: 2.7290678322315216, Final Batch Loss: 0.5579561591148376\n",
      "Epoch 1666, Loss: 2.9457441568374634, Final Batch Loss: 0.6285132765769958\n",
      "Epoch 1667, Loss: 3.0040834546089172, Final Batch Loss: 0.6851014494895935\n",
      "Epoch 1668, Loss: 2.7255280017852783, Final Batch Loss: 0.5749894976615906\n",
      "Epoch 1669, Loss: 2.8461976647377014, Final Batch Loss: 0.6401375532150269\n",
      "Epoch 1670, Loss: 2.7719604074954987, Final Batch Loss: 0.6071909666061401\n",
      "Epoch 1671, Loss: 2.854687452316284, Final Batch Loss: 0.62199866771698\n",
      "Epoch 1672, Loss: 2.6935527324676514, Final Batch Loss: 0.5455892086029053\n",
      "Epoch 1673, Loss: 2.887877643108368, Final Batch Loss: 0.4865224361419678\n",
      "Epoch 1674, Loss: 2.8195266127586365, Final Batch Loss: 0.5468894839286804\n",
      "Epoch 1675, Loss: 2.985089361667633, Final Batch Loss: 0.7271155118942261\n",
      "Epoch 1676, Loss: 2.7601966857910156, Final Batch Loss: 0.5138785243034363\n",
      "Epoch 1677, Loss: 2.9463961124420166, Final Batch Loss: 0.5104166865348816\n",
      "Epoch 1678, Loss: 2.7564311623573303, Final Batch Loss: 0.4972077012062073\n",
      "Epoch 1679, Loss: 2.816511482000351, Final Batch Loss: 0.4720083177089691\n",
      "Epoch 1680, Loss: 2.869951069355011, Final Batch Loss: 0.5566474795341492\n",
      "Epoch 1681, Loss: 2.9038988649845123, Final Batch Loss: 0.5998462438583374\n",
      "Epoch 1682, Loss: 2.7509971857070923, Final Batch Loss: 0.539925217628479\n",
      "Epoch 1683, Loss: 2.7724640667438507, Final Batch Loss: 0.6570291519165039\n",
      "Epoch 1684, Loss: 2.8857441544532776, Final Batch Loss: 0.5244016051292419\n",
      "Epoch 1685, Loss: 2.5593899488449097, Final Batch Loss: 0.5101866722106934\n",
      "Epoch 1686, Loss: 2.9339967370033264, Final Batch Loss: 0.5916246175765991\n",
      "Epoch 1687, Loss: 2.9345520734786987, Final Batch Loss: 0.6145668625831604\n",
      "Epoch 1688, Loss: 3.0340510606765747, Final Batch Loss: 0.5751971006393433\n",
      "Epoch 1689, Loss: 2.777309477329254, Final Batch Loss: 0.6155913472175598\n",
      "Epoch 1690, Loss: 2.7497276961803436, Final Batch Loss: 0.494680255651474\n",
      "Epoch 1691, Loss: 2.7447185814380646, Final Batch Loss: 0.6021428108215332\n",
      "Epoch 1692, Loss: 2.807248443365097, Final Batch Loss: 0.6814394593238831\n",
      "Epoch 1693, Loss: 2.7765993773937225, Final Batch Loss: 0.46239975094795227\n",
      "Epoch 1694, Loss: 2.968151092529297, Final Batch Loss: 0.604403555393219\n",
      "Epoch 1695, Loss: 2.8001816272735596, Final Batch Loss: 0.5010650753974915\n",
      "Epoch 1696, Loss: 2.8384686410427094, Final Batch Loss: 0.6277177929878235\n",
      "Epoch 1697, Loss: 2.9247480034828186, Final Batch Loss: 0.5542222857475281\n",
      "Epoch 1698, Loss: 2.81876802444458, Final Batch Loss: 0.5353841185569763\n",
      "Epoch 1699, Loss: 2.8977538645267487, Final Batch Loss: 0.6131250858306885\n",
      "Epoch 1700, Loss: 2.85817289352417, Final Batch Loss: 0.6511847376823425\n",
      "Epoch 1701, Loss: 2.9071803092956543, Final Batch Loss: 0.7283693552017212\n",
      "Epoch 1702, Loss: 2.7304545640945435, Final Batch Loss: 0.5671534538269043\n",
      "Epoch 1703, Loss: 2.710196793079376, Final Batch Loss: 0.5166272521018982\n",
      "Epoch 1704, Loss: 2.851073682308197, Final Batch Loss: 0.4617076516151428\n",
      "Epoch 1705, Loss: 2.811623513698578, Final Batch Loss: 0.5789245367050171\n",
      "Epoch 1706, Loss: 2.883962094783783, Final Batch Loss: 0.4302644729614258\n",
      "Epoch 1707, Loss: 2.988595426082611, Final Batch Loss: 0.5446888208389282\n",
      "Epoch 1708, Loss: 2.8664882481098175, Final Batch Loss: 0.4402201473712921\n",
      "Epoch 1709, Loss: 2.9046225547790527, Final Batch Loss: 0.5820446014404297\n",
      "Epoch 1710, Loss: 2.7676636576652527, Final Batch Loss: 0.4825361967086792\n",
      "Epoch 1711, Loss: 2.8403416574001312, Final Batch Loss: 0.624367356300354\n",
      "Epoch 1712, Loss: 2.9658336639404297, Final Batch Loss: 0.5948905944824219\n",
      "Epoch 1713, Loss: 3.020546615123749, Final Batch Loss: 0.7989397644996643\n",
      "Epoch 1714, Loss: 2.7276379764080048, Final Batch Loss: 0.4935157001018524\n",
      "Epoch 1715, Loss: 2.7921872437000275, Final Batch Loss: 0.6039929389953613\n",
      "Epoch 1716, Loss: 2.7936821579933167, Final Batch Loss: 0.5863160490989685\n",
      "Epoch 1717, Loss: 2.8413471579551697, Final Batch Loss: 0.5133180022239685\n",
      "Epoch 1718, Loss: 2.7210760414600372, Final Batch Loss: 0.5182340145111084\n",
      "Epoch 1719, Loss: 2.9178767800331116, Final Batch Loss: 0.7421896457672119\n",
      "Epoch 1720, Loss: 2.8864702582359314, Final Batch Loss: 0.5989574790000916\n",
      "Epoch 1721, Loss: 2.7847834527492523, Final Batch Loss: 0.6104419231414795\n",
      "Epoch 1722, Loss: 2.759927451610565, Final Batch Loss: 0.5608147978782654\n",
      "Epoch 1723, Loss: 2.6845809519290924, Final Batch Loss: 0.5010438561439514\n",
      "Epoch 1724, Loss: 2.916093349456787, Final Batch Loss: 0.7057313323020935\n",
      "Epoch 1725, Loss: 2.71292906999588, Final Batch Loss: 0.49262890219688416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1726, Loss: 2.8206211626529694, Final Batch Loss: 0.5639466643333435\n",
      "Epoch 1727, Loss: 2.789883017539978, Final Batch Loss: 0.5232554078102112\n",
      "Epoch 1728, Loss: 2.769422858953476, Final Batch Loss: 0.5864080786705017\n",
      "Epoch 1729, Loss: 2.90303772687912, Final Batch Loss: 0.5998011827468872\n",
      "Epoch 1730, Loss: 2.83940452337265, Final Batch Loss: 0.6097287535667419\n",
      "Epoch 1731, Loss: 2.8931204080581665, Final Batch Loss: 0.6766798496246338\n",
      "Epoch 1732, Loss: 2.841367393732071, Final Batch Loss: 0.5547986626625061\n",
      "Epoch 1733, Loss: 2.814615309238434, Final Batch Loss: 0.5452514886856079\n",
      "Epoch 1734, Loss: 2.974999487400055, Final Batch Loss: 0.5790837407112122\n",
      "Epoch 1735, Loss: 2.7262951731681824, Final Batch Loss: 0.49437928199768066\n",
      "Epoch 1736, Loss: 2.742064595222473, Final Batch Loss: 0.5668066143989563\n",
      "Epoch 1737, Loss: 3.0065845251083374, Final Batch Loss: 0.7458823323249817\n",
      "Epoch 1738, Loss: 2.8651621341705322, Final Batch Loss: 0.5868905782699585\n",
      "Epoch 1739, Loss: 2.772157907485962, Final Batch Loss: 0.48931610584259033\n",
      "Epoch 1740, Loss: 2.6848325431346893, Final Batch Loss: 0.6122316122055054\n",
      "Epoch 1741, Loss: 2.759493738412857, Final Batch Loss: 0.5635915398597717\n",
      "Epoch 1742, Loss: 2.780262768268585, Final Batch Loss: 0.49252814054489136\n",
      "Epoch 1743, Loss: 2.85304456949234, Final Batch Loss: 0.6608509421348572\n",
      "Epoch 1744, Loss: 2.8361659944057465, Final Batch Loss: 0.6700698137283325\n",
      "Epoch 1745, Loss: 2.8488638401031494, Final Batch Loss: 0.5763677358627319\n",
      "Epoch 1746, Loss: 2.7728328704833984, Final Batch Loss: 0.573316752910614\n",
      "Epoch 1747, Loss: 2.7167330384254456, Final Batch Loss: 0.5475521087646484\n",
      "Epoch 1748, Loss: 2.8968262672424316, Final Batch Loss: 0.6253805756568909\n",
      "Epoch 1749, Loss: 2.8684036433696747, Final Batch Loss: 0.6159766316413879\n",
      "Epoch 1750, Loss: 2.742002934217453, Final Batch Loss: 0.4340023100376129\n",
      "Epoch 1751, Loss: 3.071430265903473, Final Batch Loss: 0.7850003242492676\n",
      "Epoch 1752, Loss: 2.8588878512382507, Final Batch Loss: 0.5979408621788025\n",
      "Epoch 1753, Loss: 2.790888011455536, Final Batch Loss: 0.5550286173820496\n",
      "Epoch 1754, Loss: 2.9324830174446106, Final Batch Loss: 0.6256991624832153\n",
      "Epoch 1755, Loss: 2.664783388376236, Final Batch Loss: 0.5242626070976257\n",
      "Epoch 1756, Loss: 2.7515374422073364, Final Batch Loss: 0.4960574507713318\n",
      "Epoch 1757, Loss: 2.6876118183135986, Final Batch Loss: 0.5034761428833008\n",
      "Epoch 1758, Loss: 2.888327717781067, Final Batch Loss: 0.5163096189498901\n",
      "Epoch 1759, Loss: 2.831622749567032, Final Batch Loss: 0.4245016276836395\n",
      "Epoch 1760, Loss: 2.8246749937534332, Final Batch Loss: 0.5625038743019104\n",
      "Epoch 1761, Loss: 2.8634342551231384, Final Batch Loss: 0.5831125974655151\n",
      "Epoch 1762, Loss: 2.8332213759422302, Final Batch Loss: 0.4954608678817749\n",
      "Epoch 1763, Loss: 2.6321632862091064, Final Batch Loss: 0.5112319588661194\n",
      "Epoch 1764, Loss: 2.817812353372574, Final Batch Loss: 0.42593273520469666\n",
      "Epoch 1765, Loss: 2.7453484535217285, Final Batch Loss: 0.5595963001251221\n",
      "Epoch 1766, Loss: 2.77921786904335, Final Batch Loss: 0.49975934624671936\n",
      "Epoch 1767, Loss: 2.6981410682201385, Final Batch Loss: 0.4446011185646057\n",
      "Epoch 1768, Loss: 2.8556824326515198, Final Batch Loss: 0.5250627994537354\n",
      "Epoch 1769, Loss: 2.684473752975464, Final Batch Loss: 0.6148459911346436\n",
      "Epoch 1770, Loss: 2.7514876425266266, Final Batch Loss: 0.49652352929115295\n",
      "Epoch 1771, Loss: 2.693579614162445, Final Batch Loss: 0.6221082210540771\n",
      "Epoch 1772, Loss: 2.795863687992096, Final Batch Loss: 0.5079880952835083\n",
      "Epoch 1773, Loss: 2.7061684131622314, Final Batch Loss: 0.39912182092666626\n",
      "Epoch 1774, Loss: 2.796341121196747, Final Batch Loss: 0.6900165677070618\n",
      "Epoch 1775, Loss: 2.73996901512146, Final Batch Loss: 0.4728947579860687\n",
      "Epoch 1776, Loss: 2.7889151573181152, Final Batch Loss: 0.6005448698997498\n",
      "Epoch 1777, Loss: 2.89334774017334, Final Batch Loss: 0.5975733995437622\n",
      "Epoch 1778, Loss: 2.840470254421234, Final Batch Loss: 0.6200922727584839\n",
      "Epoch 1779, Loss: 2.7842371463775635, Final Batch Loss: 0.5345029830932617\n",
      "Epoch 1780, Loss: 2.836432248353958, Final Batch Loss: 0.47828295826911926\n",
      "Epoch 1781, Loss: 2.861875534057617, Final Batch Loss: 0.452039897441864\n",
      "Epoch 1782, Loss: 2.945768177509308, Final Batch Loss: 0.7051666378974915\n",
      "Epoch 1783, Loss: 2.9275134801864624, Final Batch Loss: 0.5943427681922913\n",
      "Epoch 1784, Loss: 2.740860641002655, Final Batch Loss: 0.4590691924095154\n",
      "Epoch 1785, Loss: 2.879646062850952, Final Batch Loss: 0.6162608861923218\n",
      "Epoch 1786, Loss: 2.7611448764801025, Final Batch Loss: 0.5499992370605469\n",
      "Epoch 1787, Loss: 2.654238075017929, Final Batch Loss: 0.4371444582939148\n",
      "Epoch 1788, Loss: 2.7911638617515564, Final Batch Loss: 0.6003352999687195\n",
      "Epoch 1789, Loss: 2.781923234462738, Final Batch Loss: 0.5007039308547974\n",
      "Epoch 1790, Loss: 2.6483031809329987, Final Batch Loss: 0.48699820041656494\n",
      "Epoch 1791, Loss: 2.9341289401054382, Final Batch Loss: 0.5063365697860718\n",
      "Epoch 1792, Loss: 2.7738239765167236, Final Batch Loss: 0.6230770945549011\n",
      "Epoch 1793, Loss: 2.7327656745910645, Final Batch Loss: 0.4075123965740204\n",
      "Epoch 1794, Loss: 2.7933695018291473, Final Batch Loss: 0.5358210206031799\n",
      "Epoch 1795, Loss: 2.759733110666275, Final Batch Loss: 0.6615279316902161\n",
      "Epoch 1796, Loss: 2.880857676267624, Final Batch Loss: 0.5634855628013611\n",
      "Epoch 1797, Loss: 2.693948656320572, Final Batch Loss: 0.5071563124656677\n",
      "Epoch 1798, Loss: 2.754229962825775, Final Batch Loss: 0.4869910478591919\n",
      "Epoch 1799, Loss: 2.74451407790184, Final Batch Loss: 0.5980352759361267\n",
      "Epoch 1800, Loss: 2.9650785326957703, Final Batch Loss: 0.6470460295677185\n",
      "Epoch 1801, Loss: 2.8003416657447815, Final Batch Loss: 0.5610116124153137\n",
      "Epoch 1802, Loss: 2.704604208469391, Final Batch Loss: 0.48267585039138794\n",
      "Epoch 1803, Loss: 2.867743968963623, Final Batch Loss: 0.5912207961082458\n",
      "Epoch 1804, Loss: 2.7843963503837585, Final Batch Loss: 0.44545888900756836\n",
      "Epoch 1805, Loss: 2.7330812513828278, Final Batch Loss: 0.47732213139533997\n",
      "Epoch 1806, Loss: 2.7048463225364685, Final Batch Loss: 0.5464088320732117\n",
      "Epoch 1807, Loss: 2.7429061830043793, Final Batch Loss: 0.6622292399406433\n",
      "Epoch 1808, Loss: 2.7761316895484924, Final Batch Loss: 0.6281224489212036\n",
      "Epoch 1809, Loss: 2.743297040462494, Final Batch Loss: 0.4381512999534607\n",
      "Epoch 1810, Loss: 2.7152950167655945, Final Batch Loss: 0.6187965273857117\n",
      "Epoch 1811, Loss: 2.758667767047882, Final Batch Loss: 0.614810049533844\n",
      "Epoch 1812, Loss: 2.690686285495758, Final Batch Loss: 0.5563530325889587\n",
      "Epoch 1813, Loss: 2.9010451436042786, Final Batch Loss: 0.5064311027526855\n",
      "Epoch 1814, Loss: 2.8323609828948975, Final Batch Loss: 0.5089511871337891\n",
      "Epoch 1815, Loss: 2.9886967539787292, Final Batch Loss: 0.6863288879394531\n",
      "Epoch 1816, Loss: 2.831986665725708, Final Batch Loss: 0.5581086874008179\n",
      "Epoch 1817, Loss: 2.8101765513420105, Final Batch Loss: 0.5617710947990417\n",
      "Epoch 1818, Loss: 2.6765114068984985, Final Batch Loss: 0.41139382123947144\n",
      "Epoch 1819, Loss: 2.9518616795539856, Final Batch Loss: 0.6118585467338562\n",
      "Epoch 1820, Loss: 2.891762912273407, Final Batch Loss: 0.5581137537956238\n",
      "Epoch 1821, Loss: 2.8399986624717712, Final Batch Loss: 0.58023601770401\n",
      "Epoch 1822, Loss: 2.8710787296295166, Final Batch Loss: 0.5470244288444519\n",
      "Epoch 1823, Loss: 2.790206700563431, Final Batch Loss: 0.6259175539016724\n",
      "Epoch 1824, Loss: 2.8001471757888794, Final Batch Loss: 0.5049325227737427\n",
      "Epoch 1825, Loss: 2.9971272945404053, Final Batch Loss: 0.7050294280052185\n",
      "Epoch 1826, Loss: 2.6580115258693695, Final Batch Loss: 0.5036690831184387\n",
      "Epoch 1827, Loss: 2.7081815600395203, Final Batch Loss: 0.5584183931350708\n",
      "Epoch 1828, Loss: 2.7003654539585114, Final Batch Loss: 0.42240095138549805\n",
      "Epoch 1829, Loss: 2.7469112873077393, Final Batch Loss: 0.5264438390731812\n",
      "Epoch 1830, Loss: 2.9815138578414917, Final Batch Loss: 0.4254934787750244\n",
      "Epoch 1831, Loss: 2.841564506292343, Final Batch Loss: 0.6215915679931641\n",
      "Epoch 1832, Loss: 2.8995445370674133, Final Batch Loss: 0.5089415311813354\n",
      "Epoch 1833, Loss: 2.7701489329338074, Final Batch Loss: 0.6078695058822632\n",
      "Epoch 1834, Loss: 2.7685922384262085, Final Batch Loss: 0.5830273628234863\n",
      "Epoch 1835, Loss: 2.808483123779297, Final Batch Loss: 0.6269651651382446\n",
      "Epoch 1836, Loss: 2.7882778644561768, Final Batch Loss: 0.6099179983139038\n",
      "Epoch 1837, Loss: 2.9690860211849213, Final Batch Loss: 0.7000125646591187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1838, Loss: 2.823214292526245, Final Batch Loss: 0.5384266376495361\n",
      "Epoch 1839, Loss: 2.8313535153865814, Final Batch Loss: 0.6177690029144287\n",
      "Epoch 1840, Loss: 2.981215536594391, Final Batch Loss: 0.6230631470680237\n",
      "Epoch 1841, Loss: 2.7770460546016693, Final Batch Loss: 0.6618233919143677\n",
      "Epoch 1842, Loss: 2.6393503844738007, Final Batch Loss: 0.5818214416503906\n",
      "Epoch 1843, Loss: 2.81864532828331, Final Batch Loss: 0.4752182066440582\n",
      "Epoch 1844, Loss: 2.761695146560669, Final Batch Loss: 0.476445734500885\n",
      "Epoch 1845, Loss: 2.9804980158805847, Final Batch Loss: 0.5910055637359619\n",
      "Epoch 1846, Loss: 2.7384573817253113, Final Batch Loss: 0.5104695558547974\n",
      "Epoch 1847, Loss: 2.702300786972046, Final Batch Loss: 0.38344472646713257\n",
      "Epoch 1848, Loss: 2.8531663715839386, Final Batch Loss: 0.5815977454185486\n",
      "Epoch 1849, Loss: 2.6661790013313293, Final Batch Loss: 0.5204335451126099\n",
      "Epoch 1850, Loss: 2.7996785938739777, Final Batch Loss: 0.6210140585899353\n",
      "Epoch 1851, Loss: 2.843294084072113, Final Batch Loss: 0.5466669797897339\n",
      "Epoch 1852, Loss: 2.9066067934036255, Final Batch Loss: 0.5787054300308228\n",
      "Epoch 1853, Loss: 2.754701256752014, Final Batch Loss: 0.5996171236038208\n",
      "Epoch 1854, Loss: 2.5991560220718384, Final Batch Loss: 0.5403326153755188\n",
      "Epoch 1855, Loss: 2.7228920459747314, Final Batch Loss: 0.5494480729103088\n",
      "Epoch 1856, Loss: 2.821237087249756, Final Batch Loss: 0.5258636474609375\n",
      "Epoch 1857, Loss: 2.673645168542862, Final Batch Loss: 0.43673786520957947\n",
      "Epoch 1858, Loss: 2.914833664894104, Final Batch Loss: 0.6251737475395203\n",
      "Epoch 1859, Loss: 2.8583632111549377, Final Batch Loss: 0.7175229787826538\n",
      "Epoch 1860, Loss: 2.6890547573566437, Final Batch Loss: 0.5493308305740356\n",
      "Epoch 1861, Loss: 2.8010120391845703, Final Batch Loss: 0.5858128666877747\n",
      "Epoch 1862, Loss: 2.8234987258911133, Final Batch Loss: 0.5953155159950256\n",
      "Epoch 1863, Loss: 2.6745668947696686, Final Batch Loss: 0.42751845717430115\n",
      "Epoch 1864, Loss: 2.6653700470924377, Final Batch Loss: 0.45897796750068665\n",
      "Epoch 1865, Loss: 2.774086982011795, Final Batch Loss: 0.5161380767822266\n",
      "Epoch 1866, Loss: 2.7135924100875854, Final Batch Loss: 0.5256113409996033\n",
      "Epoch 1867, Loss: 2.897971123456955, Final Batch Loss: 0.5780856609344482\n",
      "Epoch 1868, Loss: 2.7971006631851196, Final Batch Loss: 0.4670843780040741\n",
      "Epoch 1869, Loss: 2.7926076650619507, Final Batch Loss: 0.5044910311698914\n",
      "Epoch 1870, Loss: 2.870346486568451, Final Batch Loss: 0.721258282661438\n",
      "Epoch 1871, Loss: 2.9061511754989624, Final Batch Loss: 0.7484864592552185\n",
      "Epoch 1872, Loss: 2.645389825105667, Final Batch Loss: 0.4896835386753082\n",
      "Epoch 1873, Loss: 2.837783694267273, Final Batch Loss: 0.6909967064857483\n",
      "Epoch 1874, Loss: 2.6429917216300964, Final Batch Loss: 0.4748072028160095\n",
      "Epoch 1875, Loss: 2.891759693622589, Final Batch Loss: 0.5012598037719727\n",
      "Epoch 1876, Loss: 2.616772085428238, Final Batch Loss: 0.5342126488685608\n",
      "Epoch 1877, Loss: 2.777102828025818, Final Batch Loss: 0.6042493581771851\n",
      "Epoch 1878, Loss: 2.9408692121505737, Final Batch Loss: 0.5843183398246765\n",
      "Epoch 1879, Loss: 2.7419070303440094, Final Batch Loss: 0.49849867820739746\n",
      "Epoch 1880, Loss: 2.6190817654132843, Final Batch Loss: 0.5184196829795837\n",
      "Epoch 1881, Loss: 2.7983571588993073, Final Batch Loss: 0.5740517973899841\n",
      "Epoch 1882, Loss: 2.624118208885193, Final Batch Loss: 0.5370669364929199\n",
      "Epoch 1883, Loss: 2.71198770403862, Final Batch Loss: 0.6034252047538757\n",
      "Epoch 1884, Loss: 2.905246078968048, Final Batch Loss: 0.6655778884887695\n",
      "Epoch 1885, Loss: 2.813064396381378, Final Batch Loss: 0.6247846484184265\n",
      "Epoch 1886, Loss: 2.8255832195281982, Final Batch Loss: 0.6445455551147461\n",
      "Epoch 1887, Loss: 2.9868476390838623, Final Batch Loss: 0.7020928859710693\n",
      "Epoch 1888, Loss: 2.8451215028762817, Final Batch Loss: 0.5777065753936768\n",
      "Epoch 1889, Loss: 2.7658039331436157, Final Batch Loss: 0.5287594795227051\n",
      "Epoch 1890, Loss: 2.776960253715515, Final Batch Loss: 0.6784776449203491\n",
      "Epoch 1891, Loss: 2.652181327342987, Final Batch Loss: 0.5463767647743225\n",
      "Epoch 1892, Loss: 2.5973528027534485, Final Batch Loss: 0.5172221064567566\n",
      "Epoch 1893, Loss: 3.0345281958580017, Final Batch Loss: 0.6707206964492798\n",
      "Epoch 1894, Loss: 2.606305241584778, Final Batch Loss: 0.4434446096420288\n",
      "Epoch 1895, Loss: 2.8646788597106934, Final Batch Loss: 0.5415400266647339\n",
      "Epoch 1896, Loss: 2.7617881894111633, Final Batch Loss: 0.5163585543632507\n",
      "Epoch 1897, Loss: 2.784212827682495, Final Batch Loss: 0.4219619631767273\n",
      "Epoch 1898, Loss: 2.7580864131450653, Final Batch Loss: 0.6135538816452026\n",
      "Epoch 1899, Loss: 2.568850189447403, Final Batch Loss: 0.48397842049598694\n",
      "Epoch 1900, Loss: 2.795187532901764, Final Batch Loss: 0.5999528765678406\n",
      "Epoch 1901, Loss: 2.972525715827942, Final Batch Loss: 0.5601494312286377\n",
      "Epoch 1902, Loss: 2.66137558221817, Final Batch Loss: 0.5248465538024902\n",
      "Epoch 1903, Loss: 2.768393814563751, Final Batch Loss: 0.4868028163909912\n",
      "Epoch 1904, Loss: 2.8665655851364136, Final Batch Loss: 0.5919961333274841\n",
      "Epoch 1905, Loss: 2.764245957136154, Final Batch Loss: 0.47809118032455444\n",
      "Epoch 1906, Loss: 2.8813297748565674, Final Batch Loss: 0.557235836982727\n",
      "Epoch 1907, Loss: 2.837844133377075, Final Batch Loss: 0.5341273546218872\n",
      "Epoch 1908, Loss: 2.9104936718940735, Final Batch Loss: 0.6530928611755371\n",
      "Epoch 1909, Loss: 2.6555126905441284, Final Batch Loss: 0.5741161108016968\n",
      "Epoch 1910, Loss: 2.7163535952568054, Final Batch Loss: 0.5238465666770935\n",
      "Epoch 1911, Loss: 2.899635970592499, Final Batch Loss: 0.5846714377403259\n",
      "Epoch 1912, Loss: 3.039631485939026, Final Batch Loss: 0.651560366153717\n",
      "Epoch 1913, Loss: 2.8368418216705322, Final Batch Loss: 0.599980354309082\n",
      "Epoch 1914, Loss: 2.8736082017421722, Final Batch Loss: 0.5558642745018005\n",
      "Epoch 1915, Loss: 2.6575742065906525, Final Batch Loss: 0.5261290669441223\n",
      "Epoch 1916, Loss: 2.684536397457123, Final Batch Loss: 0.5143479704856873\n",
      "Epoch 1917, Loss: 2.7391335368156433, Final Batch Loss: 0.5144781470298767\n",
      "Epoch 1918, Loss: 2.649593412876129, Final Batch Loss: 0.5104976296424866\n",
      "Epoch 1919, Loss: 2.985335558652878, Final Batch Loss: 0.600989580154419\n",
      "Epoch 1920, Loss: 2.8671939373016357, Final Batch Loss: 0.5663131475448608\n",
      "Epoch 1921, Loss: 2.7419648468494415, Final Batch Loss: 0.5564945936203003\n",
      "Epoch 1922, Loss: 2.772237777709961, Final Batch Loss: 0.52823805809021\n",
      "Epoch 1923, Loss: 2.871525824069977, Final Batch Loss: 0.5054366588592529\n",
      "Epoch 1924, Loss: 2.8448252081871033, Final Batch Loss: 0.48392191529273987\n",
      "Epoch 1925, Loss: 2.9063568115234375, Final Batch Loss: 0.6329382658004761\n",
      "Epoch 1926, Loss: 2.934514880180359, Final Batch Loss: 0.4474312663078308\n",
      "Epoch 1927, Loss: 2.7012155950069427, Final Batch Loss: 0.5727093815803528\n",
      "Epoch 1928, Loss: 2.71887868642807, Final Batch Loss: 0.5541120767593384\n",
      "Epoch 1929, Loss: 2.8020336031913757, Final Batch Loss: 0.5709287524223328\n",
      "Epoch 1930, Loss: 2.743402421474457, Final Batch Loss: 0.5884402990341187\n",
      "Epoch 1931, Loss: 2.643253833055496, Final Batch Loss: 0.519163191318512\n",
      "Epoch 1932, Loss: 2.777892291545868, Final Batch Loss: 0.47401055693626404\n",
      "Epoch 1933, Loss: 2.87676078081131, Final Batch Loss: 0.5481189489364624\n",
      "Epoch 1934, Loss: 2.677196979522705, Final Batch Loss: 0.521869957447052\n",
      "Epoch 1935, Loss: 2.927310287952423, Final Batch Loss: 0.5162820219993591\n",
      "Epoch 1936, Loss: 2.681235134601593, Final Batch Loss: 0.4831336736679077\n",
      "Epoch 1937, Loss: 2.7951582074165344, Final Batch Loss: 0.6915001273155212\n",
      "Epoch 1938, Loss: 2.7307801246643066, Final Batch Loss: 0.5505198240280151\n",
      "Epoch 1939, Loss: 2.6556964218616486, Final Batch Loss: 0.45927104353904724\n",
      "Epoch 1940, Loss: 2.656187981367111, Final Batch Loss: 0.46479639410972595\n",
      "Epoch 1941, Loss: 2.7503286004066467, Final Batch Loss: 0.4544740319252014\n",
      "Epoch 1942, Loss: 2.8218048214912415, Final Batch Loss: 0.6347619295120239\n",
      "Epoch 1943, Loss: 2.673110604286194, Final Batch Loss: 0.49839210510253906\n",
      "Epoch 1944, Loss: 2.7744646966457367, Final Batch Loss: 0.5606064200401306\n",
      "Epoch 1945, Loss: 2.693930983543396, Final Batch Loss: 0.43228960037231445\n",
      "Epoch 1946, Loss: 2.63094162940979, Final Batch Loss: 0.4392862617969513\n",
      "Epoch 1947, Loss: 2.836767315864563, Final Batch Loss: 0.5215255618095398\n",
      "Epoch 1948, Loss: 2.93291836977005, Final Batch Loss: 0.7121373414993286\n",
      "Epoch 1949, Loss: 2.6483982503414154, Final Batch Loss: 0.4530515968799591\n",
      "Epoch 1950, Loss: 2.738140255212784, Final Batch Loss: 0.5692408680915833\n",
      "Epoch 1951, Loss: 2.744902551174164, Final Batch Loss: 0.5787212252616882\n",
      "Epoch 1952, Loss: 2.8535998463630676, Final Batch Loss: 0.5494167804718018\n",
      "Epoch 1953, Loss: 2.657426565885544, Final Batch Loss: 0.5596885085105896\n",
      "Epoch 1954, Loss: 2.8311564326286316, Final Batch Loss: 0.556673526763916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1955, Loss: 2.8617709279060364, Final Batch Loss: 0.5397481322288513\n",
      "Epoch 1956, Loss: 2.8151919543743134, Final Batch Loss: 0.7161299586296082\n",
      "Epoch 1957, Loss: 2.794290602207184, Final Batch Loss: 0.6344727873802185\n",
      "Epoch 1958, Loss: 2.6991660594940186, Final Batch Loss: 0.6194370985031128\n",
      "Epoch 1959, Loss: 2.6830689907073975, Final Batch Loss: 0.49696555733680725\n",
      "Epoch 1960, Loss: 2.773616135120392, Final Batch Loss: 0.46677660942077637\n",
      "Epoch 1961, Loss: 2.627872735261917, Final Batch Loss: 0.5802821516990662\n",
      "Epoch 1962, Loss: 2.652667671442032, Final Batch Loss: 0.5160279273986816\n",
      "Epoch 1963, Loss: 2.5549381971359253, Final Batch Loss: 0.42639994621276855\n",
      "Epoch 1964, Loss: 2.724448800086975, Final Batch Loss: 0.5328568816184998\n",
      "Epoch 1965, Loss: 2.6819299459457397, Final Batch Loss: 0.5162003636360168\n",
      "Epoch 1966, Loss: 2.6327235102653503, Final Batch Loss: 0.453132301568985\n",
      "Epoch 1967, Loss: 2.6874477565288544, Final Batch Loss: 0.4870625436306\n",
      "Epoch 1968, Loss: 2.577550381422043, Final Batch Loss: 0.5214153528213501\n",
      "Epoch 1969, Loss: 2.7639459371566772, Final Batch Loss: 0.5925756692886353\n",
      "Epoch 1970, Loss: 2.5373045206069946, Final Batch Loss: 0.5752987861633301\n",
      "Epoch 1971, Loss: 2.6623734533786774, Final Batch Loss: 0.5335396528244019\n",
      "Epoch 1972, Loss: 2.821030765771866, Final Batch Loss: 0.8142485618591309\n",
      "Epoch 1973, Loss: 2.593239724636078, Final Batch Loss: 0.5725523829460144\n",
      "Epoch 1974, Loss: 2.6555396914482117, Final Batch Loss: 0.5104616284370422\n",
      "Epoch 1975, Loss: 2.784533202648163, Final Batch Loss: 0.5468607544898987\n",
      "Epoch 1976, Loss: 2.7555174231529236, Final Batch Loss: 0.45665574073791504\n",
      "Epoch 1977, Loss: 2.7302724719047546, Final Batch Loss: 0.4532583951950073\n",
      "Epoch 1978, Loss: 2.8556875586509705, Final Batch Loss: 0.5189812779426575\n",
      "Epoch 1979, Loss: 2.786942481994629, Final Batch Loss: 0.46184325218200684\n",
      "Epoch 1980, Loss: 2.65975758433342, Final Batch Loss: 0.48529890179634094\n",
      "Epoch 1981, Loss: 2.705018162727356, Final Batch Loss: 0.6098648309707642\n",
      "Epoch 1982, Loss: 2.7397251427173615, Final Batch Loss: 0.47964826226234436\n",
      "Epoch 1983, Loss: 2.733096867799759, Final Batch Loss: 0.6634544134140015\n",
      "Epoch 1984, Loss: 2.822439730167389, Final Batch Loss: 0.6542547941207886\n",
      "Epoch 1985, Loss: 2.7737064361572266, Final Batch Loss: 0.5745335817337036\n",
      "Epoch 1986, Loss: 2.7755413353443146, Final Batch Loss: 0.6033313870429993\n",
      "Epoch 1987, Loss: 2.8291515111923218, Final Batch Loss: 0.5767289996147156\n",
      "Epoch 1988, Loss: 2.6427870392799377, Final Batch Loss: 0.5937275290489197\n",
      "Epoch 1989, Loss: 2.5148178339004517, Final Batch Loss: 0.44706106185913086\n",
      "Epoch 1990, Loss: 2.6916119754314423, Final Batch Loss: 0.5476202368736267\n",
      "Epoch 1991, Loss: 2.791398674249649, Final Batch Loss: 0.497166246175766\n",
      "Epoch 1992, Loss: 2.631368041038513, Final Batch Loss: 0.4728069305419922\n",
      "Epoch 1993, Loss: 2.6227691769599915, Final Batch Loss: 0.4775473177433014\n",
      "Epoch 1994, Loss: 2.6030922532081604, Final Batch Loss: 0.5604236721992493\n",
      "Epoch 1995, Loss: 2.6280128359794617, Final Batch Loss: 0.5115892291069031\n",
      "Epoch 1996, Loss: 2.7581531703472137, Final Batch Loss: 0.5737714171409607\n",
      "Epoch 1997, Loss: 2.7832479774951935, Final Batch Loss: 0.5785287618637085\n",
      "Epoch 1998, Loss: 2.7191896736621857, Final Batch Loss: 0.5330369472503662\n",
      "Epoch 1999, Loss: 2.593793660402298, Final Batch Loss: 0.47868457436561584\n",
      "Epoch 2000, Loss: 2.6901386082172394, Final Batch Loss: 0.5706385970115662\n",
      "Epoch 2001, Loss: 2.6351816952228546, Final Batch Loss: 0.4908616542816162\n",
      "Epoch 2002, Loss: 2.5750352144241333, Final Batch Loss: 0.456011027097702\n",
      "Epoch 2003, Loss: 2.691098153591156, Final Batch Loss: 0.5910892486572266\n",
      "Epoch 2004, Loss: 2.8289616107940674, Final Batch Loss: 0.6509354710578918\n",
      "Epoch 2005, Loss: 2.7911185324192047, Final Batch Loss: 0.7284291386604309\n",
      "Epoch 2006, Loss: 2.807141065597534, Final Batch Loss: 0.5027250647544861\n",
      "Epoch 2007, Loss: 2.6845226883888245, Final Batch Loss: 0.6177669763565063\n",
      "Epoch 2008, Loss: 2.7416406869888306, Final Batch Loss: 0.5406930446624756\n",
      "Epoch 2009, Loss: 2.988163083791733, Final Batch Loss: 0.5265955328941345\n",
      "Epoch 2010, Loss: 2.6299988925457, Final Batch Loss: 0.5496615171432495\n",
      "Epoch 2011, Loss: 2.9665614664554596, Final Batch Loss: 0.5465788245201111\n",
      "Epoch 2012, Loss: 2.8292548656463623, Final Batch Loss: 0.6444114446640015\n",
      "Epoch 2013, Loss: 2.6664466857910156, Final Batch Loss: 0.546718955039978\n",
      "Epoch 2014, Loss: 2.5835432708263397, Final Batch Loss: 0.49666449427604675\n",
      "Epoch 2015, Loss: 2.7574811577796936, Final Batch Loss: 0.5120531916618347\n",
      "Epoch 2016, Loss: 2.5866744816303253, Final Batch Loss: 0.4096917510032654\n",
      "Epoch 2017, Loss: 2.8986048102378845, Final Batch Loss: 0.6287447214126587\n",
      "Epoch 2018, Loss: 2.6362406611442566, Final Batch Loss: 0.5046294331550598\n",
      "Epoch 2019, Loss: 2.826467990875244, Final Batch Loss: 0.6519717574119568\n",
      "Epoch 2020, Loss: 2.8342395424842834, Final Batch Loss: 0.516261637210846\n",
      "Epoch 2021, Loss: 2.738265097141266, Final Batch Loss: 0.5422071814537048\n",
      "Epoch 2022, Loss: 2.7131406664848328, Final Batch Loss: 0.5314468145370483\n",
      "Epoch 2023, Loss: 2.6517389118671417, Final Batch Loss: 0.5109654068946838\n",
      "Epoch 2024, Loss: 2.8055301308631897, Final Batch Loss: 0.5974040627479553\n",
      "Epoch 2025, Loss: 2.6752713918685913, Final Batch Loss: 0.5646163821220398\n",
      "Epoch 2026, Loss: 2.6208641827106476, Final Batch Loss: 0.48171088099479675\n",
      "Epoch 2027, Loss: 2.6399811506271362, Final Batch Loss: 0.42767030000686646\n",
      "Epoch 2028, Loss: 2.845465302467346, Final Batch Loss: 0.7758675217628479\n",
      "Epoch 2029, Loss: 2.5897811353206635, Final Batch Loss: 0.47914350032806396\n",
      "Epoch 2030, Loss: 2.781648814678192, Final Batch Loss: 0.4610949158668518\n",
      "Epoch 2031, Loss: 2.7516843676567078, Final Batch Loss: 0.5064698457717896\n",
      "Epoch 2032, Loss: 2.8202503323554993, Final Batch Loss: 0.6069719791412354\n",
      "Epoch 2033, Loss: 2.6940940022468567, Final Batch Loss: 0.4734960198402405\n",
      "Epoch 2034, Loss: 2.6710756421089172, Final Batch Loss: 0.38463282585144043\n",
      "Epoch 2035, Loss: 2.768855929374695, Final Batch Loss: 0.5406701564788818\n",
      "Epoch 2036, Loss: 2.7378446757793427, Final Batch Loss: 0.5886428952217102\n",
      "Epoch 2037, Loss: 2.870734453201294, Final Batch Loss: 0.6307083368301392\n",
      "Epoch 2038, Loss: 2.6230483651161194, Final Batch Loss: 0.5378860235214233\n",
      "Epoch 2039, Loss: 2.6198029816150665, Final Batch Loss: 0.4155166447162628\n",
      "Epoch 2040, Loss: 2.856697916984558, Final Batch Loss: 0.4987078309059143\n",
      "Epoch 2041, Loss: 2.58151975274086, Final Batch Loss: 0.45544129610061646\n",
      "Epoch 2042, Loss: 2.78440323472023, Final Batch Loss: 0.4692355990409851\n",
      "Epoch 2043, Loss: 2.7448751628398895, Final Batch Loss: 0.6681140065193176\n",
      "Epoch 2044, Loss: 2.626189649105072, Final Batch Loss: 0.5421742796897888\n",
      "Epoch 2045, Loss: 2.674370437860489, Final Batch Loss: 0.5642868876457214\n",
      "Epoch 2046, Loss: 2.5990853905677795, Final Batch Loss: 0.48093825578689575\n",
      "Epoch 2047, Loss: 2.8233882188796997, Final Batch Loss: 0.5216884613037109\n",
      "Epoch 2048, Loss: 2.7113441824913025, Final Batch Loss: 0.5766887068748474\n",
      "Epoch 2049, Loss: 2.815932482481003, Final Batch Loss: 0.5071597099304199\n",
      "Epoch 2050, Loss: 2.7408914864063263, Final Batch Loss: 0.5669858455657959\n",
      "Epoch 2051, Loss: 2.6912421882152557, Final Batch Loss: 0.4444572627544403\n",
      "Epoch 2052, Loss: 2.6597719490528107, Final Batch Loss: 0.5790101289749146\n",
      "Epoch 2053, Loss: 2.854426771402359, Final Batch Loss: 0.5670397281646729\n",
      "Epoch 2054, Loss: 2.627284824848175, Final Batch Loss: 0.43291205167770386\n",
      "Epoch 2055, Loss: 2.7420371770858765, Final Batch Loss: 0.47667357325553894\n",
      "Epoch 2056, Loss: 2.825881391763687, Final Batch Loss: 0.5024145841598511\n",
      "Epoch 2057, Loss: 2.654680073261261, Final Batch Loss: 0.5034655928611755\n",
      "Epoch 2058, Loss: 2.734576463699341, Final Batch Loss: 0.531461775302887\n",
      "Epoch 2059, Loss: 2.6607356667518616, Final Batch Loss: 0.4088951349258423\n",
      "Epoch 2060, Loss: 2.7084386944770813, Final Batch Loss: 0.47006410360336304\n",
      "Epoch 2061, Loss: 2.7325092554092407, Final Batch Loss: 0.6061715483665466\n",
      "Epoch 2062, Loss: 2.8228154480457306, Final Batch Loss: 0.697902262210846\n",
      "Epoch 2063, Loss: 2.6015228927135468, Final Batch Loss: 0.4186447262763977\n",
      "Epoch 2064, Loss: 2.6381871104240417, Final Batch Loss: 0.47887834906578064\n",
      "Epoch 2065, Loss: 2.7725844383239746, Final Batch Loss: 0.5239506363868713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2066, Loss: 2.574485957622528, Final Batch Loss: 0.5670568943023682\n",
      "Epoch 2067, Loss: 2.8236986696720123, Final Batch Loss: 0.618590235710144\n",
      "Epoch 2068, Loss: 2.5432808995246887, Final Batch Loss: 0.5547922849655151\n",
      "Epoch 2069, Loss: 2.6222731471061707, Final Batch Loss: 0.44578373432159424\n",
      "Epoch 2070, Loss: 2.7610217928886414, Final Batch Loss: 0.5267104506492615\n",
      "Epoch 2071, Loss: 2.816186100244522, Final Batch Loss: 0.5858507752418518\n",
      "Epoch 2072, Loss: 2.689959704875946, Final Batch Loss: 0.5057940483093262\n",
      "Epoch 2073, Loss: 2.786071240901947, Final Batch Loss: 0.6181737184524536\n",
      "Epoch 2074, Loss: 2.658730983734131, Final Batch Loss: 0.5100040435791016\n",
      "Epoch 2075, Loss: 2.6059248745441437, Final Batch Loss: 0.43777909874916077\n",
      "Epoch 2076, Loss: 2.733048915863037, Final Batch Loss: 0.5698292851448059\n",
      "Epoch 2077, Loss: 2.696222335100174, Final Batch Loss: 0.5971478223800659\n",
      "Epoch 2078, Loss: 2.742678940296173, Final Batch Loss: 0.44522348046302795\n",
      "Epoch 2079, Loss: 2.7154563069343567, Final Batch Loss: 0.5603601336479187\n",
      "Epoch 2080, Loss: 2.7816244661808014, Final Batch Loss: 0.5975923538208008\n",
      "Epoch 2081, Loss: 2.709556370973587, Final Batch Loss: 0.578652560710907\n",
      "Epoch 2082, Loss: 2.7500529289245605, Final Batch Loss: 0.5394002199172974\n",
      "Epoch 2083, Loss: 2.950170159339905, Final Batch Loss: 0.8337321877479553\n",
      "Epoch 2084, Loss: 2.7989232540130615, Final Batch Loss: 0.6284297704696655\n",
      "Epoch 2085, Loss: 2.59314888715744, Final Batch Loss: 0.48368093371391296\n",
      "Epoch 2086, Loss: 2.6444010734558105, Final Batch Loss: 0.6228305101394653\n",
      "Epoch 2087, Loss: 2.6878152787685394, Final Batch Loss: 0.6041167974472046\n",
      "Epoch 2088, Loss: 2.7538393437862396, Final Batch Loss: 0.6274812817573547\n",
      "Epoch 2089, Loss: 2.694570779800415, Final Batch Loss: 0.534601628780365\n",
      "Epoch 2090, Loss: 2.5508406162261963, Final Batch Loss: 0.5196707844734192\n",
      "Epoch 2091, Loss: 2.7430238127708435, Final Batch Loss: 0.5496536493301392\n",
      "Epoch 2092, Loss: 2.7896372973918915, Final Batch Loss: 0.5020637512207031\n",
      "Epoch 2093, Loss: 2.6753602623939514, Final Batch Loss: 0.5868006944656372\n",
      "Epoch 2094, Loss: 2.612914115190506, Final Batch Loss: 0.5651556849479675\n",
      "Epoch 2095, Loss: 2.601952701807022, Final Batch Loss: 0.45334067940711975\n",
      "Epoch 2096, Loss: 2.5651409029960632, Final Batch Loss: 0.5377804636955261\n",
      "Epoch 2097, Loss: 2.7113893926143646, Final Batch Loss: 0.6163004636764526\n",
      "Epoch 2098, Loss: 2.5178123116493225, Final Batch Loss: 0.44090986251831055\n",
      "Epoch 2099, Loss: 2.661034971475601, Final Batch Loss: 0.48292821645736694\n",
      "Epoch 2100, Loss: 2.7297865748405457, Final Batch Loss: 0.45297887921333313\n",
      "Epoch 2101, Loss: 2.8382253646850586, Final Batch Loss: 0.652060329914093\n",
      "Epoch 2102, Loss: 2.5729511380195618, Final Batch Loss: 0.4691479802131653\n",
      "Epoch 2103, Loss: 2.783214807510376, Final Batch Loss: 0.47897112369537354\n",
      "Epoch 2104, Loss: 2.6689051687717438, Final Batch Loss: 0.5687201619148254\n",
      "Epoch 2105, Loss: 2.7454925179481506, Final Batch Loss: 0.5495349764823914\n",
      "Epoch 2106, Loss: 2.5532112419605255, Final Batch Loss: 0.40695416927337646\n",
      "Epoch 2107, Loss: 2.623993217945099, Final Batch Loss: 0.4874033033847809\n",
      "Epoch 2108, Loss: 2.67994886636734, Final Batch Loss: 0.43260014057159424\n",
      "Epoch 2109, Loss: 2.659730404615402, Final Batch Loss: 0.47171035408973694\n",
      "Epoch 2110, Loss: 2.790727585554123, Final Batch Loss: 0.6567631363868713\n",
      "Epoch 2111, Loss: 2.8696118891239166, Final Batch Loss: 0.6410070061683655\n",
      "Epoch 2112, Loss: 2.696112334728241, Final Batch Loss: 0.5492982268333435\n",
      "Epoch 2113, Loss: 2.714452028274536, Final Batch Loss: 0.5067019462585449\n",
      "Epoch 2114, Loss: 2.639559328556061, Final Batch Loss: 0.5420885682106018\n",
      "Epoch 2115, Loss: 2.8191173672676086, Final Batch Loss: 0.5743506550788879\n",
      "Epoch 2116, Loss: 2.7119878828525543, Final Batch Loss: 0.5796648263931274\n",
      "Epoch 2117, Loss: 2.6652137339115143, Final Batch Loss: 0.5721408128738403\n",
      "Epoch 2118, Loss: 2.698413610458374, Final Batch Loss: 0.5847111940383911\n",
      "Epoch 2119, Loss: 2.829179883003235, Final Batch Loss: 0.5901176333427429\n",
      "Epoch 2120, Loss: 2.8209699392318726, Final Batch Loss: 0.5451783537864685\n",
      "Epoch 2121, Loss: 2.74454528093338, Final Batch Loss: 0.6087899804115295\n",
      "Epoch 2122, Loss: 2.6905276477336884, Final Batch Loss: 0.5727491974830627\n",
      "Epoch 2123, Loss: 2.6780529022216797, Final Batch Loss: 0.5157052874565125\n",
      "Epoch 2124, Loss: 2.7458062767982483, Final Batch Loss: 0.5636555552482605\n",
      "Epoch 2125, Loss: 2.6402023136615753, Final Batch Loss: 0.4181690216064453\n",
      "Epoch 2126, Loss: 2.9523963630199432, Final Batch Loss: 0.7150062322616577\n",
      "Epoch 2127, Loss: 2.6663560271263123, Final Batch Loss: 0.5228502154350281\n",
      "Epoch 2128, Loss: 2.591998279094696, Final Batch Loss: 0.4321429133415222\n",
      "Epoch 2129, Loss: 2.588295340538025, Final Batch Loss: 0.5293117761611938\n",
      "Epoch 2130, Loss: 2.8255586326122284, Final Batch Loss: 0.7995903491973877\n",
      "Epoch 2131, Loss: 2.658299148082733, Final Batch Loss: 0.5414546132087708\n",
      "Epoch 2132, Loss: 2.855901777744293, Final Batch Loss: 0.6765915155410767\n",
      "Epoch 2133, Loss: 2.7801986038684845, Final Batch Loss: 0.5501516461372375\n",
      "Epoch 2134, Loss: 2.6492465138435364, Final Batch Loss: 0.5369963645935059\n",
      "Epoch 2135, Loss: 2.814888596534729, Final Batch Loss: 0.599974513053894\n",
      "Epoch 2136, Loss: 2.5595222413539886, Final Batch Loss: 0.47945094108581543\n",
      "Epoch 2137, Loss: 2.9027917981147766, Final Batch Loss: 0.5734142065048218\n",
      "Epoch 2138, Loss: 2.526101678609848, Final Batch Loss: 0.43381622433662415\n",
      "Epoch 2139, Loss: 2.6489014625549316, Final Batch Loss: 0.5323470234870911\n",
      "Epoch 2140, Loss: 2.704844057559967, Final Batch Loss: 0.6057174205780029\n",
      "Epoch 2141, Loss: 2.6729222536087036, Final Batch Loss: 0.4966304302215576\n",
      "Epoch 2142, Loss: 2.600554436445236, Final Batch Loss: 0.47178998589515686\n",
      "Epoch 2143, Loss: 2.6757419109344482, Final Batch Loss: 0.5778829455375671\n",
      "Epoch 2144, Loss: 2.6198743879795074, Final Batch Loss: 0.5028087496757507\n",
      "Epoch 2145, Loss: 2.5367774963378906, Final Batch Loss: 0.5635044574737549\n",
      "Epoch 2146, Loss: 2.587657153606415, Final Batch Loss: 0.47123628854751587\n",
      "Epoch 2147, Loss: 2.744825631380081, Final Batch Loss: 0.6187200546264648\n",
      "Epoch 2148, Loss: 2.7888915836811066, Final Batch Loss: 0.6097462177276611\n",
      "Epoch 2149, Loss: 2.6954795122146606, Final Batch Loss: 0.5723645091056824\n",
      "Epoch 2150, Loss: 2.751479744911194, Final Batch Loss: 0.4983360469341278\n",
      "Epoch 2151, Loss: 2.6919313073158264, Final Batch Loss: 0.5078850984573364\n",
      "Epoch 2152, Loss: 2.804827928543091, Final Batch Loss: 0.6149736642837524\n",
      "Epoch 2153, Loss: 2.7993715703487396, Final Batch Loss: 0.45626816153526306\n",
      "Epoch 2154, Loss: 2.6636660397052765, Final Batch Loss: 0.6910091638565063\n",
      "Epoch 2155, Loss: 2.7982263565063477, Final Batch Loss: 0.5663108825683594\n",
      "Epoch 2156, Loss: 2.6035351753234863, Final Batch Loss: 0.46898865699768066\n",
      "Epoch 2157, Loss: 2.780313163995743, Final Batch Loss: 0.6164827346801758\n",
      "Epoch 2158, Loss: 2.71619713306427, Final Batch Loss: 0.6284193396568298\n",
      "Epoch 2159, Loss: 2.831061005592346, Final Batch Loss: 0.6545151472091675\n",
      "Epoch 2160, Loss: 2.6281890869140625, Final Batch Loss: 0.5536636114120483\n",
      "Epoch 2161, Loss: 2.7140951454639435, Final Batch Loss: 0.5211135149002075\n",
      "Epoch 2162, Loss: 2.930161237716675, Final Batch Loss: 0.6147465705871582\n",
      "Epoch 2163, Loss: 2.836874783039093, Final Batch Loss: 0.5788060426712036\n",
      "Epoch 2164, Loss: 2.806182235479355, Final Batch Loss: 0.49242255091667175\n",
      "Epoch 2165, Loss: 2.607947587966919, Final Batch Loss: 0.6124606132507324\n",
      "Epoch 2166, Loss: 2.6671060025691986, Final Batch Loss: 0.5619634389877319\n",
      "Epoch 2167, Loss: 2.6879099905490875, Final Batch Loss: 0.5517324209213257\n",
      "Epoch 2168, Loss: 2.495139926671982, Final Batch Loss: 0.4359857141971588\n",
      "Epoch 2169, Loss: 2.7470523715019226, Final Batch Loss: 0.7047446370124817\n",
      "Epoch 2170, Loss: 2.6422811448574066, Final Batch Loss: 0.5246186852455139\n",
      "Epoch 2171, Loss: 2.7421077489852905, Final Batch Loss: 0.5937818884849548\n",
      "Epoch 2172, Loss: 2.646346092224121, Final Batch Loss: 0.5770761370658875\n",
      "Epoch 2173, Loss: 2.7592954337596893, Final Batch Loss: 0.5907498002052307\n",
      "Epoch 2174, Loss: 2.638016998767853, Final Batch Loss: 0.5235583186149597\n",
      "Epoch 2175, Loss: 2.8739988207817078, Final Batch Loss: 0.5124168395996094\n",
      "Epoch 2176, Loss: 2.655121088027954, Final Batch Loss: 0.4422432780265808\n",
      "Epoch 2177, Loss: 2.6784915030002594, Final Batch Loss: 0.5816109776496887\n",
      "Epoch 2178, Loss: 2.759201467037201, Final Batch Loss: 0.6707024574279785\n",
      "Epoch 2179, Loss: 2.660636782646179, Final Batch Loss: 0.680894672870636\n",
      "Epoch 2180, Loss: 2.622618705034256, Final Batch Loss: 0.4674641489982605\n",
      "Epoch 2181, Loss: 2.73991060256958, Final Batch Loss: 0.5828192830085754\n",
      "Epoch 2182, Loss: 2.7245404422283173, Final Batch Loss: 0.7240519523620605\n",
      "Epoch 2183, Loss: 2.601742208003998, Final Batch Loss: 0.4521034061908722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2184, Loss: 2.7373183369636536, Final Batch Loss: 0.5853726863861084\n",
      "Epoch 2185, Loss: 2.603794813156128, Final Batch Loss: 0.5973643064498901\n",
      "Epoch 2186, Loss: 2.5964043140411377, Final Batch Loss: 0.5937142968177795\n",
      "Epoch 2187, Loss: 2.6419915556907654, Final Batch Loss: 0.6130876541137695\n",
      "Epoch 2188, Loss: 2.614433616399765, Final Batch Loss: 0.3566255271434784\n",
      "Epoch 2189, Loss: 2.6648162603378296, Final Batch Loss: 0.44569432735443115\n",
      "Epoch 2190, Loss: 2.66034272313118, Final Batch Loss: 0.5550773739814758\n",
      "Epoch 2191, Loss: 2.9036166071891785, Final Batch Loss: 0.6571085453033447\n",
      "Epoch 2192, Loss: 2.7170038521289825, Final Batch Loss: 0.524967610836029\n",
      "Epoch 2193, Loss: 2.552332878112793, Final Batch Loss: 0.42638108134269714\n",
      "Epoch 2194, Loss: 2.655018925666809, Final Batch Loss: 0.485769122838974\n",
      "Epoch 2195, Loss: 2.5662501454353333, Final Batch Loss: 0.5632653832435608\n",
      "Epoch 2196, Loss: 2.6821421682834625, Final Batch Loss: 0.5886204838752747\n",
      "Epoch 2197, Loss: 2.8572553992271423, Final Batch Loss: 0.6550889611244202\n",
      "Epoch 2198, Loss: 2.6314522325992584, Final Batch Loss: 0.49339914321899414\n",
      "Epoch 2199, Loss: 2.609544485807419, Final Batch Loss: 0.5476424694061279\n",
      "Epoch 2200, Loss: 2.693159431219101, Final Batch Loss: 0.6018950343132019\n",
      "Epoch 2201, Loss: 2.6799628734588623, Final Batch Loss: 0.4658423066139221\n",
      "Epoch 2202, Loss: 2.723058730363846, Final Batch Loss: 0.6530922651290894\n",
      "Epoch 2203, Loss: 2.6248596608638763, Final Batch Loss: 0.5951696038246155\n",
      "Epoch 2204, Loss: 2.679877817630768, Final Batch Loss: 0.5787459015846252\n",
      "Epoch 2205, Loss: 2.6231389343738556, Final Batch Loss: 0.6085938215255737\n",
      "Epoch 2206, Loss: 2.63260155916214, Final Batch Loss: 0.5955824851989746\n",
      "Epoch 2207, Loss: 2.884223997592926, Final Batch Loss: 0.5821577906608582\n",
      "Epoch 2208, Loss: 2.8076385855674744, Final Batch Loss: 0.5401571989059448\n",
      "Epoch 2209, Loss: 2.544892340898514, Final Batch Loss: 0.5040966868400574\n",
      "Epoch 2210, Loss: 2.5367923378944397, Final Batch Loss: 0.4762929379940033\n",
      "Epoch 2211, Loss: 2.6125758290290833, Final Batch Loss: 0.46960917115211487\n",
      "Epoch 2212, Loss: 2.6169897317886353, Final Batch Loss: 0.5052631497383118\n",
      "Epoch 2213, Loss: 2.6490336060523987, Final Batch Loss: 0.6051334142684937\n",
      "Epoch 2214, Loss: 2.5868819057941437, Final Batch Loss: 0.45875754952430725\n",
      "Epoch 2215, Loss: 2.4834377765655518, Final Batch Loss: 0.5496042966842651\n",
      "Epoch 2216, Loss: 2.754685938358307, Final Batch Loss: 0.6681694388389587\n",
      "Epoch 2217, Loss: 2.6706361174583435, Final Batch Loss: 0.47878897190093994\n",
      "Epoch 2218, Loss: 2.71084201335907, Final Batch Loss: 0.61067134141922\n",
      "Epoch 2219, Loss: 2.8302814066410065, Final Batch Loss: 0.6148597598075867\n",
      "Epoch 2220, Loss: 2.6264794766902924, Final Batch Loss: 0.573870062828064\n",
      "Epoch 2221, Loss: 2.5703978836536407, Final Batch Loss: 0.3614451289176941\n",
      "Epoch 2222, Loss: 2.4778549671173096, Final Batch Loss: 0.48019376397132874\n",
      "Epoch 2223, Loss: 2.698082983493805, Final Batch Loss: 0.50520920753479\n",
      "Epoch 2224, Loss: 2.814856231212616, Final Batch Loss: 0.7418475151062012\n",
      "Epoch 2225, Loss: 2.58896341919899, Final Batch Loss: 0.5708628296852112\n",
      "Epoch 2226, Loss: 2.5909752249717712, Final Batch Loss: 0.5262365937232971\n",
      "Epoch 2227, Loss: 2.52469265460968, Final Batch Loss: 0.4559671878814697\n",
      "Epoch 2228, Loss: 2.73946675658226, Final Batch Loss: 0.6326470375061035\n",
      "Epoch 2229, Loss: 2.7174464762210846, Final Batch Loss: 0.4848909080028534\n",
      "Epoch 2230, Loss: 2.526330202817917, Final Batch Loss: 0.42483359575271606\n",
      "Epoch 2231, Loss: 2.715383857488632, Final Batch Loss: 0.48554107546806335\n",
      "Epoch 2232, Loss: 2.6936709582805634, Final Batch Loss: 0.517995297908783\n",
      "Epoch 2233, Loss: 2.5094591975212097, Final Batch Loss: 0.4378548562526703\n",
      "Epoch 2234, Loss: 2.707897812128067, Final Batch Loss: 0.5875048637390137\n",
      "Epoch 2235, Loss: 2.573810249567032, Final Batch Loss: 0.4882403016090393\n",
      "Epoch 2236, Loss: 2.5532855689525604, Final Batch Loss: 0.415917307138443\n",
      "Epoch 2237, Loss: 2.5569493174552917, Final Batch Loss: 0.5370622873306274\n",
      "Epoch 2238, Loss: 2.561155319213867, Final Batch Loss: 0.5585839748382568\n",
      "Epoch 2239, Loss: 2.707747220993042, Final Batch Loss: 0.5491386651992798\n",
      "Epoch 2240, Loss: 2.6850741505622864, Final Batch Loss: 0.47610461711883545\n",
      "Epoch 2241, Loss: 2.683551549911499, Final Batch Loss: 0.5462402105331421\n",
      "Epoch 2242, Loss: 2.6868125200271606, Final Batch Loss: 0.6285477876663208\n",
      "Epoch 2243, Loss: 2.6501362025737762, Final Batch Loss: 0.5682414770126343\n",
      "Epoch 2244, Loss: 2.698023736476898, Final Batch Loss: 0.5915881395339966\n",
      "Epoch 2245, Loss: 2.624685972929001, Final Batch Loss: 0.6111963987350464\n",
      "Epoch 2246, Loss: 2.6786635518074036, Final Batch Loss: 0.5192907452583313\n",
      "Epoch 2247, Loss: 2.6905753910541534, Final Batch Loss: 0.5516126155853271\n",
      "Epoch 2248, Loss: 2.5476283729076385, Final Batch Loss: 0.5043380260467529\n",
      "Epoch 2249, Loss: 2.5244440734386444, Final Batch Loss: 0.5337439179420471\n",
      "Epoch 2250, Loss: 2.6046198308467865, Final Batch Loss: 0.5083069801330566\n",
      "Epoch 2251, Loss: 2.664726287126541, Final Batch Loss: 0.6132608652114868\n",
      "Epoch 2252, Loss: 2.531097948551178, Final Batch Loss: 0.3655770719051361\n",
      "Epoch 2253, Loss: 2.5438577830791473, Final Batch Loss: 0.5003105401992798\n",
      "Epoch 2254, Loss: 2.6844452619552612, Final Batch Loss: 0.6088582277297974\n",
      "Epoch 2255, Loss: 2.6410748958587646, Final Batch Loss: 0.4633808434009552\n",
      "Epoch 2256, Loss: 2.6703110337257385, Final Batch Loss: 0.5901079773902893\n",
      "Epoch 2257, Loss: 2.5718745589256287, Final Batch Loss: 0.6419743299484253\n",
      "Epoch 2258, Loss: 2.7609666287899017, Final Batch Loss: 0.4510233998298645\n",
      "Epoch 2259, Loss: 2.58137509226799, Final Batch Loss: 0.5647459626197815\n",
      "Epoch 2260, Loss: 2.7588168382644653, Final Batch Loss: 0.6296229362487793\n",
      "Epoch 2261, Loss: 2.540829300880432, Final Batch Loss: 0.5585804581642151\n",
      "Epoch 2262, Loss: 2.6853278279304504, Final Batch Loss: 0.6127668619155884\n",
      "Epoch 2263, Loss: 2.756966710090637, Final Batch Loss: 0.5643254518508911\n",
      "Epoch 2264, Loss: 2.807601511478424, Final Batch Loss: 0.5718991756439209\n",
      "Epoch 2265, Loss: 2.6650829911231995, Final Batch Loss: 0.4899412989616394\n",
      "Epoch 2266, Loss: 2.745925486087799, Final Batch Loss: 0.46535369753837585\n",
      "Epoch 2267, Loss: 2.786190450191498, Final Batch Loss: 0.649540901184082\n",
      "Epoch 2268, Loss: 2.6196301877498627, Final Batch Loss: 0.5547282099723816\n",
      "Epoch 2269, Loss: 2.6451174318790436, Final Batch Loss: 0.45467308163642883\n",
      "Epoch 2270, Loss: 2.4637680053710938, Final Batch Loss: 0.5094507932662964\n",
      "Epoch 2271, Loss: 2.6257151663303375, Final Batch Loss: 0.582149088382721\n",
      "Epoch 2272, Loss: 2.597756803035736, Final Batch Loss: 0.42423877120018005\n",
      "Epoch 2273, Loss: 2.791563928127289, Final Batch Loss: 0.6473428606987\n",
      "Epoch 2274, Loss: 2.6545744836330414, Final Batch Loss: 0.5022623538970947\n",
      "Epoch 2275, Loss: 2.5546048879623413, Final Batch Loss: 0.513487696647644\n",
      "Epoch 2276, Loss: 2.5336846113204956, Final Batch Loss: 0.40664908289909363\n",
      "Epoch 2277, Loss: 2.7251326143741608, Final Batch Loss: 0.5258073210716248\n",
      "Epoch 2278, Loss: 2.5692642629146576, Final Batch Loss: 0.5472548007965088\n",
      "Epoch 2279, Loss: 2.543234020471573, Final Batch Loss: 0.4963315725326538\n",
      "Epoch 2280, Loss: 2.6019331216812134, Final Batch Loss: 0.5281256437301636\n",
      "Epoch 2281, Loss: 2.379218965768814, Final Batch Loss: 0.3786498010158539\n",
      "Epoch 2282, Loss: 2.682693213224411, Final Batch Loss: 0.6718143820762634\n",
      "Epoch 2283, Loss: 2.5149026215076447, Final Batch Loss: 0.5311851501464844\n",
      "Epoch 2284, Loss: 2.4442729353904724, Final Batch Loss: 0.4148433208465576\n",
      "Epoch 2285, Loss: 2.636228561401367, Final Batch Loss: 0.3871918320655823\n",
      "Epoch 2286, Loss: 2.735560178756714, Final Batch Loss: 0.6138485074043274\n",
      "Epoch 2287, Loss: 2.709533005952835, Final Batch Loss: 0.5938524603843689\n",
      "Epoch 2288, Loss: 2.680448591709137, Final Batch Loss: 0.5183617472648621\n",
      "Epoch 2289, Loss: 2.657243311405182, Final Batch Loss: 0.5086579322814941\n",
      "Epoch 2290, Loss: 2.715747743844986, Final Batch Loss: 0.5180701017379761\n",
      "Epoch 2291, Loss: 2.783426284790039, Final Batch Loss: 0.5140077471733093\n",
      "Epoch 2292, Loss: 2.530629336833954, Final Batch Loss: 0.43271011114120483\n",
      "Epoch 2293, Loss: 2.565027266740799, Final Batch Loss: 0.4885282814502716\n",
      "Epoch 2294, Loss: 2.737991154193878, Final Batch Loss: 0.5661987066268921\n",
      "Epoch 2295, Loss: 2.63420233130455, Final Batch Loss: 0.617131769657135\n",
      "Epoch 2296, Loss: 2.750359058380127, Final Batch Loss: 0.5070192813873291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2297, Loss: 2.7198140919208527, Final Batch Loss: 0.5329068303108215\n",
      "Epoch 2298, Loss: 2.531718671321869, Final Batch Loss: 0.44962984323501587\n",
      "Epoch 2299, Loss: 2.654204785823822, Final Batch Loss: 0.5933828353881836\n",
      "Epoch 2300, Loss: 2.591764360666275, Final Batch Loss: 0.5087670683860779\n",
      "Epoch 2301, Loss: 2.5003137290477753, Final Batch Loss: 0.5313498973846436\n",
      "Epoch 2302, Loss: 2.6400698125362396, Final Batch Loss: 0.472724974155426\n",
      "Epoch 2303, Loss: 2.778908431529999, Final Batch Loss: 0.5453233122825623\n",
      "Epoch 2304, Loss: 2.7040061354637146, Final Batch Loss: 0.535221517086029\n",
      "Epoch 2305, Loss: 2.5520588159561157, Final Batch Loss: 0.46755075454711914\n",
      "Epoch 2306, Loss: 2.5804093182086945, Final Batch Loss: 0.583482027053833\n",
      "Epoch 2307, Loss: 2.7867972254753113, Final Batch Loss: 0.5519959330558777\n",
      "Epoch 2308, Loss: 2.5822998881340027, Final Batch Loss: 0.4831446707248688\n",
      "Epoch 2309, Loss: 2.465689331293106, Final Batch Loss: 0.46034741401672363\n",
      "Epoch 2310, Loss: 2.688427358865738, Final Batch Loss: 0.6151987910270691\n",
      "Epoch 2311, Loss: 2.598440021276474, Final Batch Loss: 0.5714911222457886\n",
      "Epoch 2312, Loss: 2.678322672843933, Final Batch Loss: 0.5133613348007202\n",
      "Epoch 2313, Loss: 2.713989704847336, Final Batch Loss: 0.6722318530082703\n",
      "Epoch 2314, Loss: 2.9598641097545624, Final Batch Loss: 0.7254658341407776\n",
      "Epoch 2315, Loss: 2.626174211502075, Final Batch Loss: 0.5093327760696411\n",
      "Epoch 2316, Loss: 2.7530232965946198, Final Batch Loss: 0.4473327696323395\n",
      "Epoch 2317, Loss: 2.5554619133472443, Final Batch Loss: 0.4704169034957886\n",
      "Epoch 2318, Loss: 2.642325669527054, Final Batch Loss: 0.49367401003837585\n",
      "Epoch 2319, Loss: 2.6874921917915344, Final Batch Loss: 0.5161208510398865\n",
      "Epoch 2320, Loss: 2.5691795647144318, Final Batch Loss: 0.5354170799255371\n",
      "Epoch 2321, Loss: 2.63510924577713, Final Batch Loss: 0.45233654975891113\n",
      "Epoch 2322, Loss: 2.68702232837677, Final Batch Loss: 0.5189664363861084\n",
      "Epoch 2323, Loss: 2.5883360505104065, Final Batch Loss: 0.47598952054977417\n",
      "Epoch 2324, Loss: 2.5293620228767395, Final Batch Loss: 0.46407002210617065\n",
      "Epoch 2325, Loss: 2.6686620712280273, Final Batch Loss: 0.5583233833312988\n",
      "Epoch 2326, Loss: 2.5181324183940887, Final Batch Loss: 0.482772558927536\n",
      "Epoch 2327, Loss: 2.641393303871155, Final Batch Loss: 0.5107259154319763\n",
      "Epoch 2328, Loss: 2.5350269079208374, Final Batch Loss: 0.3993600904941559\n",
      "Epoch 2329, Loss: 2.4095923602581024, Final Batch Loss: 0.4702008068561554\n",
      "Epoch 2330, Loss: 2.770267993211746, Final Batch Loss: 0.6046737432479858\n",
      "Epoch 2331, Loss: 2.779388815164566, Final Batch Loss: 0.6538533568382263\n",
      "Epoch 2332, Loss: 2.7675862312316895, Final Batch Loss: 0.6437766551971436\n",
      "Epoch 2333, Loss: 2.694907784461975, Final Batch Loss: 0.5261683464050293\n",
      "Epoch 2334, Loss: 2.5004848539829254, Final Batch Loss: 0.4675363600254059\n",
      "Epoch 2335, Loss: 2.6761377453804016, Final Batch Loss: 0.47558730840682983\n",
      "Epoch 2336, Loss: 2.7929932177066803, Final Batch Loss: 0.590968906879425\n",
      "Epoch 2337, Loss: 2.6279064416885376, Final Batch Loss: 0.5396715402603149\n",
      "Epoch 2338, Loss: 2.554760217666626, Final Batch Loss: 0.4893777668476105\n",
      "Epoch 2339, Loss: 2.607151508331299, Final Batch Loss: 0.573874831199646\n",
      "Epoch 2340, Loss: 2.410288989543915, Final Batch Loss: 0.5085646510124207\n",
      "Epoch 2341, Loss: 2.6405673027038574, Final Batch Loss: 0.5199376940727234\n",
      "Epoch 2342, Loss: 2.544649511575699, Final Batch Loss: 0.48443520069122314\n",
      "Epoch 2343, Loss: 2.570182591676712, Final Batch Loss: 0.4816981256008148\n",
      "Epoch 2344, Loss: 2.616080790758133, Final Batch Loss: 0.49684086441993713\n",
      "Epoch 2345, Loss: 2.694301426410675, Final Batch Loss: 0.522734522819519\n",
      "Epoch 2346, Loss: 2.882901430130005, Final Batch Loss: 0.6257887482643127\n",
      "Epoch 2347, Loss: 2.653450846672058, Final Batch Loss: 0.5891125202178955\n",
      "Epoch 2348, Loss: 2.824535697698593, Final Batch Loss: 0.5699707269668579\n",
      "Epoch 2349, Loss: 2.5928770303726196, Final Batch Loss: 0.35016143321990967\n",
      "Epoch 2350, Loss: 2.7119628190994263, Final Batch Loss: 0.5403851866722107\n",
      "Epoch 2351, Loss: 2.5867964029312134, Final Batch Loss: 0.4816473126411438\n",
      "Epoch 2352, Loss: 2.7094473242759705, Final Batch Loss: 0.535240113735199\n",
      "Epoch 2353, Loss: 2.642566531896591, Final Batch Loss: 0.5530518889427185\n",
      "Epoch 2354, Loss: 2.6434525847434998, Final Batch Loss: 0.5557662844657898\n",
      "Epoch 2355, Loss: 2.613576978445053, Final Batch Loss: 0.5355048179626465\n",
      "Epoch 2356, Loss: 2.615779370069504, Final Batch Loss: 0.49041423201560974\n",
      "Epoch 2357, Loss: 2.6325946152210236, Final Batch Loss: 0.37740370631217957\n",
      "Epoch 2358, Loss: 2.6848843097686768, Final Batch Loss: 0.5362828373908997\n",
      "Epoch 2359, Loss: 2.633680909872055, Final Batch Loss: 0.5056427717208862\n",
      "Epoch 2360, Loss: 2.6284897923469543, Final Batch Loss: 0.5571601390838623\n",
      "Epoch 2361, Loss: 2.7427564561367035, Final Batch Loss: 0.54917311668396\n",
      "Epoch 2362, Loss: 2.575538545846939, Final Batch Loss: 0.4885168671607971\n",
      "Epoch 2363, Loss: 2.434643566608429, Final Batch Loss: 0.5163629651069641\n",
      "Epoch 2364, Loss: 2.7867971658706665, Final Batch Loss: 0.48853927850723267\n",
      "Epoch 2365, Loss: 2.6989868581295013, Final Batch Loss: 0.5954180359840393\n",
      "Epoch 2366, Loss: 2.4636718928813934, Final Batch Loss: 0.47196298837661743\n",
      "Epoch 2367, Loss: 2.5484313666820526, Final Batch Loss: 0.48321080207824707\n",
      "Epoch 2368, Loss: 2.871945023536682, Final Batch Loss: 0.6237226128578186\n",
      "Epoch 2369, Loss: 2.6544254422187805, Final Batch Loss: 0.5119253993034363\n",
      "Epoch 2370, Loss: 2.663961350917816, Final Batch Loss: 0.4992659389972687\n",
      "Epoch 2371, Loss: 2.657315582036972, Final Batch Loss: 0.47714391350746155\n",
      "Epoch 2372, Loss: 2.707188695669174, Final Batch Loss: 0.6472439765930176\n",
      "Epoch 2373, Loss: 2.606812685728073, Final Batch Loss: 0.5579734444618225\n",
      "Epoch 2374, Loss: 2.5606629252433777, Final Batch Loss: 0.5577332973480225\n",
      "Epoch 2375, Loss: 2.8849936723709106, Final Batch Loss: 0.5143296718597412\n",
      "Epoch 2376, Loss: 2.567934572696686, Final Batch Loss: 0.4723998010158539\n",
      "Epoch 2377, Loss: 2.584770619869232, Final Batch Loss: 0.5214869379997253\n",
      "Epoch 2378, Loss: 2.6522791981697083, Final Batch Loss: 0.4933006465435028\n",
      "Epoch 2379, Loss: 2.523009777069092, Final Batch Loss: 0.4981526732444763\n",
      "Epoch 2380, Loss: 2.7382654547691345, Final Batch Loss: 0.542150616645813\n",
      "Epoch 2381, Loss: 2.625751346349716, Final Batch Loss: 0.4819520115852356\n",
      "Epoch 2382, Loss: 2.808884382247925, Final Batch Loss: 0.507884681224823\n",
      "Epoch 2383, Loss: 2.714868426322937, Final Batch Loss: 0.5251973867416382\n",
      "Epoch 2384, Loss: 2.530626803636551, Final Batch Loss: 0.5850227475166321\n",
      "Epoch 2385, Loss: 2.684463083744049, Final Batch Loss: 0.5660543441772461\n",
      "Epoch 2386, Loss: 2.586301237344742, Final Batch Loss: 0.48711875081062317\n",
      "Epoch 2387, Loss: 2.612696588039398, Final Batch Loss: 0.549858033657074\n",
      "Epoch 2388, Loss: 2.6207601726055145, Final Batch Loss: 0.5104858875274658\n",
      "Epoch 2389, Loss: 2.6833200454711914, Final Batch Loss: 0.5849871635437012\n",
      "Epoch 2390, Loss: 2.5039966702461243, Final Batch Loss: 0.5606775283813477\n",
      "Epoch 2391, Loss: 2.9145449101924896, Final Batch Loss: 0.7358479499816895\n",
      "Epoch 2392, Loss: 2.65014111995697, Final Batch Loss: 0.5979948043823242\n",
      "Epoch 2393, Loss: 2.6482487618923187, Final Batch Loss: 0.4588955044746399\n",
      "Epoch 2394, Loss: 2.7669966518878937, Final Batch Loss: 0.5698539018630981\n",
      "Epoch 2395, Loss: 2.679290145635605, Final Batch Loss: 0.6322830319404602\n",
      "Epoch 2396, Loss: 2.651459753513336, Final Batch Loss: 0.6034708023071289\n",
      "Epoch 2397, Loss: 2.8198786973953247, Final Batch Loss: 0.6730571389198303\n",
      "Epoch 2398, Loss: 2.7751922607421875, Final Batch Loss: 0.5930745601654053\n",
      "Epoch 2399, Loss: 2.6020114719867706, Final Batch Loss: 0.5353118181228638\n",
      "Epoch 2400, Loss: 2.60868102312088, Final Batch Loss: 0.467849463224411\n",
      "Epoch 2401, Loss: 2.7345613539218903, Final Batch Loss: 0.4868856966495514\n",
      "Epoch 2402, Loss: 2.63617604970932, Final Batch Loss: 0.47667819261550903\n",
      "Epoch 2403, Loss: 2.546974539756775, Final Batch Loss: 0.4487669765949249\n",
      "Epoch 2404, Loss: 2.5482374131679535, Final Batch Loss: 0.5083353519439697\n",
      "Epoch 2405, Loss: 2.811066120862961, Final Batch Loss: 0.6186093091964722\n",
      "Epoch 2406, Loss: 2.887162446975708, Final Batch Loss: 0.6530764102935791\n",
      "Epoch 2407, Loss: 2.7690517008304596, Final Batch Loss: 0.6224902868270874\n",
      "Epoch 2408, Loss: 2.695512145757675, Final Batch Loss: 0.6517536640167236\n",
      "Epoch 2409, Loss: 2.6016151309013367, Final Batch Loss: 0.5008295774459839\n",
      "Epoch 2410, Loss: 2.726942777633667, Final Batch Loss: 0.6045517325401306\n",
      "Epoch 2411, Loss: 2.6798457205295563, Final Batch Loss: 0.5525858402252197\n",
      "Epoch 2412, Loss: 2.587190568447113, Final Batch Loss: 0.5351892709732056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2413, Loss: 2.5029087364673615, Final Batch Loss: 0.5347878336906433\n",
      "Epoch 2414, Loss: 2.6179235875606537, Final Batch Loss: 0.5460817813873291\n",
      "Epoch 2415, Loss: 2.4931246042251587, Final Batch Loss: 0.4226456880569458\n",
      "Epoch 2416, Loss: 2.6445847153663635, Final Batch Loss: 0.594628095626831\n",
      "Epoch 2417, Loss: 2.580137401819229, Final Batch Loss: 0.5218387246131897\n",
      "Epoch 2418, Loss: 2.4807405173778534, Final Batch Loss: 0.5462619066238403\n",
      "Epoch 2419, Loss: 2.6441694796085358, Final Batch Loss: 0.44765520095825195\n",
      "Epoch 2420, Loss: 2.621445953845978, Final Batch Loss: 0.5705887079238892\n",
      "Epoch 2421, Loss: 2.558555692434311, Final Batch Loss: 0.48115888237953186\n",
      "Epoch 2422, Loss: 2.47066330909729, Final Batch Loss: 0.46761444211006165\n",
      "Epoch 2423, Loss: 2.6307061314582825, Final Batch Loss: 0.512698769569397\n",
      "Epoch 2424, Loss: 2.5456909239292145, Final Batch Loss: 0.45480725169181824\n",
      "Epoch 2425, Loss: 2.5482615530490875, Final Batch Loss: 0.561386227607727\n",
      "Epoch 2426, Loss: 2.651443511247635, Final Batch Loss: 0.5428422689437866\n",
      "Epoch 2427, Loss: 2.6186775267124176, Final Batch Loss: 0.47485050559043884\n",
      "Epoch 2428, Loss: 2.7830952405929565, Final Batch Loss: 0.5159186720848083\n",
      "Epoch 2429, Loss: 2.593495488166809, Final Batch Loss: 0.46600142121315\n",
      "Epoch 2430, Loss: 2.59682297706604, Final Batch Loss: 0.4609401226043701\n",
      "Epoch 2431, Loss: 2.4631590843200684, Final Batch Loss: 0.48217105865478516\n",
      "Epoch 2432, Loss: 2.507264405488968, Final Batch Loss: 0.44693902134895325\n",
      "Epoch 2433, Loss: 2.6258559226989746, Final Batch Loss: 0.6027505993843079\n",
      "Epoch 2434, Loss: 2.628559499979019, Final Batch Loss: 0.6813908815383911\n",
      "Epoch 2435, Loss: 2.431259363889694, Final Batch Loss: 0.462930291891098\n",
      "Epoch 2436, Loss: 2.683385819196701, Final Batch Loss: 0.48121121525764465\n",
      "Epoch 2437, Loss: 2.807500958442688, Final Batch Loss: 0.6593442559242249\n",
      "Epoch 2438, Loss: 2.7532390356063843, Final Batch Loss: 0.5560051798820496\n",
      "Epoch 2439, Loss: 2.555025428533554, Final Batch Loss: 0.48788172006607056\n",
      "Epoch 2440, Loss: 2.646073490381241, Final Batch Loss: 0.404597669839859\n",
      "Epoch 2441, Loss: 2.4266472160816193, Final Batch Loss: 0.4340905547142029\n",
      "Epoch 2442, Loss: 2.5002622604370117, Final Batch Loss: 0.42677566409111023\n",
      "Epoch 2443, Loss: 2.728259563446045, Final Batch Loss: 0.46212267875671387\n",
      "Epoch 2444, Loss: 2.662664145231247, Final Batch Loss: 0.49995747208595276\n",
      "Epoch 2445, Loss: 2.552860200405121, Final Batch Loss: 0.49741899967193604\n",
      "Epoch 2446, Loss: 2.701583743095398, Final Batch Loss: 0.4583672285079956\n",
      "Epoch 2447, Loss: 2.7154137790203094, Final Batch Loss: 0.5302424430847168\n",
      "Epoch 2448, Loss: 2.5186778008937836, Final Batch Loss: 0.6048895120620728\n",
      "Epoch 2449, Loss: 2.632395088672638, Final Batch Loss: 0.5378867983818054\n",
      "Epoch 2450, Loss: 2.6893439292907715, Final Batch Loss: 0.5209550261497498\n",
      "Epoch 2451, Loss: 2.5779692828655243, Final Batch Loss: 0.42258384823799133\n",
      "Epoch 2452, Loss: 2.5497588515281677, Final Batch Loss: 0.42665594816207886\n",
      "Epoch 2453, Loss: 2.671212464570999, Final Batch Loss: 0.5339741110801697\n",
      "Epoch 2454, Loss: 2.667836368083954, Final Batch Loss: 0.5413433313369751\n",
      "Epoch 2455, Loss: 2.451035112142563, Final Batch Loss: 0.5252948999404907\n",
      "Epoch 2456, Loss: 2.622426450252533, Final Batch Loss: 0.5535836219787598\n",
      "Epoch 2457, Loss: 2.4265590608119965, Final Batch Loss: 0.4683062732219696\n",
      "Epoch 2458, Loss: 2.665764093399048, Final Batch Loss: 0.4570326805114746\n",
      "Epoch 2459, Loss: 2.6564202904701233, Final Batch Loss: 0.6294580101966858\n",
      "Epoch 2460, Loss: 2.61055725812912, Final Batch Loss: 0.4512791037559509\n",
      "Epoch 2461, Loss: 2.627390503883362, Final Batch Loss: 0.5382102727890015\n",
      "Epoch 2462, Loss: 2.723462551832199, Final Batch Loss: 0.48083236813545227\n",
      "Epoch 2463, Loss: 2.5958747267723083, Final Batch Loss: 0.6124986410140991\n",
      "Epoch 2464, Loss: 2.6349753737449646, Final Batch Loss: 0.48320475220680237\n",
      "Epoch 2465, Loss: 2.576586663722992, Final Batch Loss: 0.41749218106269836\n",
      "Epoch 2466, Loss: 2.5458501279354095, Final Batch Loss: 0.4447047710418701\n",
      "Epoch 2467, Loss: 2.768375128507614, Final Batch Loss: 0.5283704996109009\n",
      "Epoch 2468, Loss: 2.5209890604019165, Final Batch Loss: 0.4702284336090088\n",
      "Epoch 2469, Loss: 2.5555995106697083, Final Batch Loss: 0.5106837749481201\n",
      "Epoch 2470, Loss: 2.560277283191681, Final Batch Loss: 0.5058765411376953\n",
      "Epoch 2471, Loss: 2.7027280926704407, Final Batch Loss: 0.5538880825042725\n",
      "Epoch 2472, Loss: 2.5935795605182648, Final Batch Loss: 0.5270377397537231\n",
      "Epoch 2473, Loss: 2.4143950641155243, Final Batch Loss: 0.5293930768966675\n",
      "Epoch 2474, Loss: 2.5502456724643707, Final Batch Loss: 0.5000507831573486\n",
      "Epoch 2475, Loss: 2.507080763578415, Final Batch Loss: 0.451304167509079\n",
      "Epoch 2476, Loss: 2.7529779374599457, Final Batch Loss: 0.5104701519012451\n",
      "Epoch 2477, Loss: 2.3952125906944275, Final Batch Loss: 0.4838389456272125\n",
      "Epoch 2478, Loss: 2.475463390350342, Final Batch Loss: 0.5019017457962036\n",
      "Epoch 2479, Loss: 2.43243470788002, Final Batch Loss: 0.44506511092185974\n",
      "Epoch 2480, Loss: 2.7425873279571533, Final Batch Loss: 0.667857825756073\n",
      "Epoch 2481, Loss: 2.4352552592754364, Final Batch Loss: 0.42517825961112976\n",
      "Epoch 2482, Loss: 2.3916828334331512, Final Batch Loss: 0.4047822952270508\n",
      "Epoch 2483, Loss: 2.6557560563087463, Final Batch Loss: 0.5744083523750305\n",
      "Epoch 2484, Loss: 2.4126705527305603, Final Batch Loss: 0.4511726200580597\n",
      "Epoch 2485, Loss: 2.5683033764362335, Final Batch Loss: 0.5524119138717651\n",
      "Epoch 2486, Loss: 2.4732003808021545, Final Batch Loss: 0.4152941107749939\n",
      "Epoch 2487, Loss: 2.4499701857566833, Final Batch Loss: 0.5380125641822815\n",
      "Epoch 2488, Loss: 2.732660710811615, Final Batch Loss: 0.6868665218353271\n",
      "Epoch 2489, Loss: 2.8165681958198547, Final Batch Loss: 0.55838543176651\n",
      "Epoch 2490, Loss: 2.5647628903388977, Final Batch Loss: 0.47819945216178894\n",
      "Epoch 2491, Loss: 2.485042542219162, Final Batch Loss: 0.462877094745636\n",
      "Epoch 2492, Loss: 2.4801109433174133, Final Batch Loss: 0.5677483081817627\n",
      "Epoch 2493, Loss: 2.6889261603355408, Final Batch Loss: 0.5563439130783081\n",
      "Epoch 2494, Loss: 2.586634039878845, Final Batch Loss: 0.5118983387947083\n",
      "Epoch 2495, Loss: 2.551370084285736, Final Batch Loss: 0.5740712285041809\n",
      "Epoch 2496, Loss: 2.6126178801059723, Final Batch Loss: 0.4356718361377716\n",
      "Epoch 2497, Loss: 2.7229340374469757, Final Batch Loss: 0.5667750835418701\n",
      "Epoch 2498, Loss: 2.433386117219925, Final Batch Loss: 0.42021599411964417\n",
      "Epoch 2499, Loss: 2.498669445514679, Final Batch Loss: 0.5217767357826233\n",
      "Epoch 2500, Loss: 2.4146177768707275, Final Batch Loss: 0.40502628684043884\n",
      "Epoch 2501, Loss: 2.3449151813983917, Final Batch Loss: 0.4503057599067688\n",
      "Epoch 2502, Loss: 2.6472687423229218, Final Batch Loss: 0.6961283683776855\n",
      "Epoch 2503, Loss: 2.582254081964493, Final Batch Loss: 0.6087924838066101\n",
      "Epoch 2504, Loss: 2.5812027752399445, Final Batch Loss: 0.48987647891044617\n",
      "Epoch 2505, Loss: 2.5796377062797546, Final Batch Loss: 0.521811306476593\n",
      "Epoch 2506, Loss: 2.698809653520584, Final Batch Loss: 0.6190925240516663\n",
      "Epoch 2507, Loss: 2.5661157369613647, Final Batch Loss: 0.4787627160549164\n",
      "Epoch 2508, Loss: 2.713391661643982, Final Batch Loss: 0.5683515071868896\n",
      "Epoch 2509, Loss: 2.490413159132004, Final Batch Loss: 0.4955231845378876\n",
      "Epoch 2510, Loss: 2.465990334749222, Final Batch Loss: 0.44993019104003906\n",
      "Epoch 2511, Loss: 2.4549971520900726, Final Batch Loss: 0.551142156124115\n",
      "Epoch 2512, Loss: 2.4447838962078094, Final Batch Loss: 0.6340625286102295\n",
      "Epoch 2513, Loss: 2.5645463168621063, Final Batch Loss: 0.5937697887420654\n",
      "Epoch 2514, Loss: 2.641965687274933, Final Batch Loss: 0.5266755819320679\n",
      "Epoch 2515, Loss: 2.526105582714081, Final Batch Loss: 0.4829294681549072\n",
      "Epoch 2516, Loss: 2.538832187652588, Final Batch Loss: 0.5125225186347961\n",
      "Epoch 2517, Loss: 2.5075231194496155, Final Batch Loss: 0.4707857370376587\n",
      "Epoch 2518, Loss: 2.7897868156433105, Final Batch Loss: 0.5512930154800415\n",
      "Epoch 2519, Loss: 2.677690029144287, Final Batch Loss: 0.52596515417099\n",
      "Epoch 2520, Loss: 2.370912551879883, Final Batch Loss: 0.41118288040161133\n",
      "Epoch 2521, Loss: 2.682122588157654, Final Batch Loss: 0.5898922085762024\n",
      "Epoch 2522, Loss: 2.7374663650989532, Final Batch Loss: 0.6810697913169861\n",
      "Epoch 2523, Loss: 2.5086524188518524, Final Batch Loss: 0.45847636461257935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2524, Loss: 2.610816776752472, Final Batch Loss: 0.5299649238586426\n",
      "Epoch 2525, Loss: 2.5134503543376923, Final Batch Loss: 0.5157986283302307\n",
      "Epoch 2526, Loss: 2.6617183685302734, Final Batch Loss: 0.564128041267395\n",
      "Epoch 2527, Loss: 2.587273597717285, Final Batch Loss: 0.6099652051925659\n",
      "Epoch 2528, Loss: 2.507290691137314, Final Batch Loss: 0.3923407793045044\n",
      "Epoch 2529, Loss: 2.7225330471992493, Final Batch Loss: 0.6019013524055481\n",
      "Epoch 2530, Loss: 2.7429268956184387, Final Batch Loss: 0.5618107914924622\n",
      "Epoch 2531, Loss: 2.5150229930877686, Final Batch Loss: 0.4537527561187744\n",
      "Epoch 2532, Loss: 2.5262148082256317, Final Batch Loss: 0.5268254280090332\n",
      "Epoch 2533, Loss: 2.361714094877243, Final Batch Loss: 0.4539976418018341\n",
      "Epoch 2534, Loss: 2.442750930786133, Final Batch Loss: 0.46125325560569763\n",
      "Epoch 2535, Loss: 2.955206334590912, Final Batch Loss: 0.692524254322052\n",
      "Epoch 2536, Loss: 2.613638758659363, Final Batch Loss: 0.5950478911399841\n",
      "Epoch 2537, Loss: 2.4747131168842316, Final Batch Loss: 0.5258792042732239\n",
      "Epoch 2538, Loss: 2.4948820173740387, Final Batch Loss: 0.517702043056488\n",
      "Epoch 2539, Loss: 2.4535301327705383, Final Batch Loss: 0.45596298575401306\n",
      "Epoch 2540, Loss: 2.543196350336075, Final Batch Loss: 0.6154127717018127\n",
      "Epoch 2541, Loss: 2.595310926437378, Final Batch Loss: 0.505436897277832\n",
      "Epoch 2542, Loss: 2.4034945964813232, Final Batch Loss: 0.46347030997276306\n",
      "Epoch 2543, Loss: 2.488144874572754, Final Batch Loss: 0.424623042345047\n",
      "Epoch 2544, Loss: 2.4595267176628113, Final Batch Loss: 0.4624219834804535\n",
      "Epoch 2545, Loss: 2.493680953979492, Final Batch Loss: 0.4881255626678467\n",
      "Epoch 2546, Loss: 2.523479998111725, Final Batch Loss: 0.5766117572784424\n",
      "Epoch 2547, Loss: 2.5851567685604095, Final Batch Loss: 0.510133683681488\n",
      "Epoch 2548, Loss: 2.594963103532791, Final Batch Loss: 0.5568481683731079\n",
      "Epoch 2549, Loss: 2.432799309492111, Final Batch Loss: 0.5850367546081543\n",
      "Epoch 2550, Loss: 2.4129016399383545, Final Batch Loss: 0.4786244332790375\n",
      "Epoch 2551, Loss: 2.519036829471588, Final Batch Loss: 0.48507657647132874\n",
      "Epoch 2552, Loss: 2.5976790487766266, Final Batch Loss: 0.5816991329193115\n",
      "Epoch 2553, Loss: 2.7882576882839203, Final Batch Loss: 0.577272355556488\n",
      "Epoch 2554, Loss: 2.658096969127655, Final Batch Loss: 0.42814207077026367\n",
      "Epoch 2555, Loss: 2.435780256986618, Final Batch Loss: 0.49191921949386597\n",
      "Epoch 2556, Loss: 2.6403149366378784, Final Batch Loss: 0.4919448792934418\n",
      "Epoch 2557, Loss: 2.7122364044189453, Final Batch Loss: 0.6621685028076172\n",
      "Epoch 2558, Loss: 2.4464871883392334, Final Batch Loss: 0.47492432594299316\n",
      "Epoch 2559, Loss: 2.6760349571704865, Final Batch Loss: 0.533242404460907\n",
      "Epoch 2560, Loss: 2.653903007507324, Final Batch Loss: 0.5491170287132263\n",
      "Epoch 2561, Loss: 2.5812991559505463, Final Batch Loss: 0.4932664930820465\n",
      "Epoch 2562, Loss: 2.4956878423690796, Final Batch Loss: 0.5055883526802063\n",
      "Epoch 2563, Loss: 2.6478803157806396, Final Batch Loss: 0.6129279732704163\n",
      "Epoch 2564, Loss: 2.5150371491909027, Final Batch Loss: 0.5319396257400513\n",
      "Epoch 2565, Loss: 2.3868606090545654, Final Batch Loss: 0.46739545464515686\n",
      "Epoch 2566, Loss: 2.605664014816284, Final Batch Loss: 0.45788684487342834\n",
      "Epoch 2567, Loss: 2.5431821942329407, Final Batch Loss: 0.5002186298370361\n",
      "Epoch 2568, Loss: 2.472263991832733, Final Batch Loss: 0.4096817672252655\n",
      "Epoch 2569, Loss: 2.542794793844223, Final Batch Loss: 0.6269794702529907\n",
      "Epoch 2570, Loss: 2.6309428811073303, Final Batch Loss: 0.28654322028160095\n",
      "Epoch 2571, Loss: 2.4069209694862366, Final Batch Loss: 0.4962383806705475\n",
      "Epoch 2572, Loss: 2.5292311310768127, Final Batch Loss: 0.600524365901947\n",
      "Epoch 2573, Loss: 2.54795378446579, Final Batch Loss: 0.5301929116249084\n",
      "Epoch 2574, Loss: 2.738744616508484, Final Batch Loss: 0.6262816190719604\n",
      "Epoch 2575, Loss: 2.5912182927131653, Final Batch Loss: 0.47991153597831726\n",
      "Epoch 2576, Loss: 2.599372059106827, Final Batch Loss: 0.5808501243591309\n",
      "Epoch 2577, Loss: 2.3184123635292053, Final Batch Loss: 0.4312545955181122\n",
      "Epoch 2578, Loss: 2.488753378391266, Final Batch Loss: 0.45727407932281494\n",
      "Epoch 2579, Loss: 2.6137524247169495, Final Batch Loss: 0.4326038360595703\n",
      "Epoch 2580, Loss: 2.5952368080615997, Final Batch Loss: 0.4563837945461273\n",
      "Epoch 2581, Loss: 2.6805544793605804, Final Batch Loss: 0.6037919521331787\n",
      "Epoch 2582, Loss: 2.568920314311981, Final Batch Loss: 0.42670202255249023\n",
      "Epoch 2583, Loss: 2.6105818450450897, Final Batch Loss: 0.5598663687705994\n",
      "Epoch 2584, Loss: 2.476770907640457, Final Batch Loss: 0.6368577480316162\n",
      "Epoch 2585, Loss: 2.427064538002014, Final Batch Loss: 0.4255295991897583\n",
      "Epoch 2586, Loss: 2.5319591760635376, Final Batch Loss: 0.545341432094574\n",
      "Epoch 2587, Loss: 2.662476271390915, Final Batch Loss: 0.5664000511169434\n",
      "Epoch 2588, Loss: 2.4640701711177826, Final Batch Loss: 0.4638702869415283\n",
      "Epoch 2589, Loss: 2.5688557624816895, Final Batch Loss: 0.6507956981658936\n",
      "Epoch 2590, Loss: 2.562178373336792, Final Batch Loss: 0.5466808080673218\n",
      "Epoch 2591, Loss: 2.5546261966228485, Final Batch Loss: 0.467155396938324\n",
      "Epoch 2592, Loss: 2.5988809764385223, Final Batch Loss: 0.523822546005249\n",
      "Epoch 2593, Loss: 2.670867472887039, Final Batch Loss: 0.5628967881202698\n",
      "Epoch 2594, Loss: 2.510589510202408, Final Batch Loss: 0.49791333079338074\n",
      "Epoch 2595, Loss: 2.607341706752777, Final Batch Loss: 0.49634262919425964\n",
      "Epoch 2596, Loss: 2.5401351153850555, Final Batch Loss: 0.46538645029067993\n",
      "Epoch 2597, Loss: 2.363704204559326, Final Batch Loss: 0.46008020639419556\n",
      "Epoch 2598, Loss: 2.562836468219757, Final Batch Loss: 0.47491806745529175\n",
      "Epoch 2599, Loss: 2.4389483630657196, Final Batch Loss: 0.46071866154670715\n",
      "Epoch 2600, Loss: 2.7667814195156097, Final Batch Loss: 0.48173028230667114\n",
      "Epoch 2601, Loss: 2.386118322610855, Final Batch Loss: 0.42740726470947266\n",
      "Epoch 2602, Loss: 2.6869896948337555, Final Batch Loss: 0.5310792922973633\n",
      "Epoch 2603, Loss: 2.656981199979782, Final Batch Loss: 0.48639994859695435\n",
      "Epoch 2604, Loss: 2.3841300308704376, Final Batch Loss: 0.38760557770729065\n",
      "Epoch 2605, Loss: 2.5870834589004517, Final Batch Loss: 0.5863471627235413\n",
      "Epoch 2606, Loss: 2.575478285551071, Final Batch Loss: 0.4728735089302063\n",
      "Epoch 2607, Loss: 2.564370572566986, Final Batch Loss: 0.5671152472496033\n",
      "Epoch 2608, Loss: 2.441098928451538, Final Batch Loss: 0.430504709482193\n",
      "Epoch 2609, Loss: 2.489086151123047, Final Batch Loss: 0.52989661693573\n",
      "Epoch 2610, Loss: 2.481866717338562, Final Batch Loss: 0.6167357563972473\n",
      "Epoch 2611, Loss: 2.454954594373703, Final Batch Loss: 0.4294470548629761\n",
      "Epoch 2612, Loss: 2.575663238763809, Final Batch Loss: 0.49605685472488403\n",
      "Epoch 2613, Loss: 2.5461585521698, Final Batch Loss: 0.5614222288131714\n",
      "Epoch 2614, Loss: 2.5899863839149475, Final Batch Loss: 0.5617824792861938\n",
      "Epoch 2615, Loss: 2.694739431142807, Final Batch Loss: 0.6839523315429688\n",
      "Epoch 2616, Loss: 2.688532769680023, Final Batch Loss: 0.588980495929718\n",
      "Epoch 2617, Loss: 2.514126032590866, Final Batch Loss: 0.5408291816711426\n",
      "Epoch 2618, Loss: 2.4968035221099854, Final Batch Loss: 0.5621711015701294\n",
      "Epoch 2619, Loss: 2.486449509859085, Final Batch Loss: 0.48976850509643555\n",
      "Epoch 2620, Loss: 2.5779635310173035, Final Batch Loss: 0.6767584681510925\n",
      "Epoch 2621, Loss: 2.393486112356186, Final Batch Loss: 0.48660966753959656\n",
      "Epoch 2622, Loss: 2.6567229628562927, Final Batch Loss: 0.7069094181060791\n",
      "Epoch 2623, Loss: 2.3902690708637238, Final Batch Loss: 0.462270587682724\n",
      "Epoch 2624, Loss: 2.562899500131607, Final Batch Loss: 0.5727339386940002\n",
      "Epoch 2625, Loss: 2.439645081758499, Final Batch Loss: 0.5476461052894592\n",
      "Epoch 2626, Loss: 2.7123090028762817, Final Batch Loss: 0.5409002900123596\n",
      "Epoch 2627, Loss: 2.7536010444164276, Final Batch Loss: 0.618284285068512\n",
      "Epoch 2628, Loss: 2.538342773914337, Final Batch Loss: 0.6068534851074219\n",
      "Epoch 2629, Loss: 2.6023323833942413, Final Batch Loss: 0.5880337953567505\n",
      "Epoch 2630, Loss: 2.576019823551178, Final Batch Loss: 0.49056190252304077\n",
      "Epoch 2631, Loss: 2.480311095714569, Final Batch Loss: 0.41665971279144287\n",
      "Epoch 2632, Loss: 2.491763085126877, Final Batch Loss: 0.5076584815979004\n",
      "Epoch 2633, Loss: 2.431033283472061, Final Batch Loss: 0.4061706066131592\n",
      "Epoch 2634, Loss: 2.628370523452759, Final Batch Loss: 0.5779642462730408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2635, Loss: 2.4930554032325745, Final Batch Loss: 0.5600354075431824\n",
      "Epoch 2636, Loss: 2.372483968734741, Final Batch Loss: 0.4871705174446106\n",
      "Epoch 2637, Loss: 2.5148468017578125, Final Batch Loss: 0.5361352562904358\n",
      "Epoch 2638, Loss: 2.622199594974518, Final Batch Loss: 0.572666347026825\n",
      "Epoch 2639, Loss: 2.5588481426239014, Final Batch Loss: 0.5217224359512329\n",
      "Epoch 2640, Loss: 2.4293580055236816, Final Batch Loss: 0.4316815435886383\n",
      "Epoch 2641, Loss: 2.5589589178562164, Final Batch Loss: 0.5851023197174072\n",
      "Epoch 2642, Loss: 2.5419965386390686, Final Batch Loss: 0.46744054555892944\n",
      "Epoch 2643, Loss: 2.5261422097682953, Final Batch Loss: 0.5272802114486694\n",
      "Epoch 2644, Loss: 2.6085670590400696, Final Batch Loss: 0.4763588309288025\n",
      "Epoch 2645, Loss: 2.388476938009262, Final Batch Loss: 0.44874462485313416\n",
      "Epoch 2646, Loss: 2.447235494852066, Final Batch Loss: 0.44309109449386597\n",
      "Epoch 2647, Loss: 2.5227681398391724, Final Batch Loss: 0.47944173216819763\n",
      "Epoch 2648, Loss: 2.6729268729686737, Final Batch Loss: 0.5161046981811523\n",
      "Epoch 2649, Loss: 2.4788542687892914, Final Batch Loss: 0.5553568601608276\n",
      "Epoch 2650, Loss: 2.806089699268341, Final Batch Loss: 0.5926071405410767\n",
      "Epoch 2651, Loss: 2.466699928045273, Final Batch Loss: 0.4071691930294037\n",
      "Epoch 2652, Loss: 2.4443295001983643, Final Batch Loss: 0.5537683963775635\n",
      "Epoch 2653, Loss: 2.604736328125, Final Batch Loss: 0.5144908428192139\n",
      "Epoch 2654, Loss: 2.622528076171875, Final Batch Loss: 0.5831774473190308\n",
      "Epoch 2655, Loss: 2.6472874581813812, Final Batch Loss: 0.5148457884788513\n",
      "Epoch 2656, Loss: 2.490740269422531, Final Batch Loss: 0.43011215329170227\n",
      "Epoch 2657, Loss: 2.469207137823105, Final Batch Loss: 0.47050580382347107\n",
      "Epoch 2658, Loss: 2.4942887723445892, Final Batch Loss: 0.5054118633270264\n",
      "Epoch 2659, Loss: 2.4031828939914703, Final Batch Loss: 0.430942565202713\n",
      "Epoch 2660, Loss: 2.7108538448810577, Final Batch Loss: 0.39739421010017395\n",
      "Epoch 2661, Loss: 2.479244440793991, Final Batch Loss: 0.5547160506248474\n",
      "Epoch 2662, Loss: 2.4304317831993103, Final Batch Loss: 0.42974138259887695\n",
      "Epoch 2663, Loss: 2.4166438281536102, Final Batch Loss: 0.4623798131942749\n",
      "Epoch 2664, Loss: 2.340454548597336, Final Batch Loss: 0.4988020360469818\n",
      "Epoch 2665, Loss: 2.5358303487300873, Final Batch Loss: 0.4292250871658325\n",
      "Epoch 2666, Loss: 2.560885488986969, Final Batch Loss: 0.6164145469665527\n",
      "Epoch 2667, Loss: 2.5085196793079376, Final Batch Loss: 0.5117719769477844\n",
      "Epoch 2668, Loss: 2.8128910660743713, Final Batch Loss: 0.690230667591095\n",
      "Epoch 2669, Loss: 2.586338758468628, Final Batch Loss: 0.511227011680603\n",
      "Epoch 2670, Loss: 2.378478467464447, Final Batch Loss: 0.3929639160633087\n",
      "Epoch 2671, Loss: 2.5124455392360687, Final Batch Loss: 0.535857081413269\n",
      "Epoch 2672, Loss: 2.722675710916519, Final Batch Loss: 0.7066104412078857\n",
      "Epoch 2673, Loss: 2.5475527942180634, Final Batch Loss: 0.5087911486625671\n",
      "Epoch 2674, Loss: 2.432510882616043, Final Batch Loss: 0.38494160771369934\n",
      "Epoch 2675, Loss: 2.5581910610198975, Final Batch Loss: 0.5200751423835754\n",
      "Epoch 2676, Loss: 2.5828712582588196, Final Batch Loss: 0.49086546897888184\n",
      "Epoch 2677, Loss: 2.509611636400223, Final Batch Loss: 0.5270742774009705\n",
      "Epoch 2678, Loss: 2.588504374027252, Final Batch Loss: 0.46694767475128174\n",
      "Epoch 2679, Loss: 2.4228179156780243, Final Batch Loss: 0.47146323323249817\n",
      "Epoch 2680, Loss: 2.678506761789322, Final Batch Loss: 0.645438015460968\n",
      "Epoch 2681, Loss: 2.4652114510536194, Final Batch Loss: 0.6269201636314392\n",
      "Epoch 2682, Loss: 2.500294268131256, Final Batch Loss: 0.43307337164878845\n",
      "Epoch 2683, Loss: 2.402529001235962, Final Batch Loss: 0.541022002696991\n",
      "Epoch 2684, Loss: 2.5049502849578857, Final Batch Loss: 0.510421097278595\n",
      "Epoch 2685, Loss: 2.496401935815811, Final Batch Loss: 0.4564525783061981\n",
      "Epoch 2686, Loss: 2.468070864677429, Final Batch Loss: 0.39945515990257263\n",
      "Epoch 2687, Loss: 2.7177259624004364, Final Batch Loss: 0.731211245059967\n",
      "Epoch 2688, Loss: 2.510127305984497, Final Batch Loss: 0.49034738540649414\n",
      "Epoch 2689, Loss: 3.0021251142024994, Final Batch Loss: 0.9208769798278809\n",
      "Epoch 2690, Loss: 2.625128388404846, Final Batch Loss: 0.4791576564311981\n",
      "Epoch 2691, Loss: 2.5207769572734833, Final Batch Loss: 0.5112816095352173\n",
      "Epoch 2692, Loss: 2.4328432083129883, Final Batch Loss: 0.445148766040802\n",
      "Epoch 2693, Loss: 2.5045755207538605, Final Batch Loss: 0.5902986526489258\n",
      "Epoch 2694, Loss: 2.4893301129341125, Final Batch Loss: 0.5230439901351929\n",
      "Epoch 2695, Loss: 2.5429488718509674, Final Batch Loss: 0.4906371533870697\n",
      "Epoch 2696, Loss: 2.3719249069690704, Final Batch Loss: 0.42947065830230713\n",
      "Epoch 2697, Loss: 2.5548005402088165, Final Batch Loss: 0.6140857338905334\n",
      "Epoch 2698, Loss: 2.535834163427353, Final Batch Loss: 0.477016419172287\n",
      "Epoch 2699, Loss: 2.4071940779685974, Final Batch Loss: 0.4660503566265106\n",
      "Epoch 2700, Loss: 2.500148445367813, Final Batch Loss: 0.46727946400642395\n",
      "Epoch 2701, Loss: 2.505815863609314, Final Batch Loss: 0.4992571175098419\n",
      "Epoch 2702, Loss: 2.625273734331131, Final Batch Loss: 0.5968878865242004\n",
      "Epoch 2703, Loss: 2.54634690284729, Final Batch Loss: 0.4479820132255554\n",
      "Epoch 2704, Loss: 2.521567225456238, Final Batch Loss: 0.4751386046409607\n",
      "Epoch 2705, Loss: 2.4090503454208374, Final Batch Loss: 0.47356098890304565\n",
      "Epoch 2706, Loss: 2.7332251965999603, Final Batch Loss: 0.6174202561378479\n",
      "Epoch 2707, Loss: 2.5034225285053253, Final Batch Loss: 0.5505856871604919\n",
      "Epoch 2708, Loss: 2.421828657388687, Final Batch Loss: 0.4946821630001068\n",
      "Epoch 2709, Loss: 2.5243878066539764, Final Batch Loss: 0.5110415816307068\n",
      "Epoch 2710, Loss: 2.4974460005760193, Final Batch Loss: 0.4613784849643707\n",
      "Epoch 2711, Loss: 2.559381902217865, Final Batch Loss: 0.5480591058731079\n",
      "Epoch 2712, Loss: 2.5812356770038605, Final Batch Loss: 0.5407523512840271\n",
      "Epoch 2713, Loss: 2.317566990852356, Final Batch Loss: 0.37013664841651917\n",
      "Epoch 2714, Loss: 2.458191454410553, Final Batch Loss: 0.5116356611251831\n",
      "Epoch 2715, Loss: 2.579454332590103, Final Batch Loss: 0.43279775977134705\n",
      "Epoch 2716, Loss: 2.3780031502246857, Final Batch Loss: 0.4781612455844879\n",
      "Epoch 2717, Loss: 2.52291938662529, Final Batch Loss: 0.5878612995147705\n",
      "Epoch 2718, Loss: 2.470927447080612, Final Batch Loss: 0.4897626042366028\n",
      "Epoch 2719, Loss: 2.4982593655586243, Final Batch Loss: 0.4789547026157379\n",
      "Epoch 2720, Loss: 2.592787981033325, Final Batch Loss: 0.6399681568145752\n",
      "Epoch 2721, Loss: 2.4393844306468964, Final Batch Loss: 0.47902631759643555\n",
      "Epoch 2722, Loss: 2.481510877609253, Final Batch Loss: 0.49149322509765625\n",
      "Epoch 2723, Loss: 2.7527459859848022, Final Batch Loss: 0.4978346824645996\n",
      "Epoch 2724, Loss: 2.574984222650528, Final Batch Loss: 0.5295597314834595\n",
      "Epoch 2725, Loss: 2.440881848335266, Final Batch Loss: 0.45715901255607605\n",
      "Epoch 2726, Loss: 2.7154603004455566, Final Batch Loss: 0.6268417239189148\n",
      "Epoch 2727, Loss: 2.3906012177467346, Final Batch Loss: 0.41703394055366516\n",
      "Epoch 2728, Loss: 2.4223800599575043, Final Batch Loss: 0.5172565579414368\n",
      "Epoch 2729, Loss: 2.5455667972564697, Final Batch Loss: 0.45087626576423645\n",
      "Epoch 2730, Loss: 2.4996051490306854, Final Batch Loss: 0.5501838326454163\n",
      "Epoch 2731, Loss: 2.326845735311508, Final Batch Loss: 0.46037065982818604\n",
      "Epoch 2732, Loss: 2.5454896986484528, Final Batch Loss: 0.6532914042472839\n",
      "Epoch 2733, Loss: 2.4511484801769257, Final Batch Loss: 0.5801365971565247\n",
      "Epoch 2734, Loss: 2.394532948732376, Final Batch Loss: 0.4905935823917389\n",
      "Epoch 2735, Loss: 2.344499111175537, Final Batch Loss: 0.39162683486938477\n",
      "Epoch 2736, Loss: 2.447761118412018, Final Batch Loss: 0.45442816615104675\n",
      "Epoch 2737, Loss: 2.569451719522476, Final Batch Loss: 0.7201398015022278\n",
      "Epoch 2738, Loss: 2.382552206516266, Final Batch Loss: 0.47424647212028503\n",
      "Epoch 2739, Loss: 2.57538178563118, Final Batch Loss: 0.4958241283893585\n",
      "Epoch 2740, Loss: 2.325679123401642, Final Batch Loss: 0.49984124302864075\n",
      "Epoch 2741, Loss: 2.3391703069210052, Final Batch Loss: 0.3606822192668915\n",
      "Epoch 2742, Loss: 2.564998686313629, Final Batch Loss: 0.44966983795166016\n",
      "Epoch 2743, Loss: 2.4358560740947723, Final Batch Loss: 0.5197017192840576\n",
      "Epoch 2744, Loss: 2.5410771667957306, Final Batch Loss: 0.39535510540008545\n",
      "Epoch 2745, Loss: 2.6592955589294434, Final Batch Loss: 0.47559693455696106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2746, Loss: 2.5711552798748016, Final Batch Loss: 0.5004583597183228\n",
      "Epoch 2747, Loss: 2.5861860513687134, Final Batch Loss: 0.4742659330368042\n",
      "Epoch 2748, Loss: 2.593604564666748, Final Batch Loss: 0.4812229573726654\n",
      "Epoch 2749, Loss: 2.3855444192886353, Final Batch Loss: 0.41845086216926575\n",
      "Epoch 2750, Loss: 2.3854637145996094, Final Batch Loss: 0.449506938457489\n",
      "Epoch 2751, Loss: 2.398902952671051, Final Batch Loss: 0.4890925884246826\n",
      "Epoch 2752, Loss: 2.4225596487522125, Final Batch Loss: 0.4678416848182678\n",
      "Epoch 2753, Loss: 2.371466875076294, Final Batch Loss: 0.4408184289932251\n",
      "Epoch 2754, Loss: 2.50715234875679, Final Batch Loss: 0.4891163408756256\n",
      "Epoch 2755, Loss: 2.580998659133911, Final Batch Loss: 0.5591230392456055\n",
      "Epoch 2756, Loss: 2.441654682159424, Final Batch Loss: 0.4290328323841095\n",
      "Epoch 2757, Loss: 2.422651022672653, Final Batch Loss: 0.47298121452331543\n",
      "Epoch 2758, Loss: 2.5243420004844666, Final Batch Loss: 0.5213325023651123\n",
      "Epoch 2759, Loss: 2.6215112507343292, Final Batch Loss: 0.5684751868247986\n",
      "Epoch 2760, Loss: 2.5147100389003754, Final Batch Loss: 0.5463644862174988\n",
      "Epoch 2761, Loss: 2.406470835208893, Final Batch Loss: 0.5074523091316223\n",
      "Epoch 2762, Loss: 2.593727707862854, Final Batch Loss: 0.5053147673606873\n",
      "Epoch 2763, Loss: 2.3977406919002533, Final Batch Loss: 0.34718847274780273\n",
      "Epoch 2764, Loss: 2.437887668609619, Final Batch Loss: 0.4137074947357178\n",
      "Epoch 2765, Loss: 2.3773750364780426, Final Batch Loss: 0.5002260804176331\n",
      "Epoch 2766, Loss: 2.310538023710251, Final Batch Loss: 0.4121972620487213\n",
      "Epoch 2767, Loss: 2.4015877544879913, Final Batch Loss: 0.5117825269699097\n",
      "Epoch 2768, Loss: 2.568279206752777, Final Batch Loss: 0.4688500761985779\n",
      "Epoch 2769, Loss: 2.4705463647842407, Final Batch Loss: 0.49631667137145996\n",
      "Epoch 2770, Loss: 2.574485033750534, Final Batch Loss: 0.4940410256385803\n",
      "Epoch 2771, Loss: 2.5132050216197968, Final Batch Loss: 0.5125178694725037\n",
      "Epoch 2772, Loss: 2.394952714443207, Final Batch Loss: 0.42006051540374756\n",
      "Epoch 2773, Loss: 2.3711065649986267, Final Batch Loss: 0.498580664396286\n",
      "Epoch 2774, Loss: 2.6491871774196625, Final Batch Loss: 0.580065906047821\n",
      "Epoch 2775, Loss: 2.348518282175064, Final Batch Loss: 0.5348930358886719\n",
      "Epoch 2776, Loss: 2.560847759246826, Final Batch Loss: 0.560826301574707\n",
      "Epoch 2777, Loss: 2.4743312001228333, Final Batch Loss: 0.4548976421356201\n",
      "Epoch 2778, Loss: 2.4343408048152924, Final Batch Loss: 0.5579906702041626\n",
      "Epoch 2779, Loss: 2.397891581058502, Final Batch Loss: 0.44475993514060974\n",
      "Epoch 2780, Loss: 2.377570629119873, Final Batch Loss: 0.46947336196899414\n",
      "Epoch 2781, Loss: 2.3284063935279846, Final Batch Loss: 0.36193791031837463\n",
      "Epoch 2782, Loss: 2.509999692440033, Final Batch Loss: 0.46864229440689087\n",
      "Epoch 2783, Loss: 2.6488237380981445, Final Batch Loss: 0.5589150786399841\n",
      "Epoch 2784, Loss: 2.2647624909877777, Final Batch Loss: 0.4310220777988434\n",
      "Epoch 2785, Loss: 2.309453070163727, Final Batch Loss: 0.41787269711494446\n",
      "Epoch 2786, Loss: 2.4960822463035583, Final Batch Loss: 0.6027460694313049\n",
      "Epoch 2787, Loss: 2.4570744931697845, Final Batch Loss: 0.5104696750640869\n",
      "Epoch 2788, Loss: 2.5276787281036377, Final Batch Loss: 0.4723518490791321\n",
      "Epoch 2789, Loss: 2.5597223043441772, Final Batch Loss: 0.5010713338851929\n",
      "Epoch 2790, Loss: 2.4471414983272552, Final Batch Loss: 0.42763715982437134\n",
      "Epoch 2791, Loss: 2.4618685841560364, Final Batch Loss: 0.40497058629989624\n",
      "Epoch 2792, Loss: 2.4574644565582275, Final Batch Loss: 0.5091962814331055\n",
      "Epoch 2793, Loss: 2.462512493133545, Final Batch Loss: 0.4740833044052124\n",
      "Epoch 2794, Loss: 2.4579865634441376, Final Batch Loss: 0.4942377507686615\n",
      "Epoch 2795, Loss: 2.502702534198761, Final Batch Loss: 0.5971733331680298\n",
      "Epoch 2796, Loss: 2.5405865907669067, Final Batch Loss: 0.5145670175552368\n",
      "Epoch 2797, Loss: 2.4943365454673767, Final Batch Loss: 0.5047346353530884\n",
      "Epoch 2798, Loss: 2.3313062489032745, Final Batch Loss: 0.40613967180252075\n",
      "Epoch 2799, Loss: 2.4967625737190247, Final Batch Loss: 0.5383886694908142\n",
      "Epoch 2800, Loss: 2.4616748094558716, Final Batch Loss: 0.5179854035377502\n",
      "Epoch 2801, Loss: 2.459199011325836, Final Batch Loss: 0.5037930607795715\n",
      "Epoch 2802, Loss: 2.512591004371643, Final Batch Loss: 0.627062976360321\n",
      "Epoch 2803, Loss: 2.5966033339500427, Final Batch Loss: 0.5456379652023315\n",
      "Epoch 2804, Loss: 2.5045948922634125, Final Batch Loss: 0.5179988741874695\n",
      "Epoch 2805, Loss: 2.3157990276813507, Final Batch Loss: 0.4868287146091461\n",
      "Epoch 2806, Loss: 2.519308924674988, Final Batch Loss: 0.5266306400299072\n",
      "Epoch 2807, Loss: 2.3946100771427155, Final Batch Loss: 0.5702624917030334\n",
      "Epoch 2808, Loss: 2.5521905422210693, Final Batch Loss: 0.47340309619903564\n",
      "Epoch 2809, Loss: 2.4324706494808197, Final Batch Loss: 0.5079157948493958\n",
      "Epoch 2810, Loss: 2.408181756734848, Final Batch Loss: 0.41365036368370056\n",
      "Epoch 2811, Loss: 2.680845707654953, Final Batch Loss: 0.5990073084831238\n",
      "Epoch 2812, Loss: 2.6425682604312897, Final Batch Loss: 0.7378599047660828\n",
      "Epoch 2813, Loss: 2.729449689388275, Final Batch Loss: 0.5317648649215698\n",
      "Epoch 2814, Loss: 2.4067377150058746, Final Batch Loss: 0.45709213614463806\n",
      "Epoch 2815, Loss: 2.5153113901615143, Final Batch Loss: 0.6336223483085632\n",
      "Epoch 2816, Loss: 2.6021608412265778, Final Batch Loss: 0.4219793677330017\n",
      "Epoch 2817, Loss: 2.615170657634735, Final Batch Loss: 0.5424332022666931\n",
      "Epoch 2818, Loss: 2.541297495365143, Final Batch Loss: 0.5928082466125488\n",
      "Epoch 2819, Loss: 2.473896414041519, Final Batch Loss: 0.5044493675231934\n",
      "Epoch 2820, Loss: 2.4027897119522095, Final Batch Loss: 0.46478644013404846\n",
      "Epoch 2821, Loss: 2.6409955620765686, Final Batch Loss: 0.3910101056098938\n",
      "Epoch 2822, Loss: 2.366773873567581, Final Batch Loss: 0.4354607164859772\n",
      "Epoch 2823, Loss: 2.6764268279075623, Final Batch Loss: 0.4521636366844177\n",
      "Epoch 2824, Loss: 2.394503891468048, Final Batch Loss: 0.45943328738212585\n",
      "Epoch 2825, Loss: 2.478414863348007, Final Batch Loss: 0.545168936252594\n",
      "Epoch 2826, Loss: 2.5111806392669678, Final Batch Loss: 0.514388382434845\n",
      "Epoch 2827, Loss: 2.647590458393097, Final Batch Loss: 0.6148658990859985\n",
      "Epoch 2828, Loss: 2.5640664100646973, Final Batch Loss: 0.6953450441360474\n",
      "Epoch 2829, Loss: 2.422383427619934, Final Batch Loss: 0.41062623262405396\n",
      "Epoch 2830, Loss: 2.4640378952026367, Final Batch Loss: 0.4108279049396515\n",
      "Epoch 2831, Loss: 2.4234898388385773, Final Batch Loss: 0.438460111618042\n",
      "Epoch 2832, Loss: 2.5535014271736145, Final Batch Loss: 0.5692064166069031\n",
      "Epoch 2833, Loss: 2.4687457978725433, Final Batch Loss: 0.5499756336212158\n",
      "Epoch 2834, Loss: 2.510399580001831, Final Batch Loss: 0.4754071533679962\n",
      "Epoch 2835, Loss: 2.469062715768814, Final Batch Loss: 0.5948721766471863\n",
      "Epoch 2836, Loss: 2.4390305876731873, Final Batch Loss: 0.4922069013118744\n",
      "Epoch 2837, Loss: 2.4135168194770813, Final Batch Loss: 0.40016689896583557\n",
      "Epoch 2838, Loss: 2.4048417806625366, Final Batch Loss: 0.4193374216556549\n",
      "Epoch 2839, Loss: 2.5479190945625305, Final Batch Loss: 0.4365428686141968\n",
      "Epoch 2840, Loss: 2.3616080284118652, Final Batch Loss: 0.48315292596817017\n",
      "Epoch 2841, Loss: 2.389273077249527, Final Batch Loss: 0.5324241518974304\n",
      "Epoch 2842, Loss: 2.354143351316452, Final Batch Loss: 0.5257344841957092\n",
      "Epoch 2843, Loss: 2.4927998185157776, Final Batch Loss: 0.4664960205554962\n",
      "Epoch 2844, Loss: 2.5141113996505737, Final Batch Loss: 0.466743141412735\n",
      "Epoch 2845, Loss: 2.420401155948639, Final Batch Loss: 0.44689494371414185\n",
      "Epoch 2846, Loss: 2.4097021520137787, Final Batch Loss: 0.44692718982696533\n",
      "Epoch 2847, Loss: 2.5044883489608765, Final Batch Loss: 0.5377220511436462\n",
      "Epoch 2848, Loss: 2.5322733521461487, Final Batch Loss: 0.5125673413276672\n",
      "Epoch 2849, Loss: 2.4318000972270966, Final Batch Loss: 0.4424714744091034\n",
      "Epoch 2850, Loss: 2.386031925678253, Final Batch Loss: 0.5105538964271545\n",
      "Epoch 2851, Loss: 2.2491227984428406, Final Batch Loss: 0.3962753117084503\n",
      "Epoch 2852, Loss: 2.366912990808487, Final Batch Loss: 0.5191313624382019\n",
      "Epoch 2853, Loss: 2.47485214471817, Final Batch Loss: 0.536837100982666\n",
      "Epoch 2854, Loss: 2.2235283255577087, Final Batch Loss: 0.43554848432540894\n",
      "Epoch 2855, Loss: 2.38277330994606, Final Batch Loss: 0.46435368061065674\n",
      "Epoch 2856, Loss: 2.4416098296642303, Final Batch Loss: 0.5765553116798401\n",
      "Epoch 2857, Loss: 2.4571580290794373, Final Batch Loss: 0.6424787044525146\n",
      "Epoch 2858, Loss: 2.3178703784942627, Final Batch Loss: 0.4930287301540375\n",
      "Epoch 2859, Loss: 2.5608841478824615, Final Batch Loss: 0.6256501078605652\n",
      "Epoch 2860, Loss: 2.624344766139984, Final Batch Loss: 0.661043643951416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2861, Loss: 2.636629670858383, Final Batch Loss: 0.5011946558952332\n",
      "Epoch 2862, Loss: 2.4524459540843964, Final Batch Loss: 0.49225157499313354\n",
      "Epoch 2863, Loss: 2.6972488164901733, Final Batch Loss: 0.6912213563919067\n",
      "Epoch 2864, Loss: 2.466494768857956, Final Batch Loss: 0.4403470754623413\n",
      "Epoch 2865, Loss: 2.484742522239685, Final Batch Loss: 0.48184314370155334\n",
      "Epoch 2866, Loss: 2.5174659490585327, Final Batch Loss: 0.5572450160980225\n",
      "Epoch 2867, Loss: 2.6116314828395844, Final Batch Loss: 0.5438624024391174\n",
      "Epoch 2868, Loss: 2.5524838864803314, Final Batch Loss: 0.49167510867118835\n",
      "Epoch 2869, Loss: 2.3525748550891876, Final Batch Loss: 0.44269078969955444\n",
      "Epoch 2870, Loss: 2.181135207414627, Final Batch Loss: 0.37023916840553284\n",
      "Epoch 2871, Loss: 2.423904985189438, Final Batch Loss: 0.47907620668411255\n",
      "Epoch 2872, Loss: 2.472289264202118, Final Batch Loss: 0.4812909960746765\n",
      "Epoch 2873, Loss: 2.480646252632141, Final Batch Loss: 0.6151344776153564\n",
      "Epoch 2874, Loss: 2.5397519171237946, Final Batch Loss: 0.43307673931121826\n",
      "Epoch 2875, Loss: 2.581232786178589, Final Batch Loss: 0.5119598507881165\n",
      "Epoch 2876, Loss: 2.719122678041458, Final Batch Loss: 0.43448179960250854\n",
      "Epoch 2877, Loss: 2.3781684041023254, Final Batch Loss: 0.5230735540390015\n",
      "Epoch 2878, Loss: 2.3155781626701355, Final Batch Loss: 0.3820462226867676\n",
      "Epoch 2879, Loss: 2.4568879306316376, Final Batch Loss: 0.3828490674495697\n",
      "Epoch 2880, Loss: 2.4312608540058136, Final Batch Loss: 0.49906405806541443\n",
      "Epoch 2881, Loss: 2.3078658282756805, Final Batch Loss: 0.4802198112010956\n",
      "Epoch 2882, Loss: 2.341723710298538, Final Batch Loss: 0.38829317688941956\n",
      "Epoch 2883, Loss: 2.4681575894355774, Final Batch Loss: 0.5365097522735596\n",
      "Epoch 2884, Loss: 2.4312958121299744, Final Batch Loss: 0.47718173265457153\n",
      "Epoch 2885, Loss: 2.5835247933864594, Final Batch Loss: 0.4378965497016907\n",
      "Epoch 2886, Loss: 2.4739663898944855, Final Batch Loss: 0.5017487406730652\n",
      "Epoch 2887, Loss: 2.4862672686576843, Final Batch Loss: 0.48110678791999817\n",
      "Epoch 2888, Loss: 2.433956414461136, Final Batch Loss: 0.41890138387680054\n",
      "Epoch 2889, Loss: 2.5061726570129395, Final Batch Loss: 0.4197342097759247\n",
      "Epoch 2890, Loss: 2.3894243240356445, Final Batch Loss: 0.4785950183868408\n",
      "Epoch 2891, Loss: 2.5328676402568817, Final Batch Loss: 0.4741041660308838\n",
      "Epoch 2892, Loss: 2.5947543680667877, Final Batch Loss: 0.4914385676383972\n",
      "Epoch 2893, Loss: 2.388357937335968, Final Batch Loss: 0.49529793858528137\n",
      "Epoch 2894, Loss: 2.666693091392517, Final Batch Loss: 0.5587542057037354\n",
      "Epoch 2895, Loss: 2.3776154220104218, Final Batch Loss: 0.46422356367111206\n",
      "Epoch 2896, Loss: 2.5462836027145386, Final Batch Loss: 0.5343703031539917\n",
      "Epoch 2897, Loss: 2.5391503870487213, Final Batch Loss: 0.6114281415939331\n",
      "Epoch 2898, Loss: 2.5333547592163086, Final Batch Loss: 0.40821290016174316\n",
      "Epoch 2899, Loss: 2.4278750121593475, Final Batch Loss: 0.5663080811500549\n",
      "Epoch 2900, Loss: 2.33371764421463, Final Batch Loss: 0.4021656811237335\n",
      "Epoch 2901, Loss: 2.3819338381290436, Final Batch Loss: 0.37426748871803284\n",
      "Epoch 2902, Loss: 2.593876600265503, Final Batch Loss: 0.6094160079956055\n",
      "Epoch 2903, Loss: 2.454866796731949, Final Batch Loss: 0.5365777015686035\n",
      "Epoch 2904, Loss: 2.4516439139842987, Final Batch Loss: 0.4276042878627777\n",
      "Epoch 2905, Loss: 2.287790834903717, Final Batch Loss: 0.5199924111366272\n",
      "Epoch 2906, Loss: 2.525418996810913, Final Batch Loss: 0.434047669172287\n",
      "Epoch 2907, Loss: 2.529914140701294, Final Batch Loss: 0.49909546971321106\n",
      "Epoch 2908, Loss: 2.3757115602493286, Final Batch Loss: 0.4393467903137207\n",
      "Epoch 2909, Loss: 2.457639515399933, Final Batch Loss: 0.5292518138885498\n",
      "Epoch 2910, Loss: 2.5021331012248993, Final Batch Loss: 0.4557398855686188\n",
      "Epoch 2911, Loss: 2.3203073143959045, Final Batch Loss: 0.4990178048610687\n",
      "Epoch 2912, Loss: 2.498563677072525, Final Batch Loss: 0.46340325474739075\n",
      "Epoch 2913, Loss: 2.425223708152771, Final Batch Loss: 0.4079184830188751\n",
      "Epoch 2914, Loss: 2.255853295326233, Final Batch Loss: 0.42437270283699036\n",
      "Epoch 2915, Loss: 2.47690811753273, Final Batch Loss: 0.6316623091697693\n",
      "Epoch 2916, Loss: 2.4366470873355865, Final Batch Loss: 0.4928838908672333\n",
      "Epoch 2917, Loss: 2.3756037950515747, Final Batch Loss: 0.3870874047279358\n",
      "Epoch 2918, Loss: 2.6632895469665527, Final Batch Loss: 0.5336564779281616\n",
      "Epoch 2919, Loss: 2.369934320449829, Final Batch Loss: 0.4438396096229553\n",
      "Epoch 2920, Loss: 2.4216851592063904, Final Batch Loss: 0.5693947076797485\n",
      "Epoch 2921, Loss: 2.4954882860183716, Final Batch Loss: 0.4205959141254425\n",
      "Epoch 2922, Loss: 2.3908827900886536, Final Batch Loss: 0.31101685762405396\n",
      "Epoch 2923, Loss: 2.300549626350403, Final Batch Loss: 0.4916839599609375\n",
      "Epoch 2924, Loss: 2.4068575501441956, Final Batch Loss: 0.5898299217224121\n",
      "Epoch 2925, Loss: 2.390883684158325, Final Batch Loss: 0.47798603773117065\n",
      "Epoch 2926, Loss: 2.39764204621315, Final Batch Loss: 0.4204222857952118\n",
      "Epoch 2927, Loss: 2.57270011305809, Final Batch Loss: 0.4642648696899414\n",
      "Epoch 2928, Loss: 2.46787565946579, Final Batch Loss: 0.4778728187084198\n",
      "Epoch 2929, Loss: 2.6768878400325775, Final Batch Loss: 0.5132526159286499\n",
      "Epoch 2930, Loss: 2.387708157300949, Final Batch Loss: 0.4760035574436188\n",
      "Epoch 2931, Loss: 2.305621027946472, Final Batch Loss: 0.4640064239501953\n",
      "Epoch 2932, Loss: 2.4634940922260284, Final Batch Loss: 0.40465548634529114\n",
      "Epoch 2933, Loss: 2.5328939259052277, Final Batch Loss: 0.6029748320579529\n",
      "Epoch 2934, Loss: 2.3774602115154266, Final Batch Loss: 0.4089009165763855\n",
      "Epoch 2935, Loss: 2.315959870815277, Final Batch Loss: 0.47530055046081543\n",
      "Epoch 2936, Loss: 2.3557611107826233, Final Batch Loss: 0.40299078822135925\n",
      "Epoch 2937, Loss: 2.5121699273586273, Final Batch Loss: 0.6088957786560059\n",
      "Epoch 2938, Loss: 2.2244034707546234, Final Batch Loss: 0.37746453285217285\n",
      "Epoch 2939, Loss: 2.4352498650550842, Final Batch Loss: 0.5994961261749268\n",
      "Epoch 2940, Loss: 2.677647888660431, Final Batch Loss: 0.43638449907302856\n",
      "Epoch 2941, Loss: 2.509987860918045, Final Batch Loss: 0.5218761563301086\n",
      "Epoch 2942, Loss: 2.5751309990882874, Final Batch Loss: 0.4904210567474365\n",
      "Epoch 2943, Loss: 2.5529165863990784, Final Batch Loss: 0.5450651049613953\n",
      "Epoch 2944, Loss: 2.5817126631736755, Final Batch Loss: 0.6332793235778809\n",
      "Epoch 2945, Loss: 2.2914680540561676, Final Batch Loss: 0.449720561504364\n",
      "Epoch 2946, Loss: 2.482183963060379, Final Batch Loss: 0.57389897108078\n",
      "Epoch 2947, Loss: 2.6365147829055786, Final Batch Loss: 0.49732959270477295\n",
      "Epoch 2948, Loss: 2.5096116960048676, Final Batch Loss: 0.4577637314796448\n",
      "Epoch 2949, Loss: 2.4373143017292023, Final Batch Loss: 0.5559516549110413\n",
      "Epoch 2950, Loss: 2.4704950749874115, Final Batch Loss: 0.560555100440979\n",
      "Epoch 2951, Loss: 2.4518028795719147, Final Batch Loss: 0.4780384600162506\n",
      "Epoch 2952, Loss: 2.3331406116485596, Final Batch Loss: 0.5711771845817566\n",
      "Epoch 2953, Loss: 2.4289943277835846, Final Batch Loss: 0.4850439727306366\n",
      "Epoch 2954, Loss: 2.371109515428543, Final Batch Loss: 0.49209415912628174\n",
      "Epoch 2955, Loss: 2.442524641752243, Final Batch Loss: 0.5159565210342407\n",
      "Epoch 2956, Loss: 2.5799361169338226, Final Batch Loss: 0.6140263080596924\n",
      "Epoch 2957, Loss: 2.3651614487171173, Final Batch Loss: 0.4534558057785034\n",
      "Epoch 2958, Loss: 2.663832873106003, Final Batch Loss: 0.5701250433921814\n",
      "Epoch 2959, Loss: 2.4814271926879883, Final Batch Loss: 0.451038658618927\n",
      "Epoch 2960, Loss: 2.4305619299411774, Final Batch Loss: 0.5542177557945251\n",
      "Epoch 2961, Loss: 2.6307193338871, Final Batch Loss: 0.4603256285190582\n",
      "Epoch 2962, Loss: 2.4509710371494293, Final Batch Loss: 0.5784229636192322\n",
      "Epoch 2963, Loss: 2.560512602329254, Final Batch Loss: 0.5230000019073486\n",
      "Epoch 2964, Loss: 2.5030597150325775, Final Batch Loss: 0.5669398903846741\n",
      "Epoch 2965, Loss: 2.2784058153629303, Final Batch Loss: 0.36100974678993225\n",
      "Epoch 2966, Loss: 2.4540569484233856, Final Batch Loss: 0.5457897782325745\n",
      "Epoch 2967, Loss: 2.4336652755737305, Final Batch Loss: 0.5108649730682373\n",
      "Epoch 2968, Loss: 2.5071718990802765, Final Batch Loss: 0.48494115471839905\n",
      "Epoch 2969, Loss: 2.322888672351837, Final Batch Loss: 0.3845307528972626\n",
      "Epoch 2970, Loss: 2.3356290459632874, Final Batch Loss: 0.3855977952480316\n",
      "Epoch 2971, Loss: 2.6889021396636963, Final Batch Loss: 0.46951189637184143\n",
      "Epoch 2972, Loss: 2.2966571748256683, Final Batch Loss: 0.35922771692276\n",
      "Epoch 2973, Loss: 2.527274876832962, Final Batch Loss: 0.550899863243103\n",
      "Epoch 2974, Loss: 2.3341462314128876, Final Batch Loss: 0.436572402715683\n",
      "Epoch 2975, Loss: 2.3499973118305206, Final Batch Loss: 0.5775678157806396\n",
      "Epoch 2976, Loss: 2.286268651485443, Final Batch Loss: 0.47580692172050476\n",
      "Epoch 2977, Loss: 2.4069587886333466, Final Batch Loss: 0.5095568299293518\n",
      "Epoch 2978, Loss: 2.589551717042923, Final Batch Loss: 0.6159051060676575\n",
      "Epoch 2979, Loss: 2.422452747821808, Final Batch Loss: 0.47326022386550903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2980, Loss: 2.424896538257599, Final Batch Loss: 0.34657299518585205\n",
      "Epoch 2981, Loss: 2.376314252614975, Final Batch Loss: 0.4611884355545044\n",
      "Epoch 2982, Loss: 2.405226618051529, Final Batch Loss: 0.5015307664871216\n",
      "Epoch 2983, Loss: 2.320762276649475, Final Batch Loss: 0.455861359834671\n",
      "Epoch 2984, Loss: 2.4813880026340485, Final Batch Loss: 0.49088597297668457\n",
      "Epoch 2985, Loss: 2.420486092567444, Final Batch Loss: 0.5053644180297852\n",
      "Epoch 2986, Loss: 2.5553620755672455, Final Batch Loss: 0.5212680101394653\n",
      "Epoch 2987, Loss: 2.44318088889122, Final Batch Loss: 0.41146859526634216\n",
      "Epoch 2988, Loss: 2.469443142414093, Final Batch Loss: 0.5180228352546692\n",
      "Epoch 2989, Loss: 2.5481428503990173, Final Batch Loss: 0.5780563950538635\n",
      "Epoch 2990, Loss: 2.537564367055893, Final Batch Loss: 0.5640842318534851\n",
      "Epoch 2991, Loss: 2.6146678626537323, Final Batch Loss: 0.6713847517967224\n",
      "Epoch 2992, Loss: 2.3434562385082245, Final Batch Loss: 0.3931104242801666\n",
      "Epoch 2993, Loss: 2.537040024995804, Final Batch Loss: 0.5267688632011414\n",
      "Epoch 2994, Loss: 2.4194024205207825, Final Batch Loss: 0.47326940298080444\n",
      "Epoch 2995, Loss: 2.4904232919216156, Final Batch Loss: 0.5183941125869751\n",
      "Epoch 2996, Loss: 2.5380589365959167, Final Batch Loss: 0.5335721373558044\n",
      "Epoch 2997, Loss: 2.249327629804611, Final Batch Loss: 0.4016926884651184\n",
      "Epoch 2998, Loss: 2.261488676071167, Final Batch Loss: 0.48112303018569946\n",
      "Epoch 2999, Loss: 2.6514534652233124, Final Batch Loss: 0.5589466094970703\n",
      "Epoch 3000, Loss: 2.2991566359996796, Final Batch Loss: 0.47463899850845337\n",
      "Epoch 3001, Loss: 2.598568022251129, Final Batch Loss: 0.365263432264328\n",
      "Epoch 3002, Loss: 2.637423098087311, Final Batch Loss: 0.5881574153900146\n",
      "Epoch 3003, Loss: 2.437824547290802, Final Batch Loss: 0.421782523393631\n",
      "Epoch 3004, Loss: 2.515662521123886, Final Batch Loss: 0.5504780411720276\n",
      "Epoch 3005, Loss: 2.484495908021927, Final Batch Loss: 0.5937384366989136\n",
      "Epoch 3006, Loss: 2.440757632255554, Final Batch Loss: 0.5172104835510254\n",
      "Epoch 3007, Loss: 2.3931442201137543, Final Batch Loss: 0.46569228172302246\n",
      "Epoch 3008, Loss: 2.5025191009044647, Final Batch Loss: 0.5411251783370972\n",
      "Epoch 3009, Loss: 2.500467747449875, Final Batch Loss: 0.4965659976005554\n",
      "Epoch 3010, Loss: 2.2152123749256134, Final Batch Loss: 0.4036833643913269\n",
      "Epoch 3011, Loss: 2.3608760237693787, Final Batch Loss: 0.4740777909755707\n",
      "Epoch 3012, Loss: 2.3914296627044678, Final Batch Loss: 0.4499153792858124\n",
      "Epoch 3013, Loss: 2.272484987974167, Final Batch Loss: 0.4375333786010742\n",
      "Epoch 3014, Loss: 2.5086957216262817, Final Batch Loss: 0.5416073799133301\n",
      "Epoch 3015, Loss: 2.4571692049503326, Final Batch Loss: 0.5063648223876953\n",
      "Epoch 3016, Loss: 2.4464752972126007, Final Batch Loss: 0.49011629819869995\n",
      "Epoch 3017, Loss: 2.379782259464264, Final Batch Loss: 0.5743087530136108\n",
      "Epoch 3018, Loss: 2.3525084257125854, Final Batch Loss: 0.47925588488578796\n",
      "Epoch 3019, Loss: 2.445624500513077, Final Batch Loss: 0.5603112578392029\n",
      "Epoch 3020, Loss: 2.430100917816162, Final Batch Loss: 0.5304756164550781\n",
      "Epoch 3021, Loss: 2.333263546228409, Final Batch Loss: 0.5547701120376587\n",
      "Epoch 3022, Loss: 2.6683928966522217, Final Batch Loss: 0.5364608764648438\n",
      "Epoch 3023, Loss: 2.5382259786129, Final Batch Loss: 0.5079107880592346\n",
      "Epoch 3024, Loss: 2.32513764500618, Final Batch Loss: 0.48335301876068115\n",
      "Epoch 3025, Loss: 2.344460129737854, Final Batch Loss: 0.5627343058586121\n",
      "Epoch 3026, Loss: 2.304690897464752, Final Batch Loss: 0.4976568818092346\n",
      "Epoch 3027, Loss: 2.3848160803318024, Final Batch Loss: 0.47282588481903076\n",
      "Epoch 3028, Loss: 2.5028509199619293, Final Batch Loss: 0.5860897898674011\n",
      "Epoch 3029, Loss: 2.5492289066314697, Final Batch Loss: 0.6306055188179016\n",
      "Epoch 3030, Loss: 2.2970822751522064, Final Batch Loss: 0.40041878819465637\n",
      "Epoch 3031, Loss: 2.385404795408249, Final Batch Loss: 0.49687910079956055\n",
      "Epoch 3032, Loss: 2.3242680728435516, Final Batch Loss: 0.38905927538871765\n",
      "Epoch 3033, Loss: 2.2305255830287933, Final Batch Loss: 0.4558604657649994\n",
      "Epoch 3034, Loss: 2.461210697889328, Final Batch Loss: 0.5254405736923218\n",
      "Epoch 3035, Loss: 2.446249336004257, Final Batch Loss: 0.6066612601280212\n",
      "Epoch 3036, Loss: 2.411023586988449, Final Batch Loss: 0.44941622018814087\n",
      "Epoch 3037, Loss: 2.338340312242508, Final Batch Loss: 0.49170830845832825\n",
      "Epoch 3038, Loss: 2.5912800431251526, Final Batch Loss: 0.5942077040672302\n",
      "Epoch 3039, Loss: 2.2899661362171173, Final Batch Loss: 0.4333718419075012\n",
      "Epoch 3040, Loss: 2.4810642302036285, Final Batch Loss: 0.44762861728668213\n",
      "Epoch 3041, Loss: 2.474778890609741, Final Batch Loss: 0.6178925037384033\n",
      "Epoch 3042, Loss: 2.529679447412491, Final Batch Loss: 0.6516456007957458\n",
      "Epoch 3043, Loss: 2.446010559797287, Final Batch Loss: 0.4149891436100006\n",
      "Epoch 3044, Loss: 2.324659615755081, Final Batch Loss: 0.4250374436378479\n",
      "Epoch 3045, Loss: 2.466292828321457, Final Batch Loss: 0.48678621649742126\n",
      "Epoch 3046, Loss: 2.4504071176052094, Final Batch Loss: 0.4540763199329376\n",
      "Epoch 3047, Loss: 2.609730839729309, Final Batch Loss: 0.3917439877986908\n",
      "Epoch 3048, Loss: 2.4098416566848755, Final Batch Loss: 0.4175467789173126\n",
      "Epoch 3049, Loss: 2.3040145933628082, Final Batch Loss: 0.44025787711143494\n",
      "Epoch 3050, Loss: 2.434800773859024, Final Batch Loss: 0.5804095268249512\n",
      "Epoch 3051, Loss: 2.231879383325577, Final Batch Loss: 0.413101464509964\n",
      "Epoch 3052, Loss: 2.423043519258499, Final Batch Loss: 0.5023030042648315\n",
      "Epoch 3053, Loss: 2.242133229970932, Final Batch Loss: 0.4465894103050232\n",
      "Epoch 3054, Loss: 2.547022819519043, Final Batch Loss: 0.6107646226882935\n",
      "Epoch 3055, Loss: 2.2987640500068665, Final Batch Loss: 0.5118473172187805\n",
      "Epoch 3056, Loss: 2.6935688257217407, Final Batch Loss: 0.900967538356781\n",
      "Epoch 3057, Loss: 2.346886068582535, Final Batch Loss: 0.5118702054023743\n",
      "Epoch 3058, Loss: 2.3497227132320404, Final Batch Loss: 0.4048301875591278\n",
      "Epoch 3059, Loss: 2.532728761434555, Final Batch Loss: 0.5287122130393982\n",
      "Epoch 3060, Loss: 2.265264630317688, Final Batch Loss: 0.45841091871261597\n",
      "Epoch 3061, Loss: 2.2218350768089294, Final Batch Loss: 0.4528447687625885\n",
      "Epoch 3062, Loss: 2.492329776287079, Final Batch Loss: 0.4851686656475067\n",
      "Epoch 3063, Loss: 2.398608773946762, Final Batch Loss: 0.5979901552200317\n",
      "Epoch 3064, Loss: 2.397697776556015, Final Batch Loss: 0.4742373824119568\n",
      "Epoch 3065, Loss: 2.541510045528412, Final Batch Loss: 0.5027454495429993\n",
      "Epoch 3066, Loss: 2.4313544034957886, Final Batch Loss: 0.5446735620498657\n",
      "Epoch 3067, Loss: 2.3888095915317535, Final Batch Loss: 0.42007315158843994\n",
      "Epoch 3068, Loss: 2.683906465768814, Final Batch Loss: 0.5892186760902405\n",
      "Epoch 3069, Loss: 2.5065743923187256, Final Batch Loss: 0.49220600724220276\n",
      "Epoch 3070, Loss: 2.346024602651596, Final Batch Loss: 0.3702058792114258\n",
      "Epoch 3071, Loss: 2.4009661078453064, Final Batch Loss: 0.46996670961380005\n",
      "Epoch 3072, Loss: 2.478986978530884, Final Batch Loss: 0.7004151940345764\n",
      "Epoch 3073, Loss: 2.3082869350910187, Final Batch Loss: 0.40832462906837463\n",
      "Epoch 3074, Loss: 2.4366665184497833, Final Batch Loss: 0.4449741244316101\n",
      "Epoch 3075, Loss: 2.3613652288913727, Final Batch Loss: 0.3687710464000702\n",
      "Epoch 3076, Loss: 2.439931809902191, Final Batch Loss: 0.41198256611824036\n",
      "Epoch 3077, Loss: 2.59147310256958, Final Batch Loss: 0.5752628445625305\n",
      "Epoch 3078, Loss: 2.4501280784606934, Final Batch Loss: 0.5643453001976013\n",
      "Epoch 3079, Loss: 2.4141350090503693, Final Batch Loss: 0.4820580780506134\n",
      "Epoch 3080, Loss: 2.328159838914871, Final Batch Loss: 0.4416455030441284\n",
      "Epoch 3081, Loss: 2.488668590784073, Final Batch Loss: 0.5360111594200134\n",
      "Epoch 3082, Loss: 2.395587205886841, Final Batch Loss: 0.5016137957572937\n",
      "Epoch 3083, Loss: 2.2619902789592743, Final Batch Loss: 0.43736541271209717\n",
      "Epoch 3084, Loss: 2.424395442008972, Final Batch Loss: 0.4601311981678009\n",
      "Epoch 3085, Loss: 2.4859395623207092, Final Batch Loss: 0.510140061378479\n",
      "Epoch 3086, Loss: 2.2330756783485413, Final Batch Loss: 0.42856842279434204\n",
      "Epoch 3087, Loss: 2.440702646970749, Final Batch Loss: 0.47297435998916626\n",
      "Epoch 3088, Loss: 2.323199063539505, Final Batch Loss: 0.5608713030815125\n",
      "Epoch 3089, Loss: 2.327418625354767, Final Batch Loss: 0.42990660667419434\n",
      "Epoch 3090, Loss: 2.2524588108062744, Final Batch Loss: 0.41719183325767517\n",
      "Epoch 3091, Loss: 2.405281811952591, Final Batch Loss: 0.5341870784759521\n",
      "Epoch 3092, Loss: 2.4670917987823486, Final Batch Loss: 0.5183627605438232\n",
      "Epoch 3093, Loss: 2.6104443669319153, Final Batch Loss: 0.4902704060077667\n",
      "Epoch 3094, Loss: 2.53156578540802, Final Batch Loss: 0.5721957087516785\n",
      "Epoch 3095, Loss: 2.4874528646469116, Final Batch Loss: 0.5142248868942261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3096, Loss: 2.5556992292404175, Final Batch Loss: 0.4911997318267822\n",
      "Epoch 3097, Loss: 2.289599657058716, Final Batch Loss: 0.40676769614219666\n",
      "Epoch 3098, Loss: 2.475684881210327, Final Batch Loss: 0.45366084575653076\n",
      "Epoch 3099, Loss: 2.4254398941993713, Final Batch Loss: 0.45262598991394043\n",
      "Epoch 3100, Loss: 2.2321914434432983, Final Batch Loss: 0.42739206552505493\n",
      "Epoch 3101, Loss: 2.394418865442276, Final Batch Loss: 0.46776503324508667\n",
      "Epoch 3102, Loss: 2.2083801329135895, Final Batch Loss: 0.37806111574172974\n",
      "Epoch 3103, Loss: 2.461757391691208, Final Batch Loss: 0.5750635862350464\n",
      "Epoch 3104, Loss: 2.4090163707733154, Final Batch Loss: 0.3985787034034729\n",
      "Epoch 3105, Loss: 2.3929219245910645, Final Batch Loss: 0.4856083393096924\n",
      "Epoch 3106, Loss: 2.4268727898597717, Final Batch Loss: 0.4402451515197754\n",
      "Epoch 3107, Loss: 2.3873883485794067, Final Batch Loss: 0.5287736654281616\n",
      "Epoch 3108, Loss: 2.440283387899399, Final Batch Loss: 0.5609526634216309\n",
      "Epoch 3109, Loss: 2.253676325082779, Final Batch Loss: 0.4188377857208252\n",
      "Epoch 3110, Loss: 2.2031810879707336, Final Batch Loss: 0.5290053486824036\n",
      "Epoch 3111, Loss: 2.3939469754695892, Final Batch Loss: 0.530019223690033\n",
      "Epoch 3112, Loss: 2.440244108438492, Final Batch Loss: 0.45743802189826965\n",
      "Epoch 3113, Loss: 2.2976438105106354, Final Batch Loss: 0.4594762325286865\n",
      "Epoch 3114, Loss: 2.5128318667411804, Final Batch Loss: 0.5899900794029236\n",
      "Epoch 3115, Loss: 2.4304580986499786, Final Batch Loss: 0.6248664855957031\n",
      "Epoch 3116, Loss: 2.488037496805191, Final Batch Loss: 0.5456608533859253\n",
      "Epoch 3117, Loss: 2.3884741067886353, Final Batch Loss: 0.5081284642219543\n",
      "Epoch 3118, Loss: 2.4246731400489807, Final Batch Loss: 0.48585736751556396\n",
      "Epoch 3119, Loss: 2.502319425344467, Final Batch Loss: 0.6730861067771912\n",
      "Epoch 3120, Loss: 2.4903473258018494, Final Batch Loss: 0.4647488296031952\n",
      "Epoch 3121, Loss: 2.2212306559085846, Final Batch Loss: 0.35459572076797485\n",
      "Epoch 3122, Loss: 2.618745267391205, Final Batch Loss: 0.567871630191803\n",
      "Epoch 3123, Loss: 2.3570253252983093, Final Batch Loss: 0.46186718344688416\n",
      "Epoch 3124, Loss: 2.1596356630325317, Final Batch Loss: 0.29850369691848755\n",
      "Epoch 3125, Loss: 2.1911157965660095, Final Batch Loss: 0.3624986708164215\n",
      "Epoch 3126, Loss: 2.3874221444129944, Final Batch Loss: 0.3954356908798218\n",
      "Epoch 3127, Loss: 2.4367833137512207, Final Batch Loss: 0.6411139369010925\n",
      "Epoch 3128, Loss: 2.5044501423835754, Final Batch Loss: 0.6079006195068359\n",
      "Epoch 3129, Loss: 2.3498362600803375, Final Batch Loss: 0.6071344017982483\n",
      "Epoch 3130, Loss: 2.376125931739807, Final Batch Loss: 0.4935758411884308\n",
      "Epoch 3131, Loss: 2.2748817801475525, Final Batch Loss: 0.4352497458457947\n",
      "Epoch 3132, Loss: 2.3458482921123505, Final Batch Loss: 0.4054851531982422\n",
      "Epoch 3133, Loss: 2.3330344557762146, Final Batch Loss: 0.38452160358428955\n",
      "Epoch 3134, Loss: 2.484061598777771, Final Batch Loss: 0.456163227558136\n",
      "Epoch 3135, Loss: 2.4632903933525085, Final Batch Loss: 0.49705925583839417\n",
      "Epoch 3136, Loss: 2.3448462188243866, Final Batch Loss: 0.4377007782459259\n",
      "Epoch 3137, Loss: 2.3767819106578827, Final Batch Loss: 0.5418180823326111\n",
      "Epoch 3138, Loss: 2.350556403398514, Final Batch Loss: 0.5251808762550354\n",
      "Epoch 3139, Loss: 2.4772602021694183, Final Batch Loss: 0.5836358666419983\n",
      "Epoch 3140, Loss: 2.3806527853012085, Final Batch Loss: 0.3309416174888611\n",
      "Epoch 3141, Loss: 2.3537338972091675, Final Batch Loss: 0.4751201868057251\n",
      "Epoch 3142, Loss: 2.2901885509490967, Final Batch Loss: 0.44727352261543274\n",
      "Epoch 3143, Loss: 2.2945737838745117, Final Batch Loss: 0.4548223614692688\n",
      "Epoch 3144, Loss: 2.1763418316841125, Final Batch Loss: 0.40375906229019165\n",
      "Epoch 3145, Loss: 2.45340234041214, Final Batch Loss: 0.4517125189304352\n",
      "Epoch 3146, Loss: 2.3263809978961945, Final Batch Loss: 0.4406772255897522\n",
      "Epoch 3147, Loss: 2.4286384880542755, Final Batch Loss: 0.4859350919723511\n",
      "Epoch 3148, Loss: 2.301127254962921, Final Batch Loss: 0.3665490448474884\n",
      "Epoch 3149, Loss: 2.2373952865600586, Final Batch Loss: 0.4109867513179779\n",
      "Epoch 3150, Loss: 2.428536146879196, Final Batch Loss: 0.52532958984375\n",
      "Epoch 3151, Loss: 2.313461720943451, Final Batch Loss: 0.4685496389865875\n",
      "Epoch 3152, Loss: 2.320032387971878, Final Batch Loss: 0.39043551683425903\n",
      "Epoch 3153, Loss: 2.28651225566864, Final Batch Loss: 0.4931941032409668\n",
      "Epoch 3154, Loss: 2.2346587777137756, Final Batch Loss: 0.40241989493370056\n",
      "Epoch 3155, Loss: 2.274087756872177, Final Batch Loss: 0.41364529728889465\n",
      "Epoch 3156, Loss: 2.558843344449997, Final Batch Loss: 0.5190499424934387\n",
      "Epoch 3157, Loss: 2.242628425359726, Final Batch Loss: 0.4340430200099945\n",
      "Epoch 3158, Loss: 2.4585748314857483, Final Batch Loss: 0.5406363606452942\n",
      "Epoch 3159, Loss: 2.379915416240692, Final Batch Loss: 0.43406611680984497\n",
      "Epoch 3160, Loss: 2.3684317469596863, Final Batch Loss: 0.5724910497665405\n",
      "Epoch 3161, Loss: 2.3501286804676056, Final Batch Loss: 0.41717904806137085\n",
      "Epoch 3162, Loss: 2.361321449279785, Final Batch Loss: 0.4382241666316986\n",
      "Epoch 3163, Loss: 2.3214398324489594, Final Batch Loss: 0.48948994278907776\n",
      "Epoch 3164, Loss: 2.3344217240810394, Final Batch Loss: 0.38942110538482666\n",
      "Epoch 3165, Loss: 2.5434682369232178, Final Batch Loss: 0.6285395622253418\n",
      "Epoch 3166, Loss: 2.258124053478241, Final Batch Loss: 0.44789308309555054\n",
      "Epoch 3167, Loss: 2.401714861392975, Final Batch Loss: 0.5803129076957703\n",
      "Epoch 3168, Loss: 2.4433391988277435, Final Batch Loss: 0.5900189280509949\n",
      "Epoch 3169, Loss: 2.4392600655555725, Final Batch Loss: 0.5026750564575195\n",
      "Epoch 3170, Loss: 2.2738148868083954, Final Batch Loss: 0.3824586570262909\n",
      "Epoch 3171, Loss: 2.4090835452079773, Final Batch Loss: 0.43100953102111816\n",
      "Epoch 3172, Loss: 2.3090629875659943, Final Batch Loss: 0.5806876420974731\n",
      "Epoch 3173, Loss: 2.4265835285186768, Final Batch Loss: 0.6081423759460449\n",
      "Epoch 3174, Loss: 2.4765381515026093, Final Batch Loss: 0.4815417528152466\n",
      "Epoch 3175, Loss: 2.4910248517990112, Final Batch Loss: 0.4791923761367798\n",
      "Epoch 3176, Loss: 2.6142395436763763, Final Batch Loss: 0.5595973134040833\n",
      "Epoch 3177, Loss: 2.291222006082535, Final Batch Loss: 0.39442235231399536\n",
      "Epoch 3178, Loss: 2.410638689994812, Final Batch Loss: 0.5302064418792725\n",
      "Epoch 3179, Loss: 2.2015746235847473, Final Batch Loss: 0.31985655426979065\n",
      "Epoch 3180, Loss: 2.4305584728717804, Final Batch Loss: 0.5644213557243347\n",
      "Epoch 3181, Loss: 2.3220877051353455, Final Batch Loss: 0.43353912234306335\n",
      "Epoch 3182, Loss: 2.3530288338661194, Final Batch Loss: 0.519351065158844\n",
      "Epoch 3183, Loss: 2.37109112739563, Final Batch Loss: 0.5074219107627869\n",
      "Epoch 3184, Loss: 2.2489423155784607, Final Batch Loss: 0.4146406650543213\n",
      "Epoch 3185, Loss: 2.4678874611854553, Final Batch Loss: 0.49591317772865295\n",
      "Epoch 3186, Loss: 2.293377995491028, Final Batch Loss: 0.3739550709724426\n",
      "Epoch 3187, Loss: 2.36370387673378, Final Batch Loss: 0.42675843834877014\n",
      "Epoch 3188, Loss: 2.375247746706009, Final Batch Loss: 0.4436112940311432\n",
      "Epoch 3189, Loss: 2.6571798622608185, Final Batch Loss: 0.4764167070388794\n",
      "Epoch 3190, Loss: 2.4510403871536255, Final Batch Loss: 0.4260791838169098\n",
      "Epoch 3191, Loss: 2.4482617378234863, Final Batch Loss: 0.5275317430496216\n",
      "Epoch 3192, Loss: 2.3603114187717438, Final Batch Loss: 0.5275613069534302\n",
      "Epoch 3193, Loss: 2.084483504295349, Final Batch Loss: 0.3777769207954407\n",
      "Epoch 3194, Loss: 2.538734048604965, Final Batch Loss: 0.5590651035308838\n",
      "Epoch 3195, Loss: 2.423106551170349, Final Batch Loss: 0.5540400743484497\n",
      "Epoch 3196, Loss: 2.2546730041503906, Final Batch Loss: 0.4642883837223053\n",
      "Epoch 3197, Loss: 2.609922856092453, Final Batch Loss: 0.5915265083312988\n",
      "Epoch 3198, Loss: 2.2601927518844604, Final Batch Loss: 0.4235897958278656\n",
      "Epoch 3199, Loss: 2.168450653553009, Final Batch Loss: 0.4496457278728485\n",
      "Epoch 3200, Loss: 2.243648052215576, Final Batch Loss: 0.39262908697128296\n",
      "Epoch 3201, Loss: 2.235259532928467, Final Batch Loss: 0.5225219130516052\n",
      "Epoch 3202, Loss: 2.391384482383728, Final Batch Loss: 0.41507044434547424\n",
      "Epoch 3203, Loss: 2.400663822889328, Final Batch Loss: 0.4828058183193207\n",
      "Epoch 3204, Loss: 2.2823862731456757, Final Batch Loss: 0.5459755063056946\n",
      "Epoch 3205, Loss: 2.443394899368286, Final Batch Loss: 0.6012558341026306\n",
      "Epoch 3206, Loss: 2.171049952507019, Final Batch Loss: 0.442218542098999\n",
      "Epoch 3207, Loss: 2.2690081000328064, Final Batch Loss: 0.426573783159256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3208, Loss: 2.3509571850299835, Final Batch Loss: 0.40591931343078613\n",
      "Epoch 3209, Loss: 2.350168377161026, Final Batch Loss: 0.49353405833244324\n",
      "Epoch 3210, Loss: 2.5348199605941772, Final Batch Loss: 0.459477037191391\n",
      "Epoch 3211, Loss: 2.5734925866127014, Final Batch Loss: 0.542943000793457\n",
      "Epoch 3212, Loss: 2.5054145753383636, Final Batch Loss: 0.537475049495697\n",
      "Epoch 3213, Loss: 2.3090390861034393, Final Batch Loss: 0.4989663064479828\n",
      "Epoch 3214, Loss: 2.474030911922455, Final Batch Loss: 0.43680453300476074\n",
      "Epoch 3215, Loss: 2.2744010984897614, Final Batch Loss: 0.5031706094741821\n",
      "Epoch 3216, Loss: 2.446331709623337, Final Batch Loss: 0.4814906716346741\n",
      "Epoch 3217, Loss: 2.2779871225357056, Final Batch Loss: 0.34819158911705017\n",
      "Epoch 3218, Loss: 2.4602997601032257, Final Batch Loss: 0.49150991439819336\n",
      "Epoch 3219, Loss: 2.273765951395035, Final Batch Loss: 0.4172821640968323\n",
      "Epoch 3220, Loss: 2.3914838433265686, Final Batch Loss: 0.47451499104499817\n",
      "Epoch 3221, Loss: 2.3140279352664948, Final Batch Loss: 0.3947030007839203\n",
      "Epoch 3222, Loss: 2.5052312910556793, Final Batch Loss: 0.5306735038757324\n",
      "Epoch 3223, Loss: 2.394857943058014, Final Batch Loss: 0.4727952182292938\n",
      "Epoch 3224, Loss: 2.353813648223877, Final Batch Loss: 0.46335917711257935\n",
      "Epoch 3225, Loss: 2.4990638196468353, Final Batch Loss: 0.42903774976730347\n",
      "Epoch 3226, Loss: 2.4107093513011932, Final Batch Loss: 0.5844777822494507\n",
      "Epoch 3227, Loss: 2.3175325989723206, Final Batch Loss: 0.4282993972301483\n",
      "Epoch 3228, Loss: 2.170129656791687, Final Batch Loss: 0.431169718503952\n",
      "Epoch 3229, Loss: 2.463008552789688, Final Batch Loss: 0.5228222012519836\n",
      "Epoch 3230, Loss: 2.3375505208969116, Final Batch Loss: 0.5147866010665894\n",
      "Epoch 3231, Loss: 2.3787922859191895, Final Batch Loss: 0.48956647515296936\n",
      "Epoch 3232, Loss: 2.266505777835846, Final Batch Loss: 0.4189284145832062\n",
      "Epoch 3233, Loss: 2.2181423604488373, Final Batch Loss: 0.39810508489608765\n",
      "Epoch 3234, Loss: 2.3648733496665955, Final Batch Loss: 0.41495487093925476\n",
      "Epoch 3235, Loss: 2.4428835809230804, Final Batch Loss: 0.6274352073669434\n",
      "Epoch 3236, Loss: 2.193292498588562, Final Batch Loss: 0.35318824648857117\n",
      "Epoch 3237, Loss: 2.1841740012168884, Final Batch Loss: 0.380881130695343\n",
      "Epoch 3238, Loss: 2.330090820789337, Final Batch Loss: 0.5092674493789673\n",
      "Epoch 3239, Loss: 2.3035085201263428, Final Batch Loss: 0.43659380078315735\n",
      "Epoch 3240, Loss: 2.3707078099250793, Final Batch Loss: 0.6122819185256958\n",
      "Epoch 3241, Loss: 2.2959327697753906, Final Batch Loss: 0.5740681290626526\n",
      "Epoch 3242, Loss: 2.3987886905670166, Final Batch Loss: 0.5691529512405396\n",
      "Epoch 3243, Loss: 2.3919043242931366, Final Batch Loss: 0.5467835068702698\n",
      "Epoch 3244, Loss: 2.505332499742508, Final Batch Loss: 0.4802369177341461\n",
      "Epoch 3245, Loss: 2.4196378588676453, Final Batch Loss: 0.6107708811759949\n",
      "Epoch 3246, Loss: 2.58851221203804, Final Batch Loss: 0.6315908432006836\n",
      "Epoch 3247, Loss: 2.2829295694828033, Final Batch Loss: 0.3782151937484741\n",
      "Epoch 3248, Loss: 2.380149394273758, Final Batch Loss: 0.47240757942199707\n",
      "Epoch 3249, Loss: 2.3775313794612885, Final Batch Loss: 0.4479343295097351\n",
      "Epoch 3250, Loss: 2.2797171771526337, Final Batch Loss: 0.45179131627082825\n",
      "Epoch 3251, Loss: 2.5009052455425262, Final Batch Loss: 0.5136075019836426\n",
      "Epoch 3252, Loss: 2.402193605899811, Final Batch Loss: 0.40320253372192383\n",
      "Epoch 3253, Loss: 2.1902493834495544, Final Batch Loss: 0.4382423758506775\n",
      "Epoch 3254, Loss: 2.2531432509422302, Final Batch Loss: 0.4554154574871063\n",
      "Epoch 3255, Loss: 2.340291291475296, Final Batch Loss: 0.5117079615592957\n",
      "Epoch 3256, Loss: 2.2182404696941376, Final Batch Loss: 0.4112274944782257\n",
      "Epoch 3257, Loss: 2.7156653106212616, Final Batch Loss: 0.5616763234138489\n",
      "Epoch 3258, Loss: 2.2321036756038666, Final Batch Loss: 0.5131553411483765\n",
      "Epoch 3259, Loss: 2.2054916620254517, Final Batch Loss: 0.4409731328487396\n",
      "Epoch 3260, Loss: 2.382170021533966, Final Batch Loss: 0.4780300259590149\n",
      "Epoch 3261, Loss: 2.221565395593643, Final Batch Loss: 0.3759317100048065\n",
      "Epoch 3262, Loss: 2.456508845090866, Final Batch Loss: 0.5382134318351746\n",
      "Epoch 3263, Loss: 2.4226841032505035, Final Batch Loss: 0.6538887023925781\n",
      "Epoch 3264, Loss: 2.559068024158478, Final Batch Loss: 0.5090274214744568\n",
      "Epoch 3265, Loss: 2.354255646467209, Final Batch Loss: 0.5008801817893982\n",
      "Epoch 3266, Loss: 2.5257672667503357, Final Batch Loss: 0.4796014130115509\n",
      "Epoch 3267, Loss: 2.3681746125221252, Final Batch Loss: 0.5653405785560608\n",
      "Epoch 3268, Loss: 2.249261647462845, Final Batch Loss: 0.45090359449386597\n",
      "Epoch 3269, Loss: 2.510990411043167, Final Batch Loss: 0.48767220973968506\n",
      "Epoch 3270, Loss: 2.3096554577350616, Final Batch Loss: 0.5193851590156555\n",
      "Epoch 3271, Loss: 2.4739428758621216, Final Batch Loss: 0.4953320622444153\n",
      "Epoch 3272, Loss: 2.285389870405197, Final Batch Loss: 0.4400006830692291\n",
      "Epoch 3273, Loss: 2.4456037282943726, Final Batch Loss: 0.5033577680587769\n",
      "Epoch 3274, Loss: 2.348554253578186, Final Batch Loss: 0.46893006563186646\n",
      "Epoch 3275, Loss: 2.533509373664856, Final Batch Loss: 0.5574604868888855\n",
      "Epoch 3276, Loss: 2.2297483086586, Final Batch Loss: 0.3944315016269684\n",
      "Epoch 3277, Loss: 2.2367601096630096, Final Batch Loss: 0.4060303270816803\n",
      "Epoch 3278, Loss: 2.4260457158088684, Final Batch Loss: 0.3780681788921356\n",
      "Epoch 3279, Loss: 2.3149978518486023, Final Batch Loss: 0.5276228189468384\n",
      "Epoch 3280, Loss: 2.3174678087234497, Final Batch Loss: 0.4440170228481293\n",
      "Epoch 3281, Loss: 2.3998700380325317, Final Batch Loss: 0.5263554453849792\n",
      "Epoch 3282, Loss: 2.430727630853653, Final Batch Loss: 0.44470474123954773\n",
      "Epoch 3283, Loss: 2.3912298679351807, Final Batch Loss: 0.5185153484344482\n",
      "Epoch 3284, Loss: 2.3493845462799072, Final Batch Loss: 0.44175955653190613\n",
      "Epoch 3285, Loss: 2.337000846862793, Final Batch Loss: 0.4658477306365967\n",
      "Epoch 3286, Loss: 2.473235309123993, Final Batch Loss: 0.45544618368148804\n",
      "Epoch 3287, Loss: 2.5561437606811523, Final Batch Loss: 0.6311271786689758\n",
      "Epoch 3288, Loss: 2.4293768107891083, Final Batch Loss: 0.39862215518951416\n",
      "Epoch 3289, Loss: 2.5137237310409546, Final Batch Loss: 0.6030887365341187\n",
      "Epoch 3290, Loss: 2.377000093460083, Final Batch Loss: 0.5185232758522034\n",
      "Epoch 3291, Loss: 2.4467020630836487, Final Batch Loss: 0.466990202665329\n",
      "Epoch 3292, Loss: 2.3725173473358154, Final Batch Loss: 0.4053347408771515\n",
      "Epoch 3293, Loss: 2.2298002541065216, Final Batch Loss: 0.3732694387435913\n",
      "Epoch 3294, Loss: 2.345516473054886, Final Batch Loss: 0.47524014115333557\n",
      "Epoch 3295, Loss: 2.5362108647823334, Final Batch Loss: 0.546432614326477\n",
      "Epoch 3296, Loss: 2.250112771987915, Final Batch Loss: 0.4381923973560333\n",
      "Epoch 3297, Loss: 2.4068367183208466, Final Batch Loss: 0.4951529800891876\n",
      "Epoch 3298, Loss: 2.3110960125923157, Final Batch Loss: 0.48889392614364624\n",
      "Epoch 3299, Loss: 2.302092283964157, Final Batch Loss: 0.33706173300743103\n",
      "Epoch 3300, Loss: 2.3258113265037537, Final Batch Loss: 0.4938417077064514\n",
      "Epoch 3301, Loss: 2.3191803991794586, Final Batch Loss: 0.5582715272903442\n",
      "Epoch 3302, Loss: 2.314630687236786, Final Batch Loss: 0.4897347688674927\n",
      "Epoch 3303, Loss: 2.5097218453884125, Final Batch Loss: 0.5606796145439148\n",
      "Epoch 3304, Loss: 2.191458076238632, Final Batch Loss: 0.5135369896888733\n",
      "Epoch 3305, Loss: 2.5913049578666687, Final Batch Loss: 0.522416889667511\n",
      "Epoch 3306, Loss: 2.641322433948517, Final Batch Loss: 0.524810791015625\n",
      "Epoch 3307, Loss: 2.3078943490982056, Final Batch Loss: 0.4304860830307007\n",
      "Epoch 3308, Loss: 2.5448146760463715, Final Batch Loss: 0.5813817977905273\n",
      "Epoch 3309, Loss: 2.397440195083618, Final Batch Loss: 0.467283695936203\n",
      "Epoch 3310, Loss: 2.578458935022354, Final Batch Loss: 0.6571957468986511\n",
      "Epoch 3311, Loss: 2.38369157910347, Final Batch Loss: 0.4899733364582062\n",
      "Epoch 3312, Loss: 2.2377550899982452, Final Batch Loss: 0.4753778874874115\n",
      "Epoch 3313, Loss: 2.426293671131134, Final Batch Loss: 0.5549511909484863\n",
      "Epoch 3314, Loss: 2.2160623371601105, Final Batch Loss: 0.4741383492946625\n",
      "Epoch 3315, Loss: 2.315215528011322, Final Batch Loss: 0.4005305767059326\n",
      "Epoch 3316, Loss: 2.350851446390152, Final Batch Loss: 0.4848843812942505\n",
      "Epoch 3317, Loss: 2.391060769557953, Final Batch Loss: 0.474538654088974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3318, Loss: 2.3007545173168182, Final Batch Loss: 0.4669603109359741\n",
      "Epoch 3319, Loss: 2.3873985707759857, Final Batch Loss: 0.5670254230499268\n",
      "Epoch 3320, Loss: 2.2176462411880493, Final Batch Loss: 0.4694904685020447\n",
      "Epoch 3321, Loss: 2.347075402736664, Final Batch Loss: 0.4822375178337097\n",
      "Epoch 3322, Loss: 2.3618569672107697, Final Batch Loss: 0.4634372591972351\n",
      "Epoch 3323, Loss: 2.4835455119609833, Final Batch Loss: 0.5529573559761047\n",
      "Epoch 3324, Loss: 2.5080671310424805, Final Batch Loss: 0.5883291363716125\n",
      "Epoch 3325, Loss: 2.0964801907539368, Final Batch Loss: 0.4284060299396515\n",
      "Epoch 3326, Loss: 2.2318930327892303, Final Batch Loss: 0.35149919986724854\n",
      "Epoch 3327, Loss: 2.288482218980789, Final Batch Loss: 0.51902836561203\n",
      "Epoch 3328, Loss: 2.140886902809143, Final Batch Loss: 0.303467333316803\n",
      "Epoch 3329, Loss: 2.215988963842392, Final Batch Loss: 0.43702933192253113\n",
      "Epoch 3330, Loss: 2.252928286790848, Final Batch Loss: 0.3909895122051239\n",
      "Epoch 3331, Loss: 2.2300924360752106, Final Batch Loss: 0.4259940981864929\n",
      "Epoch 3332, Loss: 2.5043638944625854, Final Batch Loss: 0.45192593336105347\n",
      "Epoch 3333, Loss: 2.4526932537555695, Final Batch Loss: 0.5190701484680176\n",
      "Epoch 3334, Loss: 2.50206795334816, Final Batch Loss: 0.4583735466003418\n",
      "Epoch 3335, Loss: 2.3426241278648376, Final Batch Loss: 0.470270574092865\n",
      "Epoch 3336, Loss: 2.3073270320892334, Final Batch Loss: 0.5091684460639954\n",
      "Epoch 3337, Loss: 2.303125113248825, Final Batch Loss: 0.41872304677963257\n",
      "Epoch 3338, Loss: 2.3258018493652344, Final Batch Loss: 0.44127416610717773\n",
      "Epoch 3339, Loss: 2.4374155700206757, Final Batch Loss: 0.5037489533424377\n",
      "Epoch 3340, Loss: 2.333394944667816, Final Batch Loss: 0.4960118234157562\n",
      "Epoch 3341, Loss: 2.523099422454834, Final Batch Loss: 0.617181122303009\n",
      "Epoch 3342, Loss: 2.2224996089935303, Final Batch Loss: 0.4539610743522644\n",
      "Epoch 3343, Loss: 2.147490620613098, Final Batch Loss: 0.36397168040275574\n",
      "Epoch 3344, Loss: 2.1788523197174072, Final Batch Loss: 0.3324832320213318\n",
      "Epoch 3345, Loss: 2.3589502573013306, Final Batch Loss: 0.5048136711120605\n",
      "Epoch 3346, Loss: 2.241225153207779, Final Batch Loss: 0.4139464795589447\n",
      "Epoch 3347, Loss: 2.317189574241638, Final Batch Loss: 0.48109209537506104\n",
      "Epoch 3348, Loss: 2.3995618522167206, Final Batch Loss: 0.42606836557388306\n",
      "Epoch 3349, Loss: 2.2625704407691956, Final Batch Loss: 0.39030006527900696\n",
      "Epoch 3350, Loss: 2.378052771091461, Final Batch Loss: 0.5474658012390137\n",
      "Epoch 3351, Loss: 2.323391407728195, Final Batch Loss: 0.48278146982192993\n",
      "Epoch 3352, Loss: 2.520135313272476, Final Batch Loss: 0.47708749771118164\n",
      "Epoch 3353, Loss: 2.3770101070404053, Final Batch Loss: 0.3618299067020416\n",
      "Epoch 3354, Loss: 2.3554451763629913, Final Batch Loss: 0.49381178617477417\n",
      "Epoch 3355, Loss: 2.437439799308777, Final Batch Loss: 0.4461152255535126\n",
      "Epoch 3356, Loss: 2.584349900484085, Final Batch Loss: 0.6619547605514526\n",
      "Epoch 3357, Loss: 2.1289163529872894, Final Batch Loss: 0.2592785954475403\n",
      "Epoch 3358, Loss: 2.5331716537475586, Final Batch Loss: 0.5360583662986755\n",
      "Epoch 3359, Loss: 2.2793847620487213, Final Batch Loss: 0.4741392433643341\n",
      "Epoch 3360, Loss: 2.2513101994991302, Final Batch Loss: 0.4815591871738434\n",
      "Epoch 3361, Loss: 2.2531071603298187, Final Batch Loss: 0.3977090120315552\n",
      "Epoch 3362, Loss: 2.28435942530632, Final Batch Loss: 0.5213522911071777\n",
      "Epoch 3363, Loss: 2.2714957296848297, Final Batch Loss: 0.3305070400238037\n",
      "Epoch 3364, Loss: 2.345165252685547, Final Batch Loss: 0.48336929082870483\n",
      "Epoch 3365, Loss: 2.499474823474884, Final Batch Loss: 0.6014471650123596\n",
      "Epoch 3366, Loss: 2.3262764513492584, Final Batch Loss: 0.537930428981781\n",
      "Epoch 3367, Loss: 2.2420458793640137, Final Batch Loss: 0.3851779103279114\n",
      "Epoch 3368, Loss: 2.293869733810425, Final Batch Loss: 0.346873939037323\n",
      "Epoch 3369, Loss: 2.302612394094467, Final Batch Loss: 0.3908922076225281\n",
      "Epoch 3370, Loss: 2.1555184423923492, Final Batch Loss: 0.4278818666934967\n",
      "Epoch 3371, Loss: 2.1878092885017395, Final Batch Loss: 0.47618934512138367\n",
      "Epoch 3372, Loss: 2.4933851063251495, Final Batch Loss: 0.7599525451660156\n",
      "Epoch 3373, Loss: 2.2826929092407227, Final Batch Loss: 0.4240095913410187\n",
      "Epoch 3374, Loss: 2.2899408638477325, Final Batch Loss: 0.46407216787338257\n",
      "Epoch 3375, Loss: 2.3201212882995605, Final Batch Loss: 0.4836779236793518\n",
      "Epoch 3376, Loss: 2.1347465813159943, Final Batch Loss: 0.3752487301826477\n",
      "Epoch 3377, Loss: 2.121416300535202, Final Batch Loss: 0.3643208146095276\n",
      "Epoch 3378, Loss: 2.2576282918453217, Final Batch Loss: 0.4251824617385864\n",
      "Epoch 3379, Loss: 2.326337516307831, Final Batch Loss: 0.4193112254142761\n",
      "Epoch 3380, Loss: 2.257610112428665, Final Batch Loss: 0.4502291977405548\n",
      "Epoch 3381, Loss: 2.414791464805603, Final Batch Loss: 0.3927425146102905\n",
      "Epoch 3382, Loss: 2.5317373275756836, Final Batch Loss: 0.5462753176689148\n",
      "Epoch 3383, Loss: 2.481757938861847, Final Batch Loss: 0.5415524840354919\n",
      "Epoch 3384, Loss: 2.3766293823719025, Final Batch Loss: 0.45288369059562683\n",
      "Epoch 3385, Loss: 2.366687834262848, Final Batch Loss: 0.4545213282108307\n",
      "Epoch 3386, Loss: 2.48223739862442, Final Batch Loss: 0.5464509129524231\n",
      "Epoch 3387, Loss: 2.3849646747112274, Final Batch Loss: 0.3958953022956848\n",
      "Epoch 3388, Loss: 2.632403075695038, Final Batch Loss: 0.6388677954673767\n",
      "Epoch 3389, Loss: 2.3357664346694946, Final Batch Loss: 0.4600585997104645\n",
      "Epoch 3390, Loss: 2.4713889956474304, Final Batch Loss: 0.35823115706443787\n",
      "Epoch 3391, Loss: 2.436286151409149, Final Batch Loss: 0.4439583122730255\n",
      "Epoch 3392, Loss: 2.3073451817035675, Final Batch Loss: 0.49974432587623596\n",
      "Epoch 3393, Loss: 2.37931090593338, Final Batch Loss: 0.448053240776062\n",
      "Epoch 3394, Loss: 2.314379781484604, Final Batch Loss: 0.4451751112937927\n",
      "Epoch 3395, Loss: 2.4455560445785522, Final Batch Loss: 0.45750895142555237\n",
      "Epoch 3396, Loss: 2.227820247411728, Final Batch Loss: 0.48071742057800293\n",
      "Epoch 3397, Loss: 2.187327116727829, Final Batch Loss: 0.3562171757221222\n",
      "Epoch 3398, Loss: 2.2982261776924133, Final Batch Loss: 0.5400689244270325\n",
      "Epoch 3399, Loss: 2.3420750498771667, Final Batch Loss: 0.44681796431541443\n",
      "Epoch 3400, Loss: 2.515418589115143, Final Batch Loss: 0.5019764304161072\n",
      "Epoch 3401, Loss: 2.2540451288223267, Final Batch Loss: 0.42757299542427063\n",
      "Epoch 3402, Loss: 2.4950285255908966, Final Batch Loss: 0.619168221950531\n",
      "Epoch 3403, Loss: 2.1512721180915833, Final Batch Loss: 0.45524391531944275\n",
      "Epoch 3404, Loss: 2.172580063343048, Final Batch Loss: 0.35466110706329346\n",
      "Epoch 3405, Loss: 2.2965251207351685, Final Batch Loss: 0.47646936774253845\n",
      "Epoch 3406, Loss: 2.270966798067093, Final Batch Loss: 0.4275423288345337\n",
      "Epoch 3407, Loss: 2.272367149591446, Final Batch Loss: 0.35182806849479675\n",
      "Epoch 3408, Loss: 2.1332502365112305, Final Batch Loss: 0.36643746495246887\n",
      "Epoch 3409, Loss: 2.20931214094162, Final Batch Loss: 0.41090255975723267\n",
      "Epoch 3410, Loss: 2.2043595612049103, Final Batch Loss: 0.3811272084712982\n",
      "Epoch 3411, Loss: 2.2815964818000793, Final Batch Loss: 0.46683749556541443\n",
      "Epoch 3412, Loss: 2.204342544078827, Final Batch Loss: 0.3849532902240753\n",
      "Epoch 3413, Loss: 2.2096636593341827, Final Batch Loss: 0.4124334454536438\n",
      "Epoch 3414, Loss: 2.199437737464905, Final Batch Loss: 0.43057218194007874\n",
      "Epoch 3415, Loss: 2.2444792687892914, Final Batch Loss: 0.3824302554130554\n",
      "Epoch 3416, Loss: 2.570984899997711, Final Batch Loss: 0.8054405450820923\n",
      "Epoch 3417, Loss: 2.5968821346759796, Final Batch Loss: 0.5624114274978638\n",
      "Epoch 3418, Loss: 2.3162779808044434, Final Batch Loss: 0.4592500925064087\n",
      "Epoch 3419, Loss: 2.2707794308662415, Final Batch Loss: 0.41498494148254395\n",
      "Epoch 3420, Loss: 2.404756635427475, Final Batch Loss: 0.44396623969078064\n",
      "Epoch 3421, Loss: 2.432825267314911, Final Batch Loss: 0.5638176202774048\n",
      "Epoch 3422, Loss: 2.4002970159053802, Final Batch Loss: 0.5087986588478088\n",
      "Epoch 3423, Loss: 2.2956745326519012, Final Batch Loss: 0.46615684032440186\n",
      "Epoch 3424, Loss: 2.1459790468215942, Final Batch Loss: 0.4325037896633148\n",
      "Epoch 3425, Loss: 2.251060426235199, Final Batch Loss: 0.4723992645740509\n",
      "Epoch 3426, Loss: 2.1730304062366486, Final Batch Loss: 0.3863167464733124\n",
      "Epoch 3427, Loss: 2.1941080689430237, Final Batch Loss: 0.43746688961982727\n",
      "Epoch 3428, Loss: 2.2432399690151215, Final Batch Loss: 0.46310651302337646\n",
      "Epoch 3429, Loss: 2.272836834192276, Final Batch Loss: 0.5234790444374084\n",
      "Epoch 3430, Loss: 2.2317416965961456, Final Batch Loss: 0.46540695428848267\n",
      "Epoch 3431, Loss: 2.303581088781357, Final Batch Loss: 0.5492759346961975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3432, Loss: 2.410440683364868, Final Batch Loss: 0.43294087052345276\n",
      "Epoch 3433, Loss: 2.6976676285266876, Final Batch Loss: 0.4776341915130615\n",
      "Epoch 3434, Loss: 2.2453608214855194, Final Batch Loss: 0.4122794270515442\n",
      "Epoch 3435, Loss: 2.476437419652939, Final Batch Loss: 0.3919925093650818\n",
      "Epoch 3436, Loss: 2.384331077337265, Final Batch Loss: 0.46015581488609314\n",
      "Epoch 3437, Loss: 2.5007455945014954, Final Batch Loss: 0.6342959403991699\n",
      "Epoch 3438, Loss: 2.3039194643497467, Final Batch Loss: 0.45080551505088806\n",
      "Epoch 3439, Loss: 2.3186240792274475, Final Batch Loss: 0.5186412930488586\n",
      "Epoch 3440, Loss: 2.2866216003894806, Final Batch Loss: 0.4380158483982086\n",
      "Epoch 3441, Loss: 2.188380002975464, Final Batch Loss: 0.45070648193359375\n",
      "Epoch 3442, Loss: 2.2199762165546417, Final Batch Loss: 0.3945859968662262\n",
      "Epoch 3443, Loss: 2.3032363951206207, Final Batch Loss: 0.48448365926742554\n",
      "Epoch 3444, Loss: 2.2994969189167023, Final Batch Loss: 0.471435010433197\n",
      "Epoch 3445, Loss: 2.2922931611537933, Final Batch Loss: 0.45050936937332153\n",
      "Epoch 3446, Loss: 2.2800077497959137, Final Batch Loss: 0.38450729846954346\n",
      "Epoch 3447, Loss: 2.5207794904708862, Final Batch Loss: 0.7334570288658142\n",
      "Epoch 3448, Loss: 2.145869195461273, Final Batch Loss: 0.3611261248588562\n",
      "Epoch 3449, Loss: 2.439843535423279, Final Batch Loss: 0.5251706838607788\n",
      "Epoch 3450, Loss: 2.360699713230133, Final Batch Loss: 0.4599073827266693\n",
      "Epoch 3451, Loss: 2.338167816400528, Final Batch Loss: 0.48404547572135925\n",
      "Epoch 3452, Loss: 2.5912278294563293, Final Batch Loss: 0.5498291254043579\n",
      "Epoch 3453, Loss: 2.4302369356155396, Final Batch Loss: 0.48410871624946594\n",
      "Epoch 3454, Loss: 2.3329524993896484, Final Batch Loss: 0.5045644640922546\n",
      "Epoch 3455, Loss: 2.300918608903885, Final Batch Loss: 0.438853919506073\n",
      "Epoch 3456, Loss: 2.2637812793254852, Final Batch Loss: 0.4510723650455475\n",
      "Epoch 3457, Loss: 2.123018443584442, Final Batch Loss: 0.39608922600746155\n",
      "Epoch 3458, Loss: 2.1667657494544983, Final Batch Loss: 0.4230060279369354\n",
      "Epoch 3459, Loss: 2.1591721773147583, Final Batch Loss: 0.4088106155395508\n",
      "Epoch 3460, Loss: 2.34660604596138, Final Batch Loss: 0.4067232310771942\n",
      "Epoch 3461, Loss: 2.2789407670497894, Final Batch Loss: 0.4015912413597107\n",
      "Epoch 3462, Loss: 2.4366480708122253, Final Batch Loss: 0.48354649543762207\n",
      "Epoch 3463, Loss: 2.238076627254486, Final Batch Loss: 0.5081959962844849\n",
      "Epoch 3464, Loss: 2.248230367898941, Final Batch Loss: 0.30912885069847107\n",
      "Epoch 3465, Loss: 2.331652045249939, Final Batch Loss: 0.4621247351169586\n",
      "Epoch 3466, Loss: 2.3452168405056, Final Batch Loss: 0.35770919919013977\n",
      "Epoch 3467, Loss: 2.265346199274063, Final Batch Loss: 0.4375624656677246\n",
      "Epoch 3468, Loss: 2.3792890906333923, Final Batch Loss: 0.43337973952293396\n",
      "Epoch 3469, Loss: 2.233853906393051, Final Batch Loss: 0.38003191351890564\n",
      "Epoch 3470, Loss: 2.2804683446884155, Final Batch Loss: 0.43999046087265015\n",
      "Epoch 3471, Loss: 2.2651716470718384, Final Batch Loss: 0.3633856177330017\n",
      "Epoch 3472, Loss: 2.235614061355591, Final Batch Loss: 0.536720871925354\n",
      "Epoch 3473, Loss: 2.2186380326747894, Final Batch Loss: 0.5050493478775024\n",
      "Epoch 3474, Loss: 2.235704779624939, Final Batch Loss: 0.4769870638847351\n",
      "Epoch 3475, Loss: 2.2728300392627716, Final Batch Loss: 0.3500102162361145\n",
      "Epoch 3476, Loss: 2.3603086471557617, Final Batch Loss: 0.34176549315452576\n",
      "Epoch 3477, Loss: 2.3054348528385162, Final Batch Loss: 0.45999717712402344\n",
      "Epoch 3478, Loss: 2.2680247128009796, Final Batch Loss: 0.4916987419128418\n",
      "Epoch 3479, Loss: 2.247305750846863, Final Batch Loss: 0.4837786555290222\n",
      "Epoch 3480, Loss: 2.234873980283737, Final Batch Loss: 0.39692723751068115\n",
      "Epoch 3481, Loss: 2.5720671713352203, Final Batch Loss: 0.4120633006095886\n",
      "Epoch 3482, Loss: 2.250745803117752, Final Batch Loss: 0.5082647800445557\n",
      "Epoch 3483, Loss: 2.305442452430725, Final Batch Loss: 0.455887109041214\n",
      "Epoch 3484, Loss: 2.268298625946045, Final Batch Loss: 0.4992924630641937\n",
      "Epoch 3485, Loss: 2.2822588086128235, Final Batch Loss: 0.47147247195243835\n",
      "Epoch 3486, Loss: 2.2632820308208466, Final Batch Loss: 0.3729369342327118\n",
      "Epoch 3487, Loss: 2.4983600974082947, Final Batch Loss: 0.4827854633331299\n",
      "Epoch 3488, Loss: 2.4010263979434967, Final Batch Loss: 0.6193462610244751\n",
      "Epoch 3489, Loss: 2.3319559693336487, Final Batch Loss: 0.5146359205245972\n",
      "Epoch 3490, Loss: 2.170568138360977, Final Batch Loss: 0.39707720279693604\n",
      "Epoch 3491, Loss: 2.407922148704529, Final Batch Loss: 0.38969674706459045\n",
      "Epoch 3492, Loss: 2.176288664340973, Final Batch Loss: 0.414882093667984\n",
      "Epoch 3493, Loss: 2.4612008333206177, Final Batch Loss: 0.4261474907398224\n",
      "Epoch 3494, Loss: 2.3822831213474274, Final Batch Loss: 0.5207086801528931\n",
      "Epoch 3495, Loss: 2.249130517244339, Final Batch Loss: 0.37898755073547363\n",
      "Epoch 3496, Loss: 2.125211477279663, Final Batch Loss: 0.43573692440986633\n",
      "Epoch 3497, Loss: 2.229349732398987, Final Batch Loss: 0.5645926594734192\n",
      "Epoch 3498, Loss: 2.358945995569229, Final Batch Loss: 0.4938722848892212\n",
      "Epoch 3499, Loss: 2.3288455307483673, Final Batch Loss: 0.5492752194404602\n",
      "Epoch 3500, Loss: 2.3251146972179413, Final Batch Loss: 0.505177915096283\n",
      "Epoch 3501, Loss: 2.6908336877822876, Final Batch Loss: 0.42299845814704895\n",
      "Epoch 3502, Loss: 2.2121943533420563, Final Batch Loss: 0.3269155025482178\n",
      "Epoch 3503, Loss: 2.1972270011901855, Final Batch Loss: 0.4723801612854004\n",
      "Epoch 3504, Loss: 2.3384033143520355, Final Batch Loss: 0.5135220289230347\n",
      "Epoch 3505, Loss: 2.4543647468090057, Final Batch Loss: 0.4915362000465393\n",
      "Epoch 3506, Loss: 2.2730028927326202, Final Batch Loss: 0.39916107058525085\n",
      "Epoch 3507, Loss: 2.4067022502422333, Final Batch Loss: 0.5472587943077087\n",
      "Epoch 3508, Loss: 2.4239507913589478, Final Batch Loss: 0.638476550579071\n",
      "Epoch 3509, Loss: 2.3824985921382904, Final Batch Loss: 0.5119010210037231\n",
      "Epoch 3510, Loss: 2.3118847608566284, Final Batch Loss: 0.3864114582538605\n",
      "Epoch 3511, Loss: 2.3685466945171356, Final Batch Loss: 0.4587302505970001\n",
      "Epoch 3512, Loss: 2.167736381292343, Final Batch Loss: 0.308182954788208\n",
      "Epoch 3513, Loss: 2.1683490574359894, Final Batch Loss: 0.41915321350097656\n",
      "Epoch 3514, Loss: 2.4096989035606384, Final Batch Loss: 0.549727737903595\n",
      "Epoch 3515, Loss: 2.092416375875473, Final Batch Loss: 0.445692241191864\n",
      "Epoch 3516, Loss: 2.368568867444992, Final Batch Loss: 0.5025309920310974\n",
      "Epoch 3517, Loss: 2.3290971219539642, Final Batch Loss: 0.5222071409225464\n",
      "Epoch 3518, Loss: 2.3109433948993683, Final Batch Loss: 0.47585082054138184\n",
      "Epoch 3519, Loss: 2.1894435584545135, Final Batch Loss: 0.36790499091148376\n",
      "Epoch 3520, Loss: 2.3709160685539246, Final Batch Loss: 0.5194606184959412\n",
      "Epoch 3521, Loss: 2.341969460248947, Final Batch Loss: 0.474195271730423\n",
      "Epoch 3522, Loss: 2.2348230183124542, Final Batch Loss: 0.4751821458339691\n",
      "Epoch 3523, Loss: 2.329550623893738, Final Batch Loss: 0.540683388710022\n",
      "Epoch 3524, Loss: 2.277845472097397, Final Batch Loss: 0.4389732778072357\n",
      "Epoch 3525, Loss: 2.207392632961273, Final Batch Loss: 0.45425793528556824\n",
      "Epoch 3526, Loss: 2.266782432794571, Final Batch Loss: 0.42602258920669556\n",
      "Epoch 3527, Loss: 2.289304167032242, Final Batch Loss: 0.4028278887271881\n",
      "Epoch 3528, Loss: 2.2353210151195526, Final Batch Loss: 0.41929012537002563\n",
      "Epoch 3529, Loss: 2.217353403568268, Final Batch Loss: 0.5615024566650391\n",
      "Epoch 3530, Loss: 2.266405075788498, Final Batch Loss: 0.3397383987903595\n",
      "Epoch 3531, Loss: 2.2360351383686066, Final Batch Loss: 0.3976781964302063\n",
      "Epoch 3532, Loss: 2.2651915550231934, Final Batch Loss: 0.40458378195762634\n",
      "Epoch 3533, Loss: 2.376677095890045, Final Batch Loss: 0.5557634830474854\n",
      "Epoch 3534, Loss: 2.449833393096924, Final Batch Loss: 0.5215758085250854\n",
      "Epoch 3535, Loss: 2.257489651441574, Final Batch Loss: 0.4652861952781677\n",
      "Epoch 3536, Loss: 2.2788259983062744, Final Batch Loss: 0.39277708530426025\n",
      "Epoch 3537, Loss: 2.0380666851997375, Final Batch Loss: 0.2846490144729614\n",
      "Epoch 3538, Loss: 2.0076289772987366, Final Batch Loss: 0.31795546412467957\n",
      "Epoch 3539, Loss: 2.3085328340530396, Final Batch Loss: 0.4377880096435547\n",
      "Epoch 3540, Loss: 2.3652507960796356, Final Batch Loss: 0.4819520115852356\n",
      "Epoch 3541, Loss: 2.3713683485984802, Final Batch Loss: 0.49457627534866333\n",
      "Epoch 3542, Loss: 2.317242592573166, Final Batch Loss: 0.38629651069641113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3543, Loss: 2.332371801137924, Final Batch Loss: 0.4813358783721924\n",
      "Epoch 3544, Loss: 2.337253898382187, Final Batch Loss: 0.4914107620716095\n",
      "Epoch 3545, Loss: 2.3219906985759735, Final Batch Loss: 0.6460811495780945\n",
      "Epoch 3546, Loss: 2.4125825464725494, Final Batch Loss: 0.5164220333099365\n",
      "Epoch 3547, Loss: 2.513803720474243, Final Batch Loss: 0.496664434671402\n",
      "Epoch 3548, Loss: 2.13895583152771, Final Batch Loss: 0.3758839964866638\n",
      "Epoch 3549, Loss: 2.2545820474624634, Final Batch Loss: 0.43735089898109436\n",
      "Epoch 3550, Loss: 2.470296561717987, Final Batch Loss: 0.6367331147193909\n",
      "Epoch 3551, Loss: 2.3584622740745544, Final Batch Loss: 0.3913218379020691\n",
      "Epoch 3552, Loss: 2.2371682226657867, Final Batch Loss: 0.40365660190582275\n",
      "Epoch 3553, Loss: 2.280839681625366, Final Batch Loss: 0.43737250566482544\n",
      "Epoch 3554, Loss: 2.370667040348053, Final Batch Loss: 0.5166067481040955\n",
      "Epoch 3555, Loss: 2.3343814611434937, Final Batch Loss: 0.4538748860359192\n",
      "Epoch 3556, Loss: 2.281884342432022, Final Batch Loss: 0.37711596488952637\n",
      "Epoch 3557, Loss: 2.1861454844474792, Final Batch Loss: 0.4989275634288788\n",
      "Epoch 3558, Loss: 2.262779325246811, Final Batch Loss: 0.3951510488986969\n",
      "Epoch 3559, Loss: 2.388088345527649, Final Batch Loss: 0.45078882575035095\n",
      "Epoch 3560, Loss: 2.346480816602707, Final Batch Loss: 0.5240417122840881\n",
      "Epoch 3561, Loss: 2.166750878095627, Final Batch Loss: 0.491540789604187\n",
      "Epoch 3562, Loss: 2.342202812433243, Final Batch Loss: 0.4905167818069458\n",
      "Epoch 3563, Loss: 2.628378838300705, Final Batch Loss: 0.5952561497688293\n",
      "Epoch 3564, Loss: 2.196630984544754, Final Batch Loss: 0.5391659736633301\n",
      "Epoch 3565, Loss: 2.3237501680850983, Final Batch Loss: 0.4427575469017029\n",
      "Epoch 3566, Loss: 2.072876065969467, Final Batch Loss: 0.3979632258415222\n",
      "Epoch 3567, Loss: 2.3088061809539795, Final Batch Loss: 0.5135828256607056\n",
      "Epoch 3568, Loss: 2.254225730895996, Final Batch Loss: 0.44889914989471436\n",
      "Epoch 3569, Loss: 2.213621884584427, Final Batch Loss: 0.4564030170440674\n",
      "Epoch 3570, Loss: 2.1364447474479675, Final Batch Loss: 0.2655351459980011\n",
      "Epoch 3571, Loss: 2.3045147359371185, Final Batch Loss: 0.4434834420681\n",
      "Epoch 3572, Loss: 2.1124472320079803, Final Batch Loss: 0.4188310205936432\n",
      "Epoch 3573, Loss: 2.343857377767563, Final Batch Loss: 0.5634737610816956\n",
      "Epoch 3574, Loss: 2.269052892923355, Final Batch Loss: 0.5428996682167053\n",
      "Epoch 3575, Loss: 2.2151239812374115, Final Batch Loss: 0.48684588074684143\n",
      "Epoch 3576, Loss: 2.2357744574546814, Final Batch Loss: 0.5437614917755127\n",
      "Epoch 3577, Loss: 2.482826292514801, Final Batch Loss: 0.4398346245288849\n",
      "Epoch 3578, Loss: 2.3738752603530884, Final Batch Loss: 0.5719401836395264\n",
      "Epoch 3579, Loss: 2.3452511727809906, Final Batch Loss: 0.4705066978931427\n",
      "Epoch 3580, Loss: 2.344854772090912, Final Batch Loss: 0.5642154812812805\n",
      "Epoch 3581, Loss: 2.317917078733444, Final Batch Loss: 0.39968857169151306\n",
      "Epoch 3582, Loss: 2.380887120962143, Final Batch Loss: 0.37954604625701904\n",
      "Epoch 3583, Loss: 2.466936856508255, Final Batch Loss: 0.4982554018497467\n",
      "Epoch 3584, Loss: 2.4084348380565643, Final Batch Loss: 0.487745076417923\n",
      "Epoch 3585, Loss: 2.3686284124851227, Final Batch Loss: 0.5470900535583496\n",
      "Epoch 3586, Loss: 2.4638264775276184, Final Batch Loss: 0.4725882112979889\n",
      "Epoch 3587, Loss: 2.2695677280426025, Final Batch Loss: 0.4141679108142853\n",
      "Epoch 3588, Loss: 2.316859871149063, Final Batch Loss: 0.5269864797592163\n",
      "Epoch 3589, Loss: 2.459238350391388, Final Batch Loss: 0.4668627977371216\n",
      "Epoch 3590, Loss: 2.2339212894439697, Final Batch Loss: 0.4215792119503021\n",
      "Epoch 3591, Loss: 2.363378494977951, Final Batch Loss: 0.4221459627151489\n",
      "Epoch 3592, Loss: 2.3418673872947693, Final Batch Loss: 0.4324822723865509\n",
      "Epoch 3593, Loss: 2.307979166507721, Final Batch Loss: 0.35566166043281555\n",
      "Epoch 3594, Loss: 2.351503163576126, Final Batch Loss: 0.4815968871116638\n",
      "Epoch 3595, Loss: 2.3747450709342957, Final Batch Loss: 0.5031914114952087\n",
      "Epoch 3596, Loss: 2.309473603963852, Final Batch Loss: 0.5415565371513367\n",
      "Epoch 3597, Loss: 2.3072547018527985, Final Batch Loss: 0.48600754141807556\n",
      "Epoch 3598, Loss: 2.536470741033554, Final Batch Loss: 0.6331813335418701\n",
      "Epoch 3599, Loss: 2.2860373854637146, Final Batch Loss: 0.4539380967617035\n",
      "Epoch 3600, Loss: 2.3765784800052643, Final Batch Loss: 0.47789958119392395\n",
      "Epoch 3601, Loss: 2.2981061935424805, Final Batch Loss: 0.46837422251701355\n",
      "Epoch 3602, Loss: 2.2475412487983704, Final Batch Loss: 0.4832228124141693\n",
      "Epoch 3603, Loss: 2.2683244049549103, Final Batch Loss: 0.44948145747184753\n",
      "Epoch 3604, Loss: 2.419541984796524, Final Batch Loss: 0.42382389307022095\n",
      "Epoch 3605, Loss: 2.232542634010315, Final Batch Loss: 0.46077626943588257\n",
      "Epoch 3606, Loss: 2.3447574973106384, Final Batch Loss: 0.5441317558288574\n",
      "Epoch 3607, Loss: 2.317381501197815, Final Batch Loss: 0.5386610627174377\n",
      "Epoch 3608, Loss: 2.3432289958000183, Final Batch Loss: 0.3441190719604492\n",
      "Epoch 3609, Loss: 2.411935329437256, Final Batch Loss: 0.5075869560241699\n",
      "Epoch 3610, Loss: 2.2586753964424133, Final Batch Loss: 0.5229417681694031\n",
      "Epoch 3611, Loss: 2.321244865655899, Final Batch Loss: 0.4634236693382263\n",
      "Epoch 3612, Loss: 2.2388890981674194, Final Batch Loss: 0.3894446790218353\n",
      "Epoch 3613, Loss: 2.1875174939632416, Final Batch Loss: 0.39994513988494873\n",
      "Epoch 3614, Loss: 2.2549183070659637, Final Batch Loss: 0.4700344204902649\n",
      "Epoch 3615, Loss: 2.1760246753692627, Final Batch Loss: 0.3955056071281433\n",
      "Epoch 3616, Loss: 2.4216705560684204, Final Batch Loss: 0.5348435640335083\n",
      "Epoch 3617, Loss: 2.4324411749839783, Final Batch Loss: 0.45351436734199524\n",
      "Epoch 3618, Loss: 2.257735550403595, Final Batch Loss: 0.48320019245147705\n",
      "Epoch 3619, Loss: 2.1864445507526398, Final Batch Loss: 0.37182751297950745\n",
      "Epoch 3620, Loss: 2.3139403462409973, Final Batch Loss: 0.3840871751308441\n",
      "Epoch 3621, Loss: 2.2455852925777435, Final Batch Loss: 0.42380625009536743\n",
      "Epoch 3622, Loss: 2.221059948205948, Final Batch Loss: 0.49758559465408325\n",
      "Epoch 3623, Loss: 2.296562284231186, Final Batch Loss: 0.39675936102867126\n",
      "Epoch 3624, Loss: 2.344490736722946, Final Batch Loss: 0.39137202501296997\n",
      "Epoch 3625, Loss: 2.3150084614753723, Final Batch Loss: 0.3734263479709625\n",
      "Epoch 3626, Loss: 2.5831160247325897, Final Batch Loss: 0.5187892913818359\n",
      "Epoch 3627, Loss: 2.2525589168071747, Final Batch Loss: 0.5492004156112671\n",
      "Epoch 3628, Loss: 2.4206434786319733, Final Batch Loss: 0.38777658343315125\n",
      "Epoch 3629, Loss: 2.474033921957016, Final Batch Loss: 0.4679356515407562\n",
      "Epoch 3630, Loss: 2.4011756479740143, Final Batch Loss: 0.557351291179657\n",
      "Epoch 3631, Loss: 2.333104819059372, Final Batch Loss: 0.47840017080307007\n",
      "Epoch 3632, Loss: 2.216849058866501, Final Batch Loss: 0.40788137912750244\n",
      "Epoch 3633, Loss: 2.416014552116394, Final Batch Loss: 0.46876829862594604\n",
      "Epoch 3634, Loss: 2.242988020181656, Final Batch Loss: 0.3492726981639862\n",
      "Epoch 3635, Loss: 2.3178118467330933, Final Batch Loss: 0.4879392385482788\n",
      "Epoch 3636, Loss: 2.6172724962234497, Final Batch Loss: 0.6090890169143677\n",
      "Epoch 3637, Loss: 2.2648786306381226, Final Batch Loss: 0.519164502620697\n",
      "Epoch 3638, Loss: 2.3543838262557983, Final Batch Loss: 0.37139514088630676\n",
      "Epoch 3639, Loss: 2.121212810277939, Final Batch Loss: 0.46227917075157166\n",
      "Epoch 3640, Loss: 2.3562454283237457, Final Batch Loss: 0.37818795442581177\n",
      "Epoch 3641, Loss: 2.3043737411499023, Final Batch Loss: 0.4770704209804535\n",
      "Epoch 3642, Loss: 2.276784598827362, Final Batch Loss: 0.5110499858856201\n",
      "Epoch 3643, Loss: 2.343317061662674, Final Batch Loss: 0.46747294068336487\n",
      "Epoch 3644, Loss: 2.347170054912567, Final Batch Loss: 0.4909442365169525\n",
      "Epoch 3645, Loss: 2.1487108170986176, Final Batch Loss: 0.31520316004753113\n",
      "Epoch 3646, Loss: 2.3804599940776825, Final Batch Loss: 0.5726062059402466\n",
      "Epoch 3647, Loss: 2.2123367190361023, Final Batch Loss: 0.40138036012649536\n",
      "Epoch 3648, Loss: 2.3273109197616577, Final Batch Loss: 0.43359726667404175\n",
      "Epoch 3649, Loss: 2.331524580717087, Final Batch Loss: 0.44059187173843384\n",
      "Epoch 3650, Loss: 2.3102019131183624, Final Batch Loss: 0.43161243200302124\n",
      "Epoch 3651, Loss: 2.251575380563736, Final Batch Loss: 0.528032124042511\n",
      "Epoch 3652, Loss: 2.449661910533905, Final Batch Loss: 0.4953756332397461\n",
      "Epoch 3653, Loss: 2.407283902168274, Final Batch Loss: 0.6692270636558533\n",
      "Epoch 3654, Loss: 2.150278478860855, Final Batch Loss: 0.5085330009460449\n",
      "Epoch 3655, Loss: 2.357771933078766, Final Batch Loss: 0.5283960103988647\n",
      "Epoch 3656, Loss: 2.545273542404175, Final Batch Loss: 0.47057706117630005\n",
      "Epoch 3657, Loss: 2.446503847837448, Final Batch Loss: 0.6457471251487732\n",
      "Epoch 3658, Loss: 2.1233535408973694, Final Batch Loss: 0.38509103655815125\n",
      "Epoch 3659, Loss: 2.306548982858658, Final Batch Loss: 0.5024973154067993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3660, Loss: 2.1804375052452087, Final Batch Loss: 0.36809417605400085\n",
      "Epoch 3661, Loss: 2.1589043140411377, Final Batch Loss: 0.4781439006328583\n",
      "Epoch 3662, Loss: 2.4099471271038055, Final Batch Loss: 0.41304948925971985\n",
      "Epoch 3663, Loss: 2.3593465089797974, Final Batch Loss: 0.48097318410873413\n",
      "Epoch 3664, Loss: 2.178677797317505, Final Batch Loss: 0.49928921461105347\n",
      "Epoch 3665, Loss: 2.3332794308662415, Final Batch Loss: 0.5049985647201538\n",
      "Epoch 3666, Loss: 2.301252782344818, Final Batch Loss: 0.5017644762992859\n",
      "Epoch 3667, Loss: 2.3254058361053467, Final Batch Loss: 0.5597921013832092\n",
      "Epoch 3668, Loss: 2.14028537273407, Final Batch Loss: 0.398349791765213\n",
      "Epoch 3669, Loss: 2.4733220040798187, Final Batch Loss: 0.5533024072647095\n",
      "Epoch 3670, Loss: 2.6296913027763367, Final Batch Loss: 0.6131054162979126\n",
      "Epoch 3671, Loss: 2.3795256912708282, Final Batch Loss: 0.4574258327484131\n",
      "Epoch 3672, Loss: 2.3697207272052765, Final Batch Loss: 0.657746434211731\n",
      "Epoch 3673, Loss: 2.290665417909622, Final Batch Loss: 0.6061594486236572\n",
      "Epoch 3674, Loss: 2.4257150292396545, Final Batch Loss: 0.5100249648094177\n",
      "Epoch 3675, Loss: 2.549048513174057, Final Batch Loss: 0.5880551338195801\n",
      "Epoch 3676, Loss: 2.3596628606319427, Final Batch Loss: 0.5780155658721924\n",
      "Epoch 3677, Loss: 2.207581728696823, Final Batch Loss: 0.3558071553707123\n",
      "Epoch 3678, Loss: 2.1717322170734406, Final Batch Loss: 0.3622322380542755\n",
      "Epoch 3679, Loss: 2.284110516309738, Final Batch Loss: 0.48236095905303955\n",
      "Epoch 3680, Loss: 2.515652447938919, Final Batch Loss: 0.45180243253707886\n",
      "Epoch 3681, Loss: 2.2762062549591064, Final Batch Loss: 0.43211084604263306\n",
      "Epoch 3682, Loss: 2.1873803436756134, Final Batch Loss: 0.4407946765422821\n",
      "Epoch 3683, Loss: 2.1428248584270477, Final Batch Loss: 0.47654375433921814\n",
      "Epoch 3684, Loss: 2.2526526749134064, Final Batch Loss: 0.41945680975914\n",
      "Epoch 3685, Loss: 2.3229156732559204, Final Batch Loss: 0.5114103555679321\n",
      "Epoch 3686, Loss: 2.2601005733013153, Final Batch Loss: 0.3004297614097595\n",
      "Epoch 3687, Loss: 2.278723269701004, Final Batch Loss: 0.3959827423095703\n",
      "Epoch 3688, Loss: 2.1524451971054077, Final Batch Loss: 0.4963809847831726\n",
      "Epoch 3689, Loss: 2.4161656498908997, Final Batch Loss: 0.5504751205444336\n",
      "Epoch 3690, Loss: 2.1225421130657196, Final Batch Loss: 0.4762197434902191\n",
      "Epoch 3691, Loss: 2.2562857270240784, Final Batch Loss: 0.40578410029411316\n",
      "Epoch 3692, Loss: 2.283454090356827, Final Batch Loss: 0.345056414604187\n",
      "Epoch 3693, Loss: 2.1015281677246094, Final Batch Loss: 0.4315909147262573\n",
      "Epoch 3694, Loss: 2.3765177130699158, Final Batch Loss: 0.5621446371078491\n",
      "Epoch 3695, Loss: 2.445243000984192, Final Batch Loss: 0.50753253698349\n",
      "Epoch 3696, Loss: 2.2916747629642487, Final Batch Loss: 0.471843957901001\n",
      "Epoch 3697, Loss: 2.1541800796985626, Final Batch Loss: 0.4448520541191101\n",
      "Epoch 3698, Loss: 2.2048845887184143, Final Batch Loss: 0.46352922916412354\n",
      "Epoch 3699, Loss: 2.178697168827057, Final Batch Loss: 0.3806198239326477\n",
      "Epoch 3700, Loss: 1.9081079363822937, Final Batch Loss: 0.243914395570755\n",
      "Epoch 3701, Loss: 2.2269122898578644, Final Batch Loss: 0.47392919659614563\n",
      "Epoch 3702, Loss: 2.2164919078350067, Final Batch Loss: 0.35974037647247314\n",
      "Epoch 3703, Loss: 2.274852454662323, Final Batch Loss: 0.4105755388736725\n",
      "Epoch 3704, Loss: 2.2593027651309967, Final Batch Loss: 0.47951918840408325\n",
      "Epoch 3705, Loss: 2.322465032339096, Final Batch Loss: 0.40030738711357117\n",
      "Epoch 3706, Loss: 2.2441281378269196, Final Batch Loss: 0.4621734619140625\n",
      "Epoch 3707, Loss: 2.3599604666233063, Final Batch Loss: 0.4903199374675751\n",
      "Epoch 3708, Loss: 2.301504820585251, Final Batch Loss: 0.38135698437690735\n",
      "Epoch 3709, Loss: 2.3272755444049835, Final Batch Loss: 0.5062780976295471\n",
      "Epoch 3710, Loss: 2.3389807641506195, Final Batch Loss: 0.40227994322776794\n",
      "Epoch 3711, Loss: 2.2536396980285645, Final Batch Loss: 0.3741839528083801\n",
      "Epoch 3712, Loss: 2.307511806488037, Final Batch Loss: 0.3689681887626648\n",
      "Epoch 3713, Loss: 2.203224629163742, Final Batch Loss: 0.4220307767391205\n",
      "Epoch 3714, Loss: 2.218312233686447, Final Batch Loss: 0.44480985403060913\n",
      "Epoch 3715, Loss: 2.1564943194389343, Final Batch Loss: 0.40821656584739685\n",
      "Epoch 3716, Loss: 2.2848359644412994, Final Batch Loss: 0.45157966017723083\n",
      "Epoch 3717, Loss: 2.2635703086853027, Final Batch Loss: 0.43412500619888306\n",
      "Epoch 3718, Loss: 2.2068452537059784, Final Batch Loss: 0.3777017891407013\n",
      "Epoch 3719, Loss: 2.4208446741104126, Final Batch Loss: 0.5528274178504944\n",
      "Epoch 3720, Loss: 2.4932532906532288, Final Batch Loss: 0.7001519799232483\n",
      "Epoch 3721, Loss: 2.2257951498031616, Final Batch Loss: 0.4448856711387634\n",
      "Epoch 3722, Loss: 2.200062334537506, Final Batch Loss: 0.5210201144218445\n",
      "Epoch 3723, Loss: 2.3085229992866516, Final Batch Loss: 0.5572758913040161\n",
      "Epoch 3724, Loss: 2.3810939490795135, Final Batch Loss: 0.490890771150589\n",
      "Epoch 3725, Loss: 2.170822948217392, Final Batch Loss: 0.33925336599349976\n",
      "Epoch 3726, Loss: 2.4669291377067566, Final Batch Loss: 0.6013394594192505\n",
      "Epoch 3727, Loss: 2.4268692433834076, Final Batch Loss: 0.37352585792541504\n",
      "Epoch 3728, Loss: 2.2054411470890045, Final Batch Loss: 0.45224273204803467\n",
      "Epoch 3729, Loss: 2.272676169872284, Final Batch Loss: 0.4589540660381317\n",
      "Epoch 3730, Loss: 2.482931822538376, Final Batch Loss: 0.46129339933395386\n",
      "Epoch 3731, Loss: 2.2679006159305573, Final Batch Loss: 0.4831606149673462\n",
      "Epoch 3732, Loss: 2.3273435831069946, Final Batch Loss: 0.39931240677833557\n",
      "Epoch 3733, Loss: 2.3907141387462616, Final Batch Loss: 0.5761470794677734\n",
      "Epoch 3734, Loss: 2.2444685995578766, Final Batch Loss: 0.34522315859794617\n",
      "Epoch 3735, Loss: 2.375857502222061, Final Batch Loss: 0.5077769756317139\n",
      "Epoch 3736, Loss: 2.446323573589325, Final Batch Loss: 0.6145890355110168\n",
      "Epoch 3737, Loss: 2.2664138078689575, Final Batch Loss: 0.48550572991371155\n",
      "Epoch 3738, Loss: 2.201350152492523, Final Batch Loss: 0.4297207295894623\n",
      "Epoch 3739, Loss: 2.226031482219696, Final Batch Loss: 0.4562661945819855\n",
      "Epoch 3740, Loss: 2.258188486099243, Final Batch Loss: 0.4295862913131714\n",
      "Epoch 3741, Loss: 2.2177714407444, Final Batch Loss: 0.4710851311683655\n",
      "Epoch 3742, Loss: 2.5480967462062836, Final Batch Loss: 0.4987562894821167\n",
      "Epoch 3743, Loss: 2.3778251111507416, Final Batch Loss: 0.5088114142417908\n",
      "Epoch 3744, Loss: 2.3116733133792877, Final Batch Loss: 0.40738147497177124\n",
      "Epoch 3745, Loss: 2.25650754570961, Final Batch Loss: 0.435425728559494\n",
      "Epoch 3746, Loss: 2.1879817843437195, Final Batch Loss: 0.5953533053398132\n",
      "Epoch 3747, Loss: 2.4677077531814575, Final Batch Loss: 0.6002386212348938\n",
      "Epoch 3748, Loss: 2.3331193327903748, Final Batch Loss: 0.3982916474342346\n",
      "Epoch 3749, Loss: 2.3843353986740112, Final Batch Loss: 0.3516257107257843\n",
      "Epoch 3750, Loss: 2.3160769939422607, Final Batch Loss: 0.44277048110961914\n",
      "Epoch 3751, Loss: 2.292518585920334, Final Batch Loss: 0.4660928547382355\n",
      "Epoch 3752, Loss: 2.6480390429496765, Final Batch Loss: 0.37646961212158203\n",
      "Epoch 3753, Loss: 2.3432198464870453, Final Batch Loss: 0.46061602234840393\n",
      "Epoch 3754, Loss: 2.383133113384247, Final Batch Loss: 0.42724305391311646\n",
      "Epoch 3755, Loss: 2.195812851190567, Final Batch Loss: 0.426327645778656\n",
      "Epoch 3756, Loss: 2.1553539633750916, Final Batch Loss: 0.4274666905403137\n",
      "Epoch 3757, Loss: 2.191348612308502, Final Batch Loss: 0.4554188549518585\n",
      "Epoch 3758, Loss: 2.420186758041382, Final Batch Loss: 0.5835348963737488\n",
      "Epoch 3759, Loss: 2.4800282418727875, Final Batch Loss: 0.6343576908111572\n",
      "Epoch 3760, Loss: 2.1661461889743805, Final Batch Loss: 0.35805943608283997\n",
      "Epoch 3761, Loss: 2.2244666814804077, Final Batch Loss: 0.4780704379081726\n",
      "Epoch 3762, Loss: 2.1990300714969635, Final Batch Loss: 0.4355211555957794\n",
      "Epoch 3763, Loss: 2.280069023370743, Final Batch Loss: 0.40306517481803894\n",
      "Epoch 3764, Loss: 2.1898462772369385, Final Batch Loss: 0.4996911287307739\n",
      "Epoch 3765, Loss: 2.2948562800884247, Final Batch Loss: 0.49766939878463745\n",
      "Epoch 3766, Loss: 2.2215302288532257, Final Batch Loss: 0.3912506699562073\n",
      "Epoch 3767, Loss: 2.1249310970306396, Final Batch Loss: 0.35503023862838745\n",
      "Epoch 3768, Loss: 2.3122144639492035, Final Batch Loss: 0.3497481644153595\n",
      "Epoch 3769, Loss: 2.350790172815323, Final Batch Loss: 0.4812643527984619\n",
      "Epoch 3770, Loss: 2.41445130109787, Final Batch Loss: 0.43195465207099915\n",
      "Epoch 3771, Loss: 2.368394136428833, Final Batch Loss: 0.5276938080787659\n",
      "Epoch 3772, Loss: 2.0976702272892, Final Batch Loss: 0.3613617420196533\n",
      "Epoch 3773, Loss: 2.249234765768051, Final Batch Loss: 0.4396315813064575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3774, Loss: 2.4230087101459503, Final Batch Loss: 0.5722267031669617\n",
      "Epoch 3775, Loss: 2.3084740340709686, Final Batch Loss: 0.4387999176979065\n",
      "Epoch 3776, Loss: 2.1972501277923584, Final Batch Loss: 0.526473879814148\n",
      "Epoch 3777, Loss: 2.366032510995865, Final Batch Loss: 0.5269725918769836\n",
      "Epoch 3778, Loss: 2.1711463928222656, Final Batch Loss: 0.42612287402153015\n",
      "Epoch 3779, Loss: 2.3335773050785065, Final Batch Loss: 0.43431416153907776\n",
      "Epoch 3780, Loss: 2.328969657421112, Final Batch Loss: 0.4643734097480774\n",
      "Epoch 3781, Loss: 2.5443065464496613, Final Batch Loss: 0.6896392107009888\n",
      "Epoch 3782, Loss: 2.349699705839157, Final Batch Loss: 0.4950384795665741\n",
      "Epoch 3783, Loss: 2.5664961636066437, Final Batch Loss: 0.5437923073768616\n",
      "Epoch 3784, Loss: 2.441034972667694, Final Batch Loss: 0.4206686019897461\n",
      "Epoch 3785, Loss: 2.3714428544044495, Final Batch Loss: 0.5088641047477722\n",
      "Epoch 3786, Loss: 2.233943611383438, Final Batch Loss: 0.4338260293006897\n",
      "Epoch 3787, Loss: 2.336912900209427, Final Batch Loss: 0.46491074562072754\n",
      "Epoch 3788, Loss: 2.311873495578766, Final Batch Loss: 0.42659640312194824\n",
      "Epoch 3789, Loss: 2.2491840422153473, Final Batch Loss: 0.5467575788497925\n",
      "Epoch 3790, Loss: 2.2367821633815765, Final Batch Loss: 0.47959551215171814\n",
      "Epoch 3791, Loss: 2.2908563017845154, Final Batch Loss: 0.4655795991420746\n",
      "Epoch 3792, Loss: 2.1324157118797302, Final Batch Loss: 0.4040796756744385\n",
      "Epoch 3793, Loss: 2.2355331480503082, Final Batch Loss: 0.34276485443115234\n",
      "Epoch 3794, Loss: 2.4359398186206818, Final Batch Loss: 0.5752012133598328\n",
      "Epoch 3795, Loss: 2.2449357211589813, Final Batch Loss: 0.4268251359462738\n",
      "Epoch 3796, Loss: 2.450806438922882, Final Batch Loss: 0.5233741998672485\n",
      "Epoch 3797, Loss: 2.222491055727005, Final Batch Loss: 0.47552216053009033\n",
      "Epoch 3798, Loss: 2.4250491559505463, Final Batch Loss: 0.4832715690135956\n",
      "Epoch 3799, Loss: 2.31889009475708, Final Batch Loss: 0.5155621767044067\n",
      "Epoch 3800, Loss: 2.185257375240326, Final Batch Loss: 0.3302486538887024\n",
      "Epoch 3801, Loss: 2.0643057227134705, Final Batch Loss: 0.5034554600715637\n",
      "Epoch 3802, Loss: 2.113730400800705, Final Batch Loss: 0.36371269822120667\n",
      "Epoch 3803, Loss: 2.1852219104766846, Final Batch Loss: 0.3297923505306244\n",
      "Epoch 3804, Loss: 2.2697263956069946, Final Batch Loss: 0.5389731526374817\n",
      "Epoch 3805, Loss: 2.212193727493286, Final Batch Loss: 0.4035165011882782\n",
      "Epoch 3806, Loss: 2.1792791187763214, Final Batch Loss: 0.47418487071990967\n",
      "Epoch 3807, Loss: 2.233766406774521, Final Batch Loss: 0.42361271381378174\n",
      "Epoch 3808, Loss: 2.2204157412052155, Final Batch Loss: 0.405068039894104\n",
      "Epoch 3809, Loss: 2.1458264589309692, Final Batch Loss: 0.40949562191963196\n",
      "Epoch 3810, Loss: 2.3724214136600494, Final Batch Loss: 0.556581974029541\n",
      "Epoch 3811, Loss: 2.3787443339824677, Final Batch Loss: 0.47694432735443115\n",
      "Epoch 3812, Loss: 2.0777894854545593, Final Batch Loss: 0.2992842495441437\n",
      "Epoch 3813, Loss: 2.4090584218502045, Final Batch Loss: 0.42890357971191406\n",
      "Epoch 3814, Loss: 2.3116950690746307, Final Batch Loss: 0.46005332469940186\n",
      "Epoch 3815, Loss: 2.289134055376053, Final Batch Loss: 0.4510349631309509\n",
      "Epoch 3816, Loss: 2.127907156944275, Final Batch Loss: 0.4399620294570923\n",
      "Epoch 3817, Loss: 2.263900727033615, Final Batch Loss: 0.5399612784385681\n",
      "Epoch 3818, Loss: 2.2480103373527527, Final Batch Loss: 0.36426886916160583\n",
      "Epoch 3819, Loss: 2.2259331047534943, Final Batch Loss: 0.4410623013973236\n",
      "Epoch 3820, Loss: 2.10879185795784, Final Batch Loss: 0.5147415995597839\n",
      "Epoch 3821, Loss: 2.07828688621521, Final Batch Loss: 0.3358181118965149\n",
      "Epoch 3822, Loss: 2.040979325771332, Final Batch Loss: 0.4299311935901642\n",
      "Epoch 3823, Loss: 2.1685721576213837, Final Batch Loss: 0.46776285767555237\n",
      "Epoch 3824, Loss: 2.0921695828437805, Final Batch Loss: 0.36340266466140747\n",
      "Epoch 3825, Loss: 2.2079143822193146, Final Batch Loss: 0.38091912865638733\n",
      "Epoch 3826, Loss: 2.2427091896533966, Final Batch Loss: 0.3144589364528656\n",
      "Epoch 3827, Loss: 2.51027113199234, Final Batch Loss: 0.3739243447780609\n",
      "Epoch 3828, Loss: 2.2222081422805786, Final Batch Loss: 0.4841751754283905\n",
      "Epoch 3829, Loss: 2.3218035995960236, Final Batch Loss: 0.3587122857570648\n",
      "Epoch 3830, Loss: 2.449245184659958, Final Batch Loss: 0.504342257976532\n",
      "Epoch 3831, Loss: 2.135783702135086, Final Batch Loss: 0.4199715852737427\n",
      "Epoch 3832, Loss: 2.384371042251587, Final Batch Loss: 0.46344882249832153\n",
      "Epoch 3833, Loss: 2.195881813764572, Final Batch Loss: 0.3543115556240082\n",
      "Epoch 3834, Loss: 2.1583269238471985, Final Batch Loss: 0.43018636107444763\n",
      "Epoch 3835, Loss: 2.2061122059822083, Final Batch Loss: 0.36809349060058594\n",
      "Epoch 3836, Loss: 2.1549237966537476, Final Batch Loss: 0.3800373077392578\n",
      "Epoch 3837, Loss: 2.4086248576641083, Final Batch Loss: 0.4980803430080414\n",
      "Epoch 3838, Loss: 2.3546931743621826, Final Batch Loss: 0.3794826567173004\n",
      "Epoch 3839, Loss: 2.290475755929947, Final Batch Loss: 0.4616350829601288\n",
      "Epoch 3840, Loss: 2.4184004068374634, Final Batch Loss: 0.45218801498413086\n",
      "Epoch 3841, Loss: 2.162514716386795, Final Batch Loss: 0.40983182191848755\n",
      "Epoch 3842, Loss: 2.223706543445587, Final Batch Loss: 0.37245824933052063\n",
      "Epoch 3843, Loss: 2.079404890537262, Final Batch Loss: 0.39284586906433105\n",
      "Epoch 3844, Loss: 2.0931981801986694, Final Batch Loss: 0.39066100120544434\n",
      "Epoch 3845, Loss: 2.48858579993248, Final Batch Loss: 0.4733569920063019\n",
      "Epoch 3846, Loss: 2.100978583097458, Final Batch Loss: 0.3800714910030365\n",
      "Epoch 3847, Loss: 2.1336382627487183, Final Batch Loss: 0.3890203833580017\n",
      "Epoch 3848, Loss: 2.2883335649967194, Final Batch Loss: 0.4196668565273285\n",
      "Epoch 3849, Loss: 2.1483697295188904, Final Batch Loss: 0.3843090832233429\n",
      "Epoch 3850, Loss: 2.2156749963760376, Final Batch Loss: 0.5205445885658264\n",
      "Epoch 3851, Loss: 2.2656257152557373, Final Batch Loss: 0.5885007977485657\n",
      "Epoch 3852, Loss: 2.2581759095191956, Final Batch Loss: 0.4870760142803192\n",
      "Epoch 3853, Loss: 2.134739398956299, Final Batch Loss: 0.42421966791152954\n",
      "Epoch 3854, Loss: 2.107296258211136, Final Batch Loss: 0.4433271884918213\n",
      "Epoch 3855, Loss: 2.1667850613594055, Final Batch Loss: 0.4435639977455139\n",
      "Epoch 3856, Loss: 2.364515870809555, Final Batch Loss: 0.6113744974136353\n",
      "Epoch 3857, Loss: 2.416898638010025, Final Batch Loss: 0.4217287003993988\n",
      "Epoch 3858, Loss: 2.2044535279273987, Final Batch Loss: 0.4034024477005005\n",
      "Epoch 3859, Loss: 2.2241820096969604, Final Batch Loss: 0.46553555130958557\n",
      "Epoch 3860, Loss: 2.3806531727313995, Final Batch Loss: 0.3738293945789337\n",
      "Epoch 3861, Loss: 2.2724773287773132, Final Batch Loss: 0.5204570889472961\n",
      "Epoch 3862, Loss: 2.342785894870758, Final Batch Loss: 0.47123202681541443\n",
      "Epoch 3863, Loss: 2.2064505219459534, Final Batch Loss: 0.46410247683525085\n",
      "Epoch 3864, Loss: 2.4428330063819885, Final Batch Loss: 0.623652994632721\n",
      "Epoch 3865, Loss: 2.1936865746974945, Final Batch Loss: 0.3885715901851654\n",
      "Epoch 3866, Loss: 2.3428467512130737, Final Batch Loss: 0.42255961894989014\n",
      "Epoch 3867, Loss: 2.234165906906128, Final Batch Loss: 0.3919464647769928\n",
      "Epoch 3868, Loss: 2.3843600153923035, Final Batch Loss: 0.5751969814300537\n",
      "Epoch 3869, Loss: 2.3103572726249695, Final Batch Loss: 0.5335464477539062\n",
      "Epoch 3870, Loss: 2.3910288214683533, Final Batch Loss: 0.533200740814209\n",
      "Epoch 3871, Loss: 2.3076427280902863, Final Batch Loss: 0.42210152745246887\n",
      "Epoch 3872, Loss: 2.238598793745041, Final Batch Loss: 0.36726114153862\n",
      "Epoch 3873, Loss: 2.1379005014896393, Final Batch Loss: 0.3517407178878784\n",
      "Epoch 3874, Loss: 2.03780198097229, Final Batch Loss: 0.46462908387184143\n",
      "Epoch 3875, Loss: 2.3181375563144684, Final Batch Loss: 0.5066145658493042\n",
      "Epoch 3876, Loss: 2.222395598888397, Final Batch Loss: 0.42912939190864563\n",
      "Epoch 3877, Loss: 2.33529269695282, Final Batch Loss: 0.6064839363098145\n",
      "Epoch 3878, Loss: 2.2126260101795197, Final Batch Loss: 0.4037415683269501\n",
      "Epoch 3879, Loss: 2.4812690913677216, Final Batch Loss: 0.4301774501800537\n",
      "Epoch 3880, Loss: 2.1557391583919525, Final Batch Loss: 0.4577135741710663\n",
      "Epoch 3881, Loss: 2.0840762555599213, Final Batch Loss: 0.5406559109687805\n",
      "Epoch 3882, Loss: 2.188822031021118, Final Batch Loss: 0.5177414417266846\n",
      "Epoch 3883, Loss: 2.266092896461487, Final Batch Loss: 0.4294823706150055\n",
      "Epoch 3884, Loss: 2.2960956692695618, Final Batch Loss: 0.5551019310951233\n",
      "Epoch 3885, Loss: 2.224150002002716, Final Batch Loss: 0.3921980857849121\n",
      "Epoch 3886, Loss: 2.4266158044338226, Final Batch Loss: 0.5666946768760681\n",
      "Epoch 3887, Loss: 2.2858528792858124, Final Batch Loss: 0.40356093645095825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3888, Loss: 2.3394831717014313, Final Batch Loss: 0.5768793225288391\n",
      "Epoch 3889, Loss: 2.3841774463653564, Final Batch Loss: 0.6793776154518127\n",
      "Epoch 3890, Loss: 2.321536511182785, Final Batch Loss: 0.5038002729415894\n",
      "Epoch 3891, Loss: 2.4350507855415344, Final Batch Loss: 0.5164428949356079\n",
      "Epoch 3892, Loss: 2.4019752740859985, Final Batch Loss: 0.5163680911064148\n",
      "Epoch 3893, Loss: 2.193712741136551, Final Batch Loss: 0.3412624001502991\n",
      "Epoch 3894, Loss: 2.087255537509918, Final Batch Loss: 0.40698015689849854\n",
      "Epoch 3895, Loss: 2.072265088558197, Final Batch Loss: 0.3647709786891937\n",
      "Epoch 3896, Loss: 2.1928781270980835, Final Batch Loss: 0.3563563823699951\n",
      "Epoch 3897, Loss: 1.952620655298233, Final Batch Loss: 0.3442152440547943\n",
      "Epoch 3898, Loss: 2.172341614961624, Final Batch Loss: 0.4834105372428894\n",
      "Epoch 3899, Loss: 2.278166741132736, Final Batch Loss: 0.5391709804534912\n",
      "Epoch 3900, Loss: 2.2819749414920807, Final Batch Loss: 0.45115986466407776\n",
      "Epoch 3901, Loss: 2.0703601241111755, Final Batch Loss: 0.30389833450317383\n",
      "Epoch 3902, Loss: 2.3338828682899475, Final Batch Loss: 0.5183749198913574\n",
      "Epoch 3903, Loss: 2.409582793712616, Final Batch Loss: 0.5129638910293579\n",
      "Epoch 3904, Loss: 2.1316536962985992, Final Batch Loss: 0.37171709537506104\n",
      "Epoch 3905, Loss: 2.224952757358551, Final Batch Loss: 0.532879650592804\n",
      "Epoch 3906, Loss: 2.2633538246154785, Final Batch Loss: 0.47386422753334045\n",
      "Epoch 3907, Loss: 2.0646274089813232, Final Batch Loss: 0.47642064094543457\n",
      "Epoch 3908, Loss: 2.2692378163337708, Final Batch Loss: 0.4648948013782501\n",
      "Epoch 3909, Loss: 2.3623973429203033, Final Batch Loss: 0.5240060091018677\n",
      "Epoch 3910, Loss: 2.3789122700691223, Final Batch Loss: 0.36483103036880493\n",
      "Epoch 3911, Loss: 2.2241562008857727, Final Batch Loss: 0.43894287943840027\n",
      "Epoch 3912, Loss: 2.1568973660469055, Final Batch Loss: 0.3380388617515564\n",
      "Epoch 3913, Loss: 2.36076283454895, Final Batch Loss: 0.47056952118873596\n",
      "Epoch 3914, Loss: 2.1458429992198944, Final Batch Loss: 0.42334046959877014\n",
      "Epoch 3915, Loss: 2.2448941469192505, Final Batch Loss: 0.5088229179382324\n",
      "Epoch 3916, Loss: 2.364307850599289, Final Batch Loss: 0.5135235786437988\n",
      "Epoch 3917, Loss: 2.4559299647808075, Final Batch Loss: 0.5320190191268921\n",
      "Epoch 3918, Loss: 2.298855245113373, Final Batch Loss: 0.4682023525238037\n",
      "Epoch 3919, Loss: 2.218912899494171, Final Batch Loss: 0.5106365084648132\n",
      "Epoch 3920, Loss: 2.172859162092209, Final Batch Loss: 0.39749762415885925\n",
      "Epoch 3921, Loss: 2.114483892917633, Final Batch Loss: 0.34901663661003113\n",
      "Epoch 3922, Loss: 2.1920207738876343, Final Batch Loss: 0.44472697377204895\n",
      "Epoch 3923, Loss: 2.235416054725647, Final Batch Loss: 0.48915332555770874\n",
      "Epoch 3924, Loss: 2.203037917613983, Final Batch Loss: 0.43928802013397217\n",
      "Epoch 3925, Loss: 2.2318229973316193, Final Batch Loss: 0.42678216099739075\n",
      "Epoch 3926, Loss: 2.3169168531894684, Final Batch Loss: 0.42686548829078674\n",
      "Epoch 3927, Loss: 2.1696762442588806, Final Batch Loss: 0.40576282143592834\n",
      "Epoch 3928, Loss: 2.184493362903595, Final Batch Loss: 0.407997190952301\n",
      "Epoch 3929, Loss: 2.0755715370178223, Final Batch Loss: 0.35000258684158325\n",
      "Epoch 3930, Loss: 2.251513957977295, Final Batch Loss: 0.46270284056663513\n",
      "Epoch 3931, Loss: 2.4300818741321564, Final Batch Loss: 0.6670524477958679\n",
      "Epoch 3932, Loss: 2.2229423224925995, Final Batch Loss: 0.42995065450668335\n",
      "Epoch 3933, Loss: 2.3398608565330505, Final Batch Loss: 0.48009127378463745\n",
      "Epoch 3934, Loss: 2.3064146041870117, Final Batch Loss: 0.4600983262062073\n",
      "Epoch 3935, Loss: 2.3667794466018677, Final Batch Loss: 0.42221900820732117\n",
      "Epoch 3936, Loss: 2.5630635619163513, Final Batch Loss: 0.4756224453449249\n",
      "Epoch 3937, Loss: 2.3421355485916138, Final Batch Loss: 0.5154190063476562\n",
      "Epoch 3938, Loss: 2.338350921869278, Final Batch Loss: 0.4855583906173706\n",
      "Epoch 3939, Loss: 2.115369439125061, Final Batch Loss: 0.4089576303958893\n",
      "Epoch 3940, Loss: 2.4203625917434692, Final Batch Loss: 0.504505455493927\n",
      "Epoch 3941, Loss: 2.3759925961494446, Final Batch Loss: 0.4998839199542999\n",
      "Epoch 3942, Loss: 2.4772987961769104, Final Batch Loss: 0.5162681341171265\n",
      "Epoch 3943, Loss: 2.289079487323761, Final Batch Loss: 0.4788260757923126\n",
      "Epoch 3944, Loss: 2.297734707593918, Final Batch Loss: 0.5782279968261719\n",
      "Epoch 3945, Loss: 2.3741916120052338, Final Batch Loss: 0.5094029307365417\n",
      "Epoch 3946, Loss: 2.1622168123722076, Final Batch Loss: 0.45221030712127686\n",
      "Epoch 3947, Loss: 2.4136657416820526, Final Batch Loss: 0.5493764877319336\n",
      "Epoch 3948, Loss: 2.2349159121513367, Final Batch Loss: 0.43491628766059875\n",
      "Epoch 3949, Loss: 2.3122778236865997, Final Batch Loss: 0.4597782492637634\n",
      "Epoch 3950, Loss: 2.2868983447551727, Final Batch Loss: 0.4677877426147461\n",
      "Epoch 3951, Loss: 2.3679282665252686, Final Batch Loss: 0.45605772733688354\n",
      "Epoch 3952, Loss: 2.2518513202667236, Final Batch Loss: 0.5105035305023193\n",
      "Epoch 3953, Loss: 2.1041602194309235, Final Batch Loss: 0.3926638960838318\n",
      "Epoch 3954, Loss: 2.358084052801132, Final Batch Loss: 0.6475518345832825\n",
      "Epoch 3955, Loss: 2.1876445710659027, Final Batch Loss: 0.47909945249557495\n",
      "Epoch 3956, Loss: 2.0922997891902924, Final Batch Loss: 0.365305632352829\n",
      "Epoch 3957, Loss: 2.096947729587555, Final Batch Loss: 0.379177987575531\n",
      "Epoch 3958, Loss: 2.211849182844162, Final Batch Loss: 0.48177459836006165\n",
      "Epoch 3959, Loss: 2.2610741555690765, Final Batch Loss: 0.49795854091644287\n",
      "Epoch 3960, Loss: 2.1409785747528076, Final Batch Loss: 0.41176149249076843\n",
      "Epoch 3961, Loss: 2.097413420677185, Final Batch Loss: 0.2824532985687256\n",
      "Epoch 3962, Loss: 2.28521329164505, Final Batch Loss: 0.5094942450523376\n",
      "Epoch 3963, Loss: 2.3572687208652496, Final Batch Loss: 0.5863980650901794\n",
      "Epoch 3964, Loss: 2.0124784409999847, Final Batch Loss: 0.44672873616218567\n",
      "Epoch 3965, Loss: 2.2487673163414, Final Batch Loss: 0.39367929100990295\n",
      "Epoch 3966, Loss: 2.371712327003479, Final Batch Loss: 0.4968194365501404\n",
      "Epoch 3967, Loss: 2.391363501548767, Final Batch Loss: 0.5920699834823608\n",
      "Epoch 3968, Loss: 2.1546119153499603, Final Batch Loss: 0.4645695090293884\n",
      "Epoch 3969, Loss: 2.079222470521927, Final Batch Loss: 0.44174498319625854\n",
      "Epoch 3970, Loss: 2.3710047900676727, Final Batch Loss: 0.5955936312675476\n",
      "Epoch 3971, Loss: 2.3894501328468323, Final Batch Loss: 0.47698336839675903\n",
      "Epoch 3972, Loss: 2.10072860121727, Final Batch Loss: 0.3642421066761017\n",
      "Epoch 3973, Loss: 2.1954050064086914, Final Batch Loss: 0.430019736289978\n",
      "Epoch 3974, Loss: 2.4694995284080505, Final Batch Loss: 0.5360952615737915\n",
      "Epoch 3975, Loss: 2.4364996254444122, Final Batch Loss: 0.5579835772514343\n",
      "Epoch 3976, Loss: 2.2227665185928345, Final Batch Loss: 0.38181471824645996\n",
      "Epoch 3977, Loss: 2.1700688898563385, Final Batch Loss: 0.5030692219734192\n",
      "Epoch 3978, Loss: 2.2120092809200287, Final Batch Loss: 0.5211655497550964\n",
      "Epoch 3979, Loss: 2.0779638588428497, Final Batch Loss: 0.377946674823761\n",
      "Epoch 3980, Loss: 2.241079270839691, Final Batch Loss: 0.47940364480018616\n",
      "Epoch 3981, Loss: 2.178356558084488, Final Batch Loss: 0.34830811619758606\n",
      "Epoch 3982, Loss: 2.0937812626361847, Final Batch Loss: 0.4350050985813141\n",
      "Epoch 3983, Loss: 2.447474956512451, Final Batch Loss: 0.5620536804199219\n",
      "Epoch 3984, Loss: 2.227198302745819, Final Batch Loss: 0.5508484244346619\n",
      "Epoch 3985, Loss: 2.3322764933109283, Final Batch Loss: 0.4549744725227356\n",
      "Epoch 3986, Loss: 2.273639976978302, Final Batch Loss: 0.4291510283946991\n",
      "Epoch 3987, Loss: 2.212371051311493, Final Batch Loss: 0.43359994888305664\n",
      "Epoch 3988, Loss: 2.5075938999652863, Final Batch Loss: 0.6188664436340332\n",
      "Epoch 3989, Loss: 2.4786069691181183, Final Batch Loss: 0.42689210176467896\n",
      "Epoch 3990, Loss: 2.328649938106537, Final Batch Loss: 0.4079068899154663\n",
      "Epoch 3991, Loss: 2.470733642578125, Final Batch Loss: 0.4903241991996765\n",
      "Epoch 3992, Loss: 2.416501998901367, Final Batch Loss: 0.44579795002937317\n",
      "Epoch 3993, Loss: 2.318399429321289, Final Batch Loss: 0.47744351625442505\n",
      "Epoch 3994, Loss: 2.222407251596451, Final Batch Loss: 0.3899516463279724\n",
      "Epoch 3995, Loss: 2.310852438211441, Final Batch Loss: 0.5232782959938049\n",
      "Epoch 3996, Loss: 2.443380892276764, Final Batch Loss: 0.5690293312072754\n",
      "Epoch 3997, Loss: 2.249831795692444, Final Batch Loss: 0.49814707040786743\n",
      "Epoch 3998, Loss: 2.0508905947208405, Final Batch Loss: 0.4392533302307129\n",
      "Epoch 3999, Loss: 2.28082475066185, Final Batch Loss: 0.5983120203018188\n",
      "Epoch 4000, Loss: 2.29813015460968, Final Batch Loss: 0.41953304409980774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4001, Loss: 2.071864575147629, Final Batch Loss: 0.3712198734283447\n",
      "Epoch 4002, Loss: 2.3218440413475037, Final Batch Loss: 0.5515277981758118\n",
      "Epoch 4003, Loss: 2.1662690937519073, Final Batch Loss: 0.40494030714035034\n",
      "Epoch 4004, Loss: 2.609785407781601, Final Batch Loss: 0.8100259900093079\n",
      "Epoch 4005, Loss: 2.168553948402405, Final Batch Loss: 0.41114914417266846\n",
      "Epoch 4006, Loss: 2.1520341634750366, Final Batch Loss: 0.4992242157459259\n",
      "Epoch 4007, Loss: 2.2948637306690216, Final Batch Loss: 0.5303468108177185\n",
      "Epoch 4008, Loss: 2.159200757741928, Final Batch Loss: 0.3696207106113434\n",
      "Epoch 4009, Loss: 2.0916226506233215, Final Batch Loss: 0.38520413637161255\n",
      "Epoch 4010, Loss: 2.0577166974544525, Final Batch Loss: 0.3028908967971802\n",
      "Epoch 4011, Loss: 2.350778967142105, Final Batch Loss: 0.42863503098487854\n",
      "Epoch 4012, Loss: 2.3500046730041504, Final Batch Loss: 0.49948394298553467\n",
      "Epoch 4013, Loss: 2.34806826710701, Final Batch Loss: 0.5219483375549316\n",
      "Epoch 4014, Loss: 2.1815112829208374, Final Batch Loss: 0.5761282444000244\n",
      "Epoch 4015, Loss: 2.1672258973121643, Final Batch Loss: 0.3943385183811188\n",
      "Epoch 4016, Loss: 2.2703818678855896, Final Batch Loss: 0.6178878545761108\n",
      "Epoch 4017, Loss: 2.091871351003647, Final Batch Loss: 0.3600116968154907\n",
      "Epoch 4018, Loss: 2.448209911584854, Final Batch Loss: 0.6523569822311401\n",
      "Epoch 4019, Loss: 2.5154583156108856, Final Batch Loss: 0.5797439813613892\n",
      "Epoch 4020, Loss: 2.305301547050476, Final Batch Loss: 0.3866254985332489\n",
      "Epoch 4021, Loss: 2.118074059486389, Final Batch Loss: 0.4207744002342224\n",
      "Epoch 4022, Loss: 2.356393873691559, Final Batch Loss: 0.5850846171379089\n",
      "Epoch 4023, Loss: 2.1027399599552155, Final Batch Loss: 0.4705211818218231\n",
      "Epoch 4024, Loss: 2.210353910923004, Final Batch Loss: 0.4244915246963501\n",
      "Epoch 4025, Loss: 2.0929973125457764, Final Batch Loss: 0.2660124897956848\n",
      "Epoch 4026, Loss: 2.198154866695404, Final Batch Loss: 0.38280197978019714\n",
      "Epoch 4027, Loss: 2.1773737370967865, Final Batch Loss: 0.43276235461235046\n",
      "Epoch 4028, Loss: 2.078425645828247, Final Batch Loss: 0.3384200632572174\n",
      "Epoch 4029, Loss: 2.4219084680080414, Final Batch Loss: 0.4519321322441101\n",
      "Epoch 4030, Loss: 2.2454926669597626, Final Batch Loss: 0.3732873499393463\n",
      "Epoch 4031, Loss: 2.2344751954078674, Final Batch Loss: 0.4371040463447571\n",
      "Epoch 4032, Loss: 2.1934922337532043, Final Batch Loss: 0.5891708731651306\n",
      "Epoch 4033, Loss: 2.187623381614685, Final Batch Loss: 0.4977388083934784\n",
      "Epoch 4034, Loss: 2.1917885839939117, Final Batch Loss: 0.3922531008720398\n",
      "Epoch 4035, Loss: 2.2642582952976227, Final Batch Loss: 0.4550524950027466\n",
      "Epoch 4036, Loss: 2.1928990483283997, Final Batch Loss: 0.3714754283428192\n",
      "Epoch 4037, Loss: 2.1729367673397064, Final Batch Loss: 0.469085156917572\n",
      "Epoch 4038, Loss: 2.348630130290985, Final Batch Loss: 0.5137644410133362\n",
      "Epoch 4039, Loss: 2.1422829627990723, Final Batch Loss: 0.38529732823371887\n",
      "Epoch 4040, Loss: 2.1918384432792664, Final Batch Loss: 0.43529212474823\n",
      "Epoch 4041, Loss: 2.351605534553528, Final Batch Loss: 0.5547592639923096\n",
      "Epoch 4042, Loss: 2.4365583658218384, Final Batch Loss: 0.409896582365036\n",
      "Epoch 4043, Loss: 2.3670548498630524, Final Batch Loss: 0.4058886468410492\n",
      "Epoch 4044, Loss: 2.2854581773281097, Final Batch Loss: 0.39872393012046814\n",
      "Epoch 4045, Loss: 2.1061676144599915, Final Batch Loss: 0.43658703565597534\n",
      "Epoch 4046, Loss: 2.3540377616882324, Final Batch Loss: 0.4699423015117645\n",
      "Epoch 4047, Loss: 2.4758267402648926, Final Batch Loss: 0.506498396396637\n",
      "Epoch 4048, Loss: 2.380570203065872, Final Batch Loss: 0.38021183013916016\n",
      "Epoch 4049, Loss: 2.1439419090747833, Final Batch Loss: 0.36249810457229614\n",
      "Epoch 4050, Loss: 2.3202669620513916, Final Batch Loss: 0.5195364356040955\n",
      "Epoch 4051, Loss: 2.1636962294578552, Final Batch Loss: 0.38738036155700684\n",
      "Epoch 4052, Loss: 2.281649798154831, Final Batch Loss: 0.4406122863292694\n",
      "Epoch 4053, Loss: 2.253907948732376, Final Batch Loss: 0.4682186245918274\n",
      "Epoch 4054, Loss: 2.111238181591034, Final Batch Loss: 0.3933500647544861\n",
      "Epoch 4055, Loss: 2.3463508784770966, Final Batch Loss: 0.4062439799308777\n",
      "Epoch 4056, Loss: 2.278049796819687, Final Batch Loss: 0.3586043119430542\n",
      "Epoch 4057, Loss: 2.078514277935028, Final Batch Loss: 0.3808366656303406\n",
      "Epoch 4058, Loss: 2.2674178779125214, Final Batch Loss: 0.4387446641921997\n",
      "Epoch 4059, Loss: 2.279803365468979, Final Batch Loss: 0.5304393172264099\n",
      "Epoch 4060, Loss: 2.1515394151210785, Final Batch Loss: 0.4274075925350189\n",
      "Epoch 4061, Loss: 2.1869750022888184, Final Batch Loss: 0.40771156549453735\n",
      "Epoch 4062, Loss: 2.0569999516010284, Final Batch Loss: 0.46288374066352844\n",
      "Epoch 4063, Loss: 2.176102101802826, Final Batch Loss: 0.3862362205982208\n",
      "Epoch 4064, Loss: 2.1651216745376587, Final Batch Loss: 0.5170596837997437\n",
      "Epoch 4065, Loss: 2.16496342420578, Final Batch Loss: 0.3177368640899658\n",
      "Epoch 4066, Loss: 2.3687750101089478, Final Batch Loss: 0.4537699818611145\n",
      "Epoch 4067, Loss: 2.3484755754470825, Final Batch Loss: 0.4815637469291687\n",
      "Epoch 4068, Loss: 2.2301524579524994, Final Batch Loss: 0.4757935106754303\n",
      "Epoch 4069, Loss: 2.3136348128318787, Final Batch Loss: 0.4284007251262665\n",
      "Epoch 4070, Loss: 2.229395180940628, Final Batch Loss: 0.49154675006866455\n",
      "Epoch 4071, Loss: 2.2319256961345673, Final Batch Loss: 0.47758716344833374\n",
      "Epoch 4072, Loss: 1.9778245389461517, Final Batch Loss: 0.3583661615848541\n",
      "Epoch 4073, Loss: 2.2231867611408234, Final Batch Loss: 0.5039401054382324\n",
      "Epoch 4074, Loss: 2.3141420781612396, Final Batch Loss: 0.5482078790664673\n",
      "Epoch 4075, Loss: 2.068297356367111, Final Batch Loss: 0.32900065183639526\n",
      "Epoch 4076, Loss: 2.3946765661239624, Final Batch Loss: 0.5292748808860779\n",
      "Epoch 4077, Loss: 2.274005740880966, Final Batch Loss: 0.44477811455726624\n",
      "Epoch 4078, Loss: 2.214420795440674, Final Batch Loss: 0.4196208715438843\n",
      "Epoch 4079, Loss: 2.4051779210567474, Final Batch Loss: 0.5240349769592285\n",
      "Epoch 4080, Loss: 2.1669542491436005, Final Batch Loss: 0.446077436208725\n",
      "Epoch 4081, Loss: 2.2922824323177338, Final Batch Loss: 0.486431360244751\n",
      "Epoch 4082, Loss: 2.240018218755722, Final Batch Loss: 0.4818273186683655\n",
      "Epoch 4083, Loss: 2.319951891899109, Final Batch Loss: 0.5380639433860779\n",
      "Epoch 4084, Loss: 2.2524295151233673, Final Batch Loss: 0.4250522553920746\n",
      "Epoch 4085, Loss: 2.325506627559662, Final Batch Loss: 0.6472877860069275\n",
      "Epoch 4086, Loss: 2.2228437066078186, Final Batch Loss: 0.36548179388046265\n",
      "Epoch 4087, Loss: 2.1841906905174255, Final Batch Loss: 0.40741246938705444\n",
      "Epoch 4088, Loss: 2.226195514202118, Final Batch Loss: 0.5112436413764954\n",
      "Epoch 4089, Loss: 2.170551687479019, Final Batch Loss: 0.4501594305038452\n",
      "Epoch 4090, Loss: 2.2088452875614166, Final Batch Loss: 0.29824474453926086\n",
      "Epoch 4091, Loss: 2.3306460082530975, Final Batch Loss: 0.5346217155456543\n",
      "Epoch 4092, Loss: 2.089329719543457, Final Batch Loss: 0.3880220353603363\n",
      "Epoch 4093, Loss: 2.2309325337409973, Final Batch Loss: 0.4224923253059387\n",
      "Epoch 4094, Loss: 2.283968299627304, Final Batch Loss: 0.4763853847980499\n",
      "Epoch 4095, Loss: 2.3509419560432434, Final Batch Loss: 0.46441030502319336\n",
      "Epoch 4096, Loss: 2.2283903062343597, Final Batch Loss: 0.41179320216178894\n",
      "Epoch 4097, Loss: 2.1846772134304047, Final Batch Loss: 0.3648698031902313\n",
      "Epoch 4098, Loss: 2.2959060668945312, Final Batch Loss: 0.5051621794700623\n",
      "Epoch 4099, Loss: 2.3989937007427216, Final Batch Loss: 0.4675428867340088\n",
      "Epoch 4100, Loss: 2.456949293613434, Final Batch Loss: 0.4865058660507202\n",
      "Epoch 4101, Loss: 2.1490345299243927, Final Batch Loss: 0.44605782628059387\n",
      "Epoch 4102, Loss: 2.2262998521327972, Final Batch Loss: 0.4015035033226013\n",
      "Epoch 4103, Loss: 2.282058000564575, Final Batch Loss: 0.43402624130249023\n",
      "Epoch 4104, Loss: 2.065777599811554, Final Batch Loss: 0.3304772973060608\n",
      "Epoch 4105, Loss: 2.4420105814933777, Final Batch Loss: 0.5894100666046143\n",
      "Epoch 4106, Loss: 2.306748628616333, Final Batch Loss: 0.45985686779022217\n",
      "Epoch 4107, Loss: 2.2852334678173065, Final Batch Loss: 0.5471227169036865\n",
      "Epoch 4108, Loss: 2.362779825925827, Final Batch Loss: 0.5067919492721558\n",
      "Epoch 4109, Loss: 2.3295031785964966, Final Batch Loss: 0.4705321788787842\n",
      "Epoch 4110, Loss: 2.1560277938842773, Final Batch Loss: 0.47099825739860535\n",
      "Epoch 4111, Loss: 2.1885247826576233, Final Batch Loss: 0.4305051565170288\n",
      "Epoch 4112, Loss: 2.3025297820568085, Final Batch Loss: 0.3766927123069763\n",
      "Epoch 4113, Loss: 2.147873282432556, Final Batch Loss: 0.5062208771705627\n",
      "Epoch 4114, Loss: 2.32004714012146, Final Batch Loss: 0.4436231553554535\n",
      "Epoch 4115, Loss: 2.135475277900696, Final Batch Loss: 0.40924420952796936\n",
      "Epoch 4116, Loss: 2.140150338411331, Final Batch Loss: 0.37289494276046753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4117, Loss: 2.1591956615448, Final Batch Loss: 0.40885353088378906\n",
      "Epoch 4118, Loss: 2.288115054368973, Final Batch Loss: 0.5911494493484497\n",
      "Epoch 4119, Loss: 2.290347695350647, Final Batch Loss: 0.44890111684799194\n",
      "Epoch 4120, Loss: 2.1671142578125, Final Batch Loss: 0.3578418493270874\n",
      "Epoch 4121, Loss: 2.266631096601486, Final Batch Loss: 0.5008161067962646\n",
      "Epoch 4122, Loss: 2.313786029815674, Final Batch Loss: 0.582041323184967\n",
      "Epoch 4123, Loss: 2.1557086408138275, Final Batch Loss: 0.5930883884429932\n",
      "Epoch 4124, Loss: 2.037174791097641, Final Batch Loss: 0.3733532726764679\n",
      "Epoch 4125, Loss: 2.3189477026462555, Final Batch Loss: 0.44355010986328125\n",
      "Epoch 4126, Loss: 2.3856445848941803, Final Batch Loss: 0.4216989576816559\n",
      "Epoch 4127, Loss: 2.134191483259201, Final Batch Loss: 0.4260559380054474\n",
      "Epoch 4128, Loss: 2.1010661721229553, Final Batch Loss: 0.4336298704147339\n",
      "Epoch 4129, Loss: 2.2688380777835846, Final Batch Loss: 0.4572540819644928\n",
      "Epoch 4130, Loss: 2.245378226041794, Final Batch Loss: 0.5367977023124695\n",
      "Epoch 4131, Loss: 2.1564361453056335, Final Batch Loss: 0.4605009853839874\n",
      "Epoch 4132, Loss: 2.25375235080719, Final Batch Loss: 0.33707934617996216\n",
      "Epoch 4133, Loss: 2.021656423807144, Final Batch Loss: 0.37424346804618835\n",
      "Epoch 4134, Loss: 2.3113405108451843, Final Batch Loss: 0.38372570276260376\n",
      "Epoch 4135, Loss: 2.2951582074165344, Final Batch Loss: 0.40463781356811523\n",
      "Epoch 4136, Loss: 2.005202502012253, Final Batch Loss: 0.35192570090293884\n",
      "Epoch 4137, Loss: 2.466899484395981, Final Batch Loss: 0.5250864028930664\n",
      "Epoch 4138, Loss: 2.164052963256836, Final Batch Loss: 0.4019070863723755\n",
      "Epoch 4139, Loss: 2.123790293931961, Final Batch Loss: 0.34913718700408936\n",
      "Epoch 4140, Loss: 2.086869090795517, Final Batch Loss: 0.43241605162620544\n",
      "Epoch 4141, Loss: 2.239654153585434, Final Batch Loss: 0.43558377027511597\n",
      "Epoch 4142, Loss: 2.1361683309078217, Final Batch Loss: 0.3619742691516876\n",
      "Epoch 4143, Loss: 2.267039179801941, Final Batch Loss: 0.5044545531272888\n",
      "Epoch 4144, Loss: 2.0880852341651917, Final Batch Loss: 0.4217453896999359\n",
      "Epoch 4145, Loss: 2.328608751296997, Final Batch Loss: 0.44881612062454224\n",
      "Epoch 4146, Loss: 2.1447417736053467, Final Batch Loss: 0.35056236386299133\n",
      "Epoch 4147, Loss: 2.2373811304569244, Final Batch Loss: 0.5040492415428162\n",
      "Epoch 4148, Loss: 2.14791277050972, Final Batch Loss: 0.38191911578178406\n",
      "Epoch 4149, Loss: 2.322067469358444, Final Batch Loss: 0.5521033406257629\n",
      "Epoch 4150, Loss: 2.2126539647579193, Final Batch Loss: 0.4576233923435211\n",
      "Epoch 4151, Loss: 2.0881120562553406, Final Batch Loss: 0.3055074214935303\n",
      "Epoch 4152, Loss: 2.2322021424770355, Final Batch Loss: 0.484817236661911\n",
      "Epoch 4153, Loss: 2.2354635894298553, Final Batch Loss: 0.3582106828689575\n",
      "Epoch 4154, Loss: 2.1775380671024323, Final Batch Loss: 0.45245614647865295\n",
      "Epoch 4155, Loss: 2.120529353618622, Final Batch Loss: 0.4450227916240692\n",
      "Epoch 4156, Loss: 2.125648468732834, Final Batch Loss: 0.3447493314743042\n",
      "Epoch 4157, Loss: 2.283177763223648, Final Batch Loss: 0.6168996691703796\n",
      "Epoch 4158, Loss: 2.233765661716461, Final Batch Loss: 0.33986103534698486\n",
      "Epoch 4159, Loss: 2.192560911178589, Final Batch Loss: 0.4568280577659607\n",
      "Epoch 4160, Loss: 2.3063578605651855, Final Batch Loss: 0.6173298358917236\n",
      "Epoch 4161, Loss: 2.099458396434784, Final Batch Loss: 0.5038771033287048\n",
      "Epoch 4162, Loss: 2.226849764585495, Final Batch Loss: 0.4379490315914154\n",
      "Epoch 4163, Loss: 2.5491560995578766, Final Batch Loss: 0.7226453423500061\n",
      "Epoch 4164, Loss: 2.2682363986968994, Final Batch Loss: 0.45208197832107544\n",
      "Epoch 4165, Loss: 2.1951351165771484, Final Batch Loss: 0.40180906653404236\n",
      "Epoch 4166, Loss: 2.16289421916008, Final Batch Loss: 0.33137911558151245\n",
      "Epoch 4167, Loss: 2.229950100183487, Final Batch Loss: 0.3744177520275116\n",
      "Epoch 4168, Loss: 2.1605066657066345, Final Batch Loss: 0.4737173616886139\n",
      "Epoch 4169, Loss: 2.2782223224639893, Final Batch Loss: 0.49537357687950134\n",
      "Epoch 4170, Loss: 2.088549882173538, Final Batch Loss: 0.41459423303604126\n",
      "Epoch 4171, Loss: 2.21347975730896, Final Batch Loss: 0.382581502199173\n",
      "Epoch 4172, Loss: 2.28846338391304, Final Batch Loss: 0.463638037443161\n",
      "Epoch 4173, Loss: 2.26174196600914, Final Batch Loss: 0.4071938693523407\n",
      "Epoch 4174, Loss: 2.2952322363853455, Final Batch Loss: 0.38677167892456055\n",
      "Epoch 4175, Loss: 2.24479940533638, Final Batch Loss: 0.47950732707977295\n",
      "Epoch 4176, Loss: 2.096757799386978, Final Batch Loss: 0.3766002953052521\n",
      "Epoch 4177, Loss: 2.2206418812274933, Final Batch Loss: 0.5302928686141968\n",
      "Epoch 4178, Loss: 2.3291033506393433, Final Batch Loss: 0.4440144896507263\n",
      "Epoch 4179, Loss: 1.9029337465763092, Final Batch Loss: 0.3946947157382965\n",
      "Epoch 4180, Loss: 2.1831221878528595, Final Batch Loss: 0.4809354543685913\n",
      "Epoch 4181, Loss: 2.2015670239925385, Final Batch Loss: 0.3817601203918457\n",
      "Epoch 4182, Loss: 2.1301524937152863, Final Batch Loss: 0.4230300486087799\n",
      "Epoch 4183, Loss: 2.130511909723282, Final Batch Loss: 0.2691630423069\n",
      "Epoch 4184, Loss: 2.157454639673233, Final Batch Loss: 0.3040483891963959\n",
      "Epoch 4185, Loss: 2.0666012167930603, Final Batch Loss: 0.33988890051841736\n",
      "Epoch 4186, Loss: 2.4280052185058594, Final Batch Loss: 0.4622863233089447\n",
      "Epoch 4187, Loss: 2.2621060013771057, Final Batch Loss: 0.5923418998718262\n",
      "Epoch 4188, Loss: 2.13830828666687, Final Batch Loss: 0.44286009669303894\n",
      "Epoch 4189, Loss: 2.342127174139023, Final Batch Loss: 0.49024394154548645\n",
      "Epoch 4190, Loss: 2.2833718061447144, Final Batch Loss: 0.5608114004135132\n",
      "Epoch 4191, Loss: 2.1939277052879333, Final Batch Loss: 0.4609656035900116\n",
      "Epoch 4192, Loss: 2.1406079530715942, Final Batch Loss: 0.40622469782829285\n",
      "Epoch 4193, Loss: 2.228252112865448, Final Batch Loss: 0.3926278054714203\n",
      "Epoch 4194, Loss: 2.352508991956711, Final Batch Loss: 0.3991829752922058\n",
      "Epoch 4195, Loss: 2.2259568572044373, Final Batch Loss: 0.4820173680782318\n",
      "Epoch 4196, Loss: 2.3400379717350006, Final Batch Loss: 0.3501744866371155\n",
      "Epoch 4197, Loss: 2.0817941427230835, Final Batch Loss: 0.38168764114379883\n",
      "Epoch 4198, Loss: 2.3100019097328186, Final Batch Loss: 0.44233715534210205\n",
      "Epoch 4199, Loss: 2.1188439428806305, Final Batch Loss: 0.33090174198150635\n",
      "Epoch 4200, Loss: 2.28735414147377, Final Batch Loss: 0.4477187693119049\n",
      "Epoch 4201, Loss: 2.0480473339557648, Final Batch Loss: 0.4683021903038025\n",
      "Epoch 4202, Loss: 2.250021278858185, Final Batch Loss: 0.511965811252594\n",
      "Epoch 4203, Loss: 2.2699167728424072, Final Batch Loss: 0.47704657912254333\n",
      "Epoch 4204, Loss: 2.174075275659561, Final Batch Loss: 0.5021852254867554\n",
      "Epoch 4205, Loss: 2.211156338453293, Final Batch Loss: 0.43601319193840027\n",
      "Epoch 4206, Loss: 2.224321663379669, Final Batch Loss: 0.47266870737075806\n",
      "Epoch 4207, Loss: 2.092491328716278, Final Batch Loss: 0.3920673727989197\n",
      "Epoch 4208, Loss: 2.21015602350235, Final Batch Loss: 0.48164957761764526\n",
      "Epoch 4209, Loss: 2.060695558786392, Final Batch Loss: 0.41340288519859314\n",
      "Epoch 4210, Loss: 2.2404740154743195, Final Batch Loss: 0.47735926508903503\n",
      "Epoch 4211, Loss: 2.2444728016853333, Final Batch Loss: 0.459980309009552\n",
      "Epoch 4212, Loss: 2.2634979486465454, Final Batch Loss: 0.4106062650680542\n",
      "Epoch 4213, Loss: 2.2012729346752167, Final Batch Loss: 0.3911958932876587\n",
      "Epoch 4214, Loss: 2.439122796058655, Final Batch Loss: 0.5813690423965454\n",
      "Epoch 4215, Loss: 2.3927227556705475, Final Batch Loss: 0.6310610771179199\n",
      "Epoch 4216, Loss: 2.2923927903175354, Final Batch Loss: 0.5205997228622437\n",
      "Epoch 4217, Loss: 2.209591120481491, Final Batch Loss: 0.40012532472610474\n",
      "Epoch 4218, Loss: 2.2657274901866913, Final Batch Loss: 0.5132656693458557\n",
      "Epoch 4219, Loss: 2.0389035046100616, Final Batch Loss: 0.43739423155784607\n",
      "Epoch 4220, Loss: 2.2163795828819275, Final Batch Loss: 0.5370969772338867\n",
      "Epoch 4221, Loss: 2.247053861618042, Final Batch Loss: 0.45610615611076355\n",
      "Epoch 4222, Loss: 2.0862914323806763, Final Batch Loss: 0.35846036672592163\n",
      "Epoch 4223, Loss: 2.0688127279281616, Final Batch Loss: 0.47955062985420227\n",
      "Epoch 4224, Loss: 2.034082531929016, Final Batch Loss: 0.5403693318367004\n",
      "Epoch 4225, Loss: 2.1848367750644684, Final Batch Loss: 0.49600061774253845\n",
      "Epoch 4226, Loss: 2.0679933726787567, Final Batch Loss: 0.4381561577320099\n",
      "Epoch 4227, Loss: 2.1891560554504395, Final Batch Loss: 0.3742922246456146\n",
      "Epoch 4228, Loss: 2.280813068151474, Final Batch Loss: 0.4268236458301544\n",
      "Epoch 4229, Loss: 2.182430922985077, Final Batch Loss: 0.4181886315345764\n",
      "Epoch 4230, Loss: 2.4218410551548004, Final Batch Loss: 0.5378568172454834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4231, Loss: 2.1553343534469604, Final Batch Loss: 0.3577383756637573\n",
      "Epoch 4232, Loss: 2.3455311954021454, Final Batch Loss: 0.4782988429069519\n",
      "Epoch 4233, Loss: 2.287470668554306, Final Batch Loss: 0.5361570119857788\n",
      "Epoch 4234, Loss: 2.2837773859500885, Final Batch Loss: 0.45497557520866394\n",
      "Epoch 4235, Loss: 2.113267868757248, Final Batch Loss: 0.4316450357437134\n",
      "Epoch 4236, Loss: 2.223747134208679, Final Batch Loss: 0.39833828806877136\n",
      "Epoch 4237, Loss: 2.2325193285942078, Final Batch Loss: 0.4951140880584717\n",
      "Epoch 4238, Loss: 2.177375555038452, Final Batch Loss: 0.4167727828025818\n",
      "Epoch 4239, Loss: 2.465458929538727, Final Batch Loss: 0.7249019145965576\n",
      "Epoch 4240, Loss: 2.2580400705337524, Final Batch Loss: 0.5066447854042053\n",
      "Epoch 4241, Loss: 2.316037744283676, Final Batch Loss: 0.5136368274688721\n",
      "Epoch 4242, Loss: 2.1678736209869385, Final Batch Loss: 0.4844802916049957\n",
      "Epoch 4243, Loss: 2.236083000898361, Final Batch Loss: 0.43565353751182556\n",
      "Epoch 4244, Loss: 2.140045642852783, Final Batch Loss: 0.4301447868347168\n",
      "Epoch 4245, Loss: 2.075505495071411, Final Batch Loss: 0.3661254942417145\n",
      "Epoch 4246, Loss: 2.3312129378318787, Final Batch Loss: 0.5092494487762451\n",
      "Epoch 4247, Loss: 2.1175100803375244, Final Batch Loss: 0.27830997109413147\n",
      "Epoch 4248, Loss: 2.2875219583511353, Final Batch Loss: 0.31976935267448425\n",
      "Epoch 4249, Loss: 2.1507613360881805, Final Batch Loss: 0.38359129428863525\n",
      "Epoch 4250, Loss: 2.3036111295223236, Final Batch Loss: 0.6030784249305725\n",
      "Epoch 4251, Loss: 2.2805680632591248, Final Batch Loss: 0.44807422161102295\n",
      "Epoch 4252, Loss: 2.1689655780792236, Final Batch Loss: 0.4946649372577667\n",
      "Epoch 4253, Loss: 2.129096895456314, Final Batch Loss: 0.5264714956283569\n",
      "Epoch 4254, Loss: 2.138357847929001, Final Batch Loss: 0.4570552408695221\n",
      "Epoch 4255, Loss: 2.0854653120040894, Final Batch Loss: 0.4380151927471161\n",
      "Epoch 4256, Loss: 2.5586741268634796, Final Batch Loss: 0.43163856863975525\n",
      "Epoch 4257, Loss: 2.2046567797660828, Final Batch Loss: 0.4809531271457672\n",
      "Epoch 4258, Loss: 2.123758763074875, Final Batch Loss: 0.384597510099411\n",
      "Epoch 4259, Loss: 2.0885433852672577, Final Batch Loss: 0.3465730845928192\n",
      "Epoch 4260, Loss: 2.3450485169887543, Final Batch Loss: 0.3724988102912903\n",
      "Epoch 4261, Loss: 2.0697831213474274, Final Batch Loss: 0.5035524964332581\n",
      "Epoch 4262, Loss: 2.131448358297348, Final Batch Loss: 0.31347399950027466\n",
      "Epoch 4263, Loss: 2.1896174252033234, Final Batch Loss: 0.4576132893562317\n",
      "Epoch 4264, Loss: 2.032218247652054, Final Batch Loss: 0.3776356279850006\n",
      "Epoch 4265, Loss: 2.2544108033180237, Final Batch Loss: 0.4739983379840851\n",
      "Epoch 4266, Loss: 2.2087318301200867, Final Batch Loss: 0.408963143825531\n",
      "Epoch 4267, Loss: 2.2448069155216217, Final Batch Loss: 0.45727401971817017\n",
      "Epoch 4268, Loss: 2.2386436760425568, Final Batch Loss: 0.387807160615921\n",
      "Epoch 4269, Loss: 2.1627940833568573, Final Batch Loss: 0.40868496894836426\n",
      "Epoch 4270, Loss: 2.1413240134716034, Final Batch Loss: 0.43406373262405396\n",
      "Epoch 4271, Loss: 2.171055018901825, Final Batch Loss: 0.3902604579925537\n",
      "Epoch 4272, Loss: 2.23781481385231, Final Batch Loss: 0.4836929440498352\n",
      "Epoch 4273, Loss: 2.157399743795395, Final Batch Loss: 0.4661007523536682\n",
      "Epoch 4274, Loss: 2.1058535873889923, Final Batch Loss: 0.4882989227771759\n",
      "Epoch 4275, Loss: 2.072807252407074, Final Batch Loss: 0.4508249759674072\n",
      "Epoch 4276, Loss: 2.2092783451080322, Final Batch Loss: 0.3536682724952698\n",
      "Epoch 4277, Loss: 2.220480591058731, Final Batch Loss: 0.4873969852924347\n",
      "Epoch 4278, Loss: 2.352582722902298, Final Batch Loss: 0.47192108631134033\n",
      "Epoch 4279, Loss: 2.1902403831481934, Final Batch Loss: 0.37735170125961304\n",
      "Epoch 4280, Loss: 2.231632888317108, Final Batch Loss: 0.43364524841308594\n",
      "Epoch 4281, Loss: 2.195605993270874, Final Batch Loss: 0.5236521363258362\n",
      "Epoch 4282, Loss: 2.3250721096992493, Final Batch Loss: 0.5981839299201965\n",
      "Epoch 4283, Loss: 2.1808811724185944, Final Batch Loss: 0.39969831705093384\n",
      "Epoch 4284, Loss: 2.131255805492401, Final Batch Loss: 0.43429306149482727\n",
      "Epoch 4285, Loss: 2.205282211303711, Final Batch Loss: 0.3889618515968323\n",
      "Epoch 4286, Loss: 2.1986164450645447, Final Batch Loss: 0.4798203706741333\n",
      "Epoch 4287, Loss: 2.1609192192554474, Final Batch Loss: 0.5808596611022949\n",
      "Epoch 4288, Loss: 2.136383593082428, Final Batch Loss: 0.35973799228668213\n",
      "Epoch 4289, Loss: 2.230653315782547, Final Batch Loss: 0.5290839076042175\n",
      "Epoch 4290, Loss: 2.2028779089450836, Final Batch Loss: 0.5260710716247559\n",
      "Epoch 4291, Loss: 2.26068452000618, Final Batch Loss: 0.6242982745170593\n",
      "Epoch 4292, Loss: 2.320386618375778, Final Batch Loss: 0.42351049184799194\n",
      "Epoch 4293, Loss: 2.265291690826416, Final Batch Loss: 0.5549724698066711\n",
      "Epoch 4294, Loss: 2.2684200406074524, Final Batch Loss: 0.520980715751648\n",
      "Epoch 4295, Loss: 1.99188432097435, Final Batch Loss: 0.4207715094089508\n",
      "Epoch 4296, Loss: 2.0398964881896973, Final Batch Loss: 0.3857332766056061\n",
      "Epoch 4297, Loss: 2.2586249113082886, Final Batch Loss: 0.4064555764198303\n",
      "Epoch 4298, Loss: 2.2826686799526215, Final Batch Loss: 0.27975955605506897\n",
      "Epoch 4299, Loss: 2.3136203587055206, Final Batch Loss: 0.375941663980484\n",
      "Epoch 4300, Loss: 2.1486067175865173, Final Batch Loss: 0.4535771906375885\n",
      "Epoch 4301, Loss: 2.1799283027648926, Final Batch Loss: 0.4554940462112427\n",
      "Epoch 4302, Loss: 2.035114586353302, Final Batch Loss: 0.34373918175697327\n",
      "Epoch 4303, Loss: 2.2887847125530243, Final Batch Loss: 0.41846969723701477\n",
      "Epoch 4304, Loss: 2.0678336024284363, Final Batch Loss: 0.43577906489372253\n",
      "Epoch 4305, Loss: 2.229896754026413, Final Batch Loss: 0.4253615736961365\n",
      "Epoch 4306, Loss: 2.1099510192871094, Final Batch Loss: 0.26481565833091736\n",
      "Epoch 4307, Loss: 2.170081913471222, Final Batch Loss: 0.45234179496765137\n",
      "Epoch 4308, Loss: 2.13493949174881, Final Batch Loss: 0.4253048896789551\n",
      "Epoch 4309, Loss: 2.191662847995758, Final Batch Loss: 0.4850122332572937\n",
      "Epoch 4310, Loss: 2.2040880024433136, Final Batch Loss: 0.3929629325866699\n",
      "Epoch 4311, Loss: 2.240945518016815, Final Batch Loss: 0.3799886107444763\n",
      "Epoch 4312, Loss: 2.085106462240219, Final Batch Loss: 0.365102082490921\n",
      "Epoch 4313, Loss: 2.17099392414093, Final Batch Loss: 0.4763495624065399\n",
      "Epoch 4314, Loss: 2.2610153555870056, Final Batch Loss: 0.5474350452423096\n",
      "Epoch 4315, Loss: 2.1328451335430145, Final Batch Loss: 0.4484809637069702\n",
      "Epoch 4316, Loss: 2.180873155593872, Final Batch Loss: 0.37659668922424316\n",
      "Epoch 4317, Loss: 2.165337771177292, Final Batch Loss: 0.4701131284236908\n",
      "Epoch 4318, Loss: 2.2417027354240417, Final Batch Loss: 0.4377363622188568\n",
      "Epoch 4319, Loss: 2.2118498980998993, Final Batch Loss: 0.5221232175827026\n",
      "Epoch 4320, Loss: 2.2282165586948395, Final Batch Loss: 0.3994860351085663\n",
      "Epoch 4321, Loss: 2.229706287384033, Final Batch Loss: 0.4849227964878082\n",
      "Epoch 4322, Loss: 2.2088067829608917, Final Batch Loss: 0.4328395426273346\n",
      "Epoch 4323, Loss: 2.4157320261001587, Final Batch Loss: 0.5597689151763916\n",
      "Epoch 4324, Loss: 2.1968033015727997, Final Batch Loss: 0.4448246359825134\n",
      "Epoch 4325, Loss: 2.2829297482967377, Final Batch Loss: 0.45342737436294556\n",
      "Epoch 4326, Loss: 2.3541984260082245, Final Batch Loss: 0.374738484621048\n",
      "Epoch 4327, Loss: 2.2108007073402405, Final Batch Loss: 0.4790092706680298\n",
      "Epoch 4328, Loss: 2.0476055443286896, Final Batch Loss: 0.4223378598690033\n",
      "Epoch 4329, Loss: 2.184747099876404, Final Batch Loss: 0.42347252368927\n",
      "Epoch 4330, Loss: 2.224977672100067, Final Batch Loss: 0.5079482197761536\n",
      "Epoch 4331, Loss: 2.250588119029999, Final Batch Loss: 0.3998788595199585\n",
      "Epoch 4332, Loss: 1.9945897161960602, Final Batch Loss: 0.3256910741329193\n",
      "Epoch 4333, Loss: 2.1968888342380524, Final Batch Loss: 0.40793144702911377\n",
      "Epoch 4334, Loss: 2.1681279838085175, Final Batch Loss: 0.39525195956230164\n",
      "Epoch 4335, Loss: 2.2516957819461823, Final Batch Loss: 0.5347055792808533\n",
      "Epoch 4336, Loss: 2.2137350738048553, Final Batch Loss: 0.4169882535934448\n",
      "Epoch 4337, Loss: 2.15811750292778, Final Batch Loss: 0.3283631503582001\n",
      "Epoch 4338, Loss: 2.251985490322113, Final Batch Loss: 0.36479759216308594\n",
      "Epoch 4339, Loss: 2.1084257066249847, Final Batch Loss: 0.3449317514896393\n",
      "Epoch 4340, Loss: 2.329829305410385, Final Batch Loss: 0.39595845341682434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4341, Loss: 2.152196615934372, Final Batch Loss: 0.4977090358734131\n",
      "Epoch 4342, Loss: 2.0224483609199524, Final Batch Loss: 0.37852177023887634\n",
      "Epoch 4343, Loss: 2.1963113248348236, Final Batch Loss: 0.4478171467781067\n",
      "Epoch 4344, Loss: 2.3172891438007355, Final Batch Loss: 0.5188771486282349\n",
      "Epoch 4345, Loss: 2.1236684024333954, Final Batch Loss: 0.3634888827800751\n",
      "Epoch 4346, Loss: 2.0601476430892944, Final Batch Loss: 0.3838635981082916\n",
      "Epoch 4347, Loss: 2.26998108625412, Final Batch Loss: 0.5153547525405884\n",
      "Epoch 4348, Loss: 2.1237141489982605, Final Batch Loss: 0.3815685510635376\n",
      "Epoch 4349, Loss: 2.2427503168582916, Final Batch Loss: 0.485371857881546\n",
      "Epoch 4350, Loss: 2.2032767236232758, Final Batch Loss: 0.5259885787963867\n",
      "Epoch 4351, Loss: 2.2130158841609955, Final Batch Loss: 0.4359830319881439\n",
      "Epoch 4352, Loss: 2.09768208861351, Final Batch Loss: 0.36585313081741333\n",
      "Epoch 4353, Loss: 2.139622837305069, Final Batch Loss: 0.37701088190078735\n",
      "Epoch 4354, Loss: 2.3577701449394226, Final Batch Loss: 0.5582488775253296\n",
      "Epoch 4355, Loss: 2.141127973794937, Final Batch Loss: 0.38673871755599976\n",
      "Epoch 4356, Loss: 2.1532255709171295, Final Batch Loss: 0.511194109916687\n",
      "Epoch 4357, Loss: 2.1779654026031494, Final Batch Loss: 0.507473349571228\n",
      "Epoch 4358, Loss: 2.348318725824356, Final Batch Loss: 0.692202627658844\n",
      "Epoch 4359, Loss: 2.1616720855236053, Final Batch Loss: 0.4117146134376526\n",
      "Epoch 4360, Loss: 2.135052502155304, Final Batch Loss: 0.43151259422302246\n",
      "Epoch 4361, Loss: 2.2220600247383118, Final Batch Loss: 0.40485334396362305\n",
      "Epoch 4362, Loss: 2.2267025113105774, Final Batch Loss: 0.4241570830345154\n",
      "Epoch 4363, Loss: 2.09643092751503, Final Batch Loss: 0.42673423886299133\n",
      "Epoch 4364, Loss: 2.0474867820739746, Final Batch Loss: 0.36205586791038513\n",
      "Epoch 4365, Loss: 2.3724514544010162, Final Batch Loss: 0.540819525718689\n",
      "Epoch 4366, Loss: 2.236880600452423, Final Batch Loss: 0.5550816655158997\n",
      "Epoch 4367, Loss: 2.0916357040405273, Final Batch Loss: 0.3834165930747986\n",
      "Epoch 4368, Loss: 2.188180088996887, Final Batch Loss: 0.3839128017425537\n",
      "Epoch 4369, Loss: 2.225585013628006, Final Batch Loss: 0.4433998465538025\n",
      "Epoch 4370, Loss: 2.4317163825035095, Final Batch Loss: 0.581439733505249\n",
      "Epoch 4371, Loss: 2.201170802116394, Final Batch Loss: 0.439980685710907\n",
      "Epoch 4372, Loss: 2.2152572572231293, Final Batch Loss: 0.479116827249527\n",
      "Epoch 4373, Loss: 2.057461231946945, Final Batch Loss: 0.4023405909538269\n",
      "Epoch 4374, Loss: 1.9095702171325684, Final Batch Loss: 0.3461352586746216\n",
      "Epoch 4375, Loss: 2.176101267337799, Final Batch Loss: 0.4664512276649475\n",
      "Epoch 4376, Loss: 2.112566292285919, Final Batch Loss: 0.4340570867061615\n",
      "Epoch 4377, Loss: 2.068437099456787, Final Batch Loss: 0.4601553976535797\n",
      "Epoch 4378, Loss: 2.229294627904892, Final Batch Loss: 0.47459548711776733\n",
      "Epoch 4379, Loss: 2.23436838388443, Final Batch Loss: 0.4570426642894745\n",
      "Epoch 4380, Loss: 2.2371853590011597, Final Batch Loss: 0.5029323101043701\n",
      "Epoch 4381, Loss: 2.146495372056961, Final Batch Loss: 0.3444758653640747\n",
      "Epoch 4382, Loss: 2.192641109228134, Final Batch Loss: 0.40747085213661194\n",
      "Epoch 4383, Loss: 2.1293269991874695, Final Batch Loss: 0.3962344825267792\n",
      "Epoch 4384, Loss: 2.076282650232315, Final Batch Loss: 0.502034068107605\n",
      "Epoch 4385, Loss: 2.1735134720802307, Final Batch Loss: 0.45841163396835327\n",
      "Epoch 4386, Loss: 2.3096189200878143, Final Batch Loss: 0.4892468750476837\n",
      "Epoch 4387, Loss: 2.08818319439888, Final Batch Loss: 0.4792039096355438\n",
      "Epoch 4388, Loss: 2.32081800699234, Final Batch Loss: 0.5818057060241699\n",
      "Epoch 4389, Loss: 2.142859697341919, Final Batch Loss: 0.3563237190246582\n",
      "Epoch 4390, Loss: 2.1615763902664185, Final Batch Loss: 0.4692898392677307\n",
      "Epoch 4391, Loss: 2.0420904457569122, Final Batch Loss: 0.4393828809261322\n",
      "Epoch 4392, Loss: 2.109964072704315, Final Batch Loss: 0.4169415831565857\n",
      "Epoch 4393, Loss: 2.164265424013138, Final Batch Loss: 0.5225210785865784\n",
      "Epoch 4394, Loss: 2.1511597633361816, Final Batch Loss: 0.4494856595993042\n",
      "Epoch 4395, Loss: 2.1662109792232513, Final Batch Loss: 0.3750111758708954\n",
      "Epoch 4396, Loss: 2.092335134744644, Final Batch Loss: 0.4264799952507019\n",
      "Epoch 4397, Loss: 2.266108810901642, Final Batch Loss: 0.4642203450202942\n",
      "Epoch 4398, Loss: 2.2188603281974792, Final Batch Loss: 0.44041088223457336\n",
      "Epoch 4399, Loss: 2.3837132155895233, Final Batch Loss: 0.5904067754745483\n",
      "Epoch 4400, Loss: 2.3697759807109833, Final Batch Loss: 0.49592822790145874\n",
      "Epoch 4401, Loss: 2.307327151298523, Final Batch Loss: 0.3975793123245239\n",
      "Epoch 4402, Loss: 2.15536305308342, Final Batch Loss: 0.40765467286109924\n",
      "Epoch 4403, Loss: 2.186457633972168, Final Batch Loss: 0.403861939907074\n",
      "Epoch 4404, Loss: 2.0968023538589478, Final Batch Loss: 0.36248475313186646\n",
      "Epoch 4405, Loss: 2.342741698026657, Final Batch Loss: 0.461129754781723\n",
      "Epoch 4406, Loss: 2.2250477373600006, Final Batch Loss: 0.4027663767337799\n",
      "Epoch 4407, Loss: 2.207427531480789, Final Batch Loss: 0.40055137872695923\n",
      "Epoch 4408, Loss: 2.1344591677188873, Final Batch Loss: 0.4186221659183502\n",
      "Epoch 4409, Loss: 2.2337117195129395, Final Batch Loss: 0.4255909025669098\n",
      "Epoch 4410, Loss: 2.287324398756027, Final Batch Loss: 0.3971172571182251\n",
      "Epoch 4411, Loss: 2.159583956003189, Final Batch Loss: 0.5427329540252686\n",
      "Epoch 4412, Loss: 2.0482268631458282, Final Batch Loss: 0.43992212414741516\n",
      "Epoch 4413, Loss: 2.082226723432541, Final Batch Loss: 0.4704580307006836\n",
      "Epoch 4414, Loss: 2.0754461586475372, Final Batch Loss: 0.4154547452926636\n",
      "Epoch 4415, Loss: 2.05625581741333, Final Batch Loss: 0.36572569608688354\n",
      "Epoch 4416, Loss: 2.1976479589939117, Final Batch Loss: 0.4642919600009918\n",
      "Epoch 4417, Loss: 1.8967996835708618, Final Batch Loss: 0.31025296449661255\n",
      "Epoch 4418, Loss: 2.3105793595314026, Final Batch Loss: 0.47586292028427124\n",
      "Epoch 4419, Loss: 2.2762908935546875, Final Batch Loss: 0.5209226012229919\n",
      "Epoch 4420, Loss: 2.1514502465724945, Final Batch Loss: 0.3233567178249359\n",
      "Epoch 4421, Loss: 2.127894341945648, Final Batch Loss: 0.4080139696598053\n",
      "Epoch 4422, Loss: 2.1751669347286224, Final Batch Loss: 0.5824418663978577\n",
      "Epoch 4423, Loss: 2.2158551812171936, Final Batch Loss: 0.4040933847427368\n",
      "Epoch 4424, Loss: 2.0860863626003265, Final Batch Loss: 0.3946719467639923\n",
      "Epoch 4425, Loss: 2.223369926214218, Final Batch Loss: 0.41270187497138977\n",
      "Epoch 4426, Loss: 2.2078972458839417, Final Batch Loss: 0.6271964311599731\n",
      "Epoch 4427, Loss: 2.2291585505008698, Final Batch Loss: 0.38554200530052185\n",
      "Epoch 4428, Loss: 2.170755386352539, Final Batch Loss: 0.4556206464767456\n",
      "Epoch 4429, Loss: 2.347123235464096, Final Batch Loss: 0.49248525500297546\n",
      "Epoch 4430, Loss: 2.0430107414722443, Final Batch Loss: 0.34048599004745483\n",
      "Epoch 4431, Loss: 2.1863530576229095, Final Batch Loss: 0.5327980518341064\n",
      "Epoch 4432, Loss: 1.983154684305191, Final Batch Loss: 0.40606218576431274\n",
      "Epoch 4433, Loss: 2.2818191945552826, Final Batch Loss: 0.43070343136787415\n",
      "Epoch 4434, Loss: 2.248685985803604, Final Batch Loss: 0.3527701199054718\n",
      "Epoch 4435, Loss: 2.4519867300987244, Final Batch Loss: 0.5424177050590515\n",
      "Epoch 4436, Loss: 2.1484603881835938, Final Batch Loss: 0.4073808789253235\n",
      "Epoch 4437, Loss: 2.1534612476825714, Final Batch Loss: 0.5062949061393738\n",
      "Epoch 4438, Loss: 2.1625376641750336, Final Batch Loss: 0.4030314087867737\n",
      "Epoch 4439, Loss: 2.137672185897827, Final Batch Loss: 0.505069375038147\n",
      "Epoch 4440, Loss: 2.4319347143173218, Final Batch Loss: 0.5209182500839233\n",
      "Epoch 4441, Loss: 2.3025659322738647, Final Batch Loss: 0.4398733973503113\n",
      "Epoch 4442, Loss: 2.130608171224594, Final Batch Loss: 0.4751724302768707\n",
      "Epoch 4443, Loss: 2.2165344655513763, Final Batch Loss: 0.5019224882125854\n",
      "Epoch 4444, Loss: 2.1506849229335785, Final Batch Loss: 0.42101964354515076\n",
      "Epoch 4445, Loss: 2.179238349199295, Final Batch Loss: 0.48660266399383545\n",
      "Epoch 4446, Loss: 2.159848779439926, Final Batch Loss: 0.39642056822776794\n",
      "Epoch 4447, Loss: 2.2458363473415375, Final Batch Loss: 0.6203104257583618\n",
      "Epoch 4448, Loss: 2.1832184493541718, Final Batch Loss: 0.4761745035648346\n",
      "Epoch 4449, Loss: 2.4273220896720886, Final Batch Loss: 0.5080650448799133\n",
      "Epoch 4450, Loss: 2.136982947587967, Final Batch Loss: 0.40934062004089355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4451, Loss: 2.0983318090438843, Final Batch Loss: 0.37815535068511963\n",
      "Epoch 4452, Loss: 2.1075579226017, Final Batch Loss: 0.4056564271450043\n",
      "Epoch 4453, Loss: 2.111655831336975, Final Batch Loss: 0.3324441909790039\n",
      "Epoch 4454, Loss: 2.2056251764297485, Final Batch Loss: 0.5237995982170105\n",
      "Epoch 4455, Loss: 1.9880897998809814, Final Batch Loss: 0.2929728925228119\n",
      "Epoch 4456, Loss: 2.0531961917877197, Final Batch Loss: 0.4528500735759735\n",
      "Epoch 4457, Loss: 2.2339885532855988, Final Batch Loss: 0.4978046715259552\n",
      "Epoch 4458, Loss: 2.08158415555954, Final Batch Loss: 0.3931923508644104\n",
      "Epoch 4459, Loss: 2.16865274310112, Final Batch Loss: 0.4379329979419708\n",
      "Epoch 4460, Loss: 2.157636284828186, Final Batch Loss: 0.4232673645019531\n",
      "Epoch 4461, Loss: 2.1879113614559174, Final Batch Loss: 0.40377599000930786\n",
      "Epoch 4462, Loss: 2.022456079721451, Final Batch Loss: 0.36717796325683594\n",
      "Epoch 4463, Loss: 2.192520648241043, Final Batch Loss: 0.5645829439163208\n",
      "Epoch 4464, Loss: 2.513358175754547, Final Batch Loss: 0.6506603956222534\n",
      "Epoch 4465, Loss: 2.167142689228058, Final Batch Loss: 0.45647376775741577\n",
      "Epoch 4466, Loss: 2.3943129777908325, Final Batch Loss: 0.43339887261390686\n",
      "Epoch 4467, Loss: 2.4345278441905975, Final Batch Loss: 0.454313725233078\n",
      "Epoch 4468, Loss: 2.157954305410385, Final Batch Loss: 0.44797036051750183\n",
      "Epoch 4469, Loss: 2.2624522745609283, Final Batch Loss: 0.3858380913734436\n",
      "Epoch 4470, Loss: 2.1919766068458557, Final Batch Loss: 0.5027099251747131\n",
      "Epoch 4471, Loss: 2.28057262301445, Final Batch Loss: 0.44701218605041504\n",
      "Epoch 4472, Loss: 2.305226683616638, Final Batch Loss: 0.4957883059978485\n",
      "Epoch 4473, Loss: 2.290838599205017, Final Batch Loss: 0.49816006422042847\n",
      "Epoch 4474, Loss: 2.231587290763855, Final Batch Loss: 0.34970933198928833\n",
      "Epoch 4475, Loss: 2.169075459241867, Final Batch Loss: 0.4319427013397217\n",
      "Epoch 4476, Loss: 2.168942868709564, Final Batch Loss: 0.448927104473114\n",
      "Epoch 4477, Loss: 2.255377382040024, Final Batch Loss: 0.5705541968345642\n",
      "Epoch 4478, Loss: 2.1768008172512054, Final Batch Loss: 0.47082293033599854\n",
      "Epoch 4479, Loss: 2.3045437037944794, Final Batch Loss: 0.4217120110988617\n",
      "Epoch 4480, Loss: 2.0173589885234833, Final Batch Loss: 0.27428287267684937\n",
      "Epoch 4481, Loss: 2.1082019805908203, Final Batch Loss: 0.392894446849823\n",
      "Epoch 4482, Loss: 2.0951468646526337, Final Batch Loss: 0.3691098392009735\n",
      "Epoch 4483, Loss: 2.061935633420944, Final Batch Loss: 0.42727869749069214\n",
      "Epoch 4484, Loss: 2.268507272005081, Final Batch Loss: 0.4390453100204468\n",
      "Epoch 4485, Loss: 2.1223025023937225, Final Batch Loss: 0.45896926522254944\n",
      "Epoch 4486, Loss: 2.0190617442131042, Final Batch Loss: 0.4445277750492096\n",
      "Epoch 4487, Loss: 2.3045504987239838, Final Batch Loss: 0.43074628710746765\n",
      "Epoch 4488, Loss: 2.0466299653053284, Final Batch Loss: 0.3347918391227722\n",
      "Epoch 4489, Loss: 2.0404773354530334, Final Batch Loss: 0.3760213851928711\n",
      "Epoch 4490, Loss: 2.165675848722458, Final Batch Loss: 0.3225926458835602\n",
      "Epoch 4491, Loss: 2.1353630423545837, Final Batch Loss: 0.4435241222381592\n",
      "Epoch 4492, Loss: 1.9899725914001465, Final Batch Loss: 0.3751493990421295\n",
      "Epoch 4493, Loss: 2.092791050672531, Final Batch Loss: 0.42312002182006836\n",
      "Epoch 4494, Loss: 2.0345596969127655, Final Batch Loss: 0.4475313425064087\n",
      "Epoch 4495, Loss: 2.0059715807437897, Final Batch Loss: 0.36078718304634094\n",
      "Epoch 4496, Loss: 2.1197084486484528, Final Batch Loss: 0.38528093695640564\n",
      "Epoch 4497, Loss: 2.0768849551677704, Final Batch Loss: 0.4521501660346985\n",
      "Epoch 4498, Loss: 2.2092438638210297, Final Batch Loss: 0.3426666855812073\n",
      "Epoch 4499, Loss: 2.146650642156601, Final Batch Loss: 0.3843423128128052\n",
      "Epoch 4500, Loss: 2.093431979417801, Final Batch Loss: 0.3633461892604828\n",
      "Epoch 4501, Loss: 2.183403432369232, Final Batch Loss: 0.33551597595214844\n",
      "Epoch 4502, Loss: 2.268394649028778, Final Batch Loss: 0.2875497043132782\n",
      "Epoch 4503, Loss: 2.243864953517914, Final Batch Loss: 0.5537712574005127\n",
      "Epoch 4504, Loss: 2.1584510505199432, Final Batch Loss: 0.42413923144340515\n",
      "Epoch 4505, Loss: 1.9172767400741577, Final Batch Loss: 0.3793736696243286\n",
      "Epoch 4506, Loss: 2.308637857437134, Final Batch Loss: 0.3965925872325897\n",
      "Epoch 4507, Loss: 2.272374391555786, Final Batch Loss: 0.4102490246295929\n",
      "Epoch 4508, Loss: 2.2440766394138336, Final Batch Loss: 0.5008561611175537\n",
      "Epoch 4509, Loss: 2.190644294023514, Final Batch Loss: 0.4923746585845947\n",
      "Epoch 4510, Loss: 2.178126960992813, Final Batch Loss: 0.4738590717315674\n",
      "Epoch 4511, Loss: 2.515513390302658, Final Batch Loss: 0.47699570655822754\n",
      "Epoch 4512, Loss: 2.1352876722812653, Final Batch Loss: 0.49404144287109375\n",
      "Epoch 4513, Loss: 2.194553941488266, Final Batch Loss: 0.42078912258148193\n",
      "Epoch 4514, Loss: 2.140162169933319, Final Batch Loss: 0.3811572194099426\n",
      "Epoch 4515, Loss: 2.073260247707367, Final Batch Loss: 0.29954856634140015\n",
      "Epoch 4516, Loss: 2.18838831782341, Final Batch Loss: 0.5560842156410217\n",
      "Epoch 4517, Loss: 2.211405485868454, Final Batch Loss: 0.4257950484752655\n",
      "Epoch 4518, Loss: 2.1122040152549744, Final Batch Loss: 0.44542914628982544\n",
      "Epoch 4519, Loss: 2.071754425764084, Final Batch Loss: 0.45471376180648804\n",
      "Epoch 4520, Loss: 2.0519484877586365, Final Batch Loss: 0.2950035631656647\n",
      "Epoch 4521, Loss: 2.350163608789444, Final Batch Loss: 0.5621212124824524\n",
      "Epoch 4522, Loss: 2.1959662437438965, Final Batch Loss: 0.5330866575241089\n",
      "Epoch 4523, Loss: 2.060556560754776, Final Batch Loss: 0.39766228199005127\n",
      "Epoch 4524, Loss: 2.3982776403427124, Final Batch Loss: 0.43421727418899536\n",
      "Epoch 4525, Loss: 2.194533586502075, Final Batch Loss: 0.4929785430431366\n",
      "Epoch 4526, Loss: 2.3387107849121094, Final Batch Loss: 0.4921647310256958\n",
      "Epoch 4527, Loss: 2.3812477588653564, Final Batch Loss: 0.42659950256347656\n",
      "Epoch 4528, Loss: 2.180693417787552, Final Batch Loss: 0.41343051195144653\n",
      "Epoch 4529, Loss: 2.0783938467502594, Final Batch Loss: 0.394826203584671\n",
      "Epoch 4530, Loss: 2.2844801545143127, Final Batch Loss: 0.35724323987960815\n",
      "Epoch 4531, Loss: 2.2769954204559326, Final Batch Loss: 0.5745887756347656\n",
      "Epoch 4532, Loss: 2.083910644054413, Final Batch Loss: 0.4061623215675354\n",
      "Epoch 4533, Loss: 2.0866541266441345, Final Batch Loss: 0.44170427322387695\n",
      "Epoch 4534, Loss: 2.0773358941078186, Final Batch Loss: 0.41908544301986694\n",
      "Epoch 4535, Loss: 2.2797074019908905, Final Batch Loss: 0.5894829034805298\n",
      "Epoch 4536, Loss: 2.464985191822052, Final Batch Loss: 0.5947173833847046\n",
      "Epoch 4537, Loss: 2.1623177230358124, Final Batch Loss: 0.4115590751171112\n",
      "Epoch 4538, Loss: 2.318080097436905, Final Batch Loss: 0.3979234993457794\n",
      "Epoch 4539, Loss: 2.336889147758484, Final Batch Loss: 0.5827117562294006\n",
      "Epoch 4540, Loss: 2.122582644224167, Final Batch Loss: 0.4909321367740631\n",
      "Epoch 4541, Loss: 2.123159795999527, Final Batch Loss: 0.4451402425765991\n",
      "Epoch 4542, Loss: 2.3918858766555786, Final Batch Loss: 0.5331243872642517\n",
      "Epoch 4543, Loss: 2.2532535195350647, Final Batch Loss: 0.5274618864059448\n",
      "Epoch 4544, Loss: 2.1853805482387543, Final Batch Loss: 0.5276683568954468\n",
      "Epoch 4545, Loss: 2.2550224661827087, Final Batch Loss: 0.46567776799201965\n",
      "Epoch 4546, Loss: 2.084477663040161, Final Batch Loss: 0.37926605343818665\n",
      "Epoch 4547, Loss: 2.1979618072509766, Final Batch Loss: 0.47375476360321045\n",
      "Epoch 4548, Loss: 2.0418528616428375, Final Batch Loss: 0.2999674677848816\n",
      "Epoch 4549, Loss: 2.309955060482025, Final Batch Loss: 0.4442790448665619\n",
      "Epoch 4550, Loss: 2.0571246445178986, Final Batch Loss: 0.40697574615478516\n",
      "Epoch 4551, Loss: 2.11380073428154, Final Batch Loss: 0.38526445627212524\n",
      "Epoch 4552, Loss: 2.2321635484695435, Final Batch Loss: 0.3806823194026947\n",
      "Epoch 4553, Loss: 2.0158205330371857, Final Batch Loss: 0.42594102025032043\n",
      "Epoch 4554, Loss: 2.19634672999382, Final Batch Loss: 0.5267471671104431\n",
      "Epoch 4555, Loss: 2.2801309525966644, Final Batch Loss: 0.5418162941932678\n",
      "Epoch 4556, Loss: 2.166804552078247, Final Batch Loss: 0.5386127829551697\n",
      "Epoch 4557, Loss: 2.005727231502533, Final Batch Loss: 0.35793593525886536\n",
      "Epoch 4558, Loss: 2.276297867298126, Final Batch Loss: 0.42326903343200684\n",
      "Epoch 4559, Loss: 2.2272695302963257, Final Batch Loss: 0.45955169200897217\n",
      "Epoch 4560, Loss: 2.0596659183502197, Final Batch Loss: 0.3778155446052551\n",
      "Epoch 4561, Loss: 2.363120287656784, Final Batch Loss: 0.46322542428970337\n",
      "Epoch 4562, Loss: 2.1651580333709717, Final Batch Loss: 0.4354986548423767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4563, Loss: 2.1250555515289307, Final Batch Loss: 0.4507352113723755\n",
      "Epoch 4564, Loss: 2.3629488945007324, Final Batch Loss: 0.5259184837341309\n",
      "Epoch 4565, Loss: 2.096640557050705, Final Batch Loss: 0.32972508668899536\n",
      "Epoch 4566, Loss: 2.055159956216812, Final Batch Loss: 0.3358539938926697\n",
      "Epoch 4567, Loss: 2.136697471141815, Final Batch Loss: 0.4865906834602356\n",
      "Epoch 4568, Loss: 2.08938404917717, Final Batch Loss: 0.3845284879207611\n",
      "Epoch 4569, Loss: 1.9929120540618896, Final Batch Loss: 0.37800049781799316\n",
      "Epoch 4570, Loss: 2.239750474691391, Final Batch Loss: 0.455837219953537\n",
      "Epoch 4571, Loss: 1.9760316014289856, Final Batch Loss: 0.2849593460559845\n",
      "Epoch 4572, Loss: 2.116790294647217, Final Batch Loss: 0.33996838331222534\n",
      "Epoch 4573, Loss: 2.0667037069797516, Final Batch Loss: 0.33721452951431274\n",
      "Epoch 4574, Loss: 2.179705023765564, Final Batch Loss: 0.4737177789211273\n",
      "Epoch 4575, Loss: 2.1171768605709076, Final Batch Loss: 0.5444234609603882\n",
      "Epoch 4576, Loss: 2.247133105993271, Final Batch Loss: 0.46867817640304565\n",
      "Epoch 4577, Loss: 2.176651507616043, Final Batch Loss: 0.417836457490921\n",
      "Epoch 4578, Loss: 2.2256472408771515, Final Batch Loss: 0.3658652603626251\n",
      "Epoch 4579, Loss: 2.1821765899658203, Final Batch Loss: 0.4598047733306885\n",
      "Epoch 4580, Loss: 1.9866420924663544, Final Batch Loss: 0.35642972588539124\n",
      "Epoch 4581, Loss: 2.069228947162628, Final Batch Loss: 0.38562193512916565\n",
      "Epoch 4582, Loss: 2.088850110769272, Final Batch Loss: 0.443448930978775\n",
      "Epoch 4583, Loss: 2.0871045291423798, Final Batch Loss: 0.362266480922699\n",
      "Epoch 4584, Loss: 2.470996707677841, Final Batch Loss: 0.5118311047554016\n",
      "Epoch 4585, Loss: 2.0961445569992065, Final Batch Loss: 0.4185318648815155\n",
      "Epoch 4586, Loss: 2.118514448404312, Final Batch Loss: 0.4260425567626953\n",
      "Epoch 4587, Loss: 2.037046790122986, Final Batch Loss: 0.46904975175857544\n",
      "Epoch 4588, Loss: 2.0533020198345184, Final Batch Loss: 0.4433048665523529\n",
      "Epoch 4589, Loss: 2.277894288301468, Final Batch Loss: 0.41265350580215454\n",
      "Epoch 4590, Loss: 2.2384717762470245, Final Batch Loss: 0.5852727890014648\n",
      "Epoch 4591, Loss: 2.1645556688308716, Final Batch Loss: 0.4153277575969696\n",
      "Epoch 4592, Loss: 2.029954969882965, Final Batch Loss: 0.38340041041374207\n",
      "Epoch 4593, Loss: 2.3826645016670227, Final Batch Loss: 0.6522946357727051\n",
      "Epoch 4594, Loss: 2.194053679704666, Final Batch Loss: 0.48569393157958984\n",
      "Epoch 4595, Loss: 2.1310918033123016, Final Batch Loss: 0.41506779193878174\n",
      "Epoch 4596, Loss: 2.0604577362537384, Final Batch Loss: 0.38893547654151917\n",
      "Epoch 4597, Loss: 2.176633596420288, Final Batch Loss: 0.4291650056838989\n",
      "Epoch 4598, Loss: 2.1224341690540314, Final Batch Loss: 0.44518810510635376\n",
      "Epoch 4599, Loss: 2.1251048743724823, Final Batch Loss: 0.4219760298728943\n",
      "Epoch 4600, Loss: 2.1048279106616974, Final Batch Loss: 0.46242544054985046\n",
      "Epoch 4601, Loss: 2.190402776002884, Final Batch Loss: 0.45235589146614075\n",
      "Epoch 4602, Loss: 2.036348670721054, Final Batch Loss: 0.4073043167591095\n",
      "Epoch 4603, Loss: 1.9512308239936829, Final Batch Loss: 0.3851074278354645\n",
      "Epoch 4604, Loss: 2.079913407564163, Final Batch Loss: 0.3834564685821533\n",
      "Epoch 4605, Loss: 2.1918571293354034, Final Batch Loss: 0.42822712659835815\n",
      "Epoch 4606, Loss: 2.10792937874794, Final Batch Loss: 0.47227972745895386\n",
      "Epoch 4607, Loss: 2.157073050737381, Final Batch Loss: 0.4149717688560486\n",
      "Epoch 4608, Loss: 2.0508247315883636, Final Batch Loss: 0.3771774172782898\n",
      "Epoch 4609, Loss: 2.135335385799408, Final Batch Loss: 0.41636356711387634\n",
      "Epoch 4610, Loss: 2.1254274547100067, Final Batch Loss: 0.5157065987586975\n",
      "Epoch 4611, Loss: 2.1833401322364807, Final Batch Loss: 0.4672567546367645\n",
      "Epoch 4612, Loss: 2.086833566427231, Final Batch Loss: 0.39062750339508057\n",
      "Epoch 4613, Loss: 2.0456229150295258, Final Batch Loss: 0.362154483795166\n",
      "Epoch 4614, Loss: 2.2224748730659485, Final Batch Loss: 0.42321839928627014\n",
      "Epoch 4615, Loss: 2.4214457869529724, Final Batch Loss: 0.7818744778633118\n",
      "Epoch 4616, Loss: 2.198326200246811, Final Batch Loss: 0.38275137543678284\n",
      "Epoch 4617, Loss: 2.25435733795166, Final Batch Loss: 0.5517818331718445\n",
      "Epoch 4618, Loss: 2.2338093519210815, Final Batch Loss: 0.46917924284935\n",
      "Epoch 4619, Loss: 2.063928246498108, Final Batch Loss: 0.4199925363063812\n",
      "Epoch 4620, Loss: 2.3668037354946136, Final Batch Loss: 0.5559431314468384\n",
      "Epoch 4621, Loss: 2.185042053461075, Final Batch Loss: 0.5107974410057068\n",
      "Epoch 4622, Loss: 2.1601888239383698, Final Batch Loss: 0.3748546242713928\n",
      "Epoch 4623, Loss: 2.2445731461048126, Final Batch Loss: 0.5116610527038574\n",
      "Epoch 4624, Loss: 2.136472910642624, Final Batch Loss: 0.5045691728591919\n",
      "Epoch 4625, Loss: 2.236244410276413, Final Batch Loss: 0.4160871207714081\n",
      "Epoch 4626, Loss: 2.2348940670490265, Final Batch Loss: 0.5751979947090149\n",
      "Epoch 4627, Loss: 2.2133246660232544, Final Batch Loss: 0.46823760867118835\n",
      "Epoch 4628, Loss: 2.0896773040294647, Final Batch Loss: 0.4309561848640442\n",
      "Epoch 4629, Loss: 2.0784060657024384, Final Batch Loss: 0.37200450897216797\n",
      "Epoch 4630, Loss: 2.102546900510788, Final Batch Loss: 0.4292496144771576\n",
      "Epoch 4631, Loss: 2.145382970571518, Final Batch Loss: 0.39986005425453186\n",
      "Epoch 4632, Loss: 2.333600252866745, Final Batch Loss: 0.5117224454879761\n",
      "Epoch 4633, Loss: 2.1327659487724304, Final Batch Loss: 0.3412364423274994\n",
      "Epoch 4634, Loss: 2.0490685999393463, Final Batch Loss: 0.40319257974624634\n",
      "Epoch 4635, Loss: 2.175391525030136, Final Batch Loss: 0.47013622522354126\n",
      "Epoch 4636, Loss: 2.0365755558013916, Final Batch Loss: 0.4765775501728058\n",
      "Epoch 4637, Loss: 2.140955775976181, Final Batch Loss: 0.4608970284461975\n",
      "Epoch 4638, Loss: 2.2400185465812683, Final Batch Loss: 0.5862094759941101\n",
      "Epoch 4639, Loss: 2.021090090274811, Final Batch Loss: 0.3103364109992981\n",
      "Epoch 4640, Loss: 2.2743413150310516, Final Batch Loss: 0.3623191714286804\n",
      "Epoch 4641, Loss: 2.053265929222107, Final Batch Loss: 0.2898404896259308\n",
      "Epoch 4642, Loss: 2.0995959639549255, Final Batch Loss: 0.3901493549346924\n",
      "Epoch 4643, Loss: 2.0563643872737885, Final Batch Loss: 0.38911616802215576\n",
      "Epoch 4644, Loss: 2.0192040503025055, Final Batch Loss: 0.3822181820869446\n",
      "Epoch 4645, Loss: 2.3566595315933228, Final Batch Loss: 0.4217815101146698\n",
      "Epoch 4646, Loss: 2.036443054676056, Final Batch Loss: 0.3520604372024536\n",
      "Epoch 4647, Loss: 2.2470867931842804, Final Batch Loss: 0.4911588430404663\n",
      "Epoch 4648, Loss: 2.107317239046097, Final Batch Loss: 0.30208057165145874\n",
      "Epoch 4649, Loss: 2.1381943225860596, Final Batch Loss: 0.5524851679801941\n",
      "Epoch 4650, Loss: 2.117534726858139, Final Batch Loss: 0.4523787498474121\n",
      "Epoch 4651, Loss: 2.2989413142204285, Final Batch Loss: 0.5266777873039246\n",
      "Epoch 4652, Loss: 2.1367506980895996, Final Batch Loss: 0.41708046197891235\n",
      "Epoch 4653, Loss: 2.2393306493759155, Final Batch Loss: 0.3907483220100403\n",
      "Epoch 4654, Loss: 2.0379629135131836, Final Batch Loss: 0.3628677427768707\n",
      "Epoch 4655, Loss: 2.2596592605113983, Final Batch Loss: 0.43237704038619995\n",
      "Epoch 4656, Loss: 2.040275454521179, Final Batch Loss: 0.41930848360061646\n",
      "Epoch 4657, Loss: 2.2044224739074707, Final Batch Loss: 0.3891425132751465\n",
      "Epoch 4658, Loss: 2.26957306265831, Final Batch Loss: 0.41830581426620483\n",
      "Epoch 4659, Loss: 2.104300796985626, Final Batch Loss: 0.4652769863605499\n",
      "Epoch 4660, Loss: 2.101436972618103, Final Batch Loss: 0.3616046905517578\n",
      "Epoch 4661, Loss: 2.182922512292862, Final Batch Loss: 0.323923259973526\n",
      "Epoch 4662, Loss: 2.297441452741623, Final Batch Loss: 0.43955114483833313\n",
      "Epoch 4663, Loss: 2.0740765035152435, Final Batch Loss: 0.4714849591255188\n",
      "Epoch 4664, Loss: 2.3025649189949036, Final Batch Loss: 0.4941127896308899\n",
      "Epoch 4665, Loss: 2.138334333896637, Final Batch Loss: 0.42612019181251526\n",
      "Epoch 4666, Loss: 2.0482620000839233, Final Batch Loss: 0.3655027151107788\n",
      "Epoch 4667, Loss: 2.1255096793174744, Final Batch Loss: 0.37695351243019104\n",
      "Epoch 4668, Loss: 2.2169933319091797, Final Batch Loss: 0.4126609265804291\n",
      "Epoch 4669, Loss: 2.2186880111694336, Final Batch Loss: 0.4665951430797577\n",
      "Epoch 4670, Loss: 2.118419885635376, Final Batch Loss: 0.38619673252105713\n",
      "Epoch 4671, Loss: 2.214092880487442, Final Batch Loss: 0.4856283366680145\n",
      "Epoch 4672, Loss: 1.9892211556434631, Final Batch Loss: 0.3219785988330841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4673, Loss: 2.0884355306625366, Final Batch Loss: 0.39927348494529724\n",
      "Epoch 4674, Loss: 2.2775045037269592, Final Batch Loss: 0.5048390030860901\n",
      "Epoch 4675, Loss: 1.9847075045108795, Final Batch Loss: 0.3280131220817566\n",
      "Epoch 4676, Loss: 2.2081611454486847, Final Batch Loss: 0.4455491602420807\n",
      "Epoch 4677, Loss: 2.0916886031627655, Final Batch Loss: 0.37947821617126465\n",
      "Epoch 4678, Loss: 2.1173008382320404, Final Batch Loss: 0.47061818838119507\n",
      "Epoch 4679, Loss: 1.9347547590732574, Final Batch Loss: 0.2924173176288605\n",
      "Epoch 4680, Loss: 2.403619110584259, Final Batch Loss: 0.4951193034648895\n",
      "Epoch 4681, Loss: 2.1061605513095856, Final Batch Loss: 0.44075170159339905\n",
      "Epoch 4682, Loss: 1.999784380197525, Final Batch Loss: 0.3378671705722809\n",
      "Epoch 4683, Loss: 1.9776089787483215, Final Batch Loss: 0.36221280694007874\n",
      "Epoch 4684, Loss: 2.0697841942310333, Final Batch Loss: 0.4231967628002167\n",
      "Epoch 4685, Loss: 2.0167326629161835, Final Batch Loss: 0.32874345779418945\n",
      "Epoch 4686, Loss: 2.1053999066352844, Final Batch Loss: 0.3381425142288208\n",
      "Epoch 4687, Loss: 2.055125743150711, Final Batch Loss: 0.41539543867111206\n",
      "Epoch 4688, Loss: 2.0570589900016785, Final Batch Loss: 0.24949020147323608\n",
      "Epoch 4689, Loss: 2.096607744693756, Final Batch Loss: 0.39065030217170715\n",
      "Epoch 4690, Loss: 2.1181772351264954, Final Batch Loss: 0.3882094621658325\n",
      "Epoch 4691, Loss: 2.241111934185028, Final Batch Loss: 0.37266966700553894\n",
      "Epoch 4692, Loss: 2.201579660177231, Final Batch Loss: 0.4708576500415802\n",
      "Epoch 4693, Loss: 2.154126316308975, Final Batch Loss: 0.30904197692871094\n",
      "Epoch 4694, Loss: 1.9757238626480103, Final Batch Loss: 0.31355562806129456\n",
      "Epoch 4695, Loss: 2.012250632047653, Final Batch Loss: 0.39650261402130127\n",
      "Epoch 4696, Loss: 2.0169264376163483, Final Batch Loss: 0.2815706729888916\n",
      "Epoch 4697, Loss: 2.0297940969467163, Final Batch Loss: 0.3185153603553772\n",
      "Epoch 4698, Loss: 2.3765131533145905, Final Batch Loss: 0.4999391734600067\n",
      "Epoch 4699, Loss: 2.2106725573539734, Final Batch Loss: 0.383372962474823\n",
      "Epoch 4700, Loss: 1.9769679009914398, Final Batch Loss: 0.42229530215263367\n",
      "Epoch 4701, Loss: 2.0953803956508636, Final Batch Loss: 0.44133898615837097\n",
      "Epoch 4702, Loss: 2.1583486199378967, Final Batch Loss: 0.31029632687568665\n",
      "Epoch 4703, Loss: 2.1041030883789062, Final Batch Loss: 0.4028591513633728\n",
      "Epoch 4704, Loss: 2.1237317323684692, Final Batch Loss: 0.49848464131355286\n",
      "Epoch 4705, Loss: 2.024959236383438, Final Batch Loss: 0.47069692611694336\n",
      "Epoch 4706, Loss: 2.366834372282028, Final Batch Loss: 0.47475212812423706\n",
      "Epoch 4707, Loss: 2.3208552300930023, Final Batch Loss: 0.39747077226638794\n",
      "Epoch 4708, Loss: 2.0598088800907135, Final Batch Loss: 0.42937788367271423\n",
      "Epoch 4709, Loss: 2.1511490643024445, Final Batch Loss: 0.45791900157928467\n",
      "Epoch 4710, Loss: 2.0370104908943176, Final Batch Loss: 0.376902312040329\n",
      "Epoch 4711, Loss: 2.271077662706375, Final Batch Loss: 0.5913705229759216\n",
      "Epoch 4712, Loss: 2.157678037881851, Final Batch Loss: 0.40920740365982056\n",
      "Epoch 4713, Loss: 2.0493079125881195, Final Batch Loss: 0.37839674949645996\n",
      "Epoch 4714, Loss: 2.171707808971405, Final Batch Loss: 0.3679153621196747\n",
      "Epoch 4715, Loss: 2.2794239223003387, Final Batch Loss: 0.5844072103500366\n",
      "Epoch 4716, Loss: 2.1313253045082092, Final Batch Loss: 0.4858848750591278\n",
      "Epoch 4717, Loss: 2.2462151050567627, Final Batch Loss: 0.4464280605316162\n",
      "Epoch 4718, Loss: 2.192964494228363, Final Batch Loss: 0.43128088116645813\n",
      "Epoch 4719, Loss: 2.013209968805313, Final Batch Loss: 0.47943079471588135\n",
      "Epoch 4720, Loss: 2.2108734846115112, Final Batch Loss: 0.4710770845413208\n",
      "Epoch 4721, Loss: 1.9780771434307098, Final Batch Loss: 0.3705711364746094\n",
      "Epoch 4722, Loss: 1.9581622779369354, Final Batch Loss: 0.29662930965423584\n",
      "Epoch 4723, Loss: 2.0239082276821136, Final Batch Loss: 0.3384934663772583\n",
      "Epoch 4724, Loss: 2.132179915904999, Final Batch Loss: 0.43571504950523376\n",
      "Epoch 4725, Loss: 2.176242232322693, Final Batch Loss: 0.44118157029151917\n",
      "Epoch 4726, Loss: 2.1938152611255646, Final Batch Loss: 0.41416698694229126\n",
      "Epoch 4727, Loss: 2.0384249687194824, Final Batch Loss: 0.3808325231075287\n",
      "Epoch 4728, Loss: 2.1156243681907654, Final Batch Loss: 0.43636244535446167\n",
      "Epoch 4729, Loss: 2.023690849542618, Final Batch Loss: 0.3214893341064453\n",
      "Epoch 4730, Loss: 2.1901674568653107, Final Batch Loss: 0.4848363995552063\n",
      "Epoch 4731, Loss: 2.0729150474071503, Final Batch Loss: 0.3408941328525543\n",
      "Epoch 4732, Loss: 2.111582040786743, Final Batch Loss: 0.3722008466720581\n",
      "Epoch 4733, Loss: 2.0836295783519745, Final Batch Loss: 0.43655160069465637\n",
      "Epoch 4734, Loss: 2.310817152261734, Final Batch Loss: 0.3937857151031494\n",
      "Epoch 4735, Loss: 2.2121520340442657, Final Batch Loss: 0.4021604359149933\n",
      "Epoch 4736, Loss: 2.117343842983246, Final Batch Loss: 0.3816390037536621\n",
      "Epoch 4737, Loss: 2.0922216176986694, Final Batch Loss: 0.3599840998649597\n",
      "Epoch 4738, Loss: 2.245212644338608, Final Batch Loss: 0.41608887910842896\n",
      "Epoch 4739, Loss: 2.2125687301158905, Final Batch Loss: 0.5220096111297607\n",
      "Epoch 4740, Loss: 2.1749091744422913, Final Batch Loss: 0.43985939025878906\n",
      "Epoch 4741, Loss: 2.2006700932979584, Final Batch Loss: 0.31311744451522827\n",
      "Epoch 4742, Loss: 2.2020163238048553, Final Batch Loss: 0.5877354741096497\n",
      "Epoch 4743, Loss: 2.0351475179195404, Final Batch Loss: 0.33557212352752686\n",
      "Epoch 4744, Loss: 1.9998025596141815, Final Batch Loss: 0.3274659812450409\n",
      "Epoch 4745, Loss: 2.013325959444046, Final Batch Loss: 0.4109943211078644\n",
      "Epoch 4746, Loss: 2.052418887615204, Final Batch Loss: 0.33786508440971375\n",
      "Epoch 4747, Loss: 2.055971384048462, Final Batch Loss: 0.3560350239276886\n",
      "Epoch 4748, Loss: 2.0743435323238373, Final Batch Loss: 0.48427554965019226\n",
      "Epoch 4749, Loss: 2.0543956756591797, Final Batch Loss: 0.35732290148735046\n",
      "Epoch 4750, Loss: 2.2435783445835114, Final Batch Loss: 0.42669203877449036\n",
      "Epoch 4751, Loss: 1.943045973777771, Final Batch Loss: 0.37931329011917114\n",
      "Epoch 4752, Loss: 2.1448549330234528, Final Batch Loss: 0.4124583303928375\n",
      "Epoch 4753, Loss: 2.2834428250789642, Final Batch Loss: 0.5251356959342957\n",
      "Epoch 4754, Loss: 2.1832238733768463, Final Batch Loss: 0.4972706735134125\n",
      "Epoch 4755, Loss: 2.255663424730301, Final Batch Loss: 0.452099084854126\n",
      "Epoch 4756, Loss: 2.478973150253296, Final Batch Loss: 0.5487630367279053\n",
      "Epoch 4757, Loss: 2.3202754259109497, Final Batch Loss: 0.580219566822052\n",
      "Epoch 4758, Loss: 2.336873859167099, Final Batch Loss: 0.5208524465560913\n",
      "Epoch 4759, Loss: 2.385057181119919, Final Batch Loss: 0.5481749176979065\n",
      "Epoch 4760, Loss: 2.2258788347244263, Final Batch Loss: 0.5488815903663635\n",
      "Epoch 4761, Loss: 2.0764116644859314, Final Batch Loss: 0.45930054783821106\n",
      "Epoch 4762, Loss: 2.106429100036621, Final Batch Loss: 0.5418134331703186\n",
      "Epoch 4763, Loss: 2.0915992856025696, Final Batch Loss: 0.3933042585849762\n",
      "Epoch 4764, Loss: 2.130470871925354, Final Batch Loss: 0.41415128111839294\n",
      "Epoch 4765, Loss: 2.2497286200523376, Final Batch Loss: 0.5554593205451965\n",
      "Epoch 4766, Loss: 2.0440343022346497, Final Batch Loss: 0.46124646067619324\n",
      "Epoch 4767, Loss: 2.2360879480838776, Final Batch Loss: 0.4204786419868469\n",
      "Epoch 4768, Loss: 2.145082950592041, Final Batch Loss: 0.43818002939224243\n",
      "Epoch 4769, Loss: 2.116427391767502, Final Batch Loss: 0.49648112058639526\n",
      "Epoch 4770, Loss: 2.1161170601844788, Final Batch Loss: 0.4903792142868042\n",
      "Epoch 4771, Loss: 2.1495710015296936, Final Batch Loss: 0.4284680485725403\n",
      "Epoch 4772, Loss: 2.2471953332424164, Final Batch Loss: 0.5441046357154846\n",
      "Epoch 4773, Loss: 2.1881455779075623, Final Batch Loss: 0.4617583751678467\n",
      "Epoch 4774, Loss: 2.236572712659836, Final Batch Loss: 0.586284875869751\n",
      "Epoch 4775, Loss: 2.0626162588596344, Final Batch Loss: 0.3608439564704895\n",
      "Epoch 4776, Loss: 2.237036943435669, Final Batch Loss: 0.5291499495506287\n",
      "Epoch 4777, Loss: 2.271009147167206, Final Batch Loss: 0.45532479882240295\n",
      "Epoch 4778, Loss: 2.21576064825058, Final Batch Loss: 0.42140132188796997\n",
      "Epoch 4779, Loss: 2.065945565700531, Final Batch Loss: 0.49453005194664\n",
      "Epoch 4780, Loss: 2.1579648852348328, Final Batch Loss: 0.43210774660110474\n",
      "Epoch 4781, Loss: 2.075375884771347, Final Batch Loss: 0.325022429227829\n",
      "Epoch 4782, Loss: 2.228371024131775, Final Batch Loss: 0.4499499499797821\n",
      "Epoch 4783, Loss: 2.262485593557358, Final Batch Loss: 0.5097849369049072\n",
      "Epoch 4784, Loss: 2.359075963497162, Final Batch Loss: 0.6474788784980774\n",
      "Epoch 4785, Loss: 2.1565147936344147, Final Batch Loss: 0.36805275082588196\n",
      "Epoch 4786, Loss: 2.1703595221042633, Final Batch Loss: 0.39420342445373535\n",
      "Epoch 4787, Loss: 2.1879056692123413, Final Batch Loss: 0.4203348159790039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4788, Loss: 2.1322875022888184, Final Batch Loss: 0.2702133357524872\n",
      "Epoch 4789, Loss: 1.940009742975235, Final Batch Loss: 0.42521029710769653\n",
      "Epoch 4790, Loss: 2.0490313172340393, Final Batch Loss: 0.3455272912979126\n",
      "Epoch 4791, Loss: 2.3222460746765137, Final Batch Loss: 0.47113558650016785\n",
      "Epoch 4792, Loss: 2.21945258975029, Final Batch Loss: 0.30983054637908936\n",
      "Epoch 4793, Loss: 2.2153990268707275, Final Batch Loss: 0.5820939540863037\n",
      "Epoch 4794, Loss: 2.1733870804309845, Final Batch Loss: 0.44869711995124817\n",
      "Epoch 4795, Loss: 2.141135483980179, Final Batch Loss: 0.4466915726661682\n",
      "Epoch 4796, Loss: 2.080715239048004, Final Batch Loss: 0.30135342478752136\n",
      "Epoch 4797, Loss: 2.1149931848049164, Final Batch Loss: 0.41843533515930176\n",
      "Epoch 4798, Loss: 2.0450952649116516, Final Batch Loss: 0.35364022850990295\n",
      "Epoch 4799, Loss: 2.1057695150375366, Final Batch Loss: 0.4295162260532379\n",
      "Epoch 4800, Loss: 2.0802571177482605, Final Batch Loss: 0.46879929304122925\n",
      "Epoch 4801, Loss: 2.035209506750107, Final Batch Loss: 0.44992396235466003\n",
      "Epoch 4802, Loss: 1.9796518981456757, Final Batch Loss: 0.4616364538669586\n",
      "Epoch 4803, Loss: 2.1150868237018585, Final Batch Loss: 0.4409877359867096\n",
      "Epoch 4804, Loss: 2.2052456736564636, Final Batch Loss: 0.4835014045238495\n",
      "Epoch 4805, Loss: 2.1191059052944183, Final Batch Loss: 0.4157722294330597\n",
      "Epoch 4806, Loss: 2.1548074185848236, Final Batch Loss: 0.3415795564651489\n",
      "Epoch 4807, Loss: 1.9984329342842102, Final Batch Loss: 0.3880106210708618\n",
      "Epoch 4808, Loss: 2.0770057439804077, Final Batch Loss: 0.34911081194877625\n",
      "Epoch 4809, Loss: 2.2461545169353485, Final Batch Loss: 0.42894110083580017\n",
      "Epoch 4810, Loss: 2.010870724916458, Final Batch Loss: 0.43929678201675415\n",
      "Epoch 4811, Loss: 2.001451998949051, Final Batch Loss: 0.4515679180622101\n",
      "Epoch 4812, Loss: 2.2202323377132416, Final Batch Loss: 0.507638692855835\n",
      "Epoch 4813, Loss: 2.0954707860946655, Final Batch Loss: 0.42195576429367065\n",
      "Epoch 4814, Loss: 2.169717639684677, Final Batch Loss: 0.42424076795578003\n",
      "Epoch 4815, Loss: 2.051821857690811, Final Batch Loss: 0.37483295798301697\n",
      "Epoch 4816, Loss: 1.9976021945476532, Final Batch Loss: 0.36935099959373474\n",
      "Epoch 4817, Loss: 2.142657309770584, Final Batch Loss: 0.4197283685207367\n",
      "Epoch 4818, Loss: 2.0915799140930176, Final Batch Loss: 0.4583832323551178\n",
      "Epoch 4819, Loss: 2.0377731919288635, Final Batch Loss: 0.3804910182952881\n",
      "Epoch 4820, Loss: 1.9775605648756027, Final Batch Loss: 0.24959911406040192\n",
      "Epoch 4821, Loss: 2.2570334374904633, Final Batch Loss: 0.3418276011943817\n",
      "Epoch 4822, Loss: 1.8755094707012177, Final Batch Loss: 0.44739586114883423\n",
      "Epoch 4823, Loss: 2.143454432487488, Final Batch Loss: 0.3877880275249481\n",
      "Epoch 4824, Loss: 2.1437677443027496, Final Batch Loss: 0.3971273899078369\n",
      "Epoch 4825, Loss: 2.1396550834178925, Final Batch Loss: 0.39878028631210327\n",
      "Epoch 4826, Loss: 2.047008752822876, Final Batch Loss: 0.4717070460319519\n",
      "Epoch 4827, Loss: 1.973230630159378, Final Batch Loss: 0.3617018163204193\n",
      "Epoch 4828, Loss: 1.9113002717494965, Final Batch Loss: 0.40914541482925415\n",
      "Epoch 4829, Loss: 1.9987675845623016, Final Batch Loss: 0.3279169201850891\n",
      "Epoch 4830, Loss: 2.236773818731308, Final Batch Loss: 0.48600059747695923\n",
      "Epoch 4831, Loss: 1.908841848373413, Final Batch Loss: 0.3002815246582031\n",
      "Epoch 4832, Loss: 2.1127271354198456, Final Batch Loss: 0.4931430220603943\n",
      "Epoch 4833, Loss: 2.039771020412445, Final Batch Loss: 0.44263920187950134\n",
      "Epoch 4834, Loss: 2.071758359670639, Final Batch Loss: 0.3508705794811249\n",
      "Epoch 4835, Loss: 2.0318554043769836, Final Batch Loss: 0.27885183691978455\n",
      "Epoch 4836, Loss: 2.2060684859752655, Final Batch Loss: 0.46520522236824036\n",
      "Epoch 4837, Loss: 2.050759643316269, Final Batch Loss: 0.3443717360496521\n",
      "Epoch 4838, Loss: 2.326941579580307, Final Batch Loss: 0.45320218801498413\n",
      "Epoch 4839, Loss: 1.8483886122703552, Final Batch Loss: 0.3391314744949341\n",
      "Epoch 4840, Loss: 2.0970854461193085, Final Batch Loss: 0.415034681558609\n",
      "Epoch 4841, Loss: 2.1378393173217773, Final Batch Loss: 0.40699663758277893\n",
      "Epoch 4842, Loss: 2.1911004185676575, Final Batch Loss: 0.400724321603775\n",
      "Epoch 4843, Loss: 2.1022201478481293, Final Batch Loss: 0.4251384437084198\n",
      "Epoch 4844, Loss: 2.0929804146289825, Final Batch Loss: 0.4412429928779602\n",
      "Epoch 4845, Loss: 2.0949611365795135, Final Batch Loss: 0.3666799068450928\n",
      "Epoch 4846, Loss: 2.1850281953811646, Final Batch Loss: 0.4306615889072418\n",
      "Epoch 4847, Loss: 2.2237653732299805, Final Batch Loss: 0.42541536688804626\n",
      "Epoch 4848, Loss: 2.3144291639328003, Final Batch Loss: 0.4550056755542755\n",
      "Epoch 4849, Loss: 2.0518377125263214, Final Batch Loss: 0.30936938524246216\n",
      "Epoch 4850, Loss: 2.2396983802318573, Final Batch Loss: 0.5289869904518127\n",
      "Epoch 4851, Loss: 2.14180189371109, Final Batch Loss: 0.4496541917324066\n",
      "Epoch 4852, Loss: 2.1596404910087585, Final Batch Loss: 0.509598970413208\n",
      "Epoch 4853, Loss: 2.1781885027885437, Final Batch Loss: 0.465443879365921\n",
      "Epoch 4854, Loss: 2.0728558003902435, Final Batch Loss: 0.48589807748794556\n",
      "Epoch 4855, Loss: 2.268642485141754, Final Batch Loss: 0.46700581908226013\n",
      "Epoch 4856, Loss: 2.1693624556064606, Final Batch Loss: 0.5015507936477661\n",
      "Epoch 4857, Loss: 2.216479539871216, Final Batch Loss: 0.45387551188468933\n",
      "Epoch 4858, Loss: 2.1290335059165955, Final Batch Loss: 0.35600170493125916\n",
      "Epoch 4859, Loss: 2.213211238384247, Final Batch Loss: 0.40180301666259766\n",
      "Epoch 4860, Loss: 2.2529490292072296, Final Batch Loss: 0.5475048422813416\n",
      "Epoch 4861, Loss: 1.9263368248939514, Final Batch Loss: 0.2932778596878052\n",
      "Epoch 4862, Loss: 2.0662038922309875, Final Batch Loss: 0.38619479537010193\n",
      "Epoch 4863, Loss: 2.174565613269806, Final Batch Loss: 0.32543686032295227\n",
      "Epoch 4864, Loss: 2.1628415882587433, Final Batch Loss: 0.3865414559841156\n",
      "Epoch 4865, Loss: 2.0446580946445465, Final Batch Loss: 0.46275845170021057\n",
      "Epoch 4866, Loss: 2.1638033986091614, Final Batch Loss: 0.47277557849884033\n",
      "Epoch 4867, Loss: 2.1348118782043457, Final Batch Loss: 0.43925780057907104\n",
      "Epoch 4868, Loss: 2.004902631044388, Final Batch Loss: 0.3984772562980652\n",
      "Epoch 4869, Loss: 2.243192881345749, Final Batch Loss: 0.4457145035266876\n",
      "Epoch 4870, Loss: 2.0860701501369476, Final Batch Loss: 0.4051533639431\n",
      "Epoch 4871, Loss: 1.8516919612884521, Final Batch Loss: 0.41208434104919434\n",
      "Epoch 4872, Loss: 2.195842534303665, Final Batch Loss: 0.45526227355003357\n",
      "Epoch 4873, Loss: 2.23722380399704, Final Batch Loss: 0.42427879571914673\n",
      "Epoch 4874, Loss: 2.331402510404587, Final Batch Loss: 0.4229567050933838\n",
      "Epoch 4875, Loss: 2.211715579032898, Final Batch Loss: 0.5178218483924866\n",
      "Epoch 4876, Loss: 2.1041668355464935, Final Batch Loss: 0.4362054765224457\n",
      "Epoch 4877, Loss: 1.9816433191299438, Final Batch Loss: 0.3395634591579437\n",
      "Epoch 4878, Loss: 1.9716129899024963, Final Batch Loss: 0.3598308563232422\n",
      "Epoch 4879, Loss: 2.1740402579307556, Final Batch Loss: 0.506531298160553\n",
      "Epoch 4880, Loss: 2.215924024581909, Final Batch Loss: 0.41926589608192444\n",
      "Epoch 4881, Loss: 2.118080884218216, Final Batch Loss: 0.43221068382263184\n",
      "Epoch 4882, Loss: 1.9977484047412872, Final Batch Loss: 0.36284738779067993\n",
      "Epoch 4883, Loss: 2.3485575318336487, Final Batch Loss: 0.31765756011009216\n",
      "Epoch 4884, Loss: 2.0301515758037567, Final Batch Loss: 0.35804814100265503\n",
      "Epoch 4885, Loss: 2.099337935447693, Final Batch Loss: 0.3802916705608368\n",
      "Epoch 4886, Loss: 2.07488951086998, Final Batch Loss: 0.32062944769859314\n",
      "Epoch 4887, Loss: 2.1180644929409027, Final Batch Loss: 0.44847196340560913\n",
      "Epoch 4888, Loss: 2.16177299618721, Final Batch Loss: 0.41642698645591736\n",
      "Epoch 4889, Loss: 2.0810196697711945, Final Batch Loss: 0.4326976537704468\n",
      "Epoch 4890, Loss: 2.135859251022339, Final Batch Loss: 0.42247340083122253\n",
      "Epoch 4891, Loss: 2.0622252821922302, Final Batch Loss: 0.39079543948173523\n",
      "Epoch 4892, Loss: 2.1794514060020447, Final Batch Loss: 0.4322018623352051\n",
      "Epoch 4893, Loss: 2.2700145542621613, Final Batch Loss: 0.4224895238876343\n",
      "Epoch 4894, Loss: 2.0913668870925903, Final Batch Loss: 0.42124560475349426\n",
      "Epoch 4895, Loss: 2.1358740627765656, Final Batch Loss: 0.4728982448577881\n",
      "Epoch 4896, Loss: 1.9597722291946411, Final Batch Loss: 0.42368191480636597\n",
      "Epoch 4897, Loss: 2.078955978155136, Final Batch Loss: 0.4731672704219818\n",
      "Epoch 4898, Loss: 2.236370772123337, Final Batch Loss: 0.48739007115364075\n",
      "Epoch 4899, Loss: 2.235265374183655, Final Batch Loss: 0.47384390234947205\n",
      "Epoch 4900, Loss: 2.2339226603507996, Final Batch Loss: 0.5130941271781921\n",
      "Epoch 4901, Loss: 2.126348853111267, Final Batch Loss: 0.45546233654022217\n",
      "Epoch 4902, Loss: 2.195324629545212, Final Batch Loss: 0.5314227342605591\n",
      "Epoch 4903, Loss: 2.0164762437343597, Final Batch Loss: 0.37362951040267944\n",
      "Epoch 4904, Loss: 2.075501173734665, Final Batch Loss: 0.37262216210365295\n",
      "Epoch 4905, Loss: 2.186716854572296, Final Batch Loss: 0.468918114900589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4906, Loss: 2.318873703479767, Final Batch Loss: 0.43948817253112793\n",
      "Epoch 4907, Loss: 1.9870153367519379, Final Batch Loss: 0.4135054647922516\n",
      "Epoch 4908, Loss: 2.0838368237018585, Final Batch Loss: 0.3480391502380371\n",
      "Epoch 4909, Loss: 2.1862980723381042, Final Batch Loss: 0.5202606916427612\n",
      "Epoch 4910, Loss: 2.3746437728405, Final Batch Loss: 0.5625106692314148\n",
      "Epoch 4911, Loss: 1.985874891281128, Final Batch Loss: 0.3637462854385376\n",
      "Epoch 4912, Loss: 2.2280750572681427, Final Batch Loss: 0.42762768268585205\n",
      "Epoch 4913, Loss: 2.1117416322231293, Final Batch Loss: 0.43653059005737305\n",
      "Epoch 4914, Loss: 2.1910162568092346, Final Batch Loss: 0.5207259654998779\n",
      "Epoch 4915, Loss: 2.0304798781871796, Final Batch Loss: 0.34532827138900757\n",
      "Epoch 4916, Loss: 2.1524984538555145, Final Batch Loss: 0.4050543010234833\n",
      "Epoch 4917, Loss: 2.185010612010956, Final Batch Loss: 0.3901727497577667\n",
      "Epoch 4918, Loss: 2.0964953005313873, Final Batch Loss: 0.4281543493270874\n",
      "Epoch 4919, Loss: 2.333168238401413, Final Batch Loss: 0.5532347559928894\n",
      "Epoch 4920, Loss: 1.9916832447052002, Final Batch Loss: 0.43915554881095886\n",
      "Epoch 4921, Loss: 2.0849761962890625, Final Batch Loss: 0.43310055136680603\n",
      "Epoch 4922, Loss: 2.169668346643448, Final Batch Loss: 0.4249686002731323\n",
      "Epoch 4923, Loss: 2.08327254652977, Final Batch Loss: 0.33547645807266235\n",
      "Epoch 4924, Loss: 2.1810253858566284, Final Batch Loss: 0.42592206597328186\n",
      "Epoch 4925, Loss: 2.0925129055976868, Final Batch Loss: 0.43527090549468994\n",
      "Epoch 4926, Loss: 2.0879334211349487, Final Batch Loss: 0.48635807633399963\n",
      "Epoch 4927, Loss: 2.1659056544303894, Final Batch Loss: 0.47270724177360535\n",
      "Epoch 4928, Loss: 2.2740272283554077, Final Batch Loss: 0.47326385974884033\n",
      "Epoch 4929, Loss: 2.0651795566082, Final Batch Loss: 0.4459913969039917\n",
      "Epoch 4930, Loss: 2.111667424440384, Final Batch Loss: 0.3837169110774994\n",
      "Epoch 4931, Loss: 2.2262352406978607, Final Batch Loss: 0.46350598335266113\n",
      "Epoch 4932, Loss: 2.1478101313114166, Final Batch Loss: 0.4606322646141052\n",
      "Epoch 4933, Loss: 2.070631593465805, Final Batch Loss: 0.419711172580719\n",
      "Epoch 4934, Loss: 2.058692842721939, Final Batch Loss: 0.33980032801628113\n",
      "Epoch 4935, Loss: 2.0431515276432037, Final Batch Loss: 0.35832634568214417\n",
      "Epoch 4936, Loss: 2.021965444087982, Final Batch Loss: 0.3762313425540924\n",
      "Epoch 4937, Loss: 2.205214649438858, Final Batch Loss: 0.42963841557502747\n",
      "Epoch 4938, Loss: 2.100884199142456, Final Batch Loss: 0.4842193126678467\n",
      "Epoch 4939, Loss: 2.0813902020454407, Final Batch Loss: 0.5355138778686523\n",
      "Epoch 4940, Loss: 1.8515382707118988, Final Batch Loss: 0.3923635482788086\n",
      "Epoch 4941, Loss: 2.1776314079761505, Final Batch Loss: 0.4441130459308624\n",
      "Epoch 4942, Loss: 2.036576807498932, Final Batch Loss: 0.409335732460022\n",
      "Epoch 4943, Loss: 2.1340360939502716, Final Batch Loss: 0.46633437275886536\n",
      "Epoch 4944, Loss: 1.9409353733062744, Final Batch Loss: 0.38483694195747375\n",
      "Epoch 4945, Loss: 2.2276832163333893, Final Batch Loss: 0.3814448118209839\n",
      "Epoch 4946, Loss: 2.074071317911148, Final Batch Loss: 0.3433237671852112\n",
      "Epoch 4947, Loss: 2.1366136968135834, Final Batch Loss: 0.4911954700946808\n",
      "Epoch 4948, Loss: 2.162793457508087, Final Batch Loss: 0.5065404176712036\n",
      "Epoch 4949, Loss: 2.01015168428421, Final Batch Loss: 0.38038021326065063\n",
      "Epoch 4950, Loss: 1.9667634963989258, Final Batch Loss: 0.3607156574726105\n",
      "Epoch 4951, Loss: 1.9984955787658691, Final Batch Loss: 0.3776687979698181\n",
      "Epoch 4952, Loss: 1.9920893609523773, Final Batch Loss: 0.3111782968044281\n",
      "Epoch 4953, Loss: 1.9974656403064728, Final Batch Loss: 0.4066280126571655\n",
      "Epoch 4954, Loss: 2.1208027005195618, Final Batch Loss: 0.3396393358707428\n",
      "Epoch 4955, Loss: 2.1595765352249146, Final Batch Loss: 0.4192728102207184\n",
      "Epoch 4956, Loss: 2.060165286064148, Final Batch Loss: 0.45365849137306213\n",
      "Epoch 4957, Loss: 2.0782030820846558, Final Batch Loss: 0.39904409646987915\n",
      "Epoch 4958, Loss: 2.1162764728069305, Final Batch Loss: 0.36224666237831116\n",
      "Epoch 4959, Loss: 2.067487359046936, Final Batch Loss: 0.32313263416290283\n",
      "Epoch 4960, Loss: 2.255088448524475, Final Batch Loss: 0.42054483294487\n",
      "Epoch 4961, Loss: 2.118020683526993, Final Batch Loss: 0.41178256273269653\n",
      "Epoch 4962, Loss: 2.179837793111801, Final Batch Loss: 0.4193660616874695\n",
      "Epoch 4963, Loss: 2.2511604726314545, Final Batch Loss: 0.5194451212882996\n",
      "Epoch 4964, Loss: 2.1587653160095215, Final Batch Loss: 0.5106678605079651\n",
      "Epoch 4965, Loss: 1.9982811510562897, Final Batch Loss: 0.3531210422515869\n",
      "Epoch 4966, Loss: 2.191512703895569, Final Batch Loss: 0.4123322069644928\n",
      "Epoch 4967, Loss: 2.2754150331020355, Final Batch Loss: 0.5082049369812012\n",
      "Epoch 4968, Loss: 2.0767080187797546, Final Batch Loss: 0.44388458132743835\n",
      "Epoch 4969, Loss: 2.0733211636543274, Final Batch Loss: 0.4029468894004822\n",
      "Epoch 4970, Loss: 1.8515294194221497, Final Batch Loss: 0.31218138337135315\n",
      "Epoch 4971, Loss: 2.1901424527168274, Final Batch Loss: 0.4454767107963562\n",
      "Epoch 4972, Loss: 2.1201415956020355, Final Batch Loss: 0.40372809767723083\n",
      "Epoch 4973, Loss: 1.925267606973648, Final Batch Loss: 0.362479031085968\n",
      "Epoch 4974, Loss: 2.0666381120681763, Final Batch Loss: 0.5044952630996704\n",
      "Epoch 4975, Loss: 2.136273205280304, Final Batch Loss: 0.41218551993370056\n",
      "Epoch 4976, Loss: 2.163861960172653, Final Batch Loss: 0.48456069827079773\n",
      "Epoch 4977, Loss: 1.9013908207416534, Final Batch Loss: 0.3737908899784088\n",
      "Epoch 4978, Loss: 2.1019429862499237, Final Batch Loss: 0.46018192172050476\n",
      "Epoch 4979, Loss: 2.0627059042453766, Final Batch Loss: 0.5432116985321045\n",
      "Epoch 4980, Loss: 1.9891858398914337, Final Batch Loss: 0.4351663291454315\n",
      "Epoch 4981, Loss: 2.171654313802719, Final Batch Loss: 0.3932051360607147\n",
      "Epoch 4982, Loss: 2.0960816144943237, Final Batch Loss: 0.5181945562362671\n",
      "Epoch 4983, Loss: 2.0447099804878235, Final Batch Loss: 0.49210941791534424\n",
      "Epoch 4984, Loss: 2.168068140745163, Final Batch Loss: 0.421740859746933\n",
      "Epoch 4985, Loss: 1.962380975484848, Final Batch Loss: 0.3121134340763092\n",
      "Epoch 4986, Loss: 2.3518782258033752, Final Batch Loss: 0.4793051779270172\n",
      "Epoch 4987, Loss: 2.1123549044132233, Final Batch Loss: 0.4151294231414795\n",
      "Epoch 4988, Loss: 1.9404098987579346, Final Batch Loss: 0.343495637178421\n",
      "Epoch 4989, Loss: 2.134231775999069, Final Batch Loss: 0.2931657135486603\n",
      "Epoch 4990, Loss: 2.1452111899852753, Final Batch Loss: 0.41152074933052063\n",
      "Epoch 4991, Loss: 1.9401700496673584, Final Batch Loss: 0.3396565318107605\n",
      "Epoch 4992, Loss: 2.083774358034134, Final Batch Loss: 0.35685649514198303\n",
      "Epoch 4993, Loss: 2.2527368366718292, Final Batch Loss: 0.4952073097229004\n",
      "Epoch 4994, Loss: 2.2234910428524017, Final Batch Loss: 0.5137500762939453\n",
      "Epoch 4995, Loss: 2.0173676908016205, Final Batch Loss: 0.30914896726608276\n",
      "Epoch 4996, Loss: 1.9302367866039276, Final Batch Loss: 0.3259940445423126\n",
      "Epoch 4997, Loss: 2.058129847049713, Final Batch Loss: 0.4064685106277466\n",
      "Epoch 4998, Loss: 2.060299038887024, Final Batch Loss: 0.3891206681728363\n",
      "Epoch 4999, Loss: 2.1745599806308746, Final Batch Loss: 0.4162620007991791\n",
      "Epoch 5000, Loss: 2.142549842596054, Final Batch Loss: 0.480001300573349\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model_subject(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  1  0  0  1  0  0  0  2]\n",
      " [ 0 19  0  1  0  3  0  2  3]\n",
      " [ 0  0 16  0  0  0  0  0  0]\n",
      " [ 0  0  0 42  0  0  1  0  0]\n",
      " [ 1  4  1  1 23  0  0  0  4]\n",
      " [ 0  0  1  0  4 16  0  0  0]\n",
      " [ 0  1  3  2  1  0 25  1  0]\n",
      " [ 0  1  0  1  0  0  0 36  1]\n",
      " [ 1  0  1  0  0  0  0  0 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.95122   0.90698   0.92857        43\n",
      "           1    0.73077   0.67857   0.70370        28\n",
      "           2    0.72727   1.00000   0.84211        16\n",
      "           3    0.89362   0.97674   0.93333        43\n",
      "           4    0.79310   0.67647   0.73016        34\n",
      "           5    0.84211   0.76190   0.80000        21\n",
      "           6    0.96154   0.75758   0.84746        33\n",
      "           7    0.92308   0.92308   0.92308        39\n",
      "           8    0.73684   0.93333   0.82353        30\n",
      "\n",
      "    accuracy                        0.85017       287\n",
      "   macro avg    0.83995   0.84607   0.83688       287\n",
      "weighted avg    0.85683   0.85017   0.84856       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model_subject.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model_subject(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  0  0  0  0  0  0  0  0]\n",
      " [ 0 26  0  0  0  0  0  0  0]\n",
      " [ 0  0 22  0  0  0  0  0  0]\n",
      " [ 0  0  0 28  0  0  0  5  0]\n",
      " [ 0 11  0  0 15  0  0  0  9]\n",
      " [ 0  0  0  0  0 31  0  0  0]\n",
      " [ 0  0  0  0  0  0 25  0  0]\n",
      " [ 0  0  0  0  0  0  0 42  0]\n",
      " [ 0  0  0  0  0  0  0  0 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        36\n",
      "           1    0.70270   1.00000   0.82540        26\n",
      "           2    1.00000   1.00000   1.00000        22\n",
      "           3    1.00000   0.84848   0.91803        33\n",
      "           4    1.00000   0.42857   0.60000        35\n",
      "           5    1.00000   1.00000   1.00000        31\n",
      "           6    1.00000   1.00000   1.00000        25\n",
      "           7    0.89362   1.00000   0.94382        42\n",
      "           8    0.80435   1.00000   0.89157        37\n",
      "\n",
      "    accuracy                        0.91289       287\n",
      "   macro avg    0.93341   0.91967   0.90876       287\n",
      "weighted avg    0.93228   0.91289   0.90378       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model_subject(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix(usr_vectors[0], preds.cpu()))\n",
    "print(metrics.classification_report(usr_vectors[0], preds.cpu(), digits = 5, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
