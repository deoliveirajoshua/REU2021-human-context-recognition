{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>475 fBodyGyro-bandsEnergy()-1,8</th>\n",
       "      <th>...</th>\n",
       "      <th>272 fBodyAcc-mad()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996889</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997890</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994097</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994547</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.999926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997725</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>-0.053258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050748</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177661</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.161404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249486</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247028</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114475</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  475 fBodyGyro-bandsEnergy()-1,8  ...  \\\n",
       "0                     -0.975510                        -0.999454  ...   \n",
       "1                     -0.978500                        -0.999856  ...   \n",
       "2                     -0.981672                        -0.999954  ...   \n",
       "3                     -0.982420                        -0.999931  ...   \n",
       "4                     -0.984363                        -0.999926  ...   \n",
       "...                         ...                              ...  ...   \n",
       "7347                  -0.995193                        -0.053258  ...   \n",
       "7348                  -0.995151                        -0.029411  ...   \n",
       "7349                  -0.995450                         0.161404  ...   \n",
       "7350                  -0.998824                         0.193585  ...   \n",
       "7351                  -0.998144                        -0.129277  ...   \n",
       "\n",
       "      272 fBodyAcc-mad()-X  282 fBodyAcc-energy()-X  \\\n",
       "0                -0.996889                -0.999968   \n",
       "1                -0.997890                -0.999991   \n",
       "2                -0.994097                -0.999969   \n",
       "3                -0.994547                -0.999975   \n",
       "4                -0.997725                -0.999990   \n",
       "...                    ...                      ...   \n",
       "7347             -0.050748                -0.674230   \n",
       "7348             -0.177661                -0.705580   \n",
       "7349             -0.249486                -0.692379   \n",
       "7350             -0.247028                -0.693098   \n",
       "7351             -0.114475                -0.731037   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                 -0.948870                 -0.998285        1         5  \n",
       "1                 -0.975777                 -0.999472        1         5  \n",
       "2                 -0.985594                 -0.999807        1         5  \n",
       "3                 -0.983524                 -0.999770        1         5  \n",
       "4                 -0.992324                 -0.999873        1         5  \n",
       "...                     ...                       ...      ...       ...  \n",
       "7347              -0.007392                 -0.584282       30         2  \n",
       "7348              -0.172448                 -0.632536       30         2  \n",
       "7349              -0.216074                 -0.641170       30         2  \n",
       "7350              -0.220443                 -0.663579       30         2  \n",
       "7351              -0.146649                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14, 17, 19])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14, 17, 19])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    elif y_train[k] == 5:\n",
    "        y_train[k] = 2\n",
    "    elif y_train[k] == 7:\n",
    "        y_train[k] = 3\n",
    "    elif y_train[k] == 8:\n",
    "        y_train[k] = 4\n",
    "    elif y_train[k] == 11:\n",
    "        y_train[k] = 5\n",
    "    elif y_train[k] == 14:\n",
    "        y_train[k] = 6\n",
    "    elif y_train[k] == 17:\n",
    "        y_train[k] = 7\n",
    "    else:\n",
    "        y_train[k] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 7500\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.062401056289673, Final Batch Loss: 2.206573009490967\n",
      "Epoch 2, Loss: 11.047949314117432, Final Batch Loss: 2.20697283744812\n",
      "Epoch 3, Loss: 11.031243562698364, Final Batch Loss: 2.213114023208618\n",
      "Epoch 4, Loss: 11.01078987121582, Final Batch Loss: 2.1969521045684814\n",
      "Epoch 5, Loss: 10.995674133300781, Final Batch Loss: 2.1987383365631104\n",
      "Epoch 6, Loss: 10.975208520889282, Final Batch Loss: 2.194032669067383\n",
      "Epoch 7, Loss: 10.942647695541382, Final Batch Loss: 2.1809425354003906\n",
      "Epoch 8, Loss: 10.914919137954712, Final Batch Loss: 2.1807444095611572\n",
      "Epoch 9, Loss: 10.878669261932373, Final Batch Loss: 2.1838366985321045\n",
      "Epoch 10, Loss: 10.84848666191101, Final Batch Loss: 2.163916826248169\n",
      "Epoch 11, Loss: 10.804754495620728, Final Batch Loss: 2.1554720401763916\n",
      "Epoch 12, Loss: 10.783970355987549, Final Batch Loss: 2.1681947708129883\n",
      "Epoch 13, Loss: 10.701191186904907, Final Batch Loss: 2.124854564666748\n",
      "Epoch 14, Loss: 10.644819498062134, Final Batch Loss: 2.139167308807373\n",
      "Epoch 15, Loss: 10.570288181304932, Final Batch Loss: 2.096000909805298\n",
      "Epoch 16, Loss: 10.415396213531494, Final Batch Loss: 2.0686349868774414\n",
      "Epoch 17, Loss: 10.388126134872437, Final Batch Loss: 2.0549538135528564\n",
      "Epoch 18, Loss: 10.271119117736816, Final Batch Loss: 2.0262680053710938\n",
      "Epoch 19, Loss: 10.197502374649048, Final Batch Loss: 2.033510446548462\n",
      "Epoch 20, Loss: 10.153302431106567, Final Batch Loss: 2.0499420166015625\n",
      "Epoch 21, Loss: 10.08456563949585, Final Batch Loss: 2.0387089252471924\n",
      "Epoch 22, Loss: 9.964965462684631, Final Batch Loss: 2.021472454071045\n",
      "Epoch 23, Loss: 9.89987862110138, Final Batch Loss: 1.9409979581832886\n",
      "Epoch 24, Loss: 9.821226358413696, Final Batch Loss: 2.0041728019714355\n",
      "Epoch 25, Loss: 9.643488883972168, Final Batch Loss: 1.885909080505371\n",
      "Epoch 26, Loss: 9.611942887306213, Final Batch Loss: 1.9181013107299805\n",
      "Epoch 27, Loss: 9.558192014694214, Final Batch Loss: 1.879767656326294\n",
      "Epoch 28, Loss: 9.593703866004944, Final Batch Loss: 1.8885653018951416\n",
      "Epoch 29, Loss: 9.463419914245605, Final Batch Loss: 1.9197784662246704\n",
      "Epoch 30, Loss: 9.390241980552673, Final Batch Loss: 1.7977323532104492\n",
      "Epoch 31, Loss: 9.319372773170471, Final Batch Loss: 1.8908714056015015\n",
      "Epoch 32, Loss: 9.23362147808075, Final Batch Loss: 1.8183133602142334\n",
      "Epoch 33, Loss: 9.30050539970398, Final Batch Loss: 1.889073371887207\n",
      "Epoch 34, Loss: 9.201749801635742, Final Batch Loss: 1.834937572479248\n",
      "Epoch 35, Loss: 9.207196354866028, Final Batch Loss: 1.8410115242004395\n",
      "Epoch 36, Loss: 9.137277364730835, Final Batch Loss: 1.7825855016708374\n",
      "Epoch 37, Loss: 9.086069345474243, Final Batch Loss: 1.8292087316513062\n",
      "Epoch 38, Loss: 8.918408870697021, Final Batch Loss: 1.8358139991760254\n",
      "Epoch 39, Loss: 8.815815329551697, Final Batch Loss: 1.798993468284607\n",
      "Epoch 40, Loss: 8.94726014137268, Final Batch Loss: 1.792766809463501\n",
      "Epoch 41, Loss: 8.793355226516724, Final Batch Loss: 1.7906450033187866\n",
      "Epoch 42, Loss: 8.692876935005188, Final Batch Loss: 1.669729471206665\n",
      "Epoch 43, Loss: 8.706408619880676, Final Batch Loss: 1.6642000675201416\n",
      "Epoch 44, Loss: 8.688811540603638, Final Batch Loss: 1.7353428602218628\n",
      "Epoch 45, Loss: 8.556793570518494, Final Batch Loss: 1.653275489807129\n",
      "Epoch 46, Loss: 8.523959517478943, Final Batch Loss: 1.693176507949829\n",
      "Epoch 47, Loss: 8.438658356666565, Final Batch Loss: 1.5829516649246216\n",
      "Epoch 48, Loss: 8.431372046470642, Final Batch Loss: 1.67141854763031\n",
      "Epoch 49, Loss: 8.38877284526825, Final Batch Loss: 1.6345196962356567\n",
      "Epoch 50, Loss: 8.399963617324829, Final Batch Loss: 1.6485180854797363\n",
      "Epoch 51, Loss: 8.43814992904663, Final Batch Loss: 1.7807362079620361\n",
      "Epoch 52, Loss: 8.225807785987854, Final Batch Loss: 1.633481502532959\n",
      "Epoch 53, Loss: 8.250667095184326, Final Batch Loss: 1.7163746356964111\n",
      "Epoch 54, Loss: 8.237429141998291, Final Batch Loss: 1.6739710569381714\n",
      "Epoch 55, Loss: 8.226191282272339, Final Batch Loss: 1.5732189416885376\n",
      "Epoch 56, Loss: 8.107388019561768, Final Batch Loss: 1.5328283309936523\n",
      "Epoch 57, Loss: 8.13484799861908, Final Batch Loss: 1.5802547931671143\n",
      "Epoch 58, Loss: 8.014544248580933, Final Batch Loss: 1.5059021711349487\n",
      "Epoch 59, Loss: 7.884026646614075, Final Batch Loss: 1.5912233591079712\n",
      "Epoch 60, Loss: 7.904991984367371, Final Batch Loss: 1.6130070686340332\n",
      "Epoch 61, Loss: 7.988104462623596, Final Batch Loss: 1.6876420974731445\n",
      "Epoch 62, Loss: 7.954840898513794, Final Batch Loss: 1.531385064125061\n",
      "Epoch 63, Loss: 7.801186203956604, Final Batch Loss: 1.568630576133728\n",
      "Epoch 64, Loss: 7.90625, Final Batch Loss: 1.5506247282028198\n",
      "Epoch 65, Loss: 7.807435750961304, Final Batch Loss: 1.5344877243041992\n",
      "Epoch 66, Loss: 7.741651296615601, Final Batch Loss: 1.4808216094970703\n",
      "Epoch 67, Loss: 7.730041265487671, Final Batch Loss: 1.5947829484939575\n",
      "Epoch 68, Loss: 7.540795803070068, Final Batch Loss: 1.4853694438934326\n",
      "Epoch 69, Loss: 7.700472116470337, Final Batch Loss: 1.489128589630127\n",
      "Epoch 70, Loss: 7.730968475341797, Final Batch Loss: 1.5257399082183838\n",
      "Epoch 71, Loss: 7.546892166137695, Final Batch Loss: 1.4489117860794067\n",
      "Epoch 72, Loss: 7.566029787063599, Final Batch Loss: 1.5385856628417969\n",
      "Epoch 73, Loss: 7.428124308586121, Final Batch Loss: 1.4782477617263794\n",
      "Epoch 74, Loss: 7.5707807540893555, Final Batch Loss: 1.4980376958847046\n",
      "Epoch 75, Loss: 7.413101077079773, Final Batch Loss: 1.5087013244628906\n",
      "Epoch 76, Loss: 7.383159756660461, Final Batch Loss: 1.4143385887145996\n",
      "Epoch 77, Loss: 7.411512494087219, Final Batch Loss: 1.5144847631454468\n",
      "Epoch 78, Loss: 7.328059434890747, Final Batch Loss: 1.46184504032135\n",
      "Epoch 79, Loss: 7.315860748291016, Final Batch Loss: 1.4547743797302246\n",
      "Epoch 80, Loss: 7.456152558326721, Final Batch Loss: 1.5100806951522827\n",
      "Epoch 81, Loss: 7.357902526855469, Final Batch Loss: 1.4431533813476562\n",
      "Epoch 82, Loss: 7.229219675064087, Final Batch Loss: 1.4596259593963623\n",
      "Epoch 83, Loss: 7.3778791427612305, Final Batch Loss: 1.496140956878662\n",
      "Epoch 84, Loss: 7.263601899147034, Final Batch Loss: 1.491126537322998\n",
      "Epoch 85, Loss: 7.300440430641174, Final Batch Loss: 1.5142745971679688\n",
      "Epoch 86, Loss: 7.224821448326111, Final Batch Loss: 1.4613091945648193\n",
      "Epoch 87, Loss: 7.271632671356201, Final Batch Loss: 1.4496192932128906\n",
      "Epoch 88, Loss: 7.09110152721405, Final Batch Loss: 1.3569166660308838\n",
      "Epoch 89, Loss: 7.230826497077942, Final Batch Loss: 1.4485414028167725\n",
      "Epoch 90, Loss: 7.228277683258057, Final Batch Loss: 1.458621859550476\n",
      "Epoch 91, Loss: 7.091557264328003, Final Batch Loss: 1.4323087930679321\n",
      "Epoch 92, Loss: 7.033116698265076, Final Batch Loss: 1.3602216243743896\n",
      "Epoch 93, Loss: 7.216788649559021, Final Batch Loss: 1.4354499578475952\n",
      "Epoch 94, Loss: 7.073536515235901, Final Batch Loss: 1.3856186866760254\n",
      "Epoch 95, Loss: 7.0966397523880005, Final Batch Loss: 1.3809564113616943\n",
      "Epoch 96, Loss: 7.07795250415802, Final Batch Loss: 1.3887680768966675\n",
      "Epoch 97, Loss: 7.0155088901519775, Final Batch Loss: 1.446305751800537\n",
      "Epoch 98, Loss: 7.039673089981079, Final Batch Loss: 1.4346721172332764\n",
      "Epoch 99, Loss: 7.001977205276489, Final Batch Loss: 1.4732557535171509\n",
      "Epoch 100, Loss: 6.851367115974426, Final Batch Loss: 1.3960860967636108\n",
      "Epoch 101, Loss: 6.766597270965576, Final Batch Loss: 1.38771390914917\n",
      "Epoch 102, Loss: 6.738576650619507, Final Batch Loss: 1.4090067148208618\n",
      "Epoch 103, Loss: 6.846735954284668, Final Batch Loss: 1.5336217880249023\n",
      "Epoch 104, Loss: 6.769903898239136, Final Batch Loss: 1.4369444847106934\n",
      "Epoch 105, Loss: 6.7363035678863525, Final Batch Loss: 1.3074870109558105\n",
      "Epoch 106, Loss: 6.686734318733215, Final Batch Loss: 1.3209460973739624\n",
      "Epoch 107, Loss: 6.7429739236831665, Final Batch Loss: 1.311753749847412\n",
      "Epoch 108, Loss: 6.585190653800964, Final Batch Loss: 1.2717152833938599\n",
      "Epoch 109, Loss: 6.567805886268616, Final Batch Loss: 1.335035800933838\n",
      "Epoch 110, Loss: 6.555616497993469, Final Batch Loss: 1.3232671022415161\n",
      "Epoch 111, Loss: 6.752645492553711, Final Batch Loss: 1.328905701637268\n",
      "Epoch 112, Loss: 6.485550880432129, Final Batch Loss: 1.3506221771240234\n",
      "Epoch 113, Loss: 6.544318318367004, Final Batch Loss: 1.347066879272461\n",
      "Epoch 114, Loss: 6.558940887451172, Final Batch Loss: 1.2884804010391235\n",
      "Epoch 115, Loss: 6.579524517059326, Final Batch Loss: 1.2695882320404053\n",
      "Epoch 116, Loss: 6.45307469367981, Final Batch Loss: 1.2428596019744873\n",
      "Epoch 117, Loss: 6.544125199317932, Final Batch Loss: 1.334737777709961\n",
      "Epoch 118, Loss: 6.486969351768494, Final Batch Loss: 1.2551747560501099\n",
      "Epoch 119, Loss: 6.517108201980591, Final Batch Loss: 1.3184492588043213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 6.510164141654968, Final Batch Loss: 1.2849467992782593\n",
      "Epoch 121, Loss: 6.531868815422058, Final Batch Loss: 1.4228031635284424\n",
      "Epoch 122, Loss: 6.40522563457489, Final Batch Loss: 1.2522951364517212\n",
      "Epoch 123, Loss: 6.377614617347717, Final Batch Loss: 1.3634977340698242\n",
      "Epoch 124, Loss: 6.299414873123169, Final Batch Loss: 1.2905850410461426\n",
      "Epoch 125, Loss: 6.339807987213135, Final Batch Loss: 1.2545335292816162\n",
      "Epoch 126, Loss: 6.367006778717041, Final Batch Loss: 1.231135606765747\n",
      "Epoch 127, Loss: 6.168930411338806, Final Batch Loss: 1.198753833770752\n",
      "Epoch 128, Loss: 6.301113963127136, Final Batch Loss: 1.2911702394485474\n",
      "Epoch 129, Loss: 6.331954836845398, Final Batch Loss: 1.2811098098754883\n",
      "Epoch 130, Loss: 6.242249011993408, Final Batch Loss: 1.2198376655578613\n",
      "Epoch 131, Loss: 6.228593111038208, Final Batch Loss: 1.2517372369766235\n",
      "Epoch 132, Loss: 6.157120704650879, Final Batch Loss: 1.3363173007965088\n",
      "Epoch 133, Loss: 6.05823278427124, Final Batch Loss: 1.1754473447799683\n",
      "Epoch 134, Loss: 6.198566198348999, Final Batch Loss: 1.2260843515396118\n",
      "Epoch 135, Loss: 6.094865322113037, Final Batch Loss: 1.268633246421814\n",
      "Epoch 136, Loss: 6.153919816017151, Final Batch Loss: 1.2259767055511475\n",
      "Epoch 137, Loss: 6.114351272583008, Final Batch Loss: 1.1588454246520996\n",
      "Epoch 138, Loss: 6.2510154247283936, Final Batch Loss: 1.222230076789856\n",
      "Epoch 139, Loss: 6.1902488470077515, Final Batch Loss: 1.3217008113861084\n",
      "Epoch 140, Loss: 6.080758333206177, Final Batch Loss: 1.1971880197525024\n",
      "Epoch 141, Loss: 6.069360017776489, Final Batch Loss: 1.221701741218567\n",
      "Epoch 142, Loss: 6.187801718711853, Final Batch Loss: 1.203714370727539\n",
      "Epoch 143, Loss: 6.085192084312439, Final Batch Loss: 1.1429485082626343\n",
      "Epoch 144, Loss: 6.017408013343811, Final Batch Loss: 1.183511734008789\n",
      "Epoch 145, Loss: 6.015959143638611, Final Batch Loss: 1.1742184162139893\n",
      "Epoch 146, Loss: 6.049663424491882, Final Batch Loss: 1.2932205200195312\n",
      "Epoch 147, Loss: 6.056159615516663, Final Batch Loss: 1.1896854639053345\n",
      "Epoch 148, Loss: 5.930288195610046, Final Batch Loss: 1.1790494918823242\n",
      "Epoch 149, Loss: 5.947115182876587, Final Batch Loss: 1.180712342262268\n",
      "Epoch 150, Loss: 5.97920548915863, Final Batch Loss: 1.1030265092849731\n",
      "Epoch 151, Loss: 5.91365373134613, Final Batch Loss: 1.2089974880218506\n",
      "Epoch 152, Loss: 6.013290882110596, Final Batch Loss: 1.192318320274353\n",
      "Epoch 153, Loss: 5.83894157409668, Final Batch Loss: 1.1542384624481201\n",
      "Epoch 154, Loss: 5.960818886756897, Final Batch Loss: 1.2500079870224\n",
      "Epoch 155, Loss: 5.812536716461182, Final Batch Loss: 1.1502056121826172\n",
      "Epoch 156, Loss: 6.000925421714783, Final Batch Loss: 1.295421838760376\n",
      "Epoch 157, Loss: 5.898995518684387, Final Batch Loss: 1.1079552173614502\n",
      "Epoch 158, Loss: 5.986437082290649, Final Batch Loss: 1.184208869934082\n",
      "Epoch 159, Loss: 5.856949687004089, Final Batch Loss: 1.2258687019348145\n",
      "Epoch 160, Loss: 5.750234842300415, Final Batch Loss: 1.1611028909683228\n",
      "Epoch 161, Loss: 5.698508024215698, Final Batch Loss: 1.125820517539978\n",
      "Epoch 162, Loss: 5.84989607334137, Final Batch Loss: 1.2024073600769043\n",
      "Epoch 163, Loss: 5.914513945579529, Final Batch Loss: 1.1768537759780884\n",
      "Epoch 164, Loss: 5.829659581184387, Final Batch Loss: 1.199695348739624\n",
      "Epoch 165, Loss: 5.923367381095886, Final Batch Loss: 1.2590185403823853\n",
      "Epoch 166, Loss: 5.8457350730896, Final Batch Loss: 1.0897057056427002\n",
      "Epoch 167, Loss: 5.650625467300415, Final Batch Loss: 1.094008207321167\n",
      "Epoch 168, Loss: 5.752439260482788, Final Batch Loss: 1.0395697355270386\n",
      "Epoch 169, Loss: 5.868533730506897, Final Batch Loss: 1.2403346300125122\n",
      "Epoch 170, Loss: 5.863070487976074, Final Batch Loss: 1.1721824407577515\n",
      "Epoch 171, Loss: 5.599464654922485, Final Batch Loss: 1.076080083847046\n",
      "Epoch 172, Loss: 5.7716275453567505, Final Batch Loss: 1.2211143970489502\n",
      "Epoch 173, Loss: 5.699573755264282, Final Batch Loss: 1.1387901306152344\n",
      "Epoch 174, Loss: 5.805227994918823, Final Batch Loss: 1.1255327463150024\n",
      "Epoch 175, Loss: 5.751230776309967, Final Batch Loss: 1.2018711566925049\n",
      "Epoch 176, Loss: 5.595983386039734, Final Batch Loss: 1.098198652267456\n",
      "Epoch 177, Loss: 5.557497143745422, Final Batch Loss: 1.1171455383300781\n",
      "Epoch 178, Loss: 5.514875292778015, Final Batch Loss: 1.1128239631652832\n",
      "Epoch 179, Loss: 5.619242310523987, Final Batch Loss: 1.1473370790481567\n",
      "Epoch 180, Loss: 5.483179807662964, Final Batch Loss: 1.0126619338989258\n",
      "Epoch 181, Loss: 5.553874611854553, Final Batch Loss: 1.1102029085159302\n",
      "Epoch 182, Loss: 5.653466463088989, Final Batch Loss: 1.1856969594955444\n",
      "Epoch 183, Loss: 5.7092859745025635, Final Batch Loss: 1.0495412349700928\n",
      "Epoch 184, Loss: 5.5345340967178345, Final Batch Loss: 1.1828550100326538\n",
      "Epoch 185, Loss: 5.68669581413269, Final Batch Loss: 1.2075880765914917\n",
      "Epoch 186, Loss: 5.587966024875641, Final Batch Loss: 1.2072933912277222\n",
      "Epoch 187, Loss: 5.638826489448547, Final Batch Loss: 1.1149098873138428\n",
      "Epoch 188, Loss: 5.71655547618866, Final Batch Loss: 1.1853121519088745\n",
      "Epoch 189, Loss: 5.537909269332886, Final Batch Loss: 1.0891022682189941\n",
      "Epoch 190, Loss: 5.708028197288513, Final Batch Loss: 1.1866207122802734\n",
      "Epoch 191, Loss: 5.566579222679138, Final Batch Loss: 1.1487672328948975\n",
      "Epoch 192, Loss: 5.5448373556137085, Final Batch Loss: 1.1347904205322266\n",
      "Epoch 193, Loss: 5.6306434869766235, Final Batch Loss: 1.090194821357727\n",
      "Epoch 194, Loss: 5.628048062324524, Final Batch Loss: 1.0100194215774536\n",
      "Epoch 195, Loss: 5.5918519496917725, Final Batch Loss: 1.097200632095337\n",
      "Epoch 196, Loss: 5.583902955055237, Final Batch Loss: 1.1564433574676514\n",
      "Epoch 197, Loss: 5.538546562194824, Final Batch Loss: 1.20973539352417\n",
      "Epoch 198, Loss: 5.572482585906982, Final Batch Loss: 1.1147074699401855\n",
      "Epoch 199, Loss: 5.38767945766449, Final Batch Loss: 1.030712604522705\n",
      "Epoch 200, Loss: 5.509792923927307, Final Batch Loss: 1.147618293762207\n",
      "Epoch 201, Loss: 5.284860253334045, Final Batch Loss: 1.0255674123764038\n",
      "Epoch 202, Loss: 5.417873919010162, Final Batch Loss: 1.152692437171936\n",
      "Epoch 203, Loss: 5.546128988265991, Final Batch Loss: 1.140895962715149\n",
      "Epoch 204, Loss: 5.5220032930374146, Final Batch Loss: 1.1078791618347168\n",
      "Epoch 205, Loss: 5.4079588651657104, Final Batch Loss: 1.1174219846725464\n",
      "Epoch 206, Loss: 5.312011241912842, Final Batch Loss: 1.093061923980713\n",
      "Epoch 207, Loss: 5.532158017158508, Final Batch Loss: 1.169246792793274\n",
      "Epoch 208, Loss: 5.528459072113037, Final Batch Loss: 1.0960423946380615\n",
      "Epoch 209, Loss: 5.269831299781799, Final Batch Loss: 1.023468255996704\n",
      "Epoch 210, Loss: 5.397881627082825, Final Batch Loss: 1.1744980812072754\n",
      "Epoch 211, Loss: 5.3433603048324585, Final Batch Loss: 1.1543796062469482\n",
      "Epoch 212, Loss: 5.444662570953369, Final Batch Loss: 1.1418607234954834\n",
      "Epoch 213, Loss: 5.4066321849823, Final Batch Loss: 1.057379961013794\n",
      "Epoch 214, Loss: 5.437554478645325, Final Batch Loss: 1.1867234706878662\n",
      "Epoch 215, Loss: 5.392174124717712, Final Batch Loss: 1.120202660560608\n",
      "Epoch 216, Loss: 5.235808372497559, Final Batch Loss: 1.1051979064941406\n",
      "Epoch 217, Loss: 5.2892709374427795, Final Batch Loss: 1.0442347526550293\n",
      "Epoch 218, Loss: 5.34882527589798, Final Batch Loss: 1.1114187240600586\n",
      "Epoch 219, Loss: 5.306523323059082, Final Batch Loss: 1.0630130767822266\n",
      "Epoch 220, Loss: 5.362685918807983, Final Batch Loss: 1.0827860832214355\n",
      "Epoch 221, Loss: 5.133445560932159, Final Batch Loss: 1.038389801979065\n",
      "Epoch 222, Loss: 5.349039733409882, Final Batch Loss: 0.8958823084831238\n",
      "Epoch 223, Loss: 5.215769708156586, Final Batch Loss: 0.9893375039100647\n",
      "Epoch 224, Loss: 5.361861944198608, Final Batch Loss: 1.0751283168792725\n",
      "Epoch 225, Loss: 5.215699017047882, Final Batch Loss: 1.0714110136032104\n",
      "Epoch 226, Loss: 5.416668057441711, Final Batch Loss: 1.1772828102111816\n",
      "Epoch 227, Loss: 5.280765652656555, Final Batch Loss: 1.1022294759750366\n",
      "Epoch 228, Loss: 5.169312298297882, Final Batch Loss: 1.049463152885437\n",
      "Epoch 229, Loss: 5.378067672252655, Final Batch Loss: 0.979170024394989\n",
      "Epoch 230, Loss: 5.210789442062378, Final Batch Loss: 1.0023701190948486\n",
      "Epoch 231, Loss: 5.486730694770813, Final Batch Loss: 1.0366131067276\n",
      "Epoch 232, Loss: 5.343472838401794, Final Batch Loss: 1.0615785121917725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233, Loss: 5.233388245105743, Final Batch Loss: 1.0708333253860474\n",
      "Epoch 234, Loss: 5.409892678260803, Final Batch Loss: 1.1055485010147095\n",
      "Epoch 235, Loss: 5.148668050765991, Final Batch Loss: 1.1084096431732178\n",
      "Epoch 236, Loss: 5.2527337074279785, Final Batch Loss: 1.0462441444396973\n",
      "Epoch 237, Loss: 5.285170555114746, Final Batch Loss: 0.9205756187438965\n",
      "Epoch 238, Loss: 5.193244278430939, Final Batch Loss: 1.0352177619934082\n",
      "Epoch 239, Loss: 5.1697341203689575, Final Batch Loss: 0.9533716440200806\n",
      "Epoch 240, Loss: 5.315905451774597, Final Batch Loss: 1.0744231939315796\n",
      "Epoch 241, Loss: 5.213665008544922, Final Batch Loss: 1.0120534896850586\n",
      "Epoch 242, Loss: 5.12600439786911, Final Batch Loss: 0.9205324649810791\n",
      "Epoch 243, Loss: 5.084780156612396, Final Batch Loss: 1.0538547039031982\n",
      "Epoch 244, Loss: 5.13839066028595, Final Batch Loss: 1.0994629859924316\n",
      "Epoch 245, Loss: 5.115194618701935, Final Batch Loss: 0.9699789881706238\n",
      "Epoch 246, Loss: 5.179529905319214, Final Batch Loss: 1.0706738233566284\n",
      "Epoch 247, Loss: 5.094554662704468, Final Batch Loss: 0.9872809648513794\n",
      "Epoch 248, Loss: 4.996472001075745, Final Batch Loss: 0.9865033030509949\n",
      "Epoch 249, Loss: 5.119056046009064, Final Batch Loss: 1.060369610786438\n",
      "Epoch 250, Loss: 5.271936058998108, Final Batch Loss: 1.15962553024292\n",
      "Epoch 251, Loss: 5.193087697029114, Final Batch Loss: 1.1559654474258423\n",
      "Epoch 252, Loss: 5.029780864715576, Final Batch Loss: 0.9646536707878113\n",
      "Epoch 253, Loss: 5.3212698101997375, Final Batch Loss: 1.1327391862869263\n",
      "Epoch 254, Loss: 5.173499643802643, Final Batch Loss: 0.9797558188438416\n",
      "Epoch 255, Loss: 5.04500287771225, Final Batch Loss: 0.9653028249740601\n",
      "Epoch 256, Loss: 5.068487763404846, Final Batch Loss: 0.9782412648200989\n",
      "Epoch 257, Loss: 5.157794654369354, Final Batch Loss: 1.0524662733078003\n",
      "Epoch 258, Loss: 5.039418876171112, Final Batch Loss: 1.013668417930603\n",
      "Epoch 259, Loss: 5.081348180770874, Final Batch Loss: 0.9560686945915222\n",
      "Epoch 260, Loss: 4.958284676074982, Final Batch Loss: 0.972342312335968\n",
      "Epoch 261, Loss: 5.200170457363129, Final Batch Loss: 1.1103920936584473\n",
      "Epoch 262, Loss: 5.0760762095451355, Final Batch Loss: 1.0720709562301636\n",
      "Epoch 263, Loss: 5.056647837162018, Final Batch Loss: 0.9725934863090515\n",
      "Epoch 264, Loss: 4.928432941436768, Final Batch Loss: 0.9632306694984436\n",
      "Epoch 265, Loss: 4.939490079879761, Final Batch Loss: 0.9992613196372986\n",
      "Epoch 266, Loss: 4.994278848171234, Final Batch Loss: 0.8863728046417236\n",
      "Epoch 267, Loss: 5.118284046649933, Final Batch Loss: 0.9874475598335266\n",
      "Epoch 268, Loss: 5.081059157848358, Final Batch Loss: 1.1128147840499878\n",
      "Epoch 269, Loss: 5.106187105178833, Final Batch Loss: 0.9727162718772888\n",
      "Epoch 270, Loss: 5.008302688598633, Final Batch Loss: 1.0555295944213867\n",
      "Epoch 271, Loss: 5.16639769077301, Final Batch Loss: 1.052808165550232\n",
      "Epoch 272, Loss: 5.129615366458893, Final Batch Loss: 1.0047328472137451\n",
      "Epoch 273, Loss: 5.018036425113678, Final Batch Loss: 1.063852310180664\n",
      "Epoch 274, Loss: 5.127267360687256, Final Batch Loss: 1.0370893478393555\n",
      "Epoch 275, Loss: 4.862379431724548, Final Batch Loss: 0.9227753281593323\n",
      "Epoch 276, Loss: 4.969505310058594, Final Batch Loss: 0.9901596903800964\n",
      "Epoch 277, Loss: 5.172106504440308, Final Batch Loss: 0.9636238813400269\n",
      "Epoch 278, Loss: 4.94648414850235, Final Batch Loss: 0.9876241087913513\n",
      "Epoch 279, Loss: 5.029066801071167, Final Batch Loss: 1.0805054903030396\n",
      "Epoch 280, Loss: 5.09365838766098, Final Batch Loss: 0.9682785868644714\n",
      "Epoch 281, Loss: 4.995000600814819, Final Batch Loss: 0.9651410579681396\n",
      "Epoch 282, Loss: 5.021621763706207, Final Batch Loss: 1.0251035690307617\n",
      "Epoch 283, Loss: 5.054604113101959, Final Batch Loss: 0.9730764031410217\n",
      "Epoch 284, Loss: 5.013847231864929, Final Batch Loss: 0.8865658640861511\n",
      "Epoch 285, Loss: 5.028890371322632, Final Batch Loss: 1.0436396598815918\n",
      "Epoch 286, Loss: 4.886732637882233, Final Batch Loss: 0.9208840131759644\n",
      "Epoch 287, Loss: 4.918162286281586, Final Batch Loss: 0.9870690703392029\n",
      "Epoch 288, Loss: 4.947633326053619, Final Batch Loss: 1.0333553552627563\n",
      "Epoch 289, Loss: 4.966011703014374, Final Batch Loss: 1.054345726966858\n",
      "Epoch 290, Loss: 4.8970359563827515, Final Batch Loss: 0.9206452369689941\n",
      "Epoch 291, Loss: 4.806731939315796, Final Batch Loss: 0.9100525379180908\n",
      "Epoch 292, Loss: 4.863968551158905, Final Batch Loss: 0.9890429973602295\n",
      "Epoch 293, Loss: 4.882928133010864, Final Batch Loss: 0.8989196419715881\n",
      "Epoch 294, Loss: 5.056550323963165, Final Batch Loss: 0.877694308757782\n",
      "Epoch 295, Loss: 4.938309669494629, Final Batch Loss: 1.017183542251587\n",
      "Epoch 296, Loss: 4.960624575614929, Final Batch Loss: 1.0379531383514404\n",
      "Epoch 297, Loss: 5.015156924724579, Final Batch Loss: 1.0115035772323608\n",
      "Epoch 298, Loss: 4.901772856712341, Final Batch Loss: 0.8703895211219788\n",
      "Epoch 299, Loss: 4.85574996471405, Final Batch Loss: 0.9221070408821106\n",
      "Epoch 300, Loss: 4.924774348735809, Final Batch Loss: 1.040605902671814\n",
      "Epoch 301, Loss: 4.751004159450531, Final Batch Loss: 0.8887200951576233\n",
      "Epoch 302, Loss: 4.907021105289459, Final Batch Loss: 0.9654467105865479\n",
      "Epoch 303, Loss: 5.093228459358215, Final Batch Loss: 1.0930063724517822\n",
      "Epoch 304, Loss: 5.100450694561005, Final Batch Loss: 1.0463823080062866\n",
      "Epoch 305, Loss: 4.875123500823975, Final Batch Loss: 0.9582882523536682\n",
      "Epoch 306, Loss: 4.754593372344971, Final Batch Loss: 1.0346384048461914\n",
      "Epoch 307, Loss: 5.013961493968964, Final Batch Loss: 1.0105897188186646\n",
      "Epoch 308, Loss: 4.83336615562439, Final Batch Loss: 0.9681638479232788\n",
      "Epoch 309, Loss: 4.863183498382568, Final Batch Loss: 0.9593183994293213\n",
      "Epoch 310, Loss: 4.870383679866791, Final Batch Loss: 1.0037575960159302\n",
      "Epoch 311, Loss: 4.898381948471069, Final Batch Loss: 0.9424750208854675\n",
      "Epoch 312, Loss: 4.697197318077087, Final Batch Loss: 0.8555693030357361\n",
      "Epoch 313, Loss: 4.799785792827606, Final Batch Loss: 0.9996998310089111\n",
      "Epoch 314, Loss: 4.87193489074707, Final Batch Loss: 0.8851659297943115\n",
      "Epoch 315, Loss: 4.771477222442627, Final Batch Loss: 1.053895354270935\n",
      "Epoch 316, Loss: 4.847114861011505, Final Batch Loss: 0.9718281030654907\n",
      "Epoch 317, Loss: 4.785475373268127, Final Batch Loss: 0.9814282059669495\n",
      "Epoch 318, Loss: 4.739413738250732, Final Batch Loss: 1.0034067630767822\n",
      "Epoch 319, Loss: 4.922882616519928, Final Batch Loss: 1.1015052795410156\n",
      "Epoch 320, Loss: 4.719077885150909, Final Batch Loss: 0.9178381562232971\n",
      "Epoch 321, Loss: 4.8001203536987305, Final Batch Loss: 0.9832674860954285\n",
      "Epoch 322, Loss: 4.850291609764099, Final Batch Loss: 0.9999537467956543\n",
      "Epoch 323, Loss: 4.912016272544861, Final Batch Loss: 1.0310600996017456\n",
      "Epoch 324, Loss: 4.868744850158691, Final Batch Loss: 1.0407756567001343\n",
      "Epoch 325, Loss: 4.7913336753845215, Final Batch Loss: 0.8754754066467285\n",
      "Epoch 326, Loss: 4.914871454238892, Final Batch Loss: 0.937738299369812\n",
      "Epoch 327, Loss: 4.815781950950623, Final Batch Loss: 0.9397966265678406\n",
      "Epoch 328, Loss: 4.789580225944519, Final Batch Loss: 0.9058032035827637\n",
      "Epoch 329, Loss: 4.809729635715485, Final Batch Loss: 0.9393066763877869\n",
      "Epoch 330, Loss: 4.72221976518631, Final Batch Loss: 1.0248340368270874\n",
      "Epoch 331, Loss: 4.704292058944702, Final Batch Loss: 0.8868454694747925\n",
      "Epoch 332, Loss: 4.77115261554718, Final Batch Loss: 0.9850794076919556\n",
      "Epoch 333, Loss: 4.775530993938446, Final Batch Loss: 0.8652516603469849\n",
      "Epoch 334, Loss: 4.7334675788879395, Final Batch Loss: 0.9750748872756958\n",
      "Epoch 335, Loss: 4.755351483821869, Final Batch Loss: 0.9556087255477905\n",
      "Epoch 336, Loss: 4.681899785995483, Final Batch Loss: 0.9112944006919861\n",
      "Epoch 337, Loss: 4.711962699890137, Final Batch Loss: 0.9387730956077576\n",
      "Epoch 338, Loss: 4.71904981136322, Final Batch Loss: 0.9310534000396729\n",
      "Epoch 339, Loss: 4.596642196178436, Final Batch Loss: 0.9223809242248535\n",
      "Epoch 340, Loss: 4.748441934585571, Final Batch Loss: 0.9834571480751038\n",
      "Epoch 341, Loss: 4.631379544734955, Final Batch Loss: 0.9571990370750427\n",
      "Epoch 342, Loss: 4.66923052072525, Final Batch Loss: 0.8864727020263672\n",
      "Epoch 343, Loss: 4.7559091448783875, Final Batch Loss: 0.9485567212104797\n",
      "Epoch 344, Loss: 4.7956255078315735, Final Batch Loss: 0.9373093843460083\n",
      "Epoch 345, Loss: 4.7669837474823, Final Batch Loss: 1.0162326097488403\n",
      "Epoch 346, Loss: 4.879496395587921, Final Batch Loss: 0.9472205638885498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347, Loss: 4.6550344824790955, Final Batch Loss: 0.9377731084823608\n",
      "Epoch 348, Loss: 4.766660928726196, Final Batch Loss: 0.923858642578125\n",
      "Epoch 349, Loss: 4.642500400543213, Final Batch Loss: 0.8751749396324158\n",
      "Epoch 350, Loss: 4.588007748126984, Final Batch Loss: 0.9333604574203491\n",
      "Epoch 351, Loss: 4.627453148365021, Final Batch Loss: 1.0226633548736572\n",
      "Epoch 352, Loss: 4.832939505577087, Final Batch Loss: 0.9493738412857056\n",
      "Epoch 353, Loss: 4.666880309581757, Final Batch Loss: 0.9508885741233826\n",
      "Epoch 354, Loss: 4.680314540863037, Final Batch Loss: 0.8849192261695862\n",
      "Epoch 355, Loss: 4.549729347229004, Final Batch Loss: 0.8895740509033203\n",
      "Epoch 356, Loss: 4.748805582523346, Final Batch Loss: 1.031207799911499\n",
      "Epoch 357, Loss: 4.7753530740737915, Final Batch Loss: 0.9393377304077148\n",
      "Epoch 358, Loss: 4.679135262966156, Final Batch Loss: 0.9126588106155396\n",
      "Epoch 359, Loss: 4.6729336977005005, Final Batch Loss: 0.9123710989952087\n",
      "Epoch 360, Loss: 4.583589851856232, Final Batch Loss: 0.8076436519622803\n",
      "Epoch 361, Loss: 4.58580082654953, Final Batch Loss: 0.9225738644599915\n",
      "Epoch 362, Loss: 4.606233835220337, Final Batch Loss: 0.8887039422988892\n",
      "Epoch 363, Loss: 4.528656363487244, Final Batch Loss: 0.9685605764389038\n",
      "Epoch 364, Loss: 4.489516615867615, Final Batch Loss: 0.9399319887161255\n",
      "Epoch 365, Loss: 4.685908496379852, Final Batch Loss: 1.1015615463256836\n",
      "Epoch 366, Loss: 4.6490232944488525, Final Batch Loss: 0.9562036991119385\n",
      "Epoch 367, Loss: 4.583417117595673, Final Batch Loss: 0.8784763216972351\n",
      "Epoch 368, Loss: 4.516085267066956, Final Batch Loss: 0.8526549935340881\n",
      "Epoch 369, Loss: 4.53894168138504, Final Batch Loss: 0.83867347240448\n",
      "Epoch 370, Loss: 4.589085578918457, Final Batch Loss: 0.972623884677887\n",
      "Epoch 371, Loss: 4.54910272359848, Final Batch Loss: 0.9837720990180969\n",
      "Epoch 372, Loss: 4.497450172901154, Final Batch Loss: 0.9295389652252197\n",
      "Epoch 373, Loss: 4.557102680206299, Final Batch Loss: 0.8689592480659485\n",
      "Epoch 374, Loss: 4.46301656961441, Final Batch Loss: 0.9207792282104492\n",
      "Epoch 375, Loss: 4.604272961616516, Final Batch Loss: 0.9577205777168274\n",
      "Epoch 376, Loss: 4.6060808300971985, Final Batch Loss: 0.9222948551177979\n",
      "Epoch 377, Loss: 4.461503505706787, Final Batch Loss: 0.8846787214279175\n",
      "Epoch 378, Loss: 4.511358916759491, Final Batch Loss: 0.8639365434646606\n",
      "Epoch 379, Loss: 4.6179205775260925, Final Batch Loss: 0.9321892261505127\n",
      "Epoch 380, Loss: 4.544614791870117, Final Batch Loss: 0.9332740306854248\n",
      "Epoch 381, Loss: 4.612452149391174, Final Batch Loss: 1.016953945159912\n",
      "Epoch 382, Loss: 4.5647077560424805, Final Batch Loss: 0.927811861038208\n",
      "Epoch 383, Loss: 4.553379535675049, Final Batch Loss: 0.9045511484146118\n",
      "Epoch 384, Loss: 4.44896936416626, Final Batch Loss: 0.8663777112960815\n",
      "Epoch 385, Loss: 4.4298800230026245, Final Batch Loss: 0.8795995116233826\n",
      "Epoch 386, Loss: 4.6006691455841064, Final Batch Loss: 0.953535795211792\n",
      "Epoch 387, Loss: 4.435090780258179, Final Batch Loss: 0.8307077288627625\n",
      "Epoch 388, Loss: 4.476772964000702, Final Batch Loss: 0.8671793937683105\n",
      "Epoch 389, Loss: 4.527712047100067, Final Batch Loss: 0.8495393991470337\n",
      "Epoch 390, Loss: 4.4600441455841064, Final Batch Loss: 0.8334289193153381\n",
      "Epoch 391, Loss: 4.379525542259216, Final Batch Loss: 0.8333990573883057\n",
      "Epoch 392, Loss: 4.545817255973816, Final Batch Loss: 0.9237753748893738\n",
      "Epoch 393, Loss: 4.642890274524689, Final Batch Loss: 0.9175179600715637\n",
      "Epoch 394, Loss: 4.582249343395233, Final Batch Loss: 0.8676979541778564\n",
      "Epoch 395, Loss: 4.440884649753571, Final Batch Loss: 0.8364382386207581\n",
      "Epoch 396, Loss: 4.457000494003296, Final Batch Loss: 0.9076167941093445\n",
      "Epoch 397, Loss: 4.520784497261047, Final Batch Loss: 0.8480839133262634\n",
      "Epoch 398, Loss: 4.552133142948151, Final Batch Loss: 0.9351455569267273\n",
      "Epoch 399, Loss: 4.496116816997528, Final Batch Loss: 0.833306074142456\n",
      "Epoch 400, Loss: 4.430367827415466, Final Batch Loss: 0.8616639375686646\n",
      "Epoch 401, Loss: 4.449532449245453, Final Batch Loss: 0.7962506413459778\n",
      "Epoch 402, Loss: 4.499220371246338, Final Batch Loss: 0.7983601093292236\n",
      "Epoch 403, Loss: 4.46598482131958, Final Batch Loss: 0.8575462102890015\n",
      "Epoch 404, Loss: 4.353205382823944, Final Batch Loss: 0.8294482231140137\n",
      "Epoch 405, Loss: 4.539126455783844, Final Batch Loss: 0.9104332327842712\n",
      "Epoch 406, Loss: 4.571238577365875, Final Batch Loss: 0.9524517059326172\n",
      "Epoch 407, Loss: 4.536965429782867, Final Batch Loss: 0.8461997509002686\n",
      "Epoch 408, Loss: 4.527916073799133, Final Batch Loss: 0.8672884106636047\n",
      "Epoch 409, Loss: 4.3289971351623535, Final Batch Loss: 0.8952080607414246\n",
      "Epoch 410, Loss: 4.553907513618469, Final Batch Loss: 0.8556323647499084\n",
      "Epoch 411, Loss: 4.411540389060974, Final Batch Loss: 0.8737183213233948\n",
      "Epoch 412, Loss: 4.614205479621887, Final Batch Loss: 0.912355899810791\n",
      "Epoch 413, Loss: 4.485508918762207, Final Batch Loss: 0.8829425573348999\n",
      "Epoch 414, Loss: 4.53933709859848, Final Batch Loss: 0.9556179642677307\n",
      "Epoch 415, Loss: 4.66925323009491, Final Batch Loss: 0.9581502676010132\n",
      "Epoch 416, Loss: 4.517930686473846, Final Batch Loss: 0.9577765464782715\n",
      "Epoch 417, Loss: 4.374294221401215, Final Batch Loss: 0.9056259393692017\n",
      "Epoch 418, Loss: 4.410779714584351, Final Batch Loss: 0.8870465159416199\n",
      "Epoch 419, Loss: 4.322899281978607, Final Batch Loss: 0.9320165514945984\n",
      "Epoch 420, Loss: 4.421181917190552, Final Batch Loss: 0.9349077343940735\n",
      "Epoch 421, Loss: 4.450737178325653, Final Batch Loss: 0.8227700591087341\n",
      "Epoch 422, Loss: 4.585372567176819, Final Batch Loss: 0.8879536986351013\n",
      "Epoch 423, Loss: 4.549115836620331, Final Batch Loss: 0.9800527691841125\n",
      "Epoch 424, Loss: 4.321419417858124, Final Batch Loss: 0.857332170009613\n",
      "Epoch 425, Loss: 4.351319491863251, Final Batch Loss: 0.901444673538208\n",
      "Epoch 426, Loss: 4.4304962158203125, Final Batch Loss: 0.9498257040977478\n",
      "Epoch 427, Loss: 4.395617246627808, Final Batch Loss: 0.9062479734420776\n",
      "Epoch 428, Loss: 4.556053817272186, Final Batch Loss: 1.0026005506515503\n",
      "Epoch 429, Loss: 4.444249331951141, Final Batch Loss: 0.9110586047172546\n",
      "Epoch 430, Loss: 4.287009954452515, Final Batch Loss: 0.7694512009620667\n",
      "Epoch 431, Loss: 4.431526601314545, Final Batch Loss: 0.8294709324836731\n",
      "Epoch 432, Loss: 4.504268050193787, Final Batch Loss: 0.8595595955848694\n",
      "Epoch 433, Loss: 4.491806507110596, Final Batch Loss: 0.8490697741508484\n",
      "Epoch 434, Loss: 4.382350862026215, Final Batch Loss: 0.896538257598877\n",
      "Epoch 435, Loss: 4.385985314846039, Final Batch Loss: 0.9147788286209106\n",
      "Epoch 436, Loss: 4.540826916694641, Final Batch Loss: 0.9256529808044434\n",
      "Epoch 437, Loss: 4.354021191596985, Final Batch Loss: 0.8597019910812378\n",
      "Epoch 438, Loss: 4.392401099205017, Final Batch Loss: 0.9060973525047302\n",
      "Epoch 439, Loss: 4.240211844444275, Final Batch Loss: 0.8230259418487549\n",
      "Epoch 440, Loss: 4.362999975681305, Final Batch Loss: 1.0084556341171265\n",
      "Epoch 441, Loss: 4.261345684528351, Final Batch Loss: 0.9438413977622986\n",
      "Epoch 442, Loss: 4.45536196231842, Final Batch Loss: 0.9045177698135376\n",
      "Epoch 443, Loss: 4.408794641494751, Final Batch Loss: 0.9376534819602966\n",
      "Epoch 444, Loss: 4.226597487926483, Final Batch Loss: 0.8210746645927429\n",
      "Epoch 445, Loss: 4.459738790988922, Final Batch Loss: 1.1067525148391724\n",
      "Epoch 446, Loss: 4.218431770801544, Final Batch Loss: 0.9238429665565491\n",
      "Epoch 447, Loss: 4.464146673679352, Final Batch Loss: 0.8931574821472168\n",
      "Epoch 448, Loss: 4.376706063747406, Final Batch Loss: 0.8074367046356201\n",
      "Epoch 449, Loss: 4.324387550354004, Final Batch Loss: 0.9455429315567017\n",
      "Epoch 450, Loss: 4.283705711364746, Final Batch Loss: 0.8777257204055786\n",
      "Epoch 451, Loss: 4.433671534061432, Final Batch Loss: 0.8593103885650635\n",
      "Epoch 452, Loss: 4.356267035007477, Final Batch Loss: 0.9457632899284363\n",
      "Epoch 453, Loss: 4.467370688915253, Final Batch Loss: 0.8954921364784241\n",
      "Epoch 454, Loss: 4.372506082057953, Final Batch Loss: 0.8607895374298096\n",
      "Epoch 455, Loss: 4.521596848964691, Final Batch Loss: 0.9628944993019104\n",
      "Epoch 456, Loss: 4.376012086868286, Final Batch Loss: 0.9495059847831726\n",
      "Epoch 457, Loss: 4.253280520439148, Final Batch Loss: 0.8447107672691345\n",
      "Epoch 458, Loss: 4.468811094760895, Final Batch Loss: 0.913531482219696\n",
      "Epoch 459, Loss: 4.279329776763916, Final Batch Loss: 0.8373552560806274\n",
      "Epoch 460, Loss: 4.237422227859497, Final Batch Loss: 0.8945632576942444\n",
      "Epoch 461, Loss: 4.368615448474884, Final Batch Loss: 0.9669591188430786\n",
      "Epoch 462, Loss: 4.396271228790283, Final Batch Loss: 0.9456701278686523\n",
      "Epoch 463, Loss: 4.322132468223572, Final Batch Loss: 0.852528989315033\n",
      "Epoch 464, Loss: 4.358468770980835, Final Batch Loss: 0.779573917388916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465, Loss: 4.348120748996735, Final Batch Loss: 0.8001621961593628\n",
      "Epoch 466, Loss: 4.333989322185516, Final Batch Loss: 0.8978897333145142\n",
      "Epoch 467, Loss: 4.279716193675995, Final Batch Loss: 0.9478644728660583\n",
      "Epoch 468, Loss: 4.540060520172119, Final Batch Loss: 0.8721200823783875\n",
      "Epoch 469, Loss: 4.373999893665314, Final Batch Loss: 0.823943555355072\n",
      "Epoch 470, Loss: 4.2347413301467896, Final Batch Loss: 0.8879616856575012\n",
      "Epoch 471, Loss: 4.237214088439941, Final Batch Loss: 0.7355800271034241\n",
      "Epoch 472, Loss: 4.26641446352005, Final Batch Loss: 0.897770345211029\n",
      "Epoch 473, Loss: 4.3121243715286255, Final Batch Loss: 0.8725606799125671\n",
      "Epoch 474, Loss: 4.248126268386841, Final Batch Loss: 0.8842424750328064\n",
      "Epoch 475, Loss: 4.275539755821228, Final Batch Loss: 0.8620871901512146\n",
      "Epoch 476, Loss: 4.242079377174377, Final Batch Loss: 0.9529075026512146\n",
      "Epoch 477, Loss: 4.158293068408966, Final Batch Loss: 0.8300679922103882\n",
      "Epoch 478, Loss: 4.283039331436157, Final Batch Loss: 0.8395621180534363\n",
      "Epoch 479, Loss: 4.244745552539825, Final Batch Loss: 0.8575553297996521\n",
      "Epoch 480, Loss: 4.237946510314941, Final Batch Loss: 0.8078115582466125\n",
      "Epoch 481, Loss: 4.164809167385101, Final Batch Loss: 0.8415317535400391\n",
      "Epoch 482, Loss: 4.295704126358032, Final Batch Loss: 0.8506090044975281\n",
      "Epoch 483, Loss: 4.168427288532257, Final Batch Loss: 0.8092115521430969\n",
      "Epoch 484, Loss: 4.325190544128418, Final Batch Loss: 0.8004886507987976\n",
      "Epoch 485, Loss: 4.141506671905518, Final Batch Loss: 0.9003345966339111\n",
      "Epoch 486, Loss: 4.1718510389328, Final Batch Loss: 0.8948760032653809\n",
      "Epoch 487, Loss: 4.3003939390182495, Final Batch Loss: 0.8715566396713257\n",
      "Epoch 488, Loss: 4.191101491451263, Final Batch Loss: 0.7820327877998352\n",
      "Epoch 489, Loss: 4.104710102081299, Final Batch Loss: 0.8263410329818726\n",
      "Epoch 490, Loss: 4.462462246417999, Final Batch Loss: 0.8832280039787292\n",
      "Epoch 491, Loss: 4.099620878696442, Final Batch Loss: 0.7675113677978516\n",
      "Epoch 492, Loss: 4.314007937908173, Final Batch Loss: 0.961783766746521\n",
      "Epoch 493, Loss: 4.126290738582611, Final Batch Loss: 0.8163076639175415\n",
      "Epoch 494, Loss: 4.36167573928833, Final Batch Loss: 0.866664469242096\n",
      "Epoch 495, Loss: 4.357657015323639, Final Batch Loss: 0.9130277633666992\n",
      "Epoch 496, Loss: 4.138788640499115, Final Batch Loss: 0.8413704633712769\n",
      "Epoch 497, Loss: 4.202630579471588, Final Batch Loss: 0.8998541235923767\n",
      "Epoch 498, Loss: 4.1940725445747375, Final Batch Loss: 0.7807446122169495\n",
      "Epoch 499, Loss: 4.232131540775299, Final Batch Loss: 0.8994967341423035\n",
      "Epoch 500, Loss: 4.079105257987976, Final Batch Loss: 0.8523053526878357\n",
      "Epoch 501, Loss: 4.270234405994415, Final Batch Loss: 0.9256494045257568\n",
      "Epoch 502, Loss: 4.276372909545898, Final Batch Loss: 0.8841177225112915\n",
      "Epoch 503, Loss: 4.21453469991684, Final Batch Loss: 0.909345805644989\n",
      "Epoch 504, Loss: 3.9885554909706116, Final Batch Loss: 0.7948808670043945\n",
      "Epoch 505, Loss: 4.237119197845459, Final Batch Loss: 0.9732844829559326\n",
      "Epoch 506, Loss: 4.198494911193848, Final Batch Loss: 0.8703715205192566\n",
      "Epoch 507, Loss: 4.254218518733978, Final Batch Loss: 0.7946227192878723\n",
      "Epoch 508, Loss: 4.2137322425842285, Final Batch Loss: 0.9172779321670532\n",
      "Epoch 509, Loss: 4.2220141887664795, Final Batch Loss: 0.8323732018470764\n",
      "Epoch 510, Loss: 4.218248069286346, Final Batch Loss: 0.9022089242935181\n",
      "Epoch 511, Loss: 4.236281931400299, Final Batch Loss: 0.8752551078796387\n",
      "Epoch 512, Loss: 4.265030562877655, Final Batch Loss: 0.8063037991523743\n",
      "Epoch 513, Loss: 4.143205285072327, Final Batch Loss: 0.7585932016372681\n",
      "Epoch 514, Loss: 4.3137622475624084, Final Batch Loss: 0.829637885093689\n",
      "Epoch 515, Loss: 4.175699889659882, Final Batch Loss: 0.9000800251960754\n",
      "Epoch 516, Loss: 4.061127662658691, Final Batch Loss: 0.7641037702560425\n",
      "Epoch 517, Loss: 4.173787832260132, Final Batch Loss: 0.8810341954231262\n",
      "Epoch 518, Loss: 4.1278263330459595, Final Batch Loss: 0.857903778553009\n",
      "Epoch 519, Loss: 4.178469240665436, Final Batch Loss: 0.8107770085334778\n",
      "Epoch 520, Loss: 4.086181998252869, Final Batch Loss: 0.8967493176460266\n",
      "Epoch 521, Loss: 4.137561857700348, Final Batch Loss: 0.8628751635551453\n",
      "Epoch 522, Loss: 4.209955334663391, Final Batch Loss: 0.8108037710189819\n",
      "Epoch 523, Loss: 4.174002289772034, Final Batch Loss: 0.765433669090271\n",
      "Epoch 524, Loss: 4.184488296508789, Final Batch Loss: 0.7677369117736816\n",
      "Epoch 525, Loss: 4.093344271183014, Final Batch Loss: 0.8212054371833801\n",
      "Epoch 526, Loss: 4.146520614624023, Final Batch Loss: 0.8391896486282349\n",
      "Epoch 527, Loss: 4.070541262626648, Final Batch Loss: 0.8483366370201111\n",
      "Epoch 528, Loss: 4.265141487121582, Final Batch Loss: 0.9034380912780762\n",
      "Epoch 529, Loss: 4.105040967464447, Final Batch Loss: 0.8663784861564636\n",
      "Epoch 530, Loss: 4.247358083724976, Final Batch Loss: 0.9032116532325745\n",
      "Epoch 531, Loss: 4.152213275432587, Final Batch Loss: 0.832536518573761\n",
      "Epoch 532, Loss: 4.068597972393036, Final Batch Loss: 0.8267391324043274\n",
      "Epoch 533, Loss: 4.122492969036102, Final Batch Loss: 0.8236339688301086\n",
      "Epoch 534, Loss: 4.111494183540344, Final Batch Loss: 0.7924366593360901\n",
      "Epoch 535, Loss: 4.140347063541412, Final Batch Loss: 0.7520654797554016\n",
      "Epoch 536, Loss: 4.144467651844025, Final Batch Loss: 0.7427570819854736\n",
      "Epoch 537, Loss: 4.180984795093536, Final Batch Loss: 0.8854554295539856\n",
      "Epoch 538, Loss: 4.191985189914703, Final Batch Loss: 0.8156312704086304\n",
      "Epoch 539, Loss: 4.24031001329422, Final Batch Loss: 0.8723157644271851\n",
      "Epoch 540, Loss: 4.062853515148163, Final Batch Loss: 0.7879310846328735\n",
      "Epoch 541, Loss: 4.145406901836395, Final Batch Loss: 0.8169687390327454\n",
      "Epoch 542, Loss: 4.1604360938072205, Final Batch Loss: 0.8136738538742065\n",
      "Epoch 543, Loss: 4.195281386375427, Final Batch Loss: 0.9171636700630188\n",
      "Epoch 544, Loss: 4.141882956027985, Final Batch Loss: 0.8329537510871887\n",
      "Epoch 545, Loss: 4.067702054977417, Final Batch Loss: 0.8381017446517944\n",
      "Epoch 546, Loss: 4.184634149074554, Final Batch Loss: 0.8070545792579651\n",
      "Epoch 547, Loss: 4.120832979679108, Final Batch Loss: 0.8512240052223206\n",
      "Epoch 548, Loss: 3.9497151970863342, Final Batch Loss: 0.7707914710044861\n",
      "Epoch 549, Loss: 4.0589393973350525, Final Batch Loss: 0.7719525694847107\n",
      "Epoch 550, Loss: 3.997323453426361, Final Batch Loss: 0.7794181108474731\n",
      "Epoch 551, Loss: 4.1504539251327515, Final Batch Loss: 0.7833537459373474\n",
      "Epoch 552, Loss: 4.112490236759186, Final Batch Loss: 0.770413875579834\n",
      "Epoch 553, Loss: 4.184870004653931, Final Batch Loss: 0.8478906750679016\n",
      "Epoch 554, Loss: 4.0198695063591, Final Batch Loss: 0.7240167260169983\n",
      "Epoch 555, Loss: 4.1671388149261475, Final Batch Loss: 0.8782278895378113\n",
      "Epoch 556, Loss: 4.168053328990936, Final Batch Loss: 0.8679976463317871\n",
      "Epoch 557, Loss: 4.10353022813797, Final Batch Loss: 0.7547479867935181\n",
      "Epoch 558, Loss: 4.081648230552673, Final Batch Loss: 0.8513308167457581\n",
      "Epoch 559, Loss: 4.025538265705109, Final Batch Loss: 0.811177670955658\n",
      "Epoch 560, Loss: 4.167850375175476, Final Batch Loss: 0.9195876121520996\n",
      "Epoch 561, Loss: 3.9211326837539673, Final Batch Loss: 0.7817734479904175\n",
      "Epoch 562, Loss: 3.963365852832794, Final Batch Loss: 0.8031079769134521\n",
      "Epoch 563, Loss: 4.003076434135437, Final Batch Loss: 0.7821913957595825\n",
      "Epoch 564, Loss: 3.957914173603058, Final Batch Loss: 0.797111451625824\n",
      "Epoch 565, Loss: 4.131887376308441, Final Batch Loss: 0.7617297768592834\n",
      "Epoch 566, Loss: 3.966178834438324, Final Batch Loss: 0.7631934881210327\n",
      "Epoch 567, Loss: 4.069119572639465, Final Batch Loss: 0.8889568448066711\n",
      "Epoch 568, Loss: 4.032929241657257, Final Batch Loss: 0.8264817595481873\n",
      "Epoch 569, Loss: 3.9782638549804688, Final Batch Loss: 0.7719699144363403\n",
      "Epoch 570, Loss: 4.135239362716675, Final Batch Loss: 0.8269515037536621\n",
      "Epoch 571, Loss: 4.010926842689514, Final Batch Loss: 0.8309429287910461\n",
      "Epoch 572, Loss: 4.086884617805481, Final Batch Loss: 0.8741012811660767\n",
      "Epoch 573, Loss: 4.035813987255096, Final Batch Loss: 0.807398796081543\n",
      "Epoch 574, Loss: 4.02611380815506, Final Batch Loss: 0.7932605743408203\n",
      "Epoch 575, Loss: 3.972068727016449, Final Batch Loss: 0.802461564540863\n",
      "Epoch 576, Loss: 3.8992552757263184, Final Batch Loss: 0.8119887709617615\n",
      "Epoch 577, Loss: 4.092831254005432, Final Batch Loss: 0.894635796546936\n",
      "Epoch 578, Loss: 3.9675713181495667, Final Batch Loss: 0.7422127723693848\n",
      "Epoch 579, Loss: 4.019596874713898, Final Batch Loss: 0.9161903858184814\n",
      "Epoch 580, Loss: 3.993495285511017, Final Batch Loss: 0.7525117993354797\n",
      "Epoch 581, Loss: 4.010142743587494, Final Batch Loss: 0.8164950013160706\n",
      "Epoch 582, Loss: 4.043766140937805, Final Batch Loss: 0.8386446833610535\n",
      "Epoch 583, Loss: 4.04674905538559, Final Batch Loss: 0.8174616098403931\n",
      "Epoch 584, Loss: 3.892381429672241, Final Batch Loss: 0.781032383441925\n",
      "Epoch 585, Loss: 3.9797586798667908, Final Batch Loss: 0.8575357794761658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 586, Loss: 3.8879000544548035, Final Batch Loss: 0.6813853979110718\n",
      "Epoch 587, Loss: 4.02436101436615, Final Batch Loss: 0.7779916524887085\n",
      "Epoch 588, Loss: 3.9992308616638184, Final Batch Loss: 0.8608558773994446\n",
      "Epoch 589, Loss: 3.8764371275901794, Final Batch Loss: 0.8552789092063904\n",
      "Epoch 590, Loss: 3.922635078430176, Final Batch Loss: 0.6767045259475708\n",
      "Epoch 591, Loss: 3.895815670490265, Final Batch Loss: 0.6882982850074768\n",
      "Epoch 592, Loss: 3.9292299151420593, Final Batch Loss: 0.7926284670829773\n",
      "Epoch 593, Loss: 4.003520369529724, Final Batch Loss: 0.7430780529975891\n",
      "Epoch 594, Loss: 3.9306086897850037, Final Batch Loss: 0.7827595472335815\n",
      "Epoch 595, Loss: 3.8757523894309998, Final Batch Loss: 0.7719783782958984\n",
      "Epoch 596, Loss: 4.061184227466583, Final Batch Loss: 0.7878754138946533\n",
      "Epoch 597, Loss: 3.9959014654159546, Final Batch Loss: 0.8114981651306152\n",
      "Epoch 598, Loss: 3.860050141811371, Final Batch Loss: 0.7743942141532898\n",
      "Epoch 599, Loss: 4.0436166524887085, Final Batch Loss: 0.7891944646835327\n",
      "Epoch 600, Loss: 3.8723455667495728, Final Batch Loss: 0.913600504398346\n",
      "Epoch 601, Loss: 4.082915306091309, Final Batch Loss: 0.7597655057907104\n",
      "Epoch 602, Loss: 3.8396341800689697, Final Batch Loss: 0.7554653882980347\n",
      "Epoch 603, Loss: 3.9615159034729004, Final Batch Loss: 0.8586902022361755\n",
      "Epoch 604, Loss: 4.096493184566498, Final Batch Loss: 0.8148177862167358\n",
      "Epoch 605, Loss: 3.898398697376251, Final Batch Loss: 0.7903392314910889\n",
      "Epoch 606, Loss: 3.906586527824402, Final Batch Loss: 0.7090834975242615\n",
      "Epoch 607, Loss: 4.001768350601196, Final Batch Loss: 0.7882229089736938\n",
      "Epoch 608, Loss: 3.9160324931144714, Final Batch Loss: 0.7758238911628723\n",
      "Epoch 609, Loss: 3.86271595954895, Final Batch Loss: 0.7532439827919006\n",
      "Epoch 610, Loss: 3.9065815806388855, Final Batch Loss: 0.7673488855361938\n",
      "Epoch 611, Loss: 3.917048752307892, Final Batch Loss: 0.8104666471481323\n",
      "Epoch 612, Loss: 4.049535036087036, Final Batch Loss: 0.7192350029945374\n",
      "Epoch 613, Loss: 4.094741642475128, Final Batch Loss: 0.8628717660903931\n",
      "Epoch 614, Loss: 3.9933159947395325, Final Batch Loss: 0.8720373511314392\n",
      "Epoch 615, Loss: 3.928730070590973, Final Batch Loss: 0.8025039434432983\n",
      "Epoch 616, Loss: 3.9362332224845886, Final Batch Loss: 0.7432291507720947\n",
      "Epoch 617, Loss: 3.8456640243530273, Final Batch Loss: 0.8496872782707214\n",
      "Epoch 618, Loss: 4.02271842956543, Final Batch Loss: 0.8042932152748108\n",
      "Epoch 619, Loss: 3.9481051564216614, Final Batch Loss: 0.845906674861908\n",
      "Epoch 620, Loss: 3.940275728702545, Final Batch Loss: 0.7001462578773499\n",
      "Epoch 621, Loss: 3.9346179962158203, Final Batch Loss: 0.834462583065033\n",
      "Epoch 622, Loss: 3.953277349472046, Final Batch Loss: 0.697413980960846\n",
      "Epoch 623, Loss: 3.8638601899147034, Final Batch Loss: 0.8676823973655701\n",
      "Epoch 624, Loss: 3.9338783621788025, Final Batch Loss: 0.7800639867782593\n",
      "Epoch 625, Loss: 4.042438864707947, Final Batch Loss: 0.7493183612823486\n",
      "Epoch 626, Loss: 3.8314422369003296, Final Batch Loss: 0.7445709109306335\n",
      "Epoch 627, Loss: 4.098063945770264, Final Batch Loss: 0.8264517784118652\n",
      "Epoch 628, Loss: 4.021106541156769, Final Batch Loss: 0.7270662784576416\n",
      "Epoch 629, Loss: 3.9364302158355713, Final Batch Loss: 0.7808867692947388\n",
      "Epoch 630, Loss: 3.789593458175659, Final Batch Loss: 0.7035694718360901\n",
      "Epoch 631, Loss: 3.8602530360221863, Final Batch Loss: 0.7822759747505188\n",
      "Epoch 632, Loss: 3.8788389563560486, Final Batch Loss: 0.73641037940979\n",
      "Epoch 633, Loss: 3.9410592913627625, Final Batch Loss: 0.7482154965400696\n",
      "Epoch 634, Loss: 3.8702402114868164, Final Batch Loss: 0.8058227896690369\n",
      "Epoch 635, Loss: 3.778618097305298, Final Batch Loss: 0.7940088510513306\n",
      "Epoch 636, Loss: 4.022733569145203, Final Batch Loss: 0.7949268817901611\n",
      "Epoch 637, Loss: 3.9751405119895935, Final Batch Loss: 0.7519805431365967\n",
      "Epoch 638, Loss: 3.9871460795402527, Final Batch Loss: 0.7816111445426941\n",
      "Epoch 639, Loss: 3.908474385738373, Final Batch Loss: 0.7921133637428284\n",
      "Epoch 640, Loss: 3.756805121898651, Final Batch Loss: 0.784976601600647\n",
      "Epoch 641, Loss: 3.866688311100006, Final Batch Loss: 0.7529687881469727\n",
      "Epoch 642, Loss: 3.8462756872177124, Final Batch Loss: 0.789940595626831\n",
      "Epoch 643, Loss: 3.8236294984817505, Final Batch Loss: 0.8072426915168762\n",
      "Epoch 644, Loss: 3.7441136240959167, Final Batch Loss: 0.8106459975242615\n",
      "Epoch 645, Loss: 3.9723411798477173, Final Batch Loss: 0.8902043104171753\n",
      "Epoch 646, Loss: 3.873894989490509, Final Batch Loss: 0.8447732329368591\n",
      "Epoch 647, Loss: 3.822086215019226, Final Batch Loss: 0.8107913136482239\n",
      "Epoch 648, Loss: 3.7652701139450073, Final Batch Loss: 0.7594859004020691\n",
      "Epoch 649, Loss: 3.664535701274872, Final Batch Loss: 0.6097005605697632\n",
      "Epoch 650, Loss: 3.837556481361389, Final Batch Loss: 0.7732338905334473\n",
      "Epoch 651, Loss: 3.7417861819267273, Final Batch Loss: 0.7163617610931396\n",
      "Epoch 652, Loss: 3.817953884601593, Final Batch Loss: 0.833436906337738\n",
      "Epoch 653, Loss: 3.936187505722046, Final Batch Loss: 0.6978568434715271\n",
      "Epoch 654, Loss: 4.016233444213867, Final Batch Loss: 0.8390166759490967\n",
      "Epoch 655, Loss: 3.9002461433410645, Final Batch Loss: 0.8077185153961182\n",
      "Epoch 656, Loss: 3.8728280067443848, Final Batch Loss: 0.7989250421524048\n",
      "Epoch 657, Loss: 3.8115692138671875, Final Batch Loss: 0.8060253262519836\n",
      "Epoch 658, Loss: 3.846849739551544, Final Batch Loss: 0.7378720641136169\n",
      "Epoch 659, Loss: 3.9102577567100525, Final Batch Loss: 0.7964855432510376\n",
      "Epoch 660, Loss: 3.670114517211914, Final Batch Loss: 0.7542386651039124\n",
      "Epoch 661, Loss: 3.7968241572380066, Final Batch Loss: 0.7307701110839844\n",
      "Epoch 662, Loss: 3.940892457962036, Final Batch Loss: 0.7369716763496399\n",
      "Epoch 663, Loss: 3.9328114986419678, Final Batch Loss: 0.7212001085281372\n",
      "Epoch 664, Loss: 3.8935861587524414, Final Batch Loss: 0.7677087783813477\n",
      "Epoch 665, Loss: 3.84846293926239, Final Batch Loss: 0.8068700432777405\n",
      "Epoch 666, Loss: 3.8926364183425903, Final Batch Loss: 0.8194866180419922\n",
      "Epoch 667, Loss: 3.713386595249176, Final Batch Loss: 0.7022029161453247\n",
      "Epoch 668, Loss: 3.872538208961487, Final Batch Loss: 0.7200244069099426\n",
      "Epoch 669, Loss: 3.8523784279823303, Final Batch Loss: 0.7734227776527405\n",
      "Epoch 670, Loss: 3.7941760420799255, Final Batch Loss: 0.7996716499328613\n",
      "Epoch 671, Loss: 3.82809978723526, Final Batch Loss: 0.8138430118560791\n",
      "Epoch 672, Loss: 3.711246609687805, Final Batch Loss: 0.7496764063835144\n",
      "Epoch 673, Loss: 3.7733927369117737, Final Batch Loss: 0.7446188926696777\n",
      "Epoch 674, Loss: 3.7090181708335876, Final Batch Loss: 0.7810010313987732\n",
      "Epoch 675, Loss: 3.761904239654541, Final Batch Loss: 0.7016372084617615\n",
      "Epoch 676, Loss: 3.885872781276703, Final Batch Loss: 0.6954465508460999\n",
      "Epoch 677, Loss: 3.804284632205963, Final Batch Loss: 0.8294425010681152\n",
      "Epoch 678, Loss: 3.79470956325531, Final Batch Loss: 0.6972829699516296\n",
      "Epoch 679, Loss: 3.909328877925873, Final Batch Loss: 0.7695026993751526\n",
      "Epoch 680, Loss: 3.748106598854065, Final Batch Loss: 0.694533109664917\n",
      "Epoch 681, Loss: 3.856850802898407, Final Batch Loss: 0.7345282435417175\n",
      "Epoch 682, Loss: 3.960767388343811, Final Batch Loss: 0.8466307520866394\n",
      "Epoch 683, Loss: 3.739900290966034, Final Batch Loss: 0.7530012130737305\n",
      "Epoch 684, Loss: 3.9347627758979797, Final Batch Loss: 0.783502459526062\n",
      "Epoch 685, Loss: 3.820609450340271, Final Batch Loss: 0.7310133576393127\n",
      "Epoch 686, Loss: 3.5975659489631653, Final Batch Loss: 0.7002416253089905\n",
      "Epoch 687, Loss: 3.851964831352234, Final Batch Loss: 0.7859647274017334\n",
      "Epoch 688, Loss: 3.66588431596756, Final Batch Loss: 0.6695780754089355\n",
      "Epoch 689, Loss: 3.693687856197357, Final Batch Loss: 0.6703291535377502\n",
      "Epoch 690, Loss: 3.8441630601882935, Final Batch Loss: 0.8024812936782837\n",
      "Epoch 691, Loss: 3.7346960306167603, Final Batch Loss: 0.7380204200744629\n",
      "Epoch 692, Loss: 3.6375772356987, Final Batch Loss: 0.708722710609436\n",
      "Epoch 693, Loss: 3.749871075153351, Final Batch Loss: 0.8361101150512695\n",
      "Epoch 694, Loss: 3.7137756943702698, Final Batch Loss: 0.7499101758003235\n",
      "Epoch 695, Loss: 3.745402753353119, Final Batch Loss: 0.7472744584083557\n",
      "Epoch 696, Loss: 3.727061688899994, Final Batch Loss: 0.7764586210250854\n",
      "Epoch 697, Loss: 3.7018930315971375, Final Batch Loss: 0.6593834161758423\n",
      "Epoch 698, Loss: 3.6901997327804565, Final Batch Loss: 0.783312976360321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699, Loss: 3.8493740558624268, Final Batch Loss: 0.6828287839889526\n",
      "Epoch 700, Loss: 3.6615414023399353, Final Batch Loss: 0.7726878523826599\n",
      "Epoch 701, Loss: 3.658474862575531, Final Batch Loss: 0.7673324346542358\n",
      "Epoch 702, Loss: 3.7038779258728027, Final Batch Loss: 0.7589032649993896\n",
      "Epoch 703, Loss: 3.694653630256653, Final Batch Loss: 0.7610716819763184\n",
      "Epoch 704, Loss: 3.77815705537796, Final Batch Loss: 0.7504515647888184\n",
      "Epoch 705, Loss: 3.625352442264557, Final Batch Loss: 0.7415422797203064\n",
      "Epoch 706, Loss: 3.7897112369537354, Final Batch Loss: 0.6947289109230042\n",
      "Epoch 707, Loss: 3.806276798248291, Final Batch Loss: 0.7822905778884888\n",
      "Epoch 708, Loss: 3.6845943927764893, Final Batch Loss: 0.6414749622344971\n",
      "Epoch 709, Loss: 3.770639479160309, Final Batch Loss: 0.7740300893783569\n",
      "Epoch 710, Loss: 3.578685998916626, Final Batch Loss: 0.690627932548523\n",
      "Epoch 711, Loss: 3.6853480339050293, Final Batch Loss: 0.7777935862541199\n",
      "Epoch 712, Loss: 3.8553969860076904, Final Batch Loss: 0.7402927875518799\n",
      "Epoch 713, Loss: 3.7847172617912292, Final Batch Loss: 0.7901983261108398\n",
      "Epoch 714, Loss: 3.737288177013397, Final Batch Loss: 0.8532809615135193\n",
      "Epoch 715, Loss: 3.6790220141410828, Final Batch Loss: 0.6895865201950073\n",
      "Epoch 716, Loss: 3.9493834376335144, Final Batch Loss: 0.8164418339729309\n",
      "Epoch 717, Loss: 3.63049179315567, Final Batch Loss: 0.6929805278778076\n",
      "Epoch 718, Loss: 3.6206276416778564, Final Batch Loss: 0.745614767074585\n",
      "Epoch 719, Loss: 3.90468567609787, Final Batch Loss: 0.7306600213050842\n",
      "Epoch 720, Loss: 3.7274045944213867, Final Batch Loss: 0.8367642164230347\n",
      "Epoch 721, Loss: 3.6940351128578186, Final Batch Loss: 0.6892504096031189\n",
      "Epoch 722, Loss: 3.6805614829063416, Final Batch Loss: 0.779693067073822\n",
      "Epoch 723, Loss: 3.582634150981903, Final Batch Loss: 0.6587602496147156\n",
      "Epoch 724, Loss: 3.7413477897644043, Final Batch Loss: 0.7343610525131226\n",
      "Epoch 725, Loss: 3.691329598426819, Final Batch Loss: 0.7425590753555298\n",
      "Epoch 726, Loss: 3.779683291912079, Final Batch Loss: 0.7973966598510742\n",
      "Epoch 727, Loss: 3.5940062403678894, Final Batch Loss: 0.713975727558136\n",
      "Epoch 728, Loss: 3.6092171669006348, Final Batch Loss: 0.673818051815033\n",
      "Epoch 729, Loss: 3.583787500858307, Final Batch Loss: 0.7272182703018188\n",
      "Epoch 730, Loss: 3.7942711114883423, Final Batch Loss: 0.7012506723403931\n",
      "Epoch 731, Loss: 3.7485352754592896, Final Batch Loss: 0.7521766424179077\n",
      "Epoch 732, Loss: 3.669990360736847, Final Batch Loss: 0.693837583065033\n",
      "Epoch 733, Loss: 3.6843329668045044, Final Batch Loss: 0.7154450416564941\n",
      "Epoch 734, Loss: 3.6914368867874146, Final Batch Loss: 0.7129539847373962\n",
      "Epoch 735, Loss: 3.756260633468628, Final Batch Loss: 0.6836672425270081\n",
      "Epoch 736, Loss: 3.591301202774048, Final Batch Loss: 0.8262343406677246\n",
      "Epoch 737, Loss: 3.7585047483444214, Final Batch Loss: 0.7679376006126404\n",
      "Epoch 738, Loss: 3.7716420888900757, Final Batch Loss: 0.7717945575714111\n",
      "Epoch 739, Loss: 3.7077117562294006, Final Batch Loss: 0.780874490737915\n",
      "Epoch 740, Loss: 3.6261225938796997, Final Batch Loss: 0.678706705570221\n",
      "Epoch 741, Loss: 3.4702417254447937, Final Batch Loss: 0.6258047819137573\n",
      "Epoch 742, Loss: 3.7295520305633545, Final Batch Loss: 0.7359216213226318\n",
      "Epoch 743, Loss: 3.720023274421692, Final Batch Loss: 0.627812385559082\n",
      "Epoch 744, Loss: 3.641180992126465, Final Batch Loss: 0.6980873346328735\n",
      "Epoch 745, Loss: 3.6796088814735413, Final Batch Loss: 0.6843204498291016\n",
      "Epoch 746, Loss: 3.6544098258018494, Final Batch Loss: 0.7698490619659424\n",
      "Epoch 747, Loss: 3.6325467228889465, Final Batch Loss: 0.6747198700904846\n",
      "Epoch 748, Loss: 3.505243241786957, Final Batch Loss: 0.6856034994125366\n",
      "Epoch 749, Loss: 3.6108739376068115, Final Batch Loss: 0.6333324313163757\n",
      "Epoch 750, Loss: 3.5247796773910522, Final Batch Loss: 0.58807373046875\n",
      "Epoch 751, Loss: 3.591858148574829, Final Batch Loss: 0.7102276682853699\n",
      "Epoch 752, Loss: 3.730683147907257, Final Batch Loss: 0.7432434558868408\n",
      "Epoch 753, Loss: 3.654695153236389, Final Batch Loss: 0.7412940263748169\n",
      "Epoch 754, Loss: 3.6059568524360657, Final Batch Loss: 0.697287917137146\n",
      "Epoch 755, Loss: 3.689535915851593, Final Batch Loss: 0.6966065168380737\n",
      "Epoch 756, Loss: 3.657681703567505, Final Batch Loss: 0.7622734904289246\n",
      "Epoch 757, Loss: 3.543448269367218, Final Batch Loss: 0.67928147315979\n",
      "Epoch 758, Loss: 3.6198201775550842, Final Batch Loss: 0.7605575919151306\n",
      "Epoch 759, Loss: 3.5978429317474365, Final Batch Loss: 0.8030973672866821\n",
      "Epoch 760, Loss: 3.583221197128296, Final Batch Loss: 0.7554731965065002\n",
      "Epoch 761, Loss: 3.5967490673065186, Final Batch Loss: 0.7377437949180603\n",
      "Epoch 762, Loss: 3.6433167457580566, Final Batch Loss: 0.7512925863265991\n",
      "Epoch 763, Loss: 3.64797180891037, Final Batch Loss: 0.7602836489677429\n",
      "Epoch 764, Loss: 3.6098055243492126, Final Batch Loss: 0.7549618482589722\n",
      "Epoch 765, Loss: 3.684294879436493, Final Batch Loss: 0.728040337562561\n",
      "Epoch 766, Loss: 3.588225483894348, Final Batch Loss: 0.7336357831954956\n",
      "Epoch 767, Loss: 3.6178893446922302, Final Batch Loss: 0.7692205309867859\n",
      "Epoch 768, Loss: 3.6814897656440735, Final Batch Loss: 0.6822188496589661\n",
      "Epoch 769, Loss: 3.6079180240631104, Final Batch Loss: 0.733659029006958\n",
      "Epoch 770, Loss: 3.606841027736664, Final Batch Loss: 0.8112500309944153\n",
      "Epoch 771, Loss: 3.5259593725204468, Final Batch Loss: 0.62237548828125\n",
      "Epoch 772, Loss: 3.5931140184402466, Final Batch Loss: 0.6367571949958801\n",
      "Epoch 773, Loss: 3.8333672881126404, Final Batch Loss: 0.7273030877113342\n",
      "Epoch 774, Loss: 3.6030797362327576, Final Batch Loss: 0.8257883787155151\n",
      "Epoch 775, Loss: 3.5691640377044678, Final Batch Loss: 0.7520238161087036\n",
      "Epoch 776, Loss: 3.720914304256439, Final Batch Loss: 0.819633424282074\n",
      "Epoch 777, Loss: 3.532601833343506, Final Batch Loss: 0.673687756061554\n",
      "Epoch 778, Loss: 3.392740786075592, Final Batch Loss: 0.6509013175964355\n",
      "Epoch 779, Loss: 3.659563183784485, Final Batch Loss: 0.819099485874176\n",
      "Epoch 780, Loss: 3.551380932331085, Final Batch Loss: 0.687647819519043\n",
      "Epoch 781, Loss: 3.6040456891059875, Final Batch Loss: 0.76661616563797\n",
      "Epoch 782, Loss: 3.418723940849304, Final Batch Loss: 0.6472976207733154\n",
      "Epoch 783, Loss: 3.5878392457962036, Final Batch Loss: 0.7485292553901672\n",
      "Epoch 784, Loss: 3.4679182171821594, Final Batch Loss: 0.6445042490959167\n",
      "Epoch 785, Loss: 3.6665096282958984, Final Batch Loss: 0.7302232980728149\n",
      "Epoch 786, Loss: 3.6897999048233032, Final Batch Loss: 0.7094574570655823\n",
      "Epoch 787, Loss: 3.535147964954376, Final Batch Loss: 0.6839544773101807\n",
      "Epoch 788, Loss: 3.4029605388641357, Final Batch Loss: 0.7135049700737\n",
      "Epoch 789, Loss: 3.361652970314026, Final Batch Loss: 0.7016486525535583\n",
      "Epoch 790, Loss: 3.5934433937072754, Final Batch Loss: 0.6885929107666016\n",
      "Epoch 791, Loss: 3.5077436566352844, Final Batch Loss: 0.6735842823982239\n",
      "Epoch 792, Loss: 3.580420196056366, Final Batch Loss: 0.6788715124130249\n",
      "Epoch 793, Loss: 3.569273591041565, Final Batch Loss: 0.7777413129806519\n",
      "Epoch 794, Loss: 3.54319030046463, Final Batch Loss: 0.6907293200492859\n",
      "Epoch 795, Loss: 3.5300391912460327, Final Batch Loss: 0.7284476161003113\n",
      "Epoch 796, Loss: 3.6389259696006775, Final Batch Loss: 0.6844030022621155\n",
      "Epoch 797, Loss: 3.425357937812805, Final Batch Loss: 0.6488028168678284\n",
      "Epoch 798, Loss: 3.5774685740470886, Final Batch Loss: 0.6782518625259399\n",
      "Epoch 799, Loss: 3.498853921890259, Final Batch Loss: 0.666375994682312\n",
      "Epoch 800, Loss: 3.6595179438591003, Final Batch Loss: 0.7604374885559082\n",
      "Epoch 801, Loss: 3.591379761695862, Final Batch Loss: 0.8116917610168457\n",
      "Epoch 802, Loss: 3.5291258096694946, Final Batch Loss: 0.7203574180603027\n",
      "Epoch 803, Loss: 3.629349410533905, Final Batch Loss: 0.6733874678611755\n",
      "Epoch 804, Loss: 3.4535497426986694, Final Batch Loss: 0.7146860957145691\n",
      "Epoch 805, Loss: 3.4995853900909424, Final Batch Loss: 0.679757833480835\n",
      "Epoch 806, Loss: 3.6066768765449524, Final Batch Loss: 0.7614573836326599\n",
      "Epoch 807, Loss: 3.390221357345581, Final Batch Loss: 0.6314212083816528\n",
      "Epoch 808, Loss: 3.5427903532981873, Final Batch Loss: 0.7096882462501526\n",
      "Epoch 809, Loss: 3.43549245595932, Final Batch Loss: 0.6123813986778259\n",
      "Epoch 810, Loss: 3.648473799228668, Final Batch Loss: 0.7902037501335144\n",
      "Epoch 811, Loss: 3.5775665640830994, Final Batch Loss: 0.7188099026679993\n",
      "Epoch 812, Loss: 3.636757254600525, Final Batch Loss: 0.6770311594009399\n",
      "Epoch 813, Loss: 3.507882297039032, Final Batch Loss: 0.6917425990104675\n",
      "Epoch 814, Loss: 3.584080696105957, Final Batch Loss: 0.703332245349884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 815, Loss: 3.4667702317237854, Final Batch Loss: 0.7454596757888794\n",
      "Epoch 816, Loss: 3.537601411342621, Final Batch Loss: 0.6408413052558899\n",
      "Epoch 817, Loss: 3.492796838283539, Final Batch Loss: 0.767426609992981\n",
      "Epoch 818, Loss: 3.551115930080414, Final Batch Loss: 0.6340333223342896\n",
      "Epoch 819, Loss: 3.5461124181747437, Final Batch Loss: 0.6496687531471252\n",
      "Epoch 820, Loss: 3.584877908229828, Final Batch Loss: 0.7278006672859192\n",
      "Epoch 821, Loss: 3.515230178833008, Final Batch Loss: 0.7370468974113464\n",
      "Epoch 822, Loss: 3.449674606323242, Final Batch Loss: 0.8208447098731995\n",
      "Epoch 823, Loss: 3.586811065673828, Final Batch Loss: 0.8156633973121643\n",
      "Epoch 824, Loss: 3.4789421558380127, Final Batch Loss: 0.7766590118408203\n",
      "Epoch 825, Loss: 3.5726186633110046, Final Batch Loss: 0.7378486394882202\n",
      "Epoch 826, Loss: 3.5396753549575806, Final Batch Loss: 0.7008420825004578\n",
      "Epoch 827, Loss: 3.51154488325119, Final Batch Loss: 0.7984803318977356\n",
      "Epoch 828, Loss: 3.620787560939789, Final Batch Loss: 0.7019463181495667\n",
      "Epoch 829, Loss: 3.3839731216430664, Final Batch Loss: 0.6837552189826965\n",
      "Epoch 830, Loss: 3.54683917760849, Final Batch Loss: 0.6628514528274536\n",
      "Epoch 831, Loss: 3.4416353702545166, Final Batch Loss: 0.6563798189163208\n",
      "Epoch 832, Loss: 3.5522561073303223, Final Batch Loss: 0.7073491811752319\n",
      "Epoch 833, Loss: 3.5672962069511414, Final Batch Loss: 0.7890034914016724\n",
      "Epoch 834, Loss: 3.6239635348320007, Final Batch Loss: 0.7831968069076538\n",
      "Epoch 835, Loss: 3.491089403629303, Final Batch Loss: 0.724273681640625\n",
      "Epoch 836, Loss: 3.4574808478355408, Final Batch Loss: 0.7476528882980347\n",
      "Epoch 837, Loss: 3.434685230255127, Final Batch Loss: 0.6076536178588867\n",
      "Epoch 838, Loss: 3.5652252435684204, Final Batch Loss: 0.6192295551300049\n",
      "Epoch 839, Loss: 3.4969533681869507, Final Batch Loss: 0.6759642362594604\n",
      "Epoch 840, Loss: 3.5028001070022583, Final Batch Loss: 0.6773115396499634\n",
      "Epoch 841, Loss: 3.3758016228675842, Final Batch Loss: 0.6343218088150024\n",
      "Epoch 842, Loss: 3.540992498397827, Final Batch Loss: 0.667595624923706\n",
      "Epoch 843, Loss: 3.3752304911613464, Final Batch Loss: 0.6514406204223633\n",
      "Epoch 844, Loss: 3.2656733989715576, Final Batch Loss: 0.66315096616745\n",
      "Epoch 845, Loss: 3.5710543394088745, Final Batch Loss: 0.6914328336715698\n",
      "Epoch 846, Loss: 3.466920554637909, Final Batch Loss: 0.6366782188415527\n",
      "Epoch 847, Loss: 3.4609341621398926, Final Batch Loss: 0.6978411674499512\n",
      "Epoch 848, Loss: 3.5242782831192017, Final Batch Loss: 0.6581908464431763\n",
      "Epoch 849, Loss: 3.3566611409187317, Final Batch Loss: 0.6331993341445923\n",
      "Epoch 850, Loss: 3.4272512197494507, Final Batch Loss: 0.6700431108474731\n",
      "Epoch 851, Loss: 3.488698422908783, Final Batch Loss: 0.6337021589279175\n",
      "Epoch 852, Loss: 3.3679542541503906, Final Batch Loss: 0.6557498574256897\n",
      "Epoch 853, Loss: 3.3547346591949463, Final Batch Loss: 0.6656957268714905\n",
      "Epoch 854, Loss: 3.3966987133026123, Final Batch Loss: 0.7272495031356812\n",
      "Epoch 855, Loss: 3.4619139432907104, Final Batch Loss: 0.8373385667800903\n",
      "Epoch 856, Loss: 3.4457584619522095, Final Batch Loss: 0.6959315538406372\n",
      "Epoch 857, Loss: 3.3675822615623474, Final Batch Loss: 0.7482604384422302\n",
      "Epoch 858, Loss: 3.3616568446159363, Final Batch Loss: 0.6676432490348816\n",
      "Epoch 859, Loss: 3.400239050388336, Final Batch Loss: 0.6590455770492554\n",
      "Epoch 860, Loss: 3.399480879306793, Final Batch Loss: 0.6726770997047424\n",
      "Epoch 861, Loss: 3.4002779722213745, Final Batch Loss: 0.6501867771148682\n",
      "Epoch 862, Loss: 3.4099904894828796, Final Batch Loss: 0.6350122094154358\n",
      "Epoch 863, Loss: 3.4818716049194336, Final Batch Loss: 0.6573066711425781\n",
      "Epoch 864, Loss: 3.4171916246414185, Final Batch Loss: 0.6972155570983887\n",
      "Epoch 865, Loss: 3.31946861743927, Final Batch Loss: 0.6847922205924988\n",
      "Epoch 866, Loss: 3.4659587740898132, Final Batch Loss: 0.6918842792510986\n",
      "Epoch 867, Loss: 3.3029749989509583, Final Batch Loss: 0.7131745219230652\n",
      "Epoch 868, Loss: 3.2383099794387817, Final Batch Loss: 0.728245735168457\n",
      "Epoch 869, Loss: 3.4261192679405212, Final Batch Loss: 0.7128275036811829\n",
      "Epoch 870, Loss: 3.3113447427749634, Final Batch Loss: 0.5909736752510071\n",
      "Epoch 871, Loss: 3.288734555244446, Final Batch Loss: 0.7430521845817566\n",
      "Epoch 872, Loss: 3.400362968444824, Final Batch Loss: 0.6375332474708557\n",
      "Epoch 873, Loss: 3.2538139820098877, Final Batch Loss: 0.628157913684845\n",
      "Epoch 874, Loss: 3.386797249317169, Final Batch Loss: 0.685679018497467\n",
      "Epoch 875, Loss: 3.380216598510742, Final Batch Loss: 0.619777500629425\n",
      "Epoch 876, Loss: 3.3916818499565125, Final Batch Loss: 0.5908301472663879\n",
      "Epoch 877, Loss: 3.4392611384391785, Final Batch Loss: 0.6588894128799438\n",
      "Epoch 878, Loss: 3.3513256907463074, Final Batch Loss: 0.6898944973945618\n",
      "Epoch 879, Loss: 3.383750557899475, Final Batch Loss: 0.7075203061103821\n",
      "Epoch 880, Loss: 3.59653502702713, Final Batch Loss: 0.6968692541122437\n",
      "Epoch 881, Loss: 3.415141224861145, Final Batch Loss: 0.6560879945755005\n",
      "Epoch 882, Loss: 3.3379722237586975, Final Batch Loss: 0.6558137536048889\n",
      "Epoch 883, Loss: 3.3856875896453857, Final Batch Loss: 0.6519421935081482\n",
      "Epoch 884, Loss: 3.426680088043213, Final Batch Loss: 0.6156731247901917\n",
      "Epoch 885, Loss: 3.380765438079834, Final Batch Loss: 0.752632737159729\n",
      "Epoch 886, Loss: 3.3756386637687683, Final Batch Loss: 0.5531894564628601\n",
      "Epoch 887, Loss: 3.42134690284729, Final Batch Loss: 0.7064604163169861\n",
      "Epoch 888, Loss: 3.493709146976471, Final Batch Loss: 0.7335087656974792\n",
      "Epoch 889, Loss: 3.4327383041381836, Final Batch Loss: 0.703088104724884\n",
      "Epoch 890, Loss: 3.41080379486084, Final Batch Loss: 0.6149256229400635\n",
      "Epoch 891, Loss: 3.213208854198456, Final Batch Loss: 0.5507100820541382\n",
      "Epoch 892, Loss: 3.34499853849411, Final Batch Loss: 0.6307269930839539\n",
      "Epoch 893, Loss: 3.391456186771393, Final Batch Loss: 0.6410925388336182\n",
      "Epoch 894, Loss: 3.2714520692825317, Final Batch Loss: 0.715217113494873\n",
      "Epoch 895, Loss: 3.4049289226531982, Final Batch Loss: 0.7815332412719727\n",
      "Epoch 896, Loss: 3.296095550060272, Final Batch Loss: 0.6279217600822449\n",
      "Epoch 897, Loss: 3.3501622676849365, Final Batch Loss: 0.7069892287254333\n",
      "Epoch 898, Loss: 3.4477111101150513, Final Batch Loss: 0.6707717180252075\n",
      "Epoch 899, Loss: 3.4002886414527893, Final Batch Loss: 0.6373498439788818\n",
      "Epoch 900, Loss: 3.35699200630188, Final Batch Loss: 0.5695982575416565\n",
      "Epoch 901, Loss: 3.448724687099457, Final Batch Loss: 0.6436344981193542\n",
      "Epoch 902, Loss: 3.399761915206909, Final Batch Loss: 0.5809978246688843\n",
      "Epoch 903, Loss: 3.3841710090637207, Final Batch Loss: 0.735434889793396\n",
      "Epoch 904, Loss: 3.2932839393615723, Final Batch Loss: 0.7043604254722595\n",
      "Epoch 905, Loss: 3.282459795475006, Final Batch Loss: 0.6954395174980164\n",
      "Epoch 906, Loss: 3.3651942014694214, Final Batch Loss: 0.6813420057296753\n",
      "Epoch 907, Loss: 3.3048303723335266, Final Batch Loss: 0.6449048519134521\n",
      "Epoch 908, Loss: 3.347698926925659, Final Batch Loss: 0.638899028301239\n",
      "Epoch 909, Loss: 3.3696696162223816, Final Batch Loss: 0.5871593952178955\n",
      "Epoch 910, Loss: 3.4436839818954468, Final Batch Loss: 0.662540078163147\n",
      "Epoch 911, Loss: 3.387074887752533, Final Batch Loss: 0.5396527051925659\n",
      "Epoch 912, Loss: 3.2300854325294495, Final Batch Loss: 0.6807782649993896\n",
      "Epoch 913, Loss: 3.426192581653595, Final Batch Loss: 0.5803898572921753\n",
      "Epoch 914, Loss: 3.1672491431236267, Final Batch Loss: 0.5413903594017029\n",
      "Epoch 915, Loss: 3.3432727456092834, Final Batch Loss: 0.7324836850166321\n",
      "Epoch 916, Loss: 3.3313101530075073, Final Batch Loss: 0.6839059591293335\n",
      "Epoch 917, Loss: 3.2939358353614807, Final Batch Loss: 0.6385277509689331\n",
      "Epoch 918, Loss: 3.4041462540626526, Final Batch Loss: 0.6609203219413757\n",
      "Epoch 919, Loss: 3.2321497201919556, Final Batch Loss: 0.6176377534866333\n",
      "Epoch 920, Loss: 3.2571970224380493, Final Batch Loss: 0.7040759921073914\n",
      "Epoch 921, Loss: 3.3322259187698364, Final Batch Loss: 0.6692570447921753\n",
      "Epoch 922, Loss: 3.3182066679000854, Final Batch Loss: 0.6360669732093811\n",
      "Epoch 923, Loss: 3.302604615688324, Final Batch Loss: 0.6112815141677856\n",
      "Epoch 924, Loss: 3.309258997440338, Final Batch Loss: 0.6942757964134216\n",
      "Epoch 925, Loss: 3.45736426115036, Final Batch Loss: 0.6627107858657837\n",
      "Epoch 926, Loss: 3.296646237373352, Final Batch Loss: 0.6934632062911987\n",
      "Epoch 927, Loss: 3.275524079799652, Final Batch Loss: 0.6966130137443542\n",
      "Epoch 928, Loss: 3.2072291374206543, Final Batch Loss: 0.6447996497154236\n",
      "Epoch 929, Loss: 3.308522880077362, Final Batch Loss: 0.6677020192146301\n",
      "Epoch 930, Loss: 3.273760735988617, Final Batch Loss: 0.6984642744064331\n",
      "Epoch 931, Loss: 3.3680057525634766, Final Batch Loss: 0.7094526290893555\n",
      "Epoch 932, Loss: 3.2969784140586853, Final Batch Loss: 0.5943009853363037\n",
      "Epoch 933, Loss: 3.395661234855652, Final Batch Loss: 0.6631730794906616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 934, Loss: 3.2974398732185364, Final Batch Loss: 0.6875313520431519\n",
      "Epoch 935, Loss: 3.425290048122406, Final Batch Loss: 0.6583263278007507\n",
      "Epoch 936, Loss: 3.369379460811615, Final Batch Loss: 0.7966982126235962\n",
      "Epoch 937, Loss: 3.5731205344200134, Final Batch Loss: 0.6978294849395752\n",
      "Epoch 938, Loss: 3.171746015548706, Final Batch Loss: 0.6076674461364746\n",
      "Epoch 939, Loss: 3.209573745727539, Final Batch Loss: 0.6769977807998657\n",
      "Epoch 940, Loss: 3.3890175819396973, Final Batch Loss: 0.78947913646698\n",
      "Epoch 941, Loss: 3.2272775769233704, Final Batch Loss: 0.5739383101463318\n",
      "Epoch 942, Loss: 3.1247050166130066, Final Batch Loss: 0.5729924440383911\n",
      "Epoch 943, Loss: 3.3432366251945496, Final Batch Loss: 0.5948424935340881\n",
      "Epoch 944, Loss: 3.214491367340088, Final Batch Loss: 0.5904264450073242\n",
      "Epoch 945, Loss: 3.175258696079254, Final Batch Loss: 0.5977370738983154\n",
      "Epoch 946, Loss: 3.2017945051193237, Final Batch Loss: 0.6019980311393738\n",
      "Epoch 947, Loss: 3.246358573436737, Final Batch Loss: 0.6553179025650024\n",
      "Epoch 948, Loss: 3.2841923236846924, Final Batch Loss: 0.6014571785926819\n",
      "Epoch 949, Loss: 3.222850799560547, Final Batch Loss: 0.6499595046043396\n",
      "Epoch 950, Loss: 3.3562859296798706, Final Batch Loss: 0.6799216270446777\n",
      "Epoch 951, Loss: 3.295576572418213, Final Batch Loss: 0.5852397084236145\n",
      "Epoch 952, Loss: 3.2658082842826843, Final Batch Loss: 0.6495090126991272\n",
      "Epoch 953, Loss: 3.1654635667800903, Final Batch Loss: 0.5502430200576782\n",
      "Epoch 954, Loss: 3.2787688970565796, Final Batch Loss: 0.5201780200004578\n",
      "Epoch 955, Loss: 3.3111429810523987, Final Batch Loss: 0.8084903955459595\n",
      "Epoch 956, Loss: 3.1563831567764282, Final Batch Loss: 0.6009243726730347\n",
      "Epoch 957, Loss: 3.1781963109970093, Final Batch Loss: 0.674409806728363\n",
      "Epoch 958, Loss: 3.335334360599518, Final Batch Loss: 0.7318235039710999\n",
      "Epoch 959, Loss: 3.255717396736145, Final Batch Loss: 0.6469634175300598\n",
      "Epoch 960, Loss: 3.2154725193977356, Final Batch Loss: 0.5856781601905823\n",
      "Epoch 961, Loss: 3.2942689657211304, Final Batch Loss: 0.6404389142990112\n",
      "Epoch 962, Loss: 3.3544145226478577, Final Batch Loss: 0.7088649868965149\n",
      "Epoch 963, Loss: 3.381310999393463, Final Batch Loss: 0.6624602675437927\n",
      "Epoch 964, Loss: 3.2773640155792236, Final Batch Loss: 0.6076577305793762\n",
      "Epoch 965, Loss: 3.298541486263275, Final Batch Loss: 0.6046706438064575\n",
      "Epoch 966, Loss: 3.2985751032829285, Final Batch Loss: 0.6113733649253845\n",
      "Epoch 967, Loss: 3.244720220565796, Final Batch Loss: 0.6259965300559998\n",
      "Epoch 968, Loss: 3.3470484614372253, Final Batch Loss: 0.6653046011924744\n",
      "Epoch 969, Loss: 3.338412880897522, Final Batch Loss: 0.7511253952980042\n",
      "Epoch 970, Loss: 3.3395328521728516, Final Batch Loss: 0.6818355917930603\n",
      "Epoch 971, Loss: 3.27523535490036, Final Batch Loss: 0.6326348781585693\n",
      "Epoch 972, Loss: 3.35658597946167, Final Batch Loss: 0.7357469797134399\n",
      "Epoch 973, Loss: 3.265469014644623, Final Batch Loss: 0.6763148903846741\n",
      "Epoch 974, Loss: 3.2894104719161987, Final Batch Loss: 0.6770450472831726\n",
      "Epoch 975, Loss: 3.2755656838417053, Final Batch Loss: 0.6283725500106812\n",
      "Epoch 976, Loss: 3.2821019887924194, Final Batch Loss: 0.6076147556304932\n",
      "Epoch 977, Loss: 3.3859728574752808, Final Batch Loss: 0.7877575755119324\n",
      "Epoch 978, Loss: 3.105559766292572, Final Batch Loss: 0.6242107152938843\n",
      "Epoch 979, Loss: 3.2621920704841614, Final Batch Loss: 0.6384955048561096\n",
      "Epoch 980, Loss: 3.2242189049720764, Final Batch Loss: 0.612539529800415\n",
      "Epoch 981, Loss: 3.127469301223755, Final Batch Loss: 0.6375775933265686\n",
      "Epoch 982, Loss: 3.258215308189392, Final Batch Loss: 0.611430287361145\n",
      "Epoch 983, Loss: 3.1771994829177856, Final Batch Loss: 0.6336496472358704\n",
      "Epoch 984, Loss: 3.300585091114044, Final Batch Loss: 0.7261930704116821\n",
      "Epoch 985, Loss: 3.26998108625412, Final Batch Loss: 0.6848151683807373\n",
      "Epoch 986, Loss: 3.342088043689728, Final Batch Loss: 0.545844316482544\n",
      "Epoch 987, Loss: 3.31007719039917, Final Batch Loss: 0.6051899194717407\n",
      "Epoch 988, Loss: 3.190955400466919, Final Batch Loss: 0.6132984757423401\n",
      "Epoch 989, Loss: 3.374090850353241, Final Batch Loss: 0.7682032585144043\n",
      "Epoch 990, Loss: 3.18121737241745, Final Batch Loss: 0.6437762379646301\n",
      "Epoch 991, Loss: 3.2625373005867004, Final Batch Loss: 0.6403304934501648\n",
      "Epoch 992, Loss: 3.086724579334259, Final Batch Loss: 0.6379591226577759\n",
      "Epoch 993, Loss: 3.469375431537628, Final Batch Loss: 0.6111397743225098\n",
      "Epoch 994, Loss: 3.244178056716919, Final Batch Loss: 0.6310592293739319\n",
      "Epoch 995, Loss: 3.1431092619895935, Final Batch Loss: 0.6353737115859985\n",
      "Epoch 996, Loss: 3.075512945652008, Final Batch Loss: 0.6293959617614746\n",
      "Epoch 997, Loss: 3.199362635612488, Final Batch Loss: 0.5926893353462219\n",
      "Epoch 998, Loss: 3.0897249579429626, Final Batch Loss: 0.677510678768158\n",
      "Epoch 999, Loss: 3.2163687348365784, Final Batch Loss: 0.6555305123329163\n",
      "Epoch 1000, Loss: 3.176491439342499, Final Batch Loss: 0.6794540286064148\n",
      "Epoch 1001, Loss: 3.2772600650787354, Final Batch Loss: 0.6352624893188477\n",
      "Epoch 1002, Loss: 3.1710962653160095, Final Batch Loss: 0.5490607023239136\n",
      "Epoch 1003, Loss: 3.2659682631492615, Final Batch Loss: 0.725344181060791\n",
      "Epoch 1004, Loss: 3.0840861201286316, Final Batch Loss: 0.630088746547699\n",
      "Epoch 1005, Loss: 3.205058455467224, Final Batch Loss: 0.6252639293670654\n",
      "Epoch 1006, Loss: 3.1514360904693604, Final Batch Loss: 0.6816784143447876\n",
      "Epoch 1007, Loss: 3.1834404468536377, Final Batch Loss: 0.7255064249038696\n",
      "Epoch 1008, Loss: 3.1664700508117676, Final Batch Loss: 0.7754031419754028\n",
      "Epoch 1009, Loss: 3.2541431188583374, Final Batch Loss: 0.7034980058670044\n",
      "Epoch 1010, Loss: 3.1980143189430237, Final Batch Loss: 0.6674139499664307\n",
      "Epoch 1011, Loss: 3.2191965579986572, Final Batch Loss: 0.665141224861145\n",
      "Epoch 1012, Loss: 3.155510902404785, Final Batch Loss: 0.6745927333831787\n",
      "Epoch 1013, Loss: 3.1867820024490356, Final Batch Loss: 0.6532856225967407\n",
      "Epoch 1014, Loss: 3.252896785736084, Final Batch Loss: 0.6532173156738281\n",
      "Epoch 1015, Loss: 3.229628026485443, Final Batch Loss: 0.5927879810333252\n",
      "Epoch 1016, Loss: 3.142946422100067, Final Batch Loss: 0.6544787287712097\n",
      "Epoch 1017, Loss: 3.1948095560073853, Final Batch Loss: 0.6287571787834167\n",
      "Epoch 1018, Loss: 3.060185670852661, Final Batch Loss: 0.6254034042358398\n",
      "Epoch 1019, Loss: 3.157051682472229, Final Batch Loss: 0.6367227435112\n",
      "Epoch 1020, Loss: 3.2977866530418396, Final Batch Loss: 0.6977348923683167\n",
      "Epoch 1021, Loss: 3.1857263445854187, Final Batch Loss: 0.65175461769104\n",
      "Epoch 1022, Loss: 3.075220227241516, Final Batch Loss: 0.581122100353241\n",
      "Epoch 1023, Loss: 3.3339527249336243, Final Batch Loss: 0.5699037909507751\n",
      "Epoch 1024, Loss: 3.1443881392478943, Final Batch Loss: 0.5912249088287354\n",
      "Epoch 1025, Loss: 3.109071671962738, Final Batch Loss: 0.6688889265060425\n",
      "Epoch 1026, Loss: 3.150535762310028, Final Batch Loss: 0.7506811618804932\n",
      "Epoch 1027, Loss: 3.1141592264175415, Final Batch Loss: 0.6707022786140442\n",
      "Epoch 1028, Loss: 3.2059385776519775, Final Batch Loss: 0.6432507634162903\n",
      "Epoch 1029, Loss: 3.2915444374084473, Final Batch Loss: 0.7017455697059631\n",
      "Epoch 1030, Loss: 2.9640966057777405, Final Batch Loss: 0.5642021894454956\n",
      "Epoch 1031, Loss: 3.3174972534179688, Final Batch Loss: 0.706180214881897\n",
      "Epoch 1032, Loss: 3.1034072041511536, Final Batch Loss: 0.6254286766052246\n",
      "Epoch 1033, Loss: 3.104471802711487, Final Batch Loss: 0.5994817614555359\n",
      "Epoch 1034, Loss: 3.260492503643036, Final Batch Loss: 0.679563045501709\n",
      "Epoch 1035, Loss: 3.196889281272888, Final Batch Loss: 0.6321426033973694\n",
      "Epoch 1036, Loss: 3.26042377948761, Final Batch Loss: 0.6572869420051575\n",
      "Epoch 1037, Loss: 3.1796762943267822, Final Batch Loss: 0.6011354923248291\n",
      "Epoch 1038, Loss: 3.2036588191986084, Final Batch Loss: 0.5729717016220093\n",
      "Epoch 1039, Loss: 3.239286482334137, Final Batch Loss: 0.6671615839004517\n",
      "Epoch 1040, Loss: 3.2385879158973694, Final Batch Loss: 0.6570490598678589\n",
      "Epoch 1041, Loss: 3.1620249152183533, Final Batch Loss: 0.6670159697532654\n",
      "Epoch 1042, Loss: 3.1032755374908447, Final Batch Loss: 0.6330133676528931\n",
      "Epoch 1043, Loss: 3.2784297466278076, Final Batch Loss: 0.6197124719619751\n",
      "Epoch 1044, Loss: 3.122131645679474, Final Batch Loss: 0.6881729364395142\n",
      "Epoch 1045, Loss: 3.1489407420158386, Final Batch Loss: 0.6168845891952515\n",
      "Epoch 1046, Loss: 3.10219144821167, Final Batch Loss: 0.7769782543182373\n",
      "Epoch 1047, Loss: 3.0478833317756653, Final Batch Loss: 0.5802873373031616\n",
      "Epoch 1048, Loss: 3.087505042552948, Final Batch Loss: 0.6773525476455688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1049, Loss: 3.0928969979286194, Final Batch Loss: 0.6100412607192993\n",
      "Epoch 1050, Loss: 3.2501736879348755, Final Batch Loss: 0.603462278842926\n",
      "Epoch 1051, Loss: 3.1131503582000732, Final Batch Loss: 0.5767608880996704\n",
      "Epoch 1052, Loss: 3.126617431640625, Final Batch Loss: 0.6940739154815674\n",
      "Epoch 1053, Loss: 3.0329174995422363, Final Batch Loss: 0.6441933512687683\n",
      "Epoch 1054, Loss: 3.1977888345718384, Final Batch Loss: 0.5683015584945679\n",
      "Epoch 1055, Loss: 3.1391252875328064, Final Batch Loss: 0.6267123818397522\n",
      "Epoch 1056, Loss: 3.1280662417411804, Final Batch Loss: 0.5369333624839783\n",
      "Epoch 1057, Loss: 3.100320816040039, Final Batch Loss: 0.675839900970459\n",
      "Epoch 1058, Loss: 3.1683704257011414, Final Batch Loss: 0.6720284223556519\n",
      "Epoch 1059, Loss: 3.060379147529602, Final Batch Loss: 0.5627036690711975\n",
      "Epoch 1060, Loss: 3.108841300010681, Final Batch Loss: 0.5864914059638977\n",
      "Epoch 1061, Loss: 3.19815331697464, Final Batch Loss: 0.7656567692756653\n",
      "Epoch 1062, Loss: 3.1426497101783752, Final Batch Loss: 0.7182813882827759\n",
      "Epoch 1063, Loss: 3.0435776114463806, Final Batch Loss: 0.59540194272995\n",
      "Epoch 1064, Loss: 3.1496180295944214, Final Batch Loss: 0.5744152665138245\n",
      "Epoch 1065, Loss: 3.1890807151794434, Final Batch Loss: 0.5592789053916931\n",
      "Epoch 1066, Loss: 3.241642653942108, Final Batch Loss: 0.6201474070549011\n",
      "Epoch 1067, Loss: 3.157033681869507, Final Batch Loss: 0.6338585615158081\n",
      "Epoch 1068, Loss: 3.3019205927848816, Final Batch Loss: 0.6191480755805969\n",
      "Epoch 1069, Loss: 3.048861563205719, Final Batch Loss: 0.5985527634620667\n",
      "Epoch 1070, Loss: 3.154166519641876, Final Batch Loss: 0.6719895005226135\n",
      "Epoch 1071, Loss: 3.0536458492279053, Final Batch Loss: 0.5685915946960449\n",
      "Epoch 1072, Loss: 3.1296510100364685, Final Batch Loss: 0.6477407813072205\n",
      "Epoch 1073, Loss: 3.103858232498169, Final Batch Loss: 0.5629739165306091\n",
      "Epoch 1074, Loss: 3.0607606172561646, Final Batch Loss: 0.6097491979598999\n",
      "Epoch 1075, Loss: 3.0363507866859436, Final Batch Loss: 0.5910526514053345\n",
      "Epoch 1076, Loss: 2.955575406551361, Final Batch Loss: 0.6026471257209778\n",
      "Epoch 1077, Loss: 3.037456691265106, Final Batch Loss: 0.6504790782928467\n",
      "Epoch 1078, Loss: 3.0916849970817566, Final Batch Loss: 0.7026203274726868\n",
      "Epoch 1079, Loss: 3.2011682391166687, Final Batch Loss: 0.6273338198661804\n",
      "Epoch 1080, Loss: 3.080369710922241, Final Batch Loss: 0.7184399962425232\n",
      "Epoch 1081, Loss: 3.1858105063438416, Final Batch Loss: 0.6090364456176758\n",
      "Epoch 1082, Loss: 3.1057021617889404, Final Batch Loss: 0.5130968689918518\n",
      "Epoch 1083, Loss: 3.0839032530784607, Final Batch Loss: 0.6152061820030212\n",
      "Epoch 1084, Loss: 3.2772889733314514, Final Batch Loss: 0.7387533783912659\n",
      "Epoch 1085, Loss: 3.1077396869659424, Final Batch Loss: 0.5436915755271912\n",
      "Epoch 1086, Loss: 3.2353196144104004, Final Batch Loss: 0.6386152505874634\n",
      "Epoch 1087, Loss: 3.06598961353302, Final Batch Loss: 0.616325318813324\n",
      "Epoch 1088, Loss: 3.1131818890571594, Final Batch Loss: 0.6226197481155396\n",
      "Epoch 1089, Loss: 3.1323031783103943, Final Batch Loss: 0.616408109664917\n",
      "Epoch 1090, Loss: 3.1726402044296265, Final Batch Loss: 0.664898157119751\n",
      "Epoch 1091, Loss: 3.170188307762146, Final Batch Loss: 0.5991476774215698\n",
      "Epoch 1092, Loss: 3.1540300846099854, Final Batch Loss: 0.6028019189834595\n",
      "Epoch 1093, Loss: 3.0827285647392273, Final Batch Loss: 0.6547428965568542\n",
      "Epoch 1094, Loss: 3.226428747177124, Final Batch Loss: 0.5591381192207336\n",
      "Epoch 1095, Loss: 3.123357653617859, Final Batch Loss: 0.6384394764900208\n",
      "Epoch 1096, Loss: 3.0138747096061707, Final Batch Loss: 0.5996081233024597\n",
      "Epoch 1097, Loss: 3.144384443759918, Final Batch Loss: 0.5567748546600342\n",
      "Epoch 1098, Loss: 3.062027633190155, Final Batch Loss: 0.6211720705032349\n",
      "Epoch 1099, Loss: 3.2085208892822266, Final Batch Loss: 0.5693254470825195\n",
      "Epoch 1100, Loss: 3.028754413127899, Final Batch Loss: 0.6246662139892578\n",
      "Epoch 1101, Loss: 3.073417544364929, Final Batch Loss: 0.6599845886230469\n",
      "Epoch 1102, Loss: 3.0598414540290833, Final Batch Loss: 0.6799725890159607\n",
      "Epoch 1103, Loss: 3.108892261981964, Final Batch Loss: 0.6519350409507751\n",
      "Epoch 1104, Loss: 3.0919209718704224, Final Batch Loss: 0.6607699990272522\n",
      "Epoch 1105, Loss: 3.047780990600586, Final Batch Loss: 0.6003352999687195\n",
      "Epoch 1106, Loss: 3.007474958896637, Final Batch Loss: 0.6552960276603699\n",
      "Epoch 1107, Loss: 3.063072621822357, Final Batch Loss: 0.6258564591407776\n",
      "Epoch 1108, Loss: 2.8802581429481506, Final Batch Loss: 0.5393370389938354\n",
      "Epoch 1109, Loss: 3.2168480157852173, Final Batch Loss: 0.6238943338394165\n",
      "Epoch 1110, Loss: 3.052119880914688, Final Batch Loss: 0.4917980134487152\n",
      "Epoch 1111, Loss: 2.93324738740921, Final Batch Loss: 0.5449685454368591\n",
      "Epoch 1112, Loss: 3.315441370010376, Final Batch Loss: 0.6099642515182495\n",
      "Epoch 1113, Loss: 3.1978551149368286, Final Batch Loss: 0.6300444006919861\n",
      "Epoch 1114, Loss: 3.204015612602234, Final Batch Loss: 0.6855154633522034\n",
      "Epoch 1115, Loss: 3.162610173225403, Final Batch Loss: 0.5998338460922241\n",
      "Epoch 1116, Loss: 3.1142600774765015, Final Batch Loss: 0.6343163251876831\n",
      "Epoch 1117, Loss: 3.1655160188674927, Final Batch Loss: 0.5842063426971436\n",
      "Epoch 1118, Loss: 3.0433310866355896, Final Batch Loss: 0.6010802388191223\n",
      "Epoch 1119, Loss: 3.1323692202568054, Final Batch Loss: 0.6712120771408081\n",
      "Epoch 1120, Loss: 3.047887682914734, Final Batch Loss: 0.600142240524292\n",
      "Epoch 1121, Loss: 3.0961577892303467, Final Batch Loss: 0.5211734175682068\n",
      "Epoch 1122, Loss: 3.004569947719574, Final Batch Loss: 0.6458501815795898\n",
      "Epoch 1123, Loss: 2.8925687670707703, Final Batch Loss: 0.6566261053085327\n",
      "Epoch 1124, Loss: 3.0674415230751038, Final Batch Loss: 0.6984265446662903\n",
      "Epoch 1125, Loss: 3.280416488647461, Final Batch Loss: 0.823041558265686\n",
      "Epoch 1126, Loss: 2.94069904088974, Final Batch Loss: 0.6234745383262634\n",
      "Epoch 1127, Loss: 3.038972318172455, Final Batch Loss: 0.5824296474456787\n",
      "Epoch 1128, Loss: 3.096871078014374, Final Batch Loss: 0.6231314539909363\n",
      "Epoch 1129, Loss: 3.0975837111473083, Final Batch Loss: 0.6173648238182068\n",
      "Epoch 1130, Loss: 3.117810904979706, Final Batch Loss: 0.6826637983322144\n",
      "Epoch 1131, Loss: 3.0792859196662903, Final Batch Loss: 0.6614559292793274\n",
      "Epoch 1132, Loss: 3.1390862464904785, Final Batch Loss: 0.5671989917755127\n",
      "Epoch 1133, Loss: 3.046457052230835, Final Batch Loss: 0.6913182139396667\n",
      "Epoch 1134, Loss: 2.9937289357185364, Final Batch Loss: 0.5131047964096069\n",
      "Epoch 1135, Loss: 3.1396390199661255, Final Batch Loss: 0.636547863483429\n",
      "Epoch 1136, Loss: 2.975817084312439, Final Batch Loss: 0.5533095598220825\n",
      "Epoch 1137, Loss: 2.939531445503235, Final Batch Loss: 0.5400047302246094\n",
      "Epoch 1138, Loss: 3.0924535393714905, Final Batch Loss: 0.5632741451263428\n",
      "Epoch 1139, Loss: 3.070443481206894, Final Batch Loss: 0.4986165463924408\n",
      "Epoch 1140, Loss: 3.1181867718696594, Final Batch Loss: 0.6901635527610779\n",
      "Epoch 1141, Loss: 2.946177363395691, Final Batch Loss: 0.5824088454246521\n",
      "Epoch 1142, Loss: 3.065288722515106, Final Batch Loss: 0.643524169921875\n",
      "Epoch 1143, Loss: 3.1942042112350464, Final Batch Loss: 0.7224165201187134\n",
      "Epoch 1144, Loss: 2.833613306283951, Final Batch Loss: 0.48100221157073975\n",
      "Epoch 1145, Loss: 3.027333974838257, Final Batch Loss: 0.6282266974449158\n",
      "Epoch 1146, Loss: 2.9463517665863037, Final Batch Loss: 0.6709696054458618\n",
      "Epoch 1147, Loss: 2.9344467520713806, Final Batch Loss: 0.5808247327804565\n",
      "Epoch 1148, Loss: 3.071787178516388, Final Batch Loss: 0.597214937210083\n",
      "Epoch 1149, Loss: 2.97308349609375, Final Batch Loss: 0.6002307534217834\n",
      "Epoch 1150, Loss: 2.9478434324264526, Final Batch Loss: 0.5646153688430786\n",
      "Epoch 1151, Loss: 2.9510117173194885, Final Batch Loss: 0.6043392419815063\n",
      "Epoch 1152, Loss: 2.993872880935669, Final Batch Loss: 0.6872557401657104\n",
      "Epoch 1153, Loss: 3.088507115840912, Final Batch Loss: 0.5776410698890686\n",
      "Epoch 1154, Loss: 3.0694273710250854, Final Batch Loss: 0.6623241305351257\n",
      "Epoch 1155, Loss: 3.1693816781044006, Final Batch Loss: 0.6404276490211487\n",
      "Epoch 1156, Loss: 2.9669845700263977, Final Batch Loss: 0.6011382341384888\n",
      "Epoch 1157, Loss: 3.0004072189331055, Final Batch Loss: 0.6401506662368774\n",
      "Epoch 1158, Loss: 2.9553357362747192, Final Batch Loss: 0.5390563607215881\n",
      "Epoch 1159, Loss: 2.866237699985504, Final Batch Loss: 0.6771497130393982\n",
      "Epoch 1160, Loss: 2.9750806093215942, Final Batch Loss: 0.5979205965995789\n",
      "Epoch 1161, Loss: 3.021847665309906, Final Batch Loss: 0.5774877667427063\n",
      "Epoch 1162, Loss: 3.037905514240265, Final Batch Loss: 0.6355670094490051\n",
      "Epoch 1163, Loss: 2.8580421805381775, Final Batch Loss: 0.5785282254219055\n",
      "Epoch 1164, Loss: 3.1253620386123657, Final Batch Loss: 0.6274054050445557\n",
      "Epoch 1165, Loss: 3.1521248817443848, Final Batch Loss: 0.7188289761543274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1166, Loss: 3.0163812041282654, Final Batch Loss: 0.5645742416381836\n",
      "Epoch 1167, Loss: 3.112269878387451, Final Batch Loss: 0.6137621402740479\n",
      "Epoch 1168, Loss: 2.9903604984283447, Final Batch Loss: 0.579729437828064\n",
      "Epoch 1169, Loss: 2.9715590476989746, Final Batch Loss: 0.5345096588134766\n",
      "Epoch 1170, Loss: 2.9695725440979004, Final Batch Loss: 0.5546098947525024\n",
      "Epoch 1171, Loss: 3.132481873035431, Final Batch Loss: 0.6573463678359985\n",
      "Epoch 1172, Loss: 3.0215349197387695, Final Batch Loss: 0.5659177303314209\n",
      "Epoch 1173, Loss: 3.0538036823272705, Final Batch Loss: 0.5849133133888245\n",
      "Epoch 1174, Loss: 3.103124439716339, Final Batch Loss: 0.6593012809753418\n",
      "Epoch 1175, Loss: 3.0125468373298645, Final Batch Loss: 0.6582438945770264\n",
      "Epoch 1176, Loss: 2.96516877412796, Final Batch Loss: 0.5232487320899963\n",
      "Epoch 1177, Loss: 2.869567394256592, Final Batch Loss: 0.6155006885528564\n",
      "Epoch 1178, Loss: 3.096614897251129, Final Batch Loss: 0.7317357659339905\n",
      "Epoch 1179, Loss: 3.05886971950531, Final Batch Loss: 0.6384023427963257\n",
      "Epoch 1180, Loss: 3.1560264825820923, Final Batch Loss: 0.6795186400413513\n",
      "Epoch 1181, Loss: 3.1372743248939514, Final Batch Loss: 0.6104326248168945\n",
      "Epoch 1182, Loss: 3.120938539505005, Final Batch Loss: 0.644506573677063\n",
      "Epoch 1183, Loss: 2.9987950921058655, Final Batch Loss: 0.589484453201294\n",
      "Epoch 1184, Loss: 2.9587196111679077, Final Batch Loss: 0.6219286322593689\n",
      "Epoch 1185, Loss: 3.0146037340164185, Final Batch Loss: 0.5869794487953186\n",
      "Epoch 1186, Loss: 3.2492725253105164, Final Batch Loss: 0.6155145168304443\n",
      "Epoch 1187, Loss: 3.1079342365264893, Final Batch Loss: 0.6708505749702454\n",
      "Epoch 1188, Loss: 2.9215227365493774, Final Batch Loss: 0.5731961727142334\n",
      "Epoch 1189, Loss: 2.942545235157013, Final Batch Loss: 0.5925552845001221\n",
      "Epoch 1190, Loss: 3.020886778831482, Final Batch Loss: 0.6836017370223999\n",
      "Epoch 1191, Loss: 2.967067450284958, Final Batch Loss: 0.6319673657417297\n",
      "Epoch 1192, Loss: 2.9398884177207947, Final Batch Loss: 0.5349128246307373\n",
      "Epoch 1193, Loss: 3.047552466392517, Final Batch Loss: 0.529492974281311\n",
      "Epoch 1194, Loss: 2.966604709625244, Final Batch Loss: 0.6747281551361084\n",
      "Epoch 1195, Loss: 3.0435516834259033, Final Batch Loss: 0.6032472252845764\n",
      "Epoch 1196, Loss: 2.8878559470176697, Final Batch Loss: 0.5910046100616455\n",
      "Epoch 1197, Loss: 2.9512789249420166, Final Batch Loss: 0.6101431250572205\n",
      "Epoch 1198, Loss: 2.9669485092163086, Final Batch Loss: 0.6968768835067749\n",
      "Epoch 1199, Loss: 3.0354755520820618, Final Batch Loss: 0.5757001638412476\n",
      "Epoch 1200, Loss: 2.9927388429641724, Final Batch Loss: 0.6163125038146973\n",
      "Epoch 1201, Loss: 2.981142997741699, Final Batch Loss: 0.5872265696525574\n",
      "Epoch 1202, Loss: 2.9718706011772156, Final Batch Loss: 0.5992302298545837\n",
      "Epoch 1203, Loss: 3.1692615747451782, Final Batch Loss: 0.6376209259033203\n",
      "Epoch 1204, Loss: 2.9093009531497955, Final Batch Loss: 0.5012204647064209\n",
      "Epoch 1205, Loss: 2.9807685017585754, Final Batch Loss: 0.5653584003448486\n",
      "Epoch 1206, Loss: 2.8154111206531525, Final Batch Loss: 0.49310895800590515\n",
      "Epoch 1207, Loss: 3.0770052671432495, Final Batch Loss: 0.6143101453781128\n",
      "Epoch 1208, Loss: 3.042318046092987, Final Batch Loss: 0.6569489240646362\n",
      "Epoch 1209, Loss: 2.983235001564026, Final Batch Loss: 0.6874637603759766\n",
      "Epoch 1210, Loss: 2.9073528051376343, Final Batch Loss: 0.5790479183197021\n",
      "Epoch 1211, Loss: 2.9915289878845215, Final Batch Loss: 0.642871618270874\n",
      "Epoch 1212, Loss: 3.03357857465744, Final Batch Loss: 0.632633626461029\n",
      "Epoch 1213, Loss: 3.1570552587509155, Final Batch Loss: 0.5626309514045715\n",
      "Epoch 1214, Loss: 2.900534212589264, Final Batch Loss: 0.5717419981956482\n",
      "Epoch 1215, Loss: 2.9815409779548645, Final Batch Loss: 0.5742618441581726\n",
      "Epoch 1216, Loss: 2.8883913159370422, Final Batch Loss: 0.5461133122444153\n",
      "Epoch 1217, Loss: 3.018026649951935, Final Batch Loss: 0.5640811324119568\n",
      "Epoch 1218, Loss: 2.978234827518463, Final Batch Loss: 0.5645859241485596\n",
      "Epoch 1219, Loss: 3.03465735912323, Final Batch Loss: 0.6444344520568848\n",
      "Epoch 1220, Loss: 2.9998499751091003, Final Batch Loss: 0.6405186057090759\n",
      "Epoch 1221, Loss: 2.8917969465255737, Final Batch Loss: 0.5725386142730713\n",
      "Epoch 1222, Loss: 2.897219657897949, Final Batch Loss: 0.5655637979507446\n",
      "Epoch 1223, Loss: 2.851876139640808, Final Batch Loss: 0.6102588176727295\n",
      "Epoch 1224, Loss: 3.077846348285675, Final Batch Loss: 0.6547151803970337\n",
      "Epoch 1225, Loss: 2.781039595603943, Final Batch Loss: 0.6193022131919861\n",
      "Epoch 1226, Loss: 2.9121119379997253, Final Batch Loss: 0.6574605703353882\n",
      "Epoch 1227, Loss: 2.8677562475204468, Final Batch Loss: 0.5865064263343811\n",
      "Epoch 1228, Loss: 2.9403873682022095, Final Batch Loss: 0.6547848582267761\n",
      "Epoch 1229, Loss: 3.047840118408203, Final Batch Loss: 0.5776360630989075\n",
      "Epoch 1230, Loss: 2.9140793681144714, Final Batch Loss: 0.6143835186958313\n",
      "Epoch 1231, Loss: 2.9562443494796753, Final Batch Loss: 0.591870903968811\n",
      "Epoch 1232, Loss: 3.0026533603668213, Final Batch Loss: 0.6145141124725342\n",
      "Epoch 1233, Loss: 2.918017625808716, Final Batch Loss: 0.544653058052063\n",
      "Epoch 1234, Loss: 3.0351954102516174, Final Batch Loss: 0.7448689937591553\n",
      "Epoch 1235, Loss: 2.919573485851288, Final Batch Loss: 0.6174720525741577\n",
      "Epoch 1236, Loss: 2.8892709612846375, Final Batch Loss: 0.4960055947303772\n",
      "Epoch 1237, Loss: 3.0518022179603577, Final Batch Loss: 0.5234720706939697\n",
      "Epoch 1238, Loss: 2.9942046999931335, Final Batch Loss: 0.5775330662727356\n",
      "Epoch 1239, Loss: 3.077131748199463, Final Batch Loss: 0.6796692609786987\n",
      "Epoch 1240, Loss: 3.1104740500450134, Final Batch Loss: 0.6848337054252625\n",
      "Epoch 1241, Loss: 2.8593926429748535, Final Batch Loss: 0.6127111315727234\n",
      "Epoch 1242, Loss: 2.8632280230522156, Final Batch Loss: 0.5181554555892944\n",
      "Epoch 1243, Loss: 2.914763331413269, Final Batch Loss: 0.5566605925559998\n",
      "Epoch 1244, Loss: 2.8981207609176636, Final Batch Loss: 0.5591834783554077\n",
      "Epoch 1245, Loss: 3.0272756814956665, Final Batch Loss: 0.5992447137832642\n",
      "Epoch 1246, Loss: 2.867167294025421, Final Batch Loss: 0.553030788898468\n",
      "Epoch 1247, Loss: 2.9413583874702454, Final Batch Loss: 0.6453403234481812\n",
      "Epoch 1248, Loss: 2.9977777004241943, Final Batch Loss: 0.5455526113510132\n",
      "Epoch 1249, Loss: 2.9559914469718933, Final Batch Loss: 0.5880511403083801\n",
      "Epoch 1250, Loss: 2.890106439590454, Final Batch Loss: 0.5452174544334412\n",
      "Epoch 1251, Loss: 2.9548829793930054, Final Batch Loss: 0.5386146306991577\n",
      "Epoch 1252, Loss: 2.86748206615448, Final Batch Loss: 0.4822824001312256\n",
      "Epoch 1253, Loss: 3.014167547225952, Final Batch Loss: 0.5540635585784912\n",
      "Epoch 1254, Loss: 2.7542671859264374, Final Batch Loss: 0.4773424565792084\n",
      "Epoch 1255, Loss: 2.818565309047699, Final Batch Loss: 0.5842794179916382\n",
      "Epoch 1256, Loss: 2.871891438961029, Final Batch Loss: 0.5394909381866455\n",
      "Epoch 1257, Loss: 2.7994277477264404, Final Batch Loss: 0.509255588054657\n",
      "Epoch 1258, Loss: 2.902216672897339, Final Batch Loss: 0.5304973721504211\n",
      "Epoch 1259, Loss: 2.7936102151870728, Final Batch Loss: 0.5478552579879761\n",
      "Epoch 1260, Loss: 2.931007146835327, Final Batch Loss: 0.5835644006729126\n",
      "Epoch 1261, Loss: 2.9048346281051636, Final Batch Loss: 0.5716857314109802\n",
      "Epoch 1262, Loss: 2.8860729336738586, Final Batch Loss: 0.589378297328949\n",
      "Epoch 1263, Loss: 2.88752743601799, Final Batch Loss: 0.5526986122131348\n",
      "Epoch 1264, Loss: 3.0822518467903137, Final Batch Loss: 0.6581287384033203\n",
      "Epoch 1265, Loss: 2.923213541507721, Final Batch Loss: 0.5505503416061401\n",
      "Epoch 1266, Loss: 2.8662968277931213, Final Batch Loss: 0.5143115520477295\n",
      "Epoch 1267, Loss: 3.048979699611664, Final Batch Loss: 0.6748656034469604\n",
      "Epoch 1268, Loss: 2.7707464694976807, Final Batch Loss: 0.48574280738830566\n",
      "Epoch 1269, Loss: 2.9815922379493713, Final Batch Loss: 0.6083769202232361\n",
      "Epoch 1270, Loss: 2.9389365315437317, Final Batch Loss: 0.5598509311676025\n",
      "Epoch 1271, Loss: 2.9467325806617737, Final Batch Loss: 0.5847517251968384\n",
      "Epoch 1272, Loss: 2.8484019935131073, Final Batch Loss: 0.5544008612632751\n",
      "Epoch 1273, Loss: 2.922149509191513, Final Batch Loss: 0.6363949179649353\n",
      "Epoch 1274, Loss: 2.8497194647789, Final Batch Loss: 0.542874813079834\n",
      "Epoch 1275, Loss: 2.9084168076515198, Final Batch Loss: 0.5657230615615845\n",
      "Epoch 1276, Loss: 2.831724613904953, Final Batch Loss: 0.6103159785270691\n",
      "Epoch 1277, Loss: 2.7899582982063293, Final Batch Loss: 0.5776236653327942\n",
      "Epoch 1278, Loss: 2.9213922023773193, Final Batch Loss: 0.5403900742530823\n",
      "Epoch 1279, Loss: 2.824514329433441, Final Batch Loss: 0.5963701009750366\n",
      "Epoch 1280, Loss: 2.9657556414604187, Final Batch Loss: 0.7086479663848877\n",
      "Epoch 1281, Loss: 2.8806338906288147, Final Batch Loss: 0.6046947836875916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1282, Loss: 3.095049500465393, Final Batch Loss: 0.5696596503257751\n",
      "Epoch 1283, Loss: 3.033641517162323, Final Batch Loss: 0.5416191816329956\n",
      "Epoch 1284, Loss: 2.913210690021515, Final Batch Loss: 0.57127445936203\n",
      "Epoch 1285, Loss: 2.884980261325836, Final Batch Loss: 0.592375636100769\n",
      "Epoch 1286, Loss: 2.9030643701553345, Final Batch Loss: 0.598780632019043\n",
      "Epoch 1287, Loss: 2.906621813774109, Final Batch Loss: 0.6224241256713867\n",
      "Epoch 1288, Loss: 2.8815419673919678, Final Batch Loss: 0.5346021056175232\n",
      "Epoch 1289, Loss: 2.734760284423828, Final Batch Loss: 0.6144412755966187\n",
      "Epoch 1290, Loss: 2.8665584325790405, Final Batch Loss: 0.565559446811676\n",
      "Epoch 1291, Loss: 2.819209337234497, Final Batch Loss: 0.5260084867477417\n",
      "Epoch 1292, Loss: 2.8268163800239563, Final Batch Loss: 0.5643186569213867\n",
      "Epoch 1293, Loss: 2.779478669166565, Final Batch Loss: 0.6125249862670898\n",
      "Epoch 1294, Loss: 2.863826632499695, Final Batch Loss: 0.6043267846107483\n",
      "Epoch 1295, Loss: 2.905553698539734, Final Batch Loss: 0.6462913751602173\n",
      "Epoch 1296, Loss: 2.9843910932540894, Final Batch Loss: 0.5556309223175049\n",
      "Epoch 1297, Loss: 2.836174190044403, Final Batch Loss: 0.6878291964530945\n",
      "Epoch 1298, Loss: 2.915432929992676, Final Batch Loss: 0.5292119383811951\n",
      "Epoch 1299, Loss: 2.7784438133239746, Final Batch Loss: 0.44931668043136597\n",
      "Epoch 1300, Loss: 2.8864575028419495, Final Batch Loss: 0.5175486207008362\n",
      "Epoch 1301, Loss: 2.9577809870243073, Final Batch Loss: 0.6795849800109863\n",
      "Epoch 1302, Loss: 3.057801365852356, Final Batch Loss: 0.6056632995605469\n",
      "Epoch 1303, Loss: 2.9799907207489014, Final Batch Loss: 0.6165978908538818\n",
      "Epoch 1304, Loss: 2.8300649523735046, Final Batch Loss: 0.5479234457015991\n",
      "Epoch 1305, Loss: 2.9591455161571503, Final Batch Loss: 0.5414109230041504\n",
      "Epoch 1306, Loss: 2.8970725536346436, Final Batch Loss: 0.5839691758155823\n",
      "Epoch 1307, Loss: 2.870430737733841, Final Batch Loss: 0.573763370513916\n",
      "Epoch 1308, Loss: 2.800900638103485, Final Batch Loss: 0.6212900280952454\n",
      "Epoch 1309, Loss: 2.995325744152069, Final Batch Loss: 0.6927540898323059\n",
      "Epoch 1310, Loss: 2.7728287279605865, Final Batch Loss: 0.6507424712181091\n",
      "Epoch 1311, Loss: 2.900000035762787, Final Batch Loss: 0.619503378868103\n",
      "Epoch 1312, Loss: 2.896293580532074, Final Batch Loss: 0.6565827131271362\n",
      "Epoch 1313, Loss: 2.877112865447998, Final Batch Loss: 0.6155318021774292\n",
      "Epoch 1314, Loss: 2.8978880047798157, Final Batch Loss: 0.5044205188751221\n",
      "Epoch 1315, Loss: 2.808513104915619, Final Batch Loss: 0.4996744394302368\n",
      "Epoch 1316, Loss: 2.9243891537189484, Final Batch Loss: 0.6639358401298523\n",
      "Epoch 1317, Loss: 2.96785569190979, Final Batch Loss: 0.6419304609298706\n",
      "Epoch 1318, Loss: 2.8198224008083344, Final Batch Loss: 0.5071504712104797\n",
      "Epoch 1319, Loss: 2.852318048477173, Final Batch Loss: 0.6025371551513672\n",
      "Epoch 1320, Loss: 2.9623213410377502, Final Batch Loss: 0.5564599633216858\n",
      "Epoch 1321, Loss: 2.9305070638656616, Final Batch Loss: 0.6085748672485352\n",
      "Epoch 1322, Loss: 2.9615272879600525, Final Batch Loss: 0.6111459136009216\n",
      "Epoch 1323, Loss: 2.8316847681999207, Final Batch Loss: 0.5293073654174805\n",
      "Epoch 1324, Loss: 2.8651068806648254, Final Batch Loss: 0.5348083972930908\n",
      "Epoch 1325, Loss: 3.0367905497550964, Final Batch Loss: 0.630961000919342\n",
      "Epoch 1326, Loss: 2.8090145885944366, Final Batch Loss: 0.5960705280303955\n",
      "Epoch 1327, Loss: 2.7635700404644012, Final Batch Loss: 0.5930829644203186\n",
      "Epoch 1328, Loss: 2.868248164653778, Final Batch Loss: 0.5853081345558167\n",
      "Epoch 1329, Loss: 2.856509506702423, Final Batch Loss: 0.6164195537567139\n",
      "Epoch 1330, Loss: 2.7742419242858887, Final Batch Loss: 0.5628513097763062\n",
      "Epoch 1331, Loss: 2.9354259371757507, Final Batch Loss: 0.5960053205490112\n",
      "Epoch 1332, Loss: 2.7866621613502502, Final Batch Loss: 0.537234902381897\n",
      "Epoch 1333, Loss: 2.918162167072296, Final Batch Loss: 0.6756477952003479\n",
      "Epoch 1334, Loss: 2.830446779727936, Final Batch Loss: 0.5733517408370972\n",
      "Epoch 1335, Loss: 2.842384934425354, Final Batch Loss: 0.5846047401428223\n",
      "Epoch 1336, Loss: 2.6966187953948975, Final Batch Loss: 0.5192058682441711\n",
      "Epoch 1337, Loss: 2.881542384624481, Final Batch Loss: 0.6502656936645508\n",
      "Epoch 1338, Loss: 2.7358440160751343, Final Batch Loss: 0.5897507071495056\n",
      "Epoch 1339, Loss: 2.8052600622177124, Final Batch Loss: 0.566571831703186\n",
      "Epoch 1340, Loss: 2.7828238010406494, Final Batch Loss: 0.7685452699661255\n",
      "Epoch 1341, Loss: 2.9352845549583435, Final Batch Loss: 0.5791764259338379\n",
      "Epoch 1342, Loss: 2.887187659740448, Final Batch Loss: 0.5410208702087402\n",
      "Epoch 1343, Loss: 2.8237447440624237, Final Batch Loss: 0.5729147791862488\n",
      "Epoch 1344, Loss: 2.865954875946045, Final Batch Loss: 0.6372590661048889\n",
      "Epoch 1345, Loss: 2.82339346408844, Final Batch Loss: 0.5480436682701111\n",
      "Epoch 1346, Loss: 2.807663768529892, Final Batch Loss: 0.5437227487564087\n",
      "Epoch 1347, Loss: 2.8675816655158997, Final Batch Loss: 0.6510903239250183\n",
      "Epoch 1348, Loss: 2.722943603992462, Final Batch Loss: 0.5788878798484802\n",
      "Epoch 1349, Loss: 2.717887371778488, Final Batch Loss: 0.4148807227611542\n",
      "Epoch 1350, Loss: 2.800043076276779, Final Batch Loss: 0.5837821960449219\n",
      "Epoch 1351, Loss: 2.867885172367096, Final Batch Loss: 0.5965532064437866\n",
      "Epoch 1352, Loss: 2.826432764530182, Final Batch Loss: 0.6371139883995056\n",
      "Epoch 1353, Loss: 2.917538583278656, Final Batch Loss: 0.5776507258415222\n",
      "Epoch 1354, Loss: 2.792757570743561, Final Batch Loss: 0.5040917992591858\n",
      "Epoch 1355, Loss: 2.80335134267807, Final Batch Loss: 0.5396424531936646\n",
      "Epoch 1356, Loss: 2.9017435908317566, Final Batch Loss: 0.571512758731842\n",
      "Epoch 1357, Loss: 2.746016502380371, Final Batch Loss: 0.45043981075286865\n",
      "Epoch 1358, Loss: 2.8235089778900146, Final Batch Loss: 0.5198504328727722\n",
      "Epoch 1359, Loss: 2.7800264954566956, Final Batch Loss: 0.5461744070053101\n",
      "Epoch 1360, Loss: 3.0644269585609436, Final Batch Loss: 0.523260772228241\n",
      "Epoch 1361, Loss: 2.777255564928055, Final Batch Loss: 0.5879014730453491\n",
      "Epoch 1362, Loss: 2.7099647223949432, Final Batch Loss: 0.555404007434845\n",
      "Epoch 1363, Loss: 2.913863182067871, Final Batch Loss: 0.6429964900016785\n",
      "Epoch 1364, Loss: 2.7991157174110413, Final Batch Loss: 0.4834665358066559\n",
      "Epoch 1365, Loss: 2.829241693019867, Final Batch Loss: 0.5819286704063416\n",
      "Epoch 1366, Loss: 2.6765317916870117, Final Batch Loss: 0.5470235347747803\n",
      "Epoch 1367, Loss: 2.9089847803115845, Final Batch Loss: 0.5516049861907959\n",
      "Epoch 1368, Loss: 2.8234366178512573, Final Batch Loss: 0.6293907761573792\n",
      "Epoch 1369, Loss: 2.9213727712631226, Final Batch Loss: 0.7084662914276123\n",
      "Epoch 1370, Loss: 2.801876336336136, Final Batch Loss: 0.4828619062900543\n",
      "Epoch 1371, Loss: 2.819339156150818, Final Batch Loss: 0.6162146925926208\n",
      "Epoch 1372, Loss: 3.0094693303108215, Final Batch Loss: 0.6312294006347656\n",
      "Epoch 1373, Loss: 2.785031020641327, Final Batch Loss: 0.6167833209037781\n",
      "Epoch 1374, Loss: 2.914247155189514, Final Batch Loss: 0.5883365869522095\n",
      "Epoch 1375, Loss: 2.7096601128578186, Final Batch Loss: 0.5694600939750671\n",
      "Epoch 1376, Loss: 2.7433815598487854, Final Batch Loss: 0.5806797742843628\n",
      "Epoch 1377, Loss: 2.928810775279999, Final Batch Loss: 0.6106305718421936\n",
      "Epoch 1378, Loss: 2.8546186089515686, Final Batch Loss: 0.5196014046669006\n",
      "Epoch 1379, Loss: 2.830607086420059, Final Batch Loss: 0.537027895450592\n",
      "Epoch 1380, Loss: 2.6772134006023407, Final Batch Loss: 0.554921567440033\n",
      "Epoch 1381, Loss: 2.74045330286026, Final Batch Loss: 0.5158045291900635\n",
      "Epoch 1382, Loss: 2.872383236885071, Final Batch Loss: 0.6209960579872131\n",
      "Epoch 1383, Loss: 2.8640570044517517, Final Batch Loss: 0.5731430053710938\n",
      "Epoch 1384, Loss: 2.7814345955848694, Final Batch Loss: 0.492881178855896\n",
      "Epoch 1385, Loss: 2.8904555439949036, Final Batch Loss: 0.4921104609966278\n",
      "Epoch 1386, Loss: 2.850515127182007, Final Batch Loss: 0.5129619836807251\n",
      "Epoch 1387, Loss: 2.849985897541046, Final Batch Loss: 0.5654509663581848\n",
      "Epoch 1388, Loss: 2.761431097984314, Final Batch Loss: 0.5462570190429688\n",
      "Epoch 1389, Loss: 2.7416002452373505, Final Batch Loss: 0.48837611079216003\n",
      "Epoch 1390, Loss: 2.8462149798870087, Final Batch Loss: 0.48479515314102173\n",
      "Epoch 1391, Loss: 2.821629285812378, Final Batch Loss: 0.5545641183853149\n",
      "Epoch 1392, Loss: 2.851219207048416, Final Batch Loss: 0.6093105673789978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1393, Loss: 2.93294095993042, Final Batch Loss: 0.6398836970329285\n",
      "Epoch 1394, Loss: 2.8984909653663635, Final Batch Loss: 0.576663076877594\n",
      "Epoch 1395, Loss: 2.7829034626483917, Final Batch Loss: 0.4984278380870819\n",
      "Epoch 1396, Loss: 2.794533371925354, Final Batch Loss: 0.5099480748176575\n",
      "Epoch 1397, Loss: 2.7723166942596436, Final Batch Loss: 0.46654438972473145\n",
      "Epoch 1398, Loss: 2.8894878029823303, Final Batch Loss: 0.5130332112312317\n",
      "Epoch 1399, Loss: 2.8804683089256287, Final Batch Loss: 0.6627775430679321\n",
      "Epoch 1400, Loss: 2.8699002861976624, Final Batch Loss: 0.5222565531730652\n",
      "Epoch 1401, Loss: 2.692684292793274, Final Batch Loss: 0.518452525138855\n",
      "Epoch 1402, Loss: 2.7791323363780975, Final Batch Loss: 0.5093792080879211\n",
      "Epoch 1403, Loss: 2.872053563594818, Final Batch Loss: 0.556286633014679\n",
      "Epoch 1404, Loss: 2.8059521317481995, Final Batch Loss: 0.5204970836639404\n",
      "Epoch 1405, Loss: 2.774373173713684, Final Batch Loss: 0.5658631324768066\n",
      "Epoch 1406, Loss: 2.710280478000641, Final Batch Loss: 0.5230934023857117\n",
      "Epoch 1407, Loss: 2.810032457113266, Final Batch Loss: 0.5784981846809387\n",
      "Epoch 1408, Loss: 2.84750035405159, Final Batch Loss: 0.4570636451244354\n",
      "Epoch 1409, Loss: 2.8413912057876587, Final Batch Loss: 0.6155465841293335\n",
      "Epoch 1410, Loss: 2.8066349625587463, Final Batch Loss: 0.5577780604362488\n",
      "Epoch 1411, Loss: 2.85723215341568, Final Batch Loss: 0.6320240497589111\n",
      "Epoch 1412, Loss: 2.7287557125091553, Final Batch Loss: 0.5059611201286316\n",
      "Epoch 1413, Loss: 2.8016157746315002, Final Batch Loss: 0.5487474203109741\n",
      "Epoch 1414, Loss: 2.79658842086792, Final Batch Loss: 0.6611157059669495\n",
      "Epoch 1415, Loss: 2.8235965967178345, Final Batch Loss: 0.5812013745307922\n",
      "Epoch 1416, Loss: 2.7837210595607758, Final Batch Loss: 0.5894010066986084\n",
      "Epoch 1417, Loss: 2.9448822140693665, Final Batch Loss: 0.5768839120864868\n",
      "Epoch 1418, Loss: 2.82134348154068, Final Batch Loss: 0.5352057814598083\n",
      "Epoch 1419, Loss: 2.7792318761348724, Final Batch Loss: 0.5423324108123779\n",
      "Epoch 1420, Loss: 2.7000867128372192, Final Batch Loss: 0.5741751790046692\n",
      "Epoch 1421, Loss: 2.6804098784923553, Final Batch Loss: 0.6202229857444763\n",
      "Epoch 1422, Loss: 2.871226131916046, Final Batch Loss: 0.6207759976387024\n",
      "Epoch 1423, Loss: 2.9665074348449707, Final Batch Loss: 0.579652726650238\n",
      "Epoch 1424, Loss: 2.813078820705414, Final Batch Loss: 0.48895367980003357\n",
      "Epoch 1425, Loss: 2.70001482963562, Final Batch Loss: 0.5100279450416565\n",
      "Epoch 1426, Loss: 2.807474374771118, Final Batch Loss: 0.5065456032752991\n",
      "Epoch 1427, Loss: 2.709914803504944, Final Batch Loss: 0.5821828842163086\n",
      "Epoch 1428, Loss: 2.6728988885879517, Final Batch Loss: 0.5184116363525391\n",
      "Epoch 1429, Loss: 2.7971580922603607, Final Batch Loss: 0.6079856157302856\n",
      "Epoch 1430, Loss: 2.8608848452568054, Final Batch Loss: 0.5303715467453003\n",
      "Epoch 1431, Loss: 2.712905764579773, Final Batch Loss: 0.57740718126297\n",
      "Epoch 1432, Loss: 2.7840002179145813, Final Batch Loss: 0.5106509923934937\n",
      "Epoch 1433, Loss: 2.758044183254242, Final Batch Loss: 0.6191312670707703\n",
      "Epoch 1434, Loss: 2.7936782836914062, Final Batch Loss: 0.573840320110321\n",
      "Epoch 1435, Loss: 2.706346571445465, Final Batch Loss: 0.5256675481796265\n",
      "Epoch 1436, Loss: 2.7955575585365295, Final Batch Loss: 0.5376586318016052\n",
      "Epoch 1437, Loss: 2.7384690046310425, Final Batch Loss: 0.569170355796814\n",
      "Epoch 1438, Loss: 2.881395637989044, Final Batch Loss: 0.6080157160758972\n",
      "Epoch 1439, Loss: 2.771450996398926, Final Batch Loss: 0.623355507850647\n",
      "Epoch 1440, Loss: 2.697336107492447, Final Batch Loss: 0.5138915777206421\n",
      "Epoch 1441, Loss: 2.7908880710601807, Final Batch Loss: 0.5607652068138123\n",
      "Epoch 1442, Loss: 2.6696510016918182, Final Batch Loss: 0.5584136247634888\n",
      "Epoch 1443, Loss: 2.8686731457710266, Final Batch Loss: 0.6719772219657898\n",
      "Epoch 1444, Loss: 2.8023940920829773, Final Batch Loss: 0.5917877554893494\n",
      "Epoch 1445, Loss: 2.8201584219932556, Final Batch Loss: 0.5642037391662598\n",
      "Epoch 1446, Loss: 2.8478565514087677, Final Batch Loss: 0.5710594654083252\n",
      "Epoch 1447, Loss: 2.8293694257736206, Final Batch Loss: 0.5557385683059692\n",
      "Epoch 1448, Loss: 2.730746805667877, Final Batch Loss: 0.4717843532562256\n",
      "Epoch 1449, Loss: 2.833094894886017, Final Batch Loss: 0.5445429682731628\n",
      "Epoch 1450, Loss: 2.8187496066093445, Final Batch Loss: 0.6330714821815491\n",
      "Epoch 1451, Loss: 2.6996482610702515, Final Batch Loss: 0.528968334197998\n",
      "Epoch 1452, Loss: 2.6573952734470367, Final Batch Loss: 0.4722529351711273\n",
      "Epoch 1453, Loss: 2.812263071537018, Final Batch Loss: 0.6110096573829651\n",
      "Epoch 1454, Loss: 2.8367855846881866, Final Batch Loss: 0.5179706811904907\n",
      "Epoch 1455, Loss: 2.7352152466773987, Final Batch Loss: 0.48239272832870483\n",
      "Epoch 1456, Loss: 2.9241588413715363, Final Batch Loss: 0.6114110946655273\n",
      "Epoch 1457, Loss: 2.6946024298667908, Final Batch Loss: 0.5431223511695862\n",
      "Epoch 1458, Loss: 2.806234508752823, Final Batch Loss: 0.6225123405456543\n",
      "Epoch 1459, Loss: 2.707873284816742, Final Batch Loss: 0.5894118547439575\n",
      "Epoch 1460, Loss: 2.8262417912483215, Final Batch Loss: 0.5836478471755981\n",
      "Epoch 1461, Loss: 2.783316969871521, Final Batch Loss: 0.5012593269348145\n",
      "Epoch 1462, Loss: 2.6923119127750397, Final Batch Loss: 0.5684535503387451\n",
      "Epoch 1463, Loss: 2.7860400080680847, Final Batch Loss: 0.5601444840431213\n",
      "Epoch 1464, Loss: 2.720116972923279, Final Batch Loss: 0.5909270644187927\n",
      "Epoch 1465, Loss: 2.736438751220703, Final Batch Loss: 0.5918665528297424\n",
      "Epoch 1466, Loss: 2.6706779301166534, Final Batch Loss: 0.5170197486877441\n",
      "Epoch 1467, Loss: 2.7904983162879944, Final Batch Loss: 0.579633891582489\n",
      "Epoch 1468, Loss: 2.6994577944278717, Final Batch Loss: 0.5777488946914673\n",
      "Epoch 1469, Loss: 2.780847132205963, Final Batch Loss: 0.6301485300064087\n",
      "Epoch 1470, Loss: 2.800950825214386, Final Batch Loss: 0.5797562599182129\n",
      "Epoch 1471, Loss: 2.8509185314178467, Final Batch Loss: 0.4929664731025696\n",
      "Epoch 1472, Loss: 2.691397935152054, Final Batch Loss: 0.5641429424285889\n",
      "Epoch 1473, Loss: 2.699920177459717, Final Batch Loss: 0.49468207359313965\n",
      "Epoch 1474, Loss: 2.760406494140625, Final Batch Loss: 0.5626820921897888\n",
      "Epoch 1475, Loss: 2.744768649339676, Final Batch Loss: 0.538517415523529\n",
      "Epoch 1476, Loss: 2.7686559557914734, Final Batch Loss: 0.6718987822532654\n",
      "Epoch 1477, Loss: 2.733353614807129, Final Batch Loss: 0.6590298414230347\n",
      "Epoch 1478, Loss: 2.705685079097748, Final Batch Loss: 0.6128140687942505\n",
      "Epoch 1479, Loss: 2.6416298747062683, Final Batch Loss: 0.5049299001693726\n",
      "Epoch 1480, Loss: 2.672499716281891, Final Batch Loss: 0.5098862648010254\n",
      "Epoch 1481, Loss: 2.8407139778137207, Final Batch Loss: 0.6066466569900513\n",
      "Epoch 1482, Loss: 2.6647371649742126, Final Batch Loss: 0.5367085933685303\n",
      "Epoch 1483, Loss: 2.7005313336849213, Final Batch Loss: 0.5464566349983215\n",
      "Epoch 1484, Loss: 2.6948013603687286, Final Batch Loss: 0.5740013718605042\n",
      "Epoch 1485, Loss: 2.662743002176285, Final Batch Loss: 0.5343601703643799\n",
      "Epoch 1486, Loss: 2.864975333213806, Final Batch Loss: 0.7088133692741394\n",
      "Epoch 1487, Loss: 2.7428587079048157, Final Batch Loss: 0.5277329683303833\n",
      "Epoch 1488, Loss: 2.7735792994499207, Final Batch Loss: 0.5111557841300964\n",
      "Epoch 1489, Loss: 2.727005362510681, Final Batch Loss: 0.5774178504943848\n",
      "Epoch 1490, Loss: 2.6550419628620148, Final Batch Loss: 0.5359769463539124\n",
      "Epoch 1491, Loss: 2.7726851999759674, Final Batch Loss: 0.5234278440475464\n",
      "Epoch 1492, Loss: 2.7072371542453766, Final Batch Loss: 0.47937557101249695\n",
      "Epoch 1493, Loss: 2.543816089630127, Final Batch Loss: 0.47583431005477905\n",
      "Epoch 1494, Loss: 2.7076792418956757, Final Batch Loss: 0.6748009324073792\n",
      "Epoch 1495, Loss: 2.8549026250839233, Final Batch Loss: 0.4997687041759491\n",
      "Epoch 1496, Loss: 2.5929333865642548, Final Batch Loss: 0.4815642535686493\n",
      "Epoch 1497, Loss: 2.6473541259765625, Final Batch Loss: 0.5426322817802429\n",
      "Epoch 1498, Loss: 2.723348945379257, Final Batch Loss: 0.5732297897338867\n",
      "Epoch 1499, Loss: 2.6139031052589417, Final Batch Loss: 0.5338109135627747\n",
      "Epoch 1500, Loss: 2.7507753670215607, Final Batch Loss: 0.5393155217170715\n",
      "Epoch 1501, Loss: 2.695488750934601, Final Batch Loss: 0.46977898478507996\n",
      "Epoch 1502, Loss: 2.638079047203064, Final Batch Loss: 0.547895073890686\n",
      "Epoch 1503, Loss: 2.6349746584892273, Final Batch Loss: 0.506351888179779\n",
      "Epoch 1504, Loss: 2.6961192190647125, Final Batch Loss: 0.5954867601394653\n",
      "Epoch 1505, Loss: 2.75907626748085, Final Batch Loss: 0.5340209007263184\n",
      "Epoch 1506, Loss: 2.6662890911102295, Final Batch Loss: 0.5169466733932495\n",
      "Epoch 1507, Loss: 2.7161396145820618, Final Batch Loss: 0.5909510254859924\n",
      "Epoch 1508, Loss: 2.757551848888397, Final Batch Loss: 0.6532691717147827\n",
      "Epoch 1509, Loss: 2.7051780223846436, Final Batch Loss: 0.5323768854141235\n",
      "Epoch 1510, Loss: 2.5628853142261505, Final Batch Loss: 0.4965762197971344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1511, Loss: 2.7051559686660767, Final Batch Loss: 0.6196540594100952\n",
      "Epoch 1512, Loss: 2.678219348192215, Final Batch Loss: 0.5834619402885437\n",
      "Epoch 1513, Loss: 2.7525760531425476, Final Batch Loss: 0.5133490562438965\n",
      "Epoch 1514, Loss: 2.7084745466709137, Final Batch Loss: 0.6013553738594055\n",
      "Epoch 1515, Loss: 2.6725276112556458, Final Batch Loss: 0.504212498664856\n",
      "Epoch 1516, Loss: 2.507662534713745, Final Batch Loss: 0.5085469484329224\n",
      "Epoch 1517, Loss: 2.6009156107902527, Final Batch Loss: 0.5342074036598206\n",
      "Epoch 1518, Loss: 2.862189769744873, Final Batch Loss: 0.6005723476409912\n",
      "Epoch 1519, Loss: 2.539687305688858, Final Batch Loss: 0.5309904217720032\n",
      "Epoch 1520, Loss: 2.8171989917755127, Final Batch Loss: 0.5546829700469971\n",
      "Epoch 1521, Loss: 2.5808404684066772, Final Batch Loss: 0.5444957613945007\n",
      "Epoch 1522, Loss: 2.6368490755558014, Final Batch Loss: 0.5453577637672424\n",
      "Epoch 1523, Loss: 2.6201188266277313, Final Batch Loss: 0.6026677489280701\n",
      "Epoch 1524, Loss: 2.6917909383773804, Final Batch Loss: 0.4882187843322754\n",
      "Epoch 1525, Loss: 2.634125590324402, Final Batch Loss: 0.5869905352592468\n",
      "Epoch 1526, Loss: 2.6347624361515045, Final Batch Loss: 0.5429090857505798\n",
      "Epoch 1527, Loss: 2.688037872314453, Final Batch Loss: 0.5360828042030334\n",
      "Epoch 1528, Loss: 2.802238166332245, Final Batch Loss: 0.5865501165390015\n",
      "Epoch 1529, Loss: 2.722677230834961, Final Batch Loss: 0.5269607305526733\n",
      "Epoch 1530, Loss: 2.8195375204086304, Final Batch Loss: 0.534552276134491\n",
      "Epoch 1531, Loss: 2.876067101955414, Final Batch Loss: 0.49922704696655273\n",
      "Epoch 1532, Loss: 2.7007839381694794, Final Batch Loss: 0.5970309972763062\n",
      "Epoch 1533, Loss: 2.6601493656635284, Final Batch Loss: 0.491023451089859\n",
      "Epoch 1534, Loss: 2.780899554491043, Final Batch Loss: 0.574553370475769\n",
      "Epoch 1535, Loss: 2.7315292358398438, Final Batch Loss: 0.5309911370277405\n",
      "Epoch 1536, Loss: 2.7019495964050293, Final Batch Loss: 0.5332785248756409\n",
      "Epoch 1537, Loss: 2.851900637149811, Final Batch Loss: 0.582636833190918\n",
      "Epoch 1538, Loss: 2.4896070659160614, Final Batch Loss: 0.48036113381385803\n",
      "Epoch 1539, Loss: 2.7470442950725555, Final Batch Loss: 0.5649276375770569\n",
      "Epoch 1540, Loss: 2.801831066608429, Final Batch Loss: 0.6256721019744873\n",
      "Epoch 1541, Loss: 2.5673469603061676, Final Batch Loss: 0.501183271408081\n",
      "Epoch 1542, Loss: 2.8390375673770905, Final Batch Loss: 0.7349611520767212\n",
      "Epoch 1543, Loss: 2.7479795813560486, Final Batch Loss: 0.5942773818969727\n",
      "Epoch 1544, Loss: 2.6740699112415314, Final Batch Loss: 0.5836809873580933\n",
      "Epoch 1545, Loss: 2.6571733951568604, Final Batch Loss: 0.4523135721683502\n",
      "Epoch 1546, Loss: 2.8424232006073, Final Batch Loss: 0.7095701098442078\n",
      "Epoch 1547, Loss: 2.695102244615555, Final Batch Loss: 0.5185569524765015\n",
      "Epoch 1548, Loss: 2.5527414977550507, Final Batch Loss: 0.5208076238632202\n",
      "Epoch 1549, Loss: 2.6474801898002625, Final Batch Loss: 0.5306084156036377\n",
      "Epoch 1550, Loss: 2.8679007291793823, Final Batch Loss: 0.5183382630348206\n",
      "Epoch 1551, Loss: 2.732865035533905, Final Batch Loss: 0.5112464427947998\n",
      "Epoch 1552, Loss: 2.720494508743286, Final Batch Loss: 0.5123839378356934\n",
      "Epoch 1553, Loss: 2.7893607318401337, Final Batch Loss: 0.5386184453964233\n",
      "Epoch 1554, Loss: 2.6581903398036957, Final Batch Loss: 0.5783798098564148\n",
      "Epoch 1555, Loss: 2.7523578107357025, Final Batch Loss: 0.6779743432998657\n",
      "Epoch 1556, Loss: 2.6620351374149323, Final Batch Loss: 0.6102221608161926\n",
      "Epoch 1557, Loss: 2.8219599425792694, Final Batch Loss: 0.49766650795936584\n",
      "Epoch 1558, Loss: 2.59945946931839, Final Batch Loss: 0.4715563654899597\n",
      "Epoch 1559, Loss: 2.759545177221298, Final Batch Loss: 0.5309122800827026\n",
      "Epoch 1560, Loss: 2.642498105764389, Final Batch Loss: 0.47449734807014465\n",
      "Epoch 1561, Loss: 2.721527934074402, Final Batch Loss: 0.6364926695823669\n",
      "Epoch 1562, Loss: 2.69460591673851, Final Batch Loss: 0.5721146464347839\n",
      "Epoch 1563, Loss: 2.564615786075592, Final Batch Loss: 0.5173510313034058\n",
      "Epoch 1564, Loss: 2.5983542799949646, Final Batch Loss: 0.5369291305541992\n",
      "Epoch 1565, Loss: 2.785820722579956, Final Batch Loss: 0.5429090261459351\n",
      "Epoch 1566, Loss: 2.617403566837311, Final Batch Loss: 0.5161980390548706\n",
      "Epoch 1567, Loss: 2.7388554215431213, Final Batch Loss: 0.5689836740493774\n",
      "Epoch 1568, Loss: 2.7906285524368286, Final Batch Loss: 0.60093754529953\n",
      "Epoch 1569, Loss: 2.694007158279419, Final Batch Loss: 0.5534470677375793\n",
      "Epoch 1570, Loss: 2.7850625216960907, Final Batch Loss: 0.6629859209060669\n",
      "Epoch 1571, Loss: 2.6759999990463257, Final Batch Loss: 0.4852922260761261\n",
      "Epoch 1572, Loss: 2.803462564945221, Final Batch Loss: 0.5536640882492065\n",
      "Epoch 1573, Loss: 2.8360195755958557, Final Batch Loss: 0.4627397656440735\n",
      "Epoch 1574, Loss: 2.6542321741580963, Final Batch Loss: 0.6315633058547974\n",
      "Epoch 1575, Loss: 2.6851909160614014, Final Batch Loss: 0.6048527956008911\n",
      "Epoch 1576, Loss: 2.6501955091953278, Final Batch Loss: 0.5768001079559326\n",
      "Epoch 1577, Loss: 2.6083088517189026, Final Batch Loss: 0.5076673030853271\n",
      "Epoch 1578, Loss: 2.672089457511902, Final Batch Loss: 0.5365279316902161\n",
      "Epoch 1579, Loss: 2.5834367871284485, Final Batch Loss: 0.5191254019737244\n",
      "Epoch 1580, Loss: 2.5296203196048737, Final Batch Loss: 0.49963679909706116\n",
      "Epoch 1581, Loss: 2.577196776866913, Final Batch Loss: 0.5073626637458801\n",
      "Epoch 1582, Loss: 2.5852837562561035, Final Batch Loss: 0.5159763693809509\n",
      "Epoch 1583, Loss: 2.784198522567749, Final Batch Loss: 0.5584201812744141\n",
      "Epoch 1584, Loss: 2.463173508644104, Final Batch Loss: 0.4632854163646698\n",
      "Epoch 1585, Loss: 2.727651685476303, Final Batch Loss: 0.5721905827522278\n",
      "Epoch 1586, Loss: 2.587166428565979, Final Batch Loss: 0.44546765089035034\n",
      "Epoch 1587, Loss: 2.825482875108719, Final Batch Loss: 0.5654239058494568\n",
      "Epoch 1588, Loss: 2.7798236906528473, Final Batch Loss: 0.5400997996330261\n",
      "Epoch 1589, Loss: 2.5288411676883698, Final Batch Loss: 0.49269983172416687\n",
      "Epoch 1590, Loss: 2.6200098395347595, Final Batch Loss: 0.4957561492919922\n",
      "Epoch 1591, Loss: 2.744268298149109, Final Batch Loss: 0.5252425670623779\n",
      "Epoch 1592, Loss: 2.6979233026504517, Final Batch Loss: 0.48640966415405273\n",
      "Epoch 1593, Loss: 2.64389368891716, Final Batch Loss: 0.48500314354896545\n",
      "Epoch 1594, Loss: 2.7830236554145813, Final Batch Loss: 0.5129777193069458\n",
      "Epoch 1595, Loss: 2.684920608997345, Final Batch Loss: 0.5931043028831482\n",
      "Epoch 1596, Loss: 2.7933084070682526, Final Batch Loss: 0.5140562057495117\n",
      "Epoch 1597, Loss: 2.5456780195236206, Final Batch Loss: 0.41319847106933594\n",
      "Epoch 1598, Loss: 2.560620754957199, Final Batch Loss: 0.49897730350494385\n",
      "Epoch 1599, Loss: 2.7027435898780823, Final Batch Loss: 0.607210636138916\n",
      "Epoch 1600, Loss: 2.6048310697078705, Final Batch Loss: 0.5493128299713135\n",
      "Epoch 1601, Loss: 2.5234890282154083, Final Batch Loss: 0.49365606904029846\n",
      "Epoch 1602, Loss: 2.8414422273635864, Final Batch Loss: 0.5813468098640442\n",
      "Epoch 1603, Loss: 2.62996506690979, Final Batch Loss: 0.5075534582138062\n",
      "Epoch 1604, Loss: 2.64114049077034, Final Batch Loss: 0.49727264046669006\n",
      "Epoch 1605, Loss: 2.5873106718063354, Final Batch Loss: 0.5159615874290466\n",
      "Epoch 1606, Loss: 2.645056903362274, Final Batch Loss: 0.5388756394386292\n",
      "Epoch 1607, Loss: 2.6640617847442627, Final Batch Loss: 0.4886336922645569\n",
      "Epoch 1608, Loss: 2.5293405652046204, Final Batch Loss: 0.5225327610969543\n",
      "Epoch 1609, Loss: 2.5548086762428284, Final Batch Loss: 0.4808157682418823\n",
      "Epoch 1610, Loss: 2.6359812915325165, Final Batch Loss: 0.5620374083518982\n",
      "Epoch 1611, Loss: 2.6907611191272736, Final Batch Loss: 0.4903081953525543\n",
      "Epoch 1612, Loss: 2.5736304819583893, Final Batch Loss: 0.5607789754867554\n",
      "Epoch 1613, Loss: 2.5419858396053314, Final Batch Loss: 0.5309963822364807\n",
      "Epoch 1614, Loss: 2.5688306391239166, Final Batch Loss: 0.5435687899589539\n",
      "Epoch 1615, Loss: 2.57025608420372, Final Batch Loss: 0.5357955098152161\n",
      "Epoch 1616, Loss: 2.613883674144745, Final Batch Loss: 0.5906197428703308\n",
      "Epoch 1617, Loss: 2.606392115354538, Final Batch Loss: 0.4554636478424072\n",
      "Epoch 1618, Loss: 2.574660450220108, Final Batch Loss: 0.4193045198917389\n",
      "Epoch 1619, Loss: 2.6993712186813354, Final Batch Loss: 0.537273645401001\n",
      "Epoch 1620, Loss: 2.6962379217147827, Final Batch Loss: 0.5359338521957397\n",
      "Epoch 1621, Loss: 2.697156846523285, Final Batch Loss: 0.6867150664329529\n",
      "Epoch 1622, Loss: 2.6872343719005585, Final Batch Loss: 0.5808196067810059\n",
      "Epoch 1623, Loss: 2.6759681701660156, Final Batch Loss: 0.5385184288024902\n",
      "Epoch 1624, Loss: 2.6462093591690063, Final Batch Loss: 0.6439217329025269\n",
      "Epoch 1625, Loss: 2.6341629028320312, Final Batch Loss: 0.5346726179122925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1626, Loss: 2.724540591239929, Final Batch Loss: 0.5804985165596008\n",
      "Epoch 1627, Loss: 2.5772441029548645, Final Batch Loss: 0.5781037211418152\n",
      "Epoch 1628, Loss: 2.642917275428772, Final Batch Loss: 0.6090264320373535\n",
      "Epoch 1629, Loss: 2.6480129957199097, Final Batch Loss: 0.5636753439903259\n",
      "Epoch 1630, Loss: 2.5985591113567352, Final Batch Loss: 0.49511560797691345\n",
      "Epoch 1631, Loss: 2.667780727148056, Final Batch Loss: 0.6049508452415466\n",
      "Epoch 1632, Loss: 2.7065044045448303, Final Batch Loss: 0.577643632888794\n",
      "Epoch 1633, Loss: 2.795363187789917, Final Batch Loss: 0.5387208461761475\n",
      "Epoch 1634, Loss: 2.6252598762512207, Final Batch Loss: 0.5083907246589661\n",
      "Epoch 1635, Loss: 2.611361503601074, Final Batch Loss: 0.5198080539703369\n",
      "Epoch 1636, Loss: 2.5697686672210693, Final Batch Loss: 0.5479133725166321\n",
      "Epoch 1637, Loss: 2.6309251189231873, Final Batch Loss: 0.518538773059845\n",
      "Epoch 1638, Loss: 2.7073987126350403, Final Batch Loss: 0.5640615820884705\n",
      "Epoch 1639, Loss: 2.5728534162044525, Final Batch Loss: 0.5589618682861328\n",
      "Epoch 1640, Loss: 2.599658340215683, Final Batch Loss: 0.5364684462547302\n",
      "Epoch 1641, Loss: 2.6760659217834473, Final Batch Loss: 0.6195914149284363\n",
      "Epoch 1642, Loss: 2.6109489798545837, Final Batch Loss: 0.4644617438316345\n",
      "Epoch 1643, Loss: 2.549565762281418, Final Batch Loss: 0.47950056195259094\n",
      "Epoch 1644, Loss: 2.6074202954769135, Final Batch Loss: 0.4305042326450348\n",
      "Epoch 1645, Loss: 2.60353621840477, Final Batch Loss: 0.536415159702301\n",
      "Epoch 1646, Loss: 2.6929483711719513, Final Batch Loss: 0.4948505163192749\n",
      "Epoch 1647, Loss: 2.711226999759674, Final Batch Loss: 0.5168669819831848\n",
      "Epoch 1648, Loss: 2.5002008378505707, Final Batch Loss: 0.4624565839767456\n",
      "Epoch 1649, Loss: 2.7115544974803925, Final Batch Loss: 0.4380654990673065\n",
      "Epoch 1650, Loss: 2.85919725894928, Final Batch Loss: 0.557942807674408\n",
      "Epoch 1651, Loss: 2.5771614015102386, Final Batch Loss: 0.46778857707977295\n",
      "Epoch 1652, Loss: 2.7354702949523926, Final Batch Loss: 0.5192681550979614\n",
      "Epoch 1653, Loss: 2.5665290355682373, Final Batch Loss: 0.5192152261734009\n",
      "Epoch 1654, Loss: 2.618112415075302, Final Batch Loss: 0.6088913083076477\n",
      "Epoch 1655, Loss: 2.5447409749031067, Final Batch Loss: 0.5204680562019348\n",
      "Epoch 1656, Loss: 2.6089521050453186, Final Batch Loss: 0.5257900357246399\n",
      "Epoch 1657, Loss: 2.7789775133132935, Final Batch Loss: 0.6147198677062988\n",
      "Epoch 1658, Loss: 2.6950147449970245, Final Batch Loss: 0.5348830223083496\n",
      "Epoch 1659, Loss: 2.5448372662067413, Final Batch Loss: 0.4994208812713623\n",
      "Epoch 1660, Loss: 2.7530627250671387, Final Batch Loss: 0.5824505686759949\n",
      "Epoch 1661, Loss: 2.488600939512253, Final Batch Loss: 0.5808280110359192\n",
      "Epoch 1662, Loss: 2.5684999227523804, Final Batch Loss: 0.5564254522323608\n",
      "Epoch 1663, Loss: 2.5522278249263763, Final Batch Loss: 0.44569870829582214\n",
      "Epoch 1664, Loss: 2.667453557252884, Final Batch Loss: 0.49709656834602356\n",
      "Epoch 1665, Loss: 2.5330013930797577, Final Batch Loss: 0.5576679110527039\n",
      "Epoch 1666, Loss: 2.5766543447971344, Final Batch Loss: 0.5742639303207397\n",
      "Epoch 1667, Loss: 2.692509740591049, Final Batch Loss: 0.5181934833526611\n",
      "Epoch 1668, Loss: 2.6773957014083862, Final Batch Loss: 0.5928233861923218\n",
      "Epoch 1669, Loss: 2.6194045543670654, Final Batch Loss: 0.514880895614624\n",
      "Epoch 1670, Loss: 2.8088302612304688, Final Batch Loss: 0.5475112795829773\n",
      "Epoch 1671, Loss: 2.680719256401062, Final Batch Loss: 0.4984949827194214\n",
      "Epoch 1672, Loss: 2.5732638239860535, Final Batch Loss: 0.5948968529701233\n",
      "Epoch 1673, Loss: 2.67641618847847, Final Batch Loss: 0.5151041746139526\n",
      "Epoch 1674, Loss: 2.6847954094409943, Final Batch Loss: 0.5164526700973511\n",
      "Epoch 1675, Loss: 2.520538240671158, Final Batch Loss: 0.5408720374107361\n",
      "Epoch 1676, Loss: 2.663244664669037, Final Batch Loss: 0.49021703004837036\n",
      "Epoch 1677, Loss: 2.741812765598297, Final Batch Loss: 0.5080406665802002\n",
      "Epoch 1678, Loss: 2.6766435503959656, Final Batch Loss: 0.43707191944122314\n",
      "Epoch 1679, Loss: 2.7261644899845123, Final Batch Loss: 0.5261237621307373\n",
      "Epoch 1680, Loss: 2.6789127588272095, Final Batch Loss: 0.5815834999084473\n",
      "Epoch 1681, Loss: 2.5780001878738403, Final Batch Loss: 0.5729700922966003\n",
      "Epoch 1682, Loss: 2.7287097573280334, Final Batch Loss: 0.5511530637741089\n",
      "Epoch 1683, Loss: 2.708740472793579, Final Batch Loss: 0.58995121717453\n",
      "Epoch 1684, Loss: 2.6931342780590057, Final Batch Loss: 0.5301695466041565\n",
      "Epoch 1685, Loss: 2.5441401600837708, Final Batch Loss: 0.5957022309303284\n",
      "Epoch 1686, Loss: 2.61771696805954, Final Batch Loss: 0.520950973033905\n",
      "Epoch 1687, Loss: 2.8334808945655823, Final Batch Loss: 0.5636173486709595\n",
      "Epoch 1688, Loss: 2.5850230753421783, Final Batch Loss: 0.5495997071266174\n",
      "Epoch 1689, Loss: 2.6258549988269806, Final Batch Loss: 0.4581102430820465\n",
      "Epoch 1690, Loss: 2.6189627051353455, Final Batch Loss: 0.44728320837020874\n",
      "Epoch 1691, Loss: 2.584719479084015, Final Batch Loss: 0.5069584846496582\n",
      "Epoch 1692, Loss: 2.610698878765106, Final Batch Loss: 0.5831452012062073\n",
      "Epoch 1693, Loss: 2.602051258087158, Final Batch Loss: 0.5280726552009583\n",
      "Epoch 1694, Loss: 2.669589549303055, Final Batch Loss: 0.6075400114059448\n",
      "Epoch 1695, Loss: 2.791604846715927, Final Batch Loss: 0.6863024234771729\n",
      "Epoch 1696, Loss: 2.648881882429123, Final Batch Loss: 0.5627593994140625\n",
      "Epoch 1697, Loss: 2.7817652225494385, Final Batch Loss: 0.5020468235015869\n",
      "Epoch 1698, Loss: 2.592932939529419, Final Batch Loss: 0.522466778755188\n",
      "Epoch 1699, Loss: 2.5649124681949615, Final Batch Loss: 0.4350326359272003\n",
      "Epoch 1700, Loss: 2.499573767185211, Final Batch Loss: 0.4886791408061981\n",
      "Epoch 1701, Loss: 2.4927369952201843, Final Batch Loss: 0.4681640565395355\n",
      "Epoch 1702, Loss: 2.627163350582123, Final Batch Loss: 0.5096497535705566\n",
      "Epoch 1703, Loss: 2.7021936178207397, Final Batch Loss: 0.5162487626075745\n",
      "Epoch 1704, Loss: 2.7033490538597107, Final Batch Loss: 0.4874478578567505\n",
      "Epoch 1705, Loss: 2.5950330197811127, Final Batch Loss: 0.5310535430908203\n",
      "Epoch 1706, Loss: 2.4332872331142426, Final Batch Loss: 0.5056748986244202\n",
      "Epoch 1707, Loss: 2.706984132528305, Final Batch Loss: 0.5090926289558411\n",
      "Epoch 1708, Loss: 2.9075982570648193, Final Batch Loss: 0.5556778907775879\n",
      "Epoch 1709, Loss: 2.573869615793228, Final Batch Loss: 0.5187654495239258\n",
      "Epoch 1710, Loss: 2.5253568589687347, Final Batch Loss: 0.43926337361335754\n",
      "Epoch 1711, Loss: 2.8092061281204224, Final Batch Loss: 0.5768081545829773\n",
      "Epoch 1712, Loss: 2.5016814470291138, Final Batch Loss: 0.4547657370567322\n",
      "Epoch 1713, Loss: 2.631349265575409, Final Batch Loss: 0.4742262661457062\n",
      "Epoch 1714, Loss: 2.657572567462921, Final Batch Loss: 0.5736859440803528\n",
      "Epoch 1715, Loss: 2.5099765956401825, Final Batch Loss: 0.38001206517219543\n",
      "Epoch 1716, Loss: 2.5361276268959045, Final Batch Loss: 0.5982630848884583\n",
      "Epoch 1717, Loss: 2.589729130268097, Final Batch Loss: 0.5337189435958862\n",
      "Epoch 1718, Loss: 2.57408607006073, Final Batch Loss: 0.5181384086608887\n",
      "Epoch 1719, Loss: 2.6981844902038574, Final Batch Loss: 0.5331541895866394\n",
      "Epoch 1720, Loss: 2.5204544067382812, Final Batch Loss: 0.4644722044467926\n",
      "Epoch 1721, Loss: 2.7654683887958527, Final Batch Loss: 0.6544421315193176\n",
      "Epoch 1722, Loss: 2.615604519844055, Final Batch Loss: 0.48894479870796204\n",
      "Epoch 1723, Loss: 2.4385689795017242, Final Batch Loss: 0.5380759835243225\n",
      "Epoch 1724, Loss: 2.4281931817531586, Final Batch Loss: 0.552142858505249\n",
      "Epoch 1725, Loss: 2.5700081288814545, Final Batch Loss: 0.469870388507843\n",
      "Epoch 1726, Loss: 2.5967435240745544, Final Batch Loss: 0.5066152811050415\n",
      "Epoch 1727, Loss: 2.5644521713256836, Final Batch Loss: 0.5015449523925781\n",
      "Epoch 1728, Loss: 2.7351267337799072, Final Batch Loss: 0.645827054977417\n",
      "Epoch 1729, Loss: 2.4864996671676636, Final Batch Loss: 0.430181086063385\n",
      "Epoch 1730, Loss: 2.555219829082489, Final Batch Loss: 0.47593021392822266\n",
      "Epoch 1731, Loss: 2.525329738855362, Final Batch Loss: 0.6464618444442749\n",
      "Epoch 1732, Loss: 2.4639184176921844, Final Batch Loss: 0.5020789504051208\n",
      "Epoch 1733, Loss: 2.5977859795093536, Final Batch Loss: 0.528497040271759\n",
      "Epoch 1734, Loss: 2.4954744279384613, Final Batch Loss: 0.5605261325836182\n",
      "Epoch 1735, Loss: 2.569165378808975, Final Batch Loss: 0.5287839770317078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1736, Loss: 2.4951697885990143, Final Batch Loss: 0.4725428521633148\n",
      "Epoch 1737, Loss: 2.6803491413593292, Final Batch Loss: 0.49785763025283813\n",
      "Epoch 1738, Loss: 2.388642579317093, Final Batch Loss: 0.4554261863231659\n",
      "Epoch 1739, Loss: 2.685260981321335, Final Batch Loss: 0.41320177912712097\n",
      "Epoch 1740, Loss: 2.5336983501911163, Final Batch Loss: 0.5440686941146851\n",
      "Epoch 1741, Loss: 2.498659312725067, Final Batch Loss: 0.590563952922821\n",
      "Epoch 1742, Loss: 2.6267266273498535, Final Batch Loss: 0.513333797454834\n",
      "Epoch 1743, Loss: 2.6054881513118744, Final Batch Loss: 0.5000452399253845\n",
      "Epoch 1744, Loss: 2.587650239467621, Final Batch Loss: 0.49984434247016907\n",
      "Epoch 1745, Loss: 2.750956207513809, Final Batch Loss: 0.6728777289390564\n",
      "Epoch 1746, Loss: 2.504902482032776, Final Batch Loss: 0.41579964756965637\n",
      "Epoch 1747, Loss: 2.5841601490974426, Final Batch Loss: 0.48137202858924866\n",
      "Epoch 1748, Loss: 2.6262186765670776, Final Batch Loss: 0.503027617931366\n",
      "Epoch 1749, Loss: 2.6094110012054443, Final Batch Loss: 0.5203521847724915\n",
      "Epoch 1750, Loss: 2.553115427494049, Final Batch Loss: 0.40134328603744507\n",
      "Epoch 1751, Loss: 2.792993575334549, Final Batch Loss: 0.44337427616119385\n",
      "Epoch 1752, Loss: 2.432456225156784, Final Batch Loss: 0.4171983003616333\n",
      "Epoch 1753, Loss: 2.5071458220481873, Final Batch Loss: 0.5060186982154846\n",
      "Epoch 1754, Loss: 2.5114827156066895, Final Batch Loss: 0.5524306297302246\n",
      "Epoch 1755, Loss: 2.5997197926044464, Final Batch Loss: 0.527355432510376\n",
      "Epoch 1756, Loss: 2.732019066810608, Final Batch Loss: 0.5581843852996826\n",
      "Epoch 1757, Loss: 2.5881105959415436, Final Batch Loss: 0.5761198997497559\n",
      "Epoch 1758, Loss: 2.4836702942848206, Final Batch Loss: 0.5771612524986267\n",
      "Epoch 1759, Loss: 2.556312620639801, Final Batch Loss: 0.3880941569805145\n",
      "Epoch 1760, Loss: 2.7110990583896637, Final Batch Loss: 0.5199267864227295\n",
      "Epoch 1761, Loss: 2.5347095727920532, Final Batch Loss: 0.6270846128463745\n",
      "Epoch 1762, Loss: 2.574740916490555, Final Batch Loss: 0.48487797379493713\n",
      "Epoch 1763, Loss: 2.702252596616745, Final Batch Loss: 0.4689854681491852\n",
      "Epoch 1764, Loss: 2.609682083129883, Final Batch Loss: 0.545520544052124\n",
      "Epoch 1765, Loss: 2.5547804832458496, Final Batch Loss: 0.5691555142402649\n",
      "Epoch 1766, Loss: 2.467075824737549, Final Batch Loss: 0.5238814353942871\n",
      "Epoch 1767, Loss: 2.5255461037158966, Final Batch Loss: 0.43265315890312195\n",
      "Epoch 1768, Loss: 2.453941434621811, Final Batch Loss: 0.5413563847541809\n",
      "Epoch 1769, Loss: 2.375818520784378, Final Batch Loss: 0.39312759041786194\n",
      "Epoch 1770, Loss: 2.608089029788971, Final Batch Loss: 0.4771129786968231\n",
      "Epoch 1771, Loss: 2.5399655997753143, Final Batch Loss: 0.48086392879486084\n",
      "Epoch 1772, Loss: 2.5596711933612823, Final Batch Loss: 0.48619747161865234\n",
      "Epoch 1773, Loss: 2.5421086847782135, Final Batch Loss: 0.46195659041404724\n",
      "Epoch 1774, Loss: 2.472584843635559, Final Batch Loss: 0.5110089182853699\n",
      "Epoch 1775, Loss: 2.4719237685203552, Final Batch Loss: 0.551510214805603\n",
      "Epoch 1776, Loss: 2.5455152094364166, Final Batch Loss: 0.531200110912323\n",
      "Epoch 1777, Loss: 2.5971251130104065, Final Batch Loss: 0.5261655449867249\n",
      "Epoch 1778, Loss: 2.6028527319431305, Final Batch Loss: 0.5915570259094238\n",
      "Epoch 1779, Loss: 2.5916421711444855, Final Batch Loss: 0.5414563417434692\n",
      "Epoch 1780, Loss: 2.681017428636551, Final Batch Loss: 0.4930262267589569\n",
      "Epoch 1781, Loss: 2.6617266833782196, Final Batch Loss: 0.447310209274292\n",
      "Epoch 1782, Loss: 2.679438591003418, Final Batch Loss: 0.5679830312728882\n",
      "Epoch 1783, Loss: 2.5012386739254, Final Batch Loss: 0.4303336441516876\n",
      "Epoch 1784, Loss: 2.4859984517097473, Final Batch Loss: 0.45241719484329224\n",
      "Epoch 1785, Loss: 2.6008243560791016, Final Batch Loss: 0.5672907829284668\n",
      "Epoch 1786, Loss: 2.4373396039009094, Final Batch Loss: 0.5172118544578552\n",
      "Epoch 1787, Loss: 2.4568892419338226, Final Batch Loss: 0.4524289667606354\n",
      "Epoch 1788, Loss: 2.48323592543602, Final Batch Loss: 0.4776611030101776\n",
      "Epoch 1789, Loss: 2.5508748590946198, Final Batch Loss: 0.5036686062812805\n",
      "Epoch 1790, Loss: 2.740406632423401, Final Batch Loss: 0.6985892057418823\n",
      "Epoch 1791, Loss: 2.422872632741928, Final Batch Loss: 0.5246288180351257\n",
      "Epoch 1792, Loss: 2.614643782377243, Final Batch Loss: 0.6319168210029602\n",
      "Epoch 1793, Loss: 2.605409264564514, Final Batch Loss: 0.45687147974967957\n",
      "Epoch 1794, Loss: 2.45392706990242, Final Batch Loss: 0.542512834072113\n",
      "Epoch 1795, Loss: 2.5044596791267395, Final Batch Loss: 0.4717347025871277\n",
      "Epoch 1796, Loss: 2.581660032272339, Final Batch Loss: 0.5137923359870911\n",
      "Epoch 1797, Loss: 2.6200228333473206, Final Batch Loss: 0.505100667476654\n",
      "Epoch 1798, Loss: 2.5455230474472046, Final Batch Loss: 0.5693930983543396\n",
      "Epoch 1799, Loss: 2.622076243162155, Final Batch Loss: 0.4619702398777008\n",
      "Epoch 1800, Loss: 2.4017196595668793, Final Batch Loss: 0.42086490988731384\n",
      "Epoch 1801, Loss: 2.489476263523102, Final Batch Loss: 0.4985775947570801\n",
      "Epoch 1802, Loss: 2.5767412185668945, Final Batch Loss: 0.5185423493385315\n",
      "Epoch 1803, Loss: 2.5480579137802124, Final Batch Loss: 0.5432503819465637\n",
      "Epoch 1804, Loss: 2.5082157254219055, Final Batch Loss: 0.6377856135368347\n",
      "Epoch 1805, Loss: 2.570963680744171, Final Batch Loss: 0.5131761431694031\n",
      "Epoch 1806, Loss: 2.5249012410640717, Final Batch Loss: 0.5485952496528625\n",
      "Epoch 1807, Loss: 2.584418296813965, Final Batch Loss: 0.4965319335460663\n",
      "Epoch 1808, Loss: 2.5441746711730957, Final Batch Loss: 0.554583728313446\n",
      "Epoch 1809, Loss: 2.4672508537769318, Final Batch Loss: 0.5254984498023987\n",
      "Epoch 1810, Loss: 2.480219304561615, Final Batch Loss: 0.5189701318740845\n",
      "Epoch 1811, Loss: 2.696180671453476, Final Batch Loss: 0.6568177342414856\n",
      "Epoch 1812, Loss: 2.7979039549827576, Final Batch Loss: 0.5263277292251587\n",
      "Epoch 1813, Loss: 2.5124901235103607, Final Batch Loss: 0.5485460758209229\n",
      "Epoch 1814, Loss: 2.4957153499126434, Final Batch Loss: 0.45746085047721863\n",
      "Epoch 1815, Loss: 2.483988732099533, Final Batch Loss: 0.408966600894928\n",
      "Epoch 1816, Loss: 2.3722814321517944, Final Batch Loss: 0.44522374868392944\n",
      "Epoch 1817, Loss: 2.6423388719558716, Final Batch Loss: 0.48583588004112244\n",
      "Epoch 1818, Loss: 2.3244738578796387, Final Batch Loss: 0.3611714243888855\n",
      "Epoch 1819, Loss: 2.5068431198596954, Final Batch Loss: 0.44581422209739685\n",
      "Epoch 1820, Loss: 2.5729590356349945, Final Batch Loss: 0.4929012954235077\n",
      "Epoch 1821, Loss: 2.4263954162597656, Final Batch Loss: 0.536618709564209\n",
      "Epoch 1822, Loss: 2.4530845284461975, Final Batch Loss: 0.4828042984008789\n",
      "Epoch 1823, Loss: 2.489358425140381, Final Batch Loss: 0.4478204846382141\n",
      "Epoch 1824, Loss: 2.654302090406418, Final Batch Loss: 0.6390436291694641\n",
      "Epoch 1825, Loss: 2.447882264852524, Final Batch Loss: 0.5257518887519836\n",
      "Epoch 1826, Loss: 2.5229914486408234, Final Batch Loss: 0.46469756960868835\n",
      "Epoch 1827, Loss: 2.4697432816028595, Final Batch Loss: 0.4800892174243927\n",
      "Epoch 1828, Loss: 2.489571124315262, Final Batch Loss: 0.5476078987121582\n",
      "Epoch 1829, Loss: 2.517646461725235, Final Batch Loss: 0.5353808403015137\n",
      "Epoch 1830, Loss: 2.38401061296463, Final Batch Loss: 0.5051203966140747\n",
      "Epoch 1831, Loss: 2.548354983329773, Final Batch Loss: 0.4849116802215576\n",
      "Epoch 1832, Loss: 2.62427294254303, Final Batch Loss: 0.586088240146637\n",
      "Epoch 1833, Loss: 2.532129943370819, Final Batch Loss: 0.4436275362968445\n",
      "Epoch 1834, Loss: 2.5374677777290344, Final Batch Loss: 0.5294225215911865\n",
      "Epoch 1835, Loss: 2.582954376935959, Final Batch Loss: 0.5223736763000488\n",
      "Epoch 1836, Loss: 2.431150406599045, Final Batch Loss: 0.505994439125061\n",
      "Epoch 1837, Loss: 2.6292003095149994, Final Batch Loss: 0.5376224517822266\n",
      "Epoch 1838, Loss: 2.5216685831546783, Final Batch Loss: 0.47127479314804077\n",
      "Epoch 1839, Loss: 2.476283609867096, Final Batch Loss: 0.46152520179748535\n",
      "Epoch 1840, Loss: 2.6929426789283752, Final Batch Loss: 0.5375266075134277\n",
      "Epoch 1841, Loss: 2.6605085730552673, Final Batch Loss: 0.499550461769104\n",
      "Epoch 1842, Loss: 2.422284632921219, Final Batch Loss: 0.5760539174079895\n",
      "Epoch 1843, Loss: 2.5627800822257996, Final Batch Loss: 0.5239337682723999\n",
      "Epoch 1844, Loss: 2.5300817489624023, Final Batch Loss: 0.5234470367431641\n",
      "Epoch 1845, Loss: 2.500842183828354, Final Batch Loss: 0.42288267612457275\n",
      "Epoch 1846, Loss: 2.6338445842266083, Final Batch Loss: 0.5748687386512756\n",
      "Epoch 1847, Loss: 2.514728158712387, Final Batch Loss: 0.5312387943267822\n",
      "Epoch 1848, Loss: 2.7182727456092834, Final Batch Loss: 0.5090184807777405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1849, Loss: 2.49068221449852, Final Batch Loss: 0.47629234194755554\n",
      "Epoch 1850, Loss: 2.5677454471588135, Final Batch Loss: 0.4150570333003998\n",
      "Epoch 1851, Loss: 2.583175003528595, Final Batch Loss: 0.5868091583251953\n",
      "Epoch 1852, Loss: 2.578313320875168, Final Batch Loss: 0.5633584260940552\n",
      "Epoch 1853, Loss: 2.5450603365898132, Final Batch Loss: 0.5041848421096802\n",
      "Epoch 1854, Loss: 2.5291435420513153, Final Batch Loss: 0.5120682716369629\n",
      "Epoch 1855, Loss: 2.4384137094020844, Final Batch Loss: 0.4835718870162964\n",
      "Epoch 1856, Loss: 2.538274884223938, Final Batch Loss: 0.5029624104499817\n",
      "Epoch 1857, Loss: 2.722166895866394, Final Batch Loss: 0.6285700798034668\n",
      "Epoch 1858, Loss: 2.500484436750412, Final Batch Loss: 0.54244464635849\n",
      "Epoch 1859, Loss: 2.499933958053589, Final Batch Loss: 0.4896619915962219\n",
      "Epoch 1860, Loss: 2.522306501865387, Final Batch Loss: 0.5297068357467651\n",
      "Epoch 1861, Loss: 2.7155210077762604, Final Batch Loss: 0.5644540190696716\n",
      "Epoch 1862, Loss: 2.418452739715576, Final Batch Loss: 0.5092992186546326\n",
      "Epoch 1863, Loss: 2.406208783388138, Final Batch Loss: 0.5196223258972168\n",
      "Epoch 1864, Loss: 2.4236097037792206, Final Batch Loss: 0.4749898910522461\n",
      "Epoch 1865, Loss: 2.4891485571861267, Final Batch Loss: 0.5277376770973206\n",
      "Epoch 1866, Loss: 2.4902096688747406, Final Batch Loss: 0.4556310772895813\n",
      "Epoch 1867, Loss: 2.3105949759483337, Final Batch Loss: 0.4993295669555664\n",
      "Epoch 1868, Loss: 2.7253620624542236, Final Batch Loss: 0.5925673246383667\n",
      "Epoch 1869, Loss: 2.5061987936496735, Final Batch Loss: 0.4995695948600769\n",
      "Epoch 1870, Loss: 2.632156163454056, Final Batch Loss: 0.5258532166481018\n",
      "Epoch 1871, Loss: 2.729945957660675, Final Batch Loss: 0.5009593963623047\n",
      "Epoch 1872, Loss: 2.5121995508670807, Final Batch Loss: 0.5070075392723083\n",
      "Epoch 1873, Loss: 2.545330226421356, Final Batch Loss: 0.5620247721672058\n",
      "Epoch 1874, Loss: 2.4532108306884766, Final Batch Loss: 0.4793670177459717\n",
      "Epoch 1875, Loss: 2.4591888785362244, Final Batch Loss: 0.47395187616348267\n",
      "Epoch 1876, Loss: 2.4046832025051117, Final Batch Loss: 0.5126193761825562\n",
      "Epoch 1877, Loss: 2.5649171471595764, Final Batch Loss: 0.47003474831581116\n",
      "Epoch 1878, Loss: 2.4332415163517, Final Batch Loss: 0.5219643115997314\n",
      "Epoch 1879, Loss: 2.490081787109375, Final Batch Loss: 0.5038613677024841\n",
      "Epoch 1880, Loss: 2.531260669231415, Final Batch Loss: 0.5005472302436829\n",
      "Epoch 1881, Loss: 2.522189050912857, Final Batch Loss: 0.4875917136669159\n",
      "Epoch 1882, Loss: 2.460576832294464, Final Batch Loss: 0.46993160247802734\n",
      "Epoch 1883, Loss: 2.3581252992153168, Final Batch Loss: 0.5125659704208374\n",
      "Epoch 1884, Loss: 2.489137589931488, Final Batch Loss: 0.5534229278564453\n",
      "Epoch 1885, Loss: 2.552268922328949, Final Batch Loss: 0.5776354074478149\n",
      "Epoch 1886, Loss: 2.558559387922287, Final Batch Loss: 0.5479291081428528\n",
      "Epoch 1887, Loss: 2.467597097158432, Final Batch Loss: 0.5230033993721008\n",
      "Epoch 1888, Loss: 2.6273546516895294, Final Batch Loss: 0.6157551407814026\n",
      "Epoch 1889, Loss: 2.4932456016540527, Final Batch Loss: 0.3409978747367859\n",
      "Epoch 1890, Loss: 2.532836437225342, Final Batch Loss: 0.522134006023407\n",
      "Epoch 1891, Loss: 2.499983012676239, Final Batch Loss: 0.4664694368839264\n",
      "Epoch 1892, Loss: 2.5737138390541077, Final Batch Loss: 0.4369463324546814\n",
      "Epoch 1893, Loss: 2.467401772737503, Final Batch Loss: 0.5309610962867737\n",
      "Epoch 1894, Loss: 2.740108996629715, Final Batch Loss: 0.5976406335830688\n",
      "Epoch 1895, Loss: 2.3977674543857574, Final Batch Loss: 0.5028501152992249\n",
      "Epoch 1896, Loss: 2.5721985399723053, Final Batch Loss: 0.589043915271759\n",
      "Epoch 1897, Loss: 2.4701943397521973, Final Batch Loss: 0.42801764607429504\n",
      "Epoch 1898, Loss: 2.6749247312545776, Final Batch Loss: 0.5107359886169434\n",
      "Epoch 1899, Loss: 2.4872619807720184, Final Batch Loss: 0.48514583706855774\n",
      "Epoch 1900, Loss: 2.4229811429977417, Final Batch Loss: 0.5449314713478088\n",
      "Epoch 1901, Loss: 2.5497463643550873, Final Batch Loss: 0.4313671886920929\n",
      "Epoch 1902, Loss: 2.4879219830036163, Final Batch Loss: 0.5392153859138489\n",
      "Epoch 1903, Loss: 2.4566886126995087, Final Batch Loss: 0.4758111834526062\n",
      "Epoch 1904, Loss: 2.4909971356391907, Final Batch Loss: 0.4553581178188324\n",
      "Epoch 1905, Loss: 2.607387602329254, Final Batch Loss: 0.5129102468490601\n",
      "Epoch 1906, Loss: 2.6084809005260468, Final Batch Loss: 0.5726643204689026\n",
      "Epoch 1907, Loss: 2.449591249227524, Final Batch Loss: 0.4612596929073334\n",
      "Epoch 1908, Loss: 2.596571832895279, Final Batch Loss: 0.5929486751556396\n",
      "Epoch 1909, Loss: 2.4520933032035828, Final Batch Loss: 0.5019989609718323\n",
      "Epoch 1910, Loss: 2.49364697933197, Final Batch Loss: 0.5051031708717346\n",
      "Epoch 1911, Loss: 2.474286377429962, Final Batch Loss: 0.45517826080322266\n",
      "Epoch 1912, Loss: 2.4695532023906708, Final Batch Loss: 0.46236729621887207\n",
      "Epoch 1913, Loss: 2.410122662782669, Final Batch Loss: 0.4962990880012512\n",
      "Epoch 1914, Loss: 2.603974997997284, Final Batch Loss: 0.49910345673561096\n",
      "Epoch 1915, Loss: 2.388191521167755, Final Batch Loss: 0.4277494549751282\n",
      "Epoch 1916, Loss: 2.555598169565201, Final Batch Loss: 0.4511635899543762\n",
      "Epoch 1917, Loss: 2.4158087074756622, Final Batch Loss: 0.5103973746299744\n",
      "Epoch 1918, Loss: 2.403921812772751, Final Batch Loss: 0.4271366000175476\n",
      "Epoch 1919, Loss: 2.459323614835739, Final Batch Loss: 0.5242859721183777\n",
      "Epoch 1920, Loss: 2.4150294363498688, Final Batch Loss: 0.48394331336021423\n",
      "Epoch 1921, Loss: 2.5070067942142487, Final Batch Loss: 0.4791882038116455\n",
      "Epoch 1922, Loss: 2.455413907766342, Final Batch Loss: 0.5226361751556396\n",
      "Epoch 1923, Loss: 2.45746648311615, Final Batch Loss: 0.45919156074523926\n",
      "Epoch 1924, Loss: 2.467452734708786, Final Batch Loss: 0.5601375699043274\n",
      "Epoch 1925, Loss: 2.554215669631958, Final Batch Loss: 0.42840510606765747\n",
      "Epoch 1926, Loss: 2.5056872367858887, Final Batch Loss: 0.449221670627594\n",
      "Epoch 1927, Loss: 2.38760307431221, Final Batch Loss: 0.4584753215312958\n",
      "Epoch 1928, Loss: 2.5850506722927094, Final Batch Loss: 0.5951533317565918\n",
      "Epoch 1929, Loss: 2.4451435804367065, Final Batch Loss: 0.556993305683136\n",
      "Epoch 1930, Loss: 2.451990842819214, Final Batch Loss: 0.5495012402534485\n",
      "Epoch 1931, Loss: 2.501339942216873, Final Batch Loss: 0.4152252674102783\n",
      "Epoch 1932, Loss: 2.452959030866623, Final Batch Loss: 0.4666500389575958\n",
      "Epoch 1933, Loss: 2.6097620725631714, Final Batch Loss: 0.5049002170562744\n",
      "Epoch 1934, Loss: 2.591836541891098, Final Batch Loss: 0.6283960938453674\n",
      "Epoch 1935, Loss: 2.528587728738785, Final Batch Loss: 0.49066361784935\n",
      "Epoch 1936, Loss: 2.4873073399066925, Final Batch Loss: 0.5719774961471558\n",
      "Epoch 1937, Loss: 2.453019231557846, Final Batch Loss: 0.5156676769256592\n",
      "Epoch 1938, Loss: 2.487383931875229, Final Batch Loss: 0.430486261844635\n",
      "Epoch 1939, Loss: 2.4373940527439117, Final Batch Loss: 0.4276583194732666\n",
      "Epoch 1940, Loss: 2.406464159488678, Final Batch Loss: 0.4208938777446747\n",
      "Epoch 1941, Loss: 2.5009686946868896, Final Batch Loss: 0.5113614201545715\n",
      "Epoch 1942, Loss: 2.386911302804947, Final Batch Loss: 0.46542540192604065\n",
      "Epoch 1943, Loss: 2.2350530922412872, Final Batch Loss: 0.501815915107727\n",
      "Epoch 1944, Loss: 2.4236055612564087, Final Batch Loss: 0.4388551414012909\n",
      "Epoch 1945, Loss: 2.3990548253059387, Final Batch Loss: 0.4580566883087158\n",
      "Epoch 1946, Loss: 2.476824074983597, Final Batch Loss: 0.4921308159828186\n",
      "Epoch 1947, Loss: 2.500203788280487, Final Batch Loss: 0.5285226106643677\n",
      "Epoch 1948, Loss: 2.4421149492263794, Final Batch Loss: 0.45652827620506287\n",
      "Epoch 1949, Loss: 2.3952924013137817, Final Batch Loss: 0.5248477458953857\n",
      "Epoch 1950, Loss: 2.3670170605182648, Final Batch Loss: 0.4005128741264343\n",
      "Epoch 1951, Loss: 2.5064048767089844, Final Batch Loss: 0.47085586190223694\n",
      "Epoch 1952, Loss: 2.354484587907791, Final Batch Loss: 0.46667617559432983\n",
      "Epoch 1953, Loss: 2.4955456852912903, Final Batch Loss: 0.5644941329956055\n",
      "Epoch 1954, Loss: 2.4457561671733856, Final Batch Loss: 0.4372085630893707\n",
      "Epoch 1955, Loss: 2.555711030960083, Final Batch Loss: 0.5344625115394592\n",
      "Epoch 1956, Loss: 2.3549461662769318, Final Batch Loss: 0.42621105909347534\n",
      "Epoch 1957, Loss: 2.5896301567554474, Final Batch Loss: 0.5385554432868958\n",
      "Epoch 1958, Loss: 2.450761914253235, Final Batch Loss: 0.5026629567146301\n",
      "Epoch 1959, Loss: 2.3025293350219727, Final Batch Loss: 0.46626871824264526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1960, Loss: 2.558573931455612, Final Batch Loss: 0.5344655513763428\n",
      "Epoch 1961, Loss: 2.499711811542511, Final Batch Loss: 0.5424498915672302\n",
      "Epoch 1962, Loss: 2.4799621403217316, Final Batch Loss: 0.4839904308319092\n",
      "Epoch 1963, Loss: 2.3864265978336334, Final Batch Loss: 0.4903844892978668\n",
      "Epoch 1964, Loss: 2.5305900275707245, Final Batch Loss: 0.4572090804576874\n",
      "Epoch 1965, Loss: 2.3060201704502106, Final Batch Loss: 0.4669555127620697\n",
      "Epoch 1966, Loss: 2.4219065606594086, Final Batch Loss: 0.49243396520614624\n",
      "Epoch 1967, Loss: 2.4030078053474426, Final Batch Loss: 0.5429941415786743\n",
      "Epoch 1968, Loss: 2.524262845516205, Final Batch Loss: 0.6101174354553223\n",
      "Epoch 1969, Loss: 2.669690102338791, Final Batch Loss: 0.6122211217880249\n",
      "Epoch 1970, Loss: 2.3043902814388275, Final Batch Loss: 0.425593763589859\n",
      "Epoch 1971, Loss: 2.537868320941925, Final Batch Loss: 0.5510953664779663\n",
      "Epoch 1972, Loss: 2.636168956756592, Final Batch Loss: 0.579319417476654\n",
      "Epoch 1973, Loss: 2.3930209279060364, Final Batch Loss: 0.3996129631996155\n",
      "Epoch 1974, Loss: 2.503890097141266, Final Batch Loss: 0.5435988903045654\n",
      "Epoch 1975, Loss: 2.4743843972682953, Final Batch Loss: 0.47366511821746826\n",
      "Epoch 1976, Loss: 2.4720947444438934, Final Batch Loss: 0.5226243734359741\n",
      "Epoch 1977, Loss: 2.5027095675468445, Final Batch Loss: 0.49396175146102905\n",
      "Epoch 1978, Loss: 2.40975022315979, Final Batch Loss: 0.4934278428554535\n",
      "Epoch 1979, Loss: 2.4621915221214294, Final Batch Loss: 0.4733656048774719\n",
      "Epoch 1980, Loss: 2.3644880652427673, Final Batch Loss: 0.46822038292884827\n",
      "Epoch 1981, Loss: 2.469262957572937, Final Batch Loss: 0.504133939743042\n",
      "Epoch 1982, Loss: 2.3957996666431427, Final Batch Loss: 0.4180854558944702\n",
      "Epoch 1983, Loss: 2.2883386611938477, Final Batch Loss: 0.4719502627849579\n",
      "Epoch 1984, Loss: 2.3463470935821533, Final Batch Loss: 0.4182140529155731\n",
      "Epoch 1985, Loss: 2.522819548845291, Final Batch Loss: 0.6184883713722229\n",
      "Epoch 1986, Loss: 2.4664971232414246, Final Batch Loss: 0.49161016941070557\n",
      "Epoch 1987, Loss: 2.5536501705646515, Final Batch Loss: 0.4821801483631134\n",
      "Epoch 1988, Loss: 2.4417348504066467, Final Batch Loss: 0.47654271125793457\n",
      "Epoch 1989, Loss: 2.5782899260520935, Final Batch Loss: 0.5116627216339111\n",
      "Epoch 1990, Loss: 2.4261054694652557, Final Batch Loss: 0.4621582627296448\n",
      "Epoch 1991, Loss: 2.436179667711258, Final Batch Loss: 0.5383927226066589\n",
      "Epoch 1992, Loss: 2.3463285863399506, Final Batch Loss: 0.42713555693626404\n",
      "Epoch 1993, Loss: 2.336114823818207, Final Batch Loss: 0.5213897228240967\n",
      "Epoch 1994, Loss: 2.442376673221588, Final Batch Loss: 0.458091676235199\n",
      "Epoch 1995, Loss: 2.1904239654541016, Final Batch Loss: 0.4994499087333679\n",
      "Epoch 1996, Loss: 2.458802878856659, Final Batch Loss: 0.4848247766494751\n",
      "Epoch 1997, Loss: 2.3220367431640625, Final Batch Loss: 0.4492805600166321\n",
      "Epoch 1998, Loss: 2.378358483314514, Final Batch Loss: 0.5013309717178345\n",
      "Epoch 1999, Loss: 2.514382004737854, Final Batch Loss: 0.44916149973869324\n",
      "Epoch 2000, Loss: 2.468859851360321, Final Batch Loss: 0.40065741539001465\n",
      "Epoch 2001, Loss: 2.53487691283226, Final Batch Loss: 0.49053797125816345\n",
      "Epoch 2002, Loss: 2.4779564440250397, Final Batch Loss: 0.6011242270469666\n",
      "Epoch 2003, Loss: 2.3870889246463776, Final Batch Loss: 0.4636799991130829\n",
      "Epoch 2004, Loss: 2.519971638917923, Final Batch Loss: 0.4303883910179138\n",
      "Epoch 2005, Loss: 2.443433791399002, Final Batch Loss: 0.4649067521095276\n",
      "Epoch 2006, Loss: 2.368056535720825, Final Batch Loss: 0.39571088552474976\n",
      "Epoch 2007, Loss: 2.3823035657405853, Final Batch Loss: 0.4576696455478668\n",
      "Epoch 2008, Loss: 2.588915914297104, Final Batch Loss: 0.5660866498947144\n",
      "Epoch 2009, Loss: 2.4862599670886993, Final Batch Loss: 0.5554097890853882\n",
      "Epoch 2010, Loss: 2.4959222972393036, Final Batch Loss: 0.5120783448219299\n",
      "Epoch 2011, Loss: 2.4633820950984955, Final Batch Loss: 0.4416821599006653\n",
      "Epoch 2012, Loss: 2.5341452956199646, Final Batch Loss: 0.5211470127105713\n",
      "Epoch 2013, Loss: 2.4796900749206543, Final Batch Loss: 0.5248836278915405\n",
      "Epoch 2014, Loss: 2.5667066276073456, Final Batch Loss: 0.47292008996009827\n",
      "Epoch 2015, Loss: 2.6447675228118896, Final Batch Loss: 0.5098609924316406\n",
      "Epoch 2016, Loss: 2.55274561047554, Final Batch Loss: 0.5191526412963867\n",
      "Epoch 2017, Loss: 2.5731522142887115, Final Batch Loss: 0.5444933772087097\n",
      "Epoch 2018, Loss: 2.446445018053055, Final Batch Loss: 0.43389925360679626\n",
      "Epoch 2019, Loss: 2.6197716891765594, Final Batch Loss: 0.5449696779251099\n",
      "Epoch 2020, Loss: 2.338032364845276, Final Batch Loss: 0.49333152174949646\n",
      "Epoch 2021, Loss: 2.371182143688202, Final Batch Loss: 0.549281120300293\n",
      "Epoch 2022, Loss: 2.3722163438796997, Final Batch Loss: 0.5253998637199402\n",
      "Epoch 2023, Loss: 2.441648483276367, Final Batch Loss: 0.4703165590763092\n",
      "Epoch 2024, Loss: 2.5174367427825928, Final Batch Loss: 0.5530871748924255\n",
      "Epoch 2025, Loss: 2.3318293690681458, Final Batch Loss: 0.40596455335617065\n",
      "Epoch 2026, Loss: 2.482813984155655, Final Batch Loss: 0.5064208507537842\n",
      "Epoch 2027, Loss: 2.4505471289157867, Final Batch Loss: 0.5100096464157104\n",
      "Epoch 2028, Loss: 2.487539231777191, Final Batch Loss: 0.47068750858306885\n",
      "Epoch 2029, Loss: 2.429395467042923, Final Batch Loss: 0.4218875467777252\n",
      "Epoch 2030, Loss: 2.4218060672283173, Final Batch Loss: 0.464632511138916\n",
      "Epoch 2031, Loss: 2.416182428598404, Final Batch Loss: 0.4407857060432434\n",
      "Epoch 2032, Loss: 2.4103819727897644, Final Batch Loss: 0.49890437722206116\n",
      "Epoch 2033, Loss: 2.487991839647293, Final Batch Loss: 0.4976436495780945\n",
      "Epoch 2034, Loss: 2.2556914687156677, Final Batch Loss: 0.43073922395706177\n",
      "Epoch 2035, Loss: 2.4126272797584534, Final Batch Loss: 0.5054797530174255\n",
      "Epoch 2036, Loss: 2.385202556848526, Final Batch Loss: 0.5012957453727722\n",
      "Epoch 2037, Loss: 2.4288500249385834, Final Batch Loss: 0.5429079532623291\n",
      "Epoch 2038, Loss: 2.442050039768219, Final Batch Loss: 0.44891613721847534\n",
      "Epoch 2039, Loss: 2.410575747489929, Final Batch Loss: 0.5559752583503723\n",
      "Epoch 2040, Loss: 2.3434433937072754, Final Batch Loss: 0.47029951214790344\n",
      "Epoch 2041, Loss: 2.4229289293289185, Final Batch Loss: 0.4370516240596771\n",
      "Epoch 2042, Loss: 2.4371519684791565, Final Batch Loss: 0.39276981353759766\n",
      "Epoch 2043, Loss: 2.519411563873291, Final Batch Loss: 0.5638007521629333\n",
      "Epoch 2044, Loss: 2.442699372768402, Final Batch Loss: 0.5181804895401001\n",
      "Epoch 2045, Loss: 2.3224292993545532, Final Batch Loss: 0.541577935218811\n",
      "Epoch 2046, Loss: 2.7249591052532196, Final Batch Loss: 0.5785237550735474\n",
      "Epoch 2047, Loss: 2.2324807941913605, Final Batch Loss: 0.5154526233673096\n",
      "Epoch 2048, Loss: 2.402368038892746, Final Batch Loss: 0.48786690831184387\n",
      "Epoch 2049, Loss: 2.384126216173172, Final Batch Loss: 0.3656310737133026\n",
      "Epoch 2050, Loss: 2.307095617055893, Final Batch Loss: 0.5333069562911987\n",
      "Epoch 2051, Loss: 2.4599642753601074, Final Batch Loss: 0.48927605152130127\n",
      "Epoch 2052, Loss: 2.2365625500679016, Final Batch Loss: 0.47920849919319153\n",
      "Epoch 2053, Loss: 2.1995767951011658, Final Batch Loss: 0.49981239438056946\n",
      "Epoch 2054, Loss: 2.485308051109314, Final Batch Loss: 0.5257611870765686\n",
      "Epoch 2055, Loss: 2.4283350110054016, Final Batch Loss: 0.4931720197200775\n",
      "Epoch 2056, Loss: 2.4900339543819427, Final Batch Loss: 0.5141350626945496\n",
      "Epoch 2057, Loss: 2.301625907421112, Final Batch Loss: 0.44674599170684814\n",
      "Epoch 2058, Loss: 2.2738537192344666, Final Batch Loss: 0.44276198744773865\n",
      "Epoch 2059, Loss: 2.39915668964386, Final Batch Loss: 0.48187652230262756\n",
      "Epoch 2060, Loss: 2.773820608854294, Final Batch Loss: 0.758926272392273\n",
      "Epoch 2061, Loss: 2.46572345495224, Final Batch Loss: 0.4012974202632904\n",
      "Epoch 2062, Loss: 2.5195948481559753, Final Batch Loss: 0.48128965497016907\n",
      "Epoch 2063, Loss: 2.369945764541626, Final Batch Loss: 0.5114577412605286\n",
      "Epoch 2064, Loss: 2.34384486079216, Final Batch Loss: 0.4656801223754883\n",
      "Epoch 2065, Loss: 2.3451667428016663, Final Batch Loss: 0.5309234261512756\n",
      "Epoch 2066, Loss: 2.3978952765464783, Final Batch Loss: 0.4354948401451111\n",
      "Epoch 2067, Loss: 2.417847067117691, Final Batch Loss: 0.5542861223220825\n",
      "Epoch 2068, Loss: 2.4842142164707184, Final Batch Loss: 0.4076882302761078\n",
      "Epoch 2069, Loss: 2.355755925178528, Final Batch Loss: 0.500045120716095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2070, Loss: 2.3986337184906006, Final Batch Loss: 0.407706618309021\n",
      "Epoch 2071, Loss: 2.4992980659008026, Final Batch Loss: 0.5115360021591187\n",
      "Epoch 2072, Loss: 2.541442036628723, Final Batch Loss: 0.4350449740886688\n",
      "Epoch 2073, Loss: 2.321144849061966, Final Batch Loss: 0.5172981023788452\n",
      "Epoch 2074, Loss: 2.1912584602832794, Final Batch Loss: 0.3894643783569336\n",
      "Epoch 2075, Loss: 2.3726374208927155, Final Batch Loss: 0.4550904333591461\n",
      "Epoch 2076, Loss: 2.2772354781627655, Final Batch Loss: 0.49138176441192627\n",
      "Epoch 2077, Loss: 2.4147210121154785, Final Batch Loss: 0.47549811005592346\n",
      "Epoch 2078, Loss: 2.426054745912552, Final Batch Loss: 0.48699405789375305\n",
      "Epoch 2079, Loss: 2.4297580122947693, Final Batch Loss: 0.4247000217437744\n",
      "Epoch 2080, Loss: 2.518689811229706, Final Batch Loss: 0.5085585117340088\n",
      "Epoch 2081, Loss: 2.3402681052684784, Final Batch Loss: 0.4381885528564453\n",
      "Epoch 2082, Loss: 2.31607785820961, Final Batch Loss: 0.47070538997650146\n",
      "Epoch 2083, Loss: 2.5192973911762238, Final Batch Loss: 0.4804348349571228\n",
      "Epoch 2084, Loss: 2.2147608399391174, Final Batch Loss: 0.4315643906593323\n",
      "Epoch 2085, Loss: 2.3372697234153748, Final Batch Loss: 0.47841599583625793\n",
      "Epoch 2086, Loss: 2.3778021037578583, Final Batch Loss: 0.4566159248352051\n",
      "Epoch 2087, Loss: 2.3902429938316345, Final Batch Loss: 0.4984970688819885\n",
      "Epoch 2088, Loss: 2.2719848453998566, Final Batch Loss: 0.3705810010433197\n",
      "Epoch 2089, Loss: 2.4603134989738464, Final Batch Loss: 0.48197343945503235\n",
      "Epoch 2090, Loss: 2.330495446920395, Final Batch Loss: 0.48566898703575134\n",
      "Epoch 2091, Loss: 2.4233087301254272, Final Batch Loss: 0.4757402837276459\n",
      "Epoch 2092, Loss: 2.435481905937195, Final Batch Loss: 0.49297070503234863\n",
      "Epoch 2093, Loss: 2.3838634192943573, Final Batch Loss: 0.6297503113746643\n",
      "Epoch 2094, Loss: 2.275845915079117, Final Batch Loss: 0.4355793297290802\n",
      "Epoch 2095, Loss: 2.492670953273773, Final Batch Loss: 0.4716292917728424\n",
      "Epoch 2096, Loss: 2.3685784339904785, Final Batch Loss: 0.5484780073165894\n",
      "Epoch 2097, Loss: 2.4145263731479645, Final Batch Loss: 0.49380096793174744\n",
      "Epoch 2098, Loss: 2.371167093515396, Final Batch Loss: 0.5286678075790405\n",
      "Epoch 2099, Loss: 2.416475296020508, Final Batch Loss: 0.5388340950012207\n",
      "Epoch 2100, Loss: 2.2688519656658173, Final Batch Loss: 0.5149325728416443\n",
      "Epoch 2101, Loss: 2.313412308692932, Final Batch Loss: 0.39425793290138245\n",
      "Epoch 2102, Loss: 2.497619390487671, Final Batch Loss: 0.5351319313049316\n",
      "Epoch 2103, Loss: 2.456225633621216, Final Batch Loss: 0.3930596113204956\n",
      "Epoch 2104, Loss: 2.329657405614853, Final Batch Loss: 0.3700917661190033\n",
      "Epoch 2105, Loss: 2.292955219745636, Final Batch Loss: 0.5242666602134705\n",
      "Epoch 2106, Loss: 2.394599735736847, Final Batch Loss: 0.5038294196128845\n",
      "Epoch 2107, Loss: 2.321952223777771, Final Batch Loss: 0.5026392936706543\n",
      "Epoch 2108, Loss: 2.3778491616249084, Final Batch Loss: 0.45296844840049744\n",
      "Epoch 2109, Loss: 2.634402424097061, Final Batch Loss: 0.42090481519699097\n",
      "Epoch 2110, Loss: 2.4561299681663513, Final Batch Loss: 0.5495123267173767\n",
      "Epoch 2111, Loss: 2.4306129217147827, Final Batch Loss: 0.4645969271659851\n",
      "Epoch 2112, Loss: 2.327749192714691, Final Batch Loss: 0.4214589297771454\n",
      "Epoch 2113, Loss: 2.492392420768738, Final Batch Loss: 0.45866909623146057\n",
      "Epoch 2114, Loss: 2.3191055953502655, Final Batch Loss: 0.4155154526233673\n",
      "Epoch 2115, Loss: 2.569770187139511, Final Batch Loss: 0.5787002444267273\n",
      "Epoch 2116, Loss: 2.4492639005184174, Final Batch Loss: 0.46591347455978394\n",
      "Epoch 2117, Loss: 2.2925606966018677, Final Batch Loss: 0.4809272289276123\n",
      "Epoch 2118, Loss: 2.408115327358246, Final Batch Loss: 0.44357457756996155\n",
      "Epoch 2119, Loss: 2.320947676897049, Final Batch Loss: 0.4373481571674347\n",
      "Epoch 2120, Loss: 2.692379653453827, Final Batch Loss: 0.5007380247116089\n",
      "Epoch 2121, Loss: 2.4116590321063995, Final Batch Loss: 0.3708815276622772\n",
      "Epoch 2122, Loss: 2.413643926382065, Final Batch Loss: 0.44897037744522095\n",
      "Epoch 2123, Loss: 2.2902754843235016, Final Batch Loss: 0.4284602999687195\n",
      "Epoch 2124, Loss: 2.4339349269866943, Final Batch Loss: 0.5034428238868713\n",
      "Epoch 2125, Loss: 2.3891751766204834, Final Batch Loss: 0.5180827379226685\n",
      "Epoch 2126, Loss: 2.460335820913315, Final Batch Loss: 0.4592527151107788\n",
      "Epoch 2127, Loss: 2.3228471279144287, Final Batch Loss: 0.3736873269081116\n",
      "Epoch 2128, Loss: 2.4458593130111694, Final Batch Loss: 0.4694642722606659\n",
      "Epoch 2129, Loss: 2.4231114089488983, Final Batch Loss: 0.40344446897506714\n",
      "Epoch 2130, Loss: 2.3390379548072815, Final Batch Loss: 0.47563308477401733\n",
      "Epoch 2131, Loss: 2.1881884932518005, Final Batch Loss: 0.42545804381370544\n",
      "Epoch 2132, Loss: 2.3756171464920044, Final Batch Loss: 0.44802096486091614\n",
      "Epoch 2133, Loss: 2.514896869659424, Final Batch Loss: 0.5002740025520325\n",
      "Epoch 2134, Loss: 2.4013034999370575, Final Batch Loss: 0.4522325098514557\n",
      "Epoch 2135, Loss: 2.2868307530879974, Final Batch Loss: 0.4774184226989746\n",
      "Epoch 2136, Loss: 2.35586741566658, Final Batch Loss: 0.44830021262168884\n",
      "Epoch 2137, Loss: 2.378920465707779, Final Batch Loss: 0.4989035427570343\n",
      "Epoch 2138, Loss: 2.1622838377952576, Final Batch Loss: 0.4013844132423401\n",
      "Epoch 2139, Loss: 2.4714095890522003, Final Batch Loss: 0.4970909357070923\n",
      "Epoch 2140, Loss: 2.4623205363750458, Final Batch Loss: 0.46204671263694763\n",
      "Epoch 2141, Loss: 2.426204890012741, Final Batch Loss: 0.5219874382019043\n",
      "Epoch 2142, Loss: 2.555701106786728, Final Batch Loss: 0.44584915041923523\n",
      "Epoch 2143, Loss: 2.4146477580070496, Final Batch Loss: 0.4858512580394745\n",
      "Epoch 2144, Loss: 2.389257490634918, Final Batch Loss: 0.47686854004859924\n",
      "Epoch 2145, Loss: 2.3563838601112366, Final Batch Loss: 0.5553967952728271\n",
      "Epoch 2146, Loss: 2.5131109058856964, Final Batch Loss: 0.5240483283996582\n",
      "Epoch 2147, Loss: 2.3631367087364197, Final Batch Loss: 0.37326449155807495\n",
      "Epoch 2148, Loss: 2.3003616631031036, Final Batch Loss: 0.39912575483322144\n",
      "Epoch 2149, Loss: 2.4986273050308228, Final Batch Loss: 0.43968597054481506\n",
      "Epoch 2150, Loss: 2.4217436015605927, Final Batch Loss: 0.4827723801136017\n",
      "Epoch 2151, Loss: 2.32686910033226, Final Batch Loss: 0.40329501032829285\n",
      "Epoch 2152, Loss: 2.388015925884247, Final Batch Loss: 0.4404546916484833\n",
      "Epoch 2153, Loss: 2.4965569376945496, Final Batch Loss: 0.5544324517250061\n",
      "Epoch 2154, Loss: 2.4114583134651184, Final Batch Loss: 0.46495798230171204\n",
      "Epoch 2155, Loss: 2.49996879696846, Final Batch Loss: 0.47908976674079895\n",
      "Epoch 2156, Loss: 2.5876425206661224, Final Batch Loss: 0.47931286692619324\n",
      "Epoch 2157, Loss: 2.392975330352783, Final Batch Loss: 0.4394912123680115\n",
      "Epoch 2158, Loss: 2.2180128693580627, Final Batch Loss: 0.4296751916408539\n",
      "Epoch 2159, Loss: 2.3056471943855286, Final Batch Loss: 0.3974606692790985\n",
      "Epoch 2160, Loss: 2.2341386675834656, Final Batch Loss: 0.4524384140968323\n",
      "Epoch 2161, Loss: 2.4507953822612762, Final Batch Loss: 0.5303627252578735\n",
      "Epoch 2162, Loss: 2.446667045354843, Final Batch Loss: 0.5797146558761597\n",
      "Epoch 2163, Loss: 2.4406930208206177, Final Batch Loss: 0.48370274901390076\n",
      "Epoch 2164, Loss: 2.4513540267944336, Final Batch Loss: 0.4451867938041687\n",
      "Epoch 2165, Loss: 2.41896390914917, Final Batch Loss: 0.3979531526565552\n",
      "Epoch 2166, Loss: 2.3814362287521362, Final Batch Loss: 0.47635921835899353\n",
      "Epoch 2167, Loss: 2.2994005382061005, Final Batch Loss: 0.5193280577659607\n",
      "Epoch 2168, Loss: 2.300445020198822, Final Batch Loss: 0.5424196720123291\n",
      "Epoch 2169, Loss: 2.566390186548233, Final Batch Loss: 0.5195433497428894\n",
      "Epoch 2170, Loss: 2.4050664603710175, Final Batch Loss: 0.4441050887107849\n",
      "Epoch 2171, Loss: 2.3938609659671783, Final Batch Loss: 0.504298746585846\n",
      "Epoch 2172, Loss: 2.466100811958313, Final Batch Loss: 0.5090082287788391\n",
      "Epoch 2173, Loss: 2.2751458883285522, Final Batch Loss: 0.4984218180179596\n",
      "Epoch 2174, Loss: 2.4086438417434692, Final Batch Loss: 0.47358664870262146\n",
      "Epoch 2175, Loss: 2.411798447370529, Final Batch Loss: 0.5028184056282043\n",
      "Epoch 2176, Loss: 2.2846169769763947, Final Batch Loss: 0.40798038244247437\n",
      "Epoch 2177, Loss: 2.4627426266670227, Final Batch Loss: 0.42521920800209045\n",
      "Epoch 2178, Loss: 2.2580772042274475, Final Batch Loss: 0.491401344537735\n",
      "Epoch 2179, Loss: 2.398113042116165, Final Batch Loss: 0.5589545369148254\n",
      "Epoch 2180, Loss: 2.415922373533249, Final Batch Loss: 0.4754946529865265\n",
      "Epoch 2181, Loss: 2.2933787405490875, Final Batch Loss: 0.3650311529636383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2182, Loss: 2.3851765990257263, Final Batch Loss: 0.48374292254447937\n",
      "Epoch 2183, Loss: 2.517037659883499, Final Batch Loss: 0.4960029721260071\n",
      "Epoch 2184, Loss: 2.301916927099228, Final Batch Loss: 0.420341432094574\n",
      "Epoch 2185, Loss: 2.5365993678569794, Final Batch Loss: 0.5810160040855408\n",
      "Epoch 2186, Loss: 2.5546217262744904, Final Batch Loss: 0.44748982787132263\n",
      "Epoch 2187, Loss: 2.377540022134781, Final Batch Loss: 0.3953401446342468\n",
      "Epoch 2188, Loss: 2.549416661262512, Final Batch Loss: 0.5584629774093628\n",
      "Epoch 2189, Loss: 2.426939517259598, Final Batch Loss: 0.570489227771759\n",
      "Epoch 2190, Loss: 2.5542628467082977, Final Batch Loss: 0.474379301071167\n",
      "Epoch 2191, Loss: 2.3634683787822723, Final Batch Loss: 0.4841279089450836\n",
      "Epoch 2192, Loss: 2.4472320079803467, Final Batch Loss: 0.4917162358760834\n",
      "Epoch 2193, Loss: 2.3645245730876923, Final Batch Loss: 0.47023916244506836\n",
      "Epoch 2194, Loss: 2.403414696455002, Final Batch Loss: 0.5049533843994141\n",
      "Epoch 2195, Loss: 2.310167819261551, Final Batch Loss: 0.46451953053474426\n",
      "Epoch 2196, Loss: 2.3236882984638214, Final Batch Loss: 0.5391902327537537\n",
      "Epoch 2197, Loss: 2.2571008801460266, Final Batch Loss: 0.5273765921592712\n",
      "Epoch 2198, Loss: 2.357937514781952, Final Batch Loss: 0.4019775390625\n",
      "Epoch 2199, Loss: 2.4275344908237457, Final Batch Loss: 0.4135509729385376\n",
      "Epoch 2200, Loss: 2.4319812953472137, Final Batch Loss: 0.4877203404903412\n",
      "Epoch 2201, Loss: 2.1214256286621094, Final Batch Loss: 0.4819696545600891\n",
      "Epoch 2202, Loss: 2.417929470539093, Final Batch Loss: 0.4765726625919342\n",
      "Epoch 2203, Loss: 2.1260582208633423, Final Batch Loss: 0.42951077222824097\n",
      "Epoch 2204, Loss: 2.4472230970859528, Final Batch Loss: 0.5076825022697449\n",
      "Epoch 2205, Loss: 2.3774604201316833, Final Batch Loss: 0.47102662920951843\n",
      "Epoch 2206, Loss: 2.2162147164344788, Final Batch Loss: 0.43793097138404846\n",
      "Epoch 2207, Loss: 2.2911570370197296, Final Batch Loss: 0.4552123546600342\n",
      "Epoch 2208, Loss: 2.267227530479431, Final Batch Loss: 0.47642529010772705\n",
      "Epoch 2209, Loss: 2.293699085712433, Final Batch Loss: 0.41042360663414\n",
      "Epoch 2210, Loss: 2.3917233645915985, Final Batch Loss: 0.5040997862815857\n",
      "Epoch 2211, Loss: 2.341805785894394, Final Batch Loss: 0.48706188797950745\n",
      "Epoch 2212, Loss: 2.3665566742420197, Final Batch Loss: 0.5041518807411194\n",
      "Epoch 2213, Loss: 2.236869513988495, Final Batch Loss: 0.46002310514450073\n",
      "Epoch 2214, Loss: 2.4126055240631104, Final Batch Loss: 0.49208298325538635\n",
      "Epoch 2215, Loss: 2.240381598472595, Final Batch Loss: 0.4936160147190094\n",
      "Epoch 2216, Loss: 2.2562122344970703, Final Batch Loss: 0.4237612187862396\n",
      "Epoch 2217, Loss: 2.314339965581894, Final Batch Loss: 0.44916510581970215\n",
      "Epoch 2218, Loss: 2.3266661763191223, Final Batch Loss: 0.39878806471824646\n",
      "Epoch 2219, Loss: 2.1780073046684265, Final Batch Loss: 0.46250468492507935\n",
      "Epoch 2220, Loss: 2.2660109400749207, Final Batch Loss: 0.39379194378852844\n",
      "Epoch 2221, Loss: 2.4742521941661835, Final Batch Loss: 0.5004294514656067\n",
      "Epoch 2222, Loss: 2.264629602432251, Final Batch Loss: 0.42026692628860474\n",
      "Epoch 2223, Loss: 2.308104485273361, Final Batch Loss: 0.437262624502182\n",
      "Epoch 2224, Loss: 2.394348233938217, Final Batch Loss: 0.4560490548610687\n",
      "Epoch 2225, Loss: 2.270540803670883, Final Batch Loss: 0.4742830991744995\n",
      "Epoch 2226, Loss: 2.4021007120609283, Final Batch Loss: 0.4817555546760559\n",
      "Epoch 2227, Loss: 2.323810577392578, Final Batch Loss: 0.4641806185245514\n",
      "Epoch 2228, Loss: 2.3278023302555084, Final Batch Loss: 0.418601930141449\n",
      "Epoch 2229, Loss: 2.2209113240242004, Final Batch Loss: 0.3929644227027893\n",
      "Epoch 2230, Loss: 2.464406669139862, Final Batch Loss: 0.4332246482372284\n",
      "Epoch 2231, Loss: 2.3766099214553833, Final Batch Loss: 0.48081469535827637\n",
      "Epoch 2232, Loss: 2.3747387528419495, Final Batch Loss: 0.46826764941215515\n",
      "Epoch 2233, Loss: 2.381838321685791, Final Batch Loss: 0.5126023888587952\n",
      "Epoch 2234, Loss: 2.3284943401813507, Final Batch Loss: 0.43984317779541016\n",
      "Epoch 2235, Loss: 2.385147124528885, Final Batch Loss: 0.5162930488586426\n",
      "Epoch 2236, Loss: 2.397251456975937, Final Batch Loss: 0.4583757519721985\n",
      "Epoch 2237, Loss: 2.4171387553215027, Final Batch Loss: 0.4147167503833771\n",
      "Epoch 2238, Loss: 2.4171270430088043, Final Batch Loss: 0.5872625112533569\n",
      "Epoch 2239, Loss: 2.1821384131908417, Final Batch Loss: 0.4624912142753601\n",
      "Epoch 2240, Loss: 2.1996108889579773, Final Batch Loss: 0.4418753683567047\n",
      "Epoch 2241, Loss: 2.439651697874069, Final Batch Loss: 0.5418018698692322\n",
      "Epoch 2242, Loss: 2.332793653011322, Final Batch Loss: 0.4415331780910492\n",
      "Epoch 2243, Loss: 2.2353003919124603, Final Batch Loss: 0.4197566509246826\n",
      "Epoch 2244, Loss: 2.1259664595127106, Final Batch Loss: 0.45310285687446594\n",
      "Epoch 2245, Loss: 2.458799034357071, Final Batch Loss: 0.42093175649642944\n",
      "Epoch 2246, Loss: 2.225674957036972, Final Batch Loss: 0.47888755798339844\n",
      "Epoch 2247, Loss: 2.2189623415470123, Final Batch Loss: 0.38479530811309814\n",
      "Epoch 2248, Loss: 2.3546352982521057, Final Batch Loss: 0.5457115173339844\n",
      "Epoch 2249, Loss: 2.5002653896808624, Final Batch Loss: 0.5283915996551514\n",
      "Epoch 2250, Loss: 2.3663090765476227, Final Batch Loss: 0.5169375538825989\n",
      "Epoch 2251, Loss: 2.3742328584194183, Final Batch Loss: 0.4834980070590973\n",
      "Epoch 2252, Loss: 2.1554059386253357, Final Batch Loss: 0.4664798974990845\n",
      "Epoch 2253, Loss: 2.243685483932495, Final Batch Loss: 0.4006190896034241\n",
      "Epoch 2254, Loss: 2.2506528794765472, Final Batch Loss: 0.48593226075172424\n",
      "Epoch 2255, Loss: 2.2654546201229095, Final Batch Loss: 0.40143296122550964\n",
      "Epoch 2256, Loss: 2.4224389493465424, Final Batch Loss: 0.43932580947875977\n",
      "Epoch 2257, Loss: 2.2381325364112854, Final Batch Loss: 0.41028857231140137\n",
      "Epoch 2258, Loss: 2.3244344890117645, Final Batch Loss: 0.440044105052948\n",
      "Epoch 2259, Loss: 2.3335390985012054, Final Batch Loss: 0.4733239710330963\n",
      "Epoch 2260, Loss: 2.3400246500968933, Final Batch Loss: 0.43578657507896423\n",
      "Epoch 2261, Loss: 2.377372443675995, Final Batch Loss: 0.501781165599823\n",
      "Epoch 2262, Loss: 2.3667868971824646, Final Batch Loss: 0.5212415456771851\n",
      "Epoch 2263, Loss: 2.4968854784965515, Final Batch Loss: 0.5027070641517639\n",
      "Epoch 2264, Loss: 2.3593415915966034, Final Batch Loss: 0.4819336533546448\n",
      "Epoch 2265, Loss: 2.3811021745204926, Final Batch Loss: 0.5018843412399292\n",
      "Epoch 2266, Loss: 2.20158788561821, Final Batch Loss: 0.41220515966415405\n",
      "Epoch 2267, Loss: 2.289740711450577, Final Batch Loss: 0.4296754002571106\n",
      "Epoch 2268, Loss: 2.1893676221370697, Final Batch Loss: 0.456830769777298\n",
      "Epoch 2269, Loss: 2.1274890899658203, Final Batch Loss: 0.3985462784767151\n",
      "Epoch 2270, Loss: 2.4224751591682434, Final Batch Loss: 0.5072681307792664\n",
      "Epoch 2271, Loss: 2.336682468652725, Final Batch Loss: 0.4544169008731842\n",
      "Epoch 2272, Loss: 2.292982816696167, Final Batch Loss: 0.46075811982154846\n",
      "Epoch 2273, Loss: 2.173851430416107, Final Batch Loss: 0.38239404559135437\n",
      "Epoch 2274, Loss: 2.4369785487651825, Final Batch Loss: 0.4211409389972687\n",
      "Epoch 2275, Loss: 2.433173418045044, Final Batch Loss: 0.493110716342926\n",
      "Epoch 2276, Loss: 2.2563168704509735, Final Batch Loss: 0.4233072102069855\n",
      "Epoch 2277, Loss: 2.3097574412822723, Final Batch Loss: 0.5426791906356812\n",
      "Epoch 2278, Loss: 2.324658453464508, Final Batch Loss: 0.38073548674583435\n",
      "Epoch 2279, Loss: 2.2812070548534393, Final Batch Loss: 0.4467596113681793\n",
      "Epoch 2280, Loss: 2.382314532995224, Final Batch Loss: 0.4827130138874054\n",
      "Epoch 2281, Loss: 2.505647599697113, Final Batch Loss: 0.5321117043495178\n",
      "Epoch 2282, Loss: 2.2793857157230377, Final Batch Loss: 0.5608698129653931\n",
      "Epoch 2283, Loss: 2.4366559386253357, Final Batch Loss: 0.4670567810535431\n",
      "Epoch 2284, Loss: 2.3922221660614014, Final Batch Loss: 0.5790852308273315\n",
      "Epoch 2285, Loss: 2.3070971965789795, Final Batch Loss: 0.4038539528846741\n",
      "Epoch 2286, Loss: 2.3396880328655243, Final Batch Loss: 0.5039211511611938\n",
      "Epoch 2287, Loss: 2.3794286847114563, Final Batch Loss: 0.42431607842445374\n",
      "Epoch 2288, Loss: 2.300649881362915, Final Batch Loss: 0.4721231758594513\n",
      "Epoch 2289, Loss: 2.314493179321289, Final Batch Loss: 0.4688095450401306\n",
      "Epoch 2290, Loss: 2.2547096610069275, Final Batch Loss: 0.42524072527885437\n",
      "Epoch 2291, Loss: 2.4301080405712128, Final Batch Loss: 0.35919249057769775\n",
      "Epoch 2292, Loss: 2.2643498182296753, Final Batch Loss: 0.45427757501602173\n",
      "Epoch 2293, Loss: 2.4167591631412506, Final Batch Loss: 0.47773846983909607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2294, Loss: 2.235348880290985, Final Batch Loss: 0.43984392285346985\n",
      "Epoch 2295, Loss: 2.212860405445099, Final Batch Loss: 0.49393048882484436\n",
      "Epoch 2296, Loss: 2.1651900708675385, Final Batch Loss: 0.4377879500389099\n",
      "Epoch 2297, Loss: 2.251946300268173, Final Batch Loss: 0.45958638191223145\n",
      "Epoch 2298, Loss: 2.323911041021347, Final Batch Loss: 0.4730844795703888\n",
      "Epoch 2299, Loss: 2.258580893278122, Final Batch Loss: 0.44654834270477295\n",
      "Epoch 2300, Loss: 2.4532162845134735, Final Batch Loss: 0.5054312944412231\n",
      "Epoch 2301, Loss: 2.2231653928756714, Final Batch Loss: 0.4726702868938446\n",
      "Epoch 2302, Loss: 2.239121198654175, Final Batch Loss: 0.4784125089645386\n",
      "Epoch 2303, Loss: 2.210750252008438, Final Batch Loss: 0.38357698917388916\n",
      "Epoch 2304, Loss: 2.2946817874908447, Final Batch Loss: 0.44379723072052\n",
      "Epoch 2305, Loss: 2.490112841129303, Final Batch Loss: 0.48131582140922546\n",
      "Epoch 2306, Loss: 2.348639816045761, Final Batch Loss: 0.4294739067554474\n",
      "Epoch 2307, Loss: 2.3537388741970062, Final Batch Loss: 0.38287797570228577\n",
      "Epoch 2308, Loss: 2.135136127471924, Final Batch Loss: 0.4252793490886688\n",
      "Epoch 2309, Loss: 2.4348945021629333, Final Batch Loss: 0.4947288930416107\n",
      "Epoch 2310, Loss: 2.245290517807007, Final Batch Loss: 0.3874714970588684\n",
      "Epoch 2311, Loss: 2.26606222987175, Final Batch Loss: 0.4671689569950104\n",
      "Epoch 2312, Loss: 2.091284453868866, Final Batch Loss: 0.41769295930862427\n",
      "Epoch 2313, Loss: 2.3687926828861237, Final Batch Loss: 0.4378683269023895\n",
      "Epoch 2314, Loss: 2.3500098288059235, Final Batch Loss: 0.41947758197784424\n",
      "Epoch 2315, Loss: 2.2281548380851746, Final Batch Loss: 0.4454768896102905\n",
      "Epoch 2316, Loss: 2.341048240661621, Final Batch Loss: 0.36453017592430115\n",
      "Epoch 2317, Loss: 2.3148415088653564, Final Batch Loss: 0.5139764547348022\n",
      "Epoch 2318, Loss: 2.311947077512741, Final Batch Loss: 0.3875831663608551\n",
      "Epoch 2319, Loss: 2.3074402809143066, Final Batch Loss: 0.4086637496948242\n",
      "Epoch 2320, Loss: 2.4263253808021545, Final Batch Loss: 0.4531157910823822\n",
      "Epoch 2321, Loss: 2.352741152048111, Final Batch Loss: 0.40270501375198364\n",
      "Epoch 2322, Loss: 2.3649992644786835, Final Batch Loss: 0.3545100688934326\n",
      "Epoch 2323, Loss: 2.464828073978424, Final Batch Loss: 0.4267215430736542\n",
      "Epoch 2324, Loss: 2.317661464214325, Final Batch Loss: 0.48385944962501526\n",
      "Epoch 2325, Loss: 2.314232289791107, Final Batch Loss: 0.4835328459739685\n",
      "Epoch 2326, Loss: 2.3087970912456512, Final Batch Loss: 0.5309277772903442\n",
      "Epoch 2327, Loss: 2.4019745588302612, Final Batch Loss: 0.6598466634750366\n",
      "Epoch 2328, Loss: 2.262527495622635, Final Batch Loss: 0.47188034653663635\n",
      "Epoch 2329, Loss: 2.2740504443645477, Final Batch Loss: 0.5129966735839844\n",
      "Epoch 2330, Loss: 2.3503193855285645, Final Batch Loss: 0.5196185111999512\n",
      "Epoch 2331, Loss: 2.2981119751930237, Final Batch Loss: 0.4740995168685913\n",
      "Epoch 2332, Loss: 2.3996582627296448, Final Batch Loss: 0.5172960758209229\n",
      "Epoch 2333, Loss: 2.274543195962906, Final Batch Loss: 0.5291731953620911\n",
      "Epoch 2334, Loss: 2.4045156240463257, Final Batch Loss: 0.5263665318489075\n",
      "Epoch 2335, Loss: 2.3458681404590607, Final Batch Loss: 0.3680838942527771\n",
      "Epoch 2336, Loss: 2.333128869533539, Final Batch Loss: 0.4772081971168518\n",
      "Epoch 2337, Loss: 2.231438159942627, Final Batch Loss: 0.379340797662735\n",
      "Epoch 2338, Loss: 2.395418345928192, Final Batch Loss: 0.45925068855285645\n",
      "Epoch 2339, Loss: 2.371460735797882, Final Batch Loss: 0.45935267210006714\n",
      "Epoch 2340, Loss: 2.340720295906067, Final Batch Loss: 0.5401011109352112\n",
      "Epoch 2341, Loss: 2.3287278711795807, Final Batch Loss: 0.4972735047340393\n",
      "Epoch 2342, Loss: 2.1509989202022552, Final Batch Loss: 0.3827548027038574\n",
      "Epoch 2343, Loss: 2.1600640416145325, Final Batch Loss: 0.47173306345939636\n",
      "Epoch 2344, Loss: 2.3112915754318237, Final Batch Loss: 0.48320913314819336\n",
      "Epoch 2345, Loss: 2.1983882784843445, Final Batch Loss: 0.41237229108810425\n",
      "Epoch 2346, Loss: 2.2505604326725006, Final Batch Loss: 0.3888850808143616\n",
      "Epoch 2347, Loss: 2.228372037410736, Final Batch Loss: 0.4692932665348053\n",
      "Epoch 2348, Loss: 2.301393210887909, Final Batch Loss: 0.3802536427974701\n",
      "Epoch 2349, Loss: 2.2497763633728027, Final Batch Loss: 0.45191460847854614\n",
      "Epoch 2350, Loss: 2.3003255128860474, Final Batch Loss: 0.39609503746032715\n",
      "Epoch 2351, Loss: 2.4115526378154755, Final Batch Loss: 0.47653937339782715\n",
      "Epoch 2352, Loss: 2.3381698727607727, Final Batch Loss: 0.42226195335388184\n",
      "Epoch 2353, Loss: 2.288344085216522, Final Batch Loss: 0.4961646795272827\n",
      "Epoch 2354, Loss: 2.3293578922748566, Final Batch Loss: 0.37064844369888306\n",
      "Epoch 2355, Loss: 2.312206745147705, Final Batch Loss: 0.4032871127128601\n",
      "Epoch 2356, Loss: 2.1383851766586304, Final Batch Loss: 0.4176432490348816\n",
      "Epoch 2357, Loss: 2.4355185627937317, Final Batch Loss: 0.5004174113273621\n",
      "Epoch 2358, Loss: 2.338747054338455, Final Batch Loss: 0.48787134885787964\n",
      "Epoch 2359, Loss: 2.328426331281662, Final Batch Loss: 0.46010321378707886\n",
      "Epoch 2360, Loss: 2.3241272568702698, Final Batch Loss: 0.5026514530181885\n",
      "Epoch 2361, Loss: 2.286420166492462, Final Batch Loss: 0.5311942100524902\n",
      "Epoch 2362, Loss: 2.466276168823242, Final Batch Loss: 0.46682095527648926\n",
      "Epoch 2363, Loss: 2.383093297481537, Final Batch Loss: 0.4223601520061493\n",
      "Epoch 2364, Loss: 2.408189535140991, Final Batch Loss: 0.534160315990448\n",
      "Epoch 2365, Loss: 2.197324901819229, Final Batch Loss: 0.4711000919342041\n",
      "Epoch 2366, Loss: 2.306918978691101, Final Batch Loss: 0.5661031603813171\n",
      "Epoch 2367, Loss: 2.2161943912506104, Final Batch Loss: 0.3984954059123993\n",
      "Epoch 2368, Loss: 2.4440929293632507, Final Batch Loss: 0.42539140582084656\n",
      "Epoch 2369, Loss: 2.351451575756073, Final Batch Loss: 0.5855987071990967\n",
      "Epoch 2370, Loss: 2.3603064119815826, Final Batch Loss: 0.36218205094337463\n",
      "Epoch 2371, Loss: 2.1647146940231323, Final Batch Loss: 0.5087008476257324\n",
      "Epoch 2372, Loss: 2.277178108692169, Final Batch Loss: 0.5280208587646484\n",
      "Epoch 2373, Loss: 2.2965755462646484, Final Batch Loss: 0.5276957750320435\n",
      "Epoch 2374, Loss: 2.3702661991119385, Final Batch Loss: 0.5129030346870422\n",
      "Epoch 2375, Loss: 2.3330033719539642, Final Batch Loss: 0.5339639186859131\n",
      "Epoch 2376, Loss: 2.218865931034088, Final Batch Loss: 0.5066813230514526\n",
      "Epoch 2377, Loss: 2.3387463688850403, Final Batch Loss: 0.4580441415309906\n",
      "Epoch 2378, Loss: 2.3182864487171173, Final Batch Loss: 0.39296799898147583\n",
      "Epoch 2379, Loss: 2.2103925347328186, Final Batch Loss: 0.4385000467300415\n",
      "Epoch 2380, Loss: 2.448021709918976, Final Batch Loss: 0.450656920671463\n",
      "Epoch 2381, Loss: 2.29955717921257, Final Batch Loss: 0.47480785846710205\n",
      "Epoch 2382, Loss: 2.1074671745300293, Final Batch Loss: 0.4810180366039276\n",
      "Epoch 2383, Loss: 2.3660454750061035, Final Batch Loss: 0.40828582644462585\n",
      "Epoch 2384, Loss: 2.2147631347179413, Final Batch Loss: 0.459430992603302\n",
      "Epoch 2385, Loss: 2.3439858853816986, Final Batch Loss: 0.5286622047424316\n",
      "Epoch 2386, Loss: 2.257496327161789, Final Batch Loss: 0.4879862666130066\n",
      "Epoch 2387, Loss: 2.278295159339905, Final Batch Loss: 0.3680156469345093\n",
      "Epoch 2388, Loss: 2.195854425430298, Final Batch Loss: 0.4763471186161041\n",
      "Epoch 2389, Loss: 2.31429722905159, Final Batch Loss: 0.4579566717147827\n",
      "Epoch 2390, Loss: 2.174386203289032, Final Batch Loss: 0.3809462785720825\n",
      "Epoch 2391, Loss: 2.143580824136734, Final Batch Loss: 0.41962918639183044\n",
      "Epoch 2392, Loss: 2.1536563634872437, Final Batch Loss: 0.4649036228656769\n",
      "Epoch 2393, Loss: 2.3035459220409393, Final Batch Loss: 0.4327974021434784\n",
      "Epoch 2394, Loss: 2.2471943497657776, Final Batch Loss: 0.48278751969337463\n",
      "Epoch 2395, Loss: 2.4210272431373596, Final Batch Loss: 0.4841707646846771\n",
      "Epoch 2396, Loss: 2.312135875225067, Final Batch Loss: 0.47225987911224365\n",
      "Epoch 2397, Loss: 2.2812943756580353, Final Batch Loss: 0.4348962604999542\n",
      "Epoch 2398, Loss: 2.212643265724182, Final Batch Loss: 0.4402753710746765\n",
      "Epoch 2399, Loss: 2.3169612884521484, Final Batch Loss: 0.47262829542160034\n",
      "Epoch 2400, Loss: 2.3661285042762756, Final Batch Loss: 0.5579036474227905\n",
      "Epoch 2401, Loss: 2.2981576323509216, Final Batch Loss: 0.4607340395450592\n",
      "Epoch 2402, Loss: 2.125960052013397, Final Batch Loss: 0.415147602558136\n",
      "Epoch 2403, Loss: 2.204928755760193, Final Batch Loss: 0.4307539761066437\n",
      "Epoch 2404, Loss: 2.214650273323059, Final Batch Loss: 0.37826794385910034\n",
      "Epoch 2405, Loss: 2.252380132675171, Final Batch Loss: 0.4865588843822479\n",
      "Epoch 2406, Loss: 2.379297763109207, Final Batch Loss: 0.5619888305664062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2407, Loss: 2.1722548604011536, Final Batch Loss: 0.4076659381389618\n",
      "Epoch 2408, Loss: 2.492581933736801, Final Batch Loss: 0.4655706584453583\n",
      "Epoch 2409, Loss: 2.141101211309433, Final Batch Loss: 0.419063925743103\n",
      "Epoch 2410, Loss: 2.3626298904418945, Final Batch Loss: 0.3946574926376343\n",
      "Epoch 2411, Loss: 2.2770342230796814, Final Batch Loss: 0.43254461884498596\n",
      "Epoch 2412, Loss: 2.4838394224643707, Final Batch Loss: 0.471509724855423\n",
      "Epoch 2413, Loss: 2.270322173833847, Final Batch Loss: 0.5154166221618652\n",
      "Epoch 2414, Loss: 2.2071551084518433, Final Batch Loss: 0.47829189896583557\n",
      "Epoch 2415, Loss: 2.183979421854019, Final Batch Loss: 0.3824709355831146\n",
      "Epoch 2416, Loss: 2.1686873137950897, Final Batch Loss: 0.44841641187667847\n",
      "Epoch 2417, Loss: 2.235454797744751, Final Batch Loss: 0.4143831729888916\n",
      "Epoch 2418, Loss: 2.2294736206531525, Final Batch Loss: 0.38975414633750916\n",
      "Epoch 2419, Loss: 2.1619684398174286, Final Batch Loss: 0.4592442810535431\n",
      "Epoch 2420, Loss: 2.377033054828644, Final Batch Loss: 0.4946364760398865\n",
      "Epoch 2421, Loss: 2.2578722536563873, Final Batch Loss: 0.4288921356201172\n",
      "Epoch 2422, Loss: 2.336697518825531, Final Batch Loss: 0.466439425945282\n",
      "Epoch 2423, Loss: 2.3089835345745087, Final Batch Loss: 0.5118448138237\n",
      "Epoch 2424, Loss: 2.271628737449646, Final Batch Loss: 0.43844297528266907\n",
      "Epoch 2425, Loss: 2.2278529703617096, Final Batch Loss: 0.5491008758544922\n",
      "Epoch 2426, Loss: 2.256098985671997, Final Batch Loss: 0.41825905442237854\n",
      "Epoch 2427, Loss: 2.316978335380554, Final Batch Loss: 0.447783887386322\n",
      "Epoch 2428, Loss: 2.4769553542137146, Final Batch Loss: 0.4462001919746399\n",
      "Epoch 2429, Loss: 2.3164291977882385, Final Batch Loss: 0.363056480884552\n",
      "Epoch 2430, Loss: 2.3296463787555695, Final Batch Loss: 0.4543549716472626\n",
      "Epoch 2431, Loss: 2.183521568775177, Final Batch Loss: 0.4341931939125061\n",
      "Epoch 2432, Loss: 2.3806824386119843, Final Batch Loss: 0.42497265338897705\n",
      "Epoch 2433, Loss: 2.3653090596199036, Final Batch Loss: 0.4167100787162781\n",
      "Epoch 2434, Loss: 2.1312583088874817, Final Batch Loss: 0.5263752937316895\n",
      "Epoch 2435, Loss: 2.3356997668743134, Final Batch Loss: 0.4394837021827698\n",
      "Epoch 2436, Loss: 2.3873129189014435, Final Batch Loss: 0.37845292687416077\n",
      "Epoch 2437, Loss: 2.2534597516059875, Final Batch Loss: 0.4541783630847931\n",
      "Epoch 2438, Loss: 2.4158489406108856, Final Batch Loss: 0.48517367243766785\n",
      "Epoch 2439, Loss: 2.237504482269287, Final Batch Loss: 0.45233234763145447\n",
      "Epoch 2440, Loss: 2.3329703211784363, Final Batch Loss: 0.5266103744506836\n",
      "Epoch 2441, Loss: 2.244726747274399, Final Batch Loss: 0.44118672609329224\n",
      "Epoch 2442, Loss: 2.3410963118076324, Final Batch Loss: 0.46376994252204895\n",
      "Epoch 2443, Loss: 2.34619602560997, Final Batch Loss: 0.4738716781139374\n",
      "Epoch 2444, Loss: 2.296854257583618, Final Batch Loss: 0.414680153131485\n",
      "Epoch 2445, Loss: 2.0896419286727905, Final Batch Loss: 0.3965723514556885\n",
      "Epoch 2446, Loss: 2.31110742688179, Final Batch Loss: 0.44897234439849854\n",
      "Epoch 2447, Loss: 2.2364493012428284, Final Batch Loss: 0.46498504281044006\n",
      "Epoch 2448, Loss: 2.160157084465027, Final Batch Loss: 0.42324063181877136\n",
      "Epoch 2449, Loss: 2.294444054365158, Final Batch Loss: 0.4094424545764923\n",
      "Epoch 2450, Loss: 2.3176184594631195, Final Batch Loss: 0.5276725888252258\n",
      "Epoch 2451, Loss: 2.358594983816147, Final Batch Loss: 0.456977903842926\n",
      "Epoch 2452, Loss: 2.356040507555008, Final Batch Loss: 0.4033128321170807\n",
      "Epoch 2453, Loss: 2.1559502482414246, Final Batch Loss: 0.37911278009414673\n",
      "Epoch 2454, Loss: 2.228501319885254, Final Batch Loss: 0.4515531361103058\n",
      "Epoch 2455, Loss: 2.346542865037918, Final Batch Loss: 0.5939409136772156\n",
      "Epoch 2456, Loss: 2.1138561964035034, Final Batch Loss: 0.3654610514640808\n",
      "Epoch 2457, Loss: 2.267967075109482, Final Batch Loss: 0.48045825958251953\n",
      "Epoch 2458, Loss: 2.2026824057102203, Final Batch Loss: 0.4382707476615906\n",
      "Epoch 2459, Loss: 2.2276738584041595, Final Batch Loss: 0.45483148097991943\n",
      "Epoch 2460, Loss: 2.191020667552948, Final Batch Loss: 0.4022737741470337\n",
      "Epoch 2461, Loss: 2.1548758149147034, Final Batch Loss: 0.3781433701515198\n",
      "Epoch 2462, Loss: 2.18359038233757, Final Batch Loss: 0.41438528895378113\n",
      "Epoch 2463, Loss: 2.2140693962574005, Final Batch Loss: 0.4545212984085083\n",
      "Epoch 2464, Loss: 2.34621998667717, Final Batch Loss: 0.5537129640579224\n",
      "Epoch 2465, Loss: 2.3459702134132385, Final Batch Loss: 0.4843723177909851\n",
      "Epoch 2466, Loss: 2.1806695759296417, Final Batch Loss: 0.37094542384147644\n",
      "Epoch 2467, Loss: 2.1455068588256836, Final Batch Loss: 0.5000049471855164\n",
      "Epoch 2468, Loss: 2.141315519809723, Final Batch Loss: 0.38770923018455505\n",
      "Epoch 2469, Loss: 2.379748374223709, Final Batch Loss: 0.4615456461906433\n",
      "Epoch 2470, Loss: 2.1462434232234955, Final Batch Loss: 0.4558389186859131\n",
      "Epoch 2471, Loss: 2.1954576075077057, Final Batch Loss: 0.44339853525161743\n",
      "Epoch 2472, Loss: 2.3652117550373077, Final Batch Loss: 0.5396755337715149\n",
      "Epoch 2473, Loss: 2.218360871076584, Final Batch Loss: 0.5074284672737122\n",
      "Epoch 2474, Loss: 2.2571245431900024, Final Batch Loss: 0.4098972976207733\n",
      "Epoch 2475, Loss: 2.2511328756809235, Final Batch Loss: 0.4868118464946747\n",
      "Epoch 2476, Loss: 2.291998654603958, Final Batch Loss: 0.47087377309799194\n",
      "Epoch 2477, Loss: 2.22477525472641, Final Batch Loss: 0.4131084978580475\n",
      "Epoch 2478, Loss: 2.2061169743537903, Final Batch Loss: 0.47619810700416565\n",
      "Epoch 2479, Loss: 2.4145946204662323, Final Batch Loss: 0.41277697682380676\n",
      "Epoch 2480, Loss: 2.0797824263572693, Final Batch Loss: 0.33981984853744507\n",
      "Epoch 2481, Loss: 2.22070649266243, Final Batch Loss: 0.4505646824836731\n",
      "Epoch 2482, Loss: 2.16022926568985, Final Batch Loss: 0.44118162989616394\n",
      "Epoch 2483, Loss: 2.1863672733306885, Final Batch Loss: 0.43204236030578613\n",
      "Epoch 2484, Loss: 2.3119248151779175, Final Batch Loss: 0.43560591340065\n",
      "Epoch 2485, Loss: 2.1727039515972137, Final Batch Loss: 0.4131168723106384\n",
      "Epoch 2486, Loss: 2.1692968010902405, Final Batch Loss: 0.5073268413543701\n",
      "Epoch 2487, Loss: 2.1870756447315216, Final Batch Loss: 0.4195311367511749\n",
      "Epoch 2488, Loss: 2.1868850886821747, Final Batch Loss: 0.46995413303375244\n",
      "Epoch 2489, Loss: 2.267660081386566, Final Batch Loss: 0.44749462604522705\n",
      "Epoch 2490, Loss: 2.2148247063159943, Final Batch Loss: 0.49121230840682983\n",
      "Epoch 2491, Loss: 2.364071160554886, Final Batch Loss: 0.5787508487701416\n",
      "Epoch 2492, Loss: 2.1300023794174194, Final Batch Loss: 0.4473302960395813\n",
      "Epoch 2493, Loss: 2.3276959359645844, Final Batch Loss: 0.5321305394172668\n",
      "Epoch 2494, Loss: 2.258138030767441, Final Batch Loss: 0.4805620312690735\n",
      "Epoch 2495, Loss: 2.2254990935325623, Final Batch Loss: 0.41117942333221436\n",
      "Epoch 2496, Loss: 2.2248455584049225, Final Batch Loss: 0.3777714967727661\n",
      "Epoch 2497, Loss: 2.1858072876930237, Final Batch Loss: 0.4270637631416321\n",
      "Epoch 2498, Loss: 2.2268015444278717, Final Batch Loss: 0.37150347232818604\n",
      "Epoch 2499, Loss: 2.163895398378372, Final Batch Loss: 0.44748517870903015\n",
      "Epoch 2500, Loss: 2.275772511959076, Final Batch Loss: 0.4609806537628174\n",
      "Epoch 2501, Loss: 2.18411123752594, Final Batch Loss: 0.4347383379936218\n",
      "Epoch 2502, Loss: 2.352366656064987, Final Batch Loss: 0.5199755430221558\n",
      "Epoch 2503, Loss: 2.324581801891327, Final Batch Loss: 0.5881605744361877\n",
      "Epoch 2504, Loss: 2.2358256578445435, Final Batch Loss: 0.48310455679893494\n",
      "Epoch 2505, Loss: 2.2941159307956696, Final Batch Loss: 0.4652225077152252\n",
      "Epoch 2506, Loss: 2.2270882427692413, Final Batch Loss: 0.46842288970947266\n",
      "Epoch 2507, Loss: 2.179642379283905, Final Batch Loss: 0.4096054136753082\n",
      "Epoch 2508, Loss: 2.1906729340553284, Final Batch Loss: 0.4769902527332306\n",
      "Epoch 2509, Loss: 2.2467063665390015, Final Batch Loss: 0.4948175251483917\n",
      "Epoch 2510, Loss: 2.2230995297431946, Final Batch Loss: 0.36418673396110535\n",
      "Epoch 2511, Loss: 2.3017599880695343, Final Batch Loss: 0.467697411775589\n",
      "Epoch 2512, Loss: 2.3041082322597504, Final Batch Loss: 0.4941187798976898\n",
      "Epoch 2513, Loss: 2.3910839557647705, Final Batch Loss: 0.5171135663986206\n",
      "Epoch 2514, Loss: 2.122133821249008, Final Batch Loss: 0.4030393660068512\n",
      "Epoch 2515, Loss: 2.237869292497635, Final Batch Loss: 0.49343141913414\n",
      "Epoch 2516, Loss: 2.0955429077148438, Final Batch Loss: 0.4456869661808014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2517, Loss: 2.2245884239673615, Final Batch Loss: 0.3370572328567505\n",
      "Epoch 2518, Loss: 2.06882706284523, Final Batch Loss: 0.4626440107822418\n",
      "Epoch 2519, Loss: 2.264911949634552, Final Batch Loss: 0.535144567489624\n",
      "Epoch 2520, Loss: 2.291991025209427, Final Batch Loss: 0.4623713195323944\n",
      "Epoch 2521, Loss: 2.237613320350647, Final Batch Loss: 0.4064706861972809\n",
      "Epoch 2522, Loss: 2.2333986461162567, Final Batch Loss: 0.34422314167022705\n",
      "Epoch 2523, Loss: 2.2823342382907867, Final Batch Loss: 0.44193655252456665\n",
      "Epoch 2524, Loss: 2.4170614182949066, Final Batch Loss: 0.5082860589027405\n",
      "Epoch 2525, Loss: 2.2475161850452423, Final Batch Loss: 0.3698342740535736\n",
      "Epoch 2526, Loss: 2.334991753101349, Final Batch Loss: 0.5386381149291992\n",
      "Epoch 2527, Loss: 2.23565274477005, Final Batch Loss: 0.4246987998485565\n",
      "Epoch 2528, Loss: 2.243298888206482, Final Batch Loss: 0.5190644264221191\n",
      "Epoch 2529, Loss: 2.422059088945389, Final Batch Loss: 0.48614129424095154\n",
      "Epoch 2530, Loss: 2.3422920405864716, Final Batch Loss: 0.49011707305908203\n",
      "Epoch 2531, Loss: 2.3483491837978363, Final Batch Loss: 0.4672841429710388\n",
      "Epoch 2532, Loss: 2.2032693326473236, Final Batch Loss: 0.4105307459831238\n",
      "Epoch 2533, Loss: 2.346024453639984, Final Batch Loss: 0.5050938725471497\n",
      "Epoch 2534, Loss: 2.3679458498954773, Final Batch Loss: 0.3996180295944214\n",
      "Epoch 2535, Loss: 2.2593443989753723, Final Batch Loss: 0.4036024212837219\n",
      "Epoch 2536, Loss: 2.2059068977832794, Final Batch Loss: 0.4189157485961914\n",
      "Epoch 2537, Loss: 2.256211996078491, Final Batch Loss: 0.5261949300765991\n",
      "Epoch 2538, Loss: 2.3189184069633484, Final Batch Loss: 0.4785021245479584\n",
      "Epoch 2539, Loss: 2.2458914518356323, Final Batch Loss: 0.48921847343444824\n",
      "Epoch 2540, Loss: 2.1055067479610443, Final Batch Loss: 0.4209769368171692\n",
      "Epoch 2541, Loss: 2.3281260430812836, Final Batch Loss: 0.5572153329849243\n",
      "Epoch 2542, Loss: 2.320545643568039, Final Batch Loss: 0.36320337653160095\n",
      "Epoch 2543, Loss: 2.1912125945091248, Final Batch Loss: 0.4675281345844269\n",
      "Epoch 2544, Loss: 2.1442368030548096, Final Batch Loss: 0.42197149991989136\n",
      "Epoch 2545, Loss: 2.40427303314209, Final Batch Loss: 0.4979771673679352\n",
      "Epoch 2546, Loss: 2.2560348510742188, Final Batch Loss: 0.35611456632614136\n",
      "Epoch 2547, Loss: 2.1590526700019836, Final Batch Loss: 0.38931533694267273\n",
      "Epoch 2548, Loss: 2.363216668367386, Final Batch Loss: 0.44875508546829224\n",
      "Epoch 2549, Loss: 2.3255422115325928, Final Batch Loss: 0.4998444616794586\n",
      "Epoch 2550, Loss: 2.3512540757656097, Final Batch Loss: 0.4057927429676056\n",
      "Epoch 2551, Loss: 2.2039677798748016, Final Batch Loss: 0.44168585538864136\n",
      "Epoch 2552, Loss: 2.1465339958667755, Final Batch Loss: 0.4184610843658447\n",
      "Epoch 2553, Loss: 2.25271412730217, Final Batch Loss: 0.4321267604827881\n",
      "Epoch 2554, Loss: 2.328633815050125, Final Batch Loss: 0.517827570438385\n",
      "Epoch 2555, Loss: 2.2364687025547028, Final Batch Loss: 0.5104917287826538\n",
      "Epoch 2556, Loss: 2.463022291660309, Final Batch Loss: 0.56562340259552\n",
      "Epoch 2557, Loss: 2.2511520981788635, Final Batch Loss: 0.5083850622177124\n",
      "Epoch 2558, Loss: 2.3159816563129425, Final Batch Loss: 0.4573301374912262\n",
      "Epoch 2559, Loss: 2.2731567919254303, Final Batch Loss: 0.5122414231300354\n",
      "Epoch 2560, Loss: 2.427907407283783, Final Batch Loss: 0.38931724429130554\n",
      "Epoch 2561, Loss: 2.221228063106537, Final Batch Loss: 0.4461055397987366\n",
      "Epoch 2562, Loss: 2.1732789874076843, Final Batch Loss: 0.41988834738731384\n",
      "Epoch 2563, Loss: 2.1317319571971893, Final Batch Loss: 0.44000720977783203\n",
      "Epoch 2564, Loss: 2.183571368455887, Final Batch Loss: 0.4659781754016876\n",
      "Epoch 2565, Loss: 2.1982759535312653, Final Batch Loss: 0.5176658034324646\n",
      "Epoch 2566, Loss: 2.1505370438098907, Final Batch Loss: 0.4257524907588959\n",
      "Epoch 2567, Loss: 2.221295654773712, Final Batch Loss: 0.49638304114341736\n",
      "Epoch 2568, Loss: 2.1084199845790863, Final Batch Loss: 0.42736315727233887\n",
      "Epoch 2569, Loss: 2.241750717163086, Final Batch Loss: 0.49361711740493774\n",
      "Epoch 2570, Loss: 2.355727434158325, Final Batch Loss: 0.4924248456954956\n",
      "Epoch 2571, Loss: 2.199233263731003, Final Batch Loss: 0.430276095867157\n",
      "Epoch 2572, Loss: 2.2779571413993835, Final Batch Loss: 0.4088911712169647\n",
      "Epoch 2573, Loss: 2.175672858953476, Final Batch Loss: 0.34323984384536743\n",
      "Epoch 2574, Loss: 2.4171712696552277, Final Batch Loss: 0.6694093346595764\n",
      "Epoch 2575, Loss: 2.195947051048279, Final Batch Loss: 0.4345018267631531\n",
      "Epoch 2576, Loss: 2.333736777305603, Final Batch Loss: 0.4068634510040283\n",
      "Epoch 2577, Loss: 2.206584393978119, Final Batch Loss: 0.4800938665866852\n",
      "Epoch 2578, Loss: 2.2630008459091187, Final Batch Loss: 0.4299604594707489\n",
      "Epoch 2579, Loss: 2.1541261672973633, Final Batch Loss: 0.4035252630710602\n",
      "Epoch 2580, Loss: 2.318926066160202, Final Batch Loss: 0.49234676361083984\n",
      "Epoch 2581, Loss: 2.306199610233307, Final Batch Loss: 0.4906570017337799\n",
      "Epoch 2582, Loss: 2.4748095273971558, Final Batch Loss: 0.5742127895355225\n",
      "Epoch 2583, Loss: 2.2560185492038727, Final Batch Loss: 0.42184117436408997\n",
      "Epoch 2584, Loss: 2.258832097053528, Final Batch Loss: 0.4307381808757782\n",
      "Epoch 2585, Loss: 2.2439023554325104, Final Batch Loss: 0.4274153411388397\n",
      "Epoch 2586, Loss: 2.3536756336688995, Final Batch Loss: 0.5336666703224182\n",
      "Epoch 2587, Loss: 2.322299748659134, Final Batch Loss: 0.46773484349250793\n",
      "Epoch 2588, Loss: 2.471802979707718, Final Batch Loss: 0.4946935176849365\n",
      "Epoch 2589, Loss: 2.36110720038414, Final Batch Loss: 0.46062082052230835\n",
      "Epoch 2590, Loss: 2.2867319881916046, Final Batch Loss: 0.48196691274642944\n",
      "Epoch 2591, Loss: 2.293853908777237, Final Batch Loss: 0.5007444620132446\n",
      "Epoch 2592, Loss: 2.0757263004779816, Final Batch Loss: 0.4565468430519104\n",
      "Epoch 2593, Loss: 2.2963251769542694, Final Batch Loss: 0.44442182779312134\n",
      "Epoch 2594, Loss: 1.9666603803634644, Final Batch Loss: 0.3797653615474701\n",
      "Epoch 2595, Loss: 2.2243224680423737, Final Batch Loss: 0.37400463223457336\n",
      "Epoch 2596, Loss: 2.254964530467987, Final Batch Loss: 0.41769468784332275\n",
      "Epoch 2597, Loss: 2.2718484103679657, Final Batch Loss: 0.5439708232879639\n",
      "Epoch 2598, Loss: 2.17379292845726, Final Batch Loss: 0.4255160391330719\n",
      "Epoch 2599, Loss: 2.052566736936569, Final Batch Loss: 0.41724011301994324\n",
      "Epoch 2600, Loss: 2.274156004190445, Final Batch Loss: 0.3654477298259735\n",
      "Epoch 2601, Loss: 2.1519491970539093, Final Batch Loss: 0.3717244267463684\n",
      "Epoch 2602, Loss: 2.4475942254066467, Final Batch Loss: 0.4635715186595917\n",
      "Epoch 2603, Loss: 2.2120897471904755, Final Batch Loss: 0.38299599289894104\n",
      "Epoch 2604, Loss: 2.2977465391159058, Final Batch Loss: 0.5140717625617981\n",
      "Epoch 2605, Loss: 2.124757558107376, Final Batch Loss: 0.4546396732330322\n",
      "Epoch 2606, Loss: 2.1232233941555023, Final Batch Loss: 0.3355569541454315\n",
      "Epoch 2607, Loss: 2.3115254044532776, Final Batch Loss: 0.6230456829071045\n",
      "Epoch 2608, Loss: 2.2649192810058594, Final Batch Loss: 0.41349396109580994\n",
      "Epoch 2609, Loss: 2.213118314743042, Final Batch Loss: 0.42998206615448\n",
      "Epoch 2610, Loss: 2.2352273166179657, Final Batch Loss: 0.4710431396961212\n",
      "Epoch 2611, Loss: 2.3397806584835052, Final Batch Loss: 0.4724729359149933\n",
      "Epoch 2612, Loss: 2.1842703819274902, Final Batch Loss: 0.4540442228317261\n",
      "Epoch 2613, Loss: 2.164699286222458, Final Batch Loss: 0.5590930581092834\n",
      "Epoch 2614, Loss: 2.262708067893982, Final Batch Loss: 0.46323418617248535\n",
      "Epoch 2615, Loss: 2.1127395629882812, Final Batch Loss: 0.4852825701236725\n",
      "Epoch 2616, Loss: 2.2526445388793945, Final Batch Loss: 0.4541008472442627\n",
      "Epoch 2617, Loss: 2.3089115917682648, Final Batch Loss: 0.4453648030757904\n",
      "Epoch 2618, Loss: 2.0730639696121216, Final Batch Loss: 0.41530367732048035\n",
      "Epoch 2619, Loss: 2.3404616117477417, Final Batch Loss: 0.40556302666664124\n",
      "Epoch 2620, Loss: 2.256399095058441, Final Batch Loss: 0.44403257966041565\n",
      "Epoch 2621, Loss: 2.2643920183181763, Final Batch Loss: 0.4978950321674347\n",
      "Epoch 2622, Loss: 2.1084213852882385, Final Batch Loss: 0.3678666353225708\n",
      "Epoch 2623, Loss: 2.202230453491211, Final Batch Loss: 0.44785019755363464\n",
      "Epoch 2624, Loss: 2.284296452999115, Final Batch Loss: 0.43797221779823303\n",
      "Epoch 2625, Loss: 2.1965948343276978, Final Batch Loss: 0.4009283781051636\n",
      "Epoch 2626, Loss: 2.1965803802013397, Final Batch Loss: 0.404795378446579\n",
      "Epoch 2627, Loss: 2.2140378654003143, Final Batch Loss: 0.42876797914505005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2628, Loss: 2.312436193227768, Final Batch Loss: 0.47185218334198\n",
      "Epoch 2629, Loss: 2.056120425462723, Final Batch Loss: 0.283399373292923\n",
      "Epoch 2630, Loss: 2.3336862921714783, Final Batch Loss: 0.47981640696525574\n",
      "Epoch 2631, Loss: 2.111819088459015, Final Batch Loss: 0.3601868748664856\n",
      "Epoch 2632, Loss: 2.202388882637024, Final Batch Loss: 0.4921216368675232\n",
      "Epoch 2633, Loss: 2.1329606771469116, Final Batch Loss: 0.43487751483917236\n",
      "Epoch 2634, Loss: 2.2409600913524628, Final Batch Loss: 0.37482088804244995\n",
      "Epoch 2635, Loss: 2.2881220877170563, Final Batch Loss: 0.4491463005542755\n",
      "Epoch 2636, Loss: 2.24272957444191, Final Batch Loss: 0.4315318465232849\n",
      "Epoch 2637, Loss: 2.1540815830230713, Final Batch Loss: 0.4248165786266327\n",
      "Epoch 2638, Loss: 1.9904606640338898, Final Batch Loss: 0.3545967936515808\n",
      "Epoch 2639, Loss: 2.20078444480896, Final Batch Loss: 0.563616156578064\n",
      "Epoch 2640, Loss: 2.207072913646698, Final Batch Loss: 0.3480169177055359\n",
      "Epoch 2641, Loss: 2.2846920490264893, Final Batch Loss: 0.4791533946990967\n",
      "Epoch 2642, Loss: 2.266361802816391, Final Batch Loss: 0.4793674945831299\n",
      "Epoch 2643, Loss: 2.360451400279999, Final Batch Loss: 0.4242386221885681\n",
      "Epoch 2644, Loss: 2.4163617193698883, Final Batch Loss: 0.48531296849250793\n",
      "Epoch 2645, Loss: 2.3087515830993652, Final Batch Loss: 0.4985335171222687\n",
      "Epoch 2646, Loss: 2.1575250923633575, Final Batch Loss: 0.4324720799922943\n",
      "Epoch 2647, Loss: 2.343304991722107, Final Batch Loss: 0.4702451527118683\n",
      "Epoch 2648, Loss: 2.0756781101226807, Final Batch Loss: 0.39802777767181396\n",
      "Epoch 2649, Loss: 2.2914765775203705, Final Batch Loss: 0.41262078285217285\n",
      "Epoch 2650, Loss: 2.5300029814243317, Final Batch Loss: 0.47819191217422485\n",
      "Epoch 2651, Loss: 2.40162393450737, Final Batch Loss: 0.5491277575492859\n",
      "Epoch 2652, Loss: 2.220706820487976, Final Batch Loss: 0.34805378317832947\n",
      "Epoch 2653, Loss: 2.270562559366226, Final Batch Loss: 0.49316704273223877\n",
      "Epoch 2654, Loss: 2.2850000262260437, Final Batch Loss: 0.385172575712204\n",
      "Epoch 2655, Loss: 2.1438222229480743, Final Batch Loss: 0.40699344873428345\n",
      "Epoch 2656, Loss: 2.15553942322731, Final Batch Loss: 0.4078058898448944\n",
      "Epoch 2657, Loss: 2.2443667352199554, Final Batch Loss: 0.400316059589386\n",
      "Epoch 2658, Loss: 2.167308807373047, Final Batch Loss: 0.47622957825660706\n",
      "Epoch 2659, Loss: 2.0360212326049805, Final Batch Loss: 0.40173789858818054\n",
      "Epoch 2660, Loss: 2.1655096113681793, Final Batch Loss: 0.35739201307296753\n",
      "Epoch 2661, Loss: 2.191357374191284, Final Batch Loss: 0.3736802339553833\n",
      "Epoch 2662, Loss: 2.302606910467148, Final Batch Loss: 0.4396890103816986\n",
      "Epoch 2663, Loss: 2.1812117397785187, Final Batch Loss: 0.4023943543434143\n",
      "Epoch 2664, Loss: 2.200795531272888, Final Batch Loss: 0.38190340995788574\n",
      "Epoch 2665, Loss: 2.308366447687149, Final Batch Loss: 0.5979611873626709\n",
      "Epoch 2666, Loss: 2.385077029466629, Final Batch Loss: 0.5340442061424255\n",
      "Epoch 2667, Loss: 2.2366987466812134, Final Batch Loss: 0.37184208631515503\n",
      "Epoch 2668, Loss: 2.1457610726356506, Final Batch Loss: 0.40321972966194153\n",
      "Epoch 2669, Loss: 2.1318039894104004, Final Batch Loss: 0.38616517186164856\n",
      "Epoch 2670, Loss: 2.2129546403884888, Final Batch Loss: 0.31496912240982056\n",
      "Epoch 2671, Loss: 2.2443785667419434, Final Batch Loss: 0.47274380922317505\n",
      "Epoch 2672, Loss: 2.3829135298728943, Final Batch Loss: 0.48682495951652527\n",
      "Epoch 2673, Loss: 2.1434743106365204, Final Batch Loss: 0.40169990062713623\n",
      "Epoch 2674, Loss: 2.315410792827606, Final Batch Loss: 0.5056621432304382\n",
      "Epoch 2675, Loss: 2.123027354478836, Final Batch Loss: 0.44281989336013794\n",
      "Epoch 2676, Loss: 2.1355099380016327, Final Batch Loss: 0.45317140221595764\n",
      "Epoch 2677, Loss: 2.183394640684128, Final Batch Loss: 0.3973748981952667\n",
      "Epoch 2678, Loss: 2.3434974253177643, Final Batch Loss: 0.41527360677719116\n",
      "Epoch 2679, Loss: 2.228813409805298, Final Batch Loss: 0.4478057622909546\n",
      "Epoch 2680, Loss: 2.1395750045776367, Final Batch Loss: 0.42771756649017334\n",
      "Epoch 2681, Loss: 2.1841512322425842, Final Batch Loss: 0.4079844057559967\n",
      "Epoch 2682, Loss: 2.216465950012207, Final Batch Loss: 0.39055171608924866\n",
      "Epoch 2683, Loss: 2.1315560042858124, Final Batch Loss: 0.41626283526420593\n",
      "Epoch 2684, Loss: 2.1335725486278534, Final Batch Loss: 0.3471992611885071\n",
      "Epoch 2685, Loss: 2.2170320749282837, Final Batch Loss: 0.4549603760242462\n",
      "Epoch 2686, Loss: 2.1522492170333862, Final Batch Loss: 0.4421357214450836\n",
      "Epoch 2687, Loss: 2.159414768218994, Final Batch Loss: 0.46489888429641724\n",
      "Epoch 2688, Loss: 2.2348441183567047, Final Batch Loss: 0.4126269519329071\n",
      "Epoch 2689, Loss: 2.244631588459015, Final Batch Loss: 0.4549330174922943\n",
      "Epoch 2690, Loss: 2.2793509364128113, Final Batch Loss: 0.48450571298599243\n",
      "Epoch 2691, Loss: 2.1092961728572845, Final Batch Loss: 0.39415818452835083\n",
      "Epoch 2692, Loss: 2.173405706882477, Final Batch Loss: 0.3878081440925598\n",
      "Epoch 2693, Loss: 2.227720469236374, Final Batch Loss: 0.46364814043045044\n",
      "Epoch 2694, Loss: 2.2809636294841766, Final Batch Loss: 0.462185263633728\n",
      "Epoch 2695, Loss: 2.280117928981781, Final Batch Loss: 0.5205308794975281\n",
      "Epoch 2696, Loss: 2.3345536291599274, Final Batch Loss: 0.46376049518585205\n",
      "Epoch 2697, Loss: 2.1925816237926483, Final Batch Loss: 0.49896496534347534\n",
      "Epoch 2698, Loss: 2.1993882954120636, Final Batch Loss: 0.3887604773044586\n",
      "Epoch 2699, Loss: 2.089361995458603, Final Batch Loss: 0.45224738121032715\n",
      "Epoch 2700, Loss: 2.2165239453315735, Final Batch Loss: 0.44727665185928345\n",
      "Epoch 2701, Loss: 2.243928015232086, Final Batch Loss: 0.47100380063056946\n",
      "Epoch 2702, Loss: 2.1245563626289368, Final Batch Loss: 0.3844694197177887\n",
      "Epoch 2703, Loss: 2.1493827402591705, Final Batch Loss: 0.4382287561893463\n",
      "Epoch 2704, Loss: 2.1910866498947144, Final Batch Loss: 0.3755686581134796\n",
      "Epoch 2705, Loss: 2.232096791267395, Final Batch Loss: 0.3894510567188263\n",
      "Epoch 2706, Loss: 2.213586837053299, Final Batch Loss: 0.41308748722076416\n",
      "Epoch 2707, Loss: 2.1703849732875824, Final Batch Loss: 0.42980384826660156\n",
      "Epoch 2708, Loss: 2.302192062139511, Final Batch Loss: 0.4537818431854248\n",
      "Epoch 2709, Loss: 2.218875825405121, Final Batch Loss: 0.4578138291835785\n",
      "Epoch 2710, Loss: 2.2580262422561646, Final Batch Loss: 0.4109668731689453\n",
      "Epoch 2711, Loss: 2.196490615606308, Final Batch Loss: 0.47574445605278015\n",
      "Epoch 2712, Loss: 2.3949058949947357, Final Batch Loss: 0.5692217350006104\n",
      "Epoch 2713, Loss: 2.1555636525154114, Final Batch Loss: 0.42690375447273254\n",
      "Epoch 2714, Loss: 2.235090911388397, Final Batch Loss: 0.49057549238204956\n",
      "Epoch 2715, Loss: 2.1672672629356384, Final Batch Loss: 0.4785831868648529\n",
      "Epoch 2716, Loss: 2.1560075283050537, Final Batch Loss: 0.45110422372817993\n",
      "Epoch 2717, Loss: 2.270370364189148, Final Batch Loss: 0.40160059928894043\n",
      "Epoch 2718, Loss: 2.241327404975891, Final Batch Loss: 0.418511301279068\n",
      "Epoch 2719, Loss: 2.1127379834651947, Final Batch Loss: 0.33482617139816284\n",
      "Epoch 2720, Loss: 2.128069192171097, Final Batch Loss: 0.4330969750881195\n",
      "Epoch 2721, Loss: 2.236271560192108, Final Batch Loss: 0.5277419686317444\n",
      "Epoch 2722, Loss: 2.1516295075416565, Final Batch Loss: 0.46665191650390625\n",
      "Epoch 2723, Loss: 2.222286194562912, Final Batch Loss: 0.5085939764976501\n",
      "Epoch 2724, Loss: 2.32574126124382, Final Batch Loss: 0.5195758938789368\n",
      "Epoch 2725, Loss: 2.055485725402832, Final Batch Loss: 0.3268677294254303\n",
      "Epoch 2726, Loss: 2.1132501661777496, Final Batch Loss: 0.40078210830688477\n",
      "Epoch 2727, Loss: 2.183511435985565, Final Batch Loss: 0.4357428550720215\n",
      "Epoch 2728, Loss: 2.063224643468857, Final Batch Loss: 0.4495027959346771\n",
      "Epoch 2729, Loss: 2.1891925036907196, Final Batch Loss: 0.3978743255138397\n",
      "Epoch 2730, Loss: 2.121829569339752, Final Batch Loss: 0.49281179904937744\n",
      "Epoch 2731, Loss: 2.303044319152832, Final Batch Loss: 0.43672284483909607\n",
      "Epoch 2732, Loss: 2.1287650763988495, Final Batch Loss: 0.3585949242115021\n",
      "Epoch 2733, Loss: 2.1715764105319977, Final Batch Loss: 0.47989383339881897\n",
      "Epoch 2734, Loss: 2.035400837659836, Final Batch Loss: 0.400319367647171\n",
      "Epoch 2735, Loss: 2.378975123167038, Final Batch Loss: 0.5895877480506897\n",
      "Epoch 2736, Loss: 2.219517469406128, Final Batch Loss: 0.33397167921066284\n",
      "Epoch 2737, Loss: 2.1077536046504974, Final Batch Loss: 0.38799184560775757\n",
      "Epoch 2738, Loss: 2.201887786388397, Final Batch Loss: 0.501840353012085\n",
      "Epoch 2739, Loss: 2.134419709444046, Final Batch Loss: 0.36094042658805847\n",
      "Epoch 2740, Loss: 2.1099115014076233, Final Batch Loss: 0.39729833602905273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2741, Loss: 2.118499219417572, Final Batch Loss: 0.38243281841278076\n",
      "Epoch 2742, Loss: 2.1813421845436096, Final Batch Loss: 0.43506547808647156\n",
      "Epoch 2743, Loss: 2.24619460105896, Final Batch Loss: 0.49709802865982056\n",
      "Epoch 2744, Loss: 2.270318329334259, Final Batch Loss: 0.47387638688087463\n",
      "Epoch 2745, Loss: 2.2218991816043854, Final Batch Loss: 0.363008975982666\n",
      "Epoch 2746, Loss: 2.0399839282035828, Final Batch Loss: 0.43940067291259766\n",
      "Epoch 2747, Loss: 2.0562278628349304, Final Batch Loss: 0.3648989200592041\n",
      "Epoch 2748, Loss: 1.9461719989776611, Final Batch Loss: 0.3633964955806732\n",
      "Epoch 2749, Loss: 2.4850862324237823, Final Batch Loss: 0.4271162748336792\n",
      "Epoch 2750, Loss: 2.23884853720665, Final Batch Loss: 0.3959050476551056\n",
      "Epoch 2751, Loss: 2.3260401487350464, Final Batch Loss: 0.4913780093193054\n",
      "Epoch 2752, Loss: 2.257955938577652, Final Batch Loss: 0.5425426363945007\n",
      "Epoch 2753, Loss: 2.1785119771957397, Final Batch Loss: 0.5049213171005249\n",
      "Epoch 2754, Loss: 2.2686405181884766, Final Batch Loss: 0.49480104446411133\n",
      "Epoch 2755, Loss: 2.1500116884708405, Final Batch Loss: 0.451168030500412\n",
      "Epoch 2756, Loss: 2.1889378130435944, Final Batch Loss: 0.49872079491615295\n",
      "Epoch 2757, Loss: 2.180491864681244, Final Batch Loss: 0.4523540139198303\n",
      "Epoch 2758, Loss: 2.0580222606658936, Final Batch Loss: 0.3710348308086395\n",
      "Epoch 2759, Loss: 2.2881536781787872, Final Batch Loss: 0.443095862865448\n",
      "Epoch 2760, Loss: 2.295836389064789, Final Batch Loss: 0.5300278067588806\n",
      "Epoch 2761, Loss: 2.152095854282379, Final Batch Loss: 0.4694754183292389\n",
      "Epoch 2762, Loss: 2.023955851793289, Final Batch Loss: 0.37194550037384033\n",
      "Epoch 2763, Loss: 2.3359054028987885, Final Batch Loss: 0.4066021740436554\n",
      "Epoch 2764, Loss: 2.0533467531204224, Final Batch Loss: 0.3437882661819458\n",
      "Epoch 2765, Loss: 2.15478453040123, Final Batch Loss: 0.4212714731693268\n",
      "Epoch 2766, Loss: 2.361377000808716, Final Batch Loss: 0.4958825409412384\n",
      "Epoch 2767, Loss: 2.3044813871383667, Final Batch Loss: 0.467974990606308\n",
      "Epoch 2768, Loss: 2.080378979444504, Final Batch Loss: 0.4211374819278717\n",
      "Epoch 2769, Loss: 2.134866774082184, Final Batch Loss: 0.5678396224975586\n",
      "Epoch 2770, Loss: 2.2531261146068573, Final Batch Loss: 0.4375115633010864\n",
      "Epoch 2771, Loss: 2.2513112127780914, Final Batch Loss: 0.41825059056282043\n",
      "Epoch 2772, Loss: 2.310693383216858, Final Batch Loss: 0.629048764705658\n",
      "Epoch 2773, Loss: 2.265740007162094, Final Batch Loss: 0.477077841758728\n",
      "Epoch 2774, Loss: 2.19205242395401, Final Batch Loss: 0.5294349193572998\n",
      "Epoch 2775, Loss: 2.220031291246414, Final Batch Loss: 0.5265746712684631\n",
      "Epoch 2776, Loss: 2.3181540966033936, Final Batch Loss: 0.5271281599998474\n",
      "Epoch 2777, Loss: 2.1834418773651123, Final Batch Loss: 0.4189322590827942\n",
      "Epoch 2778, Loss: 2.172060638666153, Final Batch Loss: 0.41644829511642456\n",
      "Epoch 2779, Loss: 2.218358099460602, Final Batch Loss: 0.4341098666191101\n",
      "Epoch 2780, Loss: 2.1401695907115936, Final Batch Loss: 0.38515937328338623\n",
      "Epoch 2781, Loss: 2.1506883203983307, Final Batch Loss: 0.491485595703125\n",
      "Epoch 2782, Loss: 2.0826616287231445, Final Batch Loss: 0.4677278995513916\n",
      "Epoch 2783, Loss: 2.226249575614929, Final Batch Loss: 0.4452672600746155\n",
      "Epoch 2784, Loss: 2.1332322359085083, Final Batch Loss: 0.3935709297657013\n",
      "Epoch 2785, Loss: 2.094912588596344, Final Batch Loss: 0.4310397803783417\n",
      "Epoch 2786, Loss: 2.216167062520981, Final Batch Loss: 0.44700372219085693\n",
      "Epoch 2787, Loss: 2.195510596036911, Final Batch Loss: 0.3541354238986969\n",
      "Epoch 2788, Loss: 2.0273537039756775, Final Batch Loss: 0.418296217918396\n",
      "Epoch 2789, Loss: 2.219986617565155, Final Batch Loss: 0.4906657934188843\n",
      "Epoch 2790, Loss: 2.252747654914856, Final Batch Loss: 0.4462240934371948\n",
      "Epoch 2791, Loss: 2.433003395795822, Final Batch Loss: 0.4416927695274353\n",
      "Epoch 2792, Loss: 2.2645854353904724, Final Batch Loss: 0.5156485438346863\n",
      "Epoch 2793, Loss: 2.2617497742176056, Final Batch Loss: 0.4587799906730652\n",
      "Epoch 2794, Loss: 2.2559416592121124, Final Batch Loss: 0.49869203567504883\n",
      "Epoch 2795, Loss: 2.2425283193588257, Final Batch Loss: 0.3945435881614685\n",
      "Epoch 2796, Loss: 2.2924766540527344, Final Batch Loss: 0.4620778262615204\n",
      "Epoch 2797, Loss: 2.0606898069381714, Final Batch Loss: 0.3843991756439209\n",
      "Epoch 2798, Loss: 2.1383246183395386, Final Batch Loss: 0.44955843687057495\n",
      "Epoch 2799, Loss: 2.184086263179779, Final Batch Loss: 0.47707271575927734\n",
      "Epoch 2800, Loss: 2.0513584315776825, Final Batch Loss: 0.38316014409065247\n",
      "Epoch 2801, Loss: 2.1484939754009247, Final Batch Loss: 0.4778374135494232\n",
      "Epoch 2802, Loss: 2.088579386472702, Final Batch Loss: 0.36747875809669495\n",
      "Epoch 2803, Loss: 2.2099786400794983, Final Batch Loss: 0.3315827250480652\n",
      "Epoch 2804, Loss: 2.1317711770534515, Final Batch Loss: 0.4093015193939209\n",
      "Epoch 2805, Loss: 2.121158391237259, Final Batch Loss: 0.3463452458381653\n",
      "Epoch 2806, Loss: 2.0579133927822113, Final Batch Loss: 0.46488481760025024\n",
      "Epoch 2807, Loss: 2.1612561345100403, Final Batch Loss: 0.39209598302841187\n",
      "Epoch 2808, Loss: 2.2911821603775024, Final Batch Loss: 0.3951396644115448\n",
      "Epoch 2809, Loss: 2.058238238096237, Final Batch Loss: 0.3693797290325165\n",
      "Epoch 2810, Loss: 2.184900462627411, Final Batch Loss: 0.4458601176738739\n",
      "Epoch 2811, Loss: 2.1934199035167694, Final Batch Loss: 0.32844147086143494\n",
      "Epoch 2812, Loss: 2.192628711462021, Final Batch Loss: 0.3873142600059509\n",
      "Epoch 2813, Loss: 2.1769056618213654, Final Batch Loss: 0.437401682138443\n",
      "Epoch 2814, Loss: 2.111346513032913, Final Batch Loss: 0.49969568848609924\n",
      "Epoch 2815, Loss: 2.211192548274994, Final Batch Loss: 0.4196777045726776\n",
      "Epoch 2816, Loss: 2.1797860860824585, Final Batch Loss: 0.4322749674320221\n",
      "Epoch 2817, Loss: 2.180985003709793, Final Batch Loss: 0.4236855208873749\n",
      "Epoch 2818, Loss: 2.212809830904007, Final Batch Loss: 0.4421446919441223\n",
      "Epoch 2819, Loss: 2.136191099882126, Final Batch Loss: 0.44886600971221924\n",
      "Epoch 2820, Loss: 1.9949450492858887, Final Batch Loss: 0.3307759463787079\n",
      "Epoch 2821, Loss: 2.297866702079773, Final Batch Loss: 0.3954622447490692\n",
      "Epoch 2822, Loss: 2.106460362672806, Final Batch Loss: 0.3627471625804901\n",
      "Epoch 2823, Loss: 2.127782881259918, Final Batch Loss: 0.3730842173099518\n",
      "Epoch 2824, Loss: 2.2141161262989044, Final Batch Loss: 0.5356650948524475\n",
      "Epoch 2825, Loss: 2.1023676991462708, Final Batch Loss: 0.40404942631721497\n",
      "Epoch 2826, Loss: 2.1092768609523773, Final Batch Loss: 0.37492069602012634\n",
      "Epoch 2827, Loss: 2.0999715328216553, Final Batch Loss: 0.3793124854564667\n",
      "Epoch 2828, Loss: 2.3347097635269165, Final Batch Loss: 0.5076736211776733\n",
      "Epoch 2829, Loss: 2.1718631088733673, Final Batch Loss: 0.40161609649658203\n",
      "Epoch 2830, Loss: 2.1554340422153473, Final Batch Loss: 0.38004404306411743\n",
      "Epoch 2831, Loss: 2.3926354944705963, Final Batch Loss: 0.4552614986896515\n",
      "Epoch 2832, Loss: 2.0993854105472565, Final Batch Loss: 0.45188915729522705\n",
      "Epoch 2833, Loss: 2.143329083919525, Final Batch Loss: 0.3948225975036621\n",
      "Epoch 2834, Loss: 2.2481005489826202, Final Batch Loss: 0.4560135304927826\n",
      "Epoch 2835, Loss: 1.987906038761139, Final Batch Loss: 0.40632882714271545\n",
      "Epoch 2836, Loss: 2.2328154146671295, Final Batch Loss: 0.41574832797050476\n",
      "Epoch 2837, Loss: 2.164818584918976, Final Batch Loss: 0.4292701780796051\n",
      "Epoch 2838, Loss: 2.1530781388282776, Final Batch Loss: 0.40781840682029724\n",
      "Epoch 2839, Loss: 2.2875439524650574, Final Batch Loss: 0.4821758568286896\n",
      "Epoch 2840, Loss: 2.0054818987846375, Final Batch Loss: 0.42889073491096497\n",
      "Epoch 2841, Loss: 2.3446895480155945, Final Batch Loss: 0.46251973509788513\n",
      "Epoch 2842, Loss: 1.9943000674247742, Final Batch Loss: 0.3610803484916687\n",
      "Epoch 2843, Loss: 2.1844893991947174, Final Batch Loss: 0.4817015528678894\n",
      "Epoch 2844, Loss: 2.1299192309379578, Final Batch Loss: 0.49438902735710144\n",
      "Epoch 2845, Loss: 2.155138462781906, Final Batch Loss: 0.3523484766483307\n",
      "Epoch 2846, Loss: 2.2460191547870636, Final Batch Loss: 0.4053069055080414\n",
      "Epoch 2847, Loss: 2.2298334538936615, Final Batch Loss: 0.46105703711509705\n",
      "Epoch 2848, Loss: 2.1390067040920258, Final Batch Loss: 0.46071475744247437\n",
      "Epoch 2849, Loss: 2.0215896368026733, Final Batch Loss: 0.3919263184070587\n",
      "Epoch 2850, Loss: 2.2340639233589172, Final Batch Loss: 0.5233067274093628\n",
      "Epoch 2851, Loss: 2.1239006519317627, Final Batch Loss: 0.35208016633987427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2852, Loss: 2.1309049129486084, Final Batch Loss: 0.4064173400402069\n",
      "Epoch 2853, Loss: 2.137495458126068, Final Batch Loss: 0.5106035470962524\n",
      "Epoch 2854, Loss: 2.011680483818054, Final Batch Loss: 0.4039926528930664\n",
      "Epoch 2855, Loss: 2.1114805936813354, Final Batch Loss: 0.40573182702064514\n",
      "Epoch 2856, Loss: 2.0933868885040283, Final Batch Loss: 0.4813247323036194\n",
      "Epoch 2857, Loss: 2.105548173189163, Final Batch Loss: 0.3485899269580841\n",
      "Epoch 2858, Loss: 2.0736337900161743, Final Batch Loss: 0.3684435486793518\n",
      "Epoch 2859, Loss: 2.20786714553833, Final Batch Loss: 0.44453954696655273\n",
      "Epoch 2860, Loss: 2.091911792755127, Final Batch Loss: 0.4614878296852112\n",
      "Epoch 2861, Loss: 1.990794152021408, Final Batch Loss: 0.3459106385707855\n",
      "Epoch 2862, Loss: 1.968151867389679, Final Batch Loss: 0.33134546875953674\n",
      "Epoch 2863, Loss: 1.9336711764335632, Final Batch Loss: 0.35913923382759094\n",
      "Epoch 2864, Loss: 2.0643912851810455, Final Batch Loss: 0.38762176036834717\n",
      "Epoch 2865, Loss: 1.945337563753128, Final Batch Loss: 0.4239661693572998\n",
      "Epoch 2866, Loss: 2.14948308467865, Final Batch Loss: 0.3967049717903137\n",
      "Epoch 2867, Loss: 2.040128380060196, Final Batch Loss: 0.3751108944416046\n",
      "Epoch 2868, Loss: 1.97901451587677, Final Batch Loss: 0.3930175006389618\n",
      "Epoch 2869, Loss: 2.173067659139633, Final Batch Loss: 0.40622612833976746\n",
      "Epoch 2870, Loss: 2.111659586429596, Final Batch Loss: 0.3861967623233795\n",
      "Epoch 2871, Loss: 2.2862681448459625, Final Batch Loss: 0.5248330235481262\n",
      "Epoch 2872, Loss: 2.1784256994724274, Final Batch Loss: 0.46800294518470764\n",
      "Epoch 2873, Loss: 2.1592437624931335, Final Batch Loss: 0.3444100022315979\n",
      "Epoch 2874, Loss: 2.1269619166851044, Final Batch Loss: 0.5491229891777039\n",
      "Epoch 2875, Loss: 2.2431749403476715, Final Batch Loss: 0.491525262594223\n",
      "Epoch 2876, Loss: 2.1420361399650574, Final Batch Loss: 0.4613074064254761\n",
      "Epoch 2877, Loss: 2.3646880984306335, Final Batch Loss: 0.41376224160194397\n",
      "Epoch 2878, Loss: 2.2751166820526123, Final Batch Loss: 0.43642523884773254\n",
      "Epoch 2879, Loss: 2.2214718759059906, Final Batch Loss: 0.3549652695655823\n",
      "Epoch 2880, Loss: 2.263723373413086, Final Batch Loss: 0.5280243754386902\n",
      "Epoch 2881, Loss: 2.2318795919418335, Final Batch Loss: 0.520094633102417\n",
      "Epoch 2882, Loss: 2.2820542454719543, Final Batch Loss: 0.4919202923774719\n",
      "Epoch 2883, Loss: 2.163799285888672, Final Batch Loss: 0.363174170255661\n",
      "Epoch 2884, Loss: 2.1912948191165924, Final Batch Loss: 0.5147602558135986\n",
      "Epoch 2885, Loss: 2.1226651072502136, Final Batch Loss: 0.4396737515926361\n",
      "Epoch 2886, Loss: 2.165266454219818, Final Batch Loss: 0.3943493962287903\n",
      "Epoch 2887, Loss: 2.0427787601947784, Final Batch Loss: 0.4609472155570984\n",
      "Epoch 2888, Loss: 2.405615895986557, Final Batch Loss: 0.449606716632843\n",
      "Epoch 2889, Loss: 2.2188343703746796, Final Batch Loss: 0.4589284658432007\n",
      "Epoch 2890, Loss: 2.165127009153366, Final Batch Loss: 0.4030151963233948\n",
      "Epoch 2891, Loss: 2.1690317690372467, Final Batch Loss: 0.423198938369751\n",
      "Epoch 2892, Loss: 2.258048176765442, Final Batch Loss: 0.4613801836967468\n",
      "Epoch 2893, Loss: 2.0918853282928467, Final Batch Loss: 0.44900843501091003\n",
      "Epoch 2894, Loss: 2.066645234823227, Final Batch Loss: 0.4469558000564575\n",
      "Epoch 2895, Loss: 2.0796855092048645, Final Batch Loss: 0.42590364813804626\n",
      "Epoch 2896, Loss: 2.091555505990982, Final Batch Loss: 0.4604874849319458\n",
      "Epoch 2897, Loss: 1.9686395823955536, Final Batch Loss: 0.3231266140937805\n",
      "Epoch 2898, Loss: 2.2234172224998474, Final Batch Loss: 0.4526306390762329\n",
      "Epoch 2899, Loss: 2.1861452460289, Final Batch Loss: 0.4345519244670868\n",
      "Epoch 2900, Loss: 2.1099860966205597, Final Batch Loss: 0.4351806640625\n",
      "Epoch 2901, Loss: 2.2014307975769043, Final Batch Loss: 0.4476187229156494\n",
      "Epoch 2902, Loss: 2.0719571113586426, Final Batch Loss: 0.4404958486557007\n",
      "Epoch 2903, Loss: 2.205700606107712, Final Batch Loss: 0.42764732241630554\n",
      "Epoch 2904, Loss: 2.025739014148712, Final Batch Loss: 0.4154050350189209\n",
      "Epoch 2905, Loss: 2.2181332111358643, Final Batch Loss: 0.49447259306907654\n",
      "Epoch 2906, Loss: 2.247739225625992, Final Batch Loss: 0.39779728651046753\n",
      "Epoch 2907, Loss: 2.200920730829239, Final Batch Loss: 0.4134341776371002\n",
      "Epoch 2908, Loss: 2.243464857339859, Final Batch Loss: 0.42186397314071655\n",
      "Epoch 2909, Loss: 2.1603880524635315, Final Batch Loss: 0.46493497490882874\n",
      "Epoch 2910, Loss: 2.216076076030731, Final Batch Loss: 0.4248250126838684\n",
      "Epoch 2911, Loss: 2.165579527616501, Final Batch Loss: 0.5107152462005615\n",
      "Epoch 2912, Loss: 2.297236353158951, Final Batch Loss: 0.520307719707489\n",
      "Epoch 2913, Loss: 2.142360359430313, Final Batch Loss: 0.4220917224884033\n",
      "Epoch 2914, Loss: 2.162136197090149, Final Batch Loss: 0.4035245180130005\n",
      "Epoch 2915, Loss: 2.0511346459388733, Final Batch Loss: 0.3621285557746887\n",
      "Epoch 2916, Loss: 2.377550184726715, Final Batch Loss: 0.32834315299987793\n",
      "Epoch 2917, Loss: 2.1969379782676697, Final Batch Loss: 0.44109421968460083\n",
      "Epoch 2918, Loss: 2.0545351207256317, Final Batch Loss: 0.4282137155532837\n",
      "Epoch 2919, Loss: 2.0802222788333893, Final Batch Loss: 0.3907676935195923\n",
      "Epoch 2920, Loss: 2.2038401663303375, Final Batch Loss: 0.486961305141449\n",
      "Epoch 2921, Loss: 2.149259716272354, Final Batch Loss: 0.36105936765670776\n",
      "Epoch 2922, Loss: 2.2000450491905212, Final Batch Loss: 0.5401105284690857\n",
      "Epoch 2923, Loss: 2.14894500374794, Final Batch Loss: 0.4955886900424957\n",
      "Epoch 2924, Loss: 2.1023885309696198, Final Batch Loss: 0.4263962507247925\n",
      "Epoch 2925, Loss: 2.240444242954254, Final Batch Loss: 0.3941676616668701\n",
      "Epoch 2926, Loss: 2.1856943666934967, Final Batch Loss: 0.3888281285762787\n",
      "Epoch 2927, Loss: 2.1252091825008392, Final Batch Loss: 0.43592849373817444\n",
      "Epoch 2928, Loss: 2.1724592447280884, Final Batch Loss: 0.3924766182899475\n",
      "Epoch 2929, Loss: 2.0432233810424805, Final Batch Loss: 0.43106749653816223\n",
      "Epoch 2930, Loss: 2.0932937562465668, Final Batch Loss: 0.44249117374420166\n",
      "Epoch 2931, Loss: 2.1258663833141327, Final Batch Loss: 0.40565407276153564\n",
      "Epoch 2932, Loss: 2.0702563524246216, Final Batch Loss: 0.42303797602653503\n",
      "Epoch 2933, Loss: 2.1365381479263306, Final Batch Loss: 0.4257979094982147\n",
      "Epoch 2934, Loss: 2.13421967625618, Final Batch Loss: 0.4372684061527252\n",
      "Epoch 2935, Loss: 2.0557903349399567, Final Batch Loss: 0.4393485188484192\n",
      "Epoch 2936, Loss: 2.257671505212784, Final Batch Loss: 0.5281127095222473\n",
      "Epoch 2937, Loss: 2.1186007261276245, Final Batch Loss: 0.3733140826225281\n",
      "Epoch 2938, Loss: 1.9949012994766235, Final Batch Loss: 0.33633193373680115\n",
      "Epoch 2939, Loss: 2.356032371520996, Final Batch Loss: 0.44664230942726135\n",
      "Epoch 2940, Loss: 2.164301574230194, Final Batch Loss: 0.4657590985298157\n",
      "Epoch 2941, Loss: 2.134517878293991, Final Batch Loss: 0.40789222717285156\n",
      "Epoch 2942, Loss: 2.3419048488140106, Final Batch Loss: 0.3911006450653076\n",
      "Epoch 2943, Loss: 2.166669547557831, Final Batch Loss: 0.35748159885406494\n",
      "Epoch 2944, Loss: 2.121214121580124, Final Batch Loss: 0.37872645258903503\n",
      "Epoch 2945, Loss: 2.156256914138794, Final Batch Loss: 0.38814568519592285\n",
      "Epoch 2946, Loss: 2.159410148859024, Final Batch Loss: 0.5269091725349426\n",
      "Epoch 2947, Loss: 2.24020254611969, Final Batch Loss: 0.39762383699417114\n",
      "Epoch 2948, Loss: 2.14774888753891, Final Batch Loss: 0.42541617155075073\n",
      "Epoch 2949, Loss: 2.0004122853279114, Final Batch Loss: 0.4206067621707916\n",
      "Epoch 2950, Loss: 2.248637765645981, Final Batch Loss: 0.5294839143753052\n",
      "Epoch 2951, Loss: 2.1160516142845154, Final Batch Loss: 0.3861834704875946\n",
      "Epoch 2952, Loss: 2.1599631011486053, Final Batch Loss: 0.3894658386707306\n",
      "Epoch 2953, Loss: 2.12549090385437, Final Batch Loss: 0.36408376693725586\n",
      "Epoch 2954, Loss: 2.1685753762722015, Final Batch Loss: 0.38489195704460144\n",
      "Epoch 2955, Loss: 2.1649437844753265, Final Batch Loss: 0.45247018337249756\n",
      "Epoch 2956, Loss: 2.35502165555954, Final Batch Loss: 0.5694563984870911\n",
      "Epoch 2957, Loss: 2.0563161373138428, Final Batch Loss: 0.41719916462898254\n",
      "Epoch 2958, Loss: 2.019747167825699, Final Batch Loss: 0.4056345224380493\n",
      "Epoch 2959, Loss: 2.004018187522888, Final Batch Loss: 0.40530237555503845\n",
      "Epoch 2960, Loss: 2.285868674516678, Final Batch Loss: 0.4563218951225281\n",
      "Epoch 2961, Loss: 2.1609672009944916, Final Batch Loss: 0.4479076862335205\n",
      "Epoch 2962, Loss: 1.9309941232204437, Final Batch Loss: 0.33161255717277527\n",
      "Epoch 2963, Loss: 1.9656354188919067, Final Batch Loss: 0.3584575355052948\n",
      "Epoch 2964, Loss: 2.237474709749222, Final Batch Loss: 0.4522152245044708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2965, Loss: 2.0069788992404938, Final Batch Loss: 0.41665956377983093\n",
      "Epoch 2966, Loss: 2.1580061316490173, Final Batch Loss: 0.38525211811065674\n",
      "Epoch 2967, Loss: 2.075133055448532, Final Batch Loss: 0.4866049885749817\n",
      "Epoch 2968, Loss: 2.1894465684890747, Final Batch Loss: 0.3806040585041046\n",
      "Epoch 2969, Loss: 2.1707387268543243, Final Batch Loss: 0.4018022119998932\n",
      "Epoch 2970, Loss: 2.1453275084495544, Final Batch Loss: 0.3299175202846527\n",
      "Epoch 2971, Loss: 1.9933198988437653, Final Batch Loss: 0.4420144259929657\n",
      "Epoch 2972, Loss: 2.057951867580414, Final Batch Loss: 0.4029052257537842\n",
      "Epoch 2973, Loss: 1.9377034902572632, Final Batch Loss: 0.4184471070766449\n",
      "Epoch 2974, Loss: 2.0222887992858887, Final Batch Loss: 0.3970213532447815\n",
      "Epoch 2975, Loss: 2.1961028575897217, Final Batch Loss: 0.4501938223838806\n",
      "Epoch 2976, Loss: 2.1062367260456085, Final Batch Loss: 0.47308656573295593\n",
      "Epoch 2977, Loss: 2.030586898326874, Final Batch Loss: 0.4104022979736328\n",
      "Epoch 2978, Loss: 2.1360018849372864, Final Batch Loss: 0.37028902769088745\n",
      "Epoch 2979, Loss: 2.1325386464595795, Final Batch Loss: 0.3922765851020813\n",
      "Epoch 2980, Loss: 2.0411468744277954, Final Batch Loss: 0.4213096499443054\n",
      "Epoch 2981, Loss: 2.0393878519535065, Final Batch Loss: 0.3921933174133301\n",
      "Epoch 2982, Loss: 2.117601990699768, Final Batch Loss: 0.4716712534427643\n",
      "Epoch 2983, Loss: 2.283046782016754, Final Batch Loss: 0.5739617347717285\n",
      "Epoch 2984, Loss: 2.1341546177864075, Final Batch Loss: 0.45737171173095703\n",
      "Epoch 2985, Loss: 2.0151102542877197, Final Batch Loss: 0.37771132588386536\n",
      "Epoch 2986, Loss: 2.2939376831054688, Final Batch Loss: 0.5017646551132202\n",
      "Epoch 2987, Loss: 2.1588964462280273, Final Batch Loss: 0.4363686442375183\n",
      "Epoch 2988, Loss: 2.202474147081375, Final Batch Loss: 0.44179248809814453\n",
      "Epoch 2989, Loss: 2.1259122490882874, Final Batch Loss: 0.38529282808303833\n",
      "Epoch 2990, Loss: 2.1531348824501038, Final Batch Loss: 0.3116270899772644\n",
      "Epoch 2991, Loss: 2.1102514266967773, Final Batch Loss: 0.37630346417427063\n",
      "Epoch 2992, Loss: 2.1248379349708557, Final Batch Loss: 0.40376782417297363\n",
      "Epoch 2993, Loss: 2.342410385608673, Final Batch Loss: 0.5231989622116089\n",
      "Epoch 2994, Loss: 1.9141114056110382, Final Batch Loss: 0.35610413551330566\n",
      "Epoch 2995, Loss: 2.263775736093521, Final Batch Loss: 0.5229564905166626\n",
      "Epoch 2996, Loss: 2.0528944730758667, Final Batch Loss: 0.32995858788490295\n",
      "Epoch 2997, Loss: 2.0995755195617676, Final Batch Loss: 0.35642242431640625\n",
      "Epoch 2998, Loss: 2.087978959083557, Final Batch Loss: 0.5244114398956299\n",
      "Epoch 2999, Loss: 2.150868386030197, Final Batch Loss: 0.3851470351219177\n",
      "Epoch 3000, Loss: 2.231071025133133, Final Batch Loss: 0.4909800887107849\n",
      "Epoch 3001, Loss: 2.0083048045635223, Final Batch Loss: 0.36727792024612427\n",
      "Epoch 3002, Loss: 2.253515064716339, Final Batch Loss: 0.46922433376312256\n",
      "Epoch 3003, Loss: 2.2634376883506775, Final Batch Loss: 0.5586336851119995\n",
      "Epoch 3004, Loss: 2.03645458817482, Final Batch Loss: 0.40030938386917114\n",
      "Epoch 3005, Loss: 2.1612276136875153, Final Batch Loss: 0.44252991676330566\n",
      "Epoch 3006, Loss: 2.1259459853172302, Final Batch Loss: 0.47457456588745117\n",
      "Epoch 3007, Loss: 2.211937367916107, Final Batch Loss: 0.3714076578617096\n",
      "Epoch 3008, Loss: 2.0823213160037994, Final Batch Loss: 0.4118560552597046\n",
      "Epoch 3009, Loss: 2.071045011281967, Final Batch Loss: 0.33693259954452515\n",
      "Epoch 3010, Loss: 2.0220431089401245, Final Batch Loss: 0.35704249143600464\n",
      "Epoch 3011, Loss: 2.2379318475723267, Final Batch Loss: 0.4599730670452118\n",
      "Epoch 3012, Loss: 2.2168347239494324, Final Batch Loss: 0.3854692578315735\n",
      "Epoch 3013, Loss: 2.110906273126602, Final Batch Loss: 0.3862028419971466\n",
      "Epoch 3014, Loss: 2.328952193260193, Final Batch Loss: 0.512022078037262\n",
      "Epoch 3015, Loss: 2.202655464410782, Final Batch Loss: 0.5090239644050598\n",
      "Epoch 3016, Loss: 2.235148787498474, Final Batch Loss: 0.4633394181728363\n",
      "Epoch 3017, Loss: 2.084678828716278, Final Batch Loss: 0.3996778130531311\n",
      "Epoch 3018, Loss: 2.1344589591026306, Final Batch Loss: 0.4482213258743286\n",
      "Epoch 3019, Loss: 2.145616501569748, Final Batch Loss: 0.4977775812149048\n",
      "Epoch 3020, Loss: 2.192942887544632, Final Batch Loss: 0.4490070343017578\n",
      "Epoch 3021, Loss: 1.9824335873126984, Final Batch Loss: 0.36934325098991394\n",
      "Epoch 3022, Loss: 2.0977461338043213, Final Batch Loss: 0.35744619369506836\n",
      "Epoch 3023, Loss: 2.295667916536331, Final Batch Loss: 0.5306469202041626\n",
      "Epoch 3024, Loss: 2.1345852315425873, Final Batch Loss: 0.39321643114089966\n",
      "Epoch 3025, Loss: 2.005821466445923, Final Batch Loss: 0.39425960183143616\n",
      "Epoch 3026, Loss: 2.093244433403015, Final Batch Loss: 0.4769727289676666\n",
      "Epoch 3027, Loss: 1.9876071214675903, Final Batch Loss: 0.46116721630096436\n",
      "Epoch 3028, Loss: 2.240772843360901, Final Batch Loss: 0.5450698733329773\n",
      "Epoch 3029, Loss: 2.0779569149017334, Final Batch Loss: 0.414021760225296\n",
      "Epoch 3030, Loss: 2.258963257074356, Final Batch Loss: 0.5433181524276733\n",
      "Epoch 3031, Loss: 2.081913322210312, Final Batch Loss: 0.3991621732711792\n",
      "Epoch 3032, Loss: 2.1592675149440765, Final Batch Loss: 0.34584516286849976\n",
      "Epoch 3033, Loss: 2.083351045846939, Final Batch Loss: 0.4332157373428345\n",
      "Epoch 3034, Loss: 2.072602868080139, Final Batch Loss: 0.4293845295906067\n",
      "Epoch 3035, Loss: 2.11990824341774, Final Batch Loss: 0.4114503264427185\n",
      "Epoch 3036, Loss: 2.218666046857834, Final Batch Loss: 0.4181106388568878\n",
      "Epoch 3037, Loss: 2.1353901028633118, Final Batch Loss: 0.42681220173835754\n",
      "Epoch 3038, Loss: 2.0365488529205322, Final Batch Loss: 0.31920337677001953\n",
      "Epoch 3039, Loss: 2.1630205512046814, Final Batch Loss: 0.43403857946395874\n",
      "Epoch 3040, Loss: 1.994956225156784, Final Batch Loss: 0.401001900434494\n",
      "Epoch 3041, Loss: 2.038301944732666, Final Batch Loss: 0.4780346155166626\n",
      "Epoch 3042, Loss: 2.0663537085056305, Final Batch Loss: 0.44662684202194214\n",
      "Epoch 3043, Loss: 2.038613587617874, Final Batch Loss: 0.43972229957580566\n",
      "Epoch 3044, Loss: 2.1277213096618652, Final Batch Loss: 0.5845404863357544\n",
      "Epoch 3045, Loss: 1.9466960430145264, Final Batch Loss: 0.4316384196281433\n",
      "Epoch 3046, Loss: 2.0534328520298004, Final Batch Loss: 0.4043647050857544\n",
      "Epoch 3047, Loss: 2.1086595952510834, Final Batch Loss: 0.3533502519130707\n",
      "Epoch 3048, Loss: 2.0429337322711945, Final Batch Loss: 0.3657267093658447\n",
      "Epoch 3049, Loss: 2.039832979440689, Final Batch Loss: 0.42359477281570435\n",
      "Epoch 3050, Loss: 2.4035977721214294, Final Batch Loss: 0.341633141040802\n",
      "Epoch 3051, Loss: 2.09357613325119, Final Batch Loss: 0.41691434383392334\n",
      "Epoch 3052, Loss: 2.1548712253570557, Final Batch Loss: 0.40140122175216675\n",
      "Epoch 3053, Loss: 2.06841379404068, Final Batch Loss: 0.47282975912094116\n",
      "Epoch 3054, Loss: 2.219934046268463, Final Batch Loss: 0.43313106894493103\n",
      "Epoch 3055, Loss: 2.087977409362793, Final Batch Loss: 0.47202426195144653\n",
      "Epoch 3056, Loss: 2.053538680076599, Final Batch Loss: 0.4044993817806244\n",
      "Epoch 3057, Loss: 1.93653604388237, Final Batch Loss: 0.41908693313598633\n",
      "Epoch 3058, Loss: 2.134391039609909, Final Batch Loss: 0.4387637674808502\n",
      "Epoch 3059, Loss: 2.2166179418563843, Final Batch Loss: 0.5756485462188721\n",
      "Epoch 3060, Loss: 2.0865798890590668, Final Batch Loss: 0.4095918834209442\n",
      "Epoch 3061, Loss: 2.0329894423484802, Final Batch Loss: 0.39887282252311707\n",
      "Epoch 3062, Loss: 2.1015430986881256, Final Batch Loss: 0.42982345819473267\n",
      "Epoch 3063, Loss: 2.1961103081703186, Final Batch Loss: 0.396740198135376\n",
      "Epoch 3064, Loss: 2.1538142263889313, Final Batch Loss: 0.45700719952583313\n",
      "Epoch 3065, Loss: 1.988279104232788, Final Batch Loss: 0.42493388056755066\n",
      "Epoch 3066, Loss: 2.0237024426460266, Final Batch Loss: 0.40165477991104126\n",
      "Epoch 3067, Loss: 2.060840755701065, Final Batch Loss: 0.417632520198822\n",
      "Epoch 3068, Loss: 2.150015413761139, Final Batch Loss: 0.41583362221717834\n",
      "Epoch 3069, Loss: 2.1230312883853912, Final Batch Loss: 0.36415398120880127\n",
      "Epoch 3070, Loss: 2.353983461856842, Final Batch Loss: 0.5179702639579773\n",
      "Epoch 3071, Loss: 2.0871693193912506, Final Batch Loss: 0.48101237416267395\n",
      "Epoch 3072, Loss: 2.0449905693531036, Final Batch Loss: 0.2867955267429352\n",
      "Epoch 3073, Loss: 1.9626959562301636, Final Batch Loss: 0.40099674463272095\n",
      "Epoch 3074, Loss: 2.167574316263199, Final Batch Loss: 0.46008941531181335\n",
      "Epoch 3075, Loss: 1.9328162968158722, Final Batch Loss: 0.4287232756614685\n",
      "Epoch 3076, Loss: 2.0949583053588867, Final Batch Loss: 0.3889019191265106\n",
      "Epoch 3077, Loss: 2.0811719596385956, Final Batch Loss: 0.44196754693984985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3078, Loss: 2.2260603308677673, Final Batch Loss: 0.5127829909324646\n",
      "Epoch 3079, Loss: 2.2331565618515015, Final Batch Loss: 0.35834628343582153\n",
      "Epoch 3080, Loss: 2.3107339441776276, Final Batch Loss: 0.48924481868743896\n",
      "Epoch 3081, Loss: 2.1604557931423187, Final Batch Loss: 0.3503977656364441\n",
      "Epoch 3082, Loss: 2.1228055357933044, Final Batch Loss: 0.4338444769382477\n",
      "Epoch 3083, Loss: 2.0100085735321045, Final Batch Loss: 0.3575868308544159\n",
      "Epoch 3084, Loss: 1.9660585224628448, Final Batch Loss: 0.47341954708099365\n",
      "Epoch 3085, Loss: 2.134120225906372, Final Batch Loss: 0.5222039222717285\n",
      "Epoch 3086, Loss: 1.9957135319709778, Final Batch Loss: 0.37972140312194824\n",
      "Epoch 3087, Loss: 2.0079849660396576, Final Batch Loss: 0.45520874857902527\n",
      "Epoch 3088, Loss: 2.0228105783462524, Final Batch Loss: 0.4130310118198395\n",
      "Epoch 3089, Loss: 2.0621990263462067, Final Batch Loss: 0.44090771675109863\n",
      "Epoch 3090, Loss: 2.036148816347122, Final Batch Loss: 0.3794063329696655\n",
      "Epoch 3091, Loss: 2.023649722337723, Final Batch Loss: 0.31368082761764526\n",
      "Epoch 3092, Loss: 2.083160012960434, Final Batch Loss: 0.48947274684906006\n",
      "Epoch 3093, Loss: 2.1507679522037506, Final Batch Loss: 0.4349871873855591\n",
      "Epoch 3094, Loss: 2.2348901331424713, Final Batch Loss: 0.39345744252204895\n",
      "Epoch 3095, Loss: 2.235128730535507, Final Batch Loss: 0.4941100776195526\n",
      "Epoch 3096, Loss: 2.0864826440811157, Final Batch Loss: 0.3991929590702057\n",
      "Epoch 3097, Loss: 2.142035484313965, Final Batch Loss: 0.39755576848983765\n",
      "Epoch 3098, Loss: 2.086275041103363, Final Batch Loss: 0.3489309549331665\n",
      "Epoch 3099, Loss: 1.9876053035259247, Final Batch Loss: 0.3779948949813843\n",
      "Epoch 3100, Loss: 2.1236350536346436, Final Batch Loss: 0.4169420599937439\n",
      "Epoch 3101, Loss: 2.163679927587509, Final Batch Loss: 0.43440505862236023\n",
      "Epoch 3102, Loss: 2.1186031699180603, Final Batch Loss: 0.4250797927379608\n",
      "Epoch 3103, Loss: 2.0261223018169403, Final Batch Loss: 0.4959440231323242\n",
      "Epoch 3104, Loss: 1.9452206194400787, Final Batch Loss: 0.475956529378891\n",
      "Epoch 3105, Loss: 2.259613513946533, Final Batch Loss: 0.4665280878543854\n",
      "Epoch 3106, Loss: 2.1663180589675903, Final Batch Loss: 0.34743162989616394\n",
      "Epoch 3107, Loss: 2.0709403455257416, Final Batch Loss: 0.3172280192375183\n",
      "Epoch 3108, Loss: 2.266605317592621, Final Batch Loss: 0.43016645312309265\n",
      "Epoch 3109, Loss: 2.209461748600006, Final Batch Loss: 0.5104588866233826\n",
      "Epoch 3110, Loss: 2.088338702917099, Final Batch Loss: 0.438423752784729\n",
      "Epoch 3111, Loss: 2.096349388360977, Final Batch Loss: 0.461351603269577\n",
      "Epoch 3112, Loss: 2.0745208263397217, Final Batch Loss: 0.4312157928943634\n",
      "Epoch 3113, Loss: 2.1816879212856293, Final Batch Loss: 0.4028947055339813\n",
      "Epoch 3114, Loss: 1.975691705942154, Final Batch Loss: 0.4103824198246002\n",
      "Epoch 3115, Loss: 2.0894967913627625, Final Batch Loss: 0.4232555329799652\n",
      "Epoch 3116, Loss: 2.11019104719162, Final Batch Loss: 0.436962366104126\n",
      "Epoch 3117, Loss: 2.0566374361515045, Final Batch Loss: 0.4744091331958771\n",
      "Epoch 3118, Loss: 2.007857084274292, Final Batch Loss: 0.3443928360939026\n",
      "Epoch 3119, Loss: 2.032428413629532, Final Batch Loss: 0.42078647017478943\n",
      "Epoch 3120, Loss: 2.179674118757248, Final Batch Loss: 0.48603880405426025\n",
      "Epoch 3121, Loss: 2.0397473573684692, Final Batch Loss: 0.4446350038051605\n",
      "Epoch 3122, Loss: 2.117412507534027, Final Batch Loss: 0.453399658203125\n",
      "Epoch 3123, Loss: 1.9483125805854797, Final Batch Loss: 0.3680300712585449\n",
      "Epoch 3124, Loss: 2.09145450592041, Final Batch Loss: 0.39357757568359375\n",
      "Epoch 3125, Loss: 2.0424642860889435, Final Batch Loss: 0.38973546028137207\n",
      "Epoch 3126, Loss: 2.2065429985523224, Final Batch Loss: 0.47383570671081543\n",
      "Epoch 3127, Loss: 2.141379475593567, Final Batch Loss: 0.43323054909706116\n",
      "Epoch 3128, Loss: 2.060316890478134, Final Batch Loss: 0.4411699175834656\n",
      "Epoch 3129, Loss: 2.011671245098114, Final Batch Loss: 0.36939024925231934\n",
      "Epoch 3130, Loss: 1.9159305095672607, Final Batch Loss: 0.32007262110710144\n",
      "Epoch 3131, Loss: 1.955496072769165, Final Batch Loss: 0.4119010269641876\n",
      "Epoch 3132, Loss: 2.060395359992981, Final Batch Loss: 0.4106334149837494\n",
      "Epoch 3133, Loss: 2.029722899198532, Final Batch Loss: 0.46937525272369385\n",
      "Epoch 3134, Loss: 2.2118664979934692, Final Batch Loss: 0.4731031060218811\n",
      "Epoch 3135, Loss: 1.9651790261268616, Final Batch Loss: 0.4509103298187256\n",
      "Epoch 3136, Loss: 2.0512224435806274, Final Batch Loss: 0.31570759415626526\n",
      "Epoch 3137, Loss: 1.925325632095337, Final Batch Loss: 0.32314321398735046\n",
      "Epoch 3138, Loss: 2.0786977112293243, Final Batch Loss: 0.4415675103664398\n",
      "Epoch 3139, Loss: 2.0951158106327057, Final Batch Loss: 0.46256670355796814\n",
      "Epoch 3140, Loss: 1.9487595856189728, Final Batch Loss: 0.378298819065094\n",
      "Epoch 3141, Loss: 1.9578774869441986, Final Batch Loss: 0.41277801990509033\n",
      "Epoch 3142, Loss: 2.094737410545349, Final Batch Loss: 0.42413005232810974\n",
      "Epoch 3143, Loss: 2.032754749059677, Final Batch Loss: 0.33544474840164185\n",
      "Epoch 3144, Loss: 2.0019862055778503, Final Batch Loss: 0.36307844519615173\n",
      "Epoch 3145, Loss: 2.1034630239009857, Final Batch Loss: 0.403187096118927\n",
      "Epoch 3146, Loss: 2.206002175807953, Final Batch Loss: 0.546156108379364\n",
      "Epoch 3147, Loss: 1.989405781030655, Final Batch Loss: 0.5274391770362854\n",
      "Epoch 3148, Loss: 1.9993810057640076, Final Batch Loss: 0.5067266225814819\n",
      "Epoch 3149, Loss: 2.113992840051651, Final Batch Loss: 0.40976810455322266\n",
      "Epoch 3150, Loss: 2.0858495831489563, Final Batch Loss: 0.41659286618232727\n",
      "Epoch 3151, Loss: 2.113180309534073, Final Batch Loss: 0.5201941132545471\n",
      "Epoch 3152, Loss: 2.1357032358646393, Final Batch Loss: 0.40078070759773254\n",
      "Epoch 3153, Loss: 2.164119839668274, Final Batch Loss: 0.4522773027420044\n",
      "Epoch 3154, Loss: 2.026100665330887, Final Batch Loss: 0.4292965233325958\n",
      "Epoch 3155, Loss: 2.2082704305648804, Final Batch Loss: 0.5047807097434998\n",
      "Epoch 3156, Loss: 2.207186460494995, Final Batch Loss: 0.3692336082458496\n",
      "Epoch 3157, Loss: 2.039000302553177, Final Batch Loss: 0.3631948232650757\n",
      "Epoch 3158, Loss: 2.182274341583252, Final Batch Loss: 0.3959174156188965\n",
      "Epoch 3159, Loss: 2.1582745611667633, Final Batch Loss: 0.4421466290950775\n",
      "Epoch 3160, Loss: 2.0944226682186127, Final Batch Loss: 0.4921393394470215\n",
      "Epoch 3161, Loss: 2.142674148082733, Final Batch Loss: 0.3730892837047577\n",
      "Epoch 3162, Loss: 2.030438870191574, Final Batch Loss: 0.37108901143074036\n",
      "Epoch 3163, Loss: 2.0699044466018677, Final Batch Loss: 0.43343162536621094\n",
      "Epoch 3164, Loss: 2.043362706899643, Final Batch Loss: 0.4243183434009552\n",
      "Epoch 3165, Loss: 2.074645698070526, Final Batch Loss: 0.5671506524085999\n",
      "Epoch 3166, Loss: 1.9672762751579285, Final Batch Loss: 0.4069351255893707\n",
      "Epoch 3167, Loss: 2.0778614282608032, Final Batch Loss: 0.40386340022087097\n",
      "Epoch 3168, Loss: 2.055207908153534, Final Batch Loss: 0.4588659703731537\n",
      "Epoch 3169, Loss: 2.1186389923095703, Final Batch Loss: 0.39232316613197327\n",
      "Epoch 3170, Loss: 1.990235447883606, Final Batch Loss: 0.3420775532722473\n",
      "Epoch 3171, Loss: 2.1886923909187317, Final Batch Loss: 0.5967760682106018\n",
      "Epoch 3172, Loss: 1.991490364074707, Final Batch Loss: 0.433089017868042\n",
      "Epoch 3173, Loss: 1.9490486085414886, Final Batch Loss: 0.35144734382629395\n",
      "Epoch 3174, Loss: 2.1508606374263763, Final Batch Loss: 0.4535280466079712\n",
      "Epoch 3175, Loss: 2.0014857351779938, Final Batch Loss: 0.44051593542099\n",
      "Epoch 3176, Loss: 2.0925210416316986, Final Batch Loss: 0.3939741849899292\n",
      "Epoch 3177, Loss: 2.059768408536911, Final Batch Loss: 0.42272865772247314\n",
      "Epoch 3178, Loss: 2.021434098482132, Final Batch Loss: 0.39339226484298706\n",
      "Epoch 3179, Loss: 2.0738800168037415, Final Batch Loss: 0.3628455400466919\n",
      "Epoch 3180, Loss: 2.0403700470924377, Final Batch Loss: 0.3166472315788269\n",
      "Epoch 3181, Loss: 2.0991976261138916, Final Batch Loss: 0.47398439049720764\n",
      "Epoch 3182, Loss: 2.06354558467865, Final Batch Loss: 0.34825071692466736\n",
      "Epoch 3183, Loss: 2.2170694768428802, Final Batch Loss: 0.6053329110145569\n",
      "Epoch 3184, Loss: 1.9781995713710785, Final Batch Loss: 0.33931490778923035\n",
      "Epoch 3185, Loss: 2.0257145762443542, Final Batch Loss: 0.5050212740898132\n",
      "Epoch 3186, Loss: 2.147208720445633, Final Batch Loss: 0.4747847020626068\n",
      "Epoch 3187, Loss: 2.07631054520607, Final Batch Loss: 0.38400405645370483\n",
      "Epoch 3188, Loss: 2.084849625825882, Final Batch Loss: 0.41072145104408264\n",
      "Epoch 3189, Loss: 2.123184949159622, Final Batch Loss: 0.4104677438735962\n",
      "Epoch 3190, Loss: 1.9478482604026794, Final Batch Loss: 0.39675605297088623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3191, Loss: 2.1677196323871613, Final Batch Loss: 0.5061107277870178\n",
      "Epoch 3192, Loss: 1.9973106384277344, Final Batch Loss: 0.35805290937423706\n",
      "Epoch 3193, Loss: 1.9896712005138397, Final Batch Loss: 0.3538217544555664\n",
      "Epoch 3194, Loss: 2.0617640912532806, Final Batch Loss: 0.4767168462276459\n",
      "Epoch 3195, Loss: 1.893468588590622, Final Batch Loss: 0.45540526509284973\n",
      "Epoch 3196, Loss: 2.224110782146454, Final Batch Loss: 0.4621581733226776\n",
      "Epoch 3197, Loss: 1.9481765925884247, Final Batch Loss: 0.38989928364753723\n",
      "Epoch 3198, Loss: 2.156766504049301, Final Batch Loss: 0.46251770853996277\n",
      "Epoch 3199, Loss: 2.05002024769783, Final Batch Loss: 0.3913213610649109\n",
      "Epoch 3200, Loss: 2.1312916576862335, Final Batch Loss: 0.35645294189453125\n",
      "Epoch 3201, Loss: 1.9587113857269287, Final Batch Loss: 0.43380776047706604\n",
      "Epoch 3202, Loss: 2.0478895008563995, Final Batch Loss: 0.3606139123439789\n",
      "Epoch 3203, Loss: 2.214493006467819, Final Batch Loss: 0.5237584114074707\n",
      "Epoch 3204, Loss: 2.2304168045520782, Final Batch Loss: 0.4142170548439026\n",
      "Epoch 3205, Loss: 1.9886662065982819, Final Batch Loss: 0.3991856276988983\n",
      "Epoch 3206, Loss: 1.970080405473709, Final Batch Loss: 0.4078998267650604\n",
      "Epoch 3207, Loss: 2.047560304403305, Final Batch Loss: 0.40648311376571655\n",
      "Epoch 3208, Loss: 2.0111364126205444, Final Batch Loss: 0.3885795474052429\n",
      "Epoch 3209, Loss: 2.1637082695961, Final Batch Loss: 0.47971010208129883\n",
      "Epoch 3210, Loss: 2.0240170657634735, Final Batch Loss: 0.370974063873291\n",
      "Epoch 3211, Loss: 2.061568856239319, Final Batch Loss: 0.45619529485702515\n",
      "Epoch 3212, Loss: 2.298580437898636, Final Batch Loss: 0.4184078872203827\n",
      "Epoch 3213, Loss: 2.0265489518642426, Final Batch Loss: 0.3986951410770416\n",
      "Epoch 3214, Loss: 2.125454306602478, Final Batch Loss: 0.4210875332355499\n",
      "Epoch 3215, Loss: 2.0293527245521545, Final Batch Loss: 0.39539146423339844\n",
      "Epoch 3216, Loss: 1.9543766975402832, Final Batch Loss: 0.32394182682037354\n",
      "Epoch 3217, Loss: 2.1175354719161987, Final Batch Loss: 0.5464968085289001\n",
      "Epoch 3218, Loss: 1.9820193946361542, Final Batch Loss: 0.3795720338821411\n",
      "Epoch 3219, Loss: 2.0102207958698273, Final Batch Loss: 0.49262210726737976\n",
      "Epoch 3220, Loss: 2.0945399403572083, Final Batch Loss: 0.3976088762283325\n",
      "Epoch 3221, Loss: 2.0770790576934814, Final Batch Loss: 0.376482218503952\n",
      "Epoch 3222, Loss: 2.0385395288467407, Final Batch Loss: 0.3824869990348816\n",
      "Epoch 3223, Loss: 2.160629391670227, Final Batch Loss: 0.3751625120639801\n",
      "Epoch 3224, Loss: 2.0824784338474274, Final Batch Loss: 0.3953389525413513\n",
      "Epoch 3225, Loss: 2.1824145913124084, Final Batch Loss: 0.4175829589366913\n",
      "Epoch 3226, Loss: 2.05303892493248, Final Batch Loss: 0.32528531551361084\n",
      "Epoch 3227, Loss: 1.968628853559494, Final Batch Loss: 0.3717700242996216\n",
      "Epoch 3228, Loss: 2.1032171845436096, Final Batch Loss: 0.39637625217437744\n",
      "Epoch 3229, Loss: 2.043107956647873, Final Batch Loss: 0.4562722444534302\n",
      "Epoch 3230, Loss: 1.962998777627945, Final Batch Loss: 0.38915956020355225\n",
      "Epoch 3231, Loss: 2.051854521036148, Final Batch Loss: 0.474953293800354\n",
      "Epoch 3232, Loss: 2.2577398121356964, Final Batch Loss: 0.4337236285209656\n",
      "Epoch 3233, Loss: 2.0197170674800873, Final Batch Loss: 0.3717053234577179\n",
      "Epoch 3234, Loss: 1.9798943400382996, Final Batch Loss: 0.3919241726398468\n",
      "Epoch 3235, Loss: 2.0535390377044678, Final Batch Loss: 0.33287301659584045\n",
      "Epoch 3236, Loss: 1.9933505654335022, Final Batch Loss: 0.4145641326904297\n",
      "Epoch 3237, Loss: 1.9738079011440277, Final Batch Loss: 0.4950956702232361\n",
      "Epoch 3238, Loss: 1.9313299357891083, Final Batch Loss: 0.45487797260284424\n",
      "Epoch 3239, Loss: 2.0186141431331635, Final Batch Loss: 0.3548254072666168\n",
      "Epoch 3240, Loss: 2.156567633152008, Final Batch Loss: 0.2916709780693054\n",
      "Epoch 3241, Loss: 2.0143390595912933, Final Batch Loss: 0.42812860012054443\n",
      "Epoch 3242, Loss: 2.123460054397583, Final Batch Loss: 0.533317506313324\n",
      "Epoch 3243, Loss: 2.0116013884544373, Final Batch Loss: 0.44360676407814026\n",
      "Epoch 3244, Loss: 2.017671138048172, Final Batch Loss: 0.4473062753677368\n",
      "Epoch 3245, Loss: 2.124935567378998, Final Batch Loss: 0.44801220297813416\n",
      "Epoch 3246, Loss: 2.0645084977149963, Final Batch Loss: 0.4067637324333191\n",
      "Epoch 3247, Loss: 1.9844562113285065, Final Batch Loss: 0.4554938077926636\n",
      "Epoch 3248, Loss: 2.0897504687309265, Final Batch Loss: 0.4088499844074249\n",
      "Epoch 3249, Loss: 1.9071271419525146, Final Batch Loss: 0.3878719210624695\n",
      "Epoch 3250, Loss: 2.107392728328705, Final Batch Loss: 0.4506632387638092\n",
      "Epoch 3251, Loss: 2.2281933426856995, Final Batch Loss: 0.4463946223258972\n",
      "Epoch 3252, Loss: 2.250081390142441, Final Batch Loss: 0.4398953914642334\n",
      "Epoch 3253, Loss: 2.0778578519821167, Final Batch Loss: 0.3823462724685669\n",
      "Epoch 3254, Loss: 2.1668633222579956, Final Batch Loss: 0.40619567036628723\n",
      "Epoch 3255, Loss: 1.97796168923378, Final Batch Loss: 0.41148078441619873\n",
      "Epoch 3256, Loss: 2.0587507486343384, Final Batch Loss: 0.412875771522522\n",
      "Epoch 3257, Loss: 2.089544504880905, Final Batch Loss: 0.352155476808548\n",
      "Epoch 3258, Loss: 1.998174399137497, Final Batch Loss: 0.4207933247089386\n",
      "Epoch 3259, Loss: 2.1987554132938385, Final Batch Loss: 0.39048537611961365\n",
      "Epoch 3260, Loss: 2.031996339559555, Final Batch Loss: 0.37691083550453186\n",
      "Epoch 3261, Loss: 1.8928085267543793, Final Batch Loss: 0.42197296023368835\n",
      "Epoch 3262, Loss: 1.9992039799690247, Final Batch Loss: 0.43825075030326843\n",
      "Epoch 3263, Loss: 1.970873087644577, Final Batch Loss: 0.39834821224212646\n",
      "Epoch 3264, Loss: 2.096820831298828, Final Batch Loss: 0.44558361172676086\n",
      "Epoch 3265, Loss: 2.038960725069046, Final Batch Loss: 0.4849741756916046\n",
      "Epoch 3266, Loss: 1.9770514965057373, Final Batch Loss: 0.3859703540802002\n",
      "Epoch 3267, Loss: 2.0270639955997467, Final Batch Loss: 0.4131339192390442\n",
      "Epoch 3268, Loss: 2.1901439130306244, Final Batch Loss: 0.43631884455680847\n",
      "Epoch 3269, Loss: 1.9803102612495422, Final Batch Loss: 0.3991946280002594\n",
      "Epoch 3270, Loss: 2.0357289910316467, Final Batch Loss: 0.3894817531108856\n",
      "Epoch 3271, Loss: 1.9167760908603668, Final Batch Loss: 0.35586071014404297\n",
      "Epoch 3272, Loss: 2.002606689929962, Final Batch Loss: 0.42020025849342346\n",
      "Epoch 3273, Loss: 2.1054622530937195, Final Batch Loss: 0.42828452587127686\n",
      "Epoch 3274, Loss: 2.103655368089676, Final Batch Loss: 0.4725838303565979\n",
      "Epoch 3275, Loss: 2.175050050020218, Final Batch Loss: 0.4598816931247711\n",
      "Epoch 3276, Loss: 2.0970722436904907, Final Batch Loss: 0.48050570487976074\n",
      "Epoch 3277, Loss: 2.0307419896125793, Final Batch Loss: 0.41843852400779724\n",
      "Epoch 3278, Loss: 2.01472744345665, Final Batch Loss: 0.38103047013282776\n",
      "Epoch 3279, Loss: 2.3551836609840393, Final Batch Loss: 0.42249172925949097\n",
      "Epoch 3280, Loss: 2.0396724939346313, Final Batch Loss: 0.43113383650779724\n",
      "Epoch 3281, Loss: 2.0622273087501526, Final Batch Loss: 0.4449152648448944\n",
      "Epoch 3282, Loss: 2.0406107008457184, Final Batch Loss: 0.46922382712364197\n",
      "Epoch 3283, Loss: 2.1717070043087006, Final Batch Loss: 0.4210473299026489\n",
      "Epoch 3284, Loss: 2.091474086046219, Final Batch Loss: 0.39065754413604736\n",
      "Epoch 3285, Loss: 2.07950696349144, Final Batch Loss: 0.44435662031173706\n",
      "Epoch 3286, Loss: 1.9814057052135468, Final Batch Loss: 0.39442411065101624\n",
      "Epoch 3287, Loss: 1.9664853811264038, Final Batch Loss: 0.39130914211273193\n",
      "Epoch 3288, Loss: 2.114132672548294, Final Batch Loss: 0.39914247393608093\n",
      "Epoch 3289, Loss: 2.148063689470291, Final Batch Loss: 0.40966665744781494\n",
      "Epoch 3290, Loss: 2.1677847504615784, Final Batch Loss: 0.43337520956993103\n",
      "Epoch 3291, Loss: 2.1471216082572937, Final Batch Loss: 0.46344345808029175\n",
      "Epoch 3292, Loss: 2.071004569530487, Final Batch Loss: 0.3956773579120636\n",
      "Epoch 3293, Loss: 2.3451940417289734, Final Batch Loss: 0.5996109247207642\n",
      "Epoch 3294, Loss: 2.0734632909297943, Final Batch Loss: 0.43304526805877686\n",
      "Epoch 3295, Loss: 1.8419662714004517, Final Batch Loss: 0.3117099404335022\n",
      "Epoch 3296, Loss: 2.066254496574402, Final Batch Loss: 0.389168381690979\n",
      "Epoch 3297, Loss: 2.0528025031089783, Final Batch Loss: 0.43355047702789307\n",
      "Epoch 3298, Loss: 2.0478444695472717, Final Batch Loss: 0.3067947030067444\n",
      "Epoch 3299, Loss: 2.022036164999008, Final Batch Loss: 0.38462504744529724\n",
      "Epoch 3300, Loss: 2.1487411558628082, Final Batch Loss: 0.485012412071228\n",
      "Epoch 3301, Loss: 1.9395297765731812, Final Batch Loss: 0.335147500038147\n",
      "Epoch 3302, Loss: 2.0247232913970947, Final Batch Loss: 0.34463798999786377\n",
      "Epoch 3303, Loss: 2.088886082172394, Final Batch Loss: 0.40313589572906494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3304, Loss: 2.0062725245952606, Final Batch Loss: 0.3523823022842407\n",
      "Epoch 3305, Loss: 2.121240735054016, Final Batch Loss: 0.47582921385765076\n",
      "Epoch 3306, Loss: 1.8240675032138824, Final Batch Loss: 0.35458528995513916\n",
      "Epoch 3307, Loss: 2.124968081712723, Final Batch Loss: 0.3012285828590393\n",
      "Epoch 3308, Loss: 2.017645299434662, Final Batch Loss: 0.4154525399208069\n",
      "Epoch 3309, Loss: 1.9403935670852661, Final Batch Loss: 0.3438381850719452\n",
      "Epoch 3310, Loss: 2.068389892578125, Final Batch Loss: 0.42603909969329834\n",
      "Epoch 3311, Loss: 2.156112551689148, Final Batch Loss: 0.46314340829849243\n",
      "Epoch 3312, Loss: 1.9391209483146667, Final Batch Loss: 0.39926162362098694\n",
      "Epoch 3313, Loss: 2.008856922388077, Final Batch Loss: 0.4249594211578369\n",
      "Epoch 3314, Loss: 1.9989375174045563, Final Batch Loss: 0.4764590561389923\n",
      "Epoch 3315, Loss: 1.9611110985279083, Final Batch Loss: 0.398078590631485\n",
      "Epoch 3316, Loss: 2.0585760474205017, Final Batch Loss: 0.4447789490222931\n",
      "Epoch 3317, Loss: 1.9482662081718445, Final Batch Loss: 0.4580763280391693\n",
      "Epoch 3318, Loss: 2.1627635657787323, Final Batch Loss: 0.3967273235321045\n",
      "Epoch 3319, Loss: 2.0768111050128937, Final Batch Loss: 0.3751305639743805\n",
      "Epoch 3320, Loss: 2.0763980746269226, Final Batch Loss: 0.3942950665950775\n",
      "Epoch 3321, Loss: 2.151340574026108, Final Batch Loss: 0.4945089519023895\n",
      "Epoch 3322, Loss: 2.03859344124794, Final Batch Loss: 0.46815237402915955\n",
      "Epoch 3323, Loss: 2.315394550561905, Final Batch Loss: 0.4298053979873657\n",
      "Epoch 3324, Loss: 2.1608950197696686, Final Batch Loss: 0.39215123653411865\n",
      "Epoch 3325, Loss: 1.9899771511554718, Final Batch Loss: 0.40694186091423035\n",
      "Epoch 3326, Loss: 2.1553913056850433, Final Batch Loss: 0.37545841932296753\n",
      "Epoch 3327, Loss: 1.9007030129432678, Final Batch Loss: 0.4851644039154053\n",
      "Epoch 3328, Loss: 1.9798118770122528, Final Batch Loss: 0.33653995394706726\n",
      "Epoch 3329, Loss: 2.1467981040477753, Final Batch Loss: 0.36500105261802673\n",
      "Epoch 3330, Loss: 1.986094892024994, Final Batch Loss: 0.3539082109928131\n",
      "Epoch 3331, Loss: 2.1026148200035095, Final Batch Loss: 0.41950884461402893\n",
      "Epoch 3332, Loss: 2.124470055103302, Final Batch Loss: 0.4209693968296051\n",
      "Epoch 3333, Loss: 2.1612989604473114, Final Batch Loss: 0.5514212250709534\n",
      "Epoch 3334, Loss: 2.0868155658245087, Final Batch Loss: 0.496969997882843\n",
      "Epoch 3335, Loss: 1.941641241312027, Final Batch Loss: 0.3851442039012909\n",
      "Epoch 3336, Loss: 2.2172341644763947, Final Batch Loss: 0.5157825350761414\n",
      "Epoch 3337, Loss: 2.173167794942856, Final Batch Loss: 0.49728262424468994\n",
      "Epoch 3338, Loss: 1.9019824862480164, Final Batch Loss: 0.35580334067344666\n",
      "Epoch 3339, Loss: 2.241510570049286, Final Batch Loss: 0.44669851660728455\n",
      "Epoch 3340, Loss: 2.2163509726524353, Final Batch Loss: 0.44353175163269043\n",
      "Epoch 3341, Loss: 2.128898322582245, Final Batch Loss: 0.5723617076873779\n",
      "Epoch 3342, Loss: 2.2020675241947174, Final Batch Loss: 0.47038599848747253\n",
      "Epoch 3343, Loss: 1.898304432630539, Final Batch Loss: 0.339341402053833\n",
      "Epoch 3344, Loss: 1.9106067717075348, Final Batch Loss: 0.38301020860671997\n",
      "Epoch 3345, Loss: 2.1200315952301025, Final Batch Loss: 0.5648344159126282\n",
      "Epoch 3346, Loss: 2.0345877408981323, Final Batch Loss: 0.41434696316719055\n",
      "Epoch 3347, Loss: 2.0044190287590027, Final Batch Loss: 0.39426153898239136\n",
      "Epoch 3348, Loss: 2.22687366604805, Final Batch Loss: 0.4865935444831848\n",
      "Epoch 3349, Loss: 1.888917326927185, Final Batch Loss: 0.35210826992988586\n",
      "Epoch 3350, Loss: 1.9183334708213806, Final Batch Loss: 0.42712709307670593\n",
      "Epoch 3351, Loss: 2.1489259898662567, Final Batch Loss: 0.36595916748046875\n",
      "Epoch 3352, Loss: 1.9935033023357391, Final Batch Loss: 0.44992902874946594\n",
      "Epoch 3353, Loss: 2.0457917153835297, Final Batch Loss: 0.4056163728237152\n",
      "Epoch 3354, Loss: 2.0567036271095276, Final Batch Loss: 0.5022928714752197\n",
      "Epoch 3355, Loss: 1.9910044074058533, Final Batch Loss: 0.39451053738594055\n",
      "Epoch 3356, Loss: 2.188919246196747, Final Batch Loss: 0.47266462445259094\n",
      "Epoch 3357, Loss: 2.099840432405472, Final Batch Loss: 0.42808064818382263\n",
      "Epoch 3358, Loss: 1.9822112321853638, Final Batch Loss: 0.3126223385334015\n",
      "Epoch 3359, Loss: 2.127401649951935, Final Batch Loss: 0.47968241572380066\n",
      "Epoch 3360, Loss: 2.0755841732025146, Final Batch Loss: 0.42163822054862976\n",
      "Epoch 3361, Loss: 2.115680009126663, Final Batch Loss: 0.38018226623535156\n",
      "Epoch 3362, Loss: 2.0880833864212036, Final Batch Loss: 0.3059651851654053\n",
      "Epoch 3363, Loss: 2.172298103570938, Final Batch Loss: 0.5097920894622803\n",
      "Epoch 3364, Loss: 1.9656595885753632, Final Batch Loss: 0.48659542202949524\n",
      "Epoch 3365, Loss: 1.9630554020404816, Final Batch Loss: 0.4077279567718506\n",
      "Epoch 3366, Loss: 2.100134938955307, Final Batch Loss: 0.43149030208587646\n",
      "Epoch 3367, Loss: 2.1559612452983856, Final Batch Loss: 0.4178934693336487\n",
      "Epoch 3368, Loss: 2.016024261713028, Final Batch Loss: 0.42903897166252136\n",
      "Epoch 3369, Loss: 1.9805980026721954, Final Batch Loss: 0.4138967990875244\n",
      "Epoch 3370, Loss: 2.051191747188568, Final Batch Loss: 0.43984970450401306\n",
      "Epoch 3371, Loss: 2.149640381336212, Final Batch Loss: 0.499691367149353\n",
      "Epoch 3372, Loss: 1.950283408164978, Final Batch Loss: 0.38163745403289795\n",
      "Epoch 3373, Loss: 2.025944322347641, Final Batch Loss: 0.4185812771320343\n",
      "Epoch 3374, Loss: 2.183531403541565, Final Batch Loss: 0.3795122802257538\n",
      "Epoch 3375, Loss: 1.8659295439720154, Final Batch Loss: 0.48232507705688477\n",
      "Epoch 3376, Loss: 1.9808834493160248, Final Batch Loss: 0.3519342541694641\n",
      "Epoch 3377, Loss: 1.9659554958343506, Final Batch Loss: 0.34527722001075745\n",
      "Epoch 3378, Loss: 2.123240113258362, Final Batch Loss: 0.33714258670806885\n",
      "Epoch 3379, Loss: 2.0564824640750885, Final Batch Loss: 0.3819088339805603\n",
      "Epoch 3380, Loss: 2.1713812053203583, Final Batch Loss: 0.4583606719970703\n",
      "Epoch 3381, Loss: 1.8824017941951752, Final Batch Loss: 0.38599592447280884\n",
      "Epoch 3382, Loss: 1.9974189698696136, Final Batch Loss: 0.3432283103466034\n",
      "Epoch 3383, Loss: 1.931997001171112, Final Batch Loss: 0.3574185073375702\n",
      "Epoch 3384, Loss: 1.931648463010788, Final Batch Loss: 0.3521643877029419\n",
      "Epoch 3385, Loss: 1.9916881024837494, Final Batch Loss: 0.3554669916629791\n",
      "Epoch 3386, Loss: 1.9255492389202118, Final Batch Loss: 0.358439564704895\n",
      "Epoch 3387, Loss: 1.996900200843811, Final Batch Loss: 0.4546688199043274\n",
      "Epoch 3388, Loss: 1.7378436625003815, Final Batch Loss: 0.3592756986618042\n",
      "Epoch 3389, Loss: 2.029653310775757, Final Batch Loss: 0.5180730223655701\n",
      "Epoch 3390, Loss: 2.1939397156238556, Final Batch Loss: 0.46985939145088196\n",
      "Epoch 3391, Loss: 1.9181463420391083, Final Batch Loss: 0.3678186535835266\n",
      "Epoch 3392, Loss: 2.064676135778427, Final Batch Loss: 0.3926450312137604\n",
      "Epoch 3393, Loss: 1.96849325299263, Final Batch Loss: 0.37994149327278137\n",
      "Epoch 3394, Loss: 1.9881248474121094, Final Batch Loss: 0.42786696553230286\n",
      "Epoch 3395, Loss: 2.086268037557602, Final Batch Loss: 0.41520652174949646\n",
      "Epoch 3396, Loss: 2.1058693528175354, Final Batch Loss: 0.39861536026000977\n",
      "Epoch 3397, Loss: 1.946012556552887, Final Batch Loss: 0.4349280297756195\n",
      "Epoch 3398, Loss: 2.0090120136737823, Final Batch Loss: 0.3923681676387787\n",
      "Epoch 3399, Loss: 1.8624103665351868, Final Batch Loss: 0.3424750864505768\n",
      "Epoch 3400, Loss: 2.1242571473121643, Final Batch Loss: 0.43738362193107605\n",
      "Epoch 3401, Loss: 2.0535521507263184, Final Batch Loss: 0.3187274634838104\n",
      "Epoch 3402, Loss: 2.058139979839325, Final Batch Loss: 0.35701027512550354\n",
      "Epoch 3403, Loss: 1.8963743150234222, Final Batch Loss: 0.3634130656719208\n",
      "Epoch 3404, Loss: 2.023804008960724, Final Batch Loss: 0.46766185760498047\n",
      "Epoch 3405, Loss: 1.9433861374855042, Final Batch Loss: 0.36502134799957275\n",
      "Epoch 3406, Loss: 1.9772266149520874, Final Batch Loss: 0.44400399923324585\n",
      "Epoch 3407, Loss: 1.8679084479808807, Final Batch Loss: 0.36818498373031616\n",
      "Epoch 3408, Loss: 2.060565561056137, Final Batch Loss: 0.40108901262283325\n",
      "Epoch 3409, Loss: 2.0397708117961884, Final Batch Loss: 0.4050436019897461\n",
      "Epoch 3410, Loss: 1.9802564978599548, Final Batch Loss: 0.35579943656921387\n",
      "Epoch 3411, Loss: 2.1303537786006927, Final Batch Loss: 0.4153248369693756\n",
      "Epoch 3412, Loss: 2.1359810829162598, Final Batch Loss: 0.37608301639556885\n",
      "Epoch 3413, Loss: 2.0698212683200836, Final Batch Loss: 0.4284077286720276\n",
      "Epoch 3414, Loss: 1.9007535874843597, Final Batch Loss: 0.31482070684432983\n",
      "Epoch 3415, Loss: 2.0981617867946625, Final Batch Loss: 0.45449021458625793\n",
      "Epoch 3416, Loss: 1.9817358255386353, Final Batch Loss: 0.3806297183036804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3417, Loss: 2.0616028010845184, Final Batch Loss: 0.4826713800430298\n",
      "Epoch 3418, Loss: 2.0329433381557465, Final Batch Loss: 0.4321809411048889\n",
      "Epoch 3419, Loss: 1.833776831626892, Final Batch Loss: 0.35615840554237366\n",
      "Epoch 3420, Loss: 2.001224935054779, Final Batch Loss: 0.35244590044021606\n",
      "Epoch 3421, Loss: 1.8867455422878265, Final Batch Loss: 0.3478201627731323\n",
      "Epoch 3422, Loss: 2.1443809270858765, Final Batch Loss: 0.4419761002063751\n",
      "Epoch 3423, Loss: 1.9890080988407135, Final Batch Loss: 0.3639299273490906\n",
      "Epoch 3424, Loss: 1.9194526374340057, Final Batch Loss: 0.5300908088684082\n",
      "Epoch 3425, Loss: 2.005830079317093, Final Batch Loss: 0.41439753770828247\n",
      "Epoch 3426, Loss: 2.212896943092346, Final Batch Loss: 0.4350343346595764\n",
      "Epoch 3427, Loss: 2.0684987604618073, Final Batch Loss: 0.4133320152759552\n",
      "Epoch 3428, Loss: 1.9564110338687897, Final Batch Loss: 0.36387068033218384\n",
      "Epoch 3429, Loss: 2.0763314962387085, Final Batch Loss: 0.4308389723300934\n",
      "Epoch 3430, Loss: 2.0527763068675995, Final Batch Loss: 0.34246984124183655\n",
      "Epoch 3431, Loss: 1.965168684720993, Final Batch Loss: 0.3548261523246765\n",
      "Epoch 3432, Loss: 1.933957725763321, Final Batch Loss: 0.3634275197982788\n",
      "Epoch 3433, Loss: 2.0692566335201263, Final Batch Loss: 0.5102311372756958\n",
      "Epoch 3434, Loss: 2.072681337594986, Final Batch Loss: 0.45703092217445374\n",
      "Epoch 3435, Loss: 2.0812089145183563, Final Batch Loss: 0.3476673662662506\n",
      "Epoch 3436, Loss: 1.9385587573051453, Final Batch Loss: 0.3461204767227173\n",
      "Epoch 3437, Loss: 2.3365436792373657, Final Batch Loss: 0.47052866220474243\n",
      "Epoch 3438, Loss: 1.8986689448356628, Final Batch Loss: 0.32520243525505066\n",
      "Epoch 3439, Loss: 2.0909854769706726, Final Batch Loss: 0.42866063117980957\n",
      "Epoch 3440, Loss: 2.010512590408325, Final Batch Loss: 0.38367515802383423\n",
      "Epoch 3441, Loss: 2.0478544533252716, Final Batch Loss: 0.4311472475528717\n",
      "Epoch 3442, Loss: 1.9506613910198212, Final Batch Loss: 0.3680741488933563\n",
      "Epoch 3443, Loss: 2.028654009103775, Final Batch Loss: 0.3827389180660248\n",
      "Epoch 3444, Loss: 2.1372845470905304, Final Batch Loss: 0.41895532608032227\n",
      "Epoch 3445, Loss: 1.8643819391727448, Final Batch Loss: 0.3752569258213043\n",
      "Epoch 3446, Loss: 1.9723751246929169, Final Batch Loss: 0.384515643119812\n",
      "Epoch 3447, Loss: 2.030590921640396, Final Batch Loss: 0.3927711248397827\n",
      "Epoch 3448, Loss: 1.995858073234558, Final Batch Loss: 0.4830193817615509\n",
      "Epoch 3449, Loss: 2.045228123664856, Final Batch Loss: 0.32340115308761597\n",
      "Epoch 3450, Loss: 2.0230912566184998, Final Batch Loss: 0.4477115273475647\n",
      "Epoch 3451, Loss: 1.9331772923469543, Final Batch Loss: 0.32464390993118286\n",
      "Epoch 3452, Loss: 1.9377658069133759, Final Batch Loss: 0.37123119831085205\n",
      "Epoch 3453, Loss: 1.8258828520774841, Final Batch Loss: 0.38234013319015503\n",
      "Epoch 3454, Loss: 1.897265374660492, Final Batch Loss: 0.36329448223114014\n",
      "Epoch 3455, Loss: 1.8705950379371643, Final Batch Loss: 0.297089159488678\n",
      "Epoch 3456, Loss: 2.3141908943653107, Final Batch Loss: 0.531416118144989\n",
      "Epoch 3457, Loss: 2.0724853575229645, Final Batch Loss: 0.4866347014904022\n",
      "Epoch 3458, Loss: 1.998429000377655, Final Batch Loss: 0.35195842385292053\n",
      "Epoch 3459, Loss: 1.9394077956676483, Final Batch Loss: 0.43146052956581116\n",
      "Epoch 3460, Loss: 2.0402332544326782, Final Batch Loss: 0.3417155146598816\n",
      "Epoch 3461, Loss: 2.00099378824234, Final Batch Loss: 0.48034656047821045\n",
      "Epoch 3462, Loss: 1.9724791049957275, Final Batch Loss: 0.42188066244125366\n",
      "Epoch 3463, Loss: 2.110618442296982, Final Batch Loss: 0.3926438093185425\n",
      "Epoch 3464, Loss: 1.9820622503757477, Final Batch Loss: 0.3796854019165039\n",
      "Epoch 3465, Loss: 2.0755899846553802, Final Batch Loss: 0.40681543946266174\n",
      "Epoch 3466, Loss: 2.201836585998535, Final Batch Loss: 0.4339785575866699\n",
      "Epoch 3467, Loss: 2.009493887424469, Final Batch Loss: 0.3466963768005371\n",
      "Epoch 3468, Loss: 2.1304678916931152, Final Batch Loss: 0.4131258428096771\n",
      "Epoch 3469, Loss: 1.9839839935302734, Final Batch Loss: 0.45413604378700256\n",
      "Epoch 3470, Loss: 2.139838010072708, Final Batch Loss: 0.442619264125824\n",
      "Epoch 3471, Loss: 1.967619150876999, Final Batch Loss: 0.3574790060520172\n",
      "Epoch 3472, Loss: 2.067664235830307, Final Batch Loss: 0.5238562822341919\n",
      "Epoch 3473, Loss: 1.9627721905708313, Final Batch Loss: 0.3756362497806549\n",
      "Epoch 3474, Loss: 2.1191336810588837, Final Batch Loss: 0.42298316955566406\n",
      "Epoch 3475, Loss: 2.0532646775245667, Final Batch Loss: 0.39006054401397705\n",
      "Epoch 3476, Loss: 1.837540179491043, Final Batch Loss: 0.35295945405960083\n",
      "Epoch 3477, Loss: 1.863157719373703, Final Batch Loss: 0.3754860758781433\n",
      "Epoch 3478, Loss: 1.7874185740947723, Final Batch Loss: 0.3369484841823578\n",
      "Epoch 3479, Loss: 2.140991061925888, Final Batch Loss: 0.42618876695632935\n",
      "Epoch 3480, Loss: 2.219389855861664, Final Batch Loss: 0.49175894260406494\n",
      "Epoch 3481, Loss: 1.8830027282238007, Final Batch Loss: 0.44242724776268005\n",
      "Epoch 3482, Loss: 1.989583045244217, Final Batch Loss: 0.4033762514591217\n",
      "Epoch 3483, Loss: 1.9456420838832855, Final Batch Loss: 0.42434948682785034\n",
      "Epoch 3484, Loss: 2.1953761279582977, Final Batch Loss: 0.5174705982208252\n",
      "Epoch 3485, Loss: 2.0825238823890686, Final Batch Loss: 0.42097264528274536\n",
      "Epoch 3486, Loss: 2.038266122341156, Final Batch Loss: 0.43549561500549316\n",
      "Epoch 3487, Loss: 2.109111100435257, Final Batch Loss: 0.3151131868362427\n",
      "Epoch 3488, Loss: 2.0207631587982178, Final Batch Loss: 0.39162078499794006\n",
      "Epoch 3489, Loss: 1.989044576883316, Final Batch Loss: 0.44813185930252075\n",
      "Epoch 3490, Loss: 2.0051559805870056, Final Batch Loss: 0.37978288531303406\n",
      "Epoch 3491, Loss: 2.1954020261764526, Final Batch Loss: 0.45429331064224243\n",
      "Epoch 3492, Loss: 2.1505913138389587, Final Batch Loss: 0.4237039089202881\n",
      "Epoch 3493, Loss: 2.047398626804352, Final Batch Loss: 0.4793672561645508\n",
      "Epoch 3494, Loss: 2.0861619114875793, Final Batch Loss: 0.4602336585521698\n",
      "Epoch 3495, Loss: 2.005539983510971, Final Batch Loss: 0.35355669260025024\n",
      "Epoch 3496, Loss: 2.0726570785045624, Final Batch Loss: 0.39426857233047485\n",
      "Epoch 3497, Loss: 2.1106573343276978, Final Batch Loss: 0.47345641255378723\n",
      "Epoch 3498, Loss: 1.903040885925293, Final Batch Loss: 0.40942972898483276\n",
      "Epoch 3499, Loss: 1.9875282943248749, Final Batch Loss: 0.46996158361434937\n",
      "Epoch 3500, Loss: 2.0253196954727173, Final Batch Loss: 0.30289387702941895\n",
      "Epoch 3501, Loss: 2.1973663568496704, Final Batch Loss: 0.5044063925743103\n",
      "Epoch 3502, Loss: 2.0099596679210663, Final Batch Loss: 0.39942723512649536\n",
      "Epoch 3503, Loss: 1.9507428109645844, Final Batch Loss: 0.39002519845962524\n",
      "Epoch 3504, Loss: 1.9732457101345062, Final Batch Loss: 0.43574413657188416\n",
      "Epoch 3505, Loss: 1.8932505249977112, Final Batch Loss: 0.3314948081970215\n",
      "Epoch 3506, Loss: 1.9530003368854523, Final Batch Loss: 0.420945942401886\n",
      "Epoch 3507, Loss: 1.9786077439785004, Final Batch Loss: 0.41530370712280273\n",
      "Epoch 3508, Loss: 1.9184572398662567, Final Batch Loss: 0.31621408462524414\n",
      "Epoch 3509, Loss: 2.007042557001114, Final Batch Loss: 0.3661954700946808\n",
      "Epoch 3510, Loss: 1.931543081998825, Final Batch Loss: 0.408096045255661\n",
      "Epoch 3511, Loss: 2.2433795630931854, Final Batch Loss: 0.5466330051422119\n",
      "Epoch 3512, Loss: 1.9776600897312164, Final Batch Loss: 0.39349865913391113\n",
      "Epoch 3513, Loss: 2.0134919583797455, Final Batch Loss: 0.4842546880245209\n",
      "Epoch 3514, Loss: 2.2263154089450836, Final Batch Loss: 0.3688676953315735\n",
      "Epoch 3515, Loss: 2.0876316130161285, Final Batch Loss: 0.29745960235595703\n",
      "Epoch 3516, Loss: 2.0273450911045074, Final Batch Loss: 0.3510115444660187\n",
      "Epoch 3517, Loss: 1.8915792107582092, Final Batch Loss: 0.34772464632987976\n",
      "Epoch 3518, Loss: 1.9880266189575195, Final Batch Loss: 0.43486452102661133\n",
      "Epoch 3519, Loss: 1.8653717339038849, Final Batch Loss: 0.366564005613327\n",
      "Epoch 3520, Loss: 1.7937245965003967, Final Batch Loss: 0.27615422010421753\n",
      "Epoch 3521, Loss: 1.998253345489502, Final Batch Loss: 0.4005979299545288\n",
      "Epoch 3522, Loss: 1.9503026902675629, Final Batch Loss: 0.38060271739959717\n",
      "Epoch 3523, Loss: 1.9622335135936737, Final Batch Loss: 0.4391891062259674\n",
      "Epoch 3524, Loss: 1.9289602637290955, Final Batch Loss: 0.37813761830329895\n",
      "Epoch 3525, Loss: 2.0542694330215454, Final Batch Loss: 0.45230355858802795\n",
      "Epoch 3526, Loss: 2.009891599416733, Final Batch Loss: 0.4060947895050049\n",
      "Epoch 3527, Loss: 1.9418046176433563, Final Batch Loss: 0.3414144217967987\n",
      "Epoch 3528, Loss: 1.8907632231712341, Final Batch Loss: 0.4010217487812042\n",
      "Epoch 3529, Loss: 1.9026223421096802, Final Batch Loss: 0.3072878122329712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3530, Loss: 2.0256710946559906, Final Batch Loss: 0.34153059124946594\n",
      "Epoch 3531, Loss: 1.7823973596096039, Final Batch Loss: 0.3330482542514801\n",
      "Epoch 3532, Loss: 1.991626650094986, Final Batch Loss: 0.4172455072402954\n",
      "Epoch 3533, Loss: 1.907781720161438, Final Batch Loss: 0.39596325159072876\n",
      "Epoch 3534, Loss: 1.9662539660930634, Final Batch Loss: 0.41884127259254456\n",
      "Epoch 3535, Loss: 1.9653243124485016, Final Batch Loss: 0.33537647128105164\n",
      "Epoch 3536, Loss: 1.7974806427955627, Final Batch Loss: 0.40191957354545593\n",
      "Epoch 3537, Loss: 1.882328450679779, Final Batch Loss: 0.3507050573825836\n",
      "Epoch 3538, Loss: 1.961331158876419, Final Batch Loss: 0.39647942781448364\n",
      "Epoch 3539, Loss: 1.9794562757015228, Final Batch Loss: 0.4624377489089966\n",
      "Epoch 3540, Loss: 2.0854902267456055, Final Batch Loss: 0.484613835811615\n",
      "Epoch 3541, Loss: 2.157954603433609, Final Batch Loss: 0.4291006028652191\n",
      "Epoch 3542, Loss: 2.1649856567382812, Final Batch Loss: 0.46755948662757874\n",
      "Epoch 3543, Loss: 1.876452088356018, Final Batch Loss: 0.3603666424751282\n",
      "Epoch 3544, Loss: 1.9724387526512146, Final Batch Loss: 0.35982781648635864\n",
      "Epoch 3545, Loss: 2.099099338054657, Final Batch Loss: 0.3597026765346527\n",
      "Epoch 3546, Loss: 1.9209509789943695, Final Batch Loss: 0.44141897559165955\n",
      "Epoch 3547, Loss: 2.1019144356250763, Final Batch Loss: 0.3910030722618103\n",
      "Epoch 3548, Loss: 1.9140073955059052, Final Batch Loss: 0.42030972242355347\n",
      "Epoch 3549, Loss: 1.9923319816589355, Final Batch Loss: 0.4316529333591461\n",
      "Epoch 3550, Loss: 2.035367727279663, Final Batch Loss: 0.4435105323791504\n",
      "Epoch 3551, Loss: 1.991193413734436, Final Batch Loss: 0.3880334198474884\n",
      "Epoch 3552, Loss: 1.988806277513504, Final Batch Loss: 0.36029696464538574\n",
      "Epoch 3553, Loss: 2.168456017971039, Final Batch Loss: 0.3444662094116211\n",
      "Epoch 3554, Loss: 1.9993753135204315, Final Batch Loss: 0.43702757358551025\n",
      "Epoch 3555, Loss: 1.8189659118652344, Final Batch Loss: 0.3368178904056549\n",
      "Epoch 3556, Loss: 1.9525876343250275, Final Batch Loss: 0.4005289077758789\n",
      "Epoch 3557, Loss: 2.0292175114154816, Final Batch Loss: 0.43376943469047546\n",
      "Epoch 3558, Loss: 1.9560176134109497, Final Batch Loss: 0.3836737871170044\n",
      "Epoch 3559, Loss: 1.933646023273468, Final Batch Loss: 0.362149715423584\n",
      "Epoch 3560, Loss: 2.080420821905136, Final Batch Loss: 0.4232298731803894\n",
      "Epoch 3561, Loss: 1.8223020434379578, Final Batch Loss: 0.33706218004226685\n",
      "Epoch 3562, Loss: 2.009152889251709, Final Batch Loss: 0.4136435389518738\n",
      "Epoch 3563, Loss: 2.155242681503296, Final Batch Loss: 0.47380489110946655\n",
      "Epoch 3564, Loss: 1.9118363559246063, Final Batch Loss: 0.27449432015419006\n",
      "Epoch 3565, Loss: 2.101612150669098, Final Batch Loss: 0.4064452350139618\n",
      "Epoch 3566, Loss: 1.8604525327682495, Final Batch Loss: 0.3659099340438843\n",
      "Epoch 3567, Loss: 1.828455239534378, Final Batch Loss: 0.3629797101020813\n",
      "Epoch 3568, Loss: 1.93922159075737, Final Batch Loss: 0.4004664123058319\n",
      "Epoch 3569, Loss: 1.93596151471138, Final Batch Loss: 0.36864355206489563\n",
      "Epoch 3570, Loss: 2.012088805437088, Final Batch Loss: 0.3719732165336609\n",
      "Epoch 3571, Loss: 1.998820185661316, Final Batch Loss: 0.408287912607193\n",
      "Epoch 3572, Loss: 1.8850036561489105, Final Batch Loss: 0.3814539313316345\n",
      "Epoch 3573, Loss: 1.9064351618289948, Final Batch Loss: 0.36230647563934326\n",
      "Epoch 3574, Loss: 2.031822293996811, Final Batch Loss: 0.32581794261932373\n",
      "Epoch 3575, Loss: 2.0733860433101654, Final Batch Loss: 0.45668044686317444\n",
      "Epoch 3576, Loss: 2.275268793106079, Final Batch Loss: 0.41656768321990967\n",
      "Epoch 3577, Loss: 1.8223816454410553, Final Batch Loss: 0.2899828255176544\n",
      "Epoch 3578, Loss: 2.0816700160503387, Final Batch Loss: 0.46233275532722473\n",
      "Epoch 3579, Loss: 1.9711676836013794, Final Batch Loss: 0.34359484910964966\n",
      "Epoch 3580, Loss: 1.8674514293670654, Final Batch Loss: 0.37993863224983215\n",
      "Epoch 3581, Loss: 1.9779991507530212, Final Batch Loss: 0.42868900299072266\n",
      "Epoch 3582, Loss: 2.148717910051346, Final Batch Loss: 0.5085695385932922\n",
      "Epoch 3583, Loss: 2.1692389249801636, Final Batch Loss: 0.385964035987854\n",
      "Epoch 3584, Loss: 1.944040596485138, Final Batch Loss: 0.45431405305862427\n",
      "Epoch 3585, Loss: 1.9022715091705322, Final Batch Loss: 0.3886665105819702\n",
      "Epoch 3586, Loss: 1.8428501188755035, Final Batch Loss: 0.3660021424293518\n",
      "Epoch 3587, Loss: 1.8357056379318237, Final Batch Loss: 0.39273449778556824\n",
      "Epoch 3588, Loss: 1.838505893945694, Final Batch Loss: 0.4796287417411804\n",
      "Epoch 3589, Loss: 1.9146907329559326, Final Batch Loss: 0.366279661655426\n",
      "Epoch 3590, Loss: 1.9991100430488586, Final Batch Loss: 0.3960144817829132\n",
      "Epoch 3591, Loss: 1.979576289653778, Final Batch Loss: 0.4204687774181366\n",
      "Epoch 3592, Loss: 1.9532169103622437, Final Batch Loss: 0.3461221158504486\n",
      "Epoch 3593, Loss: 2.0865559577941895, Final Batch Loss: 0.47623148560523987\n",
      "Epoch 3594, Loss: 1.9951246976852417, Final Batch Loss: 0.349141001701355\n",
      "Epoch 3595, Loss: 1.951260894536972, Final Batch Loss: 0.3756999373435974\n",
      "Epoch 3596, Loss: 1.8619384169578552, Final Batch Loss: 0.39454585313796997\n",
      "Epoch 3597, Loss: 1.9979251027107239, Final Batch Loss: 0.38767901062965393\n",
      "Epoch 3598, Loss: 1.981916218996048, Final Batch Loss: 0.34860891103744507\n",
      "Epoch 3599, Loss: 2.1509528160095215, Final Batch Loss: 0.504102349281311\n",
      "Epoch 3600, Loss: 2.0515851080417633, Final Batch Loss: 0.3850993514060974\n",
      "Epoch 3601, Loss: 1.9103272557258606, Final Batch Loss: 0.3656652867794037\n",
      "Epoch 3602, Loss: 1.8859313428401947, Final Batch Loss: 0.4125462770462036\n",
      "Epoch 3603, Loss: 1.8450821340084076, Final Batch Loss: 0.3558378517627716\n",
      "Epoch 3604, Loss: 2.0584945678710938, Final Batch Loss: 0.4572627544403076\n",
      "Epoch 3605, Loss: 1.9256934225559235, Final Batch Loss: 0.3656458556652069\n",
      "Epoch 3606, Loss: 1.779583603143692, Final Batch Loss: 0.344546914100647\n",
      "Epoch 3607, Loss: 1.9200166761875153, Final Batch Loss: 0.33933818340301514\n",
      "Epoch 3608, Loss: 1.9352850914001465, Final Batch Loss: 0.40257519483566284\n",
      "Epoch 3609, Loss: 2.0083708465099335, Final Batch Loss: 0.4543294608592987\n",
      "Epoch 3610, Loss: 1.9774237871170044, Final Batch Loss: 0.2975168824195862\n",
      "Epoch 3611, Loss: 2.0520790219306946, Final Batch Loss: 0.39612242579460144\n",
      "Epoch 3612, Loss: 1.8456483781337738, Final Batch Loss: 0.3630836009979248\n",
      "Epoch 3613, Loss: 1.8324432671070099, Final Batch Loss: 0.29434844851493835\n",
      "Epoch 3614, Loss: 1.790775865316391, Final Batch Loss: 0.3904242217540741\n",
      "Epoch 3615, Loss: 1.8384839594364166, Final Batch Loss: 0.28839120268821716\n",
      "Epoch 3616, Loss: 1.8089838922023773, Final Batch Loss: 0.3457195460796356\n",
      "Epoch 3617, Loss: 2.2433836460113525, Final Batch Loss: 0.4674668312072754\n",
      "Epoch 3618, Loss: 2.077364534139633, Final Batch Loss: 0.48878008127212524\n",
      "Epoch 3619, Loss: 2.0258868038654327, Final Batch Loss: 0.41629981994628906\n",
      "Epoch 3620, Loss: 2.0313564240932465, Final Batch Loss: 0.42097631096839905\n",
      "Epoch 3621, Loss: 1.9450885355472565, Final Batch Loss: 0.415237694978714\n",
      "Epoch 3622, Loss: 1.9301941990852356, Final Batch Loss: 0.43917572498321533\n",
      "Epoch 3623, Loss: 1.9889526069164276, Final Batch Loss: 0.39472702145576477\n",
      "Epoch 3624, Loss: 1.9669063687324524, Final Batch Loss: 0.3806080222129822\n",
      "Epoch 3625, Loss: 2.0050794184207916, Final Batch Loss: 0.3441181480884552\n",
      "Epoch 3626, Loss: 2.1324367225170135, Final Batch Loss: 0.385503351688385\n",
      "Epoch 3627, Loss: 2.0981347262859344, Final Batch Loss: 0.5002624988555908\n",
      "Epoch 3628, Loss: 2.14789280295372, Final Batch Loss: 0.459983766078949\n",
      "Epoch 3629, Loss: 1.9721927642822266, Final Batch Loss: 0.4130794107913971\n",
      "Epoch 3630, Loss: 1.918024629354477, Final Batch Loss: 0.3665146231651306\n",
      "Epoch 3631, Loss: 1.8391029834747314, Final Batch Loss: 0.3408358097076416\n",
      "Epoch 3632, Loss: 1.8215526938438416, Final Batch Loss: 0.4311716556549072\n",
      "Epoch 3633, Loss: 2.0225205421447754, Final Batch Loss: 0.31747689843177795\n",
      "Epoch 3634, Loss: 2.077050894498825, Final Batch Loss: 0.3978583812713623\n",
      "Epoch 3635, Loss: 1.951388657093048, Final Batch Loss: 0.42357906699180603\n",
      "Epoch 3636, Loss: 1.9190071821212769, Final Batch Loss: 0.3037644624710083\n",
      "Epoch 3637, Loss: 1.97111114859581, Final Batch Loss: 0.4069043695926666\n",
      "Epoch 3638, Loss: 1.9814482927322388, Final Batch Loss: 0.45483770966529846\n",
      "Epoch 3639, Loss: 1.9331079423427582, Final Batch Loss: 0.43310120701789856\n",
      "Epoch 3640, Loss: 2.002140998840332, Final Batch Loss: 0.3617388606071472\n",
      "Epoch 3641, Loss: 1.8854126334190369, Final Batch Loss: 0.34242305159568787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3642, Loss: 1.9722141027450562, Final Batch Loss: 0.38949936628341675\n",
      "Epoch 3643, Loss: 1.8898323774337769, Final Batch Loss: 0.45594900846481323\n",
      "Epoch 3644, Loss: 1.8769595324993134, Final Batch Loss: 0.37697285413742065\n",
      "Epoch 3645, Loss: 1.8850793242454529, Final Batch Loss: 0.36601772904396057\n",
      "Epoch 3646, Loss: 2.045761823654175, Final Batch Loss: 0.4326077699661255\n",
      "Epoch 3647, Loss: 2.0212391912937164, Final Batch Loss: 0.3323199450969696\n",
      "Epoch 3648, Loss: 1.9959406852722168, Final Batch Loss: 0.4409412145614624\n",
      "Epoch 3649, Loss: 2.0053253173828125, Final Batch Loss: 0.4163040220737457\n",
      "Epoch 3650, Loss: 1.807824581861496, Final Batch Loss: 0.4121127426624298\n",
      "Epoch 3651, Loss: 1.99088853597641, Final Batch Loss: 0.4047049582004547\n",
      "Epoch 3652, Loss: 1.8375653326511383, Final Batch Loss: 0.3583803176879883\n",
      "Epoch 3653, Loss: 2.046549826860428, Final Batch Loss: 0.40688613057136536\n",
      "Epoch 3654, Loss: 1.9541244506835938, Final Batch Loss: 0.3806866705417633\n",
      "Epoch 3655, Loss: 1.751344919204712, Final Batch Loss: 0.36210188269615173\n",
      "Epoch 3656, Loss: 2.072549432516098, Final Batch Loss: 0.3661091923713684\n",
      "Epoch 3657, Loss: 2.0924910604953766, Final Batch Loss: 0.46251875162124634\n",
      "Epoch 3658, Loss: 1.9101418554782867, Final Batch Loss: 0.46108555793762207\n",
      "Epoch 3659, Loss: 1.906333476305008, Final Batch Loss: 0.3275945782661438\n",
      "Epoch 3660, Loss: 1.9511387348175049, Final Batch Loss: 0.43320539593696594\n",
      "Epoch 3661, Loss: 1.8830175995826721, Final Batch Loss: 0.38779258728027344\n",
      "Epoch 3662, Loss: 2.0680200457572937, Final Batch Loss: 0.3972126245498657\n",
      "Epoch 3663, Loss: 2.035144716501236, Final Batch Loss: 0.4817776381969452\n",
      "Epoch 3664, Loss: 1.8854151666164398, Final Batch Loss: 0.5022155046463013\n",
      "Epoch 3665, Loss: 1.9052114486694336, Final Batch Loss: 0.44105905294418335\n",
      "Epoch 3666, Loss: 1.8587442934513092, Final Batch Loss: 0.37632396817207336\n",
      "Epoch 3667, Loss: 2.0326999127864838, Final Batch Loss: 0.44340065121650696\n",
      "Epoch 3668, Loss: 1.9475435018539429, Final Batch Loss: 0.4101484715938568\n",
      "Epoch 3669, Loss: 1.976729542016983, Final Batch Loss: 0.4002489149570465\n",
      "Epoch 3670, Loss: 1.947360634803772, Final Batch Loss: 0.3751091957092285\n",
      "Epoch 3671, Loss: 1.9663013517856598, Final Batch Loss: 0.3996380567550659\n",
      "Epoch 3672, Loss: 2.090566098690033, Final Batch Loss: 0.37123093008995056\n",
      "Epoch 3673, Loss: 2.028040826320648, Final Batch Loss: 0.4493021070957184\n",
      "Epoch 3674, Loss: 1.8405213952064514, Final Batch Loss: 0.300365686416626\n",
      "Epoch 3675, Loss: 2.0886664390563965, Final Batch Loss: 0.4237225651741028\n",
      "Epoch 3676, Loss: 1.9597755074501038, Final Batch Loss: 0.35900190472602844\n",
      "Epoch 3677, Loss: 2.0673100352287292, Final Batch Loss: 0.45703285932540894\n",
      "Epoch 3678, Loss: 2.053017556667328, Final Batch Loss: 0.31449005007743835\n",
      "Epoch 3679, Loss: 1.8919903934001923, Final Batch Loss: 0.3416329026222229\n",
      "Epoch 3680, Loss: 1.9606203138828278, Final Batch Loss: 0.38314491510391235\n",
      "Epoch 3681, Loss: 1.9049100577831268, Final Batch Loss: 0.4313037097454071\n",
      "Epoch 3682, Loss: 1.8331727385520935, Final Batch Loss: 0.3490617573261261\n",
      "Epoch 3683, Loss: 1.9359210431575775, Final Batch Loss: 0.47215160727500916\n",
      "Epoch 3684, Loss: 1.966781198978424, Final Batch Loss: 0.3560188114643097\n",
      "Epoch 3685, Loss: 2.0491954386234283, Final Batch Loss: 0.43719086050987244\n",
      "Epoch 3686, Loss: 1.9106976389884949, Final Batch Loss: 0.4855858087539673\n",
      "Epoch 3687, Loss: 1.9766332805156708, Final Batch Loss: 0.42532673478126526\n",
      "Epoch 3688, Loss: 1.9882737398147583, Final Batch Loss: 0.326932430267334\n",
      "Epoch 3689, Loss: 1.8520310521125793, Final Batch Loss: 0.38525763154029846\n",
      "Epoch 3690, Loss: 1.9721077382564545, Final Batch Loss: 0.2974058985710144\n",
      "Epoch 3691, Loss: 2.2408753633499146, Final Batch Loss: 0.5545213222503662\n",
      "Epoch 3692, Loss: 2.104604035615921, Final Batch Loss: 0.4491666555404663\n",
      "Epoch 3693, Loss: 1.9523592293262482, Final Batch Loss: 0.46968719363212585\n",
      "Epoch 3694, Loss: 2.0802392661571503, Final Batch Loss: 0.43314799666404724\n",
      "Epoch 3695, Loss: 1.8915433585643768, Final Batch Loss: 0.36931484937667847\n",
      "Epoch 3696, Loss: 2.0995462834835052, Final Batch Loss: 0.43478256464004517\n",
      "Epoch 3697, Loss: 2.1019338071346283, Final Batch Loss: 0.3690366744995117\n",
      "Epoch 3698, Loss: 2.044388473033905, Final Batch Loss: 0.5173220038414001\n",
      "Epoch 3699, Loss: 2.0062337815761566, Final Batch Loss: 0.5074878334999084\n",
      "Epoch 3700, Loss: 1.9560437500476837, Final Batch Loss: 0.4036429524421692\n",
      "Epoch 3701, Loss: 1.9612293243408203, Final Batch Loss: 0.3725683093070984\n",
      "Epoch 3702, Loss: 2.045891582965851, Final Batch Loss: 0.4007485806941986\n",
      "Epoch 3703, Loss: 1.9526599049568176, Final Batch Loss: 0.35575518012046814\n",
      "Epoch 3704, Loss: 1.9430442154407501, Final Batch Loss: 0.3845216631889343\n",
      "Epoch 3705, Loss: 2.037395566701889, Final Batch Loss: 0.3591463565826416\n",
      "Epoch 3706, Loss: 1.775121122598648, Final Batch Loss: 0.3984696567058563\n",
      "Epoch 3707, Loss: 1.8262747824192047, Final Batch Loss: 0.35539376735687256\n",
      "Epoch 3708, Loss: 1.9185500144958496, Final Batch Loss: 0.2774571478366852\n",
      "Epoch 3709, Loss: 1.9327058792114258, Final Batch Loss: 0.3730408549308777\n",
      "Epoch 3710, Loss: 1.904671311378479, Final Batch Loss: 0.29157859086990356\n",
      "Epoch 3711, Loss: 1.7878990769386292, Final Batch Loss: 0.2829201817512512\n",
      "Epoch 3712, Loss: 1.9909844398498535, Final Batch Loss: 0.4050332009792328\n",
      "Epoch 3713, Loss: 1.7744575440883636, Final Batch Loss: 0.3748640716075897\n",
      "Epoch 3714, Loss: 1.9574607610702515, Final Batch Loss: 0.4142164885997772\n",
      "Epoch 3715, Loss: 1.9867531657218933, Final Batch Loss: 0.363105833530426\n",
      "Epoch 3716, Loss: 1.927998036146164, Final Batch Loss: 0.34798645973205566\n",
      "Epoch 3717, Loss: 1.9997747838497162, Final Batch Loss: 0.3628765642642975\n",
      "Epoch 3718, Loss: 1.8357405364513397, Final Batch Loss: 0.37954866886138916\n",
      "Epoch 3719, Loss: 2.0437030494213104, Final Batch Loss: 0.49600255489349365\n",
      "Epoch 3720, Loss: 1.9722804725170135, Final Batch Loss: 0.4192599356174469\n",
      "Epoch 3721, Loss: 2.077183812856674, Final Batch Loss: 0.464129775762558\n",
      "Epoch 3722, Loss: 2.0447250306606293, Final Batch Loss: 0.39902031421661377\n",
      "Epoch 3723, Loss: 2.073967307806015, Final Batch Loss: 0.43290722370147705\n",
      "Epoch 3724, Loss: 1.9278505742549896, Final Batch Loss: 0.3259303569793701\n",
      "Epoch 3725, Loss: 2.000274360179901, Final Batch Loss: 0.429060697555542\n",
      "Epoch 3726, Loss: 1.9555682241916656, Final Batch Loss: 0.39185723662376404\n",
      "Epoch 3727, Loss: 2.0486257076263428, Final Batch Loss: 0.39676547050476074\n",
      "Epoch 3728, Loss: 1.8284710943698883, Final Batch Loss: 0.3249281644821167\n",
      "Epoch 3729, Loss: 1.9539397358894348, Final Batch Loss: 0.3291093707084656\n",
      "Epoch 3730, Loss: 2.2018748223781586, Final Batch Loss: 0.4694834351539612\n",
      "Epoch 3731, Loss: 1.7862915694713593, Final Batch Loss: 0.31719616055488586\n",
      "Epoch 3732, Loss: 2.1748484075069427, Final Batch Loss: 0.5135244131088257\n",
      "Epoch 3733, Loss: 2.103284776210785, Final Batch Loss: 0.3320220410823822\n",
      "Epoch 3734, Loss: 1.9292679131031036, Final Batch Loss: 0.3574995994567871\n",
      "Epoch 3735, Loss: 2.076645612716675, Final Batch Loss: 0.46049994230270386\n",
      "Epoch 3736, Loss: 2.061743199825287, Final Batch Loss: 0.3591250479221344\n",
      "Epoch 3737, Loss: 1.91311514377594, Final Batch Loss: 0.38616442680358887\n",
      "Epoch 3738, Loss: 1.9828148186206818, Final Batch Loss: 0.3820487856864929\n",
      "Epoch 3739, Loss: 1.9790306389331818, Final Batch Loss: 0.4734307825565338\n",
      "Epoch 3740, Loss: 1.8350737988948822, Final Batch Loss: 0.3317939341068268\n",
      "Epoch 3741, Loss: 1.9968876242637634, Final Batch Loss: 0.34353774785995483\n",
      "Epoch 3742, Loss: 2.1230366826057434, Final Batch Loss: 0.3817209303379059\n",
      "Epoch 3743, Loss: 1.9760720133781433, Final Batch Loss: 0.46885842084884644\n",
      "Epoch 3744, Loss: 1.9641415476799011, Final Batch Loss: 0.3515017330646515\n",
      "Epoch 3745, Loss: 1.91134974360466, Final Batch Loss: 0.34563761949539185\n",
      "Epoch 3746, Loss: 1.9672287106513977, Final Batch Loss: 0.4130772650241852\n",
      "Epoch 3747, Loss: 2.0535868406295776, Final Batch Loss: 0.47313883900642395\n",
      "Epoch 3748, Loss: 1.8812876343727112, Final Batch Loss: 0.3979509770870209\n",
      "Epoch 3749, Loss: 2.0620262920856476, Final Batch Loss: 0.4766206741333008\n",
      "Epoch 3750, Loss: 1.8921169936656952, Final Batch Loss: 0.3391295373439789\n",
      "Epoch 3751, Loss: 1.7981766760349274, Final Batch Loss: 0.35238412022590637\n",
      "Epoch 3752, Loss: 2.039956271648407, Final Batch Loss: 0.48941555619239807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3753, Loss: 2.0614418387413025, Final Batch Loss: 0.43934357166290283\n",
      "Epoch 3754, Loss: 2.0460903346538544, Final Batch Loss: 0.42792466282844543\n",
      "Epoch 3755, Loss: 1.9186017513275146, Final Batch Loss: 0.36161717772483826\n",
      "Epoch 3756, Loss: 1.8598414063453674, Final Batch Loss: 0.4459225833415985\n",
      "Epoch 3757, Loss: 1.9726627171039581, Final Batch Loss: 0.33149078488349915\n",
      "Epoch 3758, Loss: 1.9718973934650421, Final Batch Loss: 0.46074604988098145\n",
      "Epoch 3759, Loss: 1.9584287405014038, Final Batch Loss: 0.32307955622673035\n",
      "Epoch 3760, Loss: 2.069983571767807, Final Batch Loss: 0.39390453696250916\n",
      "Epoch 3761, Loss: 1.9918439090251923, Final Batch Loss: 0.3746286630630493\n",
      "Epoch 3762, Loss: 1.9408296346664429, Final Batch Loss: 0.42985621094703674\n",
      "Epoch 3763, Loss: 1.961274415254593, Final Batch Loss: 0.41700080037117004\n",
      "Epoch 3764, Loss: 1.9834239482879639, Final Batch Loss: 0.3515453338623047\n",
      "Epoch 3765, Loss: 2.078038662672043, Final Batch Loss: 0.39230361580848694\n",
      "Epoch 3766, Loss: 1.8610981404781342, Final Batch Loss: 0.3275434672832489\n",
      "Epoch 3767, Loss: 1.973298817873001, Final Batch Loss: 0.3919941186904907\n",
      "Epoch 3768, Loss: 2.254821300506592, Final Batch Loss: 0.5112907886505127\n",
      "Epoch 3769, Loss: 1.8259242177009583, Final Batch Loss: 0.3259279131889343\n",
      "Epoch 3770, Loss: 2.0807635486125946, Final Batch Loss: 0.36999666690826416\n",
      "Epoch 3771, Loss: 1.9141422510147095, Final Batch Loss: 0.36502596735954285\n",
      "Epoch 3772, Loss: 1.9821823835372925, Final Batch Loss: 0.4012737572193146\n",
      "Epoch 3773, Loss: 1.9458066523075104, Final Batch Loss: 0.34416505694389343\n",
      "Epoch 3774, Loss: 1.9078040719032288, Final Batch Loss: 0.37888017296791077\n",
      "Epoch 3775, Loss: 1.8258243799209595, Final Batch Loss: 0.3344710171222687\n",
      "Epoch 3776, Loss: 1.911566436290741, Final Batch Loss: 0.35123226046562195\n",
      "Epoch 3777, Loss: 1.9937514960765839, Final Batch Loss: 0.3294993042945862\n",
      "Epoch 3778, Loss: 1.9221104085445404, Final Batch Loss: 0.35643091797828674\n",
      "Epoch 3779, Loss: 2.0508232414722443, Final Batch Loss: 0.40994319319725037\n",
      "Epoch 3780, Loss: 2.1640610098838806, Final Batch Loss: 0.4693030118942261\n",
      "Epoch 3781, Loss: 1.899309128522873, Final Batch Loss: 0.37531423568725586\n",
      "Epoch 3782, Loss: 1.9474356174468994, Final Batch Loss: 0.3741808831691742\n",
      "Epoch 3783, Loss: 1.9833303689956665, Final Batch Loss: 0.4772113859653473\n",
      "Epoch 3784, Loss: 1.9239839613437653, Final Batch Loss: 0.42457810044288635\n",
      "Epoch 3785, Loss: 1.879262536764145, Final Batch Loss: 0.3382333219051361\n",
      "Epoch 3786, Loss: 1.9613386392593384, Final Batch Loss: 0.4121558666229248\n",
      "Epoch 3787, Loss: 1.792104572057724, Final Batch Loss: 0.36688077449798584\n",
      "Epoch 3788, Loss: 1.8773500323295593, Final Batch Loss: 0.41315752267837524\n",
      "Epoch 3789, Loss: 2.0503408312797546, Final Batch Loss: 0.46757298707962036\n",
      "Epoch 3790, Loss: 2.0204209089279175, Final Batch Loss: 0.43970149755477905\n",
      "Epoch 3791, Loss: 1.888427495956421, Final Batch Loss: 0.3326111137866974\n",
      "Epoch 3792, Loss: 1.990617036819458, Final Batch Loss: 0.3216674327850342\n",
      "Epoch 3793, Loss: 1.9687686264514923, Final Batch Loss: 0.3083728551864624\n",
      "Epoch 3794, Loss: 2.051978975534439, Final Batch Loss: 0.4516288936138153\n",
      "Epoch 3795, Loss: 1.9866690337657928, Final Batch Loss: 0.45275625586509705\n",
      "Epoch 3796, Loss: 1.9009788930416107, Final Batch Loss: 0.37873923778533936\n",
      "Epoch 3797, Loss: 1.9280289113521576, Final Batch Loss: 0.32781141996383667\n",
      "Epoch 3798, Loss: 1.7389236092567444, Final Batch Loss: 0.35797563195228577\n",
      "Epoch 3799, Loss: 1.8033051788806915, Final Batch Loss: 0.4418734312057495\n",
      "Epoch 3800, Loss: 1.8421248197555542, Final Batch Loss: 0.3419117033481598\n",
      "Epoch 3801, Loss: 1.8864882588386536, Final Batch Loss: 0.38366028666496277\n",
      "Epoch 3802, Loss: 1.8702615201473236, Final Batch Loss: 0.39757636189460754\n",
      "Epoch 3803, Loss: 2.090271145105362, Final Batch Loss: 0.4674062132835388\n",
      "Epoch 3804, Loss: 2.1908916234970093, Final Batch Loss: 0.3627950847148895\n",
      "Epoch 3805, Loss: 2.087525248527527, Final Batch Loss: 0.44245445728302\n",
      "Epoch 3806, Loss: 1.9057371020317078, Final Batch Loss: 0.3508036732673645\n",
      "Epoch 3807, Loss: 1.893282175064087, Final Batch Loss: 0.37921997904777527\n",
      "Epoch 3808, Loss: 1.826646327972412, Final Batch Loss: 0.4347879886627197\n",
      "Epoch 3809, Loss: 1.8531048595905304, Final Batch Loss: 0.3728213310241699\n",
      "Epoch 3810, Loss: 1.8942560255527496, Final Batch Loss: 0.4587102234363556\n",
      "Epoch 3811, Loss: 1.9966413974761963, Final Batch Loss: 0.43225330114364624\n",
      "Epoch 3812, Loss: 1.9465049803256989, Final Batch Loss: 0.3555644452571869\n",
      "Epoch 3813, Loss: 2.009817600250244, Final Batch Loss: 0.41835135221481323\n",
      "Epoch 3814, Loss: 1.8580619990825653, Final Batch Loss: 0.36198949813842773\n",
      "Epoch 3815, Loss: 2.010489374399185, Final Batch Loss: 0.3604867458343506\n",
      "Epoch 3816, Loss: 1.7471981346607208, Final Batch Loss: 0.373630553483963\n",
      "Epoch 3817, Loss: 1.9766422808170319, Final Batch Loss: 0.36566677689552307\n",
      "Epoch 3818, Loss: 1.9816443026065826, Final Batch Loss: 0.3821423053741455\n",
      "Epoch 3819, Loss: 2.01048344373703, Final Batch Loss: 0.37316781282424927\n",
      "Epoch 3820, Loss: 1.732865035533905, Final Batch Loss: 0.3349160850048065\n",
      "Epoch 3821, Loss: 1.9094595909118652, Final Batch Loss: 0.396665096282959\n",
      "Epoch 3822, Loss: 1.981430321931839, Final Batch Loss: 0.34441083669662476\n",
      "Epoch 3823, Loss: 2.1728818118572235, Final Batch Loss: 0.4537200629711151\n",
      "Epoch 3824, Loss: 2.0248366594314575, Final Batch Loss: 0.38110750913619995\n",
      "Epoch 3825, Loss: 2.012298434972763, Final Batch Loss: 0.37378284335136414\n",
      "Epoch 3826, Loss: 1.8519271612167358, Final Batch Loss: 0.4264041483402252\n",
      "Epoch 3827, Loss: 1.9357403814792633, Final Batch Loss: 0.39106422662734985\n",
      "Epoch 3828, Loss: 1.7745154798030853, Final Batch Loss: 0.36841970682144165\n",
      "Epoch 3829, Loss: 2.0630593597888947, Final Batch Loss: 0.4189043641090393\n",
      "Epoch 3830, Loss: 2.0402496457099915, Final Batch Loss: 0.501003623008728\n",
      "Epoch 3831, Loss: 1.9295701086521149, Final Batch Loss: 0.39357656240463257\n",
      "Epoch 3832, Loss: 2.0451945662498474, Final Batch Loss: 0.346687376499176\n",
      "Epoch 3833, Loss: 1.954164057970047, Final Batch Loss: 0.3901834785938263\n",
      "Epoch 3834, Loss: 1.798368752002716, Final Batch Loss: 0.335659921169281\n",
      "Epoch 3835, Loss: 1.8940798342227936, Final Batch Loss: 0.3191542625427246\n",
      "Epoch 3836, Loss: 1.8652817010879517, Final Batch Loss: 0.36046284437179565\n",
      "Epoch 3837, Loss: 1.9470652341842651, Final Batch Loss: 0.47349682450294495\n",
      "Epoch 3838, Loss: 1.9128529727458954, Final Batch Loss: 0.34539759159088135\n",
      "Epoch 3839, Loss: 2.076960027217865, Final Batch Loss: 0.3945169150829315\n",
      "Epoch 3840, Loss: 1.9190344214439392, Final Batch Loss: 0.41820117831230164\n",
      "Epoch 3841, Loss: 1.9375216662883759, Final Batch Loss: 0.4532952308654785\n",
      "Epoch 3842, Loss: 1.909583330154419, Final Batch Loss: 0.4010985493659973\n",
      "Epoch 3843, Loss: 2.060143142938614, Final Batch Loss: 0.4048977196216583\n",
      "Epoch 3844, Loss: 2.1171503365039825, Final Batch Loss: 0.4996567964553833\n",
      "Epoch 3845, Loss: 1.917055457830429, Final Batch Loss: 0.4305422306060791\n",
      "Epoch 3846, Loss: 2.112959563732147, Final Batch Loss: 0.4436931610107422\n",
      "Epoch 3847, Loss: 1.9584065675735474, Final Batch Loss: 0.4494311511516571\n",
      "Epoch 3848, Loss: 1.7945608496665955, Final Batch Loss: 0.4397917687892914\n",
      "Epoch 3849, Loss: 1.7779653072357178, Final Batch Loss: 0.3500431180000305\n",
      "Epoch 3850, Loss: 1.9381165504455566, Final Batch Loss: 0.37545517086982727\n",
      "Epoch 3851, Loss: 1.7140521109104156, Final Batch Loss: 0.32558146119117737\n",
      "Epoch 3852, Loss: 1.8639139533042908, Final Batch Loss: 0.3961818218231201\n",
      "Epoch 3853, Loss: 1.9418250918388367, Final Batch Loss: 0.450141966342926\n",
      "Epoch 3854, Loss: 2.0669313073158264, Final Batch Loss: 0.3979068100452423\n",
      "Epoch 3855, Loss: 1.9248959720134735, Final Batch Loss: 0.4918617606163025\n",
      "Epoch 3856, Loss: 2.151129335165024, Final Batch Loss: 0.4633830189704895\n",
      "Epoch 3857, Loss: 1.8642497658729553, Final Batch Loss: 0.298142671585083\n",
      "Epoch 3858, Loss: 1.7323134541511536, Final Batch Loss: 0.39212390780448914\n",
      "Epoch 3859, Loss: 2.0653273463249207, Final Batch Loss: 0.4193110167980194\n",
      "Epoch 3860, Loss: 1.9454932808876038, Final Batch Loss: 0.4309009909629822\n",
      "Epoch 3861, Loss: 1.822488248348236, Final Batch Loss: 0.27986109256744385\n",
      "Epoch 3862, Loss: 2.0065965950489044, Final Batch Loss: 0.4352290630340576\n",
      "Epoch 3863, Loss: 1.9062062501907349, Final Batch Loss: 0.3428955376148224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3864, Loss: 1.9449683725833893, Final Batch Loss: 0.37263837456703186\n",
      "Epoch 3865, Loss: 1.8764572143554688, Final Batch Loss: 0.37151241302490234\n",
      "Epoch 3866, Loss: 2.0112063884735107, Final Batch Loss: 0.33956658840179443\n",
      "Epoch 3867, Loss: 1.8476060628890991, Final Batch Loss: 0.3697601854801178\n",
      "Epoch 3868, Loss: 1.9187114834785461, Final Batch Loss: 0.38646748661994934\n",
      "Epoch 3869, Loss: 1.883438616991043, Final Batch Loss: 0.3702091574668884\n",
      "Epoch 3870, Loss: 1.9260582327842712, Final Batch Loss: 0.33532655239105225\n",
      "Epoch 3871, Loss: 1.9682594239711761, Final Batch Loss: 0.4703225791454315\n",
      "Epoch 3872, Loss: 1.914519876241684, Final Batch Loss: 0.381075918674469\n",
      "Epoch 3873, Loss: 1.9149418473243713, Final Batch Loss: 0.4064379930496216\n",
      "Epoch 3874, Loss: 1.9452594220638275, Final Batch Loss: 0.4547746777534485\n",
      "Epoch 3875, Loss: 2.107225924730301, Final Batch Loss: 0.4300887882709503\n",
      "Epoch 3876, Loss: 1.7410646975040436, Final Batch Loss: 0.2934221625328064\n",
      "Epoch 3877, Loss: 2.061722606420517, Final Batch Loss: 0.5063167214393616\n",
      "Epoch 3878, Loss: 1.8083953261375427, Final Batch Loss: 0.28966009616851807\n",
      "Epoch 3879, Loss: 1.9673576653003693, Final Batch Loss: 0.4194481372833252\n",
      "Epoch 3880, Loss: 1.9617623686790466, Final Batch Loss: 0.4269847571849823\n",
      "Epoch 3881, Loss: 1.94085031747818, Final Batch Loss: 0.34169989824295044\n",
      "Epoch 3882, Loss: 1.9336661994457245, Final Batch Loss: 0.40074488520622253\n",
      "Epoch 3883, Loss: 1.842663824558258, Final Batch Loss: 0.28919142484664917\n",
      "Epoch 3884, Loss: 1.7646400034427643, Final Batch Loss: 0.41756612062454224\n",
      "Epoch 3885, Loss: 2.0237747132778168, Final Batch Loss: 0.38720494508743286\n",
      "Epoch 3886, Loss: 1.940409630537033, Final Batch Loss: 0.5057704448699951\n",
      "Epoch 3887, Loss: 2.1189854443073273, Final Batch Loss: 0.4576573669910431\n",
      "Epoch 3888, Loss: 1.9383674561977386, Final Batch Loss: 0.370679646730423\n",
      "Epoch 3889, Loss: 1.8703009486198425, Final Batch Loss: 0.44251564145088196\n",
      "Epoch 3890, Loss: 1.908041089773178, Final Batch Loss: 0.47474727034568787\n",
      "Epoch 3891, Loss: 1.7904666066169739, Final Batch Loss: 0.2700819969177246\n",
      "Epoch 3892, Loss: 1.8600035905838013, Final Batch Loss: 0.3746659457683563\n",
      "Epoch 3893, Loss: 2.006789118051529, Final Batch Loss: 0.4750401973724365\n",
      "Epoch 3894, Loss: 2.0122187733650208, Final Batch Loss: 0.49235010147094727\n",
      "Epoch 3895, Loss: 1.9615293741226196, Final Batch Loss: 0.46806779503822327\n",
      "Epoch 3896, Loss: 1.8470351994037628, Final Batch Loss: 0.3840779662132263\n",
      "Epoch 3897, Loss: 1.8684396743774414, Final Batch Loss: 0.3680785298347473\n",
      "Epoch 3898, Loss: 1.857772946357727, Final Batch Loss: 0.28865131735801697\n",
      "Epoch 3899, Loss: 1.9539420306682587, Final Batch Loss: 0.349409818649292\n",
      "Epoch 3900, Loss: 1.9370379447937012, Final Batch Loss: 0.3628414571285248\n",
      "Epoch 3901, Loss: 1.8872021734714508, Final Batch Loss: 0.39774289727211\n",
      "Epoch 3902, Loss: 1.9819870591163635, Final Batch Loss: 0.35552090406417847\n",
      "Epoch 3903, Loss: 2.0076884031295776, Final Batch Loss: 0.4199620485305786\n",
      "Epoch 3904, Loss: 1.8753920793533325, Final Batch Loss: 0.37483686208724976\n",
      "Epoch 3905, Loss: 1.8963035643100739, Final Batch Loss: 0.4164671003818512\n",
      "Epoch 3906, Loss: 2.083112448453903, Final Batch Loss: 0.44814518094062805\n",
      "Epoch 3907, Loss: 1.9575322568416595, Final Batch Loss: 0.3407808244228363\n",
      "Epoch 3908, Loss: 1.8558626174926758, Final Batch Loss: 0.33981043100357056\n",
      "Epoch 3909, Loss: 1.9437994956970215, Final Batch Loss: 0.41936194896698\n",
      "Epoch 3910, Loss: 1.8731187880039215, Final Batch Loss: 0.42658373713493347\n",
      "Epoch 3911, Loss: 1.9811798334121704, Final Batch Loss: 0.31965476274490356\n",
      "Epoch 3912, Loss: 2.0683874785900116, Final Batch Loss: 0.36672738194465637\n",
      "Epoch 3913, Loss: 1.9316900074481964, Final Batch Loss: 0.3214816749095917\n",
      "Epoch 3914, Loss: 1.9739105701446533, Final Batch Loss: 0.48092275857925415\n",
      "Epoch 3915, Loss: 1.9922444224357605, Final Batch Loss: 0.5004275441169739\n",
      "Epoch 3916, Loss: 1.8979936838150024, Final Batch Loss: 0.29753947257995605\n",
      "Epoch 3917, Loss: 1.8840617388486862, Final Batch Loss: 0.43108096718788147\n",
      "Epoch 3918, Loss: 1.8841196596622467, Final Batch Loss: 0.318062961101532\n",
      "Epoch 3919, Loss: 1.8323619067668915, Final Batch Loss: 0.3815970718860626\n",
      "Epoch 3920, Loss: 1.9598545134067535, Final Batch Loss: 0.38789549469947815\n",
      "Epoch 3921, Loss: 2.0085344910621643, Final Batch Loss: 0.4806283116340637\n",
      "Epoch 3922, Loss: 2.0267459750175476, Final Batch Loss: 0.46320006251335144\n",
      "Epoch 3923, Loss: 1.7434594929218292, Final Batch Loss: 0.297663152217865\n",
      "Epoch 3924, Loss: 1.907235473394394, Final Batch Loss: 0.3144555985927582\n",
      "Epoch 3925, Loss: 2.0904332995414734, Final Batch Loss: 0.5362233519554138\n",
      "Epoch 3926, Loss: 1.946677714586258, Final Batch Loss: 0.4495021402835846\n",
      "Epoch 3927, Loss: 1.9800486862659454, Final Batch Loss: 0.43860533833503723\n",
      "Epoch 3928, Loss: 2.089728206396103, Final Batch Loss: 0.4781809151172638\n",
      "Epoch 3929, Loss: 1.8891543447971344, Final Batch Loss: 0.35080668330192566\n",
      "Epoch 3930, Loss: 1.9225111901760101, Final Batch Loss: 0.3087175786495209\n",
      "Epoch 3931, Loss: 1.9334977865219116, Final Batch Loss: 0.41241616010665894\n",
      "Epoch 3932, Loss: 1.9614684283733368, Final Batch Loss: 0.36207717657089233\n",
      "Epoch 3933, Loss: 2.109402656555176, Final Batch Loss: 0.40912047028541565\n",
      "Epoch 3934, Loss: 1.9540005326271057, Final Batch Loss: 0.5299438238143921\n",
      "Epoch 3935, Loss: 2.008992463350296, Final Batch Loss: 0.40688127279281616\n",
      "Epoch 3936, Loss: 1.8806979060173035, Final Batch Loss: 0.3584841191768646\n",
      "Epoch 3937, Loss: 1.8835453391075134, Final Batch Loss: 0.43596771359443665\n",
      "Epoch 3938, Loss: 2.0393709242343903, Final Batch Loss: 0.3574635684490204\n",
      "Epoch 3939, Loss: 1.8751260936260223, Final Batch Loss: 0.37758082151412964\n",
      "Epoch 3940, Loss: 1.85064297914505, Final Batch Loss: 0.3414749503135681\n",
      "Epoch 3941, Loss: 2.0997374951839447, Final Batch Loss: 0.5573670864105225\n",
      "Epoch 3942, Loss: 1.7680159211158752, Final Batch Loss: 0.40298932790756226\n",
      "Epoch 3943, Loss: 1.8603747189044952, Final Batch Loss: 0.4082365930080414\n",
      "Epoch 3944, Loss: 1.8205902576446533, Final Batch Loss: 0.3251981735229492\n",
      "Epoch 3945, Loss: 1.85076904296875, Final Batch Loss: 0.3922443687915802\n",
      "Epoch 3946, Loss: 1.8685641884803772, Final Batch Loss: 0.3943176865577698\n",
      "Epoch 3947, Loss: 1.9273954331874847, Final Batch Loss: 0.31764763593673706\n",
      "Epoch 3948, Loss: 2.044831156730652, Final Batch Loss: 0.3548612594604492\n",
      "Epoch 3949, Loss: 2.0022081434726715, Final Batch Loss: 0.40381699800491333\n",
      "Epoch 3950, Loss: 1.8508535325527191, Final Batch Loss: 0.379592627286911\n",
      "Epoch 3951, Loss: 1.8991166651248932, Final Batch Loss: 0.5324872732162476\n",
      "Epoch 3952, Loss: 1.8111693561077118, Final Batch Loss: 0.34888264536857605\n",
      "Epoch 3953, Loss: 1.8639022409915924, Final Batch Loss: 0.29798513650894165\n",
      "Epoch 3954, Loss: 1.8770579099655151, Final Batch Loss: 0.3405146896839142\n",
      "Epoch 3955, Loss: 1.9007653892040253, Final Batch Loss: 0.3535134494304657\n",
      "Epoch 3956, Loss: 2.012249678373337, Final Batch Loss: 0.3930681347846985\n",
      "Epoch 3957, Loss: 1.911333441734314, Final Batch Loss: 0.4154442548751831\n",
      "Epoch 3958, Loss: 1.9489702582359314, Final Batch Loss: 0.35258248448371887\n",
      "Epoch 3959, Loss: 1.9829040169715881, Final Batch Loss: 0.31203150749206543\n",
      "Epoch 3960, Loss: 1.8878892064094543, Final Batch Loss: 0.3774647116661072\n",
      "Epoch 3961, Loss: 1.90262970328331, Final Batch Loss: 0.32532018423080444\n",
      "Epoch 3962, Loss: 1.9579091668128967, Final Batch Loss: 0.43057751655578613\n",
      "Epoch 3963, Loss: 1.9501469731330872, Final Batch Loss: 0.38694071769714355\n",
      "Epoch 3964, Loss: 1.8986434042453766, Final Batch Loss: 0.36734968423843384\n",
      "Epoch 3965, Loss: 1.8701944053173065, Final Batch Loss: 0.2811899185180664\n",
      "Epoch 3966, Loss: 1.7598878741264343, Final Batch Loss: 0.3431985378265381\n",
      "Epoch 3967, Loss: 1.8227989375591278, Final Batch Loss: 0.36801669001579285\n",
      "Epoch 3968, Loss: 1.9666290581226349, Final Batch Loss: 0.45391979813575745\n",
      "Epoch 3969, Loss: 1.8099722862243652, Final Batch Loss: 0.3767758905887604\n",
      "Epoch 3970, Loss: 1.8432582020759583, Final Batch Loss: 0.4267299175262451\n",
      "Epoch 3971, Loss: 1.8308436274528503, Final Batch Loss: 0.3107881247997284\n",
      "Epoch 3972, Loss: 1.7771316468715668, Final Batch Loss: 0.3452838659286499\n",
      "Epoch 3973, Loss: 1.8198990523815155, Final Batch Loss: 0.38961219787597656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3974, Loss: 1.9625552892684937, Final Batch Loss: 0.41169068217277527\n",
      "Epoch 3975, Loss: 1.869713455438614, Final Batch Loss: 0.33776354789733887\n",
      "Epoch 3976, Loss: 1.8972524106502533, Final Batch Loss: 0.4909460246562958\n",
      "Epoch 3977, Loss: 1.9341289699077606, Final Batch Loss: 0.3809494078159332\n",
      "Epoch 3978, Loss: 1.7110323309898376, Final Batch Loss: 0.36682581901550293\n",
      "Epoch 3979, Loss: 1.9664061665534973, Final Batch Loss: 0.4838026463985443\n",
      "Epoch 3980, Loss: 1.8754127323627472, Final Batch Loss: 0.33238062262535095\n",
      "Epoch 3981, Loss: 1.7692872285842896, Final Batch Loss: 0.35705024003982544\n",
      "Epoch 3982, Loss: 1.9623171985149384, Final Batch Loss: 0.4405532479286194\n",
      "Epoch 3983, Loss: 2.056635469198227, Final Batch Loss: 0.4344633221626282\n",
      "Epoch 3984, Loss: 1.675975888967514, Final Batch Loss: 0.32589587569236755\n",
      "Epoch 3985, Loss: 1.825654000043869, Final Batch Loss: 0.48373645544052124\n",
      "Epoch 3986, Loss: 1.808725655078888, Final Batch Loss: 0.27902981638908386\n",
      "Epoch 3987, Loss: 1.8418618738651276, Final Batch Loss: 0.3906581997871399\n",
      "Epoch 3988, Loss: 1.7652573585510254, Final Batch Loss: 0.28720781207084656\n",
      "Epoch 3989, Loss: 2.0737520456314087, Final Batch Loss: 0.44001665711402893\n",
      "Epoch 3990, Loss: 2.026619255542755, Final Batch Loss: 0.46509766578674316\n",
      "Epoch 3991, Loss: 1.732666254043579, Final Batch Loss: 0.30681028962135315\n",
      "Epoch 3992, Loss: 2.0229897499084473, Final Batch Loss: 0.48468270897865295\n",
      "Epoch 3993, Loss: 1.9787800014019012, Final Batch Loss: 0.4251961410045624\n",
      "Epoch 3994, Loss: 1.8544023633003235, Final Batch Loss: 0.3400489389896393\n",
      "Epoch 3995, Loss: 1.8929312825202942, Final Batch Loss: 0.3480411767959595\n",
      "Epoch 3996, Loss: 1.8210374414920807, Final Batch Loss: 0.35145261883735657\n",
      "Epoch 3997, Loss: 1.8836169242858887, Final Batch Loss: 0.35415902733802795\n",
      "Epoch 3998, Loss: 1.7908819615840912, Final Batch Loss: 0.3610612452030182\n",
      "Epoch 3999, Loss: 1.8180234730243683, Final Batch Loss: 0.3944297432899475\n",
      "Epoch 4000, Loss: 2.103887677192688, Final Batch Loss: 0.5325279235839844\n",
      "Epoch 4001, Loss: 1.7722555696964264, Final Batch Loss: 0.344790518283844\n",
      "Epoch 4002, Loss: 1.8353224396705627, Final Batch Loss: 0.38736119866371155\n",
      "Epoch 4003, Loss: 1.8172514736652374, Final Batch Loss: 0.33657217025756836\n",
      "Epoch 4004, Loss: 1.8922268450260162, Final Batch Loss: 0.3817189633846283\n",
      "Epoch 4005, Loss: 1.8850287199020386, Final Batch Loss: 0.41983845829963684\n",
      "Epoch 4006, Loss: 1.9866484701633453, Final Batch Loss: 0.37125301361083984\n",
      "Epoch 4007, Loss: 1.9083618223667145, Final Batch Loss: 0.4363442063331604\n",
      "Epoch 4008, Loss: 1.8774848282337189, Final Batch Loss: 0.4202951192855835\n",
      "Epoch 4009, Loss: 1.864274650812149, Final Batch Loss: 0.3279949724674225\n",
      "Epoch 4010, Loss: 1.8695625960826874, Final Batch Loss: 0.3528077006340027\n",
      "Epoch 4011, Loss: 1.9873946607112885, Final Batch Loss: 0.3964042067527771\n",
      "Epoch 4012, Loss: 2.0592240691184998, Final Batch Loss: 0.36762627959251404\n",
      "Epoch 4013, Loss: 1.8509792983531952, Final Batch Loss: 0.2909562885761261\n",
      "Epoch 4014, Loss: 1.9782878756523132, Final Batch Loss: 0.38291430473327637\n",
      "Epoch 4015, Loss: 1.9412683546543121, Final Batch Loss: 0.3316115736961365\n",
      "Epoch 4016, Loss: 1.863460510969162, Final Batch Loss: 0.4225170612335205\n",
      "Epoch 4017, Loss: 1.806832641363144, Final Batch Loss: 0.3378108739852905\n",
      "Epoch 4018, Loss: 1.9604125916957855, Final Batch Loss: 0.3915238380432129\n",
      "Epoch 4019, Loss: 2.0892360508441925, Final Batch Loss: 0.4273214340209961\n",
      "Epoch 4020, Loss: 1.9217589199543, Final Batch Loss: 0.33487656712532043\n",
      "Epoch 4021, Loss: 1.825361430644989, Final Batch Loss: 0.4345383942127228\n",
      "Epoch 4022, Loss: 2.002146452665329, Final Batch Loss: 0.3566448986530304\n",
      "Epoch 4023, Loss: 1.9026876091957092, Final Batch Loss: 0.39679262042045593\n",
      "Epoch 4024, Loss: 1.9548764526844025, Final Batch Loss: 0.5176742672920227\n",
      "Epoch 4025, Loss: 1.897031992673874, Final Batch Loss: 0.29264384508132935\n",
      "Epoch 4026, Loss: 1.7916808128356934, Final Batch Loss: 0.3236895799636841\n",
      "Epoch 4027, Loss: 1.939276546239853, Final Batch Loss: 0.3889571726322174\n",
      "Epoch 4028, Loss: 1.8250683844089508, Final Batch Loss: 0.37227863073349\n",
      "Epoch 4029, Loss: 1.7840615808963776, Final Batch Loss: 0.37867724895477295\n",
      "Epoch 4030, Loss: 1.764854609966278, Final Batch Loss: 0.3288625180721283\n",
      "Epoch 4031, Loss: 1.908803790807724, Final Batch Loss: 0.3544939160346985\n",
      "Epoch 4032, Loss: 2.0016640424728394, Final Batch Loss: 0.4285493493080139\n",
      "Epoch 4033, Loss: 1.8965676724910736, Final Batch Loss: 0.3628493547439575\n",
      "Epoch 4034, Loss: 1.830851823091507, Final Batch Loss: 0.32920342683792114\n",
      "Epoch 4035, Loss: 1.9779285788536072, Final Batch Loss: 0.3678663671016693\n",
      "Epoch 4036, Loss: 1.864800363779068, Final Batch Loss: 0.41301244497299194\n",
      "Epoch 4037, Loss: 1.7154477834701538, Final Batch Loss: 0.3463563621044159\n",
      "Epoch 4038, Loss: 1.9131362438201904, Final Batch Loss: 0.33407625555992126\n",
      "Epoch 4039, Loss: 2.0175395011901855, Final Batch Loss: 0.43916523456573486\n",
      "Epoch 4040, Loss: 1.939009577035904, Final Batch Loss: 0.42796966433525085\n",
      "Epoch 4041, Loss: 1.9541021287441254, Final Batch Loss: 0.40824928879737854\n",
      "Epoch 4042, Loss: 1.815144956111908, Final Batch Loss: 0.34416434168815613\n",
      "Epoch 4043, Loss: 1.9229747951030731, Final Batch Loss: 0.4007299542427063\n",
      "Epoch 4044, Loss: 1.9213673770427704, Final Batch Loss: 0.4162253141403198\n",
      "Epoch 4045, Loss: 1.9581888318061829, Final Batch Loss: 0.4243605434894562\n",
      "Epoch 4046, Loss: 1.9634419679641724, Final Batch Loss: 0.371894896030426\n",
      "Epoch 4047, Loss: 1.9663918316364288, Final Batch Loss: 0.42197081446647644\n",
      "Epoch 4048, Loss: 1.8888322710990906, Final Batch Loss: 0.36514517664909363\n",
      "Epoch 4049, Loss: 1.8885221779346466, Final Batch Loss: 0.36431145668029785\n",
      "Epoch 4050, Loss: 1.8470207750797272, Final Batch Loss: 0.3742581903934479\n",
      "Epoch 4051, Loss: 1.9124347269535065, Final Batch Loss: 0.28484025597572327\n",
      "Epoch 4052, Loss: 1.8543951511383057, Final Batch Loss: 0.4687824249267578\n",
      "Epoch 4053, Loss: 1.9580107927322388, Final Batch Loss: 0.4511926770210266\n",
      "Epoch 4054, Loss: 1.8694299161434174, Final Batch Loss: 0.4039888083934784\n",
      "Epoch 4055, Loss: 1.8395976126194, Final Batch Loss: 0.3983266055583954\n",
      "Epoch 4056, Loss: 1.7411360144615173, Final Batch Loss: 0.3652781546115875\n",
      "Epoch 4057, Loss: 1.834610939025879, Final Batch Loss: 0.3482650816440582\n",
      "Epoch 4058, Loss: 1.933029055595398, Final Batch Loss: 0.47169286012649536\n",
      "Epoch 4059, Loss: 1.883492887020111, Final Batch Loss: 0.4194738566875458\n",
      "Epoch 4060, Loss: 1.9427664875984192, Final Batch Loss: 0.41759616136550903\n",
      "Epoch 4061, Loss: 1.8593086302280426, Final Batch Loss: 0.3263170123100281\n",
      "Epoch 4062, Loss: 2.149480849504471, Final Batch Loss: 0.5582413673400879\n",
      "Epoch 4063, Loss: 2.000690847635269, Final Batch Loss: 0.3923826217651367\n",
      "Epoch 4064, Loss: 1.9049245715141296, Final Batch Loss: 0.42083659768104553\n",
      "Epoch 4065, Loss: 1.836771935224533, Final Batch Loss: 0.33867210149765015\n",
      "Epoch 4066, Loss: 1.8108254373073578, Final Batch Loss: 0.36427757143974304\n",
      "Epoch 4067, Loss: 1.9069802165031433, Final Batch Loss: 0.27337759733200073\n",
      "Epoch 4068, Loss: 1.734617829322815, Final Batch Loss: 0.31857264041900635\n",
      "Epoch 4069, Loss: 1.9049400985240936, Final Batch Loss: 0.3797069489955902\n",
      "Epoch 4070, Loss: 1.901267558336258, Final Batch Loss: 0.38064610958099365\n",
      "Epoch 4071, Loss: 2.03886279463768, Final Batch Loss: 0.34012818336486816\n",
      "Epoch 4072, Loss: 1.8014632761478424, Final Batch Loss: 0.29849669337272644\n",
      "Epoch 4073, Loss: 1.880667895078659, Final Batch Loss: 0.43248146772384644\n",
      "Epoch 4074, Loss: 1.84895858168602, Final Batch Loss: 0.39289090037345886\n",
      "Epoch 4075, Loss: 2.037220060825348, Final Batch Loss: 0.4787616431713104\n",
      "Epoch 4076, Loss: 1.9136064052581787, Final Batch Loss: 0.3367040157318115\n",
      "Epoch 4077, Loss: 1.8330557346343994, Final Batch Loss: 0.3539135456085205\n",
      "Epoch 4078, Loss: 2.016000360250473, Final Batch Loss: 0.4956569969654083\n",
      "Epoch 4079, Loss: 1.9377143681049347, Final Batch Loss: 0.4371837377548218\n",
      "Epoch 4080, Loss: 1.709664672613144, Final Batch Loss: 0.3233860433101654\n",
      "Epoch 4081, Loss: 1.9243886172771454, Final Batch Loss: 0.4196989834308624\n",
      "Epoch 4082, Loss: 1.998353660106659, Final Batch Loss: 0.4472828507423401\n",
      "Epoch 4083, Loss: 1.9790122509002686, Final Batch Loss: 0.3804214894771576\n",
      "Epoch 4084, Loss: 1.8927085101604462, Final Batch Loss: 0.4247008264064789\n",
      "Epoch 4085, Loss: 1.9253897964954376, Final Batch Loss: 0.4429573118686676\n",
      "Epoch 4086, Loss: 1.8064160645008087, Final Batch Loss: 0.3370085656642914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4087, Loss: 1.8654774129390717, Final Batch Loss: 0.4542982876300812\n",
      "Epoch 4088, Loss: 1.8285686075687408, Final Batch Loss: 0.3453245460987091\n",
      "Epoch 4089, Loss: 1.9660670459270477, Final Batch Loss: 0.32440221309661865\n",
      "Epoch 4090, Loss: 1.890065610408783, Final Batch Loss: 0.3830062448978424\n",
      "Epoch 4091, Loss: 1.9179697930812836, Final Batch Loss: 0.43471577763557434\n",
      "Epoch 4092, Loss: 1.919391691684723, Final Batch Loss: 0.41963183879852295\n",
      "Epoch 4093, Loss: 1.8906721770763397, Final Batch Loss: 0.42513561248779297\n",
      "Epoch 4094, Loss: 1.9626010656356812, Final Batch Loss: 0.3887253403663635\n",
      "Epoch 4095, Loss: 1.6712633967399597, Final Batch Loss: 0.3551209568977356\n",
      "Epoch 4096, Loss: 1.9070657789707184, Final Batch Loss: 0.3420163094997406\n",
      "Epoch 4097, Loss: 1.9437115788459778, Final Batch Loss: 0.43046000599861145\n",
      "Epoch 4098, Loss: 1.8990603685379028, Final Batch Loss: 0.3579407036304474\n",
      "Epoch 4099, Loss: 1.9070857465267181, Final Batch Loss: 0.3240656554698944\n",
      "Epoch 4100, Loss: 1.9256936609745026, Final Batch Loss: 0.364420622587204\n",
      "Epoch 4101, Loss: 2.0374097526073456, Final Batch Loss: 0.4604165256023407\n",
      "Epoch 4102, Loss: 1.8343445658683777, Final Batch Loss: 0.43923237919807434\n",
      "Epoch 4103, Loss: 2.0174277126789093, Final Batch Loss: 0.35188496112823486\n",
      "Epoch 4104, Loss: 1.6567175388336182, Final Batch Loss: 0.32051366567611694\n",
      "Epoch 4105, Loss: 2.0696230828762054, Final Batch Loss: 0.3814358115196228\n",
      "Epoch 4106, Loss: 1.9913690984249115, Final Batch Loss: 0.37268921732902527\n",
      "Epoch 4107, Loss: 1.911670058965683, Final Batch Loss: 0.3460398018360138\n",
      "Epoch 4108, Loss: 2.054654210805893, Final Batch Loss: 0.4272221624851227\n",
      "Epoch 4109, Loss: 1.960185319185257, Final Batch Loss: 0.38392403721809387\n",
      "Epoch 4110, Loss: 1.8982186019420624, Final Batch Loss: 0.4204126298427582\n",
      "Epoch 4111, Loss: 1.7700571119785309, Final Batch Loss: 0.329745888710022\n",
      "Epoch 4112, Loss: 1.8127013742923737, Final Batch Loss: 0.29870110750198364\n",
      "Epoch 4113, Loss: 1.7305116653442383, Final Batch Loss: 0.40764087438583374\n",
      "Epoch 4114, Loss: 2.022510528564453, Final Batch Loss: 0.38170552253723145\n",
      "Epoch 4115, Loss: 1.9886648952960968, Final Batch Loss: 0.42024490237236023\n",
      "Epoch 4116, Loss: 1.7643866837024689, Final Batch Loss: 0.28220441937446594\n",
      "Epoch 4117, Loss: 1.8639044761657715, Final Batch Loss: 0.3499886691570282\n",
      "Epoch 4118, Loss: 1.8152913451194763, Final Batch Loss: 0.3526843786239624\n",
      "Epoch 4119, Loss: 1.990146964788437, Final Batch Loss: 0.3629443347454071\n",
      "Epoch 4120, Loss: 1.811036080121994, Final Batch Loss: 0.3243006467819214\n",
      "Epoch 4121, Loss: 1.8341144025325775, Final Batch Loss: 0.38674870133399963\n",
      "Epoch 4122, Loss: 1.9202325642108917, Final Batch Loss: 0.43825703859329224\n",
      "Epoch 4123, Loss: 1.9095468521118164, Final Batch Loss: 0.4631662666797638\n",
      "Epoch 4124, Loss: 1.7184086740016937, Final Batch Loss: 0.3073241710662842\n",
      "Epoch 4125, Loss: 1.9247947931289673, Final Batch Loss: 0.32059064507484436\n",
      "Epoch 4126, Loss: 1.8897187113761902, Final Batch Loss: 0.4115377962589264\n",
      "Epoch 4127, Loss: 1.9020223021507263, Final Batch Loss: 0.3483864665031433\n",
      "Epoch 4128, Loss: 1.8075141608715057, Final Batch Loss: 0.29396677017211914\n",
      "Epoch 4129, Loss: 1.8990302979946136, Final Batch Loss: 0.47074562311172485\n",
      "Epoch 4130, Loss: 1.857930064201355, Final Batch Loss: 0.38954663276672363\n",
      "Epoch 4131, Loss: 2.1760246455669403, Final Batch Loss: 0.41743287444114685\n",
      "Epoch 4132, Loss: 1.695486158132553, Final Batch Loss: 0.27752983570098877\n",
      "Epoch 4133, Loss: 1.9585925340652466, Final Batch Loss: 0.35957610607147217\n",
      "Epoch 4134, Loss: 1.94247305393219, Final Batch Loss: 0.31767600774765015\n",
      "Epoch 4135, Loss: 1.94893416762352, Final Batch Loss: 0.3989250063896179\n",
      "Epoch 4136, Loss: 1.7659108638763428, Final Batch Loss: 0.3623594343662262\n",
      "Epoch 4137, Loss: 1.848876804113388, Final Batch Loss: 0.471047580242157\n",
      "Epoch 4138, Loss: 2.0065761506557465, Final Batch Loss: 0.36359351873397827\n",
      "Epoch 4139, Loss: 1.83055979013443, Final Batch Loss: 0.35305139422416687\n",
      "Epoch 4140, Loss: 1.9307814538478851, Final Batch Loss: 0.40744102001190186\n",
      "Epoch 4141, Loss: 1.9775286614894867, Final Batch Loss: 0.4199495017528534\n",
      "Epoch 4142, Loss: 1.8001896440982819, Final Batch Loss: 0.5135762691497803\n",
      "Epoch 4143, Loss: 1.8970648348331451, Final Batch Loss: 0.3661547601222992\n",
      "Epoch 4144, Loss: 1.7874119579792023, Final Batch Loss: 0.32477042078971863\n",
      "Epoch 4145, Loss: 1.810183197259903, Final Batch Loss: 0.3309186100959778\n",
      "Epoch 4146, Loss: 1.844131976366043, Final Batch Loss: 0.4147600829601288\n",
      "Epoch 4147, Loss: 1.8652977645397186, Final Batch Loss: 0.3633272349834442\n",
      "Epoch 4148, Loss: 1.6346123814582825, Final Batch Loss: 0.32532328367233276\n",
      "Epoch 4149, Loss: 1.686765193939209, Final Batch Loss: 0.3579886555671692\n",
      "Epoch 4150, Loss: 1.8362055718898773, Final Batch Loss: 0.3814196288585663\n",
      "Epoch 4151, Loss: 1.8278025686740875, Final Batch Loss: 0.373447448015213\n",
      "Epoch 4152, Loss: 1.8361400067806244, Final Batch Loss: 0.4023549258708954\n",
      "Epoch 4153, Loss: 1.9775215089321136, Final Batch Loss: 0.3282979428768158\n",
      "Epoch 4154, Loss: 2.0015455186367035, Final Batch Loss: 0.3980453908443451\n",
      "Epoch 4155, Loss: 1.7558555603027344, Final Batch Loss: 0.3964919447898865\n",
      "Epoch 4156, Loss: 1.7717992067337036, Final Batch Loss: 0.37369170784950256\n",
      "Epoch 4157, Loss: 1.873083621263504, Final Batch Loss: 0.35300779342651367\n",
      "Epoch 4158, Loss: 1.8858775198459625, Final Batch Loss: 0.4416974186897278\n",
      "Epoch 4159, Loss: 1.9019650220870972, Final Batch Loss: 0.37551039457321167\n",
      "Epoch 4160, Loss: 1.8769912719726562, Final Batch Loss: 0.3265431523323059\n",
      "Epoch 4161, Loss: 1.688016563653946, Final Batch Loss: 0.30616071820259094\n",
      "Epoch 4162, Loss: 1.7638162672519684, Final Batch Loss: 0.3060203790664673\n",
      "Epoch 4163, Loss: 1.8368830382823944, Final Batch Loss: 0.2781422436237335\n",
      "Epoch 4164, Loss: 1.8418368995189667, Final Batch Loss: 0.3470813035964966\n",
      "Epoch 4165, Loss: 1.8209663927555084, Final Batch Loss: 0.32389774918556213\n",
      "Epoch 4166, Loss: 1.9862054288387299, Final Batch Loss: 0.4089132845401764\n",
      "Epoch 4167, Loss: 1.8920136094093323, Final Batch Loss: 0.4381988048553467\n",
      "Epoch 4168, Loss: 1.7809607088565826, Final Batch Loss: 0.4022537171840668\n",
      "Epoch 4169, Loss: 1.9680745005607605, Final Batch Loss: 0.3543664813041687\n",
      "Epoch 4170, Loss: 1.7985622882843018, Final Batch Loss: 0.37533631920814514\n",
      "Epoch 4171, Loss: 1.9240872263908386, Final Batch Loss: 0.3915571868419647\n",
      "Epoch 4172, Loss: 1.8303315341472626, Final Batch Loss: 0.36018550395965576\n",
      "Epoch 4173, Loss: 1.8453558385372162, Final Batch Loss: 0.37053734064102173\n",
      "Epoch 4174, Loss: 1.9501137733459473, Final Batch Loss: 0.34947070479393005\n",
      "Epoch 4175, Loss: 1.8322221338748932, Final Batch Loss: 0.4577666223049164\n",
      "Epoch 4176, Loss: 1.8019907176494598, Final Batch Loss: 0.3061148226261139\n",
      "Epoch 4177, Loss: 1.8710595071315765, Final Batch Loss: 0.304495632648468\n",
      "Epoch 4178, Loss: 2.033258020877838, Final Batch Loss: 0.3353714942932129\n",
      "Epoch 4179, Loss: 1.7937471866607666, Final Batch Loss: 0.3467845022678375\n",
      "Epoch 4180, Loss: 2.0963882505893707, Final Batch Loss: 0.4137168526649475\n",
      "Epoch 4181, Loss: 1.774346798658371, Final Batch Loss: 0.3624460995197296\n",
      "Epoch 4182, Loss: 1.931145340204239, Final Batch Loss: 0.3082740902900696\n",
      "Epoch 4183, Loss: 1.8774865567684174, Final Batch Loss: 0.4093036949634552\n",
      "Epoch 4184, Loss: 1.785721242427826, Final Batch Loss: 0.3942102789878845\n",
      "Epoch 4185, Loss: 1.9725362360477448, Final Batch Loss: 0.49986088275909424\n",
      "Epoch 4186, Loss: 1.7117047607898712, Final Batch Loss: 0.2956848442554474\n",
      "Epoch 4187, Loss: 1.9273503720760345, Final Batch Loss: 0.4295331835746765\n",
      "Epoch 4188, Loss: 1.7475436329841614, Final Batch Loss: 0.3189539313316345\n",
      "Epoch 4189, Loss: 2.1036003828048706, Final Batch Loss: 0.39092299342155457\n",
      "Epoch 4190, Loss: 1.8469831347465515, Final Batch Loss: 0.43569862842559814\n",
      "Epoch 4191, Loss: 1.8289909064769745, Final Batch Loss: 0.3704342246055603\n",
      "Epoch 4192, Loss: 1.9007513225078583, Final Batch Loss: 0.4129936397075653\n",
      "Epoch 4193, Loss: 1.7947539687156677, Final Batch Loss: 0.3879895806312561\n",
      "Epoch 4194, Loss: 1.9042423069477081, Final Batch Loss: 0.3193150460720062\n",
      "Epoch 4195, Loss: 1.99267578125, Final Batch Loss: 0.35324984788894653\n",
      "Epoch 4196, Loss: 1.8863097429275513, Final Batch Loss: 0.3587149977684021\n",
      "Epoch 4197, Loss: 1.843100756406784, Final Batch Loss: 0.37038183212280273\n",
      "Epoch 4198, Loss: 1.8796185553073883, Final Batch Loss: 0.4541001319885254\n",
      "Epoch 4199, Loss: 1.827063262462616, Final Batch Loss: 0.29194676876068115\n",
      "Epoch 4200, Loss: 1.92979234457016, Final Batch Loss: 0.39931705594062805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4201, Loss: 1.7817080318927765, Final Batch Loss: 0.2979496121406555\n",
      "Epoch 4202, Loss: 1.8176425099372864, Final Batch Loss: 0.339679479598999\n",
      "Epoch 4203, Loss: 1.9263059198856354, Final Batch Loss: 0.42155787348747253\n",
      "Epoch 4204, Loss: 1.9858132600784302, Final Batch Loss: 0.4216082990169525\n",
      "Epoch 4205, Loss: 1.7730233669281006, Final Batch Loss: 0.3598431944847107\n",
      "Epoch 4206, Loss: 1.7516989707946777, Final Batch Loss: 0.3369987905025482\n",
      "Epoch 4207, Loss: 1.9020425081253052, Final Batch Loss: 0.38286828994750977\n",
      "Epoch 4208, Loss: 1.7584803104400635, Final Batch Loss: 0.3089691996574402\n",
      "Epoch 4209, Loss: 1.928918719291687, Final Batch Loss: 0.3651498258113861\n",
      "Epoch 4210, Loss: 2.0468180775642395, Final Batch Loss: 0.3960633873939514\n",
      "Epoch 4211, Loss: 1.8377533853054047, Final Batch Loss: 0.42232969403266907\n",
      "Epoch 4212, Loss: 1.8371612131595612, Final Batch Loss: 0.3619859516620636\n",
      "Epoch 4213, Loss: 1.898701548576355, Final Batch Loss: 0.37313592433929443\n",
      "Epoch 4214, Loss: 1.8906419277191162, Final Batch Loss: 0.36416080594062805\n",
      "Epoch 4215, Loss: 1.9072979092597961, Final Batch Loss: 0.4457471966743469\n",
      "Epoch 4216, Loss: 1.8859517574310303, Final Batch Loss: 0.37593501806259155\n",
      "Epoch 4217, Loss: 2.0385981500148773, Final Batch Loss: 0.31976625323295593\n",
      "Epoch 4218, Loss: 2.013199418783188, Final Batch Loss: 0.45244571566581726\n",
      "Epoch 4219, Loss: 1.7736510038375854, Final Batch Loss: 0.3890453577041626\n",
      "Epoch 4220, Loss: 1.8141807913780212, Final Batch Loss: 0.36473339796066284\n",
      "Epoch 4221, Loss: 1.8299376368522644, Final Batch Loss: 0.2902413308620453\n",
      "Epoch 4222, Loss: 1.7308815717697144, Final Batch Loss: 0.38090816140174866\n",
      "Epoch 4223, Loss: 1.8772284984588623, Final Batch Loss: 0.363535612821579\n",
      "Epoch 4224, Loss: 1.8137600719928741, Final Batch Loss: 0.3576151728630066\n",
      "Epoch 4225, Loss: 1.7819717526435852, Final Batch Loss: 0.3838513493537903\n",
      "Epoch 4226, Loss: 1.9846109449863434, Final Batch Loss: 0.4746977388858795\n",
      "Epoch 4227, Loss: 1.8490458726882935, Final Batch Loss: 0.36596086621284485\n",
      "Epoch 4228, Loss: 1.811578780412674, Final Batch Loss: 0.3550274670124054\n",
      "Epoch 4229, Loss: 1.8609159588813782, Final Batch Loss: 0.2933735251426697\n",
      "Epoch 4230, Loss: 1.8061981797218323, Final Batch Loss: 0.40747034549713135\n",
      "Epoch 4231, Loss: 1.9057863056659698, Final Batch Loss: 0.35068321228027344\n",
      "Epoch 4232, Loss: 1.8576417863368988, Final Batch Loss: 0.32998624444007874\n",
      "Epoch 4233, Loss: 1.890043556690216, Final Batch Loss: 0.35834765434265137\n",
      "Epoch 4234, Loss: 1.7909586429595947, Final Batch Loss: 0.3506162464618683\n",
      "Epoch 4235, Loss: 1.8767207264900208, Final Batch Loss: 0.4333622455596924\n",
      "Epoch 4236, Loss: 1.7838830947875977, Final Batch Loss: 0.31747597455978394\n",
      "Epoch 4237, Loss: 1.7744731605052948, Final Batch Loss: 0.3586893081665039\n",
      "Epoch 4238, Loss: 1.9064949452877045, Final Batch Loss: 0.41580966114997864\n",
      "Epoch 4239, Loss: 1.8547676503658295, Final Batch Loss: 0.36075735092163086\n",
      "Epoch 4240, Loss: 1.8731396794319153, Final Batch Loss: 0.3979852497577667\n",
      "Epoch 4241, Loss: 1.784639298915863, Final Batch Loss: 0.36981454491615295\n",
      "Epoch 4242, Loss: 1.7566337585449219, Final Batch Loss: 0.34190502762794495\n",
      "Epoch 4243, Loss: 1.7705735564231873, Final Batch Loss: 0.3724461495876312\n",
      "Epoch 4244, Loss: 1.7518892884254456, Final Batch Loss: 0.40233463048934937\n",
      "Epoch 4245, Loss: 1.7104096412658691, Final Batch Loss: 0.3263600170612335\n",
      "Epoch 4246, Loss: 1.9860282242298126, Final Batch Loss: 0.46335911750793457\n",
      "Epoch 4247, Loss: 1.8385469019412994, Final Batch Loss: 0.41297319531440735\n",
      "Epoch 4248, Loss: 1.9438626766204834, Final Batch Loss: 0.31345754861831665\n",
      "Epoch 4249, Loss: 2.019525319337845, Final Batch Loss: 0.322502076625824\n",
      "Epoch 4250, Loss: 1.8218536078929901, Final Batch Loss: 0.39319390058517456\n",
      "Epoch 4251, Loss: 2.0318880677223206, Final Batch Loss: 0.4166128635406494\n",
      "Epoch 4252, Loss: 1.9275545477867126, Final Batch Loss: 0.40787529945373535\n",
      "Epoch 4253, Loss: 1.8586851954460144, Final Batch Loss: 0.41343632340431213\n",
      "Epoch 4254, Loss: 1.8426727950572968, Final Batch Loss: 0.37439975142478943\n",
      "Epoch 4255, Loss: 1.887430489063263, Final Batch Loss: 0.4210379719734192\n",
      "Epoch 4256, Loss: 1.8361550569534302, Final Batch Loss: 0.3631834387779236\n",
      "Epoch 4257, Loss: 1.8973559439182281, Final Batch Loss: 0.4650426208972931\n",
      "Epoch 4258, Loss: 1.8021220862865448, Final Batch Loss: 0.3199881613254547\n",
      "Epoch 4259, Loss: 1.7700179368257523, Final Batch Loss: 0.4307672083377838\n",
      "Epoch 4260, Loss: 1.9333559274673462, Final Batch Loss: 0.3333825170993805\n",
      "Epoch 4261, Loss: 1.9623386561870575, Final Batch Loss: 0.38600847125053406\n",
      "Epoch 4262, Loss: 1.8561893105506897, Final Batch Loss: 0.4450419247150421\n",
      "Epoch 4263, Loss: 1.9919708371162415, Final Batch Loss: 0.4193655550479889\n",
      "Epoch 4264, Loss: 1.8178885877132416, Final Batch Loss: 0.2686541676521301\n",
      "Epoch 4265, Loss: 1.8855116665363312, Final Batch Loss: 0.3683135211467743\n",
      "Epoch 4266, Loss: 1.8285681009292603, Final Batch Loss: 0.2604731619358063\n",
      "Epoch 4267, Loss: 1.9449604749679565, Final Batch Loss: 0.3924856185913086\n",
      "Epoch 4268, Loss: 1.9931893050670624, Final Batch Loss: 0.34187281131744385\n",
      "Epoch 4269, Loss: 1.8340392708778381, Final Batch Loss: 0.397732138633728\n",
      "Epoch 4270, Loss: 1.8053759932518005, Final Batch Loss: 0.3416688144207001\n",
      "Epoch 4271, Loss: 1.8317635655403137, Final Batch Loss: 0.3131721317768097\n",
      "Epoch 4272, Loss: 2.0645858347415924, Final Batch Loss: 0.31465446949005127\n",
      "Epoch 4273, Loss: 1.8538373708724976, Final Batch Loss: 0.3471209704875946\n",
      "Epoch 4274, Loss: 1.8864011466503143, Final Batch Loss: 0.3552449941635132\n",
      "Epoch 4275, Loss: 1.906255692243576, Final Batch Loss: 0.3773402273654938\n",
      "Epoch 4276, Loss: 1.9173442721366882, Final Batch Loss: 0.44416412711143494\n",
      "Epoch 4277, Loss: 1.894182711839676, Final Batch Loss: 0.42945796251296997\n",
      "Epoch 4278, Loss: 1.9657461047172546, Final Batch Loss: 0.4340026080608368\n",
      "Epoch 4279, Loss: 1.8971137404441833, Final Batch Loss: 0.36472785472869873\n",
      "Epoch 4280, Loss: 1.8367761075496674, Final Batch Loss: 0.34499591588974\n",
      "Epoch 4281, Loss: 1.8561526834964752, Final Batch Loss: 0.3602263331413269\n",
      "Epoch 4282, Loss: 1.8140986561775208, Final Batch Loss: 0.29839247465133667\n",
      "Epoch 4283, Loss: 1.7977311611175537, Final Batch Loss: 0.3848749101161957\n",
      "Epoch 4284, Loss: 1.7972405850887299, Final Batch Loss: 0.4064328968524933\n",
      "Epoch 4285, Loss: 1.6322037279605865, Final Batch Loss: 0.3938567042350769\n",
      "Epoch 4286, Loss: 1.8151978552341461, Final Batch Loss: 0.4148435592651367\n",
      "Epoch 4287, Loss: 1.7454741597175598, Final Batch Loss: 0.329026460647583\n",
      "Epoch 4288, Loss: 1.8722193539142609, Final Batch Loss: 0.44688960909843445\n",
      "Epoch 4289, Loss: 1.928795874118805, Final Batch Loss: 0.48190921545028687\n",
      "Epoch 4290, Loss: 1.744726449251175, Final Batch Loss: 0.373363733291626\n",
      "Epoch 4291, Loss: 1.7469376921653748, Final Batch Loss: 0.3538561761379242\n",
      "Epoch 4292, Loss: 1.8794181048870087, Final Batch Loss: 0.322678804397583\n",
      "Epoch 4293, Loss: 1.8835927546024323, Final Batch Loss: 0.28633633255958557\n",
      "Epoch 4294, Loss: 1.8067246973514557, Final Batch Loss: 0.35729655623435974\n",
      "Epoch 4295, Loss: 1.79265558719635, Final Batch Loss: 0.3146542012691498\n",
      "Epoch 4296, Loss: 1.8117346465587616, Final Batch Loss: 0.39821407198905945\n",
      "Epoch 4297, Loss: 1.895365297794342, Final Batch Loss: 0.3397325873374939\n",
      "Epoch 4298, Loss: 1.9480283856391907, Final Batch Loss: 0.3764796555042267\n",
      "Epoch 4299, Loss: 1.9509789943695068, Final Batch Loss: 0.4614763855934143\n",
      "Epoch 4300, Loss: 2.0510780215263367, Final Batch Loss: 0.38479411602020264\n",
      "Epoch 4301, Loss: 1.8912030458450317, Final Batch Loss: 0.36703255772590637\n",
      "Epoch 4302, Loss: 1.9376572966575623, Final Batch Loss: 0.35148295760154724\n",
      "Epoch 4303, Loss: 1.90693998336792, Final Batch Loss: 0.4868408441543579\n",
      "Epoch 4304, Loss: 1.8008450865745544, Final Batch Loss: 0.39553603529930115\n",
      "Epoch 4305, Loss: 2.1076011657714844, Final Batch Loss: 0.3453657925128937\n",
      "Epoch 4306, Loss: 1.9695258736610413, Final Batch Loss: 0.39375680685043335\n",
      "Epoch 4307, Loss: 1.7806282937526703, Final Batch Loss: 0.3520801365375519\n",
      "Epoch 4308, Loss: 1.7504341900348663, Final Batch Loss: 0.4252684712409973\n",
      "Epoch 4309, Loss: 1.7928526699543, Final Batch Loss: 0.3543078303337097\n",
      "Epoch 4310, Loss: 1.8182648718357086, Final Batch Loss: 0.39173954725265503\n",
      "Epoch 4311, Loss: 1.8820176124572754, Final Batch Loss: 0.4146210849285126\n",
      "Epoch 4312, Loss: 1.8335196673870087, Final Batch Loss: 0.41269969940185547\n",
      "Epoch 4313, Loss: 1.7748185396194458, Final Batch Loss: 0.3427935540676117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4314, Loss: 1.7639665007591248, Final Batch Loss: 0.3041709065437317\n",
      "Epoch 4315, Loss: 1.8519915640354156, Final Batch Loss: 0.4781811833381653\n",
      "Epoch 4316, Loss: 1.945770114660263, Final Batch Loss: 0.3692111074924469\n",
      "Epoch 4317, Loss: 1.7515921592712402, Final Batch Loss: 0.3698573410511017\n",
      "Epoch 4318, Loss: 1.9143488109111786, Final Batch Loss: 0.3913879692554474\n",
      "Epoch 4319, Loss: 1.8920174539089203, Final Batch Loss: 0.42397597432136536\n",
      "Epoch 4320, Loss: 1.9790920615196228, Final Batch Loss: 0.4916026294231415\n",
      "Epoch 4321, Loss: 1.9242285192012787, Final Batch Loss: 0.3305717706680298\n",
      "Epoch 4322, Loss: 1.9471918046474457, Final Batch Loss: 0.4936436116695404\n",
      "Epoch 4323, Loss: 1.7974345088005066, Final Batch Loss: 0.34929463267326355\n",
      "Epoch 4324, Loss: 1.9854725301265717, Final Batch Loss: 0.44124218821525574\n",
      "Epoch 4325, Loss: 1.8312384188175201, Final Batch Loss: 0.40510883927345276\n",
      "Epoch 4326, Loss: 1.9136535227298737, Final Batch Loss: 0.40699341893196106\n",
      "Epoch 4327, Loss: 1.889557272195816, Final Batch Loss: 0.34168580174446106\n",
      "Epoch 4328, Loss: 1.8688311278820038, Final Batch Loss: 0.40016013383865356\n",
      "Epoch 4329, Loss: 1.8602217435836792, Final Batch Loss: 0.43235617876052856\n",
      "Epoch 4330, Loss: 1.8567488193511963, Final Batch Loss: 0.3839220404624939\n",
      "Epoch 4331, Loss: 1.9710164368152618, Final Batch Loss: 0.3793945610523224\n",
      "Epoch 4332, Loss: 1.9410870671272278, Final Batch Loss: 0.395861953496933\n",
      "Epoch 4333, Loss: 1.7277322113513947, Final Batch Loss: 0.3127118945121765\n",
      "Epoch 4334, Loss: 2.112892806529999, Final Batch Loss: 0.4813169240951538\n",
      "Epoch 4335, Loss: 1.7177526354789734, Final Batch Loss: 0.31217333674430847\n",
      "Epoch 4336, Loss: 1.7275229394435883, Final Batch Loss: 0.286237508058548\n",
      "Epoch 4337, Loss: 1.8834710121154785, Final Batch Loss: 0.44929781556129456\n",
      "Epoch 4338, Loss: 1.9673081934452057, Final Batch Loss: 0.43359220027923584\n",
      "Epoch 4339, Loss: 1.8392362594604492, Final Batch Loss: 0.4571757912635803\n",
      "Epoch 4340, Loss: 1.9650256037712097, Final Batch Loss: 0.372977077960968\n",
      "Epoch 4341, Loss: 1.8554879128932953, Final Batch Loss: 0.4532841145992279\n",
      "Epoch 4342, Loss: 2.0169133245944977, Final Batch Loss: 0.34839603304862976\n",
      "Epoch 4343, Loss: 1.8169388473033905, Final Batch Loss: 0.3085186779499054\n",
      "Epoch 4344, Loss: 1.814117580652237, Final Batch Loss: 0.27360081672668457\n",
      "Epoch 4345, Loss: 1.889252632856369, Final Batch Loss: 0.5609312057495117\n",
      "Epoch 4346, Loss: 1.7986253798007965, Final Batch Loss: 0.2898940443992615\n",
      "Epoch 4347, Loss: 1.8833334147930145, Final Batch Loss: 0.42219647765159607\n",
      "Epoch 4348, Loss: 1.9513767659664154, Final Batch Loss: 0.37856388092041016\n",
      "Epoch 4349, Loss: 1.8073053061962128, Final Batch Loss: 0.38349100947380066\n",
      "Epoch 4350, Loss: 1.983929306268692, Final Batch Loss: 0.4605625569820404\n",
      "Epoch 4351, Loss: 1.981153964996338, Final Batch Loss: 0.332706481218338\n",
      "Epoch 4352, Loss: 1.83914253115654, Final Batch Loss: 0.38992273807525635\n",
      "Epoch 4353, Loss: 1.8851096332073212, Final Batch Loss: 0.38626810908317566\n",
      "Epoch 4354, Loss: 1.8320248126983643, Final Batch Loss: 0.3058651387691498\n",
      "Epoch 4355, Loss: 1.9253256022930145, Final Batch Loss: 0.4205525517463684\n",
      "Epoch 4356, Loss: 1.8123054206371307, Final Batch Loss: 0.38390231132507324\n",
      "Epoch 4357, Loss: 1.8004330396652222, Final Batch Loss: 0.36107417941093445\n",
      "Epoch 4358, Loss: 1.8171123266220093, Final Batch Loss: 0.3761461079120636\n",
      "Epoch 4359, Loss: 1.970792293548584, Final Batch Loss: 0.4426584839820862\n",
      "Epoch 4360, Loss: 1.843328982591629, Final Batch Loss: 0.3497253954410553\n",
      "Epoch 4361, Loss: 1.842225432395935, Final Batch Loss: 0.3022388815879822\n",
      "Epoch 4362, Loss: 1.8001333475112915, Final Batch Loss: 0.3116242587566376\n",
      "Epoch 4363, Loss: 1.7762073278427124, Final Batch Loss: 0.3411431610584259\n",
      "Epoch 4364, Loss: 1.8763616681098938, Final Batch Loss: 0.31460097432136536\n",
      "Epoch 4365, Loss: 1.819486379623413, Final Batch Loss: 0.37809813022613525\n",
      "Epoch 4366, Loss: 1.8516316413879395, Final Batch Loss: 0.33143532276153564\n",
      "Epoch 4367, Loss: 1.8478727340698242, Final Batch Loss: 0.33310312032699585\n",
      "Epoch 4368, Loss: 1.8459453284740448, Final Batch Loss: 0.31695985794067383\n",
      "Epoch 4369, Loss: 1.899384319782257, Final Batch Loss: 0.3249656856060028\n",
      "Epoch 4370, Loss: 1.9449499547481537, Final Batch Loss: 0.4030880928039551\n",
      "Epoch 4371, Loss: 2.112516462802887, Final Batch Loss: 0.4385360777378082\n",
      "Epoch 4372, Loss: 1.8961436450481415, Final Batch Loss: 0.356071412563324\n",
      "Epoch 4373, Loss: 1.8608488142490387, Final Batch Loss: 0.34306421875953674\n",
      "Epoch 4374, Loss: 1.6388434767723083, Final Batch Loss: 0.31855762004852295\n",
      "Epoch 4375, Loss: 1.824553370475769, Final Batch Loss: 0.3888605833053589\n",
      "Epoch 4376, Loss: 1.8160044848918915, Final Batch Loss: 0.3275623917579651\n",
      "Epoch 4377, Loss: 1.8717008829116821, Final Batch Loss: 0.30100274085998535\n",
      "Epoch 4378, Loss: 1.7902849614620209, Final Batch Loss: 0.3252444267272949\n",
      "Epoch 4379, Loss: 1.7991872131824493, Final Batch Loss: 0.346588671207428\n",
      "Epoch 4380, Loss: 2.0347225964069366, Final Batch Loss: 0.5067386627197266\n",
      "Epoch 4381, Loss: 1.8935173749923706, Final Batch Loss: 0.33333292603492737\n",
      "Epoch 4382, Loss: 1.907324731349945, Final Batch Loss: 0.3873574435710907\n",
      "Epoch 4383, Loss: 1.9284024834632874, Final Batch Loss: 0.3952998220920563\n",
      "Epoch 4384, Loss: 1.8435889780521393, Final Batch Loss: 0.33092230558395386\n",
      "Epoch 4385, Loss: 1.9404954016208649, Final Batch Loss: 0.3374202847480774\n",
      "Epoch 4386, Loss: 1.6721198856830597, Final Batch Loss: 0.2688150703907013\n",
      "Epoch 4387, Loss: 1.9841443002223969, Final Batch Loss: 0.4057116210460663\n",
      "Epoch 4388, Loss: 2.0318984389305115, Final Batch Loss: 0.31617099046707153\n",
      "Epoch 4389, Loss: 1.807979941368103, Final Batch Loss: 0.3162384629249573\n",
      "Epoch 4390, Loss: 1.8298722803592682, Final Batch Loss: 0.43751099705696106\n",
      "Epoch 4391, Loss: 1.9337623417377472, Final Batch Loss: 0.4261927306652069\n",
      "Epoch 4392, Loss: 1.690234124660492, Final Batch Loss: 0.34832340478897095\n",
      "Epoch 4393, Loss: 1.8517272770404816, Final Batch Loss: 0.37823668122291565\n",
      "Epoch 4394, Loss: 1.7952580451965332, Final Batch Loss: 0.2760590612888336\n",
      "Epoch 4395, Loss: 1.7740545868873596, Final Batch Loss: 0.40981999039649963\n",
      "Epoch 4396, Loss: 1.8682951033115387, Final Batch Loss: 0.36610332131385803\n",
      "Epoch 4397, Loss: 1.8721297979354858, Final Batch Loss: 0.28727051615715027\n",
      "Epoch 4398, Loss: 1.9199516475200653, Final Batch Loss: 0.31279775500297546\n",
      "Epoch 4399, Loss: 1.7547358870506287, Final Batch Loss: 0.27657198905944824\n",
      "Epoch 4400, Loss: 1.7588708698749542, Final Batch Loss: 0.36334770917892456\n",
      "Epoch 4401, Loss: 1.784428745508194, Final Batch Loss: 0.3738345205783844\n",
      "Epoch 4402, Loss: 1.692099153995514, Final Batch Loss: 0.31796616315841675\n",
      "Epoch 4403, Loss: 1.6876049935817719, Final Batch Loss: 0.3497192859649658\n",
      "Epoch 4404, Loss: 1.945927768945694, Final Batch Loss: 0.36383044719696045\n",
      "Epoch 4405, Loss: 1.8905021250247955, Final Batch Loss: 0.45615774393081665\n",
      "Epoch 4406, Loss: 1.9394378662109375, Final Batch Loss: 0.36174818873405457\n",
      "Epoch 4407, Loss: 2.2240631878376007, Final Batch Loss: 0.4267486035823822\n",
      "Epoch 4408, Loss: 1.7951688915491104, Final Batch Loss: 0.3888772428035736\n",
      "Epoch 4409, Loss: 1.9932919144630432, Final Batch Loss: 0.3795069456100464\n",
      "Epoch 4410, Loss: 1.8003846108913422, Final Batch Loss: 0.3251625895500183\n",
      "Epoch 4411, Loss: 2.0163917541503906, Final Batch Loss: 0.42796915769577026\n",
      "Epoch 4412, Loss: 1.8341877162456512, Final Batch Loss: 0.4668196439743042\n",
      "Epoch 4413, Loss: 1.9819523692131042, Final Batch Loss: 0.3956989347934723\n",
      "Epoch 4414, Loss: 1.9464085400104523, Final Batch Loss: 0.36959052085876465\n",
      "Epoch 4415, Loss: 1.9552434980869293, Final Batch Loss: 0.41888049244880676\n",
      "Epoch 4416, Loss: 1.9265839457511902, Final Batch Loss: 0.35125645995140076\n",
      "Epoch 4417, Loss: 1.8698035180568695, Final Batch Loss: 0.3653147220611572\n",
      "Epoch 4418, Loss: 1.8800727427005768, Final Batch Loss: 0.40189120173454285\n",
      "Epoch 4419, Loss: 1.8151344954967499, Final Batch Loss: 0.40323182940483093\n",
      "Epoch 4420, Loss: 1.8108924329280853, Final Batch Loss: 0.397053599357605\n",
      "Epoch 4421, Loss: 2.0095303058624268, Final Batch Loss: 0.45503634214401245\n",
      "Epoch 4422, Loss: 1.7570070326328278, Final Batch Loss: 0.37558701634407043\n",
      "Epoch 4423, Loss: 1.8429388999938965, Final Batch Loss: 0.3840431272983551\n",
      "Epoch 4424, Loss: 1.8365237414836884, Final Batch Loss: 0.33224257826805115\n",
      "Epoch 4425, Loss: 1.90945702791214, Final Batch Loss: 0.3736506402492523\n",
      "Epoch 4426, Loss: 1.8163845837116241, Final Batch Loss: 0.4085908532142639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4427, Loss: 1.8371221721172333, Final Batch Loss: 0.25888916850090027\n",
      "Epoch 4428, Loss: 1.9417167901992798, Final Batch Loss: 0.28082582354545593\n",
      "Epoch 4429, Loss: 1.92672997713089, Final Batch Loss: 0.42935147881507874\n",
      "Epoch 4430, Loss: 1.8203694224357605, Final Batch Loss: 0.4657551944255829\n",
      "Epoch 4431, Loss: 2.214835673570633, Final Batch Loss: 0.37189263105392456\n",
      "Epoch 4432, Loss: 1.8231333494186401, Final Batch Loss: 0.3678444027900696\n",
      "Epoch 4433, Loss: 1.832588404417038, Final Batch Loss: 0.26010552048683167\n",
      "Epoch 4434, Loss: 1.9966470301151276, Final Batch Loss: 0.40104925632476807\n",
      "Epoch 4435, Loss: 1.8761211037635803, Final Batch Loss: 0.37038350105285645\n",
      "Epoch 4436, Loss: 1.8843438029289246, Final Batch Loss: 0.389034241437912\n",
      "Epoch 4437, Loss: 1.9394381940364838, Final Batch Loss: 0.3709736168384552\n",
      "Epoch 4438, Loss: 1.8735077679157257, Final Batch Loss: 0.41251230239868164\n",
      "Epoch 4439, Loss: 1.7671498358249664, Final Batch Loss: 0.33622708916664124\n",
      "Epoch 4440, Loss: 1.6675075888633728, Final Batch Loss: 0.3205413520336151\n",
      "Epoch 4441, Loss: 1.8338290750980377, Final Batch Loss: 0.325825035572052\n",
      "Epoch 4442, Loss: 1.8359830677509308, Final Batch Loss: 0.27299922704696655\n",
      "Epoch 4443, Loss: 1.8151653110980988, Final Batch Loss: 0.2904881536960602\n",
      "Epoch 4444, Loss: 1.8639595210552216, Final Batch Loss: 0.3506557047367096\n",
      "Epoch 4445, Loss: 2.018911898136139, Final Batch Loss: 0.4244420528411865\n",
      "Epoch 4446, Loss: 1.8559277653694153, Final Batch Loss: 0.40975913405418396\n",
      "Epoch 4447, Loss: 1.9109694063663483, Final Batch Loss: 0.36724230647087097\n",
      "Epoch 4448, Loss: 1.8290331661701202, Final Batch Loss: 0.35659199953079224\n",
      "Epoch 4449, Loss: 1.9717770218849182, Final Batch Loss: 0.3734379708766937\n",
      "Epoch 4450, Loss: 1.9275307655334473, Final Batch Loss: 0.46310627460479736\n",
      "Epoch 4451, Loss: 1.7255233228206635, Final Batch Loss: 0.3378821909427643\n",
      "Epoch 4452, Loss: 1.9690269529819489, Final Batch Loss: 0.34347450733184814\n",
      "Epoch 4453, Loss: 1.7297709584236145, Final Batch Loss: 0.299693763256073\n",
      "Epoch 4454, Loss: 1.7336325645446777, Final Batch Loss: 0.37936025857925415\n",
      "Epoch 4455, Loss: 1.9288437068462372, Final Batch Loss: 0.37187373638153076\n",
      "Epoch 4456, Loss: 1.853954404592514, Final Batch Loss: 0.33565056324005127\n",
      "Epoch 4457, Loss: 1.8238660991191864, Final Batch Loss: 0.31789204478263855\n",
      "Epoch 4458, Loss: 1.8157873153686523, Final Batch Loss: 0.3725118041038513\n",
      "Epoch 4459, Loss: 1.8530340492725372, Final Batch Loss: 0.39807435870170593\n",
      "Epoch 4460, Loss: 1.786982148885727, Final Batch Loss: 0.2988232970237732\n",
      "Epoch 4461, Loss: 1.8432320952415466, Final Batch Loss: 0.4712013900279999\n",
      "Epoch 4462, Loss: 1.9386788606643677, Final Batch Loss: 0.39621785283088684\n",
      "Epoch 4463, Loss: 1.6704341173171997, Final Batch Loss: 0.3034398555755615\n",
      "Epoch 4464, Loss: 1.903739482164383, Final Batch Loss: 0.3717953860759735\n",
      "Epoch 4465, Loss: 1.7664843499660492, Final Batch Loss: 0.34691181778907776\n",
      "Epoch 4466, Loss: 1.6875943839550018, Final Batch Loss: 0.37696391344070435\n",
      "Epoch 4467, Loss: 1.7006512582302094, Final Batch Loss: 0.3819683790206909\n",
      "Epoch 4468, Loss: 1.7956531643867493, Final Batch Loss: 0.38571619987487793\n",
      "Epoch 4469, Loss: 2.0143165588378906, Final Batch Loss: 0.38259032368659973\n",
      "Epoch 4470, Loss: 1.7310338020324707, Final Batch Loss: 0.33114856481552124\n",
      "Epoch 4471, Loss: 1.946111798286438, Final Batch Loss: 0.381976455450058\n",
      "Epoch 4472, Loss: 1.6668617725372314, Final Batch Loss: 0.26110199093818665\n",
      "Epoch 4473, Loss: 1.864167869091034, Final Batch Loss: 0.31463974714279175\n",
      "Epoch 4474, Loss: 1.8433925211429596, Final Batch Loss: 0.3485707938671112\n",
      "Epoch 4475, Loss: 1.8594486713409424, Final Batch Loss: 0.2830265462398529\n",
      "Epoch 4476, Loss: 1.8311322629451752, Final Batch Loss: 0.39315536618232727\n",
      "Epoch 4477, Loss: 1.7409485876560211, Final Batch Loss: 0.33315807580947876\n",
      "Epoch 4478, Loss: 1.6863441467285156, Final Batch Loss: 0.30309176445007324\n",
      "Epoch 4479, Loss: 1.7423956096172333, Final Batch Loss: 0.3502923250198364\n",
      "Epoch 4480, Loss: 1.6796877086162567, Final Batch Loss: 0.36371442675590515\n",
      "Epoch 4481, Loss: 1.7248305678367615, Final Batch Loss: 0.29601818323135376\n",
      "Epoch 4482, Loss: 1.7232931852340698, Final Batch Loss: 0.4099069833755493\n",
      "Epoch 4483, Loss: 1.842227190732956, Final Batch Loss: 0.3802272379398346\n",
      "Epoch 4484, Loss: 1.9611498415470123, Final Batch Loss: 0.5009254217147827\n",
      "Epoch 4485, Loss: 2.0986522138118744, Final Batch Loss: 0.38079652190208435\n",
      "Epoch 4486, Loss: 1.8112888038158417, Final Batch Loss: 0.29771342873573303\n",
      "Epoch 4487, Loss: 1.6843620240688324, Final Batch Loss: 0.3586219847202301\n",
      "Epoch 4488, Loss: 1.8816079795360565, Final Batch Loss: 0.414156973361969\n",
      "Epoch 4489, Loss: 1.895904004573822, Final Batch Loss: 0.30055078864097595\n",
      "Epoch 4490, Loss: 1.93511164188385, Final Batch Loss: 0.33965998888015747\n",
      "Epoch 4491, Loss: 1.6027473658323288, Final Batch Loss: 0.3300752341747284\n",
      "Epoch 4492, Loss: 1.870934009552002, Final Batch Loss: 0.41710540652275085\n",
      "Epoch 4493, Loss: 1.8196607828140259, Final Batch Loss: 0.33886295557022095\n",
      "Epoch 4494, Loss: 1.7491147220134735, Final Batch Loss: 0.365464985370636\n",
      "Epoch 4495, Loss: 1.7903432250022888, Final Batch Loss: 0.3750382959842682\n",
      "Epoch 4496, Loss: 1.9157558977603912, Final Batch Loss: 0.4218108654022217\n",
      "Epoch 4497, Loss: 1.8313523530960083, Final Batch Loss: 0.4374030828475952\n",
      "Epoch 4498, Loss: 1.8514591455459595, Final Batch Loss: 0.3848411440849304\n",
      "Epoch 4499, Loss: 1.7250547111034393, Final Batch Loss: 0.3516276180744171\n",
      "Epoch 4500, Loss: 1.9482459425926208, Final Batch Loss: 0.5200218558311462\n",
      "Epoch 4501, Loss: 2.00617977976799, Final Batch Loss: 0.5417926907539368\n",
      "Epoch 4502, Loss: 1.9280024468898773, Final Batch Loss: 0.40616557002067566\n",
      "Epoch 4503, Loss: 1.9744983315467834, Final Batch Loss: 0.3205629587173462\n",
      "Epoch 4504, Loss: 1.8756247758865356, Final Batch Loss: 0.39789193868637085\n",
      "Epoch 4505, Loss: 1.9626553058624268, Final Batch Loss: 0.2871912717819214\n",
      "Epoch 4506, Loss: 1.8079426288604736, Final Batch Loss: 0.327862024307251\n",
      "Epoch 4507, Loss: 1.8080497086048126, Final Batch Loss: 0.38235506415367126\n",
      "Epoch 4508, Loss: 2.026046544313431, Final Batch Loss: 0.32389530539512634\n",
      "Epoch 4509, Loss: 1.8307673037052155, Final Batch Loss: 0.36094406247138977\n",
      "Epoch 4510, Loss: 1.7952617704868317, Final Batch Loss: 0.3420737385749817\n",
      "Epoch 4511, Loss: 1.945719599723816, Final Batch Loss: 0.3672924041748047\n",
      "Epoch 4512, Loss: 1.7564201951026917, Final Batch Loss: 0.3925665318965912\n",
      "Epoch 4513, Loss: 1.7924247980117798, Final Batch Loss: 0.29275596141815186\n",
      "Epoch 4514, Loss: 1.9693170487880707, Final Batch Loss: 0.2985724210739136\n",
      "Epoch 4515, Loss: 1.8257253170013428, Final Batch Loss: 0.33500605821609497\n",
      "Epoch 4516, Loss: 1.9529437124729156, Final Batch Loss: 0.39599597454071045\n",
      "Epoch 4517, Loss: 1.7698883712291718, Final Batch Loss: 0.33790940046310425\n",
      "Epoch 4518, Loss: 1.9367942214012146, Final Batch Loss: 0.472686767578125\n",
      "Epoch 4519, Loss: 1.916810154914856, Final Batch Loss: 0.5325091481208801\n",
      "Epoch 4520, Loss: 1.7686499655246735, Final Batch Loss: 0.31636250019073486\n",
      "Epoch 4521, Loss: 1.8202985525131226, Final Batch Loss: 0.35227203369140625\n",
      "Epoch 4522, Loss: 1.790399432182312, Final Batch Loss: 0.3526201844215393\n",
      "Epoch 4523, Loss: 1.7310176193714142, Final Batch Loss: 0.36568957567214966\n",
      "Epoch 4524, Loss: 1.880570501089096, Final Batch Loss: 0.33893775939941406\n",
      "Epoch 4525, Loss: 1.7469281256198883, Final Batch Loss: 0.41273126006126404\n",
      "Epoch 4526, Loss: 1.9341121315956116, Final Batch Loss: 0.32864731550216675\n",
      "Epoch 4527, Loss: 1.987039476633072, Final Batch Loss: 0.432319313287735\n",
      "Epoch 4528, Loss: 1.7178872525691986, Final Batch Loss: 0.35229116678237915\n",
      "Epoch 4529, Loss: 1.7855393886566162, Final Batch Loss: 0.39144548773765564\n",
      "Epoch 4530, Loss: 1.8929525911808014, Final Batch Loss: 0.3777647912502289\n",
      "Epoch 4531, Loss: 1.8851878345012665, Final Batch Loss: 0.3655283451080322\n",
      "Epoch 4532, Loss: 1.8952072858810425, Final Batch Loss: 0.33940884470939636\n",
      "Epoch 4533, Loss: 1.7729756832122803, Final Batch Loss: 0.3945107161998749\n",
      "Epoch 4534, Loss: 1.8228801786899567, Final Batch Loss: 0.3768281936645508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4535, Loss: 1.7634947001934052, Final Batch Loss: 0.37222394347190857\n",
      "Epoch 4536, Loss: 1.7133878767490387, Final Batch Loss: 0.33286014199256897\n",
      "Epoch 4537, Loss: 1.7993507385253906, Final Batch Loss: 0.2940593957901001\n",
      "Epoch 4538, Loss: 1.884092539548874, Final Batch Loss: 0.33167561888694763\n",
      "Epoch 4539, Loss: 1.8026958107948303, Final Batch Loss: 0.41080260276794434\n",
      "Epoch 4540, Loss: 1.7983338534832, Final Batch Loss: 0.37164944410324097\n",
      "Epoch 4541, Loss: 1.8798887729644775, Final Batch Loss: 0.434148907661438\n",
      "Epoch 4542, Loss: 1.8845452070236206, Final Batch Loss: 0.45823192596435547\n",
      "Epoch 4543, Loss: 1.881992518901825, Final Batch Loss: 0.4021398723125458\n",
      "Epoch 4544, Loss: 1.808133602142334, Final Batch Loss: 0.3186839520931244\n",
      "Epoch 4545, Loss: 1.7271936237812042, Final Batch Loss: 0.285530149936676\n",
      "Epoch 4546, Loss: 1.887728214263916, Final Batch Loss: 0.27463498711586\n",
      "Epoch 4547, Loss: 1.8470992147922516, Final Batch Loss: 0.4483643174171448\n",
      "Epoch 4548, Loss: 2.2277172803878784, Final Batch Loss: 0.5402347445487976\n",
      "Epoch 4549, Loss: 1.7365485429763794, Final Batch Loss: 0.277693510055542\n",
      "Epoch 4550, Loss: 1.7267887592315674, Final Batch Loss: 0.36338010430336\n",
      "Epoch 4551, Loss: 1.7123226523399353, Final Batch Loss: 0.31556248664855957\n",
      "Epoch 4552, Loss: 1.8135053217411041, Final Batch Loss: 0.4264412522315979\n",
      "Epoch 4553, Loss: 1.870133399963379, Final Batch Loss: 0.4239329695701599\n",
      "Epoch 4554, Loss: 1.796109288930893, Final Batch Loss: 0.3686824440956116\n",
      "Epoch 4555, Loss: 1.8669168949127197, Final Batch Loss: 0.41877833008766174\n",
      "Epoch 4556, Loss: 1.87100350856781, Final Batch Loss: 0.31590449810028076\n",
      "Epoch 4557, Loss: 1.8812419772148132, Final Batch Loss: 0.39430463314056396\n",
      "Epoch 4558, Loss: 1.8911015093326569, Final Batch Loss: 0.2870684564113617\n",
      "Epoch 4559, Loss: 1.7915309369564056, Final Batch Loss: 0.36517640948295593\n",
      "Epoch 4560, Loss: 1.8134844899177551, Final Batch Loss: 0.37649691104888916\n",
      "Epoch 4561, Loss: 1.9480849504470825, Final Batch Loss: 0.4336271584033966\n",
      "Epoch 4562, Loss: 2.023736447095871, Final Batch Loss: 0.3609204888343811\n",
      "Epoch 4563, Loss: 1.8376526534557343, Final Batch Loss: 0.44619670510292053\n",
      "Epoch 4564, Loss: 1.7557868659496307, Final Batch Loss: 0.36190494894981384\n",
      "Epoch 4565, Loss: 2.04841947555542, Final Batch Loss: 0.39195409417152405\n",
      "Epoch 4566, Loss: 1.8264998197555542, Final Batch Loss: 0.4661511480808258\n",
      "Epoch 4567, Loss: 1.7971423864364624, Final Batch Loss: 0.3083980977535248\n",
      "Epoch 4568, Loss: 1.7791159451007843, Final Batch Loss: 0.3909948468208313\n",
      "Epoch 4569, Loss: 1.9013359248638153, Final Batch Loss: 0.36630409955978394\n",
      "Epoch 4570, Loss: 1.9404489398002625, Final Batch Loss: 0.4099694490432739\n",
      "Epoch 4571, Loss: 1.7216932773590088, Final Batch Loss: 0.2788461744785309\n",
      "Epoch 4572, Loss: 1.8827661275863647, Final Batch Loss: 0.40321147441864014\n",
      "Epoch 4573, Loss: 1.8868863880634308, Final Batch Loss: 0.3913617432117462\n",
      "Epoch 4574, Loss: 2.009152591228485, Final Batch Loss: 0.4483482241630554\n",
      "Epoch 4575, Loss: 1.8137255609035492, Final Batch Loss: 0.34775441884994507\n",
      "Epoch 4576, Loss: 1.6930218636989594, Final Batch Loss: 0.31798255443573\n",
      "Epoch 4577, Loss: 1.7400166988372803, Final Batch Loss: 0.3094117343425751\n",
      "Epoch 4578, Loss: 1.7514162063598633, Final Batch Loss: 0.3729063868522644\n",
      "Epoch 4579, Loss: 1.9111533164978027, Final Batch Loss: 0.43663519620895386\n",
      "Epoch 4580, Loss: 1.6838173270225525, Final Batch Loss: 0.293146014213562\n",
      "Epoch 4581, Loss: 1.8031034767627716, Final Batch Loss: 0.27713966369628906\n",
      "Epoch 4582, Loss: 1.9842981696128845, Final Batch Loss: 0.35121992230415344\n",
      "Epoch 4583, Loss: 1.872639238834381, Final Batch Loss: 0.3118404150009155\n",
      "Epoch 4584, Loss: 1.7295754551887512, Final Batch Loss: 0.42767783999443054\n",
      "Epoch 4585, Loss: 1.9433485269546509, Final Batch Loss: 0.3573160171508789\n",
      "Epoch 4586, Loss: 1.7440332472324371, Final Batch Loss: 0.3390040993690491\n",
      "Epoch 4587, Loss: 1.8033722937107086, Final Batch Loss: 0.3614371418952942\n",
      "Epoch 4588, Loss: 1.8106729388237, Final Batch Loss: 0.35020002722740173\n",
      "Epoch 4589, Loss: 1.8640741407871246, Final Batch Loss: 0.44999757409095764\n",
      "Epoch 4590, Loss: 1.7971185743808746, Final Batch Loss: 0.28590089082717896\n",
      "Epoch 4591, Loss: 1.9331898987293243, Final Batch Loss: 0.40632155537605286\n",
      "Epoch 4592, Loss: 1.6407299935817719, Final Batch Loss: 0.29068490862846375\n",
      "Epoch 4593, Loss: 1.9231437742710114, Final Batch Loss: 0.4534832537174225\n",
      "Epoch 4594, Loss: 1.8323646783828735, Final Batch Loss: 0.35122251510620117\n",
      "Epoch 4595, Loss: 1.8939392864704132, Final Batch Loss: 0.3252624571323395\n",
      "Epoch 4596, Loss: 1.8867145776748657, Final Batch Loss: 0.47280970215797424\n",
      "Epoch 4597, Loss: 1.8385698795318604, Final Batch Loss: 0.33389419317245483\n",
      "Epoch 4598, Loss: 1.8159952461719513, Final Batch Loss: 0.3063599467277527\n",
      "Epoch 4599, Loss: 1.7662001252174377, Final Batch Loss: 0.3848479092121124\n",
      "Epoch 4600, Loss: 1.8888930678367615, Final Batch Loss: 0.36093464493751526\n",
      "Epoch 4601, Loss: 1.9690247476100922, Final Batch Loss: 0.32476893067359924\n",
      "Epoch 4602, Loss: 1.8698096871376038, Final Batch Loss: 0.33404165506362915\n",
      "Epoch 4603, Loss: 1.8990915417671204, Final Batch Loss: 0.36519718170166016\n",
      "Epoch 4604, Loss: 1.7601877748966217, Final Batch Loss: 0.32943591475486755\n",
      "Epoch 4605, Loss: 1.8828387558460236, Final Batch Loss: 0.39905622601509094\n",
      "Epoch 4606, Loss: 1.8030549883842468, Final Batch Loss: 0.36104440689086914\n",
      "Epoch 4607, Loss: 1.8732267618179321, Final Batch Loss: 0.26973623037338257\n",
      "Epoch 4608, Loss: 1.752953439950943, Final Batch Loss: 0.2649717330932617\n",
      "Epoch 4609, Loss: 1.7990885078907013, Final Batch Loss: 0.3312908113002777\n",
      "Epoch 4610, Loss: 1.8386021554470062, Final Batch Loss: 0.4165734350681305\n",
      "Epoch 4611, Loss: 1.8111951649188995, Final Batch Loss: 0.3257448673248291\n",
      "Epoch 4612, Loss: 1.8969508111476898, Final Batch Loss: 0.3030436635017395\n",
      "Epoch 4613, Loss: 1.8479182124137878, Final Batch Loss: 0.3471352458000183\n",
      "Epoch 4614, Loss: 1.6881247460842133, Final Batch Loss: 0.30817726254463196\n",
      "Epoch 4615, Loss: 1.7490188479423523, Final Batch Loss: 0.3582417964935303\n",
      "Epoch 4616, Loss: 1.7460002601146698, Final Batch Loss: 0.34016093611717224\n",
      "Epoch 4617, Loss: 1.503032922744751, Final Batch Loss: 0.36220866441726685\n",
      "Epoch 4618, Loss: 1.7669773399829865, Final Batch Loss: 0.32254207134246826\n",
      "Epoch 4619, Loss: 1.834303468465805, Final Batch Loss: 0.44798609614372253\n",
      "Epoch 4620, Loss: 1.8140157461166382, Final Batch Loss: 0.4456998407840729\n",
      "Epoch 4621, Loss: 1.6456844508647919, Final Batch Loss: 0.3776504695415497\n",
      "Epoch 4622, Loss: 1.774134874343872, Final Batch Loss: 0.4418635070323944\n",
      "Epoch 4623, Loss: 1.7409202754497528, Final Batch Loss: 0.29964178800582886\n",
      "Epoch 4624, Loss: 1.950025051832199, Final Batch Loss: 0.3233087360858917\n",
      "Epoch 4625, Loss: 1.9001675248146057, Final Batch Loss: 0.42782214283943176\n",
      "Epoch 4626, Loss: 1.966598004102707, Final Batch Loss: 0.39459893107414246\n",
      "Epoch 4627, Loss: 1.9281266927719116, Final Batch Loss: 0.33747127652168274\n",
      "Epoch 4628, Loss: 1.815605789422989, Final Batch Loss: 0.3590911626815796\n",
      "Epoch 4629, Loss: 1.8010041117668152, Final Batch Loss: 0.3696286678314209\n",
      "Epoch 4630, Loss: 1.9780833423137665, Final Batch Loss: 0.44108834862709045\n",
      "Epoch 4631, Loss: 1.7351374328136444, Final Batch Loss: 0.39912334084510803\n",
      "Epoch 4632, Loss: 1.910395324230194, Final Batch Loss: 0.3926490545272827\n",
      "Epoch 4633, Loss: 1.7886102795600891, Final Batch Loss: 0.31900790333747864\n",
      "Epoch 4634, Loss: 1.7856342792510986, Final Batch Loss: 0.2736513614654541\n",
      "Epoch 4635, Loss: 1.727031946182251, Final Batch Loss: 0.2990608811378479\n",
      "Epoch 4636, Loss: 1.8633123934268951, Final Batch Loss: 0.3802916705608368\n",
      "Epoch 4637, Loss: 1.7944930493831635, Final Batch Loss: 0.4234265685081482\n",
      "Epoch 4638, Loss: 1.643741935491562, Final Batch Loss: 0.37462228536605835\n",
      "Epoch 4639, Loss: 1.8672612309455872, Final Batch Loss: 0.2968730926513672\n",
      "Epoch 4640, Loss: 1.791961520910263, Final Batch Loss: 0.40805235505104065\n",
      "Epoch 4641, Loss: 1.747996062040329, Final Batch Loss: 0.4546583890914917\n",
      "Epoch 4642, Loss: 1.8867952525615692, Final Batch Loss: 0.4226793348789215\n",
      "Epoch 4643, Loss: 1.73545902967453, Final Batch Loss: 0.4035288393497467\n",
      "Epoch 4644, Loss: 1.8931248486042023, Final Batch Loss: 0.2958693206310272\n",
      "Epoch 4645, Loss: 2.026493579149246, Final Batch Loss: 0.48106545209884644\n",
      "Epoch 4646, Loss: 1.7520919740200043, Final Batch Loss: 0.42924439907073975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4647, Loss: 1.771375149488449, Final Batch Loss: 0.30705785751342773\n",
      "Epoch 4648, Loss: 1.7785332798957825, Final Batch Loss: 0.36557841300964355\n",
      "Epoch 4649, Loss: 2.0195508301258087, Final Batch Loss: 0.4613772928714752\n",
      "Epoch 4650, Loss: 1.8953404128551483, Final Batch Loss: 0.37509819865226746\n",
      "Epoch 4651, Loss: 1.8478255569934845, Final Batch Loss: 0.32036539912223816\n",
      "Epoch 4652, Loss: 1.7056398689746857, Final Batch Loss: 0.4127333462238312\n",
      "Epoch 4653, Loss: 2.0442548990249634, Final Batch Loss: 0.3913463354110718\n",
      "Epoch 4654, Loss: 1.713363230228424, Final Batch Loss: 0.262270987033844\n",
      "Epoch 4655, Loss: 1.800883799791336, Final Batch Loss: 0.32932230830192566\n",
      "Epoch 4656, Loss: 1.872361570596695, Final Batch Loss: 0.45318740606307983\n",
      "Epoch 4657, Loss: 1.8489191830158234, Final Batch Loss: 0.2797015905380249\n",
      "Epoch 4658, Loss: 1.8156035840511322, Final Batch Loss: 0.31654050946235657\n",
      "Epoch 4659, Loss: 1.7784971296787262, Final Batch Loss: 0.31624165177345276\n",
      "Epoch 4660, Loss: 1.8663544952869415, Final Batch Loss: 0.33619606494903564\n",
      "Epoch 4661, Loss: 1.6395508646965027, Final Batch Loss: 0.3479876220226288\n",
      "Epoch 4662, Loss: 1.7547814100980759, Final Batch Loss: 0.4108319878578186\n",
      "Epoch 4663, Loss: 1.945818543434143, Final Batch Loss: 0.4191295802593231\n",
      "Epoch 4664, Loss: 1.8244717121124268, Final Batch Loss: 0.33439064025878906\n",
      "Epoch 4665, Loss: 1.8644428551197052, Final Batch Loss: 0.3096233308315277\n",
      "Epoch 4666, Loss: 1.8757224678993225, Final Batch Loss: 0.3164704442024231\n",
      "Epoch 4667, Loss: 1.7825883626937866, Final Batch Loss: 0.3359892964363098\n",
      "Epoch 4668, Loss: 1.8492328226566315, Final Batch Loss: 0.3185032308101654\n",
      "Epoch 4669, Loss: 1.7278295755386353, Final Batch Loss: 0.35512107610702515\n",
      "Epoch 4670, Loss: 1.6369006037712097, Final Batch Loss: 0.3308820128440857\n",
      "Epoch 4671, Loss: 2.0211357474327087, Final Batch Loss: 0.49906548857688904\n",
      "Epoch 4672, Loss: 1.7523590624332428, Final Batch Loss: 0.4096589982509613\n",
      "Epoch 4673, Loss: 1.6308045983314514, Final Batch Loss: 0.3601410388946533\n",
      "Epoch 4674, Loss: 1.9826268255710602, Final Batch Loss: 0.4023858308792114\n",
      "Epoch 4675, Loss: 1.8391752541065216, Final Batch Loss: 0.3412543833255768\n",
      "Epoch 4676, Loss: 1.5060248970985413, Final Batch Loss: 0.2745630741119385\n",
      "Epoch 4677, Loss: 1.912365436553955, Final Batch Loss: 0.4286048412322998\n",
      "Epoch 4678, Loss: 1.7627105116844177, Final Batch Loss: 0.3525089919567108\n",
      "Epoch 4679, Loss: 1.868292510509491, Final Batch Loss: 0.370402991771698\n",
      "Epoch 4680, Loss: 1.829224407672882, Final Batch Loss: 0.3690357804298401\n",
      "Epoch 4681, Loss: 1.816139668226242, Final Batch Loss: 0.3929060697555542\n",
      "Epoch 4682, Loss: 1.942589819431305, Final Batch Loss: 0.4902792274951935\n",
      "Epoch 4683, Loss: 1.7832550406455994, Final Batch Loss: 0.4465128779411316\n",
      "Epoch 4684, Loss: 1.9004829823970795, Final Batch Loss: 0.3207189440727234\n",
      "Epoch 4685, Loss: 1.9856916666030884, Final Batch Loss: 0.4332464337348938\n",
      "Epoch 4686, Loss: 1.8372479677200317, Final Batch Loss: 0.35636281967163086\n",
      "Epoch 4687, Loss: 1.6057709455490112, Final Batch Loss: 0.300366073846817\n",
      "Epoch 4688, Loss: 1.7077808678150177, Final Batch Loss: 0.35346168279647827\n",
      "Epoch 4689, Loss: 1.962773472070694, Final Batch Loss: 0.3669280409812927\n",
      "Epoch 4690, Loss: 1.9777792990207672, Final Batch Loss: 0.3864562213420868\n",
      "Epoch 4691, Loss: 1.8908211886882782, Final Batch Loss: 0.33587560057640076\n",
      "Epoch 4692, Loss: 1.8540721833705902, Final Batch Loss: 0.38819971680641174\n",
      "Epoch 4693, Loss: 1.9457871317863464, Final Batch Loss: 0.43880027532577515\n",
      "Epoch 4694, Loss: 1.7639832198619843, Final Batch Loss: 0.34338298439979553\n",
      "Epoch 4695, Loss: 1.7605397701263428, Final Batch Loss: 0.382185697555542\n",
      "Epoch 4696, Loss: 1.9528178572654724, Final Batch Loss: 0.36691102385520935\n",
      "Epoch 4697, Loss: 2.000440001487732, Final Batch Loss: 0.42311891913414\n",
      "Epoch 4698, Loss: 1.7075369358062744, Final Batch Loss: 0.31493404507637024\n",
      "Epoch 4699, Loss: 1.7800174057483673, Final Batch Loss: 0.4290864169597626\n",
      "Epoch 4700, Loss: 1.901722013950348, Final Batch Loss: 0.404074490070343\n",
      "Epoch 4701, Loss: 1.9228324592113495, Final Batch Loss: 0.3571339249610901\n",
      "Epoch 4702, Loss: 1.892824649810791, Final Batch Loss: 0.38749662041664124\n",
      "Epoch 4703, Loss: 1.6716061532497406, Final Batch Loss: 0.25292718410491943\n",
      "Epoch 4704, Loss: 1.852798342704773, Final Batch Loss: 0.46563923358917236\n",
      "Epoch 4705, Loss: 1.8966175019741058, Final Batch Loss: 0.3124840259552002\n",
      "Epoch 4706, Loss: 1.869808316230774, Final Batch Loss: 0.35123711824417114\n",
      "Epoch 4707, Loss: 1.7490738332271576, Final Batch Loss: 0.3438519239425659\n",
      "Epoch 4708, Loss: 1.7848961055278778, Final Batch Loss: 0.37030816078186035\n",
      "Epoch 4709, Loss: 1.6606484055519104, Final Batch Loss: 0.25838014483451843\n",
      "Epoch 4710, Loss: 1.7890775501728058, Final Batch Loss: 0.324537456035614\n",
      "Epoch 4711, Loss: 1.8868111968040466, Final Batch Loss: 0.46435365080833435\n",
      "Epoch 4712, Loss: 2.043558359146118, Final Batch Loss: 0.38630321621894836\n",
      "Epoch 4713, Loss: 1.8553090691566467, Final Batch Loss: 0.30801013112068176\n",
      "Epoch 4714, Loss: 1.7430218756198883, Final Batch Loss: 0.28702545166015625\n",
      "Epoch 4715, Loss: 1.9486928880214691, Final Batch Loss: 0.3684583306312561\n",
      "Epoch 4716, Loss: 1.8197502195835114, Final Batch Loss: 0.3368731141090393\n",
      "Epoch 4717, Loss: 1.938131421804428, Final Batch Loss: 0.5005765557289124\n",
      "Epoch 4718, Loss: 1.7776702642440796, Final Batch Loss: 0.43915608525276184\n",
      "Epoch 4719, Loss: 1.8326289653778076, Final Batch Loss: 0.4807591140270233\n",
      "Epoch 4720, Loss: 1.9818829894065857, Final Batch Loss: 0.2843303382396698\n",
      "Epoch 4721, Loss: 1.8291155099868774, Final Batch Loss: 0.42424899339675903\n",
      "Epoch 4722, Loss: 1.9902642965316772, Final Batch Loss: 0.44828248023986816\n",
      "Epoch 4723, Loss: 1.7478629350662231, Final Batch Loss: 0.3445460796356201\n",
      "Epoch 4724, Loss: 2.075392425060272, Final Batch Loss: 0.30156123638153076\n",
      "Epoch 4725, Loss: 1.9058088958263397, Final Batch Loss: 0.3950164318084717\n",
      "Epoch 4726, Loss: 1.744170993566513, Final Batch Loss: 0.4252466559410095\n",
      "Epoch 4727, Loss: 1.8579286932945251, Final Batch Loss: 0.41068968176841736\n",
      "Epoch 4728, Loss: 1.9234676659107208, Final Batch Loss: 0.44424015283584595\n",
      "Epoch 4729, Loss: 1.8475589156150818, Final Batch Loss: 0.3210865259170532\n",
      "Epoch 4730, Loss: 2.0225392878055573, Final Batch Loss: 0.38951244950294495\n",
      "Epoch 4731, Loss: 1.7609530687332153, Final Batch Loss: 0.31378480792045593\n",
      "Epoch 4732, Loss: 1.7609593868255615, Final Batch Loss: 0.24658459424972534\n",
      "Epoch 4733, Loss: 1.6598380208015442, Final Batch Loss: 0.3835528790950775\n",
      "Epoch 4734, Loss: 1.847399890422821, Final Batch Loss: 0.3828968405723572\n",
      "Epoch 4735, Loss: 1.9072371125221252, Final Batch Loss: 0.40003886818885803\n",
      "Epoch 4736, Loss: 1.749424010515213, Final Batch Loss: 0.3678736984729767\n",
      "Epoch 4737, Loss: 1.9063104391098022, Final Batch Loss: 0.4231709837913513\n",
      "Epoch 4738, Loss: 1.71083402633667, Final Batch Loss: 0.2866896390914917\n",
      "Epoch 4739, Loss: 1.8921899199485779, Final Batch Loss: 0.4730166494846344\n",
      "Epoch 4740, Loss: 1.8513460755348206, Final Batch Loss: 0.27633965015411377\n",
      "Epoch 4741, Loss: 1.790714055299759, Final Batch Loss: 0.4200180768966675\n",
      "Epoch 4742, Loss: 1.80244979262352, Final Batch Loss: 0.38980260491371155\n",
      "Epoch 4743, Loss: 1.8178254663944244, Final Batch Loss: 0.4004814028739929\n",
      "Epoch 4744, Loss: 1.84674471616745, Final Batch Loss: 0.32660168409347534\n",
      "Epoch 4745, Loss: 1.8863312005996704, Final Batch Loss: 0.3814663887023926\n",
      "Epoch 4746, Loss: 1.9783216714859009, Final Batch Loss: 0.3458515703678131\n",
      "Epoch 4747, Loss: 1.8126687705516815, Final Batch Loss: 0.45457854866981506\n",
      "Epoch 4748, Loss: 1.7229690253734589, Final Batch Loss: 0.33765456080436707\n",
      "Epoch 4749, Loss: 1.7514462172985077, Final Batch Loss: 0.3621322810649872\n",
      "Epoch 4750, Loss: 1.8652905225753784, Final Batch Loss: 0.4001266062259674\n",
      "Epoch 4751, Loss: 1.7799616158008575, Final Batch Loss: 0.321331650018692\n",
      "Epoch 4752, Loss: 1.566671997308731, Final Batch Loss: 0.28774791955947876\n",
      "Epoch 4753, Loss: 1.8637981414794922, Final Batch Loss: 0.2918027341365814\n",
      "Epoch 4754, Loss: 1.9780547618865967, Final Batch Loss: 0.406917929649353\n",
      "Epoch 4755, Loss: 1.6971255540847778, Final Batch Loss: 0.27610939741134644\n",
      "Epoch 4756, Loss: 1.933734506368637, Final Batch Loss: 0.4140278995037079\n",
      "Epoch 4757, Loss: 1.88751620054245, Final Batch Loss: 0.40577542781829834\n",
      "Epoch 4758, Loss: 1.8729749619960785, Final Batch Loss: 0.3380007743835449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4759, Loss: 1.7999573647975922, Final Batch Loss: 0.3650344908237457\n",
      "Epoch 4760, Loss: 1.854941487312317, Final Batch Loss: 0.3459833860397339\n",
      "Epoch 4761, Loss: 1.9045557975769043, Final Batch Loss: 0.42789408564567566\n",
      "Epoch 4762, Loss: 1.88839390873909, Final Batch Loss: 0.32802116870880127\n",
      "Epoch 4763, Loss: 1.828121155500412, Final Batch Loss: 0.30666571855545044\n",
      "Epoch 4764, Loss: 1.8212873935699463, Final Batch Loss: 0.31639379262924194\n",
      "Epoch 4765, Loss: 1.8098943829536438, Final Batch Loss: 0.312084823846817\n",
      "Epoch 4766, Loss: 1.7272337079048157, Final Batch Loss: 0.299956351518631\n",
      "Epoch 4767, Loss: 1.7334629893302917, Final Batch Loss: 0.37860962748527527\n",
      "Epoch 4768, Loss: 1.7333222925662994, Final Batch Loss: 0.32527369260787964\n",
      "Epoch 4769, Loss: 1.841566413640976, Final Batch Loss: 0.29582276940345764\n",
      "Epoch 4770, Loss: 1.9540452063083649, Final Batch Loss: 0.40141868591308594\n",
      "Epoch 4771, Loss: 1.7558292150497437, Final Batch Loss: 0.36692628264427185\n",
      "Epoch 4772, Loss: 1.800790011882782, Final Batch Loss: 0.3383181393146515\n",
      "Epoch 4773, Loss: 1.754088819026947, Final Batch Loss: 0.3794366419315338\n",
      "Epoch 4774, Loss: 1.865990161895752, Final Batch Loss: 0.4156748056411743\n",
      "Epoch 4775, Loss: 1.7597127556800842, Final Batch Loss: 0.3552747070789337\n",
      "Epoch 4776, Loss: 1.6113052815198898, Final Batch Loss: 0.23596914112567902\n",
      "Epoch 4777, Loss: 1.8426028192043304, Final Batch Loss: 0.3276321589946747\n",
      "Epoch 4778, Loss: 1.8051436841487885, Final Batch Loss: 0.40501779317855835\n",
      "Epoch 4779, Loss: 1.913219392299652, Final Batch Loss: 0.3742709159851074\n",
      "Epoch 4780, Loss: 1.7259941697120667, Final Batch Loss: 0.3417014181613922\n",
      "Epoch 4781, Loss: 1.8285698890686035, Final Batch Loss: 0.3816833198070526\n",
      "Epoch 4782, Loss: 1.8472577631473541, Final Batch Loss: 0.36289748549461365\n",
      "Epoch 4783, Loss: 1.7681270837783813, Final Batch Loss: 0.31159088015556335\n",
      "Epoch 4784, Loss: 1.8954545259475708, Final Batch Loss: 0.356763631105423\n",
      "Epoch 4785, Loss: 1.7248783111572266, Final Batch Loss: 0.3809256851673126\n",
      "Epoch 4786, Loss: 1.8386795818805695, Final Batch Loss: 0.39805421233177185\n",
      "Epoch 4787, Loss: 1.780681699514389, Final Batch Loss: 0.33020690083503723\n",
      "Epoch 4788, Loss: 1.8462944626808167, Final Batch Loss: 0.3963693380355835\n",
      "Epoch 4789, Loss: 1.924948513507843, Final Batch Loss: 0.46049460768699646\n",
      "Epoch 4790, Loss: 1.7478549182415009, Final Batch Loss: 0.33076542615890503\n",
      "Epoch 4791, Loss: 1.8170841038227081, Final Batch Loss: 0.3449343144893646\n",
      "Epoch 4792, Loss: 1.8714511394500732, Final Batch Loss: 0.3039611577987671\n",
      "Epoch 4793, Loss: 1.8611163794994354, Final Batch Loss: 0.4164712429046631\n",
      "Epoch 4794, Loss: 1.7579357028007507, Final Batch Loss: 0.38840457797050476\n",
      "Epoch 4795, Loss: 1.666379600763321, Final Batch Loss: 0.353321373462677\n",
      "Epoch 4796, Loss: 1.8250553011894226, Final Batch Loss: 0.3422531485557556\n",
      "Epoch 4797, Loss: 1.8443616330623627, Final Batch Loss: 0.35610923171043396\n",
      "Epoch 4798, Loss: 1.8004775941371918, Final Batch Loss: 0.40182968974113464\n",
      "Epoch 4799, Loss: 1.8951607048511505, Final Batch Loss: 0.5150379538536072\n",
      "Epoch 4800, Loss: 1.883885145187378, Final Batch Loss: 0.360594779253006\n",
      "Epoch 4801, Loss: 1.8583241403102875, Final Batch Loss: 0.332273006439209\n",
      "Epoch 4802, Loss: 1.8433447778224945, Final Batch Loss: 0.32297274470329285\n",
      "Epoch 4803, Loss: 1.8835083842277527, Final Batch Loss: 0.4885334074497223\n",
      "Epoch 4804, Loss: 1.8044638335704803, Final Batch Loss: 0.31357342004776\n",
      "Epoch 4805, Loss: 1.8164407908916473, Final Batch Loss: 0.3679260313510895\n",
      "Epoch 4806, Loss: 1.7483053505420685, Final Batch Loss: 0.3714933693408966\n",
      "Epoch 4807, Loss: 1.870754361152649, Final Batch Loss: 0.4033646881580353\n",
      "Epoch 4808, Loss: 1.9545798003673553, Final Batch Loss: 0.5000454187393188\n",
      "Epoch 4809, Loss: 1.7111785113811493, Final Batch Loss: 0.33105868101119995\n",
      "Epoch 4810, Loss: 1.8558295369148254, Final Batch Loss: 0.2937186062335968\n",
      "Epoch 4811, Loss: 1.9606159627437592, Final Batch Loss: 0.4295410215854645\n",
      "Epoch 4812, Loss: 1.8593871295452118, Final Batch Loss: 0.35883763432502747\n",
      "Epoch 4813, Loss: 2.0290707051754, Final Batch Loss: 0.3862167000770569\n",
      "Epoch 4814, Loss: 1.7832163572311401, Final Batch Loss: 0.35983723402023315\n",
      "Epoch 4815, Loss: 1.7923837304115295, Final Batch Loss: 0.32908204197883606\n",
      "Epoch 4816, Loss: 1.9385813474655151, Final Batch Loss: 0.40820181369781494\n",
      "Epoch 4817, Loss: 1.9413552582263947, Final Batch Loss: 0.3341720402240753\n",
      "Epoch 4818, Loss: 1.7699741423130035, Final Batch Loss: 0.3501480519771576\n",
      "Epoch 4819, Loss: 1.786126732826233, Final Batch Loss: 0.3517409861087799\n",
      "Epoch 4820, Loss: 1.7781441509723663, Final Batch Loss: 0.3873518109321594\n",
      "Epoch 4821, Loss: 1.8348422050476074, Final Batch Loss: 0.3349839448928833\n",
      "Epoch 4822, Loss: 1.7928378283977509, Final Batch Loss: 0.4151351749897003\n",
      "Epoch 4823, Loss: 1.7198885679244995, Final Batch Loss: 0.28168928623199463\n",
      "Epoch 4824, Loss: 1.8054892420768738, Final Batch Loss: 0.3683708906173706\n",
      "Epoch 4825, Loss: 1.9217831492424011, Final Batch Loss: 0.46143603324890137\n",
      "Epoch 4826, Loss: 2.02258637547493, Final Batch Loss: 0.47255513072013855\n",
      "Epoch 4827, Loss: 1.8610662519931793, Final Batch Loss: 0.34155187010765076\n",
      "Epoch 4828, Loss: 1.9328416287899017, Final Batch Loss: 0.3447140157222748\n",
      "Epoch 4829, Loss: 1.7718243598937988, Final Batch Loss: 0.3448483645915985\n",
      "Epoch 4830, Loss: 1.7441461384296417, Final Batch Loss: 0.41042250394821167\n",
      "Epoch 4831, Loss: 1.8757403790950775, Final Batch Loss: 0.4095715880393982\n",
      "Epoch 4832, Loss: 1.8850180208683014, Final Batch Loss: 0.40156397223472595\n",
      "Epoch 4833, Loss: 1.9019648730754852, Final Batch Loss: 0.37743377685546875\n",
      "Epoch 4834, Loss: 1.70098015666008, Final Batch Loss: 0.35230937600135803\n",
      "Epoch 4835, Loss: 1.849242627620697, Final Batch Loss: 0.39553219079971313\n",
      "Epoch 4836, Loss: 1.7650858461856842, Final Batch Loss: 0.3590676784515381\n",
      "Epoch 4837, Loss: 2.0110726952552795, Final Batch Loss: 0.48947158455848694\n",
      "Epoch 4838, Loss: 1.9716005623340607, Final Batch Loss: 0.5437278747558594\n",
      "Epoch 4839, Loss: 1.66915962100029, Final Batch Loss: 0.3115479350090027\n",
      "Epoch 4840, Loss: 1.772204577922821, Final Batch Loss: 0.30098849534988403\n",
      "Epoch 4841, Loss: 1.8475797176361084, Final Batch Loss: 0.43285906314849854\n",
      "Epoch 4842, Loss: 1.7419201731681824, Final Batch Loss: 0.3324618339538574\n",
      "Epoch 4843, Loss: 1.8074411749839783, Final Batch Loss: 0.357458233833313\n",
      "Epoch 4844, Loss: 1.886130154132843, Final Batch Loss: 0.3889845609664917\n",
      "Epoch 4845, Loss: 1.7365434765815735, Final Batch Loss: 0.37320980429649353\n",
      "Epoch 4846, Loss: 1.9414982795715332, Final Batch Loss: 0.45391371846199036\n",
      "Epoch 4847, Loss: 1.6511308252811432, Final Batch Loss: 0.2632838785648346\n",
      "Epoch 4848, Loss: 1.9093020558357239, Final Batch Loss: 0.4049501121044159\n",
      "Epoch 4849, Loss: 1.8443633615970612, Final Batch Loss: 0.39165812730789185\n",
      "Epoch 4850, Loss: 1.8512009978294373, Final Batch Loss: 0.3818628191947937\n",
      "Epoch 4851, Loss: 1.719382405281067, Final Batch Loss: 0.30807924270629883\n",
      "Epoch 4852, Loss: 1.7246335744857788, Final Batch Loss: 0.31871750950813293\n",
      "Epoch 4853, Loss: 1.8289168179035187, Final Batch Loss: 0.32377544045448303\n",
      "Epoch 4854, Loss: 2.0246265828609467, Final Batch Loss: 0.489246129989624\n",
      "Epoch 4855, Loss: 1.9015608727931976, Final Batch Loss: 0.3946649432182312\n",
      "Epoch 4856, Loss: 1.7573676705360413, Final Batch Loss: 0.37335753440856934\n",
      "Epoch 4857, Loss: 1.8148993253707886, Final Batch Loss: 0.41459381580352783\n",
      "Epoch 4858, Loss: 1.757713943719864, Final Batch Loss: 0.2930278480052948\n",
      "Epoch 4859, Loss: 1.9200720489025116, Final Batch Loss: 0.32272717356681824\n",
      "Epoch 4860, Loss: 1.8627418875694275, Final Batch Loss: 0.3541324734687805\n",
      "Epoch 4861, Loss: 1.8963426649570465, Final Batch Loss: 0.369445264339447\n",
      "Epoch 4862, Loss: 1.6865718066692352, Final Batch Loss: 0.36444583535194397\n",
      "Epoch 4863, Loss: 1.8282618522644043, Final Batch Loss: 0.32978564500808716\n",
      "Epoch 4864, Loss: 1.6535752415657043, Final Batch Loss: 0.32232120633125305\n",
      "Epoch 4865, Loss: 1.7614004015922546, Final Batch Loss: 0.3828496038913727\n",
      "Epoch 4866, Loss: 1.7631141543388367, Final Batch Loss: 0.3470996916294098\n",
      "Epoch 4867, Loss: 1.7501807510852814, Final Batch Loss: 0.3030945360660553\n",
      "Epoch 4868, Loss: 1.8646121323108673, Final Batch Loss: 0.3787744641304016\n",
      "Epoch 4869, Loss: 1.7210516035556793, Final Batch Loss: 0.3267720341682434\n",
      "Epoch 4870, Loss: 1.9664390683174133, Final Batch Loss: 0.31595751643180847\n",
      "Epoch 4871, Loss: 1.7438368201255798, Final Batch Loss: 0.3192262053489685\n",
      "Epoch 4872, Loss: 1.6642346680164337, Final Batch Loss: 0.2825098931789398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4873, Loss: 1.8043873012065887, Final Batch Loss: 0.404940128326416\n",
      "Epoch 4874, Loss: 1.7541296780109406, Final Batch Loss: 0.3122006356716156\n",
      "Epoch 4875, Loss: 1.9078821241855621, Final Batch Loss: 0.3788796365261078\n",
      "Epoch 4876, Loss: 1.7657745480537415, Final Batch Loss: 0.35844334959983826\n",
      "Epoch 4877, Loss: 1.87002432346344, Final Batch Loss: 0.3224259912967682\n",
      "Epoch 4878, Loss: 1.8844822943210602, Final Batch Loss: 0.3835221827030182\n",
      "Epoch 4879, Loss: 1.898571491241455, Final Batch Loss: 0.3267354667186737\n",
      "Epoch 4880, Loss: 1.8561020493507385, Final Batch Loss: 0.42933189868927\n",
      "Epoch 4881, Loss: 1.7450003325939178, Final Batch Loss: 0.36102819442749023\n",
      "Epoch 4882, Loss: 1.93827423453331, Final Batch Loss: 0.39243409037590027\n",
      "Epoch 4883, Loss: 1.7485085725784302, Final Batch Loss: 0.3471865952014923\n",
      "Epoch 4884, Loss: 1.689540445804596, Final Batch Loss: 0.3080979585647583\n",
      "Epoch 4885, Loss: 1.8866169452667236, Final Batch Loss: 0.4156160056591034\n",
      "Epoch 4886, Loss: 1.7788403034210205, Final Batch Loss: 0.356903076171875\n",
      "Epoch 4887, Loss: 1.7083827555179596, Final Batch Loss: 0.2985341250896454\n",
      "Epoch 4888, Loss: 1.9267602860927582, Final Batch Loss: 0.34067872166633606\n",
      "Epoch 4889, Loss: 1.84500190615654, Final Batch Loss: 0.43882477283477783\n",
      "Epoch 4890, Loss: 1.7816887199878693, Final Batch Loss: 0.27780747413635254\n",
      "Epoch 4891, Loss: 1.9618298411369324, Final Batch Loss: 0.39608678221702576\n",
      "Epoch 4892, Loss: 1.6979058980941772, Final Batch Loss: 0.3112015128135681\n",
      "Epoch 4893, Loss: 1.7321712374687195, Final Batch Loss: 0.34693971276283264\n",
      "Epoch 4894, Loss: 1.7191015779972076, Final Batch Loss: 0.28099167346954346\n",
      "Epoch 4895, Loss: 1.8816722333431244, Final Batch Loss: 0.351105272769928\n",
      "Epoch 4896, Loss: 1.8793032765388489, Final Batch Loss: 0.2369781732559204\n",
      "Epoch 4897, Loss: 1.828806072473526, Final Batch Loss: 0.28383129835128784\n",
      "Epoch 4898, Loss: 1.8888191282749176, Final Batch Loss: 0.43679699301719666\n",
      "Epoch 4899, Loss: 1.7865090668201447, Final Batch Loss: 0.3742824196815491\n",
      "Epoch 4900, Loss: 1.7561528980731964, Final Batch Loss: 0.2643316090106964\n",
      "Epoch 4901, Loss: 1.8733160495758057, Final Batch Loss: 0.35707005858421326\n",
      "Epoch 4902, Loss: 1.8039329648017883, Final Batch Loss: 0.3381015360355377\n",
      "Epoch 4903, Loss: 1.6930474042892456, Final Batch Loss: 0.2807198762893677\n",
      "Epoch 4904, Loss: 1.6719467341899872, Final Batch Loss: 0.3817998170852661\n",
      "Epoch 4905, Loss: 1.993609756231308, Final Batch Loss: 0.3679766356945038\n",
      "Epoch 4906, Loss: 1.9389151334762573, Final Batch Loss: 0.46629074215888977\n",
      "Epoch 4907, Loss: 1.8557385504245758, Final Batch Loss: 0.4784236550331116\n",
      "Epoch 4908, Loss: 1.8812582790851593, Final Batch Loss: 0.47434210777282715\n",
      "Epoch 4909, Loss: 1.9691599905490875, Final Batch Loss: 0.4509178400039673\n",
      "Epoch 4910, Loss: 1.758057177066803, Final Batch Loss: 0.33251190185546875\n",
      "Epoch 4911, Loss: 1.8109762072563171, Final Batch Loss: 0.2987859845161438\n",
      "Epoch 4912, Loss: 1.7448244094848633, Final Batch Loss: 0.4269784986972809\n",
      "Epoch 4913, Loss: 1.8560383915901184, Final Batch Loss: 0.3987859785556793\n",
      "Epoch 4914, Loss: 1.7874983847141266, Final Batch Loss: 0.3282517194747925\n",
      "Epoch 4915, Loss: 1.8013814389705658, Final Batch Loss: 0.36468616127967834\n",
      "Epoch 4916, Loss: 1.9325262606143951, Final Batch Loss: 0.363957941532135\n",
      "Epoch 4917, Loss: 1.7630214095115662, Final Batch Loss: 0.3475169539451599\n",
      "Epoch 4918, Loss: 1.6780007183551788, Final Batch Loss: 0.3928116261959076\n",
      "Epoch 4919, Loss: 1.7969866693019867, Final Batch Loss: 0.3421320617198944\n",
      "Epoch 4920, Loss: 1.8075408041477203, Final Batch Loss: 0.31987178325653076\n",
      "Epoch 4921, Loss: 1.8015028536319733, Final Batch Loss: 0.3601641356945038\n",
      "Epoch 4922, Loss: 2.000460982322693, Final Batch Loss: 0.5336067080497742\n",
      "Epoch 4923, Loss: 1.8126297891139984, Final Batch Loss: 0.3959790766239166\n",
      "Epoch 4924, Loss: 1.793753832578659, Final Batch Loss: 0.3633088767528534\n",
      "Epoch 4925, Loss: 1.6558406352996826, Final Batch Loss: 0.365424782037735\n",
      "Epoch 4926, Loss: 1.652383714914322, Final Batch Loss: 0.35878485441207886\n",
      "Epoch 4927, Loss: 1.7417364716529846, Final Batch Loss: 0.311114639043808\n",
      "Epoch 4928, Loss: 1.7469111680984497, Final Batch Loss: 0.38047268986701965\n",
      "Epoch 4929, Loss: 1.8743457198143005, Final Batch Loss: 0.4462912082672119\n",
      "Epoch 4930, Loss: 1.7999367415904999, Final Batch Loss: 0.34914520382881165\n",
      "Epoch 4931, Loss: 1.7323909103870392, Final Batch Loss: 0.3621096611022949\n",
      "Epoch 4932, Loss: 1.8490801751613617, Final Batch Loss: 0.41595906019210815\n",
      "Epoch 4933, Loss: 1.8150292336940765, Final Batch Loss: 0.37105631828308105\n",
      "Epoch 4934, Loss: 1.7832363843917847, Final Batch Loss: 0.4119725525379181\n",
      "Epoch 4935, Loss: 1.725414663553238, Final Batch Loss: 0.3737591803073883\n",
      "Epoch 4936, Loss: 1.745863825082779, Final Batch Loss: 0.43593069911003113\n",
      "Epoch 4937, Loss: 1.9092677235603333, Final Batch Loss: 0.37987932562828064\n",
      "Epoch 4938, Loss: 1.7583433985710144, Final Batch Loss: 0.33310189843177795\n",
      "Epoch 4939, Loss: 1.734741747379303, Final Batch Loss: 0.3324628174304962\n",
      "Epoch 4940, Loss: 1.6332576870918274, Final Batch Loss: 0.3273458480834961\n",
      "Epoch 4941, Loss: 1.6767672300338745, Final Batch Loss: 0.31301483511924744\n",
      "Epoch 4942, Loss: 1.6676409244537354, Final Batch Loss: 0.43126896023750305\n",
      "Epoch 4943, Loss: 1.6561159193515778, Final Batch Loss: 0.339362770318985\n",
      "Epoch 4944, Loss: 1.717828094959259, Final Batch Loss: 0.3055969476699829\n",
      "Epoch 4945, Loss: 1.8682169318199158, Final Batch Loss: 0.47151267528533936\n",
      "Epoch 4946, Loss: 1.8540382087230682, Final Batch Loss: 0.2912864685058594\n",
      "Epoch 4947, Loss: 1.8128123879432678, Final Batch Loss: 0.2763158679008484\n",
      "Epoch 4948, Loss: 1.7861188054084778, Final Batch Loss: 0.3052469491958618\n",
      "Epoch 4949, Loss: 1.6377712786197662, Final Batch Loss: 0.2966136932373047\n",
      "Epoch 4950, Loss: 1.731444537639618, Final Batch Loss: 0.3201059103012085\n",
      "Epoch 4951, Loss: 1.792954832315445, Final Batch Loss: 0.3692089021205902\n",
      "Epoch 4952, Loss: 1.8242783546447754, Final Batch Loss: 0.3286307156085968\n",
      "Epoch 4953, Loss: 1.731408804655075, Final Batch Loss: 0.32869410514831543\n",
      "Epoch 4954, Loss: 1.7672438621520996, Final Batch Loss: 0.26188352704048157\n",
      "Epoch 4955, Loss: 1.939343810081482, Final Batch Loss: 0.41120949387550354\n",
      "Epoch 4956, Loss: 1.8408547937870026, Final Batch Loss: 0.3586865961551666\n",
      "Epoch 4957, Loss: 1.6598056554794312, Final Batch Loss: 0.3100632429122925\n",
      "Epoch 4958, Loss: 1.6964528262615204, Final Batch Loss: 0.3067733347415924\n",
      "Epoch 4959, Loss: 1.7723762392997742, Final Batch Loss: 0.35364943742752075\n",
      "Epoch 4960, Loss: 1.71938955783844, Final Batch Loss: 0.3568198084831238\n",
      "Epoch 4961, Loss: 1.8045791685581207, Final Batch Loss: 0.4144134819507599\n",
      "Epoch 4962, Loss: 1.6928526163101196, Final Batch Loss: 0.4039546847343445\n",
      "Epoch 4963, Loss: 1.8226862251758575, Final Batch Loss: 0.26515811681747437\n",
      "Epoch 4964, Loss: 1.7154621481895447, Final Batch Loss: 0.34586045145988464\n",
      "Epoch 4965, Loss: 1.9139942228794098, Final Batch Loss: 0.3818398118019104\n",
      "Epoch 4966, Loss: 1.6843265295028687, Final Batch Loss: 0.374789297580719\n",
      "Epoch 4967, Loss: 1.768975019454956, Final Batch Loss: 0.3984110951423645\n",
      "Epoch 4968, Loss: 1.9264608919620514, Final Batch Loss: 0.30598315596580505\n",
      "Epoch 4969, Loss: 1.8025459349155426, Final Batch Loss: 0.4030557870864868\n",
      "Epoch 4970, Loss: 1.7067498564720154, Final Batch Loss: 0.30071917176246643\n",
      "Epoch 4971, Loss: 1.8483112454414368, Final Batch Loss: 0.3634394705295563\n",
      "Epoch 4972, Loss: 1.747749775648117, Final Batch Loss: 0.2605612277984619\n",
      "Epoch 4973, Loss: 1.8168246150016785, Final Batch Loss: 0.27972880005836487\n",
      "Epoch 4974, Loss: 2.1265475153923035, Final Batch Loss: 0.58262038230896\n",
      "Epoch 4975, Loss: 1.756949007511139, Final Batch Loss: 0.33096277713775635\n",
      "Epoch 4976, Loss: 2.03210112452507, Final Batch Loss: 0.4289785921573639\n",
      "Epoch 4977, Loss: 1.9220556914806366, Final Batch Loss: 0.3230690360069275\n",
      "Epoch 4978, Loss: 1.8229331076145172, Final Batch Loss: 0.40376946330070496\n",
      "Epoch 4979, Loss: 1.7054841816425323, Final Batch Loss: 0.3474830389022827\n",
      "Epoch 4980, Loss: 1.7314647734165192, Final Batch Loss: 0.33739060163497925\n",
      "Epoch 4981, Loss: 1.8259226083755493, Final Batch Loss: 0.34160327911376953\n",
      "Epoch 4982, Loss: 1.5920137465000153, Final Batch Loss: 0.3370583653450012\n",
      "Epoch 4983, Loss: 1.6728506684303284, Final Batch Loss: 0.2989331781864166\n",
      "Epoch 4984, Loss: 1.6512343287467957, Final Batch Loss: 0.35128942131996155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4985, Loss: 1.8820294439792633, Final Batch Loss: 0.32044926285743713\n",
      "Epoch 4986, Loss: 1.8054917454719543, Final Batch Loss: 0.2929190695285797\n",
      "Epoch 4987, Loss: 1.7976324260234833, Final Batch Loss: 0.2958645820617676\n",
      "Epoch 4988, Loss: 1.7094316482543945, Final Batch Loss: 0.32811465859413147\n",
      "Epoch 4989, Loss: 1.8305982947349548, Final Batch Loss: 0.48416346311569214\n",
      "Epoch 4990, Loss: 1.7054697573184967, Final Batch Loss: 0.32999297976493835\n",
      "Epoch 4991, Loss: 1.8044328391551971, Final Batch Loss: 0.3793478310108185\n",
      "Epoch 4992, Loss: 1.765004813671112, Final Batch Loss: 0.38186419010162354\n",
      "Epoch 4993, Loss: 1.9521130621433258, Final Batch Loss: 0.349833220243454\n",
      "Epoch 4994, Loss: 1.724269539117813, Final Batch Loss: 0.40107518434524536\n",
      "Epoch 4995, Loss: 1.8561604917049408, Final Batch Loss: 0.3460639715194702\n",
      "Epoch 4996, Loss: 1.7009493708610535, Final Batch Loss: 0.3142680823802948\n",
      "Epoch 4997, Loss: 1.8196102678775787, Final Batch Loss: 0.4154607951641083\n",
      "Epoch 4998, Loss: 1.7315300703048706, Final Batch Loss: 0.36129388213157654\n",
      "Epoch 4999, Loss: 1.850184142589569, Final Batch Loss: 0.4515438377857208\n",
      "Epoch 5000, Loss: 1.8408657610416412, Final Batch Loss: 0.38693010807037354\n",
      "Epoch 5001, Loss: 1.9998979270458221, Final Batch Loss: 0.3146405816078186\n",
      "Epoch 5002, Loss: 1.833087295293808, Final Batch Loss: 0.345071405172348\n",
      "Epoch 5003, Loss: 1.6825603246688843, Final Batch Loss: 0.3299778997898102\n",
      "Epoch 5004, Loss: 1.7689785063266754, Final Batch Loss: 0.4740908443927765\n",
      "Epoch 5005, Loss: 1.8351358473300934, Final Batch Loss: 0.3962269127368927\n",
      "Epoch 5006, Loss: 1.8602757155895233, Final Batch Loss: 0.42705342173576355\n",
      "Epoch 5007, Loss: 1.7831682562828064, Final Batch Loss: 0.4097166359424591\n",
      "Epoch 5008, Loss: 1.776353120803833, Final Batch Loss: 0.3512677252292633\n",
      "Epoch 5009, Loss: 1.742603063583374, Final Batch Loss: 0.3045094609260559\n",
      "Epoch 5010, Loss: 1.8911642730236053, Final Batch Loss: 0.32927441596984863\n",
      "Epoch 5011, Loss: 1.7246682941913605, Final Batch Loss: 0.31887972354888916\n",
      "Epoch 5012, Loss: 1.5825950801372528, Final Batch Loss: 0.2349894940853119\n",
      "Epoch 5013, Loss: 1.6406303346157074, Final Batch Loss: 0.31661128997802734\n",
      "Epoch 5014, Loss: 1.6982668936252594, Final Batch Loss: 0.37451156973838806\n",
      "Epoch 5015, Loss: 1.733927309513092, Final Batch Loss: 0.3531467914581299\n",
      "Epoch 5016, Loss: 1.9294882416725159, Final Batch Loss: 0.42358601093292236\n",
      "Epoch 5017, Loss: 1.9229203462600708, Final Batch Loss: 0.3475390076637268\n",
      "Epoch 5018, Loss: 1.7297410368919373, Final Batch Loss: 0.3565417528152466\n",
      "Epoch 5019, Loss: 1.847252368927002, Final Batch Loss: 0.3760535717010498\n",
      "Epoch 5020, Loss: 1.77584907412529, Final Batch Loss: 0.32035118341445923\n",
      "Epoch 5021, Loss: 1.7928812801837921, Final Batch Loss: 0.3590426743030548\n",
      "Epoch 5022, Loss: 1.8322101831436157, Final Batch Loss: 0.35409384965896606\n",
      "Epoch 5023, Loss: 1.7688794434070587, Final Batch Loss: 0.27392545342445374\n",
      "Epoch 5024, Loss: 1.8183226585388184, Final Batch Loss: 0.43215107917785645\n",
      "Epoch 5025, Loss: 1.8447418808937073, Final Batch Loss: 0.3722720742225647\n",
      "Epoch 5026, Loss: 1.7407914102077484, Final Batch Loss: 0.386916846036911\n",
      "Epoch 5027, Loss: 1.7183035910129547, Final Batch Loss: 0.3651277720928192\n",
      "Epoch 5028, Loss: 1.7372560799121857, Final Batch Loss: 0.3888394832611084\n",
      "Epoch 5029, Loss: 1.660518765449524, Final Batch Loss: 0.27637046575546265\n",
      "Epoch 5030, Loss: 1.739062786102295, Final Batch Loss: 0.3401179015636444\n",
      "Epoch 5031, Loss: 1.8951510190963745, Final Batch Loss: 0.3749110698699951\n",
      "Epoch 5032, Loss: 1.7684145867824554, Final Batch Loss: 0.3479306399822235\n",
      "Epoch 5033, Loss: 1.788034349679947, Final Batch Loss: 0.4323565661907196\n",
      "Epoch 5034, Loss: 1.8237746357917786, Final Batch Loss: 0.30120065808296204\n",
      "Epoch 5035, Loss: 1.8209618479013443, Final Batch Loss: 0.3095819056034088\n",
      "Epoch 5036, Loss: 1.7526077330112457, Final Batch Loss: 0.30234915018081665\n",
      "Epoch 5037, Loss: 1.7093925476074219, Final Batch Loss: 0.27067437767982483\n",
      "Epoch 5038, Loss: 1.7164621353149414, Final Batch Loss: 0.3479880094528198\n",
      "Epoch 5039, Loss: 1.675330400466919, Final Batch Loss: 0.27027422189712524\n",
      "Epoch 5040, Loss: 1.7506262362003326, Final Batch Loss: 0.4348610043525696\n",
      "Epoch 5041, Loss: 1.7147791385650635, Final Batch Loss: 0.2920909821987152\n",
      "Epoch 5042, Loss: 1.8139698505401611, Final Batch Loss: 0.27158570289611816\n",
      "Epoch 5043, Loss: 1.750779241323471, Final Batch Loss: 0.4350295066833496\n",
      "Epoch 5044, Loss: 1.7368904948234558, Final Batch Loss: 0.3125763535499573\n",
      "Epoch 5045, Loss: 1.788430243730545, Final Batch Loss: 0.4273780882358551\n",
      "Epoch 5046, Loss: 1.8838312029838562, Final Batch Loss: 0.315194696187973\n",
      "Epoch 5047, Loss: 1.7458041608333588, Final Batch Loss: 0.3536577820777893\n",
      "Epoch 5048, Loss: 1.7615656852722168, Final Batch Loss: 0.4470447301864624\n",
      "Epoch 5049, Loss: 2.0003021955490112, Final Batch Loss: 0.521735429763794\n",
      "Epoch 5050, Loss: 1.6598871946334839, Final Batch Loss: 0.26847124099731445\n",
      "Epoch 5051, Loss: 1.7904073894023895, Final Batch Loss: 0.35009485483169556\n",
      "Epoch 5052, Loss: 1.8653209805488586, Final Batch Loss: 0.46721282601356506\n",
      "Epoch 5053, Loss: 1.991755485534668, Final Batch Loss: 0.3178137242794037\n",
      "Epoch 5054, Loss: 1.8684082627296448, Final Batch Loss: 0.4005224406719208\n",
      "Epoch 5055, Loss: 1.8435369431972504, Final Batch Loss: 0.3634575307369232\n",
      "Epoch 5056, Loss: 1.9775350689888, Final Batch Loss: 0.3924781084060669\n",
      "Epoch 5057, Loss: 1.7709889113903046, Final Batch Loss: 0.39374443888664246\n",
      "Epoch 5058, Loss: 1.8257544338703156, Final Batch Loss: 0.354885995388031\n",
      "Epoch 5059, Loss: 1.8487649857997894, Final Batch Loss: 0.3347005844116211\n",
      "Epoch 5060, Loss: 1.7865874469280243, Final Batch Loss: 0.2853294610977173\n",
      "Epoch 5061, Loss: 1.8056080639362335, Final Batch Loss: 0.33502888679504395\n",
      "Epoch 5062, Loss: 1.8423429727554321, Final Batch Loss: 0.4108699560165405\n",
      "Epoch 5063, Loss: 1.612700030207634, Final Batch Loss: 0.2254096418619156\n",
      "Epoch 5064, Loss: 1.9516423046588898, Final Batch Loss: 0.2799670398235321\n",
      "Epoch 5065, Loss: 1.8246957063674927, Final Batch Loss: 0.4263368248939514\n",
      "Epoch 5066, Loss: 1.8561407923698425, Final Batch Loss: 0.39761054515838623\n",
      "Epoch 5067, Loss: 1.8352002501487732, Final Batch Loss: 0.3940463364124298\n",
      "Epoch 5068, Loss: 1.673653095960617, Final Batch Loss: 0.28897321224212646\n",
      "Epoch 5069, Loss: 1.799368530511856, Final Batch Loss: 0.3896017074584961\n",
      "Epoch 5070, Loss: 1.9572922885417938, Final Batch Loss: 0.3982856869697571\n",
      "Epoch 5071, Loss: 1.8411831557750702, Final Batch Loss: 0.309049129486084\n",
      "Epoch 5072, Loss: 1.8164026737213135, Final Batch Loss: 0.3476272225379944\n",
      "Epoch 5073, Loss: 1.6595963835716248, Final Batch Loss: 0.30192580819129944\n",
      "Epoch 5074, Loss: 1.6856005489826202, Final Batch Loss: 0.31616002321243286\n",
      "Epoch 5075, Loss: 1.8988050818443298, Final Batch Loss: 0.44310975074768066\n",
      "Epoch 5076, Loss: 1.6779535412788391, Final Batch Loss: 0.30909574031829834\n",
      "Epoch 5077, Loss: 1.5670364499092102, Final Batch Loss: 0.37322312593460083\n",
      "Epoch 5078, Loss: 1.7779485285282135, Final Batch Loss: 0.47415873408317566\n",
      "Epoch 5079, Loss: 1.9813858270645142, Final Batch Loss: 0.5276188254356384\n",
      "Epoch 5080, Loss: 1.902853637933731, Final Batch Loss: 0.3945525884628296\n",
      "Epoch 5081, Loss: 1.8057544529438019, Final Batch Loss: 0.38348615169525146\n",
      "Epoch 5082, Loss: 1.6577740013599396, Final Batch Loss: 0.46098145842552185\n",
      "Epoch 5083, Loss: 1.7010380923748016, Final Batch Loss: 0.34884074330329895\n",
      "Epoch 5084, Loss: 1.8486626148223877, Final Batch Loss: 0.4055691957473755\n",
      "Epoch 5085, Loss: 1.626750186085701, Final Batch Loss: 0.3554784655570984\n",
      "Epoch 5086, Loss: 1.6650608330965042, Final Batch Loss: 0.33871135115623474\n",
      "Epoch 5087, Loss: 1.8368763029575348, Final Batch Loss: 0.3816303312778473\n",
      "Epoch 5088, Loss: 1.8113174438476562, Final Batch Loss: 0.32559147477149963\n",
      "Epoch 5089, Loss: 1.82869553565979, Final Batch Loss: 0.3960716426372528\n",
      "Epoch 5090, Loss: 1.7282549738883972, Final Batch Loss: 0.38853883743286133\n",
      "Epoch 5091, Loss: 1.8019631803035736, Final Batch Loss: 0.34722375869750977\n",
      "Epoch 5092, Loss: 1.6849847435951233, Final Batch Loss: 0.25721219182014465\n",
      "Epoch 5093, Loss: 1.6428058445453644, Final Batch Loss: 0.32128340005874634\n",
      "Epoch 5094, Loss: 1.629843920469284, Final Batch Loss: 0.3532249927520752\n",
      "Epoch 5095, Loss: 1.7552916705608368, Final Batch Loss: 0.32264357805252075\n",
      "Epoch 5096, Loss: 1.8257133960723877, Final Batch Loss: 0.4310482442378998\n",
      "Epoch 5097, Loss: 1.7135363817214966, Final Batch Loss: 0.40530869364738464\n",
      "Epoch 5098, Loss: 1.7223816812038422, Final Batch Loss: 0.27666953206062317\n",
      "Epoch 5099, Loss: 1.7575335204601288, Final Batch Loss: 0.34371218085289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5100, Loss: 1.7314831912517548, Final Batch Loss: 0.3532988727092743\n",
      "Epoch 5101, Loss: 1.751351773738861, Final Batch Loss: 0.35821473598480225\n",
      "Epoch 5102, Loss: 1.7451083958148956, Final Batch Loss: 0.28350427746772766\n",
      "Epoch 5103, Loss: 1.8345335721969604, Final Batch Loss: 0.30402395129203796\n",
      "Epoch 5104, Loss: 1.9089044332504272, Final Batch Loss: 0.3905316889286041\n",
      "Epoch 5105, Loss: 1.8969647586345673, Final Batch Loss: 0.5244496464729309\n",
      "Epoch 5106, Loss: 1.7659940421581268, Final Batch Loss: 0.3599255084991455\n",
      "Epoch 5107, Loss: 1.6215855479240417, Final Batch Loss: 0.32600733637809753\n",
      "Epoch 5108, Loss: 1.7094250321388245, Final Batch Loss: 0.30670878291130066\n",
      "Epoch 5109, Loss: 1.768738567829132, Final Batch Loss: 0.3712416887283325\n",
      "Epoch 5110, Loss: 1.618053823709488, Final Batch Loss: 0.23339593410491943\n",
      "Epoch 5111, Loss: 1.8975229859352112, Final Batch Loss: 0.36655306816101074\n",
      "Epoch 5112, Loss: 1.8830472826957703, Final Batch Loss: 0.4049833118915558\n",
      "Epoch 5113, Loss: 1.7792192995548248, Final Batch Loss: 0.3398648202419281\n",
      "Epoch 5114, Loss: 1.6550605595111847, Final Batch Loss: 0.3290684223175049\n",
      "Epoch 5115, Loss: 1.8126096427440643, Final Batch Loss: 0.41833096742630005\n",
      "Epoch 5116, Loss: 1.9417099952697754, Final Batch Loss: 0.36990055441856384\n",
      "Epoch 5117, Loss: 1.7832846939563751, Final Batch Loss: 0.3402765393257141\n",
      "Epoch 5118, Loss: 1.6556886434555054, Final Batch Loss: 0.29301097989082336\n",
      "Epoch 5119, Loss: 1.7226326763629913, Final Batch Loss: 0.2757921516895294\n",
      "Epoch 5120, Loss: 1.7739589512348175, Final Batch Loss: 0.4049573838710785\n",
      "Epoch 5121, Loss: 1.708014577627182, Final Batch Loss: 0.37134313583374023\n",
      "Epoch 5122, Loss: 1.75187087059021, Final Batch Loss: 0.351244181394577\n",
      "Epoch 5123, Loss: 1.9541140496730804, Final Batch Loss: 0.36767613887786865\n",
      "Epoch 5124, Loss: 1.7215132415294647, Final Batch Loss: 0.3080877959728241\n",
      "Epoch 5125, Loss: 1.8071618974208832, Final Batch Loss: 0.36063820123672485\n",
      "Epoch 5126, Loss: 1.797751545906067, Final Batch Loss: 0.3898635506629944\n",
      "Epoch 5127, Loss: 1.7994142472743988, Final Batch Loss: 0.3384740352630615\n",
      "Epoch 5128, Loss: 1.6726946830749512, Final Batch Loss: 0.293377161026001\n",
      "Epoch 5129, Loss: 1.7165656089782715, Final Batch Loss: 0.45197388529777527\n",
      "Epoch 5130, Loss: 1.5774884819984436, Final Batch Loss: 0.34728550910949707\n",
      "Epoch 5131, Loss: 1.6943290382623672, Final Batch Loss: 0.2944049537181854\n",
      "Epoch 5132, Loss: 1.7485372722148895, Final Batch Loss: 0.40832701325416565\n",
      "Epoch 5133, Loss: 1.736458033323288, Final Batch Loss: 0.29835665225982666\n",
      "Epoch 5134, Loss: 1.8510129749774933, Final Batch Loss: 0.4294048547744751\n",
      "Epoch 5135, Loss: 1.7635989785194397, Final Batch Loss: 0.35908424854278564\n",
      "Epoch 5136, Loss: 1.6575863659381866, Final Batch Loss: 0.3339344561100006\n",
      "Epoch 5137, Loss: 1.7503607869148254, Final Batch Loss: 0.33732980489730835\n",
      "Epoch 5138, Loss: 1.7693061232566833, Final Batch Loss: 0.3089298605918884\n",
      "Epoch 5139, Loss: 1.7536468803882599, Final Batch Loss: 0.2717095911502838\n",
      "Epoch 5140, Loss: 1.6988153457641602, Final Batch Loss: 0.32969239354133606\n",
      "Epoch 5141, Loss: 1.7562269866466522, Final Batch Loss: 0.3375071883201599\n",
      "Epoch 5142, Loss: 1.6506904065608978, Final Batch Loss: 0.3248075246810913\n",
      "Epoch 5143, Loss: 1.711379885673523, Final Batch Loss: 0.32000598311424255\n",
      "Epoch 5144, Loss: 1.796841949224472, Final Batch Loss: 0.30729642510414124\n",
      "Epoch 5145, Loss: 1.7412481606006622, Final Batch Loss: 0.43816936016082764\n",
      "Epoch 5146, Loss: 1.771809995174408, Final Batch Loss: 0.39160484075546265\n",
      "Epoch 5147, Loss: 1.8030950427055359, Final Batch Loss: 0.3112833499908447\n",
      "Epoch 5148, Loss: 1.817318469285965, Final Batch Loss: 0.37806034088134766\n",
      "Epoch 5149, Loss: 1.8790605068206787, Final Batch Loss: 0.383232057094574\n",
      "Epoch 5150, Loss: 1.842207819223404, Final Batch Loss: 0.4490775763988495\n",
      "Epoch 5151, Loss: 1.7694088220596313, Final Batch Loss: 0.3955111801624298\n",
      "Epoch 5152, Loss: 1.7734069228172302, Final Batch Loss: 0.31712403893470764\n",
      "Epoch 5153, Loss: 1.905005782842636, Final Batch Loss: 0.3809225559234619\n",
      "Epoch 5154, Loss: 2.0535191893577576, Final Batch Loss: 0.3503919541835785\n",
      "Epoch 5155, Loss: 1.8694748878479004, Final Batch Loss: 0.45255404710769653\n",
      "Epoch 5156, Loss: 1.8908906280994415, Final Batch Loss: 0.3463897705078125\n",
      "Epoch 5157, Loss: 1.8648085296154022, Final Batch Loss: 0.4502391219139099\n",
      "Epoch 5158, Loss: 1.668088287115097, Final Batch Loss: 0.40536779165267944\n",
      "Epoch 5159, Loss: 1.6793111562728882, Final Batch Loss: 0.3444788455963135\n",
      "Epoch 5160, Loss: 1.6110204756259918, Final Batch Loss: 0.2760334014892578\n",
      "Epoch 5161, Loss: 1.7158967852592468, Final Batch Loss: 0.31269195675849915\n",
      "Epoch 5162, Loss: 1.7901217639446259, Final Batch Loss: 0.25563523173332214\n",
      "Epoch 5163, Loss: 1.6442875266075134, Final Batch Loss: 0.33372965455055237\n",
      "Epoch 5164, Loss: 1.7364134639501572, Final Batch Loss: 0.22459359467029572\n",
      "Epoch 5165, Loss: 1.755103200674057, Final Batch Loss: 0.36538806557655334\n",
      "Epoch 5166, Loss: 1.8085747063159943, Final Batch Loss: 0.3526129722595215\n",
      "Epoch 5167, Loss: 1.8966010808944702, Final Batch Loss: 0.41921567916870117\n",
      "Epoch 5168, Loss: 1.806016355752945, Final Batch Loss: 0.40183043479919434\n",
      "Epoch 5169, Loss: 1.9361125826835632, Final Batch Loss: 0.45613574981689453\n",
      "Epoch 5170, Loss: 1.7589561343193054, Final Batch Loss: 0.34622448682785034\n",
      "Epoch 5171, Loss: 1.9759251475334167, Final Batch Loss: 0.4163549840450287\n",
      "Epoch 5172, Loss: 1.7521850168704987, Final Batch Loss: 0.3056599497795105\n",
      "Epoch 5173, Loss: 1.8695400655269623, Final Batch Loss: 0.3973942697048187\n",
      "Epoch 5174, Loss: 1.6726582944393158, Final Batch Loss: 0.29563769698143005\n",
      "Epoch 5175, Loss: 1.8639764487743378, Final Batch Loss: 0.3509501814842224\n",
      "Epoch 5176, Loss: 1.5880667567253113, Final Batch Loss: 0.3272804319858551\n",
      "Epoch 5177, Loss: 1.9298806488513947, Final Batch Loss: 0.37123388051986694\n",
      "Epoch 5178, Loss: 1.8733889162540436, Final Batch Loss: 0.3909536302089691\n",
      "Epoch 5179, Loss: 1.7196412682533264, Final Batch Loss: 0.37090346217155457\n",
      "Epoch 5180, Loss: 1.7769089043140411, Final Batch Loss: 0.3797371983528137\n",
      "Epoch 5181, Loss: 1.9352551102638245, Final Batch Loss: 0.4236573874950409\n",
      "Epoch 5182, Loss: 1.6727826297283173, Final Batch Loss: 0.2937169671058655\n",
      "Epoch 5183, Loss: 1.662345051765442, Final Batch Loss: 0.34642544388771057\n",
      "Epoch 5184, Loss: 1.706114500761032, Final Batch Loss: 0.3258582651615143\n",
      "Epoch 5185, Loss: 1.939366489648819, Final Batch Loss: 0.3935849368572235\n",
      "Epoch 5186, Loss: 1.8305339217185974, Final Batch Loss: 0.32743802666664124\n",
      "Epoch 5187, Loss: 1.7232094407081604, Final Batch Loss: 0.2633782625198364\n",
      "Epoch 5188, Loss: 1.7115597426891327, Final Batch Loss: 0.31409960985183716\n",
      "Epoch 5189, Loss: 1.7978966236114502, Final Batch Loss: 0.38683226704597473\n",
      "Epoch 5190, Loss: 1.7013135254383087, Final Batch Loss: 0.2921805679798126\n",
      "Epoch 5191, Loss: 1.6200550496578217, Final Batch Loss: 0.36217400431632996\n",
      "Epoch 5192, Loss: 1.756454974412918, Final Batch Loss: 0.36190471053123474\n",
      "Epoch 5193, Loss: 1.84697425365448, Final Batch Loss: 0.4944988489151001\n",
      "Epoch 5194, Loss: 1.7956231534481049, Final Batch Loss: 0.3500843942165375\n",
      "Epoch 5195, Loss: 1.8269839882850647, Final Batch Loss: 0.3766186833381653\n",
      "Epoch 5196, Loss: 1.7296109199523926, Final Batch Loss: 0.3000567853450775\n",
      "Epoch 5197, Loss: 1.6400545537471771, Final Batch Loss: 0.3001742660999298\n",
      "Epoch 5198, Loss: 1.7922454476356506, Final Batch Loss: 0.3150177299976349\n",
      "Epoch 5199, Loss: 1.8755639493465424, Final Batch Loss: 0.44148939847946167\n",
      "Epoch 5200, Loss: 1.6797727048397064, Final Batch Loss: 0.43002939224243164\n",
      "Epoch 5201, Loss: 1.6687453985214233, Final Batch Loss: 0.2916963994503021\n",
      "Epoch 5202, Loss: 1.8867948055267334, Final Batch Loss: 0.422384649515152\n",
      "Epoch 5203, Loss: 1.7817142307758331, Final Batch Loss: 0.3622870147228241\n",
      "Epoch 5204, Loss: 1.6572235524654388, Final Batch Loss: 0.3907071352005005\n",
      "Epoch 5205, Loss: 1.7955401241779327, Final Batch Loss: 0.3601587414741516\n",
      "Epoch 5206, Loss: 1.8120056092739105, Final Batch Loss: 0.4737122356891632\n",
      "Epoch 5207, Loss: 1.6299839913845062, Final Batch Loss: 0.2903014123439789\n",
      "Epoch 5208, Loss: 1.8449573516845703, Final Batch Loss: 0.3587227761745453\n",
      "Epoch 5209, Loss: 1.773383229970932, Final Batch Loss: 0.347806453704834\n",
      "Epoch 5210, Loss: 1.8232698142528534, Final Batch Loss: 0.3144889175891876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5211, Loss: 1.7384507656097412, Final Batch Loss: 0.39572766423225403\n",
      "Epoch 5212, Loss: 1.6665897071361542, Final Batch Loss: 0.30030015110969543\n",
      "Epoch 5213, Loss: 1.7220804393291473, Final Batch Loss: 0.25918135046958923\n",
      "Epoch 5214, Loss: 1.6592224687337875, Final Batch Loss: 0.3693682849407196\n",
      "Epoch 5215, Loss: 1.767132580280304, Final Batch Loss: 0.29605695605278015\n",
      "Epoch 5216, Loss: 1.7059605121612549, Final Batch Loss: 0.39451029896736145\n",
      "Epoch 5217, Loss: 1.7598706185817719, Final Batch Loss: 0.35266977548599243\n",
      "Epoch 5218, Loss: 1.6991314589977264, Final Batch Loss: 0.31150302290916443\n",
      "Epoch 5219, Loss: 1.7428278625011444, Final Batch Loss: 0.4051353931427002\n",
      "Epoch 5220, Loss: 1.7000052630901337, Final Batch Loss: 0.3417150676250458\n",
      "Epoch 5221, Loss: 1.7808184325695038, Final Batch Loss: 0.29444295167922974\n",
      "Epoch 5222, Loss: 1.7117169797420502, Final Batch Loss: 0.437240868806839\n",
      "Epoch 5223, Loss: 1.8844152390956879, Final Batch Loss: 0.31449809670448303\n",
      "Epoch 5224, Loss: 1.8470410406589508, Final Batch Loss: 0.31021732091903687\n",
      "Epoch 5225, Loss: 1.5514552295207977, Final Batch Loss: 0.3226192593574524\n",
      "Epoch 5226, Loss: 1.679604411125183, Final Batch Loss: 0.3237743675708771\n",
      "Epoch 5227, Loss: 1.872906893491745, Final Batch Loss: 0.3367510735988617\n",
      "Epoch 5228, Loss: 1.8096829056739807, Final Batch Loss: 0.34654542803764343\n",
      "Epoch 5229, Loss: 1.6785805523395538, Final Batch Loss: 0.3379993438720703\n",
      "Epoch 5230, Loss: 1.7614858150482178, Final Batch Loss: 0.39662155508995056\n",
      "Epoch 5231, Loss: 1.794887363910675, Final Batch Loss: 0.303810179233551\n",
      "Epoch 5232, Loss: 2.1401277482509613, Final Batch Loss: 0.3819383680820465\n",
      "Epoch 5233, Loss: 1.916616976261139, Final Batch Loss: 0.43086352944374084\n",
      "Epoch 5234, Loss: 1.8161933720111847, Final Batch Loss: 0.38950115442276\n",
      "Epoch 5235, Loss: 1.8537302911281586, Final Batch Loss: 0.33781659603118896\n",
      "Epoch 5236, Loss: 1.9533641636371613, Final Batch Loss: 0.39118802547454834\n",
      "Epoch 5237, Loss: 1.7359065115451813, Final Batch Loss: 0.3317941725254059\n",
      "Epoch 5238, Loss: 1.6886744499206543, Final Batch Loss: 0.34596478939056396\n",
      "Epoch 5239, Loss: 1.703714370727539, Final Batch Loss: 0.28819939494132996\n",
      "Epoch 5240, Loss: 1.669281706213951, Final Batch Loss: 0.3654763400554657\n",
      "Epoch 5241, Loss: 1.779539242386818, Final Batch Loss: 0.47010406851768494\n",
      "Epoch 5242, Loss: 1.8323973417282104, Final Batch Loss: 0.3039745092391968\n",
      "Epoch 5243, Loss: 1.807638019323349, Final Batch Loss: 0.33181118965148926\n",
      "Epoch 5244, Loss: 1.7976359128952026, Final Batch Loss: 0.37812551856040955\n",
      "Epoch 5245, Loss: 1.8329890668392181, Final Batch Loss: 0.32206958532333374\n",
      "Epoch 5246, Loss: 1.8127437233924866, Final Batch Loss: 0.3717033565044403\n",
      "Epoch 5247, Loss: 1.777732789516449, Final Batch Loss: 0.368283748626709\n",
      "Epoch 5248, Loss: 1.7661631107330322, Final Batch Loss: 0.47127076983451843\n",
      "Epoch 5249, Loss: 1.6738038957118988, Final Batch Loss: 0.3427777886390686\n",
      "Epoch 5250, Loss: 1.596040427684784, Final Batch Loss: 0.307150274515152\n",
      "Epoch 5251, Loss: 1.732749193906784, Final Batch Loss: 0.3443596065044403\n",
      "Epoch 5252, Loss: 1.8036540150642395, Final Batch Loss: 0.3868161141872406\n",
      "Epoch 5253, Loss: 1.6844026744365692, Final Batch Loss: 0.3187407851219177\n",
      "Epoch 5254, Loss: 1.8203720450401306, Final Batch Loss: 0.39919719099998474\n",
      "Epoch 5255, Loss: 1.9009372889995575, Final Batch Loss: 0.5026840567588806\n",
      "Epoch 5256, Loss: 1.8109653890132904, Final Batch Loss: 0.36203819513320923\n",
      "Epoch 5257, Loss: 1.85201957821846, Final Batch Loss: 0.3631868064403534\n",
      "Epoch 5258, Loss: 1.8789827227592468, Final Batch Loss: 0.42876136302948\n",
      "Epoch 5259, Loss: 1.6836433708667755, Final Batch Loss: 0.35572463274002075\n",
      "Epoch 5260, Loss: 1.85374516248703, Final Batch Loss: 0.3550017476081848\n",
      "Epoch 5261, Loss: 1.6700393557548523, Final Batch Loss: 0.32547110319137573\n",
      "Epoch 5262, Loss: 1.8069071173667908, Final Batch Loss: 0.3403249979019165\n",
      "Epoch 5263, Loss: 1.7811569273471832, Final Batch Loss: 0.353617399930954\n",
      "Epoch 5264, Loss: 1.7752299308776855, Final Batch Loss: 0.32619205117225647\n",
      "Epoch 5265, Loss: 1.7392796874046326, Final Batch Loss: 0.28979015350341797\n",
      "Epoch 5266, Loss: 1.772899180650711, Final Batch Loss: 0.36756160855293274\n",
      "Epoch 5267, Loss: 1.7540216445922852, Final Batch Loss: 0.3879070281982422\n",
      "Epoch 5268, Loss: 1.8364140689373016, Final Batch Loss: 0.36378827691078186\n",
      "Epoch 5269, Loss: 1.7327455878257751, Final Batch Loss: 0.35108861327171326\n",
      "Epoch 5270, Loss: 1.7286734282970428, Final Batch Loss: 0.4206855595111847\n",
      "Epoch 5271, Loss: 1.8067570328712463, Final Batch Loss: 0.34067460894584656\n",
      "Epoch 5272, Loss: 1.9284420609474182, Final Batch Loss: 0.32804596424102783\n",
      "Epoch 5273, Loss: 1.7032521963119507, Final Batch Loss: 0.31142133474349976\n",
      "Epoch 5274, Loss: 1.9458163380622864, Final Batch Loss: 0.31019127368927\n",
      "Epoch 5275, Loss: 1.6685942709445953, Final Batch Loss: 0.3379848003387451\n",
      "Epoch 5276, Loss: 1.634292095899582, Final Batch Loss: 0.3568241000175476\n",
      "Epoch 5277, Loss: 1.814055472612381, Final Batch Loss: 0.3912384510040283\n",
      "Epoch 5278, Loss: 1.825652003288269, Final Batch Loss: 0.3525139093399048\n",
      "Epoch 5279, Loss: 1.731312781572342, Final Batch Loss: 0.3708440959453583\n",
      "Epoch 5280, Loss: 1.937235951423645, Final Batch Loss: 0.33717814087867737\n",
      "Epoch 5281, Loss: 1.7734248340129852, Final Batch Loss: 0.295896053314209\n",
      "Epoch 5282, Loss: 1.806724339723587, Final Batch Loss: 0.408786416053772\n",
      "Epoch 5283, Loss: 1.9603091180324554, Final Batch Loss: 0.47992998361587524\n",
      "Epoch 5284, Loss: 1.6116224229335785, Final Batch Loss: 0.25590842962265015\n",
      "Epoch 5285, Loss: 1.7914170622825623, Final Batch Loss: 0.3533529043197632\n",
      "Epoch 5286, Loss: 1.7068929970264435, Final Batch Loss: 0.2930149734020233\n",
      "Epoch 5287, Loss: 1.789936125278473, Final Batch Loss: 0.37518271803855896\n",
      "Epoch 5288, Loss: 1.803288608789444, Final Batch Loss: 0.3785865604877472\n",
      "Epoch 5289, Loss: 1.6001723408699036, Final Batch Loss: 0.31964901089668274\n",
      "Epoch 5290, Loss: 1.7419686615467072, Final Batch Loss: 0.3224486708641052\n",
      "Epoch 5291, Loss: 1.9224872887134552, Final Batch Loss: 0.39676570892333984\n",
      "Epoch 5292, Loss: 1.878254473209381, Final Batch Loss: 0.4223203957080841\n",
      "Epoch 5293, Loss: 1.9561142325401306, Final Batch Loss: 0.4531450867652893\n",
      "Epoch 5294, Loss: 1.7807626724243164, Final Batch Loss: 0.28547924757003784\n",
      "Epoch 5295, Loss: 1.6467420160770416, Final Batch Loss: 0.3466218113899231\n",
      "Epoch 5296, Loss: 1.8446764051914215, Final Batch Loss: 0.32413750886917114\n",
      "Epoch 5297, Loss: 1.855918526649475, Final Batch Loss: 0.3257893919944763\n",
      "Epoch 5298, Loss: 1.732806146144867, Final Batch Loss: 0.27789542078971863\n",
      "Epoch 5299, Loss: 1.639222651720047, Final Batch Loss: 0.33419063687324524\n",
      "Epoch 5300, Loss: 1.9609023928642273, Final Batch Loss: 0.3198283314704895\n",
      "Epoch 5301, Loss: 1.8675015270709991, Final Batch Loss: 0.30770909786224365\n",
      "Epoch 5302, Loss: 1.7414009869098663, Final Batch Loss: 0.35507795214653015\n",
      "Epoch 5303, Loss: 1.912559300661087, Final Batch Loss: 0.2996290624141693\n",
      "Epoch 5304, Loss: 1.7979828715324402, Final Batch Loss: 0.36867958307266235\n",
      "Epoch 5305, Loss: 1.7143501937389374, Final Batch Loss: 0.43558672070503235\n",
      "Epoch 5306, Loss: 1.7169755399227142, Final Batch Loss: 0.3848317265510559\n",
      "Epoch 5307, Loss: 1.7707876861095428, Final Batch Loss: 0.357634574174881\n",
      "Epoch 5308, Loss: 1.740929663181305, Final Batch Loss: 0.46868741512298584\n",
      "Epoch 5309, Loss: 1.8997854888439178, Final Batch Loss: 0.3372354209423065\n",
      "Epoch 5310, Loss: 1.870969533920288, Final Batch Loss: 0.5188032388687134\n",
      "Epoch 5311, Loss: 1.7667937576770782, Final Batch Loss: 0.2883415222167969\n",
      "Epoch 5312, Loss: 1.6364882588386536, Final Batch Loss: 0.258796751499176\n",
      "Epoch 5313, Loss: 1.811232089996338, Final Batch Loss: 0.3738948702812195\n",
      "Epoch 5314, Loss: 1.6765551567077637, Final Batch Loss: 0.3449859321117401\n",
      "Epoch 5315, Loss: 1.744299829006195, Final Batch Loss: 0.3691924512386322\n",
      "Epoch 5316, Loss: 1.686489760875702, Final Batch Loss: 0.33912211656570435\n",
      "Epoch 5317, Loss: 2.0265455543994904, Final Batch Loss: 0.5067245960235596\n",
      "Epoch 5318, Loss: 1.508457973599434, Final Batch Loss: 0.23302046954631805\n",
      "Epoch 5319, Loss: 1.818904846906662, Final Batch Loss: 0.3727911114692688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5320, Loss: 1.6383792757987976, Final Batch Loss: 0.29845738410949707\n",
      "Epoch 5321, Loss: 1.7626926004886627, Final Batch Loss: 0.37152084708213806\n",
      "Epoch 5322, Loss: 2.0670633614063263, Final Batch Loss: 0.4196268320083618\n",
      "Epoch 5323, Loss: 1.6764143109321594, Final Batch Loss: 0.3172030746936798\n",
      "Epoch 5324, Loss: 2.0073684453964233, Final Batch Loss: 0.45507076382637024\n",
      "Epoch 5325, Loss: 1.6635651290416718, Final Batch Loss: 0.29544588923454285\n",
      "Epoch 5326, Loss: 1.6994400918483734, Final Batch Loss: 0.3971937894821167\n",
      "Epoch 5327, Loss: 1.5746277570724487, Final Batch Loss: 0.2816679775714874\n",
      "Epoch 5328, Loss: 1.6818885207176208, Final Batch Loss: 0.3490072190761566\n",
      "Epoch 5329, Loss: 1.9181403368711472, Final Batch Loss: 0.43281781673431396\n",
      "Epoch 5330, Loss: 1.642422467470169, Final Batch Loss: 0.36721935868263245\n",
      "Epoch 5331, Loss: 1.5473265498876572, Final Batch Loss: 0.23614318668842316\n",
      "Epoch 5332, Loss: 1.7810722887516022, Final Batch Loss: 0.42896392941474915\n",
      "Epoch 5333, Loss: 1.9249885380268097, Final Batch Loss: 0.32627901434898376\n",
      "Epoch 5334, Loss: 1.766432523727417, Final Batch Loss: 0.29074394702911377\n",
      "Epoch 5335, Loss: 1.7787026166915894, Final Batch Loss: 0.3336966335773468\n",
      "Epoch 5336, Loss: 1.826607882976532, Final Batch Loss: 0.4327009320259094\n",
      "Epoch 5337, Loss: 1.8045040667057037, Final Batch Loss: 0.3804141879081726\n",
      "Epoch 5338, Loss: 1.7881017327308655, Final Batch Loss: 0.2917262613773346\n",
      "Epoch 5339, Loss: 1.6390564739704132, Final Batch Loss: 0.3156035542488098\n",
      "Epoch 5340, Loss: 1.7165597677230835, Final Batch Loss: 0.274376779794693\n",
      "Epoch 5341, Loss: 1.7147292792797089, Final Batch Loss: 0.3096941411495209\n",
      "Epoch 5342, Loss: 1.7216042876243591, Final Batch Loss: 0.3383217453956604\n",
      "Epoch 5343, Loss: 1.5951509773731232, Final Batch Loss: 0.35137251019477844\n",
      "Epoch 5344, Loss: 1.9162299036979675, Final Batch Loss: 0.5249107480049133\n",
      "Epoch 5345, Loss: 1.8887978792190552, Final Batch Loss: 0.43157100677490234\n",
      "Epoch 5346, Loss: 1.9087636768817902, Final Batch Loss: 0.4855690896511078\n",
      "Epoch 5347, Loss: 1.8732243478298187, Final Batch Loss: 0.30700352787971497\n",
      "Epoch 5348, Loss: 1.7836337387561798, Final Batch Loss: 0.44237127900123596\n",
      "Epoch 5349, Loss: 1.9254751801490784, Final Batch Loss: 0.42133212089538574\n",
      "Epoch 5350, Loss: 1.8066664338111877, Final Batch Loss: 0.3371290862560272\n",
      "Epoch 5351, Loss: 1.6118314862251282, Final Batch Loss: 0.3152700364589691\n",
      "Epoch 5352, Loss: 1.7865720093250275, Final Batch Loss: 0.268804132938385\n",
      "Epoch 5353, Loss: 1.8127877414226532, Final Batch Loss: 0.34805840253829956\n",
      "Epoch 5354, Loss: 1.8036292493343353, Final Batch Loss: 0.33671313524246216\n",
      "Epoch 5355, Loss: 1.6756402850151062, Final Batch Loss: 0.40070441365242004\n",
      "Epoch 5356, Loss: 1.7952679097652435, Final Batch Loss: 0.36729079484939575\n",
      "Epoch 5357, Loss: 1.7576711475849152, Final Batch Loss: 0.37467285990715027\n",
      "Epoch 5358, Loss: 1.7738186120986938, Final Batch Loss: 0.37824150919914246\n",
      "Epoch 5359, Loss: 1.6208354532718658, Final Batch Loss: 0.34450769424438477\n",
      "Epoch 5360, Loss: 1.6147298514842987, Final Batch Loss: 0.35140326619148254\n",
      "Epoch 5361, Loss: 1.7457128763198853, Final Batch Loss: 0.3171553313732147\n",
      "Epoch 5362, Loss: 1.7317178845405579, Final Batch Loss: 0.3928090035915375\n",
      "Epoch 5363, Loss: 1.689227283000946, Final Batch Loss: 0.3755446970462799\n",
      "Epoch 5364, Loss: 1.9652920365333557, Final Batch Loss: 0.3584096431732178\n",
      "Epoch 5365, Loss: 1.7422479391098022, Final Batch Loss: 0.3587455153465271\n",
      "Epoch 5366, Loss: 1.5484581291675568, Final Batch Loss: 0.37666743993759155\n",
      "Epoch 5367, Loss: 1.7311500906944275, Final Batch Loss: 0.42365726828575134\n",
      "Epoch 5368, Loss: 1.760094314813614, Final Batch Loss: 0.2841053903102875\n",
      "Epoch 5369, Loss: 1.78122678399086, Final Batch Loss: 0.3757038116455078\n",
      "Epoch 5370, Loss: 1.8063045740127563, Final Batch Loss: 0.48852527141571045\n",
      "Epoch 5371, Loss: 1.641334354877472, Final Batch Loss: 0.2723056674003601\n",
      "Epoch 5372, Loss: 1.7710905373096466, Final Batch Loss: 0.33606669306755066\n",
      "Epoch 5373, Loss: 1.7522100508213043, Final Batch Loss: 0.27129578590393066\n",
      "Epoch 5374, Loss: 1.782369077205658, Final Batch Loss: 0.31889429688453674\n",
      "Epoch 5375, Loss: 1.998352199792862, Final Batch Loss: 0.49035370349884033\n",
      "Epoch 5376, Loss: 1.6922398507595062, Final Batch Loss: 0.3282254934310913\n",
      "Epoch 5377, Loss: 1.8212964832782745, Final Batch Loss: 0.429033488035202\n",
      "Epoch 5378, Loss: 1.7566388547420502, Final Batch Loss: 0.39301639795303345\n",
      "Epoch 5379, Loss: 1.7709197998046875, Final Batch Loss: 0.38982680439949036\n",
      "Epoch 5380, Loss: 1.790115773677826, Final Batch Loss: 0.4600520431995392\n",
      "Epoch 5381, Loss: 1.6395400166511536, Final Batch Loss: 0.2755962014198303\n",
      "Epoch 5382, Loss: 1.531257301568985, Final Batch Loss: 0.3250237703323364\n",
      "Epoch 5383, Loss: 1.7734739184379578, Final Batch Loss: 0.3995479941368103\n",
      "Epoch 5384, Loss: 1.5220683217048645, Final Batch Loss: 0.3219608962535858\n",
      "Epoch 5385, Loss: 1.7321507930755615, Final Batch Loss: 0.3746754825115204\n",
      "Epoch 5386, Loss: 1.8065442442893982, Final Batch Loss: 0.434209942817688\n",
      "Epoch 5387, Loss: 1.8271693587303162, Final Batch Loss: 0.30957046151161194\n",
      "Epoch 5388, Loss: 1.803535133600235, Final Batch Loss: 0.4138854444026947\n",
      "Epoch 5389, Loss: 1.9939600229263306, Final Batch Loss: 0.40968695282936096\n",
      "Epoch 5390, Loss: 2.001907706260681, Final Batch Loss: 0.40101298689842224\n",
      "Epoch 5391, Loss: 1.778946340084076, Final Batch Loss: 0.3861675560474396\n",
      "Epoch 5392, Loss: 1.8782563209533691, Final Batch Loss: 0.35471272468566895\n",
      "Epoch 5393, Loss: 1.9170392155647278, Final Batch Loss: 0.41733086109161377\n",
      "Epoch 5394, Loss: 1.6326264142990112, Final Batch Loss: 0.35359928011894226\n",
      "Epoch 5395, Loss: 1.8260031938552856, Final Batch Loss: 0.3337245285511017\n",
      "Epoch 5396, Loss: 1.7822770178318024, Final Batch Loss: 0.3886122405529022\n",
      "Epoch 5397, Loss: 1.7241668403148651, Final Batch Loss: 0.3504689931869507\n",
      "Epoch 5398, Loss: 1.7627600729465485, Final Batch Loss: 0.3654542565345764\n",
      "Epoch 5399, Loss: 1.8422302305698395, Final Batch Loss: 0.4722047746181488\n",
      "Epoch 5400, Loss: 1.6797580122947693, Final Batch Loss: 0.3500857651233673\n",
      "Epoch 5401, Loss: 1.822810173034668, Final Batch Loss: 0.3733745515346527\n",
      "Epoch 5402, Loss: 1.8800062835216522, Final Batch Loss: 0.34770336747169495\n",
      "Epoch 5403, Loss: 1.7736991047859192, Final Batch Loss: 0.3533675968647003\n",
      "Epoch 5404, Loss: 1.6571169793605804, Final Batch Loss: 0.3111141622066498\n",
      "Epoch 5405, Loss: 1.7364047169685364, Final Batch Loss: 0.26623374223709106\n",
      "Epoch 5406, Loss: 1.7403790354728699, Final Batch Loss: 0.3527417480945587\n",
      "Epoch 5407, Loss: 1.8395521938800812, Final Batch Loss: 0.5281456708908081\n",
      "Epoch 5408, Loss: 1.8132835924625397, Final Batch Loss: 0.2723641097545624\n",
      "Epoch 5409, Loss: 1.7956955134868622, Final Batch Loss: 0.4069293737411499\n",
      "Epoch 5410, Loss: 1.6843470931053162, Final Batch Loss: 0.3465774357318878\n",
      "Epoch 5411, Loss: 1.66622856259346, Final Batch Loss: 0.27115917205810547\n",
      "Epoch 5412, Loss: 1.811268001794815, Final Batch Loss: 0.38186678290367126\n",
      "Epoch 5413, Loss: 1.5781360864639282, Final Batch Loss: 0.3222956359386444\n",
      "Epoch 5414, Loss: 1.728794276714325, Final Batch Loss: 0.29988768696784973\n",
      "Epoch 5415, Loss: 1.6947151720523834, Final Batch Loss: 0.29050424695014954\n",
      "Epoch 5416, Loss: 1.9224309921264648, Final Batch Loss: 0.38903990387916565\n",
      "Epoch 5417, Loss: 1.743555724620819, Final Batch Loss: 0.3528621792793274\n",
      "Epoch 5418, Loss: 1.625417023897171, Final Batch Loss: 0.25339117646217346\n",
      "Epoch 5419, Loss: 1.749524712562561, Final Batch Loss: 0.3556431233882904\n",
      "Epoch 5420, Loss: 1.7143989503383636, Final Batch Loss: 0.2900232970714569\n",
      "Epoch 5421, Loss: 1.7425151765346527, Final Batch Loss: 0.4221173822879791\n",
      "Epoch 5422, Loss: 1.646318107843399, Final Batch Loss: 0.3103497326374054\n",
      "Epoch 5423, Loss: 1.7185470461845398, Final Batch Loss: 0.37220191955566406\n",
      "Epoch 5424, Loss: 1.7514200806617737, Final Batch Loss: 0.2812035381793976\n",
      "Epoch 5425, Loss: 1.8519477546215057, Final Batch Loss: 0.3897351622581482\n",
      "Epoch 5426, Loss: 1.7580934464931488, Final Batch Loss: 0.389161080121994\n",
      "Epoch 5427, Loss: 1.714616745710373, Final Batch Loss: 0.31816646456718445\n",
      "Epoch 5428, Loss: 1.654268205165863, Final Batch Loss: 0.346439927816391\n",
      "Epoch 5429, Loss: 1.6552923023700714, Final Batch Loss: 0.36664485931396484\n",
      "Epoch 5430, Loss: 1.7147391140460968, Final Batch Loss: 0.3478690981864929\n",
      "Epoch 5431, Loss: 1.7791471183300018, Final Batch Loss: 0.3375879228115082\n",
      "Epoch 5432, Loss: 1.6047517955303192, Final Batch Loss: 0.36548009514808655\n",
      "Epoch 5433, Loss: 1.720065712928772, Final Batch Loss: 0.3002501130104065\n",
      "Epoch 5434, Loss: 1.8422682881355286, Final Batch Loss: 0.2856012284755707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5435, Loss: 1.7442278265953064, Final Batch Loss: 0.32550111413002014\n",
      "Epoch 5436, Loss: 1.8011272251605988, Final Batch Loss: 0.33041417598724365\n",
      "Epoch 5437, Loss: 1.7128762900829315, Final Batch Loss: 0.4121238589286804\n",
      "Epoch 5438, Loss: 1.7374933958053589, Final Batch Loss: 0.34987932443618774\n",
      "Epoch 5439, Loss: 1.745051383972168, Final Batch Loss: 0.3477953374385834\n",
      "Epoch 5440, Loss: 1.7595529556274414, Final Batch Loss: 0.348239004611969\n",
      "Epoch 5441, Loss: 1.6798515915870667, Final Batch Loss: 0.3845600485801697\n",
      "Epoch 5442, Loss: 1.8657768070697784, Final Batch Loss: 0.3977428078651428\n",
      "Epoch 5443, Loss: 1.7252642512321472, Final Batch Loss: 0.3501388132572174\n",
      "Epoch 5444, Loss: 1.7240051329135895, Final Batch Loss: 0.40525245666503906\n",
      "Epoch 5445, Loss: 1.7501437067985535, Final Batch Loss: 0.2895960509777069\n",
      "Epoch 5446, Loss: 1.697776734828949, Final Batch Loss: 0.34483814239501953\n",
      "Epoch 5447, Loss: 1.6336083710193634, Final Batch Loss: 0.295001745223999\n",
      "Epoch 5448, Loss: 1.6448372602462769, Final Batch Loss: 0.29978862404823303\n",
      "Epoch 5449, Loss: 1.6408544778823853, Final Batch Loss: 0.35554882884025574\n",
      "Epoch 5450, Loss: 1.7987644374370575, Final Batch Loss: 0.36651352047920227\n",
      "Epoch 5451, Loss: 1.898062378168106, Final Batch Loss: 0.3111410439014435\n",
      "Epoch 5452, Loss: 1.5846507847309113, Final Batch Loss: 0.30285778641700745\n",
      "Epoch 5453, Loss: 1.6036696434020996, Final Batch Loss: 0.27401217818260193\n",
      "Epoch 5454, Loss: 1.757385641336441, Final Batch Loss: 0.3328399062156677\n",
      "Epoch 5455, Loss: 1.7812362611293793, Final Batch Loss: 0.29829463362693787\n",
      "Epoch 5456, Loss: 1.6857932806015015, Final Batch Loss: 0.33931440114974976\n",
      "Epoch 5457, Loss: 1.6692114174365997, Final Batch Loss: 0.4082722067832947\n",
      "Epoch 5458, Loss: 1.633186399936676, Final Batch Loss: 0.4345656931400299\n",
      "Epoch 5459, Loss: 1.8053301572799683, Final Batch Loss: 0.310690701007843\n",
      "Epoch 5460, Loss: 1.602007806301117, Final Batch Loss: 0.281129390001297\n",
      "Epoch 5461, Loss: 1.659220665693283, Final Batch Loss: 0.40012747049331665\n",
      "Epoch 5462, Loss: 1.6006509959697723, Final Batch Loss: 0.29689785838127136\n",
      "Epoch 5463, Loss: 1.6529321074485779, Final Batch Loss: 0.36873918771743774\n",
      "Epoch 5464, Loss: 1.8044308125972748, Final Batch Loss: 0.4001116156578064\n",
      "Epoch 5465, Loss: 1.673487812280655, Final Batch Loss: 0.3458070755004883\n",
      "Epoch 5466, Loss: 1.759870320558548, Final Batch Loss: 0.389604389667511\n",
      "Epoch 5467, Loss: 1.6843445003032684, Final Batch Loss: 0.31009283661842346\n",
      "Epoch 5468, Loss: 1.6608413457870483, Final Batch Loss: 0.29645517468452454\n",
      "Epoch 5469, Loss: 1.800339549779892, Final Batch Loss: 0.4852655231952667\n",
      "Epoch 5470, Loss: 1.9003638327121735, Final Batch Loss: 0.3592948019504547\n",
      "Epoch 5471, Loss: 1.7203867733478546, Final Batch Loss: 0.3913135826587677\n",
      "Epoch 5472, Loss: 1.7455324828624725, Final Batch Loss: 0.50617915391922\n",
      "Epoch 5473, Loss: 1.7753625810146332, Final Batch Loss: 0.4325113296508789\n",
      "Epoch 5474, Loss: 1.7733465433120728, Final Batch Loss: 0.3851049542427063\n",
      "Epoch 5475, Loss: 1.638440579175949, Final Batch Loss: 0.33581018447875977\n",
      "Epoch 5476, Loss: 1.8193159699440002, Final Batch Loss: 0.3921160399913788\n",
      "Epoch 5477, Loss: 1.8142724633216858, Final Batch Loss: 0.3015865683555603\n",
      "Epoch 5478, Loss: 1.6718197762966156, Final Batch Loss: 0.3673064708709717\n",
      "Epoch 5479, Loss: 1.7247960269451141, Final Batch Loss: 0.3114303648471832\n",
      "Epoch 5480, Loss: 1.7452692687511444, Final Batch Loss: 0.3558329939842224\n",
      "Epoch 5481, Loss: 1.7945061028003693, Final Batch Loss: 0.48086100816726685\n",
      "Epoch 5482, Loss: 1.7199362814426422, Final Batch Loss: 0.39129161834716797\n",
      "Epoch 5483, Loss: 1.7016094624996185, Final Batch Loss: 0.2520972788333893\n",
      "Epoch 5484, Loss: 1.9062162339687347, Final Batch Loss: 0.39956530928611755\n",
      "Epoch 5485, Loss: 1.9028286337852478, Final Batch Loss: 0.42570433020591736\n",
      "Epoch 5486, Loss: 1.6122881174087524, Final Batch Loss: 0.3289361894130707\n",
      "Epoch 5487, Loss: 1.896653026342392, Final Batch Loss: 0.3596281111240387\n",
      "Epoch 5488, Loss: 1.755343645811081, Final Batch Loss: 0.3342077136039734\n",
      "Epoch 5489, Loss: 1.7694456577301025, Final Batch Loss: 0.40548351407051086\n",
      "Epoch 5490, Loss: 1.5575271844863892, Final Batch Loss: 0.2708714008331299\n",
      "Epoch 5491, Loss: 1.9287693202495575, Final Batch Loss: 0.39622795581817627\n",
      "Epoch 5492, Loss: 1.795424371957779, Final Batch Loss: 0.3172093331813812\n",
      "Epoch 5493, Loss: 1.7071428894996643, Final Batch Loss: 0.3688434958457947\n",
      "Epoch 5494, Loss: 1.6817374527454376, Final Batch Loss: 0.3265872001647949\n",
      "Epoch 5495, Loss: 1.6544632017612457, Final Batch Loss: 0.39266249537467957\n",
      "Epoch 5496, Loss: 1.8038789331912994, Final Batch Loss: 0.36284735798835754\n",
      "Epoch 5497, Loss: 1.8465871512889862, Final Batch Loss: 0.41780468821525574\n",
      "Epoch 5498, Loss: 1.701472669839859, Final Batch Loss: 0.28861096501350403\n",
      "Epoch 5499, Loss: 1.7911608815193176, Final Batch Loss: 0.37240225076675415\n",
      "Epoch 5500, Loss: 1.5669914782047272, Final Batch Loss: 0.36521971225738525\n",
      "Epoch 5501, Loss: 1.6530254483222961, Final Batch Loss: 0.3475656509399414\n",
      "Epoch 5502, Loss: 1.8489783704280853, Final Batch Loss: 0.431930810213089\n",
      "Epoch 5503, Loss: 1.7601844370365143, Final Batch Loss: 0.4962327182292938\n",
      "Epoch 5504, Loss: 1.6869303584098816, Final Batch Loss: 0.3957867920398712\n",
      "Epoch 5505, Loss: 1.6458329260349274, Final Batch Loss: 0.30737385153770447\n",
      "Epoch 5506, Loss: 1.7489199042320251, Final Batch Loss: 0.3028741776943207\n",
      "Epoch 5507, Loss: 1.9062566757202148, Final Batch Loss: 0.38698485493659973\n",
      "Epoch 5508, Loss: 1.743269920349121, Final Batch Loss: 0.26408153772354126\n",
      "Epoch 5509, Loss: 1.748329997062683, Final Batch Loss: 0.29114773869514465\n",
      "Epoch 5510, Loss: 1.6425857245922089, Final Batch Loss: 0.3773435652256012\n",
      "Epoch 5511, Loss: 1.7040036618709564, Final Batch Loss: 0.32088133692741394\n",
      "Epoch 5512, Loss: 1.695592999458313, Final Batch Loss: 0.3362213373184204\n",
      "Epoch 5513, Loss: 1.7303558886051178, Final Batch Loss: 0.33562034368515015\n",
      "Epoch 5514, Loss: 1.7872079908847809, Final Batch Loss: 0.3641417920589447\n",
      "Epoch 5515, Loss: 1.5987529158592224, Final Batch Loss: 0.3091845214366913\n",
      "Epoch 5516, Loss: 1.7578361928462982, Final Batch Loss: 0.3397631049156189\n",
      "Epoch 5517, Loss: 1.7205885350704193, Final Batch Loss: 0.30259278416633606\n",
      "Epoch 5518, Loss: 1.6619961559772491, Final Batch Loss: 0.36726120114326477\n",
      "Epoch 5519, Loss: 1.6933374106884003, Final Batch Loss: 0.3245340883731842\n",
      "Epoch 5520, Loss: 1.8447563350200653, Final Batch Loss: 0.29727599024772644\n",
      "Epoch 5521, Loss: 1.571930319070816, Final Batch Loss: 0.2387264370918274\n",
      "Epoch 5522, Loss: 1.7948247492313385, Final Batch Loss: 0.330926775932312\n",
      "Epoch 5523, Loss: 1.7574071288108826, Final Batch Loss: 0.3839501440525055\n",
      "Epoch 5524, Loss: 1.672355830669403, Final Batch Loss: 0.40073373913764954\n",
      "Epoch 5525, Loss: 1.874978244304657, Final Batch Loss: 0.3350829482078552\n",
      "Epoch 5526, Loss: 1.7577280402183533, Final Batch Loss: 0.3032321631908417\n",
      "Epoch 5527, Loss: 1.7041912376880646, Final Batch Loss: 0.30205294489860535\n",
      "Epoch 5528, Loss: 1.5612958073616028, Final Batch Loss: 0.35805511474609375\n",
      "Epoch 5529, Loss: 1.7010647356510162, Final Batch Loss: 0.3225192129611969\n",
      "Epoch 5530, Loss: 1.582055777311325, Final Batch Loss: 0.3004799783229828\n",
      "Epoch 5531, Loss: 1.79951411485672, Final Batch Loss: 0.308295339345932\n",
      "Epoch 5532, Loss: 1.7146729528903961, Final Batch Loss: 0.3730979263782501\n",
      "Epoch 5533, Loss: 1.6608394086360931, Final Batch Loss: 0.33661261200904846\n",
      "Epoch 5534, Loss: 1.8830216526985168, Final Batch Loss: 0.4411464035511017\n",
      "Epoch 5535, Loss: 1.619918331503868, Final Batch Loss: 0.3450447916984558\n",
      "Epoch 5536, Loss: 1.6686093509197235, Final Batch Loss: 0.31498393416404724\n",
      "Epoch 5537, Loss: 1.7171707451343536, Final Batch Loss: 0.27436283230781555\n",
      "Epoch 5538, Loss: 1.633716493844986, Final Batch Loss: 0.3315730392932892\n",
      "Epoch 5539, Loss: 1.9268152117729187, Final Batch Loss: 0.2764120399951935\n",
      "Epoch 5540, Loss: 1.6821452230215073, Final Batch Loss: 0.21170981228351593\n",
      "Epoch 5541, Loss: 1.8484555780887604, Final Batch Loss: 0.35734885931015015\n",
      "Epoch 5542, Loss: 1.6895960867404938, Final Batch Loss: 0.2807743549346924\n",
      "Epoch 5543, Loss: 1.94414022564888, Final Batch Loss: 0.4288437068462372\n",
      "Epoch 5544, Loss: 1.822231948375702, Final Batch Loss: 0.3767322599887848\n",
      "Epoch 5545, Loss: 1.7724154889583588, Final Batch Loss: 0.3947362005710602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5546, Loss: 1.8178727328777313, Final Batch Loss: 0.36579957604408264\n",
      "Epoch 5547, Loss: 1.6690975427627563, Final Batch Loss: 0.28457894921302795\n",
      "Epoch 5548, Loss: 1.7677526772022247, Final Batch Loss: 0.36386504769325256\n",
      "Epoch 5549, Loss: 1.8504577279090881, Final Batch Loss: 0.21358150243759155\n",
      "Epoch 5550, Loss: 1.6691414713859558, Final Batch Loss: 0.31194987893104553\n",
      "Epoch 5551, Loss: 1.6407935321331024, Final Batch Loss: 0.46126681566238403\n",
      "Epoch 5552, Loss: 1.4519627094268799, Final Batch Loss: 0.27998560667037964\n",
      "Epoch 5553, Loss: 1.5827019810676575, Final Batch Loss: 0.37441906332969666\n",
      "Epoch 5554, Loss: 1.7555932700634003, Final Batch Loss: 0.37211179733276367\n",
      "Epoch 5555, Loss: 1.6582546383142471, Final Batch Loss: 0.23550857603549957\n",
      "Epoch 5556, Loss: 1.726057380437851, Final Batch Loss: 0.3869253098964691\n",
      "Epoch 5557, Loss: 1.6125816702842712, Final Batch Loss: 0.3535672426223755\n",
      "Epoch 5558, Loss: 1.6172335743904114, Final Batch Loss: 0.3028179109096527\n",
      "Epoch 5559, Loss: 1.6915058493614197, Final Batch Loss: 0.3670671880245209\n",
      "Epoch 5560, Loss: 1.8038538992404938, Final Batch Loss: 0.35515493154525757\n",
      "Epoch 5561, Loss: 1.6392188668251038, Final Batch Loss: 0.31475281715393066\n",
      "Epoch 5562, Loss: 1.7732775211334229, Final Batch Loss: 0.2856834828853607\n",
      "Epoch 5563, Loss: 1.6632258594036102, Final Batch Loss: 0.34792304039001465\n",
      "Epoch 5564, Loss: 1.7406655550003052, Final Batch Loss: 0.30236557126045227\n",
      "Epoch 5565, Loss: 1.7195380926132202, Final Batch Loss: 0.364191472530365\n",
      "Epoch 5566, Loss: 1.832368165254593, Final Batch Loss: 0.408825159072876\n",
      "Epoch 5567, Loss: 1.7172009646892548, Final Batch Loss: 0.3171652853488922\n",
      "Epoch 5568, Loss: 1.753007560968399, Final Batch Loss: 0.3476482927799225\n",
      "Epoch 5569, Loss: 1.6281492114067078, Final Batch Loss: 0.27493855357170105\n",
      "Epoch 5570, Loss: 1.6737891733646393, Final Batch Loss: 0.334014892578125\n",
      "Epoch 5571, Loss: 1.833525836467743, Final Batch Loss: 0.28345876932144165\n",
      "Epoch 5572, Loss: 1.7574829459190369, Final Batch Loss: 0.40501850843429565\n",
      "Epoch 5573, Loss: 1.6519772708415985, Final Batch Loss: 0.28350430727005005\n",
      "Epoch 5574, Loss: 1.810403287410736, Final Batch Loss: 0.2962535321712494\n",
      "Epoch 5575, Loss: 1.649595469236374, Final Batch Loss: 0.29934924840927124\n",
      "Epoch 5576, Loss: 1.6389462649822235, Final Batch Loss: 0.3680245876312256\n",
      "Epoch 5577, Loss: 1.7427778244018555, Final Batch Loss: 0.3781169056892395\n",
      "Epoch 5578, Loss: 1.7284324765205383, Final Batch Loss: 0.3492138385772705\n",
      "Epoch 5579, Loss: 1.8994490802288055, Final Batch Loss: 0.4069676697254181\n",
      "Epoch 5580, Loss: 1.834126889705658, Final Batch Loss: 0.3590949773788452\n",
      "Epoch 5581, Loss: 1.6414415836334229, Final Batch Loss: 0.3379734456539154\n",
      "Epoch 5582, Loss: 1.616282343864441, Final Batch Loss: 0.2877004146575928\n",
      "Epoch 5583, Loss: 1.5918155014514923, Final Batch Loss: 0.3100104331970215\n",
      "Epoch 5584, Loss: 1.6748845726251602, Final Batch Loss: 0.20665977895259857\n",
      "Epoch 5585, Loss: 1.7099711000919342, Final Batch Loss: 0.3867765963077545\n",
      "Epoch 5586, Loss: 1.6746850609779358, Final Batch Loss: 0.3204103708267212\n",
      "Epoch 5587, Loss: 1.632061392068863, Final Batch Loss: 0.2836109399795532\n",
      "Epoch 5588, Loss: 1.6259092688560486, Final Batch Loss: 0.34237125515937805\n",
      "Epoch 5589, Loss: 1.6539742648601532, Final Batch Loss: 0.3323803246021271\n",
      "Epoch 5590, Loss: 1.6762517392635345, Final Batch Loss: 0.30420389771461487\n",
      "Epoch 5591, Loss: 1.7121004462242126, Final Batch Loss: 0.3429039716720581\n",
      "Epoch 5592, Loss: 1.865110844373703, Final Batch Loss: 0.4091816544532776\n",
      "Epoch 5593, Loss: 1.8881244361400604, Final Batch Loss: 0.4260334372520447\n",
      "Epoch 5594, Loss: 1.6847484707832336, Final Batch Loss: 0.34934526681900024\n",
      "Epoch 5595, Loss: 1.7660429179668427, Final Batch Loss: 0.3503977656364441\n",
      "Epoch 5596, Loss: 1.7894382774829865, Final Batch Loss: 0.3446158468723297\n",
      "Epoch 5597, Loss: 1.765455424785614, Final Batch Loss: 0.4430457353591919\n",
      "Epoch 5598, Loss: 1.7251158356666565, Final Batch Loss: 0.31320443749427795\n",
      "Epoch 5599, Loss: 1.8517114520072937, Final Batch Loss: 0.45945653319358826\n",
      "Epoch 5600, Loss: 1.8352031707763672, Final Batch Loss: 0.333506315946579\n",
      "Epoch 5601, Loss: 1.695519119501114, Final Batch Loss: 0.3631190359592438\n",
      "Epoch 5602, Loss: 1.757480412721634, Final Batch Loss: 0.29919886589050293\n",
      "Epoch 5603, Loss: 1.6528836786746979, Final Batch Loss: 0.29569289088249207\n",
      "Epoch 5604, Loss: 1.6324157118797302, Final Batch Loss: 0.3191894590854645\n",
      "Epoch 5605, Loss: 1.7291979491710663, Final Batch Loss: 0.4153149127960205\n",
      "Epoch 5606, Loss: 1.6809372901916504, Final Batch Loss: 0.33417564630508423\n",
      "Epoch 5607, Loss: 1.843737006187439, Final Batch Loss: 0.3885710537433624\n",
      "Epoch 5608, Loss: 1.8857714533805847, Final Batch Loss: 0.34955617785453796\n",
      "Epoch 5609, Loss: 1.8043353259563446, Final Batch Loss: 0.381630003452301\n",
      "Epoch 5610, Loss: 1.6860105097293854, Final Batch Loss: 0.3204523026943207\n",
      "Epoch 5611, Loss: 1.7761326134204865, Final Batch Loss: 0.30189090967178345\n",
      "Epoch 5612, Loss: 1.9133240580558777, Final Batch Loss: 0.3505835235118866\n",
      "Epoch 5613, Loss: 1.735162377357483, Final Batch Loss: 0.40491366386413574\n",
      "Epoch 5614, Loss: 1.869728147983551, Final Batch Loss: 0.30977579951286316\n",
      "Epoch 5615, Loss: 1.7123452425003052, Final Batch Loss: 0.3491913378238678\n",
      "Epoch 5616, Loss: 1.7440348863601685, Final Batch Loss: 0.3948032259941101\n",
      "Epoch 5617, Loss: 1.6832121014595032, Final Batch Loss: 0.31314316391944885\n",
      "Epoch 5618, Loss: 1.7729967832565308, Final Batch Loss: 0.3095890283584595\n",
      "Epoch 5619, Loss: 1.7000442147254944, Final Batch Loss: 0.35061055421829224\n",
      "Epoch 5620, Loss: 1.6926634311676025, Final Batch Loss: 0.305447518825531\n",
      "Epoch 5621, Loss: 1.5997759401798248, Final Batch Loss: 0.271994948387146\n",
      "Epoch 5622, Loss: 1.8633286356925964, Final Batch Loss: 0.33114710450172424\n",
      "Epoch 5623, Loss: 1.697473406791687, Final Batch Loss: 0.29038068652153015\n",
      "Epoch 5624, Loss: 1.746526300907135, Final Batch Loss: 0.3531686067581177\n",
      "Epoch 5625, Loss: 1.7168289422988892, Final Batch Loss: 0.37491434812545776\n",
      "Epoch 5626, Loss: 1.747081220149994, Final Batch Loss: 0.2712264060974121\n",
      "Epoch 5627, Loss: 1.70938840508461, Final Batch Loss: 0.31918564438819885\n",
      "Epoch 5628, Loss: 1.6206844747066498, Final Batch Loss: 0.31251347064971924\n",
      "Epoch 5629, Loss: 2.020750105381012, Final Batch Loss: 0.5046342015266418\n",
      "Epoch 5630, Loss: 1.6523489654064178, Final Batch Loss: 0.3073992133140564\n",
      "Epoch 5631, Loss: 1.8281067907810211, Final Batch Loss: 0.3964519202709198\n",
      "Epoch 5632, Loss: 1.6644509136676788, Final Batch Loss: 0.3063768446445465\n",
      "Epoch 5633, Loss: 1.7927336692810059, Final Batch Loss: 0.3535807132720947\n",
      "Epoch 5634, Loss: 1.6462880969047546, Final Batch Loss: 0.32120010256767273\n",
      "Epoch 5635, Loss: 1.7710513472557068, Final Batch Loss: 0.39241477847099304\n",
      "Epoch 5636, Loss: 1.5670612752437592, Final Batch Loss: 0.3236166536808014\n",
      "Epoch 5637, Loss: 1.6491484344005585, Final Batch Loss: 0.43607422709465027\n",
      "Epoch 5638, Loss: 1.7767042219638824, Final Batch Loss: 0.2791772782802582\n",
      "Epoch 5639, Loss: 1.8495041728019714, Final Batch Loss: 0.36954209208488464\n",
      "Epoch 5640, Loss: 1.6265396773815155, Final Batch Loss: 0.2923329770565033\n",
      "Epoch 5641, Loss: 1.7451062500476837, Final Batch Loss: 0.3083556294441223\n",
      "Epoch 5642, Loss: 1.62788325548172, Final Batch Loss: 0.2657791078090668\n",
      "Epoch 5643, Loss: 1.7557100355625153, Final Batch Loss: 0.30537402629852295\n",
      "Epoch 5644, Loss: 1.8855880498886108, Final Batch Loss: 0.3291183412075043\n",
      "Epoch 5645, Loss: 1.6504158675670624, Final Batch Loss: 0.32664191722869873\n",
      "Epoch 5646, Loss: 1.5561563372612, Final Batch Loss: 0.23233044147491455\n",
      "Epoch 5647, Loss: 1.681188941001892, Final Batch Loss: 0.3531588613986969\n",
      "Epoch 5648, Loss: 1.8627394139766693, Final Batch Loss: 0.39328986406326294\n",
      "Epoch 5649, Loss: 1.5946243405342102, Final Batch Loss: 0.29115596413612366\n",
      "Epoch 5650, Loss: 1.64444300532341, Final Batch Loss: 0.33531421422958374\n",
      "Epoch 5651, Loss: 1.770084410905838, Final Batch Loss: 0.3737066388130188\n",
      "Epoch 5652, Loss: 1.6157596111297607, Final Batch Loss: 0.32780396938323975\n",
      "Epoch 5653, Loss: 1.730959713459015, Final Batch Loss: 0.34482768177986145\n",
      "Epoch 5654, Loss: 1.6214919090270996, Final Batch Loss: 0.2690519690513611\n",
      "Epoch 5655, Loss: 1.660640224814415, Final Batch Loss: 0.38875612616539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5656, Loss: 1.6684090197086334, Final Batch Loss: 0.33701691031455994\n",
      "Epoch 5657, Loss: 1.8353118896484375, Final Batch Loss: 0.3816123306751251\n",
      "Epoch 5658, Loss: 1.8753696382045746, Final Batch Loss: 0.47340860962867737\n",
      "Epoch 5659, Loss: 1.702241450548172, Final Batch Loss: 0.41744938492774963\n",
      "Epoch 5660, Loss: 1.8376976549625397, Final Batch Loss: 0.3714620769023895\n",
      "Epoch 5661, Loss: 1.6672268211841583, Final Batch Loss: 0.28622591495513916\n",
      "Epoch 5662, Loss: 1.8752962052822113, Final Batch Loss: 0.3913017511367798\n",
      "Epoch 5663, Loss: 1.6549579501152039, Final Batch Loss: 0.33388465642929077\n",
      "Epoch 5664, Loss: 1.9556203484535217, Final Batch Loss: 0.3968929946422577\n",
      "Epoch 5665, Loss: 1.8400627672672272, Final Batch Loss: 0.34708261489868164\n",
      "Epoch 5666, Loss: 1.717564433813095, Final Batch Loss: 0.33945947885513306\n",
      "Epoch 5667, Loss: 1.721925526857376, Final Batch Loss: 0.37509429454803467\n",
      "Epoch 5668, Loss: 1.7018116116523743, Final Batch Loss: 0.2562553286552429\n",
      "Epoch 5669, Loss: 1.779543161392212, Final Batch Loss: 0.27477607131004333\n",
      "Epoch 5670, Loss: 1.7100817561149597, Final Batch Loss: 0.3463362157344818\n",
      "Epoch 5671, Loss: 1.5754081010818481, Final Batch Loss: 0.2559731602668762\n",
      "Epoch 5672, Loss: 1.909917414188385, Final Batch Loss: 0.3760245144367218\n",
      "Epoch 5673, Loss: 1.6697972118854523, Final Batch Loss: 0.33541494607925415\n",
      "Epoch 5674, Loss: 1.926755428314209, Final Batch Loss: 0.26489606499671936\n",
      "Epoch 5675, Loss: 1.6200938522815704, Final Batch Loss: 0.3547561466693878\n",
      "Epoch 5676, Loss: 1.8670434951782227, Final Batch Loss: 0.33936434984207153\n",
      "Epoch 5677, Loss: 1.6401161849498749, Final Batch Loss: 0.2780548632144928\n",
      "Epoch 5678, Loss: 1.742426872253418, Final Batch Loss: 0.38886725902557373\n",
      "Epoch 5679, Loss: 1.7610123753547668, Final Batch Loss: 0.3152567446231842\n",
      "Epoch 5680, Loss: 1.6647224128246307, Final Batch Loss: 0.27504032850265503\n",
      "Epoch 5681, Loss: 1.7825505435466766, Final Batch Loss: 0.35162153840065\n",
      "Epoch 5682, Loss: 1.6456905007362366, Final Batch Loss: 0.29989194869995117\n",
      "Epoch 5683, Loss: 1.67089381814003, Final Batch Loss: 0.3269512355327606\n",
      "Epoch 5684, Loss: 1.7680645436048508, Final Batch Loss: 0.36916816234588623\n",
      "Epoch 5685, Loss: 1.7353246212005615, Final Batch Loss: 0.3386344611644745\n",
      "Epoch 5686, Loss: 1.6532179713249207, Final Batch Loss: 0.2665581703186035\n",
      "Epoch 5687, Loss: 1.719672530889511, Final Batch Loss: 0.3490343987941742\n",
      "Epoch 5688, Loss: 1.7912319004535675, Final Batch Loss: 0.4256274402141571\n",
      "Epoch 5689, Loss: 1.6563989222049713, Final Batch Loss: 0.3017081916332245\n",
      "Epoch 5690, Loss: 1.6005921363830566, Final Batch Loss: 0.2311801016330719\n",
      "Epoch 5691, Loss: 1.540824145078659, Final Batch Loss: 0.28911709785461426\n",
      "Epoch 5692, Loss: 1.5592329502105713, Final Batch Loss: 0.2706626057624817\n",
      "Epoch 5693, Loss: 1.756643682718277, Final Batch Loss: 0.33366748690605164\n",
      "Epoch 5694, Loss: 1.5456772148609161, Final Batch Loss: 0.3287602663040161\n",
      "Epoch 5695, Loss: 1.644260436296463, Final Batch Loss: 0.3247230052947998\n",
      "Epoch 5696, Loss: 1.6115189492702484, Final Batch Loss: 0.3338787853717804\n",
      "Epoch 5697, Loss: 1.6472454369068146, Final Batch Loss: 0.38103875517845154\n",
      "Epoch 5698, Loss: 1.7670040428638458, Final Batch Loss: 0.33609142899513245\n",
      "Epoch 5699, Loss: 1.760223001241684, Final Batch Loss: 0.2926487326622009\n",
      "Epoch 5700, Loss: 1.7000486850738525, Final Batch Loss: 0.4119478166103363\n",
      "Epoch 5701, Loss: 1.7910420894622803, Final Batch Loss: 0.3121756315231323\n",
      "Epoch 5702, Loss: 1.766891211271286, Final Batch Loss: 0.41265711188316345\n",
      "Epoch 5703, Loss: 1.635146051645279, Final Batch Loss: 0.30547475814819336\n",
      "Epoch 5704, Loss: 1.705857738852501, Final Batch Loss: 0.2447662204504013\n",
      "Epoch 5705, Loss: 1.7833810150623322, Final Batch Loss: 0.3641921579837799\n",
      "Epoch 5706, Loss: 1.555738165974617, Final Batch Loss: 0.23400811851024628\n",
      "Epoch 5707, Loss: 1.6847133040428162, Final Batch Loss: 0.3620961904525757\n",
      "Epoch 5708, Loss: 1.8258771300315857, Final Batch Loss: 0.4373563826084137\n",
      "Epoch 5709, Loss: 1.7885113656520844, Final Batch Loss: 0.4053075313568115\n",
      "Epoch 5710, Loss: 1.6649322807788849, Final Batch Loss: 0.36232495307922363\n",
      "Epoch 5711, Loss: 1.5940503478050232, Final Batch Loss: 0.3107890784740448\n",
      "Epoch 5712, Loss: 1.4166987538337708, Final Batch Loss: 0.23299869894981384\n",
      "Epoch 5713, Loss: 1.678913414478302, Final Batch Loss: 0.3468877077102661\n",
      "Epoch 5714, Loss: 1.676156759262085, Final Batch Loss: 0.3344232141971588\n",
      "Epoch 5715, Loss: 1.7052195966243744, Final Batch Loss: 0.4415750205516815\n",
      "Epoch 5716, Loss: 1.5490984618663788, Final Batch Loss: 0.3132384717464447\n",
      "Epoch 5717, Loss: 1.6541086435317993, Final Batch Loss: 0.33049190044403076\n",
      "Epoch 5718, Loss: 1.688784807920456, Final Batch Loss: 0.38031381368637085\n",
      "Epoch 5719, Loss: 1.546621710062027, Final Batch Loss: 0.2397593855857849\n",
      "Epoch 5720, Loss: 1.6118460595607758, Final Batch Loss: 0.34041309356689453\n",
      "Epoch 5721, Loss: 1.605266511440277, Final Batch Loss: 0.3210245668888092\n",
      "Epoch 5722, Loss: 1.6198933720588684, Final Batch Loss: 0.30290085077285767\n",
      "Epoch 5723, Loss: 1.6955845654010773, Final Batch Loss: 0.3552676737308502\n",
      "Epoch 5724, Loss: 1.776822417974472, Final Batch Loss: 0.4173627197742462\n",
      "Epoch 5725, Loss: 1.754243552684784, Final Batch Loss: 0.3801058530807495\n",
      "Epoch 5726, Loss: 1.7106314301490784, Final Batch Loss: 0.39924153685569763\n",
      "Epoch 5727, Loss: 1.9655585289001465, Final Batch Loss: 0.42613011598587036\n",
      "Epoch 5728, Loss: 1.7793666422367096, Final Batch Loss: 0.3346201777458191\n",
      "Epoch 5729, Loss: 1.5949708223342896, Final Batch Loss: 0.2950790226459503\n",
      "Epoch 5730, Loss: 1.6057033240795135, Final Batch Loss: 0.29334381222724915\n",
      "Epoch 5731, Loss: 1.6646156311035156, Final Batch Loss: 0.3883882164955139\n",
      "Epoch 5732, Loss: 1.7197484076023102, Final Batch Loss: 0.33560672402381897\n",
      "Epoch 5733, Loss: 1.7137166857719421, Final Batch Loss: 0.3068859875202179\n",
      "Epoch 5734, Loss: 1.7138527482748032, Final Batch Loss: 0.4622728228569031\n",
      "Epoch 5735, Loss: 1.9248372614383698, Final Batch Loss: 0.5026442408561707\n",
      "Epoch 5736, Loss: 1.642650157213211, Final Batch Loss: 0.2626495063304901\n",
      "Epoch 5737, Loss: 1.6846021711826324, Final Batch Loss: 0.30407050251960754\n",
      "Epoch 5738, Loss: 1.724578320980072, Final Batch Loss: 0.3986378014087677\n",
      "Epoch 5739, Loss: 1.77138552069664, Final Batch Loss: 0.42975914478302\n",
      "Epoch 5740, Loss: 1.5978594422340393, Final Batch Loss: 0.3259529173374176\n",
      "Epoch 5741, Loss: 1.8683192729949951, Final Batch Loss: 0.299886018037796\n",
      "Epoch 5742, Loss: 1.7421307861804962, Final Batch Loss: 0.2627604603767395\n",
      "Epoch 5743, Loss: 1.655688539147377, Final Batch Loss: 0.3893982172012329\n",
      "Epoch 5744, Loss: 1.7324614822864532, Final Batch Loss: 0.3133784234523773\n",
      "Epoch 5745, Loss: 1.7106278538703918, Final Batch Loss: 0.3178480565547943\n",
      "Epoch 5746, Loss: 1.6709350943565369, Final Batch Loss: 0.34015128016471863\n",
      "Epoch 5747, Loss: 1.730495125055313, Final Batch Loss: 0.40127184987068176\n",
      "Epoch 5748, Loss: 1.8519886434078217, Final Batch Loss: 0.36632785201072693\n",
      "Epoch 5749, Loss: 1.725067675113678, Final Batch Loss: 0.30927062034606934\n",
      "Epoch 5750, Loss: 1.7040055096149445, Final Batch Loss: 0.32025304436683655\n",
      "Epoch 5751, Loss: 1.7615248262882233, Final Batch Loss: 0.3535417914390564\n",
      "Epoch 5752, Loss: 1.7246537804603577, Final Batch Loss: 0.28120434284210205\n",
      "Epoch 5753, Loss: 1.6959397494792938, Final Batch Loss: 0.3000330328941345\n",
      "Epoch 5754, Loss: 1.757966309785843, Final Batch Loss: 0.3083425462245941\n",
      "Epoch 5755, Loss: 1.7101848423480988, Final Batch Loss: 0.32248562574386597\n",
      "Epoch 5756, Loss: 1.600250855088234, Final Batch Loss: 0.3308357298374176\n",
      "Epoch 5757, Loss: 1.7888369858264923, Final Batch Loss: 0.28652843832969666\n",
      "Epoch 5758, Loss: 1.583055168390274, Final Batch Loss: 0.3434189558029175\n",
      "Epoch 5759, Loss: 1.644145131111145, Final Batch Loss: 0.39761513471603394\n",
      "Epoch 5760, Loss: 1.6130933463573456, Final Batch Loss: 0.2778441309928894\n",
      "Epoch 5761, Loss: 1.6741065382957458, Final Batch Loss: 0.30208688974380493\n",
      "Epoch 5762, Loss: 1.6661639213562012, Final Batch Loss: 0.3668515682220459\n",
      "Epoch 5763, Loss: 1.7188423573970795, Final Batch Loss: 0.3805774748325348\n",
      "Epoch 5764, Loss: 1.6221319139003754, Final Batch Loss: 0.29361453652381897\n",
      "Epoch 5765, Loss: 1.9253886342048645, Final Batch Loss: 0.2987173795700073\n",
      "Epoch 5766, Loss: 1.7566606104373932, Final Batch Loss: 0.28557971119880676\n",
      "Epoch 5767, Loss: 1.9021704196929932, Final Batch Loss: 0.35846731066703796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5768, Loss: 1.843513697385788, Final Batch Loss: 0.27937784790992737\n",
      "Epoch 5769, Loss: 1.8107043206691742, Final Batch Loss: 0.4073442816734314\n",
      "Epoch 5770, Loss: 1.535967767238617, Final Batch Loss: 0.26907747983932495\n",
      "Epoch 5771, Loss: 1.8719036877155304, Final Batch Loss: 0.33625689148902893\n",
      "Epoch 5772, Loss: 1.6650799810886383, Final Batch Loss: 0.35997432470321655\n",
      "Epoch 5773, Loss: 1.4779843091964722, Final Batch Loss: 0.29404833912849426\n",
      "Epoch 5774, Loss: 1.7870952486991882, Final Batch Loss: 0.3000607192516327\n",
      "Epoch 5775, Loss: 1.8382620215415955, Final Batch Loss: 0.3911725580692291\n",
      "Epoch 5776, Loss: 1.5895735621452332, Final Batch Loss: 0.3371581435203552\n",
      "Epoch 5777, Loss: 1.7973085641860962, Final Batch Loss: 0.4400474429130554\n",
      "Epoch 5778, Loss: 1.7035042941570282, Final Batch Loss: 0.2974918484687805\n",
      "Epoch 5779, Loss: 1.6280537247657776, Final Batch Loss: 0.28402259945869446\n",
      "Epoch 5780, Loss: 1.7827848494052887, Final Batch Loss: 0.3210301697254181\n",
      "Epoch 5781, Loss: 1.7468125224113464, Final Batch Loss: 0.28908005356788635\n",
      "Epoch 5782, Loss: 1.6536864936351776, Final Batch Loss: 0.380018413066864\n",
      "Epoch 5783, Loss: 1.6919247508049011, Final Batch Loss: 0.3204690217971802\n",
      "Epoch 5784, Loss: 1.6958584487438202, Final Batch Loss: 0.25742489099502563\n",
      "Epoch 5785, Loss: 1.6938625276088715, Final Batch Loss: 0.34604838490486145\n",
      "Epoch 5786, Loss: 1.8810059130191803, Final Batch Loss: 0.4015025198459625\n",
      "Epoch 5787, Loss: 1.7969259023666382, Final Batch Loss: 0.40662580728530884\n",
      "Epoch 5788, Loss: 1.6877979934215546, Final Batch Loss: 0.30257222056388855\n",
      "Epoch 5789, Loss: 1.7019266486167908, Final Batch Loss: 0.40540385246276855\n",
      "Epoch 5790, Loss: 1.6683324873447418, Final Batch Loss: 0.31586360931396484\n",
      "Epoch 5791, Loss: 1.7249502837657928, Final Batch Loss: 0.36986681818962097\n",
      "Epoch 5792, Loss: 1.8090031445026398, Final Batch Loss: 0.3497030735015869\n",
      "Epoch 5793, Loss: 1.7826587557792664, Final Batch Loss: 0.3281005620956421\n",
      "Epoch 5794, Loss: 1.5901513993740082, Final Batch Loss: 0.3933422863483429\n",
      "Epoch 5795, Loss: 1.86180979013443, Final Batch Loss: 0.3827957510948181\n",
      "Epoch 5796, Loss: 1.9681549966335297, Final Batch Loss: 0.3733200430870056\n",
      "Epoch 5797, Loss: 1.913626104593277, Final Batch Loss: 0.31344661116600037\n",
      "Epoch 5798, Loss: 1.720722496509552, Final Batch Loss: 0.35127636790275574\n",
      "Epoch 5799, Loss: 1.650423377752304, Final Batch Loss: 0.3178480863571167\n",
      "Epoch 5800, Loss: 1.8634316325187683, Final Batch Loss: 0.3891049325466156\n",
      "Epoch 5801, Loss: 1.701976627111435, Final Batch Loss: 0.27135130763053894\n",
      "Epoch 5802, Loss: 1.760439783334732, Final Batch Loss: 0.33424365520477295\n",
      "Epoch 5803, Loss: 1.558664470911026, Final Batch Loss: 0.3277139961719513\n",
      "Epoch 5804, Loss: 1.6791351437568665, Final Batch Loss: 0.2986805737018585\n",
      "Epoch 5805, Loss: 1.801015943288803, Final Batch Loss: 0.40669646859169006\n",
      "Epoch 5806, Loss: 1.618265151977539, Final Batch Loss: 0.36418190598487854\n",
      "Epoch 5807, Loss: 1.6046923696994781, Final Batch Loss: 0.31789276003837585\n",
      "Epoch 5808, Loss: 1.7068576514720917, Final Batch Loss: 0.3113456070423126\n",
      "Epoch 5809, Loss: 1.5908394753932953, Final Batch Loss: 0.33805832266807556\n",
      "Epoch 5810, Loss: 1.6765028238296509, Final Batch Loss: 0.4136817753314972\n",
      "Epoch 5811, Loss: 1.5544824600219727, Final Batch Loss: 0.3189558684825897\n",
      "Epoch 5812, Loss: 1.7905317842960358, Final Batch Loss: 0.3639979064464569\n",
      "Epoch 5813, Loss: 1.6331662833690643, Final Batch Loss: 0.3548260033130646\n",
      "Epoch 5814, Loss: 1.631663292646408, Final Batch Loss: 0.2771892547607422\n",
      "Epoch 5815, Loss: 1.735644370317459, Final Batch Loss: 0.34230276942253113\n",
      "Epoch 5816, Loss: 1.5947255492210388, Final Batch Loss: 0.3228680193424225\n",
      "Epoch 5817, Loss: 1.7093665897846222, Final Batch Loss: 0.3374103009700775\n",
      "Epoch 5818, Loss: 1.7272681295871735, Final Batch Loss: 0.36533644795417786\n",
      "Epoch 5819, Loss: 1.5863526165485382, Final Batch Loss: 0.2808937430381775\n",
      "Epoch 5820, Loss: 1.7214011251926422, Final Batch Loss: 0.39046263694763184\n",
      "Epoch 5821, Loss: 1.7731895744800568, Final Batch Loss: 0.2941506803035736\n",
      "Epoch 5822, Loss: 1.772439181804657, Final Batch Loss: 0.313886433839798\n",
      "Epoch 5823, Loss: 1.680722177028656, Final Batch Loss: 0.3821379542350769\n",
      "Epoch 5824, Loss: 1.694709688425064, Final Batch Loss: 0.29612255096435547\n",
      "Epoch 5825, Loss: 1.6083033382892609, Final Batch Loss: 0.28958675265312195\n",
      "Epoch 5826, Loss: 1.760259747505188, Final Batch Loss: 0.5198100805282593\n",
      "Epoch 5827, Loss: 1.6566464304924011, Final Batch Loss: 0.33394673466682434\n",
      "Epoch 5828, Loss: 1.5938239097595215, Final Batch Loss: 0.3614429235458374\n",
      "Epoch 5829, Loss: 1.7848243415355682, Final Batch Loss: 0.35296279191970825\n",
      "Epoch 5830, Loss: 1.786942958831787, Final Batch Loss: 0.35624459385871887\n",
      "Epoch 5831, Loss: 1.741922914981842, Final Batch Loss: 0.345049113035202\n",
      "Epoch 5832, Loss: 1.709263175725937, Final Batch Loss: 0.3921630084514618\n",
      "Epoch 5833, Loss: 1.5928411483764648, Final Batch Loss: 0.3042098581790924\n",
      "Epoch 5834, Loss: 1.5731710493564606, Final Batch Loss: 0.2497575581073761\n",
      "Epoch 5835, Loss: 1.7048254311084747, Final Batch Loss: 0.3280545473098755\n",
      "Epoch 5836, Loss: 1.9242431819438934, Final Batch Loss: 0.3584213852882385\n",
      "Epoch 5837, Loss: 1.8313158750534058, Final Batch Loss: 0.3928403854370117\n",
      "Epoch 5838, Loss: 1.8352010548114777, Final Batch Loss: 0.4559312164783478\n",
      "Epoch 5839, Loss: 1.5695437788963318, Final Batch Loss: 0.2949969470500946\n",
      "Epoch 5840, Loss: 1.667550653219223, Final Batch Loss: 0.3176201581954956\n",
      "Epoch 5841, Loss: 1.7280352413654327, Final Batch Loss: 0.3158457279205322\n",
      "Epoch 5842, Loss: 1.8056683838367462, Final Batch Loss: 0.3497932255268097\n",
      "Epoch 5843, Loss: 1.5858908742666245, Final Batch Loss: 0.3553849458694458\n",
      "Epoch 5844, Loss: 1.7364304959774017, Final Batch Loss: 0.32305124402046204\n",
      "Epoch 5845, Loss: 1.81569042801857, Final Batch Loss: 0.4665735363960266\n",
      "Epoch 5846, Loss: 1.6513898968696594, Final Batch Loss: 0.279572069644928\n",
      "Epoch 5847, Loss: 1.9351773262023926, Final Batch Loss: 0.4192641079425812\n",
      "Epoch 5848, Loss: 1.6911533176898956, Final Batch Loss: 0.3507453501224518\n",
      "Epoch 5849, Loss: 1.78496253490448, Final Batch Loss: 0.3416222333908081\n",
      "Epoch 5850, Loss: 1.7032466232776642, Final Batch Loss: 0.303888201713562\n",
      "Epoch 5851, Loss: 1.7868106365203857, Final Batch Loss: 0.32046839594841003\n",
      "Epoch 5852, Loss: 1.6637308895587921, Final Batch Loss: 0.2788691520690918\n",
      "Epoch 5853, Loss: 1.6973092555999756, Final Batch Loss: 0.2735952138900757\n",
      "Epoch 5854, Loss: 1.7420240342617035, Final Batch Loss: 0.3610441982746124\n",
      "Epoch 5855, Loss: 1.7017415463924408, Final Batch Loss: 0.3778335750102997\n",
      "Epoch 5856, Loss: 1.8095073103904724, Final Batch Loss: 0.2834683358669281\n",
      "Epoch 5857, Loss: 1.6858291625976562, Final Batch Loss: 0.34779390692710876\n",
      "Epoch 5858, Loss: 1.7067262828350067, Final Batch Loss: 0.327405720949173\n",
      "Epoch 5859, Loss: 1.7532666325569153, Final Batch Loss: 0.3653026223182678\n",
      "Epoch 5860, Loss: 1.570329487323761, Final Batch Loss: 0.33206668496131897\n",
      "Epoch 5861, Loss: 1.7902744710445404, Final Batch Loss: 0.4457589387893677\n",
      "Epoch 5862, Loss: 1.7377145886421204, Final Batch Loss: 0.2818034589290619\n",
      "Epoch 5863, Loss: 1.7708929777145386, Final Batch Loss: 0.37711742520332336\n",
      "Epoch 5864, Loss: 1.6949105858802795, Final Batch Loss: 0.3299671709537506\n",
      "Epoch 5865, Loss: 1.6424858570098877, Final Batch Loss: 0.3708786964416504\n",
      "Epoch 5866, Loss: 1.8087853789329529, Final Batch Loss: 0.3300088942050934\n",
      "Epoch 5867, Loss: 1.6668742001056671, Final Batch Loss: 0.3260055482387543\n",
      "Epoch 5868, Loss: 1.7945201098918915, Final Batch Loss: 0.3581850230693817\n",
      "Epoch 5869, Loss: 1.768504112958908, Final Batch Loss: 0.3674265146255493\n",
      "Epoch 5870, Loss: 1.7474755197763443, Final Batch Loss: 0.47527945041656494\n",
      "Epoch 5871, Loss: 1.6545296907424927, Final Batch Loss: 0.24895831942558289\n",
      "Epoch 5872, Loss: 1.720881074666977, Final Batch Loss: 0.4669596552848816\n",
      "Epoch 5873, Loss: 1.6188709437847137, Final Batch Loss: 0.3148016035556793\n",
      "Epoch 5874, Loss: 1.7594992816448212, Final Batch Loss: 0.33649173378944397\n",
      "Epoch 5875, Loss: 1.7596527636051178, Final Batch Loss: 0.302459716796875\n",
      "Epoch 5876, Loss: 1.6499549746513367, Final Batch Loss: 0.3186381757259369\n",
      "Epoch 5877, Loss: 1.6057285368442535, Final Batch Loss: 0.2990097999572754\n",
      "Epoch 5878, Loss: 1.6330115795135498, Final Batch Loss: 0.2975630760192871\n",
      "Epoch 5879, Loss: 1.8236091136932373, Final Batch Loss: 0.3304250240325928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5880, Loss: 1.637336790561676, Final Batch Loss: 0.26729702949523926\n",
      "Epoch 5881, Loss: 1.64080148935318, Final Batch Loss: 0.3122638761997223\n",
      "Epoch 5882, Loss: 1.8482433259487152, Final Batch Loss: 0.3755663335323334\n",
      "Epoch 5883, Loss: 1.7309970557689667, Final Batch Loss: 0.35150620341300964\n",
      "Epoch 5884, Loss: 1.726600021123886, Final Batch Loss: 0.2969318628311157\n",
      "Epoch 5885, Loss: 1.827606201171875, Final Batch Loss: 0.3975796103477478\n",
      "Epoch 5886, Loss: 1.6215865314006805, Final Batch Loss: 0.3476964235305786\n",
      "Epoch 5887, Loss: 1.706614226102829, Final Batch Loss: 0.3355073928833008\n",
      "Epoch 5888, Loss: 1.598413199186325, Final Batch Loss: 0.33748555183410645\n",
      "Epoch 5889, Loss: 1.8979820609092712, Final Batch Loss: 0.3263055086135864\n",
      "Epoch 5890, Loss: 1.6943632364273071, Final Batch Loss: 0.30222296714782715\n",
      "Epoch 5891, Loss: 1.6208178400993347, Final Batch Loss: 0.3921996057033539\n",
      "Epoch 5892, Loss: 1.6820642054080963, Final Batch Loss: 0.4243529736995697\n",
      "Epoch 5893, Loss: 1.7033116817474365, Final Batch Loss: 0.27194440364837646\n",
      "Epoch 5894, Loss: 1.9542894065380096, Final Batch Loss: 0.34285861253738403\n",
      "Epoch 5895, Loss: 1.6376014649868011, Final Batch Loss: 0.35922539234161377\n",
      "Epoch 5896, Loss: 1.6607182919979095, Final Batch Loss: 0.32239946722984314\n",
      "Epoch 5897, Loss: 1.9536294639110565, Final Batch Loss: 0.394238144159317\n",
      "Epoch 5898, Loss: 1.948441982269287, Final Batch Loss: 0.5163248181343079\n",
      "Epoch 5899, Loss: 1.5785740613937378, Final Batch Loss: 0.32590383291244507\n",
      "Epoch 5900, Loss: 1.596070796251297, Final Batch Loss: 0.2581155002117157\n",
      "Epoch 5901, Loss: 1.588311642408371, Final Batch Loss: 0.28719374537467957\n",
      "Epoch 5902, Loss: 1.5119064450263977, Final Batch Loss: 0.3063223958015442\n",
      "Epoch 5903, Loss: 1.7570825219154358, Final Batch Loss: 0.4107963442802429\n",
      "Epoch 5904, Loss: 1.961926132440567, Final Batch Loss: 0.4087180495262146\n",
      "Epoch 5905, Loss: 1.7555945217609406, Final Batch Loss: 0.3183085322380066\n",
      "Epoch 5906, Loss: 1.7935380935668945, Final Batch Loss: 0.35399511456489563\n",
      "Epoch 5907, Loss: 1.9651359915733337, Final Batch Loss: 0.33709052205085754\n",
      "Epoch 5908, Loss: 1.7932530343532562, Final Batch Loss: 0.301288366317749\n",
      "Epoch 5909, Loss: 1.791410744190216, Final Batch Loss: 0.34695494174957275\n",
      "Epoch 5910, Loss: 1.7551792562007904, Final Batch Loss: 0.3461812436580658\n",
      "Epoch 5911, Loss: 1.8360383212566376, Final Batch Loss: 0.38940897583961487\n",
      "Epoch 5912, Loss: 2.0218940377235413, Final Batch Loss: 0.36933475732803345\n",
      "Epoch 5913, Loss: 1.6881857812404633, Final Batch Loss: 0.3428029716014862\n",
      "Epoch 5914, Loss: 1.6771813035011292, Final Batch Loss: 0.2939148247241974\n",
      "Epoch 5915, Loss: 1.6851511299610138, Final Batch Loss: 0.40301722288131714\n",
      "Epoch 5916, Loss: 1.6963977813720703, Final Batch Loss: 0.2837282419204712\n",
      "Epoch 5917, Loss: 1.846842110157013, Final Batch Loss: 0.35744574666023254\n",
      "Epoch 5918, Loss: 1.68411223590374, Final Batch Loss: 0.23944257199764252\n",
      "Epoch 5919, Loss: 1.5721417516469955, Final Batch Loss: 0.28053906559944153\n",
      "Epoch 5920, Loss: 1.6469405889511108, Final Batch Loss: 0.31893447041511536\n",
      "Epoch 5921, Loss: 1.7228720784187317, Final Batch Loss: 0.3536324203014374\n",
      "Epoch 5922, Loss: 1.804912030696869, Final Batch Loss: 0.34587255120277405\n",
      "Epoch 5923, Loss: 1.6460480988025665, Final Batch Loss: 0.29057788848876953\n",
      "Epoch 5924, Loss: 1.5856561362743378, Final Batch Loss: 0.28902676701545715\n",
      "Epoch 5925, Loss: 1.667328417301178, Final Batch Loss: 0.28661033511161804\n",
      "Epoch 5926, Loss: 1.8902616202831268, Final Batch Loss: 0.3714049458503723\n",
      "Epoch 5927, Loss: 1.7710115909576416, Final Batch Loss: 0.35127827525138855\n",
      "Epoch 5928, Loss: 1.6392091512680054, Final Batch Loss: 0.25466883182525635\n",
      "Epoch 5929, Loss: 1.7221752405166626, Final Batch Loss: 0.3623863160610199\n",
      "Epoch 5930, Loss: 1.6985461711883545, Final Batch Loss: 0.43217530846595764\n",
      "Epoch 5931, Loss: 1.5775699019432068, Final Batch Loss: 0.33168599009513855\n",
      "Epoch 5932, Loss: 1.690464049577713, Final Batch Loss: 0.37723344564437866\n",
      "Epoch 5933, Loss: 1.7539022862911224, Final Batch Loss: 0.36749741435050964\n",
      "Epoch 5934, Loss: 1.7155580818653107, Final Batch Loss: 0.2999763786792755\n",
      "Epoch 5935, Loss: 1.7751328945159912, Final Batch Loss: 0.33121249079704285\n",
      "Epoch 5936, Loss: 1.8050091564655304, Final Batch Loss: 0.3075944185256958\n",
      "Epoch 5937, Loss: 1.6662451326847076, Final Batch Loss: 0.30738499760627747\n",
      "Epoch 5938, Loss: 1.6475008726119995, Final Batch Loss: 0.27837079763412476\n",
      "Epoch 5939, Loss: 1.6879624128341675, Final Batch Loss: 0.37428903579711914\n",
      "Epoch 5940, Loss: 1.547353520989418, Final Batch Loss: 0.34414732456207275\n",
      "Epoch 5941, Loss: 1.6563328206539154, Final Batch Loss: 0.2884100675582886\n",
      "Epoch 5942, Loss: 1.8258390724658966, Final Batch Loss: 0.34347617626190186\n",
      "Epoch 5943, Loss: 1.5888394713401794, Final Batch Loss: 0.27079570293426514\n",
      "Epoch 5944, Loss: 1.6889557242393494, Final Batch Loss: 0.3406373858451843\n",
      "Epoch 5945, Loss: 1.6135678589344025, Final Batch Loss: 0.3725586235523224\n",
      "Epoch 5946, Loss: 1.6444520950317383, Final Batch Loss: 0.2573245167732239\n",
      "Epoch 5947, Loss: 1.7971788048744202, Final Batch Loss: 0.4423038363456726\n",
      "Epoch 5948, Loss: 1.7726128697395325, Final Batch Loss: 0.26389068365097046\n",
      "Epoch 5949, Loss: 1.6556068658828735, Final Batch Loss: 0.2712576985359192\n",
      "Epoch 5950, Loss: 1.7700036466121674, Final Batch Loss: 0.4086889922618866\n",
      "Epoch 5951, Loss: 1.7147423923015594, Final Batch Loss: 0.41724124550819397\n",
      "Epoch 5952, Loss: 1.8078793287277222, Final Batch Loss: 0.36307114362716675\n",
      "Epoch 5953, Loss: 1.6206761002540588, Final Batch Loss: 0.3101784586906433\n",
      "Epoch 5954, Loss: 1.5550232529640198, Final Batch Loss: 0.31528812646865845\n",
      "Epoch 5955, Loss: 1.722785860300064, Final Batch Loss: 0.3560449481010437\n",
      "Epoch 5956, Loss: 1.7346709668636322, Final Batch Loss: 0.38973602652549744\n",
      "Epoch 5957, Loss: 1.666569709777832, Final Batch Loss: 0.3868725299835205\n",
      "Epoch 5958, Loss: 1.5911599397659302, Final Batch Loss: 0.29530707001686096\n",
      "Epoch 5959, Loss: 1.7114846110343933, Final Batch Loss: 0.2987438142299652\n",
      "Epoch 5960, Loss: 1.6483007371425629, Final Batch Loss: 0.376091331243515\n",
      "Epoch 5961, Loss: 1.4322356134653091, Final Batch Loss: 0.32227659225463867\n",
      "Epoch 5962, Loss: 1.71303129196167, Final Batch Loss: 0.3929508626461029\n",
      "Epoch 5963, Loss: 1.6941344439983368, Final Batch Loss: 0.4157956838607788\n",
      "Epoch 5964, Loss: 1.848244845867157, Final Batch Loss: 0.27938854694366455\n",
      "Epoch 5965, Loss: 1.7599256932735443, Final Batch Loss: 0.271951287984848\n",
      "Epoch 5966, Loss: 1.7986371517181396, Final Batch Loss: 0.3524150252342224\n",
      "Epoch 5967, Loss: 1.6791615784168243, Final Batch Loss: 0.3098653554916382\n",
      "Epoch 5968, Loss: 1.703474447131157, Final Batch Loss: 0.3452243506908417\n",
      "Epoch 5969, Loss: 1.579607903957367, Final Batch Loss: 0.29801803827285767\n",
      "Epoch 5970, Loss: 1.9483350813388824, Final Batch Loss: 0.40104812383651733\n",
      "Epoch 5971, Loss: 1.6249229609966278, Final Batch Loss: 0.3531069755554199\n",
      "Epoch 5972, Loss: 1.604386955499649, Final Batch Loss: 0.33191442489624023\n",
      "Epoch 5973, Loss: 1.5939634442329407, Final Batch Loss: 0.36797192692756653\n",
      "Epoch 5974, Loss: 1.6672479510307312, Final Batch Loss: 0.31859731674194336\n",
      "Epoch 5975, Loss: 1.5267120003700256, Final Batch Loss: 0.2854347825050354\n",
      "Epoch 5976, Loss: 1.8402535021305084, Final Batch Loss: 0.2664613723754883\n",
      "Epoch 5977, Loss: 1.5859355181455612, Final Batch Loss: 0.24261267483234406\n",
      "Epoch 5978, Loss: 1.6081119179725647, Final Batch Loss: 0.28295210003852844\n",
      "Epoch 5979, Loss: 1.769952654838562, Final Batch Loss: 0.40912359952926636\n",
      "Epoch 5980, Loss: 1.6319408118724823, Final Batch Loss: 0.3001220226287842\n",
      "Epoch 5981, Loss: 1.4477567821741104, Final Batch Loss: 0.22603315114974976\n",
      "Epoch 5982, Loss: 1.6907640993595123, Final Batch Loss: 0.2976934313774109\n",
      "Epoch 5983, Loss: 1.6961900889873505, Final Batch Loss: 0.3580455780029297\n",
      "Epoch 5984, Loss: 1.5433555841445923, Final Batch Loss: 0.2809736430644989\n",
      "Epoch 5985, Loss: 1.593024104833603, Final Batch Loss: 0.3411353826522827\n",
      "Epoch 5986, Loss: 1.7291612923145294, Final Batch Loss: 0.393498033285141\n",
      "Epoch 5987, Loss: 1.8684096336364746, Final Batch Loss: 0.34645354747772217\n",
      "Epoch 5988, Loss: 1.7751616537570953, Final Batch Loss: 0.45411232113838196\n",
      "Epoch 5989, Loss: 1.6893312633037567, Final Batch Loss: 0.3321707248687744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5990, Loss: 1.6642938256263733, Final Batch Loss: 0.29853054881095886\n",
      "Epoch 5991, Loss: 1.5346499532461166, Final Batch Loss: 0.2817775309085846\n",
      "Epoch 5992, Loss: 1.7043578028678894, Final Batch Loss: 0.31745344400405884\n",
      "Epoch 5993, Loss: 1.8177677392959595, Final Batch Loss: 0.4187825918197632\n",
      "Epoch 5994, Loss: 1.5963741540908813, Final Batch Loss: 0.24509841203689575\n",
      "Epoch 5995, Loss: 1.6107746362686157, Final Batch Loss: 0.27886146306991577\n",
      "Epoch 5996, Loss: 1.6082367449998856, Final Batch Loss: 0.22638852894306183\n",
      "Epoch 5997, Loss: 1.8367835581302643, Final Batch Loss: 0.37145429849624634\n",
      "Epoch 5998, Loss: 1.619421660900116, Final Batch Loss: 0.21362873911857605\n",
      "Epoch 5999, Loss: 1.6558335423469543, Final Batch Loss: 0.44232264161109924\n",
      "Epoch 6000, Loss: 1.6639221608638763, Final Batch Loss: 0.3521897494792938\n",
      "Epoch 6001, Loss: 1.6992084980010986, Final Batch Loss: 0.25331440567970276\n",
      "Epoch 6002, Loss: 1.8595978915691376, Final Batch Loss: 0.42703181505203247\n",
      "Epoch 6003, Loss: 1.8117005825042725, Final Batch Loss: 0.4591374695301056\n",
      "Epoch 6004, Loss: 1.8075573444366455, Final Batch Loss: 0.36784517765045166\n",
      "Epoch 6005, Loss: 1.6947977542877197, Final Batch Loss: 0.37163618206977844\n",
      "Epoch 6006, Loss: 1.6621569097042084, Final Batch Loss: 0.2621372640132904\n",
      "Epoch 6007, Loss: 1.8039759993553162, Final Batch Loss: 0.41191548109054565\n",
      "Epoch 6008, Loss: 1.7464617192745209, Final Batch Loss: 0.3030014634132385\n",
      "Epoch 6009, Loss: 1.792020708322525, Final Batch Loss: 0.35242342948913574\n",
      "Epoch 6010, Loss: 1.7237483859062195, Final Batch Loss: 0.30868369340896606\n",
      "Epoch 6011, Loss: 1.798353523015976, Final Batch Loss: 0.4140464961528778\n",
      "Epoch 6012, Loss: 1.6777067482471466, Final Batch Loss: 0.3646426796913147\n",
      "Epoch 6013, Loss: 1.8240419626235962, Final Batch Loss: 0.42560678720474243\n",
      "Epoch 6014, Loss: 1.626081794500351, Final Batch Loss: 0.2954511046409607\n",
      "Epoch 6015, Loss: 1.7435668110847473, Final Batch Loss: 0.35431215167045593\n",
      "Epoch 6016, Loss: 1.6513015925884247, Final Batch Loss: 0.2701397240161896\n",
      "Epoch 6017, Loss: 1.7757472097873688, Final Batch Loss: 0.3865167498588562\n",
      "Epoch 6018, Loss: 1.5409943759441376, Final Batch Loss: 0.34219998121261597\n",
      "Epoch 6019, Loss: 1.7582536041736603, Final Batch Loss: 0.3572690784931183\n",
      "Epoch 6020, Loss: 1.6359564661979675, Final Batch Loss: 0.3257104158401489\n",
      "Epoch 6021, Loss: 1.7824526131153107, Final Batch Loss: 0.3910975158214569\n",
      "Epoch 6022, Loss: 1.7118037641048431, Final Batch Loss: 0.3656524121761322\n",
      "Epoch 6023, Loss: 1.777660310268402, Final Batch Loss: 0.3807937800884247\n",
      "Epoch 6024, Loss: 1.6165245175361633, Final Batch Loss: 0.29610955715179443\n",
      "Epoch 6025, Loss: 1.6035293638706207, Final Batch Loss: 0.30294811725616455\n",
      "Epoch 6026, Loss: 1.682700514793396, Final Batch Loss: 0.32862094044685364\n",
      "Epoch 6027, Loss: 1.8669548034667969, Final Batch Loss: 0.3603729009628296\n",
      "Epoch 6028, Loss: 1.6180709898471832, Final Batch Loss: 0.2991895079612732\n",
      "Epoch 6029, Loss: 1.7296975553035736, Final Batch Loss: 0.3365617096424103\n",
      "Epoch 6030, Loss: 1.7332829535007477, Final Batch Loss: 0.3696633577346802\n",
      "Epoch 6031, Loss: 1.654759556055069, Final Batch Loss: 0.2867317795753479\n",
      "Epoch 6032, Loss: 1.793381541967392, Final Batch Loss: 0.3934471607208252\n",
      "Epoch 6033, Loss: 1.7897215783596039, Final Batch Loss: 0.43484607338905334\n",
      "Epoch 6034, Loss: 1.7991612255573273, Final Batch Loss: 0.36280032992362976\n",
      "Epoch 6035, Loss: 1.6345725357532501, Final Batch Loss: 0.31878677010536194\n",
      "Epoch 6036, Loss: 1.6331087350845337, Final Batch Loss: 0.29982659220695496\n",
      "Epoch 6037, Loss: 1.712385207414627, Final Batch Loss: 0.4020489752292633\n",
      "Epoch 6038, Loss: 1.7610391080379486, Final Batch Loss: 0.331863671541214\n",
      "Epoch 6039, Loss: 1.6842577457427979, Final Batch Loss: 0.2635400593280792\n",
      "Epoch 6040, Loss: 1.6188892126083374, Final Batch Loss: 0.34104159474372864\n",
      "Epoch 6041, Loss: 1.5886575281620026, Final Batch Loss: 0.46226173639297485\n",
      "Epoch 6042, Loss: 1.70064315199852, Final Batch Loss: 0.36725345253944397\n",
      "Epoch 6043, Loss: 1.8065767288208008, Final Batch Loss: 0.29565057158470154\n",
      "Epoch 6044, Loss: 1.5371865630149841, Final Batch Loss: 0.26403605937957764\n",
      "Epoch 6045, Loss: 1.6750271916389465, Final Batch Loss: 0.4000478684902191\n",
      "Epoch 6046, Loss: 1.570317029953003, Final Batch Loss: 0.3218892812728882\n",
      "Epoch 6047, Loss: 1.8455684185028076, Final Batch Loss: 0.44906696677207947\n",
      "Epoch 6048, Loss: 1.6623183190822601, Final Batch Loss: 0.415396511554718\n",
      "Epoch 6049, Loss: 1.715678632259369, Final Batch Loss: 0.3866077661514282\n",
      "Epoch 6050, Loss: 1.806205153465271, Final Batch Loss: 0.4148504436016083\n",
      "Epoch 6051, Loss: 1.600301742553711, Final Batch Loss: 0.27001166343688965\n",
      "Epoch 6052, Loss: 1.8718882203102112, Final Batch Loss: 0.36473825573921204\n",
      "Epoch 6053, Loss: 1.7404040694236755, Final Batch Loss: 0.3848879635334015\n",
      "Epoch 6054, Loss: 1.8213799595832825, Final Batch Loss: 0.3310180604457855\n",
      "Epoch 6055, Loss: 1.7343426048755646, Final Batch Loss: 0.394626647233963\n",
      "Epoch 6056, Loss: 1.8060247898101807, Final Batch Loss: 0.38281264901161194\n",
      "Epoch 6057, Loss: 1.6971925497055054, Final Batch Loss: 0.4325169026851654\n",
      "Epoch 6058, Loss: 1.9399402141571045, Final Batch Loss: 0.38932502269744873\n",
      "Epoch 6059, Loss: 1.9178131520748138, Final Batch Loss: 0.2968408465385437\n",
      "Epoch 6060, Loss: 1.8667896091938019, Final Batch Loss: 0.41368624567985535\n",
      "Epoch 6061, Loss: 1.6871954202651978, Final Batch Loss: 0.2844182848930359\n",
      "Epoch 6062, Loss: 1.6793256551027298, Final Batch Loss: 0.246160626411438\n",
      "Epoch 6063, Loss: 1.6575884222984314, Final Batch Loss: 0.35237252712249756\n",
      "Epoch 6064, Loss: 1.723181128501892, Final Batch Loss: 0.2942988872528076\n",
      "Epoch 6065, Loss: 1.65543532371521, Final Batch Loss: 0.33817464113235474\n",
      "Epoch 6066, Loss: 1.7927127182483673, Final Batch Loss: 0.3761858642101288\n",
      "Epoch 6067, Loss: 1.6314472258090973, Final Batch Loss: 0.36200767755508423\n",
      "Epoch 6068, Loss: 1.6481384932994843, Final Batch Loss: 0.27133092284202576\n",
      "Epoch 6069, Loss: 1.7068740129470825, Final Batch Loss: 0.24537500739097595\n",
      "Epoch 6070, Loss: 1.8326910138130188, Final Batch Loss: 0.34517860412597656\n",
      "Epoch 6071, Loss: 1.6798604428768158, Final Batch Loss: 0.3516834080219269\n",
      "Epoch 6072, Loss: 1.6409651041030884, Final Batch Loss: 0.33528199791908264\n",
      "Epoch 6073, Loss: 1.715419739484787, Final Batch Loss: 0.3887484073638916\n",
      "Epoch 6074, Loss: 1.6647896468639374, Final Batch Loss: 0.33166664838790894\n",
      "Epoch 6075, Loss: 1.8101907074451447, Final Batch Loss: 0.3738287687301636\n",
      "Epoch 6076, Loss: 1.659008949995041, Final Batch Loss: 0.32475656270980835\n",
      "Epoch 6077, Loss: 1.6318484246730804, Final Batch Loss: 0.3408935070037842\n",
      "Epoch 6078, Loss: 1.6407833099365234, Final Batch Loss: 0.341886043548584\n",
      "Epoch 6079, Loss: 2.0542837381362915, Final Batch Loss: 0.40975069999694824\n",
      "Epoch 6080, Loss: 1.7108827531337738, Final Batch Loss: 0.37065577507019043\n",
      "Epoch 6081, Loss: 1.5610376596450806, Final Batch Loss: 0.2951909303665161\n",
      "Epoch 6082, Loss: 1.8447179198265076, Final Batch Loss: 0.3631666898727417\n",
      "Epoch 6083, Loss: 1.6985849738121033, Final Batch Loss: 0.3335643708705902\n",
      "Epoch 6084, Loss: 1.6842947900295258, Final Batch Loss: 0.282225638628006\n",
      "Epoch 6085, Loss: 1.788868248462677, Final Batch Loss: 0.37529799342155457\n",
      "Epoch 6086, Loss: 1.7388238608837128, Final Batch Loss: 0.2928261160850525\n",
      "Epoch 6087, Loss: 1.773825317621231, Final Batch Loss: 0.3602091073989868\n",
      "Epoch 6088, Loss: 1.7310844361782074, Final Batch Loss: 0.43173205852508545\n",
      "Epoch 6089, Loss: 1.735503613948822, Final Batch Loss: 0.28382793068885803\n",
      "Epoch 6090, Loss: 1.8235139548778534, Final Batch Loss: 0.34404629468917847\n",
      "Epoch 6091, Loss: 1.6571818888187408, Final Batch Loss: 0.32123979926109314\n",
      "Epoch 6092, Loss: 1.5462415963411331, Final Batch Loss: 0.36300525069236755\n",
      "Epoch 6093, Loss: 1.6449192464351654, Final Batch Loss: 0.32632899284362793\n",
      "Epoch 6094, Loss: 1.7380741238594055, Final Batch Loss: 0.27568453550338745\n",
      "Epoch 6095, Loss: 1.5535928905010223, Final Batch Loss: 0.32025033235549927\n",
      "Epoch 6096, Loss: 1.762962281703949, Final Batch Loss: 0.42485836148262024\n",
      "Epoch 6097, Loss: 1.7304432392120361, Final Batch Loss: 0.3151323199272156\n",
      "Epoch 6098, Loss: 1.8164790570735931, Final Batch Loss: 0.37957045435905457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6099, Loss: 1.899007111787796, Final Batch Loss: 0.3163377344608307\n",
      "Epoch 6100, Loss: 1.5175617039203644, Final Batch Loss: 0.3150736391544342\n",
      "Epoch 6101, Loss: 1.6547950208187103, Final Batch Loss: 0.3675385117530823\n",
      "Epoch 6102, Loss: 1.7413941323757172, Final Batch Loss: 0.2844155430793762\n",
      "Epoch 6103, Loss: 1.684901475906372, Final Batch Loss: 0.3419787585735321\n",
      "Epoch 6104, Loss: 1.787539780139923, Final Batch Loss: 0.3798407316207886\n",
      "Epoch 6105, Loss: 1.8184969127178192, Final Batch Loss: 0.3120484948158264\n",
      "Epoch 6106, Loss: 1.6551831364631653, Final Batch Loss: 0.29471972584724426\n",
      "Epoch 6107, Loss: 1.5083812028169632, Final Batch Loss: 0.31886133551597595\n",
      "Epoch 6108, Loss: 1.583071231842041, Final Batch Loss: 0.3225177228450775\n",
      "Epoch 6109, Loss: 1.8815893828868866, Final Batch Loss: 0.43780893087387085\n",
      "Epoch 6110, Loss: 1.789880484342575, Final Batch Loss: 0.3359493613243103\n",
      "Epoch 6111, Loss: 1.75435471534729, Final Batch Loss: 0.36804232001304626\n",
      "Epoch 6112, Loss: 1.7320216000080109, Final Batch Loss: 0.38749250769615173\n",
      "Epoch 6113, Loss: 1.7587368786334991, Final Batch Loss: 0.3347388505935669\n",
      "Epoch 6114, Loss: 1.7872933447360992, Final Batch Loss: 0.2892821431159973\n",
      "Epoch 6115, Loss: 1.4879147112369537, Final Batch Loss: 0.2931390702724457\n",
      "Epoch 6116, Loss: 1.6384685039520264, Final Batch Loss: 0.31931769847869873\n",
      "Epoch 6117, Loss: 1.7477384805679321, Final Batch Loss: 0.4282274842262268\n",
      "Epoch 6118, Loss: 1.9188252836465836, Final Batch Loss: 0.429418683052063\n",
      "Epoch 6119, Loss: 1.6932395100593567, Final Batch Loss: 0.39175495505332947\n",
      "Epoch 6120, Loss: 1.6676250100135803, Final Batch Loss: 0.3680484890937805\n",
      "Epoch 6121, Loss: 1.6637565195560455, Final Batch Loss: 0.2847544550895691\n",
      "Epoch 6122, Loss: 1.6536893844604492, Final Batch Loss: 0.29007843136787415\n",
      "Epoch 6123, Loss: 1.6630898714065552, Final Batch Loss: 0.372835248708725\n",
      "Epoch 6124, Loss: 1.8156123161315918, Final Batch Loss: 0.3134080171585083\n",
      "Epoch 6125, Loss: 1.8030617535114288, Final Batch Loss: 0.3554994761943817\n",
      "Epoch 6126, Loss: 1.6505037099123, Final Batch Loss: 0.340086966753006\n",
      "Epoch 6127, Loss: 1.7505503296852112, Final Batch Loss: 0.3581078052520752\n",
      "Epoch 6128, Loss: 1.8133273720741272, Final Batch Loss: 0.44031283259391785\n",
      "Epoch 6129, Loss: 1.5650026500225067, Final Batch Loss: 0.35682398080825806\n",
      "Epoch 6130, Loss: 1.5677609145641327, Final Batch Loss: 0.3262731432914734\n",
      "Epoch 6131, Loss: 1.7864387333393097, Final Batch Loss: 0.3234822154045105\n",
      "Epoch 6132, Loss: 1.5270681977272034, Final Batch Loss: 0.30123406648635864\n",
      "Epoch 6133, Loss: 1.9098331034183502, Final Batch Loss: 0.4708191454410553\n",
      "Epoch 6134, Loss: 1.6996060609817505, Final Batch Loss: 0.3244094252586365\n",
      "Epoch 6135, Loss: 1.710389792919159, Final Batch Loss: 0.41702231764793396\n",
      "Epoch 6136, Loss: 1.8633898198604584, Final Batch Loss: 0.4010380208492279\n",
      "Epoch 6137, Loss: 1.945060819387436, Final Batch Loss: 0.36891376972198486\n",
      "Epoch 6138, Loss: 1.7475025951862335, Final Batch Loss: 0.380409836769104\n",
      "Epoch 6139, Loss: 1.7499677240848541, Final Batch Loss: 0.3428983986377716\n",
      "Epoch 6140, Loss: 1.623613029718399, Final Batch Loss: 0.395699143409729\n",
      "Epoch 6141, Loss: 1.7978141009807587, Final Batch Loss: 0.2835167348384857\n",
      "Epoch 6142, Loss: 1.7104305624961853, Final Batch Loss: 0.3720693588256836\n",
      "Epoch 6143, Loss: 1.4983812868595123, Final Batch Loss: 0.3479812443256378\n",
      "Epoch 6144, Loss: 1.6167765259742737, Final Batch Loss: 0.2897244095802307\n",
      "Epoch 6145, Loss: 1.90760537981987, Final Batch Loss: 0.3680875599384308\n",
      "Epoch 6146, Loss: 1.6412042081356049, Final Batch Loss: 0.3462732136249542\n",
      "Epoch 6147, Loss: 1.7086706757545471, Final Batch Loss: 0.3479195535182953\n",
      "Epoch 6148, Loss: 1.6538620293140411, Final Batch Loss: 0.31313398480415344\n",
      "Epoch 6149, Loss: 1.722573697566986, Final Batch Loss: 0.49223241209983826\n",
      "Epoch 6150, Loss: 1.8351229131221771, Final Batch Loss: 0.33221516013145447\n",
      "Epoch 6151, Loss: 1.6473291218280792, Final Batch Loss: 0.34661802649497986\n",
      "Epoch 6152, Loss: 1.6323774755001068, Final Batch Loss: 0.3164075016975403\n",
      "Epoch 6153, Loss: 1.8534507751464844, Final Batch Loss: 0.30948787927627563\n",
      "Epoch 6154, Loss: 1.8012334406375885, Final Batch Loss: 0.44344526529312134\n",
      "Epoch 6155, Loss: 1.649461269378662, Final Batch Loss: 0.30422714352607727\n",
      "Epoch 6156, Loss: 1.6834303140640259, Final Batch Loss: 0.2995903193950653\n",
      "Epoch 6157, Loss: 1.691341608762741, Final Batch Loss: 0.4036027491092682\n",
      "Epoch 6158, Loss: 1.8077379763126373, Final Batch Loss: 0.4480717182159424\n",
      "Epoch 6159, Loss: 1.8389387428760529, Final Batch Loss: 0.316849023103714\n",
      "Epoch 6160, Loss: 1.7114374041557312, Final Batch Loss: 0.28351518511772156\n",
      "Epoch 6161, Loss: 1.6957637965679169, Final Batch Loss: 0.40048277378082275\n",
      "Epoch 6162, Loss: 1.7348139882087708, Final Batch Loss: 0.39788374304771423\n",
      "Epoch 6163, Loss: 1.8368487060070038, Final Batch Loss: 0.46151262521743774\n",
      "Epoch 6164, Loss: 1.4745734333992004, Final Batch Loss: 0.18631665408611298\n",
      "Epoch 6165, Loss: 1.7233371138572693, Final Batch Loss: 0.3571041524410248\n",
      "Epoch 6166, Loss: 1.5167298316955566, Final Batch Loss: 0.32845523953437805\n",
      "Epoch 6167, Loss: 1.5834429562091827, Final Batch Loss: 0.28873053193092346\n",
      "Epoch 6168, Loss: 1.598703384399414, Final Batch Loss: 0.43038859963417053\n",
      "Epoch 6169, Loss: 1.8734308779239655, Final Batch Loss: 0.3123444616794586\n",
      "Epoch 6170, Loss: 1.7145064175128937, Final Batch Loss: 0.3490341007709503\n",
      "Epoch 6171, Loss: 1.8322588205337524, Final Batch Loss: 0.3822920024394989\n",
      "Epoch 6172, Loss: 1.7609961926937103, Final Batch Loss: 0.3809773623943329\n",
      "Epoch 6173, Loss: 1.815540224313736, Final Batch Loss: 0.30054643750190735\n",
      "Epoch 6174, Loss: 1.5690730959177017, Final Batch Loss: 0.30835965275764465\n",
      "Epoch 6175, Loss: 1.7188043892383575, Final Batch Loss: 0.3123793303966522\n",
      "Epoch 6176, Loss: 1.8763474524021149, Final Batch Loss: 0.4887517988681793\n",
      "Epoch 6177, Loss: 1.7340222001075745, Final Batch Loss: 0.3548728823661804\n",
      "Epoch 6178, Loss: 1.503757268190384, Final Batch Loss: 0.22814121842384338\n",
      "Epoch 6179, Loss: 1.6877382099628448, Final Batch Loss: 0.3424622118473053\n",
      "Epoch 6180, Loss: 1.504542738199234, Final Batch Loss: 0.2570403516292572\n",
      "Epoch 6181, Loss: 1.628875881433487, Final Batch Loss: 0.33022961020469666\n",
      "Epoch 6182, Loss: 1.720407783985138, Final Batch Loss: 0.3710898756980896\n",
      "Epoch 6183, Loss: 1.839522808790207, Final Batch Loss: 0.3945409655570984\n",
      "Epoch 6184, Loss: 1.6974442899227142, Final Batch Loss: 0.37748631834983826\n",
      "Epoch 6185, Loss: 1.6355185806751251, Final Batch Loss: 0.400845468044281\n",
      "Epoch 6186, Loss: 1.7562375664710999, Final Batch Loss: 0.369145005941391\n",
      "Epoch 6187, Loss: 1.826072782278061, Final Batch Loss: 0.30890321731567383\n",
      "Epoch 6188, Loss: 1.7664569914340973, Final Batch Loss: 0.3332590162754059\n",
      "Epoch 6189, Loss: 1.6833903789520264, Final Batch Loss: 0.3472500741481781\n",
      "Epoch 6190, Loss: 1.595525547862053, Final Batch Loss: 0.38644930720329285\n",
      "Epoch 6191, Loss: 2.052241563796997, Final Batch Loss: 0.7169749140739441\n",
      "Epoch 6192, Loss: 1.6710176765918732, Final Batch Loss: 0.2789468467235565\n",
      "Epoch 6193, Loss: 1.8343718349933624, Final Batch Loss: 0.3943091332912445\n",
      "Epoch 6194, Loss: 1.6181134283542633, Final Batch Loss: 0.3633599281311035\n",
      "Epoch 6195, Loss: 1.7593429684638977, Final Batch Loss: 0.2889820337295532\n",
      "Epoch 6196, Loss: 1.7671029567718506, Final Batch Loss: 0.31544429063796997\n",
      "Epoch 6197, Loss: 1.6537059545516968, Final Batch Loss: 0.3543980121612549\n",
      "Epoch 6198, Loss: 1.5964192748069763, Final Batch Loss: 0.2784229516983032\n",
      "Epoch 6199, Loss: 1.7215678691864014, Final Batch Loss: 0.3313644826412201\n",
      "Epoch 6200, Loss: 1.651130199432373, Final Batch Loss: 0.2995366156101227\n",
      "Epoch 6201, Loss: 1.649493157863617, Final Batch Loss: 0.37180638313293457\n",
      "Epoch 6202, Loss: 1.8684885799884796, Final Batch Loss: 0.4803851544857025\n",
      "Epoch 6203, Loss: 1.7731256186962128, Final Batch Loss: 0.37023991346359253\n",
      "Epoch 6204, Loss: 1.5993050336837769, Final Batch Loss: 0.29409754276275635\n",
      "Epoch 6205, Loss: 1.608491063117981, Final Batch Loss: 0.3857066035270691\n",
      "Epoch 6206, Loss: 1.65206977725029, Final Batch Loss: 0.30020782351493835\n",
      "Epoch 6207, Loss: 1.7423450201749802, Final Batch Loss: 0.3534323275089264\n",
      "Epoch 6208, Loss: 1.6810713410377502, Final Batch Loss: 0.38103756308555603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6209, Loss: 1.6948583722114563, Final Batch Loss: 0.33884820342063904\n",
      "Epoch 6210, Loss: 1.7868016362190247, Final Batch Loss: 0.33532190322875977\n",
      "Epoch 6211, Loss: 1.7042684257030487, Final Batch Loss: 0.3367534577846527\n",
      "Epoch 6212, Loss: 1.8425151407718658, Final Batch Loss: 0.36810070276260376\n",
      "Epoch 6213, Loss: 1.767568290233612, Final Batch Loss: 0.47841933369636536\n",
      "Epoch 6214, Loss: 1.748373806476593, Final Batch Loss: 0.2776443660259247\n",
      "Epoch 6215, Loss: 1.6756462156772614, Final Batch Loss: 0.30502673983573914\n",
      "Epoch 6216, Loss: 1.6570071578025818, Final Batch Loss: 0.3452633321285248\n",
      "Epoch 6217, Loss: 1.7917914986610413, Final Batch Loss: 0.31335189938545227\n",
      "Epoch 6218, Loss: 1.6765156090259552, Final Batch Loss: 0.326708048582077\n",
      "Epoch 6219, Loss: 1.6968351006507874, Final Batch Loss: 0.3626008927822113\n",
      "Epoch 6220, Loss: 1.7314175069332123, Final Batch Loss: 0.34073421359062195\n",
      "Epoch 6221, Loss: 1.7917276322841644, Final Batch Loss: 0.39762449264526367\n",
      "Epoch 6222, Loss: 1.7476403713226318, Final Batch Loss: 0.41338011622428894\n",
      "Epoch 6223, Loss: 1.7342568635940552, Final Batch Loss: 0.30050334334373474\n",
      "Epoch 6224, Loss: 1.8447406888008118, Final Batch Loss: 0.35196471214294434\n",
      "Epoch 6225, Loss: 1.705583617091179, Final Batch Loss: 0.23381967842578888\n",
      "Epoch 6226, Loss: 1.7343100607395172, Final Batch Loss: 0.37589162588119507\n",
      "Epoch 6227, Loss: 1.7110463082790375, Final Batch Loss: 0.3288489878177643\n",
      "Epoch 6228, Loss: 1.627938687801361, Final Batch Loss: 0.4079784154891968\n",
      "Epoch 6229, Loss: 1.6685974299907684, Final Batch Loss: 0.32025766372680664\n",
      "Epoch 6230, Loss: 1.6467982232570648, Final Batch Loss: 0.2921575903892517\n",
      "Epoch 6231, Loss: 1.8171616792678833, Final Batch Loss: 0.36862149834632874\n",
      "Epoch 6232, Loss: 1.6874946355819702, Final Batch Loss: 0.35570013523101807\n",
      "Epoch 6233, Loss: 1.7156792283058167, Final Batch Loss: 0.3118205964565277\n",
      "Epoch 6234, Loss: 1.6994349956512451, Final Batch Loss: 0.36514824628829956\n",
      "Epoch 6235, Loss: 1.795053392648697, Final Batch Loss: 0.3542118966579437\n",
      "Epoch 6236, Loss: 1.5986008942127228, Final Batch Loss: 0.2772335112094879\n",
      "Epoch 6237, Loss: 1.6761567294597626, Final Batch Loss: 0.37402597069740295\n",
      "Epoch 6238, Loss: 1.7523654699325562, Final Batch Loss: 0.3170299530029297\n",
      "Epoch 6239, Loss: 1.7008084654808044, Final Batch Loss: 0.2648642361164093\n",
      "Epoch 6240, Loss: 1.6207467913627625, Final Batch Loss: 0.33222073316574097\n",
      "Epoch 6241, Loss: 1.7870192527770996, Final Batch Loss: 0.2860628366470337\n",
      "Epoch 6242, Loss: 1.4830158948898315, Final Batch Loss: 0.2548750638961792\n",
      "Epoch 6243, Loss: 1.7361421585083008, Final Batch Loss: 0.34448638558387756\n",
      "Epoch 6244, Loss: 1.7604412734508514, Final Batch Loss: 0.4333377182483673\n",
      "Epoch 6245, Loss: 1.7110138535499573, Final Batch Loss: 0.3087277412414551\n",
      "Epoch 6246, Loss: 1.7559552788734436, Final Batch Loss: 0.37712734937667847\n",
      "Epoch 6247, Loss: 1.7705931961536407, Final Batch Loss: 0.38654735684394836\n",
      "Epoch 6248, Loss: 1.7898076176643372, Final Batch Loss: 0.3484340012073517\n",
      "Epoch 6249, Loss: 1.5726352632045746, Final Batch Loss: 0.2790530025959015\n",
      "Epoch 6250, Loss: 1.7014323472976685, Final Batch Loss: 0.27603718638420105\n",
      "Epoch 6251, Loss: 1.7490169405937195, Final Batch Loss: 0.3483452796936035\n",
      "Epoch 6252, Loss: 1.5271756649017334, Final Batch Loss: 0.312010794878006\n",
      "Epoch 6253, Loss: 1.6863145530223846, Final Batch Loss: 0.2885824143886566\n",
      "Epoch 6254, Loss: 1.5241949707269669, Final Batch Loss: 0.31744614243507385\n",
      "Epoch 6255, Loss: 1.7587120234966278, Final Batch Loss: 0.3953607380390167\n",
      "Epoch 6256, Loss: 1.6676233410835266, Final Batch Loss: 0.351497083902359\n",
      "Epoch 6257, Loss: 1.657263696193695, Final Batch Loss: 0.339363694190979\n",
      "Epoch 6258, Loss: 1.68492591381073, Final Batch Loss: 0.3812882602214813\n",
      "Epoch 6259, Loss: 1.7995128333568573, Final Batch Loss: 0.3618662655353546\n",
      "Epoch 6260, Loss: 1.765805572271347, Final Batch Loss: 0.3728925883769989\n",
      "Epoch 6261, Loss: 1.6564319133758545, Final Batch Loss: 0.38366156816482544\n",
      "Epoch 6262, Loss: 1.619653731584549, Final Batch Loss: 0.26832887530326843\n",
      "Epoch 6263, Loss: 1.6637139320373535, Final Batch Loss: 0.2693046033382416\n",
      "Epoch 6264, Loss: 1.6154513955116272, Final Batch Loss: 0.4232109785079956\n",
      "Epoch 6265, Loss: 1.7966386377811432, Final Batch Loss: 0.3126259744167328\n",
      "Epoch 6266, Loss: 1.7566936314105988, Final Batch Loss: 0.3726886212825775\n",
      "Epoch 6267, Loss: 1.9174452126026154, Final Batch Loss: 0.37062868475914\n",
      "Epoch 6268, Loss: 1.540536880493164, Final Batch Loss: 0.2830139100551605\n",
      "Epoch 6269, Loss: 1.7590221464633942, Final Batch Loss: 0.26730671525001526\n",
      "Epoch 6270, Loss: 1.7216728031635284, Final Batch Loss: 0.29543888568878174\n",
      "Epoch 6271, Loss: 1.7293130159378052, Final Batch Loss: 0.3108813166618347\n",
      "Epoch 6272, Loss: 1.7905835807323456, Final Batch Loss: 0.355628103017807\n",
      "Epoch 6273, Loss: 1.6098707914352417, Final Batch Loss: 0.26919490098953247\n",
      "Epoch 6274, Loss: 1.6148960292339325, Final Batch Loss: 0.4098735749721527\n",
      "Epoch 6275, Loss: 1.7014618813991547, Final Batch Loss: 0.3225036561489105\n",
      "Epoch 6276, Loss: 1.7155705392360687, Final Batch Loss: 0.37666550278663635\n",
      "Epoch 6277, Loss: 1.664745956659317, Final Batch Loss: 0.3494266867637634\n",
      "Epoch 6278, Loss: 1.619164690375328, Final Batch Loss: 0.364295095205307\n",
      "Epoch 6279, Loss: 1.637979507446289, Final Batch Loss: 0.3575202524662018\n",
      "Epoch 6280, Loss: 1.6843261867761612, Final Batch Loss: 0.4306081533432007\n",
      "Epoch 6281, Loss: 1.6867167800664902, Final Batch Loss: 0.40521708130836487\n",
      "Epoch 6282, Loss: 1.7426485121250153, Final Batch Loss: 0.30620288848876953\n",
      "Epoch 6283, Loss: 1.5988920331001282, Final Batch Loss: 0.31652331352233887\n",
      "Epoch 6284, Loss: 1.7156058549880981, Final Batch Loss: 0.4547058939933777\n",
      "Epoch 6285, Loss: 1.6066988110542297, Final Batch Loss: 0.3767530620098114\n",
      "Epoch 6286, Loss: 1.6999690532684326, Final Batch Loss: 0.3850713074207306\n",
      "Epoch 6287, Loss: 1.4259676039218903, Final Batch Loss: 0.24366909265518188\n",
      "Epoch 6288, Loss: 1.599246010184288, Final Batch Loss: 0.23836733400821686\n",
      "Epoch 6289, Loss: 1.8392154574394226, Final Batch Loss: 0.4088246524333954\n",
      "Epoch 6290, Loss: 1.5294097065925598, Final Batch Loss: 0.26691946387290955\n",
      "Epoch 6291, Loss: 1.7554543912410736, Final Batch Loss: 0.36704543232917786\n",
      "Epoch 6292, Loss: 1.722048044204712, Final Batch Loss: 0.37644025683403015\n",
      "Epoch 6293, Loss: 1.625666320323944, Final Batch Loss: 0.32518279552459717\n",
      "Epoch 6294, Loss: 1.5980619490146637, Final Batch Loss: 0.33315199613571167\n",
      "Epoch 6295, Loss: 1.5388351380825043, Final Batch Loss: 0.2794051468372345\n",
      "Epoch 6296, Loss: 1.7115680575370789, Final Batch Loss: 0.3050285577774048\n",
      "Epoch 6297, Loss: 1.667944848537445, Final Batch Loss: 0.34814783930778503\n",
      "Epoch 6298, Loss: 1.4877387881278992, Final Batch Loss: 0.3269284963607788\n",
      "Epoch 6299, Loss: 1.726208209991455, Final Batch Loss: 0.22683444619178772\n",
      "Epoch 6300, Loss: 1.7284384667873383, Final Batch Loss: 0.412517249584198\n",
      "Epoch 6301, Loss: 1.6582313179969788, Final Batch Loss: 0.286785364151001\n",
      "Epoch 6302, Loss: 1.7031553983688354, Final Batch Loss: 0.3087019920349121\n",
      "Epoch 6303, Loss: 1.5686883479356766, Final Batch Loss: 0.24291802942752838\n",
      "Epoch 6304, Loss: 1.6834285855293274, Final Batch Loss: 0.3142472505569458\n",
      "Epoch 6305, Loss: 1.7549534142017365, Final Batch Loss: 0.2760087251663208\n",
      "Epoch 6306, Loss: 1.6663285195827484, Final Batch Loss: 0.30366554856300354\n",
      "Epoch 6307, Loss: 1.752740502357483, Final Batch Loss: 0.3425101041793823\n",
      "Epoch 6308, Loss: 1.8513862788677216, Final Batch Loss: 0.2910795211791992\n",
      "Epoch 6309, Loss: 1.5891091227531433, Final Batch Loss: 0.2840092182159424\n",
      "Epoch 6310, Loss: 1.6445975750684738, Final Batch Loss: 0.24832408130168915\n",
      "Epoch 6311, Loss: 1.7702964842319489, Final Batch Loss: 0.33680522441864014\n",
      "Epoch 6312, Loss: 1.7307429313659668, Final Batch Loss: 0.33870014548301697\n",
      "Epoch 6313, Loss: 1.6242412328720093, Final Batch Loss: 0.33009371161460876\n",
      "Epoch 6314, Loss: 1.545486718416214, Final Batch Loss: 0.25636518001556396\n",
      "Epoch 6315, Loss: 1.6902903318405151, Final Batch Loss: 0.3439146876335144\n",
      "Epoch 6316, Loss: 1.734510213136673, Final Batch Loss: 0.3368830382823944\n",
      "Epoch 6317, Loss: 1.627091497182846, Final Batch Loss: 0.34591004252433777\n",
      "Epoch 6318, Loss: 1.5689623951911926, Final Batch Loss: 0.2992511987686157\n",
      "Epoch 6319, Loss: 1.6780728995800018, Final Batch Loss: 0.3620067834854126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6320, Loss: 1.7061505317687988, Final Batch Loss: 0.3324115574359894\n",
      "Epoch 6321, Loss: 1.679908573627472, Final Batch Loss: 0.3261428475379944\n",
      "Epoch 6322, Loss: 1.6882622838020325, Final Batch Loss: 0.3104487955570221\n",
      "Epoch 6323, Loss: 1.7372756004333496, Final Batch Loss: 0.3278350234031677\n",
      "Epoch 6324, Loss: 1.6695877015590668, Final Batch Loss: 0.2671804428100586\n",
      "Epoch 6325, Loss: 1.728061854839325, Final Batch Loss: 0.31290459632873535\n",
      "Epoch 6326, Loss: 1.7001317143440247, Final Batch Loss: 0.30680879950523376\n",
      "Epoch 6327, Loss: 1.6782699823379517, Final Batch Loss: 0.34279927611351013\n",
      "Epoch 6328, Loss: 1.5688640624284744, Final Batch Loss: 0.23996315896511078\n",
      "Epoch 6329, Loss: 1.6773258745670319, Final Batch Loss: 0.33072903752326965\n",
      "Epoch 6330, Loss: 1.5233693569898605, Final Batch Loss: 0.34970036149024963\n",
      "Epoch 6331, Loss: 1.6751311123371124, Final Batch Loss: 0.2912839949131012\n",
      "Epoch 6332, Loss: 1.736097812652588, Final Batch Loss: 0.3058554232120514\n",
      "Epoch 6333, Loss: 1.7327432930469513, Final Batch Loss: 0.2780936360359192\n",
      "Epoch 6334, Loss: 1.7252190709114075, Final Batch Loss: 0.388209730386734\n",
      "Epoch 6335, Loss: 1.4743423759937286, Final Batch Loss: 0.31167522072792053\n",
      "Epoch 6336, Loss: 1.6274572610855103, Final Batch Loss: 0.3993540406227112\n",
      "Epoch 6337, Loss: 1.74948850274086, Final Batch Loss: 0.2657809555530548\n",
      "Epoch 6338, Loss: 1.4774886965751648, Final Batch Loss: 0.30609405040740967\n",
      "Epoch 6339, Loss: 1.5700980722904205, Final Batch Loss: 0.29946741461753845\n",
      "Epoch 6340, Loss: 1.5219978988170624, Final Batch Loss: 0.2599972188472748\n",
      "Epoch 6341, Loss: 1.5699347257614136, Final Batch Loss: 0.3495224416255951\n",
      "Epoch 6342, Loss: 1.5655004680156708, Final Batch Loss: 0.34425297379493713\n",
      "Epoch 6343, Loss: 1.7352165877819061, Final Batch Loss: 0.30909594893455505\n",
      "Epoch 6344, Loss: 1.7924538254737854, Final Batch Loss: 0.30239972472190857\n",
      "Epoch 6345, Loss: 1.5814542770385742, Final Batch Loss: 0.34888947010040283\n",
      "Epoch 6346, Loss: 1.6872968673706055, Final Batch Loss: 0.41401901841163635\n",
      "Epoch 6347, Loss: 1.6075384318828583, Final Batch Loss: 0.34630003571510315\n",
      "Epoch 6348, Loss: 1.6187083721160889, Final Batch Loss: 0.3038545548915863\n",
      "Epoch 6349, Loss: 1.5173314809799194, Final Batch Loss: 0.3431306481361389\n",
      "Epoch 6350, Loss: 1.6293910145759583, Final Batch Loss: 0.3374338448047638\n",
      "Epoch 6351, Loss: 1.5311774015426636, Final Batch Loss: 0.27512097358703613\n",
      "Epoch 6352, Loss: 1.454662948846817, Final Batch Loss: 0.26291176676750183\n",
      "Epoch 6353, Loss: 1.8717306852340698, Final Batch Loss: 0.5341399908065796\n",
      "Epoch 6354, Loss: 1.5361993908882141, Final Batch Loss: 0.3331416845321655\n",
      "Epoch 6355, Loss: 1.7852721214294434, Final Batch Loss: 0.3295539915561676\n",
      "Epoch 6356, Loss: 1.5571945309638977, Final Batch Loss: 0.3569655120372772\n",
      "Epoch 6357, Loss: 1.8058035969734192, Final Batch Loss: 0.3909449279308319\n",
      "Epoch 6358, Loss: 1.5770595222711563, Final Batch Loss: 0.2558501660823822\n",
      "Epoch 6359, Loss: 1.7216203808784485, Final Batch Loss: 0.34990784525871277\n",
      "Epoch 6360, Loss: 1.590985655784607, Final Batch Loss: 0.29504629969596863\n",
      "Epoch 6361, Loss: 1.7003585994243622, Final Batch Loss: 0.2968605160713196\n",
      "Epoch 6362, Loss: 1.753673642873764, Final Batch Loss: 0.35043516755104065\n",
      "Epoch 6363, Loss: 1.7710341811180115, Final Batch Loss: 0.3311573266983032\n",
      "Epoch 6364, Loss: 1.6266933381557465, Final Batch Loss: 0.3276887834072113\n",
      "Epoch 6365, Loss: 1.6739523708820343, Final Batch Loss: 0.2873682677745819\n",
      "Epoch 6366, Loss: 1.8662603199481964, Final Batch Loss: 0.433385968208313\n",
      "Epoch 6367, Loss: 1.6640992164611816, Final Batch Loss: 0.30852195620536804\n",
      "Epoch 6368, Loss: 1.6814928650856018, Final Batch Loss: 0.277362585067749\n",
      "Epoch 6369, Loss: 1.6392830908298492, Final Batch Loss: 0.32945993542671204\n",
      "Epoch 6370, Loss: 1.763595461845398, Final Batch Loss: 0.32731014490127563\n",
      "Epoch 6371, Loss: 1.7567020654678345, Final Batch Loss: 0.4065316319465637\n",
      "Epoch 6372, Loss: 1.7096522748470306, Final Batch Loss: 0.2948221266269684\n",
      "Epoch 6373, Loss: 1.6320052742958069, Final Batch Loss: 0.36424198746681213\n",
      "Epoch 6374, Loss: 1.7823732197284698, Final Batch Loss: 0.4756728410720825\n",
      "Epoch 6375, Loss: 1.7879729270935059, Final Batch Loss: 0.3654433488845825\n",
      "Epoch 6376, Loss: 1.7253116965293884, Final Batch Loss: 0.41942498087882996\n",
      "Epoch 6377, Loss: 1.8377492725849152, Final Batch Loss: 0.28929370641708374\n",
      "Epoch 6378, Loss: 1.5852296948432922, Final Batch Loss: 0.26439887285232544\n",
      "Epoch 6379, Loss: 1.6912590265274048, Final Batch Loss: 0.3395181894302368\n",
      "Epoch 6380, Loss: 1.7297228872776031, Final Batch Loss: 0.3056133985519409\n",
      "Epoch 6381, Loss: 1.6888515651226044, Final Batch Loss: 0.3701317310333252\n",
      "Epoch 6382, Loss: 1.6234880983829498, Final Batch Loss: 0.2503683865070343\n",
      "Epoch 6383, Loss: 1.6826276183128357, Final Batch Loss: 0.28182870149612427\n",
      "Epoch 6384, Loss: 1.6234790682792664, Final Batch Loss: 0.3271666467189789\n",
      "Epoch 6385, Loss: 1.6734987497329712, Final Batch Loss: 0.2777412235736847\n",
      "Epoch 6386, Loss: 1.7098348438739777, Final Batch Loss: 0.2735007107257843\n",
      "Epoch 6387, Loss: 1.731205701828003, Final Batch Loss: 0.4479160010814667\n",
      "Epoch 6388, Loss: 1.6006855368614197, Final Batch Loss: 0.3673539459705353\n",
      "Epoch 6389, Loss: 1.9090886116027832, Final Batch Loss: 0.44674357771873474\n",
      "Epoch 6390, Loss: 1.4747957587242126, Final Batch Loss: 0.3545287549495697\n",
      "Epoch 6391, Loss: 1.602065160870552, Final Batch Loss: 0.22461237013339996\n",
      "Epoch 6392, Loss: 1.7498625218868256, Final Batch Loss: 0.31883493065834045\n",
      "Epoch 6393, Loss: 1.7340581715106964, Final Batch Loss: 0.34665417671203613\n",
      "Epoch 6394, Loss: 1.6867809295654297, Final Batch Loss: 0.3902599513530731\n",
      "Epoch 6395, Loss: 1.7583329379558563, Final Batch Loss: 0.3698355257511139\n",
      "Epoch 6396, Loss: 1.7220772802829742, Final Batch Loss: 0.3299672603607178\n",
      "Epoch 6397, Loss: 1.7427306771278381, Final Batch Loss: 0.2964569330215454\n",
      "Epoch 6398, Loss: 1.6168519854545593, Final Batch Loss: 0.3251160681247711\n",
      "Epoch 6399, Loss: 1.7100209891796112, Final Batch Loss: 0.4053068161010742\n",
      "Epoch 6400, Loss: 1.6393510103225708, Final Batch Loss: 0.3677408695220947\n",
      "Epoch 6401, Loss: 1.6159241795539856, Final Batch Loss: 0.3915943503379822\n",
      "Epoch 6402, Loss: 1.57144296169281, Final Batch Loss: 0.33992087841033936\n",
      "Epoch 6403, Loss: 1.7576404809951782, Final Batch Loss: 0.41516029834747314\n",
      "Epoch 6404, Loss: 1.8321833908557892, Final Batch Loss: 0.31699803471565247\n",
      "Epoch 6405, Loss: 1.7723281979560852, Final Batch Loss: 0.33920419216156006\n",
      "Epoch 6406, Loss: 1.7293502986431122, Final Batch Loss: 0.3464984595775604\n",
      "Epoch 6407, Loss: 1.6372798681259155, Final Batch Loss: 0.3761345148086548\n",
      "Epoch 6408, Loss: 1.5964635908603668, Final Batch Loss: 0.2611430585384369\n",
      "Epoch 6409, Loss: 1.6651355922222137, Final Batch Loss: 0.28781336545944214\n",
      "Epoch 6410, Loss: 1.6940955519676208, Final Batch Loss: 0.40615254640579224\n",
      "Epoch 6411, Loss: 1.636443018913269, Final Batch Loss: 0.31540754437446594\n",
      "Epoch 6412, Loss: 1.7678927183151245, Final Batch Loss: 0.3890385925769806\n",
      "Epoch 6413, Loss: 1.6051934957504272, Final Batch Loss: 0.29735440015792847\n",
      "Epoch 6414, Loss: 1.6766732931137085, Final Batch Loss: 0.2820805013179779\n",
      "Epoch 6415, Loss: 1.7025597989559174, Final Batch Loss: 0.3212267756462097\n",
      "Epoch 6416, Loss: 1.6726796627044678, Final Batch Loss: 0.3425590693950653\n",
      "Epoch 6417, Loss: 1.6386623084545135, Final Batch Loss: 0.334513783454895\n",
      "Epoch 6418, Loss: 1.823327749967575, Final Batch Loss: 0.41467955708503723\n",
      "Epoch 6419, Loss: 1.6820141524076462, Final Batch Loss: 0.23963479697704315\n",
      "Epoch 6420, Loss: 1.62539142370224, Final Batch Loss: 0.27936655282974243\n",
      "Epoch 6421, Loss: 1.5781104862689972, Final Batch Loss: 0.30926892161369324\n",
      "Epoch 6422, Loss: 1.8023767173290253, Final Batch Loss: 0.42744001746177673\n",
      "Epoch 6423, Loss: 1.648291438817978, Final Batch Loss: 0.3079126477241516\n",
      "Epoch 6424, Loss: 1.7096112966537476, Final Batch Loss: 0.33431103825569153\n",
      "Epoch 6425, Loss: 1.716728389263153, Final Batch Loss: 0.36080896854400635\n",
      "Epoch 6426, Loss: 1.6307249963283539, Final Batch Loss: 0.35069653391838074\n",
      "Epoch 6427, Loss: 1.744940310716629, Final Batch Loss: 0.27462145686149597\n",
      "Epoch 6428, Loss: 1.6147468388080597, Final Batch Loss: 0.33883076906204224\n",
      "Epoch 6429, Loss: 1.6802034378051758, Final Batch Loss: 0.2714158892631531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6430, Loss: 1.6356495469808578, Final Batch Loss: 0.35858309268951416\n",
      "Epoch 6431, Loss: 1.733172744512558, Final Batch Loss: 0.4197433292865753\n",
      "Epoch 6432, Loss: 1.487030029296875, Final Batch Loss: 0.2694203555583954\n",
      "Epoch 6433, Loss: 1.70973140001297, Final Batch Loss: 0.334466814994812\n",
      "Epoch 6434, Loss: 1.5396378487348557, Final Batch Loss: 0.36044055223464966\n",
      "Epoch 6435, Loss: 1.5684564411640167, Final Batch Loss: 0.36632171273231506\n",
      "Epoch 6436, Loss: 1.8301260471343994, Final Batch Loss: 0.2945915460586548\n",
      "Epoch 6437, Loss: 1.8079193532466888, Final Batch Loss: 0.29117199778556824\n",
      "Epoch 6438, Loss: 1.7978343069553375, Final Batch Loss: 0.3059217631816864\n",
      "Epoch 6439, Loss: 1.6750583052635193, Final Batch Loss: 0.3725607097148895\n",
      "Epoch 6440, Loss: 1.5838046371936798, Final Batch Loss: 0.2574864625930786\n",
      "Epoch 6441, Loss: 1.702886313199997, Final Batch Loss: 0.32009974122047424\n",
      "Epoch 6442, Loss: 1.7548335492610931, Final Batch Loss: 0.3113311231136322\n",
      "Epoch 6443, Loss: 1.719471037387848, Final Batch Loss: 0.28825029730796814\n",
      "Epoch 6444, Loss: 1.5657443851232529, Final Batch Loss: 0.2822314500808716\n",
      "Epoch 6445, Loss: 1.5545509159564972, Final Batch Loss: 0.24348002672195435\n",
      "Epoch 6446, Loss: 1.6400519609451294, Final Batch Loss: 0.25690436363220215\n",
      "Epoch 6447, Loss: 1.676097184419632, Final Batch Loss: 0.3066974878311157\n",
      "Epoch 6448, Loss: 1.8084306418895721, Final Batch Loss: 0.38295862078666687\n",
      "Epoch 6449, Loss: 1.6595939993858337, Final Batch Loss: 0.2819136381149292\n",
      "Epoch 6450, Loss: 1.588946908712387, Final Batch Loss: 0.33132579922676086\n",
      "Epoch 6451, Loss: 1.484523206949234, Final Batch Loss: 0.30911892652511597\n",
      "Epoch 6452, Loss: 1.545450747013092, Final Batch Loss: 0.3355514705181122\n",
      "Epoch 6453, Loss: 1.69530388712883, Final Batch Loss: 0.20672860741615295\n",
      "Epoch 6454, Loss: 1.7859719693660736, Final Batch Loss: 0.33508923649787903\n",
      "Epoch 6455, Loss: 1.6974256932735443, Final Batch Loss: 0.29678308963775635\n",
      "Epoch 6456, Loss: 1.6466167569160461, Final Batch Loss: 0.3607284724712372\n",
      "Epoch 6457, Loss: 1.573734611272812, Final Batch Loss: 0.32741785049438477\n",
      "Epoch 6458, Loss: 1.7969275414943695, Final Batch Loss: 0.41667699813842773\n",
      "Epoch 6459, Loss: 1.6666158884763718, Final Batch Loss: 0.331207275390625\n",
      "Epoch 6460, Loss: 1.733891785144806, Final Batch Loss: 0.3261289596557617\n",
      "Epoch 6461, Loss: 1.6612142026424408, Final Batch Loss: 0.32265180349349976\n",
      "Epoch 6462, Loss: 1.653632014989853, Final Batch Loss: 0.41657334566116333\n",
      "Epoch 6463, Loss: 1.7288924157619476, Final Batch Loss: 0.30984315276145935\n",
      "Epoch 6464, Loss: 1.677223950624466, Final Batch Loss: 0.32832178473472595\n",
      "Epoch 6465, Loss: 1.6001338362693787, Final Batch Loss: 0.3561954200267792\n",
      "Epoch 6466, Loss: 1.7172722220420837, Final Batch Loss: 0.2740286588668823\n",
      "Epoch 6467, Loss: 1.5950400233268738, Final Batch Loss: 0.32906755805015564\n",
      "Epoch 6468, Loss: 1.6868110001087189, Final Batch Loss: 0.3831568658351898\n",
      "Epoch 6469, Loss: 1.6385089457035065, Final Batch Loss: 0.2901414930820465\n",
      "Epoch 6470, Loss: 1.8450092673301697, Final Batch Loss: 0.38784319162368774\n",
      "Epoch 6471, Loss: 1.6548961400985718, Final Batch Loss: 0.301747590303421\n",
      "Epoch 6472, Loss: 1.6258117854595184, Final Batch Loss: 0.2861095070838928\n",
      "Epoch 6473, Loss: 1.5668262243270874, Final Batch Loss: 0.3096267282962799\n",
      "Epoch 6474, Loss: 1.6614191234111786, Final Batch Loss: 0.3935586214065552\n",
      "Epoch 6475, Loss: 1.7805317640304565, Final Batch Loss: 0.3707437515258789\n",
      "Epoch 6476, Loss: 1.5912497490644455, Final Batch Loss: 0.3085581660270691\n",
      "Epoch 6477, Loss: 1.7772842347621918, Final Batch Loss: 0.31210002303123474\n",
      "Epoch 6478, Loss: 1.6786790490150452, Final Batch Loss: 0.37389662861824036\n",
      "Epoch 6479, Loss: 1.5593461990356445, Final Batch Loss: 0.25503772497177124\n",
      "Epoch 6480, Loss: 1.7051020562648773, Final Batch Loss: 0.35830625891685486\n",
      "Epoch 6481, Loss: 1.7039640247821808, Final Batch Loss: 0.3068694472312927\n",
      "Epoch 6482, Loss: 1.8576399981975555, Final Batch Loss: 0.324809193611145\n",
      "Epoch 6483, Loss: 1.6467697620391846, Final Batch Loss: 0.2710793912410736\n",
      "Epoch 6484, Loss: 1.6867700815200806, Final Batch Loss: 0.3821403980255127\n",
      "Epoch 6485, Loss: 1.5950364768505096, Final Batch Loss: 0.322015643119812\n",
      "Epoch 6486, Loss: 1.8053613901138306, Final Batch Loss: 0.3230399489402771\n",
      "Epoch 6487, Loss: 1.7778312861919403, Final Batch Loss: 0.28853094577789307\n",
      "Epoch 6488, Loss: 1.552136868238449, Final Batch Loss: 0.3042713403701782\n",
      "Epoch 6489, Loss: 1.5961918532848358, Final Batch Loss: 0.3230466842651367\n",
      "Epoch 6490, Loss: 1.6703271269798279, Final Batch Loss: 0.3994172513484955\n",
      "Epoch 6491, Loss: 1.712370216846466, Final Batch Loss: 0.37888437509536743\n",
      "Epoch 6492, Loss: 1.5708229839801788, Final Batch Loss: 0.41388726234436035\n",
      "Epoch 6493, Loss: 1.8412549495697021, Final Batch Loss: 0.3665983974933624\n",
      "Epoch 6494, Loss: 1.7558696568012238, Final Batch Loss: 0.337883323431015\n",
      "Epoch 6495, Loss: 1.6145883202552795, Final Batch Loss: 0.2867659330368042\n",
      "Epoch 6496, Loss: 1.720237910747528, Final Batch Loss: 0.3885543644428253\n",
      "Epoch 6497, Loss: 1.6960926353931427, Final Batch Loss: 0.3343288004398346\n",
      "Epoch 6498, Loss: 1.642330527305603, Final Batch Loss: 0.2836226522922516\n",
      "Epoch 6499, Loss: 1.5680842995643616, Final Batch Loss: 0.3986588716506958\n",
      "Epoch 6500, Loss: 1.5658625960350037, Final Batch Loss: 0.27914416790008545\n",
      "Epoch 6501, Loss: 1.6654163002967834, Final Batch Loss: 0.2861446142196655\n",
      "Epoch 6502, Loss: 1.6181910932064056, Final Batch Loss: 0.23541328310966492\n",
      "Epoch 6503, Loss: 1.5996912121772766, Final Batch Loss: 0.31181061267852783\n",
      "Epoch 6504, Loss: 1.6968868374824524, Final Batch Loss: 0.2880852222442627\n",
      "Epoch 6505, Loss: 1.5684982538223267, Final Batch Loss: 0.2869129478931427\n",
      "Epoch 6506, Loss: 1.6590133607387543, Final Batch Loss: 0.34734347462654114\n",
      "Epoch 6507, Loss: 1.6252285242080688, Final Batch Loss: 0.30324336886405945\n",
      "Epoch 6508, Loss: 1.7004958093166351, Final Batch Loss: 0.4147149622440338\n",
      "Epoch 6509, Loss: 1.6094994693994522, Final Batch Loss: 0.36515095829963684\n",
      "Epoch 6510, Loss: 1.6562080681324005, Final Batch Loss: 0.3312186300754547\n",
      "Epoch 6511, Loss: 1.6475618481636047, Final Batch Loss: 0.40706169605255127\n",
      "Epoch 6512, Loss: 1.7498949468135834, Final Batch Loss: 0.3722290098667145\n",
      "Epoch 6513, Loss: 1.840944916009903, Final Batch Loss: 0.5488461852073669\n",
      "Epoch 6514, Loss: 1.5626005828380585, Final Batch Loss: 0.3097749352455139\n",
      "Epoch 6515, Loss: 1.898741990327835, Final Batch Loss: 0.3045293092727661\n",
      "Epoch 6516, Loss: 1.7391869276762009, Final Batch Loss: 0.31370213627815247\n",
      "Epoch 6517, Loss: 1.6931764632463455, Final Batch Loss: 0.3977070152759552\n",
      "Epoch 6518, Loss: 1.644069343805313, Final Batch Loss: 0.3648949861526489\n",
      "Epoch 6519, Loss: 1.794076144695282, Final Batch Loss: 0.4802823066711426\n",
      "Epoch 6520, Loss: 1.5425783097743988, Final Batch Loss: 0.2805148661136627\n",
      "Epoch 6521, Loss: 1.6242904663085938, Final Batch Loss: 0.28826677799224854\n",
      "Epoch 6522, Loss: 1.5769039392471313, Final Batch Loss: 0.33470386266708374\n",
      "Epoch 6523, Loss: 1.6932021379470825, Final Batch Loss: 0.3563326895236969\n",
      "Epoch 6524, Loss: 1.6394357979297638, Final Batch Loss: 0.37280288338661194\n",
      "Epoch 6525, Loss: 1.6818345487117767, Final Batch Loss: 0.32150930166244507\n",
      "Epoch 6526, Loss: 1.6914541721343994, Final Batch Loss: 0.38325661420822144\n",
      "Epoch 6527, Loss: 1.5723476707935333, Final Batch Loss: 0.2653595209121704\n",
      "Epoch 6528, Loss: 1.793245106935501, Final Batch Loss: 0.3487410545349121\n",
      "Epoch 6529, Loss: 1.5311653316020966, Final Batch Loss: 0.3211953639984131\n",
      "Epoch 6530, Loss: 1.8299627006053925, Final Batch Loss: 0.3155924379825592\n",
      "Epoch 6531, Loss: 1.750422641634941, Final Batch Loss: 0.23477928340435028\n",
      "Epoch 6532, Loss: 1.642408847808838, Final Batch Loss: 0.3392559587955475\n",
      "Epoch 6533, Loss: 1.6869862079620361, Final Batch Loss: 0.2374786138534546\n",
      "Epoch 6534, Loss: 1.8593847453594208, Final Batch Loss: 0.40048009157180786\n",
      "Epoch 6535, Loss: 1.6270842850208282, Final Batch Loss: 0.39203017950057983\n",
      "Epoch 6536, Loss: 1.5078235268592834, Final Batch Loss: 0.3083905279636383\n",
      "Epoch 6537, Loss: 1.8688587844371796, Final Batch Loss: 0.2734381854534149\n",
      "Epoch 6538, Loss: 1.7023933976888657, Final Batch Loss: 0.32164180278778076\n",
      "Epoch 6539, Loss: 1.5896957218647003, Final Batch Loss: 0.34694704413414\n",
      "Epoch 6540, Loss: 1.6892053484916687, Final Batch Loss: 0.31535616517066956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6541, Loss: 1.6408242881298065, Final Batch Loss: 0.3453652262687683\n",
      "Epoch 6542, Loss: 1.7219994068145752, Final Batch Loss: 0.2911706864833832\n",
      "Epoch 6543, Loss: 1.7223860621452332, Final Batch Loss: 0.4392174184322357\n",
      "Epoch 6544, Loss: 1.7658160030841827, Final Batch Loss: 0.3560734689235687\n",
      "Epoch 6545, Loss: 1.6920075416564941, Final Batch Loss: 0.2779630422592163\n",
      "Epoch 6546, Loss: 1.6729681193828583, Final Batch Loss: 0.3212992548942566\n",
      "Epoch 6547, Loss: 1.8173128366470337, Final Batch Loss: 0.46614503860473633\n",
      "Epoch 6548, Loss: 1.7464789152145386, Final Batch Loss: 0.38336771726608276\n",
      "Epoch 6549, Loss: 1.6553209125995636, Final Batch Loss: 0.4116300344467163\n",
      "Epoch 6550, Loss: 1.6455001831054688, Final Batch Loss: 0.3333825170993805\n",
      "Epoch 6551, Loss: 1.647590309381485, Final Batch Loss: 0.32048916816711426\n",
      "Epoch 6552, Loss: 1.7001085877418518, Final Batch Loss: 0.39287611842155457\n",
      "Epoch 6553, Loss: 1.762083649635315, Final Batch Loss: 0.21069544553756714\n",
      "Epoch 6554, Loss: 1.7358969151973724, Final Batch Loss: 0.3336023688316345\n",
      "Epoch 6555, Loss: 1.7339876294136047, Final Batch Loss: 0.3340950906276703\n",
      "Epoch 6556, Loss: 1.8494182527065277, Final Batch Loss: 0.4732913076877594\n",
      "Epoch 6557, Loss: 1.6971793323755264, Final Batch Loss: 0.36696258187294006\n",
      "Epoch 6558, Loss: 1.5579391121864319, Final Batch Loss: 0.29433420300483704\n",
      "Epoch 6559, Loss: 1.6347682774066925, Final Batch Loss: 0.377850204706192\n",
      "Epoch 6560, Loss: 1.6201488673686981, Final Batch Loss: 0.3351351022720337\n",
      "Epoch 6561, Loss: 1.6084271669387817, Final Batch Loss: 0.3891843557357788\n",
      "Epoch 6562, Loss: 1.7898803651332855, Final Batch Loss: 0.43674197793006897\n",
      "Epoch 6563, Loss: 1.5677192658185959, Final Batch Loss: 0.33297938108444214\n",
      "Epoch 6564, Loss: 1.702954113483429, Final Batch Loss: 0.2967926263809204\n",
      "Epoch 6565, Loss: 1.7468596696853638, Final Batch Loss: 0.29993197321891785\n",
      "Epoch 6566, Loss: 1.781593769788742, Final Batch Loss: 0.4252873361110687\n",
      "Epoch 6567, Loss: 1.5382499694824219, Final Batch Loss: 0.2364366054534912\n",
      "Epoch 6568, Loss: 1.6147843599319458, Final Batch Loss: 0.26109448075294495\n",
      "Epoch 6569, Loss: 1.8277869522571564, Final Batch Loss: 0.3569720685482025\n",
      "Epoch 6570, Loss: 1.5993068516254425, Final Batch Loss: 0.3539242744445801\n",
      "Epoch 6571, Loss: 1.5515536665916443, Final Batch Loss: 0.30587711930274963\n",
      "Epoch 6572, Loss: 1.6825840175151825, Final Batch Loss: 0.3955315351486206\n",
      "Epoch 6573, Loss: 1.7191346287727356, Final Batch Loss: 0.3335697650909424\n",
      "Epoch 6574, Loss: 1.7457917630672455, Final Batch Loss: 0.3105149269104004\n",
      "Epoch 6575, Loss: 1.65189728140831, Final Batch Loss: 0.24850991368293762\n",
      "Epoch 6576, Loss: 1.7132051587104797, Final Batch Loss: 0.34690937399864197\n",
      "Epoch 6577, Loss: 1.6109262555837631, Final Batch Loss: 0.36009347438812256\n",
      "Epoch 6578, Loss: 1.6498246788978577, Final Batch Loss: 0.38496240973472595\n",
      "Epoch 6579, Loss: 1.7342541813850403, Final Batch Loss: 0.38015735149383545\n",
      "Epoch 6580, Loss: 1.7490801215171814, Final Batch Loss: 0.31556156277656555\n",
      "Epoch 6581, Loss: 1.4730058014392853, Final Batch Loss: 0.3025955259799957\n",
      "Epoch 6582, Loss: 1.5473130643367767, Final Batch Loss: 0.33164462447166443\n",
      "Epoch 6583, Loss: 1.8326396644115448, Final Batch Loss: 0.37115633487701416\n",
      "Epoch 6584, Loss: 1.6585812270641327, Final Batch Loss: 0.3000635802745819\n",
      "Epoch 6585, Loss: 1.7145284712314606, Final Batch Loss: 0.38970258831977844\n",
      "Epoch 6586, Loss: 1.5374862253665924, Final Batch Loss: 0.30633920431137085\n",
      "Epoch 6587, Loss: 1.5558390021324158, Final Batch Loss: 0.35039713978767395\n",
      "Epoch 6588, Loss: 1.5743745863437653, Final Batch Loss: 0.373270720243454\n",
      "Epoch 6589, Loss: 1.7076388001441956, Final Batch Loss: 0.29956716299057007\n",
      "Epoch 6590, Loss: 1.5227842777967453, Final Batch Loss: 0.3472996652126312\n",
      "Epoch 6591, Loss: 1.5879192054271698, Final Batch Loss: 0.3608801066875458\n",
      "Epoch 6592, Loss: 1.5576384663581848, Final Batch Loss: 0.28771838545799255\n",
      "Epoch 6593, Loss: 1.7747564613819122, Final Batch Loss: 0.43265730142593384\n",
      "Epoch 6594, Loss: 1.566707283258438, Final Batch Loss: 0.3816251754760742\n",
      "Epoch 6595, Loss: 1.6926150619983673, Final Batch Loss: 0.34725168347358704\n",
      "Epoch 6596, Loss: 1.6761622428894043, Final Batch Loss: 0.36342811584472656\n",
      "Epoch 6597, Loss: 1.7250937819480896, Final Batch Loss: 0.33987095952033997\n",
      "Epoch 6598, Loss: 1.4745022803544998, Final Batch Loss: 0.21281571686267853\n",
      "Epoch 6599, Loss: 1.5708258599042892, Final Batch Loss: 0.2688721716403961\n",
      "Epoch 6600, Loss: 1.709518849849701, Final Batch Loss: 0.3637462258338928\n",
      "Epoch 6601, Loss: 1.7264549136161804, Final Batch Loss: 0.2976481318473816\n",
      "Epoch 6602, Loss: 1.7255950570106506, Final Batch Loss: 0.39575862884521484\n",
      "Epoch 6603, Loss: 1.6507771611213684, Final Batch Loss: 0.2967749536037445\n",
      "Epoch 6604, Loss: 1.6655206084251404, Final Batch Loss: 0.32049083709716797\n",
      "Epoch 6605, Loss: 1.7978828847408295, Final Batch Loss: 0.37556785345077515\n",
      "Epoch 6606, Loss: 1.595872849225998, Final Batch Loss: 0.3052241802215576\n",
      "Epoch 6607, Loss: 1.7299494445323944, Final Batch Loss: 0.4175177812576294\n",
      "Epoch 6608, Loss: 1.6971541047096252, Final Batch Loss: 0.23474806547164917\n",
      "Epoch 6609, Loss: 1.7746714651584625, Final Batch Loss: 0.2801361382007599\n",
      "Epoch 6610, Loss: 1.5940596014261246, Final Batch Loss: 0.33915311098098755\n",
      "Epoch 6611, Loss: 1.5277008712291718, Final Batch Loss: 0.3089265525341034\n",
      "Epoch 6612, Loss: 1.633323758840561, Final Batch Loss: 0.2749931216239929\n",
      "Epoch 6613, Loss: 1.5650736391544342, Final Batch Loss: 0.35208916664123535\n",
      "Epoch 6614, Loss: 1.6548527777194977, Final Batch Loss: 0.2990919053554535\n",
      "Epoch 6615, Loss: 1.5216257274150848, Final Batch Loss: 0.3429776132106781\n",
      "Epoch 6616, Loss: 1.5545195490121841, Final Batch Loss: 0.3389546573162079\n",
      "Epoch 6617, Loss: 1.5388101637363434, Final Batch Loss: 0.2700974643230438\n",
      "Epoch 6618, Loss: 1.6205246150493622, Final Batch Loss: 0.27763253450393677\n",
      "Epoch 6619, Loss: 1.4267380237579346, Final Batch Loss: 0.3069343864917755\n",
      "Epoch 6620, Loss: 1.7399846613407135, Final Batch Loss: 0.2815209925174713\n",
      "Epoch 6621, Loss: 1.6342288255691528, Final Batch Loss: 0.34838131070137024\n",
      "Epoch 6622, Loss: 1.8062546849250793, Final Batch Loss: 0.37889739871025085\n",
      "Epoch 6623, Loss: 1.658704936504364, Final Batch Loss: 0.314815491437912\n",
      "Epoch 6624, Loss: 1.7214547395706177, Final Batch Loss: 0.28646114468574524\n",
      "Epoch 6625, Loss: 1.6433424949645996, Final Batch Loss: 0.2652958929538727\n",
      "Epoch 6626, Loss: 1.5881602764129639, Final Batch Loss: 0.26307857036590576\n",
      "Epoch 6627, Loss: 1.6594178974628448, Final Batch Loss: 0.3513542115688324\n",
      "Epoch 6628, Loss: 1.6974535286426544, Final Batch Loss: 0.2786716818809509\n",
      "Epoch 6629, Loss: 1.79450261592865, Final Batch Loss: 0.3153287470340729\n",
      "Epoch 6630, Loss: 1.5517347157001495, Final Batch Loss: 0.35311317443847656\n",
      "Epoch 6631, Loss: 1.6794427633285522, Final Batch Loss: 0.38457974791526794\n",
      "Epoch 6632, Loss: 1.663750320672989, Final Batch Loss: 0.302092969417572\n",
      "Epoch 6633, Loss: 1.5806899964809418, Final Batch Loss: 0.30778583884239197\n",
      "Epoch 6634, Loss: 1.7193041443824768, Final Batch Loss: 0.31069326400756836\n",
      "Epoch 6635, Loss: 1.7411304414272308, Final Batch Loss: 0.30112695693969727\n",
      "Epoch 6636, Loss: 1.7005274444818497, Final Batch Loss: 0.4720444977283478\n",
      "Epoch 6637, Loss: 1.563696175813675, Final Batch Loss: 0.32290276885032654\n",
      "Epoch 6638, Loss: 1.5394324362277985, Final Batch Loss: 0.3104439675807953\n",
      "Epoch 6639, Loss: 1.8378306925296783, Final Batch Loss: 0.5033477544784546\n",
      "Epoch 6640, Loss: 1.5377052277326584, Final Batch Loss: 0.24728314578533173\n",
      "Epoch 6641, Loss: 1.5892340242862701, Final Batch Loss: 0.25944986939430237\n",
      "Epoch 6642, Loss: 1.6954730451107025, Final Batch Loss: 0.3593156933784485\n",
      "Epoch 6643, Loss: 1.6713132113218307, Final Batch Loss: 0.3253653049468994\n",
      "Epoch 6644, Loss: 1.6149951815605164, Final Batch Loss: 0.36595943570137024\n",
      "Epoch 6645, Loss: 1.7572530806064606, Final Batch Loss: 0.3686565160751343\n",
      "Epoch 6646, Loss: 1.6852066218852997, Final Batch Loss: 0.3656451106071472\n",
      "Epoch 6647, Loss: 1.61428964138031, Final Batch Loss: 0.3144110441207886\n",
      "Epoch 6648, Loss: 1.7404527962207794, Final Batch Loss: 0.330081045627594\n",
      "Epoch 6649, Loss: 1.5741302371025085, Final Batch Loss: 0.2936229407787323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6650, Loss: 1.6828893721103668, Final Batch Loss: 0.3789826035499573\n",
      "Epoch 6651, Loss: 1.6331976056098938, Final Batch Loss: 0.31895920634269714\n",
      "Epoch 6652, Loss: 1.5470969676971436, Final Batch Loss: 0.33091846108436584\n",
      "Epoch 6653, Loss: 1.6117674708366394, Final Batch Loss: 0.37993860244750977\n",
      "Epoch 6654, Loss: 1.796251118183136, Final Batch Loss: 0.31545257568359375\n",
      "Epoch 6655, Loss: 1.6661154627799988, Final Batch Loss: 0.30706655979156494\n",
      "Epoch 6656, Loss: 1.5496488511562347, Final Batch Loss: 0.35299959778785706\n",
      "Epoch 6657, Loss: 1.5438454002141953, Final Batch Loss: 0.28579071164131165\n",
      "Epoch 6658, Loss: 1.5112482011318207, Final Batch Loss: 0.3075616955757141\n",
      "Epoch 6659, Loss: 1.4894248247146606, Final Batch Loss: 0.30748698115348816\n",
      "Epoch 6660, Loss: 1.481079339981079, Final Batch Loss: 0.33206427097320557\n",
      "Epoch 6661, Loss: 1.5735197365283966, Final Batch Loss: 0.359904944896698\n",
      "Epoch 6662, Loss: 1.6341683268547058, Final Batch Loss: 0.3309716284275055\n",
      "Epoch 6663, Loss: 1.7264006733894348, Final Batch Loss: 0.45837458968162537\n",
      "Epoch 6664, Loss: 1.584911584854126, Final Batch Loss: 0.3492441475391388\n",
      "Epoch 6665, Loss: 1.807519257068634, Final Batch Loss: 0.3038652241230011\n",
      "Epoch 6666, Loss: 1.65277099609375, Final Batch Loss: 0.41103729605674744\n",
      "Epoch 6667, Loss: 1.6626020967960358, Final Batch Loss: 0.3065348267555237\n",
      "Epoch 6668, Loss: 1.7470337450504303, Final Batch Loss: 0.35096806287765503\n",
      "Epoch 6669, Loss: 1.7527433931827545, Final Batch Loss: 0.3514067530632019\n",
      "Epoch 6670, Loss: 1.6264533996582031, Final Batch Loss: 0.34264740347862244\n",
      "Epoch 6671, Loss: 1.587817221879959, Final Batch Loss: 0.2892906963825226\n",
      "Epoch 6672, Loss: 1.6301689445972443, Final Batch Loss: 0.2788057327270508\n",
      "Epoch 6673, Loss: 1.7067652642726898, Final Batch Loss: 0.32267463207244873\n",
      "Epoch 6674, Loss: 1.6779659986495972, Final Batch Loss: 0.3740527927875519\n",
      "Epoch 6675, Loss: 1.7661175429821014, Final Batch Loss: 0.2971241772174835\n",
      "Epoch 6676, Loss: 1.7876014113426208, Final Batch Loss: 0.3394746482372284\n",
      "Epoch 6677, Loss: 1.6523862183094025, Final Batch Loss: 0.38580355048179626\n",
      "Epoch 6678, Loss: 1.6236715912818909, Final Batch Loss: 0.2730804681777954\n",
      "Epoch 6679, Loss: 1.5908541679382324, Final Batch Loss: 0.2943337559700012\n",
      "Epoch 6680, Loss: 1.577685222029686, Final Batch Loss: 0.31110572814941406\n",
      "Epoch 6681, Loss: 1.6709847152233124, Final Batch Loss: 0.38535723090171814\n",
      "Epoch 6682, Loss: 1.6917773187160492, Final Batch Loss: 0.23051145672798157\n",
      "Epoch 6683, Loss: 1.5863308012485504, Final Batch Loss: 0.29160839319229126\n",
      "Epoch 6684, Loss: 1.5558591783046722, Final Batch Loss: 0.30048638582229614\n",
      "Epoch 6685, Loss: 1.5357351899147034, Final Batch Loss: 0.35060086846351624\n",
      "Epoch 6686, Loss: 1.4847789108753204, Final Batch Loss: 0.3447601795196533\n",
      "Epoch 6687, Loss: 1.797008991241455, Final Batch Loss: 0.35819780826568604\n",
      "Epoch 6688, Loss: 1.6243294477462769, Final Batch Loss: 0.3085605502128601\n",
      "Epoch 6689, Loss: 1.6810788810253143, Final Batch Loss: 0.3480336368083954\n",
      "Epoch 6690, Loss: 1.588343307375908, Final Batch Loss: 0.3306158781051636\n",
      "Epoch 6691, Loss: 1.7003338634967804, Final Batch Loss: 0.3912466764450073\n",
      "Epoch 6692, Loss: 1.4892569482326508, Final Batch Loss: 0.3087801933288574\n",
      "Epoch 6693, Loss: 1.4904864430427551, Final Batch Loss: 0.313209593296051\n",
      "Epoch 6694, Loss: 1.516905814409256, Final Batch Loss: 0.27840691804885864\n",
      "Epoch 6695, Loss: 1.5873935222625732, Final Batch Loss: 0.36422160267829895\n",
      "Epoch 6696, Loss: 1.748622089624405, Final Batch Loss: 0.3757171332836151\n",
      "Epoch 6697, Loss: 1.7778684198856354, Final Batch Loss: 0.2680550217628479\n",
      "Epoch 6698, Loss: 1.783967912197113, Final Batch Loss: 0.4251522421836853\n",
      "Epoch 6699, Loss: 1.6695392727851868, Final Batch Loss: 0.28260067105293274\n",
      "Epoch 6700, Loss: 1.5604840815067291, Final Batch Loss: 0.30255192518234253\n",
      "Epoch 6701, Loss: 1.5042843371629715, Final Batch Loss: 0.3408993184566498\n",
      "Epoch 6702, Loss: 1.761376291513443, Final Batch Loss: 0.3517582416534424\n",
      "Epoch 6703, Loss: 1.6515504717826843, Final Batch Loss: 0.36026138067245483\n",
      "Epoch 6704, Loss: 1.6971741616725922, Final Batch Loss: 0.32713374495506287\n",
      "Epoch 6705, Loss: 1.6512089669704437, Final Batch Loss: 0.31571897864341736\n",
      "Epoch 6706, Loss: 1.697448581457138, Final Batch Loss: 0.269717276096344\n",
      "Epoch 6707, Loss: 1.5145321041345596, Final Batch Loss: 0.3474239110946655\n",
      "Epoch 6708, Loss: 1.5496824532747269, Final Batch Loss: 0.3090049624443054\n",
      "Epoch 6709, Loss: 1.5945011377334595, Final Batch Loss: 0.2858002483844757\n",
      "Epoch 6710, Loss: 1.6256585121154785, Final Batch Loss: 0.27535414695739746\n",
      "Epoch 6711, Loss: 1.5888218581676483, Final Batch Loss: 0.28080934286117554\n",
      "Epoch 6712, Loss: 1.6974460184574127, Final Batch Loss: 0.35799404978752136\n",
      "Epoch 6713, Loss: 1.560744971036911, Final Batch Loss: 0.39145731925964355\n",
      "Epoch 6714, Loss: 1.4388232231140137, Final Batch Loss: 0.3246724605560303\n",
      "Epoch 6715, Loss: 1.6781209409236908, Final Batch Loss: 0.2602701783180237\n",
      "Epoch 6716, Loss: 1.5770394206047058, Final Batch Loss: 0.28583675622940063\n",
      "Epoch 6717, Loss: 1.6076833605766296, Final Batch Loss: 0.32876530289649963\n",
      "Epoch 6718, Loss: 1.6587382853031158, Final Batch Loss: 0.37533432245254517\n",
      "Epoch 6719, Loss: 1.7160780429840088, Final Batch Loss: 0.43870073556900024\n",
      "Epoch 6720, Loss: 1.567269891500473, Final Batch Loss: 0.34943532943725586\n",
      "Epoch 6721, Loss: 1.7078283727169037, Final Batch Loss: 0.2730881869792938\n",
      "Epoch 6722, Loss: 1.601656436920166, Final Batch Loss: 0.2919478118419647\n",
      "Epoch 6723, Loss: 1.6789984703063965, Final Batch Loss: 0.3039373457431793\n",
      "Epoch 6724, Loss: 1.4982531517744064, Final Batch Loss: 0.34517043828964233\n",
      "Epoch 6725, Loss: 1.6843679547309875, Final Batch Loss: 0.34949615597724915\n",
      "Epoch 6726, Loss: 1.9218196272850037, Final Batch Loss: 0.5277509689331055\n",
      "Epoch 6727, Loss: 1.5938914716243744, Final Batch Loss: 0.2702822685241699\n",
      "Epoch 6728, Loss: 1.6267465651035309, Final Batch Loss: 0.32952621579170227\n",
      "Epoch 6729, Loss: 1.6065223217010498, Final Batch Loss: 0.3095678389072418\n",
      "Epoch 6730, Loss: 1.7363898754119873, Final Batch Loss: 0.29492130875587463\n",
      "Epoch 6731, Loss: 1.7044731676578522, Final Batch Loss: 0.2966275215148926\n",
      "Epoch 6732, Loss: 1.640792429447174, Final Batch Loss: 0.33746084570884705\n",
      "Epoch 6733, Loss: 1.6224925220012665, Final Batch Loss: 0.2796480655670166\n",
      "Epoch 6734, Loss: 1.819732904434204, Final Batch Loss: 0.32122159004211426\n",
      "Epoch 6735, Loss: 1.813618004322052, Final Batch Loss: 0.3647007346153259\n",
      "Epoch 6736, Loss: 1.7147698998451233, Final Batch Loss: 0.3206816613674164\n",
      "Epoch 6737, Loss: 1.5718189179897308, Final Batch Loss: 0.2898712754249573\n",
      "Epoch 6738, Loss: 1.6323702037334442, Final Batch Loss: 0.3407212793827057\n",
      "Epoch 6739, Loss: 1.8920409381389618, Final Batch Loss: 0.4055287539958954\n",
      "Epoch 6740, Loss: 1.5507501512765884, Final Batch Loss: 0.455562025308609\n",
      "Epoch 6741, Loss: 1.585002839565277, Final Batch Loss: 0.3233746886253357\n",
      "Epoch 6742, Loss: 1.578346997499466, Final Batch Loss: 0.3448537588119507\n",
      "Epoch 6743, Loss: 1.613667070865631, Final Batch Loss: 0.28580471873283386\n",
      "Epoch 6744, Loss: 1.554655760526657, Final Batch Loss: 0.3070133328437805\n",
      "Epoch 6745, Loss: 1.4063697457313538, Final Batch Loss: 0.31031373143196106\n",
      "Epoch 6746, Loss: 1.757559061050415, Final Batch Loss: 0.34775224328041077\n",
      "Epoch 6747, Loss: 1.6988901197910309, Final Batch Loss: 0.31393104791641235\n",
      "Epoch 6748, Loss: 1.526103600859642, Final Batch Loss: 0.23699414730072021\n",
      "Epoch 6749, Loss: 1.7499257922172546, Final Batch Loss: 0.41914400458335876\n",
      "Epoch 6750, Loss: 1.6144790649414062, Final Batch Loss: 0.3313220739364624\n",
      "Epoch 6751, Loss: 1.6513092815876007, Final Batch Loss: 0.2931809723377228\n",
      "Epoch 6752, Loss: 1.6588035225868225, Final Batch Loss: 0.271208792924881\n",
      "Epoch 6753, Loss: 1.70985946059227, Final Batch Loss: 0.39055123925209045\n",
      "Epoch 6754, Loss: 1.628069818019867, Final Batch Loss: 0.32185521721839905\n",
      "Epoch 6755, Loss: 1.9181284308433533, Final Batch Loss: 0.472744345664978\n",
      "Epoch 6756, Loss: 1.747427523136139, Final Batch Loss: 0.2967592477798462\n",
      "Epoch 6757, Loss: 1.6890400350093842, Final Batch Loss: 0.2934862971305847\n",
      "Epoch 6758, Loss: 1.5921487212181091, Final Batch Loss: 0.33446401357650757\n",
      "Epoch 6759, Loss: 1.5894849002361298, Final Batch Loss: 0.3381199240684509\n",
      "Epoch 6760, Loss: 1.6376297771930695, Final Batch Loss: 0.33726173639297485\n",
      "Epoch 6761, Loss: 1.5693557262420654, Final Batch Loss: 0.33504462242126465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6762, Loss: 1.5788449347019196, Final Batch Loss: 0.3405057191848755\n",
      "Epoch 6763, Loss: 1.5232145339250565, Final Batch Loss: 0.3728626072406769\n",
      "Epoch 6764, Loss: 1.8175083696842194, Final Batch Loss: 0.39137935638427734\n",
      "Epoch 6765, Loss: 1.5325618386268616, Final Batch Loss: 0.31908005475997925\n",
      "Epoch 6766, Loss: 1.6530945301055908, Final Batch Loss: 0.3553358018398285\n",
      "Epoch 6767, Loss: 1.6224119663238525, Final Batch Loss: 0.30634352564811707\n",
      "Epoch 6768, Loss: 1.4625497460365295, Final Batch Loss: 0.2782430648803711\n",
      "Epoch 6769, Loss: 1.6848349869251251, Final Batch Loss: 0.3197184205055237\n",
      "Epoch 6770, Loss: 1.6185005009174347, Final Batch Loss: 0.36212822794914246\n",
      "Epoch 6771, Loss: 1.5462671369314194, Final Batch Loss: 0.3010101914405823\n",
      "Epoch 6772, Loss: 1.4881555140018463, Final Batch Loss: 0.22484973073005676\n",
      "Epoch 6773, Loss: 1.564442217350006, Final Batch Loss: 0.280053973197937\n",
      "Epoch 6774, Loss: 1.8590781688690186, Final Batch Loss: 0.3951142430305481\n",
      "Epoch 6775, Loss: 1.8123912513256073, Final Batch Loss: 0.43545907735824585\n",
      "Epoch 6776, Loss: 1.8063198626041412, Final Batch Loss: 0.3420356512069702\n",
      "Epoch 6777, Loss: 1.786974549293518, Final Batch Loss: 0.3700007498264313\n",
      "Epoch 6778, Loss: 1.7527435719966888, Final Batch Loss: 0.3788243532180786\n",
      "Epoch 6779, Loss: 1.7947835624217987, Final Batch Loss: 0.34060803055763245\n",
      "Epoch 6780, Loss: 1.922043353319168, Final Batch Loss: 0.36445683240890503\n",
      "Epoch 6781, Loss: 1.6507858633995056, Final Batch Loss: 0.3603508174419403\n",
      "Epoch 6782, Loss: 1.5160498917102814, Final Batch Loss: 0.2753380835056305\n",
      "Epoch 6783, Loss: 1.595259666442871, Final Batch Loss: 0.27196431159973145\n",
      "Epoch 6784, Loss: 1.7600241899490356, Final Batch Loss: 0.26088589429855347\n",
      "Epoch 6785, Loss: 1.802204042673111, Final Batch Loss: 0.3344900608062744\n",
      "Epoch 6786, Loss: 1.711094856262207, Final Batch Loss: 0.35140833258628845\n",
      "Epoch 6787, Loss: 1.6379294395446777, Final Batch Loss: 0.3086282014846802\n",
      "Epoch 6788, Loss: 1.6776430904865265, Final Batch Loss: 0.30506637692451477\n",
      "Epoch 6789, Loss: 1.840868055820465, Final Batch Loss: 0.32592669129371643\n",
      "Epoch 6790, Loss: 1.6464711129665375, Final Batch Loss: 0.3886260688304901\n",
      "Epoch 6791, Loss: 1.694680780172348, Final Batch Loss: 0.3348003327846527\n",
      "Epoch 6792, Loss: 1.6166490763425827, Final Batch Loss: 0.374935507774353\n",
      "Epoch 6793, Loss: 1.7206706404685974, Final Batch Loss: 0.2572341859340668\n",
      "Epoch 6794, Loss: 1.853008657693863, Final Batch Loss: 0.38303276896476746\n",
      "Epoch 6795, Loss: 1.9146151542663574, Final Batch Loss: 0.3504962623119354\n",
      "Epoch 6796, Loss: 1.5256717652082443, Final Batch Loss: 0.2754235863685608\n",
      "Epoch 6797, Loss: 1.797437310218811, Final Batch Loss: 0.2624151110649109\n",
      "Epoch 6798, Loss: 1.5238490551710129, Final Batch Loss: 0.2835226058959961\n",
      "Epoch 6799, Loss: 1.5680206269025803, Final Batch Loss: 0.32280609011650085\n",
      "Epoch 6800, Loss: 1.6730973422527313, Final Batch Loss: 0.385834276676178\n",
      "Epoch 6801, Loss: 1.597216084599495, Final Batch Loss: 0.34211763739585876\n",
      "Epoch 6802, Loss: 1.6748708188533783, Final Batch Loss: 0.32590723037719727\n",
      "Epoch 6803, Loss: 1.5395228564739227, Final Batch Loss: 0.28757038712501526\n",
      "Epoch 6804, Loss: 1.5674084573984146, Final Batch Loss: 0.4444785416126251\n",
      "Epoch 6805, Loss: 1.396205186843872, Final Batch Loss: 0.21371790766716003\n",
      "Epoch 6806, Loss: 1.5675847828388214, Final Batch Loss: 0.3292711079120636\n",
      "Epoch 6807, Loss: 1.6302529871463776, Final Batch Loss: 0.2940560579299927\n",
      "Epoch 6808, Loss: 1.6988966763019562, Final Batch Loss: 0.37849804759025574\n",
      "Epoch 6809, Loss: 1.6938885152339935, Final Batch Loss: 0.3594290614128113\n",
      "Epoch 6810, Loss: 1.514658898115158, Final Batch Loss: 0.3006126284599304\n",
      "Epoch 6811, Loss: 1.5765436887741089, Final Batch Loss: 0.26686882972717285\n",
      "Epoch 6812, Loss: 1.671376794576645, Final Batch Loss: 0.3255518674850464\n",
      "Epoch 6813, Loss: 1.6336717307567596, Final Batch Loss: 0.4024215340614319\n",
      "Epoch 6814, Loss: 1.6566741317510605, Final Batch Loss: 0.3675905168056488\n",
      "Epoch 6815, Loss: 1.7597449123859406, Final Batch Loss: 0.4222487807273865\n",
      "Epoch 6816, Loss: 1.5552833080291748, Final Batch Loss: 0.3654302656650543\n",
      "Epoch 6817, Loss: 1.6824594140052795, Final Batch Loss: 0.31241199374198914\n",
      "Epoch 6818, Loss: 1.625434398651123, Final Batch Loss: 0.30330890417099\n",
      "Epoch 6819, Loss: 1.6989983022212982, Final Batch Loss: 0.37532293796539307\n",
      "Epoch 6820, Loss: 1.5663750767707825, Final Batch Loss: 0.2938072681427002\n",
      "Epoch 6821, Loss: 1.494181752204895, Final Batch Loss: 0.2805907428264618\n",
      "Epoch 6822, Loss: 1.6858105063438416, Final Batch Loss: 0.28460493683815\n",
      "Epoch 6823, Loss: 1.447304219007492, Final Batch Loss: 0.2614157795906067\n",
      "Epoch 6824, Loss: 1.6310584843158722, Final Batch Loss: 0.29151487350463867\n",
      "Epoch 6825, Loss: 1.7373576164245605, Final Batch Loss: 0.41807064414024353\n",
      "Epoch 6826, Loss: 1.6910356879234314, Final Batch Loss: 0.44792717695236206\n",
      "Epoch 6827, Loss: 1.7287465333938599, Final Batch Loss: 0.314818412065506\n",
      "Epoch 6828, Loss: 1.7798308432102203, Final Batch Loss: 0.3704739809036255\n",
      "Epoch 6829, Loss: 1.5271219909191132, Final Batch Loss: 0.3668529987335205\n",
      "Epoch 6830, Loss: 1.6667318046092987, Final Batch Loss: 0.36678844690322876\n",
      "Epoch 6831, Loss: 1.5695199519395828, Final Batch Loss: 0.2959382236003876\n",
      "Epoch 6832, Loss: 1.537179172039032, Final Batch Loss: 0.3132849633693695\n",
      "Epoch 6833, Loss: 1.6031089127063751, Final Batch Loss: 0.28893986344337463\n",
      "Epoch 6834, Loss: 1.6323201358318329, Final Batch Loss: 0.31382569670677185\n",
      "Epoch 6835, Loss: 1.5757357776165009, Final Batch Loss: 0.3974258601665497\n",
      "Epoch 6836, Loss: 1.6900204122066498, Final Batch Loss: 0.29458218812942505\n",
      "Epoch 6837, Loss: 1.8007976710796356, Final Batch Loss: 0.3183189630508423\n",
      "Epoch 6838, Loss: 1.7611389756202698, Final Batch Loss: 0.29576951265335083\n",
      "Epoch 6839, Loss: 1.5739223659038544, Final Batch Loss: 0.2658523619174957\n",
      "Epoch 6840, Loss: 1.5929487347602844, Final Batch Loss: 0.3672826290130615\n",
      "Epoch 6841, Loss: 1.5846996307373047, Final Batch Loss: 0.3426607549190521\n",
      "Epoch 6842, Loss: 1.7481828331947327, Final Batch Loss: 0.3624719977378845\n",
      "Epoch 6843, Loss: 1.5711554288864136, Final Batch Loss: 0.33051401376724243\n",
      "Epoch 6844, Loss: 1.7127206921577454, Final Batch Loss: 0.36381661891937256\n",
      "Epoch 6845, Loss: 1.6589819490909576, Final Batch Loss: 0.32693031430244446\n",
      "Epoch 6846, Loss: 1.5228145718574524, Final Batch Loss: 0.2641585171222687\n",
      "Epoch 6847, Loss: 1.728624165058136, Final Batch Loss: 0.40476930141448975\n",
      "Epoch 6848, Loss: 1.6713136434555054, Final Batch Loss: 0.23371511697769165\n",
      "Epoch 6849, Loss: 1.629426509141922, Final Batch Loss: 0.3493213355541229\n",
      "Epoch 6850, Loss: 1.602677434682846, Final Batch Loss: 0.36448273062705994\n",
      "Epoch 6851, Loss: 1.6465912461280823, Final Batch Loss: 0.2789212167263031\n",
      "Epoch 6852, Loss: 1.664872944355011, Final Batch Loss: 0.3776569962501526\n",
      "Epoch 6853, Loss: 1.579328328371048, Final Batch Loss: 0.35651886463165283\n",
      "Epoch 6854, Loss: 1.70834881067276, Final Batch Loss: 0.2615075707435608\n",
      "Epoch 6855, Loss: 1.5340725630521774, Final Batch Loss: 0.2653936743736267\n",
      "Epoch 6856, Loss: 1.580319583415985, Final Batch Loss: 0.31278878450393677\n",
      "Epoch 6857, Loss: 1.722867488861084, Final Batch Loss: 0.3506520688533783\n",
      "Epoch 6858, Loss: 1.5720222294330597, Final Batch Loss: 0.2351602017879486\n",
      "Epoch 6859, Loss: 1.5620217025279999, Final Batch Loss: 0.31603530049324036\n",
      "Epoch 6860, Loss: 1.5967597365379333, Final Batch Loss: 0.3051823079586029\n",
      "Epoch 6861, Loss: 1.5152978599071503, Final Batch Loss: 0.24863100051879883\n",
      "Epoch 6862, Loss: 1.551278680562973, Final Batch Loss: 0.33743366599082947\n",
      "Epoch 6863, Loss: 1.7603648602962494, Final Batch Loss: 0.26766330003738403\n",
      "Epoch 6864, Loss: 1.5824547559022903, Final Batch Loss: 0.37198346853256226\n",
      "Epoch 6865, Loss: 1.5180336833000183, Final Batch Loss: 0.27488580346107483\n",
      "Epoch 6866, Loss: 1.4517414271831512, Final Batch Loss: 0.2742924094200134\n",
      "Epoch 6867, Loss: 1.6447365880012512, Final Batch Loss: 0.30574139952659607\n",
      "Epoch 6868, Loss: 1.7036001682281494, Final Batch Loss: 0.3989132344722748\n",
      "Epoch 6869, Loss: 1.5271778404712677, Final Batch Loss: 0.3340300917625427\n",
      "Epoch 6870, Loss: 1.590355023741722, Final Batch Loss: 0.35785791277885437\n",
      "Epoch 6871, Loss: 1.5839010924100876, Final Batch Loss: 0.32280224561691284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6872, Loss: 1.5185065865516663, Final Batch Loss: 0.27207064628601074\n",
      "Epoch 6873, Loss: 1.7689822316169739, Final Batch Loss: 0.38872456550598145\n",
      "Epoch 6874, Loss: 1.5826161801815033, Final Batch Loss: 0.3237321972846985\n",
      "Epoch 6875, Loss: 1.4193681478500366, Final Batch Loss: 0.311306893825531\n",
      "Epoch 6876, Loss: 1.485504388809204, Final Batch Loss: 0.2737554609775543\n",
      "Epoch 6877, Loss: 1.5765990614891052, Final Batch Loss: 0.3050265908241272\n",
      "Epoch 6878, Loss: 1.6026298105716705, Final Batch Loss: 0.3165130615234375\n",
      "Epoch 6879, Loss: 1.693299800157547, Final Batch Loss: 0.3059207797050476\n",
      "Epoch 6880, Loss: 1.5852274894714355, Final Batch Loss: 0.29645389318466187\n",
      "Epoch 6881, Loss: 1.561269074678421, Final Batch Loss: 0.2600404620170593\n",
      "Epoch 6882, Loss: 1.7019563913345337, Final Batch Loss: 0.3243425786495209\n",
      "Epoch 6883, Loss: 1.5455763936042786, Final Batch Loss: 0.3428441882133484\n",
      "Epoch 6884, Loss: 1.5528845191001892, Final Batch Loss: 0.2865145206451416\n",
      "Epoch 6885, Loss: 1.4884146451950073, Final Batch Loss: 0.3561563193798065\n",
      "Epoch 6886, Loss: 1.6632406413555145, Final Batch Loss: 0.3603801131248474\n",
      "Epoch 6887, Loss: 1.6351830065250397, Final Batch Loss: 0.328593909740448\n",
      "Epoch 6888, Loss: 1.8490609228610992, Final Batch Loss: 0.30536723136901855\n",
      "Epoch 6889, Loss: 1.5631323456764221, Final Batch Loss: 0.3025766611099243\n",
      "Epoch 6890, Loss: 1.5607225000858307, Final Batch Loss: 0.28552427887916565\n",
      "Epoch 6891, Loss: 1.6457558423280716, Final Batch Loss: 0.21999917924404144\n",
      "Epoch 6892, Loss: 1.6352373659610748, Final Batch Loss: 0.33224326372146606\n",
      "Epoch 6893, Loss: 1.740979939699173, Final Batch Loss: 0.3289605677127838\n",
      "Epoch 6894, Loss: 1.6107778549194336, Final Batch Loss: 0.3050910532474518\n",
      "Epoch 6895, Loss: 1.6291636228561401, Final Batch Loss: 0.34685012698173523\n",
      "Epoch 6896, Loss: 1.6169648170471191, Final Batch Loss: 0.3925887644290924\n",
      "Epoch 6897, Loss: 1.5346285700798035, Final Batch Loss: 0.3397463858127594\n",
      "Epoch 6898, Loss: 1.6492786556482315, Final Batch Loss: 0.2745651602745056\n",
      "Epoch 6899, Loss: 1.7552577555179596, Final Batch Loss: 0.47049593925476074\n",
      "Epoch 6900, Loss: 1.799252212047577, Final Batch Loss: 0.40709155797958374\n",
      "Epoch 6901, Loss: 1.5727029740810394, Final Batch Loss: 0.34393641352653503\n",
      "Epoch 6902, Loss: 1.5836325883865356, Final Batch Loss: 0.27111488580703735\n",
      "Epoch 6903, Loss: 1.5878386795520782, Final Batch Loss: 0.3480026423931122\n",
      "Epoch 6904, Loss: 1.6764735579490662, Final Batch Loss: 0.32594746351242065\n",
      "Epoch 6905, Loss: 1.568387508392334, Final Batch Loss: 0.32368630170822144\n",
      "Epoch 6906, Loss: 1.5938661992549896, Final Batch Loss: 0.3035562336444855\n",
      "Epoch 6907, Loss: 1.8462371230125427, Final Batch Loss: 0.4553591012954712\n",
      "Epoch 6908, Loss: 1.5274508595466614, Final Batch Loss: 0.20778444409370422\n",
      "Epoch 6909, Loss: 1.6453368663787842, Final Batch Loss: 0.298525333404541\n",
      "Epoch 6910, Loss: 1.7130845487117767, Final Batch Loss: 0.298822820186615\n",
      "Epoch 6911, Loss: 1.50882887840271, Final Batch Loss: 0.33844730257987976\n",
      "Epoch 6912, Loss: 1.7473052144050598, Final Batch Loss: 0.31958112120628357\n",
      "Epoch 6913, Loss: 1.6976584494113922, Final Batch Loss: 0.45642417669296265\n",
      "Epoch 6914, Loss: 1.5738510191440582, Final Batch Loss: 0.23145252466201782\n",
      "Epoch 6915, Loss: 1.5583327114582062, Final Batch Loss: 0.26572123169898987\n",
      "Epoch 6916, Loss: 1.6697138249874115, Final Batch Loss: 0.2686183452606201\n",
      "Epoch 6917, Loss: 1.6579305231571198, Final Batch Loss: 0.442021906375885\n",
      "Epoch 6918, Loss: 1.507619857788086, Final Batch Loss: 0.3182736933231354\n",
      "Epoch 6919, Loss: 1.636918991804123, Final Batch Loss: 0.33535656332969666\n",
      "Epoch 6920, Loss: 1.6869741082191467, Final Batch Loss: 0.3634856343269348\n",
      "Epoch 6921, Loss: 1.7743285596370697, Final Batch Loss: 0.35384082794189453\n",
      "Epoch 6922, Loss: 1.668302983045578, Final Batch Loss: 0.4129302501678467\n",
      "Epoch 6923, Loss: 1.8453166782855988, Final Batch Loss: 0.36906683444976807\n",
      "Epoch 6924, Loss: 1.550346001982689, Final Batch Loss: 0.31667038798332214\n",
      "Epoch 6925, Loss: 1.752022236585617, Final Batch Loss: 0.3362976908683777\n",
      "Epoch 6926, Loss: 1.6084630191326141, Final Batch Loss: 0.3583933711051941\n",
      "Epoch 6927, Loss: 1.6616740971803665, Final Batch Loss: 0.4399109184741974\n",
      "Epoch 6928, Loss: 1.5657978504896164, Final Batch Loss: 0.24849934875965118\n",
      "Epoch 6929, Loss: 1.7954742908477783, Final Batch Loss: 0.4457206130027771\n",
      "Epoch 6930, Loss: 1.6229817271232605, Final Batch Loss: 0.31404781341552734\n",
      "Epoch 6931, Loss: 1.6535826325416565, Final Batch Loss: 0.29794150590896606\n",
      "Epoch 6932, Loss: 1.7656930088996887, Final Batch Loss: 0.37847644090652466\n",
      "Epoch 6933, Loss: 1.6969634890556335, Final Batch Loss: 0.3070254921913147\n",
      "Epoch 6934, Loss: 1.6647508442401886, Final Batch Loss: 0.28063273429870605\n",
      "Epoch 6935, Loss: 1.8513569235801697, Final Batch Loss: 0.3911782503128052\n",
      "Epoch 6936, Loss: 1.5890054404735565, Final Batch Loss: 0.2690046429634094\n",
      "Epoch 6937, Loss: 1.6726113259792328, Final Batch Loss: 0.4734638035297394\n",
      "Epoch 6938, Loss: 1.6047848463058472, Final Batch Loss: 0.3642047345638275\n",
      "Epoch 6939, Loss: 1.6892029643058777, Final Batch Loss: 0.33306971192359924\n",
      "Epoch 6940, Loss: 1.6882510483264923, Final Batch Loss: 0.3694972097873688\n",
      "Epoch 6941, Loss: 1.5555401742458344, Final Batch Loss: 0.32066676020622253\n",
      "Epoch 6942, Loss: 1.6504757702350616, Final Batch Loss: 0.3585284650325775\n",
      "Epoch 6943, Loss: 1.585949957370758, Final Batch Loss: 0.2998979389667511\n",
      "Epoch 6944, Loss: 1.5477959215641022, Final Batch Loss: 0.2729680836200714\n",
      "Epoch 6945, Loss: 1.6719567477703094, Final Batch Loss: 0.34577080607414246\n",
      "Epoch 6946, Loss: 1.6823601126670837, Final Batch Loss: 0.338189959526062\n",
      "Epoch 6947, Loss: 1.4851146936416626, Final Batch Loss: 0.30688610672950745\n",
      "Epoch 6948, Loss: 1.6731724292039871, Final Batch Loss: 0.40978947281837463\n",
      "Epoch 6949, Loss: 1.5073061287403107, Final Batch Loss: 0.2787701189517975\n",
      "Epoch 6950, Loss: 1.526210367679596, Final Batch Loss: 0.30394142866134644\n",
      "Epoch 6951, Loss: 1.7259257584810257, Final Batch Loss: 0.31410712003707886\n",
      "Epoch 6952, Loss: 1.5985315293073654, Final Batch Loss: 0.2745438814163208\n",
      "Epoch 6953, Loss: 1.5069465339183807, Final Batch Loss: 0.2744077742099762\n",
      "Epoch 6954, Loss: 1.584165245294571, Final Batch Loss: 0.2806302607059479\n",
      "Epoch 6955, Loss: 1.5728243589401245, Final Batch Loss: 0.3198705315589905\n",
      "Epoch 6956, Loss: 1.6194445490837097, Final Batch Loss: 0.3425463140010834\n",
      "Epoch 6957, Loss: 1.5633819550275803, Final Batch Loss: 0.32752302289009094\n",
      "Epoch 6958, Loss: 1.5784586668014526, Final Batch Loss: 0.3141864538192749\n",
      "Epoch 6959, Loss: 1.5886994898319244, Final Batch Loss: 0.30436939001083374\n",
      "Epoch 6960, Loss: 1.6273365169763565, Final Batch Loss: 0.38576602935791016\n",
      "Epoch 6961, Loss: 1.6335672438144684, Final Batch Loss: 0.3761438727378845\n",
      "Epoch 6962, Loss: 1.806501865386963, Final Batch Loss: 0.4089381694793701\n",
      "Epoch 6963, Loss: 1.656996250152588, Final Batch Loss: 0.351154625415802\n",
      "Epoch 6964, Loss: 1.7100100815296173, Final Batch Loss: 0.36747434735298157\n",
      "Epoch 6965, Loss: 1.7283475995063782, Final Batch Loss: 0.31706300377845764\n",
      "Epoch 6966, Loss: 1.6868311762809753, Final Batch Loss: 0.2912343740463257\n",
      "Epoch 6967, Loss: 1.6158139109611511, Final Batch Loss: 0.3228718340396881\n",
      "Epoch 6968, Loss: 1.5683353245258331, Final Batch Loss: 0.3119870126247406\n",
      "Epoch 6969, Loss: 1.5824333131313324, Final Batch Loss: 0.3563154935836792\n",
      "Epoch 6970, Loss: 1.6715724468231201, Final Batch Loss: 0.32614395022392273\n",
      "Epoch 6971, Loss: 1.7183400690555573, Final Batch Loss: 0.38558530807495117\n",
      "Epoch 6972, Loss: 1.6158889383077621, Final Batch Loss: 0.32187679409980774\n",
      "Epoch 6973, Loss: 1.631643682718277, Final Batch Loss: 0.3516186475753784\n",
      "Epoch 6974, Loss: 1.7893849313259125, Final Batch Loss: 0.37697818875312805\n",
      "Epoch 6975, Loss: 1.7875246107578278, Final Batch Loss: 0.366977334022522\n",
      "Epoch 6976, Loss: 1.6114411354064941, Final Batch Loss: 0.36837542057037354\n",
      "Epoch 6977, Loss: 1.5995504260063171, Final Batch Loss: 0.3527683913707733\n",
      "Epoch 6978, Loss: 1.5648943781852722, Final Batch Loss: 0.2994482219219208\n",
      "Epoch 6979, Loss: 1.6912096738815308, Final Batch Loss: 0.32733240723609924\n",
      "Epoch 6980, Loss: 1.585978090763092, Final Batch Loss: 0.290263831615448\n",
      "Epoch 6981, Loss: 1.5891737639904022, Final Batch Loss: 0.3456088602542877\n",
      "Epoch 6982, Loss: 1.594293624162674, Final Batch Loss: 0.2777603566646576\n",
      "Epoch 6983, Loss: 1.5113833993673325, Final Batch Loss: 0.36026516556739807\n",
      "Epoch 6984, Loss: 1.541617527604103, Final Batch Loss: 0.22837992012500763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6985, Loss: 1.4862850904464722, Final Batch Loss: 0.27653589844703674\n",
      "Epoch 6986, Loss: 1.545218139886856, Final Batch Loss: 0.250889390707016\n",
      "Epoch 6987, Loss: 1.842942714691162, Final Batch Loss: 0.5221807360649109\n",
      "Epoch 6988, Loss: 1.6658715903759003, Final Batch Loss: 0.3468872606754303\n",
      "Epoch 6989, Loss: 1.5920735895633698, Final Batch Loss: 0.30974671244621277\n",
      "Epoch 6990, Loss: 1.726595789194107, Final Batch Loss: 0.2596084773540497\n",
      "Epoch 6991, Loss: 1.65511953830719, Final Batch Loss: 0.32098713517189026\n",
      "Epoch 6992, Loss: 1.6266534924507141, Final Batch Loss: 0.31876620650291443\n",
      "Epoch 6993, Loss: 1.6357618570327759, Final Batch Loss: 0.39411988854408264\n",
      "Epoch 6994, Loss: 1.6959164440631866, Final Batch Loss: 0.3002450466156006\n",
      "Epoch 6995, Loss: 1.6770785748958588, Final Batch Loss: 0.38182199001312256\n",
      "Epoch 6996, Loss: 1.6038898527622223, Final Batch Loss: 0.31106817722320557\n",
      "Epoch 6997, Loss: 1.7933639287948608, Final Batch Loss: 0.4241999685764313\n",
      "Epoch 6998, Loss: 1.724033385515213, Final Batch Loss: 0.3668052852153778\n",
      "Epoch 6999, Loss: 1.5498752295970917, Final Batch Loss: 0.29247117042541504\n",
      "Epoch 7000, Loss: 1.5766546577215195, Final Batch Loss: 0.37160825729370117\n",
      "Epoch 7001, Loss: 1.5066328644752502, Final Batch Loss: 0.3241616189479828\n",
      "Epoch 7002, Loss: 1.5562217831611633, Final Batch Loss: 0.2545294463634491\n",
      "Epoch 7003, Loss: 1.6395486891269684, Final Batch Loss: 0.3309019207954407\n",
      "Epoch 7004, Loss: 1.6582639068365097, Final Batch Loss: 0.22517965734004974\n",
      "Epoch 7005, Loss: 1.4300051033496857, Final Batch Loss: 0.23549139499664307\n",
      "Epoch 7006, Loss: 1.764435350894928, Final Batch Loss: 0.3280978202819824\n",
      "Epoch 7007, Loss: 1.5220878571271896, Final Batch Loss: 0.3627057671546936\n",
      "Epoch 7008, Loss: 1.5636116117238998, Final Batch Loss: 0.28556007146835327\n",
      "Epoch 7009, Loss: 1.519877403974533, Final Batch Loss: 0.27445024251937866\n",
      "Epoch 7010, Loss: 1.72903111577034, Final Batch Loss: 0.2667926251888275\n",
      "Epoch 7011, Loss: 1.6097496300935745, Final Batch Loss: 0.32763364911079407\n",
      "Epoch 7012, Loss: 1.6953642070293427, Final Batch Loss: 0.38873231410980225\n",
      "Epoch 7013, Loss: 1.5815190970897675, Final Batch Loss: 0.2438228726387024\n",
      "Epoch 7014, Loss: 1.7721800208091736, Final Batch Loss: 0.454394668340683\n",
      "Epoch 7015, Loss: 1.7200840711593628, Final Batch Loss: 0.3496301770210266\n",
      "Epoch 7016, Loss: 1.8872758150100708, Final Batch Loss: 0.3932497799396515\n",
      "Epoch 7017, Loss: 1.7480591237545013, Final Batch Loss: 0.40038397908210754\n",
      "Epoch 7018, Loss: 1.7350183725357056, Final Batch Loss: 0.3327644169330597\n",
      "Epoch 7019, Loss: 1.452752247452736, Final Batch Loss: 0.32603806257247925\n",
      "Epoch 7020, Loss: 1.74310901761055, Final Batch Loss: 0.3765174448490143\n",
      "Epoch 7021, Loss: 1.658917397260666, Final Batch Loss: 0.32860058546066284\n",
      "Epoch 7022, Loss: 1.8386418521404266, Final Batch Loss: 0.33846375346183777\n",
      "Epoch 7023, Loss: 1.622158169746399, Final Batch Loss: 0.3104023039340973\n",
      "Epoch 7024, Loss: 1.6081882417201996, Final Batch Loss: 0.3374316990375519\n",
      "Epoch 7025, Loss: 1.7108781039714813, Final Batch Loss: 0.30566179752349854\n",
      "Epoch 7026, Loss: 1.6081515848636627, Final Batch Loss: 0.38490456342697144\n",
      "Epoch 7027, Loss: 1.6099395453929901, Final Batch Loss: 0.35113680362701416\n",
      "Epoch 7028, Loss: 1.6338517665863037, Final Batch Loss: 0.3751295804977417\n",
      "Epoch 7029, Loss: 1.5597944259643555, Final Batch Loss: 0.43419384956359863\n",
      "Epoch 7030, Loss: 1.578875869512558, Final Batch Loss: 0.3102383017539978\n",
      "Epoch 7031, Loss: 2.0653532445430756, Final Batch Loss: 0.2732892632484436\n",
      "Epoch 7032, Loss: 1.759002149105072, Final Batch Loss: 0.3482803404331207\n",
      "Epoch 7033, Loss: 1.595680147409439, Final Batch Loss: 0.34124988317489624\n",
      "Epoch 7034, Loss: 1.6623444855213165, Final Batch Loss: 0.3056531548500061\n",
      "Epoch 7035, Loss: 1.573809415102005, Final Batch Loss: 0.3210339844226837\n",
      "Epoch 7036, Loss: 1.6423413753509521, Final Batch Loss: 0.26903748512268066\n",
      "Epoch 7037, Loss: 1.592505156993866, Final Batch Loss: 0.34161585569381714\n",
      "Epoch 7038, Loss: 1.7697852551937103, Final Batch Loss: 0.34449735283851624\n",
      "Epoch 7039, Loss: 1.6002004146575928, Final Batch Loss: 0.3407262861728668\n",
      "Epoch 7040, Loss: 1.674035221338272, Final Batch Loss: 0.4186372458934784\n",
      "Epoch 7041, Loss: 1.4981242716312408, Final Batch Loss: 0.2603027820587158\n",
      "Epoch 7042, Loss: 1.518954336643219, Final Batch Loss: 0.2856581509113312\n",
      "Epoch 7043, Loss: 1.683505266904831, Final Batch Loss: 0.3191956579685211\n",
      "Epoch 7044, Loss: 1.4649131745100021, Final Batch Loss: 0.3137383460998535\n",
      "Epoch 7045, Loss: 1.5340748876333237, Final Batch Loss: 0.29938578605651855\n",
      "Epoch 7046, Loss: 1.569272518157959, Final Batch Loss: 0.2950117886066437\n",
      "Epoch 7047, Loss: 1.6068543195724487, Final Batch Loss: 0.40771713852882385\n",
      "Epoch 7048, Loss: 1.612703263759613, Final Batch Loss: 0.34914618730545044\n",
      "Epoch 7049, Loss: 1.6371155381202698, Final Batch Loss: 0.40351927280426025\n",
      "Epoch 7050, Loss: 1.7003441452980042, Final Batch Loss: 0.41667717695236206\n",
      "Epoch 7051, Loss: 1.6812511384487152, Final Batch Loss: 0.286751925945282\n",
      "Epoch 7052, Loss: 1.5653547048568726, Final Batch Loss: 0.33920469880104065\n",
      "Epoch 7053, Loss: 1.5882899165153503, Final Batch Loss: 0.2514570355415344\n",
      "Epoch 7054, Loss: 1.617876648902893, Final Batch Loss: 0.2529781758785248\n",
      "Epoch 7055, Loss: 1.4857613444328308, Final Batch Loss: 0.37069451808929443\n",
      "Epoch 7056, Loss: 1.7461641728878021, Final Batch Loss: 0.31447118520736694\n",
      "Epoch 7057, Loss: 1.599996641278267, Final Batch Loss: 0.29498839378356934\n",
      "Epoch 7058, Loss: 1.5965401232242584, Final Batch Loss: 0.29438161849975586\n",
      "Epoch 7059, Loss: 1.5984817445278168, Final Batch Loss: 0.328946590423584\n",
      "Epoch 7060, Loss: 1.534528911113739, Final Batch Loss: 0.34840336441993713\n",
      "Epoch 7061, Loss: 1.5709783732891083, Final Batch Loss: 0.34184637665748596\n",
      "Epoch 7062, Loss: 1.506202608346939, Final Batch Loss: 0.28336986899375916\n",
      "Epoch 7063, Loss: 1.5963615775108337, Final Batch Loss: 0.22184684872627258\n",
      "Epoch 7064, Loss: 1.486658826470375, Final Batch Loss: 0.30277782678604126\n",
      "Epoch 7065, Loss: 1.8086926341056824, Final Batch Loss: 0.44127553701400757\n",
      "Epoch 7066, Loss: 1.6889157891273499, Final Batch Loss: 0.292402982711792\n",
      "Epoch 7067, Loss: 1.5210621058940887, Final Batch Loss: 0.340996116399765\n",
      "Epoch 7068, Loss: 1.6030852794647217, Final Batch Loss: 0.42088088393211365\n",
      "Epoch 7069, Loss: 1.6782940924167633, Final Batch Loss: 0.458596795797348\n",
      "Epoch 7070, Loss: 1.7122799158096313, Final Batch Loss: 0.37248730659484863\n",
      "Epoch 7071, Loss: 1.5203925669193268, Final Batch Loss: 0.2664521634578705\n",
      "Epoch 7072, Loss: 1.6356151401996613, Final Batch Loss: 0.3728405237197876\n",
      "Epoch 7073, Loss: 1.7321163713932037, Final Batch Loss: 0.20832005143165588\n",
      "Epoch 7074, Loss: 1.6224663257598877, Final Batch Loss: 0.30468863248825073\n",
      "Epoch 7075, Loss: 1.5399798303842545, Final Batch Loss: 0.29845333099365234\n",
      "Epoch 7076, Loss: 1.5252735614776611, Final Batch Loss: 0.34307917952537537\n",
      "Epoch 7077, Loss: 1.7284037470817566, Final Batch Loss: 0.2667296528816223\n",
      "Epoch 7078, Loss: 1.6622258126735687, Final Batch Loss: 0.2803807258605957\n",
      "Epoch 7079, Loss: 1.5510957539081573, Final Batch Loss: 0.41688042879104614\n",
      "Epoch 7080, Loss: 1.8141641914844513, Final Batch Loss: 0.35817891359329224\n",
      "Epoch 7081, Loss: 1.7369698286056519, Final Batch Loss: 0.3128846287727356\n",
      "Epoch 7082, Loss: 1.5607241839170456, Final Batch Loss: 0.4195931851863861\n",
      "Epoch 7083, Loss: 1.6296151876449585, Final Batch Loss: 0.3876931965351105\n",
      "Epoch 7084, Loss: 1.6190036237239838, Final Batch Loss: 0.2443738877773285\n",
      "Epoch 7085, Loss: 1.6959411948919296, Final Batch Loss: 0.4638681411743164\n",
      "Epoch 7086, Loss: 1.6333284974098206, Final Batch Loss: 0.33548858761787415\n",
      "Epoch 7087, Loss: 1.6378679871559143, Final Batch Loss: 0.2856747806072235\n",
      "Epoch 7088, Loss: 1.6787342727184296, Final Batch Loss: 0.25646087527275085\n",
      "Epoch 7089, Loss: 1.9829373955726624, Final Batch Loss: 0.3797700107097626\n",
      "Epoch 7090, Loss: 1.679078221321106, Final Batch Loss: 0.33486467599868774\n",
      "Epoch 7091, Loss: 1.7396263480186462, Final Batch Loss: 0.41031014919281006\n",
      "Epoch 7092, Loss: 1.6648065894842148, Final Batch Loss: 0.3415776193141937\n",
      "Epoch 7093, Loss: 1.7089344561100006, Final Batch Loss: 0.29153141379356384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7094, Loss: 1.6233860552310944, Final Batch Loss: 0.2902008295059204\n",
      "Epoch 7095, Loss: 1.7352189123630524, Final Batch Loss: 0.3774230480194092\n",
      "Epoch 7096, Loss: 1.713057041168213, Final Batch Loss: 0.3264218270778656\n",
      "Epoch 7097, Loss: 1.6714014112949371, Final Batch Loss: 0.3465871810913086\n",
      "Epoch 7098, Loss: 1.5866968631744385, Final Batch Loss: 0.296328067779541\n",
      "Epoch 7099, Loss: 1.5355514734983444, Final Batch Loss: 0.3755209445953369\n",
      "Epoch 7100, Loss: 1.6131525039672852, Final Batch Loss: 0.2772896885871887\n",
      "Epoch 7101, Loss: 1.582241028547287, Final Batch Loss: 0.2998253107070923\n",
      "Epoch 7102, Loss: 1.7089681029319763, Final Batch Loss: 0.3208198845386505\n",
      "Epoch 7103, Loss: 1.5802728235721588, Final Batch Loss: 0.32834094762802124\n",
      "Epoch 7104, Loss: 1.697683870792389, Final Batch Loss: 0.34229451417922974\n",
      "Epoch 7105, Loss: 1.6481459438800812, Final Batch Loss: 0.3274984359741211\n",
      "Epoch 7106, Loss: 1.7004454135894775, Final Batch Loss: 0.31681764125823975\n",
      "Epoch 7107, Loss: 1.7283305525779724, Final Batch Loss: 0.44789496064186096\n",
      "Epoch 7108, Loss: 1.7056698203086853, Final Batch Loss: 0.3726986050605774\n",
      "Epoch 7109, Loss: 1.7990723848342896, Final Batch Loss: 0.2921332120895386\n",
      "Epoch 7110, Loss: 1.589423730969429, Final Batch Loss: 0.24994446337223053\n",
      "Epoch 7111, Loss: 1.4985427260398865, Final Batch Loss: 0.3450191020965576\n",
      "Epoch 7112, Loss: 1.419856607913971, Final Batch Loss: 0.2609337568283081\n",
      "Epoch 7113, Loss: 1.7431261837482452, Final Batch Loss: 0.38360151648521423\n",
      "Epoch 7114, Loss: 1.6803927421569824, Final Batch Loss: 0.38802677392959595\n",
      "Epoch 7115, Loss: 1.650357186794281, Final Batch Loss: 0.2808725833892822\n",
      "Epoch 7116, Loss: 1.6724834442138672, Final Batch Loss: 0.3104725778102875\n",
      "Epoch 7117, Loss: 1.5931003093719482, Final Batch Loss: 0.3311169445514679\n",
      "Epoch 7118, Loss: 1.6918735355138779, Final Batch Loss: 0.29411935806274414\n",
      "Epoch 7119, Loss: 1.6671508252620697, Final Batch Loss: 0.29627957940101624\n",
      "Epoch 7120, Loss: 1.474203810095787, Final Batch Loss: 0.22508405148983002\n",
      "Epoch 7121, Loss: 1.6966941356658936, Final Batch Loss: 0.2909032702445984\n",
      "Epoch 7122, Loss: 1.664557933807373, Final Batch Loss: 0.3579919636249542\n",
      "Epoch 7123, Loss: 1.6330133080482483, Final Batch Loss: 0.27261319756507874\n",
      "Epoch 7124, Loss: 1.716466248035431, Final Batch Loss: 0.42138510942459106\n",
      "Epoch 7125, Loss: 1.758771151304245, Final Batch Loss: 0.26598334312438965\n",
      "Epoch 7126, Loss: 1.618699312210083, Final Batch Loss: 0.3201182186603546\n",
      "Epoch 7127, Loss: 1.6687577366828918, Final Batch Loss: 0.30289971828460693\n",
      "Epoch 7128, Loss: 1.6081036925315857, Final Batch Loss: 0.2289814054965973\n",
      "Epoch 7129, Loss: 1.5316828489303589, Final Batch Loss: 0.3538482189178467\n",
      "Epoch 7130, Loss: 1.5616795420646667, Final Batch Loss: 0.29191166162490845\n",
      "Epoch 7131, Loss: 1.7281805574893951, Final Batch Loss: 0.4135212302207947\n",
      "Epoch 7132, Loss: 1.6527888178825378, Final Batch Loss: 0.27376553416252136\n",
      "Epoch 7133, Loss: 1.7312069833278656, Final Batch Loss: 0.33535054326057434\n",
      "Epoch 7134, Loss: 1.5405367016792297, Final Batch Loss: 0.29839858412742615\n",
      "Epoch 7135, Loss: 1.6311893165111542, Final Batch Loss: 0.3465465009212494\n",
      "Epoch 7136, Loss: 1.6081989407539368, Final Batch Loss: 0.2751671075820923\n",
      "Epoch 7137, Loss: 1.4911041557788849, Final Batch Loss: 0.3050621747970581\n",
      "Epoch 7138, Loss: 1.6210036128759384, Final Batch Loss: 0.3507348895072937\n",
      "Epoch 7139, Loss: 1.4828788340091705, Final Batch Loss: 0.296167254447937\n",
      "Epoch 7140, Loss: 1.5939654111862183, Final Batch Loss: 0.2615852355957031\n",
      "Epoch 7141, Loss: 1.6805575489997864, Final Batch Loss: 0.34197720885276794\n",
      "Epoch 7142, Loss: 1.6379764080047607, Final Batch Loss: 0.3545924723148346\n",
      "Epoch 7143, Loss: 1.7700987756252289, Final Batch Loss: 0.4257182478904724\n",
      "Epoch 7144, Loss: 1.6110267639160156, Final Batch Loss: 0.37404999136924744\n",
      "Epoch 7145, Loss: 1.7450009286403656, Final Batch Loss: 0.36458078026771545\n",
      "Epoch 7146, Loss: 1.4981859624385834, Final Batch Loss: 0.3131677210330963\n",
      "Epoch 7147, Loss: 1.5536808967590332, Final Batch Loss: 0.34290000796318054\n",
      "Epoch 7148, Loss: 1.6011741757392883, Final Batch Loss: 0.3741932809352875\n",
      "Epoch 7149, Loss: 1.5599572658538818, Final Batch Loss: 0.26610586047172546\n",
      "Epoch 7150, Loss: 1.5242175608873367, Final Batch Loss: 0.39516815543174744\n",
      "Epoch 7151, Loss: 1.709776222705841, Final Batch Loss: 0.27905210852622986\n",
      "Epoch 7152, Loss: 1.6698573231697083, Final Batch Loss: 0.39820507168769836\n",
      "Epoch 7153, Loss: 1.630283772945404, Final Batch Loss: 0.2626712918281555\n",
      "Epoch 7154, Loss: 1.5051385462284088, Final Batch Loss: 0.26851120591163635\n",
      "Epoch 7155, Loss: 1.4994609653949738, Final Batch Loss: 0.35428664088249207\n",
      "Epoch 7156, Loss: 1.5142757892608643, Final Batch Loss: 0.29221150279045105\n",
      "Epoch 7157, Loss: 1.5095281451940536, Final Batch Loss: 0.23478339612483978\n",
      "Epoch 7158, Loss: 1.6030795574188232, Final Batch Loss: 0.2980874478816986\n",
      "Epoch 7159, Loss: 1.775230586528778, Final Batch Loss: 0.321718692779541\n",
      "Epoch 7160, Loss: 1.8077691495418549, Final Batch Loss: 0.38246577978134155\n",
      "Epoch 7161, Loss: 1.6293297111988068, Final Batch Loss: 0.32779407501220703\n",
      "Epoch 7162, Loss: 1.4504478871822357, Final Batch Loss: 0.338779479265213\n",
      "Epoch 7163, Loss: 1.766714632511139, Final Batch Loss: 0.4852971136569977\n",
      "Epoch 7164, Loss: 1.48143470287323, Final Batch Loss: 0.3627760112285614\n",
      "Epoch 7165, Loss: 1.6375592052936554, Final Batch Loss: 0.3435889482498169\n",
      "Epoch 7166, Loss: 1.5767085552215576, Final Batch Loss: 0.27973172068595886\n",
      "Epoch 7167, Loss: 1.476401686668396, Final Batch Loss: 0.26945313811302185\n",
      "Epoch 7168, Loss: 1.7404611706733704, Final Batch Loss: 0.2787180542945862\n",
      "Epoch 7169, Loss: 1.4428860545158386, Final Batch Loss: 0.25741124153137207\n",
      "Epoch 7170, Loss: 1.4642718732357025, Final Batch Loss: 0.2187948226928711\n",
      "Epoch 7171, Loss: 1.6473886668682098, Final Batch Loss: 0.2977219223976135\n",
      "Epoch 7172, Loss: 1.5606649219989777, Final Batch Loss: 0.3319963812828064\n",
      "Epoch 7173, Loss: 1.5622521340847015, Final Batch Loss: 0.26046818494796753\n",
      "Epoch 7174, Loss: 1.6048961877822876, Final Batch Loss: 0.34446144104003906\n",
      "Epoch 7175, Loss: 1.6472133100032806, Final Batch Loss: 0.3214908242225647\n",
      "Epoch 7176, Loss: 1.5816251635551453, Final Batch Loss: 0.3357660174369812\n",
      "Epoch 7177, Loss: 1.742485523223877, Final Batch Loss: 0.2907811999320984\n",
      "Epoch 7178, Loss: 1.6199005842208862, Final Batch Loss: 0.3367288410663605\n",
      "Epoch 7179, Loss: 1.5267507880926132, Final Batch Loss: 0.3249830901622772\n",
      "Epoch 7180, Loss: 1.6645746231079102, Final Batch Loss: 0.2793717086315155\n",
      "Epoch 7181, Loss: 1.6903463304042816, Final Batch Loss: 0.4028971791267395\n",
      "Epoch 7182, Loss: 1.6052464544773102, Final Batch Loss: 0.43241021037101746\n",
      "Epoch 7183, Loss: 1.8052311539649963, Final Batch Loss: 0.2896772027015686\n",
      "Epoch 7184, Loss: 1.6989363729953766, Final Batch Loss: 0.3103044331073761\n",
      "Epoch 7185, Loss: 1.5942196249961853, Final Batch Loss: 0.323753297328949\n",
      "Epoch 7186, Loss: 1.8352297246456146, Final Batch Loss: 0.36772507429122925\n",
      "Epoch 7187, Loss: 1.5694872438907623, Final Batch Loss: 0.3715119957923889\n",
      "Epoch 7188, Loss: 1.564269557595253, Final Batch Loss: 0.25713643431663513\n",
      "Epoch 7189, Loss: 1.676046445965767, Final Batch Loss: 0.3414348065853119\n",
      "Epoch 7190, Loss: 1.6666482985019684, Final Batch Loss: 0.23046091198921204\n",
      "Epoch 7191, Loss: 1.5837422311306, Final Batch Loss: 0.27009403705596924\n",
      "Epoch 7192, Loss: 1.5106617212295532, Final Batch Loss: 0.26661452651023865\n",
      "Epoch 7193, Loss: 1.7737754583358765, Final Batch Loss: 0.38768452405929565\n",
      "Epoch 7194, Loss: 1.6966951787471771, Final Batch Loss: 0.3507816195487976\n",
      "Epoch 7195, Loss: 1.652567833662033, Final Batch Loss: 0.37746384739875793\n",
      "Epoch 7196, Loss: 1.7141337990760803, Final Batch Loss: 0.37646999955177307\n",
      "Epoch 7197, Loss: 1.6812490820884705, Final Batch Loss: 0.31142324209213257\n",
      "Epoch 7198, Loss: 1.8171616196632385, Final Batch Loss: 0.32839488983154297\n",
      "Epoch 7199, Loss: 1.6874755024909973, Final Batch Loss: 0.3754754364490509\n",
      "Epoch 7200, Loss: 1.5552119016647339, Final Batch Loss: 0.27846240997314453\n",
      "Epoch 7201, Loss: 1.7064268290996552, Final Batch Loss: 0.2611605226993561\n",
      "Epoch 7202, Loss: 1.6556750535964966, Final Batch Loss: 0.43230944871902466\n",
      "Epoch 7203, Loss: 1.5153341889381409, Final Batch Loss: 0.2197677493095398\n",
      "Epoch 7204, Loss: 1.5561365485191345, Final Batch Loss: 0.37483707070350647\n",
      "Epoch 7205, Loss: 1.538978099822998, Final Batch Loss: 0.4051949977874756\n",
      "Epoch 7206, Loss: 1.6377185881137848, Final Batch Loss: 0.37270602583885193\n",
      "Epoch 7207, Loss: 1.5555111169815063, Final Batch Loss: 0.32730576395988464\n",
      "Epoch 7208, Loss: 1.5516453087329865, Final Batch Loss: 0.33664417266845703\n",
      "Epoch 7209, Loss: 1.517466425895691, Final Batch Loss: 0.30521702766418457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7210, Loss: 1.7940165400505066, Final Batch Loss: 0.3204159140586853\n",
      "Epoch 7211, Loss: 1.8099472224712372, Final Batch Loss: 0.33623242378234863\n",
      "Epoch 7212, Loss: 1.4524375200271606, Final Batch Loss: 0.2976800799369812\n",
      "Epoch 7213, Loss: 1.5182016491889954, Final Batch Loss: 0.28702548146247864\n",
      "Epoch 7214, Loss: 1.6344954669475555, Final Batch Loss: 0.35592901706695557\n",
      "Epoch 7215, Loss: 1.6327647268772125, Final Batch Loss: 0.35395076870918274\n",
      "Epoch 7216, Loss: 1.5695189833641052, Final Batch Loss: 0.30261099338531494\n",
      "Epoch 7217, Loss: 1.6409631073474884, Final Batch Loss: 0.3381268084049225\n",
      "Epoch 7218, Loss: 1.6660723090171814, Final Batch Loss: 0.3285170793533325\n",
      "Epoch 7219, Loss: 1.4476156681776047, Final Batch Loss: 0.2475397139787674\n",
      "Epoch 7220, Loss: 1.4957085251808167, Final Batch Loss: 0.27686819434165955\n",
      "Epoch 7221, Loss: 1.773669809103012, Final Batch Loss: 0.3594565689563751\n",
      "Epoch 7222, Loss: 1.3860109895467758, Final Batch Loss: 0.30874431133270264\n",
      "Epoch 7223, Loss: 1.5791142284870148, Final Batch Loss: 0.39554929733276367\n",
      "Epoch 7224, Loss: 1.6876217424869537, Final Batch Loss: 0.33249786496162415\n",
      "Epoch 7225, Loss: 1.4995595663785934, Final Batch Loss: 0.24935851991176605\n",
      "Epoch 7226, Loss: 1.6059247553348541, Final Batch Loss: 0.32934147119522095\n",
      "Epoch 7227, Loss: 1.5676281154155731, Final Batch Loss: 0.31781473755836487\n",
      "Epoch 7228, Loss: 1.7259999364614487, Final Batch Loss: 0.37243592739105225\n",
      "Epoch 7229, Loss: 1.5909741967916489, Final Batch Loss: 0.3195186257362366\n",
      "Epoch 7230, Loss: 1.6558910012245178, Final Batch Loss: 0.3099813461303711\n",
      "Epoch 7231, Loss: 1.5260789841413498, Final Batch Loss: 0.22286055982112885\n",
      "Epoch 7232, Loss: 1.7353333830833435, Final Batch Loss: 0.3973960280418396\n",
      "Epoch 7233, Loss: 1.5774915218353271, Final Batch Loss: 0.2828431725502014\n",
      "Epoch 7234, Loss: 1.6759204268455505, Final Batch Loss: 0.3717418909072876\n",
      "Epoch 7235, Loss: 1.673179805278778, Final Batch Loss: 0.30505019426345825\n",
      "Epoch 7236, Loss: 1.6971661746501923, Final Batch Loss: 0.5863805413246155\n",
      "Epoch 7237, Loss: 1.551418125629425, Final Batch Loss: 0.29793035984039307\n",
      "Epoch 7238, Loss: 1.7263467609882355, Final Batch Loss: 0.3437289297580719\n",
      "Epoch 7239, Loss: 1.4349872022867203, Final Batch Loss: 0.2946268916130066\n",
      "Epoch 7240, Loss: 1.7455672025680542, Final Batch Loss: 0.339782178401947\n",
      "Epoch 7241, Loss: 1.4987571239471436, Final Batch Loss: 0.34372299909591675\n",
      "Epoch 7242, Loss: 1.6767483055591583, Final Batch Loss: 0.40852752327919006\n",
      "Epoch 7243, Loss: 1.652756243944168, Final Batch Loss: 0.3834880590438843\n",
      "Epoch 7244, Loss: 1.5523623526096344, Final Batch Loss: 0.2901139259338379\n",
      "Epoch 7245, Loss: 1.6487814486026764, Final Batch Loss: 0.3799290060997009\n",
      "Epoch 7246, Loss: 1.608136236667633, Final Batch Loss: 0.2654082179069519\n",
      "Epoch 7247, Loss: 1.6220072209835052, Final Batch Loss: 0.3259715139865875\n",
      "Epoch 7248, Loss: 1.6635516583919525, Final Batch Loss: 0.33376020193099976\n",
      "Epoch 7249, Loss: 1.6234959959983826, Final Batch Loss: 0.3460521996021271\n",
      "Epoch 7250, Loss: 1.6357403546571732, Final Batch Loss: 0.30244335532188416\n",
      "Epoch 7251, Loss: 1.7958783507347107, Final Batch Loss: 0.3408956527709961\n",
      "Epoch 7252, Loss: 1.6035001575946808, Final Batch Loss: 0.23755136132240295\n",
      "Epoch 7253, Loss: 1.6101052463054657, Final Batch Loss: 0.3012548089027405\n",
      "Epoch 7254, Loss: 1.5501548647880554, Final Batch Loss: 0.27420201897621155\n",
      "Epoch 7255, Loss: 1.4803621768951416, Final Batch Loss: 0.2727697193622589\n",
      "Epoch 7256, Loss: 1.7811287939548492, Final Batch Loss: 0.41579607129096985\n",
      "Epoch 7257, Loss: 1.7648162841796875, Final Batch Loss: 0.32142123579978943\n",
      "Epoch 7258, Loss: 1.4318220615386963, Final Batch Loss: 0.28567320108413696\n",
      "Epoch 7259, Loss: 1.7278610467910767, Final Batch Loss: 0.30311450362205505\n",
      "Epoch 7260, Loss: 1.8281807899475098, Final Batch Loss: 0.29948583245277405\n",
      "Epoch 7261, Loss: 1.6304430663585663, Final Batch Loss: 0.34585076570510864\n",
      "Epoch 7262, Loss: 1.4564441442489624, Final Batch Loss: 0.33170539140701294\n",
      "Epoch 7263, Loss: 1.5460610389709473, Final Batch Loss: 0.2821066677570343\n",
      "Epoch 7264, Loss: 1.5656078457832336, Final Batch Loss: 0.23178112506866455\n",
      "Epoch 7265, Loss: 1.745841920375824, Final Batch Loss: 0.34174618124961853\n",
      "Epoch 7266, Loss: 1.6507940292358398, Final Batch Loss: 0.3738389015197754\n",
      "Epoch 7267, Loss: 1.6372738778591156, Final Batch Loss: 0.35637903213500977\n",
      "Epoch 7268, Loss: 1.5750575959682465, Final Batch Loss: 0.3675483465194702\n",
      "Epoch 7269, Loss: 1.794787883758545, Final Batch Loss: 0.3828543722629547\n",
      "Epoch 7270, Loss: 1.7782069444656372, Final Batch Loss: 0.342417448759079\n",
      "Epoch 7271, Loss: 1.5287676751613617, Final Batch Loss: 0.3286035358905792\n",
      "Epoch 7272, Loss: 1.6803799867630005, Final Batch Loss: 0.37212657928466797\n",
      "Epoch 7273, Loss: 1.7721267938613892, Final Batch Loss: 0.4039340317249298\n",
      "Epoch 7274, Loss: 1.630966991186142, Final Batch Loss: 0.3428660035133362\n",
      "Epoch 7275, Loss: 1.6100790798664093, Final Batch Loss: 0.29873740673065186\n",
      "Epoch 7276, Loss: 1.4940907806158066, Final Batch Loss: 0.2898872494697571\n",
      "Epoch 7277, Loss: 1.5971996486186981, Final Batch Loss: 0.2781369388103485\n",
      "Epoch 7278, Loss: 1.7640757858753204, Final Batch Loss: 0.3056335151195526\n",
      "Epoch 7279, Loss: 1.7278388440608978, Final Batch Loss: 0.37027448415756226\n",
      "Epoch 7280, Loss: 1.8688533902168274, Final Batch Loss: 0.4658221900463104\n",
      "Epoch 7281, Loss: 1.5848260819911957, Final Batch Loss: 0.3152036666870117\n",
      "Epoch 7282, Loss: 1.8593557476997375, Final Batch Loss: 0.36700814962387085\n",
      "Epoch 7283, Loss: 1.6741341650485992, Final Batch Loss: 0.34472838044166565\n",
      "Epoch 7284, Loss: 1.4990040808916092, Final Batch Loss: 0.2385399490594864\n",
      "Epoch 7285, Loss: 1.7938249111175537, Final Batch Loss: 0.32581138610839844\n",
      "Epoch 7286, Loss: 1.8245468735694885, Final Batch Loss: 0.3363553583621979\n",
      "Epoch 7287, Loss: 1.5652186572551727, Final Batch Loss: 0.2896507978439331\n",
      "Epoch 7288, Loss: 1.6673938930034637, Final Batch Loss: 0.30706560611724854\n",
      "Epoch 7289, Loss: 1.6243909299373627, Final Batch Loss: 0.2871231734752655\n",
      "Epoch 7290, Loss: 1.6053719222545624, Final Batch Loss: 0.31806573271751404\n",
      "Epoch 7291, Loss: 1.7056222558021545, Final Batch Loss: 0.29506608843803406\n",
      "Epoch 7292, Loss: 1.678672417998314, Final Batch Loss: 0.4242382049560547\n",
      "Epoch 7293, Loss: 1.7513347417116165, Final Batch Loss: 0.3937349319458008\n",
      "Epoch 7294, Loss: 1.663924053311348, Final Batch Loss: 0.36366525292396545\n",
      "Epoch 7295, Loss: 1.5561136603355408, Final Batch Loss: 0.33300620317459106\n",
      "Epoch 7296, Loss: 1.5186012387275696, Final Batch Loss: 0.33130359649658203\n",
      "Epoch 7297, Loss: 1.6533908545970917, Final Batch Loss: 0.33986014127731323\n",
      "Epoch 7298, Loss: 1.645959198474884, Final Batch Loss: 0.30389875173568726\n",
      "Epoch 7299, Loss: 1.6333400905132294, Final Batch Loss: 0.26598089933395386\n",
      "Epoch 7300, Loss: 1.5985123217105865, Final Batch Loss: 0.33317792415618896\n",
      "Epoch 7301, Loss: 1.5592221915721893, Final Batch Loss: 0.37457504868507385\n",
      "Epoch 7302, Loss: 1.4516760557889938, Final Batch Loss: 0.3282913267612457\n",
      "Epoch 7303, Loss: 1.5277356654405594, Final Batch Loss: 0.24875597655773163\n",
      "Epoch 7304, Loss: 1.774268627166748, Final Batch Loss: 0.3294540345668793\n",
      "Epoch 7305, Loss: 1.6909882426261902, Final Batch Loss: 0.3846486210823059\n",
      "Epoch 7306, Loss: 1.5177156329154968, Final Batch Loss: 0.2815471291542053\n",
      "Epoch 7307, Loss: 1.628763198852539, Final Batch Loss: 0.3865468204021454\n",
      "Epoch 7308, Loss: 1.7971862256526947, Final Batch Loss: 0.3694732189178467\n",
      "Epoch 7309, Loss: 1.4256760627031326, Final Batch Loss: 0.25983113050460815\n",
      "Epoch 7310, Loss: 1.7044490277767181, Final Batch Loss: 0.45509400963783264\n",
      "Epoch 7311, Loss: 1.5636524111032486, Final Batch Loss: 0.37105801701545715\n",
      "Epoch 7312, Loss: 1.5716168880462646, Final Batch Loss: 0.28540313243865967\n",
      "Epoch 7313, Loss: 1.670969843864441, Final Batch Loss: 0.48917707800865173\n",
      "Epoch 7314, Loss: 1.5950842797756195, Final Batch Loss: 0.30001121759414673\n",
      "Epoch 7315, Loss: 1.617733657360077, Final Batch Loss: 0.4372454583644867\n",
      "Epoch 7316, Loss: 1.7455334961414337, Final Batch Loss: 0.2885141372680664\n",
      "Epoch 7317, Loss: 1.437321588397026, Final Batch Loss: 0.24283654987812042\n",
      "Epoch 7318, Loss: 1.559114083647728, Final Batch Loss: 0.26412075757980347\n",
      "Epoch 7319, Loss: 1.6512641906738281, Final Batch Loss: 0.28042423725128174\n",
      "Epoch 7320, Loss: 1.7564911544322968, Final Batch Loss: 0.3405693769454956\n",
      "Epoch 7321, Loss: 1.4891553223133087, Final Batch Loss: 0.2385639250278473\n",
      "Epoch 7322, Loss: 1.5456633865833282, Final Batch Loss: 0.3447732925415039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7323, Loss: 1.8267613351345062, Final Batch Loss: 0.5001497864723206\n",
      "Epoch 7324, Loss: 1.7259505093097687, Final Batch Loss: 0.2619786858558655\n",
      "Epoch 7325, Loss: 1.7815261781215668, Final Batch Loss: 0.48257169127464294\n",
      "Epoch 7326, Loss: 1.619974672794342, Final Batch Loss: 0.31615355610847473\n",
      "Epoch 7327, Loss: 1.6266626715660095, Final Batch Loss: 0.38093769550323486\n",
      "Epoch 7328, Loss: 1.5539408028125763, Final Batch Loss: 0.35920751094818115\n",
      "Epoch 7329, Loss: 1.5812170207500458, Final Batch Loss: 0.3081994354724884\n",
      "Epoch 7330, Loss: 1.7661359310150146, Final Batch Loss: 0.3280143439769745\n",
      "Epoch 7331, Loss: 1.6641694009304047, Final Batch Loss: 0.3918263912200928\n",
      "Epoch 7332, Loss: 1.5665457844734192, Final Batch Loss: 0.3412179946899414\n",
      "Epoch 7333, Loss: 1.517937794327736, Final Batch Loss: 0.2443837970495224\n",
      "Epoch 7334, Loss: 1.8445504307746887, Final Batch Loss: 0.27358731627464294\n",
      "Epoch 7335, Loss: 1.684929370880127, Final Batch Loss: 0.35028064250946045\n",
      "Epoch 7336, Loss: 1.594245821237564, Final Batch Loss: 0.3108729124069214\n",
      "Epoch 7337, Loss: 1.7005453258752823, Final Batch Loss: 0.41257244348526\n",
      "Epoch 7338, Loss: 1.566556304693222, Final Batch Loss: 0.34466105699539185\n",
      "Epoch 7339, Loss: 1.6817633360624313, Final Batch Loss: 0.3849532902240753\n",
      "Epoch 7340, Loss: 1.6378022730350494, Final Batch Loss: 0.35421010851860046\n",
      "Epoch 7341, Loss: 1.6037439107894897, Final Batch Loss: 0.3369731307029724\n",
      "Epoch 7342, Loss: 1.535685956478119, Final Batch Loss: 0.2576638162136078\n",
      "Epoch 7343, Loss: 1.688909262418747, Final Batch Loss: 0.3809785544872284\n",
      "Epoch 7344, Loss: 1.405909925699234, Final Batch Loss: 0.2911904752254486\n",
      "Epoch 7345, Loss: 1.7941600680351257, Final Batch Loss: 0.34985581040382385\n",
      "Epoch 7346, Loss: 1.7491792440414429, Final Batch Loss: 0.3251313865184784\n",
      "Epoch 7347, Loss: 1.516890525817871, Final Batch Loss: 0.2497905194759369\n",
      "Epoch 7348, Loss: 1.760793775320053, Final Batch Loss: 0.3542501628398895\n",
      "Epoch 7349, Loss: 1.6385506391525269, Final Batch Loss: 0.3463517725467682\n",
      "Epoch 7350, Loss: 1.7749300301074982, Final Batch Loss: 0.3311779797077179\n",
      "Epoch 7351, Loss: 1.7638187408447266, Final Batch Loss: 0.38445886969566345\n",
      "Epoch 7352, Loss: 1.601476013660431, Final Batch Loss: 0.3382680118083954\n",
      "Epoch 7353, Loss: 1.4064383208751678, Final Batch Loss: 0.2396644949913025\n",
      "Epoch 7354, Loss: 1.7591302692890167, Final Batch Loss: 0.4195316433906555\n",
      "Epoch 7355, Loss: 1.8412264287471771, Final Batch Loss: 0.2889477014541626\n",
      "Epoch 7356, Loss: 1.6492290794849396, Final Batch Loss: 0.3786839544773102\n",
      "Epoch 7357, Loss: 1.5457240641117096, Final Batch Loss: 0.23838776350021362\n",
      "Epoch 7358, Loss: 1.5449051260948181, Final Batch Loss: 0.26060259342193604\n",
      "Epoch 7359, Loss: 1.745520904660225, Final Batch Loss: 0.41359981894493103\n",
      "Epoch 7360, Loss: 1.6858919858932495, Final Batch Loss: 0.32693052291870117\n",
      "Epoch 7361, Loss: 1.6083919703960419, Final Batch Loss: 0.2940901517868042\n",
      "Epoch 7362, Loss: 2.0508917570114136, Final Batch Loss: 0.796068012714386\n",
      "Epoch 7363, Loss: 1.5375782251358032, Final Batch Loss: 0.367506206035614\n",
      "Epoch 7364, Loss: 1.550059050321579, Final Batch Loss: 0.29691800475120544\n",
      "Epoch 7365, Loss: 1.6518247723579407, Final Batch Loss: 0.35373038053512573\n",
      "Epoch 7366, Loss: 1.583924040198326, Final Batch Loss: 0.2136680632829666\n",
      "Epoch 7367, Loss: 1.7047147750854492, Final Batch Loss: 0.3399220407009125\n",
      "Epoch 7368, Loss: 1.5824505388736725, Final Batch Loss: 0.33690696954727173\n",
      "Epoch 7369, Loss: 1.4306488782167435, Final Batch Loss: 0.2485172152519226\n",
      "Epoch 7370, Loss: 1.4837160259485245, Final Batch Loss: 0.30763277411460876\n",
      "Epoch 7371, Loss: 1.5502276867628098, Final Batch Loss: 0.39693257212638855\n",
      "Epoch 7372, Loss: 1.6454640924930573, Final Batch Loss: 0.38517120480537415\n",
      "Epoch 7373, Loss: 1.640645146369934, Final Batch Loss: 0.2575770914554596\n",
      "Epoch 7374, Loss: 1.5626903921365738, Final Batch Loss: 0.22783955931663513\n",
      "Epoch 7375, Loss: 1.6639569997787476, Final Batch Loss: 0.28974616527557373\n",
      "Epoch 7376, Loss: 1.656611979007721, Final Batch Loss: 0.4118138551712036\n",
      "Epoch 7377, Loss: 1.5820440649986267, Final Batch Loss: 0.36227017641067505\n",
      "Epoch 7378, Loss: 1.6275689899921417, Final Batch Loss: 0.306212842464447\n",
      "Epoch 7379, Loss: 1.7631841003894806, Final Batch Loss: 0.36111995577812195\n",
      "Epoch 7380, Loss: 1.6206345856189728, Final Batch Loss: 0.4192417562007904\n",
      "Epoch 7381, Loss: 1.644039735198021, Final Batch Loss: 0.3759576678276062\n",
      "Epoch 7382, Loss: 1.66256445646286, Final Batch Loss: 0.37373796105384827\n",
      "Epoch 7383, Loss: 1.7416177690029144, Final Batch Loss: 0.3535692095756531\n",
      "Epoch 7384, Loss: 1.8414556086063385, Final Batch Loss: 0.4200921952724457\n",
      "Epoch 7385, Loss: 1.7168157994747162, Final Batch Loss: 0.34384021162986755\n",
      "Epoch 7386, Loss: 1.5958313047885895, Final Batch Loss: 0.30172887444496155\n",
      "Epoch 7387, Loss: 1.4703657925128937, Final Batch Loss: 0.2602556347846985\n",
      "Epoch 7388, Loss: 1.512471228837967, Final Batch Loss: 0.2945156693458557\n",
      "Epoch 7389, Loss: 1.5642746686935425, Final Batch Loss: 0.27468371391296387\n",
      "Epoch 7390, Loss: 1.6013955771923065, Final Batch Loss: 0.3886493444442749\n",
      "Epoch 7391, Loss: 1.5548474192619324, Final Batch Loss: 0.4081266522407532\n",
      "Epoch 7392, Loss: 1.6820980906486511, Final Batch Loss: 0.25967296957969666\n",
      "Epoch 7393, Loss: 1.5322456657886505, Final Batch Loss: 0.30565235018730164\n",
      "Epoch 7394, Loss: 1.6711695790290833, Final Batch Loss: 0.26923519372940063\n",
      "Epoch 7395, Loss: 1.5998661071062088, Final Batch Loss: 0.19489245116710663\n",
      "Epoch 7396, Loss: 1.7762510180473328, Final Batch Loss: 0.296558678150177\n",
      "Epoch 7397, Loss: 1.6406580805778503, Final Batch Loss: 0.36606329679489136\n",
      "Epoch 7398, Loss: 1.6147224605083466, Final Batch Loss: 0.36097443103790283\n",
      "Epoch 7399, Loss: 1.5691624283790588, Final Batch Loss: 0.36741572618484497\n",
      "Epoch 7400, Loss: 1.6808682978153229, Final Batch Loss: 0.2812613248825073\n",
      "Epoch 7401, Loss: 1.4802700579166412, Final Batch Loss: 0.31172019243240356\n",
      "Epoch 7402, Loss: 1.6641257405281067, Final Batch Loss: 0.3196812570095062\n",
      "Epoch 7403, Loss: 1.5911941081285477, Final Batch Loss: 0.32803717255592346\n",
      "Epoch 7404, Loss: 1.754255086183548, Final Batch Loss: 0.3158201575279236\n",
      "Epoch 7405, Loss: 1.5124708116054535, Final Batch Loss: 0.3491538166999817\n",
      "Epoch 7406, Loss: 1.7216052412986755, Final Batch Loss: 0.29872485995292664\n",
      "Epoch 7407, Loss: 1.5393485128879547, Final Batch Loss: 0.26910266280174255\n",
      "Epoch 7408, Loss: 1.7264290750026703, Final Batch Loss: 0.34550967812538147\n",
      "Epoch 7409, Loss: 1.7597939372062683, Final Batch Loss: 0.31171804666519165\n",
      "Epoch 7410, Loss: 1.588162362575531, Final Batch Loss: 0.37207111716270447\n",
      "Epoch 7411, Loss: 1.5998791754245758, Final Batch Loss: 0.33539721369743347\n",
      "Epoch 7412, Loss: 1.8593222796916962, Final Batch Loss: 0.2904151678085327\n",
      "Epoch 7413, Loss: 1.724377691745758, Final Batch Loss: 0.33273041248321533\n",
      "Epoch 7414, Loss: 1.5689180940389633, Final Batch Loss: 0.3499138057231903\n",
      "Epoch 7415, Loss: 1.5822939723730087, Final Batch Loss: 0.24619750678539276\n",
      "Epoch 7416, Loss: 1.7460047900676727, Final Batch Loss: 0.3657585084438324\n",
      "Epoch 7417, Loss: 1.5476588606834412, Final Batch Loss: 0.3097340762615204\n",
      "Epoch 7418, Loss: 1.5627919435501099, Final Batch Loss: 0.28329387307167053\n",
      "Epoch 7419, Loss: 1.703580915927887, Final Batch Loss: 0.4203896224498749\n",
      "Epoch 7420, Loss: 1.508380800485611, Final Batch Loss: 0.31903141736984253\n",
      "Epoch 7421, Loss: 1.564335972070694, Final Batch Loss: 0.3535256087779999\n",
      "Epoch 7422, Loss: 1.6966914534568787, Final Batch Loss: 0.3751194477081299\n",
      "Epoch 7423, Loss: 1.6173345446586609, Final Batch Loss: 0.32045450806617737\n",
      "Epoch 7424, Loss: 1.5189083516597748, Final Batch Loss: 0.31394389271736145\n",
      "Epoch 7425, Loss: 1.5266386419534683, Final Batch Loss: 0.3256794512271881\n",
      "Epoch 7426, Loss: 1.547159105539322, Final Batch Loss: 0.29906758666038513\n",
      "Epoch 7427, Loss: 1.695155769586563, Final Batch Loss: 0.4047972559928894\n",
      "Epoch 7428, Loss: 1.3989643156528473, Final Batch Loss: 0.22809642553329468\n",
      "Epoch 7429, Loss: 1.8685455024242401, Final Batch Loss: 0.5348998308181763\n",
      "Epoch 7430, Loss: 1.616927295923233, Final Batch Loss: 0.2963675558567047\n",
      "Epoch 7431, Loss: 1.7797683477401733, Final Batch Loss: 0.32473671436309814\n",
      "Epoch 7432, Loss: 1.756052404642105, Final Batch Loss: 0.350598007440567\n",
      "Epoch 7433, Loss: 1.6558162569999695, Final Batch Loss: 0.3858800232410431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7434, Loss: 1.6140491962432861, Final Batch Loss: 0.3117530643939972\n",
      "Epoch 7435, Loss: 1.4524189233779907, Final Batch Loss: 0.32434964179992676\n",
      "Epoch 7436, Loss: 1.6977427005767822, Final Batch Loss: 0.31840136647224426\n",
      "Epoch 7437, Loss: 1.6013046503067017, Final Batch Loss: 0.36407724022865295\n",
      "Epoch 7438, Loss: 1.7006056308746338, Final Batch Loss: 0.3297506868839264\n",
      "Epoch 7439, Loss: 1.8012837171554565, Final Batch Loss: 0.3053029179573059\n",
      "Epoch 7440, Loss: 1.5385586023330688, Final Batch Loss: 0.3189316391944885\n",
      "Epoch 7441, Loss: 1.4964328110218048, Final Batch Loss: 0.3111027181148529\n",
      "Epoch 7442, Loss: 1.4834253191947937, Final Batch Loss: 0.2800998389720917\n",
      "Epoch 7443, Loss: 1.5778461694717407, Final Batch Loss: 0.3435657322406769\n",
      "Epoch 7444, Loss: 1.5158578753471375, Final Batch Loss: 0.3303665518760681\n",
      "Epoch 7445, Loss: 1.5181241184473038, Final Batch Loss: 0.2329643815755844\n",
      "Epoch 7446, Loss: 1.6185244619846344, Final Batch Loss: 0.3988698422908783\n",
      "Epoch 7447, Loss: 1.809788852930069, Final Batch Loss: 0.38074252009391785\n",
      "Epoch 7448, Loss: 1.6632724702358246, Final Batch Loss: 0.3187553584575653\n",
      "Epoch 7449, Loss: 1.7777986526489258, Final Batch Loss: 0.4382186830043793\n",
      "Epoch 7450, Loss: 1.5862063467502594, Final Batch Loss: 0.3011137843132019\n",
      "Epoch 7451, Loss: 1.4762787818908691, Final Batch Loss: 0.30014199018478394\n",
      "Epoch 7452, Loss: 1.647250473499298, Final Batch Loss: 0.33951589465141296\n",
      "Epoch 7453, Loss: 1.6888900101184845, Final Batch Loss: 0.34758737683296204\n",
      "Epoch 7454, Loss: 1.712808072566986, Final Batch Loss: 0.3242529034614563\n",
      "Epoch 7455, Loss: 1.639960527420044, Final Batch Loss: 0.38431575894355774\n",
      "Epoch 7456, Loss: 1.6791394352912903, Final Batch Loss: 0.3539053797721863\n",
      "Epoch 7457, Loss: 1.7091268002986908, Final Batch Loss: 0.3568316102027893\n",
      "Epoch 7458, Loss: 1.5587524771690369, Final Batch Loss: 0.378928542137146\n",
      "Epoch 7459, Loss: 1.725379317998886, Final Batch Loss: 0.3848421275615692\n",
      "Epoch 7460, Loss: 1.6569448113441467, Final Batch Loss: 0.322746217250824\n",
      "Epoch 7461, Loss: 1.6191085875034332, Final Batch Loss: 0.33502039313316345\n",
      "Epoch 7462, Loss: 1.615252524614334, Final Batch Loss: 0.29123255610466003\n",
      "Epoch 7463, Loss: 1.642284482717514, Final Batch Loss: 0.2847886085510254\n",
      "Epoch 7464, Loss: 1.6179336905479431, Final Batch Loss: 0.27515313029289246\n",
      "Epoch 7465, Loss: 1.6465867459774017, Final Batch Loss: 0.3819979131221771\n",
      "Epoch 7466, Loss: 1.6159556806087494, Final Batch Loss: 0.3883989453315735\n",
      "Epoch 7467, Loss: 1.7246389091014862, Final Batch Loss: 0.34112149477005005\n",
      "Epoch 7468, Loss: 1.6674905717372894, Final Batch Loss: 0.3949187994003296\n",
      "Epoch 7469, Loss: 1.5968598425388336, Final Batch Loss: 0.40383800864219666\n",
      "Epoch 7470, Loss: 1.697588950395584, Final Batch Loss: 0.3577978312969208\n",
      "Epoch 7471, Loss: 1.5709955990314484, Final Batch Loss: 0.33889904618263245\n",
      "Epoch 7472, Loss: 1.6487539261579514, Final Batch Loss: 0.4083067774772644\n",
      "Epoch 7473, Loss: 1.5578540563583374, Final Batch Loss: 0.3307647109031677\n",
      "Epoch 7474, Loss: 1.6171535849571228, Final Batch Loss: 0.362564355134964\n",
      "Epoch 7475, Loss: 1.5029101222753525, Final Batch Loss: 0.2873929738998413\n",
      "Epoch 7476, Loss: 1.6091029644012451, Final Batch Loss: 0.3335142731666565\n",
      "Epoch 7477, Loss: 1.4891615808010101, Final Batch Loss: 0.3679468631744385\n",
      "Epoch 7478, Loss: 1.5718771815299988, Final Batch Loss: 0.329118937253952\n",
      "Epoch 7479, Loss: 1.5402713119983673, Final Batch Loss: 0.2929573059082031\n",
      "Epoch 7480, Loss: 1.5325941443443298, Final Batch Loss: 0.30557987093925476\n",
      "Epoch 7481, Loss: 1.5920199751853943, Final Batch Loss: 0.34102943539619446\n",
      "Epoch 7482, Loss: 1.7517439424991608, Final Batch Loss: 0.42841824889183044\n",
      "Epoch 7483, Loss: 1.810057133436203, Final Batch Loss: 0.3548622727394104\n",
      "Epoch 7484, Loss: 1.9141103625297546, Final Batch Loss: 0.4393194615840912\n",
      "Epoch 7485, Loss: 1.6126537024974823, Final Batch Loss: 0.3357435166835785\n",
      "Epoch 7486, Loss: 1.655407726764679, Final Batch Loss: 0.4206640124320984\n",
      "Epoch 7487, Loss: 1.8123441636562347, Final Batch Loss: 0.34605640172958374\n",
      "Epoch 7488, Loss: 1.5651609748601913, Final Batch Loss: 0.24792547523975372\n",
      "Epoch 7489, Loss: 1.7815632820129395, Final Batch Loss: 0.40450894832611084\n",
      "Epoch 7490, Loss: 1.741543173789978, Final Batch Loss: 0.37551525235176086\n",
      "Epoch 7491, Loss: 1.5817535817623138, Final Batch Loss: 0.2957378923892975\n",
      "Epoch 7492, Loss: 1.6280480027198792, Final Batch Loss: 0.31498026847839355\n",
      "Epoch 7493, Loss: 1.7109601348638535, Final Batch Loss: 0.4327392280101776\n",
      "Epoch 7494, Loss: 1.733252376317978, Final Batch Loss: 0.4486052989959717\n",
      "Epoch 7495, Loss: 1.5204355418682098, Final Batch Loss: 0.2825981080532074\n",
      "Epoch 7496, Loss: 1.6861276030540466, Final Batch Loss: 0.3309672176837921\n",
      "Epoch 7497, Loss: 1.6699426472187042, Final Batch Loss: 0.30016541481018066\n",
      "Epoch 7498, Loss: 1.6042945683002472, Final Batch Loss: 0.27915695309638977\n",
      "Epoch 7499, Loss: 1.61216601729393, Final Batch Loss: 0.30074217915534973\n",
      "Epoch 7500, Loss: 1.7050027251243591, Final Batch Loss: 0.3568279445171356\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  0  0  0  1  0  0  0  1]\n",
      " [ 0 19  0  0  1  2  0  4  0]\n",
      " [ 0  0 19  0  0  0  0  0  0]\n",
      " [ 0  0  0 20  0  0  0  0  0]\n",
      " [ 2  1  0  0 19  0  0  0  1]\n",
      " [ 1  0  0  0  0 25  0  0  1]\n",
      " [ 0  0  0  5  0  0 23  0  0]\n",
      " [ 0  1  0  0  0  0  0 27  0]\n",
      " [ 0  0  0  0  0  1  0  0 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.885     0.920     0.902        25\n",
      "           1      0.905     0.731     0.809        26\n",
      "           2      1.000     1.000     1.000        19\n",
      "           3      0.800     1.000     0.889        20\n",
      "           4      0.905     0.826     0.864        23\n",
      "           5      0.893     0.926     0.909        27\n",
      "           6      1.000     0.821     0.902        28\n",
      "           7      0.871     0.964     0.915        28\n",
      "           8      0.857     0.947     0.900        19\n",
      "\n",
      "    accuracy                          0.898       215\n",
      "   macro avg      0.902     0.904     0.899       215\n",
      "weighted avg      0.903     0.898     0.896       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 9 User Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
