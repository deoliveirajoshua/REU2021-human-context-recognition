{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Activity_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Activity_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class Subject_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Subject_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_train = GAN_data.iloc[:,:-2].values\n",
    "    y_train = GAN_data[[y_label]].values\n",
    "    \n",
    "    return X_train, y_train.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)):\n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Activity_Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.514915585517883, Final Batch Loss: 1.1011919975280762\n",
      "Epoch 2, Loss: 5.489588141441345, Final Batch Loss: 1.10643470287323\n",
      "Epoch 3, Loss: 5.456097364425659, Final Batch Loss: 1.0828948020935059\n",
      "Epoch 4, Loss: 5.4219971895217896, Final Batch Loss: 1.0846818685531616\n",
      "Epoch 5, Loss: 5.379737377166748, Final Batch Loss: 1.0785868167877197\n",
      "Epoch 6, Loss: 5.313866019248962, Final Batch Loss: 1.0559115409851074\n",
      "Epoch 7, Loss: 5.225294589996338, Final Batch Loss: 1.0295816659927368\n",
      "Epoch 8, Loss: 5.1145408153533936, Final Batch Loss: 1.0136805772781372\n",
      "Epoch 9, Loss: 4.9588053822517395, Final Batch Loss: 0.9749287366867065\n",
      "Epoch 10, Loss: 4.753838300704956, Final Batch Loss: 0.9113596677780151\n",
      "Epoch 11, Loss: 4.550461649894714, Final Batch Loss: 0.9114330410957336\n",
      "Epoch 12, Loss: 4.257636606693268, Final Batch Loss: 0.8385613560676575\n",
      "Epoch 13, Loss: 3.999202072620392, Final Batch Loss: 0.7957668304443359\n",
      "Epoch 14, Loss: 3.680811583995819, Final Batch Loss: 0.7200189232826233\n",
      "Epoch 15, Loss: 3.4132383465766907, Final Batch Loss: 0.6768310070037842\n",
      "Epoch 16, Loss: 3.116593599319458, Final Batch Loss: 0.6233041286468506\n",
      "Epoch 17, Loss: 2.8763288855552673, Final Batch Loss: 0.549801230430603\n",
      "Epoch 18, Loss: 2.591990113258362, Final Batch Loss: 0.5211763978004456\n",
      "Epoch 19, Loss: 2.297322064638138, Final Batch Loss: 0.43702271580696106\n",
      "Epoch 20, Loss: 2.063100188970566, Final Batch Loss: 0.37162578105926514\n",
      "Epoch 21, Loss: 1.8486392498016357, Final Batch Loss: 0.37380146980285645\n",
      "Epoch 22, Loss: 1.689282476902008, Final Batch Loss: 0.33466872572898865\n",
      "Epoch 23, Loss: 1.5816636085510254, Final Batch Loss: 0.28476813435554504\n",
      "Epoch 24, Loss: 1.4484717845916748, Final Batch Loss: 0.2621830999851227\n",
      "Epoch 25, Loss: 1.3875157237052917, Final Batch Loss: 0.2533867359161377\n",
      "Epoch 26, Loss: 1.2436827719211578, Final Batch Loss: 0.2605021893978119\n",
      "Epoch 27, Loss: 1.1946383267641068, Final Batch Loss: 0.23097802698612213\n",
      "Epoch 28, Loss: 1.1761057674884796, Final Batch Loss: 0.24545937776565552\n",
      "Epoch 29, Loss: 1.1079849600791931, Final Batch Loss: 0.22348935902118683\n",
      "Epoch 30, Loss: 1.1462658047676086, Final Batch Loss: 0.22473938763141632\n",
      "Epoch 31, Loss: 0.9943770915269852, Final Batch Loss: 0.1733776330947876\n",
      "Epoch 32, Loss: 1.0174514651298523, Final Batch Loss: 0.22383296489715576\n",
      "Epoch 33, Loss: 0.9592666476964951, Final Batch Loss: 0.125193789601326\n",
      "Epoch 34, Loss: 1.0344137400388718, Final Batch Loss: 0.22717627882957458\n",
      "Epoch 35, Loss: 1.0569099187850952, Final Batch Loss: 0.23089946806430817\n",
      "Epoch 36, Loss: 1.0389024764299393, Final Batch Loss: 0.2583518624305725\n",
      "Epoch 37, Loss: 0.9246438145637512, Final Batch Loss: 0.22042976319789886\n",
      "Epoch 38, Loss: 0.9297277182340622, Final Batch Loss: 0.1827109158039093\n",
      "Epoch 39, Loss: 0.9069442749023438, Final Batch Loss: 0.20130737125873566\n",
      "Epoch 40, Loss: 0.8659928292036057, Final Batch Loss: 0.12310580909252167\n",
      "Epoch 41, Loss: 0.788269117474556, Final Batch Loss: 0.13487251102924347\n",
      "Epoch 42, Loss: 0.7932755574584007, Final Batch Loss: 0.14580601453781128\n",
      "Epoch 43, Loss: 0.8508037477731705, Final Batch Loss: 0.1841326653957367\n",
      "Epoch 44, Loss: 0.7820451110601425, Final Batch Loss: 0.15154601633548737\n",
      "Epoch 45, Loss: 0.8056970983743668, Final Batch Loss: 0.11924193799495697\n",
      "Epoch 46, Loss: 0.7077143713831902, Final Batch Loss: 0.1275852769613266\n",
      "Epoch 47, Loss: 0.7502775713801384, Final Batch Loss: 0.13539937138557434\n",
      "Epoch 48, Loss: 0.7441799342632294, Final Batch Loss: 0.12946711480617523\n",
      "Epoch 49, Loss: 0.7871841117739677, Final Batch Loss: 0.14359094202518463\n",
      "Epoch 50, Loss: 0.7908812090754509, Final Batch Loss: 0.13520964980125427\n",
      "Epoch 51, Loss: 0.7236671447753906, Final Batch Loss: 0.13166013360023499\n",
      "Epoch 52, Loss: 0.699571006000042, Final Batch Loss: 0.1646738052368164\n",
      "Epoch 53, Loss: 0.7309603244066238, Final Batch Loss: 0.1348288506269455\n",
      "Epoch 54, Loss: 0.7676604464650154, Final Batch Loss: 0.1554170548915863\n",
      "Epoch 55, Loss: 0.6939462199807167, Final Batch Loss: 0.13451415300369263\n",
      "Epoch 56, Loss: 0.6912086606025696, Final Batch Loss: 0.12570708990097046\n",
      "Epoch 57, Loss: 0.7506722807884216, Final Batch Loss: 0.13595618307590485\n",
      "Epoch 58, Loss: 0.7540055215358734, Final Batch Loss: 0.2639504373073578\n",
      "Epoch 59, Loss: 0.6915596574544907, Final Batch Loss: 0.1787300407886505\n",
      "Epoch 60, Loss: 0.5846936479210854, Final Batch Loss: 0.08337114006280899\n",
      "Epoch 61, Loss: 0.6113832071423531, Final Batch Loss: 0.13582639396190643\n",
      "Epoch 62, Loss: 0.6223280280828476, Final Batch Loss: 0.08165127038955688\n",
      "Epoch 63, Loss: 0.5885022357106209, Final Batch Loss: 0.12078531086444855\n",
      "Epoch 64, Loss: 0.5919267386198044, Final Batch Loss: 0.12690958380699158\n",
      "Epoch 65, Loss: 0.5997349396348, Final Batch Loss: 0.11778996884822845\n",
      "Epoch 66, Loss: 0.59222012758255, Final Batch Loss: 0.13462084531784058\n",
      "Epoch 67, Loss: 0.5843325778841972, Final Batch Loss: 0.11237173527479172\n",
      "Epoch 68, Loss: 0.5968895740807056, Final Batch Loss: 0.1347825527191162\n",
      "Epoch 69, Loss: 0.6313444972038269, Final Batch Loss: 0.11104702204465866\n",
      "Epoch 70, Loss: 0.5646805465221405, Final Batch Loss: 0.07425585389137268\n",
      "Epoch 71, Loss: 0.592696025967598, Final Batch Loss: 0.0877544954419136\n",
      "Epoch 72, Loss: 0.6026368588209152, Final Batch Loss: 0.09337559342384338\n",
      "Epoch 73, Loss: 0.5956612452864647, Final Batch Loss: 0.10481176525354385\n",
      "Epoch 74, Loss: 0.5781415477395058, Final Batch Loss: 0.11142929643392563\n",
      "Epoch 75, Loss: 0.5531790927052498, Final Batch Loss: 0.11992832273244858\n",
      "Epoch 76, Loss: 0.601353332400322, Final Batch Loss: 0.1368780881166458\n",
      "Epoch 77, Loss: 0.6319943368434906, Final Batch Loss: 0.1331750899553299\n",
      "Epoch 78, Loss: 0.5569302663207054, Final Batch Loss: 0.14897523820400238\n",
      "Epoch 79, Loss: 0.5734691396355629, Final Batch Loss: 0.12077637016773224\n",
      "Epoch 80, Loss: 0.5412909612059593, Final Batch Loss: 0.07310868054628372\n",
      "Epoch 81, Loss: 0.5807624831795692, Final Batch Loss: 0.051253482699394226\n",
      "Epoch 82, Loss: 0.5965101271867752, Final Batch Loss: 0.11187481135129929\n",
      "Epoch 83, Loss: 0.6053206697106361, Final Batch Loss: 0.1362788826227188\n",
      "Epoch 84, Loss: 0.5626403093338013, Final Batch Loss: 0.0910123959183693\n",
      "Epoch 85, Loss: 0.5455913543701172, Final Batch Loss: 0.05627581477165222\n",
      "Epoch 86, Loss: 0.5956223830580711, Final Batch Loss: 0.07661513984203339\n",
      "Epoch 87, Loss: 0.5609622374176979, Final Batch Loss: 0.043370455503463745\n",
      "Epoch 88, Loss: 0.5309753119945526, Final Batch Loss: 0.12318786233663559\n",
      "Epoch 89, Loss: 0.5607381090521812, Final Batch Loss: 0.11912696808576584\n",
      "Epoch 90, Loss: 0.5004369542002678, Final Batch Loss: 0.09626999497413635\n",
      "Epoch 91, Loss: 0.5392382144927979, Final Batch Loss: 0.08462715148925781\n",
      "Epoch 92, Loss: 0.6370846405625343, Final Batch Loss: 0.13804450631141663\n",
      "Epoch 93, Loss: 0.5413730591535568, Final Batch Loss: 0.08700936287641525\n",
      "Epoch 94, Loss: 0.551726758480072, Final Batch Loss: 0.07844850420951843\n",
      "Epoch 95, Loss: 0.5121074095368385, Final Batch Loss: 0.07064536213874817\n",
      "Epoch 96, Loss: 0.49602901190519333, Final Batch Loss: 0.0946526825428009\n",
      "Epoch 97, Loss: 0.5424212142825127, Final Batch Loss: 0.13336382806301117\n",
      "Epoch 98, Loss: 0.5098097398877144, Final Batch Loss: 0.09258393943309784\n",
      "Epoch 99, Loss: 0.5249574147164822, Final Batch Loss: 0.1385761946439743\n",
      "Epoch 100, Loss: 0.5278056263923645, Final Batch Loss: 0.11812145262956619\n",
      "Epoch 101, Loss: 0.5928622931241989, Final Batch Loss: 0.18124720454216003\n",
      "Epoch 102, Loss: 0.520722933113575, Final Batch Loss: 0.07622570544481277\n",
      "Epoch 103, Loss: 0.4191962033510208, Final Batch Loss: 0.07258977741003036\n",
      "Epoch 104, Loss: 0.49111735075712204, Final Batch Loss: 0.1033608466386795\n",
      "Epoch 105, Loss: 0.5527535900473595, Final Batch Loss: 0.12365377694368362\n",
      "Epoch 106, Loss: 0.49096740782260895, Final Batch Loss: 0.11021936684846878\n",
      "Epoch 107, Loss: 0.49055877700448036, Final Batch Loss: 0.12479883432388306\n",
      "Epoch 108, Loss: 0.46005041897296906, Final Batch Loss: 0.08471685647964478\n",
      "Epoch 109, Loss: 0.47046415507793427, Final Batch Loss: 0.13659943640232086\n",
      "Epoch 110, Loss: 0.4493197724223137, Final Batch Loss: 0.06880561262369156\n",
      "Epoch 111, Loss: 0.46728312596678734, Final Batch Loss: 0.05848168954253197\n",
      "Epoch 112, Loss: 0.45080944895744324, Final Batch Loss: 0.06806308031082153\n",
      "Epoch 113, Loss: 0.4372076652944088, Final Batch Loss: 0.06051954627037048\n",
      "Epoch 114, Loss: 0.5165874660015106, Final Batch Loss: 0.13527673482894897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: 0.46793245524168015, Final Batch Loss: 0.0630904883146286\n",
      "Epoch 116, Loss: 0.4224892519414425, Final Batch Loss: 0.04105673357844353\n",
      "Epoch 117, Loss: 0.46873022615909576, Final Batch Loss: 0.08340808749198914\n",
      "Epoch 118, Loss: 0.4479822441935539, Final Batch Loss: 0.09027165919542313\n",
      "Epoch 119, Loss: 0.510791964828968, Final Batch Loss: 0.07852832973003387\n",
      "Epoch 120, Loss: 0.49638162553310394, Final Batch Loss: 0.11950161308050156\n",
      "Epoch 121, Loss: 0.4867684468626976, Final Batch Loss: 0.09774979203939438\n",
      "Epoch 122, Loss: 0.4813336171209812, Final Batch Loss: 0.059302959591150284\n",
      "Epoch 123, Loss: 0.4717305600643158, Final Batch Loss: 0.08198735117912292\n",
      "Epoch 124, Loss: 0.4694441184401512, Final Batch Loss: 0.08642745763063431\n",
      "Epoch 125, Loss: 0.5168477967381477, Final Batch Loss: 0.18526269495487213\n",
      "Epoch 126, Loss: 0.40982795506715775, Final Batch Loss: 0.0886702910065651\n",
      "Epoch 127, Loss: 0.44424617290496826, Final Batch Loss: 0.10103639215230942\n",
      "Epoch 128, Loss: 0.4334450475871563, Final Batch Loss: 0.0726223811507225\n",
      "Epoch 129, Loss: 0.44765718281269073, Final Batch Loss: 0.07392822951078415\n",
      "Epoch 130, Loss: 0.43846139311790466, Final Batch Loss: 0.04498160630464554\n",
      "Epoch 131, Loss: 0.515749603509903, Final Batch Loss: 0.13084189593791962\n",
      "Epoch 132, Loss: 0.4881664514541626, Final Batch Loss: 0.13991552591323853\n",
      "Epoch 133, Loss: 0.4312874674797058, Final Batch Loss: 0.09998704493045807\n",
      "Epoch 134, Loss: 0.43459416180849075, Final Batch Loss: 0.07290185242891312\n",
      "Epoch 135, Loss: 0.4492340162396431, Final Batch Loss: 0.07160898298025131\n",
      "Epoch 136, Loss: 0.40893279761075974, Final Batch Loss: 0.08141951262950897\n",
      "Epoch 137, Loss: 0.4439031705260277, Final Batch Loss: 0.0806569755077362\n",
      "Epoch 138, Loss: 0.40355440601706505, Final Batch Loss: 0.07784347236156464\n",
      "Epoch 139, Loss: 0.41023775935173035, Final Batch Loss: 0.08450145274400711\n",
      "Epoch 140, Loss: 0.4673226438462734, Final Batch Loss: 0.09353777021169662\n",
      "Epoch 141, Loss: 0.4333019033074379, Final Batch Loss: 0.06834530830383301\n",
      "Epoch 142, Loss: 0.4112832061946392, Final Batch Loss: 0.11364646255970001\n",
      "Epoch 143, Loss: 0.4138466380536556, Final Batch Loss: 0.03983532264828682\n",
      "Epoch 144, Loss: 0.4237530082464218, Final Batch Loss: 0.0951118990778923\n",
      "Epoch 145, Loss: 0.44752388447523117, Final Batch Loss: 0.09448446333408356\n",
      "Epoch 146, Loss: 0.41645151749253273, Final Batch Loss: 0.10414434969425201\n",
      "Epoch 147, Loss: 0.42140747606754303, Final Batch Loss: 0.08444305509328842\n",
      "Epoch 148, Loss: 0.40919428691267967, Final Batch Loss: 0.08440364897251129\n",
      "Epoch 149, Loss: 0.3751802518963814, Final Batch Loss: 0.040904074907302856\n",
      "Epoch 150, Loss: 0.3561420403420925, Final Batch Loss: 0.04348871856927872\n",
      "Epoch 151, Loss: 0.4180172272026539, Final Batch Loss: 0.13771463930606842\n",
      "Epoch 152, Loss: 0.36077554896473885, Final Batch Loss: 0.07545682787895203\n",
      "Epoch 153, Loss: 0.4139067530632019, Final Batch Loss: 0.08224356919527054\n",
      "Epoch 154, Loss: 0.3677750676870346, Final Batch Loss: 0.07784014940261841\n",
      "Epoch 155, Loss: 0.38136858493089676, Final Batch Loss: 0.06873565912246704\n",
      "Epoch 156, Loss: 0.4598676934838295, Final Batch Loss: 0.1529628336429596\n",
      "Epoch 157, Loss: 0.3426022343337536, Final Batch Loss: 0.02767885848879814\n",
      "Epoch 158, Loss: 0.3506348505616188, Final Batch Loss: 0.08782219886779785\n",
      "Epoch 159, Loss: 0.3987708240747452, Final Batch Loss: 0.10206356644630432\n",
      "Epoch 160, Loss: 0.400894433259964, Final Batch Loss: 0.08923619985580444\n",
      "Epoch 161, Loss: 0.3813527822494507, Final Batch Loss: 0.08815423399209976\n",
      "Epoch 162, Loss: 0.3783657178282738, Final Batch Loss: 0.06288114935159683\n",
      "Epoch 163, Loss: 0.4058546796441078, Final Batch Loss: 0.102976493537426\n",
      "Epoch 164, Loss: 0.3516431860625744, Final Batch Loss: 0.04518085718154907\n",
      "Epoch 165, Loss: 0.39325686544179916, Final Batch Loss: 0.053231898695230484\n",
      "Epoch 166, Loss: 0.33343133330345154, Final Batch Loss: 0.04053597152233124\n",
      "Epoch 167, Loss: 0.3570040985941887, Final Batch Loss: 0.04335865005850792\n",
      "Epoch 168, Loss: 0.3680596090853214, Final Batch Loss: 0.09464467316865921\n",
      "Epoch 169, Loss: 0.31673724204301834, Final Batch Loss: 0.04597187042236328\n",
      "Epoch 170, Loss: 0.322207298129797, Final Batch Loss: 0.056651923805475235\n",
      "Epoch 171, Loss: 0.29848945140838623, Final Batch Loss: 0.04669490456581116\n",
      "Epoch 172, Loss: 0.3693342059850693, Final Batch Loss: 0.09421688318252563\n",
      "Epoch 173, Loss: 0.3770505040884018, Final Batch Loss: 0.09560751169919968\n",
      "Epoch 174, Loss: 0.3740820176899433, Final Batch Loss: 0.05755043774843216\n",
      "Epoch 175, Loss: 0.4021584540605545, Final Batch Loss: 0.08661826699972153\n",
      "Epoch 176, Loss: 0.35292886570096016, Final Batch Loss: 0.03572777658700943\n",
      "Epoch 177, Loss: 0.3244897127151489, Final Batch Loss: 0.037692055106163025\n",
      "Epoch 178, Loss: 0.3268587253987789, Final Batch Loss: 0.031779106706380844\n",
      "Epoch 179, Loss: 0.3668616823852062, Final Batch Loss: 0.08982933312654495\n",
      "Epoch 180, Loss: 0.3412706106901169, Final Batch Loss: 0.07765460759401321\n",
      "Epoch 181, Loss: 0.3787623941898346, Final Batch Loss: 0.0901455283164978\n",
      "Epoch 182, Loss: 0.32722627371549606, Final Batch Loss: 0.07192762941122055\n",
      "Epoch 183, Loss: 0.3279484696686268, Final Batch Loss: 0.10738115757703781\n",
      "Epoch 184, Loss: 0.34122123569250107, Final Batch Loss: 0.04666772857308388\n",
      "Epoch 185, Loss: 0.30904895067214966, Final Batch Loss: 0.048113029450178146\n",
      "Epoch 186, Loss: 0.271472442895174, Final Batch Loss: 0.05043933540582657\n",
      "Epoch 187, Loss: 0.36100222542881966, Final Batch Loss: 0.05164935812354088\n",
      "Epoch 188, Loss: 0.31144947931170464, Final Batch Loss: 0.06664751470088959\n",
      "Epoch 189, Loss: 0.31191289238631725, Final Batch Loss: 0.07346315681934357\n",
      "Epoch 190, Loss: 0.3648579977452755, Final Batch Loss: 0.03442889451980591\n",
      "Epoch 191, Loss: 0.3449827693402767, Final Batch Loss: 0.08054771274328232\n",
      "Epoch 192, Loss: 0.32532088831067085, Final Batch Loss: 0.07423706352710724\n",
      "Epoch 193, Loss: 0.388985238969326, Final Batch Loss: 0.06341500580310822\n",
      "Epoch 194, Loss: 0.364511214196682, Final Batch Loss: 0.09702476114034653\n",
      "Epoch 195, Loss: 0.3291956000030041, Final Batch Loss: 0.05617057532072067\n",
      "Epoch 196, Loss: 0.28932786360383034, Final Batch Loss: 0.04548850655555725\n",
      "Epoch 197, Loss: 0.3325286842882633, Final Batch Loss: 0.10251082479953766\n",
      "Epoch 198, Loss: 0.3450920470058918, Final Batch Loss: 0.0866241306066513\n",
      "Epoch 199, Loss: 0.32680605724453926, Final Batch Loss: 0.038168225437402725\n",
      "Epoch 200, Loss: 0.34665052965283394, Final Batch Loss: 0.08337876200675964\n",
      "Epoch 201, Loss: 0.3586883917450905, Final Batch Loss: 0.10518480837345123\n",
      "Epoch 202, Loss: 0.3438990004360676, Final Batch Loss: 0.11261671781539917\n",
      "Epoch 203, Loss: 0.3316778503358364, Final Batch Loss: 0.05042664334177971\n",
      "Epoch 204, Loss: 0.3433721363544464, Final Batch Loss: 0.052297443151474\n",
      "Epoch 205, Loss: 0.2806387096643448, Final Batch Loss: 0.05985557287931442\n",
      "Epoch 206, Loss: 0.2894383557140827, Final Batch Loss: 0.059854019433259964\n",
      "Epoch 207, Loss: 0.28245149552822113, Final Batch Loss: 0.0813312977552414\n",
      "Epoch 208, Loss: 0.3087426535785198, Final Batch Loss: 0.027120962738990784\n",
      "Epoch 209, Loss: 0.3302205540239811, Final Batch Loss: 0.06958893686532974\n",
      "Epoch 210, Loss: 0.3345879055559635, Final Batch Loss: 0.07911887764930725\n",
      "Epoch 211, Loss: 0.2767677754163742, Final Batch Loss: 0.07732938230037689\n",
      "Epoch 212, Loss: 0.36925097182393074, Final Batch Loss: 0.11122169345617294\n",
      "Epoch 213, Loss: 0.30114884674549103, Final Batch Loss: 0.045061156153678894\n",
      "Epoch 214, Loss: 0.30495021492242813, Final Batch Loss: 0.0585038997232914\n",
      "Epoch 215, Loss: 0.3398361951112747, Final Batch Loss: 0.08498150110244751\n",
      "Epoch 216, Loss: 0.3377981297671795, Final Batch Loss: 0.09598016738891602\n",
      "Epoch 217, Loss: 0.30324412137269974, Final Batch Loss: 0.06954576075077057\n",
      "Epoch 218, Loss: 0.30059071257710457, Final Batch Loss: 0.06174961477518082\n",
      "Epoch 219, Loss: 0.32729633897542953, Final Batch Loss: 0.06608311831951141\n",
      "Epoch 220, Loss: 0.30017715506255627, Final Batch Loss: 0.03478406369686127\n",
      "Epoch 221, Loss: 0.3441438302397728, Final Batch Loss: 0.11166500300168991\n",
      "Epoch 222, Loss: 0.29518481343984604, Final Batch Loss: 0.03339485451579094\n",
      "Epoch 223, Loss: 0.34129902720451355, Final Batch Loss: 0.07352741807699203\n",
      "Epoch 224, Loss: 0.30939463153481483, Final Batch Loss: 0.07610150426626205\n",
      "Epoch 225, Loss: 0.2789612915366888, Final Batch Loss: 0.02821377106010914\n",
      "Epoch 226, Loss: 0.3074749633669853, Final Batch Loss: 0.036342207342386246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227, Loss: 0.2978888899087906, Final Batch Loss: 0.03890344500541687\n",
      "Epoch 228, Loss: 0.28855664283037186, Final Batch Loss: 0.03824275732040405\n",
      "Epoch 229, Loss: 0.30608367547392845, Final Batch Loss: 0.10646327584981918\n",
      "Epoch 230, Loss: 0.3020765744149685, Final Batch Loss: 0.04046574980020523\n",
      "Epoch 231, Loss: 0.28644295409321785, Final Batch Loss: 0.047820959240198135\n",
      "Epoch 232, Loss: 0.2587142698466778, Final Batch Loss: 0.062022674828767776\n",
      "Epoch 233, Loss: 0.24068605341017246, Final Batch Loss: 0.02324770949780941\n",
      "Epoch 234, Loss: 0.27974268049001694, Final Batch Loss: 0.024447910487651825\n",
      "Epoch 235, Loss: 0.29899800568819046, Final Batch Loss: 0.041200894862413406\n",
      "Epoch 236, Loss: 0.26759523153305054, Final Batch Loss: 0.06175084039568901\n",
      "Epoch 237, Loss: 0.26199712604284286, Final Batch Loss: 0.020877104252576828\n",
      "Epoch 238, Loss: 0.27429258450865746, Final Batch Loss: 0.04204438999295235\n",
      "Epoch 239, Loss: 0.29891997016966343, Final Batch Loss: 0.10854484885931015\n",
      "Epoch 240, Loss: 0.2902235947549343, Final Batch Loss: 0.08882122486829758\n",
      "Epoch 241, Loss: 0.24560203775763512, Final Batch Loss: 0.03641073405742645\n",
      "Epoch 242, Loss: 0.28691133111715317, Final Batch Loss: 0.03659035265445709\n",
      "Epoch 243, Loss: 0.26301462203264236, Final Batch Loss: 0.043608248233795166\n",
      "Epoch 244, Loss: 0.23542017117142677, Final Batch Loss: 0.038273394107818604\n",
      "Epoch 245, Loss: 0.2758193649351597, Final Batch Loss: 0.037173833698034286\n",
      "Epoch 246, Loss: 0.2926225606352091, Final Batch Loss: 0.0977640226483345\n",
      "Epoch 247, Loss: 0.30106018856167793, Final Batch Loss: 0.09891327470541\n",
      "Epoch 248, Loss: 0.30127087980508804, Final Batch Loss: 0.06492597609758377\n",
      "Epoch 249, Loss: 0.2746892645955086, Final Batch Loss: 0.053373079746961594\n",
      "Epoch 250, Loss: 0.26522956788539886, Final Batch Loss: 0.046077702194452286\n",
      "Epoch 251, Loss: 0.23551377095282078, Final Batch Loss: 0.04024391993880272\n",
      "Epoch 252, Loss: 0.29601339250802994, Final Batch Loss: 0.09720071405172348\n",
      "Epoch 253, Loss: 0.2888330798596144, Final Batch Loss: 0.08904323726892471\n",
      "Epoch 254, Loss: 0.27707239240407944, Final Batch Loss: 0.040919531136751175\n",
      "Epoch 255, Loss: 0.25033073872327805, Final Batch Loss: 0.034272342920303345\n",
      "Epoch 256, Loss: 0.30013617873191833, Final Batch Loss: 0.09599923342466354\n",
      "Epoch 257, Loss: 0.2411900907754898, Final Batch Loss: 0.0255613811314106\n",
      "Epoch 258, Loss: 0.287862092256546, Final Batch Loss: 0.06642270088195801\n",
      "Epoch 259, Loss: 0.2431262955069542, Final Batch Loss: 0.04647568613290787\n",
      "Epoch 260, Loss: 0.25104502588510513, Final Batch Loss: 0.05165598914027214\n",
      "Epoch 261, Loss: 0.20633099600672722, Final Batch Loss: 0.033383630216121674\n",
      "Epoch 262, Loss: 0.2991323284804821, Final Batch Loss: 0.08517781645059586\n",
      "Epoch 263, Loss: 0.24059557914733887, Final Batch Loss: 0.047704339027404785\n",
      "Epoch 264, Loss: 0.26595892384648323, Final Batch Loss: 0.053625594824552536\n",
      "Epoch 265, Loss: 0.2652513552457094, Final Batch Loss: 0.05266356095671654\n",
      "Epoch 266, Loss: 0.22935757413506508, Final Batch Loss: 0.05152120068669319\n",
      "Epoch 267, Loss: 0.26071444153785706, Final Batch Loss: 0.03452685847878456\n",
      "Epoch 268, Loss: 0.23267798498272896, Final Batch Loss: 0.026139035820961\n",
      "Epoch 269, Loss: 0.241159126162529, Final Batch Loss: 0.02542955055832863\n",
      "Epoch 270, Loss: 0.25666096806526184, Final Batch Loss: 0.06015865132212639\n",
      "Epoch 271, Loss: 0.22941120527684689, Final Batch Loss: 0.02022583596408367\n",
      "Epoch 272, Loss: 0.2496605273336172, Final Batch Loss: 0.023991571739315987\n",
      "Epoch 273, Loss: 0.27698557265102863, Final Batch Loss: 0.07011229544878006\n",
      "Epoch 274, Loss: 0.2796850632876158, Final Batch Loss: 0.05726676061749458\n",
      "Epoch 275, Loss: 0.28066470474004745, Final Batch Loss: 0.0814371109008789\n",
      "Epoch 276, Loss: 0.2501312419772148, Final Batch Loss: 0.06428608298301697\n",
      "Epoch 277, Loss: 0.26856961473822594, Final Batch Loss: 0.07750853896141052\n",
      "Epoch 278, Loss: 0.26499057933688164, Final Batch Loss: 0.05607198551297188\n",
      "Epoch 279, Loss: 0.23853021301329136, Final Batch Loss: 0.045977577567100525\n",
      "Epoch 280, Loss: 0.24469493702054024, Final Batch Loss: 0.05120521038770676\n",
      "Epoch 281, Loss: 0.22626947052776814, Final Batch Loss: 0.032925717532634735\n",
      "Epoch 282, Loss: 0.24656475335359573, Final Batch Loss: 0.022841349244117737\n",
      "Epoch 283, Loss: 0.27167762257158756, Final Batch Loss: 0.08200046420097351\n",
      "Epoch 284, Loss: 0.2636150289326906, Final Batch Loss: 0.0792408362030983\n",
      "Epoch 285, Loss: 0.28772416710853577, Final Batch Loss: 0.035178590565919876\n",
      "Epoch 286, Loss: 0.2392328828573227, Final Batch Loss: 0.057630863040685654\n",
      "Epoch 287, Loss: 0.2706286869943142, Final Batch Loss: 0.05345549434423447\n",
      "Epoch 288, Loss: 0.24446736089885235, Final Batch Loss: 0.021165797486901283\n",
      "Epoch 289, Loss: 0.23855792358517647, Final Batch Loss: 0.06916900724172592\n",
      "Epoch 290, Loss: 0.2532907798886299, Final Batch Loss: 0.057401109486818314\n",
      "Epoch 291, Loss: 0.22268548049032688, Final Batch Loss: 0.03372864052653313\n",
      "Epoch 292, Loss: 0.2209300473332405, Final Batch Loss: 0.018925055861473083\n",
      "Epoch 293, Loss: 0.23878792487084866, Final Batch Loss: 0.021283259615302086\n",
      "Epoch 294, Loss: 0.24506048299372196, Final Batch Loss: 0.08418963849544525\n",
      "Epoch 295, Loss: 0.20318846497684717, Final Batch Loss: 0.03301175683736801\n",
      "Epoch 296, Loss: 0.2564341016113758, Final Batch Loss: 0.06830504536628723\n",
      "Epoch 297, Loss: 0.23283774964511395, Final Batch Loss: 0.01707124523818493\n",
      "Epoch 298, Loss: 0.23448599129915237, Final Batch Loss: 0.025522228330373764\n",
      "Epoch 299, Loss: 0.2066214680671692, Final Batch Loss: 0.04568653926253319\n",
      "Epoch 300, Loss: 0.24585147574543953, Final Batch Loss: 0.04363001883029938\n",
      "Epoch 301, Loss: 0.23788293823599815, Final Batch Loss: 0.026410840451717377\n",
      "Epoch 302, Loss: 0.2000812627375126, Final Batch Loss: 0.021624216809868813\n",
      "Epoch 303, Loss: 0.21913373842835426, Final Batch Loss: 0.023797687143087387\n",
      "Epoch 304, Loss: 0.22835779562592506, Final Batch Loss: 0.05254306271672249\n",
      "Epoch 305, Loss: 0.25671014189720154, Final Batch Loss: 0.05042168125510216\n",
      "Epoch 306, Loss: 0.2542756274342537, Final Batch Loss: 0.06354366987943649\n",
      "Epoch 307, Loss: 0.23133274167776108, Final Batch Loss: 0.042228374630212784\n",
      "Epoch 308, Loss: 0.25735550560057163, Final Batch Loss: 0.07365059852600098\n",
      "Epoch 309, Loss: 0.22084906324744225, Final Batch Loss: 0.01579706184566021\n",
      "Epoch 310, Loss: 0.20821796171367168, Final Batch Loss: 0.05858974531292915\n",
      "Epoch 311, Loss: 0.25430346466600895, Final Batch Loss: 0.02808944694697857\n",
      "Epoch 312, Loss: 0.24008581787347794, Final Batch Loss: 0.06564183533191681\n",
      "Epoch 313, Loss: 0.23501086048781872, Final Batch Loss: 0.026734773069620132\n",
      "Epoch 314, Loss: 0.2371952198445797, Final Batch Loss: 0.030355684459209442\n",
      "Epoch 315, Loss: 0.22080453857779503, Final Batch Loss: 0.0741066113114357\n",
      "Epoch 316, Loss: 0.23014833591878414, Final Batch Loss: 0.029632439836859703\n",
      "Epoch 317, Loss: 0.25616105273365974, Final Batch Loss: 0.03878754377365112\n",
      "Epoch 318, Loss: 0.23502055928111076, Final Batch Loss: 0.051659293472766876\n",
      "Epoch 319, Loss: 0.23360388353466988, Final Batch Loss: 0.07293294370174408\n",
      "Epoch 320, Loss: 0.23564007878303528, Final Batch Loss: 0.08169743418693542\n",
      "Epoch 321, Loss: 0.22824008390307426, Final Batch Loss: 0.07658780366182327\n",
      "Epoch 322, Loss: 0.2025694828480482, Final Batch Loss: 0.04900592193007469\n",
      "Epoch 323, Loss: 0.20904527604579926, Final Batch Loss: 0.018719084560871124\n",
      "Epoch 324, Loss: 0.22459965385496616, Final Batch Loss: 0.03727743774652481\n",
      "Epoch 325, Loss: 0.20346948876976967, Final Batch Loss: 0.026400381699204445\n",
      "Epoch 326, Loss: 0.20828187465667725, Final Batch Loss: 0.039794303476810455\n",
      "Epoch 327, Loss: 0.21438077837228775, Final Batch Loss: 0.030883923172950745\n",
      "Epoch 328, Loss: 0.1878206506371498, Final Batch Loss: 0.02586202695965767\n",
      "Epoch 329, Loss: 0.18235977366566658, Final Batch Loss: 0.01777545176446438\n",
      "Epoch 330, Loss: 0.20111164264380932, Final Batch Loss: 0.06832997500896454\n",
      "Epoch 331, Loss: 0.22833124920725822, Final Batch Loss: 0.027491003274917603\n",
      "Epoch 332, Loss: 0.19849413633346558, Final Batch Loss: 0.04201563447713852\n",
      "Epoch 333, Loss: 0.20106809586286545, Final Batch Loss: 0.048353735357522964\n",
      "Epoch 334, Loss: 0.2408088929951191, Final Batch Loss: 0.06336839497089386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335, Loss: 0.18943083845078945, Final Batch Loss: 0.05638075992465019\n",
      "Epoch 336, Loss: 0.20304120518267155, Final Batch Loss: 0.009536635130643845\n",
      "Epoch 337, Loss: 0.23408178659155965, Final Batch Loss: 0.0066549875773489475\n",
      "Epoch 338, Loss: 0.2193615697324276, Final Batch Loss: 0.045027658343315125\n",
      "Epoch 339, Loss: 0.22566138580441475, Final Batch Loss: 0.04453626647591591\n",
      "Epoch 340, Loss: 0.22136653773486614, Final Batch Loss: 0.018854232504963875\n",
      "Epoch 341, Loss: 0.21613389253616333, Final Batch Loss: 0.05273484066128731\n",
      "Epoch 342, Loss: 0.22010930627584457, Final Batch Loss: 0.06200401857495308\n",
      "Epoch 343, Loss: 0.22954809665679932, Final Batch Loss: 0.0401654988527298\n",
      "Epoch 344, Loss: 0.2331750150769949, Final Batch Loss: 0.07388002425432205\n",
      "Epoch 345, Loss: 0.23514451924711466, Final Batch Loss: 0.07399337738752365\n",
      "Epoch 346, Loss: 0.20971996150910854, Final Batch Loss: 0.024252761155366898\n",
      "Epoch 347, Loss: 0.17870831489562988, Final Batch Loss: 0.02977127954363823\n",
      "Epoch 348, Loss: 0.2261603120714426, Final Batch Loss: 0.05834971368312836\n",
      "Epoch 349, Loss: 0.20107577927410603, Final Batch Loss: 0.06315083056688309\n",
      "Epoch 350, Loss: 0.18215397745370865, Final Batch Loss: 0.020761094987392426\n",
      "Epoch 351, Loss: 0.19476781226694584, Final Batch Loss: 0.023994723334908485\n",
      "Epoch 352, Loss: 0.18413884937763214, Final Batch Loss: 0.03778545930981636\n",
      "Epoch 353, Loss: 0.19494035094976425, Final Batch Loss: 0.00976012647151947\n",
      "Epoch 354, Loss: 0.19033202342689037, Final Batch Loss: 0.05120161920785904\n",
      "Epoch 355, Loss: 0.19569041579961777, Final Batch Loss: 0.035278432071208954\n",
      "Epoch 356, Loss: 0.20789443887770176, Final Batch Loss: 0.025149637833237648\n",
      "Epoch 357, Loss: 0.20100184343755245, Final Batch Loss: 0.028089532628655434\n",
      "Epoch 358, Loss: 0.20121354423463345, Final Batch Loss: 0.04981788992881775\n",
      "Epoch 359, Loss: 0.1926288027316332, Final Batch Loss: 0.032342322170734406\n",
      "Epoch 360, Loss: 0.20456899143755436, Final Batch Loss: 0.03943648189306259\n",
      "Epoch 361, Loss: 0.18962496146559715, Final Batch Loss: 0.04346240311861038\n",
      "Epoch 362, Loss: 0.17663370072841644, Final Batch Loss: 0.012050889432430267\n",
      "Epoch 363, Loss: 0.18877477757632732, Final Batch Loss: 0.023128224536776543\n",
      "Epoch 364, Loss: 0.1823138389736414, Final Batch Loss: 0.04201531410217285\n",
      "Epoch 365, Loss: 0.1826421432197094, Final Batch Loss: 0.03291334584355354\n",
      "Epoch 366, Loss: 0.19796090200543404, Final Batch Loss: 0.03983224928379059\n",
      "Epoch 367, Loss: 0.16375009715557098, Final Batch Loss: 0.011585831642150879\n",
      "Epoch 368, Loss: 0.21308731101453304, Final Batch Loss: 0.04086838662624359\n",
      "Epoch 369, Loss: 0.2121071657165885, Final Batch Loss: 0.06915140151977539\n",
      "Epoch 370, Loss: 0.20313190296292305, Final Batch Loss: 0.0704982727766037\n",
      "Epoch 371, Loss: 0.17310036905109882, Final Batch Loss: 0.017843421548604965\n",
      "Epoch 372, Loss: 0.1921089794486761, Final Batch Loss: 0.04693499952554703\n",
      "Epoch 373, Loss: 0.16584409028291702, Final Batch Loss: 0.02586459368467331\n",
      "Epoch 374, Loss: 0.18751509115099907, Final Batch Loss: 0.06936296075582504\n",
      "Epoch 375, Loss: 0.19880640506744385, Final Batch Loss: 0.056105587631464005\n",
      "Epoch 376, Loss: 0.16831839457154274, Final Batch Loss: 0.02006405219435692\n",
      "Epoch 377, Loss: 0.20699965581297874, Final Batch Loss: 0.030373862013220787\n",
      "Epoch 378, Loss: 0.1799647267907858, Final Batch Loss: 0.04835104942321777\n",
      "Epoch 379, Loss: 0.1840494554489851, Final Batch Loss: 0.013041261583566666\n",
      "Epoch 380, Loss: 0.18484804406762123, Final Batch Loss: 0.06077992543578148\n",
      "Epoch 381, Loss: 0.1887786965817213, Final Batch Loss: 0.05735179781913757\n",
      "Epoch 382, Loss: 0.18720854632556438, Final Batch Loss: 0.06353025883436203\n",
      "Epoch 383, Loss: 0.15010069403797388, Final Batch Loss: 0.00770645122975111\n",
      "Epoch 384, Loss: 0.18746029399335384, Final Batch Loss: 0.04624900594353676\n",
      "Epoch 385, Loss: 0.15776050090789795, Final Batch Loss: 0.023414166644215584\n",
      "Epoch 386, Loss: 0.15493604354560375, Final Batch Loss: 0.04598346725106239\n",
      "Epoch 387, Loss: 0.14086949732154608, Final Batch Loss: 0.04377234727144241\n",
      "Epoch 388, Loss: 0.22341158613562584, Final Batch Loss: 0.07989682257175446\n",
      "Epoch 389, Loss: 0.17456334829330444, Final Batch Loss: 0.026994191110134125\n",
      "Epoch 390, Loss: 0.15442637354135513, Final Batch Loss: 0.020420586690306664\n",
      "Epoch 391, Loss: 0.1723252795636654, Final Batch Loss: 0.02109534665942192\n",
      "Epoch 392, Loss: 0.1868384601548314, Final Batch Loss: 0.013440790586173534\n",
      "Epoch 393, Loss: 0.1502027465030551, Final Batch Loss: 0.014639944769442081\n",
      "Epoch 394, Loss: 0.17977141588926315, Final Batch Loss: 0.031064437702298164\n",
      "Epoch 395, Loss: 0.182098887860775, Final Batch Loss: 0.01702496036887169\n",
      "Epoch 396, Loss: 0.13510720431804657, Final Batch Loss: 0.033944401890039444\n",
      "Epoch 397, Loss: 0.16150903515517712, Final Batch Loss: 0.04188161343336105\n",
      "Epoch 398, Loss: 0.22422879189252853, Final Batch Loss: 0.0973934680223465\n",
      "Epoch 399, Loss: 0.1751871770247817, Final Batch Loss: 0.037498265504837036\n",
      "Epoch 400, Loss: 0.16601665318012238, Final Batch Loss: 0.04067860543727875\n",
      "Epoch 401, Loss: 0.16323146037757397, Final Batch Loss: 0.014937764033675194\n",
      "Epoch 402, Loss: 0.16122020035982132, Final Batch Loss: 0.011219225823879242\n",
      "Epoch 403, Loss: 0.15533198602497578, Final Batch Loss: 0.02111729048192501\n",
      "Epoch 404, Loss: 0.16444143932312727, Final Batch Loss: 0.027237068861722946\n",
      "Epoch 405, Loss: 0.20052981190383434, Final Batch Loss: 0.09757435321807861\n",
      "Epoch 406, Loss: 0.15027325507253408, Final Batch Loss: 0.03229229524731636\n",
      "Epoch 407, Loss: 0.181507870554924, Final Batch Loss: 0.033413078635931015\n",
      "Epoch 408, Loss: 0.15739034861326218, Final Batch Loss: 0.03474883362650871\n",
      "Epoch 409, Loss: 0.16818471811711788, Final Batch Loss: 0.033426590263843536\n",
      "Epoch 410, Loss: 0.13846406526863575, Final Batch Loss: 0.015330161899328232\n",
      "Epoch 411, Loss: 0.2051988271996379, Final Batch Loss: 0.03512061759829521\n",
      "Epoch 412, Loss: 0.16028800141066313, Final Batch Loss: 0.01860361546278\n",
      "Epoch 413, Loss: 0.14154342841356993, Final Batch Loss: 0.030227480456233025\n",
      "Epoch 414, Loss: 0.15324817318469286, Final Batch Loss: 0.037425387650728226\n",
      "Epoch 415, Loss: 0.1388939470052719, Final Batch Loss: 0.019519014284014702\n",
      "Epoch 416, Loss: 0.15976938977837563, Final Batch Loss: 0.013687122613191605\n",
      "Epoch 417, Loss: 0.1374811576679349, Final Batch Loss: 0.052469559013843536\n",
      "Epoch 418, Loss: 0.14786480739712715, Final Batch Loss: 0.014014696702361107\n",
      "Epoch 419, Loss: 0.1520751565694809, Final Batch Loss: 0.03750474005937576\n",
      "Epoch 420, Loss: 0.1300973305478692, Final Batch Loss: 0.011982793919742107\n",
      "Epoch 421, Loss: 0.15748444013297558, Final Batch Loss: 0.029900817200541496\n",
      "Epoch 422, Loss: 0.1652312334626913, Final Batch Loss: 0.017401844263076782\n",
      "Epoch 423, Loss: 0.14969702251255512, Final Batch Loss: 0.033085498958826065\n",
      "Epoch 424, Loss: 0.14241274632513523, Final Batch Loss: 0.03904294595122337\n",
      "Epoch 425, Loss: 0.11817300971597433, Final Batch Loss: 0.01331227645277977\n",
      "Epoch 426, Loss: 0.13869120739400387, Final Batch Loss: 0.02583993971347809\n",
      "Epoch 427, Loss: 0.16325845196843147, Final Batch Loss: 0.034003302454948425\n",
      "Epoch 428, Loss: 0.18101540580391884, Final Batch Loss: 0.06735919415950775\n",
      "Epoch 429, Loss: 0.16043815575540066, Final Batch Loss: 0.03616977483034134\n",
      "Epoch 430, Loss: 0.15545889176428318, Final Batch Loss: 0.022239208221435547\n",
      "Epoch 431, Loss: 0.14777182787656784, Final Batch Loss: 0.03882933780550957\n",
      "Epoch 432, Loss: 0.13792198337614536, Final Batch Loss: 0.03727056458592415\n",
      "Epoch 433, Loss: 0.172455295920372, Final Batch Loss: 0.07140002399682999\n",
      "Epoch 434, Loss: 0.17045466415584087, Final Batch Loss: 0.02549266628921032\n",
      "Epoch 435, Loss: 0.1393926925957203, Final Batch Loss: 0.026720404624938965\n",
      "Epoch 436, Loss: 0.14902606140822172, Final Batch Loss: 0.07074549794197083\n",
      "Epoch 437, Loss: 0.14737858809530735, Final Batch Loss: 0.04123577103018761\n",
      "Epoch 438, Loss: 0.1143096168525517, Final Batch Loss: 0.016740385442972183\n",
      "Epoch 439, Loss: 0.17281031794846058, Final Batch Loss: 0.03669688105583191\n",
      "Epoch 440, Loss: 0.14062798582017422, Final Batch Loss: 0.024930492043495178\n",
      "Epoch 441, Loss: 0.15754639450460672, Final Batch Loss: 0.024339746683835983\n",
      "Epoch 442, Loss: 0.1369798444211483, Final Batch Loss: 0.032787878066301346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443, Loss: 0.1160535542294383, Final Batch Loss: 0.008093657903373241\n",
      "Epoch 444, Loss: 0.10227412916719913, Final Batch Loss: 0.02862432412803173\n",
      "Epoch 445, Loss: 0.10602803714573383, Final Batch Loss: 0.014287972822785378\n",
      "Epoch 446, Loss: 0.12071664538234472, Final Batch Loss: 0.010308099910616875\n",
      "Epoch 447, Loss: 0.11509758792817593, Final Batch Loss: 0.049434319138526917\n",
      "Epoch 448, Loss: 0.09716312144882977, Final Batch Loss: 0.013106651604175568\n",
      "Epoch 449, Loss: 0.15089672803878784, Final Batch Loss: 0.018473906442523003\n",
      "Epoch 450, Loss: 0.11675412161275744, Final Batch Loss: 0.04321037232875824\n",
      "Epoch 451, Loss: 0.12312627164646983, Final Batch Loss: 0.004892430733889341\n",
      "Epoch 452, Loss: 0.1054778452962637, Final Batch Loss: 0.02391728013753891\n",
      "Epoch 453, Loss: 0.11632441729307175, Final Batch Loss: 0.029690522700548172\n",
      "Epoch 454, Loss: 0.1558709954842925, Final Batch Loss: 0.052324213087558746\n",
      "Epoch 455, Loss: 0.11304044723510742, Final Batch Loss: 0.011914260685443878\n",
      "Epoch 456, Loss: 0.1084506819024682, Final Batch Loss: 0.008469601161777973\n",
      "Epoch 457, Loss: 0.13936312217265368, Final Batch Loss: 0.01826481707394123\n",
      "Epoch 458, Loss: 0.11634848639369011, Final Batch Loss: 0.03379577398300171\n",
      "Epoch 459, Loss: 0.11546042282134295, Final Batch Loss: 0.053624477237463\n",
      "Epoch 460, Loss: 0.08577472576871514, Final Batch Loss: 0.0098776426166296\n",
      "Epoch 461, Loss: 0.11236325930804014, Final Batch Loss: 0.017080172896385193\n",
      "Epoch 462, Loss: 0.12859690841287374, Final Batch Loss: 0.02586180344223976\n",
      "Epoch 463, Loss: 0.09029708430171013, Final Batch Loss: 0.012140817940235138\n",
      "Epoch 464, Loss: 0.13060565665364265, Final Batch Loss: 0.02457931637763977\n",
      "Epoch 465, Loss: 0.12316146865487099, Final Batch Loss: 0.012435788288712502\n",
      "Epoch 466, Loss: 0.11440586857497692, Final Batch Loss: 0.017288926988840103\n",
      "Epoch 467, Loss: 0.15462718391790986, Final Batch Loss: 0.011922397650778294\n",
      "Epoch 468, Loss: 0.08474736707285047, Final Batch Loss: 0.011548994109034538\n",
      "Epoch 469, Loss: 0.16823268961161375, Final Batch Loss: 0.0392577163875103\n",
      "Epoch 470, Loss: 0.13486120477318764, Final Batch Loss: 0.02297334000468254\n",
      "Epoch 471, Loss: 0.14353702124208212, Final Batch Loss: 0.006209101527929306\n",
      "Epoch 472, Loss: 0.105949517339468, Final Batch Loss: 0.027260256931185722\n",
      "Epoch 473, Loss: 0.1081910515204072, Final Batch Loss: 0.040733348578214645\n",
      "Epoch 474, Loss: 0.07979943417012691, Final Batch Loss: 0.007440567947924137\n",
      "Epoch 475, Loss: 0.11990971397608519, Final Batch Loss: 0.011465196497738361\n",
      "Epoch 476, Loss: 0.11778908967971802, Final Batch Loss: 0.031004218384623528\n",
      "Epoch 477, Loss: 0.09169999230653048, Final Batch Loss: 0.013635639101266861\n",
      "Epoch 478, Loss: 0.10884179919958115, Final Batch Loss: 0.02930777706205845\n",
      "Epoch 479, Loss: 0.09536161879077554, Final Batch Loss: 0.006412188988178968\n",
      "Epoch 480, Loss: 0.1013089562766254, Final Batch Loss: 0.02150695025920868\n",
      "Epoch 481, Loss: 0.11281631235033274, Final Batch Loss: 0.010935216210782528\n",
      "Epoch 482, Loss: 0.09462480805814266, Final Batch Loss: 0.028435874730348587\n",
      "Epoch 483, Loss: 0.055864266119897366, Final Batch Loss: 0.006852323655039072\n",
      "Epoch 484, Loss: 0.14304649643599987, Final Batch Loss: 0.030884988605976105\n",
      "Epoch 485, Loss: 0.09095130115747452, Final Batch Loss: 0.027978595346212387\n",
      "Epoch 486, Loss: 0.07629308849573135, Final Batch Loss: 0.01056673564016819\n",
      "Epoch 487, Loss: 0.0947004803456366, Final Batch Loss: 0.04478319734334946\n",
      "Epoch 488, Loss: 0.08676501363515854, Final Batch Loss: 0.00853218138217926\n",
      "Epoch 489, Loss: 0.0769375367090106, Final Batch Loss: 0.016144270077347755\n",
      "Epoch 490, Loss: 0.08501429785974324, Final Batch Loss: 0.006930867675691843\n",
      "Epoch 491, Loss: 0.1514267362654209, Final Batch Loss: 0.03775189071893692\n",
      "Epoch 492, Loss: 0.1182456323876977, Final Batch Loss: 0.03777920827269554\n",
      "Epoch 493, Loss: 0.1603380562737584, Final Batch Loss: 0.026952121406793594\n",
      "Epoch 494, Loss: 0.18489479087293148, Final Batch Loss: 0.06254283338785172\n",
      "Epoch 495, Loss: 0.10308393277227879, Final Batch Loss: 0.02611798793077469\n",
      "Epoch 496, Loss: 0.1102402787655592, Final Batch Loss: 0.04365408420562744\n",
      "Epoch 497, Loss: 0.10805689450353384, Final Batch Loss: 0.014429210685193539\n",
      "Epoch 498, Loss: 0.1263998867943883, Final Batch Loss: 0.030052442103624344\n",
      "Epoch 499, Loss: 0.0960537800565362, Final Batch Loss: 0.0062175653874874115\n",
      "Epoch 500, Loss: 0.06753152888268232, Final Batch Loss: 0.008092508651316166\n",
      "Epoch 501, Loss: 0.11967439949512482, Final Batch Loss: 0.01556505635380745\n",
      "Epoch 502, Loss: 0.11681949719786644, Final Batch Loss: 0.051476575434207916\n",
      "Epoch 503, Loss: 0.10300510190427303, Final Batch Loss: 0.013643183745443821\n",
      "Epoch 504, Loss: 0.09214378893375397, Final Batch Loss: 0.005857483483850956\n",
      "Epoch 505, Loss: 0.07061840780079365, Final Batch Loss: 0.006290926598012447\n",
      "Epoch 506, Loss: 0.10226399730890989, Final Batch Loss: 0.028458265587687492\n",
      "Epoch 507, Loss: 0.11176595743745565, Final Batch Loss: 0.019057616591453552\n",
      "Epoch 508, Loss: 0.11813565902411938, Final Batch Loss: 0.026446325704455376\n",
      "Epoch 509, Loss: 0.08522072900086641, Final Batch Loss: 0.00807001069188118\n",
      "Epoch 510, Loss: 0.10280956234782934, Final Batch Loss: 0.005207469686865807\n",
      "Epoch 511, Loss: 0.07018415769562125, Final Batch Loss: 0.005474820267409086\n",
      "Epoch 512, Loss: 0.086435092613101, Final Batch Loss: 0.009310838766396046\n",
      "Epoch 513, Loss: 0.12396722380071878, Final Batch Loss: 0.049170732498168945\n",
      "Epoch 514, Loss: 0.0848299041390419, Final Batch Loss: 0.010891123674809933\n",
      "Epoch 515, Loss: 0.08283161371946335, Final Batch Loss: 0.03419032320380211\n",
      "Epoch 516, Loss: 0.09483847487717867, Final Batch Loss: 0.04362243041396141\n",
      "Epoch 517, Loss: 0.057730041444301605, Final Batch Loss: 0.01696048118174076\n",
      "Epoch 518, Loss: 0.09912916459143162, Final Batch Loss: 0.009054392576217651\n",
      "Epoch 519, Loss: 0.1253821887075901, Final Batch Loss: 0.05324317887425423\n",
      "Epoch 520, Loss: 0.10603872872889042, Final Batch Loss: 0.041992586106061935\n",
      "Epoch 521, Loss: 0.07977965008467436, Final Batch Loss: 0.008286030031740665\n",
      "Epoch 522, Loss: 0.06003117701038718, Final Batch Loss: 0.002707442734390497\n",
      "Epoch 523, Loss: 0.07409456465393305, Final Batch Loss: 0.01026163063943386\n",
      "Epoch 524, Loss: 0.10559655260294676, Final Batch Loss: 0.06379522383213043\n",
      "Epoch 525, Loss: 0.10410226765088737, Final Batch Loss: 0.013521521352231503\n",
      "Epoch 526, Loss: 0.09706939966417849, Final Batch Loss: 0.0034508199896663427\n",
      "Epoch 527, Loss: 0.09056708496063948, Final Batch Loss: 0.022075330838561058\n",
      "Epoch 528, Loss: 0.06945151556283236, Final Batch Loss: 0.005027062259614468\n",
      "Epoch 529, Loss: 0.07289484096691012, Final Batch Loss: 0.015679093077778816\n",
      "Epoch 530, Loss: 0.08048382587730885, Final Batch Loss: 0.009273679926991463\n",
      "Epoch 531, Loss: 0.08244791580364108, Final Batch Loss: 0.00950318481773138\n",
      "Epoch 532, Loss: 0.1103886584751308, Final Batch Loss: 0.005198255646973848\n",
      "Epoch 533, Loss: 0.08279314544051886, Final Batch Loss: 0.024041926488280296\n",
      "Epoch 534, Loss: 0.0949625838547945, Final Batch Loss: 0.0037837973795831203\n",
      "Epoch 535, Loss: 0.08874156652018428, Final Batch Loss: 0.007134337909519672\n",
      "Epoch 536, Loss: 0.06802941206842661, Final Batch Loss: 0.003541788086295128\n",
      "Epoch 537, Loss: 0.11642957339063287, Final Batch Loss: 0.04298107698559761\n",
      "Epoch 538, Loss: 0.11299864109605551, Final Batch Loss: 0.007223761640489101\n",
      "Epoch 539, Loss: 0.08762106252834201, Final Batch Loss: 0.025242118164896965\n",
      "Epoch 540, Loss: 0.09203228214755654, Final Batch Loss: 0.027908502146601677\n",
      "Epoch 541, Loss: 0.06656938558444381, Final Batch Loss: 0.006344386842101812\n",
      "Epoch 542, Loss: 0.07778823957778513, Final Batch Loss: 0.002683785976842046\n",
      "Epoch 543, Loss: 0.1043770657852292, Final Batch Loss: 0.03292154148221016\n",
      "Epoch 544, Loss: 0.055928382091224194, Final Batch Loss: 0.006971285212785006\n",
      "Epoch 545, Loss: 0.0634183562360704, Final Batch Loss: 0.007600533775985241\n",
      "Epoch 546, Loss: 0.07283350778743625, Final Batch Loss: 0.009204957634210587\n",
      "Epoch 547, Loss: 0.1065178383141756, Final Batch Loss: 0.03349832072854042\n",
      "Epoch 548, Loss: 0.05927581712603569, Final Batch Loss: 0.00808012206107378\n",
      "Epoch 549, Loss: 0.07663321541622281, Final Batch Loss: 0.007091646548360586\n",
      "Epoch 550, Loss: 0.07017864962108433, Final Batch Loss: 0.001973048085346818\n",
      "Epoch 551, Loss: 0.08498169761151075, Final Batch Loss: 0.047717537730932236\n",
      "Epoch 552, Loss: 0.04554825834929943, Final Batch Loss: 0.00949584599584341\n",
      "Epoch 553, Loss: 0.08362879976630211, Final Batch Loss: 0.012490025721490383\n",
      "Epoch 554, Loss: 0.06421520793810487, Final Batch Loss: 0.008909735828638077\n",
      "Epoch 555, Loss: 0.07032448472455144, Final Batch Loss: 0.01072763279080391\n",
      "Epoch 556, Loss: 0.07923954073339701, Final Batch Loss: 0.019385498017072678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 557, Loss: 0.04800179274752736, Final Batch Loss: 0.0043265740387141705\n",
      "Epoch 558, Loss: 0.06906908610835671, Final Batch Loss: 0.019825464114546776\n",
      "Epoch 559, Loss: 0.08481833804398775, Final Batch Loss: 0.00693873967975378\n",
      "Epoch 560, Loss: 0.09582134336233139, Final Batch Loss: 0.04910266771912575\n",
      "Epoch 561, Loss: 0.10610512737184763, Final Batch Loss: 0.01956164464354515\n",
      "Epoch 562, Loss: 0.06722119729965925, Final Batch Loss: 0.00608410406857729\n",
      "Epoch 563, Loss: 0.0709356339648366, Final Batch Loss: 0.03527957201004028\n",
      "Epoch 564, Loss: 0.08970620762556791, Final Batch Loss: 0.02171352319419384\n",
      "Epoch 565, Loss: 0.09820642322301865, Final Batch Loss: 0.02388961799442768\n",
      "Epoch 566, Loss: 0.07440198585391045, Final Batch Loss: 0.003025980666279793\n",
      "Epoch 567, Loss: 0.10402579652145505, Final Batch Loss: 0.0397854819893837\n",
      "Epoch 568, Loss: 0.06618438195437193, Final Batch Loss: 0.0045052822679281235\n",
      "Epoch 569, Loss: 0.06355249043554068, Final Batch Loss: 0.014245748519897461\n",
      "Epoch 570, Loss: 0.05841355002485216, Final Batch Loss: 0.0036382859107106924\n",
      "Epoch 571, Loss: 0.04930222686380148, Final Batch Loss: 0.004815318156033754\n",
      "Epoch 572, Loss: 0.06203768402338028, Final Batch Loss: 0.02350921556353569\n",
      "Epoch 573, Loss: 0.08365041948854923, Final Batch Loss: 0.010891782119870186\n",
      "Epoch 574, Loss: 0.06855872308369726, Final Batch Loss: 0.003863818943500519\n",
      "Epoch 575, Loss: 0.024618326220661402, Final Batch Loss: 0.005504158325493336\n",
      "Epoch 576, Loss: 0.10566496243700385, Final Batch Loss: 0.014830924570560455\n",
      "Epoch 577, Loss: 0.0666484059765935, Final Batch Loss: 0.008976848796010017\n",
      "Epoch 578, Loss: 0.09249738045036793, Final Batch Loss: 0.032929498702287674\n",
      "Epoch 579, Loss: 0.05908168153837323, Final Batch Loss: 0.012486515566706657\n",
      "Epoch 580, Loss: 0.06495119677856565, Final Batch Loss: 0.006122290622442961\n",
      "Epoch 581, Loss: 0.08005514135584235, Final Batch Loss: 0.0317557193338871\n",
      "Epoch 582, Loss: 0.08599757589399815, Final Batch Loss: 0.016773154959082603\n",
      "Epoch 583, Loss: 0.08923831069841981, Final Batch Loss: 0.03795546665787697\n",
      "Epoch 584, Loss: 0.04963757726363838, Final Batch Loss: 0.006285985000431538\n",
      "Epoch 585, Loss: 0.06877256045117974, Final Batch Loss: 0.0074229477904737\n",
      "Epoch 586, Loss: 0.07346300897188485, Final Batch Loss: 0.016224030405282974\n",
      "Epoch 587, Loss: 0.059777748538181186, Final Batch Loss: 0.002939131809398532\n",
      "Epoch 588, Loss: 0.04599180677905679, Final Batch Loss: 0.017333349213004112\n",
      "Epoch 589, Loss: 0.09191541187465191, Final Batch Loss: 0.04617695137858391\n",
      "Epoch 590, Loss: 0.08231994928792119, Final Batch Loss: 0.01763763278722763\n",
      "Epoch 591, Loss: 0.04039675509557128, Final Batch Loss: 0.00697300536558032\n",
      "Epoch 592, Loss: 0.09359394013881683, Final Batch Loss: 0.025321796536445618\n",
      "Epoch 593, Loss: 0.05048471572808921, Final Batch Loss: 0.0035655556712299585\n",
      "Epoch 594, Loss: 0.07348656700924039, Final Batch Loss: 0.02938750758767128\n",
      "Epoch 595, Loss: 0.09108123555779457, Final Batch Loss: 0.007582407910376787\n",
      "Epoch 596, Loss: 0.04811278125271201, Final Batch Loss: 0.004699083976447582\n",
      "Epoch 597, Loss: 0.0719184335321188, Final Batch Loss: 0.03707428276538849\n",
      "Epoch 598, Loss: 0.051160584669560194, Final Batch Loss: 0.0036656286101788282\n",
      "Epoch 599, Loss: 0.07256893673911691, Final Batch Loss: 0.007798978593200445\n",
      "Epoch 600, Loss: 0.04959954530932009, Final Batch Loss: 0.010465886443853378\n",
      "Epoch 601, Loss: 0.04950278019532561, Final Batch Loss: 0.007691616658121347\n",
      "Epoch 602, Loss: 0.061247176956385374, Final Batch Loss: 0.008726902306079865\n",
      "Epoch 603, Loss: 0.04498664615675807, Final Batch Loss: 0.005478729959577322\n",
      "Epoch 604, Loss: 0.08253861917182803, Final Batch Loss: 0.005030973814427853\n",
      "Epoch 605, Loss: 0.03547693323343992, Final Batch Loss: 0.004182592965662479\n",
      "Epoch 606, Loss: 0.04354961682111025, Final Batch Loss: 0.0132590988650918\n",
      "Epoch 607, Loss: 0.03994820895604789, Final Batch Loss: 0.012603738345205784\n",
      "Epoch 608, Loss: 0.09249558765441179, Final Batch Loss: 0.04792337119579315\n",
      "Epoch 609, Loss: 0.05355754471383989, Final Batch Loss: 0.009248493239283562\n",
      "Epoch 610, Loss: 0.07864046329632401, Final Batch Loss: 0.00428542448207736\n",
      "Epoch 611, Loss: 0.04434046894311905, Final Batch Loss: 0.005314398091286421\n",
      "Epoch 612, Loss: 0.052130745025351644, Final Batch Loss: 0.002722051227465272\n",
      "Epoch 613, Loss: 0.03876934526488185, Final Batch Loss: 0.005374270491302013\n",
      "Epoch 614, Loss: 0.04949825629591942, Final Batch Loss: 0.013896141201257706\n",
      "Epoch 615, Loss: 0.04334650014061481, Final Batch Loss: 0.001642334391362965\n",
      "Epoch 616, Loss: 0.0494761320296675, Final Batch Loss: 0.00293942354619503\n",
      "Epoch 617, Loss: 0.06370261311531067, Final Batch Loss: 0.007266554515808821\n",
      "Epoch 618, Loss: 0.03965850477106869, Final Batch Loss: 0.0034360159188508987\n",
      "Epoch 619, Loss: 0.07243954064324498, Final Batch Loss: 0.0033168126828968525\n",
      "Epoch 620, Loss: 0.0427786149084568, Final Batch Loss: 0.003275022841989994\n",
      "Epoch 621, Loss: 0.060685981065034866, Final Batch Loss: 0.00770566938444972\n",
      "Epoch 622, Loss: 0.07028513704426587, Final Batch Loss: 0.003349999198690057\n",
      "Epoch 623, Loss: 0.06274763657711446, Final Batch Loss: 0.019762123003602028\n",
      "Epoch 624, Loss: 0.049437255365774035, Final Batch Loss: 0.02208566665649414\n",
      "Epoch 625, Loss: 0.06166369840502739, Final Batch Loss: 0.019827701151371002\n",
      "Epoch 626, Loss: 0.055328428745269775, Final Batch Loss: 0.007745750248432159\n",
      "Epoch 627, Loss: 0.050381690729409456, Final Batch Loss: 0.013646065257489681\n",
      "Epoch 628, Loss: 0.04874347010627389, Final Batch Loss: 0.014918969012796879\n",
      "Epoch 629, Loss: 0.06698270421475172, Final Batch Loss: 0.022220907732844353\n",
      "Epoch 630, Loss: 0.06553985644131899, Final Batch Loss: 0.02667423151433468\n",
      "Epoch 631, Loss: 0.10243555204942822, Final Batch Loss: 0.05105265602469444\n",
      "Epoch 632, Loss: 0.0679079033434391, Final Batch Loss: 0.030431335791945457\n",
      "Epoch 633, Loss: 0.10262933745980263, Final Batch Loss: 0.022892069071531296\n",
      "Epoch 634, Loss: 0.06977378902956843, Final Batch Loss: 0.007580742239952087\n",
      "Epoch 635, Loss: 0.05748457694426179, Final Batch Loss: 0.005791986361145973\n",
      "Epoch 636, Loss: 0.054606513818725944, Final Batch Loss: 0.0050559439696371555\n",
      "Epoch 637, Loss: 0.04800987208727747, Final Batch Loss: 0.0015110588865354657\n",
      "Epoch 638, Loss: 0.032869155053049326, Final Batch Loss: 0.0014819609932601452\n",
      "Epoch 639, Loss: 0.056501343147829175, Final Batch Loss: 0.00716387340798974\n",
      "Epoch 640, Loss: 0.049141718074679375, Final Batch Loss: 0.005410159006714821\n",
      "Epoch 641, Loss: 0.026751059107482433, Final Batch Loss: 0.009871575981378555\n",
      "Epoch 642, Loss: 0.05212197080254555, Final Batch Loss: 0.020602822303771973\n",
      "Epoch 643, Loss: 0.06747777946293354, Final Batch Loss: 0.04215208813548088\n",
      "Epoch 644, Loss: 0.03673263743985444, Final Batch Loss: 0.002285913797095418\n",
      "Epoch 645, Loss: 0.06468718964606524, Final Batch Loss: 0.021346962079405785\n",
      "Epoch 646, Loss: 0.046839755959808826, Final Batch Loss: 0.003317680209875107\n",
      "Epoch 647, Loss: 0.05052705714479089, Final Batch Loss: 0.01133707445114851\n",
      "Epoch 648, Loss: 0.07596848485991359, Final Batch Loss: 0.004267061594873667\n",
      "Epoch 649, Loss: 0.05788563872920349, Final Batch Loss: 0.0009152086568064988\n",
      "Epoch 650, Loss: 0.04001029348000884, Final Batch Loss: 0.0027046736795455217\n",
      "Epoch 651, Loss: 0.07746641617268324, Final Batch Loss: 0.005607114173471928\n",
      "Epoch 652, Loss: 0.04656775598414242, Final Batch Loss: 0.012983491644263268\n",
      "Epoch 653, Loss: 0.03477356652729213, Final Batch Loss: 0.010220358148217201\n",
      "Epoch 654, Loss: 0.08404887502547354, Final Batch Loss: 0.017247525975108147\n",
      "Epoch 655, Loss: 0.041345082223415375, Final Batch Loss: 0.006731661036610603\n",
      "Epoch 656, Loss: 0.04955150932073593, Final Batch Loss: 0.00416569784283638\n",
      "Epoch 657, Loss: 0.07389801880344748, Final Batch Loss: 0.005434548482298851\n",
      "Epoch 658, Loss: 0.05462602828629315, Final Batch Loss: 0.001727079739794135\n",
      "Epoch 659, Loss: 0.06597059685736895, Final Batch Loss: 0.028996586799621582\n",
      "Epoch 660, Loss: 0.06587020470760763, Final Batch Loss: 0.016583889722824097\n",
      "Epoch 661, Loss: 0.02230095863342285, Final Batch Loss: 0.001893392065539956\n",
      "Epoch 662, Loss: 0.03856492415070534, Final Batch Loss: 0.005635235924273729\n",
      "Epoch 663, Loss: 0.023121116333641112, Final Batch Loss: 0.001579222152940929\n",
      "Epoch 664, Loss: 0.043263114523142576, Final Batch Loss: 0.00483135087415576\n",
      "Epoch 665, Loss: 0.03182806016411632, Final Batch Loss: 0.005530385300517082\n",
      "Epoch 666, Loss: 0.037055543274618685, Final Batch Loss: 0.0007877239258959889\n",
      "Epoch 667, Loss: 0.05059353192336857, Final Batch Loss: 0.030594954267144203\n",
      "Epoch 668, Loss: 0.0526892589405179, Final Batch Loss: 0.005410881247371435\n",
      "Epoch 669, Loss: 0.025698889745399356, Final Batch Loss: 0.005486159585416317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670, Loss: 0.03770581807475537, Final Batch Loss: 0.011557154357433319\n",
      "Epoch 671, Loss: 0.03236827813088894, Final Batch Loss: 0.007203897926956415\n",
      "Epoch 672, Loss: 0.05636851186864078, Final Batch Loss: 0.02217610366642475\n",
      "Epoch 673, Loss: 0.05573554150760174, Final Batch Loss: 0.00870893057435751\n",
      "Epoch 674, Loss: 0.03591068065725267, Final Batch Loss: 0.003453223966062069\n",
      "Epoch 675, Loss: 0.03363065712619573, Final Batch Loss: 0.004168733488768339\n",
      "Epoch 676, Loss: 0.10484026465564966, Final Batch Loss: 0.07202200591564178\n",
      "Epoch 677, Loss: 0.04212882136926055, Final Batch Loss: 0.011000541038811207\n",
      "Epoch 678, Loss: 0.06697794981300831, Final Batch Loss: 0.024341441690921783\n",
      "Epoch 679, Loss: 0.030808967305347323, Final Batch Loss: 0.013002042658627033\n",
      "Epoch 680, Loss: 0.012407788715790957, Final Batch Loss: 0.000605975219514221\n",
      "Epoch 681, Loss: 0.03240743977949023, Final Batch Loss: 0.004931753035634756\n",
      "Epoch 682, Loss: 0.04099722998216748, Final Batch Loss: 0.008345628157258034\n",
      "Epoch 683, Loss: 0.05324465042212978, Final Batch Loss: 0.018536563962697983\n",
      "Epoch 684, Loss: 0.05176892154850066, Final Batch Loss: 0.008976307697594166\n",
      "Epoch 685, Loss: 0.05795479891821742, Final Batch Loss: 0.0015234006568789482\n",
      "Epoch 686, Loss: 0.014010747137945145, Final Batch Loss: 0.0019721800927072763\n",
      "Epoch 687, Loss: 0.016462790314108133, Final Batch Loss: 0.001980595989152789\n",
      "Epoch 688, Loss: 0.01355495466850698, Final Batch Loss: 0.0008732231799513102\n",
      "Epoch 689, Loss: 0.04549551289528608, Final Batch Loss: 0.0014806408435106277\n",
      "Epoch 690, Loss: 0.03922438621520996, Final Batch Loss: 0.001137132290750742\n",
      "Epoch 691, Loss: 0.07870117202401161, Final Batch Loss: 0.04906538128852844\n",
      "Epoch 692, Loss: 0.0582620952045545, Final Batch Loss: 0.005768459755927324\n",
      "Epoch 693, Loss: 0.02917817933484912, Final Batch Loss: 0.00311541766859591\n",
      "Epoch 694, Loss: 0.017130045569501817, Final Batch Loss: 0.002097516553476453\n",
      "Epoch 695, Loss: 0.031197336385957897, Final Batch Loss: 0.015264231711626053\n",
      "Epoch 696, Loss: 0.05747544066980481, Final Batch Loss: 0.0058974940329790115\n",
      "Epoch 697, Loss: 0.0336207072250545, Final Batch Loss: 0.00430134916678071\n",
      "Epoch 698, Loss: 0.09812667802907526, Final Batch Loss: 0.061018865555524826\n",
      "Epoch 699, Loss: 0.035041552037000656, Final Batch Loss: 0.004086743574589491\n",
      "Epoch 700, Loss: 0.10801923461258411, Final Batch Loss: 0.027504893019795418\n",
      "Epoch 701, Loss: 0.05895552388392389, Final Batch Loss: 0.006466940976679325\n",
      "Epoch 702, Loss: 0.03400649153627455, Final Batch Loss: 0.008326184004545212\n",
      "Epoch 703, Loss: 0.06297838676255196, Final Batch Loss: 0.013250812888145447\n",
      "Epoch 704, Loss: 0.022869335662107915, Final Batch Loss: 0.0005398673820309341\n",
      "Epoch 705, Loss: 0.052633166778832674, Final Batch Loss: 0.002251248573884368\n",
      "Epoch 706, Loss: 0.03827758098486811, Final Batch Loss: 0.0016139181097969413\n",
      "Epoch 707, Loss: 0.04237818252295256, Final Batch Loss: 0.0032431925646960735\n",
      "Epoch 708, Loss: 0.026346388971433043, Final Batch Loss: 0.002391178160905838\n",
      "Epoch 709, Loss: 0.022550479450728744, Final Batch Loss: 0.00790170393884182\n",
      "Epoch 710, Loss: 0.022011335240677, Final Batch Loss: 0.009019829332828522\n",
      "Epoch 711, Loss: 0.14738886785926297, Final Batch Loss: 0.13248378038406372\n",
      "Epoch 712, Loss: 0.030620382691267878, Final Batch Loss: 0.0011387994745746255\n",
      "Epoch 713, Loss: 0.0786730945110321, Final Batch Loss: 0.0027673346921801567\n",
      "Epoch 714, Loss: 0.045432774582877755, Final Batch Loss: 0.003563475562259555\n",
      "Epoch 715, Loss: 0.028832552023231983, Final Batch Loss: 0.004462193697690964\n",
      "Epoch 716, Loss: 0.04294580616988242, Final Batch Loss: 0.011662689968943596\n",
      "Epoch 717, Loss: 0.06105980463325977, Final Batch Loss: 0.01402552891522646\n",
      "Epoch 718, Loss: 0.03934164810925722, Final Batch Loss: 0.004882677458226681\n",
      "Epoch 719, Loss: 0.04323980456683785, Final Batch Loss: 0.003576754592359066\n",
      "Epoch 720, Loss: 0.0237867149990052, Final Batch Loss: 0.005407761782407761\n",
      "Epoch 721, Loss: 0.03555598948150873, Final Batch Loss: 0.018623709678649902\n",
      "Epoch 722, Loss: 0.05754046514630318, Final Batch Loss: 0.00424227025359869\n",
      "Epoch 723, Loss: 0.03664995636790991, Final Batch Loss: 0.006598302628844976\n",
      "Epoch 724, Loss: 0.03896145895123482, Final Batch Loss: 0.018634650856256485\n",
      "Epoch 725, Loss: 0.03677004715427756, Final Batch Loss: 0.0057082511484622955\n",
      "Epoch 726, Loss: 0.04375208227429539, Final Batch Loss: 0.0018630282720550895\n",
      "Epoch 727, Loss: 0.03154346835799515, Final Batch Loss: 0.011875024996697903\n",
      "Epoch 728, Loss: 0.051460339687764645, Final Batch Loss: 0.03463970497250557\n",
      "Epoch 729, Loss: 0.015353351947851479, Final Batch Loss: 0.0018022683216258883\n",
      "Epoch 730, Loss: 0.06561476085335016, Final Batch Loss: 0.009699935093522072\n",
      "Epoch 731, Loss: 0.04123670142143965, Final Batch Loss: 0.006468228064477444\n",
      "Epoch 732, Loss: 0.024363826494663954, Final Batch Loss: 0.0018295764457434416\n",
      "Epoch 733, Loss: 0.03768899105489254, Final Batch Loss: 0.008671591989696026\n",
      "Epoch 734, Loss: 0.06316286325454712, Final Batch Loss: 0.006463086232542992\n",
      "Epoch 735, Loss: 0.06203775969333947, Final Batch Loss: 0.045916683971881866\n",
      "Epoch 736, Loss: 0.02122802403755486, Final Batch Loss: 0.0026605569291859865\n",
      "Epoch 737, Loss: 0.03825637046247721, Final Batch Loss: 0.00377052859403193\n",
      "Epoch 738, Loss: 0.026471034390851855, Final Batch Loss: 0.004641769919544458\n",
      "Epoch 739, Loss: 0.011378116789273918, Final Batch Loss: 0.0019243670394644141\n",
      "Epoch 740, Loss: 0.03377505834214389, Final Batch Loss: 0.006252809427678585\n",
      "Epoch 741, Loss: 0.018204606603831053, Final Batch Loss: 0.0006329546449705958\n",
      "Epoch 742, Loss: 0.04992388479877263, Final Batch Loss: 0.001061021932400763\n",
      "Epoch 743, Loss: 0.02105190686415881, Final Batch Loss: 0.0024936858098953962\n",
      "Epoch 744, Loss: 0.056471551302820444, Final Batch Loss: 0.02357935719192028\n",
      "Epoch 745, Loss: 0.04307565535418689, Final Batch Loss: 0.001658999128267169\n",
      "Epoch 746, Loss: 0.028021155390888453, Final Batch Loss: 0.0074276989325881\n",
      "Epoch 747, Loss: 0.03517047211062163, Final Batch Loss: 0.022905096411705017\n",
      "Epoch 748, Loss: 0.027984011685475707, Final Batch Loss: 0.0033126280177384615\n",
      "Epoch 749, Loss: 0.04916771536227316, Final Batch Loss: 0.0010994999902322888\n",
      "Epoch 750, Loss: 0.05314739909954369, Final Batch Loss: 0.006151116918772459\n",
      "Epoch 751, Loss: 0.07850794307887554, Final Batch Loss: 0.009850575588643551\n",
      "Epoch 752, Loss: 0.04084181581856683, Final Batch Loss: 0.0003013364621438086\n",
      "Epoch 753, Loss: 0.04684386309236288, Final Batch Loss: 0.00882816407829523\n",
      "Epoch 754, Loss: 0.022392175626009703, Final Batch Loss: 0.007785690017044544\n",
      "Epoch 755, Loss: 0.04283753433264792, Final Batch Loss: 0.002475147834047675\n",
      "Epoch 756, Loss: 0.03594884788617492, Final Batch Loss: 0.0026599010452628136\n",
      "Epoch 757, Loss: 0.018436941783875227, Final Batch Loss: 0.006843599956482649\n",
      "Epoch 758, Loss: 0.03371344367042184, Final Batch Loss: 0.004450589418411255\n",
      "Epoch 759, Loss: 0.03841631195973605, Final Batch Loss: 0.0024939856957644224\n",
      "Epoch 760, Loss: 0.015172248240560293, Final Batch Loss: 0.003270793706178665\n",
      "Epoch 761, Loss: 0.05711557576432824, Final Batch Loss: 0.0023531527258455753\n",
      "Epoch 762, Loss: 0.03572930477093905, Final Batch Loss: 0.0011514754733070731\n",
      "Epoch 763, Loss: 0.03148530179169029, Final Batch Loss: 0.012891240417957306\n",
      "Epoch 764, Loss: 0.03479694703128189, Final Batch Loss: 0.00360985123552382\n",
      "Epoch 765, Loss: 0.03599970182403922, Final Batch Loss: 0.021905390545725822\n",
      "Epoch 766, Loss: 0.02480281249154359, Final Batch Loss: 0.007300203200429678\n",
      "Epoch 767, Loss: 0.04814207926392555, Final Batch Loss: 0.015067794360220432\n",
      "Epoch 768, Loss: 0.044114807969890535, Final Batch Loss: 0.0009916553972288966\n",
      "Epoch 769, Loss: 0.013597526936791837, Final Batch Loss: 0.0013153579784557223\n",
      "Epoch 770, Loss: 0.0774724967777729, Final Batch Loss: 0.0012608086690306664\n",
      "Epoch 771, Loss: 0.04662030970212072, Final Batch Loss: 0.00170235731638968\n",
      "Epoch 772, Loss: 0.12321987934410572, Final Batch Loss: 0.08326516300439835\n",
      "Epoch 773, Loss: 0.10919691901654005, Final Batch Loss: 0.010480559431016445\n",
      "Epoch 774, Loss: 0.17863418394699693, Final Batch Loss: 0.0012539331801235676\n",
      "Epoch 775, Loss: 0.04840652970597148, Final Batch Loss: 0.005305869039148092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 776, Loss: 0.06133309472352266, Final Batch Loss: 0.012784204445779324\n",
      "Epoch 777, Loss: 0.06572310207411647, Final Batch Loss: 0.004589053336530924\n",
      "Epoch 778, Loss: 0.030039167031645775, Final Batch Loss: 0.0024408067110925913\n",
      "Epoch 779, Loss: 0.026856665965169668, Final Batch Loss: 0.005175305996090174\n",
      "Epoch 780, Loss: 0.022791694849729538, Final Batch Loss: 0.0035364257637411356\n",
      "Epoch 781, Loss: 0.042484558653086424, Final Batch Loss: 0.010881789028644562\n",
      "Epoch 782, Loss: 0.031536785361822695, Final Batch Loss: 0.0008941906271502376\n",
      "Epoch 783, Loss: 0.06100999971386045, Final Batch Loss: 0.001272378140129149\n",
      "Epoch 784, Loss: 0.026405755430459976, Final Batch Loss: 0.0030988422222435474\n",
      "Epoch 785, Loss: 0.03897894034162164, Final Batch Loss: 0.005390532780438662\n",
      "Epoch 786, Loss: 0.024253308423794806, Final Batch Loss: 0.003960500471293926\n",
      "Epoch 787, Loss: 0.026382894720882177, Final Batch Loss: 0.00932823121547699\n",
      "Epoch 788, Loss: 0.028006989508867264, Final Batch Loss: 0.011629321612417698\n",
      "Epoch 789, Loss: 0.016126998118124902, Final Batch Loss: 0.0009364212164655328\n",
      "Epoch 790, Loss: 0.017991986707784235, Final Batch Loss: 0.0017374996095895767\n",
      "Epoch 791, Loss: 0.016272896667942405, Final Batch Loss: 0.0036463087890297174\n",
      "Epoch 792, Loss: 0.010463817277923226, Final Batch Loss: 0.0008000748930498958\n",
      "Epoch 793, Loss: 0.048699828796088696, Final Batch Loss: 0.006258985493332148\n",
      "Epoch 794, Loss: 0.03882010013330728, Final Batch Loss: 0.004734175279736519\n",
      "Epoch 795, Loss: 0.03490470570977777, Final Batch Loss: 0.0031054196879267693\n",
      "Epoch 796, Loss: 0.01613843534141779, Final Batch Loss: 0.0014775311574339867\n",
      "Epoch 797, Loss: 0.04006663314066827, Final Batch Loss: 0.000559402396902442\n",
      "Epoch 798, Loss: 0.030025234824279323, Final Batch Loss: 0.0013277875259518623\n",
      "Epoch 799, Loss: 0.022409808007068932, Final Batch Loss: 0.004046129994094372\n",
      "Epoch 800, Loss: 0.03887863620184362, Final Batch Loss: 0.0008918948005884886\n",
      "Epoch 801, Loss: 0.020820147125050426, Final Batch Loss: 0.004897422157227993\n",
      "Epoch 802, Loss: 0.05124463909305632, Final Batch Loss: 0.01515646930783987\n",
      "Epoch 803, Loss: 0.026053205801872537, Final Batch Loss: 0.003910284489393234\n",
      "Epoch 804, Loss: 0.02790155669208616, Final Batch Loss: 0.001367417280562222\n",
      "Epoch 805, Loss: 0.022757286671549082, Final Batch Loss: 0.001119636232033372\n",
      "Epoch 806, Loss: 0.014318296482088044, Final Batch Loss: 0.00047485684626735747\n",
      "Epoch 807, Loss: 0.02422144985757768, Final Batch Loss: 0.005414323415607214\n",
      "Epoch 808, Loss: 0.01030217605875805, Final Batch Loss: 0.000591821561101824\n",
      "Epoch 809, Loss: 0.013615958217997104, Final Batch Loss: 0.0006875964463688433\n",
      "Epoch 810, Loss: 0.02154589194105938, Final Batch Loss: 0.0005783055094070733\n",
      "Epoch 811, Loss: 0.01857875828864053, Final Batch Loss: 0.0026749917306005955\n",
      "Epoch 812, Loss: 0.028844583313912153, Final Batch Loss: 0.0009067943319678307\n",
      "Epoch 813, Loss: 0.050569082610309124, Final Batch Loss: 0.007768020033836365\n",
      "Epoch 814, Loss: 0.03974008106160909, Final Batch Loss: 0.01189019437879324\n",
      "Epoch 815, Loss: 0.06697979755699635, Final Batch Loss: 0.011372431181371212\n",
      "Epoch 816, Loss: 0.04296304355375469, Final Batch Loss: 0.026786137372255325\n",
      "Epoch 817, Loss: 0.09563461644575, Final Batch Loss: 0.0009033042006194592\n",
      "Epoch 818, Loss: 0.015349424327723682, Final Batch Loss: 0.002972069662064314\n",
      "Epoch 819, Loss: 0.06256989343091846, Final Batch Loss: 0.010741598904132843\n",
      "Epoch 820, Loss: 0.03196282219141722, Final Batch Loss: 0.004282104782760143\n",
      "Epoch 821, Loss: 0.01921945600770414, Final Batch Loss: 0.0022104906383901834\n",
      "Epoch 822, Loss: 0.06121165654622018, Final Batch Loss: 0.022049367427825928\n",
      "Epoch 823, Loss: 0.03472191793844104, Final Batch Loss: 0.007425384595990181\n",
      "Epoch 824, Loss: 0.03584586514625698, Final Batch Loss: 0.0016904753865674138\n",
      "Epoch 825, Loss: 0.023522127768956125, Final Batch Loss: 0.0015062441816553473\n",
      "Epoch 826, Loss: 0.06464033701922745, Final Batch Loss: 0.03018757700920105\n",
      "Epoch 827, Loss: 0.02353229047730565, Final Batch Loss: 0.0019048600224778056\n",
      "Epoch 828, Loss: 0.033467341447249055, Final Batch Loss: 0.022025765851140022\n",
      "Epoch 829, Loss: 0.019585364731028676, Final Batch Loss: 0.007241100072860718\n",
      "Epoch 830, Loss: 0.017058624303899705, Final Batch Loss: 0.0015268019633367658\n",
      "Epoch 831, Loss: 0.024430602323263884, Final Batch Loss: 0.0008892321493476629\n",
      "Epoch 832, Loss: 0.016915247251745313, Final Batch Loss: 0.005351584404706955\n",
      "Epoch 833, Loss: 0.037816631025634706, Final Batch Loss: 0.005812059156596661\n",
      "Epoch 834, Loss: 0.05057697254233062, Final Batch Loss: 0.031323615461587906\n",
      "Epoch 835, Loss: 0.025423783183214255, Final Batch Loss: 0.0012080931337550282\n",
      "Epoch 836, Loss: 0.04948099108878523, Final Batch Loss: 0.0008771262364462018\n",
      "Epoch 837, Loss: 0.03500755166169256, Final Batch Loss: 0.0017517099622637033\n",
      "Epoch 838, Loss: 0.0899060748051852, Final Batch Loss: 0.015645690262317657\n",
      "Epoch 839, Loss: 0.07102951366687194, Final Batch Loss: 0.019645527005195618\n",
      "Epoch 840, Loss: 0.03711432381533086, Final Batch Loss: 0.0008674247656017542\n",
      "Epoch 841, Loss: 0.027256547415163368, Final Batch Loss: 0.0023232586681842804\n",
      "Epoch 842, Loss: 0.047233667224645615, Final Batch Loss: 0.01082794088870287\n",
      "Epoch 843, Loss: 0.06968640652485192, Final Batch Loss: 0.00832888949662447\n",
      "Epoch 844, Loss: 0.03878790239105001, Final Batch Loss: 0.0008673664997331798\n",
      "Epoch 845, Loss: 0.02711533452384174, Final Batch Loss: 0.003996516577899456\n",
      "Epoch 846, Loss: 0.02552640368230641, Final Batch Loss: 0.0011978924740105867\n",
      "Epoch 847, Loss: 0.03378713031997904, Final Batch Loss: 0.0006865374161861837\n",
      "Epoch 848, Loss: 0.04856480751186609, Final Batch Loss: 0.0033839500974863768\n",
      "Epoch 849, Loss: 0.047984516248106956, Final Batch Loss: 0.010875644162297249\n",
      "Epoch 850, Loss: 0.04205412603914738, Final Batch Loss: 0.003229414578527212\n",
      "Epoch 851, Loss: 0.019419084885157645, Final Batch Loss: 0.004907969385385513\n",
      "Epoch 852, Loss: 0.018791141163092107, Final Batch Loss: 0.0006501031457446516\n",
      "Epoch 853, Loss: 0.03079897037241608, Final Batch Loss: 0.014823823235929012\n",
      "Epoch 854, Loss: 0.03669181797886267, Final Batch Loss: 0.0008066476439125836\n",
      "Epoch 855, Loss: 0.032605383079499006, Final Batch Loss: 0.007704219315201044\n",
      "Epoch 856, Loss: 0.022989411954768002, Final Batch Loss: 0.0024235930759459734\n",
      "Epoch 857, Loss: 0.07314621575642377, Final Batch Loss: 0.04112027958035469\n",
      "Epoch 858, Loss: 0.023183731827884912, Final Batch Loss: 0.0055296351201832294\n",
      "Epoch 859, Loss: 0.04548656541737728, Final Batch Loss: 0.00040760517003946006\n",
      "Epoch 860, Loss: 0.041011636378243566, Final Batch Loss: 0.02636847086250782\n",
      "Epoch 861, Loss: 0.017235911334864795, Final Batch Loss: 0.0008819972863420844\n",
      "Epoch 862, Loss: 0.09036141587421298, Final Batch Loss: 0.003936171531677246\n",
      "Epoch 863, Loss: 0.03652235236950219, Final Batch Loss: 0.0024891153443604708\n",
      "Epoch 864, Loss: 0.03382096625864506, Final Batch Loss: 0.0007831277325749397\n",
      "Epoch 865, Loss: 0.04346501745749265, Final Batch Loss: 0.004493796266615391\n",
      "Epoch 866, Loss: 0.033183734165504575, Final Batch Loss: 0.018000731244683266\n",
      "Epoch 867, Loss: 0.023458853596821427, Final Batch Loss: 0.0024113922845572233\n",
      "Epoch 868, Loss: 0.05204615858383477, Final Batch Loss: 0.010930712334811687\n",
      "Epoch 869, Loss: 0.020196624100208282, Final Batch Loss: 0.0026212958618998528\n",
      "Epoch 870, Loss: 0.06986770033836365, Final Batch Loss: 0.022471832111477852\n",
      "Epoch 871, Loss: 0.011344644066412002, Final Batch Loss: 0.0022575282491743565\n",
      "Epoch 872, Loss: 0.024220011197030544, Final Batch Loss: 0.004917253274470568\n",
      "Epoch 873, Loss: 0.04300194897223264, Final Batch Loss: 0.002367279026657343\n",
      "Epoch 874, Loss: 0.04983878368511796, Final Batch Loss: 0.004892832599580288\n",
      "Epoch 875, Loss: 0.028155487962067127, Final Batch Loss: 0.014773830771446228\n",
      "Epoch 876, Loss: 0.051435510511510074, Final Batch Loss: 0.009074359200894833\n",
      "Epoch 877, Loss: 0.038280497246887535, Final Batch Loss: 0.0006836847751401365\n",
      "Epoch 878, Loss: 0.03569905855692923, Final Batch Loss: 0.003485177643597126\n",
      "Epoch 879, Loss: 0.050544196856208146, Final Batch Loss: 0.0009697956265881658\n",
      "Epoch 880, Loss: 0.042771299136802554, Final Batch Loss: 0.023083683103322983\n",
      "Epoch 881, Loss: 0.025303527305368334, Final Batch Loss: 0.000797949091065675\n",
      "Epoch 882, Loss: 0.016552107757888734, Final Batch Loss: 0.002985674887895584\n",
      "Epoch 883, Loss: 0.027548403362743556, Final Batch Loss: 0.0036646670196205378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 884, Loss: 0.0246301757870242, Final Batch Loss: 0.011283359490334988\n",
      "Epoch 885, Loss: 0.06365557393291965, Final Batch Loss: 0.02508252114057541\n",
      "Epoch 886, Loss: 0.009887681633699685, Final Batch Loss: 0.002498046262189746\n",
      "Epoch 887, Loss: 0.026652422035112977, Final Batch Loss: 0.005111553240567446\n",
      "Epoch 888, Loss: 0.06413097796030343, Final Batch Loss: 0.005362160503864288\n",
      "Epoch 889, Loss: 0.038857305655255914, Final Batch Loss: 0.02575339376926422\n",
      "Epoch 890, Loss: 0.04913924913853407, Final Batch Loss: 0.006316532846540213\n",
      "Epoch 891, Loss: 0.05145163112320006, Final Batch Loss: 0.000803615665063262\n",
      "Epoch 892, Loss: 0.012359701213426888, Final Batch Loss: 0.0017755134031176567\n",
      "Epoch 893, Loss: 0.05030094110406935, Final Batch Loss: 0.03162752836942673\n",
      "Epoch 894, Loss: 0.015906035783700645, Final Batch Loss: 0.00331690302118659\n",
      "Epoch 895, Loss: 0.018762483028694987, Final Batch Loss: 0.0024644695222377777\n",
      "Epoch 896, Loss: 0.01742174301762134, Final Batch Loss: 0.001622017240151763\n",
      "Epoch 897, Loss: 0.023837402695789933, Final Batch Loss: 0.006598837208002806\n",
      "Epoch 898, Loss: 0.03288370976224542, Final Batch Loss: 0.011967303231358528\n",
      "Epoch 899, Loss: 0.0670368152204901, Final Batch Loss: 0.02627960965037346\n",
      "Epoch 900, Loss: 0.05341902968939394, Final Batch Loss: 0.0011345908278599381\n",
      "Epoch 901, Loss: 0.02533217379823327, Final Batch Loss: 0.008841278031468391\n",
      "Epoch 902, Loss: 0.05224699480459094, Final Batch Loss: 0.003192002885043621\n",
      "Epoch 903, Loss: 0.02424758649431169, Final Batch Loss: 0.003120423061773181\n",
      "Epoch 904, Loss: 0.028612267517019063, Final Batch Loss: 0.0009373562061227858\n",
      "Epoch 905, Loss: 0.02357940236106515, Final Batch Loss: 0.001035226508975029\n",
      "Epoch 906, Loss: 0.04796365008223802, Final Batch Loss: 0.0011317568132653832\n",
      "Epoch 907, Loss: 0.01907083875266835, Final Batch Loss: 0.0009633090230636299\n",
      "Epoch 908, Loss: 0.020135429687798023, Final Batch Loss: 0.00035529141314327717\n",
      "Epoch 909, Loss: 0.03778063599020243, Final Batch Loss: 0.0029017545748502016\n",
      "Epoch 910, Loss: 0.03741418267600238, Final Batch Loss: 0.0015520949382334948\n",
      "Epoch 911, Loss: 0.030619297991506755, Final Batch Loss: 0.0011200052686035633\n",
      "Epoch 912, Loss: 0.024687565921340138, Final Batch Loss: 0.0007016740855760872\n",
      "Epoch 913, Loss: 0.02577861864119768, Final Batch Loss: 0.009846641682088375\n",
      "Epoch 914, Loss: 0.027292971732094884, Final Batch Loss: 0.0017056337092071772\n",
      "Epoch 915, Loss: 0.03164136887062341, Final Batch Loss: 0.01855883002281189\n",
      "Epoch 916, Loss: 0.015542537323199213, Final Batch Loss: 0.0011027985019609332\n",
      "Epoch 917, Loss: 0.013374247879255563, Final Batch Loss: 0.0023756036534905434\n",
      "Epoch 918, Loss: 0.011839561047963798, Final Batch Loss: 0.0018021776340901852\n",
      "Epoch 919, Loss: 0.026746397139504552, Final Batch Loss: 0.018000861629843712\n",
      "Epoch 920, Loss: 0.015062069724081084, Final Batch Loss: 0.01068384200334549\n",
      "Epoch 921, Loss: 0.029350423719733953, Final Batch Loss: 0.004604181740432978\n",
      "Epoch 922, Loss: 0.012199360484373756, Final Batch Loss: 0.00467248260974884\n",
      "Epoch 923, Loss: 0.016424424713477492, Final Batch Loss: 0.003062585135921836\n",
      "Epoch 924, Loss: 0.03742895263712853, Final Batch Loss: 0.0017058703815564513\n",
      "Epoch 925, Loss: 0.015747173340059817, Final Batch Loss: 0.005608937703073025\n",
      "Epoch 926, Loss: 0.008356593491043895, Final Batch Loss: 0.0003459449508227408\n",
      "Epoch 927, Loss: 0.05021213530562818, Final Batch Loss: 0.022187478840351105\n",
      "Epoch 928, Loss: 0.0676652843831107, Final Batch Loss: 0.011807754635810852\n",
      "Epoch 929, Loss: 0.02646462630946189, Final Batch Loss: 0.0025305349845439196\n",
      "Epoch 930, Loss: 0.014662679692264646, Final Batch Loss: 0.004043837543576956\n",
      "Epoch 931, Loss: 0.08813823363743722, Final Batch Loss: 0.01948646269738674\n",
      "Epoch 932, Loss: 0.043169664102606475, Final Batch Loss: 0.001355215790681541\n",
      "Epoch 933, Loss: 0.017908986192196608, Final Batch Loss: 0.003381108632311225\n",
      "Epoch 934, Loss: 0.015886492794379592, Final Batch Loss: 0.003128361888229847\n",
      "Epoch 935, Loss: 0.05356453242711723, Final Batch Loss: 0.032259516417980194\n",
      "Epoch 936, Loss: 0.017098668962717056, Final Batch Loss: 0.003159311367198825\n",
      "Epoch 937, Loss: 0.032436134992167354, Final Batch Loss: 0.023749815300107002\n",
      "Epoch 938, Loss: 0.022640325594693422, Final Batch Loss: 0.0005739529151469469\n",
      "Epoch 939, Loss: 0.01938434422481805, Final Batch Loss: 0.001687771873548627\n",
      "Epoch 940, Loss: 0.010382596869021654, Final Batch Loss: 0.0023607201874256134\n",
      "Epoch 941, Loss: 0.03612473316024989, Final Batch Loss: 0.011191760189831257\n",
      "Epoch 942, Loss: 0.01281596627086401, Final Batch Loss: 0.00213127420283854\n",
      "Epoch 943, Loss: 0.10932979767676443, Final Batch Loss: 0.04227827489376068\n",
      "Epoch 944, Loss: 0.020821747661102563, Final Batch Loss: 0.0005840652156621218\n",
      "Epoch 945, Loss: 0.04153678839793429, Final Batch Loss: 0.0007715672836638987\n",
      "Epoch 946, Loss: 0.01987852598540485, Final Batch Loss: 0.0020997270476073027\n",
      "Epoch 947, Loss: 0.012941259250510484, Final Batch Loss: 0.000826083414722234\n",
      "Epoch 948, Loss: 0.026739523600554094, Final Batch Loss: 0.00015427477774210274\n",
      "Epoch 949, Loss: 0.011998615460470319, Final Batch Loss: 0.0015007585752755404\n",
      "Epoch 950, Loss: 0.012040622066706419, Final Batch Loss: 0.003585078287869692\n",
      "Epoch 951, Loss: 0.018983024638146162, Final Batch Loss: 0.0045339916832745075\n",
      "Epoch 952, Loss: 0.01769515557680279, Final Batch Loss: 0.002334348391741514\n",
      "Epoch 953, Loss: 0.013542371219955385, Final Batch Loss: 0.001171537209302187\n",
      "Epoch 954, Loss: 0.012482639460358769, Final Batch Loss: 0.00309366756118834\n",
      "Epoch 955, Loss: 0.020812941307667643, Final Batch Loss: 0.005238748155534267\n",
      "Epoch 956, Loss: 0.02102514251600951, Final Batch Loss: 0.014059758745133877\n",
      "Epoch 957, Loss: 0.00802063214359805, Final Batch Loss: 0.0029981653206050396\n",
      "Epoch 958, Loss: 0.03946791449561715, Final Batch Loss: 0.004310340154916048\n",
      "Epoch 959, Loss: 0.009100785769987851, Final Batch Loss: 0.0031591288279742002\n",
      "Epoch 960, Loss: 0.04124520136974752, Final Batch Loss: 0.0035653673112392426\n",
      "Epoch 961, Loss: 0.004483184864511713, Final Batch Loss: 0.00016996657359413803\n",
      "Epoch 962, Loss: 0.009947183774784207, Final Batch Loss: 0.00252363714389503\n",
      "Epoch 963, Loss: 0.017063683219021186, Final Batch Loss: 0.004356021992862225\n",
      "Epoch 964, Loss: 0.03944060251524206, Final Batch Loss: 0.03569808602333069\n",
      "Epoch 965, Loss: 0.012884984142147005, Final Batch Loss: 0.0020677216816693544\n",
      "Epoch 966, Loss: 0.006996533935307525, Final Batch Loss: 0.0001547967258375138\n",
      "Epoch 967, Loss: 0.025997096206992865, Final Batch Loss: 0.001273803529329598\n",
      "Epoch 968, Loss: 0.008797719201538712, Final Batch Loss: 0.0012857054825872183\n",
      "Epoch 969, Loss: 0.020160257234238088, Final Batch Loss: 0.0011587821645662189\n",
      "Epoch 970, Loss: 0.01450668481993489, Final Batch Loss: 0.0028604348190128803\n",
      "Epoch 971, Loss: 0.016867502767127007, Final Batch Loss: 0.011758200824260712\n",
      "Epoch 972, Loss: 0.06367214702186175, Final Batch Loss: 0.00036319068749435246\n",
      "Epoch 973, Loss: 0.0254907445050776, Final Batch Loss: 0.009064709767699242\n",
      "Epoch 974, Loss: 0.02942736304248683, Final Batch Loss: 0.0055454508401453495\n",
      "Epoch 975, Loss: 0.017532456782646477, Final Batch Loss: 0.002254382474347949\n",
      "Epoch 976, Loss: 0.0225458828790579, Final Batch Loss: 0.019748898223042488\n",
      "Epoch 977, Loss: 0.06699543201830238, Final Batch Loss: 0.001805157051421702\n",
      "Epoch 978, Loss: 0.026763054309412837, Final Batch Loss: 0.015664836391806602\n",
      "Epoch 979, Loss: 0.01079787005437538, Final Batch Loss: 0.002013811143115163\n",
      "Epoch 980, Loss: 0.013188404031097889, Final Batch Loss: 0.0006751426262781024\n",
      "Epoch 981, Loss: 0.01688257569912821, Final Batch Loss: 0.002055537886917591\n",
      "Epoch 982, Loss: 0.03253976360429078, Final Batch Loss: 0.0029586495365947485\n",
      "Epoch 983, Loss: 0.031631946098059416, Final Batch Loss: 0.0009339178213849664\n",
      "Epoch 984, Loss: 0.011034574155928567, Final Batch Loss: 0.0012055878760293126\n",
      "Epoch 985, Loss: 0.006170029519125819, Final Batch Loss: 0.0002749382983893156\n",
      "Epoch 986, Loss: 0.018535452079959214, Final Batch Loss: 0.001096893916837871\n",
      "Epoch 987, Loss: 0.025818741414695978, Final Batch Loss: 0.0016411487013101578\n",
      "Epoch 988, Loss: 0.053173749474808574, Final Batch Loss: 0.03653513640165329\n",
      "Epoch 989, Loss: 0.033402176573872566, Final Batch Loss: 0.007070491090416908\n",
      "Epoch 990, Loss: 0.024756186408922076, Final Batch Loss: 0.0020905646961182356\n",
      "Epoch 991, Loss: 0.012690028233919293, Final Batch Loss: 0.0007488669943995774\n",
      "Epoch 992, Loss: 0.051554407458752394, Final Batch Loss: 0.0019809578079730272\n",
      "Epoch 993, Loss: 0.009880648576654494, Final Batch Loss: 0.0010640929685905576\n",
      "Epoch 994, Loss: 0.02498788689263165, Final Batch Loss: 0.013808819465339184\n",
      "Epoch 995, Loss: 0.029352444456890225, Final Batch Loss: 0.009164578281342983\n",
      "Epoch 996, Loss: 0.035224154649768025, Final Batch Loss: 0.000520265253726393\n",
      "Epoch 997, Loss: 0.032426675606984645, Final Batch Loss: 0.0006375437951646745\n",
      "Epoch 998, Loss: 0.019700258737429976, Final Batch Loss: 0.0028330727946013212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999, Loss: 0.02479518039035611, Final Batch Loss: 0.01485627144575119\n",
      "Epoch 1000, Loss: 0.04131770436652005, Final Batch Loss: 0.001016424736008048\n",
      "Epoch 1001, Loss: 0.038596082013100386, Final Batch Loss: 0.017233729362487793\n",
      "Epoch 1002, Loss: 0.01611636925372295, Final Batch Loss: 0.00035849338746629655\n",
      "Epoch 1003, Loss: 0.016944086703006178, Final Batch Loss: 0.0013668452156707644\n",
      "Epoch 1004, Loss: 0.042085320805199444, Final Batch Loss: 0.025674454867839813\n",
      "Epoch 1005, Loss: 0.006762124190572649, Final Batch Loss: 0.0007430317346006632\n",
      "Epoch 1006, Loss: 0.02264962682966143, Final Batch Loss: 0.0014771431451663375\n",
      "Epoch 1007, Loss: 0.03350579587277025, Final Batch Loss: 0.004709375090897083\n",
      "Epoch 1008, Loss: 0.017918980331160128, Final Batch Loss: 0.004494049586355686\n",
      "Epoch 1009, Loss: 0.010409966460429132, Final Batch Loss: 0.0005829845322296023\n",
      "Epoch 1010, Loss: 0.021357843826990575, Final Batch Loss: 0.0006289427983574569\n",
      "Epoch 1011, Loss: 0.03256488929037005, Final Batch Loss: 0.006571453996002674\n",
      "Epoch 1012, Loss: 0.04051472630817443, Final Batch Loss: 0.001743223168887198\n",
      "Epoch 1013, Loss: 0.06452172598801553, Final Batch Loss: 0.002484972355887294\n",
      "Epoch 1014, Loss: 0.014276112720835954, Final Batch Loss: 0.0004542948445305228\n",
      "Epoch 1015, Loss: 0.017659505363553762, Final Batch Loss: 0.0014274033019319177\n",
      "Epoch 1016, Loss: 0.01337609684560448, Final Batch Loss: 0.0009950805688276887\n",
      "Epoch 1017, Loss: 0.013595279247965664, Final Batch Loss: 0.0021737588103860617\n",
      "Epoch 1018, Loss: 0.013810322619974613, Final Batch Loss: 0.002978699281811714\n",
      "Epoch 1019, Loss: 0.04180584260029718, Final Batch Loss: 0.010241535492241383\n",
      "Epoch 1020, Loss: 0.10828997660428286, Final Batch Loss: 0.00657952856272459\n",
      "Epoch 1021, Loss: 0.007399378489935771, Final Batch Loss: 0.00012926154886372387\n",
      "Epoch 1022, Loss: 0.07588778389617801, Final Batch Loss: 0.006473655346781015\n",
      "Epoch 1023, Loss: 0.022576044138986617, Final Batch Loss: 0.00689941830933094\n",
      "Epoch 1024, Loss: 0.023314612451940775, Final Batch Loss: 0.008813693188130856\n",
      "Epoch 1025, Loss: 0.05068184219999239, Final Batch Loss: 0.004474705085158348\n",
      "Epoch 1026, Loss: 0.03894735558424145, Final Batch Loss: 0.005406167823821306\n",
      "Epoch 1027, Loss: 0.014345650968607515, Final Batch Loss: 0.0034658850636333227\n",
      "Epoch 1028, Loss: 0.07511984510347247, Final Batch Loss: 0.007564084138721228\n",
      "Epoch 1029, Loss: 0.03996189354802482, Final Batch Loss: 0.0002905057917814702\n",
      "Epoch 1030, Loss: 0.04998038010671735, Final Batch Loss: 0.006880704779177904\n",
      "Epoch 1031, Loss: 0.012317875283770263, Final Batch Loss: 0.0017137224785983562\n",
      "Epoch 1032, Loss: 0.018080458918120712, Final Batch Loss: 0.0005222282488830388\n",
      "Epoch 1033, Loss: 0.022954550222493708, Final Batch Loss: 0.0021405378356575966\n",
      "Epoch 1034, Loss: 0.025206276681274176, Final Batch Loss: 0.0009004832245409489\n",
      "Epoch 1035, Loss: 0.03396898484788835, Final Batch Loss: 0.022334404289722443\n",
      "Epoch 1036, Loss: 0.00654832465806976, Final Batch Loss: 0.0005842826212756336\n",
      "Epoch 1037, Loss: 0.05544171773362905, Final Batch Loss: 0.0014210189692676067\n",
      "Epoch 1038, Loss: 0.01130141265457496, Final Batch Loss: 0.002120255259796977\n",
      "Epoch 1039, Loss: 0.011023985571227968, Final Batch Loss: 0.003824677085503936\n",
      "Epoch 1040, Loss: 0.04819968465017155, Final Batch Loss: 0.0008502533310092986\n",
      "Epoch 1041, Loss: 0.06447005120571703, Final Batch Loss: 0.04518068581819534\n",
      "Epoch 1042, Loss: 0.013963709527160972, Final Batch Loss: 0.002382530365139246\n",
      "Epoch 1043, Loss: 0.010903976217377931, Final Batch Loss: 0.0027066448237746954\n",
      "Epoch 1044, Loss: 0.02786616183584556, Final Batch Loss: 0.000672386318910867\n",
      "Epoch 1045, Loss: 0.014059925801120698, Final Batch Loss: 0.0018355209613218904\n",
      "Epoch 1046, Loss: 0.03752551451907493, Final Batch Loss: 0.00035334101994521916\n",
      "Epoch 1047, Loss: 0.03496063547208905, Final Batch Loss: 0.0048161884769797325\n",
      "Epoch 1048, Loss: 0.036248170770704746, Final Batch Loss: 0.0035806663800030947\n",
      "Epoch 1049, Loss: 0.004579955886583775, Final Batch Loss: 0.0014571985229849815\n",
      "Epoch 1050, Loss: 0.04978446173481643, Final Batch Loss: 0.020614126697182655\n",
      "Epoch 1051, Loss: 0.02973716298583895, Final Batch Loss: 0.006236849818378687\n",
      "Epoch 1052, Loss: 0.022892911569215357, Final Batch Loss: 0.011180618777871132\n",
      "Epoch 1053, Loss: 0.018487609340809286, Final Batch Loss: 0.00935360137373209\n",
      "Epoch 1054, Loss: 0.01569546209066175, Final Batch Loss: 0.00045421606046147645\n",
      "Epoch 1055, Loss: 0.029516015492845327, Final Batch Loss: 0.0007169251912273467\n",
      "Epoch 1056, Loss: 0.016553272143937647, Final Batch Loss: 0.0016894231084734201\n",
      "Epoch 1057, Loss: 0.012538109673187137, Final Batch Loss: 0.007629820611327887\n",
      "Epoch 1058, Loss: 0.022163389658089727, Final Batch Loss: 0.0026110620237886906\n",
      "Epoch 1059, Loss: 0.04133378295227885, Final Batch Loss: 0.0014748258981853724\n",
      "Epoch 1060, Loss: 0.015559853869490325, Final Batch Loss: 0.004127798601984978\n",
      "Epoch 1061, Loss: 0.07409194763749838, Final Batch Loss: 0.050863489508628845\n",
      "Epoch 1062, Loss: 0.02063198760151863, Final Batch Loss: 0.0005354038439691067\n",
      "Epoch 1063, Loss: 0.007328203762881458, Final Batch Loss: 0.0012298299698159099\n",
      "Epoch 1064, Loss: 0.021605515212286264, Final Batch Loss: 0.004278986249119043\n",
      "Epoch 1065, Loss: 0.01844264304963872, Final Batch Loss: 0.002729143016040325\n",
      "Epoch 1066, Loss: 0.04581074899761006, Final Batch Loss: 0.0016067782416939735\n",
      "Epoch 1067, Loss: 0.03623624151805416, Final Batch Loss: 0.016564011573791504\n",
      "Epoch 1068, Loss: 0.03518498130142689, Final Batch Loss: 0.0018389555625617504\n",
      "Epoch 1069, Loss: 0.03248908830573782, Final Batch Loss: 0.02586505375802517\n",
      "Epoch 1070, Loss: 0.04263915529008955, Final Batch Loss: 0.030141014605760574\n",
      "Epoch 1071, Loss: 0.051188963232561946, Final Batch Loss: 0.03468915820121765\n",
      "Epoch 1072, Loss: 0.09595804399577901, Final Batch Loss: 0.0006868152413517237\n",
      "Epoch 1073, Loss: 0.053567451366689056, Final Batch Loss: 0.012602767907083035\n",
      "Epoch 1074, Loss: 0.028157020220533013, Final Batch Loss: 0.0034079551696777344\n",
      "Epoch 1075, Loss: 0.07757568336091936, Final Batch Loss: 0.02544255182147026\n",
      "Epoch 1076, Loss: 0.026235786848701537, Final Batch Loss: 0.005868855863809586\n",
      "Epoch 1077, Loss: 0.013823300483636558, Final Batch Loss: 0.002712927060201764\n",
      "Epoch 1078, Loss: 0.05154458247125149, Final Batch Loss: 0.0030973213724792004\n",
      "Epoch 1079, Loss: 0.014278196496888995, Final Batch Loss: 0.0023092476185411215\n",
      "Epoch 1080, Loss: 0.017320604994893074, Final Batch Loss: 0.00401862571015954\n",
      "Epoch 1081, Loss: 0.010881841997615993, Final Batch Loss: 0.0005391873419284821\n",
      "Epoch 1082, Loss: 0.009994858060963452, Final Batch Loss: 0.0013047910761088133\n",
      "Epoch 1083, Loss: 0.009110841783694923, Final Batch Loss: 0.0038460649084299803\n",
      "Epoch 1084, Loss: 0.020791292190551758, Final Batch Loss: 0.0003658214118331671\n",
      "Epoch 1085, Loss: 0.015193693456239998, Final Batch Loss: 0.003558042226359248\n",
      "Epoch 1086, Loss: 0.02929306961596012, Final Batch Loss: 0.0012310020392760634\n",
      "Epoch 1087, Loss: 0.021976536547299474, Final Batch Loss: 0.00032331858528777957\n",
      "Epoch 1088, Loss: 0.025068412418477237, Final Batch Loss: 0.012322654947638512\n",
      "Epoch 1089, Loss: 0.027848361583892256, Final Batch Loss: 0.010812852531671524\n",
      "Epoch 1090, Loss: 0.041368689155206084, Final Batch Loss: 0.010561492294073105\n",
      "Epoch 1091, Loss: 0.02359991939738393, Final Batch Loss: 0.002029474824666977\n",
      "Epoch 1092, Loss: 0.01632350467843935, Final Batch Loss: 0.0034260458778589964\n",
      "Epoch 1093, Loss: 0.02948725171154365, Final Batch Loss: 0.00029285799246281385\n",
      "Epoch 1094, Loss: 0.07403302344027907, Final Batch Loss: 0.06459227949380875\n",
      "Epoch 1095, Loss: 0.032542812055908144, Final Batch Loss: 0.0006149213295429945\n",
      "Epoch 1096, Loss: 0.03843177133239806, Final Batch Loss: 0.001632809522561729\n",
      "Epoch 1097, Loss: 0.04244368046056479, Final Batch Loss: 0.0011210425291210413\n",
      "Epoch 1098, Loss: 0.04125632892828435, Final Batch Loss: 0.0014192952075973153\n",
      "Epoch 1099, Loss: 0.03978707082569599, Final Batch Loss: 0.004175247624516487\n",
      "Epoch 1100, Loss: 0.028508042625617236, Final Batch Loss: 0.010769711807370186\n",
      "Epoch 1101, Loss: 0.03359608293976635, Final Batch Loss: 0.022063005715608597\n",
      "Epoch 1102, Loss: 0.04369340499397367, Final Batch Loss: 0.029643205925822258\n",
      "Epoch 1103, Loss: 0.04933183384127915, Final Batch Loss: 0.0032118672970682383\n",
      "Epoch 1104, Loss: 0.06507077458081767, Final Batch Loss: 0.002893380355089903\n",
      "Epoch 1105, Loss: 0.012068843061570078, Final Batch Loss: 0.0004546366981230676\n",
      "Epoch 1106, Loss: 0.02236483944579959, Final Batch Loss: 0.014887191355228424\n",
      "Epoch 1107, Loss: 0.021496110886801034, Final Batch Loss: 0.0024617521557956934\n",
      "Epoch 1108, Loss: 0.01627019260195084, Final Batch Loss: 0.0006336282240226865\n",
      "Epoch 1109, Loss: 0.008943304070271552, Final Batch Loss: 0.003409736789762974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1110, Loss: 0.008377222868148237, Final Batch Loss: 0.001666744239628315\n",
      "Epoch 1111, Loss: 0.011235238576773554, Final Batch Loss: 0.002100929617881775\n",
      "Epoch 1112, Loss: 0.010654218320269138, Final Batch Loss: 0.0020188044290989637\n",
      "Epoch 1113, Loss: 0.033915532287210226, Final Batch Loss: 0.0026676803827285767\n",
      "Epoch 1114, Loss: 0.03921678429469466, Final Batch Loss: 0.02013329043984413\n",
      "Epoch 1115, Loss: 0.007161824876675382, Final Batch Loss: 0.0008110127528198063\n",
      "Epoch 1116, Loss: 0.009375393856316805, Final Batch Loss: 0.0002813963801600039\n",
      "Epoch 1117, Loss: 0.0057630836381576955, Final Batch Loss: 0.0014636676060035825\n",
      "Epoch 1118, Loss: 0.027303950279019773, Final Batch Loss: 0.0118164774030447\n",
      "Epoch 1119, Loss: 0.025919045612681657, Final Batch Loss: 0.0008921142434701324\n",
      "Epoch 1120, Loss: 0.014385479677002877, Final Batch Loss: 0.0047317189164459705\n",
      "Epoch 1121, Loss: 0.028023753664456308, Final Batch Loss: 0.0019641404505819082\n",
      "Epoch 1122, Loss: 0.009281982784159482, Final Batch Loss: 0.0010793526889756322\n",
      "Epoch 1123, Loss: 0.039476784084399696, Final Batch Loss: 5.430870078271255e-05\n",
      "Epoch 1124, Loss: 0.02148125891108066, Final Batch Loss: 0.010218359529972076\n",
      "Epoch 1125, Loss: 0.02164172293851152, Final Batch Loss: 0.0019688522443175316\n",
      "Epoch 1126, Loss: 0.023971740854904056, Final Batch Loss: 0.001424896763637662\n",
      "Epoch 1127, Loss: 0.015589341113809496, Final Batch Loss: 0.005192289128899574\n",
      "Epoch 1128, Loss: 0.030053740716539323, Final Batch Loss: 0.005481322295963764\n",
      "Epoch 1129, Loss: 0.024355710513191298, Final Batch Loss: 0.00018901206203736365\n",
      "Epoch 1130, Loss: 0.02377789019374177, Final Batch Loss: 0.0010532867163419724\n",
      "Epoch 1131, Loss: 0.03150958102196455, Final Batch Loss: 0.009237194433808327\n",
      "Epoch 1132, Loss: 0.018539283570135012, Final Batch Loss: 0.000275658123428002\n",
      "Epoch 1133, Loss: 0.05678307905327529, Final Batch Loss: 0.006335323676466942\n",
      "Epoch 1134, Loss: 0.048754913528682664, Final Batch Loss: 0.021798070520162582\n",
      "Epoch 1135, Loss: 0.02314489078707993, Final Batch Loss: 0.0005618532886728644\n",
      "Epoch 1136, Loss: 0.023807947407476604, Final Batch Loss: 0.01393237616866827\n",
      "Epoch 1137, Loss: 0.16500323882792145, Final Batch Loss: 0.0009177388856187463\n",
      "Epoch 1138, Loss: 0.026065541605930775, Final Batch Loss: 0.004205807112157345\n",
      "Epoch 1139, Loss: 0.036439212039113045, Final Batch Loss: 0.019413329660892487\n",
      "Epoch 1140, Loss: 0.038543077651411295, Final Batch Loss: 0.002324428642168641\n",
      "Epoch 1141, Loss: 0.01270903428667225, Final Batch Loss: 0.00037007153150625527\n",
      "Epoch 1142, Loss: 0.012788535226718523, Final Batch Loss: 0.00017768512770999223\n",
      "Epoch 1143, Loss: 0.01666277158074081, Final Batch Loss: 0.0014367187395691872\n",
      "Epoch 1144, Loss: 0.026116206543520093, Final Batch Loss: 0.0075301434844732285\n",
      "Epoch 1145, Loss: 0.05866633867844939, Final Batch Loss: 0.002705276245251298\n",
      "Epoch 1146, Loss: 0.03593341447412968, Final Batch Loss: 0.004234520252794027\n",
      "Epoch 1147, Loss: 0.013751167571172118, Final Batch Loss: 0.0006015942199155688\n",
      "Epoch 1148, Loss: 0.03697695065056905, Final Batch Loss: 0.0017055915668606758\n",
      "Epoch 1149, Loss: 0.020656492211855948, Final Batch Loss: 0.0028987969271838665\n",
      "Epoch 1150, Loss: 0.033041751419659704, Final Batch Loss: 0.00024691311409696937\n",
      "Epoch 1151, Loss: 0.022580478456802666, Final Batch Loss: 0.001816469244658947\n",
      "Epoch 1152, Loss: 0.035870150226401165, Final Batch Loss: 0.00046902967733331025\n",
      "Epoch 1153, Loss: 0.03466260153800249, Final Batch Loss: 0.0013179756933823228\n",
      "Epoch 1154, Loss: 0.014072981051867828, Final Batch Loss: 0.002332841046154499\n",
      "Epoch 1155, Loss: 0.006589740078197792, Final Batch Loss: 0.00032507957075722516\n",
      "Epoch 1156, Loss: 0.03505872539244592, Final Batch Loss: 0.01192188635468483\n",
      "Epoch 1157, Loss: 0.01742266636574641, Final Batch Loss: 0.0028483737260103226\n",
      "Epoch 1158, Loss: 0.01716766646131873, Final Batch Loss: 0.0031870612874627113\n",
      "Epoch 1159, Loss: 0.021388390799984336, Final Batch Loss: 0.005375791806727648\n",
      "Epoch 1160, Loss: 0.011637925985269248, Final Batch Loss: 0.002315099583938718\n",
      "Epoch 1161, Loss: 0.01661688310559839, Final Batch Loss: 0.0020254524424672127\n",
      "Epoch 1162, Loss: 0.027669959992635995, Final Batch Loss: 0.0033246346283704042\n",
      "Epoch 1163, Loss: 0.026326398612582125, Final Batch Loss: 0.0015343333361670375\n",
      "Epoch 1164, Loss: 0.008008103439351544, Final Batch Loss: 0.0007163421250879765\n",
      "Epoch 1165, Loss: 0.014298076974228024, Final Batch Loss: 0.0005768521805293858\n",
      "Epoch 1166, Loss: 0.023103168059606105, Final Batch Loss: 0.0005777970654889941\n",
      "Epoch 1167, Loss: 0.06171700946288183, Final Batch Loss: 0.02531997673213482\n",
      "Epoch 1168, Loss: 0.018637812056113034, Final Batch Loss: 0.006164695136249065\n",
      "Epoch 1169, Loss: 0.03314438107190654, Final Batch Loss: 0.0002787408302538097\n",
      "Epoch 1170, Loss: 0.01489964997745119, Final Batch Loss: 0.00035718802246265113\n",
      "Epoch 1171, Loss: 0.017640643345657736, Final Batch Loss: 0.0003384676238056272\n",
      "Epoch 1172, Loss: 0.028442039620131254, Final Batch Loss: 0.000807963078841567\n",
      "Epoch 1173, Loss: 0.056522522994782776, Final Batch Loss: 0.0007073153974488378\n",
      "Epoch 1174, Loss: 0.030109397834166884, Final Batch Loss: 0.006703407038003206\n",
      "Epoch 1175, Loss: 0.007917191309388727, Final Batch Loss: 0.0029251857195049524\n",
      "Epoch 1176, Loss: 0.011199415195733309, Final Batch Loss: 0.001950560137629509\n",
      "Epoch 1177, Loss: 0.008044701564358547, Final Batch Loss: 0.002115160459652543\n",
      "Epoch 1178, Loss: 0.023806463374057785, Final Batch Loss: 0.000300293235341087\n",
      "Epoch 1179, Loss: 0.009575654519721866, Final Batch Loss: 0.003312506014481187\n",
      "Epoch 1180, Loss: 0.014977940765675157, Final Batch Loss: 0.006091017741709948\n",
      "Epoch 1181, Loss: 0.009775449812877923, Final Batch Loss: 0.00034972868161275983\n",
      "Epoch 1182, Loss: 0.019664105377160013, Final Batch Loss: 0.0026026940904557705\n",
      "Epoch 1183, Loss: 0.05062277161050588, Final Batch Loss: 0.0007147021824494004\n",
      "Epoch 1184, Loss: 0.014410883537493646, Final Batch Loss: 0.000898214871995151\n",
      "Epoch 1185, Loss: 0.011073519970523193, Final Batch Loss: 0.00043320897384546697\n",
      "Epoch 1186, Loss: 0.02962083093007095, Final Batch Loss: 0.00028438682784326375\n",
      "Epoch 1187, Loss: 0.009663171775173396, Final Batch Loss: 0.0026907194405794144\n",
      "Epoch 1188, Loss: 0.016094736027298495, Final Batch Loss: 0.010293452069163322\n",
      "Epoch 1189, Loss: 0.01829553779680282, Final Batch Loss: 0.0005289656110107899\n",
      "Epoch 1190, Loss: 0.024271818925626576, Final Batch Loss: 0.016933223232626915\n",
      "Epoch 1191, Loss: 0.014893929881509393, Final Batch Loss: 0.0026608312036842108\n",
      "Epoch 1192, Loss: 0.02108807243348565, Final Batch Loss: 0.013546389527618885\n",
      "Epoch 1193, Loss: 0.020820351172005758, Final Batch Loss: 0.00040661057573743165\n",
      "Epoch 1194, Loss: 0.04888633516384289, Final Batch Loss: 0.004592288751155138\n",
      "Epoch 1195, Loss: 0.028595336014404893, Final Batch Loss: 0.009922080673277378\n",
      "Epoch 1196, Loss: 0.01943536681937985, Final Batch Loss: 0.002205889206379652\n",
      "Epoch 1197, Loss: 0.036527885240502656, Final Batch Loss: 0.0007181767141446471\n",
      "Epoch 1198, Loss: 0.015720222319941968, Final Batch Loss: 0.0032995804212987423\n",
      "Epoch 1199, Loss: 0.023065753397531807, Final Batch Loss: 0.005062594544142485\n",
      "Epoch 1200, Loss: 0.016060828624176793, Final Batch Loss: 0.00017077654774766415\n",
      "Epoch 1201, Loss: 0.043504879489773884, Final Batch Loss: 0.03040911629796028\n",
      "Epoch 1202, Loss: 0.0804921081289649, Final Batch Loss: 0.03379841893911362\n",
      "Epoch 1203, Loss: 0.034825405571609735, Final Batch Loss: 0.009330584667623043\n",
      "Epoch 1204, Loss: 0.11518729687668383, Final Batch Loss: 0.017375655472278595\n",
      "Epoch 1205, Loss: 0.022737529827281833, Final Batch Loss: 0.007697110529989004\n",
      "Epoch 1206, Loss: 0.050281111674848944, Final Batch Loss: 0.028168128803372383\n",
      "Epoch 1207, Loss: 0.020562596444506198, Final Batch Loss: 0.00020421453518792987\n",
      "Epoch 1208, Loss: 0.019147784332744777, Final Batch Loss: 0.00245272321626544\n",
      "Epoch 1209, Loss: 0.025459341355599463, Final Batch Loss: 0.0018004130106419325\n",
      "Epoch 1210, Loss: 0.009964738972485065, Final Batch Loss: 0.00327647989615798\n",
      "Epoch 1211, Loss: 0.01462352683302015, Final Batch Loss: 0.0023573513608425856\n",
      "Epoch 1212, Loss: 0.04990278175682761, Final Batch Loss: 0.00036645823274739087\n",
      "Epoch 1213, Loss: 0.013408362778136507, Final Batch Loss: 0.00470363674685359\n",
      "Epoch 1214, Loss: 0.04954423996969126, Final Batch Loss: 0.029970431700348854\n",
      "Epoch 1215, Loss: 0.05909736861940473, Final Batch Loss: 0.045969247817993164\n",
      "Epoch 1216, Loss: 0.02496500051347539, Final Batch Loss: 0.001181927276775241\n",
      "Epoch 1217, Loss: 0.012126859481213614, Final Batch Loss: 0.0016463957726955414\n",
      "Epoch 1218, Loss: 0.024842114304192364, Final Batch Loss: 0.0026031932793557644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1219, Loss: 0.006932752788998187, Final Batch Loss: 0.0008526984020136297\n",
      "Epoch 1220, Loss: 0.04865519277518615, Final Batch Loss: 0.0012868918711319566\n",
      "Epoch 1221, Loss: 0.054898507019970566, Final Batch Loss: 0.026212826371192932\n",
      "Epoch 1222, Loss: 0.12030010791204404, Final Batch Loss: 0.0001473210722906515\n",
      "Epoch 1223, Loss: 0.028093058615922928, Final Batch Loss: 0.010812181048095226\n",
      "Epoch 1224, Loss: 0.034074986702762544, Final Batch Loss: 0.002531297504901886\n",
      "Epoch 1225, Loss: 0.03710030787624419, Final Batch Loss: 0.00048461765982210636\n",
      "Epoch 1226, Loss: 0.01571352779865265, Final Batch Loss: 0.0017355121672153473\n",
      "Epoch 1227, Loss: 0.019442693650489673, Final Batch Loss: 0.00046163852675817907\n",
      "Epoch 1228, Loss: 0.010763115307781845, Final Batch Loss: 0.0006637110491283238\n",
      "Epoch 1229, Loss: 0.009141676942817867, Final Batch Loss: 0.0011072735069319606\n",
      "Epoch 1230, Loss: 0.022331513930112123, Final Batch Loss: 0.0022571859881281853\n",
      "Epoch 1231, Loss: 0.012786227744072676, Final Batch Loss: 0.0024379384703934193\n",
      "Epoch 1232, Loss: 0.011430400103563443, Final Batch Loss: 0.0035119836684316397\n",
      "Epoch 1233, Loss: 0.02440104738343507, Final Batch Loss: 0.0005623238394036889\n",
      "Epoch 1234, Loss: 0.008669625967741013, Final Batch Loss: 0.0018915064865723252\n",
      "Epoch 1235, Loss: 0.006994173920247704, Final Batch Loss: 0.0008237894508056343\n",
      "Epoch 1236, Loss: 0.011691873107338324, Final Batch Loss: 0.005381455644965172\n",
      "Epoch 1237, Loss: 0.029465960920788348, Final Batch Loss: 0.0013541446533054113\n",
      "Epoch 1238, Loss: 0.008339458843693137, Final Batch Loss: 0.0019160908414050937\n",
      "Epoch 1239, Loss: 0.020314482098910958, Final Batch Loss: 0.004964438732713461\n",
      "Epoch 1240, Loss: 0.016359801054932177, Final Batch Loss: 0.000679673277772963\n",
      "Epoch 1241, Loss: 0.05741253565065563, Final Batch Loss: 0.04064997285604477\n",
      "Epoch 1242, Loss: 0.06377200150745921, Final Batch Loss: 0.008323930203914642\n",
      "Epoch 1243, Loss: 0.008655704747070558, Final Batch Loss: 0.0002061000996036455\n",
      "Epoch 1244, Loss: 0.025997073156759143, Final Batch Loss: 0.0016332513187080622\n",
      "Epoch 1245, Loss: 0.006456853210693225, Final Batch Loss: 0.00044117713696323335\n",
      "Epoch 1246, Loss: 0.03993581305257976, Final Batch Loss: 0.024204092100262642\n",
      "Epoch 1247, Loss: 0.08087618742138147, Final Batch Loss: 0.009536880999803543\n",
      "Epoch 1248, Loss: 0.02227735021733679, Final Batch Loss: 0.0006051357486285269\n",
      "Epoch 1249, Loss: 0.012531990534625947, Final Batch Loss: 0.0003341102274134755\n",
      "Epoch 1250, Loss: 0.021955454431008548, Final Batch Loss: 0.0028302473947405815\n",
      "Epoch 1251, Loss: 0.015376470342744142, Final Batch Loss: 0.0008772400906309485\n",
      "Epoch 1252, Loss: 0.015926257823593915, Final Batch Loss: 0.0007483289809897542\n",
      "Epoch 1253, Loss: 0.006317667168332264, Final Batch Loss: 0.0006197136244736612\n",
      "Epoch 1254, Loss: 0.021166315593291074, Final Batch Loss: 0.003894123015925288\n",
      "Epoch 1255, Loss: 0.009819916391279548, Final Batch Loss: 0.0009070998639799654\n",
      "Epoch 1256, Loss: 0.03652797025279142, Final Batch Loss: 0.0003373731451574713\n",
      "Epoch 1257, Loss: 0.012170859088655561, Final Batch Loss: 0.002937782322987914\n",
      "Epoch 1258, Loss: 0.005111407968797721, Final Batch Loss: 0.0017777069006115198\n",
      "Epoch 1259, Loss: 0.007588679378386587, Final Batch Loss: 0.0004980378434993327\n",
      "Epoch 1260, Loss: 0.009884258528472856, Final Batch Loss: 0.005075095687061548\n",
      "Epoch 1261, Loss: 0.008336447353940457, Final Batch Loss: 0.0013786122435703874\n",
      "Epoch 1262, Loss: 0.009785480739083141, Final Batch Loss: 0.003514121752232313\n",
      "Epoch 1263, Loss: 0.025789827515836805, Final Batch Loss: 0.008259878493845463\n",
      "Epoch 1264, Loss: 0.023446873936336488, Final Batch Loss: 0.0005996108520776033\n",
      "Epoch 1265, Loss: 0.014730905648320913, Final Batch Loss: 0.000542041496373713\n",
      "Epoch 1266, Loss: 0.011591414571739733, Final Batch Loss: 0.00021062412997707725\n",
      "Epoch 1267, Loss: 0.013065244304016232, Final Batch Loss: 0.002079958561807871\n",
      "Epoch 1268, Loss: 0.047758081927895546, Final Batch Loss: 0.0025275617372244596\n",
      "Epoch 1269, Loss: 0.02413246452488238, Final Batch Loss: 0.004362397361546755\n",
      "Epoch 1270, Loss: 0.006978461402468383, Final Batch Loss: 0.0007476723403669894\n",
      "Epoch 1271, Loss: 0.01609478291356936, Final Batch Loss: 0.0005939744878560305\n",
      "Epoch 1272, Loss: 0.020882059121504426, Final Batch Loss: 0.0014692244585603476\n",
      "Epoch 1273, Loss: 0.05255920553463511, Final Batch Loss: 0.004364301450550556\n",
      "Epoch 1274, Loss: 0.0329068104038015, Final Batch Loss: 0.0023644166067242622\n",
      "Epoch 1275, Loss: 0.05595780606381595, Final Batch Loss: 0.021509768441319466\n",
      "Epoch 1276, Loss: 0.030417683185078204, Final Batch Loss: 0.005572040099650621\n",
      "Epoch 1277, Loss: 0.017383507918566465, Final Batch Loss: 0.0023858353961259127\n",
      "Epoch 1278, Loss: 0.017092913389205933, Final Batch Loss: 0.0026620649732649326\n",
      "Epoch 1279, Loss: 0.011896718933712691, Final Batch Loss: 0.0006042760796844959\n",
      "Epoch 1280, Loss: 0.007802298991009593, Final Batch Loss: 0.0006610918790102005\n",
      "Epoch 1281, Loss: 0.01228728290880099, Final Batch Loss: 0.0005821381928399205\n",
      "Epoch 1282, Loss: 0.03216713773144875, Final Batch Loss: 0.00010868745448533446\n",
      "Epoch 1283, Loss: 0.04330920599750243, Final Batch Loss: 0.02132244035601616\n",
      "Epoch 1284, Loss: 0.0038600465923082083, Final Batch Loss: 0.0003750132455024868\n",
      "Epoch 1285, Loss: 0.03832160832826048, Final Batch Loss: 0.00655354605987668\n",
      "Epoch 1286, Loss: 0.03241323365364224, Final Batch Loss: 0.015182632021605968\n",
      "Epoch 1287, Loss: 0.015302873667678796, Final Batch Loss: 0.0032359010074287653\n",
      "Epoch 1288, Loss: 0.017364388128044084, Final Batch Loss: 0.0014818812487646937\n",
      "Epoch 1289, Loss: 0.0061548115918412805, Final Batch Loss: 0.000580207968596369\n",
      "Epoch 1290, Loss: 0.011517038568854332, Final Batch Loss: 0.0020833900198340416\n",
      "Epoch 1291, Loss: 0.03633955662371591, Final Batch Loss: 0.007351704407483339\n",
      "Epoch 1292, Loss: 0.01641361460497137, Final Batch Loss: 0.0001647853059694171\n",
      "Epoch 1293, Loss: 0.019236443447880447, Final Batch Loss: 0.00648462912067771\n",
      "Epoch 1294, Loss: 0.009020424215123057, Final Batch Loss: 0.001072472077794373\n",
      "Epoch 1295, Loss: 0.011524116387590766, Final Batch Loss: 0.0021943834144622087\n",
      "Epoch 1296, Loss: 0.016159423510544002, Final Batch Loss: 0.007307723630219698\n",
      "Epoch 1297, Loss: 0.01171553903259337, Final Batch Loss: 0.0005668719531968236\n",
      "Epoch 1298, Loss: 0.017312891941401176, Final Batch Loss: 0.0010064797243103385\n",
      "Epoch 1299, Loss: 0.02090724226582097, Final Batch Loss: 6.13519296166487e-05\n",
      "Epoch 1300, Loss: 0.03289275077986531, Final Batch Loss: 0.02917325496673584\n",
      "Epoch 1301, Loss: 0.00638450455153361, Final Batch Loss: 0.00137228611856699\n",
      "Epoch 1302, Loss: 0.02474788820836693, Final Batch Loss: 0.00019800796872004867\n",
      "Epoch 1303, Loss: 0.02134066791040823, Final Batch Loss: 0.0008570762583985925\n",
      "Epoch 1304, Loss: 0.043480573571287096, Final Batch Loss: 0.0009370227344334126\n",
      "Epoch 1305, Loss: 0.0035514613846316934, Final Batch Loss: 0.0007829453097656369\n",
      "Epoch 1306, Loss: 0.008756749040912837, Final Batch Loss: 0.0015771853504702449\n",
      "Epoch 1307, Loss: 0.017904585285577923, Final Batch Loss: 0.0013765871990472078\n",
      "Epoch 1308, Loss: 0.0168377214577049, Final Batch Loss: 0.009722042828798294\n",
      "Epoch 1309, Loss: 0.00516571661864873, Final Batch Loss: 0.0002333662414457649\n",
      "Epoch 1310, Loss: 0.002715475420700386, Final Batch Loss: 0.00012322093243710697\n",
      "Epoch 1311, Loss: 0.011548495502211154, Final Batch Loss: 0.0010664529399946332\n",
      "Epoch 1312, Loss: 0.012558163376525044, Final Batch Loss: 0.003213419346138835\n",
      "Epoch 1313, Loss: 0.0161755200533662, Final Batch Loss: 0.0002444715064484626\n",
      "Epoch 1314, Loss: 0.031186969150439836, Final Batch Loss: 0.0003729941672645509\n",
      "Epoch 1315, Loss: 0.00637203935184516, Final Batch Loss: 0.0009401915594935417\n",
      "Epoch 1316, Loss: 0.013302391897013877, Final Batch Loss: 0.0018226311076432467\n",
      "Epoch 1317, Loss: 0.009587246808223426, Final Batch Loss: 0.004365592263638973\n",
      "Epoch 1318, Loss: 0.005147527641383931, Final Batch Loss: 0.0001530721492599696\n",
      "Epoch 1319, Loss: 0.010193900612648576, Final Batch Loss: 0.007437346503138542\n",
      "Epoch 1320, Loss: 0.017188439145684242, Final Batch Loss: 0.0009431983926333487\n",
      "Epoch 1321, Loss: 0.01670667779399082, Final Batch Loss: 0.0015231395373120904\n",
      "Epoch 1322, Loss: 0.02525084326043725, Final Batch Loss: 0.00484843086451292\n",
      "Epoch 1323, Loss: 0.027539922622963786, Final Batch Loss: 0.007336358539760113\n",
      "Epoch 1324, Loss: 0.009186618204694241, Final Batch Loss: 0.0041960738599300385\n",
      "Epoch 1325, Loss: 0.005748799012508243, Final Batch Loss: 0.0015364279970526695\n",
      "Epoch 1326, Loss: 0.0072114228387363255, Final Batch Loss: 0.0003060129238292575\n",
      "Epoch 1327, Loss: 0.03944442712236196, Final Batch Loss: 0.021472562104463577\n",
      "Epoch 1328, Loss: 0.01134274174910388, Final Batch Loss: 5.345948375179432e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1329, Loss: 0.014758607336261775, Final Batch Loss: 0.00036575872218236327\n",
      "Epoch 1330, Loss: 0.011898445547558367, Final Batch Loss: 0.003918681293725967\n",
      "Epoch 1331, Loss: 0.01398950807924848, Final Batch Loss: 0.00020023116667289287\n",
      "Epoch 1332, Loss: 0.05037298542447388, Final Batch Loss: 0.0033432128839194775\n",
      "Epoch 1333, Loss: 0.01707290005288087, Final Batch Loss: 0.005467634182423353\n",
      "Epoch 1334, Loss: 0.011040434415917844, Final Batch Loss: 0.0038235981483012438\n",
      "Epoch 1335, Loss: 0.009565593936713412, Final Batch Loss: 0.005467479582875967\n",
      "Epoch 1336, Loss: 0.0068128693383187056, Final Batch Loss: 0.002446576487272978\n",
      "Epoch 1337, Loss: 0.004021687345812097, Final Batch Loss: 0.0005862289690412581\n",
      "Epoch 1338, Loss: 0.010765720478957519, Final Batch Loss: 0.002736426657065749\n",
      "Epoch 1339, Loss: 0.012938011102960445, Final Batch Loss: 0.0006974352872930467\n",
      "Epoch 1340, Loss: 0.022581374854780734, Final Batch Loss: 0.0002104887826135382\n",
      "Epoch 1341, Loss: 0.009960929688531905, Final Batch Loss: 0.0001111854799091816\n",
      "Epoch 1342, Loss: 0.06310040497919545, Final Batch Loss: 0.0008513115462847054\n",
      "Epoch 1343, Loss: 0.02386008268513251, Final Batch Loss: 0.00020904229313600808\n",
      "Epoch 1344, Loss: 0.018284225487150252, Final Batch Loss: 0.001472783857025206\n",
      "Epoch 1345, Loss: 0.008258938149083406, Final Batch Loss: 0.0011799954809248447\n",
      "Epoch 1346, Loss: 0.0378127274743747, Final Batch Loss: 0.00025281941634602845\n",
      "Epoch 1347, Loss: 0.011948079511057585, Final Batch Loss: 0.0017593902302905917\n",
      "Epoch 1348, Loss: 0.03346059535397217, Final Batch Loss: 0.0006745598511770368\n",
      "Epoch 1349, Loss: 0.007049491046927869, Final Batch Loss: 0.0001322361349593848\n",
      "Epoch 1350, Loss: 0.008907795243430883, Final Batch Loss: 0.00070485461037606\n",
      "Epoch 1351, Loss: 0.013107197621138766, Final Batch Loss: 0.00035912502789869905\n",
      "Epoch 1352, Loss: 0.021238384302705526, Final Batch Loss: 0.0026263475883752108\n",
      "Epoch 1353, Loss: 0.0024126718890329357, Final Batch Loss: 0.00022319478739518672\n",
      "Epoch 1354, Loss: 0.006753836729330942, Final Batch Loss: 0.002033054828643799\n",
      "Epoch 1355, Loss: 0.03742703574243933, Final Batch Loss: 0.001276826485991478\n",
      "Epoch 1356, Loss: 0.005347218437236734, Final Batch Loss: 6.973401468712837e-05\n",
      "Epoch 1357, Loss: 0.009326925202913117, Final Batch Loss: 0.0005256463191471994\n",
      "Epoch 1358, Loss: 0.026697291643358767, Final Batch Loss: 0.0018415093654766679\n",
      "Epoch 1359, Loss: 0.016724252280255314, Final Batch Loss: 0.006814762018620968\n",
      "Epoch 1360, Loss: 0.01364380051381886, Final Batch Loss: 0.0018258945783600211\n",
      "Epoch 1361, Loss: 0.013911209680372849, Final Batch Loss: 0.0004751202941406518\n",
      "Epoch 1362, Loss: 0.0054106013994896784, Final Batch Loss: 0.0012505416525527835\n",
      "Epoch 1363, Loss: 0.01560426512878621, Final Batch Loss: 0.0014225519262254238\n",
      "Epoch 1364, Loss: 0.025271335034631193, Final Batch Loss: 0.0006670024013146758\n",
      "Epoch 1365, Loss: 0.011896248826815281, Final Batch Loss: 0.007690340280532837\n",
      "Epoch 1366, Loss: 0.002578580708359368, Final Batch Loss: 0.0001330930826952681\n",
      "Epoch 1367, Loss: 0.03206008552660933, Final Batch Loss: 0.028033390641212463\n",
      "Epoch 1368, Loss: 0.010376904159784317, Final Batch Loss: 0.0007846420630812645\n",
      "Epoch 1369, Loss: 0.027812212123535573, Final Batch Loss: 0.002801320282742381\n",
      "Epoch 1370, Loss: 0.0050005098164547235, Final Batch Loss: 0.0005267603555694222\n",
      "Epoch 1371, Loss: 0.010382173408288509, Final Batch Loss: 0.0004754976835101843\n",
      "Epoch 1372, Loss: 0.03203350861440413, Final Batch Loss: 0.0008211637032218277\n",
      "Epoch 1373, Loss: 0.009297329059336334, Final Batch Loss: 0.002278856234624982\n",
      "Epoch 1374, Loss: 0.023748990148305893, Final Batch Loss: 0.006772512570023537\n",
      "Epoch 1375, Loss: 0.010023299240856431, Final Batch Loss: 0.0012990268878638744\n",
      "Epoch 1376, Loss: 0.024084769102046266, Final Batch Loss: 0.0002901507541537285\n",
      "Epoch 1377, Loss: 0.018338616566325072, Final Batch Loss: 0.00010100650979438797\n",
      "Epoch 1378, Loss: 0.010435113683342934, Final Batch Loss: 0.0010051120771095157\n",
      "Epoch 1379, Loss: 0.03254821337759495, Final Batch Loss: 0.019409017637372017\n",
      "Epoch 1380, Loss: 0.023859374574385583, Final Batch Loss: 0.001854927628301084\n",
      "Epoch 1381, Loss: 0.010505101847229525, Final Batch Loss: 0.00639900891110301\n",
      "Epoch 1382, Loss: 0.010362522640207317, Final Batch Loss: 0.00011884615378221497\n",
      "Epoch 1383, Loss: 0.00812294375646161, Final Batch Loss: 5.364718526834622e-05\n",
      "Epoch 1384, Loss: 0.004069819406140596, Final Batch Loss: 0.0012364706490188837\n",
      "Epoch 1385, Loss: 0.015123059711186215, Final Batch Loss: 0.0002041094412561506\n",
      "Epoch 1386, Loss: 0.010646768845617771, Final Batch Loss: 0.00301382620818913\n",
      "Epoch 1387, Loss: 0.007332614361075684, Final Batch Loss: 0.0011819301871582866\n",
      "Epoch 1388, Loss: 0.01631138933589682, Final Batch Loss: 0.0021961433812975883\n",
      "Epoch 1389, Loss: 0.029043434478808194, Final Batch Loss: 0.0005226258072070777\n",
      "Epoch 1390, Loss: 0.018702671231949353, Final Batch Loss: 2.231764119642321e-05\n",
      "Epoch 1391, Loss: 0.02247776457807049, Final Batch Loss: 0.011776243336498737\n",
      "Epoch 1392, Loss: 0.036117842741077766, Final Batch Loss: 0.0050848680548369884\n",
      "Epoch 1393, Loss: 0.006509448157885345, Final Batch Loss: 5.7300134358229116e-05\n",
      "Epoch 1394, Loss: 0.015269564872141927, Final Batch Loss: 0.00477115623652935\n",
      "Epoch 1395, Loss: 0.04658395462320186, Final Batch Loss: 0.0008734461735002697\n",
      "Epoch 1396, Loss: 0.005517914280062541, Final Batch Loss: 0.002701851073652506\n",
      "Epoch 1397, Loss: 0.003841585239570122, Final Batch Loss: 0.0008518899558112025\n",
      "Epoch 1398, Loss: 0.02255813652300276, Final Batch Loss: 0.00015026566688902676\n",
      "Epoch 1399, Loss: 0.006544442338054068, Final Batch Loss: 0.00023464935657102615\n",
      "Epoch 1400, Loss: 0.0078481528616976, Final Batch Loss: 0.00048252512351609766\n",
      "Epoch 1401, Loss: 0.004691189795266837, Final Batch Loss: 0.00014439654478337616\n",
      "Epoch 1402, Loss: 0.005340767602319829, Final Batch Loss: 0.0005993713857606053\n",
      "Epoch 1403, Loss: 0.005325208825524896, Final Batch Loss: 0.0017131422646343708\n",
      "Epoch 1404, Loss: 0.06700006569735706, Final Batch Loss: 0.02591623365879059\n",
      "Epoch 1405, Loss: 0.0039788134599803016, Final Batch Loss: 0.0009452245431020856\n",
      "Epoch 1406, Loss: 0.0068405526981223375, Final Batch Loss: 0.0034639108926057816\n",
      "Epoch 1407, Loss: 0.03103054515668191, Final Batch Loss: 0.0005300376797094941\n",
      "Epoch 1408, Loss: 0.004115507996175438, Final Batch Loss: 0.0002853611367754638\n",
      "Epoch 1409, Loss: 0.015253998397383839, Final Batch Loss: 0.003745852969586849\n",
      "Epoch 1410, Loss: 0.07479105753009208, Final Batch Loss: 0.0012547061778604984\n",
      "Epoch 1411, Loss: 0.00823521958955098, Final Batch Loss: 0.0004923270316794515\n",
      "Epoch 1412, Loss: 0.02580382610904053, Final Batch Loss: 0.005564059596508741\n",
      "Epoch 1413, Loss: 0.0045328636188060045, Final Batch Loss: 0.0005950426566414535\n",
      "Epoch 1414, Loss: 0.0034644288680283353, Final Batch Loss: 6.430104258470237e-05\n",
      "Epoch 1415, Loss: 0.010187754465732723, Final Batch Loss: 0.0006968859815970063\n",
      "Epoch 1416, Loss: 0.005680222297087312, Final Batch Loss: 0.0010161112295463681\n",
      "Epoch 1417, Loss: 0.02226257568690926, Final Batch Loss: 0.0016279697883874178\n",
      "Epoch 1418, Loss: 0.009547288296744227, Final Batch Loss: 0.0026106208097189665\n",
      "Epoch 1419, Loss: 0.0077135092578828335, Final Batch Loss: 0.00014552961511071771\n",
      "Epoch 1420, Loss: 0.004315884521929547, Final Batch Loss: 0.0017392321024090052\n",
      "Epoch 1421, Loss: 0.0035455793840810657, Final Batch Loss: 0.00027135328855365515\n",
      "Epoch 1422, Loss: 0.053174952685367316, Final Batch Loss: 0.014941077679395676\n",
      "Epoch 1423, Loss: 0.010807316881255247, Final Batch Loss: 0.0014862900134176016\n",
      "Epoch 1424, Loss: 0.008404549007536843, Final Batch Loss: 0.00021855096565559506\n",
      "Epoch 1425, Loss: 0.004742429897305556, Final Batch Loss: 0.0005289934342727065\n",
      "Epoch 1426, Loss: 0.008856001310050488, Final Batch Loss: 0.0009428499033674598\n",
      "Epoch 1427, Loss: 0.005580561759416014, Final Batch Loss: 0.001279987976886332\n",
      "Epoch 1428, Loss: 0.04379075797623955, Final Batch Loss: 0.0002389145374763757\n",
      "Epoch 1429, Loss: 0.03033839154522866, Final Batch Loss: 0.0007230059709399939\n",
      "Epoch 1430, Loss: 0.015472072838747408, Final Batch Loss: 0.00011255332356086001\n",
      "Epoch 1431, Loss: 0.052967137831728905, Final Batch Loss: 0.0013751654187217355\n",
      "Epoch 1432, Loss: 0.039323869394138455, Final Batch Loss: 0.0013023433275520802\n",
      "Epoch 1433, Loss: 0.035014249733649194, Final Batch Loss: 0.0013504038797691464\n",
      "Epoch 1434, Loss: 0.014305279939435422, Final Batch Loss: 0.0055123018100857735\n",
      "Epoch 1435, Loss: 0.014698684273753315, Final Batch Loss: 0.0008139956626109779\n",
      "Epoch 1436, Loss: 0.06514130323193967, Final Batch Loss: 0.02806922048330307\n",
      "Epoch 1437, Loss: 0.00772600929485634, Final Batch Loss: 0.00230211578309536\n",
      "Epoch 1438, Loss: 0.02408938726875931, Final Batch Loss: 0.004547829739749432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1439, Loss: 0.040233478881418705, Final Batch Loss: 0.0012020259164273739\n",
      "Epoch 1440, Loss: 0.012377281760564074, Final Batch Loss: 0.0054586478509008884\n",
      "Epoch 1441, Loss: 0.011113853310234845, Final Batch Loss: 0.0007225524168461561\n",
      "Epoch 1442, Loss: 0.003649559017503634, Final Batch Loss: 0.00035038162604905665\n",
      "Epoch 1443, Loss: 0.006059996769181453, Final Batch Loss: 0.0020173564553260803\n",
      "Epoch 1444, Loss: 0.01048874318075832, Final Batch Loss: 0.00016651883197482675\n",
      "Epoch 1445, Loss: 0.002931761962827295, Final Batch Loss: 0.0003038358117919415\n",
      "Epoch 1446, Loss: 0.0803006625501439, Final Batch Loss: 0.04677245020866394\n",
      "Epoch 1447, Loss: 0.005608245264738798, Final Batch Loss: 0.0005668826051987708\n",
      "Epoch 1448, Loss: 0.025191037100739777, Final Batch Loss: 0.0020331244450062513\n",
      "Epoch 1449, Loss: 0.025857644039206207, Final Batch Loss: 0.003252066671848297\n",
      "Epoch 1450, Loss: 0.019410303473705426, Final Batch Loss: 0.00018754226039163768\n",
      "Epoch 1451, Loss: 0.019459477291093208, Final Batch Loss: 0.005417661275714636\n",
      "Epoch 1452, Loss: 0.0026589781627990305, Final Batch Loss: 0.0008337482577189803\n",
      "Epoch 1453, Loss: 0.006542813585838303, Final Batch Loss: 0.0011958223767578602\n",
      "Epoch 1454, Loss: 0.01987519186513964, Final Batch Loss: 0.0001256808900507167\n",
      "Epoch 1455, Loss: 0.04598951293155551, Final Batch Loss: 0.003470510011538863\n",
      "Epoch 1456, Loss: 0.025735104776686057, Final Batch Loss: 0.00031928191310726106\n",
      "Epoch 1457, Loss: 0.0024430498961010017, Final Batch Loss: 5.109562334837392e-05\n",
      "Epoch 1458, Loss: 0.0046345268783625215, Final Batch Loss: 0.0006487773498520255\n",
      "Epoch 1459, Loss: 0.013696987894945778, Final Batch Loss: 0.010087990202009678\n",
      "Epoch 1460, Loss: 0.010249477054458112, Final Batch Loss: 0.00029000965878367424\n",
      "Epoch 1461, Loss: 0.00744534564728383, Final Batch Loss: 0.00023753904679324478\n",
      "Epoch 1462, Loss: 0.03853504185099155, Final Batch Loss: 0.013413097709417343\n",
      "Epoch 1463, Loss: 0.06519715604372323, Final Batch Loss: 0.006337010767310858\n",
      "Epoch 1464, Loss: 0.012281016359338537, Final Batch Loss: 0.007522800005972385\n",
      "Epoch 1465, Loss: 0.023088765068678185, Final Batch Loss: 0.0018949565710499883\n",
      "Epoch 1466, Loss: 0.0225362240744289, Final Batch Loss: 0.002783369505777955\n",
      "Epoch 1467, Loss: 0.022676505031995475, Final Batch Loss: 0.0019920263439416885\n",
      "Epoch 1468, Loss: 0.003145467650028877, Final Batch Loss: 0.0004132716276217252\n",
      "Epoch 1469, Loss: 0.01711344174691476, Final Batch Loss: 0.0010385585483163595\n",
      "Epoch 1470, Loss: 0.02727397531270981, Final Batch Loss: 0.0006674346514046192\n",
      "Epoch 1471, Loss: 0.03229536002618261, Final Batch Loss: 0.0006912302342243493\n",
      "Epoch 1472, Loss: 0.007344937679590657, Final Batch Loss: 0.002605070825666189\n",
      "Epoch 1473, Loss: 0.013143571064574644, Final Batch Loss: 0.00021262498921714723\n",
      "Epoch 1474, Loss: 0.015964639809681103, Final Batch Loss: 0.007486362475901842\n",
      "Epoch 1475, Loss: 0.05764865202945657, Final Batch Loss: 0.0004253321385476738\n",
      "Epoch 1476, Loss: 0.005990508390823379, Final Batch Loss: 0.0006679662619717419\n",
      "Epoch 1477, Loss: 0.01002329247421585, Final Batch Loss: 0.001875916263088584\n",
      "Epoch 1478, Loss: 0.010409478389192373, Final Batch Loss: 0.003244964871555567\n",
      "Epoch 1479, Loss: 0.052094019047217444, Final Batch Loss: 0.002053502481430769\n",
      "Epoch 1480, Loss: 0.010489789943676442, Final Batch Loss: 0.00037963519571349025\n",
      "Epoch 1481, Loss: 0.0034834302932722494, Final Batch Loss: 0.0005923399585299194\n",
      "Epoch 1482, Loss: 0.0054192170791793615, Final Batch Loss: 0.0002862469700630754\n",
      "Epoch 1483, Loss: 0.01332856353110401, Final Batch Loss: 0.00010736392141552642\n",
      "Epoch 1484, Loss: 0.005133406360982917, Final Batch Loss: 0.00020944554125890136\n",
      "Epoch 1485, Loss: 0.04359934720559977, Final Batch Loss: 0.03577883914113045\n",
      "Epoch 1486, Loss: 0.029262083990033716, Final Batch Loss: 0.0009075740235857666\n",
      "Epoch 1487, Loss: 0.01842117219348438, Final Batch Loss: 0.00329155963845551\n",
      "Epoch 1488, Loss: 0.010798197850817814, Final Batch Loss: 0.0003665327385533601\n",
      "Epoch 1489, Loss: 0.006916089478181675, Final Batch Loss: 0.0016433551209047437\n",
      "Epoch 1490, Loss: 0.00997177726821974, Final Batch Loss: 0.0007349491934292018\n",
      "Epoch 1491, Loss: 0.08886628752225079, Final Batch Loss: 0.050656672567129135\n",
      "Epoch 1492, Loss: 0.014314992411527783, Final Batch Loss: 0.005069299601018429\n",
      "Epoch 1493, Loss: 0.013743636081926525, Final Batch Loss: 0.0014339853078126907\n",
      "Epoch 1494, Loss: 0.025571611360646784, Final Batch Loss: 0.0009138608584180474\n",
      "Epoch 1495, Loss: 0.01834138232516125, Final Batch Loss: 0.0010037014726549387\n",
      "Epoch 1496, Loss: 0.009306811640271917, Final Batch Loss: 0.00027314075850881636\n",
      "Epoch 1497, Loss: 0.016343663970474154, Final Batch Loss: 0.0013682295102626085\n",
      "Epoch 1498, Loss: 0.036487992358161137, Final Batch Loss: 0.0014216937124729156\n",
      "Epoch 1499, Loss: 0.015023475454654545, Final Batch Loss: 0.0002739308401942253\n",
      "Epoch 1500, Loss: 0.020069206446351018, Final Batch Loss: 0.0017954229842871428\n",
      "Epoch 1501, Loss: 0.017114626680267975, Final Batch Loss: 0.011314676143229008\n",
      "Epoch 1502, Loss: 0.02711515979899559, Final Batch Loss: 0.0001445253292331472\n",
      "Epoch 1503, Loss: 0.0070187113306019455, Final Batch Loss: 0.00044550190796144307\n",
      "Epoch 1504, Loss: 0.01923948054900393, Final Batch Loss: 0.0002654910204000771\n",
      "Epoch 1505, Loss: 0.005676414992194623, Final Batch Loss: 0.0004845686489716172\n",
      "Epoch 1506, Loss: 0.01993101178231882, Final Batch Loss: 0.0016548753483220935\n",
      "Epoch 1507, Loss: 0.009011297079268843, Final Batch Loss: 0.0017757536843419075\n",
      "Epoch 1508, Loss: 0.00850933778565377, Final Batch Loss: 0.004022563342005014\n",
      "Epoch 1509, Loss: 0.035483309358824044, Final Batch Loss: 0.02151445299386978\n",
      "Epoch 1510, Loss: 0.009404509386513382, Final Batch Loss: 0.0052797626703977585\n",
      "Epoch 1511, Loss: 0.02440315968124196, Final Batch Loss: 0.010275082662701607\n",
      "Epoch 1512, Loss: 0.004869616241194308, Final Batch Loss: 0.0007515770266763866\n",
      "Epoch 1513, Loss: 0.007022682810202241, Final Batch Loss: 0.001174482749775052\n",
      "Epoch 1514, Loss: 0.01256689673755318, Final Batch Loss: 0.0002938194083981216\n",
      "Epoch 1515, Loss: 0.012246967002283782, Final Batch Loss: 0.0016437979647889733\n",
      "Epoch 1516, Loss: 0.024413275939878076, Final Batch Loss: 0.0017214446561411023\n",
      "Epoch 1517, Loss: 0.0023771115229465067, Final Batch Loss: 0.0005738300387747586\n",
      "Epoch 1518, Loss: 0.00711095062433742, Final Batch Loss: 0.00168793520424515\n",
      "Epoch 1519, Loss: 0.03215183489373885, Final Batch Loss: 0.0018357295775786042\n",
      "Epoch 1520, Loss: 0.01552497311058687, Final Batch Loss: 0.00011186188930878416\n",
      "Epoch 1521, Loss: 0.005411595579062123, Final Batch Loss: 0.0006287521100603044\n",
      "Epoch 1522, Loss: 0.0038627132162218913, Final Batch Loss: 0.0009980443865060806\n",
      "Epoch 1523, Loss: 0.012605453201103956, Final Batch Loss: 0.001221757149323821\n",
      "Epoch 1524, Loss: 0.003672398583148606, Final Batch Loss: 0.00013206641597207636\n",
      "Epoch 1525, Loss: 0.010498608578927815, Final Batch Loss: 0.0014048963785171509\n",
      "Epoch 1526, Loss: 0.004323027103964705, Final Batch Loss: 0.0015594606520608068\n",
      "Epoch 1527, Loss: 0.002613968899822794, Final Batch Loss: 0.0001361410686513409\n",
      "Epoch 1528, Loss: 0.019271435201517306, Final Batch Loss: 0.0047206091694533825\n",
      "Epoch 1529, Loss: 0.01200559746939689, Final Batch Loss: 0.00099326076451689\n",
      "Epoch 1530, Loss: 0.0039251822454389185, Final Batch Loss: 0.0003034824039787054\n",
      "Epoch 1531, Loss: 0.012241085700225085, Final Batch Loss: 0.002762601710855961\n",
      "Epoch 1532, Loss: 0.004086995366378687, Final Batch Loss: 0.0006835651583969593\n",
      "Epoch 1533, Loss: 0.017213647544849664, Final Batch Loss: 0.0012589822290465236\n",
      "Epoch 1534, Loss: 0.0037809614077559672, Final Batch Loss: 4.549316508928314e-05\n",
      "Epoch 1535, Loss: 0.01840887387515977, Final Batch Loss: 0.004422267898917198\n",
      "Epoch 1536, Loss: 0.013722968535148539, Final Batch Loss: 0.011941684409976006\n",
      "Epoch 1537, Loss: 0.009852126553596463, Final Batch Loss: 0.007539556827396154\n",
      "Epoch 1538, Loss: 0.015987201884854585, Final Batch Loss: 0.0005686867516487837\n",
      "Epoch 1539, Loss: 0.0060349865816533566, Final Batch Loss: 0.0033745388500392437\n",
      "Epoch 1540, Loss: 0.020161961729172617, Final Batch Loss: 0.004416846204549074\n",
      "Epoch 1541, Loss: 0.0029225546568341088, Final Batch Loss: 1.441643325961195e-05\n",
      "Epoch 1542, Loss: 0.06688212451990694, Final Batch Loss: 0.05298379063606262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1543, Loss: 0.01515094525530003, Final Batch Loss: 0.0020761715713888407\n",
      "Epoch 1544, Loss: 0.006830688507761806, Final Batch Loss: 0.00035291467793285847\n",
      "Epoch 1545, Loss: 0.004273195183486678, Final Batch Loss: 0.00010354719415772706\n",
      "Epoch 1546, Loss: 0.02669021039037034, Final Batch Loss: 0.0009197226609103382\n",
      "Epoch 1547, Loss: 0.0054507190943695605, Final Batch Loss: 0.002533696126192808\n",
      "Epoch 1548, Loss: 0.002219725225586444, Final Batch Loss: 0.00025855458807200193\n",
      "Epoch 1549, Loss: 0.0037558322947006673, Final Batch Loss: 0.0004841451009269804\n",
      "Epoch 1550, Loss: 0.006767909129848704, Final Batch Loss: 0.0030522383749485016\n",
      "Epoch 1551, Loss: 0.006323057641566265, Final Batch Loss: 0.004678692668676376\n",
      "Epoch 1552, Loss: 0.0038604582514381036, Final Batch Loss: 0.0004080268554389477\n",
      "Epoch 1553, Loss: 0.009446795738767833, Final Batch Loss: 0.0006382397841662169\n",
      "Epoch 1554, Loss: 0.030218394022085704, Final Batch Loss: 0.002849700627848506\n",
      "Epoch 1555, Loss: 0.013833135104505345, Final Batch Loss: 0.00036366633139550686\n",
      "Epoch 1556, Loss: 0.015500566732953303, Final Batch Loss: 0.00020276998111512512\n",
      "Epoch 1557, Loss: 0.014371234181453474, Final Batch Loss: 0.008280341513454914\n",
      "Epoch 1558, Loss: 0.0031574064632877707, Final Batch Loss: 0.00020881398813799024\n",
      "Epoch 1559, Loss: 0.015712617692770436, Final Batch Loss: 0.0016881200717762113\n",
      "Epoch 1560, Loss: 0.06056684858049266, Final Batch Loss: 0.0004227275785524398\n",
      "Epoch 1561, Loss: 0.03892148402519524, Final Batch Loss: 0.029734984040260315\n",
      "Epoch 1562, Loss: 0.05928209883859381, Final Batch Loss: 0.04292721673846245\n",
      "Epoch 1563, Loss: 0.01629233208950609, Final Batch Loss: 0.002038586651906371\n",
      "Epoch 1564, Loss: 0.05965654188184999, Final Batch Loss: 0.03517681732773781\n",
      "Epoch 1565, Loss: 0.017508108518086374, Final Batch Loss: 0.004727873019874096\n",
      "Epoch 1566, Loss: 0.032793912207125686, Final Batch Loss: 0.0009926934726536274\n",
      "Epoch 1567, Loss: 0.04004672126029618, Final Batch Loss: 0.030950872227549553\n",
      "Epoch 1568, Loss: 0.010773652436910197, Final Batch Loss: 0.0011227247305214405\n",
      "Epoch 1569, Loss: 0.03329723121714778, Final Batch Loss: 0.0004297600535210222\n",
      "Epoch 1570, Loss: 0.008922720109694637, Final Batch Loss: 0.0001244002050952986\n",
      "Epoch 1571, Loss: 0.03378921520197764, Final Batch Loss: 0.0004970833542756736\n",
      "Epoch 1572, Loss: 0.009101235889829695, Final Batch Loss: 0.007067177444696426\n",
      "Epoch 1573, Loss: 0.003238837205572054, Final Batch Loss: 0.0006440348224714398\n",
      "Epoch 1574, Loss: 0.02666483559005428, Final Batch Loss: 0.007038289215415716\n",
      "Epoch 1575, Loss: 0.029090524796629325, Final Batch Loss: 0.00020483139087446034\n",
      "Epoch 1576, Loss: 0.01267168892081827, Final Batch Loss: 0.0010854203719645739\n",
      "Epoch 1577, Loss: 0.007871723486459814, Final Batch Loss: 0.004029581788927317\n",
      "Epoch 1578, Loss: 0.005546593223698437, Final Batch Loss: 0.0007357975700870156\n",
      "Epoch 1579, Loss: 0.008073830045759678, Final Batch Loss: 0.003065309254452586\n",
      "Epoch 1580, Loss: 0.011047752923332155, Final Batch Loss: 0.00085140933515504\n",
      "Epoch 1581, Loss: 0.01162420038599521, Final Batch Loss: 0.0001452283759135753\n",
      "Epoch 1582, Loss: 0.03460987494327128, Final Batch Loss: 0.03127773106098175\n",
      "Epoch 1583, Loss: 0.008893118181731552, Final Batch Loss: 0.0013482447247952223\n",
      "Epoch 1584, Loss: 0.02012678250321187, Final Batch Loss: 0.0003151989949401468\n",
      "Epoch 1585, Loss: 0.00858354169758968, Final Batch Loss: 0.0027162355836480856\n",
      "Epoch 1586, Loss: 0.005001778707082849, Final Batch Loss: 9.574990690452978e-05\n",
      "Epoch 1587, Loss: 0.013303406536579132, Final Batch Loss: 0.0007539346697740257\n",
      "Epoch 1588, Loss: 0.04266130656469613, Final Batch Loss: 0.02790083736181259\n",
      "Epoch 1589, Loss: 0.016060665919212624, Final Batch Loss: 0.0002677592565305531\n",
      "Epoch 1590, Loss: 0.03629134641960263, Final Batch Loss: 0.013140854425728321\n",
      "Epoch 1591, Loss: 0.02601556811714545, Final Batch Loss: 0.006429004482924938\n",
      "Epoch 1592, Loss: 0.04939713515341282, Final Batch Loss: 0.0021974509581923485\n",
      "Epoch 1593, Loss: 0.024084121105261147, Final Batch Loss: 0.017746327444911003\n",
      "Epoch 1594, Loss: 0.044538426445797086, Final Batch Loss: 0.002998719224706292\n",
      "Epoch 1595, Loss: 0.06735902017680928, Final Batch Loss: 0.000537655025254935\n",
      "Epoch 1596, Loss: 0.045788540854118764, Final Batch Loss: 0.03561711683869362\n",
      "Epoch 1597, Loss: 0.008146133681293577, Final Batch Loss: 0.0018714123871177435\n",
      "Epoch 1598, Loss: 0.023567866010125726, Final Batch Loss: 0.00293130986392498\n",
      "Epoch 1599, Loss: 0.027895702005480416, Final Batch Loss: 0.00047846193774603307\n",
      "Epoch 1600, Loss: 0.02445502416230738, Final Batch Loss: 0.003445061855018139\n",
      "Epoch 1601, Loss: 0.01330640741798561, Final Batch Loss: 0.00022108585108071566\n",
      "Epoch 1602, Loss: 0.008153037168085575, Final Batch Loss: 0.002135401824489236\n",
      "Epoch 1603, Loss: 0.009994147345423698, Final Batch Loss: 0.0006159199983812869\n",
      "Epoch 1604, Loss: 0.006109036359703168, Final Batch Loss: 0.0030079244170337915\n",
      "Epoch 1605, Loss: 0.019701055018231273, Final Batch Loss: 0.0008628348005004227\n",
      "Epoch 1606, Loss: 0.014979819388827309, Final Batch Loss: 0.0002121053112205118\n",
      "Epoch 1607, Loss: 0.021930072043687687, Final Batch Loss: 2.8293399736867286e-05\n",
      "Epoch 1608, Loss: 0.014320658519864082, Final Batch Loss: 0.001138848252594471\n",
      "Epoch 1609, Loss: 0.006136625714134425, Final Batch Loss: 0.0011544370790943503\n",
      "Epoch 1610, Loss: 0.005209909955738112, Final Batch Loss: 0.002610615221783519\n",
      "Epoch 1611, Loss: 0.004062725565745495, Final Batch Loss: 0.0009302734397351742\n",
      "Epoch 1612, Loss: 0.002753416629275307, Final Batch Loss: 0.000283108907751739\n",
      "Epoch 1613, Loss: 0.03436676657292992, Final Batch Loss: 0.0009953387780115008\n",
      "Epoch 1614, Loss: 0.013657429604791105, Final Batch Loss: 0.0024619046598672867\n",
      "Epoch 1615, Loss: 0.009208148228935897, Final Batch Loss: 0.0004966407432220876\n",
      "Epoch 1616, Loss: 0.020387090655276552, Final Batch Loss: 7.943942910060287e-05\n",
      "Epoch 1617, Loss: 0.010699225513235433, Final Batch Loss: 0.0013155867345631123\n",
      "Epoch 1618, Loss: 0.003038545692106709, Final Batch Loss: 0.00038875220343470573\n",
      "Epoch 1619, Loss: 0.004066459892783314, Final Batch Loss: 0.00029767834348604083\n",
      "Epoch 1620, Loss: 0.003883812401909381, Final Batch Loss: 0.0002551445213612169\n",
      "Epoch 1621, Loss: 0.005805139255244285, Final Batch Loss: 0.001990917604416609\n",
      "Epoch 1622, Loss: 0.0060659296868834645, Final Batch Loss: 0.0036243624053895473\n",
      "Epoch 1623, Loss: 0.02527056954568252, Final Batch Loss: 0.0009279082296416163\n",
      "Epoch 1624, Loss: 0.015458945417776704, Final Batch Loss: 0.005056321155279875\n",
      "Epoch 1625, Loss: 0.003552604131982662, Final Batch Loss: 0.000795756874140352\n",
      "Epoch 1626, Loss: 0.009454992628889158, Final Batch Loss: 0.003366500372067094\n",
      "Epoch 1627, Loss: 0.005956838562269695, Final Batch Loss: 0.0019917034078389406\n",
      "Epoch 1628, Loss: 0.006478822215285618, Final Batch Loss: 7.072772132232785e-05\n",
      "Epoch 1629, Loss: 0.030263445718446746, Final Batch Loss: 0.00027064362075179815\n",
      "Epoch 1630, Loss: 0.003769760558498092, Final Batch Loss: 9.631611465010792e-05\n",
      "Epoch 1631, Loss: 0.0030692117143189535, Final Batch Loss: 0.00023466878337785602\n",
      "Epoch 1632, Loss: 0.04379514598986134, Final Batch Loss: 0.03932998329401016\n",
      "Epoch 1633, Loss: 0.005780463136034086, Final Batch Loss: 0.0003177989274263382\n",
      "Epoch 1634, Loss: 0.013242228698800318, Final Batch Loss: 5.70718984818086e-05\n",
      "Epoch 1635, Loss: 0.020537692311336286, Final Batch Loss: 0.00023345132649410516\n",
      "Epoch 1636, Loss: 0.005165249633137137, Final Batch Loss: 0.0006284582777880132\n",
      "Epoch 1637, Loss: 0.06065598700661212, Final Batch Loss: 8.881770190782845e-05\n",
      "Epoch 1638, Loss: 0.007762953755445778, Final Batch Loss: 0.0001674593659117818\n",
      "Epoch 1639, Loss: 0.007766400754917413, Final Batch Loss: 0.002607794711366296\n",
      "Epoch 1640, Loss: 0.03260444424813613, Final Batch Loss: 0.000890890893060714\n",
      "Epoch 1641, Loss: 0.00637160510814283, Final Batch Loss: 0.0002443756675347686\n",
      "Epoch 1642, Loss: 0.010834544023964554, Final Batch Loss: 0.00038180395495146513\n",
      "Epoch 1643, Loss: 0.02096940271439962, Final Batch Loss: 0.00019451029947958887\n",
      "Epoch 1644, Loss: 0.007607076142448932, Final Batch Loss: 0.0007728574564680457\n",
      "Epoch 1645, Loss: 0.01544103742344305, Final Batch Loss: 0.0072554717771708965\n",
      "Epoch 1646, Loss: 0.0037971256679156795, Final Batch Loss: 0.00040705432184040546\n",
      "Epoch 1647, Loss: 0.025665058521553874, Final Batch Loss: 0.00047838431783020496\n",
      "Epoch 1648, Loss: 0.00863308584666811, Final Batch Loss: 0.0027579478919506073\n",
      "Epoch 1649, Loss: 0.0059999709483236074, Final Batch Loss: 0.0010228099999949336\n",
      "Epoch 1650, Loss: 0.006472219043644145, Final Batch Loss: 0.0005610190564766526\n",
      "Epoch 1651, Loss: 0.008612056553829461, Final Batch Loss: 0.0017899118829518557\n",
      "Epoch 1652, Loss: 0.024066692974884063, Final Batch Loss: 0.012399816885590553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1653, Loss: 0.005871727364137769, Final Batch Loss: 0.0007629060419276357\n",
      "Epoch 1654, Loss: 0.0028674161585513502, Final Batch Loss: 0.0010144246043637395\n",
      "Epoch 1655, Loss: 0.01039110531564802, Final Batch Loss: 0.0012681855587288737\n",
      "Epoch 1656, Loss: 0.00493126260698773, Final Batch Loss: 0.00015755475033074617\n",
      "Epoch 1657, Loss: 0.02157281039399095, Final Batch Loss: 0.0009424515883438289\n",
      "Epoch 1658, Loss: 0.015246582366671646, Final Batch Loss: 2.4259501515189186e-05\n",
      "Epoch 1659, Loss: 0.010629190030158497, Final Batch Loss: 0.0001318989525316283\n",
      "Epoch 1660, Loss: 0.00580915019963868, Final Batch Loss: 0.0015280640218406916\n",
      "Epoch 1661, Loss: 0.006932529795449227, Final Batch Loss: 0.002012333832681179\n",
      "Epoch 1662, Loss: 0.005903109926293837, Final Batch Loss: 0.0008287993259727955\n",
      "Epoch 1663, Loss: 0.004555603067274205, Final Batch Loss: 0.0004577333165798336\n",
      "Epoch 1664, Loss: 0.013160924987460021, Final Batch Loss: 8.168575732270256e-05\n",
      "Epoch 1665, Loss: 0.018976438615936786, Final Batch Loss: 0.0025402496103197336\n",
      "Epoch 1666, Loss: 0.011023842904251069, Final Batch Loss: 0.00031658142688684165\n",
      "Epoch 1667, Loss: 0.033508783075376414, Final Batch Loss: 0.00022428292140830308\n",
      "Epoch 1668, Loss: 0.012265071272850037, Final Batch Loss: 0.0012922093737870455\n",
      "Epoch 1669, Loss: 0.005081500959931873, Final Batch Loss: 0.0005732934805564582\n",
      "Epoch 1670, Loss: 0.005403035174822435, Final Batch Loss: 0.00040294145583175123\n",
      "Epoch 1671, Loss: 0.004564323593513109, Final Batch Loss: 0.00012167748354841024\n",
      "Epoch 1672, Loss: 0.005069891689345241, Final Batch Loss: 0.00037085814983583987\n",
      "Epoch 1673, Loss: 0.025434100534766912, Final Batch Loss: 0.0003735926002264023\n",
      "Epoch 1674, Loss: 0.002356677592615597, Final Batch Loss: 0.0006387298344634473\n",
      "Epoch 1675, Loss: 0.01061213156208396, Final Batch Loss: 0.0017398614436388016\n",
      "Epoch 1676, Loss: 0.005190016803680919, Final Batch Loss: 0.003266914514824748\n",
      "Epoch 1677, Loss: 0.004053625212691259, Final Batch Loss: 6.694463809253648e-05\n",
      "Epoch 1678, Loss: 0.007788029877701774, Final Batch Loss: 0.000885392480995506\n",
      "Epoch 1679, Loss: 0.002009508665651083, Final Batch Loss: 7.983842078829184e-05\n",
      "Epoch 1680, Loss: 0.0020025683916173875, Final Batch Loss: 7.12306791683659e-05\n",
      "Epoch 1681, Loss: 0.006538674686453305, Final Batch Loss: 0.0008580409921705723\n",
      "Epoch 1682, Loss: 0.044019025488523766, Final Batch Loss: 0.014499823562800884\n",
      "Epoch 1683, Loss: 0.005599954565695953, Final Batch Loss: 0.0015175881562754512\n",
      "Epoch 1684, Loss: 0.08295188160263933, Final Batch Loss: 0.06848541647195816\n",
      "Epoch 1685, Loss: 0.02590801561018452, Final Batch Loss: 0.0007209288305602968\n",
      "Epoch 1686, Loss: 0.03851647756528109, Final Batch Loss: 0.0017858274513855577\n",
      "Epoch 1687, Loss: 0.0034521119378041476, Final Batch Loss: 0.00045078262337483466\n",
      "Epoch 1688, Loss: 0.016100473258120473, Final Batch Loss: 0.00299383164383471\n",
      "Epoch 1689, Loss: 0.009765688446350396, Final Batch Loss: 0.0022344395983964205\n",
      "Epoch 1690, Loss: 0.016018874128349125, Final Batch Loss: 0.002513639163225889\n",
      "Epoch 1691, Loss: 0.01790087558038067, Final Batch Loss: 0.0012501392047852278\n",
      "Epoch 1692, Loss: 0.02455906823161058, Final Batch Loss: 0.00026687816716730595\n",
      "Epoch 1693, Loss: 0.01240762579254806, Final Batch Loss: 0.0014075289946049452\n",
      "Epoch 1694, Loss: 0.006277052394580096, Final Batch Loss: 0.0008169157081283629\n",
      "Epoch 1695, Loss: 0.0030026164604350924, Final Batch Loss: 0.00020087660232093185\n",
      "Epoch 1696, Loss: 0.003423550821025856, Final Batch Loss: 9.198485349770635e-05\n",
      "Epoch 1697, Loss: 0.004064572131028399, Final Batch Loss: 0.0009263677056878805\n",
      "Epoch 1698, Loss: 0.01132250580121763, Final Batch Loss: 0.007625124417245388\n",
      "Epoch 1699, Loss: 0.015269637646269985, Final Batch Loss: 0.0003613294684328139\n",
      "Epoch 1700, Loss: 0.030964017365477048, Final Batch Loss: 0.003747976152226329\n",
      "Epoch 1701, Loss: 0.010465162253240123, Final Batch Loss: 0.0006246034172363579\n",
      "Epoch 1702, Loss: 0.01855075662024319, Final Batch Loss: 0.0013636231888085604\n",
      "Epoch 1703, Loss: 0.013154405343811959, Final Batch Loss: 0.00012330956815276295\n",
      "Epoch 1704, Loss: 0.03341140295378864, Final Batch Loss: 0.03049544058740139\n",
      "Epoch 1705, Loss: 0.005168814939679578, Final Batch Loss: 0.002990099834278226\n",
      "Epoch 1706, Loss: 0.0044119745143689215, Final Batch Loss: 0.0018966173520311713\n",
      "Epoch 1707, Loss: 0.020074520085472614, Final Batch Loss: 0.0004584600683301687\n",
      "Epoch 1708, Loss: 0.021777415695396485, Final Batch Loss: 3.074853520956822e-05\n",
      "Epoch 1709, Loss: 0.013516420513042249, Final Batch Loss: 0.009061071090400219\n",
      "Epoch 1710, Loss: 0.013309071655385196, Final Batch Loss: 0.0030851420015096664\n",
      "Epoch 1711, Loss: 0.005424201037385501, Final Batch Loss: 0.00020662751921918243\n",
      "Epoch 1712, Loss: 0.0056190998293459415, Final Batch Loss: 0.00028135208413004875\n",
      "Epoch 1713, Loss: 0.021147027553524822, Final Batch Loss: 0.0023797352332621813\n",
      "Epoch 1714, Loss: 0.034706691454630345, Final Batch Loss: 0.02104635164141655\n",
      "Epoch 1715, Loss: 0.01178853951569181, Final Batch Loss: 0.00869093369692564\n",
      "Epoch 1716, Loss: 0.008540275164705236, Final Batch Loss: 0.002847601193934679\n",
      "Epoch 1717, Loss: 0.0060232647228986025, Final Batch Loss: 0.0006729084416292608\n",
      "Epoch 1718, Loss: 0.007162262452766299, Final Batch Loss: 0.0011993584921583533\n",
      "Epoch 1719, Loss: 0.017287568742176518, Final Batch Loss: 0.0006889274227432907\n",
      "Epoch 1720, Loss: 0.00726403744192794, Final Batch Loss: 0.0008170190849341452\n",
      "Epoch 1721, Loss: 0.0393242405843921, Final Batch Loss: 0.0035633777733892202\n",
      "Epoch 1722, Loss: 0.012210795015562326, Final Batch Loss: 0.0005274171708151698\n",
      "Epoch 1723, Loss: 0.003369620557350572, Final Batch Loss: 0.00011014550545951352\n",
      "Epoch 1724, Loss: 0.01926257931336295, Final Batch Loss: 0.00021927447232883424\n",
      "Epoch 1725, Loss: 0.00887936883373186, Final Batch Loss: 0.0002999930875375867\n",
      "Epoch 1726, Loss: 0.012899237655801699, Final Batch Loss: 0.005343460477888584\n",
      "Epoch 1727, Loss: 0.028122723459091503, Final Batch Loss: 9.657198825152591e-05\n",
      "Epoch 1728, Loss: 0.035681303808814846, Final Batch Loss: 0.0002730049891397357\n",
      "Epoch 1729, Loss: 0.027487068611662835, Final Batch Loss: 0.0012970144161954522\n",
      "Epoch 1730, Loss: 0.020715120641398244, Final Batch Loss: 9.422253060620278e-05\n",
      "Epoch 1731, Loss: 0.023942318570334464, Final Batch Loss: 0.00012582645285874605\n",
      "Epoch 1732, Loss: 0.015329226007452235, Final Batch Loss: 0.0002944569569081068\n",
      "Epoch 1733, Loss: 0.004640300845494494, Final Batch Loss: 0.00040085663204081357\n",
      "Epoch 1734, Loss: 0.00925121083855629, Final Batch Loss: 0.00030777472420595586\n",
      "Epoch 1735, Loss: 0.00558005601487821, Final Batch Loss: 0.0014921027468517423\n",
      "Epoch 1736, Loss: 0.008517749432940036, Final Batch Loss: 0.003205816261470318\n",
      "Epoch 1737, Loss: 0.012244363693753257, Final Batch Loss: 0.0022014938294887543\n",
      "Epoch 1738, Loss: 0.009900063800159842, Final Batch Loss: 0.0008570191566832364\n",
      "Epoch 1739, Loss: 0.027709995512850583, Final Batch Loss: 0.0002879258245229721\n",
      "Epoch 1740, Loss: 0.007659887123736553, Final Batch Loss: 0.0003383995499461889\n",
      "Epoch 1741, Loss: 0.005470287942443974, Final Batch Loss: 0.00022606112179346383\n",
      "Epoch 1742, Loss: 0.014845795114524662, Final Batch Loss: 0.0015068446518853307\n",
      "Epoch 1743, Loss: 0.007964964839629829, Final Batch Loss: 0.0014758355682715774\n",
      "Epoch 1744, Loss: 0.0033928265220311005, Final Batch Loss: 0.0007109018042683601\n",
      "Epoch 1745, Loss: 0.018469660950358957, Final Batch Loss: 0.0002559080603532493\n",
      "Epoch 1746, Loss: 0.007103814263246022, Final Batch Loss: 0.0009033700334839523\n",
      "Epoch 1747, Loss: 0.011417625704780221, Final Batch Loss: 4.442036151885986e-05\n",
      "Epoch 1748, Loss: 0.03560082183685154, Final Batch Loss: 0.004816979169845581\n",
      "Epoch 1749, Loss: 0.0030783669353695586, Final Batch Loss: 6.808269245084375e-05\n",
      "Epoch 1750, Loss: 0.004239393034367822, Final Batch Loss: 0.0003025268670171499\n",
      "Epoch 1751, Loss: 0.03211035029380582, Final Batch Loss: 0.00041745902854017913\n",
      "Epoch 1752, Loss: 0.0031960957567207515, Final Batch Loss: 0.00023850466823205352\n",
      "Epoch 1753, Loss: 0.027001076421584003, Final Batch Loss: 0.0002931674534920603\n",
      "Epoch 1754, Loss: 0.004184915072983131, Final Batch Loss: 8.672659168951213e-05\n",
      "Epoch 1755, Loss: 0.023417558113578707, Final Batch Loss: 0.019757844507694244\n",
      "Epoch 1756, Loss: 0.005088502657599747, Final Batch Loss: 0.00010935051250271499\n",
      "Epoch 1757, Loss: 0.02193332288879901, Final Batch Loss: 0.002660305006429553\n",
      "Epoch 1758, Loss: 0.010700087441364303, Final Batch Loss: 0.002925248583778739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1759, Loss: 0.028446698852349073, Final Batch Loss: 0.021855074912309647\n",
      "Epoch 1760, Loss: 0.0021748657163698226, Final Batch Loss: 0.00045959075214341283\n",
      "Epoch 1761, Loss: 0.012507854000432417, Final Batch Loss: 0.00019963717204518616\n",
      "Epoch 1762, Loss: 0.007410050282487646, Final Batch Loss: 0.001868405845016241\n",
      "Epoch 1763, Loss: 0.0039366200362565, Final Batch Loss: 0.0004607847658917308\n",
      "Epoch 1764, Loss: 0.005437830492155626, Final Batch Loss: 0.0005089773912914097\n",
      "Epoch 1765, Loss: 0.012260426723514684, Final Batch Loss: 0.000130950691527687\n",
      "Epoch 1766, Loss: 0.0413573999030632, Final Batch Loss: 8.105592132778838e-05\n",
      "Epoch 1767, Loss: 0.0345899885869585, Final Batch Loss: 0.0014752678107470274\n",
      "Epoch 1768, Loss: 0.016174735348613467, Final Batch Loss: 0.0005850577726960182\n",
      "Epoch 1769, Loss: 0.003442953428020701, Final Batch Loss: 0.00033274691668339074\n",
      "Epoch 1770, Loss: 0.007546052889665589, Final Batch Loss: 0.001564502832479775\n",
      "Epoch 1771, Loss: 0.017007696624204982, Final Batch Loss: 5.738019535783678e-05\n",
      "Epoch 1772, Loss: 0.007863422157242894, Final Batch Loss: 0.0023889392614364624\n",
      "Epoch 1773, Loss: 0.0070551758908550255, Final Batch Loss: 0.0001168641829281114\n",
      "Epoch 1774, Loss: 0.006309351927484386, Final Batch Loss: 0.0002419265656499192\n",
      "Epoch 1775, Loss: 0.003624082892201841, Final Batch Loss: 0.0006473701796494424\n",
      "Epoch 1776, Loss: 0.006189960608026013, Final Batch Loss: 0.0004944843240082264\n",
      "Epoch 1777, Loss: 0.007364720895566279, Final Batch Loss: 3.4933826100314036e-05\n",
      "Epoch 1778, Loss: 0.01182497147237882, Final Batch Loss: 0.008379529230296612\n",
      "Epoch 1779, Loss: 0.004734428970550653, Final Batch Loss: 8.538929250789806e-05\n",
      "Epoch 1780, Loss: 0.0039695476880297065, Final Batch Loss: 0.0002497381065040827\n",
      "Epoch 1781, Loss: 0.031673130637500435, Final Batch Loss: 0.0007644828292541206\n",
      "Epoch 1782, Loss: 0.013130250998074189, Final Batch Loss: 0.004852975718677044\n",
      "Epoch 1783, Loss: 0.009757604915648699, Final Batch Loss: 0.0012739214580506086\n",
      "Epoch 1784, Loss: 0.007096570858266205, Final Batch Loss: 0.0024894126690924168\n",
      "Epoch 1785, Loss: 0.00500790927617345, Final Batch Loss: 0.002183273434638977\n",
      "Epoch 1786, Loss: 0.005949531667283736, Final Batch Loss: 0.00010865735384868458\n",
      "Epoch 1787, Loss: 0.010737540083937347, Final Batch Loss: 0.001053377753123641\n",
      "Epoch 1788, Loss: 0.010922605368250515, Final Batch Loss: 0.001632283441722393\n",
      "Epoch 1789, Loss: 0.029116180900018662, Final Batch Loss: 0.0012463274179026484\n",
      "Epoch 1790, Loss: 0.07833262122585438, Final Batch Loss: 0.0028745182789862156\n",
      "Epoch 1791, Loss: 0.013207981188315898, Final Batch Loss: 0.008120257407426834\n",
      "Epoch 1792, Loss: 0.0032561363332206383, Final Batch Loss: 0.0012273448519408703\n",
      "Epoch 1793, Loss: 0.0033223478749278, Final Batch Loss: 0.0001617564121261239\n",
      "Epoch 1794, Loss: 0.005957165383733809, Final Batch Loss: 0.0007168003940023482\n",
      "Epoch 1795, Loss: 0.002249191311420873, Final Batch Loss: 0.0004823487251996994\n",
      "Epoch 1796, Loss: 0.006815062108216807, Final Batch Loss: 0.000280725653283298\n",
      "Epoch 1797, Loss: 0.023664279739023186, Final Batch Loss: 8.068110037129372e-05\n",
      "Epoch 1798, Loss: 0.03553921863203868, Final Batch Loss: 0.022199159488081932\n",
      "Epoch 1799, Loss: 0.006016706694936147, Final Batch Loss: 4.6481694880640134e-05\n",
      "Epoch 1800, Loss: 0.0020260941219021333, Final Batch Loss: 2.4232167561422102e-05\n",
      "Epoch 1801, Loss: 0.008344329689862207, Final Batch Loss: 0.00017477336223237216\n",
      "Epoch 1802, Loss: 0.002931575960246846, Final Batch Loss: 0.0011560992570593953\n",
      "Epoch 1803, Loss: 0.012905052361020353, Final Batch Loss: 7.449372060364112e-05\n",
      "Epoch 1804, Loss: 0.011799812491517514, Final Batch Loss: 0.007962293922901154\n",
      "Epoch 1805, Loss: 0.003677867178339511, Final Batch Loss: 0.0031698415987193584\n",
      "Epoch 1806, Loss: 0.01674627250758931, Final Batch Loss: 0.0001482789230067283\n",
      "Epoch 1807, Loss: 0.002597107602923643, Final Batch Loss: 0.0001626030425541103\n",
      "Epoch 1808, Loss: 0.0343045200133929, Final Batch Loss: 0.001829995191656053\n",
      "Epoch 1809, Loss: 0.011243300188652938, Final Batch Loss: 0.008081219159066677\n",
      "Epoch 1810, Loss: 0.015224511706037447, Final Batch Loss: 0.0002373399620410055\n",
      "Epoch 1811, Loss: 0.00869501891429536, Final Batch Loss: 0.005862380377948284\n",
      "Epoch 1812, Loss: 0.004748423627461307, Final Batch Loss: 0.0002161745069315657\n",
      "Epoch 1813, Loss: 0.004717742631328292, Final Batch Loss: 0.00016550668806303293\n",
      "Epoch 1814, Loss: 0.003326341218780726, Final Batch Loss: 0.00022571880253963172\n",
      "Epoch 1815, Loss: 0.018303660792298615, Final Batch Loss: 4.978993092663586e-05\n",
      "Epoch 1816, Loss: 0.07579668972175568, Final Batch Loss: 0.0005671238759532571\n",
      "Epoch 1817, Loss: 0.020965042000170797, Final Batch Loss: 0.0015983696794137359\n",
      "Epoch 1818, Loss: 0.02256750233937055, Final Batch Loss: 0.0006684577092528343\n",
      "Epoch 1819, Loss: 0.014356568368384615, Final Batch Loss: 0.011852970346808434\n",
      "Epoch 1820, Loss: 0.012785214319592342, Final Batch Loss: 0.0005351834115572274\n",
      "Epoch 1821, Loss: 0.05477508180774748, Final Batch Loss: 0.027613893151283264\n",
      "Epoch 1822, Loss: 0.003679987901705317, Final Batch Loss: 0.00014696297876071185\n",
      "Epoch 1823, Loss: 0.08157588267204119, Final Batch Loss: 6.292812031460926e-05\n",
      "Epoch 1824, Loss: 0.012155497359344736, Final Batch Loss: 0.0011914906790480018\n",
      "Epoch 1825, Loss: 0.007343917357502505, Final Batch Loss: 0.003195359604433179\n",
      "Epoch 1826, Loss: 0.00978727510664612, Final Batch Loss: 0.0016288533806800842\n",
      "Epoch 1827, Loss: 0.021292452060151845, Final Batch Loss: 0.002113781403750181\n",
      "Epoch 1828, Loss: 0.003299270145362243, Final Batch Loss: 0.0002246234507765621\n",
      "Epoch 1829, Loss: 0.006928989692823961, Final Batch Loss: 0.0015540082240477204\n",
      "Epoch 1830, Loss: 0.012791835033567622, Final Batch Loss: 0.00038568212767131627\n",
      "Epoch 1831, Loss: 0.012841857358580455, Final Batch Loss: 0.0002027859736699611\n",
      "Epoch 1832, Loss: 0.004450282402103767, Final Batch Loss: 0.0003063120529986918\n",
      "Epoch 1833, Loss: 0.007557082346465904, Final Batch Loss: 7.798133447067812e-05\n",
      "Epoch 1834, Loss: 0.015505701070651412, Final Batch Loss: 0.0006812017527408898\n",
      "Epoch 1835, Loss: 0.004812738989130594, Final Batch Loss: 0.0008735627052374184\n",
      "Epoch 1836, Loss: 0.007126285927370191, Final Batch Loss: 0.000985683873295784\n",
      "Epoch 1837, Loss: 0.0364204675424844, Final Batch Loss: 0.009428195655345917\n",
      "Epoch 1838, Loss: 0.01426621779683046, Final Batch Loss: 0.0008482973789796233\n",
      "Epoch 1839, Loss: 0.03939153788087424, Final Batch Loss: 0.000221786875044927\n",
      "Epoch 1840, Loss: 0.006535489490488544, Final Batch Loss: 0.003999026957899332\n",
      "Epoch 1841, Loss: 0.00927043349656742, Final Batch Loss: 0.00018068573263008147\n",
      "Epoch 1842, Loss: 0.01723108271835372, Final Batch Loss: 0.002661560196429491\n",
      "Epoch 1843, Loss: 0.03700311039574444, Final Batch Loss: 0.0031770344357937574\n",
      "Epoch 1844, Loss: 0.020140813881880604, Final Batch Loss: 0.00022894282301422209\n",
      "Epoch 1845, Loss: 0.02367290395341115, Final Batch Loss: 0.00047894864110276103\n",
      "Epoch 1846, Loss: 0.0057116057141684, Final Batch Loss: 0.0006208416307345033\n",
      "Epoch 1847, Loss: 0.009692928986623883, Final Batch Loss: 0.0017960076220333576\n",
      "Epoch 1848, Loss: 0.012544355500722304, Final Batch Loss: 0.0003689043805934489\n",
      "Epoch 1849, Loss: 0.007780505402479321, Final Batch Loss: 0.0005842205719090998\n",
      "Epoch 1850, Loss: 0.010796943213790655, Final Batch Loss: 0.0007084503304213285\n",
      "Epoch 1851, Loss: 0.005728240008465946, Final Batch Loss: 0.003969830460846424\n",
      "Epoch 1852, Loss: 0.005514953343663365, Final Batch Loss: 0.0004997003125026822\n",
      "Epoch 1853, Loss: 0.00375094439368695, Final Batch Loss: 0.000792752078268677\n",
      "Epoch 1854, Loss: 0.03007945943681989, Final Batch Loss: 0.0002207552024628967\n",
      "Epoch 1855, Loss: 0.006587729410966858, Final Batch Loss: 0.0042719063349068165\n",
      "Epoch 1856, Loss: 0.007449611817719415, Final Batch Loss: 0.00047007063403725624\n",
      "Epoch 1857, Loss: 0.011293695133645087, Final Batch Loss: 0.004872571676969528\n",
      "Epoch 1858, Loss: 0.011947894890909083, Final Batch Loss: 0.00014970132906455547\n",
      "Epoch 1859, Loss: 0.0070972408866509795, Final Batch Loss: 0.002055442426353693\n",
      "Epoch 1860, Loss: 0.013962104741949588, Final Batch Loss: 0.008269589394330978\n",
      "Epoch 1861, Loss: 0.009545729146339, Final Batch Loss: 0.0065425909124314785\n",
      "Epoch 1862, Loss: 0.01551423070486635, Final Batch Loss: 0.0014525051228702068\n",
      "Epoch 1863, Loss: 0.023187978877103887, Final Batch Loss: 0.00015293827163986862\n",
      "Epoch 1864, Loss: 0.013679847033927217, Final Batch Loss: 0.0007102455710992217\n",
      "Epoch 1865, Loss: 0.02809838632674655, Final Batch Loss: 0.0014085929142311215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1866, Loss: 0.02478169376263395, Final Batch Loss: 0.017233598977327347\n",
      "Epoch 1867, Loss: 0.010047800227766857, Final Batch Loss: 0.00027023660368286073\n",
      "Epoch 1868, Loss: 0.016884223572560586, Final Batch Loss: 0.000213651466765441\n",
      "Epoch 1869, Loss: 0.019225828451453708, Final Batch Loss: 0.00018291788001079112\n",
      "Epoch 1870, Loss: 0.05881152105575893, Final Batch Loss: 0.0062487428076565266\n",
      "Epoch 1871, Loss: 0.006451034889323637, Final Batch Loss: 0.0003606076061259955\n",
      "Epoch 1872, Loss: 0.009369525927468203, Final Batch Loss: 0.00020132838108111173\n",
      "Epoch 1873, Loss: 0.006218382972292602, Final Batch Loss: 0.0009333653724752367\n",
      "Epoch 1874, Loss: 0.022604631711146794, Final Batch Loss: 0.00016821191820781678\n",
      "Epoch 1875, Loss: 0.0024053252273006365, Final Batch Loss: 0.00020045247219968587\n",
      "Epoch 1876, Loss: 0.02916891034692526, Final Batch Loss: 0.02350425347685814\n",
      "Epoch 1877, Loss: 0.02890592091716826, Final Batch Loss: 0.015037789940834045\n",
      "Epoch 1878, Loss: 0.012005702941678464, Final Batch Loss: 0.005988602060824633\n",
      "Epoch 1879, Loss: 0.01778925911639817, Final Batch Loss: 0.0015889655333012342\n",
      "Epoch 1880, Loss: 0.017013596603646874, Final Batch Loss: 0.0017078573582693934\n",
      "Epoch 1881, Loss: 0.008145654166582972, Final Batch Loss: 0.004473824519664049\n",
      "Epoch 1882, Loss: 0.006246946861210745, Final Batch Loss: 0.0032227339688688517\n",
      "Epoch 1883, Loss: 0.019448345512500964, Final Batch Loss: 0.00018723084940575063\n",
      "Epoch 1884, Loss: 0.018122931876860093, Final Batch Loss: 0.00909169390797615\n",
      "Epoch 1885, Loss: 0.026643695775419474, Final Batch Loss: 0.016865413635969162\n",
      "Epoch 1886, Loss: 0.006105156819103286, Final Batch Loss: 0.0004046008689329028\n",
      "Epoch 1887, Loss: 0.00889673668643809, Final Batch Loss: 0.001301727257668972\n",
      "Epoch 1888, Loss: 0.026278305798768997, Final Batch Loss: 0.01980092190206051\n",
      "Epoch 1889, Loss: 0.05129784040036611, Final Batch Loss: 4.955774056725204e-05\n",
      "Epoch 1890, Loss: 0.006174965092213824, Final Batch Loss: 0.0005473137134686112\n",
      "Epoch 1891, Loss: 0.008852775004925206, Final Batch Loss: 0.0003073549596592784\n",
      "Epoch 1892, Loss: 0.013112671033013612, Final Batch Loss: 0.0009610646520741284\n",
      "Epoch 1893, Loss: 0.019030336956348037, Final Batch Loss: 4.8774145398056135e-05\n",
      "Epoch 1894, Loss: 0.02263344085076824, Final Batch Loss: 0.00040356232784688473\n",
      "Epoch 1895, Loss: 0.04543440605630167, Final Batch Loss: 0.014010283164680004\n",
      "Epoch 1896, Loss: 0.015428589191287756, Final Batch Loss: 0.0005915886140428483\n",
      "Epoch 1897, Loss: 0.006596555351279676, Final Batch Loss: 0.0005193244433030486\n",
      "Epoch 1898, Loss: 0.0045380400842987, Final Batch Loss: 0.0003037590649910271\n",
      "Epoch 1899, Loss: 0.0053415457950904965, Final Batch Loss: 0.000692406203597784\n",
      "Epoch 1900, Loss: 0.04106295603560284, Final Batch Loss: 0.0004003090434707701\n",
      "Epoch 1901, Loss: 0.003751380274479743, Final Batch Loss: 0.0009217866463586688\n",
      "Epoch 1902, Loss: 0.007449380413163453, Final Batch Loss: 0.000765660428442061\n",
      "Epoch 1903, Loss: 0.010902147172600962, Final Batch Loss: 0.0004112039168830961\n",
      "Epoch 1904, Loss: 0.0037977657630108297, Final Batch Loss: 0.001113344100303948\n",
      "Epoch 1905, Loss: 0.007617426013894146, Final Batch Loss: 5.105793752591126e-05\n",
      "Epoch 1906, Loss: 0.039290520406211726, Final Batch Loss: 0.007850156165659428\n",
      "Epoch 1907, Loss: 0.003428742114920169, Final Batch Loss: 0.0009461966110393405\n",
      "Epoch 1908, Loss: 0.0023425860854331404, Final Batch Loss: 0.00044593214988708496\n",
      "Epoch 1909, Loss: 0.07711831334745511, Final Batch Loss: 0.0014370876597240567\n",
      "Epoch 1910, Loss: 0.003543517566868104, Final Batch Loss: 0.000889807241037488\n",
      "Epoch 1911, Loss: 0.0026071868633152917, Final Batch Loss: 0.000478147849207744\n",
      "Epoch 1912, Loss: 0.0027635696751531214, Final Batch Loss: 0.000752735766582191\n",
      "Epoch 1913, Loss: 0.003956296786782332, Final Batch Loss: 0.0002103742735926062\n",
      "Epoch 1914, Loss: 0.0016346828233508859, Final Batch Loss: 5.936637535342015e-05\n",
      "Epoch 1915, Loss: 0.004517622248386033, Final Batch Loss: 4.653785435948521e-05\n",
      "Epoch 1916, Loss: 0.04562363562581595, Final Batch Loss: 0.0006355270161293447\n",
      "Epoch 1917, Loss: 0.0036850304604740813, Final Batch Loss: 9.768099698703736e-05\n",
      "Epoch 1918, Loss: 0.009573399525834247, Final Batch Loss: 0.005491678602993488\n",
      "Epoch 1919, Loss: 0.0036519502828014083, Final Batch Loss: 0.00011768307740567252\n",
      "Epoch 1920, Loss: 0.0034594571334309876, Final Batch Loss: 0.0006266377167776227\n",
      "Epoch 1921, Loss: 0.0082542791205924, Final Batch Loss: 0.0067538004368543625\n",
      "Epoch 1922, Loss: 0.01078897285333369, Final Batch Loss: 0.0003124619834125042\n",
      "Epoch 1923, Loss: 0.002396430143562611, Final Batch Loss: 8.078499377006665e-05\n",
      "Epoch 1924, Loss: 0.011367604820406996, Final Batch Loss: 0.010254080407321453\n",
      "Epoch 1925, Loss: 0.004898721497738734, Final Batch Loss: 0.0006076889112591743\n",
      "Epoch 1926, Loss: 0.0018799998215399683, Final Batch Loss: 0.0006968261441215873\n",
      "Epoch 1927, Loss: 0.05474254715954885, Final Batch Loss: 0.026334049180150032\n",
      "Epoch 1928, Loss: 0.052210083304089494, Final Batch Loss: 0.0003140338812954724\n",
      "Epoch 1929, Loss: 0.003637599860667251, Final Batch Loss: 0.00155945448204875\n",
      "Epoch 1930, Loss: 0.0060376038600225, Final Batch Loss: 0.0003012678644154221\n",
      "Epoch 1931, Loss: 0.002276148516102694, Final Batch Loss: 0.0002346033725189045\n",
      "Epoch 1932, Loss: 0.002338468882953748, Final Batch Loss: 0.0003785929293371737\n",
      "Epoch 1933, Loss: 0.011522552773385542, Final Batch Loss: 5.5437067203456536e-05\n",
      "Epoch 1934, Loss: 0.017221308808075264, Final Batch Loss: 0.00041433857404626906\n",
      "Epoch 1935, Loss: 0.029058904212433845, Final Batch Loss: 0.0002278937608934939\n",
      "Epoch 1936, Loss: 0.010688703856430948, Final Batch Loss: 0.0010965487454086542\n",
      "Epoch 1937, Loss: 0.019037764490349218, Final Batch Loss: 0.0024806014262139797\n",
      "Epoch 1938, Loss: 0.034194717154605314, Final Batch Loss: 0.0011471373727545142\n",
      "Epoch 1939, Loss: 0.009250532166333869, Final Batch Loss: 0.00016022534691728652\n",
      "Epoch 1940, Loss: 0.015737296489533037, Final Batch Loss: 0.011527519673109055\n",
      "Epoch 1941, Loss: 0.024532510782592, Final Batch Loss: 0.0005077649257145822\n",
      "Epoch 1942, Loss: 0.0037304627476260066, Final Batch Loss: 0.00030948681524023414\n",
      "Epoch 1943, Loss: 0.013841042906278744, Final Batch Loss: 0.0004294677637517452\n",
      "Epoch 1944, Loss: 0.009339206590084359, Final Batch Loss: 0.0007461452623829246\n",
      "Epoch 1945, Loss: 0.0024835093281581067, Final Batch Loss: 0.00040731928311288357\n",
      "Epoch 1946, Loss: 0.015296823665266857, Final Batch Loss: 0.00029873813036829233\n",
      "Epoch 1947, Loss: 0.005311089393217117, Final Batch Loss: 0.00177874811924994\n",
      "Epoch 1948, Loss: 0.020850697284913622, Final Batch Loss: 0.017385708168148994\n",
      "Epoch 1949, Loss: 0.016020621493225917, Final Batch Loss: 0.013164673000574112\n",
      "Epoch 1950, Loss: 0.06760239427967463, Final Batch Loss: 0.0004645872686523944\n",
      "Epoch 1951, Loss: 0.0041889058411470614, Final Batch Loss: 0.0003110835677944124\n",
      "Epoch 1952, Loss: 0.004082752420799807, Final Batch Loss: 0.0006541854818351567\n",
      "Epoch 1953, Loss: 0.019934976764488965, Final Batch Loss: 0.0008784477249719203\n",
      "Epoch 1954, Loss: 0.041293415750260465, Final Batch Loss: 0.015271096490323544\n",
      "Epoch 1955, Loss: 0.0040320795378647745, Final Batch Loss: 0.0006728472071699798\n",
      "Epoch 1956, Loss: 0.010685802022635471, Final Batch Loss: 0.00011572695075301453\n",
      "Epoch 1957, Loss: 0.022049612482078373, Final Batch Loss: 0.010496891103684902\n",
      "Epoch 1958, Loss: 0.0029887201671954244, Final Batch Loss: 0.00026884605176746845\n",
      "Epoch 1959, Loss: 0.005383814874221571, Final Batch Loss: 0.00023355586745310575\n",
      "Epoch 1960, Loss: 0.0024382033589063212, Final Batch Loss: 0.00014709382958244532\n",
      "Epoch 1961, Loss: 0.008065447858825792, Final Batch Loss: 0.0003933474945370108\n",
      "Epoch 1962, Loss: 0.009079556039068848, Final Batch Loss: 0.0035548661835491657\n",
      "Epoch 1963, Loss: 0.006415930423827376, Final Batch Loss: 5.240985774435103e-05\n",
      "Epoch 1964, Loss: 0.003235845171730034, Final Batch Loss: 0.00024269353889394552\n",
      "Epoch 1965, Loss: 0.008569067034841282, Final Batch Loss: 4.0119175537256524e-05\n",
      "Epoch 1966, Loss: 0.041954434316721745, Final Batch Loss: 0.03684408217668533\n",
      "Epoch 1967, Loss: 0.006255820771912113, Final Batch Loss: 0.00029973092023283243\n",
      "Epoch 1968, Loss: 0.01618577643239405, Final Batch Loss: 0.000840759661514312\n",
      "Epoch 1969, Loss: 0.0054838865326019, Final Batch Loss: 0.00031326431781053543\n",
      "Epoch 1970, Loss: 0.009508473915047944, Final Batch Loss: 0.005065576639026403\n",
      "Epoch 1971, Loss: 0.010451396083226427, Final Batch Loss: 0.0002721697383094579\n",
      "Epoch 1972, Loss: 0.022381447488442063, Final Batch Loss: 0.0016346604097634554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1973, Loss: 0.003299679454357829, Final Batch Loss: 0.0005604728357866406\n",
      "Epoch 1974, Loss: 0.007000484230957227, Final Batch Loss: 0.00033021444687619805\n",
      "Epoch 1975, Loss: 0.0022484835208160803, Final Batch Loss: 0.00025460944743826985\n",
      "Epoch 1976, Loss: 0.006830879108747467, Final Batch Loss: 0.0010535300243645906\n",
      "Epoch 1977, Loss: 0.023882659297669306, Final Batch Loss: 0.0008486558799631894\n",
      "Epoch 1978, Loss: 0.062312126654433087, Final Batch Loss: 0.001548814820125699\n",
      "Epoch 1979, Loss: 0.0072759354807203636, Final Batch Loss: 0.001149526797235012\n",
      "Epoch 1980, Loss: 0.0020502859697444364, Final Batch Loss: 0.00018554551934357733\n",
      "Epoch 1981, Loss: 0.025752866989932954, Final Batch Loss: 0.0008279317989945412\n",
      "Epoch 1982, Loss: 0.002812412232742645, Final Batch Loss: 0.0016553379828110337\n",
      "Epoch 1983, Loss: 0.0263752858445514, Final Batch Loss: 0.0002053275820799172\n",
      "Epoch 1984, Loss: 0.02431826178508345, Final Batch Loss: 0.00012892672384623438\n",
      "Epoch 1985, Loss: 0.036726182283018716, Final Batch Loss: 0.0006857884000055492\n",
      "Epoch 1986, Loss: 0.004172729255515151, Final Batch Loss: 0.0005893641500733793\n",
      "Epoch 1987, Loss: 0.024802027372061275, Final Batch Loss: 0.0008475445210933685\n",
      "Epoch 1988, Loss: 0.013841090607456863, Final Batch Loss: 0.005836475174874067\n",
      "Epoch 1989, Loss: 0.018539409327786416, Final Batch Loss: 0.008531039580702782\n",
      "Epoch 1990, Loss: 0.006540065136505291, Final Batch Loss: 0.0018612801795825362\n",
      "Epoch 1991, Loss: 0.004078935016877949, Final Batch Loss: 0.0009973167907446623\n",
      "Epoch 1992, Loss: 0.004886775219347328, Final Batch Loss: 0.0009211994474753737\n",
      "Epoch 1993, Loss: 0.006839626425062306, Final Batch Loss: 0.0014908668817952275\n",
      "Epoch 1994, Loss: 0.0018303965625818819, Final Batch Loss: 0.00021954934345558286\n",
      "Epoch 1995, Loss: 0.0017413491295883432, Final Batch Loss: 0.0004711257934104651\n",
      "Epoch 1996, Loss: 0.01190132403280586, Final Batch Loss: 0.001220199977979064\n",
      "Epoch 1997, Loss: 0.011357732291799039, Final Batch Loss: 0.0009729500743560493\n",
      "Epoch 1998, Loss: 0.0167412255104864, Final Batch Loss: 0.0002870849275495857\n",
      "Epoch 1999, Loss: 0.01709994420525618, Final Batch Loss: 0.00041309959487989545\n",
      "Epoch 2000, Loss: 0.002707388804992661, Final Batch Loss: 0.00038898660568520427\n",
      "Epoch 2001, Loss: 0.00763460443704389, Final Batch Loss: 0.00012246554251760244\n",
      "Epoch 2002, Loss: 0.006932925753062591, Final Batch Loss: 0.0013649348402395844\n",
      "Epoch 2003, Loss: 0.002222768154751975, Final Batch Loss: 5.574875831371173e-05\n",
      "Epoch 2004, Loss: 0.006061517517082393, Final Batch Loss: 0.00013546639820560813\n",
      "Epoch 2005, Loss: 0.009078588453121483, Final Batch Loss: 0.0004991819732822478\n",
      "Epoch 2006, Loss: 0.0025863925111480057, Final Batch Loss: 0.00018930673832073808\n",
      "Epoch 2007, Loss: 0.004498173759202473, Final Batch Loss: 0.0011332251597195864\n",
      "Epoch 2008, Loss: 0.007260934042278677, Final Batch Loss: 0.003175971331074834\n",
      "Epoch 2009, Loss: 0.008555719919968396, Final Batch Loss: 0.0008124597952701151\n",
      "Epoch 2010, Loss: 0.0014635723200626671, Final Batch Loss: 0.0002937322133220732\n",
      "Epoch 2011, Loss: 0.005341665615560487, Final Batch Loss: 5.120626883581281e-05\n",
      "Epoch 2012, Loss: 0.011853497155243531, Final Batch Loss: 0.0003010147775057703\n",
      "Epoch 2013, Loss: 0.009392749518156052, Final Batch Loss: 0.004407407715916634\n",
      "Epoch 2014, Loss: 0.026807647256646305, Final Batch Loss: 0.0037006998900324106\n",
      "Epoch 2015, Loss: 0.005504637127160095, Final Batch Loss: 0.000571210402995348\n",
      "Epoch 2016, Loss: 0.03889143603737466, Final Batch Loss: 0.00033486392931081355\n",
      "Epoch 2017, Loss: 0.005077076784800738, Final Batch Loss: 0.000391093926737085\n",
      "Epoch 2018, Loss: 0.0071189156951732, Final Batch Loss: 0.0002594047109596431\n",
      "Epoch 2019, Loss: 0.0021282188172335736, Final Batch Loss: 0.000116080125735607\n",
      "Epoch 2020, Loss: 0.004675259864598047, Final Batch Loss: 0.00010291586659150198\n",
      "Epoch 2021, Loss: 0.0062538660367863486, Final Batch Loss: 2.329411836399231e-05\n",
      "Epoch 2022, Loss: 0.010759486285678577, Final Batch Loss: 3.121425834251568e-05\n",
      "Epoch 2023, Loss: 0.03175956910854438, Final Batch Loss: 0.004485486075282097\n",
      "Epoch 2024, Loss: 0.01635899892426096, Final Batch Loss: 7.376194116659462e-05\n",
      "Epoch 2025, Loss: 0.0028425152704585344, Final Batch Loss: 0.0006979306344874203\n",
      "Epoch 2026, Loss: 0.00160552428860683, Final Batch Loss: 0.0006709051667712629\n",
      "Epoch 2027, Loss: 0.00222625452443026, Final Batch Loss: 0.0005244619096629322\n",
      "Epoch 2028, Loss: 0.004331218704464845, Final Batch Loss: 8.389465801883489e-05\n",
      "Epoch 2029, Loss: 0.005277107411529869, Final Batch Loss: 0.00021463590383064002\n",
      "Epoch 2030, Loss: 0.0037188467831583694, Final Batch Loss: 0.001983794616535306\n",
      "Epoch 2031, Loss: 0.008063005996518768, Final Batch Loss: 0.00024020129058044404\n",
      "Epoch 2032, Loss: 0.003724478359799832, Final Batch Loss: 0.0009640229400247335\n",
      "Epoch 2033, Loss: 0.0041788160015130416, Final Batch Loss: 0.0003007983323186636\n",
      "Epoch 2034, Loss: 0.004203931297524832, Final Batch Loss: 0.00017303611093666404\n",
      "Epoch 2035, Loss: 0.040355212389840744, Final Batch Loss: 0.00048089545452967286\n",
      "Epoch 2036, Loss: 0.017118969939474482, Final Batch Loss: 0.0003722983819898218\n",
      "Epoch 2037, Loss: 0.0025108810914389323, Final Batch Loss: 3.3977401471929625e-05\n",
      "Epoch 2038, Loss: 0.006764461271814071, Final Batch Loss: 0.00015678921772632748\n",
      "Epoch 2039, Loss: 0.00961248243402224, Final Batch Loss: 0.0032828878611326218\n",
      "Epoch 2040, Loss: 0.002130615583155304, Final Batch Loss: 0.000631466624327004\n",
      "Epoch 2041, Loss: 0.004229594589560293, Final Batch Loss: 0.00019418382726144046\n",
      "Epoch 2042, Loss: 0.006036011251126183, Final Batch Loss: 4.244820956955664e-05\n",
      "Epoch 2043, Loss: 0.008284564442874398, Final Batch Loss: 6.242011295398697e-05\n",
      "Epoch 2044, Loss: 0.0026647127597243525, Final Batch Loss: 9.709837468108162e-05\n",
      "Epoch 2045, Loss: 0.004125787309021689, Final Batch Loss: 0.0021681226789951324\n",
      "Epoch 2046, Loss: 0.008207112798118033, Final Batch Loss: 0.00013361709716264158\n",
      "Epoch 2047, Loss: 0.004823950395802967, Final Batch Loss: 0.0021173821296542883\n",
      "Epoch 2048, Loss: 0.01502654468640685, Final Batch Loss: 0.00032603024737909436\n",
      "Epoch 2049, Loss: 0.011764482216676697, Final Batch Loss: 0.006101395469158888\n",
      "Epoch 2050, Loss: 0.031508867570664734, Final Batch Loss: 0.0006387746543623507\n",
      "Epoch 2051, Loss: 0.009770729520823807, Final Batch Loss: 0.006440593395382166\n",
      "Epoch 2052, Loss: 0.015092470683157444, Final Batch Loss: 0.00014695673598907888\n",
      "Epoch 2053, Loss: 0.02182252380589489, Final Batch Loss: 0.00016265235899481922\n",
      "Epoch 2054, Loss: 0.02497669560398208, Final Batch Loss: 0.00025599604123272\n",
      "Epoch 2055, Loss: 0.011159071436850354, Final Batch Loss: 0.002796870656311512\n",
      "Epoch 2056, Loss: 0.006731815759849269, Final Batch Loss: 9.757545194588602e-05\n",
      "Epoch 2057, Loss: 0.06522523067542352, Final Batch Loss: 0.00034708977909758687\n",
      "Epoch 2058, Loss: 0.020530699563096277, Final Batch Loss: 0.00013991804735269397\n",
      "Epoch 2059, Loss: 0.0353006272634957, Final Batch Loss: 0.00026349167455919087\n",
      "Epoch 2060, Loss: 0.011690640792949125, Final Batch Loss: 0.008172680623829365\n",
      "Epoch 2061, Loss: 0.029332203877856955, Final Batch Loss: 0.011748299933969975\n",
      "Epoch 2062, Loss: 0.014788355270866305, Final Batch Loss: 0.0003119372995570302\n",
      "Epoch 2063, Loss: 0.04314787586918101, Final Batch Loss: 0.00039523368468508124\n",
      "Epoch 2064, Loss: 0.02083461216534488, Final Batch Loss: 0.0003274919872637838\n",
      "Epoch 2065, Loss: 0.0016385128947149497, Final Batch Loss: 0.00011251722753513604\n",
      "Epoch 2066, Loss: 0.019476792309433222, Final Batch Loss: 0.007379486691206694\n",
      "Epoch 2067, Loss: 0.02098267909605056, Final Batch Loss: 0.0007373213302344084\n",
      "Epoch 2068, Loss: 0.006160588236525655, Final Batch Loss: 0.00015444960445165634\n",
      "Epoch 2069, Loss: 0.004429577340488322, Final Batch Loss: 0.00023303042689803988\n",
      "Epoch 2070, Loss: 0.0055707497376715764, Final Batch Loss: 0.0009123697527684271\n",
      "Epoch 2071, Loss: 0.0073267109401058406, Final Batch Loss: 0.0009245840483345091\n",
      "Epoch 2072, Loss: 0.008236924797529355, Final Batch Loss: 0.0005906593869440258\n",
      "Epoch 2073, Loss: 0.005128009041072801, Final Batch Loss: 0.0005371168372221291\n",
      "Epoch 2074, Loss: 0.015130192012293264, Final Batch Loss: 0.001223139464855194\n",
      "Epoch 2075, Loss: 0.02026821048639249, Final Batch Loss: 0.004354249686002731\n",
      "Epoch 2076, Loss: 0.003987995412899181, Final Batch Loss: 0.00041996812797151506\n",
      "Epoch 2077, Loss: 0.0019913986689061858, Final Batch Loss: 0.00012325914576649666\n",
      "Epoch 2078, Loss: 0.0025092322830460034, Final Batch Loss: 0.00043500421452336013\n",
      "Epoch 2079, Loss: 0.0008873435472196434, Final Batch Loss: 0.00015626667300239205\n",
      "Epoch 2080, Loss: 0.0021191987543716095, Final Batch Loss: 0.00027658784529194236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2081, Loss: 0.021344976674299687, Final Batch Loss: 0.0006237885099835694\n",
      "Epoch 2082, Loss: 0.041076603629335295, Final Batch Loss: 0.039735835045576096\n",
      "Epoch 2083, Loss: 0.014415270590689033, Final Batch Loss: 0.00013234459038358182\n",
      "Epoch 2084, Loss: 0.025549166777636856, Final Batch Loss: 0.00030853983480483294\n",
      "Epoch 2085, Loss: 0.0033478221885161474, Final Batch Loss: 0.0001030805433401838\n",
      "Epoch 2086, Loss: 0.009396380497491919, Final Batch Loss: 0.001188780413940549\n",
      "Epoch 2087, Loss: 0.002360747046623146, Final Batch Loss: 2.5331439246656373e-05\n",
      "Epoch 2088, Loss: 0.005407205782830715, Final Batch Loss: 0.002422985853627324\n",
      "Epoch 2089, Loss: 0.0034303932334296405, Final Batch Loss: 0.00015196269669104367\n",
      "Epoch 2090, Loss: 0.004344067525380524, Final Batch Loss: 0.00020840263459831476\n",
      "Epoch 2091, Loss: 0.008860975154675543, Final Batch Loss: 0.00017599857528693974\n",
      "Epoch 2092, Loss: 0.0026063196128234267, Final Batch Loss: 0.00011220684973523021\n",
      "Epoch 2093, Loss: 0.00281253585126251, Final Batch Loss: 0.0002202980685979128\n",
      "Epoch 2094, Loss: 0.0040868197320378385, Final Batch Loss: 7.606931467307732e-05\n",
      "Epoch 2095, Loss: 0.0017946930183825316, Final Batch Loss: 1.3039289115113206e-05\n",
      "Epoch 2096, Loss: 0.007277849872480147, Final Batch Loss: 0.006017519161105156\n",
      "Epoch 2097, Loss: 0.0030633901842520572, Final Batch Loss: 0.0009984706994146109\n",
      "Epoch 2098, Loss: 0.02392305377725279, Final Batch Loss: 4.052709846291691e-05\n",
      "Epoch 2099, Loss: 0.0026685176126193255, Final Batch Loss: 0.0004992580506950617\n",
      "Epoch 2100, Loss: 0.0035830742708640173, Final Batch Loss: 8.28475458547473e-05\n",
      "Epoch 2101, Loss: 0.003914404675015248, Final Batch Loss: 0.0003235016774851829\n",
      "Epoch 2102, Loss: 0.006296830797509756, Final Batch Loss: 0.0002528224722482264\n",
      "Epoch 2103, Loss: 0.0032210657736868598, Final Batch Loss: 5.436107312561944e-05\n",
      "Epoch 2104, Loss: 0.0023384882952086627, Final Batch Loss: 0.0005807089619338512\n",
      "Epoch 2105, Loss: 0.00867905156701454, Final Batch Loss: 8.385981345782056e-06\n",
      "Epoch 2106, Loss: 0.0016151343079400249, Final Batch Loss: 0.0002868289011530578\n",
      "Epoch 2107, Loss: 0.0011943580175284296, Final Batch Loss: 0.00039415815263055265\n",
      "Epoch 2108, Loss: 0.008789000668912195, Final Batch Loss: 0.0054925307631492615\n",
      "Epoch 2109, Loss: 0.03945577499689534, Final Batch Loss: 0.0011792522855103016\n",
      "Epoch 2110, Loss: 0.0017977328679990023, Final Batch Loss: 0.0005280018085613847\n",
      "Epoch 2111, Loss: 0.020426123970537446, Final Batch Loss: 0.002017346443608403\n",
      "Epoch 2112, Loss: 0.02229172579245642, Final Batch Loss: 0.008224934339523315\n",
      "Epoch 2113, Loss: 0.0018853026849683374, Final Batch Loss: 8.692793198861182e-05\n",
      "Epoch 2114, Loss: 0.0023207556077977642, Final Batch Loss: 6.702665996272117e-05\n",
      "Epoch 2115, Loss: 0.00515660566452425, Final Batch Loss: 0.0006872775265946984\n",
      "Epoch 2116, Loss: 0.02094425274117384, Final Batch Loss: 0.019329950213432312\n",
      "Epoch 2117, Loss: 0.0014475731732090935, Final Batch Loss: 0.00041193017386831343\n",
      "Epoch 2118, Loss: 0.013304737485668738, Final Batch Loss: 0.0001076616536010988\n",
      "Epoch 2119, Loss: 0.010816257432452403, Final Batch Loss: 0.0002001821412704885\n",
      "Epoch 2120, Loss: 0.0055939606099855155, Final Batch Loss: 0.0021886799950152636\n",
      "Epoch 2121, Loss: 0.011167181175551377, Final Batch Loss: 0.006569392513483763\n",
      "Epoch 2122, Loss: 0.0010238971444778144, Final Batch Loss: 0.0002550601784605533\n",
      "Epoch 2123, Loss: 0.003216518547560554, Final Batch Loss: 0.0010081753134727478\n",
      "Epoch 2124, Loss: 0.004002642206614837, Final Batch Loss: 0.0019963032100349665\n",
      "Epoch 2125, Loss: 0.02443975763162598, Final Batch Loss: 0.00024974849657155573\n",
      "Epoch 2126, Loss: 0.012235051828611176, Final Batch Loss: 6.117053999332711e-05\n",
      "Epoch 2127, Loss: 0.015014919914392522, Final Batch Loss: 4.4049778807675466e-05\n",
      "Epoch 2128, Loss: 0.005542935978155583, Final Batch Loss: 0.0006278940127231181\n",
      "Epoch 2129, Loss: 0.002561441622674465, Final Batch Loss: 0.00013594333722721785\n",
      "Epoch 2130, Loss: 0.008519059949321672, Final Batch Loss: 0.00032890334841795266\n",
      "Epoch 2131, Loss: 0.005645647855999414, Final Batch Loss: 0.0004345040943007916\n",
      "Epoch 2132, Loss: 0.0020367266588436905, Final Batch Loss: 9.117966692429036e-05\n",
      "Epoch 2133, Loss: 0.002141238277545199, Final Batch Loss: 0.000747207086533308\n",
      "Epoch 2134, Loss: 0.005983315350022167, Final Batch Loss: 0.0030356007628142834\n",
      "Epoch 2135, Loss: 0.012992750911507756, Final Batch Loss: 0.00017923236009664834\n",
      "Epoch 2136, Loss: 0.0019215961074223742, Final Batch Loss: 6.448588101193309e-05\n",
      "Epoch 2137, Loss: 0.0028744589362759143, Final Batch Loss: 0.0004112678288947791\n",
      "Epoch 2138, Loss: 0.0054664924027747475, Final Batch Loss: 0.000941624806728214\n",
      "Epoch 2139, Loss: 0.017517988075269386, Final Batch Loss: 0.0008694689022377133\n",
      "Epoch 2140, Loss: 0.005112836122862063, Final Batch Loss: 0.00010718005069065839\n",
      "Epoch 2141, Loss: 0.0032755418797023594, Final Batch Loss: 0.0010456974850967526\n",
      "Epoch 2142, Loss: 0.006380934049957432, Final Batch Loss: 0.00017967107123695314\n",
      "Epoch 2143, Loss: 0.004792727097083116, Final Batch Loss: 4.816844375454821e-05\n",
      "Epoch 2144, Loss: 0.004698940989328548, Final Batch Loss: 0.0018566387007012963\n",
      "Epoch 2145, Loss: 0.003763309447094798, Final Batch Loss: 0.0013671189080923796\n",
      "Epoch 2146, Loss: 0.020865324666374363, Final Batch Loss: 0.000705598620697856\n",
      "Epoch 2147, Loss: 0.0014116714519332163, Final Batch Loss: 9.308505832450464e-05\n",
      "Epoch 2148, Loss: 0.0029905308329034597, Final Batch Loss: 0.0011915902141481638\n",
      "Epoch 2149, Loss: 0.0027993390103802085, Final Batch Loss: 0.0009896514238789678\n",
      "Epoch 2150, Loss: 0.005168493449673406, Final Batch Loss: 0.0008318197797052562\n",
      "Epoch 2151, Loss: 0.021997950680088252, Final Batch Loss: 0.00045863614650443196\n",
      "Epoch 2152, Loss: 0.01252311421558261, Final Batch Loss: 0.0021588883828371763\n",
      "Epoch 2153, Loss: 0.014862054493278265, Final Batch Loss: 0.007185818627476692\n",
      "Epoch 2154, Loss: 0.023206854675663635, Final Batch Loss: 0.0001110508746933192\n",
      "Epoch 2155, Loss: 0.04684981339960359, Final Batch Loss: 0.0009942884789779782\n",
      "Epoch 2156, Loss: 0.006649236544035375, Final Batch Loss: 0.0009380791452713311\n",
      "Epoch 2157, Loss: 0.07303207161021419, Final Batch Loss: 0.06366804987192154\n",
      "Epoch 2158, Loss: 0.025171084998873994, Final Batch Loss: 0.002194521715864539\n",
      "Epoch 2159, Loss: 0.02308195547084324, Final Batch Loss: 0.01032462902367115\n",
      "Epoch 2160, Loss: 0.004276081668649567, Final Batch Loss: 5.638836955768056e-05\n",
      "Epoch 2161, Loss: 0.017710757441818714, Final Batch Loss: 0.00014284648932516575\n",
      "Epoch 2162, Loss: 0.005564046165090986, Final Batch Loss: 0.0001260404969798401\n",
      "Epoch 2163, Loss: 0.006015132792526856, Final Batch Loss: 0.0009180413326248527\n",
      "Epoch 2164, Loss: 0.007478232386347372, Final Batch Loss: 0.0011153382947668433\n",
      "Epoch 2165, Loss: 0.028097985923523083, Final Batch Loss: 0.016151772812008858\n",
      "Epoch 2166, Loss: 0.054277358678518794, Final Batch Loss: 0.05173756927251816\n",
      "Epoch 2167, Loss: 0.011589115347305778, Final Batch Loss: 6.631635915255174e-05\n",
      "Epoch 2168, Loss: 0.006814310581830796, Final Batch Loss: 0.0016989868599921465\n",
      "Epoch 2169, Loss: 0.006368463422404602, Final Batch Loss: 0.0003931691462639719\n",
      "Epoch 2170, Loss: 0.013261871063150465, Final Batch Loss: 0.009234572760760784\n",
      "Epoch 2171, Loss: 0.007463760077371262, Final Batch Loss: 0.00011258693120907992\n",
      "Epoch 2172, Loss: 0.010011446520366007, Final Batch Loss: 0.0007698739063926041\n",
      "Epoch 2173, Loss: 0.0034817254199879244, Final Batch Loss: 0.00010027167445514351\n",
      "Epoch 2174, Loss: 0.001731519958411809, Final Batch Loss: 7.405911310343072e-05\n",
      "Epoch 2175, Loss: 0.006760874995961785, Final Batch Loss: 0.00040992978028953075\n",
      "Epoch 2176, Loss: 0.0024381960502068978, Final Batch Loss: 0.00012091253302060068\n",
      "Epoch 2177, Loss: 0.0023487993312301114, Final Batch Loss: 0.000616069883108139\n",
      "Epoch 2178, Loss: 0.004836146013985854, Final Batch Loss: 6.725169805577025e-05\n",
      "Epoch 2179, Loss: 0.015525905924732797, Final Batch Loss: 8.880697714630514e-05\n",
      "Epoch 2180, Loss: 0.0071670954348519444, Final Batch Loss: 0.00014528408064506948\n",
      "Epoch 2181, Loss: 0.006554159743245691, Final Batch Loss: 0.001031923689879477\n",
      "Epoch 2182, Loss: 0.002377443262957968, Final Batch Loss: 0.00020950906036887318\n",
      "Epoch 2183, Loss: 0.002115767143550329, Final Batch Loss: 0.00048233557026833296\n",
      "Epoch 2184, Loss: 0.0026665307450457476, Final Batch Loss: 7.5118376116734e-05\n",
      "Epoch 2185, Loss: 0.008292054728372023, Final Batch Loss: 0.003087032353505492\n",
      "Epoch 2186, Loss: 0.02369837130754604, Final Batch Loss: 5.9763257013401017e-05\n",
      "Epoch 2187, Loss: 0.008632629884232301, Final Batch Loss: 0.0006122065242379904\n",
      "Epoch 2188, Loss: 0.06055229448247701, Final Batch Loss: 0.030734434723854065\n",
      "Epoch 2189, Loss: 0.002934722710051574, Final Batch Loss: 0.0002955589152406901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2190, Loss: 0.0136349534150213, Final Batch Loss: 0.0031538959592580795\n",
      "Epoch 2191, Loss: 0.008041260531172156, Final Batch Loss: 0.0001521320955362171\n",
      "Epoch 2192, Loss: 0.011418140209570993, Final Batch Loss: 0.005418477579951286\n",
      "Epoch 2193, Loss: 0.003589182728319429, Final Batch Loss: 0.00023997983953449875\n",
      "Epoch 2194, Loss: 0.002720900796703063, Final Batch Loss: 0.0008555326494388282\n",
      "Epoch 2195, Loss: 0.008422958417213522, Final Batch Loss: 0.0019466872327029705\n",
      "Epoch 2196, Loss: 0.003468799201073125, Final Batch Loss: 0.00018864544108510017\n",
      "Epoch 2197, Loss: 0.0018172350028180517, Final Batch Loss: 0.0005447073490358889\n",
      "Epoch 2198, Loss: 0.004074079872225411, Final Batch Loss: 0.0006462334422394633\n",
      "Epoch 2199, Loss: 0.00413545107585378, Final Batch Loss: 0.0022251158952713013\n",
      "Epoch 2200, Loss: 0.005883547913981602, Final Batch Loss: 0.00011058457312174141\n",
      "Epoch 2201, Loss: 0.015493291430175304, Final Batch Loss: 0.0008094109944067895\n",
      "Epoch 2202, Loss: 0.003840815130388364, Final Batch Loss: 4.501736839301884e-05\n",
      "Epoch 2203, Loss: 0.013302046434546355, Final Batch Loss: 0.0005781363579444587\n",
      "Epoch 2204, Loss: 0.010419648053357378, Final Batch Loss: 7.887369429226965e-05\n",
      "Epoch 2205, Loss: 0.00656047610391397, Final Batch Loss: 0.0014591892249882221\n",
      "Epoch 2206, Loss: 0.0038425904931500554, Final Batch Loss: 0.0003221898223273456\n",
      "Epoch 2207, Loss: 0.005553725823119748, Final Batch Loss: 0.002182558411732316\n",
      "Epoch 2208, Loss: 0.004027665447210893, Final Batch Loss: 0.00016908944235183299\n",
      "Epoch 2209, Loss: 0.008197893490432762, Final Batch Loss: 4.114090552320704e-05\n",
      "Epoch 2210, Loss: 0.009894206072203815, Final Batch Loss: 0.00862049963325262\n",
      "Epoch 2211, Loss: 0.008439838304184377, Final Batch Loss: 0.00033274831366725266\n",
      "Epoch 2212, Loss: 0.013965711230412126, Final Batch Loss: 0.0002798750065267086\n",
      "Epoch 2213, Loss: 0.01739388426358346, Final Batch Loss: 7.006780651863664e-05\n",
      "Epoch 2214, Loss: 0.029859502508770674, Final Batch Loss: 0.00016256450908258557\n",
      "Epoch 2215, Loss: 0.05113390655606054, Final Batch Loss: 0.0002402920217718929\n",
      "Epoch 2216, Loss: 0.0030650298431282863, Final Batch Loss: 0.00023552852508146316\n",
      "Epoch 2217, Loss: 0.0077058162714820355, Final Batch Loss: 0.00045244142529554665\n",
      "Epoch 2218, Loss: 0.005310813110554591, Final Batch Loss: 0.003203689819201827\n",
      "Epoch 2219, Loss: 0.005269035929813981, Final Batch Loss: 0.0008315070299431682\n",
      "Epoch 2220, Loss: 0.004171338918240508, Final Batch Loss: 4.807099685422145e-05\n",
      "Epoch 2221, Loss: 0.0025463529163971543, Final Batch Loss: 0.00014030263992026448\n",
      "Epoch 2222, Loss: 0.007149428043248918, Final Batch Loss: 0.0006237193010747433\n",
      "Epoch 2223, Loss: 0.002053177795460215, Final Batch Loss: 0.00011907143925782293\n",
      "Epoch 2224, Loss: 0.007602711964864284, Final Batch Loss: 9.802063868846744e-05\n",
      "Epoch 2225, Loss: 0.005415811639977619, Final Batch Loss: 0.00011234608246013522\n",
      "Epoch 2226, Loss: 0.00743650775984861, Final Batch Loss: 0.0027605146169662476\n",
      "Epoch 2227, Loss: 0.034119892218768655, Final Batch Loss: 4.972983333573211e-06\n",
      "Epoch 2228, Loss: 0.012067520801792853, Final Batch Loss: 0.0024422486312687397\n",
      "Epoch 2229, Loss: 0.002877317177990335, Final Batch Loss: 1.638447974983137e-05\n",
      "Epoch 2230, Loss: 0.0034034373638860416, Final Batch Loss: 5.306560706230812e-05\n",
      "Epoch 2231, Loss: 0.004111744194233324, Final Batch Loss: 7.029393600532785e-05\n",
      "Epoch 2232, Loss: 0.0035910607439291198, Final Batch Loss: 0.00040971016278490424\n",
      "Epoch 2233, Loss: 0.0032128952370840125, Final Batch Loss: 7.79398760641925e-05\n",
      "Epoch 2234, Loss: 0.0013914372775616357, Final Batch Loss: 2.4529626898583956e-05\n",
      "Epoch 2235, Loss: 0.0035702288660104387, Final Batch Loss: 0.001000801450572908\n",
      "Epoch 2236, Loss: 0.06549242773326114, Final Batch Loss: 0.0006237703491933644\n",
      "Epoch 2237, Loss: 0.003735936217708513, Final Batch Loss: 0.0009046354098245502\n",
      "Epoch 2238, Loss: 0.0045507742324844, Final Batch Loss: 0.0013731495710089803\n",
      "Epoch 2239, Loss: 0.005168381263501942, Final Batch Loss: 0.003171660238876939\n",
      "Epoch 2240, Loss: 0.0041576958319637924, Final Batch Loss: 0.00017980860138777643\n",
      "Epoch 2241, Loss: 0.005543905455851927, Final Batch Loss: 0.0017109037144109607\n",
      "Epoch 2242, Loss: 0.003029689505638089, Final Batch Loss: 0.0007818612502887845\n",
      "Epoch 2243, Loss: 0.004987696866010083, Final Batch Loss: 0.000828585762064904\n",
      "Epoch 2244, Loss: 0.010436724522151053, Final Batch Loss: 0.00048580148722976446\n",
      "Epoch 2245, Loss: 0.0017371586654917337, Final Batch Loss: 5.756809332524426e-05\n",
      "Epoch 2246, Loss: 0.0012375009500829037, Final Batch Loss: 0.00013285191380418837\n",
      "Epoch 2247, Loss: 0.007455009937984869, Final Batch Loss: 0.00024068530183285475\n",
      "Epoch 2248, Loss: 0.0029954229030408897, Final Batch Loss: 0.00011913979687960818\n",
      "Epoch 2249, Loss: 0.0022038040015104343, Final Batch Loss: 0.0008085345034487545\n",
      "Epoch 2250, Loss: 0.002156438960810192, Final Batch Loss: 0.00016290716303046793\n",
      "Epoch 2251, Loss: 0.004400876659929054, Final Batch Loss: 3.8229514757404104e-05\n",
      "Epoch 2252, Loss: 0.0032371142442571, Final Batch Loss: 0.0002136249968316406\n",
      "Epoch 2253, Loss: 0.005877914518350735, Final Batch Loss: 0.00027927677729167044\n",
      "Epoch 2254, Loss: 0.0029054527076368686, Final Batch Loss: 0.0006700076046399772\n",
      "Epoch 2255, Loss: 0.01215382929694897, Final Batch Loss: 2.6304065613658167e-05\n",
      "Epoch 2256, Loss: 0.004695815616287291, Final Batch Loss: 0.001276975148357451\n",
      "Epoch 2257, Loss: 0.0012640809873118997, Final Batch Loss: 0.0004256752727087587\n",
      "Epoch 2258, Loss: 0.0053618442980223335, Final Batch Loss: 7.914601155789569e-05\n",
      "Epoch 2259, Loss: 0.0024376844294238253, Final Batch Loss: 0.0008103637374006212\n",
      "Epoch 2260, Loss: 0.002056088353128871, Final Batch Loss: 2.9960367101011798e-05\n",
      "Epoch 2261, Loss: 0.0032487133485119557, Final Batch Loss: 2.522512477298733e-05\n",
      "Epoch 2262, Loss: 0.005251504720945377, Final Batch Loss: 6.581133493455127e-05\n",
      "Epoch 2263, Loss: 0.03795199235173641, Final Batch Loss: 0.00013749879144597799\n",
      "Epoch 2264, Loss: 0.003689304990984965, Final Batch Loss: 0.002207714132964611\n",
      "Epoch 2265, Loss: 0.028840468876296654, Final Batch Loss: 0.0001986836432479322\n",
      "Epoch 2266, Loss: 0.005335556576028466, Final Batch Loss: 0.0004945045802742243\n",
      "Epoch 2267, Loss: 0.026505125570110977, Final Batch Loss: 0.0014559583505615592\n",
      "Epoch 2268, Loss: 0.0070983595978759695, Final Batch Loss: 0.00011535425437614322\n",
      "Epoch 2269, Loss: 0.006048521143384278, Final Batch Loss: 0.00017500409740023315\n",
      "Epoch 2270, Loss: 0.001984035217901692, Final Batch Loss: 0.0010214622598141432\n",
      "Epoch 2271, Loss: 0.0037426089111249894, Final Batch Loss: 0.001976701896637678\n",
      "Epoch 2272, Loss: 0.0018114812264684588, Final Batch Loss: 0.00018606170488055795\n",
      "Epoch 2273, Loss: 0.009492586672422476, Final Batch Loss: 0.0025099192280322313\n",
      "Epoch 2274, Loss: 0.0024728660901018884, Final Batch Loss: 2.6983572752214968e-05\n",
      "Epoch 2275, Loss: 0.002987011437653564, Final Batch Loss: 0.00017559829575475305\n",
      "Epoch 2276, Loss: 0.0026799928236869164, Final Batch Loss: 0.0014879085356369615\n",
      "Epoch 2277, Loss: 0.013544701359933242, Final Batch Loss: 0.00012771764886565506\n",
      "Epoch 2278, Loss: 0.013891929818782955, Final Batch Loss: 0.001206079963594675\n",
      "Epoch 2279, Loss: 0.012375892911222763, Final Batch Loss: 1.8256119801662862e-05\n",
      "Epoch 2280, Loss: 0.0015115595761017175, Final Batch Loss: 1.608165075595025e-05\n",
      "Epoch 2281, Loss: 0.0013036557720624842, Final Batch Loss: 0.000286081456579268\n",
      "Epoch 2282, Loss: 0.02554515693555004, Final Batch Loss: 0.0015665609389543533\n",
      "Epoch 2283, Loss: 0.011423130024923012, Final Batch Loss: 0.0008721673511900008\n",
      "Epoch 2284, Loss: 0.005275404255371541, Final Batch Loss: 0.001126400544308126\n",
      "Epoch 2285, Loss: 0.006483927747467533, Final Batch Loss: 0.00015548450755886734\n",
      "Epoch 2286, Loss: 0.005054861041571712, Final Batch Loss: 2.6867788619711064e-05\n",
      "Epoch 2287, Loss: 0.00467912336171139, Final Batch Loss: 0.0001658903347561136\n",
      "Epoch 2288, Loss: 0.0013254573859740049, Final Batch Loss: 0.00010800334712257609\n",
      "Epoch 2289, Loss: 0.025774152803933248, Final Batch Loss: 3.783062857110053e-05\n",
      "Epoch 2290, Loss: 0.014331570331705734, Final Batch Loss: 0.0004810294194612652\n",
      "Epoch 2291, Loss: 0.019683551232446916, Final Batch Loss: 0.010853210464119911\n",
      "Epoch 2292, Loss: 0.004302926499804016, Final Batch Loss: 8.73628378030844e-05\n",
      "Epoch 2293, Loss: 0.005395675998443039, Final Batch Loss: 5.916365989833139e-05\n",
      "Epoch 2294, Loss: 0.003461852683358302, Final Batch Loss: 4.0907543734647334e-05\n",
      "Epoch 2295, Loss: 0.016279971021504025, Final Batch Loss: 2.5458246454945765e-05\n",
      "Epoch 2296, Loss: 0.025450978821027093, Final Batch Loss: 0.02383318729698658\n",
      "Epoch 2297, Loss: 0.0044781098258681595, Final Batch Loss: 0.0004898309707641602\n",
      "Epoch 2298, Loss: 0.008909912939998321, Final Batch Loss: 0.0015785199357196689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2299, Loss: 0.0044132872089903685, Final Batch Loss: 1.3650870641868096e-05\n",
      "Epoch 2300, Loss: 0.01934729159802373, Final Batch Loss: 2.62808953266358e-05\n",
      "Epoch 2301, Loss: 0.01630186240072362, Final Batch Loss: 5.3971911256667227e-05\n",
      "Epoch 2302, Loss: 0.0022863189587951638, Final Batch Loss: 0.0009773196652531624\n",
      "Epoch 2303, Loss: 0.004220634823013825, Final Batch Loss: 7.219649432954611e-06\n",
      "Epoch 2304, Loss: 0.004765784142364282, Final Batch Loss: 0.00010239971015835181\n",
      "Epoch 2305, Loss: 0.03922719488036819, Final Batch Loss: 0.037361592054367065\n",
      "Epoch 2306, Loss: 0.0028777685365639627, Final Batch Loss: 0.0004300791770219803\n",
      "Epoch 2307, Loss: 0.002857508836314082, Final Batch Loss: 0.0012997165322303772\n",
      "Epoch 2308, Loss: 0.002260679324535886, Final Batch Loss: 4.799038651981391e-05\n",
      "Epoch 2309, Loss: 0.0017327333844150417, Final Batch Loss: 0.0001753911201376468\n",
      "Epoch 2310, Loss: 0.0009470462828176096, Final Batch Loss: 0.00012026762124150991\n",
      "Epoch 2311, Loss: 0.023006982355582295, Final Batch Loss: 0.003722751047462225\n",
      "Epoch 2312, Loss: 0.006500209135992918, Final Batch Loss: 0.006169599946588278\n",
      "Epoch 2313, Loss: 0.009584059178450843, Final Batch Loss: 0.0006360125844366848\n",
      "Epoch 2314, Loss: 0.01023451729270164, Final Batch Loss: 7.12839319021441e-05\n",
      "Epoch 2315, Loss: 0.0055915144039317966, Final Batch Loss: 0.0004914696328341961\n",
      "Epoch 2316, Loss: 0.006490395258879289, Final Batch Loss: 0.0017322817584499717\n",
      "Epoch 2317, Loss: 0.0019242506241425872, Final Batch Loss: 0.00010691402712836862\n",
      "Epoch 2318, Loss: 0.0013365711638471112, Final Batch Loss: 2.5072513381019235e-05\n",
      "Epoch 2319, Loss: 0.004230372749589151, Final Batch Loss: 0.00041811258415691555\n",
      "Epoch 2320, Loss: 0.00047993866610340774, Final Batch Loss: 5.502908970811404e-05\n",
      "Epoch 2321, Loss: 0.0009979943897633348, Final Batch Loss: 0.00011020518286386505\n",
      "Epoch 2322, Loss: 0.0023345709560089745, Final Batch Loss: 0.0002931665803771466\n",
      "Epoch 2323, Loss: 0.005762151457020082, Final Batch Loss: 0.0006469443324021995\n",
      "Epoch 2324, Loss: 0.0024753788457019255, Final Batch Loss: 8.571099897380918e-05\n",
      "Epoch 2325, Loss: 0.004249181889463216, Final Batch Loss: 0.0009243948734365404\n",
      "Epoch 2326, Loss: 0.005544281740185397, Final Batch Loss: 0.0008642877219244838\n",
      "Epoch 2327, Loss: 0.003264289720391389, Final Batch Loss: 6.365087028825656e-05\n",
      "Epoch 2328, Loss: 0.009723563811348868, Final Batch Loss: 0.00013668680912815034\n",
      "Epoch 2329, Loss: 0.021699911943869665, Final Batch Loss: 0.0006062214961275458\n",
      "Epoch 2330, Loss: 0.007647108344826847, Final Batch Loss: 0.0015576548175886273\n",
      "Epoch 2331, Loss: 0.012400198858813383, Final Batch Loss: 0.00011018793156836182\n",
      "Epoch 2332, Loss: 0.02442187536507845, Final Batch Loss: 0.0025858194567263126\n",
      "Epoch 2333, Loss: 0.003574163063603919, Final Batch Loss: 8.019931556191295e-05\n",
      "Epoch 2334, Loss: 0.00857558599091135, Final Batch Loss: 0.00040880622691474855\n",
      "Epoch 2335, Loss: 0.0018740810482995585, Final Batch Loss: 1.7387363186571747e-05\n",
      "Epoch 2336, Loss: 0.005219977458182257, Final Batch Loss: 0.0003049230726901442\n",
      "Epoch 2337, Loss: 0.002797486562485574, Final Batch Loss: 9.924195910571143e-05\n",
      "Epoch 2338, Loss: 0.0013316004988155328, Final Batch Loss: 7.293561793630943e-05\n",
      "Epoch 2339, Loss: 0.016202346043428406, Final Batch Loss: 0.00030731738661415875\n",
      "Epoch 2340, Loss: 0.0022482586791738868, Final Batch Loss: 0.00026580231497064233\n",
      "Epoch 2341, Loss: 0.022934631153475493, Final Batch Loss: 0.02082662843167782\n",
      "Epoch 2342, Loss: 0.003275536912951793, Final Batch Loss: 8.37938114273129e-06\n",
      "Epoch 2343, Loss: 0.0021642693391186185, Final Batch Loss: 1.9123101083096117e-05\n",
      "Epoch 2344, Loss: 0.010567493140115403, Final Batch Loss: 0.00032393651781603694\n",
      "Epoch 2345, Loss: 0.0014280560208135284, Final Batch Loss: 7.199618994491175e-05\n",
      "Epoch 2346, Loss: 0.006211861444171518, Final Batch Loss: 0.00044392506242729723\n",
      "Epoch 2347, Loss: 0.006353012038744055, Final Batch Loss: 0.00012129231618018821\n",
      "Epoch 2348, Loss: 0.002327513531781733, Final Batch Loss: 0.0008139206329360604\n",
      "Epoch 2349, Loss: 0.005236657365458086, Final Batch Loss: 0.0006836408865638077\n",
      "Epoch 2350, Loss: 0.016161092775291763, Final Batch Loss: 0.0014769891276955605\n",
      "Epoch 2351, Loss: 0.0036557584185175074, Final Batch Loss: 0.0015210297424346209\n",
      "Epoch 2352, Loss: 0.0027858141766046174, Final Batch Loss: 0.00021160324104130268\n",
      "Epoch 2353, Loss: 0.0025414441588509362, Final Batch Loss: 0.00013221005792729557\n",
      "Epoch 2354, Loss: 0.0018282126620761119, Final Batch Loss: 0.0003133669379167259\n",
      "Epoch 2355, Loss: 0.02411577046586899, Final Batch Loss: 4.8665555368643254e-05\n",
      "Epoch 2356, Loss: 0.0014652300160378218, Final Batch Loss: 8.174889080692083e-05\n",
      "Epoch 2357, Loss: 0.02526425980613567, Final Batch Loss: 0.0122390016913414\n",
      "Epoch 2358, Loss: 0.006061460415367037, Final Batch Loss: 0.00020691394456662238\n",
      "Epoch 2359, Loss: 0.006076473218854517, Final Batch Loss: 0.0002611782110761851\n",
      "Epoch 2360, Loss: 0.012489538465160877, Final Batch Loss: 9.585694351699203e-05\n",
      "Epoch 2361, Loss: 0.0036742388329003006, Final Batch Loss: 0.0009598626056686044\n",
      "Epoch 2362, Loss: 0.02008384431246668, Final Batch Loss: 0.0006147116655483842\n",
      "Epoch 2363, Loss: 0.01560126242111437, Final Batch Loss: 7.100529182935134e-05\n",
      "Epoch 2364, Loss: 0.0020549539258354343, Final Batch Loss: 0.00033094064565375447\n",
      "Epoch 2365, Loss: 0.007492245844332501, Final Batch Loss: 0.00022240460384637117\n",
      "Epoch 2366, Loss: 0.00898228763253428, Final Batch Loss: 0.00019980588695034385\n",
      "Epoch 2367, Loss: 0.07390945368388202, Final Batch Loss: 0.0008708509849384427\n",
      "Epoch 2368, Loss: 0.004962854727637023, Final Batch Loss: 0.00017687657964415848\n",
      "Epoch 2369, Loss: 0.003648227153462358, Final Batch Loss: 0.000461028452264145\n",
      "Epoch 2370, Loss: 0.00453669713169802, Final Batch Loss: 0.0006133881397545338\n",
      "Epoch 2371, Loss: 0.0017374993331031874, Final Batch Loss: 9.882051381282508e-05\n",
      "Epoch 2372, Loss: 0.0018137452279916033, Final Batch Loss: 0.0003283392870798707\n",
      "Epoch 2373, Loss: 0.006108394358307123, Final Batch Loss: 0.00039834226481616497\n",
      "Epoch 2374, Loss: 0.019073296280112118, Final Batch Loss: 0.0008589995559304953\n",
      "Epoch 2375, Loss: 0.002393539500189945, Final Batch Loss: 0.0003993032732978463\n",
      "Epoch 2376, Loss: 0.001916060980875045, Final Batch Loss: 0.00027896728715859354\n",
      "Epoch 2377, Loss: 0.032260463245620485, Final Batch Loss: 0.030764082446694374\n",
      "Epoch 2378, Loss: 0.030671594198793173, Final Batch Loss: 0.02776917815208435\n",
      "Epoch 2379, Loss: 0.04746618401259184, Final Batch Loss: 0.0019335364922881126\n",
      "Epoch 2380, Loss: 0.0055632584990235046, Final Batch Loss: 0.0019590880256146193\n",
      "Epoch 2381, Loss: 0.004499765316722915, Final Batch Loss: 0.00027546254568733275\n",
      "Epoch 2382, Loss: 0.005693151819286868, Final Batch Loss: 0.0010158640798181295\n",
      "Epoch 2383, Loss: 0.017614668351598084, Final Batch Loss: 0.008020157925784588\n",
      "Epoch 2384, Loss: 0.03805662508239038, Final Batch Loss: 0.00015657783660572022\n",
      "Epoch 2385, Loss: 0.05581766084651463, Final Batch Loss: 0.04772636294364929\n",
      "Epoch 2386, Loss: 0.016228694235906005, Final Batch Loss: 0.00166508078109473\n",
      "Epoch 2387, Loss: 0.016449008544441313, Final Batch Loss: 0.010894226841628551\n",
      "Epoch 2388, Loss: 0.005028948471590411, Final Batch Loss: 0.0003112391277682036\n",
      "Epoch 2389, Loss: 0.0035076722997473553, Final Batch Loss: 0.0013132225722074509\n",
      "Epoch 2390, Loss: 0.010742459853645414, Final Batch Loss: 0.00025061756605282426\n",
      "Epoch 2391, Loss: 0.014358221320435405, Final Batch Loss: 0.00024692833540029824\n",
      "Epoch 2392, Loss: 0.030640633311122656, Final Batch Loss: 0.001267376122996211\n",
      "Epoch 2393, Loss: 0.029206529739894904, Final Batch Loss: 0.00020413805032148957\n",
      "Epoch 2394, Loss: 0.006765500038454775, Final Batch Loss: 0.0001143014887929894\n",
      "Epoch 2395, Loss: 0.0035544809215934947, Final Batch Loss: 0.00048234735731966794\n",
      "Epoch 2396, Loss: 0.003596654270950239, Final Batch Loss: 0.0015665661776438355\n",
      "Epoch 2397, Loss: 0.021160278527531773, Final Batch Loss: 0.000208993413252756\n",
      "Epoch 2398, Loss: 0.0035609218466561288, Final Batch Loss: 0.000817007152363658\n",
      "Epoch 2399, Loss: 0.02531035483116284, Final Batch Loss: 0.0031428602524101734\n",
      "Epoch 2400, Loss: 0.03849084203829989, Final Batch Loss: 0.020990589633584023\n",
      "Epoch 2401, Loss: 0.022325142519548535, Final Batch Loss: 0.0007780689047649503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2402, Loss: 0.031419131279108115, Final Batch Loss: 0.00024083069001790136\n",
      "Epoch 2403, Loss: 0.005923035088926554, Final Batch Loss: 0.0006104280473664403\n",
      "Epoch 2404, Loss: 0.012023009709082544, Final Batch Loss: 0.0006879708962514997\n",
      "Epoch 2405, Loss: 0.004490800143685192, Final Batch Loss: 0.001742681604810059\n",
      "Epoch 2406, Loss: 0.08058540185447782, Final Batch Loss: 0.0004770834930241108\n",
      "Epoch 2407, Loss: 0.005657564761349931, Final Batch Loss: 0.0011227279901504517\n",
      "Epoch 2408, Loss: 0.00727912517322693, Final Batch Loss: 0.005241215229034424\n",
      "Epoch 2409, Loss: 0.0044686408364214, Final Batch Loss: 0.00039656090666539967\n",
      "Epoch 2410, Loss: 0.009665628982475027, Final Batch Loss: 0.00560284499078989\n",
      "Epoch 2411, Loss: 0.007274158648215234, Final Batch Loss: 0.00014017638750374317\n",
      "Epoch 2412, Loss: 0.005197367270739051, Final Batch Loss: 4.412419002619572e-05\n",
      "Epoch 2413, Loss: 0.004433151814737357, Final Batch Loss: 0.0008071094052866101\n",
      "Epoch 2414, Loss: 0.01718529683421366, Final Batch Loss: 0.006837846711277962\n",
      "Epoch 2415, Loss: 0.011686181664117612, Final Batch Loss: 0.00013176599168218672\n",
      "Epoch 2416, Loss: 0.07312906424340326, Final Batch Loss: 0.005091395694762468\n",
      "Epoch 2417, Loss: 0.007887925254181027, Final Batch Loss: 0.00044905050890520215\n",
      "Epoch 2418, Loss: 0.004983581224223599, Final Batch Loss: 0.0014277704758569598\n",
      "Epoch 2419, Loss: 0.008831415441818535, Final Batch Loss: 0.0004822415648959577\n",
      "Epoch 2420, Loss: 0.0037865841150050983, Final Batch Loss: 0.000597261474467814\n",
      "Epoch 2421, Loss: 0.0008486704355163965, Final Batch Loss: 2.6272224204149097e-05\n",
      "Epoch 2422, Loss: 0.0032052765491243917, Final Batch Loss: 0.001035186112858355\n",
      "Epoch 2423, Loss: 0.005671937844454078, Final Batch Loss: 0.0011169752106070518\n",
      "Epoch 2424, Loss: 0.00464005974936299, Final Batch Loss: 0.000265576527453959\n",
      "Epoch 2425, Loss: 0.002030598414421547, Final Batch Loss: 0.0013811261160299182\n",
      "Epoch 2426, Loss: 0.012388798757456243, Final Batch Loss: 0.0007222641143016517\n",
      "Epoch 2427, Loss: 0.0030400739633478224, Final Batch Loss: 0.00036583360633812845\n",
      "Epoch 2428, Loss: 0.0019499659319990315, Final Batch Loss: 7.73806605138816e-05\n",
      "Epoch 2429, Loss: 0.03384472642210312, Final Batch Loss: 0.00016201124526560307\n",
      "Epoch 2430, Loss: 0.0063075568759813905, Final Batch Loss: 0.0006298234220594168\n",
      "Epoch 2431, Loss: 0.0020074022831977345, Final Batch Loss: 0.0012272518360987306\n",
      "Epoch 2432, Loss: 0.008699505153344944, Final Batch Loss: 0.00027800892712548375\n",
      "Epoch 2433, Loss: 0.024727375464863144, Final Batch Loss: 0.0003388161421753466\n",
      "Epoch 2434, Loss: 0.0023463088291464373, Final Batch Loss: 0.00011780488421209157\n",
      "Epoch 2435, Loss: 0.028125645796535537, Final Batch Loss: 0.00298543949611485\n",
      "Epoch 2436, Loss: 0.001276225368201267, Final Batch Loss: 0.00020273405243642628\n",
      "Epoch 2437, Loss: 0.012098846651497297, Final Batch Loss: 0.0001686785399215296\n",
      "Epoch 2438, Loss: 0.004781246592756361, Final Batch Loss: 0.0011697898153215647\n",
      "Epoch 2439, Loss: 0.0039049538972903974, Final Batch Loss: 4.9037211283575743e-05\n",
      "Epoch 2440, Loss: 0.009815692141273757, Final Batch Loss: 9.764740389073268e-05\n",
      "Epoch 2441, Loss: 0.03094311689346796, Final Batch Loss: 0.0002781272924039513\n",
      "Epoch 2442, Loss: 0.0133569136432925, Final Batch Loss: 2.0847443011007272e-05\n",
      "Epoch 2443, Loss: 0.0019165541052643675, Final Batch Loss: 0.0003216671757400036\n",
      "Epoch 2444, Loss: 0.0008763813821133226, Final Batch Loss: 0.00016687838069628924\n",
      "Epoch 2445, Loss: 0.0017180791837745346, Final Batch Loss: 0.0005238670273683965\n",
      "Epoch 2446, Loss: 0.0023583291658724193, Final Batch Loss: 0.0011434846092015505\n",
      "Epoch 2447, Loss: 0.06631720181030687, Final Batch Loss: 0.049260396510362625\n",
      "Epoch 2448, Loss: 0.001965015515452251, Final Batch Loss: 0.0005326085374690592\n",
      "Epoch 2449, Loss: 0.00618468047468923, Final Batch Loss: 0.0004583640838973224\n",
      "Epoch 2450, Loss: 0.0660033585736528, Final Batch Loss: 0.0004752081586048007\n",
      "Epoch 2451, Loss: 0.0033798767253756523, Final Batch Loss: 0.0015495017869397998\n",
      "Epoch 2452, Loss: 0.008642782995593734, Final Batch Loss: 0.0007687511970289052\n",
      "Epoch 2453, Loss: 0.010072938974190038, Final Batch Loss: 7.189020834630355e-05\n",
      "Epoch 2454, Loss: 0.028647940082009882, Final Batch Loss: 0.019880933687090874\n",
      "Epoch 2455, Loss: 0.00598961349169258, Final Batch Loss: 0.0010816633002832532\n",
      "Epoch 2456, Loss: 0.01230253233370604, Final Batch Loss: 8.597619307693094e-05\n",
      "Epoch 2457, Loss: 0.005822719547722954, Final Batch Loss: 3.2203861337620765e-05\n",
      "Epoch 2458, Loss: 0.0033865178556879982, Final Batch Loss: 8.256192086264491e-05\n",
      "Epoch 2459, Loss: 0.0036519972709356807, Final Batch Loss: 0.0001309301151195541\n",
      "Epoch 2460, Loss: 0.005246727101621218, Final Batch Loss: 0.0001819994649849832\n",
      "Epoch 2461, Loss: 0.0023001306253718212, Final Batch Loss: 0.000461828924017027\n",
      "Epoch 2462, Loss: 0.018629831087309867, Final Batch Loss: 0.00040307274321094155\n",
      "Epoch 2463, Loss: 0.003341610252391547, Final Batch Loss: 0.00036447285674512386\n",
      "Epoch 2464, Loss: 0.006023693655151874, Final Batch Loss: 0.0023292971309274435\n",
      "Epoch 2465, Loss: 0.04546569574449677, Final Batch Loss: 7.673208165215328e-05\n",
      "Epoch 2466, Loss: 0.0008892212572391145, Final Batch Loss: 0.00020678582950495183\n",
      "Epoch 2467, Loss: 0.020368762081488967, Final Batch Loss: 0.00036827329313382506\n",
      "Epoch 2468, Loss: 0.04091673965740483, Final Batch Loss: 0.0006680540973320603\n",
      "Epoch 2469, Loss: 0.002950159105239436, Final Batch Loss: 0.0011221651220694184\n",
      "Epoch 2470, Loss: 0.003166846727253869, Final Batch Loss: 0.0005239782040007412\n",
      "Epoch 2471, Loss: 0.0053974353650119156, Final Batch Loss: 0.0003473589022178203\n",
      "Epoch 2472, Loss: 0.007862295664381236, Final Batch Loss: 0.0001333505497314036\n",
      "Epoch 2473, Loss: 0.06970912100223359, Final Batch Loss: 6.652790762018412e-05\n",
      "Epoch 2474, Loss: 0.0033293749147560447, Final Batch Loss: 0.0008393647149205208\n",
      "Epoch 2475, Loss: 0.004199121962301433, Final Batch Loss: 0.0006810187478549778\n",
      "Epoch 2476, Loss: 0.003072615814744495, Final Batch Loss: 0.00028261542320251465\n",
      "Epoch 2477, Loss: 0.007004600542131811, Final Batch Loss: 0.0017774953739717603\n",
      "Epoch 2478, Loss: 0.0029992160125402734, Final Batch Loss: 0.0007946737459860742\n",
      "Epoch 2479, Loss: 0.00407776140855276, Final Batch Loss: 0.0004043386725243181\n",
      "Epoch 2480, Loss: 0.005361814430216327, Final Batch Loss: 0.0014570725616067648\n",
      "Epoch 2481, Loss: 0.0013303269006428309, Final Batch Loss: 0.00016484310617670417\n",
      "Epoch 2482, Loss: 0.0016108486415760126, Final Batch Loss: 0.0009351148619316518\n",
      "Epoch 2483, Loss: 0.0016842473633005284, Final Batch Loss: 0.000309822236886248\n",
      "Epoch 2484, Loss: 0.0033727214176906273, Final Batch Loss: 0.0019220696995034814\n",
      "Epoch 2485, Loss: 0.001793993949831929, Final Batch Loss: 0.000274650810752064\n",
      "Epoch 2486, Loss: 0.0034946644154842943, Final Batch Loss: 0.000484272837638855\n",
      "Epoch 2487, Loss: 0.0031005671771708876, Final Batch Loss: 0.0016909975092858076\n",
      "Epoch 2488, Loss: 0.008329813401360298, Final Batch Loss: 4.30778345617e-05\n",
      "Epoch 2489, Loss: 0.00247577730260673, Final Batch Loss: 4.143394835409708e-05\n",
      "Epoch 2490, Loss: 0.002387372820521705, Final Batch Loss: 7.989506411831826e-05\n",
      "Epoch 2491, Loss: 0.003262235492002219, Final Batch Loss: 6.55303883831948e-05\n",
      "Epoch 2492, Loss: 0.011089467829151545, Final Batch Loss: 0.009893082082271576\n",
      "Epoch 2493, Loss: 0.004510244139964925, Final Batch Loss: 4.232058199704625e-05\n",
      "Epoch 2494, Loss: 0.014063844049815089, Final Batch Loss: 0.0007357546710409224\n",
      "Epoch 2495, Loss: 0.0071339754067594185, Final Batch Loss: 0.006092799361795187\n",
      "Epoch 2496, Loss: 0.0038456229012808762, Final Batch Loss: 0.0007902441429905593\n",
      "Epoch 2497, Loss: 0.02696981747430982, Final Batch Loss: 0.00021260313224047422\n",
      "Epoch 2498, Loss: 0.014089066469750833, Final Batch Loss: 0.006590585689991713\n",
      "Epoch 2499, Loss: 0.010264590906444937, Final Batch Loss: 0.0007875082665123045\n",
      "Epoch 2500, Loss: 0.024622864555567503, Final Batch Loss: 0.0024648879189044237\n",
      "Epoch 2501, Loss: 0.028837529549491592, Final Batch Loss: 0.0005456440267153084\n",
      "Epoch 2502, Loss: 0.012377475941320881, Final Batch Loss: 0.00010338943684473634\n",
      "Epoch 2503, Loss: 0.0024502215383108705, Final Batch Loss: 0.0005495845689438283\n",
      "Epoch 2504, Loss: 0.007210999960079789, Final Batch Loss: 0.0001406308583682403\n",
      "Epoch 2505, Loss: 0.003953344508772716, Final Batch Loss: 0.00036429951433092356\n",
      "Epoch 2506, Loss: 0.003278217278420925, Final Batch Loss: 0.00017371252761222422\n",
      "Epoch 2507, Loss: 0.00913245536503382, Final Batch Loss: 0.0002594509569462389\n",
      "Epoch 2508, Loss: 0.030889211702742614, Final Batch Loss: 0.0025942830834537745\n",
      "Epoch 2509, Loss: 0.0020218137397023384, Final Batch Loss: 5.199204679229297e-05\n",
      "Epoch 2510, Loss: 0.0029180425954109523, Final Batch Loss: 0.001724175177514553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2511, Loss: 0.0028875113275717013, Final Batch Loss: 7.300890138139948e-05\n",
      "Epoch 2512, Loss: 0.012941638866323046, Final Batch Loss: 0.00010634619684424251\n",
      "Epoch 2513, Loss: 0.021257202468405012, Final Batch Loss: 1.2571159459184855e-05\n",
      "Epoch 2514, Loss: 0.017159412665932905, Final Batch Loss: 0.0016639747191220522\n",
      "Epoch 2515, Loss: 0.03481205176649382, Final Batch Loss: 8.15434250398539e-05\n",
      "Epoch 2516, Loss: 0.016615609129075892, Final Batch Loss: 0.00021445266611408442\n",
      "Epoch 2517, Loss: 0.0037910962128080428, Final Batch Loss: 0.00046049628872424364\n",
      "Epoch 2518, Loss: 0.020515826647169888, Final Batch Loss: 0.004935491364449263\n",
      "Epoch 2519, Loss: 0.004453337081940845, Final Batch Loss: 0.0003125951625406742\n",
      "Epoch 2520, Loss: 0.037411639001220465, Final Batch Loss: 0.013684500940144062\n",
      "Epoch 2521, Loss: 0.005799363098049071, Final Batch Loss: 9.42565020523034e-05\n",
      "Epoch 2522, Loss: 0.004919932223856449, Final Batch Loss: 0.0003731256874743849\n",
      "Epoch 2523, Loss: 0.03680921721388586, Final Batch Loss: 0.00017631474474910647\n",
      "Epoch 2524, Loss: 0.010241840820526704, Final Batch Loss: 0.0058563570491969585\n",
      "Epoch 2525, Loss: 0.004544592375168577, Final Batch Loss: 0.003007703460752964\n",
      "Epoch 2526, Loss: 0.02305417854222469, Final Batch Loss: 0.0005321735516190529\n",
      "Epoch 2527, Loss: 0.02043357507500332, Final Batch Loss: 0.0001982847898034379\n",
      "Epoch 2528, Loss: 0.03218685594765702, Final Batch Loss: 0.0005894018686376512\n",
      "Epoch 2529, Loss: 0.027610810746409697, Final Batch Loss: 3.725108035723679e-05\n",
      "Epoch 2530, Loss: 0.002771919098449871, Final Batch Loss: 0.0009487899951636791\n",
      "Epoch 2531, Loss: 0.016348241304513067, Final Batch Loss: 0.0005444843554869294\n",
      "Epoch 2532, Loss: 0.007912202854640782, Final Batch Loss: 0.00013194351049605757\n",
      "Epoch 2533, Loss: 0.006653857315541245, Final Batch Loss: 0.002065925393253565\n",
      "Epoch 2534, Loss: 0.0026605982056935318, Final Batch Loss: 0.00013732006482314318\n",
      "Epoch 2535, Loss: 0.0077901377735543065, Final Batch Loss: 7.664201984880492e-05\n",
      "Epoch 2536, Loss: 0.003044995595701039, Final Batch Loss: 9.052535460796207e-05\n",
      "Epoch 2537, Loss: 0.0009561912738718092, Final Batch Loss: 0.00028697491507045925\n",
      "Epoch 2538, Loss: 0.02001818793360144, Final Batch Loss: 0.00016274735389743\n",
      "Epoch 2539, Loss: 0.0015384840953629464, Final Batch Loss: 0.00018378457752987742\n",
      "Epoch 2540, Loss: 0.0029122992273187265, Final Batch Loss: 4.7927824198268354e-05\n",
      "Epoch 2541, Loss: 0.0027392607153160498, Final Batch Loss: 0.00029891610029153526\n",
      "Epoch 2542, Loss: 0.0036221352784195915, Final Batch Loss: 0.00017958781972993165\n",
      "Epoch 2543, Loss: 0.0036348564317449927, Final Batch Loss: 0.0005699046305380762\n",
      "Epoch 2544, Loss: 0.012200883238620008, Final Batch Loss: 1.77521269506542e-05\n",
      "Epoch 2545, Loss: 0.009958737362467218, Final Batch Loss: 0.002869068179279566\n",
      "Epoch 2546, Loss: 0.016140831721713766, Final Batch Loss: 0.0022959220223128796\n",
      "Epoch 2547, Loss: 0.017301398911513388, Final Batch Loss: 0.001148789539001882\n",
      "Epoch 2548, Loss: 0.00451188362785615, Final Batch Loss: 0.00022471557895187289\n",
      "Epoch 2549, Loss: 0.01820955570656224, Final Batch Loss: 0.009302807040512562\n",
      "Epoch 2550, Loss: 0.023755178801366128, Final Batch Loss: 0.021009411662817\n",
      "Epoch 2551, Loss: 0.00377034698612988, Final Batch Loss: 0.0009967917576432228\n",
      "Epoch 2552, Loss: 0.001955627667484805, Final Batch Loss: 0.00014055015344638377\n",
      "Epoch 2553, Loss: 0.018703103996813297, Final Batch Loss: 0.0041251941584050655\n",
      "Epoch 2554, Loss: 0.006041867600288242, Final Batch Loss: 0.0005630795494653285\n",
      "Epoch 2555, Loss: 0.005973911203909665, Final Batch Loss: 0.0010780690936371684\n",
      "Epoch 2556, Loss: 0.004222532639687415, Final Batch Loss: 8.725494990358129e-05\n",
      "Epoch 2557, Loss: 0.040678712670342065, Final Batch Loss: 0.0006458663265220821\n",
      "Epoch 2558, Loss: 0.008583016824559309, Final Batch Loss: 0.00011000387894455343\n",
      "Epoch 2559, Loss: 0.008345996320713311, Final Batch Loss: 0.00591962318867445\n",
      "Epoch 2560, Loss: 0.0020790808921447024, Final Batch Loss: 0.00011129786435049027\n",
      "Epoch 2561, Loss: 0.02293036567425588, Final Batch Loss: 0.017444536089897156\n",
      "Epoch 2562, Loss: 0.021121342724654824, Final Batch Loss: 0.00013191917969379574\n",
      "Epoch 2563, Loss: 0.02236325215199031, Final Batch Loss: 0.01966809667646885\n",
      "Epoch 2564, Loss: 0.008509724022587761, Final Batch Loss: 0.00047569870366714895\n",
      "Epoch 2565, Loss: 0.01947594060038682, Final Batch Loss: 0.0002551003999542445\n",
      "Epoch 2566, Loss: 0.003209271490050014, Final Batch Loss: 0.0007760311127640307\n",
      "Epoch 2567, Loss: 0.009116227382037323, Final Batch Loss: 0.003717894898727536\n",
      "Epoch 2568, Loss: 0.006381434388458729, Final Batch Loss: 0.0013878658646717668\n",
      "Epoch 2569, Loss: 0.0024075603141682222, Final Batch Loss: 0.0004004248767159879\n",
      "Epoch 2570, Loss: 0.004950585775077343, Final Batch Loss: 0.00035585707519203424\n",
      "Epoch 2571, Loss: 0.02522351680090651, Final Batch Loss: 0.00019311512005515397\n",
      "Epoch 2572, Loss: 0.0036178940790705383, Final Batch Loss: 0.0009690565057098866\n",
      "Epoch 2573, Loss: 0.01580300681234803, Final Batch Loss: 8.458942465949804e-05\n",
      "Epoch 2574, Loss: 0.0041160842520184815, Final Batch Loss: 0.0010089990682899952\n",
      "Epoch 2575, Loss: 0.0093089825095376, Final Batch Loss: 0.00011212855315534398\n",
      "Epoch 2576, Loss: 0.012725794280413538, Final Batch Loss: 0.0012105634668841958\n",
      "Epoch 2577, Loss: 0.016865107099874876, Final Batch Loss: 0.0005582405719906092\n",
      "Epoch 2578, Loss: 0.0035736196441575885, Final Batch Loss: 0.001413899939507246\n",
      "Epoch 2579, Loss: 0.001703974550764542, Final Batch Loss: 0.0001306088815908879\n",
      "Epoch 2580, Loss: 0.0013232236342446413, Final Batch Loss: 9.951468382496387e-05\n",
      "Epoch 2581, Loss: 0.0024508436472387984, Final Batch Loss: 0.00014465264393948019\n",
      "Epoch 2582, Loss: 0.0014851138985250145, Final Batch Loss: 5.9418547607492656e-05\n",
      "Epoch 2583, Loss: 0.053507004864513874, Final Batch Loss: 0.052007533609867096\n",
      "Epoch 2584, Loss: 0.00353865820216015, Final Batch Loss: 0.0008766071405261755\n",
      "Epoch 2585, Loss: 0.0234695743129123, Final Batch Loss: 0.01497233472764492\n",
      "Epoch 2586, Loss: 0.004698251388617791, Final Batch Loss: 0.0006827893084846437\n",
      "Epoch 2587, Loss: 0.0041430902274441905, Final Batch Loss: 0.0006214253953658044\n",
      "Epoch 2588, Loss: 0.0010629500520735746, Final Batch Loss: 0.00019263218564447016\n",
      "Epoch 2589, Loss: 0.016511771940713516, Final Batch Loss: 2.2280790290096775e-05\n",
      "Epoch 2590, Loss: 0.06073620745155495, Final Batch Loss: 0.0016999017680063844\n",
      "Epoch 2591, Loss: 0.003662653653009329, Final Batch Loss: 9.074054105440155e-05\n",
      "Epoch 2592, Loss: 0.0016501061036251485, Final Batch Loss: 0.00017395286704413593\n",
      "Epoch 2593, Loss: 0.04526233572687488, Final Batch Loss: 0.00011999643174931407\n",
      "Epoch 2594, Loss: 0.003059372364077717, Final Batch Loss: 0.001426862319931388\n",
      "Epoch 2595, Loss: 0.00418561262631556, Final Batch Loss: 0.00010331832891097292\n",
      "Epoch 2596, Loss: 0.0038991063411231153, Final Batch Loss: 0.00037024420453235507\n",
      "Epoch 2597, Loss: 0.0048711273848311976, Final Batch Loss: 8.16054962342605e-05\n",
      "Epoch 2598, Loss: 0.01002301370317582, Final Batch Loss: 0.00019124186655972153\n",
      "Epoch 2599, Loss: 0.004701844274677569, Final Batch Loss: 0.0006775933434255421\n",
      "Epoch 2600, Loss: 0.0033026691016857512, Final Batch Loss: 6.163861689856276e-05\n",
      "Epoch 2601, Loss: 0.0014598208508687094, Final Batch Loss: 0.0003756227670237422\n",
      "Epoch 2602, Loss: 0.0022412415746657643, Final Batch Loss: 5.384584437706508e-05\n",
      "Epoch 2603, Loss: 0.004244136362103745, Final Batch Loss: 0.0030211741104722023\n",
      "Epoch 2604, Loss: 0.0038774950517108664, Final Batch Loss: 0.00012109546514693648\n",
      "Epoch 2605, Loss: 0.001508822912001051, Final Batch Loss: 0.00043296237709000707\n",
      "Epoch 2606, Loss: 0.001351829145278316, Final Batch Loss: 9.197273175232112e-05\n",
      "Epoch 2607, Loss: 0.002654032861755695, Final Batch Loss: 0.00021297039347700775\n",
      "Epoch 2608, Loss: 0.0037223850886221044, Final Batch Loss: 0.0005239837919361889\n",
      "Epoch 2609, Loss: 0.0013921974314143881, Final Batch Loss: 6.461510201916099e-05\n",
      "Epoch 2610, Loss: 0.006135642186563928, Final Batch Loss: 0.00011123841250082478\n",
      "Epoch 2611, Loss: 0.0026566844899207354, Final Batch Loss: 0.0010678586550056934\n",
      "Epoch 2612, Loss: 0.016841832475620322, Final Batch Loss: 0.0001540242665214464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2613, Loss: 0.0022544089952134527, Final Batch Loss: 1.5380173863377422e-05\n",
      "Epoch 2614, Loss: 0.0010492667788639665, Final Batch Loss: 8.697762677911669e-05\n",
      "Epoch 2615, Loss: 0.0019083628139924258, Final Batch Loss: 7.644928700756282e-05\n",
      "Epoch 2616, Loss: 0.001301383956160862, Final Batch Loss: 7.061563519528136e-05\n",
      "Epoch 2617, Loss: 0.006544568237586645, Final Batch Loss: 0.00016641244292259216\n",
      "Epoch 2618, Loss: 0.024432062389678322, Final Batch Loss: 0.0017778828041628003\n",
      "Epoch 2619, Loss: 0.002631891693454236, Final Batch Loss: 0.00019488704856485128\n",
      "Epoch 2620, Loss: 0.0059365491615608335, Final Batch Loss: 0.0002478290698491037\n",
      "Epoch 2621, Loss: 0.002059966660453938, Final Batch Loss: 9.629056148696691e-05\n",
      "Epoch 2622, Loss: 0.0025353053788421676, Final Batch Loss: 0.0016879963222891092\n",
      "Epoch 2623, Loss: 0.002875857215258293, Final Batch Loss: 0.001329729799181223\n",
      "Epoch 2624, Loss: 0.0008711338232387789, Final Batch Loss: 9.675481851445511e-05\n",
      "Epoch 2625, Loss: 0.0033036430140782613, Final Batch Loss: 0.0002050965849775821\n",
      "Epoch 2626, Loss: 0.0029870383586967364, Final Batch Loss: 0.00011061019176850095\n",
      "Epoch 2627, Loss: 0.004138417854846921, Final Batch Loss: 1.5423589502461255e-05\n",
      "Epoch 2628, Loss: 0.011831206807983108, Final Batch Loss: 0.009114605374634266\n",
      "Epoch 2629, Loss: 0.0017920900172612164, Final Batch Loss: 0.000558332132641226\n",
      "Epoch 2630, Loss: 0.0030855830118525773, Final Batch Loss: 0.0002764264354482293\n",
      "Epoch 2631, Loss: 0.001336217494099401, Final Batch Loss: 0.0001374273851979524\n",
      "Epoch 2632, Loss: 0.0011146570868731942, Final Batch Loss: 0.0005303647485561669\n",
      "Epoch 2633, Loss: 0.006376896466463222, Final Batch Loss: 3.683327304315753e-05\n",
      "Epoch 2634, Loss: 0.025292121368693188, Final Batch Loss: 0.00016742970910854638\n",
      "Epoch 2635, Loss: 0.016307344074448338, Final Batch Loss: 0.0003719370870385319\n",
      "Epoch 2636, Loss: 0.0022988909913692623, Final Batch Loss: 0.0002615631965454668\n",
      "Epoch 2637, Loss: 0.00400237794383429, Final Batch Loss: 0.0014215511037036777\n",
      "Epoch 2638, Loss: 0.0027693115262081847, Final Batch Loss: 4.681488644564524e-05\n",
      "Epoch 2639, Loss: 0.0017729783612594474, Final Batch Loss: 5.070948100183159e-05\n",
      "Epoch 2640, Loss: 0.0075733280209533405, Final Batch Loss: 0.00023423718812409788\n",
      "Epoch 2641, Loss: 0.012239028394105844, Final Batch Loss: 0.00031239527743309736\n",
      "Epoch 2642, Loss: 0.0008800287796475459, Final Batch Loss: 0.0001997037325054407\n",
      "Epoch 2643, Loss: 0.0013325889376574196, Final Batch Loss: 0.00016925095405895263\n",
      "Epoch 2644, Loss: 0.0029456661868607625, Final Batch Loss: 0.0002743451332207769\n",
      "Epoch 2645, Loss: 0.003110491945335525, Final Batch Loss: 2.5945437300833873e-05\n",
      "Epoch 2646, Loss: 0.0017182433330162894, Final Batch Loss: 1.8881768482970074e-05\n",
      "Epoch 2647, Loss: 0.0006598150648642331, Final Batch Loss: 8.476216316921636e-05\n",
      "Epoch 2648, Loss: 0.005161806286196224, Final Batch Loss: 0.00021655829914379865\n",
      "Epoch 2649, Loss: 0.0038431002390098, Final Batch Loss: 4.664565040002344e-06\n",
      "Epoch 2650, Loss: 0.017458811418691766, Final Batch Loss: 0.0005531613714993\n",
      "Epoch 2651, Loss: 0.03011266114481259, Final Batch Loss: 0.02793336659669876\n",
      "Epoch 2652, Loss: 0.008897805477317888, Final Batch Loss: 9.123484051087871e-05\n",
      "Epoch 2653, Loss: 0.002286865812493488, Final Batch Loss: 0.0015144795179367065\n",
      "Epoch 2654, Loss: 0.003524841435137205, Final Batch Loss: 0.00042482753633521497\n",
      "Epoch 2655, Loss: 0.015708247949078213, Final Batch Loss: 0.004712398629635572\n",
      "Epoch 2656, Loss: 0.005458909741719253, Final Batch Loss: 0.0017541443230584264\n",
      "Epoch 2657, Loss: 0.023777291440637782, Final Batch Loss: 0.000405587226850912\n",
      "Epoch 2658, Loss: 0.026116977912352013, Final Batch Loss: 7.926961552584544e-05\n",
      "Epoch 2659, Loss: 0.0066450070444261655, Final Batch Loss: 0.00019012879056390375\n",
      "Epoch 2660, Loss: 0.002161471958970651, Final Batch Loss: 2.844390837708488e-05\n",
      "Epoch 2661, Loss: 0.008814338631054852, Final Batch Loss: 6.457843846874312e-05\n",
      "Epoch 2662, Loss: 0.0031205167761072516, Final Batch Loss: 0.0016594765475019813\n",
      "Epoch 2663, Loss: 0.05151071763248183, Final Batch Loss: 0.018825622275471687\n",
      "Epoch 2664, Loss: 0.008970360522653209, Final Batch Loss: 0.0073766945861279964\n",
      "Epoch 2665, Loss: 0.005753183591878042, Final Batch Loss: 0.00029794269357807934\n",
      "Epoch 2666, Loss: 0.08064216567436233, Final Batch Loss: 0.034522973001003265\n",
      "Epoch 2667, Loss: 0.012999135062273126, Final Batch Loss: 5.08345510752406e-05\n",
      "Epoch 2668, Loss: 0.012009279525955208, Final Batch Loss: 9.24540072446689e-05\n",
      "Epoch 2669, Loss: 0.003883394810145546, Final Batch Loss: 0.0021988586522638798\n",
      "Epoch 2670, Loss: 0.015936129180772696, Final Batch Loss: 0.0015705289551988244\n",
      "Epoch 2671, Loss: 0.03218223772273632, Final Batch Loss: 0.03158004209399223\n",
      "Epoch 2672, Loss: 0.022314037778414786, Final Batch Loss: 0.0011396582704037428\n",
      "Epoch 2673, Loss: 0.009458723143325187, Final Batch Loss: 2.6578782126307487e-05\n",
      "Epoch 2674, Loss: 0.012024995754472911, Final Batch Loss: 6.589724216610193e-05\n",
      "Epoch 2675, Loss: 0.005688157118129311, Final Batch Loss: 0.00040384064777754247\n",
      "Epoch 2676, Loss: 0.01085500739281997, Final Batch Loss: 0.00017457528156228364\n",
      "Epoch 2677, Loss: 0.0052723649714607745, Final Batch Loss: 0.001664679846726358\n",
      "Epoch 2678, Loss: 0.00424713475513272, Final Batch Loss: 0.0007098385249264538\n",
      "Epoch 2679, Loss: 0.02334166409127647, Final Batch Loss: 0.00024494543322362006\n",
      "Epoch 2680, Loss: 0.001004706018647994, Final Batch Loss: 2.7381121981306933e-05\n",
      "Epoch 2681, Loss: 0.011752508420613594, Final Batch Loss: 0.000171771869645454\n",
      "Epoch 2682, Loss: 0.004716881889180513, Final Batch Loss: 0.0005959692061878741\n",
      "Epoch 2683, Loss: 0.0009970222745323554, Final Batch Loss: 9.225903340848163e-05\n",
      "Epoch 2684, Loss: 0.021078545702039264, Final Batch Loss: 0.0009118533344008029\n",
      "Epoch 2685, Loss: 0.0003953942887164885, Final Batch Loss: 4.47969141532667e-05\n",
      "Epoch 2686, Loss: 0.015339413439505734, Final Batch Loss: 0.0010694769443944097\n",
      "Epoch 2687, Loss: 0.002484555014234502, Final Batch Loss: 2.9214759706519544e-05\n",
      "Epoch 2688, Loss: 0.004907121954602189, Final Batch Loss: 0.00016820439486764371\n",
      "Epoch 2689, Loss: 0.0032084188860608265, Final Batch Loss: 0.00016595382476225495\n",
      "Epoch 2690, Loss: 0.0018508693610783666, Final Batch Loss: 0.0001153228513430804\n",
      "Epoch 2691, Loss: 0.004652016825275496, Final Batch Loss: 0.0024099559523165226\n",
      "Epoch 2692, Loss: 0.005788624523120234, Final Batch Loss: 0.0002542468428146094\n",
      "Epoch 2693, Loss: 0.003489070138130046, Final Batch Loss: 1.1756465028156526e-05\n",
      "Epoch 2694, Loss: 0.00425999793515075, Final Batch Loss: 0.00018432355136610568\n",
      "Epoch 2695, Loss: 0.0034830876284104306, Final Batch Loss: 6.065597335691564e-05\n",
      "Epoch 2696, Loss: 0.0017941138939931989, Final Batch Loss: 0.0002531810023356229\n",
      "Epoch 2697, Loss: 0.046683854179718764, Final Batch Loss: 0.0003298703522887081\n",
      "Epoch 2698, Loss: 0.0016014182183425874, Final Batch Loss: 0.0008393891621381044\n",
      "Epoch 2699, Loss: 0.0024023124569794163, Final Batch Loss: 0.00025488497340120375\n",
      "Epoch 2700, Loss: 0.01625663229060592, Final Batch Loss: 4.695505049312487e-05\n",
      "Epoch 2701, Loss: 0.013807420138618909, Final Batch Loss: 0.000884484441485256\n",
      "Epoch 2702, Loss: 0.005173575882508885, Final Batch Loss: 0.0038736467249691486\n",
      "Epoch 2703, Loss: 0.003238961726310663, Final Batch Loss: 0.0003146254166495055\n",
      "Epoch 2704, Loss: 0.005648979164107004, Final Batch Loss: 0.0001406911324011162\n",
      "Epoch 2705, Loss: 0.026740346496808343, Final Batch Loss: 0.0006316591170616448\n",
      "Epoch 2706, Loss: 0.0419944393215701, Final Batch Loss: 0.019949039444327354\n",
      "Epoch 2707, Loss: 0.025204256700817496, Final Batch Loss: 0.0012649240670725703\n",
      "Epoch 2708, Loss: 0.0035845947277266532, Final Batch Loss: 0.000768082682043314\n",
      "Epoch 2709, Loss: 0.0023671100461797323, Final Batch Loss: 7.571636524517089e-05\n",
      "Epoch 2710, Loss: 0.00956010395748308, Final Batch Loss: 0.000846599112264812\n",
      "Epoch 2711, Loss: 0.023525778873590752, Final Batch Loss: 0.0012092958204448223\n",
      "Epoch 2712, Loss: 0.01509035931667313, Final Batch Loss: 0.0062636383809149265\n",
      "Epoch 2713, Loss: 0.01531033986248076, Final Batch Loss: 0.00043666907004080713\n",
      "Epoch 2714, Loss: 0.01727812697936315, Final Batch Loss: 0.00021206635574344546\n",
      "Epoch 2715, Loss: 0.02753839868819341, Final Batch Loss: 0.0006499171140603721\n",
      "Epoch 2716, Loss: 0.016544817975955084, Final Batch Loss: 0.00023813455482013524\n",
      "Epoch 2717, Loss: 0.005802360567031428, Final Batch Loss: 0.0013233594363555312\n",
      "Epoch 2718, Loss: 0.014322430346510373, Final Batch Loss: 6.450152432080358e-05\n",
      "Epoch 2719, Loss: 0.007956735251354985, Final Batch Loss: 2.685741856112145e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2720, Loss: 0.024881509390979772, Final Batch Loss: 2.856194259948097e-05\n",
      "Epoch 2721, Loss: 0.004125907769775949, Final Batch Loss: 0.0001924406533362344\n",
      "Epoch 2722, Loss: 0.0016372291966035846, Final Batch Loss: 0.00016648610471747816\n",
      "Epoch 2723, Loss: 0.010820986222825013, Final Batch Loss: 0.002106702420860529\n",
      "Epoch 2724, Loss: 0.0025451182227698155, Final Batch Loss: 2.080178092001006e-05\n",
      "Epoch 2725, Loss: 0.002969424123875797, Final Batch Loss: 0.00034755817614495754\n",
      "Epoch 2726, Loss: 0.0013140174705768004, Final Batch Loss: 4.174327477812767e-05\n",
      "Epoch 2727, Loss: 0.001604943725396879, Final Batch Loss: 0.0003905758785549551\n",
      "Epoch 2728, Loss: 0.0011089514555351343, Final Batch Loss: 0.0001544397819088772\n",
      "Epoch 2729, Loss: 0.0023206802397908177, Final Batch Loss: 7.604733400512487e-05\n",
      "Epoch 2730, Loss: 0.02008920988009777, Final Batch Loss: 0.0004906649119220674\n",
      "Epoch 2731, Loss: 0.001565959995787125, Final Batch Loss: 0.0002710834960453212\n",
      "Epoch 2732, Loss: 0.002674089999345597, Final Batch Loss: 0.00014632334932684898\n",
      "Epoch 2733, Loss: 0.007952185173053294, Final Batch Loss: 0.0002607617061585188\n",
      "Epoch 2734, Loss: 0.005390275462559657, Final Batch Loss: 4.889845513389446e-05\n",
      "Epoch 2735, Loss: 0.0011140639471705072, Final Batch Loss: 9.675620822235942e-05\n",
      "Epoch 2736, Loss: 0.01227767589080031, Final Batch Loss: 5.824476102134213e-05\n",
      "Epoch 2737, Loss: 0.0035856910999427782, Final Batch Loss: 0.0004992447211407125\n",
      "Epoch 2738, Loss: 0.0023166156315710396, Final Batch Loss: 0.000987554551102221\n",
      "Epoch 2739, Loss: 0.0031758080440340564, Final Batch Loss: 0.0008364816312678158\n",
      "Epoch 2740, Loss: 0.004581360379233956, Final Batch Loss: 0.00031619821675121784\n",
      "Epoch 2741, Loss: 0.004690601832407992, Final Batch Loss: 0.0007103236857801676\n",
      "Epoch 2742, Loss: 0.007619553678523516, Final Batch Loss: 3.564625876606442e-05\n",
      "Epoch 2743, Loss: 0.0022252272465266287, Final Batch Loss: 0.00011139720299979672\n",
      "Epoch 2744, Loss: 0.003790760321862763, Final Batch Loss: 0.0031265029683709145\n",
      "Epoch 2745, Loss: 0.03188319958280772, Final Batch Loss: 0.002050526440143585\n",
      "Epoch 2746, Loss: 0.002670888676220784, Final Batch Loss: 0.0011388508137315512\n",
      "Epoch 2747, Loss: 0.014323866693302989, Final Batch Loss: 0.00031631701858714223\n",
      "Epoch 2748, Loss: 0.004131724141188897, Final Batch Loss: 0.0023669737856835127\n",
      "Epoch 2749, Loss: 0.00698304946126882, Final Batch Loss: 0.0037414308171719313\n",
      "Epoch 2750, Loss: 0.0057648841248010285, Final Batch Loss: 6.519203452626243e-05\n",
      "Epoch 2751, Loss: 0.00749826187529834, Final Batch Loss: 0.004329820163547993\n",
      "Epoch 2752, Loss: 0.0030214755388442427, Final Batch Loss: 0.00033000996336340904\n",
      "Epoch 2753, Loss: 0.0029853836676920764, Final Batch Loss: 0.002086743712425232\n",
      "Epoch 2754, Loss: 0.0020785645901924, Final Batch Loss: 0.0009746644063852727\n",
      "Epoch 2755, Loss: 0.004427402396686375, Final Batch Loss: 0.0008730109548196197\n",
      "Epoch 2756, Loss: 0.0018643075018189847, Final Batch Loss: 0.0003270652377977967\n",
      "Epoch 2757, Loss: 0.0028558770136442035, Final Batch Loss: 0.001870185718871653\n",
      "Epoch 2758, Loss: 0.004287608215236105, Final Batch Loss: 0.0010791054228320718\n",
      "Epoch 2759, Loss: 0.010894694190938026, Final Batch Loss: 0.0006248870049603283\n",
      "Epoch 2760, Loss: 0.0018877436177717755, Final Batch Loss: 0.00046959970495663583\n",
      "Epoch 2761, Loss: 0.001065012485923944, Final Batch Loss: 2.0289608073653653e-05\n",
      "Epoch 2762, Loss: 0.0008098699254333042, Final Batch Loss: 5.711829726351425e-05\n",
      "Epoch 2763, Loss: 0.0025259093090426177, Final Batch Loss: 0.0006304789567366242\n",
      "Epoch 2764, Loss: 0.0025643188491812907, Final Batch Loss: 8.63231616676785e-05\n",
      "Epoch 2765, Loss: 0.0019337224839546252, Final Batch Loss: 5.627201244351454e-05\n",
      "Epoch 2766, Loss: 0.0008849579826346599, Final Batch Loss: 0.00014441102393902838\n",
      "Epoch 2767, Loss: 0.005586379185842816, Final Batch Loss: 0.0003383548173587769\n",
      "Epoch 2768, Loss: 0.0019180858216714114, Final Batch Loss: 4.225160228088498e-05\n",
      "Epoch 2769, Loss: 0.003588560628486448, Final Batch Loss: 1.9316221369081177e-05\n",
      "Epoch 2770, Loss: 0.002953172202978749, Final Batch Loss: 0.002033017110079527\n",
      "Epoch 2771, Loss: 0.009189947732011206, Final Batch Loss: 1.5869887647568248e-05\n",
      "Epoch 2772, Loss: 0.0004456965471035801, Final Batch Loss: 6.403800216503441e-05\n",
      "Epoch 2773, Loss: 0.0006216822093847441, Final Batch Loss: 9.15232376428321e-05\n",
      "Epoch 2774, Loss: 0.015090734726982191, Final Batch Loss: 0.00019933402654714882\n",
      "Epoch 2775, Loss: 0.001457476246287115, Final Batch Loss: 0.0003231391601730138\n",
      "Epoch 2776, Loss: 0.0015251165577865322, Final Batch Loss: 7.981153612490743e-05\n",
      "Epoch 2777, Loss: 0.010488120817171875, Final Batch Loss: 3.503642074065283e-05\n",
      "Epoch 2778, Loss: 0.0034701797922025435, Final Batch Loss: 0.00012285978300496936\n",
      "Epoch 2779, Loss: 0.0012785335784428753, Final Batch Loss: 7.834124699002132e-05\n",
      "Epoch 2780, Loss: 0.009272564419006812, Final Batch Loss: 2.7348609364707954e-05\n",
      "Epoch 2781, Loss: 0.0012511539716797415, Final Batch Loss: 2.7414109354140237e-05\n",
      "Epoch 2782, Loss: 0.0020132794925302733, Final Batch Loss: 0.00013173629122320563\n",
      "Epoch 2783, Loss: 0.0014078150379646104, Final Batch Loss: 0.00013365370978135616\n",
      "Epoch 2784, Loss: 0.002485236505890498, Final Batch Loss: 6.505748024210334e-05\n",
      "Epoch 2785, Loss: 0.0016296465691993944, Final Batch Loss: 0.00010095629113493487\n",
      "Epoch 2786, Loss: 0.007780896816257155, Final Batch Loss: 3.170351556036621e-05\n",
      "Epoch 2787, Loss: 0.010697637299017515, Final Batch Loss: 2.3061218598741107e-05\n",
      "Epoch 2788, Loss: 0.005801111849905283, Final Batch Loss: 0.0036717907059937716\n",
      "Epoch 2789, Loss: 0.0022442062036134303, Final Batch Loss: 0.0003883745230268687\n",
      "Epoch 2790, Loss: 0.017526141658890992, Final Batch Loss: 0.0023775966838002205\n",
      "Epoch 2791, Loss: 0.013251042539195623, Final Batch Loss: 0.0008636978454887867\n",
      "Epoch 2792, Loss: 0.09410227601620136, Final Batch Loss: 0.0007169488817453384\n",
      "Epoch 2793, Loss: 0.1233587077294942, Final Batch Loss: 0.09958582371473312\n",
      "Epoch 2794, Loss: 0.03196392389872926, Final Batch Loss: 0.0007025106460787356\n",
      "Epoch 2795, Loss: 0.01727557300910121, Final Batch Loss: 0.003923324868083\n",
      "Epoch 2796, Loss: 0.009286376283853315, Final Batch Loss: 0.0014657166320830584\n",
      "Epoch 2797, Loss: 0.004399888275656849, Final Batch Loss: 0.0007135467021726072\n",
      "Epoch 2798, Loss: 0.013474568942910992, Final Batch Loss: 0.00015796608931850642\n",
      "Epoch 2799, Loss: 0.019199650472728536, Final Batch Loss: 0.0043936013244092464\n",
      "Epoch 2800, Loss: 0.0022609213992836885, Final Batch Loss: 0.0006961760809645057\n",
      "Epoch 2801, Loss: 0.0158171263319673, Final Batch Loss: 0.0021414337679743767\n",
      "Epoch 2802, Loss: 0.006764753052266315, Final Batch Loss: 0.0003494106640573591\n",
      "Epoch 2803, Loss: 0.012439723461284302, Final Batch Loss: 0.00014588669000659138\n",
      "Epoch 2804, Loss: 0.004030197509564459, Final Batch Loss: 0.0010651365155354142\n",
      "Epoch 2805, Loss: 0.003834724993794225, Final Batch Loss: 0.001858081086538732\n",
      "Epoch 2806, Loss: 0.04119896841075388, Final Batch Loss: 3.2747502700658515e-05\n",
      "Epoch 2807, Loss: 0.017492159611720126, Final Batch Loss: 4.921208164887503e-05\n",
      "Epoch 2808, Loss: 0.01239302780595608, Final Batch Loss: 0.005428855307400227\n",
      "Epoch 2809, Loss: 0.009996428805607138, Final Batch Loss: 2.5585886760381982e-05\n",
      "Epoch 2810, Loss: 0.002369602727412712, Final Batch Loss: 0.00043708784505724907\n",
      "Epoch 2811, Loss: 0.004475744717638008, Final Batch Loss: 0.0009569633402861655\n",
      "Epoch 2812, Loss: 0.003907701495336369, Final Batch Loss: 0.001027731574140489\n",
      "Epoch 2813, Loss: 0.003461603082541842, Final Batch Loss: 0.000295759440632537\n",
      "Epoch 2814, Loss: 0.002706031868001446, Final Batch Loss: 9.43256018217653e-05\n",
      "Epoch 2815, Loss: 0.004283139220206067, Final Batch Loss: 0.0009431304642930627\n",
      "Epoch 2816, Loss: 0.004387830136693083, Final Batch Loss: 0.00020516851509455591\n",
      "Epoch 2817, Loss: 0.001819645898649469, Final Batch Loss: 0.00013823628250975162\n",
      "Epoch 2818, Loss: 0.0032237792838714086, Final Batch Loss: 0.0017054619966074824\n",
      "Epoch 2819, Loss: 0.00311955143843079, Final Batch Loss: 0.0006783624412491918\n",
      "Epoch 2820, Loss: 0.0019797029563051183, Final Batch Loss: 5.5643653467996046e-05\n",
      "Epoch 2821, Loss: 0.0031982443906599656, Final Batch Loss: 0.0007760068983770907\n",
      "Epoch 2822, Loss: 0.0020786842360394076, Final Batch Loss: 0.0008978175465017557\n",
      "Epoch 2823, Loss: 0.002431945235002786, Final Batch Loss: 0.00015849301416892558\n",
      "Epoch 2824, Loss: 0.001218509023601655, Final Batch Loss: 0.0005502882995642722\n",
      "Epoch 2825, Loss: 0.001074969070032239, Final Batch Loss: 4.607116716215387e-05\n",
      "Epoch 2826, Loss: 0.013095529575366527, Final Batch Loss: 0.00012202550715301186\n",
      "Epoch 2827, Loss: 0.0010131908347830176, Final Batch Loss: 5.023482663091272e-05\n",
      "Epoch 2828, Loss: 0.024758446808846202, Final Batch Loss: 0.00040870500379242003\n",
      "Epoch 2829, Loss: 0.0019534647653927095, Final Batch Loss: 0.0004046748217660934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2830, Loss: 0.0034415395311953034, Final Batch Loss: 3.386857497389428e-05\n",
      "Epoch 2831, Loss: 0.0049511601609992795, Final Batch Loss: 0.0031646431889384985\n",
      "Epoch 2832, Loss: 0.012128477756050415, Final Batch Loss: 0.011072594672441483\n",
      "Epoch 2833, Loss: 0.007233866235765163, Final Batch Loss: 7.065365934977308e-05\n",
      "Epoch 2834, Loss: 0.021876880346098915, Final Batch Loss: 0.011599622666835785\n",
      "Epoch 2835, Loss: 0.005531635863007978, Final Batch Loss: 0.00019341058214195073\n",
      "Epoch 2836, Loss: 0.003082898067077622, Final Batch Loss: 0.00026174605591222644\n",
      "Epoch 2837, Loss: 0.0015026455221232027, Final Batch Loss: 0.0008908331510610878\n",
      "Epoch 2838, Loss: 0.003379539994057268, Final Batch Loss: 0.00023753635468892753\n",
      "Epoch 2839, Loss: 0.009741470406879671, Final Batch Loss: 9.38366720220074e-05\n",
      "Epoch 2840, Loss: 0.0024035617607296444, Final Batch Loss: 7.2290058596991e-05\n",
      "Epoch 2841, Loss: 0.0067689378611248685, Final Batch Loss: 0.0006183229852467775\n",
      "Epoch 2842, Loss: 0.02188181366364006, Final Batch Loss: 0.006358614191412926\n",
      "Epoch 2843, Loss: 0.004357210196758388, Final Batch Loss: 0.0022277499083429575\n",
      "Epoch 2844, Loss: 0.025018847576575354, Final Batch Loss: 0.0035926690325140953\n",
      "Epoch 2845, Loss: 0.0016632940678391606, Final Batch Loss: 0.00039624699275009334\n",
      "Epoch 2846, Loss: 0.0027460949204396456, Final Batch Loss: 0.00015365780564025044\n",
      "Epoch 2847, Loss: 0.016599354679783573, Final Batch Loss: 0.00027863550349138677\n",
      "Epoch 2848, Loss: 0.010847172257854254, Final Batch Loss: 0.0014016813365742564\n",
      "Epoch 2849, Loss: 0.004068850226758514, Final Batch Loss: 0.00012907209747936577\n",
      "Epoch 2850, Loss: 0.0038047325069783255, Final Batch Loss: 0.0002720216871239245\n",
      "Epoch 2851, Loss: 0.005115886167914141, Final Batch Loss: 0.00011315411393297836\n",
      "Epoch 2852, Loss: 0.0013271192729007453, Final Batch Loss: 2.4443259462714195e-05\n",
      "Epoch 2853, Loss: 0.007782701723044738, Final Batch Loss: 0.0005940954433754086\n",
      "Epoch 2854, Loss: 0.0016889289909158833, Final Batch Loss: 0.00029668904608115554\n",
      "Epoch 2855, Loss: 0.002711284534598235, Final Batch Loss: 0.00028091701096855104\n",
      "Epoch 2856, Loss: 0.014949112694012001, Final Batch Loss: 0.0003429876815062016\n",
      "Epoch 2857, Loss: 0.006821163802669616, Final Batch Loss: 0.006011438090354204\n",
      "Epoch 2858, Loss: 0.013287663707160391, Final Batch Loss: 6.80101802572608e-05\n",
      "Epoch 2859, Loss: 0.006866532394724345, Final Batch Loss: 4.375704975245753e-06\n",
      "Epoch 2860, Loss: 0.0008105792439891957, Final Batch Loss: 0.00010275677777826786\n",
      "Epoch 2861, Loss: 0.023149582913902123, Final Batch Loss: 7.801794708939269e-05\n",
      "Epoch 2862, Loss: 0.016777480763266794, Final Batch Loss: 0.00019695008813869208\n",
      "Epoch 2863, Loss: 0.005270264264254365, Final Batch Loss: 6.261248199734837e-05\n",
      "Epoch 2864, Loss: 0.0031719334056106163, Final Batch Loss: 0.00028615735936909914\n",
      "Epoch 2865, Loss: 0.0014862025709589943, Final Batch Loss: 1.614689244888723e-05\n",
      "Epoch 2866, Loss: 0.001835155169828795, Final Batch Loss: 0.0001929984282469377\n",
      "Epoch 2867, Loss: 0.0023301306064240634, Final Batch Loss: 0.00022252068447414786\n",
      "Epoch 2868, Loss: 0.014839370844129007, Final Batch Loss: 0.0003813819494098425\n",
      "Epoch 2869, Loss: 0.007878795011492912, Final Batch Loss: 0.004584194160997868\n",
      "Epoch 2870, Loss: 0.003135664781439118, Final Batch Loss: 0.00039421115070581436\n",
      "Epoch 2871, Loss: 0.009249341208487749, Final Batch Loss: 8.887019066605717e-05\n",
      "Epoch 2872, Loss: 0.001164612389402464, Final Batch Loss: 0.00011289638496236876\n",
      "Epoch 2873, Loss: 0.0024105740085360594, Final Batch Loss: 0.001015867106616497\n",
      "Epoch 2874, Loss: 0.002977412259497214, Final Batch Loss: 0.00038907333509996533\n",
      "Epoch 2875, Loss: 0.05962719475792255, Final Batch Loss: 0.0001992024335777387\n",
      "Epoch 2876, Loss: 0.017188313620863482, Final Batch Loss: 0.009236965328454971\n",
      "Epoch 2877, Loss: 0.0036563474568538368, Final Batch Loss: 0.0017478364752605557\n",
      "Epoch 2878, Loss: 0.004348916627350263, Final Batch Loss: 6.202720396686345e-05\n",
      "Epoch 2879, Loss: 0.0011319915211061016, Final Batch Loss: 0.00010955719335470349\n",
      "Epoch 2880, Loss: 0.0038133309353725053, Final Batch Loss: 0.00038434311863966286\n",
      "Epoch 2881, Loss: 0.005668394665917731, Final Batch Loss: 0.000467229459900409\n",
      "Epoch 2882, Loss: 0.005935979090281762, Final Batch Loss: 0.0005512104835361242\n",
      "Epoch 2883, Loss: 0.005293160080327652, Final Batch Loss: 0.0007739359862171113\n",
      "Epoch 2884, Loss: 0.0031679653402534313, Final Batch Loss: 0.0011431894963607192\n",
      "Epoch 2885, Loss: 0.003949531030229991, Final Batch Loss: 6.433788075810298e-05\n",
      "Epoch 2886, Loss: 0.013881499478884507, Final Batch Loss: 0.0009585021762177348\n",
      "Epoch 2887, Loss: 0.0006794437795178965, Final Batch Loss: 0.00017080661200452596\n",
      "Epoch 2888, Loss: 0.0036797467264477746, Final Batch Loss: 1.3330581168702338e-05\n",
      "Epoch 2889, Loss: 0.04111205132539908, Final Batch Loss: 2.267142008349765e-05\n",
      "Epoch 2890, Loss: 0.0008458229021925945, Final Batch Loss: 0.0001505439286120236\n",
      "Epoch 2891, Loss: 0.0021847187090315856, Final Batch Loss: 1.6297315596602857e-05\n",
      "Epoch 2892, Loss: 0.006061457424948458, Final Batch Loss: 0.00011653752153506503\n",
      "Epoch 2893, Loss: 0.017313551274128258, Final Batch Loss: 0.0016374586848542094\n",
      "Epoch 2894, Loss: 0.0048596872366033494, Final Batch Loss: 0.00032504292903468013\n",
      "Epoch 2895, Loss: 0.02508104726439342, Final Batch Loss: 0.0005418208311311901\n",
      "Epoch 2896, Loss: 0.0023823678275221027, Final Batch Loss: 0.00011534356599440798\n",
      "Epoch 2897, Loss: 0.016834680369356647, Final Batch Loss: 7.35474459361285e-05\n",
      "Epoch 2898, Loss: 0.0023298002743104007, Final Batch Loss: 0.0014282740885391831\n",
      "Epoch 2899, Loss: 0.01827988657169044, Final Batch Loss: 0.0005501107661984861\n",
      "Epoch 2900, Loss: 0.0038274437174550258, Final Batch Loss: 1.8310201994609088e-05\n",
      "Epoch 2901, Loss: 0.014199339209881146, Final Batch Loss: 8.937135135056451e-05\n",
      "Epoch 2902, Loss: 0.012348218820079637, Final Batch Loss: 1.387615247949725e-05\n",
      "Epoch 2903, Loss: 0.0009425958269275725, Final Batch Loss: 0.00013089989079162478\n",
      "Epoch 2904, Loss: 0.017384480919190537, Final Batch Loss: 7.602278401463991e-06\n",
      "Epoch 2905, Loss: 0.0873391177301528, Final Batch Loss: 0.0023953262716531754\n",
      "Epoch 2906, Loss: 0.01214167992293369, Final Batch Loss: 0.00505867600440979\n",
      "Epoch 2907, Loss: 0.018811190879205242, Final Batch Loss: 0.0007076332112774253\n",
      "Epoch 2908, Loss: 0.004853240476222709, Final Batch Loss: 0.0004297764098737389\n",
      "Epoch 2909, Loss: 0.07516364799812436, Final Batch Loss: 0.003761443542316556\n",
      "Epoch 2910, Loss: 0.014675582591735292, Final Batch Loss: 8.462102414341643e-05\n",
      "Epoch 2911, Loss: 0.00403197854757309, Final Batch Loss: 0.00025162912788800895\n",
      "Epoch 2912, Loss: 0.007015621747996192, Final Batch Loss: 0.0010885869851335883\n",
      "Epoch 2913, Loss: 0.0021942476523690857, Final Batch Loss: 6.583763024536893e-05\n",
      "Epoch 2914, Loss: 0.012751303031109273, Final Batch Loss: 0.001124200294725597\n",
      "Epoch 2915, Loss: 0.012057429408741882, Final Batch Loss: 0.0005840632366016507\n",
      "Epoch 2916, Loss: 0.015842935579712503, Final Batch Loss: 0.00023510582104790956\n",
      "Epoch 2917, Loss: 0.00268831286666682, Final Batch Loss: 0.0004916568868793547\n",
      "Epoch 2918, Loss: 0.015641828133084346, Final Batch Loss: 0.013191978447139263\n",
      "Epoch 2919, Loss: 0.0009815143639571033, Final Batch Loss: 0.0003139938635285944\n",
      "Epoch 2920, Loss: 0.002332091360585764, Final Batch Loss: 0.0007915635360404849\n",
      "Epoch 2921, Loss: 0.0037548218242591247, Final Batch Loss: 0.0008628249051980674\n",
      "Epoch 2922, Loss: 0.011958255741774337, Final Batch Loss: 5.99514642090071e-05\n",
      "Epoch 2923, Loss: 0.008084595552645624, Final Batch Loss: 0.00020026492711622268\n",
      "Epoch 2924, Loss: 0.0066693009866867214, Final Batch Loss: 0.0006973753334023058\n",
      "Epoch 2925, Loss: 0.012945634545758367, Final Batch Loss: 0.00374847324565053\n",
      "Epoch 2926, Loss: 0.04627927213732619, Final Batch Loss: 0.00016092219448182732\n",
      "Epoch 2927, Loss: 0.002851606725016609, Final Batch Loss: 0.0005693823914043605\n",
      "Epoch 2928, Loss: 0.007868361499276944, Final Batch Loss: 0.00039058199035935104\n",
      "Epoch 2929, Loss: 0.0017973844078369439, Final Batch Loss: 0.0008337603067047894\n",
      "Epoch 2930, Loss: 0.009920192271238193, Final Batch Loss: 0.0032155911903828382\n",
      "Epoch 2931, Loss: 0.010717087221564725, Final Batch Loss: 0.0022150948643684387\n",
      "Epoch 2932, Loss: 0.024724067632632796, Final Batch Loss: 0.00011209483636775985\n",
      "Epoch 2933, Loss: 0.008995371616038028, Final Batch Loss: 5.9621183027047664e-05\n",
      "Epoch 2934, Loss: 0.0037454289267770946, Final Batch Loss: 0.00013798825966659933\n",
      "Epoch 2935, Loss: 0.06330207164864987, Final Batch Loss: 0.04697205126285553\n",
      "Epoch 2936, Loss: 0.002952995040686801, Final Batch Loss: 0.00013092943117953837\n",
      "Epoch 2937, Loss: 0.05394410778171732, Final Batch Loss: 0.006892283447086811\n",
      "Epoch 2938, Loss: 0.019274715305073187, Final Batch Loss: 0.0003605745732784271\n",
      "Epoch 2939, Loss: 0.017956344410777092, Final Batch Loss: 0.0017339420737698674\n",
      "Epoch 2940, Loss: 0.08571086852316512, Final Batch Loss: 0.0002865022688638419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2941, Loss: 0.020722627981740516, Final Batch Loss: 0.00011458560038590804\n",
      "Epoch 2942, Loss: 0.008076889182120794, Final Batch Loss: 3.3175008866237476e-05\n",
      "Epoch 2943, Loss: 0.011454316001618281, Final Batch Loss: 0.002915214980021119\n",
      "Epoch 2944, Loss: 0.019476116925943643, Final Batch Loss: 0.006598955485969782\n",
      "Epoch 2945, Loss: 0.01431922396295704, Final Batch Loss: 0.0003164851514156908\n",
      "Epoch 2946, Loss: 0.0170585730811581, Final Batch Loss: 0.00027470686472952366\n",
      "Epoch 2947, Loss: 0.05344518506899476, Final Batch Loss: 0.000417502858908847\n",
      "Epoch 2948, Loss: 0.005468613293487579, Final Batch Loss: 0.0013216199586167932\n",
      "Epoch 2949, Loss: 0.032946351188002154, Final Batch Loss: 0.0006454795366153121\n",
      "Epoch 2950, Loss: 0.018433097044180613, Final Batch Loss: 0.004048970993608236\n",
      "Epoch 2951, Loss: 0.02951396201387979, Final Batch Loss: 0.00019889100803993642\n",
      "Epoch 2952, Loss: 0.004070125585712958, Final Batch Loss: 0.0014891148312017322\n",
      "Epoch 2953, Loss: 0.004561997426208109, Final Batch Loss: 0.0016195743810385466\n",
      "Epoch 2954, Loss: 0.03608443506527692, Final Batch Loss: 0.025849133729934692\n",
      "Epoch 2955, Loss: 0.03883626326569356, Final Batch Loss: 0.0006649493589065969\n",
      "Epoch 2956, Loss: 0.019333848787937313, Final Batch Loss: 0.001022848067805171\n",
      "Epoch 2957, Loss: 0.017962148049264215, Final Batch Loss: 0.004147352650761604\n",
      "Epoch 2958, Loss: 0.007806129549862817, Final Batch Loss: 0.0010047572432085872\n",
      "Epoch 2959, Loss: 0.03553746207035147, Final Batch Loss: 0.02684362232685089\n",
      "Epoch 2960, Loss: 0.01669465593295172, Final Batch Loss: 0.0013882374623790383\n",
      "Epoch 2961, Loss: 0.006392335912096314, Final Batch Loss: 0.000134770103613846\n",
      "Epoch 2962, Loss: 0.005070439103292301, Final Batch Loss: 0.0007606616709381342\n",
      "Epoch 2963, Loss: 0.010075493104523048, Final Batch Loss: 0.001226163236424327\n",
      "Epoch 2964, Loss: 0.016620907612377778, Final Batch Loss: 0.000454837892903015\n",
      "Epoch 2965, Loss: 0.0030356472416315228, Final Batch Loss: 0.00026184527087025344\n",
      "Epoch 2966, Loss: 0.00374669254233595, Final Batch Loss: 0.000391455803764984\n",
      "Epoch 2967, Loss: 0.0036730038264067844, Final Batch Loss: 0.0012933149700984359\n",
      "Epoch 2968, Loss: 0.005281479985569604, Final Batch Loss: 0.0013279742561280727\n",
      "Epoch 2969, Loss: 0.0061904961039545015, Final Batch Loss: 0.00036007887683808804\n",
      "Epoch 2970, Loss: 0.0032205417519435287, Final Batch Loss: 0.0006472048698924482\n",
      "Epoch 2971, Loss: 0.004352133750217035, Final Batch Loss: 0.0014123807195574045\n",
      "Epoch 2972, Loss: 0.002113599817676004, Final Batch Loss: 0.0001014528315863572\n",
      "Epoch 2973, Loss: 0.012824903795262799, Final Batch Loss: 0.003435030346736312\n",
      "Epoch 2974, Loss: 0.004496634312090464, Final Batch Loss: 0.0003222924715373665\n",
      "Epoch 2975, Loss: 0.02086302250972949, Final Batch Loss: 0.019172783941030502\n",
      "Epoch 2976, Loss: 0.004877729938016273, Final Batch Loss: 0.0028534720186144114\n",
      "Epoch 2977, Loss: 0.04080398962832987, Final Batch Loss: 0.03185877203941345\n",
      "Epoch 2978, Loss: 0.0017745047152857296, Final Batch Loss: 0.00014491871115751565\n",
      "Epoch 2979, Loss: 0.004221772076562047, Final Batch Loss: 0.0003184961969964206\n",
      "Epoch 2980, Loss: 0.008983236999483779, Final Batch Loss: 0.0006645239773206413\n",
      "Epoch 2981, Loss: 0.019551629025954753, Final Batch Loss: 0.016306940466165543\n",
      "Epoch 2982, Loss: 0.002205624507041648, Final Batch Loss: 0.0004349763039499521\n",
      "Epoch 2983, Loss: 0.0046946778311394155, Final Batch Loss: 0.0015287480782717466\n",
      "Epoch 2984, Loss: 0.010068603209219873, Final Batch Loss: 0.0009321690886281431\n",
      "Epoch 2985, Loss: 0.0034787055192282423, Final Batch Loss: 0.0019916787277907133\n",
      "Epoch 2986, Loss: 0.008007057793292915, Final Batch Loss: 0.007464590482413769\n",
      "Epoch 2987, Loss: 0.0029995154618518427, Final Batch Loss: 0.0004391309921629727\n",
      "Epoch 2988, Loss: 0.0019453871973382775, Final Batch Loss: 0.00041576402145437896\n",
      "Epoch 2989, Loss: 0.0019922327192034572, Final Batch Loss: 0.00026864634128287435\n",
      "Epoch 2990, Loss: 0.003476760677585844, Final Batch Loss: 9.692653111414984e-05\n",
      "Epoch 2991, Loss: 0.014309075249911984, Final Batch Loss: 0.00014432301395572722\n",
      "Epoch 2992, Loss: 0.0026118340174434707, Final Batch Loss: 0.0002496976521797478\n",
      "Epoch 2993, Loss: 0.0017285765061387792, Final Batch Loss: 2.472994674462825e-05\n",
      "Epoch 2994, Loss: 0.0013334136965568177, Final Batch Loss: 0.00018420869309920818\n",
      "Epoch 2995, Loss: 0.00664021834381856, Final Batch Loss: 0.0007358328439295292\n",
      "Epoch 2996, Loss: 0.00308739319734741, Final Batch Loss: 0.0018582491902634501\n",
      "Epoch 2997, Loss: 0.011724739095370751, Final Batch Loss: 0.003233024850487709\n",
      "Epoch 2998, Loss: 0.0043823858723044395, Final Batch Loss: 0.00047985833953134716\n",
      "Epoch 2999, Loss: 0.007875795701693278, Final Batch Loss: 0.0002948222681879997\n",
      "Epoch 3000, Loss: 0.0008748538530198857, Final Batch Loss: 9.787038288777694e-05\n",
      "Epoch 3001, Loss: 0.0016976014740066603, Final Batch Loss: 0.00031188299180939794\n",
      "Epoch 3002, Loss: 0.002715353883104399, Final Batch Loss: 8.651847019791603e-05\n",
      "Epoch 3003, Loss: 0.0067160682774556335, Final Batch Loss: 0.005816317163407803\n",
      "Epoch 3004, Loss: 0.003288010906544514, Final Batch Loss: 0.0012093641562387347\n",
      "Epoch 3005, Loss: 0.0038688887743774103, Final Batch Loss: 2.0166562535450794e-05\n",
      "Epoch 3006, Loss: 0.004487520913244225, Final Batch Loss: 9.229825809597969e-05\n",
      "Epoch 3007, Loss: 0.0011371708205842879, Final Batch Loss: 0.00013166673306841403\n",
      "Epoch 3008, Loss: 0.0018794771749526262, Final Batch Loss: 9.978831803891808e-05\n",
      "Epoch 3009, Loss: 0.001976480509256362, Final Batch Loss: 7.92126047599595e-06\n",
      "Epoch 3010, Loss: 0.003383330094948178, Final Batch Loss: 0.0003221929364372045\n",
      "Epoch 3011, Loss: 0.010397929843747988, Final Batch Loss: 0.006295213010162115\n",
      "Epoch 3012, Loss: 0.0024062267621047795, Final Batch Loss: 0.0019668813329190016\n",
      "Epoch 3013, Loss: 0.0030716408509761095, Final Batch Loss: 0.0003143685171380639\n",
      "Epoch 3014, Loss: 0.02450203827174846, Final Batch Loss: 0.001983243739232421\n",
      "Epoch 3015, Loss: 0.004472645232453942, Final Batch Loss: 0.0007606308208778501\n",
      "Epoch 3016, Loss: 0.008857928231009282, Final Batch Loss: 0.00036744779208675027\n",
      "Epoch 3017, Loss: 0.002168064476791187, Final Batch Loss: 9.283634426537901e-05\n",
      "Epoch 3018, Loss: 0.005512928561074659, Final Batch Loss: 9.134993160841987e-05\n",
      "Epoch 3019, Loss: 0.005615137441054685, Final Batch Loss: 5.7353772717760876e-05\n",
      "Epoch 3020, Loss: 0.0007067629630910233, Final Batch Loss: 0.00028717005625367165\n",
      "Epoch 3021, Loss: 0.0006420670088118641, Final Batch Loss: 2.8405547709553503e-05\n",
      "Epoch 3022, Loss: 0.0035962452966487035, Final Batch Loss: 0.00015949249791447073\n",
      "Epoch 3023, Loss: 0.00577890202839626, Final Batch Loss: 0.0008603080059401691\n",
      "Epoch 3024, Loss: 0.0031258079088729573, Final Batch Loss: 0.00011086977610830218\n",
      "Epoch 3025, Loss: 0.0036296270991442725, Final Batch Loss: 0.002829783596098423\n",
      "Epoch 3026, Loss: 0.006126002877863357, Final Batch Loss: 8.496108057443053e-05\n",
      "Epoch 3027, Loss: 0.0017060346144717187, Final Batch Loss: 0.0005579537246376276\n",
      "Epoch 3028, Loss: 0.002085698852170026, Final Batch Loss: 0.0002594729303382337\n",
      "Epoch 3029, Loss: 0.002192422456573695, Final Batch Loss: 0.0007625858415849507\n",
      "Epoch 3030, Loss: 0.0017364506420562975, Final Batch Loss: 0.0006842061411589384\n",
      "Epoch 3031, Loss: 0.0018229806337330956, Final Batch Loss: 1.926058394019492e-05\n",
      "Epoch 3032, Loss: 0.0028030709581798874, Final Batch Loss: 0.00011442682443885133\n",
      "Epoch 3033, Loss: 0.0014464271589531563, Final Batch Loss: 0.00041985706775449216\n",
      "Epoch 3034, Loss: 0.001668593682552455, Final Batch Loss: 3.526731961756013e-05\n",
      "Epoch 3035, Loss: 0.002271002249472076, Final Batch Loss: 0.0009370824554935098\n",
      "Epoch 3036, Loss: 0.000782580282248091, Final Batch Loss: 0.0003645484976004809\n",
      "Epoch 3037, Loss: 0.0023476094502257183, Final Batch Loss: 7.077635382302105e-05\n",
      "Epoch 3038, Loss: 0.0015703357421443798, Final Batch Loss: 0.0003478788712527603\n",
      "Epoch 3039, Loss: 0.0028921509092469933, Final Batch Loss: 1.218644501932431e-05\n",
      "Epoch 3040, Loss: 0.004462775523279561, Final Batch Loss: 4.063296000822447e-05\n",
      "Epoch 3041, Loss: 0.0008913991805457044, Final Batch Loss: 5.613906978396699e-06\n",
      "Epoch 3042, Loss: 0.0013960247779323254, Final Batch Loss: 4.6642035158583894e-05\n",
      "Epoch 3043, Loss: 0.005945012053416576, Final Batch Loss: 9.326708823209628e-05\n",
      "Epoch 3044, Loss: 0.03563386566383997, Final Batch Loss: 0.00021718164498452097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3045, Loss: 0.005057904520072043, Final Batch Loss: 0.0046869320794939995\n",
      "Epoch 3046, Loss: 0.005218352467636578, Final Batch Loss: 0.0002968437329400331\n",
      "Epoch 3047, Loss: 0.0016739638813305646, Final Batch Loss: 0.00016537740884814411\n",
      "Epoch 3048, Loss: 0.0063721046899445355, Final Batch Loss: 0.0004497680638451129\n",
      "Epoch 3049, Loss: 0.0088947971817106, Final Batch Loss: 0.004491843748837709\n",
      "Epoch 3050, Loss: 0.018671955142053775, Final Batch Loss: 0.0001824993232730776\n",
      "Epoch 3051, Loss: 0.02688491186563624, Final Batch Loss: 5.082108691567555e-05\n",
      "Epoch 3052, Loss: 0.004246598495228682, Final Batch Loss: 0.0004718232376035303\n",
      "Epoch 3053, Loss: 0.002963798266137019, Final Batch Loss: 0.00019717804389074445\n",
      "Epoch 3054, Loss: 0.05546367410897801, Final Batch Loss: 0.05399160459637642\n",
      "Epoch 3055, Loss: 0.007677414170757402, Final Batch Loss: 0.0005362535011954606\n",
      "Epoch 3056, Loss: 0.003818989105639048, Final Batch Loss: 0.00022497955069411546\n",
      "Epoch 3057, Loss: 0.003699405788211152, Final Batch Loss: 0.0007132678292691708\n",
      "Epoch 3058, Loss: 0.002964580951811513, Final Batch Loss: 0.0007654509390704334\n",
      "Epoch 3059, Loss: 0.002532413111111964, Final Batch Loss: 0.00021378723613452166\n",
      "Epoch 3060, Loss: 0.0015046052303659962, Final Batch Loss: 0.00047741507296450436\n",
      "Epoch 3061, Loss: 0.0030639226861239877, Final Batch Loss: 0.00036421100958250463\n",
      "Epoch 3062, Loss: 0.002970473509776639, Final Batch Loss: 0.000757165253162384\n",
      "Epoch 3063, Loss: 0.015514001686824486, Final Batch Loss: 0.0003453954413998872\n",
      "Epoch 3064, Loss: 0.011462035705335438, Final Batch Loss: 8.399685611948371e-05\n",
      "Epoch 3065, Loss: 0.003870658030791674, Final Batch Loss: 2.053607749985531e-05\n",
      "Epoch 3066, Loss: 0.000863215456774924, Final Batch Loss: 8.627736679045483e-05\n",
      "Epoch 3067, Loss: 0.004997520300094038, Final Batch Loss: 0.0005487477174028754\n",
      "Epoch 3068, Loss: 0.001087908080080524, Final Batch Loss: 0.00012073378456989303\n",
      "Epoch 3069, Loss: 0.0021010220152675174, Final Batch Loss: 0.0003934664709959179\n",
      "Epoch 3070, Loss: 0.0013904449879191816, Final Batch Loss: 0.0006662615924142301\n",
      "Epoch 3071, Loss: 0.041083424468524754, Final Batch Loss: 0.00017588866467121989\n",
      "Epoch 3072, Loss: 0.005492345022503287, Final Batch Loss: 0.0046928031370043755\n",
      "Epoch 3073, Loss: 0.0009421912473044358, Final Batch Loss: 0.00017582130385562778\n",
      "Epoch 3074, Loss: 0.0014366312898346223, Final Batch Loss: 2.6816793251782656e-05\n",
      "Epoch 3075, Loss: 0.0023522573756054044, Final Batch Loss: 4.7050489229150116e-05\n",
      "Epoch 3076, Loss: 0.02924981182877673, Final Batch Loss: 0.02861843816936016\n",
      "Epoch 3077, Loss: 0.0014925857103662565, Final Batch Loss: 0.0003710490418598056\n",
      "Epoch 3078, Loss: 0.0015962618699632003, Final Batch Loss: 1.3860512808605563e-05\n",
      "Epoch 3079, Loss: 0.0023133760696509853, Final Batch Loss: 0.00017349657719023526\n",
      "Epoch 3080, Loss: 0.0017816122926888056, Final Batch Loss: 0.00030143599724397063\n",
      "Epoch 3081, Loss: 0.0734058122325223, Final Batch Loss: 0.05299660190939903\n",
      "Epoch 3082, Loss: 0.002411838446278125, Final Batch Loss: 0.0002053814969258383\n",
      "Epoch 3083, Loss: 0.021818455730681308, Final Batch Loss: 0.00013404111086856574\n",
      "Epoch 3084, Loss: 0.005331384018063545, Final Batch Loss: 0.0018748678267002106\n",
      "Epoch 3085, Loss: 0.031012543651740998, Final Batch Loss: 0.00014505130820907652\n",
      "Epoch 3086, Loss: 0.0033790203306125477, Final Batch Loss: 9.747712465468794e-05\n",
      "Epoch 3087, Loss: 0.0021494226821232587, Final Batch Loss: 0.0007301135337911546\n",
      "Epoch 3088, Loss: 0.019799944828264415, Final Batch Loss: 0.001616671564988792\n",
      "Epoch 3089, Loss: 0.005264339968562126, Final Batch Loss: 0.00018167975940741599\n",
      "Epoch 3090, Loss: 0.001625724573386833, Final Batch Loss: 0.00024698011111468077\n",
      "Epoch 3091, Loss: 0.00434125418541953, Final Batch Loss: 0.001307230326347053\n",
      "Epoch 3092, Loss: 0.020529444380372297, Final Batch Loss: 0.019290775060653687\n",
      "Epoch 3093, Loss: 0.007707949862378882, Final Batch Loss: 3.183357694069855e-05\n",
      "Epoch 3094, Loss: 0.02116500010015443, Final Batch Loss: 0.0006639341590926051\n",
      "Epoch 3095, Loss: 0.003302050376078114, Final Batch Loss: 0.0002909316390287131\n",
      "Epoch 3096, Loss: 0.004154849430051399, Final Batch Loss: 7.879941404098645e-05\n",
      "Epoch 3097, Loss: 0.0033498875272925943, Final Batch Loss: 0.0001920028735185042\n",
      "Epoch 3098, Loss: 0.0026257940189680085, Final Batch Loss: 7.834892312530428e-05\n",
      "Epoch 3099, Loss: 0.001614152837646543, Final Batch Loss: 9.3789476522943e-06\n",
      "Epoch 3100, Loss: 0.0041720906883710995, Final Batch Loss: 0.0005830373847857118\n",
      "Epoch 3101, Loss: 0.0017532109559397213, Final Batch Loss: 0.0006084379274398088\n",
      "Epoch 3102, Loss: 0.0038590185722569004, Final Batch Loss: 0.00023084496206138283\n",
      "Epoch 3103, Loss: 0.00334910282981582, Final Batch Loss: 0.000612362870015204\n",
      "Epoch 3104, Loss: 0.008632048626168398, Final Batch Loss: 9.12806935957633e-05\n",
      "Epoch 3105, Loss: 0.002586978007457219, Final Batch Loss: 0.0002487589663360268\n",
      "Epoch 3106, Loss: 0.02378268055326771, Final Batch Loss: 0.019841013476252556\n",
      "Epoch 3107, Loss: 0.0033633225502853747, Final Batch Loss: 0.001169879687950015\n",
      "Epoch 3108, Loss: 0.012532490738522029, Final Batch Loss: 3.8099846278782934e-05\n",
      "Epoch 3109, Loss: 0.009135002372204326, Final Batch Loss: 0.0014746227534487844\n",
      "Epoch 3110, Loss: 0.0043752025521826, Final Batch Loss: 0.0007013626163825393\n",
      "Epoch 3111, Loss: 0.022080856331740506, Final Batch Loss: 0.00040694064227864146\n",
      "Epoch 3112, Loss: 0.002565312126534991, Final Batch Loss: 6.357404345180839e-05\n",
      "Epoch 3113, Loss: 0.004104088613530621, Final Batch Loss: 0.00036114334943704307\n",
      "Epoch 3114, Loss: 0.015098075084097218, Final Batch Loss: 0.00017692720575723797\n",
      "Epoch 3115, Loss: 0.002949373592855409, Final Batch Loss: 0.0017726311925798655\n",
      "Epoch 3116, Loss: 0.007849693487514742, Final Batch Loss: 0.0002102731668855995\n",
      "Epoch 3117, Loss: 0.000988041443633847, Final Batch Loss: 9.994018182624131e-05\n",
      "Epoch 3118, Loss: 0.0033611844992265105, Final Batch Loss: 0.00021074761752970517\n",
      "Epoch 3119, Loss: 0.02939847600646317, Final Batch Loss: 0.0021841637790203094\n",
      "Epoch 3120, Loss: 0.001821354062485625, Final Batch Loss: 2.070793016173411e-05\n",
      "Epoch 3121, Loss: 0.008142137368849944, Final Batch Loss: 0.0013901946367695928\n",
      "Epoch 3122, Loss: 0.022100626956671476, Final Batch Loss: 6.116121221566573e-05\n",
      "Epoch 3123, Loss: 0.010450571789988317, Final Batch Loss: 0.00019260642875451595\n",
      "Epoch 3124, Loss: 0.007353724497079384, Final Batch Loss: 4.05736718676053e-05\n",
      "Epoch 3125, Loss: 0.0052295536006568, Final Batch Loss: 2.1211992134340107e-05\n",
      "Epoch 3126, Loss: 0.0017449467068217928, Final Batch Loss: 5.047496233601123e-05\n",
      "Epoch 3127, Loss: 0.008265060896519572, Final Batch Loss: 0.001103899790905416\n",
      "Epoch 3128, Loss: 0.0071453028212999925, Final Batch Loss: 0.002951170550659299\n",
      "Epoch 3129, Loss: 0.002057336147117894, Final Batch Loss: 0.00014288953389041126\n",
      "Epoch 3130, Loss: 0.0005717727945011575, Final Batch Loss: 7.726442709099501e-05\n",
      "Epoch 3131, Loss: 0.0011972261781920679, Final Batch Loss: 0.0001232154609169811\n",
      "Epoch 3132, Loss: 0.0018080285844916943, Final Batch Loss: 0.00019166569109074771\n",
      "Epoch 3133, Loss: 0.002315850668310304, Final Batch Loss: 1.6320469512720592e-05\n",
      "Epoch 3134, Loss: 0.000886010073372745, Final Batch Loss: 2.9650478609255515e-05\n",
      "Epoch 3135, Loss: 0.0009049328527908074, Final Batch Loss: 1.3774168110103346e-05\n",
      "Epoch 3136, Loss: 0.012843677090131678, Final Batch Loss: 4.308380084694363e-05\n",
      "Epoch 3137, Loss: 0.006099347875533567, Final Batch Loss: 9.15510372578865e-06\n",
      "Epoch 3138, Loss: 0.003982513015216682, Final Batch Loss: 0.0005155706312507391\n",
      "Epoch 3139, Loss: 0.0008151419933710713, Final Batch Loss: 0.00011292035924270749\n",
      "Epoch 3140, Loss: 0.009964718426999752, Final Batch Loss: 1.9559925931389444e-05\n",
      "Epoch 3141, Loss: 0.0033021663984982297, Final Batch Loss: 0.00021229278354439884\n",
      "Epoch 3142, Loss: 0.0011879298799613025, Final Batch Loss: 1.0490690328879282e-05\n",
      "Epoch 3143, Loss: 0.006494106586615089, Final Batch Loss: 0.003961365204304457\n",
      "Epoch 3144, Loss: 0.003745180874830112, Final Batch Loss: 0.00030083878664299846\n",
      "Epoch 3145, Loss: 0.0030079633943387307, Final Batch Loss: 0.00041702218004502356\n",
      "Epoch 3146, Loss: 0.003979163368057925, Final Batch Loss: 6.406019383575767e-05\n",
      "Epoch 3147, Loss: 0.0007465535682058544, Final Batch Loss: 1.7527521777083166e-05\n",
      "Epoch 3148, Loss: 0.011271747811406385, Final Batch Loss: 7.783422915963456e-06\n",
      "Epoch 3149, Loss: 0.008387757879972924, Final Batch Loss: 6.754390778951347e-05\n",
      "Epoch 3150, Loss: 0.002970421457575867, Final Batch Loss: 4.843438000534661e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3151, Loss: 0.0027539149741642177, Final Batch Loss: 6.52218222967349e-05\n",
      "Epoch 3152, Loss: 0.02273180011252407, Final Batch Loss: 0.0002503978321328759\n",
      "Epoch 3153, Loss: 0.0030578279402107, Final Batch Loss: 0.00011321531201247126\n",
      "Epoch 3154, Loss: 0.0014431986055569723, Final Batch Loss: 0.0002149265055777505\n",
      "Epoch 3155, Loss: 0.004241689610353205, Final Batch Loss: 7.945149263832718e-06\n",
      "Epoch 3156, Loss: 0.001438344072084874, Final Batch Loss: 0.0001927707198774442\n",
      "Epoch 3157, Loss: 0.0026968400161422323, Final Batch Loss: 0.0005847620777785778\n",
      "Epoch 3158, Loss: 0.009677617519628257, Final Batch Loss: 0.0001534949551569298\n",
      "Epoch 3159, Loss: 0.0028338146075839177, Final Batch Loss: 0.0001245053281309083\n",
      "Epoch 3160, Loss: 0.00950414351245854, Final Batch Loss: 0.0006006876355968416\n",
      "Epoch 3161, Loss: 0.014647178672021255, Final Batch Loss: 0.00032108192681334913\n",
      "Epoch 3162, Loss: 0.0012679317514994182, Final Batch Loss: 0.00011832860764116049\n",
      "Epoch 3163, Loss: 0.006916725164046511, Final Batch Loss: 2.331624273210764e-05\n",
      "Epoch 3164, Loss: 0.004976422613253817, Final Batch Loss: 0.00034958735341206193\n",
      "Epoch 3165, Loss: 0.01905034700757824, Final Batch Loss: 0.000762704003136605\n",
      "Epoch 3166, Loss: 0.02322903440654045, Final Batch Loss: 9.422224684385583e-05\n",
      "Epoch 3167, Loss: 0.0016262560529867187, Final Batch Loss: 0.00015003609587438405\n",
      "Epoch 3168, Loss: 0.0010782080353237689, Final Batch Loss: 9.098385635297745e-05\n",
      "Epoch 3169, Loss: 0.0013232154760771664, Final Batch Loss: 2.446364851493854e-05\n",
      "Epoch 3170, Loss: 0.002380970134254312, Final Batch Loss: 3.8166806916706264e-05\n",
      "Epoch 3171, Loss: 0.016604105909209466, Final Batch Loss: 0.015682874247431755\n",
      "Epoch 3172, Loss: 0.0009037943600560538, Final Batch Loss: 0.0002384598192293197\n",
      "Epoch 3173, Loss: 0.004088664070877712, Final Batch Loss: 6.558398308698088e-05\n",
      "Epoch 3174, Loss: 0.002768487098364858, Final Batch Loss: 4.598042505676858e-05\n",
      "Epoch 3175, Loss: 0.007544779462477891, Final Batch Loss: 1.8090231606038287e-05\n",
      "Epoch 3176, Loss: 0.012544626166345552, Final Batch Loss: 0.010012424550950527\n",
      "Epoch 3177, Loss: 0.004381451130029745, Final Batch Loss: 0.0014136135578155518\n",
      "Epoch 3178, Loss: 0.0023966505541466177, Final Batch Loss: 0.0009367073653265834\n",
      "Epoch 3179, Loss: 0.0030267717738752253, Final Batch Loss: 0.00040674261981621385\n",
      "Epoch 3180, Loss: 0.0013058889671810903, Final Batch Loss: 0.00033497490221634507\n",
      "Epoch 3181, Loss: 0.0014514765130115848, Final Batch Loss: 0.0005097058601677418\n",
      "Epoch 3182, Loss: 0.003270722055276565, Final Batch Loss: 9.142754606727976e-06\n",
      "Epoch 3183, Loss: 0.003517477678542491, Final Batch Loss: 0.00037931493716314435\n",
      "Epoch 3184, Loss: 0.0012790133459930075, Final Batch Loss: 9.77916170086246e-06\n",
      "Epoch 3185, Loss: 0.002880584456761426, Final Batch Loss: 1.3059824595984537e-05\n",
      "Epoch 3186, Loss: 0.003295549588074209, Final Batch Loss: 0.0007796885329298675\n",
      "Epoch 3187, Loss: 0.002936716395197436, Final Batch Loss: 0.00011433142208261415\n",
      "Epoch 3188, Loss: 0.0007017039097263478, Final Batch Loss: 0.00013951963046565652\n",
      "Epoch 3189, Loss: 0.0006779096620448399, Final Batch Loss: 0.00020068571029696614\n",
      "Epoch 3190, Loss: 0.011506169779750053, Final Batch Loss: 0.0003303065604995936\n",
      "Epoch 3191, Loss: 0.0008502757491442026, Final Batch Loss: 8.457566764263902e-06\n",
      "Epoch 3192, Loss: 0.0062841132312314585, Final Batch Loss: 0.0027504286263138056\n",
      "Epoch 3193, Loss: 0.0005074523032817524, Final Batch Loss: 8.454916678601876e-05\n",
      "Epoch 3194, Loss: 0.007201199208793696, Final Batch Loss: 4.457299837667961e-06\n",
      "Epoch 3195, Loss: 0.0009514246821709094, Final Batch Loss: 5.280827281239908e-06\n",
      "Epoch 3196, Loss: 0.03888462861868902, Final Batch Loss: 0.03369021788239479\n",
      "Epoch 3197, Loss: 0.0012771737892762758, Final Batch Loss: 0.0004995762137696147\n",
      "Epoch 3198, Loss: 0.002318892726179911, Final Batch Loss: 5.9159210650250316e-05\n",
      "Epoch 3199, Loss: 0.019864053056153352, Final Batch Loss: 0.019062558189034462\n",
      "Epoch 3200, Loss: 0.00928137846494792, Final Batch Loss: 0.00042254559230059385\n",
      "Epoch 3201, Loss: 0.03985914888835396, Final Batch Loss: 0.0011889522429555655\n",
      "Epoch 3202, Loss: 0.0013472492573782802, Final Batch Loss: 0.00026465445989742875\n",
      "Epoch 3203, Loss: 0.004204967059195042, Final Batch Loss: 0.0012179827317595482\n",
      "Epoch 3204, Loss: 0.0016712518045096658, Final Batch Loss: 9.198551560984924e-05\n",
      "Epoch 3205, Loss: 0.012727288427413441, Final Batch Loss: 0.010600699111819267\n",
      "Epoch 3206, Loss: 0.0010628849704517052, Final Batch Loss: 0.0003245925181545317\n",
      "Epoch 3207, Loss: 0.003986612651260657, Final Batch Loss: 3.898375780408969e-06\n",
      "Epoch 3208, Loss: 0.007109242857040954, Final Batch Loss: 1.482079278503079e-05\n",
      "Epoch 3209, Loss: 0.001287947132368572, Final Batch Loss: 0.0001969704171642661\n",
      "Epoch 3210, Loss: 0.0016924672381719574, Final Batch Loss: 0.00018805325089488178\n",
      "Epoch 3211, Loss: 0.0031547249272989575, Final Batch Loss: 0.00018151408585254103\n",
      "Epoch 3212, Loss: 0.0048390003976237494, Final Batch Loss: 0.00029195204842835665\n",
      "Epoch 3213, Loss: 0.0033923360097105615, Final Batch Loss: 2.3403983504977077e-05\n",
      "Epoch 3214, Loss: 0.0024598577419965295, Final Batch Loss: 0.00010354697587899864\n",
      "Epoch 3215, Loss: 0.0036133625253569335, Final Batch Loss: 0.001824569539166987\n",
      "Epoch 3216, Loss: 0.0004242686591169331, Final Batch Loss: 1.7877493519335985e-05\n",
      "Epoch 3217, Loss: 0.002188666123402072, Final Batch Loss: 1.2846980098402128e-05\n",
      "Epoch 3218, Loss: 0.001004197823931463, Final Batch Loss: 7.196115620899945e-05\n",
      "Epoch 3219, Loss: 0.0035669474018504843, Final Batch Loss: 3.534039933583699e-05\n",
      "Epoch 3220, Loss: 0.0017304898537986446, Final Batch Loss: 4.295916369301267e-05\n",
      "Epoch 3221, Loss: 0.0008016176398086827, Final Batch Loss: 3.3609900128794834e-05\n",
      "Epoch 3222, Loss: 0.0006686147571599577, Final Batch Loss: 5.2780342230107635e-05\n",
      "Epoch 3223, Loss: 0.005945336057266104, Final Batch Loss: 7.209869363578036e-05\n",
      "Epoch 3224, Loss: 0.00433185192378005, Final Batch Loss: 2.2338677808875218e-05\n",
      "Epoch 3225, Loss: 0.001294456308869485, Final Batch Loss: 9.610514553060057e-07\n",
      "Epoch 3226, Loss: 0.0010830650680873077, Final Batch Loss: 0.0001598909148015082\n",
      "Epoch 3227, Loss: 0.002691415002118447, Final Batch Loss: 0.001645082258619368\n",
      "Epoch 3228, Loss: 0.000482956196719897, Final Batch Loss: 0.00010432937415316701\n",
      "Epoch 3229, Loss: 0.0015586019653710537, Final Batch Loss: 0.0001389777025906369\n",
      "Epoch 3230, Loss: 0.0020270104141673073, Final Batch Loss: 0.0003930529928766191\n",
      "Epoch 3231, Loss: 0.008654808298160788, Final Batch Loss: 0.004431186243891716\n",
      "Epoch 3232, Loss: 0.0010366505584897823, Final Batch Loss: 2.6299230739823543e-06\n",
      "Epoch 3233, Loss: 0.004118275952350814, Final Batch Loss: 0.0002198103175032884\n",
      "Epoch 3234, Loss: 0.0027365373098291457, Final Batch Loss: 0.00031426968052983284\n",
      "Epoch 3235, Loss: 0.0016596487712376984, Final Batch Loss: 7.705698953941464e-05\n",
      "Epoch 3236, Loss: 0.0048861057384783635, Final Batch Loss: 0.0005907019949518144\n",
      "Epoch 3237, Loss: 0.009761942960722081, Final Batch Loss: 0.00011980251292698085\n",
      "Epoch 3238, Loss: 0.0004264253475412261, Final Batch Loss: 0.0001387822994729504\n",
      "Epoch 3239, Loss: 0.006469446800110745, Final Batch Loss: 6.140506684459979e-06\n",
      "Epoch 3240, Loss: 0.0017434461005905177, Final Batch Loss: 0.0003542632912285626\n",
      "Epoch 3241, Loss: 0.021036765596363693, Final Batch Loss: 0.0004095181939192116\n",
      "Epoch 3242, Loss: 0.0014956572672417678, Final Batch Loss: 4.571184490487212e-06\n",
      "Epoch 3243, Loss: 0.005914295223192312, Final Batch Loss: 0.0005485524889081717\n",
      "Epoch 3244, Loss: 0.01216434847083292, Final Batch Loss: 0.0004251503851264715\n",
      "Epoch 3245, Loss: 0.004082936537997739, Final Batch Loss: 9.370428415422793e-06\n",
      "Epoch 3246, Loss: 0.003177914608386345, Final Batch Loss: 0.0003136176965199411\n",
      "Epoch 3247, Loss: 0.025456920561055085, Final Batch Loss: 1.3873192983737681e-05\n",
      "Epoch 3248, Loss: 0.0038076359778642654, Final Batch Loss: 0.002483789576217532\n",
      "Epoch 3249, Loss: 0.01404136473138351, Final Batch Loss: 4.028323746751994e-05\n",
      "Epoch 3250, Loss: 0.0016839926247484982, Final Batch Loss: 4.003338108304888e-05\n",
      "Epoch 3251, Loss: 0.006390156959241722, Final Batch Loss: 0.0007434610160999\n",
      "Epoch 3252, Loss: 0.02769381136749871, Final Batch Loss: 0.00028718248358927667\n",
      "Epoch 3253, Loss: 0.020708804862806574, Final Batch Loss: 0.019637107849121094\n",
      "Epoch 3254, Loss: 0.04043945120065473, Final Batch Loss: 0.00026771429111249745\n",
      "Epoch 3255, Loss: 0.02415473413111613, Final Batch Loss: 1.4427382666326594e-05\n",
      "Epoch 3256, Loss: 0.019376848918909673, Final Batch Loss: 0.001301129930652678\n",
      "Epoch 3257, Loss: 0.10760127851972356, Final Batch Loss: 0.0005422600661404431\n",
      "Epoch 3258, Loss: 0.0275003963324707, Final Batch Loss: 0.004020265769213438\n",
      "Epoch 3259, Loss: 0.016730884090065956, Final Batch Loss: 0.0010645423317328095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3260, Loss: 0.0719171574783104, Final Batch Loss: 5.1235609134892e-05\n",
      "Epoch 3261, Loss: 0.004186544134427095, Final Batch Loss: 0.000741500873118639\n",
      "Epoch 3262, Loss: 0.034948243701364845, Final Batch Loss: 0.0005521297571249306\n",
      "Epoch 3263, Loss: 0.005593719673925079, Final Batch Loss: 0.00030957773560658097\n",
      "Epoch 3264, Loss: 0.07649352874432225, Final Batch Loss: 0.0011385808466002345\n",
      "Epoch 3265, Loss: 0.0028907556188642047, Final Batch Loss: 8.265961514553055e-05\n",
      "Epoch 3266, Loss: 0.0037139563355594873, Final Batch Loss: 0.0003563080681487918\n",
      "Epoch 3267, Loss: 0.005906745413085446, Final Batch Loss: 0.00038339689490385354\n",
      "Epoch 3268, Loss: 0.0035714872647076845, Final Batch Loss: 0.0009732166072353721\n",
      "Epoch 3269, Loss: 0.035478262274409644, Final Batch Loss: 0.00015286127745639533\n",
      "Epoch 3270, Loss: 0.013936651215772144, Final Batch Loss: 0.00019778327259700745\n",
      "Epoch 3271, Loss: 0.016512819049239624, Final Batch Loss: 0.009411340579390526\n",
      "Epoch 3272, Loss: 0.002267881893203594, Final Batch Loss: 3.393426595721394e-05\n",
      "Epoch 3273, Loss: 0.021954568532237317, Final Batch Loss: 0.012591482140123844\n",
      "Epoch 3274, Loss: 0.01186852075625211, Final Batch Loss: 0.005014641210436821\n",
      "Epoch 3275, Loss: 0.013004004860704299, Final Batch Loss: 0.0008643674664199352\n",
      "Epoch 3276, Loss: 0.005811678594909608, Final Batch Loss: 0.0003389210905879736\n",
      "Epoch 3277, Loss: 0.005757935428846395, Final Batch Loss: 0.0014047635486349463\n",
      "Epoch 3278, Loss: 0.007571089108751039, Final Batch Loss: 1.1133959560538642e-05\n",
      "Epoch 3279, Loss: 0.02024136685213307, Final Batch Loss: 0.00010412440315121785\n",
      "Epoch 3280, Loss: 0.02011209342163056, Final Batch Loss: 0.003039645031094551\n",
      "Epoch 3281, Loss: 0.004334583594754804, Final Batch Loss: 0.0004746855702251196\n",
      "Epoch 3282, Loss: 0.004179511633992661, Final Batch Loss: 6.754266360076144e-05\n",
      "Epoch 3283, Loss: 0.007231792878883425, Final Batch Loss: 0.0014593465020880103\n",
      "Epoch 3284, Loss: 0.002192425753491989, Final Batch Loss: 3.537355951266363e-05\n",
      "Epoch 3285, Loss: 0.004674633519925919, Final Batch Loss: 3.1675651825935347e-06\n",
      "Epoch 3286, Loss: 0.001042253126797732, Final Batch Loss: 0.00016858233720995486\n",
      "Epoch 3287, Loss: 0.004267226919182576, Final Batch Loss: 8.868191798683256e-05\n",
      "Epoch 3288, Loss: 0.0014157520272419788, Final Batch Loss: 0.00019871839322149754\n",
      "Epoch 3289, Loss: 0.010950167092232732, Final Batch Loss: 0.0003072584222536534\n",
      "Epoch 3290, Loss: 0.002078112333038007, Final Batch Loss: 0.00012752943439409137\n",
      "Epoch 3291, Loss: 0.036510867004835745, Final Batch Loss: 9.68670065049082e-05\n",
      "Epoch 3292, Loss: 0.001591977332282113, Final Batch Loss: 0.0003176101017743349\n",
      "Epoch 3293, Loss: 0.003854628894259804, Final Batch Loss: 2.3608505216543563e-05\n",
      "Epoch 3294, Loss: 0.003744338420801796, Final Batch Loss: 7.851490227039903e-05\n",
      "Epoch 3295, Loss: 0.001598647861101199, Final Batch Loss: 0.00011843491665786132\n",
      "Epoch 3296, Loss: 0.004912734017125331, Final Batch Loss: 0.00020205539476592094\n",
      "Epoch 3297, Loss: 0.0013818823936162516, Final Batch Loss: 0.0003354348591528833\n",
      "Epoch 3298, Loss: 0.0021305959598976187, Final Batch Loss: 0.00022637021902482957\n",
      "Epoch 3299, Loss: 0.004411689620610559, Final Batch Loss: 2.7739723009290174e-05\n",
      "Epoch 3300, Loss: 0.013155237782484619, Final Batch Loss: 0.003146981354802847\n",
      "Epoch 3301, Loss: 0.004807173514564056, Final Batch Loss: 0.0038669537752866745\n",
      "Epoch 3302, Loss: 0.034074111230438575, Final Batch Loss: 0.0005333346780389547\n",
      "Epoch 3303, Loss: 0.0030477209002128802, Final Batch Loss: 0.00011055119102820754\n",
      "Epoch 3304, Loss: 0.0016031581071729306, Final Batch Loss: 0.00033571120002307\n",
      "Epoch 3305, Loss: 0.0018851138374884613, Final Batch Loss: 0.0001339657319476828\n",
      "Epoch 3306, Loss: 0.027538414928130805, Final Batch Loss: 0.02220618911087513\n",
      "Epoch 3307, Loss: 0.0051618308752949815, Final Batch Loss: 0.00013974119792692363\n",
      "Epoch 3308, Loss: 0.007374889319180511, Final Batch Loss: 5.269540997687727e-05\n",
      "Epoch 3309, Loss: 0.0018009059231189894, Final Batch Loss: 1.1713384992617648e-05\n",
      "Epoch 3310, Loss: 0.0005387375294958474, Final Batch Loss: 0.00022636742505710572\n",
      "Epoch 3311, Loss: 0.006188383311382495, Final Batch Loss: 9.566498920321465e-05\n",
      "Epoch 3312, Loss: 0.0020006165195809444, Final Batch Loss: 1.8497599739930592e-05\n",
      "Epoch 3313, Loss: 0.0029835242858098354, Final Batch Loss: 0.0003582908248063177\n",
      "Epoch 3314, Loss: 0.0034697261507972144, Final Batch Loss: 0.0012451305519789457\n",
      "Epoch 3315, Loss: 0.00041185133159160614, Final Batch Loss: 4.382827319204807e-05\n",
      "Epoch 3316, Loss: 0.0011854852200485766, Final Batch Loss: 7.279652345459908e-05\n",
      "Epoch 3317, Loss: 0.0021044403920313925, Final Batch Loss: 7.32579155737767e-06\n",
      "Epoch 3318, Loss: 0.0012763600207108539, Final Batch Loss: 2.3990072804735973e-05\n",
      "Epoch 3319, Loss: 0.0008026949981285725, Final Batch Loss: 4.248894765623845e-05\n",
      "Epoch 3320, Loss: 0.002080079917504918, Final Batch Loss: 0.0003122783964499831\n",
      "Epoch 3321, Loss: 0.0015902300292509608, Final Batch Loss: 5.376043918658979e-05\n",
      "Epoch 3322, Loss: 0.0013499455417331774, Final Batch Loss: 5.922545460634865e-05\n",
      "Epoch 3323, Loss: 0.0008076684098341502, Final Batch Loss: 4.6120647311909124e-05\n",
      "Epoch 3324, Loss: 0.0013438134410534985, Final Batch Loss: 0.00038442816003225744\n",
      "Epoch 3325, Loss: 0.002778879103516374, Final Batch Loss: 6.756678430974716e-06\n",
      "Epoch 3326, Loss: 0.0019742812910408247, Final Batch Loss: 3.904609911842272e-05\n",
      "Epoch 3327, Loss: 0.004767276250277064, Final Batch Loss: 1.4401235603145324e-05\n",
      "Epoch 3328, Loss: 0.0033003136923070997, Final Batch Loss: 0.00011187231575604528\n",
      "Epoch 3329, Loss: 0.001117121551942546, Final Batch Loss: 0.0002472291816957295\n",
      "Epoch 3330, Loss: 0.0015821991273696767, Final Batch Loss: 0.00011060182441724464\n",
      "Epoch 3331, Loss: 0.002077496747915575, Final Batch Loss: 1.1556460776773747e-05\n",
      "Epoch 3332, Loss: 0.010401622570498148, Final Batch Loss: 0.00029752502450719476\n",
      "Epoch 3333, Loss: 0.023231095226947218, Final Batch Loss: 0.00012213563604746014\n",
      "Epoch 3334, Loss: 0.03742187227180693, Final Batch Loss: 0.0362580306828022\n",
      "Epoch 3335, Loss: 0.03910292089858558, Final Batch Loss: 0.007416607812047005\n",
      "Epoch 3336, Loss: 0.011010691028786823, Final Batch Loss: 0.0007438758620992303\n",
      "Epoch 3337, Loss: 0.0012432016046659555, Final Batch Loss: 5.697990854969248e-05\n",
      "Epoch 3338, Loss: 0.0011006939348590095, Final Batch Loss: 4.9922320613404736e-05\n",
      "Epoch 3339, Loss: 0.0016213150302064605, Final Batch Loss: 0.00018000046839006245\n",
      "Epoch 3340, Loss: 0.0038174667133716866, Final Batch Loss: 0.0028151769656687975\n",
      "Epoch 3341, Loss: 0.000957904827373568, Final Batch Loss: 7.38898161216639e-05\n",
      "Epoch 3342, Loss: 0.010531898877161439, Final Batch Loss: 0.0005565900937654078\n",
      "Epoch 3343, Loss: 0.0012069072035956196, Final Batch Loss: 0.00039834759081713855\n",
      "Epoch 3344, Loss: 0.003240070174797438, Final Batch Loss: 0.0009441880392841995\n",
      "Epoch 3345, Loss: 0.004093192896107212, Final Batch Loss: 0.0003913786495104432\n",
      "Epoch 3346, Loss: 0.0010140938102267683, Final Batch Loss: 1.4524703146889806e-05\n",
      "Epoch 3347, Loss: 0.0006466737831942737, Final Batch Loss: 0.00020242235041223466\n",
      "Epoch 3348, Loss: 0.0013950184766144957, Final Batch Loss: 0.00015782099217176437\n",
      "Epoch 3349, Loss: 0.009202078282214643, Final Batch Loss: 1.2419032827892806e-05\n",
      "Epoch 3350, Loss: 0.01874528741609538, Final Batch Loss: 0.00010107990237884223\n",
      "Epoch 3351, Loss: 0.009485649221460335, Final Batch Loss: 0.0001448438997613266\n",
      "Epoch 3352, Loss: 0.0012661626606131904, Final Batch Loss: 0.00013823418703395873\n",
      "Epoch 3353, Loss: 0.002570420710981125, Final Batch Loss: 0.0009111526887863874\n",
      "Epoch 3354, Loss: 0.05036414601636352, Final Batch Loss: 0.04262106865644455\n",
      "Epoch 3355, Loss: 0.0014799933196627535, Final Batch Loss: 0.00024108272918965667\n",
      "Epoch 3356, Loss: 0.005102951392473187, Final Batch Loss: 0.0005085981101728976\n",
      "Epoch 3357, Loss: 0.058952359890099615, Final Batch Loss: 0.05602097511291504\n",
      "Epoch 3358, Loss: 0.0020164907909929752, Final Batch Loss: 0.0003115704166702926\n",
      "Epoch 3359, Loss: 0.05819718455313705, Final Batch Loss: 0.0011979942210018635\n",
      "Epoch 3360, Loss: 0.002395180217718007, Final Batch Loss: 6.071327152312733e-05\n",
      "Epoch 3361, Loss: 0.014290101044025505, Final Batch Loss: 0.0003260769881308079\n",
      "Epoch 3362, Loss: 0.005707736054318957, Final Batch Loss: 0.0007357212598435581\n",
      "Epoch 3363, Loss: 0.005203895088925492, Final Batch Loss: 0.0007463094079867005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3364, Loss: 0.0030157417932059616, Final Batch Loss: 0.0008896247018128633\n",
      "Epoch 3365, Loss: 0.008262148301582783, Final Batch Loss: 0.00014955687220208347\n",
      "Epoch 3366, Loss: 0.014472705443040468, Final Batch Loss: 0.010000256821513176\n",
      "Epoch 3367, Loss: 0.0173412105141324, Final Batch Loss: 0.015562557615339756\n",
      "Epoch 3368, Loss: 0.011503395886393264, Final Batch Loss: 0.0016813434194773436\n",
      "Epoch 3369, Loss: 0.02360566583229229, Final Batch Loss: 0.00018906315381173044\n",
      "Epoch 3370, Loss: 0.014203477330738679, Final Batch Loss: 0.00807629618793726\n",
      "Epoch 3371, Loss: 0.04160157052683644, Final Batch Loss: 0.028516611084342003\n",
      "Epoch 3372, Loss: 0.028053924354026094, Final Batch Loss: 6.315254722721875e-05\n",
      "Epoch 3373, Loss: 0.0015440509741893038, Final Batch Loss: 0.000681359029840678\n",
      "Epoch 3374, Loss: 0.0061910148651804775, Final Batch Loss: 0.00041033339221030474\n",
      "Epoch 3375, Loss: 0.005230784157902235, Final Batch Loss: 5.087467798148282e-05\n",
      "Epoch 3376, Loss: 0.003431362201808952, Final Batch Loss: 0.0006343825371004641\n",
      "Epoch 3377, Loss: 0.005248363704595249, Final Batch Loss: 0.0014884800184518099\n",
      "Epoch 3378, Loss: 0.021536242158617824, Final Batch Loss: 0.00014588230988010764\n",
      "Epoch 3379, Loss: 0.003425555235480715, Final Batch Loss: 1.5149465070862789e-05\n",
      "Epoch 3380, Loss: 0.013834022916853428, Final Batch Loss: 0.011439370922744274\n",
      "Epoch 3381, Loss: 0.003449590763921151, Final Batch Loss: 0.0005416737403720617\n",
      "Epoch 3382, Loss: 0.001970229030121118, Final Batch Loss: 0.00013875904551241547\n",
      "Epoch 3383, Loss: 0.0039622632175451145, Final Batch Loss: 0.003236871911212802\n",
      "Epoch 3384, Loss: 0.001719775318633765, Final Batch Loss: 0.00025979315978474915\n",
      "Epoch 3385, Loss: 0.0033761460799723864, Final Batch Loss: 0.0002836326602846384\n",
      "Epoch 3386, Loss: 0.0010115230397786945, Final Batch Loss: 0.00011243190965615213\n",
      "Epoch 3387, Loss: 0.0034084654762409627, Final Batch Loss: 0.0005161563749425113\n",
      "Epoch 3388, Loss: 0.0015991625405149534, Final Batch Loss: 3.3891250495798886e-05\n",
      "Epoch 3389, Loss: 0.0027470098502817564, Final Batch Loss: 0.00023123616119846702\n",
      "Epoch 3390, Loss: 0.0025057052189367823, Final Batch Loss: 0.0014257823349907994\n",
      "Epoch 3391, Loss: 0.007411201280774549, Final Batch Loss: 0.00043414480751380324\n",
      "Epoch 3392, Loss: 0.002637360739754513, Final Batch Loss: 5.57254534214735e-05\n",
      "Epoch 3393, Loss: 0.004750406002131058, Final Batch Loss: 8.932170021580532e-05\n",
      "Epoch 3394, Loss: 0.0010668392205843702, Final Batch Loss: 0.0004774882982019335\n",
      "Epoch 3395, Loss: 0.0017001999367494136, Final Batch Loss: 0.00014248726074583828\n",
      "Epoch 3396, Loss: 0.0030532159289577976, Final Batch Loss: 0.0019364745821803808\n",
      "Epoch 3397, Loss: 0.002109686878611683, Final Batch Loss: 0.00043671485036611557\n",
      "Epoch 3398, Loss: 0.003472489639534615, Final Batch Loss: 0.00047881429782137275\n",
      "Epoch 3399, Loss: 0.006194916582899168, Final Batch Loss: 0.002270893892273307\n",
      "Epoch 3400, Loss: 0.0025587597119738348, Final Batch Loss: 0.0004130991583224386\n",
      "Epoch 3401, Loss: 0.010660436422767816, Final Batch Loss: 0.0017450774321332574\n",
      "Epoch 3402, Loss: 0.0012291622624616139, Final Batch Loss: 4.2543484596535563e-05\n",
      "Epoch 3403, Loss: 0.004443683283170685, Final Batch Loss: 0.0003274673654232174\n",
      "Epoch 3404, Loss: 0.0014845673358649947, Final Batch Loss: 0.0003121343324892223\n",
      "Epoch 3405, Loss: 0.0011134709493489936, Final Batch Loss: 0.00020403896633069962\n",
      "Epoch 3406, Loss: 0.0015976417107594898, Final Batch Loss: 6.670341826975346e-05\n",
      "Epoch 3407, Loss: 0.0015266838527168147, Final Batch Loss: 0.0001551312889205292\n",
      "Epoch 3408, Loss: 0.0014347018513944931, Final Batch Loss: 3.0964620236773044e-05\n",
      "Epoch 3409, Loss: 0.0006908946252224268, Final Batch Loss: 1.0782854587887414e-05\n",
      "Epoch 3410, Loss: 0.0010905108429142274, Final Batch Loss: 0.0003225869731977582\n",
      "Epoch 3411, Loss: 0.0033867317397380248, Final Batch Loss: 0.00030127461650408804\n",
      "Epoch 3412, Loss: 0.0012174652674730169, Final Batch Loss: 0.0005914046778343618\n",
      "Epoch 3413, Loss: 0.00048171863818424754, Final Batch Loss: 4.3570733396336436e-05\n",
      "Epoch 3414, Loss: 0.004369025664345827, Final Batch Loss: 7.21329779480584e-05\n",
      "Epoch 3415, Loss: 0.01498910438385792, Final Batch Loss: 8.76763224368915e-05\n",
      "Epoch 3416, Loss: 0.0006363867669278989, Final Batch Loss: 0.00017019774531945586\n",
      "Epoch 3417, Loss: 0.002531554211600451, Final Batch Loss: 0.0007414431893266737\n",
      "Epoch 3418, Loss: 0.013123091805027798, Final Batch Loss: 0.00019902974599972367\n",
      "Epoch 3419, Loss: 0.022151069104438648, Final Batch Loss: 0.00032873774762265384\n",
      "Epoch 3420, Loss: 0.012762211881636176, Final Batch Loss: 0.0006321309483610094\n",
      "Epoch 3421, Loss: 0.0038883660854480695, Final Batch Loss: 0.00021973540424369276\n",
      "Epoch 3422, Loss: 0.0013350164954317734, Final Batch Loss: 0.000215675521758385\n",
      "Epoch 3423, Loss: 0.0009682313079792948, Final Batch Loss: 4.847568106924882e-06\n",
      "Epoch 3424, Loss: 0.003367552169947885, Final Batch Loss: 0.00025874999118968844\n",
      "Epoch 3425, Loss: 0.007143640745198354, Final Batch Loss: 0.0006274752668105066\n",
      "Epoch 3426, Loss: 0.0050505263498052955, Final Batch Loss: 0.00047010512207634747\n",
      "Epoch 3427, Loss: 0.02541664760792628, Final Batch Loss: 0.0008553445222787559\n",
      "Epoch 3428, Loss: 0.00381401529739378, Final Batch Loss: 0.0003640225331764668\n",
      "Epoch 3429, Loss: 0.0026675670269469265, Final Batch Loss: 0.00016565558325964957\n",
      "Epoch 3430, Loss: 0.002684653620235622, Final Batch Loss: 0.00018976343562826514\n",
      "Epoch 3431, Loss: 0.0023853076563682407, Final Batch Loss: 0.00034279259853065014\n",
      "Epoch 3432, Loss: 0.005279558434267528, Final Batch Loss: 0.0002290963166160509\n",
      "Epoch 3433, Loss: 0.018049370453809388, Final Batch Loss: 9.868281631497666e-05\n",
      "Epoch 3434, Loss: 0.005012463556340663, Final Batch Loss: 0.00020585487072821707\n",
      "Epoch 3435, Loss: 0.026013092856373987, Final Batch Loss: 0.0001970368466572836\n",
      "Epoch 3436, Loss: 0.005011831077354145, Final Batch Loss: 0.002166819991543889\n",
      "Epoch 3437, Loss: 0.002501203123756568, Final Batch Loss: 9.942281394614838e-06\n",
      "Epoch 3438, Loss: 0.008207435046188039, Final Batch Loss: 0.00014374805323313922\n",
      "Epoch 3439, Loss: 0.08624631780548953, Final Batch Loss: 0.052665919065475464\n",
      "Epoch 3440, Loss: 0.034840721113141626, Final Batch Loss: 0.0006785892765037715\n",
      "Epoch 3441, Loss: 0.037916723907983396, Final Batch Loss: 0.0006176402093842626\n",
      "Epoch 3442, Loss: 0.03941266535548493, Final Batch Loss: 0.00032054103212431073\n",
      "Epoch 3443, Loss: 0.030756465988815762, Final Batch Loss: 0.00016981872613541782\n",
      "Epoch 3444, Loss: 0.01083746312360745, Final Batch Loss: 0.0013491983991116285\n",
      "Epoch 3445, Loss: 0.0025060726402443834, Final Batch Loss: 8.734075527172536e-05\n",
      "Epoch 3446, Loss: 0.004835168016143143, Final Batch Loss: 0.0002492894418537617\n",
      "Epoch 3447, Loss: 0.008208679733797908, Final Batch Loss: 0.000612082367297262\n",
      "Epoch 3448, Loss: 0.0044326890201773494, Final Batch Loss: 0.00025098348851315677\n",
      "Epoch 3449, Loss: 0.0017485495191067457, Final Batch Loss: 0.00016811498790048063\n",
      "Epoch 3450, Loss: 0.0030062582663958892, Final Batch Loss: 0.0005831497837789357\n",
      "Epoch 3451, Loss: 0.017571729218616383, Final Batch Loss: 4.323029861552641e-05\n",
      "Epoch 3452, Loss: 0.013446132245007902, Final Batch Loss: 0.00013600097736343741\n",
      "Epoch 3453, Loss: 0.0013944008460384794, Final Batch Loss: 8.983492443803698e-05\n",
      "Epoch 3454, Loss: 0.00168090845545521, Final Batch Loss: 0.00010958090570056811\n",
      "Epoch 3455, Loss: 0.00609384441486327, Final Batch Loss: 8.079005783656612e-05\n",
      "Epoch 3456, Loss: 0.008177456009434536, Final Batch Loss: 0.00011524348519742489\n",
      "Epoch 3457, Loss: 0.0038994282149360515, Final Batch Loss: 0.0002169833896914497\n",
      "Epoch 3458, Loss: 0.0025005586721817963, Final Batch Loss: 0.00010717502300394699\n",
      "Epoch 3459, Loss: 0.0023668094727327116, Final Batch Loss: 7.414863648591563e-05\n",
      "Epoch 3460, Loss: 0.0012633087244466878, Final Batch Loss: 0.000253717735176906\n",
      "Epoch 3461, Loss: 0.017669503587967483, Final Batch Loss: 4.0676426579011604e-05\n",
      "Epoch 3462, Loss: 0.003133639314910397, Final Batch Loss: 0.0015388564206659794\n",
      "Epoch 3463, Loss: 0.002831563055224251, Final Batch Loss: 6.976808799663559e-05\n",
      "Epoch 3464, Loss: 0.004453647132322658, Final Batch Loss: 8.489229367114604e-05\n",
      "Epoch 3465, Loss: 0.002884729256038554, Final Batch Loss: 0.00013657206727657467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3466, Loss: 0.002179202150728088, Final Batch Loss: 9.530456009088084e-05\n",
      "Epoch 3467, Loss: 0.00512883992632851, Final Batch Loss: 0.0006755544454790652\n",
      "Epoch 3468, Loss: 0.003273465292295441, Final Batch Loss: 0.00034468143712729216\n",
      "Epoch 3469, Loss: 0.003104127201368101, Final Batch Loss: 0.00016699849220458418\n",
      "Epoch 3470, Loss: 0.036257896914321464, Final Batch Loss: 0.00024750918964855373\n",
      "Epoch 3471, Loss: 0.001417677156496211, Final Batch Loss: 1.8785305655910634e-05\n",
      "Epoch 3472, Loss: 0.0025295473096775822, Final Batch Loss: 0.0002972370421048254\n",
      "Epoch 3473, Loss: 0.0015543994668405503, Final Batch Loss: 0.0003983185742981732\n",
      "Epoch 3474, Loss: 0.02302822573983576, Final Batch Loss: 0.00012265467375982553\n",
      "Epoch 3475, Loss: 0.004740819342259783, Final Batch Loss: 7.031152927083895e-05\n",
      "Epoch 3476, Loss: 0.021096113647217862, Final Batch Loss: 0.00042013658094219863\n",
      "Epoch 3477, Loss: 0.024262540158815682, Final Batch Loss: 0.001411672681570053\n",
      "Epoch 3478, Loss: 0.006963642896153033, Final Batch Loss: 0.003489898517727852\n",
      "Epoch 3479, Loss: 0.004683668186771683, Final Batch Loss: 0.0001916168403113261\n",
      "Epoch 3480, Loss: 0.0029884573159506544, Final Batch Loss: 0.00027722746017389\n",
      "Epoch 3481, Loss: 0.009387100806634407, Final Batch Loss: 0.006205248646438122\n",
      "Epoch 3482, Loss: 0.003094554034760222, Final Batch Loss: 4.76113855256699e-05\n",
      "Epoch 3483, Loss: 0.0035955405473941937, Final Batch Loss: 0.00031509753898717463\n",
      "Epoch 3484, Loss: 0.005692526130587794, Final Batch Loss: 6.809357728343457e-05\n",
      "Epoch 3485, Loss: 0.0016454292854177766, Final Batch Loss: 5.976937973173335e-05\n",
      "Epoch 3486, Loss: 0.0005868631451448891, Final Batch Loss: 6.273221515584737e-05\n",
      "Epoch 3487, Loss: 0.00664359975962725, Final Batch Loss: 0.0001973253965843469\n",
      "Epoch 3488, Loss: 0.0012243286000739317, Final Batch Loss: 0.000619695580098778\n",
      "Epoch 3489, Loss: 0.013646333201904781, Final Batch Loss: 0.0011355382157489657\n",
      "Epoch 3490, Loss: 0.01314505376649322, Final Batch Loss: 8.959916158346459e-05\n",
      "Epoch 3491, Loss: 0.0017034904449246824, Final Batch Loss: 0.0004233850631862879\n",
      "Epoch 3492, Loss: 0.00498932460686774, Final Batch Loss: 0.00011898746743099764\n",
      "Epoch 3493, Loss: 0.002856602535757702, Final Batch Loss: 3.578784526325762e-05\n",
      "Epoch 3494, Loss: 0.0005128802731633186, Final Batch Loss: 3.9265407394850627e-05\n",
      "Epoch 3495, Loss: 0.019456596579402685, Final Batch Loss: 0.00039678975008428097\n",
      "Epoch 3496, Loss: 0.0064404814766021445, Final Batch Loss: 0.0029308851808309555\n",
      "Epoch 3497, Loss: 0.003999531189037953, Final Batch Loss: 0.00021401813137345016\n",
      "Epoch 3498, Loss: 0.0018874836423492525, Final Batch Loss: 0.0010278787231072783\n",
      "Epoch 3499, Loss: 0.0005933594402449671, Final Batch Loss: 6.675559416180477e-05\n",
      "Epoch 3500, Loss: 0.0025278778566644178, Final Batch Loss: 0.0005764059023931623\n",
      "Epoch 3501, Loss: 0.0011697166592057329, Final Batch Loss: 0.0001567642466397956\n",
      "Epoch 3502, Loss: 0.0012812251079594716, Final Batch Loss: 4.4167602027300745e-05\n",
      "Epoch 3503, Loss: 0.007390533879515715, Final Batch Loss: 0.006516751833260059\n",
      "Epoch 3504, Loss: 0.0007403509189316537, Final Batch Loss: 3.226498301955871e-05\n",
      "Epoch 3505, Loss: 0.00506076509191189, Final Batch Loss: 0.002903966000303626\n",
      "Epoch 3506, Loss: 0.0013173166516935453, Final Batch Loss: 0.0002937719982583076\n",
      "Epoch 3507, Loss: 0.0037752475473098457, Final Batch Loss: 0.0016192763578146696\n",
      "Epoch 3508, Loss: 0.000509349230924272, Final Batch Loss: 2.3649863578611985e-05\n",
      "Epoch 3509, Loss: 0.00174669054831611, Final Batch Loss: 0.0005088192992843688\n",
      "Epoch 3510, Loss: 0.0011071894787164638, Final Batch Loss: 0.0002165653568226844\n",
      "Epoch 3511, Loss: 0.0018583497512736358, Final Batch Loss: 0.00010337056301068515\n",
      "Epoch 3512, Loss: 0.015154603286646307, Final Batch Loss: 0.0009811740601435304\n",
      "Epoch 3513, Loss: 0.0019740436800930183, Final Batch Loss: 0.0014779248740524054\n",
      "Epoch 3514, Loss: 0.003918730693840189, Final Batch Loss: 0.0020280200988054276\n",
      "Epoch 3515, Loss: 0.018205150237918133, Final Batch Loss: 0.000161551361088641\n",
      "Epoch 3516, Loss: 0.02928947945474647, Final Batch Loss: 0.008542923256754875\n",
      "Epoch 3517, Loss: 0.010968227578814549, Final Batch Loss: 0.00026169331977143884\n",
      "Epoch 3518, Loss: 0.0011669621853798162, Final Batch Loss: 0.0007293008966371417\n",
      "Epoch 3519, Loss: 0.0026211019576294348, Final Batch Loss: 0.0009973273845389485\n",
      "Epoch 3520, Loss: 0.00461106926377397, Final Batch Loss: 2.058183599729091e-05\n",
      "Epoch 3521, Loss: 0.008201309108699206, Final Batch Loss: 0.00037240382516756654\n",
      "Epoch 3522, Loss: 0.0013882158600608818, Final Batch Loss: 3.279662269051187e-05\n",
      "Epoch 3523, Loss: 0.030879086882123374, Final Batch Loss: 0.028377098962664604\n",
      "Epoch 3524, Loss: 0.008051778346271021, Final Batch Loss: 0.0011655687121674418\n",
      "Epoch 3525, Loss: 0.06282569409813732, Final Batch Loss: 0.015768352895975113\n",
      "Epoch 3526, Loss: 0.018591124506201595, Final Batch Loss: 0.016040004789829254\n",
      "Epoch 3527, Loss: 0.02547726791817695, Final Batch Loss: 0.0010392431868240237\n",
      "Epoch 3528, Loss: 0.008982814331830014, Final Batch Loss: 0.00037263520061969757\n",
      "Epoch 3529, Loss: 0.006993750132096466, Final Batch Loss: 9.911896631820127e-05\n",
      "Epoch 3530, Loss: 0.011714375053998083, Final Batch Loss: 0.0001342612667940557\n",
      "Epoch 3531, Loss: 0.0050817706505768, Final Batch Loss: 0.0003106465155724436\n",
      "Epoch 3532, Loss: 0.007087379533913918, Final Batch Loss: 0.00043570794514380395\n",
      "Epoch 3533, Loss: 0.002319114253623411, Final Batch Loss: 3.377944813109934e-05\n",
      "Epoch 3534, Loss: 0.0013130349761922844, Final Batch Loss: 0.0005851910682395101\n",
      "Epoch 3535, Loss: 0.03156196394411381, Final Batch Loss: 0.0004821618495043367\n",
      "Epoch 3536, Loss: 0.0023529579921159893, Final Batch Loss: 0.0006064712652005255\n",
      "Epoch 3537, Loss: 0.034527191979577765, Final Batch Loss: 0.0019037271849811077\n",
      "Epoch 3538, Loss: 0.004785564364283346, Final Batch Loss: 0.00013411794498097152\n",
      "Epoch 3539, Loss: 0.004190787807601737, Final Batch Loss: 0.0007946199038997293\n",
      "Epoch 3540, Loss: 0.002607416826322151, Final Batch Loss: 0.0001355833956040442\n",
      "Epoch 3541, Loss: 0.002365123821164161, Final Batch Loss: 4.433457888808334e-06\n",
      "Epoch 3542, Loss: 0.005130613724759314, Final Batch Loss: 0.00019650922331493348\n",
      "Epoch 3543, Loss: 0.0013513619633158669, Final Batch Loss: 9.916399721987545e-05\n",
      "Epoch 3544, Loss: 0.0015393379799206741, Final Batch Loss: 0.0002749616396613419\n",
      "Epoch 3545, Loss: 0.0009546293640596559, Final Batch Loss: 0.00011160932626808062\n",
      "Epoch 3546, Loss: 0.00405705084267538, Final Batch Loss: 6.063671025913209e-05\n",
      "Epoch 3547, Loss: 0.001692865502263885, Final Batch Loss: 0.0003153833677060902\n",
      "Epoch 3548, Loss: 0.01855120343680028, Final Batch Loss: 1.876870373962447e-05\n",
      "Epoch 3549, Loss: 0.0018493444549676497, Final Batch Loss: 0.0005639360751956701\n",
      "Epoch 3550, Loss: 0.005716211187973386, Final Batch Loss: 0.004107771907001734\n",
      "Epoch 3551, Loss: 0.0022120275934867095, Final Batch Loss: 0.001114049693569541\n",
      "Epoch 3552, Loss: 0.0019251334779255558, Final Batch Loss: 0.0001090256409952417\n",
      "Epoch 3553, Loss: 0.015387023118819343, Final Batch Loss: 9.096093708649278e-05\n",
      "Epoch 3554, Loss: 0.001997825711441692, Final Batch Loss: 0.00011168271157657728\n",
      "Epoch 3555, Loss: 0.0021615205878333654, Final Batch Loss: 0.00010891089186770841\n",
      "Epoch 3556, Loss: 0.0019373370851099025, Final Batch Loss: 0.00011816443293355405\n",
      "Epoch 3557, Loss: 0.0028717884415527806, Final Batch Loss: 0.000189126018085517\n",
      "Epoch 3558, Loss: 0.001055725851074385, Final Batch Loss: 0.000496226828545332\n",
      "Epoch 3559, Loss: 0.0024295546900248155, Final Batch Loss: 0.00021874153753742576\n",
      "Epoch 3560, Loss: 0.0017613111576793017, Final Batch Loss: 2.011735523410607e-05\n",
      "Epoch 3561, Loss: 0.004607913069776259, Final Batch Loss: 1.6639765817672014e-05\n",
      "Epoch 3562, Loss: 0.002301131726198946, Final Batch Loss: 1.6280397176160477e-05\n",
      "Epoch 3563, Loss: 0.0005093541458336404, Final Batch Loss: 0.00014652669779025018\n",
      "Epoch 3564, Loss: 0.0010654904435796198, Final Batch Loss: 0.00011823073145933449\n",
      "Epoch 3565, Loss: 0.0015834676887607202, Final Batch Loss: 0.00035407906398177147\n",
      "Epoch 3566, Loss: 0.002969558354379842, Final Batch Loss: 2.5799450668273494e-05\n",
      "Epoch 3567, Loss: 0.0005286549530865159, Final Batch Loss: 5.261222395347431e-05\n",
      "Epoch 3568, Loss: 0.0009887833912216593, Final Batch Loss: 0.0004924460081383586\n",
      "Epoch 3569, Loss: 0.007963395713886712, Final Batch Loss: 0.00015485784388147295\n",
      "Epoch 3570, Loss: 0.0050819281677831896, Final Batch Loss: 0.004907791968435049\n",
      "Epoch 3571, Loss: 0.002039710907411063, Final Batch Loss: 0.0017035664059221745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3572, Loss: 0.000603391543336329, Final Batch Loss: 0.00012641071225516498\n",
      "Epoch 3573, Loss: 0.026360644438682357, Final Batch Loss: 0.021634839475154877\n",
      "Epoch 3574, Loss: 0.0025474187277723104, Final Batch Loss: 0.00024206841771956533\n",
      "Epoch 3575, Loss: 0.01754450066073332, Final Batch Loss: 0.0003304221900179982\n",
      "Epoch 3576, Loss: 0.0025065729114430724, Final Batch Loss: 5.768076880485751e-05\n",
      "Epoch 3577, Loss: 0.028709869577141944, Final Batch Loss: 0.022686604410409927\n",
      "Epoch 3578, Loss: 0.0010340116004954325, Final Batch Loss: 3.0062979931244627e-05\n",
      "Epoch 3579, Loss: 0.013499997878170689, Final Batch Loss: 0.0041455430909991264\n",
      "Epoch 3580, Loss: 0.020294338650273858, Final Batch Loss: 0.00013494394079316407\n",
      "Epoch 3581, Loss: 0.006831838400103152, Final Batch Loss: 7.069403363857418e-05\n",
      "Epoch 3582, Loss: 0.0020809023990295827, Final Batch Loss: 5.090388731332496e-05\n",
      "Epoch 3583, Loss: 0.01823026524107263, Final Batch Loss: 8.217485446948558e-05\n",
      "Epoch 3584, Loss: 0.01532584335654974, Final Batch Loss: 0.0076241870410740376\n",
      "Epoch 3585, Loss: 0.006096001154219266, Final Batch Loss: 0.004145862068980932\n",
      "Epoch 3586, Loss: 0.02173309446516214, Final Batch Loss: 0.0002006993308896199\n",
      "Epoch 3587, Loss: 0.0037896170961175812, Final Batch Loss: 0.0008900808752514422\n",
      "Epoch 3588, Loss: 0.0072751976509835, Final Batch Loss: 1.5505025885431678e-06\n",
      "Epoch 3589, Loss: 0.008747800908167847, Final Batch Loss: 0.000853752891998738\n",
      "Epoch 3590, Loss: 0.00326610201591393, Final Batch Loss: 8.432563481619582e-05\n",
      "Epoch 3591, Loss: 0.00545441557187587, Final Batch Loss: 0.00025070394622161984\n",
      "Epoch 3592, Loss: 0.0061660548108193325, Final Batch Loss: 7.854521754779853e-06\n",
      "Epoch 3593, Loss: 0.0027755761693697423, Final Batch Loss: 0.00011148572230013087\n",
      "Epoch 3594, Loss: 0.0020393757076817565, Final Batch Loss: 0.00019841003813780844\n",
      "Epoch 3595, Loss: 0.001747958202031441, Final Batch Loss: 7.146199641283602e-05\n",
      "Epoch 3596, Loss: 0.0009618094736651983, Final Batch Loss: 0.0002174812980229035\n",
      "Epoch 3597, Loss: 0.002431589642583276, Final Batch Loss: 1.9927900211769156e-05\n",
      "Epoch 3598, Loss: 0.04702574938710313, Final Batch Loss: 0.00025029986863955855\n",
      "Epoch 3599, Loss: 0.003658130648545921, Final Batch Loss: 0.0008095657103694975\n",
      "Epoch 3600, Loss: 0.0048008211379055865, Final Batch Loss: 0.0001206359374918975\n",
      "Epoch 3601, Loss: 0.0018890017527155578, Final Batch Loss: 0.0001786406064638868\n",
      "Epoch 3602, Loss: 0.0071052504717954434, Final Batch Loss: 5.508827598532662e-05\n",
      "Epoch 3603, Loss: 0.0016413597768405452, Final Batch Loss: 0.0004583070403896272\n",
      "Epoch 3604, Loss: 0.001205395587021485, Final Batch Loss: 0.0002278535976074636\n",
      "Epoch 3605, Loss: 0.00495186893385835, Final Batch Loss: 0.0015296773053705692\n",
      "Epoch 3606, Loss: 0.0011153251143696252, Final Batch Loss: 4.4173808419145644e-05\n",
      "Epoch 3607, Loss: 0.0014389971911441535, Final Batch Loss: 9.026216866914183e-05\n",
      "Epoch 3608, Loss: 0.0014751969993085368, Final Batch Loss: 1.9889570467057638e-05\n",
      "Epoch 3609, Loss: 0.0029830620042048395, Final Batch Loss: 0.0018417845712974668\n",
      "Epoch 3610, Loss: 0.0031017604560474865, Final Batch Loss: 0.0024669382255524397\n",
      "Epoch 3611, Loss: 0.0006906297094246838, Final Batch Loss: 0.00019863118359353393\n",
      "Epoch 3612, Loss: 0.002231815195045783, Final Batch Loss: 0.00016727518232073635\n",
      "Epoch 3613, Loss: 0.0014062325863051228, Final Batch Loss: 7.543610263383016e-05\n",
      "Epoch 3614, Loss: 0.002193688000261318, Final Batch Loss: 2.144066820619628e-05\n",
      "Epoch 3615, Loss: 0.0020221016129653435, Final Batch Loss: 0.0015724461991339922\n",
      "Epoch 3616, Loss: 0.0005948994130449137, Final Batch Loss: 2.156301161448937e-05\n",
      "Epoch 3617, Loss: 0.0022680444781144615, Final Batch Loss: 0.00046302995178848505\n",
      "Epoch 3618, Loss: 0.0004453102337720338, Final Batch Loss: 4.949697176925838e-05\n",
      "Epoch 3619, Loss: 0.00039702790809315047, Final Batch Loss: 5.253874769550748e-05\n",
      "Epoch 3620, Loss: 0.0004733742243843153, Final Batch Loss: 2.032703378063161e-05\n",
      "Epoch 3621, Loss: 0.016686095401382772, Final Batch Loss: 6.260694499360397e-05\n",
      "Epoch 3622, Loss: 0.06623576720812707, Final Batch Loss: 0.06359314918518066\n",
      "Epoch 3623, Loss: 0.0028575877477123868, Final Batch Loss: 2.3428638087352738e-05\n",
      "Epoch 3624, Loss: 0.000464840584754711, Final Batch Loss: 4.208508471492678e-05\n",
      "Epoch 3625, Loss: 0.04433096689535887, Final Batch Loss: 0.04338696971535683\n",
      "Epoch 3626, Loss: 0.03594716600491665, Final Batch Loss: 0.000189318903721869\n",
      "Epoch 3627, Loss: 0.009063877674634568, Final Batch Loss: 0.0009210407733917236\n",
      "Epoch 3628, Loss: 0.004765006353409262, Final Batch Loss: 3.217249832232483e-05\n",
      "Epoch 3629, Loss: 0.009687919369753217, Final Batch Loss: 5.3739346185466275e-05\n",
      "Epoch 3630, Loss: 0.0026043773468700238, Final Batch Loss: 5.4936295782681555e-05\n",
      "Epoch 3631, Loss: 0.021610397889162414, Final Batch Loss: 0.01819446124136448\n",
      "Epoch 3632, Loss: 0.005164696980500594, Final Batch Loss: 0.00024137823493219912\n",
      "Epoch 3633, Loss: 0.020976017251086887, Final Batch Loss: 6.887350900797173e-05\n",
      "Epoch 3634, Loss: 0.00667715456802398, Final Batch Loss: 0.0005453388439491391\n",
      "Epoch 3635, Loss: 0.04105849463667255, Final Batch Loss: 0.0002608716313261539\n",
      "Epoch 3636, Loss: 0.012871905317297205, Final Batch Loss: 0.0005460891406983137\n",
      "Epoch 3637, Loss: 0.03162187667294347, Final Batch Loss: 0.017059843987226486\n",
      "Epoch 3638, Loss: 0.009539088845485821, Final Batch Loss: 0.0019588316790759563\n",
      "Epoch 3639, Loss: 0.014659050917543937, Final Batch Loss: 0.011922730132937431\n",
      "Epoch 3640, Loss: 0.0026025232873507775, Final Batch Loss: 0.00030628067906945944\n",
      "Epoch 3641, Loss: 0.006466902283136733, Final Batch Loss: 0.0001666245370870456\n",
      "Epoch 3642, Loss: 0.007407751654682215, Final Batch Loss: 0.00021160754840821028\n",
      "Epoch 3643, Loss: 0.02348161917507241, Final Batch Loss: 0.00015156532754190266\n",
      "Epoch 3644, Loss: 0.05430655769305304, Final Batch Loss: 0.02414141781628132\n",
      "Epoch 3645, Loss: 0.004073078946021269, Final Batch Loss: 0.00031733899959363043\n",
      "Epoch 3646, Loss: 0.02460663826423115, Final Batch Loss: 0.00042810692684724927\n",
      "Epoch 3647, Loss: 0.03965310269768452, Final Batch Loss: 7.135420219128719e-06\n",
      "Epoch 3648, Loss: 0.004613817942299647, Final Batch Loss: 0.00031430416856892407\n",
      "Epoch 3649, Loss: 0.001631892062505358, Final Batch Loss: 0.0001084022514987737\n",
      "Epoch 3650, Loss: 0.0076075103497714736, Final Batch Loss: 0.00010440501500852406\n",
      "Epoch 3651, Loss: 0.017928591325471643, Final Batch Loss: 0.01575538143515587\n",
      "Epoch 3652, Loss: 0.0018499691541364882, Final Batch Loss: 0.0001488592242822051\n",
      "Epoch 3653, Loss: 0.0028684015123872086, Final Batch Loss: 0.0004946273402310908\n",
      "Epoch 3654, Loss: 0.0038996657385723665, Final Batch Loss: 8.369686838705093e-05\n",
      "Epoch 3655, Loss: 0.0028334480193734635, Final Batch Loss: 2.854917329386808e-05\n",
      "Epoch 3656, Loss: 0.002051600669801701, Final Batch Loss: 8.300791523652151e-05\n",
      "Epoch 3657, Loss: 0.0010147753764613299, Final Batch Loss: 0.0001260125427506864\n",
      "Epoch 3658, Loss: 0.005177218656172045, Final Batch Loss: 0.004300710745155811\n",
      "Epoch 3659, Loss: 0.0007234245204017498, Final Batch Loss: 0.00019138109928462654\n",
      "Epoch 3660, Loss: 0.00913155854868819, Final Batch Loss: 0.0001986149000003934\n",
      "Epoch 3661, Loss: 0.002434571179037448, Final Batch Loss: 0.0005954639054834843\n",
      "Epoch 3662, Loss: 0.008298999870021362, Final Batch Loss: 0.00521913543343544\n",
      "Epoch 3663, Loss: 0.004622172185918316, Final Batch Loss: 0.00028340014978311956\n",
      "Epoch 3664, Loss: 0.001829925153288059, Final Batch Loss: 9.770867472980171e-05\n",
      "Epoch 3665, Loss: 0.0028484590802690946, Final Batch Loss: 0.0013243909925222397\n",
      "Epoch 3666, Loss: 0.0047634342136007035, Final Batch Loss: 2.9779321266687475e-05\n",
      "Epoch 3667, Loss: 0.0008740336388655123, Final Batch Loss: 9.493979632679839e-06\n",
      "Epoch 3668, Loss: 0.013299005091539584, Final Batch Loss: 0.011519785039126873\n",
      "Epoch 3669, Loss: 0.002479839837178588, Final Batch Loss: 0.000431878084782511\n",
      "Epoch 3670, Loss: 0.027614209277089685, Final Batch Loss: 0.0003072581603191793\n",
      "Epoch 3671, Loss: 0.0026488705188967288, Final Batch Loss: 0.0002179362636525184\n",
      "Epoch 3672, Loss: 0.002218892836026498, Final Batch Loss: 0.00021313935576472431\n",
      "Epoch 3673, Loss: 0.005024485893954989, Final Batch Loss: 0.002907728310674429\n",
      "Epoch 3674, Loss: 0.0036381667014211416, Final Batch Loss: 7.899019692558795e-06\n",
      "Epoch 3675, Loss: 0.0012553223823488224, Final Batch Loss: 0.00012108188821002841\n",
      "Epoch 3676, Loss: 0.0033417173253837973, Final Batch Loss: 0.001480649458244443\n",
      "Epoch 3677, Loss: 0.0028134355452493764, Final Batch Loss: 6.676800694549456e-05\n",
      "Epoch 3678, Loss: 0.0048505433442187496, Final Batch Loss: 0.0002445798018015921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3679, Loss: 0.0007553903815278318, Final Batch Loss: 4.233864092384465e-05\n",
      "Epoch 3680, Loss: 0.0008441630125162192, Final Batch Loss: 0.00013116021000314504\n",
      "Epoch 3681, Loss: 0.056831418827641755, Final Batch Loss: 0.0005790159339085221\n",
      "Epoch 3682, Loss: 0.005740420267102309, Final Batch Loss: 3.3140910090878606e-05\n",
      "Epoch 3683, Loss: 0.002747530403212295, Final Batch Loss: 0.00017813764861784875\n",
      "Epoch 3684, Loss: 0.0038719846634194255, Final Batch Loss: 0.0025806871708482504\n",
      "Epoch 3685, Loss: 0.009428856799786445, Final Batch Loss: 0.0004438442992977798\n",
      "Epoch 3686, Loss: 0.005962098599411547, Final Batch Loss: 0.00026628145133145154\n",
      "Epoch 3687, Loss: 0.008917166898754658, Final Batch Loss: 0.0007375688292086124\n",
      "Epoch 3688, Loss: 0.0017338090001430828, Final Batch Loss: 0.0008669837843626738\n",
      "Epoch 3689, Loss: 0.005509400798473507, Final Batch Loss: 0.0003461519081611186\n",
      "Epoch 3690, Loss: 0.023809808975784108, Final Batch Loss: 4.7786859795451164e-05\n",
      "Epoch 3691, Loss: 0.017992951623455156, Final Batch Loss: 1.9091654394287616e-05\n",
      "Epoch 3692, Loss: 0.0022471186603070237, Final Batch Loss: 0.00011341147910570726\n",
      "Epoch 3693, Loss: 0.0029197056937846355, Final Batch Loss: 2.337135811103508e-05\n",
      "Epoch 3694, Loss: 0.0019616399658843875, Final Batch Loss: 0.00026894526672549546\n",
      "Epoch 3695, Loss: 0.0027696762699633837, Final Batch Loss: 0.0015398338437080383\n",
      "Epoch 3696, Loss: 0.014754340367289842, Final Batch Loss: 2.2830401576356962e-05\n",
      "Epoch 3697, Loss: 0.03936699908081209, Final Batch Loss: 0.00024028068582993\n",
      "Epoch 3698, Loss: 0.006896882261571591, Final Batch Loss: 2.273272730235476e-05\n",
      "Epoch 3699, Loss: 0.001647775832680054, Final Batch Loss: 6.706137355649844e-05\n",
      "Epoch 3700, Loss: 0.003536527756295982, Final Batch Loss: 1.980023444048129e-05\n",
      "Epoch 3701, Loss: 0.0013273435470182449, Final Batch Loss: 0.00010857591405510902\n",
      "Epoch 3702, Loss: 0.002315747202374041, Final Batch Loss: 0.0009211340802721679\n",
      "Epoch 3703, Loss: 0.0018880813222494908, Final Batch Loss: 0.0006164473597891629\n",
      "Epoch 3704, Loss: 0.00265204107563477, Final Batch Loss: 0.00025524350348860025\n",
      "Epoch 3705, Loss: 0.002659135485373554, Final Batch Loss: 0.00015847895701881498\n",
      "Epoch 3706, Loss: 0.0016939849883783609, Final Batch Loss: 9.010839858092368e-05\n",
      "Epoch 3707, Loss: 0.0066371021894156, Final Batch Loss: 0.0001152018885477446\n",
      "Epoch 3708, Loss: 0.0036530212641991966, Final Batch Loss: 0.0014368747361004353\n",
      "Epoch 3709, Loss: 0.000527202302691876, Final Batch Loss: 6.442462108680047e-06\n",
      "Epoch 3710, Loss: 0.006569307453901274, Final Batch Loss: 0.0035313742700964212\n",
      "Epoch 3711, Loss: 0.00455990592672606, Final Batch Loss: 0.00018586921214591712\n",
      "Epoch 3712, Loss: 0.0007708835782977985, Final Batch Loss: 0.0005190839874558151\n",
      "Epoch 3713, Loss: 0.0046322604707711434, Final Batch Loss: 3.7427871575346217e-06\n",
      "Epoch 3714, Loss: 0.0041039779048333, Final Batch Loss: 0.0005317918257787824\n",
      "Epoch 3715, Loss: 0.0018912595760411932, Final Batch Loss: 9.01168732525548e-06\n",
      "Epoch 3716, Loss: 0.005815236494527198, Final Batch Loss: 0.0009543247288092971\n",
      "Epoch 3717, Loss: 0.0031718676277705526, Final Batch Loss: 0.00015210165292955935\n",
      "Epoch 3718, Loss: 0.007085038235345564, Final Batch Loss: 0.0004000274930149317\n",
      "Epoch 3719, Loss: 0.011572946856176713, Final Batch Loss: 3.514508716762066e-05\n",
      "Epoch 3720, Loss: 0.002777841604256537, Final Batch Loss: 4.648787580663338e-05\n",
      "Epoch 3721, Loss: 0.005990323767036898, Final Batch Loss: 8.961998537415639e-05\n",
      "Epoch 3722, Loss: 0.0011798322484537493, Final Batch Loss: 4.2419094825163484e-05\n",
      "Epoch 3723, Loss: 0.0058575431612553075, Final Batch Loss: 0.002747835824266076\n",
      "Epoch 3724, Loss: 0.003005581682373304, Final Batch Loss: 4.777116555487737e-05\n",
      "Epoch 3725, Loss: 0.0007909998275863472, Final Batch Loss: 0.00016743183368816972\n",
      "Epoch 3726, Loss: 0.005046291051257867, Final Batch Loss: 0.0025067829992622137\n",
      "Epoch 3727, Loss: 0.001381746173137799, Final Batch Loss: 0.0007838302408345044\n",
      "Epoch 3728, Loss: 0.0022168291070556734, Final Batch Loss: 4.9981263146037236e-05\n",
      "Epoch 3729, Loss: 0.0007376691492027021, Final Batch Loss: 0.00036467460449784994\n",
      "Epoch 3730, Loss: 0.00919368936047249, Final Batch Loss: 0.00020256427524145693\n",
      "Epoch 3731, Loss: 0.000593838507484179, Final Batch Loss: 0.0001423426001565531\n",
      "Epoch 3732, Loss: 0.001923996038385667, Final Batch Loss: 6.203523662406951e-05\n",
      "Epoch 3733, Loss: 0.0031223473220052256, Final Batch Loss: 4.509435257205041e-06\n",
      "Epoch 3734, Loss: 0.001232924758369336, Final Batch Loss: 5.091094863018952e-05\n",
      "Epoch 3735, Loss: 0.0008871457594068488, Final Batch Loss: 0.00018216166063211858\n",
      "Epoch 3736, Loss: 0.0011111813612387778, Final Batch Loss: 3.270156639700872e-06\n",
      "Epoch 3737, Loss: 0.0006782537657272769, Final Batch Loss: 5.3122712415643036e-05\n",
      "Epoch 3738, Loss: 0.002477147394529311, Final Batch Loss: 1.86401121027302e-05\n",
      "Epoch 3739, Loss: 0.0015524923874181695, Final Batch Loss: 0.0002172384993173182\n",
      "Epoch 3740, Loss: 0.004558126483971137, Final Batch Loss: 4.786894714925438e-05\n",
      "Epoch 3741, Loss: 0.0007489889540011063, Final Batch Loss: 0.0001254743110621348\n",
      "Epoch 3742, Loss: 0.006263377155846683, Final Batch Loss: 0.00015338367666117847\n",
      "Epoch 3743, Loss: 0.0006097028563090134, Final Batch Loss: 5.692457489203662e-05\n",
      "Epoch 3744, Loss: 0.007886698713264195, Final Batch Loss: 0.006068491842597723\n",
      "Epoch 3745, Loss: 0.0033656260347925127, Final Batch Loss: 0.0009416395332664251\n",
      "Epoch 3746, Loss: 0.0014428310860239435, Final Batch Loss: 7.359433948295191e-05\n",
      "Epoch 3747, Loss: 0.013425327098957496, Final Batch Loss: 0.0003066313802264631\n",
      "Epoch 3748, Loss: 0.00463243243029865, Final Batch Loss: 0.003976504784077406\n",
      "Epoch 3749, Loss: 0.0029285773089213762, Final Batch Loss: 2.2273863578448072e-05\n",
      "Epoch 3750, Loss: 0.0015359395983978175, Final Batch Loss: 6.86266430420801e-05\n",
      "Epoch 3751, Loss: 0.005281303132505855, Final Batch Loss: 1.6669069736963138e-05\n",
      "Epoch 3752, Loss: 0.0008690096783539047, Final Batch Loss: 3.263670078013092e-05\n",
      "Epoch 3753, Loss: 0.000524197137565352, Final Batch Loss: 3.2379386539105326e-05\n",
      "Epoch 3754, Loss: 0.0033554513574927114, Final Batch Loss: 5.2545285143423826e-05\n",
      "Epoch 3755, Loss: 0.001454529192415066, Final Batch Loss: 3.528991510393098e-05\n",
      "Epoch 3756, Loss: 0.0008316292041854467, Final Batch Loss: 0.00033724322565831244\n",
      "Epoch 3757, Loss: 0.0009309885354014114, Final Batch Loss: 0.0003806706808973104\n",
      "Epoch 3758, Loss: 0.002249897188448813, Final Batch Loss: 4.963998799212277e-05\n",
      "Epoch 3759, Loss: 0.0015374592367152218, Final Batch Loss: 0.0003442612069193274\n",
      "Epoch 3760, Loss: 0.0005539678495551925, Final Batch Loss: 0.00016868275997694582\n",
      "Epoch 3761, Loss: 0.001434599602362141, Final Batch Loss: 6.726278661517426e-05\n",
      "Epoch 3762, Loss: 0.012115984209231101, Final Batch Loss: 0.002528424607589841\n",
      "Epoch 3763, Loss: 0.0022550310168298893, Final Batch Loss: 1.2555807188618928e-05\n",
      "Epoch 3764, Loss: 0.004324896122852806, Final Batch Loss: 0.0033497698605060577\n",
      "Epoch 3765, Loss: 0.0005047577706136508, Final Batch Loss: 0.0002311521238880232\n",
      "Epoch 3766, Loss: 0.002742640113865491, Final Batch Loss: 6.752069020876661e-05\n",
      "Epoch 3767, Loss: 0.0014786177835048875, Final Batch Loss: 5.653993866872042e-06\n",
      "Epoch 3768, Loss: 0.0005972187568659137, Final Batch Loss: 4.352826636022655e-06\n",
      "Epoch 3769, Loss: 8.093340579762298e-05, Final Batch Loss: 1.9640044683910673e-06\n",
      "Epoch 3770, Loss: 0.0005709515389753506, Final Batch Loss: 0.00023902762040961534\n",
      "Epoch 3771, Loss: 0.0016394534650316928, Final Batch Loss: 0.0014195706462487578\n",
      "Epoch 3772, Loss: 0.00010747241412900621, Final Batch Loss: 4.6300599933601916e-05\n",
      "Epoch 3773, Loss: 0.0017127097007687553, Final Batch Loss: 4.60636420029914e-06\n",
      "Epoch 3774, Loss: 0.0022137544069664727, Final Batch Loss: 7.88307297625579e-05\n",
      "Epoch 3775, Loss: 0.0006394251595338574, Final Batch Loss: 3.1808252970222384e-05\n",
      "Epoch 3776, Loss: 0.0007062665399644175, Final Batch Loss: 5.230900569586083e-05\n",
      "Epoch 3777, Loss: 0.0003273967267887201, Final Batch Loss: 0.0001527861604699865\n",
      "Epoch 3778, Loss: 0.001149356787209399, Final Batch Loss: 0.00024729271535761654\n",
      "Epoch 3779, Loss: 0.0007564358384115621, Final Batch Loss: 3.705266863107681e-05\n",
      "Epoch 3780, Loss: 0.0007688927307754057, Final Batch Loss: 8.034632628550753e-05\n",
      "Epoch 3781, Loss: 0.00019203375404686085, Final Batch Loss: 3.046163146791514e-05\n",
      "Epoch 3782, Loss: 0.00027941585085500265, Final Batch Loss: 7.895037924754433e-06\n",
      "Epoch 3783, Loss: 0.004051647851156304, Final Batch Loss: 0.0002712446148507297\n",
      "Epoch 3784, Loss: 0.00047843110496614827, Final Batch Loss: 0.00016448595852125436\n",
      "Epoch 3785, Loss: 0.0020625423267119913, Final Batch Loss: 0.0009034181712195277\n",
      "Epoch 3786, Loss: 0.0021822008857270703, Final Batch Loss: 0.0010927121620625257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3787, Loss: 0.0009079939627554268, Final Batch Loss: 2.595410478534177e-05\n",
      "Epoch 3788, Loss: 0.000263642446952872, Final Batch Loss: 2.4422037313343026e-05\n",
      "Epoch 3789, Loss: 0.0010404755485069472, Final Batch Loss: 0.00031304502044804394\n",
      "Epoch 3790, Loss: 0.0021104002626088914, Final Batch Loss: 1.7510505131212994e-05\n",
      "Epoch 3791, Loss: 0.0025382348085258855, Final Batch Loss: 0.0017629713984206319\n",
      "Epoch 3792, Loss: 0.0018038206007986446, Final Batch Loss: 8.642581633466762e-06\n",
      "Epoch 3793, Loss: 0.0010462491711678013, Final Batch Loss: 1.3721139566769125e-06\n",
      "Epoch 3794, Loss: 0.004668949433835223, Final Batch Loss: 3.863517122226767e-05\n",
      "Epoch 3795, Loss: 0.0003438399407968973, Final Batch Loss: 1.5696397895226255e-05\n",
      "Epoch 3796, Loss: 0.003721414656865818, Final Batch Loss: 4.109816472919192e-06\n",
      "Epoch 3797, Loss: 0.005045327226980589, Final Batch Loss: 0.000671035551931709\n",
      "Epoch 3798, Loss: 0.0006842015682195779, Final Batch Loss: 0.0001438591571059078\n",
      "Epoch 3799, Loss: 0.0164124505381551, Final Batch Loss: 3.48559879057575e-05\n",
      "Epoch 3800, Loss: 0.0005172616220079362, Final Batch Loss: 0.00028608745196834207\n",
      "Epoch 3801, Loss: 0.0023263045150088146, Final Batch Loss: 0.00012259311915840954\n",
      "Epoch 3802, Loss: 0.002133406566827034, Final Batch Loss: 0.0006086380453780293\n",
      "Epoch 3803, Loss: 0.000713492715931352, Final Batch Loss: 6.245741587918019e-06\n",
      "Epoch 3804, Loss: 0.0002664939565875102, Final Batch Loss: 3.683215254568495e-05\n",
      "Epoch 3805, Loss: 0.003144190684906789, Final Batch Loss: 0.0024501350708305836\n",
      "Epoch 3806, Loss: 0.0013953885545561207, Final Batch Loss: 0.0011152997612953186\n",
      "Epoch 3807, Loss: 0.0024365141089219833, Final Batch Loss: 0.000490875041577965\n",
      "Epoch 3808, Loss: 0.005063551787372944, Final Batch Loss: 7.119525093912671e-07\n",
      "Epoch 3809, Loss: 0.0010908543099503731, Final Batch Loss: 1.992419311136473e-05\n",
      "Epoch 3810, Loss: 0.000747896650864277, Final Batch Loss: 8.13081132946536e-05\n",
      "Epoch 3811, Loss: 0.003972498834627913, Final Batch Loss: 0.003641149029135704\n",
      "Epoch 3812, Loss: 0.008124001342707743, Final Batch Loss: 1.6096640820251196e-06\n",
      "Epoch 3813, Loss: 0.004583007238807113, Final Batch Loss: 1.6524475086043822e-06\n",
      "Epoch 3814, Loss: 0.0010340529552195221, Final Batch Loss: 0.00036935487878508866\n",
      "Epoch 3815, Loss: 0.04912182849511737, Final Batch Loss: 0.04336732253432274\n",
      "Epoch 3816, Loss: 0.00040271112447953783, Final Batch Loss: 7.542330422438681e-06\n",
      "Epoch 3817, Loss: 0.019990465938462876, Final Batch Loss: 0.0018211256247013807\n",
      "Epoch 3818, Loss: 0.052901611910783686, Final Batch Loss: 0.002749975770711899\n",
      "Epoch 3819, Loss: 0.11867696680565132, Final Batch Loss: 0.08381801098585129\n",
      "Epoch 3820, Loss: 0.005752580466833024, Final Batch Loss: 0.0007355212001129985\n",
      "Epoch 3821, Loss: 0.010542160212935414, Final Batch Loss: 3.264683618908748e-05\n",
      "Epoch 3822, Loss: 0.0009984723146772012, Final Batch Loss: 3.494742850307375e-05\n",
      "Epoch 3823, Loss: 0.013013181378482841, Final Batch Loss: 0.00012327982403803617\n",
      "Epoch 3824, Loss: 0.008595262246672064, Final Batch Loss: 0.004300599452108145\n",
      "Epoch 3825, Loss: 0.00491171372050303, Final Batch Loss: 0.0008001780952326953\n",
      "Epoch 3826, Loss: 0.014095613862082246, Final Batch Loss: 0.0002955486997961998\n",
      "Epoch 3827, Loss: 0.00033523949514346896, Final Batch Loss: 8.759753836784512e-05\n",
      "Epoch 3828, Loss: 0.002615020697703585, Final Batch Loss: 0.0008965489105321467\n",
      "Epoch 3829, Loss: 0.005942234260146506, Final Batch Loss: 0.00024258859048131853\n",
      "Epoch 3830, Loss: 0.004052129914271063, Final Batch Loss: 2.153482819267083e-05\n",
      "Epoch 3831, Loss: 0.016397913641412742, Final Batch Loss: 0.0023015651386231184\n",
      "Epoch 3832, Loss: 0.006687387987767579, Final Batch Loss: 0.0005769385606981814\n",
      "Epoch 3833, Loss: 0.004894599285762524, Final Batch Loss: 0.00067268090788275\n",
      "Epoch 3834, Loss: 0.002160196025215555, Final Batch Loss: 4.533336323220283e-05\n",
      "Epoch 3835, Loss: 0.013545732639613561, Final Batch Loss: 0.00021620526968035847\n",
      "Epoch 3836, Loss: 0.03574855119950371, Final Batch Loss: 0.003110148012638092\n",
      "Epoch 3837, Loss: 0.011015498181222938, Final Batch Loss: 3.152174031129107e-05\n",
      "Epoch 3838, Loss: 0.004866359930019826, Final Batch Loss: 0.00346982735209167\n",
      "Epoch 3839, Loss: 0.00790649103873875, Final Batch Loss: 0.0067493366077542305\n",
      "Epoch 3840, Loss: 0.0007258472342073219, Final Batch Loss: 2.3914713892736472e-05\n",
      "Epoch 3841, Loss: 0.015075313684064895, Final Batch Loss: 0.011079325340688229\n",
      "Epoch 3842, Loss: 0.002044520286290208, Final Batch Loss: 0.00025866544456221163\n",
      "Epoch 3843, Loss: 0.0021714636714023072, Final Batch Loss: 7.65131480875425e-05\n",
      "Epoch 3844, Loss: 0.01037130949771381, Final Batch Loss: 0.0073199025355279446\n",
      "Epoch 3845, Loss: 0.0021893805969739333, Final Batch Loss: 9.501393651589751e-05\n",
      "Epoch 3846, Loss: 0.04140402790653752, Final Batch Loss: 0.00010846061923075467\n",
      "Epoch 3847, Loss: 0.002746339450823143, Final Batch Loss: 0.0005937712267041206\n",
      "Epoch 3848, Loss: 0.01927891152445227, Final Batch Loss: 0.00940291490405798\n",
      "Epoch 3849, Loss: 0.0035479788930388168, Final Batch Loss: 0.0015249804127961397\n",
      "Epoch 3850, Loss: 0.004358555859653279, Final Batch Loss: 0.0009529633680358529\n",
      "Epoch 3851, Loss: 0.014748193585546687, Final Batch Loss: 0.0004217903479002416\n",
      "Epoch 3852, Loss: 0.0012989973765797913, Final Batch Loss: 0.00014635980187449604\n",
      "Epoch 3853, Loss: 0.00451760707073845, Final Batch Loss: 0.00018662000366020948\n",
      "Epoch 3854, Loss: 0.01523922910564579, Final Batch Loss: 5.421135574579239e-05\n",
      "Epoch 3855, Loss: 0.0006349305804178584, Final Batch Loss: 0.0001326322089880705\n",
      "Epoch 3856, Loss: 0.003200927545549348, Final Batch Loss: 5.848469299962744e-05\n",
      "Epoch 3857, Loss: 0.004996627270884346, Final Batch Loss: 6.290376768447459e-05\n",
      "Epoch 3858, Loss: 0.011447641598351765, Final Batch Loss: 0.0005161187145859003\n",
      "Epoch 3859, Loss: 0.009052539797266945, Final Batch Loss: 0.002195091685280204\n",
      "Epoch 3860, Loss: 0.004770779996761121, Final Batch Loss: 0.00021355088392738253\n",
      "Epoch 3861, Loss: 0.0028569974238052964, Final Batch Loss: 7.179936073953286e-05\n",
      "Epoch 3862, Loss: 0.002381505626544822, Final Batch Loss: 0.0009611968416720629\n",
      "Epoch 3863, Loss: 0.0014529238214890938, Final Batch Loss: 0.0002256705629406497\n",
      "Epoch 3864, Loss: 0.0015705525329394732, Final Batch Loss: 2.9493508918676525e-05\n",
      "Epoch 3865, Loss: 0.0010112338059116155, Final Batch Loss: 0.0005974018131382763\n",
      "Epoch 3866, Loss: 0.0010801759326568572, Final Batch Loss: 2.2565260223927908e-05\n",
      "Epoch 3867, Loss: 0.001818421071220655, Final Batch Loss: 0.0003911321400664747\n",
      "Epoch 3868, Loss: 0.0006307492421910865, Final Batch Loss: 1.3686576494365e-05\n",
      "Epoch 3869, Loss: 0.0008695451579114888, Final Batch Loss: 0.00013171849423088133\n",
      "Epoch 3870, Loss: 0.004108560548047535, Final Batch Loss: 0.0001300056610489264\n",
      "Epoch 3871, Loss: 0.0009335702998214401, Final Batch Loss: 0.00014221976744011045\n",
      "Epoch 3872, Loss: 0.0021656769968103617, Final Batch Loss: 0.00022718995751347393\n",
      "Epoch 3873, Loss: 0.0015913913885015063, Final Batch Loss: 0.0010043149814009666\n",
      "Epoch 3874, Loss: 0.0006657136054855073, Final Batch Loss: 1.7614622265682556e-05\n",
      "Epoch 3875, Loss: 0.0007109049984137528, Final Batch Loss: 5.3083786042407155e-06\n",
      "Epoch 3876, Loss: 0.0013958885792817455, Final Batch Loss: 2.0522911654552445e-05\n",
      "Epoch 3877, Loss: 0.0018153917999370606, Final Batch Loss: 8.135335519909859e-05\n",
      "Epoch 3878, Loss: 0.0008648459188407287, Final Batch Loss: 0.00030684733064845204\n",
      "Epoch 3879, Loss: 0.002404618338914588, Final Batch Loss: 0.00033560776500962675\n",
      "Epoch 3880, Loss: 0.0024683260307938326, Final Batch Loss: 0.00017683787154965103\n",
      "Epoch 3881, Loss: 0.0019201118520868476, Final Batch Loss: 0.00018968383665196598\n",
      "Epoch 3882, Loss: 0.0023112485841920716, Final Batch Loss: 0.000431929889600724\n",
      "Epoch 3883, Loss: 0.00020403318012540694, Final Batch Loss: 3.5996206861454993e-05\n",
      "Epoch 3884, Loss: 0.005413638205936877, Final Batch Loss: 0.00015806846204213798\n",
      "Epoch 3885, Loss: 0.011476575706183212, Final Batch Loss: 0.010458141565322876\n",
      "Epoch 3886, Loss: 0.009749177086632699, Final Batch Loss: 0.008100652135908604\n",
      "Epoch 3887, Loss: 0.0004965757634636248, Final Batch Loss: 9.41153266467154e-05\n",
      "Epoch 3888, Loss: 0.020745059198816307, Final Batch Loss: 0.00037794013042002916\n",
      "Epoch 3889, Loss: 0.0024521333252778277, Final Batch Loss: 0.00037485899520106614\n",
      "Epoch 3890, Loss: 0.00043494887631823076, Final Batch Loss: 6.914243385836016e-06\n",
      "Epoch 3891, Loss: 0.038829025346785784, Final Batch Loss: 0.0011212170356884599\n",
      "Epoch 3892, Loss: 0.0037927563389530405, Final Batch Loss: 0.00018817081581801176\n",
      "Epoch 3893, Loss: 0.0027169116438017227, Final Batch Loss: 0.00017387339903507382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3894, Loss: 0.004806382610695437, Final Batch Loss: 0.00036931131035089493\n",
      "Epoch 3895, Loss: 0.008806480577391085, Final Batch Loss: 1.227412781190651e-06\n",
      "Epoch 3896, Loss: 0.002526137432141695, Final Batch Loss: 0.0006398817640729249\n",
      "Epoch 3897, Loss: 0.0019407127510930877, Final Batch Loss: 0.0004377446894068271\n",
      "Epoch 3898, Loss: 0.01672175934072584, Final Batch Loss: 0.008093603886663914\n",
      "Epoch 3899, Loss: 0.0034390027030895, Final Batch Loss: 0.00015550022362731397\n",
      "Epoch 3900, Loss: 0.0005979264751658775, Final Batch Loss: 9.268563735531643e-05\n",
      "Epoch 3901, Loss: 0.020491700466664042, Final Batch Loss: 0.00017932962509803474\n",
      "Epoch 3902, Loss: 0.059373220454290276, Final Batch Loss: 3.9894173823995516e-05\n",
      "Epoch 3903, Loss: 0.002367081727470577, Final Batch Loss: 2.9536852252931567e-06\n",
      "Epoch 3904, Loss: 0.0018535967865318526, Final Batch Loss: 0.00021132476103957742\n",
      "Epoch 3905, Loss: 0.008127229695674032, Final Batch Loss: 0.0009388144826516509\n",
      "Epoch 3906, Loss: 0.08000689677646733, Final Batch Loss: 0.00012642964429687709\n",
      "Epoch 3907, Loss: 0.003267515961852041, Final Batch Loss: 0.00020847440464422107\n",
      "Epoch 3908, Loss: 0.023565022624097764, Final Batch Loss: 0.0001306429476244375\n",
      "Epoch 3909, Loss: 0.003027544396900339, Final Batch Loss: 5.2592058636946604e-05\n",
      "Epoch 3910, Loss: 0.0011204503971384838, Final Batch Loss: 3.066141653107479e-05\n",
      "Epoch 3911, Loss: 0.0007105648473952897, Final Batch Loss: 0.00024348443548660725\n",
      "Epoch 3912, Loss: 0.0018285635451320559, Final Batch Loss: 0.00028527696849778295\n",
      "Epoch 3913, Loss: 0.006728940330503974, Final Batch Loss: 0.006010178476572037\n",
      "Epoch 3914, Loss: 0.0022931695530132856, Final Batch Loss: 0.000784825999289751\n",
      "Epoch 3915, Loss: 0.008387214795220643, Final Batch Loss: 0.0023935562931001186\n",
      "Epoch 3916, Loss: 0.03835874336073175, Final Batch Loss: 7.406686199828982e-05\n",
      "Epoch 3917, Loss: 0.004807631412404589, Final Batch Loss: 0.003460228443145752\n",
      "Epoch 3918, Loss: 0.017746374949638266, Final Batch Loss: 4.085252294316888e-05\n",
      "Epoch 3919, Loss: 0.015877640202234033, Final Batch Loss: 6.21171566308476e-05\n",
      "Epoch 3920, Loss: 0.020836370065808296, Final Batch Loss: 0.00042713937000371516\n",
      "Epoch 3921, Loss: 0.04983210185309872, Final Batch Loss: 0.043989092111587524\n",
      "Epoch 3922, Loss: 0.0008296034066006541, Final Batch Loss: 9.307408618042246e-05\n",
      "Epoch 3923, Loss: 0.019860692271322478, Final Batch Loss: 9.10777089302428e-05\n",
      "Epoch 3924, Loss: 0.01933803557039937, Final Batch Loss: 0.009106120094656944\n",
      "Epoch 3925, Loss: 0.03851884288997098, Final Batch Loss: 0.006316545885056257\n",
      "Epoch 3926, Loss: 0.0013005838591197971, Final Batch Loss: 4.627470480045304e-05\n",
      "Epoch 3927, Loss: 0.015900214985776984, Final Batch Loss: 1.4504793398373295e-05\n",
      "Epoch 3928, Loss: 0.003233968047425151, Final Batch Loss: 0.0002522697614040226\n",
      "Epoch 3929, Loss: 0.0037201403247308917, Final Batch Loss: 0.0022381851449608803\n",
      "Epoch 3930, Loss: 0.0017037422803696245, Final Batch Loss: 9.882292215479538e-05\n",
      "Epoch 3931, Loss: 0.007855432500946335, Final Batch Loss: 0.006551200989633799\n",
      "Epoch 3932, Loss: 0.0037245556304696947, Final Batch Loss: 0.002833861391991377\n",
      "Epoch 3933, Loss: 0.0013173058059692266, Final Batch Loss: 1.3887914064980578e-05\n",
      "Epoch 3934, Loss: 0.0009824097505770624, Final Batch Loss: 0.0001508805580670014\n",
      "Epoch 3935, Loss: 0.006423044884286355, Final Batch Loss: 0.0026217452250421047\n",
      "Epoch 3936, Loss: 0.0034640172598301433, Final Batch Loss: 0.0021103541366755962\n",
      "Epoch 3937, Loss: 0.0015481815717066638, Final Batch Loss: 0.0002852018515113741\n",
      "Epoch 3938, Loss: 0.0008826815919746878, Final Batch Loss: 5.058705937699415e-06\n",
      "Epoch 3939, Loss: 0.0009083855657081585, Final Batch Loss: 3.379179543117061e-05\n",
      "Epoch 3940, Loss: 0.002490779286745237, Final Batch Loss: 0.0019189395243301988\n",
      "Epoch 3941, Loss: 0.0016173687035916373, Final Batch Loss: 0.00013169307203497738\n",
      "Epoch 3942, Loss: 0.0016740285864216276, Final Batch Loss: 0.0006812395295128226\n",
      "Epoch 3943, Loss: 0.009382142241520341, Final Batch Loss: 3.381238639121875e-05\n",
      "Epoch 3944, Loss: 0.0008148995730152819, Final Batch Loss: 0.0001605235447641462\n",
      "Epoch 3945, Loss: 0.0005911784392083064, Final Batch Loss: 0.00010278791887685657\n",
      "Epoch 3946, Loss: 0.004257778748069541, Final Batch Loss: 3.5469038266455755e-05\n",
      "Epoch 3947, Loss: 0.0006921570002305089, Final Batch Loss: 6.426926847780123e-05\n",
      "Epoch 3948, Loss: 0.06551752428640611, Final Batch Loss: 0.038120023906230927\n",
      "Epoch 3949, Loss: 0.0022165946356835775, Final Batch Loss: 8.269184763776138e-05\n",
      "Epoch 3950, Loss: 0.004171534779743524, Final Batch Loss: 0.0004456510941963643\n",
      "Epoch 3951, Loss: 0.018527481512137456, Final Batch Loss: 0.0008420863887295127\n",
      "Epoch 3952, Loss: 0.0013934069556853501, Final Batch Loss: 1.099640030588489e-05\n",
      "Epoch 3953, Loss: 0.0026169573175138794, Final Batch Loss: 0.00018649506091605872\n",
      "Epoch 3954, Loss: 0.012608117147465236, Final Batch Loss: 0.00019743047596421093\n",
      "Epoch 3955, Loss: 0.004301724242395721, Final Batch Loss: 0.0003468699869699776\n",
      "Epoch 3956, Loss: 0.006582307010830846, Final Batch Loss: 4.9519370804773644e-05\n",
      "Epoch 3957, Loss: 0.004470201471121982, Final Batch Loss: 0.00015296495985239744\n",
      "Epoch 3958, Loss: 0.003033110042451881, Final Batch Loss: 0.00013032605056650937\n",
      "Epoch 3959, Loss: 0.0012906162610306637, Final Batch Loss: 2.051121737167705e-05\n",
      "Epoch 3960, Loss: 0.0019835581551888026, Final Batch Loss: 0.0013172227190807462\n",
      "Epoch 3961, Loss: 0.006956932837056229, Final Batch Loss: 0.0062681944109499454\n",
      "Epoch 3962, Loss: 0.0017593344673514366, Final Batch Loss: 0.00010016580199589953\n",
      "Epoch 3963, Loss: 0.0003507536785036791, Final Batch Loss: 1.4194794857758097e-05\n",
      "Epoch 3964, Loss: 0.0015150342660490423, Final Batch Loss: 0.00011732497659977525\n",
      "Epoch 3965, Loss: 0.008611610246589407, Final Batch Loss: 0.0017070972826331854\n",
      "Epoch 3966, Loss: 0.002145991663383029, Final Batch Loss: 1.332515512331156e-05\n",
      "Epoch 3967, Loss: 0.002982938280183589, Final Batch Loss: 0.00012311206955928355\n",
      "Epoch 3968, Loss: 0.0009946521386154927, Final Batch Loss: 8.562765287933871e-05\n",
      "Epoch 3969, Loss: 0.002018937939283205, Final Batch Loss: 0.00024740281514823437\n",
      "Epoch 3970, Loss: 0.00045748622505925596, Final Batch Loss: 0.00014704777277074754\n",
      "Epoch 3971, Loss: 0.0013937945550424047, Final Batch Loss: 0.00012870656792074442\n",
      "Epoch 3972, Loss: 0.0371188339231594, Final Batch Loss: 0.00010542094241827726\n",
      "Epoch 3973, Loss: 0.030915535491658375, Final Batch Loss: 0.029606562107801437\n",
      "Epoch 3974, Loss: 0.0015352328700828366, Final Batch Loss: 4.9375776143278927e-05\n",
      "Epoch 3975, Loss: 0.04304196443990804, Final Batch Loss: 0.004967267159372568\n",
      "Epoch 3976, Loss: 0.0069798941440240014, Final Batch Loss: 8.088327740551904e-05\n",
      "Epoch 3977, Loss: 0.0063579022735211765, Final Batch Loss: 7.293253293028101e-05\n",
      "Epoch 3978, Loss: 0.0026222426204185467, Final Batch Loss: 4.587177318171598e-05\n",
      "Epoch 3979, Loss: 0.0012440413629519753, Final Batch Loss: 4.644828732125461e-05\n",
      "Epoch 3980, Loss: 0.0007606191138620488, Final Batch Loss: 0.00014475424541160464\n",
      "Epoch 3981, Loss: 0.0005557612967095338, Final Batch Loss: 9.886630141409114e-05\n",
      "Epoch 3982, Loss: 0.013240280739410082, Final Batch Loss: 0.012550607323646545\n",
      "Epoch 3983, Loss: 0.014949888907722197, Final Batch Loss: 0.0002577213745098561\n",
      "Epoch 3984, Loss: 0.04631905208225362, Final Batch Loss: 0.00644638342782855\n",
      "Epoch 3985, Loss: 0.025883708556648344, Final Batch Loss: 0.023951632902026176\n",
      "Epoch 3986, Loss: 0.027675263889250346, Final Batch Loss: 0.0001604963035788387\n",
      "Epoch 3987, Loss: 0.001921281513205031, Final Batch Loss: 0.00014784286031499505\n",
      "Epoch 3988, Loss: 0.010406663874164224, Final Batch Loss: 0.0012591570848599076\n",
      "Epoch 3989, Loss: 0.0036627217195928097, Final Batch Loss: 0.0001507886336185038\n",
      "Epoch 3990, Loss: 0.004400336580147268, Final Batch Loss: 4.9873822717927396e-05\n",
      "Epoch 3991, Loss: 0.004142553632846102, Final Batch Loss: 0.0008660048479214311\n",
      "Epoch 3992, Loss: 0.0018113659061782528, Final Batch Loss: 4.178107701591216e-05\n",
      "Epoch 3993, Loss: 0.0007348070284933783, Final Batch Loss: 0.00011815632024081424\n",
      "Epoch 3994, Loss: 0.0010060752247227356, Final Batch Loss: 7.988447032403201e-05\n",
      "Epoch 3995, Loss: 0.0026675698318285868, Final Batch Loss: 0.0011981780407950282\n",
      "Epoch 3996, Loss: 0.003537586293532513, Final Batch Loss: 0.0019231146434322\n",
      "Epoch 3997, Loss: 0.0013122415184625424, Final Batch Loss: 0.00014009058941155672\n",
      "Epoch 3998, Loss: 0.038860219705384225, Final Batch Loss: 0.0002148001076420769\n",
      "Epoch 3999, Loss: 0.0018909505233750679, Final Batch Loss: 9.788438910618424e-05\n",
      "Epoch 4000, Loss: 0.002457649345160462, Final Batch Loss: 0.0017286187503486872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4001, Loss: 0.015634378425602335, Final Batch Loss: 0.00010933899466181174\n",
      "Epoch 4002, Loss: 0.007822136161848903, Final Batch Loss: 0.00015914160758256912\n",
      "Epoch 4003, Loss: 0.0009861588805506472, Final Batch Loss: 3.2627478503854945e-05\n",
      "Epoch 4004, Loss: 0.0026499317755224183, Final Batch Loss: 0.0007010915433056653\n",
      "Epoch 4005, Loss: 0.003309510117105674, Final Batch Loss: 0.00011157316475873813\n",
      "Epoch 4006, Loss: 0.0045695788285229355, Final Batch Loss: 0.0008969389018602669\n",
      "Epoch 4007, Loss: 0.0066327510903647635, Final Batch Loss: 0.00013945333193987608\n",
      "Epoch 4008, Loss: 0.01134718490357045, Final Batch Loss: 0.0009869131026789546\n",
      "Epoch 4009, Loss: 0.003342980082379654, Final Batch Loss: 0.0001607147860340774\n",
      "Epoch 4010, Loss: 0.0016396610299125314, Final Batch Loss: 0.0001923234958667308\n",
      "Epoch 4011, Loss: 0.017926963286299724, Final Batch Loss: 0.00010828392260009423\n",
      "Epoch 4012, Loss: 0.006768456456484273, Final Batch Loss: 0.0011748262913897634\n",
      "Epoch 4013, Loss: 0.027699821337591857, Final Batch Loss: 0.0006269265431910753\n",
      "Epoch 4014, Loss: 0.002453401160892099, Final Batch Loss: 0.00015007799083832651\n",
      "Epoch 4015, Loss: 0.02792123606923269, Final Batch Loss: 0.026458773761987686\n",
      "Epoch 4016, Loss: 0.012475617302698083, Final Batch Loss: 0.011159578338265419\n",
      "Epoch 4017, Loss: 0.003938209549232852, Final Batch Loss: 0.0003919391892850399\n",
      "Epoch 4018, Loss: 0.014588602334697498, Final Batch Loss: 2.7400830731494352e-05\n",
      "Epoch 4019, Loss: 0.0017003042257783818, Final Batch Loss: 0.0009304152335971594\n",
      "Epoch 4020, Loss: 0.003862933979689842, Final Batch Loss: 0.002750879619270563\n",
      "Epoch 4021, Loss: 0.023428298580256524, Final Batch Loss: 0.000178292699274607\n",
      "Epoch 4022, Loss: 0.004822740986128338, Final Batch Loss: 0.00022237120720092207\n",
      "Epoch 4023, Loss: 0.008728784963750513, Final Batch Loss: 5.420268644229509e-05\n",
      "Epoch 4024, Loss: 0.0067311370476090815, Final Batch Loss: 3.212465890101157e-05\n",
      "Epoch 4025, Loss: 0.012335155479377136, Final Batch Loss: 0.0027608077507466078\n",
      "Epoch 4026, Loss: 0.002453243068885058, Final Batch Loss: 6.386151653714478e-05\n",
      "Epoch 4027, Loss: 0.015751307517348323, Final Batch Loss: 0.012394549325108528\n",
      "Epoch 4028, Loss: 0.0027480095595819876, Final Batch Loss: 0.0020680429879575968\n",
      "Epoch 4029, Loss: 0.001131409215304302, Final Batch Loss: 0.0007648680475540459\n",
      "Epoch 4030, Loss: 0.0006119670506450348, Final Batch Loss: 0.00011268615344306454\n",
      "Epoch 4031, Loss: 0.0010133255364053184, Final Batch Loss: 1.8569482563179918e-05\n",
      "Epoch 4032, Loss: 0.0006075637593312422, Final Batch Loss: 4.652658026316203e-05\n",
      "Epoch 4033, Loss: 0.021981640238664113, Final Batch Loss: 0.00014745743828825653\n",
      "Epoch 4034, Loss: 0.010305953285751457, Final Batch Loss: 0.0019016209989786148\n",
      "Epoch 4035, Loss: 0.001329126454947982, Final Batch Loss: 0.0003205210086889565\n",
      "Epoch 4036, Loss: 0.002288918476551771, Final Batch Loss: 6.044677866157144e-05\n",
      "Epoch 4037, Loss: 0.004140355245908722, Final Batch Loss: 0.0002297382161486894\n",
      "Epoch 4038, Loss: 0.0019259176006016787, Final Batch Loss: 0.00043848686618730426\n",
      "Epoch 4039, Loss: 0.04552807228901656, Final Batch Loss: 0.04430922120809555\n",
      "Epoch 4040, Loss: 0.0024048895265877945, Final Batch Loss: 2.677288830454927e-05\n",
      "Epoch 4041, Loss: 0.001716254795610439, Final Batch Loss: 9.517683793092147e-05\n",
      "Epoch 4042, Loss: 0.05229774423787603, Final Batch Loss: 0.05056611821055412\n",
      "Epoch 4043, Loss: 0.01698619854869321, Final Batch Loss: 8.923404675442725e-05\n",
      "Epoch 4044, Loss: 0.0013712857980863191, Final Batch Loss: 3.5738230508286506e-05\n",
      "Epoch 4045, Loss: 0.002337115911359433, Final Batch Loss: 2.7348403818905354e-05\n",
      "Epoch 4046, Loss: 0.0004724794162029866, Final Batch Loss: 4.577084837364964e-05\n",
      "Epoch 4047, Loss: 0.002985919185448438, Final Batch Loss: 7.407469092868268e-05\n",
      "Epoch 4048, Loss: 0.002372923343500588, Final Batch Loss: 0.0005325244856067002\n",
      "Epoch 4049, Loss: 0.0010665673253242858, Final Batch Loss: 4.885284215561114e-05\n",
      "Epoch 4050, Loss: 0.002106191204802599, Final Batch Loss: 0.000403870566515252\n",
      "Epoch 4051, Loss: 0.0029736569049418904, Final Batch Loss: 0.00018871165229938924\n",
      "Epoch 4052, Loss: 0.002007672199397348, Final Batch Loss: 0.00047850501141510904\n",
      "Epoch 4053, Loss: 0.0031519843978458084, Final Batch Loss: 8.387648995267227e-05\n",
      "Epoch 4054, Loss: 0.0008120886705000885, Final Batch Loss: 0.0001093899627448991\n",
      "Epoch 4055, Loss: 0.002722508186707273, Final Batch Loss: 0.0016279085539281368\n",
      "Epoch 4056, Loss: 0.001551794242914184, Final Batch Loss: 1.5509371223743074e-05\n",
      "Epoch 4057, Loss: 0.0013484469945979072, Final Batch Loss: 2.2668815290671773e-05\n",
      "Epoch 4058, Loss: 0.000537758107384434, Final Batch Loss: 7.590977475047112e-05\n",
      "Epoch 4059, Loss: 0.0009849716489043203, Final Batch Loss: 8.151924703270197e-06\n",
      "Epoch 4060, Loss: 0.001174135648398078, Final Batch Loss: 0.00029714798438362777\n",
      "Epoch 4061, Loss: 0.004473828557820525, Final Batch Loss: 0.00012369667820166796\n",
      "Epoch 4062, Loss: 0.0005812360577692743, Final Batch Loss: 3.2647320040268824e-05\n",
      "Epoch 4063, Loss: 0.0004969509136572015, Final Batch Loss: 3.307658698759042e-05\n",
      "Epoch 4064, Loss: 0.001866256665380206, Final Batch Loss: 0.0004475497407838702\n",
      "Epoch 4065, Loss: 0.0008284436953545082, Final Batch Loss: 0.0003249790461268276\n",
      "Epoch 4066, Loss: 0.0007154627965064719, Final Batch Loss: 4.22312987211626e-05\n",
      "Epoch 4067, Loss: 0.00039022623423079494, Final Batch Loss: 2.9270491722854786e-05\n",
      "Epoch 4068, Loss: 0.004227299359627068, Final Batch Loss: 6.738309457432479e-05\n",
      "Epoch 4069, Loss: 0.04079808223286818, Final Batch Loss: 0.0016877048183232546\n",
      "Epoch 4070, Loss: 0.003928265388367436, Final Batch Loss: 0.0006019651773385704\n",
      "Epoch 4071, Loss: 0.04155609552981332, Final Batch Loss: 0.026193350553512573\n",
      "Epoch 4072, Loss: 0.002007470015087165, Final Batch Loss: 5.959625559626147e-05\n",
      "Epoch 4073, Loss: 0.0006774561843485571, Final Batch Loss: 2.4194720026571304e-05\n",
      "Epoch 4074, Loss: 0.0025412200739083346, Final Batch Loss: 0.001762034953571856\n",
      "Epoch 4075, Loss: 0.017241614976228448, Final Batch Loss: 0.0005028574378229678\n",
      "Epoch 4076, Loss: 0.0034919212630484253, Final Batch Loss: 0.00021281238878145814\n",
      "Epoch 4077, Loss: 0.0019393695765757002, Final Batch Loss: 7.42301854188554e-05\n",
      "Epoch 4078, Loss: 0.001688455893599894, Final Batch Loss: 4.367034853203222e-05\n",
      "Epoch 4079, Loss: 0.0006282029025896918, Final Batch Loss: 8.743165381019935e-05\n",
      "Epoch 4080, Loss: 0.0013198020242271014, Final Batch Loss: 0.0001604360732017085\n",
      "Epoch 4081, Loss: 0.0008886869145499077, Final Batch Loss: 0.000302276574075222\n",
      "Epoch 4082, Loss: 0.0031631958790967474, Final Batch Loss: 2.8107251637266017e-05\n",
      "Epoch 4083, Loss: 0.001264999809791334, Final Batch Loss: 7.945424295030534e-05\n",
      "Epoch 4084, Loss: 0.00044696950590150664, Final Batch Loss: 1.0907810974458698e-05\n",
      "Epoch 4085, Loss: 0.0022384347757906653, Final Batch Loss: 3.9689788536634296e-05\n",
      "Epoch 4086, Loss: 0.0011751644851756282, Final Batch Loss: 8.294305007439107e-05\n",
      "Epoch 4087, Loss: 0.003949416532122996, Final Batch Loss: 6.373535870807245e-05\n",
      "Epoch 4088, Loss: 0.0006963265632293769, Final Batch Loss: 0.00010388392547611147\n",
      "Epoch 4089, Loss: 0.0010090238247357775, Final Batch Loss: 0.00014828714483883232\n",
      "Epoch 4090, Loss: 0.0019989948068541707, Final Batch Loss: 0.0007250567432492971\n",
      "Epoch 4091, Loss: 0.0018819842389348196, Final Batch Loss: 0.0006033000536262989\n",
      "Epoch 4092, Loss: 0.0016647335578454658, Final Batch Loss: 3.1041738111525774e-05\n",
      "Epoch 4093, Loss: 0.0008995730913738953, Final Batch Loss: 2.6424002498970367e-05\n",
      "Epoch 4094, Loss: 0.0003118951754004229, Final Batch Loss: 7.053728040773422e-05\n",
      "Epoch 4095, Loss: 0.0008207249775296077, Final Batch Loss: 2.0461025997065008e-05\n",
      "Epoch 4096, Loss: 0.0020166591129964218, Final Batch Loss: 0.0006165815866552293\n",
      "Epoch 4097, Loss: 0.0006096892830100842, Final Batch Loss: 1.155723657575436e-05\n",
      "Epoch 4098, Loss: 0.0011522939075803151, Final Batch Loss: 1.809766581573058e-05\n",
      "Epoch 4099, Loss: 0.0026995294174412265, Final Batch Loss: 0.0005131427897140384\n",
      "Epoch 4100, Loss: 0.0009894463637465378, Final Batch Loss: 0.000321567349601537\n",
      "Epoch 4101, Loss: 0.00024236689750978258, Final Batch Loss: 2.1704394384869374e-05\n",
      "Epoch 4102, Loss: 0.0019334208591317292, Final Batch Loss: 0.0014518898678943515\n",
      "Epoch 4103, Loss: 0.0006709680619678693, Final Batch Loss: 0.00019647755834739655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4104, Loss: 0.000623275214820751, Final Batch Loss: 3.522985934978351e-05\n",
      "Epoch 4105, Loss: 0.0008643332839710638, Final Batch Loss: 0.000197886154637672\n",
      "Epoch 4106, Loss: 0.000839723013996263, Final Batch Loss: 0.0005990099743939936\n",
      "Epoch 4107, Loss: 0.002731951694613599, Final Batch Loss: 2.974395783894579e-06\n",
      "Epoch 4108, Loss: 0.00402877030774107, Final Batch Loss: 9.465485163673293e-06\n",
      "Epoch 4109, Loss: 0.0008751591049076524, Final Batch Loss: 1.7213369574164972e-05\n",
      "Epoch 4110, Loss: 0.0008046974362514447, Final Batch Loss: 4.885201997240074e-05\n",
      "Epoch 4111, Loss: 0.021649162606991013, Final Batch Loss: 0.00010174390627071261\n",
      "Epoch 4112, Loss: 0.001631087257010222, Final Batch Loss: 1.354004143649945e-05\n",
      "Epoch 4113, Loss: 0.0008039984131755773, Final Batch Loss: 0.00035774518619291484\n",
      "Epoch 4114, Loss: 0.008333813540957635, Final Batch Loss: 0.0007489523268304765\n",
      "Epoch 4115, Loss: 0.000981224637143896, Final Batch Loss: 6.898633000673726e-05\n",
      "Epoch 4116, Loss: 0.004573941099806689, Final Batch Loss: 0.0035446323454380035\n",
      "Epoch 4117, Loss: 0.0017183289655804401, Final Batch Loss: 1.579184208821971e-05\n",
      "Epoch 4118, Loss: 0.05532749597750808, Final Batch Loss: 0.0005089600454084575\n",
      "Epoch 4119, Loss: 0.00837013429554645, Final Batch Loss: 0.0002622427127789706\n",
      "Epoch 4120, Loss: 0.0021043921806267463, Final Batch Loss: 4.3432533857412636e-05\n",
      "Epoch 4121, Loss: 0.002128128704498522, Final Batch Loss: 0.00024650944396853447\n",
      "Epoch 4122, Loss: 0.004627838257874828, Final Batch Loss: 0.0007372350664809346\n",
      "Epoch 4123, Loss: 0.017011082760291174, Final Batch Loss: 3.144008223898709e-05\n",
      "Epoch 4124, Loss: 0.003443935976974899, Final Batch Loss: 2.9317208827706054e-05\n",
      "Epoch 4125, Loss: 0.005088205347419716, Final Batch Loss: 3.753296914510429e-05\n",
      "Epoch 4126, Loss: 0.024917275317420717, Final Batch Loss: 0.001676699728704989\n",
      "Epoch 4127, Loss: 0.022606106125749648, Final Batch Loss: 0.00044146159780211747\n",
      "Epoch 4128, Loss: 0.002040561659669038, Final Batch Loss: 8.906397124519572e-05\n",
      "Epoch 4129, Loss: 0.0003439787724346388, Final Batch Loss: 1.5420308045577258e-05\n",
      "Epoch 4130, Loss: 0.0227683896155213, Final Batch Loss: 5.433527257991955e-05\n",
      "Epoch 4131, Loss: 0.0010028197466454003, Final Batch Loss: 3.5429908166406676e-05\n",
      "Epoch 4132, Loss: 0.0009267281420761719, Final Batch Loss: 4.162794357398525e-05\n",
      "Epoch 4133, Loss: 0.017697887694339443, Final Batch Loss: 1.2761821380991023e-05\n",
      "Epoch 4134, Loss: 0.007014566288376045, Final Batch Loss: 1.0210713980995934e-06\n",
      "Epoch 4135, Loss: 0.0006041188316885382, Final Batch Loss: 3.1564588425680995e-05\n",
      "Epoch 4136, Loss: 0.0011309454421279952, Final Batch Loss: 0.00019347081251908094\n",
      "Epoch 4137, Loss: 0.0014163257565087406, Final Batch Loss: 4.1158633393933997e-05\n",
      "Epoch 4138, Loss: 0.0004439451649886905, Final Batch Loss: 8.619030268164352e-05\n",
      "Epoch 4139, Loss: 0.0008715070932794333, Final Batch Loss: 0.0005187815404497087\n",
      "Epoch 4140, Loss: 0.006565642346686218, Final Batch Loss: 8.340408385265619e-06\n",
      "Epoch 4141, Loss: 0.0020107723112232634, Final Batch Loss: 0.0001845625083660707\n",
      "Epoch 4142, Loss: 0.00046348737669177353, Final Batch Loss: 6.423120066756383e-05\n",
      "Epoch 4143, Loss: 0.002521034220990259, Final Batch Loss: 0.00022566731786355376\n",
      "Epoch 4144, Loss: 0.0016936508909566328, Final Batch Loss: 0.0014801302459090948\n",
      "Epoch 4145, Loss: 0.0007243153086164966, Final Batch Loss: 9.3214308435563e-05\n",
      "Epoch 4146, Loss: 0.05892751499050064, Final Batch Loss: 0.056832075119018555\n",
      "Epoch 4147, Loss: 0.02172009392234031, Final Batch Loss: 0.011196336708962917\n",
      "Epoch 4148, Loss: 0.017173309406643966, Final Batch Loss: 2.5842953618848696e-05\n",
      "Epoch 4149, Loss: 0.045240241481224075, Final Batch Loss: 0.011297355405986309\n",
      "Epoch 4150, Loss: 0.0013066972733213333, Final Batch Loss: 0.00015327351866289973\n",
      "Epoch 4151, Loss: 0.006510429664558615, Final Batch Loss: 2.4466020477120765e-05\n",
      "Epoch 4152, Loss: 0.008081332205620129, Final Batch Loss: 0.00263970741070807\n",
      "Epoch 4153, Loss: 0.03305337498022709, Final Batch Loss: 0.0006814036751165986\n",
      "Epoch 4154, Loss: 0.006234078957277234, Final Batch Loss: 3.0404769859160297e-05\n",
      "Epoch 4155, Loss: 0.0014972433527873363, Final Batch Loss: 0.00014187104534357786\n",
      "Epoch 4156, Loss: 0.0035348003293620422, Final Batch Loss: 0.0019072876311838627\n",
      "Epoch 4157, Loss: 0.0076505963515955955, Final Batch Loss: 0.0019578852225095034\n",
      "Epoch 4158, Loss: 0.017580878073204076, Final Batch Loss: 0.0005309358239173889\n",
      "Epoch 4159, Loss: 0.00694866938283667, Final Batch Loss: 0.00015219287888612598\n",
      "Epoch 4160, Loss: 0.0040282584668602794, Final Batch Loss: 0.00020719075109809637\n",
      "Epoch 4161, Loss: 0.0016090371937025338, Final Batch Loss: 0.0004337475693318993\n",
      "Epoch 4162, Loss: 0.002694146751309745, Final Batch Loss: 0.00013519302592612803\n",
      "Epoch 4163, Loss: 0.003007932216860354, Final Batch Loss: 9.337833034805954e-05\n",
      "Epoch 4164, Loss: 0.0027903683148906566, Final Batch Loss: 0.00018851272761821747\n",
      "Epoch 4165, Loss: 0.0012474606655814569, Final Batch Loss: 1.1071791050198954e-05\n",
      "Epoch 4166, Loss: 0.0016684259135217872, Final Batch Loss: 0.00028008862864226103\n",
      "Epoch 4167, Loss: 0.0016421034051745664, Final Batch Loss: 9.814288205234334e-05\n",
      "Epoch 4168, Loss: 0.0006568182725459337, Final Batch Loss: 9.308514563599601e-06\n",
      "Epoch 4169, Loss: 0.0021141688703210093, Final Batch Loss: 8.695854194229469e-05\n",
      "Epoch 4170, Loss: 0.000564122248761123, Final Batch Loss: 7.833427662262693e-05\n",
      "Epoch 4171, Loss: 0.0018535738781793043, Final Batch Loss: 5.890849570278078e-05\n",
      "Epoch 4172, Loss: 0.13680477198795415, Final Batch Loss: 5.345983663573861e-05\n",
      "Epoch 4173, Loss: 0.001597900190972723, Final Batch Loss: 0.0004246775060892105\n",
      "Epoch 4174, Loss: 0.0016035162661864888, Final Batch Loss: 0.00018910010112449527\n",
      "Epoch 4175, Loss: 0.007125901895051356, Final Batch Loss: 6.0259077145019546e-05\n",
      "Epoch 4176, Loss: 0.00036187311343383044, Final Batch Loss: 1.6482556020491756e-05\n",
      "Epoch 4177, Loss: 0.0012225768423377303, Final Batch Loss: 0.0005024656420573592\n",
      "Epoch 4178, Loss: 0.0015151473198784515, Final Batch Loss: 0.0008200269076041877\n",
      "Epoch 4179, Loss: 0.0013442805102386046, Final Batch Loss: 0.00014988235488999635\n",
      "Epoch 4180, Loss: 0.02417973629417247, Final Batch Loss: 0.01300936657935381\n",
      "Epoch 4181, Loss: 0.010489840729860589, Final Batch Loss: 0.0006096995202824473\n",
      "Epoch 4182, Loss: 0.004361368628451601, Final Batch Loss: 0.0018705935217440128\n",
      "Epoch 4183, Loss: 0.006591769852093421, Final Batch Loss: 0.002766246208921075\n",
      "Epoch 4184, Loss: 0.005192998156417161, Final Batch Loss: 0.0013012648560106754\n",
      "Epoch 4185, Loss: 0.0021976161806378514, Final Batch Loss: 0.0009096474386751652\n",
      "Epoch 4186, Loss: 0.0246344502629654, Final Batch Loss: 0.00019401838653720915\n",
      "Epoch 4187, Loss: 0.00624217410950223, Final Batch Loss: 0.00044942082604393363\n",
      "Epoch 4188, Loss: 0.0036892249972879654, Final Batch Loss: 2.1377016309997998e-05\n",
      "Epoch 4189, Loss: 0.00172940732591087, Final Batch Loss: 0.0002659179153852165\n",
      "Epoch 4190, Loss: 0.001285088736040052, Final Batch Loss: 0.0001124582122429274\n",
      "Epoch 4191, Loss: 0.0012333236991253216, Final Batch Loss: 9.677114576334134e-05\n",
      "Epoch 4192, Loss: 0.011089955871284474, Final Batch Loss: 3.7888316001044586e-05\n",
      "Epoch 4193, Loss: 0.0012587574892677367, Final Batch Loss: 0.0004462016513571143\n",
      "Epoch 4194, Loss: 0.06183930016049999, Final Batch Loss: 0.0061321924440562725\n",
      "Epoch 4195, Loss: 0.04097260441631079, Final Batch Loss: 0.02012667991220951\n",
      "Epoch 4196, Loss: 0.007406394863210153, Final Batch Loss: 4.67793142888695e-05\n",
      "Epoch 4197, Loss: 0.03261786640359787, Final Batch Loss: 0.00020269608648959547\n",
      "Epoch 4198, Loss: 0.0020516711456366465, Final Batch Loss: 0.0009921767050400376\n",
      "Epoch 4199, Loss: 0.04056099019726389, Final Batch Loss: 2.9692066163988784e-05\n",
      "Epoch 4200, Loss: 0.006911610849783756, Final Batch Loss: 0.0010376794962212443\n",
      "Epoch 4201, Loss: 0.00876184103981359, Final Batch Loss: 0.0012575319269672036\n",
      "Epoch 4202, Loss: 0.006352123604301596, Final Batch Loss: 4.001823617727496e-05\n",
      "Epoch 4203, Loss: 0.0032929899462033063, Final Batch Loss: 0.0002532833896111697\n",
      "Epoch 4204, Loss: 0.011370809825166361, Final Batch Loss: 8.024036651477218e-05\n",
      "Epoch 4205, Loss: 0.00810297233692836, Final Batch Loss: 0.0002018345840042457\n",
      "Epoch 4206, Loss: 0.03429554216199904, Final Batch Loss: 0.0009792118798941374\n",
      "Epoch 4207, Loss: 0.020771157047420274, Final Batch Loss: 0.019551411271095276\n",
      "Epoch 4208, Loss: 0.00246139142109314, Final Batch Loss: 0.0009703370160423219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4209, Loss: 0.009243321663234383, Final Batch Loss: 0.0017122577410191298\n",
      "Epoch 4210, Loss: 0.01162026081874501, Final Batch Loss: 0.000575631856918335\n",
      "Epoch 4211, Loss: 0.002909323899075389, Final Batch Loss: 0.00022777267440687865\n",
      "Epoch 4212, Loss: 0.0052299618982942775, Final Batch Loss: 0.0011202540481463075\n",
      "Epoch 4213, Loss: 0.036780804381123744, Final Batch Loss: 0.03151706978678703\n",
      "Epoch 4214, Loss: 0.004300210228393553, Final Batch Loss: 9.113804844673723e-05\n",
      "Epoch 4215, Loss: 0.011073099682107568, Final Batch Loss: 0.001347148441709578\n",
      "Epoch 4216, Loss: 0.009038399497512728, Final Batch Loss: 0.0002724794903770089\n",
      "Epoch 4217, Loss: 0.002665488551429007, Final Batch Loss: 0.00014012148312758654\n",
      "Epoch 4218, Loss: 0.0010061932052849443, Final Batch Loss: 3.1836098060011864e-05\n",
      "Epoch 4219, Loss: 0.006824464915553108, Final Batch Loss: 0.0001462529180571437\n",
      "Epoch 4220, Loss: 0.035864373814547434, Final Batch Loss: 0.00022817225544713438\n",
      "Epoch 4221, Loss: 0.04860791157989297, Final Batch Loss: 0.00040683167753741145\n",
      "Epoch 4222, Loss: 0.015030715381726623, Final Batch Loss: 0.011811302974820137\n",
      "Epoch 4223, Loss: 0.0018633219351613661, Final Batch Loss: 0.0001993091282201931\n",
      "Epoch 4224, Loss: 0.0016106942930491641, Final Batch Loss: 0.00025139848003163934\n",
      "Epoch 4225, Loss: 0.009197362291160971, Final Batch Loss: 0.0002900919644162059\n",
      "Epoch 4226, Loss: 0.019953462258854415, Final Batch Loss: 0.0005431929021142423\n",
      "Epoch 4227, Loss: 0.003510954906232655, Final Batch Loss: 0.0010434883879497647\n",
      "Epoch 4228, Loss: 0.010167810549319256, Final Batch Loss: 0.00040859487489797175\n",
      "Epoch 4229, Loss: 0.011458271997980773, Final Batch Loss: 0.00017709293751977384\n",
      "Epoch 4230, Loss: 0.0034569361378089525, Final Batch Loss: 9.377305104862899e-05\n",
      "Epoch 4231, Loss: 0.01543898120871745, Final Batch Loss: 0.00024810052127577364\n",
      "Epoch 4232, Loss: 0.017930908557900693, Final Batch Loss: 3.6364472180139273e-05\n",
      "Epoch 4233, Loss: 0.017738875074428506, Final Batch Loss: 0.00012769123713951558\n",
      "Epoch 4234, Loss: 0.0017853371537057683, Final Batch Loss: 0.0002243832714157179\n",
      "Epoch 4235, Loss: 0.011477963849756634, Final Batch Loss: 2.9124315915396437e-05\n",
      "Epoch 4236, Loss: 0.001702107627352234, Final Batch Loss: 0.0004930683062411845\n",
      "Epoch 4237, Loss: 0.006408059234672692, Final Batch Loss: 0.000660169287584722\n",
      "Epoch 4238, Loss: 0.01384594886621926, Final Batch Loss: 0.0001734832621878013\n",
      "Epoch 4239, Loss: 0.002282118410221301, Final Batch Loss: 0.00032365910010412335\n",
      "Epoch 4240, Loss: 0.0019702249846886843, Final Batch Loss: 0.0006735300412401557\n",
      "Epoch 4241, Loss: 0.0038242636219365522, Final Batch Loss: 2.8738068067468703e-05\n",
      "Epoch 4242, Loss: 0.003453058798186248, Final Batch Loss: 0.001870851032435894\n",
      "Epoch 4243, Loss: 0.0027693773445207626, Final Batch Loss: 0.00026023408281616867\n",
      "Epoch 4244, Loss: 0.002276661260111723, Final Batch Loss: 0.0005350264837034047\n",
      "Epoch 4245, Loss: 0.002847804811608512, Final Batch Loss: 8.752190478844568e-05\n",
      "Epoch 4246, Loss: 0.001043098971422296, Final Batch Loss: 6.771861080778763e-05\n",
      "Epoch 4247, Loss: 0.0012378270039334893, Final Batch Loss: 0.00022832921240478754\n",
      "Epoch 4248, Loss: 0.0014779380653635599, Final Batch Loss: 0.0002102269500028342\n",
      "Epoch 4249, Loss: 0.002316081154276617, Final Batch Loss: 0.0015549472300335765\n",
      "Epoch 4250, Loss: 0.002794305159113719, Final Batch Loss: 2.8720982300001197e-05\n",
      "Epoch 4251, Loss: 0.0017604126405785792, Final Batch Loss: 0.00013619438686873764\n",
      "Epoch 4252, Loss: 0.0013351228462852305, Final Batch Loss: 2.8150176149210893e-05\n",
      "Epoch 4253, Loss: 0.0013213241109042428, Final Batch Loss: 0.0006276395288296044\n",
      "Epoch 4254, Loss: 0.001422165471012704, Final Batch Loss: 0.0002919877297244966\n",
      "Epoch 4255, Loss: 0.0018043173622572795, Final Batch Loss: 7.37974769435823e-05\n",
      "Epoch 4256, Loss: 0.0004895660749753006, Final Batch Loss: 5.037118535256013e-05\n",
      "Epoch 4257, Loss: 0.001778860081685707, Final Batch Loss: 0.0002934815420303494\n",
      "Epoch 4258, Loss: 0.0006387616776919458, Final Batch Loss: 0.00032569034374319017\n",
      "Epoch 4259, Loss: 0.0025019860404427163, Final Batch Loss: 0.0008243220509029925\n",
      "Epoch 4260, Loss: 0.0016399177748098737, Final Batch Loss: 7.816689321771264e-05\n",
      "Epoch 4261, Loss: 0.0012801354350813199, Final Batch Loss: 2.164620309486054e-05\n",
      "Epoch 4262, Loss: 0.001491783099481836, Final Batch Loss: 0.000516095373313874\n",
      "Epoch 4263, Loss: 0.0029813275032211095, Final Batch Loss: 0.00011441175593063235\n",
      "Epoch 4264, Loss: 0.0020930564678565133, Final Batch Loss: 3.485271008685231e-05\n",
      "Epoch 4265, Loss: 0.0008702595932845725, Final Batch Loss: 3.497096258797683e-06\n",
      "Epoch 4266, Loss: 0.0007739618522464298, Final Batch Loss: 4.293126403354108e-05\n",
      "Epoch 4267, Loss: 0.004701192852735403, Final Batch Loss: 0.00014983881555963308\n",
      "Epoch 4268, Loss: 0.0008098385897028493, Final Batch Loss: 0.0003827193286269903\n",
      "Epoch 4269, Loss: 0.0027650860356516205, Final Batch Loss: 0.0002538533881306648\n",
      "Epoch 4270, Loss: 0.0013342833208298543, Final Batch Loss: 0.0001483345840824768\n",
      "Epoch 4271, Loss: 0.00385194177943049, Final Batch Loss: 0.0012351219775155187\n",
      "Epoch 4272, Loss: 0.0036804191631745198, Final Batch Loss: 7.550458576588426e-06\n",
      "Epoch 4273, Loss: 0.0011616870410762203, Final Batch Loss: 7.833930794731714e-06\n",
      "Epoch 4274, Loss: 0.0009758451960806269, Final Batch Loss: 9.024203609442338e-05\n",
      "Epoch 4275, Loss: 0.0009191228964482434, Final Batch Loss: 0.00039445734000764787\n",
      "Epoch 4276, Loss: 0.020300881067669252, Final Batch Loss: 0.00012970568786840886\n",
      "Epoch 4277, Loss: 0.039942668910953216, Final Batch Loss: 5.356060864869505e-05\n",
      "Epoch 4278, Loss: 0.005981870977848303, Final Batch Loss: 7.511769217671826e-05\n",
      "Epoch 4279, Loss: 0.00227597192133544, Final Batch Loss: 0.001574345980770886\n",
      "Epoch 4280, Loss: 0.004223763346999476, Final Batch Loss: 1.397669348079944e-05\n",
      "Epoch 4281, Loss: 0.0036873650969937444, Final Batch Loss: 0.00044276544940657914\n",
      "Epoch 4282, Loss: 0.017931137437699363, Final Batch Loss: 0.00013226515147835016\n",
      "Epoch 4283, Loss: 0.004766884831042262, Final Batch Loss: 0.0035381242632865906\n",
      "Epoch 4284, Loss: 0.008744822058361024, Final Batch Loss: 0.0053931949660182\n",
      "Epoch 4285, Loss: 0.0020183411143079866, Final Batch Loss: 0.0003299217496532947\n",
      "Epoch 4286, Loss: 0.003095465276601317, Final Batch Loss: 1.870364576461725e-05\n",
      "Epoch 4287, Loss: 0.0005599816877293051, Final Batch Loss: 9.667922313383315e-06\n",
      "Epoch 4288, Loss: 0.010040940054750536, Final Batch Loss: 0.0007106162956915796\n",
      "Epoch 4289, Loss: 0.001672121368756052, Final Batch Loss: 6.872912490507588e-05\n",
      "Epoch 4290, Loss: 0.002001203909458127, Final Batch Loss: 0.00011783140507759526\n",
      "Epoch 4291, Loss: 0.00384733131613757, Final Batch Loss: 5.031445834902115e-05\n",
      "Epoch 4292, Loss: 0.0009961688197108742, Final Batch Loss: 2.2892509150551632e-05\n",
      "Epoch 4293, Loss: 0.0007616616021550726, Final Batch Loss: 1.8994236597791314e-05\n",
      "Epoch 4294, Loss: 0.0012380615880829282, Final Batch Loss: 0.000490173464640975\n",
      "Epoch 4295, Loss: 0.0016768189288995927, Final Batch Loss: 0.00012593349674716592\n",
      "Epoch 4296, Loss: 0.0010502235131752968, Final Batch Loss: 6.66254663883592e-06\n",
      "Epoch 4297, Loss: 0.01001048360740242, Final Batch Loss: 0.005799602717161179\n",
      "Epoch 4298, Loss: 0.005274973027553642, Final Batch Loss: 0.0028214200865477324\n",
      "Epoch 4299, Loss: 0.0007609277527080849, Final Batch Loss: 5.5833334045019e-05\n",
      "Epoch 4300, Loss: 0.0007619918796990532, Final Batch Loss: 4.240605267114006e-05\n",
      "Epoch 4301, Loss: 0.0005688657984137535, Final Batch Loss: 0.00010376406862633303\n",
      "Epoch 4302, Loss: 0.0005305159120325698, Final Batch Loss: 1.5632142094545998e-05\n",
      "Epoch 4303, Loss: 0.0012622924587049056, Final Batch Loss: 0.00026873554452322423\n",
      "Epoch 4304, Loss: 0.00031438027122021595, Final Batch Loss: 7.34585482859984e-05\n",
      "Epoch 4305, Loss: 0.000756428269596654, Final Batch Loss: 4.4001884816680104e-05\n",
      "Epoch 4306, Loss: 0.0006194666239025537, Final Batch Loss: 0.00012091873213648796\n",
      "Epoch 4307, Loss: 0.0004382988950055733, Final Batch Loss: 8.224762859754264e-05\n",
      "Epoch 4308, Loss: 0.013401830903603695, Final Batch Loss: 0.0002001119137275964\n",
      "Epoch 4309, Loss: 0.03922707644778711, Final Batch Loss: 0.037497684359550476\n",
      "Epoch 4310, Loss: 0.004954812784490059, Final Batch Loss: 0.0010158134391531348\n",
      "Epoch 4311, Loss: 0.0037865468129893998, Final Batch Loss: 0.0005458457162603736\n",
      "Epoch 4312, Loss: 0.001876658330729697, Final Batch Loss: 0.00033826520666480064\n",
      "Epoch 4313, Loss: 0.0034881678948295303, Final Batch Loss: 0.00011500328400870785\n",
      "Epoch 4314, Loss: 0.004170548385445727, Final Batch Loss: 6.433135422412306e-05\n",
      "Epoch 4315, Loss: 0.0007086371842888184, Final Batch Loss: 0.00011243260087212548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4316, Loss: 0.0009362412674818188, Final Batch Loss: 0.00035780371399596334\n",
      "Epoch 4317, Loss: 0.0030904754876246443, Final Batch Loss: 0.0014341111527755857\n",
      "Epoch 4318, Loss: 0.001421852003204549, Final Batch Loss: 2.4292880880238954e-06\n",
      "Epoch 4319, Loss: 0.0022572302477783523, Final Batch Loss: 0.00041499928920529783\n",
      "Epoch 4320, Loss: 0.005177945859031752, Final Batch Loss: 0.001968145137652755\n",
      "Epoch 4321, Loss: 0.0010364787613070803, Final Batch Loss: 0.00010149579611606896\n",
      "Epoch 4322, Loss: 0.0011023743736586766, Final Batch Loss: 0.00039597382419742644\n",
      "Epoch 4323, Loss: 0.0020773474818724935, Final Batch Loss: 1.8513630948291393e-06\n",
      "Epoch 4324, Loss: 0.0015388580795843154, Final Batch Loss: 0.0008889493183232844\n",
      "Epoch 4325, Loss: 0.0006859569757580175, Final Batch Loss: 6.635727004322689e-06\n",
      "Epoch 4326, Loss: 0.0006840681780886371, Final Batch Loss: 2.8662223485298455e-05\n",
      "Epoch 4327, Loss: 0.002937743211077759, Final Batch Loss: 0.00014048657612875104\n",
      "Epoch 4328, Loss: 0.005130828779329022, Final Batch Loss: 0.0001748434588080272\n",
      "Epoch 4329, Loss: 0.0022553000981133664, Final Batch Loss: 0.0006812879000790417\n",
      "Epoch 4330, Loss: 0.0035218873981648358, Final Batch Loss: 0.0015506901545450091\n",
      "Epoch 4331, Loss: 0.0015168523786996957, Final Batch Loss: 3.132729762000963e-05\n",
      "Epoch 4332, Loss: 0.0009180951165035367, Final Batch Loss: 1.7429432773496956e-05\n",
      "Epoch 4333, Loss: 0.000771005990827689, Final Batch Loss: 0.00022348503989633173\n",
      "Epoch 4334, Loss: 0.0014253132985686534, Final Batch Loss: 4.4286989577813074e-05\n",
      "Epoch 4335, Loss: 0.0005639967439492466, Final Batch Loss: 0.00019104761304333806\n",
      "Epoch 4336, Loss: 0.00027996899916615803, Final Batch Loss: 9.744606359163299e-05\n",
      "Epoch 4337, Loss: 0.0012579000376717886, Final Batch Loss: 2.4572849724791013e-05\n",
      "Epoch 4338, Loss: 0.0013977165808682912, Final Batch Loss: 9.82747005764395e-05\n",
      "Epoch 4339, Loss: 0.0003358200301590841, Final Batch Loss: 0.00011938758689211681\n",
      "Epoch 4340, Loss: 0.0003378196979610948, Final Batch Loss: 2.580970431154128e-05\n",
      "Epoch 4341, Loss: 0.014906276905094273, Final Batch Loss: 0.0002836963103618473\n",
      "Epoch 4342, Loss: 0.016068679669842822, Final Batch Loss: 0.005126393865793943\n",
      "Epoch 4343, Loss: 0.0013871505070710555, Final Batch Loss: 0.00023234319814946502\n",
      "Epoch 4344, Loss: 0.0026508392329560593, Final Batch Loss: 4.9549042159924284e-05\n",
      "Epoch 4345, Loss: 0.02710691415995825, Final Batch Loss: 0.0005811999435536563\n",
      "Epoch 4346, Loss: 0.01078161084308249, Final Batch Loss: 0.009492327459156513\n",
      "Epoch 4347, Loss: 0.005795459877845133, Final Batch Loss: 0.001005985657684505\n",
      "Epoch 4348, Loss: 0.018739143990387674, Final Batch Loss: 5.0015951273962855e-05\n",
      "Epoch 4349, Loss: 0.0006879577149447869, Final Batch Loss: 1.3900777958042454e-05\n",
      "Epoch 4350, Loss: 0.0011214896658202633, Final Batch Loss: 1.757371319399681e-05\n",
      "Epoch 4351, Loss: 0.004797779809450731, Final Batch Loss: 0.0005773704615421593\n",
      "Epoch 4352, Loss: 0.0053074731476954184, Final Batch Loss: 0.0011946118902415037\n",
      "Epoch 4353, Loss: 0.015075304392667022, Final Batch Loss: 6.467498315032572e-05\n",
      "Epoch 4354, Loss: 0.04409132333694288, Final Batch Loss: 0.021834908053278923\n",
      "Epoch 4355, Loss: 0.018874779531870445, Final Batch Loss: 0.00016625462740194052\n",
      "Epoch 4356, Loss: 0.02018114013480954, Final Batch Loss: 0.0009364479337818921\n",
      "Epoch 4357, Loss: 0.009108344034757465, Final Batch Loss: 9.157806198345497e-05\n",
      "Epoch 4358, Loss: 0.014112658331214334, Final Batch Loss: 0.012104901485145092\n",
      "Epoch 4359, Loss: 0.001951207162164792, Final Batch Loss: 1.5066982996359002e-05\n",
      "Epoch 4360, Loss: 0.0015682884986745194, Final Batch Loss: 0.0003115304571110755\n",
      "Epoch 4361, Loss: 0.0009468493390158983, Final Batch Loss: 3.5122400731779635e-05\n",
      "Epoch 4362, Loss: 0.0023165235252236016, Final Batch Loss: 2.9265771445352584e-05\n",
      "Epoch 4363, Loss: 0.002593181638076203, Final Batch Loss: 0.0010799325536936522\n",
      "Epoch 4364, Loss: 0.0007794677985657472, Final Batch Loss: 8.344878733623773e-05\n",
      "Epoch 4365, Loss: 0.0006591743149328977, Final Batch Loss: 4.684839223045856e-05\n",
      "Epoch 4366, Loss: 0.0007541457816842012, Final Batch Loss: 0.00031991663854569197\n",
      "Epoch 4367, Loss: 0.0007001322010182776, Final Batch Loss: 0.00029392674332484603\n",
      "Epoch 4368, Loss: 0.0067526021721278084, Final Batch Loss: 0.00018122760229744017\n",
      "Epoch 4369, Loss: 0.0006383881755027687, Final Batch Loss: 7.93951221567113e-06\n",
      "Epoch 4370, Loss: 0.010775352318887599, Final Batch Loss: 8.37551851873286e-06\n",
      "Epoch 4371, Loss: 0.0029392179189926537, Final Batch Loss: 3.0022915780136827e-06\n",
      "Epoch 4372, Loss: 0.004990319619537331, Final Batch Loss: 0.0001176628065877594\n",
      "Epoch 4373, Loss: 0.0012803938786873914, Final Batch Loss: 2.3142254121921724e-06\n",
      "Epoch 4374, Loss: 0.0008393276439164765, Final Batch Loss: 5.9234364016447216e-05\n",
      "Epoch 4375, Loss: 0.023214648157591, Final Batch Loss: 0.0002160566655220464\n",
      "Epoch 4376, Loss: 0.013806456830934621, Final Batch Loss: 6.75842456985265e-05\n",
      "Epoch 4377, Loss: 0.0009007613753055921, Final Batch Loss: 1.0515750545891933e-05\n",
      "Epoch 4378, Loss: 0.006655654942733236, Final Batch Loss: 0.00030290649738162756\n",
      "Epoch 4379, Loss: 0.003903916644048877, Final Batch Loss: 0.001353991567157209\n",
      "Epoch 4380, Loss: 0.0014319750116555952, Final Batch Loss: 0.0006498704897239804\n",
      "Epoch 4381, Loss: 0.0020355208735054475, Final Batch Loss: 0.0004918561899103224\n",
      "Epoch 4382, Loss: 0.0015540022650384344, Final Batch Loss: 0.00011455899948487058\n",
      "Epoch 4383, Loss: 0.006626023612625431, Final Batch Loss: 0.00017781576025299728\n",
      "Epoch 4384, Loss: 0.004427392937941477, Final Batch Loss: 0.0020714998245239258\n",
      "Epoch 4385, Loss: 0.0029307083095773123, Final Batch Loss: 0.0009565203217789531\n",
      "Epoch 4386, Loss: 0.005110440033604391, Final Batch Loss: 8.444119157502428e-05\n",
      "Epoch 4387, Loss: 0.0008260051690740511, Final Batch Loss: 3.407285112189129e-05\n",
      "Epoch 4388, Loss: 0.0007593353921038215, Final Batch Loss: 0.000124949001474306\n",
      "Epoch 4389, Loss: 0.01702482144901296, Final Batch Loss: 3.335541259730235e-05\n",
      "Epoch 4390, Loss: 0.026798454535310157, Final Batch Loss: 0.025791941210627556\n",
      "Epoch 4391, Loss: 0.002930292228484177, Final Batch Loss: 2.349168607906904e-05\n",
      "Epoch 4392, Loss: 0.0019511695645633154, Final Batch Loss: 0.0014219710137695074\n",
      "Epoch 4393, Loss: 0.029572037106845528, Final Batch Loss: 9.53910275711678e-05\n",
      "Epoch 4394, Loss: 0.010499863361474127, Final Batch Loss: 0.008743949234485626\n",
      "Epoch 4395, Loss: 0.0027300397014187183, Final Batch Loss: 2.0929313905071467e-05\n",
      "Epoch 4396, Loss: 0.007887580719398102, Final Batch Loss: 5.9675221564248204e-05\n",
      "Epoch 4397, Loss: 0.008202844495826866, Final Batch Loss: 0.005531489383429289\n",
      "Epoch 4398, Loss: 0.0011013658258889336, Final Batch Loss: 6.908442446729168e-05\n",
      "Epoch 4399, Loss: 0.0010146834847546415, Final Batch Loss: 1.9274741134722717e-05\n",
      "Epoch 4400, Loss: 0.00422225940565113, Final Batch Loss: 3.1023591873236e-05\n",
      "Epoch 4401, Loss: 0.007102582394509227, Final Batch Loss: 0.006481785327196121\n",
      "Epoch 4402, Loss: 0.0019135265865770634, Final Batch Loss: 0.0009119434398598969\n",
      "Epoch 4403, Loss: 0.0008453899354208261, Final Batch Loss: 0.0002519247937016189\n",
      "Epoch 4404, Loss: 0.0007093662934494205, Final Batch Loss: 6.428899359889328e-05\n",
      "Epoch 4405, Loss: 0.007386794110061601, Final Batch Loss: 4.392178379930556e-05\n",
      "Epoch 4406, Loss: 0.003107708977040602, Final Batch Loss: 5.5917789723025635e-05\n",
      "Epoch 4407, Loss: 0.0007784471581544494, Final Batch Loss: 0.0003631548606790602\n",
      "Epoch 4408, Loss: 0.002956103766337037, Final Batch Loss: 6.506260251626372e-05\n",
      "Epoch 4409, Loss: 0.000773991134337848, Final Batch Loss: 0.0002522361173760146\n",
      "Epoch 4410, Loss: 0.0007481183329218766, Final Batch Loss: 2.2447367882705294e-05\n",
      "Epoch 4411, Loss: 0.0005436104875116143, Final Batch Loss: 0.00017796095926314592\n",
      "Epoch 4412, Loss: 0.013418163580354303, Final Batch Loss: 0.0003874617977999151\n",
      "Epoch 4413, Loss: 0.0020920076094625983, Final Batch Loss: 0.00018822072888724506\n",
      "Epoch 4414, Loss: 0.0012378984320093878, Final Batch Loss: 0.00040046058711595833\n",
      "Epoch 4415, Loss: 0.005448666437587235, Final Batch Loss: 0.0011061558034271002\n",
      "Epoch 4416, Loss: 0.0035066912496404257, Final Batch Loss: 0.001555636408738792\n",
      "Epoch 4417, Loss: 0.008872677408362506, Final Batch Loss: 3.550429028109647e-05\n",
      "Epoch 4418, Loss: 0.005732335208449513, Final Batch Loss: 0.002067750319838524\n",
      "Epoch 4419, Loss: 0.009491095977864461, Final Batch Loss: 2.4000832127057947e-05\n",
      "Epoch 4420, Loss: 0.0014918204651621636, Final Batch Loss: 0.0009475084370933473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4421, Loss: 0.0007675905508222058, Final Batch Loss: 3.7810998037457466e-05\n",
      "Epoch 4422, Loss: 0.004242723349307198, Final Batch Loss: 0.00030993152176961303\n",
      "Epoch 4423, Loss: 0.013911400899814907, Final Batch Loss: 6.91243403707631e-05\n",
      "Epoch 4424, Loss: 0.002024511111812899, Final Batch Loss: 2.887768641812727e-05\n",
      "Epoch 4425, Loss: 0.010867129698453937, Final Batch Loss: 0.000102014142612461\n",
      "Epoch 4426, Loss: 0.0036278085244703107, Final Batch Loss: 0.0027610179968178272\n",
      "Epoch 4427, Loss: 0.0005589556858467404, Final Batch Loss: 7.842714694561437e-05\n",
      "Epoch 4428, Loss: 0.0024704813586140517, Final Batch Loss: 0.0005515404045581818\n",
      "Epoch 4429, Loss: 0.00040432926198263885, Final Batch Loss: 9.298694749304559e-06\n",
      "Epoch 4430, Loss: 0.010332253121305257, Final Batch Loss: 4.473633453017101e-05\n",
      "Epoch 4431, Loss: 0.0007362143023783574, Final Batch Loss: 0.0001915816537803039\n",
      "Epoch 4432, Loss: 0.002088086623189156, Final Batch Loss: 0.0002526574535295367\n",
      "Epoch 4433, Loss: 0.0018244420816699858, Final Batch Loss: 1.2428808076947462e-05\n",
      "Epoch 4434, Loss: 0.014342657683300786, Final Batch Loss: 0.00023095820506568998\n",
      "Epoch 4435, Loss: 0.007310798479011282, Final Batch Loss: 7.726819603703916e-05\n",
      "Epoch 4436, Loss: 0.0008683744890731759, Final Batch Loss: 0.00038169094477780163\n",
      "Epoch 4437, Loss: 0.00031013206535135396, Final Batch Loss: 9.176742605632171e-05\n",
      "Epoch 4438, Loss: 0.0020912417603540234, Final Batch Loss: 0.0014286547666415572\n",
      "Epoch 4439, Loss: 0.0015878720387263456, Final Batch Loss: 2.19209414353827e-05\n",
      "Epoch 4440, Loss: 0.001179833752757986, Final Batch Loss: 5.202403917792253e-05\n",
      "Epoch 4441, Loss: 0.013124368619173765, Final Batch Loss: 0.0001246186875505373\n",
      "Epoch 4442, Loss: 0.00420970097911777, Final Batch Loss: 9.81642515398562e-05\n",
      "Epoch 4443, Loss: 0.0005082666757516563, Final Batch Loss: 2.4549073714297265e-05\n",
      "Epoch 4444, Loss: 0.001410764480169746, Final Batch Loss: 8.541881834389642e-05\n",
      "Epoch 4445, Loss: 0.0008728703505767044, Final Batch Loss: 0.000244060080149211\n",
      "Epoch 4446, Loss: 0.0015929255691844446, Final Batch Loss: 6.2374324443226214e-06\n",
      "Epoch 4447, Loss: 0.0008900469983927906, Final Batch Loss: 0.00019928670371882617\n",
      "Epoch 4448, Loss: 0.0010966573245241307, Final Batch Loss: 0.0006550193065777421\n",
      "Epoch 4449, Loss: 0.0008168613649104373, Final Batch Loss: 8.457276999251917e-05\n",
      "Epoch 4450, Loss: 0.00695252232162602, Final Batch Loss: 2.8601198209798895e-06\n",
      "Epoch 4451, Loss: 0.0014274208797360188, Final Batch Loss: 1.0434531759528909e-05\n",
      "Epoch 4452, Loss: 0.003385554562555626, Final Batch Loss: 0.0003541549958754331\n",
      "Epoch 4453, Loss: 0.0011860088798130164, Final Batch Loss: 2.0026838683406822e-05\n",
      "Epoch 4454, Loss: 0.0018169692666560877, Final Batch Loss: 4.233544314047322e-05\n",
      "Epoch 4455, Loss: 0.0003445962465775665, Final Batch Loss: 4.711726796813309e-05\n",
      "Epoch 4456, Loss: 0.00045720012803940335, Final Batch Loss: 0.00013349662185646594\n",
      "Epoch 4457, Loss: 0.004554653449304169, Final Batch Loss: 2.9703920517931692e-05\n",
      "Epoch 4458, Loss: 0.0010854422844204237, Final Batch Loss: 4.341055500844959e-06\n",
      "Epoch 4459, Loss: 0.0008807399808574701, Final Batch Loss: 1.2821410564356484e-05\n",
      "Epoch 4460, Loss: 0.003656323955510743, Final Batch Loss: 0.0024881530553102493\n",
      "Epoch 4461, Loss: 0.001505810482740344, Final Batch Loss: 9.090591629501432e-05\n",
      "Epoch 4462, Loss: 0.0011745543342840392, Final Batch Loss: 0.0009886721381917596\n",
      "Epoch 4463, Loss: 0.0013715981324367021, Final Batch Loss: 3.7668426102754893e-06\n",
      "Epoch 4464, Loss: 0.0005546993952521007, Final Batch Loss: 4.52968743047677e-05\n",
      "Epoch 4465, Loss: 0.0005734446422138717, Final Batch Loss: 3.2121741242008284e-05\n",
      "Epoch 4466, Loss: 0.0021510694286916987, Final Batch Loss: 7.203547284007072e-05\n",
      "Epoch 4467, Loss: 0.0005765264058936737, Final Batch Loss: 4.505427568801679e-05\n",
      "Epoch 4468, Loss: 0.000450282637757482, Final Batch Loss: 0.0001285125908907503\n",
      "Epoch 4469, Loss: 0.003420736146381387, Final Batch Loss: 2.3600030544912443e-05\n",
      "Epoch 4470, Loss: 0.000767357177210215, Final Batch Loss: 0.0003526417422108352\n",
      "Epoch 4471, Loss: 0.0010267403940815711, Final Batch Loss: 0.000728629354853183\n",
      "Epoch 4472, Loss: 0.0008892480109352618, Final Batch Loss: 9.152732673101127e-05\n",
      "Epoch 4473, Loss: 0.00427980851236498, Final Batch Loss: 0.0003816356766037643\n",
      "Epoch 4474, Loss: 0.0035160868501407094, Final Batch Loss: 3.306083453935571e-05\n",
      "Epoch 4475, Loss: 0.026288239076166064, Final Batch Loss: 0.0004061000654473901\n",
      "Epoch 4476, Loss: 0.002363145120398258, Final Batch Loss: 0.00010962765372823924\n",
      "Epoch 4477, Loss: 0.011884170708071906, Final Batch Loss: 0.00458897789940238\n",
      "Epoch 4478, Loss: 0.003370873411768116, Final Batch Loss: 0.00016771425725892186\n",
      "Epoch 4479, Loss: 0.01126875023737739, Final Batch Loss: 1.2858422451245133e-05\n",
      "Epoch 4480, Loss: 0.01059698740937165, Final Batch Loss: 0.0031171853188425303\n",
      "Epoch 4481, Loss: 0.0363631248828824, Final Batch Loss: 0.0003088169323746115\n",
      "Epoch 4482, Loss: 0.00044527151112561114, Final Batch Loss: 0.0002465847646817565\n",
      "Epoch 4483, Loss: 0.023021474911729456, Final Batch Loss: 0.00037285368307493627\n",
      "Epoch 4484, Loss: 0.03821251120825764, Final Batch Loss: 0.00918705202639103\n",
      "Epoch 4485, Loss: 0.0004308693833081634, Final Batch Loss: 0.00010444228246342391\n",
      "Epoch 4486, Loss: 0.007002168997132685, Final Batch Loss: 2.9879265639465302e-05\n",
      "Epoch 4487, Loss: 0.002387565080425702, Final Batch Loss: 6.704954284941778e-05\n",
      "Epoch 4488, Loss: 0.003734453821380157, Final Batch Loss: 5.684890493284911e-05\n",
      "Epoch 4489, Loss: 0.0013310223439475521, Final Batch Loss: 7.230607297969982e-05\n",
      "Epoch 4490, Loss: 0.004344227074398077, Final Batch Loss: 0.0004503233649302274\n",
      "Epoch 4491, Loss: 0.000708352984929661, Final Batch Loss: 0.00018994344281964004\n",
      "Epoch 4492, Loss: 0.002474167309628683, Final Batch Loss: 0.000763549527619034\n",
      "Epoch 4493, Loss: 0.0008107684716378571, Final Batch Loss: 4.307667040848173e-06\n",
      "Epoch 4494, Loss: 0.0010043147431133548, Final Batch Loss: 0.0007448377436958253\n",
      "Epoch 4495, Loss: 0.0032542748740524985, Final Batch Loss: 0.0027739915531128645\n",
      "Epoch 4496, Loss: 0.002126261318608158, Final Batch Loss: 4.537790573522216e-06\n",
      "Epoch 4497, Loss: 0.001335136556008365, Final Batch Loss: 0.0005783813539892435\n",
      "Epoch 4498, Loss: 0.006422421623938135, Final Batch Loss: 1.7534364815219305e-05\n",
      "Epoch 4499, Loss: 0.0006337204795272555, Final Batch Loss: 7.529555296059698e-05\n",
      "Epoch 4500, Loss: 0.0008251601720985491, Final Batch Loss: 0.00016884268552530557\n",
      "Epoch 4501, Loss: 0.0006410390997189097, Final Batch Loss: 0.00036384904524311423\n",
      "Epoch 4502, Loss: 0.0009692047970020212, Final Batch Loss: 2.543823757150676e-05\n",
      "Epoch 4503, Loss: 0.000634455134786549, Final Batch Loss: 8.79969411471393e-06\n",
      "Epoch 4504, Loss: 0.0003365204429428559, Final Batch Loss: 0.00011706524674082175\n",
      "Epoch 4505, Loss: 0.01112173776527925, Final Batch Loss: 0.0003602999495342374\n",
      "Epoch 4506, Loss: 0.011607242611717083, Final Batch Loss: 0.0007189234020188451\n",
      "Epoch 4507, Loss: 0.000924058385862736, Final Batch Loss: 1.073979183274787e-05\n",
      "Epoch 4508, Loss: 0.001871894078931291, Final Batch Loss: 0.0008434879709966481\n",
      "Epoch 4509, Loss: 0.0007731184741714969, Final Batch Loss: 0.000323556101648137\n",
      "Epoch 4510, Loss: 0.0005072440708318027, Final Batch Loss: 2.2495702069136314e-05\n",
      "Epoch 4511, Loss: 0.0008212313787225867, Final Batch Loss: 2.5895098588080145e-05\n",
      "Epoch 4512, Loss: 0.0033719899390689534, Final Batch Loss: 3.6975168313801987e-06\n",
      "Epoch 4513, Loss: 0.0013155268261471065, Final Batch Loss: 0.00011455873027443886\n",
      "Epoch 4514, Loss: 0.0017243509573745541, Final Batch Loss: 0.0007342941826209426\n",
      "Epoch 4515, Loss: 0.01982882108859485, Final Batch Loss: 6.089602902648039e-05\n",
      "Epoch 4516, Loss: 0.038459516463262844, Final Batch Loss: 7.152782200137153e-05\n",
      "Epoch 4517, Loss: 0.013738683839619625, Final Batch Loss: 0.00011058693780796602\n",
      "Epoch 4518, Loss: 0.007976599657922634, Final Batch Loss: 0.003989438060671091\n",
      "Epoch 4519, Loss: 0.0037501536316995043, Final Batch Loss: 1.5553308912785724e-05\n",
      "Epoch 4520, Loss: 0.017212828795891255, Final Batch Loss: 0.0073072887025773525\n",
      "Epoch 4521, Loss: 0.012657767438213341, Final Batch Loss: 0.0005228281952440739\n",
      "Epoch 4522, Loss: 0.0016330124381056521, Final Batch Loss: 1.9330092982272618e-05\n",
      "Epoch 4523, Loss: 0.001215177559060976, Final Batch Loss: 0.0003828249464277178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4524, Loss: 0.0024061434596660547, Final Batch Loss: 8.757027535466477e-05\n",
      "Epoch 4525, Loss: 0.010163312836084515, Final Batch Loss: 0.00109297432936728\n",
      "Epoch 4526, Loss: 0.0008949198872869601, Final Batch Loss: 0.0007068770937621593\n",
      "Epoch 4527, Loss: 0.0007825769275768835, Final Batch Loss: 4.384957264846889e-06\n",
      "Epoch 4528, Loss: 0.00395037683119881, Final Batch Loss: 0.0008470123284496367\n",
      "Epoch 4529, Loss: 0.0010905580420512706, Final Batch Loss: 0.0001310614898102358\n",
      "Epoch 4530, Loss: 0.0003622505646490026, Final Batch Loss: 1.6348316421499476e-05\n",
      "Epoch 4531, Loss: 0.012046672662108904, Final Batch Loss: 0.0011079743271693587\n",
      "Epoch 4532, Loss: 0.0010142193746105477, Final Batch Loss: 7.2566376729810145e-06\n",
      "Epoch 4533, Loss: 0.003485160324999015, Final Batch Loss: 2.6596788302413188e-05\n",
      "Epoch 4534, Loss: 0.01296490547247231, Final Batch Loss: 0.00046322093112394214\n",
      "Epoch 4535, Loss: 0.023560165720482473, Final Batch Loss: 3.2552663469687104e-05\n",
      "Epoch 4536, Loss: 0.03567250768537633, Final Batch Loss: 0.026336245238780975\n",
      "Epoch 4537, Loss: 0.023701460217125714, Final Batch Loss: 4.83954900118988e-05\n",
      "Epoch 4538, Loss: 0.00321445969530032, Final Batch Loss: 4.4207568862475455e-05\n",
      "Epoch 4539, Loss: 0.017922810422533075, Final Batch Loss: 0.01450930442661047\n",
      "Epoch 4540, Loss: 0.01843972081405809, Final Batch Loss: 2.53838297794573e-05\n",
      "Epoch 4541, Loss: 0.002583293900897843, Final Batch Loss: 1.6681491615599953e-05\n",
      "Epoch 4542, Loss: 0.002105877041685744, Final Batch Loss: 0.0008844517287798226\n",
      "Epoch 4543, Loss: 0.027603758629993536, Final Batch Loss: 0.021646233275532722\n",
      "Epoch 4544, Loss: 0.001046785262587946, Final Batch Loss: 7.813956472091377e-06\n",
      "Epoch 4545, Loss: 0.0018337135343244881, Final Batch Loss: 5.41496501682559e-06\n",
      "Epoch 4546, Loss: 0.021392634491348872, Final Batch Loss: 0.0006882072775624692\n",
      "Epoch 4547, Loss: 0.0005512429233931471, Final Batch Loss: 0.0002101800637319684\n",
      "Epoch 4548, Loss: 0.003492309871944599, Final Batch Loss: 0.0005948450998403132\n",
      "Epoch 4549, Loss: 0.003167725073581096, Final Batch Loss: 9.821949788602069e-05\n",
      "Epoch 4550, Loss: 0.005635309851641068, Final Batch Loss: 0.002658513840287924\n",
      "Epoch 4551, Loss: 0.0015006550602265634, Final Batch Loss: 0.00026508120936341584\n",
      "Epoch 4552, Loss: 0.0010607369003992062, Final Batch Loss: 7.499523781007156e-05\n",
      "Epoch 4553, Loss: 0.0005527356333914213, Final Batch Loss: 9.274768672185019e-05\n",
      "Epoch 4554, Loss: 0.0008661428037157748, Final Batch Loss: 0.000363663217285648\n",
      "Epoch 4555, Loss: 0.006189440149682923, Final Batch Loss: 0.005584621801972389\n",
      "Epoch 4556, Loss: 0.0019294718113087583, Final Batch Loss: 0.0007870958652347326\n",
      "Epoch 4557, Loss: 0.0038221674258238636, Final Batch Loss: 0.00027960652369074523\n",
      "Epoch 4558, Loss: 0.005677592616848415, Final Batch Loss: 0.0001536709169158712\n",
      "Epoch 4559, Loss: 0.0017837921259342693, Final Batch Loss: 8.62430315464735e-05\n",
      "Epoch 4560, Loss: 0.0017382196092512459, Final Batch Loss: 2.5739871489349753e-05\n",
      "Epoch 4561, Loss: 0.0006388145829987479, Final Batch Loss: 2.8908345484524034e-05\n",
      "Epoch 4562, Loss: 0.0016709157080185832, Final Batch Loss: 1.2334743587416597e-05\n",
      "Epoch 4563, Loss: 0.0012260042167326901, Final Batch Loss: 0.00012411635543685406\n",
      "Epoch 4564, Loss: 0.0011006470467691543, Final Batch Loss: 6.85737031744793e-05\n",
      "Epoch 4565, Loss: 0.00015406356578751002, Final Batch Loss: 2.408174259471707e-05\n",
      "Epoch 4566, Loss: 0.00042421038961037993, Final Batch Loss: 7.273258961504325e-05\n",
      "Epoch 4567, Loss: 0.000600304778345162, Final Batch Loss: 0.00037584113306365907\n",
      "Epoch 4568, Loss: 0.0006382380252034636, Final Batch Loss: 2.3373639123747125e-05\n",
      "Epoch 4569, Loss: 0.0010283267074555624, Final Batch Loss: 4.709852873929776e-05\n",
      "Epoch 4570, Loss: 0.0032073467737063766, Final Batch Loss: 0.002979486482217908\n",
      "Epoch 4571, Loss: 0.00020510913964244537, Final Batch Loss: 6.573349673999473e-05\n",
      "Epoch 4572, Loss: 0.004745788012769481, Final Batch Loss: 1.375292686134344e-05\n",
      "Epoch 4573, Loss: 0.0013960394148853084, Final Batch Loss: 0.0005314541049301624\n",
      "Epoch 4574, Loss: 0.0003370621152498643, Final Batch Loss: 1.1816696314781439e-05\n",
      "Epoch 4575, Loss: 0.0003248214202358213, Final Batch Loss: 4.405752042657696e-05\n",
      "Epoch 4576, Loss: 0.0005903044825572579, Final Batch Loss: 6.179305728437612e-06\n",
      "Epoch 4577, Loss: 0.0008013007882254897, Final Batch Loss: 1.0655367077561095e-05\n",
      "Epoch 4578, Loss: 0.0006581605530300294, Final Batch Loss: 5.6516470067435876e-05\n",
      "Epoch 4579, Loss: 0.0019991896697320044, Final Batch Loss: 0.0001239149714820087\n",
      "Epoch 4580, Loss: 0.011015416736427142, Final Batch Loss: 0.01094149425625801\n",
      "Epoch 4581, Loss: 0.0011326362136969692, Final Batch Loss: 0.0007946151308715343\n",
      "Epoch 4582, Loss: 0.023114586343581323, Final Batch Loss: 0.00011942074343096465\n",
      "Epoch 4583, Loss: 0.059324928359274054, Final Batch Loss: 0.012532987631857395\n",
      "Epoch 4584, Loss: 0.014837602798252192, Final Batch Loss: 1.2424637134245131e-05\n",
      "Epoch 4585, Loss: 0.005933252487011487, Final Batch Loss: 0.0008371946169063449\n",
      "Epoch 4586, Loss: 0.020312255626777187, Final Batch Loss: 0.002961890073493123\n",
      "Epoch 4587, Loss: 0.005882600351469591, Final Batch Loss: 0.00135824887547642\n",
      "Epoch 4588, Loss: 0.002661249047378078, Final Batch Loss: 0.0002474435605108738\n",
      "Epoch 4589, Loss: 0.03584437276003882, Final Batch Loss: 0.0006831765058450401\n",
      "Epoch 4590, Loss: 0.004876028586295433, Final Batch Loss: 0.003707882482558489\n",
      "Epoch 4591, Loss: 0.010728565792760492, Final Batch Loss: 4.856049145018915e-06\n",
      "Epoch 4592, Loss: 0.006651129195233807, Final Batch Loss: 0.00014372322766575962\n",
      "Epoch 4593, Loss: 0.004804100250112242, Final Batch Loss: 0.00023069967573974282\n",
      "Epoch 4594, Loss: 0.011680755982524715, Final Batch Loss: 0.0004032079305034131\n",
      "Epoch 4595, Loss: 0.0037644873445970006, Final Batch Loss: 0.0010750122601166368\n",
      "Epoch 4596, Loss: 0.002553702296836491, Final Batch Loss: 3.215996230210294e-06\n",
      "Epoch 4597, Loss: 0.07379584424597851, Final Batch Loss: 2.0072486222488806e-05\n",
      "Epoch 4598, Loss: 0.001656125758017879, Final Batch Loss: 0.0009514529374428093\n",
      "Epoch 4599, Loss: 0.0050036514448947855, Final Batch Loss: 8.722066559130326e-05\n",
      "Epoch 4600, Loss: 0.044428799621528015, Final Batch Loss: 0.00021389767061918974\n",
      "Epoch 4601, Loss: 0.0008854878979036584, Final Batch Loss: 0.00017800497880671173\n",
      "Epoch 4602, Loss: 0.032137611520738574, Final Batch Loss: 0.030889179557561874\n",
      "Epoch 4603, Loss: 0.013094598327370477, Final Batch Loss: 0.005019381642341614\n",
      "Epoch 4604, Loss: 0.003975402305513853, Final Batch Loss: 0.003211011178791523\n",
      "Epoch 4605, Loss: 0.025963274834793992, Final Batch Loss: 0.00036802375689148903\n",
      "Epoch 4606, Loss: 0.010649795553035801, Final Batch Loss: 2.285238224430941e-05\n",
      "Epoch 4607, Loss: 0.03780755199841224, Final Batch Loss: 0.030876288190484047\n",
      "Epoch 4608, Loss: 0.014356987812789157, Final Batch Loss: 0.00045811920426785946\n",
      "Epoch 4609, Loss: 0.0016793981740192976, Final Batch Loss: 0.00014755634765606374\n",
      "Epoch 4610, Loss: 0.0026328642743465025, Final Batch Loss: 0.00012997331214137375\n",
      "Epoch 4611, Loss: 0.003811363209024421, Final Batch Loss: 0.00011270520190009847\n",
      "Epoch 4612, Loss: 0.011435642489232123, Final Batch Loss: 0.0002594010438770056\n",
      "Epoch 4613, Loss: 0.012767117768362368, Final Batch Loss: 4.645749413612066e-06\n",
      "Epoch 4614, Loss: 0.05502545376657508, Final Batch Loss: 0.006976977922022343\n",
      "Epoch 4615, Loss: 0.004509537141984765, Final Batch Loss: 4.405369963933481e-06\n",
      "Epoch 4616, Loss: 0.0027372322147130035, Final Batch Loss: 0.0012410854687914252\n",
      "Epoch 4617, Loss: 0.001438650270756625, Final Batch Loss: 9.260025399271399e-05\n",
      "Epoch 4618, Loss: 0.02249780945294333, Final Batch Loss: 1.0163971637666691e-05\n",
      "Epoch 4619, Loss: 0.011288734851405025, Final Batch Loss: 0.0005180887528695166\n",
      "Epoch 4620, Loss: 0.01199649834597949, Final Batch Loss: 0.00021955712873023003\n",
      "Epoch 4621, Loss: 0.0010316711850464344, Final Batch Loss: 2.9321861802600324e-05\n",
      "Epoch 4622, Loss: 0.004320776490203571, Final Batch Loss: 0.002748701721429825\n",
      "Epoch 4623, Loss: 0.04229631282032642, Final Batch Loss: 0.0002522194117773324\n",
      "Epoch 4624, Loss: 0.02812877950782422, Final Batch Loss: 0.024769848212599754\n",
      "Epoch 4625, Loss: 0.0020632252453651745, Final Batch Loss: 8.427460852544755e-05\n",
      "Epoch 4626, Loss: 0.0012953779878444038, Final Batch Loss: 5.86836613365449e-05\n",
      "Epoch 4627, Loss: 0.004016900958959013, Final Batch Loss: 0.00019474914006423205\n",
      "Epoch 4628, Loss: 0.002915828154073097, Final Batch Loss: 7.962439121911302e-05\n",
      "Epoch 4629, Loss: 0.002673861541552469, Final Batch Loss: 0.000523873430211097\n",
      "Epoch 4630, Loss: 0.0035578239112510346, Final Batch Loss: 0.00024039742129389197\n",
      "Epoch 4631, Loss: 0.0051245065551484, Final Batch Loss: 7.659978291485459e-05\n",
      "Epoch 4632, Loss: 0.0008372866545869329, Final Batch Loss: 4.681749942392344e-06\n",
      "Epoch 4633, Loss: 0.006192370738062891, Final Batch Loss: 2.0648583813454024e-05\n",
      "Epoch 4634, Loss: 0.0020129824988543987, Final Batch Loss: 0.000474236294394359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4635, Loss: 0.0016341518057743087, Final Batch Loss: 0.0004882743232883513\n",
      "Epoch 4636, Loss: 0.018396782877971418, Final Batch Loss: 0.0004318097489885986\n",
      "Epoch 4637, Loss: 0.0073585528352850815, Final Batch Loss: 1.5396535673062317e-05\n",
      "Epoch 4638, Loss: 0.017531134879391175, Final Batch Loss: 0.016561662778258324\n",
      "Epoch 4639, Loss: 0.0006657107915089, Final Batch Loss: 0.0003584538644645363\n",
      "Epoch 4640, Loss: 0.0006885601687827148, Final Batch Loss: 0.00010011107224272564\n",
      "Epoch 4641, Loss: 0.003157551105687162, Final Batch Loss: 2.9289953090483323e-05\n",
      "Epoch 4642, Loss: 0.0030079466487222817, Final Batch Loss: 0.00010595846106298268\n",
      "Epoch 4643, Loss: 0.0017337569188384805, Final Batch Loss: 0.0005082249408587813\n",
      "Epoch 4644, Loss: 0.008066618640441447, Final Batch Loss: 0.004831627476960421\n",
      "Epoch 4645, Loss: 0.00655096233367658, Final Batch Loss: 8.128125773509964e-05\n",
      "Epoch 4646, Loss: 0.0013131969681126066, Final Batch Loss: 5.35106664756313e-05\n",
      "Epoch 4647, Loss: 0.003926069548469968, Final Batch Loss: 0.00031128429691307247\n",
      "Epoch 4648, Loss: 0.0031445808035641676, Final Batch Loss: 2.5942883439711295e-05\n",
      "Epoch 4649, Loss: 0.00648001813715382, Final Batch Loss: 8.759785487200134e-06\n",
      "Epoch 4650, Loss: 0.0013642006142617902, Final Batch Loss: 1.6479707483085804e-05\n",
      "Epoch 4651, Loss: 0.003659691012217081, Final Batch Loss: 0.0001522054080851376\n",
      "Epoch 4652, Loss: 0.030333814269397408, Final Batch Loss: 5.1833183533744887e-05\n",
      "Epoch 4653, Loss: 0.013384113306528889, Final Batch Loss: 0.00022604683181270957\n",
      "Epoch 4654, Loss: 0.007592535337607842, Final Batch Loss: 0.00017210838268510997\n",
      "Epoch 4655, Loss: 0.05134703884323244, Final Batch Loss: 8.037869702093303e-05\n",
      "Epoch 4656, Loss: 0.0059796095883939415, Final Batch Loss: 7.326591003220528e-05\n",
      "Epoch 4657, Loss: 0.001557849085656926, Final Batch Loss: 0.00030431768391281366\n",
      "Epoch 4658, Loss: 0.00945167906320421, Final Batch Loss: 0.0011745640076696873\n",
      "Epoch 4659, Loss: 0.001869071289547719, Final Batch Loss: 0.0003974174614995718\n",
      "Epoch 4660, Loss: 0.0018685684954107273, Final Batch Loss: 0.0008178142597898841\n",
      "Epoch 4661, Loss: 0.0023683077915848116, Final Batch Loss: 0.0001243680017068982\n",
      "Epoch 4662, Loss: 0.0013725248209084384, Final Batch Loss: 5.497930396813899e-05\n",
      "Epoch 4663, Loss: 0.0008395478917009314, Final Batch Loss: 3.464342080405913e-05\n",
      "Epoch 4664, Loss: 0.0018665681718630367, Final Batch Loss: 7.099134563759435e-06\n",
      "Epoch 4665, Loss: 0.0007420300680678338, Final Batch Loss: 9.177843458019197e-05\n",
      "Epoch 4666, Loss: 0.0012102333712391555, Final Batch Loss: 5.886835424462333e-05\n",
      "Epoch 4667, Loss: 0.004519731210166356, Final Batch Loss: 0.004023187328130007\n",
      "Epoch 4668, Loss: 0.0013346526029636152, Final Batch Loss: 0.00018389051547273993\n",
      "Epoch 4669, Loss: 0.00047789507334528025, Final Batch Loss: 0.00014983644359745085\n",
      "Epoch 4670, Loss: 0.004142630403293879, Final Batch Loss: 0.0003079244925174862\n",
      "Epoch 4671, Loss: 0.016255104087349537, Final Batch Loss: 3.0820290248811943e-06\n",
      "Epoch 4672, Loss: 0.0018612125822983216, Final Batch Loss: 2.2249103494687006e-05\n",
      "Epoch 4673, Loss: 0.0010618341111694463, Final Batch Loss: 7.224678120110184e-05\n",
      "Epoch 4674, Loss: 0.005013273845634103, Final Batch Loss: 7.021916189842159e-06\n",
      "Epoch 4675, Loss: 0.003953144241677364, Final Batch Loss: 3.626693069236353e-05\n",
      "Epoch 4676, Loss: 0.0007920446205389453, Final Batch Loss: 3.417190964682959e-05\n",
      "Epoch 4677, Loss: 0.0003675960811051482, Final Batch Loss: 6.252669845707715e-05\n",
      "Epoch 4678, Loss: 0.0003665798867587, Final Batch Loss: 1.7435279005439952e-05\n",
      "Epoch 4679, Loss: 0.0006879307893541409, Final Batch Loss: 0.0004689279303420335\n",
      "Epoch 4680, Loss: 0.0013237050916359294, Final Batch Loss: 3.8083318941062316e-05\n",
      "Epoch 4681, Loss: 0.023012030600511935, Final Batch Loss: 8.798132330412045e-05\n",
      "Epoch 4682, Loss: 0.0009001903536045575, Final Batch Loss: 1.0195851245953236e-05\n",
      "Epoch 4683, Loss: 0.004735959169920534, Final Batch Loss: 0.0017199746798723936\n",
      "Epoch 4684, Loss: 0.004631523159332573, Final Batch Loss: 0.00011636549606919289\n",
      "Epoch 4685, Loss: 0.06033902354829479, Final Batch Loss: 0.05235965922474861\n",
      "Epoch 4686, Loss: 0.0011060840042773634, Final Batch Loss: 3.997980093117803e-05\n",
      "Epoch 4687, Loss: 0.0006779906070732977, Final Batch Loss: 0.00011047937732655555\n",
      "Epoch 4688, Loss: 0.025512453761621146, Final Batch Loss: 4.248483674018644e-05\n",
      "Epoch 4689, Loss: 0.0008495043202856323, Final Batch Loss: 1.7646456399234012e-05\n",
      "Epoch 4690, Loss: 0.003962651353504043, Final Batch Loss: 0.0033984468318521976\n",
      "Epoch 4691, Loss: 0.000558676355467469, Final Batch Loss: 7.866398846090306e-06\n",
      "Epoch 4692, Loss: 0.0019253463433415163, Final Batch Loss: 6.720537930959836e-05\n",
      "Epoch 4693, Loss: 0.000402746518375352, Final Batch Loss: 4.2916464735753834e-05\n",
      "Epoch 4694, Loss: 0.0009984611679101363, Final Batch Loss: 6.817247049184516e-05\n",
      "Epoch 4695, Loss: 0.005566749616264133, Final Batch Loss: 0.0006704652914777398\n",
      "Epoch 4696, Loss: 0.000632739502179902, Final Batch Loss: 5.817006604047492e-05\n",
      "Epoch 4697, Loss: 0.0012685209585470147, Final Batch Loss: 7.590594032080844e-05\n",
      "Epoch 4698, Loss: 0.0008296838059322909, Final Batch Loss: 0.0001055138127412647\n",
      "Epoch 4699, Loss: 0.005223154519626405, Final Batch Loss: 9.757868974702433e-05\n",
      "Epoch 4700, Loss: 0.0018727461065282114, Final Batch Loss: 0.0004708284395746887\n",
      "Epoch 4701, Loss: 0.002653860421560239, Final Batch Loss: 1.7334627045784146e-05\n",
      "Epoch 4702, Loss: 0.003460884476226056, Final Batch Loss: 0.00022794805408921093\n",
      "Epoch 4703, Loss: 0.003035294881556183, Final Batch Loss: 0.0003935221175197512\n",
      "Epoch 4704, Loss: 0.0004867993648076663, Final Batch Loss: 1.7324373402516358e-05\n",
      "Epoch 4705, Loss: 0.009081027349566284, Final Batch Loss: 3.259484583395533e-05\n",
      "Epoch 4706, Loss: 0.023043127188429935, Final Batch Loss: 0.02242838405072689\n",
      "Epoch 4707, Loss: 0.006325114831270184, Final Batch Loss: 0.002672133268788457\n",
      "Epoch 4708, Loss: 0.0031155768083408475, Final Batch Loss: 0.00015280992374755442\n",
      "Epoch 4709, Loss: 0.0040361863721045665, Final Batch Loss: 3.1997326004784554e-05\n",
      "Epoch 4710, Loss: 0.0037759844526590314, Final Batch Loss: 8.412426541326568e-05\n",
      "Epoch 4711, Loss: 0.0009289485260524089, Final Batch Loss: 4.478704067878425e-05\n",
      "Epoch 4712, Loss: 0.014206813837517984, Final Batch Loss: 0.0002739836345426738\n",
      "Epoch 4713, Loss: 0.004532523784746445, Final Batch Loss: 3.075638596783392e-05\n",
      "Epoch 4714, Loss: 0.0038846445168019272, Final Batch Loss: 6.469582876889035e-05\n",
      "Epoch 4715, Loss: 0.0015919243269308936, Final Batch Loss: 5.136607796885073e-05\n",
      "Epoch 4716, Loss: 0.0003086483011429664, Final Batch Loss: 2.755137757048942e-05\n",
      "Epoch 4717, Loss: 0.0010586731041257735, Final Batch Loss: 3.6145986086921766e-05\n",
      "Epoch 4718, Loss: 0.00019717942450370174, Final Batch Loss: 6.119212048361078e-05\n",
      "Epoch 4719, Loss: 0.0012047964100929676, Final Batch Loss: 0.00029070605523884296\n",
      "Epoch 4720, Loss: 0.00040244935189548414, Final Batch Loss: 0.0001743265602272004\n",
      "Epoch 4721, Loss: 0.0009261265931854723, Final Batch Loss: 0.00011222303146496415\n",
      "Epoch 4722, Loss: 0.04824895154524711, Final Batch Loss: 2.0923747797496617e-05\n",
      "Epoch 4723, Loss: 0.0026367111277068034, Final Batch Loss: 3.9594870031578466e-05\n",
      "Epoch 4724, Loss: 0.0014134182638372295, Final Batch Loss: 0.00011638028081506491\n",
      "Epoch 4725, Loss: 0.0012768084725394147, Final Batch Loss: 6.844294694019482e-05\n",
      "Epoch 4726, Loss: 0.0006028147436154541, Final Batch Loss: 0.0002455925277899951\n",
      "Epoch 4727, Loss: 0.0007722554801148362, Final Batch Loss: 1.1324540537316352e-05\n",
      "Epoch 4728, Loss: 0.0007442280402756296, Final Batch Loss: 0.0001895146124297753\n",
      "Epoch 4729, Loss: 0.004316347594794934, Final Batch Loss: 2.3675000193179585e-05\n",
      "Epoch 4730, Loss: 0.001659919833400636, Final Batch Loss: 9.015886462293565e-05\n",
      "Epoch 4731, Loss: 0.00038173143184394576, Final Batch Loss: 3.39725156663917e-05\n",
      "Epoch 4732, Loss: 0.00023838043216528604, Final Batch Loss: 9.356321970699355e-05\n",
      "Epoch 4733, Loss: 0.0003306897488073446, Final Batch Loss: 7.479832856915891e-05\n",
      "Epoch 4734, Loss: 0.0005074053260614164, Final Batch Loss: 2.377746204729192e-05\n",
      "Epoch 4735, Loss: 0.00033367055402777623, Final Batch Loss: 1.348206842521904e-05\n",
      "Epoch 4736, Loss: 0.0014788052867515944, Final Batch Loss: 4.456190799828619e-05\n",
      "Epoch 4737, Loss: 0.0008666010935485247, Final Batch Loss: 0.0006832124199718237\n",
      "Epoch 4738, Loss: 0.0005450819603538548, Final Batch Loss: 6.580326498806244e-06\n",
      "Epoch 4739, Loss: 9.984941880247789e-05, Final Batch Loss: 1.0222001037618611e-05\n",
      "Epoch 4740, Loss: 0.0008870649703567324, Final Batch Loss: 7.234169515868416e-06\n",
      "Epoch 4741, Loss: 0.00036485256896412466, Final Batch Loss: 3.6265224480303004e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4742, Loss: 0.00034370615821899264, Final Batch Loss: 0.00010158644727198407\n",
      "Epoch 4743, Loss: 0.0005974125961074606, Final Batch Loss: 7.901758363004774e-05\n",
      "Epoch 4744, Loss: 0.0006556500238730223, Final Batch Loss: 0.00011035437637474388\n",
      "Epoch 4745, Loss: 0.0004209249846098828, Final Batch Loss: 0.00011533797805896029\n",
      "Epoch 4746, Loss: 0.0007733778566034744, Final Batch Loss: 0.00021977940923534334\n",
      "Epoch 4747, Loss: 0.0005699388857465237, Final Batch Loss: 6.396368553396314e-05\n",
      "Epoch 4748, Loss: 0.025672039325399965, Final Batch Loss: 7.530198217864381e-06\n",
      "Epoch 4749, Loss: 0.0009452518775106, Final Batch Loss: 1.5625539163011126e-05\n",
      "Epoch 4750, Loss: 0.0005898630133742699, Final Batch Loss: 0.00020187975314911455\n",
      "Epoch 4751, Loss: 0.0007336890885198954, Final Batch Loss: 3.559183460311033e-05\n",
      "Epoch 4752, Loss: 0.000591274078033166, Final Batch Loss: 4.332894604885951e-05\n",
      "Epoch 4753, Loss: 0.0004647938330890611, Final Batch Loss: 8.064300345722586e-05\n",
      "Epoch 4754, Loss: 0.0007875125920691062, Final Batch Loss: 0.0005242342012934387\n",
      "Epoch 4755, Loss: 0.0015129455205169506, Final Batch Loss: 0.00014807729166932404\n",
      "Epoch 4756, Loss: 0.022261208096551854, Final Batch Loss: 0.003554317867383361\n",
      "Epoch 4757, Loss: 0.0005079625416328781, Final Batch Loss: 3.175967503921129e-05\n",
      "Epoch 4758, Loss: 0.006630282194237225, Final Batch Loss: 0.0002205656055593863\n",
      "Epoch 4759, Loss: 0.014482617051726265, Final Batch Loss: 3.764872963074595e-05\n",
      "Epoch 4760, Loss: 0.0003772592367567995, Final Batch Loss: 0.00010564242256805301\n",
      "Epoch 4761, Loss: 0.000510217856572126, Final Batch Loss: 2.556853723945096e-05\n",
      "Epoch 4762, Loss: 0.00039317099799518473, Final Batch Loss: 0.00014840853691566736\n",
      "Epoch 4763, Loss: 0.0008092945909083937, Final Batch Loss: 6.324391870293766e-05\n",
      "Epoch 4764, Loss: 0.0012061899906257167, Final Batch Loss: 0.00015256705228239298\n",
      "Epoch 4765, Loss: 0.0011938519855902996, Final Batch Loss: 7.460046617779881e-05\n",
      "Epoch 4766, Loss: 0.00038243001108639874, Final Batch Loss: 4.444216756382957e-05\n",
      "Epoch 4767, Loss: 0.0010982108360622078, Final Batch Loss: 0.0005255023716017604\n",
      "Epoch 4768, Loss: 0.0010684464241421665, Final Batch Loss: 0.0005198935396037996\n",
      "Epoch 4769, Loss: 0.0014311523809737992, Final Batch Loss: 0.0003513224655762315\n",
      "Epoch 4770, Loss: 0.00113560419777059, Final Batch Loss: 5.640054223476909e-05\n",
      "Epoch 4771, Loss: 0.0029402053432932007, Final Batch Loss: 1.1917373740288895e-05\n",
      "Epoch 4772, Loss: 0.0016734364526200807, Final Batch Loss: 1.848163446993567e-05\n",
      "Epoch 4773, Loss: 0.0007389464390143985, Final Batch Loss: 5.5759413953637704e-05\n",
      "Epoch 4774, Loss: 0.0008168426800239104, Final Batch Loss: 0.0001232166978297755\n",
      "Epoch 4775, Loss: 0.000556876879727497, Final Batch Loss: 0.00030286796391010284\n",
      "Epoch 4776, Loss: 0.0009462327391247527, Final Batch Loss: 0.00019084087398368865\n",
      "Epoch 4777, Loss: 0.0012700878778559854, Final Batch Loss: 3.4514763683546335e-05\n",
      "Epoch 4778, Loss: 0.02260046809351479, Final Batch Loss: 0.0007586305146105587\n",
      "Epoch 4779, Loss: 0.002080031170407892, Final Batch Loss: 0.00013770298392046243\n",
      "Epoch 4780, Loss: 0.000684372746036388, Final Batch Loss: 4.8214627895504236e-05\n",
      "Epoch 4781, Loss: 0.007308856196686975, Final Batch Loss: 1.4529809050145559e-05\n",
      "Epoch 4782, Loss: 0.000582737741751771, Final Batch Loss: 2.5016626750584692e-05\n",
      "Epoch 4783, Loss: 0.0024289381381095154, Final Batch Loss: 1.5571878975606523e-05\n",
      "Epoch 4784, Loss: 0.0022022154516889714, Final Batch Loss: 1.1947906386922114e-05\n",
      "Epoch 4785, Loss: 0.005199509037993266, Final Batch Loss: 0.00015784421702846885\n",
      "Epoch 4786, Loss: 0.0007808458904037252, Final Batch Loss: 6.097830191720277e-05\n",
      "Epoch 4787, Loss: 0.007892960049503017, Final Batch Loss: 7.238171383505687e-05\n",
      "Epoch 4788, Loss: 0.003513992717898873, Final Batch Loss: 3.3721455565682845e-06\n",
      "Epoch 4789, Loss: 0.00042668932201195275, Final Batch Loss: 8.511195483151823e-05\n",
      "Epoch 4790, Loss: 0.002050547104317957, Final Batch Loss: 8.152225200319663e-05\n",
      "Epoch 4791, Loss: 0.0008901291967049474, Final Batch Loss: 7.690243364777416e-05\n",
      "Epoch 4792, Loss: 0.0012979845259906142, Final Batch Loss: 7.736539373581763e-06\n",
      "Epoch 4793, Loss: 0.003457771453213354, Final Batch Loss: 4.076768163940869e-05\n",
      "Epoch 4794, Loss: 0.0002628182865009876, Final Batch Loss: 2.9807268219883554e-05\n",
      "Epoch 4795, Loss: 0.0014748364401384606, Final Batch Loss: 5.858025360794272e-06\n",
      "Epoch 4796, Loss: 0.0004965841480952804, Final Batch Loss: 1.6488289475091733e-05\n",
      "Epoch 4797, Loss: 0.00020781139573955443, Final Batch Loss: 1.896640424092766e-05\n",
      "Epoch 4798, Loss: 0.006354880726576084, Final Batch Loss: 5.0251008360646665e-05\n",
      "Epoch 4799, Loss: 0.00024294521858791995, Final Batch Loss: 3.083248884649947e-05\n",
      "Epoch 4800, Loss: 0.0008328515059474739, Final Batch Loss: 5.6606222642585635e-06\n",
      "Epoch 4801, Loss: 0.0002231557436971343, Final Batch Loss: 6.872311496408656e-05\n",
      "Epoch 4802, Loss: 0.0005289706623443635, Final Batch Loss: 2.8184136681375094e-05\n",
      "Epoch 4803, Loss: 0.0005339749332051724, Final Batch Loss: 9.027922351378947e-05\n",
      "Epoch 4804, Loss: 0.00042584214679664, Final Batch Loss: 0.0001005665835691616\n",
      "Epoch 4805, Loss: 0.00016461418772450997, Final Batch Loss: 3.155570084345527e-05\n",
      "Epoch 4806, Loss: 0.005469405896292301, Final Batch Loss: 7.045715756248683e-05\n",
      "Epoch 4807, Loss: 0.0004403553782594827, Final Batch Loss: 2.677532620509737e-06\n",
      "Epoch 4808, Loss: 0.00036636968434322625, Final Batch Loss: 5.681795300915837e-06\n",
      "Epoch 4809, Loss: 0.014067868358324631, Final Batch Loss: 1.2741335012833588e-05\n",
      "Epoch 4810, Loss: 0.0006181578223731776, Final Batch Loss: 4.107846962142503e-06\n",
      "Epoch 4811, Loss: 0.031894639558231574, Final Batch Loss: 0.021377231925725937\n",
      "Epoch 4812, Loss: 0.00021900631827520556, Final Batch Loss: 8.297972271975596e-06\n",
      "Epoch 4813, Loss: 0.038723609322914854, Final Batch Loss: 4.721492587123066e-05\n",
      "Epoch 4814, Loss: 0.06914369804144371, Final Batch Loss: 0.053353775292634964\n",
      "Epoch 4815, Loss: 0.001877780639915727, Final Batch Loss: 4.171956970822066e-05\n",
      "Epoch 4816, Loss: 0.002177323445721413, Final Batch Loss: 0.0012781659606844187\n",
      "Epoch 4817, Loss: 0.0026806831447174773, Final Batch Loss: 6.613100413233042e-05\n",
      "Epoch 4818, Loss: 0.0078251393060782, Final Batch Loss: 0.00030591804534196854\n",
      "Epoch 4819, Loss: 0.0018859206029446796, Final Batch Loss: 6.67655112920329e-05\n",
      "Epoch 4820, Loss: 0.008322064866661094, Final Batch Loss: 0.0021210885606706142\n",
      "Epoch 4821, Loss: 0.003638898831923143, Final Batch Loss: 0.0021482007578015327\n",
      "Epoch 4822, Loss: 0.001685802621068433, Final Batch Loss: 0.0007718734559603035\n",
      "Epoch 4823, Loss: 0.00498684305784991, Final Batch Loss: 0.0009327809093520045\n",
      "Epoch 4824, Loss: 0.001149923544289777, Final Batch Loss: 6.0959595430176705e-05\n",
      "Epoch 4825, Loss: 0.0029092652148392517, Final Batch Loss: 0.00012915546540170908\n",
      "Epoch 4826, Loss: 0.000865596994117368, Final Batch Loss: 0.00023055649944581091\n",
      "Epoch 4827, Loss: 0.000743418857382494, Final Batch Loss: 0.0002466931182425469\n",
      "Epoch 4828, Loss: 0.0012344887800281867, Final Batch Loss: 3.5271459637442604e-05\n",
      "Epoch 4829, Loss: 0.0007816591023583896, Final Batch Loss: 6.737224612152204e-05\n",
      "Epoch 4830, Loss: 0.0004599725471052807, Final Batch Loss: 1.0013354767579585e-05\n",
      "Epoch 4831, Loss: 0.039742910437780665, Final Batch Loss: 0.03406208008527756\n",
      "Epoch 4832, Loss: 0.0010304259703843854, Final Batch Loss: 3.655128239188343e-05\n",
      "Epoch 4833, Loss: 0.0049607007968006656, Final Batch Loss: 0.0006197396432980895\n",
      "Epoch 4834, Loss: 0.003393833540030755, Final Batch Loss: 0.00019531424914021045\n",
      "Epoch 4835, Loss: 0.02221384674339788, Final Batch Loss: 6.518876762129366e-05\n",
      "Epoch 4836, Loss: 0.0019397466348891612, Final Batch Loss: 9.081901953322813e-05\n",
      "Epoch 4837, Loss: 0.007608404357597465, Final Batch Loss: 0.0037484904751181602\n",
      "Epoch 4838, Loss: 0.002318788676348049, Final Batch Loss: 3.6954195820726454e-05\n",
      "Epoch 4839, Loss: 0.0013790755983791314, Final Batch Loss: 5.551801950787194e-05\n",
      "Epoch 4840, Loss: 0.006728593376465142, Final Batch Loss: 0.00024718884378671646\n",
      "Epoch 4841, Loss: 0.020859651227510767, Final Batch Loss: 2.675263021956198e-05\n",
      "Epoch 4842, Loss: 0.02364339284667949, Final Batch Loss: 8.014800550881773e-05\n",
      "Epoch 4843, Loss: 0.013741944074354251, Final Batch Loss: 0.00021825003204867244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4844, Loss: 0.005851442088896874, Final Batch Loss: 9.46120053413324e-05\n",
      "Epoch 4845, Loss: 0.0010939649291685782, Final Batch Loss: 9.887536725727841e-05\n",
      "Epoch 4846, Loss: 0.003663580631837249, Final Batch Loss: 0.00015889015048742294\n",
      "Epoch 4847, Loss: 0.002989614673424512, Final Batch Loss: 0.00047019380144774914\n",
      "Epoch 4848, Loss: 0.002790970989735797, Final Batch Loss: 0.0002976209798362106\n",
      "Epoch 4849, Loss: 0.004735136331873946, Final Batch Loss: 0.0007348680519498885\n",
      "Epoch 4850, Loss: 0.0012868197809439152, Final Batch Loss: 0.000204703988856636\n",
      "Epoch 4851, Loss: 0.0009466864576097578, Final Batch Loss: 0.00018702812667470425\n",
      "Epoch 4852, Loss: 0.0003905879109424859, Final Batch Loss: 0.0001142770197475329\n",
      "Epoch 4853, Loss: 0.006556421005370794, Final Batch Loss: 0.00012586764933075756\n",
      "Epoch 4854, Loss: 0.00046181181460269727, Final Batch Loss: 0.0001633219508221373\n",
      "Epoch 4855, Loss: 0.001143354926171014, Final Batch Loss: 0.00020984388538636267\n",
      "Epoch 4856, Loss: 0.007666499488550471, Final Batch Loss: 0.005570728797465563\n",
      "Epoch 4857, Loss: 0.00043692445615306497, Final Batch Loss: 0.00010784806363517419\n",
      "Epoch 4858, Loss: 0.0025987396693381015, Final Batch Loss: 0.0022126948460936546\n",
      "Epoch 4859, Loss: 0.0014390227661351673, Final Batch Loss: 0.0010182098485529423\n",
      "Epoch 4860, Loss: 0.0013982395030325279, Final Batch Loss: 3.066166755161248e-05\n",
      "Epoch 4861, Loss: 0.002163571596611291, Final Batch Loss: 0.0001827079977374524\n",
      "Epoch 4862, Loss: 0.0005271203128813795, Final Batch Loss: 8.311694546137005e-05\n",
      "Epoch 4863, Loss: 0.0033391605320503004, Final Batch Loss: 0.00024771233438514173\n",
      "Epoch 4864, Loss: 0.0010101167135871947, Final Batch Loss: 0.00021456924150697887\n",
      "Epoch 4865, Loss: 0.00039591129461769015, Final Batch Loss: 0.00011682122567435727\n",
      "Epoch 4866, Loss: 0.001648241999646416, Final Batch Loss: 0.000396768213249743\n",
      "Epoch 4867, Loss: 0.0008231333213188918, Final Batch Loss: 3.064546035602689e-05\n",
      "Epoch 4868, Loss: 0.0022897153176018037, Final Batch Loss: 7.167399598984048e-05\n",
      "Epoch 4869, Loss: 0.00029185730636527296, Final Batch Loss: 3.127346280962229e-05\n",
      "Epoch 4870, Loss: 0.00030853260068397503, Final Batch Loss: 4.830064790439792e-05\n",
      "Epoch 4871, Loss: 0.00041508783306198893, Final Batch Loss: 7.014434959273785e-05\n",
      "Epoch 4872, Loss: 0.0010157065280509414, Final Batch Loss: 1.9380868252483197e-05\n",
      "Epoch 4873, Loss: 0.00029697063291678205, Final Batch Loss: 6.292987382039428e-05\n",
      "Epoch 4874, Loss: 0.0002549341415942763, Final Batch Loss: 8.92549996933667e-06\n",
      "Epoch 4875, Loss: 0.0010271792416460812, Final Batch Loss: 5.706249430659227e-05\n",
      "Epoch 4876, Loss: 0.013414924550147589, Final Batch Loss: 1.449371325179527e-06\n",
      "Epoch 4877, Loss: 0.0007054424213492894, Final Batch Loss: 0.0002675691503100097\n",
      "Epoch 4878, Loss: 0.006650360009189171, Final Batch Loss: 0.00016617379151284695\n",
      "Epoch 4879, Loss: 0.0013893494779040338, Final Batch Loss: 1.0611112884362228e-05\n",
      "Epoch 4880, Loss: 0.0008873229016899131, Final Batch Loss: 0.0005497452802956104\n",
      "Epoch 4881, Loss: 0.012378401418573048, Final Batch Loss: 1.347387569694547e-05\n",
      "Epoch 4882, Loss: 0.002571456163423136, Final Batch Loss: 1.869198604254052e-05\n",
      "Epoch 4883, Loss: 0.0011095070804003626, Final Batch Loss: 0.00013507956464309245\n",
      "Epoch 4884, Loss: 0.02314903004162261, Final Batch Loss: 0.0014445761917158961\n",
      "Epoch 4885, Loss: 0.01703751699460554, Final Batch Loss: 2.2077991161495447e-05\n",
      "Epoch 4886, Loss: 0.002653065868798876, Final Batch Loss: 9.1562069428619e-05\n",
      "Epoch 4887, Loss: 0.0009606964576960308, Final Batch Loss: 0.00029793824069201946\n",
      "Epoch 4888, Loss: 0.01653895466006361, Final Batch Loss: 0.015936991199851036\n",
      "Epoch 4889, Loss: 0.007451320792824845, Final Batch Loss: 8.475920185446739e-05\n",
      "Epoch 4890, Loss: 0.012878858055046294, Final Batch Loss: 8.621958113508299e-05\n",
      "Epoch 4891, Loss: 0.002346695360756712, Final Batch Loss: 4.6005199692444876e-05\n",
      "Epoch 4892, Loss: 0.0007356487913057208, Final Batch Loss: 7.195345824584365e-05\n",
      "Epoch 4893, Loss: 0.002387869644735474, Final Batch Loss: 8.509472536388785e-05\n",
      "Epoch 4894, Loss: 0.0019857179113387247, Final Batch Loss: 6.124909123172984e-05\n",
      "Epoch 4895, Loss: 0.00653126116230851, Final Batch Loss: 1.9064827938564122e-05\n",
      "Epoch 4896, Loss: 0.006609296862734482, Final Batch Loss: 6.540837057400495e-05\n",
      "Epoch 4897, Loss: 0.0020025470812470303, Final Batch Loss: 2.6980671464116313e-05\n",
      "Epoch 4898, Loss: 0.013523740812161122, Final Batch Loss: 8.039786189328879e-05\n",
      "Epoch 4899, Loss: 0.013479351582645904, Final Batch Loss: 9.697880886960775e-05\n",
      "Epoch 4900, Loss: 0.004570796024836454, Final Batch Loss: 0.00015714731125626713\n",
      "Epoch 4901, Loss: 0.0008172156922228169, Final Batch Loss: 4.087614070158452e-05\n",
      "Epoch 4902, Loss: 0.0007217823222163133, Final Batch Loss: 0.0004054322780575603\n",
      "Epoch 4903, Loss: 0.00021460751031554537, Final Batch Loss: 2.3851154764997773e-05\n",
      "Epoch 4904, Loss: 0.0019814183351627435, Final Batch Loss: 1.1154656931466889e-05\n",
      "Epoch 4905, Loss: 0.0002855199472833192, Final Batch Loss: 5.431099998531863e-05\n",
      "Epoch 4906, Loss: 0.022475809455499984, Final Batch Loss: 4.961617378285155e-05\n",
      "Epoch 4907, Loss: 0.002021504369622562, Final Batch Loss: 0.00038731054519303143\n",
      "Epoch 4908, Loss: 0.0034922875493066385, Final Batch Loss: 6.637873593717813e-05\n",
      "Epoch 4909, Loss: 0.014428246637180564, Final Batch Loss: 9.299023076891899e-05\n",
      "Epoch 4910, Loss: 0.0035746610519709066, Final Batch Loss: 0.0006231852457858622\n",
      "Epoch 4911, Loss: 0.00023660426450078376, Final Batch Loss: 4.7598321543773636e-05\n",
      "Epoch 4912, Loss: 0.002947325283457758, Final Batch Loss: 0.00011028560402337462\n",
      "Epoch 4913, Loss: 0.0038403942744480446, Final Batch Loss: 0.0033709064591675997\n",
      "Epoch 4914, Loss: 0.0010325044277124107, Final Batch Loss: 0.00013359723379835486\n",
      "Epoch 4915, Loss: 0.001411586083122529, Final Batch Loss: 8.417776552960277e-05\n",
      "Epoch 4916, Loss: 0.0003461310679995222, Final Batch Loss: 9.650931315263733e-05\n",
      "Epoch 4917, Loss: 0.0013492076818693022, Final Batch Loss: 8.118379628285766e-05\n",
      "Epoch 4918, Loss: 0.00041140594839816913, Final Batch Loss: 4.7340312448795885e-05\n",
      "Epoch 4919, Loss: 0.004440356315171812, Final Batch Loss: 4.153034387854859e-05\n",
      "Epoch 4920, Loss: 0.0017835464495874476, Final Batch Loss: 8.62936649355106e-05\n",
      "Epoch 4921, Loss: 0.01679911932660616, Final Batch Loss: 3.108974851784296e-05\n",
      "Epoch 4922, Loss: 0.0075772886921186, Final Batch Loss: 0.0036750773433595896\n",
      "Epoch 4923, Loss: 0.010950403855531476, Final Batch Loss: 2.193291948060505e-05\n",
      "Epoch 4924, Loss: 0.005715869563573506, Final Batch Loss: 0.00045271890121512115\n",
      "Epoch 4925, Loss: 0.0027010348121621064, Final Batch Loss: 0.00045198542647995055\n",
      "Epoch 4926, Loss: 0.005219006836341578, Final Batch Loss: 6.688000212307088e-06\n",
      "Epoch 4927, Loss: 0.000572446361729817, Final Batch Loss: 7.845558866392821e-05\n",
      "Epoch 4928, Loss: 0.003402625305170659, Final Batch Loss: 0.0006388734909705818\n",
      "Epoch 4929, Loss: 0.0002675458617886761, Final Batch Loss: 2.1851319615961984e-06\n",
      "Epoch 4930, Loss: 0.0007147241030907026, Final Batch Loss: 2.3016104023554362e-05\n",
      "Epoch 4931, Loss: 0.00649463368108627, Final Batch Loss: 4.4634514779318124e-05\n",
      "Epoch 4932, Loss: 0.03267740587762091, Final Batch Loss: 0.03007025271654129\n",
      "Epoch 4933, Loss: 0.001501182901847642, Final Batch Loss: 1.6840822354424745e-05\n",
      "Epoch 4934, Loss: 0.020974416609533364, Final Batch Loss: 4.073165837326087e-05\n",
      "Epoch 4935, Loss: 0.0021756143250968307, Final Batch Loss: 0.0013126846170052886\n",
      "Epoch 4936, Loss: 0.010966322588501498, Final Batch Loss: 0.00028601850499399006\n",
      "Epoch 4937, Loss: 0.024369705410208553, Final Batch Loss: 0.022476034238934517\n",
      "Epoch 4938, Loss: 0.029571632279839832, Final Batch Loss: 0.00033672351855784655\n",
      "Epoch 4939, Loss: 0.015607761446517543, Final Batch Loss: 0.0004100065561942756\n",
      "Epoch 4940, Loss: 0.010362547549448209, Final Batch Loss: 0.0009388015023432672\n",
      "Epoch 4941, Loss: 0.020632800595194567, Final Batch Loss: 7.876165909692645e-06\n",
      "Epoch 4942, Loss: 0.030940026117605157, Final Batch Loss: 0.00018133367120753974\n",
      "Epoch 4943, Loss: 0.003334391600219533, Final Batch Loss: 2.3136177333071828e-05\n",
      "Epoch 4944, Loss: 0.006849484452686738, Final Batch Loss: 0.005053224042057991\n",
      "Epoch 4945, Loss: 0.006142008394817822, Final Batch Loss: 0.005513590294867754\n",
      "Epoch 4946, Loss: 0.0015191167767625302, Final Batch Loss: 0.00022479062317870557\n",
      "Epoch 4947, Loss: 0.0044773016416002065, Final Batch Loss: 0.00043827417539432645\n",
      "Epoch 4948, Loss: 0.004504420416196808, Final Batch Loss: 0.001736525446176529\n",
      "Epoch 4949, Loss: 0.003595962429244537, Final Batch Loss: 0.0018124645575881004\n",
      "Epoch 4950, Loss: 0.0031616367487004027, Final Batch Loss: 5.584499012911692e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4951, Loss: 0.003172165830619633, Final Batch Loss: 0.00041546658030711114\n",
      "Epoch 4952, Loss: 0.0006808409561926965, Final Batch Loss: 0.00013708087499253452\n",
      "Epoch 4953, Loss: 0.003059548116652877, Final Batch Loss: 0.00025295454543083906\n",
      "Epoch 4954, Loss: 0.0019049738948524464, Final Batch Loss: 3.2632920920150355e-05\n",
      "Epoch 4955, Loss: 0.003419393047806807, Final Batch Loss: 0.0030835552606731653\n",
      "Epoch 4956, Loss: 0.002169938314182218, Final Batch Loss: 2.8691254556179047e-05\n",
      "Epoch 4957, Loss: 0.0025996307958848774, Final Batch Loss: 0.00017428077990189195\n",
      "Epoch 4958, Loss: 0.001868224322606693, Final Batch Loss: 5.400742156780325e-05\n",
      "Epoch 4959, Loss: 0.000871011918206932, Final Batch Loss: 0.00019107124535366893\n",
      "Epoch 4960, Loss: 0.0012317275236455316, Final Batch Loss: 4.902320597466314e-06\n",
      "Epoch 4961, Loss: 0.004021988242129737, Final Batch Loss: 4.190392792224884e-05\n",
      "Epoch 4962, Loss: 0.0013163638732294203, Final Batch Loss: 2.5364774046465755e-05\n",
      "Epoch 4963, Loss: 0.00027730055398933473, Final Batch Loss: 6.510590083053103e-06\n",
      "Epoch 4964, Loss: 0.0007010984290900524, Final Batch Loss: 1.176179193862481e-05\n",
      "Epoch 4965, Loss: 0.00037449948740686523, Final Batch Loss: 8.544552656530868e-06\n",
      "Epoch 4966, Loss: 0.002304204932443099, Final Batch Loss: 6.5951855503954e-05\n",
      "Epoch 4967, Loss: 0.000510660251165973, Final Batch Loss: 2.2466570953838527e-05\n",
      "Epoch 4968, Loss: 0.0005003909891456715, Final Batch Loss: 0.00011573012307053432\n",
      "Epoch 4969, Loss: 0.0012860220406309963, Final Batch Loss: 2.744967105172691e-06\n",
      "Epoch 4970, Loss: 0.00038843035054014763, Final Batch Loss: 7.705582902417518e-06\n",
      "Epoch 4971, Loss: 0.0006027649665156787, Final Batch Loss: 4.4554340092872735e-06\n",
      "Epoch 4972, Loss: 0.0010848286738109891, Final Batch Loss: 8.751824680075515e-06\n",
      "Epoch 4973, Loss: 0.001478483885875903, Final Batch Loss: 5.33027523488272e-05\n",
      "Epoch 4974, Loss: 0.00044589059325517155, Final Batch Loss: 7.786398782627657e-05\n",
      "Epoch 4975, Loss: 0.000913407769985497, Final Batch Loss: 0.0003971178375650197\n",
      "Epoch 4976, Loss: 0.0027021669379792, Final Batch Loss: 0.0003127059026155621\n",
      "Epoch 4977, Loss: 0.004408887767340275, Final Batch Loss: 2.1078792542539304e-06\n",
      "Epoch 4978, Loss: 0.015352849874943786, Final Batch Loss: 8.369553142983932e-06\n",
      "Epoch 4979, Loss: 0.0014543909655913012, Final Batch Loss: 0.0002283690555486828\n",
      "Epoch 4980, Loss: 0.0004271786992831039, Final Batch Loss: 9.821854291658383e-06\n",
      "Epoch 4981, Loss: 0.0007270311762113124, Final Batch Loss: 3.7772682844661176e-05\n",
      "Epoch 4982, Loss: 0.00719569771172246, Final Batch Loss: 7.218444079626352e-05\n",
      "Epoch 4983, Loss: 0.0068521854182108655, Final Batch Loss: 0.006606689188629389\n",
      "Epoch 4984, Loss: 0.0004421201078912418, Final Batch Loss: 5.85815769227338e-06\n",
      "Epoch 4985, Loss: 0.00851862860872643, Final Batch Loss: 0.0008315846789628267\n",
      "Epoch 4986, Loss: 0.007303530886019871, Final Batch Loss: 0.006861075758934021\n",
      "Epoch 4987, Loss: 0.0003542065642250236, Final Batch Loss: 8.349057316081598e-05\n",
      "Epoch 4988, Loss: 0.00881445260165492, Final Batch Loss: 0.007608097977936268\n",
      "Epoch 4989, Loss: 0.0008921696698962478, Final Batch Loss: 0.00041144591523334384\n",
      "Epoch 4990, Loss: 0.020685652722022496, Final Batch Loss: 0.019226564094424248\n",
      "Epoch 4991, Loss: 0.0023857479864091147, Final Batch Loss: 0.0003493681433610618\n",
      "Epoch 4992, Loss: 0.10283690810592816, Final Batch Loss: 0.008603270165622234\n",
      "Epoch 4993, Loss: 0.0007104564065230079, Final Batch Loss: 0.00017860734078567475\n",
      "Epoch 4994, Loss: 0.005037802337028552, Final Batch Loss: 0.002052364172413945\n",
      "Epoch 4995, Loss: 0.0186783855460817, Final Batch Loss: 0.00013485188537742943\n",
      "Epoch 4996, Loss: 0.0006926136338734068, Final Batch Loss: 6.130921974545345e-05\n",
      "Epoch 4997, Loss: 0.0007426102238241583, Final Batch Loss: 4.1208597394870594e-05\n",
      "Epoch 4998, Loss: 0.0024263094637717586, Final Batch Loss: 0.001890419633127749\n",
      "Epoch 4999, Loss: 0.0017900914754136465, Final Batch Loss: 4.268935299478471e-05\n",
      "Epoch 5000, Loss: 0.0020752451982843922, Final Batch Loss: 2.5033028578036465e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122   1   0]\n",
      " [  0  71   0]\n",
      " [  0   0  93]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99187   0.99592       123\n",
      "           1    0.98611   1.00000   0.99301        71\n",
      "           2    1.00000   1.00000   1.00000        93\n",
      "\n",
      "    accuracy                        0.99652       287\n",
      "   macro avg    0.99537   0.99729   0.99631       287\n",
      "weighted avg    0.99656   0.99652   0.99652       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples * 9)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_13 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_14 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_15 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_16 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_17 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_18 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples * 9)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_19 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_20 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_21 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_22 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_23 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_24 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_25 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_26 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_27 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples * 9) + 1\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12,\n",
    "                               fake_features_13, fake_features_14, fake_features_15, fake_features_16, fake_features_17, fake_features_18,\n",
    "                               fake_features_19, fake_features_20, fake_features_21, fake_features_22, fake_features_23, fake_features_24,\n",
    "                               fake_features_25, fake_features_26, fake_features_27))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83  7  0]\n",
      " [ 7 83  0]\n",
      " [ 0  0 90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.92222   0.92222   0.92222        90\n",
      "         1.0    0.92222   0.92222   0.92222        90\n",
      "         2.0    1.00000   1.00000   1.00000        90\n",
      "\n",
      "    accuracy                        0.94815       270\n",
      "   macro avg    0.94815   0.94815   0.94815       270\n",
      "weighted avg    0.94815   0.94815   0.94815       270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Subject\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)):\n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    elif y[k] == 5:\n",
    "        y[k] = 2\n",
    "    elif y[k] == 7:\n",
    "        y[k] = 3\n",
    "    elif y[k] == 8:\n",
    "        y[k] = 4\n",
    "    elif y[k] == 11:\n",
    "        y[k] = 5\n",
    "    elif y[k] == 14:\n",
    "        y[k] = 6\n",
    "    elif y[k] == 17:\n",
    "        y[k] = 7\n",
    "    else:\n",
    "        y[k] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model_subject = Subject_Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_subject.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.055163621902466, Final Batch Loss: 2.203505039215088\n",
      "Epoch 2, Loss: 11.04562783241272, Final Batch Loss: 2.2056503295898438\n",
      "Epoch 3, Loss: 11.034548997879028, Final Batch Loss: 2.19122576713562\n",
      "Epoch 4, Loss: 11.03100037574768, Final Batch Loss: 2.2085812091827393\n",
      "Epoch 5, Loss: 11.019251585006714, Final Batch Loss: 2.195369243621826\n",
      "Epoch 6, Loss: 11.015580415725708, Final Batch Loss: 2.205585241317749\n",
      "Epoch 7, Loss: 11.001346826553345, Final Batch Loss: 2.207587957382202\n",
      "Epoch 8, Loss: 10.977611303329468, Final Batch Loss: 2.18221378326416\n",
      "Epoch 9, Loss: 10.967817783355713, Final Batch Loss: 2.1999144554138184\n",
      "Epoch 10, Loss: 10.933318614959717, Final Batch Loss: 2.180107355117798\n",
      "Epoch 11, Loss: 10.921395301818848, Final Batch Loss: 2.1962831020355225\n",
      "Epoch 12, Loss: 10.868391275405884, Final Batch Loss: 2.1656558513641357\n",
      "Epoch 13, Loss: 10.835430145263672, Final Batch Loss: 2.1803126335144043\n",
      "Epoch 14, Loss: 10.785737991333008, Final Batch Loss: 2.1605701446533203\n",
      "Epoch 15, Loss: 10.697858333587646, Final Batch Loss: 2.107106924057007\n",
      "Epoch 16, Loss: 10.613575220108032, Final Batch Loss: 2.115902900695801\n",
      "Epoch 17, Loss: 10.518950939178467, Final Batch Loss: 2.094513177871704\n",
      "Epoch 18, Loss: 10.41746211051941, Final Batch Loss: 2.0618107318878174\n",
      "Epoch 19, Loss: 10.280508279800415, Final Batch Loss: 2.050031900405884\n",
      "Epoch 20, Loss: 10.217474222183228, Final Batch Loss: 2.047295331954956\n",
      "Epoch 21, Loss: 10.169686555862427, Final Batch Loss: 2.0959396362304688\n",
      "Epoch 22, Loss: 10.028789520263672, Final Batch Loss: 2.0530266761779785\n",
      "Epoch 23, Loss: 9.913711190223694, Final Batch Loss: 1.978591799736023\n",
      "Epoch 24, Loss: 9.82500946521759, Final Batch Loss: 1.9258050918579102\n",
      "Epoch 25, Loss: 9.756852388381958, Final Batch Loss: 1.957013726234436\n",
      "Epoch 26, Loss: 9.692558646202087, Final Batch Loss: 1.9420366287231445\n",
      "Epoch 27, Loss: 9.592291831970215, Final Batch Loss: 1.8514115810394287\n",
      "Epoch 28, Loss: 9.572490930557251, Final Batch Loss: 1.9542572498321533\n",
      "Epoch 29, Loss: 9.49622929096222, Final Batch Loss: 1.9320775270462036\n",
      "Epoch 30, Loss: 9.450629830360413, Final Batch Loss: 1.8099101781845093\n",
      "Epoch 31, Loss: 9.409798979759216, Final Batch Loss: 1.897879958152771\n",
      "Epoch 32, Loss: 9.271163702011108, Final Batch Loss: 1.8627281188964844\n",
      "Epoch 33, Loss: 9.249523043632507, Final Batch Loss: 1.8177638053894043\n",
      "Epoch 34, Loss: 9.104357123374939, Final Batch Loss: 1.8169783353805542\n",
      "Epoch 35, Loss: 9.174927830696106, Final Batch Loss: 1.8658792972564697\n",
      "Epoch 36, Loss: 9.08905553817749, Final Batch Loss: 1.829354166984558\n",
      "Epoch 37, Loss: 9.062357068061829, Final Batch Loss: 1.89284348487854\n",
      "Epoch 38, Loss: 9.002705335617065, Final Batch Loss: 1.791200041770935\n",
      "Epoch 39, Loss: 8.84833538532257, Final Batch Loss: 1.7262171506881714\n",
      "Epoch 40, Loss: 8.916804432868958, Final Batch Loss: 1.8409080505371094\n",
      "Epoch 41, Loss: 8.833182573318481, Final Batch Loss: 1.8481217622756958\n",
      "Epoch 42, Loss: 8.849839925765991, Final Batch Loss: 1.8728824853897095\n",
      "Epoch 43, Loss: 8.703222513198853, Final Batch Loss: 1.7493277788162231\n",
      "Epoch 44, Loss: 8.7854083776474, Final Batch Loss: 1.784905195236206\n",
      "Epoch 45, Loss: 8.697763919830322, Final Batch Loss: 1.7085974216461182\n",
      "Epoch 46, Loss: 8.52003002166748, Final Batch Loss: 1.6165382862091064\n",
      "Epoch 47, Loss: 8.570086121559143, Final Batch Loss: 1.692677617073059\n",
      "Epoch 48, Loss: 8.550174713134766, Final Batch Loss: 1.7102298736572266\n",
      "Epoch 49, Loss: 8.493596076965332, Final Batch Loss: 1.6852397918701172\n",
      "Epoch 50, Loss: 8.44541347026825, Final Batch Loss: 1.6477514505386353\n",
      "Epoch 51, Loss: 8.47884976863861, Final Batch Loss: 1.7579833269119263\n",
      "Epoch 52, Loss: 8.43707537651062, Final Batch Loss: 1.7488490343093872\n",
      "Epoch 53, Loss: 8.348634600639343, Final Batch Loss: 1.608338475227356\n",
      "Epoch 54, Loss: 8.335749745368958, Final Batch Loss: 1.6693753004074097\n",
      "Epoch 55, Loss: 8.3629869222641, Final Batch Loss: 1.6541476249694824\n",
      "Epoch 56, Loss: 8.286367416381836, Final Batch Loss: 1.7215732336044312\n",
      "Epoch 57, Loss: 8.317743301391602, Final Batch Loss: 1.7273558378219604\n",
      "Epoch 58, Loss: 8.280821800231934, Final Batch Loss: 1.651979684829712\n",
      "Epoch 59, Loss: 8.183199405670166, Final Batch Loss: 1.5869203805923462\n",
      "Epoch 60, Loss: 8.184866309165955, Final Batch Loss: 1.6397371292114258\n",
      "Epoch 61, Loss: 8.154093623161316, Final Batch Loss: 1.6916651725769043\n",
      "Epoch 62, Loss: 8.120933175086975, Final Batch Loss: 1.5490790605545044\n",
      "Epoch 63, Loss: 8.135797619819641, Final Batch Loss: 1.7087122201919556\n",
      "Epoch 64, Loss: 8.04767370223999, Final Batch Loss: 1.5910643339157104\n",
      "Epoch 65, Loss: 8.13560175895691, Final Batch Loss: 1.6644268035888672\n",
      "Epoch 66, Loss: 8.015530109405518, Final Batch Loss: 1.5628464221954346\n",
      "Epoch 67, Loss: 8.002732872962952, Final Batch Loss: 1.6655536890029907\n",
      "Epoch 68, Loss: 7.8303542137146, Final Batch Loss: 1.5177512168884277\n",
      "Epoch 69, Loss: 7.966280460357666, Final Batch Loss: 1.6167023181915283\n",
      "Epoch 70, Loss: 7.890910029411316, Final Batch Loss: 1.561159610748291\n",
      "Epoch 71, Loss: 7.9207974672317505, Final Batch Loss: 1.624322772026062\n",
      "Epoch 72, Loss: 7.951178431510925, Final Batch Loss: 1.6596280336380005\n",
      "Epoch 73, Loss: 7.9599609375, Final Batch Loss: 1.6897211074829102\n",
      "Epoch 74, Loss: 7.836563229560852, Final Batch Loss: 1.6124392747879028\n",
      "Epoch 75, Loss: 7.845059752464294, Final Batch Loss: 1.5448583364486694\n",
      "Epoch 76, Loss: 7.6660449504852295, Final Batch Loss: 1.435035228729248\n",
      "Epoch 77, Loss: 7.865217447280884, Final Batch Loss: 1.5183876752853394\n",
      "Epoch 78, Loss: 7.87398087978363, Final Batch Loss: 1.6358962059020996\n",
      "Epoch 79, Loss: 7.640603303909302, Final Batch Loss: 1.4400930404663086\n",
      "Epoch 80, Loss: 7.79924464225769, Final Batch Loss: 1.5158213376998901\n",
      "Epoch 81, Loss: 7.8290722370147705, Final Batch Loss: 1.5941964387893677\n",
      "Epoch 82, Loss: 7.767567157745361, Final Batch Loss: 1.587200403213501\n",
      "Epoch 83, Loss: 7.774502992630005, Final Batch Loss: 1.6381642818450928\n",
      "Epoch 84, Loss: 7.672975420951843, Final Batch Loss: 1.4284273386001587\n",
      "Epoch 85, Loss: 7.729119300842285, Final Batch Loss: 1.5664118528366089\n",
      "Epoch 86, Loss: 7.682728409767151, Final Batch Loss: 1.587595820426941\n",
      "Epoch 87, Loss: 7.586604714393616, Final Batch Loss: 1.4464616775512695\n",
      "Epoch 88, Loss: 7.6681482791900635, Final Batch Loss: 1.5568296909332275\n",
      "Epoch 89, Loss: 7.5578131675720215, Final Batch Loss: 1.5902694463729858\n",
      "Epoch 90, Loss: 7.492650270462036, Final Batch Loss: 1.4801808595657349\n",
      "Epoch 91, Loss: 7.637393832206726, Final Batch Loss: 1.5374053716659546\n",
      "Epoch 92, Loss: 7.607375860214233, Final Batch Loss: 1.6039350032806396\n",
      "Epoch 93, Loss: 7.469142198562622, Final Batch Loss: 1.4700613021850586\n",
      "Epoch 94, Loss: 7.530547022819519, Final Batch Loss: 1.5239448547363281\n",
      "Epoch 95, Loss: 7.458110809326172, Final Batch Loss: 1.4850341081619263\n",
      "Epoch 96, Loss: 7.509878873825073, Final Batch Loss: 1.450792908668518\n",
      "Epoch 97, Loss: 7.536732792854309, Final Batch Loss: 1.595981240272522\n",
      "Epoch 98, Loss: 7.551498055458069, Final Batch Loss: 1.402858853340149\n",
      "Epoch 99, Loss: 7.403602719306946, Final Batch Loss: 1.487877607345581\n",
      "Epoch 100, Loss: 7.48116409778595, Final Batch Loss: 1.4944276809692383\n",
      "Epoch 101, Loss: 7.449072360992432, Final Batch Loss: 1.4531095027923584\n",
      "Epoch 102, Loss: 7.440994739532471, Final Batch Loss: 1.4934526681900024\n",
      "Epoch 103, Loss: 7.468735694885254, Final Batch Loss: 1.5206093788146973\n",
      "Epoch 104, Loss: 7.337269186973572, Final Batch Loss: 1.4169654846191406\n",
      "Epoch 105, Loss: 7.341487884521484, Final Batch Loss: 1.4793058633804321\n",
      "Epoch 106, Loss: 7.312894940376282, Final Batch Loss: 1.4403513669967651\n",
      "Epoch 107, Loss: 7.3603984117507935, Final Batch Loss: 1.392256259918213\n",
      "Epoch 108, Loss: 7.179733753204346, Final Batch Loss: 1.4270899295806885\n",
      "Epoch 109, Loss: 7.366092681884766, Final Batch Loss: 1.5091838836669922\n",
      "Epoch 110, Loss: 7.295487403869629, Final Batch Loss: 1.5452656745910645\n",
      "Epoch 111, Loss: 7.250428318977356, Final Batch Loss: 1.4849783182144165\n",
      "Epoch 112, Loss: 7.210743546485901, Final Batch Loss: 1.4463047981262207\n",
      "Epoch 113, Loss: 7.2178497314453125, Final Batch Loss: 1.418808937072754\n",
      "Epoch 114, Loss: 7.259759187698364, Final Batch Loss: 1.528918981552124\n",
      "Epoch 115, Loss: 7.140143156051636, Final Batch Loss: 1.4093785285949707\n",
      "Epoch 116, Loss: 7.222815036773682, Final Batch Loss: 1.4353936910629272\n",
      "Epoch 117, Loss: 7.128052830696106, Final Batch Loss: 1.3396728038787842\n",
      "Epoch 118, Loss: 6.973697304725647, Final Batch Loss: 1.300683617591858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119, Loss: 7.054306149482727, Final Batch Loss: 1.4786489009857178\n",
      "Epoch 120, Loss: 7.269316554069519, Final Batch Loss: 1.5844800472259521\n",
      "Epoch 121, Loss: 7.131820797920227, Final Batch Loss: 1.4547176361083984\n",
      "Epoch 122, Loss: 7.06106698513031, Final Batch Loss: 1.4305577278137207\n",
      "Epoch 123, Loss: 7.100249290466309, Final Batch Loss: 1.4645205736160278\n",
      "Epoch 124, Loss: 7.138689637184143, Final Batch Loss: 1.6001274585723877\n",
      "Epoch 125, Loss: 6.93645453453064, Final Batch Loss: 1.3076530694961548\n",
      "Epoch 126, Loss: 6.892889976501465, Final Batch Loss: 1.3479316234588623\n",
      "Epoch 127, Loss: 7.022924065589905, Final Batch Loss: 1.428602695465088\n",
      "Epoch 128, Loss: 6.864248514175415, Final Batch Loss: 1.3678630590438843\n",
      "Epoch 129, Loss: 6.877646565437317, Final Batch Loss: 1.315956950187683\n",
      "Epoch 130, Loss: 6.875125527381897, Final Batch Loss: 1.4376105070114136\n",
      "Epoch 131, Loss: 6.849817991256714, Final Batch Loss: 1.4815144538879395\n",
      "Epoch 132, Loss: 6.827344536781311, Final Batch Loss: 1.2305231094360352\n",
      "Epoch 133, Loss: 6.925485610961914, Final Batch Loss: 1.3494305610656738\n",
      "Epoch 134, Loss: 6.856758713722229, Final Batch Loss: 1.3633534908294678\n",
      "Epoch 135, Loss: 6.825809001922607, Final Batch Loss: 1.3536081314086914\n",
      "Epoch 136, Loss: 6.960610508918762, Final Batch Loss: 1.3840153217315674\n",
      "Epoch 137, Loss: 6.702217936515808, Final Batch Loss: 1.3638184070587158\n",
      "Epoch 138, Loss: 6.856327176094055, Final Batch Loss: 1.4022949934005737\n",
      "Epoch 139, Loss: 6.8629279136657715, Final Batch Loss: 1.3639206886291504\n",
      "Epoch 140, Loss: 6.770925998687744, Final Batch Loss: 1.3842991590499878\n",
      "Epoch 141, Loss: 6.782916903495789, Final Batch Loss: 1.4061157703399658\n",
      "Epoch 142, Loss: 6.662529349327087, Final Batch Loss: 1.4176509380340576\n",
      "Epoch 143, Loss: 6.70121169090271, Final Batch Loss: 1.3209854364395142\n",
      "Epoch 144, Loss: 6.7048033475875854, Final Batch Loss: 1.2683697938919067\n",
      "Epoch 145, Loss: 6.656290173530579, Final Batch Loss: 1.2757065296173096\n",
      "Epoch 146, Loss: 6.677528142929077, Final Batch Loss: 1.3978314399719238\n",
      "Epoch 147, Loss: 6.703139066696167, Final Batch Loss: 1.3965327739715576\n",
      "Epoch 148, Loss: 6.590098977088928, Final Batch Loss: 1.3217394351959229\n",
      "Epoch 149, Loss: 6.572468876838684, Final Batch Loss: 1.299216389656067\n",
      "Epoch 150, Loss: 6.493445992469788, Final Batch Loss: 1.289467453956604\n",
      "Epoch 151, Loss: 6.605170130729675, Final Batch Loss: 1.2628874778747559\n",
      "Epoch 152, Loss: 6.599737167358398, Final Batch Loss: 1.470629334449768\n",
      "Epoch 153, Loss: 6.584571838378906, Final Batch Loss: 1.3500370979309082\n",
      "Epoch 154, Loss: 6.759694814682007, Final Batch Loss: 1.4097323417663574\n",
      "Epoch 155, Loss: 6.5015798807144165, Final Batch Loss: 1.3548383712768555\n",
      "Epoch 156, Loss: 6.612292408943176, Final Batch Loss: 1.3601231575012207\n",
      "Epoch 157, Loss: 6.6167731285095215, Final Batch Loss: 1.3074589967727661\n",
      "Epoch 158, Loss: 6.551376819610596, Final Batch Loss: 1.3312010765075684\n",
      "Epoch 159, Loss: 6.402506709098816, Final Batch Loss: 1.346386432647705\n",
      "Epoch 160, Loss: 6.435299634933472, Final Batch Loss: 1.189448356628418\n",
      "Epoch 161, Loss: 6.4015514850616455, Final Batch Loss: 1.275547981262207\n",
      "Epoch 162, Loss: 6.379905462265015, Final Batch Loss: 1.264168381690979\n",
      "Epoch 163, Loss: 6.245619177818298, Final Batch Loss: 1.1834477186203003\n",
      "Epoch 164, Loss: 6.274777889251709, Final Batch Loss: 1.2167236804962158\n",
      "Epoch 165, Loss: 6.324302911758423, Final Batch Loss: 1.245137333869934\n",
      "Epoch 166, Loss: 6.197451949119568, Final Batch Loss: 1.2250417470932007\n",
      "Epoch 167, Loss: 6.1442471742630005, Final Batch Loss: 1.1925292015075684\n",
      "Epoch 168, Loss: 6.221435785293579, Final Batch Loss: 1.2016863822937012\n",
      "Epoch 169, Loss: 6.151197910308838, Final Batch Loss: 1.1639330387115479\n",
      "Epoch 170, Loss: 6.2374149560928345, Final Batch Loss: 1.2921533584594727\n",
      "Epoch 171, Loss: 6.127203941345215, Final Batch Loss: 1.283372402191162\n",
      "Epoch 172, Loss: 6.222643494606018, Final Batch Loss: 1.303869605064392\n",
      "Epoch 173, Loss: 6.099598169326782, Final Batch Loss: 1.1555477380752563\n",
      "Epoch 174, Loss: 6.188892483711243, Final Batch Loss: 1.3109853267669678\n",
      "Epoch 175, Loss: 6.103418231010437, Final Batch Loss: 1.2217456102371216\n",
      "Epoch 176, Loss: 6.1365052461624146, Final Batch Loss: 1.1956814527511597\n",
      "Epoch 177, Loss: 6.200200796127319, Final Batch Loss: 1.3109065294265747\n",
      "Epoch 178, Loss: 6.060114502906799, Final Batch Loss: 1.2307229042053223\n",
      "Epoch 179, Loss: 5.98891007900238, Final Batch Loss: 1.1200679540634155\n",
      "Epoch 180, Loss: 5.938928484916687, Final Batch Loss: 1.1599211692810059\n",
      "Epoch 181, Loss: 6.03560209274292, Final Batch Loss: 1.1722157001495361\n",
      "Epoch 182, Loss: 5.991855144500732, Final Batch Loss: 1.2242199182510376\n",
      "Epoch 183, Loss: 6.205508351325989, Final Batch Loss: 1.267340064048767\n",
      "Epoch 184, Loss: 5.854310035705566, Final Batch Loss: 1.1288626194000244\n",
      "Epoch 185, Loss: 5.9243773221969604, Final Batch Loss: 1.1896929740905762\n",
      "Epoch 186, Loss: 5.81500506401062, Final Batch Loss: 1.1098235845565796\n",
      "Epoch 187, Loss: 5.926038384437561, Final Batch Loss: 1.2280817031860352\n",
      "Epoch 188, Loss: 5.901561856269836, Final Batch Loss: 1.1673675775527954\n",
      "Epoch 189, Loss: 5.964172840118408, Final Batch Loss: 1.117621898651123\n",
      "Epoch 190, Loss: 5.902329921722412, Final Batch Loss: 1.1950472593307495\n",
      "Epoch 191, Loss: 5.965619206428528, Final Batch Loss: 1.1862047910690308\n",
      "Epoch 192, Loss: 5.926593065261841, Final Batch Loss: 1.1639264822006226\n",
      "Epoch 193, Loss: 5.939567923545837, Final Batch Loss: 1.3489633798599243\n",
      "Epoch 194, Loss: 5.898234009742737, Final Batch Loss: 1.207025170326233\n",
      "Epoch 195, Loss: 5.790461182594299, Final Batch Loss: 1.1378763914108276\n",
      "Epoch 196, Loss: 5.765289306640625, Final Batch Loss: 1.2612119913101196\n",
      "Epoch 197, Loss: 5.8417675495147705, Final Batch Loss: 1.2461942434310913\n",
      "Epoch 198, Loss: 5.838775992393494, Final Batch Loss: 1.0267478227615356\n",
      "Epoch 199, Loss: 5.764131546020508, Final Batch Loss: 1.2447044849395752\n",
      "Epoch 200, Loss: 5.836183667182922, Final Batch Loss: 1.13417387008667\n",
      "Epoch 201, Loss: 5.846598267555237, Final Batch Loss: 1.1513407230377197\n",
      "Epoch 202, Loss: 5.841615915298462, Final Batch Loss: 1.172863483428955\n",
      "Epoch 203, Loss: 5.684342741966248, Final Batch Loss: 1.0964421033859253\n",
      "Epoch 204, Loss: 5.721011757850647, Final Batch Loss: 1.0335346460342407\n",
      "Epoch 205, Loss: 5.723244071006775, Final Batch Loss: 1.1915603876113892\n",
      "Epoch 206, Loss: 5.632369875907898, Final Batch Loss: 1.0693449974060059\n",
      "Epoch 207, Loss: 5.718777537345886, Final Batch Loss: 1.052406907081604\n",
      "Epoch 208, Loss: 5.762227177619934, Final Batch Loss: 1.2157410383224487\n",
      "Epoch 209, Loss: 5.8050737380981445, Final Batch Loss: 1.180100679397583\n",
      "Epoch 210, Loss: 5.6676026582717896, Final Batch Loss: 1.1667490005493164\n",
      "Epoch 211, Loss: 5.785449504852295, Final Batch Loss: 1.1941765546798706\n",
      "Epoch 212, Loss: 5.667979121208191, Final Batch Loss: 0.9981160163879395\n",
      "Epoch 213, Loss: 5.734056115150452, Final Batch Loss: 1.165270209312439\n",
      "Epoch 214, Loss: 5.547058820724487, Final Batch Loss: 1.1024988889694214\n",
      "Epoch 215, Loss: 5.532246828079224, Final Batch Loss: 1.0729762315750122\n",
      "Epoch 216, Loss: 5.61403489112854, Final Batch Loss: 1.1787915229797363\n",
      "Epoch 217, Loss: 5.65839946269989, Final Batch Loss: 1.2667770385742188\n",
      "Epoch 218, Loss: 5.623311281204224, Final Batch Loss: 1.0586317777633667\n",
      "Epoch 219, Loss: 5.6439584493637085, Final Batch Loss: 1.1577519178390503\n",
      "Epoch 220, Loss: 5.66514778137207, Final Batch Loss: 1.1444095373153687\n",
      "Epoch 221, Loss: 5.524161219596863, Final Batch Loss: 1.1863832473754883\n",
      "Epoch 222, Loss: 5.697282910346985, Final Batch Loss: 1.1263240575790405\n",
      "Epoch 223, Loss: 5.575998902320862, Final Batch Loss: 1.1894391775131226\n",
      "Epoch 224, Loss: 5.523399829864502, Final Batch Loss: 1.0912445783615112\n",
      "Epoch 225, Loss: 5.6207029819488525, Final Batch Loss: 1.1904637813568115\n",
      "Epoch 226, Loss: 5.688855409622192, Final Batch Loss: 1.1925876140594482\n",
      "Epoch 227, Loss: 5.724607706069946, Final Batch Loss: 1.251014232635498\n",
      "Epoch 228, Loss: 5.538925766944885, Final Batch Loss: 1.0789364576339722\n",
      "Epoch 229, Loss: 5.673945665359497, Final Batch Loss: 1.1834114789962769\n",
      "Epoch 230, Loss: 5.438803434371948, Final Batch Loss: 1.0822662115097046\n",
      "Epoch 231, Loss: 5.541949391365051, Final Batch Loss: 1.0701818466186523\n",
      "Epoch 232, Loss: 5.462561905384064, Final Batch Loss: 1.3053100109100342\n",
      "Epoch 233, Loss: 5.432842791080475, Final Batch Loss: 1.1425461769104004\n",
      "Epoch 234, Loss: 5.36624401807785, Final Batch Loss: 1.0962454080581665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235, Loss: 5.427765250205994, Final Batch Loss: 1.091259479522705\n",
      "Epoch 236, Loss: 5.273787260055542, Final Batch Loss: 1.076593041419983\n",
      "Epoch 237, Loss: 5.401442050933838, Final Batch Loss: 1.0675519704818726\n",
      "Epoch 238, Loss: 5.4046547412872314, Final Batch Loss: 1.0621172189712524\n",
      "Epoch 239, Loss: 5.304116129875183, Final Batch Loss: 1.0580960512161255\n",
      "Epoch 240, Loss: 5.300789952278137, Final Batch Loss: 1.0351463556289673\n",
      "Epoch 241, Loss: 5.290010988712311, Final Batch Loss: 0.9536556005477905\n",
      "Epoch 242, Loss: 5.398372828960419, Final Batch Loss: 0.9880051016807556\n",
      "Epoch 243, Loss: 5.46056056022644, Final Batch Loss: 1.1362073421478271\n",
      "Epoch 244, Loss: 5.465312957763672, Final Batch Loss: 1.0425715446472168\n",
      "Epoch 245, Loss: 5.361910581588745, Final Batch Loss: 1.0395885705947876\n",
      "Epoch 246, Loss: 5.372401595115662, Final Batch Loss: 1.0307183265686035\n",
      "Epoch 247, Loss: 5.398649096488953, Final Batch Loss: 1.0582774877548218\n",
      "Epoch 248, Loss: 5.400021076202393, Final Batch Loss: 1.0818010568618774\n",
      "Epoch 249, Loss: 5.301393032073975, Final Batch Loss: 0.9843723773956299\n",
      "Epoch 250, Loss: 5.369832277297974, Final Batch Loss: 1.1323319673538208\n",
      "Epoch 251, Loss: 5.415830850601196, Final Batch Loss: 1.0303773880004883\n",
      "Epoch 252, Loss: 5.347037196159363, Final Batch Loss: 1.0473841428756714\n",
      "Epoch 253, Loss: 5.374920845031738, Final Batch Loss: 1.0378156900405884\n",
      "Epoch 254, Loss: 5.344961166381836, Final Batch Loss: 1.0138903856277466\n",
      "Epoch 255, Loss: 5.360183954238892, Final Batch Loss: 1.102820873260498\n",
      "Epoch 256, Loss: 5.411150813102722, Final Batch Loss: 1.0867729187011719\n",
      "Epoch 257, Loss: 5.170530378818512, Final Batch Loss: 1.0220595598220825\n",
      "Epoch 258, Loss: 5.21484762430191, Final Batch Loss: 0.9901512265205383\n",
      "Epoch 259, Loss: 5.4060657024383545, Final Batch Loss: 1.0556727647781372\n",
      "Epoch 260, Loss: 5.238926887512207, Final Batch Loss: 1.0336500406265259\n",
      "Epoch 261, Loss: 5.32861053943634, Final Batch Loss: 1.0344505310058594\n",
      "Epoch 262, Loss: 5.2631871700286865, Final Batch Loss: 1.1431351900100708\n",
      "Epoch 263, Loss: 5.265594482421875, Final Batch Loss: 0.9461690187454224\n",
      "Epoch 264, Loss: 5.349043607711792, Final Batch Loss: 1.152753472328186\n",
      "Epoch 265, Loss: 5.301471769809723, Final Batch Loss: 1.211538553237915\n",
      "Epoch 266, Loss: 5.241144776344299, Final Batch Loss: 1.020567536354065\n",
      "Epoch 267, Loss: 5.334620714187622, Final Batch Loss: 1.1197932958602905\n",
      "Epoch 268, Loss: 5.255868911743164, Final Batch Loss: 1.0448468923568726\n",
      "Epoch 269, Loss: 5.216042995452881, Final Batch Loss: 1.0712590217590332\n",
      "Epoch 270, Loss: 5.299302577972412, Final Batch Loss: 1.1214146614074707\n",
      "Epoch 271, Loss: 5.08297073841095, Final Batch Loss: 0.9837560057640076\n",
      "Epoch 272, Loss: 5.34964644908905, Final Batch Loss: 1.127172827720642\n",
      "Epoch 273, Loss: 5.09093701839447, Final Batch Loss: 1.064295768737793\n",
      "Epoch 274, Loss: 5.178752899169922, Final Batch Loss: 0.9952747225761414\n",
      "Epoch 275, Loss: 5.201135456562042, Final Batch Loss: 1.0326454639434814\n",
      "Epoch 276, Loss: 5.093973278999329, Final Batch Loss: 1.0597238540649414\n",
      "Epoch 277, Loss: 5.356439471244812, Final Batch Loss: 1.1982208490371704\n",
      "Epoch 278, Loss: 5.172735333442688, Final Batch Loss: 1.0664385557174683\n",
      "Epoch 279, Loss: 5.296430468559265, Final Batch Loss: 1.1679863929748535\n",
      "Epoch 280, Loss: 5.228524327278137, Final Batch Loss: 1.0710965394973755\n",
      "Epoch 281, Loss: 5.125297725200653, Final Batch Loss: 0.9626214504241943\n",
      "Epoch 282, Loss: 5.186091899871826, Final Batch Loss: 1.081800103187561\n",
      "Epoch 283, Loss: 5.149556219577789, Final Batch Loss: 1.1904151439666748\n",
      "Epoch 284, Loss: 5.081253945827484, Final Batch Loss: 1.058375358581543\n",
      "Epoch 285, Loss: 5.103475034236908, Final Batch Loss: 1.091878056526184\n",
      "Epoch 286, Loss: 5.10737669467926, Final Batch Loss: 1.0342665910720825\n",
      "Epoch 287, Loss: 5.0870503187179565, Final Batch Loss: 0.9835662841796875\n",
      "Epoch 288, Loss: 5.070337772369385, Final Batch Loss: 1.0350862741470337\n",
      "Epoch 289, Loss: 5.140982389450073, Final Batch Loss: 1.05561101436615\n",
      "Epoch 290, Loss: 5.364722430706024, Final Batch Loss: 1.179065465927124\n",
      "Epoch 291, Loss: 5.170010268688202, Final Batch Loss: 1.1535918712615967\n",
      "Epoch 292, Loss: 5.156433939933777, Final Batch Loss: 1.133171796798706\n",
      "Epoch 293, Loss: 5.085180401802063, Final Batch Loss: 1.0382559299468994\n",
      "Epoch 294, Loss: 5.110781371593475, Final Batch Loss: 0.9925985336303711\n",
      "Epoch 295, Loss: 5.036335706710815, Final Batch Loss: 1.0552098751068115\n",
      "Epoch 296, Loss: 5.13127601146698, Final Batch Loss: 1.0191333293914795\n",
      "Epoch 297, Loss: 5.07267826795578, Final Batch Loss: 0.9968492984771729\n",
      "Epoch 298, Loss: 4.904687762260437, Final Batch Loss: 0.9431464672088623\n",
      "Epoch 299, Loss: 5.203135788440704, Final Batch Loss: 1.0734410285949707\n",
      "Epoch 300, Loss: 5.0147625207901, Final Batch Loss: 1.0704189538955688\n",
      "Epoch 301, Loss: 5.038627803325653, Final Batch Loss: 0.977934718132019\n",
      "Epoch 302, Loss: 5.008758306503296, Final Batch Loss: 0.9722363948822021\n",
      "Epoch 303, Loss: 4.961691319942474, Final Batch Loss: 0.9523068070411682\n",
      "Epoch 304, Loss: 5.107531309127808, Final Batch Loss: 1.0104758739471436\n",
      "Epoch 305, Loss: 5.059412837028503, Final Batch Loss: 1.0718181133270264\n",
      "Epoch 306, Loss: 5.0471742153167725, Final Batch Loss: 1.0088297128677368\n",
      "Epoch 307, Loss: 4.9528809785842896, Final Batch Loss: 0.9613425135612488\n",
      "Epoch 308, Loss: 5.090664744377136, Final Batch Loss: 1.0055938959121704\n",
      "Epoch 309, Loss: 4.838242173194885, Final Batch Loss: 0.8852781057357788\n",
      "Epoch 310, Loss: 5.0117456912994385, Final Batch Loss: 1.0470837354660034\n",
      "Epoch 311, Loss: 4.878963112831116, Final Batch Loss: 0.9526874423027039\n",
      "Epoch 312, Loss: 5.110372722148895, Final Batch Loss: 1.0279911756515503\n",
      "Epoch 313, Loss: 4.9091795682907104, Final Batch Loss: 1.0340975522994995\n",
      "Epoch 314, Loss: 4.966135263442993, Final Batch Loss: 0.9172278046607971\n",
      "Epoch 315, Loss: 4.902671217918396, Final Batch Loss: 0.9440839886665344\n",
      "Epoch 316, Loss: 4.991754233837128, Final Batch Loss: 1.0962398052215576\n",
      "Epoch 317, Loss: 4.91614305973053, Final Batch Loss: 0.9385140538215637\n",
      "Epoch 318, Loss: 4.996554732322693, Final Batch Loss: 0.9862937331199646\n",
      "Epoch 319, Loss: 4.937160015106201, Final Batch Loss: 0.9366246461868286\n",
      "Epoch 320, Loss: 4.871212720870972, Final Batch Loss: 0.9694452881813049\n",
      "Epoch 321, Loss: 4.971885442733765, Final Batch Loss: 1.098551630973816\n",
      "Epoch 322, Loss: 4.828507423400879, Final Batch Loss: 0.9484504461288452\n",
      "Epoch 323, Loss: 4.814868450164795, Final Batch Loss: 0.8874137997627258\n",
      "Epoch 324, Loss: 4.8382110595703125, Final Batch Loss: 0.9572585821151733\n",
      "Epoch 325, Loss: 4.892735481262207, Final Batch Loss: 1.0082428455352783\n",
      "Epoch 326, Loss: 4.890380382537842, Final Batch Loss: 0.9676766395568848\n",
      "Epoch 327, Loss: 4.850096344947815, Final Batch Loss: 0.897182285785675\n",
      "Epoch 328, Loss: 4.820516347885132, Final Batch Loss: 0.9098153710365295\n",
      "Epoch 329, Loss: 5.002898216247559, Final Batch Loss: 1.0694832801818848\n",
      "Epoch 330, Loss: 4.9121410846710205, Final Batch Loss: 0.998077929019928\n",
      "Epoch 331, Loss: 4.843326807022095, Final Batch Loss: 0.9742692708969116\n",
      "Epoch 332, Loss: 4.868296325206757, Final Batch Loss: 0.9428158402442932\n",
      "Epoch 333, Loss: 4.839793086051941, Final Batch Loss: 1.0283153057098389\n",
      "Epoch 334, Loss: 4.656813025474548, Final Batch Loss: 0.8209724426269531\n",
      "Epoch 335, Loss: 4.9219361543655396, Final Batch Loss: 0.9690595865249634\n",
      "Epoch 336, Loss: 4.89251434803009, Final Batch Loss: 0.981738805770874\n",
      "Epoch 337, Loss: 4.952604174613953, Final Batch Loss: 1.0506356954574585\n",
      "Epoch 338, Loss: 4.788289666175842, Final Batch Loss: 0.9682933688163757\n",
      "Epoch 339, Loss: 4.743310332298279, Final Batch Loss: 0.8995381593704224\n",
      "Epoch 340, Loss: 4.97404670715332, Final Batch Loss: 0.9027073383331299\n",
      "Epoch 341, Loss: 4.892955839633942, Final Batch Loss: 1.0308475494384766\n",
      "Epoch 342, Loss: 4.712382793426514, Final Batch Loss: 0.8804049491882324\n",
      "Epoch 343, Loss: 4.776212811470032, Final Batch Loss: 0.9684414863586426\n",
      "Epoch 344, Loss: 4.75778466463089, Final Batch Loss: 0.9497034549713135\n",
      "Epoch 345, Loss: 4.737730920314789, Final Batch Loss: 0.9386914372444153\n",
      "Epoch 346, Loss: 4.752279162406921, Final Batch Loss: 0.9469397068023682\n",
      "Epoch 347, Loss: 4.77178293466568, Final Batch Loss: 0.9325194358825684\n",
      "Epoch 348, Loss: 4.789893686771393, Final Batch Loss: 0.9503293633460999\n",
      "Epoch 349, Loss: 4.797772407531738, Final Batch Loss: 0.9691972136497498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, Loss: 4.7963356375694275, Final Batch Loss: 0.8737180829048157\n",
      "Epoch 351, Loss: 4.919398546218872, Final Batch Loss: 1.0622638463974\n",
      "Epoch 352, Loss: 4.742830276489258, Final Batch Loss: 1.0197713375091553\n",
      "Epoch 353, Loss: 4.746372759342194, Final Batch Loss: 0.970211386680603\n",
      "Epoch 354, Loss: 4.740133821964264, Final Batch Loss: 0.8992952704429626\n",
      "Epoch 355, Loss: 4.798991620540619, Final Batch Loss: 0.9829128980636597\n",
      "Epoch 356, Loss: 4.67894172668457, Final Batch Loss: 0.8822739720344543\n",
      "Epoch 357, Loss: 4.654011607170105, Final Batch Loss: 0.9391233325004578\n",
      "Epoch 358, Loss: 4.813131809234619, Final Batch Loss: 0.9345154762268066\n",
      "Epoch 359, Loss: 4.886709272861481, Final Batch Loss: 1.0563281774520874\n",
      "Epoch 360, Loss: 4.722227096557617, Final Batch Loss: 1.0633641481399536\n",
      "Epoch 361, Loss: 4.675383746623993, Final Batch Loss: 0.8798708319664001\n",
      "Epoch 362, Loss: 4.7752795815467834, Final Batch Loss: 1.043976068496704\n",
      "Epoch 363, Loss: 4.719308793544769, Final Batch Loss: 0.8853279948234558\n",
      "Epoch 364, Loss: 4.601651072502136, Final Batch Loss: 0.9361880421638489\n",
      "Epoch 365, Loss: 4.739392340183258, Final Batch Loss: 0.9204114079475403\n",
      "Epoch 366, Loss: 4.797944664955139, Final Batch Loss: 0.980185866355896\n",
      "Epoch 367, Loss: 4.692526459693909, Final Batch Loss: 0.9788092374801636\n",
      "Epoch 368, Loss: 4.650093376636505, Final Batch Loss: 0.9044370651245117\n",
      "Epoch 369, Loss: 4.670040488243103, Final Batch Loss: 1.021814227104187\n",
      "Epoch 370, Loss: 4.661422252655029, Final Batch Loss: 1.0092774629592896\n",
      "Epoch 371, Loss: 4.732246279716492, Final Batch Loss: 0.9650106430053711\n",
      "Epoch 372, Loss: 4.652698874473572, Final Batch Loss: 0.8604514598846436\n",
      "Epoch 373, Loss: 4.722803831100464, Final Batch Loss: 1.031624436378479\n",
      "Epoch 374, Loss: 4.730093240737915, Final Batch Loss: 0.9525768160820007\n",
      "Epoch 375, Loss: 4.571689784526825, Final Batch Loss: 0.9262622594833374\n",
      "Epoch 376, Loss: 4.632599651813507, Final Batch Loss: 0.8098003268241882\n",
      "Epoch 377, Loss: 4.624332785606384, Final Batch Loss: 0.9271078109741211\n",
      "Epoch 378, Loss: 4.655019760131836, Final Batch Loss: 1.0071451663970947\n",
      "Epoch 379, Loss: 4.464679837226868, Final Batch Loss: 0.904775083065033\n",
      "Epoch 380, Loss: 4.6118358969688416, Final Batch Loss: 0.8465589880943298\n",
      "Epoch 381, Loss: 4.631837546825409, Final Batch Loss: 0.9615858197212219\n",
      "Epoch 382, Loss: 4.676046192646027, Final Batch Loss: 0.859458327293396\n",
      "Epoch 383, Loss: 4.687620222568512, Final Batch Loss: 0.8982171416282654\n",
      "Epoch 384, Loss: 4.602233052253723, Final Batch Loss: 0.9209549427032471\n",
      "Epoch 385, Loss: 4.708367168903351, Final Batch Loss: 0.8798633217811584\n",
      "Epoch 386, Loss: 4.630978345870972, Final Batch Loss: 0.9072746634483337\n",
      "Epoch 387, Loss: 4.606859862804413, Final Batch Loss: 0.9760889410972595\n",
      "Epoch 388, Loss: 4.674079358577728, Final Batch Loss: 0.9605960249900818\n",
      "Epoch 389, Loss: 4.5582886934280396, Final Batch Loss: 0.9029432535171509\n",
      "Epoch 390, Loss: 4.497215569019318, Final Batch Loss: 0.8553677201271057\n",
      "Epoch 391, Loss: 4.621967136859894, Final Batch Loss: 0.9624198079109192\n",
      "Epoch 392, Loss: 4.633061647415161, Final Batch Loss: 0.9276454448699951\n",
      "Epoch 393, Loss: 4.628178238868713, Final Batch Loss: 0.9208453893661499\n",
      "Epoch 394, Loss: 4.733746528625488, Final Batch Loss: 0.8653655052185059\n",
      "Epoch 395, Loss: 4.655645728111267, Final Batch Loss: 0.9268584251403809\n",
      "Epoch 396, Loss: 4.494195103645325, Final Batch Loss: 0.8761811256408691\n",
      "Epoch 397, Loss: 4.61297219991684, Final Batch Loss: 0.9590700268745422\n",
      "Epoch 398, Loss: 4.65217250585556, Final Batch Loss: 0.9891372919082642\n",
      "Epoch 399, Loss: 4.5316707491874695, Final Batch Loss: 0.9285773038864136\n",
      "Epoch 400, Loss: 4.4821906089782715, Final Batch Loss: 0.8872793316841125\n",
      "Epoch 401, Loss: 4.57412326335907, Final Batch Loss: 0.9516329169273376\n",
      "Epoch 402, Loss: 4.666783452033997, Final Batch Loss: 1.0212944746017456\n",
      "Epoch 403, Loss: 4.51622349023819, Final Batch Loss: 1.0077120065689087\n",
      "Epoch 404, Loss: 4.610861718654633, Final Batch Loss: 0.954922616481781\n",
      "Epoch 405, Loss: 4.480958104133606, Final Batch Loss: 0.9511544108390808\n",
      "Epoch 406, Loss: 4.561085522174835, Final Batch Loss: 0.8651087284088135\n",
      "Epoch 407, Loss: 4.6568081974983215, Final Batch Loss: 1.068955659866333\n",
      "Epoch 408, Loss: 4.4698567390441895, Final Batch Loss: 0.8412007689476013\n",
      "Epoch 409, Loss: 4.54774135351181, Final Batch Loss: 0.8969558477401733\n",
      "Epoch 410, Loss: 4.540606260299683, Final Batch Loss: 0.969192624092102\n",
      "Epoch 411, Loss: 4.572837769985199, Final Batch Loss: 0.9503417015075684\n",
      "Epoch 412, Loss: 4.414588272571564, Final Batch Loss: 0.9511823058128357\n",
      "Epoch 413, Loss: 4.442068338394165, Final Batch Loss: 0.8574079871177673\n",
      "Epoch 414, Loss: 4.504180431365967, Final Batch Loss: 0.7968021035194397\n",
      "Epoch 415, Loss: 4.481958389282227, Final Batch Loss: 0.9904156923294067\n",
      "Epoch 416, Loss: 4.485391139984131, Final Batch Loss: 0.8288173079490662\n",
      "Epoch 417, Loss: 4.398788869380951, Final Batch Loss: 0.8337938785552979\n",
      "Epoch 418, Loss: 4.438339471817017, Final Batch Loss: 0.9283347129821777\n",
      "Epoch 419, Loss: 4.583866059780121, Final Batch Loss: 0.9662812352180481\n",
      "Epoch 420, Loss: 4.43949830532074, Final Batch Loss: 0.8396039009094238\n",
      "Epoch 421, Loss: 4.57563853263855, Final Batch Loss: 0.9632326364517212\n",
      "Epoch 422, Loss: 4.462690830230713, Final Batch Loss: 0.911986231803894\n",
      "Epoch 423, Loss: 4.596791088581085, Final Batch Loss: 0.9198326468467712\n",
      "Epoch 424, Loss: 4.521369159221649, Final Batch Loss: 0.9125353693962097\n",
      "Epoch 425, Loss: 4.590540766716003, Final Batch Loss: 1.0074471235275269\n",
      "Epoch 426, Loss: 4.261440098285675, Final Batch Loss: 0.7571754455566406\n",
      "Epoch 427, Loss: 4.536464512348175, Final Batch Loss: 0.9575549960136414\n",
      "Epoch 428, Loss: 4.416230380535126, Final Batch Loss: 0.9089421629905701\n",
      "Epoch 429, Loss: 4.4493595361709595, Final Batch Loss: 0.8470962047576904\n",
      "Epoch 430, Loss: 4.371293842792511, Final Batch Loss: 0.9674928784370422\n",
      "Epoch 431, Loss: 4.386199235916138, Final Batch Loss: 0.8831673264503479\n",
      "Epoch 432, Loss: 4.329420983791351, Final Batch Loss: 0.8553036451339722\n",
      "Epoch 433, Loss: 4.521864175796509, Final Batch Loss: 0.9097675085067749\n",
      "Epoch 434, Loss: 4.406159162521362, Final Batch Loss: 0.9984004497528076\n",
      "Epoch 435, Loss: 4.43082994222641, Final Batch Loss: 0.9826668500900269\n",
      "Epoch 436, Loss: 4.29842472076416, Final Batch Loss: 0.9217511415481567\n",
      "Epoch 437, Loss: 4.479051232337952, Final Batch Loss: 0.8706462383270264\n",
      "Epoch 438, Loss: 4.310279905796051, Final Batch Loss: 0.887674868106842\n",
      "Epoch 439, Loss: 4.353991806507111, Final Batch Loss: 0.8606952428817749\n",
      "Epoch 440, Loss: 4.366481363773346, Final Batch Loss: 0.8823530077934265\n",
      "Epoch 441, Loss: 4.550051748752594, Final Batch Loss: 0.8899614214897156\n",
      "Epoch 442, Loss: 4.3424460887908936, Final Batch Loss: 0.9371955394744873\n",
      "Epoch 443, Loss: 4.524653494358063, Final Batch Loss: 0.8853721022605896\n",
      "Epoch 444, Loss: 4.366884529590607, Final Batch Loss: 0.8731558918952942\n",
      "Epoch 445, Loss: 4.4141106605529785, Final Batch Loss: 0.8827908039093018\n",
      "Epoch 446, Loss: 4.27500993013382, Final Batch Loss: 0.9089200496673584\n",
      "Epoch 447, Loss: 4.40849244594574, Final Batch Loss: 0.9868733286857605\n",
      "Epoch 448, Loss: 4.511472642421722, Final Batch Loss: 0.9442757368087769\n",
      "Epoch 449, Loss: 4.342523038387299, Final Batch Loss: 0.8154247999191284\n",
      "Epoch 450, Loss: 4.373215734958649, Final Batch Loss: 0.8973836898803711\n",
      "Epoch 451, Loss: 4.415313184261322, Final Batch Loss: 0.9667317271232605\n",
      "Epoch 452, Loss: 4.326659381389618, Final Batch Loss: 0.7696216702461243\n",
      "Epoch 453, Loss: 4.495255172252655, Final Batch Loss: 1.058903455734253\n",
      "Epoch 454, Loss: 4.4408732652664185, Final Batch Loss: 0.7806602120399475\n",
      "Epoch 455, Loss: 4.379688322544098, Final Batch Loss: 0.9750581979751587\n",
      "Epoch 456, Loss: 4.419169723987579, Final Batch Loss: 0.9218065738677979\n",
      "Epoch 457, Loss: 4.308515667915344, Final Batch Loss: 0.9578194618225098\n",
      "Epoch 458, Loss: 4.45745050907135, Final Batch Loss: 1.0283668041229248\n",
      "Epoch 459, Loss: 4.359773933887482, Final Batch Loss: 0.825772225856781\n",
      "Epoch 460, Loss: 4.375103652477264, Final Batch Loss: 0.9368075728416443\n",
      "Epoch 461, Loss: 4.497233986854553, Final Batch Loss: 1.0054962635040283\n",
      "Epoch 462, Loss: 4.296724081039429, Final Batch Loss: 0.826253354549408\n",
      "Epoch 463, Loss: 4.314974248409271, Final Batch Loss: 0.8641162514686584\n",
      "Epoch 464, Loss: 4.4469398856163025, Final Batch Loss: 0.8197287917137146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465, Loss: 4.281888961791992, Final Batch Loss: 0.8395787477493286\n",
      "Epoch 466, Loss: 4.417334735393524, Final Batch Loss: 0.8447540998458862\n",
      "Epoch 467, Loss: 4.138565838336945, Final Batch Loss: 0.8290268182754517\n",
      "Epoch 468, Loss: 4.172873497009277, Final Batch Loss: 0.7716038823127747\n",
      "Epoch 469, Loss: 4.3564929366111755, Final Batch Loss: 0.8727796077728271\n",
      "Epoch 470, Loss: 4.268386781215668, Final Batch Loss: 0.7743549346923828\n",
      "Epoch 471, Loss: 4.191298544406891, Final Batch Loss: 0.7113445997238159\n",
      "Epoch 472, Loss: 4.136130392551422, Final Batch Loss: 0.8425297737121582\n",
      "Epoch 473, Loss: 4.303600072860718, Final Batch Loss: 0.8321967124938965\n",
      "Epoch 474, Loss: 4.6252442598342896, Final Batch Loss: 0.9404119253158569\n",
      "Epoch 475, Loss: 4.315231919288635, Final Batch Loss: 0.9510118961334229\n",
      "Epoch 476, Loss: 4.302041232585907, Final Batch Loss: 0.8225845694541931\n",
      "Epoch 477, Loss: 4.320034384727478, Final Batch Loss: 0.832639217376709\n",
      "Epoch 478, Loss: 4.23427414894104, Final Batch Loss: 0.8477213978767395\n",
      "Epoch 479, Loss: 4.222741603851318, Final Batch Loss: 0.7962621450424194\n",
      "Epoch 480, Loss: 4.363431632518768, Final Batch Loss: 0.875484049320221\n",
      "Epoch 481, Loss: 4.199024677276611, Final Batch Loss: 0.8328171372413635\n",
      "Epoch 482, Loss: 4.477774739265442, Final Batch Loss: 1.0391064882278442\n",
      "Epoch 483, Loss: 4.296350002288818, Final Batch Loss: 0.8239056468009949\n",
      "Epoch 484, Loss: 4.255732715129852, Final Batch Loss: 0.843734860420227\n",
      "Epoch 485, Loss: 4.491923213005066, Final Batch Loss: 1.0716273784637451\n",
      "Epoch 486, Loss: 4.324055075645447, Final Batch Loss: 0.8498409986495972\n",
      "Epoch 487, Loss: 4.197264134883881, Final Batch Loss: 0.8765199780464172\n",
      "Epoch 488, Loss: 4.314823925495148, Final Batch Loss: 0.8815504312515259\n",
      "Epoch 489, Loss: 4.339519560337067, Final Batch Loss: 0.7871930599212646\n",
      "Epoch 490, Loss: 4.357322633266449, Final Batch Loss: 1.039960265159607\n",
      "Epoch 491, Loss: 4.149032890796661, Final Batch Loss: 0.7909282445907593\n",
      "Epoch 492, Loss: 4.216262102127075, Final Batch Loss: 0.8601415157318115\n",
      "Epoch 493, Loss: 4.285921990871429, Final Batch Loss: 0.7721951007843018\n",
      "Epoch 494, Loss: 4.361666142940521, Final Batch Loss: 0.8896657228469849\n",
      "Epoch 495, Loss: 4.356997847557068, Final Batch Loss: 0.8799933791160583\n",
      "Epoch 496, Loss: 4.3432976603508, Final Batch Loss: 0.9247968792915344\n",
      "Epoch 497, Loss: 4.251307308673859, Final Batch Loss: 0.8734396696090698\n",
      "Epoch 498, Loss: 4.294053792953491, Final Batch Loss: 0.940211832523346\n",
      "Epoch 499, Loss: 4.387439250946045, Final Batch Loss: 0.9535859227180481\n",
      "Epoch 500, Loss: 4.304097533226013, Final Batch Loss: 0.9305906295776367\n",
      "Epoch 501, Loss: 4.049198031425476, Final Batch Loss: 0.7167907953262329\n",
      "Epoch 502, Loss: 4.146230041980743, Final Batch Loss: 0.9097474217414856\n",
      "Epoch 503, Loss: 4.208678305149078, Final Batch Loss: 0.8289856910705566\n",
      "Epoch 504, Loss: 4.11393803358078, Final Batch Loss: 0.6880767941474915\n",
      "Epoch 505, Loss: 4.1799561977386475, Final Batch Loss: 0.9057998061180115\n",
      "Epoch 506, Loss: 4.0076287388801575, Final Batch Loss: 0.7874308228492737\n",
      "Epoch 507, Loss: 4.438669145107269, Final Batch Loss: 0.9908424019813538\n",
      "Epoch 508, Loss: 4.156015634536743, Final Batch Loss: 0.8508205413818359\n",
      "Epoch 509, Loss: 4.282367050647736, Final Batch Loss: 0.8516541123390198\n",
      "Epoch 510, Loss: 4.292762517929077, Final Batch Loss: 0.8971043229103088\n",
      "Epoch 511, Loss: 4.1086825132369995, Final Batch Loss: 0.7878021597862244\n",
      "Epoch 512, Loss: 4.170899212360382, Final Batch Loss: 0.9329182505607605\n",
      "Epoch 513, Loss: 4.326205253601074, Final Batch Loss: 0.9215691089630127\n",
      "Epoch 514, Loss: 4.148691117763519, Final Batch Loss: 0.8020453453063965\n",
      "Epoch 515, Loss: 4.18564236164093, Final Batch Loss: 0.8669536113739014\n",
      "Epoch 516, Loss: 4.230318605899811, Final Batch Loss: 0.9315165281295776\n",
      "Epoch 517, Loss: 4.30240786075592, Final Batch Loss: 0.8710249662399292\n",
      "Epoch 518, Loss: 4.092656970024109, Final Batch Loss: 0.875522792339325\n",
      "Epoch 519, Loss: 4.153010129928589, Final Batch Loss: 0.7511621117591858\n",
      "Epoch 520, Loss: 4.288652777671814, Final Batch Loss: 0.9344748854637146\n",
      "Epoch 521, Loss: 4.070881187915802, Final Batch Loss: 0.818570077419281\n",
      "Epoch 522, Loss: 4.123356819152832, Final Batch Loss: 0.7985479235649109\n",
      "Epoch 523, Loss: 4.366106450557709, Final Batch Loss: 0.9375947117805481\n",
      "Epoch 524, Loss: 4.202273786067963, Final Batch Loss: 0.91709965467453\n",
      "Epoch 525, Loss: 4.115873336791992, Final Batch Loss: 0.8383253216743469\n",
      "Epoch 526, Loss: 4.171909749507904, Final Batch Loss: 0.8673869371414185\n",
      "Epoch 527, Loss: 4.175350069999695, Final Batch Loss: 0.831392765045166\n",
      "Epoch 528, Loss: 4.0146613121032715, Final Batch Loss: 0.8333093523979187\n",
      "Epoch 529, Loss: 4.003774464130402, Final Batch Loss: 0.7144424319267273\n",
      "Epoch 530, Loss: 4.066651701927185, Final Batch Loss: 0.8349913954734802\n",
      "Epoch 531, Loss: 4.1176682114601135, Final Batch Loss: 0.7671332359313965\n",
      "Epoch 532, Loss: 4.254700481891632, Final Batch Loss: 0.8963327407836914\n",
      "Epoch 533, Loss: 4.170463144779205, Final Batch Loss: 0.8997824788093567\n",
      "Epoch 534, Loss: 4.158455789089203, Final Batch Loss: 0.9032199382781982\n",
      "Epoch 535, Loss: 4.0049227476119995, Final Batch Loss: 0.8307909369468689\n",
      "Epoch 536, Loss: 4.251438319683075, Final Batch Loss: 0.9440813660621643\n",
      "Epoch 537, Loss: 4.178835511207581, Final Batch Loss: 0.7238076329231262\n",
      "Epoch 538, Loss: 4.013469457626343, Final Batch Loss: 0.8129348158836365\n",
      "Epoch 539, Loss: 4.008772373199463, Final Batch Loss: 0.7813058495521545\n",
      "Epoch 540, Loss: 4.156396567821503, Final Batch Loss: 0.8643204569816589\n",
      "Epoch 541, Loss: 3.9956393241882324, Final Batch Loss: 0.7729914784431458\n",
      "Epoch 542, Loss: 4.118400454521179, Final Batch Loss: 0.7264549136161804\n",
      "Epoch 543, Loss: 4.0355037450790405, Final Batch Loss: 0.8339588642120361\n",
      "Epoch 544, Loss: 4.021085143089294, Final Batch Loss: 0.738955557346344\n",
      "Epoch 545, Loss: 4.165241241455078, Final Batch Loss: 0.9135392904281616\n",
      "Epoch 546, Loss: 4.189971923828125, Final Batch Loss: 0.8071527481079102\n",
      "Epoch 547, Loss: 3.9852119088172913, Final Batch Loss: 0.661581814289093\n",
      "Epoch 548, Loss: 4.101188957691193, Final Batch Loss: 0.8665249347686768\n",
      "Epoch 549, Loss: 4.200867176055908, Final Batch Loss: 0.8722942471504211\n",
      "Epoch 550, Loss: 4.220412015914917, Final Batch Loss: 0.8860965967178345\n",
      "Epoch 551, Loss: 4.050660252571106, Final Batch Loss: 0.8841512799263\n",
      "Epoch 552, Loss: 4.088639616966248, Final Batch Loss: 0.8335903286933899\n",
      "Epoch 553, Loss: 4.121844470500946, Final Batch Loss: 0.7482236623764038\n",
      "Epoch 554, Loss: 4.3230814933776855, Final Batch Loss: 0.8736610412597656\n",
      "Epoch 555, Loss: 4.1205591559410095, Final Batch Loss: 0.7004872560501099\n",
      "Epoch 556, Loss: 4.23837012052536, Final Batch Loss: 0.8725314736366272\n",
      "Epoch 557, Loss: 4.116253912448883, Final Batch Loss: 0.8409357070922852\n",
      "Epoch 558, Loss: 4.145539283752441, Final Batch Loss: 0.7934029698371887\n",
      "Epoch 559, Loss: 4.21960312128067, Final Batch Loss: 0.9435754418373108\n",
      "Epoch 560, Loss: 4.131538808345795, Final Batch Loss: 0.839190661907196\n",
      "Epoch 561, Loss: 4.039939045906067, Final Batch Loss: 0.712935745716095\n",
      "Epoch 562, Loss: 4.0795093178749084, Final Batch Loss: 0.7478675246238708\n",
      "Epoch 563, Loss: 4.156187534332275, Final Batch Loss: 0.8343368768692017\n",
      "Epoch 564, Loss: 4.202712953090668, Final Batch Loss: 0.7608899474143982\n",
      "Epoch 565, Loss: 4.077169418334961, Final Batch Loss: 0.8017725348472595\n",
      "Epoch 566, Loss: 4.030669987201691, Final Batch Loss: 0.8137070536613464\n",
      "Epoch 567, Loss: 4.118078231811523, Final Batch Loss: 0.8940869569778442\n",
      "Epoch 568, Loss: 4.112608730792999, Final Batch Loss: 0.8383114337921143\n",
      "Epoch 569, Loss: 3.989917755126953, Final Batch Loss: 0.8304535150527954\n",
      "Epoch 570, Loss: 4.013037383556366, Final Batch Loss: 0.8406625390052795\n",
      "Epoch 571, Loss: 4.2156259417533875, Final Batch Loss: 0.8563421368598938\n",
      "Epoch 572, Loss: 3.986590266227722, Final Batch Loss: 0.6819274425506592\n",
      "Epoch 573, Loss: 4.097924292087555, Final Batch Loss: 0.8032612204551697\n",
      "Epoch 574, Loss: 4.035154819488525, Final Batch Loss: 0.9079669713973999\n",
      "Epoch 575, Loss: 4.08103209733963, Final Batch Loss: 0.8348332047462463\n",
      "Epoch 576, Loss: 4.084838390350342, Final Batch Loss: 0.6772334575653076\n",
      "Epoch 577, Loss: 4.099193572998047, Final Batch Loss: 0.811488687992096\n",
      "Epoch 578, Loss: 4.22705614566803, Final Batch Loss: 1.0092830657958984\n",
      "Epoch 579, Loss: 4.023123741149902, Final Batch Loss: 0.9352004528045654\n",
      "Epoch 580, Loss: 4.139200747013092, Final Batch Loss: 0.8627362251281738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 581, Loss: 4.101456701755524, Final Batch Loss: 0.7926763892173767\n",
      "Epoch 582, Loss: 4.066276013851166, Final Batch Loss: 0.8764853477478027\n",
      "Epoch 583, Loss: 4.020437717437744, Final Batch Loss: 0.8853399753570557\n",
      "Epoch 584, Loss: 4.1221184730529785, Final Batch Loss: 0.862496554851532\n",
      "Epoch 585, Loss: 4.172791421413422, Final Batch Loss: 0.8802088499069214\n",
      "Epoch 586, Loss: 4.150289297103882, Final Batch Loss: 0.8388378024101257\n",
      "Epoch 587, Loss: 3.9424611926078796, Final Batch Loss: 0.7634257674217224\n",
      "Epoch 588, Loss: 3.9751046299934387, Final Batch Loss: 0.8712878227233887\n",
      "Epoch 589, Loss: 3.960869550704956, Final Batch Loss: 0.721019446849823\n",
      "Epoch 590, Loss: 4.075682461261749, Final Batch Loss: 0.7805830240249634\n",
      "Epoch 591, Loss: 4.038596570491791, Final Batch Loss: 0.911106526851654\n",
      "Epoch 592, Loss: 4.001075923442841, Final Batch Loss: 0.8234449028968811\n",
      "Epoch 593, Loss: 3.9340564608573914, Final Batch Loss: 0.6403307914733887\n",
      "Epoch 594, Loss: 4.0190509557724, Final Batch Loss: 0.8601881861686707\n",
      "Epoch 595, Loss: 4.0554051995277405, Final Batch Loss: 0.9519500732421875\n",
      "Epoch 596, Loss: 3.831604778766632, Final Batch Loss: 0.77199387550354\n",
      "Epoch 597, Loss: 4.022545754909515, Final Batch Loss: 0.6685857772827148\n",
      "Epoch 598, Loss: 4.009667813777924, Final Batch Loss: 0.7070165872573853\n",
      "Epoch 599, Loss: 3.9134953022003174, Final Batch Loss: 0.7361786961555481\n",
      "Epoch 600, Loss: 4.149881422519684, Final Batch Loss: 0.8278119564056396\n",
      "Epoch 601, Loss: 4.107754111289978, Final Batch Loss: 0.7430760264396667\n",
      "Epoch 602, Loss: 3.941837787628174, Final Batch Loss: 0.7432331442832947\n",
      "Epoch 603, Loss: 4.087067663669586, Final Batch Loss: 0.7661975622177124\n",
      "Epoch 604, Loss: 3.927111566066742, Final Batch Loss: 0.8429663777351379\n",
      "Epoch 605, Loss: 4.083224713802338, Final Batch Loss: 0.7776596546173096\n",
      "Epoch 606, Loss: 4.00599217414856, Final Batch Loss: 0.8029137849807739\n",
      "Epoch 607, Loss: 4.177015542984009, Final Batch Loss: 0.909730076789856\n",
      "Epoch 608, Loss: 3.8961633443832397, Final Batch Loss: 0.7800950407981873\n",
      "Epoch 609, Loss: 4.199819564819336, Final Batch Loss: 0.8479827642440796\n",
      "Epoch 610, Loss: 4.204610824584961, Final Batch Loss: 0.779951810836792\n",
      "Epoch 611, Loss: 4.076075971126556, Final Batch Loss: 0.9173956513404846\n",
      "Epoch 612, Loss: 4.166373074054718, Final Batch Loss: 0.9541323184967041\n",
      "Epoch 613, Loss: 4.0167142152786255, Final Batch Loss: 0.9794489145278931\n",
      "Epoch 614, Loss: 3.889308452606201, Final Batch Loss: 0.685793936252594\n",
      "Epoch 615, Loss: 4.139618217945099, Final Batch Loss: 0.899124264717102\n",
      "Epoch 616, Loss: 3.906842529773712, Final Batch Loss: 0.8034531474113464\n",
      "Epoch 617, Loss: 4.020329296588898, Final Batch Loss: 0.8011446595191956\n",
      "Epoch 618, Loss: 4.041883170604706, Final Batch Loss: 0.8671642541885376\n",
      "Epoch 619, Loss: 3.877157151699066, Final Batch Loss: 0.8357802033424377\n",
      "Epoch 620, Loss: 3.9506463408470154, Final Batch Loss: 0.7429149746894836\n",
      "Epoch 621, Loss: 4.0213852524757385, Final Batch Loss: 0.8357067704200745\n",
      "Epoch 622, Loss: 3.8175593614578247, Final Batch Loss: 0.7812021374702454\n",
      "Epoch 623, Loss: 4.006545603275299, Final Batch Loss: 0.8436951637268066\n",
      "Epoch 624, Loss: 4.054323196411133, Final Batch Loss: 0.8339204788208008\n",
      "Epoch 625, Loss: 3.96651953458786, Final Batch Loss: 0.7235757112503052\n",
      "Epoch 626, Loss: 3.9598188996315002, Final Batch Loss: 0.7527928352355957\n",
      "Epoch 627, Loss: 3.9336125254631042, Final Batch Loss: 0.7984103560447693\n",
      "Epoch 628, Loss: 4.011400580406189, Final Batch Loss: 0.7976797819137573\n",
      "Epoch 629, Loss: 4.073575377464294, Final Batch Loss: 0.8917726874351501\n",
      "Epoch 630, Loss: 3.9916852712631226, Final Batch Loss: 0.7853083610534668\n",
      "Epoch 631, Loss: 3.8930155634880066, Final Batch Loss: 0.8499928116798401\n",
      "Epoch 632, Loss: 3.803324818611145, Final Batch Loss: 0.7207319140434265\n",
      "Epoch 633, Loss: 3.88509738445282, Final Batch Loss: 0.8160611391067505\n",
      "Epoch 634, Loss: 4.020903825759888, Final Batch Loss: 0.9121449589729309\n",
      "Epoch 635, Loss: 3.925558626651764, Final Batch Loss: 0.762093722820282\n",
      "Epoch 636, Loss: 3.97777783870697, Final Batch Loss: 0.8370282053947449\n",
      "Epoch 637, Loss: 3.9599393010139465, Final Batch Loss: 0.7871195673942566\n",
      "Epoch 638, Loss: 3.87034410238266, Final Batch Loss: 0.8589954972267151\n",
      "Epoch 639, Loss: 3.8387036323547363, Final Batch Loss: 0.8360674381256104\n",
      "Epoch 640, Loss: 3.9162724018096924, Final Batch Loss: 0.7510169744491577\n",
      "Epoch 641, Loss: 3.9796770811080933, Final Batch Loss: 0.7536259889602661\n",
      "Epoch 642, Loss: 3.993882656097412, Final Batch Loss: 0.7589457035064697\n",
      "Epoch 643, Loss: 4.000034987926483, Final Batch Loss: 0.7416819334030151\n",
      "Epoch 644, Loss: 4.084285259246826, Final Batch Loss: 0.9803687930107117\n",
      "Epoch 645, Loss: 3.8428511023521423, Final Batch Loss: 0.6963856220245361\n",
      "Epoch 646, Loss: 3.9070210456848145, Final Batch Loss: 0.8130913376808167\n",
      "Epoch 647, Loss: 3.998710513114929, Final Batch Loss: 0.7629187107086182\n",
      "Epoch 648, Loss: 3.9409687519073486, Final Batch Loss: 0.7465566992759705\n",
      "Epoch 649, Loss: 3.9854657649993896, Final Batch Loss: 0.8303169012069702\n",
      "Epoch 650, Loss: 3.9978463649749756, Final Batch Loss: 0.7485531568527222\n",
      "Epoch 651, Loss: 3.7662217020988464, Final Batch Loss: 0.6695748567581177\n",
      "Epoch 652, Loss: 3.977922201156616, Final Batch Loss: 0.7745172381401062\n",
      "Epoch 653, Loss: 3.905946135520935, Final Batch Loss: 0.8227927088737488\n",
      "Epoch 654, Loss: 4.000327110290527, Final Batch Loss: 0.8454601764678955\n",
      "Epoch 655, Loss: 3.9455642700195312, Final Batch Loss: 0.8471287488937378\n",
      "Epoch 656, Loss: 3.924033999443054, Final Batch Loss: 0.752703845500946\n",
      "Epoch 657, Loss: 3.909855544567108, Final Batch Loss: 0.7998523116111755\n",
      "Epoch 658, Loss: 3.8018084168434143, Final Batch Loss: 0.7011737823486328\n",
      "Epoch 659, Loss: 3.995831847190857, Final Batch Loss: 0.7170032262802124\n",
      "Epoch 660, Loss: 3.9421977400779724, Final Batch Loss: 0.8245208263397217\n",
      "Epoch 661, Loss: 3.937699019908905, Final Batch Loss: 0.7357088923454285\n",
      "Epoch 662, Loss: 3.9526479244232178, Final Batch Loss: 0.748622477054596\n",
      "Epoch 663, Loss: 3.8491541147232056, Final Batch Loss: 0.8365116119384766\n",
      "Epoch 664, Loss: 4.027542591094971, Final Batch Loss: 0.8601688742637634\n",
      "Epoch 665, Loss: 3.746753752231598, Final Batch Loss: 0.7656377553939819\n",
      "Epoch 666, Loss: 4.01638799905777, Final Batch Loss: 0.7832741141319275\n",
      "Epoch 667, Loss: 3.772740125656128, Final Batch Loss: 0.7226106524467468\n",
      "Epoch 668, Loss: 3.840246260166168, Final Batch Loss: 0.7539904713630676\n",
      "Epoch 669, Loss: 4.003507614135742, Final Batch Loss: 0.7678994536399841\n",
      "Epoch 670, Loss: 3.9390780329704285, Final Batch Loss: 0.8766382336616516\n",
      "Epoch 671, Loss: 4.054585337638855, Final Batch Loss: 0.8064963221549988\n",
      "Epoch 672, Loss: 4.012601435184479, Final Batch Loss: 0.9048647284507751\n",
      "Epoch 673, Loss: 3.970991313457489, Final Batch Loss: 0.8427050709724426\n",
      "Epoch 674, Loss: 3.8003746271133423, Final Batch Loss: 0.7182156443595886\n",
      "Epoch 675, Loss: 3.908968985080719, Final Batch Loss: 0.7166547179222107\n",
      "Epoch 676, Loss: 3.949333965778351, Final Batch Loss: 0.6979824900627136\n",
      "Epoch 677, Loss: 3.898962914943695, Final Batch Loss: 0.7449650168418884\n",
      "Epoch 678, Loss: 3.8706803917884827, Final Batch Loss: 0.838299036026001\n",
      "Epoch 679, Loss: 3.8556355237960815, Final Batch Loss: 0.7687335014343262\n",
      "Epoch 680, Loss: 3.79756098985672, Final Batch Loss: 0.7681552767753601\n",
      "Epoch 681, Loss: 3.864512860774994, Final Batch Loss: 0.6963237524032593\n",
      "Epoch 682, Loss: 3.6405582427978516, Final Batch Loss: 0.7033558487892151\n",
      "Epoch 683, Loss: 3.883432388305664, Final Batch Loss: 0.7880160212516785\n",
      "Epoch 684, Loss: 3.7165557742118835, Final Batch Loss: 0.6267157793045044\n",
      "Epoch 685, Loss: 3.8690362572669983, Final Batch Loss: 0.8287677764892578\n",
      "Epoch 686, Loss: 3.8539693355560303, Final Batch Loss: 0.8532570004463196\n",
      "Epoch 687, Loss: 3.8075627088546753, Final Batch Loss: 0.7874245047569275\n",
      "Epoch 688, Loss: 3.7307308316230774, Final Batch Loss: 0.7229365110397339\n",
      "Epoch 689, Loss: 3.830525279045105, Final Batch Loss: 0.79493647813797\n",
      "Epoch 690, Loss: 3.9845357537269592, Final Batch Loss: 0.864860475063324\n",
      "Epoch 691, Loss: 3.9579272270202637, Final Batch Loss: 0.834715723991394\n",
      "Epoch 692, Loss: 3.8093894720077515, Final Batch Loss: 0.7664143443107605\n",
      "Epoch 693, Loss: 3.78965824842453, Final Batch Loss: 0.668126106262207\n",
      "Epoch 694, Loss: 3.7484092712402344, Final Batch Loss: 0.7223371267318726\n",
      "Epoch 695, Loss: 3.8977640867233276, Final Batch Loss: 0.6686444282531738\n",
      "Epoch 696, Loss: 3.8518993854522705, Final Batch Loss: 0.9081118702888489\n",
      "Epoch 697, Loss: 3.9781020283699036, Final Batch Loss: 0.9100903868675232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 698, Loss: 3.904833674430847, Final Batch Loss: 0.8099434971809387\n",
      "Epoch 699, Loss: 3.8235570192337036, Final Batch Loss: 0.7790617346763611\n",
      "Epoch 700, Loss: 3.813568890094757, Final Batch Loss: 0.8150015473365784\n",
      "Epoch 701, Loss: 4.139960765838623, Final Batch Loss: 0.8426287174224854\n",
      "Epoch 702, Loss: 3.9026910066604614, Final Batch Loss: 0.8564541339874268\n",
      "Epoch 703, Loss: 3.8203078508377075, Final Batch Loss: 0.7680650353431702\n",
      "Epoch 704, Loss: 3.8851524591445923, Final Batch Loss: 0.8239265084266663\n",
      "Epoch 705, Loss: 3.979181230068207, Final Batch Loss: 0.7693994641304016\n",
      "Epoch 706, Loss: 3.8608856797218323, Final Batch Loss: 0.8942391276359558\n",
      "Epoch 707, Loss: 3.8852416276931763, Final Batch Loss: 0.8368425965309143\n",
      "Epoch 708, Loss: 3.8159865736961365, Final Batch Loss: 0.8023643493652344\n",
      "Epoch 709, Loss: 3.7942572832107544, Final Batch Loss: 0.7896814346313477\n",
      "Epoch 710, Loss: 3.9721975922584534, Final Batch Loss: 0.8970882892608643\n",
      "Epoch 711, Loss: 3.8626063466072083, Final Batch Loss: 0.7783687114715576\n",
      "Epoch 712, Loss: 3.7154060006141663, Final Batch Loss: 0.669653058052063\n",
      "Epoch 713, Loss: 3.8544622659683228, Final Batch Loss: 0.6877369284629822\n",
      "Epoch 714, Loss: 3.9615103602409363, Final Batch Loss: 1.0020090341567993\n",
      "Epoch 715, Loss: 3.895384132862091, Final Batch Loss: 0.7554031610488892\n",
      "Epoch 716, Loss: 3.7479596734046936, Final Batch Loss: 0.7265012264251709\n",
      "Epoch 717, Loss: 4.0054017305374146, Final Batch Loss: 0.8777472376823425\n",
      "Epoch 718, Loss: 3.8365734815597534, Final Batch Loss: 0.8624078631401062\n",
      "Epoch 719, Loss: 3.9173900485038757, Final Batch Loss: 0.859915018081665\n",
      "Epoch 720, Loss: 3.7046549916267395, Final Batch Loss: 0.71429842710495\n",
      "Epoch 721, Loss: 3.76045423746109, Final Batch Loss: 0.7228056192398071\n",
      "Epoch 722, Loss: 3.882431745529175, Final Batch Loss: 0.8220697045326233\n",
      "Epoch 723, Loss: 3.7698934078216553, Final Batch Loss: 0.7576905488967896\n",
      "Epoch 724, Loss: 3.6961873173713684, Final Batch Loss: 0.709705114364624\n",
      "Epoch 725, Loss: 3.9081138372421265, Final Batch Loss: 0.8000628352165222\n",
      "Epoch 726, Loss: 3.727346122264862, Final Batch Loss: 0.6982526779174805\n",
      "Epoch 727, Loss: 3.776346445083618, Final Batch Loss: 0.7534094452857971\n",
      "Epoch 728, Loss: 3.8712233304977417, Final Batch Loss: 0.80793297290802\n",
      "Epoch 729, Loss: 3.8423725962638855, Final Batch Loss: 0.7459298372268677\n",
      "Epoch 730, Loss: 3.8472654223442078, Final Batch Loss: 0.8054243326187134\n",
      "Epoch 731, Loss: 3.7311968207359314, Final Batch Loss: 0.7277012467384338\n",
      "Epoch 732, Loss: 3.7463340163230896, Final Batch Loss: 0.7378647327423096\n",
      "Epoch 733, Loss: 3.9207813143730164, Final Batch Loss: 0.7552169561386108\n",
      "Epoch 734, Loss: 3.788940131664276, Final Batch Loss: 0.7538700103759766\n",
      "Epoch 735, Loss: 3.849339544773102, Final Batch Loss: 0.7920851111412048\n",
      "Epoch 736, Loss: 3.808069348335266, Final Batch Loss: 0.7447257041931152\n",
      "Epoch 737, Loss: 3.719521462917328, Final Batch Loss: 0.8189118504524231\n",
      "Epoch 738, Loss: 3.8760842084884644, Final Batch Loss: 0.8043152093887329\n",
      "Epoch 739, Loss: 3.8595533967018127, Final Batch Loss: 0.7806285619735718\n",
      "Epoch 740, Loss: 3.7131898403167725, Final Batch Loss: 0.6798793077468872\n",
      "Epoch 741, Loss: 3.788984537124634, Final Batch Loss: 0.7888498306274414\n",
      "Epoch 742, Loss: 3.8111825585365295, Final Batch Loss: 0.7286859750747681\n",
      "Epoch 743, Loss: 3.64533931016922, Final Batch Loss: 0.6511995196342468\n",
      "Epoch 744, Loss: 3.8414127826690674, Final Batch Loss: 0.6970728635787964\n",
      "Epoch 745, Loss: 3.8510742783546448, Final Batch Loss: 0.8660703897476196\n",
      "Epoch 746, Loss: 3.8038989901542664, Final Batch Loss: 0.8029599189758301\n",
      "Epoch 747, Loss: 3.73340380191803, Final Batch Loss: 0.6679542064666748\n",
      "Epoch 748, Loss: 3.7925702333450317, Final Batch Loss: 0.7910438776016235\n",
      "Epoch 749, Loss: 3.8812133073806763, Final Batch Loss: 0.7801936268806458\n",
      "Epoch 750, Loss: 3.7486199140548706, Final Batch Loss: 0.6942809224128723\n",
      "Epoch 751, Loss: 3.9036905169487, Final Batch Loss: 0.8456953763961792\n",
      "Epoch 752, Loss: 3.7815582752227783, Final Batch Loss: 0.7597761750221252\n",
      "Epoch 753, Loss: 3.9537470936775208, Final Batch Loss: 0.7613107562065125\n",
      "Epoch 754, Loss: 3.6058812141418457, Final Batch Loss: 0.7219940423965454\n",
      "Epoch 755, Loss: 3.8358525037765503, Final Batch Loss: 0.7474081516265869\n",
      "Epoch 756, Loss: 3.6678255796432495, Final Batch Loss: 0.7125208973884583\n",
      "Epoch 757, Loss: 3.9237643480300903, Final Batch Loss: 0.9418328404426575\n",
      "Epoch 758, Loss: 3.7659537196159363, Final Batch Loss: 0.7561621069908142\n",
      "Epoch 759, Loss: 3.7545684576034546, Final Batch Loss: 0.806993305683136\n",
      "Epoch 760, Loss: 3.889600157737732, Final Batch Loss: 0.7330141067504883\n",
      "Epoch 761, Loss: 3.7879889607429504, Final Batch Loss: 0.7454168796539307\n",
      "Epoch 762, Loss: 3.713834524154663, Final Batch Loss: 0.7151558995246887\n",
      "Epoch 763, Loss: 3.906004011631012, Final Batch Loss: 0.7849196791648865\n",
      "Epoch 764, Loss: 3.669371724128723, Final Batch Loss: 0.7514803409576416\n",
      "Epoch 765, Loss: 3.722701072692871, Final Batch Loss: 0.7776743769645691\n",
      "Epoch 766, Loss: 3.8247894644737244, Final Batch Loss: 0.8160728812217712\n",
      "Epoch 767, Loss: 3.654628872871399, Final Batch Loss: 0.7494606971740723\n",
      "Epoch 768, Loss: 3.7284106016159058, Final Batch Loss: 0.6734222173690796\n",
      "Epoch 769, Loss: 3.672845423221588, Final Batch Loss: 0.6493616700172424\n",
      "Epoch 770, Loss: 3.759245812892914, Final Batch Loss: 0.8156556487083435\n",
      "Epoch 771, Loss: 3.8047667145729065, Final Batch Loss: 0.8602145910263062\n",
      "Epoch 772, Loss: 3.722091555595398, Final Batch Loss: 0.7841609120368958\n",
      "Epoch 773, Loss: 3.731311857700348, Final Batch Loss: 0.8341353535652161\n",
      "Epoch 774, Loss: 3.847578227519989, Final Batch Loss: 0.849247932434082\n",
      "Epoch 775, Loss: 3.6351927518844604, Final Batch Loss: 0.6949660778045654\n",
      "Epoch 776, Loss: 3.721261441707611, Final Batch Loss: 0.7856326103210449\n",
      "Epoch 777, Loss: 3.8090633153915405, Final Batch Loss: 0.7672200202941895\n",
      "Epoch 778, Loss: 3.705996096134186, Final Batch Loss: 0.7439782023429871\n",
      "Epoch 779, Loss: 3.810615658760071, Final Batch Loss: 0.7556567788124084\n",
      "Epoch 780, Loss: 3.859040379524231, Final Batch Loss: 0.8749131560325623\n",
      "Epoch 781, Loss: 3.69386225938797, Final Batch Loss: 0.7230937480926514\n",
      "Epoch 782, Loss: 3.8776880502700806, Final Batch Loss: 0.7858942747116089\n",
      "Epoch 783, Loss: 3.7333820462226868, Final Batch Loss: 0.7188000082969666\n",
      "Epoch 784, Loss: 3.741070568561554, Final Batch Loss: 0.7128846049308777\n",
      "Epoch 785, Loss: 3.935487926006317, Final Batch Loss: 0.7500137090682983\n",
      "Epoch 786, Loss: 3.6955482363700867, Final Batch Loss: 0.7480580806732178\n",
      "Epoch 787, Loss: 3.662768602371216, Final Batch Loss: 0.7290390729904175\n",
      "Epoch 788, Loss: 3.8153992891311646, Final Batch Loss: 0.8417460322380066\n",
      "Epoch 789, Loss: 3.843191385269165, Final Batch Loss: 0.8464607000350952\n",
      "Epoch 790, Loss: 3.754821002483368, Final Batch Loss: 0.7430766224861145\n",
      "Epoch 791, Loss: 3.712110936641693, Final Batch Loss: 0.7497390508651733\n",
      "Epoch 792, Loss: 3.726777672767639, Final Batch Loss: 0.7837685942649841\n",
      "Epoch 793, Loss: 3.7648733854293823, Final Batch Loss: 0.7428289651870728\n",
      "Epoch 794, Loss: 3.70629346370697, Final Batch Loss: 0.6424643993377686\n",
      "Epoch 795, Loss: 3.780007481575012, Final Batch Loss: 0.7232206463813782\n",
      "Epoch 796, Loss: 3.792416036128998, Final Batch Loss: 0.7890748977661133\n",
      "Epoch 797, Loss: 3.7254111766815186, Final Batch Loss: 0.8442643880844116\n",
      "Epoch 798, Loss: 3.7471749782562256, Final Batch Loss: 0.7359878420829773\n",
      "Epoch 799, Loss: 3.910072088241577, Final Batch Loss: 0.870176374912262\n",
      "Epoch 800, Loss: 3.7486356496810913, Final Batch Loss: 0.7530290484428406\n",
      "Epoch 801, Loss: 3.671758472919464, Final Batch Loss: 0.6449373960494995\n",
      "Epoch 802, Loss: 3.6463897824287415, Final Batch Loss: 0.8314264416694641\n",
      "Epoch 803, Loss: 3.66353440284729, Final Batch Loss: 0.759501039981842\n",
      "Epoch 804, Loss: 3.7185782194137573, Final Batch Loss: 0.7112696170806885\n",
      "Epoch 805, Loss: 3.9187874794006348, Final Batch Loss: 0.8196876049041748\n",
      "Epoch 806, Loss: 3.6282781958580017, Final Batch Loss: 0.5980523228645325\n",
      "Epoch 807, Loss: 3.762786567211151, Final Batch Loss: 0.7155341506004333\n",
      "Epoch 808, Loss: 3.6417937874794006, Final Batch Loss: 0.7405750751495361\n",
      "Epoch 809, Loss: 3.6605491638183594, Final Batch Loss: 0.6991108655929565\n",
      "Epoch 810, Loss: 3.541389286518097, Final Batch Loss: 0.6310577392578125\n",
      "Epoch 811, Loss: 3.9171048402786255, Final Batch Loss: 0.7748488783836365\n",
      "Epoch 812, Loss: 3.7028398513793945, Final Batch Loss: 0.7674045562744141\n",
      "Epoch 813, Loss: 3.780210494995117, Final Batch Loss: 0.8638036847114563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 814, Loss: 3.758171319961548, Final Batch Loss: 0.8365381956100464\n",
      "Epoch 815, Loss: 3.5584513545036316, Final Batch Loss: 0.7231217622756958\n",
      "Epoch 816, Loss: 3.6485997438430786, Final Batch Loss: 0.6626449823379517\n",
      "Epoch 817, Loss: 3.5754343271255493, Final Batch Loss: 0.6043961048126221\n",
      "Epoch 818, Loss: 3.6799448132514954, Final Batch Loss: 0.6819011569023132\n",
      "Epoch 819, Loss: 3.7741259932518005, Final Batch Loss: 0.7435246109962463\n",
      "Epoch 820, Loss: 3.7692524790763855, Final Batch Loss: 0.7971310615539551\n",
      "Epoch 821, Loss: 3.710904061794281, Final Batch Loss: 0.6618587970733643\n",
      "Epoch 822, Loss: 3.637941539287567, Final Batch Loss: 0.7235987782478333\n",
      "Epoch 823, Loss: 3.7133808732032776, Final Batch Loss: 0.7913306951522827\n",
      "Epoch 824, Loss: 3.7164708971977234, Final Batch Loss: 0.7125712037086487\n",
      "Epoch 825, Loss: 3.6549788117408752, Final Batch Loss: 0.799659013748169\n",
      "Epoch 826, Loss: 3.7522428035736084, Final Batch Loss: 0.8151610493659973\n",
      "Epoch 827, Loss: 3.631836473941803, Final Batch Loss: 0.7085461616516113\n",
      "Epoch 828, Loss: 3.6648237705230713, Final Batch Loss: 0.7234644889831543\n",
      "Epoch 829, Loss: 3.668461263179779, Final Batch Loss: 0.7416646480560303\n",
      "Epoch 830, Loss: 3.6335619688034058, Final Batch Loss: 0.8261080384254456\n",
      "Epoch 831, Loss: 3.5445775985717773, Final Batch Loss: 0.6218621134757996\n",
      "Epoch 832, Loss: 3.5243553519248962, Final Batch Loss: 0.5813478827476501\n",
      "Epoch 833, Loss: 3.6590784192085266, Final Batch Loss: 0.7431227564811707\n",
      "Epoch 834, Loss: 3.56028014421463, Final Batch Loss: 0.7121683955192566\n",
      "Epoch 835, Loss: 3.697329819202423, Final Batch Loss: 0.8573534488677979\n",
      "Epoch 836, Loss: 3.7243478894233704, Final Batch Loss: 0.6584836840629578\n",
      "Epoch 837, Loss: 3.7048949003219604, Final Batch Loss: 0.6817615628242493\n",
      "Epoch 838, Loss: 3.5918927788734436, Final Batch Loss: 0.5846218466758728\n",
      "Epoch 839, Loss: 3.7985783219337463, Final Batch Loss: 0.7737654447555542\n",
      "Epoch 840, Loss: 3.6840277314186096, Final Batch Loss: 0.783703625202179\n",
      "Epoch 841, Loss: 3.488660752773285, Final Batch Loss: 0.7436745166778564\n",
      "Epoch 842, Loss: 3.5624823570251465, Final Batch Loss: 0.6833734512329102\n",
      "Epoch 843, Loss: 3.646421194076538, Final Batch Loss: 0.7141611576080322\n",
      "Epoch 844, Loss: 3.5195708870887756, Final Batch Loss: 0.7265409231185913\n",
      "Epoch 845, Loss: 3.702015459537506, Final Batch Loss: 0.7431976795196533\n",
      "Epoch 846, Loss: 3.647769033908844, Final Batch Loss: 0.7223666310310364\n",
      "Epoch 847, Loss: 3.6484349966049194, Final Batch Loss: 0.6894455552101135\n",
      "Epoch 848, Loss: 3.6649990677833557, Final Batch Loss: 0.768799364566803\n",
      "Epoch 849, Loss: 3.52634459733963, Final Batch Loss: 0.6090554594993591\n",
      "Epoch 850, Loss: 3.661547005176544, Final Batch Loss: 0.7491275668144226\n",
      "Epoch 851, Loss: 3.6066067814826965, Final Batch Loss: 0.7227239012718201\n",
      "Epoch 852, Loss: 3.6371132135391235, Final Batch Loss: 0.7140830755233765\n",
      "Epoch 853, Loss: 3.6911885142326355, Final Batch Loss: 0.7461896538734436\n",
      "Epoch 854, Loss: 3.4859445095062256, Final Batch Loss: 0.6141489148139954\n",
      "Epoch 855, Loss: 3.6307480335235596, Final Batch Loss: 0.6426462531089783\n",
      "Epoch 856, Loss: 3.549269676208496, Final Batch Loss: 0.7190078496932983\n",
      "Epoch 857, Loss: 3.5068423748016357, Final Batch Loss: 0.6878235936164856\n",
      "Epoch 858, Loss: 3.8283533453941345, Final Batch Loss: 0.8222517371177673\n",
      "Epoch 859, Loss: 3.6669493317604065, Final Batch Loss: 0.7441357970237732\n",
      "Epoch 860, Loss: 3.8069733381271362, Final Batch Loss: 0.8275986909866333\n",
      "Epoch 861, Loss: 3.701830506324768, Final Batch Loss: 0.7590818405151367\n",
      "Epoch 862, Loss: 3.5831604599952698, Final Batch Loss: 0.6712878942489624\n",
      "Epoch 863, Loss: 3.803251266479492, Final Batch Loss: 0.8143530488014221\n",
      "Epoch 864, Loss: 3.749791204929352, Final Batch Loss: 0.8041608929634094\n",
      "Epoch 865, Loss: 3.6120707392692566, Final Batch Loss: 0.7214659452438354\n",
      "Epoch 866, Loss: 3.59581983089447, Final Batch Loss: 0.7984433174133301\n",
      "Epoch 867, Loss: 3.7124040722846985, Final Batch Loss: 0.7349622249603271\n",
      "Epoch 868, Loss: 3.5184101462364197, Final Batch Loss: 0.5898951292037964\n",
      "Epoch 869, Loss: 3.6689582467079163, Final Batch Loss: 0.7069607973098755\n",
      "Epoch 870, Loss: 3.6321176290512085, Final Batch Loss: 0.6891876459121704\n",
      "Epoch 871, Loss: 3.546277940273285, Final Batch Loss: 0.7146105766296387\n",
      "Epoch 872, Loss: 3.6616331338882446, Final Batch Loss: 0.7451279759407043\n",
      "Epoch 873, Loss: 3.7757447361946106, Final Batch Loss: 0.8429201245307922\n",
      "Epoch 874, Loss: 3.6616536378860474, Final Batch Loss: 0.8600503206253052\n",
      "Epoch 875, Loss: 3.758195459842682, Final Batch Loss: 0.8011642694473267\n",
      "Epoch 876, Loss: 3.648886263370514, Final Batch Loss: 0.7689963579177856\n",
      "Epoch 877, Loss: 3.6066989302635193, Final Batch Loss: 0.7509458065032959\n",
      "Epoch 878, Loss: 3.57412326335907, Final Batch Loss: 0.7008277177810669\n",
      "Epoch 879, Loss: 3.5092227458953857, Final Batch Loss: 0.6904271245002747\n",
      "Epoch 880, Loss: 3.8250025510787964, Final Batch Loss: 0.7167021632194519\n",
      "Epoch 881, Loss: 3.733021914958954, Final Batch Loss: 0.6411145329475403\n",
      "Epoch 882, Loss: 3.7050910592079163, Final Batch Loss: 0.7351921796798706\n",
      "Epoch 883, Loss: 3.740307867527008, Final Batch Loss: 0.8294544219970703\n",
      "Epoch 884, Loss: 3.647105395793915, Final Batch Loss: 0.6929906606674194\n",
      "Epoch 885, Loss: 3.5811229944229126, Final Batch Loss: 0.7698578834533691\n",
      "Epoch 886, Loss: 3.4317187070846558, Final Batch Loss: 0.5595081448554993\n",
      "Epoch 887, Loss: 3.8754698634147644, Final Batch Loss: 0.8837881088256836\n",
      "Epoch 888, Loss: 3.641042470932007, Final Batch Loss: 0.7327975630760193\n",
      "Epoch 889, Loss: 3.5591713786125183, Final Batch Loss: 0.6642295122146606\n",
      "Epoch 890, Loss: 3.7073540687561035, Final Batch Loss: 0.7616181373596191\n",
      "Epoch 891, Loss: 3.540544331073761, Final Batch Loss: 0.7254267334938049\n",
      "Epoch 892, Loss: 3.515928268432617, Final Batch Loss: 0.7238721251487732\n",
      "Epoch 893, Loss: 3.628645956516266, Final Batch Loss: 0.6205236315727234\n",
      "Epoch 894, Loss: 3.713369846343994, Final Batch Loss: 0.7443453073501587\n",
      "Epoch 895, Loss: 3.5259664058685303, Final Batch Loss: 0.6908361911773682\n",
      "Epoch 896, Loss: 3.7105619311332703, Final Batch Loss: 0.6499022841453552\n",
      "Epoch 897, Loss: 3.6089674830436707, Final Batch Loss: 0.7069666981697083\n",
      "Epoch 898, Loss: 3.7549328804016113, Final Batch Loss: 0.6977736353874207\n",
      "Epoch 899, Loss: 3.6367934346199036, Final Batch Loss: 0.7704538106918335\n",
      "Epoch 900, Loss: 3.625899016857147, Final Batch Loss: 0.746954083442688\n",
      "Epoch 901, Loss: 3.656229317188263, Final Batch Loss: 0.7660610675811768\n",
      "Epoch 902, Loss: 3.7551888823509216, Final Batch Loss: 0.7024930119514465\n",
      "Epoch 903, Loss: 3.7481685876846313, Final Batch Loss: 0.9310867786407471\n",
      "Epoch 904, Loss: 3.66368168592453, Final Batch Loss: 0.7025522589683533\n",
      "Epoch 905, Loss: 3.6802170872688293, Final Batch Loss: 0.6826829314231873\n",
      "Epoch 906, Loss: 3.7177645564079285, Final Batch Loss: 0.7548583745956421\n",
      "Epoch 907, Loss: 3.573954701423645, Final Batch Loss: 0.7182028293609619\n",
      "Epoch 908, Loss: 3.4637500643730164, Final Batch Loss: 0.8079383373260498\n",
      "Epoch 909, Loss: 3.4701149463653564, Final Batch Loss: 0.6973536610603333\n",
      "Epoch 910, Loss: 3.548960030078888, Final Batch Loss: 0.7916427850723267\n",
      "Epoch 911, Loss: 3.4962114095687866, Final Batch Loss: 0.5847153067588806\n",
      "Epoch 912, Loss: 3.656791090965271, Final Batch Loss: 0.7739174962043762\n",
      "Epoch 913, Loss: 3.7715951204299927, Final Batch Loss: 0.7024768590927124\n",
      "Epoch 914, Loss: 3.536585807800293, Final Batch Loss: 0.6932273507118225\n",
      "Epoch 915, Loss: 3.5114628672599792, Final Batch Loss: 0.7013565301895142\n",
      "Epoch 916, Loss: 3.6516422629356384, Final Batch Loss: 0.65220046043396\n",
      "Epoch 917, Loss: 3.6891292929649353, Final Batch Loss: 0.6853047609329224\n",
      "Epoch 918, Loss: 3.6665965914726257, Final Batch Loss: 0.6364096403121948\n",
      "Epoch 919, Loss: 3.611902892589569, Final Batch Loss: 0.6701809763908386\n",
      "Epoch 920, Loss: 3.6277172565460205, Final Batch Loss: 0.6784353852272034\n",
      "Epoch 921, Loss: 3.6623549461364746, Final Batch Loss: 0.6842666268348694\n",
      "Epoch 922, Loss: 3.5749383568763733, Final Batch Loss: 0.7204610109329224\n",
      "Epoch 923, Loss: 3.525071680545807, Final Batch Loss: 0.7561151385307312\n",
      "Epoch 924, Loss: 3.6566017866134644, Final Batch Loss: 0.7685568928718567\n",
      "Epoch 925, Loss: 3.5921834111213684, Final Batch Loss: 0.6653648018836975\n",
      "Epoch 926, Loss: 3.6002004742622375, Final Batch Loss: 0.5730036497116089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 927, Loss: 3.4974916577339172, Final Batch Loss: 0.6142868995666504\n",
      "Epoch 928, Loss: 3.5857313871383667, Final Batch Loss: 0.6069051027297974\n",
      "Epoch 929, Loss: 3.4086515307426453, Final Batch Loss: 0.6836874485015869\n",
      "Epoch 930, Loss: 3.435717463493347, Final Batch Loss: 0.7068893313407898\n",
      "Epoch 931, Loss: 3.5451884865760803, Final Batch Loss: 0.8075304627418518\n",
      "Epoch 932, Loss: 3.516622543334961, Final Batch Loss: 0.6609625816345215\n",
      "Epoch 933, Loss: 3.6686235666275024, Final Batch Loss: 0.7910010814666748\n",
      "Epoch 934, Loss: 3.495519697666168, Final Batch Loss: 0.6503749489784241\n",
      "Epoch 935, Loss: 3.7563081979751587, Final Batch Loss: 0.7211320400238037\n",
      "Epoch 936, Loss: 3.500606954097748, Final Batch Loss: 0.7106555700302124\n",
      "Epoch 937, Loss: 3.4234817028045654, Final Batch Loss: 0.566135585308075\n",
      "Epoch 938, Loss: 3.594626307487488, Final Batch Loss: 0.6471063494682312\n",
      "Epoch 939, Loss: 3.521499752998352, Final Batch Loss: 0.6722265481948853\n",
      "Epoch 940, Loss: 3.6183298230171204, Final Batch Loss: 0.7235962152481079\n",
      "Epoch 941, Loss: 3.641647696495056, Final Batch Loss: 0.8185767531394958\n",
      "Epoch 942, Loss: 3.5566371083259583, Final Batch Loss: 0.7595158815383911\n",
      "Epoch 943, Loss: 3.5218026638031006, Final Batch Loss: 0.6609184741973877\n",
      "Epoch 944, Loss: 3.577241063117981, Final Batch Loss: 0.6837320327758789\n",
      "Epoch 945, Loss: 3.4345643520355225, Final Batch Loss: 0.6908193826675415\n",
      "Epoch 946, Loss: 3.6243860721588135, Final Batch Loss: 0.6539379358291626\n",
      "Epoch 947, Loss: 3.5834600925445557, Final Batch Loss: 0.6348409056663513\n",
      "Epoch 948, Loss: 3.551605761051178, Final Batch Loss: 0.6792999505996704\n",
      "Epoch 949, Loss: 3.798700273036957, Final Batch Loss: 0.7997928261756897\n",
      "Epoch 950, Loss: 3.602684736251831, Final Batch Loss: 0.7383667826652527\n",
      "Epoch 951, Loss: 3.4076516032218933, Final Batch Loss: 0.5383660793304443\n",
      "Epoch 952, Loss: 3.7058388590812683, Final Batch Loss: 0.796987771987915\n",
      "Epoch 953, Loss: 3.4261903166770935, Final Batch Loss: 0.5862598419189453\n",
      "Epoch 954, Loss: 3.6092129945755005, Final Batch Loss: 0.6404763460159302\n",
      "Epoch 955, Loss: 3.6943382620811462, Final Batch Loss: 0.7757383584976196\n",
      "Epoch 956, Loss: 3.5489509105682373, Final Batch Loss: 0.8442005515098572\n",
      "Epoch 957, Loss: 3.472044289112091, Final Batch Loss: 0.70003741979599\n",
      "Epoch 958, Loss: 3.3929566740989685, Final Batch Loss: 0.6873867511749268\n",
      "Epoch 959, Loss: 3.676173448562622, Final Batch Loss: 0.742059051990509\n",
      "Epoch 960, Loss: 3.484745740890503, Final Batch Loss: 0.7231752872467041\n",
      "Epoch 961, Loss: 3.4504311084747314, Final Batch Loss: 0.6257144212722778\n",
      "Epoch 962, Loss: 3.9806670546531677, Final Batch Loss: 0.8637931942939758\n",
      "Epoch 963, Loss: 3.6966172456741333, Final Batch Loss: 0.8249908685684204\n",
      "Epoch 964, Loss: 3.580422341823578, Final Batch Loss: 0.600043773651123\n",
      "Epoch 965, Loss: 3.6716169118881226, Final Batch Loss: 0.7460746765136719\n",
      "Epoch 966, Loss: 3.3204121589660645, Final Batch Loss: 0.5628213286399841\n",
      "Epoch 967, Loss: 3.444908320903778, Final Batch Loss: 0.7349034547805786\n",
      "Epoch 968, Loss: 3.582306385040283, Final Batch Loss: 0.7236966490745544\n",
      "Epoch 969, Loss: 3.4748713970184326, Final Batch Loss: 0.6081370711326599\n",
      "Epoch 970, Loss: 3.49103045463562, Final Batch Loss: 0.6754171848297119\n",
      "Epoch 971, Loss: 3.5995032787323, Final Batch Loss: 0.7046172618865967\n",
      "Epoch 972, Loss: 3.4667133688926697, Final Batch Loss: 0.6801365613937378\n",
      "Epoch 973, Loss: 3.3430652618408203, Final Batch Loss: 0.5817012190818787\n",
      "Epoch 974, Loss: 3.5021442770957947, Final Batch Loss: 0.6559966802597046\n",
      "Epoch 975, Loss: 3.5823205709457397, Final Batch Loss: 0.7053836584091187\n",
      "Epoch 976, Loss: 3.743111729621887, Final Batch Loss: 0.7909449338912964\n",
      "Epoch 977, Loss: 3.472608268260956, Final Batch Loss: 0.7142577767372131\n",
      "Epoch 978, Loss: 3.6514845490455627, Final Batch Loss: 0.6950376033782959\n",
      "Epoch 979, Loss: 3.6392992734909058, Final Batch Loss: 0.7170232534408569\n",
      "Epoch 980, Loss: 3.6369435787200928, Final Batch Loss: 0.7693785429000854\n",
      "Epoch 981, Loss: 3.533892273902893, Final Batch Loss: 0.6349750757217407\n",
      "Epoch 982, Loss: 3.408943474292755, Final Batch Loss: 0.5669713616371155\n",
      "Epoch 983, Loss: 3.488287568092346, Final Batch Loss: 0.6969850659370422\n",
      "Epoch 984, Loss: 3.66753089427948, Final Batch Loss: 0.6716842651367188\n",
      "Epoch 985, Loss: 3.548323690891266, Final Batch Loss: 0.630913496017456\n",
      "Epoch 986, Loss: 3.5685757994651794, Final Batch Loss: 0.6661283373832703\n",
      "Epoch 987, Loss: 3.664943516254425, Final Batch Loss: 0.7078477740287781\n",
      "Epoch 988, Loss: 3.483084559440613, Final Batch Loss: 0.6979499459266663\n",
      "Epoch 989, Loss: 3.7146604657173157, Final Batch Loss: 0.7515828013420105\n",
      "Epoch 990, Loss: 3.5113695859909058, Final Batch Loss: 0.6455966830253601\n",
      "Epoch 991, Loss: 3.513522744178772, Final Batch Loss: 0.6749775409698486\n",
      "Epoch 992, Loss: 3.5783979892730713, Final Batch Loss: 0.7626529335975647\n",
      "Epoch 993, Loss: 3.533016264438629, Final Batch Loss: 0.626335859298706\n",
      "Epoch 994, Loss: 3.4675803184509277, Final Batch Loss: 0.7673692107200623\n",
      "Epoch 995, Loss: 3.481849491596222, Final Batch Loss: 0.7226975560188293\n",
      "Epoch 996, Loss: 3.5169859528541565, Final Batch Loss: 0.6121854782104492\n",
      "Epoch 997, Loss: 3.5199078917503357, Final Batch Loss: 0.6703174114227295\n",
      "Epoch 998, Loss: 3.343404471874237, Final Batch Loss: 0.6766811013221741\n",
      "Epoch 999, Loss: 3.4217679500579834, Final Batch Loss: 0.7006533145904541\n",
      "Epoch 1000, Loss: 3.5163835883140564, Final Batch Loss: 0.7324783802032471\n",
      "Epoch 1001, Loss: 3.3210989236831665, Final Batch Loss: 0.6444167494773865\n",
      "Epoch 1002, Loss: 3.541223108768463, Final Batch Loss: 0.685706377029419\n",
      "Epoch 1003, Loss: 3.4257753491401672, Final Batch Loss: 0.5854860544204712\n",
      "Epoch 1004, Loss: 3.6019649505615234, Final Batch Loss: 0.5854660272598267\n",
      "Epoch 1005, Loss: 3.5034720301628113, Final Batch Loss: 0.7239928245544434\n",
      "Epoch 1006, Loss: 3.4696865677833557, Final Batch Loss: 0.6912232637405396\n",
      "Epoch 1007, Loss: 3.6921140551567078, Final Batch Loss: 0.7328761219978333\n",
      "Epoch 1008, Loss: 3.5276243090629578, Final Batch Loss: 0.7356210350990295\n",
      "Epoch 1009, Loss: 3.547401785850525, Final Batch Loss: 0.7724063992500305\n",
      "Epoch 1010, Loss: 3.668487548828125, Final Batch Loss: 0.7710039615631104\n",
      "Epoch 1011, Loss: 3.4508954882621765, Final Batch Loss: 0.611205518245697\n",
      "Epoch 1012, Loss: 3.563010334968567, Final Batch Loss: 0.7325499653816223\n",
      "Epoch 1013, Loss: 3.4924381971359253, Final Batch Loss: 0.6663274765014648\n",
      "Epoch 1014, Loss: 3.604902744293213, Final Batch Loss: 0.7755722999572754\n",
      "Epoch 1015, Loss: 3.5230360627174377, Final Batch Loss: 0.7901379466056824\n",
      "Epoch 1016, Loss: 3.4458547830581665, Final Batch Loss: 0.6806872487068176\n",
      "Epoch 1017, Loss: 3.4306283593177795, Final Batch Loss: 0.657244086265564\n",
      "Epoch 1018, Loss: 3.3945928812026978, Final Batch Loss: 0.6634262800216675\n",
      "Epoch 1019, Loss: 3.354011058807373, Final Batch Loss: 0.6530686616897583\n",
      "Epoch 1020, Loss: 3.489442467689514, Final Batch Loss: 0.6122507452964783\n",
      "Epoch 1021, Loss: 3.5683385729789734, Final Batch Loss: 0.7804445624351501\n",
      "Epoch 1022, Loss: 3.344703435897827, Final Batch Loss: 0.6351470351219177\n",
      "Epoch 1023, Loss: 3.378286063671112, Final Batch Loss: 0.5438989996910095\n",
      "Epoch 1024, Loss: 3.6029893159866333, Final Batch Loss: 0.8043891787528992\n",
      "Epoch 1025, Loss: 3.5080432295799255, Final Batch Loss: 0.7601229548454285\n",
      "Epoch 1026, Loss: 3.4202669858932495, Final Batch Loss: 0.713538408279419\n",
      "Epoch 1027, Loss: 3.5821300745010376, Final Batch Loss: 0.7074785828590393\n",
      "Epoch 1028, Loss: 3.516483783721924, Final Batch Loss: 0.8148982524871826\n",
      "Epoch 1029, Loss: 3.329342842102051, Final Batch Loss: 0.7213388681411743\n",
      "Epoch 1030, Loss: 3.3875986337661743, Final Batch Loss: 0.7118604183197021\n",
      "Epoch 1031, Loss: 3.3767853379249573, Final Batch Loss: 0.6272239089012146\n",
      "Epoch 1032, Loss: 3.436747908592224, Final Batch Loss: 0.703845202922821\n",
      "Epoch 1033, Loss: 3.3464537858963013, Final Batch Loss: 0.5826538801193237\n",
      "Epoch 1034, Loss: 3.5018309950828552, Final Batch Loss: 0.7499030232429504\n",
      "Epoch 1035, Loss: 3.3722291588783264, Final Batch Loss: 0.7863603830337524\n",
      "Epoch 1036, Loss: 3.446648895740509, Final Batch Loss: 0.6846348643302917\n",
      "Epoch 1037, Loss: 3.56072998046875, Final Batch Loss: 0.7946505546569824\n",
      "Epoch 1038, Loss: 3.4867022037506104, Final Batch Loss: 0.6707684397697449\n",
      "Epoch 1039, Loss: 3.518552005290985, Final Batch Loss: 0.6377729177474976\n",
      "Epoch 1040, Loss: 3.3790807127952576, Final Batch Loss: 0.7312265038490295\n",
      "Epoch 1041, Loss: 3.3553983569145203, Final Batch Loss: 0.5843613147735596\n",
      "Epoch 1042, Loss: 3.491351366043091, Final Batch Loss: 0.8295637965202332\n",
      "Epoch 1043, Loss: 3.3835821747779846, Final Batch Loss: 0.7141022682189941\n",
      "Epoch 1044, Loss: 3.3756282925605774, Final Batch Loss: 0.631025493144989\n",
      "Epoch 1045, Loss: 3.3511515855789185, Final Batch Loss: 0.626790463924408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1046, Loss: 3.40900981426239, Final Batch Loss: 0.7227495908737183\n",
      "Epoch 1047, Loss: 3.6540477871894836, Final Batch Loss: 0.7617113590240479\n",
      "Epoch 1048, Loss: 3.4866949319839478, Final Batch Loss: 0.7307737469673157\n",
      "Epoch 1049, Loss: 3.453210413455963, Final Batch Loss: 0.6079568862915039\n",
      "Epoch 1050, Loss: 3.359808087348938, Final Batch Loss: 0.7132091522216797\n",
      "Epoch 1051, Loss: 3.488295078277588, Final Batch Loss: 0.6990318894386292\n",
      "Epoch 1052, Loss: 3.361124575138092, Final Batch Loss: 0.698045015335083\n",
      "Epoch 1053, Loss: 3.3057010173797607, Final Batch Loss: 0.7361177206039429\n",
      "Epoch 1054, Loss: 3.587404787540436, Final Batch Loss: 0.6280867457389832\n",
      "Epoch 1055, Loss: 3.493666887283325, Final Batch Loss: 0.7201910018920898\n",
      "Epoch 1056, Loss: 3.5731711387634277, Final Batch Loss: 0.7268140912055969\n",
      "Epoch 1057, Loss: 3.536350667476654, Final Batch Loss: 0.8090658783912659\n",
      "Epoch 1058, Loss: 3.532142400741577, Final Batch Loss: 0.7890353202819824\n",
      "Epoch 1059, Loss: 3.675774335861206, Final Batch Loss: 0.7736932039260864\n",
      "Epoch 1060, Loss: 3.4489827156066895, Final Batch Loss: 0.712762713432312\n",
      "Epoch 1061, Loss: 3.495087504386902, Final Batch Loss: 0.7644947171211243\n",
      "Epoch 1062, Loss: 3.5011125206947327, Final Batch Loss: 0.7115485072135925\n",
      "Epoch 1063, Loss: 3.4432936310768127, Final Batch Loss: 0.7177455425262451\n",
      "Epoch 1064, Loss: 3.5421356558799744, Final Batch Loss: 0.7080961465835571\n",
      "Epoch 1065, Loss: 3.4532058238983154, Final Batch Loss: 0.5986036062240601\n",
      "Epoch 1066, Loss: 3.428880751132965, Final Batch Loss: 0.7058127522468567\n",
      "Epoch 1067, Loss: 3.4130257964134216, Final Batch Loss: 0.6839087009429932\n",
      "Epoch 1068, Loss: 3.533683717250824, Final Batch Loss: 0.7185967564582825\n",
      "Epoch 1069, Loss: 3.40306556224823, Final Batch Loss: 0.6848890781402588\n",
      "Epoch 1070, Loss: 3.4121121168136597, Final Batch Loss: 0.6844801902770996\n",
      "Epoch 1071, Loss: 3.4985015392303467, Final Batch Loss: 0.7382352352142334\n",
      "Epoch 1072, Loss: 3.4045944809913635, Final Batch Loss: 0.5985975861549377\n",
      "Epoch 1073, Loss: 3.5101426243782043, Final Batch Loss: 0.6393494606018066\n",
      "Epoch 1074, Loss: 3.4601848125457764, Final Batch Loss: 0.722939670085907\n",
      "Epoch 1075, Loss: 3.6275736689567566, Final Batch Loss: 0.7283722758293152\n",
      "Epoch 1076, Loss: 3.4792981147766113, Final Batch Loss: 0.6627088189125061\n",
      "Epoch 1077, Loss: 3.4990533590316772, Final Batch Loss: 0.7423362135887146\n",
      "Epoch 1078, Loss: 3.474263608455658, Final Batch Loss: 0.81639164686203\n",
      "Epoch 1079, Loss: 3.4243670105934143, Final Batch Loss: 0.7698940634727478\n",
      "Epoch 1080, Loss: 3.5404157042503357, Final Batch Loss: 0.7336845397949219\n",
      "Epoch 1081, Loss: 3.3508495688438416, Final Batch Loss: 0.5566081404685974\n",
      "Epoch 1082, Loss: 3.492191731929779, Final Batch Loss: 0.6985836029052734\n",
      "Epoch 1083, Loss: 3.4842273592948914, Final Batch Loss: 0.6640050411224365\n",
      "Epoch 1084, Loss: 3.5954429507255554, Final Batch Loss: 0.7886056303977966\n",
      "Epoch 1085, Loss: 3.2750978469848633, Final Batch Loss: 0.6079637408256531\n",
      "Epoch 1086, Loss: 3.5603434443473816, Final Batch Loss: 0.842117965221405\n",
      "Epoch 1087, Loss: 3.444846749305725, Final Batch Loss: 0.5232608914375305\n",
      "Epoch 1088, Loss: 3.5242637395858765, Final Batch Loss: 0.6611254215240479\n",
      "Epoch 1089, Loss: 3.4851105213165283, Final Batch Loss: 0.7355483770370483\n",
      "Epoch 1090, Loss: 3.385303318500519, Final Batch Loss: 0.5953894257545471\n",
      "Epoch 1091, Loss: 3.4542747139930725, Final Batch Loss: 0.5958797931671143\n",
      "Epoch 1092, Loss: 3.4002926349639893, Final Batch Loss: 0.6787990927696228\n",
      "Epoch 1093, Loss: 3.3882787823677063, Final Batch Loss: 0.6844404935836792\n",
      "Epoch 1094, Loss: 3.4752484560012817, Final Batch Loss: 0.7052902579307556\n",
      "Epoch 1095, Loss: 3.3728644251823425, Final Batch Loss: 0.7066782116889954\n",
      "Epoch 1096, Loss: 3.5530786514282227, Final Batch Loss: 0.8385295271873474\n",
      "Epoch 1097, Loss: 3.3156380653381348, Final Batch Loss: 0.6728917360305786\n",
      "Epoch 1098, Loss: 3.382519543170929, Final Batch Loss: 0.6610671281814575\n",
      "Epoch 1099, Loss: 3.447548985481262, Final Batch Loss: 0.7672511339187622\n",
      "Epoch 1100, Loss: 3.5103975534439087, Final Batch Loss: 0.7071478366851807\n",
      "Epoch 1101, Loss: 3.473046600818634, Final Batch Loss: 0.7506807446479797\n",
      "Epoch 1102, Loss: 3.396194636821747, Final Batch Loss: 0.7655677795410156\n",
      "Epoch 1103, Loss: 3.3755310773849487, Final Batch Loss: 0.6227082014083862\n",
      "Epoch 1104, Loss: 3.19356769323349, Final Batch Loss: 0.5859361290931702\n",
      "Epoch 1105, Loss: 3.437987446784973, Final Batch Loss: 0.6855337619781494\n",
      "Epoch 1106, Loss: 3.368323504924774, Final Batch Loss: 0.5186507701873779\n",
      "Epoch 1107, Loss: 3.5064969062805176, Final Batch Loss: 0.7183833718299866\n",
      "Epoch 1108, Loss: 3.416456699371338, Final Batch Loss: 0.6671851277351379\n",
      "Epoch 1109, Loss: 3.5445989966392517, Final Batch Loss: 0.6171365976333618\n",
      "Epoch 1110, Loss: 3.5605499148368835, Final Batch Loss: 0.703618049621582\n",
      "Epoch 1111, Loss: 3.3311203122138977, Final Batch Loss: 0.6069531440734863\n",
      "Epoch 1112, Loss: 3.2884570956230164, Final Batch Loss: 0.7190461754798889\n",
      "Epoch 1113, Loss: 3.5197922587394714, Final Batch Loss: 0.7444379329681396\n",
      "Epoch 1114, Loss: 3.4102994799613953, Final Batch Loss: 0.8285444974899292\n",
      "Epoch 1115, Loss: 3.5411237478256226, Final Batch Loss: 0.8524115681648254\n",
      "Epoch 1116, Loss: 3.3650654554367065, Final Batch Loss: 0.5682303309440613\n",
      "Epoch 1117, Loss: 3.441315531730652, Final Batch Loss: 0.7418080568313599\n",
      "Epoch 1118, Loss: 3.432572066783905, Final Batch Loss: 0.6885396838188171\n",
      "Epoch 1119, Loss: 3.489709198474884, Final Batch Loss: 0.7039339542388916\n",
      "Epoch 1120, Loss: 3.4889578819274902, Final Batch Loss: 0.6534953117370605\n",
      "Epoch 1121, Loss: 3.4163716435432434, Final Batch Loss: 0.7363609671592712\n",
      "Epoch 1122, Loss: 3.4072977900505066, Final Batch Loss: 0.6215248107910156\n",
      "Epoch 1123, Loss: 3.2920337319374084, Final Batch Loss: 0.7261881828308105\n",
      "Epoch 1124, Loss: 3.4433417916297913, Final Batch Loss: 0.7321338057518005\n",
      "Epoch 1125, Loss: 3.274453282356262, Final Batch Loss: 0.53661048412323\n",
      "Epoch 1126, Loss: 3.3534334897994995, Final Batch Loss: 0.7312576174736023\n",
      "Epoch 1127, Loss: 3.6131794452667236, Final Batch Loss: 0.751808226108551\n",
      "Epoch 1128, Loss: 3.506132483482361, Final Batch Loss: 0.5903028249740601\n",
      "Epoch 1129, Loss: 3.331509828567505, Final Batch Loss: 0.60652756690979\n",
      "Epoch 1130, Loss: 3.3644325733184814, Final Batch Loss: 0.6895495653152466\n",
      "Epoch 1131, Loss: 3.2947806119918823, Final Batch Loss: 0.6312881708145142\n",
      "Epoch 1132, Loss: 3.3228617906570435, Final Batch Loss: 0.6329350471496582\n",
      "Epoch 1133, Loss: 3.2809072136878967, Final Batch Loss: 0.6369773149490356\n",
      "Epoch 1134, Loss: 3.4761367440223694, Final Batch Loss: 0.6988871097564697\n",
      "Epoch 1135, Loss: 3.431311845779419, Final Batch Loss: 0.7542367577552795\n",
      "Epoch 1136, Loss: 3.289221704006195, Final Batch Loss: 0.6740850806236267\n",
      "Epoch 1137, Loss: 3.3043200373649597, Final Batch Loss: 0.6721989512443542\n",
      "Epoch 1138, Loss: 3.394452691078186, Final Batch Loss: 0.6449695229530334\n",
      "Epoch 1139, Loss: 3.4425318241119385, Final Batch Loss: 0.7832776308059692\n",
      "Epoch 1140, Loss: 3.5032092332839966, Final Batch Loss: 0.7014696002006531\n",
      "Epoch 1141, Loss: 3.3009273409843445, Final Batch Loss: 0.7262948155403137\n",
      "Epoch 1142, Loss: 3.4159829020500183, Final Batch Loss: 0.8542149662971497\n",
      "Epoch 1143, Loss: 3.4410020112991333, Final Batch Loss: 0.7312281131744385\n",
      "Epoch 1144, Loss: 3.361961245536804, Final Batch Loss: 0.7918666005134583\n",
      "Epoch 1145, Loss: 3.4615744948387146, Final Batch Loss: 0.7529405355453491\n",
      "Epoch 1146, Loss: 3.270280420780182, Final Batch Loss: 0.6127892136573792\n",
      "Epoch 1147, Loss: 3.3146923184394836, Final Batch Loss: 0.6310893893241882\n",
      "Epoch 1148, Loss: 3.3279772996902466, Final Batch Loss: 0.6425447463989258\n",
      "Epoch 1149, Loss: 3.309918165206909, Final Batch Loss: 0.6115476489067078\n",
      "Epoch 1150, Loss: 3.2452370524406433, Final Batch Loss: 0.694430410861969\n",
      "Epoch 1151, Loss: 3.5456679463386536, Final Batch Loss: 0.7234947085380554\n",
      "Epoch 1152, Loss: 3.3679373264312744, Final Batch Loss: 0.647282063961029\n",
      "Epoch 1153, Loss: 3.3196361660957336, Final Batch Loss: 0.6999903321266174\n",
      "Epoch 1154, Loss: 3.447386085987091, Final Batch Loss: 0.7624713778495789\n",
      "Epoch 1155, Loss: 3.4225538969039917, Final Batch Loss: 0.641509473323822\n",
      "Epoch 1156, Loss: 3.3567594289779663, Final Batch Loss: 0.6296002268791199\n",
      "Epoch 1157, Loss: 3.524043083190918, Final Batch Loss: 0.752690851688385\n",
      "Epoch 1158, Loss: 3.3232677578926086, Final Batch Loss: 0.6751304268836975\n",
      "Epoch 1159, Loss: 3.3224992156028748, Final Batch Loss: 0.6408687233924866\n",
      "Epoch 1160, Loss: 3.3905645608901978, Final Batch Loss: 0.7409045100212097\n",
      "Epoch 1161, Loss: 3.4583135843276978, Final Batch Loss: 0.6553502678871155\n",
      "Epoch 1162, Loss: 3.497247576713562, Final Batch Loss: 0.7511746883392334\n",
      "Epoch 1163, Loss: 3.2073901295661926, Final Batch Loss: 0.6226909756660461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1164, Loss: 3.4137109518051147, Final Batch Loss: 0.6655620336532593\n",
      "Epoch 1165, Loss: 3.348797380924225, Final Batch Loss: 0.5726565718650818\n",
      "Epoch 1166, Loss: 3.3443541526794434, Final Batch Loss: 0.7367169857025146\n",
      "Epoch 1167, Loss: 3.502347230911255, Final Batch Loss: 0.7758481502532959\n",
      "Epoch 1168, Loss: 3.446150064468384, Final Batch Loss: 0.6601668000221252\n",
      "Epoch 1169, Loss: 3.227548837661743, Final Batch Loss: 0.5914011001586914\n",
      "Epoch 1170, Loss: 3.466465413570404, Final Batch Loss: 0.766745388507843\n",
      "Epoch 1171, Loss: 3.319007158279419, Final Batch Loss: 0.6224918961524963\n",
      "Epoch 1172, Loss: 3.51646488904953, Final Batch Loss: 0.6749419569969177\n",
      "Epoch 1173, Loss: 3.361063241958618, Final Batch Loss: 0.7613810896873474\n",
      "Epoch 1174, Loss: 3.3534483313560486, Final Batch Loss: 0.591069757938385\n",
      "Epoch 1175, Loss: 3.287368595600128, Final Batch Loss: 0.6383403539657593\n",
      "Epoch 1176, Loss: 3.4026054739952087, Final Batch Loss: 0.6942154169082642\n",
      "Epoch 1177, Loss: 3.3714269399642944, Final Batch Loss: 0.6109557151794434\n",
      "Epoch 1178, Loss: 3.2448485493659973, Final Batch Loss: 0.6072098016738892\n",
      "Epoch 1179, Loss: 3.348494589328766, Final Batch Loss: 0.7483655214309692\n",
      "Epoch 1180, Loss: 3.190340757369995, Final Batch Loss: 0.6359649300575256\n",
      "Epoch 1181, Loss: 3.3030444979667664, Final Batch Loss: 0.7598329782485962\n",
      "Epoch 1182, Loss: 3.363487482070923, Final Batch Loss: 0.7295785546302795\n",
      "Epoch 1183, Loss: 3.3648128509521484, Final Batch Loss: 0.6842964887619019\n",
      "Epoch 1184, Loss: 3.327533483505249, Final Batch Loss: 0.7679270505905151\n",
      "Epoch 1185, Loss: 3.329551577568054, Final Batch Loss: 0.6673154234886169\n",
      "Epoch 1186, Loss: 3.440054714679718, Final Batch Loss: 0.6332308650016785\n",
      "Epoch 1187, Loss: 3.2944000959396362, Final Batch Loss: 0.6375014781951904\n",
      "Epoch 1188, Loss: 3.555130660533905, Final Batch Loss: 0.7804720401763916\n",
      "Epoch 1189, Loss: 3.5040435791015625, Final Batch Loss: 0.6741296052932739\n",
      "Epoch 1190, Loss: 3.2651774883270264, Final Batch Loss: 0.7334051132202148\n",
      "Epoch 1191, Loss: 3.3764107823371887, Final Batch Loss: 0.6083282232284546\n",
      "Epoch 1192, Loss: 3.316487431526184, Final Batch Loss: 0.6056804656982422\n",
      "Epoch 1193, Loss: 3.3473769426345825, Final Batch Loss: 0.6689450740814209\n",
      "Epoch 1194, Loss: 3.3984065651893616, Final Batch Loss: 0.8629113435745239\n",
      "Epoch 1195, Loss: 3.4684097170829773, Final Batch Loss: 0.8061425685882568\n",
      "Epoch 1196, Loss: 3.4565446972846985, Final Batch Loss: 0.7132682800292969\n",
      "Epoch 1197, Loss: 3.4129263162612915, Final Batch Loss: 0.6722788214683533\n",
      "Epoch 1198, Loss: 3.3109872341156006, Final Batch Loss: 0.6650018692016602\n",
      "Epoch 1199, Loss: 3.2834794521331787, Final Batch Loss: 0.6540853381156921\n",
      "Epoch 1200, Loss: 3.229185998439789, Final Batch Loss: 0.6732109189033508\n",
      "Epoch 1201, Loss: 3.381143629550934, Final Batch Loss: 0.5255390405654907\n",
      "Epoch 1202, Loss: 3.4018830060958862, Final Batch Loss: 0.7797542214393616\n",
      "Epoch 1203, Loss: 3.5114892721176147, Final Batch Loss: 0.6343379020690918\n",
      "Epoch 1204, Loss: 3.427663803100586, Final Batch Loss: 0.7367847561836243\n",
      "Epoch 1205, Loss: 3.4591140151023865, Final Batch Loss: 0.7058378458023071\n",
      "Epoch 1206, Loss: 3.4358882904052734, Final Batch Loss: 0.7406565546989441\n",
      "Epoch 1207, Loss: 3.297904670238495, Final Batch Loss: 0.597025454044342\n",
      "Epoch 1208, Loss: 3.540670871734619, Final Batch Loss: 0.9403865337371826\n",
      "Epoch 1209, Loss: 3.4485719799995422, Final Batch Loss: 0.7540756464004517\n",
      "Epoch 1210, Loss: 3.45062655210495, Final Batch Loss: 0.662506103515625\n",
      "Epoch 1211, Loss: 3.3322182297706604, Final Batch Loss: 0.6855872273445129\n",
      "Epoch 1212, Loss: 3.461392879486084, Final Batch Loss: 0.6739227175712585\n",
      "Epoch 1213, Loss: 3.122996747493744, Final Batch Loss: 0.5499277114868164\n",
      "Epoch 1214, Loss: 3.351321280002594, Final Batch Loss: 0.6810539960861206\n",
      "Epoch 1215, Loss: 3.3883021473884583, Final Batch Loss: 0.623840868473053\n",
      "Epoch 1216, Loss: 3.4012317657470703, Final Batch Loss: 0.8538596034049988\n",
      "Epoch 1217, Loss: 3.389140486717224, Final Batch Loss: 0.5452574491500854\n",
      "Epoch 1218, Loss: 3.331141173839569, Final Batch Loss: 0.7326467037200928\n",
      "Epoch 1219, Loss: 3.471388518810272, Final Batch Loss: 0.7622314691543579\n",
      "Epoch 1220, Loss: 3.287316381931305, Final Batch Loss: 0.6191298365592957\n",
      "Epoch 1221, Loss: 3.270661473274231, Final Batch Loss: 0.5349382162094116\n",
      "Epoch 1222, Loss: 3.462470531463623, Final Batch Loss: 0.6162294149398804\n",
      "Epoch 1223, Loss: 3.2845677733421326, Final Batch Loss: 0.5543156862258911\n",
      "Epoch 1224, Loss: 3.1869675517082214, Final Batch Loss: 0.5803256034851074\n",
      "Epoch 1225, Loss: 3.4537433981895447, Final Batch Loss: 0.7660313844680786\n",
      "Epoch 1226, Loss: 3.327943801879883, Final Batch Loss: 0.6318045854568481\n",
      "Epoch 1227, Loss: 3.2900906205177307, Final Batch Loss: 0.7094124555587769\n",
      "Epoch 1228, Loss: 3.2931755781173706, Final Batch Loss: 0.7108609080314636\n",
      "Epoch 1229, Loss: 3.163400173187256, Final Batch Loss: 0.7485986948013306\n",
      "Epoch 1230, Loss: 3.3834909200668335, Final Batch Loss: 0.7383673191070557\n",
      "Epoch 1231, Loss: 3.225012421607971, Final Batch Loss: 0.711422860622406\n",
      "Epoch 1232, Loss: 3.3065282702445984, Final Batch Loss: 0.6325744986534119\n",
      "Epoch 1233, Loss: 3.3853790163993835, Final Batch Loss: 0.7762715816497803\n",
      "Epoch 1234, Loss: 3.20273494720459, Final Batch Loss: 0.5798152089118958\n",
      "Epoch 1235, Loss: 3.3441799879074097, Final Batch Loss: 0.6831852793693542\n",
      "Epoch 1236, Loss: 3.4294919967651367, Final Batch Loss: 0.833680272102356\n",
      "Epoch 1237, Loss: 3.2913902401924133, Final Batch Loss: 0.6217303276062012\n",
      "Epoch 1238, Loss: 3.2232112288475037, Final Batch Loss: 0.6637319922447205\n",
      "Epoch 1239, Loss: 3.2684438824653625, Final Batch Loss: 0.5945411920547485\n",
      "Epoch 1240, Loss: 3.2805899381637573, Final Batch Loss: 0.6920347809791565\n",
      "Epoch 1241, Loss: 3.35595166683197, Final Batch Loss: 0.7293739318847656\n",
      "Epoch 1242, Loss: 3.435489773750305, Final Batch Loss: 0.7047210931777954\n",
      "Epoch 1243, Loss: 3.234412431716919, Final Batch Loss: 0.6039085388183594\n",
      "Epoch 1244, Loss: 3.3142224550247192, Final Batch Loss: 0.6571194529533386\n",
      "Epoch 1245, Loss: 3.4010985493659973, Final Batch Loss: 0.7929276823997498\n",
      "Epoch 1246, Loss: 3.303978979587555, Final Batch Loss: 0.712916910648346\n",
      "Epoch 1247, Loss: 3.4336747527122498, Final Batch Loss: 0.6993722319602966\n",
      "Epoch 1248, Loss: 3.345905840396881, Final Batch Loss: 0.7967581748962402\n",
      "Epoch 1249, Loss: 3.2373366951942444, Final Batch Loss: 0.6597270965576172\n",
      "Epoch 1250, Loss: 3.2601400017738342, Final Batch Loss: 0.5634331703186035\n",
      "Epoch 1251, Loss: 3.463905394077301, Final Batch Loss: 0.8315936326980591\n",
      "Epoch 1252, Loss: 3.2018160223960876, Final Batch Loss: 0.6274722814559937\n",
      "Epoch 1253, Loss: 3.2924217581748962, Final Batch Loss: 0.7287527918815613\n",
      "Epoch 1254, Loss: 3.306633412837982, Final Batch Loss: 0.7125586867332458\n",
      "Epoch 1255, Loss: 3.425745904445648, Final Batch Loss: 0.7412569522857666\n",
      "Epoch 1256, Loss: 3.356672942638397, Final Batch Loss: 0.7593808174133301\n",
      "Epoch 1257, Loss: 3.1864717602729797, Final Batch Loss: 0.6259355545043945\n",
      "Epoch 1258, Loss: 3.3888848423957825, Final Batch Loss: 0.6256049275398254\n",
      "Epoch 1259, Loss: 3.2172667384147644, Final Batch Loss: 0.6242042779922485\n",
      "Epoch 1260, Loss: 3.272099792957306, Final Batch Loss: 0.5429332852363586\n",
      "Epoch 1261, Loss: 3.297864556312561, Final Batch Loss: 0.6578700542449951\n",
      "Epoch 1262, Loss: 3.1182130575180054, Final Batch Loss: 0.6217508912086487\n",
      "Epoch 1263, Loss: 3.338061571121216, Final Batch Loss: 0.6639411449432373\n",
      "Epoch 1264, Loss: 3.151492118835449, Final Batch Loss: 0.5934167504310608\n",
      "Epoch 1265, Loss: 3.2965306639671326, Final Batch Loss: 0.615668773651123\n",
      "Epoch 1266, Loss: 3.2975876331329346, Final Batch Loss: 0.6984364986419678\n",
      "Epoch 1267, Loss: 3.2299946546554565, Final Batch Loss: 0.5478455424308777\n",
      "Epoch 1268, Loss: 3.14911550283432, Final Batch Loss: 0.6078100204467773\n",
      "Epoch 1269, Loss: 3.243622601032257, Final Batch Loss: 0.6002005934715271\n",
      "Epoch 1270, Loss: 3.340766668319702, Final Batch Loss: 0.6782932281494141\n",
      "Epoch 1271, Loss: 3.3029972910881042, Final Batch Loss: 0.642503559589386\n",
      "Epoch 1272, Loss: 3.309413254261017, Final Batch Loss: 0.7275905013084412\n",
      "Epoch 1273, Loss: 3.2847657203674316, Final Batch Loss: 0.6389325261116028\n",
      "Epoch 1274, Loss: 3.3198081254959106, Final Batch Loss: 0.6984148621559143\n",
      "Epoch 1275, Loss: 3.367245614528656, Final Batch Loss: 0.7116475105285645\n",
      "Epoch 1276, Loss: 3.24783056974411, Final Batch Loss: 0.609025776386261\n",
      "Epoch 1277, Loss: 3.26758074760437, Final Batch Loss: 0.5750393271446228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1278, Loss: 3.3848907947540283, Final Batch Loss: 0.5925092101097107\n",
      "Epoch 1279, Loss: 3.2908854484558105, Final Batch Loss: 0.6500018239021301\n",
      "Epoch 1280, Loss: 3.3612762093544006, Final Batch Loss: 0.8078389167785645\n",
      "Epoch 1281, Loss: 3.2958725094795227, Final Batch Loss: 0.5568796992301941\n",
      "Epoch 1282, Loss: 3.3069258332252502, Final Batch Loss: 0.5679858922958374\n",
      "Epoch 1283, Loss: 3.233984589576721, Final Batch Loss: 0.5418286919593811\n",
      "Epoch 1284, Loss: 3.293153464794159, Final Batch Loss: 0.6093435883522034\n",
      "Epoch 1285, Loss: 3.282755434513092, Final Batch Loss: 0.6339983940124512\n",
      "Epoch 1286, Loss: 3.16776305437088, Final Batch Loss: 0.6243337988853455\n",
      "Epoch 1287, Loss: 3.2523178458213806, Final Batch Loss: 0.6623254418373108\n",
      "Epoch 1288, Loss: 3.2840651869773865, Final Batch Loss: 0.6930727362632751\n",
      "Epoch 1289, Loss: 3.1477718353271484, Final Batch Loss: 0.6307581067085266\n",
      "Epoch 1290, Loss: 3.2649935483932495, Final Batch Loss: 0.6962946653366089\n",
      "Epoch 1291, Loss: 3.2832921147346497, Final Batch Loss: 0.7105407118797302\n",
      "Epoch 1292, Loss: 3.239207148551941, Final Batch Loss: 0.6257199048995972\n",
      "Epoch 1293, Loss: 3.216788172721863, Final Batch Loss: 0.6314682960510254\n",
      "Epoch 1294, Loss: 3.3161576986312866, Final Batch Loss: 0.8000414371490479\n",
      "Epoch 1295, Loss: 3.222164750099182, Final Batch Loss: 0.6475885510444641\n",
      "Epoch 1296, Loss: 3.29838103055954, Final Batch Loss: 0.5395766496658325\n",
      "Epoch 1297, Loss: 3.259488821029663, Final Batch Loss: 0.6551482081413269\n",
      "Epoch 1298, Loss: 3.3096126914024353, Final Batch Loss: 0.6697693467140198\n",
      "Epoch 1299, Loss: 3.236295998096466, Final Batch Loss: 0.614937961101532\n",
      "Epoch 1300, Loss: 3.078524112701416, Final Batch Loss: 0.5179283022880554\n",
      "Epoch 1301, Loss: 3.3069571256637573, Final Batch Loss: 0.736501157283783\n",
      "Epoch 1302, Loss: 3.269466459751129, Final Batch Loss: 0.6042183041572571\n",
      "Epoch 1303, Loss: 3.247577488422394, Final Batch Loss: 0.7595555186271667\n",
      "Epoch 1304, Loss: 3.145798981189728, Final Batch Loss: 0.5866329669952393\n",
      "Epoch 1305, Loss: 3.149432599544525, Final Batch Loss: 0.5384387373924255\n",
      "Epoch 1306, Loss: 3.1865904331207275, Final Batch Loss: 0.614447832107544\n",
      "Epoch 1307, Loss: 3.3587388396263123, Final Batch Loss: 0.595924973487854\n",
      "Epoch 1308, Loss: 3.1584412455558777, Final Batch Loss: 0.6014970541000366\n",
      "Epoch 1309, Loss: 3.3489211797714233, Final Batch Loss: 0.7923630475997925\n",
      "Epoch 1310, Loss: 3.152320981025696, Final Batch Loss: 0.5351732969284058\n",
      "Epoch 1311, Loss: 3.1660630106925964, Final Batch Loss: 0.6130372881889343\n",
      "Epoch 1312, Loss: 3.2454516887664795, Final Batch Loss: 0.6145899891853333\n",
      "Epoch 1313, Loss: 3.4917118549346924, Final Batch Loss: 0.7547335624694824\n",
      "Epoch 1314, Loss: 3.300543963909149, Final Batch Loss: 0.7006093859672546\n",
      "Epoch 1315, Loss: 3.3268336057662964, Final Batch Loss: 0.6652352809906006\n",
      "Epoch 1316, Loss: 3.186255395412445, Final Batch Loss: 0.6637822389602661\n",
      "Epoch 1317, Loss: 3.184372365474701, Final Batch Loss: 0.6414835453033447\n",
      "Epoch 1318, Loss: 3.2612738609313965, Final Batch Loss: 0.5733631253242493\n",
      "Epoch 1319, Loss: 3.1894917488098145, Final Batch Loss: 0.6008729338645935\n",
      "Epoch 1320, Loss: 3.3207427859306335, Final Batch Loss: 0.6770668625831604\n",
      "Epoch 1321, Loss: 3.187382996082306, Final Batch Loss: 0.6734602451324463\n",
      "Epoch 1322, Loss: 3.2324494123458862, Final Batch Loss: 0.5540146827697754\n",
      "Epoch 1323, Loss: 3.2995606660842896, Final Batch Loss: 0.6885325908660889\n",
      "Epoch 1324, Loss: 3.12873774766922, Final Batch Loss: 0.6845496892929077\n",
      "Epoch 1325, Loss: 3.3064911365509033, Final Batch Loss: 0.631210207939148\n",
      "Epoch 1326, Loss: 3.4261611104011536, Final Batch Loss: 0.6865993142127991\n",
      "Epoch 1327, Loss: 3.344932496547699, Final Batch Loss: 0.6645063161849976\n",
      "Epoch 1328, Loss: 3.2032048106193542, Final Batch Loss: 0.6512190103530884\n",
      "Epoch 1329, Loss: 3.237362563610077, Final Batch Loss: 0.5876772403717041\n",
      "Epoch 1330, Loss: 3.3646854162216187, Final Batch Loss: 0.6172379851341248\n",
      "Epoch 1331, Loss: 3.399267077445984, Final Batch Loss: 0.7651546597480774\n",
      "Epoch 1332, Loss: 3.318925619125366, Final Batch Loss: 0.7170834541320801\n",
      "Epoch 1333, Loss: 3.244882822036743, Final Batch Loss: 0.6543967127799988\n",
      "Epoch 1334, Loss: 3.221533954143524, Final Batch Loss: 0.5901015996932983\n",
      "Epoch 1335, Loss: 3.3207303881645203, Final Batch Loss: 0.7179924249649048\n",
      "Epoch 1336, Loss: 3.381572902202606, Final Batch Loss: 0.7743036150932312\n",
      "Epoch 1337, Loss: 3.304457902908325, Final Batch Loss: 0.7062922716140747\n",
      "Epoch 1338, Loss: 3.2518376111984253, Final Batch Loss: 0.7186142206192017\n",
      "Epoch 1339, Loss: 3.33687824010849, Final Batch Loss: 0.6465594172477722\n",
      "Epoch 1340, Loss: 3.2750014662742615, Final Batch Loss: 0.7107232213020325\n",
      "Epoch 1341, Loss: 3.0943756699562073, Final Batch Loss: 0.6446042656898499\n",
      "Epoch 1342, Loss: 3.1936267614364624, Final Batch Loss: 0.7262118458747864\n",
      "Epoch 1343, Loss: 3.3072760701179504, Final Batch Loss: 0.7209973931312561\n",
      "Epoch 1344, Loss: 3.187862992286682, Final Batch Loss: 0.6182113289833069\n",
      "Epoch 1345, Loss: 3.280696928501129, Final Batch Loss: 0.6973536014556885\n",
      "Epoch 1346, Loss: 3.1842615604400635, Final Batch Loss: 0.637608528137207\n",
      "Epoch 1347, Loss: 3.248279094696045, Final Batch Loss: 0.5328854322433472\n",
      "Epoch 1348, Loss: 3.2167834639549255, Final Batch Loss: 0.6124623417854309\n",
      "Epoch 1349, Loss: 3.54545396566391, Final Batch Loss: 0.7230932116508484\n",
      "Epoch 1350, Loss: 3.238388478755951, Final Batch Loss: 0.5741326212882996\n",
      "Epoch 1351, Loss: 3.2333123683929443, Final Batch Loss: 0.6772937178611755\n",
      "Epoch 1352, Loss: 3.269031524658203, Final Batch Loss: 0.6756424903869629\n",
      "Epoch 1353, Loss: 3.275884509086609, Final Batch Loss: 0.6536393165588379\n",
      "Epoch 1354, Loss: 3.2228501439094543, Final Batch Loss: 0.622927188873291\n",
      "Epoch 1355, Loss: 3.2135928869247437, Final Batch Loss: 0.6805742383003235\n",
      "Epoch 1356, Loss: 3.0247174501419067, Final Batch Loss: 0.6099243760108948\n",
      "Epoch 1357, Loss: 3.1302765011787415, Final Batch Loss: 0.72757488489151\n",
      "Epoch 1358, Loss: 3.3717580437660217, Final Batch Loss: 0.6677301526069641\n",
      "Epoch 1359, Loss: 3.1935121417045593, Final Batch Loss: 0.5913891792297363\n",
      "Epoch 1360, Loss: 3.230156123638153, Final Batch Loss: 0.6150894165039062\n",
      "Epoch 1361, Loss: 3.3336673378944397, Final Batch Loss: 0.6104341745376587\n",
      "Epoch 1362, Loss: 3.2982274889945984, Final Batch Loss: 0.6429044604301453\n",
      "Epoch 1363, Loss: 3.196295440196991, Final Batch Loss: 0.634922981262207\n",
      "Epoch 1364, Loss: 3.2764634490013123, Final Batch Loss: 0.5867588520050049\n",
      "Epoch 1365, Loss: 3.0579949617385864, Final Batch Loss: 0.6098946928977966\n",
      "Epoch 1366, Loss: 3.256310999393463, Final Batch Loss: 0.6592243313789368\n",
      "Epoch 1367, Loss: 3.1564785838127136, Final Batch Loss: 0.7178390622138977\n",
      "Epoch 1368, Loss: 3.1820719242095947, Final Batch Loss: 0.5378774404525757\n",
      "Epoch 1369, Loss: 3.2152823209762573, Final Batch Loss: 0.7155185341835022\n",
      "Epoch 1370, Loss: 3.2808927297592163, Final Batch Loss: 0.47800397872924805\n",
      "Epoch 1371, Loss: 3.449967622756958, Final Batch Loss: 0.7290976643562317\n",
      "Epoch 1372, Loss: 3.272362172603607, Final Batch Loss: 0.6730184555053711\n",
      "Epoch 1373, Loss: 3.1944199800491333, Final Batch Loss: 0.5891684889793396\n",
      "Epoch 1374, Loss: 3.377298951148987, Final Batch Loss: 0.7077249884605408\n",
      "Epoch 1375, Loss: 3.108503758907318, Final Batch Loss: 0.5828090310096741\n",
      "Epoch 1376, Loss: 3.118587911128998, Final Batch Loss: 0.528465747833252\n",
      "Epoch 1377, Loss: 3.099424719810486, Final Batch Loss: 0.6272845268249512\n",
      "Epoch 1378, Loss: 3.3314276933670044, Final Batch Loss: 0.8396065831184387\n",
      "Epoch 1379, Loss: 3.2772001028060913, Final Batch Loss: 0.704280436038971\n",
      "Epoch 1380, Loss: 3.2904502153396606, Final Batch Loss: 0.7804887294769287\n",
      "Epoch 1381, Loss: 3.1262482404708862, Final Batch Loss: 0.5680264830589294\n",
      "Epoch 1382, Loss: 3.3704386353492737, Final Batch Loss: 0.6888168454170227\n",
      "Epoch 1383, Loss: 3.1936588287353516, Final Batch Loss: 0.5444052815437317\n",
      "Epoch 1384, Loss: 3.098025679588318, Final Batch Loss: 0.6215817332267761\n",
      "Epoch 1385, Loss: 3.2322471737861633, Final Batch Loss: 0.641005277633667\n",
      "Epoch 1386, Loss: 3.152932047843933, Final Batch Loss: 0.5835056900978088\n",
      "Epoch 1387, Loss: 3.2842124700546265, Final Batch Loss: 0.6223019957542419\n",
      "Epoch 1388, Loss: 3.0736993551254272, Final Batch Loss: 0.5305657982826233\n",
      "Epoch 1389, Loss: 3.1860711574554443, Final Batch Loss: 0.8231157064437866\n",
      "Epoch 1390, Loss: 3.4002548456192017, Final Batch Loss: 0.7403377294540405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1391, Loss: 3.2267817854881287, Final Batch Loss: 0.7032110691070557\n",
      "Epoch 1392, Loss: 3.2049103379249573, Final Batch Loss: 0.5965698957443237\n",
      "Epoch 1393, Loss: 3.0607064962387085, Final Batch Loss: 0.5968557596206665\n",
      "Epoch 1394, Loss: 3.1010594069957733, Final Batch Loss: 0.6267781853675842\n",
      "Epoch 1395, Loss: 3.218595802783966, Final Batch Loss: 0.8733153939247131\n",
      "Epoch 1396, Loss: 3.198228359222412, Final Batch Loss: 0.5743129849433899\n",
      "Epoch 1397, Loss: 3.202623963356018, Final Batch Loss: 0.6796109676361084\n",
      "Epoch 1398, Loss: 3.2604745030403137, Final Batch Loss: 0.6583119630813599\n",
      "Epoch 1399, Loss: 3.19763320684433, Final Batch Loss: 0.6797013878822327\n",
      "Epoch 1400, Loss: 3.0242452025413513, Final Batch Loss: 0.5687139630317688\n",
      "Epoch 1401, Loss: 3.203445017337799, Final Batch Loss: 0.5532857179641724\n",
      "Epoch 1402, Loss: 3.233214318752289, Final Batch Loss: 0.5686623454093933\n",
      "Epoch 1403, Loss: 3.1821905970573425, Final Batch Loss: 0.6803148984909058\n",
      "Epoch 1404, Loss: 3.135494291782379, Final Batch Loss: 0.7167364954948425\n",
      "Epoch 1405, Loss: 3.1536921858787537, Final Batch Loss: 0.6222437620162964\n",
      "Epoch 1406, Loss: 3.3527490496635437, Final Batch Loss: 0.8388481140136719\n",
      "Epoch 1407, Loss: 3.0494565963745117, Final Batch Loss: 0.5768478512763977\n",
      "Epoch 1408, Loss: 3.1439931392669678, Final Batch Loss: 0.6101976633071899\n",
      "Epoch 1409, Loss: 3.2079339027404785, Final Batch Loss: 0.5615766644477844\n",
      "Epoch 1410, Loss: 3.2268131971359253, Final Batch Loss: 0.719784677028656\n",
      "Epoch 1411, Loss: 3.30636727809906, Final Batch Loss: 0.7041099667549133\n",
      "Epoch 1412, Loss: 3.3909536600112915, Final Batch Loss: 0.7657374739646912\n",
      "Epoch 1413, Loss: 3.0724642872810364, Final Batch Loss: 0.550754725933075\n",
      "Epoch 1414, Loss: 3.2703998684883118, Final Batch Loss: 0.7069613933563232\n",
      "Epoch 1415, Loss: 3.1899032592773438, Final Batch Loss: 0.6696211695671082\n",
      "Epoch 1416, Loss: 3.209489345550537, Final Batch Loss: 0.5282958745956421\n",
      "Epoch 1417, Loss: 3.0945873856544495, Final Batch Loss: 0.6146799325942993\n",
      "Epoch 1418, Loss: 3.4781322479248047, Final Batch Loss: 0.6615176200866699\n",
      "Epoch 1419, Loss: 3.3473159670829773, Final Batch Loss: 0.7203229069709778\n",
      "Epoch 1420, Loss: 3.295885145664215, Final Batch Loss: 0.5821875333786011\n",
      "Epoch 1421, Loss: 3.1439064145088196, Final Batch Loss: 0.6052178144454956\n",
      "Epoch 1422, Loss: 3.0317912697792053, Final Batch Loss: 0.5379204750061035\n",
      "Epoch 1423, Loss: 3.267997920513153, Final Batch Loss: 0.6636777520179749\n",
      "Epoch 1424, Loss: 3.2379581332206726, Final Batch Loss: 0.7263280153274536\n",
      "Epoch 1425, Loss: 3.1343746185302734, Final Batch Loss: 0.6525880098342896\n",
      "Epoch 1426, Loss: 3.198117256164551, Final Batch Loss: 0.7068959474563599\n",
      "Epoch 1427, Loss: 3.184803545475006, Final Batch Loss: 0.7163494825363159\n",
      "Epoch 1428, Loss: 3.1741647720336914, Final Batch Loss: 0.6749526858329773\n",
      "Epoch 1429, Loss: 3.2210373878479004, Final Batch Loss: 0.6681464910507202\n",
      "Epoch 1430, Loss: 3.138839066028595, Final Batch Loss: 0.7320840954780579\n",
      "Epoch 1431, Loss: 3.132099688053131, Final Batch Loss: 0.5960761904716492\n",
      "Epoch 1432, Loss: 3.1690946221351624, Final Batch Loss: 0.6004281044006348\n",
      "Epoch 1433, Loss: 3.486023426055908, Final Batch Loss: 0.736280083656311\n",
      "Epoch 1434, Loss: 3.2713706493377686, Final Batch Loss: 0.663862943649292\n",
      "Epoch 1435, Loss: 3.393416702747345, Final Batch Loss: 0.7117656469345093\n",
      "Epoch 1436, Loss: 3.0934834480285645, Final Batch Loss: 0.5755158066749573\n",
      "Epoch 1437, Loss: 3.089114725589752, Final Batch Loss: 0.5333036780357361\n",
      "Epoch 1438, Loss: 3.153958320617676, Final Batch Loss: 0.6957791447639465\n",
      "Epoch 1439, Loss: 3.0646724104881287, Final Batch Loss: 0.7041389346122742\n",
      "Epoch 1440, Loss: 3.139262855052948, Final Batch Loss: 0.6056157350540161\n",
      "Epoch 1441, Loss: 3.1941354870796204, Final Batch Loss: 0.6720709800720215\n",
      "Epoch 1442, Loss: 3.116007387638092, Final Batch Loss: 0.6105621457099915\n",
      "Epoch 1443, Loss: 2.9385370910167694, Final Batch Loss: 0.47714492678642273\n",
      "Epoch 1444, Loss: 3.1835415363311768, Final Batch Loss: 0.6104873418807983\n",
      "Epoch 1445, Loss: 3.0565627813339233, Final Batch Loss: 0.7234700918197632\n",
      "Epoch 1446, Loss: 3.136873781681061, Final Batch Loss: 0.6944504976272583\n",
      "Epoch 1447, Loss: 3.4317187666893005, Final Batch Loss: 0.7409200072288513\n",
      "Epoch 1448, Loss: 3.127498686313629, Final Batch Loss: 0.5455260276794434\n",
      "Epoch 1449, Loss: 3.202247738838196, Final Batch Loss: 0.6667999029159546\n",
      "Epoch 1450, Loss: 3.2904101610183716, Final Batch Loss: 0.5786500573158264\n",
      "Epoch 1451, Loss: 2.987668812274933, Final Batch Loss: 0.5832031965255737\n",
      "Epoch 1452, Loss: 3.1063641905784607, Final Batch Loss: 0.5656856894493103\n",
      "Epoch 1453, Loss: 3.2331334352493286, Final Batch Loss: 0.6810411810874939\n",
      "Epoch 1454, Loss: 3.0657426714897156, Final Batch Loss: 0.5788118839263916\n",
      "Epoch 1455, Loss: 3.11042058467865, Final Batch Loss: 0.5848597288131714\n",
      "Epoch 1456, Loss: 3.1832644939422607, Final Batch Loss: 0.6492869853973389\n",
      "Epoch 1457, Loss: 3.0925859212875366, Final Batch Loss: 0.6059173345565796\n",
      "Epoch 1458, Loss: 3.0342090129852295, Final Batch Loss: 0.5979908108711243\n",
      "Epoch 1459, Loss: 3.133510112762451, Final Batch Loss: 0.5760887265205383\n",
      "Epoch 1460, Loss: 3.101236045360565, Final Batch Loss: 0.7043281197547913\n",
      "Epoch 1461, Loss: 3.0829471349716187, Final Batch Loss: 0.630275309085846\n",
      "Epoch 1462, Loss: 3.1550586223602295, Final Batch Loss: 0.574747622013092\n",
      "Epoch 1463, Loss: 3.0755913853645325, Final Batch Loss: 0.5083871483802795\n",
      "Epoch 1464, Loss: 3.172864615917206, Final Batch Loss: 0.6120102405548096\n",
      "Epoch 1465, Loss: 3.2209553718566895, Final Batch Loss: 0.7201818823814392\n",
      "Epoch 1466, Loss: 3.12039577960968, Final Batch Loss: 0.5837216377258301\n",
      "Epoch 1467, Loss: 3.0263895392417908, Final Batch Loss: 0.5448522567749023\n",
      "Epoch 1468, Loss: 3.38178813457489, Final Batch Loss: 0.6814950108528137\n",
      "Epoch 1469, Loss: 3.053339183330536, Final Batch Loss: 0.6297438144683838\n",
      "Epoch 1470, Loss: 3.063551425933838, Final Batch Loss: 0.6349616050720215\n",
      "Epoch 1471, Loss: 3.043515622615814, Final Batch Loss: 0.6197909116744995\n",
      "Epoch 1472, Loss: 3.069157302379608, Final Batch Loss: 0.6989063620567322\n",
      "Epoch 1473, Loss: 3.5398483276367188, Final Batch Loss: 0.7917974591255188\n",
      "Epoch 1474, Loss: 3.1549357771873474, Final Batch Loss: 0.6200029850006104\n",
      "Epoch 1475, Loss: 3.055706202983856, Final Batch Loss: 0.571858823299408\n",
      "Epoch 1476, Loss: 3.096083343029022, Final Batch Loss: 0.5336838364601135\n",
      "Epoch 1477, Loss: 3.057147800922394, Final Batch Loss: 0.5769795775413513\n",
      "Epoch 1478, Loss: 3.0879634618759155, Final Batch Loss: 0.6106250286102295\n",
      "Epoch 1479, Loss: 3.1354735493659973, Final Batch Loss: 0.5352488160133362\n",
      "Epoch 1480, Loss: 3.141976058483124, Final Batch Loss: 0.6799436211585999\n",
      "Epoch 1481, Loss: 3.1290011405944824, Final Batch Loss: 0.6294182538986206\n",
      "Epoch 1482, Loss: 3.1572070121765137, Final Batch Loss: 0.6222083568572998\n",
      "Epoch 1483, Loss: 2.994360327720642, Final Batch Loss: 0.5153381824493408\n",
      "Epoch 1484, Loss: 3.219249725341797, Final Batch Loss: 0.6282254457473755\n",
      "Epoch 1485, Loss: 3.0324689745903015, Final Batch Loss: 0.6637409329414368\n",
      "Epoch 1486, Loss: 3.081808030605316, Final Batch Loss: 0.5724872946739197\n",
      "Epoch 1487, Loss: 3.2805155515670776, Final Batch Loss: 0.608821451663971\n",
      "Epoch 1488, Loss: 3.2857800722122192, Final Batch Loss: 0.7294291257858276\n",
      "Epoch 1489, Loss: 3.1734235286712646, Final Batch Loss: 0.753983199596405\n",
      "Epoch 1490, Loss: 3.3471952080726624, Final Batch Loss: 0.7380117774009705\n",
      "Epoch 1491, Loss: 3.1843279004096985, Final Batch Loss: 0.67637038230896\n",
      "Epoch 1492, Loss: 3.118908703327179, Final Batch Loss: 0.6510185599327087\n",
      "Epoch 1493, Loss: 3.1015342473983765, Final Batch Loss: 0.6232436299324036\n",
      "Epoch 1494, Loss: 3.268505036830902, Final Batch Loss: 0.6043211817741394\n",
      "Epoch 1495, Loss: 3.165856719017029, Final Batch Loss: 0.6192237138748169\n",
      "Epoch 1496, Loss: 3.218567371368408, Final Batch Loss: 0.47282731533050537\n",
      "Epoch 1497, Loss: 3.29070645570755, Final Batch Loss: 0.6442976593971252\n",
      "Epoch 1498, Loss: 3.0766361355781555, Final Batch Loss: 0.6738274097442627\n",
      "Epoch 1499, Loss: 3.1209126710891724, Final Batch Loss: 0.6468510031700134\n",
      "Epoch 1500, Loss: 3.174611270427704, Final Batch Loss: 0.6365805864334106\n",
      "Epoch 1501, Loss: 3.1392500400543213, Final Batch Loss: 0.630479097366333\n",
      "Epoch 1502, Loss: 3.1644471883773804, Final Batch Loss: 0.6798644661903381\n",
      "Epoch 1503, Loss: 3.1782453656196594, Final Batch Loss: 0.6161458492279053\n",
      "Epoch 1504, Loss: 3.305531084537506, Final Batch Loss: 0.6635004281997681\n",
      "Epoch 1505, Loss: 3.0624247193336487, Final Batch Loss: 0.6358169317245483\n",
      "Epoch 1506, Loss: 3.0943719148635864, Final Batch Loss: 0.5130872130393982\n",
      "Epoch 1507, Loss: 3.0460137724876404, Final Batch Loss: 0.6023553609848022\n",
      "Epoch 1508, Loss: 3.151440143585205, Final Batch Loss: 0.551800012588501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1509, Loss: 3.065022647380829, Final Batch Loss: 0.5897217988967896\n",
      "Epoch 1510, Loss: 3.046829342842102, Final Batch Loss: 0.5630492568016052\n",
      "Epoch 1511, Loss: 3.206449329853058, Final Batch Loss: 0.6752964854240417\n",
      "Epoch 1512, Loss: 3.1324819326400757, Final Batch Loss: 0.6986943483352661\n",
      "Epoch 1513, Loss: 2.9964510798454285, Final Batch Loss: 0.6222438216209412\n",
      "Epoch 1514, Loss: 3.199337661266327, Final Batch Loss: 0.5436691045761108\n",
      "Epoch 1515, Loss: 3.1879236102104187, Final Batch Loss: 0.5989207029342651\n",
      "Epoch 1516, Loss: 3.135485291481018, Final Batch Loss: 0.6129891872406006\n",
      "Epoch 1517, Loss: 2.988095462322235, Final Batch Loss: 0.6131871342658997\n",
      "Epoch 1518, Loss: 3.024749279022217, Final Batch Loss: 0.5244089365005493\n",
      "Epoch 1519, Loss: 3.0140414237976074, Final Batch Loss: 0.6175068020820618\n",
      "Epoch 1520, Loss: 3.0836739540100098, Final Batch Loss: 0.5905102491378784\n",
      "Epoch 1521, Loss: 2.9884256720542908, Final Batch Loss: 0.6497294306755066\n",
      "Epoch 1522, Loss: 3.1562549471855164, Final Batch Loss: 0.6705577373504639\n",
      "Epoch 1523, Loss: 3.015512377023697, Final Batch Loss: 0.4738081395626068\n",
      "Epoch 1524, Loss: 3.1521189212799072, Final Batch Loss: 0.6368551850318909\n",
      "Epoch 1525, Loss: 3.427152156829834, Final Batch Loss: 0.6900596618652344\n",
      "Epoch 1526, Loss: 3.129642963409424, Final Batch Loss: 0.6259838938713074\n",
      "Epoch 1527, Loss: 3.122305452823639, Final Batch Loss: 0.5713854432106018\n",
      "Epoch 1528, Loss: 3.009575068950653, Final Batch Loss: 0.5704618692398071\n",
      "Epoch 1529, Loss: 3.1322861909866333, Final Batch Loss: 0.5793379545211792\n",
      "Epoch 1530, Loss: 3.245353579521179, Final Batch Loss: 0.8322185277938843\n",
      "Epoch 1531, Loss: 3.0517020225524902, Final Batch Loss: 0.6356449127197266\n",
      "Epoch 1532, Loss: 3.13324373960495, Final Batch Loss: 0.6652987003326416\n",
      "Epoch 1533, Loss: 3.137711763381958, Final Batch Loss: 0.6793764233589172\n",
      "Epoch 1534, Loss: 3.0294623970985413, Final Batch Loss: 0.5721226930618286\n",
      "Epoch 1535, Loss: 3.171445906162262, Final Batch Loss: 0.6583357453346252\n",
      "Epoch 1536, Loss: 3.0925742983818054, Final Batch Loss: 0.6225485801696777\n",
      "Epoch 1537, Loss: 3.0272669792175293, Final Batch Loss: 0.6189607977867126\n",
      "Epoch 1538, Loss: 3.1426982283592224, Final Batch Loss: 0.5814009308815002\n",
      "Epoch 1539, Loss: 3.249600350856781, Final Batch Loss: 0.64455246925354\n",
      "Epoch 1540, Loss: 2.9481581449508667, Final Batch Loss: 0.6156492233276367\n",
      "Epoch 1541, Loss: 3.038593590259552, Final Batch Loss: 0.4151459336280823\n",
      "Epoch 1542, Loss: 3.188577950000763, Final Batch Loss: 0.5579054951667786\n",
      "Epoch 1543, Loss: 2.979553759098053, Final Batch Loss: 0.7208204865455627\n",
      "Epoch 1544, Loss: 3.065330445766449, Final Batch Loss: 0.6754441261291504\n",
      "Epoch 1545, Loss: 3.1062708497047424, Final Batch Loss: 0.6162170767784119\n",
      "Epoch 1546, Loss: 3.0533844232559204, Final Batch Loss: 0.5540094971656799\n",
      "Epoch 1547, Loss: 3.172627866268158, Final Batch Loss: 0.7293225526809692\n",
      "Epoch 1548, Loss: 3.051983594894409, Final Batch Loss: 0.5782786011695862\n",
      "Epoch 1549, Loss: 3.028643012046814, Final Batch Loss: 0.5861912369728088\n",
      "Epoch 1550, Loss: 3.071182608604431, Final Batch Loss: 0.6156829595565796\n",
      "Epoch 1551, Loss: 3.10266375541687, Final Batch Loss: 0.6723254919052124\n",
      "Epoch 1552, Loss: 3.015877842903137, Final Batch Loss: 0.5973750352859497\n",
      "Epoch 1553, Loss: 3.0516528487205505, Final Batch Loss: 0.6027590036392212\n",
      "Epoch 1554, Loss: 3.2817203998565674, Final Batch Loss: 0.7089919447898865\n",
      "Epoch 1555, Loss: 3.0968767404556274, Final Batch Loss: 0.6245734095573425\n",
      "Epoch 1556, Loss: 2.9758923649787903, Final Batch Loss: 0.6879081726074219\n",
      "Epoch 1557, Loss: 3.022379219532013, Final Batch Loss: 0.6769862771034241\n",
      "Epoch 1558, Loss: 3.130662679672241, Final Batch Loss: 0.6469511985778809\n",
      "Epoch 1559, Loss: 3.1659631729125977, Final Batch Loss: 0.5876973867416382\n",
      "Epoch 1560, Loss: 3.057481288909912, Final Batch Loss: 0.5767496824264526\n",
      "Epoch 1561, Loss: 3.1556713581085205, Final Batch Loss: 0.6345916390419006\n",
      "Epoch 1562, Loss: 2.973462402820587, Final Batch Loss: 0.5807264447212219\n",
      "Epoch 1563, Loss: 3.0202443301677704, Final Batch Loss: 0.5433513522148132\n",
      "Epoch 1564, Loss: 3.1848572492599487, Final Batch Loss: 0.6197548508644104\n",
      "Epoch 1565, Loss: 3.1499277353286743, Final Batch Loss: 0.6280914545059204\n",
      "Epoch 1566, Loss: 3.1363345980644226, Final Batch Loss: 0.5870417356491089\n",
      "Epoch 1567, Loss: 3.0963249802589417, Final Batch Loss: 0.5452014803886414\n",
      "Epoch 1568, Loss: 3.2425721287727356, Final Batch Loss: 0.5778005719184875\n",
      "Epoch 1569, Loss: 2.9141271710395813, Final Batch Loss: 0.5170012712478638\n",
      "Epoch 1570, Loss: 3.141873836517334, Final Batch Loss: 0.6053117513656616\n",
      "Epoch 1571, Loss: 3.2246217727661133, Final Batch Loss: 0.5976311564445496\n",
      "Epoch 1572, Loss: 3.2735209465026855, Final Batch Loss: 0.7573240995407104\n",
      "Epoch 1573, Loss: 3.2375048398971558, Final Batch Loss: 0.6930630803108215\n",
      "Epoch 1574, Loss: 2.961178779602051, Final Batch Loss: 0.6278731822967529\n",
      "Epoch 1575, Loss: 2.9124180674552917, Final Batch Loss: 0.5979388952255249\n",
      "Epoch 1576, Loss: 3.2079349756240845, Final Batch Loss: 0.5865933895111084\n",
      "Epoch 1577, Loss: 3.190456449985504, Final Batch Loss: 0.671293318271637\n",
      "Epoch 1578, Loss: 3.1836825013160706, Final Batch Loss: 0.5701485872268677\n",
      "Epoch 1579, Loss: 3.18532133102417, Final Batch Loss: 0.6251409649848938\n",
      "Epoch 1580, Loss: 3.069951295852661, Final Batch Loss: 0.5973252654075623\n",
      "Epoch 1581, Loss: 3.046079397201538, Final Batch Loss: 0.5877735018730164\n",
      "Epoch 1582, Loss: 3.0457475781440735, Final Batch Loss: 0.5998959541320801\n",
      "Epoch 1583, Loss: 2.987343668937683, Final Batch Loss: 0.5436244606971741\n",
      "Epoch 1584, Loss: 3.280724823474884, Final Batch Loss: 0.5293516516685486\n",
      "Epoch 1585, Loss: 3.0310020446777344, Final Batch Loss: 0.6111136674880981\n",
      "Epoch 1586, Loss: 3.0072224736213684, Final Batch Loss: 0.6921259760856628\n",
      "Epoch 1587, Loss: 2.9314388036727905, Final Batch Loss: 0.585043728351593\n",
      "Epoch 1588, Loss: 3.1686838269233704, Final Batch Loss: 0.6458495259284973\n",
      "Epoch 1589, Loss: 3.110994338989258, Final Batch Loss: 0.6869003772735596\n",
      "Epoch 1590, Loss: 3.0635918378829956, Final Batch Loss: 0.7059237360954285\n",
      "Epoch 1591, Loss: 3.036730945110321, Final Batch Loss: 0.5620214939117432\n",
      "Epoch 1592, Loss: 3.1660255789756775, Final Batch Loss: 0.6963675618171692\n",
      "Epoch 1593, Loss: 3.1158438324928284, Final Batch Loss: 0.6839830279350281\n",
      "Epoch 1594, Loss: 3.002924084663391, Final Batch Loss: 0.7271602153778076\n",
      "Epoch 1595, Loss: 3.084749937057495, Final Batch Loss: 0.5219849348068237\n",
      "Epoch 1596, Loss: 3.1283984780311584, Final Batch Loss: 0.5937827825546265\n",
      "Epoch 1597, Loss: 2.908753275871277, Final Batch Loss: 0.5590582489967346\n",
      "Epoch 1598, Loss: 3.2015939950942993, Final Batch Loss: 0.658048152923584\n",
      "Epoch 1599, Loss: 3.0946353673934937, Final Batch Loss: 0.7056573033332825\n",
      "Epoch 1600, Loss: 3.0806127786636353, Final Batch Loss: 0.5599665641784668\n",
      "Epoch 1601, Loss: 2.9150539934635162, Final Batch Loss: 0.6081772446632385\n",
      "Epoch 1602, Loss: 2.893413543701172, Final Batch Loss: 0.49204516410827637\n",
      "Epoch 1603, Loss: 3.2382243871688843, Final Batch Loss: 0.6946342587471008\n",
      "Epoch 1604, Loss: 2.963459074497223, Final Batch Loss: 0.6424351930618286\n",
      "Epoch 1605, Loss: 3.2892183661460876, Final Batch Loss: 0.6441426873207092\n",
      "Epoch 1606, Loss: 3.034663736820221, Final Batch Loss: 0.5321823358535767\n",
      "Epoch 1607, Loss: 3.1485063433647156, Final Batch Loss: 0.5356727242469788\n",
      "Epoch 1608, Loss: 3.1366395354270935, Final Batch Loss: 0.6212977170944214\n",
      "Epoch 1609, Loss: 3.079334020614624, Final Batch Loss: 0.5402098298072815\n",
      "Epoch 1610, Loss: 3.031479239463806, Final Batch Loss: 0.5144749879837036\n",
      "Epoch 1611, Loss: 2.944097638130188, Final Batch Loss: 0.503848135471344\n",
      "Epoch 1612, Loss: 2.9701254963874817, Final Batch Loss: 0.6079645156860352\n",
      "Epoch 1613, Loss: 3.037404716014862, Final Batch Loss: 0.6444892287254333\n",
      "Epoch 1614, Loss: 2.971352279186249, Final Batch Loss: 0.5031894445419312\n",
      "Epoch 1615, Loss: 2.9441250562667847, Final Batch Loss: 0.5478672981262207\n",
      "Epoch 1616, Loss: 3.0656092762947083, Final Batch Loss: 0.6536575555801392\n",
      "Epoch 1617, Loss: 2.9457305669784546, Final Batch Loss: 0.5424271821975708\n",
      "Epoch 1618, Loss: 2.9269901514053345, Final Batch Loss: 0.6173297166824341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1619, Loss: 2.898579478263855, Final Batch Loss: 0.6295772194862366\n",
      "Epoch 1620, Loss: 2.921403646469116, Final Batch Loss: 0.559561550617218\n",
      "Epoch 1621, Loss: 3.0696151852607727, Final Batch Loss: 0.5475353598594666\n",
      "Epoch 1622, Loss: 2.908594995737076, Final Batch Loss: 0.5430812835693359\n",
      "Epoch 1623, Loss: 2.9298945665359497, Final Batch Loss: 0.6107850074768066\n",
      "Epoch 1624, Loss: 3.0095561742782593, Final Batch Loss: 0.5356084108352661\n",
      "Epoch 1625, Loss: 3.0667957067489624, Final Batch Loss: 0.6127440929412842\n",
      "Epoch 1626, Loss: 2.8784061670303345, Final Batch Loss: 0.5691417455673218\n",
      "Epoch 1627, Loss: 3.118467688560486, Final Batch Loss: 0.6650079488754272\n",
      "Epoch 1628, Loss: 3.058219611644745, Final Batch Loss: 0.5452606081962585\n",
      "Epoch 1629, Loss: 3.0555800199508667, Final Batch Loss: 0.6350735425949097\n",
      "Epoch 1630, Loss: 3.0443596243858337, Final Batch Loss: 0.6300051808357239\n",
      "Epoch 1631, Loss: 3.2870266437530518, Final Batch Loss: 0.7322784662246704\n",
      "Epoch 1632, Loss: 3.0239970088005066, Final Batch Loss: 0.5899664163589478\n",
      "Epoch 1633, Loss: 2.9564552903175354, Final Batch Loss: 0.6099175810813904\n",
      "Epoch 1634, Loss: 2.8808558583259583, Final Batch Loss: 0.5755704045295715\n",
      "Epoch 1635, Loss: 2.939574718475342, Final Batch Loss: 0.5413689017295837\n",
      "Epoch 1636, Loss: 3.3477352261543274, Final Batch Loss: 0.6565409898757935\n",
      "Epoch 1637, Loss: 3.0796391367912292, Final Batch Loss: 0.5487474799156189\n",
      "Epoch 1638, Loss: 3.099416673183441, Final Batch Loss: 0.6328168511390686\n",
      "Epoch 1639, Loss: 3.0515216588974, Final Batch Loss: 0.6411629915237427\n",
      "Epoch 1640, Loss: 3.0070151686668396, Final Batch Loss: 0.6091346144676208\n",
      "Epoch 1641, Loss: 3.1167741417884827, Final Batch Loss: 0.6538142561912537\n",
      "Epoch 1642, Loss: 2.97512423992157, Final Batch Loss: 0.6557884216308594\n",
      "Epoch 1643, Loss: 3.093109965324402, Final Batch Loss: 0.7520228028297424\n",
      "Epoch 1644, Loss: 2.9666109681129456, Final Batch Loss: 0.4739421010017395\n",
      "Epoch 1645, Loss: 3.114840567111969, Final Batch Loss: 0.6899577975273132\n",
      "Epoch 1646, Loss: 3.036888897418976, Final Batch Loss: 0.6164442896842957\n",
      "Epoch 1647, Loss: 2.903204530477524, Final Batch Loss: 0.630397379398346\n",
      "Epoch 1648, Loss: 3.132357597351074, Final Batch Loss: 0.6015187501907349\n",
      "Epoch 1649, Loss: 2.9395808577537537, Final Batch Loss: 0.6109262108802795\n",
      "Epoch 1650, Loss: 3.0285422801971436, Final Batch Loss: 0.6868326663970947\n",
      "Epoch 1651, Loss: 2.9394038915634155, Final Batch Loss: 0.6040096282958984\n",
      "Epoch 1652, Loss: 2.9613766968250275, Final Batch Loss: 0.5610483884811401\n",
      "Epoch 1653, Loss: 3.1026363372802734, Final Batch Loss: 0.6078807711601257\n",
      "Epoch 1654, Loss: 3.035426378250122, Final Batch Loss: 0.6295260787010193\n",
      "Epoch 1655, Loss: 3.0118717551231384, Final Batch Loss: 0.5478029847145081\n",
      "Epoch 1656, Loss: 3.2026432156562805, Final Batch Loss: 0.6236345767974854\n",
      "Epoch 1657, Loss: 2.992539405822754, Final Batch Loss: 0.5342012047767639\n",
      "Epoch 1658, Loss: 3.215360462665558, Final Batch Loss: 0.5998380184173584\n",
      "Epoch 1659, Loss: 3.113801449537277, Final Batch Loss: 0.6829900145530701\n",
      "Epoch 1660, Loss: 3.133971929550171, Final Batch Loss: 0.6259509325027466\n",
      "Epoch 1661, Loss: 3.0594630241394043, Final Batch Loss: 0.5882160067558289\n",
      "Epoch 1662, Loss: 2.936837375164032, Final Batch Loss: 0.6603967547416687\n",
      "Epoch 1663, Loss: 2.9590298533439636, Final Batch Loss: 0.5179110765457153\n",
      "Epoch 1664, Loss: 3.016340494155884, Final Batch Loss: 0.6173664331436157\n",
      "Epoch 1665, Loss: 3.239326775074005, Final Batch Loss: 0.820705771446228\n",
      "Epoch 1666, Loss: 3.1595221161842346, Final Batch Loss: 0.7102111577987671\n",
      "Epoch 1667, Loss: 3.0143883526325226, Final Batch Loss: 0.5298003554344177\n",
      "Epoch 1668, Loss: 3.0893537402153015, Final Batch Loss: 0.7282269597053528\n",
      "Epoch 1669, Loss: 3.031751275062561, Final Batch Loss: 0.5799841284751892\n",
      "Epoch 1670, Loss: 2.994562566280365, Final Batch Loss: 0.5957127213478088\n",
      "Epoch 1671, Loss: 2.908954083919525, Final Batch Loss: 0.5403703451156616\n",
      "Epoch 1672, Loss: 3.0678075551986694, Final Batch Loss: 0.61674565076828\n",
      "Epoch 1673, Loss: 2.99953031539917, Final Batch Loss: 0.6545876264572144\n",
      "Epoch 1674, Loss: 3.0530759692192078, Final Batch Loss: 0.6666169166564941\n",
      "Epoch 1675, Loss: 3.0020830035209656, Final Batch Loss: 0.6410962343215942\n",
      "Epoch 1676, Loss: 3.0169678330421448, Final Batch Loss: 0.57261723279953\n",
      "Epoch 1677, Loss: 2.823774993419647, Final Batch Loss: 0.46547752618789673\n",
      "Epoch 1678, Loss: 3.006937623023987, Final Batch Loss: 0.6130867600440979\n",
      "Epoch 1679, Loss: 2.9541512727737427, Final Batch Loss: 0.5457825064659119\n",
      "Epoch 1680, Loss: 3.010981798171997, Final Batch Loss: 0.6098922491073608\n",
      "Epoch 1681, Loss: 3.0422750115394592, Final Batch Loss: 0.64357590675354\n",
      "Epoch 1682, Loss: 2.956768751144409, Final Batch Loss: 0.507269561290741\n",
      "Epoch 1683, Loss: 3.0709176063537598, Final Batch Loss: 0.6177286505699158\n",
      "Epoch 1684, Loss: 3.03954154253006, Final Batch Loss: 0.7709725499153137\n",
      "Epoch 1685, Loss: 3.1280917525291443, Final Batch Loss: 0.5585832595825195\n",
      "Epoch 1686, Loss: 3.2189653515815735, Final Batch Loss: 0.5115455389022827\n",
      "Epoch 1687, Loss: 3.085606575012207, Final Batch Loss: 0.5612421631813049\n",
      "Epoch 1688, Loss: 3.0530460476875305, Final Batch Loss: 0.6400533318519592\n",
      "Epoch 1689, Loss: 2.863293945789337, Final Batch Loss: 0.6032031774520874\n",
      "Epoch 1690, Loss: 2.9883581399917603, Final Batch Loss: 0.5856173634529114\n",
      "Epoch 1691, Loss: 2.8264133036136627, Final Batch Loss: 0.5680134892463684\n",
      "Epoch 1692, Loss: 3.0089147090911865, Final Batch Loss: 0.5365790724754333\n",
      "Epoch 1693, Loss: 2.96370130777359, Final Batch Loss: 0.6216781139373779\n",
      "Epoch 1694, Loss: 2.907337725162506, Final Batch Loss: 0.5647777915000916\n",
      "Epoch 1695, Loss: 3.0638996958732605, Final Batch Loss: 0.6843703389167786\n",
      "Epoch 1696, Loss: 2.811560571193695, Final Batch Loss: 0.5284784436225891\n",
      "Epoch 1697, Loss: 3.043777346611023, Final Batch Loss: 0.6113568544387817\n",
      "Epoch 1698, Loss: 2.939954102039337, Final Batch Loss: 0.5455086827278137\n",
      "Epoch 1699, Loss: 3.0863659977912903, Final Batch Loss: 0.5770342350006104\n",
      "Epoch 1700, Loss: 2.95578134059906, Final Batch Loss: 0.5563633441925049\n",
      "Epoch 1701, Loss: 3.0572815537452698, Final Batch Loss: 0.7047396302223206\n",
      "Epoch 1702, Loss: 2.8787019848823547, Final Batch Loss: 0.5556620955467224\n",
      "Epoch 1703, Loss: 2.8204044103622437, Final Batch Loss: 0.6158608198165894\n",
      "Epoch 1704, Loss: 2.8957448601722717, Final Batch Loss: 0.602424681186676\n",
      "Epoch 1705, Loss: 3.0765833854675293, Final Batch Loss: 0.691752016544342\n",
      "Epoch 1706, Loss: 3.085300385951996, Final Batch Loss: 0.6632081270217896\n",
      "Epoch 1707, Loss: 2.945691466331482, Final Batch Loss: 0.6131957769393921\n",
      "Epoch 1708, Loss: 3.0642242431640625, Final Batch Loss: 0.6528246402740479\n",
      "Epoch 1709, Loss: 3.023321747779846, Final Batch Loss: 0.5997661352157593\n",
      "Epoch 1710, Loss: 2.95984947681427, Final Batch Loss: 0.644644021987915\n",
      "Epoch 1711, Loss: 3.010193347930908, Final Batch Loss: 0.5882050395011902\n",
      "Epoch 1712, Loss: 2.8789694905281067, Final Batch Loss: 0.5918565392494202\n",
      "Epoch 1713, Loss: 2.9371635913848877, Final Batch Loss: 0.6063125729560852\n",
      "Epoch 1714, Loss: 2.93198961019516, Final Batch Loss: 0.5594991445541382\n",
      "Epoch 1715, Loss: 3.0593071579933167, Final Batch Loss: 0.6334389448165894\n",
      "Epoch 1716, Loss: 2.836788773536682, Final Batch Loss: 0.5200543403625488\n",
      "Epoch 1717, Loss: 3.0106603503227234, Final Batch Loss: 0.6502545475959778\n",
      "Epoch 1718, Loss: 2.9315832257270813, Final Batch Loss: 0.6165342330932617\n",
      "Epoch 1719, Loss: 2.8988033831119537, Final Batch Loss: 0.49340441823005676\n",
      "Epoch 1720, Loss: 2.967555046081543, Final Batch Loss: 0.6410781145095825\n",
      "Epoch 1721, Loss: 2.9219441413879395, Final Batch Loss: 0.5543275475502014\n",
      "Epoch 1722, Loss: 2.9612345099449158, Final Batch Loss: 0.5411503314971924\n",
      "Epoch 1723, Loss: 2.787912428379059, Final Batch Loss: 0.5376670956611633\n",
      "Epoch 1724, Loss: 3.167432725429535, Final Batch Loss: 0.5943584442138672\n",
      "Epoch 1725, Loss: 3.0993653535842896, Final Batch Loss: 0.6215029358863831\n",
      "Epoch 1726, Loss: 2.879437416791916, Final Batch Loss: 0.5975856184959412\n",
      "Epoch 1727, Loss: 2.839645355939865, Final Batch Loss: 0.4986930191516876\n",
      "Epoch 1728, Loss: 2.9324764013290405, Final Batch Loss: 0.5040282607078552\n",
      "Epoch 1729, Loss: 2.930223524570465, Final Batch Loss: 0.5716416239738464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1730, Loss: 2.977782964706421, Final Batch Loss: 0.589847207069397\n",
      "Epoch 1731, Loss: 2.9485978484153748, Final Batch Loss: 0.5108300447463989\n",
      "Epoch 1732, Loss: 2.9910005927085876, Final Batch Loss: 0.5853541493415833\n",
      "Epoch 1733, Loss: 2.975948452949524, Final Batch Loss: 0.5806080102920532\n",
      "Epoch 1734, Loss: 3.0172342658042908, Final Batch Loss: 0.6720452308654785\n",
      "Epoch 1735, Loss: 2.876201868057251, Final Batch Loss: 0.5250586271286011\n",
      "Epoch 1736, Loss: 2.8906012177467346, Final Batch Loss: 0.5693904757499695\n",
      "Epoch 1737, Loss: 2.943820834159851, Final Batch Loss: 0.5854741930961609\n",
      "Epoch 1738, Loss: 3.0575229823589325, Final Batch Loss: 0.6694445610046387\n",
      "Epoch 1739, Loss: 3.044044077396393, Final Batch Loss: 0.6277332901954651\n",
      "Epoch 1740, Loss: 2.9922072887420654, Final Batch Loss: 0.61903315782547\n",
      "Epoch 1741, Loss: 3.147558242082596, Final Batch Loss: 0.7006117701530457\n",
      "Epoch 1742, Loss: 2.9599724411964417, Final Batch Loss: 0.6099402904510498\n",
      "Epoch 1743, Loss: 2.9343857169151306, Final Batch Loss: 0.5712952613830566\n",
      "Epoch 1744, Loss: 2.8551488518714905, Final Batch Loss: 0.5672655701637268\n",
      "Epoch 1745, Loss: 3.0629119277000427, Final Batch Loss: 0.6377879977226257\n",
      "Epoch 1746, Loss: 2.9833720922470093, Final Batch Loss: 0.6033384203910828\n",
      "Epoch 1747, Loss: 3.0858014822006226, Final Batch Loss: 0.49481910467147827\n",
      "Epoch 1748, Loss: 2.8499979078769684, Final Batch Loss: 0.6929687261581421\n",
      "Epoch 1749, Loss: 2.9581746459007263, Final Batch Loss: 0.6220405101776123\n",
      "Epoch 1750, Loss: 3.071094274520874, Final Batch Loss: 0.6543319225311279\n",
      "Epoch 1751, Loss: 3.04487282037735, Final Batch Loss: 0.6051686406135559\n",
      "Epoch 1752, Loss: 2.855093777179718, Final Batch Loss: 0.538041353225708\n",
      "Epoch 1753, Loss: 2.8340874910354614, Final Batch Loss: 0.560547947883606\n",
      "Epoch 1754, Loss: 2.9059698581695557, Final Batch Loss: 0.5527993440628052\n",
      "Epoch 1755, Loss: 2.807763993740082, Final Batch Loss: 0.6889377236366272\n",
      "Epoch 1756, Loss: 2.851775348186493, Final Batch Loss: 0.5268780589103699\n",
      "Epoch 1757, Loss: 2.955806016921997, Final Batch Loss: 0.6137518882751465\n",
      "Epoch 1758, Loss: 2.98606538772583, Final Batch Loss: 0.662180483341217\n",
      "Epoch 1759, Loss: 3.073939085006714, Final Batch Loss: 0.5845620632171631\n",
      "Epoch 1760, Loss: 2.9735415875911713, Final Batch Loss: 0.48816874623298645\n",
      "Epoch 1761, Loss: 3.3521817326545715, Final Batch Loss: 0.6143890619277954\n",
      "Epoch 1762, Loss: 2.849179655313492, Final Batch Loss: 0.4338432848453522\n",
      "Epoch 1763, Loss: 2.9773108959198, Final Batch Loss: 0.5039706230163574\n",
      "Epoch 1764, Loss: 2.941233277320862, Final Batch Loss: 0.5803114175796509\n",
      "Epoch 1765, Loss: 2.889169156551361, Final Batch Loss: 0.6550853252410889\n",
      "Epoch 1766, Loss: 2.846107453107834, Final Batch Loss: 0.6019431352615356\n",
      "Epoch 1767, Loss: 2.700489640235901, Final Batch Loss: 0.5010115504264832\n",
      "Epoch 1768, Loss: 2.8794596195220947, Final Batch Loss: 0.5607898831367493\n",
      "Epoch 1769, Loss: 2.9901522397994995, Final Batch Loss: 0.5480034947395325\n",
      "Epoch 1770, Loss: 3.202455759048462, Final Batch Loss: 0.7991722822189331\n",
      "Epoch 1771, Loss: 2.9191441535949707, Final Batch Loss: 0.5570586323738098\n",
      "Epoch 1772, Loss: 2.9081953167915344, Final Batch Loss: 0.5202259421348572\n",
      "Epoch 1773, Loss: 2.8081385493278503, Final Batch Loss: 0.5156751275062561\n",
      "Epoch 1774, Loss: 3.0091336369514465, Final Batch Loss: 0.5226661562919617\n",
      "Epoch 1775, Loss: 2.989300847053528, Final Batch Loss: 0.760808527469635\n",
      "Epoch 1776, Loss: 3.030369758605957, Final Batch Loss: 0.4442303776741028\n",
      "Epoch 1777, Loss: 2.9681495428085327, Final Batch Loss: 0.5777460336685181\n",
      "Epoch 1778, Loss: 2.8320560455322266, Final Batch Loss: 0.5393773913383484\n",
      "Epoch 1779, Loss: 2.908255159854889, Final Batch Loss: 0.6125670075416565\n",
      "Epoch 1780, Loss: 2.82306769490242, Final Batch Loss: 0.4815407693386078\n",
      "Epoch 1781, Loss: 2.863399177789688, Final Batch Loss: 0.6504306197166443\n",
      "Epoch 1782, Loss: 2.978789806365967, Final Batch Loss: 0.6218268275260925\n",
      "Epoch 1783, Loss: 3.1223637461662292, Final Batch Loss: 0.7677309513092041\n",
      "Epoch 1784, Loss: 2.9114213585853577, Final Batch Loss: 0.5691246390342712\n",
      "Epoch 1785, Loss: 2.958972454071045, Final Batch Loss: 0.5659913420677185\n",
      "Epoch 1786, Loss: 3.0303446650505066, Final Batch Loss: 0.5656709671020508\n",
      "Epoch 1787, Loss: 3.035533308982849, Final Batch Loss: 0.5907503366470337\n",
      "Epoch 1788, Loss: 3.032958984375, Final Batch Loss: 0.6481159329414368\n",
      "Epoch 1789, Loss: 2.974418520927429, Final Batch Loss: 0.5609782934188843\n",
      "Epoch 1790, Loss: 2.835611492395401, Final Batch Loss: 0.4514189660549164\n",
      "Epoch 1791, Loss: 2.9290337562561035, Final Batch Loss: 0.5036064982414246\n",
      "Epoch 1792, Loss: 3.019728183746338, Final Batch Loss: 0.6016672849655151\n",
      "Epoch 1793, Loss: 2.8238200545310974, Final Batch Loss: 0.47982561588287354\n",
      "Epoch 1794, Loss: 2.8933802247047424, Final Batch Loss: 0.5901256203651428\n",
      "Epoch 1795, Loss: 2.903762102127075, Final Batch Loss: 0.5709660053253174\n",
      "Epoch 1796, Loss: 2.9200531244277954, Final Batch Loss: 0.5601979494094849\n",
      "Epoch 1797, Loss: 3.0143741965293884, Final Batch Loss: 0.792178213596344\n",
      "Epoch 1798, Loss: 2.9901354908943176, Final Batch Loss: 0.5416315793991089\n",
      "Epoch 1799, Loss: 2.8827452659606934, Final Batch Loss: 0.4061487913131714\n",
      "Epoch 1800, Loss: 2.929133802652359, Final Batch Loss: 0.5720775723457336\n",
      "Epoch 1801, Loss: 2.8482914566993713, Final Batch Loss: 0.52293461561203\n",
      "Epoch 1802, Loss: 3.114023804664612, Final Batch Loss: 0.6537232995033264\n",
      "Epoch 1803, Loss: 3.0607361793518066, Final Batch Loss: 0.6352146863937378\n",
      "Epoch 1804, Loss: 3.0910109877586365, Final Batch Loss: 0.6417036652565002\n",
      "Epoch 1805, Loss: 3.112504303455353, Final Batch Loss: 0.5021758079528809\n",
      "Epoch 1806, Loss: 2.866591155529022, Final Batch Loss: 0.5865458250045776\n",
      "Epoch 1807, Loss: 2.9287819862365723, Final Batch Loss: 0.680912435054779\n",
      "Epoch 1808, Loss: 3.1492819786071777, Final Batch Loss: 0.7548733949661255\n",
      "Epoch 1809, Loss: 3.086883783340454, Final Batch Loss: 0.5725988149642944\n",
      "Epoch 1810, Loss: 2.953451633453369, Final Batch Loss: 0.5317195653915405\n",
      "Epoch 1811, Loss: 2.8902326226234436, Final Batch Loss: 0.5253182649612427\n",
      "Epoch 1812, Loss: 2.774916112422943, Final Batch Loss: 0.4825183153152466\n",
      "Epoch 1813, Loss: 2.9110778272151947, Final Batch Loss: 0.6863131523132324\n",
      "Epoch 1814, Loss: 2.9419781267642975, Final Batch Loss: 0.6848676800727844\n",
      "Epoch 1815, Loss: 3.0417404770851135, Final Batch Loss: 0.5704686045646667\n",
      "Epoch 1816, Loss: 2.9940739274024963, Final Batch Loss: 0.5861538648605347\n",
      "Epoch 1817, Loss: 2.9456557035446167, Final Batch Loss: 0.5751747488975525\n",
      "Epoch 1818, Loss: 2.925475239753723, Final Batch Loss: 0.5209759473800659\n",
      "Epoch 1819, Loss: 2.8884612917900085, Final Batch Loss: 0.5702757835388184\n",
      "Epoch 1820, Loss: 2.9037291407585144, Final Batch Loss: 0.6780610084533691\n",
      "Epoch 1821, Loss: 2.9839396476745605, Final Batch Loss: 0.529351532459259\n",
      "Epoch 1822, Loss: 2.902650535106659, Final Batch Loss: 0.6244158744812012\n",
      "Epoch 1823, Loss: 2.931483805179596, Final Batch Loss: 0.6183392405509949\n",
      "Epoch 1824, Loss: 2.875013768672943, Final Batch Loss: 0.5919626951217651\n",
      "Epoch 1825, Loss: 2.9891337752342224, Final Batch Loss: 0.5146809220314026\n",
      "Epoch 1826, Loss: 3.063588261604309, Final Batch Loss: 0.6979677081108093\n",
      "Epoch 1827, Loss: 2.975585162639618, Final Batch Loss: 0.714279294013977\n",
      "Epoch 1828, Loss: 2.9565332531929016, Final Batch Loss: 0.6416199803352356\n",
      "Epoch 1829, Loss: 3.0293214321136475, Final Batch Loss: 0.5588655471801758\n",
      "Epoch 1830, Loss: 2.908421754837036, Final Batch Loss: 0.5513185262680054\n",
      "Epoch 1831, Loss: 2.9397692680358887, Final Batch Loss: 0.6789607405662537\n",
      "Epoch 1832, Loss: 2.916490375995636, Final Batch Loss: 0.5423485636711121\n",
      "Epoch 1833, Loss: 2.960034728050232, Final Batch Loss: 0.562703013420105\n",
      "Epoch 1834, Loss: 2.982952892780304, Final Batch Loss: 0.5448974370956421\n",
      "Epoch 1835, Loss: 2.9200043082237244, Final Batch Loss: 0.5578591823577881\n",
      "Epoch 1836, Loss: 3.0024751126766205, Final Batch Loss: 0.43348750472068787\n",
      "Epoch 1837, Loss: 2.9202200174331665, Final Batch Loss: 0.519626796245575\n",
      "Epoch 1838, Loss: 2.7265284657478333, Final Batch Loss: 0.5084113478660583\n",
      "Epoch 1839, Loss: 2.9206389784812927, Final Batch Loss: 0.5779627561569214\n",
      "Epoch 1840, Loss: 2.7590094208717346, Final Batch Loss: 0.5203709006309509\n",
      "Epoch 1841, Loss: 2.919226884841919, Final Batch Loss: 0.630158007144928\n",
      "Epoch 1842, Loss: 2.9308158457279205, Final Batch Loss: 0.6988213658332825\n",
      "Epoch 1843, Loss: 3.0716131925582886, Final Batch Loss: 0.6335090398788452\n",
      "Epoch 1844, Loss: 3.1181028485298157, Final Batch Loss: 0.6772920489311218\n",
      "Epoch 1845, Loss: 2.9283037781715393, Final Batch Loss: 0.6406318545341492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1846, Loss: 2.954876184463501, Final Batch Loss: 0.584495484828949\n",
      "Epoch 1847, Loss: 3.0093594193458557, Final Batch Loss: 0.6227564215660095\n",
      "Epoch 1848, Loss: 2.908121109008789, Final Batch Loss: 0.5480940937995911\n",
      "Epoch 1849, Loss: 2.8578462302684784, Final Batch Loss: 0.5947127342224121\n",
      "Epoch 1850, Loss: 3.0403817892074585, Final Batch Loss: 0.6520796418190002\n",
      "Epoch 1851, Loss: 2.824502408504486, Final Batch Loss: 0.6070423126220703\n",
      "Epoch 1852, Loss: 2.801839768886566, Final Batch Loss: 0.5044576525688171\n",
      "Epoch 1853, Loss: 2.888085186481476, Final Batch Loss: 0.6775771379470825\n",
      "Epoch 1854, Loss: 3.0327304005622864, Final Batch Loss: 0.5997123718261719\n",
      "Epoch 1855, Loss: 2.9774467051029205, Final Batch Loss: 0.6725279688835144\n",
      "Epoch 1856, Loss: 2.969253897666931, Final Batch Loss: 0.576985776424408\n",
      "Epoch 1857, Loss: 2.912963330745697, Final Batch Loss: 0.6088485717773438\n",
      "Epoch 1858, Loss: 2.9026759266853333, Final Batch Loss: 0.5580577254295349\n",
      "Epoch 1859, Loss: 2.8651633262634277, Final Batch Loss: 0.6097819209098816\n",
      "Epoch 1860, Loss: 2.917839765548706, Final Batch Loss: 0.5788641571998596\n",
      "Epoch 1861, Loss: 3.0161072611808777, Final Batch Loss: 0.7105937004089355\n",
      "Epoch 1862, Loss: 2.8550647497177124, Final Batch Loss: 0.4913499355316162\n",
      "Epoch 1863, Loss: 2.857547342777252, Final Batch Loss: 0.4604419469833374\n",
      "Epoch 1864, Loss: 2.9682774543762207, Final Batch Loss: 0.6646022796630859\n",
      "Epoch 1865, Loss: 2.9263636469841003, Final Batch Loss: 0.5782885551452637\n",
      "Epoch 1866, Loss: 2.8468863368034363, Final Batch Loss: 0.5807014107704163\n",
      "Epoch 1867, Loss: 2.791731148958206, Final Batch Loss: 0.6019097566604614\n",
      "Epoch 1868, Loss: 3.0970303416252136, Final Batch Loss: 0.589076578617096\n",
      "Epoch 1869, Loss: 2.914410173892975, Final Batch Loss: 0.6244848370552063\n",
      "Epoch 1870, Loss: 2.913130283355713, Final Batch Loss: 0.5842167735099792\n",
      "Epoch 1871, Loss: 2.8820719122886658, Final Batch Loss: 0.5567501187324524\n",
      "Epoch 1872, Loss: 2.878131628036499, Final Batch Loss: 0.5277706384658813\n",
      "Epoch 1873, Loss: 3.0642243027687073, Final Batch Loss: 0.550484836101532\n",
      "Epoch 1874, Loss: 2.864155650138855, Final Batch Loss: 0.5109506249427795\n",
      "Epoch 1875, Loss: 2.891714632511139, Final Batch Loss: 0.4967203140258789\n",
      "Epoch 1876, Loss: 2.9733564257621765, Final Batch Loss: 0.626182496547699\n",
      "Epoch 1877, Loss: 2.718279719352722, Final Batch Loss: 0.48651033639907837\n",
      "Epoch 1878, Loss: 2.874380648136139, Final Batch Loss: 0.5864977240562439\n",
      "Epoch 1879, Loss: 2.7181316614151, Final Batch Loss: 0.5030421018600464\n",
      "Epoch 1880, Loss: 2.858473300933838, Final Batch Loss: 0.6162264347076416\n",
      "Epoch 1881, Loss: 2.877518892288208, Final Batch Loss: 0.5229835510253906\n",
      "Epoch 1882, Loss: 2.924945652484894, Final Batch Loss: 0.5850754976272583\n",
      "Epoch 1883, Loss: 2.8336558640003204, Final Batch Loss: 0.5399972796440125\n",
      "Epoch 1884, Loss: 2.850339412689209, Final Batch Loss: 0.5924662351608276\n",
      "Epoch 1885, Loss: 2.825472801923752, Final Batch Loss: 0.434233695268631\n",
      "Epoch 1886, Loss: 2.7667075395584106, Final Batch Loss: 0.5966827273368835\n",
      "Epoch 1887, Loss: 2.7640355825424194, Final Batch Loss: 0.5273866057395935\n",
      "Epoch 1888, Loss: 2.961361885070801, Final Batch Loss: 0.7222092151641846\n",
      "Epoch 1889, Loss: 2.910831391811371, Final Batch Loss: 0.6871495246887207\n",
      "Epoch 1890, Loss: 2.824816882610321, Final Batch Loss: 0.620133101940155\n",
      "Epoch 1891, Loss: 2.6733485758304596, Final Batch Loss: 0.48898550868034363\n",
      "Epoch 1892, Loss: 2.854320466518402, Final Batch Loss: 0.6252045035362244\n",
      "Epoch 1893, Loss: 2.9139583110809326, Final Batch Loss: 0.46786820888519287\n",
      "Epoch 1894, Loss: 2.774095892906189, Final Batch Loss: 0.5129167437553406\n",
      "Epoch 1895, Loss: 3.0005274415016174, Final Batch Loss: 0.6598496437072754\n",
      "Epoch 1896, Loss: 2.9722132682800293, Final Batch Loss: 0.6335573792457581\n",
      "Epoch 1897, Loss: 2.8266722559928894, Final Batch Loss: 0.5626495480537415\n",
      "Epoch 1898, Loss: 3.08577823638916, Final Batch Loss: 0.7634503841400146\n",
      "Epoch 1899, Loss: 2.998492956161499, Final Batch Loss: 0.7111144065856934\n",
      "Epoch 1900, Loss: 3.0048328638076782, Final Batch Loss: 0.6431161761283875\n",
      "Epoch 1901, Loss: 2.9922485947608948, Final Batch Loss: 0.6363971829414368\n",
      "Epoch 1902, Loss: 2.8639941215515137, Final Batch Loss: 0.5009965300559998\n",
      "Epoch 1903, Loss: 2.8828846216201782, Final Batch Loss: 0.5413438081741333\n",
      "Epoch 1904, Loss: 2.8358094692230225, Final Batch Loss: 0.5818281769752502\n",
      "Epoch 1905, Loss: 2.8300126791000366, Final Batch Loss: 0.5181382894515991\n",
      "Epoch 1906, Loss: 2.9185928106307983, Final Batch Loss: 0.5899152755737305\n",
      "Epoch 1907, Loss: 2.807629704475403, Final Batch Loss: 0.5281876921653748\n",
      "Epoch 1908, Loss: 2.791783571243286, Final Batch Loss: 0.5288941264152527\n",
      "Epoch 1909, Loss: 2.8676209449768066, Final Batch Loss: 0.5109230279922485\n",
      "Epoch 1910, Loss: 2.9750062227249146, Final Batch Loss: 0.601457417011261\n",
      "Epoch 1911, Loss: 2.9916083216667175, Final Batch Loss: 0.6285849809646606\n",
      "Epoch 1912, Loss: 2.9787506759166718, Final Batch Loss: 0.6419878005981445\n",
      "Epoch 1913, Loss: 2.8327670097351074, Final Batch Loss: 0.5789858102798462\n",
      "Epoch 1914, Loss: 2.9042288661003113, Final Batch Loss: 0.5499483942985535\n",
      "Epoch 1915, Loss: 2.8454349040985107, Final Batch Loss: 0.638152539730072\n",
      "Epoch 1916, Loss: 2.7836252450942993, Final Batch Loss: 0.49218565225601196\n",
      "Epoch 1917, Loss: 2.86628121137619, Final Batch Loss: 0.5195441842079163\n",
      "Epoch 1918, Loss: 2.956885576248169, Final Batch Loss: 0.678662896156311\n",
      "Epoch 1919, Loss: 3.0105419158935547, Final Batch Loss: 0.6577395796775818\n",
      "Epoch 1920, Loss: 3.0145384073257446, Final Batch Loss: 0.6480231881141663\n",
      "Epoch 1921, Loss: 2.995862305164337, Final Batch Loss: 0.5411617755889893\n",
      "Epoch 1922, Loss: 2.8630248308181763, Final Batch Loss: 0.6559890508651733\n",
      "Epoch 1923, Loss: 2.798249214887619, Final Batch Loss: 0.5878337025642395\n",
      "Epoch 1924, Loss: 2.9854652285575867, Final Batch Loss: 0.5150775909423828\n",
      "Epoch 1925, Loss: 2.981639266014099, Final Batch Loss: 0.6603980660438538\n",
      "Epoch 1926, Loss: 2.8244496881961823, Final Batch Loss: 0.46571585536003113\n",
      "Epoch 1927, Loss: 2.83739572763443, Final Batch Loss: 0.5496717691421509\n",
      "Epoch 1928, Loss: 2.9480015635490417, Final Batch Loss: 0.6069942116737366\n",
      "Epoch 1929, Loss: 2.846769869327545, Final Batch Loss: 0.5107811689376831\n",
      "Epoch 1930, Loss: 2.8066497445106506, Final Batch Loss: 0.47617876529693604\n",
      "Epoch 1931, Loss: 2.8288672864437103, Final Batch Loss: 0.696869969367981\n",
      "Epoch 1932, Loss: 3.098270535469055, Final Batch Loss: 0.6250485777854919\n",
      "Epoch 1933, Loss: 2.8359827995300293, Final Batch Loss: 0.4999059736728668\n",
      "Epoch 1934, Loss: 2.879592001438141, Final Batch Loss: 0.5157331228256226\n",
      "Epoch 1935, Loss: 2.934755861759186, Final Batch Loss: 0.5358246564865112\n",
      "Epoch 1936, Loss: 2.787227511405945, Final Batch Loss: 0.6457889676094055\n",
      "Epoch 1937, Loss: 2.906741201877594, Final Batch Loss: 0.4834679365158081\n",
      "Epoch 1938, Loss: 2.971103012561798, Final Batch Loss: 0.5538652539253235\n",
      "Epoch 1939, Loss: 3.0061410069465637, Final Batch Loss: 0.6666299700737\n",
      "Epoch 1940, Loss: 2.92819607257843, Final Batch Loss: 0.5580980181694031\n",
      "Epoch 1941, Loss: 3.03388512134552, Final Batch Loss: 0.8290644288063049\n",
      "Epoch 1942, Loss: 3.0902275443077087, Final Batch Loss: 0.6726399064064026\n",
      "Epoch 1943, Loss: 2.840832769870758, Final Batch Loss: 0.47739315032958984\n",
      "Epoch 1944, Loss: 2.7874717116355896, Final Batch Loss: 0.49229198694229126\n",
      "Epoch 1945, Loss: 2.8713268041610718, Final Batch Loss: 0.5053232908248901\n",
      "Epoch 1946, Loss: 2.857387602329254, Final Batch Loss: 0.5417941212654114\n",
      "Epoch 1947, Loss: 2.922212779521942, Final Batch Loss: 0.6290683746337891\n",
      "Epoch 1948, Loss: 2.8137453198432922, Final Batch Loss: 0.6010467410087585\n",
      "Epoch 1949, Loss: 2.8515397906303406, Final Batch Loss: 0.5298646688461304\n",
      "Epoch 1950, Loss: 2.838552951812744, Final Batch Loss: 0.5766254663467407\n",
      "Epoch 1951, Loss: 2.639067232608795, Final Batch Loss: 0.4963114857673645\n",
      "Epoch 1952, Loss: 2.7866485118865967, Final Batch Loss: 0.5157836079597473\n",
      "Epoch 1953, Loss: 2.8451850414276123, Final Batch Loss: 0.5199072957038879\n",
      "Epoch 1954, Loss: 2.9230449199676514, Final Batch Loss: 0.5886345505714417\n",
      "Epoch 1955, Loss: 3.0016103386878967, Final Batch Loss: 0.6454893350601196\n",
      "Epoch 1956, Loss: 2.831826835870743, Final Batch Loss: 0.566752552986145\n",
      "Epoch 1957, Loss: 2.721416801214218, Final Batch Loss: 0.4529089033603668\n",
      "Epoch 1958, Loss: 2.8304948210716248, Final Batch Loss: 0.4439370036125183\n",
      "Epoch 1959, Loss: 2.8923261761665344, Final Batch Loss: 0.5743606090545654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1960, Loss: 3.0010863542556763, Final Batch Loss: 0.6284807324409485\n",
      "Epoch 1961, Loss: 2.826162666082382, Final Batch Loss: 0.4499068558216095\n",
      "Epoch 1962, Loss: 2.8813832998275757, Final Batch Loss: 0.5360018610954285\n",
      "Epoch 1963, Loss: 2.9039788246154785, Final Batch Loss: 0.4745151400566101\n",
      "Epoch 1964, Loss: 2.9716712832450867, Final Batch Loss: 0.6362611055374146\n",
      "Epoch 1965, Loss: 2.8047118186950684, Final Batch Loss: 0.5342194437980652\n",
      "Epoch 1966, Loss: 3.039582133293152, Final Batch Loss: 0.6357917785644531\n",
      "Epoch 1967, Loss: 2.899683117866516, Final Batch Loss: 0.6103094816207886\n",
      "Epoch 1968, Loss: 2.8068944215774536, Final Batch Loss: 0.5462818741798401\n",
      "Epoch 1969, Loss: 2.787255346775055, Final Batch Loss: 0.5737374424934387\n",
      "Epoch 1970, Loss: 3.0730090141296387, Final Batch Loss: 0.7185963988304138\n",
      "Epoch 1971, Loss: 2.9525111317634583, Final Batch Loss: 0.487487256526947\n",
      "Epoch 1972, Loss: 2.704493820667267, Final Batch Loss: 0.5183978080749512\n",
      "Epoch 1973, Loss: 2.8765824139118195, Final Batch Loss: 0.6584544777870178\n",
      "Epoch 1974, Loss: 2.8745980262756348, Final Batch Loss: 0.6059995293617249\n",
      "Epoch 1975, Loss: 2.898297905921936, Final Batch Loss: 0.6405642628669739\n",
      "Epoch 1976, Loss: 2.8325110971927643, Final Batch Loss: 0.4898262917995453\n",
      "Epoch 1977, Loss: 2.776349127292633, Final Batch Loss: 0.5076760053634644\n",
      "Epoch 1978, Loss: 2.925906479358673, Final Batch Loss: 0.477069616317749\n",
      "Epoch 1979, Loss: 2.8573631644248962, Final Batch Loss: 0.49335893988609314\n",
      "Epoch 1980, Loss: 2.8968818485736847, Final Batch Loss: 0.5693455934524536\n",
      "Epoch 1981, Loss: 3.1251999139785767, Final Batch Loss: 0.723240852355957\n",
      "Epoch 1982, Loss: 3.0576379895210266, Final Batch Loss: 0.6674233675003052\n",
      "Epoch 1983, Loss: 2.83594411611557, Final Batch Loss: 0.5732133388519287\n",
      "Epoch 1984, Loss: 2.7499121129512787, Final Batch Loss: 0.4339907467365265\n",
      "Epoch 1985, Loss: 2.896972954273224, Final Batch Loss: 0.5528780221939087\n",
      "Epoch 1986, Loss: 2.8833439350128174, Final Batch Loss: 0.5919862389564514\n",
      "Epoch 1987, Loss: 2.8665384650230408, Final Batch Loss: 0.6209644675254822\n",
      "Epoch 1988, Loss: 2.9006675481796265, Final Batch Loss: 0.6160904169082642\n",
      "Epoch 1989, Loss: 2.8671208024024963, Final Batch Loss: 0.6146662831306458\n",
      "Epoch 1990, Loss: 2.7318079471588135, Final Batch Loss: 0.5070428252220154\n",
      "Epoch 1991, Loss: 2.9275051653385162, Final Batch Loss: 0.5134879350662231\n",
      "Epoch 1992, Loss: 2.99536395072937, Final Batch Loss: 0.6831623911857605\n",
      "Epoch 1993, Loss: 2.8105100095272064, Final Batch Loss: 0.3986935317516327\n",
      "Epoch 1994, Loss: 2.8469502329826355, Final Batch Loss: 0.6607334613800049\n",
      "Epoch 1995, Loss: 2.9049530029296875, Final Batch Loss: 0.5132607817649841\n",
      "Epoch 1996, Loss: 2.774177134037018, Final Batch Loss: 0.5242311954498291\n",
      "Epoch 1997, Loss: 2.763205200433731, Final Batch Loss: 0.47491681575775146\n",
      "Epoch 1998, Loss: 2.744207739830017, Final Batch Loss: 0.5900220274925232\n",
      "Epoch 1999, Loss: 2.8332590460777283, Final Batch Loss: 0.5203385949134827\n",
      "Epoch 2000, Loss: 2.8164767026901245, Final Batch Loss: 0.5139175653457642\n",
      "Epoch 2001, Loss: 2.814183235168457, Final Batch Loss: 0.6048568487167358\n",
      "Epoch 2002, Loss: 2.8829569220542908, Final Batch Loss: 0.5666429996490479\n",
      "Epoch 2003, Loss: 2.794155716896057, Final Batch Loss: 0.6277894377708435\n",
      "Epoch 2004, Loss: 2.73033806681633, Final Batch Loss: 0.5816948413848877\n",
      "Epoch 2005, Loss: 2.727509528398514, Final Batch Loss: 0.5474269390106201\n",
      "Epoch 2006, Loss: 2.922704339027405, Final Batch Loss: 0.6102274656295776\n",
      "Epoch 2007, Loss: 2.925454080104828, Final Batch Loss: 0.7085223197937012\n",
      "Epoch 2008, Loss: 3.0091816186904907, Final Batch Loss: 0.5788084864616394\n",
      "Epoch 2009, Loss: 2.8899106979370117, Final Batch Loss: 0.6277952790260315\n",
      "Epoch 2010, Loss: 2.8994311690330505, Final Batch Loss: 0.5486466288566589\n",
      "Epoch 2011, Loss: 2.974910855293274, Final Batch Loss: 0.5837502479553223\n",
      "Epoch 2012, Loss: 2.8569491505622864, Final Batch Loss: 0.5187179446220398\n",
      "Epoch 2013, Loss: 2.9357820749282837, Final Batch Loss: 0.6771776676177979\n",
      "Epoch 2014, Loss: 2.8466432690620422, Final Batch Loss: 0.5987286567687988\n",
      "Epoch 2015, Loss: 2.8018817603588104, Final Batch Loss: 0.5549327731132507\n",
      "Epoch 2016, Loss: 3.036137819290161, Final Batch Loss: 0.6374455094337463\n",
      "Epoch 2017, Loss: 2.8604284822940826, Final Batch Loss: 0.6866116523742676\n",
      "Epoch 2018, Loss: 2.9783501625061035, Final Batch Loss: 0.6512490510940552\n",
      "Epoch 2019, Loss: 2.8561932146549225, Final Batch Loss: 0.46448853611946106\n",
      "Epoch 2020, Loss: 2.7834480702877045, Final Batch Loss: 0.4870382845401764\n",
      "Epoch 2021, Loss: 2.898394525051117, Final Batch Loss: 0.6142611503601074\n",
      "Epoch 2022, Loss: 2.8819448947906494, Final Batch Loss: 0.6253471374511719\n",
      "Epoch 2023, Loss: 2.927229553461075, Final Batch Loss: 0.46672484278678894\n",
      "Epoch 2024, Loss: 2.8948721289634705, Final Batch Loss: 0.596440851688385\n",
      "Epoch 2025, Loss: 2.927646219730377, Final Batch Loss: 0.6497654318809509\n",
      "Epoch 2026, Loss: 2.8009839057922363, Final Batch Loss: 0.6120712161064148\n",
      "Epoch 2027, Loss: 2.8197107911109924, Final Batch Loss: 0.5757045149803162\n",
      "Epoch 2028, Loss: 2.893496572971344, Final Batch Loss: 0.542421281337738\n",
      "Epoch 2029, Loss: 2.7935672998428345, Final Batch Loss: 0.47052282094955444\n",
      "Epoch 2030, Loss: 2.8988872170448303, Final Batch Loss: 0.6548882126808167\n",
      "Epoch 2031, Loss: 2.976883977651596, Final Batch Loss: 0.44950607419013977\n",
      "Epoch 2032, Loss: 2.7699187994003296, Final Batch Loss: 0.5510751008987427\n",
      "Epoch 2033, Loss: 2.6055120825767517, Final Batch Loss: 0.42672157287597656\n",
      "Epoch 2034, Loss: 2.7596513628959656, Final Batch Loss: 0.5777401328086853\n",
      "Epoch 2035, Loss: 2.8559072613716125, Final Batch Loss: 0.5818676352500916\n",
      "Epoch 2036, Loss: 2.884280204772949, Final Batch Loss: 0.5458981990814209\n",
      "Epoch 2037, Loss: 2.8202808499336243, Final Batch Loss: 0.5760037899017334\n",
      "Epoch 2038, Loss: 2.885028839111328, Final Batch Loss: 0.5249209403991699\n",
      "Epoch 2039, Loss: 2.893605947494507, Final Batch Loss: 0.5136547088623047\n",
      "Epoch 2040, Loss: 2.903496503829956, Final Batch Loss: 0.582686722278595\n",
      "Epoch 2041, Loss: 3.03905987739563, Final Batch Loss: 0.7337983846664429\n",
      "Epoch 2042, Loss: 2.902470111846924, Final Batch Loss: 0.5216464400291443\n",
      "Epoch 2043, Loss: 2.903626322746277, Final Batch Loss: 0.5687711238861084\n",
      "Epoch 2044, Loss: 2.78331458568573, Final Batch Loss: 0.5955352783203125\n",
      "Epoch 2045, Loss: 2.848274528980255, Final Batch Loss: 0.5382058620452881\n",
      "Epoch 2046, Loss: 2.6270925104618073, Final Batch Loss: 0.5304682850837708\n",
      "Epoch 2047, Loss: 2.8829278349876404, Final Batch Loss: 0.5520585179328918\n",
      "Epoch 2048, Loss: 2.654956668615341, Final Batch Loss: 0.46191975474357605\n",
      "Epoch 2049, Loss: 2.957438111305237, Final Batch Loss: 0.598152220249176\n",
      "Epoch 2050, Loss: 2.820858061313629, Final Batch Loss: 0.5047712922096252\n",
      "Epoch 2051, Loss: 3.0266169905662537, Final Batch Loss: 0.6559298038482666\n",
      "Epoch 2052, Loss: 2.8866825103759766, Final Batch Loss: 0.5676204562187195\n",
      "Epoch 2053, Loss: 2.8959163427352905, Final Batch Loss: 0.6198499202728271\n",
      "Epoch 2054, Loss: 2.8470810055732727, Final Batch Loss: 0.5116081833839417\n",
      "Epoch 2055, Loss: 2.8810046911239624, Final Batch Loss: 0.5885837078094482\n",
      "Epoch 2056, Loss: 2.8169389367103577, Final Batch Loss: 0.5035517811775208\n",
      "Epoch 2057, Loss: 2.706389844417572, Final Batch Loss: 0.5850050449371338\n",
      "Epoch 2058, Loss: 2.703548550605774, Final Batch Loss: 0.536163866519928\n",
      "Epoch 2059, Loss: 2.8685893416404724, Final Batch Loss: 0.6034426093101501\n",
      "Epoch 2060, Loss: 2.788993716239929, Final Batch Loss: 0.5296745300292969\n",
      "Epoch 2061, Loss: 2.7049259543418884, Final Batch Loss: 0.522428572177887\n",
      "Epoch 2062, Loss: 2.695565313100815, Final Batch Loss: 0.48588940501213074\n",
      "Epoch 2063, Loss: 2.975951910018921, Final Batch Loss: 0.621115505695343\n",
      "Epoch 2064, Loss: 2.815370589494705, Final Batch Loss: 0.6937299966812134\n",
      "Epoch 2065, Loss: 2.802752822637558, Final Batch Loss: 0.6141231060028076\n",
      "Epoch 2066, Loss: 3.0517072677612305, Final Batch Loss: 0.6303231716156006\n",
      "Epoch 2067, Loss: 2.7933425903320312, Final Batch Loss: 0.6261029839515686\n",
      "Epoch 2068, Loss: 2.8128839135169983, Final Batch Loss: 0.5038566589355469\n",
      "Epoch 2069, Loss: 2.6350406110286713, Final Batch Loss: 0.565044105052948\n",
      "Epoch 2070, Loss: 2.9035691618919373, Final Batch Loss: 0.5376673936843872\n",
      "Epoch 2071, Loss: 2.989864468574524, Final Batch Loss: 0.7690106630325317\n",
      "Epoch 2072, Loss: 2.8578131198883057, Final Batch Loss: 0.5790193676948547\n",
      "Epoch 2073, Loss: 2.8543328642845154, Final Batch Loss: 0.5681343078613281\n",
      "Epoch 2074, Loss: 2.832103431224823, Final Batch Loss: 0.5137506723403931\n",
      "Epoch 2075, Loss: 2.8981258869171143, Final Batch Loss: 0.5143615007400513\n",
      "Epoch 2076, Loss: 3.137086868286133, Final Batch Loss: 0.867278516292572\n",
      "Epoch 2077, Loss: 2.7429737746715546, Final Batch Loss: 0.5620312094688416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2078, Loss: 2.875987559556961, Final Batch Loss: 0.5302324891090393\n",
      "Epoch 2079, Loss: 2.7377258241176605, Final Batch Loss: 0.6107578873634338\n",
      "Epoch 2080, Loss: 3.079082727432251, Final Batch Loss: 0.5329363346099854\n",
      "Epoch 2081, Loss: 2.760661244392395, Final Batch Loss: 0.5906282663345337\n",
      "Epoch 2082, Loss: 2.7957464456558228, Final Batch Loss: 0.40811586380004883\n",
      "Epoch 2083, Loss: 2.79982990026474, Final Batch Loss: 0.5754391551017761\n",
      "Epoch 2084, Loss: 3.012022852897644, Final Batch Loss: 0.5954837799072266\n",
      "Epoch 2085, Loss: 2.869692087173462, Final Batch Loss: 0.6214979290962219\n",
      "Epoch 2086, Loss: 2.743101477622986, Final Batch Loss: 0.5690826177597046\n",
      "Epoch 2087, Loss: 2.8312858045101166, Final Batch Loss: 0.5674371719360352\n",
      "Epoch 2088, Loss: 3.085144340991974, Final Batch Loss: 0.6048769950866699\n",
      "Epoch 2089, Loss: 2.8686238527297974, Final Batch Loss: 0.5218528509140015\n",
      "Epoch 2090, Loss: 3.137499988079071, Final Batch Loss: 0.7832629680633545\n",
      "Epoch 2091, Loss: 2.6833072304725647, Final Batch Loss: 0.5623998641967773\n",
      "Epoch 2092, Loss: 2.6918519735336304, Final Batch Loss: 0.43662580847740173\n",
      "Epoch 2093, Loss: 2.763744354248047, Final Batch Loss: 0.5786347985267639\n",
      "Epoch 2094, Loss: 2.7144868969917297, Final Batch Loss: 0.5065674185752869\n",
      "Epoch 2095, Loss: 2.733825594186783, Final Batch Loss: 0.4868663251399994\n",
      "Epoch 2096, Loss: 2.850823402404785, Final Batch Loss: 0.5304501056671143\n",
      "Epoch 2097, Loss: 2.788700580596924, Final Batch Loss: 0.53798508644104\n",
      "Epoch 2098, Loss: 2.7938997745513916, Final Batch Loss: 0.5226628184318542\n",
      "Epoch 2099, Loss: 2.9252084493637085, Final Batch Loss: 0.6128591299057007\n",
      "Epoch 2100, Loss: 2.8259828090667725, Final Batch Loss: 0.5610039234161377\n",
      "Epoch 2101, Loss: 2.7938604056835175, Final Batch Loss: 0.46657124161720276\n",
      "Epoch 2102, Loss: 2.7285135686397552, Final Batch Loss: 0.5067275166511536\n",
      "Epoch 2103, Loss: 2.695333331823349, Final Batch Loss: 0.6060351133346558\n",
      "Epoch 2104, Loss: 2.5995349884033203, Final Batch Loss: 0.44747912883758545\n",
      "Epoch 2105, Loss: 2.7618166506290436, Final Batch Loss: 0.45768120884895325\n",
      "Epoch 2106, Loss: 2.671388179063797, Final Batch Loss: 0.478845477104187\n",
      "Epoch 2107, Loss: 2.738543599843979, Final Batch Loss: 0.4342559278011322\n",
      "Epoch 2108, Loss: 2.7495495975017548, Final Batch Loss: 0.544754683971405\n",
      "Epoch 2109, Loss: 2.7833414375782013, Final Batch Loss: 0.48839643597602844\n",
      "Epoch 2110, Loss: 2.7425163984298706, Final Batch Loss: 0.6089434623718262\n",
      "Epoch 2111, Loss: 2.8762444257736206, Final Batch Loss: 0.6835395097732544\n",
      "Epoch 2112, Loss: 2.8531657457351685, Final Batch Loss: 0.6293240189552307\n",
      "Epoch 2113, Loss: 2.882739305496216, Final Batch Loss: 0.6745544672012329\n",
      "Epoch 2114, Loss: 2.8122045397758484, Final Batch Loss: 0.580070972442627\n",
      "Epoch 2115, Loss: 2.8581660985946655, Final Batch Loss: 0.5392961502075195\n",
      "Epoch 2116, Loss: 2.701816439628601, Final Batch Loss: 0.5894251465797424\n",
      "Epoch 2117, Loss: 2.760494649410248, Final Batch Loss: 0.5403139591217041\n",
      "Epoch 2118, Loss: 2.804956167936325, Final Batch Loss: 0.4620443284511566\n",
      "Epoch 2119, Loss: 2.8214974105358124, Final Batch Loss: 0.6830723285675049\n",
      "Epoch 2120, Loss: 2.8001059889793396, Final Batch Loss: 0.5916494131088257\n",
      "Epoch 2121, Loss: 2.774298757314682, Final Batch Loss: 0.585595965385437\n",
      "Epoch 2122, Loss: 2.5848575234413147, Final Batch Loss: 0.5031397938728333\n",
      "Epoch 2123, Loss: 2.7401599287986755, Final Batch Loss: 0.5374144315719604\n",
      "Epoch 2124, Loss: 2.8176629841327667, Final Batch Loss: 0.48042359948158264\n",
      "Epoch 2125, Loss: 2.706145226955414, Final Batch Loss: 0.6216784715652466\n",
      "Epoch 2126, Loss: 2.692244976758957, Final Batch Loss: 0.4903614819049835\n",
      "Epoch 2127, Loss: 2.669094830751419, Final Batch Loss: 0.5560601353645325\n",
      "Epoch 2128, Loss: 3.018523395061493, Final Batch Loss: 0.5626122951507568\n",
      "Epoch 2129, Loss: 2.7640045881271362, Final Batch Loss: 0.4764106869697571\n",
      "Epoch 2130, Loss: 2.6889984607696533, Final Batch Loss: 0.5431132912635803\n",
      "Epoch 2131, Loss: 2.728420227766037, Final Batch Loss: 0.45217180252075195\n",
      "Epoch 2132, Loss: 2.7004282474517822, Final Batch Loss: 0.6028012633323669\n",
      "Epoch 2133, Loss: 2.616542249917984, Final Batch Loss: 0.4656044840812683\n",
      "Epoch 2134, Loss: 3.0262919068336487, Final Batch Loss: 0.6890289187431335\n",
      "Epoch 2135, Loss: 2.711668372154236, Final Batch Loss: 0.5660420060157776\n",
      "Epoch 2136, Loss: 2.609262079000473, Final Batch Loss: 0.416572242975235\n",
      "Epoch 2137, Loss: 2.988597571849823, Final Batch Loss: 0.6393818855285645\n",
      "Epoch 2138, Loss: 2.756594717502594, Final Batch Loss: 0.5183898210525513\n",
      "Epoch 2139, Loss: 2.7617231607437134, Final Batch Loss: 0.5358055233955383\n",
      "Epoch 2140, Loss: 2.817621946334839, Final Batch Loss: 0.515325129032135\n",
      "Epoch 2141, Loss: 2.9703832268714905, Final Batch Loss: 0.62834233045578\n",
      "Epoch 2142, Loss: 2.7204273641109467, Final Batch Loss: 0.5382000803947449\n",
      "Epoch 2143, Loss: 2.9961479902267456, Final Batch Loss: 0.5887850522994995\n",
      "Epoch 2144, Loss: 2.6878252923488617, Final Batch Loss: 0.490457683801651\n",
      "Epoch 2145, Loss: 2.72772878408432, Final Batch Loss: 0.5544135570526123\n",
      "Epoch 2146, Loss: 2.707046151161194, Final Batch Loss: 0.5313562750816345\n",
      "Epoch 2147, Loss: 2.8240714371204376, Final Batch Loss: 0.6601143479347229\n",
      "Epoch 2148, Loss: 2.6990509629249573, Final Batch Loss: 0.5422049760818481\n",
      "Epoch 2149, Loss: 2.7624872624874115, Final Batch Loss: 0.6478682160377502\n",
      "Epoch 2150, Loss: 2.6592011749744415, Final Batch Loss: 0.4630623757839203\n",
      "Epoch 2151, Loss: 2.7994488179683685, Final Batch Loss: 0.62666255235672\n",
      "Epoch 2152, Loss: 2.931839883327484, Final Batch Loss: 0.6783198118209839\n",
      "Epoch 2153, Loss: 2.8952276706695557, Final Batch Loss: 0.6391966938972473\n",
      "Epoch 2154, Loss: 2.8702011704444885, Final Batch Loss: 0.6192087531089783\n",
      "Epoch 2155, Loss: 2.6759012937545776, Final Batch Loss: 0.5504902005195618\n",
      "Epoch 2156, Loss: 2.9132879972457886, Final Batch Loss: 0.6945033073425293\n",
      "Epoch 2157, Loss: 2.7982386350631714, Final Batch Loss: 0.580024242401123\n",
      "Epoch 2158, Loss: 2.90742090344429, Final Batch Loss: 0.4983770549297333\n",
      "Epoch 2159, Loss: 2.7398071885108948, Final Batch Loss: 0.49530476331710815\n",
      "Epoch 2160, Loss: 2.6393536925315857, Final Batch Loss: 0.5032358765602112\n",
      "Epoch 2161, Loss: 2.7228294610977173, Final Batch Loss: 0.5688903331756592\n",
      "Epoch 2162, Loss: 2.764530062675476, Final Batch Loss: 0.5446174144744873\n",
      "Epoch 2163, Loss: 2.6601944267749786, Final Batch Loss: 0.5622504949569702\n",
      "Epoch 2164, Loss: 2.7473279237747192, Final Batch Loss: 0.49677449464797974\n",
      "Epoch 2165, Loss: 2.7159224152565002, Final Batch Loss: 0.504895806312561\n",
      "Epoch 2166, Loss: 2.747543513774872, Final Batch Loss: 0.5954387784004211\n",
      "Epoch 2167, Loss: 2.8897722959518433, Final Batch Loss: 0.5860335230827332\n",
      "Epoch 2168, Loss: 2.897140681743622, Final Batch Loss: 0.5387957096099854\n",
      "Epoch 2169, Loss: 2.7388725578784943, Final Batch Loss: 0.4938410818576813\n",
      "Epoch 2170, Loss: 2.91569185256958, Final Batch Loss: 0.5738966464996338\n",
      "Epoch 2171, Loss: 2.7783122062683105, Final Batch Loss: 0.4853045344352722\n",
      "Epoch 2172, Loss: 2.8062549829483032, Final Batch Loss: 0.5321773886680603\n",
      "Epoch 2173, Loss: 2.811765491962433, Final Batch Loss: 0.5986166596412659\n",
      "Epoch 2174, Loss: 2.7083924412727356, Final Batch Loss: 0.5549549460411072\n",
      "Epoch 2175, Loss: 2.7034107446670532, Final Batch Loss: 0.5828363299369812\n",
      "Epoch 2176, Loss: 2.7350746393203735, Final Batch Loss: 0.5705124139785767\n",
      "Epoch 2177, Loss: 2.961153984069824, Final Batch Loss: 0.7180665731430054\n",
      "Epoch 2178, Loss: 2.6423982977867126, Final Batch Loss: 0.5030071139335632\n",
      "Epoch 2179, Loss: 2.7029810547828674, Final Batch Loss: 0.5580372214317322\n",
      "Epoch 2180, Loss: 2.796825647354126, Final Batch Loss: 0.5869421362876892\n",
      "Epoch 2181, Loss: 2.915526360273361, Final Batch Loss: 0.7330992817878723\n",
      "Epoch 2182, Loss: 2.8716249465942383, Final Batch Loss: 0.622548520565033\n",
      "Epoch 2183, Loss: 2.8851279616355896, Final Batch Loss: 0.5117480754852295\n",
      "Epoch 2184, Loss: 2.81830370426178, Final Batch Loss: 0.5199585556983948\n",
      "Epoch 2185, Loss: 2.8332525491714478, Final Batch Loss: 0.6163485050201416\n",
      "Epoch 2186, Loss: 2.9465901851654053, Final Batch Loss: 0.5034699440002441\n",
      "Epoch 2187, Loss: 2.5825006663799286, Final Batch Loss: 0.4914698600769043\n",
      "Epoch 2188, Loss: 2.7082034945487976, Final Batch Loss: 0.44718581438064575\n",
      "Epoch 2189, Loss: 2.8330468833446503, Final Batch Loss: 0.6827045679092407\n",
      "Epoch 2190, Loss: 2.6700055301189423, Final Batch Loss: 0.48606547713279724\n",
      "Epoch 2191, Loss: 2.6196610629558563, Final Batch Loss: 0.40877988934516907\n",
      "Epoch 2192, Loss: 2.8859131932258606, Final Batch Loss: 0.6041138172149658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2193, Loss: 2.7470435798168182, Final Batch Loss: 0.6693589687347412\n",
      "Epoch 2194, Loss: 2.8041569590568542, Final Batch Loss: 0.6119210124015808\n",
      "Epoch 2195, Loss: 2.7309091091156006, Final Batch Loss: 0.5369779467582703\n",
      "Epoch 2196, Loss: 2.7670241594314575, Final Batch Loss: 0.6333701014518738\n",
      "Epoch 2197, Loss: 2.9318803548812866, Final Batch Loss: 0.6784432530403137\n",
      "Epoch 2198, Loss: 2.7378730177879333, Final Batch Loss: 0.5202277302742004\n",
      "Epoch 2199, Loss: 2.6970570385456085, Final Batch Loss: 0.6155785918235779\n",
      "Epoch 2200, Loss: 2.651737302541733, Final Batch Loss: 0.5249090790748596\n",
      "Epoch 2201, Loss: 2.8409405946731567, Final Batch Loss: 0.6659971475601196\n",
      "Epoch 2202, Loss: 2.7050638794898987, Final Batch Loss: 0.5667834877967834\n",
      "Epoch 2203, Loss: 2.896878182888031, Final Batch Loss: 0.5594679117202759\n",
      "Epoch 2204, Loss: 2.8281471729278564, Final Batch Loss: 0.5739955306053162\n",
      "Epoch 2205, Loss: 2.9973548650741577, Final Batch Loss: 0.7177666425704956\n",
      "Epoch 2206, Loss: 2.6969967782497406, Final Batch Loss: 0.5352197289466858\n",
      "Epoch 2207, Loss: 2.630694270133972, Final Batch Loss: 0.43140679597854614\n",
      "Epoch 2208, Loss: 2.810944616794586, Final Batch Loss: 0.5486247539520264\n",
      "Epoch 2209, Loss: 2.91848686337471, Final Batch Loss: 0.6561673879623413\n",
      "Epoch 2210, Loss: 2.8182906210422516, Final Batch Loss: 0.5645084977149963\n",
      "Epoch 2211, Loss: 2.762635290622711, Final Batch Loss: 0.5115100741386414\n",
      "Epoch 2212, Loss: 2.844822585582733, Final Batch Loss: 0.6292654871940613\n",
      "Epoch 2213, Loss: 2.7626618146896362, Final Batch Loss: 0.5017940402030945\n",
      "Epoch 2214, Loss: 2.664222627878189, Final Batch Loss: 0.4384632110595703\n",
      "Epoch 2215, Loss: 2.7442866563796997, Final Batch Loss: 0.6163156628608704\n",
      "Epoch 2216, Loss: 2.7859209179878235, Final Batch Loss: 0.4806605875492096\n",
      "Epoch 2217, Loss: 2.8424333333969116, Final Batch Loss: 0.5565404891967773\n",
      "Epoch 2218, Loss: 2.8152390718460083, Final Batch Loss: 0.5713896155357361\n",
      "Epoch 2219, Loss: 2.867918163537979, Final Batch Loss: 0.7051613926887512\n",
      "Epoch 2220, Loss: 2.770040363073349, Final Batch Loss: 0.5615639686584473\n",
      "Epoch 2221, Loss: 2.7268693149089813, Final Batch Loss: 0.5753245949745178\n",
      "Epoch 2222, Loss: 2.795141279697418, Final Batch Loss: 0.5694122314453125\n",
      "Epoch 2223, Loss: 2.64883029460907, Final Batch Loss: 0.5141686201095581\n",
      "Epoch 2224, Loss: 2.90124174952507, Final Batch Loss: 0.6432965993881226\n",
      "Epoch 2225, Loss: 2.9128754436969757, Final Batch Loss: 0.7043434977531433\n",
      "Epoch 2226, Loss: 2.8555416464805603, Final Batch Loss: 0.4770280122756958\n",
      "Epoch 2227, Loss: 2.5959722697734833, Final Batch Loss: 0.6084457039833069\n",
      "Epoch 2228, Loss: 2.814734846353531, Final Batch Loss: 0.6144669651985168\n",
      "Epoch 2229, Loss: 2.6569152772426605, Final Batch Loss: 0.41900697350502014\n",
      "Epoch 2230, Loss: 2.773046374320984, Final Batch Loss: 0.5420483350753784\n",
      "Epoch 2231, Loss: 2.8321762084960938, Final Batch Loss: 0.5608319044113159\n",
      "Epoch 2232, Loss: 2.7263630628585815, Final Batch Loss: 0.4442622661590576\n",
      "Epoch 2233, Loss: 2.6554530262947083, Final Batch Loss: 0.4312567114830017\n",
      "Epoch 2234, Loss: 2.618037700653076, Final Batch Loss: 0.4477695822715759\n",
      "Epoch 2235, Loss: 2.697263687849045, Final Batch Loss: 0.4938640296459198\n",
      "Epoch 2236, Loss: 2.726426661014557, Final Batch Loss: 0.6459569334983826\n",
      "Epoch 2237, Loss: 2.6955898106098175, Final Batch Loss: 0.5353889465332031\n",
      "Epoch 2238, Loss: 2.9971603453159332, Final Batch Loss: 0.5707247257232666\n",
      "Epoch 2239, Loss: 2.82875519990921, Final Batch Loss: 0.642638623714447\n",
      "Epoch 2240, Loss: 2.725101500749588, Final Batch Loss: 0.5541343688964844\n",
      "Epoch 2241, Loss: 2.717184454202652, Final Batch Loss: 0.5082974433898926\n",
      "Epoch 2242, Loss: 2.7604973018169403, Final Batch Loss: 0.6085684895515442\n",
      "Epoch 2243, Loss: 2.7872160375118256, Final Batch Loss: 0.6312071681022644\n",
      "Epoch 2244, Loss: 2.7346970438957214, Final Batch Loss: 0.5859974026679993\n",
      "Epoch 2245, Loss: 2.844466745853424, Final Batch Loss: 0.5923953652381897\n",
      "Epoch 2246, Loss: 2.8819169402122498, Final Batch Loss: 0.6293566823005676\n",
      "Epoch 2247, Loss: 2.822116047143936, Final Batch Loss: 0.5753297209739685\n",
      "Epoch 2248, Loss: 2.6817551255226135, Final Batch Loss: 0.6692050099372864\n",
      "Epoch 2249, Loss: 2.6648024916648865, Final Batch Loss: 0.533444344997406\n",
      "Epoch 2250, Loss: 2.7145759761333466, Final Batch Loss: 0.7073942422866821\n",
      "Epoch 2251, Loss: 2.6848700046539307, Final Batch Loss: 0.5230289697647095\n",
      "Epoch 2252, Loss: 2.930167555809021, Final Batch Loss: 0.6953219771385193\n",
      "Epoch 2253, Loss: 2.6974304914474487, Final Batch Loss: 0.5339498519897461\n",
      "Epoch 2254, Loss: 2.875180661678314, Final Batch Loss: 0.6115742921829224\n",
      "Epoch 2255, Loss: 2.7382732629776, Final Batch Loss: 0.49756526947021484\n",
      "Epoch 2256, Loss: 2.618858277797699, Final Batch Loss: 0.5525892376899719\n",
      "Epoch 2257, Loss: 2.8235930800437927, Final Batch Loss: 0.6265672445297241\n",
      "Epoch 2258, Loss: 2.7694579660892487, Final Batch Loss: 0.6123361587524414\n",
      "Epoch 2259, Loss: 3.1927725076675415, Final Batch Loss: 0.6854714751243591\n",
      "Epoch 2260, Loss: 2.910504460334778, Final Batch Loss: 0.5062585473060608\n",
      "Epoch 2261, Loss: 2.626844048500061, Final Batch Loss: 0.5278373956680298\n",
      "Epoch 2262, Loss: 2.981109857559204, Final Batch Loss: 0.6764324903488159\n",
      "Epoch 2263, Loss: 2.911562740802765, Final Batch Loss: 0.5573330521583557\n",
      "Epoch 2264, Loss: 3.0763765573501587, Final Batch Loss: 0.6449275016784668\n",
      "Epoch 2265, Loss: 2.772506594657898, Final Batch Loss: 0.5311218500137329\n",
      "Epoch 2266, Loss: 2.7675729393959045, Final Batch Loss: 0.5384343266487122\n",
      "Epoch 2267, Loss: 2.7263281643390656, Final Batch Loss: 0.50059574842453\n",
      "Epoch 2268, Loss: 2.860199511051178, Final Batch Loss: 0.5163859128952026\n",
      "Epoch 2269, Loss: 2.818920135498047, Final Batch Loss: 0.5283029675483704\n",
      "Epoch 2270, Loss: 2.826518088579178, Final Batch Loss: 0.5573491454124451\n",
      "Epoch 2271, Loss: 2.6498651802539825, Final Batch Loss: 0.47933876514434814\n",
      "Epoch 2272, Loss: 2.762639880180359, Final Batch Loss: 0.565066397190094\n",
      "Epoch 2273, Loss: 2.5986076295375824, Final Batch Loss: 0.45000210404396057\n",
      "Epoch 2274, Loss: 2.576391637325287, Final Batch Loss: 0.48589572310447693\n",
      "Epoch 2275, Loss: 2.965082347393036, Final Batch Loss: 0.5266395807266235\n",
      "Epoch 2276, Loss: 2.723123073577881, Final Batch Loss: 0.5572951436042786\n",
      "Epoch 2277, Loss: 2.72678679227829, Final Batch Loss: 0.5460247993469238\n",
      "Epoch 2278, Loss: 2.8831613063812256, Final Batch Loss: 0.494035542011261\n",
      "Epoch 2279, Loss: 2.819573938846588, Final Batch Loss: 0.5220335721969604\n",
      "Epoch 2280, Loss: 2.9258646070957184, Final Batch Loss: 0.5777553915977478\n",
      "Epoch 2281, Loss: 2.6145597994327545, Final Batch Loss: 0.40801873803138733\n",
      "Epoch 2282, Loss: 2.703775465488434, Final Batch Loss: 0.47485804557800293\n",
      "Epoch 2283, Loss: 2.644427478313446, Final Batch Loss: 0.416359007358551\n",
      "Epoch 2284, Loss: 2.8229572474956512, Final Batch Loss: 0.6111860871315002\n",
      "Epoch 2285, Loss: 2.7873942255973816, Final Batch Loss: 0.6259874105453491\n",
      "Epoch 2286, Loss: 2.849912464618683, Final Batch Loss: 0.5100790858268738\n",
      "Epoch 2287, Loss: 2.686450958251953, Final Batch Loss: 0.4502002000808716\n",
      "Epoch 2288, Loss: 2.708700031042099, Final Batch Loss: 0.5494731068611145\n",
      "Epoch 2289, Loss: 2.6867401897907257, Final Batch Loss: 0.6623706221580505\n",
      "Epoch 2290, Loss: 2.7527709305286407, Final Batch Loss: 0.565144419670105\n",
      "Epoch 2291, Loss: 2.7299615144729614, Final Batch Loss: 0.5346444845199585\n",
      "Epoch 2292, Loss: 2.7180046439170837, Final Batch Loss: 0.5397875308990479\n",
      "Epoch 2293, Loss: 3.1356621384620667, Final Batch Loss: 0.6859480738639832\n",
      "Epoch 2294, Loss: 2.7704393565654755, Final Batch Loss: 0.5328164100646973\n",
      "Epoch 2295, Loss: 2.678414136171341, Final Batch Loss: 0.4844435155391693\n",
      "Epoch 2296, Loss: 2.7438453435897827, Final Batch Loss: 0.5643026828765869\n",
      "Epoch 2297, Loss: 2.6514807641506195, Final Batch Loss: 0.4839004576206207\n",
      "Epoch 2298, Loss: 2.6323348581790924, Final Batch Loss: 0.5129053592681885\n",
      "Epoch 2299, Loss: 2.7514569759368896, Final Batch Loss: 0.6243835091590881\n",
      "Epoch 2300, Loss: 2.7719278037548065, Final Batch Loss: 0.5499874949455261\n",
      "Epoch 2301, Loss: 2.8831220269203186, Final Batch Loss: 0.5161203742027283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2302, Loss: 2.8095192909240723, Final Batch Loss: 0.582426130771637\n",
      "Epoch 2303, Loss: 2.6741850078105927, Final Batch Loss: 0.6085427403450012\n",
      "Epoch 2304, Loss: 2.6858768463134766, Final Batch Loss: 0.4772465229034424\n",
      "Epoch 2305, Loss: 2.660455882549286, Final Batch Loss: 0.4169763922691345\n",
      "Epoch 2306, Loss: 2.6250224113464355, Final Batch Loss: 0.414895623922348\n",
      "Epoch 2307, Loss: 2.783419042825699, Final Batch Loss: 0.5467650294303894\n",
      "Epoch 2308, Loss: 2.674375891685486, Final Batch Loss: 0.6053615808486938\n",
      "Epoch 2309, Loss: 2.6224708557128906, Final Batch Loss: 0.4907176196575165\n",
      "Epoch 2310, Loss: 2.8004541993141174, Final Batch Loss: 0.5564904808998108\n",
      "Epoch 2311, Loss: 2.7246203422546387, Final Batch Loss: 0.5275769829750061\n",
      "Epoch 2312, Loss: 2.675505220890045, Final Batch Loss: 0.45259594917297363\n",
      "Epoch 2313, Loss: 2.61831197142601, Final Batch Loss: 0.48397600650787354\n",
      "Epoch 2314, Loss: 2.7921406626701355, Final Batch Loss: 0.5848203301429749\n",
      "Epoch 2315, Loss: 2.7936630845069885, Final Batch Loss: 0.5364367365837097\n",
      "Epoch 2316, Loss: 2.6648354530334473, Final Batch Loss: 0.4473998546600342\n",
      "Epoch 2317, Loss: 2.646137624979019, Final Batch Loss: 0.5212565660476685\n",
      "Epoch 2318, Loss: 2.6783266365528107, Final Batch Loss: 0.5209287405014038\n",
      "Epoch 2319, Loss: 2.667342960834503, Final Batch Loss: 0.45268023014068604\n",
      "Epoch 2320, Loss: 2.72558730840683, Final Batch Loss: 0.4859377145767212\n",
      "Epoch 2321, Loss: 2.828213095664978, Final Batch Loss: 0.5818364024162292\n",
      "Epoch 2322, Loss: 2.67935711145401, Final Batch Loss: 0.45945581793785095\n",
      "Epoch 2323, Loss: 2.7137627601623535, Final Batch Loss: 0.595265805721283\n",
      "Epoch 2324, Loss: 2.691327750682831, Final Batch Loss: 0.4657517075538635\n",
      "Epoch 2325, Loss: 2.6720023453235626, Final Batch Loss: 0.5998848676681519\n",
      "Epoch 2326, Loss: 2.9286102652549744, Final Batch Loss: 0.61567622423172\n",
      "Epoch 2327, Loss: 3.053504168987274, Final Batch Loss: 0.5929750204086304\n",
      "Epoch 2328, Loss: 2.747655004262924, Final Batch Loss: 0.5682927370071411\n",
      "Epoch 2329, Loss: 2.8102903962135315, Final Batch Loss: 0.5782743096351624\n",
      "Epoch 2330, Loss: 2.8711191415786743, Final Batch Loss: 0.48617023229599\n",
      "Epoch 2331, Loss: 2.6799225211143494, Final Batch Loss: 0.5143759846687317\n",
      "Epoch 2332, Loss: 2.6753568947315216, Final Batch Loss: 0.4529440104961395\n",
      "Epoch 2333, Loss: 2.803749054670334, Final Batch Loss: 0.5171296000480652\n",
      "Epoch 2334, Loss: 2.8246079087257385, Final Batch Loss: 0.5687355995178223\n",
      "Epoch 2335, Loss: 2.932845115661621, Final Batch Loss: 0.722506582736969\n",
      "Epoch 2336, Loss: 2.76810559630394, Final Batch Loss: 0.5347317457199097\n",
      "Epoch 2337, Loss: 2.7282799184322357, Final Batch Loss: 0.4730037748813629\n",
      "Epoch 2338, Loss: 2.690402179956436, Final Batch Loss: 0.48118624091148376\n",
      "Epoch 2339, Loss: 2.8635056018829346, Final Batch Loss: 0.5503979921340942\n",
      "Epoch 2340, Loss: 3.048028886318207, Final Batch Loss: 0.6007159948348999\n",
      "Epoch 2341, Loss: 2.7196734249591827, Final Batch Loss: 0.5895516276359558\n",
      "Epoch 2342, Loss: 2.736640602350235, Final Batch Loss: 0.49602261185646057\n",
      "Epoch 2343, Loss: 2.670131117105484, Final Batch Loss: 0.570612907409668\n",
      "Epoch 2344, Loss: 2.7557954490184784, Final Batch Loss: 0.6025664806365967\n",
      "Epoch 2345, Loss: 2.766109436750412, Final Batch Loss: 0.5205126404762268\n",
      "Epoch 2346, Loss: 2.631061375141144, Final Batch Loss: 0.6041791439056396\n",
      "Epoch 2347, Loss: 2.7115049064159393, Final Batch Loss: 0.5719392895698547\n",
      "Epoch 2348, Loss: 2.564700961112976, Final Batch Loss: 0.5287661552429199\n",
      "Epoch 2349, Loss: 2.7889348566532135, Final Batch Loss: 0.5999400615692139\n",
      "Epoch 2350, Loss: 2.686445266008377, Final Batch Loss: 0.5426726341247559\n",
      "Epoch 2351, Loss: 2.6547887325286865, Final Batch Loss: 0.5653343796730042\n",
      "Epoch 2352, Loss: 2.803824335336685, Final Batch Loss: 0.6501905918121338\n",
      "Epoch 2353, Loss: 2.82860666513443, Final Batch Loss: 0.5525667667388916\n",
      "Epoch 2354, Loss: 2.809242844581604, Final Batch Loss: 0.6167552471160889\n",
      "Epoch 2355, Loss: 2.7164806127548218, Final Batch Loss: 0.5045291781425476\n",
      "Epoch 2356, Loss: 2.7002789676189423, Final Batch Loss: 0.5718780755996704\n",
      "Epoch 2357, Loss: 2.671827018260956, Final Batch Loss: 0.499267041683197\n",
      "Epoch 2358, Loss: 2.802317798137665, Final Batch Loss: 0.6419004797935486\n",
      "Epoch 2359, Loss: 2.8586556911468506, Final Batch Loss: 0.5654057860374451\n",
      "Epoch 2360, Loss: 2.5682332813739777, Final Batch Loss: 0.4288341701030731\n",
      "Epoch 2361, Loss: 2.8325271010398865, Final Batch Loss: 0.567796528339386\n",
      "Epoch 2362, Loss: 2.6297619938850403, Final Batch Loss: 0.6159292459487915\n",
      "Epoch 2363, Loss: 2.790369302034378, Final Batch Loss: 0.5188361406326294\n",
      "Epoch 2364, Loss: 2.610445648431778, Final Batch Loss: 0.41367432475090027\n",
      "Epoch 2365, Loss: 2.6058800518512726, Final Batch Loss: 0.5659942030906677\n",
      "Epoch 2366, Loss: 2.5827536582946777, Final Batch Loss: 0.5384409427642822\n",
      "Epoch 2367, Loss: 2.7245728373527527, Final Batch Loss: 0.5805789232254028\n",
      "Epoch 2368, Loss: 2.594419777393341, Final Batch Loss: 0.5182642936706543\n",
      "Epoch 2369, Loss: 2.8099284172058105, Final Batch Loss: 0.6086285710334778\n",
      "Epoch 2370, Loss: 2.7409330010414124, Final Batch Loss: 0.48604637384414673\n",
      "Epoch 2371, Loss: 2.5959772765636444, Final Batch Loss: 0.48401474952697754\n",
      "Epoch 2372, Loss: 2.8366608023643494, Final Batch Loss: 0.622105598449707\n",
      "Epoch 2373, Loss: 2.6158660650253296, Final Batch Loss: 0.5064159035682678\n",
      "Epoch 2374, Loss: 2.9288507997989655, Final Batch Loss: 0.5776860117912292\n",
      "Epoch 2375, Loss: 2.799072265625, Final Batch Loss: 0.5616879463195801\n",
      "Epoch 2376, Loss: 2.700083702802658, Final Batch Loss: 0.4999556839466095\n",
      "Epoch 2377, Loss: 2.6820425391197205, Final Batch Loss: 0.5017683506011963\n",
      "Epoch 2378, Loss: 2.7840322852134705, Final Batch Loss: 0.5985375046730042\n",
      "Epoch 2379, Loss: 2.7689415216445923, Final Batch Loss: 0.5017848014831543\n",
      "Epoch 2380, Loss: 2.704235941171646, Final Batch Loss: 0.49879613518714905\n",
      "Epoch 2381, Loss: 2.7447426319122314, Final Batch Loss: 0.4372023940086365\n",
      "Epoch 2382, Loss: 2.671678066253662, Final Batch Loss: 0.5287186503410339\n",
      "Epoch 2383, Loss: 2.7211695313453674, Final Batch Loss: 0.5605449080467224\n",
      "Epoch 2384, Loss: 2.6342568397521973, Final Batch Loss: 0.5588042140007019\n",
      "Epoch 2385, Loss: 2.746211588382721, Final Batch Loss: 0.6364269256591797\n",
      "Epoch 2386, Loss: 2.535110503435135, Final Batch Loss: 0.4916402995586395\n",
      "Epoch 2387, Loss: 2.6677149534225464, Final Batch Loss: 0.6304055452346802\n",
      "Epoch 2388, Loss: 2.891597867012024, Final Batch Loss: 0.6058117151260376\n",
      "Epoch 2389, Loss: 2.761719733476639, Final Batch Loss: 0.46146735548973083\n",
      "Epoch 2390, Loss: 2.7705841064453125, Final Batch Loss: 0.47791749238967896\n",
      "Epoch 2391, Loss: 2.8164493441581726, Final Batch Loss: 0.5738810300827026\n",
      "Epoch 2392, Loss: 2.6319443583488464, Final Batch Loss: 0.465454638004303\n",
      "Epoch 2393, Loss: 2.6176129281520844, Final Batch Loss: 0.5351586937904358\n",
      "Epoch 2394, Loss: 2.730096250772476, Final Batch Loss: 0.6193131804466248\n",
      "Epoch 2395, Loss: 2.801948130130768, Final Batch Loss: 0.6149961948394775\n",
      "Epoch 2396, Loss: 2.765499323606491, Final Batch Loss: 0.5577876567840576\n",
      "Epoch 2397, Loss: 2.632606416940689, Final Batch Loss: 0.636629581451416\n",
      "Epoch 2398, Loss: 2.7028555274009705, Final Batch Loss: 0.52649986743927\n",
      "Epoch 2399, Loss: 2.7244066298007965, Final Batch Loss: 0.4698035418987274\n",
      "Epoch 2400, Loss: 2.751462757587433, Final Batch Loss: 0.535962700843811\n",
      "Epoch 2401, Loss: 2.6851887106895447, Final Batch Loss: 0.5155584812164307\n",
      "Epoch 2402, Loss: 2.7405581772327423, Final Batch Loss: 0.5376983284950256\n",
      "Epoch 2403, Loss: 2.651779443025589, Final Batch Loss: 0.4058469831943512\n",
      "Epoch 2404, Loss: 2.8347890377044678, Final Batch Loss: 0.5861446261405945\n",
      "Epoch 2405, Loss: 2.6310962438583374, Final Batch Loss: 0.5948916077613831\n",
      "Epoch 2406, Loss: 2.657069206237793, Final Batch Loss: 0.5416208505630493\n",
      "Epoch 2407, Loss: 2.6747102439403534, Final Batch Loss: 0.643882155418396\n",
      "Epoch 2408, Loss: 2.869937688112259, Final Batch Loss: 0.49762651324272156\n",
      "Epoch 2409, Loss: 2.565911650657654, Final Batch Loss: 0.4683731198310852\n",
      "Epoch 2410, Loss: 2.601474404335022, Final Batch Loss: 0.4290342330932617\n",
      "Epoch 2411, Loss: 2.5677455961704254, Final Batch Loss: 0.41950684785842896\n",
      "Epoch 2412, Loss: 2.836275041103363, Final Batch Loss: 0.509954035282135\n",
      "Epoch 2413, Loss: 2.7462480664253235, Final Batch Loss: 0.5198011994361877\n",
      "Epoch 2414, Loss: 2.7006718516349792, Final Batch Loss: 0.48186224699020386\n",
      "Epoch 2415, Loss: 2.9661779403686523, Final Batch Loss: 0.6775888800621033\n",
      "Epoch 2416, Loss: 2.760201632976532, Final Batch Loss: 0.6083559989929199\n",
      "Epoch 2417, Loss: 2.829008400440216, Final Batch Loss: 0.5608639717102051\n",
      "Epoch 2418, Loss: 2.762837678194046, Final Batch Loss: 0.6578598022460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2419, Loss: 2.564243584871292, Final Batch Loss: 0.5681042075157166\n",
      "Epoch 2420, Loss: 2.7117393016815186, Final Batch Loss: 0.4683775305747986\n",
      "Epoch 2421, Loss: 2.7635450065135956, Final Batch Loss: 0.5673798322677612\n",
      "Epoch 2422, Loss: 2.601349323987961, Final Batch Loss: 0.42623355984687805\n",
      "Epoch 2423, Loss: 2.6723190546035767, Final Batch Loss: 0.4958544373512268\n",
      "Epoch 2424, Loss: 2.6891866326332092, Final Batch Loss: 0.49633437395095825\n",
      "Epoch 2425, Loss: 2.6054754853248596, Final Batch Loss: 0.42808613181114197\n",
      "Epoch 2426, Loss: 2.7258308231830597, Final Batch Loss: 0.5055927038192749\n",
      "Epoch 2427, Loss: 2.7280623018741608, Final Batch Loss: 0.49960947036743164\n",
      "Epoch 2428, Loss: 2.5853691697120667, Final Batch Loss: 0.5206888318061829\n",
      "Epoch 2429, Loss: 2.6916715502738953, Final Batch Loss: 0.5806238651275635\n",
      "Epoch 2430, Loss: 2.811601370573044, Final Batch Loss: 0.5857939124107361\n",
      "Epoch 2431, Loss: 2.8768462240695953, Final Batch Loss: 0.7280917763710022\n",
      "Epoch 2432, Loss: 2.7182891070842743, Final Batch Loss: 0.5671133399009705\n",
      "Epoch 2433, Loss: 2.752546340227127, Final Batch Loss: 0.54169100522995\n",
      "Epoch 2434, Loss: 2.699903428554535, Final Batch Loss: 0.5707444548606873\n",
      "Epoch 2435, Loss: 2.773742437362671, Final Batch Loss: 0.5644975900650024\n",
      "Epoch 2436, Loss: 2.7114959955215454, Final Batch Loss: 0.5490537881851196\n",
      "Epoch 2437, Loss: 2.504031926393509, Final Batch Loss: 0.5129969716072083\n",
      "Epoch 2438, Loss: 2.6504137217998505, Final Batch Loss: 0.5253744125366211\n",
      "Epoch 2439, Loss: 2.5593142807483673, Final Batch Loss: 0.4421031177043915\n",
      "Epoch 2440, Loss: 2.674724370241165, Final Batch Loss: 0.46499064564704895\n",
      "Epoch 2441, Loss: 3.0037249624729156, Final Batch Loss: 0.6053768992424011\n",
      "Epoch 2442, Loss: 2.6769899725914, Final Batch Loss: 0.6204761266708374\n",
      "Epoch 2443, Loss: 2.560356557369232, Final Batch Loss: 0.42310017347335815\n",
      "Epoch 2444, Loss: 2.742304176092148, Final Batch Loss: 0.45340225100517273\n",
      "Epoch 2445, Loss: 2.6950569450855255, Final Batch Loss: 0.5413850545883179\n",
      "Epoch 2446, Loss: 2.477815181016922, Final Batch Loss: 0.34776997566223145\n",
      "Epoch 2447, Loss: 2.5800141394138336, Final Batch Loss: 0.5055499076843262\n",
      "Epoch 2448, Loss: 2.6110917031764984, Final Batch Loss: 0.48741480708122253\n",
      "Epoch 2449, Loss: 2.7025320529937744, Final Batch Loss: 0.6112391948699951\n",
      "Epoch 2450, Loss: 2.6682752668857574, Final Batch Loss: 0.5217296481132507\n",
      "Epoch 2451, Loss: 2.742413878440857, Final Batch Loss: 0.5286625027656555\n",
      "Epoch 2452, Loss: 2.582821100950241, Final Batch Loss: 0.47474250197410583\n",
      "Epoch 2453, Loss: 2.6898563504219055, Final Batch Loss: 0.593344509601593\n",
      "Epoch 2454, Loss: 2.5644718408584595, Final Batch Loss: 0.5888903141021729\n",
      "Epoch 2455, Loss: 2.845172107219696, Final Batch Loss: 0.5949537754058838\n",
      "Epoch 2456, Loss: 2.6697471737861633, Final Batch Loss: 0.4547881484031677\n",
      "Epoch 2457, Loss: 2.535125285387039, Final Batch Loss: 0.5413842797279358\n",
      "Epoch 2458, Loss: 2.8888121843338013, Final Batch Loss: 0.7262324094772339\n",
      "Epoch 2459, Loss: 2.723728269338608, Final Batch Loss: 0.4935995638370514\n",
      "Epoch 2460, Loss: 2.7722229063510895, Final Batch Loss: 0.48418840765953064\n",
      "Epoch 2461, Loss: 2.7752034664154053, Final Batch Loss: 0.5098004341125488\n",
      "Epoch 2462, Loss: 2.5685358345508575, Final Batch Loss: 0.47880318760871887\n",
      "Epoch 2463, Loss: 2.786608397960663, Final Batch Loss: 0.6063220500946045\n",
      "Epoch 2464, Loss: 2.4712491631507874, Final Batch Loss: 0.47738972306251526\n",
      "Epoch 2465, Loss: 2.8089478611946106, Final Batch Loss: 0.511715292930603\n",
      "Epoch 2466, Loss: 2.7975999414920807, Final Batch Loss: 0.5808016657829285\n",
      "Epoch 2467, Loss: 2.7377644777297974, Final Batch Loss: 0.5662484169006348\n",
      "Epoch 2468, Loss: 2.719009518623352, Final Batch Loss: 0.4802219867706299\n",
      "Epoch 2469, Loss: 2.804437279701233, Final Batch Loss: 0.49140113592147827\n",
      "Epoch 2470, Loss: 2.604853332042694, Final Batch Loss: 0.47069570422172546\n",
      "Epoch 2471, Loss: 2.7329439222812653, Final Batch Loss: 0.5106200575828552\n",
      "Epoch 2472, Loss: 2.7633448243141174, Final Batch Loss: 0.5809173583984375\n",
      "Epoch 2473, Loss: 2.644980251789093, Final Batch Loss: 0.6814177632331848\n",
      "Epoch 2474, Loss: 2.5736102759838104, Final Batch Loss: 0.37717220187187195\n",
      "Epoch 2475, Loss: 2.5708523094654083, Final Batch Loss: 0.4222915470600128\n",
      "Epoch 2476, Loss: 2.662920504808426, Final Batch Loss: 0.5477813482284546\n",
      "Epoch 2477, Loss: 2.792170524597168, Final Batch Loss: 0.543778657913208\n",
      "Epoch 2478, Loss: 2.850525915622711, Final Batch Loss: 0.6755549907684326\n",
      "Epoch 2479, Loss: 2.633932262659073, Final Batch Loss: 0.4923175275325775\n",
      "Epoch 2480, Loss: 2.590843379497528, Final Batch Loss: 0.43328824639320374\n",
      "Epoch 2481, Loss: 2.6057726740837097, Final Batch Loss: 0.6944212317466736\n",
      "Epoch 2482, Loss: 2.577151834964752, Final Batch Loss: 0.48488664627075195\n",
      "Epoch 2483, Loss: 2.6523969769477844, Final Batch Loss: 0.492371141910553\n",
      "Epoch 2484, Loss: 2.7498663663864136, Final Batch Loss: 0.43610119819641113\n",
      "Epoch 2485, Loss: 2.731319099664688, Final Batch Loss: 0.6165176630020142\n",
      "Epoch 2486, Loss: 2.8292976915836334, Final Batch Loss: 0.490446001291275\n",
      "Epoch 2487, Loss: 2.5464488863945007, Final Batch Loss: 0.5595718622207642\n",
      "Epoch 2488, Loss: 2.90651935338974, Final Batch Loss: 0.6130210161209106\n",
      "Epoch 2489, Loss: 2.6774602234363556, Final Batch Loss: 0.5775101184844971\n",
      "Epoch 2490, Loss: 2.6996575593948364, Final Batch Loss: 0.6604125499725342\n",
      "Epoch 2491, Loss: 2.7947526276111603, Final Batch Loss: 0.5897563695907593\n",
      "Epoch 2492, Loss: 2.7397347390651703, Final Batch Loss: 0.4746357500553131\n",
      "Epoch 2493, Loss: 2.531552493572235, Final Batch Loss: 0.3865520656108856\n",
      "Epoch 2494, Loss: 2.6550553143024445, Final Batch Loss: 0.4560341536998749\n",
      "Epoch 2495, Loss: 2.430930495262146, Final Batch Loss: 0.48207998275756836\n",
      "Epoch 2496, Loss: 2.7181240618228912, Final Batch Loss: 0.5313220620155334\n",
      "Epoch 2497, Loss: 2.6124545335769653, Final Batch Loss: 0.46288442611694336\n",
      "Epoch 2498, Loss: 2.619512230157852, Final Batch Loss: 0.46791282296180725\n",
      "Epoch 2499, Loss: 2.6882967948913574, Final Batch Loss: 0.5140529274940491\n",
      "Epoch 2500, Loss: 2.6778195798397064, Final Batch Loss: 0.6730359196662903\n",
      "Epoch 2501, Loss: 2.635230153799057, Final Batch Loss: 0.49067118763923645\n",
      "Epoch 2502, Loss: 2.765553295612335, Final Batch Loss: 0.8234293460845947\n",
      "Epoch 2503, Loss: 2.6053777039051056, Final Batch Loss: 0.5687984228134155\n",
      "Epoch 2504, Loss: 2.673200398683548, Final Batch Loss: 0.5521727800369263\n",
      "Epoch 2505, Loss: 2.8205163180828094, Final Batch Loss: 0.575053870677948\n",
      "Epoch 2506, Loss: 2.622814953327179, Final Batch Loss: 0.45502787828445435\n",
      "Epoch 2507, Loss: 2.622903287410736, Final Batch Loss: 0.5605064630508423\n",
      "Epoch 2508, Loss: 2.854196012020111, Final Batch Loss: 0.536726176738739\n",
      "Epoch 2509, Loss: 2.6750782430171967, Final Batch Loss: 0.5780586004257202\n",
      "Epoch 2510, Loss: 2.612247556447983, Final Batch Loss: 0.5169299244880676\n",
      "Epoch 2511, Loss: 2.6465066969394684, Final Batch Loss: 0.4736614227294922\n",
      "Epoch 2512, Loss: 2.6529873311519623, Final Batch Loss: 0.5875521302223206\n",
      "Epoch 2513, Loss: 2.666139245033264, Final Batch Loss: 0.5116100311279297\n",
      "Epoch 2514, Loss: 2.717114597558975, Final Batch Loss: 0.4955940544605255\n",
      "Epoch 2515, Loss: 2.810914397239685, Final Batch Loss: 0.6319120526313782\n",
      "Epoch 2516, Loss: 2.9380192160606384, Final Batch Loss: 0.49928969144821167\n",
      "Epoch 2517, Loss: 2.777920752763748, Final Batch Loss: 0.6044319272041321\n",
      "Epoch 2518, Loss: 2.7071526050567627, Final Batch Loss: 0.4744367301464081\n",
      "Epoch 2519, Loss: 2.6332682371139526, Final Batch Loss: 0.453120619058609\n",
      "Epoch 2520, Loss: 2.722134441137314, Final Batch Loss: 0.602912962436676\n",
      "Epoch 2521, Loss: 2.5423977077007294, Final Batch Loss: 0.48809465765953064\n",
      "Epoch 2522, Loss: 2.6844361424446106, Final Batch Loss: 0.5278337597846985\n",
      "Epoch 2523, Loss: 2.738917827606201, Final Batch Loss: 0.6023399233818054\n",
      "Epoch 2524, Loss: 2.6904953122138977, Final Batch Loss: 0.5898267030715942\n",
      "Epoch 2525, Loss: 2.535184323787689, Final Batch Loss: 0.5078372359275818\n",
      "Epoch 2526, Loss: 2.709862172603607, Final Batch Loss: 0.49350297451019287\n",
      "Epoch 2527, Loss: 2.624491721391678, Final Batch Loss: 0.5286242365837097\n",
      "Epoch 2528, Loss: 2.947379022836685, Final Batch Loss: 0.5802873373031616\n",
      "Epoch 2529, Loss: 2.772677719593048, Final Batch Loss: 0.5920447707176208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2530, Loss: 2.5520403385162354, Final Batch Loss: 0.5117295980453491\n",
      "Epoch 2531, Loss: 2.7134390473365784, Final Batch Loss: 0.5489867925643921\n",
      "Epoch 2532, Loss: 2.5704832673072815, Final Batch Loss: 0.5942705273628235\n",
      "Epoch 2533, Loss: 2.6718468964099884, Final Batch Loss: 0.4975263476371765\n",
      "Epoch 2534, Loss: 2.7252487242221832, Final Batch Loss: 0.44865939021110535\n",
      "Epoch 2535, Loss: 2.6433944404125214, Final Batch Loss: 0.6295892000198364\n",
      "Epoch 2536, Loss: 2.6806490421295166, Final Batch Loss: 0.4574449062347412\n",
      "Epoch 2537, Loss: 2.725665509700775, Final Batch Loss: 0.5650774240493774\n",
      "Epoch 2538, Loss: 2.5330576598644257, Final Batch Loss: 0.527579128742218\n",
      "Epoch 2539, Loss: 2.6631062626838684, Final Batch Loss: 0.4187490940093994\n",
      "Epoch 2540, Loss: 2.5617741346359253, Final Batch Loss: 0.4751547574996948\n",
      "Epoch 2541, Loss: 2.8425756096839905, Final Batch Loss: 0.6660214066505432\n",
      "Epoch 2542, Loss: 2.5717276334762573, Final Batch Loss: 0.49180281162261963\n",
      "Epoch 2543, Loss: 2.584717571735382, Final Batch Loss: 0.5025176405906677\n",
      "Epoch 2544, Loss: 2.6410834193229675, Final Batch Loss: 0.5128328800201416\n",
      "Epoch 2545, Loss: 2.9184549748897552, Final Batch Loss: 0.6618170142173767\n",
      "Epoch 2546, Loss: 2.6596387326717377, Final Batch Loss: 0.4732443392276764\n",
      "Epoch 2547, Loss: 2.72080597281456, Final Batch Loss: 0.5248496532440186\n",
      "Epoch 2548, Loss: 2.6465132534503937, Final Batch Loss: 0.5282714366912842\n",
      "Epoch 2549, Loss: 2.9637971222400665, Final Batch Loss: 0.7038738131523132\n",
      "Epoch 2550, Loss: 2.767664223909378, Final Batch Loss: 0.7952507138252258\n",
      "Epoch 2551, Loss: 2.669217675924301, Final Batch Loss: 0.45864227414131165\n",
      "Epoch 2552, Loss: 2.849841058254242, Final Batch Loss: 0.5962799787521362\n",
      "Epoch 2553, Loss: 2.6264050602912903, Final Batch Loss: 0.45836055278778076\n",
      "Epoch 2554, Loss: 2.7320655584335327, Final Batch Loss: 0.48955392837524414\n",
      "Epoch 2555, Loss: 2.631792962551117, Final Batch Loss: 0.6013732552528381\n",
      "Epoch 2556, Loss: 2.5424201786518097, Final Batch Loss: 0.4985968768596649\n",
      "Epoch 2557, Loss: 2.8322295546531677, Final Batch Loss: 0.5629404187202454\n",
      "Epoch 2558, Loss: 2.7166729271411896, Final Batch Loss: 0.47287872433662415\n",
      "Epoch 2559, Loss: 2.7090799808502197, Final Batch Loss: 0.49587345123291016\n",
      "Epoch 2560, Loss: 2.7337712049484253, Final Batch Loss: 0.5607759356498718\n",
      "Epoch 2561, Loss: 2.6577167212963104, Final Batch Loss: 0.5186470746994019\n",
      "Epoch 2562, Loss: 2.5191619992256165, Final Batch Loss: 0.5240597724914551\n",
      "Epoch 2563, Loss: 2.5641077160835266, Final Batch Loss: 0.4967566728591919\n",
      "Epoch 2564, Loss: 2.522520363330841, Final Batch Loss: 0.47916266322135925\n",
      "Epoch 2565, Loss: 2.626482665538788, Final Batch Loss: 0.4570218324661255\n",
      "Epoch 2566, Loss: 2.6088201105594635, Final Batch Loss: 0.5336606502532959\n",
      "Epoch 2567, Loss: 2.6131653487682343, Final Batch Loss: 0.5247873663902283\n",
      "Epoch 2568, Loss: 2.6381069719791412, Final Batch Loss: 0.4496963620185852\n",
      "Epoch 2569, Loss: 2.793291687965393, Final Batch Loss: 0.6043061017990112\n",
      "Epoch 2570, Loss: 2.692403495311737, Final Batch Loss: 0.6955161094665527\n",
      "Epoch 2571, Loss: 2.916627287864685, Final Batch Loss: 0.7062376141548157\n",
      "Epoch 2572, Loss: 2.6606509387493134, Final Batch Loss: 0.5443099141120911\n",
      "Epoch 2573, Loss: 2.5197123885154724, Final Batch Loss: 0.42779460549354553\n",
      "Epoch 2574, Loss: 2.787302255630493, Final Batch Loss: 0.6404122710227966\n",
      "Epoch 2575, Loss: 2.498656541109085, Final Batch Loss: 0.4429877996444702\n",
      "Epoch 2576, Loss: 2.676089882850647, Final Batch Loss: 0.519718587398529\n",
      "Epoch 2577, Loss: 2.696841210126877, Final Batch Loss: 0.5943084955215454\n",
      "Epoch 2578, Loss: 2.6500056087970734, Final Batch Loss: 0.5336952805519104\n",
      "Epoch 2579, Loss: 2.677962988615036, Final Batch Loss: 0.5711637735366821\n",
      "Epoch 2580, Loss: 2.7423349618911743, Final Batch Loss: 0.4966781735420227\n",
      "Epoch 2581, Loss: 2.638921707868576, Final Batch Loss: 0.6945320963859558\n",
      "Epoch 2582, Loss: 2.8692719638347626, Final Batch Loss: 0.6502557396888733\n",
      "Epoch 2583, Loss: 2.56505224108696, Final Batch Loss: 0.4805462956428528\n",
      "Epoch 2584, Loss: 2.5300593972206116, Final Batch Loss: 0.48618394136428833\n",
      "Epoch 2585, Loss: 2.8715265095233917, Final Batch Loss: 0.7080453038215637\n",
      "Epoch 2586, Loss: 2.7660106420516968, Final Batch Loss: 0.5839906930923462\n",
      "Epoch 2587, Loss: 2.5655694901943207, Final Batch Loss: 0.46459171175956726\n",
      "Epoch 2588, Loss: 2.798933893442154, Final Batch Loss: 0.5163877606391907\n",
      "Epoch 2589, Loss: 2.6620777249336243, Final Batch Loss: 0.5037598609924316\n",
      "Epoch 2590, Loss: 2.5738268196582794, Final Batch Loss: 0.422245591878891\n",
      "Epoch 2591, Loss: 2.633578449487686, Final Batch Loss: 0.5374473929405212\n",
      "Epoch 2592, Loss: 2.742306649684906, Final Batch Loss: 0.5388845801353455\n",
      "Epoch 2593, Loss: 2.78997465968132, Final Batch Loss: 0.6308844089508057\n",
      "Epoch 2594, Loss: 2.59758460521698, Final Batch Loss: 0.5096069574356079\n",
      "Epoch 2595, Loss: 2.670094460248947, Final Batch Loss: 0.38790181279182434\n",
      "Epoch 2596, Loss: 2.836469292640686, Final Batch Loss: 0.5591737031936646\n",
      "Epoch 2597, Loss: 2.6101425886154175, Final Batch Loss: 0.5521107316017151\n",
      "Epoch 2598, Loss: 2.6868778467178345, Final Batch Loss: 0.4511135220527649\n",
      "Epoch 2599, Loss: 2.623213231563568, Final Batch Loss: 0.5595449805259705\n",
      "Epoch 2600, Loss: 2.5937924087047577, Final Batch Loss: 0.5232537984848022\n",
      "Epoch 2601, Loss: 2.5974751710891724, Final Batch Loss: 0.539983332157135\n",
      "Epoch 2602, Loss: 2.6758034229278564, Final Batch Loss: 0.5525662899017334\n",
      "Epoch 2603, Loss: 2.7548170685768127, Final Batch Loss: 0.630350649356842\n",
      "Epoch 2604, Loss: 2.5782348811626434, Final Batch Loss: 0.43475696444511414\n",
      "Epoch 2605, Loss: 2.6559017598629, Final Batch Loss: 0.6480535268783569\n",
      "Epoch 2606, Loss: 2.7977906465530396, Final Batch Loss: 0.549322247505188\n",
      "Epoch 2607, Loss: 2.6519340574741364, Final Batch Loss: 0.5646986365318298\n",
      "Epoch 2608, Loss: 2.8279545307159424, Final Batch Loss: 0.626824140548706\n",
      "Epoch 2609, Loss: 2.5812795162200928, Final Batch Loss: 0.5609449148178101\n",
      "Epoch 2610, Loss: 2.637098640203476, Final Batch Loss: 0.41922059655189514\n",
      "Epoch 2611, Loss: 2.627768963575363, Final Batch Loss: 0.4480271339416504\n",
      "Epoch 2612, Loss: 2.747451066970825, Final Batch Loss: 0.4131450653076172\n",
      "Epoch 2613, Loss: 2.7554731369018555, Final Batch Loss: 0.5420699715614319\n",
      "Epoch 2614, Loss: 2.616993874311447, Final Batch Loss: 0.5060415267944336\n",
      "Epoch 2615, Loss: 2.649382561445236, Final Batch Loss: 0.46491268277168274\n",
      "Epoch 2616, Loss: 2.6558775305747986, Final Batch Loss: 0.6351958513259888\n",
      "Epoch 2617, Loss: 2.6254759430885315, Final Batch Loss: 0.4889313280582428\n",
      "Epoch 2618, Loss: 2.5883489549160004, Final Batch Loss: 0.5481695532798767\n",
      "Epoch 2619, Loss: 2.8953747153282166, Final Batch Loss: 0.5857781171798706\n",
      "Epoch 2620, Loss: 2.5220947265625, Final Batch Loss: 0.40599626302719116\n",
      "Epoch 2621, Loss: 2.5968344509601593, Final Batch Loss: 0.606414794921875\n",
      "Epoch 2622, Loss: 2.5893310606479645, Final Batch Loss: 0.47184306383132935\n",
      "Epoch 2623, Loss: 2.547983855009079, Final Batch Loss: 0.5244855880737305\n",
      "Epoch 2624, Loss: 2.7394616901874542, Final Batch Loss: 0.6234858632087708\n",
      "Epoch 2625, Loss: 2.764581084251404, Final Batch Loss: 0.48664987087249756\n",
      "Epoch 2626, Loss: 2.6257466971874237, Final Batch Loss: 0.5793181657791138\n",
      "Epoch 2627, Loss: 2.89959317445755, Final Batch Loss: 0.6812266111373901\n",
      "Epoch 2628, Loss: 2.609293907880783, Final Batch Loss: 0.49305546283721924\n",
      "Epoch 2629, Loss: 2.6647323966026306, Final Batch Loss: 0.616715133190155\n",
      "Epoch 2630, Loss: 2.5696820318698883, Final Batch Loss: 0.5902935862541199\n",
      "Epoch 2631, Loss: 2.7941976487636566, Final Batch Loss: 0.6944088935852051\n",
      "Epoch 2632, Loss: 2.8021063804626465, Final Batch Loss: 0.5885226726531982\n",
      "Epoch 2633, Loss: 2.6168685257434845, Final Batch Loss: 0.48683860898017883\n",
      "Epoch 2634, Loss: 2.5785267651081085, Final Batch Loss: 0.47182250022888184\n",
      "Epoch 2635, Loss: 2.60029274225235, Final Batch Loss: 0.5246152877807617\n",
      "Epoch 2636, Loss: 2.627119779586792, Final Batch Loss: 0.5173527002334595\n",
      "Epoch 2637, Loss: 2.7247257232666016, Final Batch Loss: 0.6191192865371704\n",
      "Epoch 2638, Loss: 2.7094208896160126, Final Batch Loss: 0.5582132935523987\n",
      "Epoch 2639, Loss: 2.6837247610092163, Final Batch Loss: 0.6067368388175964\n",
      "Epoch 2640, Loss: 2.6599977016448975, Final Batch Loss: 0.4977904260158539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2641, Loss: 2.8379117250442505, Final Batch Loss: 0.597308337688446\n",
      "Epoch 2642, Loss: 2.6848630905151367, Final Batch Loss: 0.6512783765792847\n",
      "Epoch 2643, Loss: 2.844355523586273, Final Batch Loss: 0.5561031103134155\n",
      "Epoch 2644, Loss: 2.6263844072818756, Final Batch Loss: 0.5183196663856506\n",
      "Epoch 2645, Loss: 2.5970034897327423, Final Batch Loss: 0.5550959706306458\n",
      "Epoch 2646, Loss: 2.923566162586212, Final Batch Loss: 0.6191918253898621\n",
      "Epoch 2647, Loss: 2.6110684275627136, Final Batch Loss: 0.4091898202896118\n",
      "Epoch 2648, Loss: 2.665962725877762, Final Batch Loss: 0.4812571704387665\n",
      "Epoch 2649, Loss: 2.717841237783432, Final Batch Loss: 0.6297174096107483\n",
      "Epoch 2650, Loss: 2.6333493292331696, Final Batch Loss: 0.47292038798332214\n",
      "Epoch 2651, Loss: 2.674662560224533, Final Batch Loss: 0.5323606729507446\n",
      "Epoch 2652, Loss: 2.755042463541031, Final Batch Loss: 0.6220961809158325\n",
      "Epoch 2653, Loss: 2.6685562133789062, Final Batch Loss: 0.5368292331695557\n",
      "Epoch 2654, Loss: 2.625679075717926, Final Batch Loss: 0.6276047229766846\n",
      "Epoch 2655, Loss: 2.781839281320572, Final Batch Loss: 0.5810319185256958\n",
      "Epoch 2656, Loss: 2.592892050743103, Final Batch Loss: 0.6143518090248108\n",
      "Epoch 2657, Loss: 2.5912232398986816, Final Batch Loss: 0.6259942650794983\n",
      "Epoch 2658, Loss: 2.5459339320659637, Final Batch Loss: 0.5798660516738892\n",
      "Epoch 2659, Loss: 2.7388315200805664, Final Batch Loss: 0.626535952091217\n",
      "Epoch 2660, Loss: 2.7016399800777435, Final Batch Loss: 0.6187912821769714\n",
      "Epoch 2661, Loss: 2.4430721402168274, Final Batch Loss: 0.47016555070877075\n",
      "Epoch 2662, Loss: 2.4035970866680145, Final Batch Loss: 0.4427330791950226\n",
      "Epoch 2663, Loss: 2.765386164188385, Final Batch Loss: 0.6453511714935303\n",
      "Epoch 2664, Loss: 2.504442721605301, Final Batch Loss: 0.5503198504447937\n",
      "Epoch 2665, Loss: 2.520339161157608, Final Batch Loss: 0.4714497923851013\n",
      "Epoch 2666, Loss: 2.5693969130516052, Final Batch Loss: 0.5096955895423889\n",
      "Epoch 2667, Loss: 2.6640566885471344, Final Batch Loss: 0.4900253713130951\n",
      "Epoch 2668, Loss: 2.7085031867027283, Final Batch Loss: 0.48307090997695923\n",
      "Epoch 2669, Loss: 2.7840792536735535, Final Batch Loss: 0.596182644367218\n",
      "Epoch 2670, Loss: 2.744648516178131, Final Batch Loss: 0.569447934627533\n",
      "Epoch 2671, Loss: 2.5071201622486115, Final Batch Loss: 0.4880303144454956\n",
      "Epoch 2672, Loss: 2.5020884573459625, Final Batch Loss: 0.5274727940559387\n",
      "Epoch 2673, Loss: 2.757036864757538, Final Batch Loss: 0.5732294321060181\n",
      "Epoch 2674, Loss: 2.6668785512447357, Final Batch Loss: 0.5481371283531189\n",
      "Epoch 2675, Loss: 2.6535522639751434, Final Batch Loss: 0.582302987575531\n",
      "Epoch 2676, Loss: 2.474598705768585, Final Batch Loss: 0.47756537795066833\n",
      "Epoch 2677, Loss: 2.6098133325576782, Final Batch Loss: 0.4378252625465393\n",
      "Epoch 2678, Loss: 2.6230645179748535, Final Batch Loss: 0.5121127963066101\n",
      "Epoch 2679, Loss: 2.7247791290283203, Final Batch Loss: 0.512495756149292\n",
      "Epoch 2680, Loss: 2.6266620457172394, Final Batch Loss: 0.56229168176651\n",
      "Epoch 2681, Loss: 2.8886203169822693, Final Batch Loss: 0.6289990544319153\n",
      "Epoch 2682, Loss: 2.5234749913215637, Final Batch Loss: 0.4267573356628418\n",
      "Epoch 2683, Loss: 2.696358412504196, Final Batch Loss: 0.5463003516197205\n",
      "Epoch 2684, Loss: 2.71559801697731, Final Batch Loss: 0.5563151240348816\n",
      "Epoch 2685, Loss: 2.452838748693466, Final Batch Loss: 0.41167253255844116\n",
      "Epoch 2686, Loss: 2.5893889665603638, Final Batch Loss: 0.4708344638347626\n",
      "Epoch 2687, Loss: 2.9866238832473755, Final Batch Loss: 0.7042201161384583\n",
      "Epoch 2688, Loss: 2.706576645374298, Final Batch Loss: 0.5330938100814819\n",
      "Epoch 2689, Loss: 2.7863255441188812, Final Batch Loss: 0.6361034512519836\n",
      "Epoch 2690, Loss: 2.7581879794597626, Final Batch Loss: 0.5465177297592163\n",
      "Epoch 2691, Loss: 2.7143568098545074, Final Batch Loss: 0.5519864559173584\n",
      "Epoch 2692, Loss: 2.7577657103538513, Final Batch Loss: 0.5882506370544434\n",
      "Epoch 2693, Loss: 2.7755393981933594, Final Batch Loss: 0.5786629915237427\n",
      "Epoch 2694, Loss: 2.553828865289688, Final Batch Loss: 0.4629630744457245\n",
      "Epoch 2695, Loss: 2.7635181546211243, Final Batch Loss: 0.5161740183830261\n",
      "Epoch 2696, Loss: 2.670996606349945, Final Batch Loss: 0.4979405999183655\n",
      "Epoch 2697, Loss: 2.745551645755768, Final Batch Loss: 0.5657916069030762\n",
      "Epoch 2698, Loss: 2.6464013159275055, Final Batch Loss: 0.5422089695930481\n",
      "Epoch 2699, Loss: 2.6481465101242065, Final Batch Loss: 0.5743638277053833\n",
      "Epoch 2700, Loss: 2.8155412673950195, Final Batch Loss: 0.6657797694206238\n",
      "Epoch 2701, Loss: 2.5149933099746704, Final Batch Loss: 0.5099099278450012\n",
      "Epoch 2702, Loss: 2.5647601187229156, Final Batch Loss: 0.44850412011146545\n",
      "Epoch 2703, Loss: 2.768347203731537, Final Batch Loss: 0.6969708800315857\n",
      "Epoch 2704, Loss: 2.6148775219917297, Final Batch Loss: 0.6371138691902161\n",
      "Epoch 2705, Loss: 2.6788541078567505, Final Batch Loss: 0.5199353694915771\n",
      "Epoch 2706, Loss: 2.8740718960762024, Final Batch Loss: 0.7342613935470581\n",
      "Epoch 2707, Loss: 2.719702184200287, Final Batch Loss: 0.5518121123313904\n",
      "Epoch 2708, Loss: 2.8171768188476562, Final Batch Loss: 0.5862002372741699\n",
      "Epoch 2709, Loss: 2.639746844768524, Final Batch Loss: 0.48790013790130615\n",
      "Epoch 2710, Loss: 2.702852040529251, Final Batch Loss: 0.5754854083061218\n",
      "Epoch 2711, Loss: 2.683173716068268, Final Batch Loss: 0.6461299657821655\n",
      "Epoch 2712, Loss: 2.6247286200523376, Final Batch Loss: 0.572475790977478\n",
      "Epoch 2713, Loss: 2.7793140709400177, Final Batch Loss: 0.633328914642334\n",
      "Epoch 2714, Loss: 2.670074701309204, Final Batch Loss: 0.5017544627189636\n",
      "Epoch 2715, Loss: 2.560090571641922, Final Batch Loss: 0.6253061294555664\n",
      "Epoch 2716, Loss: 2.702942579984665, Final Batch Loss: 0.5967868566513062\n",
      "Epoch 2717, Loss: 2.514536142349243, Final Batch Loss: 0.562690794467926\n",
      "Epoch 2718, Loss: 2.5317275524139404, Final Batch Loss: 0.4428379535675049\n",
      "Epoch 2719, Loss: 2.573421448469162, Final Batch Loss: 0.45082443952560425\n",
      "Epoch 2720, Loss: 2.7084894478321075, Final Batch Loss: 0.5315630435943604\n",
      "Epoch 2721, Loss: 2.584180176258087, Final Batch Loss: 0.5441708564758301\n",
      "Epoch 2722, Loss: 2.6713223457336426, Final Batch Loss: 0.5627923011779785\n",
      "Epoch 2723, Loss: 2.6068893671035767, Final Batch Loss: 0.5570161938667297\n",
      "Epoch 2724, Loss: 2.4966897666454315, Final Batch Loss: 0.489435613155365\n",
      "Epoch 2725, Loss: 2.558507412672043, Final Batch Loss: 0.5034024715423584\n",
      "Epoch 2726, Loss: 2.743705540895462, Final Batch Loss: 0.6428102254867554\n",
      "Epoch 2727, Loss: 2.635044038295746, Final Batch Loss: 0.48740634322166443\n",
      "Epoch 2728, Loss: 2.7934426367282867, Final Batch Loss: 0.3942342698574066\n",
      "Epoch 2729, Loss: 2.552402526140213, Final Batch Loss: 0.4939093291759491\n",
      "Epoch 2730, Loss: 2.5242693722248077, Final Batch Loss: 0.37016430497169495\n",
      "Epoch 2731, Loss: 3.078263908624649, Final Batch Loss: 0.6354771852493286\n",
      "Epoch 2732, Loss: 2.557808995246887, Final Batch Loss: 0.5471425652503967\n",
      "Epoch 2733, Loss: 2.5674239695072174, Final Batch Loss: 0.46926748752593994\n",
      "Epoch 2734, Loss: 2.662705719470978, Final Batch Loss: 0.4780692160129547\n",
      "Epoch 2735, Loss: 2.5557271540164948, Final Batch Loss: 0.47934576869010925\n",
      "Epoch 2736, Loss: 2.5095300674438477, Final Batch Loss: 0.4933858811855316\n",
      "Epoch 2737, Loss: 2.477976620197296, Final Batch Loss: 0.44209811091423035\n",
      "Epoch 2738, Loss: 2.5435348749160767, Final Batch Loss: 0.6192367672920227\n",
      "Epoch 2739, Loss: 2.7216803431510925, Final Batch Loss: 0.4981235861778259\n",
      "Epoch 2740, Loss: 2.8923746049404144, Final Batch Loss: 0.7947143912315369\n",
      "Epoch 2741, Loss: 2.5335833132267, Final Batch Loss: 0.503005862236023\n",
      "Epoch 2742, Loss: 2.6073620319366455, Final Batch Loss: 0.44413095712661743\n",
      "Epoch 2743, Loss: 2.6026139557361603, Final Batch Loss: 0.4446924030780792\n",
      "Epoch 2744, Loss: 2.682930201292038, Final Batch Loss: 0.5909674763679504\n",
      "Epoch 2745, Loss: 2.489347517490387, Final Batch Loss: 0.46930399537086487\n",
      "Epoch 2746, Loss: 2.675341933965683, Final Batch Loss: 0.5682472586631775\n",
      "Epoch 2747, Loss: 2.4630293250083923, Final Batch Loss: 0.4063808023929596\n",
      "Epoch 2748, Loss: 2.5028114318847656, Final Batch Loss: 0.5004817843437195\n",
      "Epoch 2749, Loss: 2.627927780151367, Final Batch Loss: 0.519790530204773\n",
      "Epoch 2750, Loss: 2.569405436515808, Final Batch Loss: 0.4951992630958557\n",
      "Epoch 2751, Loss: 2.5875132977962494, Final Batch Loss: 0.5206752419471741\n",
      "Epoch 2752, Loss: 2.599960684776306, Final Batch Loss: 0.4700927734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2753, Loss: 2.7096324265003204, Final Batch Loss: 0.696024477481842\n",
      "Epoch 2754, Loss: 2.5544031858444214, Final Batch Loss: 0.5641010403633118\n",
      "Epoch 2755, Loss: 2.7115820050239563, Final Batch Loss: 0.5921287536621094\n",
      "Epoch 2756, Loss: 2.6626539826393127, Final Batch Loss: 0.5097339749336243\n",
      "Epoch 2757, Loss: 2.6546107828617096, Final Batch Loss: 0.6039499640464783\n",
      "Epoch 2758, Loss: 2.6854903995990753, Final Batch Loss: 0.47812578082084656\n",
      "Epoch 2759, Loss: 2.5918709337711334, Final Batch Loss: 0.4966593384742737\n",
      "Epoch 2760, Loss: 2.6202559173107147, Final Batch Loss: 0.610921323299408\n",
      "Epoch 2761, Loss: 2.530924081802368, Final Batch Loss: 0.47016066312789917\n",
      "Epoch 2762, Loss: 2.813362270593643, Final Batch Loss: 0.6533263921737671\n",
      "Epoch 2763, Loss: 2.4598730206489563, Final Batch Loss: 0.4297477602958679\n",
      "Epoch 2764, Loss: 2.718226671218872, Final Batch Loss: 0.5063965320587158\n",
      "Epoch 2765, Loss: 2.6319150030612946, Final Batch Loss: 0.6467761397361755\n",
      "Epoch 2766, Loss: 2.4684479236602783, Final Batch Loss: 0.5488062500953674\n",
      "Epoch 2767, Loss: 2.5646320581436157, Final Batch Loss: 0.47844985127449036\n",
      "Epoch 2768, Loss: 2.6768613755702972, Final Batch Loss: 0.47932785749435425\n",
      "Epoch 2769, Loss: 2.6461580395698547, Final Batch Loss: 0.5873008966445923\n",
      "Epoch 2770, Loss: 2.56727796792984, Final Batch Loss: 0.5751264095306396\n",
      "Epoch 2771, Loss: 2.5956309139728546, Final Batch Loss: 0.5933424830436707\n",
      "Epoch 2772, Loss: 2.7452968060970306, Final Batch Loss: 0.5718139410018921\n",
      "Epoch 2773, Loss: 2.752897560596466, Final Batch Loss: 0.5738070011138916\n",
      "Epoch 2774, Loss: 2.4694704413414, Final Batch Loss: 0.464523047208786\n",
      "Epoch 2775, Loss: 2.551074743270874, Final Batch Loss: 0.543221652507782\n",
      "Epoch 2776, Loss: 2.6413360238075256, Final Batch Loss: 0.5356610417366028\n",
      "Epoch 2777, Loss: 2.6791123747825623, Final Batch Loss: 0.48558902740478516\n",
      "Epoch 2778, Loss: 2.6660149693489075, Final Batch Loss: 0.5828094482421875\n",
      "Epoch 2779, Loss: 2.739219069480896, Final Batch Loss: 0.612031102180481\n",
      "Epoch 2780, Loss: 2.5230108201503754, Final Batch Loss: 0.5458849668502808\n",
      "Epoch 2781, Loss: 2.7220472395420074, Final Batch Loss: 0.6846877336502075\n",
      "Epoch 2782, Loss: 2.6466154158115387, Final Batch Loss: 0.5913985967636108\n",
      "Epoch 2783, Loss: 2.54707869887352, Final Batch Loss: 0.537865936756134\n",
      "Epoch 2784, Loss: 2.567361295223236, Final Batch Loss: 0.48516467213630676\n",
      "Epoch 2785, Loss: 2.6199414432048798, Final Batch Loss: 0.5253126621246338\n",
      "Epoch 2786, Loss: 2.669937014579773, Final Batch Loss: 0.5667493343353271\n",
      "Epoch 2787, Loss: 2.5933310985565186, Final Batch Loss: 0.49549761414527893\n",
      "Epoch 2788, Loss: 2.7139867544174194, Final Batch Loss: 0.5006782412528992\n",
      "Epoch 2789, Loss: 2.816213846206665, Final Batch Loss: 0.5925267934799194\n",
      "Epoch 2790, Loss: 2.5838575065135956, Final Batch Loss: 0.5776596069335938\n",
      "Epoch 2791, Loss: 2.5740223228931427, Final Batch Loss: 0.5108038783073425\n",
      "Epoch 2792, Loss: 2.4945884943008423, Final Batch Loss: 0.567903995513916\n",
      "Epoch 2793, Loss: 2.686073809862137, Final Batch Loss: 0.4714973568916321\n",
      "Epoch 2794, Loss: 2.6242536306381226, Final Batch Loss: 0.5710552930831909\n",
      "Epoch 2795, Loss: 2.8516511023044586, Final Batch Loss: 0.6806225776672363\n",
      "Epoch 2796, Loss: 2.5823807418346405, Final Batch Loss: 0.5004721879959106\n",
      "Epoch 2797, Loss: 2.6803590059280396, Final Batch Loss: 0.556713879108429\n",
      "Epoch 2798, Loss: 2.7945058941841125, Final Batch Loss: 0.543138325214386\n",
      "Epoch 2799, Loss: 2.581795185804367, Final Batch Loss: 0.4838190972805023\n",
      "Epoch 2800, Loss: 2.6900264620780945, Final Batch Loss: 0.5788940191268921\n",
      "Epoch 2801, Loss: 2.710668921470642, Final Batch Loss: 0.5455319285392761\n",
      "Epoch 2802, Loss: 2.4917925000190735, Final Batch Loss: 0.5820943117141724\n",
      "Epoch 2803, Loss: 2.562781661748886, Final Batch Loss: 0.5374653935432434\n",
      "Epoch 2804, Loss: 2.7358736395835876, Final Batch Loss: 0.589217483997345\n",
      "Epoch 2805, Loss: 2.6928220987319946, Final Batch Loss: 0.5193811058998108\n",
      "Epoch 2806, Loss: 2.5347191393375397, Final Batch Loss: 0.39567598700523376\n",
      "Epoch 2807, Loss: 2.8406298458576202, Final Batch Loss: 0.7475277781486511\n",
      "Epoch 2808, Loss: 2.5496483147144318, Final Batch Loss: 0.4867406487464905\n",
      "Epoch 2809, Loss: 2.5993076860904694, Final Batch Loss: 0.5757697224617004\n",
      "Epoch 2810, Loss: 2.6873508393764496, Final Batch Loss: 0.49811670184135437\n",
      "Epoch 2811, Loss: 2.6358743607997894, Final Batch Loss: 0.4917599856853485\n",
      "Epoch 2812, Loss: 2.711302936077118, Final Batch Loss: 0.6566248536109924\n",
      "Epoch 2813, Loss: 2.46743306517601, Final Batch Loss: 0.5078416466712952\n",
      "Epoch 2814, Loss: 2.671152561903, Final Batch Loss: 0.693167507648468\n",
      "Epoch 2815, Loss: 2.7509889900684357, Final Batch Loss: 0.5943070650100708\n",
      "Epoch 2816, Loss: 2.6153911352157593, Final Batch Loss: 0.5571672320365906\n",
      "Epoch 2817, Loss: 2.6068311035633087, Final Batch Loss: 0.5299900770187378\n",
      "Epoch 2818, Loss: 2.605699896812439, Final Batch Loss: 0.5342679023742676\n",
      "Epoch 2819, Loss: 2.633640795946121, Final Batch Loss: 0.5821982622146606\n",
      "Epoch 2820, Loss: 2.5214149355888367, Final Batch Loss: 0.49417436122894287\n",
      "Epoch 2821, Loss: 2.404489904642105, Final Batch Loss: 0.4448302686214447\n",
      "Epoch 2822, Loss: 2.587733715772629, Final Batch Loss: 0.49624112248420715\n",
      "Epoch 2823, Loss: 2.829504758119583, Final Batch Loss: 0.5495287179946899\n",
      "Epoch 2824, Loss: 2.5565082132816315, Final Batch Loss: 0.44040417671203613\n",
      "Epoch 2825, Loss: 2.7046238780021667, Final Batch Loss: 0.5083304643630981\n",
      "Epoch 2826, Loss: 2.3819235265254974, Final Batch Loss: 0.4574402868747711\n",
      "Epoch 2827, Loss: 2.6741263568401337, Final Batch Loss: 0.5569155812263489\n",
      "Epoch 2828, Loss: 2.726952850818634, Final Batch Loss: 0.599445641040802\n",
      "Epoch 2829, Loss: 2.6144592463970184, Final Batch Loss: 0.47814908623695374\n",
      "Epoch 2830, Loss: 2.3184193074703217, Final Batch Loss: 0.403157114982605\n",
      "Epoch 2831, Loss: 2.6359509229660034, Final Batch Loss: 0.4911574721336365\n",
      "Epoch 2832, Loss: 2.5787682235240936, Final Batch Loss: 0.5786358714103699\n",
      "Epoch 2833, Loss: 2.6431002616882324, Final Batch Loss: 0.4774259924888611\n",
      "Epoch 2834, Loss: 2.6575962007045746, Final Batch Loss: 0.4806201756000519\n",
      "Epoch 2835, Loss: 2.6418450474739075, Final Batch Loss: 0.6039690375328064\n",
      "Epoch 2836, Loss: 2.651535451412201, Final Batch Loss: 0.48710906505584717\n",
      "Epoch 2837, Loss: 2.508618503808975, Final Batch Loss: 0.4892646074295044\n",
      "Epoch 2838, Loss: 2.5153397917747498, Final Batch Loss: 0.40704965591430664\n",
      "Epoch 2839, Loss: 2.47452175617218, Final Batch Loss: 0.49488961696624756\n",
      "Epoch 2840, Loss: 2.495025157928467, Final Batch Loss: 0.4378977417945862\n",
      "Epoch 2841, Loss: 2.610829532146454, Final Batch Loss: 0.5343495607376099\n",
      "Epoch 2842, Loss: 2.5729887783527374, Final Batch Loss: 0.5967037677764893\n",
      "Epoch 2843, Loss: 2.6693761348724365, Final Batch Loss: 0.6058459281921387\n",
      "Epoch 2844, Loss: 2.5729051530361176, Final Batch Loss: 0.48862558603286743\n",
      "Epoch 2845, Loss: 2.5352103412151337, Final Batch Loss: 0.4370570480823517\n",
      "Epoch 2846, Loss: 2.3240787386894226, Final Batch Loss: 0.4334689974784851\n",
      "Epoch 2847, Loss: 2.445369929075241, Final Batch Loss: 0.48188382387161255\n",
      "Epoch 2848, Loss: 2.730462431907654, Final Batch Loss: 0.5081057548522949\n",
      "Epoch 2849, Loss: 2.610666871070862, Final Batch Loss: 0.4361574649810791\n",
      "Epoch 2850, Loss: 2.636123299598694, Final Batch Loss: 0.5689886808395386\n",
      "Epoch 2851, Loss: 2.5053069591522217, Final Batch Loss: 0.5800287127494812\n",
      "Epoch 2852, Loss: 2.9050846099853516, Final Batch Loss: 0.6535758376121521\n",
      "Epoch 2853, Loss: 2.4605799317359924, Final Batch Loss: 0.5832457542419434\n",
      "Epoch 2854, Loss: 2.71053946018219, Final Batch Loss: 0.53205806016922\n",
      "Epoch 2855, Loss: 2.7133413553237915, Final Batch Loss: 0.5357958078384399\n",
      "Epoch 2856, Loss: 2.7765382826328278, Final Batch Loss: 0.6661871075630188\n",
      "Epoch 2857, Loss: 2.6048525869846344, Final Batch Loss: 0.5227514505386353\n",
      "Epoch 2858, Loss: 2.572600305080414, Final Batch Loss: 0.4757344722747803\n",
      "Epoch 2859, Loss: 2.459530621767044, Final Batch Loss: 0.5163089036941528\n",
      "Epoch 2860, Loss: 2.5752715170383453, Final Batch Loss: 0.5184820294380188\n",
      "Epoch 2861, Loss: 2.8273193538188934, Final Batch Loss: 0.8222635984420776\n",
      "Epoch 2862, Loss: 2.5627268254756927, Final Batch Loss: 0.4741818606853485\n",
      "Epoch 2863, Loss: 2.6667186617851257, Final Batch Loss: 0.5309632420539856\n",
      "Epoch 2864, Loss: 2.471374750137329, Final Batch Loss: 0.5288236737251282\n",
      "Epoch 2865, Loss: 2.520865738391876, Final Batch Loss: 0.49647021293640137\n",
      "Epoch 2866, Loss: 2.4291814267635345, Final Batch Loss: 0.4027884900569916\n",
      "Epoch 2867, Loss: 2.6150166988372803, Final Batch Loss: 0.6388958692550659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2868, Loss: 2.7262006402015686, Final Batch Loss: 0.5074443817138672\n",
      "Epoch 2869, Loss: 2.59989532828331, Final Batch Loss: 0.4759535789489746\n",
      "Epoch 2870, Loss: 2.539015531539917, Final Batch Loss: 0.46587562561035156\n",
      "Epoch 2871, Loss: 2.6437622904777527, Final Batch Loss: 0.6377671957015991\n",
      "Epoch 2872, Loss: 2.5195766985416412, Final Batch Loss: 0.4636744558811188\n",
      "Epoch 2873, Loss: 2.551598906517029, Final Batch Loss: 0.5783541202545166\n",
      "Epoch 2874, Loss: 2.5401728451251984, Final Batch Loss: 0.5615174770355225\n",
      "Epoch 2875, Loss: 2.697183758020401, Final Batch Loss: 0.6133898496627808\n",
      "Epoch 2876, Loss: 2.622495800256729, Final Batch Loss: 0.49716082215309143\n",
      "Epoch 2877, Loss: 2.64042991399765, Final Batch Loss: 0.5247521996498108\n",
      "Epoch 2878, Loss: 2.6850981414318085, Final Batch Loss: 0.5395345687866211\n",
      "Epoch 2879, Loss: 2.6245222091674805, Final Batch Loss: 0.5282142162322998\n",
      "Epoch 2880, Loss: 2.7683134377002716, Final Batch Loss: 0.5728564858436584\n",
      "Epoch 2881, Loss: 2.6891970336437225, Final Batch Loss: 0.6853929162025452\n",
      "Epoch 2882, Loss: 2.665422946214676, Final Batch Loss: 0.46004652976989746\n",
      "Epoch 2883, Loss: 2.7882114350795746, Final Batch Loss: 0.5854422450065613\n",
      "Epoch 2884, Loss: 2.5579168498516083, Final Batch Loss: 0.41173794865608215\n",
      "Epoch 2885, Loss: 2.6167764365673065, Final Batch Loss: 0.5539433360099792\n",
      "Epoch 2886, Loss: 2.4311408400535583, Final Batch Loss: 0.4664969742298126\n",
      "Epoch 2887, Loss: 2.555468410253525, Final Batch Loss: 0.4563731253147125\n",
      "Epoch 2888, Loss: 2.6107425689697266, Final Batch Loss: 0.5693140625953674\n",
      "Epoch 2889, Loss: 2.755465567111969, Final Batch Loss: 0.5704572796821594\n",
      "Epoch 2890, Loss: 2.6951642632484436, Final Batch Loss: 0.5349152088165283\n",
      "Epoch 2891, Loss: 2.5839006304740906, Final Batch Loss: 0.5000401139259338\n",
      "Epoch 2892, Loss: 2.681601196527481, Final Batch Loss: 0.4980655014514923\n",
      "Epoch 2893, Loss: 2.7078216075897217, Final Batch Loss: 0.5417296290397644\n",
      "Epoch 2894, Loss: 2.568664252758026, Final Batch Loss: 0.5753986239433289\n",
      "Epoch 2895, Loss: 2.6881077885627747, Final Batch Loss: 0.5405328869819641\n",
      "Epoch 2896, Loss: 2.487624704837799, Final Batch Loss: 0.48419424891471863\n",
      "Epoch 2897, Loss: 2.7361822724342346, Final Batch Loss: 0.48523038625717163\n",
      "Epoch 2898, Loss: 2.551602393388748, Final Batch Loss: 0.4875517189502716\n",
      "Epoch 2899, Loss: 2.608999878168106, Final Batch Loss: 0.49658527970314026\n",
      "Epoch 2900, Loss: 2.6558461487293243, Final Batch Loss: 0.4927177131175995\n",
      "Epoch 2901, Loss: 2.632145971059799, Final Batch Loss: 0.5162883400917053\n",
      "Epoch 2902, Loss: 2.635530114173889, Final Batch Loss: 0.44774022698402405\n",
      "Epoch 2903, Loss: 2.5213671922683716, Final Batch Loss: 0.5462444424629211\n",
      "Epoch 2904, Loss: 2.4653082489967346, Final Batch Loss: 0.4648454189300537\n",
      "Epoch 2905, Loss: 2.4707577526569366, Final Batch Loss: 0.5580881237983704\n",
      "Epoch 2906, Loss: 2.700951784849167, Final Batch Loss: 0.6712155342102051\n",
      "Epoch 2907, Loss: 2.556842714548111, Final Batch Loss: 0.46649348735809326\n",
      "Epoch 2908, Loss: 2.7953224182128906, Final Batch Loss: 0.5191187262535095\n",
      "Epoch 2909, Loss: 2.5534600913524628, Final Batch Loss: 0.4479238986968994\n",
      "Epoch 2910, Loss: 2.527435153722763, Final Batch Loss: 0.46416258811950684\n",
      "Epoch 2911, Loss: 2.4276164770126343, Final Batch Loss: 0.5134292840957642\n",
      "Epoch 2912, Loss: 2.7185758352279663, Final Batch Loss: 0.5282667279243469\n",
      "Epoch 2913, Loss: 2.575706034898758, Final Batch Loss: 0.6576546430587769\n",
      "Epoch 2914, Loss: 2.613876521587372, Final Batch Loss: 0.5397400856018066\n",
      "Epoch 2915, Loss: 2.463953584432602, Final Batch Loss: 0.4940311312675476\n",
      "Epoch 2916, Loss: 2.585340738296509, Final Batch Loss: 0.5238037109375\n",
      "Epoch 2917, Loss: 2.5426641702651978, Final Batch Loss: 0.3429393768310547\n",
      "Epoch 2918, Loss: 2.6026489436626434, Final Batch Loss: 0.46386101841926575\n",
      "Epoch 2919, Loss: 2.6864185631275177, Final Batch Loss: 0.6654016375541687\n",
      "Epoch 2920, Loss: 2.773990660905838, Final Batch Loss: 0.7046222686767578\n",
      "Epoch 2921, Loss: 2.801862597465515, Final Batch Loss: 0.5977084040641785\n",
      "Epoch 2922, Loss: 2.5419899821281433, Final Batch Loss: 0.5622691512107849\n",
      "Epoch 2923, Loss: 2.615343153476715, Final Batch Loss: 0.47517985105514526\n",
      "Epoch 2924, Loss: 2.541029393672943, Final Batch Loss: 0.481403112411499\n",
      "Epoch 2925, Loss: 2.430334448814392, Final Batch Loss: 0.35693472623825073\n",
      "Epoch 2926, Loss: 2.63408625125885, Final Batch Loss: 0.604939341545105\n",
      "Epoch 2927, Loss: 2.657003492116928, Final Batch Loss: 0.658855140209198\n",
      "Epoch 2928, Loss: 2.4707109928131104, Final Batch Loss: 0.49017009139060974\n",
      "Epoch 2929, Loss: 2.5821313858032227, Final Batch Loss: 0.5090548992156982\n",
      "Epoch 2930, Loss: 2.547119826078415, Final Batch Loss: 0.5458726286888123\n",
      "Epoch 2931, Loss: 2.5635178983211517, Final Batch Loss: 0.43773773312568665\n",
      "Epoch 2932, Loss: 2.430105060338974, Final Batch Loss: 0.4444092810153961\n",
      "Epoch 2933, Loss: 2.595974773168564, Final Batch Loss: 0.48390939831733704\n",
      "Epoch 2934, Loss: 2.597994029521942, Final Batch Loss: 0.507641077041626\n",
      "Epoch 2935, Loss: 2.5444124341011047, Final Batch Loss: 0.5485773682594299\n",
      "Epoch 2936, Loss: 2.6038019359111786, Final Batch Loss: 0.6100813746452332\n",
      "Epoch 2937, Loss: 2.659140557050705, Final Batch Loss: 0.5346620678901672\n",
      "Epoch 2938, Loss: 2.5773212909698486, Final Batch Loss: 0.5397359132766724\n",
      "Epoch 2939, Loss: 2.735653907060623, Final Batch Loss: 0.5563285946846008\n",
      "Epoch 2940, Loss: 2.686229854822159, Final Batch Loss: 0.47674059867858887\n",
      "Epoch 2941, Loss: 2.525252491235733, Final Batch Loss: 0.4789421260356903\n",
      "Epoch 2942, Loss: 2.5842230319976807, Final Batch Loss: 0.6216136813163757\n",
      "Epoch 2943, Loss: 2.660553514957428, Final Batch Loss: 0.5243974328041077\n",
      "Epoch 2944, Loss: 2.4771983325481415, Final Batch Loss: 0.4322109520435333\n",
      "Epoch 2945, Loss: 2.5128424167633057, Final Batch Loss: 0.5148215889930725\n",
      "Epoch 2946, Loss: 2.590696930885315, Final Batch Loss: 0.5173599123954773\n",
      "Epoch 2947, Loss: 2.7030038237571716, Final Batch Loss: 0.5077865719795227\n",
      "Epoch 2948, Loss: 2.487640082836151, Final Batch Loss: 0.5309836268424988\n",
      "Epoch 2949, Loss: 2.561934769153595, Final Batch Loss: 0.49867600202560425\n",
      "Epoch 2950, Loss: 2.5695986449718475, Final Batch Loss: 0.6276547908782959\n",
      "Epoch 2951, Loss: 2.6431464552879333, Final Batch Loss: 0.5296422839164734\n",
      "Epoch 2952, Loss: 2.5293302834033966, Final Batch Loss: 0.4616798758506775\n",
      "Epoch 2953, Loss: 2.549242317676544, Final Batch Loss: 0.4765678644180298\n",
      "Epoch 2954, Loss: 2.57855087518692, Final Batch Loss: 0.4412948787212372\n",
      "Epoch 2955, Loss: 2.6572065353393555, Final Batch Loss: 0.46689605712890625\n",
      "Epoch 2956, Loss: 2.546616107225418, Final Batch Loss: 0.5379905700683594\n",
      "Epoch 2957, Loss: 2.700634241104126, Final Batch Loss: 0.5240129232406616\n",
      "Epoch 2958, Loss: 2.577104300260544, Final Batch Loss: 0.5080537796020508\n",
      "Epoch 2959, Loss: 2.6220788657665253, Final Batch Loss: 0.5435537695884705\n",
      "Epoch 2960, Loss: 2.5888611972332, Final Batch Loss: 0.5510451197624207\n",
      "Epoch 2961, Loss: 2.479332208633423, Final Batch Loss: 0.47898349165916443\n",
      "Epoch 2962, Loss: 2.6208135187625885, Final Batch Loss: 0.6313346028327942\n",
      "Epoch 2963, Loss: 2.517699033021927, Final Batch Loss: 0.4712505042552948\n",
      "Epoch 2964, Loss: 2.5605868697166443, Final Batch Loss: 0.4449973404407501\n",
      "Epoch 2965, Loss: 2.7567992508411407, Final Batch Loss: 0.7672299146652222\n",
      "Epoch 2966, Loss: 2.4756320416927338, Final Batch Loss: 0.448833703994751\n",
      "Epoch 2967, Loss: 2.5911674201488495, Final Batch Loss: 0.5956745743751526\n",
      "Epoch 2968, Loss: 2.6708127856254578, Final Batch Loss: 0.5001047253608704\n",
      "Epoch 2969, Loss: 2.4904306530952454, Final Batch Loss: 0.49193504452705383\n",
      "Epoch 2970, Loss: 2.6427149176597595, Final Batch Loss: 0.4713858366012573\n",
      "Epoch 2971, Loss: 2.5961913466453552, Final Batch Loss: 0.5109891891479492\n",
      "Epoch 2972, Loss: 2.296522229909897, Final Batch Loss: 0.37193870544433594\n",
      "Epoch 2973, Loss: 2.6988419890403748, Final Batch Loss: 0.519524097442627\n",
      "Epoch 2974, Loss: 2.391392707824707, Final Batch Loss: 0.4660567045211792\n",
      "Epoch 2975, Loss: 2.515907198190689, Final Batch Loss: 0.47355446219444275\n",
      "Epoch 2976, Loss: 2.771485209465027, Final Batch Loss: 0.4735336899757385\n",
      "Epoch 2977, Loss: 3.005186975002289, Final Batch Loss: 0.6254934072494507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2978, Loss: 2.5763508677482605, Final Batch Loss: 0.47397828102111816\n",
      "Epoch 2979, Loss: 2.5597926676273346, Final Batch Loss: 0.5285537242889404\n",
      "Epoch 2980, Loss: 2.4563373923301697, Final Batch Loss: 0.44677430391311646\n",
      "Epoch 2981, Loss: 2.5596620738506317, Final Batch Loss: 0.617937445640564\n",
      "Epoch 2982, Loss: 2.56224063038826, Final Batch Loss: 0.4692703187465668\n",
      "Epoch 2983, Loss: 2.5730130672454834, Final Batch Loss: 0.5111401081085205\n",
      "Epoch 2984, Loss: 2.4116188287734985, Final Batch Loss: 0.5274633765220642\n",
      "Epoch 2985, Loss: 2.6299868524074554, Final Batch Loss: 0.4903058707714081\n",
      "Epoch 2986, Loss: 2.638506382703781, Final Batch Loss: 0.5919747948646545\n",
      "Epoch 2987, Loss: 2.688187062740326, Final Batch Loss: 0.49566930532455444\n",
      "Epoch 2988, Loss: 2.580232709646225, Final Batch Loss: 0.5306538939476013\n",
      "Epoch 2989, Loss: 2.6068990230560303, Final Batch Loss: 0.5163112282752991\n",
      "Epoch 2990, Loss: 2.755553811788559, Final Batch Loss: 0.652353823184967\n",
      "Epoch 2991, Loss: 2.600109487771988, Final Batch Loss: 0.48249948024749756\n",
      "Epoch 2992, Loss: 2.47657573223114, Final Batch Loss: 0.4316112995147705\n",
      "Epoch 2993, Loss: 2.706385225057602, Final Batch Loss: 0.5960212349891663\n",
      "Epoch 2994, Loss: 2.5167517960071564, Final Batch Loss: 0.5685160160064697\n",
      "Epoch 2995, Loss: 2.6442733108997345, Final Batch Loss: 0.6155321598052979\n",
      "Epoch 2996, Loss: 2.5373511910438538, Final Batch Loss: 0.4765394628047943\n",
      "Epoch 2997, Loss: 2.6603257060050964, Final Batch Loss: 0.56223064661026\n",
      "Epoch 2998, Loss: 2.643686354160309, Final Batch Loss: 0.6114329695701599\n",
      "Epoch 2999, Loss: 2.601175993680954, Final Batch Loss: 0.49802687764167786\n",
      "Epoch 3000, Loss: 2.517271935939789, Final Batch Loss: 0.491468220949173\n",
      "Epoch 3001, Loss: 2.4933861792087555, Final Batch Loss: 0.537162721157074\n",
      "Epoch 3002, Loss: 2.5754796862602234, Final Batch Loss: 0.4000374972820282\n",
      "Epoch 3003, Loss: 2.5172463953495026, Final Batch Loss: 0.5011017322540283\n",
      "Epoch 3004, Loss: 2.4303662478923798, Final Batch Loss: 0.3975115418434143\n",
      "Epoch 3005, Loss: 2.538911610841751, Final Batch Loss: 0.5684078335762024\n",
      "Epoch 3006, Loss: 2.562631130218506, Final Batch Loss: 0.5240350365638733\n",
      "Epoch 3007, Loss: 2.4876291751861572, Final Batch Loss: 0.4296106696128845\n",
      "Epoch 3008, Loss: 2.4691549241542816, Final Batch Loss: 0.46303921937942505\n",
      "Epoch 3009, Loss: 2.6804061233997345, Final Batch Loss: 0.7021578550338745\n",
      "Epoch 3010, Loss: 2.4851624965667725, Final Batch Loss: 0.5113949775695801\n",
      "Epoch 3011, Loss: 2.6218623518943787, Final Batch Loss: 0.5310196280479431\n",
      "Epoch 3012, Loss: 2.5602213740348816, Final Batch Loss: 0.5273808836936951\n",
      "Epoch 3013, Loss: 2.4856660068035126, Final Batch Loss: 0.4820791184902191\n",
      "Epoch 3014, Loss: 2.457527905702591, Final Batch Loss: 0.49857792258262634\n",
      "Epoch 3015, Loss: 2.5227527618408203, Final Batch Loss: 0.5305768251419067\n",
      "Epoch 3016, Loss: 2.517062187194824, Final Batch Loss: 0.5735626816749573\n",
      "Epoch 3017, Loss: 2.5576780438423157, Final Batch Loss: 0.5298872590065002\n",
      "Epoch 3018, Loss: 2.6562956273555756, Final Batch Loss: 0.5442782044410706\n",
      "Epoch 3019, Loss: 2.506663829088211, Final Batch Loss: 0.45621490478515625\n",
      "Epoch 3020, Loss: 2.7321596145629883, Final Batch Loss: 0.5855645537376404\n",
      "Epoch 3021, Loss: 2.5227259695529938, Final Batch Loss: 0.5674077272415161\n",
      "Epoch 3022, Loss: 2.6908537447452545, Final Batch Loss: 0.4706933796405792\n",
      "Epoch 3023, Loss: 2.5270619690418243, Final Batch Loss: 0.49426522850990295\n",
      "Epoch 3024, Loss: 2.524257868528366, Final Batch Loss: 0.5972952246665955\n",
      "Epoch 3025, Loss: 2.6821232438087463, Final Batch Loss: 0.5516653060913086\n",
      "Epoch 3026, Loss: 2.4430914223194122, Final Batch Loss: 0.48304569721221924\n",
      "Epoch 3027, Loss: 2.5110802948474884, Final Batch Loss: 0.40651339292526245\n",
      "Epoch 3028, Loss: 2.47716560959816, Final Batch Loss: 0.5152985453605652\n",
      "Epoch 3029, Loss: 2.474549740552902, Final Batch Loss: 0.4358450770378113\n",
      "Epoch 3030, Loss: 2.620183676481247, Final Batch Loss: 0.5211747884750366\n",
      "Epoch 3031, Loss: 2.475259095430374, Final Batch Loss: 0.48493677377700806\n",
      "Epoch 3032, Loss: 2.5389366447925568, Final Batch Loss: 0.42829421162605286\n",
      "Epoch 3033, Loss: 2.4377735257148743, Final Batch Loss: 0.469780296087265\n",
      "Epoch 3034, Loss: 2.56749027967453, Final Batch Loss: 0.47357577085494995\n",
      "Epoch 3035, Loss: 2.3794502317905426, Final Batch Loss: 0.42539793252944946\n",
      "Epoch 3036, Loss: 2.602756053209305, Final Batch Loss: 0.6115790605545044\n",
      "Epoch 3037, Loss: 2.65823170542717, Final Batch Loss: 0.5661783814430237\n",
      "Epoch 3038, Loss: 2.5712934136390686, Final Batch Loss: 0.4751839339733124\n",
      "Epoch 3039, Loss: 2.5591752529144287, Final Batch Loss: 0.40310320258140564\n",
      "Epoch 3040, Loss: 2.4904857873916626, Final Batch Loss: 0.43021222949028015\n",
      "Epoch 3041, Loss: 2.5961127281188965, Final Batch Loss: 0.5538468360900879\n",
      "Epoch 3042, Loss: 2.651125431060791, Final Batch Loss: 0.54393070936203\n",
      "Epoch 3043, Loss: 2.43971911072731, Final Batch Loss: 0.5286934971809387\n",
      "Epoch 3044, Loss: 2.6439397037029266, Final Batch Loss: 0.6564450263977051\n",
      "Epoch 3045, Loss: 2.661519020795822, Final Batch Loss: 0.5849889516830444\n",
      "Epoch 3046, Loss: 2.485270708799362, Final Batch Loss: 0.441967248916626\n",
      "Epoch 3047, Loss: 2.8115111589431763, Final Batch Loss: 0.5757917165756226\n",
      "Epoch 3048, Loss: 2.4939263463020325, Final Batch Loss: 0.4825604259967804\n",
      "Epoch 3049, Loss: 2.715390235185623, Final Batch Loss: 0.647434651851654\n",
      "Epoch 3050, Loss: 2.6720127165317535, Final Batch Loss: 0.5510547757148743\n",
      "Epoch 3051, Loss: 2.801445573568344, Final Batch Loss: 0.6144729852676392\n",
      "Epoch 3052, Loss: 2.6277593076229095, Final Batch Loss: 0.44418030977249146\n",
      "Epoch 3053, Loss: 2.4076163470745087, Final Batch Loss: 0.4521644711494446\n",
      "Epoch 3054, Loss: 2.5614100992679596, Final Batch Loss: 0.4500952363014221\n",
      "Epoch 3055, Loss: 2.7214044630527496, Final Batch Loss: 0.5125159025192261\n",
      "Epoch 3056, Loss: 2.6404866576194763, Final Batch Loss: 0.5043215751647949\n",
      "Epoch 3057, Loss: 2.4984101951122284, Final Batch Loss: 0.5071538090705872\n",
      "Epoch 3058, Loss: 2.7097270488739014, Final Batch Loss: 0.4372393786907196\n",
      "Epoch 3059, Loss: 2.5143967270851135, Final Batch Loss: 0.4709048569202423\n",
      "Epoch 3060, Loss: 2.8931598365306854, Final Batch Loss: 0.7175618410110474\n",
      "Epoch 3061, Loss: 2.5599942803382874, Final Batch Loss: 0.6051941514015198\n",
      "Epoch 3062, Loss: 2.4624758660793304, Final Batch Loss: 0.4541164040565491\n",
      "Epoch 3063, Loss: 2.6068702042102814, Final Batch Loss: 0.5790385007858276\n",
      "Epoch 3064, Loss: 2.485696107149124, Final Batch Loss: 0.3661607801914215\n",
      "Epoch 3065, Loss: 2.562345653772354, Final Batch Loss: 0.5222713947296143\n",
      "Epoch 3066, Loss: 2.5137810707092285, Final Batch Loss: 0.530189037322998\n",
      "Epoch 3067, Loss: 2.605584055185318, Final Batch Loss: 0.47872665524482727\n",
      "Epoch 3068, Loss: 2.6528949439525604, Final Batch Loss: 0.5468316674232483\n",
      "Epoch 3069, Loss: 2.5661500096321106, Final Batch Loss: 0.5083901882171631\n",
      "Epoch 3070, Loss: 2.60701185464859, Final Batch Loss: 0.5716412663459778\n",
      "Epoch 3071, Loss: 2.490731418132782, Final Batch Loss: 0.6016496419906616\n",
      "Epoch 3072, Loss: 2.683231443166733, Final Batch Loss: 0.6110562086105347\n",
      "Epoch 3073, Loss: 2.4908326268196106, Final Batch Loss: 0.49364718794822693\n",
      "Epoch 3074, Loss: 2.692692756652832, Final Batch Loss: 0.5006023645401001\n",
      "Epoch 3075, Loss: 2.5520682632923126, Final Batch Loss: 0.4394599199295044\n",
      "Epoch 3076, Loss: 2.606221169233322, Final Batch Loss: 0.5000382661819458\n",
      "Epoch 3077, Loss: 2.591099739074707, Final Batch Loss: 0.5219613313674927\n",
      "Epoch 3078, Loss: 2.481283128261566, Final Batch Loss: 0.4688704311847687\n",
      "Epoch 3079, Loss: 2.590607315301895, Final Batch Loss: 0.5781775712966919\n",
      "Epoch 3080, Loss: 2.5591295659542084, Final Batch Loss: 0.43253740668296814\n",
      "Epoch 3081, Loss: 2.511566698551178, Final Batch Loss: 0.5101688504219055\n",
      "Epoch 3082, Loss: 2.692776679992676, Final Batch Loss: 0.625485897064209\n",
      "Epoch 3083, Loss: 2.5532485842704773, Final Batch Loss: 0.5176195502281189\n",
      "Epoch 3084, Loss: 2.6777520179748535, Final Batch Loss: 0.5826427936553955\n",
      "Epoch 3085, Loss: 2.4466266334056854, Final Batch Loss: 0.4032978117465973\n",
      "Epoch 3086, Loss: 2.444624572992325, Final Batch Loss: 0.5935941338539124\n",
      "Epoch 3087, Loss: 2.4886346757411957, Final Batch Loss: 0.3941425085067749\n",
      "Epoch 3088, Loss: 2.626274913549423, Final Batch Loss: 0.6263513565063477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3089, Loss: 2.6378209590911865, Final Batch Loss: 0.6007247567176819\n",
      "Epoch 3090, Loss: 2.6223093271255493, Final Batch Loss: 0.5360783934593201\n",
      "Epoch 3091, Loss: 2.4917730689048767, Final Batch Loss: 0.433601975440979\n",
      "Epoch 3092, Loss: 2.533186435699463, Final Batch Loss: 0.5407537817955017\n",
      "Epoch 3093, Loss: 2.544210433959961, Final Batch Loss: 0.4173785150051117\n",
      "Epoch 3094, Loss: 2.641255646944046, Final Batch Loss: 0.5419561862945557\n",
      "Epoch 3095, Loss: 2.5808839201927185, Final Batch Loss: 0.5016136169433594\n",
      "Epoch 3096, Loss: 2.612724721431732, Final Batch Loss: 0.496250718832016\n",
      "Epoch 3097, Loss: 2.562191069126129, Final Batch Loss: 0.5963231325149536\n",
      "Epoch 3098, Loss: 2.4745554625988007, Final Batch Loss: 0.4694013297557831\n",
      "Epoch 3099, Loss: 2.493755668401718, Final Batch Loss: 0.4111408293247223\n",
      "Epoch 3100, Loss: 2.5994734466075897, Final Batch Loss: 0.5407759547233582\n",
      "Epoch 3101, Loss: 2.5730616748332977, Final Batch Loss: 0.5171722173690796\n",
      "Epoch 3102, Loss: 2.428056091070175, Final Batch Loss: 0.5239335298538208\n",
      "Epoch 3103, Loss: 2.4601613581180573, Final Batch Loss: 0.5570101141929626\n",
      "Epoch 3104, Loss: 2.49936243891716, Final Batch Loss: 0.4806094467639923\n",
      "Epoch 3105, Loss: 2.4770240485668182, Final Batch Loss: 0.5460985898971558\n",
      "Epoch 3106, Loss: 2.4071095287799835, Final Batch Loss: 0.3441923260688782\n",
      "Epoch 3107, Loss: 2.6984938383102417, Final Batch Loss: 0.6937042474746704\n",
      "Epoch 3108, Loss: 2.6495571434497833, Final Batch Loss: 0.5906675457954407\n",
      "Epoch 3109, Loss: 2.5139988362789154, Final Batch Loss: 0.5493467450141907\n",
      "Epoch 3110, Loss: 2.4796961545944214, Final Batch Loss: 0.5188221335411072\n",
      "Epoch 3111, Loss: 2.5259034037590027, Final Batch Loss: 0.49320778250694275\n",
      "Epoch 3112, Loss: 2.6179538667201996, Final Batch Loss: 0.4987456798553467\n",
      "Epoch 3113, Loss: 2.680001676082611, Final Batch Loss: 0.6650642156600952\n",
      "Epoch 3114, Loss: 2.4084337055683136, Final Batch Loss: 0.47602686285972595\n",
      "Epoch 3115, Loss: 2.6014334857463837, Final Batch Loss: 0.4597998261451721\n",
      "Epoch 3116, Loss: 2.386917233467102, Final Batch Loss: 0.3996097445487976\n",
      "Epoch 3117, Loss: 2.6848451793193817, Final Batch Loss: 0.5172036290168762\n",
      "Epoch 3118, Loss: 2.6140095591545105, Final Batch Loss: 0.48291194438934326\n",
      "Epoch 3119, Loss: 2.6334780156612396, Final Batch Loss: 0.4666961133480072\n",
      "Epoch 3120, Loss: 2.4328605830669403, Final Batch Loss: 0.4203656017780304\n",
      "Epoch 3121, Loss: 2.4935795664787292, Final Batch Loss: 0.5857843160629272\n",
      "Epoch 3122, Loss: 2.612139970064163, Final Batch Loss: 0.5999724864959717\n",
      "Epoch 3123, Loss: 2.5123479068279266, Final Batch Loss: 0.5718204379081726\n",
      "Epoch 3124, Loss: 2.6705050468444824, Final Batch Loss: 0.4971029460430145\n",
      "Epoch 3125, Loss: 2.6610618233680725, Final Batch Loss: 0.5575748085975647\n",
      "Epoch 3126, Loss: 2.669792801141739, Final Batch Loss: 0.571122407913208\n",
      "Epoch 3127, Loss: 2.5256205201148987, Final Batch Loss: 0.5771206021308899\n",
      "Epoch 3128, Loss: 2.4885836243629456, Final Batch Loss: 0.4736535847187042\n",
      "Epoch 3129, Loss: 2.6334491968154907, Final Batch Loss: 0.4769973158836365\n",
      "Epoch 3130, Loss: 2.6513831317424774, Final Batch Loss: 0.5576649308204651\n",
      "Epoch 3131, Loss: 2.481923222541809, Final Batch Loss: 0.4721078872680664\n",
      "Epoch 3132, Loss: 2.479752153158188, Final Batch Loss: 0.49786388874053955\n",
      "Epoch 3133, Loss: 2.5345808267593384, Final Batch Loss: 0.5593828558921814\n",
      "Epoch 3134, Loss: 2.596109241247177, Final Batch Loss: 0.44522231817245483\n",
      "Epoch 3135, Loss: 2.5839966237545013, Final Batch Loss: 0.3932148814201355\n",
      "Epoch 3136, Loss: 2.536961644887924, Final Batch Loss: 0.431684285402298\n",
      "Epoch 3137, Loss: 2.7109382152557373, Final Batch Loss: 0.6757795810699463\n",
      "Epoch 3138, Loss: 2.4441057443618774, Final Batch Loss: 0.4665178954601288\n",
      "Epoch 3139, Loss: 2.526138722896576, Final Batch Loss: 0.4031427502632141\n",
      "Epoch 3140, Loss: 2.620384782552719, Final Batch Loss: 0.5400698184967041\n",
      "Epoch 3141, Loss: 2.5550615787506104, Final Batch Loss: 0.5597186088562012\n",
      "Epoch 3142, Loss: 2.342441648244858, Final Batch Loss: 0.3829672336578369\n",
      "Epoch 3143, Loss: 2.5608100593090057, Final Batch Loss: 0.5107585191726685\n",
      "Epoch 3144, Loss: 2.6971156299114227, Final Batch Loss: 0.4909887909889221\n",
      "Epoch 3145, Loss: 2.666941523551941, Final Batch Loss: 0.6194590926170349\n",
      "Epoch 3146, Loss: 2.5132473409175873, Final Batch Loss: 0.5429945588111877\n",
      "Epoch 3147, Loss: 2.4616300463676453, Final Batch Loss: 0.5675797462463379\n",
      "Epoch 3148, Loss: 2.604611486196518, Final Batch Loss: 0.4228046238422394\n",
      "Epoch 3149, Loss: 2.4299312829971313, Final Batch Loss: 0.48513808846473694\n",
      "Epoch 3150, Loss: 2.506974995136261, Final Batch Loss: 0.5597163438796997\n",
      "Epoch 3151, Loss: 2.7998392283916473, Final Batch Loss: 0.6638903617858887\n",
      "Epoch 3152, Loss: 2.486949473619461, Final Batch Loss: 0.4955734312534332\n",
      "Epoch 3153, Loss: 2.5140086710453033, Final Batch Loss: 0.4995606541633606\n",
      "Epoch 3154, Loss: 2.583436816930771, Final Batch Loss: 0.4500637650489807\n",
      "Epoch 3155, Loss: 2.423767328262329, Final Batch Loss: 0.43930232524871826\n",
      "Epoch 3156, Loss: 2.557103753089905, Final Batch Loss: 0.6031250953674316\n",
      "Epoch 3157, Loss: 2.60866317152977, Final Batch Loss: 0.44248324632644653\n",
      "Epoch 3158, Loss: 2.6127242147922516, Final Batch Loss: 0.4747115969657898\n",
      "Epoch 3159, Loss: 2.6107507944107056, Final Batch Loss: 0.43214648962020874\n",
      "Epoch 3160, Loss: 2.5153351426124573, Final Batch Loss: 0.5742926597595215\n",
      "Epoch 3161, Loss: 2.6426082849502563, Final Batch Loss: 0.539955198764801\n",
      "Epoch 3162, Loss: 2.511433333158493, Final Batch Loss: 0.5637344121932983\n",
      "Epoch 3163, Loss: 2.4432155787944794, Final Batch Loss: 0.3444436192512512\n",
      "Epoch 3164, Loss: 2.5631230175495148, Final Batch Loss: 0.5108799934387207\n",
      "Epoch 3165, Loss: 2.548491418361664, Final Batch Loss: 0.4764256477355957\n",
      "Epoch 3166, Loss: 2.4553313553333282, Final Batch Loss: 0.4861195385456085\n",
      "Epoch 3167, Loss: 2.612703502178192, Final Batch Loss: 0.4501747488975525\n",
      "Epoch 3168, Loss: 2.8511246740818024, Final Batch Loss: 0.614221453666687\n",
      "Epoch 3169, Loss: 2.6247412264347076, Final Batch Loss: 0.5733441710472107\n",
      "Epoch 3170, Loss: 2.6438898146152496, Final Batch Loss: 0.43112894892692566\n",
      "Epoch 3171, Loss: 2.717320203781128, Final Batch Loss: 0.5827513337135315\n",
      "Epoch 3172, Loss: 2.599656194448471, Final Batch Loss: 0.491537481546402\n",
      "Epoch 3173, Loss: 2.4595335125923157, Final Batch Loss: 0.5808984637260437\n",
      "Epoch 3174, Loss: 2.494422137737274, Final Batch Loss: 0.5675119757652283\n",
      "Epoch 3175, Loss: 2.4955539107322693, Final Batch Loss: 0.5083694458007812\n",
      "Epoch 3176, Loss: 2.621921807527542, Final Batch Loss: 0.47559940814971924\n",
      "Epoch 3177, Loss: 2.6224091351032257, Final Batch Loss: 0.5252292156219482\n",
      "Epoch 3178, Loss: 2.5857918858528137, Final Batch Loss: 0.5547184348106384\n",
      "Epoch 3179, Loss: 2.55477175116539, Final Batch Loss: 0.6366733908653259\n",
      "Epoch 3180, Loss: 2.378186136484146, Final Batch Loss: 0.4359625577926636\n",
      "Epoch 3181, Loss: 2.4349610805511475, Final Batch Loss: 0.4761364161968231\n",
      "Epoch 3182, Loss: 2.827790141105652, Final Batch Loss: 0.6611615419387817\n",
      "Epoch 3183, Loss: 2.6209091246128082, Final Batch Loss: 0.5994480848312378\n",
      "Epoch 3184, Loss: 2.664614647626877, Final Batch Loss: 0.596839427947998\n",
      "Epoch 3185, Loss: 2.5592422485351562, Final Batch Loss: 0.49528470635414124\n",
      "Epoch 3186, Loss: 2.4215492606163025, Final Batch Loss: 0.5696840882301331\n",
      "Epoch 3187, Loss: 2.6497344970703125, Final Batch Loss: 0.6057649254798889\n",
      "Epoch 3188, Loss: 2.5378623604774475, Final Batch Loss: 0.4894503951072693\n",
      "Epoch 3189, Loss: 2.4832027554512024, Final Batch Loss: 0.4015374481678009\n",
      "Epoch 3190, Loss: 2.7463403046131134, Final Batch Loss: 0.6412484049797058\n",
      "Epoch 3191, Loss: 2.5278067886829376, Final Batch Loss: 0.5234978795051575\n",
      "Epoch 3192, Loss: 2.7799642384052277, Final Batch Loss: 0.747309148311615\n",
      "Epoch 3193, Loss: 2.5532644987106323, Final Batch Loss: 0.5341411828994751\n",
      "Epoch 3194, Loss: 2.5385953783988953, Final Batch Loss: 0.597603976726532\n",
      "Epoch 3195, Loss: 2.359131336212158, Final Batch Loss: 0.39553484320640564\n",
      "Epoch 3196, Loss: 2.6257337629795074, Final Batch Loss: 0.579849123954773\n",
      "Epoch 3197, Loss: 2.4256224036216736, Final Batch Loss: 0.4199438989162445\n",
      "Epoch 3198, Loss: 2.5815903544425964, Final Batch Loss: 0.46540331840515137\n",
      "Epoch 3199, Loss: 2.4019394516944885, Final Batch Loss: 0.456697016954422\n",
      "Epoch 3200, Loss: 2.4845798015594482, Final Batch Loss: 0.548088550567627\n",
      "Epoch 3201, Loss: 2.5759454667568207, Final Batch Loss: 0.5614255666732788\n",
      "Epoch 3202, Loss: 2.6144308149814606, Final Batch Loss: 0.40558329224586487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3203, Loss: 2.364265412092209, Final Batch Loss: 0.4457755386829376\n",
      "Epoch 3204, Loss: 2.6433855295181274, Final Batch Loss: 0.47625601291656494\n",
      "Epoch 3205, Loss: 2.5423781275749207, Final Batch Loss: 0.4370480179786682\n",
      "Epoch 3206, Loss: 2.6201221644878387, Final Batch Loss: 0.6377884149551392\n",
      "Epoch 3207, Loss: 2.684761643409729, Final Batch Loss: 0.46677103638648987\n",
      "Epoch 3208, Loss: 2.582165837287903, Final Batch Loss: 0.688399612903595\n",
      "Epoch 3209, Loss: 2.4331945180892944, Final Batch Loss: 0.43446341156959534\n",
      "Epoch 3210, Loss: 2.5137827396392822, Final Batch Loss: 0.45553672313690186\n",
      "Epoch 3211, Loss: 2.6884802281856537, Final Batch Loss: 0.5550239086151123\n",
      "Epoch 3212, Loss: 2.6541097164154053, Final Batch Loss: 0.5453125238418579\n",
      "Epoch 3213, Loss: 2.4023992121219635, Final Batch Loss: 0.4058918356895447\n",
      "Epoch 3214, Loss: 2.420997768640518, Final Batch Loss: 0.395495742559433\n",
      "Epoch 3215, Loss: 2.480489432811737, Final Batch Loss: 0.3509965240955353\n",
      "Epoch 3216, Loss: 2.664601296186447, Final Batch Loss: 0.6667972803115845\n",
      "Epoch 3217, Loss: 2.626517415046692, Final Batch Loss: 0.5550546646118164\n",
      "Epoch 3218, Loss: 2.577418565750122, Final Batch Loss: 0.5317943692207336\n",
      "Epoch 3219, Loss: 2.4002692997455597, Final Batch Loss: 0.44814684987068176\n",
      "Epoch 3220, Loss: 2.445366770029068, Final Batch Loss: 0.439374178647995\n",
      "Epoch 3221, Loss: 2.6051154732704163, Final Batch Loss: 0.516491174697876\n",
      "Epoch 3222, Loss: 2.7074292600154877, Final Batch Loss: 0.42867693305015564\n",
      "Epoch 3223, Loss: 2.432383745908737, Final Batch Loss: 0.4549819529056549\n",
      "Epoch 3224, Loss: 2.5189476013183594, Final Batch Loss: 0.49300920963287354\n",
      "Epoch 3225, Loss: 2.559702843427658, Final Batch Loss: 0.5204464793205261\n",
      "Epoch 3226, Loss: 2.6516301929950714, Final Batch Loss: 0.5209958553314209\n",
      "Epoch 3227, Loss: 2.610982120037079, Final Batch Loss: 0.5158777236938477\n",
      "Epoch 3228, Loss: 2.596838653087616, Final Batch Loss: 0.4950043857097626\n",
      "Epoch 3229, Loss: 2.399829536676407, Final Batch Loss: 0.5447721481323242\n",
      "Epoch 3230, Loss: 2.5884721279144287, Final Batch Loss: 0.5152539014816284\n",
      "Epoch 3231, Loss: 2.545916885137558, Final Batch Loss: 0.456363707780838\n",
      "Epoch 3232, Loss: 2.537130892276764, Final Batch Loss: 0.5874051451683044\n",
      "Epoch 3233, Loss: 2.430591493844986, Final Batch Loss: 0.4408968687057495\n",
      "Epoch 3234, Loss: 2.407342940568924, Final Batch Loss: 0.3893767297267914\n",
      "Epoch 3235, Loss: 2.6017411053180695, Final Batch Loss: 0.4842371344566345\n",
      "Epoch 3236, Loss: 2.5335626304149628, Final Batch Loss: 0.4841218590736389\n",
      "Epoch 3237, Loss: 2.621362656354904, Final Batch Loss: 0.547157883644104\n",
      "Epoch 3238, Loss: 2.438466876745224, Final Batch Loss: 0.4471425414085388\n",
      "Epoch 3239, Loss: 2.628042757511139, Final Batch Loss: 0.43072521686553955\n",
      "Epoch 3240, Loss: 2.5181266963481903, Final Batch Loss: 0.5118876099586487\n",
      "Epoch 3241, Loss: 2.517221063375473, Final Batch Loss: 0.5914663672447205\n",
      "Epoch 3242, Loss: 2.4741113781929016, Final Batch Loss: 0.43518275022506714\n",
      "Epoch 3243, Loss: 2.4191280603408813, Final Batch Loss: 0.49583256244659424\n",
      "Epoch 3244, Loss: 2.3673455119132996, Final Batch Loss: 0.3607349395751953\n",
      "Epoch 3245, Loss: 2.655494272708893, Final Batch Loss: 0.6544958353042603\n",
      "Epoch 3246, Loss: 2.384004384279251, Final Batch Loss: 0.5214778780937195\n",
      "Epoch 3247, Loss: 2.5451190769672394, Final Batch Loss: 0.4868910610675812\n",
      "Epoch 3248, Loss: 2.4468618035316467, Final Batch Loss: 0.5015062093734741\n",
      "Epoch 3249, Loss: 2.5728648900985718, Final Batch Loss: 0.5930426120758057\n",
      "Epoch 3250, Loss: 2.5425787568092346, Final Batch Loss: 0.4540470540523529\n",
      "Epoch 3251, Loss: 2.646164357662201, Final Batch Loss: 0.4795299172401428\n",
      "Epoch 3252, Loss: 2.5825027227401733, Final Batch Loss: 0.5974090695381165\n",
      "Epoch 3253, Loss: 2.407947927713394, Final Batch Loss: 0.49472346901893616\n",
      "Epoch 3254, Loss: 2.565737634897232, Final Batch Loss: 0.5111093521118164\n",
      "Epoch 3255, Loss: 2.425983726978302, Final Batch Loss: 0.4473652243614197\n",
      "Epoch 3256, Loss: 2.626584082841873, Final Batch Loss: 0.47951364517211914\n",
      "Epoch 3257, Loss: 2.451524168252945, Final Batch Loss: 0.5205115675926208\n",
      "Epoch 3258, Loss: 2.7280947268009186, Final Batch Loss: 0.5541800856590271\n",
      "Epoch 3259, Loss: 2.635525643825531, Final Batch Loss: 0.6204209923744202\n",
      "Epoch 3260, Loss: 2.7244875729084015, Final Batch Loss: 0.5318514704704285\n",
      "Epoch 3261, Loss: 2.5181640088558197, Final Batch Loss: 0.5755872130393982\n",
      "Epoch 3262, Loss: 2.623564302921295, Final Batch Loss: 0.56252121925354\n",
      "Epoch 3263, Loss: 2.497792035341263, Final Batch Loss: 0.4785016179084778\n",
      "Epoch 3264, Loss: 2.5381057262420654, Final Batch Loss: 0.4248417317867279\n",
      "Epoch 3265, Loss: 2.5461140275001526, Final Batch Loss: 0.6908457279205322\n",
      "Epoch 3266, Loss: 2.666767716407776, Final Batch Loss: 0.6207753419876099\n",
      "Epoch 3267, Loss: 2.5214900970458984, Final Batch Loss: 0.5440186262130737\n",
      "Epoch 3268, Loss: 2.417871594429016, Final Batch Loss: 0.43541163206100464\n",
      "Epoch 3269, Loss: 2.5310843884944916, Final Batch Loss: 0.5581471920013428\n",
      "Epoch 3270, Loss: 2.553102523088455, Final Batch Loss: 0.5278013944625854\n",
      "Epoch 3271, Loss: 2.398758441209793, Final Batch Loss: 0.5011658668518066\n",
      "Epoch 3272, Loss: 2.436793953180313, Final Batch Loss: 0.4732145071029663\n",
      "Epoch 3273, Loss: 2.3917600512504578, Final Batch Loss: 0.3979693353176117\n",
      "Epoch 3274, Loss: 2.491044729948044, Final Batch Loss: 0.4422842562198639\n",
      "Epoch 3275, Loss: 2.5030320584774017, Final Batch Loss: 0.46032604575157166\n",
      "Epoch 3276, Loss: 2.4956665635108948, Final Batch Loss: 0.472002238035202\n",
      "Epoch 3277, Loss: 2.5212807059288025, Final Batch Loss: 0.43781161308288574\n",
      "Epoch 3278, Loss: 2.527900844812393, Final Batch Loss: 0.4354546070098877\n",
      "Epoch 3279, Loss: 2.457870692014694, Final Batch Loss: 0.4848654270172119\n",
      "Epoch 3280, Loss: 2.5093474984169006, Final Batch Loss: 0.3956681191921234\n",
      "Epoch 3281, Loss: 2.4927286207675934, Final Batch Loss: 0.45323002338409424\n",
      "Epoch 3282, Loss: 2.487495481967926, Final Batch Loss: 0.4752250015735626\n",
      "Epoch 3283, Loss: 2.4416443705558777, Final Batch Loss: 0.538716733455658\n",
      "Epoch 3284, Loss: 2.455604135990143, Final Batch Loss: 0.49446341395378113\n",
      "Epoch 3285, Loss: 2.6312229335308075, Final Batch Loss: 0.5717700719833374\n",
      "Epoch 3286, Loss: 2.6564543545246124, Final Batch Loss: 0.44239670038223267\n",
      "Epoch 3287, Loss: 2.620311349630356, Final Batch Loss: 0.49982866644859314\n",
      "Epoch 3288, Loss: 2.6256172358989716, Final Batch Loss: 0.7761572003364563\n",
      "Epoch 3289, Loss: 2.4904231429100037, Final Batch Loss: 0.47177961468696594\n",
      "Epoch 3290, Loss: 2.5604343116283417, Final Batch Loss: 0.5985168218612671\n",
      "Epoch 3291, Loss: 2.6104364097118378, Final Batch Loss: 0.4609397053718567\n",
      "Epoch 3292, Loss: 2.493071049451828, Final Batch Loss: 0.5040093064308167\n",
      "Epoch 3293, Loss: 2.488903820514679, Final Batch Loss: 0.4730197787284851\n",
      "Epoch 3294, Loss: 2.801362156867981, Final Batch Loss: 0.5423173904418945\n",
      "Epoch 3295, Loss: 2.5623163282871246, Final Batch Loss: 0.5754271149635315\n",
      "Epoch 3296, Loss: 2.5428511202335358, Final Batch Loss: 0.47027286887168884\n",
      "Epoch 3297, Loss: 2.6173418760299683, Final Batch Loss: 0.5627407431602478\n",
      "Epoch 3298, Loss: 2.4494950473308563, Final Batch Loss: 0.43329939246177673\n",
      "Epoch 3299, Loss: 2.575131982564926, Final Batch Loss: 0.5206232666969299\n",
      "Epoch 3300, Loss: 2.58785742521286, Final Batch Loss: 0.5960516929626465\n",
      "Epoch 3301, Loss: 2.534990966320038, Final Batch Loss: 0.45380547642707825\n",
      "Epoch 3302, Loss: 2.488652229309082, Final Batch Loss: 0.4783160984516144\n",
      "Epoch 3303, Loss: 2.8208138942718506, Final Batch Loss: 0.6031914353370667\n",
      "Epoch 3304, Loss: 2.749641329050064, Final Batch Loss: 0.5630908012390137\n",
      "Epoch 3305, Loss: 2.4011174142360687, Final Batch Loss: 0.3679681420326233\n",
      "Epoch 3306, Loss: 2.569800764322281, Final Batch Loss: 0.47802749276161194\n",
      "Epoch 3307, Loss: 2.7981943786144257, Final Batch Loss: 0.617886483669281\n",
      "Epoch 3308, Loss: 2.438143312931061, Final Batch Loss: 0.4676373600959778\n",
      "Epoch 3309, Loss: 2.480152428150177, Final Batch Loss: 0.5819199681282043\n",
      "Epoch 3310, Loss: 2.410270005464554, Final Batch Loss: 0.4975666403770447\n",
      "Epoch 3311, Loss: 2.6383641064167023, Final Batch Loss: 0.447092205286026\n",
      "Epoch 3312, Loss: 2.426801085472107, Final Batch Loss: 0.4619359076023102\n",
      "Epoch 3313, Loss: 2.4752086102962494, Final Batch Loss: 0.47270822525024414\n",
      "Epoch 3314, Loss: 2.6166003346443176, Final Batch Loss: 0.4801788330078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3315, Loss: 2.473234713077545, Final Batch Loss: 0.5246837735176086\n",
      "Epoch 3316, Loss: 2.5307578444480896, Final Batch Loss: 0.46552449464797974\n",
      "Epoch 3317, Loss: 2.4943372011184692, Final Batch Loss: 0.5304932594299316\n",
      "Epoch 3318, Loss: 2.4969406127929688, Final Batch Loss: 0.46014639735221863\n",
      "Epoch 3319, Loss: 2.488741844892502, Final Batch Loss: 0.4711248576641083\n",
      "Epoch 3320, Loss: 2.67946657538414, Final Batch Loss: 0.5062101483345032\n",
      "Epoch 3321, Loss: 2.449607789516449, Final Batch Loss: 0.5210629105567932\n",
      "Epoch 3322, Loss: 2.763679474592209, Final Batch Loss: 0.5189365148544312\n",
      "Epoch 3323, Loss: 2.5572321116924286, Final Batch Loss: 0.48125043511390686\n",
      "Epoch 3324, Loss: 2.5046333372592926, Final Batch Loss: 0.47601601481437683\n",
      "Epoch 3325, Loss: 2.574688047170639, Final Batch Loss: 0.5235136151313782\n",
      "Epoch 3326, Loss: 2.5106641054153442, Final Batch Loss: 0.4807969629764557\n",
      "Epoch 3327, Loss: 2.4119242429733276, Final Batch Loss: 0.4761374592781067\n",
      "Epoch 3328, Loss: 2.5028104186058044, Final Batch Loss: 0.4623432159423828\n",
      "Epoch 3329, Loss: 2.4991294741630554, Final Batch Loss: 0.5488002896308899\n",
      "Epoch 3330, Loss: 2.426085412502289, Final Batch Loss: 0.43514949083328247\n",
      "Epoch 3331, Loss: 2.6407737731933594, Final Batch Loss: 0.5552291870117188\n",
      "Epoch 3332, Loss: 2.5689044296741486, Final Batch Loss: 0.533882737159729\n",
      "Epoch 3333, Loss: 2.4175177216529846, Final Batch Loss: 0.37815648317337036\n",
      "Epoch 3334, Loss: 2.4843544363975525, Final Batch Loss: 0.5721985101699829\n",
      "Epoch 3335, Loss: 2.623565047979355, Final Batch Loss: 0.6278561353683472\n",
      "Epoch 3336, Loss: 2.5538446605205536, Final Batch Loss: 0.5436127781867981\n",
      "Epoch 3337, Loss: 2.4855248630046844, Final Batch Loss: 0.5143341422080994\n",
      "Epoch 3338, Loss: 2.431737959384918, Final Batch Loss: 0.4734424948692322\n",
      "Epoch 3339, Loss: 2.6401306986808777, Final Batch Loss: 0.5846933126449585\n",
      "Epoch 3340, Loss: 2.375748187303543, Final Batch Loss: 0.45356133580207825\n",
      "Epoch 3341, Loss: 2.4462451338768005, Final Batch Loss: 0.4720825254917145\n",
      "Epoch 3342, Loss: 2.4109362363815308, Final Batch Loss: 0.40427589416503906\n",
      "Epoch 3343, Loss: 2.491672456264496, Final Batch Loss: 0.4750911295413971\n",
      "Epoch 3344, Loss: 2.406835615634918, Final Batch Loss: 0.4497409164905548\n",
      "Epoch 3345, Loss: 2.4296192824840546, Final Batch Loss: 0.41259440779685974\n",
      "Epoch 3346, Loss: 2.515317052602768, Final Batch Loss: 0.5805086493492126\n",
      "Epoch 3347, Loss: 2.5204520225524902, Final Batch Loss: 0.42058539390563965\n",
      "Epoch 3348, Loss: 2.769537329673767, Final Batch Loss: 0.6160959601402283\n",
      "Epoch 3349, Loss: 2.455969452857971, Final Batch Loss: 0.49283209443092346\n",
      "Epoch 3350, Loss: 2.6841851472854614, Final Batch Loss: 0.5811992287635803\n",
      "Epoch 3351, Loss: 2.454332709312439, Final Batch Loss: 0.4498143196105957\n",
      "Epoch 3352, Loss: 2.453283041715622, Final Batch Loss: 0.5089867115020752\n",
      "Epoch 3353, Loss: 2.6522186994552612, Final Batch Loss: 0.5296928286552429\n",
      "Epoch 3354, Loss: 2.465052306652069, Final Batch Loss: 0.6107677221298218\n",
      "Epoch 3355, Loss: 2.6419496834278107, Final Batch Loss: 0.6225161552429199\n",
      "Epoch 3356, Loss: 2.397809475660324, Final Batch Loss: 0.48443809151649475\n",
      "Epoch 3357, Loss: 2.3734446465969086, Final Batch Loss: 0.47159314155578613\n",
      "Epoch 3358, Loss: 2.593876928091049, Final Batch Loss: 0.5141516923904419\n",
      "Epoch 3359, Loss: 2.567331165075302, Final Batch Loss: 0.5895893573760986\n",
      "Epoch 3360, Loss: 2.5142253637313843, Final Batch Loss: 0.44309201836586\n",
      "Epoch 3361, Loss: 2.4250325858592987, Final Batch Loss: 0.4808482825756073\n",
      "Epoch 3362, Loss: 2.733210116624832, Final Batch Loss: 0.5479710698127747\n",
      "Epoch 3363, Loss: 2.445271223783493, Final Batch Loss: 0.38801586627960205\n",
      "Epoch 3364, Loss: 2.4760045409202576, Final Batch Loss: 0.4030231535434723\n",
      "Epoch 3365, Loss: 2.589172661304474, Final Batch Loss: 0.4753651022911072\n",
      "Epoch 3366, Loss: 2.55609193444252, Final Batch Loss: 0.6269135475158691\n",
      "Epoch 3367, Loss: 2.610494941473007, Final Batch Loss: 0.5698931217193604\n",
      "Epoch 3368, Loss: 2.568515181541443, Final Batch Loss: 0.6081653833389282\n",
      "Epoch 3369, Loss: 2.569770395755768, Final Batch Loss: 0.5060853362083435\n",
      "Epoch 3370, Loss: 2.4287951290607452, Final Batch Loss: 0.46655505895614624\n",
      "Epoch 3371, Loss: 2.5739161372184753, Final Batch Loss: 0.49321579933166504\n",
      "Epoch 3372, Loss: 2.3520387709140778, Final Batch Loss: 0.4124598801136017\n",
      "Epoch 3373, Loss: 2.523292660713196, Final Batch Loss: 0.7163328528404236\n",
      "Epoch 3374, Loss: 2.431390196084976, Final Batch Loss: 0.4933902621269226\n",
      "Epoch 3375, Loss: 2.4238112568855286, Final Batch Loss: 0.46488717198371887\n",
      "Epoch 3376, Loss: 2.4400523602962494, Final Batch Loss: 0.5609103441238403\n",
      "Epoch 3377, Loss: 2.625483751296997, Final Batch Loss: 0.49975380301475525\n",
      "Epoch 3378, Loss: 2.34394833445549, Final Batch Loss: 0.430602103471756\n",
      "Epoch 3379, Loss: 2.483212947845459, Final Batch Loss: 0.5780135989189148\n",
      "Epoch 3380, Loss: 2.914047449827194, Final Batch Loss: 0.5697378516197205\n",
      "Epoch 3381, Loss: 2.5624647736549377, Final Batch Loss: 0.5983103513717651\n",
      "Epoch 3382, Loss: 2.724694639444351, Final Batch Loss: 0.4699428677558899\n",
      "Epoch 3383, Loss: 2.570088714361191, Final Batch Loss: 0.5057653784751892\n",
      "Epoch 3384, Loss: 2.5439910292625427, Final Batch Loss: 0.47357797622680664\n",
      "Epoch 3385, Loss: 2.518980920314789, Final Batch Loss: 0.5609937906265259\n",
      "Epoch 3386, Loss: 2.4347138106822968, Final Batch Loss: 0.5635303258895874\n",
      "Epoch 3387, Loss: 2.5890550315380096, Final Batch Loss: 0.529564619064331\n",
      "Epoch 3388, Loss: 2.541429877281189, Final Batch Loss: 0.4897572100162506\n",
      "Epoch 3389, Loss: 2.437794655561447, Final Batch Loss: 0.5135843753814697\n",
      "Epoch 3390, Loss: 2.5749464631080627, Final Batch Loss: 0.6450251340866089\n",
      "Epoch 3391, Loss: 2.4794892072677612, Final Batch Loss: 0.48459768295288086\n",
      "Epoch 3392, Loss: 2.5009375512599945, Final Batch Loss: 0.5605155229568481\n",
      "Epoch 3393, Loss: 2.5468764901161194, Final Batch Loss: 0.5689058303833008\n",
      "Epoch 3394, Loss: 2.5956581234931946, Final Batch Loss: 0.522195041179657\n",
      "Epoch 3395, Loss: 2.507441967725754, Final Batch Loss: 0.5549519658088684\n",
      "Epoch 3396, Loss: 2.402673602104187, Final Batch Loss: 0.40236926078796387\n",
      "Epoch 3397, Loss: 2.6702563166618347, Final Batch Loss: 0.5059348940849304\n",
      "Epoch 3398, Loss: 2.324927657842636, Final Batch Loss: 0.5555651187896729\n",
      "Epoch 3399, Loss: 2.395476907491684, Final Batch Loss: 0.5764827728271484\n",
      "Epoch 3400, Loss: 2.5455308854579926, Final Batch Loss: 0.5506264567375183\n",
      "Epoch 3401, Loss: 2.5043230056762695, Final Batch Loss: 0.46245691180229187\n",
      "Epoch 3402, Loss: 2.5616016685962677, Final Batch Loss: 0.46649718284606934\n",
      "Epoch 3403, Loss: 2.4721502363681793, Final Batch Loss: 0.5120895504951477\n",
      "Epoch 3404, Loss: 2.8920083045959473, Final Batch Loss: 0.6720570921897888\n",
      "Epoch 3405, Loss: 2.501966565847397, Final Batch Loss: 0.5335143804550171\n",
      "Epoch 3406, Loss: 2.5397194027900696, Final Batch Loss: 0.4696659445762634\n",
      "Epoch 3407, Loss: 2.58257395029068, Final Batch Loss: 0.5161411166191101\n",
      "Epoch 3408, Loss: 2.6295225024223328, Final Batch Loss: 0.5689356327056885\n",
      "Epoch 3409, Loss: 2.481795459985733, Final Batch Loss: 0.5037029981613159\n",
      "Epoch 3410, Loss: 2.5690487921237946, Final Batch Loss: 0.5449452996253967\n",
      "Epoch 3411, Loss: 2.583947777748108, Final Batch Loss: 0.5691086053848267\n",
      "Epoch 3412, Loss: 2.3871321380138397, Final Batch Loss: 0.4813433289527893\n",
      "Epoch 3413, Loss: 2.4856601655483246, Final Batch Loss: 0.45041799545288086\n",
      "Epoch 3414, Loss: 2.334722340106964, Final Batch Loss: 0.4910666048526764\n",
      "Epoch 3415, Loss: 2.456117957830429, Final Batch Loss: 0.5688108801841736\n",
      "Epoch 3416, Loss: 2.50530144572258, Final Batch Loss: 0.5590305328369141\n",
      "Epoch 3417, Loss: 2.5005361139774323, Final Batch Loss: 0.40948572754859924\n",
      "Epoch 3418, Loss: 2.4994219839572906, Final Batch Loss: 0.5688380002975464\n",
      "Epoch 3419, Loss: 2.6812775135040283, Final Batch Loss: 0.639634370803833\n",
      "Epoch 3420, Loss: 2.536957412958145, Final Batch Loss: 0.4676613211631775\n",
      "Epoch 3421, Loss: 2.6108286380767822, Final Batch Loss: 0.6312823295593262\n",
      "Epoch 3422, Loss: 2.393895298242569, Final Batch Loss: 0.5457934737205505\n",
      "Epoch 3423, Loss: 2.322867125272751, Final Batch Loss: 0.4559418261051178\n",
      "Epoch 3424, Loss: 2.5429278314113617, Final Batch Loss: 0.48694583773612976\n",
      "Epoch 3425, Loss: 2.4482862651348114, Final Batch Loss: 0.46446701884269714\n",
      "Epoch 3426, Loss: 2.52114075422287, Final Batch Loss: 0.5429776906967163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3427, Loss: 2.3858309388160706, Final Batch Loss: 0.5233946442604065\n",
      "Epoch 3428, Loss: 2.3438110649585724, Final Batch Loss: 0.529173731803894\n",
      "Epoch 3429, Loss: 2.458206385374069, Final Batch Loss: 0.5023461580276489\n",
      "Epoch 3430, Loss: 2.5101599991321564, Final Batch Loss: 0.5677260756492615\n",
      "Epoch 3431, Loss: 2.6456473767757416, Final Batch Loss: 0.3674197494983673\n",
      "Epoch 3432, Loss: 2.5369552075862885, Final Batch Loss: 0.5064215064048767\n",
      "Epoch 3433, Loss: 2.4419392943382263, Final Batch Loss: 0.4791408181190491\n",
      "Epoch 3434, Loss: 2.383355438709259, Final Batch Loss: 0.4368399381637573\n",
      "Epoch 3435, Loss: 2.495649576187134, Final Batch Loss: 0.5372845530509949\n",
      "Epoch 3436, Loss: 2.5329434871673584, Final Batch Loss: 0.6556307077407837\n",
      "Epoch 3437, Loss: 2.299273818731308, Final Batch Loss: 0.47354042530059814\n",
      "Epoch 3438, Loss: 2.571704238653183, Final Batch Loss: 0.500414252281189\n",
      "Epoch 3439, Loss: 2.57973650097847, Final Batch Loss: 0.4052760601043701\n",
      "Epoch 3440, Loss: 2.6413414776325226, Final Batch Loss: 0.4604882001876831\n",
      "Epoch 3441, Loss: 2.4347751438617706, Final Batch Loss: 0.5524815320968628\n",
      "Epoch 3442, Loss: 2.530200242996216, Final Batch Loss: 0.4927893280982971\n",
      "Epoch 3443, Loss: 2.567578673362732, Final Batch Loss: 0.5416204929351807\n",
      "Epoch 3444, Loss: 2.3654980957508087, Final Batch Loss: 0.40322571992874146\n",
      "Epoch 3445, Loss: 2.3865963518619537, Final Batch Loss: 0.5029324293136597\n",
      "Epoch 3446, Loss: 2.5419786274433136, Final Batch Loss: 0.6503167152404785\n",
      "Epoch 3447, Loss: 2.592884510755539, Final Batch Loss: 0.5120008587837219\n",
      "Epoch 3448, Loss: 2.4858493208885193, Final Batch Loss: 0.43074774742126465\n",
      "Epoch 3449, Loss: 2.4904189705848694, Final Batch Loss: 0.5014039874076843\n",
      "Epoch 3450, Loss: 2.475890964269638, Final Batch Loss: 0.5418556928634644\n",
      "Epoch 3451, Loss: 2.3329723477363586, Final Batch Loss: 0.38520464301109314\n",
      "Epoch 3452, Loss: 2.724437654018402, Final Batch Loss: 0.7124039530754089\n",
      "Epoch 3453, Loss: 2.6079676151275635, Final Batch Loss: 0.7034159898757935\n",
      "Epoch 3454, Loss: 2.55521360039711, Final Batch Loss: 0.5285776853561401\n",
      "Epoch 3455, Loss: 2.5499430000782013, Final Batch Loss: 0.5476205945014954\n",
      "Epoch 3456, Loss: 2.3272610902786255, Final Batch Loss: 0.4171285629272461\n",
      "Epoch 3457, Loss: 2.61156889796257, Final Batch Loss: 0.5409517288208008\n",
      "Epoch 3458, Loss: 2.5047170519828796, Final Batch Loss: 0.5424621105194092\n",
      "Epoch 3459, Loss: 2.5989949703216553, Final Batch Loss: 0.5210806727409363\n",
      "Epoch 3460, Loss: 2.5128061771392822, Final Batch Loss: 0.46776875853538513\n",
      "Epoch 3461, Loss: 2.5822700560092926, Final Batch Loss: 0.5036443471908569\n",
      "Epoch 3462, Loss: 2.379399836063385, Final Batch Loss: 0.46370574831962585\n",
      "Epoch 3463, Loss: 2.48312184214592, Final Batch Loss: 0.46740490198135376\n",
      "Epoch 3464, Loss: 2.3895543813705444, Final Batch Loss: 0.453128844499588\n",
      "Epoch 3465, Loss: 2.438912659883499, Final Batch Loss: 0.5654460191726685\n",
      "Epoch 3466, Loss: 2.4261318743228912, Final Batch Loss: 0.429148405790329\n",
      "Epoch 3467, Loss: 2.3353629410266876, Final Batch Loss: 0.42593804001808167\n",
      "Epoch 3468, Loss: 2.3866314589977264, Final Batch Loss: 0.5238659977912903\n",
      "Epoch 3469, Loss: 2.375634044408798, Final Batch Loss: 0.4250006377696991\n",
      "Epoch 3470, Loss: 2.4681668281555176, Final Batch Loss: 0.4401077628135681\n",
      "Epoch 3471, Loss: 2.5344085693359375, Final Batch Loss: 0.5941104888916016\n",
      "Epoch 3472, Loss: 2.4375972747802734, Final Batch Loss: 0.5349110960960388\n",
      "Epoch 3473, Loss: 2.629454731941223, Final Batch Loss: 0.5101137757301331\n",
      "Epoch 3474, Loss: 2.462086707353592, Final Batch Loss: 0.4364986717700958\n",
      "Epoch 3475, Loss: 2.5262839794158936, Final Batch Loss: 0.43428677320480347\n",
      "Epoch 3476, Loss: 2.3495925068855286, Final Batch Loss: 0.4728728234767914\n",
      "Epoch 3477, Loss: 2.2768813371658325, Final Batch Loss: 0.38536006212234497\n",
      "Epoch 3478, Loss: 2.38687202334404, Final Batch Loss: 0.404122918844223\n",
      "Epoch 3479, Loss: 2.5978341102600098, Final Batch Loss: 0.5488542914390564\n",
      "Epoch 3480, Loss: 2.3621439337730408, Final Batch Loss: 0.4286772906780243\n",
      "Epoch 3481, Loss: 2.4229936599731445, Final Batch Loss: 0.4954652488231659\n",
      "Epoch 3482, Loss: 2.406279593706131, Final Batch Loss: 0.6101690530776978\n",
      "Epoch 3483, Loss: 2.4094836115837097, Final Batch Loss: 0.40333980321884155\n",
      "Epoch 3484, Loss: 2.389351338148117, Final Batch Loss: 0.45433786511421204\n",
      "Epoch 3485, Loss: 2.4667408168315887, Final Batch Loss: 0.4440271258354187\n",
      "Epoch 3486, Loss: 2.2752643823623657, Final Batch Loss: 0.2898598909378052\n",
      "Epoch 3487, Loss: 2.3683508038520813, Final Batch Loss: 0.49148860573768616\n",
      "Epoch 3488, Loss: 2.3091598451137543, Final Batch Loss: 0.450099915266037\n",
      "Epoch 3489, Loss: 2.539945662021637, Final Batch Loss: 0.49196603894233704\n",
      "Epoch 3490, Loss: 2.5180376172065735, Final Batch Loss: 0.4366574287414551\n",
      "Epoch 3491, Loss: 2.4427933990955353, Final Batch Loss: 0.35289427638053894\n",
      "Epoch 3492, Loss: 2.7149528563022614, Final Batch Loss: 0.4940829575061798\n",
      "Epoch 3493, Loss: 2.411648392677307, Final Batch Loss: 0.3842775225639343\n",
      "Epoch 3494, Loss: 2.477224051952362, Final Batch Loss: 0.3763800859451294\n",
      "Epoch 3495, Loss: 2.493829160928726, Final Batch Loss: 0.49334007501602173\n",
      "Epoch 3496, Loss: 2.663742184638977, Final Batch Loss: 0.5502616763114929\n",
      "Epoch 3497, Loss: 2.571392685174942, Final Batch Loss: 0.5385967493057251\n",
      "Epoch 3498, Loss: 2.5684594213962555, Final Batch Loss: 0.4933035373687744\n",
      "Epoch 3499, Loss: 2.4512568712234497, Final Batch Loss: 0.5278888940811157\n",
      "Epoch 3500, Loss: 2.615705668926239, Final Batch Loss: 0.3909730315208435\n",
      "Epoch 3501, Loss: 2.453890770673752, Final Batch Loss: 0.4700183868408203\n",
      "Epoch 3502, Loss: 2.6944086849689484, Final Batch Loss: 0.6774237751960754\n",
      "Epoch 3503, Loss: 2.5577836632728577, Final Batch Loss: 0.6125941872596741\n",
      "Epoch 3504, Loss: 2.5851719975471497, Final Batch Loss: 0.4662463963031769\n",
      "Epoch 3505, Loss: 2.351135730743408, Final Batch Loss: 0.430165559053421\n",
      "Epoch 3506, Loss: 2.529701679944992, Final Batch Loss: 0.5367722511291504\n",
      "Epoch 3507, Loss: 2.375977635383606, Final Batch Loss: 0.4142896234989166\n",
      "Epoch 3508, Loss: 2.474108725786209, Final Batch Loss: 0.47893428802490234\n",
      "Epoch 3509, Loss: 2.5129385590553284, Final Batch Loss: 0.530656635761261\n",
      "Epoch 3510, Loss: 2.639908403158188, Final Batch Loss: 0.5125923156738281\n",
      "Epoch 3511, Loss: 2.624882936477661, Final Batch Loss: 0.5740940570831299\n",
      "Epoch 3512, Loss: 2.463994264602661, Final Batch Loss: 0.41107794642448425\n",
      "Epoch 3513, Loss: 2.489866465330124, Final Batch Loss: 0.49081793427467346\n",
      "Epoch 3514, Loss: 2.5568317472934723, Final Batch Loss: 0.6591876745223999\n",
      "Epoch 3515, Loss: 2.6843384504318237, Final Batch Loss: 0.6529004573822021\n",
      "Epoch 3516, Loss: 2.408886730670929, Final Batch Loss: 0.46613365411758423\n",
      "Epoch 3517, Loss: 2.4090562760829926, Final Batch Loss: 0.3993992507457733\n",
      "Epoch 3518, Loss: 2.517599880695343, Final Batch Loss: 0.43455687165260315\n",
      "Epoch 3519, Loss: 2.398126184940338, Final Batch Loss: 0.3804055154323578\n",
      "Epoch 3520, Loss: 2.5430126190185547, Final Batch Loss: 0.5671541094779968\n",
      "Epoch 3521, Loss: 2.6054347455501556, Final Batch Loss: 0.5651256442070007\n",
      "Epoch 3522, Loss: 2.5629615485668182, Final Batch Loss: 0.5317596793174744\n",
      "Epoch 3523, Loss: 2.442796051502228, Final Batch Loss: 0.5881548523902893\n",
      "Epoch 3524, Loss: 2.4565061032772064, Final Batch Loss: 0.40756505727767944\n",
      "Epoch 3525, Loss: 2.453676164150238, Final Batch Loss: 0.45039689540863037\n",
      "Epoch 3526, Loss: 2.647961676120758, Final Batch Loss: 0.6495044827461243\n",
      "Epoch 3527, Loss: 2.6430227160453796, Final Batch Loss: 0.6894590854644775\n",
      "Epoch 3528, Loss: 2.429390549659729, Final Batch Loss: 0.41021135449409485\n",
      "Epoch 3529, Loss: 2.5732177197933197, Final Batch Loss: 0.5395492315292358\n",
      "Epoch 3530, Loss: 2.4536498188972473, Final Batch Loss: 0.4781729280948639\n",
      "Epoch 3531, Loss: 2.448332130908966, Final Batch Loss: 0.3904670774936676\n",
      "Epoch 3532, Loss: 2.534731239080429, Final Batch Loss: 0.45468318462371826\n",
      "Epoch 3533, Loss: 2.5649206042289734, Final Batch Loss: 0.5123586058616638\n",
      "Epoch 3534, Loss: 2.527326285839081, Final Batch Loss: 0.5521869659423828\n",
      "Epoch 3535, Loss: 2.5973393321037292, Final Batch Loss: 0.5943942666053772\n",
      "Epoch 3536, Loss: 2.473079651594162, Final Batch Loss: 0.47123435139656067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3537, Loss: 2.536655694246292, Final Batch Loss: 0.5532557964324951\n",
      "Epoch 3538, Loss: 2.5607988834381104, Final Batch Loss: 0.49893808364868164\n",
      "Epoch 3539, Loss: 2.3996424078941345, Final Batch Loss: 0.3977241814136505\n",
      "Epoch 3540, Loss: 2.435852199792862, Final Batch Loss: 0.5100381970405579\n",
      "Epoch 3541, Loss: 2.484596014022827, Final Batch Loss: 0.47161123156547546\n",
      "Epoch 3542, Loss: 2.3592414557933807, Final Batch Loss: 0.4215562045574188\n",
      "Epoch 3543, Loss: 2.5485073626041412, Final Batch Loss: 0.4867108166217804\n",
      "Epoch 3544, Loss: 2.398665428161621, Final Batch Loss: 0.42608916759490967\n",
      "Epoch 3545, Loss: 2.5643979012966156, Final Batch Loss: 0.6137162446975708\n",
      "Epoch 3546, Loss: 2.458427131175995, Final Batch Loss: 0.5981811285018921\n",
      "Epoch 3547, Loss: 2.4315066039562225, Final Batch Loss: 0.4589419662952423\n",
      "Epoch 3548, Loss: 2.527073234319687, Final Batch Loss: 0.4800182580947876\n",
      "Epoch 3549, Loss: 2.6383286714553833, Final Batch Loss: 0.6197394132614136\n",
      "Epoch 3550, Loss: 2.454382598400116, Final Batch Loss: 0.465019166469574\n",
      "Epoch 3551, Loss: 2.4604766070842743, Final Batch Loss: 0.474289208650589\n",
      "Epoch 3552, Loss: 2.6418940722942352, Final Batch Loss: 0.5000625252723694\n",
      "Epoch 3553, Loss: 2.511373609304428, Final Batch Loss: 0.5394739508628845\n",
      "Epoch 3554, Loss: 2.4859275221824646, Final Batch Loss: 0.5571780800819397\n",
      "Epoch 3555, Loss: 2.3130871951580048, Final Batch Loss: 0.3548109829425812\n",
      "Epoch 3556, Loss: 2.4074504375457764, Final Batch Loss: 0.46174994111061096\n",
      "Epoch 3557, Loss: 2.880085289478302, Final Batch Loss: 0.6406778693199158\n",
      "Epoch 3558, Loss: 2.4522936642169952, Final Batch Loss: 0.48435044288635254\n",
      "Epoch 3559, Loss: 2.4785533249378204, Final Batch Loss: 0.45643094182014465\n",
      "Epoch 3560, Loss: 2.6302870213985443, Final Batch Loss: 0.7131245732307434\n",
      "Epoch 3561, Loss: 2.4755705296993256, Final Batch Loss: 0.511857271194458\n",
      "Epoch 3562, Loss: 2.4181123077869415, Final Batch Loss: 0.41454267501831055\n",
      "Epoch 3563, Loss: 2.5130591690540314, Final Batch Loss: 0.45679301023483276\n",
      "Epoch 3564, Loss: 2.3704945147037506, Final Batch Loss: 0.4854326546192169\n",
      "Epoch 3565, Loss: 2.5594286918640137, Final Batch Loss: 0.46116769313812256\n",
      "Epoch 3566, Loss: 2.5413087010383606, Final Batch Loss: 0.45552006363868713\n",
      "Epoch 3567, Loss: 2.3608358800411224, Final Batch Loss: 0.4601697623729706\n",
      "Epoch 3568, Loss: 2.6209467351436615, Final Batch Loss: 0.3953242301940918\n",
      "Epoch 3569, Loss: 2.433038830757141, Final Batch Loss: 0.5050123929977417\n",
      "Epoch 3570, Loss: 2.3349041044712067, Final Batch Loss: 0.4364268481731415\n",
      "Epoch 3571, Loss: 2.410417526960373, Final Batch Loss: 0.4894639253616333\n",
      "Epoch 3572, Loss: 2.3717510402202606, Final Batch Loss: 0.5111099481582642\n",
      "Epoch 3573, Loss: 2.3597117960453033, Final Batch Loss: 0.46947669982910156\n",
      "Epoch 3574, Loss: 2.479438006877899, Final Batch Loss: 0.5369485020637512\n",
      "Epoch 3575, Loss: 2.3357678949832916, Final Batch Loss: 0.5149308443069458\n",
      "Epoch 3576, Loss: 2.548771560192108, Final Batch Loss: 0.46836522221565247\n",
      "Epoch 3577, Loss: 2.525551825761795, Final Batch Loss: 0.49531039595603943\n",
      "Epoch 3578, Loss: 2.3321183621883392, Final Batch Loss: 0.45911887288093567\n",
      "Epoch 3579, Loss: 2.4082543551921844, Final Batch Loss: 0.410786896944046\n",
      "Epoch 3580, Loss: 2.364214599132538, Final Batch Loss: 0.44830822944641113\n",
      "Epoch 3581, Loss: 2.352822929620743, Final Batch Loss: 0.4963495135307312\n",
      "Epoch 3582, Loss: 2.647926092147827, Final Batch Loss: 0.7030019164085388\n",
      "Epoch 3583, Loss: 2.5200401544570923, Final Batch Loss: 0.5792862176895142\n",
      "Epoch 3584, Loss: 2.5410782992839813, Final Batch Loss: 0.4423723816871643\n",
      "Epoch 3585, Loss: 2.3810681998729706, Final Batch Loss: 0.4960494041442871\n",
      "Epoch 3586, Loss: 2.47493052482605, Final Batch Loss: 0.5375536680221558\n",
      "Epoch 3587, Loss: 2.539010167121887, Final Batch Loss: 0.4773520529270172\n",
      "Epoch 3588, Loss: 2.501413345336914, Final Batch Loss: 0.5747005343437195\n",
      "Epoch 3589, Loss: 2.507477790117264, Final Batch Loss: 0.5076383352279663\n",
      "Epoch 3590, Loss: 2.5350749492645264, Final Batch Loss: 0.5581244826316833\n",
      "Epoch 3591, Loss: 2.537407249212265, Final Batch Loss: 0.4479534924030304\n",
      "Epoch 3592, Loss: 2.4180208444595337, Final Batch Loss: 0.4790551960468292\n",
      "Epoch 3593, Loss: 2.4564567506313324, Final Batch Loss: 0.5326277017593384\n",
      "Epoch 3594, Loss: 2.5328510105609894, Final Batch Loss: 0.46049073338508606\n",
      "Epoch 3595, Loss: 2.43500417470932, Final Batch Loss: 0.43244799971580505\n",
      "Epoch 3596, Loss: 2.3625012934207916, Final Batch Loss: 0.43408599495887756\n",
      "Epoch 3597, Loss: 2.4754902720451355, Final Batch Loss: 0.45789968967437744\n",
      "Epoch 3598, Loss: 2.435573637485504, Final Batch Loss: 0.545592725276947\n",
      "Epoch 3599, Loss: 2.3133267164230347, Final Batch Loss: 0.57651686668396\n",
      "Epoch 3600, Loss: 2.4209187626838684, Final Batch Loss: 0.46319547295570374\n",
      "Epoch 3601, Loss: 2.4516795873641968, Final Batch Loss: 0.4574332535266876\n",
      "Epoch 3602, Loss: 2.495300769805908, Final Batch Loss: 0.5048561096191406\n",
      "Epoch 3603, Loss: 2.5707127451896667, Final Batch Loss: 0.429074227809906\n",
      "Epoch 3604, Loss: 2.4499667286872864, Final Batch Loss: 0.4261062741279602\n",
      "Epoch 3605, Loss: 2.3592113256454468, Final Batch Loss: 0.473097026348114\n",
      "Epoch 3606, Loss: 2.3590694665908813, Final Batch Loss: 0.3874465823173523\n",
      "Epoch 3607, Loss: 2.365391820669174, Final Batch Loss: 0.3759785294532776\n",
      "Epoch 3608, Loss: 2.4690054059028625, Final Batch Loss: 0.4804818034172058\n",
      "Epoch 3609, Loss: 2.4131003320217133, Final Batch Loss: 0.4739524722099304\n",
      "Epoch 3610, Loss: 2.5428272783756256, Final Batch Loss: 0.6110290884971619\n",
      "Epoch 3611, Loss: 2.5184415578842163, Final Batch Loss: 0.4242604076862335\n",
      "Epoch 3612, Loss: 2.337609350681305, Final Batch Loss: 0.4407666325569153\n",
      "Epoch 3613, Loss: 2.6780325770378113, Final Batch Loss: 0.6566576957702637\n",
      "Epoch 3614, Loss: 2.5667306780815125, Final Batch Loss: 0.5040004849433899\n",
      "Epoch 3615, Loss: 2.47274249792099, Final Batch Loss: 0.49072030186653137\n",
      "Epoch 3616, Loss: 2.3624634742736816, Final Batch Loss: 0.4260118007659912\n",
      "Epoch 3617, Loss: 2.4001176953315735, Final Batch Loss: 0.3227411210536957\n",
      "Epoch 3618, Loss: 2.4581781029701233, Final Batch Loss: 0.4045872390270233\n",
      "Epoch 3619, Loss: 2.5159516036510468, Final Batch Loss: 0.5303151607513428\n",
      "Epoch 3620, Loss: 2.2981871366500854, Final Batch Loss: 0.37388497591018677\n",
      "Epoch 3621, Loss: 2.4855653047561646, Final Batch Loss: 0.4513753354549408\n",
      "Epoch 3622, Loss: 2.513199806213379, Final Batch Loss: 0.5962486863136292\n",
      "Epoch 3623, Loss: 2.4608734250068665, Final Batch Loss: 0.5345100164413452\n",
      "Epoch 3624, Loss: 2.3271181285381317, Final Batch Loss: 0.4255965054035187\n",
      "Epoch 3625, Loss: 2.434135317802429, Final Batch Loss: 0.5743206739425659\n",
      "Epoch 3626, Loss: 2.326708972454071, Final Batch Loss: 0.5185050964355469\n",
      "Epoch 3627, Loss: 2.368151903152466, Final Batch Loss: 0.38642385601997375\n",
      "Epoch 3628, Loss: 2.5110740661621094, Final Batch Loss: 0.36532050371170044\n",
      "Epoch 3629, Loss: 2.3304403722286224, Final Batch Loss: 0.44107839465141296\n",
      "Epoch 3630, Loss: 2.4421750605106354, Final Batch Loss: 0.5038447976112366\n",
      "Epoch 3631, Loss: 2.2475235760211945, Final Batch Loss: 0.42444419860839844\n",
      "Epoch 3632, Loss: 2.418661445379257, Final Batch Loss: 0.49028921127319336\n",
      "Epoch 3633, Loss: 2.4062647223472595, Final Batch Loss: 0.48407524824142456\n",
      "Epoch 3634, Loss: 2.3685905635356903, Final Batch Loss: 0.5161471962928772\n",
      "Epoch 3635, Loss: 2.5609682202339172, Final Batch Loss: 0.43187522888183594\n",
      "Epoch 3636, Loss: 2.3299493193626404, Final Batch Loss: 0.46748048067092896\n",
      "Epoch 3637, Loss: 2.535215377807617, Final Batch Loss: 0.5396502017974854\n",
      "Epoch 3638, Loss: 2.434530884027481, Final Batch Loss: 0.6252471208572388\n",
      "Epoch 3639, Loss: 2.2516650557518005, Final Batch Loss: 0.4449880123138428\n",
      "Epoch 3640, Loss: 2.5966565012931824, Final Batch Loss: 0.6539575457572937\n",
      "Epoch 3641, Loss: 2.4790883362293243, Final Batch Loss: 0.354362815618515\n",
      "Epoch 3642, Loss: 2.5299875140190125, Final Batch Loss: 0.40805694460868835\n",
      "Epoch 3643, Loss: 2.526510775089264, Final Batch Loss: 0.48555535078048706\n",
      "Epoch 3644, Loss: 2.676441729068756, Final Batch Loss: 0.453510582447052\n",
      "Epoch 3645, Loss: 2.418430209159851, Final Batch Loss: 0.46813255548477173\n",
      "Epoch 3646, Loss: 2.625676602125168, Final Batch Loss: 0.5234699845314026\n",
      "Epoch 3647, Loss: 2.541845679283142, Final Batch Loss: 0.5430581569671631\n",
      "Epoch 3648, Loss: 2.5258357524871826, Final Batch Loss: 0.622641921043396\n",
      "Epoch 3649, Loss: 2.485704243183136, Final Batch Loss: 0.5182944536209106\n",
      "Epoch 3650, Loss: 2.487888365983963, Final Batch Loss: 0.5453144907951355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3651, Loss: 2.5164886116981506, Final Batch Loss: 0.4370378851890564\n",
      "Epoch 3652, Loss: 2.3370345532894135, Final Batch Loss: 0.4047867953777313\n",
      "Epoch 3653, Loss: 2.435393065214157, Final Batch Loss: 0.5216814875602722\n",
      "Epoch 3654, Loss: 2.42641419172287, Final Batch Loss: 0.4773164391517639\n",
      "Epoch 3655, Loss: 2.282746583223343, Final Batch Loss: 0.4884501099586487\n",
      "Epoch 3656, Loss: 2.4586545526981354, Final Batch Loss: 0.47685521841049194\n",
      "Epoch 3657, Loss: 2.445692002773285, Final Batch Loss: 0.4368995726108551\n",
      "Epoch 3658, Loss: 2.481387734413147, Final Batch Loss: 0.44702833890914917\n",
      "Epoch 3659, Loss: 2.4630702435970306, Final Batch Loss: 0.4887799918651581\n",
      "Epoch 3660, Loss: 2.4363515377044678, Final Batch Loss: 0.4217948615550995\n",
      "Epoch 3661, Loss: 2.3948913514614105, Final Batch Loss: 0.47638729214668274\n",
      "Epoch 3662, Loss: 2.327684134244919, Final Batch Loss: 0.44615936279296875\n",
      "Epoch 3663, Loss: 2.4307545721530914, Final Batch Loss: 0.4476778209209442\n",
      "Epoch 3664, Loss: 2.5696872174739838, Final Batch Loss: 0.6068957448005676\n",
      "Epoch 3665, Loss: 2.460264354944229, Final Batch Loss: 0.4357658326625824\n",
      "Epoch 3666, Loss: 2.3179490864276886, Final Batch Loss: 0.5031414031982422\n",
      "Epoch 3667, Loss: 2.381619155406952, Final Batch Loss: 0.5085985660552979\n",
      "Epoch 3668, Loss: 2.5880068242549896, Final Batch Loss: 0.5661672949790955\n",
      "Epoch 3669, Loss: 2.5074125230312347, Final Batch Loss: 0.5032051801681519\n",
      "Epoch 3670, Loss: 2.5299032032489777, Final Batch Loss: 0.5573458671569824\n",
      "Epoch 3671, Loss: 2.633878529071808, Final Batch Loss: 0.4740146994590759\n",
      "Epoch 3672, Loss: 2.4543338418006897, Final Batch Loss: 0.5092423558235168\n",
      "Epoch 3673, Loss: 2.4530360400676727, Final Batch Loss: 0.4919945001602173\n",
      "Epoch 3674, Loss: 2.487013667821884, Final Batch Loss: 0.47220391035079956\n",
      "Epoch 3675, Loss: 2.545319139957428, Final Batch Loss: 0.5028824210166931\n",
      "Epoch 3676, Loss: 2.362918436527252, Final Batch Loss: 0.4181536138057709\n",
      "Epoch 3677, Loss: 2.4660877883434296, Final Batch Loss: 0.5199897289276123\n",
      "Epoch 3678, Loss: 2.297083079814911, Final Batch Loss: 0.37115147709846497\n",
      "Epoch 3679, Loss: 2.291363924741745, Final Batch Loss: 0.45065656304359436\n",
      "Epoch 3680, Loss: 2.572520285844803, Final Batch Loss: 0.451156347990036\n",
      "Epoch 3681, Loss: 2.3920708894729614, Final Batch Loss: 0.4587874412536621\n",
      "Epoch 3682, Loss: 2.4506713151931763, Final Batch Loss: 0.46182435750961304\n",
      "Epoch 3683, Loss: 2.530638247728348, Final Batch Loss: 0.4777332842350006\n",
      "Epoch 3684, Loss: 2.5961629450321198, Final Batch Loss: 0.509757936000824\n",
      "Epoch 3685, Loss: 2.5013222992420197, Final Batch Loss: 0.5100504159927368\n",
      "Epoch 3686, Loss: 2.3414627611637115, Final Batch Loss: 0.40911468863487244\n",
      "Epoch 3687, Loss: 2.455425262451172, Final Batch Loss: 0.5486848950386047\n",
      "Epoch 3688, Loss: 2.6008065938949585, Final Batch Loss: 0.45862889289855957\n",
      "Epoch 3689, Loss: 2.424370527267456, Final Batch Loss: 0.4642036557197571\n",
      "Epoch 3690, Loss: 2.2900982797145844, Final Batch Loss: 0.47427284717559814\n",
      "Epoch 3691, Loss: 2.396159380674362, Final Batch Loss: 0.48803991079330444\n",
      "Epoch 3692, Loss: 2.463400214910507, Final Batch Loss: 0.4672410488128662\n",
      "Epoch 3693, Loss: 2.4035581946372986, Final Batch Loss: 0.4736367166042328\n",
      "Epoch 3694, Loss: 2.5058179795742035, Final Batch Loss: 0.4818430542945862\n",
      "Epoch 3695, Loss: 2.278114676475525, Final Batch Loss: 0.4058622121810913\n",
      "Epoch 3696, Loss: 2.4654164910316467, Final Batch Loss: 0.5982322096824646\n",
      "Epoch 3697, Loss: 2.492116332054138, Final Batch Loss: 0.513071596622467\n",
      "Epoch 3698, Loss: 2.547500044107437, Final Batch Loss: 0.44840899109840393\n",
      "Epoch 3699, Loss: 2.4338636994361877, Final Batch Loss: 0.46919628977775574\n",
      "Epoch 3700, Loss: 2.412720173597336, Final Batch Loss: 0.5331480503082275\n",
      "Epoch 3701, Loss: 2.5208144783973694, Final Batch Loss: 0.5436093807220459\n",
      "Epoch 3702, Loss: 2.4348825216293335, Final Batch Loss: 0.4476039409637451\n",
      "Epoch 3703, Loss: 2.310481071472168, Final Batch Loss: 0.33513057231903076\n",
      "Epoch 3704, Loss: 2.618344157934189, Final Batch Loss: 0.4575137495994568\n",
      "Epoch 3705, Loss: 2.5722632706165314, Final Batch Loss: 0.6235151886940002\n",
      "Epoch 3706, Loss: 2.4465223848819733, Final Batch Loss: 0.5387719869613647\n",
      "Epoch 3707, Loss: 2.3674378097057343, Final Batch Loss: 0.494533509016037\n",
      "Epoch 3708, Loss: 2.414696156978607, Final Batch Loss: 0.5194750428199768\n",
      "Epoch 3709, Loss: 2.45323446393013, Final Batch Loss: 0.5087592005729675\n",
      "Epoch 3710, Loss: 2.348139375448227, Final Batch Loss: 0.44084954261779785\n",
      "Epoch 3711, Loss: 2.328490763902664, Final Batch Loss: 0.448907732963562\n",
      "Epoch 3712, Loss: 2.3712834119796753, Final Batch Loss: 0.46976470947265625\n",
      "Epoch 3713, Loss: 2.3664661049842834, Final Batch Loss: 0.48920536041259766\n",
      "Epoch 3714, Loss: 2.4494482576847076, Final Batch Loss: 0.46654027700424194\n",
      "Epoch 3715, Loss: 2.3579307198524475, Final Batch Loss: 0.4735826551914215\n",
      "Epoch 3716, Loss: 2.46132430434227, Final Batch Loss: 0.476671040058136\n",
      "Epoch 3717, Loss: 2.4410363137722015, Final Batch Loss: 0.5131006240844727\n",
      "Epoch 3718, Loss: 2.4564466774463654, Final Batch Loss: 0.5318401455879211\n",
      "Epoch 3719, Loss: 2.4752039909362793, Final Batch Loss: 0.42107439041137695\n",
      "Epoch 3720, Loss: 2.461475044488907, Final Batch Loss: 0.5114685893058777\n",
      "Epoch 3721, Loss: 2.4607545137405396, Final Batch Loss: 0.5022174715995789\n",
      "Epoch 3722, Loss: 2.4702558517456055, Final Batch Loss: 0.5933012962341309\n",
      "Epoch 3723, Loss: 2.5582377314567566, Final Batch Loss: 0.5693275928497314\n",
      "Epoch 3724, Loss: 2.5286069810390472, Final Batch Loss: 0.3717631697654724\n",
      "Epoch 3725, Loss: 2.4736161828041077, Final Batch Loss: 0.5677710175514221\n",
      "Epoch 3726, Loss: 2.5281239449977875, Final Batch Loss: 0.5408076643943787\n",
      "Epoch 3727, Loss: 2.4572812616825104, Final Batch Loss: 0.5268072485923767\n",
      "Epoch 3728, Loss: 2.459849327802658, Final Batch Loss: 0.4265216290950775\n",
      "Epoch 3729, Loss: 2.4386404156684875, Final Batch Loss: 0.4771287143230438\n",
      "Epoch 3730, Loss: 2.4439889788627625, Final Batch Loss: 0.3567736744880676\n",
      "Epoch 3731, Loss: 2.425000786781311, Final Batch Loss: 0.4053535461425781\n",
      "Epoch 3732, Loss: 2.511792927980423, Final Batch Loss: 0.5061978101730347\n",
      "Epoch 3733, Loss: 2.5172401070594788, Final Batch Loss: 0.5428102016448975\n",
      "Epoch 3734, Loss: 2.5988321006298065, Final Batch Loss: 0.6655128002166748\n",
      "Epoch 3735, Loss: 2.5431641936302185, Final Batch Loss: 0.5293362736701965\n",
      "Epoch 3736, Loss: 2.409003347158432, Final Batch Loss: 0.4623749852180481\n",
      "Epoch 3737, Loss: 2.419545590877533, Final Batch Loss: 0.41636911034584045\n",
      "Epoch 3738, Loss: 2.3828439712524414, Final Batch Loss: 0.533041775226593\n",
      "Epoch 3739, Loss: 2.428149253129959, Final Batch Loss: 0.4328056275844574\n",
      "Epoch 3740, Loss: 2.420130342245102, Final Batch Loss: 0.4851931929588318\n",
      "Epoch 3741, Loss: 2.4440751671791077, Final Batch Loss: 0.48211559653282166\n",
      "Epoch 3742, Loss: 2.5098308622837067, Final Batch Loss: 0.6636149883270264\n",
      "Epoch 3743, Loss: 2.415835916996002, Final Batch Loss: 0.522894024848938\n",
      "Epoch 3744, Loss: 2.4647754430770874, Final Batch Loss: 0.47890403866767883\n",
      "Epoch 3745, Loss: 2.345965951681137, Final Batch Loss: 0.4172723889350891\n",
      "Epoch 3746, Loss: 2.513988435268402, Final Batch Loss: 0.46889716386795044\n",
      "Epoch 3747, Loss: 2.446003168821335, Final Batch Loss: 0.5039052367210388\n",
      "Epoch 3748, Loss: 2.267675966024399, Final Batch Loss: 0.4955584704875946\n",
      "Epoch 3749, Loss: 2.5784535706043243, Final Batch Loss: 0.6304013729095459\n",
      "Epoch 3750, Loss: 2.3581860661506653, Final Batch Loss: 0.42471691966056824\n",
      "Epoch 3751, Loss: 2.3747214674949646, Final Batch Loss: 0.3999205529689789\n",
      "Epoch 3752, Loss: 2.659873068332672, Final Batch Loss: 0.6443929076194763\n",
      "Epoch 3753, Loss: 2.5314112305641174, Final Batch Loss: 0.5500265955924988\n",
      "Epoch 3754, Loss: 2.4699393808841705, Final Batch Loss: 0.45406630635261536\n",
      "Epoch 3755, Loss: 2.3501648008823395, Final Batch Loss: 0.42436009645462036\n",
      "Epoch 3756, Loss: 2.4513351023197174, Final Batch Loss: 0.5142869353294373\n",
      "Epoch 3757, Loss: 2.2520599365234375, Final Batch Loss: 0.3993871808052063\n",
      "Epoch 3758, Loss: 2.5053736865520477, Final Batch Loss: 0.506336510181427\n",
      "Epoch 3759, Loss: 2.5829571783542633, Final Batch Loss: 0.5435539484024048\n",
      "Epoch 3760, Loss: 2.3709879517555237, Final Batch Loss: 0.4222559630870819\n",
      "Epoch 3761, Loss: 2.4484078884124756, Final Batch Loss: 0.46145641803741455\n",
      "Epoch 3762, Loss: 2.3683692514896393, Final Batch Loss: 0.4382633864879608\n",
      "Epoch 3763, Loss: 2.42990043759346, Final Batch Loss: 0.47703179717063904\n",
      "Epoch 3764, Loss: 2.596733808517456, Final Batch Loss: 0.4646734893321991\n",
      "Epoch 3765, Loss: 2.365089386701584, Final Batch Loss: 0.47634708881378174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3766, Loss: 2.346737176179886, Final Batch Loss: 0.5119717121124268\n",
      "Epoch 3767, Loss: 2.365509033203125, Final Batch Loss: 0.41619670391082764\n",
      "Epoch 3768, Loss: 2.584858238697052, Final Batch Loss: 0.5968844890594482\n",
      "Epoch 3769, Loss: 2.4073688089847565, Final Batch Loss: 0.5892138481140137\n",
      "Epoch 3770, Loss: 2.378828227519989, Final Batch Loss: 0.44562777876853943\n",
      "Epoch 3771, Loss: 2.598360389471054, Final Batch Loss: 0.5817980766296387\n",
      "Epoch 3772, Loss: 2.5881281197071075, Final Batch Loss: 0.528770923614502\n",
      "Epoch 3773, Loss: 2.6252437233924866, Final Batch Loss: 0.5817221403121948\n",
      "Epoch 3774, Loss: 2.367564082145691, Final Batch Loss: 0.5172519087791443\n",
      "Epoch 3775, Loss: 2.5564358234405518, Final Batch Loss: 0.5282135009765625\n",
      "Epoch 3776, Loss: 2.413275122642517, Final Batch Loss: 0.3880278170108795\n",
      "Epoch 3777, Loss: 2.458275943994522, Final Batch Loss: 0.4337998926639557\n",
      "Epoch 3778, Loss: 2.4031320810317993, Final Batch Loss: 0.4645058512687683\n",
      "Epoch 3779, Loss: 2.4355849623680115, Final Batch Loss: 0.570334255695343\n",
      "Epoch 3780, Loss: 2.4708786606788635, Final Batch Loss: 0.4981216490268707\n",
      "Epoch 3781, Loss: 2.4959274530410767, Final Batch Loss: 0.5340021848678589\n",
      "Epoch 3782, Loss: 2.4617049992084503, Final Batch Loss: 0.4867390990257263\n",
      "Epoch 3783, Loss: 2.2397815585136414, Final Batch Loss: 0.4129839837551117\n",
      "Epoch 3784, Loss: 2.3705444037914276, Final Batch Loss: 0.5153523683547974\n",
      "Epoch 3785, Loss: 2.333004057407379, Final Batch Loss: 0.49319320917129517\n",
      "Epoch 3786, Loss: 2.72078737616539, Final Batch Loss: 0.4794933497905731\n",
      "Epoch 3787, Loss: 2.5947960913181305, Final Batch Loss: 0.7159749865531921\n",
      "Epoch 3788, Loss: 2.4998623430728912, Final Batch Loss: 0.42678818106651306\n",
      "Epoch 3789, Loss: 2.4444169402122498, Final Batch Loss: 0.4751747250556946\n",
      "Epoch 3790, Loss: 2.3052951991558075, Final Batch Loss: 0.4039051830768585\n",
      "Epoch 3791, Loss: 2.489559441804886, Final Batch Loss: 0.5515540838241577\n",
      "Epoch 3792, Loss: 2.3245634138584137, Final Batch Loss: 0.4328024387359619\n",
      "Epoch 3793, Loss: 2.5269967019557953, Final Batch Loss: 0.5424372553825378\n",
      "Epoch 3794, Loss: 2.378687083721161, Final Batch Loss: 0.4936181902885437\n",
      "Epoch 3795, Loss: 2.394666761159897, Final Batch Loss: 0.49157068133354187\n",
      "Epoch 3796, Loss: 2.5230929851531982, Final Batch Loss: 0.5030905604362488\n",
      "Epoch 3797, Loss: 2.2901148200035095, Final Batch Loss: 0.5024402141571045\n",
      "Epoch 3798, Loss: 2.3805181980133057, Final Batch Loss: 0.530264675617218\n",
      "Epoch 3799, Loss: 2.5166790783405304, Final Batch Loss: 0.39982157945632935\n",
      "Epoch 3800, Loss: 2.4734613597393036, Final Batch Loss: 0.5161983370780945\n",
      "Epoch 3801, Loss: 2.479040563106537, Final Batch Loss: 0.4555027484893799\n",
      "Epoch 3802, Loss: 2.3637344241142273, Final Batch Loss: 0.3789752721786499\n",
      "Epoch 3803, Loss: 2.540640503168106, Final Batch Loss: 0.5645227432250977\n",
      "Epoch 3804, Loss: 2.435476928949356, Final Batch Loss: 0.4710434377193451\n",
      "Epoch 3805, Loss: 2.35380682349205, Final Batch Loss: 0.5075528025627136\n",
      "Epoch 3806, Loss: 2.352125406265259, Final Batch Loss: 0.42378711700439453\n",
      "Epoch 3807, Loss: 2.2719476521015167, Final Batch Loss: 0.4253186583518982\n",
      "Epoch 3808, Loss: 2.674994945526123, Final Batch Loss: 0.592462956905365\n",
      "Epoch 3809, Loss: 2.5358538925647736, Final Batch Loss: 0.4243299961090088\n",
      "Epoch 3810, Loss: 2.42832288146019, Final Batch Loss: 0.3958587944507599\n",
      "Epoch 3811, Loss: 2.2932236790657043, Final Batch Loss: 0.4354166090488434\n",
      "Epoch 3812, Loss: 2.4337931275367737, Final Batch Loss: 0.5141042470932007\n",
      "Epoch 3813, Loss: 2.4354691207408905, Final Batch Loss: 0.44135719537734985\n",
      "Epoch 3814, Loss: 2.329195588827133, Final Batch Loss: 0.414640873670578\n",
      "Epoch 3815, Loss: 2.585856407880783, Final Batch Loss: 0.6163499355316162\n",
      "Epoch 3816, Loss: 2.4996755719184875, Final Batch Loss: 0.6290704011917114\n",
      "Epoch 3817, Loss: 2.4539646208286285, Final Batch Loss: 0.5584297180175781\n",
      "Epoch 3818, Loss: 2.2887738049030304, Final Batch Loss: 0.39621925354003906\n",
      "Epoch 3819, Loss: 2.5290514528751373, Final Batch Loss: 0.583037257194519\n",
      "Epoch 3820, Loss: 2.5414097607135773, Final Batch Loss: 0.5322321653366089\n",
      "Epoch 3821, Loss: 2.4842526018619537, Final Batch Loss: 0.4925118684768677\n",
      "Epoch 3822, Loss: 2.425688862800598, Final Batch Loss: 0.5211682319641113\n",
      "Epoch 3823, Loss: 2.4714005887508392, Final Batch Loss: 0.6350794434547424\n",
      "Epoch 3824, Loss: 2.360259383916855, Final Batch Loss: 0.48886018991470337\n",
      "Epoch 3825, Loss: 2.266416519880295, Final Batch Loss: 0.33515986800193787\n",
      "Epoch 3826, Loss: 2.3734201192855835, Final Batch Loss: 0.46944889426231384\n",
      "Epoch 3827, Loss: 2.375074177980423, Final Batch Loss: 0.42518672347068787\n",
      "Epoch 3828, Loss: 2.3983245491981506, Final Batch Loss: 0.453661173582077\n",
      "Epoch 3829, Loss: 2.3347677886486053, Final Batch Loss: 0.3913396894931793\n",
      "Epoch 3830, Loss: 2.452522575855255, Final Batch Loss: 0.4616166651248932\n",
      "Epoch 3831, Loss: 2.5855593979358673, Final Batch Loss: 0.620553195476532\n",
      "Epoch 3832, Loss: 2.4745327830314636, Final Batch Loss: 0.5055765509605408\n",
      "Epoch 3833, Loss: 2.640797585248947, Final Batch Loss: 0.633962094783783\n",
      "Epoch 3834, Loss: 2.419957786798477, Final Batch Loss: 0.45444342494010925\n",
      "Epoch 3835, Loss: 2.390863448381424, Final Batch Loss: 0.44507041573524475\n",
      "Epoch 3836, Loss: 2.4273265600204468, Final Batch Loss: 0.5317367315292358\n",
      "Epoch 3837, Loss: 2.4064223170280457, Final Batch Loss: 0.5796151757240295\n",
      "Epoch 3838, Loss: 2.431904971599579, Final Batch Loss: 0.5051590204238892\n",
      "Epoch 3839, Loss: 2.3320622742176056, Final Batch Loss: 0.4762069582939148\n",
      "Epoch 3840, Loss: 2.6084026396274567, Final Batch Loss: 0.5824031829833984\n",
      "Epoch 3841, Loss: 2.5400281250476837, Final Batch Loss: 0.4486202299594879\n",
      "Epoch 3842, Loss: 2.433600574731827, Final Batch Loss: 0.44243717193603516\n",
      "Epoch 3843, Loss: 2.527974158525467, Final Batch Loss: 0.5350196361541748\n",
      "Epoch 3844, Loss: 2.4878073036670685, Final Batch Loss: 0.4032565951347351\n",
      "Epoch 3845, Loss: 2.4761246144771576, Final Batch Loss: 0.41353607177734375\n",
      "Epoch 3846, Loss: 2.4519712328910828, Final Batch Loss: 0.4020187556743622\n",
      "Epoch 3847, Loss: 2.3764986991882324, Final Batch Loss: 0.4051380753517151\n",
      "Epoch 3848, Loss: 2.363259494304657, Final Batch Loss: 0.34740692377090454\n",
      "Epoch 3849, Loss: 2.313059687614441, Final Batch Loss: 0.4615730941295624\n",
      "Epoch 3850, Loss: 2.4124025106430054, Final Batch Loss: 0.5239633321762085\n",
      "Epoch 3851, Loss: 2.5791136026382446, Final Batch Loss: 0.5004637241363525\n",
      "Epoch 3852, Loss: 2.5754759311676025, Final Batch Loss: 0.5740551948547363\n",
      "Epoch 3853, Loss: 2.571942985057831, Final Batch Loss: 0.47036442160606384\n",
      "Epoch 3854, Loss: 2.2098508179187775, Final Batch Loss: 0.4079475700855255\n",
      "Epoch 3855, Loss: 2.426836371421814, Final Batch Loss: 0.4781356751918793\n",
      "Epoch 3856, Loss: 2.422749310731888, Final Batch Loss: 0.4332444667816162\n",
      "Epoch 3857, Loss: 2.2475125193595886, Final Batch Loss: 0.31950539350509644\n",
      "Epoch 3858, Loss: 2.242473602294922, Final Batch Loss: 0.36555609107017517\n",
      "Epoch 3859, Loss: 2.47144016623497, Final Batch Loss: 0.5230598449707031\n",
      "Epoch 3860, Loss: 2.3709940314292908, Final Batch Loss: 0.5513443350791931\n",
      "Epoch 3861, Loss: 2.4014123380184174, Final Batch Loss: 0.5561996102333069\n",
      "Epoch 3862, Loss: 2.588079661130905, Final Batch Loss: 0.6099302172660828\n",
      "Epoch 3863, Loss: 2.237427741289139, Final Batch Loss: 0.3478293716907501\n",
      "Epoch 3864, Loss: 2.4293260872364044, Final Batch Loss: 0.434254914522171\n",
      "Epoch 3865, Loss: 2.403786689043045, Final Batch Loss: 0.5138322114944458\n",
      "Epoch 3866, Loss: 2.409701257944107, Final Batch Loss: 0.5006658434867859\n",
      "Epoch 3867, Loss: 2.4973227977752686, Final Batch Loss: 0.5118476152420044\n",
      "Epoch 3868, Loss: 2.4847107231616974, Final Batch Loss: 0.463091105222702\n",
      "Epoch 3869, Loss: 2.466324657201767, Final Batch Loss: 0.6219794154167175\n",
      "Epoch 3870, Loss: 2.182813137769699, Final Batch Loss: 0.364273339509964\n",
      "Epoch 3871, Loss: 2.3936988413333893, Final Batch Loss: 0.4997149705886841\n",
      "Epoch 3872, Loss: 2.375050038099289, Final Batch Loss: 0.4707021713256836\n",
      "Epoch 3873, Loss: 2.4497000575065613, Final Batch Loss: 0.6078643798828125\n",
      "Epoch 3874, Loss: 2.4472205340862274, Final Batch Loss: 0.4746564030647278\n",
      "Epoch 3875, Loss: 2.4511202573776245, Final Batch Loss: 0.5555866360664368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3876, Loss: 2.479657232761383, Final Batch Loss: 0.4849258363246918\n",
      "Epoch 3877, Loss: 2.5167073905467987, Final Batch Loss: 0.5600048303604126\n",
      "Epoch 3878, Loss: 2.3960354030132294, Final Batch Loss: 0.36117294430732727\n",
      "Epoch 3879, Loss: 2.318395346403122, Final Batch Loss: 0.48987022042274475\n",
      "Epoch 3880, Loss: 2.247306168079376, Final Batch Loss: 0.43199822306632996\n",
      "Epoch 3881, Loss: 2.4214178025722504, Final Batch Loss: 0.5028291940689087\n",
      "Epoch 3882, Loss: 2.38304004073143, Final Batch Loss: 0.5110480189323425\n",
      "Epoch 3883, Loss: 2.391673266887665, Final Batch Loss: 0.48982375860214233\n",
      "Epoch 3884, Loss: 2.391593337059021, Final Batch Loss: 0.3269719183444977\n",
      "Epoch 3885, Loss: 2.5440139770507812, Final Batch Loss: 0.6098690032958984\n",
      "Epoch 3886, Loss: 2.5157009661197662, Final Batch Loss: 0.5050476789474487\n",
      "Epoch 3887, Loss: 2.4463980197906494, Final Batch Loss: 0.4803645610809326\n",
      "Epoch 3888, Loss: 2.38966166973114, Final Batch Loss: 0.4285779297351837\n",
      "Epoch 3889, Loss: 2.7920749187469482, Final Batch Loss: 0.47243553400039673\n",
      "Epoch 3890, Loss: 2.5084938406944275, Final Batch Loss: 0.5117703080177307\n",
      "Epoch 3891, Loss: 2.3692116737365723, Final Batch Loss: 0.4709703028202057\n",
      "Epoch 3892, Loss: 2.4639499187469482, Final Batch Loss: 0.5549159049987793\n",
      "Epoch 3893, Loss: 2.4597045183181763, Final Batch Loss: 0.4300720691680908\n",
      "Epoch 3894, Loss: 2.5322103202342987, Final Batch Loss: 0.5944402813911438\n",
      "Epoch 3895, Loss: 2.354237735271454, Final Batch Loss: 0.46148812770843506\n",
      "Epoch 3896, Loss: 2.619594842195511, Final Batch Loss: 0.534098744392395\n",
      "Epoch 3897, Loss: 2.436787724494934, Final Batch Loss: 0.42427533864974976\n",
      "Epoch 3898, Loss: 2.433777928352356, Final Batch Loss: 0.4466879069805145\n",
      "Epoch 3899, Loss: 2.3693572878837585, Final Batch Loss: 0.42467620968818665\n",
      "Epoch 3900, Loss: 2.4662973880767822, Final Batch Loss: 0.45316755771636963\n",
      "Epoch 3901, Loss: 2.5497445464134216, Final Batch Loss: 0.4785238206386566\n",
      "Epoch 3902, Loss: 2.2942810654640198, Final Batch Loss: 0.44106772541999817\n",
      "Epoch 3903, Loss: 2.41933935880661, Final Batch Loss: 0.4685139060020447\n",
      "Epoch 3904, Loss: 2.2982528805732727, Final Batch Loss: 0.6070559620857239\n",
      "Epoch 3905, Loss: 2.4616098403930664, Final Batch Loss: 0.43113332986831665\n",
      "Epoch 3906, Loss: 2.385481357574463, Final Batch Loss: 0.4607241451740265\n",
      "Epoch 3907, Loss: 2.3798359632492065, Final Batch Loss: 0.4631603956222534\n",
      "Epoch 3908, Loss: 2.361965239048004, Final Batch Loss: 0.4800962209701538\n",
      "Epoch 3909, Loss: 2.469673603773117, Final Batch Loss: 0.4747467041015625\n",
      "Epoch 3910, Loss: 2.5313090682029724, Final Batch Loss: 0.5245000123977661\n",
      "Epoch 3911, Loss: 2.366170257329941, Final Batch Loss: 0.5468007326126099\n",
      "Epoch 3912, Loss: 2.5515449047088623, Final Batch Loss: 0.5157960057258606\n",
      "Epoch 3913, Loss: 2.588086426258087, Final Batch Loss: 0.5656780004501343\n",
      "Epoch 3914, Loss: 2.4982022643089294, Final Batch Loss: 0.4791586101055145\n",
      "Epoch 3915, Loss: 2.4724006056785583, Final Batch Loss: 0.48996856808662415\n",
      "Epoch 3916, Loss: 2.2822080850601196, Final Batch Loss: 0.37305688858032227\n",
      "Epoch 3917, Loss: 2.4788200557231903, Final Batch Loss: 0.5615909099578857\n",
      "Epoch 3918, Loss: 2.546717882156372, Final Batch Loss: 0.4371209740638733\n",
      "Epoch 3919, Loss: 2.368372827768326, Final Batch Loss: 0.4872506558895111\n",
      "Epoch 3920, Loss: 2.369497239589691, Final Batch Loss: 0.43713948130607605\n",
      "Epoch 3921, Loss: 2.424281746149063, Final Batch Loss: 0.4002542197704315\n",
      "Epoch 3922, Loss: 2.4412615299224854, Final Batch Loss: 0.4665991961956024\n",
      "Epoch 3923, Loss: 2.4732172787189484, Final Batch Loss: 0.534810483455658\n",
      "Epoch 3924, Loss: 2.494355857372284, Final Batch Loss: 0.5496630072593689\n",
      "Epoch 3925, Loss: 2.3978573977947235, Final Batch Loss: 0.49182161688804626\n",
      "Epoch 3926, Loss: 2.3283399045467377, Final Batch Loss: 0.5559765100479126\n",
      "Epoch 3927, Loss: 2.4157203137874603, Final Batch Loss: 0.3695787489414215\n",
      "Epoch 3928, Loss: 2.3216890692710876, Final Batch Loss: 0.4465582072734833\n",
      "Epoch 3929, Loss: 2.3745656311511993, Final Batch Loss: 0.30543649196624756\n",
      "Epoch 3930, Loss: 2.6158829629421234, Final Batch Loss: 0.5577115416526794\n",
      "Epoch 3931, Loss: 2.4128170907497406, Final Batch Loss: 0.40149930119514465\n",
      "Epoch 3932, Loss: 2.32615265250206, Final Batch Loss: 0.4527354836463928\n",
      "Epoch 3933, Loss: 2.307188242673874, Final Batch Loss: 0.36608877778053284\n",
      "Epoch 3934, Loss: 2.378863960504532, Final Batch Loss: 0.4559434950351715\n",
      "Epoch 3935, Loss: 2.4127625226974487, Final Batch Loss: 0.5193816423416138\n",
      "Epoch 3936, Loss: 2.3970670104026794, Final Batch Loss: 0.483932226896286\n",
      "Epoch 3937, Loss: 2.5009078085422516, Final Batch Loss: 0.41528865694999695\n",
      "Epoch 3938, Loss: 2.570923835039139, Final Batch Loss: 0.5909466743469238\n",
      "Epoch 3939, Loss: 2.4712042212486267, Final Batch Loss: 0.5919873714447021\n",
      "Epoch 3940, Loss: 2.2417722940444946, Final Batch Loss: 0.4254292845726013\n",
      "Epoch 3941, Loss: 2.22433802485466, Final Batch Loss: 0.3868715763092041\n",
      "Epoch 3942, Loss: 2.3307410180568695, Final Batch Loss: 0.39227044582366943\n",
      "Epoch 3943, Loss: 2.55380642414093, Final Batch Loss: 0.6987606883049011\n",
      "Epoch 3944, Loss: 2.3587765097618103, Final Batch Loss: 0.40851980447769165\n",
      "Epoch 3945, Loss: 2.5325406789779663, Final Batch Loss: 0.40016981959342957\n",
      "Epoch 3946, Loss: 2.526100814342499, Final Batch Loss: 0.5545993447303772\n",
      "Epoch 3947, Loss: 2.4067304134368896, Final Batch Loss: 0.48890554904937744\n",
      "Epoch 3948, Loss: 2.342290550470352, Final Batch Loss: 0.464923620223999\n",
      "Epoch 3949, Loss: 2.2359756231307983, Final Batch Loss: 0.37097808718681335\n",
      "Epoch 3950, Loss: 2.312858074903488, Final Batch Loss: 0.43033909797668457\n",
      "Epoch 3951, Loss: 2.6223843693733215, Final Batch Loss: 0.5917948484420776\n",
      "Epoch 3952, Loss: 2.5833812952041626, Final Batch Loss: 0.4649265706539154\n",
      "Epoch 3953, Loss: 2.273361474275589, Final Batch Loss: 0.41054096817970276\n",
      "Epoch 3954, Loss: 2.5586256980895996, Final Batch Loss: 0.5661668181419373\n",
      "Epoch 3955, Loss: 2.619161695241928, Final Batch Loss: 0.5431260466575623\n",
      "Epoch 3956, Loss: 2.5011232495307922, Final Batch Loss: 0.4939281642436981\n",
      "Epoch 3957, Loss: 2.4096052944660187, Final Batch Loss: 0.4678440988063812\n",
      "Epoch 3958, Loss: 2.4978436529636383, Final Batch Loss: 0.5716726183891296\n",
      "Epoch 3959, Loss: 2.559538632631302, Final Batch Loss: 0.584036648273468\n",
      "Epoch 3960, Loss: 2.4158018827438354, Final Batch Loss: 0.3903351426124573\n",
      "Epoch 3961, Loss: 2.423246830701828, Final Batch Loss: 0.3965529799461365\n",
      "Epoch 3962, Loss: 2.400162309408188, Final Batch Loss: 0.5540594458580017\n",
      "Epoch 3963, Loss: 2.491073042154312, Final Batch Loss: 0.5753772258758545\n",
      "Epoch 3964, Loss: 2.5412961840629578, Final Batch Loss: 0.6145781874656677\n",
      "Epoch 3965, Loss: 2.3660649061203003, Final Batch Loss: 0.3928073048591614\n",
      "Epoch 3966, Loss: 2.354325830936432, Final Batch Loss: 0.5374181866645813\n",
      "Epoch 3967, Loss: 2.4091791808605194, Final Batch Loss: 0.5606115460395813\n",
      "Epoch 3968, Loss: 2.441828101873398, Final Batch Loss: 0.48778223991394043\n",
      "Epoch 3969, Loss: 2.375036060810089, Final Batch Loss: 0.3840899169445038\n",
      "Epoch 3970, Loss: 2.3596553206443787, Final Batch Loss: 0.45444461703300476\n",
      "Epoch 3971, Loss: 2.3440869748592377, Final Batch Loss: 0.3741644620895386\n",
      "Epoch 3972, Loss: 2.469496041536331, Final Batch Loss: 0.458761602640152\n",
      "Epoch 3973, Loss: 2.447281450033188, Final Batch Loss: 0.4071522057056427\n",
      "Epoch 3974, Loss: 2.3309541642665863, Final Batch Loss: 0.43341705203056335\n",
      "Epoch 3975, Loss: 2.2652181684970856, Final Batch Loss: 0.5868300795555115\n",
      "Epoch 3976, Loss: 2.4131722450256348, Final Batch Loss: 0.4698019027709961\n",
      "Epoch 3977, Loss: 2.3021422922611237, Final Batch Loss: 0.4415316879749298\n",
      "Epoch 3978, Loss: 2.496317982673645, Final Batch Loss: 0.5179355144500732\n",
      "Epoch 3979, Loss: 2.2502789199352264, Final Batch Loss: 0.34041738510131836\n",
      "Epoch 3980, Loss: 2.393701732158661, Final Batch Loss: 0.49993717670440674\n",
      "Epoch 3981, Loss: 2.45050585269928, Final Batch Loss: 0.5526016354560852\n",
      "Epoch 3982, Loss: 2.3168997764587402, Final Batch Loss: 0.5435325503349304\n",
      "Epoch 3983, Loss: 2.3306744396686554, Final Batch Loss: 0.4759038984775543\n",
      "Epoch 3984, Loss: 2.6554943919181824, Final Batch Loss: 0.4150768518447876\n",
      "Epoch 3985, Loss: 2.569443643093109, Final Batch Loss: 0.5319754481315613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3986, Loss: 2.4548887610435486, Final Batch Loss: 0.4393720030784607\n",
      "Epoch 3987, Loss: 2.523858428001404, Final Batch Loss: 0.5158714056015015\n",
      "Epoch 3988, Loss: 2.548457682132721, Final Batch Loss: 0.5599015951156616\n",
      "Epoch 3989, Loss: 2.3563586473464966, Final Batch Loss: 0.4955493211746216\n",
      "Epoch 3990, Loss: 2.2820707261562347, Final Batch Loss: 0.4394037127494812\n",
      "Epoch 3991, Loss: 2.485032081604004, Final Batch Loss: 0.7360271215438843\n",
      "Epoch 3992, Loss: 2.521329402923584, Final Batch Loss: 0.44507497549057007\n",
      "Epoch 3993, Loss: 2.3960169553756714, Final Batch Loss: 0.45476463437080383\n",
      "Epoch 3994, Loss: 2.4487111568450928, Final Batch Loss: 0.49610042572021484\n",
      "Epoch 3995, Loss: 2.45806285738945, Final Batch Loss: 0.5016244053840637\n",
      "Epoch 3996, Loss: 2.574976861476898, Final Batch Loss: 0.5888740420341492\n",
      "Epoch 3997, Loss: 2.379022032022476, Final Batch Loss: 0.45401695370674133\n",
      "Epoch 3998, Loss: 2.663787215948105, Final Batch Loss: 0.7496542930603027\n",
      "Epoch 3999, Loss: 2.424878418445587, Final Batch Loss: 0.5859509110450745\n",
      "Epoch 4000, Loss: 2.588990092277527, Final Batch Loss: 0.6065771579742432\n",
      "Epoch 4001, Loss: 2.377200722694397, Final Batch Loss: 0.4077680706977844\n",
      "Epoch 4002, Loss: 2.4274175465106964, Final Batch Loss: 0.5300639867782593\n",
      "Epoch 4003, Loss: 2.392193704843521, Final Batch Loss: 0.4470233917236328\n",
      "Epoch 4004, Loss: 2.5449350774288177, Final Batch Loss: 0.7397475242614746\n",
      "Epoch 4005, Loss: 2.532174974679947, Final Batch Loss: 0.5953999757766724\n",
      "Epoch 4006, Loss: 2.2695587277412415, Final Batch Loss: 0.38884463906288147\n",
      "Epoch 4007, Loss: 2.425806403160095, Final Batch Loss: 0.4703966975212097\n",
      "Epoch 4008, Loss: 2.480128586292267, Final Batch Loss: 0.5296027064323425\n",
      "Epoch 4009, Loss: 2.4787142276763916, Final Batch Loss: 0.5463770627975464\n",
      "Epoch 4010, Loss: 2.4224738776683807, Final Batch Loss: 0.5216351747512817\n",
      "Epoch 4011, Loss: 2.420835107564926, Final Batch Loss: 0.4391929805278778\n",
      "Epoch 4012, Loss: 2.470355361700058, Final Batch Loss: 0.5347204804420471\n",
      "Epoch 4013, Loss: 2.387295365333557, Final Batch Loss: 0.4098507761955261\n",
      "Epoch 4014, Loss: 2.4559547901153564, Final Batch Loss: 0.5606862902641296\n",
      "Epoch 4015, Loss: 2.4085825383663177, Final Batch Loss: 0.49208864569664\n",
      "Epoch 4016, Loss: 2.3678184151649475, Final Batch Loss: 0.4970734715461731\n",
      "Epoch 4017, Loss: 2.459255874156952, Final Batch Loss: 0.46556660532951355\n",
      "Epoch 4018, Loss: 2.3583229780197144, Final Batch Loss: 0.47631019353866577\n",
      "Epoch 4019, Loss: 2.2941975593566895, Final Batch Loss: 0.41641953587532043\n",
      "Epoch 4020, Loss: 2.3701497316360474, Final Batch Loss: 0.5169620513916016\n",
      "Epoch 4021, Loss: 2.3093551993370056, Final Batch Loss: 0.44306445121765137\n",
      "Epoch 4022, Loss: 2.3908687233924866, Final Batch Loss: 0.3906368613243103\n",
      "Epoch 4023, Loss: 2.2656002044677734, Final Batch Loss: 0.29475468397140503\n",
      "Epoch 4024, Loss: 2.540970116853714, Final Batch Loss: 0.6167681813240051\n",
      "Epoch 4025, Loss: 2.5739255845546722, Final Batch Loss: 0.5300573706626892\n",
      "Epoch 4026, Loss: 2.324413478374481, Final Batch Loss: 0.4899347722530365\n",
      "Epoch 4027, Loss: 2.352259486913681, Final Batch Loss: 0.44827890396118164\n",
      "Epoch 4028, Loss: 2.4033747017383575, Final Batch Loss: 0.4614429473876953\n",
      "Epoch 4029, Loss: 2.5608922839164734, Final Batch Loss: 0.5562314987182617\n",
      "Epoch 4030, Loss: 2.292982369661331, Final Batch Loss: 0.40643277764320374\n",
      "Epoch 4031, Loss: 2.5351825952529907, Final Batch Loss: 0.4298197627067566\n",
      "Epoch 4032, Loss: 2.464357376098633, Final Batch Loss: 0.540168046951294\n",
      "Epoch 4033, Loss: 2.3381262123584747, Final Batch Loss: 0.45925483107566833\n",
      "Epoch 4034, Loss: 2.376401364803314, Final Batch Loss: 0.4549826383590698\n",
      "Epoch 4035, Loss: 2.3828417360782623, Final Batch Loss: 0.37068861722946167\n",
      "Epoch 4036, Loss: 2.5097888112068176, Final Batch Loss: 0.6421278119087219\n",
      "Epoch 4037, Loss: 2.4957908391952515, Final Batch Loss: 0.5061544179916382\n",
      "Epoch 4038, Loss: 2.2223316729068756, Final Batch Loss: 0.38421010971069336\n",
      "Epoch 4039, Loss: 2.3416720032691956, Final Batch Loss: 0.43333151936531067\n",
      "Epoch 4040, Loss: 2.2626525163650513, Final Batch Loss: 0.3739806115627289\n",
      "Epoch 4041, Loss: 2.5410219728946686, Final Batch Loss: 0.4957432150840759\n",
      "Epoch 4042, Loss: 2.4847424030303955, Final Batch Loss: 0.6920836567878723\n",
      "Epoch 4043, Loss: 2.3802254498004913, Final Batch Loss: 0.5153400897979736\n",
      "Epoch 4044, Loss: 2.458720773458481, Final Batch Loss: 0.40073347091674805\n",
      "Epoch 4045, Loss: 2.526774078607559, Final Batch Loss: 0.4588284194469452\n",
      "Epoch 4046, Loss: 2.4465020298957825, Final Batch Loss: 0.38973936438560486\n",
      "Epoch 4047, Loss: 2.418050706386566, Final Batch Loss: 0.5129873156547546\n",
      "Epoch 4048, Loss: 2.3631316423416138, Final Batch Loss: 0.5305978059768677\n",
      "Epoch 4049, Loss: 2.441739797592163, Final Batch Loss: 0.41831114888191223\n",
      "Epoch 4050, Loss: 2.380481719970703, Final Batch Loss: 0.5409765243530273\n",
      "Epoch 4051, Loss: 2.5027635991573334, Final Batch Loss: 0.48383820056915283\n",
      "Epoch 4052, Loss: 2.584251344203949, Final Batch Loss: 0.47320225834846497\n",
      "Epoch 4053, Loss: 2.291037678718567, Final Batch Loss: 0.3623043894767761\n",
      "Epoch 4054, Loss: 2.550191640853882, Final Batch Loss: 0.5211257338523865\n",
      "Epoch 4055, Loss: 2.3268619179725647, Final Batch Loss: 0.4860694706439972\n",
      "Epoch 4056, Loss: 2.4048080146312714, Final Batch Loss: 0.5279420018196106\n",
      "Epoch 4057, Loss: 2.385459542274475, Final Batch Loss: 0.4898792803287506\n",
      "Epoch 4058, Loss: 2.3349561989307404, Final Batch Loss: 0.5493841171264648\n",
      "Epoch 4059, Loss: 2.2399483621120453, Final Batch Loss: 0.40095242857933044\n",
      "Epoch 4060, Loss: 2.3979627192020416, Final Batch Loss: 0.5394542217254639\n",
      "Epoch 4061, Loss: 2.4434735476970673, Final Batch Loss: 0.4484144151210785\n",
      "Epoch 4062, Loss: 2.3616777658462524, Final Batch Loss: 0.4148728847503662\n",
      "Epoch 4063, Loss: 2.5760840475559235, Final Batch Loss: 0.5076637268066406\n",
      "Epoch 4064, Loss: 2.376833200454712, Final Batch Loss: 0.5350350141525269\n",
      "Epoch 4065, Loss: 2.475869506597519, Final Batch Loss: 0.5033972263336182\n",
      "Epoch 4066, Loss: 2.461287945508957, Final Batch Loss: 0.5316066145896912\n",
      "Epoch 4067, Loss: 2.4630753099918365, Final Batch Loss: 0.5182793140411377\n",
      "Epoch 4068, Loss: 2.346725434064865, Final Batch Loss: 0.432432621717453\n",
      "Epoch 4069, Loss: 2.4090047478675842, Final Batch Loss: 0.5674148797988892\n",
      "Epoch 4070, Loss: 2.536567062139511, Final Batch Loss: 0.5762010216712952\n",
      "Epoch 4071, Loss: 2.5501511991024017, Final Batch Loss: 0.4378102123737335\n",
      "Epoch 4072, Loss: 2.3558937907218933, Final Batch Loss: 0.43067723512649536\n",
      "Epoch 4073, Loss: 2.440764397382736, Final Batch Loss: 0.47176629304885864\n",
      "Epoch 4074, Loss: 2.35938036441803, Final Batch Loss: 0.5041088461875916\n",
      "Epoch 4075, Loss: 2.427512526512146, Final Batch Loss: 0.5601324439048767\n",
      "Epoch 4076, Loss: 2.2899951934814453, Final Batch Loss: 0.4793034493923187\n",
      "Epoch 4077, Loss: 2.333519369363785, Final Batch Loss: 0.4945874512195587\n",
      "Epoch 4078, Loss: 2.5545804500579834, Final Batch Loss: 0.451469361782074\n",
      "Epoch 4079, Loss: 2.4979690611362457, Final Batch Loss: 0.40319499373435974\n",
      "Epoch 4080, Loss: 2.469109207391739, Final Batch Loss: 0.42575526237487793\n",
      "Epoch 4081, Loss: 2.585007756948471, Final Batch Loss: 0.5409783720970154\n",
      "Epoch 4082, Loss: 2.21395006775856, Final Batch Loss: 0.4147603213787079\n",
      "Epoch 4083, Loss: 2.294695943593979, Final Batch Loss: 0.44994837045669556\n",
      "Epoch 4084, Loss: 2.4159820079803467, Final Batch Loss: 0.550711989402771\n",
      "Epoch 4085, Loss: 2.4625086784362793, Final Batch Loss: 0.46115732192993164\n",
      "Epoch 4086, Loss: 2.4040898382663727, Final Batch Loss: 0.4348203241825104\n",
      "Epoch 4087, Loss: 2.575461208820343, Final Batch Loss: 0.5104964971542358\n",
      "Epoch 4088, Loss: 2.4759442806243896, Final Batch Loss: 0.652542769908905\n",
      "Epoch 4089, Loss: 2.306891977787018, Final Batch Loss: 0.5066016316413879\n",
      "Epoch 4090, Loss: 2.4447577595710754, Final Batch Loss: 0.5455499291419983\n",
      "Epoch 4091, Loss: 2.6039710342884064, Final Batch Loss: 0.5963289737701416\n",
      "Epoch 4092, Loss: 2.443490833044052, Final Batch Loss: 0.47735023498535156\n",
      "Epoch 4093, Loss: 2.374740421772003, Final Batch Loss: 0.40187621116638184\n",
      "Epoch 4094, Loss: 2.4564390778541565, Final Batch Loss: 0.5039986968040466\n",
      "Epoch 4095, Loss: 2.3789442777633667, Final Batch Loss: 0.45561113953590393\n",
      "Epoch 4096, Loss: 2.348595231771469, Final Batch Loss: 0.5006335377693176\n",
      "Epoch 4097, Loss: 2.4771874845027924, Final Batch Loss: 0.3471338152885437\n",
      "Epoch 4098, Loss: 2.320755362510681, Final Batch Loss: 0.45410844683647156\n",
      "Epoch 4099, Loss: 2.480827510356903, Final Batch Loss: 0.548717200756073\n",
      "Epoch 4100, Loss: 2.4299910068511963, Final Batch Loss: 0.5004525184631348\n",
      "Epoch 4101, Loss: 2.41705060005188, Final Batch Loss: 0.4879612326622009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4102, Loss: 2.3452199399471283, Final Batch Loss: 0.43592286109924316\n",
      "Epoch 4103, Loss: 2.5216648280620575, Final Batch Loss: 0.5175071358680725\n",
      "Epoch 4104, Loss: 2.297966867685318, Final Batch Loss: 0.5341874361038208\n",
      "Epoch 4105, Loss: 2.465842068195343, Final Batch Loss: 0.4916773736476898\n",
      "Epoch 4106, Loss: 2.511137694120407, Final Batch Loss: 0.607584536075592\n",
      "Epoch 4107, Loss: 2.3860394656658173, Final Batch Loss: 0.4643622040748596\n",
      "Epoch 4108, Loss: 2.311747282743454, Final Batch Loss: 0.4303660988807678\n",
      "Epoch 4109, Loss: 2.59541916847229, Final Batch Loss: 0.617497444152832\n",
      "Epoch 4110, Loss: 2.2765259444713593, Final Batch Loss: 0.39443254470825195\n",
      "Epoch 4111, Loss: 2.350122958421707, Final Batch Loss: 0.4793834388256073\n",
      "Epoch 4112, Loss: 2.398969203233719, Final Batch Loss: 0.3967590034008026\n",
      "Epoch 4113, Loss: 2.3941659927368164, Final Batch Loss: 0.49712198972702026\n",
      "Epoch 4114, Loss: 2.4937230944633484, Final Batch Loss: 0.5709477066993713\n",
      "Epoch 4115, Loss: 2.3723261058330536, Final Batch Loss: 0.45263004302978516\n",
      "Epoch 4116, Loss: 2.3483262956142426, Final Batch Loss: 0.4730575680732727\n",
      "Epoch 4117, Loss: 2.593879848718643, Final Batch Loss: 0.4590287208557129\n",
      "Epoch 4118, Loss: 2.208580583333969, Final Batch Loss: 0.3923598527908325\n",
      "Epoch 4119, Loss: 2.5066262781620026, Final Batch Loss: 0.5301095843315125\n",
      "Epoch 4120, Loss: 2.440668612718582, Final Batch Loss: 0.5104444026947021\n",
      "Epoch 4121, Loss: 2.3036099672317505, Final Batch Loss: 0.35564786195755005\n",
      "Epoch 4122, Loss: 2.3416003584861755, Final Batch Loss: 0.4608376920223236\n",
      "Epoch 4123, Loss: 2.5274712443351746, Final Batch Loss: 0.6115769147872925\n",
      "Epoch 4124, Loss: 2.6325200498104095, Final Batch Loss: 0.5750860571861267\n",
      "Epoch 4125, Loss: 2.391060918569565, Final Batch Loss: 0.5272946357727051\n",
      "Epoch 4126, Loss: 2.3253265619277954, Final Batch Loss: 0.3978559076786041\n",
      "Epoch 4127, Loss: 2.2570930421352386, Final Batch Loss: 0.41609296202659607\n",
      "Epoch 4128, Loss: 2.4783219397068024, Final Batch Loss: 0.4768866300582886\n",
      "Epoch 4129, Loss: 2.455900341272354, Final Batch Loss: 0.48775050044059753\n",
      "Epoch 4130, Loss: 2.267075091600418, Final Batch Loss: 0.3474295735359192\n",
      "Epoch 4131, Loss: 2.5337630212306976, Final Batch Loss: 0.5254960060119629\n",
      "Epoch 4132, Loss: 2.4190482199192047, Final Batch Loss: 0.5179905295372009\n",
      "Epoch 4133, Loss: 2.4302505254745483, Final Batch Loss: 0.4718599021434784\n",
      "Epoch 4134, Loss: 2.4212652444839478, Final Batch Loss: 0.562035083770752\n",
      "Epoch 4135, Loss: 2.6531070172786713, Final Batch Loss: 0.6149678230285645\n",
      "Epoch 4136, Loss: 2.3756222426891327, Final Batch Loss: 0.42183542251586914\n",
      "Epoch 4137, Loss: 2.433788388967514, Final Batch Loss: 0.5250758528709412\n",
      "Epoch 4138, Loss: 2.420163184404373, Final Batch Loss: 0.40926864743232727\n",
      "Epoch 4139, Loss: 2.291866809129715, Final Batch Loss: 0.5291987657546997\n",
      "Epoch 4140, Loss: 2.567191421985626, Final Batch Loss: 0.5636565089225769\n",
      "Epoch 4141, Loss: 2.209111660718918, Final Batch Loss: 0.46594128012657166\n",
      "Epoch 4142, Loss: 2.48273041844368, Final Batch Loss: 0.3987204134464264\n",
      "Epoch 4143, Loss: 2.36345037817955, Final Batch Loss: 0.43940848112106323\n",
      "Epoch 4144, Loss: 2.4907991886138916, Final Batch Loss: 0.6125714778900146\n",
      "Epoch 4145, Loss: 2.307413935661316, Final Batch Loss: 0.43648475408554077\n",
      "Epoch 4146, Loss: 2.6171270310878754, Final Batch Loss: 0.5155869722366333\n",
      "Epoch 4147, Loss: 2.206574469804764, Final Batch Loss: 0.3271014988422394\n",
      "Epoch 4148, Loss: 2.373602271080017, Final Batch Loss: 0.4597151577472687\n",
      "Epoch 4149, Loss: 2.3486139476299286, Final Batch Loss: 0.4098525643348694\n",
      "Epoch 4150, Loss: 2.5265291035175323, Final Batch Loss: 0.4688863754272461\n",
      "Epoch 4151, Loss: 2.3560326993465424, Final Batch Loss: 0.49012550711631775\n",
      "Epoch 4152, Loss: 2.371186912059784, Final Batch Loss: 0.4159018099308014\n",
      "Epoch 4153, Loss: 2.44952329993248, Final Batch Loss: 0.42794692516326904\n",
      "Epoch 4154, Loss: 2.5145671665668488, Final Batch Loss: 0.5543504357337952\n",
      "Epoch 4155, Loss: 2.684716075658798, Final Batch Loss: 0.6897990703582764\n",
      "Epoch 4156, Loss: 2.5902204513549805, Final Batch Loss: 0.5109416246414185\n",
      "Epoch 4157, Loss: 2.3891512155532837, Final Batch Loss: 0.6034685969352722\n",
      "Epoch 4158, Loss: 2.4236404299736023, Final Batch Loss: 0.4832140803337097\n",
      "Epoch 4159, Loss: 2.3589137196540833, Final Batch Loss: 0.451625257730484\n",
      "Epoch 4160, Loss: 2.4256717562675476, Final Batch Loss: 0.46445512771606445\n",
      "Epoch 4161, Loss: 2.4521379470825195, Final Batch Loss: 0.5278614163398743\n",
      "Epoch 4162, Loss: 2.2401938438415527, Final Batch Loss: 0.43352392315864563\n",
      "Epoch 4163, Loss: 2.570108324289322, Final Batch Loss: 0.3759702742099762\n",
      "Epoch 4164, Loss: 2.479365795850754, Final Batch Loss: 0.4480128884315491\n",
      "Epoch 4165, Loss: 2.383563905954361, Final Batch Loss: 0.5851110219955444\n",
      "Epoch 4166, Loss: 2.409016728401184, Final Batch Loss: 0.5244011282920837\n",
      "Epoch 4167, Loss: 2.362272620201111, Final Batch Loss: 0.49951550364494324\n",
      "Epoch 4168, Loss: 2.385743260383606, Final Batch Loss: 0.6223739981651306\n",
      "Epoch 4169, Loss: 2.2863171696662903, Final Batch Loss: 0.39472565054893494\n",
      "Epoch 4170, Loss: 2.3599376678466797, Final Batch Loss: 0.48552677035331726\n",
      "Epoch 4171, Loss: 2.397973418235779, Final Batch Loss: 0.5176118016242981\n",
      "Epoch 4172, Loss: 2.490713745355606, Final Batch Loss: 0.5335325598716736\n",
      "Epoch 4173, Loss: 2.4635927975177765, Final Batch Loss: 0.49849095940589905\n",
      "Epoch 4174, Loss: 2.4082340002059937, Final Batch Loss: 0.4100298583507538\n",
      "Epoch 4175, Loss: 2.4883400201797485, Final Batch Loss: 0.49260208010673523\n",
      "Epoch 4176, Loss: 2.5017809867858887, Final Batch Loss: 0.4272785186767578\n",
      "Epoch 4177, Loss: 2.4859538078308105, Final Batch Loss: 0.4957464337348938\n",
      "Epoch 4178, Loss: 2.4868623316287994, Final Batch Loss: 0.5298652052879333\n",
      "Epoch 4179, Loss: 2.3801209628582, Final Batch Loss: 0.4520931839942932\n",
      "Epoch 4180, Loss: 2.2820306420326233, Final Batch Loss: 0.42449265718460083\n",
      "Epoch 4181, Loss: 2.4309623539447784, Final Batch Loss: 0.5552690029144287\n",
      "Epoch 4182, Loss: 2.479182094335556, Final Batch Loss: 0.4573773145675659\n",
      "Epoch 4183, Loss: 2.707697033882141, Final Batch Loss: 0.64231938123703\n",
      "Epoch 4184, Loss: 2.4204929172992706, Final Batch Loss: 0.538998007774353\n",
      "Epoch 4185, Loss: 2.497925043106079, Final Batch Loss: 0.48673516511917114\n",
      "Epoch 4186, Loss: 2.3716568052768707, Final Batch Loss: 0.3995915949344635\n",
      "Epoch 4187, Loss: 2.3022003173828125, Final Batch Loss: 0.43591710925102234\n",
      "Epoch 4188, Loss: 2.450426995754242, Final Batch Loss: 0.5504046678543091\n",
      "Epoch 4189, Loss: 2.2588462829589844, Final Batch Loss: 0.44460418820381165\n",
      "Epoch 4190, Loss: 2.4736869633197784, Final Batch Loss: 0.6707913875579834\n",
      "Epoch 4191, Loss: 2.3302866518497467, Final Batch Loss: 0.4154673218727112\n",
      "Epoch 4192, Loss: 2.390201151371002, Final Batch Loss: 0.544484555721283\n",
      "Epoch 4193, Loss: 2.3191426396369934, Final Batch Loss: 0.41952410340309143\n",
      "Epoch 4194, Loss: 2.269000470638275, Final Batch Loss: 0.4334102272987366\n",
      "Epoch 4195, Loss: 2.540703237056732, Final Batch Loss: 0.6148119568824768\n",
      "Epoch 4196, Loss: 2.495736837387085, Final Batch Loss: 0.5894696116447449\n",
      "Epoch 4197, Loss: 2.413719117641449, Final Batch Loss: 0.48339128494262695\n",
      "Epoch 4198, Loss: 2.266816735267639, Final Batch Loss: 0.3676743805408478\n",
      "Epoch 4199, Loss: 2.408954679965973, Final Batch Loss: 0.456015020608902\n",
      "Epoch 4200, Loss: 2.538052350282669, Final Batch Loss: 0.523614227771759\n",
      "Epoch 4201, Loss: 2.3941301703453064, Final Batch Loss: 0.5358313322067261\n",
      "Epoch 4202, Loss: 2.4470596611499786, Final Batch Loss: 0.3732481002807617\n",
      "Epoch 4203, Loss: 2.3478699326515198, Final Batch Loss: 0.4642983675003052\n",
      "Epoch 4204, Loss: 2.357249468564987, Final Batch Loss: 0.5099119544029236\n",
      "Epoch 4205, Loss: 2.350127786397934, Final Batch Loss: 0.4503708481788635\n",
      "Epoch 4206, Loss: 2.3421745598316193, Final Batch Loss: 0.4553142786026001\n",
      "Epoch 4207, Loss: 2.4364385902881622, Final Batch Loss: 0.511233925819397\n",
      "Epoch 4208, Loss: 2.467967301607132, Final Batch Loss: 0.5601583123207092\n",
      "Epoch 4209, Loss: 2.3159614503383636, Final Batch Loss: 0.3789360225200653\n",
      "Epoch 4210, Loss: 2.3743132054805756, Final Batch Loss: 0.510396420955658\n",
      "Epoch 4211, Loss: 2.4347845315933228, Final Batch Loss: 0.5356542468070984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4212, Loss: 2.208477884531021, Final Batch Loss: 0.3344559371471405\n",
      "Epoch 4213, Loss: 2.4051617980003357, Final Batch Loss: 0.4825034737586975\n",
      "Epoch 4214, Loss: 2.3789913952350616, Final Batch Loss: 0.44408732652664185\n",
      "Epoch 4215, Loss: 2.2855916917324066, Final Batch Loss: 0.35199499130249023\n",
      "Epoch 4216, Loss: 2.431844413280487, Final Batch Loss: 0.4719715416431427\n",
      "Epoch 4217, Loss: 2.4992816746234894, Final Batch Loss: 0.48951461911201477\n",
      "Epoch 4218, Loss: 2.177492380142212, Final Batch Loss: 0.32383739948272705\n",
      "Epoch 4219, Loss: 2.348140299320221, Final Batch Loss: 0.45264825224876404\n",
      "Epoch 4220, Loss: 2.472653090953827, Final Batch Loss: 0.5556027889251709\n",
      "Epoch 4221, Loss: 2.372058391571045, Final Batch Loss: 0.5176140069961548\n",
      "Epoch 4222, Loss: 2.3683600425720215, Final Batch Loss: 0.5586828589439392\n",
      "Epoch 4223, Loss: 2.4593111276626587, Final Batch Loss: 0.4331590533256531\n",
      "Epoch 4224, Loss: 2.3589809834957123, Final Batch Loss: 0.5265259146690369\n",
      "Epoch 4225, Loss: 2.275771588087082, Final Batch Loss: 0.4035620093345642\n",
      "Epoch 4226, Loss: 2.447935700416565, Final Batch Loss: 0.45560207962989807\n",
      "Epoch 4227, Loss: 2.4794940650463104, Final Batch Loss: 0.38098791241645813\n",
      "Epoch 4228, Loss: 2.2597527503967285, Final Batch Loss: 0.5133799314498901\n",
      "Epoch 4229, Loss: 2.3606317341327667, Final Batch Loss: 0.5233345031738281\n",
      "Epoch 4230, Loss: 2.3600368797779083, Final Batch Loss: 0.4687439501285553\n",
      "Epoch 4231, Loss: 2.4097539484500885, Final Batch Loss: 0.48774978518486023\n",
      "Epoch 4232, Loss: 2.3642690777778625, Final Batch Loss: 0.4356054365634918\n",
      "Epoch 4233, Loss: 2.5006371438503265, Final Batch Loss: 0.42933177947998047\n",
      "Epoch 4234, Loss: 2.4338075816631317, Final Batch Loss: 0.4709397852420807\n",
      "Epoch 4235, Loss: 2.4272282123565674, Final Batch Loss: 0.45085248351097107\n",
      "Epoch 4236, Loss: 2.4043449461460114, Final Batch Loss: 0.4643869996070862\n",
      "Epoch 4237, Loss: 2.4148730635643005, Final Batch Loss: 0.5995188355445862\n",
      "Epoch 4238, Loss: 2.4272047877311707, Final Batch Loss: 0.39333832263946533\n",
      "Epoch 4239, Loss: 2.4573957324028015, Final Batch Loss: 0.45967960357666016\n",
      "Epoch 4240, Loss: 2.5073030591011047, Final Batch Loss: 0.5893162488937378\n",
      "Epoch 4241, Loss: 2.4767559468746185, Final Batch Loss: 0.5139562487602234\n",
      "Epoch 4242, Loss: 2.57501357793808, Final Batch Loss: 0.4589381217956543\n",
      "Epoch 4243, Loss: 2.498069852590561, Final Batch Loss: 0.4938104450702667\n",
      "Epoch 4244, Loss: 2.353907495737076, Final Batch Loss: 0.4508533775806427\n",
      "Epoch 4245, Loss: 2.265229672193527, Final Batch Loss: 0.43595030903816223\n",
      "Epoch 4246, Loss: 2.3894719779491425, Final Batch Loss: 0.5192117094993591\n",
      "Epoch 4247, Loss: 2.280867576599121, Final Batch Loss: 0.3582037687301636\n",
      "Epoch 4248, Loss: 2.5432837903499603, Final Batch Loss: 0.5715890526771545\n",
      "Epoch 4249, Loss: 2.3616127967834473, Final Batch Loss: 0.4037432074546814\n",
      "Epoch 4250, Loss: 2.4355840981006622, Final Batch Loss: 0.6042837500572205\n",
      "Epoch 4251, Loss: 2.2798006534576416, Final Batch Loss: 0.4168801009654999\n",
      "Epoch 4252, Loss: 2.3167725801467896, Final Batch Loss: 0.39803892374038696\n",
      "Epoch 4253, Loss: 2.2992580831050873, Final Batch Loss: 0.581305205821991\n",
      "Epoch 4254, Loss: 2.208817571401596, Final Batch Loss: 0.38593199849128723\n",
      "Epoch 4255, Loss: 2.508748382329941, Final Batch Loss: 0.4710427522659302\n",
      "Epoch 4256, Loss: 2.261779546737671, Final Batch Loss: 0.42943206429481506\n",
      "Epoch 4257, Loss: 2.502314507961273, Final Batch Loss: 0.4517291784286499\n",
      "Epoch 4258, Loss: 2.477144479751587, Final Batch Loss: 0.6391648054122925\n",
      "Epoch 4259, Loss: 2.396248906850815, Final Batch Loss: 0.429997056722641\n",
      "Epoch 4260, Loss: 2.374727040529251, Final Batch Loss: 0.48856401443481445\n",
      "Epoch 4261, Loss: 2.178419202566147, Final Batch Loss: 0.4399854838848114\n",
      "Epoch 4262, Loss: 2.302651435136795, Final Batch Loss: 0.5288043022155762\n",
      "Epoch 4263, Loss: 2.576214551925659, Final Batch Loss: 0.5917567610740662\n",
      "Epoch 4264, Loss: 2.3543680906295776, Final Batch Loss: 0.45454370975494385\n",
      "Epoch 4265, Loss: 2.3754532635211945, Final Batch Loss: 0.4533619284629822\n",
      "Epoch 4266, Loss: 2.423187702894211, Final Batch Loss: 0.5119593739509583\n",
      "Epoch 4267, Loss: 2.516376703977585, Final Batch Loss: 0.450634241104126\n",
      "Epoch 4268, Loss: 2.548539102077484, Final Batch Loss: 0.4260050654411316\n",
      "Epoch 4269, Loss: 2.3482980132102966, Final Batch Loss: 0.45658910274505615\n",
      "Epoch 4270, Loss: 2.3365145325660706, Final Batch Loss: 0.47192519903182983\n",
      "Epoch 4271, Loss: 2.4357964992523193, Final Batch Loss: 0.5522538423538208\n",
      "Epoch 4272, Loss: 2.4275465607643127, Final Batch Loss: 0.4561796486377716\n",
      "Epoch 4273, Loss: 2.4405310451984406, Final Batch Loss: 0.48546043038368225\n",
      "Epoch 4274, Loss: 2.432378351688385, Final Batch Loss: 0.4347952604293823\n",
      "Epoch 4275, Loss: 2.3730313181877136, Final Batch Loss: 0.5311442017555237\n",
      "Epoch 4276, Loss: 2.1565867960453033, Final Batch Loss: 0.3723640739917755\n",
      "Epoch 4277, Loss: 2.360715627670288, Final Batch Loss: 0.5420717000961304\n",
      "Epoch 4278, Loss: 2.579301655292511, Final Batch Loss: 0.5502338409423828\n",
      "Epoch 4279, Loss: 2.374522864818573, Final Batch Loss: 0.4221230745315552\n",
      "Epoch 4280, Loss: 2.518260270357132, Final Batch Loss: 0.42480799555778503\n",
      "Epoch 4281, Loss: 2.354927271604538, Final Batch Loss: 0.45687687397003174\n",
      "Epoch 4282, Loss: 2.447667807340622, Final Batch Loss: 0.5631042718887329\n",
      "Epoch 4283, Loss: 2.4956932961940765, Final Batch Loss: 0.5587081909179688\n",
      "Epoch 4284, Loss: 2.292309582233429, Final Batch Loss: 0.3501482903957367\n",
      "Epoch 4285, Loss: 2.3052503764629364, Final Batch Loss: 0.5506186485290527\n",
      "Epoch 4286, Loss: 2.3275489509105682, Final Batch Loss: 0.42966750264167786\n",
      "Epoch 4287, Loss: 2.4215022325515747, Final Batch Loss: 0.5755929350852966\n",
      "Epoch 4288, Loss: 2.515093743801117, Final Batch Loss: 0.496865838766098\n",
      "Epoch 4289, Loss: 2.47609743475914, Final Batch Loss: 0.5167232155799866\n",
      "Epoch 4290, Loss: 2.364441066980362, Final Batch Loss: 0.5340741276741028\n",
      "Epoch 4291, Loss: 2.4406657814979553, Final Batch Loss: 0.49981069564819336\n",
      "Epoch 4292, Loss: 2.3905992805957794, Final Batch Loss: 0.4608730971813202\n",
      "Epoch 4293, Loss: 2.27865731716156, Final Batch Loss: 0.40066877007484436\n",
      "Epoch 4294, Loss: 2.3356251418590546, Final Batch Loss: 0.4727321267127991\n",
      "Epoch 4295, Loss: 2.4826970994472504, Final Batch Loss: 0.4570237696170807\n",
      "Epoch 4296, Loss: 2.2653723061084747, Final Batch Loss: 0.42509904503822327\n",
      "Epoch 4297, Loss: 2.5088462829589844, Final Batch Loss: 0.5240020751953125\n",
      "Epoch 4298, Loss: 2.407587558031082, Final Batch Loss: 0.46099627017974854\n",
      "Epoch 4299, Loss: 2.3485491573810577, Final Batch Loss: 0.4316709339618683\n",
      "Epoch 4300, Loss: 2.435064911842346, Final Batch Loss: 0.556288480758667\n",
      "Epoch 4301, Loss: 2.172954946756363, Final Batch Loss: 0.4395731985569\n",
      "Epoch 4302, Loss: 2.226657450199127, Final Batch Loss: 0.45212632417678833\n",
      "Epoch 4303, Loss: 2.477581948041916, Final Batch Loss: 0.5278975963592529\n",
      "Epoch 4304, Loss: 2.207174062728882, Final Batch Loss: 0.5164339542388916\n",
      "Epoch 4305, Loss: 2.445824235677719, Final Batch Loss: 0.46609237790107727\n",
      "Epoch 4306, Loss: 2.4164101779460907, Final Batch Loss: 0.5591396689414978\n",
      "Epoch 4307, Loss: 2.422864854335785, Final Batch Loss: 0.38611066341400146\n",
      "Epoch 4308, Loss: 2.3517507314682007, Final Batch Loss: 0.4573226273059845\n",
      "Epoch 4309, Loss: 2.2630122900009155, Final Batch Loss: 0.42757314443588257\n",
      "Epoch 4310, Loss: 2.554928183555603, Final Batch Loss: 0.5535004734992981\n",
      "Epoch 4311, Loss: 2.4718400835990906, Final Batch Loss: 0.581483006477356\n",
      "Epoch 4312, Loss: 2.4270898401737213, Final Batch Loss: 0.5524886846542358\n",
      "Epoch 4313, Loss: 2.4813856184482574, Final Batch Loss: 0.4603860378265381\n",
      "Epoch 4314, Loss: 2.3894456326961517, Final Batch Loss: 0.39863595366477966\n",
      "Epoch 4315, Loss: 2.2845824658870697, Final Batch Loss: 0.415585994720459\n",
      "Epoch 4316, Loss: 2.487464338541031, Final Batch Loss: 0.5662195086479187\n",
      "Epoch 4317, Loss: 2.29177126288414, Final Batch Loss: 0.4373679459095001\n",
      "Epoch 4318, Loss: 2.3881770968437195, Final Batch Loss: 0.4450969696044922\n",
      "Epoch 4319, Loss: 2.5694154500961304, Final Batch Loss: 0.5504120588302612\n",
      "Epoch 4320, Loss: 2.4885553121566772, Final Batch Loss: 0.5800647139549255\n",
      "Epoch 4321, Loss: 2.468565344810486, Final Batch Loss: 0.41672128438949585\n",
      "Epoch 4322, Loss: 2.362330883741379, Final Batch Loss: 0.47449836134910583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4323, Loss: 2.399214029312134, Final Batch Loss: 0.4791705310344696\n",
      "Epoch 4324, Loss: 2.476231426000595, Final Batch Loss: 0.48371371626853943\n",
      "Epoch 4325, Loss: 2.746216982603073, Final Batch Loss: 0.511919379234314\n",
      "Epoch 4326, Loss: 2.2979929745197296, Final Batch Loss: 0.4065335690975189\n",
      "Epoch 4327, Loss: 2.2656944394111633, Final Batch Loss: 0.5435042381286621\n",
      "Epoch 4328, Loss: 2.3071843087673187, Final Batch Loss: 0.3362378478050232\n",
      "Epoch 4329, Loss: 2.5306902825832367, Final Batch Loss: 0.6609403491020203\n",
      "Epoch 4330, Loss: 2.444819837808609, Final Batch Loss: 0.5291653275489807\n",
      "Epoch 4331, Loss: 2.38780814409256, Final Batch Loss: 0.4710373878479004\n",
      "Epoch 4332, Loss: 2.376004695892334, Final Batch Loss: 0.4673880338668823\n",
      "Epoch 4333, Loss: 2.3838245272636414, Final Batch Loss: 0.533132016658783\n",
      "Epoch 4334, Loss: 2.6508730947971344, Final Batch Loss: 0.6170453429222107\n",
      "Epoch 4335, Loss: 2.4306711852550507, Final Batch Loss: 0.4600743353366852\n",
      "Epoch 4336, Loss: 2.6037499010562897, Final Batch Loss: 0.5401448607444763\n",
      "Epoch 4337, Loss: 2.583619236946106, Final Batch Loss: 0.6221466064453125\n",
      "Epoch 4338, Loss: 2.478192687034607, Final Batch Loss: 0.3938765823841095\n",
      "Epoch 4339, Loss: 2.5313885509967804, Final Batch Loss: 0.45409297943115234\n",
      "Epoch 4340, Loss: 2.318863958120346, Final Batch Loss: 0.5603224039077759\n",
      "Epoch 4341, Loss: 2.5277709662914276, Final Batch Loss: 0.5527999997138977\n",
      "Epoch 4342, Loss: 2.3720411360263824, Final Batch Loss: 0.5542447566986084\n",
      "Epoch 4343, Loss: 2.352844476699829, Final Batch Loss: 0.5109982490539551\n",
      "Epoch 4344, Loss: 2.3681526482105255, Final Batch Loss: 0.5323470234870911\n",
      "Epoch 4345, Loss: 2.4173915684223175, Final Batch Loss: 0.525687575340271\n",
      "Epoch 4346, Loss: 2.643633395433426, Final Batch Loss: 0.5046289563179016\n",
      "Epoch 4347, Loss: 2.2967268526554108, Final Batch Loss: 0.3701651394367218\n",
      "Epoch 4348, Loss: 2.204029232263565, Final Batch Loss: 0.44643354415893555\n",
      "Epoch 4349, Loss: 2.510191559791565, Final Batch Loss: 0.39143669605255127\n",
      "Epoch 4350, Loss: 2.6028596460819244, Final Batch Loss: 0.6100384593009949\n",
      "Epoch 4351, Loss: 2.2860496938228607, Final Batch Loss: 0.4782971143722534\n",
      "Epoch 4352, Loss: 2.412960469722748, Final Batch Loss: 0.4427497982978821\n",
      "Epoch 4353, Loss: 2.36244198679924, Final Batch Loss: 0.46430033445358276\n",
      "Epoch 4354, Loss: 2.3871347904205322, Final Batch Loss: 0.38806208968162537\n",
      "Epoch 4355, Loss: 2.421872556209564, Final Batch Loss: 0.5369656682014465\n",
      "Epoch 4356, Loss: 2.3680257499217987, Final Batch Loss: 0.5203425884246826\n",
      "Epoch 4357, Loss: 2.429578483104706, Final Batch Loss: 0.49383509159088135\n",
      "Epoch 4358, Loss: 2.365084707736969, Final Batch Loss: 0.37157729268074036\n",
      "Epoch 4359, Loss: 2.3139469623565674, Final Batch Loss: 0.4856328070163727\n",
      "Epoch 4360, Loss: 2.269031524658203, Final Batch Loss: 0.43991756439208984\n",
      "Epoch 4361, Loss: 2.3546468317508698, Final Batch Loss: 0.5499590039253235\n",
      "Epoch 4362, Loss: 2.355142742395401, Final Batch Loss: 0.3447294533252716\n",
      "Epoch 4363, Loss: 2.471775770187378, Final Batch Loss: 0.455530047416687\n",
      "Epoch 4364, Loss: 2.383018732070923, Final Batch Loss: 0.42641085386276245\n",
      "Epoch 4365, Loss: 2.506598860025406, Final Batch Loss: 0.5113024115562439\n",
      "Epoch 4366, Loss: 2.3398901224136353, Final Batch Loss: 0.46161237359046936\n",
      "Epoch 4367, Loss: 2.2742659747600555, Final Batch Loss: 0.5151321291923523\n",
      "Epoch 4368, Loss: 2.2686798870563507, Final Batch Loss: 0.4407886564731598\n",
      "Epoch 4369, Loss: 2.396287828683853, Final Batch Loss: 0.4375339150428772\n",
      "Epoch 4370, Loss: 2.474053829908371, Final Batch Loss: 0.5333094000816345\n",
      "Epoch 4371, Loss: 2.281199127435684, Final Batch Loss: 0.3005600869655609\n",
      "Epoch 4372, Loss: 2.3726986348629, Final Batch Loss: 0.4988211393356323\n",
      "Epoch 4373, Loss: 2.315725475549698, Final Batch Loss: 0.4748018980026245\n",
      "Epoch 4374, Loss: 2.329169899225235, Final Batch Loss: 0.5103940367698669\n",
      "Epoch 4375, Loss: 2.3846478164196014, Final Batch Loss: 0.3663993179798126\n",
      "Epoch 4376, Loss: 2.2664782404899597, Final Batch Loss: 0.48949459195137024\n",
      "Epoch 4377, Loss: 2.456243246793747, Final Batch Loss: 0.5354641079902649\n",
      "Epoch 4378, Loss: 2.3534866273403168, Final Batch Loss: 0.4578840732574463\n",
      "Epoch 4379, Loss: 2.3153777718544006, Final Batch Loss: 0.5053460597991943\n",
      "Epoch 4380, Loss: 2.24984347820282, Final Batch Loss: 0.3419533967971802\n",
      "Epoch 4381, Loss: 2.489879012107849, Final Batch Loss: 0.46547967195510864\n",
      "Epoch 4382, Loss: 2.2550176978111267, Final Batch Loss: 0.5029065608978271\n",
      "Epoch 4383, Loss: 2.3603886365890503, Final Batch Loss: 0.3990991711616516\n",
      "Epoch 4384, Loss: 2.1940630972385406, Final Batch Loss: 0.37576279044151306\n",
      "Epoch 4385, Loss: 2.4312878251075745, Final Batch Loss: 0.527586817741394\n",
      "Epoch 4386, Loss: 2.1904213428497314, Final Batch Loss: 0.3134467601776123\n",
      "Epoch 4387, Loss: 2.427351653575897, Final Batch Loss: 0.4488000273704529\n",
      "Epoch 4388, Loss: 2.4956791698932648, Final Batch Loss: 0.515079140663147\n",
      "Epoch 4389, Loss: 2.3622982800006866, Final Batch Loss: 0.5602458119392395\n",
      "Epoch 4390, Loss: 2.356159210205078, Final Batch Loss: 0.5269764065742493\n",
      "Epoch 4391, Loss: 2.2050740718841553, Final Batch Loss: 0.396589457988739\n",
      "Epoch 4392, Loss: 2.3059118390083313, Final Batch Loss: 0.5162997841835022\n",
      "Epoch 4393, Loss: 2.369542896747589, Final Batch Loss: 0.4760628938674927\n",
      "Epoch 4394, Loss: 2.365403264760971, Final Batch Loss: 0.46502795815467834\n",
      "Epoch 4395, Loss: 2.2129174172878265, Final Batch Loss: 0.47309958934783936\n",
      "Epoch 4396, Loss: 2.5002061426639557, Final Batch Loss: 0.5692810416221619\n",
      "Epoch 4397, Loss: 2.2204988300800323, Final Batch Loss: 0.42586749792099\n",
      "Epoch 4398, Loss: 2.3420713543891907, Final Batch Loss: 0.42322394251823425\n",
      "Epoch 4399, Loss: 2.2867092192173004, Final Batch Loss: 0.4225532114505768\n",
      "Epoch 4400, Loss: 2.421657979488373, Final Batch Loss: 0.4985080063343048\n",
      "Epoch 4401, Loss: 2.3209000527858734, Final Batch Loss: 0.4432988166809082\n",
      "Epoch 4402, Loss: 2.4958347976207733, Final Batch Loss: 0.43695008754730225\n",
      "Epoch 4403, Loss: 2.3905270099639893, Final Batch Loss: 0.47733065485954285\n",
      "Epoch 4404, Loss: 2.508518159389496, Final Batch Loss: 0.48675277829170227\n",
      "Epoch 4405, Loss: 2.3431306779384613, Final Batch Loss: 0.4994926154613495\n",
      "Epoch 4406, Loss: 2.325596511363983, Final Batch Loss: 0.5133624076843262\n",
      "Epoch 4407, Loss: 2.351790577173233, Final Batch Loss: 0.5409193634986877\n",
      "Epoch 4408, Loss: 2.509801983833313, Final Batch Loss: 0.6839099526405334\n",
      "Epoch 4409, Loss: 2.3336020708084106, Final Batch Loss: 0.4634817838668823\n",
      "Epoch 4410, Loss: 2.332433760166168, Final Batch Loss: 0.4818134307861328\n",
      "Epoch 4411, Loss: 2.2986004054546356, Final Batch Loss: 0.48199594020843506\n",
      "Epoch 4412, Loss: 2.249831587076187, Final Batch Loss: 0.429134339094162\n",
      "Epoch 4413, Loss: 2.376077711582184, Final Batch Loss: 0.4555884003639221\n",
      "Epoch 4414, Loss: 2.5168846249580383, Final Batch Loss: 0.4790152311325073\n",
      "Epoch 4415, Loss: 2.387923240661621, Final Batch Loss: 0.4782693684101105\n",
      "Epoch 4416, Loss: 2.4032345712184906, Final Batch Loss: 0.4575030505657196\n",
      "Epoch 4417, Loss: 2.253013104200363, Final Batch Loss: 0.4613606929779053\n",
      "Epoch 4418, Loss: 2.5733197927474976, Final Batch Loss: 0.5053958296775818\n",
      "Epoch 4419, Loss: 2.380103588104248, Final Batch Loss: 0.4995102882385254\n",
      "Epoch 4420, Loss: 2.4433530271053314, Final Batch Loss: 0.5627555251121521\n",
      "Epoch 4421, Loss: 2.3635277450084686, Final Batch Loss: 0.45151692628860474\n",
      "Epoch 4422, Loss: 2.220202535390854, Final Batch Loss: 0.4695289731025696\n",
      "Epoch 4423, Loss: 2.3266241252422333, Final Batch Loss: 0.44772765040397644\n",
      "Epoch 4424, Loss: 2.34502911567688, Final Batch Loss: 0.4512735903263092\n",
      "Epoch 4425, Loss: 2.2691389322280884, Final Batch Loss: 0.2788800299167633\n",
      "Epoch 4426, Loss: 2.4089511930942535, Final Batch Loss: 0.44020339846611023\n",
      "Epoch 4427, Loss: 2.3379464149475098, Final Batch Loss: 0.4482590854167938\n",
      "Epoch 4428, Loss: 2.49424210190773, Final Batch Loss: 0.5087268352508545\n",
      "Epoch 4429, Loss: 2.388667345046997, Final Batch Loss: 0.4281059205532074\n",
      "Epoch 4430, Loss: 2.483662724494934, Final Batch Loss: 0.5505703091621399\n",
      "Epoch 4431, Loss: 2.407891124486923, Final Batch Loss: 0.41893520951271057\n",
      "Epoch 4432, Loss: 2.231604367494583, Final Batch Loss: 0.45587605237960815\n",
      "Epoch 4433, Loss: 2.535446137189865, Final Batch Loss: 0.48078182339668274\n",
      "Epoch 4434, Loss: 2.4809356033802032, Final Batch Loss: 0.4736526608467102\n",
      "Epoch 4435, Loss: 2.305395722389221, Final Batch Loss: 0.447818785905838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4436, Loss: 2.3120592534542084, Final Batch Loss: 0.5059369206428528\n",
      "Epoch 4437, Loss: 2.5011718571186066, Final Batch Loss: 0.507696807384491\n",
      "Epoch 4438, Loss: 2.3481993675231934, Final Batch Loss: 0.5148206353187561\n",
      "Epoch 4439, Loss: 2.3214962482452393, Final Batch Loss: 0.43224212527275085\n",
      "Epoch 4440, Loss: 2.2966255247592926, Final Batch Loss: 0.42409536242485046\n",
      "Epoch 4441, Loss: 2.407034158706665, Final Batch Loss: 0.5350145101547241\n",
      "Epoch 4442, Loss: 2.272421717643738, Final Batch Loss: 0.45437201857566833\n",
      "Epoch 4443, Loss: 2.2973690927028656, Final Batch Loss: 0.3971330523490906\n",
      "Epoch 4444, Loss: 2.3742934465408325, Final Batch Loss: 0.5720450282096863\n",
      "Epoch 4445, Loss: 2.4862035512924194, Final Batch Loss: 0.5532819628715515\n",
      "Epoch 4446, Loss: 2.349211275577545, Final Batch Loss: 0.517646312713623\n",
      "Epoch 4447, Loss: 2.172530025243759, Final Batch Loss: 0.42027878761291504\n",
      "Epoch 4448, Loss: 2.5084463953971863, Final Batch Loss: 0.5862271785736084\n",
      "Epoch 4449, Loss: 2.35703906416893, Final Batch Loss: 0.5930933356285095\n",
      "Epoch 4450, Loss: 2.417746126651764, Final Batch Loss: 0.5261164903640747\n",
      "Epoch 4451, Loss: 2.530389815568924, Final Batch Loss: 0.6528263092041016\n",
      "Epoch 4452, Loss: 2.5184936821460724, Final Batch Loss: 0.5438656806945801\n",
      "Epoch 4453, Loss: 2.293915569782257, Final Batch Loss: 0.4361061453819275\n",
      "Epoch 4454, Loss: 2.377344638109207, Final Batch Loss: 0.5269372463226318\n",
      "Epoch 4455, Loss: 2.2502101957798004, Final Batch Loss: 0.446461945772171\n",
      "Epoch 4456, Loss: 2.3325480222702026, Final Batch Loss: 0.5207722187042236\n",
      "Epoch 4457, Loss: 2.4064821302890778, Final Batch Loss: 0.507546603679657\n",
      "Epoch 4458, Loss: 2.3453590869903564, Final Batch Loss: 0.4813918471336365\n",
      "Epoch 4459, Loss: 2.3370716869831085, Final Batch Loss: 0.42475175857543945\n",
      "Epoch 4460, Loss: 2.4295357167720795, Final Batch Loss: 0.48987969756126404\n",
      "Epoch 4461, Loss: 2.2400538623332977, Final Batch Loss: 0.4563120901584625\n",
      "Epoch 4462, Loss: 2.530234009027481, Final Batch Loss: 0.7955369353294373\n",
      "Epoch 4463, Loss: 2.5805287957191467, Final Batch Loss: 0.4736340343952179\n",
      "Epoch 4464, Loss: 2.2426734268665314, Final Batch Loss: 0.3550456464290619\n",
      "Epoch 4465, Loss: 2.3990731835365295, Final Batch Loss: 0.6218686699867249\n",
      "Epoch 4466, Loss: 2.3514106273651123, Final Batch Loss: 0.4677649736404419\n",
      "Epoch 4467, Loss: 2.292798340320587, Final Batch Loss: 0.37735769152641296\n",
      "Epoch 4468, Loss: 2.3774755597114563, Final Batch Loss: 0.4448232054710388\n",
      "Epoch 4469, Loss: 2.5029941499233246, Final Batch Loss: 0.5957370400428772\n",
      "Epoch 4470, Loss: 2.2286277413368225, Final Batch Loss: 0.4185646176338196\n",
      "Epoch 4471, Loss: 2.3667054176330566, Final Batch Loss: 0.5318172574043274\n",
      "Epoch 4472, Loss: 2.3123502135276794, Final Batch Loss: 0.4975625276565552\n",
      "Epoch 4473, Loss: 2.4033550024032593, Final Batch Loss: 0.4819967746734619\n",
      "Epoch 4474, Loss: 2.3345278799533844, Final Batch Loss: 0.3998291790485382\n",
      "Epoch 4475, Loss: 2.3227799832820892, Final Batch Loss: 0.41677552461624146\n",
      "Epoch 4476, Loss: 2.3689171075820923, Final Batch Loss: 0.4110727906227112\n",
      "Epoch 4477, Loss: 2.33883073925972, Final Batch Loss: 0.4044157564640045\n",
      "Epoch 4478, Loss: 2.3276498913764954, Final Batch Loss: 0.5186825394630432\n",
      "Epoch 4479, Loss: 2.238146096467972, Final Batch Loss: 0.38783690333366394\n",
      "Epoch 4480, Loss: 2.251304507255554, Final Batch Loss: 0.41901135444641113\n",
      "Epoch 4481, Loss: 2.196559816598892, Final Batch Loss: 0.2973131835460663\n",
      "Epoch 4482, Loss: 2.2647555470466614, Final Batch Loss: 0.4083954989910126\n",
      "Epoch 4483, Loss: 2.4193640053272247, Final Batch Loss: 0.48971500992774963\n",
      "Epoch 4484, Loss: 2.4879625141620636, Final Batch Loss: 0.5785567760467529\n",
      "Epoch 4485, Loss: 2.264900505542755, Final Batch Loss: 0.42182987928390503\n",
      "Epoch 4486, Loss: 2.3773433566093445, Final Batch Loss: 0.49256202578544617\n",
      "Epoch 4487, Loss: 2.373043805360794, Final Batch Loss: 0.5599870085716248\n",
      "Epoch 4488, Loss: 2.22510102391243, Final Batch Loss: 0.32796379923820496\n",
      "Epoch 4489, Loss: 2.314910978078842, Final Batch Loss: 0.3996464014053345\n",
      "Epoch 4490, Loss: 2.2666473388671875, Final Batch Loss: 0.47124314308166504\n",
      "Epoch 4491, Loss: 2.58921217918396, Final Batch Loss: 0.6574327945709229\n",
      "Epoch 4492, Loss: 2.4363452792167664, Final Batch Loss: 0.39345574378967285\n",
      "Epoch 4493, Loss: 2.40126234292984, Final Batch Loss: 0.40328249335289\n",
      "Epoch 4494, Loss: 2.3729844093322754, Final Batch Loss: 0.40332797169685364\n",
      "Epoch 4495, Loss: 2.348157584667206, Final Batch Loss: 0.5440641045570374\n",
      "Epoch 4496, Loss: 2.25017848610878, Final Batch Loss: 0.44164323806762695\n",
      "Epoch 4497, Loss: 2.326344907283783, Final Batch Loss: 0.45753636956214905\n",
      "Epoch 4498, Loss: 2.323312282562256, Final Batch Loss: 0.5230666399002075\n",
      "Epoch 4499, Loss: 2.33764785528183, Final Batch Loss: 0.462785542011261\n",
      "Epoch 4500, Loss: 2.477940082550049, Final Batch Loss: 0.541577935218811\n",
      "Epoch 4501, Loss: 2.3292702734470367, Final Batch Loss: 0.452114075422287\n",
      "Epoch 4502, Loss: 2.3421782851219177, Final Batch Loss: 0.5407716631889343\n",
      "Epoch 4503, Loss: 2.399039775133133, Final Batch Loss: 0.40935656428337097\n",
      "Epoch 4504, Loss: 2.361469477415085, Final Batch Loss: 0.4408435821533203\n",
      "Epoch 4505, Loss: 2.2686934769153595, Final Batch Loss: 0.498031884431839\n",
      "Epoch 4506, Loss: 2.553052634000778, Final Batch Loss: 0.39158716797828674\n",
      "Epoch 4507, Loss: 2.1661878526210785, Final Batch Loss: 0.5406240820884705\n",
      "Epoch 4508, Loss: 2.3435234129428864, Final Batch Loss: 0.4622662663459778\n",
      "Epoch 4509, Loss: 2.2170335054397583, Final Batch Loss: 0.3645857572555542\n",
      "Epoch 4510, Loss: 2.3380686044692993, Final Batch Loss: 0.3969670534133911\n",
      "Epoch 4511, Loss: 2.285919666290283, Final Batch Loss: 0.5227743983268738\n",
      "Epoch 4512, Loss: 2.3211256563663483, Final Batch Loss: 0.43714895844459534\n",
      "Epoch 4513, Loss: 2.2504409551620483, Final Batch Loss: 0.45441025495529175\n",
      "Epoch 4514, Loss: 2.2421325147151947, Final Batch Loss: 0.5175873041152954\n",
      "Epoch 4515, Loss: 2.286524474620819, Final Batch Loss: 0.5396609306335449\n",
      "Epoch 4516, Loss: 2.3411490619182587, Final Batch Loss: 0.5057129859924316\n",
      "Epoch 4517, Loss: 2.5675211548805237, Final Batch Loss: 0.6581011414527893\n",
      "Epoch 4518, Loss: 2.2751434445381165, Final Batch Loss: 0.5171822905540466\n",
      "Epoch 4519, Loss: 2.2188455164432526, Final Batch Loss: 0.3964006304740906\n",
      "Epoch 4520, Loss: 2.3571584224700928, Final Batch Loss: 0.41202396154403687\n",
      "Epoch 4521, Loss: 2.328419029712677, Final Batch Loss: 0.5287989377975464\n",
      "Epoch 4522, Loss: 2.2587450742721558, Final Batch Loss: 0.4137406051158905\n",
      "Epoch 4523, Loss: 2.3607232868671417, Final Batch Loss: 0.46719828248023987\n",
      "Epoch 4524, Loss: 2.2615097165107727, Final Batch Loss: 0.3843505084514618\n",
      "Epoch 4525, Loss: 2.3529317677021027, Final Batch Loss: 0.4254876971244812\n",
      "Epoch 4526, Loss: 2.3916072249412537, Final Batch Loss: 0.44154104590415955\n",
      "Epoch 4527, Loss: 2.3399072885513306, Final Batch Loss: 0.4574914276599884\n",
      "Epoch 4528, Loss: 2.2873592376708984, Final Batch Loss: 0.47710224986076355\n",
      "Epoch 4529, Loss: 2.4720064103603363, Final Batch Loss: 0.5039154291152954\n",
      "Epoch 4530, Loss: 2.271955668926239, Final Batch Loss: 0.5361202359199524\n",
      "Epoch 4531, Loss: 2.4631039202213287, Final Batch Loss: 0.5084536671638489\n",
      "Epoch 4532, Loss: 2.5629408955574036, Final Batch Loss: 0.5344167351722717\n",
      "Epoch 4533, Loss: 2.3075539767742157, Final Batch Loss: 0.4970627427101135\n",
      "Epoch 4534, Loss: 2.284775674343109, Final Batch Loss: 0.3762511909008026\n",
      "Epoch 4535, Loss: 2.18513885140419, Final Batch Loss: 0.3838178217411041\n",
      "Epoch 4536, Loss: 2.3244476914405823, Final Batch Loss: 0.6017284393310547\n",
      "Epoch 4537, Loss: 2.304145932197571, Final Batch Loss: 0.484380304813385\n",
      "Epoch 4538, Loss: 2.2378136217594147, Final Batch Loss: 0.5254862308502197\n",
      "Epoch 4539, Loss: 2.5054270029067993, Final Batch Loss: 0.6455411314964294\n",
      "Epoch 4540, Loss: 2.310150533914566, Final Batch Loss: 0.39404746890068054\n",
      "Epoch 4541, Loss: 2.382903039455414, Final Batch Loss: 0.45644378662109375\n",
      "Epoch 4542, Loss: 2.301796793937683, Final Batch Loss: 0.4398420751094818\n",
      "Epoch 4543, Loss: 2.269686669111252, Final Batch Loss: 0.4542195796966553\n",
      "Epoch 4544, Loss: 2.358612895011902, Final Batch Loss: 0.6101685166358948\n",
      "Epoch 4545, Loss: 2.3095023036003113, Final Batch Loss: 0.42535844445228577\n",
      "Epoch 4546, Loss: 2.4705294966697693, Final Batch Loss: 0.656669557094574\n",
      "Epoch 4547, Loss: 2.238182455301285, Final Batch Loss: 0.3919369578361511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4548, Loss: 2.537673830986023, Final Batch Loss: 0.5436596870422363\n",
      "Epoch 4549, Loss: 2.2869727313518524, Final Batch Loss: 0.5526507496833801\n",
      "Epoch 4550, Loss: 2.3151935040950775, Final Batch Loss: 0.36421999335289\n",
      "Epoch 4551, Loss: 2.3792520463466644, Final Batch Loss: 0.4729315936565399\n",
      "Epoch 4552, Loss: 2.391135185956955, Final Batch Loss: 0.42890775203704834\n",
      "Epoch 4553, Loss: 2.2594364285469055, Final Batch Loss: 0.4137433171272278\n",
      "Epoch 4554, Loss: 2.327757179737091, Final Batch Loss: 0.413507878780365\n",
      "Epoch 4555, Loss: 2.3365108370780945, Final Batch Loss: 0.3308623135089874\n",
      "Epoch 4556, Loss: 2.325100690126419, Final Batch Loss: 0.4388049840927124\n",
      "Epoch 4557, Loss: 2.2392952740192413, Final Batch Loss: 0.42033639550209045\n",
      "Epoch 4558, Loss: 2.2593028247356415, Final Batch Loss: 0.478781133890152\n",
      "Epoch 4559, Loss: 2.4310477674007416, Final Batch Loss: 0.38471877574920654\n",
      "Epoch 4560, Loss: 2.5282900035381317, Final Batch Loss: 0.6344799995422363\n",
      "Epoch 4561, Loss: 2.4604981541633606, Final Batch Loss: 0.41859909892082214\n",
      "Epoch 4562, Loss: 2.310590624809265, Final Batch Loss: 0.526762843132019\n",
      "Epoch 4563, Loss: 2.153785824775696, Final Batch Loss: 0.31205272674560547\n",
      "Epoch 4564, Loss: 2.2375415563583374, Final Batch Loss: 0.4947744309902191\n",
      "Epoch 4565, Loss: 2.225623369216919, Final Batch Loss: 0.37324076890945435\n",
      "Epoch 4566, Loss: 2.397785574197769, Final Batch Loss: 0.5174105167388916\n",
      "Epoch 4567, Loss: 2.2253432273864746, Final Batch Loss: 0.42765775322914124\n",
      "Epoch 4568, Loss: 2.168358325958252, Final Batch Loss: 0.3875894546508789\n",
      "Epoch 4569, Loss: 2.2629596292972565, Final Batch Loss: 0.4080595374107361\n",
      "Epoch 4570, Loss: 2.5706258714199066, Final Batch Loss: 0.7610628604888916\n",
      "Epoch 4571, Loss: 2.3924897611141205, Final Batch Loss: 0.4573827385902405\n",
      "Epoch 4572, Loss: 2.354014962911606, Final Batch Loss: 0.47252339124679565\n",
      "Epoch 4573, Loss: 2.469165086746216, Final Batch Loss: 0.4146440625190735\n",
      "Epoch 4574, Loss: 2.394162595272064, Final Batch Loss: 0.4371146857738495\n",
      "Epoch 4575, Loss: 2.083073168992996, Final Batch Loss: 0.3970458507537842\n",
      "Epoch 4576, Loss: 2.2359162867069244, Final Batch Loss: 0.4123365879058838\n",
      "Epoch 4577, Loss: 2.4850857853889465, Final Batch Loss: 0.6050868034362793\n",
      "Epoch 4578, Loss: 2.31424617767334, Final Batch Loss: 0.4655527174472809\n",
      "Epoch 4579, Loss: 2.3865436017513275, Final Batch Loss: 0.48493310809135437\n",
      "Epoch 4580, Loss: 2.3705219328403473, Final Batch Loss: 0.4769091308116913\n",
      "Epoch 4581, Loss: 2.2960351705551147, Final Batch Loss: 0.4266843795776367\n",
      "Epoch 4582, Loss: 2.360274523496628, Final Batch Loss: 0.5771044492721558\n",
      "Epoch 4583, Loss: 2.387515068054199, Final Batch Loss: 0.44289928674697876\n",
      "Epoch 4584, Loss: 2.3886601328849792, Final Batch Loss: 0.4380064904689789\n",
      "Epoch 4585, Loss: 2.2893485128879547, Final Batch Loss: 0.4559027850627899\n",
      "Epoch 4586, Loss: 2.41180819272995, Final Batch Loss: 0.5699572563171387\n",
      "Epoch 4587, Loss: 2.3834384977817535, Final Batch Loss: 0.4477657973766327\n",
      "Epoch 4588, Loss: 2.623303771018982, Final Batch Loss: 0.6282927989959717\n",
      "Epoch 4589, Loss: 2.300414562225342, Final Batch Loss: 0.5174177289009094\n",
      "Epoch 4590, Loss: 2.454425662755966, Final Batch Loss: 0.4987366199493408\n",
      "Epoch 4591, Loss: 2.24896639585495, Final Batch Loss: 0.411376953125\n",
      "Epoch 4592, Loss: 2.3646180331707, Final Batch Loss: 0.4126853048801422\n",
      "Epoch 4593, Loss: 2.409326285123825, Final Batch Loss: 0.39189082384109497\n",
      "Epoch 4594, Loss: 2.37229922413826, Final Batch Loss: 0.5669786930084229\n",
      "Epoch 4595, Loss: 2.4301121532917023, Final Batch Loss: 0.35874947905540466\n",
      "Epoch 4596, Loss: 2.2621214389801025, Final Batch Loss: 0.42479556798934937\n",
      "Epoch 4597, Loss: 2.481219559907913, Final Batch Loss: 0.47157949209213257\n",
      "Epoch 4598, Loss: 2.1774457693099976, Final Batch Loss: 0.40803104639053345\n",
      "Epoch 4599, Loss: 2.3390848338603973, Final Batch Loss: 0.44624632596969604\n",
      "Epoch 4600, Loss: 2.2862043976783752, Final Batch Loss: 0.477742463350296\n",
      "Epoch 4601, Loss: 2.3799628615379333, Final Batch Loss: 0.4656383693218231\n",
      "Epoch 4602, Loss: 2.469104826450348, Final Batch Loss: 0.44642603397369385\n",
      "Epoch 4603, Loss: 2.397173196077347, Final Batch Loss: 0.4796047806739807\n",
      "Epoch 4604, Loss: 2.3583235442638397, Final Batch Loss: 0.42525148391723633\n",
      "Epoch 4605, Loss: 2.423397481441498, Final Batch Loss: 0.4573551118373871\n",
      "Epoch 4606, Loss: 2.2904856503009796, Final Batch Loss: 0.5285549759864807\n",
      "Epoch 4607, Loss: 2.453448235988617, Final Batch Loss: 0.4787836968898773\n",
      "Epoch 4608, Loss: 2.133474588394165, Final Batch Loss: 0.3791356086730957\n",
      "Epoch 4609, Loss: 2.3363168239593506, Final Batch Loss: 0.49858441948890686\n",
      "Epoch 4610, Loss: 2.2718445956707, Final Batch Loss: 0.4870645999908447\n",
      "Epoch 4611, Loss: 2.3586080372333527, Final Batch Loss: 0.4424256980419159\n",
      "Epoch 4612, Loss: 2.297359734773636, Final Batch Loss: 0.3666726052761078\n",
      "Epoch 4613, Loss: 2.525617927312851, Final Batch Loss: 0.5525556802749634\n",
      "Epoch 4614, Loss: 2.423642724752426, Final Batch Loss: 0.4921855926513672\n",
      "Epoch 4615, Loss: 2.1427564322948456, Final Batch Loss: 0.47930070757865906\n",
      "Epoch 4616, Loss: 2.293351650238037, Final Batch Loss: 0.43965768814086914\n",
      "Epoch 4617, Loss: 2.248358130455017, Final Batch Loss: 0.3975689113140106\n",
      "Epoch 4618, Loss: 2.18107008934021, Final Batch Loss: 0.4117291271686554\n",
      "Epoch 4619, Loss: 2.241754472255707, Final Batch Loss: 0.356788694858551\n",
      "Epoch 4620, Loss: 2.4075503051280975, Final Batch Loss: 0.40414053201675415\n",
      "Epoch 4621, Loss: 2.24934783577919, Final Batch Loss: 0.4992174804210663\n",
      "Epoch 4622, Loss: 2.3395701944828033, Final Batch Loss: 0.43893060088157654\n",
      "Epoch 4623, Loss: 2.3752708435058594, Final Batch Loss: 0.5477741956710815\n",
      "Epoch 4624, Loss: 2.2989051043987274, Final Batch Loss: 0.41612470149993896\n",
      "Epoch 4625, Loss: 2.383149594068527, Final Batch Loss: 0.4684269428253174\n",
      "Epoch 4626, Loss: 2.3109227120876312, Final Batch Loss: 0.4986713230609894\n",
      "Epoch 4627, Loss: 2.2859801054000854, Final Batch Loss: 0.347807377576828\n",
      "Epoch 4628, Loss: 2.426382005214691, Final Batch Loss: 0.4941937029361725\n",
      "Epoch 4629, Loss: 2.5727595686912537, Final Batch Loss: 0.7415084838867188\n",
      "Epoch 4630, Loss: 2.4022575318813324, Final Batch Loss: 0.5030453205108643\n",
      "Epoch 4631, Loss: 2.3472661077976227, Final Batch Loss: 0.4052099585533142\n",
      "Epoch 4632, Loss: 2.2777610421180725, Final Batch Loss: 0.41211163997650146\n",
      "Epoch 4633, Loss: 2.386991709470749, Final Batch Loss: 0.3469093143939972\n",
      "Epoch 4634, Loss: 2.174445927143097, Final Batch Loss: 0.377881795167923\n",
      "Epoch 4635, Loss: 2.4735796749591827, Final Batch Loss: 0.5587697625160217\n",
      "Epoch 4636, Loss: 2.2527035772800446, Final Batch Loss: 0.4606352150440216\n",
      "Epoch 4637, Loss: 2.133245289325714, Final Batch Loss: 0.38420143723487854\n",
      "Epoch 4638, Loss: 2.483523339033127, Final Batch Loss: 0.5200215578079224\n",
      "Epoch 4639, Loss: 2.297174781560898, Final Batch Loss: 0.317850261926651\n",
      "Epoch 4640, Loss: 2.385716825723648, Final Batch Loss: 0.5184297561645508\n",
      "Epoch 4641, Loss: 2.3391071259975433, Final Batch Loss: 0.5649330615997314\n",
      "Epoch 4642, Loss: 2.2904407382011414, Final Batch Loss: 0.37898755073547363\n",
      "Epoch 4643, Loss: 2.2693984508514404, Final Batch Loss: 0.4207567274570465\n",
      "Epoch 4644, Loss: 2.379255563020706, Final Batch Loss: 0.4111679196357727\n",
      "Epoch 4645, Loss: 2.3838879466056824, Final Batch Loss: 0.4751196503639221\n",
      "Epoch 4646, Loss: 2.4128074645996094, Final Batch Loss: 0.5261896848678589\n",
      "Epoch 4647, Loss: 2.329493999481201, Final Batch Loss: 0.5123773217201233\n",
      "Epoch 4648, Loss: 2.512027680873871, Final Batch Loss: 0.613467276096344\n",
      "Epoch 4649, Loss: 2.3800970017910004, Final Batch Loss: 0.4467327296733856\n",
      "Epoch 4650, Loss: 2.403360366821289, Final Batch Loss: 0.35026201605796814\n",
      "Epoch 4651, Loss: 2.3730772137641907, Final Batch Loss: 0.42415475845336914\n",
      "Epoch 4652, Loss: 2.3827971816062927, Final Batch Loss: 0.5045004487037659\n",
      "Epoch 4653, Loss: 2.2705828845500946, Final Batch Loss: 0.49365323781967163\n",
      "Epoch 4654, Loss: 2.4688486754894257, Final Batch Loss: 0.47294706106185913\n",
      "Epoch 4655, Loss: 2.395212858915329, Final Batch Loss: 0.6325390934944153\n",
      "Epoch 4656, Loss: 2.313756048679352, Final Batch Loss: 0.42843863368034363\n",
      "Epoch 4657, Loss: 2.2323267459869385, Final Batch Loss: 0.37980011105537415\n",
      "Epoch 4658, Loss: 2.2186346352100372, Final Batch Loss: 0.42263153195381165\n",
      "Epoch 4659, Loss: 2.2217335999011993, Final Batch Loss: 0.3587336838245392\n",
      "Epoch 4660, Loss: 2.351701557636261, Final Batch Loss: 0.3737855851650238\n",
      "Epoch 4661, Loss: 2.2709007263183594, Final Batch Loss: 0.3597947061061859\n",
      "Epoch 4662, Loss: 2.209468722343445, Final Batch Loss: 0.43330299854278564\n",
      "Epoch 4663, Loss: 2.328680545091629, Final Batch Loss: 0.48053446412086487\n",
      "Epoch 4664, Loss: 2.2550495862960815, Final Batch Loss: 0.4595849812030792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4665, Loss: 2.213467240333557, Final Batch Loss: 0.5229021310806274\n",
      "Epoch 4666, Loss: 2.260302871465683, Final Batch Loss: 0.338103324174881\n",
      "Epoch 4667, Loss: 2.342590183019638, Final Batch Loss: 0.48314061760902405\n",
      "Epoch 4668, Loss: 2.3180956840515137, Final Batch Loss: 0.5258289575576782\n",
      "Epoch 4669, Loss: 2.431383192539215, Final Batch Loss: 0.5539456605911255\n",
      "Epoch 4670, Loss: 2.6173094511032104, Final Batch Loss: 0.39259618520736694\n",
      "Epoch 4671, Loss: 2.211905151605606, Final Batch Loss: 0.4133433699607849\n",
      "Epoch 4672, Loss: 2.3885309994220734, Final Batch Loss: 0.43136754631996155\n",
      "Epoch 4673, Loss: 2.2687161564826965, Final Batch Loss: 0.4821147620677948\n",
      "Epoch 4674, Loss: 2.309437394142151, Final Batch Loss: 0.47762641310691833\n",
      "Epoch 4675, Loss: 2.24212583899498, Final Batch Loss: 0.499641478061676\n",
      "Epoch 4676, Loss: 2.368300199508667, Final Batch Loss: 0.41343507170677185\n",
      "Epoch 4677, Loss: 2.2873435616493225, Final Batch Loss: 0.42038917541503906\n",
      "Epoch 4678, Loss: 2.417726993560791, Final Batch Loss: 0.4298672378063202\n",
      "Epoch 4679, Loss: 2.28998139500618, Final Batch Loss: 0.4061618149280548\n",
      "Epoch 4680, Loss: 2.291734457015991, Final Batch Loss: 0.4913117587566376\n",
      "Epoch 4681, Loss: 2.3289269506931305, Final Batch Loss: 0.4713321030139923\n",
      "Epoch 4682, Loss: 2.4772160053253174, Final Batch Loss: 0.5088836550712585\n",
      "Epoch 4683, Loss: 2.391187161207199, Final Batch Loss: 0.4702899158000946\n",
      "Epoch 4684, Loss: 2.456352025270462, Final Batch Loss: 0.5731375217437744\n",
      "Epoch 4685, Loss: 2.3680748343467712, Final Batch Loss: 0.47481569647789\n",
      "Epoch 4686, Loss: 2.1619567275047302, Final Batch Loss: 0.4356010854244232\n",
      "Epoch 4687, Loss: 2.534358024597168, Final Batch Loss: 0.5663641691207886\n",
      "Epoch 4688, Loss: 2.3436449766159058, Final Batch Loss: 0.37524300813674927\n",
      "Epoch 4689, Loss: 2.5786217153072357, Final Batch Loss: 0.5557733178138733\n",
      "Epoch 4690, Loss: 2.31009578704834, Final Batch Loss: 0.41410982608795166\n",
      "Epoch 4691, Loss: 2.3062480688095093, Final Batch Loss: 0.4070635735988617\n",
      "Epoch 4692, Loss: 2.3248941004276276, Final Batch Loss: 0.4012763798236847\n",
      "Epoch 4693, Loss: 2.193818986415863, Final Batch Loss: 0.31300804018974304\n",
      "Epoch 4694, Loss: 2.6406131088733673, Final Batch Loss: 0.5168755650520325\n",
      "Epoch 4695, Loss: 2.3809365928173065, Final Batch Loss: 0.42811957001686096\n",
      "Epoch 4696, Loss: 2.4875791370868683, Final Batch Loss: 0.5538091063499451\n",
      "Epoch 4697, Loss: 2.3306908905506134, Final Batch Loss: 0.3854462206363678\n",
      "Epoch 4698, Loss: 2.3928990960121155, Final Batch Loss: 0.4080330729484558\n",
      "Epoch 4699, Loss: 2.2635570764541626, Final Batch Loss: 0.39903825521469116\n",
      "Epoch 4700, Loss: 2.500912368297577, Final Batch Loss: 0.4978209435939789\n",
      "Epoch 4701, Loss: 2.3277901709079742, Final Batch Loss: 0.562737226486206\n",
      "Epoch 4702, Loss: 2.227210432291031, Final Batch Loss: 0.39493831992149353\n",
      "Epoch 4703, Loss: 2.25307959318161, Final Batch Loss: 0.4551505744457245\n",
      "Epoch 4704, Loss: 2.319958984851837, Final Batch Loss: 0.39366063475608826\n",
      "Epoch 4705, Loss: 2.3753181993961334, Final Batch Loss: 0.4349510073661804\n",
      "Epoch 4706, Loss: 2.344278931617737, Final Batch Loss: 0.5442014932632446\n",
      "Epoch 4707, Loss: 2.2963290214538574, Final Batch Loss: 0.42764273285865784\n",
      "Epoch 4708, Loss: 2.3262148797512054, Final Batch Loss: 0.5507387518882751\n",
      "Epoch 4709, Loss: 2.5685915648937225, Final Batch Loss: 0.6524176001548767\n",
      "Epoch 4710, Loss: 2.3846882581710815, Final Batch Loss: 0.5126567482948303\n",
      "Epoch 4711, Loss: 2.3998402655124664, Final Batch Loss: 0.634324312210083\n",
      "Epoch 4712, Loss: 2.4409117698669434, Final Batch Loss: 0.5045774579048157\n",
      "Epoch 4713, Loss: 2.5000601708889008, Final Batch Loss: 0.49179020524024963\n",
      "Epoch 4714, Loss: 2.2692388594150543, Final Batch Loss: 0.3447713553905487\n",
      "Epoch 4715, Loss: 2.3943366408348083, Final Batch Loss: 0.5131230354309082\n",
      "Epoch 4716, Loss: 2.300542324781418, Final Batch Loss: 0.43597355484962463\n",
      "Epoch 4717, Loss: 2.508150279521942, Final Batch Loss: 0.520993173122406\n",
      "Epoch 4718, Loss: 2.2508175373077393, Final Batch Loss: 0.4574887454509735\n",
      "Epoch 4719, Loss: 2.3486227989196777, Final Batch Loss: 0.4361630380153656\n",
      "Epoch 4720, Loss: 2.34939181804657, Final Batch Loss: 0.4425850212574005\n",
      "Epoch 4721, Loss: 2.1822258830070496, Final Batch Loss: 0.39197665452957153\n",
      "Epoch 4722, Loss: 2.3500256836414337, Final Batch Loss: 0.5147455334663391\n",
      "Epoch 4723, Loss: 2.3659609258174896, Final Batch Loss: 0.4811680316925049\n",
      "Epoch 4724, Loss: 2.249625325202942, Final Batch Loss: 0.3838407099246979\n",
      "Epoch 4725, Loss: 2.2733199298381805, Final Batch Loss: 0.4451350271701813\n",
      "Epoch 4726, Loss: 2.286575436592102, Final Batch Loss: 0.3956657350063324\n",
      "Epoch 4727, Loss: 2.2041229605674744, Final Batch Loss: 0.40673133730888367\n",
      "Epoch 4728, Loss: 2.2093222737312317, Final Batch Loss: 0.404477059841156\n",
      "Epoch 4729, Loss: 2.2406651079654694, Final Batch Loss: 0.4432252049446106\n",
      "Epoch 4730, Loss: 2.1855871975421906, Final Batch Loss: 0.46604761481285095\n",
      "Epoch 4731, Loss: 2.328622132539749, Final Batch Loss: 0.44206953048706055\n",
      "Epoch 4732, Loss: 2.3580568730831146, Final Batch Loss: 0.4822351336479187\n",
      "Epoch 4733, Loss: 2.404877781867981, Final Batch Loss: 0.49222689867019653\n",
      "Epoch 4734, Loss: 2.1661457419395447, Final Batch Loss: 0.4222821593284607\n",
      "Epoch 4735, Loss: 2.258806884288788, Final Batch Loss: 0.45225051045417786\n",
      "Epoch 4736, Loss: 2.3527763187885284, Final Batch Loss: 0.44522804021835327\n",
      "Epoch 4737, Loss: 2.3969529271125793, Final Batch Loss: 0.5065697431564331\n",
      "Epoch 4738, Loss: 2.2315531373023987, Final Batch Loss: 0.382508784532547\n",
      "Epoch 4739, Loss: 2.2861153185367584, Final Batch Loss: 0.3908896744251251\n",
      "Epoch 4740, Loss: 2.337921619415283, Final Batch Loss: 0.5592333078384399\n",
      "Epoch 4741, Loss: 2.2937428057193756, Final Batch Loss: 0.38551101088523865\n",
      "Epoch 4742, Loss: 2.1914132833480835, Final Batch Loss: 0.36368614435195923\n",
      "Epoch 4743, Loss: 2.176068961620331, Final Batch Loss: 0.42219048738479614\n",
      "Epoch 4744, Loss: 2.3038931787014008, Final Batch Loss: 0.5239798426628113\n",
      "Epoch 4745, Loss: 2.5103918612003326, Final Batch Loss: 0.6497724056243896\n",
      "Epoch 4746, Loss: 2.3793854415416718, Final Batch Loss: 0.39149436354637146\n",
      "Epoch 4747, Loss: 2.3304412364959717, Final Batch Loss: 0.42945659160614014\n",
      "Epoch 4748, Loss: 2.216874450445175, Final Batch Loss: 0.4574741721153259\n",
      "Epoch 4749, Loss: 2.340157628059387, Final Batch Loss: 0.40595367550849915\n",
      "Epoch 4750, Loss: 2.4604428112506866, Final Batch Loss: 0.5514844059944153\n",
      "Epoch 4751, Loss: 2.3571557998657227, Final Batch Loss: 0.4677891135215759\n",
      "Epoch 4752, Loss: 2.6695207953453064, Final Batch Loss: 0.3957377076148987\n",
      "Epoch 4753, Loss: 2.194577246904373, Final Batch Loss: 0.36893385648727417\n",
      "Epoch 4754, Loss: 2.338743180036545, Final Batch Loss: 0.4897322654724121\n",
      "Epoch 4755, Loss: 2.385913908481598, Final Batch Loss: 0.3671087920665741\n",
      "Epoch 4756, Loss: 2.3127081990242004, Final Batch Loss: 0.4601826071739197\n",
      "Epoch 4757, Loss: 2.4692189693450928, Final Batch Loss: 0.5175308585166931\n",
      "Epoch 4758, Loss: 2.306698799133301, Final Batch Loss: 0.4621333181858063\n",
      "Epoch 4759, Loss: 2.3371748328208923, Final Batch Loss: 0.358920156955719\n",
      "Epoch 4760, Loss: 2.214432507753372, Final Batch Loss: 0.502187192440033\n",
      "Epoch 4761, Loss: 2.3474575579166412, Final Batch Loss: 0.48406708240509033\n",
      "Epoch 4762, Loss: 2.3556070625782013, Final Batch Loss: 0.4341970980167389\n",
      "Epoch 4763, Loss: 2.2217674553394318, Final Batch Loss: 0.3973710536956787\n",
      "Epoch 4764, Loss: 2.407819479703903, Final Batch Loss: 0.5175702571868896\n",
      "Epoch 4765, Loss: 2.433096319437027, Final Batch Loss: 0.44248709082603455\n",
      "Epoch 4766, Loss: 2.206310600042343, Final Batch Loss: 0.37521785497665405\n",
      "Epoch 4767, Loss: 2.340829849243164, Final Batch Loss: 0.43994268774986267\n",
      "Epoch 4768, Loss: 2.316060781478882, Final Batch Loss: 0.4501919448375702\n",
      "Epoch 4769, Loss: 2.304776817560196, Final Batch Loss: 0.454959899187088\n",
      "Epoch 4770, Loss: 2.494300127029419, Final Batch Loss: 0.5576250553131104\n",
      "Epoch 4771, Loss: 2.3086028397083282, Final Batch Loss: 0.34311148524284363\n",
      "Epoch 4772, Loss: 2.4307741224765778, Final Batch Loss: 0.43836161494255066\n",
      "Epoch 4773, Loss: 2.3421363830566406, Final Batch Loss: 0.49441540241241455\n",
      "Epoch 4774, Loss: 2.272747904062271, Final Batch Loss: 0.5523562431335449\n",
      "Epoch 4775, Loss: 2.3986741304397583, Final Batch Loss: 0.4977194368839264\n",
      "Epoch 4776, Loss: 2.2860327064990997, Final Batch Loss: 0.4478035867214203\n",
      "Epoch 4777, Loss: 2.271948516368866, Final Batch Loss: 0.4121593236923218\n",
      "Epoch 4778, Loss: 2.3292742669582367, Final Batch Loss: 0.4601045250892639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4779, Loss: 2.603848546743393, Final Batch Loss: 0.4679848551750183\n",
      "Epoch 4780, Loss: 2.299544006586075, Final Batch Loss: 0.5023078918457031\n",
      "Epoch 4781, Loss: 2.464641958475113, Final Batch Loss: 0.5245978832244873\n",
      "Epoch 4782, Loss: 2.148727208375931, Final Batch Loss: 0.37774768471717834\n",
      "Epoch 4783, Loss: 2.3387354016304016, Final Batch Loss: 0.5680729746818542\n",
      "Epoch 4784, Loss: 2.4390821754932404, Final Batch Loss: 0.6236097812652588\n",
      "Epoch 4785, Loss: 2.3410279154777527, Final Batch Loss: 0.5380823016166687\n",
      "Epoch 4786, Loss: 2.1677286028862, Final Batch Loss: 0.3759858012199402\n",
      "Epoch 4787, Loss: 2.3262006640434265, Final Batch Loss: 0.41826415061950684\n",
      "Epoch 4788, Loss: 2.2363391518592834, Final Batch Loss: 0.4305074214935303\n",
      "Epoch 4789, Loss: 2.3426513373851776, Final Batch Loss: 0.5209507346153259\n",
      "Epoch 4790, Loss: 2.358883857727051, Final Batch Loss: 0.4094976782798767\n",
      "Epoch 4791, Loss: 2.259942799806595, Final Batch Loss: 0.4207371175289154\n",
      "Epoch 4792, Loss: 2.3262257277965546, Final Batch Loss: 0.4619758427143097\n",
      "Epoch 4793, Loss: 2.412204623222351, Final Batch Loss: 0.45245394110679626\n",
      "Epoch 4794, Loss: 2.302755743265152, Final Batch Loss: 0.5079664587974548\n",
      "Epoch 4795, Loss: 2.152472674846649, Final Batch Loss: 0.3895772695541382\n",
      "Epoch 4796, Loss: 2.295334428548813, Final Batch Loss: 0.5868033766746521\n",
      "Epoch 4797, Loss: 2.1657803058624268, Final Batch Loss: 0.38401880860328674\n",
      "Epoch 4798, Loss: 2.348458081483841, Final Batch Loss: 0.4729193449020386\n",
      "Epoch 4799, Loss: 2.2375850081443787, Final Batch Loss: 0.3706158995628357\n",
      "Epoch 4800, Loss: 2.251492738723755, Final Batch Loss: 0.4375176429748535\n",
      "Epoch 4801, Loss: 2.196737676858902, Final Batch Loss: 0.3883224129676819\n",
      "Epoch 4802, Loss: 2.2079827189445496, Final Batch Loss: 0.5397247076034546\n",
      "Epoch 4803, Loss: 2.2954157888889313, Final Batch Loss: 0.5565704107284546\n",
      "Epoch 4804, Loss: 2.2205834686756134, Final Batch Loss: 0.4738917946815491\n",
      "Epoch 4805, Loss: 2.3304761052131653, Final Batch Loss: 0.5489466786384583\n",
      "Epoch 4806, Loss: 2.332536965608597, Final Batch Loss: 0.407624214887619\n",
      "Epoch 4807, Loss: 2.2540519535541534, Final Batch Loss: 0.4742281138896942\n",
      "Epoch 4808, Loss: 2.3576498329639435, Final Batch Loss: 0.5133009552955627\n",
      "Epoch 4809, Loss: 2.3169083297252655, Final Batch Loss: 0.4083586037158966\n",
      "Epoch 4810, Loss: 2.317151755094528, Final Batch Loss: 0.49932917952537537\n",
      "Epoch 4811, Loss: 2.1784506142139435, Final Batch Loss: 0.38334307074546814\n",
      "Epoch 4812, Loss: 2.278985768556595, Final Batch Loss: 0.47602155804634094\n",
      "Epoch 4813, Loss: 2.362039476633072, Final Batch Loss: 0.4208909273147583\n",
      "Epoch 4814, Loss: 2.342377007007599, Final Batch Loss: 0.4564923644065857\n",
      "Epoch 4815, Loss: 2.31500181555748, Final Batch Loss: 0.5161752700805664\n",
      "Epoch 4816, Loss: 2.2213715612888336, Final Batch Loss: 0.3665301501750946\n",
      "Epoch 4817, Loss: 2.297220528125763, Final Batch Loss: 0.39233148097991943\n",
      "Epoch 4818, Loss: 2.3382344245910645, Final Batch Loss: 0.5353268384933472\n",
      "Epoch 4819, Loss: 2.4594384133815765, Final Batch Loss: 0.5093297958374023\n",
      "Epoch 4820, Loss: 2.2545407712459564, Final Batch Loss: 0.4294723570346832\n",
      "Epoch 4821, Loss: 2.4445117115974426, Final Batch Loss: 0.5591515898704529\n",
      "Epoch 4822, Loss: 2.1373765766620636, Final Batch Loss: 0.3880136013031006\n",
      "Epoch 4823, Loss: 2.4022378027439117, Final Batch Loss: 0.41375207901000977\n",
      "Epoch 4824, Loss: 2.330107659101486, Final Batch Loss: 0.4858909249305725\n",
      "Epoch 4825, Loss: 2.2236053943634033, Final Batch Loss: 0.4536033868789673\n",
      "Epoch 4826, Loss: 2.3783354461193085, Final Batch Loss: 0.526504635810852\n",
      "Epoch 4827, Loss: 2.277233123779297, Final Batch Loss: 0.4236941933631897\n",
      "Epoch 4828, Loss: 2.1921899914741516, Final Batch Loss: 0.382808119058609\n",
      "Epoch 4829, Loss: 2.47222101688385, Final Batch Loss: 0.43056008219718933\n",
      "Epoch 4830, Loss: 2.2802996933460236, Final Batch Loss: 0.41094040870666504\n",
      "Epoch 4831, Loss: 2.2953387796878815, Final Batch Loss: 0.49166402220726013\n",
      "Epoch 4832, Loss: 2.3157063722610474, Final Batch Loss: 0.4223524034023285\n",
      "Epoch 4833, Loss: 2.1916695535182953, Final Batch Loss: 0.4188973307609558\n",
      "Epoch 4834, Loss: 2.17621111869812, Final Batch Loss: 0.4261613190174103\n",
      "Epoch 4835, Loss: 2.168279677629471, Final Batch Loss: 0.4216570258140564\n",
      "Epoch 4836, Loss: 2.2536588311195374, Final Batch Loss: 0.5008832812309265\n",
      "Epoch 4837, Loss: 2.3671226501464844, Final Batch Loss: 0.3647979497909546\n",
      "Epoch 4838, Loss: 2.28537854552269, Final Batch Loss: 0.5605358481407166\n",
      "Epoch 4839, Loss: 2.2884870767593384, Final Batch Loss: 0.4924900531768799\n",
      "Epoch 4840, Loss: 2.1743006706237793, Final Batch Loss: 0.38983139395713806\n",
      "Epoch 4841, Loss: 2.138210743665695, Final Batch Loss: 0.39897453784942627\n",
      "Epoch 4842, Loss: 2.235703408718109, Final Batch Loss: 0.4525182247161865\n",
      "Epoch 4843, Loss: 2.499914586544037, Final Batch Loss: 0.45769959688186646\n",
      "Epoch 4844, Loss: 2.298373281955719, Final Batch Loss: 0.5006242990493774\n",
      "Epoch 4845, Loss: 2.315517097711563, Final Batch Loss: 0.4812919795513153\n",
      "Epoch 4846, Loss: 2.3256235122680664, Final Batch Loss: 0.4234284460544586\n",
      "Epoch 4847, Loss: 2.183882027864456, Final Batch Loss: 0.3576280176639557\n",
      "Epoch 4848, Loss: 2.289660632610321, Final Batch Loss: 0.49292734265327454\n",
      "Epoch 4849, Loss: 2.1011018455028534, Final Batch Loss: 0.3060573637485504\n",
      "Epoch 4850, Loss: 2.203788548707962, Final Batch Loss: 0.39817461371421814\n",
      "Epoch 4851, Loss: 2.5489503741264343, Final Batch Loss: 0.44921380281448364\n",
      "Epoch 4852, Loss: 2.1776760518550873, Final Batch Loss: 0.4812734127044678\n",
      "Epoch 4853, Loss: 2.3131598830223083, Final Batch Loss: 0.4265255928039551\n",
      "Epoch 4854, Loss: 2.3160261809825897, Final Batch Loss: 0.4479564130306244\n",
      "Epoch 4855, Loss: 2.2891646921634674, Final Batch Loss: 0.42309290170669556\n",
      "Epoch 4856, Loss: 2.190750926733017, Final Batch Loss: 0.5209330320358276\n",
      "Epoch 4857, Loss: 2.292722314596176, Final Batch Loss: 0.49435219168663025\n",
      "Epoch 4858, Loss: 2.332890599966049, Final Batch Loss: 0.4789779782295227\n",
      "Epoch 4859, Loss: 2.383922904729843, Final Batch Loss: 0.5705603361129761\n",
      "Epoch 4860, Loss: 2.250351905822754, Final Batch Loss: 0.41613101959228516\n",
      "Epoch 4861, Loss: 2.3663462102413177, Final Batch Loss: 0.41044557094573975\n",
      "Epoch 4862, Loss: 2.209842026233673, Final Batch Loss: 0.42855915427207947\n",
      "Epoch 4863, Loss: 2.3167680501937866, Final Batch Loss: 0.4243307113647461\n",
      "Epoch 4864, Loss: 2.361136347055435, Final Batch Loss: 0.4335942268371582\n",
      "Epoch 4865, Loss: 2.255849778652191, Final Batch Loss: 0.3923908770084381\n",
      "Epoch 4866, Loss: 2.228944092988968, Final Batch Loss: 0.37287577986717224\n",
      "Epoch 4867, Loss: 2.1793141663074493, Final Batch Loss: 0.458050012588501\n",
      "Epoch 4868, Loss: 2.2696346044540405, Final Batch Loss: 0.48202234506607056\n",
      "Epoch 4869, Loss: 2.3886037468910217, Final Batch Loss: 0.5001343488693237\n",
      "Epoch 4870, Loss: 2.2197884917259216, Final Batch Loss: 0.3350231647491455\n",
      "Epoch 4871, Loss: 2.303147614002228, Final Batch Loss: 0.458507776260376\n",
      "Epoch 4872, Loss: 2.38052698969841, Final Batch Loss: 0.560757577419281\n",
      "Epoch 4873, Loss: 2.4839133620262146, Final Batch Loss: 0.4817678928375244\n",
      "Epoch 4874, Loss: 2.3142181038856506, Final Batch Loss: 0.5322484374046326\n",
      "Epoch 4875, Loss: 2.3554199039936066, Final Batch Loss: 0.49521100521087646\n",
      "Epoch 4876, Loss: 2.2545211017131805, Final Batch Loss: 0.38851696252822876\n",
      "Epoch 4877, Loss: 2.416425406932831, Final Batch Loss: 0.4853440523147583\n",
      "Epoch 4878, Loss: 2.3522335290908813, Final Batch Loss: 0.5084567070007324\n",
      "Epoch 4879, Loss: 2.3222863972187042, Final Batch Loss: 0.5383888483047485\n",
      "Epoch 4880, Loss: 2.2609677016735077, Final Batch Loss: 0.3817383050918579\n",
      "Epoch 4881, Loss: 2.2846860587596893, Final Batch Loss: 0.4931688606739044\n",
      "Epoch 4882, Loss: 2.287717491388321, Final Batch Loss: 0.4926730990409851\n",
      "Epoch 4883, Loss: 2.4514714181423187, Final Batch Loss: 0.43325793743133545\n",
      "Epoch 4884, Loss: 2.2066661417484283, Final Batch Loss: 0.44795021414756775\n",
      "Epoch 4885, Loss: 2.3606196641921997, Final Batch Loss: 0.4877782464027405\n",
      "Epoch 4886, Loss: 2.3737820088863373, Final Batch Loss: 0.626835286617279\n",
      "Epoch 4887, Loss: 2.307118237018585, Final Batch Loss: 0.4774009883403778\n",
      "Epoch 4888, Loss: 2.2808104753494263, Final Batch Loss: 0.46157732605934143\n",
      "Epoch 4889, Loss: 2.362043708562851, Final Batch Loss: 0.4892660081386566\n",
      "Epoch 4890, Loss: 2.321214199066162, Final Batch Loss: 0.49108317494392395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4891, Loss: 2.2388941645622253, Final Batch Loss: 0.46360573172569275\n",
      "Epoch 4892, Loss: 2.1241341829299927, Final Batch Loss: 0.4240874648094177\n",
      "Epoch 4893, Loss: 2.347633421421051, Final Batch Loss: 0.5509334206581116\n",
      "Epoch 4894, Loss: 2.290385752916336, Final Batch Loss: 0.42553797364234924\n",
      "Epoch 4895, Loss: 2.329567402601242, Final Batch Loss: 0.5181394815444946\n",
      "Epoch 4896, Loss: 2.3819008469581604, Final Batch Loss: 0.5158463716506958\n",
      "Epoch 4897, Loss: 2.2817800641059875, Final Batch Loss: 0.60768723487854\n",
      "Epoch 4898, Loss: 2.2612231075763702, Final Batch Loss: 0.5525745749473572\n",
      "Epoch 4899, Loss: 2.3436092138290405, Final Batch Loss: 0.49248573184013367\n",
      "Epoch 4900, Loss: 2.1972638964653015, Final Batch Loss: 0.3456678092479706\n",
      "Epoch 4901, Loss: 2.3011880815029144, Final Batch Loss: 0.42733827233314514\n",
      "Epoch 4902, Loss: 2.2064786553382874, Final Batch Loss: 0.4125523269176483\n",
      "Epoch 4903, Loss: 2.4059090316295624, Final Batch Loss: 0.47100192308425903\n",
      "Epoch 4904, Loss: 2.319863259792328, Final Batch Loss: 0.4972253143787384\n",
      "Epoch 4905, Loss: 2.2837319374084473, Final Batch Loss: 0.3673876225948334\n",
      "Epoch 4906, Loss: 2.3629337549209595, Final Batch Loss: 0.4492899477481842\n",
      "Epoch 4907, Loss: 2.294096350669861, Final Batch Loss: 0.4865385890007019\n",
      "Epoch 4908, Loss: 2.497019499540329, Final Batch Loss: 0.5514902472496033\n",
      "Epoch 4909, Loss: 2.4203687012195587, Final Batch Loss: 0.5088368654251099\n",
      "Epoch 4910, Loss: 2.2824485301971436, Final Batch Loss: 0.3813878893852234\n",
      "Epoch 4911, Loss: 2.3488084971904755, Final Batch Loss: 0.5577665567398071\n",
      "Epoch 4912, Loss: 2.2690036594867706, Final Batch Loss: 0.48543623089790344\n",
      "Epoch 4913, Loss: 2.2761422395706177, Final Batch Loss: 0.3808889389038086\n",
      "Epoch 4914, Loss: 2.369897246360779, Final Batch Loss: 0.5666957497596741\n",
      "Epoch 4915, Loss: 2.1818366050720215, Final Batch Loss: 0.44588613510131836\n",
      "Epoch 4916, Loss: 2.231424927711487, Final Batch Loss: 0.5102179646492004\n",
      "Epoch 4917, Loss: 2.1357935965061188, Final Batch Loss: 0.365512877702713\n",
      "Epoch 4918, Loss: 2.3543857038021088, Final Batch Loss: 0.4209955632686615\n",
      "Epoch 4919, Loss: 2.2114039063453674, Final Batch Loss: 0.3818996846675873\n",
      "Epoch 4920, Loss: 2.1946220099925995, Final Batch Loss: 0.4318614900112152\n",
      "Epoch 4921, Loss: 2.3843480348587036, Final Batch Loss: 0.5242114067077637\n",
      "Epoch 4922, Loss: 2.261594295501709, Final Batch Loss: 0.4072116017341614\n",
      "Epoch 4923, Loss: 2.3316161930561066, Final Batch Loss: 0.3971169590950012\n",
      "Epoch 4924, Loss: 2.339194357395172, Final Batch Loss: 0.5058319568634033\n",
      "Epoch 4925, Loss: 2.4073864221572876, Final Batch Loss: 0.5368468165397644\n",
      "Epoch 4926, Loss: 2.550937533378601, Final Batch Loss: 0.4575936794281006\n",
      "Epoch 4927, Loss: 2.116330921649933, Final Batch Loss: 0.3449970781803131\n",
      "Epoch 4928, Loss: 2.251063734292984, Final Batch Loss: 0.3028370141983032\n",
      "Epoch 4929, Loss: 2.4648272693157196, Final Batch Loss: 0.46956759691238403\n",
      "Epoch 4930, Loss: 2.2409621477127075, Final Batch Loss: 0.44192931056022644\n",
      "Epoch 4931, Loss: 2.1566449105739594, Final Batch Loss: 0.4063607454299927\n",
      "Epoch 4932, Loss: 2.163696438074112, Final Batch Loss: 0.41011613607406616\n",
      "Epoch 4933, Loss: 2.397122174501419, Final Batch Loss: 0.4380505084991455\n",
      "Epoch 4934, Loss: 2.3086129426956177, Final Batch Loss: 0.39635878801345825\n",
      "Epoch 4935, Loss: 2.4086905419826508, Final Batch Loss: 0.6148892641067505\n",
      "Epoch 4936, Loss: 2.185824304819107, Final Batch Loss: 0.45009511709213257\n",
      "Epoch 4937, Loss: 2.3232873678207397, Final Batch Loss: 0.46836453676223755\n",
      "Epoch 4938, Loss: 2.3146869838237762, Final Batch Loss: 0.4886711835861206\n",
      "Epoch 4939, Loss: 2.1874175369739532, Final Batch Loss: 0.407432496547699\n",
      "Epoch 4940, Loss: 2.364889442920685, Final Batch Loss: 0.4483044743537903\n",
      "Epoch 4941, Loss: 2.266266942024231, Final Batch Loss: 0.3763715326786041\n",
      "Epoch 4942, Loss: 2.421354651451111, Final Batch Loss: 0.4054611325263977\n",
      "Epoch 4943, Loss: 2.144333302974701, Final Batch Loss: 0.4169033169746399\n",
      "Epoch 4944, Loss: 2.3140114843845367, Final Batch Loss: 0.42697015404701233\n",
      "Epoch 4945, Loss: 2.3217815160751343, Final Batch Loss: 0.4408051371574402\n",
      "Epoch 4946, Loss: 2.2711322605609894, Final Batch Loss: 0.4453440308570862\n",
      "Epoch 4947, Loss: 2.2792611122131348, Final Batch Loss: 0.4015115797519684\n",
      "Epoch 4948, Loss: 2.3998966813087463, Final Batch Loss: 0.4957830011844635\n",
      "Epoch 4949, Loss: 2.454908400774002, Final Batch Loss: 0.42937517166137695\n",
      "Epoch 4950, Loss: 2.310617595911026, Final Batch Loss: 0.39905187487602234\n",
      "Epoch 4951, Loss: 2.307270437479019, Final Batch Loss: 0.42099729180336\n",
      "Epoch 4952, Loss: 2.3096963465213776, Final Batch Loss: 0.434395968914032\n",
      "Epoch 4953, Loss: 2.3639440834522247, Final Batch Loss: 0.4050358235836029\n",
      "Epoch 4954, Loss: 2.2787713706493378, Final Batch Loss: 0.503231942653656\n",
      "Epoch 4955, Loss: 2.076610952615738, Final Batch Loss: 0.39596256613731384\n",
      "Epoch 4956, Loss: 2.33427432179451, Final Batch Loss: 0.4940720796585083\n",
      "Epoch 4957, Loss: 2.3012382984161377, Final Batch Loss: 0.5661125183105469\n",
      "Epoch 4958, Loss: 2.3017143607139587, Final Batch Loss: 0.5970919132232666\n",
      "Epoch 4959, Loss: 2.1431561708450317, Final Batch Loss: 0.3565577268600464\n",
      "Epoch 4960, Loss: 2.280194580554962, Final Batch Loss: 0.51157146692276\n",
      "Epoch 4961, Loss: 2.4174212217330933, Final Batch Loss: 0.6293249726295471\n",
      "Epoch 4962, Loss: 2.1107948422431946, Final Batch Loss: 0.45240914821624756\n",
      "Epoch 4963, Loss: 2.2409651279449463, Final Batch Loss: 0.5054013729095459\n",
      "Epoch 4964, Loss: 2.161277115345001, Final Batch Loss: 0.37934887409210205\n",
      "Epoch 4965, Loss: 2.3326894342899323, Final Batch Loss: 0.358793169260025\n",
      "Epoch 4966, Loss: 2.349815160036087, Final Batch Loss: 0.44573506712913513\n",
      "Epoch 4967, Loss: 2.2467965185642242, Final Batch Loss: 0.5114477276802063\n",
      "Epoch 4968, Loss: 2.2215378284454346, Final Batch Loss: 0.5008116960525513\n",
      "Epoch 4969, Loss: 2.2926089465618134, Final Batch Loss: 0.439676970243454\n",
      "Epoch 4970, Loss: 2.1554273664951324, Final Batch Loss: 0.458549439907074\n",
      "Epoch 4971, Loss: 2.356936454772949, Final Batch Loss: 0.46576523780822754\n",
      "Epoch 4972, Loss: 2.4235534071922302, Final Batch Loss: 0.5530105233192444\n",
      "Epoch 4973, Loss: 2.365003138780594, Final Batch Loss: 0.42096856236457825\n",
      "Epoch 4974, Loss: 2.4423513412475586, Final Batch Loss: 0.5081802010536194\n",
      "Epoch 4975, Loss: 2.4802054464817047, Final Batch Loss: 0.411874383687973\n",
      "Epoch 4976, Loss: 2.2394349575042725, Final Batch Loss: 0.4062771797180176\n",
      "Epoch 4977, Loss: 2.2622502148151398, Final Batch Loss: 0.3743635416030884\n",
      "Epoch 4978, Loss: 2.594479024410248, Final Batch Loss: 0.5859004855155945\n",
      "Epoch 4979, Loss: 2.3505370020866394, Final Batch Loss: 0.4360203444957733\n",
      "Epoch 4980, Loss: 2.405641585588455, Final Batch Loss: 0.5715347528457642\n",
      "Epoch 4981, Loss: 2.205030679702759, Final Batch Loss: 0.3855384886264801\n",
      "Epoch 4982, Loss: 2.3868414163589478, Final Batch Loss: 0.500630795955658\n",
      "Epoch 4983, Loss: 2.199921101331711, Final Batch Loss: 0.45607393980026245\n",
      "Epoch 4984, Loss: 2.2620391249656677, Final Batch Loss: 0.3974318206310272\n",
      "Epoch 4985, Loss: 2.4994024634361267, Final Batch Loss: 0.580964207649231\n",
      "Epoch 4986, Loss: 2.3697054386138916, Final Batch Loss: 0.38504090905189514\n",
      "Epoch 4987, Loss: 2.25262251496315, Final Batch Loss: 0.38048675656318665\n",
      "Epoch 4988, Loss: 2.273612231016159, Final Batch Loss: 0.4952540993690491\n",
      "Epoch 4989, Loss: 2.2688426077365875, Final Batch Loss: 0.4844137728214264\n",
      "Epoch 4990, Loss: 2.2782059609889984, Final Batch Loss: 0.5328692197799683\n",
      "Epoch 4991, Loss: 2.2633038759231567, Final Batch Loss: 0.35470569133758545\n",
      "Epoch 4992, Loss: 2.2888626754283905, Final Batch Loss: 0.4067144989967346\n",
      "Epoch 4993, Loss: 2.39559268951416, Final Batch Loss: 0.5758360028266907\n",
      "Epoch 4994, Loss: 2.4078913927078247, Final Batch Loss: 0.4860057532787323\n",
      "Epoch 4995, Loss: 2.375706762075424, Final Batch Loss: 0.3646559715270996\n",
      "Epoch 4996, Loss: 2.3656907081604004, Final Batch Loss: 0.5203478336334229\n",
      "Epoch 4997, Loss: 2.2762669920921326, Final Batch Loss: 0.38641229271888733\n",
      "Epoch 4998, Loss: 2.4541670382022858, Final Batch Loss: 0.5426250696182251\n",
      "Epoch 4999, Loss: 2.3388733863830566, Final Batch Loss: 0.46260955929756165\n",
      "Epoch 5000, Loss: 2.368994176387787, Final Batch Loss: 0.43178629875183105\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model_subject(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29  0  0  0  1  0  0  0  8]\n",
      " [ 1 21  1  0  5  4  2  0  0]\n",
      " [ 0  0 28  3  0  3  0  0  0]\n",
      " [ 0  0  1 32  0  0  0  0  0]\n",
      " [ 0  0  1  1 20  2  1  0  3]\n",
      " [ 0  0  0  0  0 31  0  0  1]\n",
      " [ 0  0  0  5  0  0 24  0  0]\n",
      " [ 0  0  0  1  0  0  0 30  0]\n",
      " [ 1  0  1  0  3  0  0  0 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.93548   0.76316   0.84058        38\n",
      "           1    1.00000   0.61765   0.76364        34\n",
      "           2    0.87500   0.82353   0.84848        34\n",
      "           3    0.76190   0.96970   0.85333        33\n",
      "           4    0.68966   0.71429   0.70175        28\n",
      "           5    0.77500   0.96875   0.86111        32\n",
      "           6    0.88889   0.82759   0.85714        29\n",
      "           7    1.00000   0.96774   0.98361        31\n",
      "           8    0.65714   0.82143   0.73016        28\n",
      "\n",
      "    accuracy                        0.82927       287\n",
      "   macro avg    0.84256   0.83042   0.82665       287\n",
      "weighted avg    0.84923   0.82927   0.82896       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model_subject.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model_subject(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_labels = [0] * n_samples + [1] * n_samples + [2] * n_samples + [3] * n_samples + [4] * n_samples + [5] * n_samples + [6] * n_samples + [7] * n_samples + [8] * n_samples + [0] * n_samples + [1] * n_samples + [2] * n_samples + [3] * n_samples + [4] * n_samples + [5] * n_samples + [6] * n_samples  + [7] * n_samples + [8] * n_samples + [0] * n_samples + [1] * n_samples + [2] * n_samples + [3] * n_samples + [4] * n_samples + [5] * n_samples + [6] * n_samples + [7] * n_samples + [8] * n_samples\n",
    "fake_labels = np.asarray(fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  1  0  3  5  1  0  1]\n",
      " [ 6 11  0  4  1  5  0  1  2]\n",
      " [ 0  0 21  4  2  3  0  0  0]\n",
      " [ 0  0  3 14  4  0  5  2  2]\n",
      " [ 2  0  3  9  5  1  1  5  4]\n",
      " [ 0  0  0  0  1 23  0  0  6]\n",
      " [ 0  0  4  0  0  2 21  3  0]\n",
      " [ 2  6  1  4  1  0  1 15  0]\n",
      " [ 1  0  0  0  3  0  0  0 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.63333   0.63333   0.63333        30\n",
      "           1    0.64706   0.36667   0.46809        30\n",
      "           2    0.63636   0.70000   0.66667        30\n",
      "           3    0.40000   0.46667   0.43077        30\n",
      "           4    0.25000   0.16667   0.20000        30\n",
      "           5    0.58974   0.76667   0.66667        30\n",
      "           6    0.72414   0.70000   0.71186        30\n",
      "           7    0.57692   0.50000   0.53571        30\n",
      "           8    0.63415   0.86667   0.73239        30\n",
      "\n",
      "    accuracy                        0.57407       270\n",
      "   macro avg    0.56575   0.57407   0.56061       270\n",
      "weighted avg    0.56575   0.57407   0.56061       270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model_subject(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix(fake_labels, preds.cpu()))\n",
    "print(metrics.classification_report(fake_labels, preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
