{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [14, 15, 17]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.440162181854248, Final Batch Loss: 2.2082464694976807\n",
      "Epoch 2, Loss: 4.431634902954102, Final Batch Loss: 2.1955196857452393\n",
      "Epoch 3, Loss: 4.43436861038208, Final Batch Loss: 2.2138521671295166\n",
      "Epoch 4, Loss: 4.429298400878906, Final Batch Loss: 2.202758312225342\n",
      "Epoch 5, Loss: 4.432542324066162, Final Batch Loss: 2.2178549766540527\n",
      "Epoch 6, Loss: 4.4314258098602295, Final Batch Loss: 2.2220101356506348\n",
      "Epoch 7, Loss: 4.422696590423584, Final Batch Loss: 2.209352493286133\n",
      "Epoch 8, Loss: 4.416726350784302, Final Batch Loss: 2.2054061889648438\n",
      "Epoch 9, Loss: 4.411235570907593, Final Batch Loss: 2.2046492099761963\n",
      "Epoch 10, Loss: 4.4143922328948975, Final Batch Loss: 2.2101757526397705\n",
      "Epoch 11, Loss: 4.403315305709839, Final Batch Loss: 2.195671796798706\n",
      "Epoch 12, Loss: 4.398383378982544, Final Batch Loss: 2.190040349960327\n",
      "Epoch 13, Loss: 4.407072067260742, Final Batch Loss: 2.2158682346343994\n",
      "Epoch 14, Loss: 4.391805171966553, Final Batch Loss: 2.197657823562622\n",
      "Epoch 15, Loss: 4.381596803665161, Final Batch Loss: 2.188720941543579\n",
      "Epoch 16, Loss: 4.379194974899292, Final Batch Loss: 2.1862564086914062\n",
      "Epoch 17, Loss: 4.37225079536438, Final Batch Loss: 2.1822762489318848\n",
      "Epoch 18, Loss: 4.364810466766357, Final Batch Loss: 2.181429862976074\n",
      "Epoch 19, Loss: 4.355276823043823, Final Batch Loss: 2.176802396774292\n",
      "Epoch 20, Loss: 4.332030773162842, Final Batch Loss: 2.154475450515747\n",
      "Epoch 21, Loss: 4.327169418334961, Final Batch Loss: 2.162242889404297\n",
      "Epoch 22, Loss: 4.320433616638184, Final Batch Loss: 2.1618854999542236\n",
      "Epoch 23, Loss: 4.29994797706604, Final Batch Loss: 2.14097261428833\n",
      "Epoch 24, Loss: 4.28040623664856, Final Batch Loss: 2.141042470932007\n",
      "Epoch 25, Loss: 4.260002851486206, Final Batch Loss: 2.120527982711792\n",
      "Epoch 26, Loss: 4.247527599334717, Final Batch Loss: 2.12654972076416\n",
      "Epoch 27, Loss: 4.20644211769104, Final Batch Loss: 2.088545322418213\n",
      "Epoch 28, Loss: 4.18609881401062, Final Batch Loss: 2.0892255306243896\n",
      "Epoch 29, Loss: 4.161109209060669, Final Batch Loss: 2.0815024375915527\n",
      "Epoch 30, Loss: 4.132458448410034, Final Batch Loss: 2.0623292922973633\n",
      "Epoch 31, Loss: 4.0837085247039795, Final Batch Loss: 2.0186166763305664\n",
      "Epoch 32, Loss: 4.070190191268921, Final Batch Loss: 2.032747745513916\n",
      "Epoch 33, Loss: 4.007169723510742, Final Batch Loss: 1.9993696212768555\n",
      "Epoch 34, Loss: 3.986664891242981, Final Batch Loss: 2.0047013759613037\n",
      "Epoch 35, Loss: 3.9582871198654175, Final Batch Loss: 1.9978594779968262\n",
      "Epoch 36, Loss: 3.9075485467910767, Final Batch Loss: 1.9598357677459717\n",
      "Epoch 37, Loss: 3.811636805534363, Final Batch Loss: 1.8773434162139893\n",
      "Epoch 38, Loss: 3.7930946350097656, Final Batch Loss: 1.907684326171875\n",
      "Epoch 39, Loss: 3.710580825805664, Final Batch Loss: 1.852731466293335\n",
      "Epoch 40, Loss: 3.663049340248108, Final Batch Loss: 1.811357021331787\n",
      "Epoch 41, Loss: 3.6064854860305786, Final Batch Loss: 1.7716035842895508\n",
      "Epoch 42, Loss: 3.600795269012451, Final Batch Loss: 1.772703766822815\n",
      "Epoch 43, Loss: 3.572065234184265, Final Batch Loss: 1.8158751726150513\n",
      "Epoch 44, Loss: 3.5498842000961304, Final Batch Loss: 1.7935328483581543\n",
      "Epoch 45, Loss: 3.515352487564087, Final Batch Loss: 1.7544670104980469\n",
      "Epoch 46, Loss: 3.4311416149139404, Final Batch Loss: 1.6918927431106567\n",
      "Epoch 47, Loss: 3.4461069107055664, Final Batch Loss: 1.7205525636672974\n",
      "Epoch 48, Loss: 3.3618990182876587, Final Batch Loss: 1.6443125009536743\n",
      "Epoch 49, Loss: 3.3697094917297363, Final Batch Loss: 1.6473138332366943\n",
      "Epoch 50, Loss: 3.317744016647339, Final Batch Loss: 1.620102882385254\n",
      "Epoch 51, Loss: 3.327249526977539, Final Batch Loss: 1.667331337928772\n",
      "Epoch 52, Loss: 3.298160672187805, Final Batch Loss: 1.650321364402771\n",
      "Epoch 53, Loss: 3.2839505672454834, Final Batch Loss: 1.6161508560180664\n",
      "Epoch 54, Loss: 3.2422173023223877, Final Batch Loss: 1.603508710861206\n",
      "Epoch 55, Loss: 3.25735867023468, Final Batch Loss: 1.6557787656784058\n",
      "Epoch 56, Loss: 3.1929128170013428, Final Batch Loss: 1.6034537553787231\n",
      "Epoch 57, Loss: 3.176586627960205, Final Batch Loss: 1.5694479942321777\n",
      "Epoch 58, Loss: 3.168908953666687, Final Batch Loss: 1.5574012994766235\n",
      "Epoch 59, Loss: 3.1667739152908325, Final Batch Loss: 1.5629398822784424\n",
      "Epoch 60, Loss: 3.1896597146987915, Final Batch Loss: 1.5776461362838745\n",
      "Epoch 61, Loss: 3.181397318840027, Final Batch Loss: 1.6026959419250488\n",
      "Epoch 62, Loss: 3.1564592123031616, Final Batch Loss: 1.5330135822296143\n",
      "Epoch 63, Loss: 3.1657111644744873, Final Batch Loss: 1.603179931640625\n",
      "Epoch 64, Loss: 3.0874247550964355, Final Batch Loss: 1.4962279796600342\n",
      "Epoch 65, Loss: 3.1079267263412476, Final Batch Loss: 1.5481692552566528\n",
      "Epoch 66, Loss: 3.0354291200637817, Final Batch Loss: 1.509702444076538\n",
      "Epoch 67, Loss: 3.125843048095703, Final Batch Loss: 1.569465160369873\n",
      "Epoch 68, Loss: 2.9919263124465942, Final Batch Loss: 1.456865906715393\n",
      "Epoch 69, Loss: 3.000641345977783, Final Batch Loss: 1.4911179542541504\n",
      "Epoch 70, Loss: 3.020879626274109, Final Batch Loss: 1.5041407346725464\n",
      "Epoch 71, Loss: 2.9938313961029053, Final Batch Loss: 1.5309884548187256\n",
      "Epoch 72, Loss: 2.9118359088897705, Final Batch Loss: 1.430808663368225\n",
      "Epoch 73, Loss: 2.9194893836975098, Final Batch Loss: 1.4690208435058594\n",
      "Epoch 74, Loss: 2.9697576761245728, Final Batch Loss: 1.533309817314148\n",
      "Epoch 75, Loss: 2.883947968482971, Final Batch Loss: 1.4500192403793335\n",
      "Epoch 76, Loss: 2.7762506008148193, Final Batch Loss: 1.411386251449585\n",
      "Epoch 77, Loss: 2.7911990880966187, Final Batch Loss: 1.379046082496643\n",
      "Epoch 78, Loss: 2.7609810829162598, Final Batch Loss: 1.3675899505615234\n",
      "Epoch 79, Loss: 2.6934105157852173, Final Batch Loss: 1.3835445642471313\n",
      "Epoch 80, Loss: 2.7090948820114136, Final Batch Loss: 1.396625280380249\n",
      "Epoch 81, Loss: 2.7850308418273926, Final Batch Loss: 1.4028043746948242\n",
      "Epoch 82, Loss: 2.661758780479431, Final Batch Loss: 1.320783257484436\n",
      "Epoch 83, Loss: 2.639634847640991, Final Batch Loss: 1.3321733474731445\n",
      "Epoch 84, Loss: 2.5467766523361206, Final Batch Loss: 1.2638059854507446\n",
      "Epoch 85, Loss: 2.5471900701522827, Final Batch Loss: 1.280252456665039\n",
      "Epoch 86, Loss: 2.51750910282135, Final Batch Loss: 1.224279761314392\n",
      "Epoch 87, Loss: 2.4166394472122192, Final Batch Loss: 1.1455926895141602\n",
      "Epoch 88, Loss: 2.51193904876709, Final Batch Loss: 1.2708016633987427\n",
      "Epoch 89, Loss: 2.4752548933029175, Final Batch Loss: 1.2365936040878296\n",
      "Epoch 90, Loss: 2.46055805683136, Final Batch Loss: 1.2293206453323364\n",
      "Epoch 91, Loss: 2.4387106895446777, Final Batch Loss: 1.2223730087280273\n",
      "Epoch 92, Loss: 2.4949960708618164, Final Batch Loss: 1.2968964576721191\n",
      "Epoch 93, Loss: 2.4036483764648438, Final Batch Loss: 1.172820806503296\n",
      "Epoch 94, Loss: 2.3113969564437866, Final Batch Loss: 1.1731644868850708\n",
      "Epoch 95, Loss: 2.4136080741882324, Final Batch Loss: 1.2275867462158203\n",
      "Epoch 96, Loss: 2.3729299306869507, Final Batch Loss: 1.246046543121338\n",
      "Epoch 97, Loss: 2.337700605392456, Final Batch Loss: 1.1406797170639038\n",
      "Epoch 98, Loss: 2.3886533975601196, Final Batch Loss: 1.1995482444763184\n",
      "Epoch 99, Loss: 2.2550597190856934, Final Batch Loss: 1.0709846019744873\n",
      "Epoch 100, Loss: 2.2983895540237427, Final Batch Loss: 1.1377073526382446\n",
      "Epoch 101, Loss: 2.3296499252319336, Final Batch Loss: 1.207388162612915\n",
      "Epoch 102, Loss: 2.3477901220321655, Final Batch Loss: 1.192706823348999\n",
      "Epoch 103, Loss: 2.2828983068466187, Final Batch Loss: 1.1569756269454956\n",
      "Epoch 104, Loss: 2.1872177124023438, Final Batch Loss: 1.0940361022949219\n",
      "Epoch 105, Loss: 2.21014940738678, Final Batch Loss: 1.1463357210159302\n",
      "Epoch 106, Loss: 2.2458107471466064, Final Batch Loss: 1.1645660400390625\n",
      "Epoch 107, Loss: 2.1904797554016113, Final Batch Loss: 1.0904616117477417\n",
      "Epoch 108, Loss: 2.165191411972046, Final Batch Loss: 1.0526015758514404\n",
      "Epoch 109, Loss: 2.201248526573181, Final Batch Loss: 1.0816192626953125\n",
      "Epoch 110, Loss: 2.188114881515503, Final Batch Loss: 1.0596550703048706\n",
      "Epoch 111, Loss: 2.1146292686462402, Final Batch Loss: 1.0350079536437988\n",
      "Epoch 112, Loss: 2.153896450996399, Final Batch Loss: 1.0815171003341675\n",
      "Epoch 113, Loss: 2.115243673324585, Final Batch Loss: 1.0454655885696411\n",
      "Epoch 114, Loss: 2.0462790727615356, Final Batch Loss: 1.01438307762146\n",
      "Epoch 115, Loss: 2.0861464738845825, Final Batch Loss: 1.0368198156356812\n",
      "Epoch 116, Loss: 2.0247408747673035, Final Batch Loss: 1.0687955617904663\n",
      "Epoch 117, Loss: 2.0643030405044556, Final Batch Loss: 1.0104568004608154\n",
      "Epoch 118, Loss: 2.1022201776504517, Final Batch Loss: 1.0799392461776733\n",
      "Epoch 119, Loss: 2.142715334892273, Final Batch Loss: 1.172695517539978\n",
      "Epoch 120, Loss: 1.9893606305122375, Final Batch Loss: 0.9690800309181213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, Loss: 2.027718961238861, Final Batch Loss: 0.9879737496376038\n",
      "Epoch 122, Loss: 2.083704948425293, Final Batch Loss: 1.0287251472473145\n",
      "Epoch 123, Loss: 1.9738521575927734, Final Batch Loss: 0.9669747352600098\n",
      "Epoch 124, Loss: 2.032165765762329, Final Batch Loss: 1.0507198572158813\n",
      "Epoch 125, Loss: 2.105739712715149, Final Batch Loss: 1.0992822647094727\n",
      "Epoch 126, Loss: 1.9250598549842834, Final Batch Loss: 0.9483267068862915\n",
      "Epoch 127, Loss: 1.954932153224945, Final Batch Loss: 0.9554834365844727\n",
      "Epoch 128, Loss: 1.9411250352859497, Final Batch Loss: 0.9258109331130981\n",
      "Epoch 129, Loss: 2.0324826836586, Final Batch Loss: 1.0531355142593384\n",
      "Epoch 130, Loss: 1.914914608001709, Final Batch Loss: 0.9776797294616699\n",
      "Epoch 131, Loss: 1.9749276041984558, Final Batch Loss: 0.9987398982048035\n",
      "Epoch 132, Loss: 1.9969715476036072, Final Batch Loss: 0.9913132786750793\n",
      "Epoch 133, Loss: 2.0436244010925293, Final Batch Loss: 1.0102170705795288\n",
      "Epoch 134, Loss: 1.9552233219146729, Final Batch Loss: 0.9744547605514526\n",
      "Epoch 135, Loss: 1.9273988008499146, Final Batch Loss: 0.875095009803772\n",
      "Epoch 136, Loss: 1.8778850436210632, Final Batch Loss: 0.9398297071456909\n",
      "Epoch 137, Loss: 1.8178907632827759, Final Batch Loss: 0.8744583129882812\n",
      "Epoch 138, Loss: 1.9161654114723206, Final Batch Loss: 0.9546297192573547\n",
      "Epoch 139, Loss: 1.8795025944709778, Final Batch Loss: 0.9369266033172607\n",
      "Epoch 140, Loss: 1.864667534828186, Final Batch Loss: 0.8858457207679749\n",
      "Epoch 141, Loss: 1.8846075534820557, Final Batch Loss: 0.9953716993331909\n",
      "Epoch 142, Loss: 1.8233309388160706, Final Batch Loss: 0.8865326642990112\n",
      "Epoch 143, Loss: 1.6616584658622742, Final Batch Loss: 0.7936114072799683\n",
      "Epoch 144, Loss: 1.6576080322265625, Final Batch Loss: 0.8011691570281982\n",
      "Epoch 145, Loss: 1.7912052273750305, Final Batch Loss: 0.8543013334274292\n",
      "Epoch 146, Loss: 1.739938199520111, Final Batch Loss: 0.8735859990119934\n",
      "Epoch 147, Loss: 1.7096484899520874, Final Batch Loss: 0.8828831315040588\n",
      "Epoch 148, Loss: 1.7733078002929688, Final Batch Loss: 0.9019262790679932\n",
      "Epoch 149, Loss: 1.7521316409111023, Final Batch Loss: 0.8808987736701965\n",
      "Epoch 150, Loss: 1.7364090085029602, Final Batch Loss: 0.837056040763855\n",
      "Epoch 151, Loss: 1.6945888996124268, Final Batch Loss: 0.8713880777359009\n",
      "Epoch 152, Loss: 1.7321080565452576, Final Batch Loss: 0.8788967728614807\n",
      "Epoch 153, Loss: 1.7220816612243652, Final Batch Loss: 0.8678622245788574\n",
      "Epoch 154, Loss: 1.744844913482666, Final Batch Loss: 0.9557105302810669\n",
      "Epoch 155, Loss: 1.5445539951324463, Final Batch Loss: 0.7489731907844543\n",
      "Epoch 156, Loss: 1.7295185923576355, Final Batch Loss: 0.9008203744888306\n",
      "Epoch 157, Loss: 1.592796504497528, Final Batch Loss: 0.7639972567558289\n",
      "Epoch 158, Loss: 1.6507214307785034, Final Batch Loss: 0.8260577321052551\n",
      "Epoch 159, Loss: 1.5348422527313232, Final Batch Loss: 0.7641746997833252\n",
      "Epoch 160, Loss: 1.6423300504684448, Final Batch Loss: 0.7968244552612305\n",
      "Epoch 161, Loss: 1.6335924863815308, Final Batch Loss: 0.7541676163673401\n",
      "Epoch 162, Loss: 1.585769534111023, Final Batch Loss: 0.7824126482009888\n",
      "Epoch 163, Loss: 1.6475931406021118, Final Batch Loss: 0.8265779614448547\n",
      "Epoch 164, Loss: 1.5744556188583374, Final Batch Loss: 0.7676026821136475\n",
      "Epoch 165, Loss: 1.5649127960205078, Final Batch Loss: 0.8114007711410522\n",
      "Epoch 166, Loss: 1.686438262462616, Final Batch Loss: 0.8117597103118896\n",
      "Epoch 167, Loss: 1.5401415824890137, Final Batch Loss: 0.7539626359939575\n",
      "Epoch 168, Loss: 1.567133903503418, Final Batch Loss: 0.8045556545257568\n",
      "Epoch 169, Loss: 1.5080084800720215, Final Batch Loss: 0.7093903422355652\n",
      "Epoch 170, Loss: 1.4882020354270935, Final Batch Loss: 0.715557336807251\n",
      "Epoch 171, Loss: 1.5669887065887451, Final Batch Loss: 0.7657718658447266\n",
      "Epoch 172, Loss: 1.5264888405799866, Final Batch Loss: 0.7403086423873901\n",
      "Epoch 173, Loss: 1.5933870077133179, Final Batch Loss: 0.8033506274223328\n",
      "Epoch 174, Loss: 1.5865101218223572, Final Batch Loss: 0.7683741450309753\n",
      "Epoch 175, Loss: 1.5263894200325012, Final Batch Loss: 0.7713558077812195\n",
      "Epoch 176, Loss: 1.4863817691802979, Final Batch Loss: 0.7505494952201843\n",
      "Epoch 177, Loss: 1.4897047281265259, Final Batch Loss: 0.6710147261619568\n",
      "Epoch 178, Loss: 1.6009668707847595, Final Batch Loss: 0.8068050146102905\n",
      "Epoch 179, Loss: 1.5003948211669922, Final Batch Loss: 0.7854541540145874\n",
      "Epoch 180, Loss: 1.475871741771698, Final Batch Loss: 0.7442270517349243\n",
      "Epoch 181, Loss: 1.5289698839187622, Final Batch Loss: 0.7701543569564819\n",
      "Epoch 182, Loss: 1.5632001161575317, Final Batch Loss: 0.7147907614707947\n",
      "Epoch 183, Loss: 1.59048330783844, Final Batch Loss: 0.7826700806617737\n",
      "Epoch 184, Loss: 1.5510842204093933, Final Batch Loss: 0.8363232016563416\n",
      "Epoch 185, Loss: 1.5121474266052246, Final Batch Loss: 0.7152305841445923\n",
      "Epoch 186, Loss: 1.4282872676849365, Final Batch Loss: 0.6848954558372498\n",
      "Epoch 187, Loss: 1.4465319514274597, Final Batch Loss: 0.7337282299995422\n",
      "Epoch 188, Loss: 1.5002567172050476, Final Batch Loss: 0.7324008345603943\n",
      "Epoch 189, Loss: 1.483594834804535, Final Batch Loss: 0.7330600619316101\n",
      "Epoch 190, Loss: 1.4638301134109497, Final Batch Loss: 0.7320926785469055\n",
      "Epoch 191, Loss: 1.503760576248169, Final Batch Loss: 0.7647320628166199\n",
      "Epoch 192, Loss: 1.4184240698814392, Final Batch Loss: 0.7266560792922974\n",
      "Epoch 193, Loss: 1.4319481253623962, Final Batch Loss: 0.7131165266036987\n",
      "Epoch 194, Loss: 1.4044829607009888, Final Batch Loss: 0.6751188635826111\n",
      "Epoch 195, Loss: 1.4320870637893677, Final Batch Loss: 0.7005828619003296\n",
      "Epoch 196, Loss: 1.4668539762496948, Final Batch Loss: 0.6828694343566895\n",
      "Epoch 197, Loss: 1.4819974899291992, Final Batch Loss: 0.7453077435493469\n",
      "Epoch 198, Loss: 1.5290066599845886, Final Batch Loss: 0.813707172870636\n",
      "Epoch 199, Loss: 1.4964014291763306, Final Batch Loss: 0.7388485670089722\n",
      "Epoch 200, Loss: 1.5085542798042297, Final Batch Loss: 0.7920200824737549\n",
      "Epoch 201, Loss: 1.3536242246627808, Final Batch Loss: 0.6096206903457642\n",
      "Epoch 202, Loss: 1.4118624329566956, Final Batch Loss: 0.6589520573616028\n",
      "Epoch 203, Loss: 1.4488569498062134, Final Batch Loss: 0.7083795070648193\n",
      "Epoch 204, Loss: 1.4235213994979858, Final Batch Loss: 0.74817955493927\n",
      "Epoch 205, Loss: 1.455434262752533, Final Batch Loss: 0.7172225117683411\n",
      "Epoch 206, Loss: 1.4317836165428162, Final Batch Loss: 0.7137924432754517\n",
      "Epoch 207, Loss: 1.5289487838745117, Final Batch Loss: 0.8214870691299438\n",
      "Epoch 208, Loss: 1.409579873085022, Final Batch Loss: 0.7410181164741516\n",
      "Epoch 209, Loss: 1.469529390335083, Final Batch Loss: 0.7181635499000549\n",
      "Epoch 210, Loss: 1.4160897731781006, Final Batch Loss: 0.7139729857444763\n",
      "Epoch 211, Loss: 1.4033523797988892, Final Batch Loss: 0.6429579854011536\n",
      "Epoch 212, Loss: 1.398461103439331, Final Batch Loss: 0.7263653874397278\n",
      "Epoch 213, Loss: 1.3611412048339844, Final Batch Loss: 0.6912656426429749\n",
      "Epoch 214, Loss: 1.4500019550323486, Final Batch Loss: 0.718744158744812\n",
      "Epoch 215, Loss: 1.3279675245285034, Final Batch Loss: 0.5936656594276428\n",
      "Epoch 216, Loss: 1.4947102069854736, Final Batch Loss: 0.7525080442428589\n",
      "Epoch 217, Loss: 1.4661383628845215, Final Batch Loss: 0.7762184739112854\n",
      "Epoch 218, Loss: 1.4381502866744995, Final Batch Loss: 0.765966534614563\n",
      "Epoch 219, Loss: 1.4430772066116333, Final Batch Loss: 0.6939573287963867\n",
      "Epoch 220, Loss: 1.3988246321678162, Final Batch Loss: 0.686620831489563\n",
      "Epoch 221, Loss: 1.3957046866416931, Final Batch Loss: 0.6637844443321228\n",
      "Epoch 222, Loss: 1.3741825222969055, Final Batch Loss: 0.6892094612121582\n",
      "Epoch 223, Loss: 1.3911641240119934, Final Batch Loss: 0.6625919342041016\n",
      "Epoch 224, Loss: 1.3275012373924255, Final Batch Loss: 0.6487295031547546\n",
      "Epoch 225, Loss: 1.3420377969741821, Final Batch Loss: 0.6663071513175964\n",
      "Epoch 226, Loss: 1.3399815559387207, Final Batch Loss: 0.6348441243171692\n",
      "Epoch 227, Loss: 1.322309136390686, Final Batch Loss: 0.660312831401825\n",
      "Epoch 228, Loss: 1.4149742722511292, Final Batch Loss: 0.7379688024520874\n",
      "Epoch 229, Loss: 1.4150115251541138, Final Batch Loss: 0.7766515016555786\n",
      "Epoch 230, Loss: 1.450052261352539, Final Batch Loss: 0.7516428828239441\n",
      "Epoch 231, Loss: 1.386811912059784, Final Batch Loss: 0.7240747213363647\n",
      "Epoch 232, Loss: 1.3851242065429688, Final Batch Loss: 0.6401529908180237\n",
      "Epoch 233, Loss: 1.3670689463615417, Final Batch Loss: 0.7525736689567566\n",
      "Epoch 234, Loss: 1.3422231078147888, Final Batch Loss: 0.6423308849334717\n",
      "Epoch 235, Loss: 1.3632229566574097, Final Batch Loss: 0.6549705266952515\n",
      "Epoch 236, Loss: 1.354606568813324, Final Batch Loss: 0.6911245584487915\n",
      "Epoch 237, Loss: 1.4507273435592651, Final Batch Loss: 0.7902683019638062\n",
      "Epoch 238, Loss: 1.3529319763183594, Final Batch Loss: 0.6208550930023193\n",
      "Epoch 239, Loss: 1.3079107403755188, Final Batch Loss: 0.6630682945251465\n",
      "Epoch 240, Loss: 1.4204348921775818, Final Batch Loss: 0.7681238651275635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241, Loss: 1.3535632491111755, Final Batch Loss: 0.7030386924743652\n",
      "Epoch 242, Loss: 1.3702980279922485, Final Batch Loss: 0.6922874450683594\n",
      "Epoch 243, Loss: 1.3329049348831177, Final Batch Loss: 0.6790294647216797\n",
      "Epoch 244, Loss: 1.289788842201233, Final Batch Loss: 0.6131523251533508\n",
      "Epoch 245, Loss: 1.3759021759033203, Final Batch Loss: 0.6969937086105347\n",
      "Epoch 246, Loss: 1.358367145061493, Final Batch Loss: 0.6670178174972534\n",
      "Epoch 247, Loss: 1.385493278503418, Final Batch Loss: 0.770465075969696\n",
      "Epoch 248, Loss: 1.3358246684074402, Final Batch Loss: 0.6873610019683838\n",
      "Epoch 249, Loss: 1.2903789281845093, Final Batch Loss: 0.5875228643417358\n",
      "Epoch 250, Loss: 1.256798267364502, Final Batch Loss: 0.619580090045929\n",
      "Epoch 251, Loss: 1.3335672616958618, Final Batch Loss: 0.6402621269226074\n",
      "Epoch 252, Loss: 1.277637243270874, Final Batch Loss: 0.6371472477912903\n",
      "Epoch 253, Loss: 1.282669186592102, Final Batch Loss: 0.6854606866836548\n",
      "Epoch 254, Loss: 1.2250239253044128, Final Batch Loss: 0.5720085501670837\n",
      "Epoch 255, Loss: 1.2523980736732483, Final Batch Loss: 0.6147222518920898\n",
      "Epoch 256, Loss: 1.2501450181007385, Final Batch Loss: 0.6553512215614319\n",
      "Epoch 257, Loss: 1.26714426279068, Final Batch Loss: 0.6362903118133545\n",
      "Epoch 258, Loss: 1.2839626669883728, Final Batch Loss: 0.6181632280349731\n",
      "Epoch 259, Loss: 1.2445582747459412, Final Batch Loss: 0.5797115564346313\n",
      "Epoch 260, Loss: 1.271330714225769, Final Batch Loss: 0.6525353193283081\n",
      "Epoch 261, Loss: 1.2194029092788696, Final Batch Loss: 0.6460138559341431\n",
      "Epoch 262, Loss: 1.2903911471366882, Final Batch Loss: 0.6655717492103577\n",
      "Epoch 263, Loss: 1.2832472324371338, Final Batch Loss: 0.6163486242294312\n",
      "Epoch 264, Loss: 1.2582939863204956, Final Batch Loss: 0.645473837852478\n",
      "Epoch 265, Loss: 1.315782070159912, Final Batch Loss: 0.6781766414642334\n",
      "Epoch 266, Loss: 1.1899869441986084, Final Batch Loss: 0.6034969687461853\n",
      "Epoch 267, Loss: 1.3348486423492432, Final Batch Loss: 0.7042222023010254\n",
      "Epoch 268, Loss: 1.2984427213668823, Final Batch Loss: 0.6475372910499573\n",
      "Epoch 269, Loss: 1.241684377193451, Final Batch Loss: 0.6164894104003906\n",
      "Epoch 270, Loss: 1.324405014514923, Final Batch Loss: 0.6610614061355591\n",
      "Epoch 271, Loss: 1.168753445148468, Final Batch Loss: 0.5799006819725037\n",
      "Epoch 272, Loss: 1.1654744744300842, Final Batch Loss: 0.6186673045158386\n",
      "Epoch 273, Loss: 1.2186890840530396, Final Batch Loss: 0.6388596296310425\n",
      "Epoch 274, Loss: 1.264392614364624, Final Batch Loss: 0.6467892527580261\n",
      "Epoch 275, Loss: 1.2176635265350342, Final Batch Loss: 0.6214669942855835\n",
      "Epoch 276, Loss: 1.2127192616462708, Final Batch Loss: 0.5464992523193359\n",
      "Epoch 277, Loss: 1.1487506031990051, Final Batch Loss: 0.5339524745941162\n",
      "Epoch 278, Loss: 1.207409143447876, Final Batch Loss: 0.6218017935752869\n",
      "Epoch 279, Loss: 1.1290343403816223, Final Batch Loss: 0.5210942625999451\n",
      "Epoch 280, Loss: 1.176243007183075, Final Batch Loss: 0.5946854948997498\n",
      "Epoch 281, Loss: 1.180892288684845, Final Batch Loss: 0.5883869528770447\n",
      "Epoch 282, Loss: 1.242829978466034, Final Batch Loss: 0.68730628490448\n",
      "Epoch 283, Loss: 1.2227346301078796, Final Batch Loss: 0.6551300883293152\n",
      "Epoch 284, Loss: 1.279891014099121, Final Batch Loss: 0.6878231167793274\n",
      "Epoch 285, Loss: 1.229353427886963, Final Batch Loss: 0.6288855075836182\n",
      "Epoch 286, Loss: 1.2219513654708862, Final Batch Loss: 0.620218813419342\n",
      "Epoch 287, Loss: 1.1667465567588806, Final Batch Loss: 0.5599228739738464\n",
      "Epoch 288, Loss: 1.2017326951026917, Final Batch Loss: 0.6009908318519592\n",
      "Epoch 289, Loss: 1.1848517656326294, Final Batch Loss: 0.5809348225593567\n",
      "Epoch 290, Loss: 1.2260522246360779, Final Batch Loss: 0.6518365144729614\n",
      "Epoch 291, Loss: 1.1475048661231995, Final Batch Loss: 0.6095055937767029\n",
      "Epoch 292, Loss: 1.1179551482200623, Final Batch Loss: 0.5683259963989258\n",
      "Epoch 293, Loss: 1.1165198683738708, Final Batch Loss: 0.5471322536468506\n",
      "Epoch 294, Loss: 1.120079517364502, Final Batch Loss: 0.5548381805419922\n",
      "Epoch 295, Loss: 1.072143167257309, Final Batch Loss: 0.472521036863327\n",
      "Epoch 296, Loss: 1.1595712900161743, Final Batch Loss: 0.5859591364860535\n",
      "Epoch 297, Loss: 1.1011391878128052, Final Batch Loss: 0.5298434495925903\n",
      "Epoch 298, Loss: 1.1752371788024902, Final Batch Loss: 0.59388267993927\n",
      "Epoch 299, Loss: 1.1712111830711365, Final Batch Loss: 0.5324844121932983\n",
      "Epoch 300, Loss: 1.065596342086792, Final Batch Loss: 0.5356483459472656\n",
      "Epoch 301, Loss: 1.0683594942092896, Final Batch Loss: 0.5244966149330139\n",
      "Epoch 302, Loss: 1.1542322635650635, Final Batch Loss: 0.5830072164535522\n",
      "Epoch 303, Loss: 1.0753095149993896, Final Batch Loss: 0.5076938271522522\n",
      "Epoch 304, Loss: 1.1702542304992676, Final Batch Loss: 0.5854126214981079\n",
      "Epoch 305, Loss: 1.1226720809936523, Final Batch Loss: 0.5457277894020081\n",
      "Epoch 306, Loss: 1.1549291610717773, Final Batch Loss: 0.6169871687889099\n",
      "Epoch 307, Loss: 0.9865947365760803, Final Batch Loss: 0.442838191986084\n",
      "Epoch 308, Loss: 1.0656476616859436, Final Batch Loss: 0.5065881013870239\n",
      "Epoch 309, Loss: 1.0225729942321777, Final Batch Loss: 0.5088768005371094\n",
      "Epoch 310, Loss: 1.0738227367401123, Final Batch Loss: 0.5026808977127075\n",
      "Epoch 311, Loss: 1.045602947473526, Final Batch Loss: 0.4754658639431\n",
      "Epoch 312, Loss: 1.0738416910171509, Final Batch Loss: 0.5405290722846985\n",
      "Epoch 313, Loss: 1.159501552581787, Final Batch Loss: 0.5989041328430176\n",
      "Epoch 314, Loss: 1.0359301269054413, Final Batch Loss: 0.440266877412796\n",
      "Epoch 315, Loss: 1.073729157447815, Final Batch Loss: 0.5299317836761475\n",
      "Epoch 316, Loss: 1.122382402420044, Final Batch Loss: 0.5974896550178528\n",
      "Epoch 317, Loss: 1.0701386332511902, Final Batch Loss: 0.5321403741836548\n",
      "Epoch 318, Loss: 1.1069388389587402, Final Batch Loss: 0.5188628435134888\n",
      "Epoch 319, Loss: 1.0836135745048523, Final Batch Loss: 0.5541543960571289\n",
      "Epoch 320, Loss: 1.048364520072937, Final Batch Loss: 0.5181024670600891\n",
      "Epoch 321, Loss: 1.013156145811081, Final Batch Loss: 0.498125284910202\n",
      "Epoch 322, Loss: 1.0361247062683105, Final Batch Loss: 0.5181055068969727\n",
      "Epoch 323, Loss: 1.067380666732788, Final Batch Loss: 0.5207854509353638\n",
      "Epoch 324, Loss: 1.062577247619629, Final Batch Loss: 0.5432730317115784\n",
      "Epoch 325, Loss: 1.0378484427928925, Final Batch Loss: 0.5529996156692505\n",
      "Epoch 326, Loss: 1.0372422933578491, Final Batch Loss: 0.5107539892196655\n",
      "Epoch 327, Loss: 1.0683082938194275, Final Batch Loss: 0.502682626247406\n",
      "Epoch 328, Loss: 0.9681761264801025, Final Batch Loss: 0.44437170028686523\n",
      "Epoch 329, Loss: 1.0809729099273682, Final Batch Loss: 0.5438438057899475\n",
      "Epoch 330, Loss: 1.0323571562767029, Final Batch Loss: 0.45423173904418945\n",
      "Epoch 331, Loss: 1.071929156780243, Final Batch Loss: 0.5196240544319153\n",
      "Epoch 332, Loss: 1.0415217280387878, Final Batch Loss: 0.5240215063095093\n",
      "Epoch 333, Loss: 1.0209178924560547, Final Batch Loss: 0.5064852237701416\n",
      "Epoch 334, Loss: 1.0253084301948547, Final Batch Loss: 0.5237553119659424\n",
      "Epoch 335, Loss: 1.137737214565277, Final Batch Loss: 0.6062874794006348\n",
      "Epoch 336, Loss: 1.0608478784561157, Final Batch Loss: 0.586124837398529\n",
      "Epoch 337, Loss: 0.993488997220993, Final Batch Loss: 0.4665078818798065\n",
      "Epoch 338, Loss: 1.0354860424995422, Final Batch Loss: 0.49548494815826416\n",
      "Epoch 339, Loss: 1.0922219157218933, Final Batch Loss: 0.5477160811424255\n",
      "Epoch 340, Loss: 1.0752054452896118, Final Batch Loss: 0.5991726517677307\n",
      "Epoch 341, Loss: 0.9604447185993195, Final Batch Loss: 0.4924752414226532\n",
      "Epoch 342, Loss: 1.0701241493225098, Final Batch Loss: 0.5725521445274353\n",
      "Epoch 343, Loss: 1.0163182616233826, Final Batch Loss: 0.49049121141433716\n",
      "Epoch 344, Loss: 0.9381004869937897, Final Batch Loss: 0.4935421943664551\n",
      "Epoch 345, Loss: 1.0918022990226746, Final Batch Loss: 0.5677521824836731\n",
      "Epoch 346, Loss: 1.0159143209457397, Final Batch Loss: 0.5154111385345459\n",
      "Epoch 347, Loss: 1.0504986643791199, Final Batch Loss: 0.5075774788856506\n",
      "Epoch 348, Loss: 1.0374812185764313, Final Batch Loss: 0.5554930567741394\n",
      "Epoch 349, Loss: 1.006635844707489, Final Batch Loss: 0.5275193452835083\n",
      "Epoch 350, Loss: 1.0865176320075989, Final Batch Loss: 0.6001052260398865\n",
      "Epoch 351, Loss: 0.9630587100982666, Final Batch Loss: 0.49504756927490234\n",
      "Epoch 352, Loss: 0.9918438494205475, Final Batch Loss: 0.5027941465377808\n",
      "Epoch 353, Loss: 1.0451698899269104, Final Batch Loss: 0.545080840587616\n",
      "Epoch 354, Loss: 0.9929488599300385, Final Batch Loss: 0.5074321627616882\n",
      "Epoch 355, Loss: 0.9202699065208435, Final Batch Loss: 0.42808791995048523\n",
      "Epoch 356, Loss: 0.9533936977386475, Final Batch Loss: 0.47752949595451355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357, Loss: 0.9990724325180054, Final Batch Loss: 0.5215614438056946\n",
      "Epoch 358, Loss: 1.0800487995147705, Final Batch Loss: 0.5731357932090759\n",
      "Epoch 359, Loss: 0.9824163317680359, Final Batch Loss: 0.5382736921310425\n",
      "Epoch 360, Loss: 0.9794229865074158, Final Batch Loss: 0.4740772247314453\n",
      "Epoch 361, Loss: 0.9346596896648407, Final Batch Loss: 0.4541826546192169\n",
      "Epoch 362, Loss: 0.9205688834190369, Final Batch Loss: 0.43905705213546753\n",
      "Epoch 363, Loss: 0.9127344191074371, Final Batch Loss: 0.3824264109134674\n",
      "Epoch 364, Loss: 1.0033100843429565, Final Batch Loss: 0.4915015697479248\n",
      "Epoch 365, Loss: 1.0985193252563477, Final Batch Loss: 0.564925491809845\n",
      "Epoch 366, Loss: 1.0098675191402435, Final Batch Loss: 0.6032668352127075\n",
      "Epoch 367, Loss: 1.0161621272563934, Final Batch Loss: 0.5790647268295288\n",
      "Epoch 368, Loss: 1.020173966884613, Final Batch Loss: 0.5213600993156433\n",
      "Epoch 369, Loss: 0.9244889318943024, Final Batch Loss: 0.4272715151309967\n",
      "Epoch 370, Loss: 0.9644765555858612, Final Batch Loss: 0.47253650426864624\n",
      "Epoch 371, Loss: 0.9969760179519653, Final Batch Loss: 0.46677714586257935\n",
      "Epoch 372, Loss: 1.0247657299041748, Final Batch Loss: 0.5527797341346741\n",
      "Epoch 373, Loss: 0.9070581495761871, Final Batch Loss: 0.4168972074985504\n",
      "Epoch 374, Loss: 0.9773240983486176, Final Batch Loss: 0.43203285336494446\n",
      "Epoch 375, Loss: 1.0277018547058105, Final Batch Loss: 0.5384825468063354\n",
      "Epoch 376, Loss: 0.9283398985862732, Final Batch Loss: 0.47870373725891113\n",
      "Epoch 377, Loss: 0.9719501733779907, Final Batch Loss: 0.49045518040657043\n",
      "Epoch 378, Loss: 0.9728600978851318, Final Batch Loss: 0.4473540186882019\n",
      "Epoch 379, Loss: 0.8917299509048462, Final Batch Loss: 0.466480016708374\n",
      "Epoch 380, Loss: 1.0362390279769897, Final Batch Loss: 0.5424551963806152\n",
      "Epoch 381, Loss: 0.9491565525531769, Final Batch Loss: 0.4690227210521698\n",
      "Epoch 382, Loss: 0.9498574137687683, Final Batch Loss: 0.43809717893600464\n",
      "Epoch 383, Loss: 1.0278318226337433, Final Batch Loss: 0.5290061831474304\n",
      "Epoch 384, Loss: 0.932394951581955, Final Batch Loss: 0.49663832783699036\n",
      "Epoch 385, Loss: 0.9675532281398773, Final Batch Loss: 0.47599998116493225\n",
      "Epoch 386, Loss: 1.0315152406692505, Final Batch Loss: 0.523823618888855\n",
      "Epoch 387, Loss: 0.937633603811264, Final Batch Loss: 0.46524685621261597\n",
      "Epoch 388, Loss: 0.9402305781841278, Final Batch Loss: 0.4977973401546478\n",
      "Epoch 389, Loss: 0.9810794293880463, Final Batch Loss: 0.5031040906906128\n",
      "Epoch 390, Loss: 1.0020501911640167, Final Batch Loss: 0.5846858024597168\n",
      "Epoch 391, Loss: 0.9456523656845093, Final Batch Loss: 0.5064916610717773\n",
      "Epoch 392, Loss: 0.892144650220871, Final Batch Loss: 0.45799607038497925\n",
      "Epoch 393, Loss: 0.8946426808834076, Final Batch Loss: 0.4137052297592163\n",
      "Epoch 394, Loss: 0.9113048911094666, Final Batch Loss: 0.4201323091983795\n",
      "Epoch 395, Loss: 0.9038814008235931, Final Batch Loss: 0.41924816370010376\n",
      "Epoch 396, Loss: 0.9532686471939087, Final Batch Loss: 0.4733702838420868\n",
      "Epoch 397, Loss: 0.905014306306839, Final Batch Loss: 0.4227805733680725\n",
      "Epoch 398, Loss: 0.9489817321300507, Final Batch Loss: 0.4634345769882202\n",
      "Epoch 399, Loss: 0.9130882322788239, Final Batch Loss: 0.47356492280960083\n",
      "Epoch 400, Loss: 0.9120971262454987, Final Batch Loss: 0.461479514837265\n",
      "Epoch 401, Loss: 0.9219060838222504, Final Batch Loss: 0.5237022638320923\n",
      "Epoch 402, Loss: 0.9716415107250214, Final Batch Loss: 0.49956628680229187\n",
      "Epoch 403, Loss: 0.9425306916236877, Final Batch Loss: 0.4775189161300659\n",
      "Epoch 404, Loss: 0.9326984584331512, Final Batch Loss: 0.4655068516731262\n",
      "Epoch 405, Loss: 0.9384622871875763, Final Batch Loss: 0.45808032155036926\n",
      "Epoch 406, Loss: 0.9746111631393433, Final Batch Loss: 0.5561459064483643\n",
      "Epoch 407, Loss: 0.9488139152526855, Final Batch Loss: 0.5108242630958557\n",
      "Epoch 408, Loss: 0.9117330312728882, Final Batch Loss: 0.4445362091064453\n",
      "Epoch 409, Loss: 0.8871950209140778, Final Batch Loss: 0.44736459851264954\n",
      "Epoch 410, Loss: 0.9219584465026855, Final Batch Loss: 0.4510296583175659\n",
      "Epoch 411, Loss: 0.9132329821586609, Final Batch Loss: 0.43635129928588867\n",
      "Epoch 412, Loss: 0.8921475410461426, Final Batch Loss: 0.4456152021884918\n",
      "Epoch 413, Loss: 0.9057972431182861, Final Batch Loss: 0.494979590177536\n",
      "Epoch 414, Loss: 0.8990960419178009, Final Batch Loss: 0.42756569385528564\n",
      "Epoch 415, Loss: 0.8984871208667755, Final Batch Loss: 0.46807071566581726\n",
      "Epoch 416, Loss: 0.8318569660186768, Final Batch Loss: 0.4010438621044159\n",
      "Epoch 417, Loss: 0.8132510185241699, Final Batch Loss: 0.3892814815044403\n",
      "Epoch 418, Loss: 0.9627891480922699, Final Batch Loss: 0.5121674537658691\n",
      "Epoch 419, Loss: 0.9109801352024078, Final Batch Loss: 0.44101762771606445\n",
      "Epoch 420, Loss: 0.913126140832901, Final Batch Loss: 0.411980003118515\n",
      "Epoch 421, Loss: 0.9024272263050079, Final Batch Loss: 0.40931010246276855\n",
      "Epoch 422, Loss: 0.9481211006641388, Final Batch Loss: 0.47095561027526855\n",
      "Epoch 423, Loss: 0.9345238208770752, Final Batch Loss: 0.4781520962715149\n",
      "Epoch 424, Loss: 0.9043698906898499, Final Batch Loss: 0.4897244870662689\n",
      "Epoch 425, Loss: 0.9287036955356598, Final Batch Loss: 0.5050090551376343\n",
      "Epoch 426, Loss: 0.8893972933292389, Final Batch Loss: 0.4570375084877014\n",
      "Epoch 427, Loss: 0.8443982601165771, Final Batch Loss: 0.41974058747291565\n",
      "Epoch 428, Loss: 0.9242889583110809, Final Batch Loss: 0.5280077457427979\n",
      "Epoch 429, Loss: 0.9246588945388794, Final Batch Loss: 0.41470789909362793\n",
      "Epoch 430, Loss: 0.9105922281742096, Final Batch Loss: 0.5023950338363647\n",
      "Epoch 431, Loss: 0.8774015009403229, Final Batch Loss: 0.40564578771591187\n",
      "Epoch 432, Loss: 0.9795381724834442, Final Batch Loss: 0.519144594669342\n",
      "Epoch 433, Loss: 0.9293926656246185, Final Batch Loss: 0.5035393834114075\n",
      "Epoch 434, Loss: 0.988901674747467, Final Batch Loss: 0.5431323051452637\n",
      "Epoch 435, Loss: 0.8794696033000946, Final Batch Loss: 0.42801591753959656\n",
      "Epoch 436, Loss: 0.8838983774185181, Final Batch Loss: 0.4295945167541504\n",
      "Epoch 437, Loss: 0.86371710896492, Final Batch Loss: 0.42838647961616516\n",
      "Epoch 438, Loss: 0.8469032347202301, Final Batch Loss: 0.37780308723449707\n",
      "Epoch 439, Loss: 0.9711631238460541, Final Batch Loss: 0.4785110056400299\n",
      "Epoch 440, Loss: 0.9356549084186554, Final Batch Loss: 0.4495348036289215\n",
      "Epoch 441, Loss: 0.8958253860473633, Final Batch Loss: 0.4154767394065857\n",
      "Epoch 442, Loss: 0.9019517600536346, Final Batch Loss: 0.45026347041130066\n",
      "Epoch 443, Loss: 0.8775236904621124, Final Batch Loss: 0.41943758726119995\n",
      "Epoch 444, Loss: 0.9318378865718842, Final Batch Loss: 0.4974701702594757\n",
      "Epoch 445, Loss: 0.864378809928894, Final Batch Loss: 0.4324396848678589\n",
      "Epoch 446, Loss: 0.8830901384353638, Final Batch Loss: 0.3997008204460144\n",
      "Epoch 447, Loss: 0.8309323787689209, Final Batch Loss: 0.3620625436306\n",
      "Epoch 448, Loss: 0.8370766043663025, Final Batch Loss: 0.3789274990558624\n",
      "Epoch 449, Loss: 0.8543983697891235, Final Batch Loss: 0.397304505109787\n",
      "Epoch 450, Loss: 1.0076946914196014, Final Batch Loss: 0.5432107448577881\n",
      "Epoch 451, Loss: 0.8265665769577026, Final Batch Loss: 0.3808801472187042\n",
      "Epoch 452, Loss: 0.9099165499210358, Final Batch Loss: 0.5050379037857056\n",
      "Epoch 453, Loss: 0.9276995062828064, Final Batch Loss: 0.5118141770362854\n",
      "Epoch 454, Loss: 0.8759832680225372, Final Batch Loss: 0.4298986494541168\n",
      "Epoch 455, Loss: 0.9085775911808014, Final Batch Loss: 0.4361301362514496\n",
      "Epoch 456, Loss: 0.8621495068073273, Final Batch Loss: 0.38861358165740967\n",
      "Epoch 457, Loss: 0.8112747073173523, Final Batch Loss: 0.3750184178352356\n",
      "Epoch 458, Loss: 0.8873283863067627, Final Batch Loss: 0.4507359266281128\n",
      "Epoch 459, Loss: 0.8988851606845856, Final Batch Loss: 0.44551074504852295\n",
      "Epoch 460, Loss: 0.841950535774231, Final Batch Loss: 0.40542808175086975\n",
      "Epoch 461, Loss: 0.8604989051818848, Final Batch Loss: 0.4321540296077728\n",
      "Epoch 462, Loss: 0.8913565874099731, Final Batch Loss: 0.4639667570590973\n",
      "Epoch 463, Loss: 0.8808518052101135, Final Batch Loss: 0.42570960521698\n",
      "Epoch 464, Loss: 0.9149448871612549, Final Batch Loss: 0.4609531760215759\n",
      "Epoch 465, Loss: 0.9884060025215149, Final Batch Loss: 0.5206239223480225\n",
      "Epoch 466, Loss: 0.9951578378677368, Final Batch Loss: 0.5467584729194641\n",
      "Epoch 467, Loss: 0.8506924510002136, Final Batch Loss: 0.3935582935810089\n",
      "Epoch 468, Loss: 0.8909516632556915, Final Batch Loss: 0.46919602155685425\n",
      "Epoch 469, Loss: 0.8146949410438538, Final Batch Loss: 0.36403796076774597\n",
      "Epoch 470, Loss: 0.8575787544250488, Final Batch Loss: 0.40814632177352905\n",
      "Epoch 471, Loss: 0.8897430896759033, Final Batch Loss: 0.44330814480781555\n",
      "Epoch 472, Loss: 0.8538804948329926, Final Batch Loss: 0.4599066972732544\n",
      "Epoch 473, Loss: 0.8878577947616577, Final Batch Loss: 0.47662582993507385\n",
      "Epoch 474, Loss: 0.8585580289363861, Final Batch Loss: 0.4059370756149292\n",
      "Epoch 475, Loss: 0.941455066204071, Final Batch Loss: 0.5413128137588501\n",
      "Epoch 476, Loss: 0.8575118482112885, Final Batch Loss: 0.41137683391571045\n",
      "Epoch 477, Loss: 0.96177539229393, Final Batch Loss: 0.48314645886421204\n",
      "Epoch 478, Loss: 0.8795295059680939, Final Batch Loss: 0.43220505118370056\n",
      "Epoch 479, Loss: 0.8970054090023041, Final Batch Loss: 0.45764535665512085\n",
      "Epoch 480, Loss: 0.8913560509681702, Final Batch Loss: 0.46344900131225586\n",
      "Epoch 481, Loss: 0.8796959817409515, Final Batch Loss: 0.46478331089019775\n",
      "Epoch 482, Loss: 0.8355521857738495, Final Batch Loss: 0.44510236382484436\n",
      "Epoch 483, Loss: 0.8566715717315674, Final Batch Loss: 0.44160473346710205\n",
      "Epoch 484, Loss: 0.8413833975791931, Final Batch Loss: 0.40380343794822693\n",
      "Epoch 485, Loss: 0.8648611009120941, Final Batch Loss: 0.4176616966724396\n",
      "Epoch 486, Loss: 0.8491350412368774, Final Batch Loss: 0.4201066195964813\n",
      "Epoch 487, Loss: 0.893292635679245, Final Batch Loss: 0.4866420030593872\n",
      "Epoch 488, Loss: 0.8036481142044067, Final Batch Loss: 0.3441358208656311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 489, Loss: 0.8452869057655334, Final Batch Loss: 0.4414781332015991\n",
      "Epoch 490, Loss: 0.8302426338195801, Final Batch Loss: 0.4073777496814728\n",
      "Epoch 491, Loss: 0.8284578621387482, Final Batch Loss: 0.41619402170181274\n",
      "Epoch 492, Loss: 0.8495442569255829, Final Batch Loss: 0.45137977600097656\n",
      "Epoch 493, Loss: 0.8466687500476837, Final Batch Loss: 0.42650124430656433\n",
      "Epoch 494, Loss: 0.8626956045627594, Final Batch Loss: 0.40636932849884033\n",
      "Epoch 495, Loss: 0.8121715486049652, Final Batch Loss: 0.3614071309566498\n",
      "Epoch 496, Loss: 0.8299790024757385, Final Batch Loss: 0.3609566390514374\n",
      "Epoch 497, Loss: 0.8750272691249847, Final Batch Loss: 0.4366535544395447\n",
      "Epoch 498, Loss: 0.9393079876899719, Final Batch Loss: 0.47879213094711304\n",
      "Epoch 499, Loss: 0.891334593296051, Final Batch Loss: 0.4050208330154419\n",
      "Epoch 500, Loss: 0.8612225651741028, Final Batch Loss: 0.46480703353881836\n",
      "Epoch 501, Loss: 0.8440901041030884, Final Batch Loss: 0.4137289822101593\n",
      "Epoch 502, Loss: 0.8530102968215942, Final Batch Loss: 0.3917520344257355\n",
      "Epoch 503, Loss: 0.8486585915088654, Final Batch Loss: 0.39183545112609863\n",
      "Epoch 504, Loss: 0.852891355752945, Final Batch Loss: 0.4443856179714203\n",
      "Epoch 505, Loss: 0.851182758808136, Final Batch Loss: 0.44603270292282104\n",
      "Epoch 506, Loss: 0.8557077646255493, Final Batch Loss: 0.45295262336730957\n",
      "Epoch 507, Loss: 0.8613286912441254, Final Batch Loss: 0.45201876759529114\n",
      "Epoch 508, Loss: 0.785237729549408, Final Batch Loss: 0.37970489263534546\n",
      "Epoch 509, Loss: 0.8296746611595154, Final Batch Loss: 0.41096022725105286\n",
      "Epoch 510, Loss: 0.7889188826084137, Final Batch Loss: 0.36058828234672546\n",
      "Epoch 511, Loss: 0.8232878744602203, Final Batch Loss: 0.40745776891708374\n",
      "Epoch 512, Loss: 0.8362523913383484, Final Batch Loss: 0.3916005790233612\n",
      "Epoch 513, Loss: 0.8696693480014801, Final Batch Loss: 0.3993554711341858\n",
      "Epoch 514, Loss: 0.8484334051609039, Final Batch Loss: 0.4387901723384857\n",
      "Epoch 515, Loss: 0.8189173638820648, Final Batch Loss: 0.4095941185951233\n",
      "Epoch 516, Loss: 0.8766193389892578, Final Batch Loss: 0.48254984617233276\n",
      "Epoch 517, Loss: 0.7526218891143799, Final Batch Loss: 0.3297249674797058\n",
      "Epoch 518, Loss: 0.876266598701477, Final Batch Loss: 0.44320133328437805\n",
      "Epoch 519, Loss: 0.8495719730854034, Final Batch Loss: 0.4118979275226593\n",
      "Epoch 520, Loss: 0.7984975874423981, Final Batch Loss: 0.40088751912117004\n",
      "Epoch 521, Loss: 0.8499322235584259, Final Batch Loss: 0.3998061418533325\n",
      "Epoch 522, Loss: 0.8556622862815857, Final Batch Loss: 0.44460931420326233\n",
      "Epoch 523, Loss: 0.8374871611595154, Final Batch Loss: 0.446081280708313\n",
      "Epoch 524, Loss: 0.820295661687851, Final Batch Loss: 0.4096429646015167\n",
      "Epoch 525, Loss: 0.8182196319103241, Final Batch Loss: 0.3807734251022339\n",
      "Epoch 526, Loss: 0.8795023262500763, Final Batch Loss: 0.47860902547836304\n",
      "Epoch 527, Loss: 0.8224381804466248, Final Batch Loss: 0.4109593629837036\n",
      "Epoch 528, Loss: 0.8385714590549469, Final Batch Loss: 0.3822517395019531\n",
      "Epoch 529, Loss: 0.8118034899234772, Final Batch Loss: 0.39859646558761597\n",
      "Epoch 530, Loss: 0.8665811121463776, Final Batch Loss: 0.431976318359375\n",
      "Epoch 531, Loss: 0.8215104937553406, Final Batch Loss: 0.40587764978408813\n",
      "Epoch 532, Loss: 0.791231244802475, Final Batch Loss: 0.3910956084728241\n",
      "Epoch 533, Loss: 0.8389885425567627, Final Batch Loss: 0.3964385986328125\n",
      "Epoch 534, Loss: 0.8021703362464905, Final Batch Loss: 0.3948895037174225\n",
      "Epoch 535, Loss: 0.8119879364967346, Final Batch Loss: 0.40818682312965393\n",
      "Epoch 536, Loss: 0.8916787803173065, Final Batch Loss: 0.4303991198539734\n",
      "Epoch 537, Loss: 0.9117802083492279, Final Batch Loss: 0.504412829875946\n",
      "Epoch 538, Loss: 0.8383889496326447, Final Batch Loss: 0.3791154623031616\n",
      "Epoch 539, Loss: 0.8645304441452026, Final Batch Loss: 0.4257877469062805\n",
      "Epoch 540, Loss: 0.8358617424964905, Final Batch Loss: 0.3983323574066162\n",
      "Epoch 541, Loss: 0.8086485862731934, Final Batch Loss: 0.42812129855155945\n",
      "Epoch 542, Loss: 0.8255032896995544, Final Batch Loss: 0.4145515263080597\n",
      "Epoch 543, Loss: 0.8844014704227448, Final Batch Loss: 0.479725182056427\n",
      "Epoch 544, Loss: 0.8003286719322205, Final Batch Loss: 0.43732693791389465\n",
      "Epoch 545, Loss: 0.8336318135261536, Final Batch Loss: 0.3971407115459442\n",
      "Epoch 546, Loss: 0.8665633201599121, Final Batch Loss: 0.45945602655410767\n",
      "Epoch 547, Loss: 0.8606584668159485, Final Batch Loss: 0.45794034004211426\n",
      "Epoch 548, Loss: 0.857599288225174, Final Batch Loss: 0.44248563051223755\n",
      "Epoch 549, Loss: 0.8047440052032471, Final Batch Loss: 0.4069914221763611\n",
      "Epoch 550, Loss: 0.8000532388687134, Final Batch Loss: 0.4113011658191681\n",
      "Epoch 551, Loss: 0.8845073580741882, Final Batch Loss: 0.44944679737091064\n",
      "Epoch 552, Loss: 0.8403412699699402, Final Batch Loss: 0.3829898536205292\n",
      "Epoch 553, Loss: 0.8450164496898651, Final Batch Loss: 0.45946285128593445\n",
      "Epoch 554, Loss: 0.7847025394439697, Final Batch Loss: 0.3766089677810669\n",
      "Epoch 555, Loss: 0.8393718004226685, Final Batch Loss: 0.4120202660560608\n",
      "Epoch 556, Loss: 0.8265426456928253, Final Batch Loss: 0.4015628695487976\n",
      "Epoch 557, Loss: 0.7702475190162659, Final Batch Loss: 0.37370598316192627\n",
      "Epoch 558, Loss: 0.8270116150379181, Final Batch Loss: 0.38790738582611084\n",
      "Epoch 559, Loss: 0.7932741940021515, Final Batch Loss: 0.40219730138778687\n",
      "Epoch 560, Loss: 0.7753141224384308, Final Batch Loss: 0.34976956248283386\n",
      "Epoch 561, Loss: 0.7699562311172485, Final Batch Loss: 0.3630058765411377\n",
      "Epoch 562, Loss: 0.8657980561256409, Final Batch Loss: 0.43339020013809204\n",
      "Epoch 563, Loss: 0.8693385720252991, Final Batch Loss: 0.49140051007270813\n",
      "Epoch 564, Loss: 0.9238405227661133, Final Batch Loss: 0.4968229830265045\n",
      "Epoch 565, Loss: 0.7520323097705841, Final Batch Loss: 0.3657072186470032\n",
      "Epoch 566, Loss: 0.8185066282749176, Final Batch Loss: 0.3727317452430725\n",
      "Epoch 567, Loss: 0.8135164380073547, Final Batch Loss: 0.4180350601673126\n",
      "Epoch 568, Loss: 0.819473922252655, Final Batch Loss: 0.396333247423172\n",
      "Epoch 569, Loss: 0.8018753528594971, Final Batch Loss: 0.3876562714576721\n",
      "Epoch 570, Loss: 0.8177953958511353, Final Batch Loss: 0.40307003259658813\n",
      "Epoch 571, Loss: 0.8061460256576538, Final Batch Loss: 0.3834873139858246\n",
      "Epoch 572, Loss: 0.8575682044029236, Final Batch Loss: 0.4469209611415863\n",
      "Epoch 573, Loss: 0.8557525277137756, Final Batch Loss: 0.4114476144313812\n",
      "Epoch 574, Loss: 0.7768515050411224, Final Batch Loss: 0.38275161385536194\n",
      "Epoch 575, Loss: 0.8225989937782288, Final Batch Loss: 0.44032150506973267\n",
      "Epoch 576, Loss: 0.8098300695419312, Final Batch Loss: 0.37110230326652527\n",
      "Epoch 577, Loss: 0.9041662216186523, Final Batch Loss: 0.4697452187538147\n",
      "Epoch 578, Loss: 0.823520690202713, Final Batch Loss: 0.4697759449481964\n",
      "Epoch 579, Loss: 0.8375332653522491, Final Batch Loss: 0.40262386202812195\n",
      "Epoch 580, Loss: 0.8712275922298431, Final Batch Loss: 0.48689204454421997\n",
      "Epoch 581, Loss: 0.824618399143219, Final Batch Loss: 0.38682821393013\n",
      "Epoch 582, Loss: 0.8114787042140961, Final Batch Loss: 0.3842613995075226\n",
      "Epoch 583, Loss: 0.7964269816875458, Final Batch Loss: 0.3999069631099701\n",
      "Epoch 584, Loss: 0.7982351183891296, Final Batch Loss: 0.4238569736480713\n",
      "Epoch 585, Loss: 0.8581809997558594, Final Batch Loss: 0.47409069538116455\n",
      "Epoch 586, Loss: 0.7846916615962982, Final Batch Loss: 0.3589896559715271\n",
      "Epoch 587, Loss: 0.798009842634201, Final Batch Loss: 0.37665632367134094\n",
      "Epoch 588, Loss: 0.8238137066364288, Final Batch Loss: 0.44802185893058777\n",
      "Epoch 589, Loss: 0.8606784641742706, Final Batch Loss: 0.40523117780685425\n",
      "Epoch 590, Loss: 0.7811433374881744, Final Batch Loss: 0.37488171458244324\n",
      "Epoch 591, Loss: 0.7399624586105347, Final Batch Loss: 0.33355051279067993\n",
      "Epoch 592, Loss: 0.8112569153308868, Final Batch Loss: 0.37498241662979126\n",
      "Epoch 593, Loss: 0.8131045997142792, Final Batch Loss: 0.31116917729377747\n",
      "Epoch 594, Loss: 0.891445130109787, Final Batch Loss: 0.48904016613960266\n",
      "Epoch 595, Loss: 0.8191471099853516, Final Batch Loss: 0.36016297340393066\n",
      "Epoch 596, Loss: 0.8203748762607574, Final Batch Loss: 0.45356303453445435\n",
      "Epoch 597, Loss: 0.8432513475418091, Final Batch Loss: 0.4444062411785126\n",
      "Epoch 598, Loss: 0.7603369653224945, Final Batch Loss: 0.3772394061088562\n",
      "Epoch 599, Loss: 0.81293985247612, Final Batch Loss: 0.4277445673942566\n",
      "Epoch 600, Loss: 0.8342021703720093, Final Batch Loss: 0.384086936712265\n",
      "Epoch 601, Loss: 0.7665724158287048, Final Batch Loss: 0.3455011546611786\n",
      "Epoch 602, Loss: 0.8234237134456635, Final Batch Loss: 0.3530559539794922\n",
      "Epoch 603, Loss: 0.7895522117614746, Final Batch Loss: 0.39446821808815\n",
      "Epoch 604, Loss: 0.884853333234787, Final Batch Loss: 0.4675471782684326\n",
      "Epoch 605, Loss: 0.7985550761222839, Final Batch Loss: 0.4165520668029785\n",
      "Epoch 606, Loss: 0.7829363942146301, Final Batch Loss: 0.38452276587486267\n",
      "Epoch 607, Loss: 0.8678387701511383, Final Batch Loss: 0.43446898460388184\n",
      "Epoch 608, Loss: 0.8467158079147339, Final Batch Loss: 0.4408755600452423\n",
      "Epoch 609, Loss: 0.8269868791103363, Final Batch Loss: 0.39587241411209106\n",
      "Epoch 610, Loss: 0.8230013251304626, Final Batch Loss: 0.44185543060302734\n",
      "Epoch 611, Loss: 0.7841683626174927, Final Batch Loss: 0.35309574007987976\n",
      "Epoch 612, Loss: 0.880033016204834, Final Batch Loss: 0.4306565523147583\n",
      "Epoch 613, Loss: 0.809339851140976, Final Batch Loss: 0.4390382766723633\n",
      "Epoch 614, Loss: 0.8111407458782196, Final Batch Loss: 0.3463999927043915\n",
      "Epoch 615, Loss: 0.8059386312961578, Final Batch Loss: 0.4664149582386017\n",
      "Epoch 616, Loss: 0.7864554226398468, Final Batch Loss: 0.42224013805389404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617, Loss: 0.7919317483901978, Final Batch Loss: 0.44741883873939514\n",
      "Epoch 618, Loss: 0.7514171004295349, Final Batch Loss: 0.3329709768295288\n",
      "Epoch 619, Loss: 0.8035233020782471, Final Batch Loss: 0.3886737823486328\n",
      "Epoch 620, Loss: 0.8048924207687378, Final Batch Loss: 0.3677983582019806\n",
      "Epoch 621, Loss: 0.844156414270401, Final Batch Loss: 0.47586789727211\n",
      "Epoch 622, Loss: 0.8567602038383484, Final Batch Loss: 0.4759347140789032\n",
      "Epoch 623, Loss: 0.807996392250061, Final Batch Loss: 0.4019576609134674\n",
      "Epoch 624, Loss: 0.7982386946678162, Final Batch Loss: 0.38517677783966064\n",
      "Epoch 625, Loss: 0.81346395611763, Final Batch Loss: 0.4017103910446167\n",
      "Epoch 626, Loss: 0.7584227621555328, Final Batch Loss: 0.3392518162727356\n",
      "Epoch 627, Loss: 0.8559302389621735, Final Batch Loss: 0.4868301749229431\n",
      "Epoch 628, Loss: 0.8274253308773041, Final Batch Loss: 0.45438265800476074\n",
      "Epoch 629, Loss: 0.825180321931839, Final Batch Loss: 0.4425080716609955\n",
      "Epoch 630, Loss: 0.7823302149772644, Final Batch Loss: 0.32880890369415283\n",
      "Epoch 631, Loss: 0.8081687092781067, Final Batch Loss: 0.403085857629776\n",
      "Epoch 632, Loss: 0.8104506731033325, Final Batch Loss: 0.4192951023578644\n",
      "Epoch 633, Loss: 0.8067978620529175, Final Batch Loss: 0.3956282436847687\n",
      "Epoch 634, Loss: 0.8219075202941895, Final Batch Loss: 0.45212092995643616\n",
      "Epoch 635, Loss: 0.7716362476348877, Final Batch Loss: 0.3380162715911865\n",
      "Epoch 636, Loss: 0.7685687839984894, Final Batch Loss: 0.36853307485580444\n",
      "Epoch 637, Loss: 0.7953871488571167, Final Batch Loss: 0.39575377106666565\n",
      "Epoch 638, Loss: 0.8109578192234039, Final Batch Loss: 0.4374838173389435\n",
      "Epoch 639, Loss: 0.8072430491447449, Final Batch Loss: 0.4039153754711151\n",
      "Epoch 640, Loss: 0.805329829454422, Final Batch Loss: 0.4027504324913025\n",
      "Epoch 641, Loss: 0.8144986033439636, Final Batch Loss: 0.4389131963253021\n",
      "Epoch 642, Loss: 0.8751107752323151, Final Batch Loss: 0.4447132349014282\n",
      "Epoch 643, Loss: 0.7855669260025024, Final Batch Loss: 0.37326931953430176\n",
      "Epoch 644, Loss: 0.8476693630218506, Final Batch Loss: 0.4457499086856842\n",
      "Epoch 645, Loss: 0.7555763423442841, Final Batch Loss: 0.35353031754493713\n",
      "Epoch 646, Loss: 0.7695590853691101, Final Batch Loss: 0.40403178334236145\n",
      "Epoch 647, Loss: 0.8105064034461975, Final Batch Loss: 0.3986492156982422\n",
      "Epoch 648, Loss: 0.848644345998764, Final Batch Loss: 0.41059818863868713\n",
      "Epoch 649, Loss: 0.7458034455776215, Final Batch Loss: 0.3730678856372833\n",
      "Epoch 650, Loss: 0.7769909203052521, Final Batch Loss: 0.3726513385772705\n",
      "Epoch 651, Loss: 0.8047964870929718, Final Batch Loss: 0.4119638204574585\n",
      "Epoch 652, Loss: 0.776020348072052, Final Batch Loss: 0.3574412167072296\n",
      "Epoch 653, Loss: 0.7610067129135132, Final Batch Loss: 0.3855607807636261\n",
      "Epoch 654, Loss: 0.7522571682929993, Final Batch Loss: 0.39075967669487\n",
      "Epoch 655, Loss: 0.7901043593883514, Final Batch Loss: 0.4047435522079468\n",
      "Epoch 656, Loss: 0.776847630739212, Final Batch Loss: 0.383535236120224\n",
      "Epoch 657, Loss: 0.7964785993099213, Final Batch Loss: 0.3882916271686554\n",
      "Epoch 658, Loss: 0.7762840390205383, Final Batch Loss: 0.3866596519947052\n",
      "Epoch 659, Loss: 0.8080329895019531, Final Batch Loss: 0.39494606852531433\n",
      "Epoch 660, Loss: 0.7792574167251587, Final Batch Loss: 0.3917272090911865\n",
      "Epoch 661, Loss: 0.7884421646595001, Final Batch Loss: 0.3681790232658386\n",
      "Epoch 662, Loss: 0.7639392018318176, Final Batch Loss: 0.36422789096832275\n",
      "Epoch 663, Loss: 0.7310236990451813, Final Batch Loss: 0.32633140683174133\n",
      "Epoch 664, Loss: 0.8221573829650879, Final Batch Loss: 0.42468103766441345\n",
      "Epoch 665, Loss: 0.7579585611820221, Final Batch Loss: 0.3993135690689087\n",
      "Epoch 666, Loss: 0.8047657012939453, Final Batch Loss: 0.4322342574596405\n",
      "Epoch 667, Loss: 0.7790585458278656, Final Batch Loss: 0.42466267943382263\n",
      "Epoch 668, Loss: 0.8030755817890167, Final Batch Loss: 0.44189339876174927\n",
      "Epoch 669, Loss: 0.7069305181503296, Final Batch Loss: 0.3256484866142273\n",
      "Epoch 670, Loss: 0.763600617647171, Final Batch Loss: 0.3751859664916992\n",
      "Epoch 671, Loss: 0.8034776151180267, Final Batch Loss: 0.41433101892471313\n",
      "Epoch 672, Loss: 0.836417943239212, Final Batch Loss: 0.39574190974235535\n",
      "Epoch 673, Loss: 0.7347512245178223, Final Batch Loss: 0.2644440531730652\n",
      "Epoch 674, Loss: 0.7545397877693176, Final Batch Loss: 0.3450707197189331\n",
      "Epoch 675, Loss: 0.7752659022808075, Final Batch Loss: 0.36178022623062134\n",
      "Epoch 676, Loss: 0.7470525205135345, Final Batch Loss: 0.3731595575809479\n",
      "Epoch 677, Loss: 0.7249459624290466, Final Batch Loss: 0.3346714973449707\n",
      "Epoch 678, Loss: 0.7075939476490021, Final Batch Loss: 0.2809607684612274\n",
      "Epoch 679, Loss: 0.8970125615596771, Final Batch Loss: 0.49898868799209595\n",
      "Epoch 680, Loss: 0.8156723976135254, Final Batch Loss: 0.44576069712638855\n",
      "Epoch 681, Loss: 0.7854203581809998, Final Batch Loss: 0.37832966446876526\n",
      "Epoch 682, Loss: 0.7511418759822845, Final Batch Loss: 0.39904358983039856\n",
      "Epoch 683, Loss: 0.7552435994148254, Final Batch Loss: 0.37552034854888916\n",
      "Epoch 684, Loss: 0.7564527094364166, Final Batch Loss: 0.4027186632156372\n",
      "Epoch 685, Loss: 0.7691937983036041, Final Batch Loss: 0.38398030400276184\n",
      "Epoch 686, Loss: 0.7804728448390961, Final Batch Loss: 0.4203040301799774\n",
      "Epoch 687, Loss: 0.7651416063308716, Final Batch Loss: 0.3821508288383484\n",
      "Epoch 688, Loss: 0.7663452625274658, Final Batch Loss: 0.37522271275520325\n",
      "Epoch 689, Loss: 0.7623467147350311, Final Batch Loss: 0.3951663374900818\n",
      "Epoch 690, Loss: 0.7766956388950348, Final Batch Loss: 0.3710485100746155\n",
      "Epoch 691, Loss: 0.8034348785877228, Final Batch Loss: 0.3444391191005707\n",
      "Epoch 692, Loss: 0.7228462398052216, Final Batch Loss: 0.348685622215271\n",
      "Epoch 693, Loss: 0.7781389951705933, Final Batch Loss: 0.43592551350593567\n",
      "Epoch 694, Loss: 0.7623299360275269, Final Batch Loss: 0.31652769446372986\n",
      "Epoch 695, Loss: 0.8779540061950684, Final Batch Loss: 0.4862017035484314\n",
      "Epoch 696, Loss: 0.8315569460391998, Final Batch Loss: 0.37208378314971924\n",
      "Epoch 697, Loss: 0.8053647577762604, Final Batch Loss: 0.41275477409362793\n",
      "Epoch 698, Loss: 0.7290359735488892, Final Batch Loss: 0.3168501555919647\n",
      "Epoch 699, Loss: 0.7848971486091614, Final Batch Loss: 0.35953134298324585\n",
      "Epoch 700, Loss: 0.7187944650650024, Final Batch Loss: 0.31159117817878723\n",
      "Epoch 701, Loss: 0.8465743064880371, Final Batch Loss: 0.4646224081516266\n",
      "Epoch 702, Loss: 0.790621429681778, Final Batch Loss: 0.4622761309146881\n",
      "Epoch 703, Loss: 0.7170791327953339, Final Batch Loss: 0.32920244336128235\n",
      "Epoch 704, Loss: 0.7825252115726471, Final Batch Loss: 0.365816593170166\n",
      "Epoch 705, Loss: 0.7313065230846405, Final Batch Loss: 0.36839401721954346\n",
      "Epoch 706, Loss: 0.8639082610607147, Final Batch Loss: 0.49861273169517517\n",
      "Epoch 707, Loss: 0.7273285686969757, Final Batch Loss: 0.29878056049346924\n",
      "Epoch 708, Loss: 0.7935118675231934, Final Batch Loss: 0.4446461498737335\n",
      "Epoch 709, Loss: 0.7771759927272797, Final Batch Loss: 0.3601004183292389\n",
      "Epoch 710, Loss: 0.7666785717010498, Final Batch Loss: 0.38525819778442383\n",
      "Epoch 711, Loss: 0.7580417096614838, Final Batch Loss: 0.34762468934059143\n",
      "Epoch 712, Loss: 0.7956584990024567, Final Batch Loss: 0.3992845416069031\n",
      "Epoch 713, Loss: 0.7524499595165253, Final Batch Loss: 0.38747090101242065\n",
      "Epoch 714, Loss: 0.7210246026515961, Final Batch Loss: 0.37614476680755615\n",
      "Epoch 715, Loss: 0.794127881526947, Final Batch Loss: 0.4308246374130249\n",
      "Epoch 716, Loss: 0.8182478249073029, Final Batch Loss: 0.4033384621143341\n",
      "Epoch 717, Loss: 0.7613949179649353, Final Batch Loss: 0.3749080300331116\n",
      "Epoch 718, Loss: 0.7533462941646576, Final Batch Loss: 0.37696579098701477\n",
      "Epoch 719, Loss: 0.7475655674934387, Final Batch Loss: 0.3386831283569336\n",
      "Epoch 720, Loss: 0.7950780391693115, Final Batch Loss: 0.4117807447910309\n",
      "Epoch 721, Loss: 0.7443303465843201, Final Batch Loss: 0.33656564354896545\n",
      "Epoch 722, Loss: 0.7374939024448395, Final Batch Loss: 0.3589843213558197\n",
      "Epoch 723, Loss: 0.7526146173477173, Final Batch Loss: 0.3832486867904663\n",
      "Epoch 724, Loss: 0.7787182331085205, Final Batch Loss: 0.4258992671966553\n",
      "Epoch 725, Loss: 0.9008801281452179, Final Batch Loss: 0.4483593702316284\n",
      "Epoch 726, Loss: 0.7018064260482788, Final Batch Loss: 0.32514509558677673\n",
      "Epoch 727, Loss: 0.7717751860618591, Final Batch Loss: 0.33421599864959717\n",
      "Epoch 728, Loss: 0.7346431314945221, Final Batch Loss: 0.3409343361854553\n",
      "Epoch 729, Loss: 0.6943783760070801, Final Batch Loss: 0.29842716455459595\n",
      "Epoch 730, Loss: 0.7529639601707458, Final Batch Loss: 0.3706642985343933\n",
      "Epoch 731, Loss: 0.7392005920410156, Final Batch Loss: 0.3681870102882385\n",
      "Epoch 732, Loss: 0.7969210147857666, Final Batch Loss: 0.3880033493041992\n",
      "Epoch 733, Loss: 0.8068006932735443, Final Batch Loss: 0.4520880877971649\n",
      "Epoch 734, Loss: 0.7484112977981567, Final Batch Loss: 0.2973766326904297\n",
      "Epoch 735, Loss: 0.8203501999378204, Final Batch Loss: 0.43162184953689575\n",
      "Epoch 736, Loss: 0.7713072001934052, Final Batch Loss: 0.4144253134727478\n",
      "Epoch 737, Loss: 0.7456362843513489, Final Batch Loss: 0.3014019727706909\n",
      "Epoch 738, Loss: 0.8098124861717224, Final Batch Loss: 0.3995214104652405\n",
      "Epoch 739, Loss: 0.743820309638977, Final Batch Loss: 0.32902202010154724\n",
      "Epoch 740, Loss: 0.7771659791469574, Final Batch Loss: 0.36024901270866394\n",
      "Epoch 741, Loss: 0.7814410924911499, Final Batch Loss: 0.39624083042144775\n",
      "Epoch 742, Loss: 0.754453033208847, Final Batch Loss: 0.3848404288291931\n",
      "Epoch 743, Loss: 0.743366926908493, Final Batch Loss: 0.3349396586418152\n",
      "Epoch 744, Loss: 0.7199512422084808, Final Batch Loss: 0.31379589438438416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 745, Loss: 0.7609599828720093, Final Batch Loss: 0.35450172424316406\n",
      "Epoch 746, Loss: 0.769745260477066, Final Batch Loss: 0.4312070310115814\n",
      "Epoch 747, Loss: 0.7731520533561707, Final Batch Loss: 0.3842511773109436\n",
      "Epoch 748, Loss: 0.7242460250854492, Final Batch Loss: 0.3515700101852417\n",
      "Epoch 749, Loss: 0.7528943419456482, Final Batch Loss: 0.36238110065460205\n",
      "Epoch 750, Loss: 0.7402602136135101, Final Batch Loss: 0.3820503354072571\n",
      "Epoch 751, Loss: 0.7248794734477997, Final Batch Loss: 0.38495394587516785\n",
      "Epoch 752, Loss: 0.8115681707859039, Final Batch Loss: 0.42504727840423584\n",
      "Epoch 753, Loss: 0.762953132390976, Final Batch Loss: 0.3759625256061554\n",
      "Epoch 754, Loss: 0.7006478309631348, Final Batch Loss: 0.3610050678253174\n",
      "Epoch 755, Loss: 0.7963029742240906, Final Batch Loss: 0.31627678871154785\n",
      "Epoch 756, Loss: 0.7429707050323486, Final Batch Loss: 0.3824307918548584\n",
      "Epoch 757, Loss: 0.7689498960971832, Final Batch Loss: 0.3655736744403839\n",
      "Epoch 758, Loss: 0.7996677756309509, Final Batch Loss: 0.40083733201026917\n",
      "Epoch 759, Loss: 0.7620576918125153, Final Batch Loss: 0.36233067512512207\n",
      "Epoch 760, Loss: 0.766951858997345, Final Batch Loss: 0.39920371770858765\n",
      "Epoch 761, Loss: 0.7751104235649109, Final Batch Loss: 0.3805689811706543\n",
      "Epoch 762, Loss: 0.7433034479618073, Final Batch Loss: 0.355953186750412\n",
      "Epoch 763, Loss: 0.7713747024536133, Final Batch Loss: 0.3791966736316681\n",
      "Epoch 764, Loss: 0.787136971950531, Final Batch Loss: 0.42627397179603577\n",
      "Epoch 765, Loss: 0.8262460231781006, Final Batch Loss: 0.4443342983722687\n",
      "Epoch 766, Loss: 0.8541443049907684, Final Batch Loss: 0.48191216588020325\n",
      "Epoch 767, Loss: 0.6924197375774384, Final Batch Loss: 0.3175657093524933\n",
      "Epoch 768, Loss: 0.7279636263847351, Final Batch Loss: 0.38369810581207275\n",
      "Epoch 769, Loss: 0.7938141524791718, Final Batch Loss: 0.4389616549015045\n",
      "Epoch 770, Loss: 0.7782206535339355, Final Batch Loss: 0.3748418688774109\n",
      "Epoch 771, Loss: 0.6764034628868103, Final Batch Loss: 0.2945775091648102\n",
      "Epoch 772, Loss: 0.7796498537063599, Final Batch Loss: 0.3121293783187866\n",
      "Epoch 773, Loss: 0.7158515155315399, Final Batch Loss: 0.34914273023605347\n",
      "Epoch 774, Loss: 0.7725700438022614, Final Batch Loss: 0.42370375990867615\n",
      "Epoch 775, Loss: 0.7147785723209381, Final Batch Loss: 0.36060774326324463\n",
      "Epoch 776, Loss: 0.752772867679596, Final Batch Loss: 0.4059675931930542\n",
      "Epoch 777, Loss: 0.7671299874782562, Final Batch Loss: 0.40070489048957825\n",
      "Epoch 778, Loss: 0.7434956133365631, Final Batch Loss: 0.320618212223053\n",
      "Epoch 779, Loss: 0.703860342502594, Final Batch Loss: 0.29083681106567383\n",
      "Epoch 780, Loss: 0.7161960899829865, Final Batch Loss: 0.3465835750102997\n",
      "Epoch 781, Loss: 0.8080271780490875, Final Batch Loss: 0.4365918040275574\n",
      "Epoch 782, Loss: 0.8417371511459351, Final Batch Loss: 0.448711633682251\n",
      "Epoch 783, Loss: 0.7629391252994537, Final Batch Loss: 0.3364258408546448\n",
      "Epoch 784, Loss: 0.7107014060020447, Final Batch Loss: 0.36115026473999023\n",
      "Epoch 785, Loss: 0.6923122107982635, Final Batch Loss: 0.33760714530944824\n",
      "Epoch 786, Loss: 0.7421858608722687, Final Batch Loss: 0.4339124262332916\n",
      "Epoch 787, Loss: 0.7814933955669403, Final Batch Loss: 0.39933228492736816\n",
      "Epoch 788, Loss: 0.8682167828083038, Final Batch Loss: 0.4870697259902954\n",
      "Epoch 789, Loss: 0.6725625991821289, Final Batch Loss: 0.3134198784828186\n",
      "Epoch 790, Loss: 0.7638857960700989, Final Batch Loss: 0.33027392625808716\n",
      "Epoch 791, Loss: 0.692353367805481, Final Batch Loss: 0.2922775447368622\n",
      "Epoch 792, Loss: 0.7216329276561737, Final Batch Loss: 0.3577994108200073\n",
      "Epoch 793, Loss: 0.7241240441799164, Final Batch Loss: 0.3708898723125458\n",
      "Epoch 794, Loss: 0.7744449079036713, Final Batch Loss: 0.37786898016929626\n",
      "Epoch 795, Loss: 0.7832231223583221, Final Batch Loss: 0.36938348412513733\n",
      "Epoch 796, Loss: 0.7239224910736084, Final Batch Loss: 0.36806702613830566\n",
      "Epoch 797, Loss: 0.7432125806808472, Final Batch Loss: 0.379623144865036\n",
      "Epoch 798, Loss: 0.7456806302070618, Final Batch Loss: 0.3493989408016205\n",
      "Epoch 799, Loss: 0.8398898839950562, Final Batch Loss: 0.4242301881313324\n",
      "Epoch 800, Loss: 0.721551239490509, Final Batch Loss: 0.3805636465549469\n",
      "Epoch 801, Loss: 0.7037299871444702, Final Batch Loss: 0.37902987003326416\n",
      "Epoch 802, Loss: 0.7401549816131592, Final Batch Loss: 0.38074207305908203\n",
      "Epoch 803, Loss: 0.7581111490726471, Final Batch Loss: 0.36088764667510986\n",
      "Epoch 804, Loss: 0.7344985902309418, Final Batch Loss: 0.3424633741378784\n",
      "Epoch 805, Loss: 0.7278832197189331, Final Batch Loss: 0.3407757580280304\n",
      "Epoch 806, Loss: 0.7167781293392181, Final Batch Loss: 0.3593880236148834\n",
      "Epoch 807, Loss: 0.7613177597522736, Final Batch Loss: 0.38392364978790283\n",
      "Epoch 808, Loss: 0.7543296217918396, Final Batch Loss: 0.4050048887729645\n",
      "Epoch 809, Loss: 0.7052753865718842, Final Batch Loss: 0.35098084807395935\n",
      "Epoch 810, Loss: 0.6653617918491364, Final Batch Loss: 0.2996579110622406\n",
      "Epoch 811, Loss: 0.7590183317661285, Final Batch Loss: 0.3767617642879486\n",
      "Epoch 812, Loss: 0.7457373738288879, Final Batch Loss: 0.39695921540260315\n",
      "Epoch 813, Loss: 0.9091736674308777, Final Batch Loss: 0.4492844045162201\n",
      "Epoch 814, Loss: 0.7040661573410034, Final Batch Loss: 0.3043210804462433\n",
      "Epoch 815, Loss: 0.7978725135326385, Final Batch Loss: 0.3974629342556\n",
      "Epoch 816, Loss: 0.7805587947368622, Final Batch Loss: 0.4238620400428772\n",
      "Epoch 817, Loss: 0.7316022217273712, Final Batch Loss: 0.3516623079776764\n",
      "Epoch 818, Loss: 0.6898988485336304, Final Batch Loss: 0.3154902756214142\n",
      "Epoch 819, Loss: 0.7089793086051941, Final Batch Loss: 0.3741356134414673\n",
      "Epoch 820, Loss: 0.7056629955768585, Final Batch Loss: 0.3495230972766876\n",
      "Epoch 821, Loss: 0.7365858554840088, Final Batch Loss: 0.3510567247867584\n",
      "Epoch 822, Loss: 0.7688207626342773, Final Batch Loss: 0.35914841294288635\n",
      "Epoch 823, Loss: 0.7410883009433746, Final Batch Loss: 0.3824768364429474\n",
      "Epoch 824, Loss: 0.7173692584037781, Final Batch Loss: 0.3284136652946472\n",
      "Epoch 825, Loss: 0.7944907546043396, Final Batch Loss: 0.4739331007003784\n",
      "Epoch 826, Loss: 0.7616578936576843, Final Batch Loss: 0.4026634395122528\n",
      "Epoch 827, Loss: 0.7443022727966309, Final Batch Loss: 0.43549078702926636\n",
      "Epoch 828, Loss: 0.7215945422649384, Final Batch Loss: 0.345310777425766\n",
      "Epoch 829, Loss: 0.711186945438385, Final Batch Loss: 0.33543744683265686\n",
      "Epoch 830, Loss: 0.6889353096485138, Final Batch Loss: 0.33819714188575745\n",
      "Epoch 831, Loss: 0.7052148878574371, Final Batch Loss: 0.30681854486465454\n",
      "Epoch 832, Loss: 0.7341026961803436, Final Batch Loss: 0.3446899354457855\n",
      "Epoch 833, Loss: 0.7690367102622986, Final Batch Loss: 0.4294929504394531\n",
      "Epoch 834, Loss: 0.7843416631221771, Final Batch Loss: 0.3947097659111023\n",
      "Epoch 835, Loss: 0.8099445104598999, Final Batch Loss: 0.38270115852355957\n",
      "Epoch 836, Loss: 0.788155198097229, Final Batch Loss: 0.42289572954177856\n",
      "Epoch 837, Loss: 0.7190819382667542, Final Batch Loss: 0.3258262276649475\n",
      "Epoch 838, Loss: 0.6823188960552216, Final Batch Loss: 0.2793259024620056\n",
      "Epoch 839, Loss: 0.6810092628002167, Final Batch Loss: 0.31684067845344543\n",
      "Epoch 840, Loss: 0.7596388459205627, Final Batch Loss: 0.3792377710342407\n",
      "Epoch 841, Loss: 0.7231684923171997, Final Batch Loss: 0.3692415952682495\n",
      "Epoch 842, Loss: 0.7387564778327942, Final Batch Loss: 0.3878096640110016\n",
      "Epoch 843, Loss: 0.72550368309021, Final Batch Loss: 0.3487670421600342\n",
      "Epoch 844, Loss: 0.7218197584152222, Final Batch Loss: 0.37382468581199646\n",
      "Epoch 845, Loss: 0.75235715508461, Final Batch Loss: 0.4018898904323578\n",
      "Epoch 846, Loss: 0.7150612473487854, Final Batch Loss: 0.3400211036205292\n",
      "Epoch 847, Loss: 0.7235840260982513, Final Batch Loss: 0.40058887004852295\n",
      "Epoch 848, Loss: 0.7809170484542847, Final Batch Loss: 0.37897932529449463\n",
      "Epoch 849, Loss: 0.732174426317215, Final Batch Loss: 0.3552667498588562\n",
      "Epoch 850, Loss: 0.7080273330211639, Final Batch Loss: 0.3886568248271942\n",
      "Epoch 851, Loss: 0.723321259021759, Final Batch Loss: 0.34922856092453003\n",
      "Epoch 852, Loss: 0.6693746447563171, Final Batch Loss: 0.32828837633132935\n",
      "Epoch 853, Loss: 0.7135424017906189, Final Batch Loss: 0.3409372866153717\n",
      "Epoch 854, Loss: 0.7483408749103546, Final Batch Loss: 0.36853304505348206\n",
      "Epoch 855, Loss: 0.6962486803531647, Final Batch Loss: 0.33500248193740845\n",
      "Epoch 856, Loss: 0.7254371345043182, Final Batch Loss: 0.3496572971343994\n",
      "Epoch 857, Loss: 0.7041441798210144, Final Batch Loss: 0.3641475737094879\n",
      "Epoch 858, Loss: 0.7392560243606567, Final Batch Loss: 0.40538644790649414\n",
      "Epoch 859, Loss: 0.7822408080101013, Final Batch Loss: 0.39486199617385864\n",
      "Epoch 860, Loss: 0.6832867562770844, Final Batch Loss: 0.30901557207107544\n",
      "Epoch 861, Loss: 0.7539303600788116, Final Batch Loss: 0.3863430619239807\n",
      "Epoch 862, Loss: 0.7014228403568268, Final Batch Loss: 0.3304879367351532\n",
      "Epoch 863, Loss: 0.694220095872879, Final Batch Loss: 0.319598525762558\n",
      "Epoch 864, Loss: 0.6663536429405212, Final Batch Loss: 0.33971697092056274\n",
      "Epoch 865, Loss: 0.7848829925060272, Final Batch Loss: 0.409625768661499\n",
      "Epoch 866, Loss: 0.7075047791004181, Final Batch Loss: 0.3582356572151184\n",
      "Epoch 867, Loss: 0.7532475888729095, Final Batch Loss: 0.45425963401794434\n",
      "Epoch 868, Loss: 0.6754841506481171, Final Batch Loss: 0.31683793663978577\n",
      "Epoch 869, Loss: 0.8004376590251923, Final Batch Loss: 0.43708497285842896\n",
      "Epoch 870, Loss: 0.7407979071140289, Final Batch Loss: 0.3616154193878174\n",
      "Epoch 871, Loss: 0.8142802715301514, Final Batch Loss: 0.44224801659584045\n",
      "Epoch 872, Loss: 0.7055684626102448, Final Batch Loss: 0.33577364683151245\n",
      "Epoch 873, Loss: 0.712660014629364, Final Batch Loss: 0.36757561564445496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 874, Loss: 0.7555351853370667, Final Batch Loss: 0.33875906467437744\n",
      "Epoch 875, Loss: 0.6858914792537689, Final Batch Loss: 0.32895371317863464\n",
      "Epoch 876, Loss: 0.704905241727829, Final Batch Loss: 0.3730643689632416\n",
      "Epoch 877, Loss: 0.7372937798500061, Final Batch Loss: 0.41311487555503845\n",
      "Epoch 878, Loss: 0.7066936492919922, Final Batch Loss: 0.3642067313194275\n",
      "Epoch 879, Loss: 0.6694838106632233, Final Batch Loss: 0.31543704867362976\n",
      "Epoch 880, Loss: 0.7028918564319611, Final Batch Loss: 0.3706105947494507\n",
      "Epoch 881, Loss: 0.7246206998825073, Final Batch Loss: 0.337998628616333\n",
      "Epoch 882, Loss: 0.7254526019096375, Final Batch Loss: 0.32693883776664734\n",
      "Epoch 883, Loss: 0.6992147266864777, Final Batch Loss: 0.3319903314113617\n",
      "Epoch 884, Loss: 0.720938503742218, Final Batch Loss: 0.3180919587612152\n",
      "Epoch 885, Loss: 0.6976231336593628, Final Batch Loss: 0.32680782675743103\n",
      "Epoch 886, Loss: 0.6715425848960876, Final Batch Loss: 0.31126970052719116\n",
      "Epoch 887, Loss: 0.7279309928417206, Final Batch Loss: 0.3528291881084442\n",
      "Epoch 888, Loss: 0.7317877113819122, Final Batch Loss: 0.39138221740722656\n",
      "Epoch 889, Loss: 0.7274816632270813, Final Batch Loss: 0.3607862591743469\n",
      "Epoch 890, Loss: 0.700003057718277, Final Batch Loss: 0.35053563117980957\n",
      "Epoch 891, Loss: 0.7362717986106873, Final Batch Loss: 0.3918714225292206\n",
      "Epoch 892, Loss: 0.7934276461601257, Final Batch Loss: 0.43909212946891785\n",
      "Epoch 893, Loss: 0.7068109214305878, Final Batch Loss: 0.36878228187561035\n",
      "Epoch 894, Loss: 0.7031208872795105, Final Batch Loss: 0.3411386013031006\n",
      "Epoch 895, Loss: 0.708189457654953, Final Batch Loss: 0.40268757939338684\n",
      "Epoch 896, Loss: 0.7679638266563416, Final Batch Loss: 0.4247533977031708\n",
      "Epoch 897, Loss: 0.7237122356891632, Final Batch Loss: 0.3293626606464386\n",
      "Epoch 898, Loss: 0.7190823256969452, Final Batch Loss: 0.3953098952770233\n",
      "Epoch 899, Loss: 0.7493245601654053, Final Batch Loss: 0.35892996191978455\n",
      "Epoch 900, Loss: 0.7559756338596344, Final Batch Loss: 0.38509437441825867\n",
      "Epoch 901, Loss: 0.7353185415267944, Final Batch Loss: 0.3868662118911743\n",
      "Epoch 902, Loss: 0.6996554434299469, Final Batch Loss: 0.3352000117301941\n",
      "Epoch 903, Loss: 0.7074627578258514, Final Batch Loss: 0.34885838627815247\n",
      "Epoch 904, Loss: 0.7774748206138611, Final Batch Loss: 0.38823795318603516\n",
      "Epoch 905, Loss: 0.6915856003761292, Final Batch Loss: 0.3740527331829071\n",
      "Epoch 906, Loss: 0.7113955020904541, Final Batch Loss: 0.31743958592414856\n",
      "Epoch 907, Loss: 0.7279890775680542, Final Batch Loss: 0.393585741519928\n",
      "Epoch 908, Loss: 0.7146879136562347, Final Batch Loss: 0.40108147263526917\n",
      "Epoch 909, Loss: 0.7030309438705444, Final Batch Loss: 0.32247796654701233\n",
      "Epoch 910, Loss: 0.6755474805831909, Final Batch Loss: 0.32351937890052795\n",
      "Epoch 911, Loss: 0.6869184672832489, Final Batch Loss: 0.34875863790512085\n",
      "Epoch 912, Loss: 0.7168430984020233, Final Batch Loss: 0.3741384744644165\n",
      "Epoch 913, Loss: 0.7096185982227325, Final Batch Loss: 0.3688841462135315\n",
      "Epoch 914, Loss: 0.6452375650405884, Final Batch Loss: 0.26437243819236755\n",
      "Epoch 915, Loss: 0.6687816679477692, Final Batch Loss: 0.35622891783714294\n",
      "Epoch 916, Loss: 0.7064357995986938, Final Batch Loss: 0.32390478253364563\n",
      "Epoch 917, Loss: 0.7084435224533081, Final Batch Loss: 0.36839228868484497\n",
      "Epoch 918, Loss: 0.7036049365997314, Final Batch Loss: 0.3366597592830658\n",
      "Epoch 919, Loss: 0.7422493994235992, Final Batch Loss: 0.3352316617965698\n",
      "Epoch 920, Loss: 0.7249996364116669, Final Batch Loss: 0.3481442928314209\n",
      "Epoch 921, Loss: 0.713208943605423, Final Batch Loss: 0.326057106256485\n",
      "Epoch 922, Loss: 0.6762300431728363, Final Batch Loss: 0.3742833733558655\n",
      "Epoch 923, Loss: 0.7087209522724152, Final Batch Loss: 0.3539069592952728\n",
      "Epoch 924, Loss: 0.7060167193412781, Final Batch Loss: 0.3896218538284302\n",
      "Epoch 925, Loss: 0.703960508108139, Final Batch Loss: 0.34355926513671875\n",
      "Epoch 926, Loss: 0.725843071937561, Final Batch Loss: 0.3852681815624237\n",
      "Epoch 927, Loss: 0.7081281244754791, Final Batch Loss: 0.3507261574268341\n",
      "Epoch 928, Loss: 0.6684516966342926, Final Batch Loss: 0.34275248646736145\n",
      "Epoch 929, Loss: 0.7147548794746399, Final Batch Loss: 0.3573741316795349\n",
      "Epoch 930, Loss: 0.7017613351345062, Final Batch Loss: 0.31593450903892517\n",
      "Epoch 931, Loss: 0.7113779783248901, Final Batch Loss: 0.34657472372055054\n",
      "Epoch 932, Loss: 0.7381748557090759, Final Batch Loss: 0.3691832721233368\n",
      "Epoch 933, Loss: 0.6883667707443237, Final Batch Loss: 0.314874529838562\n",
      "Epoch 934, Loss: 0.6474346220493317, Final Batch Loss: 0.27384355664253235\n",
      "Epoch 935, Loss: 0.6598482131958008, Final Batch Loss: 0.310692697763443\n",
      "Epoch 936, Loss: 0.7246425449848175, Final Batch Loss: 0.3684122860431671\n",
      "Epoch 937, Loss: 0.6203453540802002, Final Batch Loss: 0.27164438366889954\n",
      "Epoch 938, Loss: 0.7551678717136383, Final Batch Loss: 0.36937257647514343\n",
      "Epoch 939, Loss: 0.6533111035823822, Final Batch Loss: 0.3183228671550751\n",
      "Epoch 940, Loss: 0.6742978394031525, Final Batch Loss: 0.33187389373779297\n",
      "Epoch 941, Loss: 0.6979146003723145, Final Batch Loss: 0.32383349537849426\n",
      "Epoch 942, Loss: 0.7086429595947266, Final Batch Loss: 0.41943976283073425\n",
      "Epoch 943, Loss: 0.7362467348575592, Final Batch Loss: 0.4184013903141022\n",
      "Epoch 944, Loss: 0.727860301733017, Final Batch Loss: 0.4009169340133667\n",
      "Epoch 945, Loss: 0.7092916369438171, Final Batch Loss: 0.33008232712745667\n",
      "Epoch 946, Loss: 0.7059965431690216, Final Batch Loss: 0.36377987265586853\n",
      "Epoch 947, Loss: 0.6421903073787689, Final Batch Loss: 0.2807807922363281\n",
      "Epoch 948, Loss: 0.7436326146125793, Final Batch Loss: 0.4144604206085205\n",
      "Epoch 949, Loss: 0.7084882259368896, Final Batch Loss: 0.3618637025356293\n",
      "Epoch 950, Loss: 0.6681207716464996, Final Batch Loss: 0.34490397572517395\n",
      "Epoch 951, Loss: 0.7307710349559784, Final Batch Loss: 0.37925711274147034\n",
      "Epoch 952, Loss: 0.7203270792961121, Final Batch Loss: 0.3924241364002228\n",
      "Epoch 953, Loss: 0.7350664138793945, Final Batch Loss: 0.4121628403663635\n",
      "Epoch 954, Loss: 0.6788072288036346, Final Batch Loss: 0.3176029622554779\n",
      "Epoch 955, Loss: 0.7398984730243683, Final Batch Loss: 0.3678343594074249\n",
      "Epoch 956, Loss: 0.7059198319911957, Final Batch Loss: 0.31965720653533936\n",
      "Epoch 957, Loss: 0.7104674875736237, Final Batch Loss: 0.400717169046402\n",
      "Epoch 958, Loss: 0.6818960309028625, Final Batch Loss: 0.3167555332183838\n",
      "Epoch 959, Loss: 0.7172649502754211, Final Batch Loss: 0.3772020637989044\n",
      "Epoch 960, Loss: 0.6932713985443115, Final Batch Loss: 0.37763527035713196\n",
      "Epoch 961, Loss: 0.6586337387561798, Final Batch Loss: 0.33711734414100647\n",
      "Epoch 962, Loss: 0.7512648105621338, Final Batch Loss: 0.36805471777915955\n",
      "Epoch 963, Loss: 0.7462503910064697, Final Batch Loss: 0.3508891761302948\n",
      "Epoch 964, Loss: 0.6448170244693756, Final Batch Loss: 0.27424195408821106\n",
      "Epoch 965, Loss: 0.694934606552124, Final Batch Loss: 0.3572809100151062\n",
      "Epoch 966, Loss: 0.7135390341281891, Final Batch Loss: 0.3152952194213867\n",
      "Epoch 967, Loss: 0.6920855641365051, Final Batch Loss: 0.34221160411834717\n",
      "Epoch 968, Loss: 0.6870563626289368, Final Batch Loss: 0.34373512864112854\n",
      "Epoch 969, Loss: 0.6921364367008209, Final Batch Loss: 0.36407554149627686\n",
      "Epoch 970, Loss: 0.655387818813324, Final Batch Loss: 0.2882455587387085\n",
      "Epoch 971, Loss: 0.7114683091640472, Final Batch Loss: 0.3888668119907379\n",
      "Epoch 972, Loss: 0.6932163834571838, Final Batch Loss: 0.3020353615283966\n",
      "Epoch 973, Loss: 0.7156795859336853, Final Batch Loss: 0.36746424436569214\n",
      "Epoch 974, Loss: 0.7086259722709656, Final Batch Loss: 0.364482045173645\n",
      "Epoch 975, Loss: 0.725724071264267, Final Batch Loss: 0.3939073383808136\n",
      "Epoch 976, Loss: 0.6907266974449158, Final Batch Loss: 0.3769972324371338\n",
      "Epoch 977, Loss: 0.6246813535690308, Final Batch Loss: 0.2622670531272888\n",
      "Epoch 978, Loss: 0.6896815001964569, Final Batch Loss: 0.354105144739151\n",
      "Epoch 979, Loss: 0.6790203750133514, Final Batch Loss: 0.3320271670818329\n",
      "Epoch 980, Loss: 0.7029610872268677, Final Batch Loss: 0.34427598118782043\n",
      "Epoch 981, Loss: 0.6746907532215118, Final Batch Loss: 0.3316127061843872\n",
      "Epoch 982, Loss: 0.7263780236244202, Final Batch Loss: 0.3877512216567993\n",
      "Epoch 983, Loss: 0.7021024525165558, Final Batch Loss: 0.3765104115009308\n",
      "Epoch 984, Loss: 0.6937202513217926, Final Batch Loss: 0.31205305457115173\n",
      "Epoch 985, Loss: 0.7240611910820007, Final Batch Loss: 0.360762357711792\n",
      "Epoch 986, Loss: 0.7238172292709351, Final Batch Loss: 0.3702475428581238\n",
      "Epoch 987, Loss: 0.6823172867298126, Final Batch Loss: 0.35893797874450684\n",
      "Epoch 988, Loss: 0.7063312828540802, Final Batch Loss: 0.35647687315940857\n",
      "Epoch 989, Loss: 0.6729295551776886, Final Batch Loss: 0.3611276149749756\n",
      "Epoch 990, Loss: 0.7348567247390747, Final Batch Loss: 0.39718759059906006\n",
      "Epoch 991, Loss: 0.7105729877948761, Final Batch Loss: 0.3446695804595947\n",
      "Epoch 992, Loss: 0.6955136954784393, Final Batch Loss: 0.3647896945476532\n",
      "Epoch 993, Loss: 0.7297312915325165, Final Batch Loss: 0.3843468129634857\n",
      "Epoch 994, Loss: 0.7576828300952911, Final Batch Loss: 0.4434281587600708\n",
      "Epoch 995, Loss: 0.6462089419364929, Final Batch Loss: 0.3210010528564453\n",
      "Epoch 996, Loss: 0.6375346183776855, Final Batch Loss: 0.28688696026802063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 997, Loss: 0.6989153623580933, Final Batch Loss: 0.308536171913147\n",
      "Epoch 998, Loss: 0.7320312261581421, Final Batch Loss: 0.39330944418907166\n",
      "Epoch 999, Loss: 0.7139775454998016, Final Batch Loss: 0.3469749987125397\n",
      "Epoch 1000, Loss: 0.7434061765670776, Final Batch Loss: 0.4451986849308014\n",
      "Epoch 1001, Loss: 0.6915625035762787, Final Batch Loss: 0.35356804728507996\n",
      "Epoch 1002, Loss: 0.6892466247081757, Final Batch Loss: 0.3697410821914673\n",
      "Epoch 1003, Loss: 0.6833967864513397, Final Batch Loss: 0.3640322983264923\n",
      "Epoch 1004, Loss: 0.6814523339271545, Final Batch Loss: 0.3621789515018463\n",
      "Epoch 1005, Loss: 0.7477120757102966, Final Batch Loss: 0.37007254362106323\n",
      "Epoch 1006, Loss: 0.7004427909851074, Final Batch Loss: 0.3102808892726898\n",
      "Epoch 1007, Loss: 0.7114290297031403, Final Batch Loss: 0.3715290129184723\n",
      "Epoch 1008, Loss: 0.6367690563201904, Final Batch Loss: 0.3196122646331787\n",
      "Epoch 1009, Loss: 0.7816676199436188, Final Batch Loss: 0.44092053174972534\n",
      "Epoch 1010, Loss: 0.6653136014938354, Final Batch Loss: 0.3419176936149597\n",
      "Epoch 1011, Loss: 0.6615158021450043, Final Batch Loss: 0.3423413932323456\n",
      "Epoch 1012, Loss: 0.6813164055347443, Final Batch Loss: 0.3261323869228363\n",
      "Epoch 1013, Loss: 0.6630622744560242, Final Batch Loss: 0.3248383700847626\n",
      "Epoch 1014, Loss: 0.6918177008628845, Final Batch Loss: 0.35264986753463745\n",
      "Epoch 1015, Loss: 0.6586707532405853, Final Batch Loss: 0.3305453360080719\n",
      "Epoch 1016, Loss: 0.6512205898761749, Final Batch Loss: 0.3101561963558197\n",
      "Epoch 1017, Loss: 0.6540082395076752, Final Batch Loss: 0.31721192598342896\n",
      "Epoch 1018, Loss: 0.6170297563076019, Final Batch Loss: 0.2819857597351074\n",
      "Epoch 1019, Loss: 0.6156825423240662, Final Batch Loss: 0.2673880457878113\n",
      "Epoch 1020, Loss: 0.7040974199771881, Final Batch Loss: 0.40101033449172974\n",
      "Epoch 1021, Loss: 0.7162053883075714, Final Batch Loss: 0.36247000098228455\n",
      "Epoch 1022, Loss: 0.6905215978622437, Final Batch Loss: 0.33145633339881897\n",
      "Epoch 1023, Loss: 0.6750886142253876, Final Batch Loss: 0.307718962430954\n",
      "Epoch 1024, Loss: 0.6795783936977386, Final Batch Loss: 0.2978604733943939\n",
      "Epoch 1025, Loss: 0.6813717484474182, Final Batch Loss: 0.34493911266326904\n",
      "Epoch 1026, Loss: 0.657715916633606, Final Batch Loss: 0.3141039311885834\n",
      "Epoch 1027, Loss: 0.6314135193824768, Final Batch Loss: 0.27947989106178284\n",
      "Epoch 1028, Loss: 0.6753293573856354, Final Batch Loss: 0.3102837800979614\n",
      "Epoch 1029, Loss: 0.7114241421222687, Final Batch Loss: 0.2665099501609802\n",
      "Epoch 1030, Loss: 0.7274165749549866, Final Batch Loss: 0.4014751613140106\n",
      "Epoch 1031, Loss: 0.7006008625030518, Final Batch Loss: 0.3404478132724762\n",
      "Epoch 1032, Loss: 0.6818550229072571, Final Batch Loss: 0.3541031777858734\n",
      "Epoch 1033, Loss: 0.6549676358699799, Final Batch Loss: 0.32078078389167786\n",
      "Epoch 1034, Loss: 0.6918810904026031, Final Batch Loss: 0.29603326320648193\n",
      "Epoch 1035, Loss: 0.6588937938213348, Final Batch Loss: 0.29666590690612793\n",
      "Epoch 1036, Loss: 0.7011426985263824, Final Batch Loss: 0.341094434261322\n",
      "Epoch 1037, Loss: 0.7219856679439545, Final Batch Loss: 0.38029366731643677\n",
      "Epoch 1038, Loss: 0.6686600148677826, Final Batch Loss: 0.3201720416545868\n",
      "Epoch 1039, Loss: 0.7073552906513214, Final Batch Loss: 0.3897094130516052\n",
      "Epoch 1040, Loss: 0.6086067855358124, Final Batch Loss: 0.22448638081550598\n",
      "Epoch 1041, Loss: 0.8275156021118164, Final Batch Loss: 0.552428126335144\n",
      "Epoch 1042, Loss: 0.6822715103626251, Final Batch Loss: 0.345285564661026\n",
      "Epoch 1043, Loss: 0.6062860190868378, Final Batch Loss: 0.289460152387619\n",
      "Epoch 1044, Loss: 0.63304802775383, Final Batch Loss: 0.2992488145828247\n",
      "Epoch 1045, Loss: 0.6529872119426727, Final Batch Loss: 0.3172498345375061\n",
      "Epoch 1046, Loss: 0.7631188929080963, Final Batch Loss: 0.39823469519615173\n",
      "Epoch 1047, Loss: 0.6634372472763062, Final Batch Loss: 0.32334116101264954\n",
      "Epoch 1048, Loss: 0.7691048383712769, Final Batch Loss: 0.3873382806777954\n",
      "Epoch 1049, Loss: 0.7652749121189117, Final Batch Loss: 0.4485263228416443\n",
      "Epoch 1050, Loss: 0.6693207323551178, Final Batch Loss: 0.3032873570919037\n",
      "Epoch 1051, Loss: 0.6880075037479401, Final Batch Loss: 0.3332637548446655\n",
      "Epoch 1052, Loss: 0.7362770140171051, Final Batch Loss: 0.4135717749595642\n",
      "Epoch 1053, Loss: 0.7040582001209259, Final Batch Loss: 0.3799465596675873\n",
      "Epoch 1054, Loss: 0.6809353232383728, Final Batch Loss: 0.3507603406906128\n",
      "Epoch 1055, Loss: 0.6503203511238098, Final Batch Loss: 0.3146536946296692\n",
      "Epoch 1056, Loss: 0.7632237076759338, Final Batch Loss: 0.3960421085357666\n",
      "Epoch 1057, Loss: 0.6163766384124756, Final Batch Loss: 0.28331390023231506\n",
      "Epoch 1058, Loss: 0.7143620550632477, Final Batch Loss: 0.3780886232852936\n",
      "Epoch 1059, Loss: 0.6673376858234406, Final Batch Loss: 0.3284744918346405\n",
      "Epoch 1060, Loss: 0.6957641243934631, Final Batch Loss: 0.38912659883499146\n",
      "Epoch 1061, Loss: 0.702043354511261, Final Batch Loss: 0.36658796668052673\n",
      "Epoch 1062, Loss: 0.6303189098834991, Final Batch Loss: 0.31374937295913696\n",
      "Epoch 1063, Loss: 0.6687763631343842, Final Batch Loss: 0.33571287989616394\n",
      "Epoch 1064, Loss: 0.6647930443286896, Final Batch Loss: 0.33264172077178955\n",
      "Epoch 1065, Loss: 0.6663912236690521, Final Batch Loss: 0.341080904006958\n",
      "Epoch 1066, Loss: 0.6788364946842194, Final Batch Loss: 0.32047969102859497\n",
      "Epoch 1067, Loss: 0.6967070698738098, Final Batch Loss: 0.4066888988018036\n",
      "Epoch 1068, Loss: 0.646085798740387, Final Batch Loss: 0.30914220213890076\n",
      "Epoch 1069, Loss: 0.6366493701934814, Final Batch Loss: 0.2763963043689728\n",
      "Epoch 1070, Loss: 0.648389458656311, Final Batch Loss: 0.34187763929367065\n",
      "Epoch 1071, Loss: 0.6972773373126984, Final Batch Loss: 0.3474563658237457\n",
      "Epoch 1072, Loss: 0.6305776834487915, Final Batch Loss: 0.3315744400024414\n",
      "Epoch 1073, Loss: 0.7114913761615753, Final Batch Loss: 0.35528165102005005\n",
      "Epoch 1074, Loss: 0.6975470781326294, Final Batch Loss: 0.35505539178848267\n",
      "Epoch 1075, Loss: 0.6762987375259399, Final Batch Loss: 0.34907880425453186\n",
      "Epoch 1076, Loss: 0.6977425515651703, Final Batch Loss: 0.3588433563709259\n",
      "Epoch 1077, Loss: 0.6787366271018982, Final Batch Loss: 0.3689560294151306\n",
      "Epoch 1078, Loss: 0.6421274244785309, Final Batch Loss: 0.3059077560901642\n",
      "Epoch 1079, Loss: 0.6578869521617889, Final Batch Loss: 0.34969136118888855\n",
      "Epoch 1080, Loss: 0.636585146188736, Final Batch Loss: 0.32241547107696533\n",
      "Epoch 1081, Loss: 0.6730799973011017, Final Batch Loss: 0.3643127977848053\n",
      "Epoch 1082, Loss: 0.636227548122406, Final Batch Loss: 0.3179173767566681\n",
      "Epoch 1083, Loss: 0.6209246814250946, Final Batch Loss: 0.33466729521751404\n",
      "Epoch 1084, Loss: 0.6760827898979187, Final Batch Loss: 0.32904666662216187\n",
      "Epoch 1085, Loss: 0.7219708561897278, Final Batch Loss: 0.2986151874065399\n",
      "Epoch 1086, Loss: 0.6787875592708588, Final Batch Loss: 0.37080106139183044\n",
      "Epoch 1087, Loss: 0.6472501158714294, Final Batch Loss: 0.32673680782318115\n",
      "Epoch 1088, Loss: 0.6326465904712677, Final Batch Loss: 0.3124672770500183\n",
      "Epoch 1089, Loss: 0.6809507310390472, Final Batch Loss: 0.3723931312561035\n",
      "Epoch 1090, Loss: 0.6924517452716827, Final Batch Loss: 0.31383031606674194\n",
      "Epoch 1091, Loss: 0.7034944891929626, Final Batch Loss: 0.32063937187194824\n",
      "Epoch 1092, Loss: 0.6395043134689331, Final Batch Loss: 0.33258676528930664\n",
      "Epoch 1093, Loss: 0.6507227718830109, Final Batch Loss: 0.31851381063461304\n",
      "Epoch 1094, Loss: 0.6485011577606201, Final Batch Loss: 0.32079875469207764\n",
      "Epoch 1095, Loss: 0.685401201248169, Final Batch Loss: 0.33683058619499207\n",
      "Epoch 1096, Loss: 0.6338722109794617, Final Batch Loss: 0.3134448826313019\n",
      "Epoch 1097, Loss: 0.5847715735435486, Final Batch Loss: 0.26000726222991943\n",
      "Epoch 1098, Loss: 0.6556143760681152, Final Batch Loss: 0.3271031379699707\n",
      "Epoch 1099, Loss: 0.6218633949756622, Final Batch Loss: 0.28885310888290405\n",
      "Epoch 1100, Loss: 0.6408660709857941, Final Batch Loss: 0.3478315770626068\n",
      "Epoch 1101, Loss: 0.6531359851360321, Final Batch Loss: 0.3141101598739624\n",
      "Epoch 1102, Loss: 0.572687178850174, Final Batch Loss: 0.2390385866165161\n",
      "Epoch 1103, Loss: 0.6494086384773254, Final Batch Loss: 0.2573227882385254\n",
      "Epoch 1104, Loss: 0.6718966066837311, Final Batch Loss: 0.32516366243362427\n",
      "Epoch 1105, Loss: 0.6790699660778046, Final Batch Loss: 0.3294660449028015\n",
      "Epoch 1106, Loss: 0.6449863314628601, Final Batch Loss: 0.32594722509384155\n",
      "Epoch 1107, Loss: 0.7302737534046173, Final Batch Loss: 0.3826170861721039\n",
      "Epoch 1108, Loss: 0.650833010673523, Final Batch Loss: 0.28656306862831116\n",
      "Epoch 1109, Loss: 0.6060050427913666, Final Batch Loss: 0.27096858620643616\n",
      "Epoch 1110, Loss: 0.6810990273952484, Final Batch Loss: 0.3168649971485138\n",
      "Epoch 1111, Loss: 0.6552875339984894, Final Batch Loss: 0.3210853934288025\n",
      "Epoch 1112, Loss: 0.6215398013591766, Final Batch Loss: 0.309099942445755\n",
      "Epoch 1113, Loss: 0.6616036891937256, Final Batch Loss: 0.3257085084915161\n",
      "Epoch 1114, Loss: 0.6742281019687653, Final Batch Loss: 0.31111857295036316\n",
      "Epoch 1115, Loss: 0.6996601819992065, Final Batch Loss: 0.360461562871933\n",
      "Epoch 1116, Loss: 0.6366740465164185, Final Batch Loss: 0.3686046600341797\n",
      "Epoch 1117, Loss: 0.6500493884086609, Final Batch Loss: 0.3155994415283203\n",
      "Epoch 1118, Loss: 0.6828765273094177, Final Batch Loss: 0.27933236956596375\n",
      "Epoch 1119, Loss: 0.6295359134674072, Final Batch Loss: 0.26938849687576294\n",
      "Epoch 1120, Loss: 0.6831964254379272, Final Batch Loss: 0.37233203649520874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1121, Loss: 0.6386202275753021, Final Batch Loss: 0.29207050800323486\n",
      "Epoch 1122, Loss: 0.6547160744667053, Final Batch Loss: 0.3426332175731659\n",
      "Epoch 1123, Loss: 0.6539117991924286, Final Batch Loss: 0.31534522771835327\n",
      "Epoch 1124, Loss: 0.6310699582099915, Final Batch Loss: 0.294392466545105\n",
      "Epoch 1125, Loss: 0.6565142869949341, Final Batch Loss: 0.32249391078948975\n",
      "Epoch 1126, Loss: 0.6744058430194855, Final Batch Loss: 0.3632356822490692\n",
      "Epoch 1127, Loss: 0.6651150286197662, Final Batch Loss: 0.3400048613548279\n",
      "Epoch 1128, Loss: 0.659231036901474, Final Batch Loss: 0.3365570306777954\n",
      "Epoch 1129, Loss: 0.6700442731380463, Final Batch Loss: 0.34710854291915894\n",
      "Epoch 1130, Loss: 0.6018147468566895, Final Batch Loss: 0.27949923276901245\n",
      "Epoch 1131, Loss: 0.6036454737186432, Final Batch Loss: 0.28348278999328613\n",
      "Epoch 1132, Loss: 0.6068529486656189, Final Batch Loss: 0.3134276270866394\n",
      "Epoch 1133, Loss: 0.6462826132774353, Final Batch Loss: 0.34274882078170776\n",
      "Epoch 1134, Loss: 0.6627275049686432, Final Batch Loss: 0.3649069368839264\n",
      "Epoch 1135, Loss: 0.6425667405128479, Final Batch Loss: 0.3223331570625305\n",
      "Epoch 1136, Loss: 0.6007881164550781, Final Batch Loss: 0.3132634460926056\n",
      "Epoch 1137, Loss: 0.6288751363754272, Final Batch Loss: 0.32357728481292725\n",
      "Epoch 1138, Loss: 0.6153213381767273, Final Batch Loss: 0.26105940341949463\n",
      "Epoch 1139, Loss: 0.6065019369125366, Final Batch Loss: 0.3362320065498352\n",
      "Epoch 1140, Loss: 0.5856898427009583, Final Batch Loss: 0.3173636198043823\n",
      "Epoch 1141, Loss: 0.6368953585624695, Final Batch Loss: 0.2777203619480133\n",
      "Epoch 1142, Loss: 0.6598734855651855, Final Batch Loss: 0.2782084047794342\n",
      "Epoch 1143, Loss: 0.6768107116222382, Final Batch Loss: 0.36081433296203613\n",
      "Epoch 1144, Loss: 0.7677638232707977, Final Batch Loss: 0.44467368721961975\n",
      "Epoch 1145, Loss: 0.6259193122386932, Final Batch Loss: 0.29313716292381287\n",
      "Epoch 1146, Loss: 0.6772897839546204, Final Batch Loss: 0.361549973487854\n",
      "Epoch 1147, Loss: 0.6715947687625885, Final Batch Loss: 0.3887057602405548\n",
      "Epoch 1148, Loss: 0.7633598148822784, Final Batch Loss: 0.4606148898601532\n",
      "Epoch 1149, Loss: 0.7181589901447296, Final Batch Loss: 0.393761545419693\n",
      "Epoch 1150, Loss: 0.6155162751674652, Final Batch Loss: 0.28301846981048584\n",
      "Epoch 1151, Loss: 0.6507898271083832, Final Batch Loss: 0.3364195227622986\n",
      "Epoch 1152, Loss: 0.6005865037441254, Final Batch Loss: 0.29613813757896423\n",
      "Epoch 1153, Loss: 0.6712131798267365, Final Batch Loss: 0.33057501912117004\n",
      "Epoch 1154, Loss: 0.6434162855148315, Final Batch Loss: 0.32729536294937134\n",
      "Epoch 1155, Loss: 0.6564008295536041, Final Batch Loss: 0.3402765095233917\n",
      "Epoch 1156, Loss: 0.6444195210933685, Final Batch Loss: 0.33792251348495483\n",
      "Epoch 1157, Loss: 0.6617012619972229, Final Batch Loss: 0.3370322287082672\n",
      "Epoch 1158, Loss: 0.6493688225746155, Final Batch Loss: 0.3904312252998352\n",
      "Epoch 1159, Loss: 0.619176059961319, Final Batch Loss: 0.29836443066596985\n",
      "Epoch 1160, Loss: 0.5962446928024292, Final Batch Loss: 0.29703375697135925\n",
      "Epoch 1161, Loss: 0.5967046320438385, Final Batch Loss: 0.28488942980766296\n",
      "Epoch 1162, Loss: 0.6527183055877686, Final Batch Loss: 0.3268629014492035\n",
      "Epoch 1163, Loss: 0.6278515160083771, Final Batch Loss: 0.3508061170578003\n",
      "Epoch 1164, Loss: 0.649592250585556, Final Batch Loss: 0.3336780369281769\n",
      "Epoch 1165, Loss: 0.664105236530304, Final Batch Loss: 0.39632847905158997\n",
      "Epoch 1166, Loss: 0.6357213258743286, Final Batch Loss: 0.3350315988063812\n",
      "Epoch 1167, Loss: 0.670352578163147, Final Batch Loss: 0.28461265563964844\n",
      "Epoch 1168, Loss: 0.5772485733032227, Final Batch Loss: 0.24181655049324036\n",
      "Epoch 1169, Loss: 0.6165042817592621, Final Batch Loss: 0.336417555809021\n",
      "Epoch 1170, Loss: 0.6548710763454437, Final Batch Loss: 0.3300641179084778\n",
      "Epoch 1171, Loss: 0.6126522719860077, Final Batch Loss: 0.2303662896156311\n",
      "Epoch 1172, Loss: 0.6516179144382477, Final Batch Loss: 0.3351355493068695\n",
      "Epoch 1173, Loss: 0.6021709144115448, Final Batch Loss: 0.2418389916419983\n",
      "Epoch 1174, Loss: 0.6112509369850159, Final Batch Loss: 0.26682910323143005\n",
      "Epoch 1175, Loss: 0.6578587889671326, Final Batch Loss: 0.3557628393173218\n",
      "Epoch 1176, Loss: 0.6939658224582672, Final Batch Loss: 0.33815667033195496\n",
      "Epoch 1177, Loss: 0.6233145892620087, Final Batch Loss: 0.30939266085624695\n",
      "Epoch 1178, Loss: 0.6123849749565125, Final Batch Loss: 0.3050357699394226\n",
      "Epoch 1179, Loss: 0.7193648219108582, Final Batch Loss: 0.3842194974422455\n",
      "Epoch 1180, Loss: 0.642545223236084, Final Batch Loss: 0.35763368010520935\n",
      "Epoch 1181, Loss: 0.6239176988601685, Final Batch Loss: 0.29433009028434753\n",
      "Epoch 1182, Loss: 0.6145991533994675, Final Batch Loss: 0.23385940492153168\n",
      "Epoch 1183, Loss: 0.6566881239414215, Final Batch Loss: 0.33502668142318726\n",
      "Epoch 1184, Loss: 0.6060842871665955, Final Batch Loss: 0.3003298342227936\n",
      "Epoch 1185, Loss: 0.624956488609314, Final Batch Loss: 0.2868730127811432\n",
      "Epoch 1186, Loss: 0.6386275887489319, Final Batch Loss: 0.3183966875076294\n",
      "Epoch 1187, Loss: 0.6343432366847992, Final Batch Loss: 0.3387974500656128\n",
      "Epoch 1188, Loss: 0.6185801327228546, Final Batch Loss: 0.3091531991958618\n",
      "Epoch 1189, Loss: 0.6239416301250458, Final Batch Loss: 0.32839909195899963\n",
      "Epoch 1190, Loss: 0.6501683294773102, Final Batch Loss: 0.3561418354511261\n",
      "Epoch 1191, Loss: 0.7048150300979614, Final Batch Loss: 0.3796404302120209\n",
      "Epoch 1192, Loss: 0.6855762600898743, Final Batch Loss: 0.3889809846878052\n",
      "Epoch 1193, Loss: 0.632513016462326, Final Batch Loss: 0.2862483859062195\n",
      "Epoch 1194, Loss: 0.6087087392807007, Final Batch Loss: 0.2905556261539459\n",
      "Epoch 1195, Loss: 0.6610432267189026, Final Batch Loss: 0.2797483205795288\n",
      "Epoch 1196, Loss: 0.6928453147411346, Final Batch Loss: 0.3497794270515442\n",
      "Epoch 1197, Loss: 0.6307081580162048, Final Batch Loss: 0.30100947618484497\n",
      "Epoch 1198, Loss: 0.605574756860733, Final Batch Loss: 0.31452080607414246\n",
      "Epoch 1199, Loss: 0.5886895507574081, Final Batch Loss: 0.24158330261707306\n",
      "Epoch 1200, Loss: 0.6668260395526886, Final Batch Loss: 0.3659471571445465\n",
      "Epoch 1201, Loss: 0.5849878489971161, Final Batch Loss: 0.27637115120887756\n",
      "Epoch 1202, Loss: 0.6822032630443573, Final Batch Loss: 0.3412899672985077\n",
      "Epoch 1203, Loss: 0.68165323138237, Final Batch Loss: 0.39010176062583923\n",
      "Epoch 1204, Loss: 0.6213889122009277, Final Batch Loss: 0.3024410307407379\n",
      "Epoch 1205, Loss: 0.6622270345687866, Final Batch Loss: 0.3487134575843811\n",
      "Epoch 1206, Loss: 0.6623378098011017, Final Batch Loss: 0.29972296953201294\n",
      "Epoch 1207, Loss: 0.6536164283752441, Final Batch Loss: 0.3575509488582611\n",
      "Epoch 1208, Loss: 0.6219976842403412, Final Batch Loss: 0.295459508895874\n",
      "Epoch 1209, Loss: 0.6332345902919769, Final Batch Loss: 0.32221147418022156\n",
      "Epoch 1210, Loss: 0.5837529003620148, Final Batch Loss: 0.27649274468421936\n",
      "Epoch 1211, Loss: 0.6203082501888275, Final Batch Loss: 0.32611212134361267\n",
      "Epoch 1212, Loss: 0.5916524529457092, Final Batch Loss: 0.27091264724731445\n",
      "Epoch 1213, Loss: 0.6262633502483368, Final Batch Loss: 0.3097507655620575\n",
      "Epoch 1214, Loss: 0.6999041140079498, Final Batch Loss: 0.4165975749492645\n",
      "Epoch 1215, Loss: 0.6441869735717773, Final Batch Loss: 0.324770987033844\n",
      "Epoch 1216, Loss: 0.6100903153419495, Final Batch Loss: 0.27735909819602966\n",
      "Epoch 1217, Loss: 0.6674274504184723, Final Batch Loss: 0.3692776560783386\n",
      "Epoch 1218, Loss: 0.6413851082324982, Final Batch Loss: 0.33111634850502014\n",
      "Epoch 1219, Loss: 0.6458514034748077, Final Batch Loss: 0.3598027527332306\n",
      "Epoch 1220, Loss: 0.7163129150867462, Final Batch Loss: 0.3987920582294464\n",
      "Epoch 1221, Loss: 0.6088533699512482, Final Batch Loss: 0.31615033745765686\n",
      "Epoch 1222, Loss: 0.6939832270145416, Final Batch Loss: 0.38321542739868164\n",
      "Epoch 1223, Loss: 0.6098975539207458, Final Batch Loss: 0.3368552029132843\n",
      "Epoch 1224, Loss: 0.6016746163368225, Final Batch Loss: 0.2633860111236572\n",
      "Epoch 1225, Loss: 0.6456064581871033, Final Batch Loss: 0.32390105724334717\n",
      "Epoch 1226, Loss: 0.6130775809288025, Final Batch Loss: 0.3147234320640564\n",
      "Epoch 1227, Loss: 0.6129039227962494, Final Batch Loss: 0.3396417200565338\n",
      "Epoch 1228, Loss: 0.6108594238758087, Final Batch Loss: 0.2907014787197113\n",
      "Epoch 1229, Loss: 0.6604287326335907, Final Batch Loss: 0.3242742419242859\n",
      "Epoch 1230, Loss: 0.6390265226364136, Final Batch Loss: 0.3019317388534546\n",
      "Epoch 1231, Loss: 0.6113077104091644, Final Batch Loss: 0.3206770718097687\n",
      "Epoch 1232, Loss: 0.6085641980171204, Final Batch Loss: 0.31599339842796326\n",
      "Epoch 1233, Loss: 0.6298367381095886, Final Batch Loss: 0.3042249381542206\n",
      "Epoch 1234, Loss: 0.6202155947685242, Final Batch Loss: 0.3138541877269745\n",
      "Epoch 1235, Loss: 0.6163444221019745, Final Batch Loss: 0.3396007716655731\n",
      "Epoch 1236, Loss: 0.6497222185134888, Final Batch Loss: 0.2814740836620331\n",
      "Epoch 1237, Loss: 0.6383878290653229, Final Batch Loss: 0.3115343749523163\n",
      "Epoch 1238, Loss: 0.5894326865673065, Final Batch Loss: 0.3229080140590668\n",
      "Epoch 1239, Loss: 0.652771532535553, Final Batch Loss: 0.3542252779006958\n",
      "Epoch 1240, Loss: 0.5845727622509003, Final Batch Loss: 0.2804388701915741\n",
      "Epoch 1241, Loss: 0.635688066482544, Final Batch Loss: 0.3548755943775177\n",
      "Epoch 1242, Loss: 0.6384289860725403, Final Batch Loss: 0.28916338086128235\n",
      "Epoch 1243, Loss: 0.6241399347782135, Final Batch Loss: 0.3536125421524048\n",
      "Epoch 1244, Loss: 0.5928830355405807, Final Batch Loss: 0.24710093438625336\n",
      "Epoch 1245, Loss: 0.6291073262691498, Final Batch Loss: 0.33826372027397156\n",
      "Epoch 1246, Loss: 0.6424368023872375, Final Batch Loss: 0.32455411553382874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1247, Loss: 0.6130620837211609, Final Batch Loss: 0.2440037727355957\n",
      "Epoch 1248, Loss: 0.639846920967102, Final Batch Loss: 0.34182068705558777\n",
      "Epoch 1249, Loss: 0.7396516799926758, Final Batch Loss: 0.4024307429790497\n",
      "Epoch 1250, Loss: 0.5829660594463348, Final Batch Loss: 0.2779996395111084\n",
      "Epoch 1251, Loss: 0.6870647072792053, Final Batch Loss: 0.3687606155872345\n",
      "Epoch 1252, Loss: 0.5860988795757294, Final Batch Loss: 0.3054403066635132\n",
      "Epoch 1253, Loss: 0.6617991328239441, Final Batch Loss: 0.3229638636112213\n",
      "Epoch 1254, Loss: 0.7157657742500305, Final Batch Loss: 0.36694321036338806\n",
      "Epoch 1255, Loss: 0.6094053089618683, Final Batch Loss: 0.29462891817092896\n",
      "Epoch 1256, Loss: 0.6853485703468323, Final Batch Loss: 0.372807115316391\n",
      "Epoch 1257, Loss: 0.6220984756946564, Final Batch Loss: 0.3403001129627228\n",
      "Epoch 1258, Loss: 0.6343886256217957, Final Batch Loss: 0.3254227638244629\n",
      "Epoch 1259, Loss: 0.6659730672836304, Final Batch Loss: 0.3509462773799896\n",
      "Epoch 1260, Loss: 0.5877961218357086, Final Batch Loss: 0.25262293219566345\n",
      "Epoch 1261, Loss: 0.6726319491863251, Final Batch Loss: 0.3770415782928467\n",
      "Epoch 1262, Loss: 0.5956808924674988, Final Batch Loss: 0.26880401372909546\n",
      "Epoch 1263, Loss: 0.6307728886604309, Final Batch Loss: 0.3497868776321411\n",
      "Epoch 1264, Loss: 0.6549113988876343, Final Batch Loss: 0.31733083724975586\n",
      "Epoch 1265, Loss: 0.6303970217704773, Final Batch Loss: 0.3178548812866211\n",
      "Epoch 1266, Loss: 0.6616850793361664, Final Batch Loss: 0.34521639347076416\n",
      "Epoch 1267, Loss: 0.5923767387866974, Final Batch Loss: 0.2523653209209442\n",
      "Epoch 1268, Loss: 0.5981382727622986, Final Batch Loss: 0.27690646052360535\n",
      "Epoch 1269, Loss: 0.602035254240036, Final Batch Loss: 0.30404114723205566\n",
      "Epoch 1270, Loss: 0.6218708455562592, Final Batch Loss: 0.2971217930316925\n",
      "Epoch 1271, Loss: 0.5632722973823547, Final Batch Loss: 0.24226811528205872\n",
      "Epoch 1272, Loss: 0.6820979714393616, Final Batch Loss: 0.2787010967731476\n",
      "Epoch 1273, Loss: 0.6554374694824219, Final Batch Loss: 0.33732175827026367\n",
      "Epoch 1274, Loss: 0.6533319354057312, Final Batch Loss: 0.3197210431098938\n",
      "Epoch 1275, Loss: 0.6298872232437134, Final Batch Loss: 0.29339566826820374\n",
      "Epoch 1276, Loss: 0.5494935512542725, Final Batch Loss: 0.23612844944000244\n",
      "Epoch 1277, Loss: 0.6472593247890472, Final Batch Loss: 0.3270777463912964\n",
      "Epoch 1278, Loss: 0.610781341791153, Final Batch Loss: 0.30827146768569946\n",
      "Epoch 1279, Loss: 0.5956598520278931, Final Batch Loss: 0.29071786999702454\n",
      "Epoch 1280, Loss: 0.6006336510181427, Final Batch Loss: 0.29461869597435\n",
      "Epoch 1281, Loss: 0.5854224562644958, Final Batch Loss: 0.25444987416267395\n",
      "Epoch 1282, Loss: 0.6498026549816132, Final Batch Loss: 0.33135101199150085\n",
      "Epoch 1283, Loss: 0.6455734670162201, Final Batch Loss: 0.3216482102870941\n",
      "Epoch 1284, Loss: 0.617905855178833, Final Batch Loss: 0.31637805700302124\n",
      "Epoch 1285, Loss: 0.661756843328476, Final Batch Loss: 0.3775288760662079\n",
      "Epoch 1286, Loss: 0.6225571632385254, Final Batch Loss: 0.310297429561615\n",
      "Epoch 1287, Loss: 0.5699808448553085, Final Batch Loss: 0.23829807341098785\n",
      "Epoch 1288, Loss: 0.6589208543300629, Final Batch Loss: 0.35081198811531067\n",
      "Epoch 1289, Loss: 0.6505149602890015, Final Batch Loss: 0.33013150095939636\n",
      "Epoch 1290, Loss: 0.6529175043106079, Final Batch Loss: 0.33737069368362427\n",
      "Epoch 1291, Loss: 0.6584236919879913, Final Batch Loss: 0.33298009634017944\n",
      "Epoch 1292, Loss: 0.642040342092514, Final Batch Loss: 0.30186522006988525\n",
      "Epoch 1293, Loss: 0.6710400581359863, Final Batch Loss: 0.36151546239852905\n",
      "Epoch 1294, Loss: 0.6867319941520691, Final Batch Loss: 0.32382118701934814\n",
      "Epoch 1295, Loss: 0.6422432661056519, Final Batch Loss: 0.36837005615234375\n",
      "Epoch 1296, Loss: 0.6089966297149658, Final Batch Loss: 0.27210912108421326\n",
      "Epoch 1297, Loss: 0.65289705991745, Final Batch Loss: 0.35918721556663513\n",
      "Epoch 1298, Loss: 0.7162362337112427, Final Batch Loss: 0.4354175627231598\n",
      "Epoch 1299, Loss: 0.6121126413345337, Final Batch Loss: 0.34379470348358154\n",
      "Epoch 1300, Loss: 0.6017569303512573, Final Batch Loss: 0.2982093393802643\n",
      "Epoch 1301, Loss: 0.5800470113754272, Final Batch Loss: 0.28215664625167847\n",
      "Epoch 1302, Loss: 0.6172710359096527, Final Batch Loss: 0.30888521671295166\n",
      "Epoch 1303, Loss: 0.6785953938961029, Final Batch Loss: 0.32527318596839905\n",
      "Epoch 1304, Loss: 0.6143105924129486, Final Batch Loss: 0.33821457624435425\n",
      "Epoch 1305, Loss: 0.6391521096229553, Final Batch Loss: 0.32408854365348816\n",
      "Epoch 1306, Loss: 0.6009341180324554, Final Batch Loss: 0.3106074929237366\n",
      "Epoch 1307, Loss: 0.6339753568172455, Final Batch Loss: 0.32934534549713135\n",
      "Epoch 1308, Loss: 0.5961715281009674, Final Batch Loss: 0.2939077317714691\n",
      "Epoch 1309, Loss: 0.6091521978378296, Final Batch Loss: 0.3095845580101013\n",
      "Epoch 1310, Loss: 0.6301730573177338, Final Batch Loss: 0.32264193892478943\n",
      "Epoch 1311, Loss: 0.623191237449646, Final Batch Loss: 0.3306301236152649\n",
      "Epoch 1312, Loss: 0.6363972127437592, Final Batch Loss: 0.3093169033527374\n",
      "Epoch 1313, Loss: 0.6392057836055756, Final Batch Loss: 0.3487548232078552\n",
      "Epoch 1314, Loss: 0.6228184103965759, Final Batch Loss: 0.2907431423664093\n",
      "Epoch 1315, Loss: 0.6368216574192047, Final Batch Loss: 0.321136474609375\n",
      "Epoch 1316, Loss: 0.5995689034461975, Final Batch Loss: 0.2849491834640503\n",
      "Epoch 1317, Loss: 0.6515934765338898, Final Batch Loss: 0.3229927122592926\n",
      "Epoch 1318, Loss: 0.6258940100669861, Final Batch Loss: 0.3140864968299866\n",
      "Epoch 1319, Loss: 0.5809876620769501, Final Batch Loss: 0.27492755651474\n",
      "Epoch 1320, Loss: 0.6019288003444672, Final Batch Loss: 0.2647605538368225\n",
      "Epoch 1321, Loss: 0.665122389793396, Final Batch Loss: 0.3175157308578491\n",
      "Epoch 1322, Loss: 0.6390971839427948, Final Batch Loss: 0.3381486237049103\n",
      "Epoch 1323, Loss: 0.625173956155777, Final Batch Loss: 0.31440430879592896\n",
      "Epoch 1324, Loss: 0.6338556706905365, Final Batch Loss: 0.29492926597595215\n",
      "Epoch 1325, Loss: 0.6408850252628326, Final Batch Loss: 0.3266429901123047\n",
      "Epoch 1326, Loss: 0.6280792057514191, Final Batch Loss: 0.34660714864730835\n",
      "Epoch 1327, Loss: 0.6004616320133209, Final Batch Loss: 0.30191391706466675\n",
      "Epoch 1328, Loss: 0.7085832059383392, Final Batch Loss: 0.3034329116344452\n",
      "Epoch 1329, Loss: 0.6130578815937042, Final Batch Loss: 0.3172532021999359\n",
      "Epoch 1330, Loss: 0.6278941035270691, Final Batch Loss: 0.3389574885368347\n",
      "Epoch 1331, Loss: 0.6067846119403839, Final Batch Loss: 0.28939288854599\n",
      "Epoch 1332, Loss: 0.6139195561408997, Final Batch Loss: 0.3024744987487793\n",
      "Epoch 1333, Loss: 0.6433700323104858, Final Batch Loss: 0.30114176869392395\n",
      "Epoch 1334, Loss: 0.6096128821372986, Final Batch Loss: 0.32058021426200867\n",
      "Epoch 1335, Loss: 0.6021695137023926, Final Batch Loss: 0.2722993791103363\n",
      "Epoch 1336, Loss: 0.7185671329498291, Final Batch Loss: 0.3647715151309967\n",
      "Epoch 1337, Loss: 0.6400136649608612, Final Batch Loss: 0.3701067864894867\n",
      "Epoch 1338, Loss: 0.6092295050621033, Final Batch Loss: 0.27448534965515137\n",
      "Epoch 1339, Loss: 0.6165634095668793, Final Batch Loss: 0.3054288327693939\n",
      "Epoch 1340, Loss: 0.713001549243927, Final Batch Loss: 0.43374013900756836\n",
      "Epoch 1341, Loss: 0.5614193677902222, Final Batch Loss: 0.22926336526870728\n",
      "Epoch 1342, Loss: 0.6304863095283508, Final Batch Loss: 0.32427605986595154\n",
      "Epoch 1343, Loss: 0.6498714089393616, Final Batch Loss: 0.32618892192840576\n",
      "Epoch 1344, Loss: 0.6157613694667816, Final Batch Loss: 0.3524584174156189\n",
      "Epoch 1345, Loss: 0.6356120407581329, Final Batch Loss: 0.284201443195343\n",
      "Epoch 1346, Loss: 0.5817326903343201, Final Batch Loss: 0.2578863799571991\n",
      "Epoch 1347, Loss: 0.5732322037220001, Final Batch Loss: 0.2528456151485443\n",
      "Epoch 1348, Loss: 0.6142237484455109, Final Batch Loss: 0.2770096957683563\n",
      "Epoch 1349, Loss: 0.6485815942287445, Final Batch Loss: 0.3288470506668091\n",
      "Epoch 1350, Loss: 0.5911209583282471, Final Batch Loss: 0.2617746889591217\n",
      "Epoch 1351, Loss: 0.6286382675170898, Final Batch Loss: 0.3461754024028778\n",
      "Epoch 1352, Loss: 0.7065356075763702, Final Batch Loss: 0.3726748526096344\n",
      "Epoch 1353, Loss: 0.5924125015735626, Final Batch Loss: 0.28437182307243347\n",
      "Epoch 1354, Loss: 0.61905637383461, Final Batch Loss: 0.26746442914009094\n",
      "Epoch 1355, Loss: 0.603447288274765, Final Batch Loss: 0.2940661907196045\n",
      "Epoch 1356, Loss: 0.6497193872928619, Final Batch Loss: 0.3258063495159149\n",
      "Epoch 1357, Loss: 0.6097659170627594, Final Batch Loss: 0.2901749014854431\n",
      "Epoch 1358, Loss: 0.6784868836402893, Final Batch Loss: 0.3643762171268463\n",
      "Epoch 1359, Loss: 0.6103313565254211, Final Batch Loss: 0.3270748555660248\n",
      "Epoch 1360, Loss: 0.561330035328865, Final Batch Loss: 0.23594821989536285\n",
      "Epoch 1361, Loss: 0.5569770336151123, Final Batch Loss: 0.24498578906059265\n",
      "Epoch 1362, Loss: 0.6065868139266968, Final Batch Loss: 0.30420342087745667\n",
      "Epoch 1363, Loss: 0.5348241925239563, Final Batch Loss: 0.23273056745529175\n",
      "Epoch 1364, Loss: 0.6377114951610565, Final Batch Loss: 0.29794585704803467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1365, Loss: 0.5917130708694458, Final Batch Loss: 0.2950183153152466\n",
      "Epoch 1366, Loss: 0.6125487983226776, Final Batch Loss: 0.29015693068504333\n",
      "Epoch 1367, Loss: 0.6267406642436981, Final Batch Loss: 0.3317665755748749\n",
      "Epoch 1368, Loss: 0.6350703537464142, Final Batch Loss: 0.34474119544029236\n",
      "Epoch 1369, Loss: 0.6295132339000702, Final Batch Loss: 0.3334846496582031\n",
      "Epoch 1370, Loss: 0.6361751556396484, Final Batch Loss: 0.3059360086917877\n",
      "Epoch 1371, Loss: 0.6208906769752502, Final Batch Loss: 0.3026474118232727\n",
      "Epoch 1372, Loss: 0.5830206573009491, Final Batch Loss: 0.31881776452064514\n",
      "Epoch 1373, Loss: 0.656565934419632, Final Batch Loss: 0.4026574194431305\n",
      "Epoch 1374, Loss: 0.6793861389160156, Final Batch Loss: 0.40656179189682007\n",
      "Epoch 1375, Loss: 0.6225528120994568, Final Batch Loss: 0.3359020948410034\n",
      "Epoch 1376, Loss: 0.6408877670764923, Final Batch Loss: 0.3511998951435089\n",
      "Epoch 1377, Loss: 0.662937194108963, Final Batch Loss: 0.31298622488975525\n",
      "Epoch 1378, Loss: 0.5925662815570831, Final Batch Loss: 0.29194119572639465\n",
      "Epoch 1379, Loss: 0.5876204967498779, Final Batch Loss: 0.2648797035217285\n",
      "Epoch 1380, Loss: 0.6335645616054535, Final Batch Loss: 0.2883927524089813\n",
      "Epoch 1381, Loss: 0.5706555843353271, Final Batch Loss: 0.2672566771507263\n",
      "Epoch 1382, Loss: 0.6344630122184753, Final Batch Loss: 0.3012588322162628\n",
      "Epoch 1383, Loss: 0.5484155416488647, Final Batch Loss: 0.2751151919364929\n",
      "Epoch 1384, Loss: 0.6197700798511505, Final Batch Loss: 0.3164465129375458\n",
      "Epoch 1385, Loss: 0.6205802857875824, Final Batch Loss: 0.3588569462299347\n",
      "Epoch 1386, Loss: 0.6245729923248291, Final Batch Loss: 0.3375414311885834\n",
      "Epoch 1387, Loss: 0.6845932304859161, Final Batch Loss: 0.3815845847129822\n",
      "Epoch 1388, Loss: 0.5682264268398285, Final Batch Loss: 0.2722759246826172\n",
      "Epoch 1389, Loss: 0.6099365651607513, Final Batch Loss: 0.295294851064682\n",
      "Epoch 1390, Loss: 0.6685556471347809, Final Batch Loss: 0.36049985885620117\n",
      "Epoch 1391, Loss: 0.5868244767189026, Final Batch Loss: 0.3244723379611969\n",
      "Epoch 1392, Loss: 0.6002901792526245, Final Batch Loss: 0.3325917422771454\n",
      "Epoch 1393, Loss: 0.5875013768672943, Final Batch Loss: 0.32856062054634094\n",
      "Epoch 1394, Loss: 0.578061431646347, Final Batch Loss: 0.2642609775066376\n",
      "Epoch 1395, Loss: 0.661625325679779, Final Batch Loss: 0.38290324807167053\n",
      "Epoch 1396, Loss: 0.5550854504108429, Final Batch Loss: 0.2659015655517578\n",
      "Epoch 1397, Loss: 0.587307870388031, Final Batch Loss: 0.2566981613636017\n",
      "Epoch 1398, Loss: 0.5922912657260895, Final Batch Loss: 0.26136061549186707\n",
      "Epoch 1399, Loss: 0.6338303983211517, Final Batch Loss: 0.3483167290687561\n",
      "Epoch 1400, Loss: 0.6656020283699036, Final Batch Loss: 0.3908236026763916\n",
      "Epoch 1401, Loss: 0.5758940577507019, Final Batch Loss: 0.25507044792175293\n",
      "Epoch 1402, Loss: 0.6427542865276337, Final Batch Loss: 0.3276630938053131\n",
      "Epoch 1403, Loss: 0.6420430541038513, Final Batch Loss: 0.33705195784568787\n",
      "Epoch 1404, Loss: 0.6210462749004364, Final Batch Loss: 0.30808404088020325\n",
      "Epoch 1405, Loss: 0.6186181008815765, Final Batch Loss: 0.30603307485580444\n",
      "Epoch 1406, Loss: 0.6382798552513123, Final Batch Loss: 0.374384343624115\n",
      "Epoch 1407, Loss: 0.5784577429294586, Final Batch Loss: 0.28816553950309753\n",
      "Epoch 1408, Loss: 0.6106738150119781, Final Batch Loss: 0.3328355848789215\n",
      "Epoch 1409, Loss: 0.6300607323646545, Final Batch Loss: 0.3129914700984955\n",
      "Epoch 1410, Loss: 0.6048652827739716, Final Batch Loss: 0.30245184898376465\n",
      "Epoch 1411, Loss: 0.5881159603595734, Final Batch Loss: 0.3136928677558899\n",
      "Epoch 1412, Loss: 0.5663327127695084, Final Batch Loss: 0.228098526597023\n",
      "Epoch 1413, Loss: 0.6176292300224304, Final Batch Loss: 0.29764509201049805\n",
      "Epoch 1414, Loss: 0.6132231652736664, Final Batch Loss: 0.3010006844997406\n",
      "Epoch 1415, Loss: 0.5790307819843292, Final Batch Loss: 0.27673259377479553\n",
      "Epoch 1416, Loss: 0.6313215792179108, Final Batch Loss: 0.3080289959907532\n",
      "Epoch 1417, Loss: 0.5594838857650757, Final Batch Loss: 0.2601677179336548\n",
      "Epoch 1418, Loss: 0.6016768515110016, Final Batch Loss: 0.3290700614452362\n",
      "Epoch 1419, Loss: 0.6440625488758087, Final Batch Loss: 0.3498559892177582\n",
      "Epoch 1420, Loss: 0.5353570878505707, Final Batch Loss: 0.2725711166858673\n",
      "Epoch 1421, Loss: 0.6096657961606979, Final Batch Loss: 0.3695003390312195\n",
      "Epoch 1422, Loss: 0.6174305081367493, Final Batch Loss: 0.25677594542503357\n",
      "Epoch 1423, Loss: 0.6235458254814148, Final Batch Loss: 0.31441235542297363\n",
      "Epoch 1424, Loss: 0.5561663210391998, Final Batch Loss: 0.2422865927219391\n",
      "Epoch 1425, Loss: 0.5614707618951797, Final Batch Loss: 0.248520627617836\n",
      "Epoch 1426, Loss: 0.5733202695846558, Final Batch Loss: 0.2586795687675476\n",
      "Epoch 1427, Loss: 0.5152490884065628, Final Batch Loss: 0.2345929890871048\n",
      "Epoch 1428, Loss: 0.6607309281826019, Final Batch Loss: 0.3655434548854828\n",
      "Epoch 1429, Loss: 0.727674663066864, Final Batch Loss: 0.44658342003822327\n",
      "Epoch 1430, Loss: 0.5937704145908356, Final Batch Loss: 0.2687884569168091\n",
      "Epoch 1431, Loss: 0.5627192258834839, Final Batch Loss: 0.26197361946105957\n",
      "Epoch 1432, Loss: 0.6731826364994049, Final Batch Loss: 0.3985040485858917\n",
      "Epoch 1433, Loss: 0.5870057940483093, Final Batch Loss: 0.26881274580955505\n",
      "Epoch 1434, Loss: 0.5811871290206909, Final Batch Loss: 0.2604236900806427\n",
      "Epoch 1435, Loss: 0.5054094046354294, Final Batch Loss: 0.22834770381450653\n",
      "Epoch 1436, Loss: 0.5771822035312653, Final Batch Loss: 0.26066145300865173\n",
      "Epoch 1437, Loss: 0.6472571492195129, Final Batch Loss: 0.3362545371055603\n",
      "Epoch 1438, Loss: 0.6074926555156708, Final Batch Loss: 0.33583950996398926\n",
      "Epoch 1439, Loss: 0.5925624668598175, Final Batch Loss: 0.2883412837982178\n",
      "Epoch 1440, Loss: 0.6294568181037903, Final Batch Loss: 0.3385550379753113\n",
      "Epoch 1441, Loss: 0.6146734803915024, Final Batch Loss: 0.235305055975914\n",
      "Epoch 1442, Loss: 0.6161925196647644, Final Batch Loss: 0.3302604854106903\n",
      "Epoch 1443, Loss: 0.6632428765296936, Final Batch Loss: 0.33362051844596863\n",
      "Epoch 1444, Loss: 0.6572074890136719, Final Batch Loss: 0.35623544454574585\n",
      "Epoch 1445, Loss: 0.6162492632865906, Final Batch Loss: 0.2791750729084015\n",
      "Epoch 1446, Loss: 0.6386634707450867, Final Batch Loss: 0.3194643259048462\n",
      "Epoch 1447, Loss: 0.5877744853496552, Final Batch Loss: 0.3227286636829376\n",
      "Epoch 1448, Loss: 0.609798789024353, Final Batch Loss: 0.2971844971179962\n",
      "Epoch 1449, Loss: 0.6348037421703339, Final Batch Loss: 0.3183777630329132\n",
      "Epoch 1450, Loss: 0.6552667915821075, Final Batch Loss: 0.3435145616531372\n",
      "Epoch 1451, Loss: 0.6105618476867676, Final Batch Loss: 0.3421906530857086\n",
      "Epoch 1452, Loss: 0.5602518320083618, Final Batch Loss: 0.2320985198020935\n",
      "Epoch 1453, Loss: 0.6397796869277954, Final Batch Loss: 0.35773956775665283\n",
      "Epoch 1454, Loss: 0.568227231502533, Final Batch Loss: 0.25253450870513916\n",
      "Epoch 1455, Loss: 0.6400271952152252, Final Batch Loss: 0.34832143783569336\n",
      "Epoch 1456, Loss: 0.6219733655452728, Final Batch Loss: 0.3121030330657959\n",
      "Epoch 1457, Loss: 0.5752256810665131, Final Batch Loss: 0.27271756529808044\n",
      "Epoch 1458, Loss: 0.65836301445961, Final Batch Loss: 0.3678910434246063\n",
      "Epoch 1459, Loss: 0.6117308139801025, Final Batch Loss: 0.354979008436203\n",
      "Epoch 1460, Loss: 0.5709923803806305, Final Batch Loss: 0.26662197709083557\n",
      "Epoch 1461, Loss: 0.567357137799263, Final Batch Loss: 0.22665344178676605\n",
      "Epoch 1462, Loss: 0.6437540650367737, Final Batch Loss: 0.29148441553115845\n",
      "Epoch 1463, Loss: 0.6172364950180054, Final Batch Loss: 0.30873891711235046\n",
      "Epoch 1464, Loss: 0.5606618821620941, Final Batch Loss: 0.27813515067100525\n",
      "Epoch 1465, Loss: 0.570768803358078, Final Batch Loss: 0.3086516857147217\n",
      "Epoch 1466, Loss: 0.5789615213871002, Final Batch Loss: 0.2596433758735657\n",
      "Epoch 1467, Loss: 0.5753047168254852, Final Batch Loss: 0.3061530888080597\n",
      "Epoch 1468, Loss: 0.6079256534576416, Final Batch Loss: 0.3217523992061615\n",
      "Epoch 1469, Loss: 0.6122454702854156, Final Batch Loss: 0.35287240147590637\n",
      "Epoch 1470, Loss: 0.6310794055461884, Final Batch Loss: 0.36086171865463257\n",
      "Epoch 1471, Loss: 0.6205885410308838, Final Batch Loss: 0.32251518964767456\n",
      "Epoch 1472, Loss: 0.598534882068634, Final Batch Loss: 0.27434974908828735\n",
      "Epoch 1473, Loss: 0.5831958055496216, Final Batch Loss: 0.2830260694026947\n",
      "Epoch 1474, Loss: 0.5639043599367142, Final Batch Loss: 0.23663274943828583\n",
      "Epoch 1475, Loss: 0.656365305185318, Final Batch Loss: 0.37545791268348694\n",
      "Epoch 1476, Loss: 0.5649538040161133, Final Batch Loss: 0.3207191526889801\n",
      "Epoch 1477, Loss: 0.5978818237781525, Final Batch Loss: 0.2945360243320465\n",
      "Epoch 1478, Loss: 0.562012642621994, Final Batch Loss: 0.2615746557712555\n",
      "Epoch 1479, Loss: 0.615926057100296, Final Batch Loss: 0.3381832540035248\n",
      "Epoch 1480, Loss: 0.5932578146457672, Final Batch Loss: 0.26609593629837036\n",
      "Epoch 1481, Loss: 0.6158115565776825, Final Batch Loss: 0.34070974588394165\n",
      "Epoch 1482, Loss: 0.630993664264679, Final Batch Loss: 0.34175196290016174\n",
      "Epoch 1483, Loss: 0.653903067111969, Final Batch Loss: 0.3412827253341675\n",
      "Epoch 1484, Loss: 0.5966348052024841, Final Batch Loss: 0.33833128213882446\n",
      "Epoch 1485, Loss: 0.5275042504072189, Final Batch Loss: 0.23562340438365936\n",
      "Epoch 1486, Loss: 0.5935867130756378, Final Batch Loss: 0.3208840489387512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1487, Loss: 0.6320710182189941, Final Batch Loss: 0.3182299733161926\n",
      "Epoch 1488, Loss: 0.6239182651042938, Final Batch Loss: 0.3645954132080078\n",
      "Epoch 1489, Loss: 0.611097127199173, Final Batch Loss: 0.2925628423690796\n",
      "Epoch 1490, Loss: 0.5946861803531647, Final Batch Loss: 0.2588089406490326\n",
      "Epoch 1491, Loss: 0.5941075086593628, Final Batch Loss: 0.29043444991111755\n",
      "Epoch 1492, Loss: 0.5862491726875305, Final Batch Loss: 0.3004751205444336\n",
      "Epoch 1493, Loss: 0.64591184258461, Final Batch Loss: 0.35206568241119385\n",
      "Epoch 1494, Loss: 0.5639536529779434, Final Batch Loss: 0.2488846331834793\n",
      "Epoch 1495, Loss: 0.5954731702804565, Final Batch Loss: 0.2739435136318207\n",
      "Epoch 1496, Loss: 0.5961821675300598, Final Batch Loss: 0.2500636875629425\n",
      "Epoch 1497, Loss: 0.5789524018764496, Final Batch Loss: 0.29892948269844055\n",
      "Epoch 1498, Loss: 0.5961765050888062, Final Batch Loss: 0.311470627784729\n",
      "Epoch 1499, Loss: 0.6042765378952026, Final Batch Loss: 0.32390570640563965\n",
      "Epoch 1500, Loss: 0.5892073214054108, Final Batch Loss: 0.29150527715682983\n",
      "Epoch 1501, Loss: 0.5965374112129211, Final Batch Loss: 0.2635367512702942\n",
      "Epoch 1502, Loss: 0.6154675185680389, Final Batch Loss: 0.36489763855934143\n",
      "Epoch 1503, Loss: 0.5896027088165283, Final Batch Loss: 0.2774890959262848\n",
      "Epoch 1504, Loss: 0.649215430021286, Final Batch Loss: 0.3504799008369446\n",
      "Epoch 1505, Loss: 0.6051317751407623, Final Batch Loss: 0.27176082134246826\n",
      "Epoch 1506, Loss: 0.5437283962965012, Final Batch Loss: 0.2340468019247055\n",
      "Epoch 1507, Loss: 0.6058844029903412, Final Batch Loss: 0.32770857214927673\n",
      "Epoch 1508, Loss: 0.5431921184062958, Final Batch Loss: 0.24937278032302856\n",
      "Epoch 1509, Loss: 0.6032803952693939, Final Batch Loss: 0.2932194471359253\n",
      "Epoch 1510, Loss: 0.5682087242603302, Final Batch Loss: 0.29322731494903564\n",
      "Epoch 1511, Loss: 0.612202376127243, Final Batch Loss: 0.2940356731414795\n",
      "Epoch 1512, Loss: 0.5866986215114594, Final Batch Loss: 0.2858920991420746\n",
      "Epoch 1513, Loss: 0.5860949009656906, Final Batch Loss: 0.24842040240764618\n",
      "Epoch 1514, Loss: 0.5534746050834656, Final Batch Loss: 0.2883540391921997\n",
      "Epoch 1515, Loss: 0.5684815943241119, Final Batch Loss: 0.2869236469268799\n",
      "Epoch 1516, Loss: 0.5755311250686646, Final Batch Loss: 0.27500125765800476\n",
      "Epoch 1517, Loss: 0.6373160183429718, Final Batch Loss: 0.3729987144470215\n",
      "Epoch 1518, Loss: 0.5962286293506622, Final Batch Loss: 0.3553401231765747\n",
      "Epoch 1519, Loss: 0.5352739691734314, Final Batch Loss: 0.21520209312438965\n",
      "Epoch 1520, Loss: 0.5550275593996048, Final Batch Loss: 0.24551574885845184\n",
      "Epoch 1521, Loss: 0.541447326540947, Final Batch Loss: 0.24823559820652008\n",
      "Epoch 1522, Loss: 0.6118811964988708, Final Batch Loss: 0.35750260949134827\n",
      "Epoch 1523, Loss: 0.6977567672729492, Final Batch Loss: 0.3829682469367981\n",
      "Epoch 1524, Loss: 0.6392886638641357, Final Batch Loss: 0.3247196674346924\n",
      "Epoch 1525, Loss: 0.608306884765625, Final Batch Loss: 0.3298667073249817\n",
      "Epoch 1526, Loss: 0.5893617868423462, Final Batch Loss: 0.29520082473754883\n",
      "Epoch 1527, Loss: 0.5694555342197418, Final Batch Loss: 0.2910793125629425\n",
      "Epoch 1528, Loss: 0.563073992729187, Final Batch Loss: 0.2839547097682953\n",
      "Epoch 1529, Loss: 0.659978985786438, Final Batch Loss: 0.38557684421539307\n",
      "Epoch 1530, Loss: 0.6022111475467682, Final Batch Loss: 0.33179938793182373\n",
      "Epoch 1531, Loss: 0.5448342114686966, Final Batch Loss: 0.24959973990917206\n",
      "Epoch 1532, Loss: 0.6253717541694641, Final Batch Loss: 0.31541895866394043\n",
      "Epoch 1533, Loss: 0.5657073557376862, Final Batch Loss: 0.28770357370376587\n",
      "Epoch 1534, Loss: 0.609272688627243, Final Batch Loss: 0.2996949255466461\n",
      "Epoch 1535, Loss: 0.602400541305542, Final Batch Loss: 0.3260132074356079\n",
      "Epoch 1536, Loss: 0.6919330060482025, Final Batch Loss: 0.3656146228313446\n",
      "Epoch 1537, Loss: 0.5821891725063324, Final Batch Loss: 0.2362925112247467\n",
      "Epoch 1538, Loss: 0.6053589880466461, Final Batch Loss: 0.32549721002578735\n",
      "Epoch 1539, Loss: 0.5565800368785858, Final Batch Loss: 0.2696141004562378\n",
      "Epoch 1540, Loss: 0.732684850692749, Final Batch Loss: 0.44490113854408264\n",
      "Epoch 1541, Loss: 0.6578203737735748, Final Batch Loss: 0.3442367613315582\n",
      "Epoch 1542, Loss: 0.5785617828369141, Final Batch Loss: 0.24148085713386536\n",
      "Epoch 1543, Loss: 0.5736404955387115, Final Batch Loss: 0.2777974009513855\n",
      "Epoch 1544, Loss: 0.5789927542209625, Final Batch Loss: 0.3087064027786255\n",
      "Epoch 1545, Loss: 0.6375965476036072, Final Batch Loss: 0.3006508946418762\n",
      "Epoch 1546, Loss: 0.5889734029769897, Final Batch Loss: 0.30761152505874634\n",
      "Epoch 1547, Loss: 0.5985196232795715, Final Batch Loss: 0.3099507689476013\n",
      "Epoch 1548, Loss: 0.58146733045578, Final Batch Loss: 0.2872726023197174\n",
      "Epoch 1549, Loss: 0.6253914833068848, Final Batch Loss: 0.35601159930229187\n",
      "Epoch 1550, Loss: 0.6170384883880615, Final Batch Loss: 0.2854987680912018\n",
      "Epoch 1551, Loss: 0.5503793060779572, Final Batch Loss: 0.2290010154247284\n",
      "Epoch 1552, Loss: 0.588422954082489, Final Batch Loss: 0.31135159730911255\n",
      "Epoch 1553, Loss: 0.6313709318637848, Final Batch Loss: 0.3339889943599701\n",
      "Epoch 1554, Loss: 0.6284195184707642, Final Batch Loss: 0.3484020531177521\n",
      "Epoch 1555, Loss: 0.581373542547226, Final Batch Loss: 0.2575504779815674\n",
      "Epoch 1556, Loss: 0.6211360991001129, Final Batch Loss: 0.3172287940979004\n",
      "Epoch 1557, Loss: 0.61037677526474, Final Batch Loss: 0.30986636877059937\n",
      "Epoch 1558, Loss: 0.5859566032886505, Final Batch Loss: 0.2881881594657898\n",
      "Epoch 1559, Loss: 0.5833151340484619, Final Batch Loss: 0.283408522605896\n",
      "Epoch 1560, Loss: 0.634198784828186, Final Batch Loss: 0.35260123014450073\n",
      "Epoch 1561, Loss: 0.55766361951828, Final Batch Loss: 0.275265634059906\n",
      "Epoch 1562, Loss: 0.5528750568628311, Final Batch Loss: 0.24100272357463837\n",
      "Epoch 1563, Loss: 0.585692286491394, Final Batch Loss: 0.2712295353412628\n",
      "Epoch 1564, Loss: 0.6358432173728943, Final Batch Loss: 0.3017224669456482\n",
      "Epoch 1565, Loss: 0.5845145583152771, Final Batch Loss: 0.284339964389801\n",
      "Epoch 1566, Loss: 0.5891572833061218, Final Batch Loss: 0.3072625696659088\n",
      "Epoch 1567, Loss: 0.6059451699256897, Final Batch Loss: 0.2957463264465332\n",
      "Epoch 1568, Loss: 0.5814171433448792, Final Batch Loss: 0.28942960500717163\n",
      "Epoch 1569, Loss: 0.5962581932544708, Final Batch Loss: 0.3229767382144928\n",
      "Epoch 1570, Loss: 0.6230879127979279, Final Batch Loss: 0.30657508969306946\n",
      "Epoch 1571, Loss: 0.5827281177043915, Final Batch Loss: 0.3187011778354645\n",
      "Epoch 1572, Loss: 0.630809098482132, Final Batch Loss: 0.30865806341171265\n",
      "Epoch 1573, Loss: 0.6602042615413666, Final Batch Loss: 0.37377625703811646\n",
      "Epoch 1574, Loss: 0.5554722547531128, Final Batch Loss: 0.26693010330200195\n",
      "Epoch 1575, Loss: 0.5711435973644257, Final Batch Loss: 0.31458616256713867\n",
      "Epoch 1576, Loss: 0.6300685405731201, Final Batch Loss: 0.319953978061676\n",
      "Epoch 1577, Loss: 0.5442316830158234, Final Batch Loss: 0.2570594251155853\n",
      "Epoch 1578, Loss: 0.5963033437728882, Final Batch Loss: 0.28685262799263\n",
      "Epoch 1579, Loss: 0.5938274562358856, Final Batch Loss: 0.2578296363353729\n",
      "Epoch 1580, Loss: 0.6140346527099609, Final Batch Loss: 0.3105907738208771\n",
      "Epoch 1581, Loss: 0.5598739236593246, Final Batch Loss: 0.22404281795024872\n",
      "Epoch 1582, Loss: 0.5594856142997742, Final Batch Loss: 0.28974685072898865\n",
      "Epoch 1583, Loss: 0.6263421773910522, Final Batch Loss: 0.29670795798301697\n",
      "Epoch 1584, Loss: 0.546199768781662, Final Batch Loss: 0.25569501519203186\n",
      "Epoch 1585, Loss: 0.5963232219219208, Final Batch Loss: 0.32213881611824036\n",
      "Epoch 1586, Loss: 0.5850543081760406, Final Batch Loss: 0.2907788157463074\n",
      "Epoch 1587, Loss: 0.5491255819797516, Final Batch Loss: 0.23675885796546936\n",
      "Epoch 1588, Loss: 0.6438924670219421, Final Batch Loss: 0.3153076767921448\n",
      "Epoch 1589, Loss: 0.5682980716228485, Final Batch Loss: 0.31117403507232666\n",
      "Epoch 1590, Loss: 0.5814633071422577, Final Batch Loss: 0.27742817997932434\n",
      "Epoch 1591, Loss: 0.6103150248527527, Final Batch Loss: 0.3487498164176941\n",
      "Epoch 1592, Loss: 0.6072241365909576, Final Batch Loss: 0.3592318296432495\n",
      "Epoch 1593, Loss: 0.5378095805644989, Final Batch Loss: 0.22463715076446533\n",
      "Epoch 1594, Loss: 0.6169567108154297, Final Batch Loss: 0.33547723293304443\n",
      "Epoch 1595, Loss: 0.6109960973262787, Final Batch Loss: 0.31668564677238464\n",
      "Epoch 1596, Loss: 0.593059778213501, Final Batch Loss: 0.3187044858932495\n",
      "Epoch 1597, Loss: 0.5906599462032318, Final Batch Loss: 0.2828126847743988\n",
      "Epoch 1598, Loss: 0.5484423935413361, Final Batch Loss: 0.28996750712394714\n",
      "Epoch 1599, Loss: 0.6150456666946411, Final Batch Loss: 0.350848525762558\n",
      "Epoch 1600, Loss: 0.5450841337442398, Final Batch Loss: 0.30263808369636536\n",
      "Epoch 1601, Loss: 0.5845035910606384, Final Batch Loss: 0.27995792031288147\n",
      "Epoch 1602, Loss: 0.5795663595199585, Final Batch Loss: 0.29236164689064026\n",
      "Epoch 1603, Loss: 0.5937099754810333, Final Batch Loss: 0.2642318606376648\n",
      "Epoch 1604, Loss: 0.5437540411949158, Final Batch Loss: 0.2604113817214966\n",
      "Epoch 1605, Loss: 0.5784539580345154, Final Batch Loss: 0.25878986716270447\n",
      "Epoch 1606, Loss: 0.5952397584915161, Final Batch Loss: 0.27829205989837646\n",
      "Epoch 1607, Loss: 0.5980893671512604, Final Batch Loss: 0.3042651414871216\n",
      "Epoch 1608, Loss: 0.6436408758163452, Final Batch Loss: 0.36329662799835205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1609, Loss: 0.5729998052120209, Final Batch Loss: 0.3017065227031708\n",
      "Epoch 1610, Loss: 0.5308035314083099, Final Batch Loss: 0.2189059555530548\n",
      "Epoch 1611, Loss: 0.6346472799777985, Final Batch Loss: 0.34635424613952637\n",
      "Epoch 1612, Loss: 0.6293015480041504, Final Batch Loss: 0.28928089141845703\n",
      "Epoch 1613, Loss: 0.5711809396743774, Final Batch Loss: 0.2646466791629791\n",
      "Epoch 1614, Loss: 0.5708140730857849, Final Batch Loss: 0.2707836329936981\n",
      "Epoch 1615, Loss: 0.6153852641582489, Final Batch Loss: 0.3002975583076477\n",
      "Epoch 1616, Loss: 0.5885220468044281, Final Batch Loss: 0.3131459355354309\n",
      "Epoch 1617, Loss: 0.5731571018695831, Final Batch Loss: 0.2886207103729248\n",
      "Epoch 1618, Loss: 0.5697273910045624, Final Batch Loss: 0.26041683554649353\n",
      "Epoch 1619, Loss: 0.5944064855575562, Final Batch Loss: 0.3471089005470276\n",
      "Epoch 1620, Loss: 0.59166219830513, Final Batch Loss: 0.2692350745201111\n",
      "Epoch 1621, Loss: 0.5817244052886963, Final Batch Loss: 0.27217817306518555\n",
      "Epoch 1622, Loss: 0.5672254264354706, Final Batch Loss: 0.29317155480384827\n",
      "Epoch 1623, Loss: 0.5675157308578491, Final Batch Loss: 0.2556890845298767\n",
      "Epoch 1624, Loss: 0.5730451941490173, Final Batch Loss: 0.2735028564929962\n",
      "Epoch 1625, Loss: 0.5754248797893524, Final Batch Loss: 0.2826881408691406\n",
      "Epoch 1626, Loss: 0.5534493774175644, Final Batch Loss: 0.310342937707901\n",
      "Epoch 1627, Loss: 0.6013652384281158, Final Batch Loss: 0.33991652727127075\n",
      "Epoch 1628, Loss: 0.5836485773324966, Final Batch Loss: 0.2120826095342636\n",
      "Epoch 1629, Loss: 0.537909746170044, Final Batch Loss: 0.22503429651260376\n",
      "Epoch 1630, Loss: 0.5948913097381592, Final Batch Loss: 0.31826701760292053\n",
      "Epoch 1631, Loss: 0.5885717570781708, Final Batch Loss: 0.29751601815223694\n",
      "Epoch 1632, Loss: 0.5994867384433746, Final Batch Loss: 0.319745808839798\n",
      "Epoch 1633, Loss: 0.6141454577445984, Final Batch Loss: 0.3105284571647644\n",
      "Epoch 1634, Loss: 0.5643981397151947, Final Batch Loss: 0.2945196032524109\n",
      "Epoch 1635, Loss: 0.5009807646274567, Final Batch Loss: 0.2250290811061859\n",
      "Epoch 1636, Loss: 0.6519558429718018, Final Batch Loss: 0.2766070067882538\n",
      "Epoch 1637, Loss: 0.5508827865123749, Final Batch Loss: 0.2768296003341675\n",
      "Epoch 1638, Loss: 0.6280339658260345, Final Batch Loss: 0.3425647020339966\n",
      "Epoch 1639, Loss: 0.6229799687862396, Final Batch Loss: 0.34350255131721497\n",
      "Epoch 1640, Loss: 0.5806988775730133, Final Batch Loss: 0.29345598816871643\n",
      "Epoch 1641, Loss: 0.5903535783290863, Final Batch Loss: 0.34039223194122314\n",
      "Epoch 1642, Loss: 0.5407897233963013, Final Batch Loss: 0.2895154654979706\n",
      "Epoch 1643, Loss: 0.5464440733194351, Final Batch Loss: 0.22333426773548126\n",
      "Epoch 1644, Loss: 0.5608877539634705, Final Batch Loss: 0.2858683168888092\n",
      "Epoch 1645, Loss: 0.5873201787471771, Final Batch Loss: 0.3256562650203705\n",
      "Epoch 1646, Loss: 0.575739860534668, Final Batch Loss: 0.306414932012558\n",
      "Epoch 1647, Loss: 0.60650235414505, Final Batch Loss: 0.2794618010520935\n",
      "Epoch 1648, Loss: 0.5804180800914764, Final Batch Loss: 0.2537638545036316\n",
      "Epoch 1649, Loss: 0.6030371487140656, Final Batch Loss: 0.3332797884941101\n",
      "Epoch 1650, Loss: 0.5604275166988373, Final Batch Loss: 0.2648167312145233\n",
      "Epoch 1651, Loss: 0.5295484811067581, Final Batch Loss: 0.2106042057275772\n",
      "Epoch 1652, Loss: 0.5683664381504059, Final Batch Loss: 0.2424502968788147\n",
      "Epoch 1653, Loss: 0.553305447101593, Final Batch Loss: 0.2782040238380432\n",
      "Epoch 1654, Loss: 0.6016174554824829, Final Batch Loss: 0.31263846158981323\n",
      "Epoch 1655, Loss: 0.5951715707778931, Final Batch Loss: 0.3080978989601135\n",
      "Epoch 1656, Loss: 0.5838317573070526, Final Batch Loss: 0.3106801211833954\n",
      "Epoch 1657, Loss: 0.5918465554714203, Final Batch Loss: 0.30852004885673523\n",
      "Epoch 1658, Loss: 0.5551174879074097, Final Batch Loss: 0.27055829763412476\n",
      "Epoch 1659, Loss: 0.5829207301139832, Final Batch Loss: 0.31848829984664917\n",
      "Epoch 1660, Loss: 0.5282647907733917, Final Batch Loss: 0.27927547693252563\n",
      "Epoch 1661, Loss: 0.548547774553299, Final Batch Loss: 0.24685153365135193\n",
      "Epoch 1662, Loss: 0.6093852818012238, Final Batch Loss: 0.2967313230037689\n",
      "Epoch 1663, Loss: 0.5231189876794815, Final Batch Loss: 0.23256801068782806\n",
      "Epoch 1664, Loss: 0.5843387246131897, Final Batch Loss: 0.2774454951286316\n",
      "Epoch 1665, Loss: 0.574354499578476, Final Batch Loss: 0.3083595037460327\n",
      "Epoch 1666, Loss: 0.551550954580307, Final Batch Loss: 0.25121068954467773\n",
      "Epoch 1667, Loss: 0.5849827826023102, Final Batch Loss: 0.28540411591529846\n",
      "Epoch 1668, Loss: 0.6218564212322235, Final Batch Loss: 0.33869925141334534\n",
      "Epoch 1669, Loss: 0.5674462616443634, Final Batch Loss: 0.26886695623397827\n",
      "Epoch 1670, Loss: 0.5428420305252075, Final Batch Loss: 0.2821510136127472\n",
      "Epoch 1671, Loss: 0.6106374561786652, Final Batch Loss: 0.3672424256801605\n",
      "Epoch 1672, Loss: 0.5348955988883972, Final Batch Loss: 0.24175941944122314\n",
      "Epoch 1673, Loss: 0.6156574487686157, Final Batch Loss: 0.2928338050842285\n",
      "Epoch 1674, Loss: 0.6434490382671356, Final Batch Loss: 0.29106631875038147\n",
      "Epoch 1675, Loss: 0.6014589667320251, Final Batch Loss: 0.33484330773353577\n",
      "Epoch 1676, Loss: 0.5364674031734467, Final Batch Loss: 0.25040245056152344\n",
      "Epoch 1677, Loss: 0.5839740037918091, Final Batch Loss: 0.3194179832935333\n",
      "Epoch 1678, Loss: 0.5864083766937256, Final Batch Loss: 0.27926912903785706\n",
      "Epoch 1679, Loss: 0.6031815409660339, Final Batch Loss: 0.3180308938026428\n",
      "Epoch 1680, Loss: 0.656658947467804, Final Batch Loss: 0.2595321536064148\n",
      "Epoch 1681, Loss: 0.5958842635154724, Final Batch Loss: 0.33293479681015015\n",
      "Epoch 1682, Loss: 0.6157784461975098, Final Batch Loss: 0.3463762402534485\n",
      "Epoch 1683, Loss: 0.6137290298938751, Final Batch Loss: 0.3267361521720886\n",
      "Epoch 1684, Loss: 0.5747766196727753, Final Batch Loss: 0.2745939791202545\n",
      "Epoch 1685, Loss: 0.6327107548713684, Final Batch Loss: 0.33849024772644043\n",
      "Epoch 1686, Loss: 0.616670697927475, Final Batch Loss: 0.19449153542518616\n",
      "Epoch 1687, Loss: 0.5865148603916168, Final Batch Loss: 0.29041364789009094\n",
      "Epoch 1688, Loss: 0.530766099691391, Final Batch Loss: 0.257050096988678\n",
      "Epoch 1689, Loss: 0.5468806326389313, Final Batch Loss: 0.24700501561164856\n",
      "Epoch 1690, Loss: 0.5808890461921692, Final Batch Loss: 0.30352169275283813\n",
      "Epoch 1691, Loss: 0.555170863866806, Final Batch Loss: 0.24321579933166504\n",
      "Epoch 1692, Loss: 0.5540668666362762, Final Batch Loss: 0.2825515866279602\n",
      "Epoch 1693, Loss: 0.5350491404533386, Final Batch Loss: 0.23452115058898926\n",
      "Epoch 1694, Loss: 0.5173573344945908, Final Batch Loss: 0.22137565910816193\n",
      "Epoch 1695, Loss: 0.6178183555603027, Final Batch Loss: 0.34067657589912415\n",
      "Epoch 1696, Loss: 0.5977743417024612, Final Batch Loss: 0.2482239454984665\n",
      "Epoch 1697, Loss: 0.6082091927528381, Final Batch Loss: 0.34318554401397705\n",
      "Epoch 1698, Loss: 0.5583016276359558, Final Batch Loss: 0.272339791059494\n",
      "Epoch 1699, Loss: 0.5760655999183655, Final Batch Loss: 0.3228384852409363\n",
      "Epoch 1700, Loss: 0.5501005351543427, Final Batch Loss: 0.2543180286884308\n",
      "Epoch 1701, Loss: 0.6496945321559906, Final Batch Loss: 0.32572394609451294\n",
      "Epoch 1702, Loss: 0.541395753622055, Final Batch Loss: 0.2574580907821655\n",
      "Epoch 1703, Loss: 0.6296256184577942, Final Batch Loss: 0.3136477768421173\n",
      "Epoch 1704, Loss: 0.5525539219379425, Final Batch Loss: 0.27687904238700867\n",
      "Epoch 1705, Loss: 0.5701530277729034, Final Batch Loss: 0.31234756112098694\n",
      "Epoch 1706, Loss: 0.5905284881591797, Final Batch Loss: 0.2878348231315613\n",
      "Epoch 1707, Loss: 0.6030337810516357, Final Batch Loss: 0.3102378845214844\n",
      "Epoch 1708, Loss: 0.5656826496124268, Final Batch Loss: 0.2605220675468445\n",
      "Epoch 1709, Loss: 0.650945782661438, Final Batch Loss: 0.3536631762981415\n",
      "Epoch 1710, Loss: 0.6090155243873596, Final Batch Loss: 0.3153666853904724\n",
      "Epoch 1711, Loss: 0.5805332362651825, Final Batch Loss: 0.26642996072769165\n",
      "Epoch 1712, Loss: 0.5584217011928558, Final Batch Loss: 0.27459147572517395\n",
      "Epoch 1713, Loss: 0.6776514947414398, Final Batch Loss: 0.4163362681865692\n",
      "Epoch 1714, Loss: 0.5390298813581467, Final Batch Loss: 0.24201400578022003\n",
      "Epoch 1715, Loss: 0.5300193428993225, Final Batch Loss: 0.2839369773864746\n",
      "Epoch 1716, Loss: 0.5831314027309418, Final Batch Loss: 0.2812754213809967\n",
      "Epoch 1717, Loss: 0.6017241775989532, Final Batch Loss: 0.33116719126701355\n",
      "Epoch 1718, Loss: 0.5992000997066498, Final Batch Loss: 0.2921381890773773\n",
      "Epoch 1719, Loss: 0.5295341163873672, Final Batch Loss: 0.21650554239749908\n",
      "Epoch 1720, Loss: 0.5799877494573593, Final Batch Loss: 0.3482515215873718\n",
      "Epoch 1721, Loss: 0.5354105979204178, Final Batch Loss: 0.24794842302799225\n",
      "Epoch 1722, Loss: 0.5463303327560425, Final Batch Loss: 0.2696639895439148\n",
      "Epoch 1723, Loss: 0.5346758216619492, Final Batch Loss: 0.23671241104602814\n",
      "Epoch 1724, Loss: 0.640696257352829, Final Batch Loss: 0.30229270458221436\n",
      "Epoch 1725, Loss: 0.5499680042266846, Final Batch Loss: 0.28145578503608704\n",
      "Epoch 1726, Loss: 0.6156913340091705, Final Batch Loss: 0.3003300428390503\n",
      "Epoch 1727, Loss: 0.5784313678741455, Final Batch Loss: 0.302883505821228\n",
      "Epoch 1728, Loss: 0.5930872559547424, Final Batch Loss: 0.28564903140068054\n",
      "Epoch 1729, Loss: 0.542819619178772, Final Batch Loss: 0.28679224848747253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1730, Loss: 0.577917218208313, Final Batch Loss: 0.3071400821208954\n",
      "Epoch 1731, Loss: 0.6492262482643127, Final Batch Loss: 0.3869382441043854\n",
      "Epoch 1732, Loss: 0.5683488845825195, Final Batch Loss: 0.2613300085067749\n",
      "Epoch 1733, Loss: 0.6097801327705383, Final Batch Loss: 0.3333297371864319\n",
      "Epoch 1734, Loss: 0.5722737610340118, Final Batch Loss: 0.28432950377464294\n",
      "Epoch 1735, Loss: 0.5596771538257599, Final Batch Loss: 0.2695685923099518\n",
      "Epoch 1736, Loss: 0.5655449479818344, Final Batch Loss: 0.24591900408267975\n",
      "Epoch 1737, Loss: 0.6024670898914337, Final Batch Loss: 0.28348731994628906\n",
      "Epoch 1738, Loss: 0.5739938020706177, Final Batch Loss: 0.2778196334838867\n",
      "Epoch 1739, Loss: 0.5675080716609955, Final Batch Loss: 0.27501145005226135\n",
      "Epoch 1740, Loss: 0.5538050830364227, Final Batch Loss: 0.2576783001422882\n",
      "Epoch 1741, Loss: 0.5803565382957458, Final Batch Loss: 0.29436060786247253\n",
      "Epoch 1742, Loss: 0.5862798690795898, Final Batch Loss: 0.27137795090675354\n",
      "Epoch 1743, Loss: 0.5332356095314026, Final Batch Loss: 0.25252461433410645\n",
      "Epoch 1744, Loss: 0.5633112490177155, Final Batch Loss: 0.25647836923599243\n",
      "Epoch 1745, Loss: 0.5887415707111359, Final Batch Loss: 0.3369043469429016\n",
      "Epoch 1746, Loss: 0.5358412563800812, Final Batch Loss: 0.2901921272277832\n",
      "Epoch 1747, Loss: 0.5788295269012451, Final Batch Loss: 0.27667832374572754\n",
      "Epoch 1748, Loss: 0.5904081463813782, Final Batch Loss: 0.2644140124320984\n",
      "Epoch 1749, Loss: 0.5904549956321716, Final Batch Loss: 0.32410937547683716\n",
      "Epoch 1750, Loss: 0.5543119609355927, Final Batch Loss: 0.26202714443206787\n",
      "Epoch 1751, Loss: 0.5471902042627335, Final Batch Loss: 0.23961783945560455\n",
      "Epoch 1752, Loss: 0.54366135597229, Final Batch Loss: 0.27571526169776917\n",
      "Epoch 1753, Loss: 0.5769900381565094, Final Batch Loss: 0.2643139660358429\n",
      "Epoch 1754, Loss: 0.5599185824394226, Final Batch Loss: 0.2975776791572571\n",
      "Epoch 1755, Loss: 0.5966501533985138, Final Batch Loss: 0.3227011263370514\n",
      "Epoch 1756, Loss: 0.596383810043335, Final Batch Loss: 0.33999139070510864\n",
      "Epoch 1757, Loss: 0.6526257693767548, Final Batch Loss: 0.35202503204345703\n",
      "Epoch 1758, Loss: 0.5070824325084686, Final Batch Loss: 0.18609869480133057\n",
      "Epoch 1759, Loss: 0.5753725469112396, Final Batch Loss: 0.26709601283073425\n",
      "Epoch 1760, Loss: 0.6165945827960968, Final Batch Loss: 0.33441364765167236\n",
      "Epoch 1761, Loss: 0.5310587286949158, Final Batch Loss: 0.26040416955947876\n",
      "Epoch 1762, Loss: 0.6061776876449585, Final Batch Loss: 0.27688801288604736\n",
      "Epoch 1763, Loss: 0.5643742084503174, Final Batch Loss: 0.2710316479206085\n",
      "Epoch 1764, Loss: 0.5422917753458023, Final Batch Loss: 0.24456973373889923\n",
      "Epoch 1765, Loss: 0.5887057483196259, Final Batch Loss: 0.3193221688270569\n",
      "Epoch 1766, Loss: 0.5454075336456299, Final Batch Loss: 0.2842763066291809\n",
      "Epoch 1767, Loss: 0.5059849768877029, Final Batch Loss: 0.24384449422359467\n",
      "Epoch 1768, Loss: 0.5726802200078964, Final Batch Loss: 0.23914994299411774\n",
      "Epoch 1769, Loss: 0.5516638904809952, Final Batch Loss: 0.23317088186740875\n",
      "Epoch 1770, Loss: 0.6316703259944916, Final Batch Loss: 0.30418527126312256\n",
      "Epoch 1771, Loss: 0.6155466735363007, Final Batch Loss: 0.2809320092201233\n",
      "Epoch 1772, Loss: 0.5738539695739746, Final Batch Loss: 0.279858261346817\n",
      "Epoch 1773, Loss: 0.5635299980640411, Final Batch Loss: 0.3092953860759735\n",
      "Epoch 1774, Loss: 0.5899564027786255, Final Batch Loss: 0.2840396463871002\n",
      "Epoch 1775, Loss: 0.600914478302002, Final Batch Loss: 0.3052225708961487\n",
      "Epoch 1776, Loss: 0.550087034702301, Final Batch Loss: 0.27711349725723267\n",
      "Epoch 1777, Loss: 0.6073709726333618, Final Batch Loss: 0.329854279756546\n",
      "Epoch 1778, Loss: 0.5053386241197586, Final Batch Loss: 0.1913888305425644\n",
      "Epoch 1779, Loss: 0.5931882858276367, Final Batch Loss: 0.29259100556373596\n",
      "Epoch 1780, Loss: 0.582589715719223, Final Batch Loss: 0.29216790199279785\n",
      "Epoch 1781, Loss: 0.6393243670463562, Final Batch Loss: 0.36118966341018677\n",
      "Epoch 1782, Loss: 0.534402072429657, Final Batch Loss: 0.2639932632446289\n",
      "Epoch 1783, Loss: 0.5901971161365509, Final Batch Loss: 0.32199978828430176\n",
      "Epoch 1784, Loss: 0.5343816727399826, Final Batch Loss: 0.2448054701089859\n",
      "Epoch 1785, Loss: 0.5853893160820007, Final Batch Loss: 0.3012072443962097\n",
      "Epoch 1786, Loss: 0.5578790009021759, Final Batch Loss: 0.30729910731315613\n",
      "Epoch 1787, Loss: 0.5681096911430359, Final Batch Loss: 0.2676554024219513\n",
      "Epoch 1788, Loss: 0.5542025566101074, Final Batch Loss: 0.2610667049884796\n",
      "Epoch 1789, Loss: 0.5687749683856964, Final Batch Loss: 0.2998710870742798\n",
      "Epoch 1790, Loss: 0.5547452718019485, Final Batch Loss: 0.23648898303508759\n",
      "Epoch 1791, Loss: 0.5386121869087219, Final Batch Loss: 0.24651086330413818\n",
      "Epoch 1792, Loss: 0.5415201485157013, Final Batch Loss: 0.28936901688575745\n",
      "Epoch 1793, Loss: 0.5558882355690002, Final Batch Loss: 0.26660704612731934\n",
      "Epoch 1794, Loss: 0.5425823628902435, Final Batch Loss: 0.2509807348251343\n",
      "Epoch 1795, Loss: 0.6003658473491669, Final Batch Loss: 0.3383444547653198\n",
      "Epoch 1796, Loss: 0.6061475276947021, Final Batch Loss: 0.2922816872596741\n",
      "Epoch 1797, Loss: 0.5608896315097809, Final Batch Loss: 0.23698639869689941\n",
      "Epoch 1798, Loss: 0.6339607238769531, Final Batch Loss: 0.34423258900642395\n",
      "Epoch 1799, Loss: 0.5403774827718735, Final Batch Loss: 0.21738280355930328\n",
      "Epoch 1800, Loss: 0.5959645807743073, Final Batch Loss: 0.29837366938591003\n",
      "Epoch 1801, Loss: 0.5523723661899567, Final Batch Loss: 0.26319006085395813\n",
      "Epoch 1802, Loss: 0.5073497891426086, Final Batch Loss: 0.20585092902183533\n",
      "Epoch 1803, Loss: 0.5266624391078949, Final Batch Loss: 0.25610825419425964\n",
      "Epoch 1804, Loss: 0.5738739669322968, Final Batch Loss: 0.30198052525520325\n",
      "Epoch 1805, Loss: 0.6214997470378876, Final Batch Loss: 0.32122159004211426\n",
      "Epoch 1806, Loss: 0.5975157916545868, Final Batch Loss: 0.3202660083770752\n",
      "Epoch 1807, Loss: 0.5969778895378113, Final Batch Loss: 0.293711394071579\n",
      "Epoch 1808, Loss: 0.5971091389656067, Final Batch Loss: 0.2751138210296631\n",
      "Epoch 1809, Loss: 0.5716452896595001, Final Batch Loss: 0.33823540806770325\n",
      "Epoch 1810, Loss: 0.5995354354381561, Final Batch Loss: 0.3262716829776764\n",
      "Epoch 1811, Loss: 0.5856920778751373, Final Batch Loss: 0.3086797595024109\n",
      "Epoch 1812, Loss: 0.5683635771274567, Final Batch Loss: 0.2587658762931824\n",
      "Epoch 1813, Loss: 0.5602763891220093, Final Batch Loss: 0.2650613784790039\n",
      "Epoch 1814, Loss: 0.519569531083107, Final Batch Loss: 0.21431107819080353\n",
      "Epoch 1815, Loss: 0.5970184206962585, Final Batch Loss: 0.29584047198295593\n",
      "Epoch 1816, Loss: 0.5602655410766602, Final Batch Loss: 0.2602790296077728\n",
      "Epoch 1817, Loss: 0.5470663011074066, Final Batch Loss: 0.25433549284935\n",
      "Epoch 1818, Loss: 0.5563931465148926, Final Batch Loss: 0.2745550274848938\n",
      "Epoch 1819, Loss: 0.5890110433101654, Final Batch Loss: 0.32430362701416016\n",
      "Epoch 1820, Loss: 0.5786707699298859, Final Batch Loss: 0.2692332863807678\n",
      "Epoch 1821, Loss: 0.5897813439369202, Final Batch Loss: 0.30393463373184204\n",
      "Epoch 1822, Loss: 0.5753615498542786, Final Batch Loss: 0.2547893524169922\n",
      "Epoch 1823, Loss: 0.5840470790863037, Final Batch Loss: 0.268376886844635\n",
      "Epoch 1824, Loss: 0.6254404485225677, Final Batch Loss: 0.32060757279396057\n",
      "Epoch 1825, Loss: 0.5772216320037842, Final Batch Loss: 0.2711610198020935\n",
      "Epoch 1826, Loss: 0.5436675250530243, Final Batch Loss: 0.25790876150131226\n",
      "Epoch 1827, Loss: 0.5950290858745575, Final Batch Loss: 0.3057496249675751\n",
      "Epoch 1828, Loss: 0.5956796705722809, Final Batch Loss: 0.3067031502723694\n",
      "Epoch 1829, Loss: 0.548731118440628, Final Batch Loss: 0.25199300050735474\n",
      "Epoch 1830, Loss: 0.6028149127960205, Final Batch Loss: 0.2788366377353668\n",
      "Epoch 1831, Loss: 0.5655817985534668, Final Batch Loss: 0.26768484711647034\n",
      "Epoch 1832, Loss: 0.5598113834857941, Final Batch Loss: 0.2627440094947815\n",
      "Epoch 1833, Loss: 0.5479192733764648, Final Batch Loss: 0.2766789495944977\n",
      "Epoch 1834, Loss: 0.552717000246048, Final Batch Loss: 0.29344332218170166\n",
      "Epoch 1835, Loss: 0.5222520232200623, Final Batch Loss: 0.2253001630306244\n",
      "Epoch 1836, Loss: 0.5638250112533569, Final Batch Loss: 0.2986096739768982\n",
      "Epoch 1837, Loss: 0.6266885101795197, Final Batch Loss: 0.32264769077301025\n",
      "Epoch 1838, Loss: 0.5678092241287231, Final Batch Loss: 0.30639347434043884\n",
      "Epoch 1839, Loss: 0.5701684057712555, Final Batch Loss: 0.2999664843082428\n",
      "Epoch 1840, Loss: 0.5579472780227661, Final Batch Loss: 0.27750587463378906\n",
      "Epoch 1841, Loss: 0.5934161245822906, Final Batch Loss: 0.26323628425598145\n",
      "Epoch 1842, Loss: 0.544447511434555, Final Batch Loss: 0.2747834026813507\n",
      "Epoch 1843, Loss: 0.5372920334339142, Final Batch Loss: 0.240640789270401\n",
      "Epoch 1844, Loss: 0.5407993942499161, Final Batch Loss: 0.18331097066402435\n",
      "Epoch 1845, Loss: 0.5704767405986786, Final Batch Loss: 0.2834644317626953\n",
      "Epoch 1846, Loss: 0.5930888652801514, Final Batch Loss: 0.3021653890609741\n",
      "Epoch 1847, Loss: 0.5469762980937958, Final Batch Loss: 0.23675864934921265\n",
      "Epoch 1848, Loss: 0.5766233205795288, Final Batch Loss: 0.3102024793624878\n",
      "Epoch 1849, Loss: 0.5760981440544128, Final Batch Loss: 0.3179667294025421\n",
      "Epoch 1850, Loss: 0.5472679138183594, Final Batch Loss: 0.26784154772758484\n",
      "Epoch 1851, Loss: 0.5808267891407013, Final Batch Loss: 0.3055174946784973\n",
      "Epoch 1852, Loss: 0.5082855671644211, Final Batch Loss: 0.18043632805347443\n",
      "Epoch 1853, Loss: 0.6025784611701965, Final Batch Loss: 0.28572171926498413\n",
      "Epoch 1854, Loss: 0.5525805056095123, Final Batch Loss: 0.2604723274707794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1855, Loss: 0.5593834221363068, Final Batch Loss: 0.28195059299468994\n",
      "Epoch 1856, Loss: 0.5785250067710876, Final Batch Loss: 0.27623993158340454\n",
      "Epoch 1857, Loss: 0.5938472151756287, Final Batch Loss: 0.3008357584476471\n",
      "Epoch 1858, Loss: 0.5711258947849274, Final Batch Loss: 0.3099996745586395\n",
      "Epoch 1859, Loss: 0.5841314196586609, Final Batch Loss: 0.32628583908081055\n",
      "Epoch 1860, Loss: 0.524741142988205, Final Batch Loss: 0.2525193989276886\n",
      "Epoch 1861, Loss: 0.5829418003559113, Final Batch Loss: 0.3229890763759613\n",
      "Epoch 1862, Loss: 0.55030956864357, Final Batch Loss: 0.24480387568473816\n",
      "Epoch 1863, Loss: 0.5255547165870667, Final Batch Loss: 0.2500917613506317\n",
      "Epoch 1864, Loss: 0.576495349407196, Final Batch Loss: 0.3186669945716858\n",
      "Epoch 1865, Loss: 0.5501312017440796, Final Batch Loss: 0.2827024459838867\n",
      "Epoch 1866, Loss: 0.5449056029319763, Final Batch Loss: 0.290546715259552\n",
      "Epoch 1867, Loss: 0.5459988713264465, Final Batch Loss: 0.25183287262916565\n",
      "Epoch 1868, Loss: 0.5879676938056946, Final Batch Loss: 0.29032179713249207\n",
      "Epoch 1869, Loss: 0.5470910668373108, Final Batch Loss: 0.29490435123443604\n",
      "Epoch 1870, Loss: 0.5251774340867996, Final Batch Loss: 0.21511469781398773\n",
      "Epoch 1871, Loss: 0.4766066074371338, Final Batch Loss: 0.19406390190124512\n",
      "Epoch 1872, Loss: 0.5963056981563568, Final Batch Loss: 0.3236579895019531\n",
      "Epoch 1873, Loss: 0.5982120931148529, Final Batch Loss: 0.2774556875228882\n",
      "Epoch 1874, Loss: 0.5201482623815536, Final Batch Loss: 0.248662069439888\n",
      "Epoch 1875, Loss: 0.5595719218254089, Final Batch Loss: 0.2925352454185486\n",
      "Epoch 1876, Loss: 0.5888427197933197, Final Batch Loss: 0.30850744247436523\n",
      "Epoch 1877, Loss: 0.5416496396064758, Final Batch Loss: 0.2813628911972046\n",
      "Epoch 1878, Loss: 0.5758430659770966, Final Batch Loss: 0.2823823392391205\n",
      "Epoch 1879, Loss: 0.5870536863803864, Final Batch Loss: 0.3297837972640991\n",
      "Epoch 1880, Loss: 0.6371327340602875, Final Batch Loss: 0.3726852536201477\n",
      "Epoch 1881, Loss: 0.598777711391449, Final Batch Loss: 0.3236556947231293\n",
      "Epoch 1882, Loss: 0.58607417345047, Final Batch Loss: 0.32569777965545654\n",
      "Epoch 1883, Loss: 0.5677935779094696, Final Batch Loss: 0.29153159260749817\n",
      "Epoch 1884, Loss: 0.5785446166992188, Final Batch Loss: 0.271009624004364\n",
      "Epoch 1885, Loss: 0.550717756152153, Final Batch Loss: 0.3090343177318573\n",
      "Epoch 1886, Loss: 0.582609087228775, Final Batch Loss: 0.3155430853366852\n",
      "Epoch 1887, Loss: 0.5710844993591309, Final Batch Loss: 0.3079904615879059\n",
      "Epoch 1888, Loss: 0.5785798728466034, Final Batch Loss: 0.2961180508136749\n",
      "Epoch 1889, Loss: 0.5977706611156464, Final Batch Loss: 0.3239568769931793\n",
      "Epoch 1890, Loss: 0.5941101908683777, Final Batch Loss: 0.2856428027153015\n",
      "Epoch 1891, Loss: 0.5830890834331512, Final Batch Loss: 0.28186216950416565\n",
      "Epoch 1892, Loss: 0.5367973446846008, Final Batch Loss: 0.2529667913913727\n",
      "Epoch 1893, Loss: 0.5601090490818024, Final Batch Loss: 0.30228692293167114\n",
      "Epoch 1894, Loss: 0.49330078065395355, Final Batch Loss: 0.20136477053165436\n",
      "Epoch 1895, Loss: 0.5396796464920044, Final Batch Loss: 0.22201967239379883\n",
      "Epoch 1896, Loss: 0.5604265630245209, Final Batch Loss: 0.2771749794483185\n",
      "Epoch 1897, Loss: 0.59617480635643, Final Batch Loss: 0.31222715973854065\n",
      "Epoch 1898, Loss: 0.549893856048584, Final Batch Loss: 0.3047506511211395\n",
      "Epoch 1899, Loss: 0.603060245513916, Final Batch Loss: 0.34701302647590637\n",
      "Epoch 1900, Loss: 0.5782603919506073, Final Batch Loss: 0.30025485157966614\n",
      "Epoch 1901, Loss: 0.5701913237571716, Final Batch Loss: 0.30166974663734436\n",
      "Epoch 1902, Loss: 0.5447951555252075, Final Batch Loss: 0.2982831597328186\n",
      "Epoch 1903, Loss: 0.612354964017868, Final Batch Loss: 0.33649662137031555\n",
      "Epoch 1904, Loss: 0.5359883010387421, Final Batch Loss: 0.2636536657810211\n",
      "Epoch 1905, Loss: 0.5865080952644348, Final Batch Loss: 0.3247426450252533\n",
      "Epoch 1906, Loss: 0.6139326393604279, Final Batch Loss: 0.35170531272888184\n",
      "Epoch 1907, Loss: 0.556089460849762, Final Batch Loss: 0.2568173110485077\n",
      "Epoch 1908, Loss: 0.5452163964509964, Final Batch Loss: 0.22172318398952484\n",
      "Epoch 1909, Loss: 0.533429354429245, Final Batch Loss: 0.2505883574485779\n",
      "Epoch 1910, Loss: 0.5832060277462006, Final Batch Loss: 0.2775442600250244\n",
      "Epoch 1911, Loss: 0.5396305918693542, Final Batch Loss: 0.26038092374801636\n",
      "Epoch 1912, Loss: 0.5660715699195862, Final Batch Loss: 0.22996637225151062\n",
      "Epoch 1913, Loss: 0.5789166241884232, Final Batch Loss: 0.3348274528980255\n",
      "Epoch 1914, Loss: 0.5963636040687561, Final Batch Loss: 0.3513357639312744\n",
      "Epoch 1915, Loss: 0.5552172958850861, Final Batch Loss: 0.2743102014064789\n",
      "Epoch 1916, Loss: 0.5231675803661346, Final Batch Loss: 0.23785656690597534\n",
      "Epoch 1917, Loss: 0.6459862291812897, Final Batch Loss: 0.3663608133792877\n",
      "Epoch 1918, Loss: 0.5458658039569855, Final Batch Loss: 0.25679147243499756\n",
      "Epoch 1919, Loss: 0.5188271403312683, Final Batch Loss: 0.23062798380851746\n",
      "Epoch 1920, Loss: 0.5737553536891937, Final Batch Loss: 0.27602577209472656\n",
      "Epoch 1921, Loss: 0.571469783782959, Final Batch Loss: 0.3087197542190552\n",
      "Epoch 1922, Loss: 0.5575695782899857, Final Batch Loss: 0.32614853978157043\n",
      "Epoch 1923, Loss: 0.5462545454502106, Final Batch Loss: 0.28197982907295227\n",
      "Epoch 1924, Loss: 0.5811372101306915, Final Batch Loss: 0.29500019550323486\n",
      "Epoch 1925, Loss: 0.5629910826683044, Final Batch Loss: 0.29017260670661926\n",
      "Epoch 1926, Loss: 0.5622778236865997, Final Batch Loss: 0.3264424502849579\n",
      "Epoch 1927, Loss: 0.5996879041194916, Final Batch Loss: 0.3061223030090332\n",
      "Epoch 1928, Loss: 0.5344121754169464, Final Batch Loss: 0.25685274600982666\n",
      "Epoch 1929, Loss: 0.5503435134887695, Final Batch Loss: 0.30200132727622986\n",
      "Epoch 1930, Loss: 0.5408742427825928, Final Batch Loss: 0.23345178365707397\n",
      "Epoch 1931, Loss: 0.5982589721679688, Final Batch Loss: 0.3130100965499878\n",
      "Epoch 1932, Loss: 0.5608247518539429, Final Batch Loss: 0.2838273346424103\n",
      "Epoch 1933, Loss: 0.5028611570596695, Final Batch Loss: 0.19008316099643707\n",
      "Epoch 1934, Loss: 0.5644818246364594, Final Batch Loss: 0.2741951644420624\n",
      "Epoch 1935, Loss: 0.5429999232292175, Final Batch Loss: 0.25554296374320984\n",
      "Epoch 1936, Loss: 0.531705379486084, Final Batch Loss: 0.26614055037498474\n",
      "Epoch 1937, Loss: 0.5832688808441162, Final Batch Loss: 0.28385818004608154\n",
      "Epoch 1938, Loss: 0.5754625797271729, Final Batch Loss: 0.3045039772987366\n",
      "Epoch 1939, Loss: 0.581440269947052, Final Batch Loss: 0.30684155225753784\n",
      "Epoch 1940, Loss: 0.5642472505569458, Final Batch Loss: 0.2947072386741638\n",
      "Epoch 1941, Loss: 0.545954018831253, Final Batch Loss: 0.26037856936454773\n",
      "Epoch 1942, Loss: 0.5541089624166489, Final Batch Loss: 0.23597408831119537\n",
      "Epoch 1943, Loss: 0.5857324004173279, Final Batch Loss: 0.27974894642829895\n",
      "Epoch 1944, Loss: 0.5347093045711517, Final Batch Loss: 0.25400272011756897\n",
      "Epoch 1945, Loss: 0.5478938221931458, Final Batch Loss: 0.28362783789634705\n",
      "Epoch 1946, Loss: 0.5888617038726807, Final Batch Loss: 0.25701603293418884\n",
      "Epoch 1947, Loss: 0.630356639623642, Final Batch Loss: 0.34674131870269775\n",
      "Epoch 1948, Loss: 0.5985971391201019, Final Batch Loss: 0.3285515606403351\n",
      "Epoch 1949, Loss: 0.6068886816501617, Final Batch Loss: 0.37647172808647156\n",
      "Epoch 1950, Loss: 0.5918166041374207, Final Batch Loss: 0.3084673285484314\n",
      "Epoch 1951, Loss: 0.566094309091568, Final Batch Loss: 0.2905018627643585\n",
      "Epoch 1952, Loss: 0.5426859557628632, Final Batch Loss: 0.2707018256187439\n",
      "Epoch 1953, Loss: 0.5320786833763123, Final Batch Loss: 0.2911428213119507\n",
      "Epoch 1954, Loss: 0.5457966029644012, Final Batch Loss: 0.23457083106040955\n",
      "Epoch 1955, Loss: 0.5598319321870804, Final Batch Loss: 0.2494635432958603\n",
      "Epoch 1956, Loss: 0.5255067199468613, Final Batch Loss: 0.22836725413799286\n",
      "Epoch 1957, Loss: 0.5771870613098145, Final Batch Loss: 0.3167058825492859\n",
      "Epoch 1958, Loss: 0.539562851190567, Final Batch Loss: 0.27391573786735535\n",
      "Epoch 1959, Loss: 0.5110397785902023, Final Batch Loss: 0.22284187376499176\n",
      "Epoch 1960, Loss: 0.5178464949131012, Final Batch Loss: 0.24806684255599976\n",
      "Epoch 1961, Loss: 0.5641172230243683, Final Batch Loss: 0.2953803837299347\n",
      "Epoch 1962, Loss: 0.563432365655899, Final Batch Loss: 0.29376107454299927\n",
      "Epoch 1963, Loss: 0.5815266370773315, Final Batch Loss: 0.2850012183189392\n",
      "Epoch 1964, Loss: 0.5864856243133545, Final Batch Loss: 0.28215715289115906\n",
      "Epoch 1965, Loss: 0.5662653148174286, Final Batch Loss: 0.2914079427719116\n",
      "Epoch 1966, Loss: 0.554343193769455, Final Batch Loss: 0.2500596344470978\n",
      "Epoch 1967, Loss: 0.5752584636211395, Final Batch Loss: 0.27686628699302673\n",
      "Epoch 1968, Loss: 0.5686566829681396, Final Batch Loss: 0.31772077083587646\n",
      "Epoch 1969, Loss: 0.5467497408390045, Final Batch Loss: 0.29202625155448914\n",
      "Epoch 1970, Loss: 0.539894238114357, Final Batch Loss: 0.300287127494812\n",
      "Epoch 1971, Loss: 0.542664110660553, Final Batch Loss: 0.25953972339630127\n",
      "Epoch 1972, Loss: 0.6150025427341461, Final Batch Loss: 0.2802068591117859\n",
      "Epoch 1973, Loss: 0.606507271528244, Final Batch Loss: 0.3119906783103943\n",
      "Epoch 1974, Loss: 0.6266734898090363, Final Batch Loss: 0.3267318606376648\n",
      "Epoch 1975, Loss: 0.5614519417285919, Final Batch Loss: 0.2631877064704895\n",
      "Epoch 1976, Loss: 0.5994952023029327, Final Batch Loss: 0.33544161915779114\n",
      "Epoch 1977, Loss: 0.5532014518976212, Final Batch Loss: 0.32765811681747437\n",
      "Epoch 1978, Loss: 0.5402924418449402, Final Batch Loss: 0.26769256591796875\n",
      "Epoch 1979, Loss: 0.616297721862793, Final Batch Loss: 0.321530818939209\n",
      "Epoch 1980, Loss: 0.5551178902387619, Final Batch Loss: 0.3053014576435089\n",
      "Epoch 1981, Loss: 0.5720637142658234, Final Batch Loss: 0.26969796419143677\n",
      "Epoch 1982, Loss: 0.5523928999900818, Final Batch Loss: 0.2637425661087036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1983, Loss: 0.6223128437995911, Final Batch Loss: 0.31826600432395935\n",
      "Epoch 1984, Loss: 0.5717242062091827, Final Batch Loss: 0.23556554317474365\n",
      "Epoch 1985, Loss: 0.5519953072071075, Final Batch Loss: 0.30856451392173767\n",
      "Epoch 1986, Loss: 0.579115629196167, Final Batch Loss: 0.302033394575119\n",
      "Epoch 1987, Loss: 0.5599138140678406, Final Batch Loss: 0.2574498951435089\n",
      "Epoch 1988, Loss: 0.5694517195224762, Final Batch Loss: 0.307646781206131\n",
      "Epoch 1989, Loss: 0.5533845126628876, Final Batch Loss: 0.25553473830223083\n",
      "Epoch 1990, Loss: 0.5297215133905411, Final Batch Loss: 0.23672674596309662\n",
      "Epoch 1991, Loss: 0.5619896948337555, Final Batch Loss: 0.284932941198349\n",
      "Epoch 1992, Loss: 0.5086250007152557, Final Batch Loss: 0.20379683375358582\n",
      "Epoch 1993, Loss: 0.5722776353359222, Final Batch Loss: 0.2907322347164154\n",
      "Epoch 1994, Loss: 0.5573713779449463, Final Batch Loss: 0.27929747104644775\n",
      "Epoch 1995, Loss: 0.6389966309070587, Final Batch Loss: 0.34591957926750183\n",
      "Epoch 1996, Loss: 0.5745763629674911, Final Batch Loss: 0.3359544575214386\n",
      "Epoch 1997, Loss: 0.6902366280555725, Final Batch Loss: 0.4053100049495697\n",
      "Epoch 1998, Loss: 0.5813864469528198, Final Batch Loss: 0.29656580090522766\n",
      "Epoch 1999, Loss: 0.5480239391326904, Final Batch Loss: 0.2586185932159424\n",
      "Epoch 2000, Loss: 0.573121964931488, Final Batch Loss: 0.28810691833496094\n",
      "Epoch 2001, Loss: 0.5386397242546082, Final Batch Loss: 0.26760250329971313\n",
      "Epoch 2002, Loss: 0.5170463472604752, Final Batch Loss: 0.21364052593708038\n",
      "Epoch 2003, Loss: 0.554095059633255, Final Batch Loss: 0.30002591013908386\n",
      "Epoch 2004, Loss: 0.50931116938591, Final Batch Loss: 0.21739152073860168\n",
      "Epoch 2005, Loss: 0.5891958177089691, Final Batch Loss: 0.30229541659355164\n",
      "Epoch 2006, Loss: 0.5450976639986038, Final Batch Loss: 0.29925066232681274\n",
      "Epoch 2007, Loss: 0.5881047546863556, Final Batch Loss: 0.33771073818206787\n",
      "Epoch 2008, Loss: 0.5588322281837463, Final Batch Loss: 0.2939256727695465\n",
      "Epoch 2009, Loss: 0.6334330141544342, Final Batch Loss: 0.3485279679298401\n",
      "Epoch 2010, Loss: 0.508767157793045, Final Batch Loss: 0.18725281953811646\n",
      "Epoch 2011, Loss: 0.5521787106990814, Final Batch Loss: 0.29907798767089844\n",
      "Epoch 2012, Loss: 0.5326273590326309, Final Batch Loss: 0.23655129969120026\n",
      "Epoch 2013, Loss: 0.5239233374595642, Final Batch Loss: 0.24765995144844055\n",
      "Epoch 2014, Loss: 0.591659426689148, Final Batch Loss: 0.2958952784538269\n",
      "Epoch 2015, Loss: 0.5581364631652832, Final Batch Loss: 0.2658504247665405\n",
      "Epoch 2016, Loss: 0.5040277540683746, Final Batch Loss: 0.23451316356658936\n",
      "Epoch 2017, Loss: 0.6174706071615219, Final Batch Loss: 0.3819986879825592\n",
      "Epoch 2018, Loss: 0.5991592705249786, Final Batch Loss: 0.32442811131477356\n",
      "Epoch 2019, Loss: 0.6629128754138947, Final Batch Loss: 0.41971275210380554\n",
      "Epoch 2020, Loss: 0.5726972818374634, Final Batch Loss: 0.27911102771759033\n",
      "Epoch 2021, Loss: 0.5334752798080444, Final Batch Loss: 0.265707790851593\n",
      "Epoch 2022, Loss: 0.5568356513977051, Final Batch Loss: 0.26829978823661804\n",
      "Epoch 2023, Loss: 0.5721077919006348, Final Batch Loss: 0.2977283000946045\n",
      "Epoch 2024, Loss: 0.5573953092098236, Final Batch Loss: 0.296147882938385\n",
      "Epoch 2025, Loss: 0.5961297899484634, Final Batch Loss: 0.35901087522506714\n",
      "Epoch 2026, Loss: 0.5848352611064911, Final Batch Loss: 0.3304891586303711\n",
      "Epoch 2027, Loss: 0.5046095699071884, Final Batch Loss: 0.23179815709590912\n",
      "Epoch 2028, Loss: 0.519944429397583, Final Batch Loss: 0.2587405741214752\n",
      "Epoch 2029, Loss: 0.5867782831192017, Final Batch Loss: 0.31856468319892883\n",
      "Epoch 2030, Loss: 0.5762988030910492, Final Batch Loss: 0.3072744607925415\n",
      "Epoch 2031, Loss: 0.5775701701641083, Final Batch Loss: 0.3068610429763794\n",
      "Epoch 2032, Loss: 0.4872381389141083, Final Batch Loss: 0.23463192582130432\n",
      "Epoch 2033, Loss: 0.5794942677021027, Final Batch Loss: 0.3123152554035187\n",
      "Epoch 2034, Loss: 0.5785988122224808, Final Batch Loss: 0.3375433683395386\n",
      "Epoch 2035, Loss: 0.5758861005306244, Final Batch Loss: 0.32529857754707336\n",
      "Epoch 2036, Loss: 0.6160714328289032, Final Batch Loss: 0.3286620080471039\n",
      "Epoch 2037, Loss: 0.6057789623737335, Final Batch Loss: 0.3043617606163025\n",
      "Epoch 2038, Loss: 0.5419721305370331, Final Batch Loss: 0.28470924496650696\n",
      "Epoch 2039, Loss: 0.5545899868011475, Final Batch Loss: 0.2809118926525116\n",
      "Epoch 2040, Loss: 0.5748081207275391, Final Batch Loss: 0.3180984854698181\n",
      "Epoch 2041, Loss: 0.49921077489852905, Final Batch Loss: 0.2277730405330658\n",
      "Epoch 2042, Loss: 0.5340646952390671, Final Batch Loss: 0.23295564949512482\n",
      "Epoch 2043, Loss: 0.5625244528055191, Final Batch Loss: 0.3218739926815033\n",
      "Epoch 2044, Loss: 0.549292653799057, Final Batch Loss: 0.2836330235004425\n",
      "Epoch 2045, Loss: 0.5703713297843933, Final Batch Loss: 0.2789479196071625\n",
      "Epoch 2046, Loss: 0.5699566900730133, Final Batch Loss: 0.3222518265247345\n",
      "Epoch 2047, Loss: 0.6099084317684174, Final Batch Loss: 0.32384881377220154\n",
      "Epoch 2048, Loss: 0.5236471742391586, Final Batch Loss: 0.22957395017147064\n",
      "Epoch 2049, Loss: 0.5760251879692078, Final Batch Loss: 0.2984386682510376\n",
      "Epoch 2050, Loss: 0.5080236345529556, Final Batch Loss: 0.20761965215206146\n",
      "Epoch 2051, Loss: 0.5700855255126953, Final Batch Loss: 0.2917948365211487\n",
      "Epoch 2052, Loss: 0.5874522626399994, Final Batch Loss: 0.2833431661128998\n",
      "Epoch 2053, Loss: 0.528265506029129, Final Batch Loss: 0.26833170652389526\n",
      "Epoch 2054, Loss: 0.5040375739336014, Final Batch Loss: 0.2087647169828415\n",
      "Epoch 2055, Loss: 0.5487718880176544, Final Batch Loss: 0.22913619875907898\n",
      "Epoch 2056, Loss: 0.5921427011489868, Final Batch Loss: 0.3356510400772095\n",
      "Epoch 2057, Loss: 0.5495696514844894, Final Batch Loss: 0.30888116359710693\n",
      "Epoch 2058, Loss: 0.5258984714746475, Final Batch Loss: 0.23981507122516632\n",
      "Epoch 2059, Loss: 0.5402571558952332, Final Batch Loss: 0.2542591691017151\n",
      "Epoch 2060, Loss: 0.5657788515090942, Final Batch Loss: 0.24934649467468262\n",
      "Epoch 2061, Loss: 0.5277133584022522, Final Batch Loss: 0.25755801796913147\n",
      "Epoch 2062, Loss: 0.6187600791454315, Final Batch Loss: 0.3102908730506897\n",
      "Epoch 2063, Loss: 0.5619796812534332, Final Batch Loss: 0.23869654536247253\n",
      "Epoch 2064, Loss: 0.6377486884593964, Final Batch Loss: 0.35231032967567444\n",
      "Epoch 2065, Loss: 0.5700298547744751, Final Batch Loss: 0.28226128220558167\n",
      "Epoch 2066, Loss: 0.5189491212368011, Final Batch Loss: 0.2534337341785431\n",
      "Epoch 2067, Loss: 0.5374431312084198, Final Batch Loss: 0.27498552203178406\n",
      "Epoch 2068, Loss: 0.5916265696287155, Final Batch Loss: 0.34533563256263733\n",
      "Epoch 2069, Loss: 0.5597722828388214, Final Batch Loss: 0.2997998297214508\n",
      "Epoch 2070, Loss: 0.6077568829059601, Final Batch Loss: 0.3380649983882904\n",
      "Epoch 2071, Loss: 0.5244084298610687, Final Batch Loss: 0.28112971782684326\n",
      "Epoch 2072, Loss: 0.5549016892910004, Final Batch Loss: 0.22200730443000793\n",
      "Epoch 2073, Loss: 0.5777125060558319, Final Batch Loss: 0.28758499026298523\n",
      "Epoch 2074, Loss: 0.6171257644891739, Final Batch Loss: 0.3674580454826355\n",
      "Epoch 2075, Loss: 0.492392435669899, Final Batch Loss: 0.2303563505411148\n",
      "Epoch 2076, Loss: 0.5475009679794312, Final Batch Loss: 0.2773430347442627\n",
      "Epoch 2077, Loss: 0.5055665075778961, Final Batch Loss: 0.2252659797668457\n",
      "Epoch 2078, Loss: 0.55523282289505, Final Batch Loss: 0.2748783528804779\n",
      "Epoch 2079, Loss: 0.6091917157173157, Final Batch Loss: 0.339204877614975\n",
      "Epoch 2080, Loss: 0.5884100794792175, Final Batch Loss: 0.3147832751274109\n",
      "Epoch 2081, Loss: 0.5762552618980408, Final Batch Loss: 0.2756005823612213\n",
      "Epoch 2082, Loss: 0.549935907125473, Final Batch Loss: 0.27402931451797485\n",
      "Epoch 2083, Loss: 0.5633449554443359, Final Batch Loss: 0.26065561175346375\n",
      "Epoch 2084, Loss: 0.5790642201900482, Final Batch Loss: 0.2983108460903168\n",
      "Epoch 2085, Loss: 0.5609794855117798, Final Batch Loss: 0.2936437427997589\n",
      "Epoch 2086, Loss: 0.5625267028808594, Final Batch Loss: 0.30028441548347473\n",
      "Epoch 2087, Loss: 0.5652996003627777, Final Batch Loss: 0.26452475786209106\n",
      "Epoch 2088, Loss: 0.5640867352485657, Final Batch Loss: 0.2974805533885956\n",
      "Epoch 2089, Loss: 0.5892890095710754, Final Batch Loss: 0.3243284821510315\n",
      "Epoch 2090, Loss: 0.5337719321250916, Final Batch Loss: 0.27601316571235657\n",
      "Epoch 2091, Loss: 0.6424862146377563, Final Batch Loss: 0.37573006749153137\n",
      "Epoch 2092, Loss: 0.5358504056930542, Final Batch Loss: 0.26190948486328125\n",
      "Epoch 2093, Loss: 0.5093839019536972, Final Batch Loss: 0.22528739273548126\n",
      "Epoch 2094, Loss: 0.5200159847736359, Final Batch Loss: 0.22776848077774048\n",
      "Epoch 2095, Loss: 0.5551234781742096, Final Batch Loss: 0.29645758867263794\n",
      "Epoch 2096, Loss: 0.5812572240829468, Final Batch Loss: 0.31599587202072144\n",
      "Epoch 2097, Loss: 0.5472842752933502, Final Batch Loss: 0.29616156220436096\n",
      "Epoch 2098, Loss: 0.5503489375114441, Final Batch Loss: 0.2790728807449341\n",
      "Epoch 2099, Loss: 0.5817682892084122, Final Batch Loss: 0.3381107449531555\n",
      "Epoch 2100, Loss: 0.5325722992420197, Final Batch Loss: 0.282336950302124\n",
      "Epoch 2101, Loss: 0.5851759314537048, Final Batch Loss: 0.32135775685310364\n",
      "Epoch 2102, Loss: 0.5695388615131378, Final Batch Loss: 0.2578742504119873\n",
      "Epoch 2103, Loss: 0.5563904643058777, Final Batch Loss: 0.26839423179626465\n",
      "Epoch 2104, Loss: 0.5826465487480164, Final Batch Loss: 0.32207536697387695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2105, Loss: 0.5549182891845703, Final Batch Loss: 0.27007606625556946\n",
      "Epoch 2106, Loss: 0.547751784324646, Final Batch Loss: 0.3039959669113159\n",
      "Epoch 2107, Loss: 0.5423287600278854, Final Batch Loss: 0.24152089655399323\n",
      "Epoch 2108, Loss: 0.5788542628288269, Final Batch Loss: 0.33348187804222107\n",
      "Epoch 2109, Loss: 0.5587893426418304, Final Batch Loss: 0.2814916968345642\n",
      "Epoch 2110, Loss: 0.5204913020133972, Final Batch Loss: 0.24745908379554749\n",
      "Epoch 2111, Loss: 0.5695949196815491, Final Batch Loss: 0.2990018129348755\n",
      "Epoch 2112, Loss: 0.5914301574230194, Final Batch Loss: 0.2679036855697632\n",
      "Epoch 2113, Loss: 0.5476921051740646, Final Batch Loss: 0.3086026906967163\n",
      "Epoch 2114, Loss: 0.6427418291568756, Final Batch Loss: 0.28691715002059937\n",
      "Epoch 2115, Loss: 0.548381507396698, Final Batch Loss: 0.2827129364013672\n",
      "Epoch 2116, Loss: 0.5444745421409607, Final Batch Loss: 0.25781163573265076\n",
      "Epoch 2117, Loss: 0.5857146680355072, Final Batch Loss: 0.2530786097049713\n",
      "Epoch 2118, Loss: 0.5308891832828522, Final Batch Loss: 0.23955699801445007\n",
      "Epoch 2119, Loss: 0.5315558165311813, Final Batch Loss: 0.23353828489780426\n",
      "Epoch 2120, Loss: 0.5439496040344238, Final Batch Loss: 0.2982391119003296\n",
      "Epoch 2121, Loss: 0.6029451191425323, Final Batch Loss: 0.3370310366153717\n",
      "Epoch 2122, Loss: 0.5433130264282227, Final Batch Loss: 0.26253649592399597\n",
      "Epoch 2123, Loss: 0.550168514251709, Final Batch Loss: 0.2541930079460144\n",
      "Epoch 2124, Loss: 0.5661211907863617, Final Batch Loss: 0.304146945476532\n",
      "Epoch 2125, Loss: 0.5088942348957062, Final Batch Loss: 0.24480363726615906\n",
      "Epoch 2126, Loss: 0.5486899614334106, Final Batch Loss: 0.2685794532299042\n",
      "Epoch 2127, Loss: 0.5561780035495758, Final Batch Loss: 0.30206426978111267\n",
      "Epoch 2128, Loss: 0.5618828535079956, Final Batch Loss: 0.2510954439640045\n",
      "Epoch 2129, Loss: 0.49728554487228394, Final Batch Loss: 0.24074968695640564\n",
      "Epoch 2130, Loss: 0.5586681365966797, Final Batch Loss: 0.2553877532482147\n",
      "Epoch 2131, Loss: 0.5300705432891846, Final Batch Loss: 0.28836068511009216\n",
      "Epoch 2132, Loss: 0.5125009417533875, Final Batch Loss: 0.25797247886657715\n",
      "Epoch 2133, Loss: 0.5564147233963013, Final Batch Loss: 0.25886428356170654\n",
      "Epoch 2134, Loss: 0.5626195967197418, Final Batch Loss: 0.28551220893859863\n",
      "Epoch 2135, Loss: 0.5708041489124298, Final Batch Loss: 0.31818079948425293\n",
      "Epoch 2136, Loss: 0.5775589197874069, Final Batch Loss: 0.243051216006279\n",
      "Epoch 2137, Loss: 0.5726967453956604, Final Batch Loss: 0.30675461888313293\n",
      "Epoch 2138, Loss: 0.5391871482133865, Final Batch Loss: 0.29378923773765564\n",
      "Epoch 2139, Loss: 0.5790051221847534, Final Batch Loss: 0.2710438370704651\n",
      "Epoch 2140, Loss: 0.5485500246286392, Final Batch Loss: 0.3080906271934509\n",
      "Epoch 2141, Loss: 0.5546817481517792, Final Batch Loss: 0.3019886016845703\n",
      "Epoch 2142, Loss: 0.633496105670929, Final Batch Loss: 0.34902632236480713\n",
      "Epoch 2143, Loss: 0.5721841752529144, Final Batch Loss: 0.27936258912086487\n",
      "Epoch 2144, Loss: 0.5737220048904419, Final Batch Loss: 0.28650566935539246\n",
      "Epoch 2145, Loss: 0.5623734593391418, Final Batch Loss: 0.26800110936164856\n",
      "Epoch 2146, Loss: 0.5671996772289276, Final Batch Loss: 0.2895597219467163\n",
      "Epoch 2147, Loss: 0.5307103395462036, Final Batch Loss: 0.25186312198638916\n",
      "Epoch 2148, Loss: 0.5725556015968323, Final Batch Loss: 0.2830704152584076\n",
      "Epoch 2149, Loss: 0.5115082412958145, Final Batch Loss: 0.23213987052440643\n",
      "Epoch 2150, Loss: 0.5390274226665497, Final Batch Loss: 0.2765139937400818\n",
      "Epoch 2151, Loss: 0.5691438615322113, Final Batch Loss: 0.31557658314704895\n",
      "Epoch 2152, Loss: 0.5231296420097351, Final Batch Loss: 0.2599227726459503\n",
      "Epoch 2153, Loss: 0.5431378781795502, Final Batch Loss: 0.27297791838645935\n",
      "Epoch 2154, Loss: 0.5909113883972168, Final Batch Loss: 0.2796522080898285\n",
      "Epoch 2155, Loss: 0.5340444147586823, Final Batch Loss: 0.26551660895347595\n",
      "Epoch 2156, Loss: 0.49391672015190125, Final Batch Loss: 0.21759018301963806\n",
      "Epoch 2157, Loss: 0.5264136493206024, Final Batch Loss: 0.2433854043483734\n",
      "Epoch 2158, Loss: 0.5389414131641388, Final Batch Loss: 0.2793983817100525\n",
      "Epoch 2159, Loss: 0.5545180439949036, Final Batch Loss: 0.2820250988006592\n",
      "Epoch 2160, Loss: 0.579903781414032, Final Batch Loss: 0.3143877387046814\n",
      "Epoch 2161, Loss: 0.6358742117881775, Final Batch Loss: 0.3432583808898926\n",
      "Epoch 2162, Loss: 0.5463409423828125, Final Batch Loss: 0.2625979781150818\n",
      "Epoch 2163, Loss: 0.5523675084114075, Final Batch Loss: 0.24710333347320557\n",
      "Epoch 2164, Loss: 0.564979687333107, Final Batch Loss: 0.22357209026813507\n",
      "Epoch 2165, Loss: 0.566747635602951, Final Batch Loss: 0.2847058176994324\n",
      "Epoch 2166, Loss: 0.5155830383300781, Final Batch Loss: 0.23650532960891724\n",
      "Epoch 2167, Loss: 0.5436145961284637, Final Batch Loss: 0.27506476640701294\n",
      "Epoch 2168, Loss: 0.5142754018306732, Final Batch Loss: 0.2760620713233948\n",
      "Epoch 2169, Loss: 0.5396699011325836, Final Batch Loss: 0.2599779963493347\n",
      "Epoch 2170, Loss: 0.5520416349172592, Final Batch Loss: 0.31426239013671875\n",
      "Epoch 2171, Loss: 0.5338491797447205, Final Batch Loss: 0.24453657865524292\n",
      "Epoch 2172, Loss: 0.543935239315033, Final Batch Loss: 0.29147276282310486\n",
      "Epoch 2173, Loss: 0.5918403714895248, Final Batch Loss: 0.34484046697616577\n",
      "Epoch 2174, Loss: 0.5921671092510223, Final Batch Loss: 0.30637049674987793\n",
      "Epoch 2175, Loss: 0.5094220042228699, Final Batch Loss: 0.25243568420410156\n",
      "Epoch 2176, Loss: 0.5551211833953857, Final Batch Loss: 0.2646329998970032\n",
      "Epoch 2177, Loss: 0.5348292887210846, Final Batch Loss: 0.2676766812801361\n",
      "Epoch 2178, Loss: 0.5318175554275513, Final Batch Loss: 0.26732337474823\n",
      "Epoch 2179, Loss: 0.5436807870864868, Final Batch Loss: 0.23389694094657898\n",
      "Epoch 2180, Loss: 0.5603023767471313, Final Batch Loss: 0.27416250109672546\n",
      "Epoch 2181, Loss: 0.5196261256933212, Final Batch Loss: 0.24788443744182587\n",
      "Epoch 2182, Loss: 0.5677176415920258, Final Batch Loss: 0.2939597964286804\n",
      "Epoch 2183, Loss: 0.5034606903791428, Final Batch Loss: 0.2551094889640808\n",
      "Epoch 2184, Loss: 0.5145888030529022, Final Batch Loss: 0.23819944262504578\n",
      "Epoch 2185, Loss: 0.5237374603748322, Final Batch Loss: 0.25598031282424927\n",
      "Epoch 2186, Loss: 0.5845376551151276, Final Batch Loss: 0.3066326081752777\n",
      "Epoch 2187, Loss: 0.5068299174308777, Final Batch Loss: 0.20899450778961182\n",
      "Epoch 2188, Loss: 0.5375763475894928, Final Batch Loss: 0.2513725161552429\n",
      "Epoch 2189, Loss: 0.5420196950435638, Final Batch Loss: 0.2723740339279175\n",
      "Epoch 2190, Loss: 0.5192287713289261, Final Batch Loss: 0.225564643740654\n",
      "Epoch 2191, Loss: 0.5694560259580612, Final Batch Loss: 0.338345468044281\n",
      "Epoch 2192, Loss: 0.5633693486452103, Final Batch Loss: 0.3341614305973053\n",
      "Epoch 2193, Loss: 0.5485604405403137, Final Batch Loss: 0.28752535581588745\n",
      "Epoch 2194, Loss: 0.5512392818927765, Final Batch Loss: 0.2695941925048828\n",
      "Epoch 2195, Loss: 0.5820267200469971, Final Batch Loss: 0.30055272579193115\n",
      "Epoch 2196, Loss: 0.5142851024866104, Final Batch Loss: 0.2208433300256729\n",
      "Epoch 2197, Loss: 0.5664443373680115, Final Batch Loss: 0.2610982656478882\n",
      "Epoch 2198, Loss: 0.6097677648067474, Final Batch Loss: 0.3109833002090454\n",
      "Epoch 2199, Loss: 0.5483918786048889, Final Batch Loss: 0.3099648654460907\n",
      "Epoch 2200, Loss: 0.588384598493576, Final Batch Loss: 0.2645036280155182\n",
      "Epoch 2201, Loss: 0.5334293991327286, Final Batch Loss: 0.21834515035152435\n",
      "Epoch 2202, Loss: 0.5497444868087769, Final Batch Loss: 0.25381579995155334\n",
      "Epoch 2203, Loss: 0.5489666163921356, Final Batch Loss: 0.3022841513156891\n",
      "Epoch 2204, Loss: 0.5681974589824677, Final Batch Loss: 0.2613701820373535\n",
      "Epoch 2205, Loss: 0.5948778986930847, Final Batch Loss: 0.3198818564414978\n",
      "Epoch 2206, Loss: 0.5352399349212646, Final Batch Loss: 0.23812571167945862\n",
      "Epoch 2207, Loss: 0.6079646646976471, Final Batch Loss: 0.2875998020172119\n",
      "Epoch 2208, Loss: 0.5600785613059998, Final Batch Loss: 0.3075657784938812\n",
      "Epoch 2209, Loss: 0.5501721501350403, Final Batch Loss: 0.2796473205089569\n",
      "Epoch 2210, Loss: 0.5418644845485687, Final Batch Loss: 0.2875913083553314\n",
      "Epoch 2211, Loss: 0.5782917737960815, Final Batch Loss: 0.30333179235458374\n",
      "Epoch 2212, Loss: 0.5239173620939255, Final Batch Loss: 0.21300368010997772\n",
      "Epoch 2213, Loss: 0.5606178045272827, Final Batch Loss: 0.25553956627845764\n",
      "Epoch 2214, Loss: 0.5571274161338806, Final Batch Loss: 0.29879385232925415\n",
      "Epoch 2215, Loss: 0.5454116463661194, Final Batch Loss: 0.2844206690788269\n",
      "Epoch 2216, Loss: 0.5661424696445465, Final Batch Loss: 0.3088371455669403\n",
      "Epoch 2217, Loss: 0.5161571502685547, Final Batch Loss: 0.24387747049331665\n",
      "Epoch 2218, Loss: 0.5328272581100464, Final Batch Loss: 0.23831552267074585\n",
      "Epoch 2219, Loss: 0.537168100476265, Final Batch Loss: 0.23209954798221588\n",
      "Epoch 2220, Loss: 0.527877926826477, Final Batch Loss: 0.24750173091888428\n",
      "Epoch 2221, Loss: 0.5485392510890961, Final Batch Loss: 0.25978583097457886\n",
      "Epoch 2222, Loss: 0.5410246849060059, Final Batch Loss: 0.2913344204425812\n",
      "Epoch 2223, Loss: 0.5327975749969482, Final Batch Loss: 0.2442343831062317\n",
      "Epoch 2224, Loss: 0.6483944952487946, Final Batch Loss: 0.4354253113269806\n",
      "Epoch 2225, Loss: 0.5176123976707458, Final Batch Loss: 0.2528013586997986\n",
      "Epoch 2226, Loss: 0.5157478749752045, Final Batch Loss: 0.24052736163139343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2227, Loss: 0.5496582537889481, Final Batch Loss: 0.30587804317474365\n",
      "Epoch 2228, Loss: 0.5225252360105515, Final Batch Loss: 0.22960178554058075\n",
      "Epoch 2229, Loss: 0.5492673218250275, Final Batch Loss: 0.2898198366165161\n",
      "Epoch 2230, Loss: 0.48781082034111023, Final Batch Loss: 0.2096288502216339\n",
      "Epoch 2231, Loss: 0.5464075207710266, Final Batch Loss: 0.27158206701278687\n",
      "Epoch 2232, Loss: 0.5562611073255539, Final Batch Loss: 0.23175479471683502\n",
      "Epoch 2233, Loss: 0.584447592496872, Final Batch Loss: 0.2921495735645294\n",
      "Epoch 2234, Loss: 0.6008591949939728, Final Batch Loss: 0.33082401752471924\n",
      "Epoch 2235, Loss: 0.5564267039299011, Final Batch Loss: 0.2949632704257965\n",
      "Epoch 2236, Loss: 0.5352008640766144, Final Batch Loss: 0.2665865421295166\n",
      "Epoch 2237, Loss: 0.5583010911941528, Final Batch Loss: 0.28693896532058716\n",
      "Epoch 2238, Loss: 0.4960643947124481, Final Batch Loss: 0.21809741854667664\n",
      "Epoch 2239, Loss: 0.5379095673561096, Final Batch Loss: 0.2857295572757721\n",
      "Epoch 2240, Loss: 0.5547417104244232, Final Batch Loss: 0.2823486626148224\n",
      "Epoch 2241, Loss: 0.477734312415123, Final Batch Loss: 0.18052376806735992\n",
      "Epoch 2242, Loss: 0.5885781645774841, Final Batch Loss: 0.2775783836841583\n",
      "Epoch 2243, Loss: 0.5197220742702484, Final Batch Loss: 0.23825886845588684\n",
      "Epoch 2244, Loss: 0.5346545279026031, Final Batch Loss: 0.2580024302005768\n",
      "Epoch 2245, Loss: 0.5351621955633163, Final Batch Loss: 0.2944766879081726\n",
      "Epoch 2246, Loss: 0.4964686632156372, Final Batch Loss: 0.2115975320339203\n",
      "Epoch 2247, Loss: 0.5457236170768738, Final Batch Loss: 0.27965277433395386\n",
      "Epoch 2248, Loss: 0.5446755141019821, Final Batch Loss: 0.3025173842906952\n",
      "Epoch 2249, Loss: 0.6534754037857056, Final Batch Loss: 0.3509635627269745\n",
      "Epoch 2250, Loss: 0.5318228751420975, Final Batch Loss: 0.22858570516109467\n",
      "Epoch 2251, Loss: 0.6033300161361694, Final Batch Loss: 0.3370227515697479\n",
      "Epoch 2252, Loss: 0.5332441478967667, Final Batch Loss: 0.22796006500720978\n",
      "Epoch 2253, Loss: 0.5639807879924774, Final Batch Loss: 0.2954789400100708\n",
      "Epoch 2254, Loss: 0.5256226658821106, Final Batch Loss: 0.2040954828262329\n",
      "Epoch 2255, Loss: 0.5140250325202942, Final Batch Loss: 0.24326491355895996\n",
      "Epoch 2256, Loss: 0.5964204967021942, Final Batch Loss: 0.29440179467201233\n",
      "Epoch 2257, Loss: 0.5813681185245514, Final Batch Loss: 0.29466602206230164\n",
      "Epoch 2258, Loss: 0.5356745272874832, Final Batch Loss: 0.30828580260276794\n",
      "Epoch 2259, Loss: 0.5286759734153748, Final Batch Loss: 0.27413299679756165\n",
      "Epoch 2260, Loss: 0.6038736999034882, Final Batch Loss: 0.3449510335922241\n",
      "Epoch 2261, Loss: 0.5805171728134155, Final Batch Loss: 0.31244638562202454\n",
      "Epoch 2262, Loss: 0.5563001036643982, Final Batch Loss: 0.2946195602416992\n",
      "Epoch 2263, Loss: 0.5288301408290863, Final Batch Loss: 0.25036266446113586\n",
      "Epoch 2264, Loss: 0.6312954127788544, Final Batch Loss: 0.3032325804233551\n",
      "Epoch 2265, Loss: 0.5795444250106812, Final Batch Loss: 0.31232500076293945\n",
      "Epoch 2266, Loss: 0.5150772929191589, Final Batch Loss: 0.2638501226902008\n",
      "Epoch 2267, Loss: 0.585928201675415, Final Batch Loss: 0.32048508524894714\n",
      "Epoch 2268, Loss: 0.5617237389087677, Final Batch Loss: 0.262380987405777\n",
      "Epoch 2269, Loss: 0.572437584400177, Final Batch Loss: 0.30336546897888184\n",
      "Epoch 2270, Loss: 0.5433642864227295, Final Batch Loss: 0.29265397787094116\n",
      "Epoch 2271, Loss: 0.5129248797893524, Final Batch Loss: 0.21112075448036194\n",
      "Epoch 2272, Loss: 0.549039363861084, Final Batch Loss: 0.24088847637176514\n",
      "Epoch 2273, Loss: 0.6136190295219421, Final Batch Loss: 0.3240932524204254\n",
      "Epoch 2274, Loss: 0.5949679911136627, Final Batch Loss: 0.3425520956516266\n",
      "Epoch 2275, Loss: 0.5356304943561554, Final Batch Loss: 0.2633160650730133\n",
      "Epoch 2276, Loss: 0.5756796598434448, Final Batch Loss: 0.25143754482269287\n",
      "Epoch 2277, Loss: 0.5733993053436279, Final Batch Loss: 0.31145089864730835\n",
      "Epoch 2278, Loss: 0.5546466112136841, Final Batch Loss: 0.286765992641449\n",
      "Epoch 2279, Loss: 0.5518111139535904, Final Batch Loss: 0.31720489263534546\n",
      "Epoch 2280, Loss: 0.5104374438524246, Final Batch Loss: 0.19505099952220917\n",
      "Epoch 2281, Loss: 0.5323111116886139, Final Batch Loss: 0.24867931008338928\n",
      "Epoch 2282, Loss: 0.5846382528543472, Final Batch Loss: 0.3595007658004761\n",
      "Epoch 2283, Loss: 0.4896273910999298, Final Batch Loss: 0.22477781772613525\n",
      "Epoch 2284, Loss: 0.552825003862381, Final Batch Loss: 0.29084286093711853\n",
      "Epoch 2285, Loss: 0.5450333505868912, Final Batch Loss: 0.24629731476306915\n",
      "Epoch 2286, Loss: 0.5400253236293793, Final Batch Loss: 0.27295005321502686\n",
      "Epoch 2287, Loss: 0.5335463136434555, Final Batch Loss: 0.28453171253204346\n",
      "Epoch 2288, Loss: 0.546737790107727, Final Batch Loss: 0.3090727627277374\n",
      "Epoch 2289, Loss: 0.5731894671916962, Final Batch Loss: 0.31053388118743896\n",
      "Epoch 2290, Loss: 0.5857631862163544, Final Batch Loss: 0.32346776127815247\n",
      "Epoch 2291, Loss: 0.5699416995048523, Final Batch Loss: 0.31484612822532654\n",
      "Epoch 2292, Loss: 0.6043083667755127, Final Batch Loss: 0.3503366708755493\n",
      "Epoch 2293, Loss: 0.5750335454940796, Final Batch Loss: 0.30608630180358887\n",
      "Epoch 2294, Loss: 0.5189492702484131, Final Batch Loss: 0.21068903803825378\n",
      "Epoch 2295, Loss: 0.6532023549079895, Final Batch Loss: 0.3731839954853058\n",
      "Epoch 2296, Loss: 0.5179249793291092, Final Batch Loss: 0.2743535339832306\n",
      "Epoch 2297, Loss: 0.5793377459049225, Final Batch Loss: 0.3323151171207428\n",
      "Epoch 2298, Loss: 0.5475422441959381, Final Batch Loss: 0.27263835072517395\n",
      "Epoch 2299, Loss: 0.5151239335536957, Final Batch Loss: 0.2447459101676941\n",
      "Epoch 2300, Loss: 0.5219953805208206, Final Batch Loss: 0.2790663242340088\n",
      "Epoch 2301, Loss: 0.5863828063011169, Final Batch Loss: 0.31079772114753723\n",
      "Epoch 2302, Loss: 0.5248996466398239, Final Batch Loss: 0.28963401913642883\n",
      "Epoch 2303, Loss: 0.600697934627533, Final Batch Loss: 0.3483024537563324\n",
      "Epoch 2304, Loss: 0.5414590984582901, Final Batch Loss: 0.24099241197109222\n",
      "Epoch 2305, Loss: 0.506891742348671, Final Batch Loss: 0.2462664395570755\n",
      "Epoch 2306, Loss: 0.5093911290168762, Final Batch Loss: 0.23457035422325134\n",
      "Epoch 2307, Loss: 0.5543093681335449, Final Batch Loss: 0.27238714694976807\n",
      "Epoch 2308, Loss: 0.5580516755580902, Final Batch Loss: 0.30083775520324707\n",
      "Epoch 2309, Loss: 0.5283739864826202, Final Batch Loss: 0.2353227138519287\n",
      "Epoch 2310, Loss: 0.5356511175632477, Final Batch Loss: 0.2570943236351013\n",
      "Epoch 2311, Loss: 0.5422342717647552, Final Batch Loss: 0.2721802890300751\n",
      "Epoch 2312, Loss: 0.5790902376174927, Final Batch Loss: 0.3267536163330078\n",
      "Epoch 2313, Loss: 0.5407840013504028, Final Batch Loss: 0.27072271704673767\n",
      "Epoch 2314, Loss: 0.5265236794948578, Final Batch Loss: 0.26007404923439026\n",
      "Epoch 2315, Loss: 0.5539207756519318, Final Batch Loss: 0.2684420347213745\n",
      "Epoch 2316, Loss: 0.533565491437912, Final Batch Loss: 0.2553130090236664\n",
      "Epoch 2317, Loss: 0.5360263586044312, Final Batch Loss: 0.2621811330318451\n",
      "Epoch 2318, Loss: 0.560989260673523, Final Batch Loss: 0.30673661828041077\n",
      "Epoch 2319, Loss: 0.5564427077770233, Final Batch Loss: 0.2772921919822693\n",
      "Epoch 2320, Loss: 0.5329126417636871, Final Batch Loss: 0.2553477883338928\n",
      "Epoch 2321, Loss: 0.523249477148056, Final Batch Loss: 0.2575360834598541\n",
      "Epoch 2322, Loss: 0.5314876735210419, Final Batch Loss: 0.26773762702941895\n",
      "Epoch 2323, Loss: 0.5572479665279388, Final Batch Loss: 0.23762348294258118\n",
      "Epoch 2324, Loss: 0.5494562089443207, Final Batch Loss: 0.2128840982913971\n",
      "Epoch 2325, Loss: 0.55649134516716, Final Batch Loss: 0.28629347681999207\n",
      "Epoch 2326, Loss: 0.5294958204030991, Final Batch Loss: 0.23433025181293488\n",
      "Epoch 2327, Loss: 0.5303647667169571, Final Batch Loss: 0.24411042034626007\n",
      "Epoch 2328, Loss: 0.546178787946701, Final Batch Loss: 0.27143174409866333\n",
      "Epoch 2329, Loss: 0.5157178640365601, Final Batch Loss: 0.22394362092018127\n",
      "Epoch 2330, Loss: 0.5164772421121597, Final Batch Loss: 0.2725125253200531\n",
      "Epoch 2331, Loss: 0.5263815820217133, Final Batch Loss: 0.2909174859523773\n",
      "Epoch 2332, Loss: 0.5172150731086731, Final Batch Loss: 0.24951857328414917\n",
      "Epoch 2333, Loss: 0.5653166472911835, Final Batch Loss: 0.28549057245254517\n",
      "Epoch 2334, Loss: 0.571015477180481, Final Batch Loss: 0.298898845911026\n",
      "Epoch 2335, Loss: 0.52442467212677, Final Batch Loss: 0.23316708207130432\n",
      "Epoch 2336, Loss: 0.5133990943431854, Final Batch Loss: 0.25777360796928406\n",
      "Epoch 2337, Loss: 0.5508532226085663, Final Batch Loss: 0.27404123544692993\n",
      "Epoch 2338, Loss: 0.5586162209510803, Final Batch Loss: 0.2597903907299042\n",
      "Epoch 2339, Loss: 0.5609790086746216, Final Batch Loss: 0.28384512662887573\n",
      "Epoch 2340, Loss: 0.5466178059577942, Final Batch Loss: 0.24539393186569214\n",
      "Epoch 2341, Loss: 0.5623560845851898, Final Batch Loss: 0.3086673617362976\n",
      "Epoch 2342, Loss: 0.5334446132183075, Final Batch Loss: 0.2764674127101898\n",
      "Epoch 2343, Loss: 0.5402684807777405, Final Batch Loss: 0.2615474760532379\n",
      "Epoch 2344, Loss: 0.5311005562543869, Final Batch Loss: 0.24601690471172333\n",
      "Epoch 2345, Loss: 0.5497832894325256, Final Batch Loss: 0.24733597040176392\n",
      "Epoch 2346, Loss: 0.5310587286949158, Final Batch Loss: 0.22526243329048157\n",
      "Epoch 2347, Loss: 0.5364163219928741, Final Batch Loss: 0.2797185182571411\n",
      "Epoch 2348, Loss: 0.5779271125793457, Final Batch Loss: 0.29231345653533936\n",
      "Epoch 2349, Loss: 0.5083978176116943, Final Batch Loss: 0.256695419549942\n",
      "Epoch 2350, Loss: 0.5455234050750732, Final Batch Loss: 0.28983351588249207\n",
      "Epoch 2351, Loss: 0.5181834399700165, Final Batch Loss: 0.26618465781211853\n",
      "Epoch 2352, Loss: 0.5583597719669342, Final Batch Loss: 0.31893593072891235\n",
      "Epoch 2353, Loss: 0.5446120202541351, Final Batch Loss: 0.25374719500541687\n",
      "Epoch 2354, Loss: 0.5417685806751251, Final Batch Loss: 0.2623246908187866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2355, Loss: 0.542633980512619, Final Batch Loss: 0.2810805141925812\n",
      "Epoch 2356, Loss: 0.5291769504547119, Final Batch Loss: 0.24144530296325684\n",
      "Epoch 2357, Loss: 0.5482105612754822, Final Batch Loss: 0.24532189965248108\n",
      "Epoch 2358, Loss: 0.5547191500663757, Final Batch Loss: 0.2964995503425598\n",
      "Epoch 2359, Loss: 0.5302388370037079, Final Batch Loss: 0.269479900598526\n",
      "Epoch 2360, Loss: 0.6286898553371429, Final Batch Loss: 0.40000778436660767\n",
      "Epoch 2361, Loss: 0.573045015335083, Final Batch Loss: 0.26030564308166504\n",
      "Epoch 2362, Loss: 0.5179869532585144, Final Batch Loss: 0.2691055238246918\n",
      "Epoch 2363, Loss: 0.5087407231330872, Final Batch Loss: 0.21341589093208313\n",
      "Epoch 2364, Loss: 0.556306779384613, Final Batch Loss: 0.3054441213607788\n",
      "Epoch 2365, Loss: 0.5216449797153473, Final Batch Loss: 0.24671819806098938\n",
      "Epoch 2366, Loss: 0.5600070357322693, Final Batch Loss: 0.2895645797252655\n",
      "Epoch 2367, Loss: 0.5594681948423386, Final Batch Loss: 0.3196122348308563\n",
      "Epoch 2368, Loss: 0.5241386890411377, Final Batch Loss: 0.24623233079910278\n",
      "Epoch 2369, Loss: 0.4980514347553253, Final Batch Loss: 0.22555455565452576\n",
      "Epoch 2370, Loss: 0.5365151017904282, Final Batch Loss: 0.23383234441280365\n",
      "Epoch 2371, Loss: 0.510646402835846, Final Batch Loss: 0.26042255759239197\n",
      "Epoch 2372, Loss: 0.568058118224144, Final Batch Loss: 0.32113999128341675\n",
      "Epoch 2373, Loss: 0.512669637799263, Final Batch Loss: 0.19957853853702545\n",
      "Epoch 2374, Loss: 0.5281887799501419, Final Batch Loss: 0.295027494430542\n",
      "Epoch 2375, Loss: 0.575019508600235, Final Batch Loss: 0.2763842046260834\n",
      "Epoch 2376, Loss: 0.5367049276828766, Final Batch Loss: 0.2673491835594177\n",
      "Epoch 2377, Loss: 0.5417385995388031, Final Batch Loss: 0.2812719941139221\n",
      "Epoch 2378, Loss: 0.5055159032344818, Final Batch Loss: 0.23197057843208313\n",
      "Epoch 2379, Loss: 0.5436301827430725, Final Batch Loss: 0.27812111377716064\n",
      "Epoch 2380, Loss: 0.5666496455669403, Final Batch Loss: 0.3055340051651001\n",
      "Epoch 2381, Loss: 0.6196379959583282, Final Batch Loss: 0.3353753387928009\n",
      "Epoch 2382, Loss: 0.5342710316181183, Final Batch Loss: 0.25437867641448975\n",
      "Epoch 2383, Loss: 0.5455543100833893, Final Batch Loss: 0.3045545518398285\n",
      "Epoch 2384, Loss: 0.6087796986103058, Final Batch Loss: 0.3147686719894409\n",
      "Epoch 2385, Loss: 0.5447136759757996, Final Batch Loss: 0.2751188278198242\n",
      "Epoch 2386, Loss: 0.5295828282833099, Final Batch Loss: 0.25224700570106506\n",
      "Epoch 2387, Loss: 0.5202962458133698, Final Batch Loss: 0.2596697211265564\n",
      "Epoch 2388, Loss: 0.5548890829086304, Final Batch Loss: 0.3021601736545563\n",
      "Epoch 2389, Loss: 0.5186174511909485, Final Batch Loss: 0.23586517572402954\n",
      "Epoch 2390, Loss: 0.5300733745098114, Final Batch Loss: 0.26440054178237915\n",
      "Epoch 2391, Loss: 0.5491269826889038, Final Batch Loss: 0.28909745812416077\n",
      "Epoch 2392, Loss: 0.5649647414684296, Final Batch Loss: 0.3127511143684387\n",
      "Epoch 2393, Loss: 0.4980795681476593, Final Batch Loss: 0.2371576726436615\n",
      "Epoch 2394, Loss: 0.5293163657188416, Final Batch Loss: 0.2564653158187866\n",
      "Epoch 2395, Loss: 0.5523387789726257, Final Batch Loss: 0.25206977128982544\n",
      "Epoch 2396, Loss: 0.5111250281333923, Final Batch Loss: 0.2522376775741577\n",
      "Epoch 2397, Loss: 0.5193758457899094, Final Batch Loss: 0.293133407831192\n",
      "Epoch 2398, Loss: 0.5498041063547134, Final Batch Loss: 0.32595294713974\n",
      "Epoch 2399, Loss: 0.5053279548883438, Final Batch Loss: 0.2359236627817154\n",
      "Epoch 2400, Loss: 0.5424759685993195, Final Batch Loss: 0.2886942923069\n",
      "Epoch 2401, Loss: 0.5764721035957336, Final Batch Loss: 0.2869637608528137\n",
      "Epoch 2402, Loss: 0.5474168658256531, Final Batch Loss: 0.2507267892360687\n",
      "Epoch 2403, Loss: 0.4896111786365509, Final Batch Loss: 0.16508761048316956\n",
      "Epoch 2404, Loss: 0.5441973507404327, Final Batch Loss: 0.28954729437828064\n",
      "Epoch 2405, Loss: 0.5942654311656952, Final Batch Loss: 0.3293153941631317\n",
      "Epoch 2406, Loss: 0.5875819623470306, Final Batch Loss: 0.33215975761413574\n",
      "Epoch 2407, Loss: 0.5636476874351501, Final Batch Loss: 0.3085600435733795\n",
      "Epoch 2408, Loss: 0.5235162079334259, Final Batch Loss: 0.24806630611419678\n",
      "Epoch 2409, Loss: 0.6209986805915833, Final Batch Loss: 0.3325574994087219\n",
      "Epoch 2410, Loss: 0.5342065244913101, Final Batch Loss: 0.23209427297115326\n",
      "Epoch 2411, Loss: 0.5230981260538101, Final Batch Loss: 0.22014449536800385\n",
      "Epoch 2412, Loss: 0.549730658531189, Final Batch Loss: 0.26011574268341064\n",
      "Epoch 2413, Loss: 0.5243921279907227, Final Batch Loss: 0.274122029542923\n",
      "Epoch 2414, Loss: 0.5280547738075256, Final Batch Loss: 0.2504866123199463\n",
      "Epoch 2415, Loss: 0.4968305677175522, Final Batch Loss: 0.21406565606594086\n",
      "Epoch 2416, Loss: 0.5521505922079086, Final Batch Loss: 0.22960256040096283\n",
      "Epoch 2417, Loss: 0.5288134217262268, Final Batch Loss: 0.26732951402664185\n",
      "Epoch 2418, Loss: 0.5499933809041977, Final Batch Loss: 0.2458489090204239\n",
      "Epoch 2419, Loss: 0.5328046083450317, Final Batch Loss: 0.2901615500450134\n",
      "Epoch 2420, Loss: 0.5634116530418396, Final Batch Loss: 0.3097456395626068\n",
      "Epoch 2421, Loss: 0.5283773839473724, Final Batch Loss: 0.25849035382270813\n",
      "Epoch 2422, Loss: 0.6268162429332733, Final Batch Loss: 0.31644707918167114\n",
      "Epoch 2423, Loss: 0.5477080941200256, Final Batch Loss: 0.2877276539802551\n",
      "Epoch 2424, Loss: 0.6293640434741974, Final Batch Loss: 0.3784652352333069\n",
      "Epoch 2425, Loss: 0.5171280801296234, Final Batch Loss: 0.25423604249954224\n",
      "Epoch 2426, Loss: 0.538935586810112, Final Batch Loss: 0.2490609735250473\n",
      "Epoch 2427, Loss: 0.5366340279579163, Final Batch Loss: 0.2642506957054138\n",
      "Epoch 2428, Loss: 0.5497749149799347, Final Batch Loss: 0.2780066132545471\n",
      "Epoch 2429, Loss: 0.5709424912929535, Final Batch Loss: 0.31645461916923523\n",
      "Epoch 2430, Loss: 0.4999450445175171, Final Batch Loss: 0.2561274766921997\n",
      "Epoch 2431, Loss: 0.5861933529376984, Final Batch Loss: 0.31704777479171753\n",
      "Epoch 2432, Loss: 0.5507649779319763, Final Batch Loss: 0.2869166135787964\n",
      "Epoch 2433, Loss: 0.486873596906662, Final Batch Loss: 0.20430338382720947\n",
      "Epoch 2434, Loss: 0.5171423256397247, Final Batch Loss: 0.2105599045753479\n",
      "Epoch 2435, Loss: 0.5256607383489609, Final Batch Loss: 0.23132474720478058\n",
      "Epoch 2436, Loss: 0.5004467517137527, Final Batch Loss: 0.24787576496601105\n",
      "Epoch 2437, Loss: 0.48836947977542877, Final Batch Loss: 0.22884495556354523\n",
      "Epoch 2438, Loss: 0.5553766191005707, Final Batch Loss: 0.3008224070072174\n",
      "Epoch 2439, Loss: 0.5197301357984543, Final Batch Loss: 0.2252095490694046\n",
      "Epoch 2440, Loss: 0.5612927675247192, Final Batch Loss: 0.29465195536613464\n",
      "Epoch 2441, Loss: 0.599359393119812, Final Batch Loss: 0.33929574489593506\n",
      "Epoch 2442, Loss: 0.5143154859542847, Final Batch Loss: 0.27063292264938354\n",
      "Epoch 2443, Loss: 0.6040018200874329, Final Batch Loss: 0.34838640689849854\n",
      "Epoch 2444, Loss: 0.5091857463121414, Final Batch Loss: 0.2609406113624573\n",
      "Epoch 2445, Loss: 0.4964253306388855, Final Batch Loss: 0.2115401029586792\n",
      "Epoch 2446, Loss: 0.524934709072113, Final Batch Loss: 0.2613464295864105\n",
      "Epoch 2447, Loss: 0.5611984133720398, Final Batch Loss: 0.28931471705436707\n",
      "Epoch 2448, Loss: 0.5154968649148941, Final Batch Loss: 0.245291605591774\n",
      "Epoch 2449, Loss: 0.53605517745018, Final Batch Loss: 0.2743643522262573\n",
      "Epoch 2450, Loss: 0.563380777835846, Final Batch Loss: 0.287850558757782\n",
      "Epoch 2451, Loss: 0.5439765900373459, Final Batch Loss: 0.3173397183418274\n",
      "Epoch 2452, Loss: 0.4941808581352234, Final Batch Loss: 0.22032782435417175\n",
      "Epoch 2453, Loss: 0.6052631437778473, Final Batch Loss: 0.3306617736816406\n",
      "Epoch 2454, Loss: 0.5214481055736542, Final Batch Loss: 0.25880229473114014\n",
      "Epoch 2455, Loss: 0.5195894092321396, Final Batch Loss: 0.24508346617221832\n",
      "Epoch 2456, Loss: 0.5461701154708862, Final Batch Loss: 0.27315765619277954\n",
      "Epoch 2457, Loss: 0.5809272676706314, Final Batch Loss: 0.34185200929641724\n",
      "Epoch 2458, Loss: 0.5060887038707733, Final Batch Loss: 0.22424980998039246\n",
      "Epoch 2459, Loss: 0.5122538059949875, Final Batch Loss: 0.2647956609725952\n",
      "Epoch 2460, Loss: 0.5624826401472092, Final Batch Loss: 0.3370950520038605\n",
      "Epoch 2461, Loss: 0.5671986937522888, Final Batch Loss: 0.30103543400764465\n",
      "Epoch 2462, Loss: 0.5200572907924652, Final Batch Loss: 0.24242687225341797\n",
      "Epoch 2463, Loss: 0.5346564948558807, Final Batch Loss: 0.2843807339668274\n",
      "Epoch 2464, Loss: 0.5360720455646515, Final Batch Loss: 0.27021804451942444\n",
      "Epoch 2465, Loss: 0.5422635674476624, Final Batch Loss: 0.23780065774917603\n",
      "Epoch 2466, Loss: 0.5425858795642853, Final Batch Loss: 0.27550238370895386\n",
      "Epoch 2467, Loss: 0.5655882954597473, Final Batch Loss: 0.26971131563186646\n",
      "Epoch 2468, Loss: 0.5114990174770355, Final Batch Loss: 0.2257697880268097\n",
      "Epoch 2469, Loss: 0.5441069006919861, Final Batch Loss: 0.258538156747818\n",
      "Epoch 2470, Loss: 0.5261285901069641, Final Batch Loss: 0.2599462568759918\n",
      "Epoch 2471, Loss: 0.5076478719711304, Final Batch Loss: 0.2616529166698456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2472, Loss: 0.5653980225324631, Final Batch Loss: 0.3505537509918213\n",
      "Epoch 2473, Loss: 0.5437068045139313, Final Batch Loss: 0.26043519377708435\n",
      "Epoch 2474, Loss: 0.5513234734535217, Final Batch Loss: 0.30407094955444336\n",
      "Epoch 2475, Loss: 0.5548265278339386, Final Batch Loss: 0.2899363040924072\n",
      "Epoch 2476, Loss: 0.5583748817443848, Final Batch Loss: 0.2668590247631073\n",
      "Epoch 2477, Loss: 0.5304127633571625, Final Batch Loss: 0.2680489122867584\n",
      "Epoch 2478, Loss: 0.5724478960037231, Final Batch Loss: 0.3191353380680084\n",
      "Epoch 2479, Loss: 0.6091930568218231, Final Batch Loss: 0.3187822103500366\n",
      "Epoch 2480, Loss: 0.5281606614589691, Final Batch Loss: 0.26251909136772156\n",
      "Epoch 2481, Loss: 0.5572423934936523, Final Batch Loss: 0.2931540012359619\n",
      "Epoch 2482, Loss: 0.543410062789917, Final Batch Loss: 0.27454236149787903\n",
      "Epoch 2483, Loss: 0.5539261996746063, Final Batch Loss: 0.2590370178222656\n",
      "Epoch 2484, Loss: 0.543411135673523, Final Batch Loss: 0.2988300621509552\n",
      "Epoch 2485, Loss: 0.5297194123268127, Final Batch Loss: 0.21994644403457642\n",
      "Epoch 2486, Loss: 0.5768417418003082, Final Batch Loss: 0.3383364677429199\n",
      "Epoch 2487, Loss: 0.6239402890205383, Final Batch Loss: 0.3529195487499237\n",
      "Epoch 2488, Loss: 0.5235000550746918, Final Batch Loss: 0.24856558442115784\n",
      "Epoch 2489, Loss: 0.5428551435470581, Final Batch Loss: 0.2990056276321411\n",
      "Epoch 2490, Loss: 0.558690756559372, Final Batch Loss: 0.29976725578308105\n",
      "Epoch 2491, Loss: 0.5456182360649109, Final Batch Loss: 0.27208104729652405\n",
      "Epoch 2492, Loss: 0.5458424687385559, Final Batch Loss: 0.28460121154785156\n",
      "Epoch 2493, Loss: 0.4898965060710907, Final Batch Loss: 0.22236767411231995\n",
      "Epoch 2494, Loss: 0.5513401925563812, Final Batch Loss: 0.28078484535217285\n",
      "Epoch 2495, Loss: 0.5481139421463013, Final Batch Loss: 0.31115370988845825\n",
      "Epoch 2496, Loss: 0.5839471817016602, Final Batch Loss: 0.31773364543914795\n",
      "Epoch 2497, Loss: 0.5313141047954559, Final Batch Loss: 0.24657952785491943\n",
      "Epoch 2498, Loss: 0.5510147958993912, Final Batch Loss: 0.30400338768959045\n",
      "Epoch 2499, Loss: 0.5641955733299255, Final Batch Loss: 0.3474899232387543\n",
      "Epoch 2500, Loss: 0.5049043148756027, Final Batch Loss: 0.25738051533699036\n",
      "Epoch 2501, Loss: 0.5122013837099075, Final Batch Loss: 0.21021361649036407\n",
      "Epoch 2502, Loss: 0.59953573346138, Final Batch Loss: 0.3003745675086975\n",
      "Epoch 2503, Loss: 0.5203965604305267, Final Batch Loss: 0.25921881198883057\n",
      "Epoch 2504, Loss: 0.5317114144563675, Final Batch Loss: 0.28957799077033997\n",
      "Epoch 2505, Loss: 0.5990429222583771, Final Batch Loss: 0.32953059673309326\n",
      "Epoch 2506, Loss: 0.5729358941316605, Final Batch Loss: 0.24154101312160492\n",
      "Epoch 2507, Loss: 0.530764102935791, Final Batch Loss: 0.25207310914993286\n",
      "Epoch 2508, Loss: 0.5071699172258377, Final Batch Loss: 0.24728117883205414\n",
      "Epoch 2509, Loss: 0.5915467739105225, Final Batch Loss: 0.3431686758995056\n",
      "Epoch 2510, Loss: 0.5126716047525406, Final Batch Loss: 0.24196775257587433\n",
      "Epoch 2511, Loss: 0.5442083925008774, Final Batch Loss: 0.23838452994823456\n",
      "Epoch 2512, Loss: 0.5222953259944916, Final Batch Loss: 0.21711164712905884\n",
      "Epoch 2513, Loss: 0.5119378119707108, Final Batch Loss: 0.223030224442482\n",
      "Epoch 2514, Loss: 0.5424148291349411, Final Batch Loss: 0.31554800271987915\n",
      "Epoch 2515, Loss: 0.5479276180267334, Final Batch Loss: 0.25353729724884033\n",
      "Epoch 2516, Loss: 0.5778194069862366, Final Batch Loss: 0.2624368369579315\n",
      "Epoch 2517, Loss: 0.5250204205513, Final Batch Loss: 0.26460540294647217\n",
      "Epoch 2518, Loss: 0.5284881591796875, Final Batch Loss: 0.2853604853153229\n",
      "Epoch 2519, Loss: 0.5362302958965302, Final Batch Loss: 0.2796823978424072\n",
      "Epoch 2520, Loss: 0.537534087896347, Final Batch Loss: 0.26784032583236694\n",
      "Epoch 2521, Loss: 0.5595306158065796, Final Batch Loss: 0.2907460331916809\n",
      "Epoch 2522, Loss: 0.47911109030246735, Final Batch Loss: 0.20954696834087372\n",
      "Epoch 2523, Loss: 0.5387390553951263, Final Batch Loss: 0.2771266996860504\n",
      "Epoch 2524, Loss: 0.5385395437479019, Final Batch Loss: 0.2987329661846161\n",
      "Epoch 2525, Loss: 0.5774985551834106, Final Batch Loss: 0.3261677622795105\n",
      "Epoch 2526, Loss: 0.5309427827596664, Final Batch Loss: 0.28590127825737\n",
      "Epoch 2527, Loss: 0.5419566333293915, Final Batch Loss: 0.29782748222351074\n",
      "Epoch 2528, Loss: 0.5314909815788269, Final Batch Loss: 0.2938498854637146\n",
      "Epoch 2529, Loss: 0.5071115046739578, Final Batch Loss: 0.26105988025665283\n",
      "Epoch 2530, Loss: 0.5289081037044525, Final Batch Loss: 0.27224302291870117\n",
      "Epoch 2531, Loss: 0.49186545610427856, Final Batch Loss: 0.2213502824306488\n",
      "Epoch 2532, Loss: 0.509779542684555, Final Batch Loss: 0.2667609751224518\n",
      "Epoch 2533, Loss: 0.5408828109502792, Final Batch Loss: 0.2988618314266205\n",
      "Epoch 2534, Loss: 0.5016254037618637, Final Batch Loss: 0.19333641231060028\n",
      "Epoch 2535, Loss: 0.5245859026908875, Final Batch Loss: 0.24744382500648499\n",
      "Epoch 2536, Loss: 0.4662202149629593, Final Batch Loss: 0.18861053884029388\n",
      "Epoch 2537, Loss: 0.49632760882377625, Final Batch Loss: 0.229450523853302\n",
      "Epoch 2538, Loss: 0.5750529170036316, Final Batch Loss: 0.3165642321109772\n",
      "Epoch 2539, Loss: 0.5271370708942413, Final Batch Loss: 0.23682114481925964\n",
      "Epoch 2540, Loss: 0.5041272044181824, Final Batch Loss: 0.2376670241355896\n",
      "Epoch 2541, Loss: 0.488007053732872, Final Batch Loss: 0.21158795058727264\n",
      "Epoch 2542, Loss: 0.5021388977766037, Final Batch Loss: 0.20879758894443512\n",
      "Epoch 2543, Loss: 0.5234933793544769, Final Batch Loss: 0.2577115297317505\n",
      "Epoch 2544, Loss: 0.5248771607875824, Final Batch Loss: 0.2573454678058624\n",
      "Epoch 2545, Loss: 0.52403923869133, Final Batch Loss: 0.2912023365497589\n",
      "Epoch 2546, Loss: 0.5738644003868103, Final Batch Loss: 0.22302448749542236\n",
      "Epoch 2547, Loss: 0.5576832294464111, Final Batch Loss: 0.31084635853767395\n",
      "Epoch 2548, Loss: 0.5806667804718018, Final Batch Loss: 0.3188317120075226\n",
      "Epoch 2549, Loss: 0.5492561161518097, Final Batch Loss: 0.2977846562862396\n",
      "Epoch 2550, Loss: 0.5331979990005493, Final Batch Loss: 0.2968555986881256\n",
      "Epoch 2551, Loss: 0.5063982307910919, Final Batch Loss: 0.27809953689575195\n",
      "Epoch 2552, Loss: 0.5808488130569458, Final Batch Loss: 0.32998549938201904\n",
      "Epoch 2553, Loss: 0.5370914936065674, Final Batch Loss: 0.27035415172576904\n",
      "Epoch 2554, Loss: 0.5017982125282288, Final Batch Loss: 0.2529773712158203\n",
      "Epoch 2555, Loss: 0.47834958136081696, Final Batch Loss: 0.20036731660366058\n",
      "Epoch 2556, Loss: 0.5479043424129486, Final Batch Loss: 0.2953049838542938\n",
      "Epoch 2557, Loss: 0.5170810222625732, Final Batch Loss: 0.26535913348197937\n",
      "Epoch 2558, Loss: 0.5179280936717987, Final Batch Loss: 0.25198763608932495\n",
      "Epoch 2559, Loss: 0.5726550966501236, Final Batch Loss: 0.32688215374946594\n",
      "Epoch 2560, Loss: 0.5294377505779266, Final Batch Loss: 0.25329992175102234\n",
      "Epoch 2561, Loss: 0.5108137279748917, Final Batch Loss: 0.2322656363248825\n",
      "Epoch 2562, Loss: 0.6024990379810333, Final Batch Loss: 0.2974899709224701\n",
      "Epoch 2563, Loss: 0.5171938538551331, Final Batch Loss: 0.24559110403060913\n",
      "Epoch 2564, Loss: 0.474889874458313, Final Batch Loss: 0.18594816327095032\n",
      "Epoch 2565, Loss: 0.5131472647190094, Final Batch Loss: 0.22665199637413025\n",
      "Epoch 2566, Loss: 0.5446036905050278, Final Batch Loss: 0.19820959866046906\n",
      "Epoch 2567, Loss: 0.5159881711006165, Final Batch Loss: 0.26344379782676697\n",
      "Epoch 2568, Loss: 0.6021190285682678, Final Batch Loss: 0.32334619760513306\n",
      "Epoch 2569, Loss: 0.560277134180069, Final Batch Loss: 0.28319400548934937\n",
      "Epoch 2570, Loss: 0.551793172955513, Final Batch Loss: 0.31884944438934326\n",
      "Epoch 2571, Loss: 0.5246178358793259, Final Batch Loss: 0.29015618562698364\n",
      "Epoch 2572, Loss: 0.5126050114631653, Final Batch Loss: 0.26371991634368896\n",
      "Epoch 2573, Loss: 0.5415160357952118, Final Batch Loss: 0.2744491398334503\n",
      "Epoch 2574, Loss: 0.5703339874744415, Final Batch Loss: 0.32375267148017883\n",
      "Epoch 2575, Loss: 0.5024184286594391, Final Batch Loss: 0.23302584886550903\n",
      "Epoch 2576, Loss: 0.5431855320930481, Final Batch Loss: 0.2869736850261688\n",
      "Epoch 2577, Loss: 0.52137491106987, Final Batch Loss: 0.2522026300430298\n",
      "Epoch 2578, Loss: 0.5552427470684052, Final Batch Loss: 0.28900331258773804\n",
      "Epoch 2579, Loss: 0.5111672431230545, Final Batch Loss: 0.2664952576160431\n",
      "Epoch 2580, Loss: 0.5005353093147278, Final Batch Loss: 0.2337024211883545\n",
      "Epoch 2581, Loss: 0.5141418278217316, Final Batch Loss: 0.22469431161880493\n",
      "Epoch 2582, Loss: 0.5217378288507462, Final Batch Loss: 0.24550826847553253\n",
      "Epoch 2583, Loss: 0.5458987802267075, Final Batch Loss: 0.2477046102285385\n",
      "Epoch 2584, Loss: 0.5235997140407562, Final Batch Loss: 0.2516685426235199\n",
      "Epoch 2585, Loss: 0.5380613654851913, Final Batch Loss: 0.22444875538349152\n",
      "Epoch 2586, Loss: 0.5393312126398087, Final Batch Loss: 0.3220660090446472\n",
      "Epoch 2587, Loss: 0.5201266258955002, Final Batch Loss: 0.24350135028362274\n",
      "Epoch 2588, Loss: 0.5134552717208862, Final Batch Loss: 0.2375912070274353\n",
      "Epoch 2589, Loss: 0.5204672813415527, Final Batch Loss: 0.2798807919025421\n",
      "Epoch 2590, Loss: 0.5719341933727264, Final Batch Loss: 0.2717146873474121\n",
      "Epoch 2591, Loss: 0.5586771368980408, Final Batch Loss: 0.3207627832889557\n",
      "Epoch 2592, Loss: 0.5350095927715302, Final Batch Loss: 0.2540600895881653\n",
      "Epoch 2593, Loss: 0.5244251042604446, Final Batch Loss: 0.2792714834213257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2594, Loss: 0.551766037940979, Final Batch Loss: 0.2593284249305725\n",
      "Epoch 2595, Loss: 0.5380735397338867, Final Batch Loss: 0.2739587426185608\n",
      "Epoch 2596, Loss: 0.49694670736789703, Final Batch Loss: 0.2726362645626068\n",
      "Epoch 2597, Loss: 0.5221980512142181, Final Batch Loss: 0.25330373644828796\n",
      "Epoch 2598, Loss: 0.582032710313797, Final Batch Loss: 0.34425994753837585\n",
      "Epoch 2599, Loss: 0.5377397835254669, Final Batch Loss: 0.264737069606781\n",
      "Epoch 2600, Loss: 0.5162771046161652, Final Batch Loss: 0.25684717297554016\n",
      "Epoch 2601, Loss: 0.5693722069263458, Final Batch Loss: 0.32414546608924866\n",
      "Epoch 2602, Loss: 0.6003498435020447, Final Batch Loss: 0.33685699105262756\n",
      "Epoch 2603, Loss: 0.5737380385398865, Final Batch Loss: 0.31812262535095215\n",
      "Epoch 2604, Loss: 0.5663098394870758, Final Batch Loss: 0.3082168400287628\n",
      "Epoch 2605, Loss: 0.5222759246826172, Final Batch Loss: 0.2545442283153534\n",
      "Epoch 2606, Loss: 0.5262747406959534, Final Batch Loss: 0.2649245858192444\n",
      "Epoch 2607, Loss: 0.5509761869907379, Final Batch Loss: 0.2697484791278839\n",
      "Epoch 2608, Loss: 0.5626112520694733, Final Batch Loss: 0.3052842617034912\n",
      "Epoch 2609, Loss: 0.5297597646713257, Final Batch Loss: 0.2575644850730896\n",
      "Epoch 2610, Loss: 0.5313821285963058, Final Batch Loss: 0.24795465171337128\n",
      "Epoch 2611, Loss: 0.5303467810153961, Final Batch Loss: 0.27100157737731934\n",
      "Epoch 2612, Loss: 0.5716322809457779, Final Batch Loss: 0.3235708773136139\n",
      "Epoch 2613, Loss: 0.5579312294721603, Final Batch Loss: 0.31148093938827515\n",
      "Epoch 2614, Loss: 0.5146990567445755, Final Batch Loss: 0.21679295599460602\n",
      "Epoch 2615, Loss: 0.5551349520683289, Final Batch Loss: 0.29684707522392273\n",
      "Epoch 2616, Loss: 0.5336280465126038, Final Batch Loss: 0.2524334490299225\n",
      "Epoch 2617, Loss: 0.5311913192272186, Final Batch Loss: 0.28068891167640686\n",
      "Epoch 2618, Loss: 0.5241770893335342, Final Batch Loss: 0.23071567714214325\n",
      "Epoch 2619, Loss: 0.5246696174144745, Final Batch Loss: 0.2546699345111847\n",
      "Epoch 2620, Loss: 0.5076937675476074, Final Batch Loss: 0.23290026187896729\n",
      "Epoch 2621, Loss: 0.516510009765625, Final Batch Loss: 0.2508630156517029\n",
      "Epoch 2622, Loss: 0.5312725007534027, Final Batch Loss: 0.2591142952442169\n",
      "Epoch 2623, Loss: 0.5308140218257904, Final Batch Loss: 0.2710048258304596\n",
      "Epoch 2624, Loss: 0.47807393968105316, Final Batch Loss: 0.24414299428462982\n",
      "Epoch 2625, Loss: 0.5253069400787354, Final Batch Loss: 0.2685231566429138\n",
      "Epoch 2626, Loss: 0.5481284558773041, Final Batch Loss: 0.27837589383125305\n",
      "Epoch 2627, Loss: 0.5167427211999893, Final Batch Loss: 0.2480359822511673\n",
      "Epoch 2628, Loss: 0.5325168967247009, Final Batch Loss: 0.28506800532341003\n",
      "Epoch 2629, Loss: 0.49899645149707794, Final Batch Loss: 0.24294085800647736\n",
      "Epoch 2630, Loss: 0.5180206298828125, Final Batch Loss: 0.2505410313606262\n",
      "Epoch 2631, Loss: 0.5382560789585114, Final Batch Loss: 0.2933114171028137\n",
      "Epoch 2632, Loss: 0.5244291871786118, Final Batch Loss: 0.23157157003879547\n",
      "Epoch 2633, Loss: 0.4959481358528137, Final Batch Loss: 0.21454071998596191\n",
      "Epoch 2634, Loss: 0.5581192970275879, Final Batch Loss: 0.28748783469200134\n",
      "Epoch 2635, Loss: 0.5313243418931961, Final Batch Loss: 0.2827247977256775\n",
      "Epoch 2636, Loss: 0.5349562168121338, Final Batch Loss: 0.2530005872249603\n",
      "Epoch 2637, Loss: 0.492652490735054, Final Batch Loss: 0.19553209841251373\n",
      "Epoch 2638, Loss: 0.5065535455942154, Final Batch Loss: 0.2220129817724228\n",
      "Epoch 2639, Loss: 0.5102318227291107, Final Batch Loss: 0.2739999294281006\n",
      "Epoch 2640, Loss: 0.5717424750328064, Final Batch Loss: 0.2955082058906555\n",
      "Epoch 2641, Loss: 0.5079761147499084, Final Batch Loss: 0.25210729241371155\n",
      "Epoch 2642, Loss: 0.5163262635469437, Final Batch Loss: 0.245536670088768\n",
      "Epoch 2643, Loss: 0.4874299466609955, Final Batch Loss: 0.18831026554107666\n",
      "Epoch 2644, Loss: 0.5014043003320694, Final Batch Loss: 0.20418326556682587\n",
      "Epoch 2645, Loss: 0.5498805940151215, Final Batch Loss: 0.2827490270137787\n",
      "Epoch 2646, Loss: 0.545274943113327, Final Batch Loss: 0.29051002860069275\n",
      "Epoch 2647, Loss: 0.5206671953201294, Final Batch Loss: 0.27320602536201477\n",
      "Epoch 2648, Loss: 0.5431520640850067, Final Batch Loss: 0.2975546419620514\n",
      "Epoch 2649, Loss: 0.536624550819397, Final Batch Loss: 0.2743041217327118\n",
      "Epoch 2650, Loss: 0.5104889273643494, Final Batch Loss: 0.2502317726612091\n",
      "Epoch 2651, Loss: 0.4946778267621994, Final Batch Loss: 0.19804684817790985\n",
      "Epoch 2652, Loss: 0.5380877703428268, Final Batch Loss: 0.30354833602905273\n",
      "Epoch 2653, Loss: 0.5245690643787384, Final Batch Loss: 0.27443820238113403\n",
      "Epoch 2654, Loss: 0.48032374680042267, Final Batch Loss: 0.22082243859767914\n",
      "Epoch 2655, Loss: 0.5246444940567017, Final Batch Loss: 0.2710578441619873\n",
      "Epoch 2656, Loss: 0.5247077941894531, Final Batch Loss: 0.2425490915775299\n",
      "Epoch 2657, Loss: 0.5124184191226959, Final Batch Loss: 0.24550029635429382\n",
      "Epoch 2658, Loss: 0.5574572384357452, Final Batch Loss: 0.27966147661209106\n",
      "Epoch 2659, Loss: 0.5550872087478638, Final Batch Loss: 0.28961193561553955\n",
      "Epoch 2660, Loss: 0.4831221401691437, Final Batch Loss: 0.18382731080055237\n",
      "Epoch 2661, Loss: 0.5350886285305023, Final Batch Loss: 0.23507386445999146\n",
      "Epoch 2662, Loss: 0.5464965552091599, Final Batch Loss: 0.3074302673339844\n",
      "Epoch 2663, Loss: 0.5340969264507294, Final Batch Loss: 0.26387694478034973\n",
      "Epoch 2664, Loss: 0.4739881008863449, Final Batch Loss: 0.19805185496807098\n",
      "Epoch 2665, Loss: 0.5188905000686646, Final Batch Loss: 0.26642167568206787\n",
      "Epoch 2666, Loss: 0.5773572027683258, Final Batch Loss: 0.3566730320453644\n",
      "Epoch 2667, Loss: 0.48340196907520294, Final Batch Loss: 0.20702813565731049\n",
      "Epoch 2668, Loss: 0.5508463680744171, Final Batch Loss: 0.3116251528263092\n",
      "Epoch 2669, Loss: 0.5300987958908081, Final Batch Loss: 0.27199220657348633\n",
      "Epoch 2670, Loss: 0.5719907879829407, Final Batch Loss: 0.29348281025886536\n",
      "Epoch 2671, Loss: 0.48390020430088043, Final Batch Loss: 0.22741590440273285\n",
      "Epoch 2672, Loss: 0.5194071680307388, Final Batch Loss: 0.2864643633365631\n",
      "Epoch 2673, Loss: 0.498338520526886, Final Batch Loss: 0.25786206126213074\n",
      "Epoch 2674, Loss: 0.5168923139572144, Final Batch Loss: 0.24501299858093262\n",
      "Epoch 2675, Loss: 0.538795679807663, Final Batch Loss: 0.2676236927509308\n",
      "Epoch 2676, Loss: 0.5736345946788788, Final Batch Loss: 0.2659054398536682\n",
      "Epoch 2677, Loss: 0.5577129423618317, Final Batch Loss: 0.2850799262523651\n",
      "Epoch 2678, Loss: 0.5541165173053741, Final Batch Loss: 0.32043689489364624\n",
      "Epoch 2679, Loss: 0.5226112604141235, Final Batch Loss: 0.24433255195617676\n",
      "Epoch 2680, Loss: 0.6138201951980591, Final Batch Loss: 0.30407223105430603\n",
      "Epoch 2681, Loss: 0.5877732634544373, Final Batch Loss: 0.2785748541355133\n",
      "Epoch 2682, Loss: 0.532686710357666, Final Batch Loss: 0.26646679639816284\n",
      "Epoch 2683, Loss: 0.5308473408222198, Final Batch Loss: 0.26283982396125793\n",
      "Epoch 2684, Loss: 0.48872868716716766, Final Batch Loss: 0.21061091125011444\n",
      "Epoch 2685, Loss: 0.5062438994646072, Final Batch Loss: 0.19147242605686188\n",
      "Epoch 2686, Loss: 0.5397606194019318, Final Batch Loss: 0.2692692279815674\n",
      "Epoch 2687, Loss: 0.498766764998436, Final Batch Loss: 0.25653862953186035\n",
      "Epoch 2688, Loss: 0.5111146569252014, Final Batch Loss: 0.24725720286369324\n",
      "Epoch 2689, Loss: 0.5472287833690643, Final Batch Loss: 0.2960415482521057\n",
      "Epoch 2690, Loss: 0.5682273358106613, Final Batch Loss: 0.31827986240386963\n",
      "Epoch 2691, Loss: 0.4665660411119461, Final Batch Loss: 0.19710256159305573\n",
      "Epoch 2692, Loss: 0.5073997676372528, Final Batch Loss: 0.20963743329048157\n",
      "Epoch 2693, Loss: 0.5341516137123108, Final Batch Loss: 0.2810722291469574\n",
      "Epoch 2694, Loss: 0.5562747418880463, Final Batch Loss: 0.2524774670600891\n",
      "Epoch 2695, Loss: 0.5016002953052521, Final Batch Loss: 0.24169465899467468\n",
      "Epoch 2696, Loss: 0.5594210922718048, Final Batch Loss: 0.2767709195613861\n",
      "Epoch 2697, Loss: 0.540345698595047, Final Batch Loss: 0.2661920189857483\n",
      "Epoch 2698, Loss: 0.5408304333686829, Final Batch Loss: 0.2666986286640167\n",
      "Epoch 2699, Loss: 0.5102385133504868, Final Batch Loss: 0.26403525471687317\n",
      "Epoch 2700, Loss: 0.4954250603914261, Final Batch Loss: 0.2667124271392822\n",
      "Epoch 2701, Loss: 0.47945378720760345, Final Batch Loss: 0.20097510516643524\n",
      "Epoch 2702, Loss: 0.49913403391838074, Final Batch Loss: 0.23455432057380676\n",
      "Epoch 2703, Loss: 0.5340058207511902, Final Batch Loss: 0.2787947356700897\n",
      "Epoch 2704, Loss: 0.5023486316204071, Final Batch Loss: 0.22535187005996704\n",
      "Epoch 2705, Loss: 0.550234317779541, Final Batch Loss: 0.29827871918678284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2706, Loss: 0.5448936522006989, Final Batch Loss: 0.2143380343914032\n",
      "Epoch 2707, Loss: 0.5426910817623138, Final Batch Loss: 0.2779368460178375\n",
      "Epoch 2708, Loss: 0.5281067788600922, Final Batch Loss: 0.2578635811805725\n",
      "Epoch 2709, Loss: 0.562650054693222, Final Batch Loss: 0.3302938938140869\n",
      "Epoch 2710, Loss: 0.4820479452610016, Final Batch Loss: 0.23893100023269653\n",
      "Epoch 2711, Loss: 0.5020334720611572, Final Batch Loss: 0.2643378973007202\n",
      "Epoch 2712, Loss: 0.49363210797309875, Final Batch Loss: 0.2278718650341034\n",
      "Epoch 2713, Loss: 0.5479637980461121, Final Batch Loss: 0.3143254518508911\n",
      "Epoch 2714, Loss: 0.5606943964958191, Final Batch Loss: 0.295817106962204\n",
      "Epoch 2715, Loss: 0.5798607766628265, Final Batch Loss: 0.3166274130344391\n",
      "Epoch 2716, Loss: 0.4982350319623947, Final Batch Loss: 0.21315397322177887\n",
      "Epoch 2717, Loss: 0.5671564042568207, Final Batch Loss: 0.3096086382865906\n",
      "Epoch 2718, Loss: 0.515727311372757, Final Batch Loss: 0.2432767152786255\n",
      "Epoch 2719, Loss: 0.5677742511034012, Final Batch Loss: 0.243657186627388\n",
      "Epoch 2720, Loss: 0.5439025461673737, Final Batch Loss: 0.3069879412651062\n",
      "Epoch 2721, Loss: 0.5225380510091782, Final Batch Loss: 0.2788853049278259\n",
      "Epoch 2722, Loss: 0.5391036868095398, Final Batch Loss: 0.27708789706230164\n",
      "Epoch 2723, Loss: 0.48796989023685455, Final Batch Loss: 0.22939278185367584\n",
      "Epoch 2724, Loss: 0.5128168761730194, Final Batch Loss: 0.21851253509521484\n",
      "Epoch 2725, Loss: 0.5555843412876129, Final Batch Loss: 0.32077744603157043\n",
      "Epoch 2726, Loss: 0.5045801997184753, Final Batch Loss: 0.23019245266914368\n",
      "Epoch 2727, Loss: 0.5309829264879227, Final Batch Loss: 0.3000643849372864\n",
      "Epoch 2728, Loss: 0.4660290330648422, Final Batch Loss: 0.19324927031993866\n",
      "Epoch 2729, Loss: 0.5226226150989532, Final Batch Loss: 0.26655611395835876\n",
      "Epoch 2730, Loss: 0.5431435406208038, Final Batch Loss: 0.2693110704421997\n",
      "Epoch 2731, Loss: 0.5219989120960236, Final Batch Loss: 0.269220769405365\n",
      "Epoch 2732, Loss: 0.5279827415943146, Final Batch Loss: 0.2172931730747223\n",
      "Epoch 2733, Loss: 0.5353233218193054, Final Batch Loss: 0.2658724784851074\n",
      "Epoch 2734, Loss: 0.5153369605541229, Final Batch Loss: 0.2520202100276947\n",
      "Epoch 2735, Loss: 0.47014717757701874, Final Batch Loss: 0.18091218173503876\n",
      "Epoch 2736, Loss: 0.5577313899993896, Final Batch Loss: 0.2771420180797577\n",
      "Epoch 2737, Loss: 0.521207332611084, Final Batch Loss: 0.2059052288532257\n",
      "Epoch 2738, Loss: 0.5385152995586395, Final Batch Loss: 0.3006013333797455\n",
      "Epoch 2739, Loss: 0.5177463293075562, Final Batch Loss: 0.24661636352539062\n",
      "Epoch 2740, Loss: 0.5145102590322495, Final Batch Loss: 0.22406335175037384\n",
      "Epoch 2741, Loss: 0.5295017957687378, Final Batch Loss: 0.27952760457992554\n",
      "Epoch 2742, Loss: 0.5443980395793915, Final Batch Loss: 0.2851782739162445\n",
      "Epoch 2743, Loss: 0.5345796346664429, Final Batch Loss: 0.2640116512775421\n",
      "Epoch 2744, Loss: 0.491443395614624, Final Batch Loss: 0.21010679006576538\n",
      "Epoch 2745, Loss: 0.5384762287139893, Final Batch Loss: 0.2813583016395569\n",
      "Epoch 2746, Loss: 0.5355172753334045, Final Batch Loss: 0.26156073808670044\n",
      "Epoch 2747, Loss: 0.5140306949615479, Final Batch Loss: 0.25748562812805176\n",
      "Epoch 2748, Loss: 0.49157391488552094, Final Batch Loss: 0.23917700350284576\n",
      "Epoch 2749, Loss: 0.5461978167295456, Final Batch Loss: 0.29939842224121094\n",
      "Epoch 2750, Loss: 0.5320427119731903, Final Batch Loss: 0.26084110140800476\n",
      "Epoch 2751, Loss: 0.609612762928009, Final Batch Loss: 0.32844018936157227\n",
      "Epoch 2752, Loss: 0.5177531242370605, Final Batch Loss: 0.2546537220478058\n",
      "Epoch 2753, Loss: 0.5438635945320129, Final Batch Loss: 0.2714879512786865\n",
      "Epoch 2754, Loss: 0.5487203598022461, Final Batch Loss: 0.34660711884498596\n",
      "Epoch 2755, Loss: 0.53190878033638, Final Batch Loss: 0.27178528904914856\n",
      "Epoch 2756, Loss: 0.5170266777276993, Final Batch Loss: 0.26834753155708313\n",
      "Epoch 2757, Loss: 0.5490880310535431, Final Batch Loss: 0.27078777551651\n",
      "Epoch 2758, Loss: 0.5025256723165512, Final Batch Loss: 0.22234632074832916\n",
      "Epoch 2759, Loss: 0.5210764110088348, Final Batch Loss: 0.2496805191040039\n",
      "Epoch 2760, Loss: 0.5008391737937927, Final Batch Loss: 0.2451993227005005\n",
      "Epoch 2761, Loss: 0.5153020769357681, Final Batch Loss: 0.2668289542198181\n",
      "Epoch 2762, Loss: 0.4769444316625595, Final Batch Loss: 0.20685620605945587\n",
      "Epoch 2763, Loss: 0.516463041305542, Final Batch Loss: 0.2615126967430115\n",
      "Epoch 2764, Loss: 0.4924863427877426, Final Batch Loss: 0.21045689284801483\n",
      "Epoch 2765, Loss: 0.5147846043109894, Final Batch Loss: 0.2389073371887207\n",
      "Epoch 2766, Loss: 0.5497990548610687, Final Batch Loss: 0.28143110871315\n",
      "Epoch 2767, Loss: 0.5085417479276657, Final Batch Loss: 0.2399301677942276\n",
      "Epoch 2768, Loss: 0.5749650299549103, Final Batch Loss: 0.3105977773666382\n",
      "Epoch 2769, Loss: 0.5561793446540833, Final Batch Loss: 0.2925048768520355\n",
      "Epoch 2770, Loss: 0.5455136001110077, Final Batch Loss: 0.256708562374115\n",
      "Epoch 2771, Loss: 0.472116082906723, Final Batch Loss: 0.22449488937854767\n",
      "Epoch 2772, Loss: 0.5262558907270432, Final Batch Loss: 0.2917252779006958\n",
      "Epoch 2773, Loss: 0.4962310492992401, Final Batch Loss: 0.2425265610218048\n",
      "Epoch 2774, Loss: 0.5632761418819427, Final Batch Loss: 0.3050806522369385\n",
      "Epoch 2775, Loss: 0.5286771208047867, Final Batch Loss: 0.2393270581960678\n",
      "Epoch 2776, Loss: 0.5279174745082855, Final Batch Loss: 0.23083949089050293\n",
      "Epoch 2777, Loss: 0.5364430248737335, Final Batch Loss: 0.26408398151397705\n",
      "Epoch 2778, Loss: 0.5326936990022659, Final Batch Loss: 0.24784712493419647\n",
      "Epoch 2779, Loss: 0.5410228967666626, Final Batch Loss: 0.2702670097351074\n",
      "Epoch 2780, Loss: 0.5391895771026611, Final Batch Loss: 0.2916010320186615\n",
      "Epoch 2781, Loss: 0.5095390379428864, Final Batch Loss: 0.2662048041820526\n",
      "Epoch 2782, Loss: 0.5147513002157211, Final Batch Loss: 0.23171041905879974\n",
      "Epoch 2783, Loss: 0.5633632242679596, Final Batch Loss: 0.32259324193000793\n",
      "Epoch 2784, Loss: 0.5528808534145355, Final Batch Loss: 0.28992071747779846\n",
      "Epoch 2785, Loss: 0.5615681409835815, Final Batch Loss: 0.2924044132232666\n",
      "Epoch 2786, Loss: 0.5122024267911911, Final Batch Loss: 0.2835012376308441\n",
      "Epoch 2787, Loss: 0.5393792986869812, Final Batch Loss: 0.304021954536438\n",
      "Epoch 2788, Loss: 0.548795759677887, Final Batch Loss: 0.3050796687602997\n",
      "Epoch 2789, Loss: 0.5086332559585571, Final Batch Loss: 0.2571461796760559\n",
      "Epoch 2790, Loss: 0.4928003400564194, Final Batch Loss: 0.2185303419828415\n",
      "Epoch 2791, Loss: 0.5713467299938202, Final Batch Loss: 0.25886374711990356\n",
      "Epoch 2792, Loss: 0.5299631953239441, Final Batch Loss: 0.27543315291404724\n",
      "Epoch 2793, Loss: 0.5182535350322723, Final Batch Loss: 0.2485928237438202\n",
      "Epoch 2794, Loss: 0.5481993556022644, Final Batch Loss: 0.29115474224090576\n",
      "Epoch 2795, Loss: 0.4886459857225418, Final Batch Loss: 0.25241297483444214\n",
      "Epoch 2796, Loss: 0.6140564978122711, Final Batch Loss: 0.3476400375366211\n",
      "Epoch 2797, Loss: 0.5043726712465286, Final Batch Loss: 0.27160021662712097\n",
      "Epoch 2798, Loss: 0.5780392289161682, Final Batch Loss: 0.29234975576400757\n",
      "Epoch 2799, Loss: 0.531130388379097, Final Batch Loss: 0.3020232617855072\n",
      "Epoch 2800, Loss: 0.5553941130638123, Final Batch Loss: 0.26164084672927856\n",
      "Epoch 2801, Loss: 0.497240275144577, Final Batch Loss: 0.2411384880542755\n",
      "Epoch 2802, Loss: 0.53968346118927, Final Batch Loss: 0.2560478448867798\n",
      "Epoch 2803, Loss: 0.48392197489738464, Final Batch Loss: 0.2026655077934265\n",
      "Epoch 2804, Loss: 0.6554641723632812, Final Batch Loss: 0.37459221482276917\n",
      "Epoch 2805, Loss: 0.554676964879036, Final Batch Loss: 0.23318825662136078\n",
      "Epoch 2806, Loss: 0.5120935887098312, Final Batch Loss: 0.26219791173934937\n",
      "Epoch 2807, Loss: 0.49036741256713867, Final Batch Loss: 0.22469589114189148\n",
      "Epoch 2808, Loss: 0.5110509693622589, Final Batch Loss: 0.2278505265712738\n",
      "Epoch 2809, Loss: 0.5157697200775146, Final Batch Loss: 0.21929088234901428\n",
      "Epoch 2810, Loss: 0.51979860663414, Final Batch Loss: 0.26901066303253174\n",
      "Epoch 2811, Loss: 0.5351601541042328, Final Batch Loss: 0.27394890785217285\n",
      "Epoch 2812, Loss: 0.5459030866622925, Final Batch Loss: 0.2698260545730591\n",
      "Epoch 2813, Loss: 0.5369580388069153, Final Batch Loss: 0.2917448580265045\n",
      "Epoch 2814, Loss: 0.49721238017082214, Final Batch Loss: 0.24733243882656097\n",
      "Epoch 2815, Loss: 0.4873621165752411, Final Batch Loss: 0.2108912169933319\n",
      "Epoch 2816, Loss: 0.5458484590053558, Final Batch Loss: 0.2836369574069977\n",
      "Epoch 2817, Loss: 0.5298181921243668, Final Batch Loss: 0.29572486877441406\n",
      "Epoch 2818, Loss: 0.5276190340518951, Final Batch Loss: 0.26406556367874146\n",
      "Epoch 2819, Loss: 0.4947456568479538, Final Batch Loss: 0.25375285744667053\n",
      "Epoch 2820, Loss: 0.49053482711315155, Final Batch Loss: 0.22770844399929047\n",
      "Epoch 2821, Loss: 0.5512184500694275, Final Batch Loss: 0.2957671582698822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2822, Loss: 0.5173740386962891, Final Batch Loss: 0.2514370381832123\n",
      "Epoch 2823, Loss: 0.49238406121730804, Final Batch Loss: 0.2484915554523468\n",
      "Epoch 2824, Loss: 0.5131877958774567, Final Batch Loss: 0.2526693046092987\n",
      "Epoch 2825, Loss: 0.49908019602298737, Final Batch Loss: 0.20848165452480316\n",
      "Epoch 2826, Loss: 0.5521559119224548, Final Batch Loss: 0.3127630054950714\n",
      "Epoch 2827, Loss: 0.5184736400842667, Final Batch Loss: 0.2480669468641281\n",
      "Epoch 2828, Loss: 0.5408186912536621, Final Batch Loss: 0.2737289071083069\n",
      "Epoch 2829, Loss: 0.5041858851909637, Final Batch Loss: 0.27246278524398804\n",
      "Epoch 2830, Loss: 0.5391403883695602, Final Batch Loss: 0.333312064409256\n",
      "Epoch 2831, Loss: 0.5368827730417252, Final Batch Loss: 0.28809940814971924\n",
      "Epoch 2832, Loss: 0.5320113599300385, Final Batch Loss: 0.2569708228111267\n",
      "Epoch 2833, Loss: 0.5126164555549622, Final Batch Loss: 0.2612832486629486\n",
      "Epoch 2834, Loss: 0.5153068006038666, Final Batch Loss: 0.2872726023197174\n",
      "Epoch 2835, Loss: 0.509144589304924, Final Batch Loss: 0.271618127822876\n",
      "Epoch 2836, Loss: 0.5343018174171448, Final Batch Loss: 0.2711804211139679\n",
      "Epoch 2837, Loss: 0.4988062083721161, Final Batch Loss: 0.2522996962070465\n",
      "Epoch 2838, Loss: 0.5435392409563065, Final Batch Loss: 0.31518808007240295\n",
      "Epoch 2839, Loss: 0.5106808692216873, Final Batch Loss: 0.24919740855693817\n",
      "Epoch 2840, Loss: 0.49965745210647583, Final Batch Loss: 0.20130708813667297\n",
      "Epoch 2841, Loss: 0.5066075921058655, Final Batch Loss: 0.2575743496417999\n",
      "Epoch 2842, Loss: 0.5045027583837509, Final Batch Loss: 0.29533979296684265\n",
      "Epoch 2843, Loss: 0.5363845229148865, Final Batch Loss: 0.2430797517299652\n",
      "Epoch 2844, Loss: 0.5142953991889954, Final Batch Loss: 0.25569531321525574\n",
      "Epoch 2845, Loss: 0.5267038643360138, Final Batch Loss: 0.2872411608695984\n",
      "Epoch 2846, Loss: 0.5007628947496414, Final Batch Loss: 0.24155016243457794\n",
      "Epoch 2847, Loss: 0.5415945500135422, Final Batch Loss: 0.2943340241909027\n",
      "Epoch 2848, Loss: 0.49873121082782745, Final Batch Loss: 0.23820124566555023\n",
      "Epoch 2849, Loss: 0.5340943038463593, Final Batch Loss: 0.2617594599723816\n",
      "Epoch 2850, Loss: 0.5167904645204544, Final Batch Loss: 0.2808014154434204\n",
      "Epoch 2851, Loss: 0.5856532156467438, Final Batch Loss: 0.34092265367507935\n",
      "Epoch 2852, Loss: 0.5331971347332001, Final Batch Loss: 0.28057920932769775\n",
      "Epoch 2853, Loss: 0.49978838860988617, Final Batch Loss: 0.2369285374879837\n",
      "Epoch 2854, Loss: 0.5225294679403305, Final Batch Loss: 0.28309300541877747\n",
      "Epoch 2855, Loss: 0.4988958090543747, Final Batch Loss: 0.23516522347927094\n",
      "Epoch 2856, Loss: 0.5880186855792999, Final Batch Loss: 0.29905983805656433\n",
      "Epoch 2857, Loss: 0.5343305170536041, Final Batch Loss: 0.2811211347579956\n",
      "Epoch 2858, Loss: 0.5313804447650909, Final Batch Loss: 0.3002249002456665\n",
      "Epoch 2859, Loss: 0.5511672794818878, Final Batch Loss: 0.296304851770401\n",
      "Epoch 2860, Loss: 0.5043674260377884, Final Batch Loss: 0.264578253030777\n",
      "Epoch 2861, Loss: 0.5379490554332733, Final Batch Loss: 0.2760908603668213\n",
      "Epoch 2862, Loss: 0.5356612503528595, Final Batch Loss: 0.2621005177497864\n",
      "Epoch 2863, Loss: 0.4792425185441971, Final Batch Loss: 0.20400454103946686\n",
      "Epoch 2864, Loss: 0.5343801081180573, Final Batch Loss: 0.26492512226104736\n",
      "Epoch 2865, Loss: 0.5264555960893631, Final Batch Loss: 0.29484039545059204\n",
      "Epoch 2866, Loss: 0.5844134390354156, Final Batch Loss: 0.3062947392463684\n",
      "Epoch 2867, Loss: 0.5790639668703079, Final Batch Loss: 0.3359615206718445\n",
      "Epoch 2868, Loss: 0.5417979657649994, Final Batch Loss: 0.2805940806865692\n",
      "Epoch 2869, Loss: 0.48258620500564575, Final Batch Loss: 0.1918521225452423\n",
      "Epoch 2870, Loss: 0.510923907160759, Final Batch Loss: 0.27123478055000305\n",
      "Epoch 2871, Loss: 0.5064557790756226, Final Batch Loss: 0.23094844818115234\n",
      "Epoch 2872, Loss: 0.5046233534812927, Final Batch Loss: 0.2572290897369385\n",
      "Epoch 2873, Loss: 0.515598326921463, Final Batch Loss: 0.2353159785270691\n",
      "Epoch 2874, Loss: 0.47087462246418, Final Batch Loss: 0.21530093252658844\n",
      "Epoch 2875, Loss: 0.5589417815208435, Final Batch Loss: 0.2979061007499695\n",
      "Epoch 2876, Loss: 0.5363959074020386, Final Batch Loss: 0.2958945333957672\n",
      "Epoch 2877, Loss: 0.5180711001157761, Final Batch Loss: 0.2841099500656128\n",
      "Epoch 2878, Loss: 0.525729775428772, Final Batch Loss: 0.262961745262146\n",
      "Epoch 2879, Loss: 0.479375883936882, Final Batch Loss: 0.2162306159734726\n",
      "Epoch 2880, Loss: 0.5240537375211716, Final Batch Loss: 0.30703991651535034\n",
      "Epoch 2881, Loss: 0.48341070115566254, Final Batch Loss: 0.18985067307949066\n",
      "Epoch 2882, Loss: 0.525488406419754, Final Batch Loss: 0.26556557416915894\n",
      "Epoch 2883, Loss: 0.47579652070999146, Final Batch Loss: 0.20273667573928833\n",
      "Epoch 2884, Loss: 0.4992358982563019, Final Batch Loss: 0.23833170533180237\n",
      "Epoch 2885, Loss: 0.5509389340877533, Final Batch Loss: 0.2599184513092041\n",
      "Epoch 2886, Loss: 0.4750427007675171, Final Batch Loss: 0.24677562713623047\n",
      "Epoch 2887, Loss: 0.5173615366220474, Final Batch Loss: 0.2934388518333435\n",
      "Epoch 2888, Loss: 0.5174596160650253, Final Batch Loss: 0.2797468602657318\n",
      "Epoch 2889, Loss: 0.490142360329628, Final Batch Loss: 0.24196340143680573\n",
      "Epoch 2890, Loss: 0.537465050816536, Final Batch Loss: 0.211586132645607\n",
      "Epoch 2891, Loss: 0.49821244180202484, Final Batch Loss: 0.2522393465042114\n",
      "Epoch 2892, Loss: 0.5545625686645508, Final Batch Loss: 0.2748682200908661\n",
      "Epoch 2893, Loss: 0.5301898121833801, Final Batch Loss: 0.2770094871520996\n",
      "Epoch 2894, Loss: 0.5237889140844345, Final Batch Loss: 0.24873583018779755\n",
      "Epoch 2895, Loss: 0.4820508807897568, Final Batch Loss: 0.20827098190784454\n",
      "Epoch 2896, Loss: 0.5165199935436249, Final Batch Loss: 0.26789748668670654\n",
      "Epoch 2897, Loss: 0.5163295418024063, Final Batch Loss: 0.2722408175468445\n",
      "Epoch 2898, Loss: 0.5085039585828781, Final Batch Loss: 0.24929438531398773\n",
      "Epoch 2899, Loss: 0.5049139559268951, Final Batch Loss: 0.2181665003299713\n",
      "Epoch 2900, Loss: 0.4617994725704193, Final Batch Loss: 0.21169322729110718\n",
      "Epoch 2901, Loss: 0.5659581422805786, Final Batch Loss: 0.29488620162010193\n",
      "Epoch 2902, Loss: 0.5617147982120514, Final Batch Loss: 0.2863805294036865\n",
      "Epoch 2903, Loss: 0.5256292223930359, Final Batch Loss: 0.27039703726768494\n",
      "Epoch 2904, Loss: 0.5112209618091583, Final Batch Loss: 0.25126001238822937\n",
      "Epoch 2905, Loss: 0.5161712169647217, Final Batch Loss: 0.24626794457435608\n",
      "Epoch 2906, Loss: 0.5522998422384262, Final Batch Loss: 0.2345736175775528\n",
      "Epoch 2907, Loss: 0.5082893073558807, Final Batch Loss: 0.25363269448280334\n",
      "Epoch 2908, Loss: 0.5139364302158356, Final Batch Loss: 0.2579640746116638\n",
      "Epoch 2909, Loss: 0.5072877258062363, Final Batch Loss: 0.26736730337142944\n",
      "Epoch 2910, Loss: 0.48590344190597534, Final Batch Loss: 0.19308653473854065\n",
      "Epoch 2911, Loss: 0.5036733001470566, Final Batch Loss: 0.2539641261100769\n",
      "Epoch 2912, Loss: 0.5242448449134827, Final Batch Loss: 0.25396066904067993\n",
      "Epoch 2913, Loss: 0.49634915590286255, Final Batch Loss: 0.2310047149658203\n",
      "Epoch 2914, Loss: 0.4882532060146332, Final Batch Loss: 0.26977717876434326\n",
      "Epoch 2915, Loss: 0.526450902223587, Final Batch Loss: 0.234188050031662\n",
      "Epoch 2916, Loss: 0.5785500109195709, Final Batch Loss: 0.32642701268196106\n",
      "Epoch 2917, Loss: 0.4614085555076599, Final Batch Loss: 0.19706785678863525\n",
      "Epoch 2918, Loss: 0.5217538774013519, Final Batch Loss: 0.305932879447937\n",
      "Epoch 2919, Loss: 0.49337656795978546, Final Batch Loss: 0.26296043395996094\n",
      "Epoch 2920, Loss: 0.5516154170036316, Final Batch Loss: 0.3103666305541992\n",
      "Epoch 2921, Loss: 0.46877066791057587, Final Batch Loss: 0.22643893957138062\n",
      "Epoch 2922, Loss: 0.5018503367900848, Final Batch Loss: 0.29429858922958374\n",
      "Epoch 2923, Loss: 0.44505463540554047, Final Batch Loss: 0.16225896775722504\n",
      "Epoch 2924, Loss: 0.4978495091199875, Final Batch Loss: 0.2172393649816513\n",
      "Epoch 2925, Loss: 0.5284787565469742, Final Batch Loss: 0.31264907121658325\n",
      "Epoch 2926, Loss: 0.5056572556495667, Final Batch Loss: 0.2535153925418854\n",
      "Epoch 2927, Loss: 0.49850407242774963, Final Batch Loss: 0.23219642043113708\n",
      "Epoch 2928, Loss: 0.5075448900461197, Final Batch Loss: 0.2724608778953552\n",
      "Epoch 2929, Loss: 0.4943689852952957, Final Batch Loss: 0.26386988162994385\n",
      "Epoch 2930, Loss: 0.5060850530862808, Final Batch Loss: 0.24682851135730743\n",
      "Epoch 2931, Loss: 0.5520689785480499, Final Batch Loss: 0.33524852991104126\n",
      "Epoch 2932, Loss: 0.5255513340234756, Final Batch Loss: 0.2830056846141815\n",
      "Epoch 2933, Loss: 0.5172177404165268, Final Batch Loss: 0.2678998112678528\n",
      "Epoch 2934, Loss: 0.5147963762283325, Final Batch Loss: 0.2850007712841034\n",
      "Epoch 2935, Loss: 0.497665137052536, Final Batch Loss: 0.2593524754047394\n",
      "Epoch 2936, Loss: 0.5201064944267273, Final Batch Loss: 0.2619476616382599\n",
      "Epoch 2937, Loss: 0.5227790027856827, Final Batch Loss: 0.27597784996032715\n",
      "Epoch 2938, Loss: 0.5284371376037598, Final Batch Loss: 0.28731611371040344\n",
      "Epoch 2939, Loss: 0.47204624116420746, Final Batch Loss: 0.20324672758579254\n",
      "Epoch 2940, Loss: 0.5520112216472626, Final Batch Loss: 0.30073726177215576\n",
      "Epoch 2941, Loss: 0.557310551404953, Final Batch Loss: 0.299032598733902\n",
      "Epoch 2942, Loss: 0.5342922806739807, Final Batch Loss: 0.2909458875656128\n",
      "Epoch 2943, Loss: 0.5285449624061584, Final Batch Loss: 0.30591514706611633\n",
      "Epoch 2944, Loss: 0.6095517873764038, Final Batch Loss: 0.3460778295993805\n",
      "Epoch 2945, Loss: 0.5345478504896164, Final Batch Loss: 0.24156369268894196\n",
      "Epoch 2946, Loss: 0.5687735676765442, Final Batch Loss: 0.295232892036438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2947, Loss: 0.5116845816373825, Final Batch Loss: 0.2690621316432953\n",
      "Epoch 2948, Loss: 0.48537391424179077, Final Batch Loss: 0.22200796008110046\n",
      "Epoch 2949, Loss: 0.49008049070835114, Final Batch Loss: 0.217363640666008\n",
      "Epoch 2950, Loss: 0.5413659512996674, Final Batch Loss: 0.28933829069137573\n",
      "Epoch 2951, Loss: 0.5073462128639221, Final Batch Loss: 0.23638704419136047\n",
      "Epoch 2952, Loss: 0.490152508020401, Final Batch Loss: 0.2620851993560791\n",
      "Epoch 2953, Loss: 0.48701027035713196, Final Batch Loss: 0.2635754644870758\n",
      "Epoch 2954, Loss: 0.504748061299324, Final Batch Loss: 0.24094928801059723\n",
      "Epoch 2955, Loss: 0.4993771016597748, Final Batch Loss: 0.21512192487716675\n",
      "Epoch 2956, Loss: 0.5170360803604126, Final Batch Loss: 0.26956906914711\n",
      "Epoch 2957, Loss: 0.5352354049682617, Final Batch Loss: 0.25178709626197815\n",
      "Epoch 2958, Loss: 0.5100596100091934, Final Batch Loss: 0.23853491246700287\n",
      "Epoch 2959, Loss: 0.5532160997390747, Final Batch Loss: 0.28095173835754395\n",
      "Epoch 2960, Loss: 0.5206059813499451, Final Batch Loss: 0.26727473735809326\n",
      "Epoch 2961, Loss: 0.49155525863170624, Final Batch Loss: 0.21362067759037018\n",
      "Epoch 2962, Loss: 0.5317498743534088, Final Batch Loss: 0.3071727454662323\n",
      "Epoch 2963, Loss: 0.48833340406417847, Final Batch Loss: 0.22243446111679077\n",
      "Epoch 2964, Loss: 0.49561603367328644, Final Batch Loss: 0.23197396099567413\n",
      "Epoch 2965, Loss: 0.4703051596879959, Final Batch Loss: 0.1715453714132309\n",
      "Epoch 2966, Loss: 0.4907374680042267, Final Batch Loss: 0.23436829447746277\n",
      "Epoch 2967, Loss: 0.5040034055709839, Final Batch Loss: 0.26286113262176514\n",
      "Epoch 2968, Loss: 0.4905342012643814, Final Batch Loss: 0.2180471271276474\n",
      "Epoch 2969, Loss: 0.4694742411375046, Final Batch Loss: 0.19358263909816742\n",
      "Epoch 2970, Loss: 0.5072237104177475, Final Batch Loss: 0.27709197998046875\n",
      "Epoch 2971, Loss: 0.47383594512939453, Final Batch Loss: 0.2246522754430771\n",
      "Epoch 2972, Loss: 0.5364246964454651, Final Batch Loss: 0.27426427602767944\n",
      "Epoch 2973, Loss: 0.4950408488512039, Final Batch Loss: 0.23342914879322052\n",
      "Epoch 2974, Loss: 0.49053359031677246, Final Batch Loss: 0.2491714358329773\n",
      "Epoch 2975, Loss: 0.5200866162776947, Final Batch Loss: 0.30408453941345215\n",
      "Epoch 2976, Loss: 0.5226982086896896, Final Batch Loss: 0.2893837094306946\n",
      "Epoch 2977, Loss: 0.6292302310466766, Final Batch Loss: 0.3722381889820099\n",
      "Epoch 2978, Loss: 0.5263223350048065, Final Batch Loss: 0.30808618664741516\n",
      "Epoch 2979, Loss: 0.5121229887008667, Final Batch Loss: 0.2893725037574768\n",
      "Epoch 2980, Loss: 0.4727132171392441, Final Batch Loss: 0.22578175365924835\n",
      "Epoch 2981, Loss: 0.4517987370491028, Final Batch Loss: 0.20868508517742157\n",
      "Epoch 2982, Loss: 0.4596141278743744, Final Batch Loss: 0.20740696787834167\n",
      "Epoch 2983, Loss: 0.500519722700119, Final Batch Loss: 0.27430447936058044\n",
      "Epoch 2984, Loss: 0.5069067478179932, Final Batch Loss: 0.20990252494812012\n",
      "Epoch 2985, Loss: 0.48777036368846893, Final Batch Loss: 0.197702094912529\n",
      "Epoch 2986, Loss: 0.5054684579372406, Final Batch Loss: 0.24885857105255127\n",
      "Epoch 2987, Loss: 0.5080092400312424, Final Batch Loss: 0.27924972772598267\n",
      "Epoch 2988, Loss: 0.48262614011764526, Final Batch Loss: 0.22694486379623413\n",
      "Epoch 2989, Loss: 0.5073993057012558, Final Batch Loss: 0.21624566614627838\n",
      "Epoch 2990, Loss: 0.5208372473716736, Final Batch Loss: 0.25748908519744873\n",
      "Epoch 2991, Loss: 0.5080680996179581, Final Batch Loss: 0.226295605301857\n",
      "Epoch 2992, Loss: 0.5175362229347229, Final Batch Loss: 0.25172629952430725\n",
      "Epoch 2993, Loss: 0.4773249477148056, Final Batch Loss: 0.21888066828250885\n",
      "Epoch 2994, Loss: 0.5183586776256561, Final Batch Loss: 0.2636953890323639\n",
      "Epoch 2995, Loss: 0.527876228094101, Final Batch Loss: 0.24215325713157654\n",
      "Epoch 2996, Loss: 0.5249386578798294, Final Batch Loss: 0.3102876543998718\n",
      "Epoch 2997, Loss: 0.5661034286022186, Final Batch Loss: 0.28308162093162537\n",
      "Epoch 2998, Loss: 0.6150670349597931, Final Batch Loss: 0.27147236466407776\n",
      "Epoch 2999, Loss: 0.46294887363910675, Final Batch Loss: 0.22843652963638306\n",
      "Epoch 3000, Loss: 0.528499186038971, Final Batch Loss: 0.26449331641197205\n",
      "Epoch 3001, Loss: 0.47689542174339294, Final Batch Loss: 0.24800193309783936\n",
      "Epoch 3002, Loss: 0.5063193440437317, Final Batch Loss: 0.23029881715774536\n",
      "Epoch 3003, Loss: 0.5129860043525696, Final Batch Loss: 0.25565120577812195\n",
      "Epoch 3004, Loss: 0.48350633680820465, Final Batch Loss: 0.2230996936559677\n",
      "Epoch 3005, Loss: 0.5305452048778534, Final Batch Loss: 0.27107688784599304\n",
      "Epoch 3006, Loss: 0.4948398768901825, Final Batch Loss: 0.26035669445991516\n",
      "Epoch 3007, Loss: 0.5236142575740814, Final Batch Loss: 0.25931090116500854\n",
      "Epoch 3008, Loss: 0.5139762759208679, Final Batch Loss: 0.25475019216537476\n",
      "Epoch 3009, Loss: 0.47063057124614716, Final Batch Loss: 0.18890540301799774\n",
      "Epoch 3010, Loss: 0.5155918002128601, Final Batch Loss: 0.26467570662498474\n",
      "Epoch 3011, Loss: 0.5079600512981415, Final Batch Loss: 0.2515701651573181\n",
      "Epoch 3012, Loss: 0.5040891319513321, Final Batch Loss: 0.2393757551908493\n",
      "Epoch 3013, Loss: 0.49717162549495697, Final Batch Loss: 0.2435593456029892\n",
      "Epoch 3014, Loss: 0.5067837536334991, Final Batch Loss: 0.27376097440719604\n",
      "Epoch 3015, Loss: 0.4710594415664673, Final Batch Loss: 0.23269322514533997\n",
      "Epoch 3016, Loss: 0.4839866906404495, Final Batch Loss: 0.21693848073482513\n",
      "Epoch 3017, Loss: 0.49057771265506744, Final Batch Loss: 0.2079860419034958\n",
      "Epoch 3018, Loss: 0.5407752692699432, Final Batch Loss: 0.2511197626590729\n",
      "Epoch 3019, Loss: 0.512128472328186, Final Batch Loss: 0.27768856287002563\n",
      "Epoch 3020, Loss: 0.5154943615198135, Final Batch Loss: 0.2877509593963623\n",
      "Epoch 3021, Loss: 0.49203766882419586, Final Batch Loss: 0.2358611673116684\n",
      "Epoch 3022, Loss: 0.5628736019134521, Final Batch Loss: 0.3241313099861145\n",
      "Epoch 3023, Loss: 0.49154140055179596, Final Batch Loss: 0.23707906901836395\n",
      "Epoch 3024, Loss: 0.47164884209632874, Final Batch Loss: 0.2193322479724884\n",
      "Epoch 3025, Loss: 0.5011919438838959, Final Batch Loss: 0.2750885784626007\n",
      "Epoch 3026, Loss: 0.47184813022613525, Final Batch Loss: 0.22391264140605927\n",
      "Epoch 3027, Loss: 0.5806611776351929, Final Batch Loss: 0.3214043080806732\n",
      "Epoch 3028, Loss: 0.49141789972782135, Final Batch Loss: 0.23952125012874603\n",
      "Epoch 3029, Loss: 0.5367282629013062, Final Batch Loss: 0.31645575165748596\n",
      "Epoch 3030, Loss: 0.5069263130426407, Final Batch Loss: 0.24362574517726898\n",
      "Epoch 3031, Loss: 0.5103812664747238, Final Batch Loss: 0.24597488343715668\n",
      "Epoch 3032, Loss: 0.4860610067844391, Final Batch Loss: 0.2048499882221222\n",
      "Epoch 3033, Loss: 0.5573752373456955, Final Batch Loss: 0.3325841724872589\n",
      "Epoch 3034, Loss: 0.4857328236103058, Final Batch Loss: 0.2186858057975769\n",
      "Epoch 3035, Loss: 0.4747263491153717, Final Batch Loss: 0.2410978376865387\n",
      "Epoch 3036, Loss: 0.4747515469789505, Final Batch Loss: 0.20266495645046234\n",
      "Epoch 3037, Loss: 0.4944911450147629, Final Batch Loss: 0.2355441004037857\n",
      "Epoch 3038, Loss: 0.49147629737854004, Final Batch Loss: 0.23433604836463928\n",
      "Epoch 3039, Loss: 0.49803580343723297, Final Batch Loss: 0.2595977783203125\n",
      "Epoch 3040, Loss: 0.491948738694191, Final Batch Loss: 0.23614437878131866\n",
      "Epoch 3041, Loss: 0.5502090156078339, Final Batch Loss: 0.32806557416915894\n",
      "Epoch 3042, Loss: 0.5221144258975983, Final Batch Loss: 0.28267860412597656\n",
      "Epoch 3043, Loss: 0.5448148548603058, Final Batch Loss: 0.32039445638656616\n",
      "Epoch 3044, Loss: 0.4919683337211609, Final Batch Loss: 0.2570159137248993\n",
      "Epoch 3045, Loss: 0.48402319848537445, Final Batch Loss: 0.2436022013425827\n",
      "Epoch 3046, Loss: 0.6092142760753632, Final Batch Loss: 0.3656240701675415\n",
      "Epoch 3047, Loss: 0.4991912543773651, Final Batch Loss: 0.28458544611930847\n",
      "Epoch 3048, Loss: 0.5018076598644257, Final Batch Loss: 0.2649867832660675\n",
      "Epoch 3049, Loss: 0.47511112689971924, Final Batch Loss: 0.2053259313106537\n",
      "Epoch 3050, Loss: 0.5604748278856277, Final Batch Loss: 0.3334895968437195\n",
      "Epoch 3051, Loss: 0.4684800058603287, Final Batch Loss: 0.2277831733226776\n",
      "Epoch 3052, Loss: 0.49945977330207825, Final Batch Loss: 0.2562914192676544\n",
      "Epoch 3053, Loss: 0.526188924908638, Final Batch Loss: 0.2819157540798187\n",
      "Epoch 3054, Loss: 0.47936825454235077, Final Batch Loss: 0.19659064710140228\n",
      "Epoch 3055, Loss: 0.47510847449302673, Final Batch Loss: 0.22679221630096436\n",
      "Epoch 3056, Loss: 0.5107170790433884, Final Batch Loss: 0.2436101883649826\n",
      "Epoch 3057, Loss: 0.4987794905900955, Final Batch Loss: 0.26422247290611267\n",
      "Epoch 3058, Loss: 0.4715135246515274, Final Batch Loss: 0.2136608511209488\n",
      "Epoch 3059, Loss: 0.5253841876983643, Final Batch Loss: 0.25777381658554077\n",
      "Epoch 3060, Loss: 0.5122448354959488, Final Batch Loss: 0.24330206215381622\n",
      "Epoch 3061, Loss: 0.4623916745185852, Final Batch Loss: 0.21646443009376526\n",
      "Epoch 3062, Loss: 0.5385134220123291, Final Batch Loss: 0.24020349979400635\n",
      "Epoch 3063, Loss: 0.46808038651943207, Final Batch Loss: 0.24346740543842316\n",
      "Epoch 3064, Loss: 0.5442872643470764, Final Batch Loss: 0.3019784986972809\n",
      "Epoch 3065, Loss: 0.4606267809867859, Final Batch Loss: 0.1755257248878479\n",
      "Epoch 3066, Loss: 0.5180874913930893, Final Batch Loss: 0.2803763747215271\n",
      "Epoch 3067, Loss: 0.5000174939632416, Final Batch Loss: 0.27131977677345276\n",
      "Epoch 3068, Loss: 0.47637055814266205, Final Batch Loss: 0.21370749175548553\n",
      "Epoch 3069, Loss: 0.5935897380113602, Final Batch Loss: 0.3608217239379883\n",
      "Epoch 3070, Loss: 0.5011473596096039, Final Batch Loss: 0.22255158424377441\n",
      "Epoch 3071, Loss: 0.48379452526569366, Final Batch Loss: 0.16111411154270172\n",
      "Epoch 3072, Loss: 0.5183338522911072, Final Batch Loss: 0.2653832733631134\n",
      "Epoch 3073, Loss: 0.4781007766723633, Final Batch Loss: 0.23638024926185608\n",
      "Epoch 3074, Loss: 0.48019374907016754, Final Batch Loss: 0.24075807631015778\n",
      "Epoch 3075, Loss: 0.5211744159460068, Final Batch Loss: 0.2958492636680603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3076, Loss: 0.5111462771892548, Final Batch Loss: 0.23959341645240784\n",
      "Epoch 3077, Loss: 0.5195751935243607, Final Batch Loss: 0.27076220512390137\n",
      "Epoch 3078, Loss: 0.5346251130104065, Final Batch Loss: 0.26508527994155884\n",
      "Epoch 3079, Loss: 0.4921409636735916, Final Batch Loss: 0.2746562957763672\n",
      "Epoch 3080, Loss: 0.46765607595443726, Final Batch Loss: 0.19980087876319885\n",
      "Epoch 3081, Loss: 0.4817612022161484, Final Batch Loss: 0.2623738944530487\n",
      "Epoch 3082, Loss: 0.4677038639783859, Final Batch Loss: 0.21982073783874512\n",
      "Epoch 3083, Loss: 0.48075735569000244, Final Batch Loss: 0.22871094942092896\n",
      "Epoch 3084, Loss: 0.48060280084609985, Final Batch Loss: 0.24162860214710236\n",
      "Epoch 3085, Loss: 0.5473520010709763, Final Batch Loss: 0.31623387336730957\n",
      "Epoch 3086, Loss: 0.46977710723876953, Final Batch Loss: 0.2084023356437683\n",
      "Epoch 3087, Loss: 0.5043725371360779, Final Batch Loss: 0.23593851923942566\n",
      "Epoch 3088, Loss: 0.5139530599117279, Final Batch Loss: 0.2731752097606659\n",
      "Epoch 3089, Loss: 0.534256100654602, Final Batch Loss: 0.29441967606544495\n",
      "Epoch 3090, Loss: 0.47855789959430695, Final Batch Loss: 0.22446681559085846\n",
      "Epoch 3091, Loss: 0.5007478296756744, Final Batch Loss: 0.28524067997932434\n",
      "Epoch 3092, Loss: 0.49780556559562683, Final Batch Loss: 0.2247505784034729\n",
      "Epoch 3093, Loss: 0.5200584232807159, Final Batch Loss: 0.326324999332428\n",
      "Epoch 3094, Loss: 0.4574768394231796, Final Batch Loss: 0.198628768324852\n",
      "Epoch 3095, Loss: 0.4769870936870575, Final Batch Loss: 0.21681082248687744\n",
      "Epoch 3096, Loss: 0.4661242365837097, Final Batch Loss: 0.22678636014461517\n",
      "Epoch 3097, Loss: 0.46301865577697754, Final Batch Loss: 0.21864181756973267\n",
      "Epoch 3098, Loss: 0.5841386616230011, Final Batch Loss: 0.31451594829559326\n",
      "Epoch 3099, Loss: 0.48139432072639465, Final Batch Loss: 0.23410986363887787\n",
      "Epoch 3100, Loss: 0.4667688310146332, Final Batch Loss: 0.22654841840267181\n",
      "Epoch 3101, Loss: 0.49833518266677856, Final Batch Loss: 0.2779957354068756\n",
      "Epoch 3102, Loss: 0.49806448817253113, Final Batch Loss: 0.23388662934303284\n",
      "Epoch 3103, Loss: 0.49055786430835724, Final Batch Loss: 0.23042617738246918\n",
      "Epoch 3104, Loss: 0.45697033405303955, Final Batch Loss: 0.21125288307666779\n",
      "Epoch 3105, Loss: 0.5074156671762466, Final Batch Loss: 0.2329055517911911\n",
      "Epoch 3106, Loss: 0.5209249258041382, Final Batch Loss: 0.25896894931793213\n",
      "Epoch 3107, Loss: 0.48483437299728394, Final Batch Loss: 0.2332758903503418\n",
      "Epoch 3108, Loss: 0.5018796026706696, Final Batch Loss: 0.283649742603302\n",
      "Epoch 3109, Loss: 0.4780603349208832, Final Batch Loss: 0.2327788770198822\n",
      "Epoch 3110, Loss: 0.4714483171701431, Final Batch Loss: 0.26174449920654297\n",
      "Epoch 3111, Loss: 0.5125726610422134, Final Batch Loss: 0.2700018882751465\n",
      "Epoch 3112, Loss: 0.49196842312812805, Final Batch Loss: 0.24929286539554596\n",
      "Epoch 3113, Loss: 0.48857012391090393, Final Batch Loss: 0.23574456572532654\n",
      "Epoch 3114, Loss: 0.48572930693626404, Final Batch Loss: 0.252298504114151\n",
      "Epoch 3115, Loss: 0.4837433397769928, Final Batch Loss: 0.2340768724679947\n",
      "Epoch 3116, Loss: 0.5092957019805908, Final Batch Loss: 0.27022746205329895\n",
      "Epoch 3117, Loss: 0.47790251672267914, Final Batch Loss: 0.26736214756965637\n",
      "Epoch 3118, Loss: 0.5217616558074951, Final Batch Loss: 0.2821493446826935\n",
      "Epoch 3119, Loss: 0.5430910587310791, Final Batch Loss: 0.31038233637809753\n",
      "Epoch 3120, Loss: 0.502063661813736, Final Batch Loss: 0.27277371287345886\n",
      "Epoch 3121, Loss: 0.4482639729976654, Final Batch Loss: 0.20750556886196136\n",
      "Epoch 3122, Loss: 0.4453191012144089, Final Batch Loss: 0.20178119838237762\n",
      "Epoch 3123, Loss: 0.4768129140138626, Final Batch Loss: 0.23452967405319214\n",
      "Epoch 3124, Loss: 0.47291652858257294, Final Batch Loss: 0.2471216320991516\n",
      "Epoch 3125, Loss: 0.5014369189739227, Final Batch Loss: 0.2616766095161438\n",
      "Epoch 3126, Loss: 0.48772822320461273, Final Batch Loss: 0.24886155128479004\n",
      "Epoch 3127, Loss: 0.4649297744035721, Final Batch Loss: 0.207236185669899\n",
      "Epoch 3128, Loss: 0.47172971069812775, Final Batch Loss: 0.2831587493419647\n",
      "Epoch 3129, Loss: 0.43744096159935, Final Batch Loss: 0.20790964365005493\n",
      "Epoch 3130, Loss: 0.4986360967159271, Final Batch Loss: 0.28611570596694946\n",
      "Epoch 3131, Loss: 0.48678311705589294, Final Batch Loss: 0.23179152607917786\n",
      "Epoch 3132, Loss: 0.48885872960090637, Final Batch Loss: 0.2612740695476532\n",
      "Epoch 3133, Loss: 0.5119854211807251, Final Batch Loss: 0.28291529417037964\n",
      "Epoch 3134, Loss: 0.48412808775901794, Final Batch Loss: 0.25660911202430725\n",
      "Epoch 3135, Loss: 0.4652945250272751, Final Batch Loss: 0.22450540959835052\n",
      "Epoch 3136, Loss: 0.47274014353752136, Final Batch Loss: 0.2409069836139679\n",
      "Epoch 3137, Loss: 0.5156327933073044, Final Batch Loss: 0.2888948321342468\n",
      "Epoch 3138, Loss: 0.4330381751060486, Final Batch Loss: 0.20045407116413116\n",
      "Epoch 3139, Loss: 0.46737122535705566, Final Batch Loss: 0.24276970326900482\n",
      "Epoch 3140, Loss: 0.45983923971652985, Final Batch Loss: 0.17919035255908966\n",
      "Epoch 3141, Loss: 0.4576863497495651, Final Batch Loss: 0.19654037058353424\n",
      "Epoch 3142, Loss: 0.4434085339307785, Final Batch Loss: 0.21356883645057678\n",
      "Epoch 3143, Loss: 0.4501755088567734, Final Batch Loss: 0.2101876139640808\n",
      "Epoch 3144, Loss: 0.4747127592563629, Final Batch Loss: 0.29057249426841736\n",
      "Epoch 3145, Loss: 0.4815986752510071, Final Batch Loss: 0.20744842290878296\n",
      "Epoch 3146, Loss: 0.48542197048664093, Final Batch Loss: 0.2394692599773407\n",
      "Epoch 3147, Loss: 0.4753373712301254, Final Batch Loss: 0.23779341578483582\n",
      "Epoch 3148, Loss: 0.49556924402713776, Final Batch Loss: 0.21411128342151642\n",
      "Epoch 3149, Loss: 0.45359963178634644, Final Batch Loss: 0.2276044487953186\n",
      "Epoch 3150, Loss: 0.41463224589824677, Final Batch Loss: 0.20712345838546753\n",
      "Epoch 3151, Loss: 0.5212389975786209, Final Batch Loss: 0.3050672113895416\n",
      "Epoch 3152, Loss: 0.45956045389175415, Final Batch Loss: 0.21801874041557312\n",
      "Epoch 3153, Loss: 0.44578514993190765, Final Batch Loss: 0.19616053998470306\n",
      "Epoch 3154, Loss: 0.4609568566083908, Final Batch Loss: 0.21707333624362946\n",
      "Epoch 3155, Loss: 0.4411376267671585, Final Batch Loss: 0.2117834836244583\n",
      "Epoch 3156, Loss: 0.43837203085422516, Final Batch Loss: 0.21988391876220703\n",
      "Epoch 3157, Loss: 0.4826756417751312, Final Batch Loss: 0.22983050346374512\n",
      "Epoch 3158, Loss: 0.47441257536411285, Final Batch Loss: 0.22979214787483215\n",
      "Epoch 3159, Loss: 0.4612652659416199, Final Batch Loss: 0.27133214473724365\n",
      "Epoch 3160, Loss: 0.473030760884285, Final Batch Loss: 0.24329876899719238\n",
      "Epoch 3161, Loss: 0.461820051074028, Final Batch Loss: 0.20895983278751373\n",
      "Epoch 3162, Loss: 0.45405930280685425, Final Batch Loss: 0.2011854648590088\n",
      "Epoch 3163, Loss: 0.4630955010652542, Final Batch Loss: 0.272305965423584\n",
      "Epoch 3164, Loss: 0.5262013673782349, Final Batch Loss: 0.25938379764556885\n",
      "Epoch 3165, Loss: 0.47057928144931793, Final Batch Loss: 0.23129726946353912\n",
      "Epoch 3166, Loss: 0.4839300066232681, Final Batch Loss: 0.25038325786590576\n",
      "Epoch 3167, Loss: 0.4721749871969223, Final Batch Loss: 0.2465490698814392\n",
      "Epoch 3168, Loss: 0.5260839611291885, Final Batch Loss: 0.30108383297920227\n",
      "Epoch 3169, Loss: 0.5421868562698364, Final Batch Loss: 0.2690834105014801\n",
      "Epoch 3170, Loss: 0.4486810266971588, Final Batch Loss: 0.18912982940673828\n",
      "Epoch 3171, Loss: 0.46756136417388916, Final Batch Loss: 0.17978522181510925\n",
      "Epoch 3172, Loss: 0.5569669604301453, Final Batch Loss: 0.2667738199234009\n",
      "Epoch 3173, Loss: 0.4898591786623001, Final Batch Loss: 0.24194175004959106\n",
      "Epoch 3174, Loss: 0.46187856793403625, Final Batch Loss: 0.24008771777153015\n",
      "Epoch 3175, Loss: 0.4760863035917282, Final Batch Loss: 0.25776395201683044\n",
      "Epoch 3176, Loss: 0.4847748279571533, Final Batch Loss: 0.2453981637954712\n",
      "Epoch 3177, Loss: 0.5454323291778564, Final Batch Loss: 0.31633391976356506\n",
      "Epoch 3178, Loss: 0.47522956132888794, Final Batch Loss: 0.23971928656101227\n",
      "Epoch 3179, Loss: 0.5076700747013092, Final Batch Loss: 0.2917366325855255\n",
      "Epoch 3180, Loss: 0.4842071086168289, Final Batch Loss: 0.26388952136039734\n",
      "Epoch 3181, Loss: 0.5112784951925278, Final Batch Loss: 0.2739931344985962\n",
      "Epoch 3182, Loss: 0.4872937351465225, Final Batch Loss: 0.24856336414813995\n",
      "Epoch 3183, Loss: 0.4555461257696152, Final Batch Loss: 0.2152651697397232\n",
      "Epoch 3184, Loss: 0.5016204863786697, Final Batch Loss: 0.2558470368385315\n",
      "Epoch 3185, Loss: 0.44078752398490906, Final Batch Loss: 0.2155393362045288\n",
      "Epoch 3186, Loss: 0.46519485116004944, Final Batch Loss: 0.2826354503631592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3187, Loss: 0.5268070697784424, Final Batch Loss: 0.2666202187538147\n",
      "Epoch 3188, Loss: 0.46052883565425873, Final Batch Loss: 0.24894221127033234\n",
      "Epoch 3189, Loss: 0.49111491441726685, Final Batch Loss: 0.2477698177099228\n",
      "Epoch 3190, Loss: 0.488884761929512, Final Batch Loss: 0.24491219222545624\n",
      "Epoch 3191, Loss: 0.5405632853507996, Final Batch Loss: 0.3408639132976532\n",
      "Epoch 3192, Loss: 0.48314929008483887, Final Batch Loss: 0.25844451785087585\n",
      "Epoch 3193, Loss: 0.4677671045064926, Final Batch Loss: 0.2553272247314453\n",
      "Epoch 3194, Loss: 0.48255808651447296, Final Batch Loss: 0.20932398736476898\n",
      "Epoch 3195, Loss: 0.5021511018276215, Final Batch Loss: 0.2838052213191986\n",
      "Epoch 3196, Loss: 0.4537498652935028, Final Batch Loss: 0.22858862578868866\n",
      "Epoch 3197, Loss: 0.43770577013492584, Final Batch Loss: 0.2049049735069275\n",
      "Epoch 3198, Loss: 0.4670752286911011, Final Batch Loss: 0.23722685873508453\n",
      "Epoch 3199, Loss: 0.4751638174057007, Final Batch Loss: 0.2021382749080658\n",
      "Epoch 3200, Loss: 0.46475112438201904, Final Batch Loss: 0.2662336230278015\n",
      "Epoch 3201, Loss: 0.4513744115829468, Final Batch Loss: 0.23157188296318054\n",
      "Epoch 3202, Loss: 0.4148297905921936, Final Batch Loss: 0.1907476931810379\n",
      "Epoch 3203, Loss: 0.4611869156360626, Final Batch Loss: 0.22122028470039368\n",
      "Epoch 3204, Loss: 0.49901339411735535, Final Batch Loss: 0.290539413690567\n",
      "Epoch 3205, Loss: 0.4873924255371094, Final Batch Loss: 0.24929988384246826\n",
      "Epoch 3206, Loss: 0.4505006819963455, Final Batch Loss: 0.21967865526676178\n",
      "Epoch 3207, Loss: 0.40209899842739105, Final Batch Loss: 0.19348680973052979\n",
      "Epoch 3208, Loss: 0.4386402517557144, Final Batch Loss: 0.2216225564479828\n",
      "Epoch 3209, Loss: 0.4575505256652832, Final Batch Loss: 0.21709686517715454\n",
      "Epoch 3210, Loss: 0.4531342685222626, Final Batch Loss: 0.204517662525177\n",
      "Epoch 3211, Loss: 0.46882714331150055, Final Batch Loss: 0.21503321826457977\n",
      "Epoch 3212, Loss: 0.459097757935524, Final Batch Loss: 0.23302511870861053\n",
      "Epoch 3213, Loss: 0.49347521364688873, Final Batch Loss: 0.2791596055030823\n",
      "Epoch 3214, Loss: 0.43041782081127167, Final Batch Loss: 0.19640685617923737\n",
      "Epoch 3215, Loss: 0.4511328935623169, Final Batch Loss: 0.2375439554452896\n",
      "Epoch 3216, Loss: 0.4398229867219925, Final Batch Loss: 0.24441221356391907\n",
      "Epoch 3217, Loss: 0.43641050159931183, Final Batch Loss: 0.20165729522705078\n",
      "Epoch 3218, Loss: 0.46795229613780975, Final Batch Loss: 0.24558544158935547\n",
      "Epoch 3219, Loss: 0.48223158717155457, Final Batch Loss: 0.21577298641204834\n",
      "Epoch 3220, Loss: 0.5434395223855972, Final Batch Loss: 0.29962703585624695\n",
      "Epoch 3221, Loss: 0.4638708829879761, Final Batch Loss: 0.22281469404697418\n",
      "Epoch 3222, Loss: 0.5303772687911987, Final Batch Loss: 0.27436357736587524\n",
      "Epoch 3223, Loss: 0.429198682308197, Final Batch Loss: 0.1792604923248291\n",
      "Epoch 3224, Loss: 0.4645765423774719, Final Batch Loss: 0.25430285930633545\n",
      "Epoch 3225, Loss: 0.4903743714094162, Final Batch Loss: 0.26208093762397766\n",
      "Epoch 3226, Loss: 0.4441162496805191, Final Batch Loss: 0.24728107452392578\n",
      "Epoch 3227, Loss: 0.4276587516069412, Final Batch Loss: 0.18765124678611755\n",
      "Epoch 3228, Loss: 0.5474518090486526, Final Batch Loss: 0.23965062201023102\n",
      "Epoch 3229, Loss: 0.42547689378261566, Final Batch Loss: 0.21573077142238617\n",
      "Epoch 3230, Loss: 0.47835712134838104, Final Batch Loss: 0.2230960577726364\n",
      "Epoch 3231, Loss: 0.4204089492559433, Final Batch Loss: 0.15828193724155426\n",
      "Epoch 3232, Loss: 0.42391811311244965, Final Batch Loss: 0.22078806161880493\n",
      "Epoch 3233, Loss: 0.4207444339990616, Final Batch Loss: 0.2216518670320511\n",
      "Epoch 3234, Loss: 0.43295595049858093, Final Batch Loss: 0.2393304854631424\n",
      "Epoch 3235, Loss: 0.431243434548378, Final Batch Loss: 0.22839096188545227\n",
      "Epoch 3236, Loss: 0.43294477462768555, Final Batch Loss: 0.1906655877828598\n",
      "Epoch 3237, Loss: 0.4419727772474289, Final Batch Loss: 0.2387983202934265\n",
      "Epoch 3238, Loss: 0.49754613637924194, Final Batch Loss: 0.30630794167518616\n",
      "Epoch 3239, Loss: 0.5066158324480057, Final Batch Loss: 0.2902180850505829\n",
      "Epoch 3240, Loss: 0.4716004878282547, Final Batch Loss: 0.2693319320678711\n",
      "Epoch 3241, Loss: 0.46091364324092865, Final Batch Loss: 0.26663732528686523\n",
      "Epoch 3242, Loss: 0.49170371890068054, Final Batch Loss: 0.24561017751693726\n",
      "Epoch 3243, Loss: 0.46764034032821655, Final Batch Loss: 0.26237693428993225\n",
      "Epoch 3244, Loss: 0.4513609856367111, Final Batch Loss: 0.22101110219955444\n",
      "Epoch 3245, Loss: 0.47581925988197327, Final Batch Loss: 0.28548723459243774\n",
      "Epoch 3246, Loss: 0.45708324015140533, Final Batch Loss: 0.24473324418067932\n",
      "Epoch 3247, Loss: 0.46059270203113556, Final Batch Loss: 0.2564750015735626\n",
      "Epoch 3248, Loss: 0.5247513949871063, Final Batch Loss: 0.27714648842811584\n",
      "Epoch 3249, Loss: 0.47947365045547485, Final Batch Loss: 0.2326744794845581\n",
      "Epoch 3250, Loss: 0.4566001445055008, Final Batch Loss: 0.1896335929632187\n",
      "Epoch 3251, Loss: 0.453799232840538, Final Batch Loss: 0.22024008631706238\n",
      "Epoch 3252, Loss: 0.45224490761756897, Final Batch Loss: 0.18668925762176514\n",
      "Epoch 3253, Loss: 0.37315379083156586, Final Batch Loss: 0.18248923122882843\n",
      "Epoch 3254, Loss: 0.4940487891435623, Final Batch Loss: 0.26739799976348877\n",
      "Epoch 3255, Loss: 0.47234387695789337, Final Batch Loss: 0.27364543080329895\n",
      "Epoch 3256, Loss: 0.4135531485080719, Final Batch Loss: 0.19038106501102448\n",
      "Epoch 3257, Loss: 0.44498884677886963, Final Batch Loss: 0.23530200123786926\n",
      "Epoch 3258, Loss: 0.3981305658817291, Final Batch Loss: 0.18233008682727814\n",
      "Epoch 3259, Loss: 0.44582466781139374, Final Batch Loss: 0.23706981539726257\n",
      "Epoch 3260, Loss: 0.39304056763648987, Final Batch Loss: 0.16896671056747437\n",
      "Epoch 3261, Loss: 0.48116207122802734, Final Batch Loss: 0.2606470584869385\n",
      "Epoch 3262, Loss: 0.4565311670303345, Final Batch Loss: 0.23433785140514374\n",
      "Epoch 3263, Loss: 0.4692032039165497, Final Batch Loss: 0.19295147061347961\n",
      "Epoch 3264, Loss: 0.4535568803548813, Final Batch Loss: 0.23993608355522156\n",
      "Epoch 3265, Loss: 0.4250811040401459, Final Batch Loss: 0.20740528404712677\n",
      "Epoch 3266, Loss: 0.40927331149578094, Final Batch Loss: 0.18175990879535675\n",
      "Epoch 3267, Loss: 0.45254796743392944, Final Batch Loss: 0.22926458716392517\n",
      "Epoch 3268, Loss: 0.421178936958313, Final Batch Loss: 0.18751053512096405\n",
      "Epoch 3269, Loss: 0.39232316613197327, Final Batch Loss: 0.1727704256772995\n",
      "Epoch 3270, Loss: 0.43544483184814453, Final Batch Loss: 0.2032778263092041\n",
      "Epoch 3271, Loss: 0.42230409383773804, Final Batch Loss: 0.20112618803977966\n",
      "Epoch 3272, Loss: 0.464179202914238, Final Batch Loss: 0.2144334614276886\n",
      "Epoch 3273, Loss: 0.4364471435546875, Final Batch Loss: 0.2295742928981781\n",
      "Epoch 3274, Loss: 0.45697836577892303, Final Batch Loss: 0.2240237146615982\n",
      "Epoch 3275, Loss: 0.48791778087615967, Final Batch Loss: 0.2290692925453186\n",
      "Epoch 3276, Loss: 0.3891107141971588, Final Batch Loss: 0.18266116082668304\n",
      "Epoch 3277, Loss: 0.4209737330675125, Final Batch Loss: 0.1582837551832199\n",
      "Epoch 3278, Loss: 0.43956349790096283, Final Batch Loss: 0.19109658896923065\n",
      "Epoch 3279, Loss: 0.38094624876976013, Final Batch Loss: 0.16274309158325195\n",
      "Epoch 3280, Loss: 0.42077112197875977, Final Batch Loss: 0.21344423294067383\n",
      "Epoch 3281, Loss: 0.4866609126329422, Final Batch Loss: 0.304437518119812\n",
      "Epoch 3282, Loss: 0.43165723979473114, Final Batch Loss: 0.2056715041399002\n",
      "Epoch 3283, Loss: 0.4374384731054306, Final Batch Loss: 0.18379215896129608\n",
      "Epoch 3284, Loss: 0.4309433251619339, Final Batch Loss: 0.2373325377702713\n",
      "Epoch 3285, Loss: 0.44086749851703644, Final Batch Loss: 0.24484358727931976\n",
      "Epoch 3286, Loss: 0.4826892167329788, Final Batch Loss: 0.20852382481098175\n",
      "Epoch 3287, Loss: 0.4375333786010742, Final Batch Loss: 0.21499687433242798\n",
      "Epoch 3288, Loss: 0.41673797369003296, Final Batch Loss: 0.21720881760120392\n",
      "Epoch 3289, Loss: 0.4089634418487549, Final Batch Loss: 0.18757610023021698\n",
      "Epoch 3290, Loss: 0.4873359054327011, Final Batch Loss: 0.263326495885849\n",
      "Epoch 3291, Loss: 0.4060838222503662, Final Batch Loss: 0.1800190955400467\n",
      "Epoch 3292, Loss: 0.4209525138139725, Final Batch Loss: 0.22094416618347168\n",
      "Epoch 3293, Loss: 0.4452548176050186, Final Batch Loss: 0.21481212973594666\n",
      "Epoch 3294, Loss: 0.45167766511440277, Final Batch Loss: 0.24436961114406586\n",
      "Epoch 3295, Loss: 0.39628802239894867, Final Batch Loss: 0.1802469938993454\n",
      "Epoch 3296, Loss: 0.4130747616291046, Final Batch Loss: 0.17347897589206696\n",
      "Epoch 3297, Loss: 0.4056380093097687, Final Batch Loss: 0.16943952441215515\n",
      "Epoch 3298, Loss: 0.48165373504161835, Final Batch Loss: 0.2671060562133789\n",
      "Epoch 3299, Loss: 0.39420536160469055, Final Batch Loss: 0.21018144488334656\n",
      "Epoch 3300, Loss: 0.43378691375255585, Final Batch Loss: 0.2345089316368103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3301, Loss: 0.3983009457588196, Final Batch Loss: 0.19742423295974731\n",
      "Epoch 3302, Loss: 0.41970326006412506, Final Batch Loss: 0.18158474564552307\n",
      "Epoch 3303, Loss: 0.47926919162273407, Final Batch Loss: 0.2541133463382721\n",
      "Epoch 3304, Loss: 0.4213765114545822, Final Batch Loss: 0.2039583921432495\n",
      "Epoch 3305, Loss: 0.4106338322162628, Final Batch Loss: 0.16174007952213287\n",
      "Epoch 3306, Loss: 0.43238380551338196, Final Batch Loss: 0.21018129587173462\n",
      "Epoch 3307, Loss: 0.4171200841665268, Final Batch Loss: 0.17127010226249695\n",
      "Epoch 3308, Loss: 0.4694162756204605, Final Batch Loss: 0.2464452087879181\n",
      "Epoch 3309, Loss: 0.42866507172584534, Final Batch Loss: 0.23772433400154114\n",
      "Epoch 3310, Loss: 0.3722241073846817, Final Batch Loss: 0.1786646991968155\n",
      "Epoch 3311, Loss: 0.449867308139801, Final Batch Loss: 0.23745349049568176\n",
      "Epoch 3312, Loss: 0.44952113926410675, Final Batch Loss: 0.26866644620895386\n",
      "Epoch 3313, Loss: 0.44462767243385315, Final Batch Loss: 0.252439022064209\n",
      "Epoch 3314, Loss: 0.4067959636449814, Final Batch Loss: 0.21569162607192993\n",
      "Epoch 3315, Loss: 0.4201967716217041, Final Batch Loss: 0.22306518256664276\n",
      "Epoch 3316, Loss: 0.44953402876853943, Final Batch Loss: 0.24614404141902924\n",
      "Epoch 3317, Loss: 0.4746926873922348, Final Batch Loss: 0.25650492310523987\n",
      "Epoch 3318, Loss: 0.3885349780321121, Final Batch Loss: 0.1841767430305481\n",
      "Epoch 3319, Loss: 0.43775998055934906, Final Batch Loss: 0.19557790458202362\n",
      "Epoch 3320, Loss: 0.43387366831302643, Final Batch Loss: 0.22165755927562714\n",
      "Epoch 3321, Loss: 0.43670736253261566, Final Batch Loss: 0.2183394879102707\n",
      "Epoch 3322, Loss: 0.40375256538391113, Final Batch Loss: 0.22384250164031982\n",
      "Epoch 3323, Loss: 0.45316295325756073, Final Batch Loss: 0.25552091002464294\n",
      "Epoch 3324, Loss: 0.42613157629966736, Final Batch Loss: 0.17503395676612854\n",
      "Epoch 3325, Loss: 0.44386668503284454, Final Batch Loss: 0.23621679842472076\n",
      "Epoch 3326, Loss: 0.4365430623292923, Final Batch Loss: 0.24296113848686218\n",
      "Epoch 3327, Loss: 0.4165976196527481, Final Batch Loss: 0.1890345811843872\n",
      "Epoch 3328, Loss: 0.4226418286561966, Final Batch Loss: 0.19996735453605652\n",
      "Epoch 3329, Loss: 0.35568760335445404, Final Batch Loss: 0.16015559434890747\n",
      "Epoch 3330, Loss: 0.43154361844062805, Final Batch Loss: 0.23317937552928925\n",
      "Epoch 3331, Loss: 0.4472056031227112, Final Batch Loss: 0.26554885506629944\n",
      "Epoch 3332, Loss: 0.3929189145565033, Final Batch Loss: 0.2017373889684677\n",
      "Epoch 3333, Loss: 0.4365922212600708, Final Batch Loss: 0.2546325623989105\n",
      "Epoch 3334, Loss: 0.49047113955020905, Final Batch Loss: 0.28097906708717346\n",
      "Epoch 3335, Loss: 0.37990039587020874, Final Batch Loss: 0.20131731033325195\n",
      "Epoch 3336, Loss: 0.4224892407655716, Final Batch Loss: 0.17158393561840057\n",
      "Epoch 3337, Loss: 0.39404284954071045, Final Batch Loss: 0.21318018436431885\n",
      "Epoch 3338, Loss: 0.44229748845100403, Final Batch Loss: 0.21521669626235962\n",
      "Epoch 3339, Loss: 0.45933619141578674, Final Batch Loss: 0.2639707028865814\n",
      "Epoch 3340, Loss: 0.4022882431745529, Final Batch Loss: 0.21676407754421234\n",
      "Epoch 3341, Loss: 0.397757425904274, Final Batch Loss: 0.190993994474411\n",
      "Epoch 3342, Loss: 0.3267808333039284, Final Batch Loss: 0.11996828764677048\n",
      "Epoch 3343, Loss: 0.39638274908065796, Final Batch Loss: 0.15440091490745544\n",
      "Epoch 3344, Loss: 0.34913191199302673, Final Batch Loss: 0.14424395561218262\n",
      "Epoch 3345, Loss: 0.4156056195497513, Final Batch Loss: 0.16959859430789948\n",
      "Epoch 3346, Loss: 0.4468398690223694, Final Batch Loss: 0.24569140374660492\n",
      "Epoch 3347, Loss: 0.4170384109020233, Final Batch Loss: 0.2037981152534485\n",
      "Epoch 3348, Loss: 0.4482276439666748, Final Batch Loss: 0.23146513104438782\n",
      "Epoch 3349, Loss: 0.40288059413433075, Final Batch Loss: 0.22375346720218658\n",
      "Epoch 3350, Loss: 0.4311417490243912, Final Batch Loss: 0.21434348821640015\n",
      "Epoch 3351, Loss: 0.3670334964990616, Final Batch Loss: 0.16726143658161163\n",
      "Epoch 3352, Loss: 0.35064399242401123, Final Batch Loss: 0.15240587294101715\n",
      "Epoch 3353, Loss: 0.45970650017261505, Final Batch Loss: 0.21440409123897552\n",
      "Epoch 3354, Loss: 0.4265819936990738, Final Batch Loss: 0.1901322603225708\n",
      "Epoch 3355, Loss: 0.41741418838500977, Final Batch Loss: 0.22436091303825378\n",
      "Epoch 3356, Loss: 0.427861824631691, Final Batch Loss: 0.2641761600971222\n",
      "Epoch 3357, Loss: 0.4181113988161087, Final Batch Loss: 0.24384717643260956\n",
      "Epoch 3358, Loss: 0.45666922628879547, Final Batch Loss: 0.21859632432460785\n",
      "Epoch 3359, Loss: 0.4945549815893173, Final Batch Loss: 0.294172078371048\n",
      "Epoch 3360, Loss: 0.42940282821655273, Final Batch Loss: 0.1780177354812622\n",
      "Epoch 3361, Loss: 0.43229885399341583, Final Batch Loss: 0.228368878364563\n",
      "Epoch 3362, Loss: 0.47149117290973663, Final Batch Loss: 0.25873443484306335\n",
      "Epoch 3363, Loss: 0.4069531112909317, Final Batch Loss: 0.18127629160881042\n",
      "Epoch 3364, Loss: 0.3450898230075836, Final Batch Loss: 0.1399986892938614\n",
      "Epoch 3365, Loss: 0.3772033154964447, Final Batch Loss: 0.21198754012584686\n",
      "Epoch 3366, Loss: 0.4046918898820877, Final Batch Loss: 0.2068408727645874\n",
      "Epoch 3367, Loss: 0.36243949830532074, Final Batch Loss: 0.16615276038646698\n",
      "Epoch 3368, Loss: 0.36002254486083984, Final Batch Loss: 0.18736810982227325\n",
      "Epoch 3369, Loss: 0.41536855697631836, Final Batch Loss: 0.2213936299085617\n",
      "Epoch 3370, Loss: 0.42819246649742126, Final Batch Loss: 0.23605971038341522\n",
      "Epoch 3371, Loss: 0.3849380612373352, Final Batch Loss: 0.1553584784269333\n",
      "Epoch 3372, Loss: 0.44097258150577545, Final Batch Loss: 0.22592538595199585\n",
      "Epoch 3373, Loss: 0.38542304933071136, Final Batch Loss: 0.16146403551101685\n",
      "Epoch 3374, Loss: 0.37688878178596497, Final Batch Loss: 0.15542913973331451\n",
      "Epoch 3375, Loss: 0.38491374254226685, Final Batch Loss: 0.18457286059856415\n",
      "Epoch 3376, Loss: 0.4124637395143509, Final Batch Loss: 0.23162336647510529\n",
      "Epoch 3377, Loss: 0.36850208044052124, Final Batch Loss: 0.18746381998062134\n",
      "Epoch 3378, Loss: 0.4590505063533783, Final Batch Loss: 0.23053336143493652\n",
      "Epoch 3379, Loss: 0.43763017654418945, Final Batch Loss: 0.23160898685455322\n",
      "Epoch 3380, Loss: 0.44076327979564667, Final Batch Loss: 0.21135585010051727\n",
      "Epoch 3381, Loss: 0.45196621119976044, Final Batch Loss: 0.2848595678806305\n",
      "Epoch 3382, Loss: 0.44313478469848633, Final Batch Loss: 0.2231030911207199\n",
      "Epoch 3383, Loss: 0.3769712448120117, Final Batch Loss: 0.14810395240783691\n",
      "Epoch 3384, Loss: 0.3868598937988281, Final Batch Loss: 0.20589444041252136\n",
      "Epoch 3385, Loss: 0.4373261034488678, Final Batch Loss: 0.23243674635887146\n",
      "Epoch 3386, Loss: 0.3720596581697464, Final Batch Loss: 0.16657888889312744\n",
      "Epoch 3387, Loss: 0.3614559769630432, Final Batch Loss: 0.1703050285577774\n",
      "Epoch 3388, Loss: 0.41733117401599884, Final Batch Loss: 0.19681672751903534\n",
      "Epoch 3389, Loss: 0.4286031275987625, Final Batch Loss: 0.17790965735912323\n",
      "Epoch 3390, Loss: 0.39256924390792847, Final Batch Loss: 0.17369426786899567\n",
      "Epoch 3391, Loss: 0.40485037863254547, Final Batch Loss: 0.20365440845489502\n",
      "Epoch 3392, Loss: 0.3612476885318756, Final Batch Loss: 0.15136070549488068\n",
      "Epoch 3393, Loss: 0.4250495731830597, Final Batch Loss: 0.2246493548154831\n",
      "Epoch 3394, Loss: 0.42617183923721313, Final Batch Loss: 0.24616102874279022\n",
      "Epoch 3395, Loss: 0.5011505633592606, Final Batch Loss: 0.3229850232601166\n",
      "Epoch 3396, Loss: 0.43242068588733673, Final Batch Loss: 0.21309934556484222\n",
      "Epoch 3397, Loss: 0.3776809275150299, Final Batch Loss: 0.1731932908296585\n",
      "Epoch 3398, Loss: 0.37293925881385803, Final Batch Loss: 0.1606701761484146\n",
      "Epoch 3399, Loss: 0.3852703720331192, Final Batch Loss: 0.2071099877357483\n",
      "Epoch 3400, Loss: 0.349409744143486, Final Batch Loss: 0.1487032175064087\n",
      "Epoch 3401, Loss: 0.36012569069862366, Final Batch Loss: 0.15136095881462097\n",
      "Epoch 3402, Loss: 0.4362858831882477, Final Batch Loss: 0.24817651510238647\n",
      "Epoch 3403, Loss: 0.39730218052864075, Final Batch Loss: 0.2201872169971466\n",
      "Epoch 3404, Loss: 0.4042339026927948, Final Batch Loss: 0.182713121175766\n",
      "Epoch 3405, Loss: 0.3897676020860672, Final Batch Loss: 0.1964872032403946\n",
      "Epoch 3406, Loss: 0.361985445022583, Final Batch Loss: 0.179949089884758\n",
      "Epoch 3407, Loss: 0.45453377068042755, Final Batch Loss: 0.24604524672031403\n",
      "Epoch 3408, Loss: 0.40820474922657013, Final Batch Loss: 0.1849764585494995\n",
      "Epoch 3409, Loss: 0.40849095582962036, Final Batch Loss: 0.2022596299648285\n",
      "Epoch 3410, Loss: 0.40611550211906433, Final Batch Loss: 0.24348923563957214\n",
      "Epoch 3411, Loss: 0.4082048386335373, Final Batch Loss: 0.20621782541275024\n",
      "Epoch 3412, Loss: 0.4104176163673401, Final Batch Loss: 0.1912299245595932\n",
      "Epoch 3413, Loss: 0.3726591616868973, Final Batch Loss: 0.19904959201812744\n",
      "Epoch 3414, Loss: 0.433564692735672, Final Batch Loss: 0.22077257931232452\n",
      "Epoch 3415, Loss: 0.4416874051094055, Final Batch Loss: 0.21478548645973206\n",
      "Epoch 3416, Loss: 0.3757138103246689, Final Batch Loss: 0.15758942067623138\n",
      "Epoch 3417, Loss: 0.41608551144599915, Final Batch Loss: 0.20443296432495117\n",
      "Epoch 3418, Loss: 0.3580345958471298, Final Batch Loss: 0.18453043699264526\n",
      "Epoch 3419, Loss: 0.41093818843364716, Final Batch Loss: 0.22133111953735352\n",
      "Epoch 3420, Loss: 0.4056021422147751, Final Batch Loss: 0.178288534283638\n",
      "Epoch 3421, Loss: 0.3487003892660141, Final Batch Loss: 0.15631571412086487\n",
      "Epoch 3422, Loss: 0.44277459383010864, Final Batch Loss: 0.23942777514457703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3423, Loss: 0.3852575123310089, Final Batch Loss: 0.18197090923786163\n",
      "Epoch 3424, Loss: 0.4064635783433914, Final Batch Loss: 0.21466873586177826\n",
      "Epoch 3425, Loss: 0.3678978830575943, Final Batch Loss: 0.2103307843208313\n",
      "Epoch 3426, Loss: 0.3802524507045746, Final Batch Loss: 0.18714125454425812\n",
      "Epoch 3427, Loss: 0.3818894773721695, Final Batch Loss: 0.18808838725090027\n",
      "Epoch 3428, Loss: 0.45235849916934967, Final Batch Loss: 0.2509556710720062\n",
      "Epoch 3429, Loss: 0.44480061531066895, Final Batch Loss: 0.22636346518993378\n",
      "Epoch 3430, Loss: 0.38333937525749207, Final Batch Loss: 0.2146369218826294\n",
      "Epoch 3431, Loss: 0.43075020611286163, Final Batch Loss: 0.21422062814235687\n",
      "Epoch 3432, Loss: 0.426318421959877, Final Batch Loss: 0.21058203279972076\n",
      "Epoch 3433, Loss: 0.36666345596313477, Final Batch Loss: 0.1554660052061081\n",
      "Epoch 3434, Loss: 0.44406160712242126, Final Batch Loss: 0.2436983734369278\n",
      "Epoch 3435, Loss: 0.40118078887462616, Final Batch Loss: 0.2273586094379425\n",
      "Epoch 3436, Loss: 0.45853909850120544, Final Batch Loss: 0.23293457925319672\n",
      "Epoch 3437, Loss: 0.3853946775197983, Final Batch Loss: 0.19889047741889954\n",
      "Epoch 3438, Loss: 0.33696436882019043, Final Batch Loss: 0.14066998660564423\n",
      "Epoch 3439, Loss: 0.4119131416082382, Final Batch Loss: 0.21020853519439697\n",
      "Epoch 3440, Loss: 0.4425867795944214, Final Batch Loss: 0.22495369613170624\n",
      "Epoch 3441, Loss: 0.4005071222782135, Final Batch Loss: 0.22776052355766296\n",
      "Epoch 3442, Loss: 0.40347155928611755, Final Batch Loss: 0.178499236702919\n",
      "Epoch 3443, Loss: 0.34886784851551056, Final Batch Loss: 0.17410621047019958\n",
      "Epoch 3444, Loss: 0.39909984171390533, Final Batch Loss: 0.21210110187530518\n",
      "Epoch 3445, Loss: 0.47391246259212494, Final Batch Loss: 0.25932469964027405\n",
      "Epoch 3446, Loss: 0.4508681446313858, Final Batch Loss: 0.2102740854024887\n",
      "Epoch 3447, Loss: 0.3416965901851654, Final Batch Loss: 0.18460401892662048\n",
      "Epoch 3448, Loss: 0.44391532242298126, Final Batch Loss: 0.22390241920948029\n",
      "Epoch 3449, Loss: 0.40662699937820435, Final Batch Loss: 0.21490579843521118\n",
      "Epoch 3450, Loss: 0.3587566316127777, Final Batch Loss: 0.17267286777496338\n",
      "Epoch 3451, Loss: 0.3832741975784302, Final Batch Loss: 0.1882670670747757\n",
      "Epoch 3452, Loss: 0.38360142707824707, Final Batch Loss: 0.15849249064922333\n",
      "Epoch 3453, Loss: 0.3854948580265045, Final Batch Loss: 0.1694134920835495\n",
      "Epoch 3454, Loss: 0.4650367647409439, Final Batch Loss: 0.29118072986602783\n",
      "Epoch 3455, Loss: 0.44618238508701324, Final Batch Loss: 0.25546303391456604\n",
      "Epoch 3456, Loss: 0.3989114314317703, Final Batch Loss: 0.2165660709142685\n",
      "Epoch 3457, Loss: 0.4128439128398895, Final Batch Loss: 0.21451154351234436\n",
      "Epoch 3458, Loss: 0.3891665041446686, Final Batch Loss: 0.19200925529003143\n",
      "Epoch 3459, Loss: 0.4012235999107361, Final Batch Loss: 0.2393903285264969\n",
      "Epoch 3460, Loss: 0.4003676176071167, Final Batch Loss: 0.18906502425670624\n",
      "Epoch 3461, Loss: 0.3776785284280777, Final Batch Loss: 0.15222540497779846\n",
      "Epoch 3462, Loss: 0.36115503311157227, Final Batch Loss: 0.12834157049655914\n",
      "Epoch 3463, Loss: 0.3994736820459366, Final Batch Loss: 0.163033127784729\n",
      "Epoch 3464, Loss: 0.38223783671855927, Final Batch Loss: 0.14058652520179749\n",
      "Epoch 3465, Loss: 0.409877747297287, Final Batch Loss: 0.1658107042312622\n",
      "Epoch 3466, Loss: 0.3773431032896042, Final Batch Loss: 0.19100293517112732\n",
      "Epoch 3467, Loss: 0.4128574728965759, Final Batch Loss: 0.23077945411205292\n",
      "Epoch 3468, Loss: 0.3927115499973297, Final Batch Loss: 0.1959885209798813\n",
      "Epoch 3469, Loss: 0.3820727616548538, Final Batch Loss: 0.21699976921081543\n",
      "Epoch 3470, Loss: 0.41009221971035004, Final Batch Loss: 0.20987986028194427\n",
      "Epoch 3471, Loss: 0.3794746398925781, Final Batch Loss: 0.17292284965515137\n",
      "Epoch 3472, Loss: 0.388491228222847, Final Batch Loss: 0.19341586530208588\n",
      "Epoch 3473, Loss: 0.4094719737768173, Final Batch Loss: 0.2095986008644104\n",
      "Epoch 3474, Loss: 0.4574609249830246, Final Batch Loss: 0.2181146889925003\n",
      "Epoch 3475, Loss: 0.37432654201984406, Final Batch Loss: 0.14234186708927155\n",
      "Epoch 3476, Loss: 0.3554241806268692, Final Batch Loss: 0.1596888303756714\n",
      "Epoch 3477, Loss: 0.3403002619743347, Final Batch Loss: 0.1624438762664795\n",
      "Epoch 3478, Loss: 0.371734082698822, Final Batch Loss: 0.19559700787067413\n",
      "Epoch 3479, Loss: 0.40564851462841034, Final Batch Loss: 0.17853105068206787\n",
      "Epoch 3480, Loss: 0.32180650532245636, Final Batch Loss: 0.1340872049331665\n",
      "Epoch 3481, Loss: 0.41299009323120117, Final Batch Loss: 0.21756817400455475\n",
      "Epoch 3482, Loss: 0.34696023166179657, Final Batch Loss: 0.186356320977211\n",
      "Epoch 3483, Loss: 0.3654349744319916, Final Batch Loss: 0.15374919772148132\n",
      "Epoch 3484, Loss: 0.4175140708684921, Final Batch Loss: 0.23865801095962524\n",
      "Epoch 3485, Loss: 0.4217631220817566, Final Batch Loss: 0.19359412789344788\n",
      "Epoch 3486, Loss: 0.39243805408477783, Final Batch Loss: 0.20918059349060059\n",
      "Epoch 3487, Loss: 0.4127728044986725, Final Batch Loss: 0.22065331041812897\n",
      "Epoch 3488, Loss: 0.3699629306793213, Final Batch Loss: 0.17733889818191528\n",
      "Epoch 3489, Loss: 0.46281272172927856, Final Batch Loss: 0.18745285272598267\n",
      "Epoch 3490, Loss: 0.4161911904811859, Final Batch Loss: 0.2119353860616684\n",
      "Epoch 3491, Loss: 0.33460482954978943, Final Batch Loss: 0.17058390378952026\n",
      "Epoch 3492, Loss: 0.36596277356147766, Final Batch Loss: 0.1514890342950821\n",
      "Epoch 3493, Loss: 0.3846534639596939, Final Batch Loss: 0.17962133884429932\n",
      "Epoch 3494, Loss: 0.4071570187807083, Final Batch Loss: 0.23865091800689697\n",
      "Epoch 3495, Loss: 0.3759840875864029, Final Batch Loss: 0.17358581721782684\n",
      "Epoch 3496, Loss: 0.4007638543844223, Final Batch Loss: 0.20729944109916687\n",
      "Epoch 3497, Loss: 0.4161021560430527, Final Batch Loss: 0.2184876948595047\n",
      "Epoch 3498, Loss: 0.35390858352184296, Final Batch Loss: 0.14583082497119904\n",
      "Epoch 3499, Loss: 0.34384436905384064, Final Batch Loss: 0.18004043400287628\n",
      "Epoch 3500, Loss: 0.3138558566570282, Final Batch Loss: 0.15319137275218964\n",
      "Epoch 3501, Loss: 0.35316042602062225, Final Batch Loss: 0.18561266362667084\n",
      "Epoch 3502, Loss: 0.37241432070732117, Final Batch Loss: 0.17216961085796356\n",
      "Epoch 3503, Loss: 0.40158534049987793, Final Batch Loss: 0.17357298731803894\n",
      "Epoch 3504, Loss: 0.40656520426273346, Final Batch Loss: 0.18513581156730652\n",
      "Epoch 3505, Loss: 0.4353228956460953, Final Batch Loss: 0.18120194971561432\n",
      "Epoch 3506, Loss: 0.4412298649549484, Final Batch Loss: 0.23343877494335175\n",
      "Epoch 3507, Loss: 0.5055899322032928, Final Batch Loss: 0.3359202444553375\n",
      "Epoch 3508, Loss: 0.35948336124420166, Final Batch Loss: 0.19778648018836975\n",
      "Epoch 3509, Loss: 0.38647250831127167, Final Batch Loss: 0.2346629649400711\n",
      "Epoch 3510, Loss: 0.44861264526844025, Final Batch Loss: 0.26246440410614014\n",
      "Epoch 3511, Loss: 0.36518359184265137, Final Batch Loss: 0.18110814690589905\n",
      "Epoch 3512, Loss: 0.32084324955940247, Final Batch Loss: 0.12503109872341156\n",
      "Epoch 3513, Loss: 0.36105045676231384, Final Batch Loss: 0.18359920382499695\n",
      "Epoch 3514, Loss: 0.36274322867393494, Final Batch Loss: 0.15971997380256653\n",
      "Epoch 3515, Loss: 0.43233253061771393, Final Batch Loss: 0.21665361523628235\n",
      "Epoch 3516, Loss: 0.38323555886745453, Final Batch Loss: 0.19329622387886047\n",
      "Epoch 3517, Loss: 0.4237707257270813, Final Batch Loss: 0.21480312943458557\n",
      "Epoch 3518, Loss: 0.3299599289894104, Final Batch Loss: 0.1429903209209442\n",
      "Epoch 3519, Loss: 0.41323117911815643, Final Batch Loss: 0.2316504269838333\n",
      "Epoch 3520, Loss: 0.3429471105337143, Final Batch Loss: 0.15716151893138885\n",
      "Epoch 3521, Loss: 0.38650451600551605, Final Batch Loss: 0.22446779906749725\n",
      "Epoch 3522, Loss: 0.37556521594524384, Final Batch Loss: 0.1925530731678009\n",
      "Epoch 3523, Loss: 0.3770347833633423, Final Batch Loss: 0.1606898456811905\n",
      "Epoch 3524, Loss: 0.3667386323213577, Final Batch Loss: 0.16324223577976227\n",
      "Epoch 3525, Loss: 0.43174396455287933, Final Batch Loss: 0.22633787989616394\n",
      "Epoch 3526, Loss: 0.3921944350004196, Final Batch Loss: 0.20923709869384766\n",
      "Epoch 3527, Loss: 0.33391065895557404, Final Batch Loss: 0.16478845477104187\n",
      "Epoch 3528, Loss: 0.43041492998600006, Final Batch Loss: 0.2171747237443924\n",
      "Epoch 3529, Loss: 0.3615420311689377, Final Batch Loss: 0.18036946654319763\n",
      "Epoch 3530, Loss: 0.37207357585430145, Final Batch Loss: 0.17895106971263885\n",
      "Epoch 3531, Loss: 0.4267215132713318, Final Batch Loss: 0.23259413242340088\n",
      "Epoch 3532, Loss: 0.4303762763738632, Final Batch Loss: 0.24294540286064148\n",
      "Epoch 3533, Loss: 0.41461870074272156, Final Batch Loss: 0.22303859889507294\n",
      "Epoch 3534, Loss: 0.4357364922761917, Final Batch Loss: 0.24364672601222992\n",
      "Epoch 3535, Loss: 0.38227446377277374, Final Batch Loss: 0.17528675496578217\n",
      "Epoch 3536, Loss: 0.345335990190506, Final Batch Loss: 0.1908748596906662\n",
      "Epoch 3537, Loss: 0.3870415687561035, Final Batch Loss: 0.2258973866701126\n",
      "Epoch 3538, Loss: 0.43325966596603394, Final Batch Loss: 0.2209583967924118\n",
      "Epoch 3539, Loss: 0.4183901995420456, Final Batch Loss: 0.23995526134967804\n",
      "Epoch 3540, Loss: 0.3439350426197052, Final Batch Loss: 0.1440441906452179\n",
      "Epoch 3541, Loss: 0.32487232983112335, Final Batch Loss: 0.1603487879037857\n",
      "Epoch 3542, Loss: 0.35657450556755066, Final Batch Loss: 0.171595498919487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3543, Loss: 0.4232787936925888, Final Batch Loss: 0.22388826310634613\n",
      "Epoch 3544, Loss: 0.3946709930896759, Final Batch Loss: 0.2026059329509735\n",
      "Epoch 3545, Loss: 0.4107537418603897, Final Batch Loss: 0.22199293971061707\n",
      "Epoch 3546, Loss: 0.3990206718444824, Final Batch Loss: 0.22301538288593292\n",
      "Epoch 3547, Loss: 0.4188574552536011, Final Batch Loss: 0.22619085013866425\n",
      "Epoch 3548, Loss: 0.46596159040927887, Final Batch Loss: 0.27154111862182617\n",
      "Epoch 3549, Loss: 0.4756678640842438, Final Batch Loss: 0.29407739639282227\n",
      "Epoch 3550, Loss: 0.4367543160915375, Final Batch Loss: 0.20448586344718933\n",
      "Epoch 3551, Loss: 0.35876281559467316, Final Batch Loss: 0.13466887176036835\n",
      "Epoch 3552, Loss: 0.4362339675426483, Final Batch Loss: 0.22509250044822693\n",
      "Epoch 3553, Loss: 0.3844630569219589, Final Batch Loss: 0.21237453818321228\n",
      "Epoch 3554, Loss: 0.3384060710668564, Final Batch Loss: 0.18281462788581848\n",
      "Epoch 3555, Loss: 0.3741077035665512, Final Batch Loss: 0.1877761334180832\n",
      "Epoch 3556, Loss: 0.4275661110877991, Final Batch Loss: 0.1568106710910797\n",
      "Epoch 3557, Loss: 0.34923219680786133, Final Batch Loss: 0.17330941557884216\n",
      "Epoch 3558, Loss: 0.4325634688138962, Final Batch Loss: 0.20802582800388336\n",
      "Epoch 3559, Loss: 0.35417935252189636, Final Batch Loss: 0.17261749505996704\n",
      "Epoch 3560, Loss: 0.500290110707283, Final Batch Loss: 0.27485859394073486\n",
      "Epoch 3561, Loss: 0.3340757042169571, Final Batch Loss: 0.12992489337921143\n",
      "Epoch 3562, Loss: 0.3935229629278183, Final Batch Loss: 0.19580887258052826\n",
      "Epoch 3563, Loss: 0.36906370520591736, Final Batch Loss: 0.2158185839653015\n",
      "Epoch 3564, Loss: 0.4180525094270706, Final Batch Loss: 0.2583085894584656\n",
      "Epoch 3565, Loss: 0.4421241134405136, Final Batch Loss: 0.20820999145507812\n",
      "Epoch 3566, Loss: 0.3790804296731949, Final Batch Loss: 0.20519109070301056\n",
      "Epoch 3567, Loss: 0.3153025209903717, Final Batch Loss: 0.13491714000701904\n",
      "Epoch 3568, Loss: 0.3819815069437027, Final Batch Loss: 0.17684882879257202\n",
      "Epoch 3569, Loss: 0.37024517357349396, Final Batch Loss: 0.1769963651895523\n",
      "Epoch 3570, Loss: 0.39913493394851685, Final Batch Loss: 0.24529340863227844\n",
      "Epoch 3571, Loss: 0.4385637193918228, Final Batch Loss: 0.23279158771038055\n",
      "Epoch 3572, Loss: 0.3621901124715805, Final Batch Loss: 0.18066978454589844\n",
      "Epoch 3573, Loss: 0.4122770130634308, Final Batch Loss: 0.2457038164138794\n",
      "Epoch 3574, Loss: 0.377407506108284, Final Batch Loss: 0.17663952708244324\n",
      "Epoch 3575, Loss: 0.34621138870716095, Final Batch Loss: 0.1695365309715271\n",
      "Epoch 3576, Loss: 0.4249536395072937, Final Batch Loss: 0.2009771764278412\n",
      "Epoch 3577, Loss: 0.38570769131183624, Final Batch Loss: 0.22162942588329315\n",
      "Epoch 3578, Loss: 0.4059396833181381, Final Batch Loss: 0.2241630107164383\n",
      "Epoch 3579, Loss: 0.33327971398830414, Final Batch Loss: 0.1736459881067276\n",
      "Epoch 3580, Loss: 0.34597454965114594, Final Batch Loss: 0.1826130896806717\n",
      "Epoch 3581, Loss: 0.33630916476249695, Final Batch Loss: 0.13054688274860382\n",
      "Epoch 3582, Loss: 0.3592974543571472, Final Batch Loss: 0.16728362441062927\n",
      "Epoch 3583, Loss: 0.5099472254514694, Final Batch Loss: 0.3014404773712158\n",
      "Epoch 3584, Loss: 0.3850944936275482, Final Batch Loss: 0.22810198366641998\n",
      "Epoch 3585, Loss: 0.4285784065723419, Final Batch Loss: 0.2087922990322113\n",
      "Epoch 3586, Loss: 0.3254730850458145, Final Batch Loss: 0.13420067727565765\n",
      "Epoch 3587, Loss: 0.39592719078063965, Final Batch Loss: 0.2068634033203125\n",
      "Epoch 3588, Loss: 0.36135075986385345, Final Batch Loss: 0.17640027403831482\n",
      "Epoch 3589, Loss: 0.34298259019851685, Final Batch Loss: 0.16732071340084076\n",
      "Epoch 3590, Loss: 0.3399047553539276, Final Batch Loss: 0.18868471682071686\n",
      "Epoch 3591, Loss: 0.41875123977661133, Final Batch Loss: 0.20710009336471558\n",
      "Epoch 3592, Loss: 0.42188914120197296, Final Batch Loss: 0.19513936340808868\n",
      "Epoch 3593, Loss: 0.39648768305778503, Final Batch Loss: 0.21543541550636292\n",
      "Epoch 3594, Loss: 0.3636537939310074, Final Batch Loss: 0.18474814295768738\n",
      "Epoch 3595, Loss: 0.3878279775381088, Final Batch Loss: 0.21443389356136322\n",
      "Epoch 3596, Loss: 0.3630557209253311, Final Batch Loss: 0.18418176472187042\n",
      "Epoch 3597, Loss: 0.36200809478759766, Final Batch Loss: 0.13493454456329346\n",
      "Epoch 3598, Loss: 0.5912435054779053, Final Batch Loss: 0.29820653796195984\n",
      "Epoch 3599, Loss: 0.35197846591472626, Final Batch Loss: 0.16910690069198608\n",
      "Epoch 3600, Loss: 0.39246533811092377, Final Batch Loss: 0.18197275698184967\n",
      "Epoch 3601, Loss: 0.3698304444551468, Final Batch Loss: 0.18972761929035187\n",
      "Epoch 3602, Loss: 0.35392506420612335, Final Batch Loss: 0.19463753700256348\n",
      "Epoch 3603, Loss: 0.32829034328460693, Final Batch Loss: 0.16053198277950287\n",
      "Epoch 3604, Loss: 0.34700027108192444, Final Batch Loss: 0.14518707990646362\n",
      "Epoch 3605, Loss: 0.33895809948444366, Final Batch Loss: 0.16350995004177094\n",
      "Epoch 3606, Loss: 0.3473310023546219, Final Batch Loss: 0.14856339991092682\n",
      "Epoch 3607, Loss: 0.3399132639169693, Final Batch Loss: 0.1857142597436905\n",
      "Epoch 3608, Loss: 0.3750499337911606, Final Batch Loss: 0.16817857325077057\n",
      "Epoch 3609, Loss: 0.4138954132795334, Final Batch Loss: 0.20685137808322906\n",
      "Epoch 3610, Loss: 0.3705393075942993, Final Batch Loss: 0.22458705306053162\n",
      "Epoch 3611, Loss: 0.39041732251644135, Final Batch Loss: 0.21372047066688538\n",
      "Epoch 3612, Loss: 0.39782826602458954, Final Batch Loss: 0.20147804915905\n",
      "Epoch 3613, Loss: 0.3228706568479538, Final Batch Loss: 0.14745190739631653\n",
      "Epoch 3614, Loss: 0.4022678732872009, Final Batch Loss: 0.17121323943138123\n",
      "Epoch 3615, Loss: 0.37301765382289886, Final Batch Loss: 0.18992696702480316\n",
      "Epoch 3616, Loss: 0.4135930985212326, Final Batch Loss: 0.20689933001995087\n",
      "Epoch 3617, Loss: 0.3272368460893631, Final Batch Loss: 0.15504315495491028\n",
      "Epoch 3618, Loss: 0.4412373751401901, Final Batch Loss: 0.19289687275886536\n",
      "Epoch 3619, Loss: 0.40363267064094543, Final Batch Loss: 0.20281189680099487\n",
      "Epoch 3620, Loss: 0.37044571340084076, Final Batch Loss: 0.18866349756717682\n",
      "Epoch 3621, Loss: 0.42827455699443817, Final Batch Loss: 0.22681215405464172\n",
      "Epoch 3622, Loss: 0.46141332387924194, Final Batch Loss: 0.2663635015487671\n",
      "Epoch 3623, Loss: 0.3696070909500122, Final Batch Loss: 0.19095396995544434\n",
      "Epoch 3624, Loss: 0.3494441658258438, Final Batch Loss: 0.1963644027709961\n",
      "Epoch 3625, Loss: 0.33741846680641174, Final Batch Loss: 0.17381469905376434\n",
      "Epoch 3626, Loss: 0.4273623675107956, Final Batch Loss: 0.1990436315536499\n",
      "Epoch 3627, Loss: 0.42682354152202606, Final Batch Loss: 0.20737846195697784\n",
      "Epoch 3628, Loss: 0.3989465981721878, Final Batch Loss: 0.17491929233074188\n",
      "Epoch 3629, Loss: 0.35717885196208954, Final Batch Loss: 0.18325422704219818\n",
      "Epoch 3630, Loss: 0.41941478848457336, Final Batch Loss: 0.25743335485458374\n",
      "Epoch 3631, Loss: 0.3952431082725525, Final Batch Loss: 0.2009134292602539\n",
      "Epoch 3632, Loss: 0.3874789774417877, Final Batch Loss: 0.21644146740436554\n",
      "Epoch 3633, Loss: 0.3585668206214905, Final Batch Loss: 0.1589183360338211\n",
      "Epoch 3634, Loss: 0.3953859508037567, Final Batch Loss: 0.22619754076004028\n",
      "Epoch 3635, Loss: 0.3777557462453842, Final Batch Loss: 0.17021380364894867\n",
      "Epoch 3636, Loss: 0.4582755118608475, Final Batch Loss: 0.2889132499694824\n",
      "Epoch 3637, Loss: 0.4316898286342621, Final Batch Loss: 0.2561750113964081\n",
      "Epoch 3638, Loss: 0.3654183894395828, Final Batch Loss: 0.18364395201206207\n",
      "Epoch 3639, Loss: 0.37618987262248993, Final Batch Loss: 0.1981891244649887\n",
      "Epoch 3640, Loss: 0.3625750243663788, Final Batch Loss: 0.1978767216205597\n",
      "Epoch 3641, Loss: 0.35809941589832306, Final Batch Loss: 0.15588417649269104\n",
      "Epoch 3642, Loss: 0.37700343132019043, Final Batch Loss: 0.1688123196363449\n",
      "Epoch 3643, Loss: 0.3661185950040817, Final Batch Loss: 0.20301739871501923\n",
      "Epoch 3644, Loss: 0.36311402916908264, Final Batch Loss: 0.15817226469516754\n",
      "Epoch 3645, Loss: 0.35538896918296814, Final Batch Loss: 0.1358468234539032\n",
      "Epoch 3646, Loss: 0.34685690701007843, Final Batch Loss: 0.16133490204811096\n",
      "Epoch 3647, Loss: 0.3667246997356415, Final Batch Loss: 0.20801351964473724\n",
      "Epoch 3648, Loss: 0.3695697784423828, Final Batch Loss: 0.20213356614112854\n",
      "Epoch 3649, Loss: 0.4058789014816284, Final Batch Loss: 0.21061038970947266\n",
      "Epoch 3650, Loss: 0.4128503203392029, Final Batch Loss: 0.2495904415845871\n",
      "Epoch 3651, Loss: 0.3129853159189224, Final Batch Loss: 0.12796375155448914\n",
      "Epoch 3652, Loss: 0.3706929385662079, Final Batch Loss: 0.1971474289894104\n",
      "Epoch 3653, Loss: 0.35349129140377045, Final Batch Loss: 0.1861903965473175\n",
      "Epoch 3654, Loss: 0.328422948718071, Final Batch Loss: 0.15702587366104126\n",
      "Epoch 3655, Loss: 0.40950606763362885, Final Batch Loss: 0.23454804718494415\n",
      "Epoch 3656, Loss: 0.37053969502449036, Final Batch Loss: 0.19688323140144348\n",
      "Epoch 3657, Loss: 0.35509881377220154, Final Batch Loss: 0.18307651579380035\n",
      "Epoch 3658, Loss: 0.43307963013648987, Final Batch Loss: 0.256201833486557\n",
      "Epoch 3659, Loss: 0.3414917588233948, Final Batch Loss: 0.1512288600206375\n",
      "Epoch 3660, Loss: 0.4123269021511078, Final Batch Loss: 0.22940540313720703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3661, Loss: 0.4347022622823715, Final Batch Loss: 0.2487190067768097\n",
      "Epoch 3662, Loss: 0.3451423645019531, Final Batch Loss: 0.1519845575094223\n",
      "Epoch 3663, Loss: 0.34711819887161255, Final Batch Loss: 0.17520178854465485\n",
      "Epoch 3664, Loss: 0.33022768795490265, Final Batch Loss: 0.1512102633714676\n",
      "Epoch 3665, Loss: 0.3761025220155716, Final Batch Loss: 0.179692804813385\n",
      "Epoch 3666, Loss: 0.3876389265060425, Final Batch Loss: 0.17333872616291046\n",
      "Epoch 3667, Loss: 0.31859734654426575, Final Batch Loss: 0.1471511274576187\n",
      "Epoch 3668, Loss: 0.33547188341617584, Final Batch Loss: 0.1554838865995407\n",
      "Epoch 3669, Loss: 0.37753449380397797, Final Batch Loss: 0.16687098145484924\n",
      "Epoch 3670, Loss: 0.3815714716911316, Final Batch Loss: 0.17076578736305237\n",
      "Epoch 3671, Loss: 0.4036497473716736, Final Batch Loss: 0.2029608190059662\n",
      "Epoch 3672, Loss: 0.34064243733882904, Final Batch Loss: 0.14262525737285614\n",
      "Epoch 3673, Loss: 0.36072225868701935, Final Batch Loss: 0.1804637461900711\n",
      "Epoch 3674, Loss: 0.3984982967376709, Final Batch Loss: 0.19429592788219452\n",
      "Epoch 3675, Loss: 0.4072587937116623, Final Batch Loss: 0.1883372962474823\n",
      "Epoch 3676, Loss: 0.32314111292362213, Final Batch Loss: 0.1508752554655075\n",
      "Epoch 3677, Loss: 0.36705493927001953, Final Batch Loss: 0.13478727638721466\n",
      "Epoch 3678, Loss: 0.3451237380504608, Final Batch Loss: 0.15562835335731506\n",
      "Epoch 3679, Loss: 0.3906873017549515, Final Batch Loss: 0.18751473724842072\n",
      "Epoch 3680, Loss: 0.3814350664615631, Final Batch Loss: 0.18005777895450592\n",
      "Epoch 3681, Loss: 0.3689713776111603, Final Batch Loss: 0.20754149556159973\n",
      "Epoch 3682, Loss: 0.307380810379982, Final Batch Loss: 0.14035403728485107\n",
      "Epoch 3683, Loss: 0.40444640815258026, Final Batch Loss: 0.22459766268730164\n",
      "Epoch 3684, Loss: 0.3971274644136429, Final Batch Loss: 0.2049797624349594\n",
      "Epoch 3685, Loss: 0.3678168058395386, Final Batch Loss: 0.18251535296440125\n",
      "Epoch 3686, Loss: 0.37584634125232697, Final Batch Loss: 0.18113753199577332\n",
      "Epoch 3687, Loss: 0.3970164805650711, Final Batch Loss: 0.20850643515586853\n",
      "Epoch 3688, Loss: 0.4008243829011917, Final Batch Loss: 0.22815082967281342\n",
      "Epoch 3689, Loss: 0.4334712624549866, Final Batch Loss: 0.2230156511068344\n",
      "Epoch 3690, Loss: 0.3468969613313675, Final Batch Loss: 0.15436489880084991\n",
      "Epoch 3691, Loss: 0.37973950803279877, Final Batch Loss: 0.16946375370025635\n",
      "Epoch 3692, Loss: 0.40086400508880615, Final Batch Loss: 0.25075042247772217\n",
      "Epoch 3693, Loss: 0.44081704318523407, Final Batch Loss: 0.24372296035289764\n",
      "Epoch 3694, Loss: 0.31445570290088654, Final Batch Loss: 0.12571527063846588\n",
      "Epoch 3695, Loss: 0.3556160032749176, Final Batch Loss: 0.1674177646636963\n",
      "Epoch 3696, Loss: 0.31617340445518494, Final Batch Loss: 0.15017756819725037\n",
      "Epoch 3697, Loss: 0.35691794753074646, Final Batch Loss: 0.18829376995563507\n",
      "Epoch 3698, Loss: 0.4148099422454834, Final Batch Loss: 0.24229954183101654\n",
      "Epoch 3699, Loss: 0.39850202202796936, Final Batch Loss: 0.19679370522499084\n",
      "Epoch 3700, Loss: 0.37113167345523834, Final Batch Loss: 0.14067994058132172\n",
      "Epoch 3701, Loss: 0.42015865445137024, Final Batch Loss: 0.21721284091472626\n",
      "Epoch 3702, Loss: 0.36707668006420135, Final Batch Loss: 0.15151767432689667\n",
      "Epoch 3703, Loss: 0.3996601700782776, Final Batch Loss: 0.21706049144268036\n",
      "Epoch 3704, Loss: 0.40341004729270935, Final Batch Loss: 0.24351520836353302\n",
      "Epoch 3705, Loss: 0.3320769816637039, Final Batch Loss: 0.1542131006717682\n",
      "Epoch 3706, Loss: 0.3421565443277359, Final Batch Loss: 0.17692339420318604\n",
      "Epoch 3707, Loss: 0.34625646471977234, Final Batch Loss: 0.1480294018983841\n",
      "Epoch 3708, Loss: 0.33422866463661194, Final Batch Loss: 0.1679786592721939\n",
      "Epoch 3709, Loss: 0.3844147175550461, Final Batch Loss: 0.20281268656253815\n",
      "Epoch 3710, Loss: 0.35854871571063995, Final Batch Loss: 0.20441266894340515\n",
      "Epoch 3711, Loss: 0.35902756452560425, Final Batch Loss: 0.1994418054819107\n",
      "Epoch 3712, Loss: 0.3498934805393219, Final Batch Loss: 0.18653172254562378\n",
      "Epoch 3713, Loss: 0.3585680276155472, Final Batch Loss: 0.15533329546451569\n",
      "Epoch 3714, Loss: 0.33530010282993317, Final Batch Loss: 0.15236631035804749\n",
      "Epoch 3715, Loss: 0.32676587998867035, Final Batch Loss: 0.14455540478229523\n",
      "Epoch 3716, Loss: 0.35805292427539825, Final Batch Loss: 0.20942941308021545\n",
      "Epoch 3717, Loss: 0.4012872278690338, Final Batch Loss: 0.20204533636569977\n",
      "Epoch 3718, Loss: 0.34322334825992584, Final Batch Loss: 0.14282448589801788\n",
      "Epoch 3719, Loss: 0.34139153361320496, Final Batch Loss: 0.16368503868579865\n",
      "Epoch 3720, Loss: 0.36173367500305176, Final Batch Loss: 0.1972143054008484\n",
      "Epoch 3721, Loss: 0.2916923835873604, Final Batch Loss: 0.12428490072488785\n",
      "Epoch 3722, Loss: 0.281632199883461, Final Batch Loss: 0.11208929121494293\n",
      "Epoch 3723, Loss: 0.36648009717464447, Final Batch Loss: 0.15155231952667236\n",
      "Epoch 3724, Loss: 0.41767704486846924, Final Batch Loss: 0.2604529559612274\n",
      "Epoch 3725, Loss: 0.34121347963809967, Final Batch Loss: 0.17652222514152527\n",
      "Epoch 3726, Loss: 0.3474646210670471, Final Batch Loss: 0.2189444601535797\n",
      "Epoch 3727, Loss: 0.3916052281856537, Final Batch Loss: 0.21285280585289001\n",
      "Epoch 3728, Loss: 0.349841371178627, Final Batch Loss: 0.1311902552843094\n",
      "Epoch 3729, Loss: 0.3088997006416321, Final Batch Loss: 0.15804637968540192\n",
      "Epoch 3730, Loss: 0.31790539622306824, Final Batch Loss: 0.14637479186058044\n",
      "Epoch 3731, Loss: 0.39404796063899994, Final Batch Loss: 0.20497208833694458\n",
      "Epoch 3732, Loss: 0.3330118805170059, Final Batch Loss: 0.1717788279056549\n",
      "Epoch 3733, Loss: 0.35226331651210785, Final Batch Loss: 0.2045680731534958\n",
      "Epoch 3734, Loss: 0.4058524817228317, Final Batch Loss: 0.21819885075092316\n",
      "Epoch 3735, Loss: 0.3734491914510727, Final Batch Loss: 0.1945504993200302\n",
      "Epoch 3736, Loss: 0.33331790566444397, Final Batch Loss: 0.15147434175014496\n",
      "Epoch 3737, Loss: 0.46434830129146576, Final Batch Loss: 0.2556304931640625\n",
      "Epoch 3738, Loss: 0.38583505153656006, Final Batch Loss: 0.17435260117053986\n",
      "Epoch 3739, Loss: 0.36426007747650146, Final Batch Loss: 0.20099222660064697\n",
      "Epoch 3740, Loss: 0.3575299382209778, Final Batch Loss: 0.164461150765419\n",
      "Epoch 3741, Loss: 0.40725143253803253, Final Batch Loss: 0.22898408770561218\n",
      "Epoch 3742, Loss: 0.30887041985988617, Final Batch Loss: 0.17899586260318756\n",
      "Epoch 3743, Loss: 0.3556177318096161, Final Batch Loss: 0.14969679713249207\n",
      "Epoch 3744, Loss: 0.40681876242160797, Final Batch Loss: 0.22210802137851715\n",
      "Epoch 3745, Loss: 0.3782939314842224, Final Batch Loss: 0.1623583734035492\n",
      "Epoch 3746, Loss: 0.31546616554260254, Final Batch Loss: 0.11944137513637543\n",
      "Epoch 3747, Loss: 0.3653590530157089, Final Batch Loss: 0.19781571626663208\n",
      "Epoch 3748, Loss: 0.3542994409799576, Final Batch Loss: 0.2010231465101242\n",
      "Epoch 3749, Loss: 0.32047393918037415, Final Batch Loss: 0.16034822165966034\n",
      "Epoch 3750, Loss: 0.36159126460552216, Final Batch Loss: 0.1691027730703354\n",
      "Epoch 3751, Loss: 0.38860076665878296, Final Batch Loss: 0.20158842206001282\n",
      "Epoch 3752, Loss: 0.37749049067497253, Final Batch Loss: 0.17814378440380096\n",
      "Epoch 3753, Loss: 0.39718927443027496, Final Batch Loss: 0.21935513615608215\n",
      "Epoch 3754, Loss: 0.3308377116918564, Final Batch Loss: 0.1581725925207138\n",
      "Epoch 3755, Loss: 0.34259508550167084, Final Batch Loss: 0.16049116849899292\n",
      "Epoch 3756, Loss: 0.32709094882011414, Final Batch Loss: 0.18235497176647186\n",
      "Epoch 3757, Loss: 0.3477816730737686, Final Batch Loss: 0.16253003478050232\n",
      "Epoch 3758, Loss: 0.34375834465026855, Final Batch Loss: 0.16669608652591705\n",
      "Epoch 3759, Loss: 0.37380462884902954, Final Batch Loss: 0.19183295965194702\n",
      "Epoch 3760, Loss: 0.3432282358407974, Final Batch Loss: 0.17357486486434937\n",
      "Epoch 3761, Loss: 0.3737436383962631, Final Batch Loss: 0.2111101895570755\n",
      "Epoch 3762, Loss: 0.3584914803504944, Final Batch Loss: 0.1810372769832611\n",
      "Epoch 3763, Loss: 0.36893466114997864, Final Batch Loss: 0.1768265664577484\n",
      "Epoch 3764, Loss: 0.4312034696340561, Final Batch Loss: 0.1853472888469696\n",
      "Epoch 3765, Loss: 0.3313198536634445, Final Batch Loss: 0.1848425418138504\n",
      "Epoch 3766, Loss: 0.4304414242506027, Final Batch Loss: 0.21536797285079956\n",
      "Epoch 3767, Loss: 0.39356543123722076, Final Batch Loss: 0.19437609612941742\n",
      "Epoch 3768, Loss: 0.362510547041893, Final Batch Loss: 0.19326134026050568\n",
      "Epoch 3769, Loss: 0.3482936769723892, Final Batch Loss: 0.16027331352233887\n",
      "Epoch 3770, Loss: 0.3735557645559311, Final Batch Loss: 0.2185841202735901\n",
      "Epoch 3771, Loss: 0.3332771509885788, Final Batch Loss: 0.1567179262638092\n",
      "Epoch 3772, Loss: 0.354855477809906, Final Batch Loss: 0.18902884423732758\n",
      "Epoch 3773, Loss: 0.36413055658340454, Final Batch Loss: 0.21460115909576416\n",
      "Epoch 3774, Loss: 0.3525945544242859, Final Batch Loss: 0.20576347410678864\n",
      "Epoch 3775, Loss: 0.4326944500207901, Final Batch Loss: 0.2785877287387848\n",
      "Epoch 3776, Loss: 0.37024733424186707, Final Batch Loss: 0.19523422420024872\n",
      "Epoch 3777, Loss: 0.36802472174167633, Final Batch Loss: 0.18828359246253967\n",
      "Epoch 3778, Loss: 0.417818158864975, Final Batch Loss: 0.23214279115200043\n",
      "Epoch 3779, Loss: 0.3831943869590759, Final Batch Loss: 0.18661248683929443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3780, Loss: 0.4272971600294113, Final Batch Loss: 0.20952102541923523\n",
      "Epoch 3781, Loss: 0.3075626343488693, Final Batch Loss: 0.14058881998062134\n",
      "Epoch 3782, Loss: 0.4189802557229996, Final Batch Loss: 0.27106645703315735\n",
      "Epoch 3783, Loss: 0.3804869055747986, Final Batch Loss: 0.19102585315704346\n",
      "Epoch 3784, Loss: 0.36788786947727203, Final Batch Loss: 0.1973772943019867\n",
      "Epoch 3785, Loss: 0.3581400513648987, Final Batch Loss: 0.14101511240005493\n",
      "Epoch 3786, Loss: 0.37291909754276276, Final Batch Loss: 0.16810736060142517\n",
      "Epoch 3787, Loss: 0.3710966408252716, Final Batch Loss: 0.19749818742275238\n",
      "Epoch 3788, Loss: 0.3753695487976074, Final Batch Loss: 0.17843623459339142\n",
      "Epoch 3789, Loss: 0.45739905536174774, Final Batch Loss: 0.27445435523986816\n",
      "Epoch 3790, Loss: 0.41094622015953064, Final Batch Loss: 0.2244853377342224\n",
      "Epoch 3791, Loss: 0.3460724502801895, Final Batch Loss: 0.18644604086875916\n",
      "Epoch 3792, Loss: 0.474569633603096, Final Batch Loss: 0.19177834689617157\n",
      "Epoch 3793, Loss: 0.33542779088020325, Final Batch Loss: 0.1526811569929123\n",
      "Epoch 3794, Loss: 0.33969832956790924, Final Batch Loss: 0.17794457077980042\n",
      "Epoch 3795, Loss: 0.3292337507009506, Final Batch Loss: 0.18114763498306274\n",
      "Epoch 3796, Loss: 0.3814248889684677, Final Batch Loss: 0.20503950119018555\n",
      "Epoch 3797, Loss: 0.393547460436821, Final Batch Loss: 0.23946836590766907\n",
      "Epoch 3798, Loss: 0.35101841390132904, Final Batch Loss: 0.15809443593025208\n",
      "Epoch 3799, Loss: 0.3751843273639679, Final Batch Loss: 0.1821778267621994\n",
      "Epoch 3800, Loss: 0.34007516503334045, Final Batch Loss: 0.14559362828731537\n",
      "Epoch 3801, Loss: 0.36578573286533356, Final Batch Loss: 0.17223462462425232\n",
      "Epoch 3802, Loss: 0.3536209315061569, Final Batch Loss: 0.1636548489332199\n",
      "Epoch 3803, Loss: 0.35643258690834045, Final Batch Loss: 0.13008125126361847\n",
      "Epoch 3804, Loss: 0.4581441283226013, Final Batch Loss: 0.27638036012649536\n",
      "Epoch 3805, Loss: 0.33251558244228363, Final Batch Loss: 0.17914216220378876\n",
      "Epoch 3806, Loss: 0.33588846027851105, Final Batch Loss: 0.1677337884902954\n",
      "Epoch 3807, Loss: 0.3193468302488327, Final Batch Loss: 0.13609983026981354\n",
      "Epoch 3808, Loss: 0.36525920033454895, Final Batch Loss: 0.18234434723854065\n",
      "Epoch 3809, Loss: 0.40064048767089844, Final Batch Loss: 0.19496729969978333\n",
      "Epoch 3810, Loss: 0.4244938790798187, Final Batch Loss: 0.2338978797197342\n",
      "Epoch 3811, Loss: 0.39569011330604553, Final Batch Loss: 0.19892792403697968\n",
      "Epoch 3812, Loss: 0.4383942037820816, Final Batch Loss: 0.26477447152137756\n",
      "Epoch 3813, Loss: 0.3865491449832916, Final Batch Loss: 0.20763260126113892\n",
      "Epoch 3814, Loss: 0.3716302961111069, Final Batch Loss: 0.14765052497386932\n",
      "Epoch 3815, Loss: 0.3522922098636627, Final Batch Loss: 0.17799317836761475\n",
      "Epoch 3816, Loss: 0.3854185938835144, Final Batch Loss: 0.21073368191719055\n",
      "Epoch 3817, Loss: 0.3350374698638916, Final Batch Loss: 0.15858736634254456\n",
      "Epoch 3818, Loss: 0.3501921594142914, Final Batch Loss: 0.16220921277999878\n",
      "Epoch 3819, Loss: 0.36931048333644867, Final Batch Loss: 0.1558922976255417\n",
      "Epoch 3820, Loss: 0.37460121512413025, Final Batch Loss: 0.16396598517894745\n",
      "Epoch 3821, Loss: 0.33554069697856903, Final Batch Loss: 0.16889218986034393\n",
      "Epoch 3822, Loss: 0.3078818917274475, Final Batch Loss: 0.14903289079666138\n",
      "Epoch 3823, Loss: 0.41836054623126984, Final Batch Loss: 0.17642876505851746\n",
      "Epoch 3824, Loss: 0.37260380387306213, Final Batch Loss: 0.19267341494560242\n",
      "Epoch 3825, Loss: 0.4040861129760742, Final Batch Loss: 0.2141181230545044\n",
      "Epoch 3826, Loss: 0.379424124956131, Final Batch Loss: 0.18680933117866516\n",
      "Epoch 3827, Loss: 0.37660834193229675, Final Batch Loss: 0.1882205605506897\n",
      "Epoch 3828, Loss: 0.36286957561969757, Final Batch Loss: 0.17208954691886902\n",
      "Epoch 3829, Loss: 0.32292822003364563, Final Batch Loss: 0.13291248679161072\n",
      "Epoch 3830, Loss: 0.33769650757312775, Final Batch Loss: 0.19502884149551392\n",
      "Epoch 3831, Loss: 0.3633173555135727, Final Batch Loss: 0.16876715421676636\n",
      "Epoch 3832, Loss: 0.34672024846076965, Final Batch Loss: 0.1834585815668106\n",
      "Epoch 3833, Loss: 0.39129242300987244, Final Batch Loss: 0.230490580201149\n",
      "Epoch 3834, Loss: 0.3336724042892456, Final Batch Loss: 0.13895653188228607\n",
      "Epoch 3835, Loss: 0.36977340281009674, Final Batch Loss: 0.21010883152484894\n",
      "Epoch 3836, Loss: 0.3869241774082184, Final Batch Loss: 0.15726658701896667\n",
      "Epoch 3837, Loss: 0.34287841618061066, Final Batch Loss: 0.20740458369255066\n",
      "Epoch 3838, Loss: 0.3005777895450592, Final Batch Loss: 0.13369816541671753\n",
      "Epoch 3839, Loss: 0.3606138080358505, Final Batch Loss: 0.17857283353805542\n",
      "Epoch 3840, Loss: 0.35979513823986053, Final Batch Loss: 0.19413326680660248\n",
      "Epoch 3841, Loss: 0.38277871906757355, Final Batch Loss: 0.18647882342338562\n",
      "Epoch 3842, Loss: 0.36663439869880676, Final Batch Loss: 0.2133236974477768\n",
      "Epoch 3843, Loss: 0.33923274278640747, Final Batch Loss: 0.18917705118656158\n",
      "Epoch 3844, Loss: 0.3738687038421631, Final Batch Loss: 0.21602776646614075\n",
      "Epoch 3845, Loss: 0.3380379378795624, Final Batch Loss: 0.15058495104312897\n",
      "Epoch 3846, Loss: 0.30438269674777985, Final Batch Loss: 0.1315016746520996\n",
      "Epoch 3847, Loss: 0.343819797039032, Final Batch Loss: 0.1568954437971115\n",
      "Epoch 3848, Loss: 0.31272293627262115, Final Batch Loss: 0.15843847393989563\n",
      "Epoch 3849, Loss: 0.36217808723449707, Final Batch Loss: 0.19198879599571228\n",
      "Epoch 3850, Loss: 0.4719283878803253, Final Batch Loss: 0.293764591217041\n",
      "Epoch 3851, Loss: 0.3216394931077957, Final Batch Loss: 0.1596297025680542\n",
      "Epoch 3852, Loss: 0.364751860499382, Final Batch Loss: 0.18085595965385437\n",
      "Epoch 3853, Loss: 0.41777607798576355, Final Batch Loss: 0.24254605174064636\n",
      "Epoch 3854, Loss: 0.3800433874130249, Final Batch Loss: 0.16097262501716614\n",
      "Epoch 3855, Loss: 0.35942213237285614, Final Batch Loss: 0.1934063881635666\n",
      "Epoch 3856, Loss: 0.3143889605998993, Final Batch Loss: 0.13780881464481354\n",
      "Epoch 3857, Loss: 0.3678431510925293, Final Batch Loss: 0.18358024954795837\n",
      "Epoch 3858, Loss: 0.3259543627500534, Final Batch Loss: 0.16288219392299652\n",
      "Epoch 3859, Loss: 0.33763767778873444, Final Batch Loss: 0.16937312483787537\n",
      "Epoch 3860, Loss: 0.32029297947883606, Final Batch Loss: 0.14170551300048828\n",
      "Epoch 3861, Loss: 0.33702510595321655, Final Batch Loss: 0.1611194610595703\n",
      "Epoch 3862, Loss: 0.30717091262340546, Final Batch Loss: 0.16931527853012085\n",
      "Epoch 3863, Loss: 0.4059130847454071, Final Batch Loss: 0.20973053574562073\n",
      "Epoch 3864, Loss: 0.35764437913894653, Final Batch Loss: 0.20986872911453247\n",
      "Epoch 3865, Loss: 0.49039846658706665, Final Batch Loss: 0.23528119921684265\n",
      "Epoch 3866, Loss: 0.32912367582321167, Final Batch Loss: 0.16839727759361267\n",
      "Epoch 3867, Loss: 0.3314947783946991, Final Batch Loss: 0.1338203102350235\n",
      "Epoch 3868, Loss: 0.3567153066396713, Final Batch Loss: 0.17959275841712952\n",
      "Epoch 3869, Loss: 0.366034597158432, Final Batch Loss: 0.1944541484117508\n",
      "Epoch 3870, Loss: 0.34299756586551666, Final Batch Loss: 0.15881359577178955\n",
      "Epoch 3871, Loss: 0.33219125866889954, Final Batch Loss: 0.14006172120571136\n",
      "Epoch 3872, Loss: 0.47589047253131866, Final Batch Loss: 0.25169309973716736\n",
      "Epoch 3873, Loss: 0.33124032616615295, Final Batch Loss: 0.15082237124443054\n",
      "Epoch 3874, Loss: 0.33074501156806946, Final Batch Loss: 0.1814528852701187\n",
      "Epoch 3875, Loss: 0.39520926773548126, Final Batch Loss: 0.17419296503067017\n",
      "Epoch 3876, Loss: 0.323872447013855, Final Batch Loss: 0.1676785796880722\n",
      "Epoch 3877, Loss: 0.3801923245191574, Final Batch Loss: 0.22971275448799133\n",
      "Epoch 3878, Loss: 0.34995630383491516, Final Batch Loss: 0.14599022269248962\n",
      "Epoch 3879, Loss: 0.39484165608882904, Final Batch Loss: 0.18768416345119476\n",
      "Epoch 3880, Loss: 0.3829433470964432, Final Batch Loss: 0.24799466133117676\n",
      "Epoch 3881, Loss: 0.3897709846496582, Final Batch Loss: 0.18117289245128632\n",
      "Epoch 3882, Loss: 0.3447406589984894, Final Batch Loss: 0.19248303771018982\n",
      "Epoch 3883, Loss: 0.33751194179058075, Final Batch Loss: 0.1478765606880188\n",
      "Epoch 3884, Loss: 0.3736887127161026, Final Batch Loss: 0.17933690547943115\n",
      "Epoch 3885, Loss: 0.36398017406463623, Final Batch Loss: 0.17168104648590088\n",
      "Epoch 3886, Loss: 0.3423624485731125, Final Batch Loss: 0.17198997735977173\n",
      "Epoch 3887, Loss: 0.3242388218641281, Final Batch Loss: 0.16596710681915283\n",
      "Epoch 3888, Loss: 0.34941716492176056, Final Batch Loss: 0.17182722687721252\n",
      "Epoch 3889, Loss: 0.33141879737377167, Final Batch Loss: 0.19241109490394592\n",
      "Epoch 3890, Loss: 0.431407630443573, Final Batch Loss: 0.2503086030483246\n",
      "Epoch 3891, Loss: 0.3885413557291031, Final Batch Loss: 0.17796827852725983\n",
      "Epoch 3892, Loss: 0.3470614105463028, Final Batch Loss: 0.1517632007598877\n",
      "Epoch 3893, Loss: 0.33111514151096344, Final Batch Loss: 0.18765100836753845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3894, Loss: 0.34914420545101166, Final Batch Loss: 0.1651422530412674\n",
      "Epoch 3895, Loss: 0.3335704058408737, Final Batch Loss: 0.15652313828468323\n",
      "Epoch 3896, Loss: 0.3175508975982666, Final Batch Loss: 0.1642780750989914\n",
      "Epoch 3897, Loss: 0.4003441333770752, Final Batch Loss: 0.19065171480178833\n",
      "Epoch 3898, Loss: 0.3495389223098755, Final Batch Loss: 0.15970192849636078\n",
      "Epoch 3899, Loss: 0.32121793925762177, Final Batch Loss: 0.13498573005199432\n",
      "Epoch 3900, Loss: 0.34252171218395233, Final Batch Loss: 0.13771094381809235\n",
      "Epoch 3901, Loss: 0.27753402292728424, Final Batch Loss: 0.10933013260364532\n",
      "Epoch 3902, Loss: 0.3784521520137787, Final Batch Loss: 0.2041836678981781\n",
      "Epoch 3903, Loss: 0.3026805967092514, Final Batch Loss: 0.1398794800043106\n",
      "Epoch 3904, Loss: 0.3468773365020752, Final Batch Loss: 0.1673293262720108\n",
      "Epoch 3905, Loss: 0.41475580632686615, Final Batch Loss: 0.24174124002456665\n",
      "Epoch 3906, Loss: 0.3283616155385971, Final Batch Loss: 0.1506107747554779\n",
      "Epoch 3907, Loss: 0.3074897676706314, Final Batch Loss: 0.14264577627182007\n",
      "Epoch 3908, Loss: 0.34478050470352173, Final Batch Loss: 0.15009649097919464\n",
      "Epoch 3909, Loss: 0.3245288133621216, Final Batch Loss: 0.15058192610740662\n",
      "Epoch 3910, Loss: 0.3865914046764374, Final Batch Loss: 0.22623039782047272\n",
      "Epoch 3911, Loss: 0.29750974476337433, Final Batch Loss: 0.15969854593276978\n",
      "Epoch 3912, Loss: 0.33151501417160034, Final Batch Loss: 0.16472381353378296\n",
      "Epoch 3913, Loss: 0.3620110899209976, Final Batch Loss: 0.21086177229881287\n",
      "Epoch 3914, Loss: 0.3233471065759659, Final Batch Loss: 0.16312307119369507\n",
      "Epoch 3915, Loss: 0.38222503662109375, Final Batch Loss: 0.19616371393203735\n",
      "Epoch 3916, Loss: 0.371377095580101, Final Batch Loss: 0.24192547798156738\n",
      "Epoch 3917, Loss: 0.3024314045906067, Final Batch Loss: 0.14229153096675873\n",
      "Epoch 3918, Loss: 0.3779277950525284, Final Batch Loss: 0.21349672973155975\n",
      "Epoch 3919, Loss: 0.2985145151615143, Final Batch Loss: 0.1627928912639618\n",
      "Epoch 3920, Loss: 0.40641117095947266, Final Batch Loss: 0.1967804729938507\n",
      "Epoch 3921, Loss: 0.3578409254550934, Final Batch Loss: 0.13663940131664276\n",
      "Epoch 3922, Loss: 0.32271234691143036, Final Batch Loss: 0.14093059301376343\n",
      "Epoch 3923, Loss: 0.3395259231328964, Final Batch Loss: 0.16331081092357635\n",
      "Epoch 3924, Loss: 0.32266557216644287, Final Batch Loss: 0.1571178138256073\n",
      "Epoch 3925, Loss: 0.3476257473230362, Final Batch Loss: 0.159249410033226\n",
      "Epoch 3926, Loss: 0.3667183071374893, Final Batch Loss: 0.18464243412017822\n",
      "Epoch 3927, Loss: 0.35357119143009186, Final Batch Loss: 0.20370136201381683\n",
      "Epoch 3928, Loss: 0.3484022468328476, Final Batch Loss: 0.18572629988193512\n",
      "Epoch 3929, Loss: 0.32742686569690704, Final Batch Loss: 0.1314101666212082\n",
      "Epoch 3930, Loss: 0.29968830943107605, Final Batch Loss: 0.14019757509231567\n",
      "Epoch 3931, Loss: 0.41441114246845245, Final Batch Loss: 0.21292869746685028\n",
      "Epoch 3932, Loss: 0.34863412380218506, Final Batch Loss: 0.16492387652397156\n",
      "Epoch 3933, Loss: 0.30684076249599457, Final Batch Loss: 0.13594138622283936\n",
      "Epoch 3934, Loss: 0.3769070655107498, Final Batch Loss: 0.21606531739234924\n",
      "Epoch 3935, Loss: 0.39184267818927765, Final Batch Loss: 0.2199014276266098\n",
      "Epoch 3936, Loss: 0.3538367450237274, Final Batch Loss: 0.16161441802978516\n",
      "Epoch 3937, Loss: 0.3762351870536804, Final Batch Loss: 0.16794100403785706\n",
      "Epoch 3938, Loss: 0.29622043669223785, Final Batch Loss: 0.15320992469787598\n",
      "Epoch 3939, Loss: 0.35468457639217377, Final Batch Loss: 0.1919817477464676\n",
      "Epoch 3940, Loss: 0.3628803789615631, Final Batch Loss: 0.14870063960552216\n",
      "Epoch 3941, Loss: 0.3206774443387985, Final Batch Loss: 0.16259443759918213\n",
      "Epoch 3942, Loss: 0.3582834005355835, Final Batch Loss: 0.19922678172588348\n",
      "Epoch 3943, Loss: 0.35296937823295593, Final Batch Loss: 0.16868256032466888\n",
      "Epoch 3944, Loss: 0.3925991505384445, Final Batch Loss: 0.2098100483417511\n",
      "Epoch 3945, Loss: 0.324304424226284, Final Batch Loss: 0.11632142215967178\n",
      "Epoch 3946, Loss: 0.43777647614479065, Final Batch Loss: 0.20507705211639404\n",
      "Epoch 3947, Loss: 0.37487758696079254, Final Batch Loss: 0.20448073744773865\n",
      "Epoch 3948, Loss: 0.41855716705322266, Final Batch Loss: 0.24918852746486664\n",
      "Epoch 3949, Loss: 0.3054613769054413, Final Batch Loss: 0.1399630457162857\n",
      "Epoch 3950, Loss: 0.3612791895866394, Final Batch Loss: 0.20154951512813568\n",
      "Epoch 3951, Loss: 0.41164542734622955, Final Batch Loss: 0.21520893275737762\n",
      "Epoch 3952, Loss: 0.3201266676187515, Final Batch Loss: 0.15727294981479645\n",
      "Epoch 3953, Loss: 0.3889867812395096, Final Batch Loss: 0.25386425852775574\n",
      "Epoch 3954, Loss: 0.48198091983795166, Final Batch Loss: 0.2555866241455078\n",
      "Epoch 3955, Loss: 0.3600293695926666, Final Batch Loss: 0.13093368709087372\n",
      "Epoch 3956, Loss: 0.2970838397741318, Final Batch Loss: 0.1425153613090515\n",
      "Epoch 3957, Loss: 0.32228057086467743, Final Batch Loss: 0.15536949038505554\n",
      "Epoch 3958, Loss: 0.3976227790117264, Final Batch Loss: 0.23997846245765686\n",
      "Epoch 3959, Loss: 0.36150161921977997, Final Batch Loss: 0.20328432321548462\n",
      "Epoch 3960, Loss: 0.29258526861667633, Final Batch Loss: 0.15980948507785797\n",
      "Epoch 3961, Loss: 0.3690863698720932, Final Batch Loss: 0.20423126220703125\n",
      "Epoch 3962, Loss: 0.3427601456642151, Final Batch Loss: 0.15681564807891846\n",
      "Epoch 3963, Loss: 0.3878304064273834, Final Batch Loss: 0.16723880171775818\n",
      "Epoch 3964, Loss: 0.34277665615081787, Final Batch Loss: 0.22123464941978455\n",
      "Epoch 3965, Loss: 0.33847881853580475, Final Batch Loss: 0.16641907393932343\n",
      "Epoch 3966, Loss: 0.3325091600418091, Final Batch Loss: 0.16666613519191742\n",
      "Epoch 3967, Loss: 0.3349580466747284, Final Batch Loss: 0.16382865607738495\n",
      "Epoch 3968, Loss: 0.33132386207580566, Final Batch Loss: 0.13675332069396973\n",
      "Epoch 3969, Loss: 0.3106968104839325, Final Batch Loss: 0.13953126966953278\n",
      "Epoch 3970, Loss: 0.3263593465089798, Final Batch Loss: 0.15405072271823883\n",
      "Epoch 3971, Loss: 0.30940815806388855, Final Batch Loss: 0.1299700140953064\n",
      "Epoch 3972, Loss: 0.3581734448671341, Final Batch Loss: 0.16685496270656586\n",
      "Epoch 3973, Loss: 0.3112398833036423, Final Batch Loss: 0.14655539393424988\n",
      "Epoch 3974, Loss: 0.2973260134458542, Final Batch Loss: 0.141179159283638\n",
      "Epoch 3975, Loss: 0.40262019634246826, Final Batch Loss: 0.2277693748474121\n",
      "Epoch 3976, Loss: 0.35779161751270294, Final Batch Loss: 0.19032496213912964\n",
      "Epoch 3977, Loss: 0.3137328326702118, Final Batch Loss: 0.15208308398723602\n",
      "Epoch 3978, Loss: 0.37134119868278503, Final Batch Loss: 0.23345014452934265\n",
      "Epoch 3979, Loss: 0.3301156610250473, Final Batch Loss: 0.14303217828273773\n",
      "Epoch 3980, Loss: 0.3502550721168518, Final Batch Loss: 0.20677725970745087\n",
      "Epoch 3981, Loss: 0.42502816021442413, Final Batch Loss: 0.22047379612922668\n",
      "Epoch 3982, Loss: 0.3681299537420273, Final Batch Loss: 0.1746537834405899\n",
      "Epoch 3983, Loss: 0.38928282260894775, Final Batch Loss: 0.20592761039733887\n",
      "Epoch 3984, Loss: 0.34697695076465607, Final Batch Loss: 0.18519020080566406\n",
      "Epoch 3985, Loss: 0.4224628806114197, Final Batch Loss: 0.2158353477716446\n",
      "Epoch 3986, Loss: 0.2873276472091675, Final Batch Loss: 0.1397724151611328\n",
      "Epoch 3987, Loss: 0.3880626708269119, Final Batch Loss: 0.20899583399295807\n",
      "Epoch 3988, Loss: 0.3627365678548813, Final Batch Loss: 0.174636572599411\n",
      "Epoch 3989, Loss: 0.3933328241109848, Final Batch Loss: 0.20362992584705353\n",
      "Epoch 3990, Loss: 0.3331514596939087, Final Batch Loss: 0.18148298561573029\n",
      "Epoch 3991, Loss: 0.3661632090806961, Final Batch Loss: 0.1695832461118698\n",
      "Epoch 3992, Loss: 0.34859442710876465, Final Batch Loss: 0.19450250267982483\n",
      "Epoch 3993, Loss: 0.35951246321201324, Final Batch Loss: 0.19801458716392517\n",
      "Epoch 3994, Loss: 0.36130213737487793, Final Batch Loss: 0.19081850349903107\n",
      "Epoch 3995, Loss: 0.33655688166618347, Final Batch Loss: 0.17208701372146606\n",
      "Epoch 3996, Loss: 0.3334735780954361, Final Batch Loss: 0.17786170542240143\n",
      "Epoch 3997, Loss: 0.3464491218328476, Final Batch Loss: 0.20875243842601776\n",
      "Epoch 3998, Loss: 0.31044092774391174, Final Batch Loss: 0.17231033742427826\n",
      "Epoch 3999, Loss: 0.360602542757988, Final Batch Loss: 0.17176277935504913\n",
      "Epoch 4000, Loss: 0.4347169101238251, Final Batch Loss: 0.19607770442962646\n",
      "Epoch 4001, Loss: 0.3085630238056183, Final Batch Loss: 0.13772360980510712\n",
      "Epoch 4002, Loss: 0.2789289802312851, Final Batch Loss: 0.1460455358028412\n",
      "Epoch 4003, Loss: 0.3503791093826294, Final Batch Loss: 0.1598767638206482\n",
      "Epoch 4004, Loss: 0.28446800261735916, Final Batch Loss: 0.09506296366453171\n",
      "Epoch 4005, Loss: 0.33871184289455414, Final Batch Loss: 0.14893394708633423\n",
      "Epoch 4006, Loss: 0.3527133911848068, Final Batch Loss: 0.19042499363422394\n",
      "Epoch 4007, Loss: 0.31984007358551025, Final Batch Loss: 0.1341906040906906\n",
      "Epoch 4008, Loss: 0.3835809528827667, Final Batch Loss: 0.17521508038043976\n",
      "Epoch 4009, Loss: 0.36950817704200745, Final Batch Loss: 0.1969234049320221\n",
      "Epoch 4010, Loss: 0.342634454369545, Final Batch Loss: 0.15895988047122955\n",
      "Epoch 4011, Loss: 0.38436825573444366, Final Batch Loss: 0.19759652018547058\n",
      "Epoch 4012, Loss: 0.38797517120838165, Final Batch Loss: 0.19124974310398102\n",
      "Epoch 4013, Loss: 0.35640159249305725, Final Batch Loss: 0.1609589010477066\n",
      "Epoch 4014, Loss: 0.3323041498661041, Final Batch Loss: 0.17640873789787292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4015, Loss: 0.33036066591739655, Final Batch Loss: 0.15951292216777802\n",
      "Epoch 4016, Loss: 0.36372236907482147, Final Batch Loss: 0.20165829360485077\n",
      "Epoch 4017, Loss: 0.29858266562223434, Final Batch Loss: 0.10130039602518082\n",
      "Epoch 4018, Loss: 0.369639128446579, Final Batch Loss: 0.19957926869392395\n",
      "Epoch 4019, Loss: 0.29039278626441956, Final Batch Loss: 0.16130533814430237\n",
      "Epoch 4020, Loss: 0.3864048272371292, Final Batch Loss: 0.23250463604927063\n",
      "Epoch 4021, Loss: 0.34913013875484467, Final Batch Loss: 0.138606458902359\n",
      "Epoch 4022, Loss: 0.33354587852954865, Final Batch Loss: 0.18771933019161224\n",
      "Epoch 4023, Loss: 0.3230518251657486, Final Batch Loss: 0.16734175384044647\n",
      "Epoch 4024, Loss: 0.27364741265773773, Final Batch Loss: 0.12769222259521484\n",
      "Epoch 4025, Loss: 0.3622729927301407, Final Batch Loss: 0.15305060148239136\n",
      "Epoch 4026, Loss: 0.33331185579299927, Final Batch Loss: 0.1439247876405716\n",
      "Epoch 4027, Loss: 0.39064665138721466, Final Batch Loss: 0.18880192935466766\n",
      "Epoch 4028, Loss: 0.38616952300071716, Final Batch Loss: 0.20821362733840942\n",
      "Epoch 4029, Loss: 0.3932031989097595, Final Batch Loss: 0.2178688645362854\n",
      "Epoch 4030, Loss: 0.298294834792614, Final Batch Loss: 0.1108972355723381\n",
      "Epoch 4031, Loss: 0.3171699792146683, Final Batch Loss: 0.1973394900560379\n",
      "Epoch 4032, Loss: 0.3451143652200699, Final Batch Loss: 0.16807501018047333\n",
      "Epoch 4033, Loss: 0.42293259501457214, Final Batch Loss: 0.22533676028251648\n",
      "Epoch 4034, Loss: 0.33891384303569794, Final Batch Loss: 0.17300760746002197\n",
      "Epoch 4035, Loss: 0.326570063829422, Final Batch Loss: 0.13107171654701233\n",
      "Epoch 4036, Loss: 0.3144478350877762, Final Batch Loss: 0.16400687396526337\n",
      "Epoch 4037, Loss: 0.2753417044878006, Final Batch Loss: 0.13494724035263062\n",
      "Epoch 4038, Loss: 0.3348570466041565, Final Batch Loss: 0.17971043288707733\n",
      "Epoch 4039, Loss: 0.32464373111724854, Final Batch Loss: 0.13751645386219025\n",
      "Epoch 4040, Loss: 0.29301784932613373, Final Batch Loss: 0.12289424240589142\n",
      "Epoch 4041, Loss: 0.3280031532049179, Final Batch Loss: 0.15321828424930573\n",
      "Epoch 4042, Loss: 0.3367040455341339, Final Batch Loss: 0.1958877146244049\n",
      "Epoch 4043, Loss: 0.39883239567279816, Final Batch Loss: 0.2363770753145218\n",
      "Epoch 4044, Loss: 0.3868955820798874, Final Batch Loss: 0.196979820728302\n",
      "Epoch 4045, Loss: 0.33758705854415894, Final Batch Loss: 0.1911924183368683\n",
      "Epoch 4046, Loss: 0.2982970029115677, Final Batch Loss: 0.16009429097175598\n",
      "Epoch 4047, Loss: 0.35177209973335266, Final Batch Loss: 0.18400593101978302\n",
      "Epoch 4048, Loss: 0.3436831831932068, Final Batch Loss: 0.19905783236026764\n",
      "Epoch 4049, Loss: 0.3164721131324768, Final Batch Loss: 0.17618952691555023\n",
      "Epoch 4050, Loss: 0.30497002601623535, Final Batch Loss: 0.14639133214950562\n",
      "Epoch 4051, Loss: 0.3403705507516861, Final Batch Loss: 0.15966659784317017\n",
      "Epoch 4052, Loss: 0.3402385413646698, Final Batch Loss: 0.21316660940647125\n",
      "Epoch 4053, Loss: 0.3247192054986954, Final Batch Loss: 0.1736987680196762\n",
      "Epoch 4054, Loss: 0.38379666209220886, Final Batch Loss: 0.2309051752090454\n",
      "Epoch 4055, Loss: 0.3251289576292038, Final Batch Loss: 0.1960766762495041\n",
      "Epoch 4056, Loss: 0.38877132534980774, Final Batch Loss: 0.24104593694210052\n",
      "Epoch 4057, Loss: 0.28053320944309235, Final Batch Loss: 0.12651565670967102\n",
      "Epoch 4058, Loss: 0.3351820707321167, Final Batch Loss: 0.2038489133119583\n",
      "Epoch 4059, Loss: 0.35848087072372437, Final Batch Loss: 0.1819985806941986\n",
      "Epoch 4060, Loss: 0.3437471240758896, Final Batch Loss: 0.16736844182014465\n",
      "Epoch 4061, Loss: 0.32505764067173004, Final Batch Loss: 0.14292310178279877\n",
      "Epoch 4062, Loss: 0.3604191392660141, Final Batch Loss: 0.19504864513874054\n",
      "Epoch 4063, Loss: 0.3271675854921341, Final Batch Loss: 0.15100513398647308\n",
      "Epoch 4064, Loss: 0.3350326269865036, Final Batch Loss: 0.1915983408689499\n",
      "Epoch 4065, Loss: 0.3791005164384842, Final Batch Loss: 0.13842307031154633\n",
      "Epoch 4066, Loss: 0.36231300234794617, Final Batch Loss: 0.18057170510292053\n",
      "Epoch 4067, Loss: 0.4050515592098236, Final Batch Loss: 0.2603145241737366\n",
      "Epoch 4068, Loss: 0.3005713000893593, Final Batch Loss: 0.09866572171449661\n",
      "Epoch 4069, Loss: 0.32728518545627594, Final Batch Loss: 0.15617649257183075\n",
      "Epoch 4070, Loss: 0.3339381664991379, Final Batch Loss: 0.1500355750322342\n",
      "Epoch 4071, Loss: 0.3745313733816147, Final Batch Loss: 0.2311834841966629\n",
      "Epoch 4072, Loss: 0.3511883169412613, Final Batch Loss: 0.18615902960300446\n",
      "Epoch 4073, Loss: 0.3599846661090851, Final Batch Loss: 0.17861084640026093\n",
      "Epoch 4074, Loss: 0.39656390249729156, Final Batch Loss: 0.19657474756240845\n",
      "Epoch 4075, Loss: 0.31763914227485657, Final Batch Loss: 0.13572841882705688\n",
      "Epoch 4076, Loss: 0.3472276031970978, Final Batch Loss: 0.19849810004234314\n",
      "Epoch 4077, Loss: 0.26463858038187027, Final Batch Loss: 0.12422742694616318\n",
      "Epoch 4078, Loss: 0.3171052932739258, Final Batch Loss: 0.17964106798171997\n",
      "Epoch 4079, Loss: 0.25788500905036926, Final Batch Loss: 0.11342795193195343\n",
      "Epoch 4080, Loss: 0.3598809838294983, Final Batch Loss: 0.2285388559103012\n",
      "Epoch 4081, Loss: 0.2968217730522156, Final Batch Loss: 0.1312776654958725\n",
      "Epoch 4082, Loss: 0.4097585380077362, Final Batch Loss: 0.2371441125869751\n",
      "Epoch 4083, Loss: 0.37925606966018677, Final Batch Loss: 0.17845922708511353\n",
      "Epoch 4084, Loss: 0.291389524936676, Final Batch Loss: 0.13727901875972748\n",
      "Epoch 4085, Loss: 0.2898094356060028, Final Batch Loss: 0.1468382477760315\n",
      "Epoch 4086, Loss: 0.3202861398458481, Final Batch Loss: 0.16274277865886688\n",
      "Epoch 4087, Loss: 0.39609596133232117, Final Batch Loss: 0.22859402000904083\n",
      "Epoch 4088, Loss: 0.31846240162849426, Final Batch Loss: 0.14932304620742798\n",
      "Epoch 4089, Loss: 0.3188151866197586, Final Batch Loss: 0.1339903324842453\n",
      "Epoch 4090, Loss: 0.36280082166194916, Final Batch Loss: 0.16183234751224518\n",
      "Epoch 4091, Loss: 0.2832740396261215, Final Batch Loss: 0.10285788774490356\n",
      "Epoch 4092, Loss: 0.3451611399650574, Final Batch Loss: 0.175115168094635\n",
      "Epoch 4093, Loss: 0.29765866696834564, Final Batch Loss: 0.14645417034626007\n",
      "Epoch 4094, Loss: 0.36143043637275696, Final Batch Loss: 0.19179683923721313\n",
      "Epoch 4095, Loss: 0.3175416588783264, Final Batch Loss: 0.14195647835731506\n",
      "Epoch 4096, Loss: 0.3032324016094208, Final Batch Loss: 0.15204018354415894\n",
      "Epoch 4097, Loss: 0.35535724461078644, Final Batch Loss: 0.18198643624782562\n",
      "Epoch 4098, Loss: 0.3357648402452469, Final Batch Loss: 0.1929735392332077\n",
      "Epoch 4099, Loss: 0.2998775839805603, Final Batch Loss: 0.12566827237606049\n",
      "Epoch 4100, Loss: 0.35309386253356934, Final Batch Loss: 0.22335530817508698\n",
      "Epoch 4101, Loss: 0.2730143517255783, Final Batch Loss: 0.11154843866825104\n",
      "Epoch 4102, Loss: 0.3579556941986084, Final Batch Loss: 0.14912007749080658\n",
      "Epoch 4103, Loss: 0.3598162680864334, Final Batch Loss: 0.15967220067977905\n",
      "Epoch 4104, Loss: 0.3313158452510834, Final Batch Loss: 0.16455073654651642\n",
      "Epoch 4105, Loss: 0.3365805000066757, Final Batch Loss: 0.17079105973243713\n",
      "Epoch 4106, Loss: 0.3654455840587616, Final Batch Loss: 0.19038674235343933\n",
      "Epoch 4107, Loss: 0.35900285840034485, Final Batch Loss: 0.18144887685775757\n",
      "Epoch 4108, Loss: 0.34413497149944305, Final Batch Loss: 0.16154393553733826\n",
      "Epoch 4109, Loss: 0.2826978713274002, Final Batch Loss: 0.10281047224998474\n",
      "Epoch 4110, Loss: 0.3448735475540161, Final Batch Loss: 0.157850444316864\n",
      "Epoch 4111, Loss: 0.319138303399086, Final Batch Loss: 0.17242532968521118\n",
      "Epoch 4112, Loss: 0.29280398041009903, Final Batch Loss: 0.12086734920740128\n",
      "Epoch 4113, Loss: 0.29499879479408264, Final Batch Loss: 0.13682420551776886\n",
      "Epoch 4114, Loss: 0.2879897952079773, Final Batch Loss: 0.13507571816444397\n",
      "Epoch 4115, Loss: 0.3365270346403122, Final Batch Loss: 0.1733935922384262\n",
      "Epoch 4116, Loss: 0.3882117420434952, Final Batch Loss: 0.19636867940425873\n",
      "Epoch 4117, Loss: 0.29406778514385223, Final Batch Loss: 0.14861340820789337\n",
      "Epoch 4118, Loss: 0.3143947124481201, Final Batch Loss: 0.14708153903484344\n",
      "Epoch 4119, Loss: 0.31284385919570923, Final Batch Loss: 0.16039438545703888\n",
      "Epoch 4120, Loss: 0.3123701810836792, Final Batch Loss: 0.1682986170053482\n",
      "Epoch 4121, Loss: 0.3343283236026764, Final Batch Loss: 0.16200090944766998\n",
      "Epoch 4122, Loss: 0.3103029280900955, Final Batch Loss: 0.17462807893753052\n",
      "Epoch 4123, Loss: 0.3524826914072037, Final Batch Loss: 0.1859784573316574\n",
      "Epoch 4124, Loss: 0.32599493861198425, Final Batch Loss: 0.15193605422973633\n",
      "Epoch 4125, Loss: 0.30691275000572205, Final Batch Loss: 0.13259102404117584\n",
      "Epoch 4126, Loss: 0.30143459141254425, Final Batch Loss: 0.1730365753173828\n",
      "Epoch 4127, Loss: 0.3098367601633072, Final Batch Loss: 0.15019048750400543\n",
      "Epoch 4128, Loss: 0.24770893156528473, Final Batch Loss: 0.11402544379234314\n",
      "Epoch 4129, Loss: 0.3454824388027191, Final Batch Loss: 0.15751221776008606\n",
      "Epoch 4130, Loss: 0.33611664175987244, Final Batch Loss: 0.16449947655200958\n",
      "Epoch 4131, Loss: 0.3445930480957031, Final Batch Loss: 0.18806985020637512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4132, Loss: 0.3416213244199753, Final Batch Loss: 0.17689478397369385\n",
      "Epoch 4133, Loss: 0.3116706311702728, Final Batch Loss: 0.12459591031074524\n",
      "Epoch 4134, Loss: 0.3270748406648636, Final Batch Loss: 0.16844947636127472\n",
      "Epoch 4135, Loss: 0.2563403695821762, Final Batch Loss: 0.11863945424556732\n",
      "Epoch 4136, Loss: 0.2962174490094185, Final Batch Loss: 0.08837974816560745\n",
      "Epoch 4137, Loss: 0.33885030448436737, Final Batch Loss: 0.17845134437084198\n",
      "Epoch 4138, Loss: 0.3005746901035309, Final Batch Loss: 0.1433028280735016\n",
      "Epoch 4139, Loss: 0.32779285311698914, Final Batch Loss: 0.13134820759296417\n",
      "Epoch 4140, Loss: 0.3464154899120331, Final Batch Loss: 0.20474115014076233\n",
      "Epoch 4141, Loss: 0.3855605125427246, Final Batch Loss: 0.19973944127559662\n",
      "Epoch 4142, Loss: 0.3168274462223053, Final Batch Loss: 0.1444244384765625\n",
      "Epoch 4143, Loss: 0.3118688762187958, Final Batch Loss: 0.13850532472133636\n",
      "Epoch 4144, Loss: 0.39941973984241486, Final Batch Loss: 0.265970379114151\n",
      "Epoch 4145, Loss: 0.30259090662002563, Final Batch Loss: 0.1544187068939209\n",
      "Epoch 4146, Loss: 0.2972925901412964, Final Batch Loss: 0.1580093801021576\n",
      "Epoch 4147, Loss: 0.3246411681175232, Final Batch Loss: 0.13426586985588074\n",
      "Epoch 4148, Loss: 0.3527529537677765, Final Batch Loss: 0.20383252203464508\n",
      "Epoch 4149, Loss: 0.3819599151611328, Final Batch Loss: 0.24582284688949585\n",
      "Epoch 4150, Loss: 0.27569498121738434, Final Batch Loss: 0.10817638039588928\n",
      "Epoch 4151, Loss: 0.3532624691724777, Final Batch Loss: 0.19842185080051422\n",
      "Epoch 4152, Loss: 0.30680689215660095, Final Batch Loss: 0.14431798458099365\n",
      "Epoch 4153, Loss: 0.3410840183496475, Final Batch Loss: 0.15504571795463562\n",
      "Epoch 4154, Loss: 0.3141731470823288, Final Batch Loss: 0.18075494468212128\n",
      "Epoch 4155, Loss: 0.2884797006845474, Final Batch Loss: 0.1422288566827774\n",
      "Epoch 4156, Loss: 0.31999723613262177, Final Batch Loss: 0.18025678396224976\n",
      "Epoch 4157, Loss: 0.3492377698421478, Final Batch Loss: 0.17731711268424988\n",
      "Epoch 4158, Loss: 0.3601277619600296, Final Batch Loss: 0.198252335190773\n",
      "Epoch 4159, Loss: 0.29140612483024597, Final Batch Loss: 0.11975331604480743\n",
      "Epoch 4160, Loss: 0.3268855810165405, Final Batch Loss: 0.18016815185546875\n",
      "Epoch 4161, Loss: 0.3295740634202957, Final Batch Loss: 0.17181281745433807\n",
      "Epoch 4162, Loss: 0.36563047766685486, Final Batch Loss: 0.2201051563024521\n",
      "Epoch 4163, Loss: 0.3471948355436325, Final Batch Loss: 0.21700561046600342\n",
      "Epoch 4164, Loss: 0.315112441778183, Final Batch Loss: 0.17189478874206543\n",
      "Epoch 4165, Loss: 0.36554965376853943, Final Batch Loss: 0.18936209380626678\n",
      "Epoch 4166, Loss: 0.3057551234960556, Final Batch Loss: 0.17068344354629517\n",
      "Epoch 4167, Loss: 0.3131687492132187, Final Batch Loss: 0.13831999897956848\n",
      "Epoch 4168, Loss: 0.3318948596715927, Final Batch Loss: 0.1475200653076172\n",
      "Epoch 4169, Loss: 0.315422385931015, Final Batch Loss: 0.1475244164466858\n",
      "Epoch 4170, Loss: 0.37467625737190247, Final Batch Loss: 0.2004624903202057\n",
      "Epoch 4171, Loss: 0.30852989852428436, Final Batch Loss: 0.14001718163490295\n",
      "Epoch 4172, Loss: 0.3605058491230011, Final Batch Loss: 0.12307272851467133\n",
      "Epoch 4173, Loss: 0.3657928854227066, Final Batch Loss: 0.1801656037569046\n",
      "Epoch 4174, Loss: 0.46265023946762085, Final Batch Loss: 0.3130117952823639\n",
      "Epoch 4175, Loss: 0.35336533188819885, Final Batch Loss: 0.17917990684509277\n",
      "Epoch 4176, Loss: 0.2672128900885582, Final Batch Loss: 0.1683977246284485\n",
      "Epoch 4177, Loss: 0.31369076669216156, Final Batch Loss: 0.14293554425239563\n",
      "Epoch 4178, Loss: 0.29595325887203217, Final Batch Loss: 0.1642792671918869\n",
      "Epoch 4179, Loss: 0.31362184882164, Final Batch Loss: 0.12165623903274536\n",
      "Epoch 4180, Loss: 0.3407934159040451, Final Batch Loss: 0.21401138603687286\n",
      "Epoch 4181, Loss: 0.3160895109176636, Final Batch Loss: 0.16363638639450073\n",
      "Epoch 4182, Loss: 0.30001184344291687, Final Batch Loss: 0.12684045732021332\n",
      "Epoch 4183, Loss: 0.2741640508174896, Final Batch Loss: 0.11995989084243774\n",
      "Epoch 4184, Loss: 0.2844455689191818, Final Batch Loss: 0.14891120791435242\n",
      "Epoch 4185, Loss: 0.288742333650589, Final Batch Loss: 0.130900040268898\n",
      "Epoch 4186, Loss: 0.3532803952693939, Final Batch Loss: 0.1638989895582199\n",
      "Epoch 4187, Loss: 0.3928656429052353, Final Batch Loss: 0.2108893245458603\n",
      "Epoch 4188, Loss: 0.3096809536218643, Final Batch Loss: 0.15089406073093414\n",
      "Epoch 4189, Loss: 0.36433497071266174, Final Batch Loss: 0.13180319964885712\n",
      "Epoch 4190, Loss: 0.29577048122882843, Final Batch Loss: 0.1586233675479889\n",
      "Epoch 4191, Loss: 0.29487791657447815, Final Batch Loss: 0.17426517605781555\n",
      "Epoch 4192, Loss: 0.3126014173030853, Final Batch Loss: 0.12374795973300934\n",
      "Epoch 4193, Loss: 0.33690714836120605, Final Batch Loss: 0.18669869005680084\n",
      "Epoch 4194, Loss: 0.3614569455385208, Final Batch Loss: 0.17331421375274658\n",
      "Epoch 4195, Loss: 0.3282013386487961, Final Batch Loss: 0.1884278655052185\n",
      "Epoch 4196, Loss: 0.3731722831726074, Final Batch Loss: 0.2313806712627411\n",
      "Epoch 4197, Loss: 0.34998592734336853, Final Batch Loss: 0.1980711966753006\n",
      "Epoch 4198, Loss: 0.29414186626672745, Final Batch Loss: 0.18494150042533875\n",
      "Epoch 4199, Loss: 0.31627562642097473, Final Batch Loss: 0.15221910178661346\n",
      "Epoch 4200, Loss: 0.3653343915939331, Final Batch Loss: 0.20443959534168243\n",
      "Epoch 4201, Loss: 0.3089580535888672, Final Batch Loss: 0.16805769503116608\n",
      "Epoch 4202, Loss: 0.3989572077989578, Final Batch Loss: 0.18727093935012817\n",
      "Epoch 4203, Loss: 0.3776058256626129, Final Batch Loss: 0.24101994931697845\n",
      "Epoch 4204, Loss: 0.2995694875717163, Final Batch Loss: 0.1646597981452942\n",
      "Epoch 4205, Loss: 0.3142979145050049, Final Batch Loss: 0.16675090789794922\n",
      "Epoch 4206, Loss: 0.2836779057979584, Final Batch Loss: 0.09989257156848907\n",
      "Epoch 4207, Loss: 0.3121894747018814, Final Batch Loss: 0.16141007840633392\n",
      "Epoch 4208, Loss: 0.3185131549835205, Final Batch Loss: 0.19300919771194458\n",
      "Epoch 4209, Loss: 0.3321399539709091, Final Batch Loss: 0.1751711070537567\n",
      "Epoch 4210, Loss: 0.29718930274248123, Final Batch Loss: 0.17237097024917603\n",
      "Epoch 4211, Loss: 0.32234735786914825, Final Batch Loss: 0.13274456560611725\n",
      "Epoch 4212, Loss: 0.34411755204200745, Final Batch Loss: 0.20340704917907715\n",
      "Epoch 4213, Loss: 0.3686566948890686, Final Batch Loss: 0.18647710978984833\n",
      "Epoch 4214, Loss: 0.34889237582683563, Final Batch Loss: 0.1535174697637558\n",
      "Epoch 4215, Loss: 0.3509902358055115, Final Batch Loss: 0.1825178861618042\n",
      "Epoch 4216, Loss: 0.3402280807495117, Final Batch Loss: 0.17205895483493805\n",
      "Epoch 4217, Loss: 0.35000142455101013, Final Batch Loss: 0.1868889033794403\n",
      "Epoch 4218, Loss: 0.3116353899240494, Final Batch Loss: 0.13972090184688568\n",
      "Epoch 4219, Loss: 0.27121874690055847, Final Batch Loss: 0.10782705247402191\n",
      "Epoch 4220, Loss: 0.3286767601966858, Final Batch Loss: 0.17093603312969208\n",
      "Epoch 4221, Loss: 0.3354816138744354, Final Batch Loss: 0.19498242437839508\n",
      "Epoch 4222, Loss: 0.34700579941272736, Final Batch Loss: 0.18076343834400177\n",
      "Epoch 4223, Loss: 0.3311697095632553, Final Batch Loss: 0.19041289389133453\n",
      "Epoch 4224, Loss: 0.35126709938049316, Final Batch Loss: 0.1375606805086136\n",
      "Epoch 4225, Loss: 0.3799225389957428, Final Batch Loss: 0.2482926845550537\n",
      "Epoch 4226, Loss: 0.2495754510164261, Final Batch Loss: 0.09052546322345734\n",
      "Epoch 4227, Loss: 0.2718064785003662, Final Batch Loss: 0.13863512873649597\n",
      "Epoch 4228, Loss: 0.35899171233177185, Final Batch Loss: 0.18187619745731354\n",
      "Epoch 4229, Loss: 0.314192958176136, Final Batch Loss: 0.19431814551353455\n",
      "Epoch 4230, Loss: 0.36966320872306824, Final Batch Loss: 0.1583600640296936\n",
      "Epoch 4231, Loss: 0.32291416823863983, Final Batch Loss: 0.17206917703151703\n",
      "Epoch 4232, Loss: 0.31095145642757416, Final Batch Loss: 0.18034499883651733\n",
      "Epoch 4233, Loss: 0.4319430738687515, Final Batch Loss: 0.2735072672367096\n",
      "Epoch 4234, Loss: 0.2532306984066963, Final Batch Loss: 0.1319083869457245\n",
      "Epoch 4235, Loss: 0.2841867730021477, Final Batch Loss: 0.1614428609609604\n",
      "Epoch 4236, Loss: 0.3290722072124481, Final Batch Loss: 0.15913699567317963\n",
      "Epoch 4237, Loss: 0.2950243651866913, Final Batch Loss: 0.15397463738918304\n",
      "Epoch 4238, Loss: 0.2965235710144043, Final Batch Loss: 0.14065250754356384\n",
      "Epoch 4239, Loss: 0.3089155852794647, Final Batch Loss: 0.15405809879302979\n",
      "Epoch 4240, Loss: 0.33004069328308105, Final Batch Loss: 0.13860392570495605\n",
      "Epoch 4241, Loss: 0.3114232197403908, Final Batch Loss: 0.10695045441389084\n",
      "Epoch 4242, Loss: 0.26051612943410873, Final Batch Loss: 0.10222037881612778\n",
      "Epoch 4243, Loss: 0.28517570346593857, Final Batch Loss: 0.10190900415182114\n",
      "Epoch 4244, Loss: 0.3264132887125015, Final Batch Loss: 0.14542758464813232\n",
      "Epoch 4245, Loss: 0.3248557299375534, Final Batch Loss: 0.16154387593269348\n",
      "Epoch 4246, Loss: 0.3267962411046028, Final Batch Loss: 0.11391494423151016\n",
      "Epoch 4247, Loss: 0.36529259383678436, Final Batch Loss: 0.22473768889904022\n",
      "Epoch 4248, Loss: 0.37169648706912994, Final Batch Loss: 0.20937471091747284\n",
      "Epoch 4249, Loss: 0.3140318840742111, Final Batch Loss: 0.1579810380935669\n",
      "Epoch 4250, Loss: 0.3403715044260025, Final Batch Loss: 0.20824499428272247\n",
      "Epoch 4251, Loss: 0.2813454419374466, Final Batch Loss: 0.14664535224437714\n",
      "Epoch 4252, Loss: 0.34966637194156647, Final Batch Loss: 0.16654695570468903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4253, Loss: 0.32253575325012207, Final Batch Loss: 0.1421298235654831\n",
      "Epoch 4254, Loss: 0.29707978665828705, Final Batch Loss: 0.13609008491039276\n",
      "Epoch 4255, Loss: 0.36633481085300446, Final Batch Loss: 0.1543952375650406\n",
      "Epoch 4256, Loss: 0.3988683968782425, Final Batch Loss: 0.20087987184524536\n",
      "Epoch 4257, Loss: 0.32171186804771423, Final Batch Loss: 0.196061909198761\n",
      "Epoch 4258, Loss: 0.412929892539978, Final Batch Loss: 0.18730704486370087\n",
      "Epoch 4259, Loss: 0.3190901130437851, Final Batch Loss: 0.1626449078321457\n",
      "Epoch 4260, Loss: 0.33041320741176605, Final Batch Loss: 0.15395143628120422\n",
      "Epoch 4261, Loss: 0.2754710391163826, Final Batch Loss: 0.11291082948446274\n",
      "Epoch 4262, Loss: 0.3822057694196701, Final Batch Loss: 0.2293403446674347\n",
      "Epoch 4263, Loss: 0.3152769133448601, Final Batch Loss: 0.11914006620645523\n",
      "Epoch 4264, Loss: 0.3274244964122772, Final Batch Loss: 0.15431976318359375\n",
      "Epoch 4265, Loss: 0.3443792462348938, Final Batch Loss: 0.21593131124973297\n",
      "Epoch 4266, Loss: 0.29162634909152985, Final Batch Loss: 0.15191616117954254\n",
      "Epoch 4267, Loss: 0.27514132112264633, Final Batch Loss: 0.11618543416261673\n",
      "Epoch 4268, Loss: 0.32377640902996063, Final Batch Loss: 0.17586803436279297\n",
      "Epoch 4269, Loss: 0.3221626281738281, Final Batch Loss: 0.17841584980487823\n",
      "Epoch 4270, Loss: 0.2994169741868973, Final Batch Loss: 0.1512785255908966\n",
      "Epoch 4271, Loss: 0.3731582760810852, Final Batch Loss: 0.20105233788490295\n",
      "Epoch 4272, Loss: 0.3583955019712448, Final Batch Loss: 0.21278369426727295\n",
      "Epoch 4273, Loss: 0.3069882094860077, Final Batch Loss: 0.1319752335548401\n",
      "Epoch 4274, Loss: 0.3110513836145401, Final Batch Loss: 0.1640540361404419\n",
      "Epoch 4275, Loss: 0.3921484500169754, Final Batch Loss: 0.21372753381729126\n",
      "Epoch 4276, Loss: 0.31606774032115936, Final Batch Loss: 0.1592697650194168\n",
      "Epoch 4277, Loss: 0.3549993485212326, Final Batch Loss: 0.16101674735546112\n",
      "Epoch 4278, Loss: 0.3763100653886795, Final Batch Loss: 0.2148076295852661\n",
      "Epoch 4279, Loss: 0.3449733257293701, Final Batch Loss: 0.17403987050056458\n",
      "Epoch 4280, Loss: 0.30403687059879303, Final Batch Loss: 0.1544194519519806\n",
      "Epoch 4281, Loss: 0.29576558619737625, Final Batch Loss: 0.17759829759597778\n",
      "Epoch 4282, Loss: 0.30730316042900085, Final Batch Loss: 0.1266796588897705\n",
      "Epoch 4283, Loss: 0.3762813210487366, Final Batch Loss: 0.20470841228961945\n",
      "Epoch 4284, Loss: 0.31020118296146393, Final Batch Loss: 0.18790805339813232\n",
      "Epoch 4285, Loss: 0.370856910943985, Final Batch Loss: 0.18561069667339325\n",
      "Epoch 4286, Loss: 0.3746662735939026, Final Batch Loss: 0.17265605926513672\n",
      "Epoch 4287, Loss: 0.3311960846185684, Final Batch Loss: 0.13501770794391632\n",
      "Epoch 4288, Loss: 0.2787002846598625, Final Batch Loss: 0.12377137690782547\n",
      "Epoch 4289, Loss: 0.28325600922107697, Final Batch Loss: 0.10210038721561432\n",
      "Epoch 4290, Loss: 0.305608406662941, Final Batch Loss: 0.1495109498500824\n",
      "Epoch 4291, Loss: 0.2803625389933586, Final Batch Loss: 0.10896780341863632\n",
      "Epoch 4292, Loss: 0.3119071424007416, Final Batch Loss: 0.16163671016693115\n",
      "Epoch 4293, Loss: 0.28813354671001434, Final Batch Loss: 0.1485358625650406\n",
      "Epoch 4294, Loss: 0.3531505912542343, Final Batch Loss: 0.1642017662525177\n",
      "Epoch 4295, Loss: 0.38179677724838257, Final Batch Loss: 0.2498960793018341\n",
      "Epoch 4296, Loss: 0.311734676361084, Final Batch Loss: 0.1660068929195404\n",
      "Epoch 4297, Loss: 0.31235092878341675, Final Batch Loss: 0.1402488350868225\n",
      "Epoch 4298, Loss: 0.2716328576207161, Final Batch Loss: 0.11389587074518204\n",
      "Epoch 4299, Loss: 0.4996369481086731, Final Batch Loss: 0.25793394446372986\n",
      "Epoch 4300, Loss: 0.30199360847473145, Final Batch Loss: 0.15495561063289642\n",
      "Epoch 4301, Loss: 0.30191604793071747, Final Batch Loss: 0.15947796404361725\n",
      "Epoch 4302, Loss: 0.28987614810466766, Final Batch Loss: 0.1475994735956192\n",
      "Epoch 4303, Loss: 0.2940438836812973, Final Batch Loss: 0.09875743091106415\n",
      "Epoch 4304, Loss: 0.4204798638820648, Final Batch Loss: 0.237214133143425\n",
      "Epoch 4305, Loss: 0.29476530849933624, Final Batch Loss: 0.1666007786989212\n",
      "Epoch 4306, Loss: 0.31954433023929596, Final Batch Loss: 0.16111892461776733\n",
      "Epoch 4307, Loss: 0.2691854164004326, Final Batch Loss: 0.1134599968791008\n",
      "Epoch 4308, Loss: 0.28954051434993744, Final Batch Loss: 0.12534475326538086\n",
      "Epoch 4309, Loss: 0.3037848025560379, Final Batch Loss: 0.14617793262004852\n",
      "Epoch 4310, Loss: 0.3024628311395645, Final Batch Loss: 0.15678107738494873\n",
      "Epoch 4311, Loss: 0.31929172575473785, Final Batch Loss: 0.1663355678319931\n",
      "Epoch 4312, Loss: 0.3240356892347336, Final Batch Loss: 0.17861640453338623\n",
      "Epoch 4313, Loss: 0.348788782954216, Final Batch Loss: 0.15193700790405273\n",
      "Epoch 4314, Loss: 0.28611836582422256, Final Batch Loss: 0.1082988902926445\n",
      "Epoch 4315, Loss: 0.2662045806646347, Final Batch Loss: 0.1195073127746582\n",
      "Epoch 4316, Loss: 0.30392225086688995, Final Batch Loss: 0.18369635939598083\n",
      "Epoch 4317, Loss: 0.2679988220334053, Final Batch Loss: 0.10612290352582932\n",
      "Epoch 4318, Loss: 0.2877352237701416, Final Batch Loss: 0.11157725751399994\n",
      "Epoch 4319, Loss: 0.3680447190999985, Final Batch Loss: 0.19311675429344177\n",
      "Epoch 4320, Loss: 0.31253352016210556, Final Batch Loss: 0.1886500120162964\n",
      "Epoch 4321, Loss: 0.2849385291337967, Final Batch Loss: 0.11334429681301117\n",
      "Epoch 4322, Loss: 0.3613438159227371, Final Batch Loss: 0.17545492947101593\n",
      "Epoch 4323, Loss: 0.3914039880037308, Final Batch Loss: 0.175481915473938\n",
      "Epoch 4324, Loss: 0.36730536818504333, Final Batch Loss: 0.225246399641037\n",
      "Epoch 4325, Loss: 0.28962017595767975, Final Batch Loss: 0.14001117646694183\n",
      "Epoch 4326, Loss: 0.2661672681570053, Final Batch Loss: 0.14387023448944092\n",
      "Epoch 4327, Loss: 0.34511883556842804, Final Batch Loss: 0.1755254566669464\n",
      "Epoch 4328, Loss: 0.3061463162302971, Final Batch Loss: 0.11375149339437485\n",
      "Epoch 4329, Loss: 0.2851540148258209, Final Batch Loss: 0.15745845437049866\n",
      "Epoch 4330, Loss: 0.3139716535806656, Final Batch Loss: 0.12189754843711853\n",
      "Epoch 4331, Loss: 0.31372596323490143, Final Batch Loss: 0.17907024919986725\n",
      "Epoch 4332, Loss: 0.39786671102046967, Final Batch Loss: 0.24767528474330902\n",
      "Epoch 4333, Loss: 0.3172629773616791, Final Batch Loss: 0.1867464780807495\n",
      "Epoch 4334, Loss: 0.35090523958206177, Final Batch Loss: 0.14074087142944336\n",
      "Epoch 4335, Loss: 0.3240072876214981, Final Batch Loss: 0.18336741626262665\n",
      "Epoch 4336, Loss: 0.2814745604991913, Final Batch Loss: 0.11867751181125641\n",
      "Epoch 4337, Loss: 0.3070203810930252, Final Batch Loss: 0.1481473594903946\n",
      "Epoch 4338, Loss: 0.3150012120604515, Final Batch Loss: 0.11924947053194046\n",
      "Epoch 4339, Loss: 0.336296871304512, Final Batch Loss: 0.17271222174167633\n",
      "Epoch 4340, Loss: 0.2859673649072647, Final Batch Loss: 0.14872002601623535\n",
      "Epoch 4341, Loss: 0.3014032542705536, Final Batch Loss: 0.13407553732395172\n",
      "Epoch 4342, Loss: 0.31719686836004257, Final Batch Loss: 0.1148688867688179\n",
      "Epoch 4343, Loss: 0.3382083475589752, Final Batch Loss: 0.16992036998271942\n",
      "Epoch 4344, Loss: 0.2951675057411194, Final Batch Loss: 0.10890436172485352\n",
      "Epoch 4345, Loss: 0.3409883975982666, Final Batch Loss: 0.18766121566295624\n",
      "Epoch 4346, Loss: 0.3345562517642975, Final Batch Loss: 0.18918916583061218\n",
      "Epoch 4347, Loss: 0.3768126666545868, Final Batch Loss: 0.11562791466712952\n",
      "Epoch 4348, Loss: 0.3548087328672409, Final Batch Loss: 0.18594571948051453\n",
      "Epoch 4349, Loss: 0.3538035750389099, Final Batch Loss: 0.16616015136241913\n",
      "Epoch 4350, Loss: 0.28627729415893555, Final Batch Loss: 0.1402001678943634\n",
      "Epoch 4351, Loss: 0.2803791016340256, Final Batch Loss: 0.13885919749736786\n",
      "Epoch 4352, Loss: 0.3034372329711914, Final Batch Loss: 0.1478411704301834\n",
      "Epoch 4353, Loss: 0.24873194098472595, Final Batch Loss: 0.11918175220489502\n",
      "Epoch 4354, Loss: 0.31641314923763275, Final Batch Loss: 0.14490202069282532\n",
      "Epoch 4355, Loss: 0.2817605584859848, Final Batch Loss: 0.1333310306072235\n",
      "Epoch 4356, Loss: 0.317317932844162, Final Batch Loss: 0.14029906690120697\n",
      "Epoch 4357, Loss: 0.27766451239585876, Final Batch Loss: 0.13771823048591614\n",
      "Epoch 4358, Loss: 0.3467477262020111, Final Batch Loss: 0.16030600666999817\n",
      "Epoch 4359, Loss: 0.3229425773024559, Final Batch Loss: 0.21013769507408142\n",
      "Epoch 4360, Loss: 0.29755881428718567, Final Batch Loss: 0.17086635529994965\n",
      "Epoch 4361, Loss: 0.36203087866306305, Final Batch Loss: 0.1991955190896988\n",
      "Epoch 4362, Loss: 0.3481106162071228, Final Batch Loss: 0.20264890789985657\n",
      "Epoch 4363, Loss: 0.28111840784549713, Final Batch Loss: 0.12328533828258514\n",
      "Epoch 4364, Loss: 0.23768438398838043, Final Batch Loss: 0.12386102229356766\n",
      "Epoch 4365, Loss: 0.34236180782318115, Final Batch Loss: 0.17505685985088348\n",
      "Epoch 4366, Loss: 0.3687923550605774, Final Batch Loss: 0.21607448160648346\n",
      "Epoch 4367, Loss: 0.3139803409576416, Final Batch Loss: 0.13249000906944275\n",
      "Epoch 4368, Loss: 0.289459764957428, Final Batch Loss: 0.13573329150676727\n",
      "Epoch 4369, Loss: 0.3627132922410965, Final Batch Loss: 0.2022378146648407\n",
      "Epoch 4370, Loss: 0.30842532217502594, Final Batch Loss: 0.14336265623569489\n",
      "Epoch 4371, Loss: 0.30977654457092285, Final Batch Loss: 0.17448395490646362\n",
      "Epoch 4372, Loss: 0.29987117648124695, Final Batch Loss: 0.14178432524204254\n",
      "Epoch 4373, Loss: 0.2607274055480957, Final Batch Loss: 0.12633977830410004\n",
      "Epoch 4374, Loss: 0.27300146222114563, Final Batch Loss: 0.13460594415664673\n",
      "Epoch 4375, Loss: 0.3026334047317505, Final Batch Loss: 0.13685095310211182\n",
      "Epoch 4376, Loss: 0.3192293345928192, Final Batch Loss: 0.17589634656906128\n",
      "Epoch 4377, Loss: 0.2897968590259552, Final Batch Loss: 0.1432027965784073\n",
      "Epoch 4378, Loss: 0.3580353558063507, Final Batch Loss: 0.16991426050662994\n",
      "Epoch 4379, Loss: 0.3758111596107483, Final Batch Loss: 0.21656553447246552\n",
      "Epoch 4380, Loss: 0.3364372253417969, Final Batch Loss: 0.18722735345363617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4381, Loss: 0.2744033485651016, Final Batch Loss: 0.12148435413837433\n",
      "Epoch 4382, Loss: 0.30152076482772827, Final Batch Loss: 0.15987588465213776\n",
      "Epoch 4383, Loss: 0.3613458424806595, Final Batch Loss: 0.19838951528072357\n",
      "Epoch 4384, Loss: 0.33103199303150177, Final Batch Loss: 0.1605791598558426\n",
      "Epoch 4385, Loss: 0.29959045350551605, Final Batch Loss: 0.1651233434677124\n",
      "Epoch 4386, Loss: 0.3150322437286377, Final Batch Loss: 0.17057514190673828\n",
      "Epoch 4387, Loss: 0.32835377752780914, Final Batch Loss: 0.19708716869354248\n",
      "Epoch 4388, Loss: 0.3703094869852066, Final Batch Loss: 0.14672864973545074\n",
      "Epoch 4389, Loss: 0.3114754855632782, Final Batch Loss: 0.1553472876548767\n",
      "Epoch 4390, Loss: 0.29781512916088104, Final Batch Loss: 0.16139577329158783\n",
      "Epoch 4391, Loss: 0.3997225761413574, Final Batch Loss: 0.2424367070198059\n",
      "Epoch 4392, Loss: 0.28480857610702515, Final Batch Loss: 0.13943110406398773\n",
      "Epoch 4393, Loss: 0.3304625302553177, Final Batch Loss: 0.20384544134140015\n",
      "Epoch 4394, Loss: 0.29005439579486847, Final Batch Loss: 0.14527921378612518\n",
      "Epoch 4395, Loss: 0.2672334089875221, Final Batch Loss: 0.14698298275470734\n",
      "Epoch 4396, Loss: 0.3473997563123703, Final Batch Loss: 0.18301600217819214\n",
      "Epoch 4397, Loss: 0.26829608529806137, Final Batch Loss: 0.11486121267080307\n",
      "Epoch 4398, Loss: 0.3467039316892624, Final Batch Loss: 0.21716387569904327\n",
      "Epoch 4399, Loss: 0.2823348790407181, Final Batch Loss: 0.14948749542236328\n",
      "Epoch 4400, Loss: 0.2863074541091919, Final Batch Loss: 0.1527138501405716\n",
      "Epoch 4401, Loss: 0.3478688448667526, Final Batch Loss: 0.19408871233463287\n",
      "Epoch 4402, Loss: 0.2731194496154785, Final Batch Loss: 0.10146257281303406\n",
      "Epoch 4403, Loss: 0.27239832282066345, Final Batch Loss: 0.1119053065776825\n",
      "Epoch 4404, Loss: 0.36402006447315216, Final Batch Loss: 0.18011750280857086\n",
      "Epoch 4405, Loss: 0.3288709968328476, Final Batch Loss: 0.16420072317123413\n",
      "Epoch 4406, Loss: 0.33643846213817596, Final Batch Loss: 0.1533711552619934\n",
      "Epoch 4407, Loss: 0.33650192618370056, Final Batch Loss: 0.1672128438949585\n",
      "Epoch 4408, Loss: 0.3873995542526245, Final Batch Loss: 0.24338024854660034\n",
      "Epoch 4409, Loss: 0.3268363028764725, Final Batch Loss: 0.16667123138904572\n",
      "Epoch 4410, Loss: 0.28950199484825134, Final Batch Loss: 0.14833246171474457\n",
      "Epoch 4411, Loss: 0.3123163878917694, Final Batch Loss: 0.14598743617534637\n",
      "Epoch 4412, Loss: 0.3024120181798935, Final Batch Loss: 0.16107910871505737\n",
      "Epoch 4413, Loss: 0.34403690695762634, Final Batch Loss: 0.15317460894584656\n",
      "Epoch 4414, Loss: 0.3241725265979767, Final Batch Loss: 0.16593047976493835\n",
      "Epoch 4415, Loss: 0.35107432305812836, Final Batch Loss: 0.1623840034008026\n",
      "Epoch 4416, Loss: 0.2422027662396431, Final Batch Loss: 0.11649245768785477\n",
      "Epoch 4417, Loss: 0.32404761016368866, Final Batch Loss: 0.19776256382465363\n",
      "Epoch 4418, Loss: 0.35343311727046967, Final Batch Loss: 0.19137205183506012\n",
      "Epoch 4419, Loss: 0.3180641084909439, Final Batch Loss: 0.1715797483921051\n",
      "Epoch 4420, Loss: 0.24848222732543945, Final Batch Loss: 0.12065169215202332\n",
      "Epoch 4421, Loss: 0.2962655574083328, Final Batch Loss: 0.14356455206871033\n",
      "Epoch 4422, Loss: 0.28560929745435715, Final Batch Loss: 0.10818823426961899\n",
      "Epoch 4423, Loss: 0.30054211616516113, Final Batch Loss: 0.14600837230682373\n",
      "Epoch 4424, Loss: 0.293093666434288, Final Batch Loss: 0.13074257969856262\n",
      "Epoch 4425, Loss: 0.32669323682785034, Final Batch Loss: 0.18917421996593475\n",
      "Epoch 4426, Loss: 0.34392035007476807, Final Batch Loss: 0.16601552069187164\n",
      "Epoch 4427, Loss: 0.3297821581363678, Final Batch Loss: 0.19373555481433868\n",
      "Epoch 4428, Loss: 0.34486663341522217, Final Batch Loss: 0.2111540585756302\n",
      "Epoch 4429, Loss: 0.31378431618213654, Final Batch Loss: 0.18378852307796478\n",
      "Epoch 4430, Loss: 0.29494795203208923, Final Batch Loss: 0.12151186168193817\n",
      "Epoch 4431, Loss: 0.37425628304481506, Final Batch Loss: 0.12701861560344696\n",
      "Epoch 4432, Loss: 0.2498283088207245, Final Batch Loss: 0.11260955035686493\n",
      "Epoch 4433, Loss: 0.27610859274864197, Final Batch Loss: 0.1393023133277893\n",
      "Epoch 4434, Loss: 0.31539517641067505, Final Batch Loss: 0.14400404691696167\n",
      "Epoch 4435, Loss: 0.2979237735271454, Final Batch Loss: 0.16311368346214294\n",
      "Epoch 4436, Loss: 0.3533945083618164, Final Batch Loss: 0.19976826012134552\n",
      "Epoch 4437, Loss: 0.3015977293252945, Final Batch Loss: 0.14253294467926025\n",
      "Epoch 4438, Loss: 0.30933260917663574, Final Batch Loss: 0.16450269520282745\n",
      "Epoch 4439, Loss: 0.2695384994149208, Final Batch Loss: 0.10272783786058426\n",
      "Epoch 4440, Loss: 0.31642384827136993, Final Batch Loss: 0.15013252198696136\n",
      "Epoch 4441, Loss: 0.3066735565662384, Final Batch Loss: 0.16089090704917908\n",
      "Epoch 4442, Loss: 0.26435767114162445, Final Batch Loss: 0.1273670494556427\n",
      "Epoch 4443, Loss: 0.2857156991958618, Final Batch Loss: 0.1481219083070755\n",
      "Epoch 4444, Loss: 0.2761532813310623, Final Batch Loss: 0.14828690886497498\n",
      "Epoch 4445, Loss: 0.3253730684518814, Final Batch Loss: 0.21192751824855804\n",
      "Epoch 4446, Loss: 0.2610122039914131, Final Batch Loss: 0.10727594047784805\n",
      "Epoch 4447, Loss: 0.23234037309885025, Final Batch Loss: 0.09182130545377731\n",
      "Epoch 4448, Loss: 0.25951237231492996, Final Batch Loss: 0.1370881348848343\n",
      "Epoch 4449, Loss: 0.3014306202530861, Final Batch Loss: 0.11217194050550461\n",
      "Epoch 4450, Loss: 0.3332318365573883, Final Batch Loss: 0.16761530935764313\n",
      "Epoch 4451, Loss: 0.30062928795814514, Final Batch Loss: 0.16498681902885437\n",
      "Epoch 4452, Loss: 0.32779237627983093, Final Batch Loss: 0.1537129133939743\n",
      "Epoch 4453, Loss: 0.2731980085372925, Final Batch Loss: 0.14365163445472717\n",
      "Epoch 4454, Loss: 0.2742016613483429, Final Batch Loss: 0.15025925636291504\n",
      "Epoch 4455, Loss: 0.2581387832760811, Final Batch Loss: 0.1363520324230194\n",
      "Epoch 4456, Loss: 0.24241141229867935, Final Batch Loss: 0.10712479799985886\n",
      "Epoch 4457, Loss: 0.23831772059202194, Final Batch Loss: 0.09144312888383865\n",
      "Epoch 4458, Loss: 0.295433446764946, Final Batch Loss: 0.14997611939907074\n",
      "Epoch 4459, Loss: 0.2708256170153618, Final Batch Loss: 0.09448067098855972\n",
      "Epoch 4460, Loss: 0.2589977979660034, Final Batch Loss: 0.12146967649459839\n",
      "Epoch 4461, Loss: 0.3077130764722824, Final Batch Loss: 0.15458545088768005\n",
      "Epoch 4462, Loss: 0.28813882172107697, Final Batch Loss: 0.13509514927864075\n",
      "Epoch 4463, Loss: 0.3170434832572937, Final Batch Loss: 0.19166146218776703\n",
      "Epoch 4464, Loss: 0.31502383947372437, Final Batch Loss: 0.15398423373699188\n",
      "Epoch 4465, Loss: 0.2984250783920288, Final Batch Loss: 0.17247144877910614\n",
      "Epoch 4466, Loss: 0.2975154519081116, Final Batch Loss: 0.13357135653495789\n",
      "Epoch 4467, Loss: 0.27533985674381256, Final Batch Loss: 0.14106132090091705\n",
      "Epoch 4468, Loss: 0.3165099695324898, Final Batch Loss: 0.20867778360843658\n",
      "Epoch 4469, Loss: 0.3018612861633301, Final Batch Loss: 0.16547946631908417\n",
      "Epoch 4470, Loss: 0.32988181710243225, Final Batch Loss: 0.18265599012374878\n",
      "Epoch 4471, Loss: 0.29312828183174133, Final Batch Loss: 0.13783453404903412\n",
      "Epoch 4472, Loss: 0.34062477946281433, Final Batch Loss: 0.18368054926395416\n",
      "Epoch 4473, Loss: 0.2883042097091675, Final Batch Loss: 0.1326880007982254\n",
      "Epoch 4474, Loss: 0.3748834878206253, Final Batch Loss: 0.18602117896080017\n",
      "Epoch 4475, Loss: 0.2704455628991127, Final Batch Loss: 0.10536213964223862\n",
      "Epoch 4476, Loss: 0.24798189848661423, Final Batch Loss: 0.12823499739170074\n",
      "Epoch 4477, Loss: 0.2830212712287903, Final Batch Loss: 0.14229653775691986\n",
      "Epoch 4478, Loss: 0.2577880918979645, Final Batch Loss: 0.1316092163324356\n",
      "Epoch 4479, Loss: 0.30712202936410904, Final Batch Loss: 0.12264520674943924\n",
      "Epoch 4480, Loss: 0.37407706677913666, Final Batch Loss: 0.2088439166545868\n",
      "Epoch 4481, Loss: 0.27998100221157074, Final Batch Loss: 0.1123369038105011\n",
      "Epoch 4482, Loss: 0.3225702494382858, Final Batch Loss: 0.16602759063243866\n",
      "Epoch 4483, Loss: 0.33177411556243896, Final Batch Loss: 0.1884736716747284\n",
      "Epoch 4484, Loss: 0.2996385246515274, Final Batch Loss: 0.17120113968849182\n",
      "Epoch 4485, Loss: 0.31144027411937714, Final Batch Loss: 0.17163006961345673\n",
      "Epoch 4486, Loss: 0.20316660404205322, Final Batch Loss: 0.09892868995666504\n",
      "Epoch 4487, Loss: 0.3216540813446045, Final Batch Loss: 0.17102620005607605\n",
      "Epoch 4488, Loss: 0.33184313774108887, Final Batch Loss: 0.16393715143203735\n",
      "Epoch 4489, Loss: 0.2569645270705223, Final Batch Loss: 0.14211474359035492\n",
      "Epoch 4490, Loss: 0.2930828481912613, Final Batch Loss: 0.1830655336380005\n",
      "Epoch 4491, Loss: 0.32698582112789154, Final Batch Loss: 0.19755423069000244\n",
      "Epoch 4492, Loss: 0.2960367202758789, Final Batch Loss: 0.16888029873371124\n",
      "Epoch 4493, Loss: 0.24998009949922562, Final Batch Loss: 0.10408740490674973\n",
      "Epoch 4494, Loss: 0.2564711719751358, Final Batch Loss: 0.12178230285644531\n",
      "Epoch 4495, Loss: 0.2653540000319481, Final Batch Loss: 0.17953431606292725\n",
      "Epoch 4496, Loss: 0.29909954965114594, Final Batch Loss: 0.1653888076543808\n",
      "Epoch 4497, Loss: 0.30029062926769257, Final Batch Loss: 0.17106331884860992\n",
      "Epoch 4498, Loss: 0.3500203490257263, Final Batch Loss: 0.15912537276744843\n",
      "Epoch 4499, Loss: 0.2992800325155258, Final Batch Loss: 0.12807883322238922\n",
      "Epoch 4500, Loss: 0.25766588747501373, Final Batch Loss: 0.1184639185667038\n",
      "Epoch 4501, Loss: 0.2937055826187134, Final Batch Loss: 0.1454792022705078\n",
      "Epoch 4502, Loss: 0.25814294070005417, Final Batch Loss: 0.11560388654470444\n",
      "Epoch 4503, Loss: 0.25524838268756866, Final Batch Loss: 0.13003359735012054\n",
      "Epoch 4504, Loss: 0.3330908417701721, Final Batch Loss: 0.1408967673778534\n",
      "Epoch 4505, Loss: 0.28811441361904144, Final Batch Loss: 0.11638018488883972\n",
      "Epoch 4506, Loss: 0.4240526109933853, Final Batch Loss: 0.15237365663051605\n",
      "Epoch 4507, Loss: 0.3034868836402893, Final Batch Loss: 0.1407523900270462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4508, Loss: 0.2827308624982834, Final Batch Loss: 0.11968103051185608\n",
      "Epoch 4509, Loss: 0.32745160162448883, Final Batch Loss: 0.1683339923620224\n",
      "Epoch 4510, Loss: 0.2851405292749405, Final Batch Loss: 0.15529565513134003\n",
      "Epoch 4511, Loss: 0.2862449884414673, Final Batch Loss: 0.15253616869449615\n",
      "Epoch 4512, Loss: 0.3190660774707794, Final Batch Loss: 0.15327848494052887\n",
      "Epoch 4513, Loss: 0.253009170293808, Final Batch Loss: 0.10902726650238037\n",
      "Epoch 4514, Loss: 0.25849296152591705, Final Batch Loss: 0.09745422005653381\n",
      "Epoch 4515, Loss: 0.2778814509510994, Final Batch Loss: 0.11486542969942093\n",
      "Epoch 4516, Loss: 0.2830222398042679, Final Batch Loss: 0.12991821765899658\n",
      "Epoch 4517, Loss: 0.31190650165081024, Final Batch Loss: 0.14671632647514343\n",
      "Epoch 4518, Loss: 0.31375719606876373, Final Batch Loss: 0.161209836602211\n",
      "Epoch 4519, Loss: 0.2589115649461746, Final Batch Loss: 0.12682530283927917\n",
      "Epoch 4520, Loss: 0.3650791794061661, Final Batch Loss: 0.23541750013828278\n",
      "Epoch 4521, Loss: 0.2532295659184456, Final Batch Loss: 0.15426230430603027\n",
      "Epoch 4522, Loss: 0.30567003786563873, Final Batch Loss: 0.1607201099395752\n",
      "Epoch 4523, Loss: 0.3582850396633148, Final Batch Loss: 0.2086317539215088\n",
      "Epoch 4524, Loss: 0.25404132902622223, Final Batch Loss: 0.13740065693855286\n",
      "Epoch 4525, Loss: 0.2986639589071274, Final Batch Loss: 0.12576065957546234\n",
      "Epoch 4526, Loss: 0.27802296727895737, Final Batch Loss: 0.12145204097032547\n",
      "Epoch 4527, Loss: 0.20785769820213318, Final Batch Loss: 0.10747925937175751\n",
      "Epoch 4528, Loss: 0.2745019793510437, Final Batch Loss: 0.12783658504486084\n",
      "Epoch 4529, Loss: 0.2862616926431656, Final Batch Loss: 0.12304352223873138\n",
      "Epoch 4530, Loss: 0.25451480597257614, Final Batch Loss: 0.1419917643070221\n",
      "Epoch 4531, Loss: 0.30587050318717957, Final Batch Loss: 0.15571296215057373\n",
      "Epoch 4532, Loss: 0.2498813197016716, Final Batch Loss: 0.10602281242609024\n",
      "Epoch 4533, Loss: 0.24390725046396255, Final Batch Loss: 0.08267319947481155\n",
      "Epoch 4534, Loss: 0.2944548726081848, Final Batch Loss: 0.14609648287296295\n",
      "Epoch 4535, Loss: 0.27884049713611603, Final Batch Loss: 0.14440754055976868\n",
      "Epoch 4536, Loss: 0.22537580132484436, Final Batch Loss: 0.1130262166261673\n",
      "Epoch 4537, Loss: 0.2674159109592438, Final Batch Loss: 0.1460382491350174\n",
      "Epoch 4538, Loss: 0.3506847769021988, Final Batch Loss: 0.16773724555969238\n",
      "Epoch 4539, Loss: 0.34111766517162323, Final Batch Loss: 0.14332760870456696\n",
      "Epoch 4540, Loss: 0.2642053961753845, Final Batch Loss: 0.12139317393302917\n",
      "Epoch 4541, Loss: 0.28509577363729477, Final Batch Loss: 0.11148042231798172\n",
      "Epoch 4542, Loss: 0.3635057955980301, Final Batch Loss: 0.15039511024951935\n",
      "Epoch 4543, Loss: 0.26327117532491684, Final Batch Loss: 0.12127185612916946\n",
      "Epoch 4544, Loss: 0.23983117938041687, Final Batch Loss: 0.14072959125041962\n",
      "Epoch 4545, Loss: 0.27871787548065186, Final Batch Loss: 0.13990281522274017\n",
      "Epoch 4546, Loss: 0.24458745121955872, Final Batch Loss: 0.09726132452487946\n",
      "Epoch 4547, Loss: 0.26629719138145447, Final Batch Loss: 0.08947466313838959\n",
      "Epoch 4548, Loss: 0.2565447986125946, Final Batch Loss: 0.11461964249610901\n",
      "Epoch 4549, Loss: 0.252774640917778, Final Batch Loss: 0.12814126908779144\n",
      "Epoch 4550, Loss: 0.2524769976735115, Final Batch Loss: 0.11897770315408707\n",
      "Epoch 4551, Loss: 0.23634017258882523, Final Batch Loss: 0.10042228549718857\n",
      "Epoch 4552, Loss: 0.28514179587364197, Final Batch Loss: 0.14609065651893616\n",
      "Epoch 4553, Loss: 0.3156043738126755, Final Batch Loss: 0.20033298432826996\n",
      "Epoch 4554, Loss: 0.28289513289928436, Final Batch Loss: 0.1305548995733261\n",
      "Epoch 4555, Loss: 0.22193367779254913, Final Batch Loss: 0.10596442222595215\n",
      "Epoch 4556, Loss: 0.2714221179485321, Final Batch Loss: 0.1398678421974182\n",
      "Epoch 4557, Loss: 0.34358298778533936, Final Batch Loss: 0.1807173788547516\n",
      "Epoch 4558, Loss: 0.2727739214897156, Final Batch Loss: 0.14411628246307373\n",
      "Epoch 4559, Loss: 0.26217401027679443, Final Batch Loss: 0.13660591840744019\n",
      "Epoch 4560, Loss: 0.2390548288822174, Final Batch Loss: 0.12211833149194717\n",
      "Epoch 4561, Loss: 0.3018925413489342, Final Batch Loss: 0.18002796173095703\n",
      "Epoch 4562, Loss: 0.384516105055809, Final Batch Loss: 0.25275713205337524\n",
      "Epoch 4563, Loss: 0.23423993587493896, Final Batch Loss: 0.10649694502353668\n",
      "Epoch 4564, Loss: 0.282020702958107, Final Batch Loss: 0.14052066206932068\n",
      "Epoch 4565, Loss: 0.2971813529729843, Final Batch Loss: 0.15364660322666168\n",
      "Epoch 4566, Loss: 0.3074449151754379, Final Batch Loss: 0.15978388488292694\n",
      "Epoch 4567, Loss: 0.2574176788330078, Final Batch Loss: 0.1074192076921463\n",
      "Epoch 4568, Loss: 0.2965441048145294, Final Batch Loss: 0.1567583829164505\n",
      "Epoch 4569, Loss: 0.27199599146842957, Final Batch Loss: 0.11881618201732635\n",
      "Epoch 4570, Loss: 0.37800997495651245, Final Batch Loss: 0.19120974838733673\n",
      "Epoch 4571, Loss: 0.415144145488739, Final Batch Loss: 0.26809239387512207\n",
      "Epoch 4572, Loss: 0.3669750988483429, Final Batch Loss: 0.22130462527275085\n",
      "Epoch 4573, Loss: 0.25193294882774353, Final Batch Loss: 0.15165123343467712\n",
      "Epoch 4574, Loss: 0.24894579499959946, Final Batch Loss: 0.10516277700662613\n",
      "Epoch 4575, Loss: 0.3039054274559021, Final Batch Loss: 0.1786729395389557\n",
      "Epoch 4576, Loss: 0.2741632014513016, Final Batch Loss: 0.12107108533382416\n",
      "Epoch 4577, Loss: 0.3776303678750992, Final Batch Loss: 0.2114715427160263\n",
      "Epoch 4578, Loss: 0.2735449895262718, Final Batch Loss: 0.11904556304216385\n",
      "Epoch 4579, Loss: 0.28832752257585526, Final Batch Loss: 0.12397182732820511\n",
      "Epoch 4580, Loss: 0.32177987694740295, Final Batch Loss: 0.17907248437404633\n",
      "Epoch 4581, Loss: 0.27498964220285416, Final Batch Loss: 0.11752266436815262\n",
      "Epoch 4582, Loss: 0.27991804480552673, Final Batch Loss: 0.15617063641548157\n",
      "Epoch 4583, Loss: 0.2769484668970108, Final Batch Loss: 0.0960240364074707\n",
      "Epoch 4584, Loss: 0.3000376671552658, Final Batch Loss: 0.16371984779834747\n",
      "Epoch 4585, Loss: 0.3143322765827179, Final Batch Loss: 0.1691812127828598\n",
      "Epoch 4586, Loss: 0.25546063482761383, Final Batch Loss: 0.09412306547164917\n",
      "Epoch 4587, Loss: 0.29731108248233795, Final Batch Loss: 0.15665303170681\n",
      "Epoch 4588, Loss: 0.25906819850206375, Final Batch Loss: 0.11973685771226883\n",
      "Epoch 4589, Loss: 0.23942472785711288, Final Batch Loss: 0.08477183431386948\n",
      "Epoch 4590, Loss: 0.2637113630771637, Final Batch Loss: 0.131477490067482\n",
      "Epoch 4591, Loss: 0.25224312394857407, Final Batch Loss: 0.13286460936069489\n",
      "Epoch 4592, Loss: 0.2908440828323364, Final Batch Loss: 0.16063983738422394\n",
      "Epoch 4593, Loss: 0.24474598467350006, Final Batch Loss: 0.08623188734054565\n",
      "Epoch 4594, Loss: 0.33641181886196136, Final Batch Loss: 0.19131895899772644\n",
      "Epoch 4595, Loss: 0.31868937611579895, Final Batch Loss: 0.13494917750358582\n",
      "Epoch 4596, Loss: 0.2638338729739189, Final Batch Loss: 0.1591830998659134\n",
      "Epoch 4597, Loss: 0.18643195927143097, Final Batch Loss: 0.049170076847076416\n",
      "Epoch 4598, Loss: 0.3337656259536743, Final Batch Loss: 0.18119968473911285\n",
      "Epoch 4599, Loss: 0.26992373913526535, Final Batch Loss: 0.12186779826879501\n",
      "Epoch 4600, Loss: 0.2084771692752838, Final Batch Loss: 0.09737400710582733\n",
      "Epoch 4601, Loss: 0.2605913430452347, Final Batch Loss: 0.1282098889350891\n",
      "Epoch 4602, Loss: 0.32542242109775543, Final Batch Loss: 0.21947027742862701\n",
      "Epoch 4603, Loss: 0.2883986532688141, Final Batch Loss: 0.11105899512767792\n",
      "Epoch 4604, Loss: 0.329952672123909, Final Batch Loss: 0.20240390300750732\n",
      "Epoch 4605, Loss: 0.3033328354358673, Final Batch Loss: 0.20292118191719055\n",
      "Epoch 4606, Loss: 0.2625283822417259, Final Batch Loss: 0.12107837945222855\n",
      "Epoch 4607, Loss: 0.3021419644355774, Final Batch Loss: 0.1346077173948288\n",
      "Epoch 4608, Loss: 0.38210447132587433, Final Batch Loss: 0.21266211569309235\n",
      "Epoch 4609, Loss: 0.2818702980875969, Final Batch Loss: 0.10849214345216751\n",
      "Epoch 4610, Loss: 0.25278647243976593, Final Batch Loss: 0.1480974406003952\n",
      "Epoch 4611, Loss: 0.21707924455404282, Final Batch Loss: 0.11364130675792694\n",
      "Epoch 4612, Loss: 0.2598215788602829, Final Batch Loss: 0.13586026430130005\n",
      "Epoch 4613, Loss: 0.3630594313144684, Final Batch Loss: 0.22611819207668304\n",
      "Epoch 4614, Loss: 0.2985537648200989, Final Batch Loss: 0.19622591137886047\n",
      "Epoch 4615, Loss: 0.44865524768829346, Final Batch Loss: 0.22684071958065033\n",
      "Epoch 4616, Loss: 0.28485626727342606, Final Batch Loss: 0.1610802263021469\n",
      "Epoch 4617, Loss: 0.2471189647912979, Final Batch Loss: 0.10051706433296204\n",
      "Epoch 4618, Loss: 0.2744826674461365, Final Batch Loss: 0.14772653579711914\n",
      "Epoch 4619, Loss: 0.25620099157094955, Final Batch Loss: 0.13559485971927643\n",
      "Epoch 4620, Loss: 0.3444748595356941, Final Batch Loss: 0.21972887217998505\n",
      "Epoch 4621, Loss: 0.28568483889102936, Final Batch Loss: 0.15943275392055511\n",
      "Epoch 4622, Loss: 0.205057755112648, Final Batch Loss: 0.0815158560872078\n",
      "Epoch 4623, Loss: 0.2589053437113762, Final Batch Loss: 0.10399851948022842\n",
      "Epoch 4624, Loss: 0.2899240627884865, Final Batch Loss: 0.1068091168999672\n",
      "Epoch 4625, Loss: 0.27856969833374023, Final Batch Loss: 0.15146219730377197\n",
      "Epoch 4626, Loss: 0.24735567718744278, Final Batch Loss: 0.1365472674369812\n",
      "Epoch 4627, Loss: 0.3972466289997101, Final Batch Loss: 0.2583458423614502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4628, Loss: 0.2794994115829468, Final Batch Loss: 0.13618822395801544\n",
      "Epoch 4629, Loss: 0.2572117894887924, Final Batch Loss: 0.12174546718597412\n",
      "Epoch 4630, Loss: 0.27354899048805237, Final Batch Loss: 0.13367369771003723\n",
      "Epoch 4631, Loss: 0.2942562773823738, Final Batch Loss: 0.09936103969812393\n",
      "Epoch 4632, Loss: 0.27601318061351776, Final Batch Loss: 0.14363504946231842\n",
      "Epoch 4633, Loss: 0.269070141017437, Final Batch Loss: 0.14906112849712372\n",
      "Epoch 4634, Loss: 0.26276180893182755, Final Batch Loss: 0.14243581891059875\n",
      "Epoch 4635, Loss: 0.3446711525321007, Final Batch Loss: 0.22491636872291565\n",
      "Epoch 4636, Loss: 0.260465145111084, Final Batch Loss: 0.11023594439029694\n",
      "Epoch 4637, Loss: 0.25081755220890045, Final Batch Loss: 0.1330433338880539\n",
      "Epoch 4638, Loss: 0.29255741834640503, Final Batch Loss: 0.16908258199691772\n",
      "Epoch 4639, Loss: 0.29705360531806946, Final Batch Loss: 0.1680937111377716\n",
      "Epoch 4640, Loss: 0.237098827958107, Final Batch Loss: 0.1367906928062439\n",
      "Epoch 4641, Loss: 0.23459341377019882, Final Batch Loss: 0.12794746458530426\n",
      "Epoch 4642, Loss: 0.28391464054584503, Final Batch Loss: 0.14810174703598022\n",
      "Epoch 4643, Loss: 0.33708328753709793, Final Batch Loss: 0.21292202174663544\n",
      "Epoch 4644, Loss: 0.27529188990592957, Final Batch Loss: 0.13946281373500824\n",
      "Epoch 4645, Loss: 0.23822569102048874, Final Batch Loss: 0.12928986549377441\n",
      "Epoch 4646, Loss: 0.3244694322347641, Final Batch Loss: 0.18107366561889648\n",
      "Epoch 4647, Loss: 0.30830568075180054, Final Batch Loss: 0.14881759881973267\n",
      "Epoch 4648, Loss: 0.2028166949748993, Final Batch Loss: 0.07777328789234161\n",
      "Epoch 4649, Loss: 0.22966239601373672, Final Batch Loss: 0.10434997826814651\n",
      "Epoch 4650, Loss: 0.24082719534635544, Final Batch Loss: 0.09529811888933182\n",
      "Epoch 4651, Loss: 0.30080078542232513, Final Batch Loss: 0.18377253413200378\n",
      "Epoch 4652, Loss: 0.25688279420137405, Final Batch Loss: 0.09465203434228897\n",
      "Epoch 4653, Loss: 0.21197009086608887, Final Batch Loss: 0.0633956640958786\n",
      "Epoch 4654, Loss: 0.24076709896326065, Final Batch Loss: 0.15536601841449738\n",
      "Epoch 4655, Loss: 0.24299277365207672, Final Batch Loss: 0.1511511206626892\n",
      "Epoch 4656, Loss: 0.2528698369860649, Final Batch Loss: 0.11158140748739243\n",
      "Epoch 4657, Loss: 0.3111363351345062, Final Batch Loss: 0.1584593653678894\n",
      "Epoch 4658, Loss: 0.3304903879761696, Final Batch Loss: 0.21234457194805145\n",
      "Epoch 4659, Loss: 0.25839992612600327, Final Batch Loss: 0.12116227298974991\n",
      "Epoch 4660, Loss: 0.2673742473125458, Final Batch Loss: 0.15532542765140533\n",
      "Epoch 4661, Loss: 0.21976514905691147, Final Batch Loss: 0.11287805438041687\n",
      "Epoch 4662, Loss: 0.266128771007061, Final Batch Loss: 0.1196177527308464\n",
      "Epoch 4663, Loss: 0.31256209313869476, Final Batch Loss: 0.13447657227516174\n",
      "Epoch 4664, Loss: 0.2896324545145035, Final Batch Loss: 0.1397191733121872\n",
      "Epoch 4665, Loss: 0.2733891233801842, Final Batch Loss: 0.15825523436069489\n",
      "Epoch 4666, Loss: 0.268279105424881, Final Batch Loss: 0.12598882615566254\n",
      "Epoch 4667, Loss: 0.34464848041534424, Final Batch Loss: 0.09103018045425415\n",
      "Epoch 4668, Loss: 0.23367954790592194, Final Batch Loss: 0.12447972595691681\n",
      "Epoch 4669, Loss: 0.29641132056713104, Final Batch Loss: 0.12903650104999542\n",
      "Epoch 4670, Loss: 0.2740163207054138, Final Batch Loss: 0.09654825925827026\n",
      "Epoch 4671, Loss: 0.3070915788412094, Final Batch Loss: 0.1581239402294159\n",
      "Epoch 4672, Loss: 0.24802367389202118, Final Batch Loss: 0.12024520337581635\n",
      "Epoch 4673, Loss: 0.29519274085760117, Final Batch Loss: 0.11827728897333145\n",
      "Epoch 4674, Loss: 0.24233479797840118, Final Batch Loss: 0.1390373855829239\n",
      "Epoch 4675, Loss: 0.2748698741197586, Final Batch Loss: 0.12889300286769867\n",
      "Epoch 4676, Loss: 0.3364552706480026, Final Batch Loss: 0.19122032821178436\n",
      "Epoch 4677, Loss: 0.2525293752551079, Final Batch Loss: 0.1412770003080368\n",
      "Epoch 4678, Loss: 0.24791522324085236, Final Batch Loss: 0.10826347768306732\n",
      "Epoch 4679, Loss: 0.26730094850063324, Final Batch Loss: 0.1541132926940918\n",
      "Epoch 4680, Loss: 0.20133521407842636, Final Batch Loss: 0.10431483387947083\n",
      "Epoch 4681, Loss: 0.22477587312459946, Final Batch Loss: 0.10849639028310776\n",
      "Epoch 4682, Loss: 0.29009294509887695, Final Batch Loss: 0.13937440514564514\n",
      "Epoch 4683, Loss: 0.3339376151561737, Final Batch Loss: 0.17052391171455383\n",
      "Epoch 4684, Loss: 0.2729765549302101, Final Batch Loss: 0.11495698243379593\n",
      "Epoch 4685, Loss: 0.24997840076684952, Final Batch Loss: 0.1393912136554718\n",
      "Epoch 4686, Loss: 0.27415674179792404, Final Batch Loss: 0.08868569880723953\n",
      "Epoch 4687, Loss: 0.33357417583465576, Final Batch Loss: 0.1425066441297531\n",
      "Epoch 4688, Loss: 0.2631795331835747, Final Batch Loss: 0.1128796860575676\n",
      "Epoch 4689, Loss: 0.2424497976899147, Final Batch Loss: 0.12937043607234955\n",
      "Epoch 4690, Loss: 0.24162104725837708, Final Batch Loss: 0.11890128999948502\n",
      "Epoch 4691, Loss: 0.3073355108499527, Final Batch Loss: 0.1676391214132309\n",
      "Epoch 4692, Loss: 0.2261059284210205, Final Batch Loss: 0.12596924602985382\n",
      "Epoch 4693, Loss: 0.20145449042320251, Final Batch Loss: 0.055205389857292175\n",
      "Epoch 4694, Loss: 0.2258608117699623, Final Batch Loss: 0.0901670977473259\n",
      "Epoch 4695, Loss: 0.27975592017173767, Final Batch Loss: 0.12885978817939758\n",
      "Epoch 4696, Loss: 0.23504190891981125, Final Batch Loss: 0.08197986334562302\n",
      "Epoch 4697, Loss: 0.3185824155807495, Final Batch Loss: 0.14134125411510468\n",
      "Epoch 4698, Loss: 0.24349702894687653, Final Batch Loss: 0.06924651563167572\n",
      "Epoch 4699, Loss: 0.2378748580813408, Final Batch Loss: 0.11861873418092728\n",
      "Epoch 4700, Loss: 0.2733064442873001, Final Batch Loss: 0.11729520559310913\n",
      "Epoch 4701, Loss: 0.2891632467508316, Final Batch Loss: 0.12674517929553986\n",
      "Epoch 4702, Loss: 0.22141524404287338, Final Batch Loss: 0.1041625514626503\n",
      "Epoch 4703, Loss: 0.2521677613258362, Final Batch Loss: 0.11743651330471039\n",
      "Epoch 4704, Loss: 0.2693348154425621, Final Batch Loss: 0.1533956676721573\n",
      "Epoch 4705, Loss: 0.2597430646419525, Final Batch Loss: 0.1300668716430664\n",
      "Epoch 4706, Loss: 0.2216430902481079, Final Batch Loss: 0.11865202337503433\n",
      "Epoch 4707, Loss: 0.39584462344646454, Final Batch Loss: 0.1920243352651596\n",
      "Epoch 4708, Loss: 0.24878361076116562, Final Batch Loss: 0.11934780329465866\n",
      "Epoch 4709, Loss: 0.3236081451177597, Final Batch Loss: 0.16940933465957642\n",
      "Epoch 4710, Loss: 0.25568924844264984, Final Batch Loss: 0.12840963900089264\n",
      "Epoch 4711, Loss: 0.35109037160873413, Final Batch Loss: 0.13922807574272156\n",
      "Epoch 4712, Loss: 0.47153257578611374, Final Batch Loss: 0.34681040048599243\n",
      "Epoch 4713, Loss: 0.2683631330728531, Final Batch Loss: 0.12783871591091156\n",
      "Epoch 4714, Loss: 0.24582809954881668, Final Batch Loss: 0.15110348165035248\n",
      "Epoch 4715, Loss: 0.30580003559589386, Final Batch Loss: 0.15715718269348145\n",
      "Epoch 4716, Loss: 0.22496003657579422, Final Batch Loss: 0.09430255740880966\n",
      "Epoch 4717, Loss: 0.34288354218006134, Final Batch Loss: 0.21251948177814484\n",
      "Epoch 4718, Loss: 0.35398995876312256, Final Batch Loss: 0.21123552322387695\n",
      "Epoch 4719, Loss: 0.29739442467689514, Final Batch Loss: 0.13976392149925232\n",
      "Epoch 4720, Loss: 0.21725476533174515, Final Batch Loss: 0.09705200046300888\n",
      "Epoch 4721, Loss: 0.22132250666618347, Final Batch Loss: 0.09092123806476593\n",
      "Epoch 4722, Loss: 0.2547549530863762, Final Batch Loss: 0.12130596488714218\n",
      "Epoch 4723, Loss: 0.2806285098195076, Final Batch Loss: 0.12245891243219376\n",
      "Epoch 4724, Loss: 0.2501938119530678, Final Batch Loss: 0.11009908467531204\n",
      "Epoch 4725, Loss: 0.26420119404792786, Final Batch Loss: 0.1709943562746048\n",
      "Epoch 4726, Loss: 0.30591924488544464, Final Batch Loss: 0.2030189484357834\n",
      "Epoch 4727, Loss: 0.2939867526292801, Final Batch Loss: 0.14413172006607056\n",
      "Epoch 4728, Loss: 0.21567294746637344, Final Batch Loss: 0.10039621591567993\n",
      "Epoch 4729, Loss: 0.29293930530548096, Final Batch Loss: 0.1334201693534851\n",
      "Epoch 4730, Loss: 0.2633948549628258, Final Batch Loss: 0.10204165428876877\n",
      "Epoch 4731, Loss: 0.28703583776950836, Final Batch Loss: 0.16631768643856049\n",
      "Epoch 4732, Loss: 0.23054387420415878, Final Batch Loss: 0.12052406370639801\n",
      "Epoch 4733, Loss: 0.20153582096099854, Final Batch Loss: 0.08984655886888504\n",
      "Epoch 4734, Loss: 0.3256930857896805, Final Batch Loss: 0.18280617892742157\n",
      "Epoch 4735, Loss: 0.2423860803246498, Final Batch Loss: 0.13878630101680756\n",
      "Epoch 4736, Loss: 0.29533281922340393, Final Batch Loss: 0.14069585502147675\n",
      "Epoch 4737, Loss: 0.20941831171512604, Final Batch Loss: 0.08684099465608597\n",
      "Epoch 4738, Loss: 0.27008237689733505, Final Batch Loss: 0.12492866069078445\n",
      "Epoch 4739, Loss: 0.266166128218174, Final Batch Loss: 0.16710156202316284\n",
      "Epoch 4740, Loss: 0.2796377167105675, Final Batch Loss: 0.12040027230978012\n",
      "Epoch 4741, Loss: 0.2644268795847893, Final Batch Loss: 0.14315365254878998\n",
      "Epoch 4742, Loss: 0.2541488856077194, Final Batch Loss: 0.1277182549238205\n",
      "Epoch 4743, Loss: 0.2622179687023163, Final Batch Loss: 0.1377839595079422\n",
      "Epoch 4744, Loss: 0.26987622678279877, Final Batch Loss: 0.12836281955242157\n",
      "Epoch 4745, Loss: 0.4088471829891205, Final Batch Loss: 0.2618415057659149\n",
      "Epoch 4746, Loss: 0.2855345755815506, Final Batch Loss: 0.14241908490657806\n",
      "Epoch 4747, Loss: 0.31203803420066833, Final Batch Loss: 0.18530842661857605\n",
      "Epoch 4748, Loss: 0.33589041233062744, Final Batch Loss: 0.2073136270046234\n",
      "Epoch 4749, Loss: 0.23931357264518738, Final Batch Loss: 0.11091385781764984\n",
      "Epoch 4750, Loss: 0.24427206069231033, Final Batch Loss: 0.09911913424730301\n",
      "Epoch 4751, Loss: 0.2876366674900055, Final Batch Loss: 0.1285092979669571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4752, Loss: 0.23391831666231155, Final Batch Loss: 0.10725107043981552\n",
      "Epoch 4753, Loss: 0.2413797527551651, Final Batch Loss: 0.06834663450717926\n",
      "Epoch 4754, Loss: 0.24910999834537506, Final Batch Loss: 0.12400482594966888\n",
      "Epoch 4755, Loss: 0.24998673051595688, Final Batch Loss: 0.13530948758125305\n",
      "Epoch 4756, Loss: 0.2742750197649002, Final Batch Loss: 0.1396322399377823\n",
      "Epoch 4757, Loss: 0.2880241423845291, Final Batch Loss: 0.1346760094165802\n",
      "Epoch 4758, Loss: 0.2760499566793442, Final Batch Loss: 0.1686081886291504\n",
      "Epoch 4759, Loss: 0.2778678983449936, Final Batch Loss: 0.1319492608308792\n",
      "Epoch 4760, Loss: 0.2600501477718353, Final Batch Loss: 0.10302591323852539\n",
      "Epoch 4761, Loss: 0.28955867886543274, Final Batch Loss: 0.16494335234165192\n",
      "Epoch 4762, Loss: 0.23326408118009567, Final Batch Loss: 0.12123018503189087\n",
      "Epoch 4763, Loss: 0.23719732463359833, Final Batch Loss: 0.11204126477241516\n",
      "Epoch 4764, Loss: 0.24299712479114532, Final Batch Loss: 0.13099108636379242\n",
      "Epoch 4765, Loss: 0.31122373044490814, Final Batch Loss: 0.1423955261707306\n",
      "Epoch 4766, Loss: 0.24845892935991287, Final Batch Loss: 0.1147303357720375\n",
      "Epoch 4767, Loss: 0.2827249616384506, Final Batch Loss: 0.1377815455198288\n",
      "Epoch 4768, Loss: 0.2979758679866791, Final Batch Loss: 0.16721779108047485\n",
      "Epoch 4769, Loss: 0.2646315395832062, Final Batch Loss: 0.12874889373779297\n",
      "Epoch 4770, Loss: 0.30223754048347473, Final Batch Loss: 0.164992094039917\n",
      "Epoch 4771, Loss: 0.22639763355255127, Final Batch Loss: 0.10866997390985489\n",
      "Epoch 4772, Loss: 0.3121981993317604, Final Batch Loss: 0.20488648116588593\n",
      "Epoch 4773, Loss: 0.22033873945474625, Final Batch Loss: 0.1042025163769722\n",
      "Epoch 4774, Loss: 0.3007824271917343, Final Batch Loss: 0.13395653665065765\n",
      "Epoch 4775, Loss: 0.24266699701547623, Final Batch Loss: 0.1107996329665184\n",
      "Epoch 4776, Loss: 0.27650075405836105, Final Batch Loss: 0.11172614246606827\n",
      "Epoch 4777, Loss: 0.2681342586874962, Final Batch Loss: 0.11952408403158188\n",
      "Epoch 4778, Loss: 0.3485962301492691, Final Batch Loss: 0.2431546002626419\n",
      "Epoch 4779, Loss: 0.22390662878751755, Final Batch Loss: 0.12274782359600067\n",
      "Epoch 4780, Loss: 0.2862200289964676, Final Batch Loss: 0.1501457542181015\n",
      "Epoch 4781, Loss: 0.3607029840350151, Final Batch Loss: 0.24598312377929688\n",
      "Epoch 4782, Loss: 0.31070324778556824, Final Batch Loss: 0.132760152220726\n",
      "Epoch 4783, Loss: 0.2829703241586685, Final Batch Loss: 0.15235596895217896\n",
      "Epoch 4784, Loss: 0.18695008754730225, Final Batch Loss: 0.08278873562812805\n",
      "Epoch 4785, Loss: 0.22870661318302155, Final Batch Loss: 0.13269087672233582\n",
      "Epoch 4786, Loss: 0.24650852382183075, Final Batch Loss: 0.10973924398422241\n",
      "Epoch 4787, Loss: 0.27089323848485947, Final Batch Loss: 0.161214679479599\n",
      "Epoch 4788, Loss: 0.2758282199501991, Final Batch Loss: 0.15503208339214325\n",
      "Epoch 4789, Loss: 0.21128643304109573, Final Batch Loss: 0.08311649411916733\n",
      "Epoch 4790, Loss: 0.2657354176044464, Final Batch Loss: 0.13810093700885773\n",
      "Epoch 4791, Loss: 0.24970336258411407, Final Batch Loss: 0.1309407651424408\n",
      "Epoch 4792, Loss: 0.24235034734010696, Final Batch Loss: 0.127545565366745\n",
      "Epoch 4793, Loss: 0.25920403003692627, Final Batch Loss: 0.1540745347738266\n",
      "Epoch 4794, Loss: 0.2250952199101448, Final Batch Loss: 0.10888782888650894\n",
      "Epoch 4795, Loss: 0.310886912047863, Final Batch Loss: 0.09773790091276169\n",
      "Epoch 4796, Loss: 0.23913665860891342, Final Batch Loss: 0.10981180518865585\n",
      "Epoch 4797, Loss: 0.3238738179206848, Final Batch Loss: 0.10812237858772278\n",
      "Epoch 4798, Loss: 0.23701852560043335, Final Batch Loss: 0.10820211470127106\n",
      "Epoch 4799, Loss: 0.26106443256139755, Final Batch Loss: 0.11265561729669571\n",
      "Epoch 4800, Loss: 0.3055700287222862, Final Batch Loss: 0.1954362839460373\n",
      "Epoch 4801, Loss: 0.2863400727510452, Final Batch Loss: 0.14808891713619232\n",
      "Epoch 4802, Loss: 0.2739513888955116, Final Batch Loss: 0.11867766827344894\n",
      "Epoch 4803, Loss: 0.25265178829431534, Final Batch Loss: 0.1545126885175705\n",
      "Epoch 4804, Loss: 0.21399109065532684, Final Batch Loss: 0.0853947103023529\n",
      "Epoch 4805, Loss: 0.21350541710853577, Final Batch Loss: 0.08787253499031067\n",
      "Epoch 4806, Loss: 0.2765847444534302, Final Batch Loss: 0.15047089755535126\n",
      "Epoch 4807, Loss: 0.31960196793079376, Final Batch Loss: 0.20154258608818054\n",
      "Epoch 4808, Loss: 0.3111833781003952, Final Batch Loss: 0.16734685003757477\n",
      "Epoch 4809, Loss: 0.21346678584814072, Final Batch Loss: 0.0815948024392128\n",
      "Epoch 4810, Loss: 0.23688634485006332, Final Batch Loss: 0.1312698870897293\n",
      "Epoch 4811, Loss: 0.22407515347003937, Final Batch Loss: 0.10102961957454681\n",
      "Epoch 4812, Loss: 0.23695067316293716, Final Batch Loss: 0.09882868081331253\n",
      "Epoch 4813, Loss: 0.2741640582680702, Final Batch Loss: 0.09702473133802414\n",
      "Epoch 4814, Loss: 0.20990824699401855, Final Batch Loss: 0.06986980140209198\n",
      "Epoch 4815, Loss: 0.2240130826830864, Final Batch Loss: 0.13849957287311554\n",
      "Epoch 4816, Loss: 0.240331768989563, Final Batch Loss: 0.13920730352401733\n",
      "Epoch 4817, Loss: 0.23204097896814346, Final Batch Loss: 0.11063294857740402\n",
      "Epoch 4818, Loss: 0.2504398450255394, Final Batch Loss: 0.09359725564718246\n",
      "Epoch 4819, Loss: 0.22221174836158752, Final Batch Loss: 0.12304315716028214\n",
      "Epoch 4820, Loss: 0.25435905903577805, Final Batch Loss: 0.1230393722653389\n",
      "Epoch 4821, Loss: 0.2884683907032013, Final Batch Loss: 0.13242527842521667\n",
      "Epoch 4822, Loss: 0.31826595962047577, Final Batch Loss: 0.17132247984409332\n",
      "Epoch 4823, Loss: 0.19700459390878677, Final Batch Loss: 0.1137637346982956\n",
      "Epoch 4824, Loss: 0.28994208574295044, Final Batch Loss: 0.10245200991630554\n",
      "Epoch 4825, Loss: 0.26732612401247025, Final Batch Loss: 0.11731288582086563\n",
      "Epoch 4826, Loss: 0.1912490874528885, Final Batch Loss: 0.08488909900188446\n",
      "Epoch 4827, Loss: 0.2558964639902115, Final Batch Loss: 0.12849529087543488\n",
      "Epoch 4828, Loss: 0.24821288883686066, Final Batch Loss: 0.11484532058238983\n",
      "Epoch 4829, Loss: 0.2850823923945427, Final Batch Loss: 0.1812431663274765\n",
      "Epoch 4830, Loss: 0.21744908392429352, Final Batch Loss: 0.0946817696094513\n",
      "Epoch 4831, Loss: 0.29881978034973145, Final Batch Loss: 0.13644646108150482\n",
      "Epoch 4832, Loss: 0.22244588285684586, Final Batch Loss: 0.10526695847511292\n",
      "Epoch 4833, Loss: 0.2848663330078125, Final Batch Loss: 0.16182485222816467\n",
      "Epoch 4834, Loss: 0.28323157131671906, Final Batch Loss: 0.14861531555652618\n",
      "Epoch 4835, Loss: 0.26806551218032837, Final Batch Loss: 0.13522066175937653\n",
      "Epoch 4836, Loss: 0.31781309843063354, Final Batch Loss: 0.15607018768787384\n",
      "Epoch 4837, Loss: 0.3002418130636215, Final Batch Loss: 0.1231352835893631\n",
      "Epoch 4838, Loss: 0.306025892496109, Final Batch Loss: 0.17978070676326752\n",
      "Epoch 4839, Loss: 0.2497693970799446, Final Batch Loss: 0.09023023396730423\n",
      "Epoch 4840, Loss: 0.2447279915213585, Final Batch Loss: 0.12942750751972198\n",
      "Epoch 4841, Loss: 0.26408177614212036, Final Batch Loss: 0.13513502478599548\n",
      "Epoch 4842, Loss: 0.22956009209156036, Final Batch Loss: 0.09248293936252594\n",
      "Epoch 4843, Loss: 0.2364073097705841, Final Batch Loss: 0.08065344393253326\n",
      "Epoch 4844, Loss: 0.24190129339694977, Final Batch Loss: 0.10321822762489319\n",
      "Epoch 4845, Loss: 0.2514822259545326, Final Batch Loss: 0.13250595331192017\n",
      "Epoch 4846, Loss: 0.21009892970323563, Final Batch Loss: 0.11909767240285873\n",
      "Epoch 4847, Loss: 0.29610081017017365, Final Batch Loss: 0.15772366523742676\n",
      "Epoch 4848, Loss: 0.1942998543381691, Final Batch Loss: 0.10207615792751312\n",
      "Epoch 4849, Loss: 0.2642133831977844, Final Batch Loss: 0.13448944687843323\n",
      "Epoch 4850, Loss: 0.2501275986433029, Final Batch Loss: 0.1096876710653305\n",
      "Epoch 4851, Loss: 0.26822812110185623, Final Batch Loss: 0.1221664771437645\n",
      "Epoch 4852, Loss: 0.2633722797036171, Final Batch Loss: 0.08383408933877945\n",
      "Epoch 4853, Loss: 0.20334988832473755, Final Batch Loss: 0.09571108222007751\n",
      "Epoch 4854, Loss: 0.21442709863185883, Final Batch Loss: 0.1067429706454277\n",
      "Epoch 4855, Loss: 0.20170792192220688, Final Batch Loss: 0.09236709028482437\n",
      "Epoch 4856, Loss: 0.22188405692577362, Final Batch Loss: 0.11245159059762955\n",
      "Epoch 4857, Loss: 0.2601461410522461, Final Batch Loss: 0.10470883548259735\n",
      "Epoch 4858, Loss: 0.2653310224413872, Final Batch Loss: 0.1561790108680725\n",
      "Epoch 4859, Loss: 0.27535396069288254, Final Batch Loss: 0.1124144122004509\n",
      "Epoch 4860, Loss: 0.2604497969150543, Final Batch Loss: 0.13061924278736115\n",
      "Epoch 4861, Loss: 0.23912616074085236, Final Batch Loss: 0.08819273114204407\n",
      "Epoch 4862, Loss: 0.21873078495264053, Final Batch Loss: 0.10795693844556808\n",
      "Epoch 4863, Loss: 0.21646717935800552, Final Batch Loss: 0.09735314548015594\n",
      "Epoch 4864, Loss: 0.24167533218860626, Final Batch Loss: 0.1285708248615265\n",
      "Epoch 4865, Loss: 0.21910417079925537, Final Batch Loss: 0.09819801896810532\n",
      "Epoch 4866, Loss: 0.22546451538801193, Final Batch Loss: 0.07916579395532608\n",
      "Epoch 4867, Loss: 0.23801222443580627, Final Batch Loss: 0.12038040161132812\n",
      "Epoch 4868, Loss: 0.23693010210990906, Final Batch Loss: 0.12248576432466507\n",
      "Epoch 4869, Loss: 0.2263462021946907, Final Batch Loss: 0.07969445735216141\n",
      "Epoch 4870, Loss: 0.20961470901966095, Final Batch Loss: 0.12702201306819916\n",
      "Epoch 4871, Loss: 0.2962222248315811, Final Batch Loss: 0.1584165245294571\n",
      "Epoch 4872, Loss: 0.26942161470651627, Final Batch Loss: 0.14969822764396667\n",
      "Epoch 4873, Loss: 0.26645371317863464, Final Batch Loss: 0.13679136335849762\n",
      "Epoch 4874, Loss: 0.2725818455219269, Final Batch Loss: 0.1553502380847931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4875, Loss: 0.2379748523235321, Final Batch Loss: 0.08487054705619812\n",
      "Epoch 4876, Loss: 0.21940472722053528, Final Batch Loss: 0.07686467468738556\n",
      "Epoch 4877, Loss: 0.2784995883703232, Final Batch Loss: 0.1452629417181015\n",
      "Epoch 4878, Loss: 0.23596389591693878, Final Batch Loss: 0.12260337918996811\n",
      "Epoch 4879, Loss: 0.26924092322587967, Final Batch Loss: 0.14860346913337708\n",
      "Epoch 4880, Loss: 0.28359922021627426, Final Batch Loss: 0.18739593029022217\n",
      "Epoch 4881, Loss: 0.20189018547534943, Final Batch Loss: 0.09767189621925354\n",
      "Epoch 4882, Loss: 0.2128836140036583, Final Batch Loss: 0.07322227209806442\n",
      "Epoch 4883, Loss: 0.23301928490400314, Final Batch Loss: 0.10111527889966965\n",
      "Epoch 4884, Loss: 0.21720115840435028, Final Batch Loss: 0.08708122372627258\n",
      "Epoch 4885, Loss: 0.2452443689107895, Final Batch Loss: 0.08419455587863922\n",
      "Epoch 4886, Loss: 0.1967778503894806, Final Batch Loss: 0.08174130320549011\n",
      "Epoch 4887, Loss: 0.2409558668732643, Final Batch Loss: 0.12070439755916595\n",
      "Epoch 4888, Loss: 0.2244897037744522, Final Batch Loss: 0.0932193249464035\n",
      "Epoch 4889, Loss: 0.27271582186222076, Final Batch Loss: 0.14208759367465973\n",
      "Epoch 4890, Loss: 0.29577894508838654, Final Batch Loss: 0.19603721797466278\n",
      "Epoch 4891, Loss: 0.23227843642234802, Final Batch Loss: 0.09755891561508179\n",
      "Epoch 4892, Loss: 0.28440433740615845, Final Batch Loss: 0.1569025069475174\n",
      "Epoch 4893, Loss: 0.27231427282094955, Final Batch Loss: 0.18776476383209229\n",
      "Epoch 4894, Loss: 0.2141420915722847, Final Batch Loss: 0.12383994460105896\n",
      "Epoch 4895, Loss: 0.2532079368829727, Final Batch Loss: 0.08808796107769012\n",
      "Epoch 4896, Loss: 0.21466203778982162, Final Batch Loss: 0.0930265486240387\n",
      "Epoch 4897, Loss: 0.20451658219099045, Final Batch Loss: 0.08038486540317535\n",
      "Epoch 4898, Loss: 0.3102002963423729, Final Batch Loss: 0.1224934384226799\n",
      "Epoch 4899, Loss: 0.2373429462313652, Final Batch Loss: 0.11866303533315659\n",
      "Epoch 4900, Loss: 0.27076585590839386, Final Batch Loss: 0.13842658698558807\n",
      "Epoch 4901, Loss: 0.2566182389855385, Final Batch Loss: 0.11868589371442795\n",
      "Epoch 4902, Loss: 0.2990427762269974, Final Batch Loss: 0.19460086524486542\n",
      "Epoch 4903, Loss: 0.22222357243299484, Final Batch Loss: 0.10115256905555725\n",
      "Epoch 4904, Loss: 0.24153438955545425, Final Batch Loss: 0.11085484176874161\n",
      "Epoch 4905, Loss: 0.2444264143705368, Final Batch Loss: 0.14540676772594452\n",
      "Epoch 4906, Loss: 0.2456737458705902, Final Batch Loss: 0.12008923292160034\n",
      "Epoch 4907, Loss: 0.3076583445072174, Final Batch Loss: 0.1686273068189621\n",
      "Epoch 4908, Loss: 0.22981934994459152, Final Batch Loss: 0.11871238797903061\n",
      "Epoch 4909, Loss: 0.25931239128112793, Final Batch Loss: 0.14118748903274536\n",
      "Epoch 4910, Loss: 0.243807815015316, Final Batch Loss: 0.0645185336470604\n",
      "Epoch 4911, Loss: 0.2940554618835449, Final Batch Loss: 0.1750134527683258\n",
      "Epoch 4912, Loss: 0.2069968432188034, Final Batch Loss: 0.08921422064304352\n",
      "Epoch 4913, Loss: 0.27650661021471024, Final Batch Loss: 0.1128445491194725\n",
      "Epoch 4914, Loss: 0.3014853745698929, Final Batch Loss: 0.15246666967868805\n",
      "Epoch 4915, Loss: 0.2664715498685837, Final Batch Loss: 0.11551791429519653\n",
      "Epoch 4916, Loss: 0.18800432235002518, Final Batch Loss: 0.09346908330917358\n",
      "Epoch 4917, Loss: 0.2306588962674141, Final Batch Loss: 0.12641990184783936\n",
      "Epoch 4918, Loss: 0.26315221935510635, Final Batch Loss: 0.11266989260911942\n",
      "Epoch 4919, Loss: 0.2507714480161667, Final Batch Loss: 0.12573854625225067\n",
      "Epoch 4920, Loss: 0.23125211894512177, Final Batch Loss: 0.11817944794893265\n",
      "Epoch 4921, Loss: 0.3169633597135544, Final Batch Loss: 0.1778445690870285\n",
      "Epoch 4922, Loss: 0.3148368149995804, Final Batch Loss: 0.1488809585571289\n",
      "Epoch 4923, Loss: 0.30620458722114563, Final Batch Loss: 0.15017175674438477\n",
      "Epoch 4924, Loss: 0.22000724077224731, Final Batch Loss: 0.11011187732219696\n",
      "Epoch 4925, Loss: 0.2969930171966553, Final Batch Loss: 0.16922836005687714\n",
      "Epoch 4926, Loss: 0.21320146322250366, Final Batch Loss: 0.08601130545139313\n",
      "Epoch 4927, Loss: 0.24519629776477814, Final Batch Loss: 0.13313619792461395\n",
      "Epoch 4928, Loss: 0.21603988856077194, Final Batch Loss: 0.12696805596351624\n",
      "Epoch 4929, Loss: 0.22475284337997437, Final Batch Loss: 0.10602939128875732\n",
      "Epoch 4930, Loss: 0.24961154162883759, Final Batch Loss: 0.11421214044094086\n",
      "Epoch 4931, Loss: 0.2853578105568886, Final Batch Loss: 0.1695106327533722\n",
      "Epoch 4932, Loss: 0.3410748541355133, Final Batch Loss: 0.20710808038711548\n",
      "Epoch 4933, Loss: 0.24373458325862885, Final Batch Loss: 0.12338283658027649\n",
      "Epoch 4934, Loss: 0.25399700552225113, Final Batch Loss: 0.13200604915618896\n",
      "Epoch 4935, Loss: 0.23728177696466446, Final Batch Loss: 0.12148108333349228\n",
      "Epoch 4936, Loss: 0.19641905277967453, Final Batch Loss: 0.062156595289707184\n",
      "Epoch 4937, Loss: 0.33166247606277466, Final Batch Loss: 0.21741530299186707\n",
      "Epoch 4938, Loss: 0.301162526011467, Final Batch Loss: 0.15829844772815704\n",
      "Epoch 4939, Loss: 0.21724367886781693, Final Batch Loss: 0.10900089889764786\n",
      "Epoch 4940, Loss: 0.3065198212862015, Final Batch Loss: 0.1730479598045349\n",
      "Epoch 4941, Loss: 0.3280342370271683, Final Batch Loss: 0.19394725561141968\n",
      "Epoch 4942, Loss: 0.2603885978460312, Final Batch Loss: 0.13309170305728912\n",
      "Epoch 4943, Loss: 0.27850405126810074, Final Batch Loss: 0.1228034719824791\n",
      "Epoch 4944, Loss: 0.2754054516553879, Final Batch Loss: 0.13757188618183136\n",
      "Epoch 4945, Loss: 0.21649958938360214, Final Batch Loss: 0.0793122872710228\n",
      "Epoch 4946, Loss: 0.25633687525987625, Final Batch Loss: 0.12125111371278763\n",
      "Epoch 4947, Loss: 0.2661598026752472, Final Batch Loss: 0.1379476636648178\n",
      "Epoch 4948, Loss: 0.2457500472664833, Final Batch Loss: 0.12799780070781708\n",
      "Epoch 4949, Loss: 0.2611752450466156, Final Batch Loss: 0.1016070693731308\n",
      "Epoch 4950, Loss: 0.19418597221374512, Final Batch Loss: 0.09346739202737808\n",
      "Epoch 4951, Loss: 0.2340424805879593, Final Batch Loss: 0.09947650134563446\n",
      "Epoch 4952, Loss: 0.2645554319024086, Final Batch Loss: 0.14477135241031647\n",
      "Epoch 4953, Loss: 0.3038576990365982, Final Batch Loss: 0.21153290569782257\n",
      "Epoch 4954, Loss: 0.2356828823685646, Final Batch Loss: 0.14671599864959717\n",
      "Epoch 4955, Loss: 0.2588798552751541, Final Batch Loss: 0.10696996748447418\n",
      "Epoch 4956, Loss: 0.2170620858669281, Final Batch Loss: 0.12917892634868622\n",
      "Epoch 4957, Loss: 0.20237616449594498, Final Batch Loss: 0.10847721993923187\n",
      "Epoch 4958, Loss: 0.22256918251514435, Final Batch Loss: 0.10448563098907471\n",
      "Epoch 4959, Loss: 0.2742112725973129, Final Batch Loss: 0.16308273375034332\n",
      "Epoch 4960, Loss: 0.2289261668920517, Final Batch Loss: 0.07458028197288513\n",
      "Epoch 4961, Loss: 0.3042902797460556, Final Batch Loss: 0.14280830323696136\n",
      "Epoch 4962, Loss: 0.22124501317739487, Final Batch Loss: 0.10661432892084122\n",
      "Epoch 4963, Loss: 0.23993372917175293, Final Batch Loss: 0.10472442209720612\n",
      "Epoch 4964, Loss: 0.2903800904750824, Final Batch Loss: 0.1518147885799408\n",
      "Epoch 4965, Loss: 0.21020053327083588, Final Batch Loss: 0.10338541865348816\n",
      "Epoch 4966, Loss: 0.27026014775037766, Final Batch Loss: 0.1800544112920761\n",
      "Epoch 4967, Loss: 0.23450548946857452, Final Batch Loss: 0.08713330328464508\n",
      "Epoch 4968, Loss: 0.19959460198879242, Final Batch Loss: 0.10948438942432404\n",
      "Epoch 4969, Loss: 0.22262440621852875, Final Batch Loss: 0.13883300125598907\n",
      "Epoch 4970, Loss: 0.23179946839809418, Final Batch Loss: 0.11203821003437042\n",
      "Epoch 4971, Loss: 0.28874388337135315, Final Batch Loss: 0.14065223932266235\n",
      "Epoch 4972, Loss: 0.25876684486866, Final Batch Loss: 0.11674520373344421\n",
      "Epoch 4973, Loss: 0.21663907915353775, Final Batch Loss: 0.09741344302892685\n",
      "Epoch 4974, Loss: 0.2321997731924057, Final Batch Loss: 0.09769953787326813\n",
      "Epoch 4975, Loss: 0.2646731436252594, Final Batch Loss: 0.17340339720249176\n",
      "Epoch 4976, Loss: 0.2407175526022911, Final Batch Loss: 0.12634341418743134\n",
      "Epoch 4977, Loss: 0.23677504807710648, Final Batch Loss: 0.09661594778299332\n",
      "Epoch 4978, Loss: 0.3603459596633911, Final Batch Loss: 0.22760558128356934\n",
      "Epoch 4979, Loss: 0.2158244252204895, Final Batch Loss: 0.10761888325214386\n",
      "Epoch 4980, Loss: 0.2528357356786728, Final Batch Loss: 0.1295842081308365\n",
      "Epoch 4981, Loss: 0.29091813415288925, Final Batch Loss: 0.16594603657722473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4982, Loss: 0.19325220584869385, Final Batch Loss: 0.09454800188541412\n",
      "Epoch 4983, Loss: 0.17383059859275818, Final Batch Loss: 0.0881182923913002\n",
      "Epoch 4984, Loss: 0.2427009865641594, Final Batch Loss: 0.10072367638349533\n",
      "Epoch 4985, Loss: 0.2325357049703598, Final Batch Loss: 0.13621850311756134\n",
      "Epoch 4986, Loss: 0.2838699221611023, Final Batch Loss: 0.15411975979804993\n",
      "Epoch 4987, Loss: 0.2473076656460762, Final Batch Loss: 0.12275644391775131\n",
      "Epoch 4988, Loss: 0.2797350138425827, Final Batch Loss: 0.1437150090932846\n",
      "Epoch 4989, Loss: 0.20456843823194504, Final Batch Loss: 0.10762491077184677\n",
      "Epoch 4990, Loss: 0.21720967441797256, Final Batch Loss: 0.07143130153417587\n",
      "Epoch 4991, Loss: 0.26669611781835556, Final Batch Loss: 0.11088188737630844\n",
      "Epoch 4992, Loss: 0.25098979473114014, Final Batch Loss: 0.10707665979862213\n",
      "Epoch 4993, Loss: 0.2279892861843109, Final Batch Loss: 0.14592181146144867\n",
      "Epoch 4994, Loss: 0.26798000931739807, Final Batch Loss: 0.12589074671268463\n",
      "Epoch 4995, Loss: 0.27353622019290924, Final Batch Loss: 0.14027592539787292\n",
      "Epoch 4996, Loss: 0.24185747653245926, Final Batch Loss: 0.11400703340768814\n",
      "Epoch 4997, Loss: 0.20826345682144165, Final Batch Loss: 0.07363821566104889\n",
      "Epoch 4998, Loss: 0.1882149577140808, Final Batch Loss: 0.068048395216465\n",
      "Epoch 4999, Loss: 0.2234412431716919, Final Batch Loss: 0.09605628252029419\n",
      "Epoch 5000, Loss: 0.27579157054424286, Final Batch Loss: 0.15664547681808472\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  1]\n",
      " [ 0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0]\n",
      " [ 0  0  1  0  0 11  0  0  1]\n",
      " [ 0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  1  0  0  0  0  0 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        14\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    0.84615   0.91667   0.88000        12\n",
      "           3    1.00000   1.00000   1.00000         7\n",
      "           4    1.00000   1.00000   1.00000         6\n",
      "           5    1.00000   0.84615   0.91667        13\n",
      "           6    1.00000   1.00000   1.00000        12\n",
      "           7    1.00000   1.00000   1.00000         9\n",
      "           8    0.87500   0.93333   0.90323        15\n",
      "\n",
      "    accuracy                        0.95876        97\n",
      "   macro avg    0.96902   0.96624   0.96665        97\n",
      "weighted avg    0.96164   0.95876   0.95902        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Group 3_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  3  0  0  5]\n",
      " [ 0  0  0 19  1  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  0  3  0]\n",
      " [ 0  0  4  0  0 15  0  0  1]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  2  0  0  3  0  0 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    1.00000   1.00000   1.00000        20\n",
      "         2.0    0.66667   0.60000   0.63158        20\n",
      "         3.0    1.00000   0.95000   0.97436        20\n",
      "         4.0    0.94444   0.85000   0.89474        20\n",
      "         5.0    0.71429   0.75000   0.73171        20\n",
      "         6.0    1.00000   1.00000   1.00000        20\n",
      "         7.0    0.86957   1.00000   0.93023        20\n",
      "         8.0    0.71429   0.75000   0.73171        20\n",
      "\n",
      "    accuracy                        0.87778       180\n",
      "   macro avg    0.87881   0.87778   0.87715       180\n",
      "weighted avg    0.87881   0.87778   0.87715       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
