{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A0 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_1 = gen(to_gen).detach().numpy()\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A1 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_2 = gen(to_gen).detach().numpy()\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A2 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_3 = gen(to_gen).detach().numpy()\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A0 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_4 = gen(to_gen).detach().numpy()\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A1 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_5 = gen(to_gen).detach().numpy()\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A2 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_6 = gen(to_gen).detach().numpy()\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A0 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_7 = gen(to_gen).detach().numpy()\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A1 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_8 = gen(to_gen).detach().numpy()\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A2 Excluded Group 2_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_9 = gen(to_gen).detach().numpy()\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_test = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_test = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [7, 8, 11]\n",
    "X_train, y_train = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.436090469360352, Final Batch Loss: 2.2120330333709717\n",
      "Epoch 2, Loss: 4.427571773529053, Final Batch Loss: 2.212995767593384\n",
      "Epoch 3, Loss: 4.429664373397827, Final Batch Loss: 2.219550371170044\n",
      "Epoch 4, Loss: 4.424816608428955, Final Batch Loss: 2.212001085281372\n",
      "Epoch 5, Loss: 4.416194915771484, Final Batch Loss: 2.1977732181549072\n",
      "Epoch 6, Loss: 4.4121458530426025, Final Batch Loss: 2.2031941413879395\n",
      "Epoch 7, Loss: 4.403523206710815, Final Batch Loss: 2.1976230144500732\n",
      "Epoch 8, Loss: 4.401529550552368, Final Batch Loss: 2.200727701187134\n",
      "Epoch 9, Loss: 4.399978399276733, Final Batch Loss: 2.2128751277923584\n",
      "Epoch 10, Loss: 4.38575005531311, Final Batch Loss: 2.1941640377044678\n",
      "Epoch 11, Loss: 4.375905752182007, Final Batch Loss: 2.190572500228882\n",
      "Epoch 12, Loss: 4.366783380508423, Final Batch Loss: 2.1805036067962646\n",
      "Epoch 13, Loss: 4.354376316070557, Final Batch Loss: 2.1786131858825684\n",
      "Epoch 14, Loss: 4.354064702987671, Final Batch Loss: 2.179180383682251\n",
      "Epoch 15, Loss: 4.338732957839966, Final Batch Loss: 2.1713411808013916\n",
      "Epoch 16, Loss: 4.31153678894043, Final Batch Loss: 2.1539623737335205\n",
      "Epoch 17, Loss: 4.296289443969727, Final Batch Loss: 2.1493723392486572\n",
      "Epoch 18, Loss: 4.2805094718933105, Final Batch Loss: 2.136922597885132\n",
      "Epoch 19, Loss: 4.256627798080444, Final Batch Loss: 2.118701696395874\n",
      "Epoch 20, Loss: 4.231312036514282, Final Batch Loss: 2.113433599472046\n",
      "Epoch 21, Loss: 4.204329013824463, Final Batch Loss: 2.0959556102752686\n",
      "Epoch 22, Loss: 4.1728105545043945, Final Batch Loss: 2.0806939601898193\n",
      "Epoch 23, Loss: 4.134597539901733, Final Batch Loss: 2.0481932163238525\n",
      "Epoch 24, Loss: 4.093055009841919, Final Batch Loss: 2.0263278484344482\n",
      "Epoch 25, Loss: 4.060379266738892, Final Batch Loss: 2.018498182296753\n",
      "Epoch 26, Loss: 4.006158232688904, Final Batch Loss: 1.9933034181594849\n",
      "Epoch 27, Loss: 3.963820695877075, Final Batch Loss: 1.978303074836731\n",
      "Epoch 28, Loss: 3.9266369342803955, Final Batch Loss: 1.9361810684204102\n",
      "Epoch 29, Loss: 3.8734978437423706, Final Batch Loss: 1.9293217658996582\n",
      "Epoch 30, Loss: 3.8353928327560425, Final Batch Loss: 1.9159111976623535\n",
      "Epoch 31, Loss: 3.7835938930511475, Final Batch Loss: 1.8751506805419922\n",
      "Epoch 32, Loss: 3.7037957906723022, Final Batch Loss: 1.8372174501419067\n",
      "Epoch 33, Loss: 3.6775057315826416, Final Batch Loss: 1.845422625541687\n",
      "Epoch 34, Loss: 3.6299766302108765, Final Batch Loss: 1.8156194686889648\n",
      "Epoch 35, Loss: 3.6003923416137695, Final Batch Loss: 1.8529351949691772\n",
      "Epoch 36, Loss: 3.5246047973632812, Final Batch Loss: 1.718082308769226\n",
      "Epoch 37, Loss: 3.488680362701416, Final Batch Loss: 1.7532458305358887\n",
      "Epoch 38, Loss: 3.427594780921936, Final Batch Loss: 1.6806901693344116\n",
      "Epoch 39, Loss: 3.4018112421035767, Final Batch Loss: 1.7013587951660156\n",
      "Epoch 40, Loss: 3.4196289777755737, Final Batch Loss: 1.7443829774856567\n",
      "Epoch 41, Loss: 3.281232237815857, Final Batch Loss: 1.6379791498184204\n",
      "Epoch 42, Loss: 3.2884703874588013, Final Batch Loss: 1.6713463068008423\n",
      "Epoch 43, Loss: 3.24497389793396, Final Batch Loss: 1.6414207220077515\n",
      "Epoch 44, Loss: 3.23275363445282, Final Batch Loss: 1.629153847694397\n",
      "Epoch 45, Loss: 3.224241256713867, Final Batch Loss: 1.6777464151382446\n",
      "Epoch 46, Loss: 3.174357056617737, Final Batch Loss: 1.5881909132003784\n",
      "Epoch 47, Loss: 3.079567313194275, Final Batch Loss: 1.5381289720535278\n",
      "Epoch 48, Loss: 3.0550386905670166, Final Batch Loss: 1.535579800605774\n",
      "Epoch 49, Loss: 3.082435965538025, Final Batch Loss: 1.5518361330032349\n",
      "Epoch 50, Loss: 3.046432375907898, Final Batch Loss: 1.5770539045333862\n",
      "Epoch 51, Loss: 2.965305209159851, Final Batch Loss: 1.4692974090576172\n",
      "Epoch 52, Loss: 2.9553165435791016, Final Batch Loss: 1.4508017301559448\n",
      "Epoch 53, Loss: 2.9143404960632324, Final Batch Loss: 1.4800611734390259\n",
      "Epoch 54, Loss: 2.9352219104766846, Final Batch Loss: 1.4826189279556274\n",
      "Epoch 55, Loss: 2.9295533895492554, Final Batch Loss: 1.4889675378799438\n",
      "Epoch 56, Loss: 2.941168427467346, Final Batch Loss: 1.4790550470352173\n",
      "Epoch 57, Loss: 2.898654818534851, Final Batch Loss: 1.486211895942688\n",
      "Epoch 58, Loss: 2.8138917684555054, Final Batch Loss: 1.395029902458191\n",
      "Epoch 59, Loss: 2.8613356351852417, Final Batch Loss: 1.4174160957336426\n",
      "Epoch 60, Loss: 2.8547277450561523, Final Batch Loss: 1.3965439796447754\n",
      "Epoch 61, Loss: 2.8016642332077026, Final Batch Loss: 1.4034799337387085\n",
      "Epoch 62, Loss: 2.773324131965637, Final Batch Loss: 1.362202763557434\n",
      "Epoch 63, Loss: 2.783549427986145, Final Batch Loss: 1.3824505805969238\n",
      "Epoch 64, Loss: 2.737097144126892, Final Batch Loss: 1.309850811958313\n",
      "Epoch 65, Loss: 2.754446029663086, Final Batch Loss: 1.3340626955032349\n",
      "Epoch 66, Loss: 2.657947301864624, Final Batch Loss: 1.3422199487686157\n",
      "Epoch 67, Loss: 2.718687653541565, Final Batch Loss: 1.3298230171203613\n",
      "Epoch 68, Loss: 2.7545902729034424, Final Batch Loss: 1.4143524169921875\n",
      "Epoch 69, Loss: 2.720703601837158, Final Batch Loss: 1.3937596082687378\n",
      "Epoch 70, Loss: 2.691275954246521, Final Batch Loss: 1.3985795974731445\n",
      "Epoch 71, Loss: 2.6638025045394897, Final Batch Loss: 1.3578333854675293\n",
      "Epoch 72, Loss: 2.6808393001556396, Final Batch Loss: 1.2786556482315063\n",
      "Epoch 73, Loss: 2.6465524435043335, Final Batch Loss: 1.3038877248764038\n",
      "Epoch 74, Loss: 2.61981999874115, Final Batch Loss: 1.3133665323257446\n",
      "Epoch 75, Loss: 2.5627881288528442, Final Batch Loss: 1.2789584398269653\n",
      "Epoch 76, Loss: 2.5940810441970825, Final Batch Loss: 1.2689716815948486\n",
      "Epoch 77, Loss: 2.503288984298706, Final Batch Loss: 1.2330724000930786\n",
      "Epoch 78, Loss: 2.623552441596985, Final Batch Loss: 1.2921228408813477\n",
      "Epoch 79, Loss: 2.590919613838196, Final Batch Loss: 1.2827502489089966\n",
      "Epoch 80, Loss: 2.518109917640686, Final Batch Loss: 1.2553669214248657\n",
      "Epoch 81, Loss: 2.503496766090393, Final Batch Loss: 1.2360187768936157\n",
      "Epoch 82, Loss: 2.4840089082717896, Final Batch Loss: 1.2797520160675049\n",
      "Epoch 83, Loss: 2.485582947731018, Final Batch Loss: 1.2834233045578003\n",
      "Epoch 84, Loss: 2.527117609977722, Final Batch Loss: 1.2562408447265625\n",
      "Epoch 85, Loss: 2.418605327606201, Final Batch Loss: 1.2375768423080444\n",
      "Epoch 86, Loss: 2.43909752368927, Final Batch Loss: 1.238071322441101\n",
      "Epoch 87, Loss: 2.4331276416778564, Final Batch Loss: 1.2495336532592773\n",
      "Epoch 88, Loss: 2.41689670085907, Final Batch Loss: 1.212230920791626\n",
      "Epoch 89, Loss: 2.388164758682251, Final Batch Loss: 1.1705254316329956\n",
      "Epoch 90, Loss: 2.4057663679122925, Final Batch Loss: 1.2236329317092896\n",
      "Epoch 91, Loss: 2.439878225326538, Final Batch Loss: 1.245071530342102\n",
      "Epoch 92, Loss: 2.3245816230773926, Final Batch Loss: 1.1625174283981323\n",
      "Epoch 93, Loss: 2.3177114725112915, Final Batch Loss: 1.1107233762741089\n",
      "Epoch 94, Loss: 2.350889563560486, Final Batch Loss: 1.143406867980957\n",
      "Epoch 95, Loss: 2.3413861989974976, Final Batch Loss: 1.1383123397827148\n",
      "Epoch 96, Loss: 2.242263913154602, Final Batch Loss: 1.070388913154602\n",
      "Epoch 97, Loss: 2.3272924423217773, Final Batch Loss: 1.1347765922546387\n",
      "Epoch 98, Loss: 2.3374985456466675, Final Batch Loss: 1.1597172021865845\n",
      "Epoch 99, Loss: 2.349853277206421, Final Batch Loss: 1.134924054145813\n",
      "Epoch 100, Loss: 2.2602142095565796, Final Batch Loss: 1.1510494947433472\n",
      "Epoch 101, Loss: 2.2539700269699097, Final Batch Loss: 1.1415921449661255\n",
      "Epoch 102, Loss: 2.2382006645202637, Final Batch Loss: 1.1319217681884766\n",
      "Epoch 103, Loss: 2.227365732192993, Final Batch Loss: 1.1061776876449585\n",
      "Epoch 104, Loss: 2.1782952547073364, Final Batch Loss: 1.0867477655410767\n",
      "Epoch 105, Loss: 2.171872138977051, Final Batch Loss: 1.0645021200180054\n",
      "Epoch 106, Loss: 2.1370118856430054, Final Batch Loss: 1.0110496282577515\n",
      "Epoch 107, Loss: 2.169476270675659, Final Batch Loss: 1.074256420135498\n",
      "Epoch 108, Loss: 2.2109687328338623, Final Batch Loss: 1.1285055875778198\n",
      "Epoch 109, Loss: 2.359007716178894, Final Batch Loss: 1.172497272491455\n",
      "Epoch 110, Loss: 2.1918855905532837, Final Batch Loss: 1.1016669273376465\n",
      "Epoch 111, Loss: 2.173653244972229, Final Batch Loss: 1.1248703002929688\n",
      "Epoch 112, Loss: 2.290132999420166, Final Batch Loss: 1.1649185419082642\n",
      "Epoch 113, Loss: 2.135379195213318, Final Batch Loss: 1.0711292028427124\n",
      "Epoch 114, Loss: 2.100159525871277, Final Batch Loss: 1.0035353899002075\n",
      "Epoch 115, Loss: 2.156057119369507, Final Batch Loss: 1.060460090637207\n",
      "Epoch 116, Loss: 2.148024797439575, Final Batch Loss: 1.0818976163864136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: 2.067794442176819, Final Batch Loss: 1.0469146966934204\n",
      "Epoch 118, Loss: 2.19669771194458, Final Batch Loss: 1.132976770401001\n",
      "Epoch 119, Loss: 2.0918298959732056, Final Batch Loss: 1.0353814363479614\n",
      "Epoch 120, Loss: 2.151216745376587, Final Batch Loss: 1.0733643770217896\n",
      "Epoch 121, Loss: 2.093443810939789, Final Batch Loss: 1.1132031679153442\n",
      "Epoch 122, Loss: 2.100745439529419, Final Batch Loss: 1.0356786251068115\n",
      "Epoch 123, Loss: 2.103292226791382, Final Batch Loss: 1.0179170370101929\n",
      "Epoch 124, Loss: 2.1298831701278687, Final Batch Loss: 1.0719382762908936\n",
      "Epoch 125, Loss: 1.9885579943656921, Final Batch Loss: 1.0052675008773804\n",
      "Epoch 126, Loss: 2.1439878940582275, Final Batch Loss: 1.091478943824768\n",
      "Epoch 127, Loss: 1.9871558547019958, Final Batch Loss: 1.0031118392944336\n",
      "Epoch 128, Loss: 2.1162445545196533, Final Batch Loss: 1.0823851823806763\n",
      "Epoch 129, Loss: 2.078346371650696, Final Batch Loss: 1.0224554538726807\n",
      "Epoch 130, Loss: 2.06830370426178, Final Batch Loss: 1.0113404989242554\n",
      "Epoch 131, Loss: 2.105245590209961, Final Batch Loss: 1.0589343309402466\n",
      "Epoch 132, Loss: 2.0256776809692383, Final Batch Loss: 1.0048834085464478\n",
      "Epoch 133, Loss: 2.0568060874938965, Final Batch Loss: 1.044677972793579\n",
      "Epoch 134, Loss: 2.047606348991394, Final Batch Loss: 1.0489591360092163\n",
      "Epoch 135, Loss: 2.1384743452072144, Final Batch Loss: 1.0785807371139526\n",
      "Epoch 136, Loss: 2.01604026556015, Final Batch Loss: 0.9653961062431335\n",
      "Epoch 137, Loss: 2.1257691383361816, Final Batch Loss: 1.0656287670135498\n",
      "Epoch 138, Loss: 1.9624913334846497, Final Batch Loss: 0.9974488615989685\n",
      "Epoch 139, Loss: 2.047833204269409, Final Batch Loss: 1.0358401536941528\n",
      "Epoch 140, Loss: 2.00247061252594, Final Batch Loss: 0.9986178874969482\n",
      "Epoch 141, Loss: 2.029365837574005, Final Batch Loss: 1.0561976432800293\n",
      "Epoch 142, Loss: 2.0008585453033447, Final Batch Loss: 0.959691047668457\n",
      "Epoch 143, Loss: 2.0250797867774963, Final Batch Loss: 0.9766835570335388\n",
      "Epoch 144, Loss: 2.0347973108291626, Final Batch Loss: 1.025138258934021\n",
      "Epoch 145, Loss: 1.9973896741867065, Final Batch Loss: 1.0063554048538208\n",
      "Epoch 146, Loss: 2.005548417568207, Final Batch Loss: 1.011662244796753\n",
      "Epoch 147, Loss: 1.9599937200546265, Final Batch Loss: 0.9944214820861816\n",
      "Epoch 148, Loss: 1.988292932510376, Final Batch Loss: 0.9905068278312683\n",
      "Epoch 149, Loss: 1.941964566707611, Final Batch Loss: 0.9812576174736023\n",
      "Epoch 150, Loss: 2.0449496507644653, Final Batch Loss: 1.0590966939926147\n",
      "Epoch 151, Loss: 2.008195459842682, Final Batch Loss: 0.9755213856697083\n",
      "Epoch 152, Loss: 1.931022822856903, Final Batch Loss: 0.9948384761810303\n",
      "Epoch 153, Loss: 1.9115437865257263, Final Batch Loss: 0.9781670570373535\n",
      "Epoch 154, Loss: 1.9057307243347168, Final Batch Loss: 0.9492992758750916\n",
      "Epoch 155, Loss: 1.9899162650108337, Final Batch Loss: 0.9890865683555603\n",
      "Epoch 156, Loss: 1.9194340109825134, Final Batch Loss: 0.9745174050331116\n",
      "Epoch 157, Loss: 1.905809223651886, Final Batch Loss: 0.8943663239479065\n",
      "Epoch 158, Loss: 1.9515745043754578, Final Batch Loss: 0.9793329238891602\n",
      "Epoch 159, Loss: 1.8876227140426636, Final Batch Loss: 0.9360146522521973\n",
      "Epoch 160, Loss: 1.8900543451309204, Final Batch Loss: 0.9665809273719788\n",
      "Epoch 161, Loss: 1.9527433514595032, Final Batch Loss: 0.970925509929657\n",
      "Epoch 162, Loss: 1.9079191088676453, Final Batch Loss: 1.01691472530365\n",
      "Epoch 163, Loss: 1.8821578621864319, Final Batch Loss: 0.9537628293037415\n",
      "Epoch 164, Loss: 1.9969350099563599, Final Batch Loss: 1.0042763948440552\n",
      "Epoch 165, Loss: 1.920638382434845, Final Batch Loss: 1.0157052278518677\n",
      "Epoch 166, Loss: 1.9115175604820251, Final Batch Loss: 0.9249492287635803\n",
      "Epoch 167, Loss: 1.850248098373413, Final Batch Loss: 0.963141143321991\n",
      "Epoch 168, Loss: 1.8768656253814697, Final Batch Loss: 0.9500799179077148\n",
      "Epoch 169, Loss: 1.8852377533912659, Final Batch Loss: 0.9364900588989258\n",
      "Epoch 170, Loss: 1.8604133129119873, Final Batch Loss: 0.916723906993866\n",
      "Epoch 171, Loss: 1.9016250967979431, Final Batch Loss: 0.9801549911499023\n",
      "Epoch 172, Loss: 1.8476113080978394, Final Batch Loss: 0.8925065994262695\n",
      "Epoch 173, Loss: 1.8016314506530762, Final Batch Loss: 0.8930814862251282\n",
      "Epoch 174, Loss: 1.8959973454475403, Final Batch Loss: 0.934837818145752\n",
      "Epoch 175, Loss: 1.8618100881576538, Final Batch Loss: 0.90274578332901\n",
      "Epoch 176, Loss: 1.8446377515792847, Final Batch Loss: 0.9160225987434387\n",
      "Epoch 177, Loss: 1.824432134628296, Final Batch Loss: 0.9601180553436279\n",
      "Epoch 178, Loss: 1.854905605316162, Final Batch Loss: 0.9270171523094177\n",
      "Epoch 179, Loss: 1.832075834274292, Final Batch Loss: 0.9089500308036804\n",
      "Epoch 180, Loss: 1.8423694968223572, Final Batch Loss: 0.9333685040473938\n",
      "Epoch 181, Loss: 1.8753762245178223, Final Batch Loss: 0.9498640894889832\n",
      "Epoch 182, Loss: 1.8722774982452393, Final Batch Loss: 0.9130709767341614\n",
      "Epoch 183, Loss: 1.8106992840766907, Final Batch Loss: 0.9340460300445557\n",
      "Epoch 184, Loss: 1.790689468383789, Final Batch Loss: 0.8582561612129211\n",
      "Epoch 185, Loss: 1.7987807989120483, Final Batch Loss: 0.8870668411254883\n",
      "Epoch 186, Loss: 1.8421971797943115, Final Batch Loss: 0.8842055201530457\n",
      "Epoch 187, Loss: 1.8973663449287415, Final Batch Loss: 0.966381847858429\n",
      "Epoch 188, Loss: 1.8022844791412354, Final Batch Loss: 0.8954082131385803\n",
      "Epoch 189, Loss: 1.8827460408210754, Final Batch Loss: 0.946241557598114\n",
      "Epoch 190, Loss: 1.8301346898078918, Final Batch Loss: 0.9617640376091003\n",
      "Epoch 191, Loss: 1.7704476714134216, Final Batch Loss: 0.8475080132484436\n",
      "Epoch 192, Loss: 1.814251184463501, Final Batch Loss: 0.8758825659751892\n",
      "Epoch 193, Loss: 1.7945836186408997, Final Batch Loss: 0.8820429444313049\n",
      "Epoch 194, Loss: 1.7089017033576965, Final Batch Loss: 0.8257269263267517\n",
      "Epoch 195, Loss: 1.749824583530426, Final Batch Loss: 0.8582462668418884\n",
      "Epoch 196, Loss: 1.7568115592002869, Final Batch Loss: 0.8898124098777771\n",
      "Epoch 197, Loss: 1.8483954668045044, Final Batch Loss: 0.9514231085777283\n",
      "Epoch 198, Loss: 1.7882000207901, Final Batch Loss: 0.8859019875526428\n",
      "Epoch 199, Loss: 1.722446084022522, Final Batch Loss: 0.8475642204284668\n",
      "Epoch 200, Loss: 1.7622110247612, Final Batch Loss: 0.8771462440490723\n",
      "Epoch 201, Loss: 1.7533939480781555, Final Batch Loss: 0.8634548783302307\n",
      "Epoch 202, Loss: 1.7049601674079895, Final Batch Loss: 0.8343603610992432\n",
      "Epoch 203, Loss: 1.7268388867378235, Final Batch Loss: 0.8796975612640381\n",
      "Epoch 204, Loss: 1.7607431411743164, Final Batch Loss: 0.8625423312187195\n",
      "Epoch 205, Loss: 1.7142773866653442, Final Batch Loss: 0.8973702788352966\n",
      "Epoch 206, Loss: 1.7634207010269165, Final Batch Loss: 0.865496814250946\n",
      "Epoch 207, Loss: 1.6807399988174438, Final Batch Loss: 0.7836549282073975\n",
      "Epoch 208, Loss: 1.7238686680793762, Final Batch Loss: 0.8725919127464294\n",
      "Epoch 209, Loss: 1.6853405237197876, Final Batch Loss: 0.8367516994476318\n",
      "Epoch 210, Loss: 1.7103139162063599, Final Batch Loss: 0.8518385887145996\n",
      "Epoch 211, Loss: 1.702733039855957, Final Batch Loss: 0.8378579020500183\n",
      "Epoch 212, Loss: 1.7191096544265747, Final Batch Loss: 0.888521134853363\n",
      "Epoch 213, Loss: 1.6785803437232971, Final Batch Loss: 0.8230127692222595\n",
      "Epoch 214, Loss: 1.67866051197052, Final Batch Loss: 0.855344295501709\n",
      "Epoch 215, Loss: 1.6950823664665222, Final Batch Loss: 0.8494985103607178\n",
      "Epoch 216, Loss: 1.7273316979408264, Final Batch Loss: 0.89138263463974\n",
      "Epoch 217, Loss: 1.6830834746360779, Final Batch Loss: 0.8439977169036865\n",
      "Epoch 218, Loss: 1.69593346118927, Final Batch Loss: 0.8671122193336487\n",
      "Epoch 219, Loss: 1.5995835065841675, Final Batch Loss: 0.7839792370796204\n",
      "Epoch 220, Loss: 1.652978539466858, Final Batch Loss: 0.7969881892204285\n",
      "Epoch 221, Loss: 1.6352996230125427, Final Batch Loss: 0.8508546948432922\n",
      "Epoch 222, Loss: 1.6523890495300293, Final Batch Loss: 0.8003175258636475\n",
      "Epoch 223, Loss: 1.6285748481750488, Final Batch Loss: 0.8081294894218445\n",
      "Epoch 224, Loss: 1.6972892880439758, Final Batch Loss: 0.8661910891532898\n",
      "Epoch 225, Loss: 1.7135274410247803, Final Batch Loss: 0.8428316116333008\n",
      "Epoch 226, Loss: 1.548281192779541, Final Batch Loss: 0.7683181166648865\n",
      "Epoch 227, Loss: 1.638869285583496, Final Batch Loss: 0.7985034584999084\n",
      "Epoch 228, Loss: 1.5810953974723816, Final Batch Loss: 0.7670455574989319\n",
      "Epoch 229, Loss: 1.6363576650619507, Final Batch Loss: 0.8267902731895447\n",
      "Epoch 230, Loss: 1.6418614983558655, Final Batch Loss: 0.8341963887214661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231, Loss: 1.6452081203460693, Final Batch Loss: 0.7670490741729736\n",
      "Epoch 232, Loss: 1.5616865754127502, Final Batch Loss: 0.7840590476989746\n",
      "Epoch 233, Loss: 1.6264053583145142, Final Batch Loss: 0.8239237666130066\n",
      "Epoch 234, Loss: 1.6420702934265137, Final Batch Loss: 0.8541669249534607\n",
      "Epoch 235, Loss: 1.5841270685195923, Final Batch Loss: 0.8190181255340576\n",
      "Epoch 236, Loss: 1.6186336874961853, Final Batch Loss: 0.7853304743766785\n",
      "Epoch 237, Loss: 1.6050902009010315, Final Batch Loss: 0.8032535910606384\n",
      "Epoch 238, Loss: 1.5996180176734924, Final Batch Loss: 0.8049423694610596\n",
      "Epoch 239, Loss: 1.648067831993103, Final Batch Loss: 0.8052943348884583\n",
      "Epoch 240, Loss: 1.5816535949707031, Final Batch Loss: 0.7445734143257141\n",
      "Epoch 241, Loss: 1.5830900073051453, Final Batch Loss: 0.7921497225761414\n",
      "Epoch 242, Loss: 1.6396347284317017, Final Batch Loss: 0.850023090839386\n",
      "Epoch 243, Loss: 1.617004632949829, Final Batch Loss: 0.7895238995552063\n",
      "Epoch 244, Loss: 1.5344092845916748, Final Batch Loss: 0.7459545135498047\n",
      "Epoch 245, Loss: 1.4977216720581055, Final Batch Loss: 0.7388856410980225\n",
      "Epoch 246, Loss: 1.5433049201965332, Final Batch Loss: 0.7264097332954407\n",
      "Epoch 247, Loss: 1.588629126548767, Final Batch Loss: 0.7833473086357117\n",
      "Epoch 248, Loss: 1.53378826379776, Final Batch Loss: 0.7642488479614258\n",
      "Epoch 249, Loss: 1.4871119260787964, Final Batch Loss: 0.7795788645744324\n",
      "Epoch 250, Loss: 1.5926716327667236, Final Batch Loss: 0.7398281693458557\n",
      "Epoch 251, Loss: 1.544539988040924, Final Batch Loss: 0.7568614482879639\n",
      "Epoch 252, Loss: 1.5270133018493652, Final Batch Loss: 0.8000585436820984\n",
      "Epoch 253, Loss: 1.4855424761772156, Final Batch Loss: 0.7045082449913025\n",
      "Epoch 254, Loss: 1.5367444157600403, Final Batch Loss: 0.7924293875694275\n",
      "Epoch 255, Loss: 1.527766466140747, Final Batch Loss: 0.758728563785553\n",
      "Epoch 256, Loss: 1.5697543025016785, Final Batch Loss: 0.7769820690155029\n",
      "Epoch 257, Loss: 1.5807854533195496, Final Batch Loss: 0.808255672454834\n",
      "Epoch 258, Loss: 1.449979543685913, Final Batch Loss: 0.762929379940033\n",
      "Epoch 259, Loss: 1.5086583495140076, Final Batch Loss: 0.7528683543205261\n",
      "Epoch 260, Loss: 1.456821620464325, Final Batch Loss: 0.712651789188385\n",
      "Epoch 261, Loss: 1.5569430589675903, Final Batch Loss: 0.7689176201820374\n",
      "Epoch 262, Loss: 1.5413049459457397, Final Batch Loss: 0.7857077121734619\n",
      "Epoch 263, Loss: 1.4765287041664124, Final Batch Loss: 0.7512345910072327\n",
      "Epoch 264, Loss: 1.434076726436615, Final Batch Loss: 0.7003470063209534\n",
      "Epoch 265, Loss: 1.4691367149353027, Final Batch Loss: 0.7635316848754883\n",
      "Epoch 266, Loss: 1.4531813263893127, Final Batch Loss: 0.737373411655426\n",
      "Epoch 267, Loss: 1.46363365650177, Final Batch Loss: 0.7774608731269836\n",
      "Epoch 268, Loss: 1.4802173972129822, Final Batch Loss: 0.7493306994438171\n",
      "Epoch 269, Loss: 1.4133551120758057, Final Batch Loss: 0.7027988433837891\n",
      "Epoch 270, Loss: 1.4574100971221924, Final Batch Loss: 0.7542782425880432\n",
      "Epoch 271, Loss: 1.446330487728119, Final Batch Loss: 0.7406876087188721\n",
      "Epoch 272, Loss: 1.4296788573265076, Final Batch Loss: 0.7145641446113586\n",
      "Epoch 273, Loss: 1.36271071434021, Final Batch Loss: 0.6856009364128113\n",
      "Epoch 274, Loss: 1.4813458323478699, Final Batch Loss: 0.7480928897857666\n",
      "Epoch 275, Loss: 1.3983982801437378, Final Batch Loss: 0.6885539889335632\n",
      "Epoch 276, Loss: 1.4323933124542236, Final Batch Loss: 0.7120351791381836\n",
      "Epoch 277, Loss: 1.403861403465271, Final Batch Loss: 0.690803587436676\n",
      "Epoch 278, Loss: 1.4274279475212097, Final Batch Loss: 0.7256462574005127\n",
      "Epoch 279, Loss: 1.4408168196678162, Final Batch Loss: 0.7487266063690186\n",
      "Epoch 280, Loss: 1.4441075921058655, Final Batch Loss: 0.7351388335227966\n",
      "Epoch 281, Loss: 1.4094522595405579, Final Batch Loss: 0.7037195563316345\n",
      "Epoch 282, Loss: 1.415587604045868, Final Batch Loss: 0.712249219417572\n",
      "Epoch 283, Loss: 1.4349647760391235, Final Batch Loss: 0.7504251003265381\n",
      "Epoch 284, Loss: 1.3967572450637817, Final Batch Loss: 0.6838474273681641\n",
      "Epoch 285, Loss: 1.4113356471061707, Final Batch Loss: 0.7254151701927185\n",
      "Epoch 286, Loss: 1.418289601802826, Final Batch Loss: 0.6829920411109924\n",
      "Epoch 287, Loss: 1.3706199526786804, Final Batch Loss: 0.654485821723938\n",
      "Epoch 288, Loss: 1.359257161617279, Final Batch Loss: 0.7034767270088196\n",
      "Epoch 289, Loss: 1.364296317100525, Final Batch Loss: 0.6560357809066772\n",
      "Epoch 290, Loss: 1.3503230810165405, Final Batch Loss: 0.6345278024673462\n",
      "Epoch 291, Loss: 1.4044181108474731, Final Batch Loss: 0.7100051045417786\n",
      "Epoch 292, Loss: 1.4198222756385803, Final Batch Loss: 0.7084712386131287\n",
      "Epoch 293, Loss: 1.351152777671814, Final Batch Loss: 0.7024877667427063\n",
      "Epoch 294, Loss: 1.3430904150009155, Final Batch Loss: 0.666326105594635\n",
      "Epoch 295, Loss: 1.2707146406173706, Final Batch Loss: 0.6333299875259399\n",
      "Epoch 296, Loss: 1.3068094849586487, Final Batch Loss: 0.651474118232727\n",
      "Epoch 297, Loss: 1.2980721592903137, Final Batch Loss: 0.6480383276939392\n",
      "Epoch 298, Loss: 1.3196309804916382, Final Batch Loss: 0.6712617874145508\n",
      "Epoch 299, Loss: 1.3204132914543152, Final Batch Loss: 0.6772775053977966\n",
      "Epoch 300, Loss: 1.2654090523719788, Final Batch Loss: 0.6416866779327393\n",
      "Epoch 301, Loss: 1.3164656162261963, Final Batch Loss: 0.6045467853546143\n",
      "Epoch 302, Loss: 1.2567753195762634, Final Batch Loss: 0.632093608379364\n",
      "Epoch 303, Loss: 1.3306245803833008, Final Batch Loss: 0.6619776487350464\n",
      "Epoch 304, Loss: 1.2810354232788086, Final Batch Loss: 0.6186875104904175\n",
      "Epoch 305, Loss: 1.2340993881225586, Final Batch Loss: 0.6057114005088806\n",
      "Epoch 306, Loss: 1.2602087259292603, Final Batch Loss: 0.6377906799316406\n",
      "Epoch 307, Loss: 1.2384341359138489, Final Batch Loss: 0.6065952181816101\n",
      "Epoch 308, Loss: 1.2660439610481262, Final Batch Loss: 0.6119896769523621\n",
      "Epoch 309, Loss: 1.2634727954864502, Final Batch Loss: 0.6499703526496887\n",
      "Epoch 310, Loss: 1.2334911823272705, Final Batch Loss: 0.6433573365211487\n",
      "Epoch 311, Loss: 1.310665786266327, Final Batch Loss: 0.6982744336128235\n",
      "Epoch 312, Loss: 1.2444297075271606, Final Batch Loss: 0.6123930811882019\n",
      "Epoch 313, Loss: 1.2099609971046448, Final Batch Loss: 0.6038698554039001\n",
      "Epoch 314, Loss: 1.2576650977134705, Final Batch Loss: 0.6263659596443176\n",
      "Epoch 315, Loss: 1.305160403251648, Final Batch Loss: 0.649315595626831\n",
      "Epoch 316, Loss: 1.2339211106300354, Final Batch Loss: 0.6111673712730408\n",
      "Epoch 317, Loss: 1.2789405584335327, Final Batch Loss: 0.6479154229164124\n",
      "Epoch 318, Loss: 1.2423452734947205, Final Batch Loss: 0.6106362342834473\n",
      "Epoch 319, Loss: 1.2494628429412842, Final Batch Loss: 0.6420908570289612\n",
      "Epoch 320, Loss: 1.2285000085830688, Final Batch Loss: 0.5488699078559875\n",
      "Epoch 321, Loss: 1.18564373254776, Final Batch Loss: 0.5742958188056946\n",
      "Epoch 322, Loss: 1.3045607209205627, Final Batch Loss: 0.6688210964202881\n",
      "Epoch 323, Loss: 1.245650589466095, Final Batch Loss: 0.6570276618003845\n",
      "Epoch 324, Loss: 1.2543144226074219, Final Batch Loss: 0.6587063074111938\n",
      "Epoch 325, Loss: 1.2869171500205994, Final Batch Loss: 0.6257049441337585\n",
      "Epoch 326, Loss: 1.2548668384552002, Final Batch Loss: 0.617198646068573\n",
      "Epoch 327, Loss: 1.2553330659866333, Final Batch Loss: 0.6004434823989868\n",
      "Epoch 328, Loss: 1.2007752060890198, Final Batch Loss: 0.5899222493171692\n",
      "Epoch 329, Loss: 1.2387767434120178, Final Batch Loss: 0.6248969435691833\n",
      "Epoch 330, Loss: 1.2171791195869446, Final Batch Loss: 0.5875267386436462\n",
      "Epoch 331, Loss: 1.2394237518310547, Final Batch Loss: 0.5914673209190369\n",
      "Epoch 332, Loss: 1.2721142768859863, Final Batch Loss: 0.7064139246940613\n",
      "Epoch 333, Loss: 1.1793996691703796, Final Batch Loss: 0.5716424584388733\n",
      "Epoch 334, Loss: 1.2514866590499878, Final Batch Loss: 0.6379277110099792\n",
      "Epoch 335, Loss: 1.2634283900260925, Final Batch Loss: 0.6863405108451843\n",
      "Epoch 336, Loss: 1.2149584889411926, Final Batch Loss: 0.5358274579048157\n",
      "Epoch 337, Loss: 1.18499094247818, Final Batch Loss: 0.5975738763809204\n",
      "Epoch 338, Loss: 1.2196999192237854, Final Batch Loss: 0.5871145129203796\n",
      "Epoch 339, Loss: 1.1940472722053528, Final Batch Loss: 0.5872931480407715\n",
      "Epoch 340, Loss: 1.1858294606208801, Final Batch Loss: 0.5803138017654419\n",
      "Epoch 341, Loss: 1.2110145092010498, Final Batch Loss: 0.6046745181083679\n",
      "Epoch 342, Loss: 1.1988952159881592, Final Batch Loss: 0.5883104205131531\n",
      "Epoch 343, Loss: 1.281030535697937, Final Batch Loss: 0.6273495554924011\n",
      "Epoch 344, Loss: 1.2255714535713196, Final Batch Loss: 0.6255237460136414\n",
      "Epoch 345, Loss: 1.2066171169281006, Final Batch Loss: 0.6135675311088562\n",
      "Epoch 346, Loss: 1.235937237739563, Final Batch Loss: 0.6082064509391785\n",
      "Epoch 347, Loss: 1.1877018213272095, Final Batch Loss: 0.5716767311096191\n",
      "Epoch 348, Loss: 1.2090985774993896, Final Batch Loss: 0.6237795352935791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349, Loss: 1.186995804309845, Final Batch Loss: 0.5950477123260498\n",
      "Epoch 350, Loss: 1.179959774017334, Final Batch Loss: 0.6376290917396545\n",
      "Epoch 351, Loss: 1.2061750888824463, Final Batch Loss: 0.6100188493728638\n",
      "Epoch 352, Loss: 1.1362679600715637, Final Batch Loss: 0.503858208656311\n",
      "Epoch 353, Loss: 1.190595030784607, Final Batch Loss: 0.5793878436088562\n",
      "Epoch 354, Loss: 1.1611624360084534, Final Batch Loss: 0.5276127457618713\n",
      "Epoch 355, Loss: 1.1854472756385803, Final Batch Loss: 0.6338863968849182\n",
      "Epoch 356, Loss: 1.1407707333564758, Final Batch Loss: 0.582481324672699\n",
      "Epoch 357, Loss: 1.173663854598999, Final Batch Loss: 0.5977367758750916\n",
      "Epoch 358, Loss: 1.1583911180496216, Final Batch Loss: 0.5800211429595947\n",
      "Epoch 359, Loss: 1.1195116639137268, Final Batch Loss: 0.5852553248405457\n",
      "Epoch 360, Loss: 1.1988269686698914, Final Batch Loss: 0.6384184956550598\n",
      "Epoch 361, Loss: 1.1203550100326538, Final Batch Loss: 0.5419582724571228\n",
      "Epoch 362, Loss: 1.1200926303863525, Final Batch Loss: 0.5969352126121521\n",
      "Epoch 363, Loss: 1.1339673399925232, Final Batch Loss: 0.6185103058815002\n",
      "Epoch 364, Loss: 1.1021226644515991, Final Batch Loss: 0.564294159412384\n",
      "Epoch 365, Loss: 1.1900924444198608, Final Batch Loss: 0.5753539204597473\n",
      "Epoch 366, Loss: 1.138215571641922, Final Batch Loss: 0.4983474314212799\n",
      "Epoch 367, Loss: 1.0645042061805725, Final Batch Loss: 0.5389333367347717\n",
      "Epoch 368, Loss: 1.1508785486221313, Final Batch Loss: 0.525296151638031\n",
      "Epoch 369, Loss: 1.1199646592140198, Final Batch Loss: 0.5793236494064331\n",
      "Epoch 370, Loss: 1.1837538480758667, Final Batch Loss: 0.586494505405426\n",
      "Epoch 371, Loss: 1.1230844855308533, Final Batch Loss: 0.5535606145858765\n",
      "Epoch 372, Loss: 1.11984521150589, Final Batch Loss: 0.5275673270225525\n",
      "Epoch 373, Loss: 1.142874300479889, Final Batch Loss: 0.6332051157951355\n",
      "Epoch 374, Loss: 1.1729510426521301, Final Batch Loss: 0.5988902449607849\n",
      "Epoch 375, Loss: 1.1590903997421265, Final Batch Loss: 0.5369341969490051\n",
      "Epoch 376, Loss: 1.1636613011360168, Final Batch Loss: 0.5264496207237244\n",
      "Epoch 377, Loss: 1.1440781354904175, Final Batch Loss: 0.5171223878860474\n",
      "Epoch 378, Loss: 1.08061021566391, Final Batch Loss: 0.5242282152175903\n",
      "Epoch 379, Loss: 1.098967969417572, Final Batch Loss: 0.569205105304718\n",
      "Epoch 380, Loss: 1.1156949996948242, Final Batch Loss: 0.5311008095741272\n",
      "Epoch 381, Loss: 1.1355785727500916, Final Batch Loss: 0.559387743473053\n",
      "Epoch 382, Loss: 1.1367571353912354, Final Batch Loss: 0.5595819354057312\n",
      "Epoch 383, Loss: 1.1556474566459656, Final Batch Loss: 0.5780713558197021\n",
      "Epoch 384, Loss: 1.1067750453948975, Final Batch Loss: 0.5390128493309021\n",
      "Epoch 385, Loss: 1.1418128609657288, Final Batch Loss: 0.5692980885505676\n",
      "Epoch 386, Loss: 1.137771487236023, Final Batch Loss: 0.5760005116462708\n",
      "Epoch 387, Loss: 1.1458138823509216, Final Batch Loss: 0.6066275835037231\n",
      "Epoch 388, Loss: 1.125230073928833, Final Batch Loss: 0.5894116759300232\n",
      "Epoch 389, Loss: 1.0836445689201355, Final Batch Loss: 0.5697347521781921\n",
      "Epoch 390, Loss: 1.1144109964370728, Final Batch Loss: 0.5633921027183533\n",
      "Epoch 391, Loss: 1.1913013458251953, Final Batch Loss: 0.6201359629631042\n",
      "Epoch 392, Loss: 1.123564600944519, Final Batch Loss: 0.5600995421409607\n",
      "Epoch 393, Loss: 1.1175012588500977, Final Batch Loss: 0.5489366054534912\n",
      "Epoch 394, Loss: 1.089047133922577, Final Batch Loss: 0.5868122577667236\n",
      "Epoch 395, Loss: 1.0298328697681427, Final Batch Loss: 0.48771682381629944\n",
      "Epoch 396, Loss: 1.12930828332901, Final Batch Loss: 0.5510143637657166\n",
      "Epoch 397, Loss: 1.0849757194519043, Final Batch Loss: 0.5068309903144836\n",
      "Epoch 398, Loss: 1.0859851837158203, Final Batch Loss: 0.5093082785606384\n",
      "Epoch 399, Loss: 1.1120468974113464, Final Batch Loss: 0.5240423083305359\n",
      "Epoch 400, Loss: 1.0897345542907715, Final Batch Loss: 0.5441641807556152\n",
      "Epoch 401, Loss: 1.1230904459953308, Final Batch Loss: 0.5203043818473816\n",
      "Epoch 402, Loss: 1.0735739469528198, Final Batch Loss: 0.5372818112373352\n",
      "Epoch 403, Loss: 1.0588079690933228, Final Batch Loss: 0.5328064560890198\n",
      "Epoch 404, Loss: 1.0775647163391113, Final Batch Loss: 0.6115013957023621\n",
      "Epoch 405, Loss: 1.0467482805252075, Final Batch Loss: 0.5588237643241882\n",
      "Epoch 406, Loss: 1.0492965579032898, Final Batch Loss: 0.5518187880516052\n",
      "Epoch 407, Loss: 1.0933054089546204, Final Batch Loss: 0.5781669020652771\n",
      "Epoch 408, Loss: 1.036685585975647, Final Batch Loss: 0.48705506324768066\n",
      "Epoch 409, Loss: 1.066285490989685, Final Batch Loss: 0.557219386100769\n",
      "Epoch 410, Loss: 1.0789253115653992, Final Batch Loss: 0.545537531375885\n",
      "Epoch 411, Loss: 1.0956857204437256, Final Batch Loss: 0.5609617829322815\n",
      "Epoch 412, Loss: 1.0565376281738281, Final Batch Loss: 0.5376352071762085\n",
      "Epoch 413, Loss: 1.0499752461910248, Final Batch Loss: 0.469020813703537\n",
      "Epoch 414, Loss: 1.056116133928299, Final Batch Loss: 0.6082897782325745\n",
      "Epoch 415, Loss: 1.0430774092674255, Final Batch Loss: 0.5165506601333618\n",
      "Epoch 416, Loss: 1.0329829156398773, Final Batch Loss: 0.4930973947048187\n",
      "Epoch 417, Loss: 1.0771369934082031, Final Batch Loss: 0.5826206803321838\n",
      "Epoch 418, Loss: 1.0198177099227905, Final Batch Loss: 0.514146625995636\n",
      "Epoch 419, Loss: 1.1188737750053406, Final Batch Loss: 0.5395652055740356\n",
      "Epoch 420, Loss: 1.0167340636253357, Final Batch Loss: 0.5262677669525146\n",
      "Epoch 421, Loss: 1.0503821969032288, Final Batch Loss: 0.5233783721923828\n",
      "Epoch 422, Loss: 1.0104577541351318, Final Batch Loss: 0.4721873998641968\n",
      "Epoch 423, Loss: 1.0911828875541687, Final Batch Loss: 0.5284953713417053\n",
      "Epoch 424, Loss: 1.044866919517517, Final Batch Loss: 0.5294773578643799\n",
      "Epoch 425, Loss: 1.0549674034118652, Final Batch Loss: 0.5407025814056396\n",
      "Epoch 426, Loss: 1.1465574502944946, Final Batch Loss: 0.516038179397583\n",
      "Epoch 427, Loss: 1.0441402196884155, Final Batch Loss: 0.5189942717552185\n",
      "Epoch 428, Loss: 0.996804267168045, Final Batch Loss: 0.5073723196983337\n",
      "Epoch 429, Loss: 1.0995374917984009, Final Batch Loss: 0.5736896395683289\n",
      "Epoch 430, Loss: 1.0242112874984741, Final Batch Loss: 0.516104519367218\n",
      "Epoch 431, Loss: 1.1051865220069885, Final Batch Loss: 0.5451286435127258\n",
      "Epoch 432, Loss: 1.0189848244190216, Final Batch Loss: 0.5204583406448364\n",
      "Epoch 433, Loss: 1.074072778224945, Final Batch Loss: 0.5242586135864258\n",
      "Epoch 434, Loss: 1.0023626387119293, Final Batch Loss: 0.437135249376297\n",
      "Epoch 435, Loss: 1.0480572283267975, Final Batch Loss: 0.4968765676021576\n",
      "Epoch 436, Loss: 1.0043684542179108, Final Batch Loss: 0.48307302594184875\n",
      "Epoch 437, Loss: 1.0621445178985596, Final Batch Loss: 0.546334981918335\n",
      "Epoch 438, Loss: 1.0145663619041443, Final Batch Loss: 0.4812229871749878\n",
      "Epoch 439, Loss: 1.0296612977981567, Final Batch Loss: 0.4898085594177246\n",
      "Epoch 440, Loss: 1.0961698293685913, Final Batch Loss: 0.5779587626457214\n",
      "Epoch 441, Loss: 1.030408501625061, Final Batch Loss: 0.5174137949943542\n",
      "Epoch 442, Loss: 1.0688408017158508, Final Batch Loss: 0.5495555996894836\n",
      "Epoch 443, Loss: 1.0839486718177795, Final Batch Loss: 0.5147070288658142\n",
      "Epoch 444, Loss: 1.027329683303833, Final Batch Loss: 0.5074819922447205\n",
      "Epoch 445, Loss: 1.0612940192222595, Final Batch Loss: 0.5260182619094849\n",
      "Epoch 446, Loss: 1.0655911564826965, Final Batch Loss: 0.5554522275924683\n",
      "Epoch 447, Loss: 0.9967111051082611, Final Batch Loss: 0.4894099533557892\n",
      "Epoch 448, Loss: 1.0325974524021149, Final Batch Loss: 0.490724116563797\n",
      "Epoch 449, Loss: 1.0338642597198486, Final Batch Loss: 0.5432955622673035\n",
      "Epoch 450, Loss: 1.0278593003749847, Final Batch Loss: 0.47479578852653503\n",
      "Epoch 451, Loss: 1.0725658535957336, Final Batch Loss: 0.5444952249526978\n",
      "Epoch 452, Loss: 0.9754921197891235, Final Batch Loss: 0.4765331745147705\n",
      "Epoch 453, Loss: 1.0789272785186768, Final Batch Loss: 0.5362818241119385\n",
      "Epoch 454, Loss: 1.0405878126621246, Final Batch Loss: 0.5414040088653564\n",
      "Epoch 455, Loss: 1.0251622796058655, Final Batch Loss: 0.5214223265647888\n",
      "Epoch 456, Loss: 1.040965884923935, Final Batch Loss: 0.5810178518295288\n",
      "Epoch 457, Loss: 0.977702796459198, Final Batch Loss: 0.4926767647266388\n",
      "Epoch 458, Loss: 1.0189957320690155, Final Batch Loss: 0.566440224647522\n",
      "Epoch 459, Loss: 1.0192132890224457, Final Batch Loss: 0.47287264466285706\n",
      "Epoch 460, Loss: 1.01829332113266, Final Batch Loss: 0.5144742131233215\n",
      "Epoch 461, Loss: 1.0424732863903046, Final Batch Loss: 0.5531691908836365\n",
      "Epoch 462, Loss: 1.063139796257019, Final Batch Loss: 0.5438739657402039\n",
      "Epoch 463, Loss: 1.060000479221344, Final Batch Loss: 0.5252229571342468\n",
      "Epoch 464, Loss: 0.9647477269172668, Final Batch Loss: 0.501987099647522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465, Loss: 1.0026798844337463, Final Batch Loss: 0.47366881370544434\n",
      "Epoch 466, Loss: 1.0507892370224, Final Batch Loss: 0.5534523129463196\n",
      "Epoch 467, Loss: 1.0140421986579895, Final Batch Loss: 0.5003054141998291\n",
      "Epoch 468, Loss: 1.041752576828003, Final Batch Loss: 0.5056628584861755\n",
      "Epoch 469, Loss: 1.027500718832016, Final Batch Loss: 0.46458300948143005\n",
      "Epoch 470, Loss: 1.0339704155921936, Final Batch Loss: 0.5049787759780884\n",
      "Epoch 471, Loss: 1.0389414429664612, Final Batch Loss: 0.5635736584663391\n",
      "Epoch 472, Loss: 1.0296432673931122, Final Batch Loss: 0.5354838967323303\n",
      "Epoch 473, Loss: 1.0243191123008728, Final Batch Loss: 0.5101379752159119\n",
      "Epoch 474, Loss: 1.0259760022163391, Final Batch Loss: 0.5437419414520264\n",
      "Epoch 475, Loss: 1.0233914852142334, Final Batch Loss: 0.5088970065116882\n",
      "Epoch 476, Loss: 1.025527536869049, Final Batch Loss: 0.5064022541046143\n",
      "Epoch 477, Loss: 1.0444583892822266, Final Batch Loss: 0.5243211984634399\n",
      "Epoch 478, Loss: 0.9354425370693207, Final Batch Loss: 0.4453616142272949\n",
      "Epoch 479, Loss: 1.0347175896167755, Final Batch Loss: 0.47961077094078064\n",
      "Epoch 480, Loss: 0.9436465799808502, Final Batch Loss: 0.4663144052028656\n",
      "Epoch 481, Loss: 1.03790682554245, Final Batch Loss: 0.5218058228492737\n",
      "Epoch 482, Loss: 0.9927306175231934, Final Batch Loss: 0.5032841563224792\n",
      "Epoch 483, Loss: 0.9924726486206055, Final Batch Loss: 0.4811199903488159\n",
      "Epoch 484, Loss: 1.0701632499694824, Final Batch Loss: 0.515519917011261\n",
      "Epoch 485, Loss: 0.9998000860214233, Final Batch Loss: 0.5262776017189026\n",
      "Epoch 486, Loss: 0.9876687824726105, Final Batch Loss: 0.4904308617115021\n",
      "Epoch 487, Loss: 0.9732016026973724, Final Batch Loss: 0.4932555854320526\n",
      "Epoch 488, Loss: 1.0460809171199799, Final Batch Loss: 0.5728133320808411\n",
      "Epoch 489, Loss: 1.016660362482071, Final Batch Loss: 0.4915362298488617\n",
      "Epoch 490, Loss: 1.0010167360305786, Final Batch Loss: 0.5225403904914856\n",
      "Epoch 491, Loss: 1.0560128688812256, Final Batch Loss: 0.5469294190406799\n",
      "Epoch 492, Loss: 1.0210403203964233, Final Batch Loss: 0.5241521000862122\n",
      "Epoch 493, Loss: 1.0645385384559631, Final Batch Loss: 0.5521383881568909\n",
      "Epoch 494, Loss: 0.9933678805828094, Final Batch Loss: 0.47380003333091736\n",
      "Epoch 495, Loss: 1.0197881162166595, Final Batch Loss: 0.5321406126022339\n",
      "Epoch 496, Loss: 0.9599618315696716, Final Batch Loss: 0.48093006014823914\n",
      "Epoch 497, Loss: 0.9930585622787476, Final Batch Loss: 0.498689740896225\n",
      "Epoch 498, Loss: 0.9938141405582428, Final Batch Loss: 0.5209526419639587\n",
      "Epoch 499, Loss: 1.0186873376369476, Final Batch Loss: 0.4800918996334076\n",
      "Epoch 500, Loss: 0.9626526534557343, Final Batch Loss: 0.487034410238266\n",
      "Epoch 501, Loss: 1.002946525812149, Final Batch Loss: 0.4614328444004059\n",
      "Epoch 502, Loss: 0.9547882378101349, Final Batch Loss: 0.49257469177246094\n",
      "Epoch 503, Loss: 0.9285137057304382, Final Batch Loss: 0.449288010597229\n",
      "Epoch 504, Loss: 1.0305961072444916, Final Batch Loss: 0.5675671100616455\n",
      "Epoch 505, Loss: 0.9449647963047028, Final Batch Loss: 0.46866726875305176\n",
      "Epoch 506, Loss: 0.9902631938457489, Final Batch Loss: 0.476957768201828\n",
      "Epoch 507, Loss: 1.0672691464424133, Final Batch Loss: 0.5242748856544495\n",
      "Epoch 508, Loss: 0.9957282543182373, Final Batch Loss: 0.49633851647377014\n",
      "Epoch 509, Loss: 0.9761819839477539, Final Batch Loss: 0.4026297330856323\n",
      "Epoch 510, Loss: 1.0065121948719025, Final Batch Loss: 0.4944835603237152\n",
      "Epoch 511, Loss: 0.9961857795715332, Final Batch Loss: 0.5123822689056396\n",
      "Epoch 512, Loss: 1.0029579401016235, Final Batch Loss: 0.5382307767868042\n",
      "Epoch 513, Loss: 0.9846081435680389, Final Batch Loss: 0.5200599431991577\n",
      "Epoch 514, Loss: 1.0172011852264404, Final Batch Loss: 0.5200081467628479\n",
      "Epoch 515, Loss: 0.9823529720306396, Final Batch Loss: 0.4862217903137207\n",
      "Epoch 516, Loss: 0.9280363619327545, Final Batch Loss: 0.4514566659927368\n",
      "Epoch 517, Loss: 0.9548012614250183, Final Batch Loss: 0.48528340458869934\n",
      "Epoch 518, Loss: 0.9350992441177368, Final Batch Loss: 0.4644818603992462\n",
      "Epoch 519, Loss: 0.9698978066444397, Final Batch Loss: 0.4745119512081146\n",
      "Epoch 520, Loss: 0.9659808874130249, Final Batch Loss: 0.5022035241127014\n",
      "Epoch 521, Loss: 0.9147328734397888, Final Batch Loss: 0.43463560938835144\n",
      "Epoch 522, Loss: 1.0055349171161652, Final Batch Loss: 0.5317828059196472\n",
      "Epoch 523, Loss: 0.944121390581131, Final Batch Loss: 0.45603886246681213\n",
      "Epoch 524, Loss: 0.9961271584033966, Final Batch Loss: 0.5464481711387634\n",
      "Epoch 525, Loss: 0.973968118429184, Final Batch Loss: 0.5004581809043884\n",
      "Epoch 526, Loss: 0.9304117858409882, Final Batch Loss: 0.44747868180274963\n",
      "Epoch 527, Loss: 0.961518794298172, Final Batch Loss: 0.4565240442752838\n",
      "Epoch 528, Loss: 0.9551271200180054, Final Batch Loss: 0.46078261733055115\n",
      "Epoch 529, Loss: 0.9818100035190582, Final Batch Loss: 0.5357827544212341\n",
      "Epoch 530, Loss: 0.9284245669841766, Final Batch Loss: 0.4610966742038727\n",
      "Epoch 531, Loss: 0.952121764421463, Final Batch Loss: 0.4718230664730072\n",
      "Epoch 532, Loss: 0.9254691004753113, Final Batch Loss: 0.442780464887619\n",
      "Epoch 533, Loss: 1.0291367173194885, Final Batch Loss: 0.5798493027687073\n",
      "Epoch 534, Loss: 0.987884134054184, Final Batch Loss: 0.509711503982544\n",
      "Epoch 535, Loss: 0.9820200800895691, Final Batch Loss: 0.5193279385566711\n",
      "Epoch 536, Loss: 0.964589387178421, Final Batch Loss: 0.48325681686401367\n",
      "Epoch 537, Loss: 0.9694351851940155, Final Batch Loss: 0.5057597160339355\n",
      "Epoch 538, Loss: 0.8676362037658691, Final Batch Loss: 0.4174861013889313\n",
      "Epoch 539, Loss: 0.9836572110652924, Final Batch Loss: 0.5272369980812073\n",
      "Epoch 540, Loss: 1.0407302379608154, Final Batch Loss: 0.543164074420929\n",
      "Epoch 541, Loss: 0.9379721879959106, Final Batch Loss: 0.49033573269844055\n",
      "Epoch 542, Loss: 0.9831553995609283, Final Batch Loss: 0.5224541425704956\n",
      "Epoch 543, Loss: 0.9743992984294891, Final Batch Loss: 0.4675299823284149\n",
      "Epoch 544, Loss: 1.0340233743190765, Final Batch Loss: 0.4942786395549774\n",
      "Epoch 545, Loss: 0.9434311985969543, Final Batch Loss: 0.4558691084384918\n",
      "Epoch 546, Loss: 0.8827167451381683, Final Batch Loss: 0.4513837993144989\n",
      "Epoch 547, Loss: 0.9432801902294159, Final Batch Loss: 0.49271246790885925\n",
      "Epoch 548, Loss: 0.9745170772075653, Final Batch Loss: 0.4993119537830353\n",
      "Epoch 549, Loss: 0.964902251958847, Final Batch Loss: 0.4874666929244995\n",
      "Epoch 550, Loss: 1.0260460674762726, Final Batch Loss: 0.5370379090309143\n",
      "Epoch 551, Loss: 0.9605361223220825, Final Batch Loss: 0.47524961829185486\n",
      "Epoch 552, Loss: 0.9190531969070435, Final Batch Loss: 0.5295111536979675\n",
      "Epoch 553, Loss: 0.9446487426757812, Final Batch Loss: 0.43756818771362305\n",
      "Epoch 554, Loss: 0.9897937774658203, Final Batch Loss: 0.5428628325462341\n",
      "Epoch 555, Loss: 0.931605339050293, Final Batch Loss: 0.5073258280754089\n",
      "Epoch 556, Loss: 0.9453147649765015, Final Batch Loss: 0.4813317060470581\n",
      "Epoch 557, Loss: 0.9266310036182404, Final Batch Loss: 0.4582411050796509\n",
      "Epoch 558, Loss: 0.9350223541259766, Final Batch Loss: 0.47246623039245605\n",
      "Epoch 559, Loss: 0.9553776383399963, Final Batch Loss: 0.48202529549598694\n",
      "Epoch 560, Loss: 0.8962015807628632, Final Batch Loss: 0.43890121579170227\n",
      "Epoch 561, Loss: 0.9434153139591217, Final Batch Loss: 0.4794633090496063\n",
      "Epoch 562, Loss: 0.9016712307929993, Final Batch Loss: 0.45268014073371887\n",
      "Epoch 563, Loss: 0.9434343576431274, Final Batch Loss: 0.4589078724384308\n",
      "Epoch 564, Loss: 0.9633646607398987, Final Batch Loss: 0.48200735449790955\n",
      "Epoch 565, Loss: 0.9705349802970886, Final Batch Loss: 0.5148672461509705\n",
      "Epoch 566, Loss: 0.937263548374176, Final Batch Loss: 0.4360470771789551\n",
      "Epoch 567, Loss: 1.0116395354270935, Final Batch Loss: 0.5328184962272644\n",
      "Epoch 568, Loss: 0.984402984380722, Final Batch Loss: 0.5000748634338379\n",
      "Epoch 569, Loss: 1.0076699554920197, Final Batch Loss: 0.4624328911304474\n",
      "Epoch 570, Loss: 0.9541438221931458, Final Batch Loss: 0.48229122161865234\n",
      "Epoch 571, Loss: 0.9196694791316986, Final Batch Loss: 0.4495711028575897\n",
      "Epoch 572, Loss: 0.9210948348045349, Final Batch Loss: 0.4414914548397064\n",
      "Epoch 573, Loss: 0.9647867977619171, Final Batch Loss: 0.461577445268631\n",
      "Epoch 574, Loss: 0.9038108885288239, Final Batch Loss: 0.49450555443763733\n",
      "Epoch 575, Loss: 0.9532142281532288, Final Batch Loss: 0.48020267486572266\n",
      "Epoch 576, Loss: 0.9225389659404755, Final Batch Loss: 0.4263363778591156\n",
      "Epoch 577, Loss: 0.9418407380580902, Final Batch Loss: 0.4599572420120239\n",
      "Epoch 578, Loss: 0.8961790204048157, Final Batch Loss: 0.4510941803455353\n",
      "Epoch 579, Loss: 0.8872987329959869, Final Batch Loss: 0.4532392919063568\n",
      "Epoch 580, Loss: 0.9084888100624084, Final Batch Loss: 0.4747334420681\n",
      "Epoch 581, Loss: 0.9460282325744629, Final Batch Loss: 0.45953547954559326\n",
      "Epoch 582, Loss: 0.9388791620731354, Final Batch Loss: 0.42074164748191833\n",
      "Epoch 583, Loss: 1.0374608933925629, Final Batch Loss: 0.5714700818061829\n",
      "Epoch 584, Loss: 0.8904728889465332, Final Batch Loss: 0.4396381080150604\n",
      "Epoch 585, Loss: 0.900118499994278, Final Batch Loss: 0.4465614855289459\n",
      "Epoch 586, Loss: 0.9005270600318909, Final Batch Loss: 0.4447740316390991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 587, Loss: 0.9169407486915588, Final Batch Loss: 0.48416611552238464\n",
      "Epoch 588, Loss: 0.997590959072113, Final Batch Loss: 0.5202838182449341\n",
      "Epoch 589, Loss: 0.9310596287250519, Final Batch Loss: 0.48224952816963196\n",
      "Epoch 590, Loss: 0.9078428447246552, Final Batch Loss: 0.44541898369789124\n",
      "Epoch 591, Loss: 0.9573708474636078, Final Batch Loss: 0.5418530702590942\n",
      "Epoch 592, Loss: 0.9492815434932709, Final Batch Loss: 0.4951373338699341\n",
      "Epoch 593, Loss: 0.858778178691864, Final Batch Loss: 0.3919684588909149\n",
      "Epoch 594, Loss: 0.8586288690567017, Final Batch Loss: 0.4385218918323517\n",
      "Epoch 595, Loss: 0.9323646128177643, Final Batch Loss: 0.4320169985294342\n",
      "Epoch 596, Loss: 0.8660846054553986, Final Batch Loss: 0.38118693232536316\n",
      "Epoch 597, Loss: 0.9413498342037201, Final Batch Loss: 0.4932080805301666\n",
      "Epoch 598, Loss: 0.9164747595787048, Final Batch Loss: 0.4809826910495758\n",
      "Epoch 599, Loss: 0.8996889293193817, Final Batch Loss: 0.4297824800014496\n",
      "Epoch 600, Loss: 0.8832015693187714, Final Batch Loss: 0.40051504969596863\n",
      "Epoch 601, Loss: 0.91436567902565, Final Batch Loss: 0.507634162902832\n",
      "Epoch 602, Loss: 1.080883264541626, Final Batch Loss: 0.5592561960220337\n",
      "Epoch 603, Loss: 0.9296998083591461, Final Batch Loss: 0.4721730947494507\n",
      "Epoch 604, Loss: 0.9102484583854675, Final Batch Loss: 0.4417484700679779\n",
      "Epoch 605, Loss: 0.8510703444480896, Final Batch Loss: 0.3859391212463379\n",
      "Epoch 606, Loss: 0.9134699404239655, Final Batch Loss: 0.4566739797592163\n",
      "Epoch 607, Loss: 0.8978672325611115, Final Batch Loss: 0.44182249903678894\n",
      "Epoch 608, Loss: 0.8709053993225098, Final Batch Loss: 0.46881476044654846\n",
      "Epoch 609, Loss: 0.9737387895584106, Final Batch Loss: 0.4517475366592407\n",
      "Epoch 610, Loss: 0.9499130547046661, Final Batch Loss: 0.4794715344905853\n",
      "Epoch 611, Loss: 0.8883652687072754, Final Batch Loss: 0.3924836218357086\n",
      "Epoch 612, Loss: 0.9286368191242218, Final Batch Loss: 0.46955275535583496\n",
      "Epoch 613, Loss: 0.8816700577735901, Final Batch Loss: 0.45049455761909485\n",
      "Epoch 614, Loss: 0.8769312798976898, Final Batch Loss: 0.4619002044200897\n",
      "Epoch 615, Loss: 0.9853098690509796, Final Batch Loss: 0.5244761109352112\n",
      "Epoch 616, Loss: 0.8970401585102081, Final Batch Loss: 0.48828980326652527\n",
      "Epoch 617, Loss: 0.9599352180957794, Final Batch Loss: 0.4860096275806427\n",
      "Epoch 618, Loss: 0.979526937007904, Final Batch Loss: 0.5004953145980835\n",
      "Epoch 619, Loss: 0.974339634180069, Final Batch Loss: 0.47987744212150574\n",
      "Epoch 620, Loss: 0.9016976356506348, Final Batch Loss: 0.4742330014705658\n",
      "Epoch 621, Loss: 0.8740915060043335, Final Batch Loss: 0.4824749529361725\n",
      "Epoch 622, Loss: 0.9493042826652527, Final Batch Loss: 0.5270959734916687\n",
      "Epoch 623, Loss: 0.9575912654399872, Final Batch Loss: 0.4801034927368164\n",
      "Epoch 624, Loss: 0.8840697407722473, Final Batch Loss: 0.44411131739616394\n",
      "Epoch 625, Loss: 0.8892414569854736, Final Batch Loss: 0.4546860158443451\n",
      "Epoch 626, Loss: 0.8896851539611816, Final Batch Loss: 0.4414876699447632\n",
      "Epoch 627, Loss: 0.9013168513774872, Final Batch Loss: 0.4887371361255646\n",
      "Epoch 628, Loss: 0.8722892999649048, Final Batch Loss: 0.433774471282959\n",
      "Epoch 629, Loss: 0.8891531527042389, Final Batch Loss: 0.4255988895893097\n",
      "Epoch 630, Loss: 0.9294043481349945, Final Batch Loss: 0.5032535195350647\n",
      "Epoch 631, Loss: 0.8789174854755402, Final Batch Loss: 0.5182225704193115\n",
      "Epoch 632, Loss: 0.8621105849742889, Final Batch Loss: 0.43637314438819885\n",
      "Epoch 633, Loss: 0.9411680102348328, Final Batch Loss: 0.445128470659256\n",
      "Epoch 634, Loss: 0.9508289098739624, Final Batch Loss: 0.46503081917762756\n",
      "Epoch 635, Loss: 0.9176071286201477, Final Batch Loss: 0.447335809469223\n",
      "Epoch 636, Loss: 0.9794215857982635, Final Batch Loss: 0.4914937913417816\n",
      "Epoch 637, Loss: 0.9110395312309265, Final Batch Loss: 0.44218456745147705\n",
      "Epoch 638, Loss: 0.8761281967163086, Final Batch Loss: 0.4560950994491577\n",
      "Epoch 639, Loss: 0.8651356399059296, Final Batch Loss: 0.45783236622810364\n",
      "Epoch 640, Loss: 0.8782586455345154, Final Batch Loss: 0.47285377979278564\n",
      "Epoch 641, Loss: 0.9311414361000061, Final Batch Loss: 0.4625858962535858\n",
      "Epoch 642, Loss: 0.8362469375133514, Final Batch Loss: 0.39472857117652893\n",
      "Epoch 643, Loss: 0.9032063484191895, Final Batch Loss: 0.4735364615917206\n",
      "Epoch 644, Loss: 0.863253265619278, Final Batch Loss: 0.43898117542266846\n",
      "Epoch 645, Loss: 0.8762966096401215, Final Batch Loss: 0.4656813442707062\n",
      "Epoch 646, Loss: 0.8938276469707489, Final Batch Loss: 0.4799078702926636\n",
      "Epoch 647, Loss: 0.9019380211830139, Final Batch Loss: 0.45128896832466125\n",
      "Epoch 648, Loss: 0.9258243143558502, Final Batch Loss: 0.48532870411872864\n",
      "Epoch 649, Loss: 0.9024793207645416, Final Batch Loss: 0.47073593735694885\n",
      "Epoch 650, Loss: 0.8919855058193207, Final Batch Loss: 0.4106047451496124\n",
      "Epoch 651, Loss: 0.8880963027477264, Final Batch Loss: 0.44660255312919617\n",
      "Epoch 652, Loss: 0.8739862740039825, Final Batch Loss: 0.38906368613243103\n",
      "Epoch 653, Loss: 0.883682906627655, Final Batch Loss: 0.45450475811958313\n",
      "Epoch 654, Loss: 0.8386162519454956, Final Batch Loss: 0.4059947729110718\n",
      "Epoch 655, Loss: 0.9005800187587738, Final Batch Loss: 0.4677429497241974\n",
      "Epoch 656, Loss: 0.9048364162445068, Final Batch Loss: 0.4833172857761383\n",
      "Epoch 657, Loss: 0.8775134086608887, Final Batch Loss: 0.4224259853363037\n",
      "Epoch 658, Loss: 0.898186594247818, Final Batch Loss: 0.4712507426738739\n",
      "Epoch 659, Loss: 0.8935905694961548, Final Batch Loss: 0.48172128200531006\n",
      "Epoch 660, Loss: 0.879711389541626, Final Batch Loss: 0.4879879951477051\n",
      "Epoch 661, Loss: 0.9018657803535461, Final Batch Loss: 0.42305371165275574\n",
      "Epoch 662, Loss: 0.8946674168109894, Final Batch Loss: 0.48899564146995544\n",
      "Epoch 663, Loss: 0.8474757373332977, Final Batch Loss: 0.45300519466400146\n",
      "Epoch 664, Loss: 0.8898411989212036, Final Batch Loss: 0.5029065012931824\n",
      "Epoch 665, Loss: 0.9015931487083435, Final Batch Loss: 0.5180702805519104\n",
      "Epoch 666, Loss: 0.8760869801044464, Final Batch Loss: 0.4068361520767212\n",
      "Epoch 667, Loss: 0.8489809036254883, Final Batch Loss: 0.4389376938343048\n",
      "Epoch 668, Loss: 0.8986989259719849, Final Batch Loss: 0.4418148696422577\n",
      "Epoch 669, Loss: 0.8643315732479095, Final Batch Loss: 0.4240105152130127\n",
      "Epoch 670, Loss: 0.9675667583942413, Final Batch Loss: 0.4563353955745697\n",
      "Epoch 671, Loss: 0.8651553690433502, Final Batch Loss: 0.4759453535079956\n",
      "Epoch 672, Loss: 0.9339753985404968, Final Batch Loss: 0.44315311312675476\n",
      "Epoch 673, Loss: 0.8828677535057068, Final Batch Loss: 0.4758652150630951\n",
      "Epoch 674, Loss: 0.9134277701377869, Final Batch Loss: 0.4656863510608673\n",
      "Epoch 675, Loss: 0.9118001461029053, Final Batch Loss: 0.4319615364074707\n",
      "Epoch 676, Loss: 0.868737667798996, Final Batch Loss: 0.4120451509952545\n",
      "Epoch 677, Loss: 0.8741408288478851, Final Batch Loss: 0.43401673436164856\n",
      "Epoch 678, Loss: 0.8449642956256866, Final Batch Loss: 0.43360912799835205\n",
      "Epoch 679, Loss: 0.8965057134628296, Final Batch Loss: 0.4854068458080292\n",
      "Epoch 680, Loss: 0.8355707824230194, Final Batch Loss: 0.38255012035369873\n",
      "Epoch 681, Loss: 0.8841795921325684, Final Batch Loss: 0.4389384686946869\n",
      "Epoch 682, Loss: 0.9030145108699799, Final Batch Loss: 0.4365179240703583\n",
      "Epoch 683, Loss: 0.8549848794937134, Final Batch Loss: 0.3968164026737213\n",
      "Epoch 684, Loss: 0.7955646514892578, Final Batch Loss: 0.3808891475200653\n",
      "Epoch 685, Loss: 0.870618611574173, Final Batch Loss: 0.4353327751159668\n",
      "Epoch 686, Loss: 0.8490888178348541, Final Batch Loss: 0.38708701729774475\n",
      "Epoch 687, Loss: 0.8315357565879822, Final Batch Loss: 0.39007046818733215\n",
      "Epoch 688, Loss: 0.8519411385059357, Final Batch Loss: 0.3971976935863495\n",
      "Epoch 689, Loss: 0.85515496134758, Final Batch Loss: 0.4053274691104889\n",
      "Epoch 690, Loss: 0.8856197595596313, Final Batch Loss: 0.4545659124851227\n",
      "Epoch 691, Loss: 0.8084766566753387, Final Batch Loss: 0.3854829967021942\n",
      "Epoch 692, Loss: 0.8774541318416595, Final Batch Loss: 0.44810163974761963\n",
      "Epoch 693, Loss: 0.8486081957817078, Final Batch Loss: 0.41944488883018494\n",
      "Epoch 694, Loss: 0.9406644999980927, Final Batch Loss: 0.47831106185913086\n",
      "Epoch 695, Loss: 0.8748958706855774, Final Batch Loss: 0.44087374210357666\n",
      "Epoch 696, Loss: 0.8884710073471069, Final Batch Loss: 0.45548343658447266\n",
      "Epoch 697, Loss: 0.8634398579597473, Final Batch Loss: 0.4553546607494354\n",
      "Epoch 698, Loss: 0.904859870672226, Final Batch Loss: 0.5119495391845703\n",
      "Epoch 699, Loss: 0.8459213972091675, Final Batch Loss: 0.4517616033554077\n",
      "Epoch 700, Loss: 0.8936120569705963, Final Batch Loss: 0.47334766387939453\n",
      "Epoch 701, Loss: 0.858935534954071, Final Batch Loss: 0.37961578369140625\n",
      "Epoch 702, Loss: 0.8686530888080597, Final Batch Loss: 0.46191370487213135\n",
      "Epoch 703, Loss: 0.8220067024230957, Final Batch Loss: 0.3847167193889618\n",
      "Epoch 704, Loss: 0.8692159652709961, Final Batch Loss: 0.42787042260169983\n",
      "Epoch 705, Loss: 0.9098295569419861, Final Batch Loss: 0.48253464698791504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706, Loss: 0.8391424417495728, Final Batch Loss: 0.4624303877353668\n",
      "Epoch 707, Loss: 0.8486080169677734, Final Batch Loss: 0.42062368988990784\n",
      "Epoch 708, Loss: 0.8584883213043213, Final Batch Loss: 0.46527960896492004\n",
      "Epoch 709, Loss: 0.855802983045578, Final Batch Loss: 0.4344722330570221\n",
      "Epoch 710, Loss: 0.8864769339561462, Final Batch Loss: 0.48968613147735596\n",
      "Epoch 711, Loss: 0.8251888453960419, Final Batch Loss: 0.3846810758113861\n",
      "Epoch 712, Loss: 0.8599997758865356, Final Batch Loss: 0.4271252453327179\n",
      "Epoch 713, Loss: 0.8654775023460388, Final Batch Loss: 0.4171421527862549\n",
      "Epoch 714, Loss: 0.911833792924881, Final Batch Loss: 0.49519649147987366\n",
      "Epoch 715, Loss: 0.9046913981437683, Final Batch Loss: 0.46064749360084534\n",
      "Epoch 716, Loss: 0.8415041267871857, Final Batch Loss: 0.4470345079898834\n",
      "Epoch 717, Loss: 0.8636164367198944, Final Batch Loss: 0.44597306847572327\n",
      "Epoch 718, Loss: 0.8795865178108215, Final Batch Loss: 0.45801010727882385\n",
      "Epoch 719, Loss: 0.8438950181007385, Final Batch Loss: 0.4608038663864136\n",
      "Epoch 720, Loss: 0.9509422779083252, Final Batch Loss: 0.4614524841308594\n",
      "Epoch 721, Loss: 0.8130757510662079, Final Batch Loss: 0.39283427596092224\n",
      "Epoch 722, Loss: 0.8135144710540771, Final Batch Loss: 0.37833544611930847\n",
      "Epoch 723, Loss: 0.8744009435176849, Final Batch Loss: 0.4325600564479828\n",
      "Epoch 724, Loss: 0.8753029108047485, Final Batch Loss: 0.45266953110694885\n",
      "Epoch 725, Loss: 0.8769488632678986, Final Batch Loss: 0.46121159195899963\n",
      "Epoch 726, Loss: 0.8060559928417206, Final Batch Loss: 0.38085976243019104\n",
      "Epoch 727, Loss: 0.8480536937713623, Final Batch Loss: 0.4047316610813141\n",
      "Epoch 728, Loss: 0.7658454775810242, Final Batch Loss: 0.4231060743331909\n",
      "Epoch 729, Loss: 0.8124052584171295, Final Batch Loss: 0.3780655562877655\n",
      "Epoch 730, Loss: 0.8428477346897125, Final Batch Loss: 0.3950296640396118\n",
      "Epoch 731, Loss: 0.8181568086147308, Final Batch Loss: 0.3812384605407715\n",
      "Epoch 732, Loss: 0.8570907115936279, Final Batch Loss: 0.46252283453941345\n",
      "Epoch 733, Loss: 0.8409059941768646, Final Batch Loss: 0.43014946579933167\n",
      "Epoch 734, Loss: 0.8329803645610809, Final Batch Loss: 0.43567347526550293\n",
      "Epoch 735, Loss: 0.8523096144199371, Final Batch Loss: 0.43056902289390564\n",
      "Epoch 736, Loss: 0.8662761449813843, Final Batch Loss: 0.43071797490119934\n",
      "Epoch 737, Loss: 0.8100100755691528, Final Batch Loss: 0.44274285435676575\n",
      "Epoch 738, Loss: 0.797932893037796, Final Batch Loss: 0.4232688248157501\n",
      "Epoch 739, Loss: 0.8410020470619202, Final Batch Loss: 0.4163267910480499\n",
      "Epoch 740, Loss: 0.8992588818073273, Final Batch Loss: 0.4035240709781647\n",
      "Epoch 741, Loss: 0.7670422494411469, Final Batch Loss: 0.3803127706050873\n",
      "Epoch 742, Loss: 0.8623591065406799, Final Batch Loss: 0.45481932163238525\n",
      "Epoch 743, Loss: 0.8101292550563812, Final Batch Loss: 0.39947202801704407\n",
      "Epoch 744, Loss: 0.7801739573478699, Final Batch Loss: 0.3927723467350006\n",
      "Epoch 745, Loss: 0.777897983789444, Final Batch Loss: 0.3970870077610016\n",
      "Epoch 746, Loss: 0.8259654343128204, Final Batch Loss: 0.4158848226070404\n",
      "Epoch 747, Loss: 0.7770237028598785, Final Batch Loss: 0.4174882173538208\n",
      "Epoch 748, Loss: 0.8086231350898743, Final Batch Loss: 0.43734392523765564\n",
      "Epoch 749, Loss: 0.8003523647785187, Final Batch Loss: 0.4025280773639679\n",
      "Epoch 750, Loss: 0.8133001327514648, Final Batch Loss: 0.3747227191925049\n",
      "Epoch 751, Loss: 0.7686686217784882, Final Batch Loss: 0.4177713394165039\n",
      "Epoch 752, Loss: 0.7494503557682037, Final Batch Loss: 0.35266005992889404\n",
      "Epoch 753, Loss: 0.840660810470581, Final Batch Loss: 0.4622774124145508\n",
      "Epoch 754, Loss: 0.7886641025543213, Final Batch Loss: 0.3935021162033081\n",
      "Epoch 755, Loss: 0.8077344596385956, Final Batch Loss: 0.3892444670200348\n",
      "Epoch 756, Loss: 0.7547070980072021, Final Batch Loss: 0.3361997902393341\n",
      "Epoch 757, Loss: 0.8439844846725464, Final Batch Loss: 0.3975447118282318\n",
      "Epoch 758, Loss: 0.7731800377368927, Final Batch Loss: 0.3974556624889374\n",
      "Epoch 759, Loss: 0.8045907318592072, Final Batch Loss: 0.4443386495113373\n",
      "Epoch 760, Loss: 0.7614392340183258, Final Batch Loss: 0.37165167927742004\n",
      "Epoch 761, Loss: 0.8141635060310364, Final Batch Loss: 0.3585895597934723\n",
      "Epoch 762, Loss: 0.7666148841381073, Final Batch Loss: 0.42754271626472473\n",
      "Epoch 763, Loss: 0.7421044409275055, Final Batch Loss: 0.35989871621131897\n",
      "Epoch 764, Loss: 0.7875921130180359, Final Batch Loss: 0.4146614968776703\n",
      "Epoch 765, Loss: 0.8312322497367859, Final Batch Loss: 0.4054696559906006\n",
      "Epoch 766, Loss: 0.7703543901443481, Final Batch Loss: 0.405692458152771\n",
      "Epoch 767, Loss: 0.7648015916347504, Final Batch Loss: 0.37259557843208313\n",
      "Epoch 768, Loss: 0.8950368762016296, Final Batch Loss: 0.45514431595802307\n",
      "Epoch 769, Loss: 0.8238288462162018, Final Batch Loss: 0.46303117275238037\n",
      "Epoch 770, Loss: 0.8226135075092316, Final Batch Loss: 0.414340615272522\n",
      "Epoch 771, Loss: 0.7961293756961823, Final Batch Loss: 0.4316767156124115\n",
      "Epoch 772, Loss: 0.8487289547920227, Final Batch Loss: 0.4553668797016144\n",
      "Epoch 773, Loss: 0.7678728997707367, Final Batch Loss: 0.38757121562957764\n",
      "Epoch 774, Loss: 0.7623355686664581, Final Batch Loss: 0.3533857762813568\n",
      "Epoch 775, Loss: 0.8393301367759705, Final Batch Loss: 0.49343594908714294\n",
      "Epoch 776, Loss: 0.7846112251281738, Final Batch Loss: 0.4014837443828583\n",
      "Epoch 777, Loss: 0.8058823347091675, Final Batch Loss: 0.42944207787513733\n",
      "Epoch 778, Loss: 0.7658795714378357, Final Batch Loss: 0.396912544965744\n",
      "Epoch 779, Loss: 0.7589214742183685, Final Batch Loss: 0.3783649504184723\n",
      "Epoch 780, Loss: 0.7556214332580566, Final Batch Loss: 0.34244489669799805\n",
      "Epoch 781, Loss: 0.7503824830055237, Final Batch Loss: 0.3380563259124756\n",
      "Epoch 782, Loss: 0.8545623123645782, Final Batch Loss: 0.5009509325027466\n",
      "Epoch 783, Loss: 0.7732126414775848, Final Batch Loss: 0.3719590902328491\n",
      "Epoch 784, Loss: 0.7788003385066986, Final Batch Loss: 0.40403833985328674\n",
      "Epoch 785, Loss: 0.7668341994285583, Final Batch Loss: 0.39018264412879944\n",
      "Epoch 786, Loss: 0.7153969407081604, Final Batch Loss: 0.38060495257377625\n",
      "Epoch 787, Loss: 0.7576408982276917, Final Batch Loss: 0.3717989921569824\n",
      "Epoch 788, Loss: 0.7544897198677063, Final Batch Loss: 0.38995587825775146\n",
      "Epoch 789, Loss: 0.7686743140220642, Final Batch Loss: 0.3738607168197632\n",
      "Epoch 790, Loss: 0.7325154542922974, Final Batch Loss: 0.3538574278354645\n",
      "Epoch 791, Loss: 0.722803384065628, Final Batch Loss: 0.36234572529792786\n",
      "Epoch 792, Loss: 0.7215645611286163, Final Batch Loss: 0.39708736538887024\n",
      "Epoch 793, Loss: 0.7937910556793213, Final Batch Loss: 0.41866639256477356\n",
      "Epoch 794, Loss: 0.7841898798942566, Final Batch Loss: 0.4154270589351654\n",
      "Epoch 795, Loss: 0.713128536939621, Final Batch Loss: 0.3733278810977936\n",
      "Epoch 796, Loss: 0.7263689339160919, Final Batch Loss: 0.3223905563354492\n",
      "Epoch 797, Loss: 0.7508240938186646, Final Batch Loss: 0.4063442647457123\n",
      "Epoch 798, Loss: 0.7196313440799713, Final Batch Loss: 0.3789363205432892\n",
      "Epoch 799, Loss: 0.8264860212802887, Final Batch Loss: 0.4101303815841675\n",
      "Epoch 800, Loss: 0.7544293701648712, Final Batch Loss: 0.3680085837841034\n",
      "Epoch 801, Loss: 0.743596613407135, Final Batch Loss: 0.30796271562576294\n",
      "Epoch 802, Loss: 0.7357899844646454, Final Batch Loss: 0.35399699211120605\n",
      "Epoch 803, Loss: 0.7371234893798828, Final Batch Loss: 0.36856380105018616\n",
      "Epoch 804, Loss: 0.7491816878318787, Final Batch Loss: 0.31772366166114807\n",
      "Epoch 805, Loss: 0.7119811177253723, Final Batch Loss: 0.34589993953704834\n",
      "Epoch 806, Loss: 0.7276477813720703, Final Batch Loss: 0.30938345193862915\n",
      "Epoch 807, Loss: 0.723257303237915, Final Batch Loss: 0.3406405746936798\n",
      "Epoch 808, Loss: 0.7637841701507568, Final Batch Loss: 0.3134743571281433\n",
      "Epoch 809, Loss: 0.6824340224266052, Final Batch Loss: 0.36885976791381836\n",
      "Epoch 810, Loss: 0.7348774671554565, Final Batch Loss: 0.40180835127830505\n",
      "Epoch 811, Loss: 0.6932857632637024, Final Batch Loss: 0.3784986436367035\n",
      "Epoch 812, Loss: 0.7057778835296631, Final Batch Loss: 0.4012167155742645\n",
      "Epoch 813, Loss: 0.6525900959968567, Final Batch Loss: 0.32714563608169556\n",
      "Epoch 814, Loss: 0.7461170852184296, Final Batch Loss: 0.35142454504966736\n",
      "Epoch 815, Loss: 0.7596868574619293, Final Batch Loss: 0.3513070344924927\n",
      "Epoch 816, Loss: 0.7208259105682373, Final Batch Loss: 0.31864628195762634\n",
      "Epoch 817, Loss: 0.7235110402107239, Final Batch Loss: 0.3349634110927582\n",
      "Epoch 818, Loss: 0.726861834526062, Final Batch Loss: 0.3946467339992523\n",
      "Epoch 819, Loss: 0.7962332367897034, Final Batch Loss: 0.38416633009910583\n",
      "Epoch 820, Loss: 0.7827771604061127, Final Batch Loss: 0.4067920446395874\n",
      "Epoch 821, Loss: 0.7569657266139984, Final Batch Loss: 0.4032497704029083\n",
      "Epoch 822, Loss: 0.6813642978668213, Final Batch Loss: 0.33540549874305725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 823, Loss: 0.7492688298225403, Final Batch Loss: 0.36087992787361145\n",
      "Epoch 824, Loss: 0.755451112985611, Final Batch Loss: 0.4121774435043335\n",
      "Epoch 825, Loss: 0.7617848515510559, Final Batch Loss: 0.34405753016471863\n",
      "Epoch 826, Loss: 0.7082803547382355, Final Batch Loss: 0.31233730912208557\n",
      "Epoch 827, Loss: 0.7458897531032562, Final Batch Loss: 0.4083975851535797\n",
      "Epoch 828, Loss: 0.7178990840911865, Final Batch Loss: 0.4018692076206207\n",
      "Epoch 829, Loss: 0.6598029732704163, Final Batch Loss: 0.3311437666416168\n",
      "Epoch 830, Loss: 0.6980397999286652, Final Batch Loss: 0.3469371795654297\n",
      "Epoch 831, Loss: 0.7225068807601929, Final Batch Loss: 0.30283063650131226\n",
      "Epoch 832, Loss: 0.6744000911712646, Final Batch Loss: 0.3282308876514435\n",
      "Epoch 833, Loss: 0.7130148112773895, Final Batch Loss: 0.3684596121311188\n",
      "Epoch 834, Loss: 0.6962070465087891, Final Batch Loss: 0.36167335510253906\n",
      "Epoch 835, Loss: 0.7237981855869293, Final Batch Loss: 0.3443843424320221\n",
      "Epoch 836, Loss: 0.6990126669406891, Final Batch Loss: 0.3366104066371918\n",
      "Epoch 837, Loss: 0.6905650496482849, Final Batch Loss: 0.3577920198440552\n",
      "Epoch 838, Loss: 0.7034848630428314, Final Batch Loss: 0.33996185660362244\n",
      "Epoch 839, Loss: 0.6421932578086853, Final Batch Loss: 0.36055055260658264\n",
      "Epoch 840, Loss: 0.7414217293262482, Final Batch Loss: 0.37852707505226135\n",
      "Epoch 841, Loss: 0.7121979594230652, Final Batch Loss: 0.3647613823413849\n",
      "Epoch 842, Loss: 0.6991721987724304, Final Batch Loss: 0.4433927834033966\n",
      "Epoch 843, Loss: 0.68335822224617, Final Batch Loss: 0.3598916530609131\n",
      "Epoch 844, Loss: 0.6440297663211823, Final Batch Loss: 0.2941412627696991\n",
      "Epoch 845, Loss: 0.6891427636146545, Final Batch Loss: 0.3413943946361542\n",
      "Epoch 846, Loss: 0.6895449459552765, Final Batch Loss: 0.33768463134765625\n",
      "Epoch 847, Loss: 0.6568950116634369, Final Batch Loss: 0.3538244068622589\n",
      "Epoch 848, Loss: 0.7099481225013733, Final Batch Loss: 0.32584771513938904\n",
      "Epoch 849, Loss: 0.7791339755058289, Final Batch Loss: 0.40667200088500977\n",
      "Epoch 850, Loss: 0.6573924720287323, Final Batch Loss: 0.30979886651039124\n",
      "Epoch 851, Loss: 0.7344852387905121, Final Batch Loss: 0.33670666813850403\n",
      "Epoch 852, Loss: 0.661503255367279, Final Batch Loss: 0.27819737792015076\n",
      "Epoch 853, Loss: 0.6717332899570465, Final Batch Loss: 0.31819742918014526\n",
      "Epoch 854, Loss: 0.700971245765686, Final Batch Loss: 0.37800267338752747\n",
      "Epoch 855, Loss: 0.665579617023468, Final Batch Loss: 0.31187060475349426\n",
      "Epoch 856, Loss: 0.6711806356906891, Final Batch Loss: 0.3302628695964813\n",
      "Epoch 857, Loss: 0.6515524387359619, Final Batch Loss: 0.3151732087135315\n",
      "Epoch 858, Loss: 0.6262994408607483, Final Batch Loss: 0.3072453439235687\n",
      "Epoch 859, Loss: 0.6607336103916168, Final Batch Loss: 0.3009521961212158\n",
      "Epoch 860, Loss: 0.6077738702297211, Final Batch Loss: 0.27730509638786316\n",
      "Epoch 861, Loss: 0.7583653926849365, Final Batch Loss: 0.3515358865261078\n",
      "Epoch 862, Loss: 0.6891799569129944, Final Batch Loss: 0.3448530435562134\n",
      "Epoch 863, Loss: 0.6403924524784088, Final Batch Loss: 0.3346107006072998\n",
      "Epoch 864, Loss: 0.6798752844333649, Final Batch Loss: 0.3959096372127533\n",
      "Epoch 865, Loss: 0.663975715637207, Final Batch Loss: 0.3201356828212738\n",
      "Epoch 866, Loss: 0.6716485023498535, Final Batch Loss: 0.3663466274738312\n",
      "Epoch 867, Loss: 0.7064771354198456, Final Batch Loss: 0.3680853545665741\n",
      "Epoch 868, Loss: 0.6646523475646973, Final Batch Loss: 0.36192867159843445\n",
      "Epoch 869, Loss: 0.6143829822540283, Final Batch Loss: 0.2742237150669098\n",
      "Epoch 870, Loss: 0.6367921233177185, Final Batch Loss: 0.35989823937416077\n",
      "Epoch 871, Loss: 0.7129810154438019, Final Batch Loss: 0.3434312045574188\n",
      "Epoch 872, Loss: 0.6761313080787659, Final Batch Loss: 0.3703378438949585\n",
      "Epoch 873, Loss: 0.7236731946468353, Final Batch Loss: 0.3699887692928314\n",
      "Epoch 874, Loss: 0.7162398993968964, Final Batch Loss: 0.3535355031490326\n",
      "Epoch 875, Loss: 0.6405655741691589, Final Batch Loss: 0.30922985076904297\n",
      "Epoch 876, Loss: 0.6080946028232574, Final Batch Loss: 0.29247912764549255\n",
      "Epoch 877, Loss: 0.6511937081813812, Final Batch Loss: 0.34590473771095276\n",
      "Epoch 878, Loss: 0.6525419056415558, Final Batch Loss: 0.33342865109443665\n",
      "Epoch 879, Loss: 0.7044349908828735, Final Batch Loss: 0.3590226471424103\n",
      "Epoch 880, Loss: 0.6611187160015106, Final Batch Loss: 0.37858691811561584\n",
      "Epoch 881, Loss: 0.6205331385135651, Final Batch Loss: 0.316987007856369\n",
      "Epoch 882, Loss: 0.6691678166389465, Final Batch Loss: 0.35358214378356934\n",
      "Epoch 883, Loss: 0.6353556513786316, Final Batch Loss: 0.32952049374580383\n",
      "Epoch 884, Loss: 0.6187209784984589, Final Batch Loss: 0.3310832381248474\n",
      "Epoch 885, Loss: 0.6009703576564789, Final Batch Loss: 0.2832280695438385\n",
      "Epoch 886, Loss: 0.6123366951942444, Final Batch Loss: 0.2948302626609802\n",
      "Epoch 887, Loss: 0.6997162699699402, Final Batch Loss: 0.3549862205982208\n",
      "Epoch 888, Loss: 0.6214522123336792, Final Batch Loss: 0.3591221868991852\n",
      "Epoch 889, Loss: 0.6769182682037354, Final Batch Loss: 0.27033379673957825\n",
      "Epoch 890, Loss: 0.6801088750362396, Final Batch Loss: 0.3553553819656372\n",
      "Epoch 891, Loss: 0.7363220751285553, Final Batch Loss: 0.3864798843860626\n",
      "Epoch 892, Loss: 0.6951926052570343, Final Batch Loss: 0.3664085566997528\n",
      "Epoch 893, Loss: 0.6773398518562317, Final Batch Loss: 0.36731216311454773\n",
      "Epoch 894, Loss: 0.6906895637512207, Final Batch Loss: 0.3117043972015381\n",
      "Epoch 895, Loss: 0.6541745662689209, Final Batch Loss: 0.35117432475090027\n",
      "Epoch 896, Loss: 0.6728232502937317, Final Batch Loss: 0.3130195736885071\n",
      "Epoch 897, Loss: 0.6722067892551422, Final Batch Loss: 0.33646419644355774\n",
      "Epoch 898, Loss: 0.6699090003967285, Final Batch Loss: 0.3369159400463104\n",
      "Epoch 899, Loss: 0.7004970908164978, Final Batch Loss: 0.35940930247306824\n",
      "Epoch 900, Loss: 0.6116584539413452, Final Batch Loss: 0.29920148849487305\n",
      "Epoch 901, Loss: 0.6617108583450317, Final Batch Loss: 0.32722207903862\n",
      "Epoch 902, Loss: 0.6201134622097015, Final Batch Loss: 0.29367947578430176\n",
      "Epoch 903, Loss: 0.6464152038097382, Final Batch Loss: 0.33568087220191956\n",
      "Epoch 904, Loss: 0.6377346813678741, Final Batch Loss: 0.2845817506313324\n",
      "Epoch 905, Loss: 0.6560749709606171, Final Batch Loss: 0.33659517765045166\n",
      "Epoch 906, Loss: 0.6567372977733612, Final Batch Loss: 0.33147603273391724\n",
      "Epoch 907, Loss: 0.5763120353221893, Final Batch Loss: 0.26617899537086487\n",
      "Epoch 908, Loss: 0.5931456983089447, Final Batch Loss: 0.27186813950538635\n",
      "Epoch 909, Loss: 0.6506782472133636, Final Batch Loss: 0.3021460771560669\n",
      "Epoch 910, Loss: 0.6637313961982727, Final Batch Loss: 0.3366549015045166\n",
      "Epoch 911, Loss: 0.605036735534668, Final Batch Loss: 0.2661984860897064\n",
      "Epoch 912, Loss: 0.6317684054374695, Final Batch Loss: 0.31830039620399475\n",
      "Epoch 913, Loss: 0.6504112184047699, Final Batch Loss: 0.3724169433116913\n",
      "Epoch 914, Loss: 0.5940040051937103, Final Batch Loss: 0.3218193054199219\n",
      "Epoch 915, Loss: 0.5993501245975494, Final Batch Loss: 0.3347036838531494\n",
      "Epoch 916, Loss: 0.6214194297790527, Final Batch Loss: 0.32693126797676086\n",
      "Epoch 917, Loss: 0.5989388823509216, Final Batch Loss: 0.33701470494270325\n",
      "Epoch 918, Loss: 0.5881945490837097, Final Batch Loss: 0.31211575865745544\n",
      "Epoch 919, Loss: 0.6470150649547577, Final Batch Loss: 0.3200758993625641\n",
      "Epoch 920, Loss: 0.6140466928482056, Final Batch Loss: 0.25989070534706116\n",
      "Epoch 921, Loss: 0.6392780244350433, Final Batch Loss: 0.3040916621685028\n",
      "Epoch 922, Loss: 0.621254026889801, Final Batch Loss: 0.33680078387260437\n",
      "Epoch 923, Loss: 0.6284818649291992, Final Batch Loss: 0.30772337317466736\n",
      "Epoch 924, Loss: 0.6513331532478333, Final Batch Loss: 0.3448277711868286\n",
      "Epoch 925, Loss: 0.5942763388156891, Final Batch Loss: 0.2645627558231354\n",
      "Epoch 926, Loss: 0.603234201669693, Final Batch Loss: 0.29644909501075745\n",
      "Epoch 927, Loss: 0.6556698381900787, Final Batch Loss: 0.35469138622283936\n",
      "Epoch 928, Loss: 0.7238395512104034, Final Batch Loss: 0.4327305555343628\n",
      "Epoch 929, Loss: 0.6246935129165649, Final Batch Loss: 0.30004650354385376\n",
      "Epoch 930, Loss: 0.5542383193969727, Final Batch Loss: 0.28909093141555786\n",
      "Epoch 931, Loss: 0.6572692394256592, Final Batch Loss: 0.3163236081600189\n",
      "Epoch 932, Loss: 0.6088180541992188, Final Batch Loss: 0.275505393743515\n",
      "Epoch 933, Loss: 0.6026369631290436, Final Batch Loss: 0.2841068506240845\n",
      "Epoch 934, Loss: 0.6911112070083618, Final Batch Loss: 0.31059911847114563\n",
      "Epoch 935, Loss: 0.6479697525501251, Final Batch Loss: 0.3099091947078705\n",
      "Epoch 936, Loss: 0.5985603332519531, Final Batch Loss: 0.3270053565502167\n",
      "Epoch 937, Loss: 0.6669779419898987, Final Batch Loss: 0.3415754735469818\n",
      "Epoch 938, Loss: 0.584457129240036, Final Batch Loss: 0.2862664759159088\n",
      "Epoch 939, Loss: 0.5665450692176819, Final Batch Loss: 0.2687947154045105\n",
      "Epoch 940, Loss: 0.6000154316425323, Final Batch Loss: 0.3217204213142395\n",
      "Epoch 941, Loss: 0.7013038992881775, Final Batch Loss: 0.392404705286026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 942, Loss: 0.5833034515380859, Final Batch Loss: 0.29748180508613586\n",
      "Epoch 943, Loss: 0.6675070524215698, Final Batch Loss: 0.3493500053882599\n",
      "Epoch 944, Loss: 0.5968493521213531, Final Batch Loss: 0.29552897810935974\n",
      "Epoch 945, Loss: 0.596778005361557, Final Batch Loss: 0.2880795896053314\n",
      "Epoch 946, Loss: 0.6330281496047974, Final Batch Loss: 0.33148258924484253\n",
      "Epoch 947, Loss: 0.5922735929489136, Final Batch Loss: 0.3121356666088104\n",
      "Epoch 948, Loss: 0.6019999086856842, Final Batch Loss: 0.3262328505516052\n",
      "Epoch 949, Loss: 0.6354248225688934, Final Batch Loss: 0.3543616235256195\n",
      "Epoch 950, Loss: 0.6971173882484436, Final Batch Loss: 0.3377549350261688\n",
      "Epoch 951, Loss: 0.6080679893493652, Final Batch Loss: 0.33389750123023987\n",
      "Epoch 952, Loss: 0.5800345540046692, Final Batch Loss: 0.2796887457370758\n",
      "Epoch 953, Loss: 0.5364066958427429, Final Batch Loss: 0.22730028629302979\n",
      "Epoch 954, Loss: 0.5505204796791077, Final Batch Loss: 0.31285086274147034\n",
      "Epoch 955, Loss: 0.5943097621202469, Final Batch Loss: 0.3451765477657318\n",
      "Epoch 956, Loss: 0.5721085965633392, Final Batch Loss: 0.2712028920650482\n",
      "Epoch 957, Loss: 0.5578365325927734, Final Batch Loss: 0.26585283875465393\n",
      "Epoch 958, Loss: 0.5564654171466827, Final Batch Loss: 0.27802774310112\n",
      "Epoch 959, Loss: 0.5943651497364044, Final Batch Loss: 0.26856616139411926\n",
      "Epoch 960, Loss: 0.6277170777320862, Final Batch Loss: 0.34330877661705017\n",
      "Epoch 961, Loss: 0.5841539204120636, Final Batch Loss: 0.28075936436653137\n",
      "Epoch 962, Loss: 0.594091534614563, Final Batch Loss: 0.33634260296821594\n",
      "Epoch 963, Loss: 0.5829167068004608, Final Batch Loss: 0.28815340995788574\n",
      "Epoch 964, Loss: 0.6855630278587341, Final Batch Loss: 0.3846171796321869\n",
      "Epoch 965, Loss: 0.5704523772001266, Final Batch Loss: 0.2406744509935379\n",
      "Epoch 966, Loss: 0.6029505133628845, Final Batch Loss: 0.3052940368652344\n",
      "Epoch 967, Loss: 0.6352960467338562, Final Batch Loss: 0.3433116674423218\n",
      "Epoch 968, Loss: 0.6026728749275208, Final Batch Loss: 0.32254138588905334\n",
      "Epoch 969, Loss: 0.5971684455871582, Final Batch Loss: 0.3255148231983185\n",
      "Epoch 970, Loss: 0.6389997005462646, Final Batch Loss: 0.3616621494293213\n",
      "Epoch 971, Loss: 0.5952725410461426, Final Batch Loss: 0.2883133590221405\n",
      "Epoch 972, Loss: 0.6038109958171844, Final Batch Loss: 0.32849571108818054\n",
      "Epoch 973, Loss: 0.6026065945625305, Final Batch Loss: 0.31143391132354736\n",
      "Epoch 974, Loss: 0.608532041311264, Final Batch Loss: 0.2963884174823761\n",
      "Epoch 975, Loss: 0.6145138740539551, Final Batch Loss: 0.2780330777168274\n",
      "Epoch 976, Loss: 0.6557714343070984, Final Batch Loss: 0.32741236686706543\n",
      "Epoch 977, Loss: 0.5857848525047302, Final Batch Loss: 0.2983179986476898\n",
      "Epoch 978, Loss: 0.538182407617569, Final Batch Loss: 0.254821240901947\n",
      "Epoch 979, Loss: 0.6210399270057678, Final Batch Loss: 0.3889108896255493\n",
      "Epoch 980, Loss: 0.5451058447360992, Final Batch Loss: 0.26183927059173584\n",
      "Epoch 981, Loss: 0.6014904975891113, Final Batch Loss: 0.3527374565601349\n",
      "Epoch 982, Loss: 0.5898691713809967, Final Batch Loss: 0.303849458694458\n",
      "Epoch 983, Loss: 0.5500002205371857, Final Batch Loss: 0.2550010085105896\n",
      "Epoch 984, Loss: 0.545565128326416, Final Batch Loss: 0.24982666969299316\n",
      "Epoch 985, Loss: 0.6887269914150238, Final Batch Loss: 0.3339167535305023\n",
      "Epoch 986, Loss: 0.5866226553916931, Final Batch Loss: 0.29010793566703796\n",
      "Epoch 987, Loss: 0.5822701752185822, Final Batch Loss: 0.27328547835350037\n",
      "Epoch 988, Loss: 0.6354725062847137, Final Batch Loss: 0.2962743937969208\n",
      "Epoch 989, Loss: 0.644282877445221, Final Batch Loss: 0.34790781140327454\n",
      "Epoch 990, Loss: 0.6189171671867371, Final Batch Loss: 0.3180234730243683\n",
      "Epoch 991, Loss: 0.5459955334663391, Final Batch Loss: 0.2536744177341461\n",
      "Epoch 992, Loss: 0.5611843466758728, Final Batch Loss: 0.28059279918670654\n",
      "Epoch 993, Loss: 0.5900534987449646, Final Batch Loss: 0.2533561885356903\n",
      "Epoch 994, Loss: 0.5778293609619141, Final Batch Loss: 0.2730139195919037\n",
      "Epoch 995, Loss: 0.5573410093784332, Final Batch Loss: 0.27873948216438293\n",
      "Epoch 996, Loss: 0.5887092053890228, Final Batch Loss: 0.288303941488266\n",
      "Epoch 997, Loss: 0.5289429128170013, Final Batch Loss: 0.2386096715927124\n",
      "Epoch 998, Loss: 0.5619955658912659, Final Batch Loss: 0.28650739789009094\n",
      "Epoch 999, Loss: 0.5032979100942612, Final Batch Loss: 0.23034711182117462\n",
      "Epoch 1000, Loss: 0.5983597934246063, Final Batch Loss: 0.2991883456707001\n",
      "Epoch 1001, Loss: 0.4816275089979172, Final Batch Loss: 0.22305847704410553\n",
      "Epoch 1002, Loss: 0.531940370798111, Final Batch Loss: 0.308505117893219\n",
      "Epoch 1003, Loss: 0.5443154573440552, Final Batch Loss: 0.27507302165031433\n",
      "Epoch 1004, Loss: 0.6174252331256866, Final Batch Loss: 0.3192637264728546\n",
      "Epoch 1005, Loss: 0.5556961745023727, Final Batch Loss: 0.22770829498767853\n",
      "Epoch 1006, Loss: 0.5518946051597595, Final Batch Loss: 0.2967846691608429\n",
      "Epoch 1007, Loss: 0.5412940680980682, Final Batch Loss: 0.261096328496933\n",
      "Epoch 1008, Loss: 0.5192064195871353, Final Batch Loss: 0.288758784532547\n",
      "Epoch 1009, Loss: 0.5509633421897888, Final Batch Loss: 0.26241227984428406\n",
      "Epoch 1010, Loss: 0.5770298838615417, Final Batch Loss: 0.30827292799949646\n",
      "Epoch 1011, Loss: 0.5690829455852509, Final Batch Loss: 0.2914734184741974\n",
      "Epoch 1012, Loss: 0.5231341421604156, Final Batch Loss: 0.24410712718963623\n",
      "Epoch 1013, Loss: 0.5904548764228821, Final Batch Loss: 0.2899532616138458\n",
      "Epoch 1014, Loss: 0.5247360169887543, Final Batch Loss: 0.27758949995040894\n",
      "Epoch 1015, Loss: 0.5899473130702972, Final Batch Loss: 0.2899608016014099\n",
      "Epoch 1016, Loss: 0.5322486460208893, Final Batch Loss: 0.26164019107818604\n",
      "Epoch 1017, Loss: 0.6620973944664001, Final Batch Loss: 0.36835286021232605\n",
      "Epoch 1018, Loss: 0.6463925540447235, Final Batch Loss: 0.35880616307258606\n",
      "Epoch 1019, Loss: 0.5490050315856934, Final Batch Loss: 0.2881028950214386\n",
      "Epoch 1020, Loss: 0.5333431959152222, Final Batch Loss: 0.2510634958744049\n",
      "Epoch 1021, Loss: 0.5309445858001709, Final Batch Loss: 0.26026976108551025\n",
      "Epoch 1022, Loss: 0.5493187606334686, Final Batch Loss: 0.28791603446006775\n",
      "Epoch 1023, Loss: 0.5584296137094498, Final Batch Loss: 0.24041830003261566\n",
      "Epoch 1024, Loss: 0.6030021011829376, Final Batch Loss: 0.3754967749118805\n",
      "Epoch 1025, Loss: 0.5525546073913574, Final Batch Loss: 0.27820929884910583\n",
      "Epoch 1026, Loss: 0.5669547021389008, Final Batch Loss: 0.2689517140388489\n",
      "Epoch 1027, Loss: 0.5535537898540497, Final Batch Loss: 0.2607494294643402\n",
      "Epoch 1028, Loss: 0.6217910051345825, Final Batch Loss: 0.3096848726272583\n",
      "Epoch 1029, Loss: 0.5643564164638519, Final Batch Loss: 0.3049073815345764\n",
      "Epoch 1030, Loss: 0.5120788365602493, Final Batch Loss: 0.23382167518138885\n",
      "Epoch 1031, Loss: 0.5325536131858826, Final Batch Loss: 0.2376776933670044\n",
      "Epoch 1032, Loss: 0.5492793023586273, Final Batch Loss: 0.2765849530696869\n",
      "Epoch 1033, Loss: 0.5475317537784576, Final Batch Loss: 0.2716211974620819\n",
      "Epoch 1034, Loss: 0.5673531591892242, Final Batch Loss: 0.27640488743782043\n",
      "Epoch 1035, Loss: 0.539934054017067, Final Batch Loss: 0.2928195297718048\n",
      "Epoch 1036, Loss: 0.5405422747135162, Final Batch Loss: 0.2756817042827606\n",
      "Epoch 1037, Loss: 0.545354038476944, Final Batch Loss: 0.27691155672073364\n",
      "Epoch 1038, Loss: 0.5417812168598175, Final Batch Loss: 0.2836261987686157\n",
      "Epoch 1039, Loss: 0.543930172920227, Final Batch Loss: 0.25146058201789856\n",
      "Epoch 1040, Loss: 0.5536904633045197, Final Batch Loss: 0.2650197446346283\n",
      "Epoch 1041, Loss: 0.5656805038452148, Final Batch Loss: 0.3236350119113922\n",
      "Epoch 1042, Loss: 0.5410095453262329, Final Batch Loss: 0.26330795884132385\n",
      "Epoch 1043, Loss: 0.5696642249822617, Final Batch Loss: 0.35162198543548584\n",
      "Epoch 1044, Loss: 0.5339097678661346, Final Batch Loss: 0.2282027006149292\n",
      "Epoch 1045, Loss: 0.6109794676303864, Final Batch Loss: 0.26034170389175415\n",
      "Epoch 1046, Loss: 0.6503103375434875, Final Batch Loss: 0.3428833484649658\n",
      "Epoch 1047, Loss: 0.535216361284256, Final Batch Loss: 0.21761178970336914\n",
      "Epoch 1048, Loss: 0.5280508697032928, Final Batch Loss: 0.2864583134651184\n",
      "Epoch 1049, Loss: 0.5850384831428528, Final Batch Loss: 0.3294696807861328\n",
      "Epoch 1050, Loss: 0.5109047442674637, Final Batch Loss: 0.27599382400512695\n",
      "Epoch 1051, Loss: 0.5316330194473267, Final Batch Loss: 0.2737009823322296\n",
      "Epoch 1052, Loss: 0.5384115874767303, Final Batch Loss: 0.2661910951137543\n",
      "Epoch 1053, Loss: 0.5716070234775543, Final Batch Loss: 0.25669190287590027\n",
      "Epoch 1054, Loss: 0.5242078900337219, Final Batch Loss: 0.25483933091163635\n",
      "Epoch 1055, Loss: 0.5155012309551239, Final Batch Loss: 0.2909611463546753\n",
      "Epoch 1056, Loss: 0.5764227509498596, Final Batch Loss: 0.27723124623298645\n",
      "Epoch 1057, Loss: 0.5759834796190262, Final Batch Loss: 0.34237444400787354\n",
      "Epoch 1058, Loss: 0.5648360848426819, Final Batch Loss: 0.2546250522136688\n",
      "Epoch 1059, Loss: 0.49995459616184235, Final Batch Loss: 0.23036126792430878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1060, Loss: 0.5537135750055313, Final Batch Loss: 0.304100900888443\n",
      "Epoch 1061, Loss: 0.5640859603881836, Final Batch Loss: 0.2326960563659668\n",
      "Epoch 1062, Loss: 0.524271160364151, Final Batch Loss: 0.24466609954833984\n",
      "Epoch 1063, Loss: 0.5561081171035767, Final Batch Loss: 0.2566479742527008\n",
      "Epoch 1064, Loss: 0.5258955359458923, Final Batch Loss: 0.269915372133255\n",
      "Epoch 1065, Loss: 0.560997486114502, Final Batch Loss: 0.3116653859615326\n",
      "Epoch 1066, Loss: 0.5089123994112015, Final Batch Loss: 0.2420336753129959\n",
      "Epoch 1067, Loss: 0.5111719518899918, Final Batch Loss: 0.2153252512216568\n",
      "Epoch 1068, Loss: 0.6266779899597168, Final Batch Loss: 0.34936317801475525\n",
      "Epoch 1069, Loss: 0.5510097444057465, Final Batch Loss: 0.2746197283267975\n",
      "Epoch 1070, Loss: 0.5378251373767853, Final Batch Loss: 0.2985353171825409\n",
      "Epoch 1071, Loss: 0.5190366804599762, Final Batch Loss: 0.27667129039764404\n",
      "Epoch 1072, Loss: 0.6078996956348419, Final Batch Loss: 0.34350237250328064\n",
      "Epoch 1073, Loss: 0.5026598870754242, Final Batch Loss: 0.2562650144100189\n",
      "Epoch 1074, Loss: 0.509504109621048, Final Batch Loss: 0.2548833191394806\n",
      "Epoch 1075, Loss: 0.5336349755525589, Final Batch Loss: 0.3044264614582062\n",
      "Epoch 1076, Loss: 0.600752592086792, Final Batch Loss: 0.29889926314353943\n",
      "Epoch 1077, Loss: 0.5096520185470581, Final Batch Loss: 0.25321027636528015\n",
      "Epoch 1078, Loss: 0.5756163895130157, Final Batch Loss: 0.33097606897354126\n",
      "Epoch 1079, Loss: 0.566331535577774, Final Batch Loss: 0.2567271888256073\n",
      "Epoch 1080, Loss: 0.5485259890556335, Final Batch Loss: 0.2942677140235901\n",
      "Epoch 1081, Loss: 0.6154003143310547, Final Batch Loss: 0.29336491227149963\n",
      "Epoch 1082, Loss: 0.532620370388031, Final Batch Loss: 0.25699910521507263\n",
      "Epoch 1083, Loss: 0.5350408852100372, Final Batch Loss: 0.27439653873443604\n",
      "Epoch 1084, Loss: 0.5040120631456375, Final Batch Loss: 0.27116483449935913\n",
      "Epoch 1085, Loss: 0.5229570865631104, Final Batch Loss: 0.2555234432220459\n",
      "Epoch 1086, Loss: 0.537294864654541, Final Batch Loss: 0.3175455629825592\n",
      "Epoch 1087, Loss: 0.45365455746650696, Final Batch Loss: 0.19820481538772583\n",
      "Epoch 1088, Loss: 0.5271424204111099, Final Batch Loss: 0.28759488463401794\n",
      "Epoch 1089, Loss: 0.5075322091579437, Final Batch Loss: 0.2840404808521271\n",
      "Epoch 1090, Loss: 0.5351155698299408, Final Batch Loss: 0.28535231947898865\n",
      "Epoch 1091, Loss: 0.4955087900161743, Final Batch Loss: 0.2759597599506378\n",
      "Epoch 1092, Loss: 0.5080323070287704, Final Batch Loss: 0.2736159861087799\n",
      "Epoch 1093, Loss: 0.5096749514341354, Final Batch Loss: 0.23365144431591034\n",
      "Epoch 1094, Loss: 0.616936594247818, Final Batch Loss: 0.28759196400642395\n",
      "Epoch 1095, Loss: 0.5002640932798386, Final Batch Loss: 0.2449602335691452\n",
      "Epoch 1096, Loss: 0.5645310580730438, Final Batch Loss: 0.2633567750453949\n",
      "Epoch 1097, Loss: 0.5436460673809052, Final Batch Loss: 0.27049508690834045\n",
      "Epoch 1098, Loss: 0.5551847517490387, Final Batch Loss: 0.29945555329322815\n",
      "Epoch 1099, Loss: 0.5177562832832336, Final Batch Loss: 0.2583899199962616\n",
      "Epoch 1100, Loss: 0.5137710720300674, Final Batch Loss: 0.24151544272899628\n",
      "Epoch 1101, Loss: 0.4902699589729309, Final Batch Loss: 0.24324317276477814\n",
      "Epoch 1102, Loss: 0.5463966429233551, Final Batch Loss: 0.27956315875053406\n",
      "Epoch 1103, Loss: 0.5871076285839081, Final Batch Loss: 0.3491295278072357\n",
      "Epoch 1104, Loss: 0.4800686240196228, Final Batch Loss: 0.2068728804588318\n",
      "Epoch 1105, Loss: 0.55115807056427, Final Batch Loss: 0.25400832295417786\n",
      "Epoch 1106, Loss: 0.5077721923589706, Final Batch Loss: 0.290994793176651\n",
      "Epoch 1107, Loss: 0.5855637788772583, Final Batch Loss: 0.28957536816596985\n",
      "Epoch 1108, Loss: 0.5262191742658615, Final Batch Loss: 0.24784542620182037\n",
      "Epoch 1109, Loss: 0.5483729243278503, Final Batch Loss: 0.29649636149406433\n",
      "Epoch 1110, Loss: 0.5387157201766968, Final Batch Loss: 0.265903502702713\n",
      "Epoch 1111, Loss: 0.5126553475856781, Final Batch Loss: 0.25441163778305054\n",
      "Epoch 1112, Loss: 0.4789852201938629, Final Batch Loss: 0.24435371160507202\n",
      "Epoch 1113, Loss: 0.5613491535186768, Final Batch Loss: 0.3228466808795929\n",
      "Epoch 1114, Loss: 0.5677687227725983, Final Batch Loss: 0.29693201184272766\n",
      "Epoch 1115, Loss: 0.5488592982292175, Final Batch Loss: 0.2524982988834381\n",
      "Epoch 1116, Loss: 0.5420569628477097, Final Batch Loss: 0.2476920336484909\n",
      "Epoch 1117, Loss: 0.5223609358072281, Final Batch Loss: 0.22824354469776154\n",
      "Epoch 1118, Loss: 0.4694497883319855, Final Batch Loss: 0.230653777718544\n",
      "Epoch 1119, Loss: 0.4751592427492142, Final Batch Loss: 0.22132627665996552\n",
      "Epoch 1120, Loss: 0.5680156499147415, Final Batch Loss: 0.2386542707681656\n",
      "Epoch 1121, Loss: 0.5591888427734375, Final Batch Loss: 0.2582845389842987\n",
      "Epoch 1122, Loss: 0.49656499922275543, Final Batch Loss: 0.27305230498313904\n",
      "Epoch 1123, Loss: 0.4952467679977417, Final Batch Loss: 0.2573327124118805\n",
      "Epoch 1124, Loss: 0.49494780600070953, Final Batch Loss: 0.2428518682718277\n",
      "Epoch 1125, Loss: 0.5211511552333832, Final Batch Loss: 0.2633212208747864\n",
      "Epoch 1126, Loss: 0.5111344754695892, Final Batch Loss: 0.2850067913532257\n",
      "Epoch 1127, Loss: 0.4971804767847061, Final Batch Loss: 0.2534489929676056\n",
      "Epoch 1128, Loss: 0.5042151510715485, Final Batch Loss: 0.2926952838897705\n",
      "Epoch 1129, Loss: 0.49868452548980713, Final Batch Loss: 0.19306820631027222\n",
      "Epoch 1130, Loss: 0.5346303284168243, Final Batch Loss: 0.2618608772754669\n",
      "Epoch 1131, Loss: 0.5259928405284882, Final Batch Loss: 0.2596212327480316\n",
      "Epoch 1132, Loss: 0.5579089522361755, Final Batch Loss: 0.27368131279945374\n",
      "Epoch 1133, Loss: 0.4907529503107071, Final Batch Loss: 0.26974907517433167\n",
      "Epoch 1134, Loss: 0.4934431314468384, Final Batch Loss: 0.21837371587753296\n",
      "Epoch 1135, Loss: 0.5221083164215088, Final Batch Loss: 0.2727522552013397\n",
      "Epoch 1136, Loss: 0.5281765162944794, Final Batch Loss: 0.2475338578224182\n",
      "Epoch 1137, Loss: 0.5006840080022812, Final Batch Loss: 0.27570098638534546\n",
      "Epoch 1138, Loss: 0.5098670572042465, Final Batch Loss: 0.26188769936561584\n",
      "Epoch 1139, Loss: 0.5337346345186234, Final Batch Loss: 0.22209332883358002\n",
      "Epoch 1140, Loss: 0.5397361516952515, Final Batch Loss: 0.2884214222431183\n",
      "Epoch 1141, Loss: 0.48686806857585907, Final Batch Loss: 0.23537974059581757\n",
      "Epoch 1142, Loss: 0.5172913819551468, Final Batch Loss: 0.22755758464336395\n",
      "Epoch 1143, Loss: 0.49661150574684143, Final Batch Loss: 0.216483473777771\n",
      "Epoch 1144, Loss: 0.54147769510746, Final Batch Loss: 0.3090384304523468\n",
      "Epoch 1145, Loss: 0.5342365801334381, Final Batch Loss: 0.25504299998283386\n",
      "Epoch 1146, Loss: 0.5609029978513718, Final Batch Loss: 0.3229454457759857\n",
      "Epoch 1147, Loss: 0.5179283320903778, Final Batch Loss: 0.25502079725265503\n",
      "Epoch 1148, Loss: 0.5377660393714905, Final Batch Loss: 0.29742470383644104\n",
      "Epoch 1149, Loss: 0.5286800861358643, Final Batch Loss: 0.3030863106250763\n",
      "Epoch 1150, Loss: 0.4599354416131973, Final Batch Loss: 0.1854209154844284\n",
      "Epoch 1151, Loss: 0.47440727055072784, Final Batch Loss: 0.21393577754497528\n",
      "Epoch 1152, Loss: 0.4793408066034317, Final Batch Loss: 0.2578342854976654\n",
      "Epoch 1153, Loss: 0.47418496012687683, Final Batch Loss: 0.2564034163951874\n",
      "Epoch 1154, Loss: 0.5206723213195801, Final Batch Loss: 0.26006627082824707\n",
      "Epoch 1155, Loss: 0.4991064667701721, Final Batch Loss: 0.22336959838867188\n",
      "Epoch 1156, Loss: 0.5062775760889053, Final Batch Loss: 0.23607824742794037\n",
      "Epoch 1157, Loss: 0.5285272747278214, Final Batch Loss: 0.2390480488538742\n",
      "Epoch 1158, Loss: 0.500162735581398, Final Batch Loss: 0.24925677478313446\n",
      "Epoch 1159, Loss: 0.4944334477186203, Final Batch Loss: 0.22137822210788727\n",
      "Epoch 1160, Loss: 0.5371267795562744, Final Batch Loss: 0.26500460505485535\n",
      "Epoch 1161, Loss: 0.4888859838247299, Final Batch Loss: 0.27691975235939026\n",
      "Epoch 1162, Loss: 0.5005833059549332, Final Batch Loss: 0.2636042535305023\n",
      "Epoch 1163, Loss: 0.5224752277135849, Final Batch Loss: 0.29794320464134216\n",
      "Epoch 1164, Loss: 0.5025051534175873, Final Batch Loss: 0.24887925386428833\n",
      "Epoch 1165, Loss: 0.5401067137718201, Final Batch Loss: 0.2744315266609192\n",
      "Epoch 1166, Loss: 0.5215831100940704, Final Batch Loss: 0.24600952863693237\n",
      "Epoch 1167, Loss: 0.5161751806735992, Final Batch Loss: 0.24978816509246826\n",
      "Epoch 1168, Loss: 0.5023100674152374, Final Batch Loss: 0.2574661672115326\n",
      "Epoch 1169, Loss: 0.4946913868188858, Final Batch Loss: 0.25224319100379944\n",
      "Epoch 1170, Loss: 0.46448010206222534, Final Batch Loss: 0.23385517299175262\n",
      "Epoch 1171, Loss: 0.502429261803627, Final Batch Loss: 0.25390973687171936\n",
      "Epoch 1172, Loss: 0.49326521158218384, Final Batch Loss: 0.21797794103622437\n",
      "Epoch 1173, Loss: 0.5042284429073334, Final Batch Loss: 0.2894304096698761\n",
      "Epoch 1174, Loss: 0.5428630709648132, Final Batch Loss: 0.2802130877971649\n",
      "Epoch 1175, Loss: 0.47944752871990204, Final Batch Loss: 0.21931828558444977\n",
      "Epoch 1176, Loss: 0.4747208505868912, Final Batch Loss: 0.24052441120147705\n",
      "Epoch 1177, Loss: 0.5172467082738876, Final Batch Loss: 0.2832971513271332\n",
      "Epoch 1178, Loss: 0.504835918545723, Final Batch Loss: 0.22593478858470917\n",
      "Epoch 1179, Loss: 0.5357056856155396, Final Batch Loss: 0.2581496238708496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1180, Loss: 0.5635282695293427, Final Batch Loss: 0.3017728924751282\n",
      "Epoch 1181, Loss: 0.5658705830574036, Final Batch Loss: 0.3054790198802948\n",
      "Epoch 1182, Loss: 0.4652433395385742, Final Batch Loss: 0.22563578188419342\n",
      "Epoch 1183, Loss: 0.5153014063835144, Final Batch Loss: 0.23694747686386108\n",
      "Epoch 1184, Loss: 0.49021561443805695, Final Batch Loss: 0.21876372396945953\n",
      "Epoch 1185, Loss: 0.4822304844856262, Final Batch Loss: 0.21091949939727783\n",
      "Epoch 1186, Loss: 0.5122795104980469, Final Batch Loss: 0.25342226028442383\n",
      "Epoch 1187, Loss: 0.5402498245239258, Final Batch Loss: 0.25815343856811523\n",
      "Epoch 1188, Loss: 0.4766160398721695, Final Batch Loss: 0.24972587823867798\n",
      "Epoch 1189, Loss: 0.5101480484008789, Final Batch Loss: 0.22887861728668213\n",
      "Epoch 1190, Loss: 0.5035722851753235, Final Batch Loss: 0.257981538772583\n",
      "Epoch 1191, Loss: 0.5751985609531403, Final Batch Loss: 0.3150717318058014\n",
      "Epoch 1192, Loss: 0.49943649768829346, Final Batch Loss: 0.26045680046081543\n",
      "Epoch 1193, Loss: 0.4591723680496216, Final Batch Loss: 0.25916072726249695\n",
      "Epoch 1194, Loss: 0.6129249334335327, Final Batch Loss: 0.3149431645870209\n",
      "Epoch 1195, Loss: 0.4851546138525009, Final Batch Loss: 0.20692874491214752\n",
      "Epoch 1196, Loss: 0.5050994008779526, Final Batch Loss: 0.24711306393146515\n",
      "Epoch 1197, Loss: 0.4995664060115814, Final Batch Loss: 0.20782506465911865\n",
      "Epoch 1198, Loss: 0.4943547546863556, Final Batch Loss: 0.24726857244968414\n",
      "Epoch 1199, Loss: 0.4876043051481247, Final Batch Loss: 0.25761932134628296\n",
      "Epoch 1200, Loss: 0.5741466283798218, Final Batch Loss: 0.2778572142124176\n",
      "Epoch 1201, Loss: 0.5101508349180222, Final Batch Loss: 0.24797792732715607\n",
      "Epoch 1202, Loss: 0.5073460936546326, Final Batch Loss: 0.26869097352027893\n",
      "Epoch 1203, Loss: 0.5079459995031357, Final Batch Loss: 0.24183402955532074\n",
      "Epoch 1204, Loss: 0.5079046338796616, Final Batch Loss: 0.19657842814922333\n",
      "Epoch 1205, Loss: 0.469680979847908, Final Batch Loss: 0.23469097912311554\n",
      "Epoch 1206, Loss: 0.5484932214021683, Final Batch Loss: 0.3133990168571472\n",
      "Epoch 1207, Loss: 0.6068968176841736, Final Batch Loss: 0.37308716773986816\n",
      "Epoch 1208, Loss: 0.4645741879940033, Final Batch Loss: 0.2163284420967102\n",
      "Epoch 1209, Loss: 0.5193001329898834, Final Batch Loss: 0.261284738779068\n",
      "Epoch 1210, Loss: 0.4872162193059921, Final Batch Loss: 0.221406951546669\n",
      "Epoch 1211, Loss: 0.4708002656698227, Final Batch Loss: 0.27213093638420105\n",
      "Epoch 1212, Loss: 0.5222408920526505, Final Batch Loss: 0.22993658483028412\n",
      "Epoch 1213, Loss: 0.5651132166385651, Final Batch Loss: 0.28560110926628113\n",
      "Epoch 1214, Loss: 0.5036914050579071, Final Batch Loss: 0.28334519267082214\n",
      "Epoch 1215, Loss: 0.45108865201473236, Final Batch Loss: 0.20595169067382812\n",
      "Epoch 1216, Loss: 0.46691013872623444, Final Batch Loss: 0.20230533182621002\n",
      "Epoch 1217, Loss: 0.5085106045007706, Final Batch Loss: 0.2748100459575653\n",
      "Epoch 1218, Loss: 0.527899369597435, Final Batch Loss: 0.23186542093753815\n",
      "Epoch 1219, Loss: 0.5299287736415863, Final Batch Loss: 0.27118074893951416\n",
      "Epoch 1220, Loss: 0.5331467092037201, Final Batch Loss: 0.24831587076187134\n",
      "Epoch 1221, Loss: 0.5049526989459991, Final Batch Loss: 0.21985381841659546\n",
      "Epoch 1222, Loss: 0.5062297135591507, Final Batch Loss: 0.24245597422122955\n",
      "Epoch 1223, Loss: 0.5013966709375381, Final Batch Loss: 0.2717103660106659\n",
      "Epoch 1224, Loss: 0.46018050611019135, Final Batch Loss: 0.25071850419044495\n",
      "Epoch 1225, Loss: 0.4789443761110306, Final Batch Loss: 0.21929608285427094\n",
      "Epoch 1226, Loss: 0.4822886437177658, Final Batch Loss: 0.19697017967700958\n",
      "Epoch 1227, Loss: 0.49200664460659027, Final Batch Loss: 0.23629696667194366\n",
      "Epoch 1228, Loss: 0.4532756805419922, Final Batch Loss: 0.22069323062896729\n",
      "Epoch 1229, Loss: 0.5914618670940399, Final Batch Loss: 0.29632118344306946\n",
      "Epoch 1230, Loss: 0.5245052129030228, Final Batch Loss: 0.2431810051202774\n",
      "Epoch 1231, Loss: 0.47783833742141724, Final Batch Loss: 0.2406635880470276\n",
      "Epoch 1232, Loss: 0.4275344908237457, Final Batch Loss: 0.2331429123878479\n",
      "Epoch 1233, Loss: 0.48394879698753357, Final Batch Loss: 0.24284560978412628\n",
      "Epoch 1234, Loss: 0.4751973897218704, Final Batch Loss: 0.23503275215625763\n",
      "Epoch 1235, Loss: 0.5251614153385162, Final Batch Loss: 0.2697111964225769\n",
      "Epoch 1236, Loss: 0.43798552453517914, Final Batch Loss: 0.22972486913204193\n",
      "Epoch 1237, Loss: 0.48247843980789185, Final Batch Loss: 0.2563588321208954\n",
      "Epoch 1238, Loss: 0.4748424291610718, Final Batch Loss: 0.24193193018436432\n",
      "Epoch 1239, Loss: 0.5236698091030121, Final Batch Loss: 0.23470288515090942\n",
      "Epoch 1240, Loss: 0.4985435903072357, Final Batch Loss: 0.2530208230018616\n",
      "Epoch 1241, Loss: 0.48662249743938446, Final Batch Loss: 0.21796299517154694\n",
      "Epoch 1242, Loss: 0.5600161552429199, Final Batch Loss: 0.2587336599826813\n",
      "Epoch 1243, Loss: 0.501618504524231, Final Batch Loss: 0.28082990646362305\n",
      "Epoch 1244, Loss: 0.5263742357492447, Final Batch Loss: 0.2900504171848297\n",
      "Epoch 1245, Loss: 0.4788188338279724, Final Batch Loss: 0.2575930655002594\n",
      "Epoch 1246, Loss: 0.45277239382267, Final Batch Loss: 0.23169688880443573\n",
      "Epoch 1247, Loss: 0.4282359927892685, Final Batch Loss: 0.21633119881153107\n",
      "Epoch 1248, Loss: 0.43743813037872314, Final Batch Loss: 0.24843667447566986\n",
      "Epoch 1249, Loss: 0.488020196557045, Final Batch Loss: 0.2811332941055298\n",
      "Epoch 1250, Loss: 0.49617788195610046, Final Batch Loss: 0.2619149386882782\n",
      "Epoch 1251, Loss: 0.49648572504520416, Final Batch Loss: 0.28477564454078674\n",
      "Epoch 1252, Loss: 0.5140043795108795, Final Batch Loss: 0.2744845151901245\n",
      "Epoch 1253, Loss: 0.46742454171180725, Final Batch Loss: 0.21788708865642548\n",
      "Epoch 1254, Loss: 0.4657024145126343, Final Batch Loss: 0.21424704790115356\n",
      "Epoch 1255, Loss: 0.5152744352817535, Final Batch Loss: 0.2350451946258545\n",
      "Epoch 1256, Loss: 0.5089348405599594, Final Batch Loss: 0.21762923896312714\n",
      "Epoch 1257, Loss: 0.520122155547142, Final Batch Loss: 0.27388378977775574\n",
      "Epoch 1258, Loss: 0.5014815479516983, Final Batch Loss: 0.24386490881443024\n",
      "Epoch 1259, Loss: 0.48861123621463776, Final Batch Loss: 0.2234215885400772\n",
      "Epoch 1260, Loss: 0.5041932314634323, Final Batch Loss: 0.2445031851530075\n",
      "Epoch 1261, Loss: 0.4648529589176178, Final Batch Loss: 0.2443980723619461\n",
      "Epoch 1262, Loss: 0.487503245472908, Final Batch Loss: 0.2442709058523178\n",
      "Epoch 1263, Loss: 0.4517747312784195, Final Batch Loss: 0.21522165834903717\n",
      "Epoch 1264, Loss: 0.47618862986564636, Final Batch Loss: 0.25134992599487305\n",
      "Epoch 1265, Loss: 0.4625503271818161, Final Batch Loss: 0.1989167481660843\n",
      "Epoch 1266, Loss: 0.5517556220293045, Final Batch Loss: 0.3394542634487152\n",
      "Epoch 1267, Loss: 0.481060191988945, Final Batch Loss: 0.2728796899318695\n",
      "Epoch 1268, Loss: 0.4715290367603302, Final Batch Loss: 0.2595003545284271\n",
      "Epoch 1269, Loss: 0.4809831380844116, Final Batch Loss: 0.24021963775157928\n",
      "Epoch 1270, Loss: 0.5075598359107971, Final Batch Loss: 0.28528717160224915\n",
      "Epoch 1271, Loss: 0.47181545197963715, Final Batch Loss: 0.2953725755214691\n",
      "Epoch 1272, Loss: 0.4620750993490219, Final Batch Loss: 0.27091196179389954\n",
      "Epoch 1273, Loss: 0.5214278697967529, Final Batch Loss: 0.27282336354255676\n",
      "Epoch 1274, Loss: 0.5185757279396057, Final Batch Loss: 0.2759740352630615\n",
      "Epoch 1275, Loss: 0.4599096328020096, Final Batch Loss: 0.1878906935453415\n",
      "Epoch 1276, Loss: 0.5335169285535812, Final Batch Loss: 0.31048378348350525\n",
      "Epoch 1277, Loss: 0.4469171017408371, Final Batch Loss: 0.24128074944019318\n",
      "Epoch 1278, Loss: 0.4745573550462723, Final Batch Loss: 0.2740761637687683\n",
      "Epoch 1279, Loss: 0.4841003268957138, Final Batch Loss: 0.21969272196292877\n",
      "Epoch 1280, Loss: 0.4739612489938736, Final Batch Loss: 0.24493132531642914\n",
      "Epoch 1281, Loss: 0.4701513350009918, Final Batch Loss: 0.270032674074173\n",
      "Epoch 1282, Loss: 0.47826048731803894, Final Batch Loss: 0.2502095401287079\n",
      "Epoch 1283, Loss: 0.5085111558437347, Final Batch Loss: 0.22970974445343018\n",
      "Epoch 1284, Loss: 0.49874603748321533, Final Batch Loss: 0.25800055265426636\n",
      "Epoch 1285, Loss: 0.4513463228940964, Final Batch Loss: 0.19611836969852448\n",
      "Epoch 1286, Loss: 0.46415145695209503, Final Batch Loss: 0.24399490654468536\n",
      "Epoch 1287, Loss: 0.43492090702056885, Final Batch Loss: 0.20344550907611847\n",
      "Epoch 1288, Loss: 0.46401137113571167, Final Batch Loss: 0.23152990639209747\n",
      "Epoch 1289, Loss: 0.514626145362854, Final Batch Loss: 0.25729963183403015\n",
      "Epoch 1290, Loss: 0.4807546138763428, Final Batch Loss: 0.22271156311035156\n",
      "Epoch 1291, Loss: 0.4566015750169754, Final Batch Loss: 0.21531198918819427\n",
      "Epoch 1292, Loss: 0.5006437450647354, Final Batch Loss: 0.2591979205608368\n",
      "Epoch 1293, Loss: 0.44867759943008423, Final Batch Loss: 0.22552411258220673\n",
      "Epoch 1294, Loss: 0.49115483462810516, Final Batch Loss: 0.1980930119752884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1295, Loss: 0.4367053806781769, Final Batch Loss: 0.19970403611660004\n",
      "Epoch 1296, Loss: 0.4973880350589752, Final Batch Loss: 0.22791653871536255\n",
      "Epoch 1297, Loss: 0.5329304486513138, Final Batch Loss: 0.29231759905815125\n",
      "Epoch 1298, Loss: 0.4683469086885452, Final Batch Loss: 0.27936089038848877\n",
      "Epoch 1299, Loss: 0.5116603076457977, Final Batch Loss: 0.298182874917984\n",
      "Epoch 1300, Loss: 0.47630682587623596, Final Batch Loss: 0.22275179624557495\n",
      "Epoch 1301, Loss: 0.4785345047712326, Final Batch Loss: 0.22198288142681122\n",
      "Epoch 1302, Loss: 0.5624439567327499, Final Batch Loss: 0.321951687335968\n",
      "Epoch 1303, Loss: 0.47822800278663635, Final Batch Loss: 0.22588032484054565\n",
      "Epoch 1304, Loss: 0.4744012653827667, Final Batch Loss: 0.1928609013557434\n",
      "Epoch 1305, Loss: 0.49009354412555695, Final Batch Loss: 0.22449995577335358\n",
      "Epoch 1306, Loss: 0.5551795959472656, Final Batch Loss: 0.2813921868801117\n",
      "Epoch 1307, Loss: 0.4369807839393616, Final Batch Loss: 0.18019318580627441\n",
      "Epoch 1308, Loss: 0.4994742125272751, Final Batch Loss: 0.2330998331308365\n",
      "Epoch 1309, Loss: 0.4370907098054886, Final Batch Loss: 0.22766532003879547\n",
      "Epoch 1310, Loss: 0.4839554876089096, Final Batch Loss: 0.18862758576869965\n",
      "Epoch 1311, Loss: 0.4630923420190811, Final Batch Loss: 0.23858404159545898\n",
      "Epoch 1312, Loss: 0.4873543381690979, Final Batch Loss: 0.28319233655929565\n",
      "Epoch 1313, Loss: 0.510559469461441, Final Batch Loss: 0.308430939912796\n",
      "Epoch 1314, Loss: 0.5560120046138763, Final Batch Loss: 0.287100225687027\n",
      "Epoch 1315, Loss: 0.5191473960876465, Final Batch Loss: 0.2599424719810486\n",
      "Epoch 1316, Loss: 0.47530828416347504, Final Batch Loss: 0.23523227870464325\n",
      "Epoch 1317, Loss: 0.4904446303844452, Final Batch Loss: 0.22587448358535767\n",
      "Epoch 1318, Loss: 0.5061410218477249, Final Batch Loss: 0.27056652307510376\n",
      "Epoch 1319, Loss: 0.5438184440135956, Final Batch Loss: 0.30576205253601074\n",
      "Epoch 1320, Loss: 0.47781118750572205, Final Batch Loss: 0.2910856604576111\n",
      "Epoch 1321, Loss: 0.4192337989807129, Final Batch Loss: 0.19755756855010986\n",
      "Epoch 1322, Loss: 0.4660682678222656, Final Batch Loss: 0.23026777803897858\n",
      "Epoch 1323, Loss: 0.515494629740715, Final Batch Loss: 0.2497599571943283\n",
      "Epoch 1324, Loss: 0.46532224118709564, Final Batch Loss: 0.26100391149520874\n",
      "Epoch 1325, Loss: 0.5320802181959152, Final Batch Loss: 0.3007660508155823\n",
      "Epoch 1326, Loss: 0.48360003530979156, Final Batch Loss: 0.24371467530727386\n",
      "Epoch 1327, Loss: 0.4467575401067734, Final Batch Loss: 0.2266172617673874\n",
      "Epoch 1328, Loss: 0.43932703137397766, Final Batch Loss: 0.19519662857055664\n",
      "Epoch 1329, Loss: 0.5050181895494461, Final Batch Loss: 0.3074198067188263\n",
      "Epoch 1330, Loss: 0.469748318195343, Final Batch Loss: 0.24509543180465698\n",
      "Epoch 1331, Loss: 0.474417969584465, Final Batch Loss: 0.2047155648469925\n",
      "Epoch 1332, Loss: 0.4843543916940689, Final Batch Loss: 0.23489944636821747\n",
      "Epoch 1333, Loss: 0.42345841228961945, Final Batch Loss: 0.22440390288829803\n",
      "Epoch 1334, Loss: 0.44665418565273285, Final Batch Loss: 0.20829226076602936\n",
      "Epoch 1335, Loss: 0.4312053620815277, Final Batch Loss: 0.2365444451570511\n",
      "Epoch 1336, Loss: 0.45543915033340454, Final Batch Loss: 0.18932080268859863\n",
      "Epoch 1337, Loss: 0.4695722907781601, Final Batch Loss: 0.22580574452877045\n",
      "Epoch 1338, Loss: 0.43614061176776886, Final Batch Loss: 0.20768921077251434\n",
      "Epoch 1339, Loss: 0.5132060647010803, Final Batch Loss: 0.2826016843318939\n",
      "Epoch 1340, Loss: 0.4087153226137161, Final Batch Loss: 0.2207876443862915\n",
      "Epoch 1341, Loss: 0.4733901172876358, Final Batch Loss: 0.19819925725460052\n",
      "Epoch 1342, Loss: 0.5086144357919693, Final Batch Loss: 0.20728881657123566\n",
      "Epoch 1343, Loss: 0.4669644832611084, Final Batch Loss: 0.2175477296113968\n",
      "Epoch 1344, Loss: 0.46563178300857544, Final Batch Loss: 0.23415040969848633\n",
      "Epoch 1345, Loss: 0.458709254860878, Final Batch Loss: 0.1944621354341507\n",
      "Epoch 1346, Loss: 0.4593926966190338, Final Batch Loss: 0.25981539487838745\n",
      "Epoch 1347, Loss: 0.525750458240509, Final Batch Loss: 0.2568347156047821\n",
      "Epoch 1348, Loss: 0.45844243466854095, Final Batch Loss: 0.20343859493732452\n",
      "Epoch 1349, Loss: 0.45636704564094543, Final Batch Loss: 0.21980874240398407\n",
      "Epoch 1350, Loss: 0.4847342520952225, Final Batch Loss: 0.23961903154850006\n",
      "Epoch 1351, Loss: 0.48024697601795197, Final Batch Loss: 0.21720020473003387\n",
      "Epoch 1352, Loss: 0.4146411418914795, Final Batch Loss: 0.20067401230335236\n",
      "Epoch 1353, Loss: 0.4514399915933609, Final Batch Loss: 0.2314906269311905\n",
      "Epoch 1354, Loss: 0.4745156615972519, Final Batch Loss: 0.2157014161348343\n",
      "Epoch 1355, Loss: 0.46555276215076447, Final Batch Loss: 0.23674295842647552\n",
      "Epoch 1356, Loss: 0.47088058292865753, Final Batch Loss: 0.24290572106838226\n",
      "Epoch 1357, Loss: 0.5260703563690186, Final Batch Loss: 0.27821019291877747\n",
      "Epoch 1358, Loss: 0.45241381227970123, Final Batch Loss: 0.19994641840457916\n",
      "Epoch 1359, Loss: 0.5065629184246063, Final Batch Loss: 0.29860225319862366\n",
      "Epoch 1360, Loss: 0.4396345317363739, Final Batch Loss: 0.2111808806657791\n",
      "Epoch 1361, Loss: 0.4833644777536392, Final Batch Loss: 0.24379359185695648\n",
      "Epoch 1362, Loss: 0.4722093790769577, Final Batch Loss: 0.2080327421426773\n",
      "Epoch 1363, Loss: 0.43810537457466125, Final Batch Loss: 0.19452722370624542\n",
      "Epoch 1364, Loss: 0.5139490365982056, Final Batch Loss: 0.2588053047657013\n",
      "Epoch 1365, Loss: 0.4794484078884125, Final Batch Loss: 0.25898808240890503\n",
      "Epoch 1366, Loss: 0.4711848348379135, Final Batch Loss: 0.25965267419815063\n",
      "Epoch 1367, Loss: 0.4462038427591324, Final Batch Loss: 0.20160804688930511\n",
      "Epoch 1368, Loss: 0.4431101977825165, Final Batch Loss: 0.21437494456768036\n",
      "Epoch 1369, Loss: 0.4919239282608032, Final Batch Loss: 0.2652367055416107\n",
      "Epoch 1370, Loss: 0.4545394331216812, Final Batch Loss: 0.25364962220191956\n",
      "Epoch 1371, Loss: 0.45249757170677185, Final Batch Loss: 0.1973412036895752\n",
      "Epoch 1372, Loss: 0.5242795944213867, Final Batch Loss: 0.3111685514450073\n",
      "Epoch 1373, Loss: 0.4572128653526306, Final Batch Loss: 0.23532426357269287\n",
      "Epoch 1374, Loss: 0.4746105521917343, Final Batch Loss: 0.2579890489578247\n",
      "Epoch 1375, Loss: 0.458763912320137, Final Batch Loss: 0.203400656580925\n",
      "Epoch 1376, Loss: 0.5286663770675659, Final Batch Loss: 0.27054300904273987\n",
      "Epoch 1377, Loss: 0.44180847704410553, Final Batch Loss: 0.23047994077205658\n",
      "Epoch 1378, Loss: 0.48708105087280273, Final Batch Loss: 0.2536375820636749\n",
      "Epoch 1379, Loss: 0.46040959656238556, Final Batch Loss: 0.22657470405101776\n",
      "Epoch 1380, Loss: 0.4929347634315491, Final Batch Loss: 0.2430906742811203\n",
      "Epoch 1381, Loss: 0.5128976255655289, Final Batch Loss: 0.2650816738605499\n",
      "Epoch 1382, Loss: 0.5010916590690613, Final Batch Loss: 0.26397523283958435\n",
      "Epoch 1383, Loss: 0.48473694920539856, Final Batch Loss: 0.23951560258865356\n",
      "Epoch 1384, Loss: 0.4963050037622452, Final Batch Loss: 0.22112973034381866\n",
      "Epoch 1385, Loss: 0.4845663160085678, Final Batch Loss: 0.25095149874687195\n",
      "Epoch 1386, Loss: 0.49574364721775055, Final Batch Loss: 0.2800075113773346\n",
      "Epoch 1387, Loss: 0.4557601511478424, Final Batch Loss: 0.21678413450717926\n",
      "Epoch 1388, Loss: 0.4308425486087799, Final Batch Loss: 0.18393869698047638\n",
      "Epoch 1389, Loss: 0.5120726227760315, Final Batch Loss: 0.2798745930194855\n",
      "Epoch 1390, Loss: 0.4671805799007416, Final Batch Loss: 0.21868740022182465\n",
      "Epoch 1391, Loss: 0.4762597233057022, Final Batch Loss: 0.28507259488105774\n",
      "Epoch 1392, Loss: 0.5031618624925613, Final Batch Loss: 0.23198525607585907\n",
      "Epoch 1393, Loss: 0.4792993515729904, Final Batch Loss: 0.23301631212234497\n",
      "Epoch 1394, Loss: 0.43959473073482513, Final Batch Loss: 0.16272185742855072\n",
      "Epoch 1395, Loss: 0.4644179493188858, Final Batch Loss: 0.23907966911792755\n",
      "Epoch 1396, Loss: 0.5113503634929657, Final Batch Loss: 0.2544170916080475\n",
      "Epoch 1397, Loss: 0.43319518864154816, Final Batch Loss: 0.21582989394664764\n",
      "Epoch 1398, Loss: 0.4617530554533005, Final Batch Loss: 0.2502209544181824\n",
      "Epoch 1399, Loss: 0.44491562247276306, Final Batch Loss: 0.21594059467315674\n",
      "Epoch 1400, Loss: 0.4885827302932739, Final Batch Loss: 0.22694802284240723\n",
      "Epoch 1401, Loss: 0.4624105840921402, Final Batch Loss: 0.22439396381378174\n",
      "Epoch 1402, Loss: 0.44412484765052795, Final Batch Loss: 0.23558573424816132\n",
      "Epoch 1403, Loss: 0.4593570977449417, Final Batch Loss: 0.2108268290758133\n",
      "Epoch 1404, Loss: 0.4619712382555008, Final Batch Loss: 0.20382003486156464\n",
      "Epoch 1405, Loss: 0.44536633789539337, Final Batch Loss: 0.22076626121997833\n",
      "Epoch 1406, Loss: 0.45386625826358795, Final Batch Loss: 0.23362195491790771\n",
      "Epoch 1407, Loss: 0.5035580992698669, Final Batch Loss: 0.2900910973548889\n",
      "Epoch 1408, Loss: 0.3934183269739151, Final Batch Loss: 0.19406263530254364\n",
      "Epoch 1409, Loss: 0.4406541436910629, Final Batch Loss: 0.23895220458507538\n",
      "Epoch 1410, Loss: 0.4202504903078079, Final Batch Loss: 0.2210461050271988\n",
      "Epoch 1411, Loss: 0.48897819221019745, Final Batch Loss: 0.2516289949417114\n",
      "Epoch 1412, Loss: 0.44965894520282745, Final Batch Loss: 0.24899818003177643\n",
      "Epoch 1413, Loss: 0.4825187623500824, Final Batch Loss: 0.2248784303665161\n",
      "Epoch 1414, Loss: 0.49987415969371796, Final Batch Loss: 0.22137589752674103\n",
      "Epoch 1415, Loss: 0.44712188839912415, Final Batch Loss: 0.21505852043628693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1416, Loss: 0.39740218222141266, Final Batch Loss: 0.17861007153987885\n",
      "Epoch 1417, Loss: 0.4533783942461014, Final Batch Loss: 0.23019945621490479\n",
      "Epoch 1418, Loss: 0.5010525286197662, Final Batch Loss: 0.27911636233329773\n",
      "Epoch 1419, Loss: 0.46369992196559906, Final Batch Loss: 0.20684970915317535\n",
      "Epoch 1420, Loss: 0.44867853820323944, Final Batch Loss: 0.2227746993303299\n",
      "Epoch 1421, Loss: 0.4526357799768448, Final Batch Loss: 0.19121475517749786\n",
      "Epoch 1422, Loss: 0.4813694655895233, Final Batch Loss: 0.2637728452682495\n",
      "Epoch 1423, Loss: 0.4237724095582962, Final Batch Loss: 0.20987403392791748\n",
      "Epoch 1424, Loss: 0.503102645277977, Final Batch Loss: 0.2146696001291275\n",
      "Epoch 1425, Loss: 0.4981815069913864, Final Batch Loss: 0.2783918082714081\n",
      "Epoch 1426, Loss: 0.4710901230573654, Final Batch Loss: 0.28091105818748474\n",
      "Epoch 1427, Loss: 0.48027731478214264, Final Batch Loss: 0.2633875906467438\n",
      "Epoch 1428, Loss: 0.4299324005842209, Final Batch Loss: 0.2198277860879898\n",
      "Epoch 1429, Loss: 0.43843595683574677, Final Batch Loss: 0.21529273688793182\n",
      "Epoch 1430, Loss: 0.4725147634744644, Final Batch Loss: 0.24992768466472626\n",
      "Epoch 1431, Loss: 0.40431302785873413, Final Batch Loss: 0.20991283655166626\n",
      "Epoch 1432, Loss: 0.44942760467529297, Final Batch Loss: 0.23473238945007324\n",
      "Epoch 1433, Loss: 0.45127643644809723, Final Batch Loss: 0.27159854769706726\n",
      "Epoch 1434, Loss: 0.47331608831882477, Final Batch Loss: 0.2796708345413208\n",
      "Epoch 1435, Loss: 0.44027774035930634, Final Batch Loss: 0.19952936470508575\n",
      "Epoch 1436, Loss: 0.48843570053577423, Final Batch Loss: 0.24294303357601166\n",
      "Epoch 1437, Loss: 0.46208347380161285, Final Batch Loss: 0.2623370587825775\n",
      "Epoch 1438, Loss: 0.39515288174152374, Final Batch Loss: 0.1730952113866806\n",
      "Epoch 1439, Loss: 0.44794732332229614, Final Batch Loss: 0.21395106613636017\n",
      "Epoch 1440, Loss: 0.4570729583501816, Final Batch Loss: 0.19521798193454742\n",
      "Epoch 1441, Loss: 0.47980451583862305, Final Batch Loss: 0.24460875988006592\n",
      "Epoch 1442, Loss: 0.48326800763607025, Final Batch Loss: 0.28760403394699097\n",
      "Epoch 1443, Loss: 0.48130011558532715, Final Batch Loss: 0.2432645708322525\n",
      "Epoch 1444, Loss: 0.439176544547081, Final Batch Loss: 0.2104564905166626\n",
      "Epoch 1445, Loss: 0.41551683843135834, Final Batch Loss: 0.21314330399036407\n",
      "Epoch 1446, Loss: 0.4582746624946594, Final Batch Loss: 0.21219711005687714\n",
      "Epoch 1447, Loss: 0.4789600670337677, Final Batch Loss: 0.25879770517349243\n",
      "Epoch 1448, Loss: 0.4154525548219681, Final Batch Loss: 0.19501788914203644\n",
      "Epoch 1449, Loss: 0.41793225705623627, Final Batch Loss: 0.2004222273826599\n",
      "Epoch 1450, Loss: 0.4314536601305008, Final Batch Loss: 0.2156548947095871\n",
      "Epoch 1451, Loss: 0.4520840495824814, Final Batch Loss: 0.24038727581501007\n",
      "Epoch 1452, Loss: 0.44775305688381195, Final Batch Loss: 0.21784014999866486\n",
      "Epoch 1453, Loss: 0.46764060854911804, Final Batch Loss: 0.25359266996383667\n",
      "Epoch 1454, Loss: 0.49118107557296753, Final Batch Loss: 0.24788741767406464\n",
      "Epoch 1455, Loss: 0.44436195492744446, Final Batch Loss: 0.22772663831710815\n",
      "Epoch 1456, Loss: 0.4265029579401016, Final Batch Loss: 0.18563173711299896\n",
      "Epoch 1457, Loss: 0.5011364966630936, Final Batch Loss: 0.24461553990840912\n",
      "Epoch 1458, Loss: 0.4655795246362686, Final Batch Loss: 0.2345827966928482\n",
      "Epoch 1459, Loss: 0.5045595020055771, Final Batch Loss: 0.25721827149391174\n",
      "Epoch 1460, Loss: 0.479169026017189, Final Batch Loss: 0.25542762875556946\n",
      "Epoch 1461, Loss: 0.4191187918186188, Final Batch Loss: 0.1853761076927185\n",
      "Epoch 1462, Loss: 0.48434488475322723, Final Batch Loss: 0.26200973987579346\n",
      "Epoch 1463, Loss: 0.47798047959804535, Final Batch Loss: 0.2676216661930084\n",
      "Epoch 1464, Loss: 0.48613813519477844, Final Batch Loss: 0.20622551441192627\n",
      "Epoch 1465, Loss: 0.4443167895078659, Final Batch Loss: 0.16877920925617218\n",
      "Epoch 1466, Loss: 0.4827440679073334, Final Batch Loss: 0.2530530095100403\n",
      "Epoch 1467, Loss: 0.45604872703552246, Final Batch Loss: 0.20905524492263794\n",
      "Epoch 1468, Loss: 0.465373694896698, Final Batch Loss: 0.23569776117801666\n",
      "Epoch 1469, Loss: 0.42943549156188965, Final Batch Loss: 0.2225971817970276\n",
      "Epoch 1470, Loss: 0.4664536714553833, Final Batch Loss: 0.2574442923069\n",
      "Epoch 1471, Loss: 0.4717317968606949, Final Batch Loss: 0.20879699289798737\n",
      "Epoch 1472, Loss: 0.4396708309650421, Final Batch Loss: 0.22469361126422882\n",
      "Epoch 1473, Loss: 0.4996725171804428, Final Batch Loss: 0.2591186463832855\n",
      "Epoch 1474, Loss: 0.45896051824092865, Final Batch Loss: 0.2597782611846924\n",
      "Epoch 1475, Loss: 0.3996643126010895, Final Batch Loss: 0.15597091615200043\n",
      "Epoch 1476, Loss: 0.4710285812616348, Final Batch Loss: 0.19101424515247345\n",
      "Epoch 1477, Loss: 0.4500841647386551, Final Batch Loss: 0.21294035017490387\n",
      "Epoch 1478, Loss: 0.4505935460329056, Final Batch Loss: 0.2130805253982544\n",
      "Epoch 1479, Loss: 0.4398432672023773, Final Batch Loss: 0.20492874085903168\n",
      "Epoch 1480, Loss: 0.48098041117191315, Final Batch Loss: 0.24337248504161835\n",
      "Epoch 1481, Loss: 0.42226219177246094, Final Batch Loss: 0.19876493513584137\n",
      "Epoch 1482, Loss: 0.4242248982191086, Final Batch Loss: 0.18556512892246246\n",
      "Epoch 1483, Loss: 0.44499193131923676, Final Batch Loss: 0.21350465714931488\n",
      "Epoch 1484, Loss: 0.46763359010219574, Final Batch Loss: 0.21600984036922455\n",
      "Epoch 1485, Loss: 0.44513191282749176, Final Batch Loss: 0.2198377251625061\n",
      "Epoch 1486, Loss: 0.46755364537239075, Final Batch Loss: 0.22742033004760742\n",
      "Epoch 1487, Loss: 0.43111124634742737, Final Batch Loss: 0.2072451114654541\n",
      "Epoch 1488, Loss: 0.47585102915763855, Final Batch Loss: 0.24612487852573395\n",
      "Epoch 1489, Loss: 0.44723697006702423, Final Batch Loss: 0.2361048460006714\n",
      "Epoch 1490, Loss: 0.4583294838666916, Final Batch Loss: 0.1868733912706375\n",
      "Epoch 1491, Loss: 0.42229750752449036, Final Batch Loss: 0.191167950630188\n",
      "Epoch 1492, Loss: 0.5013246089220047, Final Batch Loss: 0.2943667471408844\n",
      "Epoch 1493, Loss: 0.46647651493549347, Final Batch Loss: 0.2363830804824829\n",
      "Epoch 1494, Loss: 0.4633335918188095, Final Batch Loss: 0.18931348621845245\n",
      "Epoch 1495, Loss: 0.43033333122730255, Final Batch Loss: 0.2320333868265152\n",
      "Epoch 1496, Loss: 0.45022134482860565, Final Batch Loss: 0.2247428297996521\n",
      "Epoch 1497, Loss: 0.560769259929657, Final Batch Loss: 0.3211781978607178\n",
      "Epoch 1498, Loss: 0.43215468525886536, Final Batch Loss: 0.19103600084781647\n",
      "Epoch 1499, Loss: 0.4384385049343109, Final Batch Loss: 0.1918039172887802\n",
      "Epoch 1500, Loss: 0.4546240568161011, Final Batch Loss: 0.267196923494339\n",
      "Epoch 1501, Loss: 0.4990569055080414, Final Batch Loss: 0.2683638632297516\n",
      "Epoch 1502, Loss: 0.41010136902332306, Final Batch Loss: 0.21709632873535156\n",
      "Epoch 1503, Loss: 0.4716324806213379, Final Batch Loss: 0.2611548900604248\n",
      "Epoch 1504, Loss: 0.4245744049549103, Final Batch Loss: 0.2132297158241272\n",
      "Epoch 1505, Loss: 0.51697838306427, Final Batch Loss: 0.26862654089927673\n",
      "Epoch 1506, Loss: 0.4242923706769943, Final Batch Loss: 0.21390654146671295\n",
      "Epoch 1507, Loss: 0.45491740107536316, Final Batch Loss: 0.21002155542373657\n",
      "Epoch 1508, Loss: 0.42998138070106506, Final Batch Loss: 0.23875315487384796\n",
      "Epoch 1509, Loss: 0.43789058923721313, Final Batch Loss: 0.23033268749713898\n",
      "Epoch 1510, Loss: 0.4549752175807953, Final Batch Loss: 0.21044178307056427\n",
      "Epoch 1511, Loss: 0.47086434066295624, Final Batch Loss: 0.2612912356853485\n",
      "Epoch 1512, Loss: 0.41581717133522034, Final Batch Loss: 0.21230445802211761\n",
      "Epoch 1513, Loss: 0.46253496408462524, Final Batch Loss: 0.2574899196624756\n",
      "Epoch 1514, Loss: 0.47035546600818634, Final Batch Loss: 0.22693496942520142\n",
      "Epoch 1515, Loss: 0.4069504290819168, Final Batch Loss: 0.19172833859920502\n",
      "Epoch 1516, Loss: 0.4406355172395706, Final Batch Loss: 0.18731801211833954\n",
      "Epoch 1517, Loss: 0.43916715681552887, Final Batch Loss: 0.16148217022418976\n",
      "Epoch 1518, Loss: 0.4594736099243164, Final Batch Loss: 0.1893482208251953\n",
      "Epoch 1519, Loss: 0.4323742836713791, Final Batch Loss: 0.18729545176029205\n",
      "Epoch 1520, Loss: 0.5002254396677017, Final Batch Loss: 0.2659466564655304\n",
      "Epoch 1521, Loss: 0.43191932141780853, Final Batch Loss: 0.204913929104805\n",
      "Epoch 1522, Loss: 0.516124814748764, Final Batch Loss: 0.2942890226840973\n",
      "Epoch 1523, Loss: 0.44958527386188507, Final Batch Loss: 0.2705744504928589\n",
      "Epoch 1524, Loss: 0.47150804102420807, Final Batch Loss: 0.21539901196956635\n",
      "Epoch 1525, Loss: 0.4549378305673599, Final Batch Loss: 0.25976085662841797\n",
      "Epoch 1526, Loss: 0.4775003343820572, Final Batch Loss: 0.2531428635120392\n",
      "Epoch 1527, Loss: 0.49979498982429504, Final Batch Loss: 0.20409125089645386\n",
      "Epoch 1528, Loss: 0.4232465326786041, Final Batch Loss: 0.21627305448055267\n",
      "Epoch 1529, Loss: 0.48640091717243195, Final Batch Loss: 0.23408536612987518\n",
      "Epoch 1530, Loss: 0.4602300375699997, Final Batch Loss: 0.256201833486557\n",
      "Epoch 1531, Loss: 0.43905267119407654, Final Batch Loss: 0.2090086191892624\n",
      "Epoch 1532, Loss: 0.4142226427793503, Final Batch Loss: 0.1754646748304367\n",
      "Epoch 1533, Loss: 0.47512853145599365, Final Batch Loss: 0.2695883810520172\n",
      "Epoch 1534, Loss: 0.46329230070114136, Final Batch Loss: 0.21204358339309692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1535, Loss: 0.4570748209953308, Final Batch Loss: 0.2506958544254303\n",
      "Epoch 1536, Loss: 0.46825413405895233, Final Batch Loss: 0.22630256414413452\n",
      "Epoch 1537, Loss: 0.42724139988422394, Final Batch Loss: 0.21075738966464996\n",
      "Epoch 1538, Loss: 0.45727068185806274, Final Batch Loss: 0.2443019151687622\n",
      "Epoch 1539, Loss: 0.44828276336193085, Final Batch Loss: 0.2262105792760849\n",
      "Epoch 1540, Loss: 0.46305665373802185, Final Batch Loss: 0.24409504234790802\n",
      "Epoch 1541, Loss: 0.4251733422279358, Final Batch Loss: 0.20613707602024078\n",
      "Epoch 1542, Loss: 0.41890406608581543, Final Batch Loss: 0.2160433977842331\n",
      "Epoch 1543, Loss: 0.4586060494184494, Final Batch Loss: 0.2014155238866806\n",
      "Epoch 1544, Loss: 0.41084757447242737, Final Batch Loss: 0.16673623025417328\n",
      "Epoch 1545, Loss: 0.4674149304628372, Final Batch Loss: 0.2594779133796692\n",
      "Epoch 1546, Loss: 0.5237216055393219, Final Batch Loss: 0.3146679401397705\n",
      "Epoch 1547, Loss: 0.454624280333519, Final Batch Loss: 0.25091156363487244\n",
      "Epoch 1548, Loss: 0.47562554478645325, Final Batch Loss: 0.2650235593318939\n",
      "Epoch 1549, Loss: 0.4795233756303787, Final Batch Loss: 0.2075209766626358\n",
      "Epoch 1550, Loss: 0.47201262414455414, Final Batch Loss: 0.22201161086559296\n",
      "Epoch 1551, Loss: 0.44351524114608765, Final Batch Loss: 0.2518705129623413\n",
      "Epoch 1552, Loss: 0.41317427158355713, Final Batch Loss: 0.2040136307477951\n",
      "Epoch 1553, Loss: 0.4637945890426636, Final Batch Loss: 0.23385274410247803\n",
      "Epoch 1554, Loss: 0.45373181998729706, Final Batch Loss: 0.19916869699954987\n",
      "Epoch 1555, Loss: 0.4801851660013199, Final Batch Loss: 0.21001391112804413\n",
      "Epoch 1556, Loss: 0.42605365812778473, Final Batch Loss: 0.21147684752941132\n",
      "Epoch 1557, Loss: 0.4180804193019867, Final Batch Loss: 0.2091137319803238\n",
      "Epoch 1558, Loss: 0.4362346678972244, Final Batch Loss: 0.24861259758472443\n",
      "Epoch 1559, Loss: 0.4126368910074234, Final Batch Loss: 0.19024032354354858\n",
      "Epoch 1560, Loss: 0.44713594019412994, Final Batch Loss: 0.20254497230052948\n",
      "Epoch 1561, Loss: 0.4833488464355469, Final Batch Loss: 0.2289048433303833\n",
      "Epoch 1562, Loss: 0.4799295663833618, Final Batch Loss: 0.26498985290527344\n",
      "Epoch 1563, Loss: 0.4843294471502304, Final Batch Loss: 0.23714949190616608\n",
      "Epoch 1564, Loss: 0.47205352783203125, Final Batch Loss: 0.19791436195373535\n",
      "Epoch 1565, Loss: 0.4608219414949417, Final Batch Loss: 0.24143396317958832\n",
      "Epoch 1566, Loss: 0.4715433567762375, Final Batch Loss: 0.25961849093437195\n",
      "Epoch 1567, Loss: 0.41208918392658234, Final Batch Loss: 0.19020779430866241\n",
      "Epoch 1568, Loss: 0.4145798087120056, Final Batch Loss: 0.2203936129808426\n",
      "Epoch 1569, Loss: 0.4826897978782654, Final Batch Loss: 0.2684520483016968\n",
      "Epoch 1570, Loss: 0.451723575592041, Final Batch Loss: 0.21601343154907227\n",
      "Epoch 1571, Loss: 0.39245353639125824, Final Batch Loss: 0.18187959492206573\n",
      "Epoch 1572, Loss: 0.40593893826007843, Final Batch Loss: 0.19953925907611847\n",
      "Epoch 1573, Loss: 0.442399725317955, Final Batch Loss: 0.2830240726470947\n",
      "Epoch 1574, Loss: 0.3900183290243149, Final Batch Loss: 0.19747501611709595\n",
      "Epoch 1575, Loss: 0.4202962964773178, Final Batch Loss: 0.2178087681531906\n",
      "Epoch 1576, Loss: 0.42555850744247437, Final Batch Loss: 0.23488879203796387\n",
      "Epoch 1577, Loss: 0.45338162779808044, Final Batch Loss: 0.2173789143562317\n",
      "Epoch 1578, Loss: 0.46296440064907074, Final Batch Loss: 0.2486667037010193\n",
      "Epoch 1579, Loss: 0.4358670860528946, Final Batch Loss: 0.2184593230485916\n",
      "Epoch 1580, Loss: 0.41462045907974243, Final Batch Loss: 0.1990935355424881\n",
      "Epoch 1581, Loss: 0.4634690284729004, Final Batch Loss: 0.2521136403083801\n",
      "Epoch 1582, Loss: 0.5136251896619797, Final Batch Loss: 0.18241144716739655\n",
      "Epoch 1583, Loss: 0.41809603571891785, Final Batch Loss: 0.24341906607151031\n",
      "Epoch 1584, Loss: 0.49832840263843536, Final Batch Loss: 0.23960773646831512\n",
      "Epoch 1585, Loss: 0.4476751834154129, Final Batch Loss: 0.18881873786449432\n",
      "Epoch 1586, Loss: 0.4323614090681076, Final Batch Loss: 0.23249168694019318\n",
      "Epoch 1587, Loss: 0.4161796271800995, Final Batch Loss: 0.1993137001991272\n",
      "Epoch 1588, Loss: 0.47145673632621765, Final Batch Loss: 0.2620527446269989\n",
      "Epoch 1589, Loss: 0.46633145213127136, Final Batch Loss: 0.24252714216709137\n",
      "Epoch 1590, Loss: 0.4898904860019684, Final Batch Loss: 0.26438212394714355\n",
      "Epoch 1591, Loss: 0.4355733096599579, Final Batch Loss: 0.22063712775707245\n",
      "Epoch 1592, Loss: 0.4217725247144699, Final Batch Loss: 0.24906711280345917\n",
      "Epoch 1593, Loss: 0.45903268456459045, Final Batch Loss: 0.1877158284187317\n",
      "Epoch 1594, Loss: 0.462847575545311, Final Batch Loss: 0.20649372041225433\n",
      "Epoch 1595, Loss: 0.43214333057403564, Final Batch Loss: 0.2051842212677002\n",
      "Epoch 1596, Loss: 0.42812420427799225, Final Batch Loss: 0.2184462696313858\n",
      "Epoch 1597, Loss: 0.4564886540174484, Final Batch Loss: 0.2037471979856491\n",
      "Epoch 1598, Loss: 0.40429964661598206, Final Batch Loss: 0.20750559866428375\n",
      "Epoch 1599, Loss: 0.4108384996652603, Final Batch Loss: 0.21856386959552765\n",
      "Epoch 1600, Loss: 0.46501223742961884, Final Batch Loss: 0.21699482202529907\n",
      "Epoch 1601, Loss: 0.4395715743303299, Final Batch Loss: 0.186667338013649\n",
      "Epoch 1602, Loss: 0.43457289040088654, Final Batch Loss: 0.22402872145175934\n",
      "Epoch 1603, Loss: 0.44560837745666504, Final Batch Loss: 0.25273728370666504\n",
      "Epoch 1604, Loss: 0.4321743845939636, Final Batch Loss: 0.23823748528957367\n",
      "Epoch 1605, Loss: 0.3844524472951889, Final Batch Loss: 0.23315201699733734\n",
      "Epoch 1606, Loss: 0.4273097962141037, Final Batch Loss: 0.20663738250732422\n",
      "Epoch 1607, Loss: 0.4361182600259781, Final Batch Loss: 0.22193880379199982\n",
      "Epoch 1608, Loss: 0.5033390372991562, Final Batch Loss: 0.2648985683917999\n",
      "Epoch 1609, Loss: 0.42128337919712067, Final Batch Loss: 0.20598648488521576\n",
      "Epoch 1610, Loss: 0.4446800649166107, Final Batch Loss: 0.2628951966762543\n",
      "Epoch 1611, Loss: 0.4352458268404007, Final Batch Loss: 0.19748280942440033\n",
      "Epoch 1612, Loss: 0.41628898680210114, Final Batch Loss: 0.178594172000885\n",
      "Epoch 1613, Loss: 0.4046529233455658, Final Batch Loss: 0.2210725098848343\n",
      "Epoch 1614, Loss: 0.4889524579048157, Final Batch Loss: 0.25482213497161865\n",
      "Epoch 1615, Loss: 0.41561321914196014, Final Batch Loss: 0.2155749350786209\n",
      "Epoch 1616, Loss: 0.41338539123535156, Final Batch Loss: 0.182573601603508\n",
      "Epoch 1617, Loss: 0.40245066583156586, Final Batch Loss: 0.1603519320487976\n",
      "Epoch 1618, Loss: 0.4341156482696533, Final Batch Loss: 0.22739844024181366\n",
      "Epoch 1619, Loss: 0.41957804560661316, Final Batch Loss: 0.19764697551727295\n",
      "Epoch 1620, Loss: 0.46590355038642883, Final Batch Loss: 0.22672580182552338\n",
      "Epoch 1621, Loss: 0.4218140095472336, Final Batch Loss: 0.21172314882278442\n",
      "Epoch 1622, Loss: 0.5012345165014267, Final Batch Loss: 0.24157436192035675\n",
      "Epoch 1623, Loss: 0.4063462167978287, Final Batch Loss: 0.19759486615657806\n",
      "Epoch 1624, Loss: 0.4182227700948715, Final Batch Loss: 0.21587921679019928\n",
      "Epoch 1625, Loss: 0.5319305956363678, Final Batch Loss: 0.2659473717212677\n",
      "Epoch 1626, Loss: 0.4346396327018738, Final Batch Loss: 0.19815458357334137\n",
      "Epoch 1627, Loss: 0.41525623202323914, Final Batch Loss: 0.22798646986484528\n",
      "Epoch 1628, Loss: 0.45214423537254333, Final Batch Loss: 0.17980742454528809\n",
      "Epoch 1629, Loss: 0.4722687751054764, Final Batch Loss: 0.26772186160087585\n",
      "Epoch 1630, Loss: 0.46776457130908966, Final Batch Loss: 0.20918302237987518\n",
      "Epoch 1631, Loss: 0.4150858372449875, Final Batch Loss: 0.21028579771518707\n",
      "Epoch 1632, Loss: 0.45315197110176086, Final Batch Loss: 0.26402148604393005\n",
      "Epoch 1633, Loss: 0.4324970841407776, Final Batch Loss: 0.2312275618314743\n",
      "Epoch 1634, Loss: 0.4464878588914871, Final Batch Loss: 0.2229291945695877\n",
      "Epoch 1635, Loss: 0.38733813166618347, Final Batch Loss: 0.19505243003368378\n",
      "Epoch 1636, Loss: 0.40070414543151855, Final Batch Loss: 0.2227325290441513\n",
      "Epoch 1637, Loss: 0.3941884934902191, Final Batch Loss: 0.17669694125652313\n",
      "Epoch 1638, Loss: 0.45251840353012085, Final Batch Loss: 0.28301745653152466\n",
      "Epoch 1639, Loss: 0.4652426391839981, Final Batch Loss: 0.25027281045913696\n",
      "Epoch 1640, Loss: 0.43098802864551544, Final Batch Loss: 0.1692170649766922\n",
      "Epoch 1641, Loss: 0.40912821888923645, Final Batch Loss: 0.2157490998506546\n",
      "Epoch 1642, Loss: 0.44238902628421783, Final Batch Loss: 0.22698169946670532\n",
      "Epoch 1643, Loss: 0.4167315810918808, Final Batch Loss: 0.21006004512310028\n",
      "Epoch 1644, Loss: 0.443792387843132, Final Batch Loss: 0.24351030588150024\n",
      "Epoch 1645, Loss: 0.4475671648979187, Final Batch Loss: 0.22543959319591522\n",
      "Epoch 1646, Loss: 0.5001036673784256, Final Batch Loss: 0.23599456250667572\n",
      "Epoch 1647, Loss: 0.42236509919166565, Final Batch Loss: 0.21684841811656952\n",
      "Epoch 1648, Loss: 0.3956979066133499, Final Batch Loss: 0.1927272081375122\n",
      "Epoch 1649, Loss: 0.437039315700531, Final Batch Loss: 0.18854112923145294\n",
      "Epoch 1650, Loss: 0.4576198607683182, Final Batch Loss: 0.2357342690229416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1651, Loss: 0.37913399934768677, Final Batch Loss: 0.1590375155210495\n",
      "Epoch 1652, Loss: 0.5080809146165848, Final Batch Loss: 0.2950843870639801\n",
      "Epoch 1653, Loss: 0.4670437276363373, Final Batch Loss: 0.28002163767814636\n",
      "Epoch 1654, Loss: 0.43926505744457245, Final Batch Loss: 0.1936032772064209\n",
      "Epoch 1655, Loss: 0.44098110496997833, Final Batch Loss: 0.17553268373012543\n",
      "Epoch 1656, Loss: 0.37380558252334595, Final Batch Loss: 0.19085346162319183\n",
      "Epoch 1657, Loss: 0.4136575907468796, Final Batch Loss: 0.20069913566112518\n",
      "Epoch 1658, Loss: 0.4848456233739853, Final Batch Loss: 0.2643145024776459\n",
      "Epoch 1659, Loss: 0.41412192583084106, Final Batch Loss: 0.17449910938739777\n",
      "Epoch 1660, Loss: 0.4067395031452179, Final Batch Loss: 0.20265674591064453\n",
      "Epoch 1661, Loss: 0.4442487359046936, Final Batch Loss: 0.2227952927350998\n",
      "Epoch 1662, Loss: 0.4621233195066452, Final Batch Loss: 0.2497115284204483\n",
      "Epoch 1663, Loss: 0.44481630623340607, Final Batch Loss: 0.22592441737651825\n",
      "Epoch 1664, Loss: 0.41980570554733276, Final Batch Loss: 0.20282812416553497\n",
      "Epoch 1665, Loss: 0.45680975914001465, Final Batch Loss: 0.205710768699646\n",
      "Epoch 1666, Loss: 0.4625987857580185, Final Batch Loss: 0.21912197768688202\n",
      "Epoch 1667, Loss: 0.4647984504699707, Final Batch Loss: 0.21096038818359375\n",
      "Epoch 1668, Loss: 0.42106352746486664, Final Batch Loss: 0.23146609961986542\n",
      "Epoch 1669, Loss: 0.49068982899188995, Final Batch Loss: 0.23443900048732758\n",
      "Epoch 1670, Loss: 0.4823236018419266, Final Batch Loss: 0.27529945969581604\n",
      "Epoch 1671, Loss: 0.42602039873600006, Final Batch Loss: 0.19728226959705353\n",
      "Epoch 1672, Loss: 0.4291178584098816, Final Batch Loss: 0.19288063049316406\n",
      "Epoch 1673, Loss: 0.39938876032829285, Final Batch Loss: 0.21752016246318817\n",
      "Epoch 1674, Loss: 0.3810403198003769, Final Batch Loss: 0.18416273593902588\n",
      "Epoch 1675, Loss: 0.44119808077812195, Final Batch Loss: 0.2689792513847351\n",
      "Epoch 1676, Loss: 0.39941778779029846, Final Batch Loss: 0.187626913189888\n",
      "Epoch 1677, Loss: 0.41705088317394257, Final Batch Loss: 0.1952866166830063\n",
      "Epoch 1678, Loss: 0.4186437726020813, Final Batch Loss: 0.21008335053920746\n",
      "Epoch 1679, Loss: 0.3957848697900772, Final Batch Loss: 0.16467511653900146\n",
      "Epoch 1680, Loss: 0.4782540500164032, Final Batch Loss: 0.2565396726131439\n",
      "Epoch 1681, Loss: 0.4254782795906067, Final Batch Loss: 0.23635198175907135\n",
      "Epoch 1682, Loss: 0.4594930559396744, Final Batch Loss: 0.210309699177742\n",
      "Epoch 1683, Loss: 0.5539662092924118, Final Batch Loss: 0.32860082387924194\n",
      "Epoch 1684, Loss: 0.41421692073345184, Final Batch Loss: 0.21042458713054657\n",
      "Epoch 1685, Loss: 0.39073002338409424, Final Batch Loss: 0.1569679081439972\n",
      "Epoch 1686, Loss: 0.39139480888843536, Final Batch Loss: 0.17354948818683624\n",
      "Epoch 1687, Loss: 0.4191797971725464, Final Batch Loss: 0.22417299449443817\n",
      "Epoch 1688, Loss: 0.3743821978569031, Final Batch Loss: 0.15003636479377747\n",
      "Epoch 1689, Loss: 0.43109284341335297, Final Batch Loss: 0.21696655452251434\n",
      "Epoch 1690, Loss: 0.43854960799217224, Final Batch Loss: 0.2006140798330307\n",
      "Epoch 1691, Loss: 0.39168597757816315, Final Batch Loss: 0.21848611533641815\n",
      "Epoch 1692, Loss: 0.37688401341438293, Final Batch Loss: 0.2050156146287918\n",
      "Epoch 1693, Loss: 0.41355371475219727, Final Batch Loss: 0.20378802716732025\n",
      "Epoch 1694, Loss: 0.44482070207595825, Final Batch Loss: 0.254794716835022\n",
      "Epoch 1695, Loss: 0.43456292152404785, Final Batch Loss: 0.23015780746936798\n",
      "Epoch 1696, Loss: 0.38477422297000885, Final Batch Loss: 0.18053346872329712\n",
      "Epoch 1697, Loss: 0.40182816982269287, Final Batch Loss: 0.19374771416187286\n",
      "Epoch 1698, Loss: 0.444465696811676, Final Batch Loss: 0.20986799895763397\n",
      "Epoch 1699, Loss: 0.4010300040245056, Final Batch Loss: 0.19093744456768036\n",
      "Epoch 1700, Loss: 0.3926650732755661, Final Batch Loss: 0.19173842668533325\n",
      "Epoch 1701, Loss: 0.4678714871406555, Final Batch Loss: 0.2119361162185669\n",
      "Epoch 1702, Loss: 0.3783602714538574, Final Batch Loss: 0.19063977897167206\n",
      "Epoch 1703, Loss: 0.4454557001590729, Final Batch Loss: 0.21374885737895966\n",
      "Epoch 1704, Loss: 0.38527506589889526, Final Batch Loss: 0.2188165783882141\n",
      "Epoch 1705, Loss: 0.412226140499115, Final Batch Loss: 0.2081616073846817\n",
      "Epoch 1706, Loss: 0.41863366961479187, Final Batch Loss: 0.20019306242465973\n",
      "Epoch 1707, Loss: 0.4726863354444504, Final Batch Loss: 0.21969275176525116\n",
      "Epoch 1708, Loss: 0.4284762889146805, Final Batch Loss: 0.16755522787570953\n",
      "Epoch 1709, Loss: 0.41017986834049225, Final Batch Loss: 0.2186458557844162\n",
      "Epoch 1710, Loss: 0.44998133182525635, Final Batch Loss: 0.20297838747501373\n",
      "Epoch 1711, Loss: 0.4281233698129654, Final Batch Loss: 0.19528035819530487\n",
      "Epoch 1712, Loss: 0.4235696941614151, Final Batch Loss: 0.21209152042865753\n",
      "Epoch 1713, Loss: 0.43867503106594086, Final Batch Loss: 0.21649575233459473\n",
      "Epoch 1714, Loss: 0.38698922097682953, Final Batch Loss: 0.17516708374023438\n",
      "Epoch 1715, Loss: 0.40798917412757874, Final Batch Loss: 0.19366419315338135\n",
      "Epoch 1716, Loss: 0.4582473635673523, Final Batch Loss: 0.21010707318782806\n",
      "Epoch 1717, Loss: 0.40847761929035187, Final Batch Loss: 0.19761210680007935\n",
      "Epoch 1718, Loss: 0.40047986805438995, Final Batch Loss: 0.16065804660320282\n",
      "Epoch 1719, Loss: 0.466635063290596, Final Batch Loss: 0.226323202252388\n",
      "Epoch 1720, Loss: 0.41583284735679626, Final Batch Loss: 0.2221878618001938\n",
      "Epoch 1721, Loss: 0.4411644786596298, Final Batch Loss: 0.22211682796478271\n",
      "Epoch 1722, Loss: 0.3920547515153885, Final Batch Loss: 0.20069247484207153\n",
      "Epoch 1723, Loss: 0.4137400835752487, Final Batch Loss: 0.19030095636844635\n",
      "Epoch 1724, Loss: 0.3889010548591614, Final Batch Loss: 0.23661263287067413\n",
      "Epoch 1725, Loss: 0.4044042378664017, Final Batch Loss: 0.18966417014598846\n",
      "Epoch 1726, Loss: 0.4139730781316757, Final Batch Loss: 0.1613769382238388\n",
      "Epoch 1727, Loss: 0.44215865433216095, Final Batch Loss: 0.2336425632238388\n",
      "Epoch 1728, Loss: 0.41166625916957855, Final Batch Loss: 0.1843024641275406\n",
      "Epoch 1729, Loss: 0.4305417835712433, Final Batch Loss: 0.21258805692195892\n",
      "Epoch 1730, Loss: 0.4351179599761963, Final Batch Loss: 0.24596810340881348\n",
      "Epoch 1731, Loss: 0.45684030652046204, Final Batch Loss: 0.2390577793121338\n",
      "Epoch 1732, Loss: 0.38086436688899994, Final Batch Loss: 0.18993651866912842\n",
      "Epoch 1733, Loss: 0.4330063462257385, Final Batch Loss: 0.19547419250011444\n",
      "Epoch 1734, Loss: 0.40888483822345734, Final Batch Loss: 0.2228947877883911\n",
      "Epoch 1735, Loss: 0.38025234639644623, Final Batch Loss: 0.18108272552490234\n",
      "Epoch 1736, Loss: 0.43429750204086304, Final Batch Loss: 0.2093701809644699\n",
      "Epoch 1737, Loss: 0.4520917236804962, Final Batch Loss: 0.18934917449951172\n",
      "Epoch 1738, Loss: 0.47123268246650696, Final Batch Loss: 0.26067426800727844\n",
      "Epoch 1739, Loss: 0.3995508849620819, Final Batch Loss: 0.2259495109319687\n",
      "Epoch 1740, Loss: 0.4108341187238693, Final Batch Loss: 0.20842455327510834\n",
      "Epoch 1741, Loss: 0.4181956350803375, Final Batch Loss: 0.1700461506843567\n",
      "Epoch 1742, Loss: 0.41444528102874756, Final Batch Loss: 0.18818658590316772\n",
      "Epoch 1743, Loss: 0.5103633105754852, Final Batch Loss: 0.19631534814834595\n",
      "Epoch 1744, Loss: 0.40272335708141327, Final Batch Loss: 0.2266598790884018\n",
      "Epoch 1745, Loss: 0.40044310688972473, Final Batch Loss: 0.19296707212924957\n",
      "Epoch 1746, Loss: 0.41274960339069366, Final Batch Loss: 0.18425975739955902\n",
      "Epoch 1747, Loss: 0.4056558609008789, Final Batch Loss: 0.19124527275562286\n",
      "Epoch 1748, Loss: 0.42719167470932007, Final Batch Loss: 0.22281555831432343\n",
      "Epoch 1749, Loss: 0.39908428490161896, Final Batch Loss: 0.1916406750679016\n",
      "Epoch 1750, Loss: 0.39206480979919434, Final Batch Loss: 0.1660051792860031\n",
      "Epoch 1751, Loss: 0.4298772066831589, Final Batch Loss: 0.20143043994903564\n",
      "Epoch 1752, Loss: 0.4108162075281143, Final Batch Loss: 0.21134088933467865\n",
      "Epoch 1753, Loss: 0.4207909554243088, Final Batch Loss: 0.21481896936893463\n",
      "Epoch 1754, Loss: 0.38274721801280975, Final Batch Loss: 0.17892992496490479\n",
      "Epoch 1755, Loss: 0.44683027267456055, Final Batch Loss: 0.2574821710586548\n",
      "Epoch 1756, Loss: 0.35023704171180725, Final Batch Loss: 0.1701502799987793\n",
      "Epoch 1757, Loss: 0.4316502809524536, Final Batch Loss: 0.20830053091049194\n",
      "Epoch 1758, Loss: 0.395487979054451, Final Batch Loss: 0.19123005867004395\n",
      "Epoch 1759, Loss: 0.410801038146019, Final Batch Loss: 0.22377406060695648\n",
      "Epoch 1760, Loss: 0.39540889859199524, Final Batch Loss: 0.16815131902694702\n",
      "Epoch 1761, Loss: 0.43673408031463623, Final Batch Loss: 0.23634731769561768\n",
      "Epoch 1762, Loss: 0.401018425822258, Final Batch Loss: 0.20281316339969635\n",
      "Epoch 1763, Loss: 0.4067303240299225, Final Batch Loss: 0.21002157032489777\n",
      "Epoch 1764, Loss: 0.46177515387535095, Final Batch Loss: 0.252936452627182\n",
      "Epoch 1765, Loss: 0.3877834677696228, Final Batch Loss: 0.20764587819576263\n",
      "Epoch 1766, Loss: 0.4313446879386902, Final Batch Loss: 0.20879708230495453\n",
      "Epoch 1767, Loss: 0.4401678740978241, Final Batch Loss: 0.2188069224357605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1768, Loss: 0.3967844992876053, Final Batch Loss: 0.17388559877872467\n",
      "Epoch 1769, Loss: 0.5118869096040726, Final Batch Loss: 0.3444254398345947\n",
      "Epoch 1770, Loss: 0.4147878885269165, Final Batch Loss: 0.1776912808418274\n",
      "Epoch 1771, Loss: 0.43596555292606354, Final Batch Loss: 0.20753039419651031\n",
      "Epoch 1772, Loss: 0.43326912820339203, Final Batch Loss: 0.24071526527404785\n",
      "Epoch 1773, Loss: 0.42204006016254425, Final Batch Loss: 0.23484182357788086\n",
      "Epoch 1774, Loss: 0.4146297127008438, Final Batch Loss: 0.2069018930196762\n",
      "Epoch 1775, Loss: 0.39862535893917084, Final Batch Loss: 0.1756332665681839\n",
      "Epoch 1776, Loss: 0.44900283217430115, Final Batch Loss: 0.20535653829574585\n",
      "Epoch 1777, Loss: 0.42240943014621735, Final Batch Loss: 0.1870885044336319\n",
      "Epoch 1778, Loss: 0.4873802214860916, Final Batch Loss: 0.26247116923332214\n",
      "Epoch 1779, Loss: 0.48195821046829224, Final Batch Loss: 0.25175240635871887\n",
      "Epoch 1780, Loss: 0.4371412843465805, Final Batch Loss: 0.1970454603433609\n",
      "Epoch 1781, Loss: 0.38688333332538605, Final Batch Loss: 0.17538684606552124\n",
      "Epoch 1782, Loss: 0.4456324726343155, Final Batch Loss: 0.22365979850292206\n",
      "Epoch 1783, Loss: 0.42872340977191925, Final Batch Loss: 0.17308087646961212\n",
      "Epoch 1784, Loss: 0.4366060793399811, Final Batch Loss: 0.19792483747005463\n",
      "Epoch 1785, Loss: 0.40643320977687836, Final Batch Loss: 0.18530923128128052\n",
      "Epoch 1786, Loss: 0.3932466208934784, Final Batch Loss: 0.20509718358516693\n",
      "Epoch 1787, Loss: 0.3636956363916397, Final Batch Loss: 0.21207280457019806\n",
      "Epoch 1788, Loss: 0.42786404490470886, Final Batch Loss: 0.24640627205371857\n",
      "Epoch 1789, Loss: 0.3891254812479019, Final Batch Loss: 0.16335558891296387\n",
      "Epoch 1790, Loss: 0.4117514342069626, Final Batch Loss: 0.19988584518432617\n",
      "Epoch 1791, Loss: 0.4153438061475754, Final Batch Loss: 0.18913836777210236\n",
      "Epoch 1792, Loss: 0.40250061452388763, Final Batch Loss: 0.24291031062602997\n",
      "Epoch 1793, Loss: 0.37643158435821533, Final Batch Loss: 0.17404599487781525\n",
      "Epoch 1794, Loss: 0.35625386238098145, Final Batch Loss: 0.14686822891235352\n",
      "Epoch 1795, Loss: 0.43493182957172394, Final Batch Loss: 0.19027765095233917\n",
      "Epoch 1796, Loss: 0.42780502140522003, Final Batch Loss: 0.19479860365390778\n",
      "Epoch 1797, Loss: 0.4525420665740967, Final Batch Loss: 0.23875625431537628\n",
      "Epoch 1798, Loss: 0.43181154131889343, Final Batch Loss: 0.2424982339143753\n",
      "Epoch 1799, Loss: 0.398288294672966, Final Batch Loss: 0.17591671645641327\n",
      "Epoch 1800, Loss: 0.45637065172195435, Final Batch Loss: 0.2175685614347458\n",
      "Epoch 1801, Loss: 0.4171130657196045, Final Batch Loss: 0.1736672967672348\n",
      "Epoch 1802, Loss: 0.38875405490398407, Final Batch Loss: 0.1905314177274704\n",
      "Epoch 1803, Loss: 0.43691109120845795, Final Batch Loss: 0.2209419459104538\n",
      "Epoch 1804, Loss: 0.39751607179641724, Final Batch Loss: 0.1789184808731079\n",
      "Epoch 1805, Loss: 0.4018677920103073, Final Batch Loss: 0.2034086138010025\n",
      "Epoch 1806, Loss: 0.3864099532365799, Final Batch Loss: 0.21194885671138763\n",
      "Epoch 1807, Loss: 0.5211281478404999, Final Batch Loss: 0.3032498061656952\n",
      "Epoch 1808, Loss: 0.4112446457147598, Final Batch Loss: 0.21171879768371582\n",
      "Epoch 1809, Loss: 0.41187213361263275, Final Batch Loss: 0.24840115010738373\n",
      "Epoch 1810, Loss: 0.4005267024040222, Final Batch Loss: 0.2196742743253708\n",
      "Epoch 1811, Loss: 0.4403187930583954, Final Batch Loss: 0.19998304545879364\n",
      "Epoch 1812, Loss: 0.39654047787189484, Final Batch Loss: 0.20394687354564667\n",
      "Epoch 1813, Loss: 0.40098273754119873, Final Batch Loss: 0.2089242786169052\n",
      "Epoch 1814, Loss: 0.38885660469532013, Final Batch Loss: 0.182170569896698\n",
      "Epoch 1815, Loss: 0.4419036954641342, Final Batch Loss: 0.22395212948322296\n",
      "Epoch 1816, Loss: 0.37384122610092163, Final Batch Loss: 0.21490955352783203\n",
      "Epoch 1817, Loss: 0.450107142329216, Final Batch Loss: 0.19094766676425934\n",
      "Epoch 1818, Loss: 0.4390737861394882, Final Batch Loss: 0.22121012210845947\n",
      "Epoch 1819, Loss: 0.39745737612247467, Final Batch Loss: 0.18914596736431122\n",
      "Epoch 1820, Loss: 0.43525025248527527, Final Batch Loss: 0.209181010723114\n",
      "Epoch 1821, Loss: 0.42281217873096466, Final Batch Loss: 0.2244524359703064\n",
      "Epoch 1822, Loss: 0.4123018682003021, Final Batch Loss: 0.1746881753206253\n",
      "Epoch 1823, Loss: 0.43752115964889526, Final Batch Loss: 0.18093591928482056\n",
      "Epoch 1824, Loss: 0.46465590596199036, Final Batch Loss: 0.23269915580749512\n",
      "Epoch 1825, Loss: 0.37096041440963745, Final Batch Loss: 0.2026912122964859\n",
      "Epoch 1826, Loss: 0.4786376357078552, Final Batch Loss: 0.20490312576293945\n",
      "Epoch 1827, Loss: 0.42867594957351685, Final Batch Loss: 0.2309785634279251\n",
      "Epoch 1828, Loss: 0.4234221875667572, Final Batch Loss: 0.20325039327144623\n",
      "Epoch 1829, Loss: 0.42208802700042725, Final Batch Loss: 0.2239181399345398\n",
      "Epoch 1830, Loss: 0.4281427413225174, Final Batch Loss: 0.28217431902885437\n",
      "Epoch 1831, Loss: 0.39132000505924225, Final Batch Loss: 0.1680738776922226\n",
      "Epoch 1832, Loss: 0.4284084439277649, Final Batch Loss: 0.23240013420581818\n",
      "Epoch 1833, Loss: 0.4010286033153534, Final Batch Loss: 0.22231777012348175\n",
      "Epoch 1834, Loss: 0.44460445642471313, Final Batch Loss: 0.25533419847488403\n",
      "Epoch 1835, Loss: 0.42711828649044037, Final Batch Loss: 0.2437390685081482\n",
      "Epoch 1836, Loss: 0.4104433059692383, Final Batch Loss: 0.198556587100029\n",
      "Epoch 1837, Loss: 0.4222262352705002, Final Batch Loss: 0.23490267992019653\n",
      "Epoch 1838, Loss: 0.40272197127342224, Final Batch Loss: 0.17841510474681854\n",
      "Epoch 1839, Loss: 0.40680789947509766, Final Batch Loss: 0.17966675758361816\n",
      "Epoch 1840, Loss: 0.42664702236652374, Final Batch Loss: 0.20675022900104523\n",
      "Epoch 1841, Loss: 0.362326979637146, Final Batch Loss: 0.15299426019191742\n",
      "Epoch 1842, Loss: 0.3987401872873306, Final Batch Loss: 0.14390437304973602\n",
      "Epoch 1843, Loss: 0.4422188848257065, Final Batch Loss: 0.213511660695076\n",
      "Epoch 1844, Loss: 0.42972631752491, Final Batch Loss: 0.19737966358661652\n",
      "Epoch 1845, Loss: 0.39341628551483154, Final Batch Loss: 0.1900509148836136\n",
      "Epoch 1846, Loss: 0.40704213082790375, Final Batch Loss: 0.19580958783626556\n",
      "Epoch 1847, Loss: 0.4425457566976547, Final Batch Loss: 0.22006340324878693\n",
      "Epoch 1848, Loss: 0.4022553712129593, Final Batch Loss: 0.19446367025375366\n",
      "Epoch 1849, Loss: 0.4120553433895111, Final Batch Loss: 0.16502715647220612\n",
      "Epoch 1850, Loss: 0.41623905301094055, Final Batch Loss: 0.22990326583385468\n",
      "Epoch 1851, Loss: 0.37505824863910675, Final Batch Loss: 0.18305158615112305\n",
      "Epoch 1852, Loss: 0.4029508978128433, Final Batch Loss: 0.21237491071224213\n",
      "Epoch 1853, Loss: 0.37101857364177704, Final Batch Loss: 0.16732554137706757\n",
      "Epoch 1854, Loss: 0.47256065905094147, Final Batch Loss: 0.26592737436294556\n",
      "Epoch 1855, Loss: 0.44363483786582947, Final Batch Loss: 0.2559351623058319\n",
      "Epoch 1856, Loss: 0.45760346949100494, Final Batch Loss: 0.27444031834602356\n",
      "Epoch 1857, Loss: 0.3736436814069748, Final Batch Loss: 0.1900307685136795\n",
      "Epoch 1858, Loss: 0.3998238891363144, Final Batch Loss: 0.16911013424396515\n",
      "Epoch 1859, Loss: 0.462055966258049, Final Batch Loss: 0.22862084209918976\n",
      "Epoch 1860, Loss: 0.40677373111248016, Final Batch Loss: 0.22205395996570587\n",
      "Epoch 1861, Loss: 0.40145011246204376, Final Batch Loss: 0.22676432132720947\n",
      "Epoch 1862, Loss: 0.43253591656684875, Final Batch Loss: 0.2204926759004593\n",
      "Epoch 1863, Loss: 0.4128803163766861, Final Batch Loss: 0.17624294757843018\n",
      "Epoch 1864, Loss: 0.39708657562732697, Final Batch Loss: 0.18193691968917847\n",
      "Epoch 1865, Loss: 0.3925023376941681, Final Batch Loss: 0.18282796442508698\n",
      "Epoch 1866, Loss: 0.41254156827926636, Final Batch Loss: 0.21724843978881836\n",
      "Epoch 1867, Loss: 0.3807252049446106, Final Batch Loss: 0.2073339819908142\n",
      "Epoch 1868, Loss: 0.41986308991909027, Final Batch Loss: 0.21748842298984528\n",
      "Epoch 1869, Loss: 0.4305824935436249, Final Batch Loss: 0.21548350155353546\n",
      "Epoch 1870, Loss: 0.4306299090385437, Final Batch Loss: 0.24748694896697998\n",
      "Epoch 1871, Loss: 0.4019821584224701, Final Batch Loss: 0.1891229897737503\n",
      "Epoch 1872, Loss: 0.39087508618831635, Final Batch Loss: 0.19655318558216095\n",
      "Epoch 1873, Loss: 0.37622474133968353, Final Batch Loss: 0.16319479048252106\n",
      "Epoch 1874, Loss: 0.4685005694627762, Final Batch Loss: 0.23092375695705414\n",
      "Epoch 1875, Loss: 0.4040869027376175, Final Batch Loss: 0.1834106296300888\n",
      "Epoch 1876, Loss: 0.3924823999404907, Final Batch Loss: 0.20978355407714844\n",
      "Epoch 1877, Loss: 0.39965690672397614, Final Batch Loss: 0.18450643122196198\n",
      "Epoch 1878, Loss: 0.44095614552497864, Final Batch Loss: 0.2683620750904083\n",
      "Epoch 1879, Loss: 0.44630834460258484, Final Batch Loss: 0.2771780788898468\n",
      "Epoch 1880, Loss: 0.3802812397480011, Final Batch Loss: 0.2111791968345642\n",
      "Epoch 1881, Loss: 0.399765208363533, Final Batch Loss: 0.21056301891803741\n",
      "Epoch 1882, Loss: 0.42840273678302765, Final Batch Loss: 0.2111368328332901\n",
      "Epoch 1883, Loss: 0.3935109078884125, Final Batch Loss: 0.1980484575033188\n",
      "Epoch 1884, Loss: 0.37445585429668427, Final Batch Loss: 0.18811172246932983\n",
      "Epoch 1885, Loss: 0.35763102769851685, Final Batch Loss: 0.1530219465494156\n",
      "Epoch 1886, Loss: 0.40320615470409393, Final Batch Loss: 0.19638091325759888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1887, Loss: 0.3961733728647232, Final Batch Loss: 0.2473716139793396\n",
      "Epoch 1888, Loss: 0.4009343087673187, Final Batch Loss: 0.15770480036735535\n",
      "Epoch 1889, Loss: 0.40735098719596863, Final Batch Loss: 0.2120947688817978\n",
      "Epoch 1890, Loss: 0.37318477034568787, Final Batch Loss: 0.1800266057252884\n",
      "Epoch 1891, Loss: 0.39780282974243164, Final Batch Loss: 0.18071185052394867\n",
      "Epoch 1892, Loss: 0.39750805497169495, Final Batch Loss: 0.18890760838985443\n",
      "Epoch 1893, Loss: 0.4573846310377121, Final Batch Loss: 0.22458668053150177\n",
      "Epoch 1894, Loss: 0.368260458111763, Final Batch Loss: 0.18254999816417694\n",
      "Epoch 1895, Loss: 0.4043097198009491, Final Batch Loss: 0.17928248643875122\n",
      "Epoch 1896, Loss: 0.4301746040582657, Final Batch Loss: 0.1982375830411911\n",
      "Epoch 1897, Loss: 0.4538256675004959, Final Batch Loss: 0.239236518740654\n",
      "Epoch 1898, Loss: 0.4127819687128067, Final Batch Loss: 0.21063150465488434\n",
      "Epoch 1899, Loss: 0.41927511990070343, Final Batch Loss: 0.2529834806919098\n",
      "Epoch 1900, Loss: 0.39599137008190155, Final Batch Loss: 0.2031896859407425\n",
      "Epoch 1901, Loss: 0.41156379878520966, Final Batch Loss: 0.22426925599575043\n",
      "Epoch 1902, Loss: 0.4133262634277344, Final Batch Loss: 0.18867211043834686\n",
      "Epoch 1903, Loss: 0.39641351997852325, Final Batch Loss: 0.18457229435443878\n",
      "Epoch 1904, Loss: 0.402376264333725, Final Batch Loss: 0.21783125400543213\n",
      "Epoch 1905, Loss: 0.38046395778656006, Final Batch Loss: 0.16616269946098328\n",
      "Epoch 1906, Loss: 0.4611806720495224, Final Batch Loss: 0.2519550323486328\n",
      "Epoch 1907, Loss: 0.429919108748436, Final Batch Loss: 0.1774834543466568\n",
      "Epoch 1908, Loss: 0.3874495327472687, Final Batch Loss: 0.16909684240818024\n",
      "Epoch 1909, Loss: 0.396974578499794, Final Batch Loss: 0.2141755372285843\n",
      "Epoch 1910, Loss: 0.4155431240797043, Final Batch Loss: 0.22395890951156616\n",
      "Epoch 1911, Loss: 0.41242721676826477, Final Batch Loss: 0.2231220006942749\n",
      "Epoch 1912, Loss: 0.4079987555742264, Final Batch Loss: 0.16526654362678528\n",
      "Epoch 1913, Loss: 0.4438563138246536, Final Batch Loss: 0.2417418211698532\n",
      "Epoch 1914, Loss: 0.4155687242746353, Final Batch Loss: 0.1963633894920349\n",
      "Epoch 1915, Loss: 0.4199175089597702, Final Batch Loss: 0.19505488872528076\n",
      "Epoch 1916, Loss: 0.40627433359622955, Final Batch Loss: 0.24255718290805817\n",
      "Epoch 1917, Loss: 0.37481173872947693, Final Batch Loss: 0.19784246385097504\n",
      "Epoch 1918, Loss: 0.36749428510665894, Final Batch Loss: 0.20513619482517242\n",
      "Epoch 1919, Loss: 0.43477386236190796, Final Batch Loss: 0.2174442559480667\n",
      "Epoch 1920, Loss: 0.42719000577926636, Final Batch Loss: 0.21642069518566132\n",
      "Epoch 1921, Loss: 0.39969807863235474, Final Batch Loss: 0.20219022035598755\n",
      "Epoch 1922, Loss: 0.47159911692142487, Final Batch Loss: 0.21316365897655487\n",
      "Epoch 1923, Loss: 0.4952392131090164, Final Batch Loss: 0.27331793308258057\n",
      "Epoch 1924, Loss: 0.3698562681674957, Final Batch Loss: 0.14827772974967957\n",
      "Epoch 1925, Loss: 0.3706437200307846, Final Batch Loss: 0.19413156807422638\n",
      "Epoch 1926, Loss: 0.4365055412054062, Final Batch Loss: 0.22435224056243896\n",
      "Epoch 1927, Loss: 0.4044102877378464, Final Batch Loss: 0.21470917761325836\n",
      "Epoch 1928, Loss: 0.497570738196373, Final Batch Loss: 0.16369734704494476\n",
      "Epoch 1929, Loss: 0.44912952184677124, Final Batch Loss: 0.23447509109973907\n",
      "Epoch 1930, Loss: 0.3837810307741165, Final Batch Loss: 0.20593935251235962\n",
      "Epoch 1931, Loss: 0.37430182099342346, Final Batch Loss: 0.17933964729309082\n",
      "Epoch 1932, Loss: 0.39040976762771606, Final Batch Loss: 0.19559688866138458\n",
      "Epoch 1933, Loss: 0.40915119647979736, Final Batch Loss: 0.20703744888305664\n",
      "Epoch 1934, Loss: 0.43190111219882965, Final Batch Loss: 0.18178920447826385\n",
      "Epoch 1935, Loss: 0.45551255345344543, Final Batch Loss: 0.20564572513103485\n",
      "Epoch 1936, Loss: 0.3968394845724106, Final Batch Loss: 0.20297187566757202\n",
      "Epoch 1937, Loss: 0.3915908485651016, Final Batch Loss: 0.1812313348054886\n",
      "Epoch 1938, Loss: 0.4048907458782196, Final Batch Loss: 0.17952857911586761\n",
      "Epoch 1939, Loss: 0.38967232406139374, Final Batch Loss: 0.18625302612781525\n",
      "Epoch 1940, Loss: 0.39568789303302765, Final Batch Loss: 0.15139643847942352\n",
      "Epoch 1941, Loss: 0.3726511299610138, Final Batch Loss: 0.19219838082790375\n",
      "Epoch 1942, Loss: 0.39495833218097687, Final Batch Loss: 0.19025903940200806\n",
      "Epoch 1943, Loss: 0.36154937744140625, Final Batch Loss: 0.1889907866716385\n",
      "Epoch 1944, Loss: 0.4018198698759079, Final Batch Loss: 0.23907321691513062\n",
      "Epoch 1945, Loss: 0.40920114517211914, Final Batch Loss: 0.2009350210428238\n",
      "Epoch 1946, Loss: 0.3887845128774643, Final Batch Loss: 0.19519396126270294\n",
      "Epoch 1947, Loss: 0.37894904613494873, Final Batch Loss: 0.18505311012268066\n",
      "Epoch 1948, Loss: 0.43928155303001404, Final Batch Loss: 0.25202083587646484\n",
      "Epoch 1949, Loss: 0.41425345838069916, Final Batch Loss: 0.2450726181268692\n",
      "Epoch 1950, Loss: 0.38823360204696655, Final Batch Loss: 0.19522009789943695\n",
      "Epoch 1951, Loss: 0.3816099464893341, Final Batch Loss: 0.20052509009838104\n",
      "Epoch 1952, Loss: 0.39107315242290497, Final Batch Loss: 0.19512856006622314\n",
      "Epoch 1953, Loss: 0.41405729949474335, Final Batch Loss: 0.22161002457141876\n",
      "Epoch 1954, Loss: 0.3855622857809067, Final Batch Loss: 0.14094935357570648\n",
      "Epoch 1955, Loss: 0.38562415540218353, Final Batch Loss: 0.18914788961410522\n",
      "Epoch 1956, Loss: 0.4195540100336075, Final Batch Loss: 0.21670396625995636\n",
      "Epoch 1957, Loss: 0.3637416660785675, Final Batch Loss: 0.20261244475841522\n",
      "Epoch 1958, Loss: 0.3714616745710373, Final Batch Loss: 0.21056948602199554\n",
      "Epoch 1959, Loss: 0.40435759723186493, Final Batch Loss: 0.21209442615509033\n",
      "Epoch 1960, Loss: 0.48350243270397186, Final Batch Loss: 0.28018805384635925\n",
      "Epoch 1961, Loss: 0.43382303416728973, Final Batch Loss: 0.22369658946990967\n",
      "Epoch 1962, Loss: 0.3944335877895355, Final Batch Loss: 0.19401486217975616\n",
      "Epoch 1963, Loss: 0.4194249212741852, Final Batch Loss: 0.22393600642681122\n",
      "Epoch 1964, Loss: 0.4577621966600418, Final Batch Loss: 0.1786188930273056\n",
      "Epoch 1965, Loss: 0.3997303545475006, Final Batch Loss: 0.20116256177425385\n",
      "Epoch 1966, Loss: 0.4093429446220398, Final Batch Loss: 0.21613019704818726\n",
      "Epoch 1967, Loss: 0.3957166522741318, Final Batch Loss: 0.1820356398820877\n",
      "Epoch 1968, Loss: 0.3879487216472626, Final Batch Loss: 0.22127841413021088\n",
      "Epoch 1969, Loss: 0.4166429340839386, Final Batch Loss: 0.2213865965604782\n",
      "Epoch 1970, Loss: 0.43485355377197266, Final Batch Loss: 0.18746767938137054\n",
      "Epoch 1971, Loss: 0.43241824209690094, Final Batch Loss: 0.18834583461284637\n",
      "Epoch 1972, Loss: 0.36171840131282806, Final Batch Loss: 0.17357005178928375\n",
      "Epoch 1973, Loss: 0.39720770716667175, Final Batch Loss: 0.19569675624370575\n",
      "Epoch 1974, Loss: 0.4123010188341141, Final Batch Loss: 0.21750850975513458\n",
      "Epoch 1975, Loss: 0.38300563395023346, Final Batch Loss: 0.1879517287015915\n",
      "Epoch 1976, Loss: 0.43342891335487366, Final Batch Loss: 0.229328915476799\n",
      "Epoch 1977, Loss: 0.4429696351289749, Final Batch Loss: 0.2471255213022232\n",
      "Epoch 1978, Loss: 0.43505868315696716, Final Batch Loss: 0.2359076738357544\n",
      "Epoch 1979, Loss: 0.4541117995977402, Final Batch Loss: 0.264806866645813\n",
      "Epoch 1980, Loss: 0.4432084858417511, Final Batch Loss: 0.19177979230880737\n",
      "Epoch 1981, Loss: 0.43342071771621704, Final Batch Loss: 0.21039903163909912\n",
      "Epoch 1982, Loss: 0.39657455682754517, Final Batch Loss: 0.19288486242294312\n",
      "Epoch 1983, Loss: 0.40209630131721497, Final Batch Loss: 0.203149676322937\n",
      "Epoch 1984, Loss: 0.38223758339881897, Final Batch Loss: 0.17036356031894684\n",
      "Epoch 1985, Loss: 0.36341285705566406, Final Batch Loss: 0.19541220366954803\n",
      "Epoch 1986, Loss: 0.4192027598619461, Final Batch Loss: 0.22145193815231323\n",
      "Epoch 1987, Loss: 0.3812881261110306, Final Batch Loss: 0.15045732259750366\n",
      "Epoch 1988, Loss: 0.3654744476079941, Final Batch Loss: 0.19750666618347168\n",
      "Epoch 1989, Loss: 0.4012231230735779, Final Batch Loss: 0.20498760044574738\n",
      "Epoch 1990, Loss: 0.3968167304992676, Final Batch Loss: 0.1863616704940796\n",
      "Epoch 1991, Loss: 0.4169530123472214, Final Batch Loss: 0.18301159143447876\n",
      "Epoch 1992, Loss: 0.36106041073799133, Final Batch Loss: 0.16562409698963165\n",
      "Epoch 1993, Loss: 0.3637387603521347, Final Batch Loss: 0.19955192506313324\n",
      "Epoch 1994, Loss: 0.3783172369003296, Final Batch Loss: 0.17238843441009521\n",
      "Epoch 1995, Loss: 0.39377328753471375, Final Batch Loss: 0.17124521732330322\n",
      "Epoch 1996, Loss: 0.393735870718956, Final Batch Loss: 0.23712654411792755\n",
      "Epoch 1997, Loss: 0.3964158445596695, Final Batch Loss: 0.19512438774108887\n",
      "Epoch 1998, Loss: 0.4141509085893631, Final Batch Loss: 0.20597587525844574\n",
      "Epoch 1999, Loss: 0.40000513195991516, Final Batch Loss: 0.2188934087753296\n",
      "Epoch 2000, Loss: 0.3679121136665344, Final Batch Loss: 0.17674560844898224\n",
      "Epoch 2001, Loss: 0.4074333757162094, Final Batch Loss: 0.21411047875881195\n",
      "Epoch 2002, Loss: 0.36233265697956085, Final Batch Loss: 0.21849621832370758\n",
      "Epoch 2003, Loss: 0.408054456114769, Final Batch Loss: 0.21321998536586761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2004, Loss: 0.3656444102525711, Final Batch Loss: 0.1956886649131775\n",
      "Epoch 2005, Loss: 0.3784017860889435, Final Batch Loss: 0.16961856186389923\n",
      "Epoch 2006, Loss: 0.359652042388916, Final Batch Loss: 0.18787850439548492\n",
      "Epoch 2007, Loss: 0.42274533212184906, Final Batch Loss: 0.19321776926517487\n",
      "Epoch 2008, Loss: 0.3757627010345459, Final Batch Loss: 0.1728484183549881\n",
      "Epoch 2009, Loss: 0.3985700458288193, Final Batch Loss: 0.2048397809267044\n",
      "Epoch 2010, Loss: 0.3874688446521759, Final Batch Loss: 0.17397058010101318\n",
      "Epoch 2011, Loss: 0.39829976856708527, Final Batch Loss: 0.1686452180147171\n",
      "Epoch 2012, Loss: 0.4185849130153656, Final Batch Loss: 0.27540475130081177\n",
      "Epoch 2013, Loss: 0.39741159975528717, Final Batch Loss: 0.14663980901241302\n",
      "Epoch 2014, Loss: 0.39100123941898346, Final Batch Loss: 0.20548991858959198\n",
      "Epoch 2015, Loss: 0.42062436044216156, Final Batch Loss: 0.2054220288991928\n",
      "Epoch 2016, Loss: 0.38235436379909515, Final Batch Loss: 0.1466997116804123\n",
      "Epoch 2017, Loss: 0.4579017609357834, Final Batch Loss: 0.23458559811115265\n",
      "Epoch 2018, Loss: 0.3460472822189331, Final Batch Loss: 0.1813543438911438\n",
      "Epoch 2019, Loss: 0.4021946042776108, Final Batch Loss: 0.21048212051391602\n",
      "Epoch 2020, Loss: 0.38598543405532837, Final Batch Loss: 0.19500239193439484\n",
      "Epoch 2021, Loss: 0.41417525708675385, Final Batch Loss: 0.24142120778560638\n",
      "Epoch 2022, Loss: 0.3701190799474716, Final Batch Loss: 0.18471449613571167\n",
      "Epoch 2023, Loss: 0.4044196307659149, Final Batch Loss: 0.18353696167469025\n",
      "Epoch 2024, Loss: 0.43709132075309753, Final Batch Loss: 0.19321884214878082\n",
      "Epoch 2025, Loss: 0.4214271456003189, Final Batch Loss: 0.2091367244720459\n",
      "Epoch 2026, Loss: 0.35252729058265686, Final Batch Loss: 0.19437237083911896\n",
      "Epoch 2027, Loss: 0.36931057274341583, Final Batch Loss: 0.1859271377325058\n",
      "Epoch 2028, Loss: 0.38767658174037933, Final Batch Loss: 0.21711339056491852\n",
      "Epoch 2029, Loss: 0.4006229043006897, Final Batch Loss: 0.23797743022441864\n",
      "Epoch 2030, Loss: 0.39406174421310425, Final Batch Loss: 0.2159922868013382\n",
      "Epoch 2031, Loss: 0.3846129924058914, Final Batch Loss: 0.21957962214946747\n",
      "Epoch 2032, Loss: 0.4666072130203247, Final Batch Loss: 0.269438773393631\n",
      "Epoch 2033, Loss: 0.40720295906066895, Final Batch Loss: 0.20843400061130524\n",
      "Epoch 2034, Loss: 0.38899798691272736, Final Batch Loss: 0.1958727240562439\n",
      "Epoch 2035, Loss: 0.4426298588514328, Final Batch Loss: 0.2553764581680298\n",
      "Epoch 2036, Loss: 0.3749297708272934, Final Batch Loss: 0.1768823117017746\n",
      "Epoch 2037, Loss: 0.35684017837047577, Final Batch Loss: 0.19000519812107086\n",
      "Epoch 2038, Loss: 0.3911256790161133, Final Batch Loss: 0.21247617900371552\n",
      "Epoch 2039, Loss: 0.4290053993463516, Final Batch Loss: 0.20822550356388092\n",
      "Epoch 2040, Loss: 0.3417002409696579, Final Batch Loss: 0.15876887738704681\n",
      "Epoch 2041, Loss: 0.35362565517425537, Final Batch Loss: 0.15495218336582184\n",
      "Epoch 2042, Loss: 0.3811998814344406, Final Batch Loss: 0.19757063686847687\n",
      "Epoch 2043, Loss: 0.4311838448047638, Final Batch Loss: 0.24082954227924347\n",
      "Epoch 2044, Loss: 0.3970864713191986, Final Batch Loss: 0.19304656982421875\n",
      "Epoch 2045, Loss: 0.38901758193969727, Final Batch Loss: 0.18736039102077484\n",
      "Epoch 2046, Loss: 0.4026418924331665, Final Batch Loss: 0.19617730379104614\n",
      "Epoch 2047, Loss: 0.3969489187002182, Final Batch Loss: 0.20557700097560883\n",
      "Epoch 2048, Loss: 0.36681540310382843, Final Batch Loss: 0.17094171047210693\n",
      "Epoch 2049, Loss: 0.3745160549879074, Final Batch Loss: 0.2007492631673813\n",
      "Epoch 2050, Loss: 0.4083494395017624, Final Batch Loss: 0.19707192480564117\n",
      "Epoch 2051, Loss: 0.400274321436882, Final Batch Loss: 0.21449702978134155\n",
      "Epoch 2052, Loss: 0.39425840973854065, Final Batch Loss: 0.1925891488790512\n",
      "Epoch 2053, Loss: 0.37447668612003326, Final Batch Loss: 0.22546200454235077\n",
      "Epoch 2054, Loss: 0.3889223635196686, Final Batch Loss: 0.19891951978206635\n",
      "Epoch 2055, Loss: 0.38762134313583374, Final Batch Loss: 0.19335025548934937\n",
      "Epoch 2056, Loss: 0.35191962122917175, Final Batch Loss: 0.18809764087200165\n",
      "Epoch 2057, Loss: 0.3562752306461334, Final Batch Loss: 0.15008445084095\n",
      "Epoch 2058, Loss: 0.3718928247690201, Final Batch Loss: 0.21002693474292755\n",
      "Epoch 2059, Loss: 0.38706107437610626, Final Batch Loss: 0.17786061763763428\n",
      "Epoch 2060, Loss: 0.35735389590263367, Final Batch Loss: 0.17805582284927368\n",
      "Epoch 2061, Loss: 0.3731420934200287, Final Batch Loss: 0.1798437088727951\n",
      "Epoch 2062, Loss: 0.4108874201774597, Final Batch Loss: 0.22849225997924805\n",
      "Epoch 2063, Loss: 0.381412997841835, Final Batch Loss: 0.18215958774089813\n",
      "Epoch 2064, Loss: 0.3852844387292862, Final Batch Loss: 0.1904546618461609\n",
      "Epoch 2065, Loss: 0.4249509423971176, Final Batch Loss: 0.22394265234470367\n",
      "Epoch 2066, Loss: 0.42093147337436676, Final Batch Loss: 0.16888923943042755\n",
      "Epoch 2067, Loss: 0.34851664304733276, Final Batch Loss: 0.15290750563144684\n",
      "Epoch 2068, Loss: 0.3495948612689972, Final Batch Loss: 0.17706839740276337\n",
      "Epoch 2069, Loss: 0.3732456862926483, Final Batch Loss: 0.15177685022354126\n",
      "Epoch 2070, Loss: 0.37992602586746216, Final Batch Loss: 0.21895290911197662\n",
      "Epoch 2071, Loss: 0.4026828110218048, Final Batch Loss: 0.2502579391002655\n",
      "Epoch 2072, Loss: 0.3756256252527237, Final Batch Loss: 0.1998814195394516\n",
      "Epoch 2073, Loss: 0.42528995871543884, Final Batch Loss: 0.19930846989154816\n",
      "Epoch 2074, Loss: 0.3726315200328827, Final Batch Loss: 0.1891089826822281\n",
      "Epoch 2075, Loss: 0.3989994376897812, Final Batch Loss: 0.21080315113067627\n",
      "Epoch 2076, Loss: 0.36712193489074707, Final Batch Loss: 0.17258596420288086\n",
      "Epoch 2077, Loss: 0.39216235280036926, Final Batch Loss: 0.2103392332792282\n",
      "Epoch 2078, Loss: 0.4319895952939987, Final Batch Loss: 0.24170146882534027\n",
      "Epoch 2079, Loss: 0.4144051969051361, Final Batch Loss: 0.2264888435602188\n",
      "Epoch 2080, Loss: 0.4133194535970688, Final Batch Loss: 0.20331573486328125\n",
      "Epoch 2081, Loss: 0.3601151704788208, Final Batch Loss: 0.1337309628725052\n",
      "Epoch 2082, Loss: 0.3646809011697769, Final Batch Loss: 0.18906636536121368\n",
      "Epoch 2083, Loss: 0.35110215842723846, Final Batch Loss: 0.17823199927806854\n",
      "Epoch 2084, Loss: 0.4229568690061569, Final Batch Loss: 0.22251945734024048\n",
      "Epoch 2085, Loss: 0.38182462751865387, Final Batch Loss: 0.174639031291008\n",
      "Epoch 2086, Loss: 0.41276533901691437, Final Batch Loss: 0.2303253561258316\n",
      "Epoch 2087, Loss: 0.42567968368530273, Final Batch Loss: 0.20929807424545288\n",
      "Epoch 2088, Loss: 0.4000496119260788, Final Batch Loss: 0.2348969578742981\n",
      "Epoch 2089, Loss: 0.40042872726917267, Final Batch Loss: 0.1575368195772171\n",
      "Epoch 2090, Loss: 0.39399129152297974, Final Batch Loss: 0.17277412116527557\n",
      "Epoch 2091, Loss: 0.396565780043602, Final Batch Loss: 0.18050670623779297\n",
      "Epoch 2092, Loss: 0.3980984389781952, Final Batch Loss: 0.19943831861019135\n",
      "Epoch 2093, Loss: 0.36688852310180664, Final Batch Loss: 0.14473983645439148\n",
      "Epoch 2094, Loss: 0.3836633712053299, Final Batch Loss: 0.18484289944171906\n",
      "Epoch 2095, Loss: 0.38042640686035156, Final Batch Loss: 0.16795144975185394\n",
      "Epoch 2096, Loss: 0.3762217164039612, Final Batch Loss: 0.20267991721630096\n",
      "Epoch 2097, Loss: 0.39192625880241394, Final Batch Loss: 0.21918950974941254\n",
      "Epoch 2098, Loss: 0.43224750459194183, Final Batch Loss: 0.2501954734325409\n",
      "Epoch 2099, Loss: 0.3539236932992935, Final Batch Loss: 0.18041807413101196\n",
      "Epoch 2100, Loss: 0.4105883836746216, Final Batch Loss: 0.1894955188035965\n",
      "Epoch 2101, Loss: 0.39183899760246277, Final Batch Loss: 0.1593237668275833\n",
      "Epoch 2102, Loss: 0.359587624669075, Final Batch Loss: 0.16197042167186737\n",
      "Epoch 2103, Loss: 0.3704526573419571, Final Batch Loss: 0.16060885787010193\n",
      "Epoch 2104, Loss: 0.466117188334465, Final Batch Loss: 0.23424787819385529\n",
      "Epoch 2105, Loss: 0.40147872269153595, Final Batch Loss: 0.20325209200382233\n",
      "Epoch 2106, Loss: 0.42681072652339935, Final Batch Loss: 0.1990511566400528\n",
      "Epoch 2107, Loss: 0.46189193427562714, Final Batch Loss: 0.26610276103019714\n",
      "Epoch 2108, Loss: 0.3833997845649719, Final Batch Loss: 0.2005256861448288\n",
      "Epoch 2109, Loss: 0.3808184713125229, Final Batch Loss: 0.18338973820209503\n",
      "Epoch 2110, Loss: 0.3937174379825592, Final Batch Loss: 0.185869038105011\n",
      "Epoch 2111, Loss: 0.4166627377271652, Final Batch Loss: 0.20045006275177002\n",
      "Epoch 2112, Loss: 0.37616923451423645, Final Batch Loss: 0.22085362672805786\n",
      "Epoch 2113, Loss: 0.37998467683792114, Final Batch Loss: 0.21693362295627594\n",
      "Epoch 2114, Loss: 0.3591417521238327, Final Batch Loss: 0.1689966320991516\n",
      "Epoch 2115, Loss: 0.3646470755338669, Final Batch Loss: 0.14249403774738312\n",
      "Epoch 2116, Loss: 0.43863384425640106, Final Batch Loss: 0.2525339424610138\n",
      "Epoch 2117, Loss: 0.3884294331073761, Final Batch Loss: 0.17842060327529907\n",
      "Epoch 2118, Loss: 0.3971560001373291, Final Batch Loss: 0.19709402322769165\n",
      "Epoch 2119, Loss: 0.356986939907074, Final Batch Loss: 0.16298814117908478\n",
      "Epoch 2120, Loss: 0.3964873403310776, Final Batch Loss: 0.20952583849430084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2121, Loss: 0.38582754135131836, Final Batch Loss: 0.15654651820659637\n",
      "Epoch 2122, Loss: 0.3728911876678467, Final Batch Loss: 0.20536933839321136\n",
      "Epoch 2123, Loss: 0.362568199634552, Final Batch Loss: 0.1717546433210373\n",
      "Epoch 2124, Loss: 0.42288507521152496, Final Batch Loss: 0.2324012666940689\n",
      "Epoch 2125, Loss: 0.36383503675460815, Final Batch Loss: 0.1664707362651825\n",
      "Epoch 2126, Loss: 0.3797697424888611, Final Batch Loss: 0.1972912698984146\n",
      "Epoch 2127, Loss: 0.44900721311569214, Final Batch Loss: 0.1835131049156189\n",
      "Epoch 2128, Loss: 0.36022451519966125, Final Batch Loss: 0.19312192499637604\n",
      "Epoch 2129, Loss: 0.36029012501239777, Final Batch Loss: 0.18588989973068237\n",
      "Epoch 2130, Loss: 0.356582447886467, Final Batch Loss: 0.14508916437625885\n",
      "Epoch 2131, Loss: 0.41909779608249664, Final Batch Loss: 0.22952909767627716\n",
      "Epoch 2132, Loss: 0.380943238735199, Final Batch Loss: 0.18239255249500275\n",
      "Epoch 2133, Loss: 0.3668229728937149, Final Batch Loss: 0.16570381820201874\n",
      "Epoch 2134, Loss: 0.42513813078403473, Final Batch Loss: 0.24252061545848846\n",
      "Epoch 2135, Loss: 0.3927711397409439, Final Batch Loss: 0.1814831644296646\n",
      "Epoch 2136, Loss: 0.3741871267557144, Final Batch Loss: 0.18620358407497406\n",
      "Epoch 2137, Loss: 0.34838341176509857, Final Batch Loss: 0.16983795166015625\n",
      "Epoch 2138, Loss: 0.39212773740291595, Final Batch Loss: 0.19191281497478485\n",
      "Epoch 2139, Loss: 0.42152635753154755, Final Batch Loss: 0.25759968161582947\n",
      "Epoch 2140, Loss: 0.3624873012304306, Final Batch Loss: 0.1797545999288559\n",
      "Epoch 2141, Loss: 0.37904219329357147, Final Batch Loss: 0.15906809270381927\n",
      "Epoch 2142, Loss: 0.38842806220054626, Final Batch Loss: 0.18740080296993256\n",
      "Epoch 2143, Loss: 0.3637894541025162, Final Batch Loss: 0.14549507200717926\n",
      "Epoch 2144, Loss: 0.3815709948539734, Final Batch Loss: 0.16632422804832458\n",
      "Epoch 2145, Loss: 0.4340888410806656, Final Batch Loss: 0.19205957651138306\n",
      "Epoch 2146, Loss: 0.40325304865837097, Final Batch Loss: 0.20570379495620728\n",
      "Epoch 2147, Loss: 0.4123460203409195, Final Batch Loss: 0.21093444526195526\n",
      "Epoch 2148, Loss: 0.38253650069236755, Final Batch Loss: 0.22055679559707642\n",
      "Epoch 2149, Loss: 0.4187292009592056, Final Batch Loss: 0.21121656894683838\n",
      "Epoch 2150, Loss: 0.40716230869293213, Final Batch Loss: 0.2317763715982437\n",
      "Epoch 2151, Loss: 0.39072392880916595, Final Batch Loss: 0.20415546000003815\n",
      "Epoch 2152, Loss: 0.3781842440366745, Final Batch Loss: 0.1580682098865509\n",
      "Epoch 2153, Loss: 0.3572336882352829, Final Batch Loss: 0.16422823071479797\n",
      "Epoch 2154, Loss: 0.538514107465744, Final Batch Loss: 0.2818811535835266\n",
      "Epoch 2155, Loss: 0.35102610290050507, Final Batch Loss: 0.1593867987394333\n",
      "Epoch 2156, Loss: 0.4257325530052185, Final Batch Loss: 0.2713383734226227\n",
      "Epoch 2157, Loss: 0.4020874500274658, Final Batch Loss: 0.18922863900661469\n",
      "Epoch 2158, Loss: 0.3983878344297409, Final Batch Loss: 0.17427371442317963\n",
      "Epoch 2159, Loss: 0.4319619685411453, Final Batch Loss: 0.23103666305541992\n",
      "Epoch 2160, Loss: 0.41983890533447266, Final Batch Loss: 0.23064225912094116\n",
      "Epoch 2161, Loss: 0.4260554015636444, Final Batch Loss: 0.22071991860866547\n",
      "Epoch 2162, Loss: 0.3779902160167694, Final Batch Loss: 0.15725645422935486\n",
      "Epoch 2163, Loss: 0.4040418416261673, Final Batch Loss: 0.21541649103164673\n",
      "Epoch 2164, Loss: 0.37769487500190735, Final Batch Loss: 0.17330600321292877\n",
      "Epoch 2165, Loss: 0.47033868730068207, Final Batch Loss: 0.2061835080385208\n",
      "Epoch 2166, Loss: 0.3658601939678192, Final Batch Loss: 0.2119266390800476\n",
      "Epoch 2167, Loss: 0.369672954082489, Final Batch Loss: 0.18424753844738007\n",
      "Epoch 2168, Loss: 0.3483841121196747, Final Batch Loss: 0.16102449595928192\n",
      "Epoch 2169, Loss: 0.3944152593612671, Final Batch Loss: 0.2191448211669922\n",
      "Epoch 2170, Loss: 0.4123481214046478, Final Batch Loss: 0.19832521677017212\n",
      "Epoch 2171, Loss: 0.33737775683403015, Final Batch Loss: 0.19343990087509155\n",
      "Epoch 2172, Loss: 0.33473217487335205, Final Batch Loss: 0.15954077243804932\n",
      "Epoch 2173, Loss: 0.3422548919916153, Final Batch Loss: 0.17062999308109283\n",
      "Epoch 2174, Loss: 0.398611456155777, Final Batch Loss: 0.22308112680912018\n",
      "Epoch 2175, Loss: 0.32508738338947296, Final Batch Loss: 0.1498522013425827\n",
      "Epoch 2176, Loss: 0.5194801688194275, Final Batch Loss: 0.33172115683555603\n",
      "Epoch 2177, Loss: 0.42761029303073883, Final Batch Loss: 0.19114701449871063\n",
      "Epoch 2178, Loss: 0.4338495433330536, Final Batch Loss: 0.24415026605129242\n",
      "Epoch 2179, Loss: 0.3595363050699234, Final Batch Loss: 0.14370961487293243\n",
      "Epoch 2180, Loss: 0.37616966664791107, Final Batch Loss: 0.189414843916893\n",
      "Epoch 2181, Loss: 0.43477191030979156, Final Batch Loss: 0.22681671380996704\n",
      "Epoch 2182, Loss: 0.37746240198612213, Final Batch Loss: 0.19645608961582184\n",
      "Epoch 2183, Loss: 0.40029050409793854, Final Batch Loss: 0.17917944490909576\n",
      "Epoch 2184, Loss: 0.40253278613090515, Final Batch Loss: 0.22341524064540863\n",
      "Epoch 2185, Loss: 0.3545243740081787, Final Batch Loss: 0.16284655034542084\n",
      "Epoch 2186, Loss: 0.42467716336250305, Final Batch Loss: 0.24420976638793945\n",
      "Epoch 2187, Loss: 0.38838808238506317, Final Batch Loss: 0.20221568644046783\n",
      "Epoch 2188, Loss: 0.39761286973953247, Final Batch Loss: 0.2215416431427002\n",
      "Epoch 2189, Loss: 0.42708365619182587, Final Batch Loss: 0.23217861354351044\n",
      "Epoch 2190, Loss: 0.40825213491916656, Final Batch Loss: 0.1976190060377121\n",
      "Epoch 2191, Loss: 0.35336387157440186, Final Batch Loss: 0.18675701320171356\n",
      "Epoch 2192, Loss: 0.3828330338001251, Final Batch Loss: 0.2118726372718811\n",
      "Epoch 2193, Loss: 0.4668986052274704, Final Batch Loss: 0.22611068189144135\n",
      "Epoch 2194, Loss: 0.4592779576778412, Final Batch Loss: 0.25877445936203003\n",
      "Epoch 2195, Loss: 0.37685348093509674, Final Batch Loss: 0.20537780225276947\n",
      "Epoch 2196, Loss: 0.3759941905736923, Final Batch Loss: 0.18837201595306396\n",
      "Epoch 2197, Loss: 0.42983999848365784, Final Batch Loss: 0.20130711793899536\n",
      "Epoch 2198, Loss: 0.40928277373313904, Final Batch Loss: 0.19166509807109833\n",
      "Epoch 2199, Loss: 0.4452635198831558, Final Batch Loss: 0.27289775013923645\n",
      "Epoch 2200, Loss: 0.37367624044418335, Final Batch Loss: 0.19017772376537323\n",
      "Epoch 2201, Loss: 0.363747701048851, Final Batch Loss: 0.1953902691602707\n",
      "Epoch 2202, Loss: 0.39240966737270355, Final Batch Loss: 0.1928030103445053\n",
      "Epoch 2203, Loss: 0.3633366525173187, Final Batch Loss: 0.15515018999576569\n",
      "Epoch 2204, Loss: 0.4108634889125824, Final Batch Loss: 0.22115077078342438\n",
      "Epoch 2205, Loss: 0.4065636545419693, Final Batch Loss: 0.2155998945236206\n",
      "Epoch 2206, Loss: 0.357577919960022, Final Batch Loss: 0.16559985280036926\n",
      "Epoch 2207, Loss: 0.38129496574401855, Final Batch Loss: 0.19604744017124176\n",
      "Epoch 2208, Loss: 0.3496926873922348, Final Batch Loss: 0.2173546701669693\n",
      "Epoch 2209, Loss: 0.37985607981681824, Final Batch Loss: 0.16845250129699707\n",
      "Epoch 2210, Loss: 0.3818605840206146, Final Batch Loss: 0.13930954039096832\n",
      "Epoch 2211, Loss: 0.4249504506587982, Final Batch Loss: 0.22825616598129272\n",
      "Epoch 2212, Loss: 0.38066715002059937, Final Batch Loss: 0.17957626283168793\n",
      "Epoch 2213, Loss: 0.44469279050827026, Final Batch Loss: 0.2556566298007965\n",
      "Epoch 2214, Loss: 0.41759900748729706, Final Batch Loss: 0.20393259823322296\n",
      "Epoch 2215, Loss: 0.39185310900211334, Final Batch Loss: 0.18851809203624725\n",
      "Epoch 2216, Loss: 0.4221590608358383, Final Batch Loss: 0.24218708276748657\n",
      "Epoch 2217, Loss: 0.38744206726551056, Final Batch Loss: 0.20833958685398102\n",
      "Epoch 2218, Loss: 0.36679698526859283, Final Batch Loss: 0.17005427181720734\n",
      "Epoch 2219, Loss: 0.3860374987125397, Final Batch Loss: 0.22604571282863617\n",
      "Epoch 2220, Loss: 0.40048249065876007, Final Batch Loss: 0.18140484392642975\n",
      "Epoch 2221, Loss: 0.43767498433589935, Final Batch Loss: 0.21287892758846283\n",
      "Epoch 2222, Loss: 0.47353406250476837, Final Batch Loss: 0.22541546821594238\n",
      "Epoch 2223, Loss: 0.3348701596260071, Final Batch Loss: 0.1632857471704483\n",
      "Epoch 2224, Loss: 0.37044180929660797, Final Batch Loss: 0.1641205996274948\n",
      "Epoch 2225, Loss: 0.35803215205669403, Final Batch Loss: 0.15619947016239166\n",
      "Epoch 2226, Loss: 0.39490680396556854, Final Batch Loss: 0.16950564086437225\n",
      "Epoch 2227, Loss: 0.44130608439445496, Final Batch Loss: 0.21982061862945557\n",
      "Epoch 2228, Loss: 0.4797162562608719, Final Batch Loss: 0.1938321739435196\n",
      "Epoch 2229, Loss: 0.38822197914123535, Final Batch Loss: 0.16300348937511444\n",
      "Epoch 2230, Loss: 0.3854121118783951, Final Batch Loss: 0.20149964094161987\n",
      "Epoch 2231, Loss: 0.4069972038269043, Final Batch Loss: 0.1851370483636856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2232, Loss: 0.3621178865432739, Final Batch Loss: 0.2110622376203537\n",
      "Epoch 2233, Loss: 0.42218463122844696, Final Batch Loss: 0.21136142313480377\n",
      "Epoch 2234, Loss: 0.4285604953765869, Final Batch Loss: 0.2128056138753891\n",
      "Epoch 2235, Loss: 0.40075884759426117, Final Batch Loss: 0.21405620872974396\n",
      "Epoch 2236, Loss: 0.41312454640865326, Final Batch Loss: 0.15610213577747345\n",
      "Epoch 2237, Loss: 0.3448803275823593, Final Batch Loss: 0.1535009741783142\n",
      "Epoch 2238, Loss: 0.39032571017742157, Final Batch Loss: 0.20893408358097076\n",
      "Epoch 2239, Loss: 0.38438892364501953, Final Batch Loss: 0.17651842534542084\n",
      "Epoch 2240, Loss: 0.4316902607679367, Final Batch Loss: 0.25718048214912415\n",
      "Epoch 2241, Loss: 0.36578305065631866, Final Batch Loss: 0.18294161558151245\n",
      "Epoch 2242, Loss: 0.39058351516723633, Final Batch Loss: 0.22416360676288605\n",
      "Epoch 2243, Loss: 0.4554856866598129, Final Batch Loss: 0.20762819051742554\n",
      "Epoch 2244, Loss: 0.39821457862854004, Final Batch Loss: 0.19290368258953094\n",
      "Epoch 2245, Loss: 0.42976926267147064, Final Batch Loss: 0.2132159322500229\n",
      "Epoch 2246, Loss: 0.3653864562511444, Final Batch Loss: 0.18923227488994598\n",
      "Epoch 2247, Loss: 0.3999543637037277, Final Batch Loss: 0.20620892941951752\n",
      "Epoch 2248, Loss: 0.36999429762363434, Final Batch Loss: 0.18179060518741608\n",
      "Epoch 2249, Loss: 0.42634980380535126, Final Batch Loss: 0.16781549155712128\n",
      "Epoch 2250, Loss: 0.3651314973831177, Final Batch Loss: 0.1580062210559845\n",
      "Epoch 2251, Loss: 0.3891661614179611, Final Batch Loss: 0.22632884979248047\n",
      "Epoch 2252, Loss: 0.3371531814336777, Final Batch Loss: 0.15880165994167328\n",
      "Epoch 2253, Loss: 0.35099631547927856, Final Batch Loss: 0.18688027560710907\n",
      "Epoch 2254, Loss: 0.3817741870880127, Final Batch Loss: 0.1679927557706833\n",
      "Epoch 2255, Loss: 0.40804508328437805, Final Batch Loss: 0.2255556434392929\n",
      "Epoch 2256, Loss: 0.3250690698623657, Final Batch Loss: 0.16771776974201202\n",
      "Epoch 2257, Loss: 0.3736702650785446, Final Batch Loss: 0.20819513499736786\n",
      "Epoch 2258, Loss: 0.40325163304805756, Final Batch Loss: 0.24347279965877533\n",
      "Epoch 2259, Loss: 0.3754690885543823, Final Batch Loss: 0.1573103964328766\n",
      "Epoch 2260, Loss: 0.37678466737270355, Final Batch Loss: 0.1948838084936142\n",
      "Epoch 2261, Loss: 0.37497736513614655, Final Batch Loss: 0.19500614702701569\n",
      "Epoch 2262, Loss: 0.4017706960439682, Final Batch Loss: 0.227058544754982\n",
      "Epoch 2263, Loss: 0.36860252916812897, Final Batch Loss: 0.18078456819057465\n",
      "Epoch 2264, Loss: 0.48102104663848877, Final Batch Loss: 0.2015032172203064\n",
      "Epoch 2265, Loss: 0.3857717663049698, Final Batch Loss: 0.20921778678894043\n",
      "Epoch 2266, Loss: 0.42130090296268463, Final Batch Loss: 0.2201913446187973\n",
      "Epoch 2267, Loss: 0.33025503158569336, Final Batch Loss: 0.17749887704849243\n",
      "Epoch 2268, Loss: 0.372869074344635, Final Batch Loss: 0.20957855880260468\n",
      "Epoch 2269, Loss: 0.4524383842945099, Final Batch Loss: 0.22514879703521729\n",
      "Epoch 2270, Loss: 0.3812348544597626, Final Batch Loss: 0.22058624029159546\n",
      "Epoch 2271, Loss: 0.3502301275730133, Final Batch Loss: 0.15184007585048676\n",
      "Epoch 2272, Loss: 0.423827201128006, Final Batch Loss: 0.19014602899551392\n",
      "Epoch 2273, Loss: 0.38413065671920776, Final Batch Loss: 0.15997324883937836\n",
      "Epoch 2274, Loss: 0.4294610917568207, Final Batch Loss: 0.18446481227874756\n",
      "Epoch 2275, Loss: 0.39698608219623566, Final Batch Loss: 0.2373691350221634\n",
      "Epoch 2276, Loss: 0.3541363924741745, Final Batch Loss: 0.13631708920001984\n",
      "Epoch 2277, Loss: 0.38530394434928894, Final Batch Loss: 0.19387082755565643\n",
      "Epoch 2278, Loss: 0.39546720683574677, Final Batch Loss: 0.19300347566604614\n",
      "Epoch 2279, Loss: 0.39376336336135864, Final Batch Loss: 0.19880734384059906\n",
      "Epoch 2280, Loss: 0.3915261924266815, Final Batch Loss: 0.2197403460741043\n",
      "Epoch 2281, Loss: 0.36972469091415405, Final Batch Loss: 0.17399664223194122\n",
      "Epoch 2282, Loss: 0.36397071182727814, Final Batch Loss: 0.18981021642684937\n",
      "Epoch 2283, Loss: 0.3840826153755188, Final Batch Loss: 0.18366730213165283\n",
      "Epoch 2284, Loss: 0.467660054564476, Final Batch Loss: 0.24934951961040497\n",
      "Epoch 2285, Loss: 0.3591153919696808, Final Batch Loss: 0.16565760970115662\n",
      "Epoch 2286, Loss: 0.37449365854263306, Final Batch Loss: 0.2028186321258545\n",
      "Epoch 2287, Loss: 0.34680742025375366, Final Batch Loss: 0.15093441307544708\n",
      "Epoch 2288, Loss: 0.4180600941181183, Final Batch Loss: 0.23232655227184296\n",
      "Epoch 2289, Loss: 0.37584955990314484, Final Batch Loss: 0.17837464809417725\n",
      "Epoch 2290, Loss: 0.34586530923843384, Final Batch Loss: 0.167469322681427\n",
      "Epoch 2291, Loss: 0.3555208891630173, Final Batch Loss: 0.180911123752594\n",
      "Epoch 2292, Loss: 0.43250297009944916, Final Batch Loss: 0.204273983836174\n",
      "Epoch 2293, Loss: 0.35347747802734375, Final Batch Loss: 0.1609456092119217\n",
      "Epoch 2294, Loss: 0.35227440297603607, Final Batch Loss: 0.15455350279808044\n",
      "Epoch 2295, Loss: 0.39809392392635345, Final Batch Loss: 0.18492263555526733\n",
      "Epoch 2296, Loss: 0.3915330320596695, Final Batch Loss: 0.20518244802951813\n",
      "Epoch 2297, Loss: 0.41621215641498566, Final Batch Loss: 0.25912073254585266\n",
      "Epoch 2298, Loss: 0.3524186760187149, Final Batch Loss: 0.16002412140369415\n",
      "Epoch 2299, Loss: 0.39028018712997437, Final Batch Loss: 0.19779761135578156\n",
      "Epoch 2300, Loss: 0.383596271276474, Final Batch Loss: 0.16307511925697327\n",
      "Epoch 2301, Loss: 0.39594535529613495, Final Batch Loss: 0.20592491328716278\n",
      "Epoch 2302, Loss: 0.3563334494829178, Final Batch Loss: 0.16533799469470978\n",
      "Epoch 2303, Loss: 0.3527720123529434, Final Batch Loss: 0.18937301635742188\n",
      "Epoch 2304, Loss: 0.42704643309116364, Final Batch Loss: 0.2129591554403305\n",
      "Epoch 2305, Loss: 0.33173325657844543, Final Batch Loss: 0.16910551488399506\n",
      "Epoch 2306, Loss: 0.3989114314317703, Final Batch Loss: 0.19666622579097748\n",
      "Epoch 2307, Loss: 0.39499208331108093, Final Batch Loss: 0.20581871271133423\n",
      "Epoch 2308, Loss: 0.430869922041893, Final Batch Loss: 0.27593252062797546\n",
      "Epoch 2309, Loss: 0.38154907524585724, Final Batch Loss: 0.20544685423374176\n",
      "Epoch 2310, Loss: 0.3786252588033676, Final Batch Loss: 0.18771480023860931\n",
      "Epoch 2311, Loss: 0.37283165752887726, Final Batch Loss: 0.17671312391757965\n",
      "Epoch 2312, Loss: 0.3778359293937683, Final Batch Loss: 0.17272280156612396\n",
      "Epoch 2313, Loss: 0.36376091837882996, Final Batch Loss: 0.20719623565673828\n",
      "Epoch 2314, Loss: 0.33678311109542847, Final Batch Loss: 0.16755616664886475\n",
      "Epoch 2315, Loss: 0.532539963722229, Final Batch Loss: 0.38331130146980286\n",
      "Epoch 2316, Loss: 0.36981892585754395, Final Batch Loss: 0.197776198387146\n",
      "Epoch 2317, Loss: 0.3473775237798691, Final Batch Loss: 0.1551516205072403\n",
      "Epoch 2318, Loss: 0.3359202891588211, Final Batch Loss: 0.14563699066638947\n",
      "Epoch 2319, Loss: 0.37877410650253296, Final Batch Loss: 0.1755170226097107\n",
      "Epoch 2320, Loss: 0.33251868188381195, Final Batch Loss: 0.14445115625858307\n",
      "Epoch 2321, Loss: 0.34661532938480377, Final Batch Loss: 0.1846722811460495\n",
      "Epoch 2322, Loss: 0.42410634458065033, Final Batch Loss: 0.24404896795749664\n",
      "Epoch 2323, Loss: 0.36254265904426575, Final Batch Loss: 0.17651516199111938\n",
      "Epoch 2324, Loss: 0.3671773374080658, Final Batch Loss: 0.2154523730278015\n",
      "Epoch 2325, Loss: 0.387252613902092, Final Batch Loss: 0.22728140652179718\n",
      "Epoch 2326, Loss: 0.35639573633670807, Final Batch Loss: 0.18825167417526245\n",
      "Epoch 2327, Loss: 0.38108015060424805, Final Batch Loss: 0.1815488487482071\n",
      "Epoch 2328, Loss: 0.33400610089302063, Final Batch Loss: 0.18303316831588745\n",
      "Epoch 2329, Loss: 0.3430909812450409, Final Batch Loss: 0.17969052493572235\n",
      "Epoch 2330, Loss: 0.3522663563489914, Final Batch Loss: 0.20181582868099213\n",
      "Epoch 2331, Loss: 0.3608950227499008, Final Batch Loss: 0.15647880733013153\n",
      "Epoch 2332, Loss: 0.33056820929050446, Final Batch Loss: 0.19327497482299805\n",
      "Epoch 2333, Loss: 0.396879181265831, Final Batch Loss: 0.16231702268123627\n",
      "Epoch 2334, Loss: 0.3525650203227997, Final Batch Loss: 0.15538741648197174\n",
      "Epoch 2335, Loss: 0.37824417650699615, Final Batch Loss: 0.173298642039299\n",
      "Epoch 2336, Loss: 0.3790653795003891, Final Batch Loss: 0.19373299181461334\n",
      "Epoch 2337, Loss: 0.3312266767024994, Final Batch Loss: 0.1645279973745346\n",
      "Epoch 2338, Loss: 0.3792125880718231, Final Batch Loss: 0.16964872181415558\n",
      "Epoch 2339, Loss: 0.3902755528688431, Final Batch Loss: 0.17876948416233063\n",
      "Epoch 2340, Loss: 0.3900367170572281, Final Batch Loss: 0.1922677755355835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2341, Loss: 0.3744081109762192, Final Batch Loss: 0.20790033042430878\n",
      "Epoch 2342, Loss: 0.4699697047472, Final Batch Loss: 0.27499619126319885\n",
      "Epoch 2343, Loss: 0.3710155487060547, Final Batch Loss: 0.16520242393016815\n",
      "Epoch 2344, Loss: 0.3557227551937103, Final Batch Loss: 0.18924520909786224\n",
      "Epoch 2345, Loss: 0.3920714408159256, Final Batch Loss: 0.21095401048660278\n",
      "Epoch 2346, Loss: 0.3794884830713272, Final Batch Loss: 0.21834422647953033\n",
      "Epoch 2347, Loss: 0.3731888383626938, Final Batch Loss: 0.1912248581647873\n",
      "Epoch 2348, Loss: 0.4081130623817444, Final Batch Loss: 0.2511572539806366\n",
      "Epoch 2349, Loss: 0.36854761838912964, Final Batch Loss: 0.16451087594032288\n",
      "Epoch 2350, Loss: 0.3934251219034195, Final Batch Loss: 0.1706811934709549\n",
      "Epoch 2351, Loss: 0.4216623306274414, Final Batch Loss: 0.19945375621318817\n",
      "Epoch 2352, Loss: 0.41417062282562256, Final Batch Loss: 0.24613286554813385\n",
      "Epoch 2353, Loss: 0.41817647218704224, Final Batch Loss: 0.19742213189601898\n",
      "Epoch 2354, Loss: 0.41806556284427643, Final Batch Loss: 0.18781666457653046\n",
      "Epoch 2355, Loss: 0.33652375638484955, Final Batch Loss: 0.15719585120677948\n",
      "Epoch 2356, Loss: 0.39073483645915985, Final Batch Loss: 0.22958701848983765\n",
      "Epoch 2357, Loss: 0.3572068214416504, Final Batch Loss: 0.17344844341278076\n",
      "Epoch 2358, Loss: 0.38881227374076843, Final Batch Loss: 0.17259208858013153\n",
      "Epoch 2359, Loss: 0.400070458650589, Final Batch Loss: 0.1897001713514328\n",
      "Epoch 2360, Loss: 0.35237622261047363, Final Batch Loss: 0.17087651789188385\n",
      "Epoch 2361, Loss: 0.40751540660858154, Final Batch Loss: 0.20341473817825317\n",
      "Epoch 2362, Loss: 0.4219920337200165, Final Batch Loss: 0.18523192405700684\n",
      "Epoch 2363, Loss: 0.3699245750904083, Final Batch Loss: 0.2388940453529358\n",
      "Epoch 2364, Loss: 0.3921278715133667, Final Batch Loss: 0.2109818309545517\n",
      "Epoch 2365, Loss: 0.3534855246543884, Final Batch Loss: 0.16802255809307098\n",
      "Epoch 2366, Loss: 0.37759584188461304, Final Batch Loss: 0.18423967063426971\n",
      "Epoch 2367, Loss: 0.3779650777578354, Final Batch Loss: 0.15008878707885742\n",
      "Epoch 2368, Loss: 0.3639601618051529, Final Batch Loss: 0.16151899099349976\n",
      "Epoch 2369, Loss: 0.31783221662044525, Final Batch Loss: 0.13636890053749084\n",
      "Epoch 2370, Loss: 0.35292667150497437, Final Batch Loss: 0.1769900768995285\n",
      "Epoch 2371, Loss: 0.3747362494468689, Final Batch Loss: 0.19580937922000885\n",
      "Epoch 2372, Loss: 0.3810473382472992, Final Batch Loss: 0.23925544321537018\n",
      "Epoch 2373, Loss: 0.4592771530151367, Final Batch Loss: 0.22589634358882904\n",
      "Epoch 2374, Loss: 0.39042867720127106, Final Batch Loss: 0.18625803291797638\n",
      "Epoch 2375, Loss: 0.35794033110141754, Final Batch Loss: 0.20043319463729858\n",
      "Epoch 2376, Loss: 0.41158026456832886, Final Batch Loss: 0.20717330276966095\n",
      "Epoch 2377, Loss: 0.3467563986778259, Final Batch Loss: 0.18390905857086182\n",
      "Epoch 2378, Loss: 0.3378501534461975, Final Batch Loss: 0.19181855022907257\n",
      "Epoch 2379, Loss: 0.3379935771226883, Final Batch Loss: 0.12553514540195465\n",
      "Epoch 2380, Loss: 0.36724619567394257, Final Batch Loss: 0.16199031472206116\n",
      "Epoch 2381, Loss: 0.3679683059453964, Final Batch Loss: 0.19941265881061554\n",
      "Epoch 2382, Loss: 0.3634396940469742, Final Batch Loss: 0.17397598922252655\n",
      "Epoch 2383, Loss: 0.39413756132125854, Final Batch Loss: 0.19166529178619385\n",
      "Epoch 2384, Loss: 0.36401602625846863, Final Batch Loss: 0.21236376464366913\n",
      "Epoch 2385, Loss: 0.35579468309879303, Final Batch Loss: 0.15633685886859894\n",
      "Epoch 2386, Loss: 0.4020838439464569, Final Batch Loss: 0.25401654839515686\n",
      "Epoch 2387, Loss: 0.369186669588089, Final Batch Loss: 0.15115122497081757\n",
      "Epoch 2388, Loss: 0.3307405263185501, Final Batch Loss: 0.191447451710701\n",
      "Epoch 2389, Loss: 0.3639627695083618, Final Batch Loss: 0.15389268100261688\n",
      "Epoch 2390, Loss: 0.37348757684230804, Final Batch Loss: 0.18013660609722137\n",
      "Epoch 2391, Loss: 0.3516102433204651, Final Batch Loss: 0.16103166341781616\n",
      "Epoch 2392, Loss: 0.4070991426706314, Final Batch Loss: 0.16635766625404358\n",
      "Epoch 2393, Loss: 0.3926074802875519, Final Batch Loss: 0.2353207916021347\n",
      "Epoch 2394, Loss: 0.33869726955890656, Final Batch Loss: 0.12504549324512482\n",
      "Epoch 2395, Loss: 0.37787768244743347, Final Batch Loss: 0.1576782763004303\n",
      "Epoch 2396, Loss: 0.39630384743213654, Final Batch Loss: 0.1882166862487793\n",
      "Epoch 2397, Loss: 0.3759024292230606, Final Batch Loss: 0.20354311168193817\n",
      "Epoch 2398, Loss: 0.39769022166728973, Final Batch Loss: 0.21552503108978271\n",
      "Epoch 2399, Loss: 0.3754778802394867, Final Batch Loss: 0.1974128931760788\n",
      "Epoch 2400, Loss: 0.3684011250734329, Final Batch Loss: 0.2053285837173462\n",
      "Epoch 2401, Loss: 0.4442795664072037, Final Batch Loss: 0.1845829039812088\n",
      "Epoch 2402, Loss: 0.4087235778570175, Final Batch Loss: 0.20550763607025146\n",
      "Epoch 2403, Loss: 0.3838595151901245, Final Batch Loss: 0.1744617074728012\n",
      "Epoch 2404, Loss: 0.31090056896209717, Final Batch Loss: 0.16442833840847015\n",
      "Epoch 2405, Loss: 0.3798331469297409, Final Batch Loss: 0.21020551025867462\n",
      "Epoch 2406, Loss: 0.3753475248813629, Final Batch Loss: 0.19343793392181396\n",
      "Epoch 2407, Loss: 0.3478912115097046, Final Batch Loss: 0.1847972422838211\n",
      "Epoch 2408, Loss: 0.4238705188035965, Final Batch Loss: 0.19937537610530853\n",
      "Epoch 2409, Loss: 0.37750306725502014, Final Batch Loss: 0.19347576797008514\n",
      "Epoch 2410, Loss: 0.35219286382198334, Final Batch Loss: 0.16111446917057037\n",
      "Epoch 2411, Loss: 0.38426339626312256, Final Batch Loss: 0.17824341356754303\n",
      "Epoch 2412, Loss: 0.3482830971479416, Final Batch Loss: 0.17365218698978424\n",
      "Epoch 2413, Loss: 0.35694631934165955, Final Batch Loss: 0.1895037442445755\n",
      "Epoch 2414, Loss: 0.3715241253376007, Final Batch Loss: 0.15016400814056396\n",
      "Epoch 2415, Loss: 0.3807560205459595, Final Batch Loss: 0.2034870982170105\n",
      "Epoch 2416, Loss: 0.43604230880737305, Final Batch Loss: 0.23913715779781342\n",
      "Epoch 2417, Loss: 0.3657630383968353, Final Batch Loss: 0.14009907841682434\n",
      "Epoch 2418, Loss: 0.37955790758132935, Final Batch Loss: 0.2150012105703354\n",
      "Epoch 2419, Loss: 0.45639969408512115, Final Batch Loss: 0.24590839445590973\n",
      "Epoch 2420, Loss: 0.3924349844455719, Final Batch Loss: 0.20038609206676483\n",
      "Epoch 2421, Loss: 0.34963029623031616, Final Batch Loss: 0.1378166228532791\n",
      "Epoch 2422, Loss: 0.35076670348644257, Final Batch Loss: 0.1453719139099121\n",
      "Epoch 2423, Loss: 0.41575780510902405, Final Batch Loss: 0.22763167321681976\n",
      "Epoch 2424, Loss: 0.36738328635692596, Final Batch Loss: 0.20146489143371582\n",
      "Epoch 2425, Loss: 0.3475075215101242, Final Batch Loss: 0.15685664117336273\n",
      "Epoch 2426, Loss: 0.3820054531097412, Final Batch Loss: 0.1794554442167282\n",
      "Epoch 2427, Loss: 0.42051856219768524, Final Batch Loss: 0.24411441385746002\n",
      "Epoch 2428, Loss: 0.38098403811454773, Final Batch Loss: 0.2286924570798874\n",
      "Epoch 2429, Loss: 0.3619202822446823, Final Batch Loss: 0.18124644458293915\n",
      "Epoch 2430, Loss: 0.3285452425479889, Final Batch Loss: 0.16527947783470154\n",
      "Epoch 2431, Loss: 0.3459184020757675, Final Batch Loss: 0.13665138185024261\n",
      "Epoch 2432, Loss: 0.3669006675481796, Final Batch Loss: 0.19309668242931366\n",
      "Epoch 2433, Loss: 0.38730384409427643, Final Batch Loss: 0.22612424194812775\n",
      "Epoch 2434, Loss: 0.3476277142763138, Final Batch Loss: 0.17047668993473053\n",
      "Epoch 2435, Loss: 0.3506549149751663, Final Batch Loss: 0.18666939437389374\n",
      "Epoch 2436, Loss: 0.3337177187204361, Final Batch Loss: 0.18171416223049164\n",
      "Epoch 2437, Loss: 0.36162354052066803, Final Batch Loss: 0.18186163902282715\n",
      "Epoch 2438, Loss: 0.4184519946575165, Final Batch Loss: 0.2248319387435913\n",
      "Epoch 2439, Loss: 0.3691917508840561, Final Batch Loss: 0.18795330822467804\n",
      "Epoch 2440, Loss: 0.34708574414253235, Final Batch Loss: 0.17954440414905548\n",
      "Epoch 2441, Loss: 0.46735090017318726, Final Batch Loss: 0.28969457745552063\n",
      "Epoch 2442, Loss: 0.4484977424144745, Final Batch Loss: 0.2308955043554306\n",
      "Epoch 2443, Loss: 0.36867286264896393, Final Batch Loss: 0.19793951511383057\n",
      "Epoch 2444, Loss: 0.36953288316726685, Final Batch Loss: 0.19705264270305634\n",
      "Epoch 2445, Loss: 0.4276452511548996, Final Batch Loss: 0.23956747353076935\n",
      "Epoch 2446, Loss: 0.35354849696159363, Final Batch Loss: 0.1592155247926712\n",
      "Epoch 2447, Loss: 0.3775744140148163, Final Batch Loss: 0.17680299282073975\n",
      "Epoch 2448, Loss: 0.3422272354364395, Final Batch Loss: 0.16293545067310333\n",
      "Epoch 2449, Loss: 0.3548406511545181, Final Batch Loss: 0.16132639348506927\n",
      "Epoch 2450, Loss: 0.3909640461206436, Final Batch Loss: 0.18502362072467804\n",
      "Epoch 2451, Loss: 0.4025416225194931, Final Batch Loss: 0.20244640111923218\n",
      "Epoch 2452, Loss: 0.3355413228273392, Final Batch Loss: 0.16212385892868042\n",
      "Epoch 2453, Loss: 0.36804720759391785, Final Batch Loss: 0.18630088865756989\n",
      "Epoch 2454, Loss: 0.3634592592716217, Final Batch Loss: 0.19308443367481232\n",
      "Epoch 2455, Loss: 0.34510673582553864, Final Batch Loss: 0.1517348438501358\n",
      "Epoch 2456, Loss: 0.37631383538246155, Final Batch Loss: 0.18339835107326508\n",
      "Epoch 2457, Loss: 0.352657288312912, Final Batch Loss: 0.14974375069141388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2458, Loss: 0.37096402049064636, Final Batch Loss: 0.2111237794160843\n",
      "Epoch 2459, Loss: 0.3889688402414322, Final Batch Loss: 0.20491009950637817\n",
      "Epoch 2460, Loss: 0.35547517240047455, Final Batch Loss: 0.20065635442733765\n",
      "Epoch 2461, Loss: 0.3507174700498581, Final Batch Loss: 0.1282561719417572\n",
      "Epoch 2462, Loss: 0.36728133261203766, Final Batch Loss: 0.1793530434370041\n",
      "Epoch 2463, Loss: 0.3614276796579361, Final Batch Loss: 0.15378281474113464\n",
      "Epoch 2464, Loss: 0.35286368429660797, Final Batch Loss: 0.19166022539138794\n",
      "Epoch 2465, Loss: 0.3597114831209183, Final Batch Loss: 0.14771829545497894\n",
      "Epoch 2466, Loss: 0.3521454632282257, Final Batch Loss: 0.17441914975643158\n",
      "Epoch 2467, Loss: 0.4146532565355301, Final Batch Loss: 0.23774085938930511\n",
      "Epoch 2468, Loss: 0.341534286737442, Final Batch Loss: 0.1625351905822754\n",
      "Epoch 2469, Loss: 0.4275025576353073, Final Batch Loss: 0.17532284557819366\n",
      "Epoch 2470, Loss: 0.3295720964670181, Final Batch Loss: 0.15528568625450134\n",
      "Epoch 2471, Loss: 0.35701675713062286, Final Batch Loss: 0.1613217294216156\n",
      "Epoch 2472, Loss: 0.3405851125717163, Final Batch Loss: 0.2041255235671997\n",
      "Epoch 2473, Loss: 0.36605989933013916, Final Batch Loss: 0.21293969452381134\n",
      "Epoch 2474, Loss: 0.3584117591381073, Final Batch Loss: 0.15577134490013123\n",
      "Epoch 2475, Loss: 0.3399057984352112, Final Batch Loss: 0.17010009288787842\n",
      "Epoch 2476, Loss: 0.3495282530784607, Final Batch Loss: 0.1423981636762619\n",
      "Epoch 2477, Loss: 0.3180306851863861, Final Batch Loss: 0.15561062097549438\n",
      "Epoch 2478, Loss: 0.386630654335022, Final Batch Loss: 0.20563918352127075\n",
      "Epoch 2479, Loss: 0.3744409680366516, Final Batch Loss: 0.17136740684509277\n",
      "Epoch 2480, Loss: 0.35574771463871, Final Batch Loss: 0.1362096518278122\n",
      "Epoch 2481, Loss: 0.3441448509693146, Final Batch Loss: 0.18413501977920532\n",
      "Epoch 2482, Loss: 0.3558637350797653, Final Batch Loss: 0.15920349955558777\n",
      "Epoch 2483, Loss: 0.3493734747171402, Final Batch Loss: 0.18807275593280792\n",
      "Epoch 2484, Loss: 0.4031737893819809, Final Batch Loss: 0.1875559538602829\n",
      "Epoch 2485, Loss: 0.33147385716438293, Final Batch Loss: 0.1903773546218872\n",
      "Epoch 2486, Loss: 0.3355713039636612, Final Batch Loss: 0.16711604595184326\n",
      "Epoch 2487, Loss: 0.31385698914527893, Final Batch Loss: 0.14916472136974335\n",
      "Epoch 2488, Loss: 0.3637802004814148, Final Batch Loss: 0.2338334172964096\n",
      "Epoch 2489, Loss: 0.40177667140960693, Final Batch Loss: 0.24355775117874146\n",
      "Epoch 2490, Loss: 0.3510070890188217, Final Batch Loss: 0.19197286665439606\n",
      "Epoch 2491, Loss: 0.3650206923484802, Final Batch Loss: 0.19026648998260498\n",
      "Epoch 2492, Loss: 0.3738257884979248, Final Batch Loss: 0.1856769323348999\n",
      "Epoch 2493, Loss: 0.3694751113653183, Final Batch Loss: 0.16676472127437592\n",
      "Epoch 2494, Loss: 0.32866623997688293, Final Batch Loss: 0.17269957065582275\n",
      "Epoch 2495, Loss: 0.3832346647977829, Final Batch Loss: 0.1559799164533615\n",
      "Epoch 2496, Loss: 0.3159259483218193, Final Batch Loss: 0.19586493074893951\n",
      "Epoch 2497, Loss: 0.36397042870521545, Final Batch Loss: 0.1789770871400833\n",
      "Epoch 2498, Loss: 0.4528072327375412, Final Batch Loss: 0.2981332838535309\n",
      "Epoch 2499, Loss: 0.3297799825668335, Final Batch Loss: 0.16282953321933746\n",
      "Epoch 2500, Loss: 0.34868955612182617, Final Batch Loss: 0.16197653114795685\n",
      "Epoch 2501, Loss: 0.36548325419425964, Final Batch Loss: 0.2117449790239334\n",
      "Epoch 2502, Loss: 0.3588133603334427, Final Batch Loss: 0.19917966425418854\n",
      "Epoch 2503, Loss: 0.3705075681209564, Final Batch Loss: 0.17638863623142242\n",
      "Epoch 2504, Loss: 0.3279745578765869, Final Batch Loss: 0.14772790670394897\n",
      "Epoch 2505, Loss: 0.44136564433574677, Final Batch Loss: 0.1731054037809372\n",
      "Epoch 2506, Loss: 0.3255310356616974, Final Batch Loss: 0.19123214483261108\n",
      "Epoch 2507, Loss: 0.32981903851032257, Final Batch Loss: 0.18767064809799194\n",
      "Epoch 2508, Loss: 0.37361373007297516, Final Batch Loss: 0.18551890552043915\n",
      "Epoch 2509, Loss: 0.3628924936056137, Final Batch Loss: 0.20330055058002472\n",
      "Epoch 2510, Loss: 0.35174134373664856, Final Batch Loss: 0.1517304629087448\n",
      "Epoch 2511, Loss: 0.363341361284256, Final Batch Loss: 0.17345649003982544\n",
      "Epoch 2512, Loss: 0.3495309203863144, Final Batch Loss: 0.18685685098171234\n",
      "Epoch 2513, Loss: 0.38446570932865143, Final Batch Loss: 0.2038712352514267\n",
      "Epoch 2514, Loss: 0.3573644310235977, Final Batch Loss: 0.18121428787708282\n",
      "Epoch 2515, Loss: 0.3431572765111923, Final Batch Loss: 0.19550848007202148\n",
      "Epoch 2516, Loss: 0.3317531645298004, Final Batch Loss: 0.1916833519935608\n",
      "Epoch 2517, Loss: 0.3345814347267151, Final Batch Loss: 0.17099982500076294\n",
      "Epoch 2518, Loss: 0.32434043288230896, Final Batch Loss: 0.18636392056941986\n",
      "Epoch 2519, Loss: 0.37720683217048645, Final Batch Loss: 0.19918997585773468\n",
      "Epoch 2520, Loss: 0.41961531341075897, Final Batch Loss: 0.220534086227417\n",
      "Epoch 2521, Loss: 0.32550618052482605, Final Batch Loss: 0.15972647070884705\n",
      "Epoch 2522, Loss: 0.34406954050064087, Final Batch Loss: 0.19248823821544647\n",
      "Epoch 2523, Loss: 0.35761161148548126, Final Batch Loss: 0.1537601500749588\n",
      "Epoch 2524, Loss: 0.3221127837896347, Final Batch Loss: 0.16317017376422882\n",
      "Epoch 2525, Loss: 0.3477182537317276, Final Batch Loss: 0.16500136256217957\n",
      "Epoch 2526, Loss: 0.343757227063179, Final Batch Loss: 0.1938256174325943\n",
      "Epoch 2527, Loss: 0.34052157402038574, Final Batch Loss: 0.15901444852352142\n",
      "Epoch 2528, Loss: 0.3834044188261032, Final Batch Loss: 0.1526651233434677\n",
      "Epoch 2529, Loss: 0.3332812637090683, Final Batch Loss: 0.17655342817306519\n",
      "Epoch 2530, Loss: 0.32744282484054565, Final Batch Loss: 0.16983652114868164\n",
      "Epoch 2531, Loss: 0.3710404634475708, Final Batch Loss: 0.19640986621379852\n",
      "Epoch 2532, Loss: 0.35113295912742615, Final Batch Loss: 0.16068555414676666\n",
      "Epoch 2533, Loss: 0.3420112878084183, Final Batch Loss: 0.16627509891986847\n",
      "Epoch 2534, Loss: 0.35334546864032745, Final Batch Loss: 0.17125071585178375\n",
      "Epoch 2535, Loss: 0.39323920011520386, Final Batch Loss: 0.20218288898468018\n",
      "Epoch 2536, Loss: 0.324165478348732, Final Batch Loss: 0.1711902767419815\n",
      "Epoch 2537, Loss: 0.34275639057159424, Final Batch Loss: 0.1726604849100113\n",
      "Epoch 2538, Loss: 0.3459126651287079, Final Batch Loss: 0.15959478914737701\n",
      "Epoch 2539, Loss: 0.38795389235019684, Final Batch Loss: 0.13387836515903473\n",
      "Epoch 2540, Loss: 0.3093450963497162, Final Batch Loss: 0.17057092487812042\n",
      "Epoch 2541, Loss: 0.32785654067993164, Final Batch Loss: 0.15822739899158478\n",
      "Epoch 2542, Loss: 0.30135777592658997, Final Batch Loss: 0.12607812881469727\n",
      "Epoch 2543, Loss: 0.3822755366563797, Final Batch Loss: 0.22293256223201752\n",
      "Epoch 2544, Loss: 0.39049556851387024, Final Batch Loss: 0.22515541315078735\n",
      "Epoch 2545, Loss: 0.3632880449295044, Final Batch Loss: 0.17872463166713715\n",
      "Epoch 2546, Loss: 0.3319282531738281, Final Batch Loss: 0.18098600208759308\n",
      "Epoch 2547, Loss: 0.37756727635860443, Final Batch Loss: 0.1555453985929489\n",
      "Epoch 2548, Loss: 0.36808718740940094, Final Batch Loss: 0.17938029766082764\n",
      "Epoch 2549, Loss: 0.3506907671689987, Final Batch Loss: 0.18927247822284698\n",
      "Epoch 2550, Loss: 0.39844241738319397, Final Batch Loss: 0.1917974352836609\n",
      "Epoch 2551, Loss: 0.37034109234809875, Final Batch Loss: 0.2082621455192566\n",
      "Epoch 2552, Loss: 0.3322766423225403, Final Batch Loss: 0.15262745320796967\n",
      "Epoch 2553, Loss: 0.3778438866138458, Final Batch Loss: 0.17610599100589752\n",
      "Epoch 2554, Loss: 0.4170398861169815, Final Batch Loss: 0.17617547512054443\n",
      "Epoch 2555, Loss: 0.34939317405223846, Final Batch Loss: 0.15959392488002777\n",
      "Epoch 2556, Loss: 0.3808787614107132, Final Batch Loss: 0.20316000282764435\n",
      "Epoch 2557, Loss: 0.33363498747348785, Final Batch Loss: 0.1700240969657898\n",
      "Epoch 2558, Loss: 0.33680348098278046, Final Batch Loss: 0.16895230114459991\n",
      "Epoch 2559, Loss: 0.34206052124500275, Final Batch Loss: 0.15408705174922943\n",
      "Epoch 2560, Loss: 0.33397167921066284, Final Batch Loss: 0.1865403652191162\n",
      "Epoch 2561, Loss: 0.3507661074399948, Final Batch Loss: 0.166800856590271\n",
      "Epoch 2562, Loss: 0.3739354908466339, Final Batch Loss: 0.16700781881809235\n",
      "Epoch 2563, Loss: 0.3400491327047348, Final Batch Loss: 0.1598430722951889\n",
      "Epoch 2564, Loss: 0.37764833867549896, Final Batch Loss: 0.17104096710681915\n",
      "Epoch 2565, Loss: 0.3454303592443466, Final Batch Loss: 0.17936454713344574\n",
      "Epoch 2566, Loss: 0.32781900465488434, Final Batch Loss: 0.16120581328868866\n",
      "Epoch 2567, Loss: 0.37866078317165375, Final Batch Loss: 0.22275929152965546\n",
      "Epoch 2568, Loss: 0.3779112845659256, Final Batch Loss: 0.17114146053791046\n",
      "Epoch 2569, Loss: 0.31802400946617126, Final Batch Loss: 0.18459852039813995\n",
      "Epoch 2570, Loss: 0.40840332210063934, Final Batch Loss: 0.23839622735977173\n",
      "Epoch 2571, Loss: 0.333863765001297, Final Batch Loss: 0.16338779032230377\n",
      "Epoch 2572, Loss: 0.3328961580991745, Final Batch Loss: 0.1376204788684845\n",
      "Epoch 2573, Loss: 0.3695969879627228, Final Batch Loss: 0.21609176695346832\n",
      "Epoch 2574, Loss: 0.3225340396165848, Final Batch Loss: 0.16333244740962982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2575, Loss: 0.4432312399148941, Final Batch Loss: 0.21555428206920624\n",
      "Epoch 2576, Loss: 0.3672148585319519, Final Batch Loss: 0.18674325942993164\n",
      "Epoch 2577, Loss: 0.3920137882232666, Final Batch Loss: 0.208708718419075\n",
      "Epoch 2578, Loss: 0.3180041164159775, Final Batch Loss: 0.1628825068473816\n",
      "Epoch 2579, Loss: 0.3429131358861923, Final Batch Loss: 0.17399998009204865\n",
      "Epoch 2580, Loss: 0.3481714129447937, Final Batch Loss: 0.16479423642158508\n",
      "Epoch 2581, Loss: 0.3212006837129593, Final Batch Loss: 0.15265578031539917\n",
      "Epoch 2582, Loss: 0.3600389212369919, Final Batch Loss: 0.13202880322933197\n",
      "Epoch 2583, Loss: 0.3695981204509735, Final Batch Loss: 0.2017810195684433\n",
      "Epoch 2584, Loss: 0.36301931738853455, Final Batch Loss: 0.1814146786928177\n",
      "Epoch 2585, Loss: 0.3409067541360855, Final Batch Loss: 0.17832505702972412\n",
      "Epoch 2586, Loss: 0.3554888218641281, Final Batch Loss: 0.20463983714580536\n",
      "Epoch 2587, Loss: 0.3315756171941757, Final Batch Loss: 0.15460829436779022\n",
      "Epoch 2588, Loss: 0.35237331688404083, Final Batch Loss: 0.1546676754951477\n",
      "Epoch 2589, Loss: 0.3721356838941574, Final Batch Loss: 0.2092924863100052\n",
      "Epoch 2590, Loss: 0.39809079468250275, Final Batch Loss: 0.1619553416967392\n",
      "Epoch 2591, Loss: 0.3761027008295059, Final Batch Loss: 0.1681913137435913\n",
      "Epoch 2592, Loss: 0.3675747960805893, Final Batch Loss: 0.18289701640605927\n",
      "Epoch 2593, Loss: 0.32556115090847015, Final Batch Loss: 0.16051234304904938\n",
      "Epoch 2594, Loss: 0.3748033344745636, Final Batch Loss: 0.1911347657442093\n",
      "Epoch 2595, Loss: 0.34059952199459076, Final Batch Loss: 0.17163048684597015\n",
      "Epoch 2596, Loss: 0.4031214565038681, Final Batch Loss: 0.22175335884094238\n",
      "Epoch 2597, Loss: 0.3433361202478409, Final Batch Loss: 0.17041760683059692\n",
      "Epoch 2598, Loss: 0.3374527394771576, Final Batch Loss: 0.18379385769367218\n",
      "Epoch 2599, Loss: 0.3379207104444504, Final Batch Loss: 0.18403242528438568\n",
      "Epoch 2600, Loss: 0.331734374165535, Final Batch Loss: 0.17816269397735596\n",
      "Epoch 2601, Loss: 0.3478303700685501, Final Batch Loss: 0.2211277335882187\n",
      "Epoch 2602, Loss: 0.32777760922908783, Final Batch Loss: 0.15744607150554657\n",
      "Epoch 2603, Loss: 0.3698839098215103, Final Batch Loss: 0.2219565361738205\n",
      "Epoch 2604, Loss: 0.31462058424949646, Final Batch Loss: 0.1384219229221344\n",
      "Epoch 2605, Loss: 0.3682934641838074, Final Batch Loss: 0.19080530107021332\n",
      "Epoch 2606, Loss: 0.3480338901281357, Final Batch Loss: 0.20898061990737915\n",
      "Epoch 2607, Loss: 0.3750464618206024, Final Batch Loss: 0.17759369313716888\n",
      "Epoch 2608, Loss: 0.3292786329984665, Final Batch Loss: 0.1833430975675583\n",
      "Epoch 2609, Loss: 0.3319336175918579, Final Batch Loss: 0.12797196209430695\n",
      "Epoch 2610, Loss: 0.3693358600139618, Final Batch Loss: 0.1857643574476242\n",
      "Epoch 2611, Loss: 0.35079629719257355, Final Batch Loss: 0.18183942139148712\n",
      "Epoch 2612, Loss: 0.33154459297657013, Final Batch Loss: 0.17026804387569427\n",
      "Epoch 2613, Loss: 0.37136395275592804, Final Batch Loss: 0.18529373407363892\n",
      "Epoch 2614, Loss: 0.310324028134346, Final Batch Loss: 0.13272826373577118\n",
      "Epoch 2615, Loss: 0.3511093854904175, Final Batch Loss: 0.20071224868297577\n",
      "Epoch 2616, Loss: 0.3493262082338333, Final Batch Loss: 0.15094724297523499\n",
      "Epoch 2617, Loss: 0.37034545838832855, Final Batch Loss: 0.20618851482868195\n",
      "Epoch 2618, Loss: 0.3205723762512207, Final Batch Loss: 0.15194953978061676\n",
      "Epoch 2619, Loss: 0.4022328555583954, Final Batch Loss: 0.18924687802791595\n",
      "Epoch 2620, Loss: 0.3630937188863754, Final Batch Loss: 0.14096282422542572\n",
      "Epoch 2621, Loss: 0.34702935814857483, Final Batch Loss: 0.14388470351696014\n",
      "Epoch 2622, Loss: 0.370327889919281, Final Batch Loss: 0.1633255034685135\n",
      "Epoch 2623, Loss: 0.36128389835357666, Final Batch Loss: 0.17596407234668732\n",
      "Epoch 2624, Loss: 0.34198567271232605, Final Batch Loss: 0.17906878888607025\n",
      "Epoch 2625, Loss: 0.37813250720500946, Final Batch Loss: 0.21587471663951874\n",
      "Epoch 2626, Loss: 0.3221176415681839, Final Batch Loss: 0.1686369776725769\n",
      "Epoch 2627, Loss: 0.3597068190574646, Final Batch Loss: 0.18614256381988525\n",
      "Epoch 2628, Loss: 0.36456841230392456, Final Batch Loss: 0.18766182661056519\n",
      "Epoch 2629, Loss: 0.3440244644880295, Final Batch Loss: 0.14668291807174683\n",
      "Epoch 2630, Loss: 0.3516553193330765, Final Batch Loss: 0.16350124776363373\n",
      "Epoch 2631, Loss: 0.3251885771751404, Final Batch Loss: 0.13849617540836334\n",
      "Epoch 2632, Loss: 0.3038238137960434, Final Batch Loss: 0.12870167195796967\n",
      "Epoch 2633, Loss: 0.32622651755809784, Final Batch Loss: 0.14981381595134735\n",
      "Epoch 2634, Loss: 0.31377817690372467, Final Batch Loss: 0.15940605103969574\n",
      "Epoch 2635, Loss: 0.2961115837097168, Final Batch Loss: 0.12848637998104095\n",
      "Epoch 2636, Loss: 0.34200596809387207, Final Batch Loss: 0.17167605459690094\n",
      "Epoch 2637, Loss: 0.38196614384651184, Final Batch Loss: 0.19847679138183594\n",
      "Epoch 2638, Loss: 0.35688525438308716, Final Batch Loss: 0.17526888847351074\n",
      "Epoch 2639, Loss: 0.37649619579315186, Final Batch Loss: 0.13101650774478912\n",
      "Epoch 2640, Loss: 0.30766503512859344, Final Batch Loss: 0.16405551135540009\n",
      "Epoch 2641, Loss: 0.39360223710536957, Final Batch Loss: 0.22992779314517975\n",
      "Epoch 2642, Loss: 0.3506326973438263, Final Batch Loss: 0.14306508004665375\n",
      "Epoch 2643, Loss: 0.33644381165504456, Final Batch Loss: 0.1372748166322708\n",
      "Epoch 2644, Loss: 0.38174644112586975, Final Batch Loss: 0.19520901143550873\n",
      "Epoch 2645, Loss: 0.35303643345832825, Final Batch Loss: 0.21194052696228027\n",
      "Epoch 2646, Loss: 0.32872283458709717, Final Batch Loss: 0.163496732711792\n",
      "Epoch 2647, Loss: 0.36380596458911896, Final Batch Loss: 0.18592184782028198\n",
      "Epoch 2648, Loss: 0.3240683823823929, Final Batch Loss: 0.137142613530159\n",
      "Epoch 2649, Loss: 0.36279891431331635, Final Batch Loss: 0.1812608242034912\n",
      "Epoch 2650, Loss: 0.37043388187885284, Final Batch Loss: 0.2048410028219223\n",
      "Epoch 2651, Loss: 0.32363350689411163, Final Batch Loss: 0.1499493420124054\n",
      "Epoch 2652, Loss: 0.31501540541648865, Final Batch Loss: 0.13662026822566986\n",
      "Epoch 2653, Loss: 0.36147721111774445, Final Batch Loss: 0.17321033775806427\n",
      "Epoch 2654, Loss: 0.3391321003437042, Final Batch Loss: 0.16610880196094513\n",
      "Epoch 2655, Loss: 0.3486221581697464, Final Batch Loss: 0.19493967294692993\n",
      "Epoch 2656, Loss: 0.38915765285491943, Final Batch Loss: 0.23737108707427979\n",
      "Epoch 2657, Loss: 0.3251282870769501, Final Batch Loss: 0.1572212427854538\n",
      "Epoch 2658, Loss: 0.3397606909275055, Final Batch Loss: 0.1621444672346115\n",
      "Epoch 2659, Loss: 0.35937361419200897, Final Batch Loss: 0.1695912629365921\n",
      "Epoch 2660, Loss: 0.31311656534671783, Final Batch Loss: 0.14366213977336884\n",
      "Epoch 2661, Loss: 0.32471685111522675, Final Batch Loss: 0.18863417208194733\n",
      "Epoch 2662, Loss: 0.3358386754989624, Final Batch Loss: 0.1954612135887146\n",
      "Epoch 2663, Loss: 0.37968482077121735, Final Batch Loss: 0.18925492465496063\n",
      "Epoch 2664, Loss: 0.3468524217605591, Final Batch Loss: 0.18633504211902618\n",
      "Epoch 2665, Loss: 0.2895094007253647, Final Batch Loss: 0.1362660974264145\n",
      "Epoch 2666, Loss: 0.35496771335601807, Final Batch Loss: 0.18955792486667633\n",
      "Epoch 2667, Loss: 0.3201781362295151, Final Batch Loss: 0.14382119476795197\n",
      "Epoch 2668, Loss: 0.3169485032558441, Final Batch Loss: 0.18000368773937225\n",
      "Epoch 2669, Loss: 0.33724139630794525, Final Batch Loss: 0.16230902075767517\n",
      "Epoch 2670, Loss: 0.33434806764125824, Final Batch Loss: 0.15701819956302643\n",
      "Epoch 2671, Loss: 0.3067654073238373, Final Batch Loss: 0.12623949348926544\n",
      "Epoch 2672, Loss: 0.3032870590686798, Final Batch Loss: 0.15229254961013794\n",
      "Epoch 2673, Loss: 0.3142724335193634, Final Batch Loss: 0.13255102932453156\n",
      "Epoch 2674, Loss: 0.33554188907146454, Final Batch Loss: 0.1631058305501938\n",
      "Epoch 2675, Loss: 0.33789651095867157, Final Batch Loss: 0.1745547205209732\n",
      "Epoch 2676, Loss: 0.3363174796104431, Final Batch Loss: 0.13532325625419617\n",
      "Epoch 2677, Loss: 0.3349846601486206, Final Batch Loss: 0.18548385798931122\n",
      "Epoch 2678, Loss: 0.35004575550556183, Final Batch Loss: 0.1768025904893875\n",
      "Epoch 2679, Loss: 0.30144383013248444, Final Batch Loss: 0.13127095997333527\n",
      "Epoch 2680, Loss: 0.34190861880779266, Final Batch Loss: 0.1367207020521164\n",
      "Epoch 2681, Loss: 0.3462059646844864, Final Batch Loss: 0.1680728942155838\n",
      "Epoch 2682, Loss: 0.3496791571378708, Final Batch Loss: 0.17093975841999054\n",
      "Epoch 2683, Loss: 0.3290458768606186, Final Batch Loss: 0.1740465611219406\n",
      "Epoch 2684, Loss: 0.3484017699956894, Final Batch Loss: 0.17107200622558594\n",
      "Epoch 2685, Loss: 0.31986185908317566, Final Batch Loss: 0.155828058719635\n",
      "Epoch 2686, Loss: 0.3266998529434204, Final Batch Loss: 0.15641231834888458\n",
      "Epoch 2687, Loss: 0.31982608139514923, Final Batch Loss: 0.15951718389987946\n",
      "Epoch 2688, Loss: 0.3797253519296646, Final Batch Loss: 0.23273999989032745\n",
      "Epoch 2689, Loss: 0.33642855286598206, Final Batch Loss: 0.1655741035938263\n",
      "Epoch 2690, Loss: 0.32189807295799255, Final Batch Loss: 0.12749910354614258\n",
      "Epoch 2691, Loss: 0.3454558104276657, Final Batch Loss: 0.1717478632926941\n",
      "Epoch 2692, Loss: 0.34257809817790985, Final Batch Loss: 0.17533783614635468\n",
      "Epoch 2693, Loss: 0.34055282175540924, Final Batch Loss: 0.16942071914672852\n",
      "Epoch 2694, Loss: 0.3580538183450699, Final Batch Loss: 0.16383542120456696\n",
      "Epoch 2695, Loss: 0.29974478483200073, Final Batch Loss: 0.1348586082458496\n",
      "Epoch 2696, Loss: 0.3391367197036743, Final Batch Loss: 0.14084701240062714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2697, Loss: 0.31250062584877014, Final Batch Loss: 0.16125939786434174\n",
      "Epoch 2698, Loss: 0.32884880900382996, Final Batch Loss: 0.15528641641139984\n",
      "Epoch 2699, Loss: 0.2994982898235321, Final Batch Loss: 0.1329200714826584\n",
      "Epoch 2700, Loss: 0.3607797473669052, Final Batch Loss: 0.1758365035057068\n",
      "Epoch 2701, Loss: 0.34013302624225616, Final Batch Loss: 0.14545880258083344\n",
      "Epoch 2702, Loss: 0.33131514489650726, Final Batch Loss: 0.13773490488529205\n",
      "Epoch 2703, Loss: 0.3082677125930786, Final Batch Loss: 0.15907631814479828\n",
      "Epoch 2704, Loss: 0.32929810881614685, Final Batch Loss: 0.15834268927574158\n",
      "Epoch 2705, Loss: 0.3717832863330841, Final Batch Loss: 0.21845994889736176\n",
      "Epoch 2706, Loss: 0.3535822629928589, Final Batch Loss: 0.17720556259155273\n",
      "Epoch 2707, Loss: 0.3327351361513138, Final Batch Loss: 0.1789691001176834\n",
      "Epoch 2708, Loss: 0.3153250366449356, Final Batch Loss: 0.17284220457077026\n",
      "Epoch 2709, Loss: 0.31527116894721985, Final Batch Loss: 0.13107697665691376\n",
      "Epoch 2710, Loss: 0.38408420979976654, Final Batch Loss: 0.19769752025604248\n",
      "Epoch 2711, Loss: 0.3768952339887619, Final Batch Loss: 0.1777312010526657\n",
      "Epoch 2712, Loss: 0.3239937424659729, Final Batch Loss: 0.15574920177459717\n",
      "Epoch 2713, Loss: 0.33002398908138275, Final Batch Loss: 0.16566134989261627\n",
      "Epoch 2714, Loss: 0.3386126905679703, Final Batch Loss: 0.1446346491575241\n",
      "Epoch 2715, Loss: 0.3070910573005676, Final Batch Loss: 0.1699734777212143\n",
      "Epoch 2716, Loss: 0.33243341743946075, Final Batch Loss: 0.18818636238574982\n",
      "Epoch 2717, Loss: 0.3358801305294037, Final Batch Loss: 0.1436784714460373\n",
      "Epoch 2718, Loss: 0.342393696308136, Final Batch Loss: 0.1522398740053177\n",
      "Epoch 2719, Loss: 0.3303253799676895, Final Batch Loss: 0.16063912212848663\n",
      "Epoch 2720, Loss: 0.35662175714969635, Final Batch Loss: 0.16594532132148743\n",
      "Epoch 2721, Loss: 0.3493874967098236, Final Batch Loss: 0.15910135209560394\n",
      "Epoch 2722, Loss: 0.3194607049226761, Final Batch Loss: 0.18725097179412842\n",
      "Epoch 2723, Loss: 0.35269804298877716, Final Batch Loss: 0.18389083445072174\n",
      "Epoch 2724, Loss: 0.29171644151210785, Final Batch Loss: 0.14248467981815338\n",
      "Epoch 2725, Loss: 0.289322629570961, Final Batch Loss: 0.15109796822071075\n",
      "Epoch 2726, Loss: 0.31487375497817993, Final Batch Loss: 0.15272223949432373\n",
      "Epoch 2727, Loss: 0.37703031301498413, Final Batch Loss: 0.16129037737846375\n",
      "Epoch 2728, Loss: 0.36245276033878326, Final Batch Loss: 0.19627785682678223\n",
      "Epoch 2729, Loss: 0.35283082723617554, Final Batch Loss: 0.19288621842861176\n",
      "Epoch 2730, Loss: 0.33253471553325653, Final Batch Loss: 0.16989493370056152\n",
      "Epoch 2731, Loss: 0.3749435693025589, Final Batch Loss: 0.16952912509441376\n",
      "Epoch 2732, Loss: 0.3723713457584381, Final Batch Loss: 0.23128600418567657\n",
      "Epoch 2733, Loss: 0.3362368792295456, Final Batch Loss: 0.17319734394550323\n",
      "Epoch 2734, Loss: 0.35182565450668335, Final Batch Loss: 0.19281160831451416\n",
      "Epoch 2735, Loss: 0.34384216368198395, Final Batch Loss: 0.19388078153133392\n",
      "Epoch 2736, Loss: 0.3261389434337616, Final Batch Loss: 0.16610535979270935\n",
      "Epoch 2737, Loss: 0.3170151263475418, Final Batch Loss: 0.15224690735340118\n",
      "Epoch 2738, Loss: 0.3409750908613205, Final Batch Loss: 0.14056992530822754\n",
      "Epoch 2739, Loss: 0.34263820946216583, Final Batch Loss: 0.15445156395435333\n",
      "Epoch 2740, Loss: 0.30173395574092865, Final Batch Loss: 0.14523769915103912\n",
      "Epoch 2741, Loss: 0.28459028899669647, Final Batch Loss: 0.13970722258090973\n",
      "Epoch 2742, Loss: 0.3154330551624298, Final Batch Loss: 0.15709374845027924\n",
      "Epoch 2743, Loss: 0.3038688749074936, Final Batch Loss: 0.16887784004211426\n",
      "Epoch 2744, Loss: 0.3655526638031006, Final Batch Loss: 0.1598120778799057\n",
      "Epoch 2745, Loss: 0.32354842126369476, Final Batch Loss: 0.18426986038684845\n",
      "Epoch 2746, Loss: 0.3286503851413727, Final Batch Loss: 0.144887313246727\n",
      "Epoch 2747, Loss: 0.31401827931404114, Final Batch Loss: 0.17719674110412598\n",
      "Epoch 2748, Loss: 0.34721308946609497, Final Batch Loss: 0.21029675006866455\n",
      "Epoch 2749, Loss: 0.327235147356987, Final Batch Loss: 0.14877165853977203\n",
      "Epoch 2750, Loss: 0.3127988874912262, Final Batch Loss: 0.14456751942634583\n",
      "Epoch 2751, Loss: 0.33187687397003174, Final Batch Loss: 0.14260710775852203\n",
      "Epoch 2752, Loss: 0.32479049265384674, Final Batch Loss: 0.16848349571228027\n",
      "Epoch 2753, Loss: 0.3110233098268509, Final Batch Loss: 0.13987533748149872\n",
      "Epoch 2754, Loss: 0.3061394691467285, Final Batch Loss: 0.17216217517852783\n",
      "Epoch 2755, Loss: 0.3292636573314667, Final Batch Loss: 0.17202425003051758\n",
      "Epoch 2756, Loss: 0.31274791061878204, Final Batch Loss: 0.1466096043586731\n",
      "Epoch 2757, Loss: 0.3130933493375778, Final Batch Loss: 0.16180402040481567\n",
      "Epoch 2758, Loss: 0.30856461822986603, Final Batch Loss: 0.12564222514629364\n",
      "Epoch 2759, Loss: 0.35340191423892975, Final Batch Loss: 0.20187795162200928\n",
      "Epoch 2760, Loss: 0.3393598198890686, Final Batch Loss: 0.15051543712615967\n",
      "Epoch 2761, Loss: 0.35106030106544495, Final Batch Loss: 0.19947481155395508\n",
      "Epoch 2762, Loss: 0.2946578711271286, Final Batch Loss: 0.12714917957782745\n",
      "Epoch 2763, Loss: 0.2967480570077896, Final Batch Loss: 0.16486772894859314\n",
      "Epoch 2764, Loss: 0.3322454243898392, Final Batch Loss: 0.19009973108768463\n",
      "Epoch 2765, Loss: 0.3713603913784027, Final Batch Loss: 0.20652705430984497\n",
      "Epoch 2766, Loss: 0.3306013196706772, Final Batch Loss: 0.17755454778671265\n",
      "Epoch 2767, Loss: 0.2920565903186798, Final Batch Loss: 0.14583928883075714\n",
      "Epoch 2768, Loss: 0.37162694334983826, Final Batch Loss: 0.19043384492397308\n",
      "Epoch 2769, Loss: 0.3237924575805664, Final Batch Loss: 0.17806337773799896\n",
      "Epoch 2770, Loss: 0.32483668625354767, Final Batch Loss: 0.15060733258724213\n",
      "Epoch 2771, Loss: 0.29829320311546326, Final Batch Loss: 0.15897801518440247\n",
      "Epoch 2772, Loss: 0.30929917097091675, Final Batch Loss: 0.16111229360103607\n",
      "Epoch 2773, Loss: 0.30593544244766235, Final Batch Loss: 0.1312309354543686\n",
      "Epoch 2774, Loss: 0.3124643415212631, Final Batch Loss: 0.16209328174591064\n",
      "Epoch 2775, Loss: 0.34303243458271027, Final Batch Loss: 0.15290842950344086\n",
      "Epoch 2776, Loss: 0.32569144666194916, Final Batch Loss: 0.18448567390441895\n",
      "Epoch 2777, Loss: 0.29846641421318054, Final Batch Loss: 0.13239741325378418\n",
      "Epoch 2778, Loss: 0.32477761805057526, Final Batch Loss: 0.15983588993549347\n",
      "Epoch 2779, Loss: 0.30567599833011627, Final Batch Loss: 0.14869016408920288\n",
      "Epoch 2780, Loss: 0.32687005400657654, Final Batch Loss: 0.14699098467826843\n",
      "Epoch 2781, Loss: 0.25891534239053726, Final Batch Loss: 0.09024225920438766\n",
      "Epoch 2782, Loss: 0.3075800687074661, Final Batch Loss: 0.1461680382490158\n",
      "Epoch 2783, Loss: 0.29634036123752594, Final Batch Loss: 0.14455877244472504\n",
      "Epoch 2784, Loss: 0.3339666798710823, Final Batch Loss: 0.20991265773773193\n",
      "Epoch 2785, Loss: 0.30349940061569214, Final Batch Loss: 0.14318345487117767\n",
      "Epoch 2786, Loss: 0.309566468000412, Final Batch Loss: 0.15646934509277344\n",
      "Epoch 2787, Loss: 0.30193573236465454, Final Batch Loss: 0.14332816004753113\n",
      "Epoch 2788, Loss: 0.349613219499588, Final Batch Loss: 0.18367528915405273\n",
      "Epoch 2789, Loss: 0.35647860169410706, Final Batch Loss: 0.16857580840587616\n",
      "Epoch 2790, Loss: 0.33296386897563934, Final Batch Loss: 0.16108079254627228\n",
      "Epoch 2791, Loss: 0.31986401975154877, Final Batch Loss: 0.16032956540584564\n",
      "Epoch 2792, Loss: 0.35854461789131165, Final Batch Loss: 0.18009096384048462\n",
      "Epoch 2793, Loss: 0.2819664850831032, Final Batch Loss: 0.16628789901733398\n",
      "Epoch 2794, Loss: 0.34548215568065643, Final Batch Loss: 0.18772242963314056\n",
      "Epoch 2795, Loss: 0.3318912237882614, Final Batch Loss: 0.1674172729253769\n",
      "Epoch 2796, Loss: 0.36685600876808167, Final Batch Loss: 0.17671245336532593\n",
      "Epoch 2797, Loss: 0.3033611476421356, Final Batch Loss: 0.14455527067184448\n",
      "Epoch 2798, Loss: 0.31231793761253357, Final Batch Loss: 0.16200627386569977\n",
      "Epoch 2799, Loss: 0.32490110397338867, Final Batch Loss: 0.18337948620319366\n",
      "Epoch 2800, Loss: 0.29824255406856537, Final Batch Loss: 0.16209104657173157\n",
      "Epoch 2801, Loss: 0.327566459774971, Final Batch Loss: 0.16263119876384735\n",
      "Epoch 2802, Loss: 0.35722698271274567, Final Batch Loss: 0.20259304344654083\n",
      "Epoch 2803, Loss: 0.35600370168685913, Final Batch Loss: 0.1882762759923935\n",
      "Epoch 2804, Loss: 0.32564350217580795, Final Batch Loss: 0.2112652063369751\n",
      "Epoch 2805, Loss: 0.2769581228494644, Final Batch Loss: 0.13452990353107452\n",
      "Epoch 2806, Loss: 0.2883322834968567, Final Batch Loss: 0.15047962963581085\n",
      "Epoch 2807, Loss: 0.3947278559207916, Final Batch Loss: 0.2066945880651474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2808, Loss: 0.3232342153787613, Final Batch Loss: 0.13791711628437042\n",
      "Epoch 2809, Loss: 0.318986639380455, Final Batch Loss: 0.15425905585289001\n",
      "Epoch 2810, Loss: 0.3570324182510376, Final Batch Loss: 0.19425933063030243\n",
      "Epoch 2811, Loss: 0.32184526324272156, Final Batch Loss: 0.17892591655254364\n",
      "Epoch 2812, Loss: 0.3827672004699707, Final Batch Loss: 0.18284021317958832\n",
      "Epoch 2813, Loss: 0.3196883946657181, Final Batch Loss: 0.16078674793243408\n",
      "Epoch 2814, Loss: 0.3524259626865387, Final Batch Loss: 0.18840175867080688\n",
      "Epoch 2815, Loss: 0.30760592222213745, Final Batch Loss: 0.14433155953884125\n",
      "Epoch 2816, Loss: 0.31399133801460266, Final Batch Loss: 0.14303499460220337\n",
      "Epoch 2817, Loss: 0.3173017352819443, Final Batch Loss: 0.14650286734104156\n",
      "Epoch 2818, Loss: 0.3048557639122009, Final Batch Loss: 0.15228435397148132\n",
      "Epoch 2819, Loss: 0.2895659804344177, Final Batch Loss: 0.15156957507133484\n",
      "Epoch 2820, Loss: 0.3146282285451889, Final Batch Loss: 0.16446872055530548\n",
      "Epoch 2821, Loss: 0.2888747602701187, Final Batch Loss: 0.14383123815059662\n",
      "Epoch 2822, Loss: 0.3146946281194687, Final Batch Loss: 0.15335161983966827\n",
      "Epoch 2823, Loss: 0.3380259573459625, Final Batch Loss: 0.16588565707206726\n",
      "Epoch 2824, Loss: 0.29806435108184814, Final Batch Loss: 0.1358249932527542\n",
      "Epoch 2825, Loss: 0.30667155236005783, Final Batch Loss: 0.11266995221376419\n",
      "Epoch 2826, Loss: 0.34660927951335907, Final Batch Loss: 0.17528407275676727\n",
      "Epoch 2827, Loss: 0.31875480711460114, Final Batch Loss: 0.18977844715118408\n",
      "Epoch 2828, Loss: 0.29031500220298767, Final Batch Loss: 0.14146268367767334\n",
      "Epoch 2829, Loss: 0.3266023099422455, Final Batch Loss: 0.15612675249576569\n",
      "Epoch 2830, Loss: 0.3124660700559616, Final Batch Loss: 0.15524576604366302\n",
      "Epoch 2831, Loss: 0.3280051499605179, Final Batch Loss: 0.1308029592037201\n",
      "Epoch 2832, Loss: 0.33360421657562256, Final Batch Loss: 0.15415403246879578\n",
      "Epoch 2833, Loss: 0.3312729001045227, Final Batch Loss: 0.1592860370874405\n",
      "Epoch 2834, Loss: 0.32338112592697144, Final Batch Loss: 0.1516556739807129\n",
      "Epoch 2835, Loss: 0.3335006535053253, Final Batch Loss: 0.17193067073822021\n",
      "Epoch 2836, Loss: 0.3185069262981415, Final Batch Loss: 0.16864778101444244\n",
      "Epoch 2837, Loss: 0.29615579545497894, Final Batch Loss: 0.14046461880207062\n",
      "Epoch 2838, Loss: 0.3133501559495926, Final Batch Loss: 0.17111976444721222\n",
      "Epoch 2839, Loss: 0.33030441403388977, Final Batch Loss: 0.20360489189624786\n",
      "Epoch 2840, Loss: 0.2668754458427429, Final Batch Loss: 0.14660178124904633\n",
      "Epoch 2841, Loss: 0.297707699239254, Final Batch Loss: 0.12432866543531418\n",
      "Epoch 2842, Loss: 0.30112791061401367, Final Batch Loss: 0.16786743700504303\n",
      "Epoch 2843, Loss: 0.342903196811676, Final Batch Loss: 0.12989841401576996\n",
      "Epoch 2844, Loss: 0.36021991074085236, Final Batch Loss: 0.1883828043937683\n",
      "Epoch 2845, Loss: 0.32268448173999786, Final Batch Loss: 0.17169994115829468\n",
      "Epoch 2846, Loss: 0.3195257931947708, Final Batch Loss: 0.15862718224525452\n",
      "Epoch 2847, Loss: 0.3070822060108185, Final Batch Loss: 0.16863448917865753\n",
      "Epoch 2848, Loss: 0.31305308640003204, Final Batch Loss: 0.15212252736091614\n",
      "Epoch 2849, Loss: 0.32041509449481964, Final Batch Loss: 0.12738348543643951\n",
      "Epoch 2850, Loss: 0.30718837678432465, Final Batch Loss: 0.17981505393981934\n",
      "Epoch 2851, Loss: 0.31921662390232086, Final Batch Loss: 0.1533057689666748\n",
      "Epoch 2852, Loss: 0.3100467622280121, Final Batch Loss: 0.1568351835012436\n",
      "Epoch 2853, Loss: 0.3340703696012497, Final Batch Loss: 0.15981818735599518\n",
      "Epoch 2854, Loss: 0.30160409212112427, Final Batch Loss: 0.15478238463401794\n",
      "Epoch 2855, Loss: 0.31848862767219543, Final Batch Loss: 0.18512769043445587\n",
      "Epoch 2856, Loss: 0.30535241961479187, Final Batch Loss: 0.14806653559207916\n",
      "Epoch 2857, Loss: 0.3420053571462631, Final Batch Loss: 0.18385624885559082\n",
      "Epoch 2858, Loss: 0.3360269367694855, Final Batch Loss: 0.19380228221416473\n",
      "Epoch 2859, Loss: 0.2713642492890358, Final Batch Loss: 0.12185027450323105\n",
      "Epoch 2860, Loss: 0.3417721539735794, Final Batch Loss: 0.1695212870836258\n",
      "Epoch 2861, Loss: 0.347690612077713, Final Batch Loss: 0.22448833286762238\n",
      "Epoch 2862, Loss: 0.3515414595603943, Final Batch Loss: 0.18097414076328278\n",
      "Epoch 2863, Loss: 0.32373273372650146, Final Batch Loss: 0.15759767591953278\n",
      "Epoch 2864, Loss: 0.35442908108234406, Final Batch Loss: 0.1901795119047165\n",
      "Epoch 2865, Loss: 0.28662803024053574, Final Batch Loss: 0.11486221104860306\n",
      "Epoch 2866, Loss: 0.34012168645858765, Final Batch Loss: 0.16711901128292084\n",
      "Epoch 2867, Loss: 0.32352565228939056, Final Batch Loss: 0.17457014322280884\n",
      "Epoch 2868, Loss: 0.3673218637704849, Final Batch Loss: 0.18006421625614166\n",
      "Epoch 2869, Loss: 0.4019242823123932, Final Batch Loss: 0.2361891269683838\n",
      "Epoch 2870, Loss: 0.38540516793727875, Final Batch Loss: 0.19988232851028442\n",
      "Epoch 2871, Loss: 0.353670135140419, Final Batch Loss: 0.19063745439052582\n",
      "Epoch 2872, Loss: 0.3193691372871399, Final Batch Loss: 0.16078178584575653\n",
      "Epoch 2873, Loss: 0.3045625686645508, Final Batch Loss: 0.12604837119579315\n",
      "Epoch 2874, Loss: 0.34986691176891327, Final Batch Loss: 0.17699645459651947\n",
      "Epoch 2875, Loss: 0.34208469092845917, Final Batch Loss: 0.18772633373737335\n",
      "Epoch 2876, Loss: 0.3085078299045563, Final Batch Loss: 0.16407497227191925\n",
      "Epoch 2877, Loss: 0.38069698214530945, Final Batch Loss: 0.19264937937259674\n",
      "Epoch 2878, Loss: 0.2838010936975479, Final Batch Loss: 0.14765015244483948\n",
      "Epoch 2879, Loss: 0.3066682815551758, Final Batch Loss: 0.16729728877544403\n",
      "Epoch 2880, Loss: 0.3557947129011154, Final Batch Loss: 0.16008810698986053\n",
      "Epoch 2881, Loss: 0.36632026731967926, Final Batch Loss: 0.18776869773864746\n",
      "Epoch 2882, Loss: 0.3223170191049576, Final Batch Loss: 0.1460070163011551\n",
      "Epoch 2883, Loss: 0.30187276005744934, Final Batch Loss: 0.16859738528728485\n",
      "Epoch 2884, Loss: 0.33667729794979095, Final Batch Loss: 0.18495480716228485\n",
      "Epoch 2885, Loss: 0.3777073472738266, Final Batch Loss: 0.23494024574756622\n",
      "Epoch 2886, Loss: 0.3681239038705826, Final Batch Loss: 0.19979500770568848\n",
      "Epoch 2887, Loss: 0.2746587246656418, Final Batch Loss: 0.12959569692611694\n",
      "Epoch 2888, Loss: 0.304258793592453, Final Batch Loss: 0.1380302608013153\n",
      "Epoch 2889, Loss: 0.3256881386041641, Final Batch Loss: 0.17037749290466309\n",
      "Epoch 2890, Loss: 0.29578956961631775, Final Batch Loss: 0.11628404259681702\n",
      "Epoch 2891, Loss: 0.2896052449941635, Final Batch Loss: 0.14052091538906097\n",
      "Epoch 2892, Loss: 0.2911970764398575, Final Batch Loss: 0.13523580133914948\n",
      "Epoch 2893, Loss: 0.34346936643123627, Final Batch Loss: 0.14319086074829102\n",
      "Epoch 2894, Loss: 0.29663991183042526, Final Batch Loss: 0.18094699084758759\n",
      "Epoch 2895, Loss: 0.30576377362012863, Final Batch Loss: 0.10691089183092117\n",
      "Epoch 2896, Loss: 0.32821956276893616, Final Batch Loss: 0.17400537431240082\n",
      "Epoch 2897, Loss: 0.30817118287086487, Final Batch Loss: 0.1653318554162979\n",
      "Epoch 2898, Loss: 0.3057134449481964, Final Batch Loss: 0.1299065202474594\n",
      "Epoch 2899, Loss: 0.37871287763118744, Final Batch Loss: 0.2115662544965744\n",
      "Epoch 2900, Loss: 0.3107714205980301, Final Batch Loss: 0.15282659232616425\n",
      "Epoch 2901, Loss: 0.33648908138275146, Final Batch Loss: 0.18769694864749908\n",
      "Epoch 2902, Loss: 0.2843411713838577, Final Batch Loss: 0.15424828231334686\n",
      "Epoch 2903, Loss: 0.26680321246385574, Final Batch Loss: 0.11750123649835587\n",
      "Epoch 2904, Loss: 0.3560686409473419, Final Batch Loss: 0.154846653342247\n",
      "Epoch 2905, Loss: 0.3610784411430359, Final Batch Loss: 0.19160987436771393\n",
      "Epoch 2906, Loss: 0.28740857541561127, Final Batch Loss: 0.1597943753004074\n",
      "Epoch 2907, Loss: 0.3221280127763748, Final Batch Loss: 0.1592433750629425\n",
      "Epoch 2908, Loss: 0.33322909474372864, Final Batch Loss: 0.1426263004541397\n",
      "Epoch 2909, Loss: 0.32555587589740753, Final Batch Loss: 0.1818728893995285\n",
      "Epoch 2910, Loss: 0.32041196525096893, Final Batch Loss: 0.15236058831214905\n",
      "Epoch 2911, Loss: 0.27996205538511276, Final Batch Loss: 0.11467873305082321\n",
      "Epoch 2912, Loss: 0.31309716403484344, Final Batch Loss: 0.1614009141921997\n",
      "Epoch 2913, Loss: 0.3576644957065582, Final Batch Loss: 0.16984815895557404\n",
      "Epoch 2914, Loss: 0.32096925377845764, Final Batch Loss: 0.11905643343925476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2915, Loss: 0.3313845843076706, Final Batch Loss: 0.15997765958309174\n",
      "Epoch 2916, Loss: 0.29610975086688995, Final Batch Loss: 0.15600307285785675\n",
      "Epoch 2917, Loss: 0.32369768619537354, Final Batch Loss: 0.1629088819026947\n",
      "Epoch 2918, Loss: 0.3001978397369385, Final Batch Loss: 0.138637512922287\n",
      "Epoch 2919, Loss: 0.3126649707555771, Final Batch Loss: 0.1455240398645401\n",
      "Epoch 2920, Loss: 0.33088718354701996, Final Batch Loss: 0.17456848919391632\n",
      "Epoch 2921, Loss: 0.3130887597799301, Final Batch Loss: 0.1796659231185913\n",
      "Epoch 2922, Loss: 0.3472581058740616, Final Batch Loss: 0.16449323296546936\n",
      "Epoch 2923, Loss: 0.30701838433742523, Final Batch Loss: 0.13711056113243103\n",
      "Epoch 2924, Loss: 0.3206493854522705, Final Batch Loss: 0.145346537232399\n",
      "Epoch 2925, Loss: 0.31297995150089264, Final Batch Loss: 0.19056105613708496\n",
      "Epoch 2926, Loss: 0.2970394194126129, Final Batch Loss: 0.1431431919336319\n",
      "Epoch 2927, Loss: 0.3374471217393875, Final Batch Loss: 0.18519002199172974\n",
      "Epoch 2928, Loss: 0.2748725265264511, Final Batch Loss: 0.14495502412319183\n",
      "Epoch 2929, Loss: 0.33325012028217316, Final Batch Loss: 0.16315259039402008\n",
      "Epoch 2930, Loss: 0.3978213518857956, Final Batch Loss: 0.2313591092824936\n",
      "Epoch 2931, Loss: 0.28742189705371857, Final Batch Loss: 0.15469594299793243\n",
      "Epoch 2932, Loss: 0.2898618131875992, Final Batch Loss: 0.14109201729297638\n",
      "Epoch 2933, Loss: 0.296766996383667, Final Batch Loss: 0.14370806515216827\n",
      "Epoch 2934, Loss: 0.3012007772922516, Final Batch Loss: 0.12632867693901062\n",
      "Epoch 2935, Loss: 0.3175985962152481, Final Batch Loss: 0.15199489891529083\n",
      "Epoch 2936, Loss: 0.31925421953201294, Final Batch Loss: 0.1704699844121933\n",
      "Epoch 2937, Loss: 0.31200340390205383, Final Batch Loss: 0.12928856909275055\n",
      "Epoch 2938, Loss: 0.29382625222206116, Final Batch Loss: 0.1292153149843216\n",
      "Epoch 2939, Loss: 0.2740154415369034, Final Batch Loss: 0.1266053020954132\n",
      "Epoch 2940, Loss: 0.2940293848514557, Final Batch Loss: 0.13530686497688293\n",
      "Epoch 2941, Loss: 0.3166811466217041, Final Batch Loss: 0.1694573611021042\n",
      "Epoch 2942, Loss: 0.30660444498062134, Final Batch Loss: 0.1178092360496521\n",
      "Epoch 2943, Loss: 0.34566162526607513, Final Batch Loss: 0.19427841901779175\n",
      "Epoch 2944, Loss: 0.34345880150794983, Final Batch Loss: 0.17103517055511475\n",
      "Epoch 2945, Loss: 0.36810028553009033, Final Batch Loss: 0.17741578817367554\n",
      "Epoch 2946, Loss: 0.3276805132627487, Final Batch Loss: 0.17820103466510773\n",
      "Epoch 2947, Loss: 0.32153694331645966, Final Batch Loss: 0.14777661859989166\n",
      "Epoch 2948, Loss: 0.32078245282173157, Final Batch Loss: 0.13927870988845825\n",
      "Epoch 2949, Loss: 0.30997171998023987, Final Batch Loss: 0.16509181261062622\n",
      "Epoch 2950, Loss: 0.30930574238300323, Final Batch Loss: 0.1654660850763321\n",
      "Epoch 2951, Loss: 0.32106904685497284, Final Batch Loss: 0.18216443061828613\n",
      "Epoch 2952, Loss: 0.31523339450359344, Final Batch Loss: 0.16507473587989807\n",
      "Epoch 2953, Loss: 0.3077705204486847, Final Batch Loss: 0.12768618762493134\n",
      "Epoch 2954, Loss: 0.33891187608242035, Final Batch Loss: 0.16428424417972565\n",
      "Epoch 2955, Loss: 0.33618035912513733, Final Batch Loss: 0.1597144603729248\n",
      "Epoch 2956, Loss: 0.3302777260541916, Final Batch Loss: 0.13607187569141388\n",
      "Epoch 2957, Loss: 0.32602229714393616, Final Batch Loss: 0.14906682074069977\n",
      "Epoch 2958, Loss: 0.31700557470321655, Final Batch Loss: 0.1609196811914444\n",
      "Epoch 2959, Loss: 0.3146200180053711, Final Batch Loss: 0.1527744084596634\n",
      "Epoch 2960, Loss: 0.2915656864643097, Final Batch Loss: 0.1488995999097824\n",
      "Epoch 2961, Loss: 0.29532676935195923, Final Batch Loss: 0.14475344121456146\n",
      "Epoch 2962, Loss: 0.29781557619571686, Final Batch Loss: 0.16583289206027985\n",
      "Epoch 2963, Loss: 0.26343514770269394, Final Batch Loss: 0.11156799644231796\n",
      "Epoch 2964, Loss: 0.319646954536438, Final Batch Loss: 0.15064343810081482\n",
      "Epoch 2965, Loss: 0.31025494635105133, Final Batch Loss: 0.15473683178424835\n",
      "Epoch 2966, Loss: 0.29341021180152893, Final Batch Loss: 0.13830570876598358\n",
      "Epoch 2967, Loss: 0.3158767521381378, Final Batch Loss: 0.13866087794303894\n",
      "Epoch 2968, Loss: 0.3022979646921158, Final Batch Loss: 0.15623009204864502\n",
      "Epoch 2969, Loss: 0.27317407727241516, Final Batch Loss: 0.14507947862148285\n",
      "Epoch 2970, Loss: 0.2991171032190323, Final Batch Loss: 0.12835142016410828\n",
      "Epoch 2971, Loss: 0.3188484460115433, Final Batch Loss: 0.15686674416065216\n",
      "Epoch 2972, Loss: 0.3112436681985855, Final Batch Loss: 0.13647134602069855\n",
      "Epoch 2973, Loss: 0.2960624396800995, Final Batch Loss: 0.14247210323810577\n",
      "Epoch 2974, Loss: 0.2969997674226761, Final Batch Loss: 0.12582553923130035\n",
      "Epoch 2975, Loss: 0.29167965054512024, Final Batch Loss: 0.13531140983104706\n",
      "Epoch 2976, Loss: 0.3093498945236206, Final Batch Loss: 0.1682540774345398\n",
      "Epoch 2977, Loss: 0.315504789352417, Final Batch Loss: 0.1502458155155182\n",
      "Epoch 2978, Loss: 0.32915881276130676, Final Batch Loss: 0.1558656096458435\n",
      "Epoch 2979, Loss: 0.31983813643455505, Final Batch Loss: 0.17210309207439423\n",
      "Epoch 2980, Loss: 0.3190821409225464, Final Batch Loss: 0.16105639934539795\n",
      "Epoch 2981, Loss: 0.36120790243148804, Final Batch Loss: 0.20204196870326996\n",
      "Epoch 2982, Loss: 0.27303822338581085, Final Batch Loss: 0.1290919929742813\n",
      "Epoch 2983, Loss: 0.27547192573547363, Final Batch Loss: 0.14155377447605133\n",
      "Epoch 2984, Loss: 0.3268749862909317, Final Batch Loss: 0.15831102430820465\n",
      "Epoch 2985, Loss: 0.32634609937667847, Final Batch Loss: 0.13512717187404633\n",
      "Epoch 2986, Loss: 0.3344658762216568, Final Batch Loss: 0.18717294931411743\n",
      "Epoch 2987, Loss: 0.3039008378982544, Final Batch Loss: 0.12763509154319763\n",
      "Epoch 2988, Loss: 0.2888869419693947, Final Batch Loss: 0.1648925393819809\n",
      "Epoch 2989, Loss: 0.3533436208963394, Final Batch Loss: 0.1523195058107376\n",
      "Epoch 2990, Loss: 0.28270043432712555, Final Batch Loss: 0.12280571460723877\n",
      "Epoch 2991, Loss: 0.3323134183883667, Final Batch Loss: 0.14926709234714508\n",
      "Epoch 2992, Loss: 0.3087046295404434, Final Batch Loss: 0.1715804487466812\n",
      "Epoch 2993, Loss: 0.31430333852767944, Final Batch Loss: 0.16177086532115936\n",
      "Epoch 2994, Loss: 0.35719168186187744, Final Batch Loss: 0.22120225429534912\n",
      "Epoch 2995, Loss: 0.3516179323196411, Final Batch Loss: 0.16301532089710236\n",
      "Epoch 2996, Loss: 0.3398006409406662, Final Batch Loss: 0.20651529729366302\n",
      "Epoch 2997, Loss: 0.29875126481056213, Final Batch Loss: 0.15268413722515106\n",
      "Epoch 2998, Loss: 0.3324471712112427, Final Batch Loss: 0.2013482302427292\n",
      "Epoch 2999, Loss: 0.32705123722553253, Final Batch Loss: 0.1725648045539856\n",
      "Epoch 3000, Loss: 0.339341938495636, Final Batch Loss: 0.17761839926242828\n",
      "Epoch 3001, Loss: 0.38245387375354767, Final Batch Loss: 0.14951041340827942\n",
      "Epoch 3002, Loss: 0.3729911595582962, Final Batch Loss: 0.20238249003887177\n",
      "Epoch 3003, Loss: 0.24741621315479279, Final Batch Loss: 0.13663087785243988\n",
      "Epoch 3004, Loss: 0.3855130821466446, Final Batch Loss: 0.20681025087833405\n",
      "Epoch 3005, Loss: 0.27717502415180206, Final Batch Loss: 0.1529654711484909\n",
      "Epoch 3006, Loss: 0.2823352515697479, Final Batch Loss: 0.13995011150836945\n",
      "Epoch 3007, Loss: 0.38417433202266693, Final Batch Loss: 0.1878647655248642\n",
      "Epoch 3008, Loss: 0.3091544806957245, Final Batch Loss: 0.15985052287578583\n",
      "Epoch 3009, Loss: 0.2876901626586914, Final Batch Loss: 0.1349075883626938\n",
      "Epoch 3010, Loss: 0.31491945683956146, Final Batch Loss: 0.15836411714553833\n",
      "Epoch 3011, Loss: 0.2832585722208023, Final Batch Loss: 0.15252946317195892\n",
      "Epoch 3012, Loss: 0.26777663826942444, Final Batch Loss: 0.14004017412662506\n",
      "Epoch 3013, Loss: 0.30872511863708496, Final Batch Loss: 0.1622464507818222\n",
      "Epoch 3014, Loss: 0.36485515534877777, Final Batch Loss: 0.22319817543029785\n",
      "Epoch 3015, Loss: 0.273863710463047, Final Batch Loss: 0.10228795558214188\n",
      "Epoch 3016, Loss: 0.31639276444911957, Final Batch Loss: 0.1517924815416336\n",
      "Epoch 3017, Loss: 0.3170662075281143, Final Batch Loss: 0.1585216224193573\n",
      "Epoch 3018, Loss: 0.2849154621362686, Final Batch Loss: 0.15410417318344116\n",
      "Epoch 3019, Loss: 0.2797018066048622, Final Batch Loss: 0.12027356773614883\n",
      "Epoch 3020, Loss: 0.3232340067625046, Final Batch Loss: 0.14191775023937225\n",
      "Epoch 3021, Loss: 0.3305569887161255, Final Batch Loss: 0.1367374211549759\n",
      "Epoch 3022, Loss: 0.31309548020362854, Final Batch Loss: 0.16325540840625763\n",
      "Epoch 3023, Loss: 0.3559691458940506, Final Batch Loss: 0.16361087560653687\n",
      "Epoch 3024, Loss: 0.3246588706970215, Final Batch Loss: 0.18543587625026703\n",
      "Epoch 3025, Loss: 0.3296307325363159, Final Batch Loss: 0.15757043659687042\n",
      "Epoch 3026, Loss: 0.33630572259426117, Final Batch Loss: 0.15307508409023285\n",
      "Epoch 3027, Loss: 0.32755616307258606, Final Batch Loss: 0.14684689044952393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3028, Loss: 0.32179275155067444, Final Batch Loss: 0.17295581102371216\n",
      "Epoch 3029, Loss: 0.3521862030029297, Final Batch Loss: 0.17620150744915009\n",
      "Epoch 3030, Loss: 0.309136763215065, Final Batch Loss: 0.15501026809215546\n",
      "Epoch 3031, Loss: 0.33895833790302277, Final Batch Loss: 0.16672419011592865\n",
      "Epoch 3032, Loss: 0.28287556767463684, Final Batch Loss: 0.12842707335948944\n",
      "Epoch 3033, Loss: 0.308393657207489, Final Batch Loss: 0.13655999302864075\n",
      "Epoch 3034, Loss: 0.32531751692295074, Final Batch Loss: 0.15868650376796722\n",
      "Epoch 3035, Loss: 0.27676714956760406, Final Batch Loss: 0.13499997556209564\n",
      "Epoch 3036, Loss: 0.3164314329624176, Final Batch Loss: 0.11458638310432434\n",
      "Epoch 3037, Loss: 0.2966001033782959, Final Batch Loss: 0.131438747048378\n",
      "Epoch 3038, Loss: 0.32541485130786896, Final Batch Loss: 0.15802915394306183\n",
      "Epoch 3039, Loss: 0.32238128781318665, Final Batch Loss: 0.17420698702335358\n",
      "Epoch 3040, Loss: 0.3324608653783798, Final Batch Loss: 0.1426907181739807\n",
      "Epoch 3041, Loss: 0.3225265145301819, Final Batch Loss: 0.1585278958082199\n",
      "Epoch 3042, Loss: 0.28121185302734375, Final Batch Loss: 0.15130101144313812\n",
      "Epoch 3043, Loss: 0.2979238033294678, Final Batch Loss: 0.1625341773033142\n",
      "Epoch 3044, Loss: 0.26717906445264816, Final Batch Loss: 0.14702390134334564\n",
      "Epoch 3045, Loss: 0.3155793100595474, Final Batch Loss: 0.15204621851444244\n",
      "Epoch 3046, Loss: 0.2902866154909134, Final Batch Loss: 0.14052914083003998\n",
      "Epoch 3047, Loss: 0.3542717695236206, Final Batch Loss: 0.17038172483444214\n",
      "Epoch 3048, Loss: 0.2873581424355507, Final Batch Loss: 0.12148851901292801\n",
      "Epoch 3049, Loss: 0.3690227121114731, Final Batch Loss: 0.19176878035068512\n",
      "Epoch 3050, Loss: 0.2916291356086731, Final Batch Loss: 0.17358088493347168\n",
      "Epoch 3051, Loss: 0.3410974591970444, Final Batch Loss: 0.16329723596572876\n",
      "Epoch 3052, Loss: 0.30384211242198944, Final Batch Loss: 0.14841939508914948\n",
      "Epoch 3053, Loss: 0.31378714740276337, Final Batch Loss: 0.1886199712753296\n",
      "Epoch 3054, Loss: 0.3074750751256943, Final Batch Loss: 0.1502140313386917\n",
      "Epoch 3055, Loss: 0.30442702770233154, Final Batch Loss: 0.1675395518541336\n",
      "Epoch 3056, Loss: 0.30294887721538544, Final Batch Loss: 0.15890513360500336\n",
      "Epoch 3057, Loss: 0.3287045806646347, Final Batch Loss: 0.1851392239332199\n",
      "Epoch 3058, Loss: 0.27631793916225433, Final Batch Loss: 0.1276218444108963\n",
      "Epoch 3059, Loss: 0.3254033103585243, Final Batch Loss: 0.11898616701364517\n",
      "Epoch 3060, Loss: 0.2953440696001053, Final Batch Loss: 0.14227010309696198\n",
      "Epoch 3061, Loss: 0.3442569822072983, Final Batch Loss: 0.176383376121521\n",
      "Epoch 3062, Loss: 0.298112228512764, Final Batch Loss: 0.16799233853816986\n",
      "Epoch 3063, Loss: 0.30759748816490173, Final Batch Loss: 0.15379425883293152\n",
      "Epoch 3064, Loss: 0.3320311903953552, Final Batch Loss: 0.16498751938343048\n",
      "Epoch 3065, Loss: 0.36451415717601776, Final Batch Loss: 0.21425606310367584\n",
      "Epoch 3066, Loss: 0.3388383835554123, Final Batch Loss: 0.18265335261821747\n",
      "Epoch 3067, Loss: 0.32190902531147003, Final Batch Loss: 0.18378375470638275\n",
      "Epoch 3068, Loss: 0.3656095862388611, Final Batch Loss: 0.22324258089065552\n",
      "Epoch 3069, Loss: 0.3239285945892334, Final Batch Loss: 0.1280391663312912\n",
      "Epoch 3070, Loss: 0.3223830312490463, Final Batch Loss: 0.14631299674510956\n",
      "Epoch 3071, Loss: 0.29931385815143585, Final Batch Loss: 0.19388581812381744\n",
      "Epoch 3072, Loss: 0.3209562450647354, Final Batch Loss: 0.1452111154794693\n",
      "Epoch 3073, Loss: 0.317708283662796, Final Batch Loss: 0.15734989941120148\n",
      "Epoch 3074, Loss: 0.2902955189347267, Final Batch Loss: 0.16543687880039215\n",
      "Epoch 3075, Loss: 0.31819356977939606, Final Batch Loss: 0.1584099382162094\n",
      "Epoch 3076, Loss: 0.28899049758911133, Final Batch Loss: 0.14008384943008423\n",
      "Epoch 3077, Loss: 0.37933680415153503, Final Batch Loss: 0.21595746278762817\n",
      "Epoch 3078, Loss: 0.30799491703510284, Final Batch Loss: 0.17540191113948822\n",
      "Epoch 3079, Loss: 0.2815028727054596, Final Batch Loss: 0.10491624474525452\n",
      "Epoch 3080, Loss: 0.2925712317228317, Final Batch Loss: 0.1342604160308838\n",
      "Epoch 3081, Loss: 0.32089748978614807, Final Batch Loss: 0.18062251806259155\n",
      "Epoch 3082, Loss: 0.266588494181633, Final Batch Loss: 0.12943986058235168\n",
      "Epoch 3083, Loss: 0.2630899026989937, Final Batch Loss: 0.16507242619991302\n",
      "Epoch 3084, Loss: 0.29647037386894226, Final Batch Loss: 0.1420518308877945\n",
      "Epoch 3085, Loss: 0.28758785128593445, Final Batch Loss: 0.14580853283405304\n",
      "Epoch 3086, Loss: 0.2911059111356735, Final Batch Loss: 0.14883272349834442\n",
      "Epoch 3087, Loss: 0.36259083449840546, Final Batch Loss: 0.17146022617816925\n",
      "Epoch 3088, Loss: 0.29146403074264526, Final Batch Loss: 0.1552421897649765\n",
      "Epoch 3089, Loss: 0.28005965054035187, Final Batch Loss: 0.14585605263710022\n",
      "Epoch 3090, Loss: 0.3020107001066208, Final Batch Loss: 0.15125519037246704\n",
      "Epoch 3091, Loss: 0.30348028242588043, Final Batch Loss: 0.15541525185108185\n",
      "Epoch 3092, Loss: 0.3059087544679642, Final Batch Loss: 0.1811361163854599\n",
      "Epoch 3093, Loss: 0.3596654087305069, Final Batch Loss: 0.21327199041843414\n",
      "Epoch 3094, Loss: 0.2797466665506363, Final Batch Loss: 0.12326684594154358\n",
      "Epoch 3095, Loss: 0.3399070054292679, Final Batch Loss: 0.18088960647583008\n",
      "Epoch 3096, Loss: 0.27646809816360474, Final Batch Loss: 0.1272040158510208\n",
      "Epoch 3097, Loss: 0.31872592866420746, Final Batch Loss: 0.17858655750751495\n",
      "Epoch 3098, Loss: 0.3300144523382187, Final Batch Loss: 0.2202250212430954\n",
      "Epoch 3099, Loss: 0.2744699865579605, Final Batch Loss: 0.1298854649066925\n",
      "Epoch 3100, Loss: 0.3341265916824341, Final Batch Loss: 0.14299307763576508\n",
      "Epoch 3101, Loss: 0.3501434624195099, Final Batch Loss: 0.1835973858833313\n",
      "Epoch 3102, Loss: 0.31604360044002533, Final Batch Loss: 0.1458686739206314\n",
      "Epoch 3103, Loss: 0.31500308215618134, Final Batch Loss: 0.17518627643585205\n",
      "Epoch 3104, Loss: 0.3405541479587555, Final Batch Loss: 0.18767817318439484\n",
      "Epoch 3105, Loss: 0.3168099895119667, Final Batch Loss: 0.11650555580854416\n",
      "Epoch 3106, Loss: 0.2727518230676651, Final Batch Loss: 0.13603797554969788\n",
      "Epoch 3107, Loss: 0.27519214898347855, Final Batch Loss: 0.11375891417264938\n",
      "Epoch 3108, Loss: 0.30988165736198425, Final Batch Loss: 0.1492161601781845\n",
      "Epoch 3109, Loss: 0.3104783147573471, Final Batch Loss: 0.14078916609287262\n",
      "Epoch 3110, Loss: 0.31104452908039093, Final Batch Loss: 0.17223195731639862\n",
      "Epoch 3111, Loss: 0.3012845516204834, Final Batch Loss: 0.13407228887081146\n",
      "Epoch 3112, Loss: 0.27858588099479675, Final Batch Loss: 0.14550776779651642\n",
      "Epoch 3113, Loss: 0.3056131601333618, Final Batch Loss: 0.15976746380329132\n",
      "Epoch 3114, Loss: 0.3626125007867813, Final Batch Loss: 0.19029678404331207\n",
      "Epoch 3115, Loss: 0.29287468641996384, Final Batch Loss: 0.17399467527866364\n",
      "Epoch 3116, Loss: 0.2771631181240082, Final Batch Loss: 0.15582919120788574\n",
      "Epoch 3117, Loss: 0.2921459302306175, Final Batch Loss: 0.1246630921959877\n",
      "Epoch 3118, Loss: 0.29564861208200455, Final Batch Loss: 0.12218362838029861\n",
      "Epoch 3119, Loss: 0.3424099236726761, Final Batch Loss: 0.1974068433046341\n",
      "Epoch 3120, Loss: 0.2927491292357445, Final Batch Loss: 0.10781338065862656\n",
      "Epoch 3121, Loss: 0.3033320754766464, Final Batch Loss: 0.15103207528591156\n",
      "Epoch 3122, Loss: 0.35721106827259064, Final Batch Loss: 0.18188875913619995\n",
      "Epoch 3123, Loss: 0.3054487854242325, Final Batch Loss: 0.1383790671825409\n",
      "Epoch 3124, Loss: 0.29106099903583527, Final Batch Loss: 0.13411884009838104\n",
      "Epoch 3125, Loss: 0.2797354757785797, Final Batch Loss: 0.14016731083393097\n",
      "Epoch 3126, Loss: 0.29462815821170807, Final Batch Loss: 0.09770312905311584\n",
      "Epoch 3127, Loss: 0.28592297434806824, Final Batch Loss: 0.1411927491426468\n",
      "Epoch 3128, Loss: 0.3050069212913513, Final Batch Loss: 0.14223577082157135\n",
      "Epoch 3129, Loss: 0.29337547719478607, Final Batch Loss: 0.11713221669197083\n",
      "Epoch 3130, Loss: 0.35210759937763214, Final Batch Loss: 0.20452821254730225\n",
      "Epoch 3131, Loss: 0.3239087164402008, Final Batch Loss: 0.15294818580150604\n",
      "Epoch 3132, Loss: 0.362247034907341, Final Batch Loss: 0.22011727094650269\n",
      "Epoch 3133, Loss: 0.2937760651111603, Final Batch Loss: 0.14985106885433197\n",
      "Epoch 3134, Loss: 0.3146210163831711, Final Batch Loss: 0.15053290128707886\n",
      "Epoch 3135, Loss: 0.3120279461145401, Final Batch Loss: 0.1576838195323944\n",
      "Epoch 3136, Loss: 0.31306980550289154, Final Batch Loss: 0.17363394796848297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3137, Loss: 0.28244027495384216, Final Batch Loss: 0.13938604295253754\n",
      "Epoch 3138, Loss: 0.2831413522362709, Final Batch Loss: 0.10560838133096695\n",
      "Epoch 3139, Loss: 0.2956296503543854, Final Batch Loss: 0.13716699182987213\n",
      "Epoch 3140, Loss: 0.29394039511680603, Final Batch Loss: 0.1458846479654312\n",
      "Epoch 3141, Loss: 0.28005123138427734, Final Batch Loss: 0.14141547679901123\n",
      "Epoch 3142, Loss: 0.27890264987945557, Final Batch Loss: 0.13286465406417847\n",
      "Epoch 3143, Loss: 0.3311043232679367, Final Batch Loss: 0.15746580064296722\n",
      "Epoch 3144, Loss: 0.29007618874311447, Final Batch Loss: 0.11873384565114975\n",
      "Epoch 3145, Loss: 0.2937033176422119, Final Batch Loss: 0.13249589502811432\n",
      "Epoch 3146, Loss: 0.2935985177755356, Final Batch Loss: 0.12657372653484344\n",
      "Epoch 3147, Loss: 0.2844116911292076, Final Batch Loss: 0.11379354447126389\n",
      "Epoch 3148, Loss: 0.2845061644911766, Final Batch Loss: 0.1702728122472763\n",
      "Epoch 3149, Loss: 0.3258969485759735, Final Batch Loss: 0.19300276041030884\n",
      "Epoch 3150, Loss: 0.26899603754282, Final Batch Loss: 0.0867023691534996\n",
      "Epoch 3151, Loss: 0.2992517426609993, Final Batch Loss: 0.11440997570753098\n",
      "Epoch 3152, Loss: 0.3108692169189453, Final Batch Loss: 0.1614961177110672\n",
      "Epoch 3153, Loss: 0.2847154587507248, Final Batch Loss: 0.15114565193653107\n",
      "Epoch 3154, Loss: 0.2972186505794525, Final Batch Loss: 0.15036669373512268\n",
      "Epoch 3155, Loss: 0.32875585556030273, Final Batch Loss: 0.1344754546880722\n",
      "Epoch 3156, Loss: 0.28435347974300385, Final Batch Loss: 0.1357012838125229\n",
      "Epoch 3157, Loss: 0.2819967418909073, Final Batch Loss: 0.11356589198112488\n",
      "Epoch 3158, Loss: 0.26675084233283997, Final Batch Loss: 0.12656670808792114\n",
      "Epoch 3159, Loss: 0.27054059505462646, Final Batch Loss: 0.12986142933368683\n",
      "Epoch 3160, Loss: 0.263475626707077, Final Batch Loss: 0.12742121517658234\n",
      "Epoch 3161, Loss: 0.3000492453575134, Final Batch Loss: 0.1800694316625595\n",
      "Epoch 3162, Loss: 0.29751691222190857, Final Batch Loss: 0.13887323439121246\n",
      "Epoch 3163, Loss: 0.28357329964637756, Final Batch Loss: 0.1399689018726349\n",
      "Epoch 3164, Loss: 0.268334299325943, Final Batch Loss: 0.11218112707138062\n",
      "Epoch 3165, Loss: 0.32236768305301666, Final Batch Loss: 0.1458430141210556\n",
      "Epoch 3166, Loss: 0.25911714136600494, Final Batch Loss: 0.13127030432224274\n",
      "Epoch 3167, Loss: 0.2854243442416191, Final Batch Loss: 0.1161685362458229\n",
      "Epoch 3168, Loss: 0.33035941421985626, Final Batch Loss: 0.21125458180904388\n",
      "Epoch 3169, Loss: 0.2969338297843933, Final Batch Loss: 0.15743933618068695\n",
      "Epoch 3170, Loss: 0.28383125364780426, Final Batch Loss: 0.11869794130325317\n",
      "Epoch 3171, Loss: 0.27315353602170944, Final Batch Loss: 0.11799905449151993\n",
      "Epoch 3172, Loss: 0.3210839629173279, Final Batch Loss: 0.17706076800823212\n",
      "Epoch 3173, Loss: 0.28169889748096466, Final Batch Loss: 0.15336255729198456\n",
      "Epoch 3174, Loss: 0.26585981249809265, Final Batch Loss: 0.1125047504901886\n",
      "Epoch 3175, Loss: 0.3203553408384323, Final Batch Loss: 0.15345652401447296\n",
      "Epoch 3176, Loss: 0.27324916422367096, Final Batch Loss: 0.12013718485832214\n",
      "Epoch 3177, Loss: 0.32545776665210724, Final Batch Loss: 0.17293648421764374\n",
      "Epoch 3178, Loss: 0.3075786530971527, Final Batch Loss: 0.17349137365818024\n",
      "Epoch 3179, Loss: 0.31121926009655, Final Batch Loss: 0.17877835035324097\n",
      "Epoch 3180, Loss: 0.32121364772319794, Final Batch Loss: 0.14129188656806946\n",
      "Epoch 3181, Loss: 0.3378809839487076, Final Batch Loss: 0.20199652016162872\n",
      "Epoch 3182, Loss: 0.33634474873542786, Final Batch Loss: 0.13438831269741058\n",
      "Epoch 3183, Loss: 0.2565658912062645, Final Batch Loss: 0.15083856880664825\n",
      "Epoch 3184, Loss: 0.2786559984087944, Final Batch Loss: 0.09940174967050552\n",
      "Epoch 3185, Loss: 0.2998068779706955, Final Batch Loss: 0.13701866567134857\n",
      "Epoch 3186, Loss: 0.3248632401227951, Final Batch Loss: 0.1551530361175537\n",
      "Epoch 3187, Loss: 0.2778080552816391, Final Batch Loss: 0.15636710822582245\n",
      "Epoch 3188, Loss: 0.27937210351228714, Final Batch Loss: 0.10890471190214157\n",
      "Epoch 3189, Loss: 0.3346215933561325, Final Batch Loss: 0.19315771758556366\n",
      "Epoch 3190, Loss: 0.2492983117699623, Final Batch Loss: 0.12302082031965256\n",
      "Epoch 3191, Loss: 0.2911766916513443, Final Batch Loss: 0.16144458949565887\n",
      "Epoch 3192, Loss: 0.2808336764574051, Final Batch Loss: 0.15818451344966888\n",
      "Epoch 3193, Loss: 0.27947790175676346, Final Batch Loss: 0.11901960521936417\n",
      "Epoch 3194, Loss: 0.2844424694776535, Final Batch Loss: 0.14998231828212738\n",
      "Epoch 3195, Loss: 0.3108258545398712, Final Batch Loss: 0.17318271100521088\n",
      "Epoch 3196, Loss: 0.31765228509902954, Final Batch Loss: 0.1740904599428177\n",
      "Epoch 3197, Loss: 0.3180207163095474, Final Batch Loss: 0.16785095632076263\n",
      "Epoch 3198, Loss: 0.3103411942720413, Final Batch Loss: 0.13851118087768555\n",
      "Epoch 3199, Loss: 0.27650614082813263, Final Batch Loss: 0.12185794115066528\n",
      "Epoch 3200, Loss: 0.32321755588054657, Final Batch Loss: 0.1775197833776474\n",
      "Epoch 3201, Loss: 0.26585307717323303, Final Batch Loss: 0.12498372793197632\n",
      "Epoch 3202, Loss: 0.2738485485315323, Final Batch Loss: 0.12801505625247955\n",
      "Epoch 3203, Loss: 0.3013387471437454, Final Batch Loss: 0.1458911895751953\n",
      "Epoch 3204, Loss: 0.276689276099205, Final Batch Loss: 0.15305432677268982\n",
      "Epoch 3205, Loss: 0.29069678485393524, Final Batch Loss: 0.12981314957141876\n",
      "Epoch 3206, Loss: 0.3365132659673691, Final Batch Loss: 0.15969879925251007\n",
      "Epoch 3207, Loss: 0.2841653525829315, Final Batch Loss: 0.14565549790859222\n",
      "Epoch 3208, Loss: 0.3281199336051941, Final Batch Loss: 0.1679125279188156\n",
      "Epoch 3209, Loss: 0.3222329318523407, Final Batch Loss: 0.13655056059360504\n",
      "Epoch 3210, Loss: 0.2800411731004715, Final Batch Loss: 0.14837290346622467\n",
      "Epoch 3211, Loss: 0.3292473405599594, Final Batch Loss: 0.1640404909849167\n",
      "Epoch 3212, Loss: 0.2923148572444916, Final Batch Loss: 0.14557386934757233\n",
      "Epoch 3213, Loss: 0.3052905350923538, Final Batch Loss: 0.15518617630004883\n",
      "Epoch 3214, Loss: 0.2912694290280342, Final Batch Loss: 0.10990799218416214\n",
      "Epoch 3215, Loss: 0.29655181616544724, Final Batch Loss: 0.17621880769729614\n",
      "Epoch 3216, Loss: 0.3187045007944107, Final Batch Loss: 0.15065345168113708\n",
      "Epoch 3217, Loss: 0.27929621934890747, Final Batch Loss: 0.1264781504869461\n",
      "Epoch 3218, Loss: 0.28428076207637787, Final Batch Loss: 0.15279534459114075\n",
      "Epoch 3219, Loss: 0.2833666056394577, Final Batch Loss: 0.1346753090620041\n",
      "Epoch 3220, Loss: 0.2792023569345474, Final Batch Loss: 0.15291444957256317\n",
      "Epoch 3221, Loss: 0.29910559952259064, Final Batch Loss: 0.14704537391662598\n",
      "Epoch 3222, Loss: 0.2975669503211975, Final Batch Loss: 0.1394156962633133\n",
      "Epoch 3223, Loss: 0.27919090539216995, Final Batch Loss: 0.10408756881952286\n",
      "Epoch 3224, Loss: 0.31072911620140076, Final Batch Loss: 0.17261844873428345\n",
      "Epoch 3225, Loss: 0.3049080967903137, Final Batch Loss: 0.15869127213954926\n",
      "Epoch 3226, Loss: 0.2913389056921005, Final Batch Loss: 0.14158760011196136\n",
      "Epoch 3227, Loss: 0.30738869309425354, Final Batch Loss: 0.1736186146736145\n",
      "Epoch 3228, Loss: 0.2686791867017746, Final Batch Loss: 0.13776610791683197\n",
      "Epoch 3229, Loss: 0.3063775449991226, Final Batch Loss: 0.13873951137065887\n",
      "Epoch 3230, Loss: 0.3093424439430237, Final Batch Loss: 0.1428053081035614\n",
      "Epoch 3231, Loss: 0.3241473436355591, Final Batch Loss: 0.17557363212108612\n",
      "Epoch 3232, Loss: 0.2856777533888817, Final Batch Loss: 0.11820008605718613\n",
      "Epoch 3233, Loss: 0.2770870178937912, Final Batch Loss: 0.14268049597740173\n",
      "Epoch 3234, Loss: 0.3147246241569519, Final Batch Loss: 0.15030980110168457\n",
      "Epoch 3235, Loss: 0.3339290916919708, Final Batch Loss: 0.18815992772579193\n",
      "Epoch 3236, Loss: 0.33503358066082, Final Batch Loss: 0.1599138230085373\n",
      "Epoch 3237, Loss: 0.2613943815231323, Final Batch Loss: 0.13106198608875275\n",
      "Epoch 3238, Loss: 0.315445676445961, Final Batch Loss: 0.14084137976169586\n",
      "Epoch 3239, Loss: 0.3307296931743622, Final Batch Loss: 0.14684198796749115\n",
      "Epoch 3240, Loss: 0.2956530898809433, Final Batch Loss: 0.15776340663433075\n",
      "Epoch 3241, Loss: 0.30094917118549347, Final Batch Loss: 0.1427149474620819\n",
      "Epoch 3242, Loss: 0.30400165915489197, Final Batch Loss: 0.14929449558258057\n",
      "Epoch 3243, Loss: 0.342483252286911, Final Batch Loss: 0.1815280169248581\n",
      "Epoch 3244, Loss: 0.281179741024971, Final Batch Loss: 0.14427761733531952\n",
      "Epoch 3245, Loss: 0.31194375455379486, Final Batch Loss: 0.16089244186878204\n",
      "Epoch 3246, Loss: 0.2965066134929657, Final Batch Loss: 0.16679604351520538\n",
      "Epoch 3247, Loss: 0.2952360659837723, Final Batch Loss: 0.13528983294963837\n",
      "Epoch 3248, Loss: 0.2871673256158829, Final Batch Loss: 0.14580032229423523\n",
      "Epoch 3249, Loss: 0.33066223561763763, Final Batch Loss: 0.19961579144001007\n",
      "Epoch 3250, Loss: 0.29580941796302795, Final Batch Loss: 0.16782043874263763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3251, Loss: 0.2870698347687721, Final Batch Loss: 0.11137480288743973\n",
      "Epoch 3252, Loss: 0.2866099700331688, Final Batch Loss: 0.122514508664608\n",
      "Epoch 3253, Loss: 0.3449889123439789, Final Batch Loss: 0.1524069756269455\n",
      "Epoch 3254, Loss: 0.25457823276519775, Final Batch Loss: 0.13698387145996094\n",
      "Epoch 3255, Loss: 0.26187633723020554, Final Batch Loss: 0.14439207315444946\n",
      "Epoch 3256, Loss: 0.2754194289445877, Final Batch Loss: 0.12637747824192047\n",
      "Epoch 3257, Loss: 0.27064140886068344, Final Batch Loss: 0.11076682060956955\n",
      "Epoch 3258, Loss: 0.27788279950618744, Final Batch Loss: 0.13851173222064972\n",
      "Epoch 3259, Loss: 0.26615964621305466, Final Batch Loss: 0.10999103635549545\n",
      "Epoch 3260, Loss: 0.2839352637529373, Final Batch Loss: 0.13509251177310944\n",
      "Epoch 3261, Loss: 0.2571147680282593, Final Batch Loss: 0.11112052202224731\n",
      "Epoch 3262, Loss: 0.32140178233385086, Final Batch Loss: 0.21037115156650543\n",
      "Epoch 3263, Loss: 0.2926555573940277, Final Batch Loss: 0.13625840842723846\n",
      "Epoch 3264, Loss: 0.3156321048736572, Final Batch Loss: 0.18232811987400055\n",
      "Epoch 3265, Loss: 0.30000700056552887, Final Batch Loss: 0.14356525242328644\n",
      "Epoch 3266, Loss: 0.25536996126174927, Final Batch Loss: 0.12651623785495758\n",
      "Epoch 3267, Loss: 0.3177592307329178, Final Batch Loss: 0.1808861941099167\n",
      "Epoch 3268, Loss: 0.256296306848526, Final Batch Loss: 0.12764273583889008\n",
      "Epoch 3269, Loss: 0.2562266141176224, Final Batch Loss: 0.12561137974262238\n",
      "Epoch 3270, Loss: 0.2759666219353676, Final Batch Loss: 0.10769688338041306\n",
      "Epoch 3271, Loss: 0.2574133798480034, Final Batch Loss: 0.12268619984388351\n",
      "Epoch 3272, Loss: 0.3169289380311966, Final Batch Loss: 0.14457033574581146\n",
      "Epoch 3273, Loss: 0.2647847980260849, Final Batch Loss: 0.11999386548995972\n",
      "Epoch 3274, Loss: 0.2950219660997391, Final Batch Loss: 0.13347677886486053\n",
      "Epoch 3275, Loss: 0.28388065099716187, Final Batch Loss: 0.15303725004196167\n",
      "Epoch 3276, Loss: 0.28671250492334366, Final Batch Loss: 0.16650833189487457\n",
      "Epoch 3277, Loss: 0.2967657744884491, Final Batch Loss: 0.1609431356191635\n",
      "Epoch 3278, Loss: 0.3061295747756958, Final Batch Loss: 0.1412750482559204\n",
      "Epoch 3279, Loss: 0.29163312166929245, Final Batch Loss: 0.12362726777791977\n",
      "Epoch 3280, Loss: 0.28160732984542847, Final Batch Loss: 0.12633122503757477\n",
      "Epoch 3281, Loss: 0.38962438702583313, Final Batch Loss: 0.2109370082616806\n",
      "Epoch 3282, Loss: 0.26127490401268005, Final Batch Loss: 0.12206432223320007\n",
      "Epoch 3283, Loss: 0.32505886256694794, Final Batch Loss: 0.16189010441303253\n",
      "Epoch 3284, Loss: 0.3469081223011017, Final Batch Loss: 0.15638510882854462\n",
      "Epoch 3285, Loss: 0.2689436450600624, Final Batch Loss: 0.14798687398433685\n",
      "Epoch 3286, Loss: 0.2617665231227875, Final Batch Loss: 0.14494028687477112\n",
      "Epoch 3287, Loss: 0.3116843178868294, Final Batch Loss: 0.19772712886333466\n",
      "Epoch 3288, Loss: 0.25691061466932297, Final Batch Loss: 0.155589759349823\n",
      "Epoch 3289, Loss: 0.294158473610878, Final Batch Loss: 0.13093386590480804\n",
      "Epoch 3290, Loss: 0.30552060902118683, Final Batch Loss: 0.16396857798099518\n",
      "Epoch 3291, Loss: 0.28832921385765076, Final Batch Loss: 0.14513127505779266\n",
      "Epoch 3292, Loss: 0.2585211917757988, Final Batch Loss: 0.1346142739057541\n",
      "Epoch 3293, Loss: 0.28205086290836334, Final Batch Loss: 0.16307981312274933\n",
      "Epoch 3294, Loss: 0.3012578636407852, Final Batch Loss: 0.14706410467624664\n",
      "Epoch 3295, Loss: 0.3141487389802933, Final Batch Loss: 0.13352179527282715\n",
      "Epoch 3296, Loss: 0.2803298979997635, Final Batch Loss: 0.10941508412361145\n",
      "Epoch 3297, Loss: 0.31307193636894226, Final Batch Loss: 0.1842387318611145\n",
      "Epoch 3298, Loss: 0.27190279960632324, Final Batch Loss: 0.1397954672574997\n",
      "Epoch 3299, Loss: 0.32899774610996246, Final Batch Loss: 0.18073010444641113\n",
      "Epoch 3300, Loss: 0.3634258806705475, Final Batch Loss: 0.16143576800823212\n",
      "Epoch 3301, Loss: 0.2964533865451813, Final Batch Loss: 0.14050635695457458\n",
      "Epoch 3302, Loss: 0.29200729727745056, Final Batch Loss: 0.1657216101884842\n",
      "Epoch 3303, Loss: 0.2662404924631119, Final Batch Loss: 0.1273866891860962\n",
      "Epoch 3304, Loss: 0.28768062591552734, Final Batch Loss: 0.1464492827653885\n",
      "Epoch 3305, Loss: 0.2957974523305893, Final Batch Loss: 0.17371821403503418\n",
      "Epoch 3306, Loss: 0.2919798940420151, Final Batch Loss: 0.1542612463235855\n",
      "Epoch 3307, Loss: 0.2993655353784561, Final Batch Loss: 0.16752512753009796\n",
      "Epoch 3308, Loss: 0.3054775297641754, Final Batch Loss: 0.14041873812675476\n",
      "Epoch 3309, Loss: 0.29052186012268066, Final Batch Loss: 0.15799672901630402\n",
      "Epoch 3310, Loss: 0.3652806431055069, Final Batch Loss: 0.2139786034822464\n",
      "Epoch 3311, Loss: 0.26752641052007675, Final Batch Loss: 0.11312546581029892\n",
      "Epoch 3312, Loss: 0.28390921652317047, Final Batch Loss: 0.14479005336761475\n",
      "Epoch 3313, Loss: 0.30078330636024475, Final Batch Loss: 0.13680940866470337\n",
      "Epoch 3314, Loss: 0.3337063044309616, Final Batch Loss: 0.13955022394657135\n",
      "Epoch 3315, Loss: 0.2743789702653885, Final Batch Loss: 0.14156357944011688\n",
      "Epoch 3316, Loss: 0.3319745808839798, Final Batch Loss: 0.15708686411380768\n",
      "Epoch 3317, Loss: 0.27396729588508606, Final Batch Loss: 0.14619438350200653\n",
      "Epoch 3318, Loss: 0.26729461550712585, Final Batch Loss: 0.12198013067245483\n",
      "Epoch 3319, Loss: 0.2818092107772827, Final Batch Loss: 0.13995130360126495\n",
      "Epoch 3320, Loss: 0.2768610417842865, Final Batch Loss: 0.12743446230888367\n",
      "Epoch 3321, Loss: 0.3173077702522278, Final Batch Loss: 0.18499810993671417\n",
      "Epoch 3322, Loss: 0.256648451089859, Final Batch Loss: 0.11974850296974182\n",
      "Epoch 3323, Loss: 0.2818651497364044, Final Batch Loss: 0.13945750892162323\n",
      "Epoch 3324, Loss: 0.26577458530664444, Final Batch Loss: 0.07318375259637833\n",
      "Epoch 3325, Loss: 0.2818742021918297, Final Batch Loss: 0.1586763560771942\n",
      "Epoch 3326, Loss: 0.2797822430729866, Final Batch Loss: 0.12199335545301437\n",
      "Epoch 3327, Loss: 0.27794792503118515, Final Batch Loss: 0.15382428467273712\n",
      "Epoch 3328, Loss: 0.30690324306488037, Final Batch Loss: 0.19557999074459076\n",
      "Epoch 3329, Loss: 0.2871057018637657, Final Batch Loss: 0.17616181075572968\n",
      "Epoch 3330, Loss: 0.25032322853803635, Final Batch Loss: 0.10664764791727066\n",
      "Epoch 3331, Loss: 0.2773294523358345, Final Batch Loss: 0.15379442274570465\n",
      "Epoch 3332, Loss: 0.280502125620842, Final Batch Loss: 0.15535405278205872\n",
      "Epoch 3333, Loss: 0.2806418538093567, Final Batch Loss: 0.1498252898454666\n",
      "Epoch 3334, Loss: 0.2570212706923485, Final Batch Loss: 0.11165668815374374\n",
      "Epoch 3335, Loss: 0.2947385236620903, Final Batch Loss: 0.20073874294757843\n",
      "Epoch 3336, Loss: 0.300275057554245, Final Batch Loss: 0.13422180712223053\n",
      "Epoch 3337, Loss: 0.3221631348133087, Final Batch Loss: 0.16520310938358307\n",
      "Epoch 3338, Loss: 0.2711291164159775, Final Batch Loss: 0.13498811423778534\n",
      "Epoch 3339, Loss: 0.25337687879800797, Final Batch Loss: 0.11848340183496475\n",
      "Epoch 3340, Loss: 0.27314795553684235, Final Batch Loss: 0.1387721598148346\n",
      "Epoch 3341, Loss: 0.27560118585824966, Final Batch Loss: 0.12272321432828903\n",
      "Epoch 3342, Loss: 0.3163607716560364, Final Batch Loss: 0.16389597952365875\n",
      "Epoch 3343, Loss: 0.28403933346271515, Final Batch Loss: 0.12469503283500671\n",
      "Epoch 3344, Loss: 0.2761746570467949, Final Batch Loss: 0.16216322779655457\n",
      "Epoch 3345, Loss: 0.25299517065286636, Final Batch Loss: 0.11248482018709183\n",
      "Epoch 3346, Loss: 0.28902778029441833, Final Batch Loss: 0.15468966960906982\n",
      "Epoch 3347, Loss: 0.27624179422855377, Final Batch Loss: 0.1191142201423645\n",
      "Epoch 3348, Loss: 0.26068831235170364, Final Batch Loss: 0.14654874801635742\n",
      "Epoch 3349, Loss: 0.2892392873764038, Final Batch Loss: 0.12137112021446228\n",
      "Epoch 3350, Loss: 0.26194316893815994, Final Batch Loss: 0.12175288051366806\n",
      "Epoch 3351, Loss: 0.32736918330192566, Final Batch Loss: 0.19661056995391846\n",
      "Epoch 3352, Loss: 0.29770827293395996, Final Batch Loss: 0.13760754466056824\n",
      "Epoch 3353, Loss: 0.3021880090236664, Final Batch Loss: 0.15985198318958282\n",
      "Epoch 3354, Loss: 0.30599749088287354, Final Batch Loss: 0.13128302991390228\n",
      "Epoch 3355, Loss: 0.31744182109832764, Final Batch Loss: 0.10604164004325867\n",
      "Epoch 3356, Loss: 0.2685356140136719, Final Batch Loss: 0.14561806619167328\n",
      "Epoch 3357, Loss: 0.29817087948322296, Final Batch Loss: 0.1535535603761673\n",
      "Epoch 3358, Loss: 0.3165215253829956, Final Batch Loss: 0.18932344019412994\n",
      "Epoch 3359, Loss: 0.2774032950401306, Final Batch Loss: 0.12413933873176575\n",
      "Epoch 3360, Loss: 0.2976806312799454, Final Batch Loss: 0.14341819286346436\n",
      "Epoch 3361, Loss: 0.2961512804031372, Final Batch Loss: 0.1599358767271042\n",
      "Epoch 3362, Loss: 0.26870933920145035, Final Batch Loss: 0.1085817739367485\n",
      "Epoch 3363, Loss: 0.30208641290664673, Final Batch Loss: 0.1599561721086502\n",
      "Epoch 3364, Loss: 0.2704012840986252, Final Batch Loss: 0.12670664489269257\n",
      "Epoch 3365, Loss: 0.29733315855264664, Final Batch Loss: 0.18103140592575073\n",
      "Epoch 3366, Loss: 0.26684944331645966, Final Batch Loss: 0.13538293540477753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3367, Loss: 0.3022661954164505, Final Batch Loss: 0.15485012531280518\n",
      "Epoch 3368, Loss: 0.3042592406272888, Final Batch Loss: 0.15979258716106415\n",
      "Epoch 3369, Loss: 0.401798777282238, Final Batch Loss: 0.2830558717250824\n",
      "Epoch 3370, Loss: 0.2701110988855362, Final Batch Loss: 0.14277665317058563\n",
      "Epoch 3371, Loss: 0.26697587221860886, Final Batch Loss: 0.1212669238448143\n",
      "Epoch 3372, Loss: 0.31141045689582825, Final Batch Loss: 0.17639859020709991\n",
      "Epoch 3373, Loss: 0.29017573595046997, Final Batch Loss: 0.15777984261512756\n",
      "Epoch 3374, Loss: 0.2667245790362358, Final Batch Loss: 0.1526820808649063\n",
      "Epoch 3375, Loss: 0.3228711783885956, Final Batch Loss: 0.17692320048809052\n",
      "Epoch 3376, Loss: 0.2623983100056648, Final Batch Loss: 0.10654113441705704\n",
      "Epoch 3377, Loss: 0.27325496077537537, Final Batch Loss: 0.1341155618429184\n",
      "Epoch 3378, Loss: 0.256593219935894, Final Batch Loss: 0.10780664533376694\n",
      "Epoch 3379, Loss: 0.29860324412584305, Final Batch Loss: 0.17364048957824707\n",
      "Epoch 3380, Loss: 0.30547647178173065, Final Batch Loss: 0.1629602462053299\n",
      "Epoch 3381, Loss: 0.29180537164211273, Final Batch Loss: 0.13812829554080963\n",
      "Epoch 3382, Loss: 0.3143162131309509, Final Batch Loss: 0.1715376228094101\n",
      "Epoch 3383, Loss: 0.2992853671312332, Final Batch Loss: 0.15633712708950043\n",
      "Epoch 3384, Loss: 0.3313306123018265, Final Batch Loss: 0.20243459939956665\n",
      "Epoch 3385, Loss: 0.2758362591266632, Final Batch Loss: 0.13656491041183472\n",
      "Epoch 3386, Loss: 0.2544367164373398, Final Batch Loss: 0.113984614610672\n",
      "Epoch 3387, Loss: 0.3081439882516861, Final Batch Loss: 0.12682750821113586\n",
      "Epoch 3388, Loss: 0.2952502369880676, Final Batch Loss: 0.15083833038806915\n",
      "Epoch 3389, Loss: 0.2864527851343155, Final Batch Loss: 0.13734400272369385\n",
      "Epoch 3390, Loss: 0.30502260476350784, Final Batch Loss: 0.12099317461252213\n",
      "Epoch 3391, Loss: 0.3064711093902588, Final Batch Loss: 0.19790799915790558\n",
      "Epoch 3392, Loss: 0.2506834343075752, Final Batch Loss: 0.08243899792432785\n",
      "Epoch 3393, Loss: 0.3086254596710205, Final Batch Loss: 0.16400645673274994\n",
      "Epoch 3394, Loss: 0.2970433533191681, Final Batch Loss: 0.1602155864238739\n",
      "Epoch 3395, Loss: 0.2900969684123993, Final Batch Loss: 0.1773105263710022\n",
      "Epoch 3396, Loss: 0.2860831618309021, Final Batch Loss: 0.13921838998794556\n",
      "Epoch 3397, Loss: 0.2730485424399376, Final Batch Loss: 0.10180860012769699\n",
      "Epoch 3398, Loss: 0.30714964866638184, Final Batch Loss: 0.15444998443126678\n",
      "Epoch 3399, Loss: 0.3222578167915344, Final Batch Loss: 0.1500513106584549\n",
      "Epoch 3400, Loss: 0.2730919197201729, Final Batch Loss: 0.11019087582826614\n",
      "Epoch 3401, Loss: 0.2765640914440155, Final Batch Loss: 0.14449898898601532\n",
      "Epoch 3402, Loss: 0.28272152692079544, Final Batch Loss: 0.16011351346969604\n",
      "Epoch 3403, Loss: 0.3089883625507355, Final Batch Loss: 0.17008252441883087\n",
      "Epoch 3404, Loss: 0.25929751992225647, Final Batch Loss: 0.09985813498497009\n",
      "Epoch 3405, Loss: 0.2593017816543579, Final Batch Loss: 0.1311509758234024\n",
      "Epoch 3406, Loss: 0.24009249359369278, Final Batch Loss: 0.11587736010551453\n",
      "Epoch 3407, Loss: 0.29569992423057556, Final Batch Loss: 0.1364375650882721\n",
      "Epoch 3408, Loss: 0.29083912819623947, Final Batch Loss: 0.12295081466436386\n",
      "Epoch 3409, Loss: 0.2694193869829178, Final Batch Loss: 0.14233416318893433\n",
      "Epoch 3410, Loss: 0.28502678871154785, Final Batch Loss: 0.13802705705165863\n",
      "Epoch 3411, Loss: 0.2789599150419235, Final Batch Loss: 0.12737254798412323\n",
      "Epoch 3412, Loss: 0.25550421327352524, Final Batch Loss: 0.13690003752708435\n",
      "Epoch 3413, Loss: 0.24539689719676971, Final Batch Loss: 0.13698716461658478\n",
      "Epoch 3414, Loss: 0.2503493055701256, Final Batch Loss: 0.12885254621505737\n",
      "Epoch 3415, Loss: 0.29443806409835815, Final Batch Loss: 0.13506245613098145\n",
      "Epoch 3416, Loss: 0.2566974461078644, Final Batch Loss: 0.11141926050186157\n",
      "Epoch 3417, Loss: 0.2694692984223366, Final Batch Loss: 0.14939077198505402\n",
      "Epoch 3418, Loss: 0.2814030200242996, Final Batch Loss: 0.1491716206073761\n",
      "Epoch 3419, Loss: 0.25876880437135696, Final Batch Loss: 0.12062311917543411\n",
      "Epoch 3420, Loss: 0.2882070541381836, Final Batch Loss: 0.152922123670578\n",
      "Epoch 3421, Loss: 0.27550700306892395, Final Batch Loss: 0.1328710913658142\n",
      "Epoch 3422, Loss: 0.2864536643028259, Final Batch Loss: 0.15566052496433258\n",
      "Epoch 3423, Loss: 0.2645373195409775, Final Batch Loss: 0.12909704446792603\n",
      "Epoch 3424, Loss: 0.2933447062969208, Final Batch Loss: 0.14759956300258636\n",
      "Epoch 3425, Loss: 0.28237318992614746, Final Batch Loss: 0.15050320327281952\n",
      "Epoch 3426, Loss: 0.24293390661478043, Final Batch Loss: 0.11874508112668991\n",
      "Epoch 3427, Loss: 0.3438956141471863, Final Batch Loss: 0.20056354999542236\n",
      "Epoch 3428, Loss: 0.3045271337032318, Final Batch Loss: 0.15146660804748535\n",
      "Epoch 3429, Loss: 0.2738274782896042, Final Batch Loss: 0.13335151970386505\n",
      "Epoch 3430, Loss: 0.2915235161781311, Final Batch Loss: 0.13871906697750092\n",
      "Epoch 3431, Loss: 0.34881508350372314, Final Batch Loss: 0.13195131719112396\n",
      "Epoch 3432, Loss: 0.2865114063024521, Final Batch Loss: 0.13713228702545166\n",
      "Epoch 3433, Loss: 0.25870902836322784, Final Batch Loss: 0.11768454313278198\n",
      "Epoch 3434, Loss: 0.22876818478107452, Final Batch Loss: 0.11771457642316818\n",
      "Epoch 3435, Loss: 0.3272397518157959, Final Batch Loss: 0.17360901832580566\n",
      "Epoch 3436, Loss: 0.27470414340496063, Final Batch Loss: 0.14676544070243835\n",
      "Epoch 3437, Loss: 0.2837919518351555, Final Batch Loss: 0.1647435873746872\n",
      "Epoch 3438, Loss: 0.282360702753067, Final Batch Loss: 0.14749640226364136\n",
      "Epoch 3439, Loss: 0.26813817024230957, Final Batch Loss: 0.13328367471694946\n",
      "Epoch 3440, Loss: 0.3063511252403259, Final Batch Loss: 0.16113491356372833\n",
      "Epoch 3441, Loss: 0.27065376937389374, Final Batch Loss: 0.13878342509269714\n",
      "Epoch 3442, Loss: 0.2895376682281494, Final Batch Loss: 0.146994948387146\n",
      "Epoch 3443, Loss: 0.29152320325374603, Final Batch Loss: 0.15500038862228394\n",
      "Epoch 3444, Loss: 0.2536422535777092, Final Batch Loss: 0.11658526211977005\n",
      "Epoch 3445, Loss: 0.2966822534799576, Final Batch Loss: 0.1253429651260376\n",
      "Epoch 3446, Loss: 0.26318904757499695, Final Batch Loss: 0.09139007329940796\n",
      "Epoch 3447, Loss: 0.3177385926246643, Final Batch Loss: 0.18900851905345917\n",
      "Epoch 3448, Loss: 0.31173375248908997, Final Batch Loss: 0.14703403413295746\n",
      "Epoch 3449, Loss: 0.292350634932518, Final Batch Loss: 0.15250736474990845\n",
      "Epoch 3450, Loss: 0.33943621814250946, Final Batch Loss: 0.17902745306491852\n",
      "Epoch 3451, Loss: 0.2562645524740219, Final Batch Loss: 0.14191634953022003\n",
      "Epoch 3452, Loss: 0.29660576581954956, Final Batch Loss: 0.16044388711452484\n",
      "Epoch 3453, Loss: 0.3068501055240631, Final Batch Loss: 0.20211555063724518\n",
      "Epoch 3454, Loss: 0.2754744291305542, Final Batch Loss: 0.14996521174907684\n",
      "Epoch 3455, Loss: 0.27771399915218353, Final Batch Loss: 0.11038807034492493\n",
      "Epoch 3456, Loss: 0.25570351630449295, Final Batch Loss: 0.12288147956132889\n",
      "Epoch 3457, Loss: 0.28757090866565704, Final Batch Loss: 0.1385335773229599\n",
      "Epoch 3458, Loss: 0.2621450796723366, Final Batch Loss: 0.10650021582841873\n",
      "Epoch 3459, Loss: 0.36298617720603943, Final Batch Loss: 0.2176615446805954\n",
      "Epoch 3460, Loss: 0.26276660710573196, Final Batch Loss: 0.13995206356048584\n",
      "Epoch 3461, Loss: 0.27178217470645905, Final Batch Loss: 0.14452792704105377\n",
      "Epoch 3462, Loss: 0.25180982053279877, Final Batch Loss: 0.10891026258468628\n",
      "Epoch 3463, Loss: 0.2874523550271988, Final Batch Loss: 0.1445521116256714\n",
      "Epoch 3464, Loss: 0.31031864881515503, Final Batch Loss: 0.14449824392795563\n",
      "Epoch 3465, Loss: 0.29917286336421967, Final Batch Loss: 0.13404180109500885\n",
      "Epoch 3466, Loss: 0.26433412730693817, Final Batch Loss: 0.13115526735782623\n",
      "Epoch 3467, Loss: 0.2954583913087845, Final Batch Loss: 0.1716442108154297\n",
      "Epoch 3468, Loss: 0.27836254239082336, Final Batch Loss: 0.1545838862657547\n",
      "Epoch 3469, Loss: 0.30106091499328613, Final Batch Loss: 0.15476037561893463\n",
      "Epoch 3470, Loss: 0.2985284626483917, Final Batch Loss: 0.1461668759584427\n",
      "Epoch 3471, Loss: 0.2873644530773163, Final Batch Loss: 0.15413469076156616\n",
      "Epoch 3472, Loss: 0.3007981479167938, Final Batch Loss: 0.1605931967496872\n",
      "Epoch 3473, Loss: 0.316911906003952, Final Batch Loss: 0.15146656334400177\n",
      "Epoch 3474, Loss: 0.2537047415971756, Final Batch Loss: 0.12469759583473206\n",
      "Epoch 3475, Loss: 0.26963039487600327, Final Batch Loss: 0.12377577275037766\n",
      "Epoch 3476, Loss: 0.26372165977954865, Final Batch Loss: 0.1494612842798233\n",
      "Epoch 3477, Loss: 0.2533564865589142, Final Batch Loss: 0.13366807997226715\n",
      "Epoch 3478, Loss: 0.3043240159749985, Final Batch Loss: 0.16571716964244843\n",
      "Epoch 3479, Loss: 0.3097253739833832, Final Batch Loss: 0.14126761257648468\n",
      "Epoch 3480, Loss: 0.3153534382581711, Final Batch Loss: 0.12806694209575653\n",
      "Epoch 3481, Loss: 0.3025420978665352, Final Batch Loss: 0.11410749703645706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3482, Loss: 0.2815604656934738, Final Batch Loss: 0.12666283547878265\n",
      "Epoch 3483, Loss: 0.3384929448366165, Final Batch Loss: 0.18897223472595215\n",
      "Epoch 3484, Loss: 0.2833649665117264, Final Batch Loss: 0.1322764903306961\n",
      "Epoch 3485, Loss: 0.2744896039366722, Final Batch Loss: 0.1497306525707245\n",
      "Epoch 3486, Loss: 0.2905276119709015, Final Batch Loss: 0.13578523695468903\n",
      "Epoch 3487, Loss: 0.31473827362060547, Final Batch Loss: 0.15824632346630096\n",
      "Epoch 3488, Loss: 0.26198286563158035, Final Batch Loss: 0.12423629313707352\n",
      "Epoch 3489, Loss: 0.2693581283092499, Final Batch Loss: 0.10982048511505127\n",
      "Epoch 3490, Loss: 0.2798966318368912, Final Batch Loss: 0.15720941126346588\n",
      "Epoch 3491, Loss: 0.26607565581798553, Final Batch Loss: 0.1271822303533554\n",
      "Epoch 3492, Loss: 0.2393576055765152, Final Batch Loss: 0.11700718849897385\n",
      "Epoch 3493, Loss: 0.2473275288939476, Final Batch Loss: 0.13630284368991852\n",
      "Epoch 3494, Loss: 0.26260819286108017, Final Batch Loss: 0.1427372246980667\n",
      "Epoch 3495, Loss: 0.3032834157347679, Final Batch Loss: 0.17931832373142242\n",
      "Epoch 3496, Loss: 0.2794947624206543, Final Batch Loss: 0.13586267828941345\n",
      "Epoch 3497, Loss: 0.2739779055118561, Final Batch Loss: 0.12277790904045105\n",
      "Epoch 3498, Loss: 0.3039353936910629, Final Batch Loss: 0.13909168541431427\n",
      "Epoch 3499, Loss: 0.32114647328853607, Final Batch Loss: 0.15641118586063385\n",
      "Epoch 3500, Loss: 0.27608922123908997, Final Batch Loss: 0.1338343769311905\n",
      "Epoch 3501, Loss: 0.27329248934984207, Final Batch Loss: 0.15743859112262726\n",
      "Epoch 3502, Loss: 0.27480629086494446, Final Batch Loss: 0.13845430314540863\n",
      "Epoch 3503, Loss: 0.2562391981482506, Final Batch Loss: 0.1144595667719841\n",
      "Epoch 3504, Loss: 0.2851191908121109, Final Batch Loss: 0.1564449816942215\n",
      "Epoch 3505, Loss: 0.25258155912160873, Final Batch Loss: 0.15427495539188385\n",
      "Epoch 3506, Loss: 0.2533024773001671, Final Batch Loss: 0.12215612083673477\n",
      "Epoch 3507, Loss: 0.2652676999568939, Final Batch Loss: 0.13957975804805756\n",
      "Epoch 3508, Loss: 0.25687727332115173, Final Batch Loss: 0.12682412564754486\n",
      "Epoch 3509, Loss: 0.27732108533382416, Final Batch Loss: 0.1157664954662323\n",
      "Epoch 3510, Loss: 0.32015152275562286, Final Batch Loss: 0.1369626820087433\n",
      "Epoch 3511, Loss: 0.2413390353322029, Final Batch Loss: 0.09396874159574509\n",
      "Epoch 3512, Loss: 0.3095320165157318, Final Batch Loss: 0.13126185536384583\n",
      "Epoch 3513, Loss: 0.29687224328517914, Final Batch Loss: 0.16142407059669495\n",
      "Epoch 3514, Loss: 0.35615334659814835, Final Batch Loss: 0.11552294343709946\n",
      "Epoch 3515, Loss: 0.29115206003189087, Final Batch Loss: 0.13853134214878082\n",
      "Epoch 3516, Loss: 0.28091228753328323, Final Batch Loss: 0.1590069979429245\n",
      "Epoch 3517, Loss: 0.2299395576119423, Final Batch Loss: 0.09756409376859665\n",
      "Epoch 3518, Loss: 0.2907221019268036, Final Batch Loss: 0.16564132273197174\n",
      "Epoch 3519, Loss: 0.2592493072152138, Final Batch Loss: 0.13579027354717255\n",
      "Epoch 3520, Loss: 0.2794181630015373, Final Batch Loss: 0.1220044270157814\n",
      "Epoch 3521, Loss: 0.22752302885055542, Final Batch Loss: 0.10758566856384277\n",
      "Epoch 3522, Loss: 0.24148159474134445, Final Batch Loss: 0.1106090322136879\n",
      "Epoch 3523, Loss: 0.3062237799167633, Final Batch Loss: 0.15939922630786896\n",
      "Epoch 3524, Loss: 0.2870115712285042, Final Batch Loss: 0.11930275708436966\n",
      "Epoch 3525, Loss: 0.2761630639433861, Final Batch Loss: 0.17189748585224152\n",
      "Epoch 3526, Loss: 0.3062983453273773, Final Batch Loss: 0.1663232147693634\n",
      "Epoch 3527, Loss: 0.27363890409469604, Final Batch Loss: 0.12599676847457886\n",
      "Epoch 3528, Loss: 0.2931758016347885, Final Batch Loss: 0.1433047354221344\n",
      "Epoch 3529, Loss: 0.2389269843697548, Final Batch Loss: 0.11831241101026535\n",
      "Epoch 3530, Loss: 0.29628391563892365, Final Batch Loss: 0.13773185014724731\n",
      "Epoch 3531, Loss: 0.26764845848083496, Final Batch Loss: 0.1298871785402298\n",
      "Epoch 3532, Loss: 0.2688961923122406, Final Batch Loss: 0.14533202350139618\n",
      "Epoch 3533, Loss: 0.27607865631580353, Final Batch Loss: 0.1546637862920761\n",
      "Epoch 3534, Loss: 0.27669209241867065, Final Batch Loss: 0.12527909874916077\n",
      "Epoch 3535, Loss: 0.2558084800839424, Final Batch Loss: 0.11060354858636856\n",
      "Epoch 3536, Loss: 0.29718221724033356, Final Batch Loss: 0.153761625289917\n",
      "Epoch 3537, Loss: 0.2802074924111366, Final Batch Loss: 0.11497614532709122\n",
      "Epoch 3538, Loss: 0.26611967384815216, Final Batch Loss: 0.12534405291080475\n",
      "Epoch 3539, Loss: 0.27441729605197906, Final Batch Loss: 0.12602899968624115\n",
      "Epoch 3540, Loss: 0.3161853849887848, Final Batch Loss: 0.19219349324703217\n",
      "Epoch 3541, Loss: 0.2609224393963814, Final Batch Loss: 0.10633819550275803\n",
      "Epoch 3542, Loss: 0.2651580646634102, Final Batch Loss: 0.11909931153059006\n",
      "Epoch 3543, Loss: 0.3274385929107666, Final Batch Loss: 0.17207515239715576\n",
      "Epoch 3544, Loss: 0.2738165110349655, Final Batch Loss: 0.12786529958248138\n",
      "Epoch 3545, Loss: 0.23869743198156357, Final Batch Loss: 0.10521135479211807\n",
      "Epoch 3546, Loss: 0.27515144646167755, Final Batch Loss: 0.14921335875988007\n",
      "Epoch 3547, Loss: 0.28349028527736664, Final Batch Loss: 0.14149507880210876\n",
      "Epoch 3548, Loss: 0.3422800153493881, Final Batch Loss: 0.1859464794397354\n",
      "Epoch 3549, Loss: 0.3186526447534561, Final Batch Loss: 0.19314050674438477\n",
      "Epoch 3550, Loss: 0.3001296669244766, Final Batch Loss: 0.16224488615989685\n",
      "Epoch 3551, Loss: 0.2689225524663925, Final Batch Loss: 0.12644018232822418\n",
      "Epoch 3552, Loss: 0.2723088413476944, Final Batch Loss: 0.1376008838415146\n",
      "Epoch 3553, Loss: 0.314168781042099, Final Batch Loss: 0.17350919544696808\n",
      "Epoch 3554, Loss: 0.28986866772174835, Final Batch Loss: 0.16452257335186005\n",
      "Epoch 3555, Loss: 0.30279454588890076, Final Batch Loss: 0.16415293514728546\n",
      "Epoch 3556, Loss: 0.39665547013282776, Final Batch Loss: 0.2615499198436737\n",
      "Epoch 3557, Loss: 0.2642037644982338, Final Batch Loss: 0.11938897520303726\n",
      "Epoch 3558, Loss: 0.28722409904003143, Final Batch Loss: 0.1412631720304489\n",
      "Epoch 3559, Loss: 0.2340168133378029, Final Batch Loss: 0.10607496649026871\n",
      "Epoch 3560, Loss: 0.2616136893630028, Final Batch Loss: 0.12489297240972519\n",
      "Epoch 3561, Loss: 0.29752276092767715, Final Batch Loss: 0.1811022311449051\n",
      "Epoch 3562, Loss: 0.2687896713614464, Final Batch Loss: 0.11225626617670059\n",
      "Epoch 3563, Loss: 0.27837005257606506, Final Batch Loss: 0.14511190354824066\n",
      "Epoch 3564, Loss: 0.30842117965221405, Final Batch Loss: 0.16371691226959229\n",
      "Epoch 3565, Loss: 0.2352140173316002, Final Batch Loss: 0.08508642762899399\n",
      "Epoch 3566, Loss: 0.28427180647850037, Final Batch Loss: 0.13667158782482147\n",
      "Epoch 3567, Loss: 0.2547728195786476, Final Batch Loss: 0.11813836544752121\n",
      "Epoch 3568, Loss: 0.2759951278567314, Final Batch Loss: 0.15383462607860565\n",
      "Epoch 3569, Loss: 0.2934473007917404, Final Batch Loss: 0.14862696826457977\n",
      "Epoch 3570, Loss: 0.24390453100204468, Final Batch Loss: 0.12917663156986237\n",
      "Epoch 3571, Loss: 0.28161539137363434, Final Batch Loss: 0.14410711824893951\n",
      "Epoch 3572, Loss: 0.23120486736297607, Final Batch Loss: 0.12428924441337585\n",
      "Epoch 3573, Loss: 0.2665954977273941, Final Batch Loss: 0.13234704732894897\n",
      "Epoch 3574, Loss: 0.274468258023262, Final Batch Loss: 0.1489955484867096\n",
      "Epoch 3575, Loss: 0.2654365003108978, Final Batch Loss: 0.16446764767169952\n",
      "Epoch 3576, Loss: 0.290363110601902, Final Batch Loss: 0.11727582663297653\n",
      "Epoch 3577, Loss: 0.2736142873764038, Final Batch Loss: 0.13145525753498077\n",
      "Epoch 3578, Loss: 0.29450857639312744, Final Batch Loss: 0.1512448489665985\n",
      "Epoch 3579, Loss: 0.26570840179920197, Final Batch Loss: 0.1220964789390564\n",
      "Epoch 3580, Loss: 0.23930878192186356, Final Batch Loss: 0.10048123449087143\n",
      "Epoch 3581, Loss: 0.2803480327129364, Final Batch Loss: 0.11205136775970459\n",
      "Epoch 3582, Loss: 0.2932298332452774, Final Batch Loss: 0.14367985725402832\n",
      "Epoch 3583, Loss: 0.2588234841823578, Final Batch Loss: 0.1277574598789215\n",
      "Epoch 3584, Loss: 0.2748781889677048, Final Batch Loss: 0.1284427046775818\n",
      "Epoch 3585, Loss: 0.2866150736808777, Final Batch Loss: 0.13778357207775116\n",
      "Epoch 3586, Loss: 0.2661050260066986, Final Batch Loss: 0.15631015598773956\n",
      "Epoch 3587, Loss: 0.2985712066292763, Final Batch Loss: 0.17466063797473907\n",
      "Epoch 3588, Loss: 0.2974366471171379, Final Batch Loss: 0.1131935641169548\n",
      "Epoch 3589, Loss: 0.24215780943632126, Final Batch Loss: 0.10859612375497818\n",
      "Epoch 3590, Loss: 0.2365216389298439, Final Batch Loss: 0.10111447423696518\n",
      "Epoch 3591, Loss: 0.29655829071998596, Final Batch Loss: 0.17858342826366425\n",
      "Epoch 3592, Loss: 0.27687065303325653, Final Batch Loss: 0.14805087447166443\n",
      "Epoch 3593, Loss: 0.29774339497089386, Final Batch Loss: 0.1583174467086792\n",
      "Epoch 3594, Loss: 0.2501770108938217, Final Batch Loss: 0.1213151216506958\n",
      "Epoch 3595, Loss: 0.30345120280981064, Final Batch Loss: 0.10153745859861374\n",
      "Epoch 3596, Loss: 0.26684246957302094, Final Batch Loss: 0.13263216614723206\n",
      "Epoch 3597, Loss: 0.2785933166742325, Final Batch Loss: 0.12907929718494415\n",
      "Epoch 3598, Loss: 0.26377443969249725, Final Batch Loss: 0.1431448608636856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3599, Loss: 0.27360131591558456, Final Batch Loss: 0.12193215638399124\n",
      "Epoch 3600, Loss: 0.3461023271083832, Final Batch Loss: 0.246420755982399\n",
      "Epoch 3601, Loss: 0.29721997678279877, Final Batch Loss: 0.15049533545970917\n",
      "Epoch 3602, Loss: 0.2417410984635353, Final Batch Loss: 0.10970177501440048\n",
      "Epoch 3603, Loss: 0.2866412103176117, Final Batch Loss: 0.141031876206398\n",
      "Epoch 3604, Loss: 0.3180353343486786, Final Batch Loss: 0.17015506327152252\n",
      "Epoch 3605, Loss: 0.30120378732681274, Final Batch Loss: 0.16191130876541138\n",
      "Epoch 3606, Loss: 0.2569112107157707, Final Batch Loss: 0.1350296586751938\n",
      "Epoch 3607, Loss: 0.2780598998069763, Final Batch Loss: 0.11023882031440735\n",
      "Epoch 3608, Loss: 0.2934502810239792, Final Batch Loss: 0.14552922546863556\n",
      "Epoch 3609, Loss: 0.2753642722964287, Final Batch Loss: 0.10502687841653824\n",
      "Epoch 3610, Loss: 0.2849379703402519, Final Batch Loss: 0.11871569603681564\n",
      "Epoch 3611, Loss: 0.2823350578546524, Final Batch Loss: 0.16184605658054352\n",
      "Epoch 3612, Loss: 0.25616641342639923, Final Batch Loss: 0.1342729777097702\n",
      "Epoch 3613, Loss: 0.252031534910202, Final Batch Loss: 0.12181022763252258\n",
      "Epoch 3614, Loss: 0.26005758345127106, Final Batch Loss: 0.10993441939353943\n",
      "Epoch 3615, Loss: 0.3131297826766968, Final Batch Loss: 0.1663161963224411\n",
      "Epoch 3616, Loss: 0.23294337838888168, Final Batch Loss: 0.09599704295396805\n",
      "Epoch 3617, Loss: 0.2680443674325943, Final Batch Loss: 0.10688838362693787\n",
      "Epoch 3618, Loss: 0.25054971128702164, Final Batch Loss: 0.12729313969612122\n",
      "Epoch 3619, Loss: 0.25675423443317413, Final Batch Loss: 0.12843774259090424\n",
      "Epoch 3620, Loss: 0.27980948984622955, Final Batch Loss: 0.14003057777881622\n",
      "Epoch 3621, Loss: 0.25787022709846497, Final Batch Loss: 0.13141687214374542\n",
      "Epoch 3622, Loss: 0.2515748143196106, Final Batch Loss: 0.13541583716869354\n",
      "Epoch 3623, Loss: 0.2788643538951874, Final Batch Loss: 0.1418583244085312\n",
      "Epoch 3624, Loss: 0.2974748983979225, Final Batch Loss: 0.17459414899349213\n",
      "Epoch 3625, Loss: 0.21850593388080597, Final Batch Loss: 0.10975264757871628\n",
      "Epoch 3626, Loss: 0.27154961228370667, Final Batch Loss: 0.14437171816825867\n",
      "Epoch 3627, Loss: 0.32054631412029266, Final Batch Loss: 0.16022638976573944\n",
      "Epoch 3628, Loss: 0.27634963393211365, Final Batch Loss: 0.12536750733852386\n",
      "Epoch 3629, Loss: 0.27522534877061844, Final Batch Loss: 0.12377830594778061\n",
      "Epoch 3630, Loss: 0.3145534098148346, Final Batch Loss: 0.1461913287639618\n",
      "Epoch 3631, Loss: 0.2728542909026146, Final Batch Loss: 0.09404788166284561\n",
      "Epoch 3632, Loss: 0.23970114439725876, Final Batch Loss: 0.12461644411087036\n",
      "Epoch 3633, Loss: 0.23202983289957047, Final Batch Loss: 0.1264813244342804\n",
      "Epoch 3634, Loss: 0.2589176297187805, Final Batch Loss: 0.14859607815742493\n",
      "Epoch 3635, Loss: 0.26674118638038635, Final Batch Loss: 0.13092215359210968\n",
      "Epoch 3636, Loss: 0.29741303622722626, Final Batch Loss: 0.12610329687595367\n",
      "Epoch 3637, Loss: 0.28479327261447906, Final Batch Loss: 0.1580023616552353\n",
      "Epoch 3638, Loss: 0.29507455229759216, Final Batch Loss: 0.1681693196296692\n",
      "Epoch 3639, Loss: 0.29203981161117554, Final Batch Loss: 0.16597852110862732\n",
      "Epoch 3640, Loss: 0.29347512125968933, Final Batch Loss: 0.15111775696277618\n",
      "Epoch 3641, Loss: 0.31759460270404816, Final Batch Loss: 0.142978236079216\n",
      "Epoch 3642, Loss: 0.28097542375326157, Final Batch Loss: 0.16135139763355255\n",
      "Epoch 3643, Loss: 0.2732197567820549, Final Batch Loss: 0.16558700799942017\n",
      "Epoch 3644, Loss: 0.23622341454029083, Final Batch Loss: 0.08805593848228455\n",
      "Epoch 3645, Loss: 0.2374742403626442, Final Batch Loss: 0.12464632838964462\n",
      "Epoch 3646, Loss: 0.3144456446170807, Final Batch Loss: 0.17599014937877655\n",
      "Epoch 3647, Loss: 0.2860577702522278, Final Batch Loss: 0.14695711433887482\n",
      "Epoch 3648, Loss: 0.283223032951355, Final Batch Loss: 0.1282152533531189\n",
      "Epoch 3649, Loss: 0.3134652376174927, Final Batch Loss: 0.19608674943447113\n",
      "Epoch 3650, Loss: 0.2679401859641075, Final Batch Loss: 0.1463208645582199\n",
      "Epoch 3651, Loss: 0.2549428790807724, Final Batch Loss: 0.13566036522388458\n",
      "Epoch 3652, Loss: 0.2682753652334213, Final Batch Loss: 0.13271863758563995\n",
      "Epoch 3653, Loss: 0.29394349455833435, Final Batch Loss: 0.1440216451883316\n",
      "Epoch 3654, Loss: 0.2437223121523857, Final Batch Loss: 0.11046385020017624\n",
      "Epoch 3655, Loss: 0.26782529056072235, Final Batch Loss: 0.13840393722057343\n",
      "Epoch 3656, Loss: 0.238080233335495, Final Batch Loss: 0.1273011863231659\n",
      "Epoch 3657, Loss: 0.33074864745140076, Final Batch Loss: 0.19010525941848755\n",
      "Epoch 3658, Loss: 0.2807208448648453, Final Batch Loss: 0.12805022299289703\n",
      "Epoch 3659, Loss: 0.26628731191158295, Final Batch Loss: 0.12941955029964447\n",
      "Epoch 3660, Loss: 0.25964317470788956, Final Batch Loss: 0.14197932183742523\n",
      "Epoch 3661, Loss: 0.250115729868412, Final Batch Loss: 0.11437197774648666\n",
      "Epoch 3662, Loss: 0.2805255353450775, Final Batch Loss: 0.1315630078315735\n",
      "Epoch 3663, Loss: 0.26412244141101837, Final Batch Loss: 0.13101352751255035\n",
      "Epoch 3664, Loss: 0.2651682272553444, Final Batch Loss: 0.10618845373392105\n",
      "Epoch 3665, Loss: 0.2757887691259384, Final Batch Loss: 0.1496564745903015\n",
      "Epoch 3666, Loss: 0.2365335002541542, Final Batch Loss: 0.11145045608282089\n",
      "Epoch 3667, Loss: 0.2689083218574524, Final Batch Loss: 0.13217175006866455\n",
      "Epoch 3668, Loss: 0.24118537455797195, Final Batch Loss: 0.10102135688066483\n",
      "Epoch 3669, Loss: 0.280802384018898, Final Batch Loss: 0.14532184600830078\n",
      "Epoch 3670, Loss: 0.28209786117076874, Final Batch Loss: 0.10920283198356628\n",
      "Epoch 3671, Loss: 0.31019796431064606, Final Batch Loss: 0.17027972638607025\n",
      "Epoch 3672, Loss: 0.3141752928495407, Final Batch Loss: 0.1552680879831314\n",
      "Epoch 3673, Loss: 0.24630206823349, Final Batch Loss: 0.12985175848007202\n",
      "Epoch 3674, Loss: 0.24579790979623795, Final Batch Loss: 0.10665186494588852\n",
      "Epoch 3675, Loss: 0.2778128311038017, Final Batch Loss: 0.12120024114847183\n",
      "Epoch 3676, Loss: 0.2633248344063759, Final Batch Loss: 0.14928583800792694\n",
      "Epoch 3677, Loss: 0.36295025050640106, Final Batch Loss: 0.14003804326057434\n",
      "Epoch 3678, Loss: 0.2653690055012703, Final Batch Loss: 0.1733984351158142\n",
      "Epoch 3679, Loss: 0.25268564373254776, Final Batch Loss: 0.1057361289858818\n",
      "Epoch 3680, Loss: 0.3080229163169861, Final Batch Loss: 0.1647268384695053\n",
      "Epoch 3681, Loss: 0.3185727745294571, Final Batch Loss: 0.1343432515859604\n",
      "Epoch 3682, Loss: 0.24865377694368362, Final Batch Loss: 0.0992521271109581\n",
      "Epoch 3683, Loss: 0.3027985394001007, Final Batch Loss: 0.17210429906845093\n",
      "Epoch 3684, Loss: 0.2682528495788574, Final Batch Loss: 0.14225734770298004\n",
      "Epoch 3685, Loss: 0.2787385880947113, Final Batch Loss: 0.15031225979328156\n",
      "Epoch 3686, Loss: 0.3029037415981293, Final Batch Loss: 0.16075310111045837\n",
      "Epoch 3687, Loss: 0.34436678886413574, Final Batch Loss: 0.19821111857891083\n",
      "Epoch 3688, Loss: 0.28790098428726196, Final Batch Loss: 0.17891238629817963\n",
      "Epoch 3689, Loss: 0.28310631215572357, Final Batch Loss: 0.14889635145664215\n",
      "Epoch 3690, Loss: 0.293851375579834, Final Batch Loss: 0.12629957497119904\n",
      "Epoch 3691, Loss: 0.25739768892526627, Final Batch Loss: 0.17570863664150238\n",
      "Epoch 3692, Loss: 0.2783450409770012, Final Batch Loss: 0.1564597338438034\n",
      "Epoch 3693, Loss: 0.27917052060365677, Final Batch Loss: 0.08860481530427933\n",
      "Epoch 3694, Loss: 0.29613909870386124, Final Batch Loss: 0.10177650302648544\n",
      "Epoch 3695, Loss: 0.28975315392017365, Final Batch Loss: 0.14100660383701324\n",
      "Epoch 3696, Loss: 0.2719351202249527, Final Batch Loss: 0.1150749921798706\n",
      "Epoch 3697, Loss: 0.2763431966304779, Final Batch Loss: 0.13382621109485626\n",
      "Epoch 3698, Loss: 0.24628254026174545, Final Batch Loss: 0.12460163235664368\n",
      "Epoch 3699, Loss: 0.24628359824419022, Final Batch Loss: 0.09798096865415573\n",
      "Epoch 3700, Loss: 0.29347869753837585, Final Batch Loss: 0.1529189497232437\n",
      "Epoch 3701, Loss: 0.27844250202178955, Final Batch Loss: 0.14050567150115967\n",
      "Epoch 3702, Loss: 0.2340736836194992, Final Batch Loss: 0.11559495329856873\n",
      "Epoch 3703, Loss: 0.2746184766292572, Final Batch Loss: 0.17391091585159302\n",
      "Epoch 3704, Loss: 0.30213768780231476, Final Batch Loss: 0.15727533400058746\n",
      "Epoch 3705, Loss: 0.22966182231903076, Final Batch Loss: 0.11388510465621948\n",
      "Epoch 3706, Loss: 0.2583073601126671, Final Batch Loss: 0.12189527601003647\n",
      "Epoch 3707, Loss: 0.2616725116968155, Final Batch Loss: 0.12486976385116577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3708, Loss: 0.29183731973171234, Final Batch Loss: 0.1742473989725113\n",
      "Epoch 3709, Loss: 0.2938036471605301, Final Batch Loss: 0.13962656259536743\n",
      "Epoch 3710, Loss: 0.25829070806503296, Final Batch Loss: 0.11921602487564087\n",
      "Epoch 3711, Loss: 0.2592220678925514, Final Batch Loss: 0.11508246511220932\n",
      "Epoch 3712, Loss: 0.25587619841098785, Final Batch Loss: 0.14531606435775757\n",
      "Epoch 3713, Loss: 0.26932793110609055, Final Batch Loss: 0.16695570945739746\n",
      "Epoch 3714, Loss: 0.29519231617450714, Final Batch Loss: 0.19777648150920868\n",
      "Epoch 3715, Loss: 0.26749324053525925, Final Batch Loss: 0.10349813848733902\n",
      "Epoch 3716, Loss: 0.2645036429166794, Final Batch Loss: 0.12730960547924042\n",
      "Epoch 3717, Loss: 0.2668968290090561, Final Batch Loss: 0.1387978196144104\n",
      "Epoch 3718, Loss: 0.2348068282008171, Final Batch Loss: 0.10053584724664688\n",
      "Epoch 3719, Loss: 0.26638808101415634, Final Batch Loss: 0.1545819193124771\n",
      "Epoch 3720, Loss: 0.2709352821111679, Final Batch Loss: 0.14700190722942352\n",
      "Epoch 3721, Loss: 0.26326365023851395, Final Batch Loss: 0.10774106532335281\n",
      "Epoch 3722, Loss: 0.25584616512060165, Final Batch Loss: 0.11834048479795456\n",
      "Epoch 3723, Loss: 0.267556756734848, Final Batch Loss: 0.13998964428901672\n",
      "Epoch 3724, Loss: 0.251347579061985, Final Batch Loss: 0.14141713082790375\n",
      "Epoch 3725, Loss: 0.2718435376882553, Final Batch Loss: 0.14163173735141754\n",
      "Epoch 3726, Loss: 0.26075518131256104, Final Batch Loss: 0.09823906421661377\n",
      "Epoch 3727, Loss: 0.2663671746850014, Final Batch Loss: 0.121696837246418\n",
      "Epoch 3728, Loss: 0.2602471113204956, Final Batch Loss: 0.1193871796131134\n",
      "Epoch 3729, Loss: 0.24292220920324326, Final Batch Loss: 0.1421661227941513\n",
      "Epoch 3730, Loss: 0.2539513036608696, Final Batch Loss: 0.11951696127653122\n",
      "Epoch 3731, Loss: 0.2657696008682251, Final Batch Loss: 0.09718215465545654\n",
      "Epoch 3732, Loss: 0.29572121798992157, Final Batch Loss: 0.15426720678806305\n",
      "Epoch 3733, Loss: 0.2684778422117233, Final Batch Loss: 0.13261978328227997\n",
      "Epoch 3734, Loss: 0.27796587347984314, Final Batch Loss: 0.12339532375335693\n",
      "Epoch 3735, Loss: 0.307461142539978, Final Batch Loss: 0.1736474484205246\n",
      "Epoch 3736, Loss: 0.2547113373875618, Final Batch Loss: 0.13096211850643158\n",
      "Epoch 3737, Loss: 0.2788667604327202, Final Batch Loss: 0.12371522933244705\n",
      "Epoch 3738, Loss: 0.2726369649171829, Final Batch Loss: 0.1561766266822815\n",
      "Epoch 3739, Loss: 0.2541830763220787, Final Batch Loss: 0.11337751895189285\n",
      "Epoch 3740, Loss: 0.25396862626075745, Final Batch Loss: 0.1347428262233734\n",
      "Epoch 3741, Loss: 0.2546734884381294, Final Batch Loss: 0.10459072142839432\n",
      "Epoch 3742, Loss: 0.3215879201889038, Final Batch Loss: 0.19716407358646393\n",
      "Epoch 3743, Loss: 0.3021373748779297, Final Batch Loss: 0.1647191196680069\n",
      "Epoch 3744, Loss: 0.2447066381573677, Final Batch Loss: 0.13508416712284088\n",
      "Epoch 3745, Loss: 0.24425892531871796, Final Batch Loss: 0.11640587449073792\n",
      "Epoch 3746, Loss: 0.27280863374471664, Final Batch Loss: 0.11975618451833725\n",
      "Epoch 3747, Loss: 0.2971940189599991, Final Batch Loss: 0.14447897672653198\n",
      "Epoch 3748, Loss: 0.27398569881916046, Final Batch Loss: 0.14815498888492584\n",
      "Epoch 3749, Loss: 0.2566067725419998, Final Batch Loss: 0.13383907079696655\n",
      "Epoch 3750, Loss: 0.2525637224316597, Final Batch Loss: 0.11108998209238052\n",
      "Epoch 3751, Loss: 0.25071050226688385, Final Batch Loss: 0.14065764844417572\n",
      "Epoch 3752, Loss: 0.2598320171236992, Final Batch Loss: 0.13710694015026093\n",
      "Epoch 3753, Loss: 0.27185939252376556, Final Batch Loss: 0.14678354561328888\n",
      "Epoch 3754, Loss: 0.25493188947439194, Final Batch Loss: 0.13494105637073517\n",
      "Epoch 3755, Loss: 0.26164568215608597, Final Batch Loss: 0.15376527607440948\n",
      "Epoch 3756, Loss: 0.26205889880657196, Final Batch Loss: 0.1278373897075653\n",
      "Epoch 3757, Loss: 0.2563040181994438, Final Batch Loss: 0.1564517319202423\n",
      "Epoch 3758, Loss: 0.28290534019470215, Final Batch Loss: 0.10571524500846863\n",
      "Epoch 3759, Loss: 0.2501460015773773, Final Batch Loss: 0.13206230103969574\n",
      "Epoch 3760, Loss: 0.28153039515018463, Final Batch Loss: 0.13876371085643768\n",
      "Epoch 3761, Loss: 0.28145909309387207, Final Batch Loss: 0.1320609301328659\n",
      "Epoch 3762, Loss: 0.27026037871837616, Final Batch Loss: 0.1306973546743393\n",
      "Epoch 3763, Loss: 0.27237163484096527, Final Batch Loss: 0.14084802567958832\n",
      "Epoch 3764, Loss: 0.24095287173986435, Final Batch Loss: 0.10827375203371048\n",
      "Epoch 3765, Loss: 0.2681645080447197, Final Batch Loss: 0.12285911291837692\n",
      "Epoch 3766, Loss: 0.2622489407658577, Final Batch Loss: 0.14254240691661835\n",
      "Epoch 3767, Loss: 0.2958081215620041, Final Batch Loss: 0.1621590107679367\n",
      "Epoch 3768, Loss: 0.2512256056070328, Final Batch Loss: 0.10951083898544312\n",
      "Epoch 3769, Loss: 0.26299111545085907, Final Batch Loss: 0.12633424997329712\n",
      "Epoch 3770, Loss: 0.2696314752101898, Final Batch Loss: 0.13429561257362366\n",
      "Epoch 3771, Loss: 0.26298838108778, Final Batch Loss: 0.16158033907413483\n",
      "Epoch 3772, Loss: 0.2957663983106613, Final Batch Loss: 0.1523464173078537\n",
      "Epoch 3773, Loss: 0.2808651030063629, Final Batch Loss: 0.1481086164712906\n",
      "Epoch 3774, Loss: 0.2476986050605774, Final Batch Loss: 0.11512678861618042\n",
      "Epoch 3775, Loss: 0.2545871287584305, Final Batch Loss: 0.09812271595001221\n",
      "Epoch 3776, Loss: 0.2848049998283386, Final Batch Loss: 0.1719384342432022\n",
      "Epoch 3777, Loss: 0.2876567393541336, Final Batch Loss: 0.15435446798801422\n",
      "Epoch 3778, Loss: 0.21753409504890442, Final Batch Loss: 0.10962646454572678\n",
      "Epoch 3779, Loss: 0.25680167227983475, Final Batch Loss: 0.13586020469665527\n",
      "Epoch 3780, Loss: 0.2497713789343834, Final Batch Loss: 0.10938870161771774\n",
      "Epoch 3781, Loss: 0.27035607397556305, Final Batch Loss: 0.14139576256275177\n",
      "Epoch 3782, Loss: 0.2676815763115883, Final Batch Loss: 0.1486503779888153\n",
      "Epoch 3783, Loss: 0.25168080627918243, Final Batch Loss: 0.1289682537317276\n",
      "Epoch 3784, Loss: 0.23826169222593307, Final Batch Loss: 0.11678601056337357\n",
      "Epoch 3785, Loss: 0.33326055109500885, Final Batch Loss: 0.17875726521015167\n",
      "Epoch 3786, Loss: 0.2808319851756096, Final Batch Loss: 0.12261531502008438\n",
      "Epoch 3787, Loss: 0.3061840161681175, Final Batch Loss: 0.1010851189494133\n",
      "Epoch 3788, Loss: 0.3123702108860016, Final Batch Loss: 0.14481867849826813\n",
      "Epoch 3789, Loss: 0.23378531634807587, Final Batch Loss: 0.10539701581001282\n",
      "Epoch 3790, Loss: 0.2583547830581665, Final Batch Loss: 0.10484004020690918\n",
      "Epoch 3791, Loss: 0.2628159373998642, Final Batch Loss: 0.13696105778217316\n",
      "Epoch 3792, Loss: 0.28743426501750946, Final Batch Loss: 0.13083769381046295\n",
      "Epoch 3793, Loss: 0.28445160388946533, Final Batch Loss: 0.1516304463148117\n",
      "Epoch 3794, Loss: 0.2623669356107712, Final Batch Loss: 0.12716259062290192\n",
      "Epoch 3795, Loss: 0.2692752256989479, Final Batch Loss: 0.11709626764059067\n",
      "Epoch 3796, Loss: 0.25346608459949493, Final Batch Loss: 0.11203739047050476\n",
      "Epoch 3797, Loss: 0.2724120542407036, Final Batch Loss: 0.14824752509593964\n",
      "Epoch 3798, Loss: 0.2554843947291374, Final Batch Loss: 0.11540370434522629\n",
      "Epoch 3799, Loss: 0.2929062023758888, Final Batch Loss: 0.10003850609064102\n",
      "Epoch 3800, Loss: 0.2912713512778282, Final Batch Loss: 0.1723017692565918\n",
      "Epoch 3801, Loss: 0.25241719186306, Final Batch Loss: 0.1178857684135437\n",
      "Epoch 3802, Loss: 0.29497572779655457, Final Batch Loss: 0.14693476259708405\n",
      "Epoch 3803, Loss: 0.2804589718580246, Final Batch Loss: 0.14163048565387726\n",
      "Epoch 3804, Loss: 0.2669418081641197, Final Batch Loss: 0.11837209016084671\n",
      "Epoch 3805, Loss: 0.24784543365240097, Final Batch Loss: 0.11268078535795212\n",
      "Epoch 3806, Loss: 0.27561039477586746, Final Batch Loss: 0.1095084473490715\n",
      "Epoch 3807, Loss: 0.2736724093556404, Final Batch Loss: 0.11782818287611008\n",
      "Epoch 3808, Loss: 0.2574542462825775, Final Batch Loss: 0.11227673292160034\n",
      "Epoch 3809, Loss: 0.2653542160987854, Final Batch Loss: 0.12500818073749542\n",
      "Epoch 3810, Loss: 0.3332057148218155, Final Batch Loss: 0.13436104357242584\n",
      "Epoch 3811, Loss: 0.2541638910770416, Final Batch Loss: 0.13005311787128448\n",
      "Epoch 3812, Loss: 0.2532026544213295, Final Batch Loss: 0.14246511459350586\n",
      "Epoch 3813, Loss: 0.2615709826350212, Final Batch Loss: 0.13961021602153778\n",
      "Epoch 3814, Loss: 0.2857707738876343, Final Batch Loss: 0.1327928602695465\n",
      "Epoch 3815, Loss: 0.26229507476091385, Final Batch Loss: 0.14836077392101288\n",
      "Epoch 3816, Loss: 0.2627626433968544, Final Batch Loss: 0.12234311550855637\n",
      "Epoch 3817, Loss: 0.2740132138133049, Final Batch Loss: 0.10438761860132217\n",
      "Epoch 3818, Loss: 0.26266737282276154, Final Batch Loss: 0.0990946888923645\n",
      "Epoch 3819, Loss: 0.30045585334300995, Final Batch Loss: 0.16602356731891632\n",
      "Epoch 3820, Loss: 0.25056296586990356, Final Batch Loss: 0.11918255686759949\n",
      "Epoch 3821, Loss: 0.2469535619020462, Final Batch Loss: 0.10737523436546326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3822, Loss: 0.27807676792144775, Final Batch Loss: 0.15069802105426788\n",
      "Epoch 3823, Loss: 0.24463478475809097, Final Batch Loss: 0.11485431343317032\n",
      "Epoch 3824, Loss: 0.2379986271262169, Final Batch Loss: 0.11578449606895447\n",
      "Epoch 3825, Loss: 0.2761526554822922, Final Batch Loss: 0.14463332295417786\n",
      "Epoch 3826, Loss: 0.266420379281044, Final Batch Loss: 0.1547037810087204\n",
      "Epoch 3827, Loss: 0.2744310423731804, Final Batch Loss: 0.11535824090242386\n",
      "Epoch 3828, Loss: 0.25233863294124603, Final Batch Loss: 0.12888896465301514\n",
      "Epoch 3829, Loss: 0.32880809903144836, Final Batch Loss: 0.13825196027755737\n",
      "Epoch 3830, Loss: 0.3003721088171005, Final Batch Loss: 0.11937898397445679\n",
      "Epoch 3831, Loss: 0.2553885728120804, Final Batch Loss: 0.13552525639533997\n",
      "Epoch 3832, Loss: 0.24064497649669647, Final Batch Loss: 0.1354505568742752\n",
      "Epoch 3833, Loss: 0.2735445201396942, Final Batch Loss: 0.1545609086751938\n",
      "Epoch 3834, Loss: 0.23549309372901917, Final Batch Loss: 0.13214851915836334\n",
      "Epoch 3835, Loss: 0.2569320276379585, Final Batch Loss: 0.11060699075460434\n",
      "Epoch 3836, Loss: 0.27014026790857315, Final Batch Loss: 0.16561686992645264\n",
      "Epoch 3837, Loss: 0.26862580329179764, Final Batch Loss: 0.1140100285410881\n",
      "Epoch 3838, Loss: 0.34005144238471985, Final Batch Loss: 0.17458666861057281\n",
      "Epoch 3839, Loss: 0.2457178458571434, Final Batch Loss: 0.11266312748193741\n",
      "Epoch 3840, Loss: 0.2811466306447983, Final Batch Loss: 0.15447407960891724\n",
      "Epoch 3841, Loss: 0.2667723149061203, Final Batch Loss: 0.13668818771839142\n",
      "Epoch 3842, Loss: 0.25960682332515717, Final Batch Loss: 0.12721483409404755\n",
      "Epoch 3843, Loss: 0.2491431087255478, Final Batch Loss: 0.11209774017333984\n",
      "Epoch 3844, Loss: 0.25981305539608, Final Batch Loss: 0.11400407552719116\n",
      "Epoch 3845, Loss: 0.2658889889717102, Final Batch Loss: 0.15187890827655792\n",
      "Epoch 3846, Loss: 0.31199028342962265, Final Batch Loss: 0.11990814656019211\n",
      "Epoch 3847, Loss: 0.2428857758641243, Final Batch Loss: 0.12399312108755112\n",
      "Epoch 3848, Loss: 0.287296786904335, Final Batch Loss: 0.15133549273014069\n",
      "Epoch 3849, Loss: 0.26873978972435, Final Batch Loss: 0.18426920473575592\n",
      "Epoch 3850, Loss: 0.21880752593278885, Final Batch Loss: 0.11885741353034973\n",
      "Epoch 3851, Loss: 0.22503765672445297, Final Batch Loss: 0.082631416618824\n",
      "Epoch 3852, Loss: 0.2959745526313782, Final Batch Loss: 0.1307479590177536\n",
      "Epoch 3853, Loss: 0.27408625930547714, Final Batch Loss: 0.1147267147898674\n",
      "Epoch 3854, Loss: 0.2292712777853012, Final Batch Loss: 0.10890715569257736\n",
      "Epoch 3855, Loss: 0.2744944542646408, Final Batch Loss: 0.16353686153888702\n",
      "Epoch 3856, Loss: 0.2472841888666153, Final Batch Loss: 0.1248793825507164\n",
      "Epoch 3857, Loss: 0.27068162709474564, Final Batch Loss: 0.1248571053147316\n",
      "Epoch 3858, Loss: 0.2349478229880333, Final Batch Loss: 0.09368161112070084\n",
      "Epoch 3859, Loss: 0.26366160064935684, Final Batch Loss: 0.11214105039834976\n",
      "Epoch 3860, Loss: 0.2708272188901901, Final Batch Loss: 0.13649606704711914\n",
      "Epoch 3861, Loss: 0.30759260058403015, Final Batch Loss: 0.17753225564956665\n",
      "Epoch 3862, Loss: 0.220806322991848, Final Batch Loss: 0.11982152611017227\n",
      "Epoch 3863, Loss: 0.2605099231004715, Final Batch Loss: 0.10442480444908142\n",
      "Epoch 3864, Loss: 0.3001003712415695, Final Batch Loss: 0.15510864555835724\n",
      "Epoch 3865, Loss: 0.30172009766101837, Final Batch Loss: 0.15524809062480927\n",
      "Epoch 3866, Loss: 0.2436918541789055, Final Batch Loss: 0.12366855889558792\n",
      "Epoch 3867, Loss: 0.25052618980407715, Final Batch Loss: 0.13025404512882233\n",
      "Epoch 3868, Loss: 0.259774811565876, Final Batch Loss: 0.135103240609169\n",
      "Epoch 3869, Loss: 0.23897502571344376, Final Batch Loss: 0.1489351987838745\n",
      "Epoch 3870, Loss: 0.2526301294565201, Final Batch Loss: 0.13202251493930817\n",
      "Epoch 3871, Loss: 0.2585683539509773, Final Batch Loss: 0.09644486755132675\n",
      "Epoch 3872, Loss: 0.2545195147395134, Final Batch Loss: 0.11817017942667007\n",
      "Epoch 3873, Loss: 0.2763882130384445, Final Batch Loss: 0.1406688541173935\n",
      "Epoch 3874, Loss: 0.24994786083698273, Final Batch Loss: 0.14812953770160675\n",
      "Epoch 3875, Loss: 0.26583968102931976, Final Batch Loss: 0.13989533483982086\n",
      "Epoch 3876, Loss: 0.2962547540664673, Final Batch Loss: 0.1186990737915039\n",
      "Epoch 3877, Loss: 0.23373428732156754, Final Batch Loss: 0.06853394955396652\n",
      "Epoch 3878, Loss: 0.2666177675127983, Final Batch Loss: 0.17153596878051758\n",
      "Epoch 3879, Loss: 0.26909907907247543, Final Batch Loss: 0.15646930038928986\n",
      "Epoch 3880, Loss: 0.2661897763609886, Final Batch Loss: 0.12380445748567581\n",
      "Epoch 3881, Loss: 0.25843042880296707, Final Batch Loss: 0.1105053648352623\n",
      "Epoch 3882, Loss: 0.2742065489292145, Final Batch Loss: 0.15040771663188934\n",
      "Epoch 3883, Loss: 0.2451448142528534, Final Batch Loss: 0.09229254722595215\n",
      "Epoch 3884, Loss: 0.26719801872968674, Final Batch Loss: 0.11924592405557632\n",
      "Epoch 3885, Loss: 0.22727720439434052, Final Batch Loss: 0.11605208367109299\n",
      "Epoch 3886, Loss: 0.24386854469776154, Final Batch Loss: 0.10117465257644653\n",
      "Epoch 3887, Loss: 0.2859758734703064, Final Batch Loss: 0.14339570701122284\n",
      "Epoch 3888, Loss: 0.2702586501836777, Final Batch Loss: 0.11936122179031372\n",
      "Epoch 3889, Loss: 0.25717488676309586, Final Batch Loss: 0.16187895834445953\n",
      "Epoch 3890, Loss: 0.2513548955321312, Final Batch Loss: 0.12407735735177994\n",
      "Epoch 3891, Loss: 0.23491641134023666, Final Batch Loss: 0.10069539397954941\n",
      "Epoch 3892, Loss: 0.24635261297225952, Final Batch Loss: 0.14501945674419403\n",
      "Epoch 3893, Loss: 0.23839537054300308, Final Batch Loss: 0.10475828498601913\n",
      "Epoch 3894, Loss: 0.2961007058620453, Final Batch Loss: 0.17684011161327362\n",
      "Epoch 3895, Loss: 0.24938788264989853, Final Batch Loss: 0.15064261853694916\n",
      "Epoch 3896, Loss: 0.24503498524427414, Final Batch Loss: 0.08408334106206894\n",
      "Epoch 3897, Loss: 0.28560473024845123, Final Batch Loss: 0.17008109390735626\n",
      "Epoch 3898, Loss: 0.26984143257141113, Final Batch Loss: 0.15594793856143951\n",
      "Epoch 3899, Loss: 0.25813882052898407, Final Batch Loss: 0.13086555898189545\n",
      "Epoch 3900, Loss: 0.25093383342027664, Final Batch Loss: 0.12634782493114471\n",
      "Epoch 3901, Loss: 0.2813059091567993, Final Batch Loss: 0.14517025649547577\n",
      "Epoch 3902, Loss: 0.3050095736980438, Final Batch Loss: 0.14874976873397827\n",
      "Epoch 3903, Loss: 0.3021833598613739, Final Batch Loss: 0.1602870374917984\n",
      "Epoch 3904, Loss: 0.31479354202747345, Final Batch Loss: 0.16774892807006836\n",
      "Epoch 3905, Loss: 0.2890904173254967, Final Batch Loss: 0.16517098248004913\n",
      "Epoch 3906, Loss: 0.28630463778972626, Final Batch Loss: 0.12090852856636047\n",
      "Epoch 3907, Loss: 0.24721659719944, Final Batch Loss: 0.14454185962677002\n",
      "Epoch 3908, Loss: 0.25268231332302094, Final Batch Loss: 0.14962948858737946\n",
      "Epoch 3909, Loss: 0.2438756301999092, Final Batch Loss: 0.1275150030851364\n",
      "Epoch 3910, Loss: 0.23802915215492249, Final Batch Loss: 0.09684699773788452\n",
      "Epoch 3911, Loss: 0.27141107618808746, Final Batch Loss: 0.14253094792366028\n",
      "Epoch 3912, Loss: 0.2123929262161255, Final Batch Loss: 0.1145893856883049\n",
      "Epoch 3913, Loss: 0.2604000195860863, Final Batch Loss: 0.15821780264377594\n",
      "Epoch 3914, Loss: 0.23930145800113678, Final Batch Loss: 0.12488218396902084\n",
      "Epoch 3915, Loss: 0.2545091211795807, Final Batch Loss: 0.09434640407562256\n",
      "Epoch 3916, Loss: 0.2512369528412819, Final Batch Loss: 0.13136564195156097\n",
      "Epoch 3917, Loss: 0.2611541748046875, Final Batch Loss: 0.12794721126556396\n",
      "Epoch 3918, Loss: 0.26686353981494904, Final Batch Loss: 0.14744730293750763\n",
      "Epoch 3919, Loss: 0.24048426002264023, Final Batch Loss: 0.12661410868167877\n",
      "Epoch 3920, Loss: 0.2640846073627472, Final Batch Loss: 0.15561972558498383\n",
      "Epoch 3921, Loss: 0.26628950238227844, Final Batch Loss: 0.13663418591022491\n",
      "Epoch 3922, Loss: 0.24694395810365677, Final Batch Loss: 0.14338888227939606\n",
      "Epoch 3923, Loss: 0.29162706434726715, Final Batch Loss: 0.1468842327594757\n",
      "Epoch 3924, Loss: 0.25459982454776764, Final Batch Loss: 0.14360587298870087\n",
      "Epoch 3925, Loss: 0.29933322966098785, Final Batch Loss: 0.17602978646755219\n",
      "Epoch 3926, Loss: 0.2533241733908653, Final Batch Loss: 0.1035594567656517\n",
      "Epoch 3927, Loss: 0.2911248803138733, Final Batch Loss: 0.1635407954454422\n",
      "Epoch 3928, Loss: 0.2863982617855072, Final Batch Loss: 0.12454330921173096\n",
      "Epoch 3929, Loss: 0.22758299857378006, Final Batch Loss: 0.09483078867197037\n",
      "Epoch 3930, Loss: 0.29493458569049835, Final Batch Loss: 0.15889620780944824\n",
      "Epoch 3931, Loss: 0.2634033262729645, Final Batch Loss: 0.13493464887142181\n",
      "Epoch 3932, Loss: 0.24134927988052368, Final Batch Loss: 0.11775132268667221\n",
      "Epoch 3933, Loss: 0.2358810231089592, Final Batch Loss: 0.11611384153366089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3934, Loss: 0.2798674553632736, Final Batch Loss: 0.13955329358577728\n",
      "Epoch 3935, Loss: 0.23663213104009628, Final Batch Loss: 0.1090526208281517\n",
      "Epoch 3936, Loss: 0.2709488645195961, Final Batch Loss: 0.15340663492679596\n",
      "Epoch 3937, Loss: 0.2685689404606819, Final Batch Loss: 0.14927108585834503\n",
      "Epoch 3938, Loss: 0.2383010983467102, Final Batch Loss: 0.11867975443601608\n",
      "Epoch 3939, Loss: 0.28918804228305817, Final Batch Loss: 0.1638609766960144\n",
      "Epoch 3940, Loss: 0.2850089967250824, Final Batch Loss: 0.14219193160533905\n",
      "Epoch 3941, Loss: 0.25430522859096527, Final Batch Loss: 0.10770812630653381\n",
      "Epoch 3942, Loss: 0.25932440161705017, Final Batch Loss: 0.13929937779903412\n",
      "Epoch 3943, Loss: 0.2771715223789215, Final Batch Loss: 0.1262783408164978\n",
      "Epoch 3944, Loss: 0.21495607495307922, Final Batch Loss: 0.09633489698171616\n",
      "Epoch 3945, Loss: 0.27560245990753174, Final Batch Loss: 0.12868532538414001\n",
      "Epoch 3946, Loss: 0.25616395473480225, Final Batch Loss: 0.11899945139884949\n",
      "Epoch 3947, Loss: 0.2364654839038849, Final Batch Loss: 0.11447799205780029\n",
      "Epoch 3948, Loss: 0.27298765629529953, Final Batch Loss: 0.10554740577936172\n",
      "Epoch 3949, Loss: 0.2674576938152313, Final Batch Loss: 0.15028789639472961\n",
      "Epoch 3950, Loss: 0.21120309829711914, Final Batch Loss: 0.10993622988462448\n",
      "Epoch 3951, Loss: 0.28770023584365845, Final Batch Loss: 0.12544523179531097\n",
      "Epoch 3952, Loss: 0.2542577534914017, Final Batch Loss: 0.13259324431419373\n",
      "Epoch 3953, Loss: 0.2584580332040787, Final Batch Loss: 0.12374034523963928\n",
      "Epoch 3954, Loss: 0.28908251225948334, Final Batch Loss: 0.12601065635681152\n",
      "Epoch 3955, Loss: 0.2386574223637581, Final Batch Loss: 0.11431727558374405\n",
      "Epoch 3956, Loss: 0.22904273122549057, Final Batch Loss: 0.12075624614953995\n",
      "Epoch 3957, Loss: 0.26042141765356064, Final Batch Loss: 0.10963457077741623\n",
      "Epoch 3958, Loss: 0.286162793636322, Final Batch Loss: 0.14833775162696838\n",
      "Epoch 3959, Loss: 0.25085652619600296, Final Batch Loss: 0.122504822909832\n",
      "Epoch 3960, Loss: 0.2596554383635521, Final Batch Loss: 0.12320341914892197\n",
      "Epoch 3961, Loss: 0.2637458071112633, Final Batch Loss: 0.12207181006669998\n",
      "Epoch 3962, Loss: 0.28969473391771317, Final Batch Loss: 0.11843652278184891\n",
      "Epoch 3963, Loss: 0.2726617157459259, Final Batch Loss: 0.14916829764842987\n",
      "Epoch 3964, Loss: 0.28495920449495316, Final Batch Loss: 0.16191466152668\n",
      "Epoch 3965, Loss: 0.2687222510576248, Final Batch Loss: 0.13341641426086426\n",
      "Epoch 3966, Loss: 0.22928356379270554, Final Batch Loss: 0.11318423599004745\n",
      "Epoch 3967, Loss: 0.26216772198677063, Final Batch Loss: 0.1303827315568924\n",
      "Epoch 3968, Loss: 0.2811350077390671, Final Batch Loss: 0.12794403731822968\n",
      "Epoch 3969, Loss: 0.29139474034309387, Final Batch Loss: 0.1340266764163971\n",
      "Epoch 3970, Loss: 0.2764684110879898, Final Batch Loss: 0.14027194678783417\n",
      "Epoch 3971, Loss: 0.25577693432569504, Final Batch Loss: 0.11271513253450394\n",
      "Epoch 3972, Loss: 0.259692944586277, Final Batch Loss: 0.11273562163114548\n",
      "Epoch 3973, Loss: 0.2537686750292778, Final Batch Loss: 0.10982898622751236\n",
      "Epoch 3974, Loss: 0.27131032943725586, Final Batch Loss: 0.14240679144859314\n",
      "Epoch 3975, Loss: 0.3308284431695938, Final Batch Loss: 0.1427394151687622\n",
      "Epoch 3976, Loss: 0.22506872564554214, Final Batch Loss: 0.09941010922193527\n",
      "Epoch 3977, Loss: 0.27976706624031067, Final Batch Loss: 0.15203256905078888\n",
      "Epoch 3978, Loss: 0.29726575314998627, Final Batch Loss: 0.15367434918880463\n",
      "Epoch 3979, Loss: 0.23999495804309845, Final Batch Loss: 0.13463418185710907\n",
      "Epoch 3980, Loss: 0.22048544883728027, Final Batch Loss: 0.11355683207511902\n",
      "Epoch 3981, Loss: 0.24866540729999542, Final Batch Loss: 0.14815838634967804\n",
      "Epoch 3982, Loss: 0.26908186823129654, Final Batch Loss: 0.16054056584835052\n",
      "Epoch 3983, Loss: 0.2959242910146713, Final Batch Loss: 0.12550945580005646\n",
      "Epoch 3984, Loss: 0.26442138105630875, Final Batch Loss: 0.12135649472475052\n",
      "Epoch 3985, Loss: 0.2591245323419571, Final Batch Loss: 0.13144350051879883\n",
      "Epoch 3986, Loss: 0.2555497884750366, Final Batch Loss: 0.10930472612380981\n",
      "Epoch 3987, Loss: 0.30516982078552246, Final Batch Loss: 0.17161166667938232\n",
      "Epoch 3988, Loss: 0.24215445667505264, Final Batch Loss: 0.15259335935115814\n",
      "Epoch 3989, Loss: 0.3160737156867981, Final Batch Loss: 0.14444376528263092\n",
      "Epoch 3990, Loss: 0.22791453450918198, Final Batch Loss: 0.14425155520439148\n",
      "Epoch 3991, Loss: 0.3091174066066742, Final Batch Loss: 0.16678841412067413\n",
      "Epoch 3992, Loss: 0.23262132704257965, Final Batch Loss: 0.10185536742210388\n",
      "Epoch 3993, Loss: 0.25700870156288147, Final Batch Loss: 0.12911662459373474\n",
      "Epoch 3994, Loss: 0.23722200095653534, Final Batch Loss: 0.09446707367897034\n",
      "Epoch 3995, Loss: 0.2302914783358574, Final Batch Loss: 0.10134100168943405\n",
      "Epoch 3996, Loss: 0.2565501928329468, Final Batch Loss: 0.14156270027160645\n",
      "Epoch 3997, Loss: 0.24446823447942734, Final Batch Loss: 0.09715098887681961\n",
      "Epoch 3998, Loss: 0.22604115307331085, Final Batch Loss: 0.10268023610115051\n",
      "Epoch 3999, Loss: 0.28775620460510254, Final Batch Loss: 0.1388070285320282\n",
      "Epoch 4000, Loss: 0.24928004294633865, Final Batch Loss: 0.13215969502925873\n",
      "Epoch 4001, Loss: 0.21717624366283417, Final Batch Loss: 0.11544517427682877\n",
      "Epoch 4002, Loss: 0.2967246174812317, Final Batch Loss: 0.19818146526813507\n",
      "Epoch 4003, Loss: 0.2940979674458504, Final Batch Loss: 0.11801857501268387\n",
      "Epoch 4004, Loss: 0.2481132596731186, Final Batch Loss: 0.15834636986255646\n",
      "Epoch 4005, Loss: 0.318946436047554, Final Batch Loss: 0.150716170668602\n",
      "Epoch 4006, Loss: 0.2515252009034157, Final Batch Loss: 0.11626876145601273\n",
      "Epoch 4007, Loss: 0.2535606697201729, Final Batch Loss: 0.1087561622262001\n",
      "Epoch 4008, Loss: 0.24841740727424622, Final Batch Loss: 0.12314477562904358\n",
      "Epoch 4009, Loss: 0.2655413821339607, Final Batch Loss: 0.1564018279314041\n",
      "Epoch 4010, Loss: 0.39706043899059296, Final Batch Loss: 0.19271571934223175\n",
      "Epoch 4011, Loss: 0.2219650149345398, Final Batch Loss: 0.10489342361688614\n",
      "Epoch 4012, Loss: 0.25515156239271164, Final Batch Loss: 0.13327017426490784\n",
      "Epoch 4013, Loss: 0.23445076495409012, Final Batch Loss: 0.11942905932664871\n",
      "Epoch 4014, Loss: 0.2832538112998009, Final Batch Loss: 0.17523889243602753\n",
      "Epoch 4015, Loss: 0.24364856630563736, Final Batch Loss: 0.12922684848308563\n",
      "Epoch 4016, Loss: 0.233810193836689, Final Batch Loss: 0.12303292006254196\n",
      "Epoch 4017, Loss: 0.27591855823993683, Final Batch Loss: 0.13395442068576813\n",
      "Epoch 4018, Loss: 0.22557255625724792, Final Batch Loss: 0.09354019165039062\n",
      "Epoch 4019, Loss: 0.2660629153251648, Final Batch Loss: 0.12410581111907959\n",
      "Epoch 4020, Loss: 0.28156451880931854, Final Batch Loss: 0.13972221314907074\n",
      "Epoch 4021, Loss: 0.27087903022766113, Final Batch Loss: 0.11320403218269348\n",
      "Epoch 4022, Loss: 0.28276030719280243, Final Batch Loss: 0.13823732733726501\n",
      "Epoch 4023, Loss: 0.2588701844215393, Final Batch Loss: 0.12644080817699432\n",
      "Epoch 4024, Loss: 0.2798397094011307, Final Batch Loss: 0.1399446576833725\n",
      "Epoch 4025, Loss: 0.2761012092232704, Final Batch Loss: 0.1141379103064537\n",
      "Epoch 4026, Loss: 0.2510029524564743, Final Batch Loss: 0.12380120158195496\n",
      "Epoch 4027, Loss: 0.26882971823215485, Final Batch Loss: 0.12230145931243896\n",
      "Epoch 4028, Loss: 0.22776449471712112, Final Batch Loss: 0.10113977640867233\n",
      "Epoch 4029, Loss: 0.2435930296778679, Final Batch Loss: 0.08966400474309921\n",
      "Epoch 4030, Loss: 0.24705450981855392, Final Batch Loss: 0.1356169581413269\n",
      "Epoch 4031, Loss: 0.25818128138780594, Final Batch Loss: 0.11489491909742355\n",
      "Epoch 4032, Loss: 0.2696888819336891, Final Batch Loss: 0.08800772577524185\n",
      "Epoch 4033, Loss: 0.28520655632019043, Final Batch Loss: 0.17050831019878387\n",
      "Epoch 4034, Loss: 0.27575647830963135, Final Batch Loss: 0.1439061164855957\n",
      "Epoch 4035, Loss: 0.26573237031698227, Final Batch Loss: 0.16868509352207184\n",
      "Epoch 4036, Loss: 0.26477760076522827, Final Batch Loss: 0.1347043365240097\n",
      "Epoch 4037, Loss: 0.2779206782579422, Final Batch Loss: 0.18869121372699738\n",
      "Epoch 4038, Loss: 0.28722594678401947, Final Batch Loss: 0.13524788618087769\n",
      "Epoch 4039, Loss: 0.3009064793586731, Final Batch Loss: 0.1527828723192215\n",
      "Epoch 4040, Loss: 0.2894420251250267, Final Batch Loss: 0.1669038087129593\n",
      "Epoch 4041, Loss: 0.24128668010234833, Final Batch Loss: 0.12823200225830078\n",
      "Epoch 4042, Loss: 0.28718598186969757, Final Batch Loss: 0.17065192759037018\n",
      "Epoch 4043, Loss: 0.25896555185317993, Final Batch Loss: 0.13753077387809753\n",
      "Epoch 4044, Loss: 0.2910993844270706, Final Batch Loss: 0.13333216309547424\n",
      "Epoch 4045, Loss: 0.2509426325559616, Final Batch Loss: 0.10986402630805969\n",
      "Epoch 4046, Loss: 0.28368306159973145, Final Batch Loss: 0.1345454752445221\n",
      "Epoch 4047, Loss: 0.23269910365343094, Final Batch Loss: 0.13110016286373138\n",
      "Epoch 4048, Loss: 0.2796270623803139, Final Batch Loss: 0.11819133907556534\n",
      "Epoch 4049, Loss: 0.2520972937345505, Final Batch Loss: 0.1019170880317688\n",
      "Epoch 4050, Loss: 0.24216966331005096, Final Batch Loss: 0.08239565789699554\n",
      "Epoch 4051, Loss: 0.24816983193159103, Final Batch Loss: 0.11679645627737045\n",
      "Epoch 4052, Loss: 0.2565947473049164, Final Batch Loss: 0.10492289066314697\n",
      "Epoch 4053, Loss: 0.2785957604646683, Final Batch Loss: 0.1302114576101303\n",
      "Epoch 4054, Loss: 0.25169282406568527, Final Batch Loss: 0.1243635043501854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4055, Loss: 0.2839047610759735, Final Batch Loss: 0.12870419025421143\n",
      "Epoch 4056, Loss: 0.24769695848226547, Final Batch Loss: 0.07326307147741318\n",
      "Epoch 4057, Loss: 0.3086165189743042, Final Batch Loss: 0.15306751430034637\n",
      "Epoch 4058, Loss: 0.28691431879997253, Final Batch Loss: 0.1276114284992218\n",
      "Epoch 4059, Loss: 0.2315862476825714, Final Batch Loss: 0.0945262610912323\n",
      "Epoch 4060, Loss: 0.24289488047361374, Final Batch Loss: 0.11437579244375229\n",
      "Epoch 4061, Loss: 0.22845641523599625, Final Batch Loss: 0.07535187155008316\n",
      "Epoch 4062, Loss: 0.2300715297460556, Final Batch Loss: 0.10667072981595993\n",
      "Epoch 4063, Loss: 0.27549563348293304, Final Batch Loss: 0.131919264793396\n",
      "Epoch 4064, Loss: 0.2151421755552292, Final Batch Loss: 0.10016149282455444\n",
      "Epoch 4065, Loss: 0.23492542654275894, Final Batch Loss: 0.1157994344830513\n",
      "Epoch 4066, Loss: 0.312828853726387, Final Batch Loss: 0.1871863156557083\n",
      "Epoch 4067, Loss: 0.27289576828479767, Final Batch Loss: 0.1291361004114151\n",
      "Epoch 4068, Loss: 0.26274028420448303, Final Batch Loss: 0.138824924826622\n",
      "Epoch 4069, Loss: 0.2715258151292801, Final Batch Loss: 0.14927098155021667\n",
      "Epoch 4070, Loss: 0.31172408163547516, Final Batch Loss: 0.15725070238113403\n",
      "Epoch 4071, Loss: 0.24774254113435745, Final Batch Loss: 0.12558920681476593\n",
      "Epoch 4072, Loss: 0.24735302478075027, Final Batch Loss: 0.13312698900699615\n",
      "Epoch 4073, Loss: 0.22665954381227493, Final Batch Loss: 0.11543092876672745\n",
      "Epoch 4074, Loss: 0.2675614506006241, Final Batch Loss: 0.1271200031042099\n",
      "Epoch 4075, Loss: 0.2135309800505638, Final Batch Loss: 0.09755270928144455\n",
      "Epoch 4076, Loss: 0.2485676184296608, Final Batch Loss: 0.11613165587186813\n",
      "Epoch 4077, Loss: 0.25074490904808044, Final Batch Loss: 0.11151531338691711\n",
      "Epoch 4078, Loss: 0.26318359375, Final Batch Loss: 0.14148138463497162\n",
      "Epoch 4079, Loss: 0.2238435372710228, Final Batch Loss: 0.08902490884065628\n",
      "Epoch 4080, Loss: 0.21401985734701157, Final Batch Loss: 0.11647500842809677\n",
      "Epoch 4081, Loss: 0.27014297246932983, Final Batch Loss: 0.1337171345949173\n",
      "Epoch 4082, Loss: 0.23783592879772186, Final Batch Loss: 0.10255670547485352\n",
      "Epoch 4083, Loss: 0.21330797672271729, Final Batch Loss: 0.09287010878324509\n",
      "Epoch 4084, Loss: 0.2467103749513626, Final Batch Loss: 0.14194421470165253\n",
      "Epoch 4085, Loss: 0.24260538071393967, Final Batch Loss: 0.12310329079627991\n",
      "Epoch 4086, Loss: 0.2533629611134529, Final Batch Loss: 0.1377217322587967\n",
      "Epoch 4087, Loss: 0.20582353323698044, Final Batch Loss: 0.07563754171133041\n",
      "Epoch 4088, Loss: 0.3777519017457962, Final Batch Loss: 0.17449747025966644\n",
      "Epoch 4089, Loss: 0.2622964456677437, Final Batch Loss: 0.1519133299589157\n",
      "Epoch 4090, Loss: 0.2882668077945709, Final Batch Loss: 0.14588941633701324\n",
      "Epoch 4091, Loss: 0.2660617306828499, Final Batch Loss: 0.14261788129806519\n",
      "Epoch 4092, Loss: 0.2681846097111702, Final Batch Loss: 0.12023638933897018\n",
      "Epoch 4093, Loss: 0.2713984549045563, Final Batch Loss: 0.11592364311218262\n",
      "Epoch 4094, Loss: 0.28703999519348145, Final Batch Loss: 0.15157479047775269\n",
      "Epoch 4095, Loss: 0.28873616456985474, Final Batch Loss: 0.12873615324497223\n",
      "Epoch 4096, Loss: 0.22184275090694427, Final Batch Loss: 0.10597486048936844\n",
      "Epoch 4097, Loss: 0.31061114370822906, Final Batch Loss: 0.18221932649612427\n",
      "Epoch 4098, Loss: 0.23375048488378525, Final Batch Loss: 0.1174941435456276\n",
      "Epoch 4099, Loss: 0.31871140003204346, Final Batch Loss: 0.1927875131368637\n",
      "Epoch 4100, Loss: 0.33274412155151367, Final Batch Loss: 0.1571095585823059\n",
      "Epoch 4101, Loss: 0.2936171665787697, Final Batch Loss: 0.12020718306303024\n",
      "Epoch 4102, Loss: 0.2732256054878235, Final Batch Loss: 0.14670909941196442\n",
      "Epoch 4103, Loss: 0.24647144228219986, Final Batch Loss: 0.1398976892232895\n",
      "Epoch 4104, Loss: 0.25943567603826523, Final Batch Loss: 0.1173195019364357\n",
      "Epoch 4105, Loss: 0.276417076587677, Final Batch Loss: 0.14856982231140137\n",
      "Epoch 4106, Loss: 0.2550588771700859, Final Batch Loss: 0.14921031892299652\n",
      "Epoch 4107, Loss: 0.30738992989063263, Final Batch Loss: 0.16319473087787628\n",
      "Epoch 4108, Loss: 0.2630695775151253, Final Batch Loss: 0.11897524446249008\n",
      "Epoch 4109, Loss: 0.29498089849948883, Final Batch Loss: 0.16432438790798187\n",
      "Epoch 4110, Loss: 0.2580413743853569, Final Batch Loss: 0.1341230720281601\n",
      "Epoch 4111, Loss: 0.2782316654920578, Final Batch Loss: 0.1715688854455948\n",
      "Epoch 4112, Loss: 0.2807805687189102, Final Batch Loss: 0.1515551060438156\n",
      "Epoch 4113, Loss: 0.2720816060900688, Final Batch Loss: 0.15865491330623627\n",
      "Epoch 4114, Loss: 0.2823372334241867, Final Batch Loss: 0.16576625406742096\n",
      "Epoch 4115, Loss: 0.23887818306684494, Final Batch Loss: 0.10985755175352097\n",
      "Epoch 4116, Loss: 0.27777890861034393, Final Batch Loss: 0.15048030018806458\n",
      "Epoch 4117, Loss: 0.24338576197624207, Final Batch Loss: 0.08655151724815369\n",
      "Epoch 4118, Loss: 0.26375459134578705, Final Batch Loss: 0.12566715478897095\n",
      "Epoch 4119, Loss: 0.235512413084507, Final Batch Loss: 0.12744396924972534\n",
      "Epoch 4120, Loss: 0.2812986969947815, Final Batch Loss: 0.12819121778011322\n",
      "Epoch 4121, Loss: 0.22994975745677948, Final Batch Loss: 0.1354674994945526\n",
      "Epoch 4122, Loss: 0.26468542218208313, Final Batch Loss: 0.127471461892128\n",
      "Epoch 4123, Loss: 0.26866716146469116, Final Batch Loss: 0.13330405950546265\n",
      "Epoch 4124, Loss: 0.2459489405155182, Final Batch Loss: 0.10463646054267883\n",
      "Epoch 4125, Loss: 0.24966704100370407, Final Batch Loss: 0.1275912970304489\n",
      "Epoch 4126, Loss: 0.2721871957182884, Final Batch Loss: 0.15409308671951294\n",
      "Epoch 4127, Loss: 0.24413186311721802, Final Batch Loss: 0.12724222242832184\n",
      "Epoch 4128, Loss: 0.2386964038014412, Final Batch Loss: 0.13385450839996338\n",
      "Epoch 4129, Loss: 0.25572216510772705, Final Batch Loss: 0.12621058523654938\n",
      "Epoch 4130, Loss: 0.29102765023708344, Final Batch Loss: 0.13448578119277954\n",
      "Epoch 4131, Loss: 0.2682678923010826, Final Batch Loss: 0.12258733063936234\n",
      "Epoch 4132, Loss: 0.27339692413806915, Final Batch Loss: 0.15812966227531433\n",
      "Epoch 4133, Loss: 0.2672269865870476, Final Batch Loss: 0.1194116547703743\n",
      "Epoch 4134, Loss: 0.26725295186042786, Final Batch Loss: 0.1238347589969635\n",
      "Epoch 4135, Loss: 0.27100086957216263, Final Batch Loss: 0.14786434173583984\n",
      "Epoch 4136, Loss: 0.22876030951738358, Final Batch Loss: 0.09105560928583145\n",
      "Epoch 4137, Loss: 0.21805734187364578, Final Batch Loss: 0.0830434188246727\n",
      "Epoch 4138, Loss: 0.24106691032648087, Final Batch Loss: 0.0978131964802742\n",
      "Epoch 4139, Loss: 0.2289363220334053, Final Batch Loss: 0.13106757402420044\n",
      "Epoch 4140, Loss: 0.30716174840927124, Final Batch Loss: 0.20167666673660278\n",
      "Epoch 4141, Loss: 0.24081260710954666, Final Batch Loss: 0.10270491987466812\n",
      "Epoch 4142, Loss: 0.23653221130371094, Final Batch Loss: 0.11460349708795547\n",
      "Epoch 4143, Loss: 0.2380925640463829, Final Batch Loss: 0.12336921691894531\n",
      "Epoch 4144, Loss: 0.27358822524547577, Final Batch Loss: 0.15591008961200714\n",
      "Epoch 4145, Loss: 0.2752310335636139, Final Batch Loss: 0.15013590455055237\n",
      "Epoch 4146, Loss: 0.2419757917523384, Final Batch Loss: 0.1421414464712143\n",
      "Epoch 4147, Loss: 0.28806477785110474, Final Batch Loss: 0.13540105521678925\n",
      "Epoch 4148, Loss: 0.2700875774025917, Final Batch Loss: 0.1705636829137802\n",
      "Epoch 4149, Loss: 0.22960566729307175, Final Batch Loss: 0.0849783644080162\n",
      "Epoch 4150, Loss: 0.23108059912919998, Final Batch Loss: 0.09763584285974503\n",
      "Epoch 4151, Loss: 0.3225868344306946, Final Batch Loss: 0.22168457508087158\n",
      "Epoch 4152, Loss: 0.23241886496543884, Final Batch Loss: 0.09526404738426208\n",
      "Epoch 4153, Loss: 0.2555171102285385, Final Batch Loss: 0.08462446928024292\n",
      "Epoch 4154, Loss: 0.22969526052474976, Final Batch Loss: 0.10425543785095215\n",
      "Epoch 4155, Loss: 0.26068010926246643, Final Batch Loss: 0.15188200771808624\n",
      "Epoch 4156, Loss: 0.24114421755075455, Final Batch Loss: 0.10347547382116318\n",
      "Epoch 4157, Loss: 0.2532660886645317, Final Batch Loss: 0.13065430521965027\n",
      "Epoch 4158, Loss: 0.2561865821480751, Final Batch Loss: 0.15434931218624115\n",
      "Epoch 4159, Loss: 0.23425788432359695, Final Batch Loss: 0.12667517364025116\n",
      "Epoch 4160, Loss: 0.21389511227607727, Final Batch Loss: 0.0962577760219574\n",
      "Epoch 4161, Loss: 0.3015967905521393, Final Batch Loss: 0.17578597366809845\n",
      "Epoch 4162, Loss: 0.2612614631652832, Final Batch Loss: 0.11107528209686279\n",
      "Epoch 4163, Loss: 0.2817547395825386, Final Batch Loss: 0.10689341276884079\n",
      "Epoch 4164, Loss: 0.28657278418540955, Final Batch Loss: 0.1397169679403305\n",
      "Epoch 4165, Loss: 0.27824273705482483, Final Batch Loss: 0.12879636883735657\n",
      "Epoch 4166, Loss: 0.26334989815950394, Final Batch Loss: 0.09308924525976181\n",
      "Epoch 4167, Loss: 0.2378605380654335, Final Batch Loss: 0.12107677012681961\n",
      "Epoch 4168, Loss: 0.25669456273317337, Final Batch Loss: 0.1206396147608757\n",
      "Epoch 4169, Loss: 0.24882043153047562, Final Batch Loss: 0.09556937962770462\n",
      "Epoch 4170, Loss: 0.28710125386714935, Final Batch Loss: 0.1620815545320511\n",
      "Epoch 4171, Loss: 0.2809479832649231, Final Batch Loss: 0.14711016416549683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4172, Loss: 0.2817782014608383, Final Batch Loss: 0.1854090690612793\n",
      "Epoch 4173, Loss: 0.26660995185375214, Final Batch Loss: 0.13199482858181\n",
      "Epoch 4174, Loss: 0.24628185480833054, Final Batch Loss: 0.13350345194339752\n",
      "Epoch 4175, Loss: 0.2727818712592125, Final Batch Loss: 0.09868844598531723\n",
      "Epoch 4176, Loss: 0.28779707849025726, Final Batch Loss: 0.14569926261901855\n",
      "Epoch 4177, Loss: 0.2538537010550499, Final Batch Loss: 0.09569155424833298\n",
      "Epoch 4178, Loss: 0.23921450972557068, Final Batch Loss: 0.12900963425636292\n",
      "Epoch 4179, Loss: 0.27681468427181244, Final Batch Loss: 0.16068215668201447\n",
      "Epoch 4180, Loss: 0.2917560338973999, Final Batch Loss: 0.13175739347934723\n",
      "Epoch 4181, Loss: 0.2081793025135994, Final Batch Loss: 0.0879010483622551\n",
      "Epoch 4182, Loss: 0.25245460867881775, Final Batch Loss: 0.13303324580192566\n",
      "Epoch 4183, Loss: 0.26832062751054764, Final Batch Loss: 0.14810489118099213\n",
      "Epoch 4184, Loss: 0.23658955097198486, Final Batch Loss: 0.11429087072610855\n",
      "Epoch 4185, Loss: 0.2928740084171295, Final Batch Loss: 0.13904409110546112\n",
      "Epoch 4186, Loss: 0.2622210830450058, Final Batch Loss: 0.1396845579147339\n",
      "Epoch 4187, Loss: 0.2712462544441223, Final Batch Loss: 0.12756334245204926\n",
      "Epoch 4188, Loss: 0.23663470894098282, Final Batch Loss: 0.0862068310379982\n",
      "Epoch 4189, Loss: 0.2654123157262802, Final Batch Loss: 0.14923690259456635\n",
      "Epoch 4190, Loss: 0.2092009335756302, Final Batch Loss: 0.10099390894174576\n",
      "Epoch 4191, Loss: 0.26274891942739487, Final Batch Loss: 0.12063028663396835\n",
      "Epoch 4192, Loss: 0.29140448570251465, Final Batch Loss: 0.13827304542064667\n",
      "Epoch 4193, Loss: 0.22006142884492874, Final Batch Loss: 0.09627213329076767\n",
      "Epoch 4194, Loss: 0.2652128040790558, Final Batch Loss: 0.13246093690395355\n",
      "Epoch 4195, Loss: 0.23354991525411606, Final Batch Loss: 0.09588216990232468\n",
      "Epoch 4196, Loss: 0.24645595997571945, Final Batch Loss: 0.11864746361970901\n",
      "Epoch 4197, Loss: 0.24595046788454056, Final Batch Loss: 0.15836693346500397\n",
      "Epoch 4198, Loss: 0.2682589814066887, Final Batch Loss: 0.17704808712005615\n",
      "Epoch 4199, Loss: 0.26738952845335007, Final Batch Loss: 0.12362813204526901\n",
      "Epoch 4200, Loss: 0.23475851118564606, Final Batch Loss: 0.09757208824157715\n",
      "Epoch 4201, Loss: 0.2902255803346634, Final Batch Loss: 0.15352872014045715\n",
      "Epoch 4202, Loss: 0.24293942004442215, Final Batch Loss: 0.1319911926984787\n",
      "Epoch 4203, Loss: 0.25534795224666595, Final Batch Loss: 0.1322399079799652\n",
      "Epoch 4204, Loss: 0.25636500120162964, Final Batch Loss: 0.13144874572753906\n",
      "Epoch 4205, Loss: 0.2734322026371956, Final Batch Loss: 0.1141408309340477\n",
      "Epoch 4206, Loss: 0.28841179609298706, Final Batch Loss: 0.1507582664489746\n",
      "Epoch 4207, Loss: 0.27703502774238586, Final Batch Loss: 0.1663483828306198\n",
      "Epoch 4208, Loss: 0.26354503631591797, Final Batch Loss: 0.14105428755283356\n",
      "Epoch 4209, Loss: 0.2553676813840866, Final Batch Loss: 0.1501314491033554\n",
      "Epoch 4210, Loss: 0.25239454954862595, Final Batch Loss: 0.12158631533384323\n",
      "Epoch 4211, Loss: 0.22394341230392456, Final Batch Loss: 0.12175813317298889\n",
      "Epoch 4212, Loss: 0.26995670795440674, Final Batch Loss: 0.10518059134483337\n",
      "Epoch 4213, Loss: 0.23433582484722137, Final Batch Loss: 0.0981849730014801\n",
      "Epoch 4214, Loss: 0.2648605927824974, Final Batch Loss: 0.11353237181901932\n",
      "Epoch 4215, Loss: 0.2614545226097107, Final Batch Loss: 0.13371583819389343\n",
      "Epoch 4216, Loss: 0.26808295398950577, Final Batch Loss: 0.1467781364917755\n",
      "Epoch 4217, Loss: 0.2591496482491493, Final Batch Loss: 0.15904943645000458\n",
      "Epoch 4218, Loss: 0.24247821420431137, Final Batch Loss: 0.11775095015764236\n",
      "Epoch 4219, Loss: 0.20297619700431824, Final Batch Loss: 0.09962523728609085\n",
      "Epoch 4220, Loss: 0.25426750630140305, Final Batch Loss: 0.08902422338724136\n",
      "Epoch 4221, Loss: 0.2183130383491516, Final Batch Loss: 0.10574141889810562\n",
      "Epoch 4222, Loss: 0.2673230320215225, Final Batch Loss: 0.1432267278432846\n",
      "Epoch 4223, Loss: 0.2316345050930977, Final Batch Loss: 0.11415813118219376\n",
      "Epoch 4224, Loss: 0.26103171706199646, Final Batch Loss: 0.11074751615524292\n",
      "Epoch 4225, Loss: 0.2562271058559418, Final Batch Loss: 0.1265278309583664\n",
      "Epoch 4226, Loss: 0.2911689281463623, Final Batch Loss: 0.1390494853258133\n",
      "Epoch 4227, Loss: 0.2576426640152931, Final Batch Loss: 0.14214640855789185\n",
      "Epoch 4228, Loss: 0.24941226840019226, Final Batch Loss: 0.12004882097244263\n",
      "Epoch 4229, Loss: 0.2126108482480049, Final Batch Loss: 0.09408103674650192\n",
      "Epoch 4230, Loss: 0.2444377765059471, Final Batch Loss: 0.1265287548303604\n",
      "Epoch 4231, Loss: 0.2316831648349762, Final Batch Loss: 0.10765131562948227\n",
      "Epoch 4232, Loss: 0.30880436301231384, Final Batch Loss: 0.15018735826015472\n",
      "Epoch 4233, Loss: 0.23653757572174072, Final Batch Loss: 0.1309843212366104\n",
      "Epoch 4234, Loss: 0.25535329431295395, Final Batch Loss: 0.15082873404026031\n",
      "Epoch 4235, Loss: 0.27731625735759735, Final Batch Loss: 0.1436341553926468\n",
      "Epoch 4236, Loss: 0.21554698795080185, Final Batch Loss: 0.0902370885014534\n",
      "Epoch 4237, Loss: 0.25043272227048874, Final Batch Loss: 0.10769050568342209\n",
      "Epoch 4238, Loss: 0.2681776136159897, Final Batch Loss: 0.12699969112873077\n",
      "Epoch 4239, Loss: 0.2581097334623337, Final Batch Loss: 0.14162902534008026\n",
      "Epoch 4240, Loss: 0.27949778735637665, Final Batch Loss: 0.1379127949476242\n",
      "Epoch 4241, Loss: 0.2354276180267334, Final Batch Loss: 0.11918151378631592\n",
      "Epoch 4242, Loss: 0.2542271837592125, Final Batch Loss: 0.11162006109952927\n",
      "Epoch 4243, Loss: 0.2294057235121727, Final Batch Loss: 0.12085498124361038\n",
      "Epoch 4244, Loss: 0.2637503892183304, Final Batch Loss: 0.16706563532352448\n",
      "Epoch 4245, Loss: 0.2748241722583771, Final Batch Loss: 0.1481066793203354\n",
      "Epoch 4246, Loss: 0.2541675865650177, Final Batch Loss: 0.1364707499742508\n",
      "Epoch 4247, Loss: 0.23569487035274506, Final Batch Loss: 0.07484187185764313\n",
      "Epoch 4248, Loss: 0.2368961051106453, Final Batch Loss: 0.10272128134965897\n",
      "Epoch 4249, Loss: 0.26430363953113556, Final Batch Loss: 0.13154949247837067\n",
      "Epoch 4250, Loss: 0.20704329758882523, Final Batch Loss: 0.09012624621391296\n",
      "Epoch 4251, Loss: 0.2318989410996437, Final Batch Loss: 0.12452053278684616\n",
      "Epoch 4252, Loss: 0.2236201986670494, Final Batch Loss: 0.0910467877984047\n",
      "Epoch 4253, Loss: 0.24790921807289124, Final Batch Loss: 0.13016168773174286\n",
      "Epoch 4254, Loss: 0.24476146697998047, Final Batch Loss: 0.15180879831314087\n",
      "Epoch 4255, Loss: 0.23395632952451706, Final Batch Loss: 0.10738346725702286\n",
      "Epoch 4256, Loss: 0.24257739633321762, Final Batch Loss: 0.11596276611089706\n",
      "Epoch 4257, Loss: 0.22328506410121918, Final Batch Loss: 0.08621156215667725\n",
      "Epoch 4258, Loss: 0.25310827046632767, Final Batch Loss: 0.1320071667432785\n",
      "Epoch 4259, Loss: 0.2717200890183449, Final Batch Loss: 0.1079404279589653\n",
      "Epoch 4260, Loss: 0.24358141422271729, Final Batch Loss: 0.13868655264377594\n",
      "Epoch 4261, Loss: 0.25799161195755005, Final Batch Loss: 0.13084565103054047\n",
      "Epoch 4262, Loss: 0.21974853426218033, Final Batch Loss: 0.1208311915397644\n",
      "Epoch 4263, Loss: 0.22820470482110977, Final Batch Loss: 0.10803371667861938\n",
      "Epoch 4264, Loss: 0.2804289162158966, Final Batch Loss: 0.13581231236457825\n",
      "Epoch 4265, Loss: 0.2510058581829071, Final Batch Loss: 0.13349805772304535\n",
      "Epoch 4266, Loss: 0.26607829332351685, Final Batch Loss: 0.17642100155353546\n",
      "Epoch 4267, Loss: 0.312657967209816, Final Batch Loss: 0.18680702149868011\n",
      "Epoch 4268, Loss: 0.2434801682829857, Final Batch Loss: 0.11893158406019211\n",
      "Epoch 4269, Loss: 0.24558033794164658, Final Batch Loss: 0.14932109415531158\n",
      "Epoch 4270, Loss: 0.2913835793733597, Final Batch Loss: 0.18140125274658203\n",
      "Epoch 4271, Loss: 0.3012402206659317, Final Batch Loss: 0.15811516344547272\n",
      "Epoch 4272, Loss: 0.35945944488048553, Final Batch Loss: 0.23393498361110687\n",
      "Epoch 4273, Loss: 0.25540490448474884, Final Batch Loss: 0.14811263978481293\n",
      "Epoch 4274, Loss: 0.2377081960439682, Final Batch Loss: 0.11229771375656128\n",
      "Epoch 4275, Loss: 0.29386909306049347, Final Batch Loss: 0.14993111789226532\n",
      "Epoch 4276, Loss: 0.2821202278137207, Final Batch Loss: 0.1374911516904831\n",
      "Epoch 4277, Loss: 0.285965271294117, Final Batch Loss: 0.12150988727807999\n",
      "Epoch 4278, Loss: 0.2754175364971161, Final Batch Loss: 0.13391366600990295\n",
      "Epoch 4279, Loss: 0.3414246588945389, Final Batch Loss: 0.16177251935005188\n",
      "Epoch 4280, Loss: 0.313097283244133, Final Batch Loss: 0.15056101977825165\n",
      "Epoch 4281, Loss: 0.26752588897943497, Final Batch Loss: 0.1060049757361412\n",
      "Epoch 4282, Loss: 0.22396808862686157, Final Batch Loss: 0.1423216313123703\n",
      "Epoch 4283, Loss: 0.2510252818465233, Final Batch Loss: 0.13558518886566162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4284, Loss: 0.262256920337677, Final Batch Loss: 0.11717447638511658\n",
      "Epoch 4285, Loss: 0.2959897071123123, Final Batch Loss: 0.16313877701759338\n",
      "Epoch 4286, Loss: 0.2703799903392792, Final Batch Loss: 0.13467462360858917\n",
      "Epoch 4287, Loss: 0.2736632823944092, Final Batch Loss: 0.13369469344615936\n",
      "Epoch 4288, Loss: 0.2854647636413574, Final Batch Loss: 0.1497514545917511\n",
      "Epoch 4289, Loss: 0.25910257548093796, Final Batch Loss: 0.14154976606369019\n",
      "Epoch 4290, Loss: 0.2401929274201393, Final Batch Loss: 0.12163975089788437\n",
      "Epoch 4291, Loss: 0.24825195968151093, Final Batch Loss: 0.13366229832172394\n",
      "Epoch 4292, Loss: 0.23958852142095566, Final Batch Loss: 0.11937189102172852\n",
      "Epoch 4293, Loss: 0.2532902956008911, Final Batch Loss: 0.10597968101501465\n",
      "Epoch 4294, Loss: 0.23198774456977844, Final Batch Loss: 0.11381521821022034\n",
      "Epoch 4295, Loss: 0.2766190841794014, Final Batch Loss: 0.16268275678157806\n",
      "Epoch 4296, Loss: 0.2436361387372017, Final Batch Loss: 0.11298330873250961\n",
      "Epoch 4297, Loss: 0.23473871499300003, Final Batch Loss: 0.10737533122301102\n",
      "Epoch 4298, Loss: 0.2265237718820572, Final Batch Loss: 0.12846185266971588\n",
      "Epoch 4299, Loss: 0.24871369451284409, Final Batch Loss: 0.11272928863763809\n",
      "Epoch 4300, Loss: 0.2403636872768402, Final Batch Loss: 0.11497899889945984\n",
      "Epoch 4301, Loss: 0.25665681064128876, Final Batch Loss: 0.12544472515583038\n",
      "Epoch 4302, Loss: 0.2373102381825447, Final Batch Loss: 0.1343224048614502\n",
      "Epoch 4303, Loss: 0.21864285320043564, Final Batch Loss: 0.10264775156974792\n",
      "Epoch 4304, Loss: 0.25446517765522003, Final Batch Loss: 0.1215067207813263\n",
      "Epoch 4305, Loss: 0.2104838788509369, Final Batch Loss: 0.09114369004964828\n",
      "Epoch 4306, Loss: 0.21631967276334763, Final Batch Loss: 0.10068613290786743\n",
      "Epoch 4307, Loss: 0.2342456430196762, Final Batch Loss: 0.11725582927465439\n",
      "Epoch 4308, Loss: 0.2569912448525429, Final Batch Loss: 0.11384708434343338\n",
      "Epoch 4309, Loss: 0.22265645116567612, Final Batch Loss: 0.1206982359290123\n",
      "Epoch 4310, Loss: 0.2453763484954834, Final Batch Loss: 0.11934873461723328\n",
      "Epoch 4311, Loss: 0.277676522731781, Final Batch Loss: 0.1565534621477127\n",
      "Epoch 4312, Loss: 0.2610001415014267, Final Batch Loss: 0.13682697713375092\n",
      "Epoch 4313, Loss: 0.25699513405561447, Final Batch Loss: 0.14105620980262756\n",
      "Epoch 4314, Loss: 0.27455705404281616, Final Batch Loss: 0.1710958480834961\n",
      "Epoch 4315, Loss: 0.24105772376060486, Final Batch Loss: 0.11222180724143982\n",
      "Epoch 4316, Loss: 0.27616630494594574, Final Batch Loss: 0.16176865994930267\n",
      "Epoch 4317, Loss: 0.2184884324669838, Final Batch Loss: 0.11728889495134354\n",
      "Epoch 4318, Loss: 0.2275191769003868, Final Batch Loss: 0.1122036948800087\n",
      "Epoch 4319, Loss: 0.23180942237377167, Final Batch Loss: 0.10270300507545471\n",
      "Epoch 4320, Loss: 0.22958216816186905, Final Batch Loss: 0.12615671753883362\n",
      "Epoch 4321, Loss: 0.2890793904662132, Final Batch Loss: 0.10432439297437668\n",
      "Epoch 4322, Loss: 0.24776462465524673, Final Batch Loss: 0.14149193465709686\n",
      "Epoch 4323, Loss: 0.25973112136125565, Final Batch Loss: 0.11947337538003922\n",
      "Epoch 4324, Loss: 0.2580421343445778, Final Batch Loss: 0.1353125125169754\n",
      "Epoch 4325, Loss: 0.22714131325483322, Final Batch Loss: 0.0996871218085289\n",
      "Epoch 4326, Loss: 0.2424086332321167, Final Batch Loss: 0.11218205094337463\n",
      "Epoch 4327, Loss: 0.22867770493030548, Final Batch Loss: 0.10749530047178268\n",
      "Epoch 4328, Loss: 0.22132251411676407, Final Batch Loss: 0.10986211895942688\n",
      "Epoch 4329, Loss: 0.24844380468130112, Final Batch Loss: 0.14390988647937775\n",
      "Epoch 4330, Loss: 0.23343654721975327, Final Batch Loss: 0.1315428763628006\n",
      "Epoch 4331, Loss: 0.2710094302892685, Final Batch Loss: 0.15435580909252167\n",
      "Epoch 4332, Loss: 0.28929993510246277, Final Batch Loss: 0.1814250499010086\n",
      "Epoch 4333, Loss: 0.2522877976298332, Final Batch Loss: 0.10302843898534775\n",
      "Epoch 4334, Loss: 0.23257750272750854, Final Batch Loss: 0.14611253142356873\n",
      "Epoch 4335, Loss: 0.2854287624359131, Final Batch Loss: 0.1625225991010666\n",
      "Epoch 4336, Loss: 0.258682020008564, Final Batch Loss: 0.14207643270492554\n",
      "Epoch 4337, Loss: 0.2669822424650192, Final Batch Loss: 0.11092010140419006\n",
      "Epoch 4338, Loss: 0.22060102969408035, Final Batch Loss: 0.11759021133184433\n",
      "Epoch 4339, Loss: 0.2357902005314827, Final Batch Loss: 0.09416768699884415\n",
      "Epoch 4340, Loss: 0.2365420162677765, Final Batch Loss: 0.11296331882476807\n",
      "Epoch 4341, Loss: 0.22797290235757828, Final Batch Loss: 0.12736672163009644\n",
      "Epoch 4342, Loss: 0.2814846485853195, Final Batch Loss: 0.13068395853042603\n",
      "Epoch 4343, Loss: 0.2707478702068329, Final Batch Loss: 0.13548539578914642\n",
      "Epoch 4344, Loss: 0.23883656412363052, Final Batch Loss: 0.10527657717466354\n",
      "Epoch 4345, Loss: 0.2941630557179451, Final Batch Loss: 0.16996711492538452\n",
      "Epoch 4346, Loss: 0.22026150673627853, Final Batch Loss: 0.11718922108411789\n",
      "Epoch 4347, Loss: 0.3045225441455841, Final Batch Loss: 0.13736391067504883\n",
      "Epoch 4348, Loss: 0.24163856357336044, Final Batch Loss: 0.10228408128023148\n",
      "Epoch 4349, Loss: 0.23758644610643387, Final Batch Loss: 0.12296552211046219\n",
      "Epoch 4350, Loss: 0.2512644827365875, Final Batch Loss: 0.09734311699867249\n",
      "Epoch 4351, Loss: 0.2585790082812309, Final Batch Loss: 0.11520352214574814\n",
      "Epoch 4352, Loss: 0.288226455450058, Final Batch Loss: 0.17704181373119354\n",
      "Epoch 4353, Loss: 0.2467552199959755, Final Batch Loss: 0.11758876591920853\n",
      "Epoch 4354, Loss: 0.2760515585541725, Final Batch Loss: 0.11322563141584396\n",
      "Epoch 4355, Loss: 0.2350328490138054, Final Batch Loss: 0.11812572926282883\n",
      "Epoch 4356, Loss: 0.26107776165008545, Final Batch Loss: 0.12937764823436737\n",
      "Epoch 4357, Loss: 0.2662501707673073, Final Batch Loss: 0.11704244464635849\n",
      "Epoch 4358, Loss: 0.21913132816553116, Final Batch Loss: 0.12430364638566971\n",
      "Epoch 4359, Loss: 0.24588536471128464, Final Batch Loss: 0.13139547407627106\n",
      "Epoch 4360, Loss: 0.24449872225522995, Final Batch Loss: 0.0828302726149559\n",
      "Epoch 4361, Loss: 0.2933351844549179, Final Batch Loss: 0.15200074017047882\n",
      "Epoch 4362, Loss: 0.20942522585391998, Final Batch Loss: 0.10770142823457718\n",
      "Epoch 4363, Loss: 0.32253311574459076, Final Batch Loss: 0.1353592574596405\n",
      "Epoch 4364, Loss: 0.2797284349799156, Final Batch Loss: 0.10325749963521957\n",
      "Epoch 4365, Loss: 0.26344479620456696, Final Batch Loss: 0.12631171941757202\n",
      "Epoch 4366, Loss: 0.2202049195766449, Final Batch Loss: 0.10620266199111938\n",
      "Epoch 4367, Loss: 0.24281779676675797, Final Batch Loss: 0.13649138808250427\n",
      "Epoch 4368, Loss: 0.2811076268553734, Final Batch Loss: 0.10966282337903976\n",
      "Epoch 4369, Loss: 0.2692412808537483, Final Batch Loss: 0.15089230239391327\n",
      "Epoch 4370, Loss: 0.30562634766101837, Final Batch Loss: 0.14725811779499054\n",
      "Epoch 4371, Loss: 0.2738885432481766, Final Batch Loss: 0.16413654386997223\n",
      "Epoch 4372, Loss: 0.26711519062519073, Final Batch Loss: 0.13751031458377838\n",
      "Epoch 4373, Loss: 0.27335893362760544, Final Batch Loss: 0.11338276416063309\n",
      "Epoch 4374, Loss: 0.23433386534452438, Final Batch Loss: 0.11739235371351242\n",
      "Epoch 4375, Loss: 0.2667207568883896, Final Batch Loss: 0.1274195909500122\n",
      "Epoch 4376, Loss: 0.23913193494081497, Final Batch Loss: 0.1129053458571434\n",
      "Epoch 4377, Loss: 0.3076806962490082, Final Batch Loss: 0.14604513347148895\n",
      "Epoch 4378, Loss: 0.2629588395357132, Final Batch Loss: 0.14055748283863068\n",
      "Epoch 4379, Loss: 0.28740455210208893, Final Batch Loss: 0.13887064158916473\n",
      "Epoch 4380, Loss: 0.24791379272937775, Final Batch Loss: 0.14531247317790985\n",
      "Epoch 4381, Loss: 0.22200898826122284, Final Batch Loss: 0.10825387388467789\n",
      "Epoch 4382, Loss: 0.2957996726036072, Final Batch Loss: 0.1427440494298935\n",
      "Epoch 4383, Loss: 0.24543672800064087, Final Batch Loss: 0.14083199203014374\n",
      "Epoch 4384, Loss: 0.2249796837568283, Final Batch Loss: 0.11803404241800308\n",
      "Epoch 4385, Loss: 0.23407956212759018, Final Batch Loss: 0.13244383037090302\n",
      "Epoch 4386, Loss: 0.2586820051074028, Final Batch Loss: 0.11091982573270798\n",
      "Epoch 4387, Loss: 0.2430037036538124, Final Batch Loss: 0.10904084891080856\n",
      "Epoch 4388, Loss: 0.2750217095017433, Final Batch Loss: 0.15418259799480438\n",
      "Epoch 4389, Loss: 0.262557215988636, Final Batch Loss: 0.15272526443004608\n",
      "Epoch 4390, Loss: 0.21677015721797943, Final Batch Loss: 0.12608672678470612\n",
      "Epoch 4391, Loss: 0.24117259681224823, Final Batch Loss: 0.13031476736068726\n",
      "Epoch 4392, Loss: 0.25069085508584976, Final Batch Loss: 0.13110534846782684\n",
      "Epoch 4393, Loss: 0.2547842487692833, Final Batch Loss: 0.10354066640138626\n",
      "Epoch 4394, Loss: 0.2927527129650116, Final Batch Loss: 0.15291251242160797\n",
      "Epoch 4395, Loss: 0.2574952021241188, Final Batch Loss: 0.14323043823242188\n",
      "Epoch 4396, Loss: 0.20612865686416626, Final Batch Loss: 0.0876488909125328\n",
      "Epoch 4397, Loss: 0.2401612475514412, Final Batch Loss: 0.09295297414064407\n",
      "Epoch 4398, Loss: 0.26168157160282135, Final Batch Loss: 0.12988163530826569\n",
      "Epoch 4399, Loss: 0.20710209012031555, Final Batch Loss: 0.07813520729541779\n",
      "Epoch 4400, Loss: 0.25819241255521774, Final Batch Loss: 0.17686671018600464\n",
      "Epoch 4401, Loss: 0.2595926597714424, Final Batch Loss: 0.08949064463376999\n",
      "Epoch 4402, Loss: 0.2689478248357773, Final Batch Loss: 0.1383034735918045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4403, Loss: 0.2456544116139412, Final Batch Loss: 0.12497228384017944\n",
      "Epoch 4404, Loss: 0.26019247621297836, Final Batch Loss: 0.13891705870628357\n",
      "Epoch 4405, Loss: 0.23880541324615479, Final Batch Loss: 0.1259046345949173\n",
      "Epoch 4406, Loss: 0.24637950956821442, Final Batch Loss: 0.1264597326517105\n",
      "Epoch 4407, Loss: 0.21868859231472015, Final Batch Loss: 0.09702331572771072\n",
      "Epoch 4408, Loss: 0.2315225601196289, Final Batch Loss: 0.12273978441953659\n",
      "Epoch 4409, Loss: 0.2618492841720581, Final Batch Loss: 0.12683340907096863\n",
      "Epoch 4410, Loss: 0.23212139308452606, Final Batch Loss: 0.14164994657039642\n",
      "Epoch 4411, Loss: 0.26060880720615387, Final Batch Loss: 0.12800371646881104\n",
      "Epoch 4412, Loss: 0.2476646676659584, Final Batch Loss: 0.10956501215696335\n",
      "Epoch 4413, Loss: 0.2329104170203209, Final Batch Loss: 0.11558429151773453\n",
      "Epoch 4414, Loss: 0.23514486849308014, Final Batch Loss: 0.09060177206993103\n",
      "Epoch 4415, Loss: 0.25647691637277603, Final Batch Loss: 0.10482605546712875\n",
      "Epoch 4416, Loss: 0.28139033168554306, Final Batch Loss: 0.09551963955163956\n",
      "Epoch 4417, Loss: 0.303558811545372, Final Batch Loss: 0.15381558239459991\n",
      "Epoch 4418, Loss: 0.25340191274881363, Final Batch Loss: 0.13423462212085724\n",
      "Epoch 4419, Loss: 0.2849794030189514, Final Batch Loss: 0.13945172727108002\n",
      "Epoch 4420, Loss: 0.22658736258745193, Final Batch Loss: 0.129302516579628\n",
      "Epoch 4421, Loss: 0.2684687674045563, Final Batch Loss: 0.14935949444770813\n",
      "Epoch 4422, Loss: 0.26319608837366104, Final Batch Loss: 0.14286832511425018\n",
      "Epoch 4423, Loss: 0.23171240836381912, Final Batch Loss: 0.10738382488489151\n",
      "Epoch 4424, Loss: 0.21450863033533096, Final Batch Loss: 0.10204135626554489\n",
      "Epoch 4425, Loss: 0.241323821246624, Final Batch Loss: 0.12535156309604645\n",
      "Epoch 4426, Loss: 0.24865803122520447, Final Batch Loss: 0.13389399647712708\n",
      "Epoch 4427, Loss: 0.22556832432746887, Final Batch Loss: 0.11155308037996292\n",
      "Epoch 4428, Loss: 0.2559855580329895, Final Batch Loss: 0.12007835507392883\n",
      "Epoch 4429, Loss: 0.22919071465730667, Final Batch Loss: 0.10232982784509659\n",
      "Epoch 4430, Loss: 0.2316405102610588, Final Batch Loss: 0.101537324488163\n",
      "Epoch 4431, Loss: 0.23651865869760513, Final Batch Loss: 0.13976804912090302\n",
      "Epoch 4432, Loss: 0.22513360530138016, Final Batch Loss: 0.11085904389619827\n",
      "Epoch 4433, Loss: 0.2135641649365425, Final Batch Loss: 0.07889560610055923\n",
      "Epoch 4434, Loss: 0.2659623995423317, Final Batch Loss: 0.14790207147598267\n",
      "Epoch 4435, Loss: 0.2247718647122383, Final Batch Loss: 0.10611612349748611\n",
      "Epoch 4436, Loss: 0.23642731457948685, Final Batch Loss: 0.11333926767110825\n",
      "Epoch 4437, Loss: 0.24669361859560013, Final Batch Loss: 0.12208646535873413\n",
      "Epoch 4438, Loss: 0.31516504287719727, Final Batch Loss: 0.17379368841648102\n",
      "Epoch 4439, Loss: 0.23843716830015182, Final Batch Loss: 0.09702717512845993\n",
      "Epoch 4440, Loss: 0.22793345153331757, Final Batch Loss: 0.10159879922866821\n",
      "Epoch 4441, Loss: 0.23430877178907394, Final Batch Loss: 0.12183187156915665\n",
      "Epoch 4442, Loss: 0.29421840608119965, Final Batch Loss: 0.1491393744945526\n",
      "Epoch 4443, Loss: 0.21237380802631378, Final Batch Loss: 0.09827235341072083\n",
      "Epoch 4444, Loss: 0.23944178968667984, Final Batch Loss: 0.10898662358522415\n",
      "Epoch 4445, Loss: 0.23049889504909515, Final Batch Loss: 0.11344171315431595\n",
      "Epoch 4446, Loss: 0.2949048727750778, Final Batch Loss: 0.1363784372806549\n",
      "Epoch 4447, Loss: 0.23482150584459305, Final Batch Loss: 0.09967336803674698\n",
      "Epoch 4448, Loss: 0.23403345048427582, Final Batch Loss: 0.12038996815681458\n",
      "Epoch 4449, Loss: 0.24581023305654526, Final Batch Loss: 0.11462951451539993\n",
      "Epoch 4450, Loss: 0.23059119284152985, Final Batch Loss: 0.10239824652671814\n",
      "Epoch 4451, Loss: 0.23122264444828033, Final Batch Loss: 0.12053356319665909\n",
      "Epoch 4452, Loss: 0.2890419363975525, Final Batch Loss: 0.12861822545528412\n",
      "Epoch 4453, Loss: 0.24305902421474457, Final Batch Loss: 0.1070033609867096\n",
      "Epoch 4454, Loss: 0.2711128741502762, Final Batch Loss: 0.13541646301746368\n",
      "Epoch 4455, Loss: 0.25272805243730545, Final Batch Loss: 0.11010577529668808\n",
      "Epoch 4456, Loss: 0.3000658303499222, Final Batch Loss: 0.16472472250461578\n",
      "Epoch 4457, Loss: 0.23103486746549606, Final Batch Loss: 0.09940608590841293\n",
      "Epoch 4458, Loss: 0.250749871134758, Final Batch Loss: 0.11745721101760864\n",
      "Epoch 4459, Loss: 0.2572818920016289, Final Batch Loss: 0.13357283174991608\n",
      "Epoch 4460, Loss: 0.24806199222803116, Final Batch Loss: 0.09966226667165756\n",
      "Epoch 4461, Loss: 0.20766759663820267, Final Batch Loss: 0.10904538631439209\n",
      "Epoch 4462, Loss: 0.24042291939258575, Final Batch Loss: 0.0990225076675415\n",
      "Epoch 4463, Loss: 0.24383925646543503, Final Batch Loss: 0.10946016758680344\n",
      "Epoch 4464, Loss: 0.2677488848567009, Final Batch Loss: 0.14952237904071808\n",
      "Epoch 4465, Loss: 0.2430441901087761, Final Batch Loss: 0.1273326724767685\n",
      "Epoch 4466, Loss: 0.2434496134519577, Final Batch Loss: 0.10056915879249573\n",
      "Epoch 4467, Loss: 0.2725975960493088, Final Batch Loss: 0.1589558869600296\n",
      "Epoch 4468, Loss: 0.2729174494743347, Final Batch Loss: 0.14034362137317657\n",
      "Epoch 4469, Loss: 0.22662966698408127, Final Batch Loss: 0.10923536866903305\n",
      "Epoch 4470, Loss: 0.24863556027412415, Final Batch Loss: 0.09479975700378418\n",
      "Epoch 4471, Loss: 0.2648017406463623, Final Batch Loss: 0.15385234355926514\n",
      "Epoch 4472, Loss: 0.22581548243761063, Final Batch Loss: 0.12593908607959747\n",
      "Epoch 4473, Loss: 0.25860314816236496, Final Batch Loss: 0.13606680929660797\n",
      "Epoch 4474, Loss: 0.24554180353879929, Final Batch Loss: 0.14023472368717194\n",
      "Epoch 4475, Loss: 0.23244045674800873, Final Batch Loss: 0.12472385913133621\n",
      "Epoch 4476, Loss: 0.2534973621368408, Final Batch Loss: 0.10288023948669434\n",
      "Epoch 4477, Loss: 0.3198021128773689, Final Batch Loss: 0.11302158981561661\n",
      "Epoch 4478, Loss: 0.2681010216474533, Final Batch Loss: 0.15223678946495056\n",
      "Epoch 4479, Loss: 0.232215516269207, Final Batch Loss: 0.11623600870370865\n",
      "Epoch 4480, Loss: 0.23038528859615326, Final Batch Loss: 0.1093369722366333\n",
      "Epoch 4481, Loss: 0.22368424385786057, Final Batch Loss: 0.12131703644990921\n",
      "Epoch 4482, Loss: 0.21561229974031448, Final Batch Loss: 0.0916963517665863\n",
      "Epoch 4483, Loss: 0.2661408707499504, Final Batch Loss: 0.10630062967538834\n",
      "Epoch 4484, Loss: 0.285764642059803, Final Batch Loss: 0.18773601949214935\n",
      "Epoch 4485, Loss: 0.22813577204942703, Final Batch Loss: 0.12147432565689087\n",
      "Epoch 4486, Loss: 0.27022983133792877, Final Batch Loss: 0.1453208178281784\n",
      "Epoch 4487, Loss: 0.23555034399032593, Final Batch Loss: 0.14736616611480713\n",
      "Epoch 4488, Loss: 0.21853838115930557, Final Batch Loss: 0.09370746463537216\n",
      "Epoch 4489, Loss: 0.28142331540584564, Final Batch Loss: 0.1303098350763321\n",
      "Epoch 4490, Loss: 0.20707286149263382, Final Batch Loss: 0.10131629556417465\n",
      "Epoch 4491, Loss: 0.23513273149728775, Final Batch Loss: 0.10472951084375381\n",
      "Epoch 4492, Loss: 0.24327784031629562, Final Batch Loss: 0.09933336824178696\n",
      "Epoch 4493, Loss: 0.23269475251436234, Final Batch Loss: 0.10242574661970139\n",
      "Epoch 4494, Loss: 0.21323882043361664, Final Batch Loss: 0.10772023350000381\n",
      "Epoch 4495, Loss: 0.3103536069393158, Final Batch Loss: 0.1346491426229477\n",
      "Epoch 4496, Loss: 0.23631592839956284, Final Batch Loss: 0.11472621560096741\n",
      "Epoch 4497, Loss: 0.29578857123851776, Final Batch Loss: 0.15144585072994232\n",
      "Epoch 4498, Loss: 0.2445659413933754, Final Batch Loss: 0.08211398869752884\n",
      "Epoch 4499, Loss: 0.23868008702993393, Final Batch Loss: 0.12234016507863998\n",
      "Epoch 4500, Loss: 0.24245606362819672, Final Batch Loss: 0.11977124214172363\n",
      "Epoch 4501, Loss: 0.19750313460826874, Final Batch Loss: 0.10344493389129639\n",
      "Epoch 4502, Loss: 0.2829092815518379, Final Batch Loss: 0.16383540630340576\n",
      "Epoch 4503, Loss: 0.24438688904047012, Final Batch Loss: 0.10430821031332016\n",
      "Epoch 4504, Loss: 0.2479262724518776, Final Batch Loss: 0.11457773298025131\n",
      "Epoch 4505, Loss: 0.22225648164749146, Final Batch Loss: 0.12432052940130234\n",
      "Epoch 4506, Loss: 0.30908389389514923, Final Batch Loss: 0.16184477508068085\n",
      "Epoch 4507, Loss: 0.27641065418720245, Final Batch Loss: 0.1253732293844223\n",
      "Epoch 4508, Loss: 0.22913837432861328, Final Batch Loss: 0.113357774913311\n",
      "Epoch 4509, Loss: 0.26211878657341003, Final Batch Loss: 0.12780019640922546\n",
      "Epoch 4510, Loss: 0.24115639179944992, Final Batch Loss: 0.11754798144102097\n",
      "Epoch 4511, Loss: 0.2910836786031723, Final Batch Loss: 0.13160642981529236\n",
      "Epoch 4512, Loss: 0.2566777616739273, Final Batch Loss: 0.10376805067062378\n",
      "Epoch 4513, Loss: 0.2984156906604767, Final Batch Loss: 0.17727838456630707\n",
      "Epoch 4514, Loss: 0.2114100083708763, Final Batch Loss: 0.08343898504972458\n",
      "Epoch 4515, Loss: 0.24852360785007477, Final Batch Loss: 0.12636585533618927\n",
      "Epoch 4516, Loss: 0.25113409012556076, Final Batch Loss: 0.13628627359867096\n",
      "Epoch 4517, Loss: 0.23550103604793549, Final Batch Loss: 0.11873504519462585\n",
      "Epoch 4518, Loss: 0.23225121200084686, Final Batch Loss: 0.12743522226810455\n",
      "Epoch 4519, Loss: 0.21720589697360992, Final Batch Loss: 0.08030509948730469\n",
      "Epoch 4520, Loss: 0.22992706298828125, Final Batch Loss: 0.11690662056207657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4521, Loss: 0.24300707876682281, Final Batch Loss: 0.12611056864261627\n",
      "Epoch 4522, Loss: 0.2444470301270485, Final Batch Loss: 0.07116212695837021\n",
      "Epoch 4523, Loss: 0.2544805705547333, Final Batch Loss: 0.14365126192569733\n",
      "Epoch 4524, Loss: 0.2547376677393913, Final Batch Loss: 0.12974295020103455\n",
      "Epoch 4525, Loss: 0.25572407245635986, Final Batch Loss: 0.14199486374855042\n",
      "Epoch 4526, Loss: 0.244171604514122, Final Batch Loss: 0.13621792197227478\n",
      "Epoch 4527, Loss: 0.25026893615722656, Final Batch Loss: 0.13014289736747742\n",
      "Epoch 4528, Loss: 0.24080803245306015, Final Batch Loss: 0.1011262759566307\n",
      "Epoch 4529, Loss: 0.23443110287189484, Final Batch Loss: 0.12597854435443878\n",
      "Epoch 4530, Loss: 0.2579769939184189, Final Batch Loss: 0.13150182366371155\n",
      "Epoch 4531, Loss: 0.3378797620534897, Final Batch Loss: 0.14702214300632477\n",
      "Epoch 4532, Loss: 0.30383381247520447, Final Batch Loss: 0.18801791965961456\n",
      "Epoch 4533, Loss: 0.24076853692531586, Final Batch Loss: 0.13085800409317017\n",
      "Epoch 4534, Loss: 0.24884744733572006, Final Batch Loss: 0.10863421112298965\n",
      "Epoch 4535, Loss: 0.27659018337726593, Final Batch Loss: 0.12249740958213806\n",
      "Epoch 4536, Loss: 0.22648029774427414, Final Batch Loss: 0.1098255142569542\n",
      "Epoch 4537, Loss: 0.20680966973304749, Final Batch Loss: 0.09140954166650772\n",
      "Epoch 4538, Loss: 0.2778865024447441, Final Batch Loss: 0.15465374290943146\n",
      "Epoch 4539, Loss: 0.29458315670490265, Final Batch Loss: 0.15296991169452667\n",
      "Epoch 4540, Loss: 0.2439868226647377, Final Batch Loss: 0.11723624914884567\n",
      "Epoch 4541, Loss: 0.22739756107330322, Final Batch Loss: 0.10198536515235901\n",
      "Epoch 4542, Loss: 0.2466236725449562, Final Batch Loss: 0.07924019545316696\n",
      "Epoch 4543, Loss: 0.23331080377101898, Final Batch Loss: 0.1307401955127716\n",
      "Epoch 4544, Loss: 0.21964317560195923, Final Batch Loss: 0.09314405918121338\n",
      "Epoch 4545, Loss: 0.22791092842817307, Final Batch Loss: 0.1021309569478035\n",
      "Epoch 4546, Loss: 0.2179045006632805, Final Batch Loss: 0.09732533246278763\n",
      "Epoch 4547, Loss: 0.23531799763441086, Final Batch Loss: 0.1242622658610344\n",
      "Epoch 4548, Loss: 0.3406652808189392, Final Batch Loss: 0.20568372309207916\n",
      "Epoch 4549, Loss: 0.2429329976439476, Final Batch Loss: 0.11485666781663895\n",
      "Epoch 4550, Loss: 0.22189360857009888, Final Batch Loss: 0.12411367893218994\n",
      "Epoch 4551, Loss: 0.22013160586357117, Final Batch Loss: 0.11818306893110275\n",
      "Epoch 4552, Loss: 0.2145964726805687, Final Batch Loss: 0.10977878421545029\n",
      "Epoch 4553, Loss: 0.22570843994617462, Final Batch Loss: 0.13832201063632965\n",
      "Epoch 4554, Loss: 0.230852410197258, Final Batch Loss: 0.12254476547241211\n",
      "Epoch 4555, Loss: 0.24944572895765305, Final Batch Loss: 0.11649312824010849\n",
      "Epoch 4556, Loss: 0.2436721920967102, Final Batch Loss: 0.13531993329524994\n",
      "Epoch 4557, Loss: 0.2203698381781578, Final Batch Loss: 0.09427977353334427\n",
      "Epoch 4558, Loss: 0.2473955750465393, Final Batch Loss: 0.10328727960586548\n",
      "Epoch 4559, Loss: 0.28403183817863464, Final Batch Loss: 0.14224381744861603\n",
      "Epoch 4560, Loss: 0.2712605521082878, Final Batch Loss: 0.11905806511640549\n",
      "Epoch 4561, Loss: 0.22525827586650848, Final Batch Loss: 0.11633238941431046\n",
      "Epoch 4562, Loss: 0.2794110178947449, Final Batch Loss: 0.1684187650680542\n",
      "Epoch 4563, Loss: 0.2456289380788803, Final Batch Loss: 0.09150990843772888\n",
      "Epoch 4564, Loss: 0.2508971765637398, Final Batch Loss: 0.12259315699338913\n",
      "Epoch 4565, Loss: 0.3386298641562462, Final Batch Loss: 0.12064582854509354\n",
      "Epoch 4566, Loss: 0.27023258060216904, Final Batch Loss: 0.10916253179311752\n",
      "Epoch 4567, Loss: 0.2204621434211731, Final Batch Loss: 0.12515132129192352\n",
      "Epoch 4568, Loss: 0.2400806024670601, Final Batch Loss: 0.12424156069755554\n",
      "Epoch 4569, Loss: 0.22963596135377884, Final Batch Loss: 0.10885121673345566\n",
      "Epoch 4570, Loss: 0.2237325757741928, Final Batch Loss: 0.1358470469713211\n",
      "Epoch 4571, Loss: 0.2249976024031639, Final Batch Loss: 0.11266451328992844\n",
      "Epoch 4572, Loss: 0.2612316980957985, Final Batch Loss: 0.15745194256305695\n",
      "Epoch 4573, Loss: 0.2720194309949875, Final Batch Loss: 0.1386837661266327\n",
      "Epoch 4574, Loss: 0.21936632692813873, Final Batch Loss: 0.1069885715842247\n",
      "Epoch 4575, Loss: 0.2389312982559204, Final Batch Loss: 0.12321094423532486\n",
      "Epoch 4576, Loss: 0.24563968181610107, Final Batch Loss: 0.14493303000926971\n",
      "Epoch 4577, Loss: 0.2570171654224396, Final Batch Loss: 0.14400357007980347\n",
      "Epoch 4578, Loss: 0.21671639382839203, Final Batch Loss: 0.10327038913965225\n",
      "Epoch 4579, Loss: 0.21917618066072464, Final Batch Loss: 0.14137615263462067\n",
      "Epoch 4580, Loss: 0.23555494099855423, Final Batch Loss: 0.10527295619249344\n",
      "Epoch 4581, Loss: 0.25754476338624954, Final Batch Loss: 0.09424354881048203\n",
      "Epoch 4582, Loss: 0.22891142964363098, Final Batch Loss: 0.10906285047531128\n",
      "Epoch 4583, Loss: 0.23294489085674286, Final Batch Loss: 0.11715448647737503\n",
      "Epoch 4584, Loss: 0.2301834672689438, Final Batch Loss: 0.10074886679649353\n",
      "Epoch 4585, Loss: 0.22326581925153732, Final Batch Loss: 0.08780563622713089\n",
      "Epoch 4586, Loss: 0.28110720217227936, Final Batch Loss: 0.1192975640296936\n",
      "Epoch 4587, Loss: 0.22554194182157516, Final Batch Loss: 0.11804842948913574\n",
      "Epoch 4588, Loss: 0.28634442389011383, Final Batch Loss: 0.15445852279663086\n",
      "Epoch 4589, Loss: 0.29284654557704926, Final Batch Loss: 0.1858510971069336\n",
      "Epoch 4590, Loss: 0.22424087673425674, Final Batch Loss: 0.10531967878341675\n",
      "Epoch 4591, Loss: 0.265641912817955, Final Batch Loss: 0.14474010467529297\n",
      "Epoch 4592, Loss: 0.23970083892345428, Final Batch Loss: 0.11966291069984436\n",
      "Epoch 4593, Loss: 0.22946323454380035, Final Batch Loss: 0.0931488573551178\n",
      "Epoch 4594, Loss: 0.271089106798172, Final Batch Loss: 0.13183197379112244\n",
      "Epoch 4595, Loss: 0.2273699790239334, Final Batch Loss: 0.11088243871927261\n",
      "Epoch 4596, Loss: 0.21992918103933334, Final Batch Loss: 0.08394572883844376\n",
      "Epoch 4597, Loss: 0.2790161222219467, Final Batch Loss: 0.14142240583896637\n",
      "Epoch 4598, Loss: 0.21403693407773972, Final Batch Loss: 0.07418332248926163\n",
      "Epoch 4599, Loss: 0.2696418911218643, Final Batch Loss: 0.15267272293567657\n",
      "Epoch 4600, Loss: 0.21868861466646194, Final Batch Loss: 0.08289744704961777\n",
      "Epoch 4601, Loss: 0.2555413618683815, Final Batch Loss: 0.12118848413228989\n",
      "Epoch 4602, Loss: 0.2503245398402214, Final Batch Loss: 0.1431562602519989\n",
      "Epoch 4603, Loss: 0.2757517993450165, Final Batch Loss: 0.14541451632976532\n",
      "Epoch 4604, Loss: 0.2277495488524437, Final Batch Loss: 0.10234499722719193\n",
      "Epoch 4605, Loss: 0.2540356516838074, Final Batch Loss: 0.11292755603790283\n",
      "Epoch 4606, Loss: 0.2454638034105301, Final Batch Loss: 0.1436842530965805\n",
      "Epoch 4607, Loss: 0.22542643547058105, Final Batch Loss: 0.138702854514122\n",
      "Epoch 4608, Loss: 0.2328273206949234, Final Batch Loss: 0.10049471259117126\n",
      "Epoch 4609, Loss: 0.22949763387441635, Final Batch Loss: 0.11938966065645218\n",
      "Epoch 4610, Loss: 0.2495238408446312, Final Batch Loss: 0.13980655372142792\n",
      "Epoch 4611, Loss: 0.25980496406555176, Final Batch Loss: 0.09270927309989929\n",
      "Epoch 4612, Loss: 0.2796529084444046, Final Batch Loss: 0.14358635246753693\n",
      "Epoch 4613, Loss: 0.32256166636943817, Final Batch Loss: 0.17308543622493744\n",
      "Epoch 4614, Loss: 0.20707745105028152, Final Batch Loss: 0.10612857341766357\n",
      "Epoch 4615, Loss: 0.22613374143838882, Final Batch Loss: 0.09127283841371536\n",
      "Epoch 4616, Loss: 0.21281643211841583, Final Batch Loss: 0.0858573317527771\n",
      "Epoch 4617, Loss: 0.21632135659456253, Final Batch Loss: 0.10859701782464981\n",
      "Epoch 4618, Loss: 0.2305518090724945, Final Batch Loss: 0.10758206248283386\n",
      "Epoch 4619, Loss: 0.2784481346607208, Final Batch Loss: 0.15353430807590485\n",
      "Epoch 4620, Loss: 0.2349764183163643, Final Batch Loss: 0.09843973070383072\n",
      "Epoch 4621, Loss: 0.22303227335214615, Final Batch Loss: 0.0919756069779396\n",
      "Epoch 4622, Loss: 0.20329374074935913, Final Batch Loss: 0.08270853757858276\n",
      "Epoch 4623, Loss: 0.27542488276958466, Final Batch Loss: 0.16153572499752045\n",
      "Epoch 4624, Loss: 0.32193081080913544, Final Batch Loss: 0.1268928200006485\n",
      "Epoch 4625, Loss: 0.2415294125676155, Final Batch Loss: 0.11734098196029663\n",
      "Epoch 4626, Loss: 0.26099245250225067, Final Batch Loss: 0.1376601606607437\n",
      "Epoch 4627, Loss: 0.23064882308244705, Final Batch Loss: 0.1313667744398117\n",
      "Epoch 4628, Loss: 0.23198248445987701, Final Batch Loss: 0.13685254752635956\n",
      "Epoch 4629, Loss: 0.26721251010894775, Final Batch Loss: 0.15038008987903595\n",
      "Epoch 4630, Loss: 0.293985977768898, Final Batch Loss: 0.13375703990459442\n",
      "Epoch 4631, Loss: 0.22331402450799942, Final Batch Loss: 0.0859425738453865\n",
      "Epoch 4632, Loss: 0.23368604481220245, Final Batch Loss: 0.13488060235977173\n",
      "Epoch 4633, Loss: 0.22458839416503906, Final Batch Loss: 0.118841253221035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4634, Loss: 0.278425969183445, Final Batch Loss: 0.17261476814746857\n",
      "Epoch 4635, Loss: 0.25900963693857193, Final Batch Loss: 0.11743123084306717\n",
      "Epoch 4636, Loss: 0.22636904567480087, Final Batch Loss: 0.08009328693151474\n",
      "Epoch 4637, Loss: 0.23010168969631195, Final Batch Loss: 0.12522132694721222\n",
      "Epoch 4638, Loss: 0.2519216611981392, Final Batch Loss: 0.14237982034683228\n",
      "Epoch 4639, Loss: 0.3340103328227997, Final Batch Loss: 0.18678469955921173\n",
      "Epoch 4640, Loss: 0.3225250542163849, Final Batch Loss: 0.15667156875133514\n",
      "Epoch 4641, Loss: 0.2564883455634117, Final Batch Loss: 0.1410682201385498\n",
      "Epoch 4642, Loss: 0.24424277245998383, Final Batch Loss: 0.14305196702480316\n",
      "Epoch 4643, Loss: 0.21786580979824066, Final Batch Loss: 0.10903000831604004\n",
      "Epoch 4644, Loss: 0.2404138222336769, Final Batch Loss: 0.1316118985414505\n",
      "Epoch 4645, Loss: 0.2433859184384346, Final Batch Loss: 0.14362657070159912\n",
      "Epoch 4646, Loss: 0.21910033375024796, Final Batch Loss: 0.09242016822099686\n",
      "Epoch 4647, Loss: 0.24297937750816345, Final Batch Loss: 0.1529640108346939\n",
      "Epoch 4648, Loss: 0.2520833685994148, Final Batch Loss: 0.15542282164096832\n",
      "Epoch 4649, Loss: 0.2564563453197479, Final Batch Loss: 0.13246560096740723\n",
      "Epoch 4650, Loss: 0.2733280807733536, Final Batch Loss: 0.14433112740516663\n",
      "Epoch 4651, Loss: 0.25078191608190536, Final Batch Loss: 0.1271541565656662\n",
      "Epoch 4652, Loss: 0.3064192235469818, Final Batch Loss: 0.1516372710466385\n",
      "Epoch 4653, Loss: 0.2677710950374603, Final Batch Loss: 0.1917264610528946\n",
      "Epoch 4654, Loss: 0.23739702254533768, Final Batch Loss: 0.10974086076021194\n",
      "Epoch 4655, Loss: 0.26485005766153336, Final Batch Loss: 0.15555734932422638\n",
      "Epoch 4656, Loss: 0.22260384261608124, Final Batch Loss: 0.1173616275191307\n",
      "Epoch 4657, Loss: 0.22789252549409866, Final Batch Loss: 0.11105198413133621\n",
      "Epoch 4658, Loss: 0.2700740844011307, Final Batch Loss: 0.12843100726604462\n",
      "Epoch 4659, Loss: 0.24471544474363327, Final Batch Loss: 0.12084269523620605\n",
      "Epoch 4660, Loss: 0.2600405216217041, Final Batch Loss: 0.1429990977048874\n",
      "Epoch 4661, Loss: 0.27937254309654236, Final Batch Loss: 0.14106042683124542\n",
      "Epoch 4662, Loss: 0.22724294662475586, Final Batch Loss: 0.12079452723264694\n",
      "Epoch 4663, Loss: 0.2570405825972557, Final Batch Loss: 0.12417013198137283\n",
      "Epoch 4664, Loss: 0.2377910390496254, Final Batch Loss: 0.09170489758253098\n",
      "Epoch 4665, Loss: 0.23611287772655487, Final Batch Loss: 0.12329297512769699\n",
      "Epoch 4666, Loss: 0.24802809953689575, Final Batch Loss: 0.1441274732351303\n",
      "Epoch 4667, Loss: 0.24449250102043152, Final Batch Loss: 0.10298249125480652\n",
      "Epoch 4668, Loss: 0.23206355422735214, Final Batch Loss: 0.10159165412187576\n",
      "Epoch 4669, Loss: 0.22438806295394897, Final Batch Loss: 0.12067549675703049\n",
      "Epoch 4670, Loss: 0.21726617217063904, Final Batch Loss: 0.11039157956838608\n",
      "Epoch 4671, Loss: 0.22699934244155884, Final Batch Loss: 0.11296231299638748\n",
      "Epoch 4672, Loss: 0.32881858944892883, Final Batch Loss: 0.17411749064922333\n",
      "Epoch 4673, Loss: 0.23237226903438568, Final Batch Loss: 0.11745399236679077\n",
      "Epoch 4674, Loss: 0.26078279316425323, Final Batch Loss: 0.1427934616804123\n",
      "Epoch 4675, Loss: 0.2580922096967697, Final Batch Loss: 0.12533243000507355\n",
      "Epoch 4676, Loss: 0.2015571966767311, Final Batch Loss: 0.08447543531656265\n",
      "Epoch 4677, Loss: 0.22931929677724838, Final Batch Loss: 0.10797813534736633\n",
      "Epoch 4678, Loss: 0.2190391793847084, Final Batch Loss: 0.11129733920097351\n",
      "Epoch 4679, Loss: 0.2450534850358963, Final Batch Loss: 0.1268811672925949\n",
      "Epoch 4680, Loss: 0.22508341819047928, Final Batch Loss: 0.13742175698280334\n",
      "Epoch 4681, Loss: 0.27872638404369354, Final Batch Loss: 0.1423352211713791\n",
      "Epoch 4682, Loss: 0.2055850550532341, Final Batch Loss: 0.09329622238874435\n",
      "Epoch 4683, Loss: 0.23525355756282806, Final Batch Loss: 0.13143138587474823\n",
      "Epoch 4684, Loss: 0.2574853450059891, Final Batch Loss: 0.12484538555145264\n",
      "Epoch 4685, Loss: 0.24065527319908142, Final Batch Loss: 0.10677361488342285\n",
      "Epoch 4686, Loss: 0.23257023841142654, Final Batch Loss: 0.11054202169179916\n",
      "Epoch 4687, Loss: 0.2668999508023262, Final Batch Loss: 0.12007688730955124\n",
      "Epoch 4688, Loss: 0.21603883057832718, Final Batch Loss: 0.12156900763511658\n",
      "Epoch 4689, Loss: 0.23252691328525543, Final Batch Loss: 0.12577877938747406\n",
      "Epoch 4690, Loss: 0.24124590307474136, Final Batch Loss: 0.11637657880783081\n",
      "Epoch 4691, Loss: 0.1904335916042328, Final Batch Loss: 0.0673757866024971\n",
      "Epoch 4692, Loss: 0.21251774579286575, Final Batch Loss: 0.08035057038068771\n",
      "Epoch 4693, Loss: 0.20449090749025345, Final Batch Loss: 0.09515324980020523\n",
      "Epoch 4694, Loss: 0.2814635634422302, Final Batch Loss: 0.1322287619113922\n",
      "Epoch 4695, Loss: 0.30470024794340134, Final Batch Loss: 0.11918113380670547\n",
      "Epoch 4696, Loss: 0.28386418521404266, Final Batch Loss: 0.1359170824289322\n",
      "Epoch 4697, Loss: 0.20347557216882706, Final Batch Loss: 0.08879130333662033\n",
      "Epoch 4698, Loss: 0.25459203124046326, Final Batch Loss: 0.13279284536838531\n",
      "Epoch 4699, Loss: 0.26988351345062256, Final Batch Loss: 0.16499044001102448\n",
      "Epoch 4700, Loss: 0.24797993153333664, Final Batch Loss: 0.12266864627599716\n",
      "Epoch 4701, Loss: 0.2553184702992439, Final Batch Loss: 0.12389137595891953\n",
      "Epoch 4702, Loss: 0.22278963774442673, Final Batch Loss: 0.10795263200998306\n",
      "Epoch 4703, Loss: 0.2843792662024498, Final Batch Loss: 0.16091836988925934\n",
      "Epoch 4704, Loss: 0.2574968785047531, Final Batch Loss: 0.11695647239685059\n",
      "Epoch 4705, Loss: 0.22005747258663177, Final Batch Loss: 0.13216976821422577\n",
      "Epoch 4706, Loss: 0.26017313450574875, Final Batch Loss: 0.09776731580495834\n",
      "Epoch 4707, Loss: 0.25961993634700775, Final Batch Loss: 0.15182872116565704\n",
      "Epoch 4708, Loss: 0.22587735205888748, Final Batch Loss: 0.11327314376831055\n",
      "Epoch 4709, Loss: 0.2419142723083496, Final Batch Loss: 0.1339445859193802\n",
      "Epoch 4710, Loss: 0.2595113515853882, Final Batch Loss: 0.13315153121948242\n",
      "Epoch 4711, Loss: 0.23788389563560486, Final Batch Loss: 0.13408775627613068\n",
      "Epoch 4712, Loss: 0.3009398505091667, Final Batch Loss: 0.11758352071046829\n",
      "Epoch 4713, Loss: 0.23245810717344284, Final Batch Loss: 0.10263318568468094\n",
      "Epoch 4714, Loss: 0.25896527618169785, Final Batch Loss: 0.11369594186544418\n",
      "Epoch 4715, Loss: 0.245432049036026, Final Batch Loss: 0.16159285604953766\n",
      "Epoch 4716, Loss: 0.2151317074894905, Final Batch Loss: 0.11640778928995132\n",
      "Epoch 4717, Loss: 0.24259865283966064, Final Batch Loss: 0.11240077018737793\n",
      "Epoch 4718, Loss: 0.23162973672151566, Final Batch Loss: 0.11450015753507614\n",
      "Epoch 4719, Loss: 0.23746085166931152, Final Batch Loss: 0.10599273443222046\n",
      "Epoch 4720, Loss: 0.2476351261138916, Final Batch Loss: 0.1472991704940796\n",
      "Epoch 4721, Loss: 0.24264107644557953, Final Batch Loss: 0.11446300148963928\n",
      "Epoch 4722, Loss: 0.21586034446954727, Final Batch Loss: 0.09612876921892166\n",
      "Epoch 4723, Loss: 0.21490102261304855, Final Batch Loss: 0.09665069729089737\n",
      "Epoch 4724, Loss: 0.24179313331842422, Final Batch Loss: 0.12050905078649521\n",
      "Epoch 4725, Loss: 0.25125589221715927, Final Batch Loss: 0.13315443694591522\n",
      "Epoch 4726, Loss: 0.25042615085840225, Final Batch Loss: 0.11144915968179703\n",
      "Epoch 4727, Loss: 0.2194845750927925, Final Batch Loss: 0.1176973283290863\n",
      "Epoch 4728, Loss: 0.20336943119764328, Final Batch Loss: 0.0988384559750557\n",
      "Epoch 4729, Loss: 0.2757634371519089, Final Batch Loss: 0.1786913424730301\n",
      "Epoch 4730, Loss: 0.2335478886961937, Final Batch Loss: 0.09930383414030075\n",
      "Epoch 4731, Loss: 0.21709786355495453, Final Batch Loss: 0.09691118448972702\n",
      "Epoch 4732, Loss: 0.2670423090457916, Final Batch Loss: 0.1491812765598297\n",
      "Epoch 4733, Loss: 0.24009356647729874, Final Batch Loss: 0.11322792619466782\n",
      "Epoch 4734, Loss: 0.2069254294037819, Final Batch Loss: 0.10211062431335449\n",
      "Epoch 4735, Loss: 0.21782910823822021, Final Batch Loss: 0.07592155039310455\n",
      "Epoch 4736, Loss: 0.26220063865184784, Final Batch Loss: 0.136439710855484\n",
      "Epoch 4737, Loss: 0.26616818457841873, Final Batch Loss: 0.12241942435503006\n",
      "Epoch 4738, Loss: 0.21433806419372559, Final Batch Loss: 0.12252160906791687\n",
      "Epoch 4739, Loss: 0.2558116242289543, Final Batch Loss: 0.14597256481647491\n",
      "Epoch 4740, Loss: 0.21990931034088135, Final Batch Loss: 0.1056073009967804\n",
      "Epoch 4741, Loss: 0.24561183899641037, Final Batch Loss: 0.12723979353904724\n",
      "Epoch 4742, Loss: 0.24593592435121536, Final Batch Loss: 0.12915869057178497\n",
      "Epoch 4743, Loss: 0.22865639626979828, Final Batch Loss: 0.10332342982292175\n",
      "Epoch 4744, Loss: 0.25379882007837296, Final Batch Loss: 0.16200338304042816\n",
      "Epoch 4745, Loss: 0.223140686750412, Final Batch Loss: 0.09370401501655579\n",
      "Epoch 4746, Loss: 0.2238747626543045, Final Batch Loss: 0.10449980944395065\n",
      "Epoch 4747, Loss: 0.20310497283935547, Final Batch Loss: 0.09917653352022171\n",
      "Epoch 4748, Loss: 0.2566084489226341, Final Batch Loss: 0.11340474337339401\n",
      "Epoch 4749, Loss: 0.26396043598651886, Final Batch Loss: 0.1580245941877365\n",
      "Epoch 4750, Loss: 0.24219512194395065, Final Batch Loss: 0.15525126457214355\n",
      "Epoch 4751, Loss: 0.249956913292408, Final Batch Loss: 0.11548931151628494\n",
      "Epoch 4752, Loss: 0.22151155024766922, Final Batch Loss: 0.08622587472200394\n",
      "Epoch 4753, Loss: 0.25685620307922363, Final Batch Loss: 0.12838533520698547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4754, Loss: 0.23554518073797226, Final Batch Loss: 0.13614709675312042\n",
      "Epoch 4755, Loss: 0.2367696687579155, Final Batch Loss: 0.08967684954404831\n",
      "Epoch 4756, Loss: 0.28521381318569183, Final Batch Loss: 0.12715883553028107\n",
      "Epoch 4757, Loss: 0.23253078013658524, Final Batch Loss: 0.11735741049051285\n",
      "Epoch 4758, Loss: 0.26664532721042633, Final Batch Loss: 0.1118631660938263\n",
      "Epoch 4759, Loss: 0.2632730081677437, Final Batch Loss: 0.14154715836048126\n",
      "Epoch 4760, Loss: 0.2902579680085182, Final Batch Loss: 0.19176743924617767\n",
      "Epoch 4761, Loss: 0.26090526580810547, Final Batch Loss: 0.13057030737400055\n",
      "Epoch 4762, Loss: 0.23716171830892563, Final Batch Loss: 0.12613703310489655\n",
      "Epoch 4763, Loss: 0.21835925430059433, Final Batch Loss: 0.11701920628547668\n",
      "Epoch 4764, Loss: 0.2740214616060257, Final Batch Loss: 0.14874477684497833\n",
      "Epoch 4765, Loss: 0.31696122884750366, Final Batch Loss: 0.1954336166381836\n",
      "Epoch 4766, Loss: 0.39437584578990936, Final Batch Loss: 0.1479300856590271\n",
      "Epoch 4767, Loss: 0.28464144468307495, Final Batch Loss: 0.14839838445186615\n",
      "Epoch 4768, Loss: 0.23142389953136444, Final Batch Loss: 0.126991406083107\n",
      "Epoch 4769, Loss: 0.23104284703731537, Final Batch Loss: 0.12132713943719864\n",
      "Epoch 4770, Loss: 0.21436761319637299, Final Batch Loss: 0.09487669914960861\n",
      "Epoch 4771, Loss: 0.24174176901578903, Final Batch Loss: 0.11485516279935837\n",
      "Epoch 4772, Loss: 0.2485673725605011, Final Batch Loss: 0.13524310290813446\n",
      "Epoch 4773, Loss: 0.25618577003479004, Final Batch Loss: 0.109779953956604\n",
      "Epoch 4774, Loss: 0.22014763951301575, Final Batch Loss: 0.12399101257324219\n",
      "Epoch 4775, Loss: 0.30641190707683563, Final Batch Loss: 0.158717080950737\n",
      "Epoch 4776, Loss: 0.22351420670747757, Final Batch Loss: 0.08457887917757034\n",
      "Epoch 4777, Loss: 0.24755627661943436, Final Batch Loss: 0.12920062243938446\n",
      "Epoch 4778, Loss: 0.26788222789764404, Final Batch Loss: 0.13851451873779297\n",
      "Epoch 4779, Loss: 0.3123006820678711, Final Batch Loss: 0.16642116010189056\n",
      "Epoch 4780, Loss: 0.2173830047249794, Final Batch Loss: 0.12340611964464188\n",
      "Epoch 4781, Loss: 0.26350291073322296, Final Batch Loss: 0.13709641993045807\n",
      "Epoch 4782, Loss: 0.2843999117612839, Final Batch Loss: 0.14879217743873596\n",
      "Epoch 4783, Loss: 0.23369129747152328, Final Batch Loss: 0.12232332676649094\n",
      "Epoch 4784, Loss: 0.19967053085565567, Final Batch Loss: 0.09557124972343445\n",
      "Epoch 4785, Loss: 0.21836189180612564, Final Batch Loss: 0.1069517657160759\n",
      "Epoch 4786, Loss: 0.23509296029806137, Final Batch Loss: 0.0951726958155632\n",
      "Epoch 4787, Loss: 0.25851815193891525, Final Batch Loss: 0.13362467288970947\n",
      "Epoch 4788, Loss: 0.257893830537796, Final Batch Loss: 0.104076087474823\n",
      "Epoch 4789, Loss: 0.2102053165435791, Final Batch Loss: 0.09106141328811646\n",
      "Epoch 4790, Loss: 0.3279959112405777, Final Batch Loss: 0.12019842863082886\n",
      "Epoch 4791, Loss: 0.27008793503046036, Final Batch Loss: 0.1552758663892746\n",
      "Epoch 4792, Loss: 0.24929001182317734, Final Batch Loss: 0.12485853582620621\n",
      "Epoch 4793, Loss: 0.20390719175338745, Final Batch Loss: 0.0998857244849205\n",
      "Epoch 4794, Loss: 0.2262091264128685, Final Batch Loss: 0.09929252415895462\n",
      "Epoch 4795, Loss: 0.23559419065713882, Final Batch Loss: 0.09333734959363937\n",
      "Epoch 4796, Loss: 0.2222946733236313, Final Batch Loss: 0.10941993445158005\n",
      "Epoch 4797, Loss: 0.24180657416582108, Final Batch Loss: 0.10944337397813797\n",
      "Epoch 4798, Loss: 0.2512095421552658, Final Batch Loss: 0.12367653846740723\n",
      "Epoch 4799, Loss: 0.2551203742623329, Final Batch Loss: 0.12478818744421005\n",
      "Epoch 4800, Loss: 0.3271019607782364, Final Batch Loss: 0.20860876142978668\n",
      "Epoch 4801, Loss: 0.2785769999027252, Final Batch Loss: 0.13519318401813507\n",
      "Epoch 4802, Loss: 0.23033609241247177, Final Batch Loss: 0.1288473904132843\n",
      "Epoch 4803, Loss: 0.21716483682394028, Final Batch Loss: 0.07990176230669022\n",
      "Epoch 4804, Loss: 0.384501576423645, Final Batch Loss: 0.18799583613872528\n",
      "Epoch 4805, Loss: 0.2559228464961052, Final Batch Loss: 0.12366116791963577\n",
      "Epoch 4806, Loss: 0.24422361701726913, Final Batch Loss: 0.0912669226527214\n",
      "Epoch 4807, Loss: 0.2761833667755127, Final Batch Loss: 0.14339731633663177\n",
      "Epoch 4808, Loss: 0.2197689488530159, Final Batch Loss: 0.11172787100076675\n",
      "Epoch 4809, Loss: 0.23302874714136124, Final Batch Loss: 0.11289163678884506\n",
      "Epoch 4810, Loss: 0.21304778009653091, Final Batch Loss: 0.10146956890821457\n",
      "Epoch 4811, Loss: 0.27520936727523804, Final Batch Loss: 0.15687812864780426\n",
      "Epoch 4812, Loss: 0.27259211242198944, Final Batch Loss: 0.1417970061302185\n",
      "Epoch 4813, Loss: 0.22370336204767227, Final Batch Loss: 0.07965675741434097\n",
      "Epoch 4814, Loss: 0.26152466982603073, Final Batch Loss: 0.1123751774430275\n",
      "Epoch 4815, Loss: 0.21771209686994553, Final Batch Loss: 0.10032666474580765\n",
      "Epoch 4816, Loss: 0.2831913232803345, Final Batch Loss: 0.12155106663703918\n",
      "Epoch 4817, Loss: 0.25877512991428375, Final Batch Loss: 0.1138458251953125\n",
      "Epoch 4818, Loss: 0.21017885208129883, Final Batch Loss: 0.09374623745679855\n",
      "Epoch 4819, Loss: 0.22576870769262314, Final Batch Loss: 0.10696026682853699\n",
      "Epoch 4820, Loss: 0.2803201377391815, Final Batch Loss: 0.15431317687034607\n",
      "Epoch 4821, Loss: 0.2556142136454582, Final Batch Loss: 0.1193217858672142\n",
      "Epoch 4822, Loss: 0.2723122760653496, Final Batch Loss: 0.15203066170215607\n",
      "Epoch 4823, Loss: 0.2338779866695404, Final Batch Loss: 0.13298644125461578\n",
      "Epoch 4824, Loss: 0.26006215065717697, Final Batch Loss: 0.10262661427259445\n",
      "Epoch 4825, Loss: 0.2527507171034813, Final Batch Loss: 0.11352359503507614\n",
      "Epoch 4826, Loss: 0.2661045491695404, Final Batch Loss: 0.1405731588602066\n",
      "Epoch 4827, Loss: 0.21466264873743057, Final Batch Loss: 0.09712064266204834\n",
      "Epoch 4828, Loss: 0.25870002061128616, Final Batch Loss: 0.123516745865345\n",
      "Epoch 4829, Loss: 0.2768561840057373, Final Batch Loss: 0.14586670696735382\n",
      "Epoch 4830, Loss: 0.23316297680139542, Final Batch Loss: 0.09917358309030533\n",
      "Epoch 4831, Loss: 0.2556961104273796, Final Batch Loss: 0.10237541049718857\n",
      "Epoch 4832, Loss: 0.19483952969312668, Final Batch Loss: 0.10066895931959152\n",
      "Epoch 4833, Loss: 0.24743086844682693, Final Batch Loss: 0.12159105390310287\n",
      "Epoch 4834, Loss: 0.2739887312054634, Final Batch Loss: 0.15981577336788177\n",
      "Epoch 4835, Loss: 0.3068342059850693, Final Batch Loss: 0.1308787316083908\n",
      "Epoch 4836, Loss: 0.2120325118303299, Final Batch Loss: 0.11620360612869263\n",
      "Epoch 4837, Loss: 0.21632488816976547, Final Batch Loss: 0.10711190104484558\n",
      "Epoch 4838, Loss: 0.23943128436803818, Final Batch Loss: 0.13693971931934357\n",
      "Epoch 4839, Loss: 0.2459770068526268, Final Batch Loss: 0.1411227285861969\n",
      "Epoch 4840, Loss: 0.2307761162519455, Final Batch Loss: 0.1281365305185318\n",
      "Epoch 4841, Loss: 0.24149397015571594, Final Batch Loss: 0.1439446061849594\n",
      "Epoch 4842, Loss: 0.23418768495321274, Final Batch Loss: 0.10936776548624039\n",
      "Epoch 4843, Loss: 0.24527473747730255, Final Batch Loss: 0.13382498919963837\n",
      "Epoch 4844, Loss: 0.2048131749033928, Final Batch Loss: 0.09112802892923355\n",
      "Epoch 4845, Loss: 0.2634345293045044, Final Batch Loss: 0.15806719660758972\n",
      "Epoch 4846, Loss: 0.20189419388771057, Final Batch Loss: 0.08701138943433762\n",
      "Epoch 4847, Loss: 0.22355340421199799, Final Batch Loss: 0.12452548742294312\n",
      "Epoch 4848, Loss: 0.219000943005085, Final Batch Loss: 0.10700652748346329\n",
      "Epoch 4849, Loss: 0.20793597400188446, Final Batch Loss: 0.11160582304000854\n",
      "Epoch 4850, Loss: 0.22976014763116837, Final Batch Loss: 0.1337241679430008\n",
      "Epoch 4851, Loss: 0.20491290092468262, Final Batch Loss: 0.06839071214199066\n",
      "Epoch 4852, Loss: 0.23026668280363083, Final Batch Loss: 0.10991426557302475\n",
      "Epoch 4853, Loss: 0.22030824422836304, Final Batch Loss: 0.12556160986423492\n",
      "Epoch 4854, Loss: 0.22497893124818802, Final Batch Loss: 0.12414576858282089\n",
      "Epoch 4855, Loss: 0.25339560955762863, Final Batch Loss: 0.12127900868654251\n",
      "Epoch 4856, Loss: 0.2305920347571373, Final Batch Loss: 0.09451770037412643\n",
      "Epoch 4857, Loss: 0.2424086034297943, Final Batch Loss: 0.11618649959564209\n",
      "Epoch 4858, Loss: 0.25732915848493576, Final Batch Loss: 0.12196864932775497\n",
      "Epoch 4859, Loss: 0.24246541410684586, Final Batch Loss: 0.12435493618249893\n",
      "Epoch 4860, Loss: 0.2506207376718521, Final Batch Loss: 0.12929695844650269\n",
      "Epoch 4861, Loss: 0.24543515592813492, Final Batch Loss: 0.11945491284132004\n",
      "Epoch 4862, Loss: 0.24588593095541, Final Batch Loss: 0.1401597112417221\n",
      "Epoch 4863, Loss: 0.23595073074102402, Final Batch Loss: 0.14615647494792938\n",
      "Epoch 4864, Loss: 0.265117309987545, Final Batch Loss: 0.1576913744211197\n",
      "Epoch 4865, Loss: 0.23561477661132812, Final Batch Loss: 0.11231762170791626\n",
      "Epoch 4866, Loss: 0.22396383434534073, Final Batch Loss: 0.11890590935945511\n",
      "Epoch 4867, Loss: 0.22866351902484894, Final Batch Loss: 0.08269192278385162\n",
      "Epoch 4868, Loss: 0.2146708071231842, Final Batch Loss: 0.1269185096025467\n",
      "Epoch 4869, Loss: 0.23383828997612, Final Batch Loss: 0.12773658335208893\n",
      "Epoch 4870, Loss: 0.29663994908332825, Final Batch Loss: 0.1733883023262024\n",
      "Epoch 4871, Loss: 0.2169141247868538, Final Batch Loss: 0.08657253533601761\n",
      "Epoch 4872, Loss: 0.2597561478614807, Final Batch Loss: 0.1339436024427414\n",
      "Epoch 4873, Loss: 0.24034535884857178, Final Batch Loss: 0.1353689730167389\n",
      "Epoch 4874, Loss: 0.1970357522368431, Final Batch Loss: 0.09161320328712463\n",
      "Epoch 4875, Loss: 0.2604079246520996, Final Batch Loss: 0.12879841029644012\n",
      "Epoch 4876, Loss: 0.2111511155962944, Final Batch Loss: 0.11235830187797546\n",
      "Epoch 4877, Loss: 0.26084528863430023, Final Batch Loss: 0.13874439895153046\n",
      "Epoch 4878, Loss: 0.22245628386735916, Final Batch Loss: 0.11956402659416199\n",
      "Epoch 4879, Loss: 0.22820166498422623, Final Batch Loss: 0.1396990567445755\n",
      "Epoch 4880, Loss: 0.26812101900577545, Final Batch Loss: 0.1516708880662918\n",
      "Epoch 4881, Loss: 0.23519066721200943, Final Batch Loss: 0.11272474378347397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4882, Loss: 0.24031904339790344, Final Batch Loss: 0.11775794625282288\n",
      "Epoch 4883, Loss: 0.2522922679781914, Final Batch Loss: 0.10496058315038681\n",
      "Epoch 4884, Loss: 0.2558635249733925, Final Batch Loss: 0.14569668471813202\n",
      "Epoch 4885, Loss: 0.2552207335829735, Final Batch Loss: 0.13116978108882904\n",
      "Epoch 4886, Loss: 0.30012836307287216, Final Batch Loss: 0.17625169456005096\n",
      "Epoch 4887, Loss: 0.2655182108283043, Final Batch Loss: 0.11241216212511063\n",
      "Epoch 4888, Loss: 0.2292715162038803, Final Batch Loss: 0.11524929851293564\n",
      "Epoch 4889, Loss: 0.2524684965610504, Final Batch Loss: 0.12135753035545349\n",
      "Epoch 4890, Loss: 0.2793705463409424, Final Batch Loss: 0.15239404141902924\n",
      "Epoch 4891, Loss: 0.2871064245700836, Final Batch Loss: 0.14058835804462433\n",
      "Epoch 4892, Loss: 0.2625136822462082, Final Batch Loss: 0.12772439420223236\n",
      "Epoch 4893, Loss: 0.30459822714328766, Final Batch Loss: 0.17958325147628784\n",
      "Epoch 4894, Loss: 0.26391537487506866, Final Batch Loss: 0.17068053781986237\n",
      "Epoch 4895, Loss: 0.22964540868997574, Final Batch Loss: 0.11417528241872787\n",
      "Epoch 4896, Loss: 0.3256261348724365, Final Batch Loss: 0.21517588198184967\n",
      "Epoch 4897, Loss: 0.2709038481116295, Final Batch Loss: 0.155422180891037\n",
      "Epoch 4898, Loss: 0.26432908326387405, Final Batch Loss: 0.13990998268127441\n",
      "Epoch 4899, Loss: 0.2961770296096802, Final Batch Loss: 0.1289224475622177\n",
      "Epoch 4900, Loss: 0.3306417465209961, Final Batch Loss: 0.2010044902563095\n",
      "Epoch 4901, Loss: 0.32086868584156036, Final Batch Loss: 0.1806238740682602\n",
      "Epoch 4902, Loss: 0.3131493180990219, Final Batch Loss: 0.19219966232776642\n",
      "Epoch 4903, Loss: 0.28552553057670593, Final Batch Loss: 0.1560393124818802\n",
      "Epoch 4904, Loss: 0.2272190898656845, Final Batch Loss: 0.11351687461137772\n",
      "Epoch 4905, Loss: 0.2883494198322296, Final Batch Loss: 0.1375342756509781\n",
      "Epoch 4906, Loss: 0.2598789110779762, Final Batch Loss: 0.13504324853420258\n",
      "Epoch 4907, Loss: 0.23542137444019318, Final Batch Loss: 0.11255311965942383\n",
      "Epoch 4908, Loss: 0.26833050698041916, Final Batch Loss: 0.14490506052970886\n",
      "Epoch 4909, Loss: 0.28954487293958664, Final Batch Loss: 0.11281544715166092\n",
      "Epoch 4910, Loss: 0.2549806162714958, Final Batch Loss: 0.10791688412427902\n",
      "Epoch 4911, Loss: 0.2632277235388756, Final Batch Loss: 0.11903762072324753\n",
      "Epoch 4912, Loss: 0.24478857964277267, Final Batch Loss: 0.16259177029132843\n",
      "Epoch 4913, Loss: 0.23028453439474106, Final Batch Loss: 0.11293008923530579\n",
      "Epoch 4914, Loss: 0.2574140354990959, Final Batch Loss: 0.14708910882472992\n",
      "Epoch 4915, Loss: 0.26030953973531723, Final Batch Loss: 0.12257774919271469\n",
      "Epoch 4916, Loss: 0.2758401185274124, Final Batch Loss: 0.11463767290115356\n",
      "Epoch 4917, Loss: 0.2164292261004448, Final Batch Loss: 0.07412604242563248\n",
      "Epoch 4918, Loss: 0.24389080703258514, Final Batch Loss: 0.13992749154567719\n",
      "Epoch 4919, Loss: 0.23698758333921432, Final Batch Loss: 0.1263984888792038\n",
      "Epoch 4920, Loss: 0.2484656646847725, Final Batch Loss: 0.13681262731552124\n",
      "Epoch 4921, Loss: 0.254621721804142, Final Batch Loss: 0.13245797157287598\n",
      "Epoch 4922, Loss: 0.24592415243387222, Final Batch Loss: 0.14046813547611237\n",
      "Epoch 4923, Loss: 0.2635752558708191, Final Batch Loss: 0.12558889389038086\n",
      "Epoch 4924, Loss: 0.24805672466754913, Final Batch Loss: 0.12921270728111267\n",
      "Epoch 4925, Loss: 0.24683021008968353, Final Batch Loss: 0.10549464821815491\n",
      "Epoch 4926, Loss: 0.28862904012203217, Final Batch Loss: 0.12685950100421906\n",
      "Epoch 4927, Loss: 0.2566162347793579, Final Batch Loss: 0.1529081016778946\n",
      "Epoch 4928, Loss: 0.2386411800980568, Final Batch Loss: 0.11310570687055588\n",
      "Epoch 4929, Loss: 0.2404366061091423, Final Batch Loss: 0.09029000252485275\n",
      "Epoch 4930, Loss: 0.291248083114624, Final Batch Loss: 0.1346229463815689\n",
      "Epoch 4931, Loss: 0.24182149022817612, Final Batch Loss: 0.13195554912090302\n",
      "Epoch 4932, Loss: 0.2408961057662964, Final Batch Loss: 0.13091187179088593\n",
      "Epoch 4933, Loss: 0.24484079331159592, Final Batch Loss: 0.12171425670385361\n",
      "Epoch 4934, Loss: 0.25330815464258194, Final Batch Loss: 0.14794422686100006\n",
      "Epoch 4935, Loss: 0.22984755784273148, Final Batch Loss: 0.10556911677122116\n",
      "Epoch 4936, Loss: 0.2681136652827263, Final Batch Loss: 0.15169258415699005\n",
      "Epoch 4937, Loss: 0.25130826979875565, Final Batch Loss: 0.14529329538345337\n",
      "Epoch 4938, Loss: 0.21562198549509048, Final Batch Loss: 0.11631268262863159\n",
      "Epoch 4939, Loss: 0.27489469945430756, Final Batch Loss: 0.14691239595413208\n",
      "Epoch 4940, Loss: 0.2662430927157402, Final Batch Loss: 0.1576249897480011\n",
      "Epoch 4941, Loss: 0.264600470662117, Final Batch Loss: 0.1289096176624298\n",
      "Epoch 4942, Loss: 0.24560247361660004, Final Batch Loss: 0.1129482090473175\n",
      "Epoch 4943, Loss: 0.25000182539224625, Final Batch Loss: 0.13554947078227997\n",
      "Epoch 4944, Loss: 0.24542757868766785, Final Batch Loss: 0.11706933379173279\n",
      "Epoch 4945, Loss: 0.21720444411039352, Final Batch Loss: 0.10911902785301208\n",
      "Epoch 4946, Loss: 0.23993608355522156, Final Batch Loss: 0.1327105015516281\n",
      "Epoch 4947, Loss: 0.30473751574754715, Final Batch Loss: 0.1927189975976944\n",
      "Epoch 4948, Loss: 0.2381230965256691, Final Batch Loss: 0.14975522458553314\n",
      "Epoch 4949, Loss: 0.23262128978967667, Final Batch Loss: 0.09081905335187912\n",
      "Epoch 4950, Loss: 0.26540205627679825, Final Batch Loss: 0.15381452441215515\n",
      "Epoch 4951, Loss: 0.23043757677078247, Final Batch Loss: 0.10549942404031754\n",
      "Epoch 4952, Loss: 0.23982375115156174, Final Batch Loss: 0.1117796078324318\n",
      "Epoch 4953, Loss: 0.21715649962425232, Final Batch Loss: 0.09538164734840393\n",
      "Epoch 4954, Loss: 0.2254190519452095, Final Batch Loss: 0.0955333486199379\n",
      "Epoch 4955, Loss: 0.2561389058828354, Final Batch Loss: 0.12826977670192719\n",
      "Epoch 4956, Loss: 0.24303414672613144, Final Batch Loss: 0.14150382578372955\n",
      "Epoch 4957, Loss: 0.24594741314649582, Final Batch Loss: 0.1076580062508583\n",
      "Epoch 4958, Loss: 0.25714248418807983, Final Batch Loss: 0.1353691965341568\n",
      "Epoch 4959, Loss: 0.26048626005649567, Final Batch Loss: 0.1313147246837616\n",
      "Epoch 4960, Loss: 0.272414855659008, Final Batch Loss: 0.16980881989002228\n",
      "Epoch 4961, Loss: 0.24720848351716995, Final Batch Loss: 0.11310141533613205\n",
      "Epoch 4962, Loss: 0.2662666514515877, Final Batch Loss: 0.17371819913387299\n",
      "Epoch 4963, Loss: 0.2336614802479744, Final Batch Loss: 0.11040988564491272\n",
      "Epoch 4964, Loss: 0.23616203665733337, Final Batch Loss: 0.0846841037273407\n",
      "Epoch 4965, Loss: 0.20661254227161407, Final Batch Loss: 0.07729767262935638\n",
      "Epoch 4966, Loss: 0.22370334714651108, Final Batch Loss: 0.08359494060277939\n",
      "Epoch 4967, Loss: 0.2610325962305069, Final Batch Loss: 0.13363422453403473\n",
      "Epoch 4968, Loss: 0.23815467208623886, Final Batch Loss: 0.12510015070438385\n",
      "Epoch 4969, Loss: 0.21897875517606735, Final Batch Loss: 0.11835592985153198\n",
      "Epoch 4970, Loss: 0.22162342816591263, Final Batch Loss: 0.11779067665338516\n",
      "Epoch 4971, Loss: 0.21906182914972305, Final Batch Loss: 0.12340307980775833\n",
      "Epoch 4972, Loss: 0.2728675305843353, Final Batch Loss: 0.14386333525180817\n",
      "Epoch 4973, Loss: 0.23446404188871384, Final Batch Loss: 0.12387051433324814\n",
      "Epoch 4974, Loss: 0.1971547231078148, Final Batch Loss: 0.07635346800088882\n",
      "Epoch 4975, Loss: 0.22944649308919907, Final Batch Loss: 0.10802733153104782\n",
      "Epoch 4976, Loss: 0.336208313703537, Final Batch Loss: 0.15446393191814423\n",
      "Epoch 4977, Loss: 0.23875492811203003, Final Batch Loss: 0.13723449409008026\n",
      "Epoch 4978, Loss: 0.22003988921642303, Final Batch Loss: 0.12184462696313858\n",
      "Epoch 4979, Loss: 0.2934018522500992, Final Batch Loss: 0.1907956749200821\n",
      "Epoch 4980, Loss: 0.23642050474882126, Final Batch Loss: 0.11914055794477463\n",
      "Epoch 4981, Loss: 0.2488381341099739, Final Batch Loss: 0.12948432564735413\n",
      "Epoch 4982, Loss: 0.28232812136411667, Final Batch Loss: 0.16596707701683044\n",
      "Epoch 4983, Loss: 0.2409399300813675, Final Batch Loss: 0.12513147294521332\n",
      "Epoch 4984, Loss: 0.2427188605070114, Final Batch Loss: 0.12268514186143875\n",
      "Epoch 4985, Loss: 0.22692330181598663, Final Batch Loss: 0.12422723323106766\n",
      "Epoch 4986, Loss: 0.3070700913667679, Final Batch Loss: 0.12618638575077057\n",
      "Epoch 4987, Loss: 0.2316548451781273, Final Batch Loss: 0.09730233997106552\n",
      "Epoch 4988, Loss: 0.21032633632421494, Final Batch Loss: 0.11880385130643845\n",
      "Epoch 4989, Loss: 0.2134864702820778, Final Batch Loss: 0.0964847132563591\n",
      "Epoch 4990, Loss: 0.21241668611764908, Final Batch Loss: 0.1040034368634224\n",
      "Epoch 4991, Loss: 0.22492683678865433, Final Batch Loss: 0.09932204335927963\n",
      "Epoch 4992, Loss: 0.23059789836406708, Final Batch Loss: 0.09005767107009888\n",
      "Epoch 4993, Loss: 0.20982573181390762, Final Batch Loss: 0.10152184218168259\n",
      "Epoch 4994, Loss: 0.22609156370162964, Final Batch Loss: 0.07622209191322327\n",
      "Epoch 4995, Loss: 0.2119203805923462, Final Batch Loss: 0.11812310665845871\n",
      "Epoch 4996, Loss: 0.24384158104658127, Final Batch Loss: 0.13096196949481964\n",
      "Epoch 4997, Loss: 0.23166754096746445, Final Batch Loss: 0.10357721894979477\n",
      "Epoch 4998, Loss: 0.2359641268849373, Final Batch Loss: 0.0946454182267189\n",
      "Epoch 4999, Loss: 0.24483512341976166, Final Batch Loss: 0.1258993148803711\n",
      "Epoch 5000, Loss: 0.2616257295012474, Final Batch Loss: 0.10737390071153641\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 33  0  0  0  2  0]\n",
      " [ 0 33  0  0  2  0  0  0  0]\n",
      " [ 3  0 32  0  0  0  0  0  0]\n",
      " [ 1  0  0 33  0  0  1  0  0]\n",
      " [ 0  0  0  0 35  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0 24]\n",
      " [ 0  0  0  0  0  0 35  0  0]\n",
      " [ 0  3  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  8  0  0 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.000     0.000     0.000        35\n",
      "         1.0      0.917     0.943     0.930        35\n",
      "         2.0      1.000     0.914     0.955        35\n",
      "         3.0      0.500     0.943     0.653        35\n",
      "         4.0      0.946     1.000     0.972        35\n",
      "         5.0      0.579     0.314     0.407        35\n",
      "         6.0      0.972     1.000     0.986        35\n",
      "         7.0      0.941     0.914     0.928        35\n",
      "         8.0      0.529     0.771     0.628        35\n",
      "\n",
      "    accuracy                          0.756       315\n",
      "   macro avg      0.709     0.756     0.718       315\n",
      "weighted avg      0.709     0.756     0.718       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET RID OF 2, 5, 8 for JUST DYNAMIC\n",
    "(0+0.930+0.653+0.972+0.986+0.928)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7176666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0+0.93+0.955+0.653+0.972+0.407+0.986+0.928+0.628)/9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
