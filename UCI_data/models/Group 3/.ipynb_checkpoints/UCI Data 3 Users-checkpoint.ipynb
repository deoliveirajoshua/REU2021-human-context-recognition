{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                 -0.976623              -0.976353  ...   \n",
       "1                 -0.989046              -0.989038  ...   \n",
       "2                 -0.993552              -0.994122  ...   \n",
       "3                 -0.992407              -0.993142  ...   \n",
       "4                 -0.992378              -0.992542  ...   \n",
       "...                     ...                    ...  ...   \n",
       "7347               0.084878               0.065142  ...   \n",
       "7348               0.098249               0.091791  ...   \n",
       "7349               0.185902               0.170686  ...   \n",
       "7350               0.190360               0.178939  ...   \n",
       "7351               0.022216              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                    -0.998285        1         5  \n",
       "1                    -0.999472        1         5  \n",
       "2                    -0.999807        1         5  \n",
       "3                    -0.999770        1         5  \n",
       "4                    -0.999873        1         5  \n",
       "...                        ...      ...       ...  \n",
       "7347                 -0.584282       30         2  \n",
       "7348                 -0.632536       30         2  \n",
       "7349                 -0.641170       30         2  \n",
       "7350                 -0.663579       30         2  \n",
       "7351                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([14, 15, 17])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([14, 15, 17])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 14:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 15:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.22450053691864, Final Batch Loss: 1.0983734130859375\n",
      "Epoch 2, Loss: 2.2207088470458984, Final Batch Loss: 1.1024796962738037\n",
      "Epoch 3, Loss: 2.215879440307617, Final Batch Loss: 1.103878378868103\n",
      "Epoch 4, Loss: 2.221069574356079, Final Batch Loss: 1.1167411804199219\n",
      "Epoch 5, Loss: 2.2180179357528687, Final Batch Loss: 1.1160398721694946\n",
      "Epoch 6, Loss: 2.20883572101593, Final Batch Loss: 1.084882378578186\n",
      "Epoch 7, Loss: 2.213125467300415, Final Batch Loss: 1.1046833992004395\n",
      "Epoch 8, Loss: 2.2096240520477295, Final Batch Loss: 1.107800841331482\n",
      "Epoch 9, Loss: 2.19993793964386, Final Batch Loss: 1.0912768840789795\n",
      "Epoch 10, Loss: 2.20457124710083, Final Batch Loss: 1.1052316427230835\n",
      "Epoch 11, Loss: 2.2037302255630493, Final Batch Loss: 1.1075506210327148\n",
      "Epoch 12, Loss: 2.1963528394699097, Final Batch Loss: 1.1006901264190674\n",
      "Epoch 13, Loss: 2.189265012741089, Final Batch Loss: 1.0866036415100098\n",
      "Epoch 14, Loss: 2.189354658126831, Final Batch Loss: 1.0949426889419556\n",
      "Epoch 15, Loss: 2.1840131282806396, Final Batch Loss: 1.0920830965042114\n",
      "Epoch 16, Loss: 2.182133436203003, Final Batch Loss: 1.0871323347091675\n",
      "Epoch 17, Loss: 2.179385542869568, Final Batch Loss: 1.0894190073013306\n",
      "Epoch 18, Loss: 2.171793222427368, Final Batch Loss: 1.0849865674972534\n",
      "Epoch 19, Loss: 2.167081117630005, Final Batch Loss: 1.0783627033233643\n",
      "Epoch 20, Loss: 2.163423180580139, Final Batch Loss: 1.0816949605941772\n",
      "Epoch 21, Loss: 2.1523358821868896, Final Batch Loss: 1.0746171474456787\n",
      "Epoch 22, Loss: 2.1422407627105713, Final Batch Loss: 1.065832495689392\n",
      "Epoch 23, Loss: 2.137669086456299, Final Batch Loss: 1.068817377090454\n",
      "Epoch 24, Loss: 2.1313804388046265, Final Batch Loss: 1.0669053792953491\n",
      "Epoch 25, Loss: 2.103273868560791, Final Batch Loss: 1.048599362373352\n",
      "Epoch 26, Loss: 2.0987693071365356, Final Batch Loss: 1.0418370962142944\n",
      "Epoch 27, Loss: 2.0988197326660156, Final Batch Loss: 1.053289771080017\n",
      "Epoch 28, Loss: 2.0736637115478516, Final Batch Loss: 1.0383950471878052\n",
      "Epoch 29, Loss: 2.0511218309402466, Final Batch Loss: 1.0282402038574219\n",
      "Epoch 30, Loss: 2.0339295864105225, Final Batch Loss: 1.020546793937683\n",
      "Epoch 31, Loss: 2.00948965549469, Final Batch Loss: 1.0095971822738647\n",
      "Epoch 32, Loss: 1.9910244345664978, Final Batch Loss: 0.9785084128379822\n",
      "Epoch 33, Loss: 1.9626532793045044, Final Batch Loss: 0.9662597179412842\n",
      "Epoch 34, Loss: 1.949296474456787, Final Batch Loss: 0.9759191870689392\n",
      "Epoch 35, Loss: 1.9199979901313782, Final Batch Loss: 0.9631933569908142\n",
      "Epoch 36, Loss: 1.912169635295868, Final Batch Loss: 0.9837409257888794\n",
      "Epoch 37, Loss: 1.8568950295448303, Final Batch Loss: 0.9219565987586975\n",
      "Epoch 38, Loss: 1.8282081484794617, Final Batch Loss: 0.9240102767944336\n",
      "Epoch 39, Loss: 1.7835763096809387, Final Batch Loss: 0.8942415118217468\n",
      "Epoch 40, Loss: 1.7693068981170654, Final Batch Loss: 0.8913688063621521\n",
      "Epoch 41, Loss: 1.7457393407821655, Final Batch Loss: 0.8633823990821838\n",
      "Epoch 42, Loss: 1.6963037848472595, Final Batch Loss: 0.8429224491119385\n",
      "Epoch 43, Loss: 1.6922571063041687, Final Batch Loss: 0.8541174530982971\n",
      "Epoch 44, Loss: 1.6883384585380554, Final Batch Loss: 0.8464921116828918\n",
      "Epoch 45, Loss: 1.6687162518501282, Final Batch Loss: 0.8650583028793335\n",
      "Epoch 46, Loss: 1.638402283191681, Final Batch Loss: 0.8376586437225342\n",
      "Epoch 47, Loss: 1.5957996249198914, Final Batch Loss: 0.7584016919136047\n",
      "Epoch 48, Loss: 1.5696104168891907, Final Batch Loss: 0.7632880806922913\n",
      "Epoch 49, Loss: 1.6212123036384583, Final Batch Loss: 0.8175100088119507\n",
      "Epoch 50, Loss: 1.5708872079849243, Final Batch Loss: 0.7987972497940063\n",
      "Epoch 51, Loss: 1.555116593837738, Final Batch Loss: 0.7342724800109863\n",
      "Epoch 52, Loss: 1.5543055534362793, Final Batch Loss: 0.7983976602554321\n",
      "Epoch 53, Loss: 1.503850281238556, Final Batch Loss: 0.7058781981468201\n",
      "Epoch 54, Loss: 1.5096784830093384, Final Batch Loss: 0.7142003178596497\n",
      "Epoch 55, Loss: 1.4921062588691711, Final Batch Loss: 0.7188030481338501\n",
      "Epoch 56, Loss: 1.5048702359199524, Final Batch Loss: 0.7624951004981995\n",
      "Epoch 57, Loss: 1.4850065112113953, Final Batch Loss: 0.691054105758667\n",
      "Epoch 58, Loss: 1.4884294271469116, Final Batch Loss: 0.7052665948867798\n",
      "Epoch 59, Loss: 1.4949567317962646, Final Batch Loss: 0.792631983757019\n",
      "Epoch 60, Loss: 1.4595165848731995, Final Batch Loss: 0.7300348877906799\n",
      "Epoch 61, Loss: 1.4567415714263916, Final Batch Loss: 0.6935913562774658\n",
      "Epoch 62, Loss: 1.4260966777801514, Final Batch Loss: 0.7092124223709106\n",
      "Epoch 63, Loss: 1.4561564326286316, Final Batch Loss: 0.761863112449646\n",
      "Epoch 64, Loss: 1.407055675983429, Final Batch Loss: 0.6759446263313293\n",
      "Epoch 65, Loss: 1.4352655410766602, Final Batch Loss: 0.7339829802513123\n",
      "Epoch 66, Loss: 1.4194287657737732, Final Batch Loss: 0.680387556552887\n",
      "Epoch 67, Loss: 1.4050964713096619, Final Batch Loss: 0.7261133790016174\n",
      "Epoch 68, Loss: 1.4027093648910522, Final Batch Loss: 0.7222540378570557\n",
      "Epoch 69, Loss: 1.37715482711792, Final Batch Loss: 0.6870342493057251\n",
      "Epoch 70, Loss: 1.3592082858085632, Final Batch Loss: 0.6644018888473511\n",
      "Epoch 71, Loss: 1.3762143850326538, Final Batch Loss: 0.6760259866714478\n",
      "Epoch 72, Loss: 1.3685815930366516, Final Batch Loss: 0.6837842464447021\n",
      "Epoch 73, Loss: 1.361216127872467, Final Batch Loss: 0.7130727171897888\n",
      "Epoch 74, Loss: 1.3596746325492859, Final Batch Loss: 0.694621741771698\n",
      "Epoch 75, Loss: 1.2681787610054016, Final Batch Loss: 0.5828267335891724\n",
      "Epoch 76, Loss: 1.3223634362220764, Final Batch Loss: 0.6841104626655579\n",
      "Epoch 77, Loss: 1.3273009061813354, Final Batch Loss: 0.6977077126502991\n",
      "Epoch 78, Loss: 1.3097200989723206, Final Batch Loss: 0.6752727627754211\n",
      "Epoch 79, Loss: 1.3281750679016113, Final Batch Loss: 0.7205590009689331\n",
      "Epoch 80, Loss: 1.2546501755714417, Final Batch Loss: 0.6003237962722778\n",
      "Epoch 81, Loss: 1.254772126674652, Final Batch Loss: 0.6348956823348999\n",
      "Epoch 82, Loss: 1.2270587682724, Final Batch Loss: 0.6006783246994019\n",
      "Epoch 83, Loss: 1.217168390750885, Final Batch Loss: 0.5967953205108643\n",
      "Epoch 84, Loss: 1.2181317806243896, Final Batch Loss: 0.5950773358345032\n",
      "Epoch 85, Loss: 1.2289848923683167, Final Batch Loss: 0.6169530749320984\n",
      "Epoch 86, Loss: 1.1585080027580261, Final Batch Loss: 0.5611044764518738\n",
      "Epoch 87, Loss: 1.1811978220939636, Final Batch Loss: 0.610983669757843\n",
      "Epoch 88, Loss: 1.1651982069015503, Final Batch Loss: 0.594694972038269\n",
      "Epoch 89, Loss: 1.1120816469192505, Final Batch Loss: 0.5907122492790222\n",
      "Epoch 90, Loss: 1.14019376039505, Final Batch Loss: 0.5994675755500793\n",
      "Epoch 91, Loss: 1.0916911363601685, Final Batch Loss: 0.5568214058876038\n",
      "Epoch 92, Loss: 1.103269100189209, Final Batch Loss: 0.5940897464752197\n",
      "Epoch 93, Loss: 1.0307117402553558, Final Batch Loss: 0.48631176352500916\n",
      "Epoch 94, Loss: 1.0196829736232758, Final Batch Loss: 0.4826219379901886\n",
      "Epoch 95, Loss: 1.0601169168949127, Final Batch Loss: 0.5611680746078491\n",
      "Epoch 96, Loss: 0.9736829102039337, Final Batch Loss: 0.4282977283000946\n",
      "Epoch 97, Loss: 1.0086817145347595, Final Batch Loss: 0.5570571422576904\n",
      "Epoch 98, Loss: 0.9928933680057526, Final Batch Loss: 0.452047735452652\n",
      "Epoch 99, Loss: 0.9364825785160065, Final Batch Loss: 0.4206016957759857\n",
      "Epoch 100, Loss: 0.9374279677867889, Final Batch Loss: 0.48091381788253784\n",
      "Epoch 101, Loss: 0.9121635854244232, Final Batch Loss: 0.4230726659297943\n",
      "Epoch 102, Loss: 0.9383143186569214, Final Batch Loss: 0.48170486092567444\n",
      "Epoch 103, Loss: 0.8948867321014404, Final Batch Loss: 0.44735386967658997\n",
      "Epoch 104, Loss: 0.8737553358078003, Final Batch Loss: 0.4056680202484131\n",
      "Epoch 105, Loss: 0.9062863290309906, Final Batch Loss: 0.4841669797897339\n",
      "Epoch 106, Loss: 0.8717334270477295, Final Batch Loss: 0.46683263778686523\n",
      "Epoch 107, Loss: 0.876339852809906, Final Batch Loss: 0.4611233174800873\n",
      "Epoch 108, Loss: 0.7782751619815826, Final Batch Loss: 0.33321356773376465\n",
      "Epoch 109, Loss: 0.8226958513259888, Final Batch Loss: 0.435198575258255\n",
      "Epoch 110, Loss: 0.7995243072509766, Final Batch Loss: 0.3835710287094116\n",
      "Epoch 111, Loss: 0.8109654486179352, Final Batch Loss: 0.4174457788467407\n",
      "Epoch 112, Loss: 0.8241656720638275, Final Batch Loss: 0.4225594103336334\n",
      "Epoch 113, Loss: 0.7652183771133423, Final Batch Loss: 0.3741561770439148\n",
      "Epoch 114, Loss: 0.7686614096164703, Final Batch Loss: 0.3844183683395386\n",
      "Epoch 115, Loss: 0.7760781943798065, Final Batch Loss: 0.41102954745292664\n",
      "Epoch 116, Loss: 0.7699987292289734, Final Batch Loss: 0.39817383885383606\n",
      "Epoch 117, Loss: 0.7322677671909332, Final Batch Loss: 0.37509268522262573\n",
      "Epoch 118, Loss: 0.7879380285739899, Final Batch Loss: 0.4237799346446991\n",
      "Epoch 119, Loss: 0.7393887341022491, Final Batch Loss: 0.37675222754478455\n",
      "Epoch 120, Loss: 0.7415585815906525, Final Batch Loss: 0.3726854622364044\n",
      "Epoch 121, Loss: 0.7153357565402985, Final Batch Loss: 0.36671510338783264\n",
      "Epoch 122, Loss: 0.7216745018959045, Final Batch Loss: 0.33088117837905884\n",
      "Epoch 123, Loss: 0.6898770034313202, Final Batch Loss: 0.31448015570640564\n",
      "Epoch 124, Loss: 0.6760070323944092, Final Batch Loss: 0.32839763164520264\n",
      "Epoch 125, Loss: 0.6764666736125946, Final Batch Loss: 0.32481545209884644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: 0.6607701480388641, Final Batch Loss: 0.36976709961891174\n",
      "Epoch 127, Loss: 0.6712877452373505, Final Batch Loss: 0.3469317555427551\n",
      "Epoch 128, Loss: 0.6579418182373047, Final Batch Loss: 0.3464471697807312\n",
      "Epoch 129, Loss: 0.6767031848430634, Final Batch Loss: 0.317008912563324\n",
      "Epoch 130, Loss: 0.6512006223201752, Final Batch Loss: 0.2916310429573059\n",
      "Epoch 131, Loss: 0.6594046354293823, Final Batch Loss: 0.34220242500305176\n",
      "Epoch 132, Loss: 0.6315808594226837, Final Batch Loss: 0.29137539863586426\n",
      "Epoch 133, Loss: 0.6505596041679382, Final Batch Loss: 0.33461710810661316\n",
      "Epoch 134, Loss: 0.622408539056778, Final Batch Loss: 0.28551605343818665\n",
      "Epoch 135, Loss: 0.6547598242759705, Final Batch Loss: 0.31861138343811035\n",
      "Epoch 136, Loss: 0.6910070478916168, Final Batch Loss: 0.39553186297416687\n",
      "Epoch 137, Loss: 0.6056554317474365, Final Batch Loss: 0.27407306432724\n",
      "Epoch 138, Loss: 0.6489114761352539, Final Batch Loss: 0.35907256603240967\n",
      "Epoch 139, Loss: 0.6203003525733948, Final Batch Loss: 0.29801496863365173\n",
      "Epoch 140, Loss: 0.5950490236282349, Final Batch Loss: 0.255367249250412\n",
      "Epoch 141, Loss: 0.5917616486549377, Final Batch Loss: 0.3059127926826477\n",
      "Epoch 142, Loss: 0.6145434379577637, Final Batch Loss: 0.28235626220703125\n",
      "Epoch 143, Loss: 0.5696904361248016, Final Batch Loss: 0.28290361166000366\n",
      "Epoch 144, Loss: 0.6298898160457611, Final Batch Loss: 0.35967737436294556\n",
      "Epoch 145, Loss: 0.6603725254535675, Final Batch Loss: 0.35938721895217896\n",
      "Epoch 146, Loss: 0.6179451644420624, Final Batch Loss: 0.31152835488319397\n",
      "Epoch 147, Loss: 0.6510191857814789, Final Batch Loss: 0.32057449221611023\n",
      "Epoch 148, Loss: 0.5836998522281647, Final Batch Loss: 0.3201613128185272\n",
      "Epoch 149, Loss: 0.5933682322502136, Final Batch Loss: 0.2603479325771332\n",
      "Epoch 150, Loss: 0.601747065782547, Final Batch Loss: 0.30074557662010193\n",
      "Epoch 151, Loss: 0.6047496497631073, Final Batch Loss: 0.2796913981437683\n",
      "Epoch 152, Loss: 0.6474955379962921, Final Batch Loss: 0.3259738087654114\n",
      "Epoch 153, Loss: 0.6121348738670349, Final Batch Loss: 0.3127060830593109\n",
      "Epoch 154, Loss: 0.5898807942867279, Final Batch Loss: 0.2711191177368164\n",
      "Epoch 155, Loss: 0.5603628605604172, Final Batch Loss: 0.24541781842708588\n",
      "Epoch 156, Loss: 0.5965034067630768, Final Batch Loss: 0.27502813935279846\n",
      "Epoch 157, Loss: 0.6118792295455933, Final Batch Loss: 0.3252888023853302\n",
      "Epoch 158, Loss: 0.5605608224868774, Final Batch Loss: 0.2682434916496277\n",
      "Epoch 159, Loss: 0.5790403485298157, Final Batch Loss: 0.2901672422885895\n",
      "Epoch 160, Loss: 0.5561219453811646, Final Batch Loss: 0.2644217014312744\n",
      "Epoch 161, Loss: 0.5958995819091797, Final Batch Loss: 0.3334093987941742\n",
      "Epoch 162, Loss: 0.5639828741550446, Final Batch Loss: 0.26293420791625977\n",
      "Epoch 163, Loss: 0.5979611277580261, Final Batch Loss: 0.3443623185157776\n",
      "Epoch 164, Loss: 0.5590352267026901, Final Batch Loss: 0.24810735881328583\n",
      "Epoch 165, Loss: 0.5706789493560791, Final Batch Loss: 0.28941968083381653\n",
      "Epoch 166, Loss: 0.5816168487071991, Final Batch Loss: 0.27914682030677795\n",
      "Epoch 167, Loss: 0.5460291653871536, Final Batch Loss: 0.22158367931842804\n",
      "Epoch 168, Loss: 0.5379129350185394, Final Batch Loss: 0.2581590712070465\n",
      "Epoch 169, Loss: 0.5374520570039749, Final Batch Loss: 0.23008687794208527\n",
      "Epoch 170, Loss: 0.5406624227762222, Final Batch Loss: 0.24456192553043365\n",
      "Epoch 171, Loss: 0.5703941583633423, Final Batch Loss: 0.29934486746788025\n",
      "Epoch 172, Loss: 0.5575032234191895, Final Batch Loss: 0.280548095703125\n",
      "Epoch 173, Loss: 0.5733793675899506, Final Batch Loss: 0.3171754479408264\n",
      "Epoch 174, Loss: 0.5803425461053848, Final Batch Loss: 0.34523263573646545\n",
      "Epoch 175, Loss: 0.49480272829532623, Final Batch Loss: 0.1879643350839615\n",
      "Epoch 176, Loss: 0.5205531716346741, Final Batch Loss: 0.24018779397010803\n",
      "Epoch 177, Loss: 0.5661381036043167, Final Batch Loss: 0.3364511728286743\n",
      "Epoch 178, Loss: 0.5540139526128769, Final Batch Loss: 0.20860154926776886\n",
      "Epoch 179, Loss: 0.5423453450202942, Final Batch Loss: 0.2513172924518585\n",
      "Epoch 180, Loss: 0.5319266021251678, Final Batch Loss: 0.28485769033432007\n",
      "Epoch 181, Loss: 0.5371692776679993, Final Batch Loss: 0.26322534680366516\n",
      "Epoch 182, Loss: 0.5502552092075348, Final Batch Loss: 0.2693464457988739\n",
      "Epoch 183, Loss: 0.5330945253372192, Final Batch Loss: 0.2739226818084717\n",
      "Epoch 184, Loss: 0.5560612082481384, Final Batch Loss: 0.28153568506240845\n",
      "Epoch 185, Loss: 0.561860129237175, Final Batch Loss: 0.3222521245479584\n",
      "Epoch 186, Loss: 0.5813129246234894, Final Batch Loss: 0.34258130192756653\n",
      "Epoch 187, Loss: 0.5169871151447296, Final Batch Loss: 0.2641100585460663\n",
      "Epoch 188, Loss: 0.541875034570694, Final Batch Loss: 0.2780817747116089\n",
      "Epoch 189, Loss: 0.5163377970457077, Final Batch Loss: 0.247369185090065\n",
      "Epoch 190, Loss: 0.5280382633209229, Final Batch Loss: 0.22092774510383606\n",
      "Epoch 191, Loss: 0.519721657037735, Final Batch Loss: 0.23432263731956482\n",
      "Epoch 192, Loss: 0.537939190864563, Final Batch Loss: 0.25374510884284973\n",
      "Epoch 193, Loss: 0.5038005858659744, Final Batch Loss: 0.23275451362133026\n",
      "Epoch 194, Loss: 0.5299824178218842, Final Batch Loss: 0.2585775554180145\n",
      "Epoch 195, Loss: 0.5113110393285751, Final Batch Loss: 0.2205212265253067\n",
      "Epoch 196, Loss: 0.5313501507043839, Final Batch Loss: 0.28404080867767334\n",
      "Epoch 197, Loss: 0.5415406823158264, Final Batch Loss: 0.286590576171875\n",
      "Epoch 198, Loss: 0.5315447002649307, Final Batch Loss: 0.2958122491836548\n",
      "Epoch 199, Loss: 0.5696671903133392, Final Batch Loss: 0.280501127243042\n",
      "Epoch 200, Loss: 0.5425236821174622, Final Batch Loss: 0.29217612743377686\n",
      "Epoch 201, Loss: 0.5144090056419373, Final Batch Loss: 0.2954426407814026\n",
      "Epoch 202, Loss: 0.5045112669467926, Final Batch Loss: 0.24066755175590515\n",
      "Epoch 203, Loss: 0.5121354013681412, Final Batch Loss: 0.22186513245105743\n",
      "Epoch 204, Loss: 0.49452701210975647, Final Batch Loss: 0.21743923425674438\n",
      "Epoch 205, Loss: 0.519861251115799, Final Batch Loss: 0.29867398738861084\n",
      "Epoch 206, Loss: 0.4973776787519455, Final Batch Loss: 0.24169601500034332\n",
      "Epoch 207, Loss: 0.5493903756141663, Final Batch Loss: 0.29156383872032166\n",
      "Epoch 208, Loss: 0.4923016279935837, Final Batch Loss: 0.2630558907985687\n",
      "Epoch 209, Loss: 0.5283762812614441, Final Batch Loss: 0.2711576223373413\n",
      "Epoch 210, Loss: 0.5092596858739853, Final Batch Loss: 0.2200278490781784\n",
      "Epoch 211, Loss: 0.5014129728078842, Final Batch Loss: 0.25759413838386536\n",
      "Epoch 212, Loss: 0.4928705394268036, Final Batch Loss: 0.23471832275390625\n",
      "Epoch 213, Loss: 0.5500299334526062, Final Batch Loss: 0.3110519051551819\n",
      "Epoch 214, Loss: 0.5608673840761185, Final Batch Loss: 0.3167726397514343\n",
      "Epoch 215, Loss: 0.4907001405954361, Final Batch Loss: 0.2643308937549591\n",
      "Epoch 216, Loss: 0.5008333027362823, Final Batch Loss: 0.23907166719436646\n",
      "Epoch 217, Loss: 0.5227591246366501, Final Batch Loss: 0.2319611757993698\n",
      "Epoch 218, Loss: 0.5071956068277359, Final Batch Loss: 0.22566230595111847\n",
      "Epoch 219, Loss: 0.5135847926139832, Final Batch Loss: 0.25137442350387573\n",
      "Epoch 220, Loss: 0.49968221783638, Final Batch Loss: 0.21685385704040527\n",
      "Epoch 221, Loss: 0.5479755699634552, Final Batch Loss: 0.2685948312282562\n",
      "Epoch 222, Loss: 0.49337251484394073, Final Batch Loss: 0.21859319508075714\n",
      "Epoch 223, Loss: 0.49924027919769287, Final Batch Loss: 0.25889530777931213\n",
      "Epoch 224, Loss: 0.5186588764190674, Final Batch Loss: 0.29765626788139343\n",
      "Epoch 225, Loss: 0.5317918062210083, Final Batch Loss: 0.25899916887283325\n",
      "Epoch 226, Loss: 0.5050088763237, Final Batch Loss: 0.24459350109100342\n",
      "Epoch 227, Loss: 0.48973533511161804, Final Batch Loss: 0.2507654130458832\n",
      "Epoch 228, Loss: 0.5109880417585373, Final Batch Loss: 0.24717025458812714\n",
      "Epoch 229, Loss: 0.5262041687965393, Final Batch Loss: 0.2859228551387787\n",
      "Epoch 230, Loss: 0.5331270545721054, Final Batch Loss: 0.3331047296524048\n",
      "Epoch 231, Loss: 0.4860358089208603, Final Batch Loss: 0.24202145636081696\n",
      "Epoch 232, Loss: 0.49824903905391693, Final Batch Loss: 0.26262974739074707\n",
      "Epoch 233, Loss: 0.5260973274707794, Final Batch Loss: 0.2698798179626465\n",
      "Epoch 234, Loss: 0.47521768510341644, Final Batch Loss: 0.20705099403858185\n",
      "Epoch 235, Loss: 0.5264308005571365, Final Batch Loss: 0.2807413339614868\n",
      "Epoch 236, Loss: 0.5042576193809509, Final Batch Loss: 0.24710997939109802\n",
      "Epoch 237, Loss: 0.5142092853784561, Final Batch Loss: 0.29755041003227234\n",
      "Epoch 238, Loss: 0.5111005306243896, Final Batch Loss: 0.2544828951358795\n",
      "Epoch 239, Loss: 0.5051868557929993, Final Batch Loss: 0.2733002305030823\n",
      "Epoch 240, Loss: 0.5292295962572098, Final Batch Loss: 0.2911020517349243\n",
      "Epoch 241, Loss: 0.49242421984672546, Final Batch Loss: 0.27090564370155334\n",
      "Epoch 242, Loss: 0.477073073387146, Final Batch Loss: 0.2313077449798584\n",
      "Epoch 243, Loss: 0.5004939138889313, Final Batch Loss: 0.27158984541893005\n",
      "Epoch 244, Loss: 0.4945456087589264, Final Batch Loss: 0.2166639268398285\n",
      "Epoch 245, Loss: 0.4598690867424011, Final Batch Loss: 0.23548275232315063\n",
      "Epoch 246, Loss: 0.4837152659893036, Final Batch Loss: 0.23524408042430878\n",
      "Epoch 247, Loss: 0.48809853196144104, Final Batch Loss: 0.28052544593811035\n",
      "Epoch 248, Loss: 0.5309931486845016, Final Batch Loss: 0.3172653615474701\n",
      "Epoch 249, Loss: 0.49071213603019714, Final Batch Loss: 0.23816382884979248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250, Loss: 0.48753610253334045, Final Batch Loss: 0.23611193895339966\n",
      "Epoch 251, Loss: 0.4480838030576706, Final Batch Loss: 0.20198115706443787\n",
      "Epoch 252, Loss: 0.4690275192260742, Final Batch Loss: 0.2298223078250885\n",
      "Epoch 253, Loss: 0.5242850482463837, Final Batch Loss: 0.2862604558467865\n",
      "Epoch 254, Loss: 0.4912642389535904, Final Batch Loss: 0.23919744789600372\n",
      "Epoch 255, Loss: 0.5036170035600662, Final Batch Loss: 0.2664923369884491\n",
      "Epoch 256, Loss: 0.49873991310596466, Final Batch Loss: 0.2398587316274643\n",
      "Epoch 257, Loss: 0.5004874914884567, Final Batch Loss: 0.26470479369163513\n",
      "Epoch 258, Loss: 0.49665066599845886, Final Batch Loss: 0.25989800691604614\n",
      "Epoch 259, Loss: 0.4474372863769531, Final Batch Loss: 0.20316392183303833\n",
      "Epoch 260, Loss: 0.4906936436891556, Final Batch Loss: 0.2210354059934616\n",
      "Epoch 261, Loss: 0.46905888617038727, Final Batch Loss: 0.19913490116596222\n",
      "Epoch 262, Loss: 0.46576640009880066, Final Batch Loss: 0.23238059878349304\n",
      "Epoch 263, Loss: 0.48665378987789154, Final Batch Loss: 0.20219267904758453\n",
      "Epoch 264, Loss: 0.5065641403198242, Final Batch Loss: 0.25217974185943604\n",
      "Epoch 265, Loss: 0.47537726163864136, Final Batch Loss: 0.2467675507068634\n",
      "Epoch 266, Loss: 0.512007474899292, Final Batch Loss: 0.2510012090206146\n",
      "Epoch 267, Loss: 0.46656540036201477, Final Batch Loss: 0.2227638214826584\n",
      "Epoch 268, Loss: 0.47939033806324005, Final Batch Loss: 0.22567535936832428\n",
      "Epoch 269, Loss: 0.45588500797748566, Final Batch Loss: 0.23729577660560608\n",
      "Epoch 270, Loss: 0.4689590036869049, Final Batch Loss: 0.2290462702512741\n",
      "Epoch 271, Loss: 0.46417492628097534, Final Batch Loss: 0.23316188156604767\n",
      "Epoch 272, Loss: 0.45746825635433197, Final Batch Loss: 0.17717738449573517\n",
      "Epoch 273, Loss: 0.48353801667690277, Final Batch Loss: 0.2667601704597473\n",
      "Epoch 274, Loss: 0.4637286961078644, Final Batch Loss: 0.24594035744667053\n",
      "Epoch 275, Loss: 0.47478289902210236, Final Batch Loss: 0.2477932870388031\n",
      "Epoch 276, Loss: 0.46762602031230927, Final Batch Loss: 0.21868696808815002\n",
      "Epoch 277, Loss: 0.4809698909521103, Final Batch Loss: 0.2630138099193573\n",
      "Epoch 278, Loss: 0.458070769906044, Final Batch Loss: 0.22965441644191742\n",
      "Epoch 279, Loss: 0.4793870896100998, Final Batch Loss: 0.237336203455925\n",
      "Epoch 280, Loss: 0.4808334708213806, Final Batch Loss: 0.22554099559783936\n",
      "Epoch 281, Loss: 0.5032430440187454, Final Batch Loss: 0.23262940347194672\n",
      "Epoch 282, Loss: 0.46685996651649475, Final Batch Loss: 0.22052070498466492\n",
      "Epoch 283, Loss: 0.45557649433612823, Final Batch Loss: 0.20542533695697784\n",
      "Epoch 284, Loss: 0.4520364999771118, Final Batch Loss: 0.17694103717803955\n",
      "Epoch 285, Loss: 0.5257274806499481, Final Batch Loss: 0.3214516341686249\n",
      "Epoch 286, Loss: 0.45500753819942474, Final Batch Loss: 0.23016522824764252\n",
      "Epoch 287, Loss: 0.5054304003715515, Final Batch Loss: 0.24666598439216614\n",
      "Epoch 288, Loss: 0.45112061500549316, Final Batch Loss: 0.2484995722770691\n",
      "Epoch 289, Loss: 0.4543151706457138, Final Batch Loss: 0.238863006234169\n",
      "Epoch 290, Loss: 0.4464631676673889, Final Batch Loss: 0.2579614222049713\n",
      "Epoch 291, Loss: 0.44308221340179443, Final Batch Loss: 0.21543140709400177\n",
      "Epoch 292, Loss: 0.448655441403389, Final Batch Loss: 0.2134578824043274\n",
      "Epoch 293, Loss: 0.43689820170402527, Final Batch Loss: 0.21785607933998108\n",
      "Epoch 294, Loss: 0.4550402909517288, Final Batch Loss: 0.22335690259933472\n",
      "Epoch 295, Loss: 0.46525508165359497, Final Batch Loss: 0.24280308187007904\n",
      "Epoch 296, Loss: 0.47800976037979126, Final Batch Loss: 0.22179079055786133\n",
      "Epoch 297, Loss: 0.4737212508916855, Final Batch Loss: 0.25926417112350464\n",
      "Epoch 298, Loss: 0.48040151596069336, Final Batch Loss: 0.25175341963768005\n",
      "Epoch 299, Loss: 0.4430481940507889, Final Batch Loss: 0.18918435275554657\n",
      "Epoch 300, Loss: 0.4728683680295944, Final Batch Loss: 0.23155753314495087\n",
      "Epoch 301, Loss: 0.43383054435253143, Final Batch Loss: 0.2342330813407898\n",
      "Epoch 302, Loss: 0.45231184363365173, Final Batch Loss: 0.25054946541786194\n",
      "Epoch 303, Loss: 0.4178464263677597, Final Batch Loss: 0.20366619527339935\n",
      "Epoch 304, Loss: 0.44109365344047546, Final Batch Loss: 0.23046410083770752\n",
      "Epoch 305, Loss: 0.4355253130197525, Final Batch Loss: 0.21642930805683136\n",
      "Epoch 306, Loss: 0.4579766094684601, Final Batch Loss: 0.25374069809913635\n",
      "Epoch 307, Loss: 0.42833128571510315, Final Batch Loss: 0.197017103433609\n",
      "Epoch 308, Loss: 0.4473976492881775, Final Batch Loss: 0.2356141358613968\n",
      "Epoch 309, Loss: 0.45343711972236633, Final Batch Loss: 0.231967031955719\n",
      "Epoch 310, Loss: 0.4323374181985855, Final Batch Loss: 0.2015645056962967\n",
      "Epoch 311, Loss: 0.4375530034303665, Final Batch Loss: 0.20675458014011383\n",
      "Epoch 312, Loss: 0.44002628326416016, Final Batch Loss: 0.22672604024410248\n",
      "Epoch 313, Loss: 0.4182804375886917, Final Batch Loss: 0.1737438589334488\n",
      "Epoch 314, Loss: 0.4010597765445709, Final Batch Loss: 0.1639009267091751\n",
      "Epoch 315, Loss: 0.4706447124481201, Final Batch Loss: 0.22713403403759003\n",
      "Epoch 316, Loss: 0.4313569664955139, Final Batch Loss: 0.20724792778491974\n",
      "Epoch 317, Loss: 0.45884808897972107, Final Batch Loss: 0.23320241272449493\n",
      "Epoch 318, Loss: 0.4610902667045593, Final Batch Loss: 0.2351067066192627\n",
      "Epoch 319, Loss: 0.4381338953971863, Final Batch Loss: 0.23438873887062073\n",
      "Epoch 320, Loss: 0.46426713466644287, Final Batch Loss: 0.23181739449501038\n",
      "Epoch 321, Loss: 0.5108496844768524, Final Batch Loss: 0.3019592761993408\n",
      "Epoch 322, Loss: 0.4348098635673523, Final Batch Loss: 0.22535523772239685\n",
      "Epoch 323, Loss: 0.41367077827453613, Final Batch Loss: 0.16967041790485382\n",
      "Epoch 324, Loss: 0.3942604660987854, Final Batch Loss: 0.1491887867450714\n",
      "Epoch 325, Loss: 0.4585845321416855, Final Batch Loss: 0.22702930867671967\n",
      "Epoch 326, Loss: 0.423080250620842, Final Batch Loss: 0.17491275072097778\n",
      "Epoch 327, Loss: 0.42691487073898315, Final Batch Loss: 0.1909024715423584\n",
      "Epoch 328, Loss: 0.45970770716667175, Final Batch Loss: 0.2050458788871765\n",
      "Epoch 329, Loss: 0.4534313380718231, Final Batch Loss: 0.20902785658836365\n",
      "Epoch 330, Loss: 0.38745614886283875, Final Batch Loss: 0.1474423110485077\n",
      "Epoch 331, Loss: 0.4648757725954056, Final Batch Loss: 0.24478964507579803\n",
      "Epoch 332, Loss: 0.4282371401786804, Final Batch Loss: 0.22180652618408203\n",
      "Epoch 333, Loss: 0.4426426291465759, Final Batch Loss: 0.2532747983932495\n",
      "Epoch 334, Loss: 0.4345052093267441, Final Batch Loss: 0.20746679604053497\n",
      "Epoch 335, Loss: 0.40204353630542755, Final Batch Loss: 0.17963209748268127\n",
      "Epoch 336, Loss: 0.409403532743454, Final Batch Loss: 0.21138089895248413\n",
      "Epoch 337, Loss: 0.37346598505973816, Final Batch Loss: 0.16044121980667114\n",
      "Epoch 338, Loss: 0.4209621548652649, Final Batch Loss: 0.205134779214859\n",
      "Epoch 339, Loss: 0.41637998819351196, Final Batch Loss: 0.2090052217245102\n",
      "Epoch 340, Loss: 0.4273287206888199, Final Batch Loss: 0.23988622426986694\n",
      "Epoch 341, Loss: 0.42044244706630707, Final Batch Loss: 0.2358752191066742\n",
      "Epoch 342, Loss: 0.4627748876810074, Final Batch Loss: 0.2639825940132141\n",
      "Epoch 343, Loss: 0.44323280453681946, Final Batch Loss: 0.24285776913166046\n",
      "Epoch 344, Loss: 0.39482007920742035, Final Batch Loss: 0.19558848440647125\n",
      "Epoch 345, Loss: 0.4247499853372574, Final Batch Loss: 0.213314950466156\n",
      "Epoch 346, Loss: 0.41291965544223785, Final Batch Loss: 0.21480704843997955\n",
      "Epoch 347, Loss: 0.4096967726945877, Final Batch Loss: 0.19453470408916473\n",
      "Epoch 348, Loss: 0.40585842728614807, Final Batch Loss: 0.21861042082309723\n",
      "Epoch 349, Loss: 0.41523194313049316, Final Batch Loss: 0.26633262634277344\n",
      "Epoch 350, Loss: 0.39123138785362244, Final Batch Loss: 0.18959002196788788\n",
      "Epoch 351, Loss: 0.42637717723846436, Final Batch Loss: 0.161153644323349\n",
      "Epoch 352, Loss: 0.4009239673614502, Final Batch Loss: 0.16606885194778442\n",
      "Epoch 353, Loss: 0.4303516745567322, Final Batch Loss: 0.23166444897651672\n",
      "Epoch 354, Loss: 0.3859592229127884, Final Batch Loss: 0.19868363440036774\n",
      "Epoch 355, Loss: 0.42925359308719635, Final Batch Loss: 0.2261480987071991\n",
      "Epoch 356, Loss: 0.4025520235300064, Final Batch Loss: 0.1600358784198761\n",
      "Epoch 357, Loss: 0.4243888854980469, Final Batch Loss: 0.22601647675037384\n",
      "Epoch 358, Loss: 0.384894534945488, Final Batch Loss: 0.18753667175769806\n",
      "Epoch 359, Loss: 0.39505550265312195, Final Batch Loss: 0.1514700949192047\n",
      "Epoch 360, Loss: 0.3863827735185623, Final Batch Loss: 0.19331589341163635\n",
      "Epoch 361, Loss: 0.447436198592186, Final Batch Loss: 0.24147149920463562\n",
      "Epoch 362, Loss: 0.419124573469162, Final Batch Loss: 0.23757624626159668\n",
      "Epoch 363, Loss: 0.38276807963848114, Final Batch Loss: 0.18609586358070374\n",
      "Epoch 364, Loss: 0.41471976041793823, Final Batch Loss: 0.1708650141954422\n",
      "Epoch 365, Loss: 0.44150523841381073, Final Batch Loss: 0.22989824414253235\n",
      "Epoch 366, Loss: 0.41326628625392914, Final Batch Loss: 0.23257756233215332\n",
      "Epoch 367, Loss: 0.39277321100234985, Final Batch Loss: 0.21616515517234802\n",
      "Epoch 368, Loss: 0.3779282122850418, Final Batch Loss: 0.18205605447292328\n",
      "Epoch 369, Loss: 0.4339596629142761, Final Batch Loss: 0.22868523001670837\n",
      "Epoch 370, Loss: 0.39183542132377625, Final Batch Loss: 0.21317195892333984\n",
      "Epoch 371, Loss: 0.4152674674987793, Final Batch Loss: 0.2455887645483017\n",
      "Epoch 372, Loss: 0.4449913799762726, Final Batch Loss: 0.2296360731124878\n",
      "Epoch 373, Loss: 0.3915479779243469, Final Batch Loss: 0.19518382847309113\n",
      "Epoch 374, Loss: 0.42057690024375916, Final Batch Loss: 0.2244403213262558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375, Loss: 0.3813699334859848, Final Batch Loss: 0.18000493943691254\n",
      "Epoch 376, Loss: 0.4136319011449814, Final Batch Loss: 0.22226440906524658\n",
      "Epoch 377, Loss: 0.38981500267982483, Final Batch Loss: 0.1471756100654602\n",
      "Epoch 378, Loss: 0.3982960283756256, Final Batch Loss: 0.2102021872997284\n",
      "Epoch 379, Loss: 0.3848656415939331, Final Batch Loss: 0.17019987106323242\n",
      "Epoch 380, Loss: 0.419973686337471, Final Batch Loss: 0.18591105937957764\n",
      "Epoch 381, Loss: 0.3928476721048355, Final Batch Loss: 0.19542129337787628\n",
      "Epoch 382, Loss: 0.394065722823143, Final Batch Loss: 0.15726816654205322\n",
      "Epoch 383, Loss: 0.36600299179553986, Final Batch Loss: 0.1579042226076126\n",
      "Epoch 384, Loss: 0.34971070289611816, Final Batch Loss: 0.1812209188938141\n",
      "Epoch 385, Loss: 0.38219092786312103, Final Batch Loss: 0.17374944686889648\n",
      "Epoch 386, Loss: 0.36888720095157623, Final Batch Loss: 0.1277356594800949\n",
      "Epoch 387, Loss: 0.39311929047107697, Final Batch Loss: 0.22987039387226105\n",
      "Epoch 388, Loss: 0.41506361961364746, Final Batch Loss: 0.25565576553344727\n",
      "Epoch 389, Loss: 0.4028189480304718, Final Batch Loss: 0.22071054577827454\n",
      "Epoch 390, Loss: 0.38527002930641174, Final Batch Loss: 0.19485753774642944\n",
      "Epoch 391, Loss: 0.38769131898880005, Final Batch Loss: 0.19226300716400146\n",
      "Epoch 392, Loss: 0.3915520906448364, Final Batch Loss: 0.16529051959514618\n",
      "Epoch 393, Loss: 0.3893866539001465, Final Batch Loss: 0.1987077295780182\n",
      "Epoch 394, Loss: 0.39243675768375397, Final Batch Loss: 0.20990009605884552\n",
      "Epoch 395, Loss: 0.3863116353750229, Final Batch Loss: 0.20976124703884125\n",
      "Epoch 396, Loss: 0.3834453672170639, Final Batch Loss: 0.17277634143829346\n",
      "Epoch 397, Loss: 0.39678357541561127, Final Batch Loss: 0.20161493122577667\n",
      "Epoch 398, Loss: 0.37210455536842346, Final Batch Loss: 0.16676071286201477\n",
      "Epoch 399, Loss: 0.3918874114751816, Final Batch Loss: 0.1982414871454239\n",
      "Epoch 400, Loss: 0.42977750301361084, Final Batch Loss: 0.1878647804260254\n",
      "Epoch 401, Loss: 0.38116878271102905, Final Batch Loss: 0.19581791758537292\n",
      "Epoch 402, Loss: 0.35834188759326935, Final Batch Loss: 0.17858444154262543\n",
      "Epoch 403, Loss: 0.3867443799972534, Final Batch Loss: 0.2157180905342102\n",
      "Epoch 404, Loss: 0.3611193895339966, Final Batch Loss: 0.1866474449634552\n",
      "Epoch 405, Loss: 0.3634849935770035, Final Batch Loss: 0.17983201146125793\n",
      "Epoch 406, Loss: 0.37079840898513794, Final Batch Loss: 0.1815430223941803\n",
      "Epoch 407, Loss: 0.3667817562818527, Final Batch Loss: 0.17152336239814758\n",
      "Epoch 408, Loss: 0.4146224111318588, Final Batch Loss: 0.21603208780288696\n",
      "Epoch 409, Loss: 0.37702158093452454, Final Batch Loss: 0.20861491560935974\n",
      "Epoch 410, Loss: 0.40739354491233826, Final Batch Loss: 0.2160685807466507\n",
      "Epoch 411, Loss: 0.4133041799068451, Final Batch Loss: 0.21599984169006348\n",
      "Epoch 412, Loss: 0.38502031564712524, Final Batch Loss: 0.22716012597084045\n",
      "Epoch 413, Loss: 0.3992421329021454, Final Batch Loss: 0.2023332715034485\n",
      "Epoch 414, Loss: 0.3797442615032196, Final Batch Loss: 0.19759422540664673\n",
      "Epoch 415, Loss: 0.3845284879207611, Final Batch Loss: 0.1858895868062973\n",
      "Epoch 416, Loss: 0.3938637226819992, Final Batch Loss: 0.24154117703437805\n",
      "Epoch 417, Loss: 0.3910454511642456, Final Batch Loss: 0.22564946115016937\n",
      "Epoch 418, Loss: 0.3880216032266617, Final Batch Loss: 0.18571853637695312\n",
      "Epoch 419, Loss: 0.3659459352493286, Final Batch Loss: 0.2061947137117386\n",
      "Epoch 420, Loss: 0.35987964272499084, Final Batch Loss: 0.19597512483596802\n",
      "Epoch 421, Loss: 0.3676273971796036, Final Batch Loss: 0.18948806822299957\n",
      "Epoch 422, Loss: 0.33784477412700653, Final Batch Loss: 0.17586296796798706\n",
      "Epoch 423, Loss: 0.3752390742301941, Final Batch Loss: 0.1819603443145752\n",
      "Epoch 424, Loss: 0.3646239936351776, Final Batch Loss: 0.14000330865383148\n",
      "Epoch 425, Loss: 0.3576706796884537, Final Batch Loss: 0.16321343183517456\n",
      "Epoch 426, Loss: 0.3905544877052307, Final Batch Loss: 0.21127749979496002\n",
      "Epoch 427, Loss: 0.3763895183801651, Final Batch Loss: 0.20661838352680206\n",
      "Epoch 428, Loss: 0.3611132651567459, Final Batch Loss: 0.17382870614528656\n",
      "Epoch 429, Loss: 0.35950809717178345, Final Batch Loss: 0.15424807369709015\n",
      "Epoch 430, Loss: 0.40835241973400116, Final Batch Loss: 0.2115538865327835\n",
      "Epoch 431, Loss: 0.36844781041145325, Final Batch Loss: 0.16453048586845398\n",
      "Epoch 432, Loss: 0.37663017213344574, Final Batch Loss: 0.21880686283111572\n",
      "Epoch 433, Loss: 0.35709089040756226, Final Batch Loss: 0.19074487686157227\n",
      "Epoch 434, Loss: 0.3646443039178848, Final Batch Loss: 0.1840345710515976\n",
      "Epoch 435, Loss: 0.4036617577075958, Final Batch Loss: 0.15067610144615173\n",
      "Epoch 436, Loss: 0.37961092591285706, Final Batch Loss: 0.1977415531873703\n",
      "Epoch 437, Loss: 0.38151074945926666, Final Batch Loss: 0.18672876060009003\n",
      "Epoch 438, Loss: 0.3700410723686218, Final Batch Loss: 0.1848689764738083\n",
      "Epoch 439, Loss: 0.3798758089542389, Final Batch Loss: 0.18026240170001984\n",
      "Epoch 440, Loss: 0.3633573204278946, Final Batch Loss: 0.19738918542861938\n",
      "Epoch 441, Loss: 0.40426021814346313, Final Batch Loss: 0.21264776587486267\n",
      "Epoch 442, Loss: 0.34929750859737396, Final Batch Loss: 0.13075393438339233\n",
      "Epoch 443, Loss: 0.3707592338323593, Final Batch Loss: 0.21585975587368011\n",
      "Epoch 444, Loss: 0.36607517302036285, Final Batch Loss: 0.17966604232788086\n",
      "Epoch 445, Loss: 0.3223629593849182, Final Batch Loss: 0.12840603291988373\n",
      "Epoch 446, Loss: 0.37799203395843506, Final Batch Loss: 0.1836654245853424\n",
      "Epoch 447, Loss: 0.3566172868013382, Final Batch Loss: 0.16264528036117554\n",
      "Epoch 448, Loss: 0.34040145576000214, Final Batch Loss: 0.1801186352968216\n",
      "Epoch 449, Loss: 0.36839520931243896, Final Batch Loss: 0.19390146434307098\n",
      "Epoch 450, Loss: 0.3659082353115082, Final Batch Loss: 0.19837209582328796\n",
      "Epoch 451, Loss: 0.40386830270290375, Final Batch Loss: 0.18647843599319458\n",
      "Epoch 452, Loss: 0.4033423513174057, Final Batch Loss: 0.24426081776618958\n",
      "Epoch 453, Loss: 0.3580073416233063, Final Batch Loss: 0.16351057589054108\n",
      "Epoch 454, Loss: 0.4014923870563507, Final Batch Loss: 0.22968493402004242\n",
      "Epoch 455, Loss: 0.36963582038879395, Final Batch Loss: 0.1596258580684662\n",
      "Epoch 456, Loss: 0.3564608246088028, Final Batch Loss: 0.1970093548297882\n",
      "Epoch 457, Loss: 0.39095014333724976, Final Batch Loss: 0.21898284554481506\n",
      "Epoch 458, Loss: 0.38616518676280975, Final Batch Loss: 0.17604848742485046\n",
      "Epoch 459, Loss: 0.3493652492761612, Final Batch Loss: 0.19300274550914764\n",
      "Epoch 460, Loss: 0.362707257270813, Final Batch Loss: 0.21813903748989105\n",
      "Epoch 461, Loss: 0.3805354982614517, Final Batch Loss: 0.1991846263408661\n",
      "Epoch 462, Loss: 0.3518597483634949, Final Batch Loss: 0.17795975506305695\n",
      "Epoch 463, Loss: 0.35827650129795074, Final Batch Loss: 0.19013230502605438\n",
      "Epoch 464, Loss: 0.38032467663288116, Final Batch Loss: 0.18613876402378082\n",
      "Epoch 465, Loss: 0.368341788649559, Final Batch Loss: 0.19578656554222107\n",
      "Epoch 466, Loss: 0.34421423077583313, Final Batch Loss: 0.17576688528060913\n",
      "Epoch 467, Loss: 0.32102978229522705, Final Batch Loss: 0.1802947074174881\n",
      "Epoch 468, Loss: 0.37282198667526245, Final Batch Loss: 0.19719462096691132\n",
      "Epoch 469, Loss: 0.32848159968852997, Final Batch Loss: 0.13762123882770538\n",
      "Epoch 470, Loss: 0.39451444149017334, Final Batch Loss: 0.18547412753105164\n",
      "Epoch 471, Loss: 0.35348187386989594, Final Batch Loss: 0.15360896289348602\n",
      "Epoch 472, Loss: 0.3584647476673126, Final Batch Loss: 0.1903151571750641\n",
      "Epoch 473, Loss: 0.37896813452243805, Final Batch Loss: 0.20864373445510864\n",
      "Epoch 474, Loss: 0.32576610147953033, Final Batch Loss: 0.12718355655670166\n",
      "Epoch 475, Loss: 0.3508247137069702, Final Batch Loss: 0.14988373219966888\n",
      "Epoch 476, Loss: 0.3398251533508301, Final Batch Loss: 0.17640143632888794\n",
      "Epoch 477, Loss: 0.3581822067499161, Final Batch Loss: 0.14981037378311157\n",
      "Epoch 478, Loss: 0.33526530861854553, Final Batch Loss: 0.15846477448940277\n",
      "Epoch 479, Loss: 0.3549071103334427, Final Batch Loss: 0.18572209775447845\n",
      "Epoch 480, Loss: 0.37374627590179443, Final Batch Loss: 0.17403896152973175\n",
      "Epoch 481, Loss: 0.34074681997299194, Final Batch Loss: 0.16479627788066864\n",
      "Epoch 482, Loss: 0.33272092044353485, Final Batch Loss: 0.14617283642292023\n",
      "Epoch 483, Loss: 0.35473892092704773, Final Batch Loss: 0.1547355055809021\n",
      "Epoch 484, Loss: 0.3367529958486557, Final Batch Loss: 0.17982900142669678\n",
      "Epoch 485, Loss: 0.33549585938453674, Final Batch Loss: 0.15875126421451569\n",
      "Epoch 486, Loss: 0.3523259311914444, Final Batch Loss: 0.17594414949417114\n",
      "Epoch 487, Loss: 0.35040701925754547, Final Batch Loss: 0.18704916536808014\n",
      "Epoch 488, Loss: 0.29510048031806946, Final Batch Loss: 0.1239481121301651\n",
      "Epoch 489, Loss: 0.39516106247901917, Final Batch Loss: 0.1980213075876236\n",
      "Epoch 490, Loss: 0.3348265290260315, Final Batch Loss: 0.15000446140766144\n",
      "Epoch 491, Loss: 0.36101147532463074, Final Batch Loss: 0.20229828357696533\n",
      "Epoch 492, Loss: 0.3127717897295952, Final Batch Loss: 0.12438712269067764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 493, Loss: 0.3281349390745163, Final Batch Loss: 0.1412542164325714\n",
      "Epoch 494, Loss: 0.3091801106929779, Final Batch Loss: 0.1355922818183899\n",
      "Epoch 495, Loss: 0.36254703998565674, Final Batch Loss: 0.21674586832523346\n",
      "Epoch 496, Loss: 0.33327047526836395, Final Batch Loss: 0.18206821382045746\n",
      "Epoch 497, Loss: 0.3315650671720505, Final Batch Loss: 0.15995918214321136\n",
      "Epoch 498, Loss: 0.33445000648498535, Final Batch Loss: 0.1739923059940338\n",
      "Epoch 499, Loss: 0.35455183684825897, Final Batch Loss: 0.20513735711574554\n",
      "Epoch 500, Loss: 0.3229648917913437, Final Batch Loss: 0.1652287244796753\n",
      "Epoch 501, Loss: 0.3267805874347687, Final Batch Loss: 0.16945171356201172\n",
      "Epoch 502, Loss: 0.35889682173728943, Final Batch Loss: 0.20069579780101776\n",
      "Epoch 503, Loss: 0.3550795465707779, Final Batch Loss: 0.18662574887275696\n",
      "Epoch 504, Loss: 0.374119371175766, Final Batch Loss: 0.18566393852233887\n",
      "Epoch 505, Loss: 0.35478608310222626, Final Batch Loss: 0.18663839995861053\n",
      "Epoch 506, Loss: 0.36771924793720245, Final Batch Loss: 0.20472228527069092\n",
      "Epoch 507, Loss: 0.31049826741218567, Final Batch Loss: 0.1266353726387024\n",
      "Epoch 508, Loss: 0.31217987835407257, Final Batch Loss: 0.13316284120082855\n",
      "Epoch 509, Loss: 0.34825582802295685, Final Batch Loss: 0.19235993921756744\n",
      "Epoch 510, Loss: 0.34307800233364105, Final Batch Loss: 0.18107570707798004\n",
      "Epoch 511, Loss: 0.3205401748418808, Final Batch Loss: 0.1713281273841858\n",
      "Epoch 512, Loss: 0.3136547654867172, Final Batch Loss: 0.14745083451271057\n",
      "Epoch 513, Loss: 0.31816668808460236, Final Batch Loss: 0.17234903573989868\n",
      "Epoch 514, Loss: 0.3504990339279175, Final Batch Loss: 0.16615892946720123\n",
      "Epoch 515, Loss: 0.30314822494983673, Final Batch Loss: 0.12287285923957825\n",
      "Epoch 516, Loss: 0.3451569825410843, Final Batch Loss: 0.16828320920467377\n",
      "Epoch 517, Loss: 0.32103656232357025, Final Batch Loss: 0.14644967019557953\n",
      "Epoch 518, Loss: 0.3340837359428406, Final Batch Loss: 0.15709689259529114\n",
      "Epoch 519, Loss: 0.33487462997436523, Final Batch Loss: 0.17948436737060547\n",
      "Epoch 520, Loss: 0.3471206873655319, Final Batch Loss: 0.17170262336730957\n",
      "Epoch 521, Loss: 0.3158113360404968, Final Batch Loss: 0.1621481031179428\n",
      "Epoch 522, Loss: 0.31923602521419525, Final Batch Loss: 0.163489431142807\n",
      "Epoch 523, Loss: 0.34872613847255707, Final Batch Loss: 0.18149952590465546\n",
      "Epoch 524, Loss: 0.34212031960487366, Final Batch Loss: 0.1647590547800064\n",
      "Epoch 525, Loss: 0.30297936499118805, Final Batch Loss: 0.14762914180755615\n",
      "Epoch 526, Loss: 0.30842673033475876, Final Batch Loss: 0.1129983440041542\n",
      "Epoch 527, Loss: 0.32026173174381256, Final Batch Loss: 0.18171992897987366\n",
      "Epoch 528, Loss: 0.34761789441108704, Final Batch Loss: 0.15156859159469604\n",
      "Epoch 529, Loss: 0.31225359439849854, Final Batch Loss: 0.14698350429534912\n",
      "Epoch 530, Loss: 0.3353462368249893, Final Batch Loss: 0.15286622941493988\n",
      "Epoch 531, Loss: 0.35874317586421967, Final Batch Loss: 0.21591852605342865\n",
      "Epoch 532, Loss: 0.33330972492694855, Final Batch Loss: 0.1564466804265976\n",
      "Epoch 533, Loss: 0.3541957587003708, Final Batch Loss: 0.20376485586166382\n",
      "Epoch 534, Loss: 0.31816062331199646, Final Batch Loss: 0.16928480565547943\n",
      "Epoch 535, Loss: 0.344111368060112, Final Batch Loss: 0.15992259979248047\n",
      "Epoch 536, Loss: 0.3185502737760544, Final Batch Loss: 0.1529676467180252\n",
      "Epoch 537, Loss: 0.32639414072036743, Final Batch Loss: 0.15269385278224945\n",
      "Epoch 538, Loss: 0.3168122470378876, Final Batch Loss: 0.16417765617370605\n",
      "Epoch 539, Loss: 0.3346700370311737, Final Batch Loss: 0.17267628014087677\n",
      "Epoch 540, Loss: 0.35511548817157745, Final Batch Loss: 0.22681981325149536\n",
      "Epoch 541, Loss: 0.34009498357772827, Final Batch Loss: 0.15785998106002808\n",
      "Epoch 542, Loss: 0.2920673191547394, Final Batch Loss: 0.11537598073482513\n",
      "Epoch 543, Loss: 0.36147449910640717, Final Batch Loss: 0.18528051674365997\n",
      "Epoch 544, Loss: 0.3120127469301224, Final Batch Loss: 0.13185083866119385\n",
      "Epoch 545, Loss: 0.3183518648147583, Final Batch Loss: 0.16302168369293213\n",
      "Epoch 546, Loss: 0.3197536915540695, Final Batch Loss: 0.16648292541503906\n",
      "Epoch 547, Loss: 0.3738844394683838, Final Batch Loss: 0.19226741790771484\n",
      "Epoch 548, Loss: 0.27836767584085464, Final Batch Loss: 0.11467764526605606\n",
      "Epoch 549, Loss: 0.32886460423469543, Final Batch Loss: 0.15629133582115173\n",
      "Epoch 550, Loss: 0.30749693512916565, Final Batch Loss: 0.15992334485054016\n",
      "Epoch 551, Loss: 0.3066791892051697, Final Batch Loss: 0.1421947181224823\n",
      "Epoch 552, Loss: 0.34952257573604584, Final Batch Loss: 0.17225778102874756\n",
      "Epoch 553, Loss: 0.3314266949892044, Final Batch Loss: 0.18305672705173492\n",
      "Epoch 554, Loss: 0.3201214522123337, Final Batch Loss: 0.17052008211612701\n",
      "Epoch 555, Loss: 0.3281598687171936, Final Batch Loss: 0.13347122073173523\n",
      "Epoch 556, Loss: 0.30582380294799805, Final Batch Loss: 0.139685720205307\n",
      "Epoch 557, Loss: 0.3038997948169708, Final Batch Loss: 0.13559618592262268\n",
      "Epoch 558, Loss: 0.3229828029870987, Final Batch Loss: 0.16348008811473846\n",
      "Epoch 559, Loss: 0.3357904553413391, Final Batch Loss: 0.19951239228248596\n",
      "Epoch 560, Loss: 0.31449396908283234, Final Batch Loss: 0.1339685618877411\n",
      "Epoch 561, Loss: 0.30466850847005844, Final Batch Loss: 0.12023986130952835\n",
      "Epoch 562, Loss: 0.2945723384618759, Final Batch Loss: 0.11410422623157501\n",
      "Epoch 563, Loss: 0.2944471091032028, Final Batch Loss: 0.13932722806930542\n",
      "Epoch 564, Loss: 0.3335622251033783, Final Batch Loss: 0.18637241423130035\n",
      "Epoch 565, Loss: 0.33657003939151764, Final Batch Loss: 0.18569406867027283\n",
      "Epoch 566, Loss: 0.30870141088962555, Final Batch Loss: 0.108209028840065\n",
      "Epoch 567, Loss: 0.3252963572740555, Final Batch Loss: 0.1587577909231186\n",
      "Epoch 568, Loss: 0.3030697852373123, Final Batch Loss: 0.1364591419696808\n",
      "Epoch 569, Loss: 0.31420494616031647, Final Batch Loss: 0.1443905383348465\n",
      "Epoch 570, Loss: 0.31617793440818787, Final Batch Loss: 0.1539202332496643\n",
      "Epoch 571, Loss: 0.2982596307992935, Final Batch Loss: 0.14283615350723267\n",
      "Epoch 572, Loss: 0.30484844744205475, Final Batch Loss: 0.12893645465373993\n",
      "Epoch 573, Loss: 0.3485618829727173, Final Batch Loss: 0.21014833450317383\n",
      "Epoch 574, Loss: 0.3303559571504593, Final Batch Loss: 0.17194032669067383\n",
      "Epoch 575, Loss: 0.3076488822698593, Final Batch Loss: 0.14225532114505768\n",
      "Epoch 576, Loss: 0.2984979450702667, Final Batch Loss: 0.1650385707616806\n",
      "Epoch 577, Loss: 0.2877066507935524, Final Batch Loss: 0.09337814897298813\n",
      "Epoch 578, Loss: 0.3069165498018265, Final Batch Loss: 0.13686877489089966\n",
      "Epoch 579, Loss: 0.31500376760959625, Final Batch Loss: 0.17270204424858093\n",
      "Epoch 580, Loss: 0.3216639757156372, Final Batch Loss: 0.1638297289609909\n",
      "Epoch 581, Loss: 0.3353194147348404, Final Batch Loss: 0.18009960651397705\n",
      "Epoch 582, Loss: 0.3067343831062317, Final Batch Loss: 0.1602475345134735\n",
      "Epoch 583, Loss: 0.31135840713977814, Final Batch Loss: 0.13831184804439545\n",
      "Epoch 584, Loss: 0.30631282925605774, Final Batch Loss: 0.15486499667167664\n",
      "Epoch 585, Loss: 0.3359992355108261, Final Batch Loss: 0.17154501378536224\n",
      "Epoch 586, Loss: 0.2920081615447998, Final Batch Loss: 0.14361879229545593\n",
      "Epoch 587, Loss: 0.29478299617767334, Final Batch Loss: 0.15376439690589905\n",
      "Epoch 588, Loss: 0.30388395488262177, Final Batch Loss: 0.15924136340618134\n",
      "Epoch 589, Loss: 0.2743681073188782, Final Batch Loss: 0.11711905896663666\n",
      "Epoch 590, Loss: 0.30087903141975403, Final Batch Loss: 0.14573252201080322\n",
      "Epoch 591, Loss: 0.3112156167626381, Final Batch Loss: 0.1207156553864479\n",
      "Epoch 592, Loss: 0.296161413192749, Final Batch Loss: 0.15523745119571686\n",
      "Epoch 593, Loss: 0.2625155970454216, Final Batch Loss: 0.10378339141607285\n",
      "Epoch 594, Loss: 0.29384365677833557, Final Batch Loss: 0.1292244791984558\n",
      "Epoch 595, Loss: 0.3232181891798973, Final Batch Loss: 0.2044767290353775\n",
      "Epoch 596, Loss: 0.31924422830343246, Final Batch Loss: 0.19656865298748016\n",
      "Epoch 597, Loss: 0.29234378039836884, Final Batch Loss: 0.1598963588476181\n",
      "Epoch 598, Loss: 0.300034299492836, Final Batch Loss: 0.1599249392747879\n",
      "Epoch 599, Loss: 0.32032372057437897, Final Batch Loss: 0.17805708944797516\n",
      "Epoch 600, Loss: 0.302543967962265, Final Batch Loss: 0.12936076521873474\n",
      "Epoch 601, Loss: 0.2946534603834152, Final Batch Loss: 0.14101286232471466\n",
      "Epoch 602, Loss: 0.29166319966316223, Final Batch Loss: 0.14048217236995697\n",
      "Epoch 603, Loss: 0.30380813777446747, Final Batch Loss: 0.13436175882816315\n",
      "Epoch 604, Loss: 0.296747550368309, Final Batch Loss: 0.1646469384431839\n",
      "Epoch 605, Loss: 0.31713099777698517, Final Batch Loss: 0.17972902953624725\n",
      "Epoch 606, Loss: 0.3465069979429245, Final Batch Loss: 0.16759556531906128\n",
      "Epoch 607, Loss: 0.28934239596128464, Final Batch Loss: 0.10791971534490585\n",
      "Epoch 608, Loss: 0.30209027230739594, Final Batch Loss: 0.15835590660572052\n",
      "Epoch 609, Loss: 0.3041332960128784, Final Batch Loss: 0.17484340071678162\n",
      "Epoch 610, Loss: 0.30729858577251434, Final Batch Loss: 0.15073265135288239\n",
      "Epoch 611, Loss: 0.3190309852361679, Final Batch Loss: 0.15932826697826385\n",
      "Epoch 612, Loss: 0.32864102721214294, Final Batch Loss: 0.19232088327407837\n",
      "Epoch 613, Loss: 0.2958161234855652, Final Batch Loss: 0.15533243119716644\n",
      "Epoch 614, Loss: 0.28133654594421387, Final Batch Loss: 0.14302222430706024\n",
      "Epoch 615, Loss: 0.3548005670309067, Final Batch Loss: 0.1723114252090454\n",
      "Epoch 616, Loss: 0.32735008001327515, Final Batch Loss: 0.14426764845848083\n",
      "Epoch 617, Loss: 0.29830291867256165, Final Batch Loss: 0.14996297657489777\n",
      "Epoch 618, Loss: 0.3136037290096283, Final Batch Loss: 0.16807898879051208\n",
      "Epoch 619, Loss: 0.2771482467651367, Final Batch Loss: 0.13333508372306824\n",
      "Epoch 620, Loss: 0.27153705060482025, Final Batch Loss: 0.12919357419013977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 621, Loss: 0.2853122651576996, Final Batch Loss: 0.17142245173454285\n",
      "Epoch 622, Loss: 0.281188502907753, Final Batch Loss: 0.14019331336021423\n",
      "Epoch 623, Loss: 0.29550783336162567, Final Batch Loss: 0.14788712561130524\n",
      "Epoch 624, Loss: 0.3029417544603348, Final Batch Loss: 0.16157878935337067\n",
      "Epoch 625, Loss: 0.27436859905719757, Final Batch Loss: 0.13192379474639893\n",
      "Epoch 626, Loss: 0.28506313264369965, Final Batch Loss: 0.15305353701114655\n",
      "Epoch 627, Loss: 0.2924106568098068, Final Batch Loss: 0.1354222446680069\n",
      "Epoch 628, Loss: 0.322485089302063, Final Batch Loss: 0.18669776618480682\n",
      "Epoch 629, Loss: 0.32408493757247925, Final Batch Loss: 0.17383071780204773\n",
      "Epoch 630, Loss: 0.29936500638723373, Final Batch Loss: 0.1811625361442566\n",
      "Epoch 631, Loss: 0.35819393396377563, Final Batch Loss: 0.22863945364952087\n",
      "Epoch 632, Loss: 0.2848179042339325, Final Batch Loss: 0.1400395780801773\n",
      "Epoch 633, Loss: 0.3069906532764435, Final Batch Loss: 0.15014471113681793\n",
      "Epoch 634, Loss: 0.3009815886616707, Final Batch Loss: 0.17852993309497833\n",
      "Epoch 635, Loss: 0.31607256829738617, Final Batch Loss: 0.184341698884964\n",
      "Epoch 636, Loss: 0.37150783836841583, Final Batch Loss: 0.24772769212722778\n",
      "Epoch 637, Loss: 0.32674640417099, Final Batch Loss: 0.14992079138755798\n",
      "Epoch 638, Loss: 0.34150463342666626, Final Batch Loss: 0.18920010328292847\n",
      "Epoch 639, Loss: 0.3269285410642624, Final Batch Loss: 0.18404921889305115\n",
      "Epoch 640, Loss: 0.286212220788002, Final Batch Loss: 0.1440792977809906\n",
      "Epoch 641, Loss: 0.3399863988161087, Final Batch Loss: 0.14017969369888306\n",
      "Epoch 642, Loss: 0.3247581124305725, Final Batch Loss: 0.13366514444351196\n",
      "Epoch 643, Loss: 0.3062184303998947, Final Batch Loss: 0.17065177857875824\n",
      "Epoch 644, Loss: 0.2794390842318535, Final Batch Loss: 0.11270403116941452\n",
      "Epoch 645, Loss: 0.3017128109931946, Final Batch Loss: 0.13411806523799896\n",
      "Epoch 646, Loss: 0.3083144724369049, Final Batch Loss: 0.13435523211956024\n",
      "Epoch 647, Loss: 0.2940743416547775, Final Batch Loss: 0.10347424447536469\n",
      "Epoch 648, Loss: 0.2826208621263504, Final Batch Loss: 0.1505817174911499\n",
      "Epoch 649, Loss: 0.26517247408628464, Final Batch Loss: 0.10383740812540054\n",
      "Epoch 650, Loss: 0.28240615129470825, Final Batch Loss: 0.1369398534297943\n",
      "Epoch 651, Loss: 0.31662003695964813, Final Batch Loss: 0.1772969514131546\n",
      "Epoch 652, Loss: 0.25158120691776276, Final Batch Loss: 0.09835346043109894\n",
      "Epoch 653, Loss: 0.3034198135137558, Final Batch Loss: 0.16663435101509094\n",
      "Epoch 654, Loss: 0.30491913855075836, Final Batch Loss: 0.17247959971427917\n",
      "Epoch 655, Loss: 0.33553026616573334, Final Batch Loss: 0.21244202554225922\n",
      "Epoch 656, Loss: 0.296609029173851, Final Batch Loss: 0.1613035798072815\n",
      "Epoch 657, Loss: 0.2873678207397461, Final Batch Loss: 0.16636987030506134\n",
      "Epoch 658, Loss: 0.3138962388038635, Final Batch Loss: 0.1799362301826477\n",
      "Epoch 659, Loss: 0.2715437263250351, Final Batch Loss: 0.12045842409133911\n",
      "Epoch 660, Loss: 0.2920655757188797, Final Batch Loss: 0.15128114819526672\n",
      "Epoch 661, Loss: 0.3164164423942566, Final Batch Loss: 0.15891900658607483\n",
      "Epoch 662, Loss: 0.27451878786087036, Final Batch Loss: 0.14077559113502502\n",
      "Epoch 663, Loss: 0.3077255040407181, Final Batch Loss: 0.1626567542552948\n",
      "Epoch 664, Loss: 0.267949178814888, Final Batch Loss: 0.12847059965133667\n",
      "Epoch 665, Loss: 0.26254667341709137, Final Batch Loss: 0.12906433641910553\n",
      "Epoch 666, Loss: 0.2859131842851639, Final Batch Loss: 0.12941226363182068\n",
      "Epoch 667, Loss: 0.27260009944438934, Final Batch Loss: 0.13881780207157135\n",
      "Epoch 668, Loss: 0.26556582748889923, Final Batch Loss: 0.12692934274673462\n",
      "Epoch 669, Loss: 0.28794533014297485, Final Batch Loss: 0.1776914894580841\n",
      "Epoch 670, Loss: 0.28878192603588104, Final Batch Loss: 0.1631993055343628\n",
      "Epoch 671, Loss: 0.28734220564365387, Final Batch Loss: 0.13605555891990662\n",
      "Epoch 672, Loss: 0.27978892624378204, Final Batch Loss: 0.14500737190246582\n",
      "Epoch 673, Loss: 0.2909851521253586, Final Batch Loss: 0.16367745399475098\n",
      "Epoch 674, Loss: 0.26906514167785645, Final Batch Loss: 0.0986207127571106\n",
      "Epoch 675, Loss: 0.2845141589641571, Final Batch Loss: 0.12790793180465698\n",
      "Epoch 676, Loss: 0.25542906671762466, Final Batch Loss: 0.1084231361746788\n",
      "Epoch 677, Loss: 0.2777296453714371, Final Batch Loss: 0.14435112476348877\n",
      "Epoch 678, Loss: 0.299667552113533, Final Batch Loss: 0.15070560574531555\n",
      "Epoch 679, Loss: 0.2910841628909111, Final Batch Loss: 0.11499645560979843\n",
      "Epoch 680, Loss: 0.320844367146492, Final Batch Loss: 0.18744422495365143\n",
      "Epoch 681, Loss: 0.2937772497534752, Final Batch Loss: 0.17708098888397217\n",
      "Epoch 682, Loss: 0.2612433210015297, Final Batch Loss: 0.1378207951784134\n",
      "Epoch 683, Loss: 0.25463467091321945, Final Batch Loss: 0.13321514427661896\n",
      "Epoch 684, Loss: 0.2714024931192398, Final Batch Loss: 0.14541158080101013\n",
      "Epoch 685, Loss: 0.30456747114658356, Final Batch Loss: 0.17810548841953278\n",
      "Epoch 686, Loss: 0.26171018928289413, Final Batch Loss: 0.1439497321844101\n",
      "Epoch 687, Loss: 0.28211696445941925, Final Batch Loss: 0.14030501246452332\n",
      "Epoch 688, Loss: 0.26531242579221725, Final Batch Loss: 0.14875969290733337\n",
      "Epoch 689, Loss: 0.26735130697488785, Final Batch Loss: 0.12249342352151871\n",
      "Epoch 690, Loss: 0.3011348992586136, Final Batch Loss: 0.15878337621688843\n",
      "Epoch 691, Loss: 0.318712055683136, Final Batch Loss: 0.1560075432062149\n",
      "Epoch 692, Loss: 0.25984975695610046, Final Batch Loss: 0.1257307231426239\n",
      "Epoch 693, Loss: 0.27626535296440125, Final Batch Loss: 0.10201407968997955\n",
      "Epoch 694, Loss: 0.2545608729124069, Final Batch Loss: 0.11421069502830505\n",
      "Epoch 695, Loss: 0.29377101361751556, Final Batch Loss: 0.16107240319252014\n",
      "Epoch 696, Loss: 0.2897518575191498, Final Batch Loss: 0.17042866349220276\n",
      "Epoch 697, Loss: 0.311709001660347, Final Batch Loss: 0.15711544454097748\n",
      "Epoch 698, Loss: 0.2925344705581665, Final Batch Loss: 0.16544155776500702\n",
      "Epoch 699, Loss: 0.2705191522836685, Final Batch Loss: 0.15355847775936127\n",
      "Epoch 700, Loss: 0.28179942816495895, Final Batch Loss: 0.16033637523651123\n",
      "Epoch 701, Loss: 0.2912070155143738, Final Batch Loss: 0.15426158905029297\n",
      "Epoch 702, Loss: 0.283340260386467, Final Batch Loss: 0.1517297476530075\n",
      "Epoch 703, Loss: 0.2720891237258911, Final Batch Loss: 0.11976318061351776\n",
      "Epoch 704, Loss: 0.26642297208309174, Final Batch Loss: 0.1285765916109085\n",
      "Epoch 705, Loss: 0.26782411336898804, Final Batch Loss: 0.11474360525608063\n",
      "Epoch 706, Loss: 0.23694468289613724, Final Batch Loss: 0.09551491588354111\n",
      "Epoch 707, Loss: 0.2775568515062332, Final Batch Loss: 0.1354236900806427\n",
      "Epoch 708, Loss: 0.2624029740691185, Final Batch Loss: 0.12257913500070572\n",
      "Epoch 709, Loss: 0.2811427712440491, Final Batch Loss: 0.15520046651363373\n",
      "Epoch 710, Loss: 0.267326295375824, Final Batch Loss: 0.137268528342247\n",
      "Epoch 711, Loss: 0.2589934691786766, Final Batch Loss: 0.10683055967092514\n",
      "Epoch 712, Loss: 0.2749553769826889, Final Batch Loss: 0.14511729776859283\n",
      "Epoch 713, Loss: 0.2731231302022934, Final Batch Loss: 0.13326296210289001\n",
      "Epoch 714, Loss: 0.27659933269023895, Final Batch Loss: 0.1393178552389145\n",
      "Epoch 715, Loss: 0.2508182153105736, Final Batch Loss: 0.1082741841673851\n",
      "Epoch 716, Loss: 0.25119665265083313, Final Batch Loss: 0.10751128196716309\n",
      "Epoch 717, Loss: 0.26640453189611435, Final Batch Loss: 0.1458326131105423\n",
      "Epoch 718, Loss: 0.2802045941352844, Final Batch Loss: 0.13171891868114471\n",
      "Epoch 719, Loss: 0.27138514816761017, Final Batch Loss: 0.10816572606563568\n",
      "Epoch 720, Loss: 0.2522467225790024, Final Batch Loss: 0.12609770894050598\n",
      "Epoch 721, Loss: 0.25343649834394455, Final Batch Loss: 0.10314545780420303\n",
      "Epoch 722, Loss: 0.25859013199806213, Final Batch Loss: 0.13158291578292847\n",
      "Epoch 723, Loss: 0.27080145478248596, Final Batch Loss: 0.12995098531246185\n",
      "Epoch 724, Loss: 0.2806231305003166, Final Batch Loss: 0.15675371885299683\n",
      "Epoch 725, Loss: 0.27078142762184143, Final Batch Loss: 0.14197386801242828\n",
      "Epoch 726, Loss: 0.2578327804803848, Final Batch Loss: 0.12238645553588867\n",
      "Epoch 727, Loss: 0.31318967044353485, Final Batch Loss: 0.17388993501663208\n",
      "Epoch 728, Loss: 0.2714089900255203, Final Batch Loss: 0.13855527341365814\n",
      "Epoch 729, Loss: 0.25624963641166687, Final Batch Loss: 0.14254456758499146\n",
      "Epoch 730, Loss: 0.30941613763570786, Final Batch Loss: 0.18794555962085724\n",
      "Epoch 731, Loss: 0.27727024257183075, Final Batch Loss: 0.13368333876132965\n",
      "Epoch 732, Loss: 0.26019856333732605, Final Batch Loss: 0.1388857066631317\n",
      "Epoch 733, Loss: 0.2620387226343155, Final Batch Loss: 0.11092959344387054\n",
      "Epoch 734, Loss: 0.23293577879667282, Final Batch Loss: 0.10047786682844162\n",
      "Epoch 735, Loss: 0.2636246308684349, Final Batch Loss: 0.11513298004865646\n",
      "Epoch 736, Loss: 0.2903536707162857, Final Batch Loss: 0.16745711863040924\n",
      "Epoch 737, Loss: 0.2587300091981888, Final Batch Loss: 0.1251644343137741\n",
      "Epoch 738, Loss: 0.2506565824151039, Final Batch Loss: 0.13954408466815948\n",
      "Epoch 739, Loss: 0.2669331803917885, Final Batch Loss: 0.1467238962650299\n",
      "Epoch 740, Loss: 0.25717223435640335, Final Batch Loss: 0.15342307090759277\n",
      "Epoch 741, Loss: 0.2570914179086685, Final Batch Loss: 0.12053817510604858\n",
      "Epoch 742, Loss: 0.2695700004696846, Final Batch Loss: 0.16834521293640137\n",
      "Epoch 743, Loss: 0.25068747997283936, Final Batch Loss: 0.13090302050113678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 744, Loss: 0.2559742331504822, Final Batch Loss: 0.12719391286373138\n",
      "Epoch 745, Loss: 0.2665982246398926, Final Batch Loss: 0.15317241847515106\n",
      "Epoch 746, Loss: 0.24102681875228882, Final Batch Loss: 0.11092133820056915\n",
      "Epoch 747, Loss: 0.2577023282647133, Final Batch Loss: 0.10611986368894577\n",
      "Epoch 748, Loss: 0.2693774253129959, Final Batch Loss: 0.13256007432937622\n",
      "Epoch 749, Loss: 0.2582671120762825, Final Batch Loss: 0.11852595955133438\n",
      "Epoch 750, Loss: 0.24229253828525543, Final Batch Loss: 0.12395019084215164\n",
      "Epoch 751, Loss: 0.2748308330774307, Final Batch Loss: 0.1329302191734314\n",
      "Epoch 752, Loss: 0.2680776119232178, Final Batch Loss: 0.13472872972488403\n",
      "Epoch 753, Loss: 0.24632438272237778, Final Batch Loss: 0.12702800333499908\n",
      "Epoch 754, Loss: 0.29271169006824493, Final Batch Loss: 0.12981805205345154\n",
      "Epoch 755, Loss: 0.2689230144023895, Final Batch Loss: 0.13735491037368774\n",
      "Epoch 756, Loss: 0.2504737600684166, Final Batch Loss: 0.12229415029287338\n",
      "Epoch 757, Loss: 0.3111441731452942, Final Batch Loss: 0.1814069300889969\n",
      "Epoch 758, Loss: 0.3024342507123947, Final Batch Loss: 0.1317797303199768\n",
      "Epoch 759, Loss: 0.25618384778499603, Final Batch Loss: 0.1300048679113388\n",
      "Epoch 760, Loss: 0.26411718130111694, Final Batch Loss: 0.13495206832885742\n",
      "Epoch 761, Loss: 0.2627773582935333, Final Batch Loss: 0.12677843868732452\n",
      "Epoch 762, Loss: 0.27734166383743286, Final Batch Loss: 0.126138836145401\n",
      "Epoch 763, Loss: 0.2566771060228348, Final Batch Loss: 0.15837998688220978\n",
      "Epoch 764, Loss: 0.27231644093990326, Final Batch Loss: 0.12261366844177246\n",
      "Epoch 765, Loss: 0.27211152017116547, Final Batch Loss: 0.12882132828235626\n",
      "Epoch 766, Loss: 0.25898732244968414, Final Batch Loss: 0.13355505466461182\n",
      "Epoch 767, Loss: 0.2558426558971405, Final Batch Loss: 0.1480173021554947\n",
      "Epoch 768, Loss: 0.24814531207084656, Final Batch Loss: 0.11683838069438934\n",
      "Epoch 769, Loss: 0.23859647661447525, Final Batch Loss: 0.09718205779790878\n",
      "Epoch 770, Loss: 0.24829349666833878, Final Batch Loss: 0.1270712912082672\n",
      "Epoch 771, Loss: 0.26219476759433746, Final Batch Loss: 0.10378411412239075\n",
      "Epoch 772, Loss: 0.2638094574213028, Final Batch Loss: 0.11291851103305817\n",
      "Epoch 773, Loss: 0.26864171028137207, Final Batch Loss: 0.14372828602790833\n",
      "Epoch 774, Loss: 0.2824513465166092, Final Batch Loss: 0.1311042755842209\n",
      "Epoch 775, Loss: 0.28211770206689835, Final Batch Loss: 0.1779772788286209\n",
      "Epoch 776, Loss: 0.27343064546585083, Final Batch Loss: 0.14083558320999146\n",
      "Epoch 777, Loss: 0.29410816729068756, Final Batch Loss: 0.1913282573223114\n",
      "Epoch 778, Loss: 0.2879594936966896, Final Batch Loss: 0.17300346493721008\n",
      "Epoch 779, Loss: 0.2424057126045227, Final Batch Loss: 0.11701911687850952\n",
      "Epoch 780, Loss: 0.24436379224061966, Final Batch Loss: 0.10624363273382187\n",
      "Epoch 781, Loss: 0.2714788615703583, Final Batch Loss: 0.1461050659418106\n",
      "Epoch 782, Loss: 0.23031312972307205, Final Batch Loss: 0.11097528040409088\n",
      "Epoch 783, Loss: 0.2953699976205826, Final Batch Loss: 0.15527991950511932\n",
      "Epoch 784, Loss: 0.27441126108169556, Final Batch Loss: 0.12592607736587524\n",
      "Epoch 785, Loss: 0.2796969637274742, Final Batch Loss: 0.1165790930390358\n",
      "Epoch 786, Loss: 0.2505945786833763, Final Batch Loss: 0.11853469163179398\n",
      "Epoch 787, Loss: 0.26661015301942825, Final Batch Loss: 0.1239684447646141\n",
      "Epoch 788, Loss: 0.23510292917490005, Final Batch Loss: 0.11006154865026474\n",
      "Epoch 789, Loss: 0.2739880904555321, Final Batch Loss: 0.16437779366970062\n",
      "Epoch 790, Loss: 0.2467467486858368, Final Batch Loss: 0.12715928256511688\n",
      "Epoch 791, Loss: 0.24932532012462616, Final Batch Loss: 0.11132270097732544\n",
      "Epoch 792, Loss: 0.24149424582719803, Final Batch Loss: 0.1272537261247635\n",
      "Epoch 793, Loss: 0.2593948692083359, Final Batch Loss: 0.12620237469673157\n",
      "Epoch 794, Loss: 0.23471952974796295, Final Batch Loss: 0.11676769703626633\n",
      "Epoch 795, Loss: 0.25045137852430344, Final Batch Loss: 0.12828467786312103\n",
      "Epoch 796, Loss: 0.2553873807191849, Final Batch Loss: 0.12713266909122467\n",
      "Epoch 797, Loss: 0.2873487025499344, Final Batch Loss: 0.15484635531902313\n",
      "Epoch 798, Loss: 0.2502162382006645, Final Batch Loss: 0.1081920638680458\n",
      "Epoch 799, Loss: 0.24282068014144897, Final Batch Loss: 0.11306893825531006\n",
      "Epoch 800, Loss: 0.22904012352228165, Final Batch Loss: 0.09917908161878586\n",
      "Epoch 801, Loss: 0.2672777846455574, Final Batch Loss: 0.16135704517364502\n",
      "Epoch 802, Loss: 0.2702491879463196, Final Batch Loss: 0.13454528152942657\n",
      "Epoch 803, Loss: 0.21611303091049194, Final Batch Loss: 0.0840829610824585\n",
      "Epoch 804, Loss: 0.25614677369594574, Final Batch Loss: 0.12204095721244812\n",
      "Epoch 805, Loss: 0.23314963281154633, Final Batch Loss: 0.11557914316654205\n",
      "Epoch 806, Loss: 0.2421664446592331, Final Batch Loss: 0.10436174273490906\n",
      "Epoch 807, Loss: 0.2508542761206627, Final Batch Loss: 0.10095921903848648\n",
      "Epoch 808, Loss: 0.25723573565483093, Final Batch Loss: 0.1522917002439499\n",
      "Epoch 809, Loss: 0.2474002242088318, Final Batch Loss: 0.1468806117773056\n",
      "Epoch 810, Loss: 0.27202294766902924, Final Batch Loss: 0.12956708669662476\n",
      "Epoch 811, Loss: 0.24653037637472153, Final Batch Loss: 0.15780648589134216\n",
      "Epoch 812, Loss: 0.20814594626426697, Final Batch Loss: 0.08428005129098892\n",
      "Epoch 813, Loss: 0.24618160724639893, Final Batch Loss: 0.12346220016479492\n",
      "Epoch 814, Loss: 0.25171685218811035, Final Batch Loss: 0.11696121096611023\n",
      "Epoch 815, Loss: 0.25599584728479385, Final Batch Loss: 0.134994775056839\n",
      "Epoch 816, Loss: 0.3219493329524994, Final Batch Loss: 0.12887153029441833\n",
      "Epoch 817, Loss: 0.2649427801370621, Final Batch Loss: 0.1200188547372818\n",
      "Epoch 818, Loss: 0.23642908036708832, Final Batch Loss: 0.10510225594043732\n",
      "Epoch 819, Loss: 0.26960645616054535, Final Batch Loss: 0.11718906462192535\n",
      "Epoch 820, Loss: 0.24930600076913834, Final Batch Loss: 0.15062971413135529\n",
      "Epoch 821, Loss: 0.2549724131822586, Final Batch Loss: 0.1299194097518921\n",
      "Epoch 822, Loss: 0.26931530982255936, Final Batch Loss: 0.11483535915613174\n",
      "Epoch 823, Loss: 0.24317330121994019, Final Batch Loss: 0.11200876533985138\n",
      "Epoch 824, Loss: 0.23054401576519012, Final Batch Loss: 0.10877790302038193\n",
      "Epoch 825, Loss: 0.23815471678972244, Final Batch Loss: 0.12267430126667023\n",
      "Epoch 826, Loss: 0.2425786331295967, Final Batch Loss: 0.10572291165590286\n",
      "Epoch 827, Loss: 0.26302674412727356, Final Batch Loss: 0.1443588137626648\n",
      "Epoch 828, Loss: 0.2726271376013756, Final Batch Loss: 0.15769101679325104\n",
      "Epoch 829, Loss: 0.24002815783023834, Final Batch Loss: 0.12314358353614807\n",
      "Epoch 830, Loss: 0.25252116471529007, Final Batch Loss: 0.11104615777730942\n",
      "Epoch 831, Loss: 0.2696667015552521, Final Batch Loss: 0.13487419486045837\n",
      "Epoch 832, Loss: 0.27561886608600616, Final Batch Loss: 0.14164362847805023\n",
      "Epoch 833, Loss: 0.2390545904636383, Final Batch Loss: 0.12420497834682465\n",
      "Epoch 834, Loss: 0.2542017996311188, Final Batch Loss: 0.14141930639743805\n",
      "Epoch 835, Loss: 0.239297516644001, Final Batch Loss: 0.1297435164451599\n",
      "Epoch 836, Loss: 0.23622780293226242, Final Batch Loss: 0.08677833527326584\n",
      "Epoch 837, Loss: 0.22172021120786667, Final Batch Loss: 0.07548303157091141\n",
      "Epoch 838, Loss: 0.27277057617902756, Final Batch Loss: 0.10968007892370224\n",
      "Epoch 839, Loss: 0.2095421887934208, Final Batch Loss: 0.06170764937996864\n",
      "Epoch 840, Loss: 0.24121373891830444, Final Batch Loss: 0.15346741676330566\n",
      "Epoch 841, Loss: 0.2510043978691101, Final Batch Loss: 0.11500698328018188\n",
      "Epoch 842, Loss: 0.24969788640737534, Final Batch Loss: 0.14288009703159332\n",
      "Epoch 843, Loss: 0.22833535820245743, Final Batch Loss: 0.12499946355819702\n",
      "Epoch 844, Loss: 0.23098837584257126, Final Batch Loss: 0.11062357574701309\n",
      "Epoch 845, Loss: 0.2614562287926674, Final Batch Loss: 0.16723838448524475\n",
      "Epoch 846, Loss: 0.27708010375499725, Final Batch Loss: 0.15164700150489807\n",
      "Epoch 847, Loss: 0.25754057615995407, Final Batch Loss: 0.09787549823522568\n",
      "Epoch 848, Loss: 0.24720431864261627, Final Batch Loss: 0.1220255196094513\n",
      "Epoch 849, Loss: 0.24717849493026733, Final Batch Loss: 0.12422370910644531\n",
      "Epoch 850, Loss: 0.22044780850410461, Final Batch Loss: 0.08491852879524231\n",
      "Epoch 851, Loss: 0.2614820897579193, Final Batch Loss: 0.12610472738742828\n",
      "Epoch 852, Loss: 0.22780822217464447, Final Batch Loss: 0.12725567817687988\n",
      "Epoch 853, Loss: 0.2500174641609192, Final Batch Loss: 0.1228167712688446\n",
      "Epoch 854, Loss: 0.23960044234991074, Final Batch Loss: 0.12049669027328491\n",
      "Epoch 855, Loss: 0.26708105206489563, Final Batch Loss: 0.14956702291965485\n",
      "Epoch 856, Loss: 0.22252780199050903, Final Batch Loss: 0.10376866161823273\n",
      "Epoch 857, Loss: 0.2233450636267662, Final Batch Loss: 0.10814259201288223\n",
      "Epoch 858, Loss: 0.24494042992591858, Final Batch Loss: 0.1169976145029068\n",
      "Epoch 859, Loss: 0.23920457810163498, Final Batch Loss: 0.12866400182247162\n",
      "Epoch 860, Loss: 0.25911062210798264, Final Batch Loss: 0.11379621177911758\n",
      "Epoch 861, Loss: 0.2304501086473465, Final Batch Loss: 0.10117468237876892\n",
      "Epoch 862, Loss: 0.2485964149236679, Final Batch Loss: 0.130780890583992\n",
      "Epoch 863, Loss: 0.24011510610580444, Final Batch Loss: 0.12510845065116882\n",
      "Epoch 864, Loss: 0.22005517035722733, Final Batch Loss: 0.080851711332798\n",
      "Epoch 865, Loss: 0.2624482214450836, Final Batch Loss: 0.13778598606586456\n",
      "Epoch 866, Loss: 0.22743722796440125, Final Batch Loss: 0.08917611837387085\n",
      "Epoch 867, Loss: 0.2565295398235321, Final Batch Loss: 0.1117815375328064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 868, Loss: 0.25056903064250946, Final Batch Loss: 0.12236747145652771\n",
      "Epoch 869, Loss: 0.24902605265378952, Final Batch Loss: 0.13752736151218414\n",
      "Epoch 870, Loss: 0.2239929884672165, Final Batch Loss: 0.09685416519641876\n",
      "Epoch 871, Loss: 0.2349868044257164, Final Batch Loss: 0.09454410523176193\n",
      "Epoch 872, Loss: 0.26034704595804214, Final Batch Loss: 0.16896313428878784\n",
      "Epoch 873, Loss: 0.2639309838414192, Final Batch Loss: 0.17059728503227234\n",
      "Epoch 874, Loss: 0.27433206886053085, Final Batch Loss: 0.17388366162776947\n",
      "Epoch 875, Loss: 0.28481628000736237, Final Batch Loss: 0.16764956712722778\n",
      "Epoch 876, Loss: 0.261499747633934, Final Batch Loss: 0.1271771788597107\n",
      "Epoch 877, Loss: 0.2656763717532158, Final Batch Loss: 0.16179385781288147\n",
      "Epoch 878, Loss: 0.2586604952812195, Final Batch Loss: 0.12869247794151306\n",
      "Epoch 879, Loss: 0.25680943578481674, Final Batch Loss: 0.12325490266084671\n",
      "Epoch 880, Loss: 0.2173781916499138, Final Batch Loss: 0.10230530053377151\n",
      "Epoch 881, Loss: 0.2560814619064331, Final Batch Loss: 0.1287800669670105\n",
      "Epoch 882, Loss: 0.2404426410794258, Final Batch Loss: 0.11707688868045807\n",
      "Epoch 883, Loss: 0.2754810079932213, Final Batch Loss: 0.15304148197174072\n",
      "Epoch 884, Loss: 0.24382621049880981, Final Batch Loss: 0.12079557776451111\n",
      "Epoch 885, Loss: 0.2628958970308304, Final Batch Loss: 0.13607285916805267\n",
      "Epoch 886, Loss: 0.27598658204078674, Final Batch Loss: 0.15222343802452087\n",
      "Epoch 887, Loss: 0.24010927975177765, Final Batch Loss: 0.13034401834011078\n",
      "Epoch 888, Loss: 0.22929436713457108, Final Batch Loss: 0.10585829615592957\n",
      "Epoch 889, Loss: 0.27021583914756775, Final Batch Loss: 0.14876744151115417\n",
      "Epoch 890, Loss: 0.25485238432884216, Final Batch Loss: 0.14746235311031342\n",
      "Epoch 891, Loss: 0.23124368488788605, Final Batch Loss: 0.1254885345697403\n",
      "Epoch 892, Loss: 0.2264600396156311, Final Batch Loss: 0.09435681998729706\n",
      "Epoch 893, Loss: 0.23292288929224014, Final Batch Loss: 0.1261923611164093\n",
      "Epoch 894, Loss: 0.2288135290145874, Final Batch Loss: 0.09140555560588837\n",
      "Epoch 895, Loss: 0.2545552998781204, Final Batch Loss: 0.13924390077590942\n",
      "Epoch 896, Loss: 0.23532044887542725, Final Batch Loss: 0.1401561200618744\n",
      "Epoch 897, Loss: 0.23570547252893448, Final Batch Loss: 0.09937790781259537\n",
      "Epoch 898, Loss: 0.2615777999162674, Final Batch Loss: 0.1515958607196808\n",
      "Epoch 899, Loss: 0.2088996171951294, Final Batch Loss: 0.08897974342107773\n",
      "Epoch 900, Loss: 0.2370877042412758, Final Batch Loss: 0.1288674920797348\n",
      "Epoch 901, Loss: 0.23994407802820206, Final Batch Loss: 0.09805362671613693\n",
      "Epoch 902, Loss: 0.267830029129982, Final Batch Loss: 0.13364435732364655\n",
      "Epoch 903, Loss: 0.24593546241521835, Final Batch Loss: 0.10257082432508469\n",
      "Epoch 904, Loss: 0.23965279012918472, Final Batch Loss: 0.12357282638549805\n",
      "Epoch 905, Loss: 0.25409914553165436, Final Batch Loss: 0.10869598388671875\n",
      "Epoch 906, Loss: 0.2325229048728943, Final Batch Loss: 0.1261293739080429\n",
      "Epoch 907, Loss: 0.23181430250406265, Final Batch Loss: 0.12659838795661926\n",
      "Epoch 908, Loss: 0.24870146065950394, Final Batch Loss: 0.12294331938028336\n",
      "Epoch 909, Loss: 0.27326811850070953, Final Batch Loss: 0.17047962546348572\n",
      "Epoch 910, Loss: 0.2405969202518463, Final Batch Loss: 0.09581571817398071\n",
      "Epoch 911, Loss: 0.24816828966140747, Final Batch Loss: 0.13049954175949097\n",
      "Epoch 912, Loss: 0.2909869998693466, Final Batch Loss: 0.15093985199928284\n",
      "Epoch 913, Loss: 0.22626009583473206, Final Batch Loss: 0.07657286524772644\n",
      "Epoch 914, Loss: 0.27202001959085464, Final Batch Loss: 0.10822293907403946\n",
      "Epoch 915, Loss: 0.22605010122060776, Final Batch Loss: 0.1157018318772316\n",
      "Epoch 916, Loss: 0.24747935682535172, Final Batch Loss: 0.12536635994911194\n",
      "Epoch 917, Loss: 0.2654266059398651, Final Batch Loss: 0.1321861743927002\n",
      "Epoch 918, Loss: 0.23240893334150314, Final Batch Loss: 0.09895407408475876\n",
      "Epoch 919, Loss: 0.2297191545367241, Final Batch Loss: 0.10129833966493607\n",
      "Epoch 920, Loss: 0.2237296998500824, Final Batch Loss: 0.11195419728755951\n",
      "Epoch 921, Loss: 0.23763110488653183, Final Batch Loss: 0.10487287491559982\n",
      "Epoch 922, Loss: 0.2262430563569069, Final Batch Loss: 0.11993187665939331\n",
      "Epoch 923, Loss: 0.2831582576036453, Final Batch Loss: 0.12399911880493164\n",
      "Epoch 924, Loss: 0.24219334870576859, Final Batch Loss: 0.13802209496498108\n",
      "Epoch 925, Loss: 0.2521742656826973, Final Batch Loss: 0.15126784145832062\n",
      "Epoch 926, Loss: 0.2260713428258896, Final Batch Loss: 0.12941458821296692\n",
      "Epoch 927, Loss: 0.26309942454099655, Final Batch Loss: 0.11664573103189468\n",
      "Epoch 928, Loss: 0.232231043279171, Final Batch Loss: 0.1203022226691246\n",
      "Epoch 929, Loss: 0.21994317322969437, Final Batch Loss: 0.11454135924577713\n",
      "Epoch 930, Loss: 0.2940192222595215, Final Batch Loss: 0.15500609576702118\n",
      "Epoch 931, Loss: 0.23245979845523834, Final Batch Loss: 0.12839879095554352\n",
      "Epoch 932, Loss: 0.2361736223101616, Final Batch Loss: 0.1624888926744461\n",
      "Epoch 933, Loss: 0.22856859862804413, Final Batch Loss: 0.11899174749851227\n",
      "Epoch 934, Loss: 0.23389776051044464, Final Batch Loss: 0.1294519007205963\n",
      "Epoch 935, Loss: 0.25280623883008957, Final Batch Loss: 0.10274236649274826\n",
      "Epoch 936, Loss: 0.23572808504104614, Final Batch Loss: 0.09428836405277252\n",
      "Epoch 937, Loss: 0.23385725170373917, Final Batch Loss: 0.1078304722905159\n",
      "Epoch 938, Loss: 0.21689458191394806, Final Batch Loss: 0.09967654943466187\n",
      "Epoch 939, Loss: 0.24684611707925797, Final Batch Loss: 0.13585294783115387\n",
      "Epoch 940, Loss: 0.2690984383225441, Final Batch Loss: 0.16573800146579742\n",
      "Epoch 941, Loss: 0.22346888482570648, Final Batch Loss: 0.103812575340271\n",
      "Epoch 942, Loss: 0.2646351084113121, Final Batch Loss: 0.16134022176265717\n",
      "Epoch 943, Loss: 0.35662589967250824, Final Batch Loss: 0.21934694051742554\n",
      "Epoch 944, Loss: 0.23204588890075684, Final Batch Loss: 0.12528330087661743\n",
      "Epoch 945, Loss: 0.24146946519613266, Final Batch Loss: 0.11828133463859558\n",
      "Epoch 946, Loss: 0.2585984691977501, Final Batch Loss: 0.0990118458867073\n",
      "Epoch 947, Loss: 0.2759459465742111, Final Batch Loss: 0.16957932710647583\n",
      "Epoch 948, Loss: 0.22037089616060257, Final Batch Loss: 0.1287614107131958\n",
      "Epoch 949, Loss: 0.23940963298082352, Final Batch Loss: 0.12604883313179016\n",
      "Epoch 950, Loss: 0.21984802931547165, Final Batch Loss: 0.07693087309598923\n",
      "Epoch 951, Loss: 0.23697438836097717, Final Batch Loss: 0.13364125788211823\n",
      "Epoch 952, Loss: 0.226459339261055, Final Batch Loss: 0.1146627888083458\n",
      "Epoch 953, Loss: 0.24191129952669144, Final Batch Loss: 0.11572954803705215\n",
      "Epoch 954, Loss: 0.2284434288740158, Final Batch Loss: 0.1197575256228447\n",
      "Epoch 955, Loss: 0.21757132560014725, Final Batch Loss: 0.09576105326414108\n",
      "Epoch 956, Loss: 0.3032491058111191, Final Batch Loss: 0.16119641065597534\n",
      "Epoch 957, Loss: 0.2377762347459793, Final Batch Loss: 0.1400357484817505\n",
      "Epoch 958, Loss: 0.2931283712387085, Final Batch Loss: 0.1300085484981537\n",
      "Epoch 959, Loss: 0.27242811024188995, Final Batch Loss: 0.14574186503887177\n",
      "Epoch 960, Loss: 0.2062206193804741, Final Batch Loss: 0.09082211554050446\n",
      "Epoch 961, Loss: 0.2639940083026886, Final Batch Loss: 0.11225903034210205\n",
      "Epoch 962, Loss: 0.26976318657398224, Final Batch Loss: 0.10921627283096313\n",
      "Epoch 963, Loss: 0.3816109523177147, Final Batch Loss: 0.2654244005680084\n",
      "Epoch 964, Loss: 0.24497633427381516, Final Batch Loss: 0.15546226501464844\n",
      "Epoch 965, Loss: 0.2172985002398491, Final Batch Loss: 0.0930648148059845\n",
      "Epoch 966, Loss: 0.2350347712635994, Final Batch Loss: 0.10300562530755997\n",
      "Epoch 967, Loss: 0.28581732511520386, Final Batch Loss: 0.15687315165996552\n",
      "Epoch 968, Loss: 0.22937240451574326, Final Batch Loss: 0.0876128301024437\n",
      "Epoch 969, Loss: 0.25095807015895844, Final Batch Loss: 0.1193811297416687\n",
      "Epoch 970, Loss: 0.26248087733983994, Final Batch Loss: 0.15387451648712158\n",
      "Epoch 971, Loss: 0.24125461280345917, Final Batch Loss: 0.13103924691677094\n",
      "Epoch 972, Loss: 0.2480425164103508, Final Batch Loss: 0.1155027523636818\n",
      "Epoch 973, Loss: 0.2242775559425354, Final Batch Loss: 0.11916027963161469\n",
      "Epoch 974, Loss: 0.23549269139766693, Final Batch Loss: 0.09765313565731049\n",
      "Epoch 975, Loss: 0.20887253433465958, Final Batch Loss: 0.1126643717288971\n",
      "Epoch 976, Loss: 0.21827609091997147, Final Batch Loss: 0.10175701230764389\n",
      "Epoch 977, Loss: 0.25450609624385834, Final Batch Loss: 0.12014463543891907\n",
      "Epoch 978, Loss: 0.2363012582063675, Final Batch Loss: 0.1231803297996521\n",
      "Epoch 979, Loss: 0.2313842698931694, Final Batch Loss: 0.09916817396879196\n",
      "Epoch 980, Loss: 0.22339963167905807, Final Batch Loss: 0.08610107749700546\n",
      "Epoch 981, Loss: 0.2699727714061737, Final Batch Loss: 0.1428518146276474\n",
      "Epoch 982, Loss: 0.21462373435497284, Final Batch Loss: 0.10943826287984848\n",
      "Epoch 983, Loss: 0.27669693529605865, Final Batch Loss: 0.1538459211587906\n",
      "Epoch 984, Loss: 0.26239363849163055, Final Batch Loss: 0.13377535343170166\n",
      "Epoch 985, Loss: 0.2120288461446762, Final Batch Loss: 0.07429669797420502\n",
      "Epoch 986, Loss: 0.25711873173713684, Final Batch Loss: 0.12795545160770416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 987, Loss: 0.23354905098676682, Final Batch Loss: 0.1127820834517479\n",
      "Epoch 988, Loss: 0.24311062693595886, Final Batch Loss: 0.11906911432743073\n",
      "Epoch 989, Loss: 0.25081201642751694, Final Batch Loss: 0.14978063106536865\n",
      "Epoch 990, Loss: 0.23004888743162155, Final Batch Loss: 0.09708701819181442\n",
      "Epoch 991, Loss: 0.26262086629867554, Final Batch Loss: 0.12990538775920868\n",
      "Epoch 992, Loss: 0.2222558856010437, Final Batch Loss: 0.10169906169176102\n",
      "Epoch 993, Loss: 0.21838537603616714, Final Batch Loss: 0.10751598328351974\n",
      "Epoch 994, Loss: 0.2273608297109604, Final Batch Loss: 0.1159951314330101\n",
      "Epoch 995, Loss: 0.23197532445192337, Final Batch Loss: 0.11364725977182388\n",
      "Epoch 996, Loss: 0.24596552550792694, Final Batch Loss: 0.11143822968006134\n",
      "Epoch 997, Loss: 0.23644183576107025, Final Batch Loss: 0.14633293449878693\n",
      "Epoch 998, Loss: 0.23451150208711624, Final Batch Loss: 0.12093377858400345\n",
      "Epoch 999, Loss: 0.2546018213033676, Final Batch Loss: 0.12608115375041962\n",
      "Epoch 1000, Loss: 0.2309819906949997, Final Batch Loss: 0.12418931722640991\n",
      "Epoch 1001, Loss: 0.2090682089328766, Final Batch Loss: 0.09981272369623184\n",
      "Epoch 1002, Loss: 0.2270323783159256, Final Batch Loss: 0.12628038227558136\n",
      "Epoch 1003, Loss: 0.24176695197820663, Final Batch Loss: 0.12468873709440231\n",
      "Epoch 1004, Loss: 0.20912718772888184, Final Batch Loss: 0.09890285134315491\n",
      "Epoch 1005, Loss: 0.2205720990896225, Final Batch Loss: 0.13396640121936798\n",
      "Epoch 1006, Loss: 0.269238606095314, Final Batch Loss: 0.15630881488323212\n",
      "Epoch 1007, Loss: 0.24351152777671814, Final Batch Loss: 0.13865849375724792\n",
      "Epoch 1008, Loss: 0.23583023250102997, Final Batch Loss: 0.10717147588729858\n",
      "Epoch 1009, Loss: 0.22370114922523499, Final Batch Loss: 0.10278310626745224\n",
      "Epoch 1010, Loss: 0.22690874338150024, Final Batch Loss: 0.09954482316970825\n",
      "Epoch 1011, Loss: 0.21795368194580078, Final Batch Loss: 0.10375910252332687\n",
      "Epoch 1012, Loss: 0.23732353746891022, Final Batch Loss: 0.07020317018032074\n",
      "Epoch 1013, Loss: 0.22213496267795563, Final Batch Loss: 0.11295551806688309\n",
      "Epoch 1014, Loss: 0.21246375143527985, Final Batch Loss: 0.08187587559223175\n",
      "Epoch 1015, Loss: 0.19435597211122513, Final Batch Loss: 0.08782292157411575\n",
      "Epoch 1016, Loss: 0.26815616339445114, Final Batch Loss: 0.14690501987934113\n",
      "Epoch 1017, Loss: 0.2561737298965454, Final Batch Loss: 0.13305938243865967\n",
      "Epoch 1018, Loss: 0.2227019965648651, Final Batch Loss: 0.11638452112674713\n",
      "Epoch 1019, Loss: 0.21336974948644638, Final Batch Loss: 0.09294050931930542\n",
      "Epoch 1020, Loss: 0.273590549826622, Final Batch Loss: 0.15979769825935364\n",
      "Epoch 1021, Loss: 0.2008921205997467, Final Batch Loss: 0.07384940981864929\n",
      "Epoch 1022, Loss: 0.24193844944238663, Final Batch Loss: 0.13172021508216858\n",
      "Epoch 1023, Loss: 0.20751899480819702, Final Batch Loss: 0.10481677949428558\n",
      "Epoch 1024, Loss: 0.2630172669887543, Final Batch Loss: 0.1386072039604187\n",
      "Epoch 1025, Loss: 0.2362627014517784, Final Batch Loss: 0.1309588998556137\n",
      "Epoch 1026, Loss: 0.242076575756073, Final Batch Loss: 0.10193604230880737\n",
      "Epoch 1027, Loss: 0.24428262561559677, Final Batch Loss: 0.10981162637472153\n",
      "Epoch 1028, Loss: 0.23179882764816284, Final Batch Loss: 0.1191166341304779\n",
      "Epoch 1029, Loss: 0.23678132146596909, Final Batch Loss: 0.12642720341682434\n",
      "Epoch 1030, Loss: 0.1935654729604721, Final Batch Loss: 0.08044128119945526\n",
      "Epoch 1031, Loss: 0.22787082195281982, Final Batch Loss: 0.11789080500602722\n",
      "Epoch 1032, Loss: 0.21050511300563812, Final Batch Loss: 0.11931955814361572\n",
      "Epoch 1033, Loss: 0.25676989555358887, Final Batch Loss: 0.11690221726894379\n",
      "Epoch 1034, Loss: 0.229584738612175, Final Batch Loss: 0.13420110940933228\n",
      "Epoch 1035, Loss: 0.21703751385211945, Final Batch Loss: 0.12100532650947571\n",
      "Epoch 1036, Loss: 0.21882736682891846, Final Batch Loss: 0.05355145037174225\n",
      "Epoch 1037, Loss: 0.22646549344062805, Final Batch Loss: 0.10118147730827332\n",
      "Epoch 1038, Loss: 0.18876488506793976, Final Batch Loss: 0.08514384925365448\n",
      "Epoch 1039, Loss: 0.22657844424247742, Final Batch Loss: 0.10714105516672134\n",
      "Epoch 1040, Loss: 0.21528322994709015, Final Batch Loss: 0.11290436238050461\n",
      "Epoch 1041, Loss: 0.22035212814807892, Final Batch Loss: 0.09358352422714233\n",
      "Epoch 1042, Loss: 0.23066329956054688, Final Batch Loss: 0.10115021467208862\n",
      "Epoch 1043, Loss: 0.2218104749917984, Final Batch Loss: 0.10800281912088394\n",
      "Epoch 1044, Loss: 0.21952706575393677, Final Batch Loss: 0.1110721006989479\n",
      "Epoch 1045, Loss: 0.2733078598976135, Final Batch Loss: 0.13607865571975708\n",
      "Epoch 1046, Loss: 0.22638381272554398, Final Batch Loss: 0.11433527618646622\n",
      "Epoch 1047, Loss: 0.223369762301445, Final Batch Loss: 0.10731891542673111\n",
      "Epoch 1048, Loss: 0.22603236138820648, Final Batch Loss: 0.11751112341880798\n",
      "Epoch 1049, Loss: 0.2251763567328453, Final Batch Loss: 0.13524234294891357\n",
      "Epoch 1050, Loss: 0.23030762374401093, Final Batch Loss: 0.12536825239658356\n",
      "Epoch 1051, Loss: 0.2177053987979889, Final Batch Loss: 0.09395492821931839\n",
      "Epoch 1052, Loss: 0.21174100041389465, Final Batch Loss: 0.10179466009140015\n",
      "Epoch 1053, Loss: 0.2189304083585739, Final Batch Loss: 0.10749019682407379\n",
      "Epoch 1054, Loss: 0.2133738398551941, Final Batch Loss: 0.0926867127418518\n",
      "Epoch 1055, Loss: 0.2161029800772667, Final Batch Loss: 0.1035284548997879\n",
      "Epoch 1056, Loss: 0.20749230682849884, Final Batch Loss: 0.09619458019733429\n",
      "Epoch 1057, Loss: 0.2238362580537796, Final Batch Loss: 0.11302002519369125\n",
      "Epoch 1058, Loss: 0.2414882853627205, Final Batch Loss: 0.1399163007736206\n",
      "Epoch 1059, Loss: 0.23495600372552872, Final Batch Loss: 0.14107385277748108\n",
      "Epoch 1060, Loss: 0.21731054037809372, Final Batch Loss: 0.12396980822086334\n",
      "Epoch 1061, Loss: 0.24423400312662125, Final Batch Loss: 0.12196443974971771\n",
      "Epoch 1062, Loss: 0.2374335303902626, Final Batch Loss: 0.1395377814769745\n",
      "Epoch 1063, Loss: 0.21416226774454117, Final Batch Loss: 0.1105966717004776\n",
      "Epoch 1064, Loss: 0.22441910207271576, Final Batch Loss: 0.11532265692949295\n",
      "Epoch 1065, Loss: 0.23738257586956024, Final Batch Loss: 0.09887024760246277\n",
      "Epoch 1066, Loss: 0.2108226791024208, Final Batch Loss: 0.09679067134857178\n",
      "Epoch 1067, Loss: 0.2688763290643692, Final Batch Loss: 0.14924511313438416\n",
      "Epoch 1068, Loss: 0.26126742362976074, Final Batch Loss: 0.12977585196495056\n",
      "Epoch 1069, Loss: 0.24206720292568207, Final Batch Loss: 0.12560981512069702\n",
      "Epoch 1070, Loss: 0.2623301297426224, Final Batch Loss: 0.12730571627616882\n",
      "Epoch 1071, Loss: 0.241233728826046, Final Batch Loss: 0.11625979095697403\n",
      "Epoch 1072, Loss: 0.23926124721765518, Final Batch Loss: 0.12490551173686981\n",
      "Epoch 1073, Loss: 0.2115931510925293, Final Batch Loss: 0.10257268697023392\n",
      "Epoch 1074, Loss: 0.2067689523100853, Final Batch Loss: 0.0843886062502861\n",
      "Epoch 1075, Loss: 0.22968902438879013, Final Batch Loss: 0.12156215310096741\n",
      "Epoch 1076, Loss: 0.24345683306455612, Final Batch Loss: 0.11595814675092697\n",
      "Epoch 1077, Loss: 0.24960613995790482, Final Batch Loss: 0.13218802213668823\n",
      "Epoch 1078, Loss: 0.3241901099681854, Final Batch Loss: 0.1993921399116516\n",
      "Epoch 1079, Loss: 0.25646230578422546, Final Batch Loss: 0.15514063835144043\n",
      "Epoch 1080, Loss: 0.24527885019779205, Final Batch Loss: 0.12100032716989517\n",
      "Epoch 1081, Loss: 0.2576887756586075, Final Batch Loss: 0.12058964371681213\n",
      "Epoch 1082, Loss: 0.23693818598985672, Final Batch Loss: 0.12421951442956924\n",
      "Epoch 1083, Loss: 0.21977704018354416, Final Batch Loss: 0.10315515846014023\n",
      "Epoch 1084, Loss: 0.24425745755434036, Final Batch Loss: 0.12496966123580933\n",
      "Epoch 1085, Loss: 0.20840782672166824, Final Batch Loss: 0.10497655719518661\n",
      "Epoch 1086, Loss: 0.23668251186609268, Final Batch Loss: 0.1135093942284584\n",
      "Epoch 1087, Loss: 0.250214047729969, Final Batch Loss: 0.14227426052093506\n",
      "Epoch 1088, Loss: 0.2647987902164459, Final Batch Loss: 0.13152460753917694\n",
      "Epoch 1089, Loss: 0.2283945381641388, Final Batch Loss: 0.070325568318367\n",
      "Epoch 1090, Loss: 0.21432793140411377, Final Batch Loss: 0.08167478442192078\n",
      "Epoch 1091, Loss: 0.25952695310115814, Final Batch Loss: 0.1487986147403717\n",
      "Epoch 1092, Loss: 0.24621521681547165, Final Batch Loss: 0.10987711697816849\n",
      "Epoch 1093, Loss: 0.22683730721473694, Final Batch Loss: 0.10655766725540161\n",
      "Epoch 1094, Loss: 0.23287560790777206, Final Batch Loss: 0.137186661362648\n",
      "Epoch 1095, Loss: 0.20288720726966858, Final Batch Loss: 0.06648454070091248\n",
      "Epoch 1096, Loss: 0.23901734501123428, Final Batch Loss: 0.14259161055088043\n",
      "Epoch 1097, Loss: 0.30733684450387955, Final Batch Loss: 0.21979647874832153\n",
      "Epoch 1098, Loss: 0.21561812609434128, Final Batch Loss: 0.1061621829867363\n",
      "Epoch 1099, Loss: 0.2291049212217331, Final Batch Loss: 0.12123676389455795\n",
      "Epoch 1100, Loss: 0.23367812484502792, Final Batch Loss: 0.08412715047597885\n",
      "Epoch 1101, Loss: 0.23504266887903214, Final Batch Loss: 0.1028275117278099\n",
      "Epoch 1102, Loss: 0.19697099179029465, Final Batch Loss: 0.10316620022058487\n",
      "Epoch 1103, Loss: 0.2125024050474167, Final Batch Loss: 0.1197499930858612\n",
      "Epoch 1104, Loss: 0.22573205828666687, Final Batch Loss: 0.10477229207754135\n",
      "Epoch 1105, Loss: 0.2162073850631714, Final Batch Loss: 0.10775362700223923\n",
      "Epoch 1106, Loss: 0.20088525861501694, Final Batch Loss: 0.07932984083890915\n",
      "Epoch 1107, Loss: 0.19330943375825882, Final Batch Loss: 0.08150133490562439\n",
      "Epoch 1108, Loss: 0.2379632592201233, Final Batch Loss: 0.12998883426189423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1109, Loss: 0.21838191896677017, Final Batch Loss: 0.12240520119667053\n",
      "Epoch 1110, Loss: 0.20727591216564178, Final Batch Loss: 0.0956505760550499\n",
      "Epoch 1111, Loss: 0.22928443551063538, Final Batch Loss: 0.1364126205444336\n",
      "Epoch 1112, Loss: 0.23221732676029205, Final Batch Loss: 0.13002780079841614\n",
      "Epoch 1113, Loss: 0.18632429093122482, Final Batch Loss: 0.07568202912807465\n",
      "Epoch 1114, Loss: 0.22872498631477356, Final Batch Loss: 0.12421242147684097\n",
      "Epoch 1115, Loss: 0.22328568994998932, Final Batch Loss: 0.11491606384515762\n",
      "Epoch 1116, Loss: 0.20450825989246368, Final Batch Loss: 0.11166149377822876\n",
      "Epoch 1117, Loss: 0.20504262298345566, Final Batch Loss: 0.10812217742204666\n",
      "Epoch 1118, Loss: 0.22918540239334106, Final Batch Loss: 0.09837290644645691\n",
      "Epoch 1119, Loss: 0.20310213416814804, Final Batch Loss: 0.08147118240594864\n",
      "Epoch 1120, Loss: 0.21181637793779373, Final Batch Loss: 0.12143070250749588\n",
      "Epoch 1121, Loss: 0.22057998925447464, Final Batch Loss: 0.12139451503753662\n",
      "Epoch 1122, Loss: 0.21188542246818542, Final Batch Loss: 0.09969893842935562\n",
      "Epoch 1123, Loss: 0.22128932178020477, Final Batch Loss: 0.12814512848854065\n",
      "Epoch 1124, Loss: 0.224966362118721, Final Batch Loss: 0.1203826516866684\n",
      "Epoch 1125, Loss: 0.24497922509908676, Final Batch Loss: 0.13080734014511108\n",
      "Epoch 1126, Loss: 0.2536534368991852, Final Batch Loss: 0.14276115596294403\n",
      "Epoch 1127, Loss: 0.2267584130167961, Final Batch Loss: 0.1354321390390396\n",
      "Epoch 1128, Loss: 0.20700228214263916, Final Batch Loss: 0.0814756453037262\n",
      "Epoch 1129, Loss: 0.24703609198331833, Final Batch Loss: 0.13189435005187988\n",
      "Epoch 1130, Loss: 0.25975102186203003, Final Batch Loss: 0.14696656167507172\n",
      "Epoch 1131, Loss: 0.20415125042200089, Final Batch Loss: 0.10771400481462479\n",
      "Epoch 1132, Loss: 0.22893190383911133, Final Batch Loss: 0.10845276713371277\n",
      "Epoch 1133, Loss: 0.2255251705646515, Final Batch Loss: 0.11747577041387558\n",
      "Epoch 1134, Loss: 0.2533976659178734, Final Batch Loss: 0.1343742161989212\n",
      "Epoch 1135, Loss: 0.22577811777591705, Final Batch Loss: 0.12276072800159454\n",
      "Epoch 1136, Loss: 0.21152771264314651, Final Batch Loss: 0.12213724851608276\n",
      "Epoch 1137, Loss: 0.2594836577773094, Final Batch Loss: 0.16464123129844666\n",
      "Epoch 1138, Loss: 0.23295707255601883, Final Batch Loss: 0.11088331043720245\n",
      "Epoch 1139, Loss: 0.19801492244005203, Final Batch Loss: 0.10272419452667236\n",
      "Epoch 1140, Loss: 0.2293764054775238, Final Batch Loss: 0.10915495455265045\n",
      "Epoch 1141, Loss: 0.2236829772591591, Final Batch Loss: 0.11374145746231079\n",
      "Epoch 1142, Loss: 0.21786539256572723, Final Batch Loss: 0.10361930727958679\n",
      "Epoch 1143, Loss: 0.24223322421312332, Final Batch Loss: 0.13140647113323212\n",
      "Epoch 1144, Loss: 0.23288488388061523, Final Batch Loss: 0.122576504945755\n",
      "Epoch 1145, Loss: 0.3095991387963295, Final Batch Loss: 0.189137801527977\n",
      "Epoch 1146, Loss: 0.20869162678718567, Final Batch Loss: 0.0809098333120346\n",
      "Epoch 1147, Loss: 0.2048983946442604, Final Batch Loss: 0.10444974154233932\n",
      "Epoch 1148, Loss: 0.22303303331136703, Final Batch Loss: 0.1060963049530983\n",
      "Epoch 1149, Loss: 0.21632909029722214, Final Batch Loss: 0.10390429943799973\n",
      "Epoch 1150, Loss: 0.202909953892231, Final Batch Loss: 0.09181711822748184\n",
      "Epoch 1151, Loss: 0.21859222650527954, Final Batch Loss: 0.11302708834409714\n",
      "Epoch 1152, Loss: 0.203854039311409, Final Batch Loss: 0.0866975262761116\n",
      "Epoch 1153, Loss: 0.2398635819554329, Final Batch Loss: 0.1494765728712082\n",
      "Epoch 1154, Loss: 0.23254074156284332, Final Batch Loss: 0.11463789641857147\n",
      "Epoch 1155, Loss: 0.23396265506744385, Final Batch Loss: 0.11688676476478577\n",
      "Epoch 1156, Loss: 0.21579769253730774, Final Batch Loss: 0.0903812050819397\n",
      "Epoch 1157, Loss: 0.2288930043578148, Final Batch Loss: 0.11339099705219269\n",
      "Epoch 1158, Loss: 0.25087958574295044, Final Batch Loss: 0.13084767758846283\n",
      "Epoch 1159, Loss: 0.2501949518918991, Final Batch Loss: 0.10124917328357697\n",
      "Epoch 1160, Loss: 0.22559372335672379, Final Batch Loss: 0.11499917507171631\n",
      "Epoch 1161, Loss: 0.23148472607135773, Final Batch Loss: 0.14134447276592255\n",
      "Epoch 1162, Loss: 0.21730534732341766, Final Batch Loss: 0.10440617799758911\n",
      "Epoch 1163, Loss: 0.22302626073360443, Final Batch Loss: 0.1327383667230606\n",
      "Epoch 1164, Loss: 0.22157712280750275, Final Batch Loss: 0.09547385573387146\n",
      "Epoch 1165, Loss: 0.24942708760499954, Final Batch Loss: 0.09073225408792496\n",
      "Epoch 1166, Loss: 0.19940169900655746, Final Batch Loss: 0.09070803225040436\n",
      "Epoch 1167, Loss: 0.20416824519634247, Final Batch Loss: 0.08707725256681442\n",
      "Epoch 1168, Loss: 0.23034972697496414, Final Batch Loss: 0.12093233317136765\n",
      "Epoch 1169, Loss: 0.21565190702676773, Final Batch Loss: 0.10436080396175385\n",
      "Epoch 1170, Loss: 0.22302039712667465, Final Batch Loss: 0.12800952792167664\n",
      "Epoch 1171, Loss: 0.20976851135492325, Final Batch Loss: 0.114264115691185\n",
      "Epoch 1172, Loss: 0.23617638647556305, Final Batch Loss: 0.12075894325971603\n",
      "Epoch 1173, Loss: 0.20539097487926483, Final Batch Loss: 0.10016869008541107\n",
      "Epoch 1174, Loss: 0.2222757339477539, Final Batch Loss: 0.13228212296962738\n",
      "Epoch 1175, Loss: 0.2026856616139412, Final Batch Loss: 0.09908390045166016\n",
      "Epoch 1176, Loss: 0.22353792935609818, Final Batch Loss: 0.10879576951265335\n",
      "Epoch 1177, Loss: 0.20195649564266205, Final Batch Loss: 0.0773114487528801\n",
      "Epoch 1178, Loss: 0.2283114269375801, Final Batch Loss: 0.11160007864236832\n",
      "Epoch 1179, Loss: 0.19966962188482285, Final Batch Loss: 0.06879029422998428\n",
      "Epoch 1180, Loss: 0.2239351123571396, Final Batch Loss: 0.12560975551605225\n",
      "Epoch 1181, Loss: 0.28483614325523376, Final Batch Loss: 0.17505715787410736\n",
      "Epoch 1182, Loss: 0.2502896711230278, Final Batch Loss: 0.1215422973036766\n",
      "Epoch 1183, Loss: 0.22905224561691284, Final Batch Loss: 0.12263356149196625\n",
      "Epoch 1184, Loss: 0.1940518096089363, Final Batch Loss: 0.07412505149841309\n",
      "Epoch 1185, Loss: 0.2409197986125946, Final Batch Loss: 0.11482138931751251\n",
      "Epoch 1186, Loss: 0.20866815000772476, Final Batch Loss: 0.11049637943506241\n",
      "Epoch 1187, Loss: 0.2230689600110054, Final Batch Loss: 0.10625692456960678\n",
      "Epoch 1188, Loss: 0.25651196390390396, Final Batch Loss: 0.14510242640972137\n",
      "Epoch 1189, Loss: 0.25922705233097076, Final Batch Loss: 0.12807561457157135\n",
      "Epoch 1190, Loss: 0.23412126302719116, Final Batch Loss: 0.11693967133760452\n",
      "Epoch 1191, Loss: 0.22111766040325165, Final Batch Loss: 0.1342228353023529\n",
      "Epoch 1192, Loss: 0.19855083525180817, Final Batch Loss: 0.08545859903097153\n",
      "Epoch 1193, Loss: 0.22057434171438217, Final Batch Loss: 0.07375765591859818\n",
      "Epoch 1194, Loss: 0.2399274706840515, Final Batch Loss: 0.14710311591625214\n",
      "Epoch 1195, Loss: 0.2939998060464859, Final Batch Loss: 0.1437169313430786\n",
      "Epoch 1196, Loss: 0.2573721408843994, Final Batch Loss: 0.1558995246887207\n",
      "Epoch 1197, Loss: 0.23002034425735474, Final Batch Loss: 0.10407590866088867\n",
      "Epoch 1198, Loss: 0.24259619414806366, Final Batch Loss: 0.12352758646011353\n",
      "Epoch 1199, Loss: 0.23544182628393173, Final Batch Loss: 0.13282370567321777\n",
      "Epoch 1200, Loss: 0.195705346763134, Final Batch Loss: 0.08573973923921585\n",
      "Epoch 1201, Loss: 0.21174567937850952, Final Batch Loss: 0.11446589231491089\n",
      "Epoch 1202, Loss: 0.2262723669409752, Final Batch Loss: 0.09937978535890579\n",
      "Epoch 1203, Loss: 0.22883670032024384, Final Batch Loss: 0.12107784301042557\n",
      "Epoch 1204, Loss: 0.21515274792909622, Final Batch Loss: 0.09681549668312073\n",
      "Epoch 1205, Loss: 0.20099874585866928, Final Batch Loss: 0.10369634628295898\n",
      "Epoch 1206, Loss: 0.2440524473786354, Final Batch Loss: 0.1313476860523224\n",
      "Epoch 1207, Loss: 0.2346631959080696, Final Batch Loss: 0.1422552913427353\n",
      "Epoch 1208, Loss: 0.23006337136030197, Final Batch Loss: 0.14076733589172363\n",
      "Epoch 1209, Loss: 0.22584880888462067, Final Batch Loss: 0.10475581884384155\n",
      "Epoch 1210, Loss: 0.1903614103794098, Final Batch Loss: 0.0795411616563797\n",
      "Epoch 1211, Loss: 0.20292840898036957, Final Batch Loss: 0.10331395268440247\n",
      "Epoch 1212, Loss: 0.20342807471752167, Final Batch Loss: 0.12254524230957031\n",
      "Epoch 1213, Loss: 0.2211749106645584, Final Batch Loss: 0.10906589031219482\n",
      "Epoch 1214, Loss: 0.2115415707230568, Final Batch Loss: 0.11751088500022888\n",
      "Epoch 1215, Loss: 0.2258574143052101, Final Batch Loss: 0.11488432437181473\n",
      "Epoch 1216, Loss: 0.19909880310297012, Final Batch Loss: 0.09630820155143738\n",
      "Epoch 1217, Loss: 0.23112650960683823, Final Batch Loss: 0.14220550656318665\n",
      "Epoch 1218, Loss: 0.2038741484284401, Final Batch Loss: 0.09891128540039062\n",
      "Epoch 1219, Loss: 0.2008483037352562, Final Batch Loss: 0.0919145941734314\n",
      "Epoch 1220, Loss: 0.25702740997076035, Final Batch Loss: 0.12258151918649673\n",
      "Epoch 1221, Loss: 0.21150101721286774, Final Batch Loss: 0.12153360992670059\n",
      "Epoch 1222, Loss: 0.23832707107067108, Final Batch Loss: 0.09787973761558533\n",
      "Epoch 1223, Loss: 0.22831498086452484, Final Batch Loss: 0.13671675324440002\n",
      "Epoch 1224, Loss: 0.2457992061972618, Final Batch Loss: 0.12691044807434082\n",
      "Epoch 1225, Loss: 0.25239740312099457, Final Batch Loss: 0.12367880344390869\n",
      "Epoch 1226, Loss: 0.24971337616443634, Final Batch Loss: 0.12513874471187592\n",
      "Epoch 1227, Loss: 0.19973722100257874, Final Batch Loss: 0.10390757024288177\n",
      "Epoch 1228, Loss: 0.2143816128373146, Final Batch Loss: 0.10068502277135849\n",
      "Epoch 1229, Loss: 0.2327892705798149, Final Batch Loss: 0.11345391720533371\n",
      "Epoch 1230, Loss: 0.2342853546142578, Final Batch Loss: 0.12310467660427094\n",
      "Epoch 1231, Loss: 0.24772122502326965, Final Batch Loss: 0.15515127778053284\n",
      "Epoch 1232, Loss: 0.2240252047777176, Final Batch Loss: 0.1354188323020935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1233, Loss: 0.19577111303806305, Final Batch Loss: 0.09826292097568512\n",
      "Epoch 1234, Loss: 0.19227732717990875, Final Batch Loss: 0.07378030568361282\n",
      "Epoch 1235, Loss: 0.19076794385910034, Final Batch Loss: 0.07654249668121338\n",
      "Epoch 1236, Loss: 0.20421366393566132, Final Batch Loss: 0.08902490139007568\n",
      "Epoch 1237, Loss: 0.20175214111804962, Final Batch Loss: 0.09484957158565521\n",
      "Epoch 1238, Loss: 0.206190824508667, Final Batch Loss: 0.08637239038944244\n",
      "Epoch 1239, Loss: 0.18825533986091614, Final Batch Loss: 0.08631350845098495\n",
      "Epoch 1240, Loss: 0.20221704244613647, Final Batch Loss: 0.07946529984474182\n",
      "Epoch 1241, Loss: 0.24102887511253357, Final Batch Loss: 0.0918973982334137\n",
      "Epoch 1242, Loss: 0.22611697763204575, Final Batch Loss: 0.11332965642213821\n",
      "Epoch 1243, Loss: 0.19819867610931396, Final Batch Loss: 0.13029809296131134\n",
      "Epoch 1244, Loss: 0.19468645751476288, Final Batch Loss: 0.07520009577274323\n",
      "Epoch 1245, Loss: 0.2075788676738739, Final Batch Loss: 0.09943415969610214\n",
      "Epoch 1246, Loss: 0.2473839893937111, Final Batch Loss: 0.11595316976308823\n",
      "Epoch 1247, Loss: 0.21568400412797928, Final Batch Loss: 0.12904858589172363\n",
      "Epoch 1248, Loss: 0.20460953563451767, Final Batch Loss: 0.11004206538200378\n",
      "Epoch 1249, Loss: 0.2217802330851555, Final Batch Loss: 0.11429276317358017\n",
      "Epoch 1250, Loss: 0.2108059525489807, Final Batch Loss: 0.10447558015584946\n",
      "Epoch 1251, Loss: 0.2300584614276886, Final Batch Loss: 0.11762454360723495\n",
      "Epoch 1252, Loss: 0.18922971189022064, Final Batch Loss: 0.08148442208766937\n",
      "Epoch 1253, Loss: 0.2055145725607872, Final Batch Loss: 0.10752177983522415\n",
      "Epoch 1254, Loss: 0.20523027330636978, Final Batch Loss: 0.10412298142910004\n",
      "Epoch 1255, Loss: 0.2354775369167328, Final Batch Loss: 0.14233936369419098\n",
      "Epoch 1256, Loss: 0.17938555032014847, Final Batch Loss: 0.082063227891922\n",
      "Epoch 1257, Loss: 0.26486096531152725, Final Batch Loss: 0.11720109730958939\n",
      "Epoch 1258, Loss: 0.23107657581567764, Final Batch Loss: 0.11646493524312973\n",
      "Epoch 1259, Loss: 0.21143533289432526, Final Batch Loss: 0.10485994070768356\n",
      "Epoch 1260, Loss: 0.22761885821819305, Final Batch Loss: 0.13615773618221283\n",
      "Epoch 1261, Loss: 0.24173059314489365, Final Batch Loss: 0.1417204588651657\n",
      "Epoch 1262, Loss: 0.19795172661542892, Final Batch Loss: 0.08433561772108078\n",
      "Epoch 1263, Loss: 0.2997405380010605, Final Batch Loss: 0.19965767860412598\n",
      "Epoch 1264, Loss: 0.21122293174266815, Final Batch Loss: 0.0843086987733841\n",
      "Epoch 1265, Loss: 0.1909571960568428, Final Batch Loss: 0.07888097316026688\n",
      "Epoch 1266, Loss: 0.23837195336818695, Final Batch Loss: 0.14088723063468933\n",
      "Epoch 1267, Loss: 0.2091115489602089, Final Batch Loss: 0.08355424553155899\n",
      "Epoch 1268, Loss: 0.18370552361011505, Final Batch Loss: 0.0915992259979248\n",
      "Epoch 1269, Loss: 0.27020859718322754, Final Batch Loss: 0.13166986405849457\n",
      "Epoch 1270, Loss: 0.26588254421949387, Final Batch Loss: 0.11499910801649094\n",
      "Epoch 1271, Loss: 0.2572600692510605, Final Batch Loss: 0.13531999289989471\n",
      "Epoch 1272, Loss: 0.21554501354694366, Final Batch Loss: 0.10002177953720093\n",
      "Epoch 1273, Loss: 0.2255771979689598, Final Batch Loss: 0.09963119775056839\n",
      "Epoch 1274, Loss: 0.23535841703414917, Final Batch Loss: 0.12982754409313202\n",
      "Epoch 1275, Loss: 0.2596200183033943, Final Batch Loss: 0.11743142455816269\n",
      "Epoch 1276, Loss: 0.22123456001281738, Final Batch Loss: 0.09476613998413086\n",
      "Epoch 1277, Loss: 0.265500009059906, Final Batch Loss: 0.15363508462905884\n",
      "Epoch 1278, Loss: 0.22943583875894547, Final Batch Loss: 0.1279934197664261\n",
      "Epoch 1279, Loss: 0.22448203712701797, Final Batch Loss: 0.11913570016622543\n",
      "Epoch 1280, Loss: 0.3265343904495239, Final Batch Loss: 0.15581735968589783\n",
      "Epoch 1281, Loss: 0.2851630449295044, Final Batch Loss: 0.14412380754947662\n",
      "Epoch 1282, Loss: 0.2972840368747711, Final Batch Loss: 0.13447995483875275\n",
      "Epoch 1283, Loss: 0.26214034110307693, Final Batch Loss: 0.13887494802474976\n",
      "Epoch 1284, Loss: 0.2507219761610031, Final Batch Loss: 0.1252974271774292\n",
      "Epoch 1285, Loss: 0.2883245199918747, Final Batch Loss: 0.15483348071575165\n",
      "Epoch 1286, Loss: 0.22733253240585327, Final Batch Loss: 0.12272442877292633\n",
      "Epoch 1287, Loss: 0.21864555776119232, Final Batch Loss: 0.10632780194282532\n",
      "Epoch 1288, Loss: 0.23568496853113174, Final Batch Loss: 0.13154546916484833\n",
      "Epoch 1289, Loss: 0.22465819120407104, Final Batch Loss: 0.10334665328264236\n",
      "Epoch 1290, Loss: 0.2294977605342865, Final Batch Loss: 0.13294918835163116\n",
      "Epoch 1291, Loss: 0.25031428039073944, Final Batch Loss: 0.1177622377872467\n",
      "Epoch 1292, Loss: 0.17839089035987854, Final Batch Loss: 0.08195894956588745\n",
      "Epoch 1293, Loss: 0.25166118890047073, Final Batch Loss: 0.15023773908615112\n",
      "Epoch 1294, Loss: 0.2295803502202034, Final Batch Loss: 0.13241389393806458\n",
      "Epoch 1295, Loss: 0.23226319253444672, Final Batch Loss: 0.1344638168811798\n",
      "Epoch 1296, Loss: 0.2556213140487671, Final Batch Loss: 0.10975693166255951\n",
      "Epoch 1297, Loss: 0.22485969960689545, Final Batch Loss: 0.12125979363918304\n",
      "Epoch 1298, Loss: 0.21647171676158905, Final Batch Loss: 0.09496618062257767\n",
      "Epoch 1299, Loss: 0.25673092901706696, Final Batch Loss: 0.14411890506744385\n",
      "Epoch 1300, Loss: 0.29951103031635284, Final Batch Loss: 0.1679266393184662\n",
      "Epoch 1301, Loss: 0.2330826297402382, Final Batch Loss: 0.10119933634996414\n",
      "Epoch 1302, Loss: 0.26079055666923523, Final Batch Loss: 0.1241150051355362\n",
      "Epoch 1303, Loss: 0.22212015092372894, Final Batch Loss: 0.1171647384762764\n",
      "Epoch 1304, Loss: 0.24372664839029312, Final Batch Loss: 0.13141140341758728\n",
      "Epoch 1305, Loss: 0.24535489827394485, Final Batch Loss: 0.11447898298501968\n",
      "Epoch 1306, Loss: 0.2671365439891815, Final Batch Loss: 0.1274951547384262\n",
      "Epoch 1307, Loss: 0.23016774654388428, Final Batch Loss: 0.1388857215642929\n",
      "Epoch 1308, Loss: 0.2291872873902321, Final Batch Loss: 0.11493110656738281\n",
      "Epoch 1309, Loss: 0.2606848478317261, Final Batch Loss: 0.10313472151756287\n",
      "Epoch 1310, Loss: 0.2411906123161316, Final Batch Loss: 0.13512925803661346\n",
      "Epoch 1311, Loss: 0.2595159336924553, Final Batch Loss: 0.13637779653072357\n",
      "Epoch 1312, Loss: 0.22327587753534317, Final Batch Loss: 0.10005191713571548\n",
      "Epoch 1313, Loss: 0.22161508351564407, Final Batch Loss: 0.13802315294742584\n",
      "Epoch 1314, Loss: 0.19709047675132751, Final Batch Loss: 0.09039731323719025\n",
      "Epoch 1315, Loss: 0.20311418175697327, Final Batch Loss: 0.07737644016742706\n",
      "Epoch 1316, Loss: 0.23785604536533356, Final Batch Loss: 0.12561699748039246\n",
      "Epoch 1317, Loss: 0.22484754770994186, Final Batch Loss: 0.13008984923362732\n",
      "Epoch 1318, Loss: 0.25654227286577225, Final Batch Loss: 0.12200941890478134\n",
      "Epoch 1319, Loss: 0.22903554886579514, Final Batch Loss: 0.12687277793884277\n",
      "Epoch 1320, Loss: 0.2375308722257614, Final Batch Loss: 0.12121280282735825\n",
      "Epoch 1321, Loss: 0.3116670548915863, Final Batch Loss: 0.13344796001911163\n",
      "Epoch 1322, Loss: 0.21551542729139328, Final Batch Loss: 0.10626473277807236\n",
      "Epoch 1323, Loss: 0.23099586367607117, Final Batch Loss: 0.10648416727781296\n",
      "Epoch 1324, Loss: 0.21285364031791687, Final Batch Loss: 0.097895547747612\n",
      "Epoch 1325, Loss: 0.21306464076042175, Final Batch Loss: 0.09386587888002396\n",
      "Epoch 1326, Loss: 0.22522402554750443, Final Batch Loss: 0.10624722391366959\n",
      "Epoch 1327, Loss: 0.21270792931318283, Final Batch Loss: 0.0974864587187767\n",
      "Epoch 1328, Loss: 0.217266745865345, Final Batch Loss: 0.13543365895748138\n",
      "Epoch 1329, Loss: 0.237126924097538, Final Batch Loss: 0.11612454056739807\n",
      "Epoch 1330, Loss: 0.24950575828552246, Final Batch Loss: 0.13626240193843842\n",
      "Epoch 1331, Loss: 0.20904134958982468, Final Batch Loss: 0.11168715357780457\n",
      "Epoch 1332, Loss: 0.22472210973501205, Final Batch Loss: 0.10957902669906616\n",
      "Epoch 1333, Loss: 0.21040640771389008, Final Batch Loss: 0.136796236038208\n",
      "Epoch 1334, Loss: 0.2061634585261345, Final Batch Loss: 0.10548244416713715\n",
      "Epoch 1335, Loss: 0.18639573454856873, Final Batch Loss: 0.08493886142969131\n",
      "Epoch 1336, Loss: 0.22480598092079163, Final Batch Loss: 0.1380927860736847\n",
      "Epoch 1337, Loss: 0.22036046534776688, Final Batch Loss: 0.09607440233230591\n",
      "Epoch 1338, Loss: 0.20162682980298996, Final Batch Loss: 0.08425422757863998\n",
      "Epoch 1339, Loss: 0.20447736978530884, Final Batch Loss: 0.10590486973524094\n",
      "Epoch 1340, Loss: 0.20762010663747787, Final Batch Loss: 0.1240072175860405\n",
      "Epoch 1341, Loss: 0.19843144714832306, Final Batch Loss: 0.07620682567358017\n",
      "Epoch 1342, Loss: 0.1929369643330574, Final Batch Loss: 0.1098029688000679\n",
      "Epoch 1343, Loss: 0.23466463387012482, Final Batch Loss: 0.14498312771320343\n",
      "Epoch 1344, Loss: 0.20194410532712936, Final Batch Loss: 0.08935640007257462\n",
      "Epoch 1345, Loss: 0.19695989787578583, Final Batch Loss: 0.08942995220422745\n",
      "Epoch 1346, Loss: 0.22739221155643463, Final Batch Loss: 0.09942446649074554\n",
      "Epoch 1347, Loss: 0.2161072939634323, Final Batch Loss: 0.10761013627052307\n",
      "Epoch 1348, Loss: 0.1985611617565155, Final Batch Loss: 0.09983177483081818\n",
      "Epoch 1349, Loss: 0.19100109487771988, Final Batch Loss: 0.08905325829982758\n",
      "Epoch 1350, Loss: 0.19445031136274338, Final Batch Loss: 0.09132326394319534\n",
      "Epoch 1351, Loss: 0.2959655076265335, Final Batch Loss: 0.17062756419181824\n",
      "Epoch 1352, Loss: 0.1799265518784523, Final Batch Loss: 0.06611694395542145\n",
      "Epoch 1353, Loss: 0.22252511978149414, Final Batch Loss: 0.08495870232582092\n",
      "Epoch 1354, Loss: 0.28855542838573456, Final Batch Loss: 0.15270990133285522\n",
      "Epoch 1355, Loss: 0.23836994916200638, Final Batch Loss: 0.1326073557138443\n",
      "Epoch 1356, Loss: 0.18457932025194168, Final Batch Loss: 0.06530295312404633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1357, Loss: 0.21500644832849503, Final Batch Loss: 0.11582188308238983\n",
      "Epoch 1358, Loss: 0.19923154264688492, Final Batch Loss: 0.12124641239643097\n",
      "Epoch 1359, Loss: 0.2629770487546921, Final Batch Loss: 0.14485090970993042\n",
      "Epoch 1360, Loss: 0.19776210188865662, Final Batch Loss: 0.08674418926239014\n",
      "Epoch 1361, Loss: 0.19641289114952087, Final Batch Loss: 0.09337002038955688\n",
      "Epoch 1362, Loss: 0.18896672129631042, Final Batch Loss: 0.09931038320064545\n",
      "Epoch 1363, Loss: 0.20177169889211655, Final Batch Loss: 0.08819799870252609\n",
      "Epoch 1364, Loss: 0.19773119688034058, Final Batch Loss: 0.08388251066207886\n",
      "Epoch 1365, Loss: 0.18635813891887665, Final Batch Loss: 0.10331420600414276\n",
      "Epoch 1366, Loss: 0.1913280114531517, Final Batch Loss: 0.09244697540998459\n",
      "Epoch 1367, Loss: 0.22870396822690964, Final Batch Loss: 0.11245548725128174\n",
      "Epoch 1368, Loss: 0.2067982628941536, Final Batch Loss: 0.12810000777244568\n",
      "Epoch 1369, Loss: 0.2047622725367546, Final Batch Loss: 0.097974494099617\n",
      "Epoch 1370, Loss: 0.23370519280433655, Final Batch Loss: 0.12445273250341415\n",
      "Epoch 1371, Loss: 0.2344711571931839, Final Batch Loss: 0.15986505150794983\n",
      "Epoch 1372, Loss: 0.21699203550815582, Final Batch Loss: 0.10199004411697388\n",
      "Epoch 1373, Loss: 0.20801003277301788, Final Batch Loss: 0.11294011026620865\n",
      "Epoch 1374, Loss: 0.17847840487957, Final Batch Loss: 0.09133841097354889\n",
      "Epoch 1375, Loss: 0.22214433550834656, Final Batch Loss: 0.12437532842159271\n",
      "Epoch 1376, Loss: 0.23271650075912476, Final Batch Loss: 0.1294364631175995\n",
      "Epoch 1377, Loss: 0.2505430355668068, Final Batch Loss: 0.13882724940776825\n",
      "Epoch 1378, Loss: 0.23132508248090744, Final Batch Loss: 0.13510951399803162\n",
      "Epoch 1379, Loss: 0.23279047012329102, Final Batch Loss: 0.12400147318840027\n",
      "Epoch 1380, Loss: 0.25106242299079895, Final Batch Loss: 0.15369868278503418\n",
      "Epoch 1381, Loss: 0.21162529289722443, Final Batch Loss: 0.10523886233568192\n",
      "Epoch 1382, Loss: 0.2096727341413498, Final Batch Loss: 0.09832565486431122\n",
      "Epoch 1383, Loss: 0.32242685556411743, Final Batch Loss: 0.1086212545633316\n",
      "Epoch 1384, Loss: 0.24156057089567184, Final Batch Loss: 0.12582388520240784\n",
      "Epoch 1385, Loss: 0.21237322688102722, Final Batch Loss: 0.1399902105331421\n",
      "Epoch 1386, Loss: 0.1945686712861061, Final Batch Loss: 0.08450274169445038\n",
      "Epoch 1387, Loss: 0.2350660338997841, Final Batch Loss: 0.1297372579574585\n",
      "Epoch 1388, Loss: 0.24272897839546204, Final Batch Loss: 0.13636310398578644\n",
      "Epoch 1389, Loss: 0.20990588515996933, Final Batch Loss: 0.08642224222421646\n",
      "Epoch 1390, Loss: 0.2541968822479248, Final Batch Loss: 0.1401897519826889\n",
      "Epoch 1391, Loss: 0.25989723205566406, Final Batch Loss: 0.14926356077194214\n",
      "Epoch 1392, Loss: 0.22607316821813583, Final Batch Loss: 0.11033915728330612\n",
      "Epoch 1393, Loss: 0.2111397311091423, Final Batch Loss: 0.11325287818908691\n",
      "Epoch 1394, Loss: 0.20944365859031677, Final Batch Loss: 0.10671229660511017\n",
      "Epoch 1395, Loss: 0.17966821789741516, Final Batch Loss: 0.10707829147577286\n",
      "Epoch 1396, Loss: 0.25306640565395355, Final Batch Loss: 0.13212113082408905\n",
      "Epoch 1397, Loss: 0.18651269376277924, Final Batch Loss: 0.10071563720703125\n",
      "Epoch 1398, Loss: 0.22119373083114624, Final Batch Loss: 0.12855510413646698\n",
      "Epoch 1399, Loss: 0.21403002738952637, Final Batch Loss: 0.09541697800159454\n",
      "Epoch 1400, Loss: 0.19004108756780624, Final Batch Loss: 0.08925043791532516\n",
      "Epoch 1401, Loss: 0.2394888550043106, Final Batch Loss: 0.12815344333648682\n",
      "Epoch 1402, Loss: 0.19919753074645996, Final Batch Loss: 0.11271365731954575\n",
      "Epoch 1403, Loss: 0.18879146128892899, Final Batch Loss: 0.08057095855474472\n",
      "Epoch 1404, Loss: 0.19319048523902893, Final Batch Loss: 0.09341464936733246\n",
      "Epoch 1405, Loss: 0.19330375641584396, Final Batch Loss: 0.10550334304571152\n",
      "Epoch 1406, Loss: 0.20729957520961761, Final Batch Loss: 0.09903740137815475\n",
      "Epoch 1407, Loss: 0.17626632750034332, Final Batch Loss: 0.06568734347820282\n",
      "Epoch 1408, Loss: 0.19847284257411957, Final Batch Loss: 0.10713906586170197\n",
      "Epoch 1409, Loss: 0.18522723764181137, Final Batch Loss: 0.08755125850439072\n",
      "Epoch 1410, Loss: 0.18836262822151184, Final Batch Loss: 0.08849810808897018\n",
      "Epoch 1411, Loss: 0.18413760513067245, Final Batch Loss: 0.0846271961927414\n",
      "Epoch 1412, Loss: 0.19112538546323776, Final Batch Loss: 0.08982641249895096\n",
      "Epoch 1413, Loss: 0.18901223689317703, Final Batch Loss: 0.09137686342000961\n",
      "Epoch 1414, Loss: 0.18820038437843323, Final Batch Loss: 0.09749554842710495\n",
      "Epoch 1415, Loss: 0.19321857392787933, Final Batch Loss: 0.11736251413822174\n",
      "Epoch 1416, Loss: 0.20080724358558655, Final Batch Loss: 0.0854521319270134\n",
      "Epoch 1417, Loss: 0.1986749917268753, Final Batch Loss: 0.08448151499032974\n",
      "Epoch 1418, Loss: 0.19966747611761093, Final Batch Loss: 0.07510314136743546\n",
      "Epoch 1419, Loss: 0.17735958844423294, Final Batch Loss: 0.09074047952890396\n",
      "Epoch 1420, Loss: 0.18561391532421112, Final Batch Loss: 0.0848122090101242\n",
      "Epoch 1421, Loss: 0.21928522735834122, Final Batch Loss: 0.10980945825576782\n",
      "Epoch 1422, Loss: 0.19878226518630981, Final Batch Loss: 0.12243256717920303\n",
      "Epoch 1423, Loss: 0.20227566361427307, Final Batch Loss: 0.09410808235406876\n",
      "Epoch 1424, Loss: 0.1852528378367424, Final Batch Loss: 0.0815964788198471\n",
      "Epoch 1425, Loss: 0.18883635103702545, Final Batch Loss: 0.08456222712993622\n",
      "Epoch 1426, Loss: 0.22429800033569336, Final Batch Loss: 0.12351290881633759\n",
      "Epoch 1427, Loss: 0.18061033636331558, Final Batch Loss: 0.0850415825843811\n",
      "Epoch 1428, Loss: 0.2547312453389168, Final Batch Loss: 0.15276780724525452\n",
      "Epoch 1429, Loss: 0.21089116483926773, Final Batch Loss: 0.09390606731176376\n",
      "Epoch 1430, Loss: 0.18883544206619263, Final Batch Loss: 0.09246301651000977\n",
      "Epoch 1431, Loss: 0.19164416193962097, Final Batch Loss: 0.08736037462949753\n",
      "Epoch 1432, Loss: 0.22226464748382568, Final Batch Loss: 0.07637596130371094\n",
      "Epoch 1433, Loss: 0.1916956976056099, Final Batch Loss: 0.07872509956359863\n",
      "Epoch 1434, Loss: 0.2497420459985733, Final Batch Loss: 0.1377720832824707\n",
      "Epoch 1435, Loss: 0.183064267039299, Final Batch Loss: 0.09402547776699066\n",
      "Epoch 1436, Loss: 0.17893359810113907, Final Batch Loss: 0.08383333683013916\n",
      "Epoch 1437, Loss: 0.20989172905683517, Final Batch Loss: 0.10963313281536102\n",
      "Epoch 1438, Loss: 0.25202345848083496, Final Batch Loss: 0.12543022632598877\n",
      "Epoch 1439, Loss: 0.170661099255085, Final Batch Loss: 0.0715010017156601\n",
      "Epoch 1440, Loss: 0.17810215055942535, Final Batch Loss: 0.08106779307126999\n",
      "Epoch 1441, Loss: 0.20281954109668732, Final Batch Loss: 0.11081589013338089\n",
      "Epoch 1442, Loss: 0.19068612903356552, Final Batch Loss: 0.10955933481454849\n",
      "Epoch 1443, Loss: 0.23713001608848572, Final Batch Loss: 0.13693037629127502\n",
      "Epoch 1444, Loss: 0.23463282734155655, Final Batch Loss: 0.15179343521595\n",
      "Epoch 1445, Loss: 0.16879374533891678, Final Batch Loss: 0.08615090698003769\n",
      "Epoch 1446, Loss: 0.18912725150585175, Final Batch Loss: 0.09752683341503143\n",
      "Epoch 1447, Loss: 0.19148962944746017, Final Batch Loss: 0.10273178666830063\n",
      "Epoch 1448, Loss: 0.19258204102516174, Final Batch Loss: 0.10333872586488724\n",
      "Epoch 1449, Loss: 0.18054912239313126, Final Batch Loss: 0.09519999474287033\n",
      "Epoch 1450, Loss: 0.18050651997327805, Final Batch Loss: 0.07993485033512115\n",
      "Epoch 1451, Loss: 0.1840723529458046, Final Batch Loss: 0.08582434058189392\n",
      "Epoch 1452, Loss: 0.2254030704498291, Final Batch Loss: 0.12479593604803085\n",
      "Epoch 1453, Loss: 0.1767277866601944, Final Batch Loss: 0.1071106567978859\n",
      "Epoch 1454, Loss: 0.18072059750556946, Final Batch Loss: 0.08240880817174911\n",
      "Epoch 1455, Loss: 0.19330152869224548, Final Batch Loss: 0.1058860570192337\n",
      "Epoch 1456, Loss: 0.16688785701990128, Final Batch Loss: 0.09048891812562943\n",
      "Epoch 1457, Loss: 0.19144437462091446, Final Batch Loss: 0.07938852161169052\n",
      "Epoch 1458, Loss: 0.17734591662883759, Final Batch Loss: 0.0919344574213028\n",
      "Epoch 1459, Loss: 0.18097348511219025, Final Batch Loss: 0.08592670410871506\n",
      "Epoch 1460, Loss: 0.19384326040744781, Final Batch Loss: 0.07924215495586395\n",
      "Epoch 1461, Loss: 0.19008664786815643, Final Batch Loss: 0.0988539382815361\n",
      "Epoch 1462, Loss: 0.1916966214776039, Final Batch Loss: 0.09553119540214539\n",
      "Epoch 1463, Loss: 0.19172877073287964, Final Batch Loss: 0.07435660809278488\n",
      "Epoch 1464, Loss: 0.1716247722506523, Final Batch Loss: 0.08214284479618073\n",
      "Epoch 1465, Loss: 0.21839063614606857, Final Batch Loss: 0.13932497799396515\n",
      "Epoch 1466, Loss: 0.17720933258533478, Final Batch Loss: 0.08350161463022232\n",
      "Epoch 1467, Loss: 0.28395236283540726, Final Batch Loss: 0.18609976768493652\n",
      "Epoch 1468, Loss: 0.18748347461223602, Final Batch Loss: 0.10444112867116928\n",
      "Epoch 1469, Loss: 0.21487069875001907, Final Batch Loss: 0.12645290791988373\n",
      "Epoch 1470, Loss: 0.18469128012657166, Final Batch Loss: 0.09898735582828522\n",
      "Epoch 1471, Loss: 0.16897229105234146, Final Batch Loss: 0.08526802062988281\n",
      "Epoch 1472, Loss: 0.18682289123535156, Final Batch Loss: 0.10366791486740112\n",
      "Epoch 1473, Loss: 0.17144273221492767, Final Batch Loss: 0.0760217010974884\n",
      "Epoch 1474, Loss: 0.17642340064048767, Final Batch Loss: 0.0928763821721077\n",
      "Epoch 1475, Loss: 0.17426809668540955, Final Batch Loss: 0.08979031443595886\n",
      "Epoch 1476, Loss: 0.20745321363210678, Final Batch Loss: 0.08141481131315231\n",
      "Epoch 1477, Loss: 0.24948886036872864, Final Batch Loss: 0.16999784111976624\n",
      "Epoch 1478, Loss: 0.2252410277724266, Final Batch Loss: 0.12926487624645233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1479, Loss: 0.19719001650810242, Final Batch Loss: 0.10014570504426956\n",
      "Epoch 1480, Loss: 0.2080182060599327, Final Batch Loss: 0.06749623268842697\n",
      "Epoch 1481, Loss: 0.1820962280035019, Final Batch Loss: 0.07711193710565567\n",
      "Epoch 1482, Loss: 0.20679761469364166, Final Batch Loss: 0.0997096449136734\n",
      "Epoch 1483, Loss: 0.20836535841226578, Final Batch Loss: 0.11296729743480682\n",
      "Epoch 1484, Loss: 0.2370256632566452, Final Batch Loss: 0.13371731340885162\n",
      "Epoch 1485, Loss: 0.2079847976565361, Final Batch Loss: 0.12278291583061218\n",
      "Epoch 1486, Loss: 0.20672332495450974, Final Batch Loss: 0.11555999517440796\n",
      "Epoch 1487, Loss: 0.25682926923036575, Final Batch Loss: 0.11859983950853348\n",
      "Epoch 1488, Loss: 0.18508783727884293, Final Batch Loss: 0.08615081012248993\n",
      "Epoch 1489, Loss: 0.1855214387178421, Final Batch Loss: 0.0850672572851181\n",
      "Epoch 1490, Loss: 0.20679832249879837, Final Batch Loss: 0.07501775771379471\n",
      "Epoch 1491, Loss: 0.1945822313427925, Final Batch Loss: 0.08116059005260468\n",
      "Epoch 1492, Loss: 0.17944278568029404, Final Batch Loss: 0.072405606508255\n",
      "Epoch 1493, Loss: 0.2000908926129341, Final Batch Loss: 0.09646695107221603\n",
      "Epoch 1494, Loss: 0.18593291193246841, Final Batch Loss: 0.07961953431367874\n",
      "Epoch 1495, Loss: 0.20086152851581573, Final Batch Loss: 0.08956306427717209\n",
      "Epoch 1496, Loss: 0.18315276503562927, Final Batch Loss: 0.10471206903457642\n",
      "Epoch 1497, Loss: 0.2267325520515442, Final Batch Loss: 0.11780872941017151\n",
      "Epoch 1498, Loss: 0.17639263719320297, Final Batch Loss: 0.07437309622764587\n",
      "Epoch 1499, Loss: 0.2615911141037941, Final Batch Loss: 0.16840054094791412\n",
      "Epoch 1500, Loss: 0.1917877048254013, Final Batch Loss: 0.09730011224746704\n",
      "Epoch 1501, Loss: 0.2077685296535492, Final Batch Loss: 0.09911251068115234\n",
      "Epoch 1502, Loss: 0.1957389935851097, Final Batch Loss: 0.10352150350809097\n",
      "Epoch 1503, Loss: 0.20884011685848236, Final Batch Loss: 0.10649390518665314\n",
      "Epoch 1504, Loss: 0.20393173396587372, Final Batch Loss: 0.10358676314353943\n",
      "Epoch 1505, Loss: 0.23122063279151917, Final Batch Loss: 0.13053005933761597\n",
      "Epoch 1506, Loss: 0.20300955325365067, Final Batch Loss: 0.10172926634550095\n",
      "Epoch 1507, Loss: 0.24826189875602722, Final Batch Loss: 0.11183814704418182\n",
      "Epoch 1508, Loss: 0.17588234692811966, Final Batch Loss: 0.07113385945558548\n",
      "Epoch 1509, Loss: 0.1666783094406128, Final Batch Loss: 0.07900173217058182\n",
      "Epoch 1510, Loss: 0.24265998601913452, Final Batch Loss: 0.14396502077579498\n",
      "Epoch 1511, Loss: 0.18380460888147354, Final Batch Loss: 0.0843966081738472\n",
      "Epoch 1512, Loss: 0.17273477464914322, Final Batch Loss: 0.09393445402383804\n",
      "Epoch 1513, Loss: 0.18225651234388351, Final Batch Loss: 0.07648710161447525\n",
      "Epoch 1514, Loss: 0.18095213919878006, Final Batch Loss: 0.08087725192308426\n",
      "Epoch 1515, Loss: 0.1826571226119995, Final Batch Loss: 0.0835319235920906\n",
      "Epoch 1516, Loss: 0.16446194052696228, Final Batch Loss: 0.08136638253927231\n",
      "Epoch 1517, Loss: 0.19510376453399658, Final Batch Loss: 0.08438939601182938\n",
      "Epoch 1518, Loss: 0.19873486459255219, Final Batch Loss: 0.10342981666326523\n",
      "Epoch 1519, Loss: 0.16706401854753494, Final Batch Loss: 0.10052899271249771\n",
      "Epoch 1520, Loss: 0.21525506675243378, Final Batch Loss: 0.0947202667593956\n",
      "Epoch 1521, Loss: 0.19032691419124603, Final Batch Loss: 0.0914459228515625\n",
      "Epoch 1522, Loss: 0.17724519968032837, Final Batch Loss: 0.07894758135080338\n",
      "Epoch 1523, Loss: 0.179810531437397, Final Batch Loss: 0.09170977771282196\n",
      "Epoch 1524, Loss: 0.214299276471138, Final Batch Loss: 0.12077885121107101\n",
      "Epoch 1525, Loss: 0.20062506943941116, Final Batch Loss: 0.1117020696401596\n",
      "Epoch 1526, Loss: 0.19519130140542984, Final Batch Loss: 0.09837853908538818\n",
      "Epoch 1527, Loss: 0.19682954996824265, Final Batch Loss: 0.09444914758205414\n",
      "Epoch 1528, Loss: 0.16848809272050858, Final Batch Loss: 0.06184179335832596\n",
      "Epoch 1529, Loss: 0.18172796070575714, Final Batch Loss: 0.09153104573488235\n",
      "Epoch 1530, Loss: 0.18682917207479477, Final Batch Loss: 0.08179762959480286\n",
      "Epoch 1531, Loss: 0.19241821765899658, Final Batch Loss: 0.06983125954866409\n",
      "Epoch 1532, Loss: 0.16671066731214523, Final Batch Loss: 0.09294815361499786\n",
      "Epoch 1533, Loss: 0.17767998576164246, Final Batch Loss: 0.07476126402616501\n",
      "Epoch 1534, Loss: 0.22336778044700623, Final Batch Loss: 0.11480049788951874\n",
      "Epoch 1535, Loss: 0.227095365524292, Final Batch Loss: 0.10452945530414581\n",
      "Epoch 1536, Loss: 0.23622070252895355, Final Batch Loss: 0.08495540916919708\n",
      "Epoch 1537, Loss: 0.18606282770633698, Final Batch Loss: 0.11117641627788544\n",
      "Epoch 1538, Loss: 0.21355042606592178, Final Batch Loss: 0.12217095494270325\n",
      "Epoch 1539, Loss: 0.1990346759557724, Final Batch Loss: 0.11119282245635986\n",
      "Epoch 1540, Loss: 0.1993977501988411, Final Batch Loss: 0.11881217360496521\n",
      "Epoch 1541, Loss: 0.2013462409377098, Final Batch Loss: 0.105048269033432\n",
      "Epoch 1542, Loss: 0.22323214262723923, Final Batch Loss: 0.13192914426326752\n",
      "Epoch 1543, Loss: 0.18359877169132233, Final Batch Loss: 0.08680717647075653\n",
      "Epoch 1544, Loss: 0.187851183116436, Final Batch Loss: 0.07720458507537842\n",
      "Epoch 1545, Loss: 0.24126482754945755, Final Batch Loss: 0.13333941996097565\n",
      "Epoch 1546, Loss: 0.19690819084644318, Final Batch Loss: 0.08618255704641342\n",
      "Epoch 1547, Loss: 0.18921339511871338, Final Batch Loss: 0.09820855408906937\n",
      "Epoch 1548, Loss: 0.1977645307779312, Final Batch Loss: 0.10216379165649414\n",
      "Epoch 1549, Loss: 0.18532739579677582, Final Batch Loss: 0.08543711155653\n",
      "Epoch 1550, Loss: 0.20194438099861145, Final Batch Loss: 0.12928612530231476\n",
      "Epoch 1551, Loss: 0.16439050436019897, Final Batch Loss: 0.07937050610780716\n",
      "Epoch 1552, Loss: 0.18689468502998352, Final Batch Loss: 0.0943097248673439\n",
      "Epoch 1553, Loss: 0.18633348494768143, Final Batch Loss: 0.11711468547582626\n",
      "Epoch 1554, Loss: 0.21282994002103806, Final Batch Loss: 0.1282949000597\n",
      "Epoch 1555, Loss: 0.1638423278927803, Final Batch Loss: 0.08405903726816177\n",
      "Epoch 1556, Loss: 0.17772334069013596, Final Batch Loss: 0.08906476199626923\n",
      "Epoch 1557, Loss: 0.15180009230971336, Final Batch Loss: 0.04903553053736687\n",
      "Epoch 1558, Loss: 0.19547118991613388, Final Batch Loss: 0.06945111602544785\n",
      "Epoch 1559, Loss: 0.19583769887685776, Final Batch Loss: 0.10151154547929764\n",
      "Epoch 1560, Loss: 0.1785203069448471, Final Batch Loss: 0.10135067999362946\n",
      "Epoch 1561, Loss: 0.1776633858680725, Final Batch Loss: 0.07905232906341553\n",
      "Epoch 1562, Loss: 0.21497032791376114, Final Batch Loss: 0.09856829792261124\n",
      "Epoch 1563, Loss: 0.20617453753948212, Final Batch Loss: 0.09453809261322021\n",
      "Epoch 1564, Loss: 0.2259107083082199, Final Batch Loss: 0.11845596134662628\n",
      "Epoch 1565, Loss: 0.1884554773569107, Final Batch Loss: 0.1100991889834404\n",
      "Epoch 1566, Loss: 0.18869521468877792, Final Batch Loss: 0.09237028658390045\n",
      "Epoch 1567, Loss: 0.18012180924415588, Final Batch Loss: 0.11203261464834213\n",
      "Epoch 1568, Loss: 0.1784745678305626, Final Batch Loss: 0.07358874380588531\n",
      "Epoch 1569, Loss: 0.15398559719324112, Final Batch Loss: 0.06398484855890274\n",
      "Epoch 1570, Loss: 0.18649191409349442, Final Batch Loss: 0.09676560014486313\n",
      "Epoch 1571, Loss: 0.1781548634171486, Final Batch Loss: 0.06953643262386322\n",
      "Epoch 1572, Loss: 0.17564869672060013, Final Batch Loss: 0.09114294499158859\n",
      "Epoch 1573, Loss: 0.16948672384023666, Final Batch Loss: 0.07026080042123795\n",
      "Epoch 1574, Loss: 0.17711441218852997, Final Batch Loss: 0.08861193805932999\n",
      "Epoch 1575, Loss: 0.18933259695768356, Final Batch Loss: 0.0841500386595726\n",
      "Epoch 1576, Loss: 0.20737546682357788, Final Batch Loss: 0.0951460748910904\n",
      "Epoch 1577, Loss: 0.1820243000984192, Final Batch Loss: 0.10206115990877151\n",
      "Epoch 1578, Loss: 0.17234215140342712, Final Batch Loss: 0.10854018479585648\n",
      "Epoch 1579, Loss: 0.17325353622436523, Final Batch Loss: 0.09231165796518326\n",
      "Epoch 1580, Loss: 0.19098373502492905, Final Batch Loss: 0.10743468999862671\n",
      "Epoch 1581, Loss: 0.20289387553930283, Final Batch Loss: 0.10278750211000443\n",
      "Epoch 1582, Loss: 0.17112385481595993, Final Batch Loss: 0.07952806353569031\n",
      "Epoch 1583, Loss: 0.17174429446458817, Final Batch Loss: 0.10576041042804718\n",
      "Epoch 1584, Loss: 0.1814102828502655, Final Batch Loss: 0.09616867452859879\n",
      "Epoch 1585, Loss: 0.22239945083856583, Final Batch Loss: 0.1503659188747406\n",
      "Epoch 1586, Loss: 0.18919532746076584, Final Batch Loss: 0.1268760710954666\n",
      "Epoch 1587, Loss: 0.18646526336669922, Final Batch Loss: 0.08465968817472458\n",
      "Epoch 1588, Loss: 0.21373265236616135, Final Batch Loss: 0.1216009184718132\n",
      "Epoch 1589, Loss: 0.19306562095880508, Final Batch Loss: 0.10052396357059479\n",
      "Epoch 1590, Loss: 0.19350706785917282, Final Batch Loss: 0.09746679663658142\n",
      "Epoch 1591, Loss: 0.17843208461999893, Final Batch Loss: 0.09809628129005432\n",
      "Epoch 1592, Loss: 0.1590174213051796, Final Batch Loss: 0.07908110320568085\n",
      "Epoch 1593, Loss: 0.16931528598070145, Final Batch Loss: 0.0989329144358635\n",
      "Epoch 1594, Loss: 0.17912157624959946, Final Batch Loss: 0.07166699320077896\n",
      "Epoch 1595, Loss: 0.17562340199947357, Final Batch Loss: 0.08404631912708282\n",
      "Epoch 1596, Loss: 0.1603911817073822, Final Batch Loss: 0.06492312252521515\n",
      "Epoch 1597, Loss: 0.18097840249538422, Final Batch Loss: 0.08911420404911041\n",
      "Epoch 1598, Loss: 0.163311168551445, Final Batch Loss: 0.07722315937280655\n",
      "Epoch 1599, Loss: 0.1900329440832138, Final Batch Loss: 0.10561985522508621\n",
      "Epoch 1600, Loss: 0.2098354771733284, Final Batch Loss: 0.12114164978265762\n",
      "Epoch 1601, Loss: 0.19372418522834778, Final Batch Loss: 0.09841626137495041\n",
      "Epoch 1602, Loss: 0.19329701364040375, Final Batch Loss: 0.10251015424728394\n",
      "Epoch 1603, Loss: 0.17082852125167847, Final Batch Loss: 0.06708508729934692\n",
      "Epoch 1604, Loss: 0.19218554347753525, Final Batch Loss: 0.08218852430582047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1605, Loss: 0.16766737401485443, Final Batch Loss: 0.08011429011821747\n",
      "Epoch 1606, Loss: 0.20483117550611496, Final Batch Loss: 0.1229313462972641\n",
      "Epoch 1607, Loss: 0.16923373192548752, Final Batch Loss: 0.07052133977413177\n",
      "Epoch 1608, Loss: 0.19685839861631393, Final Batch Loss: 0.11443190276622772\n",
      "Epoch 1609, Loss: 0.2003161609172821, Final Batch Loss: 0.10372824221849442\n",
      "Epoch 1610, Loss: 0.19023732095956802, Final Batch Loss: 0.10982360690832138\n",
      "Epoch 1611, Loss: 0.1731257364153862, Final Batch Loss: 0.08065588027238846\n",
      "Epoch 1612, Loss: 0.18901105225086212, Final Batch Loss: 0.10035868734121323\n",
      "Epoch 1613, Loss: 0.17025244235992432, Final Batch Loss: 0.06840267777442932\n",
      "Epoch 1614, Loss: 0.156291164457798, Final Batch Loss: 0.07620050758123398\n",
      "Epoch 1615, Loss: 0.20649339258670807, Final Batch Loss: 0.10457054525613785\n",
      "Epoch 1616, Loss: 0.1568790003657341, Final Batch Loss: 0.08551293611526489\n",
      "Epoch 1617, Loss: 0.16275397688150406, Final Batch Loss: 0.10164632648229599\n",
      "Epoch 1618, Loss: 0.17115890979766846, Final Batch Loss: 0.09327741712331772\n",
      "Epoch 1619, Loss: 0.1838015913963318, Final Batch Loss: 0.08455850183963776\n",
      "Epoch 1620, Loss: 0.1904471516609192, Final Batch Loss: 0.06732556223869324\n",
      "Epoch 1621, Loss: 0.16764293611049652, Final Batch Loss: 0.07938114553689957\n",
      "Epoch 1622, Loss: 0.1938389092683792, Final Batch Loss: 0.0928182601928711\n",
      "Epoch 1623, Loss: 0.19158247113227844, Final Batch Loss: 0.1267385631799698\n",
      "Epoch 1624, Loss: 0.1912406161427498, Final Batch Loss: 0.1163380816578865\n",
      "Epoch 1625, Loss: 0.1663033813238144, Final Batch Loss: 0.0733271911740303\n",
      "Epoch 1626, Loss: 0.16455084085464478, Final Batch Loss: 0.06961500644683838\n",
      "Epoch 1627, Loss: 0.15313240885734558, Final Batch Loss: 0.08711253106594086\n",
      "Epoch 1628, Loss: 0.17582811415195465, Final Batch Loss: 0.097294382750988\n",
      "Epoch 1629, Loss: 0.2020864114165306, Final Batch Loss: 0.10465077310800552\n",
      "Epoch 1630, Loss: 0.20436275005340576, Final Batch Loss: 0.10858511179685593\n",
      "Epoch 1631, Loss: 0.16284797340631485, Final Batch Loss: 0.07937097549438477\n",
      "Epoch 1632, Loss: 0.19012881815433502, Final Batch Loss: 0.10885310918092728\n",
      "Epoch 1633, Loss: 0.2108234241604805, Final Batch Loss: 0.12538646161556244\n",
      "Epoch 1634, Loss: 0.1870800442993641, Final Batch Loss: 0.12549728155136108\n",
      "Epoch 1635, Loss: 0.23626137524843216, Final Batch Loss: 0.14500543475151062\n",
      "Epoch 1636, Loss: 0.1959068328142166, Final Batch Loss: 0.1045897975564003\n",
      "Epoch 1637, Loss: 0.19420266896486282, Final Batch Loss: 0.10844200104475021\n",
      "Epoch 1638, Loss: 0.18987319618463516, Final Batch Loss: 0.09346647560596466\n",
      "Epoch 1639, Loss: 0.19305117428302765, Final Batch Loss: 0.08198591321706772\n",
      "Epoch 1640, Loss: 0.16624029725790024, Final Batch Loss: 0.08385772258043289\n",
      "Epoch 1641, Loss: 0.17940790206193924, Final Batch Loss: 0.10673308372497559\n",
      "Epoch 1642, Loss: 0.375144861638546, Final Batch Loss: 0.27963486313819885\n",
      "Epoch 1643, Loss: 0.14591284468770027, Final Batch Loss: 0.05208692327141762\n",
      "Epoch 1644, Loss: 0.1456684246659279, Final Batch Loss: 0.06666292250156403\n",
      "Epoch 1645, Loss: 0.1664837747812271, Final Batch Loss: 0.08819376677274704\n",
      "Epoch 1646, Loss: 0.15979468822479248, Final Batch Loss: 0.08526161313056946\n",
      "Epoch 1647, Loss: 0.16766047477722168, Final Batch Loss: 0.0948730930685997\n",
      "Epoch 1648, Loss: 0.1856084242463112, Final Batch Loss: 0.08994597941637039\n",
      "Epoch 1649, Loss: 0.15824005752801895, Final Batch Loss: 0.07488498091697693\n",
      "Epoch 1650, Loss: 0.16725033521652222, Final Batch Loss: 0.0758228749036789\n",
      "Epoch 1651, Loss: 0.17418747395277023, Final Batch Loss: 0.07931011915206909\n",
      "Epoch 1652, Loss: 0.196283258497715, Final Batch Loss: 0.10813114792108536\n",
      "Epoch 1653, Loss: 0.20325688272714615, Final Batch Loss: 0.07181619852781296\n",
      "Epoch 1654, Loss: 0.15090836212038994, Final Batch Loss: 0.06018028035759926\n",
      "Epoch 1655, Loss: 0.18797682225704193, Final Batch Loss: 0.09749773144721985\n",
      "Epoch 1656, Loss: 0.19726938754320145, Final Batch Loss: 0.0640222355723381\n",
      "Epoch 1657, Loss: 0.17037640511989594, Final Batch Loss: 0.0663708746433258\n",
      "Epoch 1658, Loss: 0.18138396739959717, Final Batch Loss: 0.0862518772482872\n",
      "Epoch 1659, Loss: 0.20731355249881744, Final Batch Loss: 0.09273272007703781\n",
      "Epoch 1660, Loss: 0.14942994713783264, Final Batch Loss: 0.055007509887218475\n",
      "Epoch 1661, Loss: 0.16235797852277756, Final Batch Loss: 0.07763813436031342\n",
      "Epoch 1662, Loss: 0.17411796748638153, Final Batch Loss: 0.08081046491861343\n",
      "Epoch 1663, Loss: 0.15905959904193878, Final Batch Loss: 0.0786135271191597\n",
      "Epoch 1664, Loss: 0.15877944231033325, Final Batch Loss: 0.07742580771446228\n",
      "Epoch 1665, Loss: 0.17837634682655334, Final Batch Loss: 0.10449569672346115\n",
      "Epoch 1666, Loss: 0.17317292839288712, Final Batch Loss: 0.07627856731414795\n",
      "Epoch 1667, Loss: 0.15616418421268463, Final Batch Loss: 0.07525494694709778\n",
      "Epoch 1668, Loss: 0.16824139654636383, Final Batch Loss: 0.08385330438613892\n",
      "Epoch 1669, Loss: 0.17129580676555634, Final Batch Loss: 0.09759250283241272\n",
      "Epoch 1670, Loss: 0.15953578054904938, Final Batch Loss: 0.07715602964162827\n",
      "Epoch 1671, Loss: 0.1897081881761551, Final Batch Loss: 0.11804952472448349\n",
      "Epoch 1672, Loss: 0.15682698786258698, Final Batch Loss: 0.06472627073526382\n",
      "Epoch 1673, Loss: 0.16163742542266846, Final Batch Loss: 0.07655849307775497\n",
      "Epoch 1674, Loss: 0.15707676857709885, Final Batch Loss: 0.07050514221191406\n",
      "Epoch 1675, Loss: 0.2046581208705902, Final Batch Loss: 0.10834723711013794\n",
      "Epoch 1676, Loss: 0.14807501435279846, Final Batch Loss: 0.06653867661952972\n",
      "Epoch 1677, Loss: 0.20609837025403976, Final Batch Loss: 0.12051224708557129\n",
      "Epoch 1678, Loss: 0.17672289162874222, Final Batch Loss: 0.07833121716976166\n",
      "Epoch 1679, Loss: 0.2823983430862427, Final Batch Loss: 0.20429490506649017\n",
      "Epoch 1680, Loss: 0.17897279560565948, Final Batch Loss: 0.08466532826423645\n",
      "Epoch 1681, Loss: 0.18573995679616928, Final Batch Loss: 0.10114657133817673\n",
      "Epoch 1682, Loss: 0.19133971631526947, Final Batch Loss: 0.10400992631912231\n",
      "Epoch 1683, Loss: 0.21325061470270157, Final Batch Loss: 0.11242969334125519\n",
      "Epoch 1684, Loss: 0.20125915855169296, Final Batch Loss: 0.09442519396543503\n",
      "Epoch 1685, Loss: 0.17430832237005234, Final Batch Loss: 0.11014223098754883\n",
      "Epoch 1686, Loss: 0.18963248282670975, Final Batch Loss: 0.0917033702135086\n",
      "Epoch 1687, Loss: 0.1673029288649559, Final Batch Loss: 0.08476140350103378\n",
      "Epoch 1688, Loss: 0.1569996029138565, Final Batch Loss: 0.07357535511255264\n",
      "Epoch 1689, Loss: 0.2267979010939598, Final Batch Loss: 0.1294965147972107\n",
      "Epoch 1690, Loss: 0.19943740963935852, Final Batch Loss: 0.10592685639858246\n",
      "Epoch 1691, Loss: 0.16304591298103333, Final Batch Loss: 0.08612485229969025\n",
      "Epoch 1692, Loss: 0.18430039286613464, Final Batch Loss: 0.0800616666674614\n",
      "Epoch 1693, Loss: 0.17906101793050766, Final Batch Loss: 0.07793601602315903\n",
      "Epoch 1694, Loss: 0.17058277130126953, Final Batch Loss: 0.07872523367404938\n",
      "Epoch 1695, Loss: 0.17220623791217804, Final Batch Loss: 0.08082666248083115\n",
      "Epoch 1696, Loss: 0.16162632405757904, Final Batch Loss: 0.07999303191900253\n",
      "Epoch 1697, Loss: 0.23852652311325073, Final Batch Loss: 0.138858363032341\n",
      "Epoch 1698, Loss: 0.2117043361067772, Final Batch Loss: 0.07627015560865402\n",
      "Epoch 1699, Loss: 0.1765032261610031, Final Batch Loss: 0.07928738743066788\n",
      "Epoch 1700, Loss: 0.16579922288656235, Final Batch Loss: 0.09000025689601898\n",
      "Epoch 1701, Loss: 0.16531972587108612, Final Batch Loss: 0.059688687324523926\n",
      "Epoch 1702, Loss: 0.2032136395573616, Final Batch Loss: 0.12723614275455475\n",
      "Epoch 1703, Loss: 0.19652609527111053, Final Batch Loss: 0.08736508339643478\n",
      "Epoch 1704, Loss: 0.20274055749177933, Final Batch Loss: 0.12238367646932602\n",
      "Epoch 1705, Loss: 0.18318301439285278, Final Batch Loss: 0.07618451863527298\n",
      "Epoch 1706, Loss: 0.275040403008461, Final Batch Loss: 0.18098847568035126\n",
      "Epoch 1707, Loss: 0.2050214633345604, Final Batch Loss: 0.11082720011472702\n",
      "Epoch 1708, Loss: 0.20959050953388214, Final Batch Loss: 0.1340475082397461\n",
      "Epoch 1709, Loss: 0.19474148005247116, Final Batch Loss: 0.10023847967386246\n",
      "Epoch 1710, Loss: 0.285238653421402, Final Batch Loss: 0.14267843961715698\n",
      "Epoch 1711, Loss: 0.20119312405586243, Final Batch Loss: 0.1085297092795372\n",
      "Epoch 1712, Loss: 0.14506448060274124, Final Batch Loss: 0.07304659485816956\n",
      "Epoch 1713, Loss: 0.1873476430773735, Final Batch Loss: 0.10525649785995483\n",
      "Epoch 1714, Loss: 0.16633258759975433, Final Batch Loss: 0.06510750949382782\n",
      "Epoch 1715, Loss: 0.15867622941732407, Final Batch Loss: 0.06784889101982117\n",
      "Epoch 1716, Loss: 0.1804906129837036, Final Batch Loss: 0.08250249177217484\n",
      "Epoch 1717, Loss: 0.15980151295661926, Final Batch Loss: 0.08738463371992111\n",
      "Epoch 1718, Loss: 0.2390846386551857, Final Batch Loss: 0.14337965846061707\n",
      "Epoch 1719, Loss: 0.17810654640197754, Final Batch Loss: 0.07145553827285767\n",
      "Epoch 1720, Loss: 0.16253656148910522, Final Batch Loss: 0.08453258872032166\n",
      "Epoch 1721, Loss: 0.1801026687026024, Final Batch Loss: 0.08119365572929382\n",
      "Epoch 1722, Loss: 0.1514371633529663, Final Batch Loss: 0.07648997753858566\n",
      "Epoch 1723, Loss: 0.16575108468532562, Final Batch Loss: 0.08233592659235\n",
      "Epoch 1724, Loss: 0.16752782464027405, Final Batch Loss: 0.08409206569194794\n",
      "Epoch 1725, Loss: 0.2557676210999489, Final Batch Loss: 0.17116616666316986\n",
      "Epoch 1726, Loss: 0.19491304457187653, Final Batch Loss: 0.08431629091501236\n",
      "Epoch 1727, Loss: 0.18739962577819824, Final Batch Loss: 0.12068535387516022\n",
      "Epoch 1728, Loss: 0.2887602001428604, Final Batch Loss: 0.19444894790649414\n",
      "Epoch 1729, Loss: 0.175877645611763, Final Batch Loss: 0.09092450886964798\n",
      "Epoch 1730, Loss: 0.17754695564508438, Final Batch Loss: 0.10199450701475143\n",
      "Epoch 1731, Loss: 0.17900198698043823, Final Batch Loss: 0.09429532289505005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1732, Loss: 0.17981213331222534, Final Batch Loss: 0.09212728589773178\n",
      "Epoch 1733, Loss: 0.16084563732147217, Final Batch Loss: 0.07449044287204742\n",
      "Epoch 1734, Loss: 0.22022728621959686, Final Batch Loss: 0.10273963958024979\n",
      "Epoch 1735, Loss: 0.16674171388149261, Final Batch Loss: 0.06842692941427231\n",
      "Epoch 1736, Loss: 0.17778099328279495, Final Batch Loss: 0.08852991461753845\n",
      "Epoch 1737, Loss: 0.18403686583042145, Final Batch Loss: 0.0782628059387207\n",
      "Epoch 1738, Loss: 0.16985076665878296, Final Batch Loss: 0.07896532118320465\n",
      "Epoch 1739, Loss: 0.17810869216918945, Final Batch Loss: 0.09698237478733063\n",
      "Epoch 1740, Loss: 0.2329300493001938, Final Batch Loss: 0.1248999685049057\n",
      "Epoch 1741, Loss: 0.26312507688999176, Final Batch Loss: 0.14382420480251312\n",
      "Epoch 1742, Loss: 0.22287793457508087, Final Batch Loss: 0.1358698308467865\n",
      "Epoch 1743, Loss: 0.1858665570616722, Final Batch Loss: 0.09764793515205383\n",
      "Epoch 1744, Loss: 0.16620873659849167, Final Batch Loss: 0.0783463567495346\n",
      "Epoch 1745, Loss: 0.2504163533449173, Final Batch Loss: 0.123042032122612\n",
      "Epoch 1746, Loss: 0.216574028134346, Final Batch Loss: 0.11842283606529236\n",
      "Epoch 1747, Loss: 0.20981289446353912, Final Batch Loss: 0.10082060098648071\n",
      "Epoch 1748, Loss: 0.1934889703989029, Final Batch Loss: 0.0990932285785675\n",
      "Epoch 1749, Loss: 0.17004745453596115, Final Batch Loss: 0.09288977086544037\n",
      "Epoch 1750, Loss: 0.17986749112606049, Final Batch Loss: 0.09235266596078873\n",
      "Epoch 1751, Loss: 0.20960286259651184, Final Batch Loss: 0.08704562485218048\n",
      "Epoch 1752, Loss: 0.17528721690177917, Final Batch Loss: 0.0738382488489151\n",
      "Epoch 1753, Loss: 0.16843918710947037, Final Batch Loss: 0.08739732205867767\n",
      "Epoch 1754, Loss: 0.20811935514211655, Final Batch Loss: 0.09491755813360214\n",
      "Epoch 1755, Loss: 0.20122305303812027, Final Batch Loss: 0.09998883306980133\n",
      "Epoch 1756, Loss: 0.1689755916595459, Final Batch Loss: 0.08708149194717407\n",
      "Epoch 1757, Loss: 0.1825980842113495, Final Batch Loss: 0.10206609964370728\n",
      "Epoch 1758, Loss: 0.15884146839380264, Final Batch Loss: 0.0839652493596077\n",
      "Epoch 1759, Loss: 0.15324097126722336, Final Batch Loss: 0.07607967406511307\n",
      "Epoch 1760, Loss: 0.1890697255730629, Final Batch Loss: 0.10301868617534637\n",
      "Epoch 1761, Loss: 0.1642899289727211, Final Batch Loss: 0.07382934540510178\n",
      "Epoch 1762, Loss: 0.18534979969263077, Final Batch Loss: 0.10867199301719666\n",
      "Epoch 1763, Loss: 0.1948370411992073, Final Batch Loss: 0.11747755855321884\n",
      "Epoch 1764, Loss: 0.17980117350816727, Final Batch Loss: 0.09059985727071762\n",
      "Epoch 1765, Loss: 0.18489354103803635, Final Batch Loss: 0.10943553596735\n",
      "Epoch 1766, Loss: 0.15457763522863388, Final Batch Loss: 0.06026354432106018\n",
      "Epoch 1767, Loss: 0.1563737913966179, Final Batch Loss: 0.0684867724776268\n",
      "Epoch 1768, Loss: 0.17018266022205353, Final Batch Loss: 0.0799875557422638\n",
      "Epoch 1769, Loss: 0.16425082832574844, Final Batch Loss: 0.07905884087085724\n",
      "Epoch 1770, Loss: 0.1761719360947609, Final Batch Loss: 0.07624199241399765\n",
      "Epoch 1771, Loss: 0.15822423249483109, Final Batch Loss: 0.08223032206296921\n",
      "Epoch 1772, Loss: 0.15132175385951996, Final Batch Loss: 0.0631391853094101\n",
      "Epoch 1773, Loss: 0.15030113607645035, Final Batch Loss: 0.06507597863674164\n",
      "Epoch 1774, Loss: 0.1529381200671196, Final Batch Loss: 0.07802779227495193\n",
      "Epoch 1775, Loss: 0.16696234047412872, Final Batch Loss: 0.10139735788106918\n",
      "Epoch 1776, Loss: 0.1618887111544609, Final Batch Loss: 0.06081172823905945\n",
      "Epoch 1777, Loss: 0.15575017780065536, Final Batch Loss: 0.07034129649400711\n",
      "Epoch 1778, Loss: 0.14371641725301743, Final Batch Loss: 0.06365644931793213\n",
      "Epoch 1779, Loss: 0.1501762419939041, Final Batch Loss: 0.07888071984052658\n",
      "Epoch 1780, Loss: 0.14167404919862747, Final Batch Loss: 0.07597514241933823\n",
      "Epoch 1781, Loss: 0.1611453965306282, Final Batch Loss: 0.06364282965660095\n",
      "Epoch 1782, Loss: 0.14789823442697525, Final Batch Loss: 0.06854651123285294\n",
      "Epoch 1783, Loss: 0.15765325725078583, Final Batch Loss: 0.08834279328584671\n",
      "Epoch 1784, Loss: 0.14155598729848862, Final Batch Loss: 0.07726691663265228\n",
      "Epoch 1785, Loss: 0.17870669811964035, Final Batch Loss: 0.10760192573070526\n",
      "Epoch 1786, Loss: 0.1522907093167305, Final Batch Loss: 0.07521069794893265\n",
      "Epoch 1787, Loss: 0.14219874143600464, Final Batch Loss: 0.06283458322286606\n",
      "Epoch 1788, Loss: 0.23160504549741745, Final Batch Loss: 0.14009259641170502\n",
      "Epoch 1789, Loss: 0.1544366516172886, Final Batch Loss: 0.09268122911453247\n",
      "Epoch 1790, Loss: 0.16214555501937866, Final Batch Loss: 0.07739079743623734\n",
      "Epoch 1791, Loss: 0.18071547150611877, Final Batch Loss: 0.1002899557352066\n",
      "Epoch 1792, Loss: 0.15359797328710556, Final Batch Loss: 0.10667265206575394\n",
      "Epoch 1793, Loss: 0.16622944176197052, Final Batch Loss: 0.07730063050985336\n",
      "Epoch 1794, Loss: 0.17707782238721848, Final Batch Loss: 0.09240002930164337\n",
      "Epoch 1795, Loss: 0.19594870507717133, Final Batch Loss: 0.06363588571548462\n",
      "Epoch 1796, Loss: 0.17120616883039474, Final Batch Loss: 0.08684081584215164\n",
      "Epoch 1797, Loss: 0.16873039305210114, Final Batch Loss: 0.09258807450532913\n",
      "Epoch 1798, Loss: 0.15034791082143784, Final Batch Loss: 0.06267210096120834\n",
      "Epoch 1799, Loss: 0.1642872914671898, Final Batch Loss: 0.08851101994514465\n",
      "Epoch 1800, Loss: 0.15737059712409973, Final Batch Loss: 0.08314540237188339\n",
      "Epoch 1801, Loss: 0.15092696249485016, Final Batch Loss: 0.08034032583236694\n",
      "Epoch 1802, Loss: 0.1355506330728531, Final Batch Loss: 0.06470607966184616\n",
      "Epoch 1803, Loss: 0.14469026029109955, Final Batch Loss: 0.08073963969945908\n",
      "Epoch 1804, Loss: 0.13983765244483948, Final Batch Loss: 0.07030406594276428\n",
      "Epoch 1805, Loss: 0.14584176987409592, Final Batch Loss: 0.06553138047456741\n",
      "Epoch 1806, Loss: 0.20344024151563644, Final Batch Loss: 0.09037036448717117\n",
      "Epoch 1807, Loss: 0.15116515010595322, Final Batch Loss: 0.08449657261371613\n",
      "Epoch 1808, Loss: 0.15861522406339645, Final Batch Loss: 0.07085546106100082\n",
      "Epoch 1809, Loss: 0.18908419460058212, Final Batch Loss: 0.11052354425191879\n",
      "Epoch 1810, Loss: 0.15862859785556793, Final Batch Loss: 0.08231493830680847\n",
      "Epoch 1811, Loss: 0.13523411750793457, Final Batch Loss: 0.069185271859169\n",
      "Epoch 1812, Loss: 0.14615095406770706, Final Batch Loss: 0.06874632090330124\n",
      "Epoch 1813, Loss: 0.1514134742319584, Final Batch Loss: 0.05752081796526909\n",
      "Epoch 1814, Loss: 0.12779001891613007, Final Batch Loss: 0.05993594229221344\n",
      "Epoch 1815, Loss: 0.15211346000432968, Final Batch Loss: 0.09458828717470169\n",
      "Epoch 1816, Loss: 0.17967147380113602, Final Batch Loss: 0.09513021260499954\n",
      "Epoch 1817, Loss: 0.14661870151758194, Final Batch Loss: 0.08967303484678268\n",
      "Epoch 1818, Loss: 0.1486772820353508, Final Batch Loss: 0.07137071341276169\n",
      "Epoch 1819, Loss: 0.12000641599297523, Final Batch Loss: 0.04469480738043785\n",
      "Epoch 1820, Loss: 0.16642207652330399, Final Batch Loss: 0.09511782228946686\n",
      "Epoch 1821, Loss: 0.15246791392564774, Final Batch Loss: 0.08113215863704681\n",
      "Epoch 1822, Loss: 0.13741489872336388, Final Batch Loss: 0.05807867273688316\n",
      "Epoch 1823, Loss: 0.14666063711047173, Final Batch Loss: 0.04679504409432411\n",
      "Epoch 1824, Loss: 0.13986200839281082, Final Batch Loss: 0.05129903554916382\n",
      "Epoch 1825, Loss: 0.13513097167015076, Final Batch Loss: 0.04637843370437622\n",
      "Epoch 1826, Loss: 0.13891660422086716, Final Batch Loss: 0.06676897406578064\n",
      "Epoch 1827, Loss: 0.20811321586370468, Final Batch Loss: 0.13493528962135315\n",
      "Epoch 1828, Loss: 0.1570853292942047, Final Batch Loss: 0.08223608881235123\n",
      "Epoch 1829, Loss: 0.1314559131860733, Final Batch Loss: 0.06281708925962448\n",
      "Epoch 1830, Loss: 0.17649465054273605, Final Batch Loss: 0.10347830504179001\n",
      "Epoch 1831, Loss: 0.15164092928171158, Final Batch Loss: 0.0706980898976326\n",
      "Epoch 1832, Loss: 0.15659411996603012, Final Batch Loss: 0.07508623600006104\n",
      "Epoch 1833, Loss: 0.16639742255210876, Final Batch Loss: 0.0822540745139122\n",
      "Epoch 1834, Loss: 0.22611406445503235, Final Batch Loss: 0.1239626407623291\n",
      "Epoch 1835, Loss: 0.18062763661146164, Final Batch Loss: 0.12115009129047394\n",
      "Epoch 1836, Loss: 0.162984699010849, Final Batch Loss: 0.0949457511305809\n",
      "Epoch 1837, Loss: 0.15322039276361465, Final Batch Loss: 0.06977900117635727\n",
      "Epoch 1838, Loss: 0.16392339020967484, Final Batch Loss: 0.07999174296855927\n",
      "Epoch 1839, Loss: 0.13215239718556404, Final Batch Loss: 0.058234523981809616\n",
      "Epoch 1840, Loss: 0.14807170629501343, Final Batch Loss: 0.07604911923408508\n",
      "Epoch 1841, Loss: 0.15072844177484512, Final Batch Loss: 0.07325004786252975\n",
      "Epoch 1842, Loss: 0.1446724683046341, Final Batch Loss: 0.07916275411844254\n",
      "Epoch 1843, Loss: 0.18113991618156433, Final Batch Loss: 0.11237967759370804\n",
      "Epoch 1844, Loss: 0.1426297277212143, Final Batch Loss: 0.08082432299852371\n",
      "Epoch 1845, Loss: 0.16535022854804993, Final Batch Loss: 0.07461247593164444\n",
      "Epoch 1846, Loss: 0.1442159339785576, Final Batch Loss: 0.08620493113994598\n",
      "Epoch 1847, Loss: 0.14409233629703522, Final Batch Loss: 0.06563020497560501\n",
      "Epoch 1848, Loss: 0.16543490439653397, Final Batch Loss: 0.08624367415904999\n",
      "Epoch 1849, Loss: 0.23265808820724487, Final Batch Loss: 0.09536702930927277\n",
      "Epoch 1850, Loss: 0.1147674210369587, Final Batch Loss: 0.0425461046397686\n",
      "Epoch 1851, Loss: 0.12357217818498611, Final Batch Loss: 0.04969486594200134\n",
      "Epoch 1852, Loss: 0.14700856059789658, Final Batch Loss: 0.09875702112913132\n",
      "Epoch 1853, Loss: 0.16197213530540466, Final Batch Loss: 0.061872392892837524\n",
      "Epoch 1854, Loss: 0.14808980375528336, Final Batch Loss: 0.08166443556547165\n",
      "Epoch 1855, Loss: 0.1252184994518757, Final Batch Loss: 0.06476762145757675\n",
      "Epoch 1856, Loss: 0.1262473538517952, Final Batch Loss: 0.056553490459918976\n",
      "Epoch 1857, Loss: 0.13040853291749954, Final Batch Loss: 0.07339593023061752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1858, Loss: 0.12262539565563202, Final Batch Loss: 0.04888838529586792\n",
      "Epoch 1859, Loss: 0.12724538519978523, Final Batch Loss: 0.053431589156389236\n",
      "Epoch 1860, Loss: 0.13972949981689453, Final Batch Loss: 0.07753673195838928\n",
      "Epoch 1861, Loss: 0.15285265445709229, Final Batch Loss: 0.060790494084358215\n",
      "Epoch 1862, Loss: 0.18222900480031967, Final Batch Loss: 0.10468750447034836\n",
      "Epoch 1863, Loss: 0.11562182009220123, Final Batch Loss: 0.05654243752360344\n",
      "Epoch 1864, Loss: 0.14486317336559296, Final Batch Loss: 0.0724792629480362\n",
      "Epoch 1865, Loss: 0.16460709273815155, Final Batch Loss: 0.0774829313158989\n",
      "Epoch 1866, Loss: 0.1501138061285019, Final Batch Loss: 0.08826585114002228\n",
      "Epoch 1867, Loss: 0.20527507364749908, Final Batch Loss: 0.08993774652481079\n",
      "Epoch 1868, Loss: 0.1458047479391098, Final Batch Loss: 0.09590590745210648\n",
      "Epoch 1869, Loss: 0.1381300389766693, Final Batch Loss: 0.07744355499744415\n",
      "Epoch 1870, Loss: 0.1543707177042961, Final Batch Loss: 0.06255798041820526\n",
      "Epoch 1871, Loss: 0.1276179812848568, Final Batch Loss: 0.049882274121046066\n",
      "Epoch 1872, Loss: 0.1625310331583023, Final Batch Loss: 0.0751250758767128\n",
      "Epoch 1873, Loss: 0.13037435337901115, Final Batch Loss: 0.04420122131705284\n",
      "Epoch 1874, Loss: 0.148894302546978, Final Batch Loss: 0.0670647919178009\n",
      "Epoch 1875, Loss: 0.18695610761642456, Final Batch Loss: 0.09214314818382263\n",
      "Epoch 1876, Loss: 0.168619804084301, Final Batch Loss: 0.08716657012701035\n",
      "Epoch 1877, Loss: 0.15007486939430237, Final Batch Loss: 0.08611764013767242\n",
      "Epoch 1878, Loss: 0.18985770642757416, Final Batch Loss: 0.09995000809431076\n",
      "Epoch 1879, Loss: 0.14516382664442062, Final Batch Loss: 0.06590904295444489\n",
      "Epoch 1880, Loss: 0.17166100442409515, Final Batch Loss: 0.07637985050678253\n",
      "Epoch 1881, Loss: 0.13560893386602402, Final Batch Loss: 0.06044439971446991\n",
      "Epoch 1882, Loss: 0.13611265644431114, Final Batch Loss: 0.05977975204586983\n",
      "Epoch 1883, Loss: 0.13732536137104034, Final Batch Loss: 0.06024233251810074\n",
      "Epoch 1884, Loss: 0.1535939946770668, Final Batch Loss: 0.09634705632925034\n",
      "Epoch 1885, Loss: 0.11078781634569168, Final Batch Loss: 0.048964254558086395\n",
      "Epoch 1886, Loss: 0.11582471430301666, Final Batch Loss: 0.046812109649181366\n",
      "Epoch 1887, Loss: 0.14199385046958923, Final Batch Loss: 0.06786735355854034\n",
      "Epoch 1888, Loss: 0.16600315272808075, Final Batch Loss: 0.08180088549852371\n",
      "Epoch 1889, Loss: 0.1606668159365654, Final Batch Loss: 0.060873523354530334\n",
      "Epoch 1890, Loss: 0.17290076613426208, Final Batch Loss: 0.10993336886167526\n",
      "Epoch 1891, Loss: 0.16541139036417007, Final Batch Loss: 0.07685857266187668\n",
      "Epoch 1892, Loss: 0.15410584956407547, Final Batch Loss: 0.08138203620910645\n",
      "Epoch 1893, Loss: 0.1426316499710083, Final Batch Loss: 0.07117020338773727\n",
      "Epoch 1894, Loss: 0.15272485464811325, Final Batch Loss: 0.054560378193855286\n",
      "Epoch 1895, Loss: 0.14622698724269867, Final Batch Loss: 0.07957843691110611\n",
      "Epoch 1896, Loss: 0.16394979506731033, Final Batch Loss: 0.06598244607448578\n",
      "Epoch 1897, Loss: 0.1929779052734375, Final Batch Loss: 0.12243594229221344\n",
      "Epoch 1898, Loss: 0.13410010933876038, Final Batch Loss: 0.07261031121015549\n",
      "Epoch 1899, Loss: 0.16031623631715775, Final Batch Loss: 0.07991831004619598\n",
      "Epoch 1900, Loss: 0.12249180302023888, Final Batch Loss: 0.058959346264600754\n",
      "Epoch 1901, Loss: 0.17930157482624054, Final Batch Loss: 0.11373850703239441\n",
      "Epoch 1902, Loss: 0.19462436437606812, Final Batch Loss: 0.12108220160007477\n",
      "Epoch 1903, Loss: 0.16128668934106827, Final Batch Loss: 0.09153357148170471\n",
      "Epoch 1904, Loss: 0.15020691603422165, Final Batch Loss: 0.10264754295349121\n",
      "Epoch 1905, Loss: 0.1605638638138771, Final Batch Loss: 0.08855873346328735\n",
      "Epoch 1906, Loss: 0.18192031979560852, Final Batch Loss: 0.07908253371715546\n",
      "Epoch 1907, Loss: 0.16274363547563553, Final Batch Loss: 0.0900188684463501\n",
      "Epoch 1908, Loss: 0.1633988469839096, Final Batch Loss: 0.09509368240833282\n",
      "Epoch 1909, Loss: 0.1506667509675026, Final Batch Loss: 0.08095923066139221\n",
      "Epoch 1910, Loss: 0.1445493847131729, Final Batch Loss: 0.05662417411804199\n",
      "Epoch 1911, Loss: 0.17011960595846176, Final Batch Loss: 0.08059912919998169\n",
      "Epoch 1912, Loss: 0.19614341109991074, Final Batch Loss: 0.08324693143367767\n",
      "Epoch 1913, Loss: 0.12003859132528305, Final Batch Loss: 0.06379688531160355\n",
      "Epoch 1914, Loss: 0.25890637189149857, Final Batch Loss: 0.07726774364709854\n",
      "Epoch 1915, Loss: 0.1770094409584999, Final Batch Loss: 0.07513998448848724\n",
      "Epoch 1916, Loss: 0.17948400229215622, Final Batch Loss: 0.091363325715065\n",
      "Epoch 1917, Loss: 0.1707879602909088, Final Batch Loss: 0.0715542659163475\n",
      "Epoch 1918, Loss: 0.14996477961540222, Final Batch Loss: 0.08330097049474716\n",
      "Epoch 1919, Loss: 0.1398761346936226, Final Batch Loss: 0.06154774874448776\n",
      "Epoch 1920, Loss: 0.1327047348022461, Final Batch Loss: 0.048994243144989014\n",
      "Epoch 1921, Loss: 0.1208086609840393, Final Batch Loss: 0.04847507178783417\n",
      "Epoch 1922, Loss: 0.14232834056019783, Final Batch Loss: 0.04598535969853401\n",
      "Epoch 1923, Loss: 0.1514653116464615, Final Batch Loss: 0.0576571524143219\n",
      "Epoch 1924, Loss: 0.14792432263493538, Final Batch Loss: 0.09583055973052979\n",
      "Epoch 1925, Loss: 0.1333044283092022, Final Batch Loss: 0.05918871983885765\n",
      "Epoch 1926, Loss: 0.12070260941982269, Final Batch Loss: 0.05752711743116379\n",
      "Epoch 1927, Loss: 0.1293468438088894, Final Batch Loss: 0.04214959219098091\n",
      "Epoch 1928, Loss: 0.1504868045449257, Final Batch Loss: 0.08700127899646759\n",
      "Epoch 1929, Loss: 0.13814198970794678, Final Batch Loss: 0.07237788289785385\n",
      "Epoch 1930, Loss: 0.14134477823972702, Final Batch Loss: 0.06622578948736191\n",
      "Epoch 1931, Loss: 0.13890503346920013, Final Batch Loss: 0.07264295220375061\n",
      "Epoch 1932, Loss: 0.24364741891622543, Final Batch Loss: 0.06583837419748306\n",
      "Epoch 1933, Loss: 0.142714511603117, Final Batch Loss: 0.062174055725336075\n",
      "Epoch 1934, Loss: 0.14269807934761047, Final Batch Loss: 0.0716603472828865\n",
      "Epoch 1935, Loss: 0.1170099675655365, Final Batch Loss: 0.06283190101385117\n",
      "Epoch 1936, Loss: 0.2620716616511345, Final Batch Loss: 0.17333441972732544\n",
      "Epoch 1937, Loss: 0.13559515401721, Final Batch Loss: 0.06064552441239357\n",
      "Epoch 1938, Loss: 0.13988742232322693, Final Batch Loss: 0.07248552143573761\n",
      "Epoch 1939, Loss: 0.13993562757968903, Final Batch Loss: 0.07627082616090775\n",
      "Epoch 1940, Loss: 0.14138149470090866, Final Batch Loss: 0.060159385204315186\n",
      "Epoch 1941, Loss: 0.13023211807012558, Final Batch Loss: 0.07262543588876724\n",
      "Epoch 1942, Loss: 0.1512288972735405, Final Batch Loss: 0.08501221984624863\n",
      "Epoch 1943, Loss: 0.17195194959640503, Final Batch Loss: 0.10100137442350388\n",
      "Epoch 1944, Loss: 0.16493164002895355, Final Batch Loss: 0.09280905872583389\n",
      "Epoch 1945, Loss: 0.13576315715909004, Final Batch Loss: 0.061867374926805496\n",
      "Epoch 1946, Loss: 0.24418143182992935, Final Batch Loss: 0.17739520967006683\n",
      "Epoch 1947, Loss: 0.12930432707071304, Final Batch Loss: 0.07031581550836563\n",
      "Epoch 1948, Loss: 0.1481378898024559, Final Batch Loss: 0.08979132026433945\n",
      "Epoch 1949, Loss: 0.14692077413201332, Final Batch Loss: 0.04600796476006508\n",
      "Epoch 1950, Loss: 0.14934208244085312, Final Batch Loss: 0.0836152508854866\n",
      "Epoch 1951, Loss: 0.1842118464410305, Final Batch Loss: 0.05918630585074425\n",
      "Epoch 1952, Loss: 0.1650458350777626, Final Batch Loss: 0.08602705597877502\n",
      "Epoch 1953, Loss: 0.17719309777021408, Final Batch Loss: 0.10451041907072067\n",
      "Epoch 1954, Loss: 0.1630162037909031, Final Batch Loss: 0.05645105615258217\n",
      "Epoch 1955, Loss: 0.14522825181484222, Final Batch Loss: 0.07893595099449158\n",
      "Epoch 1956, Loss: 0.13605019822716713, Final Batch Loss: 0.08761456608772278\n",
      "Epoch 1957, Loss: 0.14481983333826065, Final Batch Loss: 0.07788147777318954\n",
      "Epoch 1958, Loss: 0.18664450198411942, Final Batch Loss: 0.0768665075302124\n",
      "Epoch 1959, Loss: 0.15679503977298737, Final Batch Loss: 0.10366802662611008\n",
      "Epoch 1960, Loss: 0.13735804334282875, Final Batch Loss: 0.05854501202702522\n",
      "Epoch 1961, Loss: 0.14983488619327545, Final Batch Loss: 0.07971438765525818\n",
      "Epoch 1962, Loss: 0.1943148598074913, Final Batch Loss: 0.09345556050539017\n",
      "Epoch 1963, Loss: 0.1906995251774788, Final Batch Loss: 0.08982361853122711\n",
      "Epoch 1964, Loss: 0.13941511884331703, Final Batch Loss: 0.0859794020652771\n",
      "Epoch 1965, Loss: 0.12272506952285767, Final Batch Loss: 0.05752566456794739\n",
      "Epoch 1966, Loss: 0.12224449589848518, Final Batch Loss: 0.07797084003686905\n",
      "Epoch 1967, Loss: 0.12026198953390121, Final Batch Loss: 0.058263689279556274\n",
      "Epoch 1968, Loss: 0.17607345059514046, Final Batch Loss: 0.11435484886169434\n",
      "Epoch 1969, Loss: 0.12071283534169197, Final Batch Loss: 0.051448289304971695\n",
      "Epoch 1970, Loss: 0.1422909051179886, Final Batch Loss: 0.07233352959156036\n",
      "Epoch 1971, Loss: 0.17470689490437508, Final Batch Loss: 0.11832021921873093\n",
      "Epoch 1972, Loss: 0.16425695270299911, Final Batch Loss: 0.09099698811769485\n",
      "Epoch 1973, Loss: 0.10606923699378967, Final Batch Loss: 0.04502662271261215\n",
      "Epoch 1974, Loss: 0.11933156475424767, Final Batch Loss: 0.05568075552582741\n",
      "Epoch 1975, Loss: 0.16015781462192535, Final Batch Loss: 0.07451599836349487\n",
      "Epoch 1976, Loss: 0.170552559196949, Final Batch Loss: 0.09058284759521484\n",
      "Epoch 1977, Loss: 0.14252877980470657, Final Batch Loss: 0.06337503343820572\n",
      "Epoch 1978, Loss: 0.11752987653017044, Final Batch Loss: 0.051258817315101624\n",
      "Epoch 1979, Loss: 0.1298733912408352, Final Batch Loss: 0.0678827241063118\n",
      "Epoch 1980, Loss: 0.12084483727812767, Final Batch Loss: 0.06488793343305588\n",
      "Epoch 1981, Loss: 0.12256725504994392, Final Batch Loss: 0.04664056375622749\n",
      "Epoch 1982, Loss: 0.13647785410284996, Final Batch Loss: 0.08432957530021667\n",
      "Epoch 1983, Loss: 0.12606409937143326, Final Batch Loss: 0.05912211537361145\n",
      "Epoch 1984, Loss: 0.15735722333192825, Final Batch Loss: 0.07418689131736755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1985, Loss: 0.12842487543821335, Final Batch Loss: 0.05163811892271042\n",
      "Epoch 1986, Loss: 0.153516486287117, Final Batch Loss: 0.09299687296152115\n",
      "Epoch 1987, Loss: 0.10793478041887283, Final Batch Loss: 0.050318505614995956\n",
      "Epoch 1988, Loss: 0.12601931393146515, Final Batch Loss: 0.05118265748023987\n",
      "Epoch 1989, Loss: 0.13209744542837143, Final Batch Loss: 0.08160234242677689\n",
      "Epoch 1990, Loss: 0.14577698335051537, Final Batch Loss: 0.09317436814308167\n",
      "Epoch 1991, Loss: 0.13657207787036896, Final Batch Loss: 0.07283498346805573\n",
      "Epoch 1992, Loss: 0.11997129768133163, Final Batch Loss: 0.0628020390868187\n",
      "Epoch 1993, Loss: 0.15927478671073914, Final Batch Loss: 0.1065811738371849\n",
      "Epoch 1994, Loss: 0.14583642408251762, Final Batch Loss: 0.10479853302240372\n",
      "Epoch 1995, Loss: 0.12476666271686554, Final Batch Loss: 0.06668160855770111\n",
      "Epoch 1996, Loss: 0.1256784275174141, Final Batch Loss: 0.05966097116470337\n",
      "Epoch 1997, Loss: 0.18128962814807892, Final Batch Loss: 0.08749202638864517\n",
      "Epoch 1998, Loss: 0.12493861839175224, Final Batch Loss: 0.0487133152782917\n",
      "Epoch 1999, Loss: 0.13464105501770973, Final Batch Loss: 0.050749730318784714\n",
      "Epoch 2000, Loss: 0.11463738232851028, Final Batch Loss: 0.07235271483659744\n",
      "Epoch 2001, Loss: 0.11719577386975288, Final Batch Loss: 0.030267413705587387\n",
      "Epoch 2002, Loss: 0.12230661138892174, Final Batch Loss: 0.05155286565423012\n",
      "Epoch 2003, Loss: 0.15087281540036201, Final Batch Loss: 0.10366769134998322\n",
      "Epoch 2004, Loss: 0.12200790271162987, Final Batch Loss: 0.07059691101312637\n",
      "Epoch 2005, Loss: 0.09902026876807213, Final Batch Loss: 0.0428636409342289\n",
      "Epoch 2006, Loss: 0.11168835312128067, Final Batch Loss: 0.05319265276193619\n",
      "Epoch 2007, Loss: 0.12261345982551575, Final Batch Loss: 0.07507511228322983\n",
      "Epoch 2008, Loss: 0.10887284949421883, Final Batch Loss: 0.058392785489559174\n",
      "Epoch 2009, Loss: 0.14159901440143585, Final Batch Loss: 0.0667320266366005\n",
      "Epoch 2010, Loss: 0.10326696187257767, Final Batch Loss: 0.05877070873975754\n",
      "Epoch 2011, Loss: 0.13632560148835182, Final Batch Loss: 0.044886406511068344\n",
      "Epoch 2012, Loss: 0.09439413249492645, Final Batch Loss: 0.054778359830379486\n",
      "Epoch 2013, Loss: 0.11432361975312233, Final Batch Loss: 0.042142633348703384\n",
      "Epoch 2014, Loss: 0.09196796640753746, Final Batch Loss: 0.047296132892370224\n",
      "Epoch 2015, Loss: 0.13667017966508865, Final Batch Loss: 0.05060733109712601\n",
      "Epoch 2016, Loss: 0.11200098320841789, Final Batch Loss: 0.07146933674812317\n",
      "Epoch 2017, Loss: 0.1183638870716095, Final Batch Loss: 0.05089889466762543\n",
      "Epoch 2018, Loss: 0.1580844148993492, Final Batch Loss: 0.09406793862581253\n",
      "Epoch 2019, Loss: 0.15674392133951187, Final Batch Loss: 0.06731285899877548\n",
      "Epoch 2020, Loss: 0.14555717259645462, Final Batch Loss: 0.07299774140119553\n",
      "Epoch 2021, Loss: 0.10794835165143013, Final Batch Loss: 0.042690616101026535\n",
      "Epoch 2022, Loss: 0.12418988347053528, Final Batch Loss: 0.06368453800678253\n",
      "Epoch 2023, Loss: 0.14570031315088272, Final Batch Loss: 0.07929324358701706\n",
      "Epoch 2024, Loss: 0.12368419021368027, Final Batch Loss: 0.06385432183742523\n",
      "Epoch 2025, Loss: 0.1590973362326622, Final Batch Loss: 0.10265140980482101\n",
      "Epoch 2026, Loss: 0.15224798023700714, Final Batch Loss: 0.07168411463499069\n",
      "Epoch 2027, Loss: 0.2332836613059044, Final Batch Loss: 0.11981994658708572\n",
      "Epoch 2028, Loss: 0.14283152669668198, Final Batch Loss: 0.07925619184970856\n",
      "Epoch 2029, Loss: 0.19216948002576828, Final Batch Loss: 0.06916270405054092\n",
      "Epoch 2030, Loss: 0.15591167658567429, Final Batch Loss: 0.06704811751842499\n",
      "Epoch 2031, Loss: 0.14040635153651237, Final Batch Loss: 0.08617155998945236\n",
      "Epoch 2032, Loss: 0.15990283712744713, Final Batch Loss: 0.10245583951473236\n",
      "Epoch 2033, Loss: 0.14448785036802292, Final Batch Loss: 0.05429808050394058\n",
      "Epoch 2034, Loss: 0.1282513216137886, Final Batch Loss: 0.041778482496738434\n",
      "Epoch 2035, Loss: 0.12737230584025383, Final Batch Loss: 0.04476531967520714\n",
      "Epoch 2036, Loss: 0.1652199774980545, Final Batch Loss: 0.06778804212808609\n",
      "Epoch 2037, Loss: 0.13464833423495293, Final Batch Loss: 0.08306077867746353\n",
      "Epoch 2038, Loss: 0.15193097293376923, Final Batch Loss: 0.08761801570653915\n",
      "Epoch 2039, Loss: 0.20364917814731598, Final Batch Loss: 0.13595128059387207\n",
      "Epoch 2040, Loss: 0.10690831393003464, Final Batch Loss: 0.04674020782113075\n",
      "Epoch 2041, Loss: 0.15215051174163818, Final Batch Loss: 0.10093551874160767\n",
      "Epoch 2042, Loss: 0.15495647862553596, Final Batch Loss: 0.06173475459218025\n",
      "Epoch 2043, Loss: 0.15802782028913498, Final Batch Loss: 0.08617039769887924\n",
      "Epoch 2044, Loss: 0.12191333994269371, Final Batch Loss: 0.07911715656518936\n",
      "Epoch 2045, Loss: 0.1075175441801548, Final Batch Loss: 0.06249023973941803\n",
      "Epoch 2046, Loss: 0.2026311233639717, Final Batch Loss: 0.14242859184741974\n",
      "Epoch 2047, Loss: 0.12273123860359192, Final Batch Loss: 0.07088299095630646\n",
      "Epoch 2048, Loss: 0.14533763378858566, Final Batch Loss: 0.08843491971492767\n",
      "Epoch 2049, Loss: 0.1570478156208992, Final Batch Loss: 0.0825410932302475\n",
      "Epoch 2050, Loss: 0.11691742390394211, Final Batch Loss: 0.05572328343987465\n",
      "Epoch 2051, Loss: 0.15194196999073029, Final Batch Loss: 0.0988008975982666\n",
      "Epoch 2052, Loss: 0.14752766489982605, Final Batch Loss: 0.09885219484567642\n",
      "Epoch 2053, Loss: 0.1144188828766346, Final Batch Loss: 0.07484538853168488\n",
      "Epoch 2054, Loss: 0.10844387486577034, Final Batch Loss: 0.038797374814748764\n",
      "Epoch 2055, Loss: 0.14678718149662018, Final Batch Loss: 0.09678459167480469\n",
      "Epoch 2056, Loss: 0.14755382016301155, Final Batch Loss: 0.05024575814604759\n",
      "Epoch 2057, Loss: 0.1636040210723877, Final Batch Loss: 0.0975586548447609\n",
      "Epoch 2058, Loss: 0.10421697050333023, Final Batch Loss: 0.030734769999980927\n",
      "Epoch 2059, Loss: 0.09829081594944, Final Batch Loss: 0.056256916373968124\n",
      "Epoch 2060, Loss: 0.08616208657622337, Final Batch Loss: 0.055157843977212906\n",
      "Epoch 2061, Loss: 0.10601562261581421, Final Batch Loss: 0.04496349021792412\n",
      "Epoch 2062, Loss: 0.12200815603137016, Final Batch Loss: 0.0561998076736927\n",
      "Epoch 2063, Loss: 0.10559245198965073, Final Batch Loss: 0.0572253093123436\n",
      "Epoch 2064, Loss: 0.10360690578818321, Final Batch Loss: 0.032185304909944534\n",
      "Epoch 2065, Loss: 0.11231391876935959, Final Batch Loss: 0.06852947920560837\n",
      "Epoch 2066, Loss: 0.12140274047851562, Final Batch Loss: 0.062431443482637405\n",
      "Epoch 2067, Loss: 0.2039138376712799, Final Batch Loss: 0.0813835859298706\n",
      "Epoch 2068, Loss: 0.15935907512903214, Final Batch Loss: 0.11037025600671768\n",
      "Epoch 2069, Loss: 0.11599499732255936, Final Batch Loss: 0.05482020974159241\n",
      "Epoch 2070, Loss: 0.12801195308566093, Final Batch Loss: 0.05474251136183739\n",
      "Epoch 2071, Loss: 0.11688201501965523, Final Batch Loss: 0.07442574203014374\n",
      "Epoch 2072, Loss: 0.12184087187051773, Final Batch Loss: 0.04040598124265671\n",
      "Epoch 2073, Loss: 0.09005393087863922, Final Batch Loss: 0.0332452617585659\n",
      "Epoch 2074, Loss: 0.10968566685914993, Final Batch Loss: 0.051387131214141846\n",
      "Epoch 2075, Loss: 0.11265908926725388, Final Batch Loss: 0.042360492050647736\n",
      "Epoch 2076, Loss: 0.11120451614260674, Final Batch Loss: 0.04844460263848305\n",
      "Epoch 2077, Loss: 0.08601614087820053, Final Batch Loss: 0.05937398597598076\n",
      "Epoch 2078, Loss: 0.14548078551888466, Final Batch Loss: 0.10296514630317688\n",
      "Epoch 2079, Loss: 0.09580905549228191, Final Batch Loss: 0.026478497311472893\n",
      "Epoch 2080, Loss: 0.18722087517380714, Final Batch Loss: 0.1485096514225006\n",
      "Epoch 2081, Loss: 0.1514560952782631, Final Batch Loss: 0.09715799242258072\n",
      "Epoch 2082, Loss: 0.10874524712562561, Final Batch Loss: 0.04481203854084015\n",
      "Epoch 2083, Loss: 0.1811404973268509, Final Batch Loss: 0.11687925457954407\n",
      "Epoch 2084, Loss: 0.12690649181604385, Final Batch Loss: 0.07479628175497055\n",
      "Epoch 2085, Loss: 0.12177935987710953, Final Batch Loss: 0.06484411656856537\n",
      "Epoch 2086, Loss: 0.14889564365148544, Final Batch Loss: 0.07939549535512924\n",
      "Epoch 2087, Loss: 0.09524852782487869, Final Batch Loss: 0.04172203689813614\n",
      "Epoch 2088, Loss: 0.12456705793738365, Final Batch Loss: 0.05959208682179451\n",
      "Epoch 2089, Loss: 0.1377548761665821, Final Batch Loss: 0.07896421104669571\n",
      "Epoch 2090, Loss: 0.11212114244699478, Final Batch Loss: 0.06162324920296669\n",
      "Epoch 2091, Loss: 0.11525086313486099, Final Batch Loss: 0.05234065651893616\n",
      "Epoch 2092, Loss: 0.15499239414930344, Final Batch Loss: 0.08595506101846695\n",
      "Epoch 2093, Loss: 0.13096081838011742, Final Batch Loss: 0.09002663940191269\n",
      "Epoch 2094, Loss: 0.11275514587759972, Final Batch Loss: 0.04610462114214897\n",
      "Epoch 2095, Loss: 0.11237220838665962, Final Batch Loss: 0.05281584709882736\n",
      "Epoch 2096, Loss: 0.07397058978676796, Final Batch Loss: 0.035387199372053146\n",
      "Epoch 2097, Loss: 0.10719551518559456, Final Batch Loss: 0.05395532771945\n",
      "Epoch 2098, Loss: 0.09443685412406921, Final Batch Loss: 0.03675997629761696\n",
      "Epoch 2099, Loss: 0.18983587622642517, Final Batch Loss: 0.1337895542383194\n",
      "Epoch 2100, Loss: 0.09195883572101593, Final Batch Loss: 0.04139479994773865\n",
      "Epoch 2101, Loss: 0.0792197622358799, Final Batch Loss: 0.03217509388923645\n",
      "Epoch 2102, Loss: 0.11346470564603806, Final Batch Loss: 0.06616311520338058\n",
      "Epoch 2103, Loss: 0.07746180146932602, Final Batch Loss: 0.04757007956504822\n",
      "Epoch 2104, Loss: 0.09785613417625427, Final Batch Loss: 0.04646417126059532\n",
      "Epoch 2105, Loss: 0.09557679295539856, Final Batch Loss: 0.05976063758134842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2106, Loss: 0.10022085160017014, Final Batch Loss: 0.052789751440286636\n",
      "Epoch 2107, Loss: 0.15210628509521484, Final Batch Loss: 0.08429179340600967\n",
      "Epoch 2108, Loss: 0.1350112445652485, Final Batch Loss: 0.08411278575658798\n",
      "Epoch 2109, Loss: 0.09718555212020874, Final Batch Loss: 0.02709580957889557\n",
      "Epoch 2110, Loss: 0.059758443385362625, Final Batch Loss: 0.02089688554406166\n",
      "Epoch 2111, Loss: 0.10290404222905636, Final Batch Loss: 0.07289005815982819\n",
      "Epoch 2112, Loss: 0.11291194707155228, Final Batch Loss: 0.041202664375305176\n",
      "Epoch 2113, Loss: 0.15644056349992752, Final Batch Loss: 0.06932353228330612\n",
      "Epoch 2114, Loss: 0.11639760434627533, Final Batch Loss: 0.05073679983615875\n",
      "Epoch 2115, Loss: 0.14103751629590988, Final Batch Loss: 0.07795620709657669\n",
      "Epoch 2116, Loss: 0.18697009608149529, Final Batch Loss: 0.04454737529158592\n",
      "Epoch 2117, Loss: 0.1052037701010704, Final Batch Loss: 0.05493762344121933\n",
      "Epoch 2118, Loss: 0.10800647735595703, Final Batch Loss: 0.034163154661655426\n",
      "Epoch 2119, Loss: 0.08695770427584648, Final Batch Loss: 0.031500812619924545\n",
      "Epoch 2120, Loss: 0.12105902656912804, Final Batch Loss: 0.06210261583328247\n",
      "Epoch 2121, Loss: 0.088239386677742, Final Batch Loss: 0.05796176567673683\n",
      "Epoch 2122, Loss: 0.11164362728595734, Final Batch Loss: 0.05227336660027504\n",
      "Epoch 2123, Loss: 0.10102640092372894, Final Batch Loss: 0.05227744206786156\n",
      "Epoch 2124, Loss: 0.12011320516467094, Final Batch Loss: 0.04747433587908745\n",
      "Epoch 2125, Loss: 0.08286430314183235, Final Batch Loss: 0.042181774973869324\n",
      "Epoch 2126, Loss: 0.16665836051106453, Final Batch Loss: 0.12483663111925125\n",
      "Epoch 2127, Loss: 0.13863565027713776, Final Batch Loss: 0.10292182117700577\n",
      "Epoch 2128, Loss: 0.08531974628567696, Final Batch Loss: 0.028642121702432632\n",
      "Epoch 2129, Loss: 0.105882927775383, Final Batch Loss: 0.05419424921274185\n",
      "Epoch 2130, Loss: 0.08157295547425747, Final Batch Loss: 0.025189103558659554\n",
      "Epoch 2131, Loss: 0.09082938730716705, Final Batch Loss: 0.043271880596876144\n",
      "Epoch 2132, Loss: 0.11301415041089058, Final Batch Loss: 0.05972432717680931\n",
      "Epoch 2133, Loss: 0.10083932988345623, Final Batch Loss: 0.030995158478617668\n",
      "Epoch 2134, Loss: 0.10577678680419922, Final Batch Loss: 0.04828018322587013\n",
      "Epoch 2135, Loss: 0.09944460541009903, Final Batch Loss: 0.05113662779331207\n",
      "Epoch 2136, Loss: 0.146859522908926, Final Batch Loss: 0.08934222906827927\n",
      "Epoch 2137, Loss: 0.08367666974663734, Final Batch Loss: 0.0278400219976902\n",
      "Epoch 2138, Loss: 0.09384139627218246, Final Batch Loss: 0.06407095491886139\n",
      "Epoch 2139, Loss: 0.08345651999115944, Final Batch Loss: 0.057870522141456604\n",
      "Epoch 2140, Loss: 0.11827917397022247, Final Batch Loss: 0.03566289693117142\n",
      "Epoch 2141, Loss: 0.13814567029476166, Final Batch Loss: 0.09850206226110458\n",
      "Epoch 2142, Loss: 0.08865635469555855, Final Batch Loss: 0.056717079132795334\n",
      "Epoch 2143, Loss: 0.11913919821381569, Final Batch Loss: 0.06738158315420151\n",
      "Epoch 2144, Loss: 0.0949716717004776, Final Batch Loss: 0.04980941861867905\n",
      "Epoch 2145, Loss: 0.08321953378617764, Final Batch Loss: 0.052283480763435364\n",
      "Epoch 2146, Loss: 0.07312112674117088, Final Batch Loss: 0.02654315158724785\n",
      "Epoch 2147, Loss: 0.10167741030454636, Final Batch Loss: 0.043875303119421005\n",
      "Epoch 2148, Loss: 0.08060075528919697, Final Batch Loss: 0.05308885872364044\n",
      "Epoch 2149, Loss: 0.08293627575039864, Final Batch Loss: 0.04182228446006775\n",
      "Epoch 2150, Loss: 0.06133940815925598, Final Batch Loss: 0.03494029492139816\n",
      "Epoch 2151, Loss: 0.09627701714634895, Final Batch Loss: 0.05460024252533913\n",
      "Epoch 2152, Loss: 0.08271167799830437, Final Batch Loss: 0.03408089280128479\n",
      "Epoch 2153, Loss: 0.08539528213441372, Final Batch Loss: 0.02662677876651287\n",
      "Epoch 2154, Loss: 0.06982579082250595, Final Batch Loss: 0.02539525181055069\n",
      "Epoch 2155, Loss: 0.07020631060004234, Final Batch Loss: 0.03815697878599167\n",
      "Epoch 2156, Loss: 0.08546928688883781, Final Batch Loss: 0.029612023383378983\n",
      "Epoch 2157, Loss: 0.1070914939045906, Final Batch Loss: 0.07616087049245834\n",
      "Epoch 2158, Loss: 0.11071613058447838, Final Batch Loss: 0.03899697586894035\n",
      "Epoch 2159, Loss: 0.06596498936414719, Final Batch Loss: 0.04090970754623413\n",
      "Epoch 2160, Loss: 0.08238032460212708, Final Batch Loss: 0.03656521439552307\n",
      "Epoch 2161, Loss: 0.11288782954216003, Final Batch Loss: 0.036460310220718384\n",
      "Epoch 2162, Loss: 0.12570533528923988, Final Batch Loss: 0.048197899013757706\n",
      "Epoch 2163, Loss: 0.08166882023215294, Final Batch Loss: 0.033105045557022095\n",
      "Epoch 2164, Loss: 0.08443491905927658, Final Batch Loss: 0.04096996784210205\n",
      "Epoch 2165, Loss: 0.19565564021468163, Final Batch Loss: 0.15863709151744843\n",
      "Epoch 2166, Loss: 0.10437154397368431, Final Batch Loss: 0.069254070520401\n",
      "Epoch 2167, Loss: 0.07416138797998428, Final Batch Loss: 0.03747779130935669\n",
      "Epoch 2168, Loss: 0.12725834548473358, Final Batch Loss: 0.06474073231220245\n",
      "Epoch 2169, Loss: 0.19106003642082214, Final Batch Loss: 0.09638452529907227\n",
      "Epoch 2170, Loss: 0.08327281102538109, Final Batch Loss: 0.03246459737420082\n",
      "Epoch 2171, Loss: 0.07990952953696251, Final Batch Loss: 0.040688060224056244\n",
      "Epoch 2172, Loss: 0.09354989789426327, Final Batch Loss: 0.06711195409297943\n",
      "Epoch 2173, Loss: 0.0936041958630085, Final Batch Loss: 0.04734829440712929\n",
      "Epoch 2174, Loss: 0.0966503880918026, Final Batch Loss: 0.05289771407842636\n",
      "Epoch 2175, Loss: 0.12189903482794762, Final Batch Loss: 0.04932255670428276\n",
      "Epoch 2176, Loss: 0.07642712630331516, Final Batch Loss: 0.0516250804066658\n",
      "Epoch 2177, Loss: 0.1627878062427044, Final Batch Loss: 0.11880211532115936\n",
      "Epoch 2178, Loss: 0.09378334134817123, Final Batch Loss: 0.054374031722545624\n",
      "Epoch 2179, Loss: 0.11307702958583832, Final Batch Loss: 0.0459805503487587\n",
      "Epoch 2180, Loss: 0.09311195462942123, Final Batch Loss: 0.051653921604156494\n",
      "Epoch 2181, Loss: 0.1382238268852234, Final Batch Loss: 0.06853233277797699\n",
      "Epoch 2182, Loss: 0.11700903624296188, Final Batch Loss: 0.06815461814403534\n",
      "Epoch 2183, Loss: 0.10771328955888748, Final Batch Loss: 0.05404318496584892\n",
      "Epoch 2184, Loss: 0.10495764762163162, Final Batch Loss: 0.05084075778722763\n",
      "Epoch 2185, Loss: 0.07527382113039494, Final Batch Loss: 0.01907290704548359\n",
      "Epoch 2186, Loss: 0.08631614595651627, Final Batch Loss: 0.05100167170166969\n",
      "Epoch 2187, Loss: 0.1453380137681961, Final Batch Loss: 0.09364155679941177\n",
      "Epoch 2188, Loss: 0.14758922904729843, Final Batch Loss: 0.09444580227136612\n",
      "Epoch 2189, Loss: 0.08178862556815147, Final Batch Loss: 0.02643563598394394\n",
      "Epoch 2190, Loss: 0.10560251027345657, Final Batch Loss: 0.0584426149725914\n",
      "Epoch 2191, Loss: 0.07966475561261177, Final Batch Loss: 0.05170581489801407\n",
      "Epoch 2192, Loss: 0.08380825072526932, Final Batch Loss: 0.04820701852440834\n",
      "Epoch 2193, Loss: 0.16046632081270218, Final Batch Loss: 0.07873418182134628\n",
      "Epoch 2194, Loss: 0.0628091562539339, Final Batch Loss: 0.020069001242518425\n",
      "Epoch 2195, Loss: 0.13179069012403488, Final Batch Loss: 0.07038932293653488\n",
      "Epoch 2196, Loss: 0.08387761563062668, Final Batch Loss: 0.03902115300297737\n",
      "Epoch 2197, Loss: 0.12249531596899033, Final Batch Loss: 0.07818014174699783\n",
      "Epoch 2198, Loss: 0.07309644669294357, Final Batch Loss: 0.02810708060860634\n",
      "Epoch 2199, Loss: 0.0984523706138134, Final Batch Loss: 0.06241424009203911\n",
      "Epoch 2200, Loss: 0.04498822521418333, Final Batch Loss: 0.010904575698077679\n",
      "Epoch 2201, Loss: 0.10793285444378853, Final Batch Loss: 0.04934409633278847\n",
      "Epoch 2202, Loss: 0.07985522970557213, Final Batch Loss: 0.027798078954219818\n",
      "Epoch 2203, Loss: 0.05417206883430481, Final Batch Loss: 0.03406994789838791\n",
      "Epoch 2204, Loss: 0.09216036647558212, Final Batch Loss: 0.057493992149829865\n",
      "Epoch 2205, Loss: 0.05667266808450222, Final Batch Loss: 0.021735379472374916\n",
      "Epoch 2206, Loss: 0.05804198421537876, Final Batch Loss: 0.01674075238406658\n",
      "Epoch 2207, Loss: 0.1036476157605648, Final Batch Loss: 0.04349585250020027\n",
      "Epoch 2208, Loss: 0.0995427817106247, Final Batch Loss: 0.04081888124346733\n",
      "Epoch 2209, Loss: 0.13461878523230553, Final Batch Loss: 0.09085623174905777\n",
      "Epoch 2210, Loss: 0.06653741002082825, Final Batch Loss: 0.031782057136297226\n",
      "Epoch 2211, Loss: 0.07600019127130508, Final Batch Loss: 0.031532660126686096\n",
      "Epoch 2212, Loss: 0.0943567119538784, Final Batch Loss: 0.039846375584602356\n",
      "Epoch 2213, Loss: 0.10765420645475388, Final Batch Loss: 0.06762413680553436\n",
      "Epoch 2214, Loss: 0.11344834603369236, Final Batch Loss: 0.08497568964958191\n",
      "Epoch 2215, Loss: 0.07945787161588669, Final Batch Loss: 0.04209717735648155\n",
      "Epoch 2216, Loss: 0.08560935407876968, Final Batch Loss: 0.05438467487692833\n",
      "Epoch 2217, Loss: 0.09706518426537514, Final Batch Loss: 0.034316230565309525\n",
      "Epoch 2218, Loss: 0.09501025825738907, Final Batch Loss: 0.056122224777936935\n",
      "Epoch 2219, Loss: 0.08490335568785667, Final Batch Loss: 0.03356151282787323\n",
      "Epoch 2220, Loss: 0.10530203953385353, Final Batch Loss: 0.06470406800508499\n",
      "Epoch 2221, Loss: 0.10355088487267494, Final Batch Loss: 0.04260769486427307\n",
      "Epoch 2222, Loss: 0.04871467128396034, Final Batch Loss: 0.016931895166635513\n",
      "Epoch 2223, Loss: 0.1018410474061966, Final Batch Loss: 0.06809429824352264\n",
      "Epoch 2224, Loss: 0.10189016163349152, Final Batch Loss: 0.047117412090301514\n",
      "Epoch 2225, Loss: 0.1153334192931652, Final Batch Loss: 0.06357515603303909\n",
      "Epoch 2226, Loss: 0.11165446043014526, Final Batch Loss: 0.05948472023010254\n",
      "Epoch 2227, Loss: 0.08585904538631439, Final Batch Loss: 0.04789018630981445\n",
      "Epoch 2228, Loss: 0.06197694502770901, Final Batch Loss: 0.03627198562026024\n",
      "Epoch 2229, Loss: 0.0963570848107338, Final Batch Loss: 0.04282452166080475\n",
      "Epoch 2230, Loss: 0.07320832461118698, Final Batch Loss: 0.03219394013285637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2231, Loss: 0.06768925860524178, Final Batch Loss: 0.024828769266605377\n",
      "Epoch 2232, Loss: 0.08781333267688751, Final Batch Loss: 0.04247031733393669\n",
      "Epoch 2233, Loss: 0.10201919078826904, Final Batch Loss: 0.06819939613342285\n",
      "Epoch 2234, Loss: 0.11305641010403633, Final Batch Loss: 0.06983353942632675\n",
      "Epoch 2235, Loss: 0.12473353743553162, Final Batch Loss: 0.07417653501033783\n",
      "Epoch 2236, Loss: 0.06063420511782169, Final Batch Loss: 0.0348784476518631\n",
      "Epoch 2237, Loss: 0.13256992027163506, Final Batch Loss: 0.06071595475077629\n",
      "Epoch 2238, Loss: 0.11645490676164627, Final Batch Loss: 0.03499549627304077\n",
      "Epoch 2239, Loss: 0.0651695653796196, Final Batch Loss: 0.033779412508010864\n",
      "Epoch 2240, Loss: 0.09183582291007042, Final Batch Loss: 0.040761418640613556\n",
      "Epoch 2241, Loss: 0.08912403509020805, Final Batch Loss: 0.03380265831947327\n",
      "Epoch 2242, Loss: 0.10185952112078667, Final Batch Loss: 0.05634311959147453\n",
      "Epoch 2243, Loss: 0.11335311457514763, Final Batch Loss: 0.06975478678941727\n",
      "Epoch 2244, Loss: 0.09492754936218262, Final Batch Loss: 0.04019547253847122\n",
      "Epoch 2245, Loss: 0.0884697400033474, Final Batch Loss: 0.041076403111219406\n",
      "Epoch 2246, Loss: 0.09107299149036407, Final Batch Loss: 0.058931443840265274\n",
      "Epoch 2247, Loss: 0.039327580481767654, Final Batch Loss: 0.01982339285314083\n",
      "Epoch 2248, Loss: 0.055904997512698174, Final Batch Loss: 0.01614670641720295\n",
      "Epoch 2249, Loss: 0.11356868967413902, Final Batch Loss: 0.09021107852458954\n",
      "Epoch 2250, Loss: 0.0481627881526947, Final Batch Loss: 0.015793632715940475\n",
      "Epoch 2251, Loss: 0.12485111504793167, Final Batch Loss: 0.07172110676765442\n",
      "Epoch 2252, Loss: 0.07648495957255363, Final Batch Loss: 0.04785989224910736\n",
      "Epoch 2253, Loss: 0.0716780461370945, Final Batch Loss: 0.04069904237985611\n",
      "Epoch 2254, Loss: 0.1311910878866911, Final Batch Loss: 0.030069401487708092\n",
      "Epoch 2255, Loss: 0.11600121855735779, Final Batch Loss: 0.05514993891119957\n",
      "Epoch 2256, Loss: 0.09006107971072197, Final Batch Loss: 0.046193432062864304\n",
      "Epoch 2257, Loss: 0.09170663170516491, Final Batch Loss: 0.0210134144872427\n",
      "Epoch 2258, Loss: 0.12061619758605957, Final Batch Loss: 0.046999573707580566\n",
      "Epoch 2259, Loss: 0.06300519220530987, Final Batch Loss: 0.020951291546225548\n",
      "Epoch 2260, Loss: 0.061235636472702026, Final Batch Loss: 0.02529505267739296\n",
      "Epoch 2261, Loss: 0.12905589491128922, Final Batch Loss: 0.0681462362408638\n",
      "Epoch 2262, Loss: 0.0891822800040245, Final Batch Loss: 0.03221433237195015\n",
      "Epoch 2263, Loss: 0.10795768350362778, Final Batch Loss: 0.036297865211963654\n",
      "Epoch 2264, Loss: 0.11823192238807678, Final Batch Loss: 0.07381121069192886\n",
      "Epoch 2265, Loss: 0.07717417180538177, Final Batch Loss: 0.03258731961250305\n",
      "Epoch 2266, Loss: 0.05130540393292904, Final Batch Loss: 0.018650738522410393\n",
      "Epoch 2267, Loss: 0.12368448451161385, Final Batch Loss: 0.052568111568689346\n",
      "Epoch 2268, Loss: 0.08462855406105518, Final Batch Loss: 0.0593571811914444\n",
      "Epoch 2269, Loss: 0.05930973030626774, Final Batch Loss: 0.03045882098376751\n",
      "Epoch 2270, Loss: 0.09973788261413574, Final Batch Loss: 0.03474804013967514\n",
      "Epoch 2271, Loss: 0.06448561325669289, Final Batch Loss: 0.016124222427606583\n",
      "Epoch 2272, Loss: 0.06798042915761471, Final Batch Loss: 0.028398921713232994\n",
      "Epoch 2273, Loss: 0.0888785570859909, Final Batch Loss: 0.04640636220574379\n",
      "Epoch 2274, Loss: 0.10254747048020363, Final Batch Loss: 0.04309975728392601\n",
      "Epoch 2275, Loss: 0.06685268878936768, Final Batch Loss: 0.03685631603002548\n",
      "Epoch 2276, Loss: 0.13015291839838028, Final Batch Loss: 0.07349654287099838\n",
      "Epoch 2277, Loss: 0.05527499504387379, Final Batch Loss: 0.033964838832616806\n",
      "Epoch 2278, Loss: 0.06040719710290432, Final Batch Loss: 0.03127007931470871\n",
      "Epoch 2279, Loss: 0.07758107036352158, Final Batch Loss: 0.038823630660772324\n",
      "Epoch 2280, Loss: 0.09941187035292387, Final Batch Loss: 0.013861010782420635\n",
      "Epoch 2281, Loss: 0.09585060924291611, Final Batch Loss: 0.07183096557855606\n",
      "Epoch 2282, Loss: 0.07469355314970016, Final Batch Loss: 0.020531099289655685\n",
      "Epoch 2283, Loss: 0.0934909787029028, Final Batch Loss: 0.06676965206861496\n",
      "Epoch 2284, Loss: 0.08203991502523422, Final Batch Loss: 0.019972965121269226\n",
      "Epoch 2285, Loss: 0.09430597722530365, Final Batch Loss: 0.058844443410634995\n",
      "Epoch 2286, Loss: 0.08166898787021637, Final Batch Loss: 0.039362650364637375\n",
      "Epoch 2287, Loss: 0.10161414369940758, Final Batch Loss: 0.043812692165374756\n",
      "Epoch 2288, Loss: 0.08635376021265984, Final Batch Loss: 0.017100941389799118\n",
      "Epoch 2289, Loss: 0.09572915360331535, Final Batch Loss: 0.0399455688893795\n",
      "Epoch 2290, Loss: 0.08615065552294254, Final Batch Loss: 0.058687061071395874\n",
      "Epoch 2291, Loss: 0.07339044474065304, Final Batch Loss: 0.03111737035214901\n",
      "Epoch 2292, Loss: 0.07094007730484009, Final Batch Loss: 0.03163230046629906\n",
      "Epoch 2293, Loss: 0.12741611897945404, Final Batch Loss: 0.08942017704248428\n",
      "Epoch 2294, Loss: 0.12404395267367363, Final Batch Loss: 0.05750139430165291\n",
      "Epoch 2295, Loss: 0.07259174063801765, Final Batch Loss: 0.03436219319701195\n",
      "Epoch 2296, Loss: 0.06283644400537014, Final Batch Loss: 0.03245259076356888\n",
      "Epoch 2297, Loss: 0.08127415180206299, Final Batch Loss: 0.042594730854034424\n",
      "Epoch 2298, Loss: 0.06812532618641853, Final Batch Loss: 0.04842428117990494\n",
      "Epoch 2299, Loss: 0.07205291278660297, Final Batch Loss: 0.025406179949641228\n",
      "Epoch 2300, Loss: 0.07394443638622761, Final Batch Loss: 0.04659287631511688\n",
      "Epoch 2301, Loss: 0.0712822750210762, Final Batch Loss: 0.031164340674877167\n",
      "Epoch 2302, Loss: 0.09346086904406548, Final Batch Loss: 0.04377688094973564\n",
      "Epoch 2303, Loss: 0.061205651611089706, Final Batch Loss: 0.032374802976846695\n",
      "Epoch 2304, Loss: 0.10586152225732803, Final Batch Loss: 0.07969482243061066\n",
      "Epoch 2305, Loss: 0.08186515606939793, Final Batch Loss: 0.05984187126159668\n",
      "Epoch 2306, Loss: 0.07643421739339828, Final Batch Loss: 0.032777246087789536\n",
      "Epoch 2307, Loss: 0.05173058807849884, Final Batch Loss: 0.03327713906764984\n",
      "Epoch 2308, Loss: 0.10383511893451214, Final Batch Loss: 0.023026740178465843\n",
      "Epoch 2309, Loss: 0.13894766196608543, Final Batch Loss: 0.10526344180107117\n",
      "Epoch 2310, Loss: 0.11530419066548347, Final Batch Loss: 0.055177800357341766\n",
      "Epoch 2311, Loss: 0.054795773699879646, Final Batch Loss: 0.024935394525527954\n",
      "Epoch 2312, Loss: 0.07349148392677307, Final Batch Loss: 0.03994083032011986\n",
      "Epoch 2313, Loss: 0.08214645832777023, Final Batch Loss: 0.041240394115448\n",
      "Epoch 2314, Loss: 0.0641636773943901, Final Batch Loss: 0.03761439770460129\n",
      "Epoch 2315, Loss: 0.0598945077508688, Final Batch Loss: 0.0247707050293684\n",
      "Epoch 2316, Loss: 0.06896942667663097, Final Batch Loss: 0.038388095796108246\n",
      "Epoch 2317, Loss: 0.08224720135331154, Final Batch Loss: 0.029165949672460556\n",
      "Epoch 2318, Loss: 0.09678651764988899, Final Batch Loss: 0.07457511126995087\n",
      "Epoch 2319, Loss: 0.11814606189727783, Final Batch Loss: 0.05055956542491913\n",
      "Epoch 2320, Loss: 0.0982821136713028, Final Batch Loss: 0.04472578689455986\n",
      "Epoch 2321, Loss: 0.09629980474710464, Final Batch Loss: 0.06375666707754135\n",
      "Epoch 2322, Loss: 0.08557713031768799, Final Batch Loss: 0.049161992967128754\n",
      "Epoch 2323, Loss: 0.103920828551054, Final Batch Loss: 0.06446556746959686\n",
      "Epoch 2324, Loss: 0.07524286769330502, Final Batch Loss: 0.023988695815205574\n",
      "Epoch 2325, Loss: 0.05754968710243702, Final Batch Loss: 0.03583396226167679\n",
      "Epoch 2326, Loss: 0.09614434465765953, Final Batch Loss: 0.04386062175035477\n",
      "Epoch 2327, Loss: 0.10945208370685577, Final Batch Loss: 0.07855832576751709\n",
      "Epoch 2328, Loss: 0.08633922785520554, Final Batch Loss: 0.04270012676715851\n",
      "Epoch 2329, Loss: 0.07554974034428596, Final Batch Loss: 0.03139185905456543\n",
      "Epoch 2330, Loss: 0.06888089515268803, Final Batch Loss: 0.04037804156541824\n",
      "Epoch 2331, Loss: 0.0717460960149765, Final Batch Loss: 0.03311527520418167\n",
      "Epoch 2332, Loss: 0.09007306396961212, Final Batch Loss: 0.05751115083694458\n",
      "Epoch 2333, Loss: 0.18896517902612686, Final Batch Loss: 0.1234298050403595\n",
      "Epoch 2334, Loss: 0.10799207910895348, Final Batch Loss: 0.02700432762503624\n",
      "Epoch 2335, Loss: 0.09060493670403957, Final Batch Loss: 0.026474429294466972\n",
      "Epoch 2336, Loss: 0.04887291416525841, Final Batch Loss: 0.02809005044400692\n",
      "Epoch 2337, Loss: 0.15579906105995178, Final Batch Loss: 0.11501531302928925\n",
      "Epoch 2338, Loss: 0.05589460767805576, Final Batch Loss: 0.01433960534632206\n",
      "Epoch 2339, Loss: 0.089748065918684, Final Batch Loss: 0.042729452252388\n",
      "Epoch 2340, Loss: 0.08741497993469238, Final Batch Loss: 0.04551567882299423\n",
      "Epoch 2341, Loss: 0.03723452240228653, Final Batch Loss: 0.012597078457474709\n",
      "Epoch 2342, Loss: 0.08014468476176262, Final Batch Loss: 0.03297773748636246\n",
      "Epoch 2343, Loss: 0.11407636478543282, Final Batch Loss: 0.08866599202156067\n",
      "Epoch 2344, Loss: 0.09332198649644852, Final Batch Loss: 0.050434526056051254\n",
      "Epoch 2345, Loss: 0.06707879155874252, Final Batch Loss: 0.04934559389948845\n",
      "Epoch 2346, Loss: 0.046770451590418816, Final Batch Loss: 0.024952610954642296\n",
      "Epoch 2347, Loss: 0.04032142646610737, Final Batch Loss: 0.018742132931947708\n",
      "Epoch 2348, Loss: 0.0597953163087368, Final Batch Loss: 0.029352307319641113\n",
      "Epoch 2349, Loss: 0.07700860127806664, Final Batch Loss: 0.034064967185258865\n",
      "Epoch 2350, Loss: 0.13301928713917732, Final Batch Loss: 0.08084975183010101\n",
      "Epoch 2351, Loss: 0.11051423847675323, Final Batch Loss: 0.04214571416378021\n",
      "Epoch 2352, Loss: 0.07252676039934158, Final Batch Loss: 0.03316505253314972\n",
      "Epoch 2353, Loss: 0.11054535582661629, Final Batch Loss: 0.06370754539966583\n",
      "Epoch 2354, Loss: 0.09075897559523582, Final Batch Loss: 0.02001025900244713\n",
      "Epoch 2355, Loss: 0.0889018252491951, Final Batch Loss: 0.053550850600004196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2356, Loss: 0.06329433619976044, Final Batch Loss: 0.0296306349337101\n",
      "Epoch 2357, Loss: 0.1231005871668458, Final Batch Loss: 0.11036961525678635\n",
      "Epoch 2358, Loss: 0.0603170245885849, Final Batch Loss: 0.04539506509900093\n",
      "Epoch 2359, Loss: 0.08359502255916595, Final Batch Loss: 0.048096735030412674\n",
      "Epoch 2360, Loss: 0.08883540146052837, Final Batch Loss: 0.06348727643489838\n",
      "Epoch 2361, Loss: 0.04103217367082834, Final Batch Loss: 0.02630976028740406\n",
      "Epoch 2362, Loss: 0.06151021271944046, Final Batch Loss: 0.024286359548568726\n",
      "Epoch 2363, Loss: 0.05602732487022877, Final Batch Loss: 0.020076612010598183\n",
      "Epoch 2364, Loss: 0.08497780933976173, Final Batch Loss: 0.033487774431705475\n",
      "Epoch 2365, Loss: 0.06916311755776405, Final Batch Loss: 0.029807448387145996\n",
      "Epoch 2366, Loss: 0.08311929926276207, Final Batch Loss: 0.04546111077070236\n",
      "Epoch 2367, Loss: 0.15658576786518097, Final Batch Loss: 0.04180146008729935\n",
      "Epoch 2368, Loss: 0.14154665917158127, Final Batch Loss: 0.06230247765779495\n",
      "Epoch 2369, Loss: 0.121799954213202, Final Batch Loss: 0.10765019059181213\n",
      "Epoch 2370, Loss: 0.062056781724095345, Final Batch Loss: 0.029038483276963234\n",
      "Epoch 2371, Loss: 0.12871933355927467, Final Batch Loss: 0.05711932107806206\n",
      "Epoch 2372, Loss: 0.026889463886618614, Final Batch Loss: 0.010584482923150063\n",
      "Epoch 2373, Loss: 0.06507386267185211, Final Batch Loss: 0.026147209107875824\n",
      "Epoch 2374, Loss: 0.09290442988276482, Final Batch Loss: 0.05921991541981697\n",
      "Epoch 2375, Loss: 0.11863501742482185, Final Batch Loss: 0.05021729692816734\n",
      "Epoch 2376, Loss: 0.07569688931107521, Final Batch Loss: 0.04320463165640831\n",
      "Epoch 2377, Loss: 0.09534591808915138, Final Batch Loss: 0.04066063091158867\n",
      "Epoch 2378, Loss: 0.07862171530723572, Final Batch Loss: 0.03441229090094566\n",
      "Epoch 2379, Loss: 0.11910599656403065, Final Batch Loss: 0.028804713860154152\n",
      "Epoch 2380, Loss: 0.0497968103736639, Final Batch Loss: 0.026349730789661407\n",
      "Epoch 2381, Loss: 0.045417165383696556, Final Batch Loss: 0.029859011992812157\n",
      "Epoch 2382, Loss: 0.05743869207799435, Final Batch Loss: 0.02746613696217537\n",
      "Epoch 2383, Loss: 0.078989677131176, Final Batch Loss: 0.036844316869974136\n",
      "Epoch 2384, Loss: 0.09982038475573063, Final Batch Loss: 0.02302970178425312\n",
      "Epoch 2385, Loss: 0.0814373642206192, Final Batch Loss: 0.03839242830872536\n",
      "Epoch 2386, Loss: 0.0662881638854742, Final Batch Loss: 0.01793130673468113\n",
      "Epoch 2387, Loss: 0.06307799927890301, Final Batch Loss: 0.040474772453308105\n",
      "Epoch 2388, Loss: 0.09056980349123478, Final Batch Loss: 0.06830252707004547\n",
      "Epoch 2389, Loss: 0.12085157074034214, Final Batch Loss: 0.09304212778806686\n",
      "Epoch 2390, Loss: 0.0968271866440773, Final Batch Loss: 0.06408803164958954\n",
      "Epoch 2391, Loss: 0.06490563414990902, Final Batch Loss: 0.03109014965593815\n",
      "Epoch 2392, Loss: 0.13078449293971062, Final Batch Loss: 0.08628673851490021\n",
      "Epoch 2393, Loss: 0.06547199748456478, Final Batch Loss: 0.03897246718406677\n",
      "Epoch 2394, Loss: 0.08227401971817017, Final Batch Loss: 0.04266767203807831\n",
      "Epoch 2395, Loss: 0.07763377577066422, Final Batch Loss: 0.04101426899433136\n",
      "Epoch 2396, Loss: 0.10354433581233025, Final Batch Loss: 0.013543795794248581\n",
      "Epoch 2397, Loss: 0.05807957611978054, Final Batch Loss: 0.018405767157673836\n",
      "Epoch 2398, Loss: 0.09647979028522968, Final Batch Loss: 0.02127130515873432\n",
      "Epoch 2399, Loss: 0.05194413848221302, Final Batch Loss: 0.02823442593216896\n",
      "Epoch 2400, Loss: 0.10098149999976158, Final Batch Loss: 0.0628584772348404\n",
      "Epoch 2401, Loss: 0.10447479784488678, Final Batch Loss: 0.07261109352111816\n",
      "Epoch 2402, Loss: 0.05274377018213272, Final Batch Loss: 0.022341566160321236\n",
      "Epoch 2403, Loss: 0.06319103762507439, Final Batch Loss: 0.02231576293706894\n",
      "Epoch 2404, Loss: 0.15658141672611237, Final Batch Loss: 0.10975269228219986\n",
      "Epoch 2405, Loss: 0.1150226853787899, Final Batch Loss: 0.06062907353043556\n",
      "Epoch 2406, Loss: 0.1235736683011055, Final Batch Loss: 0.04665444791316986\n",
      "Epoch 2407, Loss: 0.08288215473294258, Final Batch Loss: 0.02921115979552269\n",
      "Epoch 2408, Loss: 0.10894374549388885, Final Batch Loss: 0.05145033821463585\n",
      "Epoch 2409, Loss: 0.12874606996774673, Final Batch Loss: 0.07172401249408722\n",
      "Epoch 2410, Loss: 0.07224749587476254, Final Batch Loss: 0.020500266924500465\n",
      "Epoch 2411, Loss: 0.08834967389702797, Final Batch Loss: 0.05337551608681679\n",
      "Epoch 2412, Loss: 0.0723252221941948, Final Batch Loss: 0.03837515786290169\n",
      "Epoch 2413, Loss: 0.09379080310463905, Final Batch Loss: 0.04147356003522873\n",
      "Epoch 2414, Loss: 0.05601189471781254, Final Batch Loss: 0.02604242041707039\n",
      "Epoch 2415, Loss: 0.1474023386836052, Final Batch Loss: 0.06487958878278732\n",
      "Epoch 2416, Loss: 0.11027242615818977, Final Batch Loss: 0.03716630861163139\n",
      "Epoch 2417, Loss: 0.15949018858373165, Final Batch Loss: 0.13166670501232147\n",
      "Epoch 2418, Loss: 0.08799970149993896, Final Batch Loss: 0.03967320919036865\n",
      "Epoch 2419, Loss: 0.06313467770814896, Final Batch Loss: 0.033614274114370346\n",
      "Epoch 2420, Loss: 0.08343833312392235, Final Batch Loss: 0.04839646816253662\n",
      "Epoch 2421, Loss: 0.08543143421411514, Final Batch Loss: 0.06052946671843529\n",
      "Epoch 2422, Loss: 0.0856899842619896, Final Batch Loss: 0.06305918842554092\n",
      "Epoch 2423, Loss: 0.08025624603033066, Final Batch Loss: 0.034935612231492996\n",
      "Epoch 2424, Loss: 0.09891606122255325, Final Batch Loss: 0.03191785514354706\n",
      "Epoch 2425, Loss: 0.05520204082131386, Final Batch Loss: 0.01864427700638771\n",
      "Epoch 2426, Loss: 0.07129776477813721, Final Batch Loss: 0.033168237656354904\n",
      "Epoch 2427, Loss: 0.03559677489101887, Final Batch Loss: 0.017049383372068405\n",
      "Epoch 2428, Loss: 0.06404763087630272, Final Batch Loss: 0.03456296771764755\n",
      "Epoch 2429, Loss: 0.11326414719223976, Final Batch Loss: 0.07694454491138458\n",
      "Epoch 2430, Loss: 0.062455056235194206, Final Batch Loss: 0.028953315690159798\n",
      "Epoch 2431, Loss: 0.09361772984266281, Final Batch Loss: 0.04911098629236221\n",
      "Epoch 2432, Loss: 0.04548790864646435, Final Batch Loss: 0.02171320654451847\n",
      "Epoch 2433, Loss: 0.11961603909730911, Final Batch Loss: 0.0720660611987114\n",
      "Epoch 2434, Loss: 0.05675528198480606, Final Batch Loss: 0.033882059156894684\n",
      "Epoch 2435, Loss: 0.05270438827574253, Final Batch Loss: 0.030490724369883537\n",
      "Epoch 2436, Loss: 0.05959739908576012, Final Batch Loss: 0.03554384037852287\n",
      "Epoch 2437, Loss: 0.052487654611468315, Final Batch Loss: 0.02826068177819252\n",
      "Epoch 2438, Loss: 0.10295622050762177, Final Batch Loss: 0.08165531605482101\n",
      "Epoch 2439, Loss: 0.045962708070874214, Final Batch Loss: 0.02278011292219162\n",
      "Epoch 2440, Loss: 0.0674302950501442, Final Batch Loss: 0.040263283997774124\n",
      "Epoch 2441, Loss: 0.08083507604897022, Final Batch Loss: 0.028928117826581\n",
      "Epoch 2442, Loss: 0.07042638957500458, Final Batch Loss: 0.03368881344795227\n",
      "Epoch 2443, Loss: 0.08210866339504719, Final Batch Loss: 0.05885906517505646\n",
      "Epoch 2444, Loss: 0.06063032150268555, Final Batch Loss: 0.03400426357984543\n",
      "Epoch 2445, Loss: 0.06725729629397392, Final Batch Loss: 0.034718550741672516\n",
      "Epoch 2446, Loss: 0.04269266687333584, Final Batch Loss: 0.022949326783418655\n",
      "Epoch 2447, Loss: 0.07261352613568306, Final Batch Loss: 0.04582621157169342\n",
      "Epoch 2448, Loss: 0.05693364143371582, Final Batch Loss: 0.020551756024360657\n",
      "Epoch 2449, Loss: 0.06526783853769302, Final Batch Loss: 0.031014252454042435\n",
      "Epoch 2450, Loss: 0.05361483618617058, Final Batch Loss: 0.019341178238391876\n",
      "Epoch 2451, Loss: 0.0443707499653101, Final Batch Loss: 0.018486950546503067\n",
      "Epoch 2452, Loss: 0.04227560851722956, Final Batch Loss: 0.01092554721981287\n",
      "Epoch 2453, Loss: 0.06382012367248535, Final Batch Loss: 0.044474147260189056\n",
      "Epoch 2454, Loss: 0.059518201276659966, Final Batch Loss: 0.01634163223206997\n",
      "Epoch 2455, Loss: 0.07809445355087519, Final Batch Loss: 0.012183018960058689\n",
      "Epoch 2456, Loss: 0.07680179923772812, Final Batch Loss: 0.03405527397990227\n",
      "Epoch 2457, Loss: 0.1267189048230648, Final Batch Loss: 0.0741952657699585\n",
      "Epoch 2458, Loss: 0.10122638568282127, Final Batch Loss: 0.05998675897717476\n",
      "Epoch 2459, Loss: 0.040168341249227524, Final Batch Loss: 0.021465085446834564\n",
      "Epoch 2460, Loss: 0.11022241786122322, Final Batch Loss: 0.07937243580818176\n",
      "Epoch 2461, Loss: 0.030016825534403324, Final Batch Loss: 0.012554085813462734\n",
      "Epoch 2462, Loss: 0.11585329473018646, Final Batch Loss: 0.09604427963495255\n",
      "Epoch 2463, Loss: 0.0634122658520937, Final Batch Loss: 0.023123035207390785\n",
      "Epoch 2464, Loss: 0.08041328005492687, Final Batch Loss: 0.056371551007032394\n",
      "Epoch 2465, Loss: 0.05370200239121914, Final Batch Loss: 0.0316302515566349\n",
      "Epoch 2466, Loss: 0.06104739010334015, Final Batch Loss: 0.02450665831565857\n",
      "Epoch 2467, Loss: 0.07740576006472111, Final Batch Loss: 0.0548013411462307\n",
      "Epoch 2468, Loss: 0.07229684852063656, Final Batch Loss: 0.01830536685883999\n",
      "Epoch 2469, Loss: 0.05389339476823807, Final Batch Loss: 0.021372351795434952\n",
      "Epoch 2470, Loss: 0.06540412828326225, Final Batch Loss: 0.037433821707963943\n",
      "Epoch 2471, Loss: 0.05685723386704922, Final Batch Loss: 0.030890299007296562\n",
      "Epoch 2472, Loss: 0.07300607860088348, Final Batch Loss: 0.05918688699603081\n",
      "Epoch 2473, Loss: 0.06546677276492119, Final Batch Loss: 0.02990567684173584\n",
      "Epoch 2474, Loss: 0.07703565806150436, Final Batch Loss: 0.036359746009111404\n",
      "Epoch 2475, Loss: 0.08598137646913528, Final Batch Loss: 0.04819202795624733\n",
      "Epoch 2476, Loss: 0.05784520320594311, Final Batch Loss: 0.03177053853869438\n",
      "Epoch 2477, Loss: 0.07124415412545204, Final Batch Loss: 0.025724250823259354\n",
      "Epoch 2478, Loss: 0.039880610071122646, Final Batch Loss: 0.012364801950752735\n",
      "Epoch 2479, Loss: 0.07037153467535973, Final Batch Loss: 0.03694187104701996\n",
      "Epoch 2480, Loss: 0.08320568315684795, Final Batch Loss: 0.06316746771335602\n",
      "Epoch 2481, Loss: 0.060185711830854416, Final Batch Loss: 0.04342905059456825\n",
      "Epoch 2482, Loss: 0.05345257185399532, Final Batch Loss: 0.03063497692346573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2483, Loss: 0.03638075292110443, Final Batch Loss: 0.017879800871014595\n",
      "Epoch 2484, Loss: 0.21477573364973068, Final Batch Loss: 0.17340345680713654\n",
      "Epoch 2485, Loss: 0.10358192026615143, Final Batch Loss: 0.06432189792394638\n",
      "Epoch 2486, Loss: 0.040177847258746624, Final Batch Loss: 0.013846307061612606\n",
      "Epoch 2487, Loss: 0.03813539911061525, Final Batch Loss: 0.01022763829678297\n",
      "Epoch 2488, Loss: 0.04758497327566147, Final Batch Loss: 0.023548781871795654\n",
      "Epoch 2489, Loss: 0.05863224156200886, Final Batch Loss: 0.02918306179344654\n",
      "Epoch 2490, Loss: 0.05705679766833782, Final Batch Loss: 0.0165236908942461\n",
      "Epoch 2491, Loss: 0.06152765825390816, Final Batch Loss: 0.02796606346964836\n",
      "Epoch 2492, Loss: 0.07861586287617683, Final Batch Loss: 0.03493475914001465\n",
      "Epoch 2493, Loss: 0.04139760136604309, Final Batch Loss: 0.026198437437415123\n",
      "Epoch 2494, Loss: 0.10954471677541733, Final Batch Loss: 0.01810041069984436\n",
      "Epoch 2495, Loss: 0.0479405801743269, Final Batch Loss: 0.03147431090474129\n",
      "Epoch 2496, Loss: 0.0710039222612977, Final Batch Loss: 0.012836498208343983\n",
      "Epoch 2497, Loss: 0.07272275164723396, Final Batch Loss: 0.042393021285533905\n",
      "Epoch 2498, Loss: 0.05899475421756506, Final Batch Loss: 0.010600189678370953\n",
      "Epoch 2499, Loss: 0.0654955767095089, Final Batch Loss: 0.0329592302441597\n",
      "Epoch 2500, Loss: 0.037303268909454346, Final Batch Loss: 0.016615737229585648\n",
      "Epoch 2501, Loss: 0.06942141428589821, Final Batch Loss: 0.04693883657455444\n",
      "Epoch 2502, Loss: 0.048883501440286636, Final Batch Loss: 0.02482197806239128\n",
      "Epoch 2503, Loss: 0.04452029149979353, Final Batch Loss: 0.012989099137485027\n",
      "Epoch 2504, Loss: 0.06358977034687996, Final Batch Loss: 0.03129976987838745\n",
      "Epoch 2505, Loss: 0.02980227768421173, Final Batch Loss: 0.01145331934094429\n",
      "Epoch 2506, Loss: 0.08543037995696068, Final Batch Loss: 0.05778159946203232\n",
      "Epoch 2507, Loss: 0.041094446554780006, Final Batch Loss: 0.018924454227089882\n",
      "Epoch 2508, Loss: 0.04764650762081146, Final Batch Loss: 0.0159260593354702\n",
      "Epoch 2509, Loss: 0.031009506434202194, Final Batch Loss: 0.011363551020622253\n",
      "Epoch 2510, Loss: 0.03973818011581898, Final Batch Loss: 0.02715146914124489\n",
      "Epoch 2511, Loss: 0.042344081215560436, Final Batch Loss: 0.014157040975987911\n",
      "Epoch 2512, Loss: 0.11284059658646584, Final Batch Loss: 0.04689345881342888\n",
      "Epoch 2513, Loss: 0.07703026011586189, Final Batch Loss: 0.02560531347990036\n",
      "Epoch 2514, Loss: 0.04533823020756245, Final Batch Loss: 0.015767017379403114\n",
      "Epoch 2515, Loss: 0.0382978655397892, Final Batch Loss: 0.010612620040774345\n",
      "Epoch 2516, Loss: 0.03492734208703041, Final Batch Loss: 0.018345186486840248\n",
      "Epoch 2517, Loss: 0.06545127369463444, Final Batch Loss: 0.021831514313817024\n",
      "Epoch 2518, Loss: 0.07161507941782475, Final Batch Loss: 0.018433155491948128\n",
      "Epoch 2519, Loss: 0.04294600151479244, Final Batch Loss: 0.025941764935851097\n",
      "Epoch 2520, Loss: 0.10476094670593739, Final Batch Loss: 0.030680039897561073\n",
      "Epoch 2521, Loss: 0.0807416420429945, Final Batch Loss: 0.05609521269798279\n",
      "Epoch 2522, Loss: 0.07500892505049706, Final Batch Loss: 0.017480861395597458\n",
      "Epoch 2523, Loss: 0.0654276143759489, Final Batch Loss: 0.029464079067111015\n",
      "Epoch 2524, Loss: 0.08172668889164925, Final Batch Loss: 0.03828872740268707\n",
      "Epoch 2525, Loss: 0.05478559620678425, Final Batch Loss: 0.018898768350481987\n",
      "Epoch 2526, Loss: 0.04546455293893814, Final Batch Loss: 0.03211893513798714\n",
      "Epoch 2527, Loss: 0.03804108127951622, Final Batch Loss: 0.021824361756443977\n",
      "Epoch 2528, Loss: 0.08264639601111412, Final Batch Loss: 0.015328537672758102\n",
      "Epoch 2529, Loss: 0.09922005981206894, Final Batch Loss: 0.07389757037162781\n",
      "Epoch 2530, Loss: 0.052968237549066544, Final Batch Loss: 0.022049300372600555\n",
      "Epoch 2531, Loss: 0.04138003382831812, Final Batch Loss: 0.012255067937076092\n",
      "Epoch 2532, Loss: 0.046473560854792595, Final Batch Loss: 0.020399009808897972\n",
      "Epoch 2533, Loss: 0.04788596089929342, Final Batch Loss: 0.012640914879739285\n",
      "Epoch 2534, Loss: 0.04709949716925621, Final Batch Loss: 0.02353123016655445\n",
      "Epoch 2535, Loss: 0.06026078388094902, Final Batch Loss: 0.016283560544252396\n",
      "Epoch 2536, Loss: 0.06166377943009138, Final Batch Loss: 0.012964273802936077\n",
      "Epoch 2537, Loss: 0.04999016039073467, Final Batch Loss: 0.018878009170293808\n",
      "Epoch 2538, Loss: 0.0602225735783577, Final Batch Loss: 0.024198871105909348\n",
      "Epoch 2539, Loss: 0.06766021810472012, Final Batch Loss: 0.0444309264421463\n",
      "Epoch 2540, Loss: 0.0396539606153965, Final Batch Loss: 0.022608544677495956\n",
      "Epoch 2541, Loss: 0.07485753297805786, Final Batch Loss: 0.044869475066661835\n",
      "Epoch 2542, Loss: 0.05683494545519352, Final Batch Loss: 0.02383766882121563\n",
      "Epoch 2543, Loss: 0.05450340732932091, Final Batch Loss: 0.016058463603258133\n",
      "Epoch 2544, Loss: 0.11561130546033382, Final Batch Loss: 0.08577776700258255\n",
      "Epoch 2545, Loss: 0.09285689517855644, Final Batch Loss: 0.048719052225351334\n",
      "Epoch 2546, Loss: 0.06534420140087605, Final Batch Loss: 0.03861413523554802\n",
      "Epoch 2547, Loss: 0.05708244442939758, Final Batch Loss: 0.029382333159446716\n",
      "Epoch 2548, Loss: 0.06037605740129948, Final Batch Loss: 0.03334528207778931\n",
      "Epoch 2549, Loss: 0.09760107845067978, Final Batch Loss: 0.039050228893756866\n",
      "Epoch 2550, Loss: 0.05658942274749279, Final Batch Loss: 0.02186891995370388\n",
      "Epoch 2551, Loss: 0.039689403027296066, Final Batch Loss: 0.024748321622610092\n",
      "Epoch 2552, Loss: 0.056213196367025375, Final Batch Loss: 0.01771734282374382\n",
      "Epoch 2553, Loss: 0.04057259112596512, Final Batch Loss: 0.018050581216812134\n",
      "Epoch 2554, Loss: 0.03963628038764, Final Batch Loss: 0.01989556849002838\n",
      "Epoch 2555, Loss: 0.13366114348173141, Final Batch Loss: 0.10342682898044586\n",
      "Epoch 2556, Loss: 0.06267526932060719, Final Batch Loss: 0.029885591939091682\n",
      "Epoch 2557, Loss: 0.0749339871108532, Final Batch Loss: 0.049916449934244156\n",
      "Epoch 2558, Loss: 0.05287219397723675, Final Batch Loss: 0.031381815671920776\n",
      "Epoch 2559, Loss: 0.11397990211844444, Final Batch Loss: 0.06567533314228058\n",
      "Epoch 2560, Loss: 0.060247182846069336, Final Batch Loss: 0.032873280346393585\n",
      "Epoch 2561, Loss: 0.10023114271461964, Final Batch Loss: 0.08332109451293945\n",
      "Epoch 2562, Loss: 0.11553959548473358, Final Batch Loss: 0.07791709899902344\n",
      "Epoch 2563, Loss: 0.06943469867110252, Final Batch Loss: 0.017057891935110092\n",
      "Epoch 2564, Loss: 0.05183176975697279, Final Batch Loss: 0.010666369460523129\n",
      "Epoch 2565, Loss: 0.1281546764075756, Final Batch Loss: 0.0743941143155098\n",
      "Epoch 2566, Loss: 0.10202515870332718, Final Batch Loss: 0.05255579575896263\n",
      "Epoch 2567, Loss: 0.07961033657193184, Final Batch Loss: 0.030875809490680695\n",
      "Epoch 2568, Loss: 0.15139102190732956, Final Batch Loss: 0.09245455265045166\n",
      "Epoch 2569, Loss: 0.12036434188485146, Final Batch Loss: 0.041428353637456894\n",
      "Epoch 2570, Loss: 0.1101456731557846, Final Batch Loss: 0.048191770911216736\n",
      "Epoch 2571, Loss: 0.18345413357019424, Final Batch Loss: 0.1207633912563324\n",
      "Epoch 2572, Loss: 0.1084054447710514, Final Batch Loss: 0.04005178436636925\n",
      "Epoch 2573, Loss: 0.11748666316270828, Final Batch Loss: 0.070552758872509\n",
      "Epoch 2574, Loss: 0.08025283925235271, Final Batch Loss: 0.02300901524722576\n",
      "Epoch 2575, Loss: 0.0627654530107975, Final Batch Loss: 0.03588750958442688\n",
      "Epoch 2576, Loss: 0.09921454638242722, Final Batch Loss: 0.05231025069952011\n",
      "Epoch 2577, Loss: 0.11548467725515366, Final Batch Loss: 0.06690315902233124\n",
      "Epoch 2578, Loss: 0.07314030081033707, Final Batch Loss: 0.029710549861192703\n",
      "Epoch 2579, Loss: 0.07489270344376564, Final Batch Loss: 0.04097554832696915\n",
      "Epoch 2580, Loss: 0.05166410095989704, Final Batch Loss: 0.025473395362496376\n",
      "Epoch 2581, Loss: 0.06465424597263336, Final Batch Loss: 0.03932575136423111\n",
      "Epoch 2582, Loss: 0.07900035381317139, Final Batch Loss: 0.03358331322669983\n",
      "Epoch 2583, Loss: 0.046181000769138336, Final Batch Loss: 0.025736737996339798\n",
      "Epoch 2584, Loss: 0.22687037102878094, Final Batch Loss: 0.19800004363059998\n",
      "Epoch 2585, Loss: 0.08241583034396172, Final Batch Loss: 0.04069925472140312\n",
      "Epoch 2586, Loss: 0.07416731491684914, Final Batch Loss: 0.044505808502435684\n",
      "Epoch 2587, Loss: 0.08126475848257542, Final Batch Loss: 0.02780461497604847\n",
      "Epoch 2588, Loss: 0.05082060769200325, Final Batch Loss: 0.0188276544213295\n",
      "Epoch 2589, Loss: 0.0459065455943346, Final Batch Loss: 0.028260784223675728\n",
      "Epoch 2590, Loss: 0.06687698885798454, Final Batch Loss: 0.03821166604757309\n",
      "Epoch 2591, Loss: 0.059468938037753105, Final Batch Loss: 0.03250567242503166\n",
      "Epoch 2592, Loss: 0.09868639335036278, Final Batch Loss: 0.04459601640701294\n",
      "Epoch 2593, Loss: 0.08526023104786873, Final Batch Loss: 0.06793436408042908\n",
      "Epoch 2594, Loss: 0.10481657460331917, Final Batch Loss: 0.06761040538549423\n",
      "Epoch 2595, Loss: 0.13797058910131454, Final Batch Loss: 0.07976086437702179\n",
      "Epoch 2596, Loss: 0.05751729570329189, Final Batch Loss: 0.037139300256967545\n",
      "Epoch 2597, Loss: 0.03531711362302303, Final Batch Loss: 0.016567157581448555\n",
      "Epoch 2598, Loss: 0.05201411060988903, Final Batch Loss: 0.022107040509581566\n",
      "Epoch 2599, Loss: 0.05688037723302841, Final Batch Loss: 0.02788124606013298\n",
      "Epoch 2600, Loss: 0.11339371651411057, Final Batch Loss: 0.08395493775606155\n",
      "Epoch 2601, Loss: 0.09509625844657421, Final Batch Loss: 0.07714919000864029\n",
      "Epoch 2602, Loss: 0.0345508074387908, Final Batch Loss: 0.020309001207351685\n",
      "Epoch 2603, Loss: 0.058356767520308495, Final Batch Loss: 0.017572296783328056\n",
      "Epoch 2604, Loss: 0.056004609912633896, Final Batch Loss: 0.036313094198703766\n",
      "Epoch 2605, Loss: 0.08869202435016632, Final Batch Loss: 0.03779299557209015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2606, Loss: 0.08231153152883053, Final Batch Loss: 0.0578843355178833\n",
      "Epoch 2607, Loss: 0.06848034635186195, Final Batch Loss: 0.047260064631700516\n",
      "Epoch 2608, Loss: 0.06044749915599823, Final Batch Loss: 0.03330974280834198\n",
      "Epoch 2609, Loss: 0.049565110355615616, Final Batch Loss: 0.022761676460504532\n",
      "Epoch 2610, Loss: 0.06005188263952732, Final Batch Loss: 0.02333400957286358\n",
      "Epoch 2611, Loss: 0.0744403526186943, Final Batch Loss: 0.05151403322815895\n",
      "Epoch 2612, Loss: 0.046788737177848816, Final Batch Loss: 0.025767074897885323\n",
      "Epoch 2613, Loss: 0.05852584354579449, Final Batch Loss: 0.027131808921694756\n",
      "Epoch 2614, Loss: 0.05179504491388798, Final Batch Loss: 0.023743269965052605\n",
      "Epoch 2615, Loss: 0.09885888732969761, Final Batch Loss: 0.06913876533508301\n",
      "Epoch 2616, Loss: 0.1289375275373459, Final Batch Loss: 0.046525485813617706\n",
      "Epoch 2617, Loss: 0.055516332387924194, Final Batch Loss: 0.015947535634040833\n",
      "Epoch 2618, Loss: 0.044788530096411705, Final Batch Loss: 0.011181829497218132\n",
      "Epoch 2619, Loss: 0.04446314927190542, Final Batch Loss: 0.033577535301446915\n",
      "Epoch 2620, Loss: 0.08986819162964821, Final Batch Loss: 0.044843774288892746\n",
      "Epoch 2621, Loss: 0.06854088045656681, Final Batch Loss: 0.03025580383837223\n",
      "Epoch 2622, Loss: 0.048437949270009995, Final Batch Loss: 0.026003681123256683\n",
      "Epoch 2623, Loss: 0.07877984829246998, Final Batch Loss: 0.05203372240066528\n",
      "Epoch 2624, Loss: 0.03795301727950573, Final Batch Loss: 0.009740142151713371\n",
      "Epoch 2625, Loss: 0.04106695577502251, Final Batch Loss: 0.020389743149280548\n",
      "Epoch 2626, Loss: 0.03216646146029234, Final Batch Loss: 0.017262455075979233\n",
      "Epoch 2627, Loss: 0.09038976952433586, Final Batch Loss: 0.06956402212381363\n",
      "Epoch 2628, Loss: 0.051932547241449356, Final Batch Loss: 0.018768951296806335\n",
      "Epoch 2629, Loss: 0.05176236480474472, Final Batch Loss: 0.034982893615961075\n",
      "Epoch 2630, Loss: 0.09756547212600708, Final Batch Loss: 0.05397892743349075\n",
      "Epoch 2631, Loss: 0.07431965135037899, Final Batch Loss: 0.022845422849059105\n",
      "Epoch 2632, Loss: 0.04629563167691231, Final Batch Loss: 0.02416149526834488\n",
      "Epoch 2633, Loss: 0.0480700908228755, Final Batch Loss: 0.032536525279283524\n",
      "Epoch 2634, Loss: 0.04736419767141342, Final Batch Loss: 0.02996951714158058\n",
      "Epoch 2635, Loss: 0.09384598396718502, Final Batch Loss: 0.023497043177485466\n",
      "Epoch 2636, Loss: 0.06931481882929802, Final Batch Loss: 0.03239782899618149\n",
      "Epoch 2637, Loss: 0.07346498221158981, Final Batch Loss: 0.035352230072021484\n",
      "Epoch 2638, Loss: 0.04586889035999775, Final Batch Loss: 0.024224920198321342\n",
      "Epoch 2639, Loss: 0.0814631748944521, Final Batch Loss: 0.029599254950881004\n",
      "Epoch 2640, Loss: 0.09762263298034668, Final Batch Loss: 0.029935143887996674\n",
      "Epoch 2641, Loss: 0.031774585135281086, Final Batch Loss: 0.013477114029228687\n",
      "Epoch 2642, Loss: 0.12556955590844154, Final Batch Loss: 0.06750191748142242\n",
      "Epoch 2643, Loss: 0.058988768607378006, Final Batch Loss: 0.018729928880929947\n",
      "Epoch 2644, Loss: 0.09833980910480022, Final Batch Loss: 0.06825383007526398\n",
      "Epoch 2645, Loss: 0.056770045310258865, Final Batch Loss: 0.021252959966659546\n",
      "Epoch 2646, Loss: 0.09223178587853909, Final Batch Loss: 0.0748271718621254\n",
      "Epoch 2647, Loss: 0.06371742859482765, Final Batch Loss: 0.02450382709503174\n",
      "Epoch 2648, Loss: 0.04726954735815525, Final Batch Loss: 0.030336877331137657\n",
      "Epoch 2649, Loss: 0.058173228055238724, Final Batch Loss: 0.036177858710289\n",
      "Epoch 2650, Loss: 0.06059181597083807, Final Batch Loss: 0.013984899036586285\n",
      "Epoch 2651, Loss: 0.060614146292209625, Final Batch Loss: 0.017444763332605362\n",
      "Epoch 2652, Loss: 0.06040886975824833, Final Batch Loss: 0.04616400599479675\n",
      "Epoch 2653, Loss: 0.06796794757246971, Final Batch Loss: 0.04217824339866638\n",
      "Epoch 2654, Loss: 0.07986580021679401, Final Batch Loss: 0.05048771947622299\n",
      "Epoch 2655, Loss: 0.09917296655476093, Final Batch Loss: 0.07443290948867798\n",
      "Epoch 2656, Loss: 0.044488104060292244, Final Batch Loss: 0.019578665494918823\n",
      "Epoch 2657, Loss: 0.0808705035597086, Final Batch Loss: 0.023191044107079506\n",
      "Epoch 2658, Loss: 0.08812512084841728, Final Batch Loss: 0.04660935327410698\n",
      "Epoch 2659, Loss: 0.04524574801325798, Final Batch Loss: 0.024114498868584633\n",
      "Epoch 2660, Loss: 0.043251994997262955, Final Batch Loss: 0.0191373061388731\n",
      "Epoch 2661, Loss: 0.048436833545565605, Final Batch Loss: 0.02558942511677742\n",
      "Epoch 2662, Loss: 0.07639802806079388, Final Batch Loss: 0.060722775757312775\n",
      "Epoch 2663, Loss: 0.040880175307393074, Final Batch Loss: 0.008456511422991753\n",
      "Epoch 2664, Loss: 0.05958960019052029, Final Batch Loss: 0.03325231373310089\n",
      "Epoch 2665, Loss: 0.036463670432567596, Final Batch Loss: 0.017489014193415642\n",
      "Epoch 2666, Loss: 0.05819959007203579, Final Batch Loss: 0.01444009505212307\n",
      "Epoch 2667, Loss: 0.0875825984403491, Final Batch Loss: 0.07288460433483124\n",
      "Epoch 2668, Loss: 0.07719823718070984, Final Batch Loss: 0.01947600021958351\n",
      "Epoch 2669, Loss: 0.07078459486365318, Final Batch Loss: 0.025574903935194016\n",
      "Epoch 2670, Loss: 0.06812714599072933, Final Batch Loss: 0.021728908643126488\n",
      "Epoch 2671, Loss: 0.059171516448259354, Final Batch Loss: 0.02334381639957428\n",
      "Epoch 2672, Loss: 0.044261349365115166, Final Batch Loss: 0.021464185789227486\n",
      "Epoch 2673, Loss: 0.07093162834644318, Final Batch Loss: 0.03653343394398689\n",
      "Epoch 2674, Loss: 0.06543747335672379, Final Batch Loss: 0.04586579278111458\n",
      "Epoch 2675, Loss: 0.07688310369849205, Final Batch Loss: 0.03175756335258484\n",
      "Epoch 2676, Loss: 0.10471395775675774, Final Batch Loss: 0.08299872279167175\n",
      "Epoch 2677, Loss: 0.046952979639172554, Final Batch Loss: 0.01628531515598297\n",
      "Epoch 2678, Loss: 0.0532281119376421, Final Batch Loss: 0.028006955981254578\n",
      "Epoch 2679, Loss: 0.06880822405219078, Final Batch Loss: 0.03526446968317032\n",
      "Epoch 2680, Loss: 0.08405401185154915, Final Batch Loss: 0.0346725694835186\n",
      "Epoch 2681, Loss: 0.06651189178228378, Final Batch Loss: 0.02971123531460762\n",
      "Epoch 2682, Loss: 0.11078032106161118, Final Batch Loss: 0.03263721615076065\n",
      "Epoch 2683, Loss: 0.06947563588619232, Final Batch Loss: 0.037093937397003174\n",
      "Epoch 2684, Loss: 0.05260409973561764, Final Batch Loss: 0.01668948493897915\n",
      "Epoch 2685, Loss: 0.10207035765051842, Final Batch Loss: 0.035293687134981155\n",
      "Epoch 2686, Loss: 0.04619280993938446, Final Batch Loss: 0.020548012107610703\n",
      "Epoch 2687, Loss: 0.05432053515687585, Final Batch Loss: 0.0042211622931063175\n",
      "Epoch 2688, Loss: 0.08484872430562973, Final Batch Loss: 0.06510920077562332\n",
      "Epoch 2689, Loss: 0.08230909146368504, Final Batch Loss: 0.017020048573613167\n",
      "Epoch 2690, Loss: 0.04376726783812046, Final Batch Loss: 0.02728605642914772\n",
      "Epoch 2691, Loss: 0.04713048879057169, Final Batch Loss: 0.011014514602720737\n",
      "Epoch 2692, Loss: 0.06267369724810123, Final Batch Loss: 0.04097747802734375\n",
      "Epoch 2693, Loss: 0.08446120843291283, Final Batch Loss: 0.03489192575216293\n",
      "Epoch 2694, Loss: 0.0545204933732748, Final Batch Loss: 0.025439659133553505\n",
      "Epoch 2695, Loss: 0.06311172433197498, Final Batch Loss: 0.04278184846043587\n",
      "Epoch 2696, Loss: 0.05554775707423687, Final Batch Loss: 0.026522871106863022\n",
      "Epoch 2697, Loss: 0.08704198524355888, Final Batch Loss: 0.03694593161344528\n",
      "Epoch 2698, Loss: 0.031007196754217148, Final Batch Loss: 0.00927356630563736\n",
      "Epoch 2699, Loss: 0.06155896931886673, Final Batch Loss: 0.035864900797605515\n",
      "Epoch 2700, Loss: 0.03788722399622202, Final Batch Loss: 0.009828544221818447\n",
      "Epoch 2701, Loss: 0.07018176093697548, Final Batch Loss: 0.045850399881601334\n",
      "Epoch 2702, Loss: 0.02439691312611103, Final Batch Loss: 0.012140370905399323\n",
      "Epoch 2703, Loss: 0.06716619245707989, Final Batch Loss: 0.04604804143309593\n",
      "Epoch 2704, Loss: 0.04668287094682455, Final Batch Loss: 0.00970491673797369\n",
      "Epoch 2705, Loss: 0.03562154807150364, Final Batch Loss: 0.01573479361832142\n",
      "Epoch 2706, Loss: 0.07867918908596039, Final Batch Loss: 0.04211316257715225\n",
      "Epoch 2707, Loss: 0.04681879281997681, Final Batch Loss: 0.020389961078763008\n",
      "Epoch 2708, Loss: 0.07217267714440823, Final Batch Loss: 0.01719597913324833\n",
      "Epoch 2709, Loss: 0.038910238072276115, Final Batch Loss: 0.019678106531500816\n",
      "Epoch 2710, Loss: 0.07992667146027088, Final Batch Loss: 0.06279288232326508\n",
      "Epoch 2711, Loss: 0.11064144596457481, Final Batch Loss: 0.04054752364754677\n",
      "Epoch 2712, Loss: 0.02747164387255907, Final Batch Loss: 0.013373457826673985\n",
      "Epoch 2713, Loss: 0.05982894450426102, Final Batch Loss: 0.03415181115269661\n",
      "Epoch 2714, Loss: 0.11471984535455704, Final Batch Loss: 0.06798291206359863\n",
      "Epoch 2715, Loss: 0.07094836421310902, Final Batch Loss: 0.023716913536190987\n",
      "Epoch 2716, Loss: 0.049366727471351624, Final Batch Loss: 0.02452586218714714\n",
      "Epoch 2717, Loss: 0.045815255492925644, Final Batch Loss: 0.0233294777572155\n",
      "Epoch 2718, Loss: 0.0807150024920702, Final Batch Loss: 0.0568498894572258\n",
      "Epoch 2719, Loss: 0.03823680058121681, Final Batch Loss: 0.03000561147928238\n",
      "Epoch 2720, Loss: 0.12131845578551292, Final Batch Loss: 0.052465420216321945\n",
      "Epoch 2721, Loss: 0.05747147463262081, Final Batch Loss: 0.04011235758662224\n",
      "Epoch 2722, Loss: 0.0349403265863657, Final Batch Loss: 0.013076003640890121\n",
      "Epoch 2723, Loss: 0.06457826495170593, Final Batch Loss: 0.027744513005018234\n",
      "Epoch 2724, Loss: 0.07769420184195042, Final Batch Loss: 0.05475541204214096\n",
      "Epoch 2725, Loss: 0.04411414451897144, Final Batch Loss: 0.023425167426466942\n",
      "Epoch 2726, Loss: 0.10997461341321468, Final Batch Loss: 0.018393253907561302\n",
      "Epoch 2727, Loss: 0.04917618725448847, Final Batch Loss: 0.014142603613436222\n",
      "Epoch 2728, Loss: 0.08346502110362053, Final Batch Loss: 0.038074832409620285\n",
      "Epoch 2729, Loss: 0.059194738045334816, Final Batch Loss: 0.03881356865167618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2730, Loss: 0.05525314435362816, Final Batch Loss: 0.031238917261362076\n",
      "Epoch 2731, Loss: 0.05795540288090706, Final Batch Loss: 0.0324612595140934\n",
      "Epoch 2732, Loss: 0.05198890343308449, Final Batch Loss: 0.021927421912550926\n",
      "Epoch 2733, Loss: 0.07105023227632046, Final Batch Loss: 0.026930151507258415\n",
      "Epoch 2734, Loss: 0.0569591224193573, Final Batch Loss: 0.03287990018725395\n",
      "Epoch 2735, Loss: 0.13302414491772652, Final Batch Loss: 0.10172796249389648\n",
      "Epoch 2736, Loss: 0.031037290580570698, Final Batch Loss: 0.016276158392429352\n",
      "Epoch 2737, Loss: 0.06296025216579437, Final Batch Loss: 0.01912664994597435\n",
      "Epoch 2738, Loss: 0.06277106516063213, Final Batch Loss: 0.03245266526937485\n",
      "Epoch 2739, Loss: 0.048209793865680695, Final Batch Loss: 0.017474660649895668\n",
      "Epoch 2740, Loss: 0.03150724247097969, Final Batch Loss: 0.017147812992334366\n",
      "Epoch 2741, Loss: 0.05459280498325825, Final Batch Loss: 0.02774220146238804\n",
      "Epoch 2742, Loss: 0.10480276867747307, Final Batch Loss: 0.08411027491092682\n",
      "Epoch 2743, Loss: 0.1117262989282608, Final Batch Loss: 0.0776272565126419\n",
      "Epoch 2744, Loss: 0.05717947706580162, Final Batch Loss: 0.040922630578279495\n",
      "Epoch 2745, Loss: 0.1523679904639721, Final Batch Loss: 0.05587087199091911\n",
      "Epoch 2746, Loss: 0.042748209089040756, Final Batch Loss: 0.013399781659245491\n",
      "Epoch 2747, Loss: 0.10136456787586212, Final Batch Loss: 0.036861613392829895\n",
      "Epoch 2748, Loss: 0.054774124175310135, Final Batch Loss: 0.03112957626581192\n",
      "Epoch 2749, Loss: 0.047054020687937737, Final Batch Loss: 0.02111201360821724\n",
      "Epoch 2750, Loss: 0.03407882899045944, Final Batch Loss: 0.012115681543946266\n",
      "Epoch 2751, Loss: 0.05922588985413313, Final Batch Loss: 0.01323761697858572\n",
      "Epoch 2752, Loss: 0.10018449276685715, Final Batch Loss: 0.05602973327040672\n",
      "Epoch 2753, Loss: 0.03581278212368488, Final Batch Loss: 0.0133204385638237\n",
      "Epoch 2754, Loss: 0.05297405645251274, Final Batch Loss: 0.02382626198232174\n",
      "Epoch 2755, Loss: 0.0775463730096817, Final Batch Loss: 0.022674422711133957\n",
      "Epoch 2756, Loss: 0.09194875694811344, Final Batch Loss: 0.0732111781835556\n",
      "Epoch 2757, Loss: 0.061209190636873245, Final Batch Loss: 0.03276921063661575\n",
      "Epoch 2758, Loss: 0.06686858274042606, Final Batch Loss: 0.045496437698602676\n",
      "Epoch 2759, Loss: 0.06622310727834702, Final Batch Loss: 0.02943424880504608\n",
      "Epoch 2760, Loss: 0.0508540291339159, Final Batch Loss: 0.02256121300160885\n",
      "Epoch 2761, Loss: 0.04972304776310921, Final Batch Loss: 0.031261373311281204\n",
      "Epoch 2762, Loss: 0.08024753630161285, Final Batch Loss: 0.039353519678115845\n",
      "Epoch 2763, Loss: 0.10638310760259628, Final Batch Loss: 0.06756321340799332\n",
      "Epoch 2764, Loss: 0.09139015898108482, Final Batch Loss: 0.049692314118146896\n",
      "Epoch 2765, Loss: 0.037561651319265366, Final Batch Loss: 0.028920141980051994\n",
      "Epoch 2766, Loss: 0.05522098485380411, Final Batch Loss: 0.04092393442988396\n",
      "Epoch 2767, Loss: 0.0606220867484808, Final Batch Loss: 0.017592886462807655\n",
      "Epoch 2768, Loss: 0.05465207248926163, Final Batch Loss: 0.02874724380671978\n",
      "Epoch 2769, Loss: 0.04883405379951, Final Batch Loss: 0.03091352991759777\n",
      "Epoch 2770, Loss: 0.06171589903533459, Final Batch Loss: 0.023031549528241158\n",
      "Epoch 2771, Loss: 0.06840525567531586, Final Batch Loss: 0.039913516491651535\n",
      "Epoch 2772, Loss: 0.04071702156215906, Final Batch Loss: 0.011051558889448643\n",
      "Epoch 2773, Loss: 0.055539827793836594, Final Batch Loss: 0.02910328470170498\n",
      "Epoch 2774, Loss: 0.03882699366658926, Final Batch Loss: 0.009311477653682232\n",
      "Epoch 2775, Loss: 0.05822490528225899, Final Batch Loss: 0.017893273383378983\n",
      "Epoch 2776, Loss: 0.043996735475957394, Final Batch Loss: 0.011185976676642895\n",
      "Epoch 2777, Loss: 0.05092266947031021, Final Batch Loss: 0.020994598045945168\n",
      "Epoch 2778, Loss: 0.03848639316856861, Final Batch Loss: 0.026522265747189522\n",
      "Epoch 2779, Loss: 0.04693903308361769, Final Batch Loss: 0.008016581647098064\n",
      "Epoch 2780, Loss: 0.08229103125631809, Final Batch Loss: 0.056904129683971405\n",
      "Epoch 2781, Loss: 0.04017018899321556, Final Batch Loss: 0.02298462949693203\n",
      "Epoch 2782, Loss: 0.05111495032906532, Final Batch Loss: 0.022437725216150284\n",
      "Epoch 2783, Loss: 0.06437996216118336, Final Batch Loss: 0.04525882005691528\n",
      "Epoch 2784, Loss: 0.06497185304760933, Final Batch Loss: 0.027526669204235077\n",
      "Epoch 2785, Loss: 0.053593654185533524, Final Batch Loss: 0.016935378313064575\n",
      "Epoch 2786, Loss: 0.08867504447698593, Final Batch Loss: 0.03855367749929428\n",
      "Epoch 2787, Loss: 0.0713375173509121, Final Batch Loss: 0.0371684804558754\n",
      "Epoch 2788, Loss: 0.0460764579474926, Final Batch Loss: 0.018037475645542145\n",
      "Epoch 2789, Loss: 0.07936499640345573, Final Batch Loss: 0.030765492469072342\n",
      "Epoch 2790, Loss: 0.10006202384829521, Final Batch Loss: 0.0677398294210434\n",
      "Epoch 2791, Loss: 0.04601509869098663, Final Batch Loss: 0.0253213569521904\n",
      "Epoch 2792, Loss: 0.03970402851700783, Final Batch Loss: 0.022831913083791733\n",
      "Epoch 2793, Loss: 0.07807516492903233, Final Batch Loss: 0.059685517102479935\n",
      "Epoch 2794, Loss: 0.0729828979820013, Final Batch Loss: 0.02471891976892948\n",
      "Epoch 2795, Loss: 0.09576927125453949, Final Batch Loss: 0.04905816167593002\n",
      "Epoch 2796, Loss: 0.026227193884551525, Final Batch Loss: 0.017630120739340782\n",
      "Epoch 2797, Loss: 0.12420376017689705, Final Batch Loss: 0.04508930817246437\n",
      "Epoch 2798, Loss: 0.06056727655231953, Final Batch Loss: 0.03912229463458061\n",
      "Epoch 2799, Loss: 0.03882407117635012, Final Batch Loss: 0.012851937673985958\n",
      "Epoch 2800, Loss: 0.042163293808698654, Final Batch Loss: 0.0228805523365736\n",
      "Epoch 2801, Loss: 0.10888898372650146, Final Batch Loss: 0.03359188884496689\n",
      "Epoch 2802, Loss: 0.05171586573123932, Final Batch Loss: 0.017659887671470642\n",
      "Epoch 2803, Loss: 0.06262417137622833, Final Batch Loss: 0.01645275577902794\n",
      "Epoch 2804, Loss: 0.05398787185549736, Final Batch Loss: 0.03070814162492752\n",
      "Epoch 2805, Loss: 0.052813539281487465, Final Batch Loss: 0.02627440355718136\n",
      "Epoch 2806, Loss: 0.05289915390312672, Final Batch Loss: 0.02502191811800003\n",
      "Epoch 2807, Loss: 0.04724231734871864, Final Batch Loss: 0.019835112616419792\n",
      "Epoch 2808, Loss: 0.07155604474246502, Final Batch Loss: 0.029243269935250282\n",
      "Epoch 2809, Loss: 0.06794007495045662, Final Batch Loss: 0.031286101788282394\n",
      "Epoch 2810, Loss: 0.051036021672189236, Final Batch Loss: 0.008591718040406704\n",
      "Epoch 2811, Loss: 0.02997648436576128, Final Batch Loss: 0.014511496759951115\n",
      "Epoch 2812, Loss: 0.04733302444219589, Final Batch Loss: 0.0170851182192564\n",
      "Epoch 2813, Loss: 0.031724478118121624, Final Batch Loss: 0.014165378175675869\n",
      "Epoch 2814, Loss: 0.06901678815484047, Final Batch Loss: 0.05173545330762863\n",
      "Epoch 2815, Loss: 0.047258101403713226, Final Batch Loss: 0.02516287751495838\n",
      "Epoch 2816, Loss: 0.14985164254903793, Final Batch Loss: 0.10634800046682358\n",
      "Epoch 2817, Loss: 0.08612806349992752, Final Batch Loss: 0.05516202002763748\n",
      "Epoch 2818, Loss: 0.031220867298543453, Final Batch Loss: 0.01604611612856388\n",
      "Epoch 2819, Loss: 0.07023255527019501, Final Batch Loss: 0.036977704614400864\n",
      "Epoch 2820, Loss: 0.06559320166707039, Final Batch Loss: 0.0426730252802372\n",
      "Epoch 2821, Loss: 0.04031074699014425, Final Batch Loss: 0.025624802336096764\n",
      "Epoch 2822, Loss: 0.07134434208273888, Final Batch Loss: 0.017528966069221497\n",
      "Epoch 2823, Loss: 0.04394391551613808, Final Batch Loss: 0.01900573819875717\n",
      "Epoch 2824, Loss: 0.05672653205692768, Final Batch Loss: 0.021768132224678993\n",
      "Epoch 2825, Loss: 0.027926838025450706, Final Batch Loss: 0.00952153280377388\n",
      "Epoch 2826, Loss: 0.05081749148666859, Final Batch Loss: 0.03504972532391548\n",
      "Epoch 2827, Loss: 0.047784674912691116, Final Batch Loss: 0.023025041446089745\n",
      "Epoch 2828, Loss: 0.05475206859409809, Final Batch Loss: 0.009551694616675377\n",
      "Epoch 2829, Loss: 0.05291854124516249, Final Batch Loss: 0.04152169078588486\n",
      "Epoch 2830, Loss: 0.08330603875219822, Final Batch Loss: 0.02646605484187603\n",
      "Epoch 2831, Loss: 0.09486530721187592, Final Batch Loss: 0.04929664731025696\n",
      "Epoch 2832, Loss: 0.021123975981026888, Final Batch Loss: 0.007723626215010881\n",
      "Epoch 2833, Loss: 0.06398585811257362, Final Batch Loss: 0.03770212456583977\n",
      "Epoch 2834, Loss: 0.11564546078443527, Final Batch Loss: 0.026686839759349823\n",
      "Epoch 2835, Loss: 0.05253206007182598, Final Batch Loss: 0.02673736959695816\n",
      "Epoch 2836, Loss: 0.05353952944278717, Final Batch Loss: 0.021282736212015152\n",
      "Epoch 2837, Loss: 0.07162629254162312, Final Batch Loss: 0.04649809002876282\n",
      "Epoch 2838, Loss: 0.027185853570699692, Final Batch Loss: 0.009448541328310966\n",
      "Epoch 2839, Loss: 0.03570071700960398, Final Batch Loss: 0.021059688180685043\n",
      "Epoch 2840, Loss: 0.05154547654092312, Final Batch Loss: 0.03331344574689865\n",
      "Epoch 2841, Loss: 0.032178218476474285, Final Batch Loss: 0.015237062238156796\n",
      "Epoch 2842, Loss: 0.05353264603763819, Final Batch Loss: 0.041225843131542206\n",
      "Epoch 2843, Loss: 0.08193415775895119, Final Batch Loss: 0.03774869814515114\n",
      "Epoch 2844, Loss: 0.05918457731604576, Final Batch Loss: 0.028196414932608604\n",
      "Epoch 2845, Loss: 0.04906967841088772, Final Batch Loss: 0.016335686668753624\n",
      "Epoch 2846, Loss: 0.05364045687019825, Final Batch Loss: 0.02183656208217144\n",
      "Epoch 2847, Loss: 0.08514338172972202, Final Batch Loss: 0.014856705442070961\n",
      "Epoch 2848, Loss: 0.036554401740431786, Final Batch Loss: 0.016678309068083763\n",
      "Epoch 2849, Loss: 0.042325472459197044, Final Batch Loss: 0.020861530676484108\n",
      "Epoch 2850, Loss: 0.051372699439525604, Final Batch Loss: 0.028151948004961014\n",
      "Epoch 2851, Loss: 0.04106651991605759, Final Batch Loss: 0.015117108821868896\n",
      "Epoch 2852, Loss: 0.044447120279073715, Final Batch Loss: 0.020298074930906296\n",
      "Epoch 2853, Loss: 0.030764613300561905, Final Batch Loss: 0.008874529972672462\n",
      "Epoch 2854, Loss: 0.07296181097626686, Final Batch Loss: 0.03456440195441246\n",
      "Epoch 2855, Loss: 0.03200720530003309, Final Batch Loss: 0.014661324210464954\n",
      "Epoch 2856, Loss: 0.09811314567923546, Final Batch Loss: 0.06522795557975769\n",
      "Epoch 2857, Loss: 0.04863761644810438, Final Batch Loss: 0.014148685149848461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2858, Loss: 0.054187674075365067, Final Batch Loss: 0.03151468187570572\n",
      "Epoch 2859, Loss: 0.10475828312337399, Final Batch Loss: 0.08577968925237656\n",
      "Epoch 2860, Loss: 0.06641481071710587, Final Batch Loss: 0.03542652726173401\n",
      "Epoch 2861, Loss: 0.05403788574039936, Final Batch Loss: 0.02231987752020359\n",
      "Epoch 2862, Loss: 0.07163547538220882, Final Batch Loss: 0.018681267276406288\n",
      "Epoch 2863, Loss: 0.03346789442002773, Final Batch Loss: 0.016474731266498566\n",
      "Epoch 2864, Loss: 0.05517728067934513, Final Batch Loss: 0.02307506464421749\n",
      "Epoch 2865, Loss: 0.06112849339842796, Final Batch Loss: 0.020900383591651917\n",
      "Epoch 2866, Loss: 0.02656694035977125, Final Batch Loss: 0.009216262958943844\n",
      "Epoch 2867, Loss: 0.07550129294395447, Final Batch Loss: 0.04336845129728317\n",
      "Epoch 2868, Loss: 0.04848616570234299, Final Batch Loss: 0.019526993855834007\n",
      "Epoch 2869, Loss: 0.08574690483510494, Final Batch Loss: 0.057248305529356\n",
      "Epoch 2870, Loss: 0.038734374567866325, Final Batch Loss: 0.020025141537189484\n",
      "Epoch 2871, Loss: 0.11916960589587688, Final Batch Loss: 0.020730650052428246\n",
      "Epoch 2872, Loss: 0.03510016668587923, Final Batch Loss: 0.025816218927502632\n",
      "Epoch 2873, Loss: 0.07271607592701912, Final Batch Loss: 0.04796077311038971\n",
      "Epoch 2874, Loss: 0.05224849283695221, Final Batch Loss: 0.02881644479930401\n",
      "Epoch 2875, Loss: 0.023801162838935852, Final Batch Loss: 0.012018642388284206\n",
      "Epoch 2876, Loss: 0.04628300108015537, Final Batch Loss: 0.0210798978805542\n",
      "Epoch 2877, Loss: 0.02781291725113988, Final Batch Loss: 0.007185301277786493\n",
      "Epoch 2878, Loss: 0.054282648488879204, Final Batch Loss: 0.024424061179161072\n",
      "Epoch 2879, Loss: 0.051506271585822105, Final Batch Loss: 0.03393503651022911\n",
      "Epoch 2880, Loss: 0.05251987464725971, Final Batch Loss: 0.03295506164431572\n",
      "Epoch 2881, Loss: 0.13897510059177876, Final Batch Loss: 0.11743465065956116\n",
      "Epoch 2882, Loss: 0.04711205139756203, Final Batch Loss: 0.029053376987576485\n",
      "Epoch 2883, Loss: 0.02650463953614235, Final Batch Loss: 0.014268144965171814\n",
      "Epoch 2884, Loss: 0.050691889598965645, Final Batch Loss: 0.025917068123817444\n",
      "Epoch 2885, Loss: 0.061652299016714096, Final Batch Loss: 0.02790689840912819\n",
      "Epoch 2886, Loss: 0.031001616269350052, Final Batch Loss: 0.010914243757724762\n",
      "Epoch 2887, Loss: 0.1450352966785431, Final Batch Loss: 0.04632282257080078\n",
      "Epoch 2888, Loss: 0.08749288320541382, Final Batch Loss: 0.04246160387992859\n",
      "Epoch 2889, Loss: 0.037032133899629116, Final Batch Loss: 0.004386386834084988\n",
      "Epoch 2890, Loss: 0.034225307404994965, Final Batch Loss: 0.01859426125884056\n",
      "Epoch 2891, Loss: 0.07868640869855881, Final Batch Loss: 0.03659598529338837\n",
      "Epoch 2892, Loss: 0.04158247634768486, Final Batch Loss: 0.02411683090031147\n",
      "Epoch 2893, Loss: 0.0416620597243309, Final Batch Loss: 0.018912455067038536\n",
      "Epoch 2894, Loss: 0.03834119997918606, Final Batch Loss: 0.02115471661090851\n",
      "Epoch 2895, Loss: 0.034126547165215015, Final Batch Loss: 0.013480202294886112\n",
      "Epoch 2896, Loss: 0.054751498624682426, Final Batch Loss: 0.032630324363708496\n",
      "Epoch 2897, Loss: 0.03862254414707422, Final Batch Loss: 0.011357217095792294\n",
      "Epoch 2898, Loss: 0.06797132268548012, Final Batch Loss: 0.03410995379090309\n",
      "Epoch 2899, Loss: 0.027925614267587662, Final Batch Loss: 0.01552588865160942\n",
      "Epoch 2900, Loss: 0.05064953677356243, Final Batch Loss: 0.027692299336194992\n",
      "Epoch 2901, Loss: 0.03242119029164314, Final Batch Loss: 0.01609652303159237\n",
      "Epoch 2902, Loss: 0.08762728795409203, Final Batch Loss: 0.0688847228884697\n",
      "Epoch 2903, Loss: 0.09274498745799065, Final Batch Loss: 0.03120676428079605\n",
      "Epoch 2904, Loss: 0.039004288613796234, Final Batch Loss: 0.02420792728662491\n",
      "Epoch 2905, Loss: 0.03173374105244875, Final Batch Loss: 0.012189907021820545\n",
      "Epoch 2906, Loss: 0.04740218073129654, Final Batch Loss: 0.02907424233853817\n",
      "Epoch 2907, Loss: 0.05220743641257286, Final Batch Loss: 0.03537154570221901\n",
      "Epoch 2908, Loss: 0.07043801993131638, Final Batch Loss: 0.03415881469845772\n",
      "Epoch 2909, Loss: 0.03763396292924881, Final Batch Loss: 0.018957560881972313\n",
      "Epoch 2910, Loss: 0.042132002767175436, Final Batch Loss: 0.007041835691779852\n",
      "Epoch 2911, Loss: 0.0455468837171793, Final Batch Loss: 0.019244736060500145\n",
      "Epoch 2912, Loss: 0.05170588567852974, Final Batch Loss: 0.011203896254301071\n",
      "Epoch 2913, Loss: 0.020059809554368258, Final Batch Loss: 0.013950922526419163\n",
      "Epoch 2914, Loss: 0.05890512280166149, Final Batch Loss: 0.03320743888616562\n",
      "Epoch 2915, Loss: 0.0513743981719017, Final Batch Loss: 0.03390032798051834\n",
      "Epoch 2916, Loss: 0.053050730377435684, Final Batch Loss: 0.030912505462765694\n",
      "Epoch 2917, Loss: 0.027458974160254, Final Batch Loss: 0.010608765296638012\n",
      "Epoch 2918, Loss: 0.05105132516473532, Final Batch Loss: 0.008165533654391766\n",
      "Epoch 2919, Loss: 0.06309903785586357, Final Batch Loss: 0.033213794231414795\n",
      "Epoch 2920, Loss: 0.05346739757806063, Final Batch Loss: 0.009876520372927189\n",
      "Epoch 2921, Loss: 0.04893447086215019, Final Batch Loss: 0.02687263861298561\n",
      "Epoch 2922, Loss: 0.0402403362095356, Final Batch Loss: 0.015767274424433708\n",
      "Epoch 2923, Loss: 0.06768180429935455, Final Batch Loss: 0.04757951200008392\n",
      "Epoch 2924, Loss: 0.08112100698053837, Final Batch Loss: 0.01828891970217228\n",
      "Epoch 2925, Loss: 0.05337238498032093, Final Batch Loss: 0.029030103236436844\n",
      "Epoch 2926, Loss: 0.0745009332895279, Final Batch Loss: 0.05538899451494217\n",
      "Epoch 2927, Loss: 0.04678451269865036, Final Batch Loss: 0.019827932119369507\n",
      "Epoch 2928, Loss: 0.030850094743072987, Final Batch Loss: 0.016288533806800842\n",
      "Epoch 2929, Loss: 0.08998109959065914, Final Batch Loss: 0.06617611646652222\n",
      "Epoch 2930, Loss: 0.044106341898441315, Final Batch Loss: 0.016059909015893936\n",
      "Epoch 2931, Loss: 0.12462995201349258, Final Batch Loss: 0.08971206843852997\n",
      "Epoch 2932, Loss: 0.05975281819701195, Final Batch Loss: 0.03430280461907387\n",
      "Epoch 2933, Loss: 0.10997624136507511, Final Batch Loss: 0.030420856550335884\n",
      "Epoch 2934, Loss: 0.06681638583540916, Final Batch Loss: 0.042636770755052567\n",
      "Epoch 2935, Loss: 0.04332013800740242, Final Batch Loss: 0.021804306656122208\n",
      "Epoch 2936, Loss: 0.10337832942605019, Final Batch Loss: 0.05139993503689766\n",
      "Epoch 2937, Loss: 0.10146028362214565, Final Batch Loss: 0.07886209338903427\n",
      "Epoch 2938, Loss: 0.06178385019302368, Final Batch Loss: 0.036331720650196075\n",
      "Epoch 2939, Loss: 0.14416948705911636, Final Batch Loss: 0.0758156105875969\n",
      "Epoch 2940, Loss: 0.05288490653038025, Final Batch Loss: 0.020530488342046738\n",
      "Epoch 2941, Loss: 0.07962946407496929, Final Batch Loss: 0.023210743442177773\n",
      "Epoch 2942, Loss: 0.06498193927109241, Final Batch Loss: 0.037300657480955124\n",
      "Epoch 2943, Loss: 0.03659121599048376, Final Batch Loss: 0.022404231131076813\n",
      "Epoch 2944, Loss: 0.09960488229990005, Final Batch Loss: 0.04320080205798149\n",
      "Epoch 2945, Loss: 0.04304882697761059, Final Batch Loss: 0.018042705953121185\n",
      "Epoch 2946, Loss: 0.0402021836489439, Final Batch Loss: 0.020652102306485176\n",
      "Epoch 2947, Loss: 0.1285831667482853, Final Batch Loss: 0.09549852460622787\n",
      "Epoch 2948, Loss: 0.07883547991514206, Final Batch Loss: 0.05625755712389946\n",
      "Epoch 2949, Loss: 0.06292048841714859, Final Batch Loss: 0.04091412201523781\n",
      "Epoch 2950, Loss: 0.04482516832649708, Final Batch Loss: 0.016804514452815056\n",
      "Epoch 2951, Loss: 0.04529874678701162, Final Batch Loss: 0.006478096358478069\n",
      "Epoch 2952, Loss: 0.09217282012104988, Final Batch Loss: 0.046460460871458054\n",
      "Epoch 2953, Loss: 0.039410192519426346, Final Batch Loss: 0.02172745205461979\n",
      "Epoch 2954, Loss: 0.08322176337242126, Final Batch Loss: 0.04207373037934303\n",
      "Epoch 2955, Loss: 0.04416867159307003, Final Batch Loss: 0.01684393733739853\n",
      "Epoch 2956, Loss: 0.04947379603981972, Final Batch Loss: 0.016246892511844635\n",
      "Epoch 2957, Loss: 0.0459450650960207, Final Batch Loss: 0.027548564597964287\n",
      "Epoch 2958, Loss: 0.05899224989116192, Final Batch Loss: 0.020873477682471275\n",
      "Epoch 2959, Loss: 0.05307565815746784, Final Batch Loss: 0.025810658931732178\n",
      "Epoch 2960, Loss: 0.048467934131622314, Final Batch Loss: 0.03649062663316727\n",
      "Epoch 2961, Loss: 0.39048420265316963, Final Batch Loss: 0.372946560382843\n",
      "Epoch 2962, Loss: 0.11189230345189571, Final Batch Loss: 0.0859324038028717\n",
      "Epoch 2963, Loss: 0.0356872221454978, Final Batch Loss: 0.008899790234863758\n",
      "Epoch 2964, Loss: 0.037406543269753456, Final Batch Loss: 0.019144006073474884\n",
      "Epoch 2965, Loss: 0.04437212459743023, Final Batch Loss: 0.023806825280189514\n",
      "Epoch 2966, Loss: 0.05241681635379791, Final Batch Loss: 0.01630822941660881\n",
      "Epoch 2967, Loss: 0.045295994728803635, Final Batch Loss: 0.012114934623241425\n",
      "Epoch 2968, Loss: 0.09204994514584541, Final Batch Loss: 0.03902578353881836\n",
      "Epoch 2969, Loss: 0.08102037385106087, Final Batch Loss: 0.027971412986516953\n",
      "Epoch 2970, Loss: 0.05672239698469639, Final Batch Loss: 0.039309125393629074\n",
      "Epoch 2971, Loss: 0.11308962479233742, Final Batch Loss: 0.01632518693804741\n",
      "Epoch 2972, Loss: 0.06660355068743229, Final Batch Loss: 0.03086167387664318\n",
      "Epoch 2973, Loss: 0.0572812519967556, Final Batch Loss: 0.012705016881227493\n",
      "Epoch 2974, Loss: 0.03034121636301279, Final Batch Loss: 0.01085613202303648\n",
      "Epoch 2975, Loss: 0.07920817658305168, Final Batch Loss: 0.041896507143974304\n",
      "Epoch 2976, Loss: 0.037288364954292774, Final Batch Loss: 0.015369621105492115\n",
      "Epoch 2977, Loss: 0.029328497126698494, Final Batch Loss: 0.006807532161474228\n",
      "Epoch 2978, Loss: 0.06013907864689827, Final Batch Loss: 0.035448282957077026\n",
      "Epoch 2979, Loss: 0.054764384403824806, Final Batch Loss: 0.029571713879704475\n",
      "Epoch 2980, Loss: 0.07526415027678013, Final Batch Loss: 0.055975109338760376\n",
      "Epoch 2981, Loss: 0.11246734485030174, Final Batch Loss: 0.02635376527905464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2982, Loss: 0.0734047144651413, Final Batch Loss: 0.05704221501946449\n",
      "Epoch 2983, Loss: 0.07726150006055832, Final Batch Loss: 0.0376947782933712\n",
      "Epoch 2984, Loss: 0.17787454649806023, Final Batch Loss: 0.05667043849825859\n",
      "Epoch 2985, Loss: 0.06632548198103905, Final Batch Loss: 0.033698294311761856\n",
      "Epoch 2986, Loss: 0.03241048101335764, Final Batch Loss: 0.013493859209120274\n",
      "Epoch 2987, Loss: 0.04870859161019325, Final Batch Loss: 0.021499482914805412\n",
      "Epoch 2988, Loss: 0.042620789259672165, Final Batch Loss: 0.01834261789917946\n",
      "Epoch 2989, Loss: 0.1415833868086338, Final Batch Loss: 0.10749907046556473\n",
      "Epoch 2990, Loss: 0.05641883425414562, Final Batch Loss: 0.01683679409325123\n",
      "Epoch 2991, Loss: 0.04989527724683285, Final Batch Loss: 0.025901978835463524\n",
      "Epoch 2992, Loss: 0.07631907984614372, Final Batch Loss: 0.022950008511543274\n",
      "Epoch 2993, Loss: 0.04042107332497835, Final Batch Loss: 0.029940824955701828\n",
      "Epoch 2994, Loss: 0.09514658898115158, Final Batch Loss: 0.029105812311172485\n",
      "Epoch 2995, Loss: 0.04980497620999813, Final Batch Loss: 0.03288334235548973\n",
      "Epoch 2996, Loss: 0.08474424853920937, Final Batch Loss: 0.06359740346670151\n",
      "Epoch 2997, Loss: 0.05622244440019131, Final Batch Loss: 0.020453838631510735\n",
      "Epoch 2998, Loss: 0.1137582790106535, Final Batch Loss: 0.01966634951531887\n",
      "Epoch 2999, Loss: 0.1399591937661171, Final Batch Loss: 0.0331096276640892\n",
      "Epoch 3000, Loss: 0.08001184463500977, Final Batch Loss: 0.031192999333143234\n",
      "Epoch 3001, Loss: 0.05714215710759163, Final Batch Loss: 0.033459167927503586\n",
      "Epoch 3002, Loss: 0.0796635802835226, Final Batch Loss: 0.04931904748082161\n",
      "Epoch 3003, Loss: 0.05778160132467747, Final Batch Loss: 0.026274455711245537\n",
      "Epoch 3004, Loss: 0.0766846053302288, Final Batch Loss: 0.03282420337200165\n",
      "Epoch 3005, Loss: 0.03833659738302231, Final Batch Loss: 0.012674635276198387\n",
      "Epoch 3006, Loss: 0.09302621148526669, Final Batch Loss: 0.014654984697699547\n",
      "Epoch 3007, Loss: 0.047469109296798706, Final Batch Loss: 0.014650598168373108\n",
      "Epoch 3008, Loss: 0.09147829562425613, Final Batch Loss: 0.03210236132144928\n",
      "Epoch 3009, Loss: 0.032124233432114124, Final Batch Loss: 0.01525911595672369\n",
      "Epoch 3010, Loss: 0.0562374833971262, Final Batch Loss: 0.023588189855217934\n",
      "Epoch 3011, Loss: 0.07174503803253174, Final Batch Loss: 0.027210447937250137\n",
      "Epoch 3012, Loss: 0.027231005020439625, Final Batch Loss: 0.01258059311658144\n",
      "Epoch 3013, Loss: 0.0487293004989624, Final Batch Loss: 0.019858933985233307\n",
      "Epoch 3014, Loss: 0.05820925161242485, Final Batch Loss: 0.027153100818395615\n",
      "Epoch 3015, Loss: 0.05733613669872284, Final Batch Loss: 0.013672228902578354\n",
      "Epoch 3016, Loss: 0.04641495645046234, Final Batch Loss: 0.014171216636896133\n",
      "Epoch 3017, Loss: 0.02441137656569481, Final Batch Loss: 0.014261850155889988\n",
      "Epoch 3018, Loss: 0.031113662756979465, Final Batch Loss: 0.02230055257678032\n",
      "Epoch 3019, Loss: 0.039349619299173355, Final Batch Loss: 0.01613672263920307\n",
      "Epoch 3020, Loss: 0.04460893198847771, Final Batch Loss: 0.02128594182431698\n",
      "Epoch 3021, Loss: 0.04341881722211838, Final Batch Loss: 0.02098170481622219\n",
      "Epoch 3022, Loss: 0.09539838507771492, Final Batch Loss: 0.03393523395061493\n",
      "Epoch 3023, Loss: 0.08785541541874409, Final Batch Loss: 0.0225787702947855\n",
      "Epoch 3024, Loss: 0.026498356834053993, Final Batch Loss: 0.008310316130518913\n",
      "Epoch 3025, Loss: 0.055004680529236794, Final Batch Loss: 0.038710080087184906\n",
      "Epoch 3026, Loss: 0.058737942948937416, Final Batch Loss: 0.017450259998440742\n",
      "Epoch 3027, Loss: 0.04889531992375851, Final Batch Loss: 0.027143990620970726\n",
      "Epoch 3028, Loss: 0.09082010481506586, Final Batch Loss: 0.07640375196933746\n",
      "Epoch 3029, Loss: 0.04872697591781616, Final Batch Loss: 0.013391219079494476\n",
      "Epoch 3030, Loss: 0.08443914353847504, Final Batch Loss: 0.03420519456267357\n",
      "Epoch 3031, Loss: 0.043616640381515026, Final Batch Loss: 0.015395824797451496\n",
      "Epoch 3032, Loss: 0.06853084824979305, Final Batch Loss: 0.048292018473148346\n",
      "Epoch 3033, Loss: 0.01733720162883401, Final Batch Loss: 0.009974186308681965\n",
      "Epoch 3034, Loss: 0.06217759661376476, Final Batch Loss: 0.045452818274497986\n",
      "Epoch 3035, Loss: 0.04033086635172367, Final Batch Loss: 0.020247913897037506\n",
      "Epoch 3036, Loss: 0.0747215785086155, Final Batch Loss: 0.039728157222270966\n",
      "Epoch 3037, Loss: 0.044349756091833115, Final Batch Loss: 0.0170576311647892\n",
      "Epoch 3038, Loss: 0.04181530140340328, Final Batch Loss: 0.024235384538769722\n",
      "Epoch 3039, Loss: 0.09552232548594475, Final Batch Loss: 0.030346523970365524\n",
      "Epoch 3040, Loss: 0.04675404727458954, Final Batch Loss: 0.02121598832309246\n",
      "Epoch 3041, Loss: 0.043127516284585, Final Batch Loss: 0.013420922681689262\n",
      "Epoch 3042, Loss: 0.08389218337833881, Final Batch Loss: 0.07129552960395813\n",
      "Epoch 3043, Loss: 0.07417831942439079, Final Batch Loss: 0.042025402188301086\n",
      "Epoch 3044, Loss: 0.036777084693312645, Final Batch Loss: 0.02401048131287098\n",
      "Epoch 3045, Loss: 0.04716818779706955, Final Batch Loss: 0.020003655925393105\n",
      "Epoch 3046, Loss: 0.03397613298147917, Final Batch Loss: 0.022723719477653503\n",
      "Epoch 3047, Loss: 0.044662049040198326, Final Batch Loss: 0.010488869622349739\n",
      "Epoch 3048, Loss: 0.032180032692849636, Final Batch Loss: 0.014540067873895168\n",
      "Epoch 3049, Loss: 0.10835477150976658, Final Batch Loss: 0.07992277294397354\n",
      "Epoch 3050, Loss: 0.04350409656763077, Final Batch Loss: 0.008157789707183838\n",
      "Epoch 3051, Loss: 0.04363132733851671, Final Batch Loss: 0.013056795112788677\n",
      "Epoch 3052, Loss: 0.045410857535898685, Final Batch Loss: 0.012716085650026798\n",
      "Epoch 3053, Loss: 0.03492062631994486, Final Batch Loss: 0.019694121554493904\n",
      "Epoch 3054, Loss: 0.03329716995358467, Final Batch Loss: 0.014339098706841469\n",
      "Epoch 3055, Loss: 0.036230257246643305, Final Batch Loss: 0.006651984993368387\n",
      "Epoch 3056, Loss: 0.02544997911900282, Final Batch Loss: 0.008813680149614811\n",
      "Epoch 3057, Loss: 0.037348490208387375, Final Batch Loss: 0.019727641716599464\n",
      "Epoch 3058, Loss: 0.025022385641932487, Final Batch Loss: 0.013298749923706055\n",
      "Epoch 3059, Loss: 0.03326991852372885, Final Batch Loss: 0.02322838082909584\n",
      "Epoch 3060, Loss: 0.039162881672382355, Final Batch Loss: 0.03037124313414097\n",
      "Epoch 3061, Loss: 0.024335755966603756, Final Batch Loss: 0.007614814676344395\n",
      "Epoch 3062, Loss: 0.03664914611726999, Final Batch Loss: 0.024034911766648293\n",
      "Epoch 3063, Loss: 0.06818808056414127, Final Batch Loss: 0.057562455534935\n",
      "Epoch 3064, Loss: 0.04405350610613823, Final Batch Loss: 0.03500097990036011\n",
      "Epoch 3065, Loss: 0.024370760656893253, Final Batch Loss: 0.0065153902396559715\n",
      "Epoch 3066, Loss: 0.03592818323522806, Final Batch Loss: 0.022976525127887726\n",
      "Epoch 3067, Loss: 0.030489278491586447, Final Batch Loss: 0.02396712265908718\n",
      "Epoch 3068, Loss: 0.0439368998631835, Final Batch Loss: 0.03433220833539963\n",
      "Epoch 3069, Loss: 0.05056650936603546, Final Batch Loss: 0.028349293395876884\n",
      "Epoch 3070, Loss: 0.060521092265844345, Final Batch Loss: 0.03579391911625862\n",
      "Epoch 3071, Loss: 0.04691061284393072, Final Batch Loss: 0.03304916247725487\n",
      "Epoch 3072, Loss: 0.0649655181914568, Final Batch Loss: 0.022619357332587242\n",
      "Epoch 3073, Loss: 0.04465243313461542, Final Batch Loss: 0.034748632460832596\n",
      "Epoch 3074, Loss: 0.037367066368460655, Final Batch Loss: 0.025732997804880142\n",
      "Epoch 3075, Loss: 0.09024626947939396, Final Batch Loss: 0.06092377007007599\n",
      "Epoch 3076, Loss: 0.04426195565611124, Final Batch Loss: 0.03470126911997795\n",
      "Epoch 3077, Loss: 0.039205520413815975, Final Batch Loss: 0.029348237439990044\n",
      "Epoch 3078, Loss: 0.03145419154316187, Final Batch Loss: 0.0052400017157197\n",
      "Epoch 3079, Loss: 0.046265059150755405, Final Batch Loss: 0.032914899289608\n",
      "Epoch 3080, Loss: 0.02937215566635132, Final Batch Loss: 0.0068814717233181\n",
      "Epoch 3081, Loss: 0.08822545222938061, Final Batch Loss: 0.06306061893701553\n",
      "Epoch 3082, Loss: 0.07349605113267899, Final Batch Loss: 0.047123983502388\n",
      "Epoch 3083, Loss: 0.040491306222975254, Final Batch Loss: 0.01455859374254942\n",
      "Epoch 3084, Loss: 0.0669769998639822, Final Batch Loss: 0.04752516746520996\n",
      "Epoch 3085, Loss: 0.06684233620762825, Final Batch Loss: 0.04840085655450821\n",
      "Epoch 3086, Loss: 0.04240862186998129, Final Batch Loss: 0.03046374022960663\n",
      "Epoch 3087, Loss: 0.05224011838436127, Final Batch Loss: 0.02687460370361805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3088, Loss: 0.043685633689165115, Final Batch Loss: 0.01860046572983265\n",
      "Epoch 3089, Loss: 0.0288376621901989, Final Batch Loss: 0.009257882833480835\n",
      "Epoch 3090, Loss: 0.07092451676726341, Final Batch Loss: 0.03960666060447693\n",
      "Epoch 3091, Loss: 0.022868488915264606, Final Batch Loss: 0.011866850778460503\n",
      "Epoch 3092, Loss: 0.040345435962080956, Final Batch Loss: 0.03173861652612686\n",
      "Epoch 3093, Loss: 0.02995483297854662, Final Batch Loss: 0.016454651951789856\n",
      "Epoch 3094, Loss: 0.06871390342712402, Final Batch Loss: 0.057869117707014084\n",
      "Epoch 3095, Loss: 0.028882184997200966, Final Batch Loss: 0.013484797440469265\n",
      "Epoch 3096, Loss: 0.03480662405490875, Final Batch Loss: 0.015885043889284134\n",
      "Epoch 3097, Loss: 0.106122687458992, Final Batch Loss: 0.05391308665275574\n",
      "Epoch 3098, Loss: 0.03628355637192726, Final Batch Loss: 0.009285921230912209\n",
      "Epoch 3099, Loss: 0.07764787971973419, Final Batch Loss: 0.03683769702911377\n",
      "Epoch 3100, Loss: 0.05244952626526356, Final Batch Loss: 0.03281935304403305\n",
      "Epoch 3101, Loss: 0.030917735304683447, Final Batch Loss: 0.0039700972847640514\n",
      "Epoch 3102, Loss: 0.025025636423379183, Final Batch Loss: 0.007740188855677843\n",
      "Epoch 3103, Loss: 0.05172612890601158, Final Batch Loss: 0.040903490036726\n",
      "Epoch 3104, Loss: 0.05242118192836642, Final Batch Loss: 0.005468431394547224\n",
      "Epoch 3105, Loss: 0.06053607724606991, Final Batch Loss: 0.028063172474503517\n",
      "Epoch 3106, Loss: 0.0716005451977253, Final Batch Loss: 0.03823302686214447\n",
      "Epoch 3107, Loss: 0.04100118111819029, Final Batch Loss: 0.01026639249175787\n",
      "Epoch 3108, Loss: 0.035740187391638756, Final Batch Loss: 0.022786393761634827\n",
      "Epoch 3109, Loss: 0.026161317713558674, Final Batch Loss: 0.01208700705319643\n",
      "Epoch 3110, Loss: 0.06841808930039406, Final Batch Loss: 0.046327270567417145\n",
      "Epoch 3111, Loss: 0.03872706741094589, Final Batch Loss: 0.008524306118488312\n",
      "Epoch 3112, Loss: 0.03279801458120346, Final Batch Loss: 0.012107564136385918\n",
      "Epoch 3113, Loss: 0.061082180589437485, Final Batch Loss: 0.016739103943109512\n",
      "Epoch 3114, Loss: 0.02587498351931572, Final Batch Loss: 0.016469363123178482\n",
      "Epoch 3115, Loss: 0.04007558524608612, Final Batch Loss: 0.017534606158733368\n",
      "Epoch 3116, Loss: 0.03995254635810852, Final Batch Loss: 0.02774190343916416\n",
      "Epoch 3117, Loss: 0.02976047992706299, Final Batch Loss: 0.015530302189290524\n",
      "Epoch 3118, Loss: 0.042944302782416344, Final Batch Loss: 0.021236199885606766\n",
      "Epoch 3119, Loss: 0.12694350257515907, Final Batch Loss: 0.09861337393522263\n",
      "Epoch 3120, Loss: 0.047886770218610764, Final Batch Loss: 0.024012284353375435\n",
      "Epoch 3121, Loss: 0.05077774077653885, Final Batch Loss: 0.03831043094396591\n",
      "Epoch 3122, Loss: 0.0317309508100152, Final Batch Loss: 0.02272539772093296\n",
      "Epoch 3123, Loss: 0.0944222304970026, Final Batch Loss: 0.06766410171985626\n",
      "Epoch 3124, Loss: 0.06514054536819458, Final Batch Loss: 0.020434018224477768\n",
      "Epoch 3125, Loss: 0.02838580682873726, Final Batch Loss: 0.014392450451850891\n",
      "Epoch 3126, Loss: 0.14221781119704247, Final Batch Loss: 0.12821057438850403\n",
      "Epoch 3127, Loss: 0.028647206723690033, Final Batch Loss: 0.020723707973957062\n",
      "Epoch 3128, Loss: 0.045051248744130135, Final Batch Loss: 0.03619462624192238\n",
      "Epoch 3129, Loss: 0.07390984147787094, Final Batch Loss: 0.03582994267344475\n",
      "Epoch 3130, Loss: 0.04695327579975128, Final Batch Loss: 0.026442350819706917\n",
      "Epoch 3131, Loss: 0.09064109902828932, Final Batch Loss: 0.07643749564886093\n",
      "Epoch 3132, Loss: 0.02955907490104437, Final Batch Loss: 0.014227132312953472\n",
      "Epoch 3133, Loss: 0.0937974825501442, Final Batch Loss: 0.05171975865960121\n",
      "Epoch 3134, Loss: 0.08152424544095993, Final Batch Loss: 0.05734383687376976\n",
      "Epoch 3135, Loss: 0.049013527110219, Final Batch Loss: 0.012961914762854576\n",
      "Epoch 3136, Loss: 0.06559875048696995, Final Batch Loss: 0.028461551293730736\n",
      "Epoch 3137, Loss: 0.03198553528636694, Final Batch Loss: 0.019292069599032402\n",
      "Epoch 3138, Loss: 0.08876620605587959, Final Batch Loss: 0.07167667895555496\n",
      "Epoch 3139, Loss: 0.031120585277676582, Final Batch Loss: 0.016213756054639816\n",
      "Epoch 3140, Loss: 0.07849440909922123, Final Batch Loss: 0.06043122708797455\n",
      "Epoch 3141, Loss: 0.048736995086073875, Final Batch Loss: 0.017236610874533653\n",
      "Epoch 3142, Loss: 0.047292678616940975, Final Batch Loss: 0.03171957656741142\n",
      "Epoch 3143, Loss: 0.02959539368748665, Final Batch Loss: 0.011506659910082817\n",
      "Epoch 3144, Loss: 0.037600308656692505, Final Batch Loss: 0.02316560596227646\n",
      "Epoch 3145, Loss: 0.03209376521408558, Final Batch Loss: 0.012879928573966026\n",
      "Epoch 3146, Loss: 0.04431515233591199, Final Batch Loss: 0.007640738505870104\n",
      "Epoch 3147, Loss: 0.045669760555028915, Final Batch Loss: 0.008547835052013397\n",
      "Epoch 3148, Loss: 0.04799939878284931, Final Batch Loss: 0.017361143603920937\n",
      "Epoch 3149, Loss: 0.03591617941856384, Final Batch Loss: 0.017454789951443672\n",
      "Epoch 3150, Loss: 0.0924636758863926, Final Batch Loss: 0.055493444204330444\n",
      "Epoch 3151, Loss: 0.05275602079927921, Final Batch Loss: 0.014877045527100563\n",
      "Epoch 3152, Loss: 0.04692344181239605, Final Batch Loss: 0.02397419884800911\n",
      "Epoch 3153, Loss: 0.0552915520966053, Final Batch Loss: 0.034891773015260696\n",
      "Epoch 3154, Loss: 0.03954589180648327, Final Batch Loss: 0.01671375334262848\n",
      "Epoch 3155, Loss: 0.058684201911091805, Final Batch Loss: 0.038948096334934235\n",
      "Epoch 3156, Loss: 0.0352510716766119, Final Batch Loss: 0.008973712101578712\n",
      "Epoch 3157, Loss: 0.049781003035604954, Final Batch Loss: 0.011271324940025806\n",
      "Epoch 3158, Loss: 0.043452681973576546, Final Batch Loss: 0.023815060034394264\n",
      "Epoch 3159, Loss: 0.058528631925582886, Final Batch Loss: 0.01908758282661438\n",
      "Epoch 3160, Loss: 0.05007350072264671, Final Batch Loss: 0.014016468077898026\n",
      "Epoch 3161, Loss: 0.04399646446108818, Final Batch Loss: 0.034327659755945206\n",
      "Epoch 3162, Loss: 0.06898865289986134, Final Batch Loss: 0.049662280827760696\n",
      "Epoch 3163, Loss: 0.03236778639256954, Final Batch Loss: 0.014460563659667969\n",
      "Epoch 3164, Loss: 0.02625882625579834, Final Batch Loss: 0.014975241385400295\n",
      "Epoch 3165, Loss: 0.027382180094718933, Final Batch Loss: 0.013992018066346645\n",
      "Epoch 3166, Loss: 0.03914554510265589, Final Batch Loss: 0.013872562907636166\n",
      "Epoch 3167, Loss: 0.040143512189388275, Final Batch Loss: 0.022708866745233536\n",
      "Epoch 3168, Loss: 0.06853917520493269, Final Batch Loss: 0.06134633347392082\n",
      "Epoch 3169, Loss: 0.02136898785829544, Final Batch Loss: 0.011718966998159885\n",
      "Epoch 3170, Loss: 0.03285248018801212, Final Batch Loss: 0.024537228047847748\n",
      "Epoch 3171, Loss: 0.0369437150657177, Final Batch Loss: 0.013543576002120972\n",
      "Epoch 3172, Loss: 0.034530216827988625, Final Batch Loss: 0.01559758372604847\n",
      "Epoch 3173, Loss: 0.021599766332656145, Final Batch Loss: 0.00645245099440217\n",
      "Epoch 3174, Loss: 0.07008670456707478, Final Batch Loss: 0.057361673563718796\n",
      "Epoch 3175, Loss: 0.06623809039592743, Final Batch Loss: 0.04092378914356232\n",
      "Epoch 3176, Loss: 0.024360029958188534, Final Batch Loss: 0.008148272521793842\n",
      "Epoch 3177, Loss: 0.02678336203098297, Final Batch Loss: 0.011874804273247719\n",
      "Epoch 3178, Loss: 0.06670648790895939, Final Batch Loss: 0.04703167825937271\n",
      "Epoch 3179, Loss: 0.031539732590317726, Final Batch Loss: 0.01749114878475666\n",
      "Epoch 3180, Loss: 0.02512410841882229, Final Batch Loss: 0.008152563124895096\n",
      "Epoch 3181, Loss: 0.0441775768995285, Final Batch Loss: 0.022308455780148506\n",
      "Epoch 3182, Loss: 0.054283736273646355, Final Batch Loss: 0.026033317670226097\n",
      "Epoch 3183, Loss: 0.015437858179211617, Final Batch Loss: 0.006844300776720047\n",
      "Epoch 3184, Loss: 0.05693524703383446, Final Batch Loss: 0.03449179604649544\n",
      "Epoch 3185, Loss: 0.06298673711717129, Final Batch Loss: 0.01690847985446453\n",
      "Epoch 3186, Loss: 0.029579520225524902, Final Batch Loss: 0.014817571267485619\n",
      "Epoch 3187, Loss: 0.045383382588624954, Final Batch Loss: 0.03167589008808136\n",
      "Epoch 3188, Loss: 0.031768111512064934, Final Batch Loss: 0.017036566510796547\n",
      "Epoch 3189, Loss: 0.07556666247546673, Final Batch Loss: 0.01869695447385311\n",
      "Epoch 3190, Loss: 0.03199400007724762, Final Batch Loss: 0.007630109786987305\n",
      "Epoch 3191, Loss: 0.021219936199486256, Final Batch Loss: 0.008434981107711792\n",
      "Epoch 3192, Loss: 0.023963364772498608, Final Batch Loss: 0.013366136699914932\n",
      "Epoch 3193, Loss: 0.04816710576415062, Final Batch Loss: 0.03562478721141815\n",
      "Epoch 3194, Loss: 0.05237124115228653, Final Batch Loss: 0.03155750408768654\n",
      "Epoch 3195, Loss: 0.030815886333584785, Final Batch Loss: 0.0217983927577734\n",
      "Epoch 3196, Loss: 0.07775567844510078, Final Batch Loss: 0.06398056447505951\n",
      "Epoch 3197, Loss: 0.07024747133255005, Final Batch Loss: 0.0538463220000267\n",
      "Epoch 3198, Loss: 0.13533767871558666, Final Batch Loss: 0.11067501455545425\n",
      "Epoch 3199, Loss: 0.019320913590490818, Final Batch Loss: 0.009228871203958988\n",
      "Epoch 3200, Loss: 0.03917707875370979, Final Batch Loss: 0.01657174900174141\n",
      "Epoch 3201, Loss: 0.04551066271960735, Final Batch Loss: 0.029480280354619026\n",
      "Epoch 3202, Loss: 0.06520810164511204, Final Batch Loss: 0.04172820597887039\n",
      "Epoch 3203, Loss: 0.072597810998559, Final Batch Loss: 0.022325655445456505\n",
      "Epoch 3204, Loss: 0.0435638464987278, Final Batch Loss: 0.02706317976117134\n",
      "Epoch 3205, Loss: 0.02784185577183962, Final Batch Loss: 0.017470214515924454\n",
      "Epoch 3206, Loss: 0.03625041898339987, Final Batch Loss: 0.00577975157648325\n",
      "Epoch 3207, Loss: 0.038228338584303856, Final Batch Loss: 0.01856154389679432\n",
      "Epoch 3208, Loss: 0.020895933732390404, Final Batch Loss: 0.011683846823871136\n",
      "Epoch 3209, Loss: 0.04013344645500183, Final Batch Loss: 0.028400525450706482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3210, Loss: 0.047779329121112823, Final Batch Loss: 0.014232195913791656\n",
      "Epoch 3211, Loss: 0.030468068085610867, Final Batch Loss: 0.017199592664837837\n",
      "Epoch 3212, Loss: 0.019883624278008938, Final Batch Loss: 0.008524567820131779\n",
      "Epoch 3213, Loss: 0.03594059869647026, Final Batch Loss: 0.019049914553761482\n",
      "Epoch 3214, Loss: 0.03660734184086323, Final Batch Loss: 0.025554314255714417\n",
      "Epoch 3215, Loss: 0.08200323209166527, Final Batch Loss: 0.026444505900144577\n",
      "Epoch 3216, Loss: 0.023800737224519253, Final Batch Loss: 0.006500854156911373\n",
      "Epoch 3217, Loss: 0.06696168705821037, Final Batch Loss: 0.04709406942129135\n",
      "Epoch 3218, Loss: 0.07347762398421764, Final Batch Loss: 0.017340967431664467\n",
      "Epoch 3219, Loss: 0.06233644671738148, Final Batch Loss: 0.03064514510333538\n",
      "Epoch 3220, Loss: 0.03479092940688133, Final Batch Loss: 0.019211649894714355\n",
      "Epoch 3221, Loss: 0.03737336490303278, Final Batch Loss: 0.009474902413785458\n",
      "Epoch 3222, Loss: 0.04152429988607764, Final Batch Loss: 0.005260826554149389\n",
      "Epoch 3223, Loss: 0.019361194223165512, Final Batch Loss: 0.008538438007235527\n",
      "Epoch 3224, Loss: 0.05894342437386513, Final Batch Loss: 0.03583848103880882\n",
      "Epoch 3225, Loss: 0.03161347098648548, Final Batch Loss: 0.021634699776768684\n",
      "Epoch 3226, Loss: 0.029553888365626335, Final Batch Loss: 0.013340162113308907\n",
      "Epoch 3227, Loss: 0.035466530825942755, Final Batch Loss: 0.0034403218887746334\n",
      "Epoch 3228, Loss: 0.03344044275581837, Final Batch Loss: 0.00846727006137371\n",
      "Epoch 3229, Loss: 0.04011551942676306, Final Batch Loss: 0.011165420524775982\n",
      "Epoch 3230, Loss: 0.08209034241735935, Final Batch Loss: 0.061707839369773865\n",
      "Epoch 3231, Loss: 0.062190307304263115, Final Batch Loss: 0.034307681024074554\n",
      "Epoch 3232, Loss: 0.05648753419518471, Final Batch Loss: 0.03601105138659477\n",
      "Epoch 3233, Loss: 0.05669083259999752, Final Batch Loss: 0.030384575948119164\n",
      "Epoch 3234, Loss: 0.039135235361754894, Final Batch Loss: 0.026212500408291817\n",
      "Epoch 3235, Loss: 0.05531799513846636, Final Batch Loss: 0.01389374677091837\n",
      "Epoch 3236, Loss: 0.034943615552037954, Final Batch Loss: 0.005290697794407606\n",
      "Epoch 3237, Loss: 0.12495424225926399, Final Batch Loss: 0.10806804895401001\n",
      "Epoch 3238, Loss: 0.041120195761322975, Final Batch Loss: 0.013022489845752716\n",
      "Epoch 3239, Loss: 0.040277812629938126, Final Batch Loss: 0.01667056418955326\n",
      "Epoch 3240, Loss: 0.03132138308137655, Final Batch Loss: 0.01753360591828823\n",
      "Epoch 3241, Loss: 0.04901524446904659, Final Batch Loss: 0.040051765739917755\n",
      "Epoch 3242, Loss: 0.03459962457418442, Final Batch Loss: 0.015810459852218628\n",
      "Epoch 3243, Loss: 0.052357522770762444, Final Batch Loss: 0.037998054176568985\n",
      "Epoch 3244, Loss: 0.05121275596320629, Final Batch Loss: 0.018550196662545204\n",
      "Epoch 3245, Loss: 0.06069091334939003, Final Batch Loss: 0.046322792768478394\n",
      "Epoch 3246, Loss: 0.03657491598278284, Final Batch Loss: 0.024266427382826805\n",
      "Epoch 3247, Loss: 0.04161289241164923, Final Batch Loss: 0.012715858407318592\n",
      "Epoch 3248, Loss: 0.024172481149435043, Final Batch Loss: 0.00875262264162302\n",
      "Epoch 3249, Loss: 0.052736952900886536, Final Batch Loss: 0.014771800488233566\n",
      "Epoch 3250, Loss: 0.06269720755517483, Final Batch Loss: 0.0461055152118206\n",
      "Epoch 3251, Loss: 0.04870534501969814, Final Batch Loss: 0.029521305114030838\n",
      "Epoch 3252, Loss: 0.032312254421412945, Final Batch Loss: 0.017306778579950333\n",
      "Epoch 3253, Loss: 0.09800169244408607, Final Batch Loss: 0.022475387901067734\n",
      "Epoch 3254, Loss: 0.05828750692307949, Final Batch Loss: 0.022014884278178215\n",
      "Epoch 3255, Loss: 0.04351291339844465, Final Batch Loss: 0.014089413918554783\n",
      "Epoch 3256, Loss: 0.02787293866276741, Final Batch Loss: 0.014075394719839096\n",
      "Epoch 3257, Loss: 0.037969985976815224, Final Batch Loss: 0.022436343133449554\n",
      "Epoch 3258, Loss: 0.07180966809391975, Final Batch Loss: 0.05112045630812645\n",
      "Epoch 3259, Loss: 0.05424960516393185, Final Batch Loss: 0.014816580340266228\n",
      "Epoch 3260, Loss: 0.0861600749194622, Final Batch Loss: 0.04275384917855263\n",
      "Epoch 3261, Loss: 0.025437920354306698, Final Batch Loss: 0.013699522241950035\n",
      "Epoch 3262, Loss: 0.013687866739928722, Final Batch Loss: 0.008709775283932686\n",
      "Epoch 3263, Loss: 0.060831066220998764, Final Batch Loss: 0.026362966746091843\n",
      "Epoch 3264, Loss: 0.04819251783192158, Final Batch Loss: 0.02615005150437355\n",
      "Epoch 3265, Loss: 0.06792745180428028, Final Batch Loss: 0.05030280724167824\n",
      "Epoch 3266, Loss: 0.06389104761183262, Final Batch Loss: 0.012552859261631966\n",
      "Epoch 3267, Loss: 0.014101222157478333, Final Batch Loss: 0.007778542581945658\n",
      "Epoch 3268, Loss: 0.07137302588671446, Final Batch Loss: 0.008181917481124401\n",
      "Epoch 3269, Loss: 0.01805182220414281, Final Batch Loss: 0.005143953021615744\n",
      "Epoch 3270, Loss: 0.03151424741372466, Final Batch Loss: 0.005051541607826948\n",
      "Epoch 3271, Loss: 0.05461556278169155, Final Batch Loss: 0.03160768747329712\n",
      "Epoch 3272, Loss: 0.07790902629494667, Final Batch Loss: 0.029736332595348358\n",
      "Epoch 3273, Loss: 0.0557367242872715, Final Batch Loss: 0.04004121944308281\n",
      "Epoch 3274, Loss: 0.06610587053000927, Final Batch Loss: 0.01278294064104557\n",
      "Epoch 3275, Loss: 0.08991158381104469, Final Batch Loss: 0.060640618205070496\n",
      "Epoch 3276, Loss: 0.03780199773609638, Final Batch Loss: 0.019676951691508293\n",
      "Epoch 3277, Loss: 0.0668235756456852, Final Batch Loss: 0.04428064078092575\n",
      "Epoch 3278, Loss: 0.034876407124102116, Final Batch Loss: 0.015498961322009563\n",
      "Epoch 3279, Loss: 0.10605695098638535, Final Batch Loss: 0.038352929055690765\n",
      "Epoch 3280, Loss: 0.04669155366718769, Final Batch Loss: 0.011026030406355858\n",
      "Epoch 3281, Loss: 0.0572945661842823, Final Batch Loss: 0.023721247911453247\n",
      "Epoch 3282, Loss: 0.0808156281709671, Final Batch Loss: 0.04615800455212593\n",
      "Epoch 3283, Loss: 0.07450252678245306, Final Batch Loss: 0.012330754660069942\n",
      "Epoch 3284, Loss: 0.10123943910002708, Final Batch Loss: 0.06174244359135628\n",
      "Epoch 3285, Loss: 0.029121021274477243, Final Batch Loss: 0.005715020466595888\n",
      "Epoch 3286, Loss: 0.07278857752680779, Final Batch Loss: 0.03997648134827614\n",
      "Epoch 3287, Loss: 0.11384779214859009, Final Batch Loss: 0.032257720828056335\n",
      "Epoch 3288, Loss: 0.08269555866718292, Final Batch Loss: 0.02122744545340538\n",
      "Epoch 3289, Loss: 0.05874646455049515, Final Batch Loss: 0.03634005784988403\n",
      "Epoch 3290, Loss: 0.051290880888700485, Final Batch Loss: 0.026930106803774834\n",
      "Epoch 3291, Loss: 0.1006423719227314, Final Batch Loss: 0.06841400265693665\n",
      "Epoch 3292, Loss: 0.07985170930624008, Final Batch Loss: 0.049166239798069\n",
      "Epoch 3293, Loss: 0.05507970880717039, Final Batch Loss: 0.04159003123641014\n",
      "Epoch 3294, Loss: 0.07363987900316715, Final Batch Loss: 0.0226582158356905\n",
      "Epoch 3295, Loss: 0.08088402450084686, Final Batch Loss: 0.03537728264927864\n",
      "Epoch 3296, Loss: 0.02687693014740944, Final Batch Loss: 0.01175886020064354\n",
      "Epoch 3297, Loss: 0.0375377107411623, Final Batch Loss: 0.01689515821635723\n",
      "Epoch 3298, Loss: 0.05893509183079004, Final Batch Loss: 0.008233797736465931\n",
      "Epoch 3299, Loss: 0.044261764734983444, Final Batch Loss: 0.01840927079319954\n",
      "Epoch 3300, Loss: 0.08168035000562668, Final Batch Loss: 0.04779591038823128\n",
      "Epoch 3301, Loss: 0.05918503552675247, Final Batch Loss: 0.026674557477235794\n",
      "Epoch 3302, Loss: 0.08157672174274921, Final Batch Loss: 0.06078047305345535\n",
      "Epoch 3303, Loss: 0.026383711956441402, Final Batch Loss: 0.009611829183995724\n",
      "Epoch 3304, Loss: 0.04281016066670418, Final Batch Loss: 0.025604061782360077\n",
      "Epoch 3305, Loss: 0.035431149415671825, Final Batch Loss: 0.02354245074093342\n",
      "Epoch 3306, Loss: 0.08686089888215065, Final Batch Loss: 0.03412030637264252\n",
      "Epoch 3307, Loss: 0.04188199155032635, Final Batch Loss: 0.02660547010600567\n",
      "Epoch 3308, Loss: 0.05904747173190117, Final Batch Loss: 0.024875741451978683\n",
      "Epoch 3309, Loss: 0.026395056396722794, Final Batch Loss: 0.010563787072896957\n",
      "Epoch 3310, Loss: 0.044266754761338234, Final Batch Loss: 0.017784923315048218\n",
      "Epoch 3311, Loss: 0.15114514995366335, Final Batch Loss: 0.13786885142326355\n",
      "Epoch 3312, Loss: 0.05645544081926346, Final Batch Loss: 0.03008468635380268\n",
      "Epoch 3313, Loss: 0.11675096675753593, Final Batch Loss: 0.06076450273394585\n",
      "Epoch 3314, Loss: 0.1281231977045536, Final Batch Loss: 0.06983896344900131\n",
      "Epoch 3315, Loss: 0.025585342198610306, Final Batch Loss: 0.007269063964486122\n",
      "Epoch 3316, Loss: 0.04606683552265167, Final Batch Loss: 0.03161442652344704\n",
      "Epoch 3317, Loss: 0.020374870859086514, Final Batch Loss: 0.007084720768034458\n",
      "Epoch 3318, Loss: 0.07071155961602926, Final Batch Loss: 0.014398246072232723\n",
      "Epoch 3319, Loss: 0.0370789710432291, Final Batch Loss: 0.026809295639395714\n",
      "Epoch 3320, Loss: 0.024219105951488018, Final Batch Loss: 0.012898796238005161\n",
      "Epoch 3321, Loss: 0.0530204251408577, Final Batch Loss: 0.026753515005111694\n",
      "Epoch 3322, Loss: 0.039660919457674026, Final Batch Loss: 0.011570408940315247\n",
      "Epoch 3323, Loss: 0.08760982379317284, Final Batch Loss: 0.04254060238599777\n",
      "Epoch 3324, Loss: 0.03669908083975315, Final Batch Loss: 0.020115628838539124\n",
      "Epoch 3325, Loss: 0.04513114504516125, Final Batch Loss: 0.0332089401781559\n",
      "Epoch 3326, Loss: 0.026855170726776123, Final Batch Loss: 0.014040891081094742\n",
      "Epoch 3327, Loss: 0.04739222303032875, Final Batch Loss: 0.009042341262102127\n",
      "Epoch 3328, Loss: 0.04787634685635567, Final Batch Loss: 0.02917572297155857\n",
      "Epoch 3329, Loss: 0.045484659262001514, Final Batch Loss: 0.03731940686702728\n",
      "Epoch 3330, Loss: 0.04402455221861601, Final Batch Loss: 0.03415895998477936\n",
      "Epoch 3331, Loss: 0.07233497127890587, Final Batch Loss: 0.05170271918177605\n",
      "Epoch 3332, Loss: 0.042383276391774416, Final Batch Loss: 0.035594772547483444\n",
      "Epoch 3333, Loss: 0.07999521866440773, Final Batch Loss: 0.04388634115457535\n",
      "Epoch 3334, Loss: 0.10738525912165642, Final Batch Loss: 0.04778103902935982\n",
      "Epoch 3335, Loss: 0.031096196733415127, Final Batch Loss: 0.0206656064838171\n",
      "Epoch 3336, Loss: 0.06679557077586651, Final Batch Loss: 0.048665862530469894\n",
      "Epoch 3337, Loss: 0.04772256687283516, Final Batch Loss: 0.025034256279468536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3338, Loss: 0.0720294751226902, Final Batch Loss: 0.03207109123468399\n",
      "Epoch 3339, Loss: 0.058151816949248314, Final Batch Loss: 0.03728500381112099\n",
      "Epoch 3340, Loss: 0.054387450218200684, Final Batch Loss: 0.03658487647771835\n",
      "Epoch 3341, Loss: 0.04442963469773531, Final Batch Loss: 0.015052900649607182\n",
      "Epoch 3342, Loss: 0.04146809596568346, Final Batch Loss: 0.03141598403453827\n",
      "Epoch 3343, Loss: 0.06156376004219055, Final Batch Loss: 0.009923629462718964\n",
      "Epoch 3344, Loss: 0.0798410139977932, Final Batch Loss: 0.05195327475667\n",
      "Epoch 3345, Loss: 0.12290892004966736, Final Batch Loss: 0.10326450318098068\n",
      "Epoch 3346, Loss: 0.07790493220090866, Final Batch Loss: 0.035876236855983734\n",
      "Epoch 3347, Loss: 0.054332987405359745, Final Batch Loss: 0.04000074416399002\n",
      "Epoch 3348, Loss: 0.10584096610546112, Final Batch Loss: 0.03829260170459747\n",
      "Epoch 3349, Loss: 0.10830092430114746, Final Batch Loss: 0.09238340705633163\n",
      "Epoch 3350, Loss: 0.036430271342396736, Final Batch Loss: 0.018017247319221497\n",
      "Epoch 3351, Loss: 0.12321775127202272, Final Batch Loss: 0.11387830972671509\n",
      "Epoch 3352, Loss: 0.10202649980783463, Final Batch Loss: 0.042547039687633514\n",
      "Epoch 3353, Loss: 0.0555960051715374, Final Batch Loss: 0.03021329641342163\n",
      "Epoch 3354, Loss: 0.06311962194740772, Final Batch Loss: 0.040003158152103424\n",
      "Epoch 3355, Loss: 0.08659514412283897, Final Batch Loss: 0.05318417772650719\n",
      "Epoch 3356, Loss: 0.12881241738796234, Final Batch Loss: 0.054681114852428436\n",
      "Epoch 3357, Loss: 0.05547806806862354, Final Batch Loss: 0.016465341672301292\n",
      "Epoch 3358, Loss: 0.057186299934983253, Final Batch Loss: 0.04078298062086105\n",
      "Epoch 3359, Loss: 0.08743774611502886, Final Batch Loss: 0.07629076391458511\n",
      "Epoch 3360, Loss: 0.04521091468632221, Final Batch Loss: 0.017297901213169098\n",
      "Epoch 3361, Loss: 0.04183037951588631, Final Batch Loss: 0.02316810004413128\n",
      "Epoch 3362, Loss: 0.048502479679882526, Final Batch Loss: 0.03814322128891945\n",
      "Epoch 3363, Loss: 0.061555156484246254, Final Batch Loss: 0.023251840844750404\n",
      "Epoch 3364, Loss: 0.02377501968294382, Final Batch Loss: 0.014903796836733818\n",
      "Epoch 3365, Loss: 0.07756835035979748, Final Batch Loss: 0.0506431981921196\n",
      "Epoch 3366, Loss: 0.048606954514980316, Final Batch Loss: 0.014291416853666306\n",
      "Epoch 3367, Loss: 0.06825623661279678, Final Batch Loss: 0.019310448318719864\n",
      "Epoch 3368, Loss: 0.05131222866475582, Final Batch Loss: 0.03639216348528862\n",
      "Epoch 3369, Loss: 0.03829735703766346, Final Batch Loss: 0.016774510964751244\n",
      "Epoch 3370, Loss: 0.08658038824796677, Final Batch Loss: 0.03739088773727417\n",
      "Epoch 3371, Loss: 0.08309703692793846, Final Batch Loss: 0.06559066474437714\n",
      "Epoch 3372, Loss: 0.09148209355771542, Final Batch Loss: 0.018044358119368553\n",
      "Epoch 3373, Loss: 0.07976647838950157, Final Batch Loss: 0.041277751326560974\n",
      "Epoch 3374, Loss: 0.054379912093281746, Final Batch Loss: 0.025654703378677368\n",
      "Epoch 3375, Loss: 0.05478385463356972, Final Batch Loss: 0.0387999452650547\n",
      "Epoch 3376, Loss: 0.04786130040884018, Final Batch Loss: 0.0246986523270607\n",
      "Epoch 3377, Loss: 0.07223967928439379, Final Batch Loss: 0.012067818082869053\n",
      "Epoch 3378, Loss: 0.12525204941630363, Final Batch Loss: 0.03667157515883446\n",
      "Epoch 3379, Loss: 0.027138306759297848, Final Batch Loss: 0.00909504946321249\n",
      "Epoch 3380, Loss: 0.05588060803711414, Final Batch Loss: 0.030863776803016663\n",
      "Epoch 3381, Loss: 0.026707567274570465, Final Batch Loss: 0.01319444552063942\n",
      "Epoch 3382, Loss: 0.038542804308235645, Final Batch Loss: 0.01458528358489275\n",
      "Epoch 3383, Loss: 0.023646008223295212, Final Batch Loss: 0.0135898282751441\n",
      "Epoch 3384, Loss: 0.08649353682994843, Final Batch Loss: 0.06897676736116409\n",
      "Epoch 3385, Loss: 0.03583944868296385, Final Batch Loss: 0.021513640880584717\n",
      "Epoch 3386, Loss: 0.06648801825940609, Final Batch Loss: 0.05020231008529663\n",
      "Epoch 3387, Loss: 0.07101533748209476, Final Batch Loss: 0.045494627207517624\n",
      "Epoch 3388, Loss: 0.0604178998619318, Final Batch Loss: 0.022427724674344063\n",
      "Epoch 3389, Loss: 0.05805536452680826, Final Batch Loss: 0.013714448548853397\n",
      "Epoch 3390, Loss: 0.04212958924472332, Final Batch Loss: 0.02272026799619198\n",
      "Epoch 3391, Loss: 0.04263113997876644, Final Batch Loss: 0.011865613982081413\n",
      "Epoch 3392, Loss: 0.10061772912740707, Final Batch Loss: 0.06253720074892044\n",
      "Epoch 3393, Loss: 0.04366944916546345, Final Batch Loss: 0.01997160166501999\n",
      "Epoch 3394, Loss: 0.05459956265985966, Final Batch Loss: 0.02827230654656887\n",
      "Epoch 3395, Loss: 0.04682890512049198, Final Batch Loss: 0.028745299205183983\n",
      "Epoch 3396, Loss: 0.12916064634919167, Final Batch Loss: 0.08600325882434845\n",
      "Epoch 3397, Loss: 0.056483518332242966, Final Batch Loss: 0.014308951795101166\n",
      "Epoch 3398, Loss: 0.05757795087993145, Final Batch Loss: 0.02428673394024372\n",
      "Epoch 3399, Loss: 0.03771951608359814, Final Batch Loss: 0.018004275858402252\n",
      "Epoch 3400, Loss: 0.06206146813929081, Final Batch Loss: 0.03792104497551918\n",
      "Epoch 3401, Loss: 0.06409071013331413, Final Batch Loss: 0.010905958712100983\n",
      "Epoch 3402, Loss: 0.05820336192846298, Final Batch Loss: 0.01655590906739235\n",
      "Epoch 3403, Loss: 0.02149870339781046, Final Batch Loss: 0.009663914330303669\n",
      "Epoch 3404, Loss: 0.07905418053269386, Final Batch Loss: 0.03371237590909004\n",
      "Epoch 3405, Loss: 0.04780884459614754, Final Batch Loss: 0.031501904129981995\n",
      "Epoch 3406, Loss: 0.049164190888404846, Final Batch Loss: 0.0291411392390728\n",
      "Epoch 3407, Loss: 0.06658892706036568, Final Batch Loss: 0.03393564745783806\n",
      "Epoch 3408, Loss: 0.04481388721615076, Final Batch Loss: 0.030104918405413628\n",
      "Epoch 3409, Loss: 0.05105851776897907, Final Batch Loss: 0.017032546922564507\n",
      "Epoch 3410, Loss: 0.05235688015818596, Final Batch Loss: 0.019284039735794067\n",
      "Epoch 3411, Loss: 0.03057434130460024, Final Batch Loss: 0.013081849552690983\n",
      "Epoch 3412, Loss: 0.03658844484016299, Final Batch Loss: 0.004664863925427198\n",
      "Epoch 3413, Loss: 0.190802663564682, Final Batch Loss: 0.1819646954536438\n",
      "Epoch 3414, Loss: 0.08069992810487747, Final Batch Loss: 0.06219106912612915\n",
      "Epoch 3415, Loss: 0.039266299456357956, Final Batch Loss: 0.021613318473100662\n",
      "Epoch 3416, Loss: 0.03310646302998066, Final Batch Loss: 0.01622115448117256\n",
      "Epoch 3417, Loss: 0.04095380660146475, Final Batch Loss: 0.025646109133958817\n",
      "Epoch 3418, Loss: 0.028620007447898388, Final Batch Loss: 0.016736164689064026\n",
      "Epoch 3419, Loss: 0.041987014934420586, Final Batch Loss: 0.025517363101243973\n",
      "Epoch 3420, Loss: 0.03059919737279415, Final Batch Loss: 0.007909111678600311\n",
      "Epoch 3421, Loss: 0.04354359954595566, Final Batch Loss: 0.01299159973859787\n",
      "Epoch 3422, Loss: 0.03957762382924557, Final Batch Loss: 0.026729976758360863\n",
      "Epoch 3423, Loss: 0.06099068094044924, Final Batch Loss: 0.015311525203287601\n",
      "Epoch 3424, Loss: 0.025528421625494957, Final Batch Loss: 0.016417276114225388\n",
      "Epoch 3425, Loss: 0.06216559000313282, Final Batch Loss: 0.034175530076026917\n",
      "Epoch 3426, Loss: 0.04110317304730415, Final Batch Loss: 0.016266334801912308\n",
      "Epoch 3427, Loss: 0.02217817772179842, Final Batch Loss: 0.0077637359499931335\n",
      "Epoch 3428, Loss: 0.03533797990530729, Final Batch Loss: 0.020221704617142677\n",
      "Epoch 3429, Loss: 0.023975342512130737, Final Batch Loss: 0.00775383785367012\n",
      "Epoch 3430, Loss: 0.02872731490060687, Final Batch Loss: 0.005587607156485319\n",
      "Epoch 3431, Loss: 0.041220592334866524, Final Batch Loss: 0.009440207853913307\n",
      "Epoch 3432, Loss: 0.03713677264750004, Final Batch Loss: 0.003976481035351753\n",
      "Epoch 3433, Loss: 0.09033945854753256, Final Batch Loss: 0.07724803686141968\n",
      "Epoch 3434, Loss: 0.06112883612513542, Final Batch Loss: 0.031038295477628708\n",
      "Epoch 3435, Loss: 0.034585557878017426, Final Batch Loss: 0.007745193317532539\n",
      "Epoch 3436, Loss: 0.06874789111316204, Final Batch Loss: 0.04414413124322891\n",
      "Epoch 3437, Loss: 0.05945166386663914, Final Batch Loss: 0.030721552670001984\n",
      "Epoch 3438, Loss: 0.026416609063744545, Final Batch Loss: 0.009416032582521439\n",
      "Epoch 3439, Loss: 0.057477870024740696, Final Batch Loss: 0.010538858361542225\n",
      "Epoch 3440, Loss: 0.023765187710523605, Final Batch Loss: 0.014661592431366444\n",
      "Epoch 3441, Loss: 0.02673725876957178, Final Batch Loss: 0.0056135645136237144\n",
      "Epoch 3442, Loss: 0.0593560915440321, Final Batch Loss: 0.0368930846452713\n",
      "Epoch 3443, Loss: 0.035293894819915295, Final Batch Loss: 0.02386121265590191\n",
      "Epoch 3444, Loss: 0.09154494851827621, Final Batch Loss: 0.020283356308937073\n",
      "Epoch 3445, Loss: 0.024975372478365898, Final Batch Loss: 0.013830017298460007\n",
      "Epoch 3446, Loss: 0.07245613820850849, Final Batch Loss: 0.02992367558181286\n",
      "Epoch 3447, Loss: 0.02417657896876335, Final Batch Loss: 0.0049228426069021225\n",
      "Epoch 3448, Loss: 0.07758512161672115, Final Batch Loss: 0.06380794942378998\n",
      "Epoch 3449, Loss: 0.08278953284025192, Final Batch Loss: 0.041239283978939056\n",
      "Epoch 3450, Loss: 0.023410465568304062, Final Batch Loss: 0.013857705518603325\n",
      "Epoch 3451, Loss: 0.056353834457695484, Final Batch Loss: 0.04730144888162613\n",
      "Epoch 3452, Loss: 0.07862478867173195, Final Batch Loss: 0.02722197026014328\n",
      "Epoch 3453, Loss: 0.062143209390342236, Final Batch Loss: 0.014155889861285686\n",
      "Epoch 3454, Loss: 0.07085747923702002, Final Batch Loss: 0.0582345575094223\n",
      "Epoch 3455, Loss: 0.11062481254339218, Final Batch Loss: 0.08681802451610565\n",
      "Epoch 3456, Loss: 0.11299463547766209, Final Batch Loss: 0.030611449852585793\n",
      "Epoch 3457, Loss: 0.025619271211326122, Final Batch Loss: 0.010710920207202435\n",
      "Epoch 3458, Loss: 0.038363851606845856, Final Batch Loss: 0.02425764873623848\n",
      "Epoch 3459, Loss: 0.041730355471372604, Final Batch Loss: 0.025004226714372635\n",
      "Epoch 3460, Loss: 0.062268560752272606, Final Batch Loss: 0.04249108210206032\n",
      "Epoch 3461, Loss: 0.06533695571124554, Final Batch Loss: 0.030612030997872353\n",
      "Epoch 3462, Loss: 0.061052992939949036, Final Batch Loss: 0.027211546897888184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3463, Loss: 0.07353542558848858, Final Batch Loss: 0.030292270705103874\n",
      "Epoch 3464, Loss: 0.04659848473966122, Final Batch Loss: 0.024312874302268028\n",
      "Epoch 3465, Loss: 0.052535382099449635, Final Batch Loss: 0.0132019417360425\n",
      "Epoch 3466, Loss: 0.02636728435754776, Final Batch Loss: 0.01666790433228016\n",
      "Epoch 3467, Loss: 0.051637924276292324, Final Batch Loss: 0.01464629452675581\n",
      "Epoch 3468, Loss: 0.06477917172014713, Final Batch Loss: 0.029010964557528496\n",
      "Epoch 3469, Loss: 0.060242170467972755, Final Batch Loss: 0.019087152555584908\n",
      "Epoch 3470, Loss: 0.056044348515570164, Final Batch Loss: 0.013921414501965046\n",
      "Epoch 3471, Loss: 0.057369853369891644, Final Batch Loss: 0.015042758546769619\n",
      "Epoch 3472, Loss: 0.0653602909296751, Final Batch Loss: 0.022078564390540123\n",
      "Epoch 3473, Loss: 0.04164447262883186, Final Batch Loss: 0.024436727166175842\n",
      "Epoch 3474, Loss: 0.06240808218717575, Final Batch Loss: 0.03034350275993347\n",
      "Epoch 3475, Loss: 0.07975460588932037, Final Batch Loss: 0.033526476472616196\n",
      "Epoch 3476, Loss: 0.021287529729306698, Final Batch Loss: 0.006995095871388912\n",
      "Epoch 3477, Loss: 0.04886364005506039, Final Batch Loss: 0.017965978011488914\n",
      "Epoch 3478, Loss: 0.04064620099961758, Final Batch Loss: 0.01187773048877716\n",
      "Epoch 3479, Loss: 0.03729057125747204, Final Batch Loss: 0.008723918348550797\n",
      "Epoch 3480, Loss: 0.04492768086493015, Final Batch Loss: 0.02901400998234749\n",
      "Epoch 3481, Loss: 0.04983663000166416, Final Batch Loss: 0.02299610711634159\n",
      "Epoch 3482, Loss: 0.03756583575159311, Final Batch Loss: 0.013085807673633099\n",
      "Epoch 3483, Loss: 0.037085218355059624, Final Batch Loss: 0.022905422374606133\n",
      "Epoch 3484, Loss: 0.04517429880797863, Final Batch Loss: 0.03295055404305458\n",
      "Epoch 3485, Loss: 0.041228145360946655, Final Batch Loss: 0.03019973449409008\n",
      "Epoch 3486, Loss: 0.056925770826637745, Final Batch Loss: 0.013991245068609715\n",
      "Epoch 3487, Loss: 0.05505260173231363, Final Batch Loss: 0.044675398617982864\n",
      "Epoch 3488, Loss: 0.03706887178122997, Final Batch Loss: 0.01658778078854084\n",
      "Epoch 3489, Loss: 0.05303155817091465, Final Batch Loss: 0.031269051134586334\n",
      "Epoch 3490, Loss: 0.04761791601777077, Final Batch Loss: 0.019116150215268135\n",
      "Epoch 3491, Loss: 0.020115328952670097, Final Batch Loss: 0.010634541511535645\n",
      "Epoch 3492, Loss: 0.06601426191627979, Final Batch Loss: 0.029748747125267982\n",
      "Epoch 3493, Loss: 0.03189736511558294, Final Batch Loss: 0.008296770043671131\n",
      "Epoch 3494, Loss: 0.03340776916593313, Final Batch Loss: 0.014463241212069988\n",
      "Epoch 3495, Loss: 0.023739532101899385, Final Batch Loss: 0.00759395444765687\n",
      "Epoch 3496, Loss: 0.05369285121560097, Final Batch Loss: 0.041254036128520966\n",
      "Epoch 3497, Loss: 0.03401933237910271, Final Batch Loss: 0.017789632081985474\n",
      "Epoch 3498, Loss: 0.050272624008357525, Final Batch Loss: 0.03647859767079353\n",
      "Epoch 3499, Loss: 0.05055974517017603, Final Batch Loss: 0.008927681483328342\n",
      "Epoch 3500, Loss: 0.03445493243634701, Final Batch Loss: 0.010736595839262009\n",
      "Epoch 3501, Loss: 0.04670083895325661, Final Batch Loss: 0.02807941474020481\n",
      "Epoch 3502, Loss: 0.028385152108967304, Final Batch Loss: 0.009120686911046505\n",
      "Epoch 3503, Loss: 0.08634083345532417, Final Batch Loss: 0.03651062026619911\n",
      "Epoch 3504, Loss: 0.035635379143059254, Final Batch Loss: 0.009303088299930096\n",
      "Epoch 3505, Loss: 0.04965213406831026, Final Batch Loss: 0.006291103549301624\n",
      "Epoch 3506, Loss: 0.03934361785650253, Final Batch Loss: 0.015741612762212753\n",
      "Epoch 3507, Loss: 0.021493785083293915, Final Batch Loss: 0.008000174537301064\n",
      "Epoch 3508, Loss: 0.01789785362780094, Final Batch Loss: 0.008547445759177208\n",
      "Epoch 3509, Loss: 0.028157821856439114, Final Batch Loss: 0.006784564815461636\n",
      "Epoch 3510, Loss: 0.04787096753716469, Final Batch Loss: 0.023649869486689568\n",
      "Epoch 3511, Loss: 0.03682004101574421, Final Batch Loss: 0.016482224687933922\n",
      "Epoch 3512, Loss: 0.03856760635972023, Final Batch Loss: 0.017777208238840103\n",
      "Epoch 3513, Loss: 0.05105798505246639, Final Batch Loss: 0.02220788411796093\n",
      "Epoch 3514, Loss: 0.05278370622545481, Final Batch Loss: 0.040549419820308685\n",
      "Epoch 3515, Loss: 0.06825594510883093, Final Batch Loss: 0.05754103511571884\n",
      "Epoch 3516, Loss: 0.01781882718205452, Final Batch Loss: 0.005296495743095875\n",
      "Epoch 3517, Loss: 0.09283349104225636, Final Batch Loss: 0.06762709468603134\n",
      "Epoch 3518, Loss: 0.04579667001962662, Final Batch Loss: 0.018756752833724022\n",
      "Epoch 3519, Loss: 0.04770439676940441, Final Batch Loss: 0.025311574339866638\n",
      "Epoch 3520, Loss: 0.05908078048378229, Final Batch Loss: 0.010360521264374256\n",
      "Epoch 3521, Loss: 0.03831702657043934, Final Batch Loss: 0.020584745332598686\n",
      "Epoch 3522, Loss: 0.04598625563085079, Final Batch Loss: 0.02540651522576809\n",
      "Epoch 3523, Loss: 0.06331620458513498, Final Batch Loss: 0.015549511648714542\n",
      "Epoch 3524, Loss: 0.07878345437347889, Final Batch Loss: 0.01368134655058384\n",
      "Epoch 3525, Loss: 0.02154015749692917, Final Batch Loss: 0.005216712132096291\n",
      "Epoch 3526, Loss: 0.040638357400894165, Final Batch Loss: 0.016182484105229378\n",
      "Epoch 3527, Loss: 0.048998091369867325, Final Batch Loss: 0.01640322431921959\n",
      "Epoch 3528, Loss: 0.03135209437459707, Final Batch Loss: 0.010230525396764278\n",
      "Epoch 3529, Loss: 0.04296890087425709, Final Batch Loss: 0.02478608302772045\n",
      "Epoch 3530, Loss: 0.018768853042274714, Final Batch Loss: 0.007379071321338415\n",
      "Epoch 3531, Loss: 0.024729518685489893, Final Batch Loss: 0.004338520113378763\n",
      "Epoch 3532, Loss: 0.05084127001464367, Final Batch Loss: 0.015568876639008522\n",
      "Epoch 3533, Loss: 0.08597374288365245, Final Batch Loss: 0.003617319744080305\n",
      "Epoch 3534, Loss: 0.020751371048390865, Final Batch Loss: 0.012078477069735527\n",
      "Epoch 3535, Loss: 0.05439296970143914, Final Batch Loss: 0.006976111326366663\n",
      "Epoch 3536, Loss: 0.03894365765154362, Final Batch Loss: 0.009653063490986824\n",
      "Epoch 3537, Loss: 0.03785643633455038, Final Batch Loss: 0.028189102187752724\n",
      "Epoch 3538, Loss: 0.04365658573806286, Final Batch Loss: 0.02615092322230339\n",
      "Epoch 3539, Loss: 0.048817120492458344, Final Batch Loss: 0.020217211917042732\n",
      "Epoch 3540, Loss: 0.029316436499357224, Final Batch Loss: 0.017916105687618256\n",
      "Epoch 3541, Loss: 0.04292982071638107, Final Batch Loss: 0.031362134963274\n",
      "Epoch 3542, Loss: 0.0417056642472744, Final Batch Loss: 0.0239811260253191\n",
      "Epoch 3543, Loss: 0.0311957485973835, Final Batch Loss: 0.013439828529953957\n",
      "Epoch 3544, Loss: 0.0533490926027298, Final Batch Loss: 0.030626308172941208\n",
      "Epoch 3545, Loss: 0.11157641187310219, Final Batch Loss: 0.095939040184021\n",
      "Epoch 3546, Loss: 0.01868114760145545, Final Batch Loss: 0.013148501515388489\n",
      "Epoch 3547, Loss: 0.04980684071779251, Final Batch Loss: 0.02976347878575325\n",
      "Epoch 3548, Loss: 0.08361339010298252, Final Batch Loss: 0.06130122020840645\n",
      "Epoch 3549, Loss: 0.044435957446694374, Final Batch Loss: 0.010648557916283607\n",
      "Epoch 3550, Loss: 0.049031791277229786, Final Batch Loss: 0.03551759570837021\n",
      "Epoch 3551, Loss: 0.044462962076067924, Final Batch Loss: 0.017300989478826523\n",
      "Epoch 3552, Loss: 0.0731000117957592, Final Batch Loss: 0.0419125072658062\n",
      "Epoch 3553, Loss: 0.02357923611998558, Final Batch Loss: 0.008039443753659725\n",
      "Epoch 3554, Loss: 0.03145471774041653, Final Batch Loss: 0.015598302707076073\n",
      "Epoch 3555, Loss: 0.06677639484405518, Final Batch Loss: 0.04965190589427948\n",
      "Epoch 3556, Loss: 0.032568708062171936, Final Batch Loss: 0.017122427001595497\n",
      "Epoch 3557, Loss: 0.04162518493831158, Final Batch Loss: 0.014515833929181099\n",
      "Epoch 3558, Loss: 0.05878526717424393, Final Batch Loss: 0.014297530055046082\n",
      "Epoch 3559, Loss: 0.04174717329442501, Final Batch Loss: 0.02502967044711113\n",
      "Epoch 3560, Loss: 0.07769818976521492, Final Batch Loss: 0.027025949209928513\n",
      "Epoch 3561, Loss: 0.03767246659845114, Final Batch Loss: 0.02900519035756588\n",
      "Epoch 3562, Loss: 0.05910814739763737, Final Batch Loss: 0.03515724837779999\n",
      "Epoch 3563, Loss: 0.029910245910286903, Final Batch Loss: 0.004717987030744553\n",
      "Epoch 3564, Loss: 0.05019913427531719, Final Batch Loss: 0.02779475226998329\n",
      "Epoch 3565, Loss: 0.04954582080245018, Final Batch Loss: 0.024469340220093727\n",
      "Epoch 3566, Loss: 0.04436561465263367, Final Batch Loss: 0.025547392666339874\n",
      "Epoch 3567, Loss: 0.041351957246661186, Final Batch Loss: 0.02220185287296772\n",
      "Epoch 3568, Loss: 0.02419311972334981, Final Batch Loss: 0.006865276489406824\n",
      "Epoch 3569, Loss: 0.03327422635629773, Final Batch Loss: 0.02607940323650837\n",
      "Epoch 3570, Loss: 0.03607568982988596, Final Batch Loss: 0.021178588271141052\n",
      "Epoch 3571, Loss: 0.06277345353737473, Final Batch Loss: 0.0044275433756411076\n",
      "Epoch 3572, Loss: 0.01901211589574814, Final Batch Loss: 0.013438384979963303\n",
      "Epoch 3573, Loss: 0.028231739066541195, Final Batch Loss: 0.017164791002869606\n",
      "Epoch 3574, Loss: 0.06445161625742912, Final Batch Loss: 0.05614006519317627\n",
      "Epoch 3575, Loss: 0.036288308911025524, Final Batch Loss: 0.022250929847359657\n",
      "Epoch 3576, Loss: 0.03099294099956751, Final Batch Loss: 0.024796925485134125\n",
      "Epoch 3577, Loss: 0.06229923479259014, Final Batch Loss: 0.008384732529520988\n",
      "Epoch 3578, Loss: 0.020225299522280693, Final Batch Loss: 0.005402354523539543\n",
      "Epoch 3579, Loss: 0.10598546266555786, Final Batch Loss: 0.04685637727379799\n",
      "Epoch 3580, Loss: 0.02543769683688879, Final Batch Loss: 0.00843712966889143\n",
      "Epoch 3581, Loss: 0.031885238364338875, Final Batch Loss: 0.017454324290156364\n",
      "Epoch 3582, Loss: 0.022499674931168556, Final Batch Loss: 0.007019166834652424\n",
      "Epoch 3583, Loss: 0.03162542264908552, Final Batch Loss: 0.02074933983385563\n",
      "Epoch 3584, Loss: 0.038634710013866425, Final Batch Loss: 0.022411415353417397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3585, Loss: 0.05610375665128231, Final Batch Loss: 0.00853959284722805\n",
      "Epoch 3586, Loss: 0.03139104135334492, Final Batch Loss: 0.016278458759188652\n",
      "Epoch 3587, Loss: 0.02630022168159485, Final Batch Loss: 0.019436253234744072\n",
      "Epoch 3588, Loss: 0.04359689261764288, Final Batch Loss: 0.014745431952178478\n",
      "Epoch 3589, Loss: 0.054495424032211304, Final Batch Loss: 0.02784709818661213\n",
      "Epoch 3590, Loss: 0.03701392561197281, Final Batch Loss: 0.009457649663090706\n",
      "Epoch 3591, Loss: 0.03207522723823786, Final Batch Loss: 0.014260672964155674\n",
      "Epoch 3592, Loss: 0.010417696554213762, Final Batch Loss: 0.0032965485006570816\n",
      "Epoch 3593, Loss: 0.07090103626251221, Final Batch Loss: 0.03159342333674431\n",
      "Epoch 3594, Loss: 0.06470825709402561, Final Batch Loss: 0.044099364429712296\n",
      "Epoch 3595, Loss: 0.032361086923629045, Final Batch Loss: 0.006888254079967737\n",
      "Epoch 3596, Loss: 0.027472328394651413, Final Batch Loss: 0.007822616025805473\n",
      "Epoch 3597, Loss: 0.02567455545067787, Final Batch Loss: 0.0056525785475969315\n",
      "Epoch 3598, Loss: 0.040207305923104286, Final Batch Loss: 0.030867131426930428\n",
      "Epoch 3599, Loss: 0.017301270738244057, Final Batch Loss: 0.01183744054287672\n",
      "Epoch 3600, Loss: 0.02318329783156514, Final Batch Loss: 0.006948028225451708\n",
      "Epoch 3601, Loss: 0.029796072281897068, Final Batch Loss: 0.01812136359512806\n",
      "Epoch 3602, Loss: 0.027727503329515457, Final Batch Loss: 0.009546680375933647\n",
      "Epoch 3603, Loss: 0.027747372165322304, Final Batch Loss: 0.011900372803211212\n",
      "Epoch 3604, Loss: 0.07371010817587376, Final Batch Loss: 0.06512653827667236\n",
      "Epoch 3605, Loss: 0.01906608324497938, Final Batch Loss: 0.004252351820468903\n",
      "Epoch 3606, Loss: 0.037994248792529106, Final Batch Loss: 0.021833552047610283\n",
      "Epoch 3607, Loss: 0.023810885846614838, Final Batch Loss: 0.011945090256631374\n",
      "Epoch 3608, Loss: 0.07650044560432434, Final Batch Loss: 0.03687513992190361\n",
      "Epoch 3609, Loss: 0.02424041088670492, Final Batch Loss: 0.009675711393356323\n",
      "Epoch 3610, Loss: 0.03909461572766304, Final Batch Loss: 0.019167279824614525\n",
      "Epoch 3611, Loss: 0.07792878337204456, Final Batch Loss: 0.03045063652098179\n",
      "Epoch 3612, Loss: 0.03715614229440689, Final Batch Loss: 0.02516268566250801\n",
      "Epoch 3613, Loss: 0.041761547327041626, Final Batch Loss: 0.025844624266028404\n",
      "Epoch 3614, Loss: 0.08617064729332924, Final Batch Loss: 0.052552253007888794\n",
      "Epoch 3615, Loss: 0.02014850825071335, Final Batch Loss: 0.008245314471423626\n",
      "Epoch 3616, Loss: 0.02723710611462593, Final Batch Loss: 0.012935768812894821\n",
      "Epoch 3617, Loss: 0.018285302445292473, Final Batch Loss: 0.00927412137389183\n",
      "Epoch 3618, Loss: 0.03441962879151106, Final Batch Loss: 0.008290219120681286\n",
      "Epoch 3619, Loss: 0.013272905256599188, Final Batch Loss: 0.006183217745274305\n",
      "Epoch 3620, Loss: 0.07489963248372078, Final Batch Loss: 0.0232170969247818\n",
      "Epoch 3621, Loss: 0.020454601384699345, Final Batch Loss: 0.010053231380879879\n",
      "Epoch 3622, Loss: 0.0196444489993155, Final Batch Loss: 0.005497563164681196\n",
      "Epoch 3623, Loss: 0.04525882750749588, Final Batch Loss: 0.019519425928592682\n",
      "Epoch 3624, Loss: 0.03604250866919756, Final Batch Loss: 0.024667151272296906\n",
      "Epoch 3625, Loss: 0.2507179453969002, Final Batch Loss: 0.14066340029239655\n",
      "Epoch 3626, Loss: 0.04862065985798836, Final Batch Loss: 0.01982729695737362\n",
      "Epoch 3627, Loss: 0.030168323777616024, Final Batch Loss: 0.02111944556236267\n",
      "Epoch 3628, Loss: 0.03216189704835415, Final Batch Loss: 0.01643957570195198\n",
      "Epoch 3629, Loss: 0.013727172277867794, Final Batch Loss: 0.00620947265997529\n",
      "Epoch 3630, Loss: 0.06521067768335342, Final Batch Loss: 0.011518195271492004\n",
      "Epoch 3631, Loss: 0.03697486501187086, Final Batch Loss: 0.01486396323889494\n",
      "Epoch 3632, Loss: 0.05463249795138836, Final Batch Loss: 0.03674401342868805\n",
      "Epoch 3633, Loss: 0.016621694201603532, Final Batch Loss: 0.0037289916072040796\n",
      "Epoch 3634, Loss: 0.05124291218817234, Final Batch Loss: 0.015814753249287605\n",
      "Epoch 3635, Loss: 0.06168033089488745, Final Batch Loss: 0.009487259201705456\n",
      "Epoch 3636, Loss: 0.051759038120508194, Final Batch Loss: 0.03316626325249672\n",
      "Epoch 3637, Loss: 0.04702762886881828, Final Batch Loss: 0.03011237271130085\n",
      "Epoch 3638, Loss: 0.0337003730237484, Final Batch Loss: 0.017551936209201813\n",
      "Epoch 3639, Loss: 0.1274676565080881, Final Batch Loss: 0.10494241863489151\n",
      "Epoch 3640, Loss: 0.03927960805594921, Final Batch Loss: 0.014358781278133392\n",
      "Epoch 3641, Loss: 0.03923765290528536, Final Batch Loss: 0.027067039161920547\n",
      "Epoch 3642, Loss: 0.03214153088629246, Final Batch Loss: 0.021276075392961502\n",
      "Epoch 3643, Loss: 0.04154712148010731, Final Batch Loss: 0.01655867137014866\n",
      "Epoch 3644, Loss: 0.049999925307929516, Final Batch Loss: 0.0024094758555293083\n",
      "Epoch 3645, Loss: 0.037960827350616455, Final Batch Loss: 0.019662722945213318\n",
      "Epoch 3646, Loss: 0.04498459631577134, Final Batch Loss: 0.005882947240024805\n",
      "Epoch 3647, Loss: 0.05128212086856365, Final Batch Loss: 0.024326395243406296\n",
      "Epoch 3648, Loss: 0.056701835710555315, Final Batch Loss: 0.050316110253334045\n",
      "Epoch 3649, Loss: 0.04440293088555336, Final Batch Loss: 0.014923883602023125\n",
      "Epoch 3650, Loss: 0.0394459031522274, Final Batch Loss: 0.009862955659627914\n",
      "Epoch 3651, Loss: 0.05252736993134022, Final Batch Loss: 0.01363394595682621\n",
      "Epoch 3652, Loss: 0.05338604561984539, Final Batch Loss: 0.026534322649240494\n",
      "Epoch 3653, Loss: 0.05181400757282972, Final Batch Loss: 0.04486525431275368\n",
      "Epoch 3654, Loss: 0.02658615168184042, Final Batch Loss: 0.014044669456779957\n",
      "Epoch 3655, Loss: 0.033669184893369675, Final Batch Loss: 0.01596132665872574\n",
      "Epoch 3656, Loss: 0.023843462578952312, Final Batch Loss: 0.008636566810309887\n",
      "Epoch 3657, Loss: 0.029496457893401384, Final Batch Loss: 0.007737081963568926\n",
      "Epoch 3658, Loss: 0.06649644300341606, Final Batch Loss: 0.03267849236726761\n",
      "Epoch 3659, Loss: 0.0342642106115818, Final Batch Loss: 0.008878195658326149\n",
      "Epoch 3660, Loss: 0.049905437510460615, Final Batch Loss: 0.007299523334950209\n",
      "Epoch 3661, Loss: 0.03829592280089855, Final Batch Loss: 0.01792958565056324\n",
      "Epoch 3662, Loss: 0.06286782585084438, Final Batch Loss: 0.02941734530031681\n",
      "Epoch 3663, Loss: 0.037077320739626884, Final Batch Loss: 0.01663457602262497\n",
      "Epoch 3664, Loss: 0.0821646936237812, Final Batch Loss: 0.04486357793211937\n",
      "Epoch 3665, Loss: 0.02128618862479925, Final Batch Loss: 0.011052420362830162\n",
      "Epoch 3666, Loss: 0.08009591698646545, Final Batch Loss: 0.055495020002126694\n",
      "Epoch 3667, Loss: 0.0506597775965929, Final Batch Loss: 0.01905428059399128\n",
      "Epoch 3668, Loss: 0.0519087091088295, Final Batch Loss: 0.03351549059152603\n",
      "Epoch 3669, Loss: 0.04905974492430687, Final Batch Loss: 0.02296702191233635\n",
      "Epoch 3670, Loss: 0.038880535401403904, Final Batch Loss: 0.013579641468822956\n",
      "Epoch 3671, Loss: 0.0626356340944767, Final Batch Loss: 0.04733101651072502\n",
      "Epoch 3672, Loss: 0.023716159164905548, Final Batch Loss: 0.010282653383910656\n",
      "Epoch 3673, Loss: 0.0291157066822052, Final Batch Loss: 0.009940255433321\n",
      "Epoch 3674, Loss: 0.08763116598129272, Final Batch Loss: 0.030001018196344376\n",
      "Epoch 3675, Loss: 0.03708952199667692, Final Batch Loss: 0.023345177993178368\n",
      "Epoch 3676, Loss: 0.017700534779578447, Final Batch Loss: 0.007145754527300596\n",
      "Epoch 3677, Loss: 0.13867896422743797, Final Batch Loss: 0.034095924347639084\n",
      "Epoch 3678, Loss: 0.03312524501234293, Final Batch Loss: 0.014625580050051212\n",
      "Epoch 3679, Loss: 0.030274024233222008, Final Batch Loss: 0.021287137642502785\n",
      "Epoch 3680, Loss: 0.03055227268487215, Final Batch Loss: 0.006262597627937794\n",
      "Epoch 3681, Loss: 0.08559563010931015, Final Batch Loss: 0.04421510174870491\n",
      "Epoch 3682, Loss: 0.03988161310553551, Final Batch Loss: 0.013945015147328377\n",
      "Epoch 3683, Loss: 0.02432348858565092, Final Batch Loss: 0.012882259674370289\n",
      "Epoch 3684, Loss: 0.07238529808819294, Final Batch Loss: 0.04850313067436218\n",
      "Epoch 3685, Loss: 0.029300302267074585, Final Batch Loss: 0.020184211432933807\n",
      "Epoch 3686, Loss: 0.0561442356556654, Final Batch Loss: 0.03927011042833328\n",
      "Epoch 3687, Loss: 0.031089413911104202, Final Batch Loss: 0.02142590656876564\n",
      "Epoch 3688, Loss: 0.05203956551849842, Final Batch Loss: 0.016009358689188957\n",
      "Epoch 3689, Loss: 0.0240690466016531, Final Batch Loss: 0.010501842945814133\n",
      "Epoch 3690, Loss: 0.023967632092535496, Final Batch Loss: 0.01309345941990614\n",
      "Epoch 3691, Loss: 0.0352231664583087, Final Batch Loss: 0.024151360616087914\n",
      "Epoch 3692, Loss: 0.021385922096669674, Final Batch Loss: 0.004043105058372021\n",
      "Epoch 3693, Loss: 0.05309087038040161, Final Batch Loss: 0.01735834777355194\n",
      "Epoch 3694, Loss: 0.02122766198590398, Final Batch Loss: 0.013974214904010296\n",
      "Epoch 3695, Loss: 0.02924168948084116, Final Batch Loss: 0.010675604455173016\n",
      "Epoch 3696, Loss: 0.04414436873048544, Final Batch Loss: 0.030217988416552544\n",
      "Epoch 3697, Loss: 0.06123587395995855, Final Batch Loss: 0.0470307320356369\n",
      "Epoch 3698, Loss: 0.05802900157868862, Final Batch Loss: 0.03360731899738312\n",
      "Epoch 3699, Loss: 0.052482230588793755, Final Batch Loss: 0.010842198505997658\n",
      "Epoch 3700, Loss: 0.03230216260999441, Final Batch Loss: 0.024101318791508675\n",
      "Epoch 3701, Loss: 0.048336392268538475, Final Batch Loss: 0.030124211683869362\n",
      "Epoch 3702, Loss: 0.043279011733829975, Final Batch Loss: 0.029134409502148628\n",
      "Epoch 3703, Loss: 0.0810063723474741, Final Batch Loss: 0.058868568390607834\n",
      "Epoch 3704, Loss: 0.01570369489490986, Final Batch Loss: 0.009563280269503593\n",
      "Epoch 3705, Loss: 0.04810244031250477, Final Batch Loss: 0.01765371486544609\n",
      "Epoch 3706, Loss: 0.06226181425154209, Final Batch Loss: 0.030336355790495872\n",
      "Epoch 3707, Loss: 0.042743053287267685, Final Batch Loss: 0.0343661792576313\n",
      "Epoch 3708, Loss: 0.039829714223742485, Final Batch Loss: 0.020408669486641884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3709, Loss: 0.05349359381943941, Final Batch Loss: 0.014250298030674458\n",
      "Epoch 3710, Loss: 0.02640981925651431, Final Batch Loss: 0.00753973750397563\n",
      "Epoch 3711, Loss: 0.02144341729581356, Final Batch Loss: 0.013355071656405926\n",
      "Epoch 3712, Loss: 0.2558903321623802, Final Batch Loss: 0.017702914774417877\n",
      "Epoch 3713, Loss: 0.033671682234853506, Final Batch Loss: 0.00595766631886363\n",
      "Epoch 3714, Loss: 0.10035230405628681, Final Batch Loss: 0.02472286857664585\n",
      "Epoch 3715, Loss: 0.08828599378466606, Final Batch Loss: 0.06008201465010643\n",
      "Epoch 3716, Loss: 0.20954986475408077, Final Batch Loss: 0.19236764311790466\n",
      "Epoch 3717, Loss: 0.0427978839725256, Final Batch Loss: 0.02452559396624565\n",
      "Epoch 3718, Loss: 0.06120700016617775, Final Batch Loss: 0.01776239275932312\n",
      "Epoch 3719, Loss: 0.09536404255777597, Final Batch Loss: 0.012646273709833622\n",
      "Epoch 3720, Loss: 0.06997616775333881, Final Batch Loss: 0.05092642828822136\n",
      "Epoch 3721, Loss: 0.05589213781058788, Final Batch Loss: 0.02078060992062092\n",
      "Epoch 3722, Loss: 0.09925641119480133, Final Batch Loss: 0.010076761245727539\n",
      "Epoch 3723, Loss: 0.037404706701636314, Final Batch Loss: 0.020730305463075638\n",
      "Epoch 3724, Loss: 0.026193369179964066, Final Batch Loss: 0.015184682793915272\n",
      "Epoch 3725, Loss: 0.07566149346530437, Final Batch Loss: 0.049698229879140854\n",
      "Epoch 3726, Loss: 0.1292111799120903, Final Batch Loss: 0.043982550501823425\n",
      "Epoch 3727, Loss: 0.022205566987395287, Final Batch Loss: 0.010538698174059391\n",
      "Epoch 3728, Loss: 0.08862922526896, Final Batch Loss: 0.027613749727606773\n",
      "Epoch 3729, Loss: 0.06934471614658833, Final Batch Loss: 0.05337877571582794\n",
      "Epoch 3730, Loss: 0.040150973945856094, Final Batch Loss: 0.02118273824453354\n",
      "Epoch 3731, Loss: 0.026963327080011368, Final Batch Loss: 0.017106283456087112\n",
      "Epoch 3732, Loss: 0.07298320531845093, Final Batch Loss: 0.050057873129844666\n",
      "Epoch 3733, Loss: 0.12453259900212288, Final Batch Loss: 0.07220640778541565\n",
      "Epoch 3734, Loss: 0.06234351731836796, Final Batch Loss: 0.02443949691951275\n",
      "Epoch 3735, Loss: 0.12723661586642265, Final Batch Loss: 0.05293980613350868\n",
      "Epoch 3736, Loss: 0.0815740991383791, Final Batch Loss: 0.06133337318897247\n",
      "Epoch 3737, Loss: 0.0451064296066761, Final Batch Loss: 0.01982816867530346\n",
      "Epoch 3738, Loss: 0.029354529920965433, Final Batch Loss: 0.007608324754983187\n",
      "Epoch 3739, Loss: 0.020473371259868145, Final Batch Loss: 0.009596563875675201\n",
      "Epoch 3740, Loss: 0.1457258053123951, Final Batch Loss: 0.09708715975284576\n",
      "Epoch 3741, Loss: 0.043482099659740925, Final Batch Loss: 0.006253586150705814\n",
      "Epoch 3742, Loss: 0.0858283769339323, Final Batch Loss: 0.05901844799518585\n",
      "Epoch 3743, Loss: 0.0568251833319664, Final Batch Loss: 0.030689097940921783\n",
      "Epoch 3744, Loss: 0.022195493802428246, Final Batch Loss: 0.00432809442281723\n",
      "Epoch 3745, Loss: 0.06057961285114288, Final Batch Loss: 0.039029560983181\n",
      "Epoch 3746, Loss: 0.05671346001327038, Final Batch Loss: 0.028537392616271973\n",
      "Epoch 3747, Loss: 0.06635394506156445, Final Batch Loss: 0.02137845568358898\n",
      "Epoch 3748, Loss: 0.07968408428132534, Final Batch Loss: 0.061810631304979324\n",
      "Epoch 3749, Loss: 0.035125408321619034, Final Batch Loss: 0.02286633662879467\n",
      "Epoch 3750, Loss: 0.050177489407360554, Final Batch Loss: 0.01554554421454668\n",
      "Epoch 3751, Loss: 0.028647661209106445, Final Batch Loss: 0.01096184179186821\n",
      "Epoch 3752, Loss: 0.059832735918462276, Final Batch Loss: 0.011296874843537807\n",
      "Epoch 3753, Loss: 0.06349359638988972, Final Batch Loss: 0.019885746762156487\n",
      "Epoch 3754, Loss: 0.06843713950365782, Final Batch Loss: 0.059630949050188065\n",
      "Epoch 3755, Loss: 0.035543073900043964, Final Batch Loss: 0.021668365225195885\n",
      "Epoch 3756, Loss: 0.026578135788440704, Final Batch Loss: 0.015293657779693604\n",
      "Epoch 3757, Loss: 0.03843509778380394, Final Batch Loss: 0.012105219066143036\n",
      "Epoch 3758, Loss: 0.05489624664187431, Final Batch Loss: 0.03272257000207901\n",
      "Epoch 3759, Loss: 0.09139123186469078, Final Batch Loss: 0.04301148280501366\n",
      "Epoch 3760, Loss: 0.08976875431835651, Final Batch Loss: 0.06807778030633926\n",
      "Epoch 3761, Loss: 0.19509606808423996, Final Batch Loss: 0.040675289928913116\n",
      "Epoch 3762, Loss: 0.0358559750020504, Final Batch Loss: 0.015350628644227982\n",
      "Epoch 3763, Loss: 0.03206075541675091, Final Batch Loss: 0.018011128529906273\n",
      "Epoch 3764, Loss: 0.058305954560637474, Final Batch Loss: 0.012922605499625206\n",
      "Epoch 3765, Loss: 0.016650541685521603, Final Batch Loss: 0.006818236783146858\n",
      "Epoch 3766, Loss: 0.07276219874620438, Final Batch Loss: 0.0488150529563427\n",
      "Epoch 3767, Loss: 0.06737179309129715, Final Batch Loss: 0.04239568114280701\n",
      "Epoch 3768, Loss: 0.0444569643586874, Final Batch Loss: 0.026422547176480293\n",
      "Epoch 3769, Loss: 0.05684695206582546, Final Batch Loss: 0.029344433918595314\n",
      "Epoch 3770, Loss: 0.04065658897161484, Final Batch Loss: 0.010259250178933144\n",
      "Epoch 3771, Loss: 0.04268316179513931, Final Batch Loss: 0.019907400012016296\n",
      "Epoch 3772, Loss: 0.0654466524720192, Final Batch Loss: 0.03775558993220329\n",
      "Epoch 3773, Loss: 0.0831932332366705, Final Batch Loss: 0.0581863708794117\n",
      "Epoch 3774, Loss: 0.04459399916231632, Final Batch Loss: 0.02466672472655773\n",
      "Epoch 3775, Loss: 0.09662897884845734, Final Batch Loss: 0.06381238251924515\n",
      "Epoch 3776, Loss: 0.03990054316818714, Final Batch Loss: 0.023158766329288483\n",
      "Epoch 3777, Loss: 0.09840027801692486, Final Batch Loss: 0.027640407904982567\n",
      "Epoch 3778, Loss: 0.040273552760481834, Final Batch Loss: 0.013360375538468361\n",
      "Epoch 3779, Loss: 0.03935790713876486, Final Batch Loss: 0.024694055318832397\n",
      "Epoch 3780, Loss: 0.04587229713797569, Final Batch Loss: 0.019519686698913574\n",
      "Epoch 3781, Loss: 0.025704222731292248, Final Batch Loss: 0.010040861554443836\n",
      "Epoch 3782, Loss: 0.034955743700265884, Final Batch Loss: 0.01800891011953354\n",
      "Epoch 3783, Loss: 0.03652220219373703, Final Batch Loss: 0.025055788457393646\n",
      "Epoch 3784, Loss: 0.03398949932307005, Final Batch Loss: 0.01264165248721838\n",
      "Epoch 3785, Loss: 0.03800330311059952, Final Batch Loss: 0.01249612681567669\n",
      "Epoch 3786, Loss: 0.06498146615922451, Final Batch Loss: 0.04646871238946915\n",
      "Epoch 3787, Loss: 0.03009301144629717, Final Batch Loss: 0.016629615798592567\n",
      "Epoch 3788, Loss: 0.043778689578175545, Final Batch Loss: 0.01907932572066784\n",
      "Epoch 3789, Loss: 0.027023973874747753, Final Batch Loss: 0.011495108716189861\n",
      "Epoch 3790, Loss: 0.04929151386022568, Final Batch Loss: 0.01957753859460354\n",
      "Epoch 3791, Loss: 0.03778807818889618, Final Batch Loss: 0.01717378757894039\n",
      "Epoch 3792, Loss: 0.024751875549554825, Final Batch Loss: 0.012988388538360596\n",
      "Epoch 3793, Loss: 0.026078948751091957, Final Batch Loss: 0.010682924650609493\n",
      "Epoch 3794, Loss: 0.022800941951572895, Final Batch Loss: 0.005677170120179653\n",
      "Epoch 3795, Loss: 0.019095206633210182, Final Batch Loss: 0.009085429832339287\n",
      "Epoch 3796, Loss: 0.025464557111263275, Final Batch Loss: 0.01701575703918934\n",
      "Epoch 3797, Loss: 0.033171690069139004, Final Batch Loss: 0.014157633297145367\n",
      "Epoch 3798, Loss: 0.0910192159935832, Final Batch Loss: 0.08086343109607697\n",
      "Epoch 3799, Loss: 0.04832102730870247, Final Batch Loss: 0.02398201823234558\n",
      "Epoch 3800, Loss: 0.10785754024982452, Final Batch Loss: 0.07319950312376022\n",
      "Epoch 3801, Loss: 0.05748205538839102, Final Batch Loss: 0.015217396430671215\n",
      "Epoch 3802, Loss: 0.017510406207293272, Final Batch Loss: 0.004859323147684336\n",
      "Epoch 3803, Loss: 0.023638730868697166, Final Batch Loss: 0.01155990269035101\n",
      "Epoch 3804, Loss: 0.032464489340782166, Final Batch Loss: 0.020773785188794136\n",
      "Epoch 3805, Loss: 0.05517861619591713, Final Batch Loss: 0.02339405193924904\n",
      "Epoch 3806, Loss: 0.10042775608599186, Final Batch Loss: 0.026789674535393715\n",
      "Epoch 3807, Loss: 0.03496486181393266, Final Batch Loss: 0.0042201136238873005\n",
      "Epoch 3808, Loss: 0.1916794814169407, Final Batch Loss: 0.14527641236782074\n",
      "Epoch 3809, Loss: 0.04472457617521286, Final Batch Loss: 0.019065910950303078\n",
      "Epoch 3810, Loss: 0.05637979693710804, Final Batch Loss: 0.020159969106316566\n",
      "Epoch 3811, Loss: 0.06360963545739651, Final Batch Loss: 0.013332994654774666\n",
      "Epoch 3812, Loss: 0.030238899402320385, Final Batch Loss: 0.014905165880918503\n",
      "Epoch 3813, Loss: 0.026983333751559258, Final Batch Loss: 0.014355622231960297\n",
      "Epoch 3814, Loss: 0.06542974710464478, Final Batch Loss: 0.042491648346185684\n",
      "Epoch 3815, Loss: 0.025787602178752422, Final Batch Loss: 0.004263744689524174\n",
      "Epoch 3816, Loss: 0.04627959430217743, Final Batch Loss: 0.025021005421876907\n",
      "Epoch 3817, Loss: 0.08129967097193003, Final Batch Loss: 0.06822849810123444\n",
      "Epoch 3818, Loss: 0.05148913897573948, Final Batch Loss: 0.027699168771505356\n",
      "Epoch 3819, Loss: 0.017581292893737555, Final Batch Loss: 0.003927290905267\n",
      "Epoch 3820, Loss: 0.021940038539469242, Final Batch Loss: 0.007315876893699169\n",
      "Epoch 3821, Loss: 0.0590969636105001, Final Batch Loss: 0.053035251796245575\n",
      "Epoch 3822, Loss: 0.015472520142793655, Final Batch Loss: 0.005617341957986355\n",
      "Epoch 3823, Loss: 0.06159359123557806, Final Batch Loss: 0.05297614634037018\n",
      "Epoch 3824, Loss: 0.07685398682951927, Final Batch Loss: 0.035407718271017075\n",
      "Epoch 3825, Loss: 0.030871485825628042, Final Batch Loss: 0.007352498825639486\n",
      "Epoch 3826, Loss: 0.025042415596544743, Final Batch Loss: 0.00894146878272295\n",
      "Epoch 3827, Loss: 0.0411510206758976, Final Batch Loss: 0.02513376995921135\n",
      "Epoch 3828, Loss: 0.021153929643332958, Final Batch Loss: 0.007087549194693565\n",
      "Epoch 3829, Loss: 0.02965227235108614, Final Batch Loss: 0.009073049761354923\n",
      "Epoch 3830, Loss: 0.036570193246006966, Final Batch Loss: 0.02057468146085739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3831, Loss: 0.05628553219139576, Final Batch Loss: 0.021281151100993156\n",
      "Epoch 3832, Loss: 0.06422113627195358, Final Batch Loss: 0.042140040546655655\n",
      "Epoch 3833, Loss: 0.07518922165036201, Final Batch Loss: 0.04095202311873436\n",
      "Epoch 3834, Loss: 0.03909705113619566, Final Batch Loss: 0.009694668464362621\n",
      "Epoch 3835, Loss: 0.025891011580824852, Final Batch Loss: 0.009556770324707031\n",
      "Epoch 3836, Loss: 0.05279373563826084, Final Batch Loss: 0.0255732499063015\n",
      "Epoch 3837, Loss: 0.05025170184671879, Final Batch Loss: 0.015267131850123405\n",
      "Epoch 3838, Loss: 0.08390403538942337, Final Batch Loss: 0.037483640015125275\n",
      "Epoch 3839, Loss: 0.04970445670187473, Final Batch Loss: 0.006935672834515572\n",
      "Epoch 3840, Loss: 0.040409089997410774, Final Batch Loss: 0.018822239711880684\n",
      "Epoch 3841, Loss: 0.022951927036046982, Final Batch Loss: 0.013810958713293076\n",
      "Epoch 3842, Loss: 0.07039497746154666, Final Batch Loss: 0.06288782507181168\n",
      "Epoch 3843, Loss: 0.049968806095421314, Final Batch Loss: 0.041435446590185165\n",
      "Epoch 3844, Loss: 0.048774661496281624, Final Batch Loss: 0.027582287788391113\n",
      "Epoch 3845, Loss: 0.06564188748598099, Final Batch Loss: 0.039330027997493744\n",
      "Epoch 3846, Loss: 0.014165231958031654, Final Batch Loss: 0.009528132155537605\n",
      "Epoch 3847, Loss: 0.060824332758784294, Final Batch Loss: 0.03896253928542137\n",
      "Epoch 3848, Loss: 0.05332778953015804, Final Batch Loss: 0.015287818387150764\n",
      "Epoch 3849, Loss: 0.10848440509289503, Final Batch Loss: 0.09894895553588867\n",
      "Epoch 3850, Loss: 0.037151120603084564, Final Batch Loss: 0.019132932648062706\n",
      "Epoch 3851, Loss: 0.06754287332296371, Final Batch Loss: 0.041174259036779404\n",
      "Epoch 3852, Loss: 0.042890059761703014, Final Batch Loss: 0.01422849390655756\n",
      "Epoch 3853, Loss: 0.04007532401010394, Final Batch Loss: 0.006827190052717924\n",
      "Epoch 3854, Loss: 0.0490922536700964, Final Batch Loss: 0.029830656945705414\n",
      "Epoch 3855, Loss: 0.029500989243388176, Final Batch Loss: 0.007903730496764183\n",
      "Epoch 3856, Loss: 0.06776520051062107, Final Batch Loss: 0.04837948456406593\n",
      "Epoch 3857, Loss: 0.05023472383618355, Final Batch Loss: 0.020505473017692566\n",
      "Epoch 3858, Loss: 0.05323739908635616, Final Batch Loss: 0.024389104917645454\n",
      "Epoch 3859, Loss: 0.0770427230745554, Final Batch Loss: 0.05335685610771179\n",
      "Epoch 3860, Loss: 0.024991201236844063, Final Batch Loss: 0.016850417479872704\n",
      "Epoch 3861, Loss: 0.07680349424481392, Final Batch Loss: 0.06687075644731522\n",
      "Epoch 3862, Loss: 0.04546920396387577, Final Batch Loss: 0.010858802124857903\n",
      "Epoch 3863, Loss: 0.06154370866715908, Final Batch Loss: 0.01743491180241108\n",
      "Epoch 3864, Loss: 0.07623155973851681, Final Batch Loss: 0.013999564573168755\n",
      "Epoch 3865, Loss: 0.051903318613767624, Final Batch Loss: 0.04015485942363739\n",
      "Epoch 3866, Loss: 0.04958095960319042, Final Batch Loss: 0.02273465134203434\n",
      "Epoch 3867, Loss: 0.10148853063583374, Final Batch Loss: 0.07114318013191223\n",
      "Epoch 3868, Loss: 0.05288149043917656, Final Batch Loss: 0.03661924973130226\n",
      "Epoch 3869, Loss: 0.04023201297968626, Final Batch Loss: 0.014052360318601131\n",
      "Epoch 3870, Loss: 0.05379031226038933, Final Batch Loss: 0.02819191850721836\n",
      "Epoch 3871, Loss: 0.06648600101470947, Final Batch Loss: 0.0347701832652092\n",
      "Epoch 3872, Loss: 0.03790172003209591, Final Batch Loss: 0.01774192601442337\n",
      "Epoch 3873, Loss: 0.05634469352662563, Final Batch Loss: 0.029801925644278526\n",
      "Epoch 3874, Loss: 0.02667185477912426, Final Batch Loss: 0.014550249092280865\n",
      "Epoch 3875, Loss: 0.08872230350971222, Final Batch Loss: 0.025143727660179138\n",
      "Epoch 3876, Loss: 0.05760233849287033, Final Batch Loss: 0.030544502660632133\n",
      "Epoch 3877, Loss: 0.04130151309072971, Final Batch Loss: 0.02016208879649639\n",
      "Epoch 3878, Loss: 0.048596957698464394, Final Batch Loss: 0.030311543494462967\n",
      "Epoch 3879, Loss: 0.05069414712488651, Final Batch Loss: 0.007112337276339531\n",
      "Epoch 3880, Loss: 0.053390804678201675, Final Batch Loss: 0.04444468021392822\n",
      "Epoch 3881, Loss: 0.02844866644591093, Final Batch Loss: 0.009466313757002354\n",
      "Epoch 3882, Loss: 0.04355739988386631, Final Batch Loss: 0.014590907841920853\n",
      "Epoch 3883, Loss: 0.04364649299532175, Final Batch Loss: 0.010941443033516407\n",
      "Epoch 3884, Loss: 0.10884599760174751, Final Batch Loss: 0.07758740335702896\n",
      "Epoch 3885, Loss: 0.022645536810159683, Final Batch Loss: 0.008153222501277924\n",
      "Epoch 3886, Loss: 0.03691981453448534, Final Batch Loss: 0.027048246935009956\n",
      "Epoch 3887, Loss: 0.05900771915912628, Final Batch Loss: 0.023433443158864975\n",
      "Epoch 3888, Loss: 0.027712440118193626, Final Batch Loss: 0.00908256322145462\n",
      "Epoch 3889, Loss: 0.02645520051009953, Final Batch Loss: 0.0032420766074210405\n",
      "Epoch 3890, Loss: 0.06774765253067017, Final Batch Loss: 0.035116396844387054\n",
      "Epoch 3891, Loss: 0.0368026178330183, Final Batch Loss: 0.008006777614355087\n",
      "Epoch 3892, Loss: 0.027558601927012205, Final Batch Loss: 0.02028646692633629\n",
      "Epoch 3893, Loss: 0.053496452048420906, Final Batch Loss: 0.03602895885705948\n",
      "Epoch 3894, Loss: 0.02458064816892147, Final Batch Loss: 0.012380626983940601\n",
      "Epoch 3895, Loss: 0.058856796473264694, Final Batch Loss: 0.021581903100013733\n",
      "Epoch 3896, Loss: 0.027111441362649202, Final Batch Loss: 0.020051509141921997\n",
      "Epoch 3897, Loss: 0.047006149776279926, Final Batch Loss: 0.033507153391838074\n",
      "Epoch 3898, Loss: 0.024163609836250544, Final Batch Loss: 0.01767907664179802\n",
      "Epoch 3899, Loss: 0.016147587448358536, Final Batch Loss: 0.00479724258184433\n",
      "Epoch 3900, Loss: 0.027115055359899998, Final Batch Loss: 0.010648216120898724\n",
      "Epoch 3901, Loss: 0.03229522239416838, Final Batch Loss: 0.00670956913381815\n",
      "Epoch 3902, Loss: 0.020440236665308475, Final Batch Loss: 0.011231007054448128\n",
      "Epoch 3903, Loss: 0.05936286970973015, Final Batch Loss: 0.03298621252179146\n",
      "Epoch 3904, Loss: 0.013867399655282497, Final Batch Loss: 0.005510829389095306\n",
      "Epoch 3905, Loss: 0.0340656372718513, Final Batch Loss: 0.00720123341307044\n",
      "Epoch 3906, Loss: 0.09262443147599697, Final Batch Loss: 0.01991005800664425\n",
      "Epoch 3907, Loss: 0.06112455017864704, Final Batch Loss: 0.03769322857260704\n",
      "Epoch 3908, Loss: 0.053475236520171165, Final Batch Loss: 0.026403361931443214\n",
      "Epoch 3909, Loss: 0.05881948582828045, Final Batch Loss: 0.044125430285930634\n",
      "Epoch 3910, Loss: 0.04624109901487827, Final Batch Loss: 0.008266596123576164\n",
      "Epoch 3911, Loss: 0.035106198862195015, Final Batch Loss: 0.018231049180030823\n",
      "Epoch 3912, Loss: 0.07241918705403805, Final Batch Loss: 0.04600353166460991\n",
      "Epoch 3913, Loss: 0.04628844931721687, Final Batch Loss: 0.023759907111525536\n",
      "Epoch 3914, Loss: 0.04654003493487835, Final Batch Loss: 0.018343040719628334\n",
      "Epoch 3915, Loss: 0.01774717215448618, Final Batch Loss: 0.008760428056120872\n",
      "Epoch 3916, Loss: 0.027433271519839764, Final Batch Loss: 0.005575249902904034\n",
      "Epoch 3917, Loss: 0.03616826515644789, Final Batch Loss: 0.025014379993081093\n",
      "Epoch 3918, Loss: 0.04053534008562565, Final Batch Loss: 0.028720449656248093\n",
      "Epoch 3919, Loss: 0.008071703370660543, Final Batch Loss: 0.0027448125183582306\n",
      "Epoch 3920, Loss: 0.06524396874010563, Final Batch Loss: 0.03969461843371391\n",
      "Epoch 3921, Loss: 0.04127247165888548, Final Batch Loss: 0.029984936118125916\n",
      "Epoch 3922, Loss: 0.027614782564342022, Final Batch Loss: 0.02089173160493374\n",
      "Epoch 3923, Loss: 0.07318627089262009, Final Batch Loss: 0.03221379220485687\n",
      "Epoch 3924, Loss: 0.028611425310373306, Final Batch Loss: 0.017497463151812553\n",
      "Epoch 3925, Loss: 0.02095239795744419, Final Batch Loss: 0.010675651952624321\n",
      "Epoch 3926, Loss: 0.012526371516287327, Final Batch Loss: 0.006134441122412682\n",
      "Epoch 3927, Loss: 0.0727080088108778, Final Batch Loss: 0.06373132765293121\n",
      "Epoch 3928, Loss: 0.04399165231734514, Final Batch Loss: 0.03225937485694885\n",
      "Epoch 3929, Loss: 0.02758980356156826, Final Batch Loss: 0.0195790883153677\n",
      "Epoch 3930, Loss: 0.06200402230024338, Final Batch Loss: 0.008209310472011566\n",
      "Epoch 3931, Loss: 0.01493127434514463, Final Batch Loss: 0.0021147967781871557\n",
      "Epoch 3932, Loss: 0.016138260252773762, Final Batch Loss: 0.008800522424280643\n",
      "Epoch 3933, Loss: 0.02504955232143402, Final Batch Loss: 0.013691036961972713\n",
      "Epoch 3934, Loss: 0.02897843671962619, Final Batch Loss: 0.0036032977513968945\n",
      "Epoch 3935, Loss: 0.027100214269012213, Final Batch Loss: 0.007173077668994665\n",
      "Epoch 3936, Loss: 0.07052695844322443, Final Batch Loss: 0.060175471007823944\n",
      "Epoch 3937, Loss: 0.009455944178625941, Final Batch Loss: 0.002799918642267585\n",
      "Epoch 3938, Loss: 0.12771847657859325, Final Batch Loss: 0.021232066676020622\n",
      "Epoch 3939, Loss: 0.0443847356364131, Final Batch Loss: 0.013824821449816227\n",
      "Epoch 3940, Loss: 0.039278979413211346, Final Batch Loss: 0.025371475145220757\n",
      "Epoch 3941, Loss: 0.050211239606142044, Final Batch Loss: 0.017684288322925568\n",
      "Epoch 3942, Loss: 0.04546764865517616, Final Batch Loss: 0.015220686793327332\n",
      "Epoch 3943, Loss: 0.0468471497297287, Final Batch Loss: 0.03256233409047127\n",
      "Epoch 3944, Loss: 0.05192696489393711, Final Batch Loss: 0.029128601774573326\n",
      "Epoch 3945, Loss: 0.07213354669511318, Final Batch Loss: 0.05925106257200241\n",
      "Epoch 3946, Loss: 0.02210479136556387, Final Batch Loss: 0.016817616298794746\n",
      "Epoch 3947, Loss: 0.04267965815961361, Final Batch Loss: 0.016137754544615746\n",
      "Epoch 3948, Loss: 0.021538451313972473, Final Batch Loss: 0.009391379542648792\n",
      "Epoch 3949, Loss: 0.02990578580647707, Final Batch Loss: 0.015395442955195904\n",
      "Epoch 3950, Loss: 0.06474287807941437, Final Batch Loss: 0.048456061631441116\n",
      "Epoch 3951, Loss: 0.018062914721667767, Final Batch Loss: 0.005624174140393734\n",
      "Epoch 3952, Loss: 0.05411613546311855, Final Batch Loss: 0.0372016616165638\n",
      "Epoch 3953, Loss: 0.06892360374331474, Final Batch Loss: 0.0472169853746891\n",
      "Epoch 3954, Loss: 0.04124699626117945, Final Batch Loss: 0.026178933680057526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3955, Loss: 0.02864763606339693, Final Batch Loss: 0.02084381878376007\n",
      "Epoch 3956, Loss: 0.033464839681982994, Final Batch Loss: 0.01576908305287361\n",
      "Epoch 3957, Loss: 0.06058996170759201, Final Batch Loss: 0.007018104195594788\n",
      "Epoch 3958, Loss: 0.04965297318994999, Final Batch Loss: 0.038518551737070084\n",
      "Epoch 3959, Loss: 0.03838430158793926, Final Batch Loss: 0.020204374566674232\n",
      "Epoch 3960, Loss: 0.026989920996129513, Final Batch Loss: 0.018642950803041458\n",
      "Epoch 3961, Loss: 0.03125599445775151, Final Batch Loss: 0.007379485759884119\n",
      "Epoch 3962, Loss: 0.06372406706213951, Final Batch Loss: 0.03231438249349594\n",
      "Epoch 3963, Loss: 0.021218741312623024, Final Batch Loss: 0.012126749381422997\n",
      "Epoch 3964, Loss: 0.06654084287583828, Final Batch Loss: 0.02673555724322796\n",
      "Epoch 3965, Loss: 0.08046498149633408, Final Batch Loss: 0.0397736132144928\n",
      "Epoch 3966, Loss: 0.029571003280580044, Final Batch Loss: 0.013560074381530285\n",
      "Epoch 3967, Loss: 0.007104652002453804, Final Batch Loss: 0.0025870492681860924\n",
      "Epoch 3968, Loss: 0.06491244025528431, Final Batch Loss: 0.039487965404987335\n",
      "Epoch 3969, Loss: 0.08941216580569744, Final Batch Loss: 0.07986516505479813\n",
      "Epoch 3970, Loss: 0.02187382709234953, Final Batch Loss: 0.0104974415153265\n",
      "Epoch 3971, Loss: 0.01123556261882186, Final Batch Loss: 0.0074232155457139015\n",
      "Epoch 3972, Loss: 0.04691676888614893, Final Batch Loss: 0.039636772125959396\n",
      "Epoch 3973, Loss: 0.02152965497225523, Final Batch Loss: 0.011149023659527302\n",
      "Epoch 3974, Loss: 0.1230272687971592, Final Batch Loss: 0.02702227607369423\n",
      "Epoch 3975, Loss: 0.030484216287732124, Final Batch Loss: 0.009347863495349884\n",
      "Epoch 3976, Loss: 0.01983448024839163, Final Batch Loss: 0.0031812461093068123\n",
      "Epoch 3977, Loss: 0.01878858986310661, Final Batch Loss: 0.003202166175469756\n",
      "Epoch 3978, Loss: 0.047141214832663536, Final Batch Loss: 0.0165008082985878\n",
      "Epoch 3979, Loss: 0.026410600170493126, Final Batch Loss: 0.010828477330505848\n",
      "Epoch 3980, Loss: 0.06747557409107685, Final Batch Loss: 0.059889692813158035\n",
      "Epoch 3981, Loss: 0.014653378166258335, Final Batch Loss: 0.0061963945627212524\n",
      "Epoch 3982, Loss: 0.017549131996929646, Final Batch Loss: 0.008855605497956276\n",
      "Epoch 3983, Loss: 0.0407352396287024, Final Batch Loss: 0.005930314306169748\n",
      "Epoch 3984, Loss: 0.045077878050506115, Final Batch Loss: 0.01257388200610876\n",
      "Epoch 3985, Loss: 0.022361551877111197, Final Batch Loss: 0.004809245001524687\n",
      "Epoch 3986, Loss: 0.029429499059915543, Final Batch Loss: 0.014804915525019169\n",
      "Epoch 3987, Loss: 0.026304861530661583, Final Batch Loss: 0.016275394707918167\n",
      "Epoch 3988, Loss: 0.03797288239002228, Final Batch Loss: 0.010632339864969254\n",
      "Epoch 3989, Loss: 0.03777234721928835, Final Batch Loss: 0.015459657646715641\n",
      "Epoch 3990, Loss: 0.029552382417023182, Final Batch Loss: 0.014054227620363235\n",
      "Epoch 3991, Loss: 0.022602524608373642, Final Batch Loss: 0.010331892408430576\n",
      "Epoch 3992, Loss: 0.017157664988189936, Final Batch Loss: 0.006903900299221277\n",
      "Epoch 3993, Loss: 0.04147091694176197, Final Batch Loss: 0.03229533135890961\n",
      "Epoch 3994, Loss: 0.04858526028692722, Final Batch Loss: 0.02655942365527153\n",
      "Epoch 3995, Loss: 0.037428841926157475, Final Batch Loss: 0.02811063826084137\n",
      "Epoch 3996, Loss: 0.06376368924975395, Final Batch Loss: 0.052088648080825806\n",
      "Epoch 3997, Loss: 0.040887430775910616, Final Batch Loss: 0.006347120273858309\n",
      "Epoch 3998, Loss: 0.012897029286250472, Final Batch Loss: 0.0038843087386339903\n",
      "Epoch 3999, Loss: 0.036270338110625744, Final Batch Loss: 0.01299257855862379\n",
      "Epoch 4000, Loss: 0.03408948332071304, Final Batch Loss: 0.012482434511184692\n",
      "Epoch 4001, Loss: 0.12362126633524895, Final Batch Loss: 0.09130100905895233\n",
      "Epoch 4002, Loss: 0.1350618526339531, Final Batch Loss: 0.03129217028617859\n",
      "Epoch 4003, Loss: 0.0909170750528574, Final Batch Loss: 0.007280165329575539\n",
      "Epoch 4004, Loss: 0.03215297311544418, Final Batch Loss: 0.012744713574647903\n",
      "Epoch 4005, Loss: 0.03521747048944235, Final Batch Loss: 0.0254893209785223\n",
      "Epoch 4006, Loss: 0.03644208237528801, Final Batch Loss: 0.020071864128112793\n",
      "Epoch 4007, Loss: 0.04493771679699421, Final Batch Loss: 0.008816378191113472\n",
      "Epoch 4008, Loss: 0.06809360440820456, Final Batch Loss: 0.05510575324296951\n",
      "Epoch 4009, Loss: 0.026727930642664433, Final Batch Loss: 0.010373101569712162\n",
      "Epoch 4010, Loss: 0.02964544715359807, Final Batch Loss: 0.022153915837407112\n",
      "Epoch 4011, Loss: 0.019296016544103622, Final Batch Loss: 0.007821904495358467\n",
      "Epoch 4012, Loss: 0.021103225648403168, Final Batch Loss: 0.01481750886887312\n",
      "Epoch 4013, Loss: 0.02977508120238781, Final Batch Loss: 0.016813937574625015\n",
      "Epoch 4014, Loss: 0.016825298313051462, Final Batch Loss: 0.009495054371654987\n",
      "Epoch 4015, Loss: 0.038502381183207035, Final Batch Loss: 0.012218822725117207\n",
      "Epoch 4016, Loss: 0.046885980293154716, Final Batch Loss: 0.02101775072515011\n",
      "Epoch 4017, Loss: 0.10403108783066273, Final Batch Loss: 0.026981638744473457\n",
      "Epoch 4018, Loss: 0.06334793474525213, Final Batch Loss: 0.008560086600482464\n",
      "Epoch 4019, Loss: 0.02830508165061474, Final Batch Loss: 0.016802703961730003\n",
      "Epoch 4020, Loss: 0.07921785861253738, Final Batch Loss: 0.0316411592066288\n",
      "Epoch 4021, Loss: 0.03805670328438282, Final Batch Loss: 0.019949890673160553\n",
      "Epoch 4022, Loss: 0.05929553951136768, Final Batch Loss: 0.003373791230842471\n",
      "Epoch 4023, Loss: 0.047493183985352516, Final Batch Loss: 0.020088868215680122\n",
      "Epoch 4024, Loss: 0.06475023925304413, Final Batch Loss: 0.026672936975955963\n",
      "Epoch 4025, Loss: 0.050048050470650196, Final Batch Loss: 0.010359887965023518\n",
      "Epoch 4026, Loss: 0.04597472492605448, Final Batch Loss: 0.012299288995563984\n",
      "Epoch 4027, Loss: 0.026563922874629498, Final Batch Loss: 0.018278971314430237\n",
      "Epoch 4028, Loss: 0.031401485204696655, Final Batch Loss: 0.022059762850403786\n",
      "Epoch 4029, Loss: 0.03495338186621666, Final Batch Loss: 0.008251192048192024\n",
      "Epoch 4030, Loss: 0.10186908580362797, Final Batch Loss: 0.07673781365156174\n",
      "Epoch 4031, Loss: 0.08322742581367493, Final Batch Loss: 0.03440217673778534\n",
      "Epoch 4032, Loss: 0.07695915549993515, Final Batch Loss: 0.02626815065741539\n",
      "Epoch 4033, Loss: 0.030532899312675, Final Batch Loss: 0.01753000169992447\n",
      "Epoch 4034, Loss: 0.028716555796563625, Final Batch Loss: 0.014220751821994781\n",
      "Epoch 4035, Loss: 0.053734565153717995, Final Batch Loss: 0.025683945044875145\n",
      "Epoch 4036, Loss: 0.03151896223425865, Final Batch Loss: 0.011045923456549644\n",
      "Epoch 4037, Loss: 0.06574128847569227, Final Batch Loss: 0.05091430991888046\n",
      "Epoch 4038, Loss: 0.031311942264437675, Final Batch Loss: 0.01905825547873974\n",
      "Epoch 4039, Loss: 0.04351117601618171, Final Batch Loss: 0.03733038157224655\n",
      "Epoch 4040, Loss: 0.018507692962884903, Final Batch Loss: 0.003960900940001011\n",
      "Epoch 4041, Loss: 0.042832151055336, Final Batch Loss: 0.02373240701854229\n",
      "Epoch 4042, Loss: 0.1069943867623806, Final Batch Loss: 0.06655707210302353\n",
      "Epoch 4043, Loss: 0.04864075034856796, Final Batch Loss: 0.034221772104501724\n",
      "Epoch 4044, Loss: 0.02535085193812847, Final Batch Loss: 0.010860199108719826\n",
      "Epoch 4045, Loss: 0.05163269303739071, Final Batch Loss: 0.023320404812693596\n",
      "Epoch 4046, Loss: 0.061872730031609535, Final Batch Loss: 0.02914455346763134\n",
      "Epoch 4047, Loss: 0.03832810837775469, Final Batch Loss: 0.014988663606345654\n",
      "Epoch 4048, Loss: 0.07585612498223782, Final Batch Loss: 0.061383720487356186\n",
      "Epoch 4049, Loss: 0.05287763476371765, Final Batch Loss: 0.018388744443655014\n",
      "Epoch 4050, Loss: 0.05540425144135952, Final Batch Loss: 0.011681804433465004\n",
      "Epoch 4051, Loss: 0.0845786053687334, Final Batch Loss: 0.06730543822050095\n",
      "Epoch 4052, Loss: 0.021600048057734966, Final Batch Loss: 0.01693703792989254\n",
      "Epoch 4053, Loss: 0.08296234346926212, Final Batch Loss: 0.05867169052362442\n",
      "Epoch 4054, Loss: 0.08124075457453728, Final Batch Loss: 0.05342986434698105\n",
      "Epoch 4055, Loss: 0.032055349089205265, Final Batch Loss: 0.02072206325829029\n",
      "Epoch 4056, Loss: 0.039935145527124405, Final Batch Loss: 0.01413046009838581\n",
      "Epoch 4057, Loss: 0.01320832222700119, Final Batch Loss: 0.0064398907124996185\n",
      "Epoch 4058, Loss: 0.019635929726064205, Final Batch Loss: 0.004551365971565247\n",
      "Epoch 4059, Loss: 0.05845421180129051, Final Batch Loss: 0.04166721925139427\n",
      "Epoch 4060, Loss: 0.020292109809815884, Final Batch Loss: 0.008128419518470764\n",
      "Epoch 4061, Loss: 0.07793907448649406, Final Batch Loss: 0.03702417388558388\n",
      "Epoch 4062, Loss: 0.04869403503835201, Final Batch Loss: 0.01836061105132103\n",
      "Epoch 4063, Loss: 0.0953718675300479, Final Batch Loss: 0.013091559521853924\n",
      "Epoch 4064, Loss: 0.05959156155586243, Final Batch Loss: 0.048556581139564514\n",
      "Epoch 4065, Loss: 0.018529781606048346, Final Batch Loss: 0.013958507217466831\n",
      "Epoch 4066, Loss: 0.07716965302824974, Final Batch Loss: 0.055596012622117996\n",
      "Epoch 4067, Loss: 0.06554647162556648, Final Batch Loss: 0.040566351264715195\n",
      "Epoch 4068, Loss: 0.025281322188675404, Final Batch Loss: 0.014437778852880001\n",
      "Epoch 4069, Loss: 0.04691660776734352, Final Batch Loss: 0.021031048148870468\n",
      "Epoch 4070, Loss: 0.049496172927320004, Final Batch Loss: 0.036938708275556564\n",
      "Epoch 4071, Loss: 0.04348452016711235, Final Batch Loss: 0.01924653723835945\n",
      "Epoch 4072, Loss: 0.023029828909784555, Final Batch Loss: 0.006495647598057985\n",
      "Epoch 4073, Loss: 0.031427924521267414, Final Batch Loss: 0.01361262146383524\n",
      "Epoch 4074, Loss: 0.05827127443626523, Final Batch Loss: 0.051490724086761475\n",
      "Epoch 4075, Loss: 0.04597430117428303, Final Batch Loss: 0.025186318904161453\n",
      "Epoch 4076, Loss: 0.02007621619850397, Final Batch Loss: 0.011460701934993267\n",
      "Epoch 4077, Loss: 0.020357508677989244, Final Batch Loss: 0.0062922644428908825\n",
      "Epoch 4078, Loss: 0.06622767355293036, Final Batch Loss: 0.051680244505405426\n",
      "Epoch 4079, Loss: 0.03733525983989239, Final Batch Loss: 0.008491862565279007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4080, Loss: 0.023865648545324802, Final Batch Loss: 0.013618160970509052\n",
      "Epoch 4081, Loss: 0.02566229202784598, Final Batch Loss: 0.002962045604363084\n",
      "Epoch 4082, Loss: 0.01305108517408371, Final Batch Loss: 0.0037185605615377426\n",
      "Epoch 4083, Loss: 0.06541675142943859, Final Batch Loss: 0.023316344246268272\n",
      "Epoch 4084, Loss: 0.0860542319715023, Final Batch Loss: 0.04241514950990677\n",
      "Epoch 4085, Loss: 0.014757668599486351, Final Batch Loss: 0.007981991395354271\n",
      "Epoch 4086, Loss: 0.0765644870698452, Final Batch Loss: 0.055017609149217606\n",
      "Epoch 4087, Loss: 0.02297295443713665, Final Batch Loss: 0.011340206488966942\n",
      "Epoch 4088, Loss: 0.06846718024462461, Final Batch Loss: 0.013580146245658398\n",
      "Epoch 4089, Loss: 0.041222840547561646, Final Batch Loss: 0.021005962044000626\n",
      "Epoch 4090, Loss: 0.04187201056629419, Final Batch Loss: 0.02753635123372078\n",
      "Epoch 4091, Loss: 0.01360058318823576, Final Batch Loss: 0.0066276188008487225\n",
      "Epoch 4092, Loss: 0.05109667778015137, Final Batch Loss: 0.023878594860434532\n",
      "Epoch 4093, Loss: 0.04509962536394596, Final Batch Loss: 0.03028707392513752\n",
      "Epoch 4094, Loss: 0.037038881331682205, Final Batch Loss: 0.010708445683121681\n",
      "Epoch 4095, Loss: 0.029556123539805412, Final Batch Loss: 0.013820337131619453\n",
      "Epoch 4096, Loss: 0.03550262749195099, Final Batch Loss: 0.026702307164669037\n",
      "Epoch 4097, Loss: 0.03494499437510967, Final Batch Loss: 0.02375076711177826\n",
      "Epoch 4098, Loss: 0.039644721895456314, Final Batch Loss: 0.029049061238765717\n",
      "Epoch 4099, Loss: 0.096986573189497, Final Batch Loss: 0.015339542180299759\n",
      "Epoch 4100, Loss: 0.0633111298084259, Final Batch Loss: 0.025711670517921448\n",
      "Epoch 4101, Loss: 0.03243531100451946, Final Batch Loss: 0.01050805114209652\n",
      "Epoch 4102, Loss: 0.04290454648435116, Final Batch Loss: 0.025501476600766182\n",
      "Epoch 4103, Loss: 0.06690872833132744, Final Batch Loss: 0.03515154495835304\n",
      "Epoch 4104, Loss: 0.05208583269268274, Final Batch Loss: 0.03984605148434639\n",
      "Epoch 4105, Loss: 0.05771100055426359, Final Batch Loss: 0.04555428400635719\n",
      "Epoch 4106, Loss: 0.1703624352812767, Final Batch Loss: 0.05097126215696335\n",
      "Epoch 4107, Loss: 0.04136652825400233, Final Batch Loss: 0.03424086421728134\n",
      "Epoch 4108, Loss: 0.15272395499050617, Final Batch Loss: 0.13228575885295868\n",
      "Epoch 4109, Loss: 0.05016837175935507, Final Batch Loss: 0.04074624553322792\n",
      "Epoch 4110, Loss: 0.10176728293299675, Final Batch Loss: 0.06348475068807602\n",
      "Epoch 4111, Loss: 0.0782031062990427, Final Batch Loss: 0.04935567453503609\n",
      "Epoch 4112, Loss: 0.04233268089592457, Final Batch Loss: 0.018539614975452423\n",
      "Epoch 4113, Loss: 0.038150825537741184, Final Batch Loss: 0.006632869131863117\n",
      "Epoch 4114, Loss: 0.054624708369374275, Final Batch Loss: 0.024202216416597366\n",
      "Epoch 4115, Loss: 0.03711681813001633, Final Batch Loss: 0.020566560328006744\n",
      "Epoch 4116, Loss: 0.075144462287426, Final Batch Loss: 0.01978234574198723\n",
      "Epoch 4117, Loss: 0.03352129086852074, Final Batch Loss: 0.01781877875328064\n",
      "Epoch 4118, Loss: 0.10630274750292301, Final Batch Loss: 0.08434687554836273\n",
      "Epoch 4119, Loss: 0.05564233474433422, Final Batch Loss: 0.022138701751828194\n",
      "Epoch 4120, Loss: 0.03491851594299078, Final Batch Loss: 0.005352969281375408\n",
      "Epoch 4121, Loss: 0.04345781356096268, Final Batch Loss: 0.011786457151174545\n",
      "Epoch 4122, Loss: 0.10421374067664146, Final Batch Loss: 0.0738254189491272\n",
      "Epoch 4123, Loss: 0.033620995469391346, Final Batch Loss: 0.007544395513832569\n",
      "Epoch 4124, Loss: 0.06437970325350761, Final Batch Loss: 0.05324283614754677\n",
      "Epoch 4125, Loss: 0.08727133180946112, Final Batch Loss: 0.015159799717366695\n",
      "Epoch 4126, Loss: 0.0158968698233366, Final Batch Loss: 0.007217404432594776\n",
      "Epoch 4127, Loss: 0.025784379802644253, Final Batch Loss: 0.010455711744725704\n",
      "Epoch 4128, Loss: 0.024184435606002808, Final Batch Loss: 0.012254690751433372\n",
      "Epoch 4129, Loss: 0.06535851862281561, Final Batch Loss: 0.055085472762584686\n",
      "Epoch 4130, Loss: 0.031614432111382484, Final Batch Loss: 0.011205075308680534\n",
      "Epoch 4131, Loss: 0.055092962458729744, Final Batch Loss: 0.02102329395711422\n",
      "Epoch 4132, Loss: 0.040133288130164146, Final Batch Loss: 0.017937958240509033\n",
      "Epoch 4133, Loss: 0.05523826740682125, Final Batch Loss: 0.016068527474999428\n",
      "Epoch 4134, Loss: 0.03703731019049883, Final Batch Loss: 0.01555145625025034\n",
      "Epoch 4135, Loss: 0.029140864964574575, Final Batch Loss: 0.02147785574197769\n",
      "Epoch 4136, Loss: 0.04116801172494888, Final Batch Loss: 0.016823982819914818\n",
      "Epoch 4137, Loss: 0.03693568613380194, Final Batch Loss: 0.015506059862673283\n",
      "Epoch 4138, Loss: 0.02643780503422022, Final Batch Loss: 0.010501154698431492\n",
      "Epoch 4139, Loss: 0.020122675225138664, Final Batch Loss: 0.008000767789781094\n",
      "Epoch 4140, Loss: 0.04222403559833765, Final Batch Loss: 0.011977645568549633\n",
      "Epoch 4141, Loss: 0.02692429069429636, Final Batch Loss: 0.011668766848742962\n",
      "Epoch 4142, Loss: 0.0290959095582366, Final Batch Loss: 0.00764507707208395\n",
      "Epoch 4143, Loss: 0.07629316300153732, Final Batch Loss: 0.04303869977593422\n",
      "Epoch 4144, Loss: 0.05051595810800791, Final Batch Loss: 0.009794616140425205\n",
      "Epoch 4145, Loss: 0.025071934796869755, Final Batch Loss: 0.006660479120910168\n",
      "Epoch 4146, Loss: 0.02539077866822481, Final Batch Loss: 0.009967065416276455\n",
      "Epoch 4147, Loss: 0.025058351457118988, Final Batch Loss: 0.009569454938173294\n",
      "Epoch 4148, Loss: 0.022992643993347883, Final Batch Loss: 0.006128575187176466\n",
      "Epoch 4149, Loss: 0.015245296061038971, Final Batch Loss: 0.002723436802625656\n",
      "Epoch 4150, Loss: 0.07248854730278254, Final Batch Loss: 0.06434626877307892\n",
      "Epoch 4151, Loss: 0.036947496235370636, Final Batch Loss: 0.021407652646303177\n",
      "Epoch 4152, Loss: 0.02603888977319002, Final Batch Loss: 0.015925144776701927\n",
      "Epoch 4153, Loss: 0.05014507845044136, Final Batch Loss: 0.030507810413837433\n",
      "Epoch 4154, Loss: 0.03912175726145506, Final Batch Loss: 0.02737133763730526\n",
      "Epoch 4155, Loss: 0.014081050641834736, Final Batch Loss: 0.009745538234710693\n",
      "Epoch 4156, Loss: 0.030708287842571735, Final Batch Loss: 0.014559830538928509\n",
      "Epoch 4157, Loss: 0.04524538945406675, Final Batch Loss: 0.03420274332165718\n",
      "Epoch 4158, Loss: 0.04563620314002037, Final Batch Loss: 0.02322457730770111\n",
      "Epoch 4159, Loss: 0.05085848830640316, Final Batch Loss: 0.02895585261285305\n",
      "Epoch 4160, Loss: 0.05276790726929903, Final Batch Loss: 0.01449992973357439\n",
      "Epoch 4161, Loss: 0.0808446267619729, Final Batch Loss: 0.07181504368782043\n",
      "Epoch 4162, Loss: 0.029365702532231808, Final Batch Loss: 0.010181690566241741\n",
      "Epoch 4163, Loss: 0.027222064323723316, Final Batch Loss: 0.011462834663689137\n",
      "Epoch 4164, Loss: 0.017071914859116077, Final Batch Loss: 0.00988425500690937\n",
      "Epoch 4165, Loss: 0.041712173260748386, Final Batch Loss: 0.032141026109457016\n",
      "Epoch 4166, Loss: 0.04591352492570877, Final Batch Loss: 0.022046471014618874\n",
      "Epoch 4167, Loss: 0.051778169348835945, Final Batch Loss: 0.020841971039772034\n",
      "Epoch 4168, Loss: 0.016890932340174913, Final Batch Loss: 0.009187446907162666\n",
      "Epoch 4169, Loss: 0.16128276009112597, Final Batch Loss: 0.1461479812860489\n",
      "Epoch 4170, Loss: 0.04866749979555607, Final Batch Loss: 0.031560271978378296\n",
      "Epoch 4171, Loss: 0.021762767108157277, Final Batch Loss: 0.0031482570338994265\n",
      "Epoch 4172, Loss: 0.019149453844875097, Final Batch Loss: 0.012238952331244946\n",
      "Epoch 4173, Loss: 0.07799888215959072, Final Batch Loss: 0.05668739229440689\n",
      "Epoch 4174, Loss: 0.036757443100214005, Final Batch Loss: 0.019330516457557678\n",
      "Epoch 4175, Loss: 0.04275648109614849, Final Batch Loss: 0.01716759242117405\n",
      "Epoch 4176, Loss: 0.03379837702959776, Final Batch Loss: 0.009541002102196217\n",
      "Epoch 4177, Loss: 0.05808120593428612, Final Batch Loss: 0.0477413535118103\n",
      "Epoch 4178, Loss: 0.038259731605648994, Final Batch Loss: 0.012620354071259499\n",
      "Epoch 4179, Loss: 0.027625052258372307, Final Batch Loss: 0.019634149968624115\n",
      "Epoch 4180, Loss: 0.024737379048019648, Final Batch Loss: 0.007804591674357653\n",
      "Epoch 4181, Loss: 0.02758712787181139, Final Batch Loss: 0.010667401365935802\n",
      "Epoch 4182, Loss: 0.04939578287303448, Final Batch Loss: 0.02599210850894451\n",
      "Epoch 4183, Loss: 0.019554640632122755, Final Batch Loss: 0.005419627297669649\n",
      "Epoch 4184, Loss: 0.05557091534137726, Final Batch Loss: 0.026999665424227715\n",
      "Epoch 4185, Loss: 0.025092306546866894, Final Batch Loss: 0.014191376976668835\n",
      "Epoch 4186, Loss: 0.05771548114717007, Final Batch Loss: 0.04746895283460617\n",
      "Epoch 4187, Loss: 0.016618459951132536, Final Batch Loss: 0.004026452545076609\n",
      "Epoch 4188, Loss: 0.0407047551125288, Final Batch Loss: 0.02053109183907509\n",
      "Epoch 4189, Loss: 0.03577706776559353, Final Batch Loss: 0.005925269797444344\n",
      "Epoch 4190, Loss: 0.04124410729855299, Final Batch Loss: 0.014532734639942646\n",
      "Epoch 4191, Loss: 0.013955124886706471, Final Batch Loss: 0.002134517068043351\n",
      "Epoch 4192, Loss: 0.03910248912870884, Final Batch Loss: 0.020686158910393715\n",
      "Epoch 4193, Loss: 0.036264908500015736, Final Batch Loss: 0.008827186189591885\n",
      "Epoch 4194, Loss: 0.01872128900140524, Final Batch Loss: 0.0027410024777054787\n",
      "Epoch 4195, Loss: 0.02069245558232069, Final Batch Loss: 0.004565608687698841\n",
      "Epoch 4196, Loss: 0.017272179014980793, Final Batch Loss: 0.005894810892641544\n",
      "Epoch 4197, Loss: 0.046897695399820805, Final Batch Loss: 0.012662199325859547\n",
      "Epoch 4198, Loss: 0.028415129403583705, Final Batch Loss: 0.0014445752603933215\n",
      "Epoch 4199, Loss: 0.02823425829410553, Final Batch Loss: 0.011995794251561165\n",
      "Epoch 4200, Loss: 0.014598680660128593, Final Batch Loss: 0.005645150318741798\n",
      "Epoch 4201, Loss: 0.053433701395988464, Final Batch Loss: 0.01738147810101509\n",
      "Epoch 4202, Loss: 0.07133987918496132, Final Batch Loss: 0.03757106885313988\n",
      "Epoch 4203, Loss: 0.019812060054391623, Final Batch Loss: 0.01344252098351717\n",
      "Epoch 4204, Loss: 0.02400998305529356, Final Batch Loss: 0.014397994615137577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4205, Loss: 0.007272833958268166, Final Batch Loss: 0.0019502080976963043\n",
      "Epoch 4206, Loss: 0.028187526389956474, Final Batch Loss: 0.020679015666246414\n",
      "Epoch 4207, Loss: 0.08379276469349861, Final Batch Loss: 0.022435542196035385\n",
      "Epoch 4208, Loss: 0.029178827069699764, Final Batch Loss: 0.009431633166968822\n",
      "Epoch 4209, Loss: 0.041927073150873184, Final Batch Loss: 0.010345645248889923\n",
      "Epoch 4210, Loss: 0.015429845079779625, Final Batch Loss: 0.007931756787002087\n",
      "Epoch 4211, Loss: 0.023404419422149658, Final Batch Loss: 0.01584930345416069\n",
      "Epoch 4212, Loss: 0.030426301062107086, Final Batch Loss: 0.008250446990132332\n",
      "Epoch 4213, Loss: 0.04146044980734587, Final Batch Loss: 0.004148795269429684\n",
      "Epoch 4214, Loss: 0.01693573035299778, Final Batch Loss: 0.00791261252015829\n",
      "Epoch 4215, Loss: 0.038876780308783054, Final Batch Loss: 0.007873251102864742\n",
      "Epoch 4216, Loss: 0.027631668373942375, Final Batch Loss: 0.0133161386474967\n",
      "Epoch 4217, Loss: 0.04379458073526621, Final Batch Loss: 0.011084836907684803\n",
      "Epoch 4218, Loss: 0.06570565840229392, Final Batch Loss: 0.005559274461120367\n",
      "Epoch 4219, Loss: 0.07851782068610191, Final Batch Loss: 0.05428864434361458\n",
      "Epoch 4220, Loss: 0.03779700957238674, Final Batch Loss: 0.012316646054387093\n",
      "Epoch 4221, Loss: 0.02285904437303543, Final Batch Loss: 0.015938514843583107\n",
      "Epoch 4222, Loss: 0.04612703435122967, Final Batch Loss: 0.029547685757279396\n",
      "Epoch 4223, Loss: 0.11122866347432137, Final Batch Loss: 0.09468815475702286\n",
      "Epoch 4224, Loss: 0.06676261778920889, Final Batch Loss: 0.053513117134571075\n",
      "Epoch 4225, Loss: 0.08418822847306728, Final Batch Loss: 0.07291237264871597\n",
      "Epoch 4226, Loss: 0.0498020239174366, Final Batch Loss: 0.031337689608335495\n",
      "Epoch 4227, Loss: 0.06426550634205341, Final Batch Loss: 0.03305719792842865\n",
      "Epoch 4228, Loss: 0.028434470295906067, Final Batch Loss: 0.01671021245419979\n",
      "Epoch 4229, Loss: 0.03884568903595209, Final Batch Loss: 0.015535605140030384\n",
      "Epoch 4230, Loss: 0.03995893895626068, Final Batch Loss: 0.020201822742819786\n",
      "Epoch 4231, Loss: 0.02622210793197155, Final Batch Loss: 0.01288707833737135\n",
      "Epoch 4232, Loss: 0.026237567886710167, Final Batch Loss: 0.01605866849422455\n",
      "Epoch 4233, Loss: 0.017214865423738956, Final Batch Loss: 0.009171231649816036\n",
      "Epoch 4234, Loss: 0.05720532685518265, Final Batch Loss: 0.02437829226255417\n",
      "Epoch 4235, Loss: 0.03431055694818497, Final Batch Loss: 0.013128885999321938\n",
      "Epoch 4236, Loss: 0.02947897557169199, Final Batch Loss: 0.024290932342410088\n",
      "Epoch 4237, Loss: 0.05353708751499653, Final Batch Loss: 0.023073943331837654\n",
      "Epoch 4238, Loss: 0.023534308187663555, Final Batch Loss: 0.008232985623180866\n",
      "Epoch 4239, Loss: 0.0293261781334877, Final Batch Loss: 0.02039031870663166\n",
      "Epoch 4240, Loss: 0.016702758381143212, Final Batch Loss: 0.013277743943035603\n",
      "Epoch 4241, Loss: 0.05576810985803604, Final Batch Loss: 0.0364435538649559\n",
      "Epoch 4242, Loss: 0.037712433375418186, Final Batch Loss: 0.02430923841893673\n",
      "Epoch 4243, Loss: 0.02019905811175704, Final Batch Loss: 0.006785073783248663\n",
      "Epoch 4244, Loss: 0.01877647591754794, Final Batch Loss: 0.011589530855417252\n",
      "Epoch 4245, Loss: 0.032214971259236336, Final Batch Loss: 0.0072225406765937805\n",
      "Epoch 4246, Loss: 0.027589425444602966, Final Batch Loss: 0.01000792533159256\n",
      "Epoch 4247, Loss: 0.054352445527911186, Final Batch Loss: 0.022867532446980476\n",
      "Epoch 4248, Loss: 0.03121726494282484, Final Batch Loss: 0.017184650525450706\n",
      "Epoch 4249, Loss: 0.010822036769241095, Final Batch Loss: 0.0049096355214715\n",
      "Epoch 4250, Loss: 0.030793524347245693, Final Batch Loss: 0.017485015094280243\n",
      "Epoch 4251, Loss: 0.013242259155958891, Final Batch Loss: 0.007580743171274662\n",
      "Epoch 4252, Loss: 0.04607129376381636, Final Batch Loss: 0.015150238759815693\n",
      "Epoch 4253, Loss: 0.023905104957520962, Final Batch Loss: 0.018119169399142265\n",
      "Epoch 4254, Loss: 0.014713037759065628, Final Batch Loss: 0.00845174677670002\n",
      "Epoch 4255, Loss: 0.08101247902959585, Final Batch Loss: 0.07170294970273972\n",
      "Epoch 4256, Loss: 0.052640125155448914, Final Batch Loss: 0.024976173415780067\n",
      "Epoch 4257, Loss: 0.0536093395203352, Final Batch Loss: 0.04335581511259079\n",
      "Epoch 4258, Loss: 0.017344395630061626, Final Batch Loss: 0.011141989380121231\n",
      "Epoch 4259, Loss: 0.018281645141541958, Final Batch Loss: 0.00908575113862753\n",
      "Epoch 4260, Loss: 0.04438092978671193, Final Batch Loss: 0.006521981675177813\n",
      "Epoch 4261, Loss: 0.01910408679395914, Final Batch Loss: 0.013322742655873299\n",
      "Epoch 4262, Loss: 0.021113981492817402, Final Batch Loss: 0.01397155411541462\n",
      "Epoch 4263, Loss: 0.02560755144804716, Final Batch Loss: 0.010201618075370789\n",
      "Epoch 4264, Loss: 0.023607174400240183, Final Batch Loss: 0.005005358252674341\n",
      "Epoch 4265, Loss: 0.04582032561302185, Final Batch Loss: 0.03443855047225952\n",
      "Epoch 4266, Loss: 0.03308723308146, Final Batch Loss: 0.017467927187681198\n",
      "Epoch 4267, Loss: 0.03151078289374709, Final Batch Loss: 0.0036086547188460827\n",
      "Epoch 4268, Loss: 0.040987880900502205, Final Batch Loss: 0.02673696167767048\n",
      "Epoch 4269, Loss: 0.013270593248307705, Final Batch Loss: 0.007699376437813044\n",
      "Epoch 4270, Loss: 0.020538300275802612, Final Batch Loss: 0.014072197489440441\n",
      "Epoch 4271, Loss: 0.028600772842764854, Final Batch Loss: 0.009421294555068016\n",
      "Epoch 4272, Loss: 0.01066714571788907, Final Batch Loss: 0.005594698712229729\n",
      "Epoch 4273, Loss: 0.039707453921437263, Final Batch Loss: 0.01618761196732521\n",
      "Epoch 4274, Loss: 0.05431005172431469, Final Batch Loss: 0.03754008188843727\n",
      "Epoch 4275, Loss: 0.04538380913436413, Final Batch Loss: 0.02004156820476055\n",
      "Epoch 4276, Loss: 0.02168558444827795, Final Batch Loss: 0.008200394921004772\n",
      "Epoch 4277, Loss: 0.014600387774407864, Final Batch Loss: 0.006202888675034046\n",
      "Epoch 4278, Loss: 0.010594629566185176, Final Batch Loss: 0.001549878972582519\n",
      "Epoch 4279, Loss: 0.0813864478841424, Final Batch Loss: 0.004463725723326206\n",
      "Epoch 4280, Loss: 0.032198318280279636, Final Batch Loss: 0.017483055591583252\n",
      "Epoch 4281, Loss: 0.038256941363215446, Final Batch Loss: 0.02007211558520794\n",
      "Epoch 4282, Loss: 0.018969382159411907, Final Batch Loss: 0.012755383737385273\n",
      "Epoch 4283, Loss: 0.016621601302176714, Final Batch Loss: 0.005956222768872976\n",
      "Epoch 4284, Loss: 0.04591384902596474, Final Batch Loss: 0.02247282862663269\n",
      "Epoch 4285, Loss: 0.020089610945433378, Final Batch Loss: 0.006572323385626078\n",
      "Epoch 4286, Loss: 0.009732203790917993, Final Batch Loss: 0.0025604756083339453\n",
      "Epoch 4287, Loss: 0.037414371967315674, Final Batch Loss: 0.0171628687530756\n",
      "Epoch 4288, Loss: 0.10489652678370476, Final Batch Loss: 0.07286441326141357\n",
      "Epoch 4289, Loss: 0.017129831481724977, Final Batch Loss: 0.004188112448900938\n",
      "Epoch 4290, Loss: 0.023374291136860847, Final Batch Loss: 0.012123308144509792\n",
      "Epoch 4291, Loss: 0.022493879310786724, Final Batch Loss: 0.013609657995402813\n",
      "Epoch 4292, Loss: 0.02029771450906992, Final Batch Loss: 0.007555841468274593\n",
      "Epoch 4293, Loss: 0.039009652100503445, Final Batch Loss: 0.011860202066600323\n",
      "Epoch 4294, Loss: 0.024744677357375622, Final Batch Loss: 0.015892518684267998\n",
      "Epoch 4295, Loss: 0.024989116936922073, Final Batch Loss: 0.013950629159808159\n",
      "Epoch 4296, Loss: 0.038717334158718586, Final Batch Loss: 0.014551707543432713\n",
      "Epoch 4297, Loss: 0.036907329224050045, Final Batch Loss: 0.015284507535398006\n",
      "Epoch 4298, Loss: 0.03109609242528677, Final Batch Loss: 0.021222753450274467\n",
      "Epoch 4299, Loss: 0.025339413434267044, Final Batch Loss: 0.006270246580243111\n",
      "Epoch 4300, Loss: 0.06946941465139389, Final Batch Loss: 0.05672510340809822\n",
      "Epoch 4301, Loss: 0.06767029222100973, Final Batch Loss: 0.011505014263093472\n",
      "Epoch 4302, Loss: 0.025178167037665844, Final Batch Loss: 0.007916024886071682\n",
      "Epoch 4303, Loss: 0.06006545294076204, Final Batch Loss: 0.013476886786520481\n",
      "Epoch 4304, Loss: 0.05189778655767441, Final Batch Loss: 0.03866182267665863\n",
      "Epoch 4305, Loss: 0.061244431883096695, Final Batch Loss: 0.041173167526721954\n",
      "Epoch 4306, Loss: 0.03622164949774742, Final Batch Loss: 0.020844968035817146\n",
      "Epoch 4307, Loss: 0.04652752075344324, Final Batch Loss: 0.03370896354317665\n",
      "Epoch 4308, Loss: 0.03494849521666765, Final Batch Loss: 0.023922355845570564\n",
      "Epoch 4309, Loss: 0.027586379554122686, Final Batch Loss: 0.006948863621801138\n",
      "Epoch 4310, Loss: 0.041870782151818275, Final Batch Loss: 0.03186718001961708\n",
      "Epoch 4311, Loss: 0.13859893567860126, Final Batch Loss: 0.010202528908848763\n",
      "Epoch 4312, Loss: 0.044781727716326714, Final Batch Loss: 0.006923025473952293\n",
      "Epoch 4313, Loss: 0.029599863104522228, Final Batch Loss: 0.01942400634288788\n",
      "Epoch 4314, Loss: 0.026280091144144535, Final Batch Loss: 0.01859728991985321\n",
      "Epoch 4315, Loss: 0.036120783537626266, Final Batch Loss: 0.012444255873560905\n",
      "Epoch 4316, Loss: 0.02614487335085869, Final Batch Loss: 0.017986556515097618\n",
      "Epoch 4317, Loss: 0.03611965989693999, Final Batch Loss: 0.006882767658680677\n",
      "Epoch 4318, Loss: 0.02467288263142109, Final Batch Loss: 0.01636112481355667\n",
      "Epoch 4319, Loss: 0.04045749455690384, Final Batch Loss: 0.027621274814009666\n",
      "Epoch 4320, Loss: 0.022372321225702763, Final Batch Loss: 0.013467201963067055\n",
      "Epoch 4321, Loss: 0.033697428181767464, Final Batch Loss: 0.009377073496580124\n",
      "Epoch 4322, Loss: 0.07444128021597862, Final Batch Loss: 0.02927352488040924\n",
      "Epoch 4323, Loss: 0.06368595082312822, Final Batch Loss: 0.010591068305075169\n",
      "Epoch 4324, Loss: 0.02702634921297431, Final Batch Loss: 0.005387046840041876\n",
      "Epoch 4325, Loss: 0.07186875492334366, Final Batch Loss: 0.051568564027547836\n",
      "Epoch 4326, Loss: 0.0641968478448689, Final Batch Loss: 0.05962483957409859\n",
      "Epoch 4327, Loss: 0.03141774330288172, Final Batch Loss: 0.026303794234991074\n",
      "Epoch 4328, Loss: 0.0607596468180418, Final Batch Loss: 0.017428690567612648\n",
      "Epoch 4329, Loss: 0.03234764561057091, Final Batch Loss: 0.011383885517716408\n",
      "Epoch 4330, Loss: 0.02405231725424528, Final Batch Loss: 0.012470807880163193\n",
      "Epoch 4331, Loss: 0.0370799545198679, Final Batch Loss: 0.010187996551394463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4332, Loss: 0.01325383735820651, Final Batch Loss: 0.00583706283941865\n",
      "Epoch 4333, Loss: 0.0228629014454782, Final Batch Loss: 0.005884153302758932\n",
      "Epoch 4334, Loss: 0.029668021947145462, Final Batch Loss: 0.017184432595968246\n",
      "Epoch 4335, Loss: 0.038795772939920425, Final Batch Loss: 0.021447407081723213\n",
      "Epoch 4336, Loss: 0.03378269076347351, Final Batch Loss: 0.018133455887436867\n",
      "Epoch 4337, Loss: 0.03704134188592434, Final Batch Loss: 0.012019921094179153\n",
      "Epoch 4338, Loss: 0.019203013740479946, Final Batch Loss: 0.008593449369072914\n",
      "Epoch 4339, Loss: 0.03978313319385052, Final Batch Loss: 0.022367915138602257\n",
      "Epoch 4340, Loss: 0.02053544484078884, Final Batch Loss: 0.014587565325200558\n",
      "Epoch 4341, Loss: 0.03668744210153818, Final Batch Loss: 0.005965285934507847\n",
      "Epoch 4342, Loss: 0.051017940044403076, Final Batch Loss: 0.022023145109415054\n",
      "Epoch 4343, Loss: 0.027627533301711082, Final Batch Loss: 0.014871117658913136\n",
      "Epoch 4344, Loss: 0.02370414324104786, Final Batch Loss: 0.009858139790594578\n",
      "Epoch 4345, Loss: 0.02443459164351225, Final Batch Loss: 0.005707147531211376\n",
      "Epoch 4346, Loss: 0.0464346413500607, Final Batch Loss: 0.007101707626134157\n",
      "Epoch 4347, Loss: 0.037216894794255495, Final Batch Loss: 0.0040796478278934956\n",
      "Epoch 4348, Loss: 0.020203829742968082, Final Batch Loss: 0.014553569257259369\n",
      "Epoch 4349, Loss: 0.023992130532860756, Final Batch Loss: 0.01458670012652874\n",
      "Epoch 4350, Loss: 0.09203638881444931, Final Batch Loss: 0.040051426738500595\n",
      "Epoch 4351, Loss: 0.03298487327992916, Final Batch Loss: 0.022475412115454674\n",
      "Epoch 4352, Loss: 0.022693699225783348, Final Batch Loss: 0.00969710573554039\n",
      "Epoch 4353, Loss: 0.051020339131355286, Final Batch Loss: 0.021667862311005592\n",
      "Epoch 4354, Loss: 0.032945478335022926, Final Batch Loss: 0.014418411999940872\n",
      "Epoch 4355, Loss: 0.03745164442807436, Final Batch Loss: 0.033392343670129776\n",
      "Epoch 4356, Loss: 0.014510510955005884, Final Batch Loss: 0.007384016644209623\n",
      "Epoch 4357, Loss: 0.0714986976236105, Final Batch Loss: 0.045083656907081604\n",
      "Epoch 4358, Loss: 0.023177051451057196, Final Batch Loss: 0.004755967762321234\n",
      "Epoch 4359, Loss: 0.049933611415326595, Final Batch Loss: 0.007762308232486248\n",
      "Epoch 4360, Loss: 0.019055981654673815, Final Batch Loss: 0.0076616988517344\n",
      "Epoch 4361, Loss: 0.04217611253261566, Final Batch Loss: 0.023368701338768005\n",
      "Epoch 4362, Loss: 0.05673861596733332, Final Batch Loss: 0.007193182595074177\n",
      "Epoch 4363, Loss: 0.021218170877546072, Final Batch Loss: 0.0047685797326266766\n",
      "Epoch 4364, Loss: 0.054222848266363144, Final Batch Loss: 0.027633395045995712\n",
      "Epoch 4365, Loss: 0.04379437863826752, Final Batch Loss: 0.020613133907318115\n",
      "Epoch 4366, Loss: 0.062207503244280815, Final Batch Loss: 0.0344531312584877\n",
      "Epoch 4367, Loss: 0.05958372540771961, Final Batch Loss: 0.02528914250433445\n",
      "Epoch 4368, Loss: 0.024985685478895903, Final Batch Loss: 0.004403212573379278\n",
      "Epoch 4369, Loss: 0.023476445581763983, Final Batch Loss: 0.005820683669298887\n",
      "Epoch 4370, Loss: 0.02262918371707201, Final Batch Loss: 0.01812056638300419\n",
      "Epoch 4371, Loss: 0.06560171768069267, Final Batch Loss: 0.02783196046948433\n",
      "Epoch 4372, Loss: 0.03996888268738985, Final Batch Loss: 0.012185431085526943\n",
      "Epoch 4373, Loss: 0.0288218823261559, Final Batch Loss: 0.021331651136279106\n",
      "Epoch 4374, Loss: 0.04820223804563284, Final Batch Loss: 0.0154761066660285\n",
      "Epoch 4375, Loss: 0.08641269337385893, Final Batch Loss: 0.07566799968481064\n",
      "Epoch 4376, Loss: 0.03308242792263627, Final Batch Loss: 0.004012834746390581\n",
      "Epoch 4377, Loss: 0.05282910354435444, Final Batch Loss: 0.019263720139861107\n",
      "Epoch 4378, Loss: 0.06474525667726994, Final Batch Loss: 0.03560268506407738\n",
      "Epoch 4379, Loss: 0.06768728233873844, Final Batch Loss: 0.010832386091351509\n",
      "Epoch 4380, Loss: 0.04404947999864817, Final Batch Loss: 0.011295509524643421\n",
      "Epoch 4381, Loss: 0.039314985275268555, Final Batch Loss: 0.017300302162766457\n",
      "Epoch 4382, Loss: 0.0699538616463542, Final Batch Loss: 0.010177060030400753\n",
      "Epoch 4383, Loss: 0.031032142229378223, Final Batch Loss: 0.01407452393323183\n",
      "Epoch 4384, Loss: 0.044672515243291855, Final Batch Loss: 0.026817895472049713\n",
      "Epoch 4385, Loss: 0.03835521172732115, Final Batch Loss: 0.013939942233264446\n",
      "Epoch 4386, Loss: 0.05990725941956043, Final Batch Loss: 0.00520726852118969\n",
      "Epoch 4387, Loss: 0.04097096249461174, Final Batch Loss: 0.018370958045125008\n",
      "Epoch 4388, Loss: 0.031760492362082005, Final Batch Loss: 0.01357307005673647\n",
      "Epoch 4389, Loss: 0.08989546820521355, Final Batch Loss: 0.06090103089809418\n",
      "Epoch 4390, Loss: 0.04827379062771797, Final Batch Loss: 0.016275938600301743\n",
      "Epoch 4391, Loss: 0.017230618745088577, Final Batch Loss: 0.010734295472502708\n",
      "Epoch 4392, Loss: 0.08231103792786598, Final Batch Loss: 0.06418418884277344\n",
      "Epoch 4393, Loss: 0.10134655237197876, Final Batch Loss: 0.03189340978860855\n",
      "Epoch 4394, Loss: 0.03517620638012886, Final Batch Loss: 0.026973232626914978\n",
      "Epoch 4395, Loss: 0.0959639772772789, Final Batch Loss: 0.0504276268184185\n",
      "Epoch 4396, Loss: 0.022632909007370472, Final Batch Loss: 0.013163608498871326\n",
      "Epoch 4397, Loss: 0.02527558710426092, Final Batch Loss: 0.009210471995174885\n",
      "Epoch 4398, Loss: 0.05591721646487713, Final Batch Loss: 0.0255479346960783\n",
      "Epoch 4399, Loss: 0.10557909682393074, Final Batch Loss: 0.032564591616392136\n",
      "Epoch 4400, Loss: 0.06431879289448261, Final Batch Loss: 0.026174521073698997\n",
      "Epoch 4401, Loss: 0.019366114400327206, Final Batch Loss: 0.009129095822572708\n",
      "Epoch 4402, Loss: 0.02792146522551775, Final Batch Loss: 0.018492383882403374\n",
      "Epoch 4403, Loss: 0.0423029875382781, Final Batch Loss: 0.027491960674524307\n",
      "Epoch 4404, Loss: 0.05562270153313875, Final Batch Loss: 0.011837617494165897\n",
      "Epoch 4405, Loss: 0.029219800606369972, Final Batch Loss: 0.02485502138733864\n",
      "Epoch 4406, Loss: 0.045999715104699135, Final Batch Loss: 0.01866488717496395\n",
      "Epoch 4407, Loss: 0.10632273647934198, Final Batch Loss: 0.0956779196858406\n",
      "Epoch 4408, Loss: 0.03264456428587437, Final Batch Loss: 0.010479925200343132\n",
      "Epoch 4409, Loss: 0.03572709113359451, Final Batch Loss: 0.020053552463650703\n",
      "Epoch 4410, Loss: 0.02444811351597309, Final Batch Loss: 0.008925720117986202\n",
      "Epoch 4411, Loss: 0.029678705148398876, Final Batch Loss: 0.009443857707083225\n",
      "Epoch 4412, Loss: 0.020056293345987797, Final Batch Loss: 0.0055609289556741714\n",
      "Epoch 4413, Loss: 0.02718156622722745, Final Batch Loss: 0.005368847865611315\n",
      "Epoch 4414, Loss: 0.07260053139179945, Final Batch Loss: 0.013332328759133816\n",
      "Epoch 4415, Loss: 0.042974271811544895, Final Batch Loss: 0.027803415432572365\n",
      "Epoch 4416, Loss: 0.0665551982820034, Final Batch Loss: 0.048915617167949677\n",
      "Epoch 4417, Loss: 0.021165694575756788, Final Batch Loss: 0.006851393263787031\n",
      "Epoch 4418, Loss: 0.032958499155938625, Final Batch Loss: 0.021786708384752274\n",
      "Epoch 4419, Loss: 0.03051262814551592, Final Batch Loss: 0.01181593257933855\n",
      "Epoch 4420, Loss: 0.05524207092821598, Final Batch Loss: 0.025991078466176987\n",
      "Epoch 4421, Loss: 0.028814449906349182, Final Batch Loss: 0.009734585881233215\n",
      "Epoch 4422, Loss: 0.023176653310656548, Final Batch Loss: 0.004097562283277512\n",
      "Epoch 4423, Loss: 0.027520500123500824, Final Batch Loss: 0.008189521729946136\n",
      "Epoch 4424, Loss: 0.046079088002443314, Final Batch Loss: 0.021712876856327057\n",
      "Epoch 4425, Loss: 0.019546933472156525, Final Batch Loss: 0.008139352314174175\n",
      "Epoch 4426, Loss: 0.07802608795464039, Final Batch Loss: 0.005873585119843483\n",
      "Epoch 4427, Loss: 0.09888058993965387, Final Batch Loss: 0.00943829957395792\n",
      "Epoch 4428, Loss: 0.025645808316767216, Final Batch Loss: 0.015219340100884438\n",
      "Epoch 4429, Loss: 0.014634051360189915, Final Batch Loss: 0.006637183018028736\n",
      "Epoch 4430, Loss: 0.044564235489815474, Final Batch Loss: 0.007422077935189009\n",
      "Epoch 4431, Loss: 0.04309689812362194, Final Batch Loss: 0.029286423698067665\n",
      "Epoch 4432, Loss: 0.030754299834370613, Final Batch Loss: 0.014477267861366272\n",
      "Epoch 4433, Loss: 0.04207530268467963, Final Batch Loss: 0.003201809013262391\n",
      "Epoch 4434, Loss: 0.02537585748359561, Final Batch Loss: 0.003529386129230261\n",
      "Epoch 4435, Loss: 0.025128181092441082, Final Batch Loss: 0.014238528907299042\n",
      "Epoch 4436, Loss: 0.02849059132859111, Final Batch Loss: 0.024109411984682083\n",
      "Epoch 4437, Loss: 0.060446847230196, Final Batch Loss: 0.021553073078393936\n",
      "Epoch 4438, Loss: 0.024780001491308212, Final Batch Loss: 0.01607741415500641\n",
      "Epoch 4439, Loss: 0.025952471420168877, Final Batch Loss: 0.0017656460404396057\n",
      "Epoch 4440, Loss: 0.03824969846755266, Final Batch Loss: 0.030147939920425415\n",
      "Epoch 4441, Loss: 0.06740612536668777, Final Batch Loss: 0.04702146723866463\n",
      "Epoch 4442, Loss: 0.045519549399614334, Final Batch Loss: 0.007044993340969086\n",
      "Epoch 4443, Loss: 0.07250336557626724, Final Batch Loss: 0.06093686819076538\n",
      "Epoch 4444, Loss: 0.051895915530622005, Final Batch Loss: 0.038993071764707565\n",
      "Epoch 4445, Loss: 0.04176938347518444, Final Batch Loss: 0.019425295293331146\n",
      "Epoch 4446, Loss: 0.044885434210300446, Final Batch Loss: 0.029172590002417564\n",
      "Epoch 4447, Loss: 0.11349799856543541, Final Batch Loss: 0.06343015283346176\n",
      "Epoch 4448, Loss: 0.06168896611779928, Final Batch Loss: 0.04820946604013443\n",
      "Epoch 4449, Loss: 0.1160227470099926, Final Batch Loss: 0.06868261843919754\n",
      "Epoch 4450, Loss: 0.06976660341024399, Final Batch Loss: 0.018125463277101517\n",
      "Epoch 4451, Loss: 0.09780050069093704, Final Batch Loss: 0.04900803044438362\n",
      "Epoch 4452, Loss: 0.16953758895397186, Final Batch Loss: 0.1151948869228363\n",
      "Epoch 4453, Loss: 0.07302305288612843, Final Batch Loss: 0.018421271815896034\n",
      "Epoch 4454, Loss: 0.04882189631462097, Final Batch Loss: 0.02912423014640808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4455, Loss: 0.017919134348630905, Final Batch Loss: 0.010616322048008442\n",
      "Epoch 4456, Loss: 0.040352968499064445, Final Batch Loss: 0.012738391757011414\n",
      "Epoch 4457, Loss: 0.04414774850010872, Final Batch Loss: 0.025274518877267838\n",
      "Epoch 4458, Loss: 0.04907034523785114, Final Batch Loss: 0.01593058370053768\n",
      "Epoch 4459, Loss: 0.03226874303072691, Final Batch Loss: 0.009626205079257488\n",
      "Epoch 4460, Loss: 0.02924242615699768, Final Batch Loss: 0.019157839938998222\n",
      "Epoch 4461, Loss: 0.034338819328695536, Final Batch Loss: 0.007067656610161066\n",
      "Epoch 4462, Loss: 0.026708331890404224, Final Batch Loss: 0.0164694394916296\n",
      "Epoch 4463, Loss: 0.020437635481357574, Final Batch Loss: 0.008490446023643017\n",
      "Epoch 4464, Loss: 0.09139195084571838, Final Batch Loss: 0.0572940930724144\n",
      "Epoch 4465, Loss: 0.032039803452789783, Final Batch Loss: 0.018078884109854698\n",
      "Epoch 4466, Loss: 0.03379750810563564, Final Batch Loss: 0.020697137340903282\n",
      "Epoch 4467, Loss: 0.023407948203384876, Final Batch Loss: 0.016391316428780556\n",
      "Epoch 4468, Loss: 0.023988387547433376, Final Batch Loss: 0.010433274321258068\n",
      "Epoch 4469, Loss: 0.04188933223485947, Final Batch Loss: 0.02435682713985443\n",
      "Epoch 4470, Loss: 0.04092440754175186, Final Batch Loss: 0.030044039711356163\n",
      "Epoch 4471, Loss: 0.04801958426833153, Final Batch Loss: 0.022193586453795433\n",
      "Epoch 4472, Loss: 0.05570529215037823, Final Batch Loss: 0.046160317957401276\n",
      "Epoch 4473, Loss: 0.19817611016333103, Final Batch Loss: 0.011076236143708229\n",
      "Epoch 4474, Loss: 0.06267633102834225, Final Batch Loss: 0.028491782024502754\n",
      "Epoch 4475, Loss: 0.03070895466953516, Final Batch Loss: 0.01184102799743414\n",
      "Epoch 4476, Loss: 0.03338790126144886, Final Batch Loss: 0.01843257248401642\n",
      "Epoch 4477, Loss: 0.04523799195885658, Final Batch Loss: 0.019218910485506058\n",
      "Epoch 4478, Loss: 0.05756388232111931, Final Batch Loss: 0.037969738245010376\n",
      "Epoch 4479, Loss: 0.13245384767651558, Final Batch Loss: 0.07224370539188385\n",
      "Epoch 4480, Loss: 0.04585217032581568, Final Batch Loss: 0.040076687932014465\n",
      "Epoch 4481, Loss: 0.04648139886558056, Final Batch Loss: 0.02681560069322586\n",
      "Epoch 4482, Loss: 0.03404219914227724, Final Batch Loss: 0.01254983525723219\n",
      "Epoch 4483, Loss: 0.02462194673717022, Final Batch Loss: 0.012600346468389034\n",
      "Epoch 4484, Loss: 0.07082321867346764, Final Batch Loss: 0.05294085294008255\n",
      "Epoch 4485, Loss: 0.041024625301361084, Final Batch Loss: 0.009534310549497604\n",
      "Epoch 4486, Loss: 0.06019803695380688, Final Batch Loss: 0.03456602245569229\n",
      "Epoch 4487, Loss: 0.04398067854344845, Final Batch Loss: 0.017208067700266838\n",
      "Epoch 4488, Loss: 0.04879622720181942, Final Batch Loss: 0.023606235161423683\n",
      "Epoch 4489, Loss: 0.021775816567242146, Final Batch Loss: 0.010067208670079708\n",
      "Epoch 4490, Loss: 0.04468412324786186, Final Batch Loss: 0.016439318656921387\n",
      "Epoch 4491, Loss: 0.050752684473991394, Final Batch Loss: 0.02898869290947914\n",
      "Epoch 4492, Loss: 0.034432632848620415, Final Batch Loss: 0.01648985594511032\n",
      "Epoch 4493, Loss: 0.009831106988713145, Final Batch Loss: 0.0064248922280967236\n",
      "Epoch 4494, Loss: 0.0937549602240324, Final Batch Loss: 0.06867104023694992\n",
      "Epoch 4495, Loss: 0.046106571331620216, Final Batch Loss: 0.03659113124012947\n",
      "Epoch 4496, Loss: 0.06147231627255678, Final Batch Loss: 0.04802657663822174\n",
      "Epoch 4497, Loss: 0.05674542672932148, Final Batch Loss: 0.03055712953209877\n",
      "Epoch 4498, Loss: 0.05028577148914337, Final Batch Loss: 0.02053016610443592\n",
      "Epoch 4499, Loss: 0.06496391072869301, Final Batch Loss: 0.018175818026065826\n",
      "Epoch 4500, Loss: 0.04390263557434082, Final Batch Loss: 0.01184813678264618\n",
      "Epoch 4501, Loss: 0.03070299094542861, Final Batch Loss: 0.007236931007355452\n",
      "Epoch 4502, Loss: 0.08355848118662834, Final Batch Loss: 0.04886334761977196\n",
      "Epoch 4503, Loss: 0.06569994427263737, Final Batch Loss: 0.043492503464221954\n",
      "Epoch 4504, Loss: 0.03854811191558838, Final Batch Loss: 0.01870580203831196\n",
      "Epoch 4505, Loss: 0.12884242087602615, Final Batch Loss: 0.06387421488761902\n",
      "Epoch 4506, Loss: 0.02974600810557604, Final Batch Loss: 0.01596810482442379\n",
      "Epoch 4507, Loss: 0.0433876421302557, Final Batch Loss: 0.015993280336260796\n",
      "Epoch 4508, Loss: 0.05970027297735214, Final Batch Loss: 0.03446182981133461\n",
      "Epoch 4509, Loss: 0.07856987230479717, Final Batch Loss: 0.061042241752147675\n",
      "Epoch 4510, Loss: 0.009540305938571692, Final Batch Loss: 0.003934246022254229\n",
      "Epoch 4511, Loss: 0.039636521600186825, Final Batch Loss: 0.011203677393496037\n",
      "Epoch 4512, Loss: 0.02963323052972555, Final Batch Loss: 0.02167871780693531\n",
      "Epoch 4513, Loss: 0.08395051956176758, Final Batch Loss: 0.0414808914065361\n",
      "Epoch 4514, Loss: 0.03356717526912689, Final Batch Loss: 0.016510101035237312\n",
      "Epoch 4515, Loss: 0.03635613713413477, Final Batch Loss: 0.011488710530102253\n",
      "Epoch 4516, Loss: 0.024279987439513206, Final Batch Loss: 0.014229729771614075\n",
      "Epoch 4517, Loss: 0.030142770148813725, Final Batch Loss: 0.02313937433063984\n",
      "Epoch 4518, Loss: 0.02376050502061844, Final Batch Loss: 0.00918789766728878\n",
      "Epoch 4519, Loss: 0.049601816572248936, Final Batch Loss: 0.040052853524684906\n",
      "Epoch 4520, Loss: 0.018736300989985466, Final Batch Loss: 0.008052748627960682\n",
      "Epoch 4521, Loss: 0.0380228515714407, Final Batch Loss: 0.01644737832248211\n",
      "Epoch 4522, Loss: 0.05043887160718441, Final Batch Loss: 0.014531733468174934\n",
      "Epoch 4523, Loss: 0.057623645290732384, Final Batch Loss: 0.023856615647673607\n",
      "Epoch 4524, Loss: 0.022534299176186323, Final Batch Loss: 0.007498393300920725\n",
      "Epoch 4525, Loss: 0.10526088159531355, Final Batch Loss: 0.014316308312118053\n",
      "Epoch 4526, Loss: 0.08090456016361713, Final Batch Loss: 0.05819094553589821\n",
      "Epoch 4527, Loss: 0.028235248290002346, Final Batch Loss: 0.018415825441479683\n",
      "Epoch 4528, Loss: 0.08565004169940948, Final Batch Loss: 0.049328092485666275\n",
      "Epoch 4529, Loss: 0.030491114128381014, Final Batch Loss: 0.006242525298148394\n",
      "Epoch 4530, Loss: 0.07765079289674759, Final Batch Loss: 0.019437801092863083\n",
      "Epoch 4531, Loss: 0.023047376424074173, Final Batch Loss: 0.011501040309667587\n",
      "Epoch 4532, Loss: 0.03269517980515957, Final Batch Loss: 0.005455233156681061\n",
      "Epoch 4533, Loss: 0.052576519548892975, Final Batch Loss: 0.030189257115125656\n",
      "Epoch 4534, Loss: 0.030606691725552082, Final Batch Loss: 0.016697613522410393\n",
      "Epoch 4535, Loss: 0.08087574318051338, Final Batch Loss: 0.05847454071044922\n",
      "Epoch 4536, Loss: 0.021269632503390312, Final Batch Loss: 0.012825469486415386\n",
      "Epoch 4537, Loss: 0.058049703016877174, Final Batch Loss: 0.01799737848341465\n",
      "Epoch 4538, Loss: 0.09144696220755577, Final Batch Loss: 0.027936454862356186\n",
      "Epoch 4539, Loss: 0.03262662515044212, Final Batch Loss: 0.015258969739079475\n",
      "Epoch 4540, Loss: 0.05120687931776047, Final Batch Loss: 0.04033990576863289\n",
      "Epoch 4541, Loss: 0.017801864072680473, Final Batch Loss: 0.007038041017949581\n",
      "Epoch 4542, Loss: 0.04336254484951496, Final Batch Loss: 0.025705719366669655\n",
      "Epoch 4543, Loss: 0.03733550291508436, Final Batch Loss: 0.023382822051644325\n",
      "Epoch 4544, Loss: 0.0601681973785162, Final Batch Loss: 0.04730933904647827\n",
      "Epoch 4545, Loss: 0.0313265984877944, Final Batch Loss: 0.021025775000452995\n",
      "Epoch 4546, Loss: 0.027629070915281773, Final Batch Loss: 0.013510521501302719\n",
      "Epoch 4547, Loss: 0.0342525914311409, Final Batch Loss: 0.006739309057593346\n",
      "Epoch 4548, Loss: 0.03516429662704468, Final Batch Loss: 0.017777612432837486\n",
      "Epoch 4549, Loss: 0.03752914257347584, Final Batch Loss: 0.012098491191864014\n",
      "Epoch 4550, Loss: 0.0166586353443563, Final Batch Loss: 0.008970026858150959\n",
      "Epoch 4551, Loss: 0.05640926770865917, Final Batch Loss: 0.048897966742515564\n",
      "Epoch 4552, Loss: 0.0222573378123343, Final Batch Loss: 0.016448335722088814\n",
      "Epoch 4553, Loss: 0.05265253968536854, Final Batch Loss: 0.020798610523343086\n",
      "Epoch 4554, Loss: 0.02051430381834507, Final Batch Loss: 0.008547528646886349\n",
      "Epoch 4555, Loss: 0.03519293013960123, Final Batch Loss: 0.023545581847429276\n",
      "Epoch 4556, Loss: 0.018121841363608837, Final Batch Loss: 0.008877048268914223\n",
      "Epoch 4557, Loss: 0.018152179196476936, Final Batch Loss: 0.009676527231931686\n",
      "Epoch 4558, Loss: 0.047466645017266273, Final Batch Loss: 0.020098673179745674\n",
      "Epoch 4559, Loss: 0.031666142866015434, Final Batch Loss: 0.02430552802979946\n",
      "Epoch 4560, Loss: 0.016836061142385006, Final Batch Loss: 0.013154844753444195\n",
      "Epoch 4561, Loss: 0.014871977735310793, Final Batch Loss: 0.010996216908097267\n",
      "Epoch 4562, Loss: 0.01859084377065301, Final Batch Loss: 0.00250657694414258\n",
      "Epoch 4563, Loss: 0.01569618657231331, Final Batch Loss: 0.003364996984601021\n",
      "Epoch 4564, Loss: 0.016327986726537347, Final Batch Loss: 0.0026368473190814257\n",
      "Epoch 4565, Loss: 0.03540443116798997, Final Batch Loss: 0.0065897428430616856\n",
      "Epoch 4566, Loss: 0.029314251616597176, Final Batch Loss: 0.008487753570079803\n",
      "Epoch 4567, Loss: 0.019345838576555252, Final Batch Loss: 0.008788620121777058\n",
      "Epoch 4568, Loss: 0.013143857475370169, Final Batch Loss: 0.006090332288295031\n",
      "Epoch 4569, Loss: 0.011638267897069454, Final Batch Loss: 0.005072725936770439\n",
      "Epoch 4570, Loss: 0.03950063697993755, Final Batch Loss: 0.009174779057502747\n",
      "Epoch 4571, Loss: 0.025740341283380985, Final Batch Loss: 0.017706919461488724\n",
      "Epoch 4572, Loss: 0.019685121718794107, Final Batch Loss: 0.015889128670096397\n",
      "Epoch 4573, Loss: 0.0206495001912117, Final Batch Loss: 0.01330313365906477\n",
      "Epoch 4574, Loss: 0.03989233449101448, Final Batch Loss: 0.01909184269607067\n",
      "Epoch 4575, Loss: 0.006060409592464566, Final Batch Loss: 0.0020636587869375944\n",
      "Epoch 4576, Loss: 0.008331795688718557, Final Batch Loss: 0.005143496207892895\n",
      "Epoch 4577, Loss: 0.0273702722042799, Final Batch Loss: 0.013331633992493153\n",
      "Epoch 4578, Loss: 0.009109530365094543, Final Batch Loss: 0.0037698957603424788\n",
      "Epoch 4579, Loss: 0.036870818585157394, Final Batch Loss: 0.01941935531795025\n",
      "Epoch 4580, Loss: 0.07375099789351225, Final Batch Loss: 0.05938384681940079\n",
      "Epoch 4581, Loss: 0.022785698994994164, Final Batch Loss: 0.016065550968050957\n",
      "Epoch 4582, Loss: 0.03244279325008392, Final Batch Loss: 0.02829829417169094\n",
      "Epoch 4583, Loss: 0.04975538095459342, Final Batch Loss: 0.004816075321286917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4584, Loss: 0.047984507866203785, Final Batch Loss: 0.009898719377815723\n",
      "Epoch 4585, Loss: 0.08642193209379911, Final Batch Loss: 0.07655610144138336\n",
      "Epoch 4586, Loss: 0.024935791734606028, Final Batch Loss: 0.017544208094477654\n",
      "Epoch 4587, Loss: 0.02002248540520668, Final Batch Loss: 0.007995090447366238\n",
      "Epoch 4588, Loss: 0.020105189410969615, Final Batch Loss: 0.0028137441258877516\n",
      "Epoch 4589, Loss: 0.019922682782635093, Final Batch Loss: 0.0025163625832647085\n",
      "Epoch 4590, Loss: 0.01770854229107499, Final Batch Loss: 0.007099922280758619\n",
      "Epoch 4591, Loss: 0.036578984931111336, Final Batch Loss: 0.01890934444963932\n",
      "Epoch 4592, Loss: 0.04627311509102583, Final Batch Loss: 0.009660339914262295\n",
      "Epoch 4593, Loss: 0.033776319585740566, Final Batch Loss: 0.015116414986550808\n",
      "Epoch 4594, Loss: 0.021016107872128487, Final Batch Loss: 0.01624767668545246\n",
      "Epoch 4595, Loss: 0.016596265602856874, Final Batch Loss: 0.0058038742281496525\n",
      "Epoch 4596, Loss: 0.10497775487601757, Final Batch Loss: 0.07474520057439804\n",
      "Epoch 4597, Loss: 0.06861025001853704, Final Batch Loss: 0.06106096878647804\n",
      "Epoch 4598, Loss: 0.022927584126591682, Final Batch Loss: 0.011769108474254608\n",
      "Epoch 4599, Loss: 0.027342782355844975, Final Batch Loss: 0.022250866517424583\n",
      "Epoch 4600, Loss: 0.03419458819553256, Final Batch Loss: 0.02772625908255577\n",
      "Epoch 4601, Loss: 0.0191453630104661, Final Batch Loss: 0.010472806170582771\n",
      "Epoch 4602, Loss: 0.015867949929088354, Final Batch Loss: 0.003144314978271723\n",
      "Epoch 4603, Loss: 0.05756849981844425, Final Batch Loss: 0.02440294437110424\n",
      "Epoch 4604, Loss: 0.029116048477590084, Final Batch Loss: 0.01169282104820013\n",
      "Epoch 4605, Loss: 0.03799176961183548, Final Batch Loss: 0.0266619473695755\n",
      "Epoch 4606, Loss: 0.00923101813532412, Final Batch Loss: 0.0025939519982784986\n",
      "Epoch 4607, Loss: 0.027296440210193396, Final Batch Loss: 0.005205371882766485\n",
      "Epoch 4608, Loss: 0.01902652345597744, Final Batch Loss: 0.011444256640970707\n",
      "Epoch 4609, Loss: 0.025592241901904345, Final Batch Loss: 0.01954212784767151\n",
      "Epoch 4610, Loss: 0.012823062017560005, Final Batch Loss: 0.005646987818181515\n",
      "Epoch 4611, Loss: 0.060110410675406456, Final Batch Loss: 0.021855534985661507\n",
      "Epoch 4612, Loss: 0.019962824881076813, Final Batch Loss: 0.011935091577470303\n",
      "Epoch 4613, Loss: 0.03147319704294205, Final Batch Loss: 0.009966479614377022\n",
      "Epoch 4614, Loss: 0.02996397390961647, Final Batch Loss: 0.01255359873175621\n",
      "Epoch 4615, Loss: 0.02531265513971448, Final Batch Loss: 0.019093072041869164\n",
      "Epoch 4616, Loss: 0.05996751133352518, Final Batch Loss: 0.05126632750034332\n",
      "Epoch 4617, Loss: 0.050420116633176804, Final Batch Loss: 0.02056233212351799\n",
      "Epoch 4618, Loss: 0.010522516444325447, Final Batch Loss: 0.004451416432857513\n",
      "Epoch 4619, Loss: 0.04120833706110716, Final Batch Loss: 0.03353866562247276\n",
      "Epoch 4620, Loss: 0.02786319889128208, Final Batch Loss: 0.0027312804013490677\n",
      "Epoch 4621, Loss: 0.07573525980114937, Final Batch Loss: 0.038352251052856445\n",
      "Epoch 4622, Loss: 0.03841869253665209, Final Batch Loss: 0.026342179626226425\n",
      "Epoch 4623, Loss: 0.02634852845221758, Final Batch Loss: 0.019145645201206207\n",
      "Epoch 4624, Loss: 0.05178745882585645, Final Batch Loss: 0.045952532440423965\n",
      "Epoch 4625, Loss: 0.07272283174097538, Final Batch Loss: 0.04581507295370102\n",
      "Epoch 4626, Loss: 0.028305099811404943, Final Batch Loss: 0.02074071392416954\n",
      "Epoch 4627, Loss: 0.026004448533058167, Final Batch Loss: 0.00863516703248024\n",
      "Epoch 4628, Loss: 0.025151217356324196, Final Batch Loss: 0.016914283856749535\n",
      "Epoch 4629, Loss: 0.03721657209098339, Final Batch Loss: 0.009005596861243248\n",
      "Epoch 4630, Loss: 0.017252156510949135, Final Batch Loss: 0.009476454928517342\n",
      "Epoch 4631, Loss: 0.03585156425833702, Final Batch Loss: 0.018108656629920006\n",
      "Epoch 4632, Loss: 0.13481489196419716, Final Batch Loss: 0.05399642512202263\n",
      "Epoch 4633, Loss: 0.02115190145559609, Final Batch Loss: 0.002834021346643567\n",
      "Epoch 4634, Loss: 0.021013204474002123, Final Batch Loss: 0.0052945599891245365\n",
      "Epoch 4635, Loss: 0.03168886713683605, Final Batch Loss: 0.013056827709078789\n",
      "Epoch 4636, Loss: 0.04766203463077545, Final Batch Loss: 0.04254705831408501\n",
      "Epoch 4637, Loss: 0.01013160776346922, Final Batch Loss: 0.00571292033419013\n",
      "Epoch 4638, Loss: 0.03836238384246826, Final Batch Loss: 0.03551432117819786\n",
      "Epoch 4639, Loss: 0.05483243428170681, Final Batch Loss: 0.03479108214378357\n",
      "Epoch 4640, Loss: 0.059333224315196276, Final Batch Loss: 0.0075433445163071156\n",
      "Epoch 4641, Loss: 0.024086914025247097, Final Batch Loss: 0.010309703648090363\n",
      "Epoch 4642, Loss: 0.038146921433508396, Final Batch Loss: 0.010518684051930904\n",
      "Epoch 4643, Loss: 0.040624234825372696, Final Batch Loss: 0.023876197636127472\n",
      "Epoch 4644, Loss: 0.05785861611366272, Final Batch Loss: 0.04306299611926079\n",
      "Epoch 4645, Loss: 0.01497966842725873, Final Batch Loss: 0.009371296502649784\n",
      "Epoch 4646, Loss: 0.019532627891749144, Final Batch Loss: 0.006058029364794493\n",
      "Epoch 4647, Loss: 0.033764131367206573, Final Batch Loss: 0.02518048882484436\n",
      "Epoch 4648, Loss: 0.02540365792810917, Final Batch Loss: 0.0038713887333869934\n",
      "Epoch 4649, Loss: 0.014897002838551998, Final Batch Loss: 0.005328437313437462\n",
      "Epoch 4650, Loss: 0.020083246752619743, Final Batch Loss: 0.016372790560126305\n",
      "Epoch 4651, Loss: 0.018078246153891087, Final Batch Loss: 0.008874638006091118\n",
      "Epoch 4652, Loss: 0.019087912514805794, Final Batch Loss: 0.012575279921293259\n",
      "Epoch 4653, Loss: 0.019498296082019806, Final Batch Loss: 0.010296533815562725\n",
      "Epoch 4654, Loss: 0.035651933401823044, Final Batch Loss: 0.013786718249320984\n",
      "Epoch 4655, Loss: 0.014539365191012621, Final Batch Loss: 0.00405478710308671\n",
      "Epoch 4656, Loss: 0.01938761305063963, Final Batch Loss: 0.009123462252318859\n",
      "Epoch 4657, Loss: 0.0493091344833374, Final Batch Loss: 0.02273484878242016\n",
      "Epoch 4658, Loss: 0.011221318505704403, Final Batch Loss: 0.0060742637142539024\n",
      "Epoch 4659, Loss: 0.04635641444474459, Final Batch Loss: 0.031204668805003166\n",
      "Epoch 4660, Loss: 0.04140040650963783, Final Batch Loss: 0.017033208161592484\n",
      "Epoch 4661, Loss: 0.18035551626235247, Final Batch Loss: 0.16516122221946716\n",
      "Epoch 4662, Loss: 0.043020350858569145, Final Batch Loss: 0.029803533107042313\n",
      "Epoch 4663, Loss: 0.04041768470779061, Final Batch Loss: 0.0029081800021231174\n",
      "Epoch 4664, Loss: 0.2832508245483041, Final Batch Loss: 0.27203238010406494\n",
      "Epoch 4665, Loss: 0.13240571692585945, Final Batch Loss: 0.08544942736625671\n",
      "Epoch 4666, Loss: 0.0670620296150446, Final Batch Loss: 0.0535106360912323\n",
      "Epoch 4667, Loss: 0.0516388937830925, Final Batch Loss: 0.01828920468688011\n",
      "Epoch 4668, Loss: 0.06790818460285664, Final Batch Loss: 0.04044032096862793\n",
      "Epoch 4669, Loss: 0.028809589333832264, Final Batch Loss: 0.012794583104550838\n",
      "Epoch 4670, Loss: 0.02929494809359312, Final Batch Loss: 0.009061840362846851\n",
      "Epoch 4671, Loss: 0.03953405376523733, Final Batch Loss: 0.032797921448946\n",
      "Epoch 4672, Loss: 0.023904655128717422, Final Batch Loss: 0.006583210080862045\n",
      "Epoch 4673, Loss: 0.04798201657831669, Final Batch Loss: 0.030060332268476486\n",
      "Epoch 4674, Loss: 0.07148316130042076, Final Batch Loss: 0.06345319747924805\n",
      "Epoch 4675, Loss: 0.04103238694369793, Final Batch Loss: 0.028752604499459267\n",
      "Epoch 4676, Loss: 0.04733100067824125, Final Batch Loss: 0.014003845863044262\n",
      "Epoch 4677, Loss: 0.03995875082910061, Final Batch Loss: 0.016157036647200584\n",
      "Epoch 4678, Loss: 0.05626648850739002, Final Batch Loss: 0.01875361241400242\n",
      "Epoch 4679, Loss: 0.04687756113708019, Final Batch Loss: 0.025704069063067436\n",
      "Epoch 4680, Loss: 0.026930682361125946, Final Batch Loss: 0.016578659415245056\n",
      "Epoch 4681, Loss: 0.023683365434408188, Final Batch Loss: 0.012161895632743835\n",
      "Epoch 4682, Loss: 0.06676144525408745, Final Batch Loss: 0.03454053774476051\n",
      "Epoch 4683, Loss: 0.04164078366011381, Final Batch Loss: 0.00716832559555769\n",
      "Epoch 4684, Loss: 0.031409977935254574, Final Batch Loss: 0.01938520185649395\n",
      "Epoch 4685, Loss: 0.021288384683430195, Final Batch Loss: 0.013264570385217667\n",
      "Epoch 4686, Loss: 0.025162140373140574, Final Batch Loss: 0.020391207188367844\n",
      "Epoch 4687, Loss: 0.02358085149899125, Final Batch Loss: 0.005457686726003885\n",
      "Epoch 4688, Loss: 0.0380717720836401, Final Batch Loss: 0.017830433323979378\n",
      "Epoch 4689, Loss: 0.06813568249344826, Final Batch Loss: 0.025032460689544678\n",
      "Epoch 4690, Loss: 0.04957718774676323, Final Batch Loss: 0.03246380761265755\n",
      "Epoch 4691, Loss: 0.05421099439263344, Final Batch Loss: 0.016199391335248947\n",
      "Epoch 4692, Loss: 0.05127917043864727, Final Batch Loss: 0.013286257162690163\n",
      "Epoch 4693, Loss: 0.03208618704229593, Final Batch Loss: 0.008068778552114964\n",
      "Epoch 4694, Loss: 0.029970867559313774, Final Batch Loss: 0.007901078090071678\n",
      "Epoch 4695, Loss: 0.015837065875530243, Final Batch Loss: 0.003922194242477417\n",
      "Epoch 4696, Loss: 0.030675689689815044, Final Batch Loss: 0.016779262572526932\n",
      "Epoch 4697, Loss: 0.04726343788206577, Final Batch Loss: 0.018041357398033142\n",
      "Epoch 4698, Loss: 0.03680828586220741, Final Batch Loss: 0.020423229783773422\n",
      "Epoch 4699, Loss: 0.04340498149394989, Final Batch Loss: 0.010616321116685867\n",
      "Epoch 4700, Loss: 0.038935067132115364, Final Batch Loss: 0.03396321088075638\n",
      "Epoch 4701, Loss: 0.01881221542134881, Final Batch Loss: 0.006999324541538954\n",
      "Epoch 4702, Loss: 0.021745329722762108, Final Batch Loss: 0.017220189794898033\n",
      "Epoch 4703, Loss: 0.03330734185874462, Final Batch Loss: 0.009125344455242157\n",
      "Epoch 4704, Loss: 0.10419989936053753, Final Batch Loss: 0.08707929402589798\n",
      "Epoch 4705, Loss: 0.050723932683467865, Final Batch Loss: 0.038757968693971634\n",
      "Epoch 4706, Loss: 0.037567839957773685, Final Batch Loss: 0.02505953423678875\n",
      "Epoch 4707, Loss: 0.05097195692360401, Final Batch Loss: 0.01594998501241207\n",
      "Epoch 4708, Loss: 0.04349642992019653, Final Batch Loss: 0.016016708686947823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4709, Loss: 0.05457543767988682, Final Batch Loss: 0.030700217932462692\n",
      "Epoch 4710, Loss: 0.1254978198558092, Final Batch Loss: 0.10619841516017914\n",
      "Epoch 4711, Loss: 0.06969907879829407, Final Batch Loss: 0.03320928290486336\n",
      "Epoch 4712, Loss: 0.06006777845323086, Final Batch Loss: 0.0196573194116354\n",
      "Epoch 4713, Loss: 0.10344504751265049, Final Batch Loss: 0.07229568809270859\n",
      "Epoch 4714, Loss: 0.01751907868310809, Final Batch Loss: 0.005881347227841616\n",
      "Epoch 4715, Loss: 0.03346668928861618, Final Batch Loss: 0.02586039900779724\n",
      "Epoch 4716, Loss: 0.04204650968313217, Final Batch Loss: 0.017534198239445686\n",
      "Epoch 4717, Loss: 0.03243145905435085, Final Batch Loss: 0.017937257885932922\n",
      "Epoch 4718, Loss: 0.046511994674801826, Final Batch Loss: 0.021954620257019997\n",
      "Epoch 4719, Loss: 0.02394349197857082, Final Batch Loss: 0.003753502620384097\n",
      "Epoch 4720, Loss: 0.06292704399675131, Final Batch Loss: 0.055830832570791245\n",
      "Epoch 4721, Loss: 0.037749330047518015, Final Batch Loss: 0.03311401978135109\n",
      "Epoch 4722, Loss: 0.015062225051224232, Final Batch Loss: 0.009590916335582733\n",
      "Epoch 4723, Loss: 0.023465799167752266, Final Batch Loss: 0.011117981746792793\n",
      "Epoch 4724, Loss: 0.06859720032662153, Final Batch Loss: 0.013811065815389156\n",
      "Epoch 4725, Loss: 0.024725780822336674, Final Batch Loss: 0.00923422072082758\n",
      "Epoch 4726, Loss: 0.019385731779038906, Final Batch Loss: 0.009415525011718273\n",
      "Epoch 4727, Loss: 0.007785847410559654, Final Batch Loss: 0.002559136599302292\n",
      "Epoch 4728, Loss: 0.03455776907503605, Final Batch Loss: 0.020080963149666786\n",
      "Epoch 4729, Loss: 0.01824118895456195, Final Batch Loss: 0.014052873477339745\n",
      "Epoch 4730, Loss: 0.04363171849399805, Final Batch Loss: 0.03651484474539757\n",
      "Epoch 4731, Loss: 0.03331913612782955, Final Batch Loss: 0.02359674498438835\n",
      "Epoch 4732, Loss: 0.046308794524520636, Final Batch Loss: 0.006447186227887869\n",
      "Epoch 4733, Loss: 0.013775566592812538, Final Batch Loss: 0.00287054106593132\n",
      "Epoch 4734, Loss: 0.02338120900094509, Final Batch Loss: 0.0062851328402757645\n",
      "Epoch 4735, Loss: 0.03996903728693724, Final Batch Loss: 0.029538627713918686\n",
      "Epoch 4736, Loss: 0.012537633534520864, Final Batch Loss: 0.0070634991861879826\n",
      "Epoch 4737, Loss: 0.03980771265923977, Final Batch Loss: 0.02335280552506447\n",
      "Epoch 4738, Loss: 0.036231521517038345, Final Batch Loss: 0.020562229678034782\n",
      "Epoch 4739, Loss: 0.02173156477510929, Final Batch Loss: 0.004565829411149025\n",
      "Epoch 4740, Loss: 0.025790957733988762, Final Batch Loss: 0.01585155911743641\n",
      "Epoch 4741, Loss: 0.03614746453240514, Final Batch Loss: 0.03150163218379021\n",
      "Epoch 4742, Loss: 0.02003486268222332, Final Batch Loss: 0.011500866152346134\n",
      "Epoch 4743, Loss: 0.06647936534136534, Final Batch Loss: 0.011879618279635906\n",
      "Epoch 4744, Loss: 0.050061129964888096, Final Batch Loss: 0.012655281461775303\n",
      "Epoch 4745, Loss: 0.03443348780274391, Final Batch Loss: 0.009038170799612999\n",
      "Epoch 4746, Loss: 0.048992035910487175, Final Batch Loss: 0.02823500521481037\n",
      "Epoch 4747, Loss: 0.03204428032040596, Final Batch Loss: 0.021030666306614876\n",
      "Epoch 4748, Loss: 0.0416504368185997, Final Batch Loss: 0.013753848150372505\n",
      "Epoch 4749, Loss: 0.14810260385274887, Final Batch Loss: 0.11327064782381058\n",
      "Epoch 4750, Loss: 0.0883325356990099, Final Batch Loss: 0.021479347720742226\n",
      "Epoch 4751, Loss: 0.046895790845155716, Final Batch Loss: 0.03294391930103302\n",
      "Epoch 4752, Loss: 0.04244723450392485, Final Batch Loss: 0.029442043974995613\n",
      "Epoch 4753, Loss: 0.040400592144578695, Final Batch Loss: 0.006079902406781912\n",
      "Epoch 4754, Loss: 0.020278424490243196, Final Batch Loss: 0.005220768507570028\n",
      "Epoch 4755, Loss: 0.018186489585787058, Final Batch Loss: 0.013931733556091785\n",
      "Epoch 4756, Loss: 0.027877547778189182, Final Batch Loss: 0.014948333613574505\n",
      "Epoch 4757, Loss: 0.032127450220286846, Final Batch Loss: 0.01520723756402731\n",
      "Epoch 4758, Loss: 0.0411820150911808, Final Batch Loss: 0.03576628491282463\n",
      "Epoch 4759, Loss: 0.05248517356812954, Final Batch Loss: 0.02812129072844982\n",
      "Epoch 4760, Loss: 0.028466765768826008, Final Batch Loss: 0.005105334334075451\n",
      "Epoch 4761, Loss: 0.07259150967001915, Final Batch Loss: 0.029163461178541183\n",
      "Epoch 4762, Loss: 0.014789351727813482, Final Batch Loss: 0.006244292948395014\n",
      "Epoch 4763, Loss: 0.0215124087408185, Final Batch Loss: 0.00405274610966444\n",
      "Epoch 4764, Loss: 0.0182040105573833, Final Batch Loss: 0.004764185752719641\n",
      "Epoch 4765, Loss: 0.049852361902594566, Final Batch Loss: 0.03355247154831886\n",
      "Epoch 4766, Loss: 0.082129567861557, Final Batch Loss: 0.06796225905418396\n",
      "Epoch 4767, Loss: 0.03525707824155688, Final Batch Loss: 0.03021968901157379\n",
      "Epoch 4768, Loss: 0.018762960098683834, Final Batch Loss: 0.011377888731658459\n",
      "Epoch 4769, Loss: 0.03998793102800846, Final Batch Loss: 0.01474391296505928\n",
      "Epoch 4770, Loss: 0.017802271991968155, Final Batch Loss: 0.005415046587586403\n",
      "Epoch 4771, Loss: 0.16222234070301056, Final Batch Loss: 0.1341114640235901\n",
      "Epoch 4772, Loss: 0.069190863519907, Final Batch Loss: 0.0447654202580452\n",
      "Epoch 4773, Loss: 0.019946761429309845, Final Batch Loss: 0.011765501461923122\n",
      "Epoch 4774, Loss: 0.0212613339535892, Final Batch Loss: 0.0017931866459548473\n",
      "Epoch 4775, Loss: 0.06425710022449493, Final Batch Loss: 0.04587510600686073\n",
      "Epoch 4776, Loss: 0.06610345467925072, Final Batch Loss: 0.03212319314479828\n",
      "Epoch 4777, Loss: 0.07474156469106674, Final Batch Loss: 0.05749068781733513\n",
      "Epoch 4778, Loss: 0.02340555191040039, Final Batch Loss: 0.012474359013140202\n",
      "Epoch 4779, Loss: 0.046306731179356575, Final Batch Loss: 0.03760132938623428\n",
      "Epoch 4780, Loss: 0.033228432992473245, Final Batch Loss: 0.0022491796407848597\n",
      "Epoch 4781, Loss: 0.01835806597955525, Final Batch Loss: 0.003562161000445485\n",
      "Epoch 4782, Loss: 0.04081757739186287, Final Batch Loss: 0.016234947368502617\n",
      "Epoch 4783, Loss: 0.03004694078117609, Final Batch Loss: 0.016109347343444824\n",
      "Epoch 4784, Loss: 0.015547073446214199, Final Batch Loss: 0.009598854929208755\n",
      "Epoch 4785, Loss: 0.10962753742933273, Final Batch Loss: 0.045060932636260986\n",
      "Epoch 4786, Loss: 0.026419990696012974, Final Batch Loss: 0.009069443680346012\n",
      "Epoch 4787, Loss: 0.056855713948607445, Final Batch Loss: 0.03159463033080101\n",
      "Epoch 4788, Loss: 0.06836959905922413, Final Batch Loss: 0.011540761217474937\n",
      "Epoch 4789, Loss: 0.06520517356693745, Final Batch Loss: 0.044251058250665665\n",
      "Epoch 4790, Loss: 0.10747035592794418, Final Batch Loss: 0.05234302952885628\n",
      "Epoch 4791, Loss: 0.0446254494599998, Final Batch Loss: 0.0041703092865645885\n",
      "Epoch 4792, Loss: 0.02070211013779044, Final Batch Loss: 0.006793920416384935\n",
      "Epoch 4793, Loss: 0.0506035927683115, Final Batch Loss: 0.005283026024699211\n",
      "Epoch 4794, Loss: 0.13230948522686958, Final Batch Loss: 0.07519000768661499\n",
      "Epoch 4795, Loss: 0.10256476234644651, Final Batch Loss: 0.08857027441263199\n",
      "Epoch 4796, Loss: 0.04196693189442158, Final Batch Loss: 0.016598530113697052\n",
      "Epoch 4797, Loss: 0.026715069077908993, Final Batch Loss: 0.014754370786249638\n",
      "Epoch 4798, Loss: 0.026931041851639748, Final Batch Loss: 0.016102753579616547\n",
      "Epoch 4799, Loss: 0.010458123404532671, Final Batch Loss: 0.006774109322577715\n",
      "Epoch 4800, Loss: 0.14097653329372406, Final Batch Loss: 0.10042718797922134\n",
      "Epoch 4801, Loss: 0.027775266207754612, Final Batch Loss: 0.009366332553327084\n",
      "Epoch 4802, Loss: 0.04216335155069828, Final Batch Loss: 0.019709743559360504\n",
      "Epoch 4803, Loss: 0.03462480613961816, Final Batch Loss: 0.005021548364311457\n",
      "Epoch 4804, Loss: 0.04984768107533455, Final Batch Loss: 0.024006668478250504\n",
      "Epoch 4805, Loss: 0.04445567913353443, Final Batch Loss: 0.006953297182917595\n",
      "Epoch 4806, Loss: 0.04759679548442364, Final Batch Loss: 0.027538681402802467\n",
      "Epoch 4807, Loss: 0.0185584076680243, Final Batch Loss: 0.002619864884763956\n",
      "Epoch 4808, Loss: 0.019076031632721424, Final Batch Loss: 0.0130390590056777\n",
      "Epoch 4809, Loss: 0.03891490213572979, Final Batch Loss: 0.025679413229227066\n",
      "Epoch 4810, Loss: 0.0344207426533103, Final Batch Loss: 0.018841125071048737\n",
      "Epoch 4811, Loss: 0.03496517986059189, Final Batch Loss: 0.02139982394874096\n",
      "Epoch 4812, Loss: 0.05653398856520653, Final Batch Loss: 0.0184481143951416\n",
      "Epoch 4813, Loss: 0.03332033660262823, Final Batch Loss: 0.01051711943000555\n",
      "Epoch 4814, Loss: 0.02714014146476984, Final Batch Loss: 0.018884018063545227\n",
      "Epoch 4815, Loss: 0.07085710018873215, Final Batch Loss: 0.05436252802610397\n",
      "Epoch 4816, Loss: 0.03514617122709751, Final Batch Loss: 0.010947944596409798\n",
      "Epoch 4817, Loss: 0.0877062319777906, Final Batch Loss: 0.0053392513655126095\n",
      "Epoch 4818, Loss: 0.02701617870479822, Final Batch Loss: 0.006536089815199375\n",
      "Epoch 4819, Loss: 0.034143936820328236, Final Batch Loss: 0.027048513293266296\n",
      "Epoch 4820, Loss: 0.017496447078883648, Final Batch Loss: 0.009320095181465149\n",
      "Epoch 4821, Loss: 0.0277128703892231, Final Batch Loss: 0.01333185937255621\n",
      "Epoch 4822, Loss: 0.018419775180518627, Final Batch Loss: 0.008401894010603428\n",
      "Epoch 4823, Loss: 0.019369962625205517, Final Batch Loss: 0.01396967563778162\n",
      "Epoch 4824, Loss: 0.03253621514886618, Final Batch Loss: 0.024473775178194046\n",
      "Epoch 4825, Loss: 0.030781135894358158, Final Batch Loss: 0.005973680876195431\n",
      "Epoch 4826, Loss: 0.01648569293320179, Final Batch Loss: 0.011748463846743107\n",
      "Epoch 4827, Loss: 0.01600624341517687, Final Batch Loss: 0.010547966696321964\n",
      "Epoch 4828, Loss: 0.021720075979828835, Final Batch Loss: 0.012863096781075\n",
      "Epoch 4829, Loss: 0.05736512504518032, Final Batch Loss: 0.030240073800086975\n",
      "Epoch 4830, Loss: 0.025999938137829304, Final Batch Loss: 0.007680411450564861\n",
      "Epoch 4831, Loss: 0.028012149967253208, Final Batch Loss: 0.011396064423024654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4832, Loss: 0.02355324011296034, Final Batch Loss: 0.018265368416905403\n",
      "Epoch 4833, Loss: 0.03132936870679259, Final Batch Loss: 0.004906463902443647\n",
      "Epoch 4834, Loss: 0.027224750258028507, Final Batch Loss: 0.01527849305421114\n",
      "Epoch 4835, Loss: 0.012308421079069376, Final Batch Loss: 0.005507605616003275\n",
      "Epoch 4836, Loss: 0.04352802410721779, Final Batch Loss: 0.02209542691707611\n",
      "Epoch 4837, Loss: 0.012592292157933116, Final Batch Loss: 0.00361334509216249\n",
      "Epoch 4838, Loss: 0.009033648297190666, Final Batch Loss: 0.0019895853474736214\n",
      "Epoch 4839, Loss: 0.010928768664598465, Final Batch Loss: 0.005773213226348162\n",
      "Epoch 4840, Loss: 0.013443947304040194, Final Batch Loss: 0.007863808423280716\n",
      "Epoch 4841, Loss: 0.015902969054877758, Final Batch Loss: 0.005210468545556068\n",
      "Epoch 4842, Loss: 0.005459067644551396, Final Batch Loss: 0.00224092579446733\n",
      "Epoch 4843, Loss: 0.024826097302138805, Final Batch Loss: 0.012607057578861713\n",
      "Epoch 4844, Loss: 0.021139467135071754, Final Batch Loss: 0.009741978719830513\n",
      "Epoch 4845, Loss: 0.022852858528494835, Final Batch Loss: 0.017352240160107613\n",
      "Epoch 4846, Loss: 0.02597114071249962, Final Batch Loss: 0.011126378551125526\n",
      "Epoch 4847, Loss: 0.014563890639692545, Final Batch Loss: 0.003371299710124731\n",
      "Epoch 4848, Loss: 0.0441966038197279, Final Batch Loss: 0.024280158802866936\n",
      "Epoch 4849, Loss: 0.013095814734697342, Final Batch Loss: 0.004957917146384716\n",
      "Epoch 4850, Loss: 0.03944637160748243, Final Batch Loss: 0.030510269105434418\n",
      "Epoch 4851, Loss: 0.04334617219865322, Final Batch Loss: 0.022675691172480583\n",
      "Epoch 4852, Loss: 0.026304728351533413, Final Batch Loss: 0.0030911164358258247\n",
      "Epoch 4853, Loss: 0.012675280682742596, Final Batch Loss: 0.004863188602030277\n",
      "Epoch 4854, Loss: 0.06429950147867203, Final Batch Loss: 0.04324180260300636\n",
      "Epoch 4855, Loss: 0.06224551424384117, Final Batch Loss: 0.02666495367884636\n",
      "Epoch 4856, Loss: 0.02377872448414564, Final Batch Loss: 0.006981705315411091\n",
      "Epoch 4857, Loss: 0.02657877281308174, Final Batch Loss: 0.013307124376296997\n",
      "Epoch 4858, Loss: 0.03484712168574333, Final Batch Loss: 0.016276616603136063\n",
      "Epoch 4859, Loss: 0.02383339125663042, Final Batch Loss: 0.007838794030249119\n",
      "Epoch 4860, Loss: 0.03209585230797529, Final Batch Loss: 0.007805940695106983\n",
      "Epoch 4861, Loss: 0.06066783983260393, Final Batch Loss: 0.048177629709243774\n",
      "Epoch 4862, Loss: 0.01354854553937912, Final Batch Loss: 0.005135205574333668\n",
      "Epoch 4863, Loss: 0.05807301215827465, Final Batch Loss: 0.012860530987381935\n",
      "Epoch 4864, Loss: 0.05585949681699276, Final Batch Loss: 0.03750103712081909\n",
      "Epoch 4865, Loss: 0.05685870349407196, Final Batch Loss: 0.042997512966394424\n",
      "Epoch 4866, Loss: 0.07230012398213148, Final Batch Loss: 0.00868562888354063\n",
      "Epoch 4867, Loss: 0.011291160015389323, Final Batch Loss: 0.007442748174071312\n",
      "Epoch 4868, Loss: 0.08870953321456909, Final Batch Loss: 0.03570716455578804\n",
      "Epoch 4869, Loss: 0.049049077555537224, Final Batch Loss: 0.0225700493901968\n",
      "Epoch 4870, Loss: 0.1140289418399334, Final Batch Loss: 0.08399885147809982\n",
      "Epoch 4871, Loss: 0.1450190730392933, Final Batch Loss: 0.12543533742427826\n",
      "Epoch 4872, Loss: 0.06654648762196302, Final Batch Loss: 0.005108184181153774\n",
      "Epoch 4873, Loss: 0.023130700923502445, Final Batch Loss: 0.014837685972452164\n",
      "Epoch 4874, Loss: 0.16636881232261658, Final Batch Loss: 0.12604624032974243\n",
      "Epoch 4875, Loss: 0.018112141638994217, Final Batch Loss: 0.008814577013254166\n",
      "Epoch 4876, Loss: 0.12453191727399826, Final Batch Loss: 0.03319648653268814\n",
      "Epoch 4877, Loss: 0.15978863462805748, Final Batch Loss: 0.043088000267744064\n",
      "Epoch 4878, Loss: 0.08976862579584122, Final Batch Loss: 0.04160696268081665\n",
      "Epoch 4879, Loss: 0.053818242624402046, Final Batch Loss: 0.03126787394285202\n",
      "Epoch 4880, Loss: 0.10924812033772469, Final Batch Loss: 0.07293730229139328\n",
      "Epoch 4881, Loss: 0.05713432841002941, Final Batch Loss: 0.03268054500222206\n",
      "Epoch 4882, Loss: 0.05433920957148075, Final Batch Loss: 0.025531096383929253\n",
      "Epoch 4883, Loss: 0.04192586150020361, Final Batch Loss: 0.009082662872970104\n",
      "Epoch 4884, Loss: 0.026552747935056686, Final Batch Loss: 0.010087795555591583\n",
      "Epoch 4885, Loss: 0.04523046314716339, Final Batch Loss: 0.022383680567145348\n",
      "Epoch 4886, Loss: 0.06349807046353817, Final Batch Loss: 0.02679392136633396\n",
      "Epoch 4887, Loss: 0.12068912014365196, Final Batch Loss: 0.012019578367471695\n",
      "Epoch 4888, Loss: 0.04432159475982189, Final Batch Loss: 0.023451490327715874\n",
      "Epoch 4889, Loss: 0.0345852579921484, Final Batch Loss: 0.013045240193605423\n",
      "Epoch 4890, Loss: 0.059681324288249016, Final Batch Loss: 0.029375381767749786\n",
      "Epoch 4891, Loss: 0.04286472871899605, Final Batch Loss: 0.008696869015693665\n",
      "Epoch 4892, Loss: 0.05875818990170956, Final Batch Loss: 0.031855259090662\n",
      "Epoch 4893, Loss: 0.061354879289865494, Final Batch Loss: 0.045231446623802185\n",
      "Epoch 4894, Loss: 0.05329076759517193, Final Batch Loss: 0.016116896644234657\n",
      "Epoch 4895, Loss: 0.044107573106884956, Final Batch Loss: 0.021712293848395348\n",
      "Epoch 4896, Loss: 0.03748259320855141, Final Batch Loss: 0.02027801424264908\n",
      "Epoch 4897, Loss: 0.10022595804184675, Final Batch Loss: 0.09067559987306595\n",
      "Epoch 4898, Loss: 0.016403660643845797, Final Batch Loss: 0.005219861399382353\n",
      "Epoch 4899, Loss: 0.03415412362664938, Final Batch Loss: 0.008566361851990223\n",
      "Epoch 4900, Loss: 0.03164396248757839, Final Batch Loss: 0.008467335253953934\n",
      "Epoch 4901, Loss: 0.08576296269893646, Final Batch Loss: 0.025368820875883102\n",
      "Epoch 4902, Loss: 0.03011212032288313, Final Batch Loss: 0.015396094880998135\n",
      "Epoch 4903, Loss: 0.02666733507066965, Final Batch Loss: 0.012991661205887794\n",
      "Epoch 4904, Loss: 0.056734926998615265, Final Batch Loss: 0.01914896070957184\n",
      "Epoch 4905, Loss: 0.06933825463056564, Final Batch Loss: 0.026968475431203842\n",
      "Epoch 4906, Loss: 0.04393315687775612, Final Batch Loss: 0.010633885860443115\n",
      "Epoch 4907, Loss: 0.0930809210985899, Final Batch Loss: 0.07009311020374298\n",
      "Epoch 4908, Loss: 0.04129771888256073, Final Batch Loss: 0.01878191903233528\n",
      "Epoch 4909, Loss: 0.0497633945196867, Final Batch Loss: 0.03375754505395889\n",
      "Epoch 4910, Loss: 0.07634126394987106, Final Batch Loss: 0.04274965822696686\n",
      "Epoch 4911, Loss: 0.06789724715054035, Final Batch Loss: 0.012366754934191704\n",
      "Epoch 4912, Loss: 0.07121423166245222, Final Batch Loss: 0.007974798791110516\n",
      "Epoch 4913, Loss: 0.03358778590336442, Final Batch Loss: 0.004076099488884211\n",
      "Epoch 4914, Loss: 0.019253131933510303, Final Batch Loss: 0.008612725883722305\n",
      "Epoch 4915, Loss: 0.034245018381625414, Final Batch Loss: 0.005392559338361025\n",
      "Epoch 4916, Loss: 0.034847261384129524, Final Batch Loss: 0.01648472622036934\n",
      "Epoch 4917, Loss: 0.09381268359720707, Final Batch Loss: 0.08356261998414993\n",
      "Epoch 4918, Loss: 0.06877524312585592, Final Batch Loss: 0.010407286696135998\n",
      "Epoch 4919, Loss: 0.07654573721811175, Final Batch Loss: 0.006816371809691191\n",
      "Epoch 4920, Loss: 0.03354032430797815, Final Batch Loss: 0.022727694362401962\n",
      "Epoch 4921, Loss: 0.05739523284137249, Final Batch Loss: 0.039554107934236526\n",
      "Epoch 4922, Loss: 0.04081033542752266, Final Batch Loss: 0.026225587353110313\n",
      "Epoch 4923, Loss: 0.08484464883804321, Final Batch Loss: 0.05298462137579918\n",
      "Epoch 4924, Loss: 0.032329327426850796, Final Batch Loss: 0.008204241283237934\n",
      "Epoch 4925, Loss: 0.1052286671474576, Final Batch Loss: 0.08983805030584335\n",
      "Epoch 4926, Loss: 0.0835841502994299, Final Batch Loss: 0.017896777018904686\n",
      "Epoch 4927, Loss: 0.07491811364889145, Final Batch Loss: 0.011919327080249786\n",
      "Epoch 4928, Loss: 0.014638011809438467, Final Batch Loss: 0.0042987908236682415\n",
      "Epoch 4929, Loss: 0.028645542450249195, Final Batch Loss: 0.0074309660121798515\n",
      "Epoch 4930, Loss: 0.12104830890893936, Final Batch Loss: 0.03280983120203018\n",
      "Epoch 4931, Loss: 0.04539912007749081, Final Batch Loss: 0.025540608912706375\n",
      "Epoch 4932, Loss: 0.06196601316332817, Final Batch Loss: 0.03121650591492653\n",
      "Epoch 4933, Loss: 0.02487741783261299, Final Batch Loss: 0.007895136252045631\n",
      "Epoch 4934, Loss: 0.029970142990350723, Final Batch Loss: 0.018531853333115578\n",
      "Epoch 4935, Loss: 0.15207676216959953, Final Batch Loss: 0.10039927810430527\n",
      "Epoch 4936, Loss: 0.042726810090243816, Final Batch Loss: 0.028883809223771095\n",
      "Epoch 4937, Loss: 0.03660646080970764, Final Batch Loss: 0.007287260144948959\n",
      "Epoch 4938, Loss: 0.03146460698917508, Final Batch Loss: 0.02484702132642269\n",
      "Epoch 4939, Loss: 0.05392301082611084, Final Batch Loss: 0.03398841246962547\n",
      "Epoch 4940, Loss: 0.028787236660718918, Final Batch Loss: 0.015205837786197662\n",
      "Epoch 4941, Loss: 0.03604262415319681, Final Batch Loss: 0.025706611573696136\n",
      "Epoch 4942, Loss: 0.08298799395561218, Final Batch Loss: 0.03439982607960701\n",
      "Epoch 4943, Loss: 0.017518428154289722, Final Batch Loss: 0.008016240783035755\n",
      "Epoch 4944, Loss: 0.04254363663494587, Final Batch Loss: 0.023565623909235\n",
      "Epoch 4945, Loss: 0.05416369251906872, Final Batch Loss: 0.020951883867383003\n",
      "Epoch 4946, Loss: 0.04507910646498203, Final Batch Loss: 0.019749470055103302\n",
      "Epoch 4947, Loss: 0.09265207313001156, Final Batch Loss: 0.012887520715594292\n",
      "Epoch 4948, Loss: 0.020160222658887506, Final Batch Loss: 0.0036799630615860224\n",
      "Epoch 4949, Loss: 0.1718963086605072, Final Batch Loss: 0.022809311747550964\n",
      "Epoch 4950, Loss: 0.030917445197701454, Final Batch Loss: 0.021951356902718544\n",
      "Epoch 4951, Loss: 0.025863454677164555, Final Batch Loss: 0.007923356257379055\n",
      "Epoch 4952, Loss: 0.02963930182158947, Final Batch Loss: 0.01333313062787056\n",
      "Epoch 4953, Loss: 0.0689395871013403, Final Batch Loss: 0.01916436292231083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4954, Loss: 0.09372740797698498, Final Batch Loss: 0.02691652439534664\n",
      "Epoch 4955, Loss: 0.016745293512940407, Final Batch Loss: 0.00862441398203373\n",
      "Epoch 4956, Loss: 0.03681943193078041, Final Batch Loss: 0.017732683569192886\n",
      "Epoch 4957, Loss: 0.040756238624453545, Final Batch Loss: 0.023726560175418854\n",
      "Epoch 4958, Loss: 0.014234106056392193, Final Batch Loss: 0.004905601032078266\n",
      "Epoch 4959, Loss: 0.0636137630790472, Final Batch Loss: 0.027525948360562325\n",
      "Epoch 4960, Loss: 0.023836851585656404, Final Batch Loss: 0.005505215842276812\n",
      "Epoch 4961, Loss: 0.022677910048514605, Final Batch Loss: 0.016285547986626625\n",
      "Epoch 4962, Loss: 0.019478947389870882, Final Batch Loss: 0.005868783686310053\n",
      "Epoch 4963, Loss: 0.0257422998547554, Final Batch Loss: 0.014523077756166458\n",
      "Epoch 4964, Loss: 0.05920174531638622, Final Batch Loss: 0.02999546006321907\n",
      "Epoch 4965, Loss: 0.0761731555685401, Final Batch Loss: 0.0078992685303092\n",
      "Epoch 4966, Loss: 0.05611003749072552, Final Batch Loss: 0.04707595333456993\n",
      "Epoch 4967, Loss: 0.054901767522096634, Final Batch Loss: 0.024185841903090477\n",
      "Epoch 4968, Loss: 0.02207243745215237, Final Batch Loss: 0.018762681633234024\n",
      "Epoch 4969, Loss: 0.015124708879739046, Final Batch Loss: 0.008176126517355442\n",
      "Epoch 4970, Loss: 0.06615893729031086, Final Batch Loss: 0.04065139964222908\n",
      "Epoch 4971, Loss: 0.05438966304063797, Final Batch Loss: 0.022075381129980087\n",
      "Epoch 4972, Loss: 0.028773820493370295, Final Batch Loss: 0.0040396531112492085\n",
      "Epoch 4973, Loss: 0.041921304538846016, Final Batch Loss: 0.021512696519494057\n",
      "Epoch 4974, Loss: 0.04805232025682926, Final Batch Loss: 0.0183689147233963\n",
      "Epoch 4975, Loss: 0.040111223235726357, Final Batch Loss: 0.012047652155160904\n",
      "Epoch 4976, Loss: 0.021496644243597984, Final Batch Loss: 0.009543070569634438\n",
      "Epoch 4977, Loss: 0.046448905020952225, Final Batch Loss: 0.03355245292186737\n",
      "Epoch 4978, Loss: 0.022498325910419226, Final Batch Loss: 0.007240156177431345\n",
      "Epoch 4979, Loss: 0.05143355205655098, Final Batch Loss: 0.03806598111987114\n",
      "Epoch 4980, Loss: 0.07096081972122192, Final Batch Loss: 0.044187020510435104\n",
      "Epoch 4981, Loss: 0.037172491662204266, Final Batch Loss: 0.009731677360832691\n",
      "Epoch 4982, Loss: 0.037881284952163696, Final Batch Loss: 0.02643977291882038\n",
      "Epoch 4983, Loss: 0.0178728555329144, Final Batch Loss: 0.004660827573388815\n",
      "Epoch 4984, Loss: 0.2028901418671012, Final Batch Loss: 0.19503171741962433\n",
      "Epoch 4985, Loss: 0.06187472492456436, Final Batch Loss: 0.027804601937532425\n",
      "Epoch 4986, Loss: 0.05879312939941883, Final Batch Loss: 0.013144997879862785\n",
      "Epoch 4987, Loss: 0.04390486143529415, Final Batch Loss: 0.019129980355501175\n",
      "Epoch 4988, Loss: 0.05670854635536671, Final Batch Loss: 0.04656369239091873\n",
      "Epoch 4989, Loss: 0.05874112620949745, Final Batch Loss: 0.03996880352497101\n",
      "Epoch 4990, Loss: 0.032502926886081696, Final Batch Loss: 0.013377102091908455\n",
      "Epoch 4991, Loss: 0.024429271928966045, Final Batch Loss: 0.010539911687374115\n",
      "Epoch 4992, Loss: 0.049608366563916206, Final Batch Loss: 0.023407429456710815\n",
      "Epoch 4993, Loss: 0.019667073152959347, Final Batch Loss: 0.008030843921005726\n",
      "Epoch 4994, Loss: 0.0523157948628068, Final Batch Loss: 0.04435213655233383\n",
      "Epoch 4995, Loss: 0.12270697578787804, Final Batch Loss: 0.0398528166115284\n",
      "Epoch 4996, Loss: 0.03871742635965347, Final Batch Loss: 0.016623731702566147\n",
      "Epoch 4997, Loss: 0.12702493369579315, Final Batch Loss: 0.06793304532766342\n",
      "Epoch 4998, Loss: 0.05697840824723244, Final Batch Loss: 0.03646102547645569\n",
      "Epoch 4999, Loss: 0.11442282795906067, Final Batch Loss: 0.006093360483646393\n",
      "Epoch 5000, Loss: 0.03210537461563945, Final Batch Loss: 0.02462456002831459\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  0  1]\n",
      " [ 0 19  0]\n",
      " [ 0  0 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.964     0.982        28\n",
      "           1      1.000     1.000     1.000        19\n",
      "           2      0.963     1.000     0.981        26\n",
      "\n",
      "    accuracy                          0.986        73\n",
      "   macro avg      0.988     0.988     0.988        73\n",
      "weighted avg      0.987     0.986     0.986        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../saved_models/UCI 3 User Classifier Group 3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
