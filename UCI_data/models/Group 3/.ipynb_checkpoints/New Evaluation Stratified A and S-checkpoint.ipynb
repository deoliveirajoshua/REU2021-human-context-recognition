{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [7, 8, 11]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.453675746917725, Final Batch Loss: 2.2290611267089844\n",
      "Epoch 2, Loss: 4.44475793838501, Final Batch Loss: 2.231189250946045\n",
      "Epoch 3, Loss: 4.4294633865356445, Final Batch Loss: 2.2035324573516846\n",
      "Epoch 4, Loss: 4.413354158401489, Final Batch Loss: 2.192084550857544\n",
      "Epoch 5, Loss: 4.423693656921387, Final Batch Loss: 2.220151662826538\n",
      "Epoch 6, Loss: 4.409757852554321, Final Batch Loss: 2.1953811645507812\n",
      "Epoch 7, Loss: 4.4122772216796875, Final Batch Loss: 2.2103021144866943\n",
      "Epoch 8, Loss: 4.400604486465454, Final Batch Loss: 2.196280002593994\n",
      "Epoch 9, Loss: 4.400053262710571, Final Batch Loss: 2.204777240753174\n",
      "Epoch 10, Loss: 4.400651216506958, Final Batch Loss: 2.209001302719116\n",
      "Epoch 11, Loss: 4.387958526611328, Final Batch Loss: 2.188906192779541\n",
      "Epoch 12, Loss: 4.364265441894531, Final Batch Loss: 2.167839527130127\n",
      "Epoch 13, Loss: 4.363017559051514, Final Batch Loss: 2.17270827293396\n",
      "Epoch 14, Loss: 4.373602390289307, Final Batch Loss: 2.196155071258545\n",
      "Epoch 15, Loss: 4.365281820297241, Final Batch Loss: 2.192244529724121\n",
      "Epoch 16, Loss: 4.343548536300659, Final Batch Loss: 2.166677236557007\n",
      "Epoch 17, Loss: 4.339323043823242, Final Batch Loss: 2.1728618144989014\n",
      "Epoch 18, Loss: 4.326205492019653, Final Batch Loss: 2.1554079055786133\n",
      "Epoch 19, Loss: 4.308491945266724, Final Batch Loss: 2.149486780166626\n",
      "Epoch 20, Loss: 4.307523965835571, Final Batch Loss: 2.150707721710205\n",
      "Epoch 21, Loss: 4.292894601821899, Final Batch Loss: 2.1395978927612305\n",
      "Epoch 22, Loss: 4.279004812240601, Final Batch Loss: 2.143580436706543\n",
      "Epoch 23, Loss: 4.274878978729248, Final Batch Loss: 2.140965223312378\n",
      "Epoch 24, Loss: 4.251274585723877, Final Batch Loss: 2.1198537349700928\n",
      "Epoch 25, Loss: 4.212418556213379, Final Batch Loss: 2.0995934009552\n",
      "Epoch 26, Loss: 4.199337720870972, Final Batch Loss: 2.0952870845794678\n",
      "Epoch 27, Loss: 4.190659046173096, Final Batch Loss: 2.090970516204834\n",
      "Epoch 28, Loss: 4.152797698974609, Final Batch Loss: 2.06821608543396\n",
      "Epoch 29, Loss: 4.116967439651489, Final Batch Loss: 2.050328493118286\n",
      "Epoch 30, Loss: 4.090266704559326, Final Batch Loss: 2.0411295890808105\n",
      "Epoch 31, Loss: 4.051245927810669, Final Batch Loss: 2.023664951324463\n",
      "Epoch 32, Loss: 3.9727143049240112, Final Batch Loss: 1.9783897399902344\n",
      "Epoch 33, Loss: 3.9664722681045532, Final Batch Loss: 1.9742754697799683\n",
      "Epoch 34, Loss: 3.8931548595428467, Final Batch Loss: 1.9253606796264648\n",
      "Epoch 35, Loss: 3.880669951438904, Final Batch Loss: 1.940528154373169\n",
      "Epoch 36, Loss: 3.785213351249695, Final Batch Loss: 1.8806583881378174\n",
      "Epoch 37, Loss: 3.803986668586731, Final Batch Loss: 1.890451431274414\n",
      "Epoch 38, Loss: 3.7421375513076782, Final Batch Loss: 1.8731050491333008\n",
      "Epoch 39, Loss: 3.7117210626602173, Final Batch Loss: 1.866871953010559\n",
      "Epoch 40, Loss: 3.655910611152649, Final Batch Loss: 1.8354957103729248\n",
      "Epoch 41, Loss: 3.595706343650818, Final Batch Loss: 1.7525982856750488\n",
      "Epoch 42, Loss: 3.6112284660339355, Final Batch Loss: 1.8111217021942139\n",
      "Epoch 43, Loss: 3.5598864555358887, Final Batch Loss: 1.7357301712036133\n",
      "Epoch 44, Loss: 3.5618064403533936, Final Batch Loss: 1.777662992477417\n",
      "Epoch 45, Loss: 3.5601632595062256, Final Batch Loss: 1.8232532739639282\n",
      "Epoch 46, Loss: 3.4892308712005615, Final Batch Loss: 1.7009692192077637\n",
      "Epoch 47, Loss: 3.456369400024414, Final Batch Loss: 1.7114588022232056\n",
      "Epoch 48, Loss: 3.4060511589050293, Final Batch Loss: 1.689465880393982\n",
      "Epoch 49, Loss: 3.4387269020080566, Final Batch Loss: 1.7095924615859985\n",
      "Epoch 50, Loss: 3.385343909263611, Final Batch Loss: 1.690827488899231\n",
      "Epoch 51, Loss: 3.397533416748047, Final Batch Loss: 1.6774674654006958\n",
      "Epoch 52, Loss: 3.3423144817352295, Final Batch Loss: 1.6538742780685425\n",
      "Epoch 53, Loss: 3.359882354736328, Final Batch Loss: 1.6737791299819946\n",
      "Epoch 54, Loss: 3.3005552291870117, Final Batch Loss: 1.683058500289917\n",
      "Epoch 55, Loss: 3.329068422317505, Final Batch Loss: 1.6398881673812866\n",
      "Epoch 56, Loss: 3.2958327531814575, Final Batch Loss: 1.6446423530578613\n",
      "Epoch 57, Loss: 3.25350284576416, Final Batch Loss: 1.6393426656723022\n",
      "Epoch 58, Loss: 3.2073123455047607, Final Batch Loss: 1.5922967195510864\n",
      "Epoch 59, Loss: 3.255353331565857, Final Batch Loss: 1.6308850049972534\n",
      "Epoch 60, Loss: 3.173385739326477, Final Batch Loss: 1.5408265590667725\n",
      "Epoch 61, Loss: 3.200333595275879, Final Batch Loss: 1.5936498641967773\n",
      "Epoch 62, Loss: 3.1215131282806396, Final Batch Loss: 1.5499323606491089\n",
      "Epoch 63, Loss: 3.160811185836792, Final Batch Loss: 1.6110974550247192\n",
      "Epoch 64, Loss: 3.1880691051483154, Final Batch Loss: 1.6318410634994507\n",
      "Epoch 65, Loss: 3.1406354904174805, Final Batch Loss: 1.557858943939209\n",
      "Epoch 66, Loss: 3.099738597869873, Final Batch Loss: 1.5824189186096191\n",
      "Epoch 67, Loss: 3.0007052421569824, Final Batch Loss: 1.468492031097412\n",
      "Epoch 68, Loss: 2.9912275075912476, Final Batch Loss: 1.4531813859939575\n",
      "Epoch 69, Loss: 3.0117517709732056, Final Batch Loss: 1.5299897193908691\n",
      "Epoch 70, Loss: 3.0439528226852417, Final Batch Loss: 1.5245989561080933\n",
      "Epoch 71, Loss: 2.9545199871063232, Final Batch Loss: 1.4261878728866577\n",
      "Epoch 72, Loss: 2.947625517845154, Final Batch Loss: 1.4527506828308105\n",
      "Epoch 73, Loss: 2.9947057962417603, Final Batch Loss: 1.5146301984786987\n",
      "Epoch 74, Loss: 2.8835227489471436, Final Batch Loss: 1.4559093713760376\n",
      "Epoch 75, Loss: 2.913887143135071, Final Batch Loss: 1.4658243656158447\n",
      "Epoch 76, Loss: 2.833186626434326, Final Batch Loss: 1.3979485034942627\n",
      "Epoch 77, Loss: 2.83685040473938, Final Batch Loss: 1.3941457271575928\n",
      "Epoch 78, Loss: 2.9471254348754883, Final Batch Loss: 1.4884196519851685\n",
      "Epoch 79, Loss: 2.8463629484176636, Final Batch Loss: 1.378605842590332\n",
      "Epoch 80, Loss: 2.7296645641326904, Final Batch Loss: 1.34113347530365\n",
      "Epoch 81, Loss: 2.8377420902252197, Final Batch Loss: 1.4781360626220703\n",
      "Epoch 82, Loss: 2.8114116191864014, Final Batch Loss: 1.3678247928619385\n",
      "Epoch 83, Loss: 2.6607978343963623, Final Batch Loss: 1.2858396768569946\n",
      "Epoch 84, Loss: 2.8267102241516113, Final Batch Loss: 1.4390958547592163\n",
      "Epoch 85, Loss: 2.6719326972961426, Final Batch Loss: 1.3160948753356934\n",
      "Epoch 86, Loss: 2.7527811527252197, Final Batch Loss: 1.3498387336730957\n",
      "Epoch 87, Loss: 2.7041237354278564, Final Batch Loss: 1.3712830543518066\n",
      "Epoch 88, Loss: 2.6886916160583496, Final Batch Loss: 1.3550875186920166\n",
      "Epoch 89, Loss: 2.566552519798279, Final Batch Loss: 1.2545771598815918\n",
      "Epoch 90, Loss: 2.597641348838806, Final Batch Loss: 1.2478656768798828\n",
      "Epoch 91, Loss: 2.578706383705139, Final Batch Loss: 1.2888555526733398\n",
      "Epoch 92, Loss: 2.677896499633789, Final Batch Loss: 1.4103342294692993\n",
      "Epoch 93, Loss: 2.6304099559783936, Final Batch Loss: 1.2827407121658325\n",
      "Epoch 94, Loss: 2.531260848045349, Final Batch Loss: 1.257590889930725\n",
      "Epoch 95, Loss: 2.539599061012268, Final Batch Loss: 1.2615087032318115\n",
      "Epoch 96, Loss: 2.553059458732605, Final Batch Loss: 1.2378133535385132\n",
      "Epoch 97, Loss: 2.409343957901001, Final Batch Loss: 1.149120569229126\n",
      "Epoch 98, Loss: 2.4012415409088135, Final Batch Loss: 1.1775014400482178\n",
      "Epoch 99, Loss: 2.404980182647705, Final Batch Loss: 1.1713428497314453\n",
      "Epoch 100, Loss: 2.398210883140564, Final Batch Loss: 1.164035439491272\n",
      "Epoch 101, Loss: 2.4492907524108887, Final Batch Loss: 1.1993381977081299\n",
      "Epoch 102, Loss: 2.264451742172241, Final Batch Loss: 1.116880178451538\n",
      "Epoch 103, Loss: 2.418958306312561, Final Batch Loss: 1.2477149963378906\n",
      "Epoch 104, Loss: 2.318527579307556, Final Batch Loss: 1.1101186275482178\n",
      "Epoch 105, Loss: 2.384351134300232, Final Batch Loss: 1.1869441270828247\n",
      "Epoch 106, Loss: 2.3145347833633423, Final Batch Loss: 1.1610721349716187\n",
      "Epoch 107, Loss: 2.4775753021240234, Final Batch Loss: 1.2938416004180908\n",
      "Epoch 108, Loss: 2.4969125986099243, Final Batch Loss: 1.2968968152999878\n",
      "Epoch 109, Loss: 2.377153754234314, Final Batch Loss: 1.1809449195861816\n",
      "Epoch 110, Loss: 2.5244985818862915, Final Batch Loss: 1.2954341173171997\n",
      "Epoch 111, Loss: 2.3071104288101196, Final Batch Loss: 1.1478039026260376\n",
      "Epoch 112, Loss: 2.2121264934539795, Final Batch Loss: 1.0925363302230835\n",
      "Epoch 113, Loss: 2.348055124282837, Final Batch Loss: 1.200623869895935\n",
      "Epoch 114, Loss: 2.202396869659424, Final Batch Loss: 1.0701684951782227\n",
      "Epoch 115, Loss: 2.2455062866210938, Final Batch Loss: 1.141377568244934\n",
      "Epoch 116, Loss: 2.1977187395095825, Final Batch Loss: 1.0974243879318237\n",
      "Epoch 117, Loss: 2.2632399797439575, Final Batch Loss: 1.0777907371520996\n",
      "Epoch 118, Loss: 2.2200140953063965, Final Batch Loss: 1.148179531097412\n",
      "Epoch 119, Loss: 2.322038769721985, Final Batch Loss: 1.1515792608261108\n",
      "Epoch 120, Loss: 2.1596580743789673, Final Batch Loss: 1.0213621854782104\n",
      "Epoch 121, Loss: 2.3228827714920044, Final Batch Loss: 1.1899570226669312\n",
      "Epoch 122, Loss: 2.220885753631592, Final Batch Loss: 1.1233612298965454\n",
      "Epoch 123, Loss: 2.116263270378113, Final Batch Loss: 1.0068868398666382\n",
      "Epoch 124, Loss: 2.2016268968582153, Final Batch Loss: 1.0883301496505737\n",
      "Epoch 125, Loss: 2.2260838747024536, Final Batch Loss: 1.1646329164505005\n",
      "Epoch 126, Loss: 2.096547484397888, Final Batch Loss: 1.0766509771347046\n",
      "Epoch 127, Loss: 2.277226209640503, Final Batch Loss: 1.1349173784255981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128, Loss: 2.1222002506256104, Final Batch Loss: 1.0191209316253662\n",
      "Epoch 129, Loss: 2.060320496559143, Final Batch Loss: 1.0155359506607056\n",
      "Epoch 130, Loss: 2.2010324001312256, Final Batch Loss: 1.0974479913711548\n",
      "Epoch 131, Loss: 2.0849387645721436, Final Batch Loss: 1.0813450813293457\n",
      "Epoch 132, Loss: 2.0368192195892334, Final Batch Loss: 0.9917807579040527\n",
      "Epoch 133, Loss: 2.044560432434082, Final Batch Loss: 1.0195772647857666\n",
      "Epoch 134, Loss: 2.1002601385116577, Final Batch Loss: 1.1065977811813354\n",
      "Epoch 135, Loss: 2.068902552127838, Final Batch Loss: 1.1425652503967285\n",
      "Epoch 136, Loss: 2.067120313644409, Final Batch Loss: 1.0306068658828735\n",
      "Epoch 137, Loss: 2.071666419506073, Final Batch Loss: 1.0812193155288696\n",
      "Epoch 138, Loss: 1.9863812923431396, Final Batch Loss: 1.0020908117294312\n",
      "Epoch 139, Loss: 2.081805109977722, Final Batch Loss: 1.0539032220840454\n",
      "Epoch 140, Loss: 1.8822236061096191, Final Batch Loss: 0.9355401396751404\n",
      "Epoch 141, Loss: 1.9166374206542969, Final Batch Loss: 1.0039969682693481\n",
      "Epoch 142, Loss: 1.9484842419624329, Final Batch Loss: 0.9634963274002075\n",
      "Epoch 143, Loss: 1.9947028160095215, Final Batch Loss: 0.9681487083435059\n",
      "Epoch 144, Loss: 2.002544403076172, Final Batch Loss: 1.0647635459899902\n",
      "Epoch 145, Loss: 1.9263404607772827, Final Batch Loss: 0.9925857782363892\n",
      "Epoch 146, Loss: 1.8459666967391968, Final Batch Loss: 0.8702138066291809\n",
      "Epoch 147, Loss: 1.895031988620758, Final Batch Loss: 0.9414743781089783\n",
      "Epoch 148, Loss: 2.0586161613464355, Final Batch Loss: 1.0234284400939941\n",
      "Epoch 149, Loss: 1.8493550419807434, Final Batch Loss: 0.871505856513977\n",
      "Epoch 150, Loss: 1.8007597923278809, Final Batch Loss: 0.8678619265556335\n",
      "Epoch 151, Loss: 1.8948144912719727, Final Batch Loss: 0.9265220165252686\n",
      "Epoch 152, Loss: 1.7282047271728516, Final Batch Loss: 0.8364055752754211\n",
      "Epoch 153, Loss: 1.8290083408355713, Final Batch Loss: 0.8586843609809875\n",
      "Epoch 154, Loss: 1.8514454364776611, Final Batch Loss: 0.9039281010627747\n",
      "Epoch 155, Loss: 1.9106692671775818, Final Batch Loss: 0.9989346861839294\n",
      "Epoch 156, Loss: 1.7973572611808777, Final Batch Loss: 0.948275089263916\n",
      "Epoch 157, Loss: 1.8496075868606567, Final Batch Loss: 0.8756189346313477\n",
      "Epoch 158, Loss: 1.8028327822685242, Final Batch Loss: 0.8904790282249451\n",
      "Epoch 159, Loss: 1.8156511783599854, Final Batch Loss: 0.892453670501709\n",
      "Epoch 160, Loss: 1.848862648010254, Final Batch Loss: 0.8642386198043823\n",
      "Epoch 161, Loss: 1.752039909362793, Final Batch Loss: 0.8765954971313477\n",
      "Epoch 162, Loss: 1.7491905093193054, Final Batch Loss: 0.8248319029808044\n",
      "Epoch 163, Loss: 1.7659167051315308, Final Batch Loss: 0.8578014969825745\n",
      "Epoch 164, Loss: 1.7191391587257385, Final Batch Loss: 0.8298905491828918\n",
      "Epoch 165, Loss: 1.7458488941192627, Final Batch Loss: 0.9057286977767944\n",
      "Epoch 166, Loss: 1.834548830986023, Final Batch Loss: 0.9322409629821777\n",
      "Epoch 167, Loss: 1.7785895466804504, Final Batch Loss: 0.9277750849723816\n",
      "Epoch 168, Loss: 1.696066439151764, Final Batch Loss: 0.7770513892173767\n",
      "Epoch 169, Loss: 1.6774592995643616, Final Batch Loss: 0.7941617965698242\n",
      "Epoch 170, Loss: 1.6437691450119019, Final Batch Loss: 0.8410632014274597\n",
      "Epoch 171, Loss: 1.7228280305862427, Final Batch Loss: 0.8704900145530701\n",
      "Epoch 172, Loss: 1.740822970867157, Final Batch Loss: 0.9194544553756714\n",
      "Epoch 173, Loss: 1.5975592136383057, Final Batch Loss: 0.7650145888328552\n",
      "Epoch 174, Loss: 1.686915099620819, Final Batch Loss: 0.833316445350647\n",
      "Epoch 175, Loss: 1.7082628011703491, Final Batch Loss: 0.8784580230712891\n",
      "Epoch 176, Loss: 1.6271864771842957, Final Batch Loss: 0.8153777718544006\n",
      "Epoch 177, Loss: 1.6723833680152893, Final Batch Loss: 0.9250560402870178\n",
      "Epoch 178, Loss: 1.6126851439476013, Final Batch Loss: 0.8377809524536133\n",
      "Epoch 179, Loss: 1.6253544688224792, Final Batch Loss: 0.8093670010566711\n",
      "Epoch 180, Loss: 1.5712236762046814, Final Batch Loss: 0.766647458076477\n",
      "Epoch 181, Loss: 1.6733559370040894, Final Batch Loss: 0.8181116580963135\n",
      "Epoch 182, Loss: 1.5972617268562317, Final Batch Loss: 0.8243955373764038\n",
      "Epoch 183, Loss: 1.6321706771850586, Final Batch Loss: 0.8033321499824524\n",
      "Epoch 184, Loss: 1.6865459084510803, Final Batch Loss: 0.8968262076377869\n",
      "Epoch 185, Loss: 1.6245340704917908, Final Batch Loss: 0.7953609824180603\n",
      "Epoch 186, Loss: 1.5398540496826172, Final Batch Loss: 0.757967472076416\n",
      "Epoch 187, Loss: 1.5482065677642822, Final Batch Loss: 0.7969368696212769\n",
      "Epoch 188, Loss: 1.561132788658142, Final Batch Loss: 0.7883383631706238\n",
      "Epoch 189, Loss: 1.5918724536895752, Final Batch Loss: 0.7785572409629822\n",
      "Epoch 190, Loss: 1.4471452236175537, Final Batch Loss: 0.669969379901886\n",
      "Epoch 191, Loss: 1.6025725603103638, Final Batch Loss: 0.8304545283317566\n",
      "Epoch 192, Loss: 1.5722100138664246, Final Batch Loss: 0.795100212097168\n",
      "Epoch 193, Loss: 1.520029067993164, Final Batch Loss: 0.739324152469635\n",
      "Epoch 194, Loss: 1.5891155004501343, Final Batch Loss: 0.7887209057807922\n",
      "Epoch 195, Loss: 1.5944998860359192, Final Batch Loss: 0.8364202380180359\n",
      "Epoch 196, Loss: 1.5964665412902832, Final Batch Loss: 0.8346134424209595\n",
      "Epoch 197, Loss: 1.5607411861419678, Final Batch Loss: 0.8289278149604797\n",
      "Epoch 198, Loss: 1.4818342924118042, Final Batch Loss: 0.7362032532691956\n",
      "Epoch 199, Loss: 1.5838326811790466, Final Batch Loss: 0.8251455426216125\n",
      "Epoch 200, Loss: 1.5307931900024414, Final Batch Loss: 0.8368407487869263\n",
      "Epoch 201, Loss: 1.4099295139312744, Final Batch Loss: 0.6342519521713257\n",
      "Epoch 202, Loss: 1.4987235069274902, Final Batch Loss: 0.7576966285705566\n",
      "Epoch 203, Loss: 1.5114840269088745, Final Batch Loss: 0.7945284843444824\n",
      "Epoch 204, Loss: 1.5535988807678223, Final Batch Loss: 0.7885895371437073\n",
      "Epoch 205, Loss: 1.490176796913147, Final Batch Loss: 0.7539347410202026\n",
      "Epoch 206, Loss: 1.3571699857711792, Final Batch Loss: 0.6384445428848267\n",
      "Epoch 207, Loss: 1.4344088435173035, Final Batch Loss: 0.6850508451461792\n",
      "Epoch 208, Loss: 1.5090271830558777, Final Batch Loss: 0.7344655394554138\n",
      "Epoch 209, Loss: 1.4164604544639587, Final Batch Loss: 0.6779950261116028\n",
      "Epoch 210, Loss: 1.4424880743026733, Final Batch Loss: 0.7629302740097046\n",
      "Epoch 211, Loss: 1.4219408631324768, Final Batch Loss: 0.6790456175804138\n",
      "Epoch 212, Loss: 1.3531270027160645, Final Batch Loss: 0.6764687299728394\n",
      "Epoch 213, Loss: 1.4751074314117432, Final Batch Loss: 0.7035692930221558\n",
      "Epoch 214, Loss: 1.3979758620262146, Final Batch Loss: 0.6694238781929016\n",
      "Epoch 215, Loss: 1.4192867279052734, Final Batch Loss: 0.6687664985656738\n",
      "Epoch 216, Loss: 1.458858072757721, Final Batch Loss: 0.8033385276794434\n",
      "Epoch 217, Loss: 1.3671543598175049, Final Batch Loss: 0.6617708206176758\n",
      "Epoch 218, Loss: 1.399806022644043, Final Batch Loss: 0.6921801567077637\n",
      "Epoch 219, Loss: 1.4221869707107544, Final Batch Loss: 0.7339083552360535\n",
      "Epoch 220, Loss: 1.4041353464126587, Final Batch Loss: 0.663061797618866\n",
      "Epoch 221, Loss: 1.2890546321868896, Final Batch Loss: 0.6304990649223328\n",
      "Epoch 222, Loss: 1.4255331754684448, Final Batch Loss: 0.7184686064720154\n",
      "Epoch 223, Loss: 1.3392806649208069, Final Batch Loss: 0.6313515901565552\n",
      "Epoch 224, Loss: 1.3590346574783325, Final Batch Loss: 0.6821132898330688\n",
      "Epoch 225, Loss: 1.3894018530845642, Final Batch Loss: 0.7189348340034485\n",
      "Epoch 226, Loss: 1.3110972046852112, Final Batch Loss: 0.6552649140357971\n",
      "Epoch 227, Loss: 1.2982330918312073, Final Batch Loss: 0.6795719265937805\n",
      "Epoch 228, Loss: 1.3278956413269043, Final Batch Loss: 0.6866880059242249\n",
      "Epoch 229, Loss: 1.3160783648490906, Final Batch Loss: 0.655807375907898\n",
      "Epoch 230, Loss: 1.369869351387024, Final Batch Loss: 0.6968886256217957\n",
      "Epoch 231, Loss: 1.4168795347213745, Final Batch Loss: 0.6818884015083313\n",
      "Epoch 232, Loss: 1.2269871830940247, Final Batch Loss: 0.5954839587211609\n",
      "Epoch 233, Loss: 1.2215960621833801, Final Batch Loss: 0.5867667198181152\n",
      "Epoch 234, Loss: 1.2071280479431152, Final Batch Loss: 0.5705606937408447\n",
      "Epoch 235, Loss: 1.3578230738639832, Final Batch Loss: 0.7599837183952332\n",
      "Epoch 236, Loss: 1.4400032758712769, Final Batch Loss: 0.764236569404602\n",
      "Epoch 237, Loss: 1.2226128578186035, Final Batch Loss: 0.6143792271614075\n",
      "Epoch 238, Loss: 1.3964422345161438, Final Batch Loss: 0.7666817903518677\n",
      "Epoch 239, Loss: 1.2130922079086304, Final Batch Loss: 0.5425551533699036\n",
      "Epoch 240, Loss: 1.355874240398407, Final Batch Loss: 0.7226868271827698\n",
      "Epoch 241, Loss: 1.2652594447135925, Final Batch Loss: 0.6071695685386658\n",
      "Epoch 242, Loss: 1.2115461826324463, Final Batch Loss: 0.5653185248374939\n",
      "Epoch 243, Loss: 1.3348089456558228, Final Batch Loss: 0.6650558710098267\n",
      "Epoch 244, Loss: 1.2924507856369019, Final Batch Loss: 0.6446280479431152\n",
      "Epoch 245, Loss: 1.301766812801361, Final Batch Loss: 0.6815486550331116\n",
      "Epoch 246, Loss: 1.1266708970069885, Final Batch Loss: 0.5028033256530762\n",
      "Epoch 247, Loss: 1.318292498588562, Final Batch Loss: 0.6320211887359619\n",
      "Epoch 248, Loss: 1.1471945643424988, Final Batch Loss: 0.5850039124488831\n",
      "Epoch 249, Loss: 1.1981689929962158, Final Batch Loss: 0.6598316431045532\n",
      "Epoch 250, Loss: 1.2021196484565735, Final Batch Loss: 0.5531406998634338\n",
      "Epoch 251, Loss: 1.252065658569336, Final Batch Loss: 0.6109038591384888\n",
      "Epoch 252, Loss: 1.202794373035431, Final Batch Loss: 0.5878773927688599\n",
      "Epoch 253, Loss: 1.279617965221405, Final Batch Loss: 0.6459838151931763\n",
      "Epoch 254, Loss: 1.1797709465026855, Final Batch Loss: 0.5855701565742493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255, Loss: 1.2742061614990234, Final Batch Loss: 0.6134803295135498\n",
      "Epoch 256, Loss: 1.2300794124603271, Final Batch Loss: 0.6623453497886658\n",
      "Epoch 257, Loss: 1.1163482069969177, Final Batch Loss: 0.49763351678848267\n",
      "Epoch 258, Loss: 1.2009881734848022, Final Batch Loss: 0.6269548535346985\n",
      "Epoch 259, Loss: 1.1169044375419617, Final Batch Loss: 0.5743025541305542\n",
      "Epoch 260, Loss: 1.202799677848816, Final Batch Loss: 0.5888945460319519\n",
      "Epoch 261, Loss: 1.047234058380127, Final Batch Loss: 0.5517752766609192\n",
      "Epoch 262, Loss: 1.232102632522583, Final Batch Loss: 0.7039804458618164\n",
      "Epoch 263, Loss: 1.2471774816513062, Final Batch Loss: 0.6308202147483826\n",
      "Epoch 264, Loss: 1.163964033126831, Final Batch Loss: 0.5617830157279968\n",
      "Epoch 265, Loss: 1.1246622204780579, Final Batch Loss: 0.5506120920181274\n",
      "Epoch 266, Loss: 1.1127955913543701, Final Batch Loss: 0.5591451525688171\n",
      "Epoch 267, Loss: 1.1168991923332214, Final Batch Loss: 0.5327377915382385\n",
      "Epoch 268, Loss: 1.16733717918396, Final Batch Loss: 0.5449864864349365\n",
      "Epoch 269, Loss: 1.2111384272575378, Final Batch Loss: 0.6005847454071045\n",
      "Epoch 270, Loss: 1.1246769428253174, Final Batch Loss: 0.6232900023460388\n",
      "Epoch 271, Loss: 1.1790793538093567, Final Batch Loss: 0.583016574382782\n",
      "Epoch 272, Loss: 1.111212134361267, Final Batch Loss: 0.547946572303772\n",
      "Epoch 273, Loss: 1.1817117929458618, Final Batch Loss: 0.6052877306938171\n",
      "Epoch 274, Loss: 1.044010728597641, Final Batch Loss: 0.4862177073955536\n",
      "Epoch 275, Loss: 1.0461981296539307, Final Batch Loss: 0.4857100248336792\n",
      "Epoch 276, Loss: 1.1155023574829102, Final Batch Loss: 0.5601598620414734\n",
      "Epoch 277, Loss: 1.123271107673645, Final Batch Loss: 0.5723299384117126\n",
      "Epoch 278, Loss: 1.2484749555587769, Final Batch Loss: 0.6350684762001038\n",
      "Epoch 279, Loss: 1.156566083431244, Final Batch Loss: 0.5415400862693787\n",
      "Epoch 280, Loss: 1.155882179737091, Final Batch Loss: 0.5822843313217163\n",
      "Epoch 281, Loss: 1.074539601802826, Final Batch Loss: 0.525588870048523\n",
      "Epoch 282, Loss: 1.1537135243415833, Final Batch Loss: 0.5831764936447144\n",
      "Epoch 283, Loss: 1.079010933637619, Final Batch Loss: 0.5964521765708923\n",
      "Epoch 284, Loss: 1.1485061049461365, Final Batch Loss: 0.6156406402587891\n",
      "Epoch 285, Loss: 1.114402949810028, Final Batch Loss: 0.6080144643783569\n",
      "Epoch 286, Loss: 1.085361123085022, Final Batch Loss: 0.571655809879303\n",
      "Epoch 287, Loss: 1.1255413889884949, Final Batch Loss: 0.5559978485107422\n",
      "Epoch 288, Loss: 1.0517420768737793, Final Batch Loss: 0.5179418325424194\n",
      "Epoch 289, Loss: 1.0566000938415527, Final Batch Loss: 0.5511389374732971\n",
      "Epoch 290, Loss: 0.9356589019298553, Final Batch Loss: 0.4340308606624603\n",
      "Epoch 291, Loss: 1.0807709693908691, Final Batch Loss: 0.5550896525382996\n",
      "Epoch 292, Loss: 1.0460467338562012, Final Batch Loss: 0.4520943760871887\n",
      "Epoch 293, Loss: 1.150796115398407, Final Batch Loss: 0.6042696833610535\n",
      "Epoch 294, Loss: 1.1068536639213562, Final Batch Loss: 0.5104782581329346\n",
      "Epoch 295, Loss: 1.0545084774494171, Final Batch Loss: 0.5587805509567261\n",
      "Epoch 296, Loss: 1.003648817539215, Final Batch Loss: 0.5056907534599304\n",
      "Epoch 297, Loss: 1.0887033343315125, Final Batch Loss: 0.5861362218856812\n",
      "Epoch 298, Loss: 1.124329924583435, Final Batch Loss: 0.6500571370124817\n",
      "Epoch 299, Loss: 1.0652400255203247, Final Batch Loss: 0.5040276050567627\n",
      "Epoch 300, Loss: 0.9974444508552551, Final Batch Loss: 0.5321749448776245\n",
      "Epoch 301, Loss: 1.0398862957954407, Final Batch Loss: 0.5131232142448425\n",
      "Epoch 302, Loss: 1.00078883767128, Final Batch Loss: 0.4835093915462494\n",
      "Epoch 303, Loss: 1.1268039345741272, Final Batch Loss: 0.562995195388794\n",
      "Epoch 304, Loss: 1.0412021279335022, Final Batch Loss: 0.5399075746536255\n",
      "Epoch 305, Loss: 0.9126110076904297, Final Batch Loss: 0.44318878650665283\n",
      "Epoch 306, Loss: 0.9817124307155609, Final Batch Loss: 0.4764523208141327\n",
      "Epoch 307, Loss: 1.0813349187374115, Final Batch Loss: 0.4463571012020111\n",
      "Epoch 308, Loss: 0.9956163763999939, Final Batch Loss: 0.5097231268882751\n",
      "Epoch 309, Loss: 1.0390680134296417, Final Batch Loss: 0.5557683706283569\n",
      "Epoch 310, Loss: 1.0674110352993011, Final Batch Loss: 0.4848189055919647\n",
      "Epoch 311, Loss: 0.9867061972618103, Final Batch Loss: 0.49001604318618774\n",
      "Epoch 312, Loss: 1.0437648594379425, Final Batch Loss: 0.546819269657135\n",
      "Epoch 313, Loss: 1.0086460709571838, Final Batch Loss: 0.48683130741119385\n",
      "Epoch 314, Loss: 1.0974168181419373, Final Batch Loss: 0.5253234505653381\n",
      "Epoch 315, Loss: 1.017838180065155, Final Batch Loss: 0.5015875101089478\n",
      "Epoch 316, Loss: 1.060401201248169, Final Batch Loss: 0.5433927178382874\n",
      "Epoch 317, Loss: 0.992027074098587, Final Batch Loss: 0.42936232686042786\n",
      "Epoch 318, Loss: 0.9160772562026978, Final Batch Loss: 0.43717214465141296\n",
      "Epoch 319, Loss: 1.0453969836235046, Final Batch Loss: 0.5205637812614441\n",
      "Epoch 320, Loss: 1.0122625529766083, Final Batch Loss: 0.5709801316261292\n",
      "Epoch 321, Loss: 1.0235826075077057, Final Batch Loss: 0.5441370010375977\n",
      "Epoch 322, Loss: 0.9321916997432709, Final Batch Loss: 0.39675256609916687\n",
      "Epoch 323, Loss: 1.0171829462051392, Final Batch Loss: 0.4747104048728943\n",
      "Epoch 324, Loss: 1.0530619323253632, Final Batch Loss: 0.5715859532356262\n",
      "Epoch 325, Loss: 0.9391475319862366, Final Batch Loss: 0.4994911551475525\n",
      "Epoch 326, Loss: 1.0052087903022766, Final Batch Loss: 0.5515604019165039\n",
      "Epoch 327, Loss: 1.0211378931999207, Final Batch Loss: 0.5018942952156067\n",
      "Epoch 328, Loss: 0.8839193880558014, Final Batch Loss: 0.4010300934314728\n",
      "Epoch 329, Loss: 0.9878415465354919, Final Batch Loss: 0.4431859850883484\n",
      "Epoch 330, Loss: 0.9281648993492126, Final Batch Loss: 0.44373637437820435\n",
      "Epoch 331, Loss: 0.9347329139709473, Final Batch Loss: 0.4379129409790039\n",
      "Epoch 332, Loss: 1.0283037722110748, Final Batch Loss: 0.49852463603019714\n",
      "Epoch 333, Loss: 0.9854737222194672, Final Batch Loss: 0.4516628682613373\n",
      "Epoch 334, Loss: 1.0065656304359436, Final Batch Loss: 0.5240089893341064\n",
      "Epoch 335, Loss: 0.8433451354503632, Final Batch Loss: 0.4380706250667572\n",
      "Epoch 336, Loss: 0.9014189839363098, Final Batch Loss: 0.46445348858833313\n",
      "Epoch 337, Loss: 0.8850760161876678, Final Batch Loss: 0.42896604537963867\n",
      "Epoch 338, Loss: 0.885920912027359, Final Batch Loss: 0.3791476786136627\n",
      "Epoch 339, Loss: 0.8977925777435303, Final Batch Loss: 0.4223872721195221\n",
      "Epoch 340, Loss: 0.9200346767902374, Final Batch Loss: 0.40453091263771057\n",
      "Epoch 341, Loss: 0.9814226627349854, Final Batch Loss: 0.5397875308990479\n",
      "Epoch 342, Loss: 0.8993251025676727, Final Batch Loss: 0.4377679228782654\n",
      "Epoch 343, Loss: 1.0050121545791626, Final Batch Loss: 0.5149356722831726\n",
      "Epoch 344, Loss: 1.0456405878067017, Final Batch Loss: 0.6118531823158264\n",
      "Epoch 345, Loss: 0.9323773980140686, Final Batch Loss: 0.4760582447052002\n",
      "Epoch 346, Loss: 1.021945297718048, Final Batch Loss: 0.5215024352073669\n",
      "Epoch 347, Loss: 0.8798971772193909, Final Batch Loss: 0.4140649437904358\n",
      "Epoch 348, Loss: 0.9664178490638733, Final Batch Loss: 0.5080121755599976\n",
      "Epoch 349, Loss: 0.9197531044483185, Final Batch Loss: 0.46096599102020264\n",
      "Epoch 350, Loss: 0.977247416973114, Final Batch Loss: 0.4787898361682892\n",
      "Epoch 351, Loss: 0.9117080867290497, Final Batch Loss: 0.47924500703811646\n",
      "Epoch 352, Loss: 0.9358096718788147, Final Batch Loss: 0.4179600477218628\n",
      "Epoch 353, Loss: 0.9281675815582275, Final Batch Loss: 0.44602712988853455\n",
      "Epoch 354, Loss: 1.0305999219417572, Final Batch Loss: 0.6012439727783203\n",
      "Epoch 355, Loss: 0.9398226141929626, Final Batch Loss: 0.4914616346359253\n",
      "Epoch 356, Loss: 0.9616286158561707, Final Batch Loss: 0.5277433395385742\n",
      "Epoch 357, Loss: 0.885834813117981, Final Batch Loss: 0.3879213333129883\n",
      "Epoch 358, Loss: 0.906285434961319, Final Batch Loss: 0.38567236065864563\n",
      "Epoch 359, Loss: 0.8954861462116241, Final Batch Loss: 0.4667835533618927\n",
      "Epoch 360, Loss: 0.9595274925231934, Final Batch Loss: 0.5624563097953796\n",
      "Epoch 361, Loss: 0.981800764799118, Final Batch Loss: 0.517325758934021\n",
      "Epoch 362, Loss: 0.8412253260612488, Final Batch Loss: 0.4448433518409729\n",
      "Epoch 363, Loss: 0.9169401526451111, Final Batch Loss: 0.47388240694999695\n",
      "Epoch 364, Loss: 0.8892702460289001, Final Batch Loss: 0.4645465016365051\n",
      "Epoch 365, Loss: 0.8706871271133423, Final Batch Loss: 0.4352918565273285\n",
      "Epoch 366, Loss: 0.868331104516983, Final Batch Loss: 0.45412927865982056\n",
      "Epoch 367, Loss: 0.8679777383804321, Final Batch Loss: 0.40599119663238525\n",
      "Epoch 368, Loss: 0.8757754266262054, Final Batch Loss: 0.4471835494041443\n",
      "Epoch 369, Loss: 0.8989228010177612, Final Batch Loss: 0.4526539742946625\n",
      "Epoch 370, Loss: 0.8314293324947357, Final Batch Loss: 0.42764928936958313\n",
      "Epoch 371, Loss: 0.9418854117393494, Final Batch Loss: 0.4965568780899048\n",
      "Epoch 372, Loss: 0.8845199048519135, Final Batch Loss: 0.45281437039375305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373, Loss: 0.8672861158847809, Final Batch Loss: 0.4369940757751465\n",
      "Epoch 374, Loss: 0.867527037858963, Final Batch Loss: 0.44675007462501526\n",
      "Epoch 375, Loss: 0.9040864408016205, Final Batch Loss: 0.4705889821052551\n",
      "Epoch 376, Loss: 0.8403051495552063, Final Batch Loss: 0.40746381878852844\n",
      "Epoch 377, Loss: 0.8913387954235077, Final Batch Loss: 0.41260939836502075\n",
      "Epoch 378, Loss: 0.811932384967804, Final Batch Loss: 0.45001277327537537\n",
      "Epoch 379, Loss: 0.7794253528118134, Final Batch Loss: 0.35423967242240906\n",
      "Epoch 380, Loss: 0.991836667060852, Final Batch Loss: 0.5301458239555359\n",
      "Epoch 381, Loss: 0.7944869697093964, Final Batch Loss: 0.37589308619499207\n",
      "Epoch 382, Loss: 0.8062061071395874, Final Batch Loss: 0.40479564666748047\n",
      "Epoch 383, Loss: 0.8711641728878021, Final Batch Loss: 0.4382861256599426\n",
      "Epoch 384, Loss: 0.9456667900085449, Final Batch Loss: 0.53939288854599\n",
      "Epoch 385, Loss: 0.7964400053024292, Final Batch Loss: 0.3627665638923645\n",
      "Epoch 386, Loss: 0.8713759779930115, Final Batch Loss: 0.4420565664768219\n",
      "Epoch 387, Loss: 0.885170042514801, Final Batch Loss: 0.470566064119339\n",
      "Epoch 388, Loss: 0.863059937953949, Final Batch Loss: 0.4085485339164734\n",
      "Epoch 389, Loss: 0.9122706651687622, Final Batch Loss: 0.4742650091648102\n",
      "Epoch 390, Loss: 0.8661971390247345, Final Batch Loss: 0.4661462604999542\n",
      "Epoch 391, Loss: 0.8493686318397522, Final Batch Loss: 0.4062312841415405\n",
      "Epoch 392, Loss: 0.922542005777359, Final Batch Loss: 0.440108984708786\n",
      "Epoch 393, Loss: 0.8448655307292938, Final Batch Loss: 0.4131383001804352\n",
      "Epoch 394, Loss: 0.7422604858875275, Final Batch Loss: 0.37825968861579895\n",
      "Epoch 395, Loss: 0.9093766510486603, Final Batch Loss: 0.4908672869205475\n",
      "Epoch 396, Loss: 0.8182928264141083, Final Batch Loss: 0.4112738072872162\n",
      "Epoch 397, Loss: 0.8970641493797302, Final Batch Loss: 0.45236560702323914\n",
      "Epoch 398, Loss: 0.924641877412796, Final Batch Loss: 0.4991528391838074\n",
      "Epoch 399, Loss: 0.8138077855110168, Final Batch Loss: 0.34160125255584717\n",
      "Epoch 400, Loss: 0.7783126533031464, Final Batch Loss: 0.3753001391887665\n",
      "Epoch 401, Loss: 0.7703294456005096, Final Batch Loss: 0.3561030328273773\n",
      "Epoch 402, Loss: 0.7812446057796478, Final Batch Loss: 0.3617045283317566\n",
      "Epoch 403, Loss: 0.8420279622077942, Final Batch Loss: 0.4026494324207306\n",
      "Epoch 404, Loss: 0.8120460212230682, Final Batch Loss: 0.44638749957084656\n",
      "Epoch 405, Loss: 0.883181095123291, Final Batch Loss: 0.45415788888931274\n",
      "Epoch 406, Loss: 0.8783627450466156, Final Batch Loss: 0.46249881386756897\n",
      "Epoch 407, Loss: 0.82988241314888, Final Batch Loss: 0.37578243017196655\n",
      "Epoch 408, Loss: 0.7616939842700958, Final Batch Loss: 0.40067586302757263\n",
      "Epoch 409, Loss: 0.7242375910282135, Final Batch Loss: 0.3288613557815552\n",
      "Epoch 410, Loss: 0.7684528231620789, Final Batch Loss: 0.39761051535606384\n",
      "Epoch 411, Loss: 0.8130007982254028, Final Batch Loss: 0.4178033471107483\n",
      "Epoch 412, Loss: 0.9188154339790344, Final Batch Loss: 0.5218188166618347\n",
      "Epoch 413, Loss: 0.8881039619445801, Final Batch Loss: 0.4453483521938324\n",
      "Epoch 414, Loss: 0.8309294283390045, Final Batch Loss: 0.4157024919986725\n",
      "Epoch 415, Loss: 0.7862613797187805, Final Batch Loss: 0.4019961953163147\n",
      "Epoch 416, Loss: 0.8907150626182556, Final Batch Loss: 0.48830142617225647\n",
      "Epoch 417, Loss: 0.89657923579216, Final Batch Loss: 0.5401392579078674\n",
      "Epoch 418, Loss: 0.7452517747879028, Final Batch Loss: 0.376193106174469\n",
      "Epoch 419, Loss: 0.8846713304519653, Final Batch Loss: 0.4797941744327545\n",
      "Epoch 420, Loss: 0.8338764607906342, Final Batch Loss: 0.43986740708351135\n",
      "Epoch 421, Loss: 0.7988334894180298, Final Batch Loss: 0.40680772066116333\n",
      "Epoch 422, Loss: 0.7855243384838104, Final Batch Loss: 0.40986257791519165\n",
      "Epoch 423, Loss: 0.7357611656188965, Final Batch Loss: 0.38376423716545105\n",
      "Epoch 424, Loss: 0.7319139242172241, Final Batch Loss: 0.3808947205543518\n",
      "Epoch 425, Loss: 0.787566065788269, Final Batch Loss: 0.4067017138004303\n",
      "Epoch 426, Loss: 0.879062831401825, Final Batch Loss: 0.4338030517101288\n",
      "Epoch 427, Loss: 0.8038600087165833, Final Batch Loss: 0.36564311385154724\n",
      "Epoch 428, Loss: 0.697499543428421, Final Batch Loss: 0.33603712916374207\n",
      "Epoch 429, Loss: 0.8584981560707092, Final Batch Loss: 0.46343091130256653\n",
      "Epoch 430, Loss: 0.8045026361942291, Final Batch Loss: 0.4256371855735779\n",
      "Epoch 431, Loss: 0.8213930428028107, Final Batch Loss: 0.4349518418312073\n",
      "Epoch 432, Loss: 0.7557677626609802, Final Batch Loss: 0.38890719413757324\n",
      "Epoch 433, Loss: 0.814978301525116, Final Batch Loss: 0.4616771340370178\n",
      "Epoch 434, Loss: 0.8415938317775726, Final Batch Loss: 0.47044652700424194\n",
      "Epoch 435, Loss: 0.8341242372989655, Final Batch Loss: 0.46718230843544006\n",
      "Epoch 436, Loss: 0.7005315124988556, Final Batch Loss: 0.3225221037864685\n",
      "Epoch 437, Loss: 0.7613562047481537, Final Batch Loss: 0.35578441619873047\n",
      "Epoch 438, Loss: 0.8005697131156921, Final Batch Loss: 0.430329293012619\n",
      "Epoch 439, Loss: 0.8172793686389923, Final Batch Loss: 0.4150121808052063\n",
      "Epoch 440, Loss: 0.6757587194442749, Final Batch Loss: 0.30577439069747925\n",
      "Epoch 441, Loss: 0.6586394011974335, Final Batch Loss: 0.2985303997993469\n",
      "Epoch 442, Loss: 0.7237119972705841, Final Batch Loss: 0.34116533398628235\n",
      "Epoch 443, Loss: 0.783419132232666, Final Batch Loss: 0.40002885460853577\n",
      "Epoch 444, Loss: 0.8330192565917969, Final Batch Loss: 0.45242419838905334\n",
      "Epoch 445, Loss: 0.7280943095684052, Final Batch Loss: 0.3530352711677551\n",
      "Epoch 446, Loss: 0.7989871799945831, Final Batch Loss: 0.43750762939453125\n",
      "Epoch 447, Loss: 0.8079523742198944, Final Batch Loss: 0.3915240466594696\n",
      "Epoch 448, Loss: 0.8463356196880341, Final Batch Loss: 0.4259411692619324\n",
      "Epoch 449, Loss: 0.7567590177059174, Final Batch Loss: 0.399177610874176\n",
      "Epoch 450, Loss: 0.8469691276550293, Final Batch Loss: 0.47038784623146057\n",
      "Epoch 451, Loss: 0.7465500831604004, Final Batch Loss: 0.3710659444332123\n",
      "Epoch 452, Loss: 0.8096487522125244, Final Batch Loss: 0.44984912872314453\n",
      "Epoch 453, Loss: 0.8608103394508362, Final Batch Loss: 0.4551418721675873\n",
      "Epoch 454, Loss: 0.7934611141681671, Final Batch Loss: 0.3876153528690338\n",
      "Epoch 455, Loss: 0.803884744644165, Final Batch Loss: 0.34176650643348694\n",
      "Epoch 456, Loss: 0.7626878619194031, Final Batch Loss: 0.35875391960144043\n",
      "Epoch 457, Loss: 0.6942399740219116, Final Batch Loss: 0.3831366002559662\n",
      "Epoch 458, Loss: 0.71341472864151, Final Batch Loss: 0.3490382432937622\n",
      "Epoch 459, Loss: 0.7494771182537079, Final Batch Loss: 0.3858197033405304\n",
      "Epoch 460, Loss: 0.7569589018821716, Final Batch Loss: 0.3318747282028198\n",
      "Epoch 461, Loss: 0.7578698396682739, Final Batch Loss: 0.3926822245121002\n",
      "Epoch 462, Loss: 0.8581830263137817, Final Batch Loss: 0.4590331017971039\n",
      "Epoch 463, Loss: 0.6949008107185364, Final Batch Loss: 0.3106628954410553\n",
      "Epoch 464, Loss: 0.7761708199977875, Final Batch Loss: 0.33888858556747437\n",
      "Epoch 465, Loss: 0.8171031773090363, Final Batch Loss: 0.4114269018173218\n",
      "Epoch 466, Loss: 0.8865036368370056, Final Batch Loss: 0.536406397819519\n",
      "Epoch 467, Loss: 0.7480921149253845, Final Batch Loss: 0.34637773036956787\n",
      "Epoch 468, Loss: 0.7145830690860748, Final Batch Loss: 0.3600112795829773\n",
      "Epoch 469, Loss: 0.7046220898628235, Final Batch Loss: 0.3664785623550415\n",
      "Epoch 470, Loss: 0.8129064440727234, Final Batch Loss: 0.4116678237915039\n",
      "Epoch 471, Loss: 0.6865419149398804, Final Batch Loss: 0.3264791667461395\n",
      "Epoch 472, Loss: 0.8189914524555206, Final Batch Loss: 0.427097350358963\n",
      "Epoch 473, Loss: 0.8030863106250763, Final Batch Loss: 0.43138960003852844\n",
      "Epoch 474, Loss: 0.7378799617290497, Final Batch Loss: 0.32322388887405396\n",
      "Epoch 475, Loss: 0.7944427132606506, Final Batch Loss: 0.4229966998100281\n",
      "Epoch 476, Loss: 0.8102068305015564, Final Batch Loss: 0.3563331961631775\n",
      "Epoch 477, Loss: 0.7322551906108856, Final Batch Loss: 0.3591935932636261\n",
      "Epoch 478, Loss: 0.7275665998458862, Final Batch Loss: 0.3400479555130005\n",
      "Epoch 479, Loss: 0.7024563550949097, Final Batch Loss: 0.3020881116390228\n",
      "Epoch 480, Loss: 0.7209843099117279, Final Batch Loss: 0.31629425287246704\n",
      "Epoch 481, Loss: 0.7828526198863983, Final Batch Loss: 0.40536996722221375\n",
      "Epoch 482, Loss: 0.7231577038764954, Final Batch Loss: 0.3682419955730438\n",
      "Epoch 483, Loss: 0.7805705666542053, Final Batch Loss: 0.405657559633255\n",
      "Epoch 484, Loss: 0.7665144801139832, Final Batch Loss: 0.42893993854522705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485, Loss: 0.6512385606765747, Final Batch Loss: 0.2848978638648987\n",
      "Epoch 486, Loss: 0.7787931263446808, Final Batch Loss: 0.39783909916877747\n",
      "Epoch 487, Loss: 0.6444219946861267, Final Batch Loss: 0.30178308486938477\n",
      "Epoch 488, Loss: 0.7199484407901764, Final Batch Loss: 0.3222852945327759\n",
      "Epoch 489, Loss: 0.7481369078159332, Final Batch Loss: 0.3423008322715759\n",
      "Epoch 490, Loss: 0.7403354942798615, Final Batch Loss: 0.39439961314201355\n",
      "Epoch 491, Loss: 0.7590352892875671, Final Batch Loss: 0.38454434275627136\n",
      "Epoch 492, Loss: 0.7711102962493896, Final Batch Loss: 0.39386245608329773\n",
      "Epoch 493, Loss: 0.8066699802875519, Final Batch Loss: 0.47095805406570435\n",
      "Epoch 494, Loss: 0.8014414012432098, Final Batch Loss: 0.44441935420036316\n",
      "Epoch 495, Loss: 0.6557479202747345, Final Batch Loss: 0.32426509261131287\n",
      "Epoch 496, Loss: 0.7333805859088898, Final Batch Loss: 0.3305043578147888\n",
      "Epoch 497, Loss: 0.780603438615799, Final Batch Loss: 0.4651423692703247\n",
      "Epoch 498, Loss: 0.7692891359329224, Final Batch Loss: 0.4629570543766022\n",
      "Epoch 499, Loss: 0.7163316905498505, Final Batch Loss: 0.34689024090766907\n",
      "Epoch 500, Loss: 0.7047268450260162, Final Batch Loss: 0.34088465571403503\n",
      "Epoch 501, Loss: 0.6843748390674591, Final Batch Loss: 0.2928878962993622\n",
      "Epoch 502, Loss: 0.7285529673099518, Final Batch Loss: 0.35589534044265747\n",
      "Epoch 503, Loss: 0.7596299648284912, Final Batch Loss: 0.3617021441459656\n",
      "Epoch 504, Loss: 0.6865960955619812, Final Batch Loss: 0.325257271528244\n",
      "Epoch 505, Loss: 0.8310690224170685, Final Batch Loss: 0.4390566051006317\n",
      "Epoch 506, Loss: 0.6716231405735016, Final Batch Loss: 0.31730392575263977\n",
      "Epoch 507, Loss: 0.7955993413925171, Final Batch Loss: 0.4110550880432129\n",
      "Epoch 508, Loss: 0.8184308111667633, Final Batch Loss: 0.3960777819156647\n",
      "Epoch 509, Loss: 0.6260180175304413, Final Batch Loss: 0.3130854368209839\n",
      "Epoch 510, Loss: 0.7547254264354706, Final Batch Loss: 0.40421658754348755\n",
      "Epoch 511, Loss: 0.6251482665538788, Final Batch Loss: 0.27027255296707153\n",
      "Epoch 512, Loss: 0.7607224583625793, Final Batch Loss: 0.4151982367038727\n",
      "Epoch 513, Loss: 0.6811120510101318, Final Batch Loss: 0.2849733531475067\n",
      "Epoch 514, Loss: 0.6628902107477188, Final Batch Loss: 0.2482747584581375\n",
      "Epoch 515, Loss: 0.6914810240268707, Final Batch Loss: 0.3258027732372284\n",
      "Epoch 516, Loss: 0.702375203371048, Final Batch Loss: 0.3433830142021179\n",
      "Epoch 517, Loss: 0.7209335267543793, Final Batch Loss: 0.4060095250606537\n",
      "Epoch 518, Loss: 0.7617877721786499, Final Batch Loss: 0.4480668902397156\n",
      "Epoch 519, Loss: 0.7682849168777466, Final Batch Loss: 0.38003918528556824\n",
      "Epoch 520, Loss: 0.6458785831928253, Final Batch Loss: 0.3511931002140045\n",
      "Epoch 521, Loss: 0.7183223366737366, Final Batch Loss: 0.31149017810821533\n",
      "Epoch 522, Loss: 0.6506961286067963, Final Batch Loss: 0.2956779897212982\n",
      "Epoch 523, Loss: 0.718250572681427, Final Batch Loss: 0.3314327895641327\n",
      "Epoch 524, Loss: 0.7542325556278229, Final Batch Loss: 0.33903107047080994\n",
      "Epoch 525, Loss: 0.6729065179824829, Final Batch Loss: 0.3343299329280853\n",
      "Epoch 526, Loss: 0.6985836029052734, Final Batch Loss: 0.3831196129322052\n",
      "Epoch 527, Loss: 0.599213182926178, Final Batch Loss: 0.28392335772514343\n",
      "Epoch 528, Loss: 0.778296709060669, Final Batch Loss: 0.44826507568359375\n",
      "Epoch 529, Loss: 0.7233652472496033, Final Batch Loss: 0.3323249816894531\n",
      "Epoch 530, Loss: 0.7119166851043701, Final Batch Loss: 0.367417573928833\n",
      "Epoch 531, Loss: 0.6297139525413513, Final Batch Loss: 0.2696079909801483\n",
      "Epoch 532, Loss: 0.7059072852134705, Final Batch Loss: 0.3318799138069153\n",
      "Epoch 533, Loss: 0.6907077133655548, Final Batch Loss: 0.3209855556488037\n",
      "Epoch 534, Loss: 0.7111908495426178, Final Batch Loss: 0.3518519699573517\n",
      "Epoch 535, Loss: 0.7157770693302155, Final Batch Loss: 0.3439004719257355\n",
      "Epoch 536, Loss: 0.7503993809223175, Final Batch Loss: 0.42447859048843384\n",
      "Epoch 537, Loss: 0.8287855982780457, Final Batch Loss: 0.4692786633968353\n",
      "Epoch 538, Loss: 0.7182697355747223, Final Batch Loss: 0.37733912467956543\n",
      "Epoch 539, Loss: 0.6907612979412079, Final Batch Loss: 0.3327033519744873\n",
      "Epoch 540, Loss: 0.7423813939094543, Final Batch Loss: 0.4053559899330139\n",
      "Epoch 541, Loss: 0.5810847878456116, Final Batch Loss: 0.27173495292663574\n",
      "Epoch 542, Loss: 0.6695519387722015, Final Batch Loss: 0.35762879252433777\n",
      "Epoch 543, Loss: 0.6432499289512634, Final Batch Loss: 0.3129092752933502\n",
      "Epoch 544, Loss: 0.7592821717262268, Final Batch Loss: 0.41063031554222107\n",
      "Epoch 545, Loss: 0.5591088831424713, Final Batch Loss: 0.23271682858467102\n",
      "Epoch 546, Loss: 0.6643199026584625, Final Batch Loss: 0.3682487905025482\n",
      "Epoch 547, Loss: 0.7594896256923676, Final Batch Loss: 0.39703312516212463\n",
      "Epoch 548, Loss: 0.6947073340415955, Final Batch Loss: 0.35586559772491455\n",
      "Epoch 549, Loss: 0.6759260892868042, Final Batch Loss: 0.3515532314777374\n",
      "Epoch 550, Loss: 0.6572923958301544, Final Batch Loss: 0.3202405273914337\n",
      "Epoch 551, Loss: 0.6998893320560455, Final Batch Loss: 0.33420008420944214\n",
      "Epoch 552, Loss: 0.597428172826767, Final Batch Loss: 0.25065186619758606\n",
      "Epoch 553, Loss: 0.7229746878147125, Final Batch Loss: 0.3619258403778076\n",
      "Epoch 554, Loss: 0.6174434125423431, Final Batch Loss: 0.29451727867126465\n",
      "Epoch 555, Loss: 0.6026148796081543, Final Batch Loss: 0.2524694502353668\n",
      "Epoch 556, Loss: 0.7226680517196655, Final Batch Loss: 0.3457723557949066\n",
      "Epoch 557, Loss: 0.6678862273693085, Final Batch Loss: 0.32919541001319885\n",
      "Epoch 558, Loss: 0.6446157246828079, Final Batch Loss: 0.24187998473644257\n",
      "Epoch 559, Loss: 0.8080721199512482, Final Batch Loss: 0.48861241340637207\n",
      "Epoch 560, Loss: 0.687702864408493, Final Batch Loss: 0.32569408416748047\n",
      "Epoch 561, Loss: 0.7574063837528229, Final Batch Loss: 0.40133121609687805\n",
      "Epoch 562, Loss: 0.6104446053504944, Final Batch Loss: 0.25360795855522156\n",
      "Epoch 563, Loss: 0.6643685400485992, Final Batch Loss: 0.36349642276763916\n",
      "Epoch 564, Loss: 0.659641683101654, Final Batch Loss: 0.31550827622413635\n",
      "Epoch 565, Loss: 0.6677736937999725, Final Batch Loss: 0.29519858956336975\n",
      "Epoch 566, Loss: 0.6737934052944183, Final Batch Loss: 0.3302699327468872\n",
      "Epoch 567, Loss: 0.5918098092079163, Final Batch Loss: 0.2531943619251251\n",
      "Epoch 568, Loss: 0.6632691323757172, Final Batch Loss: 0.2670634090900421\n",
      "Epoch 569, Loss: 0.6175554394721985, Final Batch Loss: 0.29772862792015076\n",
      "Epoch 570, Loss: 0.6137339621782303, Final Batch Loss: 0.21630094945430756\n",
      "Epoch 571, Loss: 0.5879396200180054, Final Batch Loss: 0.25545892119407654\n",
      "Epoch 572, Loss: 0.6576125025749207, Final Batch Loss: 0.3213367760181427\n",
      "Epoch 573, Loss: 0.6718660295009613, Final Batch Loss: 0.374546080827713\n",
      "Epoch 574, Loss: 0.5417162925004959, Final Batch Loss: 0.2072128802537918\n",
      "Epoch 575, Loss: 0.5982139408588409, Final Batch Loss: 0.3087102770805359\n",
      "Epoch 576, Loss: 0.6981209814548492, Final Batch Loss: 0.3392297029495239\n",
      "Epoch 577, Loss: 0.6234993636608124, Final Batch Loss: 0.34855738282203674\n",
      "Epoch 578, Loss: 0.5850780457258224, Final Batch Loss: 0.24568016827106476\n",
      "Epoch 579, Loss: 0.6455667614936829, Final Batch Loss: 0.2758057713508606\n",
      "Epoch 580, Loss: 0.7028119564056396, Final Batch Loss: 0.3226320147514343\n",
      "Epoch 581, Loss: 0.6259789168834686, Final Batch Loss: 0.3262316882610321\n",
      "Epoch 582, Loss: 0.6610901355743408, Final Batch Loss: 0.36691105365753174\n",
      "Epoch 583, Loss: 0.6871409714221954, Final Batch Loss: 0.3705184757709503\n",
      "Epoch 584, Loss: 0.6617833077907562, Final Batch Loss: 0.35341450572013855\n",
      "Epoch 585, Loss: 0.6919595003128052, Final Batch Loss: 0.37338757514953613\n",
      "Epoch 586, Loss: 0.7184333801269531, Final Batch Loss: 0.33763250708580017\n",
      "Epoch 587, Loss: 0.6388655304908752, Final Batch Loss: 0.2978871166706085\n",
      "Epoch 588, Loss: 0.6699594557285309, Final Batch Loss: 0.39114871621131897\n",
      "Epoch 589, Loss: 0.588862270116806, Final Batch Loss: 0.3028191924095154\n",
      "Epoch 590, Loss: 0.5819330215454102, Final Batch Loss: 0.32222428917884827\n",
      "Epoch 591, Loss: 0.597783088684082, Final Batch Loss: 0.2923227846622467\n",
      "Epoch 592, Loss: 0.6250968277454376, Final Batch Loss: 0.2876352369785309\n",
      "Epoch 593, Loss: 0.6621219217777252, Final Batch Loss: 0.34320950508117676\n",
      "Epoch 594, Loss: 0.5938803851604462, Final Batch Loss: 0.29235488176345825\n",
      "Epoch 595, Loss: 0.7141784429550171, Final Batch Loss: 0.421367347240448\n",
      "Epoch 596, Loss: 0.5864473879337311, Final Batch Loss: 0.3025815784931183\n",
      "Epoch 597, Loss: 0.6520943939685822, Final Batch Loss: 0.389447420835495\n",
      "Epoch 598, Loss: 0.6405248045921326, Final Batch Loss: 0.32533422112464905\n",
      "Epoch 599, Loss: 0.752011626958847, Final Batch Loss: 0.3719746768474579\n",
      "Epoch 600, Loss: 0.6397919058799744, Final Batch Loss: 0.33092886209487915\n",
      "Epoch 601, Loss: 0.6278567612171173, Final Batch Loss: 0.31108999252319336\n",
      "Epoch 602, Loss: 0.6218530535697937, Final Batch Loss: 0.26563307642936707\n",
      "Epoch 603, Loss: 0.6091493964195251, Final Batch Loss: 0.29510724544525146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 604, Loss: 0.5943471789360046, Final Batch Loss: 0.2991049587726593\n",
      "Epoch 605, Loss: 0.5437720566987991, Final Batch Loss: 0.22421224415302277\n",
      "Epoch 606, Loss: 0.6757191121578217, Final Batch Loss: 0.31095531582832336\n",
      "Epoch 607, Loss: 0.6273822784423828, Final Batch Loss: 0.32210513949394226\n",
      "Epoch 608, Loss: 0.5649348199367523, Final Batch Loss: 0.27022257447242737\n",
      "Epoch 609, Loss: 0.6083544790744781, Final Batch Loss: 0.2560519278049469\n",
      "Epoch 610, Loss: 0.6735953986644745, Final Batch Loss: 0.3779153823852539\n",
      "Epoch 611, Loss: 0.661878913640976, Final Batch Loss: 0.34255680441856384\n",
      "Epoch 612, Loss: 0.5914434492588043, Final Batch Loss: 0.29299747943878174\n",
      "Epoch 613, Loss: 0.5879680812358856, Final Batch Loss: 0.29053691029548645\n",
      "Epoch 614, Loss: 0.5553170442581177, Final Batch Loss: 0.24579963088035583\n",
      "Epoch 615, Loss: 0.6644776165485382, Final Batch Loss: 0.33136337995529175\n",
      "Epoch 616, Loss: 0.7457129657268524, Final Batch Loss: 0.37345483899116516\n",
      "Epoch 617, Loss: 0.5061892569065094, Final Batch Loss: 0.21222689747810364\n",
      "Epoch 618, Loss: 0.6235900819301605, Final Batch Loss: 0.32019928097724915\n",
      "Epoch 619, Loss: 0.5310991555452347, Final Batch Loss: 0.2473328858613968\n",
      "Epoch 620, Loss: 0.6781826913356781, Final Batch Loss: 0.3915134072303772\n",
      "Epoch 621, Loss: 0.630356639623642, Final Batch Loss: 0.3256496787071228\n",
      "Epoch 622, Loss: 0.7023006975650787, Final Batch Loss: 0.4372807443141937\n",
      "Epoch 623, Loss: 0.5995915830135345, Final Batch Loss: 0.26551392674446106\n",
      "Epoch 624, Loss: 0.5095863342285156, Final Batch Loss: 0.23018977046012878\n",
      "Epoch 625, Loss: 0.7062253654003143, Final Batch Loss: 0.4031210243701935\n",
      "Epoch 626, Loss: 0.6854662299156189, Final Batch Loss: 0.368927925825119\n",
      "Epoch 627, Loss: 0.611207127571106, Final Batch Loss: 0.312158465385437\n",
      "Epoch 628, Loss: 0.6492827832698822, Final Batch Loss: 0.32111072540283203\n",
      "Epoch 629, Loss: 0.5887495279312134, Final Batch Loss: 0.3121403753757477\n",
      "Epoch 630, Loss: 0.58011494576931, Final Batch Loss: 0.21009619534015656\n",
      "Epoch 631, Loss: 0.5976036489009857, Final Batch Loss: 0.28547823429107666\n",
      "Epoch 632, Loss: 0.5604583472013474, Final Batch Loss: 0.24624337255954742\n",
      "Epoch 633, Loss: 0.6167685091495514, Final Batch Loss: 0.27674710750579834\n",
      "Epoch 634, Loss: 0.6434585452079773, Final Batch Loss: 0.3880161941051483\n",
      "Epoch 635, Loss: 0.6079244315624237, Final Batch Loss: 0.3387868106365204\n",
      "Epoch 636, Loss: 0.6972203254699707, Final Batch Loss: 0.4006044268608093\n",
      "Epoch 637, Loss: 0.6986464262008667, Final Batch Loss: 0.4415109157562256\n",
      "Epoch 638, Loss: 0.5993485152721405, Final Batch Loss: 0.2538900077342987\n",
      "Epoch 639, Loss: 0.6108230352401733, Final Batch Loss: 0.31794479489326477\n",
      "Epoch 640, Loss: 0.5768262147903442, Final Batch Loss: 0.22903266549110413\n",
      "Epoch 641, Loss: 0.6019514501094818, Final Batch Loss: 0.33907073736190796\n",
      "Epoch 642, Loss: 0.5645491778850555, Final Batch Loss: 0.2803104519844055\n",
      "Epoch 643, Loss: 0.6112053096294403, Final Batch Loss: 0.33428457379341125\n",
      "Epoch 644, Loss: 0.5141467154026031, Final Batch Loss: 0.26136302947998047\n",
      "Epoch 645, Loss: 0.6216546595096588, Final Batch Loss: 0.3291899263858795\n",
      "Epoch 646, Loss: 0.6700673699378967, Final Batch Loss: 0.3454781770706177\n",
      "Epoch 647, Loss: 0.5967375040054321, Final Batch Loss: 0.31465739011764526\n",
      "Epoch 648, Loss: 0.6266287565231323, Final Batch Loss: 0.24856406450271606\n",
      "Epoch 649, Loss: 0.6534936130046844, Final Batch Loss: 0.34990161657333374\n",
      "Epoch 650, Loss: 0.7065773904323578, Final Batch Loss: 0.4008820950984955\n",
      "Epoch 651, Loss: 0.6209002435207367, Final Batch Loss: 0.2959027588367462\n",
      "Epoch 652, Loss: 0.5996794104576111, Final Batch Loss: 0.28374025225639343\n",
      "Epoch 653, Loss: 0.6747294664382935, Final Batch Loss: 0.3679603934288025\n",
      "Epoch 654, Loss: 0.5935972630977631, Final Batch Loss: 0.2907220423221588\n",
      "Epoch 655, Loss: 0.6324595808982849, Final Batch Loss: 0.3295655846595764\n",
      "Epoch 656, Loss: 0.5751627832651138, Final Batch Loss: 0.2474420815706253\n",
      "Epoch 657, Loss: 0.5633267760276794, Final Batch Loss: 0.286857008934021\n",
      "Epoch 658, Loss: 0.6457384824752808, Final Batch Loss: 0.323582261800766\n",
      "Epoch 659, Loss: 0.6046261787414551, Final Batch Loss: 0.3032415509223938\n",
      "Epoch 660, Loss: 0.6304818093776703, Final Batch Loss: 0.28357240557670593\n",
      "Epoch 661, Loss: 0.6628498136997223, Final Batch Loss: 0.39455121755599976\n",
      "Epoch 662, Loss: 0.6105021834373474, Final Batch Loss: 0.2614296078681946\n",
      "Epoch 663, Loss: 0.6099832653999329, Final Batch Loss: 0.2761407196521759\n",
      "Epoch 664, Loss: 0.617062509059906, Final Batch Loss: 0.2800411581993103\n",
      "Epoch 665, Loss: 0.6361709535121918, Final Batch Loss: 0.36854782700538635\n",
      "Epoch 666, Loss: 0.6079246997833252, Final Batch Loss: 0.3005479872226715\n",
      "Epoch 667, Loss: 0.635278731584549, Final Batch Loss: 0.34429019689559937\n",
      "Epoch 668, Loss: 0.5669171661138535, Final Batch Loss: 0.24629975855350494\n",
      "Epoch 669, Loss: 0.5806387066841125, Final Batch Loss: 0.3327777683734894\n",
      "Epoch 670, Loss: 0.5869685709476471, Final Batch Loss: 0.2937415540218353\n",
      "Epoch 671, Loss: 0.6808367967605591, Final Batch Loss: 0.4217815697193146\n",
      "Epoch 672, Loss: 0.5349410474300385, Final Batch Loss: 0.2599738538265228\n",
      "Epoch 673, Loss: 0.6600768566131592, Final Batch Loss: 0.3448348045349121\n",
      "Epoch 674, Loss: 0.6359657049179077, Final Batch Loss: 0.30109864473342896\n",
      "Epoch 675, Loss: 0.5210326164960861, Final Batch Loss: 0.21973182260990143\n",
      "Epoch 676, Loss: 0.6213150024414062, Final Batch Loss: 0.34047311544418335\n",
      "Epoch 677, Loss: 0.6601523458957672, Final Batch Loss: 0.32735559344291687\n",
      "Epoch 678, Loss: 0.6030661463737488, Final Batch Loss: 0.32689169049263\n",
      "Epoch 679, Loss: 0.5798390805721283, Final Batch Loss: 0.26839154958724976\n",
      "Epoch 680, Loss: 0.6041050255298615, Final Batch Loss: 0.2707946300506592\n",
      "Epoch 681, Loss: 0.6482522189617157, Final Batch Loss: 0.31579500436782837\n",
      "Epoch 682, Loss: 0.574629157781601, Final Batch Loss: 0.31047311425209045\n",
      "Epoch 683, Loss: 0.7802036702632904, Final Batch Loss: 0.47037991881370544\n",
      "Epoch 684, Loss: 0.5638307631015778, Final Batch Loss: 0.2561474144458771\n",
      "Epoch 685, Loss: 0.5708920955657959, Final Batch Loss: 0.27465471625328064\n",
      "Epoch 686, Loss: 0.6642274260520935, Final Batch Loss: 0.3381306827068329\n",
      "Epoch 687, Loss: 0.616525799036026, Final Batch Loss: 0.28208860754966736\n",
      "Epoch 688, Loss: 0.5900862216949463, Final Batch Loss: 0.33523425459861755\n",
      "Epoch 689, Loss: 0.6528252065181732, Final Batch Loss: 0.30428916215896606\n",
      "Epoch 690, Loss: 0.643298327922821, Final Batch Loss: 0.3061368465423584\n",
      "Epoch 691, Loss: 0.555822417140007, Final Batch Loss: 0.3148021101951599\n",
      "Epoch 692, Loss: 0.6107937395572662, Final Batch Loss: 0.32126638293266296\n",
      "Epoch 693, Loss: 0.5402959138154984, Final Batch Loss: 0.2405800074338913\n",
      "Epoch 694, Loss: 0.5338286459445953, Final Batch Loss: 0.2557224929332733\n",
      "Epoch 695, Loss: 0.5973778367042542, Final Batch Loss: 0.30815762281417847\n",
      "Epoch 696, Loss: 0.6776869893074036, Final Batch Loss: 0.4238390028476715\n",
      "Epoch 697, Loss: 0.6163566410541534, Final Batch Loss: 0.2991165816783905\n",
      "Epoch 698, Loss: 0.5807930380105972, Final Batch Loss: 0.24161283671855927\n",
      "Epoch 699, Loss: 0.5577943176031113, Final Batch Loss: 0.2417515367269516\n",
      "Epoch 700, Loss: 0.5765024721622467, Final Batch Loss: 0.2639172077178955\n",
      "Epoch 701, Loss: 0.5588705241680145, Final Batch Loss: 0.25168269872665405\n",
      "Epoch 702, Loss: 0.6556607186794281, Final Batch Loss: 0.3740834593772888\n",
      "Epoch 703, Loss: 0.629768431186676, Final Batch Loss: 0.2832185626029968\n",
      "Epoch 704, Loss: 0.562555342912674, Final Batch Loss: 0.31742778420448303\n",
      "Epoch 705, Loss: 0.7507713735103607, Final Batch Loss: 0.3833934962749481\n",
      "Epoch 706, Loss: 0.6258800327777863, Final Batch Loss: 0.32985860109329224\n",
      "Epoch 707, Loss: 0.6573450863361359, Final Batch Loss: 0.33318811655044556\n",
      "Epoch 708, Loss: 0.5306182950735092, Final Batch Loss: 0.29104113578796387\n",
      "Epoch 709, Loss: 0.6336323022842407, Final Batch Loss: 0.29958054423332214\n",
      "Epoch 710, Loss: 0.5173977464437485, Final Batch Loss: 0.22213517129421234\n",
      "Epoch 711, Loss: 0.5531355440616608, Final Batch Loss: 0.2507328391075134\n",
      "Epoch 712, Loss: 0.5426896214485168, Final Batch Loss: 0.2760281264781952\n",
      "Epoch 713, Loss: 0.5389900803565979, Final Batch Loss: 0.21608233451843262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714, Loss: 0.5888742804527283, Final Batch Loss: 0.28316259384155273\n",
      "Epoch 715, Loss: 0.5639466047286987, Final Batch Loss: 0.2614525854587555\n",
      "Epoch 716, Loss: 0.6393292546272278, Final Batch Loss: 0.31474658846855164\n",
      "Epoch 717, Loss: 0.554702490568161, Final Batch Loss: 0.26321941614151\n",
      "Epoch 718, Loss: 0.5817418396472931, Final Batch Loss: 0.26945728063583374\n",
      "Epoch 719, Loss: 0.6022796034812927, Final Batch Loss: 0.30293479561805725\n",
      "Epoch 720, Loss: 0.6362597346305847, Final Batch Loss: 0.33634352684020996\n",
      "Epoch 721, Loss: 0.768082320690155, Final Batch Loss: 0.4357568621635437\n",
      "Epoch 722, Loss: 0.623773068189621, Final Batch Loss: 0.33010759949684143\n",
      "Epoch 723, Loss: 0.5950808525085449, Final Batch Loss: 0.32282140851020813\n",
      "Epoch 724, Loss: 0.5769132673740387, Final Batch Loss: 0.31155142188072205\n",
      "Epoch 725, Loss: 0.5418146699666977, Final Batch Loss: 0.2378646284341812\n",
      "Epoch 726, Loss: 0.710781455039978, Final Batch Loss: 0.40449872612953186\n",
      "Epoch 727, Loss: 0.5930525362491608, Final Batch Loss: 0.2760508954524994\n",
      "Epoch 728, Loss: 0.577137291431427, Final Batch Loss: 0.2606251835823059\n",
      "Epoch 729, Loss: 0.49507559835910797, Final Batch Loss: 0.22064290940761566\n",
      "Epoch 730, Loss: 0.6008123755455017, Final Batch Loss: 0.36011046171188354\n",
      "Epoch 731, Loss: 0.6439419090747833, Final Batch Loss: 0.3143962621688843\n",
      "Epoch 732, Loss: 0.5899338126182556, Final Batch Loss: 0.356839656829834\n",
      "Epoch 733, Loss: 0.6178960502147675, Final Batch Loss: 0.35162001848220825\n",
      "Epoch 734, Loss: 0.5503004789352417, Final Batch Loss: 0.2623773515224457\n",
      "Epoch 735, Loss: 0.6652792096138, Final Batch Loss: 0.39185863733291626\n",
      "Epoch 736, Loss: 0.587509959936142, Final Batch Loss: 0.25885388255119324\n",
      "Epoch 737, Loss: 0.5533137321472168, Final Batch Loss: 0.2771700322628021\n",
      "Epoch 738, Loss: 0.6103051006793976, Final Batch Loss: 0.3308902382850647\n",
      "Epoch 739, Loss: 0.6315450072288513, Final Batch Loss: 0.3244418501853943\n",
      "Epoch 740, Loss: 0.5760916471481323, Final Batch Loss: 0.2728894352912903\n",
      "Epoch 741, Loss: 0.5671676397323608, Final Batch Loss: 0.2698810398578644\n",
      "Epoch 742, Loss: 0.6969963908195496, Final Batch Loss: 0.3844538927078247\n",
      "Epoch 743, Loss: 0.4847780168056488, Final Batch Loss: 0.24827496707439423\n",
      "Epoch 744, Loss: 0.5680287480354309, Final Batch Loss: 0.3050326406955719\n",
      "Epoch 745, Loss: 0.6241772174835205, Final Batch Loss: 0.359735369682312\n",
      "Epoch 746, Loss: 0.551351934671402, Final Batch Loss: 0.2867884337902069\n",
      "Epoch 747, Loss: 0.5978277623653412, Final Batch Loss: 0.3430926203727722\n",
      "Epoch 748, Loss: 0.6096591651439667, Final Batch Loss: 0.23509621620178223\n",
      "Epoch 749, Loss: 0.5110531896352768, Final Batch Loss: 0.24589307606220245\n",
      "Epoch 750, Loss: 0.6346434950828552, Final Batch Loss: 0.3642125129699707\n",
      "Epoch 751, Loss: 0.6447668075561523, Final Batch Loss: 0.3546280264854431\n",
      "Epoch 752, Loss: 0.5370675623416901, Final Batch Loss: 0.278337687253952\n",
      "Epoch 753, Loss: 0.5962419211864471, Final Batch Loss: 0.2978701591491699\n",
      "Epoch 754, Loss: 0.633997917175293, Final Batch Loss: 0.3277224600315094\n",
      "Epoch 755, Loss: 0.6420791447162628, Final Batch Loss: 0.3605968654155731\n",
      "Epoch 756, Loss: 0.5770756900310516, Final Batch Loss: 0.32873567938804626\n",
      "Epoch 757, Loss: 0.6839653849601746, Final Batch Loss: 0.37664705514907837\n",
      "Epoch 758, Loss: 0.5353243350982666, Final Batch Loss: 0.2452700138092041\n",
      "Epoch 759, Loss: 0.6152806580066681, Final Batch Loss: 0.33794647455215454\n",
      "Epoch 760, Loss: 0.6010794341564178, Final Batch Loss: 0.3135131299495697\n",
      "Epoch 761, Loss: 0.6429918110370636, Final Batch Loss: 0.2970384359359741\n",
      "Epoch 762, Loss: 0.5103218853473663, Final Batch Loss: 0.21413418650627136\n",
      "Epoch 763, Loss: 0.5563999712467194, Final Batch Loss: 0.25208407640457153\n",
      "Epoch 764, Loss: 0.5920789241790771, Final Batch Loss: 0.29519525170326233\n",
      "Epoch 765, Loss: 0.523581400513649, Final Batch Loss: 0.23422004282474518\n",
      "Epoch 766, Loss: 0.5674417614936829, Final Batch Loss: 0.29426631331443787\n",
      "Epoch 767, Loss: 0.6051601767539978, Final Batch Loss: 0.29666224122047424\n",
      "Epoch 768, Loss: 0.5080826878547668, Final Batch Loss: 0.19708549976348877\n",
      "Epoch 769, Loss: 0.6384271085262299, Final Batch Loss: 0.3488966226577759\n",
      "Epoch 770, Loss: 0.5608568787574768, Final Batch Loss: 0.2519332766532898\n",
      "Epoch 771, Loss: 0.5450502932071686, Final Batch Loss: 0.2823766767978668\n",
      "Epoch 772, Loss: 0.6182367205619812, Final Batch Loss: 0.3625473380088806\n",
      "Epoch 773, Loss: 0.6707817316055298, Final Batch Loss: 0.36097782850265503\n",
      "Epoch 774, Loss: 0.6630210727453232, Final Batch Loss: 0.4189419448375702\n",
      "Epoch 775, Loss: 0.5316209048032761, Final Batch Loss: 0.22355972230434418\n",
      "Epoch 776, Loss: 0.5851424038410187, Final Batch Loss: 0.31721362471580505\n",
      "Epoch 777, Loss: 0.5873252153396606, Final Batch Loss: 0.29217028617858887\n",
      "Epoch 778, Loss: 0.539085641503334, Final Batch Loss: 0.29426541924476624\n",
      "Epoch 779, Loss: 0.5689360797405243, Final Batch Loss: 0.3118741810321808\n",
      "Epoch 780, Loss: 0.5566857159137726, Final Batch Loss: 0.2556416690349579\n",
      "Epoch 781, Loss: 0.5343802571296692, Final Batch Loss: 0.21608543395996094\n",
      "Epoch 782, Loss: 0.5774177014827728, Final Batch Loss: 0.2838762402534485\n",
      "Epoch 783, Loss: 0.5566160678863525, Final Batch Loss: 0.2868530750274658\n",
      "Epoch 784, Loss: 0.513935774564743, Final Batch Loss: 0.25441616773605347\n",
      "Epoch 785, Loss: 0.6234706044197083, Final Batch Loss: 0.3582925796508789\n",
      "Epoch 786, Loss: 0.6074641942977905, Final Batch Loss: 0.2922899127006531\n",
      "Epoch 787, Loss: 0.5732487142086029, Final Batch Loss: 0.3071163594722748\n",
      "Epoch 788, Loss: 0.5271324515342712, Final Batch Loss: 0.2411450445652008\n",
      "Epoch 789, Loss: 0.5450974404811859, Final Batch Loss: 0.2778206169605255\n",
      "Epoch 790, Loss: 0.5286478102207184, Final Batch Loss: 0.23946458101272583\n",
      "Epoch 791, Loss: 0.6507853865623474, Final Batch Loss: 0.3345794379711151\n",
      "Epoch 792, Loss: 0.5576572418212891, Final Batch Loss: 0.27754175662994385\n",
      "Epoch 793, Loss: 0.5623459219932556, Final Batch Loss: 0.2559048533439636\n",
      "Epoch 794, Loss: 0.5614723563194275, Final Batch Loss: 0.2616789937019348\n",
      "Epoch 795, Loss: 0.5039070397615433, Final Batch Loss: 0.24817697703838348\n",
      "Epoch 796, Loss: 0.504485234618187, Final Batch Loss: 0.25487539172172546\n",
      "Epoch 797, Loss: 0.5622715801000595, Final Batch Loss: 0.3270496726036072\n",
      "Epoch 798, Loss: 0.5749022662639618, Final Batch Loss: 0.2838993966579437\n",
      "Epoch 799, Loss: 0.5570886433124542, Final Batch Loss: 0.2830790877342224\n",
      "Epoch 800, Loss: 0.728799045085907, Final Batch Loss: 0.4462699294090271\n",
      "Epoch 801, Loss: 0.6928443610668182, Final Batch Loss: 0.3738793432712555\n",
      "Epoch 802, Loss: 0.5323081612586975, Final Batch Loss: 0.2656775116920471\n",
      "Epoch 803, Loss: 0.6280765235424042, Final Batch Loss: 0.34809955954551697\n",
      "Epoch 804, Loss: 0.5448189079761505, Final Batch Loss: 0.26192694902420044\n",
      "Epoch 805, Loss: 0.5601020753383636, Final Batch Loss: 0.3001929223537445\n",
      "Epoch 806, Loss: 0.5114971250295639, Final Batch Loss: 0.21863938868045807\n",
      "Epoch 807, Loss: 0.5710460841655731, Final Batch Loss: 0.3170422315597534\n",
      "Epoch 808, Loss: 0.5109582394361496, Final Batch Loss: 0.22584162652492523\n",
      "Epoch 809, Loss: 0.593510776758194, Final Batch Loss: 0.3118554651737213\n",
      "Epoch 810, Loss: 0.5460185557603836, Final Batch Loss: 0.24550367891788483\n",
      "Epoch 811, Loss: 0.6399074792861938, Final Batch Loss: 0.413844496011734\n",
      "Epoch 812, Loss: 0.5206365138292313, Final Batch Loss: 0.2306368201971054\n",
      "Epoch 813, Loss: 0.626603826880455, Final Batch Loss: 0.3806851804256439\n",
      "Epoch 814, Loss: 0.6212134957313538, Final Batch Loss: 0.31070584058761597\n",
      "Epoch 815, Loss: 0.602670207619667, Final Batch Loss: 0.36362430453300476\n",
      "Epoch 816, Loss: 0.5848874747753143, Final Batch Loss: 0.2912341356277466\n",
      "Epoch 817, Loss: 0.5989998877048492, Final Batch Loss: 0.33847951889038086\n",
      "Epoch 818, Loss: 0.5933288335800171, Final Batch Loss: 0.272675484418869\n",
      "Epoch 819, Loss: 0.5514727383852005, Final Batch Loss: 0.21478594839572906\n",
      "Epoch 820, Loss: 0.4881882518529892, Final Batch Loss: 0.18186993896961212\n",
      "Epoch 821, Loss: 0.5559784173965454, Final Batch Loss: 0.2924529016017914\n",
      "Epoch 822, Loss: 0.5069771558046341, Final Batch Loss: 0.23979683220386505\n",
      "Epoch 823, Loss: 0.5520301759243011, Final Batch Loss: 0.2777442932128906\n",
      "Epoch 824, Loss: 0.4934774190187454, Final Batch Loss: 0.23183970153331757\n",
      "Epoch 825, Loss: 0.5384250581264496, Final Batch Loss: 0.2737240493297577\n",
      "Epoch 826, Loss: 0.5900057852268219, Final Batch Loss: 0.32740798592567444\n",
      "Epoch 827, Loss: 0.6328630149364471, Final Batch Loss: 0.33753204345703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 828, Loss: 0.5849997401237488, Final Batch Loss: 0.31016403436660767\n",
      "Epoch 829, Loss: 0.49414971470832825, Final Batch Loss: 0.2068166434764862\n",
      "Epoch 830, Loss: 0.5178445279598236, Final Batch Loss: 0.2591498792171478\n",
      "Epoch 831, Loss: 0.6959904730319977, Final Batch Loss: 0.3888823390007019\n",
      "Epoch 832, Loss: 0.5879152715206146, Final Batch Loss: 0.26436465978622437\n",
      "Epoch 833, Loss: 0.5312356501817703, Final Batch Loss: 0.2275775820016861\n",
      "Epoch 834, Loss: 0.47615158557891846, Final Batch Loss: 0.20979171991348267\n",
      "Epoch 835, Loss: 0.5587773025035858, Final Batch Loss: 0.2832806706428528\n",
      "Epoch 836, Loss: 0.44044503569602966, Final Batch Loss: 0.17510849237442017\n",
      "Epoch 837, Loss: 0.5763687789440155, Final Batch Loss: 0.3156074583530426\n",
      "Epoch 838, Loss: 0.5724801272153854, Final Batch Loss: 0.3386695683002472\n",
      "Epoch 839, Loss: 0.5113346874713898, Final Batch Loss: 0.24742212891578674\n",
      "Epoch 840, Loss: 0.5850889682769775, Final Batch Loss: 0.25588613748550415\n",
      "Epoch 841, Loss: 0.5131374597549438, Final Batch Loss: 0.24103501439094543\n",
      "Epoch 842, Loss: 0.576115220785141, Final Batch Loss: 0.28999388217926025\n",
      "Epoch 843, Loss: 0.5561987012624741, Final Batch Loss: 0.34027907252311707\n",
      "Epoch 844, Loss: 0.5648089945316315, Final Batch Loss: 0.3102622628211975\n",
      "Epoch 845, Loss: 0.5943616032600403, Final Batch Loss: 0.284334659576416\n",
      "Epoch 846, Loss: 0.6217283606529236, Final Batch Loss: 0.3359513282775879\n",
      "Epoch 847, Loss: 0.5231695473194122, Final Batch Loss: 0.26500648260116577\n",
      "Epoch 848, Loss: 0.5211383402347565, Final Batch Loss: 0.25820979475975037\n",
      "Epoch 849, Loss: 0.6694384515285492, Final Batch Loss: 0.42125773429870605\n",
      "Epoch 850, Loss: 0.6230064630508423, Final Batch Loss: 0.3320174217224121\n",
      "Epoch 851, Loss: 0.5976579487323761, Final Batch Loss: 0.3064708411693573\n",
      "Epoch 852, Loss: 0.5352073907852173, Final Batch Loss: 0.26819270849227905\n",
      "Epoch 853, Loss: 0.5891529321670532, Final Batch Loss: 0.2872163653373718\n",
      "Epoch 854, Loss: 0.5097605139017105, Final Batch Loss: 0.24466760456562042\n",
      "Epoch 855, Loss: 0.5076004564762115, Final Batch Loss: 0.22105863690376282\n",
      "Epoch 856, Loss: 0.5264833271503448, Final Batch Loss: 0.28665682673454285\n",
      "Epoch 857, Loss: 0.5145153552293777, Final Batch Loss: 0.24540023505687714\n",
      "Epoch 858, Loss: 0.4873792976140976, Final Batch Loss: 0.21234877407550812\n",
      "Epoch 859, Loss: 0.5755049884319305, Final Batch Loss: 0.2887267470359802\n",
      "Epoch 860, Loss: 0.5016566514968872, Final Batch Loss: 0.2501556873321533\n",
      "Epoch 861, Loss: 0.586084634065628, Final Batch Loss: 0.3109816908836365\n",
      "Epoch 862, Loss: 0.5757633745670319, Final Batch Loss: 0.3131468594074249\n",
      "Epoch 863, Loss: 0.5486475825309753, Final Batch Loss: 0.28208106756210327\n",
      "Epoch 864, Loss: 0.5455920398235321, Final Batch Loss: 0.2739616334438324\n",
      "Epoch 865, Loss: 0.5138041079044342, Final Batch Loss: 0.2462943196296692\n",
      "Epoch 866, Loss: 0.5405989736318588, Final Batch Loss: 0.2997756898403168\n",
      "Epoch 867, Loss: 0.5838670134544373, Final Batch Loss: 0.2821396291255951\n",
      "Epoch 868, Loss: 0.5665854215621948, Final Batch Loss: 0.29701676964759827\n",
      "Epoch 869, Loss: 0.5379460453987122, Final Batch Loss: 0.2688726782798767\n",
      "Epoch 870, Loss: 0.5595203936100006, Final Batch Loss: 0.2942804992198944\n",
      "Epoch 871, Loss: 0.4923139810562134, Final Batch Loss: 0.24022668600082397\n",
      "Epoch 872, Loss: 0.5097332447767258, Final Batch Loss: 0.278750479221344\n",
      "Epoch 873, Loss: 0.535174548625946, Final Batch Loss: 0.2545817196369171\n",
      "Epoch 874, Loss: 0.5074703097343445, Final Batch Loss: 0.26054519414901733\n",
      "Epoch 875, Loss: 0.5536046624183655, Final Batch Loss: 0.2773909270763397\n",
      "Epoch 876, Loss: 0.5031631141901016, Final Batch Loss: 0.260818213224411\n",
      "Epoch 877, Loss: 0.5515465140342712, Final Batch Loss: 0.2753598093986511\n",
      "Epoch 878, Loss: 0.5813056528568268, Final Batch Loss: 0.2987551987171173\n",
      "Epoch 879, Loss: 0.5114220380783081, Final Batch Loss: 0.2510678470134735\n",
      "Epoch 880, Loss: 0.49040935933589935, Final Batch Loss: 0.2250368446111679\n",
      "Epoch 881, Loss: 0.49075767397880554, Final Batch Loss: 0.239546000957489\n",
      "Epoch 882, Loss: 0.554140642285347, Final Batch Loss: 0.24234639108181\n",
      "Epoch 883, Loss: 0.5245577096939087, Final Batch Loss: 0.2499631643295288\n",
      "Epoch 884, Loss: 0.5781711041927338, Final Batch Loss: 0.3191035985946655\n",
      "Epoch 885, Loss: 0.6100108623504639, Final Batch Loss: 0.2874213457107544\n",
      "Epoch 886, Loss: 0.5242447853088379, Final Batch Loss: 0.27206990122795105\n",
      "Epoch 887, Loss: 0.513141468167305, Final Batch Loss: 0.2833387553691864\n",
      "Epoch 888, Loss: 0.5468645393848419, Final Batch Loss: 0.25145918130874634\n",
      "Epoch 889, Loss: 0.47225137054920197, Final Batch Loss: 0.23492053151130676\n",
      "Epoch 890, Loss: 0.5622440874576569, Final Batch Loss: 0.2897457480430603\n",
      "Epoch 891, Loss: 0.5626567006111145, Final Batch Loss: 0.23448798060417175\n",
      "Epoch 892, Loss: 0.5940858721733093, Final Batch Loss: 0.33941712975502014\n",
      "Epoch 893, Loss: 0.5239636301994324, Final Batch Loss: 0.2585912048816681\n",
      "Epoch 894, Loss: 0.47942325472831726, Final Batch Loss: 0.2059260606765747\n",
      "Epoch 895, Loss: 0.50273796916008, Final Batch Loss: 0.23136159777641296\n",
      "Epoch 896, Loss: 0.49228566884994507, Final Batch Loss: 0.23139852285385132\n",
      "Epoch 897, Loss: 0.5453674793243408, Final Batch Loss: 0.25641635060310364\n",
      "Epoch 898, Loss: 0.5708653628826141, Final Batch Loss: 0.3007676899433136\n",
      "Epoch 899, Loss: 0.456368625164032, Final Batch Loss: 0.20700529217720032\n",
      "Epoch 900, Loss: 0.6209952235221863, Final Batch Loss: 0.3246016502380371\n",
      "Epoch 901, Loss: 0.5347753167152405, Final Batch Loss: 0.2713327407836914\n",
      "Epoch 902, Loss: 0.5297011882066727, Final Batch Loss: 0.2967871427536011\n",
      "Epoch 903, Loss: 0.5260008573532104, Final Batch Loss: 0.23049157857894897\n",
      "Epoch 904, Loss: 0.5693378448486328, Final Batch Loss: 0.3032896816730499\n",
      "Epoch 905, Loss: 0.5159725844860077, Final Batch Loss: 0.2651847302913666\n",
      "Epoch 906, Loss: 0.5675933659076691, Final Batch Loss: 0.2843885123729706\n",
      "Epoch 907, Loss: 0.5486986339092255, Final Batch Loss: 0.2886620759963989\n",
      "Epoch 908, Loss: 0.5457125455141068, Final Batch Loss: 0.3354235589504242\n",
      "Epoch 909, Loss: 0.5381950736045837, Final Batch Loss: 0.26118040084838867\n",
      "Epoch 910, Loss: 0.49718421697616577, Final Batch Loss: 0.30018091201782227\n",
      "Epoch 911, Loss: 0.5115575790405273, Final Batch Loss: 0.2541308104991913\n",
      "Epoch 912, Loss: 0.5966382920742035, Final Batch Loss: 0.3230358958244324\n",
      "Epoch 913, Loss: 0.5368553698062897, Final Batch Loss: 0.2839055061340332\n",
      "Epoch 914, Loss: 0.4589395821094513, Final Batch Loss: 0.21766947209835052\n",
      "Epoch 915, Loss: 0.4961179196834564, Final Batch Loss: 0.2664787471294403\n",
      "Epoch 916, Loss: 0.5568086206912994, Final Batch Loss: 0.2978385090827942\n",
      "Epoch 917, Loss: 0.4898403733968735, Final Batch Loss: 0.24119676649570465\n",
      "Epoch 918, Loss: 0.5119023621082306, Final Batch Loss: 0.26238542795181274\n",
      "Epoch 919, Loss: 0.516902819275856, Final Batch Loss: 0.2195381373167038\n",
      "Epoch 920, Loss: 0.5841341316699982, Final Batch Loss: 0.26638707518577576\n",
      "Epoch 921, Loss: 0.49156953394412994, Final Batch Loss: 0.24614503979682922\n",
      "Epoch 922, Loss: 0.5348942279815674, Final Batch Loss: 0.28340235352516174\n",
      "Epoch 923, Loss: 0.5425250977277756, Final Batch Loss: 0.321015328168869\n",
      "Epoch 924, Loss: 0.5338640958070755, Final Batch Loss: 0.30706125497817993\n",
      "Epoch 925, Loss: 0.45139504969120026, Final Batch Loss: 0.19484393298625946\n",
      "Epoch 926, Loss: 0.5123342275619507, Final Batch Loss: 0.2751045525074005\n",
      "Epoch 927, Loss: 0.5286244750022888, Final Batch Loss: 0.2713920474052429\n",
      "Epoch 928, Loss: 0.4472687542438507, Final Batch Loss: 0.2113768756389618\n",
      "Epoch 929, Loss: 0.5696079432964325, Final Batch Loss: 0.2729022204875946\n",
      "Epoch 930, Loss: 0.5063043832778931, Final Batch Loss: 0.21509996056556702\n",
      "Epoch 931, Loss: 0.6127065718173981, Final Batch Loss: 0.3508312702178955\n",
      "Epoch 932, Loss: 0.5061305165290833, Final Batch Loss: 0.24396130442619324\n",
      "Epoch 933, Loss: 0.5088049173355103, Final Batch Loss: 0.22169694304466248\n",
      "Epoch 934, Loss: 0.5856995582580566, Final Batch Loss: 0.30741074681282043\n",
      "Epoch 935, Loss: 0.47324205935001373, Final Batch Loss: 0.19580884277820587\n",
      "Epoch 936, Loss: 0.5427085012197495, Final Batch Loss: 0.2320830374956131\n",
      "Epoch 937, Loss: 0.5480547994375229, Final Batch Loss: 0.3070653975009918\n",
      "Epoch 938, Loss: 0.5730499923229218, Final Batch Loss: 0.3024587035179138\n",
      "Epoch 939, Loss: 0.4910154342651367, Final Batch Loss: 0.19511038064956665\n",
      "Epoch 940, Loss: 0.5703833401203156, Final Batch Loss: 0.28459563851356506\n",
      "Epoch 941, Loss: 0.548654317855835, Final Batch Loss: 0.25101324915885925\n",
      "Epoch 942, Loss: 0.4632052183151245, Final Batch Loss: 0.21817153692245483\n",
      "Epoch 943, Loss: 0.598828136920929, Final Batch Loss: 0.3217002749443054\n",
      "Epoch 944, Loss: 0.4821151793003082, Final Batch Loss: 0.19585669040679932\n",
      "Epoch 945, Loss: 0.5237141698598862, Final Batch Loss: 0.24331457912921906\n",
      "Epoch 946, Loss: 0.6140275597572327, Final Batch Loss: 0.35433247685432434\n",
      "Epoch 947, Loss: 0.5442093759775162, Final Batch Loss: 0.3311358392238617\n",
      "Epoch 948, Loss: 0.4357055425643921, Final Batch Loss: 0.19736506044864655\n",
      "Epoch 949, Loss: 0.5359052866697311, Final Batch Loss: 0.2485893815755844\n",
      "Epoch 950, Loss: 0.4798860251903534, Final Batch Loss: 0.23749825358390808\n",
      "Epoch 951, Loss: 0.6176849901676178, Final Batch Loss: 0.33757251501083374\n",
      "Epoch 952, Loss: 0.5436988472938538, Final Batch Loss: 0.2985868752002716\n",
      "Epoch 953, Loss: 0.5432081520557404, Final Batch Loss: 0.2948455214500427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 954, Loss: 0.5714327991008759, Final Batch Loss: 0.27184543013572693\n",
      "Epoch 955, Loss: 0.60128553211689, Final Batch Loss: 0.3669237196445465\n",
      "Epoch 956, Loss: 0.49811574816703796, Final Batch Loss: 0.2120400071144104\n",
      "Epoch 957, Loss: 0.5868604481220245, Final Batch Loss: 0.27500584721565247\n",
      "Epoch 958, Loss: 0.4554288238286972, Final Batch Loss: 0.19584356248378754\n",
      "Epoch 959, Loss: 0.5257653445005417, Final Batch Loss: 0.22733734548091888\n",
      "Epoch 960, Loss: 0.5037436932325363, Final Batch Loss: 0.23789988458156586\n",
      "Epoch 961, Loss: 0.596660852432251, Final Batch Loss: 0.321437269449234\n",
      "Epoch 962, Loss: 0.4918093830347061, Final Batch Loss: 0.2203260213136673\n",
      "Epoch 963, Loss: 0.4959968328475952, Final Batch Loss: 0.24334844946861267\n",
      "Epoch 964, Loss: 0.4576764553785324, Final Batch Loss: 0.2576466202735901\n",
      "Epoch 965, Loss: 0.5191502422094345, Final Batch Loss: 0.23159600794315338\n",
      "Epoch 966, Loss: 0.617277204990387, Final Batch Loss: 0.31540799140930176\n",
      "Epoch 967, Loss: 0.6201208233833313, Final Batch Loss: 0.35805177688598633\n",
      "Epoch 968, Loss: 0.4727572351694107, Final Batch Loss: 0.19939713180065155\n",
      "Epoch 969, Loss: 0.5798695087432861, Final Batch Loss: 0.29935142397880554\n",
      "Epoch 970, Loss: 0.5550904273986816, Final Batch Loss: 0.3066171705722809\n",
      "Epoch 971, Loss: 0.6570388078689575, Final Batch Loss: 0.3356925845146179\n",
      "Epoch 972, Loss: 0.49125443398952484, Final Batch Loss: 0.250198096036911\n",
      "Epoch 973, Loss: 0.5528497099876404, Final Batch Loss: 0.29832613468170166\n",
      "Epoch 974, Loss: 0.519660472869873, Final Batch Loss: 0.2992990016937256\n",
      "Epoch 975, Loss: 0.5016810297966003, Final Batch Loss: 0.24409812688827515\n",
      "Epoch 976, Loss: 0.5721651464700699, Final Batch Loss: 0.32326555252075195\n",
      "Epoch 977, Loss: 0.525966614484787, Final Batch Loss: 0.29442915320396423\n",
      "Epoch 978, Loss: 0.5983188450336456, Final Batch Loss: 0.25006264448165894\n",
      "Epoch 979, Loss: 0.5693428367376328, Final Batch Loss: 0.339277058839798\n",
      "Epoch 980, Loss: 0.5468608736991882, Final Batch Loss: 0.2953953742980957\n",
      "Epoch 981, Loss: 0.48898936808109283, Final Batch Loss: 0.1984395831823349\n",
      "Epoch 982, Loss: 0.5312303155660629, Final Batch Loss: 0.2226622849702835\n",
      "Epoch 983, Loss: 0.5253184139728546, Final Batch Loss: 0.28817218542099\n",
      "Epoch 984, Loss: 0.5351260006427765, Final Batch Loss: 0.2620963454246521\n",
      "Epoch 985, Loss: 0.507422000169754, Final Batch Loss: 0.2410602569580078\n",
      "Epoch 986, Loss: 0.552346482872963, Final Batch Loss: 0.3391769528388977\n",
      "Epoch 987, Loss: 0.48837001621723175, Final Batch Loss: 0.25365254282951355\n",
      "Epoch 988, Loss: 0.504001647233963, Final Batch Loss: 0.20775553584098816\n",
      "Epoch 989, Loss: 0.5046436786651611, Final Batch Loss: 0.22028106451034546\n",
      "Epoch 990, Loss: 0.5414431840181351, Final Batch Loss: 0.2939293384552002\n",
      "Epoch 991, Loss: 0.5771848559379578, Final Batch Loss: 0.2948598861694336\n",
      "Epoch 992, Loss: 0.5058468282222748, Final Batch Loss: 0.25415995717048645\n",
      "Epoch 993, Loss: 0.5834386944770813, Final Batch Loss: 0.3095344007015228\n",
      "Epoch 994, Loss: 0.4219103306531906, Final Batch Loss: 0.19842839241027832\n",
      "Epoch 995, Loss: 0.5375217348337173, Final Batch Loss: 0.30358052253723145\n",
      "Epoch 996, Loss: 0.5014755129814148, Final Batch Loss: 0.25083446502685547\n",
      "Epoch 997, Loss: 0.5227354466915131, Final Batch Loss: 0.26421844959259033\n",
      "Epoch 998, Loss: 0.584717646241188, Final Batch Loss: 0.3591092526912689\n",
      "Epoch 999, Loss: 0.5222266465425491, Final Batch Loss: 0.2967754900455475\n",
      "Epoch 1000, Loss: 0.5052318274974823, Final Batch Loss: 0.24935540556907654\n",
      "Epoch 1001, Loss: 0.4418901056051254, Final Batch Loss: 0.17978264391422272\n",
      "Epoch 1002, Loss: 0.48124556243419647, Final Batch Loss: 0.190826877951622\n",
      "Epoch 1003, Loss: 0.5001185536384583, Final Batch Loss: 0.18234652280807495\n",
      "Epoch 1004, Loss: 0.4777403026819229, Final Batch Loss: 0.20838026702404022\n",
      "Epoch 1005, Loss: 0.46002036333084106, Final Batch Loss: 0.21248459815979004\n",
      "Epoch 1006, Loss: 0.4947967231273651, Final Batch Loss: 0.25869378447532654\n",
      "Epoch 1007, Loss: 0.4728538542985916, Final Batch Loss: 0.21182213723659515\n",
      "Epoch 1008, Loss: 0.52749103307724, Final Batch Loss: 0.28147220611572266\n",
      "Epoch 1009, Loss: 0.6057519614696503, Final Batch Loss: 0.35815295577049255\n",
      "Epoch 1010, Loss: 0.5773467123508453, Final Batch Loss: 0.3485628664493561\n",
      "Epoch 1011, Loss: 0.5293866097927094, Final Batch Loss: 0.26199039816856384\n",
      "Epoch 1012, Loss: 0.48078449070453644, Final Batch Loss: 0.22097252309322357\n",
      "Epoch 1013, Loss: 0.5073496401309967, Final Batch Loss: 0.2508944571018219\n",
      "Epoch 1014, Loss: 0.4808565378189087, Final Batch Loss: 0.2237572968006134\n",
      "Epoch 1015, Loss: 0.4900319278240204, Final Batch Loss: 0.24189822375774384\n",
      "Epoch 1016, Loss: 0.5469605624675751, Final Batch Loss: 0.3406142592430115\n",
      "Epoch 1017, Loss: 0.49830807745456696, Final Batch Loss: 0.19460810720920563\n",
      "Epoch 1018, Loss: 0.5229748785495758, Final Batch Loss: 0.26435616612434387\n",
      "Epoch 1019, Loss: 0.5165115594863892, Final Batch Loss: 0.2595842182636261\n",
      "Epoch 1020, Loss: 0.4976003021001816, Final Batch Loss: 0.23954366147518158\n",
      "Epoch 1021, Loss: 0.4735075384378433, Final Batch Loss: 0.25938674807548523\n",
      "Epoch 1022, Loss: 0.4960298091173172, Final Batch Loss: 0.24583403766155243\n",
      "Epoch 1023, Loss: 0.4711931496858597, Final Batch Loss: 0.23779965937137604\n",
      "Epoch 1024, Loss: 0.5363805890083313, Final Batch Loss: 0.2578005790710449\n",
      "Epoch 1025, Loss: 0.5030984282493591, Final Batch Loss: 0.2725074291229248\n",
      "Epoch 1026, Loss: 0.5101688802242279, Final Batch Loss: 0.22120168805122375\n",
      "Epoch 1027, Loss: 0.6443735361099243, Final Batch Loss: 0.3734019994735718\n",
      "Epoch 1028, Loss: 0.5129271745681763, Final Batch Loss: 0.25098004937171936\n",
      "Epoch 1029, Loss: 0.4734172970056534, Final Batch Loss: 0.24024434387683868\n",
      "Epoch 1030, Loss: 0.5041387677192688, Final Batch Loss: 0.1967940330505371\n",
      "Epoch 1031, Loss: 0.49374011158943176, Final Batch Loss: 0.26851359009742737\n",
      "Epoch 1032, Loss: 0.5527800917625427, Final Batch Loss: 0.3207888901233673\n",
      "Epoch 1033, Loss: 0.5053842663764954, Final Batch Loss: 0.2588522434234619\n",
      "Epoch 1034, Loss: 0.5480222105979919, Final Batch Loss: 0.3320735692977905\n",
      "Epoch 1035, Loss: 0.5147868394851685, Final Batch Loss: 0.24194389581680298\n",
      "Epoch 1036, Loss: 0.44027042388916016, Final Batch Loss: 0.2249194234609604\n",
      "Epoch 1037, Loss: 0.5085937082767487, Final Batch Loss: 0.241254985332489\n",
      "Epoch 1038, Loss: 0.5123921632766724, Final Batch Loss: 0.29481878876686096\n",
      "Epoch 1039, Loss: 0.5934322476387024, Final Batch Loss: 0.3005361258983612\n",
      "Epoch 1040, Loss: 0.4277714639902115, Final Batch Loss: 0.17893493175506592\n",
      "Epoch 1041, Loss: 0.39232484996318817, Final Batch Loss: 0.16697648167610168\n",
      "Epoch 1042, Loss: 0.48966890573501587, Final Batch Loss: 0.2661896049976349\n",
      "Epoch 1043, Loss: 0.48601196706295013, Final Batch Loss: 0.21693392097949982\n",
      "Epoch 1044, Loss: 0.4370381683111191, Final Batch Loss: 0.253616064786911\n",
      "Epoch 1045, Loss: 0.4553685337305069, Final Batch Loss: 0.2109985649585724\n",
      "Epoch 1046, Loss: 0.5122550278902054, Final Batch Loss: 0.2402864247560501\n",
      "Epoch 1047, Loss: 0.4713786393404007, Final Batch Loss: 0.2051926702260971\n",
      "Epoch 1048, Loss: 0.5362662225961685, Final Batch Loss: 0.2895563244819641\n",
      "Epoch 1049, Loss: 0.48542992770671844, Final Batch Loss: 0.26213666796684265\n",
      "Epoch 1050, Loss: 0.5546556115150452, Final Batch Loss: 0.27865341305732727\n",
      "Epoch 1051, Loss: 0.48615799844264984, Final Batch Loss: 0.2521081268787384\n",
      "Epoch 1052, Loss: 0.5136634558439255, Final Batch Loss: 0.282120943069458\n",
      "Epoch 1053, Loss: 0.5227650701999664, Final Batch Loss: 0.30160439014434814\n",
      "Epoch 1054, Loss: 0.42828159034252167, Final Batch Loss: 0.18120865523815155\n",
      "Epoch 1055, Loss: 0.5239996910095215, Final Batch Loss: 0.2762939929962158\n",
      "Epoch 1056, Loss: 0.4824605882167816, Final Batch Loss: 0.19394612312316895\n",
      "Epoch 1057, Loss: 0.5182417780160904, Final Batch Loss: 0.2906668782234192\n",
      "Epoch 1058, Loss: 0.4809763878583908, Final Batch Loss: 0.24919122457504272\n",
      "Epoch 1059, Loss: 0.5242233276367188, Final Batch Loss: 0.22392311692237854\n",
      "Epoch 1060, Loss: 0.4233757555484772, Final Batch Loss: 0.18114669620990753\n",
      "Epoch 1061, Loss: 0.43404315412044525, Final Batch Loss: 0.17500440776348114\n",
      "Epoch 1062, Loss: 0.47265584766864777, Final Batch Loss: 0.1943531185388565\n",
      "Epoch 1063, Loss: 0.5230804085731506, Final Batch Loss: 0.2630635201931\n",
      "Epoch 1064, Loss: 0.5167697370052338, Final Batch Loss: 0.22648459672927856\n",
      "Epoch 1065, Loss: 0.5033182203769684, Final Batch Loss: 0.2269258201122284\n",
      "Epoch 1066, Loss: 0.588980108499527, Final Batch Loss: 0.33400189876556396\n",
      "Epoch 1067, Loss: 0.44321146607398987, Final Batch Loss: 0.19288146495819092\n",
      "Epoch 1068, Loss: 0.6302809417247772, Final Batch Loss: 0.37514209747314453\n",
      "Epoch 1069, Loss: 0.5137682408094406, Final Batch Loss: 0.2674776017665863\n",
      "Epoch 1070, Loss: 0.47544121742248535, Final Batch Loss: 0.2531293034553528\n",
      "Epoch 1071, Loss: 0.528590202331543, Final Batch Loss: 0.2763579487800598\n",
      "Epoch 1072, Loss: 0.48314985632896423, Final Batch Loss: 0.26459401845932007\n",
      "Epoch 1073, Loss: 0.47191838920116425, Final Batch Loss: 0.20737479627132416\n",
      "Epoch 1074, Loss: 0.5141342282295227, Final Batch Loss: 0.2501446306705475\n",
      "Epoch 1075, Loss: 0.5723029673099518, Final Batch Loss: 0.3482455909252167\n",
      "Epoch 1076, Loss: 0.5179475843906403, Final Batch Loss: 0.251352995634079\n",
      "Epoch 1077, Loss: 0.5345361679792404, Final Batch Loss: 0.2869132459163666\n",
      "Epoch 1078, Loss: 0.4560483992099762, Final Batch Loss: 0.19801285862922668\n",
      "Epoch 1079, Loss: 0.47833679616451263, Final Batch Loss: 0.252529501914978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1080, Loss: 0.5154258161783218, Final Batch Loss: 0.285234272480011\n",
      "Epoch 1081, Loss: 0.4833514839410782, Final Batch Loss: 0.26786139607429504\n",
      "Epoch 1082, Loss: 0.5391601771116257, Final Batch Loss: 0.3053712844848633\n",
      "Epoch 1083, Loss: 0.4733768701553345, Final Batch Loss: 0.20058545470237732\n",
      "Epoch 1084, Loss: 0.5120308548212051, Final Batch Loss: 0.21180526912212372\n",
      "Epoch 1085, Loss: 0.5422457605600357, Final Batch Loss: 0.3395829498767853\n",
      "Epoch 1086, Loss: 0.5338976383209229, Final Batch Loss: 0.2529808282852173\n",
      "Epoch 1087, Loss: 0.48425740003585815, Final Batch Loss: 0.23445092141628265\n",
      "Epoch 1088, Loss: 0.5599775463342667, Final Batch Loss: 0.31323519349098206\n",
      "Epoch 1089, Loss: 0.498673677444458, Final Batch Loss: 0.27445071935653687\n",
      "Epoch 1090, Loss: 0.4920775592327118, Final Batch Loss: 0.21508145332336426\n",
      "Epoch 1091, Loss: 0.46792924404144287, Final Batch Loss: 0.19833692908287048\n",
      "Epoch 1092, Loss: 0.5031462013721466, Final Batch Loss: 0.257783442735672\n",
      "Epoch 1093, Loss: 0.5113020092248917, Final Batch Loss: 0.29369473457336426\n",
      "Epoch 1094, Loss: 0.4707629084587097, Final Batch Loss: 0.24550606310367584\n",
      "Epoch 1095, Loss: 0.4891025722026825, Final Batch Loss: 0.21806231141090393\n",
      "Epoch 1096, Loss: 0.6216730326414108, Final Batch Loss: 0.37951990962028503\n",
      "Epoch 1097, Loss: 0.43223151564598083, Final Batch Loss: 0.18211206793785095\n",
      "Epoch 1098, Loss: 0.5309395790100098, Final Batch Loss: 0.27910634875297546\n",
      "Epoch 1099, Loss: 0.4773847311735153, Final Batch Loss: 0.2267189770936966\n",
      "Epoch 1100, Loss: 0.5190994441509247, Final Batch Loss: 0.26067742705345154\n",
      "Epoch 1101, Loss: 0.4670320749282837, Final Batch Loss: 0.2307780534029007\n",
      "Epoch 1102, Loss: 0.511610746383667, Final Batch Loss: 0.2480686902999878\n",
      "Epoch 1103, Loss: 0.4799795001745224, Final Batch Loss: 0.22329910099506378\n",
      "Epoch 1104, Loss: 0.5299269258975983, Final Batch Loss: 0.29013508558273315\n",
      "Epoch 1105, Loss: 0.5202511548995972, Final Batch Loss: 0.25518032908439636\n",
      "Epoch 1106, Loss: 0.44570688903331757, Final Batch Loss: 0.2086469829082489\n",
      "Epoch 1107, Loss: 0.4635983854532242, Final Batch Loss: 0.23781506717205048\n",
      "Epoch 1108, Loss: 0.5085575431585312, Final Batch Loss: 0.21564875543117523\n",
      "Epoch 1109, Loss: 0.5401265323162079, Final Batch Loss: 0.2408813238143921\n",
      "Epoch 1110, Loss: 0.465904101729393, Final Batch Loss: 0.2130264788866043\n",
      "Epoch 1111, Loss: 0.5397913604974747, Final Batch Loss: 0.32337528467178345\n",
      "Epoch 1112, Loss: 0.5602550655603409, Final Batch Loss: 0.325641006231308\n",
      "Epoch 1113, Loss: 0.5152613073587418, Final Batch Loss: 0.28865671157836914\n",
      "Epoch 1114, Loss: 0.47846440970897675, Final Batch Loss: 0.21859894692897797\n",
      "Epoch 1115, Loss: 0.471196785569191, Final Batch Loss: 0.23576241731643677\n",
      "Epoch 1116, Loss: 0.4013487547636032, Final Batch Loss: 0.1656757891178131\n",
      "Epoch 1117, Loss: 0.4371473044157028, Final Batch Loss: 0.2271166443824768\n",
      "Epoch 1118, Loss: 0.5252222120761871, Final Batch Loss: 0.28020280599594116\n",
      "Epoch 1119, Loss: 0.528456449508667, Final Batch Loss: 0.29904279112815857\n",
      "Epoch 1120, Loss: 0.4167855679988861, Final Batch Loss: 0.18427851796150208\n",
      "Epoch 1121, Loss: 0.4553266316652298, Final Batch Loss: 0.2069268524646759\n",
      "Epoch 1122, Loss: 0.5025842189788818, Final Batch Loss: 0.24226659536361694\n",
      "Epoch 1123, Loss: 0.5350698828697205, Final Batch Loss: 0.276679128408432\n",
      "Epoch 1124, Loss: 0.4942665696144104, Final Batch Loss: 0.24778564274311066\n",
      "Epoch 1125, Loss: 0.4494069218635559, Final Batch Loss: 0.23852956295013428\n",
      "Epoch 1126, Loss: 0.4462362378835678, Final Batch Loss: 0.2156037837266922\n",
      "Epoch 1127, Loss: 0.47835950553417206, Final Batch Loss: 0.261847585439682\n",
      "Epoch 1128, Loss: 0.5281497240066528, Final Batch Loss: 0.30722498893737793\n",
      "Epoch 1129, Loss: 0.49237965047359467, Final Batch Loss: 0.23662389814853668\n",
      "Epoch 1130, Loss: 0.4584379494190216, Final Batch Loss: 0.21078568696975708\n",
      "Epoch 1131, Loss: 0.45279884338378906, Final Batch Loss: 0.2357131689786911\n",
      "Epoch 1132, Loss: 0.454231858253479, Final Batch Loss: 0.21181395649909973\n",
      "Epoch 1133, Loss: 0.44548875093460083, Final Batch Loss: 0.1859525442123413\n",
      "Epoch 1134, Loss: 0.46525539457798004, Final Batch Loss: 0.24611854553222656\n",
      "Epoch 1135, Loss: 0.4572747200727463, Final Batch Loss: 0.2282366156578064\n",
      "Epoch 1136, Loss: 0.5600134879350662, Final Batch Loss: 0.3236001431941986\n",
      "Epoch 1137, Loss: 0.5005892217159271, Final Batch Loss: 0.2757156193256378\n",
      "Epoch 1138, Loss: 0.43496352434158325, Final Batch Loss: 0.1533219814300537\n",
      "Epoch 1139, Loss: 0.5409931242465973, Final Batch Loss: 0.2992645800113678\n",
      "Epoch 1140, Loss: 0.5368729531764984, Final Batch Loss: 0.26724207401275635\n",
      "Epoch 1141, Loss: 0.4447295665740967, Final Batch Loss: 0.2188451886177063\n",
      "Epoch 1142, Loss: 0.39453887939453125, Final Batch Loss: 0.15228700637817383\n",
      "Epoch 1143, Loss: 0.5158319175243378, Final Batch Loss: 0.2550465762615204\n",
      "Epoch 1144, Loss: 0.48624131083488464, Final Batch Loss: 0.27567926049232483\n",
      "Epoch 1145, Loss: 0.47140930593013763, Final Batch Loss: 0.24806006252765656\n",
      "Epoch 1146, Loss: 0.44474832713603973, Final Batch Loss: 0.2179160863161087\n",
      "Epoch 1147, Loss: 0.5260758846998215, Final Batch Loss: 0.3127219080924988\n",
      "Epoch 1148, Loss: 0.41580498218536377, Final Batch Loss: 0.20365022122859955\n",
      "Epoch 1149, Loss: 0.42058660089969635, Final Batch Loss: 0.19380536675453186\n",
      "Epoch 1150, Loss: 0.4358331859111786, Final Batch Loss: 0.19587372243404388\n",
      "Epoch 1151, Loss: 0.47418829798698425, Final Batch Loss: 0.25595638155937195\n",
      "Epoch 1152, Loss: 0.46044857800006866, Final Batch Loss: 0.20589382946491241\n",
      "Epoch 1153, Loss: 0.4598837047815323, Final Batch Loss: 0.23480954766273499\n",
      "Epoch 1154, Loss: 0.5023521184921265, Final Batch Loss: 0.2397778332233429\n",
      "Epoch 1155, Loss: 0.438487246632576, Final Batch Loss: 0.2145308554172516\n",
      "Epoch 1156, Loss: 0.4930712580680847, Final Batch Loss: 0.24165582656860352\n",
      "Epoch 1157, Loss: 0.5155882686376572, Final Batch Loss: 0.32311907410621643\n",
      "Epoch 1158, Loss: 0.4564688205718994, Final Batch Loss: 0.2414744645357132\n",
      "Epoch 1159, Loss: 0.5464066565036774, Final Batch Loss: 0.27506303787231445\n",
      "Epoch 1160, Loss: 0.5548461377620697, Final Batch Loss: 0.34188467264175415\n",
      "Epoch 1161, Loss: 0.4749484062194824, Final Batch Loss: 0.20494332909584045\n",
      "Epoch 1162, Loss: 0.49871133267879486, Final Batch Loss: 0.24060000479221344\n",
      "Epoch 1163, Loss: 0.42596109211444855, Final Batch Loss: 0.18627749383449554\n",
      "Epoch 1164, Loss: 0.4991340935230255, Final Batch Loss: 0.24041017889976501\n",
      "Epoch 1165, Loss: 0.4495488554239273, Final Batch Loss: 0.19580985605716705\n",
      "Epoch 1166, Loss: 0.5467944145202637, Final Batch Loss: 0.22636204957962036\n",
      "Epoch 1167, Loss: 0.4591957628726959, Final Batch Loss: 0.20462298393249512\n",
      "Epoch 1168, Loss: 0.4702731966972351, Final Batch Loss: 0.2358570098876953\n",
      "Epoch 1169, Loss: 0.5320746302604675, Final Batch Loss: 0.31691601872444153\n",
      "Epoch 1170, Loss: 0.5497407913208008, Final Batch Loss: 0.3159036338329315\n",
      "Epoch 1171, Loss: 0.41656357049942017, Final Batch Loss: 0.18836820125579834\n",
      "Epoch 1172, Loss: 0.43627166748046875, Final Batch Loss: 0.21066589653491974\n",
      "Epoch 1173, Loss: 0.4780196398496628, Final Batch Loss: 0.2445678859949112\n",
      "Epoch 1174, Loss: 0.47863779962062836, Final Batch Loss: 0.25030744075775146\n",
      "Epoch 1175, Loss: 0.45983651280403137, Final Batch Loss: 0.25270265340805054\n",
      "Epoch 1176, Loss: 0.524967759847641, Final Batch Loss: 0.29983919858932495\n",
      "Epoch 1177, Loss: 0.44290266931056976, Final Batch Loss: 0.20158979296684265\n",
      "Epoch 1178, Loss: 0.5669198334217072, Final Batch Loss: 0.3024784028530121\n",
      "Epoch 1179, Loss: 0.5439892113208771, Final Batch Loss: 0.2794792056083679\n",
      "Epoch 1180, Loss: 0.5414102673530579, Final Batch Loss: 0.288157194852829\n",
      "Epoch 1181, Loss: 0.4930776059627533, Final Batch Loss: 0.24918310344219208\n",
      "Epoch 1182, Loss: 0.5547351241111755, Final Batch Loss: 0.29685744643211365\n",
      "Epoch 1183, Loss: 0.490011528134346, Final Batch Loss: 0.2392442375421524\n",
      "Epoch 1184, Loss: 0.4958319216966629, Final Batch Loss: 0.26541954278945923\n",
      "Epoch 1185, Loss: 0.47909705340862274, Final Batch Loss: 0.2174704521894455\n",
      "Epoch 1186, Loss: 0.4525638669729233, Final Batch Loss: 0.21397486329078674\n",
      "Epoch 1187, Loss: 0.5310959666967392, Final Batch Loss: 0.32206571102142334\n",
      "Epoch 1188, Loss: 0.45485225319862366, Final Batch Loss: 0.2228308767080307\n",
      "Epoch 1189, Loss: 0.4618414342403412, Final Batch Loss: 0.179319828748703\n",
      "Epoch 1190, Loss: 0.45365823805332184, Final Batch Loss: 0.20436212420463562\n",
      "Epoch 1191, Loss: 0.5351668000221252, Final Batch Loss: 0.16804969310760498\n",
      "Epoch 1192, Loss: 0.4649867117404938, Final Batch Loss: 0.21160727739334106\n",
      "Epoch 1193, Loss: 0.4504021853208542, Final Batch Loss: 0.22579577565193176\n",
      "Epoch 1194, Loss: 0.5062358975410461, Final Batch Loss: 0.2690379321575165\n",
      "Epoch 1195, Loss: 0.4781014323234558, Final Batch Loss: 0.26462244987487793\n",
      "Epoch 1196, Loss: 0.43641872704029083, Final Batch Loss: 0.23561370372772217\n",
      "Epoch 1197, Loss: 0.49688006937503815, Final Batch Loss: 0.2801740765571594\n",
      "Epoch 1198, Loss: 0.43888409435749054, Final Batch Loss: 0.23112523555755615\n",
      "Epoch 1199, Loss: 0.46225275099277496, Final Batch Loss: 0.21579615771770477\n",
      "Epoch 1200, Loss: 0.48158933222293854, Final Batch Loss: 0.22381411492824554\n",
      "Epoch 1201, Loss: 0.48758722841739655, Final Batch Loss: 0.2708286643028259\n",
      "Epoch 1202, Loss: 0.41915079951286316, Final Batch Loss: 0.17121519148349762\n",
      "Epoch 1203, Loss: 0.5326113402843475, Final Batch Loss: 0.28084254264831543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1204, Loss: 0.5453414618968964, Final Batch Loss: 0.3475673198699951\n",
      "Epoch 1205, Loss: 0.4118146300315857, Final Batch Loss: 0.19466806948184967\n",
      "Epoch 1206, Loss: 0.4334085136651993, Final Batch Loss: 0.18449769914150238\n",
      "Epoch 1207, Loss: 0.42334267497062683, Final Batch Loss: 0.1995791494846344\n",
      "Epoch 1208, Loss: 0.5722685009241104, Final Batch Loss: 0.33545300364494324\n",
      "Epoch 1209, Loss: 0.5385974943637848, Final Batch Loss: 0.27523475885391235\n",
      "Epoch 1210, Loss: 0.49488843977451324, Final Batch Loss: 0.2579730451107025\n",
      "Epoch 1211, Loss: 0.5244956314563751, Final Batch Loss: 0.24433153867721558\n",
      "Epoch 1212, Loss: 0.5099748969078064, Final Batch Loss: 0.28345006704330444\n",
      "Epoch 1213, Loss: 0.47825323045253754, Final Batch Loss: 0.22796379029750824\n",
      "Epoch 1214, Loss: 0.4524335116147995, Final Batch Loss: 0.19492383301258087\n",
      "Epoch 1215, Loss: 0.4926694929599762, Final Batch Loss: 0.2592044472694397\n",
      "Epoch 1216, Loss: 0.48167242109775543, Final Batch Loss: 0.18922187387943268\n",
      "Epoch 1217, Loss: 0.4771639257669449, Final Batch Loss: 0.24623580276966095\n",
      "Epoch 1218, Loss: 0.5719109177589417, Final Batch Loss: 0.31341442465782166\n",
      "Epoch 1219, Loss: 0.5014170557260513, Final Batch Loss: 0.267694890499115\n",
      "Epoch 1220, Loss: 0.4209078252315521, Final Batch Loss: 0.21135376393795013\n",
      "Epoch 1221, Loss: 0.47234928607940674, Final Batch Loss: 0.26613911986351013\n",
      "Epoch 1222, Loss: 0.5252472758293152, Final Batch Loss: 0.29272398352622986\n",
      "Epoch 1223, Loss: 0.5033194571733475, Final Batch Loss: 0.26577427983283997\n",
      "Epoch 1224, Loss: 0.5326735526323318, Final Batch Loss: 0.319744735956192\n",
      "Epoch 1225, Loss: 0.4676135182380676, Final Batch Loss: 0.23453199863433838\n",
      "Epoch 1226, Loss: 0.47325603663921356, Final Batch Loss: 0.2683243155479431\n",
      "Epoch 1227, Loss: 0.5172789394855499, Final Batch Loss: 0.26396122574806213\n",
      "Epoch 1228, Loss: 0.4715701639652252, Final Batch Loss: 0.24001701176166534\n",
      "Epoch 1229, Loss: 0.4237957000732422, Final Batch Loss: 0.2021835446357727\n",
      "Epoch 1230, Loss: 0.5016942769289017, Final Batch Loss: 0.2713107466697693\n",
      "Epoch 1231, Loss: 0.4087468534708023, Final Batch Loss: 0.18591955304145813\n",
      "Epoch 1232, Loss: 0.4481631964445114, Final Batch Loss: 0.22384554147720337\n",
      "Epoch 1233, Loss: 0.37864573299884796, Final Batch Loss: 0.17055730521678925\n",
      "Epoch 1234, Loss: 0.4505313038825989, Final Batch Loss: 0.22509333491325378\n",
      "Epoch 1235, Loss: 0.3884962499141693, Final Batch Loss: 0.1594802290201187\n",
      "Epoch 1236, Loss: 0.5319981873035431, Final Batch Loss: 0.26383307576179504\n",
      "Epoch 1237, Loss: 0.46332991123199463, Final Batch Loss: 0.24639374017715454\n",
      "Epoch 1238, Loss: 0.4144560247659683, Final Batch Loss: 0.17385233938694\n",
      "Epoch 1239, Loss: 0.42925915122032166, Final Batch Loss: 0.18731452524662018\n",
      "Epoch 1240, Loss: 0.45570437610149384, Final Batch Loss: 0.24845148622989655\n",
      "Epoch 1241, Loss: 0.3797186613082886, Final Batch Loss: 0.15716980397701263\n",
      "Epoch 1242, Loss: 0.456312820315361, Final Batch Loss: 0.23844222724437714\n",
      "Epoch 1243, Loss: 0.4361054450273514, Final Batch Loss: 0.18678390979766846\n",
      "Epoch 1244, Loss: 0.43120525777339935, Final Batch Loss: 0.2191239446401596\n",
      "Epoch 1245, Loss: 0.4883325546979904, Final Batch Loss: 0.25723880529403687\n",
      "Epoch 1246, Loss: 0.4982106238603592, Final Batch Loss: 0.2712053954601288\n",
      "Epoch 1247, Loss: 0.5092246234416962, Final Batch Loss: 0.28621038794517517\n",
      "Epoch 1248, Loss: 0.5006277561187744, Final Batch Loss: 0.27041545510292053\n",
      "Epoch 1249, Loss: 0.5306696891784668, Final Batch Loss: 0.29147687554359436\n",
      "Epoch 1250, Loss: 0.4331328868865967, Final Batch Loss: 0.17148348689079285\n",
      "Epoch 1251, Loss: 0.5111730992794037, Final Batch Loss: 0.3062930107116699\n",
      "Epoch 1252, Loss: 0.463645800948143, Final Batch Loss: 0.23254908621311188\n",
      "Epoch 1253, Loss: 0.3877333998680115, Final Batch Loss: 0.171824648976326\n",
      "Epoch 1254, Loss: 0.4424936920404434, Final Batch Loss: 0.24073636531829834\n",
      "Epoch 1255, Loss: 0.4103844165802002, Final Batch Loss: 0.202846959233284\n",
      "Epoch 1256, Loss: 0.4278883785009384, Final Batch Loss: 0.26140064001083374\n",
      "Epoch 1257, Loss: 0.4597224146127701, Final Batch Loss: 0.2162831723690033\n",
      "Epoch 1258, Loss: 0.4351969063282013, Final Batch Loss: 0.21287986636161804\n",
      "Epoch 1259, Loss: 0.46606382727622986, Final Batch Loss: 0.20357191562652588\n",
      "Epoch 1260, Loss: 0.47138968110084534, Final Batch Loss: 0.25543221831321716\n",
      "Epoch 1261, Loss: 0.4718649834394455, Final Batch Loss: 0.2309715747833252\n",
      "Epoch 1262, Loss: 0.5524459034204483, Final Batch Loss: 0.350536048412323\n",
      "Epoch 1263, Loss: 0.45789816975593567, Final Batch Loss: 0.2418767809867859\n",
      "Epoch 1264, Loss: 0.43931424617767334, Final Batch Loss: 0.22056573629379272\n",
      "Epoch 1265, Loss: 0.5001868009567261, Final Batch Loss: 0.23316115140914917\n",
      "Epoch 1266, Loss: 0.41832050681114197, Final Batch Loss: 0.21697767078876495\n",
      "Epoch 1267, Loss: 0.4623432606458664, Final Batch Loss: 0.24155692756175995\n",
      "Epoch 1268, Loss: 0.4521946907043457, Final Batch Loss: 0.26006609201431274\n",
      "Epoch 1269, Loss: 0.4344291388988495, Final Batch Loss: 0.22316984832286835\n",
      "Epoch 1270, Loss: 0.4903925061225891, Final Batch Loss: 0.2386438548564911\n",
      "Epoch 1271, Loss: 0.45753511786460876, Final Batch Loss: 0.2500273883342743\n",
      "Epoch 1272, Loss: 0.5096510350704193, Final Batch Loss: 0.22992375493049622\n",
      "Epoch 1273, Loss: 0.49538950622081757, Final Batch Loss: 0.28097620606422424\n",
      "Epoch 1274, Loss: 0.4275451898574829, Final Batch Loss: 0.2290468066930771\n",
      "Epoch 1275, Loss: 0.440440371632576, Final Batch Loss: 0.21267759799957275\n",
      "Epoch 1276, Loss: 0.4531724154949188, Final Batch Loss: 0.21887414157390594\n",
      "Epoch 1277, Loss: 0.37998999655246735, Final Batch Loss: 0.18582335114479065\n",
      "Epoch 1278, Loss: 0.4239175021648407, Final Batch Loss: 0.19201169908046722\n",
      "Epoch 1279, Loss: 0.4142841696739197, Final Batch Loss: 0.2156905084848404\n",
      "Epoch 1280, Loss: 0.45646393299102783, Final Batch Loss: 0.2543582320213318\n",
      "Epoch 1281, Loss: 0.4135439395904541, Final Batch Loss: 0.2002190500497818\n",
      "Epoch 1282, Loss: 0.453778013586998, Final Batch Loss: 0.21168534457683563\n",
      "Epoch 1283, Loss: 0.38336263597011566, Final Batch Loss: 0.15956467390060425\n",
      "Epoch 1284, Loss: 0.4807068109512329, Final Batch Loss: 0.23029550909996033\n",
      "Epoch 1285, Loss: 0.4135119915008545, Final Batch Loss: 0.1875579059123993\n",
      "Epoch 1286, Loss: 0.42315874993801117, Final Batch Loss: 0.22852464020252228\n",
      "Epoch 1287, Loss: 0.4091564863920212, Final Batch Loss: 0.1806294173002243\n",
      "Epoch 1288, Loss: 0.47673386335372925, Final Batch Loss: 0.26235470175743103\n",
      "Epoch 1289, Loss: 0.5039143860340118, Final Batch Loss: 0.2664451003074646\n",
      "Epoch 1290, Loss: 0.4653226137161255, Final Batch Loss: 0.2600082755088806\n",
      "Epoch 1291, Loss: 0.43168559670448303, Final Batch Loss: 0.19350621104240417\n",
      "Epoch 1292, Loss: 0.41183777153491974, Final Batch Loss: 0.1905096173286438\n",
      "Epoch 1293, Loss: 0.5150429904460907, Final Batch Loss: 0.2136252522468567\n",
      "Epoch 1294, Loss: 0.43025878071784973, Final Batch Loss: 0.1640586256980896\n",
      "Epoch 1295, Loss: 0.4499337822198868, Final Batch Loss: 0.20768176019191742\n",
      "Epoch 1296, Loss: 0.46472786366939545, Final Batch Loss: 0.25206953287124634\n",
      "Epoch 1297, Loss: 0.4296792596578598, Final Batch Loss: 0.2097247987985611\n",
      "Epoch 1298, Loss: 0.4337179660797119, Final Batch Loss: 0.21084792912006378\n",
      "Epoch 1299, Loss: 0.4284689277410507, Final Batch Loss: 0.2447574883699417\n",
      "Epoch 1300, Loss: 0.49730122089385986, Final Batch Loss: 0.23354050517082214\n",
      "Epoch 1301, Loss: 0.4405572712421417, Final Batch Loss: 0.19367268681526184\n",
      "Epoch 1302, Loss: 0.4393403083086014, Final Batch Loss: 0.22355154156684875\n",
      "Epoch 1303, Loss: 0.4090888202190399, Final Batch Loss: 0.19493402540683746\n",
      "Epoch 1304, Loss: 0.4817119985818863, Final Batch Loss: 0.22191961109638214\n",
      "Epoch 1305, Loss: 0.5721792876720428, Final Batch Loss: 0.3205379843711853\n",
      "Epoch 1306, Loss: 0.41910845041275024, Final Batch Loss: 0.20356065034866333\n",
      "Epoch 1307, Loss: 0.36074891686439514, Final Batch Loss: 0.12290462851524353\n",
      "Epoch 1308, Loss: 0.47532428801059723, Final Batch Loss: 0.22710742056369781\n",
      "Epoch 1309, Loss: 0.4586673229932785, Final Batch Loss: 0.17560289800167084\n",
      "Epoch 1310, Loss: 0.44119885563850403, Final Batch Loss: 0.22265870869159698\n",
      "Epoch 1311, Loss: 0.5264789164066315, Final Batch Loss: 0.2736510634422302\n",
      "Epoch 1312, Loss: 0.5807616412639618, Final Batch Loss: 0.28513631224632263\n",
      "Epoch 1313, Loss: 0.41580013930797577, Final Batch Loss: 0.17608599364757538\n",
      "Epoch 1314, Loss: 0.4727855920791626, Final Batch Loss: 0.2561640739440918\n",
      "Epoch 1315, Loss: 0.3580632954835892, Final Batch Loss: 0.1567985713481903\n",
      "Epoch 1316, Loss: 0.4365334212779999, Final Batch Loss: 0.1827714741230011\n",
      "Epoch 1317, Loss: 0.39664326608181, Final Batch Loss: 0.16442613303661346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1318, Loss: 0.4198768734931946, Final Batch Loss: 0.2153932899236679\n",
      "Epoch 1319, Loss: 0.40734873712062836, Final Batch Loss: 0.20624695718288422\n",
      "Epoch 1320, Loss: 0.49881120026111603, Final Batch Loss: 0.25080856680870056\n",
      "Epoch 1321, Loss: 0.49196550250053406, Final Batch Loss: 0.20029106736183167\n",
      "Epoch 1322, Loss: 0.4686856120824814, Final Batch Loss: 0.23437286913394928\n",
      "Epoch 1323, Loss: 0.4619804620742798, Final Batch Loss: 0.2601943016052246\n",
      "Epoch 1324, Loss: 0.39740778505802155, Final Batch Loss: 0.18948353826999664\n",
      "Epoch 1325, Loss: 0.4451800286769867, Final Batch Loss: 0.26169919967651367\n",
      "Epoch 1326, Loss: 0.4973333179950714, Final Batch Loss: 0.2766333818435669\n",
      "Epoch 1327, Loss: 0.5168737918138504, Final Batch Loss: 0.29559656977653503\n",
      "Epoch 1328, Loss: 0.4370548725128174, Final Batch Loss: 0.19316846132278442\n",
      "Epoch 1329, Loss: 0.44449353218078613, Final Batch Loss: 0.22214891016483307\n",
      "Epoch 1330, Loss: 0.4002896547317505, Final Batch Loss: 0.17468012869358063\n",
      "Epoch 1331, Loss: 0.4436024874448776, Final Batch Loss: 0.23905904591083527\n",
      "Epoch 1332, Loss: 0.40173810720443726, Final Batch Loss: 0.18759757280349731\n",
      "Epoch 1333, Loss: 0.4267297387123108, Final Batch Loss: 0.21875472366809845\n",
      "Epoch 1334, Loss: 0.38085417449474335, Final Batch Loss: 0.14793778955936432\n",
      "Epoch 1335, Loss: 0.5310208797454834, Final Batch Loss: 0.23875221610069275\n",
      "Epoch 1336, Loss: 0.47457508742809296, Final Batch Loss: 0.260326623916626\n",
      "Epoch 1337, Loss: 0.4353037625551224, Final Batch Loss: 0.22864924371242523\n",
      "Epoch 1338, Loss: 0.49638256430625916, Final Batch Loss: 0.28549113869667053\n",
      "Epoch 1339, Loss: 0.3923640102148056, Final Batch Loss: 0.19459818303585052\n",
      "Epoch 1340, Loss: 0.4236399233341217, Final Batch Loss: 0.18364569544792175\n",
      "Epoch 1341, Loss: 0.5986683070659637, Final Batch Loss: 0.247514009475708\n",
      "Epoch 1342, Loss: 0.4017544984817505, Final Batch Loss: 0.1846035271883011\n",
      "Epoch 1343, Loss: 0.5015906691551208, Final Batch Loss: 0.2935948371887207\n",
      "Epoch 1344, Loss: 0.4526079297065735, Final Batch Loss: 0.243582621216774\n",
      "Epoch 1345, Loss: 0.4405627101659775, Final Batch Loss: 0.2197037786245346\n",
      "Epoch 1346, Loss: 0.4648137390613556, Final Batch Loss: 0.1722082495689392\n",
      "Epoch 1347, Loss: 0.45852746069431305, Final Batch Loss: 0.27992239594459534\n",
      "Epoch 1348, Loss: 0.46877484023571014, Final Batch Loss: 0.2517673969268799\n",
      "Epoch 1349, Loss: 0.5004550069570541, Final Batch Loss: 0.29991385340690613\n",
      "Epoch 1350, Loss: 0.4634982645511627, Final Batch Loss: 0.22711363434791565\n",
      "Epoch 1351, Loss: 0.5293090790510178, Final Batch Loss: 0.30996137857437134\n",
      "Epoch 1352, Loss: 0.4145980626344681, Final Batch Loss: 0.20419488847255707\n",
      "Epoch 1353, Loss: 0.3904419094324112, Final Batch Loss: 0.1636047214269638\n",
      "Epoch 1354, Loss: 0.4490247517824173, Final Batch Loss: 0.17572717368602753\n",
      "Epoch 1355, Loss: 0.4195442944765091, Final Batch Loss: 0.21185676753520966\n",
      "Epoch 1356, Loss: 0.44511570036411285, Final Batch Loss: 0.21407607197761536\n",
      "Epoch 1357, Loss: 0.45193251967430115, Final Batch Loss: 0.2226146012544632\n",
      "Epoch 1358, Loss: 0.48218870162963867, Final Batch Loss: 0.26394590735435486\n",
      "Epoch 1359, Loss: 0.4906502962112427, Final Batch Loss: 0.27640441060066223\n",
      "Epoch 1360, Loss: 0.46947017312049866, Final Batch Loss: 0.22733739018440247\n",
      "Epoch 1361, Loss: 0.47860682010650635, Final Batch Loss: 0.24136117100715637\n",
      "Epoch 1362, Loss: 0.4595699906349182, Final Batch Loss: 0.22588416934013367\n",
      "Epoch 1363, Loss: 0.4336162656545639, Final Batch Loss: 0.19294394552707672\n",
      "Epoch 1364, Loss: 0.4783441573381424, Final Batch Loss: 0.21868185698986053\n",
      "Epoch 1365, Loss: 0.49853309988975525, Final Batch Loss: 0.27284902334213257\n",
      "Epoch 1366, Loss: 0.4972914904356003, Final Batch Loss: 0.2412693053483963\n",
      "Epoch 1367, Loss: 0.41190023720264435, Final Batch Loss: 0.20064684748649597\n",
      "Epoch 1368, Loss: 0.4334946572780609, Final Batch Loss: 0.18573914468288422\n",
      "Epoch 1369, Loss: 0.45509836077690125, Final Batch Loss: 0.25119051337242126\n",
      "Epoch 1370, Loss: 0.37074024975299835, Final Batch Loss: 0.1900937557220459\n",
      "Epoch 1371, Loss: 0.45492975413799286, Final Batch Loss: 0.21895262598991394\n",
      "Epoch 1372, Loss: 0.38939715921878815, Final Batch Loss: 0.18111328780651093\n",
      "Epoch 1373, Loss: 0.42125527560710907, Final Batch Loss: 0.21752876043319702\n",
      "Epoch 1374, Loss: 0.4293972998857498, Final Batch Loss: 0.23115164041519165\n",
      "Epoch 1375, Loss: 0.38918958604335785, Final Batch Loss: 0.1895541399717331\n",
      "Epoch 1376, Loss: 0.42647838592529297, Final Batch Loss: 0.20997068285942078\n",
      "Epoch 1377, Loss: 0.41971588134765625, Final Batch Loss: 0.23257790505886078\n",
      "Epoch 1378, Loss: 0.44107863306999207, Final Batch Loss: 0.19677497446537018\n",
      "Epoch 1379, Loss: 0.3893735110759735, Final Batch Loss: 0.19218489527702332\n",
      "Epoch 1380, Loss: 0.5442841053009033, Final Batch Loss: 0.30778613686561584\n",
      "Epoch 1381, Loss: 0.39821261167526245, Final Batch Loss: 0.2278233915567398\n",
      "Epoch 1382, Loss: 0.39137037098407745, Final Batch Loss: 0.17384092509746552\n",
      "Epoch 1383, Loss: 0.4633922874927521, Final Batch Loss: 0.27525457739830017\n",
      "Epoch 1384, Loss: 0.4330923855304718, Final Batch Loss: 0.2039741575717926\n",
      "Epoch 1385, Loss: 0.4013664722442627, Final Batch Loss: 0.1802918016910553\n",
      "Epoch 1386, Loss: 0.38148869574069977, Final Batch Loss: 0.15272852778434753\n",
      "Epoch 1387, Loss: 0.4093100130558014, Final Batch Loss: 0.1577509343624115\n",
      "Epoch 1388, Loss: 0.439859002828598, Final Batch Loss: 0.22774071991443634\n",
      "Epoch 1389, Loss: 0.47601252794265747, Final Batch Loss: 0.27899640798568726\n",
      "Epoch 1390, Loss: 0.4879859983921051, Final Batch Loss: 0.24571914970874786\n",
      "Epoch 1391, Loss: 0.4082331657409668, Final Batch Loss: 0.19747458398342133\n",
      "Epoch 1392, Loss: 0.36335036158561707, Final Batch Loss: 0.15582875907421112\n",
      "Epoch 1393, Loss: 0.4617294520139694, Final Batch Loss: 0.2612372040748596\n",
      "Epoch 1394, Loss: 0.4402036666870117, Final Batch Loss: 0.20249105989933014\n",
      "Epoch 1395, Loss: 0.5429697930812836, Final Batch Loss: 0.33890849351882935\n",
      "Epoch 1396, Loss: 0.40534868836402893, Final Batch Loss: 0.2053392231464386\n",
      "Epoch 1397, Loss: 0.4441152960062027, Final Batch Loss: 0.258835107088089\n",
      "Epoch 1398, Loss: 0.48803384602069855, Final Batch Loss: 0.2557440400123596\n",
      "Epoch 1399, Loss: 0.41409145295619965, Final Batch Loss: 0.1847006231546402\n",
      "Epoch 1400, Loss: 0.38836532831192017, Final Batch Loss: 0.19730821251869202\n",
      "Epoch 1401, Loss: 0.40233613550662994, Final Batch Loss: 0.19330720603466034\n",
      "Epoch 1402, Loss: 0.39676208794116974, Final Batch Loss: 0.1900923252105713\n",
      "Epoch 1403, Loss: 0.3961851894855499, Final Batch Loss: 0.18417885899543762\n",
      "Epoch 1404, Loss: 0.44476041197776794, Final Batch Loss: 0.22611360251903534\n",
      "Epoch 1405, Loss: 0.4827839285135269, Final Batch Loss: 0.25072988867759705\n",
      "Epoch 1406, Loss: 0.4528932124376297, Final Batch Loss: 0.22664949297904968\n",
      "Epoch 1407, Loss: 0.4642867147922516, Final Batch Loss: 0.2142198085784912\n",
      "Epoch 1408, Loss: 0.5333014279603958, Final Batch Loss: 0.30183759331703186\n",
      "Epoch 1409, Loss: 0.3947477340698242, Final Batch Loss: 0.1765955537557602\n",
      "Epoch 1410, Loss: 0.4140084236860275, Final Batch Loss: 0.17689336836338043\n",
      "Epoch 1411, Loss: 0.4004110097885132, Final Batch Loss: 0.14029455184936523\n",
      "Epoch 1412, Loss: 0.5027308911085129, Final Batch Loss: 0.2248927503824234\n",
      "Epoch 1413, Loss: 0.4274651110172272, Final Batch Loss: 0.2423330694437027\n",
      "Epoch 1414, Loss: 0.485127255320549, Final Batch Loss: 0.24483536183834076\n",
      "Epoch 1415, Loss: 0.42757171392440796, Final Batch Loss: 0.1514981985092163\n",
      "Epoch 1416, Loss: 0.44267481565475464, Final Batch Loss: 0.21549078822135925\n",
      "Epoch 1417, Loss: 0.42192286252975464, Final Batch Loss: 0.2029300034046173\n",
      "Epoch 1418, Loss: 0.4516120105981827, Final Batch Loss: 0.20966966450214386\n",
      "Epoch 1419, Loss: 0.42563459277153015, Final Batch Loss: 0.19562296569347382\n",
      "Epoch 1420, Loss: 0.419780895113945, Final Batch Loss: 0.19592086970806122\n",
      "Epoch 1421, Loss: 0.38242678344249725, Final Batch Loss: 0.155946284532547\n",
      "Epoch 1422, Loss: 0.46233396232128143, Final Batch Loss: 0.23373283445835114\n",
      "Epoch 1423, Loss: 0.37893038988113403, Final Batch Loss: 0.16129864752292633\n",
      "Epoch 1424, Loss: 0.4618104100227356, Final Batch Loss: 0.26467710733413696\n",
      "Epoch 1425, Loss: 0.44477929174900055, Final Batch Loss: 0.2326616644859314\n",
      "Epoch 1426, Loss: 0.45084497332572937, Final Batch Loss: 0.2204708307981491\n",
      "Epoch 1427, Loss: 0.4121647924184799, Final Batch Loss: 0.16473813354969025\n",
      "Epoch 1428, Loss: 0.5249344557523727, Final Batch Loss: 0.3209356665611267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1429, Loss: 0.3963880240917206, Final Batch Loss: 0.17416688799858093\n",
      "Epoch 1430, Loss: 0.43401025235652924, Final Batch Loss: 0.22883634269237518\n",
      "Epoch 1431, Loss: 0.43013519048690796, Final Batch Loss: 0.2351420372724533\n",
      "Epoch 1432, Loss: 0.38228537142276764, Final Batch Loss: 0.16171583533287048\n",
      "Epoch 1433, Loss: 0.45100197196006775, Final Batch Loss: 0.2566121220588684\n",
      "Epoch 1434, Loss: 0.44171565771102905, Final Batch Loss: 0.2088664174079895\n",
      "Epoch 1435, Loss: 0.42871329188346863, Final Batch Loss: 0.19699127972126007\n",
      "Epoch 1436, Loss: 0.44241903722286224, Final Batch Loss: 0.2524878978729248\n",
      "Epoch 1437, Loss: 0.5152338743209839, Final Batch Loss: 0.2500355541706085\n",
      "Epoch 1438, Loss: 0.3770151734352112, Final Batch Loss: 0.19620458781719208\n",
      "Epoch 1439, Loss: 0.39629969000816345, Final Batch Loss: 0.200076162815094\n",
      "Epoch 1440, Loss: 0.494319811463356, Final Batch Loss: 0.28145113587379456\n",
      "Epoch 1441, Loss: 0.46578918397426605, Final Batch Loss: 0.26276254653930664\n",
      "Epoch 1442, Loss: 0.4136779010295868, Final Batch Loss: 0.20369349420070648\n",
      "Epoch 1443, Loss: 0.4116482138633728, Final Batch Loss: 0.19448548555374146\n",
      "Epoch 1444, Loss: 0.3763652741909027, Final Batch Loss: 0.13753686845302582\n",
      "Epoch 1445, Loss: 0.4025695472955704, Final Batch Loss: 0.18339700996875763\n",
      "Epoch 1446, Loss: 0.4635758548974991, Final Batch Loss: 0.24436281621456146\n",
      "Epoch 1447, Loss: 0.42137518525123596, Final Batch Loss: 0.2087213397026062\n",
      "Epoch 1448, Loss: 0.4608811140060425, Final Batch Loss: 0.22550582885742188\n",
      "Epoch 1449, Loss: 0.4324980527162552, Final Batch Loss: 0.2505653202533722\n",
      "Epoch 1450, Loss: 0.44994401931762695, Final Batch Loss: 0.20803694427013397\n",
      "Epoch 1451, Loss: 0.45221808552742004, Final Batch Loss: 0.22213338315486908\n",
      "Epoch 1452, Loss: 0.44948136806488037, Final Batch Loss: 0.23427419364452362\n",
      "Epoch 1453, Loss: 0.38645440340042114, Final Batch Loss: 0.1759229153394699\n",
      "Epoch 1454, Loss: 0.37148405611515045, Final Batch Loss: 0.16371606290340424\n",
      "Epoch 1455, Loss: 0.40344008803367615, Final Batch Loss: 0.19147422909736633\n",
      "Epoch 1456, Loss: 0.4319290220737457, Final Batch Loss: 0.21917128562927246\n",
      "Epoch 1457, Loss: 0.3883467763662338, Final Batch Loss: 0.16442060470581055\n",
      "Epoch 1458, Loss: 0.522872805595398, Final Batch Loss: 0.26961421966552734\n",
      "Epoch 1459, Loss: 0.5094143152236938, Final Batch Loss: 0.28517886996269226\n",
      "Epoch 1460, Loss: 0.5589247643947601, Final Batch Loss: 0.29204046726226807\n",
      "Epoch 1461, Loss: 0.44479863345623016, Final Batch Loss: 0.21873541176319122\n",
      "Epoch 1462, Loss: 0.446345716714859, Final Batch Loss: 0.21583841741085052\n",
      "Epoch 1463, Loss: 0.46700112521648407, Final Batch Loss: 0.2585778832435608\n",
      "Epoch 1464, Loss: 0.4192993491888046, Final Batch Loss: 0.21824483573436737\n",
      "Epoch 1465, Loss: 0.3993673324584961, Final Batch Loss: 0.1632339507341385\n",
      "Epoch 1466, Loss: 0.3935353308916092, Final Batch Loss: 0.1901228427886963\n",
      "Epoch 1467, Loss: 0.42948371171951294, Final Batch Loss: 0.2504532039165497\n",
      "Epoch 1468, Loss: 0.5361223965883255, Final Batch Loss: 0.3428489863872528\n",
      "Epoch 1469, Loss: 0.40586893260478973, Final Batch Loss: 0.18612803518772125\n",
      "Epoch 1470, Loss: 0.43623384833335876, Final Batch Loss: 0.18184560537338257\n",
      "Epoch 1471, Loss: 0.43190234899520874, Final Batch Loss: 0.20364554226398468\n",
      "Epoch 1472, Loss: 0.46060414612293243, Final Batch Loss: 0.20958198606967926\n",
      "Epoch 1473, Loss: 0.400383323431015, Final Batch Loss: 0.1663324534893036\n",
      "Epoch 1474, Loss: 0.4229770600795746, Final Batch Loss: 0.21242766082286835\n",
      "Epoch 1475, Loss: 0.5006411224603653, Final Batch Loss: 0.2870437204837799\n",
      "Epoch 1476, Loss: 0.39798395335674286, Final Batch Loss: 0.19157597422599792\n",
      "Epoch 1477, Loss: 0.4861461669206619, Final Batch Loss: 0.28697264194488525\n",
      "Epoch 1478, Loss: 0.404675230383873, Final Batch Loss: 0.1939726322889328\n",
      "Epoch 1479, Loss: 0.4264221042394638, Final Batch Loss: 0.22811774909496307\n",
      "Epoch 1480, Loss: 0.4804971367120743, Final Batch Loss: 0.2205459028482437\n",
      "Epoch 1481, Loss: 0.42582549154758453, Final Batch Loss: 0.19881202280521393\n",
      "Epoch 1482, Loss: 0.3788479119539261, Final Batch Loss: 0.19490979611873627\n",
      "Epoch 1483, Loss: 0.4225485324859619, Final Batch Loss: 0.183668851852417\n",
      "Epoch 1484, Loss: 0.3959522545337677, Final Batch Loss: 0.19412831962108612\n",
      "Epoch 1485, Loss: 0.44156427681446075, Final Batch Loss: 0.2217094600200653\n",
      "Epoch 1486, Loss: 0.45488089323043823, Final Batch Loss: 0.27569735050201416\n",
      "Epoch 1487, Loss: 0.49574077129364014, Final Batch Loss: 0.2727784812450409\n",
      "Epoch 1488, Loss: 0.43863724172115326, Final Batch Loss: 0.18325479328632355\n",
      "Epoch 1489, Loss: 0.42249421775341034, Final Batch Loss: 0.20302894711494446\n",
      "Epoch 1490, Loss: 0.4178456664085388, Final Batch Loss: 0.19545777142047882\n",
      "Epoch 1491, Loss: 0.4483923614025116, Final Batch Loss: 0.2636428773403168\n",
      "Epoch 1492, Loss: 0.3571082055568695, Final Batch Loss: 0.16338635981082916\n",
      "Epoch 1493, Loss: 0.41845187544822693, Final Batch Loss: 0.20219820737838745\n",
      "Epoch 1494, Loss: 0.41830582916736603, Final Batch Loss: 0.1834128350019455\n",
      "Epoch 1495, Loss: 0.4169192314147949, Final Batch Loss: 0.2292855679988861\n",
      "Epoch 1496, Loss: 0.35684868693351746, Final Batch Loss: 0.15213242173194885\n",
      "Epoch 1497, Loss: 0.4116944819688797, Final Batch Loss: 0.1803303360939026\n",
      "Epoch 1498, Loss: 0.4514605402946472, Final Batch Loss: 0.19691917300224304\n",
      "Epoch 1499, Loss: 0.4269760698080063, Final Batch Loss: 0.26843440532684326\n",
      "Epoch 1500, Loss: 0.46336473524570465, Final Batch Loss: 0.2498529851436615\n",
      "Epoch 1501, Loss: 0.36621667444705963, Final Batch Loss: 0.1828751116991043\n",
      "Epoch 1502, Loss: 0.39975081384181976, Final Batch Loss: 0.18442435562610626\n",
      "Epoch 1503, Loss: 0.4330897629261017, Final Batch Loss: 0.19703125953674316\n",
      "Epoch 1504, Loss: 0.4093765318393707, Final Batch Loss: 0.17223000526428223\n",
      "Epoch 1505, Loss: 0.41158126294612885, Final Batch Loss: 0.18063336610794067\n",
      "Epoch 1506, Loss: 0.42381300032138824, Final Batch Loss: 0.21117979288101196\n",
      "Epoch 1507, Loss: 0.45893019437789917, Final Batch Loss: 0.23660336434841156\n",
      "Epoch 1508, Loss: 0.47833698987960815, Final Batch Loss: 0.2509801685810089\n",
      "Epoch 1509, Loss: 0.4888472408056259, Final Batch Loss: 0.28805020451545715\n",
      "Epoch 1510, Loss: 0.4396946132183075, Final Batch Loss: 0.15746286511421204\n",
      "Epoch 1511, Loss: 0.5049518346786499, Final Batch Loss: 0.31859394907951355\n",
      "Epoch 1512, Loss: 0.44718557596206665, Final Batch Loss: 0.23763695359230042\n",
      "Epoch 1513, Loss: 0.41503390669822693, Final Batch Loss: 0.1831476092338562\n",
      "Epoch 1514, Loss: 0.41215141117572784, Final Batch Loss: 0.23094303905963898\n",
      "Epoch 1515, Loss: 0.4027363061904907, Final Batch Loss: 0.2024586945772171\n",
      "Epoch 1516, Loss: 0.37263715267181396, Final Batch Loss: 0.17826741933822632\n",
      "Epoch 1517, Loss: 0.45177875459194183, Final Batch Loss: 0.23419053852558136\n",
      "Epoch 1518, Loss: 0.4160761386156082, Final Batch Loss: 0.18587324023246765\n",
      "Epoch 1519, Loss: 0.436196506023407, Final Batch Loss: 0.22930343449115753\n",
      "Epoch 1520, Loss: 0.4302564412355423, Final Batch Loss: 0.2203000783920288\n",
      "Epoch 1521, Loss: 0.39286305010318756, Final Batch Loss: 0.1761861890554428\n",
      "Epoch 1522, Loss: 0.4185662567615509, Final Batch Loss: 0.19595955312252045\n",
      "Epoch 1523, Loss: 0.35117536783218384, Final Batch Loss: 0.11160580813884735\n",
      "Epoch 1524, Loss: 0.4006788283586502, Final Batch Loss: 0.17198897898197174\n",
      "Epoch 1525, Loss: 0.39302971959114075, Final Batch Loss: 0.18647630512714386\n",
      "Epoch 1526, Loss: 0.4168332666158676, Final Batch Loss: 0.20809516310691833\n",
      "Epoch 1527, Loss: 0.4288489520549774, Final Batch Loss: 0.24333032965660095\n",
      "Epoch 1528, Loss: 0.4338809847831726, Final Batch Loss: 0.20647978782653809\n",
      "Epoch 1529, Loss: 0.4903113394975662, Final Batch Loss: 0.28547292947769165\n",
      "Epoch 1530, Loss: 0.4350544512271881, Final Batch Loss: 0.22440417110919952\n",
      "Epoch 1531, Loss: 0.340310662984848, Final Batch Loss: 0.17233920097351074\n",
      "Epoch 1532, Loss: 0.5558555722236633, Final Batch Loss: 0.32368865609169006\n",
      "Epoch 1533, Loss: 0.44597382843494415, Final Batch Loss: 0.24946540594100952\n",
      "Epoch 1534, Loss: 0.42760850489139557, Final Batch Loss: 0.19940927624702454\n",
      "Epoch 1535, Loss: 0.4310053139925003, Final Batch Loss: 0.1760835200548172\n",
      "Epoch 1536, Loss: 0.3320518583059311, Final Batch Loss: 0.15296326577663422\n",
      "Epoch 1537, Loss: 0.39584095776081085, Final Batch Loss: 0.19336938858032227\n",
      "Epoch 1538, Loss: 0.4302862882614136, Final Batch Loss: 0.2526857852935791\n",
      "Epoch 1539, Loss: 0.41892313957214355, Final Batch Loss: 0.22930246591567993\n",
      "Epoch 1540, Loss: 0.4034740924835205, Final Batch Loss: 0.2046433389186859\n",
      "Epoch 1541, Loss: 0.5243815332651138, Final Batch Loss: 0.274867445230484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1542, Loss: 0.4715178906917572, Final Batch Loss: 0.2791464626789093\n",
      "Epoch 1543, Loss: 0.36591118574142456, Final Batch Loss: 0.15338805317878723\n",
      "Epoch 1544, Loss: 0.4495592415332794, Final Batch Loss: 0.1913718581199646\n",
      "Epoch 1545, Loss: 0.44571787118911743, Final Batch Loss: 0.2316124141216278\n",
      "Epoch 1546, Loss: 0.5003170222043991, Final Batch Loss: 0.3044961392879486\n",
      "Epoch 1547, Loss: 0.46236173808574677, Final Batch Loss: 0.2631588578224182\n",
      "Epoch 1548, Loss: 0.37342554330825806, Final Batch Loss: 0.15060578286647797\n",
      "Epoch 1549, Loss: 0.4287153482437134, Final Batch Loss: 0.2298027127981186\n",
      "Epoch 1550, Loss: 0.37842172384262085, Final Batch Loss: 0.16596084833145142\n",
      "Epoch 1551, Loss: 0.4137255996465683, Final Batch Loss: 0.2034812867641449\n",
      "Epoch 1552, Loss: 0.38124775886535645, Final Batch Loss: 0.17226290702819824\n",
      "Epoch 1553, Loss: 0.44391465187072754, Final Batch Loss: 0.21433259546756744\n",
      "Epoch 1554, Loss: 0.44522158801555634, Final Batch Loss: 0.2401772439479828\n",
      "Epoch 1555, Loss: 0.4403880536556244, Final Batch Loss: 0.24474197626113892\n",
      "Epoch 1556, Loss: 0.4929628074169159, Final Batch Loss: 0.2283722460269928\n",
      "Epoch 1557, Loss: 0.4041910469532013, Final Batch Loss: 0.18982133269309998\n",
      "Epoch 1558, Loss: 0.44847726821899414, Final Batch Loss: 0.22346925735473633\n",
      "Epoch 1559, Loss: 0.41978229582309723, Final Batch Loss: 0.21670080721378326\n",
      "Epoch 1560, Loss: 0.3678712099790573, Final Batch Loss: 0.17572049796581268\n",
      "Epoch 1561, Loss: 0.3720240741968155, Final Batch Loss: 0.15020081400871277\n",
      "Epoch 1562, Loss: 0.4266587644815445, Final Batch Loss: 0.20629069209098816\n",
      "Epoch 1563, Loss: 0.41409146785736084, Final Batch Loss: 0.20573832094669342\n",
      "Epoch 1564, Loss: 0.3624317795038223, Final Batch Loss: 0.1553804874420166\n",
      "Epoch 1565, Loss: 0.4687229245901108, Final Batch Loss: 0.23330672085285187\n",
      "Epoch 1566, Loss: 0.4287075102329254, Final Batch Loss: 0.2212054282426834\n",
      "Epoch 1567, Loss: 0.3670978248119354, Final Batch Loss: 0.15961898863315582\n",
      "Epoch 1568, Loss: 0.36247508227825165, Final Batch Loss: 0.16530302166938782\n",
      "Epoch 1569, Loss: 0.4035259932279587, Final Batch Loss: 0.17412598431110382\n",
      "Epoch 1570, Loss: 0.40396250784397125, Final Batch Loss: 0.19399480521678925\n",
      "Epoch 1571, Loss: 0.4405165910720825, Final Batch Loss: 0.24054229259490967\n",
      "Epoch 1572, Loss: 0.3926551938056946, Final Batch Loss: 0.18616648018360138\n",
      "Epoch 1573, Loss: 0.45983271300792694, Final Batch Loss: 0.24601736664772034\n",
      "Epoch 1574, Loss: 0.4350719749927521, Final Batch Loss: 0.19565193355083466\n",
      "Epoch 1575, Loss: 0.3732975125312805, Final Batch Loss: 0.1503528654575348\n",
      "Epoch 1576, Loss: 0.4310671538114548, Final Batch Loss: 0.22740809619426727\n",
      "Epoch 1577, Loss: 0.4347136616706848, Final Batch Loss: 0.2636162042617798\n",
      "Epoch 1578, Loss: 0.3177715390920639, Final Batch Loss: 0.13251280784606934\n",
      "Epoch 1579, Loss: 0.3959546834230423, Final Batch Loss: 0.18893656134605408\n",
      "Epoch 1580, Loss: 0.48140496015548706, Final Batch Loss: 0.22768792510032654\n",
      "Epoch 1581, Loss: 0.49122799932956696, Final Batch Loss: 0.2828303575515747\n",
      "Epoch 1582, Loss: 0.3604893386363983, Final Batch Loss: 0.17102709412574768\n",
      "Epoch 1583, Loss: 0.4193970412015915, Final Batch Loss: 0.19270853698253632\n",
      "Epoch 1584, Loss: 0.45267626643180847, Final Batch Loss: 0.24741357564926147\n",
      "Epoch 1585, Loss: 0.37576110661029816, Final Batch Loss: 0.1653805524110794\n",
      "Epoch 1586, Loss: 0.4215078204870224, Final Batch Loss: 0.21115277707576752\n",
      "Epoch 1587, Loss: 0.411660373210907, Final Batch Loss: 0.18339146673679352\n",
      "Epoch 1588, Loss: 0.4087720811367035, Final Batch Loss: 0.21490991115570068\n",
      "Epoch 1589, Loss: 0.42699795961380005, Final Batch Loss: 0.23076093196868896\n",
      "Epoch 1590, Loss: 0.4390120357275009, Final Batch Loss: 0.20419687032699585\n",
      "Epoch 1591, Loss: 0.4027916043996811, Final Batch Loss: 0.17326104640960693\n",
      "Epoch 1592, Loss: 0.38368311524391174, Final Batch Loss: 0.14984385669231415\n",
      "Epoch 1593, Loss: 0.4164215177297592, Final Batch Loss: 0.2521224617958069\n",
      "Epoch 1594, Loss: 0.3671509176492691, Final Batch Loss: 0.16843821108341217\n",
      "Epoch 1595, Loss: 0.45744025707244873, Final Batch Loss: 0.2636365592479706\n",
      "Epoch 1596, Loss: 0.3736421465873718, Final Batch Loss: 0.18605156242847443\n",
      "Epoch 1597, Loss: 0.45142732560634613, Final Batch Loss: 0.2549082636833191\n",
      "Epoch 1598, Loss: 0.3781166672706604, Final Batch Loss: 0.14402388036251068\n",
      "Epoch 1599, Loss: 0.39158493280410767, Final Batch Loss: 0.19542157649993896\n",
      "Epoch 1600, Loss: 0.3951851576566696, Final Batch Loss: 0.18630658090114594\n",
      "Epoch 1601, Loss: 0.5103061646223068, Final Batch Loss: 0.28725385665893555\n",
      "Epoch 1602, Loss: 0.48412102460861206, Final Batch Loss: 0.302148699760437\n",
      "Epoch 1603, Loss: 0.5100209712982178, Final Batch Loss: 0.2676659822463989\n",
      "Epoch 1604, Loss: 0.36785197257995605, Final Batch Loss: 0.1898675411939621\n",
      "Epoch 1605, Loss: 0.5004681199789047, Final Batch Loss: 0.31632763147354126\n",
      "Epoch 1606, Loss: 0.43044741451740265, Final Batch Loss: 0.2082659751176834\n",
      "Epoch 1607, Loss: 0.4447503834962845, Final Batch Loss: 0.21652983129024506\n",
      "Epoch 1608, Loss: 0.39131785929203033, Final Batch Loss: 0.20942595601081848\n",
      "Epoch 1609, Loss: 0.4026372879743576, Final Batch Loss: 0.1812318116426468\n",
      "Epoch 1610, Loss: 0.4119054526090622, Final Batch Loss: 0.19349311292171478\n",
      "Epoch 1611, Loss: 0.4809942990541458, Final Batch Loss: 0.20142783224582672\n",
      "Epoch 1612, Loss: 0.3905870318412781, Final Batch Loss: 0.13182207942008972\n",
      "Epoch 1613, Loss: 0.3825834095478058, Final Batch Loss: 0.1690477728843689\n",
      "Epoch 1614, Loss: 0.4492473155260086, Final Batch Loss: 0.2250104397535324\n",
      "Epoch 1615, Loss: 0.4218258708715439, Final Batch Loss: 0.18153075873851776\n",
      "Epoch 1616, Loss: 0.4474099576473236, Final Batch Loss: 0.27571502327919006\n",
      "Epoch 1617, Loss: 0.4909467548131943, Final Batch Loss: 0.31167736649513245\n",
      "Epoch 1618, Loss: 0.42300038039684296, Final Batch Loss: 0.2039395272731781\n",
      "Epoch 1619, Loss: 0.46118776500225067, Final Batch Loss: 0.27481234073638916\n",
      "Epoch 1620, Loss: 0.4454144984483719, Final Batch Loss: 0.18626846373081207\n",
      "Epoch 1621, Loss: 0.39438819885253906, Final Batch Loss: 0.198627308011055\n",
      "Epoch 1622, Loss: 0.3771679699420929, Final Batch Loss: 0.19214081764221191\n",
      "Epoch 1623, Loss: 0.446328341960907, Final Batch Loss: 0.23744593560695648\n",
      "Epoch 1624, Loss: 0.39182472229003906, Final Batch Loss: 0.17913126945495605\n",
      "Epoch 1625, Loss: 0.41750897467136383, Final Batch Loss: 0.2224855273962021\n",
      "Epoch 1626, Loss: 0.3972458243370056, Final Batch Loss: 0.20357902348041534\n",
      "Epoch 1627, Loss: 0.46017707884311676, Final Batch Loss: 0.2088046818971634\n",
      "Epoch 1628, Loss: 0.4283309131860733, Final Batch Loss: 0.21422646939754486\n",
      "Epoch 1629, Loss: 0.40537092089653015, Final Batch Loss: 0.21160168945789337\n",
      "Epoch 1630, Loss: 0.3915832191705704, Final Batch Loss: 0.18228814005851746\n",
      "Epoch 1631, Loss: 0.4566807448863983, Final Batch Loss: 0.28092265129089355\n",
      "Epoch 1632, Loss: 0.34922143816947937, Final Batch Loss: 0.1867566853761673\n",
      "Epoch 1633, Loss: 0.4309152513742447, Final Batch Loss: 0.21446752548217773\n",
      "Epoch 1634, Loss: 0.40680426359176636, Final Batch Loss: 0.20552027225494385\n",
      "Epoch 1635, Loss: 0.4531753659248352, Final Batch Loss: 0.2550433576107025\n",
      "Epoch 1636, Loss: 0.5191249698400497, Final Batch Loss: 0.31973913311958313\n",
      "Epoch 1637, Loss: 0.4056704342365265, Final Batch Loss: 0.23628699779510498\n",
      "Epoch 1638, Loss: 0.4216427206993103, Final Batch Loss: 0.215934619307518\n",
      "Epoch 1639, Loss: 0.43950970470905304, Final Batch Loss: 0.20281746983528137\n",
      "Epoch 1640, Loss: 0.3501465171575546, Final Batch Loss: 0.14829453825950623\n",
      "Epoch 1641, Loss: 0.453630268573761, Final Batch Loss: 0.24867631494998932\n",
      "Epoch 1642, Loss: 0.4636746644973755, Final Batch Loss: 0.2795707583427429\n",
      "Epoch 1643, Loss: 0.4302152991294861, Final Batch Loss: 0.2130681574344635\n",
      "Epoch 1644, Loss: 0.4325903356075287, Final Batch Loss: 0.17474165558815002\n",
      "Epoch 1645, Loss: 0.3861197233200073, Final Batch Loss: 0.19707873463630676\n",
      "Epoch 1646, Loss: 0.515584260225296, Final Batch Loss: 0.2988119125366211\n",
      "Epoch 1647, Loss: 0.5125899165868759, Final Batch Loss: 0.3256398141384125\n",
      "Epoch 1648, Loss: 0.410345658659935, Final Batch Loss: 0.18969909846782684\n",
      "Epoch 1649, Loss: 0.42847348749637604, Final Batch Loss: 0.21908262372016907\n",
      "Epoch 1650, Loss: 0.432573065161705, Final Batch Loss: 0.20121216773986816\n",
      "Epoch 1651, Loss: 0.44321398437023163, Final Batch Loss: 0.21741563081741333\n",
      "Epoch 1652, Loss: 0.44754813611507416, Final Batch Loss: 0.233135387301445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1653, Loss: 0.42772263288497925, Final Batch Loss: 0.17712587118148804\n",
      "Epoch 1654, Loss: 0.3747766464948654, Final Batch Loss: 0.1798435002565384\n",
      "Epoch 1655, Loss: 0.45327867567539215, Final Batch Loss: 0.2715173661708832\n",
      "Epoch 1656, Loss: 0.4612688720226288, Final Batch Loss: 0.24805152416229248\n",
      "Epoch 1657, Loss: 0.3968425691127777, Final Batch Loss: 0.1839446872472763\n",
      "Epoch 1658, Loss: 0.4583321213722229, Final Batch Loss: 0.2363535463809967\n",
      "Epoch 1659, Loss: 0.49334658682346344, Final Batch Loss: 0.29196467995643616\n",
      "Epoch 1660, Loss: 0.34899963438510895, Final Batch Loss: 0.18515874445438385\n",
      "Epoch 1661, Loss: 0.4558435380458832, Final Batch Loss: 0.260369211435318\n",
      "Epoch 1662, Loss: 0.4097157418727875, Final Batch Loss: 0.24880801141262054\n",
      "Epoch 1663, Loss: 0.4068557173013687, Final Batch Loss: 0.18118026852607727\n",
      "Epoch 1664, Loss: 0.4145810306072235, Final Batch Loss: 0.19637301564216614\n",
      "Epoch 1665, Loss: 0.4287222921848297, Final Batch Loss: 0.20024271309375763\n",
      "Epoch 1666, Loss: 0.4310583919286728, Final Batch Loss: 0.21883273124694824\n",
      "Epoch 1667, Loss: 0.3979259580373764, Final Batch Loss: 0.13957206904888153\n",
      "Epoch 1668, Loss: 0.4881956875324249, Final Batch Loss: 0.29912489652633667\n",
      "Epoch 1669, Loss: 0.3908882439136505, Final Batch Loss: 0.1725146323442459\n",
      "Epoch 1670, Loss: 0.34590812027454376, Final Batch Loss: 0.16902536153793335\n",
      "Epoch 1671, Loss: 0.40349480509757996, Final Batch Loss: 0.1934814453125\n",
      "Epoch 1672, Loss: 0.45192354917526245, Final Batch Loss: 0.2570706009864807\n",
      "Epoch 1673, Loss: 0.4081426113843918, Final Batch Loss: 0.19906695187091827\n",
      "Epoch 1674, Loss: 0.3719406872987747, Final Batch Loss: 0.17808794975280762\n",
      "Epoch 1675, Loss: 0.38645195960998535, Final Batch Loss: 0.18856360018253326\n",
      "Epoch 1676, Loss: 0.3873663395643234, Final Batch Loss: 0.19523760676383972\n",
      "Epoch 1677, Loss: 0.3442434072494507, Final Batch Loss: 0.12582014501094818\n",
      "Epoch 1678, Loss: 0.37082767486572266, Final Batch Loss: 0.17147666215896606\n",
      "Epoch 1679, Loss: 0.4596681445837021, Final Batch Loss: 0.240569069981575\n",
      "Epoch 1680, Loss: 0.4622220993041992, Final Batch Loss: 0.28383374214172363\n",
      "Epoch 1681, Loss: 0.39699286222457886, Final Batch Loss: 0.1668238341808319\n",
      "Epoch 1682, Loss: 0.4269343614578247, Final Batch Loss: 0.22408750653266907\n",
      "Epoch 1683, Loss: 0.4447462111711502, Final Batch Loss: 0.21866503357887268\n",
      "Epoch 1684, Loss: 0.3603854924440384, Final Batch Loss: 0.18159180879592896\n",
      "Epoch 1685, Loss: 0.4165583997964859, Final Batch Loss: 0.21847403049468994\n",
      "Epoch 1686, Loss: 0.38110996782779694, Final Batch Loss: 0.1927245557308197\n",
      "Epoch 1687, Loss: 0.3945768028497696, Final Batch Loss: 0.20409391820430756\n",
      "Epoch 1688, Loss: 0.40200941264629364, Final Batch Loss: 0.20759956538677216\n",
      "Epoch 1689, Loss: 0.424444317817688, Final Batch Loss: 0.21254044771194458\n",
      "Epoch 1690, Loss: 0.4343447834253311, Final Batch Loss: 0.21815720200538635\n",
      "Epoch 1691, Loss: 0.41145719587802887, Final Batch Loss: 0.18948061764240265\n",
      "Epoch 1692, Loss: 0.4198705852031708, Final Batch Loss: 0.25504785776138306\n",
      "Epoch 1693, Loss: 0.30357545614242554, Final Batch Loss: 0.13816745579242706\n",
      "Epoch 1694, Loss: 0.37915490567684174, Final Batch Loss: 0.18242865800857544\n",
      "Epoch 1695, Loss: 0.5066345781087875, Final Batch Loss: 0.2908182740211487\n",
      "Epoch 1696, Loss: 0.35814040154218674, Final Batch Loss: 0.11325963586568832\n",
      "Epoch 1697, Loss: 0.49375012516975403, Final Batch Loss: 0.2753297686576843\n",
      "Epoch 1698, Loss: 0.3650587499141693, Final Batch Loss: 0.17647606134414673\n",
      "Epoch 1699, Loss: 0.39271558821201324, Final Batch Loss: 0.20958741009235382\n",
      "Epoch 1700, Loss: 0.4746340662240982, Final Batch Loss: 0.2575327754020691\n",
      "Epoch 1701, Loss: 0.49816955626010895, Final Batch Loss: 0.29675981402397156\n",
      "Epoch 1702, Loss: 0.4006972163915634, Final Batch Loss: 0.21570898592472076\n",
      "Epoch 1703, Loss: 0.36649535596370697, Final Batch Loss: 0.1840566098690033\n",
      "Epoch 1704, Loss: 0.406660720705986, Final Batch Loss: 0.2034904509782791\n",
      "Epoch 1705, Loss: 0.3423658311367035, Final Batch Loss: 0.14656832814216614\n",
      "Epoch 1706, Loss: 0.41345545649528503, Final Batch Loss: 0.2315939962863922\n",
      "Epoch 1707, Loss: 0.46310552954673767, Final Batch Loss: 0.24186637997627258\n",
      "Epoch 1708, Loss: 0.47131915390491486, Final Batch Loss: 0.21776486933231354\n",
      "Epoch 1709, Loss: 0.35454369336366653, Final Batch Loss: 0.1222207322716713\n",
      "Epoch 1710, Loss: 0.44705067574977875, Final Batch Loss: 0.21007370948791504\n",
      "Epoch 1711, Loss: 0.42943479120731354, Final Batch Loss: 0.17900441586971283\n",
      "Epoch 1712, Loss: 0.396087646484375, Final Batch Loss: 0.2249523252248764\n",
      "Epoch 1713, Loss: 0.35823650658130646, Final Batch Loss: 0.19363822042942047\n",
      "Epoch 1714, Loss: 0.3823583424091339, Final Batch Loss: 0.20975260436534882\n",
      "Epoch 1715, Loss: 0.4115603268146515, Final Batch Loss: 0.2199566811323166\n",
      "Epoch 1716, Loss: 0.4403979927301407, Final Batch Loss: 0.21879056096076965\n",
      "Epoch 1717, Loss: 0.39932674169540405, Final Batch Loss: 0.1812002807855606\n",
      "Epoch 1718, Loss: 0.41861510276794434, Final Batch Loss: 0.2539987564086914\n",
      "Epoch 1719, Loss: 0.41092975437641144, Final Batch Loss: 0.17066602408885956\n",
      "Epoch 1720, Loss: 0.41006381809711456, Final Batch Loss: 0.16300073266029358\n",
      "Epoch 1721, Loss: 0.42083747684955597, Final Batch Loss: 0.21665862202644348\n",
      "Epoch 1722, Loss: 0.5205619782209396, Final Batch Loss: 0.3353373408317566\n",
      "Epoch 1723, Loss: 0.38782477378845215, Final Batch Loss: 0.1984790712594986\n",
      "Epoch 1724, Loss: 0.3937258869409561, Final Batch Loss: 0.19038142263889313\n",
      "Epoch 1725, Loss: 0.401090145111084, Final Batch Loss: 0.19163785874843597\n",
      "Epoch 1726, Loss: 0.3613380044698715, Final Batch Loss: 0.1724933683872223\n",
      "Epoch 1727, Loss: 0.3844023048877716, Final Batch Loss: 0.18669739365577698\n",
      "Epoch 1728, Loss: 0.3913985937833786, Final Batch Loss: 0.1840772032737732\n",
      "Epoch 1729, Loss: 0.44626230001449585, Final Batch Loss: 0.25858554244041443\n",
      "Epoch 1730, Loss: 0.43296679854393005, Final Batch Loss: 0.2705516517162323\n",
      "Epoch 1731, Loss: 0.3920522928237915, Final Batch Loss: 0.16458705067634583\n",
      "Epoch 1732, Loss: 0.39000098407268524, Final Batch Loss: 0.19461911916732788\n",
      "Epoch 1733, Loss: 0.47377555072307587, Final Batch Loss: 0.23909807205200195\n",
      "Epoch 1734, Loss: 0.41923896968364716, Final Batch Loss: 0.20792822539806366\n",
      "Epoch 1735, Loss: 0.4712477773427963, Final Batch Loss: 0.2698080837726593\n",
      "Epoch 1736, Loss: 0.4359130412340164, Final Batch Loss: 0.2139415591955185\n",
      "Epoch 1737, Loss: 0.3695021867752075, Final Batch Loss: 0.1696901172399521\n",
      "Epoch 1738, Loss: 0.35921257734298706, Final Batch Loss: 0.13793912529945374\n",
      "Epoch 1739, Loss: 0.47523848712444305, Final Batch Loss: 0.27774760127067566\n",
      "Epoch 1740, Loss: 0.3966096192598343, Final Batch Loss: 0.20173873007297516\n",
      "Epoch 1741, Loss: 0.46994560956954956, Final Batch Loss: 0.2573695182800293\n",
      "Epoch 1742, Loss: 0.42991138994693756, Final Batch Loss: 0.19907380640506744\n",
      "Epoch 1743, Loss: 0.4317787438631058, Final Batch Loss: 0.2637142539024353\n",
      "Epoch 1744, Loss: 0.4646291881799698, Final Batch Loss: 0.24194200336933136\n",
      "Epoch 1745, Loss: 0.3962762653827667, Final Batch Loss: 0.17661742866039276\n",
      "Epoch 1746, Loss: 0.36022740602493286, Final Batch Loss: 0.1761334389448166\n",
      "Epoch 1747, Loss: 0.38474759459495544, Final Batch Loss: 0.12545007467269897\n",
      "Epoch 1748, Loss: 0.4077150821685791, Final Batch Loss: 0.20200495421886444\n",
      "Epoch 1749, Loss: 0.41723839938640594, Final Batch Loss: 0.21083328127861023\n",
      "Epoch 1750, Loss: 0.4444287419319153, Final Batch Loss: 0.26044222712516785\n",
      "Epoch 1751, Loss: 0.40162138640880585, Final Batch Loss: 0.15305137634277344\n",
      "Epoch 1752, Loss: 0.3548915684223175, Final Batch Loss: 0.15034550428390503\n",
      "Epoch 1753, Loss: 0.41927430033683777, Final Batch Loss: 0.21529705822467804\n",
      "Epoch 1754, Loss: 0.38765938580036163, Final Batch Loss: 0.17191052436828613\n",
      "Epoch 1755, Loss: 0.4453391283750534, Final Batch Loss: 0.21479761600494385\n",
      "Epoch 1756, Loss: 0.3704684376716614, Final Batch Loss: 0.153714120388031\n",
      "Epoch 1757, Loss: 0.4103815406560898, Final Batch Loss: 0.18994967639446259\n",
      "Epoch 1758, Loss: 0.4247220754623413, Final Batch Loss: 0.22253012657165527\n",
      "Epoch 1759, Loss: 0.34899625182151794, Final Batch Loss: 0.14659182727336884\n",
      "Epoch 1760, Loss: 0.4509132504463196, Final Batch Loss: 0.20403459668159485\n",
      "Epoch 1761, Loss: 0.35616590082645416, Final Batch Loss: 0.14724884927272797\n",
      "Epoch 1762, Loss: 0.41615496575832367, Final Batch Loss: 0.21961036324501038\n",
      "Epoch 1763, Loss: 0.3890302926301956, Final Batch Loss: 0.1900223344564438\n",
      "Epoch 1764, Loss: 0.3879307508468628, Final Batch Loss: 0.2057799994945526\n",
      "Epoch 1765, Loss: 0.36563722789287567, Final Batch Loss: 0.1639779806137085\n",
      "Epoch 1766, Loss: 0.4002247601747513, Final Batch Loss: 0.2042156457901001\n",
      "Epoch 1767, Loss: 0.3937274366617203, Final Batch Loss: 0.1998288631439209\n",
      "Epoch 1768, Loss: 0.3970339298248291, Final Batch Loss: 0.18660567700862885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1769, Loss: 0.3941895514726639, Final Batch Loss: 0.194500133395195\n",
      "Epoch 1770, Loss: 0.41094034910202026, Final Batch Loss: 0.2229313850402832\n",
      "Epoch 1771, Loss: 0.37250688672065735, Final Batch Loss: 0.14320699870586395\n",
      "Epoch 1772, Loss: 0.41010600328445435, Final Batch Loss: 0.1914169192314148\n",
      "Epoch 1773, Loss: 0.42128323018550873, Final Batch Loss: 0.2602170407772064\n",
      "Epoch 1774, Loss: 0.4722674489021301, Final Batch Loss: 0.25772470235824585\n",
      "Epoch 1775, Loss: 0.35421109199523926, Final Batch Loss: 0.15664924681186676\n",
      "Epoch 1776, Loss: 0.44261452555656433, Final Batch Loss: 0.22244791686534882\n",
      "Epoch 1777, Loss: 0.42218002676963806, Final Batch Loss: 0.2154126912355423\n",
      "Epoch 1778, Loss: 0.4142683148384094, Final Batch Loss: 0.2231702357530594\n",
      "Epoch 1779, Loss: 0.4099961370229721, Final Batch Loss: 0.22303372621536255\n",
      "Epoch 1780, Loss: 0.37518566846847534, Final Batch Loss: 0.1561640053987503\n",
      "Epoch 1781, Loss: 0.37803125381469727, Final Batch Loss: 0.21155354380607605\n",
      "Epoch 1782, Loss: 0.4636548161506653, Final Batch Loss: 0.2013925313949585\n",
      "Epoch 1783, Loss: 0.3520207703113556, Final Batch Loss: 0.16819962859153748\n",
      "Epoch 1784, Loss: 0.49275849759578705, Final Batch Loss: 0.30603647232055664\n",
      "Epoch 1785, Loss: 0.43335317075252533, Final Batch Loss: 0.2126297652721405\n",
      "Epoch 1786, Loss: 0.4100172221660614, Final Batch Loss: 0.20322805643081665\n",
      "Epoch 1787, Loss: 0.4057069718837738, Final Batch Loss: 0.224713534116745\n",
      "Epoch 1788, Loss: 0.4119468480348587, Final Batch Loss: 0.22977744042873383\n",
      "Epoch 1789, Loss: 0.39774394035339355, Final Batch Loss: 0.19984902441501617\n",
      "Epoch 1790, Loss: 0.4634278267621994, Final Batch Loss: 0.2717738151550293\n",
      "Epoch 1791, Loss: 0.3703121542930603, Final Batch Loss: 0.17293763160705566\n",
      "Epoch 1792, Loss: 0.3868071883916855, Final Batch Loss: 0.18311522901058197\n",
      "Epoch 1793, Loss: 0.39887650310993195, Final Batch Loss: 0.2233339101076126\n",
      "Epoch 1794, Loss: 0.4532703757286072, Final Batch Loss: 0.2704191207885742\n",
      "Epoch 1795, Loss: 0.3806614577770233, Final Batch Loss: 0.15825730562210083\n",
      "Epoch 1796, Loss: 0.3544861078262329, Final Batch Loss: 0.1567365527153015\n",
      "Epoch 1797, Loss: 0.42062240839004517, Final Batch Loss: 0.17965126037597656\n",
      "Epoch 1798, Loss: 0.42304012179374695, Final Batch Loss: 0.1757248193025589\n",
      "Epoch 1799, Loss: 0.4480258524417877, Final Batch Loss: 0.19141432642936707\n",
      "Epoch 1800, Loss: 0.48709316551685333, Final Batch Loss: 0.2614113390445709\n",
      "Epoch 1801, Loss: 0.46660400927066803, Final Batch Loss: 0.2694896161556244\n",
      "Epoch 1802, Loss: 0.37534545361995697, Final Batch Loss: 0.16002121567726135\n",
      "Epoch 1803, Loss: 0.3818589746952057, Final Batch Loss: 0.16940900683403015\n",
      "Epoch 1804, Loss: 0.37944093346595764, Final Batch Loss: 0.18141041696071625\n",
      "Epoch 1805, Loss: 0.35660453140735626, Final Batch Loss: 0.15920691192150116\n",
      "Epoch 1806, Loss: 0.4413764476776123, Final Batch Loss: 0.21078893542289734\n",
      "Epoch 1807, Loss: 0.4145571291446686, Final Batch Loss: 0.23115038871765137\n",
      "Epoch 1808, Loss: 0.37180037796497345, Final Batch Loss: 0.14673148095607758\n",
      "Epoch 1809, Loss: 0.4748687446117401, Final Batch Loss: 0.2781826853752136\n",
      "Epoch 1810, Loss: 0.333265945315361, Final Batch Loss: 0.17596131563186646\n",
      "Epoch 1811, Loss: 0.42034175992012024, Final Batch Loss: 0.22212707996368408\n",
      "Epoch 1812, Loss: 0.37513963878154755, Final Batch Loss: 0.1818341761827469\n",
      "Epoch 1813, Loss: 0.3631667494773865, Final Batch Loss: 0.15068529546260834\n",
      "Epoch 1814, Loss: 0.40408487617969513, Final Batch Loss: 0.20671693980693817\n",
      "Epoch 1815, Loss: 0.4318059831857681, Final Batch Loss: 0.24143098294734955\n",
      "Epoch 1816, Loss: 0.45980748534202576, Final Batch Loss: 0.21277481317520142\n",
      "Epoch 1817, Loss: 0.39325425028800964, Final Batch Loss: 0.13777199387550354\n",
      "Epoch 1818, Loss: 0.36764954030513763, Final Batch Loss: 0.16746194660663605\n",
      "Epoch 1819, Loss: 0.3548831045627594, Final Batch Loss: 0.15261870622634888\n",
      "Epoch 1820, Loss: 0.45757709443569183, Final Batch Loss: 0.23726995289325714\n",
      "Epoch 1821, Loss: 0.4263765513896942, Final Batch Loss: 0.2100045084953308\n",
      "Epoch 1822, Loss: 0.38254596292972565, Final Batch Loss: 0.16087232530117035\n",
      "Epoch 1823, Loss: 0.3897930681705475, Final Batch Loss: 0.19252003729343414\n",
      "Epoch 1824, Loss: 0.34476709365844727, Final Batch Loss: 0.14257915318012238\n",
      "Epoch 1825, Loss: 0.4037373661994934, Final Batch Loss: 0.1800854355096817\n",
      "Epoch 1826, Loss: 0.4399337023496628, Final Batch Loss: 0.23990556597709656\n",
      "Epoch 1827, Loss: 0.3732784390449524, Final Batch Loss: 0.1954360008239746\n",
      "Epoch 1828, Loss: 0.32668574154376984, Final Batch Loss: 0.14623254537582397\n",
      "Epoch 1829, Loss: 0.4034871459007263, Final Batch Loss: 0.19414213299751282\n",
      "Epoch 1830, Loss: 0.475426509976387, Final Batch Loss: 0.2391076683998108\n",
      "Epoch 1831, Loss: 0.37199170887470245, Final Batch Loss: 0.14080855250358582\n",
      "Epoch 1832, Loss: 0.31870102137327194, Final Batch Loss: 0.11383911222219467\n",
      "Epoch 1833, Loss: 0.3867882490158081, Final Batch Loss: 0.19596229493618011\n",
      "Epoch 1834, Loss: 0.37590019404888153, Final Batch Loss: 0.1753208190202713\n",
      "Epoch 1835, Loss: 0.4565766006708145, Final Batch Loss: 0.25383424758911133\n",
      "Epoch 1836, Loss: 0.3679383099079132, Final Batch Loss: 0.14879640936851501\n",
      "Epoch 1837, Loss: 0.3836304098367691, Final Batch Loss: 0.17404498159885406\n",
      "Epoch 1838, Loss: 0.40436311066150665, Final Batch Loss: 0.16586457192897797\n",
      "Epoch 1839, Loss: 0.5073853880167007, Final Batch Loss: 0.3363341987133026\n",
      "Epoch 1840, Loss: 0.38794007897377014, Final Batch Loss: 0.19574105739593506\n",
      "Epoch 1841, Loss: 0.3573824018239975, Final Batch Loss: 0.16872179508209229\n",
      "Epoch 1842, Loss: 0.4422401785850525, Final Batch Loss: 0.20306050777435303\n",
      "Epoch 1843, Loss: 0.39597441256046295, Final Batch Loss: 0.2062615007162094\n",
      "Epoch 1844, Loss: 0.4021991640329361, Final Batch Loss: 0.16890715062618256\n",
      "Epoch 1845, Loss: 0.4473823606967926, Final Batch Loss: 0.2339320331811905\n",
      "Epoch 1846, Loss: 0.42000603675842285, Final Batch Loss: 0.21824981272220612\n",
      "Epoch 1847, Loss: 0.38527557253837585, Final Batch Loss: 0.21505402028560638\n",
      "Epoch 1848, Loss: 0.41124488413333893, Final Batch Loss: 0.19745635986328125\n",
      "Epoch 1849, Loss: 0.378083735704422, Final Batch Loss: 0.1843872368335724\n",
      "Epoch 1850, Loss: 0.3367000222206116, Final Batch Loss: 0.13688580691814423\n",
      "Epoch 1851, Loss: 0.37329716980457306, Final Batch Loss: 0.17746450006961823\n",
      "Epoch 1852, Loss: 0.37062111496925354, Final Batch Loss: 0.1864781379699707\n",
      "Epoch 1853, Loss: 0.419140562415123, Final Batch Loss: 0.1995009481906891\n",
      "Epoch 1854, Loss: 0.37117545306682587, Final Batch Loss: 0.187964528799057\n",
      "Epoch 1855, Loss: 0.37588055431842804, Final Batch Loss: 0.19533087313175201\n",
      "Epoch 1856, Loss: 0.3468674123287201, Final Batch Loss: 0.13890832662582397\n",
      "Epoch 1857, Loss: 0.41149377822875977, Final Batch Loss: 0.21273037791252136\n",
      "Epoch 1858, Loss: 0.39630962908267975, Final Batch Loss: 0.20820432901382446\n",
      "Epoch 1859, Loss: 0.4788566827774048, Final Batch Loss: 0.23872968554496765\n",
      "Epoch 1860, Loss: 0.4336945116519928, Final Batch Loss: 0.22039443254470825\n",
      "Epoch 1861, Loss: 0.36286094784736633, Final Batch Loss: 0.14161637425422668\n",
      "Epoch 1862, Loss: 0.4394043982028961, Final Batch Loss: 0.2386799156665802\n",
      "Epoch 1863, Loss: 0.4255004823207855, Final Batch Loss: 0.17533937096595764\n",
      "Epoch 1864, Loss: 0.3497222810983658, Final Batch Loss: 0.15442664921283722\n",
      "Epoch 1865, Loss: 0.3602302819490433, Final Batch Loss: 0.16699953377246857\n",
      "Epoch 1866, Loss: 0.3125220686197281, Final Batch Loss: 0.16256342828273773\n",
      "Epoch 1867, Loss: 0.403614804148674, Final Batch Loss: 0.20645929872989655\n",
      "Epoch 1868, Loss: 0.381655678153038, Final Batch Loss: 0.2223312109708786\n",
      "Epoch 1869, Loss: 0.38299843668937683, Final Batch Loss: 0.16524988412857056\n",
      "Epoch 1870, Loss: 0.36143799126148224, Final Batch Loss: 0.17559698224067688\n",
      "Epoch 1871, Loss: 0.4518359452486038, Final Batch Loss: 0.1750621646642685\n",
      "Epoch 1872, Loss: 0.4372379779815674, Final Batch Loss: 0.2367619425058365\n",
      "Epoch 1873, Loss: 0.3631560802459717, Final Batch Loss: 0.1894986480474472\n",
      "Epoch 1874, Loss: 0.3620261698961258, Final Batch Loss: 0.19951367378234863\n",
      "Epoch 1875, Loss: 0.4270869940519333, Final Batch Loss: 0.20801080763339996\n",
      "Epoch 1876, Loss: 0.34873034060001373, Final Batch Loss: 0.15042516589164734\n",
      "Epoch 1877, Loss: 0.42040474712848663, Final Batch Loss: 0.2219647318124771\n",
      "Epoch 1878, Loss: 0.42356428503990173, Final Batch Loss: 0.2039458304643631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1879, Loss: 0.36170724034309387, Final Batch Loss: 0.17042851448059082\n",
      "Epoch 1880, Loss: 0.4628332406282425, Final Batch Loss: 0.2599051594734192\n",
      "Epoch 1881, Loss: 0.4123842567205429, Final Batch Loss: 0.23236066102981567\n",
      "Epoch 1882, Loss: 0.36262059211730957, Final Batch Loss: 0.16168281435966492\n",
      "Epoch 1883, Loss: 0.3870280832052231, Final Batch Loss: 0.165652796626091\n",
      "Epoch 1884, Loss: 0.4523593932390213, Final Batch Loss: 0.24871475994586945\n",
      "Epoch 1885, Loss: 0.36484141647815704, Final Batch Loss: 0.17659986019134521\n",
      "Epoch 1886, Loss: 0.3883449137210846, Final Batch Loss: 0.19881831109523773\n",
      "Epoch 1887, Loss: 0.39490413665771484, Final Batch Loss: 0.1667787730693817\n",
      "Epoch 1888, Loss: 0.4005099833011627, Final Batch Loss: 0.16444265842437744\n",
      "Epoch 1889, Loss: 0.4083987921476364, Final Batch Loss: 0.2484574168920517\n",
      "Epoch 1890, Loss: 0.3182462453842163, Final Batch Loss: 0.12826181948184967\n",
      "Epoch 1891, Loss: 0.4229744076728821, Final Batch Loss: 0.23723044991493225\n",
      "Epoch 1892, Loss: 0.41435132920742035, Final Batch Loss: 0.26976722478866577\n",
      "Epoch 1893, Loss: 0.4558416157960892, Final Batch Loss: 0.23524168133735657\n",
      "Epoch 1894, Loss: 0.44519853591918945, Final Batch Loss: 0.20223046839237213\n",
      "Epoch 1895, Loss: 0.3907829523086548, Final Batch Loss: 0.17580397427082062\n",
      "Epoch 1896, Loss: 0.3859669864177704, Final Batch Loss: 0.1915724128484726\n",
      "Epoch 1897, Loss: 0.3746147155761719, Final Batch Loss: 0.17897135019302368\n",
      "Epoch 1898, Loss: 0.3471484035253525, Final Batch Loss: 0.1867222785949707\n",
      "Epoch 1899, Loss: 0.41063758730888367, Final Batch Loss: 0.20441190898418427\n",
      "Epoch 1900, Loss: 0.3438291698694229, Final Batch Loss: 0.17963172495365143\n",
      "Epoch 1901, Loss: 0.37976856529712677, Final Batch Loss: 0.17707383632659912\n",
      "Epoch 1902, Loss: 0.43966685235500336, Final Batch Loss: 0.2387818992137909\n",
      "Epoch 1903, Loss: 0.38346247375011444, Final Batch Loss: 0.1689237356185913\n",
      "Epoch 1904, Loss: 0.3612978756427765, Final Batch Loss: 0.19617705047130585\n",
      "Epoch 1905, Loss: 0.36943401396274567, Final Batch Loss: 0.20272210240364075\n",
      "Epoch 1906, Loss: 0.3899803161621094, Final Batch Loss: 0.1762746274471283\n",
      "Epoch 1907, Loss: 0.4196154326200485, Final Batch Loss: 0.20420056581497192\n",
      "Epoch 1908, Loss: 0.4230268597602844, Final Batch Loss: 0.2309766411781311\n",
      "Epoch 1909, Loss: 0.34882381558418274, Final Batch Loss: 0.18952295184135437\n",
      "Epoch 1910, Loss: 0.327592208981514, Final Batch Loss: 0.13228681683540344\n",
      "Epoch 1911, Loss: 0.3864559233188629, Final Batch Loss: 0.1622898280620575\n",
      "Epoch 1912, Loss: 0.42187653481960297, Final Batch Loss: 0.20838500559329987\n",
      "Epoch 1913, Loss: 0.4088408797979355, Final Batch Loss: 0.2505812346935272\n",
      "Epoch 1914, Loss: 0.42231471836566925, Final Batch Loss: 0.21937841176986694\n",
      "Epoch 1915, Loss: 0.4228758364915848, Final Batch Loss: 0.21193991601467133\n",
      "Epoch 1916, Loss: 0.4521472305059433, Final Batch Loss: 0.16131646931171417\n",
      "Epoch 1917, Loss: 0.38820189237594604, Final Batch Loss: 0.18493744730949402\n",
      "Epoch 1918, Loss: 0.3291967660188675, Final Batch Loss: 0.1542331874370575\n",
      "Epoch 1919, Loss: 0.3666398674249649, Final Batch Loss: 0.1486344039440155\n",
      "Epoch 1920, Loss: 0.40270617604255676, Final Batch Loss: 0.19605115056037903\n",
      "Epoch 1921, Loss: 0.37459519505500793, Final Batch Loss: 0.15875905752182007\n",
      "Epoch 1922, Loss: 0.3400847464799881, Final Batch Loss: 0.15634170174598694\n",
      "Epoch 1923, Loss: 0.41083186864852905, Final Batch Loss: 0.20945128798484802\n",
      "Epoch 1924, Loss: 0.3833353817462921, Final Batch Loss: 0.19124749302864075\n",
      "Epoch 1925, Loss: 0.3906255513429642, Final Batch Loss: 0.1865309178829193\n",
      "Epoch 1926, Loss: 0.4018675535917282, Final Batch Loss: 0.20885907113552094\n",
      "Epoch 1927, Loss: 0.3523227721452713, Final Batch Loss: 0.14965736865997314\n",
      "Epoch 1928, Loss: 0.4553166776895523, Final Batch Loss: 0.2921954393386841\n",
      "Epoch 1929, Loss: 0.3509374409914017, Final Batch Loss: 0.1758425384759903\n",
      "Epoch 1930, Loss: 0.4135618507862091, Final Batch Loss: 0.23739919066429138\n",
      "Epoch 1931, Loss: 0.3875839561223984, Final Batch Loss: 0.17480748891830444\n",
      "Epoch 1932, Loss: 0.3266832083463669, Final Batch Loss: 0.15641815960407257\n",
      "Epoch 1933, Loss: 0.330457404255867, Final Batch Loss: 0.14694592356681824\n",
      "Epoch 1934, Loss: 0.3923863619565964, Final Batch Loss: 0.2005215734243393\n",
      "Epoch 1935, Loss: 0.5071477293968201, Final Batch Loss: 0.34152430295944214\n",
      "Epoch 1936, Loss: 0.4526626616716385, Final Batch Loss: 0.24818769097328186\n",
      "Epoch 1937, Loss: 0.37508510053157806, Final Batch Loss: 0.2209319919347763\n",
      "Epoch 1938, Loss: 0.33281323313713074, Final Batch Loss: 0.14880360662937164\n",
      "Epoch 1939, Loss: 0.37495534121990204, Final Batch Loss: 0.17893902957439423\n",
      "Epoch 1940, Loss: 0.36931149661540985, Final Batch Loss: 0.17949111759662628\n",
      "Epoch 1941, Loss: 0.36850328743457794, Final Batch Loss: 0.16422319412231445\n",
      "Epoch 1942, Loss: 0.41105368733406067, Final Batch Loss: 0.21552979946136475\n",
      "Epoch 1943, Loss: 0.37560102343559265, Final Batch Loss: 0.2055530548095703\n",
      "Epoch 1944, Loss: 0.37388376891613007, Final Batch Loss: 0.18210792541503906\n",
      "Epoch 1945, Loss: 0.39147475361824036, Final Batch Loss: 0.16862092912197113\n",
      "Epoch 1946, Loss: 0.33943189680576324, Final Batch Loss: 0.14622944593429565\n",
      "Epoch 1947, Loss: 0.3927529454231262, Final Batch Loss: 0.17175866663455963\n",
      "Epoch 1948, Loss: 0.42822760343551636, Final Batch Loss: 0.2111671268939972\n",
      "Epoch 1949, Loss: 0.4011520743370056, Final Batch Loss: 0.17147912085056305\n",
      "Epoch 1950, Loss: 0.3726630210876465, Final Batch Loss: 0.16273283958435059\n",
      "Epoch 1951, Loss: 0.34390704333782196, Final Batch Loss: 0.15980467200279236\n",
      "Epoch 1952, Loss: 0.3835775554180145, Final Batch Loss: 0.20357060432434082\n",
      "Epoch 1953, Loss: 0.4813668727874756, Final Batch Loss: 0.28082016110420227\n",
      "Epoch 1954, Loss: 0.3340885490179062, Final Batch Loss: 0.12800233066082\n",
      "Epoch 1955, Loss: 0.4121745675802231, Final Batch Loss: 0.22479234635829926\n",
      "Epoch 1956, Loss: 0.3700459599494934, Final Batch Loss: 0.18596914410591125\n",
      "Epoch 1957, Loss: 0.4124424010515213, Final Batch Loss: 0.22318319976329803\n",
      "Epoch 1958, Loss: 0.37961268424987793, Final Batch Loss: 0.23057271540164948\n",
      "Epoch 1959, Loss: 0.378508985042572, Final Batch Loss: 0.19249793887138367\n",
      "Epoch 1960, Loss: 0.3721749782562256, Final Batch Loss: 0.2020006775856018\n",
      "Epoch 1961, Loss: 0.4105873107910156, Final Batch Loss: 0.21410046517848969\n",
      "Epoch 1962, Loss: 0.38959670066833496, Final Batch Loss: 0.19108812510967255\n",
      "Epoch 1963, Loss: 0.40353113412857056, Final Batch Loss: 0.18893687427043915\n",
      "Epoch 1964, Loss: 0.34954847395420074, Final Batch Loss: 0.18019820749759674\n",
      "Epoch 1965, Loss: 0.38136786222457886, Final Batch Loss: 0.16671355068683624\n",
      "Epoch 1966, Loss: 0.3525761291384697, Final Batch Loss: 0.11828962713479996\n",
      "Epoch 1967, Loss: 0.45029817521572113, Final Batch Loss: 0.2943510115146637\n",
      "Epoch 1968, Loss: 0.3885118365287781, Final Batch Loss: 0.15822529792785645\n",
      "Epoch 1969, Loss: 0.36833372712135315, Final Batch Loss: 0.17506299912929535\n",
      "Epoch 1970, Loss: 0.42804092168807983, Final Batch Loss: 0.2599297761917114\n",
      "Epoch 1971, Loss: 0.3881333917379379, Final Batch Loss: 0.19434885680675507\n",
      "Epoch 1972, Loss: 0.39734214544296265, Final Batch Loss: 0.23746678233146667\n",
      "Epoch 1973, Loss: 0.31180862337350845, Final Batch Loss: 0.12023689597845078\n",
      "Epoch 1974, Loss: 0.3645440340042114, Final Batch Loss: 0.17103788256645203\n",
      "Epoch 1975, Loss: 0.3980303704738617, Final Batch Loss: 0.22115221619606018\n",
      "Epoch 1976, Loss: 0.3766680210828781, Final Batch Loss: 0.17186300456523895\n",
      "Epoch 1977, Loss: 0.38076072931289673, Final Batch Loss: 0.2059837281703949\n",
      "Epoch 1978, Loss: 0.3445376306772232, Final Batch Loss: 0.15219704806804657\n",
      "Epoch 1979, Loss: 0.38722337782382965, Final Batch Loss: 0.19376815855503082\n",
      "Epoch 1980, Loss: 0.34910088777542114, Final Batch Loss: 0.15627440810203552\n",
      "Epoch 1981, Loss: 0.35262830555438995, Final Batch Loss: 0.17988714575767517\n",
      "Epoch 1982, Loss: 0.3077021613717079, Final Batch Loss: 0.12297683209180832\n",
      "Epoch 1983, Loss: 0.35753506422042847, Final Batch Loss: 0.17193414270877838\n",
      "Epoch 1984, Loss: 0.3457731455564499, Final Batch Loss: 0.16279782354831696\n",
      "Epoch 1985, Loss: 0.3164071589708328, Final Batch Loss: 0.13548371195793152\n",
      "Epoch 1986, Loss: 0.407094344496727, Final Batch Loss: 0.21949197351932526\n",
      "Epoch 1987, Loss: 0.46782954037189484, Final Batch Loss: 0.29061561822891235\n",
      "Epoch 1988, Loss: 0.3410152345895767, Final Batch Loss: 0.1535586714744568\n",
      "Epoch 1989, Loss: 0.3575561195611954, Final Batch Loss: 0.14037345349788666\n",
      "Epoch 1990, Loss: 0.4156273752450943, Final Batch Loss: 0.20366837084293365\n",
      "Epoch 1991, Loss: 0.392225444316864, Final Batch Loss: 0.23249013721942902\n",
      "Epoch 1992, Loss: 0.37568895518779755, Final Batch Loss: 0.20516365766525269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1993, Loss: 0.3753764033317566, Final Batch Loss: 0.17823591828346252\n",
      "Epoch 1994, Loss: 0.3542744517326355, Final Batch Loss: 0.16864466667175293\n",
      "Epoch 1995, Loss: 0.32741018384695053, Final Batch Loss: 0.11310557276010513\n",
      "Epoch 1996, Loss: 0.4229334741830826, Final Batch Loss: 0.24785088002681732\n",
      "Epoch 1997, Loss: 0.3722245693206787, Final Batch Loss: 0.18537193536758423\n",
      "Epoch 1998, Loss: 0.4338116943836212, Final Batch Loss: 0.23498396575450897\n",
      "Epoch 1999, Loss: 0.40657763183116913, Final Batch Loss: 0.2399573028087616\n",
      "Epoch 2000, Loss: 0.3454745262861252, Final Batch Loss: 0.12716364860534668\n",
      "Epoch 2001, Loss: 0.35515494644641876, Final Batch Loss: 0.1778952032327652\n",
      "Epoch 2002, Loss: 0.3758518546819687, Final Batch Loss: 0.14470414817333221\n",
      "Epoch 2003, Loss: 0.4311057776212692, Final Batch Loss: 0.2778017222881317\n",
      "Epoch 2004, Loss: 0.3730213791131973, Final Batch Loss: 0.13895195722579956\n",
      "Epoch 2005, Loss: 0.328996941447258, Final Batch Loss: 0.14027650654315948\n",
      "Epoch 2006, Loss: 0.366771399974823, Final Batch Loss: 0.15155893564224243\n",
      "Epoch 2007, Loss: 0.34902696311473846, Final Batch Loss: 0.16869200766086578\n",
      "Epoch 2008, Loss: 0.37583789229393005, Final Batch Loss: 0.18810109794139862\n",
      "Epoch 2009, Loss: 0.37160059809684753, Final Batch Loss: 0.1590539664030075\n",
      "Epoch 2010, Loss: 0.3536660820245743, Final Batch Loss: 0.17138682305812836\n",
      "Epoch 2011, Loss: 0.3967588543891907, Final Batch Loss: 0.19561955332756042\n",
      "Epoch 2012, Loss: 0.3887326270341873, Final Batch Loss: 0.13822828233242035\n",
      "Epoch 2013, Loss: 0.45802219212055206, Final Batch Loss: 0.2832206189632416\n",
      "Epoch 2014, Loss: 0.3944162428379059, Final Batch Loss: 0.22368298470973969\n",
      "Epoch 2015, Loss: 0.41619718074798584, Final Batch Loss: 0.22936365008354187\n",
      "Epoch 2016, Loss: 0.3532104939222336, Final Batch Loss: 0.19005081057548523\n",
      "Epoch 2017, Loss: 0.3955470323562622, Final Batch Loss: 0.19563527405261993\n",
      "Epoch 2018, Loss: 0.4006405472755432, Final Batch Loss: 0.21684066951274872\n",
      "Epoch 2019, Loss: 0.36490118503570557, Final Batch Loss: 0.17110776901245117\n",
      "Epoch 2020, Loss: 0.3965577632188797, Final Batch Loss: 0.23099073767662048\n",
      "Epoch 2021, Loss: 0.46375346183776855, Final Batch Loss: 0.2609872817993164\n",
      "Epoch 2022, Loss: 0.37552741169929504, Final Batch Loss: 0.15814849734306335\n",
      "Epoch 2023, Loss: 0.3777890205383301, Final Batch Loss: 0.18102417886257172\n",
      "Epoch 2024, Loss: 0.384888619184494, Final Batch Loss: 0.16837872564792633\n",
      "Epoch 2025, Loss: 0.4043615907430649, Final Batch Loss: 0.19940431416034698\n",
      "Epoch 2026, Loss: 0.38441774249076843, Final Batch Loss: 0.22963190078735352\n",
      "Epoch 2027, Loss: 0.30220745503902435, Final Batch Loss: 0.1281653344631195\n",
      "Epoch 2028, Loss: 0.3835718035697937, Final Batch Loss: 0.19838815927505493\n",
      "Epoch 2029, Loss: 0.4158572256565094, Final Batch Loss: 0.24934792518615723\n",
      "Epoch 2030, Loss: 0.4238019585609436, Final Batch Loss: 0.21734938025474548\n",
      "Epoch 2031, Loss: 0.31985072791576385, Final Batch Loss: 0.13792751729488373\n",
      "Epoch 2032, Loss: 0.3810128718614578, Final Batch Loss: 0.21545356512069702\n",
      "Epoch 2033, Loss: 0.4008127599954605, Final Batch Loss: 0.23984068632125854\n",
      "Epoch 2034, Loss: 0.37116244435310364, Final Batch Loss: 0.1567007303237915\n",
      "Epoch 2035, Loss: 0.4070437401533127, Final Batch Loss: 0.22578668594360352\n",
      "Epoch 2036, Loss: 0.3433966338634491, Final Batch Loss: 0.19713208079338074\n",
      "Epoch 2037, Loss: 0.3850947916507721, Final Batch Loss: 0.17850175499916077\n",
      "Epoch 2038, Loss: 0.33964911103248596, Final Batch Loss: 0.17093047499656677\n",
      "Epoch 2039, Loss: 0.40423914790153503, Final Batch Loss: 0.2227713167667389\n",
      "Epoch 2040, Loss: 0.39450520277023315, Final Batch Loss: 0.21406927704811096\n",
      "Epoch 2041, Loss: 0.3722454607486725, Final Batch Loss: 0.1815982311964035\n",
      "Epoch 2042, Loss: 0.3026924133300781, Final Batch Loss: 0.15293116867542267\n",
      "Epoch 2043, Loss: 0.42747892439365387, Final Batch Loss: 0.258844792842865\n",
      "Epoch 2044, Loss: 0.3985110968351364, Final Batch Loss: 0.2014869749546051\n",
      "Epoch 2045, Loss: 0.3244789242744446, Final Batch Loss: 0.17925569415092468\n",
      "Epoch 2046, Loss: 0.4696442037820816, Final Batch Loss: 0.23448216915130615\n",
      "Epoch 2047, Loss: 0.37404751777648926, Final Batch Loss: 0.2017916738986969\n",
      "Epoch 2048, Loss: 0.3674275130033493, Final Batch Loss: 0.16784004867076874\n",
      "Epoch 2049, Loss: 0.4560586214065552, Final Batch Loss: 0.2425520271062851\n",
      "Epoch 2050, Loss: 0.3535793572664261, Final Batch Loss: 0.16265495121479034\n",
      "Epoch 2051, Loss: 0.37551435828208923, Final Batch Loss: 0.19187617301940918\n",
      "Epoch 2052, Loss: 0.47208496928215027, Final Batch Loss: 0.2530454099178314\n",
      "Epoch 2053, Loss: 0.37007881700992584, Final Batch Loss: 0.20201729238033295\n",
      "Epoch 2054, Loss: 0.40024979412555695, Final Batch Loss: 0.21792328357696533\n",
      "Epoch 2055, Loss: 0.37888413667678833, Final Batch Loss: 0.18560877442359924\n",
      "Epoch 2056, Loss: 0.39501215517520905, Final Batch Loss: 0.15889404714107513\n",
      "Epoch 2057, Loss: 0.3540365546941757, Final Batch Loss: 0.18714730441570282\n",
      "Epoch 2058, Loss: 0.33862559497356415, Final Batch Loss: 0.13135495781898499\n",
      "Epoch 2059, Loss: 0.36842676997184753, Final Batch Loss: 0.16540955007076263\n",
      "Epoch 2060, Loss: 0.3809729814529419, Final Batch Loss: 0.16464585065841675\n",
      "Epoch 2061, Loss: 0.4154447615146637, Final Batch Loss: 0.21048426628112793\n",
      "Epoch 2062, Loss: 0.3533295840024948, Final Batch Loss: 0.13115838170051575\n",
      "Epoch 2063, Loss: 0.3522537499666214, Final Batch Loss: 0.15834374725818634\n",
      "Epoch 2064, Loss: 0.32985658943653107, Final Batch Loss: 0.15432946383953094\n",
      "Epoch 2065, Loss: 0.3922400325536728, Final Batch Loss: 0.22890456020832062\n",
      "Epoch 2066, Loss: 0.38062043488025665, Final Batch Loss: 0.16828447580337524\n",
      "Epoch 2067, Loss: 0.35308119654655457, Final Batch Loss: 0.1845274120569229\n",
      "Epoch 2068, Loss: 0.3276936709880829, Final Batch Loss: 0.17198435962200165\n",
      "Epoch 2069, Loss: 0.3813667446374893, Final Batch Loss: 0.18208517134189606\n",
      "Epoch 2070, Loss: 0.4053071588277817, Final Batch Loss: 0.23258233070373535\n",
      "Epoch 2071, Loss: 0.3907340466976166, Final Batch Loss: 0.19487418234348297\n",
      "Epoch 2072, Loss: 0.38189971446990967, Final Batch Loss: 0.1971687376499176\n",
      "Epoch 2073, Loss: 0.41619063913822174, Final Batch Loss: 0.21539735794067383\n",
      "Epoch 2074, Loss: 0.3983467221260071, Final Batch Loss: 0.1937715709209442\n",
      "Epoch 2075, Loss: 0.37080566585063934, Final Batch Loss: 0.2082754224538803\n",
      "Epoch 2076, Loss: 0.4089280664920807, Final Batch Loss: 0.21952539682388306\n",
      "Epoch 2077, Loss: 0.311438612639904, Final Batch Loss: 0.11670363694429398\n",
      "Epoch 2078, Loss: 0.3644573241472244, Final Batch Loss: 0.174521803855896\n",
      "Epoch 2079, Loss: 0.42390742897987366, Final Batch Loss: 0.22932004928588867\n",
      "Epoch 2080, Loss: 0.38924284279346466, Final Batch Loss: 0.19467677175998688\n",
      "Epoch 2081, Loss: 0.35747000575065613, Final Batch Loss: 0.18819211423397064\n",
      "Epoch 2082, Loss: 0.33167019486427307, Final Batch Loss: 0.15520118176937103\n",
      "Epoch 2083, Loss: 0.36454808712005615, Final Batch Loss: 0.1913917362689972\n",
      "Epoch 2084, Loss: 0.3208881914615631, Final Batch Loss: 0.1384076178073883\n",
      "Epoch 2085, Loss: 0.4276583790779114, Final Batch Loss: 0.2712307572364807\n",
      "Epoch 2086, Loss: 0.33931903541088104, Final Batch Loss: 0.1750354915857315\n",
      "Epoch 2087, Loss: 0.4067743569612503, Final Batch Loss: 0.23667843639850616\n",
      "Epoch 2088, Loss: 0.44737161695957184, Final Batch Loss: 0.23102927207946777\n",
      "Epoch 2089, Loss: 0.36591625213623047, Final Batch Loss: 0.2003277689218521\n",
      "Epoch 2090, Loss: 0.38479022681713104, Final Batch Loss: 0.18058884143829346\n",
      "Epoch 2091, Loss: 0.3579223155975342, Final Batch Loss: 0.1578158438205719\n",
      "Epoch 2092, Loss: 0.41625258326530457, Final Batch Loss: 0.23853543400764465\n",
      "Epoch 2093, Loss: 0.36054082214832306, Final Batch Loss: 0.16048860549926758\n",
      "Epoch 2094, Loss: 0.3680146485567093, Final Batch Loss: 0.2001868188381195\n",
      "Epoch 2095, Loss: 0.3009665757417679, Final Batch Loss: 0.10776194930076599\n",
      "Epoch 2096, Loss: 0.44857801496982574, Final Batch Loss: 0.20153670012950897\n",
      "Epoch 2097, Loss: 0.34896229207515717, Final Batch Loss: 0.14419645071029663\n",
      "Epoch 2098, Loss: 0.36229729652404785, Final Batch Loss: 0.22024331986904144\n",
      "Epoch 2099, Loss: 0.3392314910888672, Final Batch Loss: 0.17023323476314545\n",
      "Epoch 2100, Loss: 0.3881250470876694, Final Batch Loss: 0.18310295045375824\n",
      "Epoch 2101, Loss: 0.2844284325838089, Final Batch Loss: 0.11875304579734802\n",
      "Epoch 2102, Loss: 0.3332938700914383, Final Batch Loss: 0.17392651736736298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2103, Loss: 0.36284807324409485, Final Batch Loss: 0.16329962015151978\n",
      "Epoch 2104, Loss: 0.38781705498695374, Final Batch Loss: 0.22631387412548065\n",
      "Epoch 2105, Loss: 0.35811538994312286, Final Batch Loss: 0.19010764360427856\n",
      "Epoch 2106, Loss: 0.46668246388435364, Final Batch Loss: 0.3052157461643219\n",
      "Epoch 2107, Loss: 0.33565160632133484, Final Batch Loss: 0.17439928650856018\n",
      "Epoch 2108, Loss: 0.30424104630947113, Final Batch Loss: 0.1424240618944168\n",
      "Epoch 2109, Loss: 0.33353424072265625, Final Batch Loss: 0.1472897231578827\n",
      "Epoch 2110, Loss: 0.4722091108560562, Final Batch Loss: 0.26241540908813477\n",
      "Epoch 2111, Loss: 0.3561304658651352, Final Batch Loss: 0.18629679083824158\n",
      "Epoch 2112, Loss: 0.35861344635486603, Final Batch Loss: 0.18478447198867798\n",
      "Epoch 2113, Loss: 0.353032723069191, Final Batch Loss: 0.1752520650625229\n",
      "Epoch 2114, Loss: 0.3652133196592331, Final Batch Loss: 0.17610524594783783\n",
      "Epoch 2115, Loss: 0.3153913617134094, Final Batch Loss: 0.1503472626209259\n",
      "Epoch 2116, Loss: 0.35191740095615387, Final Batch Loss: 0.17280222475528717\n",
      "Epoch 2117, Loss: 0.35932132601737976, Final Batch Loss: 0.14924733340740204\n",
      "Epoch 2118, Loss: 0.4183290898799896, Final Batch Loss: 0.2616747319698334\n",
      "Epoch 2119, Loss: 0.3682628720998764, Final Batch Loss: 0.19639378786087036\n",
      "Epoch 2120, Loss: 0.39103974401950836, Final Batch Loss: 0.21489565074443817\n",
      "Epoch 2121, Loss: 0.3740938901901245, Final Batch Loss: 0.206667959690094\n",
      "Epoch 2122, Loss: 0.4378773719072342, Final Batch Loss: 0.25714990496635437\n",
      "Epoch 2123, Loss: 0.30404311418533325, Final Batch Loss: 0.15710394084453583\n",
      "Epoch 2124, Loss: 0.47423627972602844, Final Batch Loss: 0.3172733783721924\n",
      "Epoch 2125, Loss: 0.4610934257507324, Final Batch Loss: 0.2545046806335449\n",
      "Epoch 2126, Loss: 0.3758445978164673, Final Batch Loss: 0.16092079877853394\n",
      "Epoch 2127, Loss: 0.3988359123468399, Final Batch Loss: 0.17401833832263947\n",
      "Epoch 2128, Loss: 0.3315671980381012, Final Batch Loss: 0.16208688914775848\n",
      "Epoch 2129, Loss: 0.32818029820919037, Final Batch Loss: 0.1493423879146576\n",
      "Epoch 2130, Loss: 0.36519548296928406, Final Batch Loss: 0.17852148413658142\n",
      "Epoch 2131, Loss: 0.4750097692012787, Final Batch Loss: 0.2304404228925705\n",
      "Epoch 2132, Loss: 0.34356093406677246, Final Batch Loss: 0.14675703644752502\n",
      "Epoch 2133, Loss: 0.35714517533779144, Final Batch Loss: 0.15737822651863098\n",
      "Epoch 2134, Loss: 0.3879747837781906, Final Batch Loss: 0.1724226474761963\n",
      "Epoch 2135, Loss: 0.31420281529426575, Final Batch Loss: 0.15924954414367676\n",
      "Epoch 2136, Loss: 0.4503464847803116, Final Batch Loss: 0.24617193639278412\n",
      "Epoch 2137, Loss: 0.38607659935951233, Final Batch Loss: 0.20295386016368866\n",
      "Epoch 2138, Loss: 0.43793943524360657, Final Batch Loss: 0.24342545866966248\n",
      "Epoch 2139, Loss: 0.3888678550720215, Final Batch Loss: 0.19372650980949402\n",
      "Epoch 2140, Loss: 0.37058645486831665, Final Batch Loss: 0.18191350996494293\n",
      "Epoch 2141, Loss: 0.35901501774787903, Final Batch Loss: 0.18025542795658112\n",
      "Epoch 2142, Loss: 0.3081085830926895, Final Batch Loss: 0.1407453715801239\n",
      "Epoch 2143, Loss: 0.3777807205915451, Final Batch Loss: 0.17728906869888306\n",
      "Epoch 2144, Loss: 0.36552469432353973, Final Batch Loss: 0.1798742562532425\n",
      "Epoch 2145, Loss: 0.38062135875225067, Final Batch Loss: 0.19226834177970886\n",
      "Epoch 2146, Loss: 0.3001378923654556, Final Batch Loss: 0.12214608490467072\n",
      "Epoch 2147, Loss: 0.32850562036037445, Final Batch Loss: 0.15172237157821655\n",
      "Epoch 2148, Loss: 0.39499227702617645, Final Batch Loss: 0.25071054697036743\n",
      "Epoch 2149, Loss: 0.33391720056533813, Final Batch Loss: 0.16576449573040009\n",
      "Epoch 2150, Loss: 0.36491626501083374, Final Batch Loss: 0.19481968879699707\n",
      "Epoch 2151, Loss: 0.43495579063892365, Final Batch Loss: 0.2622095048427582\n",
      "Epoch 2152, Loss: 0.3654938340187073, Final Batch Loss: 0.21295587718486786\n",
      "Epoch 2153, Loss: 0.3963967561721802, Final Batch Loss: 0.2281818687915802\n",
      "Epoch 2154, Loss: 0.3646702468395233, Final Batch Loss: 0.1752580851316452\n",
      "Epoch 2155, Loss: 0.39516037702560425, Final Batch Loss: 0.22115975618362427\n",
      "Epoch 2156, Loss: 0.37565095722675323, Final Batch Loss: 0.21075540781021118\n",
      "Epoch 2157, Loss: 0.3589031994342804, Final Batch Loss: 0.16980956494808197\n",
      "Epoch 2158, Loss: 0.3026202917098999, Final Batch Loss: 0.13052500784397125\n",
      "Epoch 2159, Loss: 0.3482232391834259, Final Batch Loss: 0.1799442172050476\n",
      "Epoch 2160, Loss: 0.39292682707309723, Final Batch Loss: 0.1983330398797989\n",
      "Epoch 2161, Loss: 0.3646615296602249, Final Batch Loss: 0.15597717463970184\n",
      "Epoch 2162, Loss: 0.41171546280384064, Final Batch Loss: 0.24389208853244781\n",
      "Epoch 2163, Loss: 0.43918803334236145, Final Batch Loss: 0.2081189900636673\n",
      "Epoch 2164, Loss: 0.425015851855278, Final Batch Loss: 0.21753181517124176\n",
      "Epoch 2165, Loss: 0.40057098865509033, Final Batch Loss: 0.20895639061927795\n",
      "Epoch 2166, Loss: 0.39721666276454926, Final Batch Loss: 0.2028350830078125\n",
      "Epoch 2167, Loss: 0.4184061586856842, Final Batch Loss: 0.23456895351409912\n",
      "Epoch 2168, Loss: 0.355842262506485, Final Batch Loss: 0.19902750849723816\n",
      "Epoch 2169, Loss: 0.3451342284679413, Final Batch Loss: 0.12142102420330048\n",
      "Epoch 2170, Loss: 0.36570771038532257, Final Batch Loss: 0.1808188408613205\n",
      "Epoch 2171, Loss: 0.34449502825737, Final Batch Loss: 0.13983072340488434\n",
      "Epoch 2172, Loss: 0.4031820446252823, Final Batch Loss: 0.17506617307662964\n",
      "Epoch 2173, Loss: 0.3071897253394127, Final Batch Loss: 0.1220250204205513\n",
      "Epoch 2174, Loss: 0.3946327716112137, Final Batch Loss: 0.19542540609836578\n",
      "Epoch 2175, Loss: 0.3152095377445221, Final Batch Loss: 0.17289991676807404\n",
      "Epoch 2176, Loss: 0.31628985702991486, Final Batch Loss: 0.1411157250404358\n",
      "Epoch 2177, Loss: 0.39683206379413605, Final Batch Loss: 0.18197068572044373\n",
      "Epoch 2178, Loss: 0.2796318531036377, Final Batch Loss: 0.12151630222797394\n",
      "Epoch 2179, Loss: 0.3702552020549774, Final Batch Loss: 0.19189505279064178\n",
      "Epoch 2180, Loss: 0.39459680020809174, Final Batch Loss: 0.20638802647590637\n",
      "Epoch 2181, Loss: 0.3662557601928711, Final Batch Loss: 0.16600960493087769\n",
      "Epoch 2182, Loss: 0.4252210259437561, Final Batch Loss: 0.27561742067337036\n",
      "Epoch 2183, Loss: 0.38500291109085083, Final Batch Loss: 0.15149694681167603\n",
      "Epoch 2184, Loss: 0.3849032521247864, Final Batch Loss: 0.17615288496017456\n",
      "Epoch 2185, Loss: 0.3251851946115494, Final Batch Loss: 0.12916091084480286\n",
      "Epoch 2186, Loss: 0.36607398092746735, Final Batch Loss: 0.17835736274719238\n",
      "Epoch 2187, Loss: 0.38458846509456635, Final Batch Loss: 0.21670153737068176\n",
      "Epoch 2188, Loss: 0.3507945388555527, Final Batch Loss: 0.17399021983146667\n",
      "Epoch 2189, Loss: 0.35276976227760315, Final Batch Loss: 0.1364021599292755\n",
      "Epoch 2190, Loss: 0.3969014883041382, Final Batch Loss: 0.22269518673419952\n",
      "Epoch 2191, Loss: 0.3369782269001007, Final Batch Loss: 0.15897847712039948\n",
      "Epoch 2192, Loss: 0.39171718060970306, Final Batch Loss: 0.22457227110862732\n",
      "Epoch 2193, Loss: 0.3367747962474823, Final Batch Loss: 0.17906823754310608\n",
      "Epoch 2194, Loss: 0.34502013027668, Final Batch Loss: 0.2042347639799118\n",
      "Epoch 2195, Loss: 0.3368254601955414, Final Batch Loss: 0.1694898158311844\n",
      "Epoch 2196, Loss: 0.40362638235092163, Final Batch Loss: 0.19219575822353363\n",
      "Epoch 2197, Loss: 0.3515634536743164, Final Batch Loss: 0.16914640367031097\n",
      "Epoch 2198, Loss: 0.38056954741477966, Final Batch Loss: 0.23100563883781433\n",
      "Epoch 2199, Loss: 0.36779241263866425, Final Batch Loss: 0.18161307275295258\n",
      "Epoch 2200, Loss: 0.4297605901956558, Final Batch Loss: 0.2841392159461975\n",
      "Epoch 2201, Loss: 0.33397163450717926, Final Batch Loss: 0.16756007075309753\n",
      "Epoch 2202, Loss: 0.3645000159740448, Final Batch Loss: 0.1401991993188858\n",
      "Epoch 2203, Loss: 0.31695323437452316, Final Batch Loss: 0.10635379701852798\n",
      "Epoch 2204, Loss: 0.36581820249557495, Final Batch Loss: 0.20344281196594238\n",
      "Epoch 2205, Loss: 0.3800404667854309, Final Batch Loss: 0.19365473091602325\n",
      "Epoch 2206, Loss: 0.39167793095111847, Final Batch Loss: 0.1763107031583786\n",
      "Epoch 2207, Loss: 0.3225495368242264, Final Batch Loss: 0.1538485884666443\n",
      "Epoch 2208, Loss: 0.3861229866743088, Final Batch Loss: 0.1963554471731186\n",
      "Epoch 2209, Loss: 0.39341050386428833, Final Batch Loss: 0.21572378277778625\n",
      "Epoch 2210, Loss: 0.30564214289188385, Final Batch Loss: 0.1551399677991867\n",
      "Epoch 2211, Loss: 0.39486898481845856, Final Batch Loss: 0.19349989295005798\n",
      "Epoch 2212, Loss: 0.37565676867961884, Final Batch Loss: 0.19639529287815094\n",
      "Epoch 2213, Loss: 0.35301174223423004, Final Batch Loss: 0.16073904931545258\n",
      "Epoch 2214, Loss: 0.3217821717262268, Final Batch Loss: 0.16136769950389862\n",
      "Epoch 2215, Loss: 0.420760840177536, Final Batch Loss: 0.2810216546058655\n",
      "Epoch 2216, Loss: 0.4043562114238739, Final Batch Loss: 0.1685042530298233\n",
      "Epoch 2217, Loss: 0.37997324764728546, Final Batch Loss: 0.20664575695991516\n",
      "Epoch 2218, Loss: 0.3639913648366928, Final Batch Loss: 0.18089322745800018\n",
      "Epoch 2219, Loss: 0.3469383865594864, Final Batch Loss: 0.13995219767093658\n",
      "Epoch 2220, Loss: 0.38727977871894836, Final Batch Loss: 0.21340078115463257\n",
      "Epoch 2221, Loss: 0.4248366206884384, Final Batch Loss: 0.2162393182516098\n",
      "Epoch 2222, Loss: 0.43207505345344543, Final Batch Loss: 0.23489664494991302\n",
      "Epoch 2223, Loss: 0.3679351508617401, Final Batch Loss: 0.1963890641927719\n",
      "Epoch 2224, Loss: 0.36817967891693115, Final Batch Loss: 0.16595818102359772\n",
      "Epoch 2225, Loss: 0.388052761554718, Final Batch Loss: 0.15039461851119995\n",
      "Epoch 2226, Loss: 0.3600085973739624, Final Batch Loss: 0.1937202364206314\n",
      "Epoch 2227, Loss: 0.3852444738149643, Final Batch Loss: 0.2138250768184662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2228, Loss: 0.4086034595966339, Final Batch Loss: 0.2078360617160797\n",
      "Epoch 2229, Loss: 0.3537014424800873, Final Batch Loss: 0.14964237809181213\n",
      "Epoch 2230, Loss: 0.35375744104385376, Final Batch Loss: 0.19255538284778595\n",
      "Epoch 2231, Loss: 0.3601302057504654, Final Batch Loss: 0.19785065948963165\n",
      "Epoch 2232, Loss: 0.41722801327705383, Final Batch Loss: 0.2230195850133896\n",
      "Epoch 2233, Loss: 0.3153100907802582, Final Batch Loss: 0.11222746968269348\n",
      "Epoch 2234, Loss: 0.35160477459430695, Final Batch Loss: 0.15405398607254028\n",
      "Epoch 2235, Loss: 0.357235848903656, Final Batch Loss: 0.1574626863002777\n",
      "Epoch 2236, Loss: 0.3816307783126831, Final Batch Loss: 0.16435889899730682\n",
      "Epoch 2237, Loss: 0.3579167425632477, Final Batch Loss: 0.17886927723884583\n",
      "Epoch 2238, Loss: 0.3775162994861603, Final Batch Loss: 0.21880963444709778\n",
      "Epoch 2239, Loss: 0.28353944420814514, Final Batch Loss: 0.10846477746963501\n",
      "Epoch 2240, Loss: 0.35522398352622986, Final Batch Loss: 0.19169721007347107\n",
      "Epoch 2241, Loss: 0.3527418226003647, Final Batch Loss: 0.19213402271270752\n",
      "Epoch 2242, Loss: 0.35329553484916687, Final Batch Loss: 0.16801466047763824\n",
      "Epoch 2243, Loss: 0.3740837574005127, Final Batch Loss: 0.18447661399841309\n",
      "Epoch 2244, Loss: 0.3643478602170944, Final Batch Loss: 0.1831490844488144\n",
      "Epoch 2245, Loss: 0.32086125016212463, Final Batch Loss: 0.14700180292129517\n",
      "Epoch 2246, Loss: 0.2941884994506836, Final Batch Loss: 0.1339845061302185\n",
      "Epoch 2247, Loss: 0.367284432053566, Final Batch Loss: 0.20833468437194824\n",
      "Epoch 2248, Loss: 0.322684183716774, Final Batch Loss: 0.09885837137699127\n",
      "Epoch 2249, Loss: 0.4417252540588379, Final Batch Loss: 0.28424322605133057\n",
      "Epoch 2250, Loss: 0.3133220672607422, Final Batch Loss: 0.14821356534957886\n",
      "Epoch 2251, Loss: 0.2987772673368454, Final Batch Loss: 0.13589860498905182\n",
      "Epoch 2252, Loss: 0.3546537607908249, Final Batch Loss: 0.1568482667207718\n",
      "Epoch 2253, Loss: 0.3512587398290634, Final Batch Loss: 0.15204547345638275\n",
      "Epoch 2254, Loss: 0.28795788437128067, Final Batch Loss: 0.11773259192705154\n",
      "Epoch 2255, Loss: 0.3936796933412552, Final Batch Loss: 0.17266234755516052\n",
      "Epoch 2256, Loss: 0.3766852915287018, Final Batch Loss: 0.21809904277324677\n",
      "Epoch 2257, Loss: 0.3600650727748871, Final Batch Loss: 0.2052738070487976\n",
      "Epoch 2258, Loss: 0.3919016122817993, Final Batch Loss: 0.1954851597547531\n",
      "Epoch 2259, Loss: 0.45117950439453125, Final Batch Loss: 0.24863100051879883\n",
      "Epoch 2260, Loss: 0.3868100792169571, Final Batch Loss: 0.13815195858478546\n",
      "Epoch 2261, Loss: 0.4080708175897598, Final Batch Loss: 0.20028138160705566\n",
      "Epoch 2262, Loss: 0.3422991633415222, Final Batch Loss: 0.14881585538387299\n",
      "Epoch 2263, Loss: 0.4717109501361847, Final Batch Loss: 0.24854470789432526\n",
      "Epoch 2264, Loss: 0.40575993061065674, Final Batch Loss: 0.2332363873720169\n",
      "Epoch 2265, Loss: 0.3116406947374344, Final Batch Loss: 0.14450442790985107\n",
      "Epoch 2266, Loss: 0.37715210020542145, Final Batch Loss: 0.20293396711349487\n",
      "Epoch 2267, Loss: 0.3385492414236069, Final Batch Loss: 0.18930532038211823\n",
      "Epoch 2268, Loss: 0.3787202090024948, Final Batch Loss: 0.1824113428592682\n",
      "Epoch 2269, Loss: 0.462335467338562, Final Batch Loss: 0.24383115768432617\n",
      "Epoch 2270, Loss: 0.3225547522306442, Final Batch Loss: 0.137427419424057\n",
      "Epoch 2271, Loss: 0.331815630197525, Final Batch Loss: 0.1982508897781372\n",
      "Epoch 2272, Loss: 0.34852781891822815, Final Batch Loss: 0.18118916451931\n",
      "Epoch 2273, Loss: 0.46110111474990845, Final Batch Loss: 0.24234449863433838\n",
      "Epoch 2274, Loss: 0.3554317206144333, Final Batch Loss: 0.1663544476032257\n",
      "Epoch 2275, Loss: 0.39827683568000793, Final Batch Loss: 0.22227630019187927\n",
      "Epoch 2276, Loss: 0.35634034872055054, Final Batch Loss: 0.20678627490997314\n",
      "Epoch 2277, Loss: 0.3426409214735031, Final Batch Loss: 0.16961556673049927\n",
      "Epoch 2278, Loss: 0.345373198390007, Final Batch Loss: 0.14550304412841797\n",
      "Epoch 2279, Loss: 0.3659404367208481, Final Batch Loss: 0.19199112057685852\n",
      "Epoch 2280, Loss: 0.35064244270324707, Final Batch Loss: 0.16014321148395538\n",
      "Epoch 2281, Loss: 0.32241976261138916, Final Batch Loss: 0.15750986337661743\n",
      "Epoch 2282, Loss: 0.2790082097053528, Final Batch Loss: 0.12272073328495026\n",
      "Epoch 2283, Loss: 0.3798501193523407, Final Batch Loss: 0.15027277171611786\n",
      "Epoch 2284, Loss: 0.32091639935970306, Final Batch Loss: 0.14487449824810028\n",
      "Epoch 2285, Loss: 0.3622884452342987, Final Batch Loss: 0.21645495295524597\n",
      "Epoch 2286, Loss: 0.3381129652261734, Final Batch Loss: 0.1512315273284912\n",
      "Epoch 2287, Loss: 0.33237311244010925, Final Batch Loss: 0.17627817392349243\n",
      "Epoch 2288, Loss: 0.3736511766910553, Final Batch Loss: 0.22951668500900269\n",
      "Epoch 2289, Loss: 0.3187403380870819, Final Batch Loss: 0.1490837186574936\n",
      "Epoch 2290, Loss: 0.32830052077770233, Final Batch Loss: 0.15517321228981018\n",
      "Epoch 2291, Loss: 0.38148239254951477, Final Batch Loss: 0.1907491534948349\n",
      "Epoch 2292, Loss: 0.4084192365407944, Final Batch Loss: 0.19846870005130768\n",
      "Epoch 2293, Loss: 0.4406388998031616, Final Batch Loss: 0.2392226904630661\n",
      "Epoch 2294, Loss: 0.2969472259283066, Final Batch Loss: 0.1261739879846573\n",
      "Epoch 2295, Loss: 0.2965431734919548, Final Batch Loss: 0.11257458478212357\n",
      "Epoch 2296, Loss: 0.34747281670570374, Final Batch Loss: 0.1805926412343979\n",
      "Epoch 2297, Loss: 0.36933836340904236, Final Batch Loss: 0.23612180352210999\n",
      "Epoch 2298, Loss: 0.3925117254257202, Final Batch Loss: 0.23387646675109863\n",
      "Epoch 2299, Loss: 0.3393632620573044, Final Batch Loss: 0.15842869877815247\n",
      "Epoch 2300, Loss: 0.4328141212463379, Final Batch Loss: 0.2191801220178604\n",
      "Epoch 2301, Loss: 0.4040775001049042, Final Batch Loss: 0.25325578451156616\n",
      "Epoch 2302, Loss: 0.3698700815439224, Final Batch Loss: 0.22948019206523895\n",
      "Epoch 2303, Loss: 0.35738323628902435, Final Batch Loss: 0.16629821062088013\n",
      "Epoch 2304, Loss: 0.36757323145866394, Final Batch Loss: 0.15723249316215515\n",
      "Epoch 2305, Loss: 0.3395416736602783, Final Batch Loss: 0.1572106033563614\n",
      "Epoch 2306, Loss: 0.4213661700487137, Final Batch Loss: 0.22848370671272278\n",
      "Epoch 2307, Loss: 0.4087700843811035, Final Batch Loss: 0.2210932821035385\n",
      "Epoch 2308, Loss: 0.42240194976329803, Final Batch Loss: 0.2295270562171936\n",
      "Epoch 2309, Loss: 0.29200099408626556, Final Batch Loss: 0.13620281219482422\n",
      "Epoch 2310, Loss: 0.3345504254102707, Final Batch Loss: 0.18941649794578552\n",
      "Epoch 2311, Loss: 0.36087799072265625, Final Batch Loss: 0.1794174164533615\n",
      "Epoch 2312, Loss: 0.32362841069698334, Final Batch Loss: 0.16423995792865753\n",
      "Epoch 2313, Loss: 0.36268350481987, Final Batch Loss: 0.16117456555366516\n",
      "Epoch 2314, Loss: 0.32843925058841705, Final Batch Loss: 0.1724201738834381\n",
      "Epoch 2315, Loss: 0.33449262380599976, Final Batch Loss: 0.16195794939994812\n",
      "Epoch 2316, Loss: 0.35834817588329315, Final Batch Loss: 0.16739432513713837\n",
      "Epoch 2317, Loss: 0.33882661163806915, Final Batch Loss: 0.15124061703681946\n",
      "Epoch 2318, Loss: 0.3438495099544525, Final Batch Loss: 0.14203011989593506\n",
      "Epoch 2319, Loss: 0.4561987668275833, Final Batch Loss: 0.27145451307296753\n",
      "Epoch 2320, Loss: 0.3570382595062256, Final Batch Loss: 0.18599671125411987\n",
      "Epoch 2321, Loss: 0.3557446002960205, Final Batch Loss: 0.22144097089767456\n",
      "Epoch 2322, Loss: 0.32924695312976837, Final Batch Loss: 0.14350314438343048\n",
      "Epoch 2323, Loss: 0.31691472232341766, Final Batch Loss: 0.13762874901294708\n",
      "Epoch 2324, Loss: 0.35321733355522156, Final Batch Loss: 0.15800224244594574\n",
      "Epoch 2325, Loss: 0.4748251587152481, Final Batch Loss: 0.2944110035896301\n",
      "Epoch 2326, Loss: 0.4269288331270218, Final Batch Loss: 0.17981643974781036\n",
      "Epoch 2327, Loss: 0.34025123715400696, Final Batch Loss: 0.17753374576568604\n",
      "Epoch 2328, Loss: 0.31379900872707367, Final Batch Loss: 0.15788564085960388\n",
      "Epoch 2329, Loss: 0.421933576464653, Final Batch Loss: 0.22379477322101593\n",
      "Epoch 2330, Loss: 0.45025597512722015, Final Batch Loss: 0.28246045112609863\n",
      "Epoch 2331, Loss: 0.37178562581539154, Final Batch Loss: 0.14856736361980438\n",
      "Epoch 2332, Loss: 0.3856816589832306, Final Batch Loss: 0.20169652998447418\n",
      "Epoch 2333, Loss: 0.37218232452869415, Final Batch Loss: 0.18557889759540558\n",
      "Epoch 2334, Loss: 0.3093799203634262, Final Batch Loss: 0.15429390966892242\n",
      "Epoch 2335, Loss: 0.4384412467479706, Final Batch Loss: 0.2294400930404663\n",
      "Epoch 2336, Loss: 0.3071559965610504, Final Batch Loss: 0.12464600801467896\n",
      "Epoch 2337, Loss: 0.35728222131729126, Final Batch Loss: 0.1589190810918808\n",
      "Epoch 2338, Loss: 0.29676225781440735, Final Batch Loss: 0.15025076270103455\n",
      "Epoch 2339, Loss: 0.34643638134002686, Final Batch Loss: 0.14998048543930054\n",
      "Epoch 2340, Loss: 0.36616696417331696, Final Batch Loss: 0.2071172147989273\n",
      "Epoch 2341, Loss: 0.3501015603542328, Final Batch Loss: 0.22502776980400085\n",
      "Epoch 2342, Loss: 0.3321143537759781, Final Batch Loss: 0.1677214801311493\n",
      "Epoch 2343, Loss: 0.3279368132352829, Final Batch Loss: 0.17973646521568298\n",
      "Epoch 2344, Loss: 0.34087611734867096, Final Batch Loss: 0.14923974871635437\n",
      "Epoch 2345, Loss: 0.3712904006242752, Final Batch Loss: 0.1404404193162918\n",
      "Epoch 2346, Loss: 0.3279758244752884, Final Batch Loss: 0.18322718143463135\n",
      "Epoch 2347, Loss: 0.3587888032197952, Final Batch Loss: 0.1643955409526825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2348, Loss: 0.2900123745203018, Final Batch Loss: 0.13088536262512207\n",
      "Epoch 2349, Loss: 0.3641740530729294, Final Batch Loss: 0.19062380492687225\n",
      "Epoch 2350, Loss: 0.4443770498037338, Final Batch Loss: 0.22535395622253418\n",
      "Epoch 2351, Loss: 0.3626067489385605, Final Batch Loss: 0.1881617158651352\n",
      "Epoch 2352, Loss: 0.3490724563598633, Final Batch Loss: 0.15143384039402008\n",
      "Epoch 2353, Loss: 0.3638027012348175, Final Batch Loss: 0.23079808056354523\n",
      "Epoch 2354, Loss: 0.3147096633911133, Final Batch Loss: 0.13779616355895996\n",
      "Epoch 2355, Loss: 0.3866693526506424, Final Batch Loss: 0.22269997000694275\n",
      "Epoch 2356, Loss: 0.3462284058332443, Final Batch Loss: 0.16984343528747559\n",
      "Epoch 2357, Loss: 0.3625708967447281, Final Batch Loss: 0.19773630797863007\n",
      "Epoch 2358, Loss: 0.3527041971683502, Final Batch Loss: 0.19548752903938293\n",
      "Epoch 2359, Loss: 0.36284714937210083, Final Batch Loss: 0.14272412657737732\n",
      "Epoch 2360, Loss: 0.3733402192592621, Final Batch Loss: 0.16556940972805023\n",
      "Epoch 2361, Loss: 0.3779755234718323, Final Batch Loss: 0.2228536158800125\n",
      "Epoch 2362, Loss: 0.32261255383491516, Final Batch Loss: 0.16952955722808838\n",
      "Epoch 2363, Loss: 0.35579392313957214, Final Batch Loss: 0.16975773870944977\n",
      "Epoch 2364, Loss: 0.4032633453607559, Final Batch Loss: 0.2167937308549881\n",
      "Epoch 2365, Loss: 0.3414239287376404, Final Batch Loss: 0.1575743854045868\n",
      "Epoch 2366, Loss: 0.3148839622735977, Final Batch Loss: 0.15070278942584991\n",
      "Epoch 2367, Loss: 0.34284064173698425, Final Batch Loss: 0.16774480044841766\n",
      "Epoch 2368, Loss: 0.3736840784549713, Final Batch Loss: 0.1892760992050171\n",
      "Epoch 2369, Loss: 0.3494086414575577, Final Batch Loss: 0.17252127826213837\n",
      "Epoch 2370, Loss: 0.34747523069381714, Final Batch Loss: 0.14397448301315308\n",
      "Epoch 2371, Loss: 0.34281693398952484, Final Batch Loss: 0.13542333245277405\n",
      "Epoch 2372, Loss: 0.33634476363658905, Final Batch Loss: 0.17546029388904572\n",
      "Epoch 2373, Loss: 0.3733639717102051, Final Batch Loss: 0.20585180819034576\n",
      "Epoch 2374, Loss: 0.34738896787166595, Final Batch Loss: 0.17314843833446503\n",
      "Epoch 2375, Loss: 0.34082111716270447, Final Batch Loss: 0.19928497076034546\n",
      "Epoch 2376, Loss: 0.36074167490005493, Final Batch Loss: 0.1903090924024582\n",
      "Epoch 2377, Loss: 0.3253176063299179, Final Batch Loss: 0.1625591367483139\n",
      "Epoch 2378, Loss: 0.34923577308654785, Final Batch Loss: 0.1302509605884552\n",
      "Epoch 2379, Loss: 0.4478862136602402, Final Batch Loss: 0.26353490352630615\n",
      "Epoch 2380, Loss: 0.34264886379241943, Final Batch Loss: 0.1533355563879013\n",
      "Epoch 2381, Loss: 0.3485593646764755, Final Batch Loss: 0.1936546266078949\n",
      "Epoch 2382, Loss: 0.3416700065135956, Final Batch Loss: 0.1457861214876175\n",
      "Epoch 2383, Loss: 0.3000333532691002, Final Batch Loss: 0.12478000670671463\n",
      "Epoch 2384, Loss: 0.3167820870876312, Final Batch Loss: 0.15516500174999237\n",
      "Epoch 2385, Loss: 0.3051857575774193, Final Batch Loss: 0.12315940111875534\n",
      "Epoch 2386, Loss: 0.4544469714164734, Final Batch Loss: 0.2776048183441162\n",
      "Epoch 2387, Loss: 0.5623763054609299, Final Batch Loss: 0.18685056269168854\n",
      "Epoch 2388, Loss: 0.3405761569738388, Final Batch Loss: 0.1919928640127182\n",
      "Epoch 2389, Loss: 0.4114004224538803, Final Batch Loss: 0.2849229872226715\n",
      "Epoch 2390, Loss: 0.32882414758205414, Final Batch Loss: 0.18015722930431366\n",
      "Epoch 2391, Loss: 0.3220171183347702, Final Batch Loss: 0.15623901784420013\n",
      "Epoch 2392, Loss: 0.3853490948677063, Final Batch Loss: 0.1917535960674286\n",
      "Epoch 2393, Loss: 0.29641686379909515, Final Batch Loss: 0.1530296802520752\n",
      "Epoch 2394, Loss: 0.4272161275148392, Final Batch Loss: 0.26471564173698425\n",
      "Epoch 2395, Loss: 0.3519727289676666, Final Batch Loss: 0.1673310101032257\n",
      "Epoch 2396, Loss: 0.3570210039615631, Final Batch Loss: 0.13422200083732605\n",
      "Epoch 2397, Loss: 0.3461820259690285, Final Batch Loss: 0.11743081361055374\n",
      "Epoch 2398, Loss: 0.429685078561306, Final Batch Loss: 0.31158921122550964\n",
      "Epoch 2399, Loss: 0.2957071512937546, Final Batch Loss: 0.13332995772361755\n",
      "Epoch 2400, Loss: 0.3575446456670761, Final Batch Loss: 0.1743791252374649\n",
      "Epoch 2401, Loss: 0.301896870136261, Final Batch Loss: 0.13730767369270325\n",
      "Epoch 2402, Loss: 0.3187658488750458, Final Batch Loss: 0.13321955502033234\n",
      "Epoch 2403, Loss: 0.37193331122398376, Final Batch Loss: 0.18879522383213043\n",
      "Epoch 2404, Loss: 0.37026871740818024, Final Batch Loss: 0.1651974320411682\n",
      "Epoch 2405, Loss: 0.3630961626768112, Final Batch Loss: 0.1682070940732956\n",
      "Epoch 2406, Loss: 0.38256096839904785, Final Batch Loss: 0.19511954486370087\n",
      "Epoch 2407, Loss: 0.3671252280473709, Final Batch Loss: 0.18549798429012299\n",
      "Epoch 2408, Loss: 0.32156872749328613, Final Batch Loss: 0.15306857228279114\n",
      "Epoch 2409, Loss: 0.4448065906763077, Final Batch Loss: 0.22634856402873993\n",
      "Epoch 2410, Loss: 0.3219585567712784, Final Batch Loss: 0.16398200392723083\n",
      "Epoch 2411, Loss: 0.35878342390060425, Final Batch Loss: 0.16790468990802765\n",
      "Epoch 2412, Loss: 0.36025579273700714, Final Batch Loss: 0.1646314263343811\n",
      "Epoch 2413, Loss: 0.3094322979450226, Final Batch Loss: 0.1518472582101822\n",
      "Epoch 2414, Loss: 0.3875925987958908, Final Batch Loss: 0.2316858023405075\n",
      "Epoch 2415, Loss: 0.34399862587451935, Final Batch Loss: 0.18256767094135284\n",
      "Epoch 2416, Loss: 0.36449216306209564, Final Batch Loss: 0.21226727962493896\n",
      "Epoch 2417, Loss: 0.3719506561756134, Final Batch Loss: 0.21041807532310486\n",
      "Epoch 2418, Loss: 0.41259683668613434, Final Batch Loss: 0.22537744045257568\n",
      "Epoch 2419, Loss: 0.32731834799051285, Final Batch Loss: 0.12386070936918259\n",
      "Epoch 2420, Loss: 0.3216320723295212, Final Batch Loss: 0.12276886403560638\n",
      "Epoch 2421, Loss: 0.3830282390117645, Final Batch Loss: 0.20458756387233734\n",
      "Epoch 2422, Loss: 0.33914025127887726, Final Batch Loss: 0.17193995416164398\n",
      "Epoch 2423, Loss: 0.3062469810247421, Final Batch Loss: 0.1516563892364502\n",
      "Epoch 2424, Loss: 0.3846414089202881, Final Batch Loss: 0.23053456842899323\n",
      "Epoch 2425, Loss: 0.3511287569999695, Final Batch Loss: 0.16515611112117767\n",
      "Epoch 2426, Loss: 0.30169253051280975, Final Batch Loss: 0.13763104379177094\n",
      "Epoch 2427, Loss: 0.36515262722969055, Final Batch Loss: 0.1527777463197708\n",
      "Epoch 2428, Loss: 0.3735707104206085, Final Batch Loss: 0.17618432641029358\n",
      "Epoch 2429, Loss: 0.331204816699028, Final Batch Loss: 0.15673674643039703\n",
      "Epoch 2430, Loss: 0.3905071020126343, Final Batch Loss: 0.2249545007944107\n",
      "Epoch 2431, Loss: 0.3392052501440048, Final Batch Loss: 0.15222619473934174\n",
      "Epoch 2432, Loss: 0.33372247219085693, Final Batch Loss: 0.18333984911441803\n",
      "Epoch 2433, Loss: 0.4048406034708023, Final Batch Loss: 0.23620733618736267\n",
      "Epoch 2434, Loss: 0.3217037320137024, Final Batch Loss: 0.14733557403087616\n",
      "Epoch 2435, Loss: 0.36826270818710327, Final Batch Loss: 0.2176344245672226\n",
      "Epoch 2436, Loss: 0.3356056958436966, Final Batch Loss: 0.1634216010570526\n",
      "Epoch 2437, Loss: 0.3359099477529526, Final Batch Loss: 0.17498137056827545\n",
      "Epoch 2438, Loss: 0.34902992844581604, Final Batch Loss: 0.1878408044576645\n",
      "Epoch 2439, Loss: 0.32655058801174164, Final Batch Loss: 0.17408494651317596\n",
      "Epoch 2440, Loss: 0.2981373369693756, Final Batch Loss: 0.13510411977767944\n",
      "Epoch 2441, Loss: 0.3397594690322876, Final Batch Loss: 0.15729962289333344\n",
      "Epoch 2442, Loss: 0.34243522584438324, Final Batch Loss: 0.19432799518108368\n",
      "Epoch 2443, Loss: 0.34890812635421753, Final Batch Loss: 0.1803114116191864\n",
      "Epoch 2444, Loss: 0.34865210950374603, Final Batch Loss: 0.1591358184814453\n",
      "Epoch 2445, Loss: 0.3269367963075638, Final Batch Loss: 0.145565927028656\n",
      "Epoch 2446, Loss: 0.3015091270208359, Final Batch Loss: 0.12591582536697388\n",
      "Epoch 2447, Loss: 0.38610491156578064, Final Batch Loss: 0.22201445698738098\n",
      "Epoch 2448, Loss: 0.3313424736261368, Final Batch Loss: 0.17649179697036743\n",
      "Epoch 2449, Loss: 0.392527237534523, Final Batch Loss: 0.23766981065273285\n",
      "Epoch 2450, Loss: 0.3542633652687073, Final Batch Loss: 0.18101143836975098\n",
      "Epoch 2451, Loss: 0.3020019382238388, Final Batch Loss: 0.143723726272583\n",
      "Epoch 2452, Loss: 0.36265550553798676, Final Batch Loss: 0.21202939748764038\n",
      "Epoch 2453, Loss: 0.35566017031669617, Final Batch Loss: 0.16471369564533234\n",
      "Epoch 2454, Loss: 0.4117453098297119, Final Batch Loss: 0.22823867201805115\n",
      "Epoch 2455, Loss: 0.3437073826789856, Final Batch Loss: 0.14363320171833038\n",
      "Epoch 2456, Loss: 0.38349057734012604, Final Batch Loss: 0.18513323366641998\n",
      "Epoch 2457, Loss: 0.34133900701999664, Final Batch Loss: 0.16209672391414642\n",
      "Epoch 2458, Loss: 0.3353826403617859, Final Batch Loss: 0.1808273047208786\n",
      "Epoch 2459, Loss: 0.3391626328229904, Final Batch Loss: 0.13346144556999207\n",
      "Epoch 2460, Loss: 0.3097236007452011, Final Batch Loss: 0.13469891250133514\n",
      "Epoch 2461, Loss: 0.3307700604200363, Final Batch Loss: 0.1714949607849121\n",
      "Epoch 2462, Loss: 0.3135951831936836, Final Batch Loss: 0.11263211816549301\n",
      "Epoch 2463, Loss: 0.36732548475265503, Final Batch Loss: 0.18889357149600983\n",
      "Epoch 2464, Loss: 0.327470064163208, Final Batch Loss: 0.18149125576019287\n",
      "Epoch 2465, Loss: 0.3362918794155121, Final Batch Loss: 0.17606109380722046\n",
      "Epoch 2466, Loss: 0.3620685189962387, Final Batch Loss: 0.21564142405986786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2467, Loss: 0.3237292915582657, Final Batch Loss: 0.16801920533180237\n",
      "Epoch 2468, Loss: 0.38230520486831665, Final Batch Loss: 0.2026698887348175\n",
      "Epoch 2469, Loss: 0.40084175765514374, Final Batch Loss: 0.21752706170082092\n",
      "Epoch 2470, Loss: 0.3714348375797272, Final Batch Loss: 0.22510050237178802\n",
      "Epoch 2471, Loss: 0.3248744457960129, Final Batch Loss: 0.12745770812034607\n",
      "Epoch 2472, Loss: 0.3792423754930496, Final Batch Loss: 0.20037370920181274\n",
      "Epoch 2473, Loss: 0.28968483209609985, Final Batch Loss: 0.1377735286951065\n",
      "Epoch 2474, Loss: 0.3787953704595566, Final Batch Loss: 0.21245881915092468\n",
      "Epoch 2475, Loss: 0.4220384359359741, Final Batch Loss: 0.24054038524627686\n",
      "Epoch 2476, Loss: 0.31581658124923706, Final Batch Loss: 0.15407121181488037\n",
      "Epoch 2477, Loss: 0.35321828722953796, Final Batch Loss: 0.18077120184898376\n",
      "Epoch 2478, Loss: 0.28226669877767563, Final Batch Loss: 0.10929413884878159\n",
      "Epoch 2479, Loss: 0.36888158321380615, Final Batch Loss: 0.22317735850811005\n",
      "Epoch 2480, Loss: 0.285226546227932, Final Batch Loss: 0.1057412251830101\n",
      "Epoch 2481, Loss: 0.31487323343753815, Final Batch Loss: 0.15447591245174408\n",
      "Epoch 2482, Loss: 0.39708511531352997, Final Batch Loss: 0.21330781280994415\n",
      "Epoch 2483, Loss: 0.34802311658859253, Final Batch Loss: 0.1927391141653061\n",
      "Epoch 2484, Loss: 0.3478083461523056, Final Batch Loss: 0.14880047738552094\n",
      "Epoch 2485, Loss: 0.30264967679977417, Final Batch Loss: 0.15521608293056488\n",
      "Epoch 2486, Loss: 0.29541251063346863, Final Batch Loss: 0.10097192227840424\n",
      "Epoch 2487, Loss: 0.3488236516714096, Final Batch Loss: 0.1373525857925415\n",
      "Epoch 2488, Loss: 0.3092053383588791, Final Batch Loss: 0.11681409180164337\n",
      "Epoch 2489, Loss: 0.2774747237563133, Final Batch Loss: 0.12237543612718582\n",
      "Epoch 2490, Loss: 0.30141739547252655, Final Batch Loss: 0.12922488152980804\n",
      "Epoch 2491, Loss: 0.3135797679424286, Final Batch Loss: 0.14349853992462158\n",
      "Epoch 2492, Loss: 0.3248814046382904, Final Batch Loss: 0.15742997825145721\n",
      "Epoch 2493, Loss: 0.3044494688510895, Final Batch Loss: 0.09913156926631927\n",
      "Epoch 2494, Loss: 0.33432437479496, Final Batch Loss: 0.16280190646648407\n",
      "Epoch 2495, Loss: 0.3227929025888443, Final Batch Loss: 0.15632560849189758\n",
      "Epoch 2496, Loss: 0.33969646692276, Final Batch Loss: 0.17942476272583008\n",
      "Epoch 2497, Loss: 0.3078355938196182, Final Batch Loss: 0.1260542869567871\n",
      "Epoch 2498, Loss: 0.2953849583864212, Final Batch Loss: 0.1278916299343109\n",
      "Epoch 2499, Loss: 0.3653697222471237, Final Batch Loss: 0.2067374289035797\n",
      "Epoch 2500, Loss: 0.29113835096359253, Final Batch Loss: 0.15234561264514923\n",
      "Epoch 2501, Loss: 0.29652296006679535, Final Batch Loss: 0.11177749931812286\n",
      "Epoch 2502, Loss: 0.373371884226799, Final Batch Loss: 0.19175007939338684\n",
      "Epoch 2503, Loss: 0.3906012624502182, Final Batch Loss: 0.22669516503810883\n",
      "Epoch 2504, Loss: 0.31424613296985626, Final Batch Loss: 0.14137442409992218\n",
      "Epoch 2505, Loss: 0.3513520658016205, Final Batch Loss: 0.16824692487716675\n",
      "Epoch 2506, Loss: 0.3298908472061157, Final Batch Loss: 0.18470452725887299\n",
      "Epoch 2507, Loss: 0.37436582148075104, Final Batch Loss: 0.18730798363685608\n",
      "Epoch 2508, Loss: 0.3524474948644638, Final Batch Loss: 0.15795476734638214\n",
      "Epoch 2509, Loss: 0.39403724670410156, Final Batch Loss: 0.2384577840566635\n",
      "Epoch 2510, Loss: 0.37501010298728943, Final Batch Loss: 0.22962768375873566\n",
      "Epoch 2511, Loss: 0.336913600564003, Final Batch Loss: 0.14083659648895264\n",
      "Epoch 2512, Loss: 0.31211984157562256, Final Batch Loss: 0.14127837121486664\n",
      "Epoch 2513, Loss: 0.3528687059879303, Final Batch Loss: 0.19671832025051117\n",
      "Epoch 2514, Loss: 0.3646543473005295, Final Batch Loss: 0.1896427869796753\n",
      "Epoch 2515, Loss: 0.3648685812950134, Final Batch Loss: 0.1230739951133728\n",
      "Epoch 2516, Loss: 0.30183687806129456, Final Batch Loss: 0.13509312272071838\n",
      "Epoch 2517, Loss: 0.35199373960494995, Final Batch Loss: 0.1688818633556366\n",
      "Epoch 2518, Loss: 0.3440845310688019, Final Batch Loss: 0.16717566549777985\n",
      "Epoch 2519, Loss: 0.3170384466648102, Final Batch Loss: 0.17470891773700714\n",
      "Epoch 2520, Loss: 0.2613750919699669, Final Batch Loss: 0.09225062280893326\n",
      "Epoch 2521, Loss: 0.29518038034439087, Final Batch Loss: 0.13899686932563782\n",
      "Epoch 2522, Loss: 0.3277266174554825, Final Batch Loss: 0.15256117284297943\n",
      "Epoch 2523, Loss: 0.2955791726708412, Final Batch Loss: 0.11709720641374588\n",
      "Epoch 2524, Loss: 0.30462123453617096, Final Batch Loss: 0.13306856155395508\n",
      "Epoch 2525, Loss: 0.40984658896923065, Final Batch Loss: 0.23158852756023407\n",
      "Epoch 2526, Loss: 0.3845093697309494, Final Batch Loss: 0.21382324397563934\n",
      "Epoch 2527, Loss: 0.33488665521144867, Final Batch Loss: 0.17768310010433197\n",
      "Epoch 2528, Loss: 0.36321626603603363, Final Batch Loss: 0.155892014503479\n",
      "Epoch 2529, Loss: 0.34285442531108856, Final Batch Loss: 0.16609975695610046\n",
      "Epoch 2530, Loss: 0.31477509438991547, Final Batch Loss: 0.17330718040466309\n",
      "Epoch 2531, Loss: 0.298752561211586, Final Batch Loss: 0.1339135468006134\n",
      "Epoch 2532, Loss: 0.3402028828859329, Final Batch Loss: 0.17223797738552094\n",
      "Epoch 2533, Loss: 0.296800822019577, Final Batch Loss: 0.13761675357818604\n",
      "Epoch 2534, Loss: 0.3229133188724518, Final Batch Loss: 0.1752578467130661\n",
      "Epoch 2535, Loss: 0.2897668778896332, Final Batch Loss: 0.14935946464538574\n",
      "Epoch 2536, Loss: 0.3535009175539017, Final Batch Loss: 0.18128830194473267\n",
      "Epoch 2537, Loss: 0.3000841289758682, Final Batch Loss: 0.1598958820104599\n",
      "Epoch 2538, Loss: 0.3062126487493515, Final Batch Loss: 0.14163649082183838\n",
      "Epoch 2539, Loss: 0.3711576759815216, Final Batch Loss: 0.1555120050907135\n",
      "Epoch 2540, Loss: 0.39398153126239777, Final Batch Loss: 0.17350153625011444\n",
      "Epoch 2541, Loss: 0.3226766437292099, Final Batch Loss: 0.1483745276927948\n",
      "Epoch 2542, Loss: 0.4092225432395935, Final Batch Loss: 0.22723086178302765\n",
      "Epoch 2543, Loss: 0.3077456206083298, Final Batch Loss: 0.137592613697052\n",
      "Epoch 2544, Loss: 0.3290766626596451, Final Batch Loss: 0.1670442819595337\n",
      "Epoch 2545, Loss: 0.4055973142385483, Final Batch Loss: 0.1924504041671753\n",
      "Epoch 2546, Loss: 0.31138475239276886, Final Batch Loss: 0.162234365940094\n",
      "Epoch 2547, Loss: 0.35224246978759766, Final Batch Loss: 0.1778060644865036\n",
      "Epoch 2548, Loss: 0.3027283400297165, Final Batch Loss: 0.13683491945266724\n",
      "Epoch 2549, Loss: 0.3092322498559952, Final Batch Loss: 0.16119180619716644\n",
      "Epoch 2550, Loss: 0.42046281695365906, Final Batch Loss: 0.15693649649620056\n",
      "Epoch 2551, Loss: 0.3602275848388672, Final Batch Loss: 0.16424721479415894\n",
      "Epoch 2552, Loss: 0.3336194008588791, Final Batch Loss: 0.16597416996955872\n",
      "Epoch 2553, Loss: 0.32353653758764267, Final Batch Loss: 0.12187250703573227\n",
      "Epoch 2554, Loss: 0.33049847185611725, Final Batch Loss: 0.1590299755334854\n",
      "Epoch 2555, Loss: 0.33068157732486725, Final Batch Loss: 0.1432567536830902\n",
      "Epoch 2556, Loss: 0.3592899739742279, Final Batch Loss: 0.1495322585105896\n",
      "Epoch 2557, Loss: 0.3319520652294159, Final Batch Loss: 0.13441713154315948\n",
      "Epoch 2558, Loss: 0.4509492963552475, Final Batch Loss: 0.26423555612564087\n",
      "Epoch 2559, Loss: 0.40809644758701324, Final Batch Loss: 0.24719461798667908\n",
      "Epoch 2560, Loss: 0.3429054170846939, Final Batch Loss: 0.14344725012779236\n",
      "Epoch 2561, Loss: 0.32568229734897614, Final Batch Loss: 0.13875171542167664\n",
      "Epoch 2562, Loss: 0.3259190320968628, Final Batch Loss: 0.16352087259292603\n",
      "Epoch 2563, Loss: 0.3533879220485687, Final Batch Loss: 0.18632647395133972\n",
      "Epoch 2564, Loss: 0.3938456177711487, Final Batch Loss: 0.2386348694562912\n",
      "Epoch 2565, Loss: 0.28547462075948715, Final Batch Loss: 0.12259618192911148\n",
      "Epoch 2566, Loss: 0.38334624469280243, Final Batch Loss: 0.23497796058654785\n",
      "Epoch 2567, Loss: 0.3134046941995621, Final Batch Loss: 0.13121278584003448\n",
      "Epoch 2568, Loss: 0.3210991322994232, Final Batch Loss: 0.1368829905986786\n",
      "Epoch 2569, Loss: 0.37891167402267456, Final Batch Loss: 0.24048550426959991\n",
      "Epoch 2570, Loss: 0.29521647095680237, Final Batch Loss: 0.1297510713338852\n",
      "Epoch 2571, Loss: 0.3582994341850281, Final Batch Loss: 0.20590417087078094\n",
      "Epoch 2572, Loss: 0.375664085149765, Final Batch Loss: 0.2049759477376938\n",
      "Epoch 2573, Loss: 0.3172811269760132, Final Batch Loss: 0.13382455706596375\n",
      "Epoch 2574, Loss: 0.36950549483299255, Final Batch Loss: 0.22597572207450867\n",
      "Epoch 2575, Loss: 0.36208392679691315, Final Batch Loss: 0.21802321076393127\n",
      "Epoch 2576, Loss: 0.2893481031060219, Final Batch Loss: 0.0666622444987297\n",
      "Epoch 2577, Loss: 0.3358784317970276, Final Batch Loss: 0.13449227809906006\n",
      "Epoch 2578, Loss: 0.3887966722249985, Final Batch Loss: 0.2354464828968048\n",
      "Epoch 2579, Loss: 0.307931512594223, Final Batch Loss: 0.15552443265914917\n",
      "Epoch 2580, Loss: 0.3275945484638214, Final Batch Loss: 0.12058018147945404\n",
      "Epoch 2581, Loss: 0.39248234033584595, Final Batch Loss: 0.18758508563041687\n",
      "Epoch 2582, Loss: 0.36300933361053467, Final Batch Loss: 0.1609349101781845\n",
      "Epoch 2583, Loss: 0.32136397063732147, Final Batch Loss: 0.13781289756298065\n",
      "Epoch 2584, Loss: 0.269346222281456, Final Batch Loss: 0.10944320261478424\n",
      "Epoch 2585, Loss: 0.3869800418615341, Final Batch Loss: 0.2211163491010666\n",
      "Epoch 2586, Loss: 0.31622442603111267, Final Batch Loss: 0.1622450202703476\n",
      "Epoch 2587, Loss: 0.3452052026987076, Final Batch Loss: 0.14182943105697632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2588, Loss: 0.3560514748096466, Final Batch Loss: 0.1626933217048645\n",
      "Epoch 2589, Loss: 0.35604624450206757, Final Batch Loss: 0.16581915318965912\n",
      "Epoch 2590, Loss: 0.3056976795196533, Final Batch Loss: 0.1434321403503418\n",
      "Epoch 2591, Loss: 0.38701915740966797, Final Batch Loss: 0.24048058688640594\n",
      "Epoch 2592, Loss: 0.35267212986946106, Final Batch Loss: 0.16528013348579407\n",
      "Epoch 2593, Loss: 0.36052581667900085, Final Batch Loss: 0.21418090164661407\n",
      "Epoch 2594, Loss: 0.29105304181575775, Final Batch Loss: 0.1377503126859665\n",
      "Epoch 2595, Loss: 0.3286489099264145, Final Batch Loss: 0.1399846374988556\n",
      "Epoch 2596, Loss: 0.33306485414505005, Final Batch Loss: 0.17920784652233124\n",
      "Epoch 2597, Loss: 0.48964712023735046, Final Batch Loss: 0.330121785402298\n",
      "Epoch 2598, Loss: 0.3079705983400345, Final Batch Loss: 0.12461212277412415\n",
      "Epoch 2599, Loss: 0.321977898478508, Final Batch Loss: 0.17111635208129883\n",
      "Epoch 2600, Loss: 0.3352929651737213, Final Batch Loss: 0.14622201025485992\n",
      "Epoch 2601, Loss: 0.31856144964694977, Final Batch Loss: 0.17000237107276917\n",
      "Epoch 2602, Loss: 0.3107506185770035, Final Batch Loss: 0.12559859454631805\n",
      "Epoch 2603, Loss: 0.3789242058992386, Final Batch Loss: 0.17410778999328613\n",
      "Epoch 2604, Loss: 0.26917459070682526, Final Batch Loss: 0.11297020316123962\n",
      "Epoch 2605, Loss: 0.3760579973459244, Final Batch Loss: 0.216715008020401\n",
      "Epoch 2606, Loss: 0.32340188324451447, Final Batch Loss: 0.18069376051425934\n",
      "Epoch 2607, Loss: 0.35489417612552643, Final Batch Loss: 0.17950277030467987\n",
      "Epoch 2608, Loss: 0.30490607023239136, Final Batch Loss: 0.16186515986919403\n",
      "Epoch 2609, Loss: 0.31760843098163605, Final Batch Loss: 0.15280981361865997\n",
      "Epoch 2610, Loss: 0.43015551567077637, Final Batch Loss: 0.24066384136676788\n",
      "Epoch 2611, Loss: 0.29929327964782715, Final Batch Loss: 0.13420936465263367\n",
      "Epoch 2612, Loss: 0.33549976348876953, Final Batch Loss: 0.17597277462482452\n",
      "Epoch 2613, Loss: 0.3198215141892433, Final Batch Loss: 0.10511418431997299\n",
      "Epoch 2614, Loss: 0.3572787642478943, Final Batch Loss: 0.1841200739145279\n",
      "Epoch 2615, Loss: 0.3770187795162201, Final Batch Loss: 0.1949702650308609\n",
      "Epoch 2616, Loss: 0.29166436195373535, Final Batch Loss: 0.15067730844020844\n",
      "Epoch 2617, Loss: 0.31636275351047516, Final Batch Loss: 0.17615437507629395\n",
      "Epoch 2618, Loss: 0.32156968116760254, Final Batch Loss: 0.16249555349349976\n",
      "Epoch 2619, Loss: 0.37724652886390686, Final Batch Loss: 0.24856314063072205\n",
      "Epoch 2620, Loss: 0.29823804646730423, Final Batch Loss: 0.12266574054956436\n",
      "Epoch 2621, Loss: 0.3089854121208191, Final Batch Loss: 0.1158655434846878\n",
      "Epoch 2622, Loss: 0.3681628108024597, Final Batch Loss: 0.16850042343139648\n",
      "Epoch 2623, Loss: 0.36971908807754517, Final Batch Loss: 0.165005624294281\n",
      "Epoch 2624, Loss: 0.3150158077478409, Final Batch Loss: 0.15523645281791687\n",
      "Epoch 2625, Loss: 0.3237948566675186, Final Batch Loss: 0.15885229408740997\n",
      "Epoch 2626, Loss: 0.36599256098270416, Final Batch Loss: 0.1894836127758026\n",
      "Epoch 2627, Loss: 0.2949094995856285, Final Batch Loss: 0.10638555139303207\n",
      "Epoch 2628, Loss: 0.3440205901861191, Final Batch Loss: 0.1960071623325348\n",
      "Epoch 2629, Loss: 0.2779211103916168, Final Batch Loss: 0.1519942283630371\n",
      "Epoch 2630, Loss: 0.3147580325603485, Final Batch Loss: 0.18196001648902893\n",
      "Epoch 2631, Loss: 0.35055510699748993, Final Batch Loss: 0.17298369109630585\n",
      "Epoch 2632, Loss: 0.3370169699192047, Final Batch Loss: 0.19131217896938324\n",
      "Epoch 2633, Loss: 0.27312932163476944, Final Batch Loss: 0.10434017330408096\n",
      "Epoch 2634, Loss: 0.47020381689071655, Final Batch Loss: 0.27429628372192383\n",
      "Epoch 2635, Loss: 0.31125858426094055, Final Batch Loss: 0.14332838356494904\n",
      "Epoch 2636, Loss: 0.30291852355003357, Final Batch Loss: 0.16558519005775452\n",
      "Epoch 2637, Loss: 0.3916650414466858, Final Batch Loss: 0.21501728892326355\n",
      "Epoch 2638, Loss: 0.34435389935970306, Final Batch Loss: 0.17335863411426544\n",
      "Epoch 2639, Loss: 0.3484252393245697, Final Batch Loss: 0.19912618398666382\n",
      "Epoch 2640, Loss: 0.3845103234052658, Final Batch Loss: 0.24110831320285797\n",
      "Epoch 2641, Loss: 0.2893867567181587, Final Batch Loss: 0.12494879215955734\n",
      "Epoch 2642, Loss: 0.30622464418411255, Final Batch Loss: 0.11691589653491974\n",
      "Epoch 2643, Loss: 0.2668028548359871, Final Batch Loss: 0.12119408696889877\n",
      "Epoch 2644, Loss: 0.3055703192949295, Final Batch Loss: 0.1253509372472763\n",
      "Epoch 2645, Loss: 0.3292551338672638, Final Batch Loss: 0.16952691972255707\n",
      "Epoch 2646, Loss: 0.2802022397518158, Final Batch Loss: 0.14114907383918762\n",
      "Epoch 2647, Loss: 0.30929452180862427, Final Batch Loss: 0.15917982161045074\n",
      "Epoch 2648, Loss: 0.36754199862480164, Final Batch Loss: 0.21803784370422363\n",
      "Epoch 2649, Loss: 0.6001207679510117, Final Batch Loss: 0.4335907995700836\n",
      "Epoch 2650, Loss: 0.3651364743709564, Final Batch Loss: 0.1720457375049591\n",
      "Epoch 2651, Loss: 0.2956750690937042, Final Batch Loss: 0.15211449563503265\n",
      "Epoch 2652, Loss: 0.3677680939435959, Final Batch Loss: 0.18347448110580444\n",
      "Epoch 2653, Loss: 0.3004058450460434, Final Batch Loss: 0.15554456412792206\n",
      "Epoch 2654, Loss: 0.2673792093992233, Final Batch Loss: 0.11350370943546295\n",
      "Epoch 2655, Loss: 0.28869087994098663, Final Batch Loss: 0.13197800517082214\n",
      "Epoch 2656, Loss: 0.29000091552734375, Final Batch Loss: 0.1433575600385666\n",
      "Epoch 2657, Loss: 0.37398768961429596, Final Batch Loss: 0.190254807472229\n",
      "Epoch 2658, Loss: 0.2931893467903137, Final Batch Loss: 0.13855749368667603\n",
      "Epoch 2659, Loss: 0.3753860145807266, Final Batch Loss: 0.17284071445465088\n",
      "Epoch 2660, Loss: 0.32575879991054535, Final Batch Loss: 0.14531715214252472\n",
      "Epoch 2661, Loss: 0.33403754234313965, Final Batch Loss: 0.19084589183330536\n",
      "Epoch 2662, Loss: 0.2894001603126526, Final Batch Loss: 0.14144457876682281\n",
      "Epoch 2663, Loss: 0.33664922416210175, Final Batch Loss: 0.1812646985054016\n",
      "Epoch 2664, Loss: 0.3119610548019409, Final Batch Loss: 0.14367951452732086\n",
      "Epoch 2665, Loss: 0.32323959469795227, Final Batch Loss: 0.17450852692127228\n",
      "Epoch 2666, Loss: 0.3385636657476425, Final Batch Loss: 0.1734081357717514\n",
      "Epoch 2667, Loss: 0.26993900537490845, Final Batch Loss: 0.0944138765335083\n",
      "Epoch 2668, Loss: 0.37435682117938995, Final Batch Loss: 0.18215401470661163\n",
      "Epoch 2669, Loss: 0.3094691038131714, Final Batch Loss: 0.13308964669704437\n",
      "Epoch 2670, Loss: 0.30007314682006836, Final Batch Loss: 0.13190652430057526\n",
      "Epoch 2671, Loss: 0.3437640517950058, Final Batch Loss: 0.17338745296001434\n",
      "Epoch 2672, Loss: 0.3863535672426224, Final Batch Loss: 0.20623743534088135\n",
      "Epoch 2673, Loss: 0.33666084706783295, Final Batch Loss: 0.1763664036989212\n",
      "Epoch 2674, Loss: 0.31423673033714294, Final Batch Loss: 0.13881736993789673\n",
      "Epoch 2675, Loss: 0.3067821189761162, Final Batch Loss: 0.11599736660718918\n",
      "Epoch 2676, Loss: 0.3707713782787323, Final Batch Loss: 0.19532227516174316\n",
      "Epoch 2677, Loss: 0.36914587020874023, Final Batch Loss: 0.1990475207567215\n",
      "Epoch 2678, Loss: 0.3365158438682556, Final Batch Loss: 0.1773248165845871\n",
      "Epoch 2679, Loss: 0.4163770526647568, Final Batch Loss: 0.284347265958786\n",
      "Epoch 2680, Loss: 0.3758210092782974, Final Batch Loss: 0.22559668123722076\n",
      "Epoch 2681, Loss: 0.26316720992326736, Final Batch Loss: 0.1111268475651741\n",
      "Epoch 2682, Loss: 0.3272415101528168, Final Batch Loss: 0.17163239419460297\n",
      "Epoch 2683, Loss: 0.29772602021694183, Final Batch Loss: 0.1455245018005371\n",
      "Epoch 2684, Loss: 0.38743364810943604, Final Batch Loss: 0.2375999242067337\n",
      "Epoch 2685, Loss: 0.34326253831386566, Final Batch Loss: 0.19850203394889832\n",
      "Epoch 2686, Loss: 0.299266055226326, Final Batch Loss: 0.14715325832366943\n",
      "Epoch 2687, Loss: 0.36494971811771393, Final Batch Loss: 0.22039489448070526\n",
      "Epoch 2688, Loss: 0.3346145451068878, Final Batch Loss: 0.20214028656482697\n",
      "Epoch 2689, Loss: 0.35374920070171356, Final Batch Loss: 0.20286376774311066\n",
      "Epoch 2690, Loss: 0.3013857305049896, Final Batch Loss: 0.14043685793876648\n",
      "Epoch 2691, Loss: 0.35862481594085693, Final Batch Loss: 0.1484972983598709\n",
      "Epoch 2692, Loss: 0.36138592660427094, Final Batch Loss: 0.20948494970798492\n",
      "Epoch 2693, Loss: 0.30776357650756836, Final Batch Loss: 0.13977591693401337\n",
      "Epoch 2694, Loss: 0.3341996967792511, Final Batch Loss: 0.18637113273143768\n",
      "Epoch 2695, Loss: 0.321723073720932, Final Batch Loss: 0.13425202667713165\n",
      "Epoch 2696, Loss: 0.3389078229665756, Final Batch Loss: 0.13967415690422058\n",
      "Epoch 2697, Loss: 0.34792137145996094, Final Batch Loss: 0.17204582691192627\n",
      "Epoch 2698, Loss: 0.2778145521879196, Final Batch Loss: 0.13084006309509277\n",
      "Epoch 2699, Loss: 0.29833316057920456, Final Batch Loss: 0.11773739010095596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2700, Loss: 0.34012462198734283, Final Batch Loss: 0.2101193517446518\n",
      "Epoch 2701, Loss: 0.2892989367246628, Final Batch Loss: 0.1520584523677826\n",
      "Epoch 2702, Loss: 0.35603776574134827, Final Batch Loss: 0.19873246550559998\n",
      "Epoch 2703, Loss: 0.363326832652092, Final Batch Loss: 0.16148541867733002\n",
      "Epoch 2704, Loss: 0.3327580839395523, Final Batch Loss: 0.19547811150550842\n",
      "Epoch 2705, Loss: 0.36767150461673737, Final Batch Loss: 0.18385711312294006\n",
      "Epoch 2706, Loss: 0.38781675696372986, Final Batch Loss: 0.18731719255447388\n",
      "Epoch 2707, Loss: 0.29377370327711105, Final Batch Loss: 0.10344109684228897\n",
      "Epoch 2708, Loss: 0.41921618580818176, Final Batch Loss: 0.25465402007102966\n",
      "Epoch 2709, Loss: 0.3452892005443573, Final Batch Loss: 0.19502992928028107\n",
      "Epoch 2710, Loss: 0.35609160363674164, Final Batch Loss: 0.19245977699756622\n",
      "Epoch 2711, Loss: 0.342224583029747, Final Batch Loss: 0.21429327130317688\n",
      "Epoch 2712, Loss: 0.2995636612176895, Final Batch Loss: 0.13293634355068207\n",
      "Epoch 2713, Loss: 0.28210629522800446, Final Batch Loss: 0.13168931007385254\n",
      "Epoch 2714, Loss: 0.366975799202919, Final Batch Loss: 0.13345323503017426\n",
      "Epoch 2715, Loss: 0.39415135979652405, Final Batch Loss: 0.23510046303272247\n",
      "Epoch 2716, Loss: 0.32614992558956146, Final Batch Loss: 0.1566488891839981\n",
      "Epoch 2717, Loss: 0.41658975183963776, Final Batch Loss: 0.24177899956703186\n",
      "Epoch 2718, Loss: 0.36728236079216003, Final Batch Loss: 0.20955680310726166\n",
      "Epoch 2719, Loss: 0.3282714784145355, Final Batch Loss: 0.19565238058567047\n",
      "Epoch 2720, Loss: 0.2793329805135727, Final Batch Loss: 0.11975806951522827\n",
      "Epoch 2721, Loss: 0.27975938469171524, Final Batch Loss: 0.08993258327245712\n",
      "Epoch 2722, Loss: 0.3205808252096176, Final Batch Loss: 0.15234574675559998\n",
      "Epoch 2723, Loss: 0.3239252120256424, Final Batch Loss: 0.18052007257938385\n",
      "Epoch 2724, Loss: 0.3183693587779999, Final Batch Loss: 0.13710761070251465\n",
      "Epoch 2725, Loss: 0.44542214274406433, Final Batch Loss: 0.26051226258277893\n",
      "Epoch 2726, Loss: 0.2786197066307068, Final Batch Loss: 0.12038421630859375\n",
      "Epoch 2727, Loss: 0.3255459815263748, Final Batch Loss: 0.15659447014331818\n",
      "Epoch 2728, Loss: 0.3239731937646866, Final Batch Loss: 0.19242402911186218\n",
      "Epoch 2729, Loss: 0.3164291977882385, Final Batch Loss: 0.17146064341068268\n",
      "Epoch 2730, Loss: 0.28677157312631607, Final Batch Loss: 0.11513251811265945\n",
      "Epoch 2731, Loss: 0.3319907933473587, Final Batch Loss: 0.13302282989025116\n",
      "Epoch 2732, Loss: 0.3062560260295868, Final Batch Loss: 0.16942323744297028\n",
      "Epoch 2733, Loss: 0.32727906107902527, Final Batch Loss: 0.14448843896389008\n",
      "Epoch 2734, Loss: 0.3049886152148247, Final Batch Loss: 0.12371932715177536\n",
      "Epoch 2735, Loss: 0.2652188688516617, Final Batch Loss: 0.12340670824050903\n",
      "Epoch 2736, Loss: 0.28917402774095535, Final Batch Loss: 0.11744800955057144\n",
      "Epoch 2737, Loss: 0.30522532761096954, Final Batch Loss: 0.11926090717315674\n",
      "Epoch 2738, Loss: 0.35762186348438263, Final Batch Loss: 0.16988283395767212\n",
      "Epoch 2739, Loss: 0.3542235642671585, Final Batch Loss: 0.1785191297531128\n",
      "Epoch 2740, Loss: 0.32212330400943756, Final Batch Loss: 0.1464616060256958\n",
      "Epoch 2741, Loss: 0.3747183382511139, Final Batch Loss: 0.1756361424922943\n",
      "Epoch 2742, Loss: 0.2833593413233757, Final Batch Loss: 0.1031307503581047\n",
      "Epoch 2743, Loss: 0.3154306262731552, Final Batch Loss: 0.16271668672561646\n",
      "Epoch 2744, Loss: 0.33047522604465485, Final Batch Loss: 0.17148779332637787\n",
      "Epoch 2745, Loss: 0.2799680829048157, Final Batch Loss: 0.13735656440258026\n",
      "Epoch 2746, Loss: 0.278704509139061, Final Batch Loss: 0.12731194496154785\n",
      "Epoch 2747, Loss: 0.3104821890592575, Final Batch Loss: 0.14811575412750244\n",
      "Epoch 2748, Loss: 0.4502743184566498, Final Batch Loss: 0.27934572100639343\n",
      "Epoch 2749, Loss: 0.3321322351694107, Final Batch Loss: 0.13991409540176392\n",
      "Epoch 2750, Loss: 0.3165746331214905, Final Batch Loss: 0.159699484705925\n",
      "Epoch 2751, Loss: 0.2923455908894539, Final Batch Loss: 0.12220221012830734\n",
      "Epoch 2752, Loss: 0.2937071770429611, Final Batch Loss: 0.16291992366313934\n",
      "Epoch 2753, Loss: 0.24257227033376694, Final Batch Loss: 0.08565100282430649\n",
      "Epoch 2754, Loss: 0.319846048951149, Final Batch Loss: 0.14262178540229797\n",
      "Epoch 2755, Loss: 0.30552223324775696, Final Batch Loss: 0.13358765840530396\n",
      "Epoch 2756, Loss: 0.35644054412841797, Final Batch Loss: 0.17522813379764557\n",
      "Epoch 2757, Loss: 0.318958580493927, Final Batch Loss: 0.13030576705932617\n",
      "Epoch 2758, Loss: 0.3230028301477432, Final Batch Loss: 0.14184680581092834\n",
      "Epoch 2759, Loss: 0.26781731843948364, Final Batch Loss: 0.13962210714817047\n",
      "Epoch 2760, Loss: 0.396125391125679, Final Batch Loss: 0.21558409929275513\n",
      "Epoch 2761, Loss: 0.27581004053354263, Final Batch Loss: 0.12131696194410324\n",
      "Epoch 2762, Loss: 0.3086375743150711, Final Batch Loss: 0.13950173556804657\n",
      "Epoch 2763, Loss: 0.2562028616666794, Final Batch Loss: 0.09859627485275269\n",
      "Epoch 2764, Loss: 0.3444199413061142, Final Batch Loss: 0.1958712786436081\n",
      "Epoch 2765, Loss: 0.2771257385611534, Final Batch Loss: 0.11475054174661636\n",
      "Epoch 2766, Loss: 0.30526066571474075, Final Batch Loss: 0.18133588135242462\n",
      "Epoch 2767, Loss: 0.32072824239730835, Final Batch Loss: 0.17849060893058777\n",
      "Epoch 2768, Loss: 0.3150177448987961, Final Batch Loss: 0.13674265146255493\n",
      "Epoch 2769, Loss: 0.30695532262325287, Final Batch Loss: 0.14813143014907837\n",
      "Epoch 2770, Loss: 0.3054615780711174, Final Batch Loss: 0.1814977079629898\n",
      "Epoch 2771, Loss: 0.33652377128601074, Final Batch Loss: 0.15835236012935638\n",
      "Epoch 2772, Loss: 0.29760532081127167, Final Batch Loss: 0.16635461151599884\n",
      "Epoch 2773, Loss: 0.27662046253681183, Final Batch Loss: 0.14377045631408691\n",
      "Epoch 2774, Loss: 0.35609741508960724, Final Batch Loss: 0.18246454000473022\n",
      "Epoch 2775, Loss: 0.3050822392106056, Final Batch Loss: 0.11506421118974686\n",
      "Epoch 2776, Loss: 0.3243270516395569, Final Batch Loss: 0.16218294203281403\n",
      "Epoch 2777, Loss: 0.32349826395511627, Final Batch Loss: 0.17937780916690826\n",
      "Epoch 2778, Loss: 0.32650765776634216, Final Batch Loss: 0.11785419285297394\n",
      "Epoch 2779, Loss: 0.3045993894338608, Final Batch Loss: 0.17955704033374786\n",
      "Epoch 2780, Loss: 0.28308213502168655, Final Batch Loss: 0.09958062320947647\n",
      "Epoch 2781, Loss: 0.31697021424770355, Final Batch Loss: 0.1610817164182663\n",
      "Epoch 2782, Loss: 0.32427656650543213, Final Batch Loss: 0.1871911585330963\n",
      "Epoch 2783, Loss: 0.3335244059562683, Final Batch Loss: 0.1622786968946457\n",
      "Epoch 2784, Loss: 0.29323017597198486, Final Batch Loss: 0.14220817387104034\n",
      "Epoch 2785, Loss: 0.34674254059791565, Final Batch Loss: 0.20995856821537018\n",
      "Epoch 2786, Loss: 0.32339900732040405, Final Batch Loss: 0.14306515455245972\n",
      "Epoch 2787, Loss: 0.36923834681510925, Final Batch Loss: 0.18959477543830872\n",
      "Epoch 2788, Loss: 0.35711751878261566, Final Batch Loss: 0.20927663147449493\n",
      "Epoch 2789, Loss: 0.3951246589422226, Final Batch Loss: 0.20727922022342682\n",
      "Epoch 2790, Loss: 0.3536199480295181, Final Batch Loss: 0.21550631523132324\n",
      "Epoch 2791, Loss: 0.3960391879081726, Final Batch Loss: 0.1412331759929657\n",
      "Epoch 2792, Loss: 0.34211158752441406, Final Batch Loss: 0.14841674268245697\n",
      "Epoch 2793, Loss: 0.3219752013683319, Final Batch Loss: 0.19039347767829895\n",
      "Epoch 2794, Loss: 0.3183371275663376, Final Batch Loss: 0.16334307193756104\n",
      "Epoch 2795, Loss: 0.3404945135116577, Final Batch Loss: 0.20404307544231415\n",
      "Epoch 2796, Loss: 0.3453745096921921, Final Batch Loss: 0.18768304586410522\n",
      "Epoch 2797, Loss: 0.29604077339172363, Final Batch Loss: 0.1622655987739563\n",
      "Epoch 2798, Loss: 0.33226320147514343, Final Batch Loss: 0.17844140529632568\n",
      "Epoch 2799, Loss: 0.3219774067401886, Final Batch Loss: 0.16557520627975464\n",
      "Epoch 2800, Loss: 0.35045628249645233, Final Batch Loss: 0.16430389881134033\n",
      "Epoch 2801, Loss: 0.27197354286909103, Final Batch Loss: 0.10346563905477524\n",
      "Epoch 2802, Loss: 0.3137761056423187, Final Batch Loss: 0.18521735072135925\n",
      "Epoch 2803, Loss: 0.26303239166736603, Final Batch Loss: 0.11029002070426941\n",
      "Epoch 2804, Loss: 0.33588361740112305, Final Batch Loss: 0.20239827036857605\n",
      "Epoch 2805, Loss: 0.2882669195532799, Final Batch Loss: 0.10564782470464706\n",
      "Epoch 2806, Loss: 0.37907445430755615, Final Batch Loss: 0.20937879383563995\n",
      "Epoch 2807, Loss: 0.33622901141643524, Final Batch Loss: 0.18232984840869904\n",
      "Epoch 2808, Loss: 0.23346928507089615, Final Batch Loss: 0.0719732716679573\n",
      "Epoch 2809, Loss: 0.32218703627586365, Final Batch Loss: 0.1731061041355133\n",
      "Epoch 2810, Loss: 0.260416217148304, Final Batch Loss: 0.12486227601766586\n",
      "Epoch 2811, Loss: 0.32224322855472565, Final Batch Loss: 0.1792185753583908\n",
      "Epoch 2812, Loss: 0.38071055710315704, Final Batch Loss: 0.18157239258289337\n",
      "Epoch 2813, Loss: 0.2535567432641983, Final Batch Loss: 0.13361354172229767\n",
      "Epoch 2814, Loss: 0.3004526197910309, Final Batch Loss: 0.14504431188106537\n",
      "Epoch 2815, Loss: 0.2835335433483124, Final Batch Loss: 0.13727706670761108\n",
      "Epoch 2816, Loss: 0.287909597158432, Final Batch Loss: 0.10517005622386932\n",
      "Epoch 2817, Loss: 0.29777856171131134, Final Batch Loss: 0.16170865297317505\n",
      "Epoch 2818, Loss: 0.29627884924411774, Final Batch Loss: 0.14150367677211761\n",
      "Epoch 2819, Loss: 0.32807154953479767, Final Batch Loss: 0.13492430746555328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2820, Loss: 0.36532458662986755, Final Batch Loss: 0.2015364021062851\n",
      "Epoch 2821, Loss: 0.33578526973724365, Final Batch Loss: 0.1655021756887436\n",
      "Epoch 2822, Loss: 0.34232114255428314, Final Batch Loss: 0.18674491345882416\n",
      "Epoch 2823, Loss: 0.31611794978380203, Final Batch Loss: 0.12060872465372086\n",
      "Epoch 2824, Loss: 0.2949056625366211, Final Batch Loss: 0.1282579004764557\n",
      "Epoch 2825, Loss: 0.3411039561033249, Final Batch Loss: 0.1897881031036377\n",
      "Epoch 2826, Loss: 0.3415653705596924, Final Batch Loss: 0.13758625090122223\n",
      "Epoch 2827, Loss: 0.3402617573738098, Final Batch Loss: 0.19446054100990295\n",
      "Epoch 2828, Loss: 0.32341189682483673, Final Batch Loss: 0.14625033736228943\n",
      "Epoch 2829, Loss: 0.30716776847839355, Final Batch Loss: 0.12969695031642914\n",
      "Epoch 2830, Loss: 0.3316190242767334, Final Batch Loss: 0.16770806908607483\n",
      "Epoch 2831, Loss: 0.3800530880689621, Final Batch Loss: 0.1367969661951065\n",
      "Epoch 2832, Loss: 0.2993001118302345, Final Batch Loss: 0.10349560528993607\n",
      "Epoch 2833, Loss: 0.29569797217845917, Final Batch Loss: 0.15327554941177368\n",
      "Epoch 2834, Loss: 0.3271214962005615, Final Batch Loss: 0.164276123046875\n",
      "Epoch 2835, Loss: 0.29114849865436554, Final Batch Loss: 0.1236465722322464\n",
      "Epoch 2836, Loss: 0.34346139430999756, Final Batch Loss: 0.14267241954803467\n",
      "Epoch 2837, Loss: 0.3007597550749779, Final Batch Loss: 0.1154884472489357\n",
      "Epoch 2838, Loss: 0.2992992699146271, Final Batch Loss: 0.12804235517978668\n",
      "Epoch 2839, Loss: 0.2965630143880844, Final Batch Loss: 0.11877928674221039\n",
      "Epoch 2840, Loss: 0.3236778825521469, Final Batch Loss: 0.1620723456144333\n",
      "Epoch 2841, Loss: 0.30061212182044983, Final Batch Loss: 0.13839426636695862\n",
      "Epoch 2842, Loss: 0.28270411491394043, Final Batch Loss: 0.1385875791311264\n",
      "Epoch 2843, Loss: 0.3089603781700134, Final Batch Loss: 0.13547399640083313\n",
      "Epoch 2844, Loss: 0.3328839838504791, Final Batch Loss: 0.18614766001701355\n",
      "Epoch 2845, Loss: 0.3034713864326477, Final Batch Loss: 0.16699591279029846\n",
      "Epoch 2846, Loss: 0.26819586753845215, Final Batch Loss: 0.1451307088136673\n",
      "Epoch 2847, Loss: 0.3555159866809845, Final Batch Loss: 0.20309598743915558\n",
      "Epoch 2848, Loss: 0.30902301520109177, Final Batch Loss: 0.11754538863897324\n",
      "Epoch 2849, Loss: 0.3237840533256531, Final Batch Loss: 0.17865079641342163\n",
      "Epoch 2850, Loss: 0.3034791797399521, Final Batch Loss: 0.16318616271018982\n",
      "Epoch 2851, Loss: 0.4121149480342865, Final Batch Loss: 0.17817984521389008\n",
      "Epoch 2852, Loss: 0.27522240579128265, Final Batch Loss: 0.11401507258415222\n",
      "Epoch 2853, Loss: 0.25348710268735886, Final Batch Loss: 0.08883854001760483\n",
      "Epoch 2854, Loss: 0.29267118871212006, Final Batch Loss: 0.1450173258781433\n",
      "Epoch 2855, Loss: 0.32009096443653107, Final Batch Loss: 0.17234407365322113\n",
      "Epoch 2856, Loss: 0.27908777445554733, Final Batch Loss: 0.11396551877260208\n",
      "Epoch 2857, Loss: 0.3034709692001343, Final Batch Loss: 0.1440708041191101\n",
      "Epoch 2858, Loss: 0.30553147196769714, Final Batch Loss: 0.16132843494415283\n",
      "Epoch 2859, Loss: 0.4194915145635605, Final Batch Loss: 0.20244409143924713\n",
      "Epoch 2860, Loss: 0.29931214451789856, Final Batch Loss: 0.1317051500082016\n",
      "Epoch 2861, Loss: 0.3031980097293854, Final Batch Loss: 0.14972302317619324\n",
      "Epoch 2862, Loss: 0.3006233274936676, Final Batch Loss: 0.11310596764087677\n",
      "Epoch 2863, Loss: 0.4549827426671982, Final Batch Loss: 0.27173668146133423\n",
      "Epoch 2864, Loss: 0.35179656744003296, Final Batch Loss: 0.1705050766468048\n",
      "Epoch 2865, Loss: 0.3166348487138748, Final Batch Loss: 0.16951176524162292\n",
      "Epoch 2866, Loss: 0.3048769235610962, Final Batch Loss: 0.14343343675136566\n",
      "Epoch 2867, Loss: 0.29106278717517853, Final Batch Loss: 0.11244063079357147\n",
      "Epoch 2868, Loss: 0.30229616165161133, Final Batch Loss: 0.13455766439437866\n",
      "Epoch 2869, Loss: 0.2989087998867035, Final Batch Loss: 0.1361529380083084\n",
      "Epoch 2870, Loss: 0.36225153505802155, Final Batch Loss: 0.194596529006958\n",
      "Epoch 2871, Loss: 0.3060535043478012, Final Batch Loss: 0.15905462205410004\n",
      "Epoch 2872, Loss: 0.27545636892318726, Final Batch Loss: 0.1383504569530487\n",
      "Epoch 2873, Loss: 0.3413742035627365, Final Batch Loss: 0.1789223700761795\n",
      "Epoch 2874, Loss: 0.32555195689201355, Final Batch Loss: 0.15400825440883636\n",
      "Epoch 2875, Loss: 0.3214126378297806, Final Batch Loss: 0.15679481625556946\n",
      "Epoch 2876, Loss: 0.31693360209465027, Final Batch Loss: 0.15499365329742432\n",
      "Epoch 2877, Loss: 0.38604089617729187, Final Batch Loss: 0.21253027021884918\n",
      "Epoch 2878, Loss: 0.2803610563278198, Final Batch Loss: 0.11753766238689423\n",
      "Epoch 2879, Loss: 0.276652455329895, Final Batch Loss: 0.12054575979709625\n",
      "Epoch 2880, Loss: 0.2891397029161453, Final Batch Loss: 0.15930739045143127\n",
      "Epoch 2881, Loss: 0.330610454082489, Final Batch Loss: 0.1608196347951889\n",
      "Epoch 2882, Loss: 0.27313823252916336, Final Batch Loss: 0.11372921615839005\n",
      "Epoch 2883, Loss: 0.3377681002020836, Final Batch Loss: 0.2159430980682373\n",
      "Epoch 2884, Loss: 0.3854532241821289, Final Batch Loss: 0.2320929914712906\n",
      "Epoch 2885, Loss: 0.3590719997882843, Final Batch Loss: 0.1710086166858673\n",
      "Epoch 2886, Loss: 0.28172358870506287, Final Batch Loss: 0.12453457713127136\n",
      "Epoch 2887, Loss: 0.28044116497039795, Final Batch Loss: 0.12755528092384338\n",
      "Epoch 2888, Loss: 0.289498046040535, Final Batch Loss: 0.15034466981887817\n",
      "Epoch 2889, Loss: 0.30146458745002747, Final Batch Loss: 0.15957528352737427\n",
      "Epoch 2890, Loss: 0.3302847743034363, Final Batch Loss: 0.18405044078826904\n",
      "Epoch 2891, Loss: 0.3268399238586426, Final Batch Loss: 0.17667894065380096\n",
      "Epoch 2892, Loss: 0.3669535517692566, Final Batch Loss: 0.2137545645236969\n",
      "Epoch 2893, Loss: 0.3215101659297943, Final Batch Loss: 0.17562174797058105\n",
      "Epoch 2894, Loss: 0.30754221975803375, Final Batch Loss: 0.12982642650604248\n",
      "Epoch 2895, Loss: 0.2888731360435486, Final Batch Loss: 0.12796473503112793\n",
      "Epoch 2896, Loss: 0.26244787126779556, Final Batch Loss: 0.12338990718126297\n",
      "Epoch 2897, Loss: 0.2981039807200432, Final Batch Loss: 0.12485375255346298\n",
      "Epoch 2898, Loss: 0.321782186627388, Final Batch Loss: 0.1374114751815796\n",
      "Epoch 2899, Loss: 0.329864501953125, Final Batch Loss: 0.16521131992340088\n",
      "Epoch 2900, Loss: 0.29130586981773376, Final Batch Loss: 0.13438281416893005\n",
      "Epoch 2901, Loss: 0.409501850605011, Final Batch Loss: 0.15141350030899048\n",
      "Epoch 2902, Loss: 0.2479267418384552, Final Batch Loss: 0.08309836685657501\n",
      "Epoch 2903, Loss: 0.36345069110393524, Final Batch Loss: 0.19553540647029877\n",
      "Epoch 2904, Loss: 0.37421219050884247, Final Batch Loss: 0.20223218202590942\n",
      "Epoch 2905, Loss: 0.28720423579216003, Final Batch Loss: 0.13656477630138397\n",
      "Epoch 2906, Loss: 0.33396993577480316, Final Batch Loss: 0.14730305969715118\n",
      "Epoch 2907, Loss: 0.32130496203899384, Final Batch Loss: 0.1623002141714096\n",
      "Epoch 2908, Loss: 0.28289519995450974, Final Batch Loss: 0.12232325226068497\n",
      "Epoch 2909, Loss: 0.27658861130476, Final Batch Loss: 0.1548236608505249\n",
      "Epoch 2910, Loss: 0.23074793815612793, Final Batch Loss: 0.11677166074514389\n",
      "Epoch 2911, Loss: 0.3151978999376297, Final Batch Loss: 0.1281859129667282\n",
      "Epoch 2912, Loss: 0.2851836383342743, Final Batch Loss: 0.11933349072933197\n",
      "Epoch 2913, Loss: 0.3327710032463074, Final Batch Loss: 0.18163307011127472\n",
      "Epoch 2914, Loss: 0.2982713580131531, Final Batch Loss: 0.1504897177219391\n",
      "Epoch 2915, Loss: 0.34490513801574707, Final Batch Loss: 0.18757514655590057\n",
      "Epoch 2916, Loss: 0.27828259766101837, Final Batch Loss: 0.11261250078678131\n",
      "Epoch 2917, Loss: 0.34300002455711365, Final Batch Loss: 0.18891401588916779\n",
      "Epoch 2918, Loss: 0.294272243976593, Final Batch Loss: 0.14032277464866638\n",
      "Epoch 2919, Loss: 0.31665025651454926, Final Batch Loss: 0.1571105271577835\n",
      "Epoch 2920, Loss: 0.3371916115283966, Final Batch Loss: 0.14951814711093903\n",
      "Epoch 2921, Loss: 0.2677862420678139, Final Batch Loss: 0.12143554538488388\n",
      "Epoch 2922, Loss: 0.3197062164545059, Final Batch Loss: 0.14860105514526367\n",
      "Epoch 2923, Loss: 0.3233587294816971, Final Batch Loss: 0.19531738758087158\n",
      "Epoch 2924, Loss: 0.3909749686717987, Final Batch Loss: 0.1798916757106781\n",
      "Epoch 2925, Loss: 0.33377404510974884, Final Batch Loss: 0.16761599481105804\n",
      "Epoch 2926, Loss: 0.3199850022792816, Final Batch Loss: 0.1644417643547058\n",
      "Epoch 2927, Loss: 0.3582065850496292, Final Batch Loss: 0.22489753365516663\n",
      "Epoch 2928, Loss: 0.5655296891927719, Final Batch Loss: 0.4183463752269745\n",
      "Epoch 2929, Loss: 0.40603119134902954, Final Batch Loss: 0.22994351387023926\n",
      "Epoch 2930, Loss: 0.30628958344459534, Final Batch Loss: 0.1398506611585617\n",
      "Epoch 2931, Loss: 0.3331577032804489, Final Batch Loss: 0.15829317271709442\n",
      "Epoch 2932, Loss: 0.3420850485563278, Final Batch Loss: 0.19162291288375854\n",
      "Epoch 2933, Loss: 0.3075200542807579, Final Batch Loss: 0.2008851319551468\n",
      "Epoch 2934, Loss: 0.305283784866333, Final Batch Loss: 0.1327785700559616\n",
      "Epoch 2935, Loss: 0.2851077765226364, Final Batch Loss: 0.14369769394397736\n",
      "Epoch 2936, Loss: 0.2995250001549721, Final Batch Loss: 0.18076662719249725\n",
      "Epoch 2937, Loss: 0.3380727767944336, Final Batch Loss: 0.19328387081623077\n",
      "Epoch 2938, Loss: 0.36284030973911285, Final Batch Loss: 0.1801505833864212\n",
      "Epoch 2939, Loss: 0.35293807089328766, Final Batch Loss: 0.1721011996269226\n",
      "Epoch 2940, Loss: 0.26728615164756775, Final Batch Loss: 0.09770327806472778\n",
      "Epoch 2941, Loss: 0.31343330442905426, Final Batch Loss: 0.14023716747760773\n",
      "Epoch 2942, Loss: 0.2637355998158455, Final Batch Loss: 0.11834307760000229\n",
      "Epoch 2943, Loss: 0.37747710943222046, Final Batch Loss: 0.1944732666015625\n",
      "Epoch 2944, Loss: 0.3046017289161682, Final Batch Loss: 0.14192268252372742\n",
      "Epoch 2945, Loss: 0.34619569778442383, Final Batch Loss: 0.15905342996120453\n",
      "Epoch 2946, Loss: 0.26914337277412415, Final Batch Loss: 0.1300888955593109\n",
      "Epoch 2947, Loss: 0.32223664224147797, Final Batch Loss: 0.15602922439575195\n",
      "Epoch 2948, Loss: 0.2561931684613228, Final Batch Loss: 0.11108037084341049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2949, Loss: 0.3494618982076645, Final Batch Loss: 0.2149384319782257\n",
      "Epoch 2950, Loss: 0.40242888033390045, Final Batch Loss: 0.23038703203201294\n",
      "Epoch 2951, Loss: 0.34401895105838776, Final Batch Loss: 0.1942083239555359\n",
      "Epoch 2952, Loss: 0.2918582707643509, Final Batch Loss: 0.14360311627388\n",
      "Epoch 2953, Loss: 0.35964377224445343, Final Batch Loss: 0.183827206492424\n",
      "Epoch 2954, Loss: 0.3663969188928604, Final Batch Loss: 0.1664981245994568\n",
      "Epoch 2955, Loss: 0.3396992161870003, Final Batch Loss: 0.22971844673156738\n",
      "Epoch 2956, Loss: 0.29732082039117813, Final Batch Loss: 0.11342313140630722\n",
      "Epoch 2957, Loss: 0.3213071823120117, Final Batch Loss: 0.14901013672351837\n",
      "Epoch 2958, Loss: 0.29394927620887756, Final Batch Loss: 0.12668552994728088\n",
      "Epoch 2959, Loss: 0.4325577914714813, Final Batch Loss: 0.26228243112564087\n",
      "Epoch 2960, Loss: 0.28275204449892044, Final Batch Loss: 0.11909934133291245\n",
      "Epoch 2961, Loss: 0.34745050966739655, Final Batch Loss: 0.16665391623973846\n",
      "Epoch 2962, Loss: 0.28473101556301117, Final Batch Loss: 0.11634600162506104\n",
      "Epoch 2963, Loss: 0.3212572783231735, Final Batch Loss: 0.13910362124443054\n",
      "Epoch 2964, Loss: 0.28403643518686295, Final Batch Loss: 0.06947890669107437\n",
      "Epoch 2965, Loss: 0.35904429852962494, Final Batch Loss: 0.2260919213294983\n",
      "Epoch 2966, Loss: 0.31576380878686905, Final Batch Loss: 0.19468756020069122\n",
      "Epoch 2967, Loss: 0.3403640687465668, Final Batch Loss: 0.15860360860824585\n",
      "Epoch 2968, Loss: 0.3264225795865059, Final Batch Loss: 0.11157145351171494\n",
      "Epoch 2969, Loss: 0.35081785917282104, Final Batch Loss: 0.20175474882125854\n",
      "Epoch 2970, Loss: 0.30264634639024734, Final Batch Loss: 0.17957235872745514\n",
      "Epoch 2971, Loss: 0.30051152408123016, Final Batch Loss: 0.13129134476184845\n",
      "Epoch 2972, Loss: 0.2571675777435303, Final Batch Loss: 0.11564265191555023\n",
      "Epoch 2973, Loss: 0.31054215878248215, Final Batch Loss: 0.1911478489637375\n",
      "Epoch 2974, Loss: 0.35147032141685486, Final Batch Loss: 0.20423181354999542\n",
      "Epoch 2975, Loss: 0.2821880429983139, Final Batch Loss: 0.13450203835964203\n",
      "Epoch 2976, Loss: 0.27766308188438416, Final Batch Loss: 0.14031371474266052\n",
      "Epoch 2977, Loss: 0.3014271631836891, Final Batch Loss: 0.11928530782461166\n",
      "Epoch 2978, Loss: 0.2746352106332779, Final Batch Loss: 0.1324206441640854\n",
      "Epoch 2979, Loss: 0.5622497797012329, Final Batch Loss: 0.167178213596344\n",
      "Epoch 2980, Loss: 0.24334929138422012, Final Batch Loss: 0.11559287458658218\n",
      "Epoch 2981, Loss: 0.2930017411708832, Final Batch Loss: 0.14544296264648438\n",
      "Epoch 2982, Loss: 0.31263378262519836, Final Batch Loss: 0.15445952117443085\n",
      "Epoch 2983, Loss: 0.35691171884536743, Final Batch Loss: 0.20924367010593414\n",
      "Epoch 2984, Loss: 0.3205236792564392, Final Batch Loss: 0.18181279301643372\n",
      "Epoch 2985, Loss: 0.3842243254184723, Final Batch Loss: 0.2661663591861725\n",
      "Epoch 2986, Loss: 0.29387491941452026, Final Batch Loss: 0.13211758434772491\n",
      "Epoch 2987, Loss: 0.266005277633667, Final Batch Loss: 0.1290595829486847\n",
      "Epoch 2988, Loss: 0.333109587430954, Final Batch Loss: 0.16204603016376495\n",
      "Epoch 2989, Loss: 0.299806647002697, Final Batch Loss: 0.189898282289505\n",
      "Epoch 2990, Loss: 0.2543119415640831, Final Batch Loss: 0.13718634843826294\n",
      "Epoch 2991, Loss: 0.31667737662792206, Final Batch Loss: 0.18607069551944733\n",
      "Epoch 2992, Loss: 0.2683558166027069, Final Batch Loss: 0.12075033783912659\n",
      "Epoch 2993, Loss: 0.2858777344226837, Final Batch Loss: 0.15005972981452942\n",
      "Epoch 2994, Loss: 0.399956077337265, Final Batch Loss: 0.26431959867477417\n",
      "Epoch 2995, Loss: 0.2768296152353287, Final Batch Loss: 0.13071206212043762\n",
      "Epoch 2996, Loss: 0.26499100774526596, Final Batch Loss: 0.11136969178915024\n",
      "Epoch 2997, Loss: 0.33926743268966675, Final Batch Loss: 0.21547533571720123\n",
      "Epoch 2998, Loss: 0.26701246201992035, Final Batch Loss: 0.1353234350681305\n",
      "Epoch 2999, Loss: 0.3549550920724869, Final Batch Loss: 0.17075732350349426\n",
      "Epoch 3000, Loss: 0.3222171813249588, Final Batch Loss: 0.19767247140407562\n",
      "Epoch 3001, Loss: 0.29862868785858154, Final Batch Loss: 0.1356920748949051\n",
      "Epoch 3002, Loss: 0.30843062698841095, Final Batch Loss: 0.14969880878925323\n",
      "Epoch 3003, Loss: 0.29199735820293427, Final Batch Loss: 0.15383359789848328\n",
      "Epoch 3004, Loss: 0.2921140491962433, Final Batch Loss: 0.13197438418865204\n",
      "Epoch 3005, Loss: 0.3065124452114105, Final Batch Loss: 0.17586158215999603\n",
      "Epoch 3006, Loss: 0.3605828210711479, Final Batch Loss: 0.24409528076648712\n",
      "Epoch 3007, Loss: 0.3656788170337677, Final Batch Loss: 0.2240796536207199\n",
      "Epoch 3008, Loss: 0.2802262455224991, Final Batch Loss: 0.11395996809005737\n",
      "Epoch 3009, Loss: 0.3454275131225586, Final Batch Loss: 0.19610817730426788\n",
      "Epoch 3010, Loss: 0.3128521293401718, Final Batch Loss: 0.1732698529958725\n",
      "Epoch 3011, Loss: 0.2470814734697342, Final Batch Loss: 0.1281791776418686\n",
      "Epoch 3012, Loss: 0.35405322909355164, Final Batch Loss: 0.1949983686208725\n",
      "Epoch 3013, Loss: 0.345928356051445, Final Batch Loss: 0.2129158079624176\n",
      "Epoch 3014, Loss: 0.3090692013502121, Final Batch Loss: 0.18252962827682495\n",
      "Epoch 3015, Loss: 0.3217732906341553, Final Batch Loss: 0.16218316555023193\n",
      "Epoch 3016, Loss: 0.24986041337251663, Final Batch Loss: 0.09273051470518112\n",
      "Epoch 3017, Loss: 0.2671273946762085, Final Batch Loss: 0.09572125971317291\n",
      "Epoch 3018, Loss: 0.28604403138160706, Final Batch Loss: 0.15292127430438995\n",
      "Epoch 3019, Loss: 0.3264356553554535, Final Batch Loss: 0.17168866097927094\n",
      "Epoch 3020, Loss: 0.27230925112962723, Final Batch Loss: 0.11008153110742569\n",
      "Epoch 3021, Loss: 0.34295639395713806, Final Batch Loss: 0.21410684287548065\n",
      "Epoch 3022, Loss: 0.26357367634773254, Final Batch Loss: 0.09941975772380829\n",
      "Epoch 3023, Loss: 0.2766083925962448, Final Batch Loss: 0.1304776668548584\n",
      "Epoch 3024, Loss: 0.2986505478620529, Final Batch Loss: 0.16053718328475952\n",
      "Epoch 3025, Loss: 0.2995256781578064, Final Batch Loss: 0.1501990705728531\n",
      "Epoch 3026, Loss: 0.2724490240216255, Final Batch Loss: 0.10320239514112473\n",
      "Epoch 3027, Loss: 0.31409670412540436, Final Batch Loss: 0.1697428673505783\n",
      "Epoch 3028, Loss: 0.4025901108980179, Final Batch Loss: 0.20722542703151703\n",
      "Epoch 3029, Loss: 0.27880236506462097, Final Batch Loss: 0.13337284326553345\n",
      "Epoch 3030, Loss: 0.3342268168926239, Final Batch Loss: 0.15181979537010193\n",
      "Epoch 3031, Loss: 0.2997678071260452, Final Batch Loss: 0.13347184658050537\n",
      "Epoch 3032, Loss: 0.3021696209907532, Final Batch Loss: 0.1499176174402237\n",
      "Epoch 3033, Loss: 0.2577393427491188, Final Batch Loss: 0.09533389657735825\n",
      "Epoch 3034, Loss: 0.33111312985420227, Final Batch Loss: 0.19995857775211334\n",
      "Epoch 3035, Loss: 0.28611166775226593, Final Batch Loss: 0.15957361459732056\n",
      "Epoch 3036, Loss: 0.3656705319881439, Final Batch Loss: 0.19495820999145508\n",
      "Epoch 3037, Loss: 0.3063346743583679, Final Batch Loss: 0.12062844634056091\n",
      "Epoch 3038, Loss: 0.32750535011291504, Final Batch Loss: 0.1929863691329956\n",
      "Epoch 3039, Loss: 0.31440334022045135, Final Batch Loss: 0.1717911809682846\n",
      "Epoch 3040, Loss: 0.4016517102718353, Final Batch Loss: 0.2574825584888458\n",
      "Epoch 3041, Loss: 0.2871786952018738, Final Batch Loss: 0.08223812282085419\n",
      "Epoch 3042, Loss: 0.3311648815870285, Final Batch Loss: 0.19802798330783844\n",
      "Epoch 3043, Loss: 0.25376469641923904, Final Batch Loss: 0.10886811465024948\n",
      "Epoch 3044, Loss: 0.26470090448856354, Final Batch Loss: 0.12953943014144897\n",
      "Epoch 3045, Loss: 0.41095302999019623, Final Batch Loss: 0.2405223846435547\n",
      "Epoch 3046, Loss: 0.3275150805711746, Final Batch Loss: 0.1673368364572525\n",
      "Epoch 3047, Loss: 0.3486037403345108, Final Batch Loss: 0.19833792746067047\n",
      "Epoch 3048, Loss: 0.2679039239883423, Final Batch Loss: 0.12401992082595825\n",
      "Epoch 3049, Loss: 0.26418638974428177, Final Batch Loss: 0.11910008639097214\n",
      "Epoch 3050, Loss: 0.28087128698825836, Final Batch Loss: 0.14382977783679962\n",
      "Epoch 3051, Loss: 0.3339696526527405, Final Batch Loss: 0.1975453943014145\n",
      "Epoch 3052, Loss: 0.27241646498441696, Final Batch Loss: 0.1232777014374733\n",
      "Epoch 3053, Loss: 0.25988759845495224, Final Batch Loss: 0.09737391024827957\n",
      "Epoch 3054, Loss: 0.2760118097066879, Final Batch Loss: 0.13592161238193512\n",
      "Epoch 3055, Loss: 0.2982863336801529, Final Batch Loss: 0.1629289984703064\n",
      "Epoch 3056, Loss: 0.3315581977367401, Final Batch Loss: 0.1381392478942871\n",
      "Epoch 3057, Loss: 0.3141307383775711, Final Batch Loss: 0.1399109661579132\n",
      "Epoch 3058, Loss: 0.33963124454021454, Final Batch Loss: 0.1994975358247757\n",
      "Epoch 3059, Loss: 0.24616989493370056, Final Batch Loss: 0.11966253817081451\n",
      "Epoch 3060, Loss: 0.31842629611492157, Final Batch Loss: 0.13944359123706818\n",
      "Epoch 3061, Loss: 0.2959577888250351, Final Batch Loss: 0.17848335206508636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3062, Loss: 0.30460236966609955, Final Batch Loss: 0.12705561518669128\n",
      "Epoch 3063, Loss: 0.33108681440353394, Final Batch Loss: 0.17017140984535217\n",
      "Epoch 3064, Loss: 0.2738946005702019, Final Batch Loss: 0.10383938997983932\n",
      "Epoch 3065, Loss: 0.2418658509850502, Final Batch Loss: 0.108037568628788\n",
      "Epoch 3066, Loss: 0.3193445950746536, Final Batch Loss: 0.1720838099718094\n",
      "Epoch 3067, Loss: 0.2615412771701813, Final Batch Loss: 0.1337110996246338\n",
      "Epoch 3068, Loss: 0.28092610090970993, Final Batch Loss: 0.15710629522800446\n",
      "Epoch 3069, Loss: 0.29285409301519394, Final Batch Loss: 0.1186174526810646\n",
      "Epoch 3070, Loss: 0.26885634660720825, Final Batch Loss: 0.1346832513809204\n",
      "Epoch 3071, Loss: 0.35777448117733, Final Batch Loss: 0.17800447344779968\n",
      "Epoch 3072, Loss: 0.31254348158836365, Final Batch Loss: 0.14366522431373596\n",
      "Epoch 3073, Loss: 0.2753727287054062, Final Batch Loss: 0.13619287312030792\n",
      "Epoch 3074, Loss: 0.32076670974493027, Final Batch Loss: 0.12424086779356003\n",
      "Epoch 3075, Loss: 0.32645338773727417, Final Batch Loss: 0.18923349678516388\n",
      "Epoch 3076, Loss: 0.327082559466362, Final Batch Loss: 0.13396814465522766\n",
      "Epoch 3077, Loss: 0.27469000220298767, Final Batch Loss: 0.11499598622322083\n",
      "Epoch 3078, Loss: 0.26176995038986206, Final Batch Loss: 0.11426495015621185\n",
      "Epoch 3079, Loss: 0.3436928167939186, Final Batch Loss: 0.22018778324127197\n",
      "Epoch 3080, Loss: 0.3627336919307709, Final Batch Loss: 0.19765740633010864\n",
      "Epoch 3081, Loss: 0.27706843614578247, Final Batch Loss: 0.1429573893547058\n",
      "Epoch 3082, Loss: 0.27190694212913513, Final Batch Loss: 0.1396866887807846\n",
      "Epoch 3083, Loss: 0.28622524440288544, Final Batch Loss: 0.14198210835456848\n",
      "Epoch 3084, Loss: 0.2662309482693672, Final Batch Loss: 0.11804813891649246\n",
      "Epoch 3085, Loss: 0.2860238552093506, Final Batch Loss: 0.13236573338508606\n",
      "Epoch 3086, Loss: 0.2790878862142563, Final Batch Loss: 0.13328997790813446\n",
      "Epoch 3087, Loss: 0.27650443464517593, Final Batch Loss: 0.11841372400522232\n",
      "Epoch 3088, Loss: 0.2768443003296852, Final Batch Loss: 0.10009226948022842\n",
      "Epoch 3089, Loss: 0.29123786091804504, Final Batch Loss: 0.12999261915683746\n",
      "Epoch 3090, Loss: 0.2852735072374344, Final Batch Loss: 0.14514760673046112\n",
      "Epoch 3091, Loss: 0.3400837630033493, Final Batch Loss: 0.16641436517238617\n",
      "Epoch 3092, Loss: 0.30460698157548904, Final Batch Loss: 0.11416210979223251\n",
      "Epoch 3093, Loss: 0.32788555324077606, Final Batch Loss: 0.1811075061559677\n",
      "Epoch 3094, Loss: 0.3221924901008606, Final Batch Loss: 0.1384834200143814\n",
      "Epoch 3095, Loss: 0.32003943622112274, Final Batch Loss: 0.16217249631881714\n",
      "Epoch 3096, Loss: 0.26790452003479004, Final Batch Loss: 0.14030787348747253\n",
      "Epoch 3097, Loss: 0.32847270369529724, Final Batch Loss: 0.16810593008995056\n",
      "Epoch 3098, Loss: 0.3320425897836685, Final Batch Loss: 0.16291560232639313\n",
      "Epoch 3099, Loss: 0.3695889115333557, Final Batch Loss: 0.20523470640182495\n",
      "Epoch 3100, Loss: 0.29855917394161224, Final Batch Loss: 0.12656757235527039\n",
      "Epoch 3101, Loss: 0.3371663987636566, Final Batch Loss: 0.16595107316970825\n",
      "Epoch 3102, Loss: 0.3205752372741699, Final Batch Loss: 0.14695850014686584\n",
      "Epoch 3103, Loss: 0.32815615832805634, Final Batch Loss: 0.11707140505313873\n",
      "Epoch 3104, Loss: 0.29986612498760223, Final Batch Loss: 0.14291280508041382\n",
      "Epoch 3105, Loss: 0.3320365697145462, Final Batch Loss: 0.16712556779384613\n",
      "Epoch 3106, Loss: 0.29740406572818756, Final Batch Loss: 0.13674065470695496\n",
      "Epoch 3107, Loss: 0.2785460948944092, Final Batch Loss: 0.14789792895317078\n",
      "Epoch 3108, Loss: 0.28859148919582367, Final Batch Loss: 0.13205312192440033\n",
      "Epoch 3109, Loss: 0.3089781552553177, Final Batch Loss: 0.15372776985168457\n",
      "Epoch 3110, Loss: 0.31292298436164856, Final Batch Loss: 0.16259874403476715\n",
      "Epoch 3111, Loss: 0.2412661761045456, Final Batch Loss: 0.12262793630361557\n",
      "Epoch 3112, Loss: 0.515866219997406, Final Batch Loss: 0.2714906632900238\n",
      "Epoch 3113, Loss: 0.3054702430963516, Final Batch Loss: 0.16135022044181824\n",
      "Epoch 3114, Loss: 0.31308627128601074, Final Batch Loss: 0.1516900509595871\n",
      "Epoch 3115, Loss: 0.31815408170223236, Final Batch Loss: 0.16531385481357574\n",
      "Epoch 3116, Loss: 0.34831270575523376, Final Batch Loss: 0.15216070413589478\n",
      "Epoch 3117, Loss: 0.322287380695343, Final Batch Loss: 0.17318953573703766\n",
      "Epoch 3118, Loss: 0.30383849143981934, Final Batch Loss: 0.15852975845336914\n",
      "Epoch 3119, Loss: 0.29585036635398865, Final Batch Loss: 0.1565103977918625\n",
      "Epoch 3120, Loss: 0.3179376572370529, Final Batch Loss: 0.16345925629138947\n",
      "Epoch 3121, Loss: 0.3252459466457367, Final Batch Loss: 0.19128359854221344\n",
      "Epoch 3122, Loss: 0.2787153273820877, Final Batch Loss: 0.10114532709121704\n",
      "Epoch 3123, Loss: 0.3266448676586151, Final Batch Loss: 0.18304643034934998\n",
      "Epoch 3124, Loss: 0.3785530775785446, Final Batch Loss: 0.2313069999217987\n",
      "Epoch 3125, Loss: 0.32011742889881134, Final Batch Loss: 0.1669445037841797\n",
      "Epoch 3126, Loss: 0.26906321197748184, Final Batch Loss: 0.09948741644620895\n",
      "Epoch 3127, Loss: 0.36539365351200104, Final Batch Loss: 0.20365265011787415\n",
      "Epoch 3128, Loss: 0.33301886916160583, Final Batch Loss: 0.1940002143383026\n",
      "Epoch 3129, Loss: 0.30422423779964447, Final Batch Loss: 0.13825944066047668\n",
      "Epoch 3130, Loss: 0.287006139755249, Final Batch Loss: 0.15223616361618042\n",
      "Epoch 3131, Loss: 0.3321130573749542, Final Batch Loss: 0.15954089164733887\n",
      "Epoch 3132, Loss: 0.30232249200344086, Final Batch Loss: 0.1385476291179657\n",
      "Epoch 3133, Loss: 0.3338661640882492, Final Batch Loss: 0.1645893156528473\n",
      "Epoch 3134, Loss: 0.28340189158916473, Final Batch Loss: 0.13939142227172852\n",
      "Epoch 3135, Loss: 0.2820193022489548, Final Batch Loss: 0.09868766367435455\n",
      "Epoch 3136, Loss: 0.29526399075984955, Final Batch Loss: 0.13293226063251495\n",
      "Epoch 3137, Loss: 0.2493899241089821, Final Batch Loss: 0.11444301158189774\n",
      "Epoch 3138, Loss: 0.33248376846313477, Final Batch Loss: 0.159128338098526\n",
      "Epoch 3139, Loss: 0.27906276285648346, Final Batch Loss: 0.14553089439868927\n",
      "Epoch 3140, Loss: 0.3113829642534256, Final Batch Loss: 0.1735931932926178\n",
      "Epoch 3141, Loss: 0.33552563190460205, Final Batch Loss: 0.1926986128091812\n",
      "Epoch 3142, Loss: 0.2831980362534523, Final Batch Loss: 0.10945158451795578\n",
      "Epoch 3143, Loss: 0.2844749763607979, Final Batch Loss: 0.1600848287343979\n",
      "Epoch 3144, Loss: 0.28636597096920013, Final Batch Loss: 0.13500238955020905\n",
      "Epoch 3145, Loss: 0.27006959915161133, Final Batch Loss: 0.11512087285518646\n",
      "Epoch 3146, Loss: 0.32751765847206116, Final Batch Loss: 0.1327100247144699\n",
      "Epoch 3147, Loss: 0.26414432376623154, Final Batch Loss: 0.10240790992975235\n",
      "Epoch 3148, Loss: 0.3108772188425064, Final Batch Loss: 0.14927469193935394\n",
      "Epoch 3149, Loss: 0.2673455849289894, Final Batch Loss: 0.11170163005590439\n",
      "Epoch 3150, Loss: 0.2718798667192459, Final Batch Loss: 0.1309698224067688\n",
      "Epoch 3151, Loss: 0.30417168140411377, Final Batch Loss: 0.14904744923114777\n",
      "Epoch 3152, Loss: 0.273676261305809, Final Batch Loss: 0.12039557099342346\n",
      "Epoch 3153, Loss: 0.3731713593006134, Final Batch Loss: 0.23093315958976746\n",
      "Epoch 3154, Loss: 0.3298002779483795, Final Batch Loss: 0.17953595519065857\n",
      "Epoch 3155, Loss: 0.3269847482442856, Final Batch Loss: 0.18976052105426788\n",
      "Epoch 3156, Loss: 0.3162859082221985, Final Batch Loss: 0.1818488985300064\n",
      "Epoch 3157, Loss: 0.27141448855400085, Final Batch Loss: 0.1422891467809677\n",
      "Epoch 3158, Loss: 0.341034933924675, Final Batch Loss: 0.1527375876903534\n",
      "Epoch 3159, Loss: 0.34926821291446686, Final Batch Loss: 0.21345408260822296\n",
      "Epoch 3160, Loss: 0.261081226170063, Final Batch Loss: 0.12440919131040573\n",
      "Epoch 3161, Loss: 0.32669082283973694, Final Batch Loss: 0.12225955724716187\n",
      "Epoch 3162, Loss: 0.3016302138566971, Final Batch Loss: 0.1405269354581833\n",
      "Epoch 3163, Loss: 0.2630022168159485, Final Batch Loss: 0.08179852366447449\n",
      "Epoch 3164, Loss: 0.29397042095661163, Final Batch Loss: 0.1641724556684494\n",
      "Epoch 3165, Loss: 0.2808085083961487, Final Batch Loss: 0.14704103767871857\n",
      "Epoch 3166, Loss: 0.3045958876609802, Final Batch Loss: 0.13163064420223236\n",
      "Epoch 3167, Loss: 0.22354689240455627, Final Batch Loss: 0.10926032066345215\n",
      "Epoch 3168, Loss: 0.3238237351179123, Final Batch Loss: 0.1587149202823639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3169, Loss: 0.2989775091409683, Final Batch Loss: 0.15745043754577637\n",
      "Epoch 3170, Loss: 0.3518963158130646, Final Batch Loss: 0.1441928595304489\n",
      "Epoch 3171, Loss: 0.3059706538915634, Final Batch Loss: 0.13616882264614105\n",
      "Epoch 3172, Loss: 0.2517736181616783, Final Batch Loss: 0.1101902648806572\n",
      "Epoch 3173, Loss: 0.2811865210533142, Final Batch Loss: 0.14336206018924713\n",
      "Epoch 3174, Loss: 0.2626829966902733, Final Batch Loss: 0.12441536039113998\n",
      "Epoch 3175, Loss: 0.28698983788490295, Final Batch Loss: 0.15329839289188385\n",
      "Epoch 3176, Loss: 0.3345903009176254, Final Batch Loss: 0.1519850790500641\n",
      "Epoch 3177, Loss: 0.2695707380771637, Final Batch Loss: 0.07001300156116486\n",
      "Epoch 3178, Loss: 0.30744974315166473, Final Batch Loss: 0.15971407294273376\n",
      "Epoch 3179, Loss: 0.2824833542108536, Final Batch Loss: 0.12854303419589996\n",
      "Epoch 3180, Loss: 0.2725434899330139, Final Batch Loss: 0.1433252990245819\n",
      "Epoch 3181, Loss: 0.31195099651813507, Final Batch Loss: 0.18276245892047882\n",
      "Epoch 3182, Loss: 0.2801855653524399, Final Batch Loss: 0.13766323029994965\n",
      "Epoch 3183, Loss: 0.3565989285707474, Final Batch Loss: 0.17679879069328308\n",
      "Epoch 3184, Loss: 0.34475041925907135, Final Batch Loss: 0.16015532612800598\n",
      "Epoch 3185, Loss: 0.3144567012786865, Final Batch Loss: 0.15726713836193085\n",
      "Epoch 3186, Loss: 0.29389210790395737, Final Batch Loss: 0.17468079924583435\n",
      "Epoch 3187, Loss: 0.32563816010951996, Final Batch Loss: 0.14255204796791077\n",
      "Epoch 3188, Loss: 0.2596995607018471, Final Batch Loss: 0.12350230664014816\n",
      "Epoch 3189, Loss: 0.35426586866378784, Final Batch Loss: 0.20453684031963348\n",
      "Epoch 3190, Loss: 0.3121568262577057, Final Batch Loss: 0.1402444690465927\n",
      "Epoch 3191, Loss: 0.2713143527507782, Final Batch Loss: 0.1428634077310562\n",
      "Epoch 3192, Loss: 0.3368627578020096, Final Batch Loss: 0.16775034368038177\n",
      "Epoch 3193, Loss: 0.3467542380094528, Final Batch Loss: 0.21714729070663452\n",
      "Epoch 3194, Loss: 0.25807957351207733, Final Batch Loss: 0.12257380783557892\n",
      "Epoch 3195, Loss: 0.21535002440214157, Final Batch Loss: 0.08901684731245041\n",
      "Epoch 3196, Loss: 0.257578581571579, Final Batch Loss: 0.0825423002243042\n",
      "Epoch 3197, Loss: 0.24883634597063065, Final Batch Loss: 0.12826497852802277\n",
      "Epoch 3198, Loss: 0.25601624697446823, Final Batch Loss: 0.11603721231222153\n",
      "Epoch 3199, Loss: 0.26425914466381073, Final Batch Loss: 0.1319892406463623\n",
      "Epoch 3200, Loss: 0.3650408685207367, Final Batch Loss: 0.15241210162639618\n",
      "Epoch 3201, Loss: 0.3797439783811569, Final Batch Loss: 0.2629629671573639\n",
      "Epoch 3202, Loss: 0.2896415889263153, Final Batch Loss: 0.1419244259595871\n",
      "Epoch 3203, Loss: 0.3122359961271286, Final Batch Loss: 0.1492679864168167\n",
      "Epoch 3204, Loss: 0.3570401817560196, Final Batch Loss: 0.1648915559053421\n",
      "Epoch 3205, Loss: 0.2823193669319153, Final Batch Loss: 0.1579653024673462\n",
      "Epoch 3206, Loss: 0.24981413036584854, Final Batch Loss: 0.11822255700826645\n",
      "Epoch 3207, Loss: 0.30456916987895966, Final Batch Loss: 0.15951447188854218\n",
      "Epoch 3208, Loss: 0.27910785377025604, Final Batch Loss: 0.11787253618240356\n",
      "Epoch 3209, Loss: 0.322643980383873, Final Batch Loss: 0.19282789528369904\n",
      "Epoch 3210, Loss: 0.3197516202926636, Final Batch Loss: 0.15279902517795563\n",
      "Epoch 3211, Loss: 0.27046841382980347, Final Batch Loss: 0.14754244685173035\n",
      "Epoch 3212, Loss: 0.3112877458333969, Final Batch Loss: 0.17040178179740906\n",
      "Epoch 3213, Loss: 0.26626400649547577, Final Batch Loss: 0.13334070146083832\n",
      "Epoch 3214, Loss: 0.27742479741573334, Final Batch Loss: 0.10929034650325775\n",
      "Epoch 3215, Loss: 0.30873705446720123, Final Batch Loss: 0.12598936259746552\n",
      "Epoch 3216, Loss: 0.3336640074849129, Final Batch Loss: 0.21904508769512177\n",
      "Epoch 3217, Loss: 0.278987854719162, Final Batch Loss: 0.154868021607399\n",
      "Epoch 3218, Loss: 0.2633284255862236, Final Batch Loss: 0.11588221043348312\n",
      "Epoch 3219, Loss: 0.30909115076065063, Final Batch Loss: 0.14778423309326172\n",
      "Epoch 3220, Loss: 0.29862555861473083, Final Batch Loss: 0.15392738580703735\n",
      "Epoch 3221, Loss: 0.3222261369228363, Final Batch Loss: 0.17083092033863068\n",
      "Epoch 3222, Loss: 0.25326449424028397, Final Batch Loss: 0.11982066184282303\n",
      "Epoch 3223, Loss: 0.3344874680042267, Final Batch Loss: 0.2113858014345169\n",
      "Epoch 3224, Loss: 0.29665298759937286, Final Batch Loss: 0.15455351769924164\n",
      "Epoch 3225, Loss: 0.271022766828537, Final Batch Loss: 0.1020471453666687\n",
      "Epoch 3226, Loss: 0.2829565554857254, Final Batch Loss: 0.15086831152439117\n",
      "Epoch 3227, Loss: 0.2948402762413025, Final Batch Loss: 0.13814851641654968\n",
      "Epoch 3228, Loss: 0.2991238832473755, Final Batch Loss: 0.1598459631204605\n",
      "Epoch 3229, Loss: 0.3369757831096649, Final Batch Loss: 0.1724710762500763\n",
      "Epoch 3230, Loss: 0.2926010936498642, Final Batch Loss: 0.13666272163391113\n",
      "Epoch 3231, Loss: 0.24995927512645721, Final Batch Loss: 0.09588171541690826\n",
      "Epoch 3232, Loss: 0.2910129278898239, Final Batch Loss: 0.12852761149406433\n",
      "Epoch 3233, Loss: 0.30789555609226227, Final Batch Loss: 0.1888616979122162\n",
      "Epoch 3234, Loss: 0.3197494447231293, Final Batch Loss: 0.15738868713378906\n",
      "Epoch 3235, Loss: 0.27996157109737396, Final Batch Loss: 0.12923309206962585\n",
      "Epoch 3236, Loss: 0.29196687042713165, Final Batch Loss: 0.14969424903392792\n",
      "Epoch 3237, Loss: 0.2826457917690277, Final Batch Loss: 0.12116013467311859\n",
      "Epoch 3238, Loss: 0.27859823405742645, Final Batch Loss: 0.13299013674259186\n",
      "Epoch 3239, Loss: 0.3174375295639038, Final Batch Loss: 0.1621730923652649\n",
      "Epoch 3240, Loss: 0.24406491219997406, Final Batch Loss: 0.12776221334934235\n",
      "Epoch 3241, Loss: 0.32883521914482117, Final Batch Loss: 0.17525365948677063\n",
      "Epoch 3242, Loss: 0.32880645990371704, Final Batch Loss: 0.19644930958747864\n",
      "Epoch 3243, Loss: 0.34037210047245026, Final Batch Loss: 0.13919422030448914\n",
      "Epoch 3244, Loss: 0.28172123432159424, Final Batch Loss: 0.15446138381958008\n",
      "Epoch 3245, Loss: 0.2749401032924652, Final Batch Loss: 0.15171223878860474\n",
      "Epoch 3246, Loss: 0.34161505103111267, Final Batch Loss: 0.19830438494682312\n",
      "Epoch 3247, Loss: 0.285301998257637, Final Batch Loss: 0.14419667422771454\n",
      "Epoch 3248, Loss: 0.2827884554862976, Final Batch Loss: 0.1490859091281891\n",
      "Epoch 3249, Loss: 0.24109333008527756, Final Batch Loss: 0.10472003370523453\n",
      "Epoch 3250, Loss: 0.3108488917350769, Final Batch Loss: 0.1711326539516449\n",
      "Epoch 3251, Loss: 0.3232019543647766, Final Batch Loss: 0.18479394912719727\n",
      "Epoch 3252, Loss: 0.25047140568494797, Final Batch Loss: 0.10462140291929245\n",
      "Epoch 3253, Loss: 0.2757558822631836, Final Batch Loss: 0.13931876420974731\n",
      "Epoch 3254, Loss: 0.3762129843235016, Final Batch Loss: 0.22382110357284546\n",
      "Epoch 3255, Loss: 0.3519073575735092, Final Batch Loss: 0.2166208028793335\n",
      "Epoch 3256, Loss: 0.3037513643503189, Final Batch Loss: 0.146802619099617\n",
      "Epoch 3257, Loss: 0.3494050204753876, Final Batch Loss: 0.20453399419784546\n",
      "Epoch 3258, Loss: 0.2888210564851761, Final Batch Loss: 0.12602180242538452\n",
      "Epoch 3259, Loss: 0.31885001063346863, Final Batch Loss: 0.15964385867118835\n",
      "Epoch 3260, Loss: 0.25955408811569214, Final Batch Loss: 0.12885700166225433\n",
      "Epoch 3261, Loss: 0.2693597674369812, Final Batch Loss: 0.12955674529075623\n",
      "Epoch 3262, Loss: 0.2795368805527687, Final Batch Loss: 0.1706712543964386\n",
      "Epoch 3263, Loss: 0.33899782598018646, Final Batch Loss: 0.2210882008075714\n",
      "Epoch 3264, Loss: 0.28356021642684937, Final Batch Loss: 0.13994689285755157\n",
      "Epoch 3265, Loss: 0.29554958641529083, Final Batch Loss: 0.14872457087039948\n",
      "Epoch 3266, Loss: 0.3587755560874939, Final Batch Loss: 0.17451950907707214\n",
      "Epoch 3267, Loss: 0.29662080109119415, Final Batch Loss: 0.1285475641489029\n",
      "Epoch 3268, Loss: 0.2687279284000397, Final Batch Loss: 0.1358921378850937\n",
      "Epoch 3269, Loss: 0.30609211325645447, Final Batch Loss: 0.1414012461900711\n",
      "Epoch 3270, Loss: 0.34902164340019226, Final Batch Loss: 0.1989891678094864\n",
      "Epoch 3271, Loss: 0.3518354445695877, Final Batch Loss: 0.20558129251003265\n",
      "Epoch 3272, Loss: 0.3147413432598114, Final Batch Loss: 0.1351049840450287\n",
      "Epoch 3273, Loss: 0.3005078434944153, Final Batch Loss: 0.13398976624011993\n",
      "Epoch 3274, Loss: 0.26264945417642593, Final Batch Loss: 0.14127671718597412\n",
      "Epoch 3275, Loss: 0.27719680219888687, Final Batch Loss: 0.1605720818042755\n",
      "Epoch 3276, Loss: 0.30337563157081604, Final Batch Loss: 0.14472560584545135\n",
      "Epoch 3277, Loss: 0.3052438497543335, Final Batch Loss: 0.15498898923397064\n",
      "Epoch 3278, Loss: 0.3322286307811737, Final Batch Loss: 0.15462973713874817\n",
      "Epoch 3279, Loss: 0.2821468412876129, Final Batch Loss: 0.1176750659942627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3280, Loss: 0.23639419674873352, Final Batch Loss: 0.11895039677619934\n",
      "Epoch 3281, Loss: 0.24752720445394516, Final Batch Loss: 0.10599713772535324\n",
      "Epoch 3282, Loss: 0.26931843161582947, Final Batch Loss: 0.12949340045452118\n",
      "Epoch 3283, Loss: 0.2629581391811371, Final Batch Loss: 0.13637812435626984\n",
      "Epoch 3284, Loss: 0.32539473474025726, Final Batch Loss: 0.14792391657829285\n",
      "Epoch 3285, Loss: 0.3021342605352402, Final Batch Loss: 0.1609208732843399\n",
      "Epoch 3286, Loss: 0.28726379573345184, Final Batch Loss: 0.14966976642608643\n",
      "Epoch 3287, Loss: 0.27695605158805847, Final Batch Loss: 0.14526891708374023\n",
      "Epoch 3288, Loss: 0.4191923886537552, Final Batch Loss: 0.16808180510997772\n",
      "Epoch 3289, Loss: 0.24257543683052063, Final Batch Loss: 0.10899819433689117\n",
      "Epoch 3290, Loss: 0.2574678808450699, Final Batch Loss: 0.11246481537818909\n",
      "Epoch 3291, Loss: 0.24097856134176254, Final Batch Loss: 0.10627802461385727\n",
      "Epoch 3292, Loss: 0.28968629240989685, Final Batch Loss: 0.13754333555698395\n",
      "Epoch 3293, Loss: 0.30644814670085907, Final Batch Loss: 0.13807737827301025\n",
      "Epoch 3294, Loss: 0.42005755007267, Final Batch Loss: 0.2195742279291153\n",
      "Epoch 3295, Loss: 0.3409842848777771, Final Batch Loss: 0.1387273222208023\n",
      "Epoch 3296, Loss: 0.2877030596137047, Final Batch Loss: 0.11967738717794418\n",
      "Epoch 3297, Loss: 0.2786382734775543, Final Batch Loss: 0.1521224081516266\n",
      "Epoch 3298, Loss: 0.3045776039361954, Final Batch Loss: 0.1630733758211136\n",
      "Epoch 3299, Loss: 0.3546998202800751, Final Batch Loss: 0.20038017630577087\n",
      "Epoch 3300, Loss: 0.2761550545692444, Final Batch Loss: 0.1434135138988495\n",
      "Epoch 3301, Loss: 0.30695323646068573, Final Batch Loss: 0.15205815434455872\n",
      "Epoch 3302, Loss: 0.27188267558813095, Final Batch Loss: 0.1183084025979042\n",
      "Epoch 3303, Loss: 0.24124238640069962, Final Batch Loss: 0.09234417229890823\n",
      "Epoch 3304, Loss: 0.26845620572566986, Final Batch Loss: 0.11461193859577179\n",
      "Epoch 3305, Loss: 0.32720811665058136, Final Batch Loss: 0.1908431351184845\n",
      "Epoch 3306, Loss: 0.31011147797107697, Final Batch Loss: 0.16880474984645844\n",
      "Epoch 3307, Loss: 0.3131624162197113, Final Batch Loss: 0.15604524314403534\n",
      "Epoch 3308, Loss: 0.36413635313510895, Final Batch Loss: 0.19081781804561615\n",
      "Epoch 3309, Loss: 0.30430130660533905, Final Batch Loss: 0.16208100318908691\n",
      "Epoch 3310, Loss: 0.3062921464443207, Final Batch Loss: 0.16744399070739746\n",
      "Epoch 3311, Loss: 0.21875297278165817, Final Batch Loss: 0.07926654070615768\n",
      "Epoch 3312, Loss: 0.3152521103620529, Final Batch Loss: 0.15459991991519928\n",
      "Epoch 3313, Loss: 0.28315603733062744, Final Batch Loss: 0.17442962527275085\n",
      "Epoch 3314, Loss: 0.3313850909471512, Final Batch Loss: 0.16809046268463135\n",
      "Epoch 3315, Loss: 0.2960817217826843, Final Batch Loss: 0.11020827293395996\n",
      "Epoch 3316, Loss: 0.2864072620868683, Final Batch Loss: 0.11925797164440155\n",
      "Epoch 3317, Loss: 0.2835875451564789, Final Batch Loss: 0.13211573660373688\n",
      "Epoch 3318, Loss: 0.27723247557878494, Final Batch Loss: 0.11773128062486649\n",
      "Epoch 3319, Loss: 0.2521071583032608, Final Batch Loss: 0.08125784993171692\n",
      "Epoch 3320, Loss: 0.27287016808986664, Final Batch Loss: 0.13934744894504547\n",
      "Epoch 3321, Loss: 0.2538275495171547, Final Batch Loss: 0.12341208010911942\n",
      "Epoch 3322, Loss: 0.25294002145528793, Final Batch Loss: 0.13392721116542816\n",
      "Epoch 3323, Loss: 0.28731900453567505, Final Batch Loss: 0.1415913999080658\n",
      "Epoch 3324, Loss: 0.32968974113464355, Final Batch Loss: 0.1531781703233719\n",
      "Epoch 3325, Loss: 0.25032589584589005, Final Batch Loss: 0.12633055448532104\n",
      "Epoch 3326, Loss: 0.29989196360111237, Final Batch Loss: 0.15737153589725494\n",
      "Epoch 3327, Loss: 0.2768578603863716, Final Batch Loss: 0.16869935393333435\n",
      "Epoch 3328, Loss: 0.29995495080947876, Final Batch Loss: 0.16755525767803192\n",
      "Epoch 3329, Loss: 0.2835392355918884, Final Batch Loss: 0.15235018730163574\n",
      "Epoch 3330, Loss: 0.2965739071369171, Final Batch Loss: 0.15850867331027985\n",
      "Epoch 3331, Loss: 0.30624496936798096, Final Batch Loss: 0.13890521228313446\n",
      "Epoch 3332, Loss: 0.37357623875141144, Final Batch Loss: 0.2267404943704605\n",
      "Epoch 3333, Loss: 0.24559876322746277, Final Batch Loss: 0.1076778918504715\n",
      "Epoch 3334, Loss: 0.2458096444606781, Final Batch Loss: 0.10060855746269226\n",
      "Epoch 3335, Loss: 0.24074099957942963, Final Batch Loss: 0.09797997772693634\n",
      "Epoch 3336, Loss: 0.3194281607866287, Final Batch Loss: 0.18643221259117126\n",
      "Epoch 3337, Loss: 0.30280129611492157, Final Batch Loss: 0.13172359764575958\n",
      "Epoch 3338, Loss: 0.30213814973831177, Final Batch Loss: 0.14304304122924805\n",
      "Epoch 3339, Loss: 0.24804307520389557, Final Batch Loss: 0.0912380963563919\n",
      "Epoch 3340, Loss: 0.40059250593185425, Final Batch Loss: 0.25243914127349854\n",
      "Epoch 3341, Loss: 0.25546181201934814, Final Batch Loss: 0.1273549646139145\n",
      "Epoch 3342, Loss: 0.3066263198852539, Final Batch Loss: 0.14446485042572021\n",
      "Epoch 3343, Loss: 0.32172565162181854, Final Batch Loss: 0.1323954313993454\n",
      "Epoch 3344, Loss: 0.3695528358221054, Final Batch Loss: 0.21573388576507568\n",
      "Epoch 3345, Loss: 0.25600671023130417, Final Batch Loss: 0.1492442488670349\n",
      "Epoch 3346, Loss: 0.3891453593969345, Final Batch Loss: 0.2526111900806427\n",
      "Epoch 3347, Loss: 0.2915862649679184, Final Batch Loss: 0.1403697431087494\n",
      "Epoch 3348, Loss: 0.24286001920700073, Final Batch Loss: 0.1032065600156784\n",
      "Epoch 3349, Loss: 0.2980915457010269, Final Batch Loss: 0.1630145162343979\n",
      "Epoch 3350, Loss: 0.34281864762306213, Final Batch Loss: 0.17512045800685883\n",
      "Epoch 3351, Loss: 0.27036454528570175, Final Batch Loss: 0.11881797760725021\n",
      "Epoch 3352, Loss: 0.31752628087997437, Final Batch Loss: 0.17381364107131958\n",
      "Epoch 3353, Loss: 0.294538214802742, Final Batch Loss: 0.16166318953037262\n",
      "Epoch 3354, Loss: 0.3236421197652817, Final Batch Loss: 0.19308175146579742\n",
      "Epoch 3355, Loss: 0.31843310594558716, Final Batch Loss: 0.17538167536258698\n",
      "Epoch 3356, Loss: 0.3150574713945389, Final Batch Loss: 0.17300905287265778\n",
      "Epoch 3357, Loss: 0.29640132188796997, Final Batch Loss: 0.12594842910766602\n",
      "Epoch 3358, Loss: 0.2837228924036026, Final Batch Loss: 0.15219657123088837\n",
      "Epoch 3359, Loss: 0.2850738912820816, Final Batch Loss: 0.16511881351470947\n",
      "Epoch 3360, Loss: 0.36200961470603943, Final Batch Loss: 0.19887825846672058\n",
      "Epoch 3361, Loss: 0.29853808879852295, Final Batch Loss: 0.19576461613178253\n",
      "Epoch 3362, Loss: 0.31420329958200455, Final Batch Loss: 0.11516781896352768\n",
      "Epoch 3363, Loss: 0.28363150358200073, Final Batch Loss: 0.11756911873817444\n",
      "Epoch 3364, Loss: 0.29951316118240356, Final Batch Loss: 0.16495195031166077\n",
      "Epoch 3365, Loss: 0.24821221083402634, Final Batch Loss: 0.10709591954946518\n",
      "Epoch 3366, Loss: 0.25242459028959274, Final Batch Loss: 0.13809587061405182\n",
      "Epoch 3367, Loss: 0.32538457214832306, Final Batch Loss: 0.19394902884960175\n",
      "Epoch 3368, Loss: 0.2569490522146225, Final Batch Loss: 0.09036839008331299\n",
      "Epoch 3369, Loss: 0.33226922154426575, Final Batch Loss: 0.17565315961837769\n",
      "Epoch 3370, Loss: 0.21457523852586746, Final Batch Loss: 0.08104550093412399\n",
      "Epoch 3371, Loss: 0.3460764139890671, Final Batch Loss: 0.19290918111801147\n",
      "Epoch 3372, Loss: 0.2821741849184036, Final Batch Loss: 0.13279478251934052\n",
      "Epoch 3373, Loss: 0.25170568376779556, Final Batch Loss: 0.1100519672036171\n",
      "Epoch 3374, Loss: 0.30285629630088806, Final Batch Loss: 0.14094461500644684\n",
      "Epoch 3375, Loss: 0.40352876484394073, Final Batch Loss: 0.174343541264534\n",
      "Epoch 3376, Loss: 0.33482006192207336, Final Batch Loss: 0.17113465070724487\n",
      "Epoch 3377, Loss: 0.3049001693725586, Final Batch Loss: 0.16218435764312744\n",
      "Epoch 3378, Loss: 0.2803604528307915, Final Batch Loss: 0.17375123500823975\n",
      "Epoch 3379, Loss: 0.2673956900835037, Final Batch Loss: 0.13492578268051147\n",
      "Epoch 3380, Loss: 0.24742133170366287, Final Batch Loss: 0.1067223772406578\n",
      "Epoch 3381, Loss: 0.25955262780189514, Final Batch Loss: 0.09530556201934814\n",
      "Epoch 3382, Loss: 0.28393425792455673, Final Batch Loss: 0.11643078178167343\n",
      "Epoch 3383, Loss: 0.3113817125558853, Final Batch Loss: 0.12654298543930054\n",
      "Epoch 3384, Loss: 0.366582415997982, Final Batch Loss: 0.24293971061706543\n",
      "Epoch 3385, Loss: 0.3062945455312729, Final Batch Loss: 0.15091995894908905\n",
      "Epoch 3386, Loss: 0.22864317148923874, Final Batch Loss: 0.08694205433130264\n",
      "Epoch 3387, Loss: 0.35457828640937805, Final Batch Loss: 0.16552266478538513\n",
      "Epoch 3388, Loss: 0.32809676229953766, Final Batch Loss: 0.14189501106739044\n",
      "Epoch 3389, Loss: 0.28823524713516235, Final Batch Loss: 0.14766287803649902\n",
      "Epoch 3390, Loss: 0.33904993534088135, Final Batch Loss: 0.17822182178497314\n",
      "Epoch 3391, Loss: 0.252299427986145, Final Batch Loss: 0.1072295755147934\n",
      "Epoch 3392, Loss: 0.2765325903892517, Final Batch Loss: 0.1423255354166031\n",
      "Epoch 3393, Loss: 0.2793838679790497, Final Batch Loss: 0.13336943089962006\n",
      "Epoch 3394, Loss: 0.2571380287408829, Final Batch Loss: 0.09507915377616882\n",
      "Epoch 3395, Loss: 0.33452197909355164, Final Batch Loss: 0.14519070088863373\n",
      "Epoch 3396, Loss: 0.2398511990904808, Final Batch Loss: 0.09060574322938919\n",
      "Epoch 3397, Loss: 0.28307050466537476, Final Batch Loss: 0.14612233638763428\n",
      "Epoch 3398, Loss: 0.2700735777616501, Final Batch Loss: 0.11623294651508331\n",
      "Epoch 3399, Loss: 0.2643727660179138, Final Batch Loss: 0.1388757973909378\n",
      "Epoch 3400, Loss: 0.2941773012280464, Final Batch Loss: 0.11940193921327591\n",
      "Epoch 3401, Loss: 0.3209133595228195, Final Batch Loss: 0.18305450677871704\n",
      "Epoch 3402, Loss: 0.3474796563386917, Final Batch Loss: 0.15290623903274536\n",
      "Epoch 3403, Loss: 0.3584352135658264, Final Batch Loss: 0.19240610301494598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3404, Loss: 0.33588235080242157, Final Batch Loss: 0.15362881124019623\n",
      "Epoch 3405, Loss: 0.2783961743116379, Final Batch Loss: 0.13642895221710205\n",
      "Epoch 3406, Loss: 0.33144114911556244, Final Batch Loss: 0.17105865478515625\n",
      "Epoch 3407, Loss: 0.2704475000500679, Final Batch Loss: 0.07373148947954178\n",
      "Epoch 3408, Loss: 0.2991870194673538, Final Batch Loss: 0.11270219087600708\n",
      "Epoch 3409, Loss: 0.33045344054698944, Final Batch Loss: 0.12494975328445435\n",
      "Epoch 3410, Loss: 0.2758510559797287, Final Batch Loss: 0.09364917874336243\n",
      "Epoch 3411, Loss: 0.4091433137655258, Final Batch Loss: 0.24788866937160492\n",
      "Epoch 3412, Loss: 0.41130800545215607, Final Batch Loss: 0.2866382598876953\n",
      "Epoch 3413, Loss: 0.29188790917396545, Final Batch Loss: 0.1629943996667862\n",
      "Epoch 3414, Loss: 0.2972313016653061, Final Batch Loss: 0.15557436645030975\n",
      "Epoch 3415, Loss: 0.2597084417939186, Final Batch Loss: 0.13854655623435974\n",
      "Epoch 3416, Loss: 0.3504639118909836, Final Batch Loss: 0.16440369188785553\n",
      "Epoch 3417, Loss: 0.3361116945743561, Final Batch Loss: 0.1918567419052124\n",
      "Epoch 3418, Loss: 0.3230571746826172, Final Batch Loss: 0.16468296945095062\n",
      "Epoch 3419, Loss: 0.2544715702533722, Final Batch Loss: 0.0923231840133667\n",
      "Epoch 3420, Loss: 0.31249628961086273, Final Batch Loss: 0.13689976930618286\n",
      "Epoch 3421, Loss: 0.340960755944252, Final Batch Loss: 0.17151761054992676\n",
      "Epoch 3422, Loss: 0.30843089520931244, Final Batch Loss: 0.1397649198770523\n",
      "Epoch 3423, Loss: 0.34554724395275116, Final Batch Loss: 0.19658011198043823\n",
      "Epoch 3424, Loss: 0.30966657400131226, Final Batch Loss: 0.14197029173374176\n",
      "Epoch 3425, Loss: 0.27657680213451385, Final Batch Loss: 0.12556131184101105\n",
      "Epoch 3426, Loss: 0.2368369922041893, Final Batch Loss: 0.10192761570215225\n",
      "Epoch 3427, Loss: 0.3459957093000412, Final Batch Loss: 0.1602390557527542\n",
      "Epoch 3428, Loss: 0.34885933995246887, Final Batch Loss: 0.20567429065704346\n",
      "Epoch 3429, Loss: 0.3438335806131363, Final Batch Loss: 0.19563916325569153\n",
      "Epoch 3430, Loss: 0.2613004893064499, Final Batch Loss: 0.14111857116222382\n",
      "Epoch 3431, Loss: 0.2777499407529831, Final Batch Loss: 0.1333020180463791\n",
      "Epoch 3432, Loss: 0.20015468448400497, Final Batch Loss: 0.061712153255939484\n",
      "Epoch 3433, Loss: 0.3671582490205765, Final Batch Loss: 0.18860329687595367\n",
      "Epoch 3434, Loss: 0.24305489659309387, Final Batch Loss: 0.08902314305305481\n",
      "Epoch 3435, Loss: 0.2767437919974327, Final Batch Loss: 0.15858054161071777\n",
      "Epoch 3436, Loss: 0.33618465065956116, Final Batch Loss: 0.1787802129983902\n",
      "Epoch 3437, Loss: 0.27835075557231903, Final Batch Loss: 0.13006989657878876\n",
      "Epoch 3438, Loss: 0.3523402363061905, Final Batch Loss: 0.219674751162529\n",
      "Epoch 3439, Loss: 0.37270887196063995, Final Batch Loss: 0.24434830248355865\n",
      "Epoch 3440, Loss: 0.3378010392189026, Final Batch Loss: 0.1788753718137741\n",
      "Epoch 3441, Loss: 0.27915041893720627, Final Batch Loss: 0.15581415593624115\n",
      "Epoch 3442, Loss: 0.29855719208717346, Final Batch Loss: 0.15008847415447235\n",
      "Epoch 3443, Loss: 0.2934522181749344, Final Batch Loss: 0.15704673528671265\n",
      "Epoch 3444, Loss: 0.2516336441040039, Final Batch Loss: 0.12259311974048615\n",
      "Epoch 3445, Loss: 0.3130755126476288, Final Batch Loss: 0.16433550417423248\n",
      "Epoch 3446, Loss: 0.2725655734539032, Final Batch Loss: 0.12970946729183197\n",
      "Epoch 3447, Loss: 0.356273889541626, Final Batch Loss: 0.2029515653848648\n",
      "Epoch 3448, Loss: 0.2858484089374542, Final Batch Loss: 0.12609608471393585\n",
      "Epoch 3449, Loss: 0.25316283851861954, Final Batch Loss: 0.10654238611459732\n",
      "Epoch 3450, Loss: 0.3014206141233444, Final Batch Loss: 0.14817598462104797\n",
      "Epoch 3451, Loss: 0.30585817992687225, Final Batch Loss: 0.11052998900413513\n",
      "Epoch 3452, Loss: 0.2744811177253723, Final Batch Loss: 0.14462828636169434\n",
      "Epoch 3453, Loss: 0.33760346472263336, Final Batch Loss: 0.177712544798851\n",
      "Epoch 3454, Loss: 0.3221272975206375, Final Batch Loss: 0.20694464445114136\n",
      "Epoch 3455, Loss: 0.2865985333919525, Final Batch Loss: 0.1354074329137802\n",
      "Epoch 3456, Loss: 0.32438988983631134, Final Batch Loss: 0.17099156975746155\n",
      "Epoch 3457, Loss: 0.22963211685419083, Final Batch Loss: 0.12077950686216354\n",
      "Epoch 3458, Loss: 0.29408854246139526, Final Batch Loss: 0.1438131183385849\n",
      "Epoch 3459, Loss: 0.2917436808347702, Final Batch Loss: 0.1467101126909256\n",
      "Epoch 3460, Loss: 0.30927422642707825, Final Batch Loss: 0.148298978805542\n",
      "Epoch 3461, Loss: 0.38167810440063477, Final Batch Loss: 0.25086840987205505\n",
      "Epoch 3462, Loss: 0.35418783128261566, Final Batch Loss: 0.21374334394931793\n",
      "Epoch 3463, Loss: 0.290378600358963, Final Batch Loss: 0.11787918210029602\n",
      "Epoch 3464, Loss: 0.310692623257637, Final Batch Loss: 0.16996900737285614\n",
      "Epoch 3465, Loss: 0.3812404274940491, Final Batch Loss: 0.18843336403369904\n",
      "Epoch 3466, Loss: 0.3315441608428955, Final Batch Loss: 0.18023483455181122\n",
      "Epoch 3467, Loss: 0.3460727035999298, Final Batch Loss: 0.20292890071868896\n",
      "Epoch 3468, Loss: 0.25706620514392853, Final Batch Loss: 0.12721246480941772\n",
      "Epoch 3469, Loss: 0.3140277862548828, Final Batch Loss: 0.16928520798683167\n",
      "Epoch 3470, Loss: 0.3243225961923599, Final Batch Loss: 0.12810258567333221\n",
      "Epoch 3471, Loss: 0.46097978949546814, Final Batch Loss: 0.2684727907180786\n",
      "Epoch 3472, Loss: 0.2676907479763031, Final Batch Loss: 0.11408115923404694\n",
      "Epoch 3473, Loss: 0.27773434668779373, Final Batch Loss: 0.1551883965730667\n",
      "Epoch 3474, Loss: 0.3387245088815689, Final Batch Loss: 0.1750131994485855\n",
      "Epoch 3475, Loss: 0.2921498939394951, Final Batch Loss: 0.16818252205848694\n",
      "Epoch 3476, Loss: 0.3348138779401779, Final Batch Loss: 0.17488156259059906\n",
      "Epoch 3477, Loss: 0.35101474076509476, Final Batch Loss: 0.11726223677396774\n",
      "Epoch 3478, Loss: 0.27074313163757324, Final Batch Loss: 0.11871641874313354\n",
      "Epoch 3479, Loss: 0.2699817270040512, Final Batch Loss: 0.132298082113266\n",
      "Epoch 3480, Loss: 0.32075484097003937, Final Batch Loss: 0.14565055072307587\n",
      "Epoch 3481, Loss: 0.29383549094200134, Final Batch Loss: 0.1351613700389862\n",
      "Epoch 3482, Loss: 0.2769535481929779, Final Batch Loss: 0.1269972026348114\n",
      "Epoch 3483, Loss: 0.2669772356748581, Final Batch Loss: 0.1319749504327774\n",
      "Epoch 3484, Loss: 0.367363378405571, Final Batch Loss: 0.14432267844676971\n",
      "Epoch 3485, Loss: 0.22194544970989227, Final Batch Loss: 0.08178092539310455\n",
      "Epoch 3486, Loss: 0.3061414062976837, Final Batch Loss: 0.14098960161209106\n",
      "Epoch 3487, Loss: 0.30278223752975464, Final Batch Loss: 0.1525142788887024\n",
      "Epoch 3488, Loss: 0.2902596443891525, Final Batch Loss: 0.1264328807592392\n",
      "Epoch 3489, Loss: 0.30267660319805145, Final Batch Loss: 0.14452287554740906\n",
      "Epoch 3490, Loss: 0.3147270828485489, Final Batch Loss: 0.17361877858638763\n",
      "Epoch 3491, Loss: 0.26704496890306473, Final Batch Loss: 0.11951851099729538\n",
      "Epoch 3492, Loss: 0.26796088367700577, Final Batch Loss: 0.1128978356719017\n",
      "Epoch 3493, Loss: 0.32013241946697235, Final Batch Loss: 0.16903603076934814\n",
      "Epoch 3494, Loss: 0.28137100487947464, Final Batch Loss: 0.16070719063282013\n",
      "Epoch 3495, Loss: 0.3190487027168274, Final Batch Loss: 0.2051740288734436\n",
      "Epoch 3496, Loss: 0.3707125335931778, Final Batch Loss: 0.20399603247642517\n",
      "Epoch 3497, Loss: 0.23943540453910828, Final Batch Loss: 0.09707653522491455\n",
      "Epoch 3498, Loss: 0.25326142460107803, Final Batch Loss: 0.09980509430170059\n",
      "Epoch 3499, Loss: 0.34213748574256897, Final Batch Loss: 0.2050754725933075\n",
      "Epoch 3500, Loss: 0.27118150144815445, Final Batch Loss: 0.11589445918798447\n",
      "Epoch 3501, Loss: 0.30745959281921387, Final Batch Loss: 0.13543017208576202\n",
      "Epoch 3502, Loss: 0.2821805626153946, Final Batch Loss: 0.13131462037563324\n",
      "Epoch 3503, Loss: 0.27622221410274506, Final Batch Loss: 0.14383551478385925\n",
      "Epoch 3504, Loss: 0.2980647385120392, Final Batch Loss: 0.1563711017370224\n",
      "Epoch 3505, Loss: 0.3871552348136902, Final Batch Loss: 0.25809329748153687\n",
      "Epoch 3506, Loss: 0.2541790008544922, Final Batch Loss: 0.12890733778476715\n",
      "Epoch 3507, Loss: 0.308260053396225, Final Batch Loss: 0.15125077962875366\n",
      "Epoch 3508, Loss: 0.268525131046772, Final Batch Loss: 0.10939880460500717\n",
      "Epoch 3509, Loss: 0.3293135315179825, Final Batch Loss: 0.13312852382659912\n",
      "Epoch 3510, Loss: 0.2778325378894806, Final Batch Loss: 0.1413889229297638\n",
      "Epoch 3511, Loss: 0.26536591351032257, Final Batch Loss: 0.13503465056419373\n",
      "Epoch 3512, Loss: 0.22730351239442825, Final Batch Loss: 0.08992180973291397\n",
      "Epoch 3513, Loss: 0.26527153700590134, Final Batch Loss: 0.09327755123376846\n",
      "Epoch 3514, Loss: 0.3048528879880905, Final Batch Loss: 0.17462964355945587\n",
      "Epoch 3515, Loss: 0.34669043123722076, Final Batch Loss: 0.13290050625801086\n",
      "Epoch 3516, Loss: 0.23960740119218826, Final Batch Loss: 0.0977281704545021\n",
      "Epoch 3517, Loss: 0.30095329880714417, Final Batch Loss: 0.14232556521892548\n",
      "Epoch 3518, Loss: 0.2753538638353348, Final Batch Loss: 0.13270272314548492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3519, Loss: 0.22601012140512466, Final Batch Loss: 0.09812558442354202\n",
      "Epoch 3520, Loss: 0.34177081286907196, Final Batch Loss: 0.16246625781059265\n",
      "Epoch 3521, Loss: 0.3167015612125397, Final Batch Loss: 0.18837618827819824\n",
      "Epoch 3522, Loss: 0.3508540689945221, Final Batch Loss: 0.21274413168430328\n",
      "Epoch 3523, Loss: 0.3483567237854004, Final Batch Loss: 0.2008417546749115\n",
      "Epoch 3524, Loss: 0.29480697959661484, Final Batch Loss: 0.1815166473388672\n",
      "Epoch 3525, Loss: 0.2740190178155899, Final Batch Loss: 0.14009740948677063\n",
      "Epoch 3526, Loss: 0.28275322914123535, Final Batch Loss: 0.1272476762533188\n",
      "Epoch 3527, Loss: 0.26507518440485, Final Batch Loss: 0.11282656341791153\n",
      "Epoch 3528, Loss: 0.2922833263874054, Final Batch Loss: 0.12275360524654388\n",
      "Epoch 3529, Loss: 0.26556210964918137, Final Batch Loss: 0.07815913110971451\n",
      "Epoch 3530, Loss: 0.33986063301563263, Final Batch Loss: 0.1913158893585205\n",
      "Epoch 3531, Loss: 0.27449965476989746, Final Batch Loss: 0.12360022962093353\n",
      "Epoch 3532, Loss: 0.2607246935367584, Final Batch Loss: 0.1039026528596878\n",
      "Epoch 3533, Loss: 0.27820339798927307, Final Batch Loss: 0.1338243931531906\n",
      "Epoch 3534, Loss: 0.2632605731487274, Final Batch Loss: 0.14709414541721344\n",
      "Epoch 3535, Loss: 0.31765303015708923, Final Batch Loss: 0.17982983589172363\n",
      "Epoch 3536, Loss: 0.28366241604089737, Final Batch Loss: 0.11116913706064224\n",
      "Epoch 3537, Loss: 0.3107345998287201, Final Batch Loss: 0.13586370646953583\n",
      "Epoch 3538, Loss: 0.2544077858328819, Final Batch Loss: 0.13149474561214447\n",
      "Epoch 3539, Loss: 0.2849096953868866, Final Batch Loss: 0.14530491828918457\n",
      "Epoch 3540, Loss: 0.31209760904312134, Final Batch Loss: 0.1612251251935959\n",
      "Epoch 3541, Loss: 0.24356136471033096, Final Batch Loss: 0.10858704894781113\n",
      "Epoch 3542, Loss: 0.2758590579032898, Final Batch Loss: 0.13794133067131042\n",
      "Epoch 3543, Loss: 0.2700914070010185, Final Batch Loss: 0.14672353863716125\n",
      "Epoch 3544, Loss: 0.2804661840200424, Final Batch Loss: 0.16872628033161163\n",
      "Epoch 3545, Loss: 0.32214246690273285, Final Batch Loss: 0.1572539061307907\n",
      "Epoch 3546, Loss: 0.2557288259267807, Final Batch Loss: 0.10766749083995819\n",
      "Epoch 3547, Loss: 0.28064848482608795, Final Batch Loss: 0.1590743511915207\n",
      "Epoch 3548, Loss: 0.29371973872184753, Final Batch Loss: 0.14166684448719025\n",
      "Epoch 3549, Loss: 0.3518831431865692, Final Batch Loss: 0.2310991883277893\n",
      "Epoch 3550, Loss: 0.3086564242839813, Final Batch Loss: 0.1846817582845688\n",
      "Epoch 3551, Loss: 0.28325699269771576, Final Batch Loss: 0.1350361555814743\n",
      "Epoch 3552, Loss: 0.2908971831202507, Final Batch Loss: 0.11549172550439835\n",
      "Epoch 3553, Loss: 0.3361024707555771, Final Batch Loss: 0.1675543338060379\n",
      "Epoch 3554, Loss: 0.26728303730487823, Final Batch Loss: 0.13143011927604675\n",
      "Epoch 3555, Loss: 0.30547867715358734, Final Batch Loss: 0.16777558624744415\n",
      "Epoch 3556, Loss: 0.2763543799519539, Final Batch Loss: 0.11612708121538162\n",
      "Epoch 3557, Loss: 0.2528951168060303, Final Batch Loss: 0.11592796444892883\n",
      "Epoch 3558, Loss: 0.31517544388771057, Final Batch Loss: 0.14318852126598358\n",
      "Epoch 3559, Loss: 0.3923581391572952, Final Batch Loss: 0.27646535634994507\n",
      "Epoch 3560, Loss: 0.3094564974308014, Final Batch Loss: 0.14001315832138062\n",
      "Epoch 3561, Loss: 0.2720743715763092, Final Batch Loss: 0.1352917104959488\n",
      "Epoch 3562, Loss: 0.29575514793395996, Final Batch Loss: 0.12724529206752777\n",
      "Epoch 3563, Loss: 0.28274086117744446, Final Batch Loss: 0.1644916534423828\n",
      "Epoch 3564, Loss: 0.31072621047496796, Final Batch Loss: 0.15488232672214508\n",
      "Epoch 3565, Loss: 0.37483012676239014, Final Batch Loss: 0.25757861137390137\n",
      "Epoch 3566, Loss: 0.27015355229377747, Final Batch Loss: 0.13200469315052032\n",
      "Epoch 3567, Loss: 0.24375252425670624, Final Batch Loss: 0.10496899485588074\n",
      "Epoch 3568, Loss: 0.3324708938598633, Final Batch Loss: 0.14284467697143555\n",
      "Epoch 3569, Loss: 0.2531108483672142, Final Batch Loss: 0.13087990880012512\n",
      "Epoch 3570, Loss: 0.2590411528944969, Final Batch Loss: 0.10514355450868607\n",
      "Epoch 3571, Loss: 0.27550312876701355, Final Batch Loss: 0.14478619396686554\n",
      "Epoch 3572, Loss: 0.30877937376499176, Final Batch Loss: 0.14137482643127441\n",
      "Epoch 3573, Loss: 0.29069778323173523, Final Batch Loss: 0.1403871476650238\n",
      "Epoch 3574, Loss: 0.32283300161361694, Final Batch Loss: 0.18228989839553833\n",
      "Epoch 3575, Loss: 0.28624676167964935, Final Batch Loss: 0.146503746509552\n",
      "Epoch 3576, Loss: 0.2744378447532654, Final Batch Loss: 0.1407414674758911\n",
      "Epoch 3577, Loss: 0.2899046242237091, Final Batch Loss: 0.13769914209842682\n",
      "Epoch 3578, Loss: 0.3445664942264557, Final Batch Loss: 0.17979341745376587\n",
      "Epoch 3579, Loss: 0.25934409350156784, Final Batch Loss: 0.09355331212282181\n",
      "Epoch 3580, Loss: 0.3295668363571167, Final Batch Loss: 0.15129727125167847\n",
      "Epoch 3581, Loss: 0.3080732524394989, Final Batch Loss: 0.13532698154449463\n",
      "Epoch 3582, Loss: 0.3248421996831894, Final Batch Loss: 0.187877357006073\n",
      "Epoch 3583, Loss: 0.31872765719890594, Final Batch Loss: 0.17721977829933167\n",
      "Epoch 3584, Loss: 0.22118902206420898, Final Batch Loss: 0.0773758590221405\n",
      "Epoch 3585, Loss: 0.2661835253238678, Final Batch Loss: 0.1220538318157196\n",
      "Epoch 3586, Loss: 0.24023626744747162, Final Batch Loss: 0.11610183119773865\n",
      "Epoch 3587, Loss: 0.26051389425992966, Final Batch Loss: 0.09729557484388351\n",
      "Epoch 3588, Loss: 0.2845345288515091, Final Batch Loss: 0.18083877861499786\n",
      "Epoch 3589, Loss: 0.26197756826877594, Final Batch Loss: 0.14801989495754242\n",
      "Epoch 3590, Loss: 0.30022427439689636, Final Batch Loss: 0.14867627620697021\n",
      "Epoch 3591, Loss: 0.2822868227958679, Final Batch Loss: 0.13015833497047424\n",
      "Epoch 3592, Loss: 0.2695508152246475, Final Batch Loss: 0.12750516831874847\n",
      "Epoch 3593, Loss: 0.2831144481897354, Final Batch Loss: 0.15011286735534668\n",
      "Epoch 3594, Loss: 0.27574776113033295, Final Batch Loss: 0.16444620490074158\n",
      "Epoch 3595, Loss: 0.2571810260415077, Final Batch Loss: 0.15290968120098114\n",
      "Epoch 3596, Loss: 0.29161326587200165, Final Batch Loss: 0.12556329369544983\n",
      "Epoch 3597, Loss: 0.2639842927455902, Final Batch Loss: 0.12333478033542633\n",
      "Epoch 3598, Loss: 0.25977928936481476, Final Batch Loss: 0.15534192323684692\n",
      "Epoch 3599, Loss: 0.29622887074947357, Final Batch Loss: 0.13711033761501312\n",
      "Epoch 3600, Loss: 0.24272151291370392, Final Batch Loss: 0.1350860446691513\n",
      "Epoch 3601, Loss: 0.3481152653694153, Final Batch Loss: 0.23878113925457\n",
      "Epoch 3602, Loss: 0.2725057378411293, Final Batch Loss: 0.10827059298753738\n",
      "Epoch 3603, Loss: 0.33924993872642517, Final Batch Loss: 0.192589670419693\n",
      "Epoch 3604, Loss: 0.29352521896362305, Final Batch Loss: 0.15653541684150696\n",
      "Epoch 3605, Loss: 0.24120599031448364, Final Batch Loss: 0.08242164552211761\n",
      "Epoch 3606, Loss: 0.24206867814064026, Final Batch Loss: 0.1153949648141861\n",
      "Epoch 3607, Loss: 0.2737009674310684, Final Batch Loss: 0.09468677639961243\n",
      "Epoch 3608, Loss: 0.2850741744041443, Final Batch Loss: 0.11589577794075012\n",
      "Epoch 3609, Loss: 0.271308034658432, Final Batch Loss: 0.14299561083316803\n",
      "Epoch 3610, Loss: 0.32349905371665955, Final Batch Loss: 0.16949684917926788\n",
      "Epoch 3611, Loss: 0.2431967779994011, Final Batch Loss: 0.09803500026464462\n",
      "Epoch 3612, Loss: 0.27311838418245316, Final Batch Loss: 0.15678945183753967\n",
      "Epoch 3613, Loss: 0.29174597561359406, Final Batch Loss: 0.14696787297725677\n",
      "Epoch 3614, Loss: 0.25342170894145966, Final Batch Loss: 0.135885551571846\n",
      "Epoch 3615, Loss: 0.3486235439777374, Final Batch Loss: 0.190970316529274\n",
      "Epoch 3616, Loss: 0.31176355481147766, Final Batch Loss: 0.1736070215702057\n",
      "Epoch 3617, Loss: 0.26009225100278854, Final Batch Loss: 0.14706957340240479\n",
      "Epoch 3618, Loss: 0.26682156324386597, Final Batch Loss: 0.13024400174617767\n",
      "Epoch 3619, Loss: 0.29802047461271286, Final Batch Loss: 0.11691538244485855\n",
      "Epoch 3620, Loss: 0.2871180772781372, Final Batch Loss: 0.13196353614330292\n",
      "Epoch 3621, Loss: 0.25891849398612976, Final Batch Loss: 0.1272924691438675\n",
      "Epoch 3622, Loss: 0.33652129769325256, Final Batch Loss: 0.16363157331943512\n",
      "Epoch 3623, Loss: 0.30710457265377045, Final Batch Loss: 0.12887965142726898\n",
      "Epoch 3624, Loss: 0.29129087924957275, Final Batch Loss: 0.13273487985134125\n",
      "Epoch 3625, Loss: 0.29360294342041016, Final Batch Loss: 0.17505212128162384\n",
      "Epoch 3626, Loss: 0.2570681720972061, Final Batch Loss: 0.1173575222492218\n",
      "Epoch 3627, Loss: 0.3295995444059372, Final Batch Loss: 0.20154350996017456\n",
      "Epoch 3628, Loss: 0.29650261253118515, Final Batch Loss: 0.11926437169313431\n",
      "Epoch 3629, Loss: 0.2966526299715042, Final Batch Loss: 0.17093247175216675\n",
      "Epoch 3630, Loss: 0.3044443279504776, Final Batch Loss: 0.19300858676433563\n",
      "Epoch 3631, Loss: 0.2591702938079834, Final Batch Loss: 0.13058143854141235\n",
      "Epoch 3632, Loss: 0.317669577896595, Final Batch Loss: 0.1975831836462021\n",
      "Epoch 3633, Loss: 0.32790958881378174, Final Batch Loss: 0.17820459604263306\n",
      "Epoch 3634, Loss: 0.2915657088160515, Final Batch Loss: 0.0913630947470665\n",
      "Epoch 3635, Loss: 0.32656269520521164, Final Batch Loss: 0.22358892858028412\n",
      "Epoch 3636, Loss: 0.23270214349031448, Final Batch Loss: 0.09726390987634659\n",
      "Epoch 3637, Loss: 0.24529017508029938, Final Batch Loss: 0.1343257576227188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3638, Loss: 0.27292198687791824, Final Batch Loss: 0.11122751981019974\n",
      "Epoch 3639, Loss: 0.40659117698669434, Final Batch Loss: 0.19427786767482758\n",
      "Epoch 3640, Loss: 0.25426004081964493, Final Batch Loss: 0.09572724252939224\n",
      "Epoch 3641, Loss: 0.32117612659931183, Final Batch Loss: 0.19019906222820282\n",
      "Epoch 3642, Loss: 0.2744799703359604, Final Batch Loss: 0.13218693435192108\n",
      "Epoch 3643, Loss: 0.27658550441265106, Final Batch Loss: 0.13860291242599487\n",
      "Epoch 3644, Loss: 0.31313830614089966, Final Batch Loss: 0.18514588475227356\n",
      "Epoch 3645, Loss: 0.3003596365451813, Final Batch Loss: 0.1336362212896347\n",
      "Epoch 3646, Loss: 0.315782368183136, Final Batch Loss: 0.14778031408786774\n",
      "Epoch 3647, Loss: 0.3244432061910629, Final Batch Loss: 0.15675599873065948\n",
      "Epoch 3648, Loss: 0.2711125984787941, Final Batch Loss: 0.09755551069974899\n",
      "Epoch 3649, Loss: 0.293037548661232, Final Batch Loss: 0.1386643946170807\n",
      "Epoch 3650, Loss: 0.30900582671165466, Final Batch Loss: 0.1574997454881668\n",
      "Epoch 3651, Loss: 0.32834452390670776, Final Batch Loss: 0.13756273686885834\n",
      "Epoch 3652, Loss: 0.2666311711072922, Final Batch Loss: 0.13518163561820984\n",
      "Epoch 3653, Loss: 0.27579365670681, Final Batch Loss: 0.13572420179843903\n",
      "Epoch 3654, Loss: 0.3643750250339508, Final Batch Loss: 0.235004261136055\n",
      "Epoch 3655, Loss: 0.24236512929201126, Final Batch Loss: 0.08951608091592789\n",
      "Epoch 3656, Loss: 0.2578909546136856, Final Batch Loss: 0.14294497668743134\n",
      "Epoch 3657, Loss: 0.24151595681905746, Final Batch Loss: 0.09835625439882278\n",
      "Epoch 3658, Loss: 0.32579928636550903, Final Batch Loss: 0.1919081062078476\n",
      "Epoch 3659, Loss: 0.2597360983490944, Final Batch Loss: 0.10039118677377701\n",
      "Epoch 3660, Loss: 0.2834080457687378, Final Batch Loss: 0.12165303528308868\n",
      "Epoch 3661, Loss: 0.3293006792664528, Final Batch Loss: 0.2123788744211197\n",
      "Epoch 3662, Loss: 0.2916932553052902, Final Batch Loss: 0.14521299302577972\n",
      "Epoch 3663, Loss: 0.31640447676181793, Final Batch Loss: 0.19112767279148102\n",
      "Epoch 3664, Loss: 0.2880089208483696, Final Batch Loss: 0.16302050650119781\n",
      "Epoch 3665, Loss: 0.29395891726017, Final Batch Loss: 0.17996835708618164\n",
      "Epoch 3666, Loss: 0.2600960433483124, Final Batch Loss: 0.13210098445415497\n",
      "Epoch 3667, Loss: 0.240678071975708, Final Batch Loss: 0.10547126829624176\n",
      "Epoch 3668, Loss: 0.3432795852422714, Final Batch Loss: 0.20334959030151367\n",
      "Epoch 3669, Loss: 0.2614445313811302, Final Batch Loss: 0.1156659945845604\n",
      "Epoch 3670, Loss: 0.30606888979673386, Final Batch Loss: 0.10640309005975723\n",
      "Epoch 3671, Loss: 0.2723875343799591, Final Batch Loss: 0.14405789971351624\n",
      "Epoch 3672, Loss: 0.40611599385738373, Final Batch Loss: 0.16033312678337097\n",
      "Epoch 3673, Loss: 0.21733848005533218, Final Batch Loss: 0.0903773382306099\n",
      "Epoch 3674, Loss: 0.2691635936498642, Final Batch Loss: 0.13210511207580566\n",
      "Epoch 3675, Loss: 0.3125549554824829, Final Batch Loss: 0.15476946532726288\n",
      "Epoch 3676, Loss: 0.309581458568573, Final Batch Loss: 0.17490969598293304\n",
      "Epoch 3677, Loss: 0.3861428499221802, Final Batch Loss: 0.2322385311126709\n",
      "Epoch 3678, Loss: 0.2655406817793846, Final Batch Loss: 0.11762943118810654\n",
      "Epoch 3679, Loss: 0.2518739029765129, Final Batch Loss: 0.08211755007505417\n",
      "Epoch 3680, Loss: 0.32432766258716583, Final Batch Loss: 0.17849288880825043\n",
      "Epoch 3681, Loss: 0.2484777793288231, Final Batch Loss: 0.08878143876791\n",
      "Epoch 3682, Loss: 0.28683851659297943, Final Batch Loss: 0.14798714220523834\n",
      "Epoch 3683, Loss: 0.2554960697889328, Final Batch Loss: 0.12500230967998505\n",
      "Epoch 3684, Loss: 0.2966376394033432, Final Batch Loss: 0.16653719544410706\n",
      "Epoch 3685, Loss: 0.2618054896593094, Final Batch Loss: 0.11678646504878998\n",
      "Epoch 3686, Loss: 0.2720075249671936, Final Batch Loss: 0.15382768213748932\n",
      "Epoch 3687, Loss: 0.22698092460632324, Final Batch Loss: 0.06846155226230621\n",
      "Epoch 3688, Loss: 0.27267317473888397, Final Batch Loss: 0.12872366607189178\n",
      "Epoch 3689, Loss: 0.2802658677101135, Final Batch Loss: 0.14247022569179535\n",
      "Epoch 3690, Loss: 0.31470175832509995, Final Batch Loss: 0.19779528677463531\n",
      "Epoch 3691, Loss: 0.33125701546669006, Final Batch Loss: 0.17348098754882812\n",
      "Epoch 3692, Loss: 0.28956420719623566, Final Batch Loss: 0.15173828601837158\n",
      "Epoch 3693, Loss: 0.28876835107803345, Final Batch Loss: 0.13077044486999512\n",
      "Epoch 3694, Loss: 0.32412759959697723, Final Batch Loss: 0.14786696434020996\n",
      "Epoch 3695, Loss: 0.2806628942489624, Final Batch Loss: 0.14845527708530426\n",
      "Epoch 3696, Loss: 0.2545642852783203, Final Batch Loss: 0.11412335932254791\n",
      "Epoch 3697, Loss: 0.3244922608137131, Final Batch Loss: 0.18899789452552795\n",
      "Epoch 3698, Loss: 0.2668445184826851, Final Batch Loss: 0.15137265622615814\n",
      "Epoch 3699, Loss: 0.23626194149255753, Final Batch Loss: 0.09931092709302902\n",
      "Epoch 3700, Loss: 0.3600218743085861, Final Batch Loss: 0.13979458808898926\n",
      "Epoch 3701, Loss: 0.31014521420001984, Final Batch Loss: 0.12821514904499054\n",
      "Epoch 3702, Loss: 0.2861635684967041, Final Batch Loss: 0.15467068552970886\n",
      "Epoch 3703, Loss: 0.2691560387611389, Final Batch Loss: 0.13003835082054138\n",
      "Epoch 3704, Loss: 0.2686082348227501, Final Batch Loss: 0.14941167831420898\n",
      "Epoch 3705, Loss: 0.2927479147911072, Final Batch Loss: 0.15186995267868042\n",
      "Epoch 3706, Loss: 0.2590511292219162, Final Batch Loss: 0.08960367739200592\n",
      "Epoch 3707, Loss: 0.3126668632030487, Final Batch Loss: 0.14953044056892395\n",
      "Epoch 3708, Loss: 0.3486849218606949, Final Batch Loss: 0.18394248187541962\n",
      "Epoch 3709, Loss: 0.321322426199913, Final Batch Loss: 0.18151815235614777\n",
      "Epoch 3710, Loss: 0.3718375861644745, Final Batch Loss: 0.20569120347499847\n",
      "Epoch 3711, Loss: 0.2676839679479599, Final Batch Loss: 0.15440787374973297\n",
      "Epoch 3712, Loss: 0.33324241638183594, Final Batch Loss: 0.15960319340229034\n",
      "Epoch 3713, Loss: 0.32840128242969513, Final Batch Loss: 0.16484534740447998\n",
      "Epoch 3714, Loss: 0.2809356451034546, Final Batch Loss: 0.1439862847328186\n",
      "Epoch 3715, Loss: 0.3081846237182617, Final Batch Loss: 0.1693054735660553\n",
      "Epoch 3716, Loss: 0.25598033517599106, Final Batch Loss: 0.12076479941606522\n",
      "Epoch 3717, Loss: 0.24749940633773804, Final Batch Loss: 0.07960659265518188\n",
      "Epoch 3718, Loss: 0.2148270457983017, Final Batch Loss: 0.0649556815624237\n",
      "Epoch 3719, Loss: 0.33244696259498596, Final Batch Loss: 0.18809884786605835\n",
      "Epoch 3720, Loss: 0.2667083740234375, Final Batch Loss: 0.14311997592449188\n",
      "Epoch 3721, Loss: 0.26720696687698364, Final Batch Loss: 0.14066651463508606\n",
      "Epoch 3722, Loss: 0.3131137192249298, Final Batch Loss: 0.1300400048494339\n",
      "Epoch 3723, Loss: 0.31664715707302094, Final Batch Loss: 0.1513288915157318\n",
      "Epoch 3724, Loss: 0.26700881123542786, Final Batch Loss: 0.1331900954246521\n",
      "Epoch 3725, Loss: 0.22206899523735046, Final Batch Loss: 0.10243294388055801\n",
      "Epoch 3726, Loss: 0.31797441840171814, Final Batch Loss: 0.17347891628742218\n",
      "Epoch 3727, Loss: 0.3381173461675644, Final Batch Loss: 0.17874036729335785\n",
      "Epoch 3728, Loss: 0.2221825271844864, Final Batch Loss: 0.0736299455165863\n",
      "Epoch 3729, Loss: 0.29265788197517395, Final Batch Loss: 0.16045427322387695\n",
      "Epoch 3730, Loss: 0.26272356510162354, Final Batch Loss: 0.13410596549510956\n",
      "Epoch 3731, Loss: 0.28484752029180527, Final Batch Loss: 0.16416189074516296\n",
      "Epoch 3732, Loss: 0.29584620893001556, Final Batch Loss: 0.1365659087896347\n",
      "Epoch 3733, Loss: 0.28919684886932373, Final Batch Loss: 0.14391955733299255\n",
      "Epoch 3734, Loss: 0.28924229741096497, Final Batch Loss: 0.14241409301757812\n",
      "Epoch 3735, Loss: 0.25309033691883087, Final Batch Loss: 0.14028137922286987\n",
      "Epoch 3736, Loss: 0.295647531747818, Final Batch Loss: 0.13988858461380005\n",
      "Epoch 3737, Loss: 0.27875302731990814, Final Batch Loss: 0.14741703867912292\n",
      "Epoch 3738, Loss: 0.28049811720848083, Final Batch Loss: 0.15297116339206696\n",
      "Epoch 3739, Loss: 0.23368237167596817, Final Batch Loss: 0.08617766946554184\n",
      "Epoch 3740, Loss: 0.2779824361205101, Final Batch Loss: 0.11182460933923721\n",
      "Epoch 3741, Loss: 0.3090427368879318, Final Batch Loss: 0.1526702344417572\n",
      "Epoch 3742, Loss: 0.23941506445407867, Final Batch Loss: 0.10647925734519958\n",
      "Epoch 3743, Loss: 0.25734295696020126, Final Batch Loss: 0.10462860018014908\n",
      "Epoch 3744, Loss: 0.29464665055274963, Final Batch Loss: 0.1685226559638977\n",
      "Epoch 3745, Loss: 0.28816257417201996, Final Batch Loss: 0.1488434225320816\n",
      "Epoch 3746, Loss: 0.2585427984595299, Final Batch Loss: 0.13379301130771637\n",
      "Epoch 3747, Loss: 0.2619214951992035, Final Batch Loss: 0.10577251017093658\n",
      "Epoch 3748, Loss: 0.2579597532749176, Final Batch Loss: 0.16426408290863037\n",
      "Epoch 3749, Loss: 0.3409975618124008, Final Batch Loss: 0.1933867484331131\n",
      "Epoch 3750, Loss: 0.28079430758953094, Final Batch Loss: 0.1536911129951477\n",
      "Epoch 3751, Loss: 0.2662384659051895, Final Batch Loss: 0.13105031847953796\n",
      "Epoch 3752, Loss: 0.22821523249149323, Final Batch Loss: 0.09820228815078735\n",
      "Epoch 3753, Loss: 0.41271934658288956, Final Batch Loss: 0.29111066460609436\n",
      "Epoch 3754, Loss: 0.21870969980955124, Final Batch Loss: 0.10246535390615463\n",
      "Epoch 3755, Loss: 0.3138616681098938, Final Batch Loss: 0.1468961536884308\n",
      "Epoch 3756, Loss: 0.33405590057373047, Final Batch Loss: 0.1320246160030365\n",
      "Epoch 3757, Loss: 0.2729603201150894, Final Batch Loss: 0.13808704912662506\n",
      "Epoch 3758, Loss: 0.3020237684249878, Final Batch Loss: 0.13435809314250946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3759, Loss: 0.2926939129829407, Final Batch Loss: 0.14038494229316711\n",
      "Epoch 3760, Loss: 0.2919882535934448, Final Batch Loss: 0.1640917956829071\n",
      "Epoch 3761, Loss: 0.3354402482509613, Final Batch Loss: 0.13429929316043854\n",
      "Epoch 3762, Loss: 0.24015557020902634, Final Batch Loss: 0.11468551307916641\n",
      "Epoch 3763, Loss: 0.514886662364006, Final Batch Loss: 0.15418581664562225\n",
      "Epoch 3764, Loss: 0.2845870405435562, Final Batch Loss: 0.14976172149181366\n",
      "Epoch 3765, Loss: 0.26183465123176575, Final Batch Loss: 0.1341547667980194\n",
      "Epoch 3766, Loss: 0.3259129077196121, Final Batch Loss: 0.18598991632461548\n",
      "Epoch 3767, Loss: 0.3414393812417984, Final Batch Loss: 0.16746030747890472\n",
      "Epoch 3768, Loss: 0.2711828798055649, Final Batch Loss: 0.1600688099861145\n",
      "Epoch 3769, Loss: 0.2560610771179199, Final Batch Loss: 0.12681639194488525\n",
      "Epoch 3770, Loss: 0.2618595287203789, Final Batch Loss: 0.15672750771045685\n",
      "Epoch 3771, Loss: 0.285216748714447, Final Batch Loss: 0.1416236162185669\n",
      "Epoch 3772, Loss: 0.27726054936647415, Final Batch Loss: 0.12355219572782516\n",
      "Epoch 3773, Loss: 0.30473656952381134, Final Batch Loss: 0.17500178515911102\n",
      "Epoch 3774, Loss: 0.2919667065143585, Final Batch Loss: 0.16671372950077057\n",
      "Epoch 3775, Loss: 0.24831701815128326, Final Batch Loss: 0.1300036460161209\n",
      "Epoch 3776, Loss: 0.2871145159006119, Final Batch Loss: 0.14468462765216827\n",
      "Epoch 3777, Loss: 0.3083079755306244, Final Batch Loss: 0.177122101187706\n",
      "Epoch 3778, Loss: 0.23105015605688095, Final Batch Loss: 0.09132861346006393\n",
      "Epoch 3779, Loss: 0.32019759714603424, Final Batch Loss: 0.17481592297554016\n",
      "Epoch 3780, Loss: 0.2703056186437607, Final Batch Loss: 0.1347535401582718\n",
      "Epoch 3781, Loss: 0.30347777158021927, Final Batch Loss: 0.18537954986095428\n",
      "Epoch 3782, Loss: 0.32095277309417725, Final Batch Loss: 0.1883094161748886\n",
      "Epoch 3783, Loss: 0.27366679161787033, Final Batch Loss: 0.11633942276239395\n",
      "Epoch 3784, Loss: 0.27444737404584885, Final Batch Loss: 0.11891733855009079\n",
      "Epoch 3785, Loss: 0.26357507705688477, Final Batch Loss: 0.09796132147312164\n",
      "Epoch 3786, Loss: 0.22490037232637405, Final Batch Loss: 0.12124965339899063\n",
      "Epoch 3787, Loss: 0.28519492596387863, Final Batch Loss: 0.12208347767591476\n",
      "Epoch 3788, Loss: 0.3072366863489151, Final Batch Loss: 0.16829390823841095\n",
      "Epoch 3789, Loss: 0.2506374940276146, Final Batch Loss: 0.13424348831176758\n",
      "Epoch 3790, Loss: 0.2970413863658905, Final Batch Loss: 0.1861351877450943\n",
      "Epoch 3791, Loss: 0.2662131041288376, Final Batch Loss: 0.1043463796377182\n",
      "Epoch 3792, Loss: 0.2817813903093338, Final Batch Loss: 0.1342468410730362\n",
      "Epoch 3793, Loss: 0.2449614256620407, Final Batch Loss: 0.10651782155036926\n",
      "Epoch 3794, Loss: 0.26357945799827576, Final Batch Loss: 0.13794849812984467\n",
      "Epoch 3795, Loss: 0.25875842571258545, Final Batch Loss: 0.12167485058307648\n",
      "Epoch 3796, Loss: 0.25624188780784607, Final Batch Loss: 0.12682995200157166\n",
      "Epoch 3797, Loss: 0.2874210476875305, Final Batch Loss: 0.1494673639535904\n",
      "Epoch 3798, Loss: 0.2700190246105194, Final Batch Loss: 0.11584220826625824\n",
      "Epoch 3799, Loss: 0.2730514407157898, Final Batch Loss: 0.13673681020736694\n",
      "Epoch 3800, Loss: 0.3594367504119873, Final Batch Loss: 0.2181367725133896\n",
      "Epoch 3801, Loss: 0.33952194452285767, Final Batch Loss: 0.1844376176595688\n",
      "Epoch 3802, Loss: 0.2380741983652115, Final Batch Loss: 0.09888805449008942\n",
      "Epoch 3803, Loss: 0.26804379373788834, Final Batch Loss: 0.15015871822834015\n",
      "Epoch 3804, Loss: 0.2957959696650505, Final Batch Loss: 0.12063624709844589\n",
      "Epoch 3805, Loss: 0.2819913476705551, Final Batch Loss: 0.1575452983379364\n",
      "Epoch 3806, Loss: 0.27394598722457886, Final Batch Loss: 0.1538805514574051\n",
      "Epoch 3807, Loss: 0.37639985978603363, Final Batch Loss: 0.23007506132125854\n",
      "Epoch 3808, Loss: 0.29530584812164307, Final Batch Loss: 0.15376530587673187\n",
      "Epoch 3809, Loss: 0.29618509113788605, Final Batch Loss: 0.1632193773984909\n",
      "Epoch 3810, Loss: 0.28869520127773285, Final Batch Loss: 0.14523813128471375\n",
      "Epoch 3811, Loss: 0.30553895235061646, Final Batch Loss: 0.16441091895103455\n",
      "Epoch 3812, Loss: 0.2726255729794502, Final Batch Loss: 0.12385562807321548\n",
      "Epoch 3813, Loss: 0.30940282344818115, Final Batch Loss: 0.13026665151119232\n",
      "Epoch 3814, Loss: 0.25033656507730484, Final Batch Loss: 0.09967982023954391\n",
      "Epoch 3815, Loss: 0.25282537192106247, Final Batch Loss: 0.12373284250497818\n",
      "Epoch 3816, Loss: 0.31371963024139404, Final Batch Loss: 0.11861008405685425\n",
      "Epoch 3817, Loss: 0.2771022319793701, Final Batch Loss: 0.15344518423080444\n",
      "Epoch 3818, Loss: 0.32788920402526855, Final Batch Loss: 0.17295511066913605\n",
      "Epoch 3819, Loss: 0.26746033877134323, Final Batch Loss: 0.11167610436677933\n",
      "Epoch 3820, Loss: 0.2894841209053993, Final Batch Loss: 0.09909562021493912\n",
      "Epoch 3821, Loss: 0.2666991353034973, Final Batch Loss: 0.13150183856487274\n",
      "Epoch 3822, Loss: 0.2575252205133438, Final Batch Loss: 0.09610240161418915\n",
      "Epoch 3823, Loss: 0.2785664498806, Final Batch Loss: 0.14599226415157318\n",
      "Epoch 3824, Loss: 0.2841123342514038, Final Batch Loss: 0.13171710073947906\n",
      "Epoch 3825, Loss: 0.3141281455755234, Final Batch Loss: 0.14131468534469604\n",
      "Epoch 3826, Loss: 0.2461795061826706, Final Batch Loss: 0.10051067173480988\n",
      "Epoch 3827, Loss: 0.27444063127040863, Final Batch Loss: 0.13904915750026703\n",
      "Epoch 3828, Loss: 0.2885117530822754, Final Batch Loss: 0.1433902382850647\n",
      "Epoch 3829, Loss: 0.24625509232282639, Final Batch Loss: 0.11950729042291641\n",
      "Epoch 3830, Loss: 0.28097132593393326, Final Batch Loss: 0.12420781701803207\n",
      "Epoch 3831, Loss: 0.253226675093174, Final Batch Loss: 0.11738605052232742\n",
      "Epoch 3832, Loss: 0.2785429209470749, Final Batch Loss: 0.14010725915431976\n",
      "Epoch 3833, Loss: 0.22546931356191635, Final Batch Loss: 0.101218082010746\n",
      "Epoch 3834, Loss: 0.28188785910606384, Final Batch Loss: 0.15001602470874786\n",
      "Epoch 3835, Loss: 0.2440541610121727, Final Batch Loss: 0.10203143209218979\n",
      "Epoch 3836, Loss: 0.24711641669273376, Final Batch Loss: 0.12053145468235016\n",
      "Epoch 3837, Loss: 0.28724488615989685, Final Batch Loss: 0.13723459839820862\n",
      "Epoch 3838, Loss: 0.2691657692193985, Final Batch Loss: 0.13546359539031982\n",
      "Epoch 3839, Loss: 0.3010469079017639, Final Batch Loss: 0.16830116510391235\n",
      "Epoch 3840, Loss: 0.3505653589963913, Final Batch Loss: 0.21547865867614746\n",
      "Epoch 3841, Loss: 0.32218945026397705, Final Batch Loss: 0.17750351130962372\n",
      "Epoch 3842, Loss: 0.23329537361860275, Final Batch Loss: 0.09258853644132614\n",
      "Epoch 3843, Loss: 0.2916272282600403, Final Batch Loss: 0.147850900888443\n",
      "Epoch 3844, Loss: 0.2577644884586334, Final Batch Loss: 0.12687307596206665\n",
      "Epoch 3845, Loss: 0.2631078064441681, Final Batch Loss: 0.1463274210691452\n",
      "Epoch 3846, Loss: 0.2531459853053093, Final Batch Loss: 0.10876869410276413\n",
      "Epoch 3847, Loss: 0.2692800313234329, Final Batch Loss: 0.1398056149482727\n",
      "Epoch 3848, Loss: 0.24768366664648056, Final Batch Loss: 0.11490707844495773\n",
      "Epoch 3849, Loss: 0.24776265770196915, Final Batch Loss: 0.11516911536455154\n",
      "Epoch 3850, Loss: 0.29350389540195465, Final Batch Loss: 0.15353316068649292\n",
      "Epoch 3851, Loss: 0.2307548113167286, Final Batch Loss: 0.06074175611138344\n",
      "Epoch 3852, Loss: 0.20949356630444527, Final Batch Loss: 0.06007393077015877\n",
      "Epoch 3853, Loss: 0.3114582896232605, Final Batch Loss: 0.18143527209758759\n",
      "Epoch 3854, Loss: 0.2916252911090851, Final Batch Loss: 0.1533549427986145\n",
      "Epoch 3855, Loss: 0.24062146991491318, Final Batch Loss: 0.11263737827539444\n",
      "Epoch 3856, Loss: 0.2735625207424164, Final Batch Loss: 0.1371859610080719\n",
      "Epoch 3857, Loss: 0.29393646121025085, Final Batch Loss: 0.1170281171798706\n",
      "Epoch 3858, Loss: 0.28613321483135223, Final Batch Loss: 0.13320715725421906\n",
      "Epoch 3859, Loss: 0.24499810487031937, Final Batch Loss: 0.13119231164455414\n",
      "Epoch 3860, Loss: 0.23073802143335342, Final Batch Loss: 0.09230280667543411\n",
      "Epoch 3861, Loss: 0.2257509082555771, Final Batch Loss: 0.08729210495948792\n",
      "Epoch 3862, Loss: 0.2612383961677551, Final Batch Loss: 0.10288393497467041\n",
      "Epoch 3863, Loss: 0.26699911057949066, Final Batch Loss: 0.14913460612297058\n",
      "Epoch 3864, Loss: 0.25063181668519974, Final Batch Loss: 0.1215795949101448\n",
      "Epoch 3865, Loss: 0.26885028183460236, Final Batch Loss: 0.14388661086559296\n",
      "Epoch 3866, Loss: 0.28516367077827454, Final Batch Loss: 0.15392832458019257\n",
      "Epoch 3867, Loss: 0.2799835205078125, Final Batch Loss: 0.08610795438289642\n",
      "Epoch 3868, Loss: 0.2698814123868942, Final Batch Loss: 0.1048312783241272\n",
      "Epoch 3869, Loss: 0.26146918535232544, Final Batch Loss: 0.11295489966869354\n",
      "Epoch 3870, Loss: 0.31124716997146606, Final Batch Loss: 0.1543574333190918\n",
      "Epoch 3871, Loss: 0.24454889446496964, Final Batch Loss: 0.08401443809270859\n",
      "Epoch 3872, Loss: 0.2556867301464081, Final Batch Loss: 0.13488177955150604\n",
      "Epoch 3873, Loss: 0.32874470204114914, Final Batch Loss: 0.2204238623380661\n",
      "Epoch 3874, Loss: 0.2604832798242569, Final Batch Loss: 0.11587569117546082\n",
      "Epoch 3875, Loss: 0.356411337852478, Final Batch Loss: 0.19793467223644257\n",
      "Epoch 3876, Loss: 0.2884530574083328, Final Batch Loss: 0.11921882629394531\n",
      "Epoch 3877, Loss: 0.2559995949268341, Final Batch Loss: 0.12700773775577545\n",
      "Epoch 3878, Loss: 0.26603028923273087, Final Batch Loss: 0.14998523890972137\n",
      "Epoch 3879, Loss: 0.31342126429080963, Final Batch Loss: 0.17668190598487854\n",
      "Epoch 3880, Loss: 0.306115061044693, Final Batch Loss: 0.17682470381259918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3881, Loss: 0.2892241105437279, Final Batch Loss: 0.1738114356994629\n",
      "Epoch 3882, Loss: 0.2629963383078575, Final Batch Loss: 0.14136813580989838\n",
      "Epoch 3883, Loss: 0.2325213998556137, Final Batch Loss: 0.06760068237781525\n",
      "Epoch 3884, Loss: 0.32124652713537216, Final Batch Loss: 0.19779199361801147\n",
      "Epoch 3885, Loss: 0.24612286686897278, Final Batch Loss: 0.10147896409034729\n",
      "Epoch 3886, Loss: 0.2882952615618706, Final Batch Loss: 0.168317973613739\n",
      "Epoch 3887, Loss: 0.2778187170624733, Final Batch Loss: 0.15665099024772644\n",
      "Epoch 3888, Loss: 0.25398845225572586, Final Batch Loss: 0.11494144052267075\n",
      "Epoch 3889, Loss: 0.25952066481113434, Final Batch Loss: 0.11005997657775879\n",
      "Epoch 3890, Loss: 0.24861357361078262, Final Batch Loss: 0.10984032601118088\n",
      "Epoch 3891, Loss: 0.21796155720949173, Final Batch Loss: 0.08808184415102005\n",
      "Epoch 3892, Loss: 0.2602737694978714, Final Batch Loss: 0.12327645719051361\n",
      "Epoch 3893, Loss: 0.26928602159023285, Final Batch Loss: 0.13892953097820282\n",
      "Epoch 3894, Loss: 0.24102141708135605, Final Batch Loss: 0.0994751825928688\n",
      "Epoch 3895, Loss: 0.22285576909780502, Final Batch Loss: 0.09591975063085556\n",
      "Epoch 3896, Loss: 0.30591244995594025, Final Batch Loss: 0.11522473394870758\n",
      "Epoch 3897, Loss: 0.32700614631175995, Final Batch Loss: 0.17645566165447235\n",
      "Epoch 3898, Loss: 0.31085342168807983, Final Batch Loss: 0.12799520790576935\n",
      "Epoch 3899, Loss: 0.22886784374713898, Final Batch Loss: 0.08087623119354248\n",
      "Epoch 3900, Loss: 0.23322757333517075, Final Batch Loss: 0.10590256005525589\n",
      "Epoch 3901, Loss: 0.2814079448580742, Final Batch Loss: 0.1700177788734436\n",
      "Epoch 3902, Loss: 0.2901434376835823, Final Batch Loss: 0.1734311878681183\n",
      "Epoch 3903, Loss: 0.23675088584423065, Final Batch Loss: 0.10708369314670563\n",
      "Epoch 3904, Loss: 0.28515294194221497, Final Batch Loss: 0.13085900247097015\n",
      "Epoch 3905, Loss: 0.2635974660515785, Final Batch Loss: 0.16268393397331238\n",
      "Epoch 3906, Loss: 0.25180869549512863, Final Batch Loss: 0.1347498893737793\n",
      "Epoch 3907, Loss: 0.2300185039639473, Final Batch Loss: 0.09691371768712997\n",
      "Epoch 3908, Loss: 0.2504798546433449, Final Batch Loss: 0.10563113540410995\n",
      "Epoch 3909, Loss: 0.2636886164546013, Final Batch Loss: 0.11332649737596512\n",
      "Epoch 3910, Loss: 0.2672055959701538, Final Batch Loss: 0.1255401074886322\n",
      "Epoch 3911, Loss: 0.38922686129808426, Final Batch Loss: 0.27279868721961975\n",
      "Epoch 3912, Loss: 0.2973640710115433, Final Batch Loss: 0.17233121395111084\n",
      "Epoch 3913, Loss: 0.3524143695831299, Final Batch Loss: 0.17536252737045288\n",
      "Epoch 3914, Loss: 0.3406376838684082, Final Batch Loss: 0.21260429918766022\n",
      "Epoch 3915, Loss: 0.23737388104200363, Final Batch Loss: 0.08067765086889267\n",
      "Epoch 3916, Loss: 0.23759156465530396, Final Batch Loss: 0.084140345454216\n",
      "Epoch 3917, Loss: 0.2642914354801178, Final Batch Loss: 0.12798228859901428\n",
      "Epoch 3918, Loss: 0.21688072383403778, Final Batch Loss: 0.07012130320072174\n",
      "Epoch 3919, Loss: 0.3078773766756058, Final Batch Loss: 0.16894559562206268\n",
      "Epoch 3920, Loss: 0.30650608241558075, Final Batch Loss: 0.0938766598701477\n",
      "Epoch 3921, Loss: 0.28750479221343994, Final Batch Loss: 0.14608298242092133\n",
      "Epoch 3922, Loss: 0.34298528730869293, Final Batch Loss: 0.18891502916812897\n",
      "Epoch 3923, Loss: 0.25497808307409286, Final Batch Loss: 0.14386197924613953\n",
      "Epoch 3924, Loss: 0.28333209455013275, Final Batch Loss: 0.17280705273151398\n",
      "Epoch 3925, Loss: 0.24514580518007278, Final Batch Loss: 0.12403442710638046\n",
      "Epoch 3926, Loss: 0.23328787088394165, Final Batch Loss: 0.10975255072116852\n",
      "Epoch 3927, Loss: 0.24635057896375656, Final Batch Loss: 0.1367221176624298\n",
      "Epoch 3928, Loss: 0.29843769967556, Final Batch Loss: 0.1515417844057083\n",
      "Epoch 3929, Loss: 0.24887851625680923, Final Batch Loss: 0.1399288773536682\n",
      "Epoch 3930, Loss: 0.2282324582338333, Final Batch Loss: 0.11826104670763016\n",
      "Epoch 3931, Loss: 0.2579429894685745, Final Batch Loss: 0.12570492923259735\n",
      "Epoch 3932, Loss: 0.2405475303530693, Final Batch Loss: 0.10974333435297012\n",
      "Epoch 3933, Loss: 0.31151916086673737, Final Batch Loss: 0.17524291574954987\n",
      "Epoch 3934, Loss: 0.2427830994129181, Final Batch Loss: 0.13177616894245148\n",
      "Epoch 3935, Loss: 0.3082969933748245, Final Batch Loss: 0.1574704498052597\n",
      "Epoch 3936, Loss: 0.25136595219373703, Final Batch Loss: 0.11364134401082993\n",
      "Epoch 3937, Loss: 0.30030570924282074, Final Batch Loss: 0.15091527998447418\n",
      "Epoch 3938, Loss: 0.2622930705547333, Final Batch Loss: 0.12797676026821136\n",
      "Epoch 3939, Loss: 0.24879906326532364, Final Batch Loss: 0.10945888608694077\n",
      "Epoch 3940, Loss: 0.21441473811864853, Final Batch Loss: 0.07968495041131973\n",
      "Epoch 3941, Loss: 0.250931940972805, Final Batch Loss: 0.10057953745126724\n",
      "Epoch 3942, Loss: 0.35128284990787506, Final Batch Loss: 0.165364071726799\n",
      "Epoch 3943, Loss: 0.23746337741613388, Final Batch Loss: 0.10144636780023575\n",
      "Epoch 3944, Loss: 0.3277907967567444, Final Batch Loss: 0.10867735743522644\n",
      "Epoch 3945, Loss: 0.3036216199398041, Final Batch Loss: 0.17667068541049957\n",
      "Epoch 3946, Loss: 0.2787959575653076, Final Batch Loss: 0.1524014174938202\n",
      "Epoch 3947, Loss: 0.2245425283908844, Final Batch Loss: 0.10873807221651077\n",
      "Epoch 3948, Loss: 0.29239508509635925, Final Batch Loss: 0.1920570284128189\n",
      "Epoch 3949, Loss: 0.2544997036457062, Final Batch Loss: 0.12102963030338287\n",
      "Epoch 3950, Loss: 0.28517693281173706, Final Batch Loss: 0.1400606632232666\n",
      "Epoch 3951, Loss: 0.2938479632139206, Final Batch Loss: 0.16081678867340088\n",
      "Epoch 3952, Loss: 0.26713448762893677, Final Batch Loss: 0.1469954550266266\n",
      "Epoch 3953, Loss: 0.22626323252916336, Final Batch Loss: 0.11786799132823944\n",
      "Epoch 3954, Loss: 0.2729116156697273, Final Batch Loss: 0.12396412342786789\n",
      "Epoch 3955, Loss: 0.29587943851947784, Final Batch Loss: 0.1511424481868744\n",
      "Epoch 3956, Loss: 0.34095484018325806, Final Batch Loss: 0.18854817748069763\n",
      "Epoch 3957, Loss: 0.2573389783501625, Final Batch Loss: 0.13750100135803223\n",
      "Epoch 3958, Loss: 0.33009447902441025, Final Batch Loss: 0.21862401068210602\n",
      "Epoch 3959, Loss: 0.33570633828639984, Final Batch Loss: 0.17069993913173676\n",
      "Epoch 3960, Loss: 0.2803179919719696, Final Batch Loss: 0.16033896803855896\n",
      "Epoch 3961, Loss: 0.2665545716881752, Final Batch Loss: 0.11818385869264603\n",
      "Epoch 3962, Loss: 0.33259567618370056, Final Batch Loss: 0.1591307669878006\n",
      "Epoch 3963, Loss: 0.30755317211151123, Final Batch Loss: 0.17376741766929626\n",
      "Epoch 3964, Loss: 0.25412411987781525, Final Batch Loss: 0.1277572363615036\n",
      "Epoch 3965, Loss: 0.32669614255428314, Final Batch Loss: 0.13521195948123932\n",
      "Epoch 3966, Loss: 0.23193679004907608, Final Batch Loss: 0.10304520279169083\n",
      "Epoch 3967, Loss: 0.2218356654047966, Final Batch Loss: 0.0842558816075325\n",
      "Epoch 3968, Loss: 0.24115344882011414, Final Batch Loss: 0.09819114208221436\n",
      "Epoch 3969, Loss: 0.23032649606466293, Final Batch Loss: 0.11932434886693954\n",
      "Epoch 3970, Loss: 0.3215896338224411, Final Batch Loss: 0.17302511632442474\n",
      "Epoch 3971, Loss: 0.26491767168045044, Final Batch Loss: 0.13989847898483276\n",
      "Epoch 3972, Loss: 0.3261948972940445, Final Batch Loss: 0.1863941103219986\n",
      "Epoch 3973, Loss: 0.3268929347395897, Final Batch Loss: 0.08188392966985703\n",
      "Epoch 3974, Loss: 0.30049973726272583, Final Batch Loss: 0.172800213098526\n",
      "Epoch 3975, Loss: 0.3235507756471634, Final Batch Loss: 0.1702294796705246\n",
      "Epoch 3976, Loss: 0.22479353100061417, Final Batch Loss: 0.1055869311094284\n",
      "Epoch 3977, Loss: 0.2731955498456955, Final Batch Loss: 0.13383571803569794\n",
      "Epoch 3978, Loss: 0.24154330044984818, Final Batch Loss: 0.12530648708343506\n",
      "Epoch 3979, Loss: 0.3817840963602066, Final Batch Loss: 0.23925668001174927\n",
      "Epoch 3980, Loss: 0.28619516640901566, Final Batch Loss: 0.16792261600494385\n",
      "Epoch 3981, Loss: 0.2986048310995102, Final Batch Loss: 0.14025677740573883\n",
      "Epoch 3982, Loss: 0.29476170241832733, Final Batch Loss: 0.1897105872631073\n",
      "Epoch 3983, Loss: 0.25353699550032616, Final Batch Loss: 0.060291390866041183\n",
      "Epoch 3984, Loss: 0.3544294983148575, Final Batch Loss: 0.12119613587856293\n",
      "Epoch 3985, Loss: 0.2845280393958092, Final Batch Loss: 0.1106150820851326\n",
      "Epoch 3986, Loss: 0.33732207119464874, Final Batch Loss: 0.16973325610160828\n",
      "Epoch 3987, Loss: 0.2456386834383011, Final Batch Loss: 0.136380136013031\n",
      "Epoch 3988, Loss: 0.2518417462706566, Final Batch Loss: 0.15318433940410614\n",
      "Epoch 3989, Loss: 0.2993725538253784, Final Batch Loss: 0.15910840034484863\n",
      "Epoch 3990, Loss: 0.21524816751480103, Final Batch Loss: 0.0881802886724472\n",
      "Epoch 3991, Loss: 0.26806311309337616, Final Batch Loss: 0.12618693709373474\n",
      "Epoch 3992, Loss: 0.2907656282186508, Final Batch Loss: 0.12653715908527374\n",
      "Epoch 3993, Loss: 0.25515832006931305, Final Batch Loss: 0.13008257746696472\n",
      "Epoch 3994, Loss: 0.2714580073952675, Final Batch Loss: 0.14754803478717804\n",
      "Epoch 3995, Loss: 0.2558087632060051, Final Batch Loss: 0.13328687846660614\n",
      "Epoch 3996, Loss: 0.35576220601797104, Final Batch Loss: 0.1085604801774025\n",
      "Epoch 3997, Loss: 0.3314719498157501, Final Batch Loss: 0.19828924536705017\n",
      "Epoch 3998, Loss: 0.30694977939128876, Final Batch Loss: 0.1623787134885788\n",
      "Epoch 3999, Loss: 0.3280206173658371, Final Batch Loss: 0.16121241450309753\n",
      "Epoch 4000, Loss: 0.2542424350976944, Final Batch Loss: 0.12766002118587494\n",
      "Epoch 4001, Loss: 0.2662128582596779, Final Batch Loss: 0.11457333713769913\n",
      "Epoch 4002, Loss: 0.2874709218740463, Final Batch Loss: 0.15086866915225983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4003, Loss: 0.3246123716235161, Final Batch Loss: 0.10602625459432602\n",
      "Epoch 4004, Loss: 0.25219210237264633, Final Batch Loss: 0.13125573098659515\n",
      "Epoch 4005, Loss: 0.24505987763404846, Final Batch Loss: 0.11481219530105591\n",
      "Epoch 4006, Loss: 0.24875490367412567, Final Batch Loss: 0.12064360082149506\n",
      "Epoch 4007, Loss: 0.2672095373272896, Final Batch Loss: 0.11140774935483932\n",
      "Epoch 4008, Loss: 0.2568240761756897, Final Batch Loss: 0.12325869500637054\n",
      "Epoch 4009, Loss: 0.2767893150448799, Final Batch Loss: 0.15931840240955353\n",
      "Epoch 4010, Loss: 0.2932899594306946, Final Batch Loss: 0.15487664937973022\n",
      "Epoch 4011, Loss: 0.28632546961307526, Final Batch Loss: 0.15313847362995148\n",
      "Epoch 4012, Loss: 0.30220718681812286, Final Batch Loss: 0.1473945528268814\n",
      "Epoch 4013, Loss: 0.2659965753555298, Final Batch Loss: 0.12271332740783691\n",
      "Epoch 4014, Loss: 0.24861131608486176, Final Batch Loss: 0.09692263603210449\n",
      "Epoch 4015, Loss: 0.2651066854596138, Final Batch Loss: 0.15190008282661438\n",
      "Epoch 4016, Loss: 0.3006238490343094, Final Batch Loss: 0.14222899079322815\n",
      "Epoch 4017, Loss: 0.2738361805677414, Final Batch Loss: 0.13421940803527832\n",
      "Epoch 4018, Loss: 0.2536357194185257, Final Batch Loss: 0.12562866508960724\n",
      "Epoch 4019, Loss: 0.28101060539484024, Final Batch Loss: 0.16000239551067352\n",
      "Epoch 4020, Loss: 0.2647455856204033, Final Batch Loss: 0.10888167470693588\n",
      "Epoch 4021, Loss: 0.35058610886335373, Final Batch Loss: 0.25665950775146484\n",
      "Epoch 4022, Loss: 0.26773251593112946, Final Batch Loss: 0.1411070078611374\n",
      "Epoch 4023, Loss: 0.2712970897555351, Final Batch Loss: 0.11623813956975937\n",
      "Epoch 4024, Loss: 0.2726188972592354, Final Batch Loss: 0.15061277151107788\n",
      "Epoch 4025, Loss: 0.2731710895895958, Final Batch Loss: 0.16690650582313538\n",
      "Epoch 4026, Loss: 0.3304210752248764, Final Batch Loss: 0.19612188637256622\n",
      "Epoch 4027, Loss: 0.24264254420995712, Final Batch Loss: 0.09502857178449631\n",
      "Epoch 4028, Loss: 0.2640843242406845, Final Batch Loss: 0.1306794136762619\n",
      "Epoch 4029, Loss: 0.3509278893470764, Final Batch Loss: 0.23561133444309235\n",
      "Epoch 4030, Loss: 0.37294287979602814, Final Batch Loss: 0.1461472064256668\n",
      "Epoch 4031, Loss: 0.2726656347513199, Final Batch Loss: 0.14137941598892212\n",
      "Epoch 4032, Loss: 0.26931032538414, Final Batch Loss: 0.15544019639492035\n",
      "Epoch 4033, Loss: 0.3013879954814911, Final Batch Loss: 0.15227095782756805\n",
      "Epoch 4034, Loss: 0.31933364272117615, Final Batch Loss: 0.15132220089435577\n",
      "Epoch 4035, Loss: 0.2692703902721405, Final Batch Loss: 0.14090244472026825\n",
      "Epoch 4036, Loss: 0.25422303378582, Final Batch Loss: 0.12656491994857788\n",
      "Epoch 4037, Loss: 0.2983328551054001, Final Batch Loss: 0.14786474406719208\n",
      "Epoch 4038, Loss: 0.24460584670305252, Final Batch Loss: 0.10033198446035385\n",
      "Epoch 4039, Loss: 0.2464175596833229, Final Batch Loss: 0.10548602789640427\n",
      "Epoch 4040, Loss: 0.27464911341667175, Final Batch Loss: 0.1410432755947113\n",
      "Epoch 4041, Loss: 0.37379950284957886, Final Batch Loss: 0.10109817981719971\n",
      "Epoch 4042, Loss: 0.28015807271003723, Final Batch Loss: 0.11144638061523438\n",
      "Epoch 4043, Loss: 0.2639034613966942, Final Batch Loss: 0.15322183072566986\n",
      "Epoch 4044, Loss: 0.26727444678545, Final Batch Loss: 0.147707998752594\n",
      "Epoch 4045, Loss: 0.302970215678215, Final Batch Loss: 0.15860749781131744\n",
      "Epoch 4046, Loss: 0.22845995426177979, Final Batch Loss: 0.09753599762916565\n",
      "Epoch 4047, Loss: 0.28100115805864334, Final Batch Loss: 0.17320723831653595\n",
      "Epoch 4048, Loss: 0.2511727139353752, Final Batch Loss: 0.1378936469554901\n",
      "Epoch 4049, Loss: 0.3017827272415161, Final Batch Loss: 0.14083951711654663\n",
      "Epoch 4050, Loss: 0.26706625521183014, Final Batch Loss: 0.15347906947135925\n",
      "Epoch 4051, Loss: 0.24283501505851746, Final Batch Loss: 0.0960213840007782\n",
      "Epoch 4052, Loss: 0.21383552253246307, Final Batch Loss: 0.08506214618682861\n",
      "Epoch 4053, Loss: 0.3002539575099945, Final Batch Loss: 0.15099100768566132\n",
      "Epoch 4054, Loss: 0.3571210652589798, Final Batch Loss: 0.15479685366153717\n",
      "Epoch 4055, Loss: 0.23700524121522903, Final Batch Loss: 0.11388125270605087\n",
      "Epoch 4056, Loss: 0.25071336328983307, Final Batch Loss: 0.08983784914016724\n",
      "Epoch 4057, Loss: 0.22947034984827042, Final Batch Loss: 0.11639919131994247\n",
      "Epoch 4058, Loss: 0.23103830218315125, Final Batch Loss: 0.11470404267311096\n",
      "Epoch 4059, Loss: 0.2880932539701462, Final Batch Loss: 0.14527395367622375\n",
      "Epoch 4060, Loss: 0.24657627195119858, Final Batch Loss: 0.10943134874105453\n",
      "Epoch 4061, Loss: 0.3480626046657562, Final Batch Loss: 0.21339739859104156\n",
      "Epoch 4062, Loss: 0.2788023054599762, Final Batch Loss: 0.1578635573387146\n",
      "Epoch 4063, Loss: 0.28993891179561615, Final Batch Loss: 0.15653954446315765\n",
      "Epoch 4064, Loss: 0.27681778371334076, Final Batch Loss: 0.13466863334178925\n",
      "Epoch 4065, Loss: 0.28724727034568787, Final Batch Loss: 0.1352190524339676\n",
      "Epoch 4066, Loss: 0.27934928983449936, Final Batch Loss: 0.17310385406017303\n",
      "Epoch 4067, Loss: 0.25458211451768875, Final Batch Loss: 0.14394618570804596\n",
      "Epoch 4068, Loss: 0.26821745932102203, Final Batch Loss: 0.13226215541362762\n",
      "Epoch 4069, Loss: 0.2992721050977707, Final Batch Loss: 0.17401884496212006\n",
      "Epoch 4070, Loss: 0.28443704545497894, Final Batch Loss: 0.10128249228000641\n",
      "Epoch 4071, Loss: 0.29668672382831573, Final Batch Loss: 0.12745042145252228\n",
      "Epoch 4072, Loss: 0.27236321568489075, Final Batch Loss: 0.1484808474779129\n",
      "Epoch 4073, Loss: 0.2989736795425415, Final Batch Loss: 0.17385391891002655\n",
      "Epoch 4074, Loss: 0.28023234754800797, Final Batch Loss: 0.1606016308069229\n",
      "Epoch 4075, Loss: 0.3075733780860901, Final Batch Loss: 0.18218019604682922\n",
      "Epoch 4076, Loss: 0.24715062975883484, Final Batch Loss: 0.12362249940633774\n",
      "Epoch 4077, Loss: 0.2589096650481224, Final Batch Loss: 0.14276544749736786\n",
      "Epoch 4078, Loss: 0.31662556529045105, Final Batch Loss: 0.1483859121799469\n",
      "Epoch 4079, Loss: 0.2764534652233124, Final Batch Loss: 0.15882574021816254\n",
      "Epoch 4080, Loss: 0.24841640144586563, Final Batch Loss: 0.0976770743727684\n",
      "Epoch 4081, Loss: 0.28368324786424637, Final Batch Loss: 0.16352631151676178\n",
      "Epoch 4082, Loss: 0.26388630270957947, Final Batch Loss: 0.10878442227840424\n",
      "Epoch 4083, Loss: 0.3103362023830414, Final Batch Loss: 0.15816566348075867\n",
      "Epoch 4084, Loss: 0.2954873815178871, Final Batch Loss: 0.18030358850955963\n",
      "Epoch 4085, Loss: 0.2421872690320015, Final Batch Loss: 0.07986108213663101\n",
      "Epoch 4086, Loss: 0.24299442023038864, Final Batch Loss: 0.12122634053230286\n",
      "Epoch 4087, Loss: 0.34456007182598114, Final Batch Loss: 0.20599833130836487\n",
      "Epoch 4088, Loss: 0.3371570110321045, Final Batch Loss: 0.20338553190231323\n",
      "Epoch 4089, Loss: 0.24956928938627243, Final Batch Loss: 0.11979610472917557\n",
      "Epoch 4090, Loss: 0.28026050329208374, Final Batch Loss: 0.1587001085281372\n",
      "Epoch 4091, Loss: 0.27090954035520554, Final Batch Loss: 0.10907404869794846\n",
      "Epoch 4092, Loss: 0.2971365302801132, Final Batch Loss: 0.1370202749967575\n",
      "Epoch 4093, Loss: 0.2533195689320564, Final Batch Loss: 0.1339835822582245\n",
      "Epoch 4094, Loss: 0.2899882197380066, Final Batch Loss: 0.16083677113056183\n",
      "Epoch 4095, Loss: 0.2920414060354233, Final Batch Loss: 0.13547976315021515\n",
      "Epoch 4096, Loss: 0.24729542434215546, Final Batch Loss: 0.09286963939666748\n",
      "Epoch 4097, Loss: 0.39218951761722565, Final Batch Loss: 0.2624436616897583\n",
      "Epoch 4098, Loss: 0.26423224806785583, Final Batch Loss: 0.15601888298988342\n",
      "Epoch 4099, Loss: 0.25136370956897736, Final Batch Loss: 0.12005344033241272\n",
      "Epoch 4100, Loss: 0.28184153139591217, Final Batch Loss: 0.14318399131298065\n",
      "Epoch 4101, Loss: 0.27039581537246704, Final Batch Loss: 0.15632310509681702\n",
      "Epoch 4102, Loss: 0.38542652130126953, Final Batch Loss: 0.22768951952457428\n",
      "Epoch 4103, Loss: 0.23704440146684647, Final Batch Loss: 0.11525296419858932\n",
      "Epoch 4104, Loss: 0.3462747633457184, Final Batch Loss: 0.16849854588508606\n",
      "Epoch 4105, Loss: 0.2871377095580101, Final Batch Loss: 0.16516804695129395\n",
      "Epoch 4106, Loss: 0.2708711326122284, Final Batch Loss: 0.1369931548833847\n",
      "Epoch 4107, Loss: 0.23797225952148438, Final Batch Loss: 0.1016421914100647\n",
      "Epoch 4108, Loss: 0.2846125364303589, Final Batch Loss: 0.1265738159418106\n",
      "Epoch 4109, Loss: 0.2685328423976898, Final Batch Loss: 0.13689379394054413\n",
      "Epoch 4110, Loss: 0.34964263439178467, Final Batch Loss: 0.20755943655967712\n",
      "Epoch 4111, Loss: 0.2411254122853279, Final Batch Loss: 0.10560256987810135\n",
      "Epoch 4112, Loss: 0.2673167511820793, Final Batch Loss: 0.09721361845731735\n",
      "Epoch 4113, Loss: 0.24010297656059265, Final Batch Loss: 0.10521292686462402\n",
      "Epoch 4114, Loss: 0.24349761009216309, Final Batch Loss: 0.13272926211357117\n",
      "Epoch 4115, Loss: 0.2654627114534378, Final Batch Loss: 0.13286393880844116\n",
      "Epoch 4116, Loss: 0.2784889489412308, Final Batch Loss: 0.12862993776798248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4117, Loss: 0.2643236964941025, Final Batch Loss: 0.13783682882785797\n",
      "Epoch 4118, Loss: 0.37839318066835403, Final Batch Loss: 0.26167774200439453\n",
      "Epoch 4119, Loss: 0.3215280622243881, Final Batch Loss: 0.1888193041086197\n",
      "Epoch 4120, Loss: 0.28938569873571396, Final Batch Loss: 0.20048728585243225\n",
      "Epoch 4121, Loss: 0.3335295617580414, Final Batch Loss: 0.20092791318893433\n",
      "Epoch 4122, Loss: 0.2573619782924652, Final Batch Loss: 0.13272938132286072\n",
      "Epoch 4123, Loss: 0.22971975803375244, Final Batch Loss: 0.10268248617649078\n",
      "Epoch 4124, Loss: 0.2864692807197571, Final Batch Loss: 0.1418815702199936\n",
      "Epoch 4125, Loss: 0.3349582701921463, Final Batch Loss: 0.2066691517829895\n",
      "Epoch 4126, Loss: 0.24260365962982178, Final Batch Loss: 0.1254930943250656\n",
      "Epoch 4127, Loss: 0.29695234447717667, Final Batch Loss: 0.1737227439880371\n",
      "Epoch 4128, Loss: 0.263355515897274, Final Batch Loss: 0.11324604600667953\n",
      "Epoch 4129, Loss: 0.2598326578736305, Final Batch Loss: 0.14058788120746613\n",
      "Epoch 4130, Loss: 0.3125302642583847, Final Batch Loss: 0.18689677119255066\n",
      "Epoch 4131, Loss: 0.23324375599622726, Final Batch Loss: 0.09061679989099503\n",
      "Epoch 4132, Loss: 0.24817339330911636, Final Batch Loss: 0.13713712990283966\n",
      "Epoch 4133, Loss: 0.31751883029937744, Final Batch Loss: 0.18182390928268433\n",
      "Epoch 4134, Loss: 0.2617897018790245, Final Batch Loss: 0.12033624202013016\n",
      "Epoch 4135, Loss: 0.2218869850039482, Final Batch Loss: 0.10002318769693375\n",
      "Epoch 4136, Loss: 0.24717704951763153, Final Batch Loss: 0.10337162017822266\n",
      "Epoch 4137, Loss: 0.22011571377515793, Final Batch Loss: 0.11564937233924866\n",
      "Epoch 4138, Loss: 0.24950634688138962, Final Batch Loss: 0.10258399695158005\n",
      "Epoch 4139, Loss: 0.34136830270290375, Final Batch Loss: 0.12712986767292023\n",
      "Epoch 4140, Loss: 0.2953319549560547, Final Batch Loss: 0.10262812674045563\n",
      "Epoch 4141, Loss: 0.30938582122325897, Final Batch Loss: 0.15259550511837006\n",
      "Epoch 4142, Loss: 0.24804455786943436, Final Batch Loss: 0.12114223092794418\n",
      "Epoch 4143, Loss: 0.26034631580114365, Final Batch Loss: 0.11809613555669785\n",
      "Epoch 4144, Loss: 0.23013266921043396, Final Batch Loss: 0.07756295800209045\n",
      "Epoch 4145, Loss: 0.2542498782277107, Final Batch Loss: 0.14365768432617188\n",
      "Epoch 4146, Loss: 0.28172193467617035, Final Batch Loss: 0.15579567849636078\n",
      "Epoch 4147, Loss: 0.22187700122594833, Final Batch Loss: 0.07580605894327164\n",
      "Epoch 4148, Loss: 0.2622999846935272, Final Batch Loss: 0.11920367181301117\n",
      "Epoch 4149, Loss: 0.25933505594730377, Final Batch Loss: 0.09743291139602661\n",
      "Epoch 4150, Loss: 0.2506609931588173, Final Batch Loss: 0.1144370511174202\n",
      "Epoch 4151, Loss: 0.22871753573417664, Final Batch Loss: 0.09135058522224426\n",
      "Epoch 4152, Loss: 0.29340681433677673, Final Batch Loss: 0.12686726450920105\n",
      "Epoch 4153, Loss: 0.26756635308265686, Final Batch Loss: 0.13312365114688873\n",
      "Epoch 4154, Loss: 0.25557830184698105, Final Batch Loss: 0.13827954232692719\n",
      "Epoch 4155, Loss: 0.3249666839838028, Final Batch Loss: 0.17489829659461975\n",
      "Epoch 4156, Loss: 0.2509302422404289, Final Batch Loss: 0.12034931033849716\n",
      "Epoch 4157, Loss: 0.25665685534477234, Final Batch Loss: 0.13415826857089996\n",
      "Epoch 4158, Loss: 0.2652474641799927, Final Batch Loss: 0.09600396454334259\n",
      "Epoch 4159, Loss: 0.24971240013837814, Final Batch Loss: 0.10770430415868759\n",
      "Epoch 4160, Loss: 0.27599628269672394, Final Batch Loss: 0.12694741785526276\n",
      "Epoch 4161, Loss: 0.34607669711112976, Final Batch Loss: 0.2179516702890396\n",
      "Epoch 4162, Loss: 0.27233317494392395, Final Batch Loss: 0.14262035489082336\n",
      "Epoch 4163, Loss: 0.2817261219024658, Final Batch Loss: 0.14825448393821716\n",
      "Epoch 4164, Loss: 0.28409768640995026, Final Batch Loss: 0.14059576392173767\n",
      "Epoch 4165, Loss: 0.2448805496096611, Final Batch Loss: 0.12728945910930634\n",
      "Epoch 4166, Loss: 0.24498462677001953, Final Batch Loss: 0.1019728034734726\n",
      "Epoch 4167, Loss: 0.25124717503786087, Final Batch Loss: 0.11151499301195145\n",
      "Epoch 4168, Loss: 0.2724650800228119, Final Batch Loss: 0.12888026237487793\n",
      "Epoch 4169, Loss: 0.615262433886528, Final Batch Loss: 0.45817235112190247\n",
      "Epoch 4170, Loss: 0.1940987929701805, Final Batch Loss: 0.06595771759748459\n",
      "Epoch 4171, Loss: 0.2482021450996399, Final Batch Loss: 0.10454997420310974\n",
      "Epoch 4172, Loss: 0.35724592208862305, Final Batch Loss: 0.15793439745903015\n",
      "Epoch 4173, Loss: 0.31847332417964935, Final Batch Loss: 0.1263682097196579\n",
      "Epoch 4174, Loss: 0.3231046423316002, Final Batch Loss: 0.2050628662109375\n",
      "Epoch 4175, Loss: 0.3472290486097336, Final Batch Loss: 0.2133329212665558\n",
      "Epoch 4176, Loss: 0.2494179531931877, Final Batch Loss: 0.1366189867258072\n",
      "Epoch 4177, Loss: 0.25120316445827484, Final Batch Loss: 0.11904856562614441\n",
      "Epoch 4178, Loss: 0.27950308471918106, Final Batch Loss: 0.18281079828739166\n",
      "Epoch 4179, Loss: 0.24576875567436218, Final Batch Loss: 0.09848278760910034\n",
      "Epoch 4180, Loss: 0.2746356725692749, Final Batch Loss: 0.11759750545024872\n",
      "Epoch 4181, Loss: 0.32375243306159973, Final Batch Loss: 0.1444319784641266\n",
      "Epoch 4182, Loss: 0.37100909650325775, Final Batch Loss: 0.20135588943958282\n",
      "Epoch 4183, Loss: 0.31955553591251373, Final Batch Loss: 0.16435576975345612\n",
      "Epoch 4184, Loss: 0.21617009490728378, Final Batch Loss: 0.07800502330064774\n",
      "Epoch 4185, Loss: 0.2837011218070984, Final Batch Loss: 0.15754728019237518\n",
      "Epoch 4186, Loss: 0.2797197699546814, Final Batch Loss: 0.1593465954065323\n",
      "Epoch 4187, Loss: 0.25585809350013733, Final Batch Loss: 0.13771690428256989\n",
      "Epoch 4188, Loss: 0.26113736629486084, Final Batch Loss: 0.12377971410751343\n",
      "Epoch 4189, Loss: 0.24964305013418198, Final Batch Loss: 0.1255953311920166\n",
      "Epoch 4190, Loss: 0.25507232546806335, Final Batch Loss: 0.1401873528957367\n",
      "Epoch 4191, Loss: 0.35583579540252686, Final Batch Loss: 0.23290260136127472\n",
      "Epoch 4192, Loss: 0.2926662862300873, Final Batch Loss: 0.1716413050889969\n",
      "Epoch 4193, Loss: 0.3024698346853256, Final Batch Loss: 0.18385276198387146\n",
      "Epoch 4194, Loss: 0.283795565366745, Final Batch Loss: 0.13119395077228546\n",
      "Epoch 4195, Loss: 0.28475433588027954, Final Batch Loss: 0.17484985291957855\n",
      "Epoch 4196, Loss: 0.29840730130672455, Final Batch Loss: 0.13753725588321686\n",
      "Epoch 4197, Loss: 0.24076487123966217, Final Batch Loss: 0.12659983336925507\n",
      "Epoch 4198, Loss: 0.2495902106165886, Final Batch Loss: 0.12918585538864136\n",
      "Epoch 4199, Loss: 0.24721074104309082, Final Batch Loss: 0.09963515400886536\n",
      "Epoch 4200, Loss: 0.3664555698633194, Final Batch Loss: 0.21318365633487701\n",
      "Epoch 4201, Loss: 0.24545209109783173, Final Batch Loss: 0.10357943177223206\n",
      "Epoch 4202, Loss: 0.25675182044506073, Final Batch Loss: 0.1350233107805252\n",
      "Epoch 4203, Loss: 0.27464917302131653, Final Batch Loss: 0.15067708492279053\n",
      "Epoch 4204, Loss: 0.25944752246141434, Final Batch Loss: 0.1434594988822937\n",
      "Epoch 4205, Loss: 0.2072392702102661, Final Batch Loss: 0.07904374599456787\n",
      "Epoch 4206, Loss: 0.2659907639026642, Final Batch Loss: 0.12922154366970062\n",
      "Epoch 4207, Loss: 0.2611277997493744, Final Batch Loss: 0.10591655969619751\n",
      "Epoch 4208, Loss: 0.3484269380569458, Final Batch Loss: 0.13969673216342926\n",
      "Epoch 4209, Loss: 0.32327647507190704, Final Batch Loss: 0.19512204825878143\n",
      "Epoch 4210, Loss: 0.28896138817071915, Final Batch Loss: 0.19213102757930756\n",
      "Epoch 4211, Loss: 0.3015225827693939, Final Batch Loss: 0.13963669538497925\n",
      "Epoch 4212, Loss: 0.27073149383068085, Final Batch Loss: 0.12564633786678314\n",
      "Epoch 4213, Loss: 0.3101574629545212, Final Batch Loss: 0.1666233092546463\n",
      "Epoch 4214, Loss: 0.24288832396268845, Final Batch Loss: 0.08673342317342758\n",
      "Epoch 4215, Loss: 0.33823566138744354, Final Batch Loss: 0.2073306143283844\n",
      "Epoch 4216, Loss: 0.2165176197886467, Final Batch Loss: 0.09001811593770981\n",
      "Epoch 4217, Loss: 0.2441805601119995, Final Batch Loss: 0.12374711036682129\n",
      "Epoch 4218, Loss: 0.2633802741765976, Final Batch Loss: 0.13527414202690125\n",
      "Epoch 4219, Loss: 0.23789584636688232, Final Batch Loss: 0.12448976933956146\n",
      "Epoch 4220, Loss: 0.286810040473938, Final Batch Loss: 0.13528916239738464\n",
      "Epoch 4221, Loss: 0.32006682455539703, Final Batch Loss: 0.17909006774425507\n",
      "Epoch 4222, Loss: 0.2768539786338806, Final Batch Loss: 0.15427839756011963\n",
      "Epoch 4223, Loss: 0.2793857306241989, Final Batch Loss: 0.1234712302684784\n",
      "Epoch 4224, Loss: 0.2615424543619156, Final Batch Loss: 0.1301058977842331\n",
      "Epoch 4225, Loss: 0.31266479194164276, Final Batch Loss: 0.14336428046226501\n",
      "Epoch 4226, Loss: 0.33274413645267487, Final Batch Loss: 0.21239271759986877\n",
      "Epoch 4227, Loss: 0.2633340433239937, Final Batch Loss: 0.15207764506340027\n",
      "Epoch 4228, Loss: 0.23762617260217667, Final Batch Loss: 0.11173415929079056\n",
      "Epoch 4229, Loss: 0.4722418934106827, Final Batch Loss: 0.3101769983768463\n",
      "Epoch 4230, Loss: 0.28171856701374054, Final Batch Loss: 0.14403370022773743\n",
      "Epoch 4231, Loss: 0.4105016440153122, Final Batch Loss: 0.28319188952445984\n",
      "Epoch 4232, Loss: 0.27600274980068207, Final Batch Loss: 0.14123006165027618\n",
      "Epoch 4233, Loss: 0.21145566552877426, Final Batch Loss: 0.07455233484506607\n",
      "Epoch 4234, Loss: 0.31241342425346375, Final Batch Loss: 0.15655067563056946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4235, Loss: 0.27685748040676117, Final Batch Loss: 0.14208662509918213\n",
      "Epoch 4236, Loss: 0.26520510762929916, Final Batch Loss: 0.1207159087061882\n",
      "Epoch 4237, Loss: 0.2336692065000534, Final Batch Loss: 0.09253156185150146\n",
      "Epoch 4238, Loss: 0.2743002250790596, Final Batch Loss: 0.14937424659729004\n",
      "Epoch 4239, Loss: 0.25772247463464737, Final Batch Loss: 0.1244044378399849\n",
      "Epoch 4240, Loss: 0.3075007349252701, Final Batch Loss: 0.20112945139408112\n",
      "Epoch 4241, Loss: 0.28835541009902954, Final Batch Loss: 0.12259908020496368\n",
      "Epoch 4242, Loss: 0.2879832834005356, Final Batch Loss: 0.17104341089725494\n",
      "Epoch 4243, Loss: 0.25873883068561554, Final Batch Loss: 0.11301180720329285\n",
      "Epoch 4244, Loss: 0.2548864707350731, Final Batch Loss: 0.14047199487686157\n",
      "Epoch 4245, Loss: 0.23899175226688385, Final Batch Loss: 0.07945896685123444\n",
      "Epoch 4246, Loss: 0.24216057360172272, Final Batch Loss: 0.10865409672260284\n",
      "Epoch 4247, Loss: 0.2439957708120346, Final Batch Loss: 0.10863250494003296\n",
      "Epoch 4248, Loss: 0.23486701399087906, Final Batch Loss: 0.1140279695391655\n",
      "Epoch 4249, Loss: 0.31332363188266754, Final Batch Loss: 0.18549761176109314\n",
      "Epoch 4250, Loss: 0.3543148934841156, Final Batch Loss: 0.23026925325393677\n",
      "Epoch 4251, Loss: 0.2842419445514679, Final Batch Loss: 0.09252059459686279\n",
      "Epoch 4252, Loss: 0.2888578027486801, Final Batch Loss: 0.1774357110261917\n",
      "Epoch 4253, Loss: 0.2736208289861679, Final Batch Loss: 0.13042528927326202\n",
      "Epoch 4254, Loss: 0.26206986606121063, Final Batch Loss: 0.135965034365654\n",
      "Epoch 4255, Loss: 0.34593869745731354, Final Batch Loss: 0.1586615890264511\n",
      "Epoch 4256, Loss: 0.27663588523864746, Final Batch Loss: 0.164247527718544\n",
      "Epoch 4257, Loss: 0.3269214630126953, Final Batch Loss: 0.1977013200521469\n",
      "Epoch 4258, Loss: 0.25382041186094284, Final Batch Loss: 0.13122086226940155\n",
      "Epoch 4259, Loss: 0.23610211163759232, Final Batch Loss: 0.09554529935121536\n",
      "Epoch 4260, Loss: 0.23409158736467361, Final Batch Loss: 0.1114954873919487\n",
      "Epoch 4261, Loss: 0.2259116917848587, Final Batch Loss: 0.10857973992824554\n",
      "Epoch 4262, Loss: 0.2834511995315552, Final Batch Loss: 0.14787855744361877\n",
      "Epoch 4263, Loss: 0.25221122056245804, Final Batch Loss: 0.1454966813325882\n",
      "Epoch 4264, Loss: 0.2558974102139473, Final Batch Loss: 0.1451614648103714\n",
      "Epoch 4265, Loss: 0.2779413163661957, Final Batch Loss: 0.10481724143028259\n",
      "Epoch 4266, Loss: 0.2710571810603142, Final Batch Loss: 0.10444340854883194\n",
      "Epoch 4267, Loss: 0.2947934791445732, Final Batch Loss: 0.19990211725234985\n",
      "Epoch 4268, Loss: 0.25634241849184036, Final Batch Loss: 0.10737992078065872\n",
      "Epoch 4269, Loss: 0.22123806923627853, Final Batch Loss: 0.08216441422700882\n",
      "Epoch 4270, Loss: 0.29501771181821823, Final Batch Loss: 0.18150444328784943\n",
      "Epoch 4271, Loss: 0.22340881079435349, Final Batch Loss: 0.08364785462617874\n",
      "Epoch 4272, Loss: 0.27485987544059753, Final Batch Loss: 0.10953305661678314\n",
      "Epoch 4273, Loss: 0.2503416910767555, Final Batch Loss: 0.12609490752220154\n",
      "Epoch 4274, Loss: 0.25287893414497375, Final Batch Loss: 0.1259365677833557\n",
      "Epoch 4275, Loss: 0.24244266748428345, Final Batch Loss: 0.13931402564048767\n",
      "Epoch 4276, Loss: 0.19696352630853653, Final Batch Loss: 0.09934497624635696\n",
      "Epoch 4277, Loss: 0.2660321295261383, Final Batch Loss: 0.12776581943035126\n",
      "Epoch 4278, Loss: 0.2658996880054474, Final Batch Loss: 0.1278853416442871\n",
      "Epoch 4279, Loss: 0.27740825712680817, Final Batch Loss: 0.14679376780986786\n",
      "Epoch 4280, Loss: 0.2789708971977234, Final Batch Loss: 0.10619083046913147\n",
      "Epoch 4281, Loss: 0.25727666169404984, Final Batch Loss: 0.1356581747531891\n",
      "Epoch 4282, Loss: 0.2672639861702919, Final Batch Loss: 0.12180984765291214\n",
      "Epoch 4283, Loss: 0.22183147817850113, Final Batch Loss: 0.08643916994333267\n",
      "Epoch 4284, Loss: 0.2707351818680763, Final Batch Loss: 0.15062008798122406\n",
      "Epoch 4285, Loss: 0.2693130671977997, Final Batch Loss: 0.12234526872634888\n",
      "Epoch 4286, Loss: 0.27053992450237274, Final Batch Loss: 0.13735394179821014\n",
      "Epoch 4287, Loss: 0.24338606745004654, Final Batch Loss: 0.10249336808919907\n",
      "Epoch 4288, Loss: 0.29573990404605865, Final Batch Loss: 0.16709014773368835\n",
      "Epoch 4289, Loss: 0.2722802087664604, Final Batch Loss: 0.16699686646461487\n",
      "Epoch 4290, Loss: 0.23791392147541046, Final Batch Loss: 0.08431434631347656\n",
      "Epoch 4291, Loss: 0.2776245102286339, Final Batch Loss: 0.161007821559906\n",
      "Epoch 4292, Loss: 0.33343809843063354, Final Batch Loss: 0.18935523927211761\n",
      "Epoch 4293, Loss: 0.2827087938785553, Final Batch Loss: 0.11402423679828644\n",
      "Epoch 4294, Loss: 0.270559124648571, Final Batch Loss: 0.15451034903526306\n",
      "Epoch 4295, Loss: 0.25062279403209686, Final Batch Loss: 0.11139224469661713\n",
      "Epoch 4296, Loss: 0.2333526760339737, Final Batch Loss: 0.08611240983009338\n",
      "Epoch 4297, Loss: 0.2712356746196747, Final Batch Loss: 0.1287047266960144\n",
      "Epoch 4298, Loss: 0.3462127447128296, Final Batch Loss: 0.22095268964767456\n",
      "Epoch 4299, Loss: 0.25758955627679825, Final Batch Loss: 0.08147851377725601\n",
      "Epoch 4300, Loss: 0.22116321325302124, Final Batch Loss: 0.09176903963088989\n",
      "Epoch 4301, Loss: 0.262240894138813, Final Batch Loss: 0.09184353798627853\n",
      "Epoch 4302, Loss: 0.2440810427069664, Final Batch Loss: 0.1113886758685112\n",
      "Epoch 4303, Loss: 0.27423378825187683, Final Batch Loss: 0.11210009455680847\n",
      "Epoch 4304, Loss: 0.21709848195314407, Final Batch Loss: 0.10360062122344971\n",
      "Epoch 4305, Loss: 0.24326083064079285, Final Batch Loss: 0.11936768144369125\n",
      "Epoch 4306, Loss: 0.2537325918674469, Final Batch Loss: 0.12764421105384827\n",
      "Epoch 4307, Loss: 0.32423994690179825, Final Batch Loss: 0.21901066601276398\n",
      "Epoch 4308, Loss: 0.27658528089523315, Final Batch Loss: 0.15368503332138062\n",
      "Epoch 4309, Loss: 0.22985856980085373, Final Batch Loss: 0.11395728588104248\n",
      "Epoch 4310, Loss: 0.2791560888290405, Final Batch Loss: 0.17961370944976807\n",
      "Epoch 4311, Loss: 0.2839652970433235, Final Batch Loss: 0.16456590592861176\n",
      "Epoch 4312, Loss: 0.21729015558958054, Final Batch Loss: 0.08611903339624405\n",
      "Epoch 4313, Loss: 0.3264383524656296, Final Batch Loss: 0.14706869423389435\n",
      "Epoch 4314, Loss: 0.3367549404501915, Final Batch Loss: 0.21777276694774628\n",
      "Epoch 4315, Loss: 0.2673085406422615, Final Batch Loss: 0.11778945475816727\n",
      "Epoch 4316, Loss: 0.20672909170389175, Final Batch Loss: 0.11506495624780655\n",
      "Epoch 4317, Loss: 0.30853714793920517, Final Batch Loss: 0.1946805715560913\n",
      "Epoch 4318, Loss: 0.30328017473220825, Final Batch Loss: 0.13542743027210236\n",
      "Epoch 4319, Loss: 0.3652712404727936, Final Batch Loss: 0.22857414186000824\n",
      "Epoch 4320, Loss: 0.2677910774946213, Final Batch Loss: 0.15434157848358154\n",
      "Epoch 4321, Loss: 0.2652254030108452, Final Batch Loss: 0.14820417761802673\n",
      "Epoch 4322, Loss: 0.2744229808449745, Final Batch Loss: 0.1537395566701889\n",
      "Epoch 4323, Loss: 0.23000547289848328, Final Batch Loss: 0.1027657687664032\n",
      "Epoch 4324, Loss: 0.27525732666254044, Final Batch Loss: 0.15290233492851257\n",
      "Epoch 4325, Loss: 0.29590894281864166, Final Batch Loss: 0.15490755438804626\n",
      "Epoch 4326, Loss: 0.2920660525560379, Final Batch Loss: 0.1938534528017044\n",
      "Epoch 4327, Loss: 0.23654700070619583, Final Batch Loss: 0.10121659189462662\n",
      "Epoch 4328, Loss: 0.2527669295668602, Final Batch Loss: 0.13257135450839996\n",
      "Epoch 4329, Loss: 0.36029449105262756, Final Batch Loss: 0.14880192279815674\n",
      "Epoch 4330, Loss: 0.3000088930130005, Final Batch Loss: 0.11093239486217499\n",
      "Epoch 4331, Loss: 0.30053019523620605, Final Batch Loss: 0.13765089213848114\n",
      "Epoch 4332, Loss: 0.281473308801651, Final Batch Loss: 0.16403093934059143\n",
      "Epoch 4333, Loss: 0.2938076853752136, Final Batch Loss: 0.13949370384216309\n",
      "Epoch 4334, Loss: 0.33183908462524414, Final Batch Loss: 0.20473921298980713\n",
      "Epoch 4335, Loss: 0.30099840462207794, Final Batch Loss: 0.1554805040359497\n",
      "Epoch 4336, Loss: 0.2814168632030487, Final Batch Loss: 0.12559637427330017\n",
      "Epoch 4337, Loss: 0.3388494551181793, Final Batch Loss: 0.21300816535949707\n",
      "Epoch 4338, Loss: 0.28046588599681854, Final Batch Loss: 0.14185746014118195\n",
      "Epoch 4339, Loss: 0.27394552528858185, Final Batch Loss: 0.1525435745716095\n",
      "Epoch 4340, Loss: 0.3166254907846451, Final Batch Loss: 0.14134114980697632\n",
      "Epoch 4341, Loss: 0.2496296763420105, Final Batch Loss: 0.11784248054027557\n",
      "Epoch 4342, Loss: 0.26032931357622147, Final Batch Loss: 0.15396946668624878\n",
      "Epoch 4343, Loss: 0.25133441388607025, Final Batch Loss: 0.10070613026618958\n",
      "Epoch 4344, Loss: 0.19115649163722992, Final Batch Loss: 0.06971240788698196\n",
      "Epoch 4345, Loss: 0.2270544096827507, Final Batch Loss: 0.0915464386343956\n",
      "Epoch 4346, Loss: 0.34911172091960907, Final Batch Loss: 0.22127215564250946\n",
      "Epoch 4347, Loss: 0.258661225438118, Final Batch Loss: 0.12957380712032318\n",
      "Epoch 4348, Loss: 0.25517941266298294, Final Batch Loss: 0.14835363626480103\n",
      "Epoch 4349, Loss: 0.2999074161052704, Final Batch Loss: 0.16139717400074005\n",
      "Epoch 4350, Loss: 0.23950264602899551, Final Batch Loss: 0.07209613174200058\n",
      "Epoch 4351, Loss: 0.24817123264074326, Final Batch Loss: 0.13202230632305145\n",
      "Epoch 4352, Loss: 0.22006893157958984, Final Batch Loss: 0.108618825674057\n",
      "Epoch 4353, Loss: 0.2853427976369858, Final Batch Loss: 0.15608830749988556\n",
      "Epoch 4354, Loss: 0.2841441184282303, Final Batch Loss: 0.1206032931804657\n",
      "Epoch 4355, Loss: 0.2810313403606415, Final Batch Loss: 0.13174912333488464\n",
      "Epoch 4356, Loss: 0.27132464945316315, Final Batch Loss: 0.14510178565979004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4357, Loss: 0.25203095376491547, Final Batch Loss: 0.14530745148658752\n",
      "Epoch 4358, Loss: 0.2334425449371338, Final Batch Loss: 0.08980394899845123\n",
      "Epoch 4359, Loss: 0.2615804076194763, Final Batch Loss: 0.09384669363498688\n",
      "Epoch 4360, Loss: 0.24601735174655914, Final Batch Loss: 0.13155613839626312\n",
      "Epoch 4361, Loss: 0.24663399159908295, Final Batch Loss: 0.11473570764064789\n",
      "Epoch 4362, Loss: 0.32546618580818176, Final Batch Loss: 0.1949843317270279\n",
      "Epoch 4363, Loss: 0.3201211839914322, Final Batch Loss: 0.18940947949886322\n",
      "Epoch 4364, Loss: 0.28502456843852997, Final Batch Loss: 0.15413327515125275\n",
      "Epoch 4365, Loss: 0.21910879015922546, Final Batch Loss: 0.08540351688861847\n",
      "Epoch 4366, Loss: 0.22253663837909698, Final Batch Loss: 0.09910240769386292\n",
      "Epoch 4367, Loss: 0.26446104794740677, Final Batch Loss: 0.10584453493356705\n",
      "Epoch 4368, Loss: 0.2502288520336151, Final Batch Loss: 0.11505492031574249\n",
      "Epoch 4369, Loss: 0.2958534061908722, Final Batch Loss: 0.1408379077911377\n",
      "Epoch 4370, Loss: 0.2962767481803894, Final Batch Loss: 0.15376709401607513\n",
      "Epoch 4371, Loss: 0.2708937078714371, Final Batch Loss: 0.14786528050899506\n",
      "Epoch 4372, Loss: 0.3121085986495018, Final Batch Loss: 0.11384735256433487\n",
      "Epoch 4373, Loss: 0.24678514152765274, Final Batch Loss: 0.11985824257135391\n",
      "Epoch 4374, Loss: 0.28059904277324677, Final Batch Loss: 0.13695275783538818\n",
      "Epoch 4375, Loss: 0.18408968299627304, Final Batch Loss: 0.05557841807603836\n",
      "Epoch 4376, Loss: 0.2517971619963646, Final Batch Loss: 0.1060604378581047\n",
      "Epoch 4377, Loss: 0.2804820388555527, Final Batch Loss: 0.13739027082920074\n",
      "Epoch 4378, Loss: 0.26286544650793076, Final Batch Loss: 0.16255393624305725\n",
      "Epoch 4379, Loss: 0.26221101731061935, Final Batch Loss: 0.12323012202978134\n",
      "Epoch 4380, Loss: 0.2915372848510742, Final Batch Loss: 0.17144273221492767\n",
      "Epoch 4381, Loss: 0.302322655916214, Final Batch Loss: 0.19192823767662048\n",
      "Epoch 4382, Loss: 0.26955531537532806, Final Batch Loss: 0.15674397349357605\n",
      "Epoch 4383, Loss: 0.27275633811950684, Final Batch Loss: 0.14136865735054016\n",
      "Epoch 4384, Loss: 0.2527659684419632, Final Batch Loss: 0.1183040589094162\n",
      "Epoch 4385, Loss: 0.301994651556015, Final Batch Loss: 0.13587309420108795\n",
      "Epoch 4386, Loss: 0.2516075074672699, Final Batch Loss: 0.10197283327579498\n",
      "Epoch 4387, Loss: 0.29696716368198395, Final Batch Loss: 0.15659716725349426\n",
      "Epoch 4388, Loss: 0.2972005158662796, Final Batch Loss: 0.12503555417060852\n",
      "Epoch 4389, Loss: 0.33054760098457336, Final Batch Loss: 0.19704754650592804\n",
      "Epoch 4390, Loss: 0.268199160695076, Final Batch Loss: 0.147346630692482\n",
      "Epoch 4391, Loss: 0.22827044129371643, Final Batch Loss: 0.08771131932735443\n",
      "Epoch 4392, Loss: 0.25648605823516846, Final Batch Loss: 0.1250874251127243\n",
      "Epoch 4393, Loss: 0.2365419566631317, Final Batch Loss: 0.1003279834985733\n",
      "Epoch 4394, Loss: 0.27804412692785263, Final Batch Loss: 0.15544535219669342\n",
      "Epoch 4395, Loss: 0.23117918521165848, Final Batch Loss: 0.10172755271196365\n",
      "Epoch 4396, Loss: 0.26532305777072906, Final Batch Loss: 0.13265031576156616\n",
      "Epoch 4397, Loss: 0.26762013137340546, Final Batch Loss: 0.14000695943832397\n",
      "Epoch 4398, Loss: 0.20276744663715363, Final Batch Loss: 0.07757848501205444\n",
      "Epoch 4399, Loss: 0.2583150714635849, Final Batch Loss: 0.08361601829528809\n",
      "Epoch 4400, Loss: 0.2181980088353157, Final Batch Loss: 0.10773414373397827\n",
      "Epoch 4401, Loss: 0.34731025993824005, Final Batch Loss: 0.17034994065761566\n",
      "Epoch 4402, Loss: 0.264100581407547, Final Batch Loss: 0.12815023958683014\n",
      "Epoch 4403, Loss: 0.23005498945713043, Final Batch Loss: 0.10159352421760559\n",
      "Epoch 4404, Loss: 0.2706492468714714, Final Batch Loss: 0.12488239258527756\n",
      "Epoch 4405, Loss: 0.2483915165066719, Final Batch Loss: 0.1285068541765213\n",
      "Epoch 4406, Loss: 0.23409909009933472, Final Batch Loss: 0.11506827920675278\n",
      "Epoch 4407, Loss: 0.2938702404499054, Final Batch Loss: 0.16621780395507812\n",
      "Epoch 4408, Loss: 0.2705748677253723, Final Batch Loss: 0.13414238393306732\n",
      "Epoch 4409, Loss: 0.25637392699718475, Final Batch Loss: 0.12574227154254913\n",
      "Epoch 4410, Loss: 0.2414805293083191, Final Batch Loss: 0.11494778096675873\n",
      "Epoch 4411, Loss: 0.34505975246429443, Final Batch Loss: 0.2180037647485733\n",
      "Epoch 4412, Loss: 0.26862096041440964, Final Batch Loss: 0.16170379519462585\n",
      "Epoch 4413, Loss: 0.24991592019796371, Final Batch Loss: 0.11874265223741531\n",
      "Epoch 4414, Loss: 0.28335385024547577, Final Batch Loss: 0.1463773101568222\n",
      "Epoch 4415, Loss: 0.21743390709161758, Final Batch Loss: 0.10942722856998444\n",
      "Epoch 4416, Loss: 0.2819086015224457, Final Batch Loss: 0.14766867458820343\n",
      "Epoch 4417, Loss: 0.29291409254074097, Final Batch Loss: 0.13556905090808868\n",
      "Epoch 4418, Loss: 0.21438656747341156, Final Batch Loss: 0.09032291173934937\n",
      "Epoch 4419, Loss: 0.27491699904203415, Final Batch Loss: 0.11155834048986435\n",
      "Epoch 4420, Loss: 0.267742857336998, Final Batch Loss: 0.13607554137706757\n",
      "Epoch 4421, Loss: 0.2272123247385025, Final Batch Loss: 0.09907250106334686\n",
      "Epoch 4422, Loss: 0.2882104516029358, Final Batch Loss: 0.13843148946762085\n",
      "Epoch 4423, Loss: 0.2274947091937065, Final Batch Loss: 0.11654999107122421\n",
      "Epoch 4424, Loss: 0.2293091043829918, Final Batch Loss: 0.10975559800863266\n",
      "Epoch 4425, Loss: 0.2601056545972824, Final Batch Loss: 0.12196795642375946\n",
      "Epoch 4426, Loss: 0.3192620724439621, Final Batch Loss: 0.1777687966823578\n",
      "Epoch 4427, Loss: 0.24274533987045288, Final Batch Loss: 0.11008758842945099\n",
      "Epoch 4428, Loss: 0.24692975729703903, Final Batch Loss: 0.11719376593828201\n",
      "Epoch 4429, Loss: 0.2782547101378441, Final Batch Loss: 0.17836618423461914\n",
      "Epoch 4430, Loss: 0.31024983525276184, Final Batch Loss: 0.1566539704799652\n",
      "Epoch 4431, Loss: 0.30241626501083374, Final Batch Loss: 0.17184148728847504\n",
      "Epoch 4432, Loss: 0.28373468667268753, Final Batch Loss: 0.12091868370771408\n",
      "Epoch 4433, Loss: 0.2947803884744644, Final Batch Loss: 0.15157344937324524\n",
      "Epoch 4434, Loss: 0.3793819844722748, Final Batch Loss: 0.12800249457359314\n",
      "Epoch 4435, Loss: 0.2586265057325363, Final Batch Loss: 0.13247206807136536\n",
      "Epoch 4436, Loss: 0.26376399397850037, Final Batch Loss: 0.15418916940689087\n",
      "Epoch 4437, Loss: 0.2689738944172859, Final Batch Loss: 0.1766139715909958\n",
      "Epoch 4438, Loss: 0.23528268188238144, Final Batch Loss: 0.1031462624669075\n",
      "Epoch 4439, Loss: 0.23071740567684174, Final Batch Loss: 0.10291966795921326\n",
      "Epoch 4440, Loss: 0.31950782239437103, Final Batch Loss: 0.17887814342975616\n",
      "Epoch 4441, Loss: 0.2570451945066452, Final Batch Loss: 0.12716004252433777\n",
      "Epoch 4442, Loss: 0.24820377677679062, Final Batch Loss: 0.11836417764425278\n",
      "Epoch 4443, Loss: 0.28014857321977615, Final Batch Loss: 0.18719938397407532\n",
      "Epoch 4444, Loss: 0.274226650595665, Final Batch Loss: 0.14815416932106018\n",
      "Epoch 4445, Loss: 0.21560218185186386, Final Batch Loss: 0.06143868714570999\n",
      "Epoch 4446, Loss: 0.25157298147678375, Final Batch Loss: 0.12533152103424072\n",
      "Epoch 4447, Loss: 0.24902643263339996, Final Batch Loss: 0.1330101490020752\n",
      "Epoch 4448, Loss: 0.2305215448141098, Final Batch Loss: 0.14054930210113525\n",
      "Epoch 4449, Loss: 0.2296665534377098, Final Batch Loss: 0.0847085639834404\n",
      "Epoch 4450, Loss: 0.2257995381951332, Final Batch Loss: 0.10328635573387146\n",
      "Epoch 4451, Loss: 0.2513926774263382, Final Batch Loss: 0.12553724646568298\n",
      "Epoch 4452, Loss: 0.2387200891971588, Final Batch Loss: 0.10301470756530762\n",
      "Epoch 4453, Loss: 0.25481710582971573, Final Batch Loss: 0.06459347158670425\n",
      "Epoch 4454, Loss: 0.24204804003238678, Final Batch Loss: 0.10065503418445587\n",
      "Epoch 4455, Loss: 0.24123947322368622, Final Batch Loss: 0.13161839544773102\n",
      "Epoch 4456, Loss: 0.28139103949069977, Final Batch Loss: 0.11006112396717072\n",
      "Epoch 4457, Loss: 0.20491910725831985, Final Batch Loss: 0.07506371289491653\n",
      "Epoch 4458, Loss: 0.3086160346865654, Final Batch Loss: 0.1839708834886551\n",
      "Epoch 4459, Loss: 0.22146737575531006, Final Batch Loss: 0.10050328075885773\n",
      "Epoch 4460, Loss: 0.2910003066062927, Final Batch Loss: 0.15944284200668335\n",
      "Epoch 4461, Loss: 0.261560320854187, Final Batch Loss: 0.1433687061071396\n",
      "Epoch 4462, Loss: 0.25242001563310623, Final Batch Loss: 0.08824927359819412\n",
      "Epoch 4463, Loss: 0.2118074670433998, Final Batch Loss: 0.10993658751249313\n",
      "Epoch 4464, Loss: 0.2830367386341095, Final Batch Loss: 0.16117769479751587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4465, Loss: 0.3036854416131973, Final Batch Loss: 0.14729498326778412\n",
      "Epoch 4466, Loss: 0.31619662046432495, Final Batch Loss: 0.20617344975471497\n",
      "Epoch 4467, Loss: 0.2650189697742462, Final Batch Loss: 0.13976790010929108\n",
      "Epoch 4468, Loss: 0.2687665820121765, Final Batch Loss: 0.1657356321811676\n",
      "Epoch 4469, Loss: 0.2519225627183914, Final Batch Loss: 0.09347225725650787\n",
      "Epoch 4470, Loss: 0.265966571867466, Final Batch Loss: 0.15185140073299408\n",
      "Epoch 4471, Loss: 0.3160702586174011, Final Batch Loss: 0.1668546348810196\n",
      "Epoch 4472, Loss: 0.30579744279384613, Final Batch Loss: 0.184715136885643\n",
      "Epoch 4473, Loss: 0.343270018696785, Final Batch Loss: 0.15449070930480957\n",
      "Epoch 4474, Loss: 0.3865080028772354, Final Batch Loss: 0.2804645001888275\n",
      "Epoch 4475, Loss: 0.26053017377853394, Final Batch Loss: 0.13303826749324799\n",
      "Epoch 4476, Loss: 0.252087764441967, Final Batch Loss: 0.13699571788311005\n",
      "Epoch 4477, Loss: 0.24934885650873184, Final Batch Loss: 0.09838464111089706\n",
      "Epoch 4478, Loss: 0.254835844039917, Final Batch Loss: 0.10447010397911072\n",
      "Epoch 4479, Loss: 0.2744198739528656, Final Batch Loss: 0.17268744111061096\n",
      "Epoch 4480, Loss: 0.23010461032390594, Final Batch Loss: 0.13165812194347382\n",
      "Epoch 4481, Loss: 0.30454476922750473, Final Batch Loss: 0.18486417829990387\n",
      "Epoch 4482, Loss: 0.2509385943412781, Final Batch Loss: 0.16096390783786774\n",
      "Epoch 4483, Loss: 0.24615998566150665, Final Batch Loss: 0.14327919483184814\n",
      "Epoch 4484, Loss: 0.29054585099220276, Final Batch Loss: 0.14580649137496948\n",
      "Epoch 4485, Loss: 0.3296344131231308, Final Batch Loss: 0.23898378014564514\n",
      "Epoch 4486, Loss: 0.2892085164785385, Final Batch Loss: 0.1512290984392166\n",
      "Epoch 4487, Loss: 0.24856983870267868, Final Batch Loss: 0.14704573154449463\n",
      "Epoch 4488, Loss: 0.2627654895186424, Final Batch Loss: 0.14541403949260712\n",
      "Epoch 4489, Loss: 0.2830296456813812, Final Batch Loss: 0.12677720189094543\n",
      "Epoch 4490, Loss: 0.27960631251335144, Final Batch Loss: 0.12756823003292084\n",
      "Epoch 4491, Loss: 0.25355182588100433, Final Batch Loss: 0.10503239929676056\n",
      "Epoch 4492, Loss: 0.277429461479187, Final Batch Loss: 0.15377654135227203\n",
      "Epoch 4493, Loss: 0.32224321365356445, Final Batch Loss: 0.13481254875659943\n",
      "Epoch 4494, Loss: 0.2636159062385559, Final Batch Loss: 0.13057558238506317\n",
      "Epoch 4495, Loss: 0.315328985452652, Final Batch Loss: 0.17757035791873932\n",
      "Epoch 4496, Loss: 0.324558861553669, Final Batch Loss: 0.2081531435251236\n",
      "Epoch 4497, Loss: 0.2330855056643486, Final Batch Loss: 0.12988032400608063\n",
      "Epoch 4498, Loss: 0.2583252191543579, Final Batch Loss: 0.11090314388275146\n",
      "Epoch 4499, Loss: 0.27892005443573, Final Batch Loss: 0.16761931777000427\n",
      "Epoch 4500, Loss: 0.29794321954250336, Final Batch Loss: 0.16766008734703064\n",
      "Epoch 4501, Loss: 0.2824045419692993, Final Batch Loss: 0.16072921454906464\n",
      "Epoch 4502, Loss: 0.2622881233692169, Final Batch Loss: 0.13202820718288422\n",
      "Epoch 4503, Loss: 0.25772955268621445, Final Batch Loss: 0.10619135946035385\n",
      "Epoch 4504, Loss: 0.36969128251075745, Final Batch Loss: 0.2515924572944641\n",
      "Epoch 4505, Loss: 0.2569092810153961, Final Batch Loss: 0.10884106159210205\n",
      "Epoch 4506, Loss: 0.21206703782081604, Final Batch Loss: 0.09149204194545746\n",
      "Epoch 4507, Loss: 0.25547943264245987, Final Batch Loss: 0.09886845201253891\n",
      "Epoch 4508, Loss: 0.32798542082309723, Final Batch Loss: 0.21888084709644318\n",
      "Epoch 4509, Loss: 0.27427394688129425, Final Batch Loss: 0.1500573456287384\n",
      "Epoch 4510, Loss: 0.25076406449079514, Final Batch Loss: 0.1364578902721405\n",
      "Epoch 4511, Loss: 0.29742780327796936, Final Batch Loss: 0.1446709781885147\n",
      "Epoch 4512, Loss: 0.2608204260468483, Final Batch Loss: 0.11558154970407486\n",
      "Epoch 4513, Loss: 0.2719862535595894, Final Batch Loss: 0.14720460772514343\n",
      "Epoch 4514, Loss: 0.27744153141975403, Final Batch Loss: 0.12492571771144867\n",
      "Epoch 4515, Loss: 0.2341136559844017, Final Batch Loss: 0.07827349752187729\n",
      "Epoch 4516, Loss: 0.278916597366333, Final Batch Loss: 0.14962099492549896\n",
      "Epoch 4517, Loss: 0.3295772224664688, Final Batch Loss: 0.2258552610874176\n",
      "Epoch 4518, Loss: 0.283182829618454, Final Batch Loss: 0.1511559933423996\n",
      "Epoch 4519, Loss: 0.2614258676767349, Final Batch Loss: 0.15478909015655518\n",
      "Epoch 4520, Loss: 0.3045457452535629, Final Batch Loss: 0.22141776978969574\n",
      "Epoch 4521, Loss: 0.27854597568511963, Final Batch Loss: 0.1552361249923706\n",
      "Epoch 4522, Loss: 0.2567407488822937, Final Batch Loss: 0.12697052955627441\n",
      "Epoch 4523, Loss: 0.2718330919742584, Final Batch Loss: 0.15092214941978455\n",
      "Epoch 4524, Loss: 0.28409185260534286, Final Batch Loss: 0.12425433844327927\n",
      "Epoch 4525, Loss: 0.25027917325496674, Final Batch Loss: 0.11953136324882507\n",
      "Epoch 4526, Loss: 0.25464237481355667, Final Batch Loss: 0.11777839809656143\n",
      "Epoch 4527, Loss: 0.21380314230918884, Final Batch Loss: 0.0777713805437088\n",
      "Epoch 4528, Loss: 0.2920311838388443, Final Batch Loss: 0.150177463889122\n",
      "Epoch 4529, Loss: 0.30997584760189056, Final Batch Loss: 0.15604017674922943\n",
      "Epoch 4530, Loss: 0.254753902554512, Final Batch Loss: 0.12052357196807861\n",
      "Epoch 4531, Loss: 0.21424976736307144, Final Batch Loss: 0.10968893766403198\n",
      "Epoch 4532, Loss: 0.26311144977808, Final Batch Loss: 0.14086595177650452\n",
      "Epoch 4533, Loss: 0.305202841758728, Final Batch Loss: 0.1854313760995865\n",
      "Epoch 4534, Loss: 0.24923771619796753, Final Batch Loss: 0.14256791770458221\n",
      "Epoch 4535, Loss: 0.29237307608127594, Final Batch Loss: 0.18579711019992828\n",
      "Epoch 4536, Loss: 0.22441232949495316, Final Batch Loss: 0.1077132523059845\n",
      "Epoch 4537, Loss: 0.2974119260907173, Final Batch Loss: 0.18399767577648163\n",
      "Epoch 4538, Loss: 0.33117644488811493, Final Batch Loss: 0.19924567639827728\n",
      "Epoch 4539, Loss: 0.25495103001594543, Final Batch Loss: 0.12563277781009674\n",
      "Epoch 4540, Loss: 0.24848005920648575, Final Batch Loss: 0.129965141415596\n",
      "Epoch 4541, Loss: 0.24203931540250778, Final Batch Loss: 0.09051849693059921\n",
      "Epoch 4542, Loss: 0.21295404434204102, Final Batch Loss: 0.08924345672130585\n",
      "Epoch 4543, Loss: 0.2903256267309189, Final Batch Loss: 0.14243969321250916\n",
      "Epoch 4544, Loss: 0.3166341483592987, Final Batch Loss: 0.1753351092338562\n",
      "Epoch 4545, Loss: 0.264646515250206, Final Batch Loss: 0.11235542595386505\n",
      "Epoch 4546, Loss: 0.2761394530534744, Final Batch Loss: 0.11112277209758759\n",
      "Epoch 4547, Loss: 0.20950084924697876, Final Batch Loss: 0.07989475131034851\n",
      "Epoch 4548, Loss: 0.2506581023335457, Final Batch Loss: 0.09921712428331375\n",
      "Epoch 4549, Loss: 0.2547149956226349, Final Batch Loss: 0.1629142016172409\n",
      "Epoch 4550, Loss: 0.2482631430029869, Final Batch Loss: 0.09144864231348038\n",
      "Epoch 4551, Loss: 0.30007949471473694, Final Batch Loss: 0.16438382863998413\n",
      "Epoch 4552, Loss: 0.31211239099502563, Final Batch Loss: 0.15160341560840607\n",
      "Epoch 4553, Loss: 0.24059191346168518, Final Batch Loss: 0.116840660572052\n",
      "Epoch 4554, Loss: 0.20602433383464813, Final Batch Loss: 0.10429451614618301\n",
      "Epoch 4555, Loss: 0.2617967501282692, Final Batch Loss: 0.1402447372674942\n",
      "Epoch 4556, Loss: 0.295577272772789, Final Batch Loss: 0.16665713489055634\n",
      "Epoch 4557, Loss: 0.2448074370622635, Final Batch Loss: 0.10828149318695068\n",
      "Epoch 4558, Loss: 0.21137158572673798, Final Batch Loss: 0.09743081033229828\n",
      "Epoch 4559, Loss: 0.2680649012327194, Final Batch Loss: 0.11435180902481079\n",
      "Epoch 4560, Loss: 0.2796476185321808, Final Batch Loss: 0.16916075348854065\n",
      "Epoch 4561, Loss: 0.23730965703725815, Final Batch Loss: 0.10436754673719406\n",
      "Epoch 4562, Loss: 0.2255202755331993, Final Batch Loss: 0.12230480462312698\n",
      "Epoch 4563, Loss: 0.23543891310691833, Final Batch Loss: 0.14198394119739532\n",
      "Epoch 4564, Loss: 0.23979229480028152, Final Batch Loss: 0.11392431706190109\n",
      "Epoch 4565, Loss: 0.32728274166584015, Final Batch Loss: 0.15607300400733948\n",
      "Epoch 4566, Loss: 0.273666687309742, Final Batch Loss: 0.11976548284292221\n",
      "Epoch 4567, Loss: 0.242355078458786, Final Batch Loss: 0.10532036423683167\n",
      "Epoch 4568, Loss: 0.2672441005706787, Final Batch Loss: 0.09917701780796051\n",
      "Epoch 4569, Loss: 0.2124730870127678, Final Batch Loss: 0.07449164241552353\n",
      "Epoch 4570, Loss: 0.27419811487197876, Final Batch Loss: 0.15708300471305847\n",
      "Epoch 4571, Loss: 0.21236951649188995, Final Batch Loss: 0.10974140465259552\n",
      "Epoch 4572, Loss: 0.23771264404058456, Final Batch Loss: 0.13156504929065704\n",
      "Epoch 4573, Loss: 0.36846084892749786, Final Batch Loss: 0.21773871779441833\n",
      "Epoch 4574, Loss: 0.25345804542303085, Final Batch Loss: 0.13485769927501678\n",
      "Epoch 4575, Loss: 0.27066974341869354, Final Batch Loss: 0.11128054559230804\n",
      "Epoch 4576, Loss: 0.3074851334095001, Final Batch Loss: 0.14674285054206848\n",
      "Epoch 4577, Loss: 0.2507263273000717, Final Batch Loss: 0.1425376981496811\n",
      "Epoch 4578, Loss: 0.477179154753685, Final Batch Loss: 0.3641067147254944\n",
      "Epoch 4579, Loss: 0.23979868739843369, Final Batch Loss: 0.10918717831373215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4580, Loss: 0.2488253116607666, Final Batch Loss: 0.13110849261283875\n",
      "Epoch 4581, Loss: 0.3056981563568115, Final Batch Loss: 0.15353621542453766\n",
      "Epoch 4582, Loss: 0.27515583485364914, Final Batch Loss: 0.1585475355386734\n",
      "Epoch 4583, Loss: 0.30796270072460175, Final Batch Loss: 0.150270476937294\n",
      "Epoch 4584, Loss: 0.26827461272478104, Final Batch Loss: 0.09284292906522751\n",
      "Epoch 4585, Loss: 0.30955033004283905, Final Batch Loss: 0.19780753552913666\n",
      "Epoch 4586, Loss: 0.286111518740654, Final Batch Loss: 0.16488958895206451\n",
      "Epoch 4587, Loss: 0.25190701335668564, Final Batch Loss: 0.11712674051523209\n",
      "Epoch 4588, Loss: 0.332268163561821, Final Batch Loss: 0.20647896826267242\n",
      "Epoch 4589, Loss: 0.2372891530394554, Final Batch Loss: 0.11375948786735535\n",
      "Epoch 4590, Loss: 0.28082364797592163, Final Batch Loss: 0.15474675595760345\n",
      "Epoch 4591, Loss: 0.2467406913638115, Final Batch Loss: 0.11529097706079483\n",
      "Epoch 4592, Loss: 0.1888696774840355, Final Batch Loss: 0.06256792694330215\n",
      "Epoch 4593, Loss: 0.2272152155637741, Final Batch Loss: 0.090545654296875\n",
      "Epoch 4594, Loss: 0.2531191483139992, Final Batch Loss: 0.08622360974550247\n",
      "Epoch 4595, Loss: 0.28516610711812973, Final Batch Loss: 0.17647673189640045\n",
      "Epoch 4596, Loss: 0.206493578851223, Final Batch Loss: 0.09665453433990479\n",
      "Epoch 4597, Loss: 0.2552824169397354, Final Batch Loss: 0.1601223200559616\n",
      "Epoch 4598, Loss: 0.2858511209487915, Final Batch Loss: 0.16218751668930054\n",
      "Epoch 4599, Loss: 0.2519703134894371, Final Batch Loss: 0.13233819603919983\n",
      "Epoch 4600, Loss: 0.2637328878045082, Final Batch Loss: 0.1427302360534668\n",
      "Epoch 4601, Loss: 0.26506412774324417, Final Batch Loss: 0.10642192512750626\n",
      "Epoch 4602, Loss: 0.2539621517062187, Final Batch Loss: 0.08282152563333511\n",
      "Epoch 4603, Loss: 0.2365855723619461, Final Batch Loss: 0.12276541441679001\n",
      "Epoch 4604, Loss: 0.22397832572460175, Final Batch Loss: 0.07378201186656952\n",
      "Epoch 4605, Loss: 0.23976048082113266, Final Batch Loss: 0.123123899102211\n",
      "Epoch 4606, Loss: 0.3161957710981369, Final Batch Loss: 0.15763670206069946\n",
      "Epoch 4607, Loss: 0.2239435464143753, Final Batch Loss: 0.0891113132238388\n",
      "Epoch 4608, Loss: 0.3037433549761772, Final Batch Loss: 0.2020280957221985\n",
      "Epoch 4609, Loss: 0.23964525014162064, Final Batch Loss: 0.11984164267778397\n",
      "Epoch 4610, Loss: 0.2258271798491478, Final Batch Loss: 0.09024447947740555\n",
      "Epoch 4611, Loss: 0.2568156495690346, Final Batch Loss: 0.09583695977926254\n",
      "Epoch 4612, Loss: 0.3328234702348709, Final Batch Loss: 0.14586515724658966\n",
      "Epoch 4613, Loss: 0.23177272081375122, Final Batch Loss: 0.1096184030175209\n",
      "Epoch 4614, Loss: 0.22171574085950851, Final Batch Loss: 0.09473758190870285\n",
      "Epoch 4615, Loss: 0.3233952224254608, Final Batch Loss: 0.1986560970544815\n",
      "Epoch 4616, Loss: 0.25846993923187256, Final Batch Loss: 0.14382392168045044\n",
      "Epoch 4617, Loss: 0.21177023649215698, Final Batch Loss: 0.08405572175979614\n",
      "Epoch 4618, Loss: 0.25991082191467285, Final Batch Loss: 0.14019636809825897\n",
      "Epoch 4619, Loss: 0.32793791592121124, Final Batch Loss: 0.2064642310142517\n",
      "Epoch 4620, Loss: 0.24119140207767487, Final Batch Loss: 0.09062473475933075\n",
      "Epoch 4621, Loss: 0.24133513867855072, Final Batch Loss: 0.09844115376472473\n",
      "Epoch 4622, Loss: 0.25634267181158066, Final Batch Loss: 0.12279953807592392\n",
      "Epoch 4623, Loss: 0.2590496391057968, Final Batch Loss: 0.12247864902019501\n",
      "Epoch 4624, Loss: 0.29482899606227875, Final Batch Loss: 0.14642146229743958\n",
      "Epoch 4625, Loss: 0.31032221019268036, Final Batch Loss: 0.19823455810546875\n",
      "Epoch 4626, Loss: 0.25086627155542374, Final Batch Loss: 0.12155044823884964\n",
      "Epoch 4627, Loss: 0.28407133370637894, Final Batch Loss: 0.15938466787338257\n",
      "Epoch 4628, Loss: 0.25438567996025085, Final Batch Loss: 0.1418020874261856\n",
      "Epoch 4629, Loss: 0.25166504085063934, Final Batch Loss: 0.1254824697971344\n",
      "Epoch 4630, Loss: 0.22336242347955704, Final Batch Loss: 0.08821112662553787\n",
      "Epoch 4631, Loss: 0.2417067438364029, Final Batch Loss: 0.12024909257888794\n",
      "Epoch 4632, Loss: 0.30958495289087296, Final Batch Loss: 0.19580186903476715\n",
      "Epoch 4633, Loss: 0.21479782462120056, Final Batch Loss: 0.07106643915176392\n",
      "Epoch 4634, Loss: 0.3731177970767021, Final Batch Loss: 0.26975589990615845\n",
      "Epoch 4635, Loss: 0.23573700338602066, Final Batch Loss: 0.11865467578172684\n",
      "Epoch 4636, Loss: 0.3015633895993233, Final Batch Loss: 0.1846860647201538\n",
      "Epoch 4637, Loss: 0.2995693311095238, Final Batch Loss: 0.1843685805797577\n",
      "Epoch 4638, Loss: 0.344262033700943, Final Batch Loss: 0.20343171060085297\n",
      "Epoch 4639, Loss: 0.28102247416973114, Final Batch Loss: 0.128195658326149\n",
      "Epoch 4640, Loss: 0.295893132686615, Final Batch Loss: 0.15159037709236145\n",
      "Epoch 4641, Loss: 0.2835833504796028, Final Batch Loss: 0.11574123054742813\n",
      "Epoch 4642, Loss: 0.2428692728281021, Final Batch Loss: 0.13214968144893646\n",
      "Epoch 4643, Loss: 0.28887760639190674, Final Batch Loss: 0.1572863757610321\n",
      "Epoch 4644, Loss: 0.3229386657476425, Final Batch Loss: 0.16341447830200195\n",
      "Epoch 4645, Loss: 0.25932252407073975, Final Batch Loss: 0.13084305822849274\n",
      "Epoch 4646, Loss: 0.21597623825073242, Final Batch Loss: 0.09523690491914749\n",
      "Epoch 4647, Loss: 0.23880942165851593, Final Batch Loss: 0.12107612937688828\n",
      "Epoch 4648, Loss: 0.35674162209033966, Final Batch Loss: 0.2435624599456787\n",
      "Epoch 4649, Loss: 0.21571409702301025, Final Batch Loss: 0.09626410901546478\n",
      "Epoch 4650, Loss: 0.23099958896636963, Final Batch Loss: 0.11545742303133011\n",
      "Epoch 4651, Loss: 0.20923302322626114, Final Batch Loss: 0.08852650225162506\n",
      "Epoch 4652, Loss: 0.20941883325576782, Final Batch Loss: 0.09728725254535675\n",
      "Epoch 4653, Loss: 0.30092237889766693, Final Batch Loss: 0.15225306153297424\n",
      "Epoch 4654, Loss: 0.256719633936882, Final Batch Loss: 0.14492067694664001\n",
      "Epoch 4655, Loss: 0.3039931207895279, Final Batch Loss: 0.1536223441362381\n",
      "Epoch 4656, Loss: 0.2613004371523857, Final Batch Loss: 0.1500145047903061\n",
      "Epoch 4657, Loss: 0.2921340689063072, Final Batch Loss: 0.17477183043956757\n",
      "Epoch 4658, Loss: 0.2693846672773361, Final Batch Loss: 0.17455410957336426\n",
      "Epoch 4659, Loss: 0.21813911944627762, Final Batch Loss: 0.11783649027347565\n",
      "Epoch 4660, Loss: 0.3109991326928139, Final Batch Loss: 0.08679424971342087\n",
      "Epoch 4661, Loss: 0.2458052933216095, Final Batch Loss: 0.12112059444189072\n",
      "Epoch 4662, Loss: 0.339945986866951, Final Batch Loss: 0.13551199436187744\n",
      "Epoch 4663, Loss: 0.1916460283100605, Final Batch Loss: 0.059956613928079605\n",
      "Epoch 4664, Loss: 0.27407175302505493, Final Batch Loss: 0.1419401615858078\n",
      "Epoch 4665, Loss: 0.24012383073568344, Final Batch Loss: 0.07002849131822586\n",
      "Epoch 4666, Loss: 0.2319517359137535, Final Batch Loss: 0.10119981318712234\n",
      "Epoch 4667, Loss: 0.3316328898072243, Final Batch Loss: 0.20987845957279205\n",
      "Epoch 4668, Loss: 0.2447507381439209, Final Batch Loss: 0.1169540286064148\n",
      "Epoch 4669, Loss: 0.26664694398641586, Final Batch Loss: 0.12417160719633102\n",
      "Epoch 4670, Loss: 0.24247169494628906, Final Batch Loss: 0.10833564400672913\n",
      "Epoch 4671, Loss: 0.2562367543578148, Final Batch Loss: 0.14021043479442596\n",
      "Epoch 4672, Loss: 0.2618028149008751, Final Batch Loss: 0.1233309879899025\n",
      "Epoch 4673, Loss: 0.3097672611474991, Final Batch Loss: 0.1800830215215683\n",
      "Epoch 4674, Loss: 0.24855030328035355, Final Batch Loss: 0.12552745640277863\n",
      "Epoch 4675, Loss: 0.23704757541418076, Final Batch Loss: 0.1183699294924736\n",
      "Epoch 4676, Loss: 0.23593000322580338, Final Batch Loss: 0.10939919203519821\n",
      "Epoch 4677, Loss: 0.2369532585144043, Final Batch Loss: 0.09377171099185944\n",
      "Epoch 4678, Loss: 0.2355559542775154, Final Batch Loss: 0.11667270958423615\n",
      "Epoch 4679, Loss: 0.25542187690734863, Final Batch Loss: 0.16605202853679657\n",
      "Epoch 4680, Loss: 0.23464363813400269, Final Batch Loss: 0.08651717007160187\n",
      "Epoch 4681, Loss: 0.236322820186615, Final Batch Loss: 0.11866230517625809\n",
      "Epoch 4682, Loss: 0.22167649865150452, Final Batch Loss: 0.07658270001411438\n",
      "Epoch 4683, Loss: 0.3113444745540619, Final Batch Loss: 0.20367322862148285\n",
      "Epoch 4684, Loss: 0.28994767367839813, Final Batch Loss: 0.17929871380329132\n",
      "Epoch 4685, Loss: 0.20926466584205627, Final Batch Loss: 0.08707549422979355\n",
      "Epoch 4686, Loss: 0.20429643988609314, Final Batch Loss: 0.0707101970911026\n",
      "Epoch 4687, Loss: 0.2443266659975052, Final Batch Loss: 0.08942240476608276\n",
      "Epoch 4688, Loss: 0.1916912943124771, Final Batch Loss: 0.07045484334230423\n",
      "Epoch 4689, Loss: 0.3297787979245186, Final Batch Loss: 0.23051321506500244\n",
      "Epoch 4690, Loss: 0.27404721081256866, Final Batch Loss: 0.12669038772583008\n",
      "Epoch 4691, Loss: 0.23701117187738419, Final Batch Loss: 0.10102104395627975\n",
      "Epoch 4692, Loss: 0.2468172237277031, Final Batch Loss: 0.11753251403570175\n",
      "Epoch 4693, Loss: 0.24187016487121582, Final Batch Loss: 0.09850098192691803\n",
      "Epoch 4694, Loss: 0.350135013461113, Final Batch Loss: 0.2154986709356308\n",
      "Epoch 4695, Loss: 0.22727017104625702, Final Batch Loss: 0.11224278807640076\n",
      "Epoch 4696, Loss: 0.2654827907681465, Final Batch Loss: 0.152530699968338\n",
      "Epoch 4697, Loss: 0.22786079347133636, Final Batch Loss: 0.12164931744337082\n",
      "Epoch 4698, Loss: 0.29525862634181976, Final Batch Loss: 0.1537613719701767\n",
      "Epoch 4699, Loss: 0.25994449853897095, Final Batch Loss: 0.08267250657081604\n",
      "Epoch 4700, Loss: 0.21015775203704834, Final Batch Loss: 0.07738515734672546\n",
      "Epoch 4701, Loss: 0.3536354750394821, Final Batch Loss: 0.17730003595352173\n",
      "Epoch 4702, Loss: 0.2559107169508934, Final Batch Loss: 0.15415552258491516\n",
      "Epoch 4703, Loss: 0.24507051706314087, Final Batch Loss: 0.11082734167575836\n",
      "Epoch 4704, Loss: 0.30197538435459137, Final Batch Loss: 0.16967718303203583\n",
      "Epoch 4705, Loss: 0.3441654443740845, Final Batch Loss: 0.14667817950248718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4706, Loss: 0.2569599449634552, Final Batch Loss: 0.14824533462524414\n",
      "Epoch 4707, Loss: 0.44254225492477417, Final Batch Loss: 0.296384334564209\n",
      "Epoch 4708, Loss: 0.2935982793569565, Final Batch Loss: 0.17150817811489105\n",
      "Epoch 4709, Loss: 0.25239717215299606, Final Batch Loss: 0.13895338773727417\n",
      "Epoch 4710, Loss: 0.25146201252937317, Final Batch Loss: 0.13678297400474548\n",
      "Epoch 4711, Loss: 0.2834591791033745, Final Batch Loss: 0.16213278472423553\n",
      "Epoch 4712, Loss: 0.4144556373357773, Final Batch Loss: 0.2576914429664612\n",
      "Epoch 4713, Loss: 0.28008227050304413, Final Batch Loss: 0.1112731546163559\n",
      "Epoch 4714, Loss: 0.2972254827618599, Final Batch Loss: 0.186102956533432\n",
      "Epoch 4715, Loss: 0.22724303603172302, Final Batch Loss: 0.08082562685012817\n",
      "Epoch 4716, Loss: 0.2486845999956131, Final Batch Loss: 0.120203897356987\n",
      "Epoch 4717, Loss: 0.22296156734228134, Final Batch Loss: 0.12303782254457474\n",
      "Epoch 4718, Loss: 0.25905589014291763, Final Batch Loss: 0.13883382081985474\n",
      "Epoch 4719, Loss: 0.30132366716861725, Final Batch Loss: 0.1650017499923706\n",
      "Epoch 4720, Loss: 0.2201521322131157, Final Batch Loss: 0.1063663586974144\n",
      "Epoch 4721, Loss: 0.2100123167037964, Final Batch Loss: 0.09451474249362946\n",
      "Epoch 4722, Loss: 0.2972882390022278, Final Batch Loss: 0.15368244051933289\n",
      "Epoch 4723, Loss: 0.20878075063228607, Final Batch Loss: 0.06407250463962555\n",
      "Epoch 4724, Loss: 0.2626902759075165, Final Batch Loss: 0.11591443419456482\n",
      "Epoch 4725, Loss: 0.25532344728708267, Final Batch Loss: 0.09584762901067734\n",
      "Epoch 4726, Loss: 0.2174067199230194, Final Batch Loss: 0.0957430973649025\n",
      "Epoch 4727, Loss: 0.22201257199048996, Final Batch Loss: 0.10786738991737366\n",
      "Epoch 4728, Loss: 0.2962298095226288, Final Batch Loss: 0.16620850563049316\n",
      "Epoch 4729, Loss: 0.22547060251235962, Final Batch Loss: 0.09328708052635193\n",
      "Epoch 4730, Loss: 0.23102527111768723, Final Batch Loss: 0.12809592485427856\n",
      "Epoch 4731, Loss: 0.2655644491314888, Final Batch Loss: 0.11918842047452927\n",
      "Epoch 4732, Loss: 0.22996694594621658, Final Batch Loss: 0.09988962858915329\n",
      "Epoch 4733, Loss: 0.24148477613925934, Final Batch Loss: 0.09353567659854889\n",
      "Epoch 4734, Loss: 0.24033036828041077, Final Batch Loss: 0.12897543609142303\n",
      "Epoch 4735, Loss: 0.21811766177415848, Final Batch Loss: 0.10285773873329163\n",
      "Epoch 4736, Loss: 0.2136029228568077, Final Batch Loss: 0.11305985599756241\n",
      "Epoch 4737, Loss: 0.2606232762336731, Final Batch Loss: 0.13730023801326752\n",
      "Epoch 4738, Loss: 0.23553627729415894, Final Batch Loss: 0.11310585588216782\n",
      "Epoch 4739, Loss: 0.27273033559322357, Final Batch Loss: 0.1834556758403778\n",
      "Epoch 4740, Loss: 0.31562916189432144, Final Batch Loss: 0.20689474046230316\n",
      "Epoch 4741, Loss: 0.3017672076821327, Final Batch Loss: 0.19819234311580658\n",
      "Epoch 4742, Loss: 0.22464188188314438, Final Batch Loss: 0.08618108183145523\n",
      "Epoch 4743, Loss: 0.31219613552093506, Final Batch Loss: 0.1560097634792328\n",
      "Epoch 4744, Loss: 0.294805109500885, Final Batch Loss: 0.16933007538318634\n",
      "Epoch 4745, Loss: 0.2407146692276001, Final Batch Loss: 0.1055450588464737\n",
      "Epoch 4746, Loss: 0.20950105786323547, Final Batch Loss: 0.08773418515920639\n",
      "Epoch 4747, Loss: 0.24608143419027328, Final Batch Loss: 0.09483214467763901\n",
      "Epoch 4748, Loss: 0.2646755576133728, Final Batch Loss: 0.1357862502336502\n",
      "Epoch 4749, Loss: 0.20651481673121452, Final Batch Loss: 0.050701793283224106\n",
      "Epoch 4750, Loss: 0.21834582835435867, Final Batch Loss: 0.10234589874744415\n",
      "Epoch 4751, Loss: 0.2514133006334305, Final Batch Loss: 0.1274387240409851\n",
      "Epoch 4752, Loss: 0.22215288877487183, Final Batch Loss: 0.09371696412563324\n",
      "Epoch 4753, Loss: 0.36343085765838623, Final Batch Loss: 0.2173730731010437\n",
      "Epoch 4754, Loss: 0.27326586842536926, Final Batch Loss: 0.11875289678573608\n",
      "Epoch 4755, Loss: 0.22636184096336365, Final Batch Loss: 0.08362369239330292\n",
      "Epoch 4756, Loss: 0.30391937494277954, Final Batch Loss: 0.20017361640930176\n",
      "Epoch 4757, Loss: 0.2531450465321541, Final Batch Loss: 0.09951592236757278\n",
      "Epoch 4758, Loss: 0.2572510913014412, Final Batch Loss: 0.1433645486831665\n",
      "Epoch 4759, Loss: 0.28397659957408905, Final Batch Loss: 0.10429123044013977\n",
      "Epoch 4760, Loss: 0.2364094778895378, Final Batch Loss: 0.11929073929786682\n",
      "Epoch 4761, Loss: 0.2492278441786766, Final Batch Loss: 0.11252845078706741\n",
      "Epoch 4762, Loss: 0.28089651465415955, Final Batch Loss: 0.14728887379169464\n",
      "Epoch 4763, Loss: 0.2235856130719185, Final Batch Loss: 0.10533086210489273\n",
      "Epoch 4764, Loss: 0.22285648435354233, Final Batch Loss: 0.12202941626310349\n",
      "Epoch 4765, Loss: 0.30528712272644043, Final Batch Loss: 0.20140734314918518\n",
      "Epoch 4766, Loss: 0.273986279964447, Final Batch Loss: 0.14987769722938538\n",
      "Epoch 4767, Loss: 0.24910657852888107, Final Batch Loss: 0.1277448683977127\n",
      "Epoch 4768, Loss: 0.27547574788331985, Final Batch Loss: 0.161062553524971\n",
      "Epoch 4769, Loss: 0.22867174446582794, Final Batch Loss: 0.10291926562786102\n",
      "Epoch 4770, Loss: 0.24992191046476364, Final Batch Loss: 0.15911218523979187\n",
      "Epoch 4771, Loss: 0.2688361257314682, Final Batch Loss: 0.15693266689777374\n",
      "Epoch 4772, Loss: 0.24224963784217834, Final Batch Loss: 0.12140132486820221\n",
      "Epoch 4773, Loss: 0.20782483369112015, Final Batch Loss: 0.08157186955213547\n",
      "Epoch 4774, Loss: 0.23966600000858307, Final Batch Loss: 0.14224450290203094\n",
      "Epoch 4775, Loss: 0.22169622033834457, Final Batch Loss: 0.10094892978668213\n",
      "Epoch 4776, Loss: 0.21618671715259552, Final Batch Loss: 0.1013648509979248\n",
      "Epoch 4777, Loss: 0.2272278219461441, Final Batch Loss: 0.0734824687242508\n",
      "Epoch 4778, Loss: 0.28560248017311096, Final Batch Loss: 0.1488076150417328\n",
      "Epoch 4779, Loss: 0.28999315202236176, Final Batch Loss: 0.15643882751464844\n",
      "Epoch 4780, Loss: 0.2216905653476715, Final Batch Loss: 0.13158762454986572\n",
      "Epoch 4781, Loss: 0.22149380296468735, Final Batch Loss: 0.10418697446584702\n",
      "Epoch 4782, Loss: 0.2676112577319145, Final Batch Loss: 0.1516602784395218\n",
      "Epoch 4783, Loss: 0.25435182452201843, Final Batch Loss: 0.1572026014328003\n",
      "Epoch 4784, Loss: 0.26799604296684265, Final Batch Loss: 0.16574478149414062\n",
      "Epoch 4785, Loss: 0.23053031414747238, Final Batch Loss: 0.12838849425315857\n",
      "Epoch 4786, Loss: 0.2588263377547264, Final Batch Loss: 0.1354687660932541\n",
      "Epoch 4787, Loss: 0.24450504779815674, Final Batch Loss: 0.08730877935886383\n",
      "Epoch 4788, Loss: 0.25559545308351517, Final Batch Loss: 0.18570108711719513\n",
      "Epoch 4789, Loss: 0.23236623406410217, Final Batch Loss: 0.12682640552520752\n",
      "Epoch 4790, Loss: 0.2269693836569786, Final Batch Loss: 0.11385547369718552\n",
      "Epoch 4791, Loss: 0.21912707388401031, Final Batch Loss: 0.1056365892291069\n",
      "Epoch 4792, Loss: 0.24409162998199463, Final Batch Loss: 0.15592947602272034\n",
      "Epoch 4793, Loss: 0.20058782398700714, Final Batch Loss: 0.07571696490049362\n",
      "Epoch 4794, Loss: 0.24915024638175964, Final Batch Loss: 0.12591737508773804\n",
      "Epoch 4795, Loss: 0.32296110689640045, Final Batch Loss: 0.14778193831443787\n",
      "Epoch 4796, Loss: 0.2066737413406372, Final Batch Loss: 0.08852814137935638\n",
      "Epoch 4797, Loss: 0.25444458425045013, Final Batch Loss: 0.1322513222694397\n",
      "Epoch 4798, Loss: 0.28045280277729034, Final Batch Loss: 0.14539089798927307\n",
      "Epoch 4799, Loss: 0.3421138897538185, Final Batch Loss: 0.2557154595851898\n",
      "Epoch 4800, Loss: 0.24096230417490005, Final Batch Loss: 0.08717133849859238\n",
      "Epoch 4801, Loss: 0.29787255078554153, Final Batch Loss: 0.18267418444156647\n",
      "Epoch 4802, Loss: 0.26215579360723495, Final Batch Loss: 0.10277783125638962\n",
      "Epoch 4803, Loss: 0.20909975469112396, Final Batch Loss: 0.09266497194766998\n",
      "Epoch 4804, Loss: 0.21996111422777176, Final Batch Loss: 0.0945342555642128\n",
      "Epoch 4805, Loss: 0.23390927910804749, Final Batch Loss: 0.14271654188632965\n",
      "Epoch 4806, Loss: 0.24947339296340942, Final Batch Loss: 0.12145425379276276\n",
      "Epoch 4807, Loss: 0.24950441718101501, Final Batch Loss: 0.14329642057418823\n",
      "Epoch 4808, Loss: 0.2175459936261177, Final Batch Loss: 0.062131188809871674\n",
      "Epoch 4809, Loss: 0.20337272435426712, Final Batch Loss: 0.09113140404224396\n",
      "Epoch 4810, Loss: 0.3201952278614044, Final Batch Loss: 0.1708388477563858\n",
      "Epoch 4811, Loss: 0.190852552652359, Final Batch Loss: 0.08898746222257614\n",
      "Epoch 4812, Loss: 0.2292979136109352, Final Batch Loss: 0.1134609654545784\n",
      "Epoch 4813, Loss: 0.2197330966591835, Final Batch Loss: 0.11733321845531464\n",
      "Epoch 4814, Loss: 0.22383390367031097, Final Batch Loss: 0.10679617524147034\n",
      "Epoch 4815, Loss: 0.2760407030582428, Final Batch Loss: 0.14204713702201843\n",
      "Epoch 4816, Loss: 0.33523376286029816, Final Batch Loss: 0.16878075897693634\n",
      "Epoch 4817, Loss: 0.26813001930713654, Final Batch Loss: 0.11794494092464447\n",
      "Epoch 4818, Loss: 0.2119998261332512, Final Batch Loss: 0.07213004678487778\n",
      "Epoch 4819, Loss: 0.242107093334198, Final Batch Loss: 0.11105895042419434\n",
      "Epoch 4820, Loss: 0.2293429672718048, Final Batch Loss: 0.0833355039358139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4821, Loss: 0.21839639544487, Final Batch Loss: 0.09876582026481628\n",
      "Epoch 4822, Loss: 0.21609608829021454, Final Batch Loss: 0.08790947496891022\n",
      "Epoch 4823, Loss: 0.2836606800556183, Final Batch Loss: 0.15678194165229797\n",
      "Epoch 4824, Loss: 0.23359280824661255, Final Batch Loss: 0.11408348381519318\n",
      "Epoch 4825, Loss: 0.22158069908618927, Final Batch Loss: 0.09210249781608582\n",
      "Epoch 4826, Loss: 0.24170396476984024, Final Batch Loss: 0.11422818154096603\n",
      "Epoch 4827, Loss: 0.2566480189561844, Final Batch Loss: 0.14469562470912933\n",
      "Epoch 4828, Loss: 0.24576938897371292, Final Batch Loss: 0.1450120061635971\n",
      "Epoch 4829, Loss: 0.21868564188480377, Final Batch Loss: 0.09510572999715805\n",
      "Epoch 4830, Loss: 0.22230274975299835, Final Batch Loss: 0.1179710328578949\n",
      "Epoch 4831, Loss: 0.3003305569291115, Final Batch Loss: 0.10900267213582993\n",
      "Epoch 4832, Loss: 0.4360816329717636, Final Batch Loss: 0.29498839378356934\n",
      "Epoch 4833, Loss: 0.29116111993789673, Final Batch Loss: 0.13975882530212402\n",
      "Epoch 4834, Loss: 0.23226046562194824, Final Batch Loss: 0.10107855498790741\n",
      "Epoch 4835, Loss: 0.24025440216064453, Final Batch Loss: 0.10764560103416443\n",
      "Epoch 4836, Loss: 0.2172437533736229, Final Batch Loss: 0.08024223893880844\n",
      "Epoch 4837, Loss: 0.28362466394901276, Final Batch Loss: 0.15066033601760864\n",
      "Epoch 4838, Loss: 0.2654418498277664, Final Batch Loss: 0.11517730355262756\n",
      "Epoch 4839, Loss: 0.22833222150802612, Final Batch Loss: 0.1277509331703186\n",
      "Epoch 4840, Loss: 0.2728676274418831, Final Batch Loss: 0.15145690739154816\n",
      "Epoch 4841, Loss: 0.2529173493385315, Final Batch Loss: 0.14469842612743378\n",
      "Epoch 4842, Loss: 0.2313782125711441, Final Batch Loss: 0.10290908813476562\n",
      "Epoch 4843, Loss: 0.2763102129101753, Final Batch Loss: 0.11904080957174301\n",
      "Epoch 4844, Loss: 0.27204982191324234, Final Batch Loss: 0.12319160252809525\n",
      "Epoch 4845, Loss: 0.2476237639784813, Final Batch Loss: 0.13543829321861267\n",
      "Epoch 4846, Loss: 0.26987072080373764, Final Batch Loss: 0.15538759529590607\n",
      "Epoch 4847, Loss: 0.22310727834701538, Final Batch Loss: 0.0918257087469101\n",
      "Epoch 4848, Loss: 0.26989559084177017, Final Batch Loss: 0.16212773323059082\n",
      "Epoch 4849, Loss: 0.2081335410475731, Final Batch Loss: 0.08679068088531494\n",
      "Epoch 4850, Loss: 0.23734907805919647, Final Batch Loss: 0.0933825820684433\n",
      "Epoch 4851, Loss: 0.268022283911705, Final Batch Loss: 0.15718980133533478\n",
      "Epoch 4852, Loss: 0.19476667046546936, Final Batch Loss: 0.09221743047237396\n",
      "Epoch 4853, Loss: 0.2236802726984024, Final Batch Loss: 0.08534897863864899\n",
      "Epoch 4854, Loss: 0.312182292342186, Final Batch Loss: 0.17152833938598633\n",
      "Epoch 4855, Loss: 0.23749926686286926, Final Batch Loss: 0.09772250056266785\n",
      "Epoch 4856, Loss: 0.22122381627559662, Final Batch Loss: 0.11354916542768478\n",
      "Epoch 4857, Loss: 0.29540159553289413, Final Batch Loss: 0.18832354247570038\n",
      "Epoch 4858, Loss: 0.20828071236610413, Final Batch Loss: 0.11008136719465256\n",
      "Epoch 4859, Loss: 0.24408674240112305, Final Batch Loss: 0.12898693978786469\n",
      "Epoch 4860, Loss: 0.2725856527686119, Final Batch Loss: 0.15992552042007446\n",
      "Epoch 4861, Loss: 0.25062452256679535, Final Batch Loss: 0.10372497141361237\n",
      "Epoch 4862, Loss: 0.28759315609931946, Final Batch Loss: 0.15314386785030365\n",
      "Epoch 4863, Loss: 0.2145935744047165, Final Batch Loss: 0.10104210674762726\n",
      "Epoch 4864, Loss: 0.3503855913877487, Final Batch Loss: 0.1957610845565796\n",
      "Epoch 4865, Loss: 0.2744503766298294, Final Batch Loss: 0.16243691742420197\n",
      "Epoch 4866, Loss: 0.23756496608257294, Final Batch Loss: 0.11001691222190857\n",
      "Epoch 4867, Loss: 0.2441817820072174, Final Batch Loss: 0.11830079555511475\n",
      "Epoch 4868, Loss: 0.2491445541381836, Final Batch Loss: 0.13500314950942993\n",
      "Epoch 4869, Loss: 0.2551467716693878, Final Batch Loss: 0.16010187566280365\n",
      "Epoch 4870, Loss: 0.21988819539546967, Final Batch Loss: 0.12085841596126556\n",
      "Epoch 4871, Loss: 0.2149834856390953, Final Batch Loss: 0.06059355288743973\n",
      "Epoch 4872, Loss: 0.3193049281835556, Final Batch Loss: 0.19034215807914734\n",
      "Epoch 4873, Loss: 0.21319547295570374, Final Batch Loss: 0.08463375270366669\n",
      "Epoch 4874, Loss: 0.24561763554811478, Final Batch Loss: 0.10985962301492691\n",
      "Epoch 4875, Loss: 0.2595098316669464, Final Batch Loss: 0.1391042172908783\n",
      "Epoch 4876, Loss: 0.3063976764678955, Final Batch Loss: 0.16165892779827118\n",
      "Epoch 4877, Loss: 0.2732686549425125, Final Batch Loss: 0.13482046127319336\n",
      "Epoch 4878, Loss: 0.3550120145082474, Final Batch Loss: 0.18828967213630676\n",
      "Epoch 4879, Loss: 0.27062302827835083, Final Batch Loss: 0.14827434718608856\n",
      "Epoch 4880, Loss: 0.2735263407230377, Final Batch Loss: 0.1549072563648224\n",
      "Epoch 4881, Loss: 0.2702130973339081, Final Batch Loss: 0.16300225257873535\n",
      "Epoch 4882, Loss: 0.22144395112991333, Final Batch Loss: 0.08297957479953766\n",
      "Epoch 4883, Loss: 0.27800261974334717, Final Batch Loss: 0.1500827670097351\n",
      "Epoch 4884, Loss: 0.24392930418252945, Final Batch Loss: 0.1441640704870224\n",
      "Epoch 4885, Loss: 0.34566357731819153, Final Batch Loss: 0.21174417436122894\n",
      "Epoch 4886, Loss: 0.24724550545215607, Final Batch Loss: 0.10605670511722565\n",
      "Epoch 4887, Loss: 0.20564711093902588, Final Batch Loss: 0.06629076600074768\n",
      "Epoch 4888, Loss: 0.3504297658801079, Final Batch Loss: 0.12166538089513779\n",
      "Epoch 4889, Loss: 0.2484000101685524, Final Batch Loss: 0.09645133465528488\n",
      "Epoch 4890, Loss: 0.23229888826608658, Final Batch Loss: 0.09869126230478287\n",
      "Epoch 4891, Loss: 0.28216156363487244, Final Batch Loss: 0.14352764189243317\n",
      "Epoch 4892, Loss: 0.23673725128173828, Final Batch Loss: 0.1033589094877243\n",
      "Epoch 4893, Loss: 0.25680309534072876, Final Batch Loss: 0.13188354671001434\n",
      "Epoch 4894, Loss: 0.38246507942676544, Final Batch Loss: 0.17295043170452118\n",
      "Epoch 4895, Loss: 0.29333050549030304, Final Batch Loss: 0.15024112164974213\n",
      "Epoch 4896, Loss: 0.24922824651002884, Final Batch Loss: 0.13118651509284973\n",
      "Epoch 4897, Loss: 0.24348613619804382, Final Batch Loss: 0.13141465187072754\n",
      "Epoch 4898, Loss: 0.29296135157346725, Final Batch Loss: 0.19471007585525513\n",
      "Epoch 4899, Loss: 0.2809983193874359, Final Batch Loss: 0.17065438628196716\n",
      "Epoch 4900, Loss: 0.30471719801425934, Final Batch Loss: 0.16154663264751434\n",
      "Epoch 4901, Loss: 0.2615751773118973, Final Batch Loss: 0.13921919465065002\n",
      "Epoch 4902, Loss: 0.23946482688188553, Final Batch Loss: 0.13975141942501068\n",
      "Epoch 4903, Loss: 0.2374689131975174, Final Batch Loss: 0.10776820778846741\n",
      "Epoch 4904, Loss: 0.2389545813202858, Final Batch Loss: 0.0793384537100792\n",
      "Epoch 4905, Loss: 0.2350343093276024, Final Batch Loss: 0.09396732598543167\n",
      "Epoch 4906, Loss: 0.26479873061180115, Final Batch Loss: 0.1441093534231186\n",
      "Epoch 4907, Loss: 0.23076846450567245, Final Batch Loss: 0.091868095099926\n",
      "Epoch 4908, Loss: 0.23500754684209824, Final Batch Loss: 0.11016463488340378\n",
      "Epoch 4909, Loss: 0.3266527056694031, Final Batch Loss: 0.20933473110198975\n",
      "Epoch 4910, Loss: 0.24994079023599625, Final Batch Loss: 0.13887712359428406\n",
      "Epoch 4911, Loss: 0.252585232257843, Final Batch Loss: 0.12705783545970917\n",
      "Epoch 4912, Loss: 0.24123411625623703, Final Batch Loss: 0.14686855673789978\n",
      "Epoch 4913, Loss: 0.259159117937088, Final Batch Loss: 0.12959598004817963\n",
      "Epoch 4914, Loss: 0.20385188609361649, Final Batch Loss: 0.09791134297847748\n",
      "Epoch 4915, Loss: 0.25660058856010437, Final Batch Loss: 0.12906718254089355\n",
      "Epoch 4916, Loss: 0.25293588638305664, Final Batch Loss: 0.11278320848941803\n",
      "Epoch 4917, Loss: 0.25441640615463257, Final Batch Loss: 0.15705057978630066\n",
      "Epoch 4918, Loss: 0.23313351720571518, Final Batch Loss: 0.09728514403104782\n",
      "Epoch 4919, Loss: 0.2138439118862152, Final Batch Loss: 0.13411939144134521\n",
      "Epoch 4920, Loss: 0.24246400594711304, Final Batch Loss: 0.1391604095697403\n",
      "Epoch 4921, Loss: 0.2762042284011841, Final Batch Loss: 0.1469486504793167\n",
      "Epoch 4922, Loss: 0.22052286565303802, Final Batch Loss: 0.10097823292016983\n",
      "Epoch 4923, Loss: 0.2584920898079872, Final Batch Loss: 0.14565356075763702\n",
      "Epoch 4924, Loss: 0.25862742960453033, Final Batch Loss: 0.12740562856197357\n",
      "Epoch 4925, Loss: 0.27889157831668854, Final Batch Loss: 0.09813255071640015\n",
      "Epoch 4926, Loss: 0.25578737258911133, Final Batch Loss: 0.12561698257923126\n",
      "Epoch 4927, Loss: 0.2382398247718811, Final Batch Loss: 0.08672469854354858\n",
      "Epoch 4928, Loss: 0.2077937126159668, Final Batch Loss: 0.08452457934617996\n",
      "Epoch 4929, Loss: 0.21092195808887482, Final Batch Loss: 0.08338825404644012\n",
      "Epoch 4930, Loss: 0.23432380706071854, Final Batch Loss: 0.12609945237636566\n",
      "Epoch 4931, Loss: 0.2582509219646454, Final Batch Loss: 0.1783985048532486\n",
      "Epoch 4932, Loss: 0.30378298461437225, Final Batch Loss: 0.177388533949852\n",
      "Epoch 4933, Loss: 0.23940172046422958, Final Batch Loss: 0.13468952476978302\n",
      "Epoch 4934, Loss: 0.21027425676584244, Final Batch Loss: 0.10739613324403763\n",
      "Epoch 4935, Loss: 0.2674202099442482, Final Batch Loss: 0.1214294359087944\n",
      "Epoch 4936, Loss: 0.23875509947538376, Final Batch Loss: 0.10124150663614273\n",
      "Epoch 4937, Loss: 0.2308785542845726, Final Batch Loss: 0.08845467120409012\n",
      "Epoch 4938, Loss: 0.27395715564489365, Final Batch Loss: 0.15056931972503662\n",
      "Epoch 4939, Loss: 0.310099259018898, Final Batch Loss: 0.1521356701850891\n",
      "Epoch 4940, Loss: 0.3062632381916046, Final Batch Loss: 0.18981720507144928\n",
      "Epoch 4941, Loss: 0.18047720193862915, Final Batch Loss: 0.08334188908338547\n",
      "Epoch 4942, Loss: 0.23005720227956772, Final Batch Loss: 0.12592291831970215\n",
      "Epoch 4943, Loss: 0.31028829514980316, Final Batch Loss: 0.15895269811153412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4944, Loss: 0.2510925158858299, Final Batch Loss: 0.10000302642583847\n",
      "Epoch 4945, Loss: 0.2279646322131157, Final Batch Loss: 0.11211151629686356\n",
      "Epoch 4946, Loss: 0.3250076621770859, Final Batch Loss: 0.1890028417110443\n",
      "Epoch 4947, Loss: 0.292865589261055, Final Batch Loss: 0.14968234300613403\n",
      "Epoch 4948, Loss: 0.24143611639738083, Final Batch Loss: 0.1362287998199463\n",
      "Epoch 4949, Loss: 0.21271295100450516, Final Batch Loss: 0.09651457518339157\n",
      "Epoch 4950, Loss: 0.4058024659752846, Final Batch Loss: 0.2847570776939392\n",
      "Epoch 4951, Loss: 0.24725830554962158, Final Batch Loss: 0.11799374222755432\n",
      "Epoch 4952, Loss: 0.28911449015140533, Final Batch Loss: 0.16255240142345428\n",
      "Epoch 4953, Loss: 0.24988631904125214, Final Batch Loss: 0.14566205441951752\n",
      "Epoch 4954, Loss: 0.2854986637830734, Final Batch Loss: 0.15534867346286774\n",
      "Epoch 4955, Loss: 0.31068871915340424, Final Batch Loss: 0.1865766942501068\n",
      "Epoch 4956, Loss: 0.27199646830558777, Final Batch Loss: 0.1523510217666626\n",
      "Epoch 4957, Loss: 0.2227664440870285, Final Batch Loss: 0.13205407559871674\n",
      "Epoch 4958, Loss: 0.29320189356803894, Final Batch Loss: 0.16363035142421722\n",
      "Epoch 4959, Loss: 0.24403272569179535, Final Batch Loss: 0.09791573882102966\n",
      "Epoch 4960, Loss: 0.2181192860007286, Final Batch Loss: 0.11159607768058777\n",
      "Epoch 4961, Loss: 0.24245473742485046, Final Batch Loss: 0.12400555610656738\n",
      "Epoch 4962, Loss: 0.23201189935207367, Final Batch Loss: 0.07632063329219818\n",
      "Epoch 4963, Loss: 0.24549449980258942, Final Batch Loss: 0.10349248349666595\n",
      "Epoch 4964, Loss: 0.3326880857348442, Final Batch Loss: 0.22124719619750977\n",
      "Epoch 4965, Loss: 0.25752441585063934, Final Batch Loss: 0.12758836150169373\n",
      "Epoch 4966, Loss: 0.2655804380774498, Final Batch Loss: 0.1078970804810524\n",
      "Epoch 4967, Loss: 0.22912407666444778, Final Batch Loss: 0.08226115256547928\n",
      "Epoch 4968, Loss: 0.2215435728430748, Final Batch Loss: 0.10026568919420242\n",
      "Epoch 4969, Loss: 0.22257080674171448, Final Batch Loss: 0.09724034368991852\n",
      "Epoch 4970, Loss: 0.2525385767221451, Final Batch Loss: 0.11748345196247101\n",
      "Epoch 4971, Loss: 0.24911724776029587, Final Batch Loss: 0.11961814016103745\n",
      "Epoch 4972, Loss: 0.3373985290527344, Final Batch Loss: 0.14040081202983856\n",
      "Epoch 4973, Loss: 0.3042222484946251, Final Batch Loss: 0.18865244090557098\n",
      "Epoch 4974, Loss: 0.24530210345983505, Final Batch Loss: 0.10221008211374283\n",
      "Epoch 4975, Loss: 0.21927613765001297, Final Batch Loss: 0.12154499441385269\n",
      "Epoch 4976, Loss: 0.23125478625297546, Final Batch Loss: 0.13147753477096558\n",
      "Epoch 4977, Loss: 0.24506189674139023, Final Batch Loss: 0.14302870631217957\n",
      "Epoch 4978, Loss: 0.2182411178946495, Final Batch Loss: 0.0788789615035057\n",
      "Epoch 4979, Loss: 0.23078156262636185, Final Batch Loss: 0.1030755266547203\n",
      "Epoch 4980, Loss: 0.1936606913805008, Final Batch Loss: 0.0687144473195076\n",
      "Epoch 4981, Loss: 0.2739781513810158, Final Batch Loss: 0.1505245864391327\n",
      "Epoch 4982, Loss: 0.2344142347574234, Final Batch Loss: 0.12362577021121979\n",
      "Epoch 4983, Loss: 0.3378738462924957, Final Batch Loss: 0.1711946725845337\n",
      "Epoch 4984, Loss: 0.3402922749519348, Final Batch Loss: 0.22381356358528137\n",
      "Epoch 4985, Loss: 0.2079768404364586, Final Batch Loss: 0.08496435731649399\n",
      "Epoch 4986, Loss: 0.2189219668507576, Final Batch Loss: 0.0944414883852005\n",
      "Epoch 4987, Loss: 0.23748161643743515, Final Batch Loss: 0.11332269012928009\n",
      "Epoch 4988, Loss: 0.25998876988887787, Final Batch Loss: 0.17014503479003906\n",
      "Epoch 4989, Loss: 0.2551659420132637, Final Batch Loss: 0.1094612255692482\n",
      "Epoch 4990, Loss: 0.2318255379796028, Final Batch Loss: 0.11700019985437393\n",
      "Epoch 4991, Loss: 0.22996718436479568, Final Batch Loss: 0.09446109086275101\n",
      "Epoch 4992, Loss: 0.28513480722904205, Final Batch Loss: 0.1577751189470291\n",
      "Epoch 4993, Loss: 0.3146899864077568, Final Batch Loss: 0.20827308297157288\n",
      "Epoch 4994, Loss: 0.2680792361497879, Final Batch Loss: 0.16104798018932343\n",
      "Epoch 4995, Loss: 0.2799237221479416, Final Batch Loss: 0.17223167419433594\n",
      "Epoch 4996, Loss: 0.3113473132252693, Final Batch Loss: 0.1204601302742958\n",
      "Epoch 4997, Loss: 0.20212280005216599, Final Batch Loss: 0.08519138395786285\n",
      "Epoch 4998, Loss: 0.24464859813451767, Final Batch Loss: 0.10205329209566116\n",
      "Epoch 4999, Loss: 0.21561677008867264, Final Batch Loss: 0.09952431172132492\n",
      "Epoch 5000, Loss: 0.3085460662841797, Final Batch Loss: 0.16036070883274078\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0  0  0  0  0  0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0]\n",
      " [ 0  2  0  0  3  0  0  0  0]\n",
      " [ 0  0  1  0  0  7  0  0  5]\n",
      " [ 0  0  0  0  0  0 15  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    0.87500   1.00000   0.93333        14\n",
      "           2    0.88889   1.00000   0.94118         8\n",
      "           3    1.00000   1.00000   1.00000        11\n",
      "           4    1.00000   0.60000   0.75000         5\n",
      "           5    1.00000   0.53846   0.70000        13\n",
      "           6    1.00000   1.00000   1.00000        15\n",
      "           7    1.00000   1.00000   1.00000         9\n",
      "           8    0.54545   1.00000   0.70588         6\n",
      "\n",
      "    accuracy                        0.91011        89\n",
      "   macro avg    0.92326   0.90427   0.89227        89\n",
      "weighted avg    0.93971   0.91011   0.90653        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Group 2_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0]\n",
      " [ 0 18  0  0  2  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0  0  0]\n",
      " [ 0  0  0 17  0  0  1  2  0]\n",
      " [ 2  1  0  0 17  0  0  0  0]\n",
      " [ 0  0  4  0  0 10  0  0  6]\n",
      " [ 0  0  0  2  0  0 18  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  1  0  0  1  0  0 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.90909   1.00000   0.95238        20\n",
      "         1.0    0.94737   0.90000   0.92308        20\n",
      "         2.0    0.80000   1.00000   0.88889        20\n",
      "         3.0    0.89474   0.85000   0.87179        20\n",
      "         4.0    0.89474   0.85000   0.87179        20\n",
      "         5.0    0.90909   0.50000   0.64516        20\n",
      "         6.0    0.94737   0.90000   0.92308        20\n",
      "         7.0    0.90909   1.00000   0.95238        20\n",
      "         8.0    0.75000   0.90000   0.81818        20\n",
      "\n",
      "    accuracy                        0.87778       180\n",
      "   macro avg    0.88461   0.87778   0.87186       180\n",
      "weighted avg    0.88461   0.87778   0.87186       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
