{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [14, 15, 17]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.433544635772705, Final Batch Loss: 2.2106847763061523\n",
      "Epoch 2, Loss: 4.440515041351318, Final Batch Loss: 2.23502254486084\n",
      "Epoch 3, Loss: 4.430473566055298, Final Batch Loss: 2.2213962078094482\n",
      "Epoch 4, Loss: 4.431300163269043, Final Batch Loss: 2.225736141204834\n",
      "Epoch 5, Loss: 4.411062955856323, Final Batch Loss: 2.1857118606567383\n",
      "Epoch 6, Loss: 4.423179864883423, Final Batch Loss: 2.2166759967803955\n",
      "Epoch 7, Loss: 4.418940782546997, Final Batch Loss: 2.2199785709381104\n",
      "Epoch 8, Loss: 4.411174297332764, Final Batch Loss: 2.206281900405884\n",
      "Epoch 9, Loss: 4.403214931488037, Final Batch Loss: 2.1941311359405518\n",
      "Epoch 10, Loss: 4.405670166015625, Final Batch Loss: 2.210071563720703\n",
      "Epoch 11, Loss: 4.397736310958862, Final Batch Loss: 2.2031235694885254\n",
      "Epoch 12, Loss: 4.385516881942749, Final Batch Loss: 2.1927988529205322\n",
      "Epoch 13, Loss: 4.3763039112091064, Final Batch Loss: 2.182901620864868\n",
      "Epoch 14, Loss: 4.361780881881714, Final Batch Loss: 2.1724157333374023\n",
      "Epoch 15, Loss: 4.356224060058594, Final Batch Loss: 2.181645631790161\n",
      "Epoch 16, Loss: 4.33606219291687, Final Batch Loss: 2.166692018508911\n",
      "Epoch 17, Loss: 4.3046159744262695, Final Batch Loss: 2.135885238647461\n",
      "Epoch 18, Loss: 4.292308568954468, Final Batch Loss: 2.142784357070923\n",
      "Epoch 19, Loss: 4.272319078445435, Final Batch Loss: 2.1380908489227295\n",
      "Epoch 20, Loss: 4.24636697769165, Final Batch Loss: 2.1208343505859375\n",
      "Epoch 21, Loss: 4.223121643066406, Final Batch Loss: 2.111549139022827\n",
      "Epoch 22, Loss: 4.178470849990845, Final Batch Loss: 2.0867276191711426\n",
      "Epoch 23, Loss: 4.140065431594849, Final Batch Loss: 2.0620479583740234\n",
      "Epoch 24, Loss: 4.112517356872559, Final Batch Loss: 2.0489065647125244\n",
      "Epoch 25, Loss: 4.090575456619263, Final Batch Loss: 2.05256986618042\n",
      "Epoch 26, Loss: 4.058299541473389, Final Batch Loss: 2.0482993125915527\n",
      "Epoch 27, Loss: 3.9926657676696777, Final Batch Loss: 1.9672200679779053\n",
      "Epoch 28, Loss: 3.9535685777664185, Final Batch Loss: 1.9773728847503662\n",
      "Epoch 29, Loss: 3.9473769664764404, Final Batch Loss: 1.9681340456008911\n",
      "Epoch 30, Loss: 3.9062743186950684, Final Batch Loss: 1.9421876668930054\n",
      "Epoch 31, Loss: 3.823103666305542, Final Batch Loss: 1.9283021688461304\n",
      "Epoch 32, Loss: 3.818213939666748, Final Batch Loss: 1.9175217151641846\n",
      "Epoch 33, Loss: 3.760002851486206, Final Batch Loss: 1.88384211063385\n",
      "Epoch 34, Loss: 3.736879587173462, Final Batch Loss: 1.8706014156341553\n",
      "Epoch 35, Loss: 3.6937848329544067, Final Batch Loss: 1.9092620611190796\n",
      "Epoch 36, Loss: 3.632704019546509, Final Batch Loss: 1.7741894721984863\n",
      "Epoch 37, Loss: 3.6278926134109497, Final Batch Loss: 1.8231041431427002\n",
      "Epoch 38, Loss: 3.6318105459213257, Final Batch Loss: 1.8444668054580688\n",
      "Epoch 39, Loss: 3.5463333129882812, Final Batch Loss: 1.7599936723709106\n",
      "Epoch 40, Loss: 3.5416353940963745, Final Batch Loss: 1.7687402963638306\n",
      "Epoch 41, Loss: 3.501674771308899, Final Batch Loss: 1.71416175365448\n",
      "Epoch 42, Loss: 3.4480388164520264, Final Batch Loss: 1.7157574892044067\n",
      "Epoch 43, Loss: 3.398285746574402, Final Batch Loss: 1.6656999588012695\n",
      "Epoch 44, Loss: 3.3704710006713867, Final Batch Loss: 1.6695395708084106\n",
      "Epoch 45, Loss: 3.355565071105957, Final Batch Loss: 1.6803792715072632\n",
      "Epoch 46, Loss: 3.3018062114715576, Final Batch Loss: 1.6351274251937866\n",
      "Epoch 47, Loss: 3.2763389348983765, Final Batch Loss: 1.604872703552246\n",
      "Epoch 48, Loss: 3.1771106719970703, Final Batch Loss: 1.565991997718811\n",
      "Epoch 49, Loss: 3.1793776750564575, Final Batch Loss: 1.5447243452072144\n",
      "Epoch 50, Loss: 3.1270647048950195, Final Batch Loss: 1.5552451610565186\n",
      "Epoch 51, Loss: 3.064597487449646, Final Batch Loss: 1.5180593729019165\n",
      "Epoch 52, Loss: 3.055097222328186, Final Batch Loss: 1.511507272720337\n",
      "Epoch 53, Loss: 3.026987671852112, Final Batch Loss: 1.5215331315994263\n",
      "Epoch 54, Loss: 2.9931851625442505, Final Batch Loss: 1.4686626195907593\n",
      "Epoch 55, Loss: 2.931969165802002, Final Batch Loss: 1.4547542333602905\n",
      "Epoch 56, Loss: 2.9376827478408813, Final Batch Loss: 1.4709043502807617\n",
      "Epoch 57, Loss: 2.896510601043701, Final Batch Loss: 1.4695309400558472\n",
      "Epoch 58, Loss: 2.8970978260040283, Final Batch Loss: 1.448346495628357\n",
      "Epoch 59, Loss: 2.7280770540237427, Final Batch Loss: 1.351772665977478\n",
      "Epoch 60, Loss: 2.8072036504745483, Final Batch Loss: 1.3859857320785522\n",
      "Epoch 61, Loss: 2.7102582454681396, Final Batch Loss: 1.3239326477050781\n",
      "Epoch 62, Loss: 2.707622766494751, Final Batch Loss: 1.3684558868408203\n",
      "Epoch 63, Loss: 2.655377745628357, Final Batch Loss: 1.3277256488800049\n",
      "Epoch 64, Loss: 2.596126675605774, Final Batch Loss: 1.2377814054489136\n",
      "Epoch 65, Loss: 2.6071035861968994, Final Batch Loss: 1.2595750093460083\n",
      "Epoch 66, Loss: 2.5746549367904663, Final Batch Loss: 1.2732610702514648\n",
      "Epoch 67, Loss: 2.5867788791656494, Final Batch Loss: 1.2878392934799194\n",
      "Epoch 68, Loss: 2.5340099334716797, Final Batch Loss: 1.2645628452301025\n",
      "Epoch 69, Loss: 2.497702479362488, Final Batch Loss: 1.2453559637069702\n",
      "Epoch 70, Loss: 2.481310725212097, Final Batch Loss: 1.2530275583267212\n",
      "Epoch 71, Loss: 2.4442723989486694, Final Batch Loss: 1.1916385889053345\n",
      "Epoch 72, Loss: 2.363888740539551, Final Batch Loss: 1.1366385221481323\n",
      "Epoch 73, Loss: 2.376744031906128, Final Batch Loss: 1.192071557044983\n",
      "Epoch 74, Loss: 2.4024877548217773, Final Batch Loss: 1.2554925680160522\n",
      "Epoch 75, Loss: 2.3487385511398315, Final Batch Loss: 1.2011160850524902\n",
      "Epoch 76, Loss: 2.299431085586548, Final Batch Loss: 1.1816157102584839\n",
      "Epoch 77, Loss: 2.317203998565674, Final Batch Loss: 1.1275129318237305\n",
      "Epoch 78, Loss: 2.3248186111450195, Final Batch Loss: 1.1472548246383667\n",
      "Epoch 79, Loss: 2.2926743030548096, Final Batch Loss: 1.1645256280899048\n",
      "Epoch 80, Loss: 2.25823175907135, Final Batch Loss: 1.112299919128418\n",
      "Epoch 81, Loss: 2.193604588508606, Final Batch Loss: 1.0861176252365112\n",
      "Epoch 82, Loss: 2.2348291873931885, Final Batch Loss: 1.1446021795272827\n",
      "Epoch 83, Loss: 2.1163583993911743, Final Batch Loss: 1.0534512996673584\n",
      "Epoch 84, Loss: 2.1857908964157104, Final Batch Loss: 1.1233669519424438\n",
      "Epoch 85, Loss: 2.260771155357361, Final Batch Loss: 1.1341463327407837\n",
      "Epoch 86, Loss: 2.121604800224304, Final Batch Loss: 1.0165715217590332\n",
      "Epoch 87, Loss: 2.15731680393219, Final Batch Loss: 1.0390818119049072\n",
      "Epoch 88, Loss: 2.0743719339370728, Final Batch Loss: 1.0185520648956299\n",
      "Epoch 89, Loss: 2.062557280063629, Final Batch Loss: 0.9876477122306824\n",
      "Epoch 90, Loss: 2.0508073568344116, Final Batch Loss: 1.0053220987319946\n",
      "Epoch 91, Loss: 2.0258527994155884, Final Batch Loss: 0.9864810705184937\n",
      "Epoch 92, Loss: 1.9738275408744812, Final Batch Loss: 1.009069800376892\n",
      "Epoch 93, Loss: 1.9911850690841675, Final Batch Loss: 1.0041759014129639\n",
      "Epoch 94, Loss: 2.0308775901794434, Final Batch Loss: 0.9987468719482422\n",
      "Epoch 95, Loss: 2.033498466014862, Final Batch Loss: 1.061435580253601\n",
      "Epoch 96, Loss: 1.928068459033966, Final Batch Loss: 0.9552526473999023\n",
      "Epoch 97, Loss: 1.918577492237091, Final Batch Loss: 0.963097333908081\n",
      "Epoch 98, Loss: 1.9409834742546082, Final Batch Loss: 0.9873397946357727\n",
      "Epoch 99, Loss: 2.037199854850769, Final Batch Loss: 1.0756899118423462\n",
      "Epoch 100, Loss: 1.9330030679702759, Final Batch Loss: 0.9615010619163513\n",
      "Epoch 101, Loss: 1.889230191707611, Final Batch Loss: 0.9532896280288696\n",
      "Epoch 102, Loss: 1.8553924560546875, Final Batch Loss: 0.8936256766319275\n",
      "Epoch 103, Loss: 2.007330894470215, Final Batch Loss: 1.0380229949951172\n",
      "Epoch 104, Loss: 1.8617379069328308, Final Batch Loss: 0.9384682178497314\n",
      "Epoch 105, Loss: 1.8959293961524963, Final Batch Loss: 0.967432975769043\n",
      "Epoch 106, Loss: 1.796132504940033, Final Batch Loss: 0.8517695069313049\n",
      "Epoch 107, Loss: 1.8839284181594849, Final Batch Loss: 0.954685389995575\n",
      "Epoch 108, Loss: 1.8636584877967834, Final Batch Loss: 0.9810308814048767\n",
      "Epoch 109, Loss: 1.8079334497451782, Final Batch Loss: 0.9341930747032166\n",
      "Epoch 110, Loss: 1.8449691534042358, Final Batch Loss: 0.9157922863960266\n",
      "Epoch 111, Loss: 1.8032828569412231, Final Batch Loss: 0.8872469663619995\n",
      "Epoch 112, Loss: 1.8660351037979126, Final Batch Loss: 0.952824592590332\n",
      "Epoch 113, Loss: 1.7925999164581299, Final Batch Loss: 0.8968725800514221\n",
      "Epoch 114, Loss: 1.8452108502388, Final Batch Loss: 0.8943169713020325\n",
      "Epoch 115, Loss: 1.778890073299408, Final Batch Loss: 0.9064716696739197\n",
      "Epoch 116, Loss: 1.791445553302765, Final Batch Loss: 0.9121387004852295\n",
      "Epoch 117, Loss: 1.8021984696388245, Final Batch Loss: 0.9098436832427979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 1.7083322405815125, Final Batch Loss: 0.778342604637146\n",
      "Epoch 119, Loss: 1.6098656058311462, Final Batch Loss: 0.7400095462799072\n",
      "Epoch 120, Loss: 1.7235711216926575, Final Batch Loss: 0.8953996300697327\n",
      "Epoch 121, Loss: 1.7585602402687073, Final Batch Loss: 0.8835165500640869\n",
      "Epoch 122, Loss: 1.6039740443229675, Final Batch Loss: 0.8180528879165649\n",
      "Epoch 123, Loss: 1.7446955442428589, Final Batch Loss: 0.9221684336662292\n",
      "Epoch 124, Loss: 1.650002121925354, Final Batch Loss: 0.8258370757102966\n",
      "Epoch 125, Loss: 1.7423155307769775, Final Batch Loss: 0.9180525541305542\n",
      "Epoch 126, Loss: 1.6701422929763794, Final Batch Loss: 0.8649399876594543\n",
      "Epoch 127, Loss: 1.7883656024932861, Final Batch Loss: 0.9268866181373596\n",
      "Epoch 128, Loss: 1.6657465100288391, Final Batch Loss: 0.7925640940666199\n",
      "Epoch 129, Loss: 1.6806427240371704, Final Batch Loss: 0.8323060870170593\n",
      "Epoch 130, Loss: 1.5882763862609863, Final Batch Loss: 0.794772207736969\n",
      "Epoch 131, Loss: 1.7139949798583984, Final Batch Loss: 0.9369204044342041\n",
      "Epoch 132, Loss: 1.6295366287231445, Final Batch Loss: 0.8056827187538147\n",
      "Epoch 133, Loss: 1.6088985204696655, Final Batch Loss: 0.7731955051422119\n",
      "Epoch 134, Loss: 1.6088367700576782, Final Batch Loss: 0.779823899269104\n",
      "Epoch 135, Loss: 1.5266406536102295, Final Batch Loss: 0.7394058108329773\n",
      "Epoch 136, Loss: 1.684560775756836, Final Batch Loss: 0.875037670135498\n",
      "Epoch 137, Loss: 1.6479961276054382, Final Batch Loss: 0.8744520545005798\n",
      "Epoch 138, Loss: 1.6321567296981812, Final Batch Loss: 0.8303858637809753\n",
      "Epoch 139, Loss: 1.5656852722167969, Final Batch Loss: 0.7673170566558838\n",
      "Epoch 140, Loss: 1.5726401209831238, Final Batch Loss: 0.7677934765815735\n",
      "Epoch 141, Loss: 1.5662162899971008, Final Batch Loss: 0.8031190037727356\n",
      "Epoch 142, Loss: 1.609165608882904, Final Batch Loss: 0.8279657363891602\n",
      "Epoch 143, Loss: 1.5223743915557861, Final Batch Loss: 0.7229390740394592\n",
      "Epoch 144, Loss: 1.5756816267967224, Final Batch Loss: 0.7871185541152954\n",
      "Epoch 145, Loss: 1.556094229221344, Final Batch Loss: 0.6940483450889587\n",
      "Epoch 146, Loss: 1.6624521613121033, Final Batch Loss: 0.815081000328064\n",
      "Epoch 147, Loss: 1.5622023344039917, Final Batch Loss: 0.7737146019935608\n",
      "Epoch 148, Loss: 1.549436867237091, Final Batch Loss: 0.761614203453064\n",
      "Epoch 149, Loss: 1.5475154519081116, Final Batch Loss: 0.8136876821517944\n",
      "Epoch 150, Loss: 1.5129393935203552, Final Batch Loss: 0.7049844264984131\n",
      "Epoch 151, Loss: 1.484586775302887, Final Batch Loss: 0.7721416354179382\n",
      "Epoch 152, Loss: 1.4990780353546143, Final Batch Loss: 0.7495684623718262\n",
      "Epoch 153, Loss: 1.4605163931846619, Final Batch Loss: 0.7348713278770447\n",
      "Epoch 154, Loss: 1.4716174006462097, Final Batch Loss: 0.7596693634986877\n",
      "Epoch 155, Loss: 1.4688768982887268, Final Batch Loss: 0.7404568195343018\n",
      "Epoch 156, Loss: 1.4791122078895569, Final Batch Loss: 0.731866180896759\n",
      "Epoch 157, Loss: 1.4609026312828064, Final Batch Loss: 0.7280868887901306\n",
      "Epoch 158, Loss: 1.4355219006538391, Final Batch Loss: 0.6412698030471802\n",
      "Epoch 159, Loss: 1.5608163475990295, Final Batch Loss: 0.7654533386230469\n",
      "Epoch 160, Loss: 1.4975948333740234, Final Batch Loss: 0.7190890312194824\n",
      "Epoch 161, Loss: 1.3915343880653381, Final Batch Loss: 0.6979779601097107\n",
      "Epoch 162, Loss: 1.521025836467743, Final Batch Loss: 0.7781112790107727\n",
      "Epoch 163, Loss: 1.4121921062469482, Final Batch Loss: 0.6461109519004822\n",
      "Epoch 164, Loss: 1.507130205631256, Final Batch Loss: 0.7909200191497803\n",
      "Epoch 165, Loss: 1.3882999420166016, Final Batch Loss: 0.7040829062461853\n",
      "Epoch 166, Loss: 1.3615970015525818, Final Batch Loss: 0.6682581901550293\n",
      "Epoch 167, Loss: 1.4155978560447693, Final Batch Loss: 0.7182526588439941\n",
      "Epoch 168, Loss: 1.572474479675293, Final Batch Loss: 0.8552826642990112\n",
      "Epoch 169, Loss: 1.426087200641632, Final Batch Loss: 0.6832307577133179\n",
      "Epoch 170, Loss: 1.4309208989143372, Final Batch Loss: 0.7070919275283813\n",
      "Epoch 171, Loss: 1.4089406728744507, Final Batch Loss: 0.7206258177757263\n",
      "Epoch 172, Loss: 1.3035146594047546, Final Batch Loss: 0.6723593473434448\n",
      "Epoch 173, Loss: 1.4010698795318604, Final Batch Loss: 0.7190003991127014\n",
      "Epoch 174, Loss: 1.4757338762283325, Final Batch Loss: 0.7098614573478699\n",
      "Epoch 175, Loss: 1.3449742794036865, Final Batch Loss: 0.6351073384284973\n",
      "Epoch 176, Loss: 1.3801644444465637, Final Batch Loss: 0.6931485533714294\n",
      "Epoch 177, Loss: 1.2778668999671936, Final Batch Loss: 0.6283700466156006\n",
      "Epoch 178, Loss: 1.364521324634552, Final Batch Loss: 0.7020614147186279\n",
      "Epoch 179, Loss: 1.3480345010757446, Final Batch Loss: 0.6363935470581055\n",
      "Epoch 180, Loss: 1.3177281022071838, Final Batch Loss: 0.6686122417449951\n",
      "Epoch 181, Loss: 1.2481632232666016, Final Batch Loss: 0.6267899870872498\n",
      "Epoch 182, Loss: 1.3509162068367004, Final Batch Loss: 0.651590883731842\n",
      "Epoch 183, Loss: 1.3219234347343445, Final Batch Loss: 0.6046847105026245\n",
      "Epoch 184, Loss: 1.3038456439971924, Final Batch Loss: 0.6888157725334167\n",
      "Epoch 185, Loss: 1.256477177143097, Final Batch Loss: 0.6509310603141785\n",
      "Epoch 186, Loss: 1.3719670176506042, Final Batch Loss: 0.7428257465362549\n",
      "Epoch 187, Loss: 1.3343222737312317, Final Batch Loss: 0.6559358835220337\n",
      "Epoch 188, Loss: 1.3097531199455261, Final Batch Loss: 0.6817412376403809\n",
      "Epoch 189, Loss: 1.3034701347351074, Final Batch Loss: 0.6814445853233337\n",
      "Epoch 190, Loss: 1.2887018322944641, Final Batch Loss: 0.6734951734542847\n",
      "Epoch 191, Loss: 1.1894028186798096, Final Batch Loss: 0.6181297302246094\n",
      "Epoch 192, Loss: 1.246012806892395, Final Batch Loss: 0.6098328828811646\n",
      "Epoch 193, Loss: 1.2959179282188416, Final Batch Loss: 0.6827146410942078\n",
      "Epoch 194, Loss: 1.329526662826538, Final Batch Loss: 0.6808297634124756\n",
      "Epoch 195, Loss: 1.357515811920166, Final Batch Loss: 0.6677258610725403\n",
      "Epoch 196, Loss: 1.2858965396881104, Final Batch Loss: 0.6470426917076111\n",
      "Epoch 197, Loss: 1.2738560438156128, Final Batch Loss: 0.6680319905281067\n",
      "Epoch 198, Loss: 1.17603999376297, Final Batch Loss: 0.5811457633972168\n",
      "Epoch 199, Loss: 1.2027319073677063, Final Batch Loss: 0.5941504240036011\n",
      "Epoch 200, Loss: 1.2847042083740234, Final Batch Loss: 0.6601455211639404\n",
      "Epoch 201, Loss: 1.2582008242607117, Final Batch Loss: 0.6267085671424866\n",
      "Epoch 202, Loss: 1.2491260170936584, Final Batch Loss: 0.5970192551612854\n",
      "Epoch 203, Loss: 1.24025297164917, Final Batch Loss: 0.6698452234268188\n",
      "Epoch 204, Loss: 1.2026285529136658, Final Batch Loss: 0.6068428754806519\n",
      "Epoch 205, Loss: 1.2339853644371033, Final Batch Loss: 0.6161131262779236\n",
      "Epoch 206, Loss: 1.2336925864219666, Final Batch Loss: 0.5646533370018005\n",
      "Epoch 207, Loss: 1.1789182424545288, Final Batch Loss: 0.5834243893623352\n",
      "Epoch 208, Loss: 1.198824167251587, Final Batch Loss: 0.6002220511436462\n",
      "Epoch 209, Loss: 1.2317331433296204, Final Batch Loss: 0.6363941431045532\n",
      "Epoch 210, Loss: 1.221441924571991, Final Batch Loss: 0.5328576564788818\n",
      "Epoch 211, Loss: 1.1360029578208923, Final Batch Loss: 0.5635957717895508\n",
      "Epoch 212, Loss: 1.2571191787719727, Final Batch Loss: 0.6640875339508057\n",
      "Epoch 213, Loss: 1.2052364945411682, Final Batch Loss: 0.6554014682769775\n",
      "Epoch 214, Loss: 1.2644506692886353, Final Batch Loss: 0.6487662196159363\n",
      "Epoch 215, Loss: 1.0800113677978516, Final Batch Loss: 0.5055050849914551\n",
      "Epoch 216, Loss: 1.180840790271759, Final Batch Loss: 0.556415319442749\n",
      "Epoch 217, Loss: 1.2846688032150269, Final Batch Loss: 0.6334893703460693\n",
      "Epoch 218, Loss: 1.1572396755218506, Final Batch Loss: 0.533044159412384\n",
      "Epoch 219, Loss: 1.1248749494552612, Final Batch Loss: 0.5582379102706909\n",
      "Epoch 220, Loss: 1.224930465221405, Final Batch Loss: 0.5712748169898987\n",
      "Epoch 221, Loss: 1.173941731452942, Final Batch Loss: 0.5923933982849121\n",
      "Epoch 222, Loss: 1.1541576385498047, Final Batch Loss: 0.5719206929206848\n",
      "Epoch 223, Loss: 1.057027667760849, Final Batch Loss: 0.48035141825675964\n",
      "Epoch 224, Loss: 1.1744582653045654, Final Batch Loss: 0.5354286432266235\n",
      "Epoch 225, Loss: 1.0459463596343994, Final Batch Loss: 0.5146678686141968\n",
      "Epoch 226, Loss: 1.119450569152832, Final Batch Loss: 0.6020482778549194\n",
      "Epoch 227, Loss: 1.1833210587501526, Final Batch Loss: 0.5704450011253357\n",
      "Epoch 228, Loss: 1.111980676651001, Final Batch Loss: 0.5777035355567932\n",
      "Epoch 229, Loss: 1.2032822370529175, Final Batch Loss: 0.5435625314712524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230, Loss: 1.0555332899093628, Final Batch Loss: 0.4826734662055969\n",
      "Epoch 231, Loss: 1.1285400986671448, Final Batch Loss: 0.5394969582557678\n",
      "Epoch 232, Loss: 1.162919282913208, Final Batch Loss: 0.5686333775520325\n",
      "Epoch 233, Loss: 1.127087414264679, Final Batch Loss: 0.608271062374115\n",
      "Epoch 234, Loss: 1.183108627796173, Final Batch Loss: 0.6173510551452637\n",
      "Epoch 235, Loss: 1.0603877902030945, Final Batch Loss: 0.53852379322052\n",
      "Epoch 236, Loss: 1.121954321861267, Final Batch Loss: 0.5203156471252441\n",
      "Epoch 237, Loss: 1.160714328289032, Final Batch Loss: 0.6041625142097473\n",
      "Epoch 238, Loss: 1.2029873728752136, Final Batch Loss: 0.6417762637138367\n",
      "Epoch 239, Loss: 1.1049920916557312, Final Batch Loss: 0.5620051026344299\n",
      "Epoch 240, Loss: 1.0321752727031708, Final Batch Loss: 0.541955292224884\n",
      "Epoch 241, Loss: 1.081311583518982, Final Batch Loss: 0.5257580876350403\n",
      "Epoch 242, Loss: 1.231972336769104, Final Batch Loss: 0.6682036519050598\n",
      "Epoch 243, Loss: 1.115147054195404, Final Batch Loss: 0.6165955066680908\n",
      "Epoch 244, Loss: 1.109223186969757, Final Batch Loss: 0.5361445546150208\n",
      "Epoch 245, Loss: 1.1204122304916382, Final Batch Loss: 0.5296030044555664\n",
      "Epoch 246, Loss: 1.1057255864143372, Final Batch Loss: 0.5038997530937195\n",
      "Epoch 247, Loss: 1.094232201576233, Final Batch Loss: 0.5282570719718933\n",
      "Epoch 248, Loss: 1.013539731502533, Final Batch Loss: 0.5079014301300049\n",
      "Epoch 249, Loss: 1.0524827539920807, Final Batch Loss: 0.59709632396698\n",
      "Epoch 250, Loss: 1.0503669381141663, Final Batch Loss: 0.5185507535934448\n",
      "Epoch 251, Loss: 1.0637797117233276, Final Batch Loss: 0.4865824580192566\n",
      "Epoch 252, Loss: 1.031237006187439, Final Batch Loss: 0.5042077302932739\n",
      "Epoch 253, Loss: 1.0491474270820618, Final Batch Loss: 0.512136697769165\n",
      "Epoch 254, Loss: 1.0320130586624146, Final Batch Loss: 0.505735456943512\n",
      "Epoch 255, Loss: 0.9910514056682587, Final Batch Loss: 0.48792997002601624\n",
      "Epoch 256, Loss: 1.0306644439697266, Final Batch Loss: 0.4947754144668579\n",
      "Epoch 257, Loss: 1.1050664186477661, Final Batch Loss: 0.567414402961731\n",
      "Epoch 258, Loss: 1.0690537095069885, Final Batch Loss: 0.5300104022026062\n",
      "Epoch 259, Loss: 1.1791231036186218, Final Batch Loss: 0.6286571025848389\n",
      "Epoch 260, Loss: 1.0195484161376953, Final Batch Loss: 0.5151268243789673\n",
      "Epoch 261, Loss: 1.0167270302772522, Final Batch Loss: 0.5000526309013367\n",
      "Epoch 262, Loss: 1.1212595701217651, Final Batch Loss: 0.5462294816970825\n",
      "Epoch 263, Loss: 1.0577125549316406, Final Batch Loss: 0.5286590456962585\n",
      "Epoch 264, Loss: 0.9989479184150696, Final Batch Loss: 0.5221157670021057\n",
      "Epoch 265, Loss: 1.0000346302986145, Final Batch Loss: 0.49620312452316284\n",
      "Epoch 266, Loss: 0.9979070425033569, Final Batch Loss: 0.5028393864631653\n",
      "Epoch 267, Loss: 0.9463814198970795, Final Batch Loss: 0.42060109972953796\n",
      "Epoch 268, Loss: 1.0393116474151611, Final Batch Loss: 0.5219815373420715\n",
      "Epoch 269, Loss: 1.1089706420898438, Final Batch Loss: 0.6327617764472961\n",
      "Epoch 270, Loss: 1.007804811000824, Final Batch Loss: 0.5255340933799744\n",
      "Epoch 271, Loss: 1.0487945079803467, Final Batch Loss: 0.5077976584434509\n",
      "Epoch 272, Loss: 1.0738625526428223, Final Batch Loss: 0.5542948246002197\n",
      "Epoch 273, Loss: 0.9969733655452728, Final Batch Loss: 0.5193654894828796\n",
      "Epoch 274, Loss: 1.0659061670303345, Final Batch Loss: 0.5360507965087891\n",
      "Epoch 275, Loss: 1.009279727935791, Final Batch Loss: 0.4971109628677368\n",
      "Epoch 276, Loss: 1.0725204348564148, Final Batch Loss: 0.5259737968444824\n",
      "Epoch 277, Loss: 0.9672708213329315, Final Batch Loss: 0.5396108627319336\n",
      "Epoch 278, Loss: 1.008623719215393, Final Batch Loss: 0.47228574752807617\n",
      "Epoch 279, Loss: 1.1085559129714966, Final Batch Loss: 0.5679633617401123\n",
      "Epoch 280, Loss: 1.0973973274230957, Final Batch Loss: 0.5790555477142334\n",
      "Epoch 281, Loss: 0.9275382459163666, Final Batch Loss: 0.43620407581329346\n",
      "Epoch 282, Loss: 0.9852432608604431, Final Batch Loss: 0.5026898980140686\n",
      "Epoch 283, Loss: 1.0044002830982208, Final Batch Loss: 0.46612748503685\n",
      "Epoch 284, Loss: 0.9666062295436859, Final Batch Loss: 0.5005258321762085\n",
      "Epoch 285, Loss: 1.0379880666732788, Final Batch Loss: 0.48854726552963257\n",
      "Epoch 286, Loss: 0.9849701821804047, Final Batch Loss: 0.5140382647514343\n",
      "Epoch 287, Loss: 0.9311245977878571, Final Batch Loss: 0.48680588603019714\n",
      "Epoch 288, Loss: 0.9000018239021301, Final Batch Loss: 0.41532278060913086\n",
      "Epoch 289, Loss: 0.8611997663974762, Final Batch Loss: 0.41267159581184387\n",
      "Epoch 290, Loss: 1.018286943435669, Final Batch Loss: 0.534918487071991\n",
      "Epoch 291, Loss: 0.9304826855659485, Final Batch Loss: 0.4044727087020874\n",
      "Epoch 292, Loss: 0.9878305494785309, Final Batch Loss: 0.4932461977005005\n",
      "Epoch 293, Loss: 0.9190783500671387, Final Batch Loss: 0.45215481519699097\n",
      "Epoch 294, Loss: 0.986614853143692, Final Batch Loss: 0.42321065068244934\n",
      "Epoch 295, Loss: 1.0292076170444489, Final Batch Loss: 0.5348834991455078\n",
      "Epoch 296, Loss: 0.991532027721405, Final Batch Loss: 0.5343250632286072\n",
      "Epoch 297, Loss: 0.9580564796924591, Final Batch Loss: 0.4800718128681183\n",
      "Epoch 298, Loss: 0.9812930822372437, Final Batch Loss: 0.5291144847869873\n",
      "Epoch 299, Loss: 1.032120406627655, Final Batch Loss: 0.5078895092010498\n",
      "Epoch 300, Loss: 0.9660500288009644, Final Batch Loss: 0.44767332077026367\n",
      "Epoch 301, Loss: 1.0006015598773956, Final Batch Loss: 0.5166926980018616\n",
      "Epoch 302, Loss: 0.944228321313858, Final Batch Loss: 0.45164036750793457\n",
      "Epoch 303, Loss: 0.9869129061698914, Final Batch Loss: 0.46577727794647217\n",
      "Epoch 304, Loss: 1.0339716672897339, Final Batch Loss: 0.4911302328109741\n",
      "Epoch 305, Loss: 0.9849754273891449, Final Batch Loss: 0.4750175178050995\n",
      "Epoch 306, Loss: 0.9836198091506958, Final Batch Loss: 0.43232405185699463\n",
      "Epoch 307, Loss: 0.9734395444393158, Final Batch Loss: 0.4989936947822571\n",
      "Epoch 308, Loss: 0.8766142129898071, Final Batch Loss: 0.40422332286834717\n",
      "Epoch 309, Loss: 0.8868191838264465, Final Batch Loss: 0.4002313017845154\n",
      "Epoch 310, Loss: 0.9085991978645325, Final Batch Loss: 0.395957887172699\n",
      "Epoch 311, Loss: 0.9735685884952545, Final Batch Loss: 0.48569199442863464\n",
      "Epoch 312, Loss: 1.0036606788635254, Final Batch Loss: 0.503264844417572\n",
      "Epoch 313, Loss: 1.015377789735794, Final Batch Loss: 0.5347246527671814\n",
      "Epoch 314, Loss: 0.9264391362667084, Final Batch Loss: 0.5051669478416443\n",
      "Epoch 315, Loss: 1.0663180649280548, Final Batch Loss: 0.5814831256866455\n",
      "Epoch 316, Loss: 0.9822449088096619, Final Batch Loss: 0.4701850414276123\n",
      "Epoch 317, Loss: 0.908048540353775, Final Batch Loss: 0.4644196629524231\n",
      "Epoch 318, Loss: 1.0715526342391968, Final Batch Loss: 0.5594314336776733\n",
      "Epoch 319, Loss: 0.8998253643512726, Final Batch Loss: 0.4301908016204834\n",
      "Epoch 320, Loss: 0.9878425002098083, Final Batch Loss: 0.5037960410118103\n",
      "Epoch 321, Loss: 0.9386695623397827, Final Batch Loss: 0.47569409012794495\n",
      "Epoch 322, Loss: 0.8715687990188599, Final Batch Loss: 0.4721291959285736\n",
      "Epoch 323, Loss: 0.9384148418903351, Final Batch Loss: 0.46666884422302246\n",
      "Epoch 324, Loss: 1.0128717422485352, Final Batch Loss: 0.5193766951560974\n",
      "Epoch 325, Loss: 0.9631991982460022, Final Batch Loss: 0.45476824045181274\n",
      "Epoch 326, Loss: 0.9520500600337982, Final Batch Loss: 0.47911694645881653\n",
      "Epoch 327, Loss: 0.8811661899089813, Final Batch Loss: 0.43768444657325745\n",
      "Epoch 328, Loss: 0.9494301974773407, Final Batch Loss: 0.4418283998966217\n",
      "Epoch 329, Loss: 0.9070497155189514, Final Batch Loss: 0.4787639379501343\n",
      "Epoch 330, Loss: 0.8536608815193176, Final Batch Loss: 0.39006662368774414\n",
      "Epoch 331, Loss: 0.9016733467578888, Final Batch Loss: 0.3979739248752594\n",
      "Epoch 332, Loss: 0.9086745083332062, Final Batch Loss: 0.48033443093299866\n",
      "Epoch 333, Loss: 0.9015537798404694, Final Batch Loss: 0.42487451434135437\n",
      "Epoch 334, Loss: 0.8730471432209015, Final Batch Loss: 0.4113287031650543\n",
      "Epoch 335, Loss: 0.8437175750732422, Final Batch Loss: 0.4232807159423828\n",
      "Epoch 336, Loss: 0.8878291845321655, Final Batch Loss: 0.43958142399787903\n",
      "Epoch 337, Loss: 0.9070068895816803, Final Batch Loss: 0.4800332486629486\n",
      "Epoch 338, Loss: 0.8701021671295166, Final Batch Loss: 0.41329264640808105\n",
      "Epoch 339, Loss: 0.9673779904842377, Final Batch Loss: 0.4628050625324249\n",
      "Epoch 340, Loss: 0.8623290956020355, Final Batch Loss: 0.42490720748901367\n",
      "Epoch 341, Loss: 0.9803198575973511, Final Batch Loss: 0.5532213449478149\n",
      "Epoch 342, Loss: 0.8242248892784119, Final Batch Loss: 0.4061809778213501\n",
      "Epoch 343, Loss: 0.9539236724376678, Final Batch Loss: 0.5439257621765137\n",
      "Epoch 344, Loss: 0.8482953011989594, Final Batch Loss: 0.38881081342697144\n",
      "Epoch 345, Loss: 0.882113128900528, Final Batch Loss: 0.42322948575019836\n",
      "Epoch 346, Loss: 0.9604063034057617, Final Batch Loss: 0.5348225235939026\n",
      "Epoch 347, Loss: 0.9341716170310974, Final Batch Loss: 0.45854058861732483\n",
      "Epoch 348, Loss: 0.9362087249755859, Final Batch Loss: 0.5032421350479126\n",
      "Epoch 349, Loss: 0.8830847442150116, Final Batch Loss: 0.42922356724739075\n",
      "Epoch 350, Loss: 0.8968403041362762, Final Batch Loss: 0.48717010021209717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351, Loss: 0.886569082736969, Final Batch Loss: 0.4495674967765808\n",
      "Epoch 352, Loss: 0.8364688158035278, Final Batch Loss: 0.40692099928855896\n",
      "Epoch 353, Loss: 0.861907958984375, Final Batch Loss: 0.3917315900325775\n",
      "Epoch 354, Loss: 0.9274495244026184, Final Batch Loss: 0.45650026202201843\n",
      "Epoch 355, Loss: 0.892513632774353, Final Batch Loss: 0.3929801285266876\n",
      "Epoch 356, Loss: 0.8842372894287109, Final Batch Loss: 0.41289493441581726\n",
      "Epoch 357, Loss: 0.8721064925193787, Final Batch Loss: 0.41930559277534485\n",
      "Epoch 358, Loss: 0.9081109166145325, Final Batch Loss: 0.4368266463279724\n",
      "Epoch 359, Loss: 0.8496342897415161, Final Batch Loss: 0.40398478507995605\n",
      "Epoch 360, Loss: 0.9173369705677032, Final Batch Loss: 0.47987687587738037\n",
      "Epoch 361, Loss: 0.8453421294689178, Final Batch Loss: 0.40940234065055847\n",
      "Epoch 362, Loss: 0.9336069226264954, Final Batch Loss: 0.5086555480957031\n",
      "Epoch 363, Loss: 0.9553285241127014, Final Batch Loss: 0.540195882320404\n",
      "Epoch 364, Loss: 0.8810694813728333, Final Batch Loss: 0.4534023702144623\n",
      "Epoch 365, Loss: 0.8939098119735718, Final Batch Loss: 0.41981253027915955\n",
      "Epoch 366, Loss: 0.9033673405647278, Final Batch Loss: 0.4958179295063019\n",
      "Epoch 367, Loss: 0.873664140701294, Final Batch Loss: 0.45478203892707825\n",
      "Epoch 368, Loss: 0.8313319385051727, Final Batch Loss: 0.3924488127231598\n",
      "Epoch 369, Loss: 0.9092662334442139, Final Batch Loss: 0.480657696723938\n",
      "Epoch 370, Loss: 0.8587065637111664, Final Batch Loss: 0.4058385193347931\n",
      "Epoch 371, Loss: 0.8583555221557617, Final Batch Loss: 0.4283691346645355\n",
      "Epoch 372, Loss: 0.8472842574119568, Final Batch Loss: 0.4352574348449707\n",
      "Epoch 373, Loss: 0.8401374220848083, Final Batch Loss: 0.4335107207298279\n",
      "Epoch 374, Loss: 0.8983294069766998, Final Batch Loss: 0.4617653489112854\n",
      "Epoch 375, Loss: 0.8922182619571686, Final Batch Loss: 0.4227781891822815\n",
      "Epoch 376, Loss: 0.8524066209793091, Final Batch Loss: 0.4109099805355072\n",
      "Epoch 377, Loss: 0.8327366411685944, Final Batch Loss: 0.41699185967445374\n",
      "Epoch 378, Loss: 0.8202999234199524, Final Batch Loss: 0.4132477343082428\n",
      "Epoch 379, Loss: 0.851916491985321, Final Batch Loss: 0.4218224585056305\n",
      "Epoch 380, Loss: 0.8130253851413727, Final Batch Loss: 0.3450663387775421\n",
      "Epoch 381, Loss: 0.8738863170146942, Final Batch Loss: 0.4741939902305603\n",
      "Epoch 382, Loss: 0.8099329173564911, Final Batch Loss: 0.3795376121997833\n",
      "Epoch 383, Loss: 0.8613416850566864, Final Batch Loss: 0.4362596273422241\n",
      "Epoch 384, Loss: 0.8320968151092529, Final Batch Loss: 0.4821583032608032\n",
      "Epoch 385, Loss: 0.8850517272949219, Final Batch Loss: 0.47420546412467957\n",
      "Epoch 386, Loss: 0.8544524908065796, Final Batch Loss: 0.4420490562915802\n",
      "Epoch 387, Loss: 0.80791836977005, Final Batch Loss: 0.37847694754600525\n",
      "Epoch 388, Loss: 0.8124412596225739, Final Batch Loss: 0.4204787015914917\n",
      "Epoch 389, Loss: 0.838696300983429, Final Batch Loss: 0.40502089262008667\n",
      "Epoch 390, Loss: 0.8588256537914276, Final Batch Loss: 0.4492216110229492\n",
      "Epoch 391, Loss: 0.9145574271678925, Final Batch Loss: 0.46669918298721313\n",
      "Epoch 392, Loss: 0.8108102679252625, Final Batch Loss: 0.4415687918663025\n",
      "Epoch 393, Loss: 0.8220691978931427, Final Batch Loss: 0.3833756148815155\n",
      "Epoch 394, Loss: 0.8157329857349396, Final Batch Loss: 0.3933314085006714\n",
      "Epoch 395, Loss: 0.8517298996448517, Final Batch Loss: 0.4529639780521393\n",
      "Epoch 396, Loss: 0.8462804555892944, Final Batch Loss: 0.3987991511821747\n",
      "Epoch 397, Loss: 0.8206818401813507, Final Batch Loss: 0.38169801235198975\n",
      "Epoch 398, Loss: 0.8604812324047089, Final Batch Loss: 0.40736085176467896\n",
      "Epoch 399, Loss: 0.8399542570114136, Final Batch Loss: 0.44545990228652954\n",
      "Epoch 400, Loss: 0.8914261758327484, Final Batch Loss: 0.43565094470977783\n",
      "Epoch 401, Loss: 0.8667376339435577, Final Batch Loss: 0.48071935772895813\n",
      "Epoch 402, Loss: 0.8007107079029083, Final Batch Loss: 0.41043826937675476\n",
      "Epoch 403, Loss: 0.8713237643241882, Final Batch Loss: 0.4237954020500183\n",
      "Epoch 404, Loss: 0.8652198314666748, Final Batch Loss: 0.44944846630096436\n",
      "Epoch 405, Loss: 0.8695810735225677, Final Batch Loss: 0.40325942635536194\n",
      "Epoch 406, Loss: 0.8628525137901306, Final Batch Loss: 0.4419898986816406\n",
      "Epoch 407, Loss: 0.7620898485183716, Final Batch Loss: 0.31085285544395447\n",
      "Epoch 408, Loss: 0.7945633232593536, Final Batch Loss: 0.4162150025367737\n",
      "Epoch 409, Loss: 0.7867019176483154, Final Batch Loss: 0.4137786328792572\n",
      "Epoch 410, Loss: 0.7762579619884491, Final Batch Loss: 0.3574880063533783\n",
      "Epoch 411, Loss: 0.7722778916358948, Final Batch Loss: 0.3559569716453552\n",
      "Epoch 412, Loss: 0.8281351923942566, Final Batch Loss: 0.3896717429161072\n",
      "Epoch 413, Loss: 0.8021864593029022, Final Batch Loss: 0.4110719561576843\n",
      "Epoch 414, Loss: 0.9060977101325989, Final Batch Loss: 0.46335774660110474\n",
      "Epoch 415, Loss: 0.875049501657486, Final Batch Loss: 0.5077182054519653\n",
      "Epoch 416, Loss: 0.8195595741271973, Final Batch Loss: 0.4245043992996216\n",
      "Epoch 417, Loss: 0.745427131652832, Final Batch Loss: 0.38831815123558044\n",
      "Epoch 418, Loss: 0.7583442032337189, Final Batch Loss: 0.34037625789642334\n",
      "Epoch 419, Loss: 0.9009738564491272, Final Batch Loss: 0.4365936517715454\n",
      "Epoch 420, Loss: 0.8426038920879364, Final Batch Loss: 0.461969792842865\n",
      "Epoch 421, Loss: 0.9469527304172516, Final Batch Loss: 0.5023343563079834\n",
      "Epoch 422, Loss: 0.8286041915416718, Final Batch Loss: 0.456794798374176\n",
      "Epoch 423, Loss: 0.833318293094635, Final Batch Loss: 0.44337430596351624\n",
      "Epoch 424, Loss: 0.8048507273197174, Final Batch Loss: 0.4098801016807556\n",
      "Epoch 425, Loss: 0.7850057780742645, Final Batch Loss: 0.39900678396224976\n",
      "Epoch 426, Loss: 0.9151009321212769, Final Batch Loss: 0.4462866485118866\n",
      "Epoch 427, Loss: 0.8500723540782928, Final Batch Loss: 0.4412209391593933\n",
      "Epoch 428, Loss: 0.7966409027576447, Final Batch Loss: 0.3738008439540863\n",
      "Epoch 429, Loss: 0.7734716832637787, Final Batch Loss: 0.3820928931236267\n",
      "Epoch 430, Loss: 0.8378873467445374, Final Batch Loss: 0.4375569522380829\n",
      "Epoch 431, Loss: 0.908093124628067, Final Batch Loss: 0.5220614075660706\n",
      "Epoch 432, Loss: 0.8654282987117767, Final Batch Loss: 0.44321370124816895\n",
      "Epoch 433, Loss: 0.7775872945785522, Final Batch Loss: 0.3828108608722687\n",
      "Epoch 434, Loss: 0.8806931972503662, Final Batch Loss: 0.4830299913883209\n",
      "Epoch 435, Loss: 0.8137931525707245, Final Batch Loss: 0.38362374901771545\n",
      "Epoch 436, Loss: 0.8384091556072235, Final Batch Loss: 0.3802490830421448\n",
      "Epoch 437, Loss: 0.8002725541591644, Final Batch Loss: 0.4135689437389374\n",
      "Epoch 438, Loss: 0.908081591129303, Final Batch Loss: 0.48078790307044983\n",
      "Epoch 439, Loss: 0.805231899023056, Final Batch Loss: 0.4220332205295563\n",
      "Epoch 440, Loss: 0.8497521579265594, Final Batch Loss: 0.44686341285705566\n",
      "Epoch 441, Loss: 0.7889736294746399, Final Batch Loss: 0.34471508860588074\n",
      "Epoch 442, Loss: 0.7444959282875061, Final Batch Loss: 0.35004711151123047\n",
      "Epoch 443, Loss: 0.7296209335327148, Final Batch Loss: 0.34822699427604675\n",
      "Epoch 444, Loss: 0.7472410202026367, Final Batch Loss: 0.37372687458992004\n",
      "Epoch 445, Loss: 0.8130766153335571, Final Batch Loss: 0.4296383857727051\n",
      "Epoch 446, Loss: 0.7888313233852386, Final Batch Loss: 0.38727718591690063\n",
      "Epoch 447, Loss: 0.7988165020942688, Final Batch Loss: 0.42080074548721313\n",
      "Epoch 448, Loss: 0.7587516605854034, Final Batch Loss: 0.3945022225379944\n",
      "Epoch 449, Loss: 0.7970324754714966, Final Batch Loss: 0.3660247325897217\n",
      "Epoch 450, Loss: 0.6831997632980347, Final Batch Loss: 0.3108927011489868\n",
      "Epoch 451, Loss: 0.8205483555793762, Final Batch Loss: 0.42507001757621765\n",
      "Epoch 452, Loss: 0.7535877823829651, Final Batch Loss: 0.3731222450733185\n",
      "Epoch 453, Loss: 0.7543661296367645, Final Batch Loss: 0.33373862504959106\n",
      "Epoch 454, Loss: 0.771558552980423, Final Batch Loss: 0.35690978169441223\n",
      "Epoch 455, Loss: 0.838668704032898, Final Batch Loss: 0.41471996903419495\n",
      "Epoch 456, Loss: 0.6985424757003784, Final Batch Loss: 0.34039831161499023\n",
      "Epoch 457, Loss: 0.8013917505741119, Final Batch Loss: 0.37159252166748047\n",
      "Epoch 458, Loss: 0.8694755136966705, Final Batch Loss: 0.4269454777240753\n",
      "Epoch 459, Loss: 0.6953112483024597, Final Batch Loss: 0.3427138924598694\n",
      "Epoch 460, Loss: 0.732986718416214, Final Batch Loss: 0.3290550112724304\n",
      "Epoch 461, Loss: 0.7211694717407227, Final Batch Loss: 0.30196329951286316\n",
      "Epoch 462, Loss: 0.8193074762821198, Final Batch Loss: 0.44326284527778625\n",
      "Epoch 463, Loss: 0.799210786819458, Final Batch Loss: 0.40634647011756897\n",
      "Epoch 464, Loss: 0.8035013377666473, Final Batch Loss: 0.4246252179145813\n",
      "Epoch 465, Loss: 0.8005545139312744, Final Batch Loss: 0.4399876892566681\n",
      "Epoch 466, Loss: 0.8047931492328644, Final Batch Loss: 0.4066077172756195\n",
      "Epoch 467, Loss: 0.840049684047699, Final Batch Loss: 0.4721188545227051\n",
      "Epoch 468, Loss: 0.7646989524364471, Final Batch Loss: 0.3775143623352051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469, Loss: 0.7059760987758636, Final Batch Loss: 0.28883010149002075\n",
      "Epoch 470, Loss: 0.8492556810379028, Final Batch Loss: 0.4425603449344635\n",
      "Epoch 471, Loss: 0.7347341179847717, Final Batch Loss: 0.29775938391685486\n",
      "Epoch 472, Loss: 0.7686463594436646, Final Batch Loss: 0.38825860619544983\n",
      "Epoch 473, Loss: 0.7727717459201813, Final Batch Loss: 0.3555545508861542\n",
      "Epoch 474, Loss: 0.7430541217327118, Final Batch Loss: 0.38435599207878113\n",
      "Epoch 475, Loss: 0.8223422467708588, Final Batch Loss: 0.4110824167728424\n",
      "Epoch 476, Loss: 0.7676631212234497, Final Batch Loss: 0.44414106011390686\n",
      "Epoch 477, Loss: 0.7778097987174988, Final Batch Loss: 0.34804069995880127\n",
      "Epoch 478, Loss: 0.8713805675506592, Final Batch Loss: 0.44631683826446533\n",
      "Epoch 479, Loss: 0.8278051614761353, Final Batch Loss: 0.4163019359111786\n",
      "Epoch 480, Loss: 0.7301775217056274, Final Batch Loss: 0.3330741822719574\n",
      "Epoch 481, Loss: 0.7611246407032013, Final Batch Loss: 0.3348386287689209\n",
      "Epoch 482, Loss: 0.7554512619972229, Final Batch Loss: 0.32914191484451294\n",
      "Epoch 483, Loss: 0.7773630023002625, Final Batch Loss: 0.4015093147754669\n",
      "Epoch 484, Loss: 0.8703401684761047, Final Batch Loss: 0.4442921280860901\n",
      "Epoch 485, Loss: 0.7109436094760895, Final Batch Loss: 0.3516958951950073\n",
      "Epoch 486, Loss: 0.7195584774017334, Final Batch Loss: 0.372540682554245\n",
      "Epoch 487, Loss: 0.6893067359924316, Final Batch Loss: 0.2917369306087494\n",
      "Epoch 488, Loss: 0.7594930529594421, Final Batch Loss: 0.40511298179626465\n",
      "Epoch 489, Loss: 0.6896745562553406, Final Batch Loss: 0.283940851688385\n",
      "Epoch 490, Loss: 0.7497803270816803, Final Batch Loss: 0.3786144554615021\n",
      "Epoch 491, Loss: 0.6862044632434845, Final Batch Loss: 0.34508875012397766\n",
      "Epoch 492, Loss: 0.8040204644203186, Final Batch Loss: 0.42287367582321167\n",
      "Epoch 493, Loss: 0.8229186236858368, Final Batch Loss: 0.3773772418498993\n",
      "Epoch 494, Loss: 0.7764740884304047, Final Batch Loss: 0.3746768534183502\n",
      "Epoch 495, Loss: 0.7207117676734924, Final Batch Loss: 0.37187299132347107\n",
      "Epoch 496, Loss: 0.699126124382019, Final Batch Loss: 0.31142657995224\n",
      "Epoch 497, Loss: 0.7105821967124939, Final Batch Loss: 0.39876070618629456\n",
      "Epoch 498, Loss: 0.7691802680492401, Final Batch Loss: 0.3992356061935425\n",
      "Epoch 499, Loss: 0.7512741386890411, Final Batch Loss: 0.3971095383167267\n",
      "Epoch 500, Loss: 0.7782892882823944, Final Batch Loss: 0.392776221036911\n",
      "Epoch 501, Loss: 0.8112849593162537, Final Batch Loss: 0.39027783274650574\n",
      "Epoch 502, Loss: 0.8012931048870087, Final Batch Loss: 0.4082062244415283\n",
      "Epoch 503, Loss: 0.7163229286670685, Final Batch Loss: 0.3453649580478668\n",
      "Epoch 504, Loss: 0.7965649962425232, Final Batch Loss: 0.41599807143211365\n",
      "Epoch 505, Loss: 0.8313694596290588, Final Batch Loss: 0.3904835283756256\n",
      "Epoch 506, Loss: 0.7480872273445129, Final Batch Loss: 0.38413190841674805\n",
      "Epoch 507, Loss: 0.7306108176708221, Final Batch Loss: 0.3609376847743988\n",
      "Epoch 508, Loss: 0.7660850882530212, Final Batch Loss: 0.39039430022239685\n",
      "Epoch 509, Loss: 0.6991269588470459, Final Batch Loss: 0.3805793523788452\n",
      "Epoch 510, Loss: 0.6834933459758759, Final Batch Loss: 0.3053336441516876\n",
      "Epoch 511, Loss: 0.7766640782356262, Final Batch Loss: 0.4056852161884308\n",
      "Epoch 512, Loss: 0.7911430597305298, Final Batch Loss: 0.4102567732334137\n",
      "Epoch 513, Loss: 0.7386690080165863, Final Batch Loss: 0.38386544585227966\n",
      "Epoch 514, Loss: 0.7011301815509796, Final Batch Loss: 0.3682568073272705\n",
      "Epoch 515, Loss: 0.7019124329090118, Final Batch Loss: 0.33182328939437866\n",
      "Epoch 516, Loss: 0.7253511250019073, Final Batch Loss: 0.3389413356781006\n",
      "Epoch 517, Loss: 0.7064146399497986, Final Batch Loss: 0.3114554286003113\n",
      "Epoch 518, Loss: 0.7332088053226471, Final Batch Loss: 0.41121309995651245\n",
      "Epoch 519, Loss: 0.724416196346283, Final Batch Loss: 0.31973758339881897\n",
      "Epoch 520, Loss: 0.7513819634914398, Final Batch Loss: 0.3773799538612366\n",
      "Epoch 521, Loss: 0.6758860945701599, Final Batch Loss: 0.2897598147392273\n",
      "Epoch 522, Loss: 0.758316695690155, Final Batch Loss: 0.4537044167518616\n",
      "Epoch 523, Loss: 0.8399215638637543, Final Batch Loss: 0.4583863317966461\n",
      "Epoch 524, Loss: 0.717128723859787, Final Batch Loss: 0.3303016424179077\n",
      "Epoch 525, Loss: 0.7524313628673553, Final Batch Loss: 0.37023618817329407\n",
      "Epoch 526, Loss: 0.7517355680465698, Final Batch Loss: 0.4253756105899811\n",
      "Epoch 527, Loss: 0.7153481245040894, Final Batch Loss: 0.3854202330112457\n",
      "Epoch 528, Loss: 0.7156501412391663, Final Batch Loss: 0.36896875500679016\n",
      "Epoch 529, Loss: 0.7747854292392731, Final Batch Loss: 0.4036850333213806\n",
      "Epoch 530, Loss: 0.6998437643051147, Final Batch Loss: 0.37567758560180664\n",
      "Epoch 531, Loss: 0.6385944485664368, Final Batch Loss: 0.26157045364379883\n",
      "Epoch 532, Loss: 0.7787474691867828, Final Batch Loss: 0.45651975274086\n",
      "Epoch 533, Loss: 0.7764006853103638, Final Batch Loss: 0.38106319308280945\n",
      "Epoch 534, Loss: 0.7464309930801392, Final Batch Loss: 0.42497533559799194\n",
      "Epoch 535, Loss: 0.7290566861629486, Final Batch Loss: 0.3656003177165985\n",
      "Epoch 536, Loss: 0.8380583822727203, Final Batch Loss: 0.4449878931045532\n",
      "Epoch 537, Loss: 0.6758162975311279, Final Batch Loss: 0.3486940562725067\n",
      "Epoch 538, Loss: 0.7660227119922638, Final Batch Loss: 0.35371774435043335\n",
      "Epoch 539, Loss: 0.8127683103084564, Final Batch Loss: 0.37774696946144104\n",
      "Epoch 540, Loss: 0.7536621391773224, Final Batch Loss: 0.4138612747192383\n",
      "Epoch 541, Loss: 0.6914079785346985, Final Batch Loss: 0.37045812606811523\n",
      "Epoch 542, Loss: 0.7614298164844513, Final Batch Loss: 0.4002319574356079\n",
      "Epoch 543, Loss: 0.6732387542724609, Final Batch Loss: 0.34623926877975464\n",
      "Epoch 544, Loss: 0.7922817468643188, Final Batch Loss: 0.37549641728401184\n",
      "Epoch 545, Loss: 0.6705508530139923, Final Batch Loss: 0.3450106382369995\n",
      "Epoch 546, Loss: 0.7518157064914703, Final Batch Loss: 0.3403864800930023\n",
      "Epoch 547, Loss: 0.7177763879299164, Final Batch Loss: 0.4145153760910034\n",
      "Epoch 548, Loss: 0.706495851278305, Final Batch Loss: 0.3840913772583008\n",
      "Epoch 549, Loss: 0.7447533905506134, Final Batch Loss: 0.3936443328857422\n",
      "Epoch 550, Loss: 0.6849552094936371, Final Batch Loss: 0.3133023977279663\n",
      "Epoch 551, Loss: 0.8048469126224518, Final Batch Loss: 0.3744431138038635\n",
      "Epoch 552, Loss: 0.7162661254405975, Final Batch Loss: 0.3765891194343567\n",
      "Epoch 553, Loss: 0.7528057396411896, Final Batch Loss: 0.39661163091659546\n",
      "Epoch 554, Loss: 0.6995925009250641, Final Batch Loss: 0.28594425320625305\n",
      "Epoch 555, Loss: 0.7444900870323181, Final Batch Loss: 0.38394609093666077\n",
      "Epoch 556, Loss: 0.7094875872135162, Final Batch Loss: 0.33972758054733276\n",
      "Epoch 557, Loss: 0.7960905432701111, Final Batch Loss: 0.44041576981544495\n",
      "Epoch 558, Loss: 0.7185395359992981, Final Batch Loss: 0.32585111260414124\n",
      "Epoch 559, Loss: 0.7440981864929199, Final Batch Loss: 0.3980923593044281\n",
      "Epoch 560, Loss: 0.651864618062973, Final Batch Loss: 0.35005632042884827\n",
      "Epoch 561, Loss: 0.7550899982452393, Final Batch Loss: 0.4027877151966095\n",
      "Epoch 562, Loss: 0.7334862649440765, Final Batch Loss: 0.36089542508125305\n",
      "Epoch 563, Loss: 0.7351756393909454, Final Batch Loss: 0.37697869539260864\n",
      "Epoch 564, Loss: 0.654867023229599, Final Batch Loss: 0.30318981409072876\n",
      "Epoch 565, Loss: 0.6163779199123383, Final Batch Loss: 0.26115599274635315\n",
      "Epoch 566, Loss: 0.7219922244548798, Final Batch Loss: 0.3210895359516144\n",
      "Epoch 567, Loss: 0.7251366674900055, Final Batch Loss: 0.3961177468299866\n",
      "Epoch 568, Loss: 0.7535289525985718, Final Batch Loss: 0.37448692321777344\n",
      "Epoch 569, Loss: 0.7009146809577942, Final Batch Loss: 0.3021968901157379\n",
      "Epoch 570, Loss: 0.7018798291683197, Final Batch Loss: 0.3771258592605591\n",
      "Epoch 571, Loss: 0.6337822675704956, Final Batch Loss: 0.3290880620479584\n",
      "Epoch 572, Loss: 0.6840184330940247, Final Batch Loss: 0.3441607654094696\n",
      "Epoch 573, Loss: 0.7433897852897644, Final Batch Loss: 0.35535484552383423\n",
      "Epoch 574, Loss: 0.6974462866783142, Final Batch Loss: 0.3157607316970825\n",
      "Epoch 575, Loss: 0.6459075510501862, Final Batch Loss: 0.35046252608299255\n",
      "Epoch 576, Loss: 0.7010179460048676, Final Batch Loss: 0.32214778661727905\n",
      "Epoch 577, Loss: 0.6598513126373291, Final Batch Loss: 0.3582681119441986\n",
      "Epoch 578, Loss: 0.8347972631454468, Final Batch Loss: 0.487480103969574\n",
      "Epoch 579, Loss: 0.6679688096046448, Final Batch Loss: 0.35315170884132385\n",
      "Epoch 580, Loss: 0.6891824007034302, Final Batch Loss: 0.3786163330078125\n",
      "Epoch 581, Loss: 0.7541484236717224, Final Batch Loss: 0.3774881064891815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 582, Loss: 0.70596843957901, Final Batch Loss: 0.41315463185310364\n",
      "Epoch 583, Loss: 0.690199613571167, Final Batch Loss: 0.33471283316612244\n",
      "Epoch 584, Loss: 0.6237455308437347, Final Batch Loss: 0.2991349995136261\n",
      "Epoch 585, Loss: 0.697933554649353, Final Batch Loss: 0.35921746492385864\n",
      "Epoch 586, Loss: 0.6357822418212891, Final Batch Loss: 0.30520525574684143\n",
      "Epoch 587, Loss: 0.6154987812042236, Final Batch Loss: 0.2682490050792694\n",
      "Epoch 588, Loss: 0.673082023859024, Final Batch Loss: 0.2976383864879608\n",
      "Epoch 589, Loss: 0.6903941929340363, Final Batch Loss: 0.38082799315452576\n",
      "Epoch 590, Loss: 0.7023516893386841, Final Batch Loss: 0.3482309877872467\n",
      "Epoch 591, Loss: 0.7399109899997711, Final Batch Loss: 0.32917752861976624\n",
      "Epoch 592, Loss: 0.6954085826873779, Final Batch Loss: 0.29716089367866516\n",
      "Epoch 593, Loss: 0.6611455678939819, Final Batch Loss: 0.2929750680923462\n",
      "Epoch 594, Loss: 0.8080171048641205, Final Batch Loss: 0.4295644462108612\n",
      "Epoch 595, Loss: 0.6868708431720734, Final Batch Loss: 0.32795387506484985\n",
      "Epoch 596, Loss: 0.6712544858455658, Final Batch Loss: 0.3233855664730072\n",
      "Epoch 597, Loss: 0.6861507892608643, Final Batch Loss: 0.35438352823257446\n",
      "Epoch 598, Loss: 0.7170408964157104, Final Batch Loss: 0.3859837055206299\n",
      "Epoch 599, Loss: 0.6489772200584412, Final Batch Loss: 0.32239559292793274\n",
      "Epoch 600, Loss: 0.6587440371513367, Final Batch Loss: 0.3041442632675171\n",
      "Epoch 601, Loss: 0.6719911694526672, Final Batch Loss: 0.3314068615436554\n",
      "Epoch 602, Loss: 0.6874494552612305, Final Batch Loss: 0.30130478739738464\n",
      "Epoch 603, Loss: 0.7065486907958984, Final Batch Loss: 0.38457581400871277\n",
      "Epoch 604, Loss: 0.736425369977951, Final Batch Loss: 0.4183860719203949\n",
      "Epoch 605, Loss: 0.6942040920257568, Final Batch Loss: 0.3572385907173157\n",
      "Epoch 606, Loss: 0.6234381198883057, Final Batch Loss: 0.32586053013801575\n",
      "Epoch 607, Loss: 0.7370753288269043, Final Batch Loss: 0.3668615520000458\n",
      "Epoch 608, Loss: 0.7113016247749329, Final Batch Loss: 0.35386642813682556\n",
      "Epoch 609, Loss: 0.6585652828216553, Final Batch Loss: 0.3589856028556824\n",
      "Epoch 610, Loss: 0.6722307205200195, Final Batch Loss: 0.35403943061828613\n",
      "Epoch 611, Loss: 0.696596771478653, Final Batch Loss: 0.3464297354221344\n",
      "Epoch 612, Loss: 0.7475258409976959, Final Batch Loss: 0.4185832738876343\n",
      "Epoch 613, Loss: 0.7414395809173584, Final Batch Loss: 0.3729292154312134\n",
      "Epoch 614, Loss: 0.6492781341075897, Final Batch Loss: 0.31288862228393555\n",
      "Epoch 615, Loss: 0.7550026774406433, Final Batch Loss: 0.34354037046432495\n",
      "Epoch 616, Loss: 0.7277486622333527, Final Batch Loss: 0.33767586946487427\n",
      "Epoch 617, Loss: 0.7309569716453552, Final Batch Loss: 0.3781922459602356\n",
      "Epoch 618, Loss: 0.6836005747318268, Final Batch Loss: 0.30508074164390564\n",
      "Epoch 619, Loss: 0.6629574000835419, Final Batch Loss: 0.32307666540145874\n",
      "Epoch 620, Loss: 0.6035849750041962, Final Batch Loss: 0.28182709217071533\n",
      "Epoch 621, Loss: 0.7468115985393524, Final Batch Loss: 0.37888485193252563\n",
      "Epoch 622, Loss: 0.6625118553638458, Final Batch Loss: 0.3158053457736969\n",
      "Epoch 623, Loss: 0.677581399679184, Final Batch Loss: 0.35359886288642883\n",
      "Epoch 624, Loss: 0.6558890044689178, Final Batch Loss: 0.3219108283519745\n",
      "Epoch 625, Loss: 0.7356943488121033, Final Batch Loss: 0.3953166902065277\n",
      "Epoch 626, Loss: 0.701087087392807, Final Batch Loss: 0.3833584785461426\n",
      "Epoch 627, Loss: 0.7186297178268433, Final Batch Loss: 0.3761098384857178\n",
      "Epoch 628, Loss: 0.7003668248653412, Final Batch Loss: 0.3885989189147949\n",
      "Epoch 629, Loss: 0.7409932315349579, Final Batch Loss: 0.4153483510017395\n",
      "Epoch 630, Loss: 0.6503519415855408, Final Batch Loss: 0.2849196791648865\n",
      "Epoch 631, Loss: 0.6458958089351654, Final Batch Loss: 0.3010897934436798\n",
      "Epoch 632, Loss: 0.6726235449314117, Final Batch Loss: 0.35492685437202454\n",
      "Epoch 633, Loss: 0.7029159367084503, Final Batch Loss: 0.3346846401691437\n",
      "Epoch 634, Loss: 0.7320588231086731, Final Batch Loss: 0.3852534592151642\n",
      "Epoch 635, Loss: 0.6678027808666229, Final Batch Loss: 0.3534775674343109\n",
      "Epoch 636, Loss: 0.6740290224552155, Final Batch Loss: 0.34387844800949097\n",
      "Epoch 637, Loss: 0.7166740894317627, Final Batch Loss: 0.3715652823448181\n",
      "Epoch 638, Loss: 0.6216786652803421, Final Batch Loss: 0.23092792928218842\n",
      "Epoch 639, Loss: 0.6257693767547607, Final Batch Loss: 0.28074848651885986\n",
      "Epoch 640, Loss: 0.7021429240703583, Final Batch Loss: 0.3610670864582062\n",
      "Epoch 641, Loss: 0.6587242484092712, Final Batch Loss: 0.33323460817337036\n",
      "Epoch 642, Loss: 0.6626860201358795, Final Batch Loss: 0.3326491117477417\n",
      "Epoch 643, Loss: 0.7420400083065033, Final Batch Loss: 0.3201964497566223\n",
      "Epoch 644, Loss: 0.6210124790668488, Final Batch Loss: 0.24994146823883057\n",
      "Epoch 645, Loss: 0.5993949770927429, Final Batch Loss: 0.2985374629497528\n",
      "Epoch 646, Loss: 0.6743500530719757, Final Batch Loss: 0.33443254232406616\n",
      "Epoch 647, Loss: 0.6626662611961365, Final Batch Loss: 0.3452557325363159\n",
      "Epoch 648, Loss: 0.6608343124389648, Final Batch Loss: 0.32655227184295654\n",
      "Epoch 649, Loss: 0.7083813548088074, Final Batch Loss: 0.3172537088394165\n",
      "Epoch 650, Loss: 0.6141797006130219, Final Batch Loss: 0.2582157850265503\n",
      "Epoch 651, Loss: 0.675905168056488, Final Batch Loss: 0.32899102568626404\n",
      "Epoch 652, Loss: 0.6704505980014801, Final Batch Loss: 0.39370182156562805\n",
      "Epoch 653, Loss: 0.6502947211265564, Final Batch Loss: 0.30881667137145996\n",
      "Epoch 654, Loss: 0.6916322410106659, Final Batch Loss: 0.4086247980594635\n",
      "Epoch 655, Loss: 0.6125956177711487, Final Batch Loss: 0.2687252461910248\n",
      "Epoch 656, Loss: 0.6792376935482025, Final Batch Loss: 0.38956400752067566\n",
      "Epoch 657, Loss: 0.6608313322067261, Final Batch Loss: 0.3279058337211609\n",
      "Epoch 658, Loss: 0.6633330881595612, Final Batch Loss: 0.32907506823539734\n",
      "Epoch 659, Loss: 0.6562960743904114, Final Batch Loss: 0.3591289222240448\n",
      "Epoch 660, Loss: 0.7203691899776459, Final Batch Loss: 0.36166995763778687\n",
      "Epoch 661, Loss: 0.6200804114341736, Final Batch Loss: 0.2602919638156891\n",
      "Epoch 662, Loss: 0.6065581142902374, Final Batch Loss: 0.3125740587711334\n",
      "Epoch 663, Loss: 0.6772886216640472, Final Batch Loss: 0.33848392963409424\n",
      "Epoch 664, Loss: 0.6267984360456467, Final Batch Loss: 0.24721048772335052\n",
      "Epoch 665, Loss: 0.6867565512657166, Final Batch Loss: 0.3136090040206909\n",
      "Epoch 666, Loss: 0.7350738346576691, Final Batch Loss: 0.3802843987941742\n",
      "Epoch 667, Loss: 0.6783257126808167, Final Batch Loss: 0.3358573913574219\n",
      "Epoch 668, Loss: 0.7225449681282043, Final Batch Loss: 0.4103941023349762\n",
      "Epoch 669, Loss: 0.608478307723999, Final Batch Loss: 0.30419501662254333\n",
      "Epoch 670, Loss: 0.6894956529140472, Final Batch Loss: 0.3809671998023987\n",
      "Epoch 671, Loss: 0.6773440837860107, Final Batch Loss: 0.33259791135787964\n",
      "Epoch 672, Loss: 0.684420108795166, Final Batch Loss: 0.3799232840538025\n",
      "Epoch 673, Loss: 0.6246711313724518, Final Batch Loss: 0.2514015734195709\n",
      "Epoch 674, Loss: 0.6522070169448853, Final Batch Loss: 0.33570605516433716\n",
      "Epoch 675, Loss: 0.6017370522022247, Final Batch Loss: 0.27182719111442566\n",
      "Epoch 676, Loss: 0.6386817395687103, Final Batch Loss: 0.31289249658584595\n",
      "Epoch 677, Loss: 0.7087498605251312, Final Batch Loss: 0.39514613151550293\n",
      "Epoch 678, Loss: 0.7077754139900208, Final Batch Loss: 0.37577250599861145\n",
      "Epoch 679, Loss: 0.6303469836711884, Final Batch Loss: 0.2793157398700714\n",
      "Epoch 680, Loss: 0.6361895501613617, Final Batch Loss: 0.30581134557724\n",
      "Epoch 681, Loss: 0.6722857058048248, Final Batch Loss: 0.33684036135673523\n",
      "Epoch 682, Loss: 0.6600658893585205, Final Batch Loss: 0.3154754042625427\n",
      "Epoch 683, Loss: 0.8254263699054718, Final Batch Loss: 0.45043039321899414\n",
      "Epoch 684, Loss: 0.6533234715461731, Final Batch Loss: 0.3412705063819885\n",
      "Epoch 685, Loss: 0.6279999613761902, Final Batch Loss: 0.33516550064086914\n",
      "Epoch 686, Loss: 0.6304751336574554, Final Batch Loss: 0.32911866903305054\n",
      "Epoch 687, Loss: 0.6219847798347473, Final Batch Loss: 0.30107009410858154\n",
      "Epoch 688, Loss: 0.6532249450683594, Final Batch Loss: 0.31730642914772034\n",
      "Epoch 689, Loss: 0.8228816986083984, Final Batch Loss: 0.40078333020210266\n",
      "Epoch 690, Loss: 0.6631931662559509, Final Batch Loss: 0.353855162858963\n",
      "Epoch 691, Loss: 0.7131907939910889, Final Batch Loss: 0.3923962414264679\n",
      "Epoch 692, Loss: 0.6215395331382751, Final Batch Loss: 0.29374122619628906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 693, Loss: 0.6332104206085205, Final Batch Loss: 0.33180779218673706\n",
      "Epoch 694, Loss: 0.697856068611145, Final Batch Loss: 0.35460957884788513\n",
      "Epoch 695, Loss: 0.744255006313324, Final Batch Loss: 0.41273656487464905\n",
      "Epoch 696, Loss: 0.6068629026412964, Final Batch Loss: 0.2950918674468994\n",
      "Epoch 697, Loss: 0.6142855286598206, Final Batch Loss: 0.3241594731807709\n",
      "Epoch 698, Loss: 0.6399534940719604, Final Batch Loss: 0.3342657685279846\n",
      "Epoch 699, Loss: 0.6263790726661682, Final Batch Loss: 0.2969689965248108\n",
      "Epoch 700, Loss: 0.6656560301780701, Final Batch Loss: 0.31766176223754883\n",
      "Epoch 701, Loss: 0.6665764451026917, Final Batch Loss: 0.33430367708206177\n",
      "Epoch 702, Loss: 0.6595441102981567, Final Batch Loss: 0.3453417718410492\n",
      "Epoch 703, Loss: 0.6403674781322479, Final Batch Loss: 0.2921430468559265\n",
      "Epoch 704, Loss: 0.719574898481369, Final Batch Loss: 0.3894396424293518\n",
      "Epoch 705, Loss: 0.6003519296646118, Final Batch Loss: 0.33774784207344055\n",
      "Epoch 706, Loss: 0.7077381312847137, Final Batch Loss: 0.3342862129211426\n",
      "Epoch 707, Loss: 0.6573143005371094, Final Batch Loss: 0.32275471091270447\n",
      "Epoch 708, Loss: 0.6485072672367096, Final Batch Loss: 0.3033209443092346\n",
      "Epoch 709, Loss: 0.6406134366989136, Final Batch Loss: 0.3744392991065979\n",
      "Epoch 710, Loss: 0.6887318193912506, Final Batch Loss: 0.3092803359031677\n",
      "Epoch 711, Loss: 0.6705606579780579, Final Batch Loss: 0.32836195826530457\n",
      "Epoch 712, Loss: 0.6366938054561615, Final Batch Loss: 0.3336693346500397\n",
      "Epoch 713, Loss: 0.5914169251918793, Final Batch Loss: 0.30353352427482605\n",
      "Epoch 714, Loss: 0.6309421956539154, Final Batch Loss: 0.32418346405029297\n",
      "Epoch 715, Loss: 0.6052414774894714, Final Batch Loss: 0.30907219648361206\n",
      "Epoch 716, Loss: 0.6798122227191925, Final Batch Loss: 0.328009694814682\n",
      "Epoch 717, Loss: 0.6579823791980743, Final Batch Loss: 0.29814213514328003\n",
      "Epoch 718, Loss: 0.6614988148212433, Final Batch Loss: 0.29460224509239197\n",
      "Epoch 719, Loss: 0.6982364058494568, Final Batch Loss: 0.3564591705799103\n",
      "Epoch 720, Loss: 0.6345572471618652, Final Batch Loss: 0.3400014340877533\n",
      "Epoch 721, Loss: 0.6859996616840363, Final Batch Loss: 0.343250036239624\n",
      "Epoch 722, Loss: 0.5750941932201385, Final Batch Loss: 0.2737255394458771\n",
      "Epoch 723, Loss: 0.5838925838470459, Final Batch Loss: 0.294294148683548\n",
      "Epoch 724, Loss: 0.5856156498193741, Final Batch Loss: 0.2357625812292099\n",
      "Epoch 725, Loss: 0.6337023973464966, Final Batch Loss: 0.3054426610469818\n",
      "Epoch 726, Loss: 0.6132711470127106, Final Batch Loss: 0.2744567096233368\n",
      "Epoch 727, Loss: 0.6328989565372467, Final Batch Loss: 0.3069378137588501\n",
      "Epoch 728, Loss: 0.6498824656009674, Final Batch Loss: 0.3394097089767456\n",
      "Epoch 729, Loss: 0.6561121940612793, Final Batch Loss: 0.3127540946006775\n",
      "Epoch 730, Loss: 0.6848156750202179, Final Batch Loss: 0.38877636194229126\n",
      "Epoch 731, Loss: 0.6169930696487427, Final Batch Loss: 0.3099859058856964\n",
      "Epoch 732, Loss: 0.6907942295074463, Final Batch Loss: 0.39264291524887085\n",
      "Epoch 733, Loss: 0.6557344198226929, Final Batch Loss: 0.3595878481864929\n",
      "Epoch 734, Loss: 0.5773106664419174, Final Batch Loss: 0.2476464956998825\n",
      "Epoch 735, Loss: 0.617164820432663, Final Batch Loss: 0.3236881494522095\n",
      "Epoch 736, Loss: 0.6862873435020447, Final Batch Loss: 0.3773517310619354\n",
      "Epoch 737, Loss: 0.6793427169322968, Final Batch Loss: 0.3959989547729492\n",
      "Epoch 738, Loss: 0.628809005022049, Final Batch Loss: 0.3517128527164459\n",
      "Epoch 739, Loss: 0.607822984457016, Final Batch Loss: 0.29976341128349304\n",
      "Epoch 740, Loss: 0.5821379125118256, Final Batch Loss: 0.2808026373386383\n",
      "Epoch 741, Loss: 0.6118100881576538, Final Batch Loss: 0.30242782831192017\n",
      "Epoch 742, Loss: 0.6608760058879852, Final Batch Loss: 0.34905269742012024\n",
      "Epoch 743, Loss: 0.6318548023700714, Final Batch Loss: 0.3092113435268402\n",
      "Epoch 744, Loss: 0.7148280143737793, Final Batch Loss: 0.39072301983833313\n",
      "Epoch 745, Loss: 0.6836482882499695, Final Batch Loss: 0.34426307678222656\n",
      "Epoch 746, Loss: 0.5891319215297699, Final Batch Loss: 0.26151421666145325\n",
      "Epoch 747, Loss: 0.5623564422130585, Final Batch Loss: 0.28584596514701843\n",
      "Epoch 748, Loss: 0.5993366539478302, Final Batch Loss: 0.30483847856521606\n",
      "Epoch 749, Loss: 0.6406150460243225, Final Batch Loss: 0.37328219413757324\n",
      "Epoch 750, Loss: 0.6630085110664368, Final Batch Loss: 0.33272257447242737\n",
      "Epoch 751, Loss: 0.6988580226898193, Final Batch Loss: 0.31784123182296753\n",
      "Epoch 752, Loss: 0.637772262096405, Final Batch Loss: 0.36870476603507996\n",
      "Epoch 753, Loss: 0.6299715042114258, Final Batch Loss: 0.3303126394748688\n",
      "Epoch 754, Loss: 0.6966420114040375, Final Batch Loss: 0.37991082668304443\n",
      "Epoch 755, Loss: 0.5645418167114258, Final Batch Loss: 0.24567678570747375\n",
      "Epoch 756, Loss: 0.6081888675689697, Final Batch Loss: 0.31082186102867126\n",
      "Epoch 757, Loss: 0.6084538698196411, Final Batch Loss: 0.27806591987609863\n",
      "Epoch 758, Loss: 0.6120469272136688, Final Batch Loss: 0.27661049365997314\n",
      "Epoch 759, Loss: 0.6666058897972107, Final Batch Loss: 0.32583382725715637\n",
      "Epoch 760, Loss: 0.5813237875699997, Final Batch Loss: 0.2482297271490097\n",
      "Epoch 761, Loss: 0.5788431763648987, Final Batch Loss: 0.27239203453063965\n",
      "Epoch 762, Loss: 0.6448995769023895, Final Batch Loss: 0.30370065569877625\n",
      "Epoch 763, Loss: 0.6240313649177551, Final Batch Loss: 0.30838268995285034\n",
      "Epoch 764, Loss: 0.6367868781089783, Final Batch Loss: 0.29442790150642395\n",
      "Epoch 765, Loss: 0.6708699464797974, Final Batch Loss: 0.4034344255924225\n",
      "Epoch 766, Loss: 0.6069944202899933, Final Batch Loss: 0.32169920206069946\n",
      "Epoch 767, Loss: 0.566757470369339, Final Batch Loss: 0.28608110547065735\n",
      "Epoch 768, Loss: 0.6344009935855865, Final Batch Loss: 0.2644464075565338\n",
      "Epoch 769, Loss: 0.6021813154220581, Final Batch Loss: 0.293769896030426\n",
      "Epoch 770, Loss: 0.6202191710472107, Final Batch Loss: 0.31266433000564575\n",
      "Epoch 771, Loss: 0.6562110185623169, Final Batch Loss: 0.305098295211792\n",
      "Epoch 772, Loss: 0.647331565618515, Final Batch Loss: 0.32740721106529236\n",
      "Epoch 773, Loss: 0.5838009119033813, Final Batch Loss: 0.27805793285369873\n",
      "Epoch 774, Loss: 0.5692249536514282, Final Batch Loss: 0.2719240188598633\n",
      "Epoch 775, Loss: 0.6604633629322052, Final Batch Loss: 0.3581668734550476\n",
      "Epoch 776, Loss: 0.6773514449596405, Final Batch Loss: 0.350982129573822\n",
      "Epoch 777, Loss: 0.6251128911972046, Final Batch Loss: 0.29548701643943787\n",
      "Epoch 778, Loss: 0.5916700661182404, Final Batch Loss: 0.29574066400527954\n",
      "Epoch 779, Loss: 0.5900631248950958, Final Batch Loss: 0.27235448360443115\n",
      "Epoch 780, Loss: 0.6614766120910645, Final Batch Loss: 0.3402445316314697\n",
      "Epoch 781, Loss: 0.5635380148887634, Final Batch Loss: 0.2731294631958008\n",
      "Epoch 782, Loss: 0.6106108725070953, Final Batch Loss: 0.30761322379112244\n",
      "Epoch 783, Loss: 0.5830631256103516, Final Batch Loss: 0.2367647886276245\n",
      "Epoch 784, Loss: 0.5400228053331375, Final Batch Loss: 0.2429569512605667\n",
      "Epoch 785, Loss: 0.5776547789573669, Final Batch Loss: 0.23221096396446228\n",
      "Epoch 786, Loss: 0.6346786320209503, Final Batch Loss: 0.33856725692749023\n",
      "Epoch 787, Loss: 0.611175149679184, Final Batch Loss: 0.2858589291572571\n",
      "Epoch 788, Loss: 0.5910540223121643, Final Batch Loss: 0.3295077979564667\n",
      "Epoch 789, Loss: 0.64545738697052, Final Batch Loss: 0.3416425883769989\n",
      "Epoch 790, Loss: 0.5818918794393539, Final Batch Loss: 0.3411191999912262\n",
      "Epoch 791, Loss: 0.5305554866790771, Final Batch Loss: 0.24346068501472473\n",
      "Epoch 792, Loss: 0.6401464343070984, Final Batch Loss: 0.298877090215683\n",
      "Epoch 793, Loss: 0.5907174646854401, Final Batch Loss: 0.31979137659072876\n",
      "Epoch 794, Loss: 0.6066813468933105, Final Batch Loss: 0.2737218141555786\n",
      "Epoch 795, Loss: 0.5925451517105103, Final Batch Loss: 0.27482423186302185\n",
      "Epoch 796, Loss: 0.6474703848361969, Final Batch Loss: 0.33786284923553467\n",
      "Epoch 797, Loss: 0.6010518372058868, Final Batch Loss: 0.25985440611839294\n",
      "Epoch 798, Loss: 0.5839095562696457, Final Batch Loss: 0.23278553783893585\n",
      "Epoch 799, Loss: 0.5765838325023651, Final Batch Loss: 0.2868916988372803\n",
      "Epoch 800, Loss: 0.6351008415222168, Final Batch Loss: 0.35051849484443665\n",
      "Epoch 801, Loss: 0.5742091536521912, Final Batch Loss: 0.2655036747455597\n",
      "Epoch 802, Loss: 0.5642909705638885, Final Batch Loss: 0.25989118218421936\n",
      "Epoch 803, Loss: 0.5825101435184479, Final Batch Loss: 0.32377082109451294\n",
      "Epoch 804, Loss: 0.62620809674263, Final Batch Loss: 0.3067077100276947\n",
      "Epoch 805, Loss: 0.6221492886543274, Final Batch Loss: 0.29106605052948\n",
      "Epoch 806, Loss: 0.6417374610900879, Final Batch Loss: 0.31213778257369995\n",
      "Epoch 807, Loss: 0.5942025184631348, Final Batch Loss: 0.2933923304080963\n",
      "Epoch 808, Loss: 0.5912339091300964, Final Batch Loss: 0.2593671381473541\n",
      "Epoch 809, Loss: 0.6561638414859772, Final Batch Loss: 0.3340015411376953\n",
      "Epoch 810, Loss: 0.6639425456523895, Final Batch Loss: 0.3567141592502594\n",
      "Epoch 811, Loss: 0.5610146820545197, Final Batch Loss: 0.29367703199386597\n",
      "Epoch 812, Loss: 0.6816962957382202, Final Batch Loss: 0.34418785572052\n",
      "Epoch 813, Loss: 0.57273069024086, Final Batch Loss: 0.29588255286216736\n",
      "Epoch 814, Loss: 0.5427862405776978, Final Batch Loss: 0.2693116366863251\n",
      "Epoch 815, Loss: 0.6549676358699799, Final Batch Loss: 0.3308459520339966\n",
      "Epoch 816, Loss: 0.6941432356834412, Final Batch Loss: 0.3873152732849121\n",
      "Epoch 817, Loss: 0.6309930384159088, Final Batch Loss: 0.2833622694015503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 818, Loss: 0.6549912691116333, Final Batch Loss: 0.35484057664871216\n",
      "Epoch 819, Loss: 0.6175611317157745, Final Batch Loss: 0.3080560564994812\n",
      "Epoch 820, Loss: 0.5614932775497437, Final Batch Loss: 0.27252212166786194\n",
      "Epoch 821, Loss: 0.5985888242721558, Final Batch Loss: 0.3037252724170685\n",
      "Epoch 822, Loss: 0.6659311354160309, Final Batch Loss: 0.37532442808151245\n",
      "Epoch 823, Loss: 0.6166651248931885, Final Batch Loss: 0.3243793547153473\n",
      "Epoch 824, Loss: 0.5824150741100311, Final Batch Loss: 0.26801982522010803\n",
      "Epoch 825, Loss: 0.571381539106369, Final Batch Loss: 0.22566360235214233\n",
      "Epoch 826, Loss: 0.5752822458744049, Final Batch Loss: 0.25660213828086853\n",
      "Epoch 827, Loss: 0.5816910266876221, Final Batch Loss: 0.2947670817375183\n",
      "Epoch 828, Loss: 0.5490164458751678, Final Batch Loss: 0.25345125794410706\n",
      "Epoch 829, Loss: 0.5619674921035767, Final Batch Loss: 0.30340179800987244\n",
      "Epoch 830, Loss: 0.6032421588897705, Final Batch Loss: 0.2974472641944885\n",
      "Epoch 831, Loss: 0.6451764702796936, Final Batch Loss: 0.2985275983810425\n",
      "Epoch 832, Loss: 0.57120481133461, Final Batch Loss: 0.3045195937156677\n",
      "Epoch 833, Loss: 0.5928795635700226, Final Batch Loss: 0.31700971722602844\n",
      "Epoch 834, Loss: 0.5354476869106293, Final Batch Loss: 0.23340490460395813\n",
      "Epoch 835, Loss: 0.6670246124267578, Final Batch Loss: 0.28605055809020996\n",
      "Epoch 836, Loss: 0.5324166268110275, Final Batch Loss: 0.2374744862318039\n",
      "Epoch 837, Loss: 0.600157767534256, Final Batch Loss: 0.32310622930526733\n",
      "Epoch 838, Loss: 0.5387783497571945, Final Batch Loss: 0.2249731868505478\n",
      "Epoch 839, Loss: 0.5930754542350769, Final Batch Loss: 0.2546396851539612\n",
      "Epoch 840, Loss: 0.5962958484888077, Final Batch Loss: 0.22432221472263336\n",
      "Epoch 841, Loss: 0.5736121833324432, Final Batch Loss: 0.26859164237976074\n",
      "Epoch 842, Loss: 0.586988627910614, Final Batch Loss: 0.33147960901260376\n",
      "Epoch 843, Loss: 0.5787154138088226, Final Batch Loss: 0.2560088336467743\n",
      "Epoch 844, Loss: 0.5904785394668579, Final Batch Loss: 0.2723662555217743\n",
      "Epoch 845, Loss: 0.5499004572629929, Final Batch Loss: 0.22874678671360016\n",
      "Epoch 846, Loss: 0.600690096616745, Final Batch Loss: 0.3451542258262634\n",
      "Epoch 847, Loss: 0.6446413397789001, Final Batch Loss: 0.30275028944015503\n",
      "Epoch 848, Loss: 0.577565461397171, Final Batch Loss: 0.22450324892997742\n",
      "Epoch 849, Loss: 0.602470725774765, Final Batch Loss: 0.3035767674446106\n",
      "Epoch 850, Loss: 0.5798958241939545, Final Batch Loss: 0.2947947382926941\n",
      "Epoch 851, Loss: 0.5058551579713821, Final Batch Loss: 0.22665075957775116\n",
      "Epoch 852, Loss: 0.6779879629611969, Final Batch Loss: 0.3479750156402588\n",
      "Epoch 853, Loss: 0.5983834266662598, Final Batch Loss: 0.32170790433883667\n",
      "Epoch 854, Loss: 0.5907277762889862, Final Batch Loss: 0.2702714502811432\n",
      "Epoch 855, Loss: 0.6422904431819916, Final Batch Loss: 0.32956546545028687\n",
      "Epoch 856, Loss: 0.5900798439979553, Final Batch Loss: 0.34271347522735596\n",
      "Epoch 857, Loss: 0.5984467267990112, Final Batch Loss: 0.27200883626937866\n",
      "Epoch 858, Loss: 0.6089185178279877, Final Batch Loss: 0.29910194873809814\n",
      "Epoch 859, Loss: 0.6215208768844604, Final Batch Loss: 0.3216506242752075\n",
      "Epoch 860, Loss: 0.6353094577789307, Final Batch Loss: 0.3482125401496887\n",
      "Epoch 861, Loss: 0.6319171786308289, Final Batch Loss: 0.3033996522426605\n",
      "Epoch 862, Loss: 0.6311130225658417, Final Batch Loss: 0.28324201703071594\n",
      "Epoch 863, Loss: 0.6043546497821808, Final Batch Loss: 0.3500800132751465\n",
      "Epoch 864, Loss: 0.6053094565868378, Final Batch Loss: 0.33237120509147644\n",
      "Epoch 865, Loss: 0.6802762150764465, Final Batch Loss: 0.29422977566719055\n",
      "Epoch 866, Loss: 0.6131045818328857, Final Batch Loss: 0.3340524137020111\n",
      "Epoch 867, Loss: 0.5929996073246002, Final Batch Loss: 0.2915201783180237\n",
      "Epoch 868, Loss: 0.6116184592247009, Final Batch Loss: 0.33551979064941406\n",
      "Epoch 869, Loss: 0.6776399612426758, Final Batch Loss: 0.38353216648101807\n",
      "Epoch 870, Loss: 0.56430384516716, Final Batch Loss: 0.2705181837081909\n",
      "Epoch 871, Loss: 0.5802172422409058, Final Batch Loss: 0.301850289106369\n",
      "Epoch 872, Loss: 0.6048691272735596, Final Batch Loss: 0.2830967903137207\n",
      "Epoch 873, Loss: 0.5794431567192078, Final Batch Loss: 0.2832055985927582\n",
      "Epoch 874, Loss: 0.6493973731994629, Final Batch Loss: 0.3503185212612152\n",
      "Epoch 875, Loss: 0.5934149920940399, Final Batch Loss: 0.32677751779556274\n",
      "Epoch 876, Loss: 0.587544322013855, Final Batch Loss: 0.2995298206806183\n",
      "Epoch 877, Loss: 0.6315532028675079, Final Batch Loss: 0.3371818959712982\n",
      "Epoch 878, Loss: 0.5459917932748795, Final Batch Loss: 0.22061921656131744\n",
      "Epoch 879, Loss: 0.5973215103149414, Final Batch Loss: 0.2893540859222412\n",
      "Epoch 880, Loss: 0.5685805380344391, Final Batch Loss: 0.29278072714805603\n",
      "Epoch 881, Loss: 0.6283085942268372, Final Batch Loss: 0.30361172556877136\n",
      "Epoch 882, Loss: 0.5637044012546539, Final Batch Loss: 0.2896646559238434\n",
      "Epoch 883, Loss: 0.6343284845352173, Final Batch Loss: 0.3110835552215576\n",
      "Epoch 884, Loss: 0.6693999469280243, Final Batch Loss: 0.4006057381629944\n",
      "Epoch 885, Loss: 0.5370441973209381, Final Batch Loss: 0.266379177570343\n",
      "Epoch 886, Loss: 0.5710902512073517, Final Batch Loss: 0.2978368401527405\n",
      "Epoch 887, Loss: 0.5698137879371643, Final Batch Loss: 0.31828099489212036\n",
      "Epoch 888, Loss: 0.5609642863273621, Final Batch Loss: 0.26888778805732727\n",
      "Epoch 889, Loss: 0.5025015324354172, Final Batch Loss: 0.21413986384868622\n",
      "Epoch 890, Loss: 0.6427159011363983, Final Batch Loss: 0.34872570633888245\n",
      "Epoch 891, Loss: 0.5779570639133453, Final Batch Loss: 0.2875273525714874\n",
      "Epoch 892, Loss: 0.6024642586708069, Final Batch Loss: 0.3093509078025818\n",
      "Epoch 893, Loss: 0.5608152449131012, Final Batch Loss: 0.2886035144329071\n",
      "Epoch 894, Loss: 0.567385345697403, Final Batch Loss: 0.3064539134502411\n",
      "Epoch 895, Loss: 0.5528091788291931, Final Batch Loss: 0.2673473358154297\n",
      "Epoch 896, Loss: 0.6263243556022644, Final Batch Loss: 0.27810391783714294\n",
      "Epoch 897, Loss: 0.5233154147863388, Final Batch Loss: 0.2334458976984024\n",
      "Epoch 898, Loss: 0.5397554039955139, Final Batch Loss: 0.26074981689453125\n",
      "Epoch 899, Loss: 0.4992682635784149, Final Batch Loss: 0.21817436814308167\n",
      "Epoch 900, Loss: 0.560901328921318, Final Batch Loss: 0.32708150148391724\n",
      "Epoch 901, Loss: 0.6154395043849945, Final Batch Loss: 0.34230515360832214\n",
      "Epoch 902, Loss: 0.5851318836212158, Final Batch Loss: 0.26050621271133423\n",
      "Epoch 903, Loss: 0.621805727481842, Final Batch Loss: 0.3303571045398712\n",
      "Epoch 904, Loss: 0.5735025405883789, Final Batch Loss: 0.2921808958053589\n",
      "Epoch 905, Loss: 0.5642814636230469, Final Batch Loss: 0.28665590286254883\n",
      "Epoch 906, Loss: 0.5955029428005219, Final Batch Loss: 0.2670079171657562\n",
      "Epoch 907, Loss: 0.6295180022716522, Final Batch Loss: 0.27543655037879944\n",
      "Epoch 908, Loss: 0.5498009771108627, Final Batch Loss: 0.2424987405538559\n",
      "Epoch 909, Loss: 0.5628684759140015, Final Batch Loss: 0.28413820266723633\n",
      "Epoch 910, Loss: 0.63862544298172, Final Batch Loss: 0.35133033990859985\n",
      "Epoch 911, Loss: 0.5497236847877502, Final Batch Loss: 0.2565295398235321\n",
      "Epoch 912, Loss: 0.6337800025939941, Final Batch Loss: 0.31991955637931824\n",
      "Epoch 913, Loss: 0.6097066402435303, Final Batch Loss: 0.33749526739120483\n",
      "Epoch 914, Loss: 0.6077137887477875, Final Batch Loss: 0.3501884639263153\n",
      "Epoch 915, Loss: 0.5854732394218445, Final Batch Loss: 0.31854790449142456\n",
      "Epoch 916, Loss: 0.5540833622217178, Final Batch Loss: 0.24807898700237274\n",
      "Epoch 917, Loss: 0.7229567170143127, Final Batch Loss: 0.42649298906326294\n",
      "Epoch 918, Loss: 0.641658753156662, Final Batch Loss: 0.40156421065330505\n",
      "Epoch 919, Loss: 0.5483331382274628, Final Batch Loss: 0.251007080078125\n",
      "Epoch 920, Loss: 0.5564457774162292, Final Batch Loss: 0.3036389648914337\n",
      "Epoch 921, Loss: 0.5895642638206482, Final Batch Loss: 0.2822420597076416\n",
      "Epoch 922, Loss: 0.6364950239658356, Final Batch Loss: 0.31134796142578125\n",
      "Epoch 923, Loss: 0.6005422472953796, Final Batch Loss: 0.3097723126411438\n",
      "Epoch 924, Loss: 0.558836355805397, Final Batch Loss: 0.24599306285381317\n",
      "Epoch 925, Loss: 0.6052058041095734, Final Batch Loss: 0.2829461097717285\n",
      "Epoch 926, Loss: 0.5332755148410797, Final Batch Loss: 0.29533812403678894\n",
      "Epoch 927, Loss: 0.6183977723121643, Final Batch Loss: 0.3466799259185791\n",
      "Epoch 928, Loss: 0.5356783866882324, Final Batch Loss: 0.24099770188331604\n",
      "Epoch 929, Loss: 0.5830403864383698, Final Batch Loss: 0.2743651866912842\n",
      "Epoch 930, Loss: 0.5367923229932785, Final Batch Loss: 0.2411624640226364\n",
      "Epoch 931, Loss: 0.5973508358001709, Final Batch Loss: 0.26381999254226685\n",
      "Epoch 932, Loss: 0.4905768483877182, Final Batch Loss: 0.22388426959514618\n",
      "Epoch 933, Loss: 0.5226726680994034, Final Batch Loss: 0.21833790838718414\n",
      "Epoch 934, Loss: 0.643185019493103, Final Batch Loss: 0.3512391149997711\n",
      "Epoch 935, Loss: 0.5877732336521149, Final Batch Loss: 0.31666597723960876\n",
      "Epoch 936, Loss: 0.5340559184551239, Final Batch Loss: 0.21863719820976257\n",
      "Epoch 937, Loss: 0.6274779736995697, Final Batch Loss: 0.36387521028518677\n",
      "Epoch 938, Loss: 0.6251676082611084, Final Batch Loss: 0.37138304114341736\n",
      "Epoch 939, Loss: 0.5726809799671173, Final Batch Loss: 0.2796698212623596\n",
      "Epoch 940, Loss: 0.5613943636417389, Final Batch Loss: 0.2578142583370209\n",
      "Epoch 941, Loss: 0.5175107419490814, Final Batch Loss: 0.21294254064559937\n",
      "Epoch 942, Loss: 0.5709150731563568, Final Batch Loss: 0.2713317573070526\n",
      "Epoch 943, Loss: 0.5554840564727783, Final Batch Loss: 0.265830934047699\n",
      "Epoch 944, Loss: 0.6128641068935394, Final Batch Loss: 0.36201804876327515\n",
      "Epoch 945, Loss: 0.5509636998176575, Final Batch Loss: 0.2922639846801758\n",
      "Epoch 946, Loss: 0.5648833215236664, Final Batch Loss: 0.31184738874435425\n",
      "Epoch 947, Loss: 0.525140106678009, Final Batch Loss: 0.2479553520679474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 948, Loss: 0.6049846410751343, Final Batch Loss: 0.2801220118999481\n",
      "Epoch 949, Loss: 0.548089325428009, Final Batch Loss: 0.2616775929927826\n",
      "Epoch 950, Loss: 0.5654671788215637, Final Batch Loss: 0.2590475082397461\n",
      "Epoch 951, Loss: 0.6845388412475586, Final Batch Loss: 0.38051676750183105\n",
      "Epoch 952, Loss: 0.5316576957702637, Final Batch Loss: 0.2897903025150299\n",
      "Epoch 953, Loss: 0.53892982006073, Final Batch Loss: 0.27482733130455017\n",
      "Epoch 954, Loss: 0.5515120625495911, Final Batch Loss: 0.23401090502738953\n",
      "Epoch 955, Loss: 0.5490535497665405, Final Batch Loss: 0.26734673976898193\n",
      "Epoch 956, Loss: 0.47932396829128265, Final Batch Loss: 0.24095740914344788\n",
      "Epoch 957, Loss: 0.5486076176166534, Final Batch Loss: 0.2695959210395813\n",
      "Epoch 958, Loss: 0.5564975738525391, Final Batch Loss: 0.2724857032299042\n",
      "Epoch 959, Loss: 0.5831862986087799, Final Batch Loss: 0.2784600555896759\n",
      "Epoch 960, Loss: 0.5485262721776962, Final Batch Loss: 0.23788268864154816\n",
      "Epoch 961, Loss: 0.5209991931915283, Final Batch Loss: 0.2496965527534485\n",
      "Epoch 962, Loss: 0.5847328007221222, Final Batch Loss: 0.3114580810070038\n",
      "Epoch 963, Loss: 0.5940181910991669, Final Batch Loss: 0.2888682186603546\n",
      "Epoch 964, Loss: 0.5883553922176361, Final Batch Loss: 0.26699936389923096\n",
      "Epoch 965, Loss: 0.6660925149917603, Final Batch Loss: 0.37936297059059143\n",
      "Epoch 966, Loss: 0.6450675427913666, Final Batch Loss: 0.3610028922557831\n",
      "Epoch 967, Loss: 0.5407918393611908, Final Batch Loss: 0.28370290994644165\n",
      "Epoch 968, Loss: 0.5098648071289062, Final Batch Loss: 0.2481985092163086\n",
      "Epoch 969, Loss: 0.5187120288610458, Final Batch Loss: 0.23531357944011688\n",
      "Epoch 970, Loss: 0.529625415802002, Final Batch Loss: 0.240586519241333\n",
      "Epoch 971, Loss: 0.5353522002696991, Final Batch Loss: 0.2630261182785034\n",
      "Epoch 972, Loss: 0.6237590610980988, Final Batch Loss: 0.35262981057167053\n",
      "Epoch 973, Loss: 0.5202179700136185, Final Batch Loss: 0.2120312601327896\n",
      "Epoch 974, Loss: 0.5244970619678497, Final Batch Loss: 0.28160062432289124\n",
      "Epoch 975, Loss: 0.5397034287452698, Final Batch Loss: 0.2697201073169708\n",
      "Epoch 976, Loss: 0.5345488786697388, Final Batch Loss: 0.27774426341056824\n",
      "Epoch 977, Loss: 0.5853344798088074, Final Batch Loss: 0.3294844329357147\n",
      "Epoch 978, Loss: 0.5184219181537628, Final Batch Loss: 0.2527163624763489\n",
      "Epoch 979, Loss: 0.5365415811538696, Final Batch Loss: 0.284714013338089\n",
      "Epoch 980, Loss: 0.5770913064479828, Final Batch Loss: 0.326454758644104\n",
      "Epoch 981, Loss: 0.5826243460178375, Final Batch Loss: 0.2761739194393158\n",
      "Epoch 982, Loss: 0.5929422378540039, Final Batch Loss: 0.30430856347084045\n",
      "Epoch 983, Loss: 0.6002568602561951, Final Batch Loss: 0.28719115257263184\n",
      "Epoch 984, Loss: 0.597259521484375, Final Batch Loss: 0.3059016168117523\n",
      "Epoch 985, Loss: 0.5389496982097626, Final Batch Loss: 0.26962220668792725\n",
      "Epoch 986, Loss: 0.5870086699724197, Final Batch Loss: 0.34023505449295044\n",
      "Epoch 987, Loss: 0.5216680765151978, Final Batch Loss: 0.27488771080970764\n",
      "Epoch 988, Loss: 0.5327987372875214, Final Batch Loss: 0.2589091658592224\n",
      "Epoch 989, Loss: 0.519452691078186, Final Batch Loss: 0.24543634057044983\n",
      "Epoch 990, Loss: 0.5235674530267715, Final Batch Loss: 0.23554636538028717\n",
      "Epoch 991, Loss: 0.552487313747406, Final Batch Loss: 0.29846203327178955\n",
      "Epoch 992, Loss: 0.5298638045787811, Final Batch Loss: 0.25243401527404785\n",
      "Epoch 993, Loss: 0.5469079315662384, Final Batch Loss: 0.24756035208702087\n",
      "Epoch 994, Loss: 0.5301497876644135, Final Batch Loss: 0.278447687625885\n",
      "Epoch 995, Loss: 0.6080536544322968, Final Batch Loss: 0.3256993591785431\n",
      "Epoch 996, Loss: 0.635982096195221, Final Batch Loss: 0.338284432888031\n",
      "Epoch 997, Loss: 0.648703008890152, Final Batch Loss: 0.33076807856559753\n",
      "Epoch 998, Loss: 0.47410616278648376, Final Batch Loss: 0.2018548846244812\n",
      "Epoch 999, Loss: 0.5522722899913788, Final Batch Loss: 0.2875221371650696\n",
      "Epoch 1000, Loss: 0.5928516983985901, Final Batch Loss: 0.2777203321456909\n",
      "Epoch 1001, Loss: 0.5393113195896149, Final Batch Loss: 0.26934486627578735\n",
      "Epoch 1002, Loss: 0.52327761054039, Final Batch Loss: 0.2508140504360199\n",
      "Epoch 1003, Loss: 0.5059981495141983, Final Batch Loss: 0.22860954701900482\n",
      "Epoch 1004, Loss: 0.5706452876329422, Final Batch Loss: 0.2394927591085434\n",
      "Epoch 1005, Loss: 0.5707469582557678, Final Batch Loss: 0.2972525954246521\n",
      "Epoch 1006, Loss: 0.5066590011119843, Final Batch Loss: 0.25071629881858826\n",
      "Epoch 1007, Loss: 0.49981631338596344, Final Batch Loss: 0.22680743038654327\n",
      "Epoch 1008, Loss: 0.5501553416252136, Final Batch Loss: 0.2536381781101227\n",
      "Epoch 1009, Loss: 0.5608010590076447, Final Batch Loss: 0.29315900802612305\n",
      "Epoch 1010, Loss: 0.5380635857582092, Final Batch Loss: 0.22253364324569702\n",
      "Epoch 1011, Loss: 0.5435509830713272, Final Batch Loss: 0.2956427335739136\n",
      "Epoch 1012, Loss: 0.6127929985523224, Final Batch Loss: 0.33189958333969116\n",
      "Epoch 1013, Loss: 0.6258070468902588, Final Batch Loss: 0.3301727771759033\n",
      "Epoch 1014, Loss: 0.5627140700817108, Final Batch Loss: 0.26320070028305054\n",
      "Epoch 1015, Loss: 0.575301855802536, Final Batch Loss: 0.28508657217025757\n",
      "Epoch 1016, Loss: 0.5849617123603821, Final Batch Loss: 0.334749698638916\n",
      "Epoch 1017, Loss: 0.5288884043693542, Final Batch Loss: 0.2635623812675476\n",
      "Epoch 1018, Loss: 0.5796650946140289, Final Batch Loss: 0.3260194957256317\n",
      "Epoch 1019, Loss: 0.5443941503763199, Final Batch Loss: 0.23783887922763824\n",
      "Epoch 1020, Loss: 0.5181470364332199, Final Batch Loss: 0.28434839844703674\n",
      "Epoch 1021, Loss: 0.5375264286994934, Final Batch Loss: 0.26382553577423096\n",
      "Epoch 1022, Loss: 0.5793334245681763, Final Batch Loss: 0.30754753947257996\n",
      "Epoch 1023, Loss: 0.5180176496505737, Final Batch Loss: 0.26109352707862854\n",
      "Epoch 1024, Loss: 0.5459921360015869, Final Batch Loss: 0.24534860253334045\n",
      "Epoch 1025, Loss: 0.5112784802913666, Final Batch Loss: 0.2753106653690338\n",
      "Epoch 1026, Loss: 0.5418751984834671, Final Batch Loss: 0.23597602546215057\n",
      "Epoch 1027, Loss: 0.5026853084564209, Final Batch Loss: 0.2576242685317993\n",
      "Epoch 1028, Loss: 0.5372810512781143, Final Batch Loss: 0.2903537452220917\n",
      "Epoch 1029, Loss: 0.5438251197338104, Final Batch Loss: 0.26460811495780945\n",
      "Epoch 1030, Loss: 0.5585917532444, Final Batch Loss: 0.2725829780101776\n",
      "Epoch 1031, Loss: 0.5251540094614029, Final Batch Loss: 0.2848641276359558\n",
      "Epoch 1032, Loss: 0.553176999092102, Final Batch Loss: 0.30480390787124634\n",
      "Epoch 1033, Loss: 0.5749384164810181, Final Batch Loss: 0.28172925114631653\n",
      "Epoch 1034, Loss: 0.5561200976371765, Final Batch Loss: 0.26398688554763794\n",
      "Epoch 1035, Loss: 0.48916637897491455, Final Batch Loss: 0.24410562217235565\n",
      "Epoch 1036, Loss: 0.4925693869590759, Final Batch Loss: 0.20499882102012634\n",
      "Epoch 1037, Loss: 0.5305844694375992, Final Batch Loss: 0.21882785856723785\n",
      "Epoch 1038, Loss: 0.5121000707149506, Final Batch Loss: 0.2503262758255005\n",
      "Epoch 1039, Loss: 0.5480960607528687, Final Batch Loss: 0.2947050631046295\n",
      "Epoch 1040, Loss: 0.5158896744251251, Final Batch Loss: 0.2373008131980896\n",
      "Epoch 1041, Loss: 0.5503906905651093, Final Batch Loss: 0.26074138283729553\n",
      "Epoch 1042, Loss: 0.6459835469722748, Final Batch Loss: 0.3421010673046112\n",
      "Epoch 1043, Loss: 0.4859222173690796, Final Batch Loss: 0.18631473183631897\n",
      "Epoch 1044, Loss: 0.5556307435035706, Final Batch Loss: 0.27469688653945923\n",
      "Epoch 1045, Loss: 0.5436455607414246, Final Batch Loss: 0.2856085002422333\n",
      "Epoch 1046, Loss: 0.6216621696949005, Final Batch Loss: 0.2906860113143921\n",
      "Epoch 1047, Loss: 0.5362766683101654, Final Batch Loss: 0.26796191930770874\n",
      "Epoch 1048, Loss: 0.5149798393249512, Final Batch Loss: 0.2626629173755646\n",
      "Epoch 1049, Loss: 0.5024303644895554, Final Batch Loss: 0.23868529498577118\n",
      "Epoch 1050, Loss: 0.5252656638622284, Final Batch Loss: 0.25483399629592896\n",
      "Epoch 1051, Loss: 0.5237916707992554, Final Batch Loss: 0.25052568316459656\n",
      "Epoch 1052, Loss: 0.5742962658405304, Final Batch Loss: 0.2879573106765747\n",
      "Epoch 1053, Loss: 0.534110814332962, Final Batch Loss: 0.25971856713294983\n",
      "Epoch 1054, Loss: 0.5372290909290314, Final Batch Loss: 0.25174757838249207\n",
      "Epoch 1055, Loss: 0.5069597661495209, Final Batch Loss: 0.24226120114326477\n",
      "Epoch 1056, Loss: 0.4691402167081833, Final Batch Loss: 0.22194111347198486\n",
      "Epoch 1057, Loss: 0.5405401885509491, Final Batch Loss: 0.24480566382408142\n",
      "Epoch 1058, Loss: 0.5291944742202759, Final Batch Loss: 0.20833930373191833\n",
      "Epoch 1059, Loss: 0.537173718214035, Final Batch Loss: 0.2857867479324341\n",
      "Epoch 1060, Loss: 0.5074440687894821, Final Batch Loss: 0.22301821410655975\n",
      "Epoch 1061, Loss: 0.5030297040939331, Final Batch Loss: 0.2514192759990692\n",
      "Epoch 1062, Loss: 0.48749031126499176, Final Batch Loss: 0.23002223670482635\n",
      "Epoch 1063, Loss: 0.5643852800130844, Final Batch Loss: 0.22283293306827545\n",
      "Epoch 1064, Loss: 0.4793449193239212, Final Batch Loss: 0.23093530535697937\n",
      "Epoch 1065, Loss: 0.5276975035667419, Final Batch Loss: 0.2895459234714508\n",
      "Epoch 1066, Loss: 0.5105887800455093, Final Batch Loss: 0.26228970289230347\n",
      "Epoch 1067, Loss: 0.49947404861450195, Final Batch Loss: 0.2134423851966858\n",
      "Epoch 1068, Loss: 0.6312593519687653, Final Batch Loss: 0.2640046179294586\n",
      "Epoch 1069, Loss: 0.49469251930713654, Final Batch Loss: 0.24756301939487457\n",
      "Epoch 1070, Loss: 0.48445503413677216, Final Batch Loss: 0.229570671916008\n",
      "Epoch 1071, Loss: 0.5102607607841492, Final Batch Loss: 0.24973031878471375\n",
      "Epoch 1072, Loss: 0.6083407998085022, Final Batch Loss: 0.32606953382492065\n",
      "Epoch 1073, Loss: 0.5484901368618011, Final Batch Loss: 0.2519960403442383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1074, Loss: 0.585425615310669, Final Batch Loss: 0.25389593839645386\n",
      "Epoch 1075, Loss: 0.5832071453332901, Final Batch Loss: 0.3512193262577057\n",
      "Epoch 1076, Loss: 0.5611009895801544, Final Batch Loss: 0.3002293109893799\n",
      "Epoch 1077, Loss: 0.5016189366579056, Final Batch Loss: 0.2623226046562195\n",
      "Epoch 1078, Loss: 0.554379016160965, Final Batch Loss: 0.29997730255126953\n",
      "Epoch 1079, Loss: 0.5152860879898071, Final Batch Loss: 0.25373175740242004\n",
      "Epoch 1080, Loss: 0.5337805151939392, Final Batch Loss: 0.25166937708854675\n",
      "Epoch 1081, Loss: 0.47437770664691925, Final Batch Loss: 0.2171727567911148\n",
      "Epoch 1082, Loss: 0.47437116503715515, Final Batch Loss: 0.2404412031173706\n",
      "Epoch 1083, Loss: 0.5531199276447296, Final Batch Loss: 0.2703004777431488\n",
      "Epoch 1084, Loss: 0.5765208005905151, Final Batch Loss: 0.273471474647522\n",
      "Epoch 1085, Loss: 0.6061970591545105, Final Batch Loss: 0.35584762692451477\n",
      "Epoch 1086, Loss: 0.5659970343112946, Final Batch Loss: 0.2879396080970764\n",
      "Epoch 1087, Loss: 0.5226145386695862, Final Batch Loss: 0.2704496681690216\n",
      "Epoch 1088, Loss: 0.5476287305355072, Final Batch Loss: 0.3073488175868988\n",
      "Epoch 1089, Loss: 0.4530019462108612, Final Batch Loss: 0.19712308049201965\n",
      "Epoch 1090, Loss: 0.5868956744670868, Final Batch Loss: 0.27863791584968567\n",
      "Epoch 1091, Loss: 0.5206954479217529, Final Batch Loss: 0.2651161551475525\n",
      "Epoch 1092, Loss: 0.5235328674316406, Final Batch Loss: 0.23379608988761902\n",
      "Epoch 1093, Loss: 0.5429109334945679, Final Batch Loss: 0.26541757583618164\n",
      "Epoch 1094, Loss: 0.5533593595027924, Final Batch Loss: 0.31685110926628113\n",
      "Epoch 1095, Loss: 0.44754166901111603, Final Batch Loss: 0.16493256390094757\n",
      "Epoch 1096, Loss: 0.553244560956955, Final Batch Loss: 0.3002941906452179\n",
      "Epoch 1097, Loss: 0.49732863903045654, Final Batch Loss: 0.2324916124343872\n",
      "Epoch 1098, Loss: 0.48490409553050995, Final Batch Loss: 0.2217332273721695\n",
      "Epoch 1099, Loss: 0.5413099527359009, Final Batch Loss: 0.252238392829895\n",
      "Epoch 1100, Loss: 0.5319123864173889, Final Batch Loss: 0.2509303092956543\n",
      "Epoch 1101, Loss: 0.5410678386688232, Final Batch Loss: 0.27920183539390564\n",
      "Epoch 1102, Loss: 0.4853746294975281, Final Batch Loss: 0.26409319043159485\n",
      "Epoch 1103, Loss: 0.5772546529769897, Final Batch Loss: 0.3193531632423401\n",
      "Epoch 1104, Loss: 0.4800771325826645, Final Batch Loss: 0.2172561138868332\n",
      "Epoch 1105, Loss: 0.4903610646724701, Final Batch Loss: 0.25147974491119385\n",
      "Epoch 1106, Loss: 0.5714995861053467, Final Batch Loss: 0.2998735010623932\n",
      "Epoch 1107, Loss: 0.5857276916503906, Final Batch Loss: 0.29266247153282166\n",
      "Epoch 1108, Loss: 0.49568575620651245, Final Batch Loss: 0.22894713282585144\n",
      "Epoch 1109, Loss: 0.4928973317146301, Final Batch Loss: 0.2725341320037842\n",
      "Epoch 1110, Loss: 0.4787243753671646, Final Batch Loss: 0.216399148106575\n",
      "Epoch 1111, Loss: 0.5106878876686096, Final Batch Loss: 0.2566803991794586\n",
      "Epoch 1112, Loss: 0.5462116301059723, Final Batch Loss: 0.29482388496398926\n",
      "Epoch 1113, Loss: 0.5245392173528671, Final Batch Loss: 0.23511238396167755\n",
      "Epoch 1114, Loss: 0.5382656455039978, Final Batch Loss: 0.264775812625885\n",
      "Epoch 1115, Loss: 0.5821543335914612, Final Batch Loss: 0.3116097152233124\n",
      "Epoch 1116, Loss: 0.46882979571819305, Final Batch Loss: 0.25786352157592773\n",
      "Epoch 1117, Loss: 0.5406632125377655, Final Batch Loss: 0.2803722321987152\n",
      "Epoch 1118, Loss: 0.5604187548160553, Final Batch Loss: 0.2763916254043579\n",
      "Epoch 1119, Loss: 0.5276603996753693, Final Batch Loss: 0.26662853360176086\n",
      "Epoch 1120, Loss: 0.5411271452903748, Final Batch Loss: 0.30484509468078613\n",
      "Epoch 1121, Loss: 0.5084682255983353, Final Batch Loss: 0.28951501846313477\n",
      "Epoch 1122, Loss: 0.5108599960803986, Final Batch Loss: 0.29462096095085144\n",
      "Epoch 1123, Loss: 0.5028299689292908, Final Batch Loss: 0.2635630667209625\n",
      "Epoch 1124, Loss: 0.5262046754360199, Final Batch Loss: 0.20760264992713928\n",
      "Epoch 1125, Loss: 0.48453283309936523, Final Batch Loss: 0.2848520278930664\n",
      "Epoch 1126, Loss: 0.48412853479385376, Final Batch Loss: 0.2147243320941925\n",
      "Epoch 1127, Loss: 0.5196059197187424, Final Batch Loss: 0.23575012385845184\n",
      "Epoch 1128, Loss: 0.4435458481311798, Final Batch Loss: 0.1645481288433075\n",
      "Epoch 1129, Loss: 0.49873606860637665, Final Batch Loss: 0.24318160116672516\n",
      "Epoch 1130, Loss: 0.6000271439552307, Final Batch Loss: 0.2979947626590729\n",
      "Epoch 1131, Loss: 0.5108832120895386, Final Batch Loss: 0.23489245772361755\n",
      "Epoch 1132, Loss: 0.514572337269783, Final Batch Loss: 0.24505342543125153\n",
      "Epoch 1133, Loss: 0.48104770481586456, Final Batch Loss: 0.2770097553730011\n",
      "Epoch 1134, Loss: 0.5226255804300308, Final Batch Loss: 0.2431156188249588\n",
      "Epoch 1135, Loss: 0.5105880200862885, Final Batch Loss: 0.23322317004203796\n",
      "Epoch 1136, Loss: 0.5156502425670624, Final Batch Loss: 0.24649649858474731\n",
      "Epoch 1137, Loss: 0.5005260109901428, Final Batch Loss: 0.24281364679336548\n",
      "Epoch 1138, Loss: 0.48785917460918427, Final Batch Loss: 0.25850287079811096\n",
      "Epoch 1139, Loss: 0.5538422167301178, Final Batch Loss: 0.3280259966850281\n",
      "Epoch 1140, Loss: 0.5719529390335083, Final Batch Loss: 0.31822648644447327\n",
      "Epoch 1141, Loss: 0.5181457251310349, Final Batch Loss: 0.24562864005565643\n",
      "Epoch 1142, Loss: 0.49766480922698975, Final Batch Loss: 0.2438850998878479\n",
      "Epoch 1143, Loss: 0.49992401897907257, Final Batch Loss: 0.23320837318897247\n",
      "Epoch 1144, Loss: 0.507342055439949, Final Batch Loss: 0.2338445633649826\n",
      "Epoch 1145, Loss: 0.5881287753582001, Final Batch Loss: 0.34968653321266174\n",
      "Epoch 1146, Loss: 0.5694840848445892, Final Batch Loss: 0.30894505977630615\n",
      "Epoch 1147, Loss: 0.485867977142334, Final Batch Loss: 0.23322221636772156\n",
      "Epoch 1148, Loss: 0.5339963138103485, Final Batch Loss: 0.2757595479488373\n",
      "Epoch 1149, Loss: 0.5052638798952103, Final Batch Loss: 0.29107508063316345\n",
      "Epoch 1150, Loss: 0.5038343816995621, Final Batch Loss: 0.23581965267658234\n",
      "Epoch 1151, Loss: 0.4741653800010681, Final Batch Loss: 0.23299634456634521\n",
      "Epoch 1152, Loss: 0.5407768189907074, Final Batch Loss: 0.29771751165390015\n",
      "Epoch 1153, Loss: 0.5221465826034546, Final Batch Loss: 0.27112072706222534\n",
      "Epoch 1154, Loss: 0.48434866964817047, Final Batch Loss: 0.25471532344818115\n",
      "Epoch 1155, Loss: 0.48901383578777313, Final Batch Loss: 0.2236909419298172\n",
      "Epoch 1156, Loss: 0.5193642973899841, Final Batch Loss: 0.2893624007701874\n",
      "Epoch 1157, Loss: 0.4904314875602722, Final Batch Loss: 0.2464183270931244\n",
      "Epoch 1158, Loss: 0.49407272040843964, Final Batch Loss: 0.23212461173534393\n",
      "Epoch 1159, Loss: 0.508105456829071, Final Batch Loss: 0.24641624093055725\n",
      "Epoch 1160, Loss: 0.5004421919584274, Final Batch Loss: 0.3008253276348114\n",
      "Epoch 1161, Loss: 0.5293987989425659, Final Batch Loss: 0.26496854424476624\n",
      "Epoch 1162, Loss: 0.5034554302692413, Final Batch Loss: 0.21317705512046814\n",
      "Epoch 1163, Loss: 0.56830233335495, Final Batch Loss: 0.28386595845222473\n",
      "Epoch 1164, Loss: 0.5290922373533249, Final Batch Loss: 0.24272184073925018\n",
      "Epoch 1165, Loss: 0.5047830939292908, Final Batch Loss: 0.25346639752388\n",
      "Epoch 1166, Loss: 0.47349192202091217, Final Batch Loss: 0.22703105211257935\n",
      "Epoch 1167, Loss: 0.5487907826900482, Final Batch Loss: 0.266158789396286\n",
      "Epoch 1168, Loss: 0.48391345143318176, Final Batch Loss: 0.24766919016838074\n",
      "Epoch 1169, Loss: 0.5358799695968628, Final Batch Loss: 0.3324829638004303\n",
      "Epoch 1170, Loss: 0.5124191343784332, Final Batch Loss: 0.2525731027126312\n",
      "Epoch 1171, Loss: 0.5140781551599503, Final Batch Loss: 0.23637612164020538\n",
      "Epoch 1172, Loss: 0.5426882803440094, Final Batch Loss: 0.27956587076187134\n",
      "Epoch 1173, Loss: 0.5077603161334991, Final Batch Loss: 0.25313952565193176\n",
      "Epoch 1174, Loss: 0.5183389782905579, Final Batch Loss: 0.29994305968284607\n",
      "Epoch 1175, Loss: 0.5333648324012756, Final Batch Loss: 0.2735992968082428\n",
      "Epoch 1176, Loss: 0.49930091202259064, Final Batch Loss: 0.2758229374885559\n",
      "Epoch 1177, Loss: 0.5392700880765915, Final Batch Loss: 0.3069567084312439\n",
      "Epoch 1178, Loss: 0.4528064727783203, Final Batch Loss: 0.20279008150100708\n",
      "Epoch 1179, Loss: 0.4834005981683731, Final Batch Loss: 0.26285114884376526\n",
      "Epoch 1180, Loss: 0.517655074596405, Final Batch Loss: 0.2540115714073181\n",
      "Epoch 1181, Loss: 0.44522392749786377, Final Batch Loss: 0.22564946115016937\n",
      "Epoch 1182, Loss: 0.4995408058166504, Final Batch Loss: 0.23444366455078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1183, Loss: 0.4727225750684738, Final Batch Loss: 0.25394314527511597\n",
      "Epoch 1184, Loss: 0.537894606590271, Final Batch Loss: 0.2813534140586853\n",
      "Epoch 1185, Loss: 0.5172464102506638, Final Batch Loss: 0.24094067513942719\n",
      "Epoch 1186, Loss: 0.49299584329128265, Final Batch Loss: 0.23676447570323944\n",
      "Epoch 1187, Loss: 0.5508642792701721, Final Batch Loss: 0.31523653864860535\n",
      "Epoch 1188, Loss: 0.5278660655021667, Final Batch Loss: 0.30367353558540344\n",
      "Epoch 1189, Loss: 0.48606105148792267, Final Batch Loss: 0.2574315369129181\n",
      "Epoch 1190, Loss: 0.4491010308265686, Final Batch Loss: 0.23408669233322144\n",
      "Epoch 1191, Loss: 0.4704289138317108, Final Batch Loss: 0.23577214777469635\n",
      "Epoch 1192, Loss: 0.5277696996927261, Final Batch Loss: 0.23026783764362335\n",
      "Epoch 1193, Loss: 0.5229829996824265, Final Batch Loss: 0.2846897840499878\n",
      "Epoch 1194, Loss: 0.4866391718387604, Final Batch Loss: 0.2647705376148224\n",
      "Epoch 1195, Loss: 0.617420107126236, Final Batch Loss: 0.3701287508010864\n",
      "Epoch 1196, Loss: 0.4909098148345947, Final Batch Loss: 0.23287910223007202\n",
      "Epoch 1197, Loss: 0.5418592691421509, Final Batch Loss: 0.2894975244998932\n",
      "Epoch 1198, Loss: 0.556702047586441, Final Batch Loss: 0.29949951171875\n",
      "Epoch 1199, Loss: 0.5554136633872986, Final Batch Loss: 0.2923216223716736\n",
      "Epoch 1200, Loss: 0.5099331140518188, Final Batch Loss: 0.2462395429611206\n",
      "Epoch 1201, Loss: 0.48791125416755676, Final Batch Loss: 0.23098531365394592\n",
      "Epoch 1202, Loss: 0.5274407267570496, Final Batch Loss: 0.23891794681549072\n",
      "Epoch 1203, Loss: 0.4943135529756546, Final Batch Loss: 0.20921815931797028\n",
      "Epoch 1204, Loss: 0.4666415899991989, Final Batch Loss: 0.21317614614963531\n",
      "Epoch 1205, Loss: 0.5371065735816956, Final Batch Loss: 0.2337312400341034\n",
      "Epoch 1206, Loss: 0.5142049044370651, Final Batch Loss: 0.23796780407428741\n",
      "Epoch 1207, Loss: 0.5059817731380463, Final Batch Loss: 0.26236581802368164\n",
      "Epoch 1208, Loss: 0.5234064608812332, Final Batch Loss: 0.2749280631542206\n",
      "Epoch 1209, Loss: 0.5872417390346527, Final Batch Loss: 0.3610209822654724\n",
      "Epoch 1210, Loss: 0.5301945805549622, Final Batch Loss: 0.30607911944389343\n",
      "Epoch 1211, Loss: 0.525154784321785, Final Batch Loss: 0.2759404182434082\n",
      "Epoch 1212, Loss: 0.49836456775665283, Final Batch Loss: 0.2650609016418457\n",
      "Epoch 1213, Loss: 0.44958899915218353, Final Batch Loss: 0.20819249749183655\n",
      "Epoch 1214, Loss: 0.4829256981611252, Final Batch Loss: 0.2443639636039734\n",
      "Epoch 1215, Loss: 0.4707421511411667, Final Batch Loss: 0.23805777728557587\n",
      "Epoch 1216, Loss: 0.5284688472747803, Final Batch Loss: 0.27740204334259033\n",
      "Epoch 1217, Loss: 0.497072234749794, Final Batch Loss: 0.26896899938583374\n",
      "Epoch 1218, Loss: 0.5157682746648788, Final Batch Loss: 0.22538058459758759\n",
      "Epoch 1219, Loss: 0.4921429753303528, Final Batch Loss: 0.24896056950092316\n",
      "Epoch 1220, Loss: 0.46867531538009644, Final Batch Loss: 0.22190521657466888\n",
      "Epoch 1221, Loss: 0.5150905698537827, Final Batch Loss: 0.2409348338842392\n",
      "Epoch 1222, Loss: 0.5167932510375977, Final Batch Loss: 0.25501343607902527\n",
      "Epoch 1223, Loss: 0.4818313419818878, Final Batch Loss: 0.24609628319740295\n",
      "Epoch 1224, Loss: 0.4754141718149185, Final Batch Loss: 0.21869699656963348\n",
      "Epoch 1225, Loss: 0.5078456997871399, Final Batch Loss: 0.23541396856307983\n",
      "Epoch 1226, Loss: 0.5209169089794159, Final Batch Loss: 0.23560848832130432\n",
      "Epoch 1227, Loss: 0.5293507277965546, Final Batch Loss: 0.25531917810440063\n",
      "Epoch 1228, Loss: 0.5041332095861435, Final Batch Loss: 0.25852006673812866\n",
      "Epoch 1229, Loss: 0.44522063434123993, Final Batch Loss: 0.22130271792411804\n",
      "Epoch 1230, Loss: 0.5168527364730835, Final Batch Loss: 0.25418391823768616\n",
      "Epoch 1231, Loss: 0.5415785908699036, Final Batch Loss: 0.30248209834098816\n",
      "Epoch 1232, Loss: 0.460333913564682, Final Batch Loss: 0.2308090329170227\n",
      "Epoch 1233, Loss: 0.5818636268377304, Final Batch Loss: 0.33496224880218506\n",
      "Epoch 1234, Loss: 0.5002264082431793, Final Batch Loss: 0.27761223912239075\n",
      "Epoch 1235, Loss: 0.5078788101673126, Final Batch Loss: 0.2670011818408966\n",
      "Epoch 1236, Loss: 0.5719402432441711, Final Batch Loss: 0.30798861384391785\n",
      "Epoch 1237, Loss: 0.5680227279663086, Final Batch Loss: 0.3228250741958618\n",
      "Epoch 1238, Loss: 0.46491843461990356, Final Batch Loss: 0.2315780073404312\n",
      "Epoch 1239, Loss: 0.504795029759407, Final Batch Loss: 0.32258936762809753\n",
      "Epoch 1240, Loss: 0.46629248559474945, Final Batch Loss: 0.20149777829647064\n",
      "Epoch 1241, Loss: 0.4384152293205261, Final Batch Loss: 0.22946558892726898\n",
      "Epoch 1242, Loss: 0.505780354142189, Final Batch Loss: 0.2653389871120453\n",
      "Epoch 1243, Loss: 0.5376464128494263, Final Batch Loss: 0.27404552698135376\n",
      "Epoch 1244, Loss: 0.5292610973119736, Final Batch Loss: 0.28418096899986267\n",
      "Epoch 1245, Loss: 0.48013199865818024, Final Batch Loss: 0.2516684830188751\n",
      "Epoch 1246, Loss: 0.46158498525619507, Final Batch Loss: 0.21256083250045776\n",
      "Epoch 1247, Loss: 0.5021293312311172, Final Batch Loss: 0.27796435356140137\n",
      "Epoch 1248, Loss: 0.5170358568429947, Final Batch Loss: 0.2794032394886017\n",
      "Epoch 1249, Loss: 0.468386247754097, Final Batch Loss: 0.22551652789115906\n",
      "Epoch 1250, Loss: 0.49446214735507965, Final Batch Loss: 0.21334557235240936\n",
      "Epoch 1251, Loss: 0.5143725872039795, Final Batch Loss: 0.26642176508903503\n",
      "Epoch 1252, Loss: 0.4637177288532257, Final Batch Loss: 0.1987656056880951\n",
      "Epoch 1253, Loss: 0.4909403622150421, Final Batch Loss: 0.21994668245315552\n",
      "Epoch 1254, Loss: 0.46972811222076416, Final Batch Loss: 0.218440979719162\n",
      "Epoch 1255, Loss: 0.49662862718105316, Final Batch Loss: 0.20745928585529327\n",
      "Epoch 1256, Loss: 0.48104508221149445, Final Batch Loss: 0.23100723326206207\n",
      "Epoch 1257, Loss: 0.5475240349769592, Final Batch Loss: 0.2855641543865204\n",
      "Epoch 1258, Loss: 0.4933538883924484, Final Batch Loss: 0.23753537237644196\n",
      "Epoch 1259, Loss: 0.5133909732103348, Final Batch Loss: 0.26868972182273865\n",
      "Epoch 1260, Loss: 0.4607975035905838, Final Batch Loss: 0.24557584524154663\n",
      "Epoch 1261, Loss: 0.48440250754356384, Final Batch Loss: 0.2668699324131012\n",
      "Epoch 1262, Loss: 0.4727511554956436, Final Batch Loss: 0.2324676662683487\n",
      "Epoch 1263, Loss: 0.5192804932594299, Final Batch Loss: 0.29852741956710815\n",
      "Epoch 1264, Loss: 0.46455930173397064, Final Batch Loss: 0.21053571999073029\n",
      "Epoch 1265, Loss: 0.46717076003551483, Final Batch Loss: 0.24436607956886292\n",
      "Epoch 1266, Loss: 0.5121202915906906, Final Batch Loss: 0.2458028346300125\n",
      "Epoch 1267, Loss: 0.4948452115058899, Final Batch Loss: 0.2519320845603943\n",
      "Epoch 1268, Loss: 0.4529688060283661, Final Batch Loss: 0.2155112475156784\n",
      "Epoch 1269, Loss: 0.5263492465019226, Final Batch Loss: 0.29407981038093567\n",
      "Epoch 1270, Loss: 0.5166857689619064, Final Batch Loss: 0.28514647483825684\n",
      "Epoch 1271, Loss: 0.49455927312374115, Final Batch Loss: 0.2241450995206833\n",
      "Epoch 1272, Loss: 0.4690004140138626, Final Batch Loss: 0.1709870845079422\n",
      "Epoch 1273, Loss: 0.4719365984201431, Final Batch Loss: 0.20844776928424835\n",
      "Epoch 1274, Loss: 0.4874340742826462, Final Batch Loss: 0.2824302017688751\n",
      "Epoch 1275, Loss: 0.4523191601037979, Final Batch Loss: 0.24945597350597382\n",
      "Epoch 1276, Loss: 0.4766387790441513, Final Batch Loss: 0.28062954545021057\n",
      "Epoch 1277, Loss: 0.5164285153150558, Final Batch Loss: 0.2717227637767792\n",
      "Epoch 1278, Loss: 0.5188303738832474, Final Batch Loss: 0.2929008901119232\n",
      "Epoch 1279, Loss: 0.5030651390552521, Final Batch Loss: 0.250332772731781\n",
      "Epoch 1280, Loss: 0.4502934068441391, Final Batch Loss: 0.20487625896930695\n",
      "Epoch 1281, Loss: 0.46298088133335114, Final Batch Loss: 0.2225920408964157\n",
      "Epoch 1282, Loss: 0.5027667135000229, Final Batch Loss: 0.2578947842121124\n",
      "Epoch 1283, Loss: 0.488280326128006, Final Batch Loss: 0.2600615620613098\n",
      "Epoch 1284, Loss: 0.5302739441394806, Final Batch Loss: 0.262370228767395\n",
      "Epoch 1285, Loss: 0.5021739602088928, Final Batch Loss: 0.252529114484787\n",
      "Epoch 1286, Loss: 0.5231370031833649, Final Batch Loss: 0.2654331922531128\n",
      "Epoch 1287, Loss: 0.4555744528770447, Final Batch Loss: 0.23632463812828064\n",
      "Epoch 1288, Loss: 0.4991505891084671, Final Batch Loss: 0.2551267445087433\n",
      "Epoch 1289, Loss: 0.48597855865955353, Final Batch Loss: 0.2823982536792755\n",
      "Epoch 1290, Loss: 0.5202487856149673, Final Batch Loss: 0.29606592655181885\n",
      "Epoch 1291, Loss: 0.4927285313606262, Final Batch Loss: 0.2864275276660919\n",
      "Epoch 1292, Loss: 0.4716889560222626, Final Batch Loss: 0.2446320801973343\n",
      "Epoch 1293, Loss: 0.5685629546642303, Final Batch Loss: 0.28197652101516724\n",
      "Epoch 1294, Loss: 0.4085662364959717, Final Batch Loss: 0.18355205655097961\n",
      "Epoch 1295, Loss: 0.4522100239992142, Final Batch Loss: 0.21042059361934662\n",
      "Epoch 1296, Loss: 0.5233018696308136, Final Batch Loss: 0.2612627446651459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1297, Loss: 0.49968716502189636, Final Batch Loss: 0.27598920464515686\n",
      "Epoch 1298, Loss: 0.5253013670444489, Final Batch Loss: 0.2605592906475067\n",
      "Epoch 1299, Loss: 0.49424146115779877, Final Batch Loss: 0.21059627830982208\n",
      "Epoch 1300, Loss: 0.4880242943763733, Final Batch Loss: 0.22305935621261597\n",
      "Epoch 1301, Loss: 0.48998694121837616, Final Batch Loss: 0.2559671401977539\n",
      "Epoch 1302, Loss: 0.48235073685646057, Final Batch Loss: 0.24551060795783997\n",
      "Epoch 1303, Loss: 0.5472047626972198, Final Batch Loss: 0.267336368560791\n",
      "Epoch 1304, Loss: 0.4405926316976547, Final Batch Loss: 0.21683940291404724\n",
      "Epoch 1305, Loss: 0.47190360724925995, Final Batch Loss: 0.26584428548812866\n",
      "Epoch 1306, Loss: 0.4623256176710129, Final Batch Loss: 0.23516948521137238\n",
      "Epoch 1307, Loss: 0.5178388208150864, Final Batch Loss: 0.2802312672138214\n",
      "Epoch 1308, Loss: 0.4828011095523834, Final Batch Loss: 0.2357773333787918\n",
      "Epoch 1309, Loss: 0.4623330235481262, Final Batch Loss: 0.20326921343803406\n",
      "Epoch 1310, Loss: 0.510548546910286, Final Batch Loss: 0.26816245913505554\n",
      "Epoch 1311, Loss: 0.4527737498283386, Final Batch Loss: 0.20927125215530396\n",
      "Epoch 1312, Loss: 0.43165701627731323, Final Batch Loss: 0.2070806920528412\n",
      "Epoch 1313, Loss: 0.4772595316171646, Final Batch Loss: 0.2509077191352844\n",
      "Epoch 1314, Loss: 0.4569980502128601, Final Batch Loss: 0.22393356263637543\n",
      "Epoch 1315, Loss: 0.5561387538909912, Final Batch Loss: 0.2927305996417999\n",
      "Epoch 1316, Loss: 0.5083645731210709, Final Batch Loss: 0.2348591834306717\n",
      "Epoch 1317, Loss: 0.4529711902141571, Final Batch Loss: 0.23757074773311615\n",
      "Epoch 1318, Loss: 0.5257366299629211, Final Batch Loss: 0.27143678069114685\n",
      "Epoch 1319, Loss: 0.463446706533432, Final Batch Loss: 0.21011635661125183\n",
      "Epoch 1320, Loss: 0.49599897861480713, Final Batch Loss: 0.24871604144573212\n",
      "Epoch 1321, Loss: 0.493355855345726, Final Batch Loss: 0.2282809466123581\n",
      "Epoch 1322, Loss: 0.4682566821575165, Final Batch Loss: 0.20942717790603638\n",
      "Epoch 1323, Loss: 0.5222072005271912, Final Batch Loss: 0.2925029695034027\n",
      "Epoch 1324, Loss: 0.5200233161449432, Final Batch Loss: 0.3192480206489563\n",
      "Epoch 1325, Loss: 0.46263743937015533, Final Batch Loss: 0.23752516508102417\n",
      "Epoch 1326, Loss: 0.4630231410264969, Final Batch Loss: 0.23621471226215363\n",
      "Epoch 1327, Loss: 0.5332387238740921, Final Batch Loss: 0.3050130009651184\n",
      "Epoch 1328, Loss: 0.4925898015499115, Final Batch Loss: 0.2243538200855255\n",
      "Epoch 1329, Loss: 0.5062709897756577, Final Batch Loss: 0.24047033488750458\n",
      "Epoch 1330, Loss: 0.5171941667795181, Final Batch Loss: 0.30056434869766235\n",
      "Epoch 1331, Loss: 0.47779861092567444, Final Batch Loss: 0.2125643789768219\n",
      "Epoch 1332, Loss: 0.5461602509021759, Final Batch Loss: 0.3021281063556671\n",
      "Epoch 1333, Loss: 0.45677196979522705, Final Batch Loss: 0.21703992784023285\n",
      "Epoch 1334, Loss: 0.4408871680498123, Final Batch Loss: 0.2043982297182083\n",
      "Epoch 1335, Loss: 0.42307859659194946, Final Batch Loss: 0.21042443811893463\n",
      "Epoch 1336, Loss: 0.43246130645275116, Final Batch Loss: 0.24890819191932678\n",
      "Epoch 1337, Loss: 0.5244702398777008, Final Batch Loss: 0.2546771764755249\n",
      "Epoch 1338, Loss: 0.5407088398933411, Final Batch Loss: 0.27058106660842896\n",
      "Epoch 1339, Loss: 0.4973171502351761, Final Batch Loss: 0.25295934081077576\n",
      "Epoch 1340, Loss: 0.4461377412080765, Final Batch Loss: 0.20316755771636963\n",
      "Epoch 1341, Loss: 0.4970291405916214, Final Batch Loss: 0.2980317771434784\n",
      "Epoch 1342, Loss: 0.4517436921596527, Final Batch Loss: 0.21908897161483765\n",
      "Epoch 1343, Loss: 0.4884152114391327, Final Batch Loss: 0.24448250234127045\n",
      "Epoch 1344, Loss: 0.5805292427539825, Final Batch Loss: 0.3271788954734802\n",
      "Epoch 1345, Loss: 0.4659043103456497, Final Batch Loss: 0.23964764177799225\n",
      "Epoch 1346, Loss: 0.44165702164173126, Final Batch Loss: 0.2493477463722229\n",
      "Epoch 1347, Loss: 0.49950580298900604, Final Batch Loss: 0.26799046993255615\n",
      "Epoch 1348, Loss: 0.45823177695274353, Final Batch Loss: 0.22540831565856934\n",
      "Epoch 1349, Loss: 0.47302304208278656, Final Batch Loss: 0.2280970960855484\n",
      "Epoch 1350, Loss: 0.5098036974668503, Final Batch Loss: 0.2859453558921814\n",
      "Epoch 1351, Loss: 0.44679711759090424, Final Batch Loss: 0.22841684520244598\n",
      "Epoch 1352, Loss: 0.46842487156391144, Final Batch Loss: 0.26089349389076233\n",
      "Epoch 1353, Loss: 0.4735022187232971, Final Batch Loss: 0.25725945830345154\n",
      "Epoch 1354, Loss: 0.48042628169059753, Final Batch Loss: 0.2094263732433319\n",
      "Epoch 1355, Loss: 0.4959215074777603, Final Batch Loss: 0.22890664637088776\n",
      "Epoch 1356, Loss: 0.49741412699222565, Final Batch Loss: 0.26643967628479004\n",
      "Epoch 1357, Loss: 0.4247623085975647, Final Batch Loss: 0.16867297887802124\n",
      "Epoch 1358, Loss: 0.47554317116737366, Final Batch Loss: 0.21795278787612915\n",
      "Epoch 1359, Loss: 0.44648778438568115, Final Batch Loss: 0.19231265783309937\n",
      "Epoch 1360, Loss: 0.5025437623262405, Final Batch Loss: 0.2771431803703308\n",
      "Epoch 1361, Loss: 0.456057533621788, Final Batch Loss: 0.2074723243713379\n",
      "Epoch 1362, Loss: 0.47529271245002747, Final Batch Loss: 0.2382996827363968\n",
      "Epoch 1363, Loss: 0.4985198676586151, Final Batch Loss: 0.27567151188850403\n",
      "Epoch 1364, Loss: 0.45883598923683167, Final Batch Loss: 0.22837483882904053\n",
      "Epoch 1365, Loss: 0.5027626305818558, Final Batch Loss: 0.28624576330184937\n",
      "Epoch 1366, Loss: 0.4515061527490616, Final Batch Loss: 0.2348737269639969\n",
      "Epoch 1367, Loss: 0.5079370439052582, Final Batch Loss: 0.316809743642807\n",
      "Epoch 1368, Loss: 0.46418696641921997, Final Batch Loss: 0.2191479653120041\n",
      "Epoch 1369, Loss: 0.4317686855792999, Final Batch Loss: 0.19661416113376617\n",
      "Epoch 1370, Loss: 0.49359090626239777, Final Batch Loss: 0.24012993276119232\n",
      "Epoch 1371, Loss: 0.44725795090198517, Final Batch Loss: 0.19387175142765045\n",
      "Epoch 1372, Loss: 0.49358806014060974, Final Batch Loss: 0.2534279227256775\n",
      "Epoch 1373, Loss: 0.45686081051826477, Final Batch Loss: 0.22470292448997498\n",
      "Epoch 1374, Loss: 0.4352652132511139, Final Batch Loss: 0.18975980579853058\n",
      "Epoch 1375, Loss: 0.44234099984169006, Final Batch Loss: 0.2057790607213974\n",
      "Epoch 1376, Loss: 0.5247787237167358, Final Batch Loss: 0.25335049629211426\n",
      "Epoch 1377, Loss: 0.4375304877758026, Final Batch Loss: 0.21660076081752777\n",
      "Epoch 1378, Loss: 0.5118078887462616, Final Batch Loss: 0.2455592155456543\n",
      "Epoch 1379, Loss: 0.4460041671991348, Final Batch Loss: 0.22056742012500763\n",
      "Epoch 1380, Loss: 0.5188694000244141, Final Batch Loss: 0.2537144720554352\n",
      "Epoch 1381, Loss: 0.47596587240695953, Final Batch Loss: 0.22656071186065674\n",
      "Epoch 1382, Loss: 0.4749543368816376, Final Batch Loss: 0.25928995013237\n",
      "Epoch 1383, Loss: 0.4810209572315216, Final Batch Loss: 0.2701091170310974\n",
      "Epoch 1384, Loss: 0.5602713525295258, Final Batch Loss: 0.31809934973716736\n",
      "Epoch 1385, Loss: 0.4838172644376755, Final Batch Loss: 0.2758994400501251\n",
      "Epoch 1386, Loss: 0.4062761217355728, Final Batch Loss: 0.21634449064731598\n",
      "Epoch 1387, Loss: 0.4724482595920563, Final Batch Loss: 0.23671641945838928\n",
      "Epoch 1388, Loss: 0.459026500582695, Final Batch Loss: 0.2424783855676651\n",
      "Epoch 1389, Loss: 0.43059560656547546, Final Batch Loss: 0.20113863050937653\n",
      "Epoch 1390, Loss: 0.5213164687156677, Final Batch Loss: 0.2292759120464325\n",
      "Epoch 1391, Loss: 0.42282402515411377, Final Batch Loss: 0.2241428792476654\n",
      "Epoch 1392, Loss: 0.4601200819015503, Final Batch Loss: 0.19247078895568848\n",
      "Epoch 1393, Loss: 0.5297073125839233, Final Batch Loss: 0.25821664929389954\n",
      "Epoch 1394, Loss: 0.5895353853702545, Final Batch Loss: 0.30259138345718384\n",
      "Epoch 1395, Loss: 0.4800938665866852, Final Batch Loss: 0.21616676449775696\n",
      "Epoch 1396, Loss: 0.4945264160633087, Final Batch Loss: 0.25518760085105896\n",
      "Epoch 1397, Loss: 0.4714324325323105, Final Batch Loss: 0.2438109666109085\n",
      "Epoch 1398, Loss: 0.4228050112724304, Final Batch Loss: 0.18892167508602142\n",
      "Epoch 1399, Loss: 0.46881359815597534, Final Batch Loss: 0.22088591754436493\n",
      "Epoch 1400, Loss: 0.4875057637691498, Final Batch Loss: 0.28634122014045715\n",
      "Epoch 1401, Loss: 0.439221128821373, Final Batch Loss: 0.23426534235477448\n",
      "Epoch 1402, Loss: 0.5009424537420273, Final Batch Loss: 0.29700735211372375\n",
      "Epoch 1403, Loss: 0.47733333706855774, Final Batch Loss: 0.2786630094051361\n",
      "Epoch 1404, Loss: 0.49279777705669403, Final Batch Loss: 0.2265395075082779\n",
      "Epoch 1405, Loss: 0.4611317068338394, Final Batch Loss: 0.2376171052455902\n",
      "Epoch 1406, Loss: 0.4169667661190033, Final Batch Loss: 0.19984190165996552\n",
      "Epoch 1407, Loss: 0.4648642838001251, Final Batch Loss: 0.22524181008338928\n",
      "Epoch 1408, Loss: 0.44495926797389984, Final Batch Loss: 0.2071128785610199\n",
      "Epoch 1409, Loss: 0.49902595579624176, Final Batch Loss: 0.25639885663986206\n",
      "Epoch 1410, Loss: 0.4530312120914459, Final Batch Loss: 0.2221926897764206\n",
      "Epoch 1411, Loss: 0.5124881863594055, Final Batch Loss: 0.2632600963115692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1412, Loss: 0.4308379143476486, Final Batch Loss: 0.16346873342990875\n",
      "Epoch 1413, Loss: 0.4986564815044403, Final Batch Loss: 0.26394402980804443\n",
      "Epoch 1414, Loss: 0.4180147498846054, Final Batch Loss: 0.21468812227249146\n",
      "Epoch 1415, Loss: 0.4463867247104645, Final Batch Loss: 0.20673559606075287\n",
      "Epoch 1416, Loss: 0.4911910891532898, Final Batch Loss: 0.2632984519004822\n",
      "Epoch 1417, Loss: 0.4279462695121765, Final Batch Loss: 0.20832210779190063\n",
      "Epoch 1418, Loss: 0.4788324236869812, Final Batch Loss: 0.23743753135204315\n",
      "Epoch 1419, Loss: 0.40682652592658997, Final Batch Loss: 0.1965099424123764\n",
      "Epoch 1420, Loss: 0.4036901593208313, Final Batch Loss: 0.21046429872512817\n",
      "Epoch 1421, Loss: 0.5844200998544693, Final Batch Loss: 0.33787980675697327\n",
      "Epoch 1422, Loss: 0.46525807678699493, Final Batch Loss: 0.2678421437740326\n",
      "Epoch 1423, Loss: 0.4793611168861389, Final Batch Loss: 0.19107088446617126\n",
      "Epoch 1424, Loss: 0.5032191872596741, Final Batch Loss: 0.2665095031261444\n",
      "Epoch 1425, Loss: 0.4280298352241516, Final Batch Loss: 0.1865570843219757\n",
      "Epoch 1426, Loss: 0.3987211734056473, Final Batch Loss: 0.17803731560707092\n",
      "Epoch 1427, Loss: 0.4325020760297775, Final Batch Loss: 0.22032232582569122\n",
      "Epoch 1428, Loss: 0.3675738275051117, Final Batch Loss: 0.1387973576784134\n",
      "Epoch 1429, Loss: 0.43392205238342285, Final Batch Loss: 0.20771707594394684\n",
      "Epoch 1430, Loss: 0.48687535524368286, Final Batch Loss: 0.24691098928451538\n",
      "Epoch 1431, Loss: 0.40442974865436554, Final Batch Loss: 0.2123277485370636\n",
      "Epoch 1432, Loss: 0.47611966729164124, Final Batch Loss: 0.27281659841537476\n",
      "Epoch 1433, Loss: 0.38129638135433197, Final Batch Loss: 0.16731560230255127\n",
      "Epoch 1434, Loss: 0.5142749845981598, Final Batch Loss: 0.2650105059146881\n",
      "Epoch 1435, Loss: 0.43301133811473846, Final Batch Loss: 0.18907523155212402\n",
      "Epoch 1436, Loss: 0.5070382058620453, Final Batch Loss: 0.283788800239563\n",
      "Epoch 1437, Loss: 0.4161766320466995, Final Batch Loss: 0.2040558159351349\n",
      "Epoch 1438, Loss: 0.521876186132431, Final Batch Loss: 0.31360694766044617\n",
      "Epoch 1439, Loss: 0.5000419318675995, Final Batch Loss: 0.26304271817207336\n",
      "Epoch 1440, Loss: 0.46251706779003143, Final Batch Loss: 0.2442496120929718\n",
      "Epoch 1441, Loss: 0.431864470243454, Final Batch Loss: 0.17967495322227478\n",
      "Epoch 1442, Loss: 0.40617814660072327, Final Batch Loss: 0.17974376678466797\n",
      "Epoch 1443, Loss: 0.4806455373764038, Final Batch Loss: 0.25279897451400757\n",
      "Epoch 1444, Loss: 0.4892708361148834, Final Batch Loss: 0.2697312533855438\n",
      "Epoch 1445, Loss: 0.45330704748630524, Final Batch Loss: 0.24075353145599365\n",
      "Epoch 1446, Loss: 0.4954931139945984, Final Batch Loss: 0.2029702067375183\n",
      "Epoch 1447, Loss: 0.4551614820957184, Final Batch Loss: 0.2388041466474533\n",
      "Epoch 1448, Loss: 0.5548251271247864, Final Batch Loss: 0.30616605281829834\n",
      "Epoch 1449, Loss: 0.49171827733516693, Final Batch Loss: 0.23118551075458527\n",
      "Epoch 1450, Loss: 0.49629801511764526, Final Batch Loss: 0.2188040018081665\n",
      "Epoch 1451, Loss: 0.4682701677083969, Final Batch Loss: 0.24535584449768066\n",
      "Epoch 1452, Loss: 0.47557568550109863, Final Batch Loss: 0.22510188817977905\n",
      "Epoch 1453, Loss: 0.4626877009868622, Final Batch Loss: 0.19259324669837952\n",
      "Epoch 1454, Loss: 0.4900263696908951, Final Batch Loss: 0.2743019461631775\n",
      "Epoch 1455, Loss: 0.4629780352115631, Final Batch Loss: 0.24549807608127594\n",
      "Epoch 1456, Loss: 0.49592502415180206, Final Batch Loss: 0.26300573348999023\n",
      "Epoch 1457, Loss: 0.45374880731105804, Final Batch Loss: 0.20023615658283234\n",
      "Epoch 1458, Loss: 0.511930987238884, Final Batch Loss: 0.2726057767868042\n",
      "Epoch 1459, Loss: 0.43693478405475616, Final Batch Loss: 0.17734016478061676\n",
      "Epoch 1460, Loss: 0.5086811780929565, Final Batch Loss: 0.2397332787513733\n",
      "Epoch 1461, Loss: 0.4861668050289154, Final Batch Loss: 0.23958376049995422\n",
      "Epoch 1462, Loss: 0.43724989891052246, Final Batch Loss: 0.22430770099163055\n",
      "Epoch 1463, Loss: 0.4564353823661804, Final Batch Loss: 0.2620547413825989\n",
      "Epoch 1464, Loss: 0.5356431007385254, Final Batch Loss: 0.3138514757156372\n",
      "Epoch 1465, Loss: 0.4771687239408493, Final Batch Loss: 0.25542470812797546\n",
      "Epoch 1466, Loss: 0.4387142211198807, Final Batch Loss: 0.22095295786857605\n",
      "Epoch 1467, Loss: 0.44914311170578003, Final Batch Loss: 0.21898235380649567\n",
      "Epoch 1468, Loss: 0.38293977081775665, Final Batch Loss: 0.18151824176311493\n",
      "Epoch 1469, Loss: 0.46182237565517426, Final Batch Loss: 0.24232450127601624\n",
      "Epoch 1470, Loss: 0.4711326062679291, Final Batch Loss: 0.2231501340866089\n",
      "Epoch 1471, Loss: 0.4488179683685303, Final Batch Loss: 0.2144278585910797\n",
      "Epoch 1472, Loss: 0.4580393582582474, Final Batch Loss: 0.24877509474754333\n",
      "Epoch 1473, Loss: 0.38652952015399933, Final Batch Loss: 0.1712411344051361\n",
      "Epoch 1474, Loss: 0.4659973978996277, Final Batch Loss: 0.25074127316474915\n",
      "Epoch 1475, Loss: 0.4816615581512451, Final Batch Loss: 0.24433350563049316\n",
      "Epoch 1476, Loss: 0.4644584506750107, Final Batch Loss: 0.25746792554855347\n",
      "Epoch 1477, Loss: 0.4339769184589386, Final Batch Loss: 0.172959566116333\n",
      "Epoch 1478, Loss: 0.4655046761035919, Final Batch Loss: 0.24128419160842896\n",
      "Epoch 1479, Loss: 0.463651642203331, Final Batch Loss: 0.24185322225093842\n",
      "Epoch 1480, Loss: 0.48605868220329285, Final Batch Loss: 0.20159482955932617\n",
      "Epoch 1481, Loss: 0.4701516628265381, Final Batch Loss: 0.19701409339904785\n",
      "Epoch 1482, Loss: 0.4407138377428055, Final Batch Loss: 0.22562092542648315\n",
      "Epoch 1483, Loss: 0.4667835980653763, Final Batch Loss: 0.24318107962608337\n",
      "Epoch 1484, Loss: 0.5098850429058075, Final Batch Loss: 0.23605674505233765\n",
      "Epoch 1485, Loss: 0.45328953862190247, Final Batch Loss: 0.22101016342639923\n",
      "Epoch 1486, Loss: 0.4803360402584076, Final Batch Loss: 0.2537449598312378\n",
      "Epoch 1487, Loss: 0.3936276286840439, Final Batch Loss: 0.18044938147068024\n",
      "Epoch 1488, Loss: 0.5404093861579895, Final Batch Loss: 0.285597562789917\n",
      "Epoch 1489, Loss: 0.49323906004428864, Final Batch Loss: 0.2642118036746979\n",
      "Epoch 1490, Loss: 0.4603882133960724, Final Batch Loss: 0.21969063580036163\n",
      "Epoch 1491, Loss: 0.4672507643699646, Final Batch Loss: 0.258594810962677\n",
      "Epoch 1492, Loss: 0.43191617727279663, Final Batch Loss: 0.23540730774402618\n",
      "Epoch 1493, Loss: 0.5055178850889206, Final Batch Loss: 0.2399497777223587\n",
      "Epoch 1494, Loss: 0.49460482597351074, Final Batch Loss: 0.2667016386985779\n",
      "Epoch 1495, Loss: 0.4548022300004959, Final Batch Loss: 0.177980437874794\n",
      "Epoch 1496, Loss: 0.41169998049736023, Final Batch Loss: 0.19917047023773193\n",
      "Epoch 1497, Loss: 0.4539279490709305, Final Batch Loss: 0.20959684252738953\n",
      "Epoch 1498, Loss: 0.4676974266767502, Final Batch Loss: 0.2623923122882843\n",
      "Epoch 1499, Loss: 0.4883163422346115, Final Batch Loss: 0.23378483951091766\n",
      "Epoch 1500, Loss: 0.43594925105571747, Final Batch Loss: 0.19579236209392548\n",
      "Epoch 1501, Loss: 0.4253489524126053, Final Batch Loss: 0.20972616970539093\n",
      "Epoch 1502, Loss: 0.46681222319602966, Final Batch Loss: 0.24344602227210999\n",
      "Epoch 1503, Loss: 0.4389677345752716, Final Batch Loss: 0.22080793976783752\n",
      "Epoch 1504, Loss: 0.4859650731086731, Final Batch Loss: 0.2715761363506317\n",
      "Epoch 1505, Loss: 0.4473327547311783, Final Batch Loss: 0.2216857224702835\n",
      "Epoch 1506, Loss: 0.4600897580385208, Final Batch Loss: 0.22355814278125763\n",
      "Epoch 1507, Loss: 0.44806306064128876, Final Batch Loss: 0.23262114822864532\n",
      "Epoch 1508, Loss: 0.4848712533712387, Final Batch Loss: 0.24162574112415314\n",
      "Epoch 1509, Loss: 0.45164424180984497, Final Batch Loss: 0.2471395879983902\n",
      "Epoch 1510, Loss: 0.40538352727890015, Final Batch Loss: 0.1448270082473755\n",
      "Epoch 1511, Loss: 0.45556920766830444, Final Batch Loss: 0.22824737429618835\n",
      "Epoch 1512, Loss: 0.41510850191116333, Final Batch Loss: 0.22900931537151337\n",
      "Epoch 1513, Loss: 0.457533061504364, Final Batch Loss: 0.20867857336997986\n",
      "Epoch 1514, Loss: 0.4778771847486496, Final Batch Loss: 0.22807539999485016\n",
      "Epoch 1515, Loss: 0.44580966234207153, Final Batch Loss: 0.20685893297195435\n",
      "Epoch 1516, Loss: 0.4637731909751892, Final Batch Loss: 0.2327653020620346\n",
      "Epoch 1517, Loss: 0.39104756712913513, Final Batch Loss: 0.19480222463607788\n",
      "Epoch 1518, Loss: 0.4243423193693161, Final Batch Loss: 0.21114489436149597\n",
      "Epoch 1519, Loss: 0.4625188559293747, Final Batch Loss: 0.23526638746261597\n",
      "Epoch 1520, Loss: 0.4037729650735855, Final Batch Loss: 0.23958642780780792\n",
      "Epoch 1521, Loss: 0.47212304174900055, Final Batch Loss: 0.2839144468307495\n",
      "Epoch 1522, Loss: 0.44936221837997437, Final Batch Loss: 0.24000875651836395\n",
      "Epoch 1523, Loss: 0.533251017332077, Final Batch Loss: 0.3031703233718872\n",
      "Epoch 1524, Loss: 0.4700433909893036, Final Batch Loss: 0.2677101790904999\n",
      "Epoch 1525, Loss: 0.42377057671546936, Final Batch Loss: 0.20890267193317413\n",
      "Epoch 1526, Loss: 0.45114482939243317, Final Batch Loss: 0.22809149324893951\n",
      "Epoch 1527, Loss: 0.5140541642904282, Final Batch Loss: 0.2820412516593933\n",
      "Epoch 1528, Loss: 0.4434647858142853, Final Batch Loss: 0.24144625663757324\n",
      "Epoch 1529, Loss: 0.4605424106121063, Final Batch Loss: 0.18655991554260254\n",
      "Epoch 1530, Loss: 0.4943588078022003, Final Batch Loss: 0.2061302661895752\n",
      "Epoch 1531, Loss: 0.47895246744155884, Final Batch Loss: 0.27592113614082336\n",
      "Epoch 1532, Loss: 0.4505920857191086, Final Batch Loss: 0.22467556595802307\n",
      "Epoch 1533, Loss: 0.47321705520153046, Final Batch Loss: 0.2906367778778076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1534, Loss: 0.4360433369874954, Final Batch Loss: 0.2157125324010849\n",
      "Epoch 1535, Loss: 0.443850114941597, Final Batch Loss: 0.23746192455291748\n",
      "Epoch 1536, Loss: 0.4514755308628082, Final Batch Loss: 0.22468720376491547\n",
      "Epoch 1537, Loss: 0.43691286444664, Final Batch Loss: 0.25379422307014465\n",
      "Epoch 1538, Loss: 0.4274645894765854, Final Batch Loss: 0.23783262073993683\n",
      "Epoch 1539, Loss: 0.4619489014148712, Final Batch Loss: 0.26629066467285156\n",
      "Epoch 1540, Loss: 0.39569929242134094, Final Batch Loss: 0.17165739834308624\n",
      "Epoch 1541, Loss: 0.4056809991598129, Final Batch Loss: 0.17865265905857086\n",
      "Epoch 1542, Loss: 0.5100191533565521, Final Batch Loss: 0.30834367871284485\n",
      "Epoch 1543, Loss: 0.47162915766239166, Final Batch Loss: 0.24211424589157104\n",
      "Epoch 1544, Loss: 0.40115225315093994, Final Batch Loss: 0.22551949322223663\n",
      "Epoch 1545, Loss: 0.45495542883872986, Final Batch Loss: 0.2318052500486374\n",
      "Epoch 1546, Loss: 0.45573604106903076, Final Batch Loss: 0.2241569310426712\n",
      "Epoch 1547, Loss: 0.4586692601442337, Final Batch Loss: 0.20657728612422943\n",
      "Epoch 1548, Loss: 0.3903403878211975, Final Batch Loss: 0.18557192385196686\n",
      "Epoch 1549, Loss: 0.414443776011467, Final Batch Loss: 0.1591162532567978\n",
      "Epoch 1550, Loss: 0.4283403754234314, Final Batch Loss: 0.19958002865314484\n",
      "Epoch 1551, Loss: 0.4002537429332733, Final Batch Loss: 0.19767190515995026\n",
      "Epoch 1552, Loss: 0.540696382522583, Final Batch Loss: 0.3290834426879883\n",
      "Epoch 1553, Loss: 0.5110096037387848, Final Batch Loss: 0.2893533706665039\n",
      "Epoch 1554, Loss: 0.4283447265625, Final Batch Loss: 0.18758520483970642\n",
      "Epoch 1555, Loss: 0.44255979359149933, Final Batch Loss: 0.21029667556285858\n",
      "Epoch 1556, Loss: 0.4326707124710083, Final Batch Loss: 0.23681053519248962\n",
      "Epoch 1557, Loss: 0.4875109791755676, Final Batch Loss: 0.2650196850299835\n",
      "Epoch 1558, Loss: 0.4236108213663101, Final Batch Loss: 0.18613776564598083\n",
      "Epoch 1559, Loss: 0.4357944577932358, Final Batch Loss: 0.22448626160621643\n",
      "Epoch 1560, Loss: 0.4155777394771576, Final Batch Loss: 0.19292329251766205\n",
      "Epoch 1561, Loss: 0.39240318536758423, Final Batch Loss: 0.20479154586791992\n",
      "Epoch 1562, Loss: 0.4109928160905838, Final Batch Loss: 0.19180932641029358\n",
      "Epoch 1563, Loss: 0.40367157757282257, Final Batch Loss: 0.18767215311527252\n",
      "Epoch 1564, Loss: 0.4094645529985428, Final Batch Loss: 0.16828787326812744\n",
      "Epoch 1565, Loss: 0.5219378918409348, Final Batch Loss: 0.3151158392429352\n",
      "Epoch 1566, Loss: 0.4400942921638489, Final Batch Loss: 0.21509523689746857\n",
      "Epoch 1567, Loss: 0.42031359672546387, Final Batch Loss: 0.21487130224704742\n",
      "Epoch 1568, Loss: 0.44443556666374207, Final Batch Loss: 0.22508779168128967\n",
      "Epoch 1569, Loss: 0.43488219380378723, Final Batch Loss: 0.20011019706726074\n",
      "Epoch 1570, Loss: 0.41869576275348663, Final Batch Loss: 0.21284769475460052\n",
      "Epoch 1571, Loss: 0.4393606632947922, Final Batch Loss: 0.20490382611751556\n",
      "Epoch 1572, Loss: 0.5271217226982117, Final Batch Loss: 0.27687421441078186\n",
      "Epoch 1573, Loss: 0.4217975437641144, Final Batch Loss: 0.20615550875663757\n",
      "Epoch 1574, Loss: 0.47933684289455414, Final Batch Loss: 0.2760157883167267\n",
      "Epoch 1575, Loss: 0.4135523736476898, Final Batch Loss: 0.17802372574806213\n",
      "Epoch 1576, Loss: 0.4985448569059372, Final Batch Loss: 0.314807653427124\n",
      "Epoch 1577, Loss: 0.4168017953634262, Final Batch Loss: 0.185589998960495\n",
      "Epoch 1578, Loss: 0.485054150223732, Final Batch Loss: 0.2505068778991699\n",
      "Epoch 1579, Loss: 0.43861496448516846, Final Batch Loss: 0.20199288427829742\n",
      "Epoch 1580, Loss: 0.4546411633491516, Final Batch Loss: 0.23794132471084595\n",
      "Epoch 1581, Loss: 0.4219880849123001, Final Batch Loss: 0.2397308349609375\n",
      "Epoch 1582, Loss: 0.4630402773618698, Final Batch Loss: 0.2451179325580597\n",
      "Epoch 1583, Loss: 0.4400535225868225, Final Batch Loss: 0.17584744095802307\n",
      "Epoch 1584, Loss: 0.44552330672740936, Final Batch Loss: 0.24352969229221344\n",
      "Epoch 1585, Loss: 0.4432818591594696, Final Batch Loss: 0.2474086731672287\n",
      "Epoch 1586, Loss: 0.5430711805820465, Final Batch Loss: 0.3259432315826416\n",
      "Epoch 1587, Loss: 0.44812609255313873, Final Batch Loss: 0.24214589595794678\n",
      "Epoch 1588, Loss: 0.4345449209213257, Final Batch Loss: 0.2737846374511719\n",
      "Epoch 1589, Loss: 0.4347585290670395, Final Batch Loss: 0.2294469028711319\n",
      "Epoch 1590, Loss: 0.36958225071430206, Final Batch Loss: 0.16247454285621643\n",
      "Epoch 1591, Loss: 0.3830201327800751, Final Batch Loss: 0.1987738162279129\n",
      "Epoch 1592, Loss: 0.36786891520023346, Final Batch Loss: 0.1856687366962433\n",
      "Epoch 1593, Loss: 0.533871054649353, Final Batch Loss: 0.2880481481552124\n",
      "Epoch 1594, Loss: 0.46921131014823914, Final Batch Loss: 0.2503594160079956\n",
      "Epoch 1595, Loss: 0.38233159482479095, Final Batch Loss: 0.1414041668176651\n",
      "Epoch 1596, Loss: 0.43315036594867706, Final Batch Loss: 0.218054860830307\n",
      "Epoch 1597, Loss: 0.38624879717826843, Final Batch Loss: 0.19764533638954163\n",
      "Epoch 1598, Loss: 0.41555556654930115, Final Batch Loss: 0.22343355417251587\n",
      "Epoch 1599, Loss: 0.45021720230579376, Final Batch Loss: 0.18851225078105927\n",
      "Epoch 1600, Loss: 0.4437999576330185, Final Batch Loss: 0.26100412011146545\n",
      "Epoch 1601, Loss: 0.5298658907413483, Final Batch Loss: 0.2725963592529297\n",
      "Epoch 1602, Loss: 0.4444235861301422, Final Batch Loss: 0.22990265488624573\n",
      "Epoch 1603, Loss: 0.4243346005678177, Final Batch Loss: 0.18380069732666016\n",
      "Epoch 1604, Loss: 0.4269254505634308, Final Batch Loss: 0.2108374685049057\n",
      "Epoch 1605, Loss: 0.4052917957305908, Final Batch Loss: 0.23514777421951294\n",
      "Epoch 1606, Loss: 0.44008421897888184, Final Batch Loss: 0.2093498557806015\n",
      "Epoch 1607, Loss: 0.4004599452018738, Final Batch Loss: 0.22328892350196838\n",
      "Epoch 1608, Loss: 0.4025641083717346, Final Batch Loss: 0.1936863511800766\n",
      "Epoch 1609, Loss: 0.4849836677312851, Final Batch Loss: 0.2540696859359741\n",
      "Epoch 1610, Loss: 0.3851110190153122, Final Batch Loss: 0.19932833313941956\n",
      "Epoch 1611, Loss: 0.3829699605703354, Final Batch Loss: 0.20829728245735168\n",
      "Epoch 1612, Loss: 0.40710897743701935, Final Batch Loss: 0.1812536120414734\n",
      "Epoch 1613, Loss: 0.41958214342594147, Final Batch Loss: 0.23024187982082367\n",
      "Epoch 1614, Loss: 0.4546700119972229, Final Batch Loss: 0.29600900411605835\n",
      "Epoch 1615, Loss: 0.33837826550006866, Final Batch Loss: 0.1512124240398407\n",
      "Epoch 1616, Loss: 0.4013700783252716, Final Batch Loss: 0.16858632862567902\n",
      "Epoch 1617, Loss: 0.44463343918323517, Final Batch Loss: 0.22905060648918152\n",
      "Epoch 1618, Loss: 0.4066159278154373, Final Batch Loss: 0.19282746315002441\n",
      "Epoch 1619, Loss: 0.48562388122081757, Final Batch Loss: 0.2477411925792694\n",
      "Epoch 1620, Loss: 0.4200289249420166, Final Batch Loss: 0.23185284435749054\n",
      "Epoch 1621, Loss: 0.4232681691646576, Final Batch Loss: 0.19277429580688477\n",
      "Epoch 1622, Loss: 0.4602366089820862, Final Batch Loss: 0.2680605351924896\n",
      "Epoch 1623, Loss: 0.4050423204898834, Final Batch Loss: 0.20818912982940674\n",
      "Epoch 1624, Loss: 0.46343643963336945, Final Batch Loss: 0.24282558262348175\n",
      "Epoch 1625, Loss: 0.4787977933883667, Final Batch Loss: 0.2720959186553955\n",
      "Epoch 1626, Loss: 0.4278402030467987, Final Batch Loss: 0.21734513342380524\n",
      "Epoch 1627, Loss: 0.3784203976392746, Final Batch Loss: 0.1697302609682083\n",
      "Epoch 1628, Loss: 0.44675976037979126, Final Batch Loss: 0.24789701402187347\n",
      "Epoch 1629, Loss: 0.3982761949300766, Final Batch Loss: 0.22191745042800903\n",
      "Epoch 1630, Loss: 0.3822886794805527, Final Batch Loss: 0.15052814781665802\n",
      "Epoch 1631, Loss: 0.519417330622673, Final Batch Loss: 0.2464754432439804\n",
      "Epoch 1632, Loss: 0.3541705310344696, Final Batch Loss: 0.15685158967971802\n",
      "Epoch 1633, Loss: 0.4190654903650284, Final Batch Loss: 0.2140059620141983\n",
      "Epoch 1634, Loss: 0.43236660957336426, Final Batch Loss: 0.18520118296146393\n",
      "Epoch 1635, Loss: 0.4094707816839218, Final Batch Loss: 0.16464222967624664\n",
      "Epoch 1636, Loss: 0.3993001729249954, Final Batch Loss: 0.187392458319664\n",
      "Epoch 1637, Loss: 0.37545546889305115, Final Batch Loss: 0.1867426633834839\n",
      "Epoch 1638, Loss: 0.4421984851360321, Final Batch Loss: 0.22487114369869232\n",
      "Epoch 1639, Loss: 0.3611244261264801, Final Batch Loss: 0.16548705101013184\n",
      "Epoch 1640, Loss: 0.4309068024158478, Final Batch Loss: 0.2146017849445343\n",
      "Epoch 1641, Loss: 0.4382382184267044, Final Batch Loss: 0.24289625883102417\n",
      "Epoch 1642, Loss: 0.43610119819641113, Final Batch Loss: 0.2388419508934021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1643, Loss: 0.47123634815216064, Final Batch Loss: 0.2645605206489563\n",
      "Epoch 1644, Loss: 0.4443068504333496, Final Batch Loss: 0.2555704712867737\n",
      "Epoch 1645, Loss: 0.39795762300491333, Final Batch Loss: 0.2027021050453186\n",
      "Epoch 1646, Loss: 0.42840254306793213, Final Batch Loss: 0.2090979367494583\n",
      "Epoch 1647, Loss: 0.3914036601781845, Final Batch Loss: 0.21533729135990143\n",
      "Epoch 1648, Loss: 0.4097244590520859, Final Batch Loss: 0.1993207186460495\n",
      "Epoch 1649, Loss: 0.45656080543994904, Final Batch Loss: 0.28065136075019836\n",
      "Epoch 1650, Loss: 0.4402911365032196, Final Batch Loss: 0.19974930584430695\n",
      "Epoch 1651, Loss: 0.3948061913251877, Final Batch Loss: 0.17206160724163055\n",
      "Epoch 1652, Loss: 0.42675916850566864, Final Batch Loss: 0.21997705101966858\n",
      "Epoch 1653, Loss: 0.42374923825263977, Final Batch Loss: 0.20546478033065796\n",
      "Epoch 1654, Loss: 0.4357960820198059, Final Batch Loss: 0.23934204876422882\n",
      "Epoch 1655, Loss: 0.39420656859874725, Final Batch Loss: 0.20155619084835052\n",
      "Epoch 1656, Loss: 0.462433785200119, Final Batch Loss: 0.23292319476604462\n",
      "Epoch 1657, Loss: 0.47432973980903625, Final Batch Loss: 0.24556253850460052\n",
      "Epoch 1658, Loss: 0.492947518825531, Final Batch Loss: 0.3002689778804779\n",
      "Epoch 1659, Loss: 0.4547833800315857, Final Batch Loss: 0.2568407952785492\n",
      "Epoch 1660, Loss: 0.4153745174407959, Final Batch Loss: 0.20725087821483612\n",
      "Epoch 1661, Loss: 0.39736707508563995, Final Batch Loss: 0.20209182798862457\n",
      "Epoch 1662, Loss: 0.4268946945667267, Final Batch Loss: 0.18880575895309448\n",
      "Epoch 1663, Loss: 0.41470967233181, Final Batch Loss: 0.20479577779769897\n",
      "Epoch 1664, Loss: 0.446393147110939, Final Batch Loss: 0.27080923318862915\n",
      "Epoch 1665, Loss: 0.4692149758338928, Final Batch Loss: 0.2650834619998932\n",
      "Epoch 1666, Loss: 0.4191029816865921, Final Batch Loss: 0.2061348408460617\n",
      "Epoch 1667, Loss: 0.4980529099702835, Final Batch Loss: 0.2636002004146576\n",
      "Epoch 1668, Loss: 0.3896592706441879, Final Batch Loss: 0.16385243833065033\n",
      "Epoch 1669, Loss: 0.441240593791008, Final Batch Loss: 0.2343561202287674\n",
      "Epoch 1670, Loss: 0.3989720791578293, Final Batch Loss: 0.20041750371456146\n",
      "Epoch 1671, Loss: 0.45206882059574127, Final Batch Loss: 0.24186094105243683\n",
      "Epoch 1672, Loss: 0.4401642680168152, Final Batch Loss: 0.23685458302497864\n",
      "Epoch 1673, Loss: 0.4332500249147415, Final Batch Loss: 0.22289438545703888\n",
      "Epoch 1674, Loss: 0.4346032291650772, Final Batch Loss: 0.2549159526824951\n",
      "Epoch 1675, Loss: 0.39077259600162506, Final Batch Loss: 0.17886096239089966\n",
      "Epoch 1676, Loss: 0.3705415725708008, Final Batch Loss: 0.1837071180343628\n",
      "Epoch 1677, Loss: 0.3729460686445236, Final Batch Loss: 0.14376452565193176\n",
      "Epoch 1678, Loss: 0.4755561649799347, Final Batch Loss: 0.19065040349960327\n",
      "Epoch 1679, Loss: 0.4446638375520706, Final Batch Loss: 0.2056540995836258\n",
      "Epoch 1680, Loss: 0.39370065927505493, Final Batch Loss: 0.17872606217861176\n",
      "Epoch 1681, Loss: 0.40779776871204376, Final Batch Loss: 0.1935242861509323\n",
      "Epoch 1682, Loss: 0.40105481445789337, Final Batch Loss: 0.2194279134273529\n",
      "Epoch 1683, Loss: 0.4107325077056885, Final Batch Loss: 0.2224992960691452\n",
      "Epoch 1684, Loss: 0.3816809356212616, Final Batch Loss: 0.1731872856616974\n",
      "Epoch 1685, Loss: 0.4121037870645523, Final Batch Loss: 0.16398243606090546\n",
      "Epoch 1686, Loss: 0.38356082141399384, Final Batch Loss: 0.17852920293807983\n",
      "Epoch 1687, Loss: 0.41690194606781006, Final Batch Loss: 0.19070638716220856\n",
      "Epoch 1688, Loss: 0.4586965888738632, Final Batch Loss: 0.2514972388744354\n",
      "Epoch 1689, Loss: 0.4593128561973572, Final Batch Loss: 0.20158466696739197\n",
      "Epoch 1690, Loss: 0.42786848545074463, Final Batch Loss: 0.1678372323513031\n",
      "Epoch 1691, Loss: 0.44465604424476624, Final Batch Loss: 0.20253892242908478\n",
      "Epoch 1692, Loss: 0.42409349977970123, Final Batch Loss: 0.23652192950248718\n",
      "Epoch 1693, Loss: 0.40572822093963623, Final Batch Loss: 0.19591878354549408\n",
      "Epoch 1694, Loss: 0.40564143657684326, Final Batch Loss: 0.1741178333759308\n",
      "Epoch 1695, Loss: 0.43751272559165955, Final Batch Loss: 0.24721363186836243\n",
      "Epoch 1696, Loss: 0.4230696111917496, Final Batch Loss: 0.17728930711746216\n",
      "Epoch 1697, Loss: 0.3466940373182297, Final Batch Loss: 0.17914916574954987\n",
      "Epoch 1698, Loss: 0.4487294852733612, Final Batch Loss: 0.24589583277702332\n",
      "Epoch 1699, Loss: 0.42773060500621796, Final Batch Loss: 0.2247404307126999\n",
      "Epoch 1700, Loss: 0.461581289768219, Final Batch Loss: 0.21444542706012726\n",
      "Epoch 1701, Loss: 0.4526858627796173, Final Batch Loss: 0.2342575490474701\n",
      "Epoch 1702, Loss: 0.39958132803440094, Final Batch Loss: 0.22626739740371704\n",
      "Epoch 1703, Loss: 0.4176073521375656, Final Batch Loss: 0.2218387871980667\n",
      "Epoch 1704, Loss: 0.485028401017189, Final Batch Loss: 0.272211492061615\n",
      "Epoch 1705, Loss: 0.4422784000635147, Final Batch Loss: 0.2367819845676422\n",
      "Epoch 1706, Loss: 0.433505654335022, Final Batch Loss: 0.1841239333152771\n",
      "Epoch 1707, Loss: 0.37691202759742737, Final Batch Loss: 0.1996241956949234\n",
      "Epoch 1708, Loss: 0.43453188240528107, Final Batch Loss: 0.2501182556152344\n",
      "Epoch 1709, Loss: 0.3912677615880966, Final Batch Loss: 0.2060430496931076\n",
      "Epoch 1710, Loss: 0.43566159904003143, Final Batch Loss: 0.17983685433864594\n",
      "Epoch 1711, Loss: 0.49453553557395935, Final Batch Loss: 0.3045276701450348\n",
      "Epoch 1712, Loss: 0.4440716952085495, Final Batch Loss: 0.2213563770055771\n",
      "Epoch 1713, Loss: 0.457623153924942, Final Batch Loss: 0.26465263962745667\n",
      "Epoch 1714, Loss: 0.36118368804454803, Final Batch Loss: 0.19791992008686066\n",
      "Epoch 1715, Loss: 0.41245076060295105, Final Batch Loss: 0.20619259774684906\n",
      "Epoch 1716, Loss: 0.39594200253486633, Final Batch Loss: 0.19800323247909546\n",
      "Epoch 1717, Loss: 0.32763438671827316, Final Batch Loss: 0.11387037485837936\n",
      "Epoch 1718, Loss: 0.4189765900373459, Final Batch Loss: 0.19279725849628448\n",
      "Epoch 1719, Loss: 0.40129366517066956, Final Batch Loss: 0.18262417614459991\n",
      "Epoch 1720, Loss: 0.3566453754901886, Final Batch Loss: 0.18060703575611115\n",
      "Epoch 1721, Loss: 0.4178798049688339, Final Batch Loss: 0.15170632302761078\n",
      "Epoch 1722, Loss: 0.4606151133775711, Final Batch Loss: 0.2475871592760086\n",
      "Epoch 1723, Loss: 0.4829591065645218, Final Batch Loss: 0.21852298080921173\n",
      "Epoch 1724, Loss: 0.5085833668708801, Final Batch Loss: 0.24595901370048523\n",
      "Epoch 1725, Loss: 0.43624337017536163, Final Batch Loss: 0.2349349856376648\n",
      "Epoch 1726, Loss: 0.40209798514842987, Final Batch Loss: 0.17545577883720398\n",
      "Epoch 1727, Loss: 0.4261940121650696, Final Batch Loss: 0.21538099646568298\n",
      "Epoch 1728, Loss: 0.4103451073169708, Final Batch Loss: 0.18538489937782288\n",
      "Epoch 1729, Loss: 0.47009459137916565, Final Batch Loss: 0.21540266275405884\n",
      "Epoch 1730, Loss: 0.433937206864357, Final Batch Loss: 0.18883231282234192\n",
      "Epoch 1731, Loss: 0.450806587934494, Final Batch Loss: 0.28314071893692017\n",
      "Epoch 1732, Loss: 0.4372473210096359, Final Batch Loss: 0.24746271967887878\n",
      "Epoch 1733, Loss: 0.39424046874046326, Final Batch Loss: 0.18873979151248932\n",
      "Epoch 1734, Loss: 0.43804599344730377, Final Batch Loss: 0.23590299487113953\n",
      "Epoch 1735, Loss: 0.3865647315979004, Final Batch Loss: 0.19338704645633698\n",
      "Epoch 1736, Loss: 0.36437810957431793, Final Batch Loss: 0.21960628032684326\n",
      "Epoch 1737, Loss: 0.4118679612874985, Final Batch Loss: 0.239296093583107\n",
      "Epoch 1738, Loss: 0.407312735915184, Final Batch Loss: 0.224848672747612\n",
      "Epoch 1739, Loss: 0.3864998519420624, Final Batch Loss: 0.19320201873779297\n",
      "Epoch 1740, Loss: 0.37582941353321075, Final Batch Loss: 0.19317646324634552\n",
      "Epoch 1741, Loss: 0.4103037118911743, Final Batch Loss: 0.2045334428548813\n",
      "Epoch 1742, Loss: 0.389327809214592, Final Batch Loss: 0.18493814766407013\n",
      "Epoch 1743, Loss: 0.3991605043411255, Final Batch Loss: 0.18301554024219513\n",
      "Epoch 1744, Loss: 0.4812236875295639, Final Batch Loss: 0.23322035372257233\n",
      "Epoch 1745, Loss: 0.37556086480617523, Final Batch Loss: 0.20499850809574127\n",
      "Epoch 1746, Loss: 0.4428841471672058, Final Batch Loss: 0.254483163356781\n",
      "Epoch 1747, Loss: 0.45301108062267303, Final Batch Loss: 0.2022925764322281\n",
      "Epoch 1748, Loss: 0.3602144420146942, Final Batch Loss: 0.16400399804115295\n",
      "Epoch 1749, Loss: 0.38325923681259155, Final Batch Loss: 0.21302270889282227\n",
      "Epoch 1750, Loss: 0.39114661514759064, Final Batch Loss: 0.1749371737241745\n",
      "Epoch 1751, Loss: 0.3624347895383835, Final Batch Loss: 0.14438438415527344\n",
      "Epoch 1752, Loss: 0.4303740859031677, Final Batch Loss: 0.22132806479930878\n",
      "Epoch 1753, Loss: 0.4787607491016388, Final Batch Loss: 0.1984381079673767\n",
      "Epoch 1754, Loss: 0.37483687698841095, Final Batch Loss: 0.17897413671016693\n",
      "Epoch 1755, Loss: 0.4353647381067276, Final Batch Loss: 0.22679007053375244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1756, Loss: 0.3533174991607666, Final Batch Loss: 0.13933254778385162\n",
      "Epoch 1757, Loss: 0.49598391354084015, Final Batch Loss: 0.27608445286750793\n",
      "Epoch 1758, Loss: 0.355780690908432, Final Batch Loss: 0.16572900116443634\n",
      "Epoch 1759, Loss: 0.38764943182468414, Final Batch Loss: 0.17948086559772491\n",
      "Epoch 1760, Loss: 0.4304691106081009, Final Batch Loss: 0.2025328427553177\n",
      "Epoch 1761, Loss: 0.41080330312252045, Final Batch Loss: 0.2313895970582962\n",
      "Epoch 1762, Loss: 0.42701955139636993, Final Batch Loss: 0.2351956069469452\n",
      "Epoch 1763, Loss: 0.37383754551410675, Final Batch Loss: 0.137503519654274\n",
      "Epoch 1764, Loss: 0.5111205577850342, Final Batch Loss: 0.28490185737609863\n",
      "Epoch 1765, Loss: 0.45751315355300903, Final Batch Loss: 0.24308276176452637\n",
      "Epoch 1766, Loss: 0.3639654070138931, Final Batch Loss: 0.15504395961761475\n",
      "Epoch 1767, Loss: 0.41651342809200287, Final Batch Loss: 0.21441394090652466\n",
      "Epoch 1768, Loss: 0.3705618977546692, Final Batch Loss: 0.20721890032291412\n",
      "Epoch 1769, Loss: 0.38365721702575684, Final Batch Loss: 0.20954985916614532\n",
      "Epoch 1770, Loss: 0.3943771868944168, Final Batch Loss: 0.14237163960933685\n",
      "Epoch 1771, Loss: 0.5034442991018295, Final Batch Loss: 0.30140420794487\n",
      "Epoch 1772, Loss: 0.40039266645908356, Final Batch Loss: 0.2292850911617279\n",
      "Epoch 1773, Loss: 0.4784362316131592, Final Batch Loss: 0.2635737657546997\n",
      "Epoch 1774, Loss: 0.38632774353027344, Final Batch Loss: 0.16942377388477325\n",
      "Epoch 1775, Loss: 0.4426960200071335, Final Batch Loss: 0.21705085039138794\n",
      "Epoch 1776, Loss: 0.45920899510383606, Final Batch Loss: 0.1832159459590912\n",
      "Epoch 1777, Loss: 0.45824822783470154, Final Batch Loss: 0.22771082818508148\n",
      "Epoch 1778, Loss: 0.4186197817325592, Final Batch Loss: 0.18069179356098175\n",
      "Epoch 1779, Loss: 0.36283715069293976, Final Batch Loss: 0.1859797239303589\n",
      "Epoch 1780, Loss: 0.42267535626888275, Final Batch Loss: 0.20525746047496796\n",
      "Epoch 1781, Loss: 0.3942590653896332, Final Batch Loss: 0.21435099840164185\n",
      "Epoch 1782, Loss: 0.35495495796203613, Final Batch Loss: 0.14379370212554932\n",
      "Epoch 1783, Loss: 0.3993379473686218, Final Batch Loss: 0.23712043464183807\n",
      "Epoch 1784, Loss: 0.38122814893722534, Final Batch Loss: 0.18114693462848663\n",
      "Epoch 1785, Loss: 0.3956083357334137, Final Batch Loss: 0.19038142263889313\n",
      "Epoch 1786, Loss: 0.4072660654783249, Final Batch Loss: 0.19256345927715302\n",
      "Epoch 1787, Loss: 0.41418126225471497, Final Batch Loss: 0.23321478068828583\n",
      "Epoch 1788, Loss: 0.3977874219417572, Final Batch Loss: 0.166209414601326\n",
      "Epoch 1789, Loss: 0.48355135321617126, Final Batch Loss: 0.23502051830291748\n",
      "Epoch 1790, Loss: 0.4758424162864685, Final Batch Loss: 0.2857595682144165\n",
      "Epoch 1791, Loss: 0.43616896867752075, Final Batch Loss: 0.2188739776611328\n",
      "Epoch 1792, Loss: 0.3996856361627579, Final Batch Loss: 0.22943085432052612\n",
      "Epoch 1793, Loss: 0.3741072863340378, Final Batch Loss: 0.16606701910495758\n",
      "Epoch 1794, Loss: 0.42451539635658264, Final Batch Loss: 0.18435341119766235\n",
      "Epoch 1795, Loss: 0.4317447394132614, Final Batch Loss: 0.21595221757888794\n",
      "Epoch 1796, Loss: 0.4275592416524887, Final Batch Loss: 0.2140379548072815\n",
      "Epoch 1797, Loss: 0.46336841583251953, Final Batch Loss: 0.2244390845298767\n",
      "Epoch 1798, Loss: 0.4517270475625992, Final Batch Loss: 0.21351739764213562\n",
      "Epoch 1799, Loss: 0.3623626232147217, Final Batch Loss: 0.14576835930347443\n",
      "Epoch 1800, Loss: 0.412028968334198, Final Batch Loss: 0.19985394179821014\n",
      "Epoch 1801, Loss: 0.3441632390022278, Final Batch Loss: 0.11968821287155151\n",
      "Epoch 1802, Loss: 0.4494057297706604, Final Batch Loss: 0.20072346925735474\n",
      "Epoch 1803, Loss: 0.4576784372329712, Final Batch Loss: 0.29241639375686646\n",
      "Epoch 1804, Loss: 0.43415360152721405, Final Batch Loss: 0.2506259083747864\n",
      "Epoch 1805, Loss: 0.4081032872200012, Final Batch Loss: 0.13483375310897827\n",
      "Epoch 1806, Loss: 0.37251846492290497, Final Batch Loss: 0.15718702971935272\n",
      "Epoch 1807, Loss: 0.43457503616809845, Final Batch Loss: 0.22127079963684082\n",
      "Epoch 1808, Loss: 0.46166186034679413, Final Batch Loss: 0.25563758611679077\n",
      "Epoch 1809, Loss: 0.41962675750255585, Final Batch Loss: 0.18767178058624268\n",
      "Epoch 1810, Loss: 0.44691002368927, Final Batch Loss: 0.2211376428604126\n",
      "Epoch 1811, Loss: 0.40947890281677246, Final Batch Loss: 0.20863014459609985\n",
      "Epoch 1812, Loss: 0.3753172904253006, Final Batch Loss: 0.17092998325824738\n",
      "Epoch 1813, Loss: 0.40761855244636536, Final Batch Loss: 0.21738269925117493\n",
      "Epoch 1814, Loss: 0.389750674366951, Final Batch Loss: 0.20459917187690735\n",
      "Epoch 1815, Loss: 0.3270055502653122, Final Batch Loss: 0.15742230415344238\n",
      "Epoch 1816, Loss: 0.3911852687597275, Final Batch Loss: 0.17041197419166565\n",
      "Epoch 1817, Loss: 0.39595185220241547, Final Batch Loss: 0.1958036571741104\n",
      "Epoch 1818, Loss: 0.3714691400527954, Final Batch Loss: 0.16940833628177643\n",
      "Epoch 1819, Loss: 0.4975809305906296, Final Batch Loss: 0.27457350492477417\n",
      "Epoch 1820, Loss: 0.40439312160015106, Final Batch Loss: 0.21572454273700714\n",
      "Epoch 1821, Loss: 0.418959379196167, Final Batch Loss: 0.21862752735614777\n",
      "Epoch 1822, Loss: 0.37368836998939514, Final Batch Loss: 0.17889277637004852\n",
      "Epoch 1823, Loss: 0.3453124612569809, Final Batch Loss: 0.18572962284088135\n",
      "Epoch 1824, Loss: 0.37575961649417877, Final Batch Loss: 0.203909769654274\n",
      "Epoch 1825, Loss: 0.4756275862455368, Final Batch Loss: 0.25824636220932007\n",
      "Epoch 1826, Loss: 0.4055207073688507, Final Batch Loss: 0.2259165346622467\n",
      "Epoch 1827, Loss: 0.40268903970718384, Final Batch Loss: 0.18851310014724731\n",
      "Epoch 1828, Loss: 0.39910438656806946, Final Batch Loss: 0.17257878184318542\n",
      "Epoch 1829, Loss: 0.4839267283678055, Final Batch Loss: 0.28466808795928955\n",
      "Epoch 1830, Loss: 0.3947310447692871, Final Batch Loss: 0.2131795883178711\n",
      "Epoch 1831, Loss: 0.44475655257701874, Final Batch Loss: 0.20667321979999542\n",
      "Epoch 1832, Loss: 0.4075242429971695, Final Batch Loss: 0.17790259420871735\n",
      "Epoch 1833, Loss: 0.41260603070259094, Final Batch Loss: 0.15796563029289246\n",
      "Epoch 1834, Loss: 0.4000261425971985, Final Batch Loss: 0.22368790209293365\n",
      "Epoch 1835, Loss: 0.4120940566062927, Final Batch Loss: 0.20695161819458008\n",
      "Epoch 1836, Loss: 0.450552374124527, Final Batch Loss: 0.24124129116535187\n",
      "Epoch 1837, Loss: 0.4086970239877701, Final Batch Loss: 0.23741668462753296\n",
      "Epoch 1838, Loss: 0.4120083600282669, Final Batch Loss: 0.2114560753107071\n",
      "Epoch 1839, Loss: 0.4357692003250122, Final Batch Loss: 0.2396804690361023\n",
      "Epoch 1840, Loss: 0.4122939705848694, Final Batch Loss: 0.24987341463565826\n",
      "Epoch 1841, Loss: 0.40828919410705566, Final Batch Loss: 0.20450030267238617\n",
      "Epoch 1842, Loss: 0.35341331362724304, Final Batch Loss: 0.1849772185087204\n",
      "Epoch 1843, Loss: 0.38278675079345703, Final Batch Loss: 0.19591884315013885\n",
      "Epoch 1844, Loss: 0.4517361521720886, Final Batch Loss: 0.21492035686969757\n",
      "Epoch 1845, Loss: 0.3867444843053818, Final Batch Loss: 0.18114924430847168\n",
      "Epoch 1846, Loss: 0.3843058794736862, Final Batch Loss: 0.19809862971305847\n",
      "Epoch 1847, Loss: 0.46363651752471924, Final Batch Loss: 0.18779578804969788\n",
      "Epoch 1848, Loss: 0.38902169466018677, Final Batch Loss: 0.18997669219970703\n",
      "Epoch 1849, Loss: 0.4063769727945328, Final Batch Loss: 0.23146821558475494\n",
      "Epoch 1850, Loss: 0.40881387889385223, Final Batch Loss: 0.21676868200302124\n",
      "Epoch 1851, Loss: 0.3499241769313812, Final Batch Loss: 0.15090137720108032\n",
      "Epoch 1852, Loss: 0.3919198662042618, Final Batch Loss: 0.2064058929681778\n",
      "Epoch 1853, Loss: 0.3894749879837036, Final Batch Loss: 0.16384008526802063\n",
      "Epoch 1854, Loss: 0.4863692820072174, Final Batch Loss: 0.2781608998775482\n",
      "Epoch 1855, Loss: 0.4585171043872833, Final Batch Loss: 0.290100634098053\n",
      "Epoch 1856, Loss: 0.37212038040161133, Final Batch Loss: 0.2045096904039383\n",
      "Epoch 1857, Loss: 0.33680370450019836, Final Batch Loss: 0.18116043508052826\n",
      "Epoch 1858, Loss: 0.3790352940559387, Final Batch Loss: 0.18508489429950714\n",
      "Epoch 1859, Loss: 0.4401405602693558, Final Batch Loss: 0.2379649430513382\n",
      "Epoch 1860, Loss: 0.37166376411914825, Final Batch Loss: 0.17575256526470184\n",
      "Epoch 1861, Loss: 0.36060304939746857, Final Batch Loss: 0.20610730350017548\n",
      "Epoch 1862, Loss: 0.38393355906009674, Final Batch Loss: 0.19158008694648743\n",
      "Epoch 1863, Loss: 0.41558632254600525, Final Batch Loss: 0.1754934936761856\n",
      "Epoch 1864, Loss: 0.4378003478050232, Final Batch Loss: 0.1956411600112915\n",
      "Epoch 1865, Loss: 0.41523076593875885, Final Batch Loss: 0.22380778193473816\n",
      "Epoch 1866, Loss: 0.4067368358373642, Final Batch Loss: 0.1594160497188568\n",
      "Epoch 1867, Loss: 0.40509718656539917, Final Batch Loss: 0.1773834228515625\n",
      "Epoch 1868, Loss: 0.40809889137744904, Final Batch Loss: 0.21989066898822784\n",
      "Epoch 1869, Loss: 0.4337950497865677, Final Batch Loss: 0.2276144027709961\n",
      "Epoch 1870, Loss: 0.4107196480035782, Final Batch Loss: 0.2156641036272049\n",
      "Epoch 1871, Loss: 0.38542118668556213, Final Batch Loss: 0.19941078126430511\n",
      "Epoch 1872, Loss: 0.3994937539100647, Final Batch Loss: 0.21634358167648315\n",
      "Epoch 1873, Loss: 0.4037446528673172, Final Batch Loss: 0.20403748750686646\n",
      "Epoch 1874, Loss: 0.4145982265472412, Final Batch Loss: 0.21012794971466064\n",
      "Epoch 1875, Loss: 0.47865286469459534, Final Batch Loss: 0.2761504352092743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1876, Loss: 0.3614365756511688, Final Batch Loss: 0.16120590269565582\n",
      "Epoch 1877, Loss: 0.35173867642879486, Final Batch Loss: 0.18871738016605377\n",
      "Epoch 1878, Loss: 0.41241125762462616, Final Batch Loss: 0.22212907671928406\n",
      "Epoch 1879, Loss: 0.39341239631175995, Final Batch Loss: 0.18818682432174683\n",
      "Epoch 1880, Loss: 0.38279373943805695, Final Batch Loss: 0.19220586121082306\n",
      "Epoch 1881, Loss: 0.3949139267206192, Final Batch Loss: 0.21180665493011475\n",
      "Epoch 1882, Loss: 0.4413254112005234, Final Batch Loss: 0.23235280811786652\n",
      "Epoch 1883, Loss: 0.36330200731754303, Final Batch Loss: 0.19616763293743134\n",
      "Epoch 1884, Loss: 0.4849066585302353, Final Batch Loss: 0.22654785215854645\n",
      "Epoch 1885, Loss: 0.4119937866926193, Final Batch Loss: 0.19293391704559326\n",
      "Epoch 1886, Loss: 0.41555874049663544, Final Batch Loss: 0.24053005874156952\n",
      "Epoch 1887, Loss: 0.428218737244606, Final Batch Loss: 0.26103726029396057\n",
      "Epoch 1888, Loss: 0.40619924664497375, Final Batch Loss: 0.19614717364311218\n",
      "Epoch 1889, Loss: 0.36885911226272583, Final Batch Loss: 0.15546530485153198\n",
      "Epoch 1890, Loss: 0.3922697454690933, Final Batch Loss: 0.17401865124702454\n",
      "Epoch 1891, Loss: 0.3927486538887024, Final Batch Loss: 0.16321194171905518\n",
      "Epoch 1892, Loss: 0.3668757975101471, Final Batch Loss: 0.13054698705673218\n",
      "Epoch 1893, Loss: 0.34174907207489014, Final Batch Loss: 0.16624562442302704\n",
      "Epoch 1894, Loss: 0.3339366018772125, Final Batch Loss: 0.16668638586997986\n",
      "Epoch 1895, Loss: 0.395561084151268, Final Batch Loss: 0.19856388866901398\n",
      "Epoch 1896, Loss: 0.37664422392845154, Final Batch Loss: 0.18654365837574005\n",
      "Epoch 1897, Loss: 0.39628390967845917, Final Batch Loss: 0.18625834584236145\n",
      "Epoch 1898, Loss: 0.41842325031757355, Final Batch Loss: 0.2106381207704544\n",
      "Epoch 1899, Loss: 0.3625367134809494, Final Batch Loss: 0.1470908224582672\n",
      "Epoch 1900, Loss: 0.3135725259780884, Final Batch Loss: 0.12846744060516357\n",
      "Epoch 1901, Loss: 0.4100847542285919, Final Batch Loss: 0.23406478762626648\n",
      "Epoch 1902, Loss: 0.4147022068500519, Final Batch Loss: 0.2016054391860962\n",
      "Epoch 1903, Loss: 0.42904984951019287, Final Batch Loss: 0.2751425802707672\n",
      "Epoch 1904, Loss: 0.4172218441963196, Final Batch Loss: 0.25090429186820984\n",
      "Epoch 1905, Loss: 0.4074324518442154, Final Batch Loss: 0.24201959371566772\n",
      "Epoch 1906, Loss: 0.4328683912754059, Final Batch Loss: 0.2698298692703247\n",
      "Epoch 1907, Loss: 0.43228068947792053, Final Batch Loss: 0.20548579096794128\n",
      "Epoch 1908, Loss: 0.36961326003074646, Final Batch Loss: 0.1635998785495758\n",
      "Epoch 1909, Loss: 0.3413989096879959, Final Batch Loss: 0.16223455965518951\n",
      "Epoch 1910, Loss: 0.37077245116233826, Final Batch Loss: 0.1584421694278717\n",
      "Epoch 1911, Loss: 0.4678473025560379, Final Batch Loss: 0.3126315474510193\n",
      "Epoch 1912, Loss: 0.3622455447912216, Final Batch Loss: 0.1802690327167511\n",
      "Epoch 1913, Loss: 0.37610605359077454, Final Batch Loss: 0.1596013605594635\n",
      "Epoch 1914, Loss: 0.365472748875618, Final Batch Loss: 0.19220858812332153\n",
      "Epoch 1915, Loss: 0.35597412288188934, Final Batch Loss: 0.14749979972839355\n",
      "Epoch 1916, Loss: 0.3944457471370697, Final Batch Loss: 0.20801228284835815\n",
      "Epoch 1917, Loss: 0.4412647634744644, Final Batch Loss: 0.21022900938987732\n",
      "Epoch 1918, Loss: 0.3818074017763138, Final Batch Loss: 0.1773293912410736\n",
      "Epoch 1919, Loss: 0.3926369845867157, Final Batch Loss: 0.23615120351314545\n",
      "Epoch 1920, Loss: 0.44009920954704285, Final Batch Loss: 0.23083974421024323\n",
      "Epoch 1921, Loss: 0.3752780258655548, Final Batch Loss: 0.1274479329586029\n",
      "Epoch 1922, Loss: 0.3984351009130478, Final Batch Loss: 0.22711633145809174\n",
      "Epoch 1923, Loss: 0.4120258241891861, Final Batch Loss: 0.18320678174495697\n",
      "Epoch 1924, Loss: 0.36610354483127594, Final Batch Loss: 0.1784304529428482\n",
      "Epoch 1925, Loss: 0.43141108751296997, Final Batch Loss: 0.22258172929286957\n",
      "Epoch 1926, Loss: 0.37521785497665405, Final Batch Loss: 0.16037896275520325\n",
      "Epoch 1927, Loss: 0.36956487596035004, Final Batch Loss: 0.18911860883235931\n",
      "Epoch 1928, Loss: 0.4365829825401306, Final Batch Loss: 0.20306777954101562\n",
      "Epoch 1929, Loss: 0.4011259824037552, Final Batch Loss: 0.17462623119354248\n",
      "Epoch 1930, Loss: 0.36222541332244873, Final Batch Loss: 0.147245392203331\n",
      "Epoch 1931, Loss: 0.4426642656326294, Final Batch Loss: 0.2768888771533966\n",
      "Epoch 1932, Loss: 0.42905621230602264, Final Batch Loss: 0.23094992339611053\n",
      "Epoch 1933, Loss: 0.43609753251075745, Final Batch Loss: 0.2295132428407669\n",
      "Epoch 1934, Loss: 0.3504693806171417, Final Batch Loss: 0.14687180519104004\n",
      "Epoch 1935, Loss: 0.387917160987854, Final Batch Loss: 0.14531919360160828\n",
      "Epoch 1936, Loss: 0.4487699121236801, Final Batch Loss: 0.19623763859272003\n",
      "Epoch 1937, Loss: 0.4044691026210785, Final Batch Loss: 0.18682600557804108\n",
      "Epoch 1938, Loss: 0.3623662143945694, Final Batch Loss: 0.14639832079410553\n",
      "Epoch 1939, Loss: 0.4565321207046509, Final Batch Loss: 0.18514695763587952\n",
      "Epoch 1940, Loss: 0.3859660029411316, Final Batch Loss: 0.20078814029693604\n",
      "Epoch 1941, Loss: 0.40820299088954926, Final Batch Loss: 0.23309701681137085\n",
      "Epoch 1942, Loss: 0.3683755099773407, Final Batch Loss: 0.1456204354763031\n",
      "Epoch 1943, Loss: 0.3070453554391861, Final Batch Loss: 0.1489526629447937\n",
      "Epoch 1944, Loss: 0.3890691548585892, Final Batch Loss: 0.2348214089870453\n",
      "Epoch 1945, Loss: 0.38505230844020844, Final Batch Loss: 0.21206854283809662\n",
      "Epoch 1946, Loss: 0.376091867685318, Final Batch Loss: 0.18744154274463654\n",
      "Epoch 1947, Loss: 0.34781862795352936, Final Batch Loss: 0.17443367838859558\n",
      "Epoch 1948, Loss: 0.41129522025585175, Final Batch Loss: 0.23268218338489532\n",
      "Epoch 1949, Loss: 0.49428871273994446, Final Batch Loss: 0.25577282905578613\n",
      "Epoch 1950, Loss: 0.35098251700401306, Final Batch Loss: 0.18936219811439514\n",
      "Epoch 1951, Loss: 0.3790968358516693, Final Batch Loss: 0.16402530670166016\n",
      "Epoch 1952, Loss: 0.3929355591535568, Final Batch Loss: 0.20173510909080505\n",
      "Epoch 1953, Loss: 0.39931976795196533, Final Batch Loss: 0.23143762350082397\n",
      "Epoch 1954, Loss: 0.3419486880302429, Final Batch Loss: 0.15660782158374786\n",
      "Epoch 1955, Loss: 0.3341774195432663, Final Batch Loss: 0.15854379534721375\n",
      "Epoch 1956, Loss: 0.4441738575696945, Final Batch Loss: 0.20646248757839203\n",
      "Epoch 1957, Loss: 0.3980262130498886, Final Batch Loss: 0.22662845253944397\n",
      "Epoch 1958, Loss: 0.3841123729944229, Final Batch Loss: 0.1698218733072281\n",
      "Epoch 1959, Loss: 0.35571685433387756, Final Batch Loss: 0.16129142045974731\n",
      "Epoch 1960, Loss: 0.4115997701883316, Final Batch Loss: 0.15905489027500153\n",
      "Epoch 1961, Loss: 0.3561738431453705, Final Batch Loss: 0.17128624022006989\n",
      "Epoch 1962, Loss: 0.4280412346124649, Final Batch Loss: 0.18465887010097504\n",
      "Epoch 1963, Loss: 0.4036298096179962, Final Batch Loss: 0.18681178987026215\n",
      "Epoch 1964, Loss: 0.40166497230529785, Final Batch Loss: 0.1685410887002945\n",
      "Epoch 1965, Loss: 0.3206147998571396, Final Batch Loss: 0.14773958921432495\n",
      "Epoch 1966, Loss: 0.3080958425998688, Final Batch Loss: 0.1254347413778305\n",
      "Epoch 1967, Loss: 0.34937335550785065, Final Batch Loss: 0.1590752899646759\n",
      "Epoch 1968, Loss: 0.3854720741510391, Final Batch Loss: 0.18342195451259613\n",
      "Epoch 1969, Loss: 0.39393751323223114, Final Batch Loss: 0.18988439440727234\n",
      "Epoch 1970, Loss: 0.3981446623802185, Final Batch Loss: 0.21837221086025238\n",
      "Epoch 1971, Loss: 0.41909241676330566, Final Batch Loss: 0.23656146228313446\n",
      "Epoch 1972, Loss: 0.3602108955383301, Final Batch Loss: 0.16389761865139008\n",
      "Epoch 1973, Loss: 0.43744800984859467, Final Batch Loss: 0.18954217433929443\n",
      "Epoch 1974, Loss: 0.3741491436958313, Final Batch Loss: 0.1739731878042221\n",
      "Epoch 1975, Loss: 0.4213717579841614, Final Batch Loss: 0.23893621563911438\n",
      "Epoch 1976, Loss: 0.41475242376327515, Final Batch Loss: 0.2524677515029907\n",
      "Epoch 1977, Loss: 0.3328116685152054, Final Batch Loss: 0.1739509403705597\n",
      "Epoch 1978, Loss: 0.3557313233613968, Final Batch Loss: 0.16388772428035736\n",
      "Epoch 1979, Loss: 0.43713101744651794, Final Batch Loss: 0.2313201129436493\n",
      "Epoch 1980, Loss: 0.43479740619659424, Final Batch Loss: 0.21437440812587738\n",
      "Epoch 1981, Loss: 0.3640555590391159, Final Batch Loss: 0.16159695386886597\n",
      "Epoch 1982, Loss: 0.3387680947780609, Final Batch Loss: 0.1360638439655304\n",
      "Epoch 1983, Loss: 0.4260954111814499, Final Batch Loss: 0.22048896551132202\n",
      "Epoch 1984, Loss: 0.3559812009334564, Final Batch Loss: 0.18325646221637726\n",
      "Epoch 1985, Loss: 0.4631691873073578, Final Batch Loss: 0.2193850725889206\n",
      "Epoch 1986, Loss: 0.3920750021934509, Final Batch Loss: 0.2142605483531952\n",
      "Epoch 1987, Loss: 0.3716658055782318, Final Batch Loss: 0.17868338525295258\n",
      "Epoch 1988, Loss: 0.4129805266857147, Final Batch Loss: 0.20125775039196014\n",
      "Epoch 1989, Loss: 0.43387843668460846, Final Batch Loss: 0.2788503170013428\n",
      "Epoch 1990, Loss: 0.3585245609283447, Final Batch Loss: 0.15673518180847168\n",
      "Epoch 1991, Loss: 0.3811323940753937, Final Batch Loss: 0.1693211793899536\n",
      "Epoch 1992, Loss: 0.42999137938022614, Final Batch Loss: 0.2416040450334549\n",
      "Epoch 1993, Loss: 0.414810910820961, Final Batch Loss: 0.2138601690530777\n",
      "Epoch 1994, Loss: 0.4171256870031357, Final Batch Loss: 0.20644254982471466\n",
      "Epoch 1995, Loss: 0.3613440692424774, Final Batch Loss: 0.18222162127494812\n",
      "Epoch 1996, Loss: 0.39055320620536804, Final Batch Loss: 0.19605094194412231\n",
      "Epoch 1997, Loss: 0.3535626083612442, Final Batch Loss: 0.13848432898521423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1998, Loss: 0.3920382708311081, Final Batch Loss: 0.18470878899097443\n",
      "Epoch 1999, Loss: 0.35908734798431396, Final Batch Loss: 0.17645220458507538\n",
      "Epoch 2000, Loss: 0.4070027768611908, Final Batch Loss: 0.18531592190265656\n",
      "Epoch 2001, Loss: 0.40693700313568115, Final Batch Loss: 0.23346249759197235\n",
      "Epoch 2002, Loss: 0.3779710680246353, Final Batch Loss: 0.18486061692237854\n",
      "Epoch 2003, Loss: 0.34229128062725067, Final Batch Loss: 0.14421968162059784\n",
      "Epoch 2004, Loss: 0.40107400715351105, Final Batch Loss: 0.14625592529773712\n",
      "Epoch 2005, Loss: 0.38940322399139404, Final Batch Loss: 0.1660521775484085\n",
      "Epoch 2006, Loss: 0.336653470993042, Final Batch Loss: 0.12770365178585052\n",
      "Epoch 2007, Loss: 0.4500293582677841, Final Batch Loss: 0.2557377815246582\n",
      "Epoch 2008, Loss: 0.3929751366376877, Final Batch Loss: 0.23943251371383667\n",
      "Epoch 2009, Loss: 0.39006489515304565, Final Batch Loss: 0.17601165175437927\n",
      "Epoch 2010, Loss: 0.38320188224315643, Final Batch Loss: 0.20826813578605652\n",
      "Epoch 2011, Loss: 0.37875042855739594, Final Batch Loss: 0.1911497265100479\n",
      "Epoch 2012, Loss: 0.3725850582122803, Final Batch Loss: 0.16737186908721924\n",
      "Epoch 2013, Loss: 0.45720934867858887, Final Batch Loss: 0.2494179606437683\n",
      "Epoch 2014, Loss: 0.37113574147224426, Final Batch Loss: 0.17302821576595306\n",
      "Epoch 2015, Loss: 0.36369891464710236, Final Batch Loss: 0.16920490562915802\n",
      "Epoch 2016, Loss: 0.4295869618654251, Final Batch Loss: 0.2917686998844147\n",
      "Epoch 2017, Loss: 0.33754871785640717, Final Batch Loss: 0.1823582947254181\n",
      "Epoch 2018, Loss: 0.37487635016441345, Final Batch Loss: 0.1818562150001526\n",
      "Epoch 2019, Loss: 0.40735748410224915, Final Batch Loss: 0.22020269930362701\n",
      "Epoch 2020, Loss: 0.3999905288219452, Final Batch Loss: 0.16076695919036865\n",
      "Epoch 2021, Loss: 0.4320972114801407, Final Batch Loss: 0.21616926789283752\n",
      "Epoch 2022, Loss: 0.38780577480793, Final Batch Loss: 0.23208755254745483\n",
      "Epoch 2023, Loss: 0.3600728213787079, Final Batch Loss: 0.16711917519569397\n",
      "Epoch 2024, Loss: 0.3919300436973572, Final Batch Loss: 0.17307694256305695\n",
      "Epoch 2025, Loss: 0.4582252502441406, Final Batch Loss: 0.24433296918869019\n",
      "Epoch 2026, Loss: 0.41914747655391693, Final Batch Loss: 0.2008119523525238\n",
      "Epoch 2027, Loss: 0.33148254454135895, Final Batch Loss: 0.13185963034629822\n",
      "Epoch 2028, Loss: 0.3724839985370636, Final Batch Loss: 0.19591356813907623\n",
      "Epoch 2029, Loss: 0.3657875508069992, Final Batch Loss: 0.18724006414413452\n",
      "Epoch 2030, Loss: 0.40819965302944183, Final Batch Loss: 0.22338108718395233\n",
      "Epoch 2031, Loss: 0.43245865404605865, Final Batch Loss: 0.19102416932582855\n",
      "Epoch 2032, Loss: 0.36060865223407745, Final Batch Loss: 0.15545545518398285\n",
      "Epoch 2033, Loss: 0.3892640173435211, Final Batch Loss: 0.1886972337961197\n",
      "Epoch 2034, Loss: 0.3904210776090622, Final Batch Loss: 0.20495344698429108\n",
      "Epoch 2035, Loss: 0.34407012164592743, Final Batch Loss: 0.16492511332035065\n",
      "Epoch 2036, Loss: 0.3793288767337799, Final Batch Loss: 0.21560139954090118\n",
      "Epoch 2037, Loss: 0.3608737140893936, Final Batch Loss: 0.16754233837127686\n",
      "Epoch 2038, Loss: 0.3542380779981613, Final Batch Loss: 0.17584338784217834\n",
      "Epoch 2039, Loss: 0.3632332682609558, Final Batch Loss: 0.19206316769123077\n",
      "Epoch 2040, Loss: 0.38899776339530945, Final Batch Loss: 0.2264181673526764\n",
      "Epoch 2041, Loss: 0.3367321342229843, Final Batch Loss: 0.18602721393108368\n",
      "Epoch 2042, Loss: 0.3590952903032303, Final Batch Loss: 0.19972661137580872\n",
      "Epoch 2043, Loss: 0.338586688041687, Final Batch Loss: 0.15292200446128845\n",
      "Epoch 2044, Loss: 0.3722343146800995, Final Batch Loss: 0.16016268730163574\n",
      "Epoch 2045, Loss: 0.5212112814188004, Final Batch Loss: 0.30706024169921875\n",
      "Epoch 2046, Loss: 0.4247162938117981, Final Batch Loss: 0.2252054661512375\n",
      "Epoch 2047, Loss: 0.4295114576816559, Final Batch Loss: 0.16565757989883423\n",
      "Epoch 2048, Loss: 0.3666747361421585, Final Batch Loss: 0.18934087455272675\n",
      "Epoch 2049, Loss: 0.37081484496593475, Final Batch Loss: 0.1478121429681778\n",
      "Epoch 2050, Loss: 0.3476085513830185, Final Batch Loss: 0.17402581870555878\n",
      "Epoch 2051, Loss: 0.3735422343015671, Final Batch Loss: 0.1903536468744278\n",
      "Epoch 2052, Loss: 0.4073450118303299, Final Batch Loss: 0.22032324969768524\n",
      "Epoch 2053, Loss: 0.36490491032600403, Final Batch Loss: 0.18014132976531982\n",
      "Epoch 2054, Loss: 0.3603924661874771, Final Batch Loss: 0.20542271435260773\n",
      "Epoch 2055, Loss: 0.40980808436870575, Final Batch Loss: 0.20762167870998383\n",
      "Epoch 2056, Loss: 0.38539502024650574, Final Batch Loss: 0.1903337836265564\n",
      "Epoch 2057, Loss: 0.36314305663108826, Final Batch Loss: 0.16628427803516388\n",
      "Epoch 2058, Loss: 0.38101305067539215, Final Batch Loss: 0.19283737242221832\n",
      "Epoch 2059, Loss: 0.3535168021917343, Final Batch Loss: 0.19201038777828217\n",
      "Epoch 2060, Loss: 0.3612019121646881, Final Batch Loss: 0.18519030511379242\n",
      "Epoch 2061, Loss: 0.3497249335050583, Final Batch Loss: 0.178972527384758\n",
      "Epoch 2062, Loss: 0.3568202704191208, Final Batch Loss: 0.1971091330051422\n",
      "Epoch 2063, Loss: 0.3534003347158432, Final Batch Loss: 0.18259400129318237\n",
      "Epoch 2064, Loss: 0.40427127480506897, Final Batch Loss: 0.19007258117198944\n",
      "Epoch 2065, Loss: 0.3689573109149933, Final Batch Loss: 0.13102424144744873\n",
      "Epoch 2066, Loss: 0.33743320405483246, Final Batch Loss: 0.1443168818950653\n",
      "Epoch 2067, Loss: 0.3949426859617233, Final Batch Loss: 0.22280927002429962\n",
      "Epoch 2068, Loss: 0.42758654057979584, Final Batch Loss: 0.2454504370689392\n",
      "Epoch 2069, Loss: 0.34166252613067627, Final Batch Loss: 0.17125912010669708\n",
      "Epoch 2070, Loss: 0.32799218595027924, Final Batch Loss: 0.17946824431419373\n",
      "Epoch 2071, Loss: 0.47936394810676575, Final Batch Loss: 0.23346073925495148\n",
      "Epoch 2072, Loss: 0.36004874110221863, Final Batch Loss: 0.19069446623325348\n",
      "Epoch 2073, Loss: 0.3659656196832657, Final Batch Loss: 0.15695789456367493\n",
      "Epoch 2074, Loss: 0.3652157783508301, Final Batch Loss: 0.17979109287261963\n",
      "Epoch 2075, Loss: 0.3043707311153412, Final Batch Loss: 0.15891900658607483\n",
      "Epoch 2076, Loss: 0.3389168232679367, Final Batch Loss: 0.14502541720867157\n",
      "Epoch 2077, Loss: 0.42646726965904236, Final Batch Loss: 0.22000746428966522\n",
      "Epoch 2078, Loss: 0.3891848027706146, Final Batch Loss: 0.2023027539253235\n",
      "Epoch 2079, Loss: 0.45221738517284393, Final Batch Loss: 0.25480812788009644\n",
      "Epoch 2080, Loss: 0.37233148515224457, Final Batch Loss: 0.22487398982048035\n",
      "Epoch 2081, Loss: 0.36182624101638794, Final Batch Loss: 0.2126336693763733\n",
      "Epoch 2082, Loss: 0.3590359538793564, Final Batch Loss: 0.17765003442764282\n",
      "Epoch 2083, Loss: 0.42746154963970184, Final Batch Loss: 0.20202375948429108\n",
      "Epoch 2084, Loss: 0.3669043630361557, Final Batch Loss: 0.16139017045497894\n",
      "Epoch 2085, Loss: 0.3918917179107666, Final Batch Loss: 0.20953533053398132\n",
      "Epoch 2086, Loss: 0.41509416699409485, Final Batch Loss: 0.2131802886724472\n",
      "Epoch 2087, Loss: 0.35146160423755646, Final Batch Loss: 0.1792164146900177\n",
      "Epoch 2088, Loss: 0.3725404739379883, Final Batch Loss: 0.1760365217924118\n",
      "Epoch 2089, Loss: 0.3386935889720917, Final Batch Loss: 0.16547635197639465\n",
      "Epoch 2090, Loss: 0.3388151526451111, Final Batch Loss: 0.17606274783611298\n",
      "Epoch 2091, Loss: 0.3909674733877182, Final Batch Loss: 0.20536287128925323\n",
      "Epoch 2092, Loss: 0.4260971248149872, Final Batch Loss: 0.2508986294269562\n",
      "Epoch 2093, Loss: 0.3729439824819565, Final Batch Loss: 0.19647057354450226\n",
      "Epoch 2094, Loss: 0.37183815240859985, Final Batch Loss: 0.17881233990192413\n",
      "Epoch 2095, Loss: 0.3255362808704376, Final Batch Loss: 0.1203797310590744\n",
      "Epoch 2096, Loss: 0.37922412157058716, Final Batch Loss: 0.19964680075645447\n",
      "Epoch 2097, Loss: 0.37693890929222107, Final Batch Loss: 0.2258341759443283\n",
      "Epoch 2098, Loss: 0.42166078090667725, Final Batch Loss: 0.2574499547481537\n",
      "Epoch 2099, Loss: 0.36397209763526917, Final Batch Loss: 0.18483980000019073\n",
      "Epoch 2100, Loss: 0.4003284275531769, Final Batch Loss: 0.17302335798740387\n",
      "Epoch 2101, Loss: 0.4305109828710556, Final Batch Loss: 0.24415969848632812\n",
      "Epoch 2102, Loss: 0.38152188062667847, Final Batch Loss: 0.21436645090579987\n",
      "Epoch 2103, Loss: 0.3426312208175659, Final Batch Loss: 0.15850085020065308\n",
      "Epoch 2104, Loss: 0.4256776124238968, Final Batch Loss: 0.21212506294250488\n",
      "Epoch 2105, Loss: 0.43354329466819763, Final Batch Loss: 0.23346516489982605\n",
      "Epoch 2106, Loss: 0.3511696606874466, Final Batch Loss: 0.14942631125450134\n",
      "Epoch 2107, Loss: 0.3989129066467285, Final Batch Loss: 0.19058343768119812\n",
      "Epoch 2108, Loss: 0.35165828466415405, Final Batch Loss: 0.17548996210098267\n",
      "Epoch 2109, Loss: 0.34544460475444794, Final Batch Loss: 0.14899475872516632\n",
      "Epoch 2110, Loss: 0.4957165867090225, Final Batch Loss: 0.2529650926589966\n",
      "Epoch 2111, Loss: 0.4193635433912277, Final Batch Loss: 0.20793600380420685\n",
      "Epoch 2112, Loss: 0.36508746445178986, Final Batch Loss: 0.2074914574623108\n",
      "Epoch 2113, Loss: 0.3755674213171005, Final Batch Loss: 0.18910491466522217\n",
      "Epoch 2114, Loss: 0.3644861876964569, Final Batch Loss: 0.2073640525341034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2115, Loss: 0.34546832740306854, Final Batch Loss: 0.1746976524591446\n",
      "Epoch 2116, Loss: 0.3682735115289688, Final Batch Loss: 0.21192820370197296\n",
      "Epoch 2117, Loss: 0.3178822696208954, Final Batch Loss: 0.150124192237854\n",
      "Epoch 2118, Loss: 0.417208656668663, Final Batch Loss: 0.18645505607128143\n",
      "Epoch 2119, Loss: 0.38008783757686615, Final Batch Loss: 0.1813795119524002\n",
      "Epoch 2120, Loss: 0.35259322822093964, Final Batch Loss: 0.13283853232860565\n",
      "Epoch 2121, Loss: 0.3615346848964691, Final Batch Loss: 0.15241989493370056\n",
      "Epoch 2122, Loss: 0.37904679775238037, Final Batch Loss: 0.1648927927017212\n",
      "Epoch 2123, Loss: 0.3753666430711746, Final Batch Loss: 0.16011986136436462\n",
      "Epoch 2124, Loss: 0.4134950339794159, Final Batch Loss: 0.22011883556842804\n",
      "Epoch 2125, Loss: 0.33683714270591736, Final Batch Loss: 0.1558377593755722\n",
      "Epoch 2126, Loss: 0.3552416265010834, Final Batch Loss: 0.1867561638355255\n",
      "Epoch 2127, Loss: 0.36500416696071625, Final Batch Loss: 0.13863180577754974\n",
      "Epoch 2128, Loss: 0.4710715264081955, Final Batch Loss: 0.28675225377082825\n",
      "Epoch 2129, Loss: 0.3402896374464035, Final Batch Loss: 0.16212010383605957\n",
      "Epoch 2130, Loss: 0.3993792235851288, Final Batch Loss: 0.2062472105026245\n",
      "Epoch 2131, Loss: 0.42085783183574677, Final Batch Loss: 0.21977919340133667\n",
      "Epoch 2132, Loss: 0.40861260890960693, Final Batch Loss: 0.21642380952835083\n",
      "Epoch 2133, Loss: 0.34968988597393036, Final Batch Loss: 0.17822183668613434\n",
      "Epoch 2134, Loss: 0.40632180869579315, Final Batch Loss: 0.21243830025196075\n",
      "Epoch 2135, Loss: 0.4567660242319107, Final Batch Loss: 0.2543645203113556\n",
      "Epoch 2136, Loss: 0.4153214395046234, Final Batch Loss: 0.22115439176559448\n",
      "Epoch 2137, Loss: 0.3606870472431183, Final Batch Loss: 0.19912217557430267\n",
      "Epoch 2138, Loss: 0.40018855035305023, Final Batch Loss: 0.1986120194196701\n",
      "Epoch 2139, Loss: 0.34283141791820526, Final Batch Loss: 0.17024986445903778\n",
      "Epoch 2140, Loss: 0.31896519660949707, Final Batch Loss: 0.14584150910377502\n",
      "Epoch 2141, Loss: 0.3871842920780182, Final Batch Loss: 0.2172119915485382\n",
      "Epoch 2142, Loss: 0.41561418771743774, Final Batch Loss: 0.23107291758060455\n",
      "Epoch 2143, Loss: 0.4154261648654938, Final Batch Loss: 0.22667892277240753\n",
      "Epoch 2144, Loss: 0.38142551481723785, Final Batch Loss: 0.19936007261276245\n",
      "Epoch 2145, Loss: 0.43263566493988037, Final Batch Loss: 0.23141612112522125\n",
      "Epoch 2146, Loss: 0.3889475166797638, Final Batch Loss: 0.18627527356147766\n",
      "Epoch 2147, Loss: 0.374451220035553, Final Batch Loss: 0.19770404696464539\n",
      "Epoch 2148, Loss: 0.3710803985595703, Final Batch Loss: 0.20859695971012115\n",
      "Epoch 2149, Loss: 0.3794090151786804, Final Batch Loss: 0.17136497795581818\n",
      "Epoch 2150, Loss: 0.404472216963768, Final Batch Loss: 0.1746608316898346\n",
      "Epoch 2151, Loss: 0.34652455151081085, Final Batch Loss: 0.18274961411952972\n",
      "Epoch 2152, Loss: 0.37909650802612305, Final Batch Loss: 0.1858958601951599\n",
      "Epoch 2153, Loss: 0.38493476808071136, Final Batch Loss: 0.18213875591754913\n",
      "Epoch 2154, Loss: 0.3248680755496025, Final Batch Loss: 0.12298081070184708\n",
      "Epoch 2155, Loss: 0.30629725009202957, Final Batch Loss: 0.12440521270036697\n",
      "Epoch 2156, Loss: 0.3333119601011276, Final Batch Loss: 0.20268060266971588\n",
      "Epoch 2157, Loss: 0.33731377124786377, Final Batch Loss: 0.16763614118099213\n",
      "Epoch 2158, Loss: 0.34972991049289703, Final Batch Loss: 0.17869356274604797\n",
      "Epoch 2159, Loss: 0.3483366519212723, Final Batch Loss: 0.16517625749111176\n",
      "Epoch 2160, Loss: 0.3446490317583084, Final Batch Loss: 0.17698362469673157\n",
      "Epoch 2161, Loss: 0.40988853573799133, Final Batch Loss: 0.1795153170824051\n",
      "Epoch 2162, Loss: 0.34897367656230927, Final Batch Loss: 0.1863584965467453\n",
      "Epoch 2163, Loss: 0.3763354569673538, Final Batch Loss: 0.20383994281291962\n",
      "Epoch 2164, Loss: 0.35907456278800964, Final Batch Loss: 0.16538256406784058\n",
      "Epoch 2165, Loss: 0.400505930185318, Final Batch Loss: 0.17513789236545563\n",
      "Epoch 2166, Loss: 0.3380806893110275, Final Batch Loss: 0.14788705110549927\n",
      "Epoch 2167, Loss: 0.39398662745952606, Final Batch Loss: 0.19108547270298004\n",
      "Epoch 2168, Loss: 0.3819933384656906, Final Batch Loss: 0.17696325480937958\n",
      "Epoch 2169, Loss: 0.4064316004514694, Final Batch Loss: 0.23756511509418488\n",
      "Epoch 2170, Loss: 0.3294164091348648, Final Batch Loss: 0.1412029266357422\n",
      "Epoch 2171, Loss: 0.40249012410640717, Final Batch Loss: 0.2339717298746109\n",
      "Epoch 2172, Loss: 0.3827672153711319, Final Batch Loss: 0.211237370967865\n",
      "Epoch 2173, Loss: 0.32690131664276123, Final Batch Loss: 0.1520976424217224\n",
      "Epoch 2174, Loss: 0.3330300748348236, Final Batch Loss: 0.11887505650520325\n",
      "Epoch 2175, Loss: 0.27970993518829346, Final Batch Loss: 0.12886056303977966\n",
      "Epoch 2176, Loss: 0.36119766533374786, Final Batch Loss: 0.15866321325302124\n",
      "Epoch 2177, Loss: 0.3223441392183304, Final Batch Loss: 0.18137970566749573\n",
      "Epoch 2178, Loss: 0.35091640055179596, Final Batch Loss: 0.1960357129573822\n",
      "Epoch 2179, Loss: 0.37072232365608215, Final Batch Loss: 0.20582398772239685\n",
      "Epoch 2180, Loss: 0.39352335035800934, Final Batch Loss: 0.18950903415679932\n",
      "Epoch 2181, Loss: 0.35618749260902405, Final Batch Loss: 0.16714707016944885\n",
      "Epoch 2182, Loss: 0.37881776690483093, Final Batch Loss: 0.21163015067577362\n",
      "Epoch 2183, Loss: 0.37759408354759216, Final Batch Loss: 0.23333010077476501\n",
      "Epoch 2184, Loss: 0.3468984216451645, Final Batch Loss: 0.15386876463890076\n",
      "Epoch 2185, Loss: 0.4273248165845871, Final Batch Loss: 0.2333345115184784\n",
      "Epoch 2186, Loss: 0.3925405591726303, Final Batch Loss: 0.21220889687538147\n",
      "Epoch 2187, Loss: 0.39763665199279785, Final Batch Loss: 0.16522465646266937\n",
      "Epoch 2188, Loss: 0.36528994143009186, Final Batch Loss: 0.16685886681079865\n",
      "Epoch 2189, Loss: 0.3329082429409027, Final Batch Loss: 0.14172467589378357\n",
      "Epoch 2190, Loss: 0.3488786369562149, Final Batch Loss: 0.17161178588867188\n",
      "Epoch 2191, Loss: 0.3446297347545624, Final Batch Loss: 0.14354625344276428\n",
      "Epoch 2192, Loss: 0.37598051130771637, Final Batch Loss: 0.20328393578529358\n",
      "Epoch 2193, Loss: 0.3089315891265869, Final Batch Loss: 0.15151403844356537\n",
      "Epoch 2194, Loss: 0.3861078917980194, Final Batch Loss: 0.19315914809703827\n",
      "Epoch 2195, Loss: 0.4026763439178467, Final Batch Loss: 0.200734943151474\n",
      "Epoch 2196, Loss: 0.338180273771286, Final Batch Loss: 0.16706538200378418\n",
      "Epoch 2197, Loss: 0.4088424891233444, Final Batch Loss: 0.23521560430526733\n",
      "Epoch 2198, Loss: 0.34267130494117737, Final Batch Loss: 0.154264897108078\n",
      "Epoch 2199, Loss: 0.4616343677043915, Final Batch Loss: 0.2511913478374481\n",
      "Epoch 2200, Loss: 0.36263394355773926, Final Batch Loss: 0.18287807703018188\n",
      "Epoch 2201, Loss: 0.3054615706205368, Final Batch Loss: 0.16001221537590027\n",
      "Epoch 2202, Loss: 0.3516159653663635, Final Batch Loss: 0.16577240824699402\n",
      "Epoch 2203, Loss: 0.36241884529590607, Final Batch Loss: 0.18615703284740448\n",
      "Epoch 2204, Loss: 0.35288964211940765, Final Batch Loss: 0.14929692447185516\n",
      "Epoch 2205, Loss: 0.2975957691669464, Final Batch Loss: 0.11728650331497192\n",
      "Epoch 2206, Loss: 0.34488222002983093, Final Batch Loss: 0.15341360867023468\n",
      "Epoch 2207, Loss: 0.35276034474372864, Final Batch Loss: 0.1784895807504654\n",
      "Epoch 2208, Loss: 0.30768074095249176, Final Batch Loss: 0.16077964007854462\n",
      "Epoch 2209, Loss: 0.3493472933769226, Final Batch Loss: 0.1619177907705307\n",
      "Epoch 2210, Loss: 0.3391711860895157, Final Batch Loss: 0.1333467811346054\n",
      "Epoch 2211, Loss: 0.3739859461784363, Final Batch Loss: 0.180418461561203\n",
      "Epoch 2212, Loss: 0.31156928837299347, Final Batch Loss: 0.15400397777557373\n",
      "Epoch 2213, Loss: 0.38035620748996735, Final Batch Loss: 0.2532437741756439\n",
      "Epoch 2214, Loss: 0.30061371624469757, Final Batch Loss: 0.15426574647426605\n",
      "Epoch 2215, Loss: 0.3942525237798691, Final Batch Loss: 0.20238526165485382\n",
      "Epoch 2216, Loss: 0.3361232429742813, Final Batch Loss: 0.15251357853412628\n",
      "Epoch 2217, Loss: 0.3439992070198059, Final Batch Loss: 0.17943626642227173\n",
      "Epoch 2218, Loss: 0.3640970438718796, Final Batch Loss: 0.21026603877544403\n",
      "Epoch 2219, Loss: 0.29310932755470276, Final Batch Loss: 0.12710902094841003\n",
      "Epoch 2220, Loss: 0.428414449095726, Final Batch Loss: 0.26127201318740845\n",
      "Epoch 2221, Loss: 0.34509360790252686, Final Batch Loss: 0.18237581849098206\n",
      "Epoch 2222, Loss: 0.35126736760139465, Final Batch Loss: 0.22528615593910217\n",
      "Epoch 2223, Loss: 0.39683930575847626, Final Batch Loss: 0.2096274197101593\n",
      "Epoch 2224, Loss: 0.3274141997098923, Final Batch Loss: 0.17322590947151184\n",
      "Epoch 2225, Loss: 0.36135725677013397, Final Batch Loss: 0.16938795149326324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2226, Loss: 0.3129587769508362, Final Batch Loss: 0.1523316204547882\n",
      "Epoch 2227, Loss: 0.35989831387996674, Final Batch Loss: 0.16822263598442078\n",
      "Epoch 2228, Loss: 0.3204917460680008, Final Batch Loss: 0.17186535894870758\n",
      "Epoch 2229, Loss: 0.35743293166160583, Final Batch Loss: 0.19244584441184998\n",
      "Epoch 2230, Loss: 0.35055507719516754, Final Batch Loss: 0.16289636492729187\n",
      "Epoch 2231, Loss: 0.35468004643917084, Final Batch Loss: 0.21769902110099792\n",
      "Epoch 2232, Loss: 0.3627757132053375, Final Batch Loss: 0.21030071377754211\n",
      "Epoch 2233, Loss: 0.3139268755912781, Final Batch Loss: 0.13180693984031677\n",
      "Epoch 2234, Loss: 0.3656272888183594, Final Batch Loss: 0.18590354919433594\n",
      "Epoch 2235, Loss: 0.30975379049777985, Final Batch Loss: 0.16897088289260864\n",
      "Epoch 2236, Loss: 0.37853536009788513, Final Batch Loss: 0.2027854025363922\n",
      "Epoch 2237, Loss: 0.3950660973787308, Final Batch Loss: 0.18837477266788483\n",
      "Epoch 2238, Loss: 0.29848383367061615, Final Batch Loss: 0.15390010178089142\n",
      "Epoch 2239, Loss: 0.30094973742961884, Final Batch Loss: 0.14198973774909973\n",
      "Epoch 2240, Loss: 0.30822721123695374, Final Batch Loss: 0.1493787318468094\n",
      "Epoch 2241, Loss: 0.33728884160518646, Final Batch Loss: 0.15939758718013763\n",
      "Epoch 2242, Loss: 0.36907055974006653, Final Batch Loss: 0.20043252408504486\n",
      "Epoch 2243, Loss: 0.38614891469478607, Final Batch Loss: 0.16022412478923798\n",
      "Epoch 2244, Loss: 0.41107846796512604, Final Batch Loss: 0.19843296706676483\n",
      "Epoch 2245, Loss: 0.3574102073907852, Final Batch Loss: 0.16672931611537933\n",
      "Epoch 2246, Loss: 0.3812045156955719, Final Batch Loss: 0.16487134993076324\n",
      "Epoch 2247, Loss: 0.3606523871421814, Final Batch Loss: 0.17447328567504883\n",
      "Epoch 2248, Loss: 0.37973572313785553, Final Batch Loss: 0.1979382336139679\n",
      "Epoch 2249, Loss: 0.33211609721183777, Final Batch Loss: 0.18135382235050201\n",
      "Epoch 2250, Loss: 0.3388528972864151, Final Batch Loss: 0.17205528914928436\n",
      "Epoch 2251, Loss: 0.41273364424705505, Final Batch Loss: 0.22375404834747314\n",
      "Epoch 2252, Loss: 0.3682970106601715, Final Batch Loss: 0.17345185577869415\n",
      "Epoch 2253, Loss: 0.3208491876721382, Final Batch Loss: 0.10709477216005325\n",
      "Epoch 2254, Loss: 0.3293500691652298, Final Batch Loss: 0.17574554681777954\n",
      "Epoch 2255, Loss: 0.33264918625354767, Final Batch Loss: 0.19811700284481049\n",
      "Epoch 2256, Loss: 0.3338574320077896, Final Batch Loss: 0.17087715864181519\n",
      "Epoch 2257, Loss: 0.3303845375776291, Final Batch Loss: 0.15189383924007416\n",
      "Epoch 2258, Loss: 0.3465391993522644, Final Batch Loss: 0.17268815636634827\n",
      "Epoch 2259, Loss: 0.286051370203495, Final Batch Loss: 0.11067806929349899\n",
      "Epoch 2260, Loss: 0.431038573384285, Final Batch Loss: 0.25694525241851807\n",
      "Epoch 2261, Loss: 0.3191642463207245, Final Batch Loss: 0.1539992243051529\n",
      "Epoch 2262, Loss: 0.36553311347961426, Final Batch Loss: 0.14887292683124542\n",
      "Epoch 2263, Loss: 0.35139988362789154, Final Batch Loss: 0.16114072501659393\n",
      "Epoch 2264, Loss: 0.33837325870990753, Final Batch Loss: 0.1730644702911377\n",
      "Epoch 2265, Loss: 0.3901626765727997, Final Batch Loss: 0.20892487466335297\n",
      "Epoch 2266, Loss: 0.31733423471450806, Final Batch Loss: 0.1921497881412506\n",
      "Epoch 2267, Loss: 0.3534296005964279, Final Batch Loss: 0.15116003155708313\n",
      "Epoch 2268, Loss: 0.39280594885349274, Final Batch Loss: 0.21343493461608887\n",
      "Epoch 2269, Loss: 0.33625127375125885, Final Batch Loss: 0.15518899261951447\n",
      "Epoch 2270, Loss: 0.36520279943943024, Final Batch Loss: 0.20153747498989105\n",
      "Epoch 2271, Loss: 0.32676808536052704, Final Batch Loss: 0.14128804206848145\n",
      "Epoch 2272, Loss: 0.3018016517162323, Final Batch Loss: 0.14433588087558746\n",
      "Epoch 2273, Loss: 0.35992997884750366, Final Batch Loss: 0.20605026185512543\n",
      "Epoch 2274, Loss: 0.3351036459207535, Final Batch Loss: 0.185003399848938\n",
      "Epoch 2275, Loss: 0.3340704143047333, Final Batch Loss: 0.15751463174819946\n",
      "Epoch 2276, Loss: 0.35657593607902527, Final Batch Loss: 0.1574433594942093\n",
      "Epoch 2277, Loss: 0.3540949672460556, Final Batch Loss: 0.15797972679138184\n",
      "Epoch 2278, Loss: 0.3720037341117859, Final Batch Loss: 0.21213114261627197\n",
      "Epoch 2279, Loss: 0.3908311128616333, Final Batch Loss: 0.22232025861740112\n",
      "Epoch 2280, Loss: 0.38852500915527344, Final Batch Loss: 0.2079942524433136\n",
      "Epoch 2281, Loss: 0.3577848821878433, Final Batch Loss: 0.17625968158245087\n",
      "Epoch 2282, Loss: 0.3767053782939911, Final Batch Loss: 0.21902033686637878\n",
      "Epoch 2283, Loss: 0.3525784760713577, Final Batch Loss: 0.18579882383346558\n",
      "Epoch 2284, Loss: 0.35693512856960297, Final Batch Loss: 0.18528583645820618\n",
      "Epoch 2285, Loss: 0.3260866552591324, Final Batch Loss: 0.13723771274089813\n",
      "Epoch 2286, Loss: 0.36819344758987427, Final Batch Loss: 0.19885669648647308\n",
      "Epoch 2287, Loss: 0.3540327847003937, Final Batch Loss: 0.1725653111934662\n",
      "Epoch 2288, Loss: 0.4198951870203018, Final Batch Loss: 0.24116557836532593\n",
      "Epoch 2289, Loss: 0.3520199954509735, Final Batch Loss: 0.14863941073417664\n",
      "Epoch 2290, Loss: 0.34287025034427643, Final Batch Loss: 0.1820221245288849\n",
      "Epoch 2291, Loss: 0.4014677405357361, Final Batch Loss: 0.18966475129127502\n",
      "Epoch 2292, Loss: 0.29518669843673706, Final Batch Loss: 0.15531928837299347\n",
      "Epoch 2293, Loss: 0.42155057191848755, Final Batch Loss: 0.24522435665130615\n",
      "Epoch 2294, Loss: 0.40987709164619446, Final Batch Loss: 0.23409004509449005\n",
      "Epoch 2295, Loss: 0.2864014282822609, Final Batch Loss: 0.16776588559150696\n",
      "Epoch 2296, Loss: 0.3209596872329712, Final Batch Loss: 0.1405251920223236\n",
      "Epoch 2297, Loss: 0.41028551757335663, Final Batch Loss: 0.22662945091724396\n",
      "Epoch 2298, Loss: 0.3520089089870453, Final Batch Loss: 0.17753802239894867\n",
      "Epoch 2299, Loss: 0.3373762220144272, Final Batch Loss: 0.15733081102371216\n",
      "Epoch 2300, Loss: 0.33721211552619934, Final Batch Loss: 0.17621856927871704\n",
      "Epoch 2301, Loss: 0.35967476665973663, Final Batch Loss: 0.1663590520620346\n",
      "Epoch 2302, Loss: 0.32546553015708923, Final Batch Loss: 0.1575675755739212\n",
      "Epoch 2303, Loss: 0.39537306129932404, Final Batch Loss: 0.18084044754505157\n",
      "Epoch 2304, Loss: 0.32832297682762146, Final Batch Loss: 0.11836767196655273\n",
      "Epoch 2305, Loss: 0.3152731955051422, Final Batch Loss: 0.13254414498806\n",
      "Epoch 2306, Loss: 0.3531770706176758, Final Batch Loss: 0.18689857423305511\n",
      "Epoch 2307, Loss: 0.3860449194908142, Final Batch Loss: 0.22554151713848114\n",
      "Epoch 2308, Loss: 0.338994562625885, Final Batch Loss: 0.1908758282661438\n",
      "Epoch 2309, Loss: 0.32744112610816956, Final Batch Loss: 0.15732276439666748\n",
      "Epoch 2310, Loss: 0.3895784467458725, Final Batch Loss: 0.20965318381786346\n",
      "Epoch 2311, Loss: 0.304980143904686, Final Batch Loss: 0.1738405078649521\n",
      "Epoch 2312, Loss: 0.4017603397369385, Final Batch Loss: 0.2034890353679657\n",
      "Epoch 2313, Loss: 0.36249101161956787, Final Batch Loss: 0.20524050295352936\n",
      "Epoch 2314, Loss: 0.33824682235717773, Final Batch Loss: 0.20138655602931976\n",
      "Epoch 2315, Loss: 0.30312351882457733, Final Batch Loss: 0.18383502960205078\n",
      "Epoch 2316, Loss: 0.302746519446373, Final Batch Loss: 0.13099509477615356\n",
      "Epoch 2317, Loss: 0.34827131032943726, Final Batch Loss: 0.1573147028684616\n",
      "Epoch 2318, Loss: 0.3309180587530136, Final Batch Loss: 0.17685829102993011\n",
      "Epoch 2319, Loss: 0.39638732373714447, Final Batch Loss: 0.22229160368442535\n",
      "Epoch 2320, Loss: 0.34415893256664276, Final Batch Loss: 0.18722018599510193\n",
      "Epoch 2321, Loss: 0.38651469349861145, Final Batch Loss: 0.20302645862102509\n",
      "Epoch 2322, Loss: 0.3296211510896683, Final Batch Loss: 0.13857999444007874\n",
      "Epoch 2323, Loss: 0.3198123872280121, Final Batch Loss: 0.1582794487476349\n",
      "Epoch 2324, Loss: 0.3658437579870224, Final Batch Loss: 0.16869692504405975\n",
      "Epoch 2325, Loss: 0.3183014690876007, Final Batch Loss: 0.1583736687898636\n",
      "Epoch 2326, Loss: 0.37274326384067535, Final Batch Loss: 0.18154726922512054\n",
      "Epoch 2327, Loss: 0.3033202141523361, Final Batch Loss: 0.16748066246509552\n",
      "Epoch 2328, Loss: 0.3706431984901428, Final Batch Loss: 0.2242426872253418\n",
      "Epoch 2329, Loss: 0.3568818122148514, Final Batch Loss: 0.20474602282047272\n",
      "Epoch 2330, Loss: 0.33290086686611176, Final Batch Loss: 0.12612146139144897\n",
      "Epoch 2331, Loss: 0.2929454669356346, Final Batch Loss: 0.11738280206918716\n",
      "Epoch 2332, Loss: 0.29056988656520844, Final Batch Loss: 0.15470263361930847\n",
      "Epoch 2333, Loss: 0.3291674852371216, Final Batch Loss: 0.14597183465957642\n",
      "Epoch 2334, Loss: 0.3766046017408371, Final Batch Loss: 0.21236936748027802\n",
      "Epoch 2335, Loss: 0.33017808198928833, Final Batch Loss: 0.19340063631534576\n",
      "Epoch 2336, Loss: 0.34304235875606537, Final Batch Loss: 0.17045608162879944\n",
      "Epoch 2337, Loss: 0.37796543538570404, Final Batch Loss: 0.21355277299880981\n",
      "Epoch 2338, Loss: 0.3291846215724945, Final Batch Loss: 0.1623755544424057\n",
      "Epoch 2339, Loss: 0.3131183087825775, Final Batch Loss: 0.1891242265701294\n",
      "Epoch 2340, Loss: 0.38753707706928253, Final Batch Loss: 0.1853640079498291\n",
      "Epoch 2341, Loss: 0.3809758126735687, Final Batch Loss: 0.1982819139957428\n",
      "Epoch 2342, Loss: 0.3383655548095703, Final Batch Loss: 0.17311245203018188\n",
      "Epoch 2343, Loss: 0.3564598858356476, Final Batch Loss: 0.17640283703804016\n",
      "Epoch 2344, Loss: 0.33360153436660767, Final Batch Loss: 0.1480468362569809\n",
      "Epoch 2345, Loss: 0.3581854850053787, Final Batch Loss: 0.15704332292079926\n",
      "Epoch 2346, Loss: 0.3378092497587204, Final Batch Loss: 0.1808074563741684\n",
      "Epoch 2347, Loss: 0.32200999557971954, Final Batch Loss: 0.15652306377887726\n",
      "Epoch 2348, Loss: 0.3821685016155243, Final Batch Loss: 0.16781671345233917\n",
      "Epoch 2349, Loss: 0.3254908174276352, Final Batch Loss: 0.15012747049331665\n",
      "Epoch 2350, Loss: 0.318225160241127, Final Batch Loss: 0.13732174038887024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2351, Loss: 0.30321966111660004, Final Batch Loss: 0.122315913438797\n",
      "Epoch 2352, Loss: 0.42394042015075684, Final Batch Loss: 0.18137791752815247\n",
      "Epoch 2353, Loss: 0.33982913196086884, Final Batch Loss: 0.19583413004875183\n",
      "Epoch 2354, Loss: 0.39147157967090607, Final Batch Loss: 0.18010424077510834\n",
      "Epoch 2355, Loss: 0.3804965019226074, Final Batch Loss: 0.20645228028297424\n",
      "Epoch 2356, Loss: 0.4306980222463608, Final Batch Loss: 0.2622014582157135\n",
      "Epoch 2357, Loss: 0.32930439710617065, Final Batch Loss: 0.18010182678699493\n",
      "Epoch 2358, Loss: 0.3345777839422226, Final Batch Loss: 0.14680767059326172\n",
      "Epoch 2359, Loss: 0.3719368875026703, Final Batch Loss: 0.17952319979667664\n",
      "Epoch 2360, Loss: 0.3485509753227234, Final Batch Loss: 0.15954262018203735\n",
      "Epoch 2361, Loss: 0.30375543236732483, Final Batch Loss: 0.1374836564064026\n",
      "Epoch 2362, Loss: 0.3248097896575928, Final Batch Loss: 0.1708579957485199\n",
      "Epoch 2363, Loss: 0.32489730417728424, Final Batch Loss: 0.15101005136966705\n",
      "Epoch 2364, Loss: 0.36992159485816956, Final Batch Loss: 0.24082216620445251\n",
      "Epoch 2365, Loss: 0.31394799053668976, Final Batch Loss: 0.13882142305374146\n",
      "Epoch 2366, Loss: 0.3781619220972061, Final Batch Loss: 0.14529158174991608\n",
      "Epoch 2367, Loss: 0.3220919519662857, Final Batch Loss: 0.15609659254550934\n",
      "Epoch 2368, Loss: 0.33666960895061493, Final Batch Loss: 0.16465327143669128\n",
      "Epoch 2369, Loss: 0.3391139805316925, Final Batch Loss: 0.15577852725982666\n",
      "Epoch 2370, Loss: 0.3307902365922928, Final Batch Loss: 0.16416990756988525\n",
      "Epoch 2371, Loss: 0.2881001904606819, Final Batch Loss: 0.10927633196115494\n",
      "Epoch 2372, Loss: 0.3215917944908142, Final Batch Loss: 0.16307894885540009\n",
      "Epoch 2373, Loss: 0.35266102850437164, Final Batch Loss: 0.22640766203403473\n",
      "Epoch 2374, Loss: 0.38718724250793457, Final Batch Loss: 0.18692241609096527\n",
      "Epoch 2375, Loss: 0.3403429090976715, Final Batch Loss: 0.18375441431999207\n",
      "Epoch 2376, Loss: 0.30807603895664215, Final Batch Loss: 0.13948188722133636\n",
      "Epoch 2377, Loss: 0.2968410849571228, Final Batch Loss: 0.13954831659793854\n",
      "Epoch 2378, Loss: 0.3809830993413925, Final Batch Loss: 0.1872643232345581\n",
      "Epoch 2379, Loss: 0.28593724966049194, Final Batch Loss: 0.1372309923171997\n",
      "Epoch 2380, Loss: 0.3273318111896515, Final Batch Loss: 0.13811025023460388\n",
      "Epoch 2381, Loss: 0.3176036924123764, Final Batch Loss: 0.15122359991073608\n",
      "Epoch 2382, Loss: 0.3189544975757599, Final Batch Loss: 0.18404610455036163\n",
      "Epoch 2383, Loss: 0.30224286019802094, Final Batch Loss: 0.14482945203781128\n",
      "Epoch 2384, Loss: 0.32310332357883453, Final Batch Loss: 0.15545475482940674\n",
      "Epoch 2385, Loss: 0.36828331649303436, Final Batch Loss: 0.20463863015174866\n",
      "Epoch 2386, Loss: 0.32768939435482025, Final Batch Loss: 0.14579564332962036\n",
      "Epoch 2387, Loss: 0.38181400299072266, Final Batch Loss: 0.23463930189609528\n",
      "Epoch 2388, Loss: 0.3181678503751755, Final Batch Loss: 0.17438799142837524\n",
      "Epoch 2389, Loss: 0.32820700109004974, Final Batch Loss: 0.19614984095096588\n",
      "Epoch 2390, Loss: 0.3710886687040329, Final Batch Loss: 0.21010440587997437\n",
      "Epoch 2391, Loss: 0.3202081322669983, Final Batch Loss: 0.16423210501670837\n",
      "Epoch 2392, Loss: 0.28995485603809357, Final Batch Loss: 0.12850554287433624\n",
      "Epoch 2393, Loss: 0.3361152857542038, Final Batch Loss: 0.13187921047210693\n",
      "Epoch 2394, Loss: 0.342259019613266, Final Batch Loss: 0.22629588842391968\n",
      "Epoch 2395, Loss: 0.343829482793808, Final Batch Loss: 0.14771582186222076\n",
      "Epoch 2396, Loss: 0.3032810091972351, Final Batch Loss: 0.17242231965065002\n",
      "Epoch 2397, Loss: 0.37970076501369476, Final Batch Loss: 0.1982056349515915\n",
      "Epoch 2398, Loss: 0.35817617177963257, Final Batch Loss: 0.17223265767097473\n",
      "Epoch 2399, Loss: 0.27905991673469543, Final Batch Loss: 0.09611029922962189\n",
      "Epoch 2400, Loss: 0.3414367139339447, Final Batch Loss: 0.2040252387523651\n",
      "Epoch 2401, Loss: 0.40587377548217773, Final Batch Loss: 0.24986036121845245\n",
      "Epoch 2402, Loss: 0.36876678466796875, Final Batch Loss: 0.20005428791046143\n",
      "Epoch 2403, Loss: 0.38488373160362244, Final Batch Loss: 0.22492165863513947\n",
      "Epoch 2404, Loss: 0.31350667774677277, Final Batch Loss: 0.1505647599697113\n",
      "Epoch 2405, Loss: 0.38276419043540955, Final Batch Loss: 0.23193536698818207\n",
      "Epoch 2406, Loss: 0.3023684173822403, Final Batch Loss: 0.09239891171455383\n",
      "Epoch 2407, Loss: 0.34631288051605225, Final Batch Loss: 0.18596482276916504\n",
      "Epoch 2408, Loss: 0.35319964587688446, Final Batch Loss: 0.13471397757530212\n",
      "Epoch 2409, Loss: 0.2835671827197075, Final Batch Loss: 0.11790204793214798\n",
      "Epoch 2410, Loss: 0.3468702584505081, Final Batch Loss: 0.14841558039188385\n",
      "Epoch 2411, Loss: 0.35636360943317413, Final Batch Loss: 0.17805004119873047\n",
      "Epoch 2412, Loss: 0.3457684963941574, Final Batch Loss: 0.1357262134552002\n",
      "Epoch 2413, Loss: 0.3598450571298599, Final Batch Loss: 0.18782144784927368\n",
      "Epoch 2414, Loss: 0.3195715546607971, Final Batch Loss: 0.16782835125923157\n",
      "Epoch 2415, Loss: 0.3516882061958313, Final Batch Loss: 0.15484066307544708\n",
      "Epoch 2416, Loss: 0.31991980969905853, Final Batch Loss: 0.14385488629341125\n",
      "Epoch 2417, Loss: 0.33929023146629333, Final Batch Loss: 0.15236970782279968\n",
      "Epoch 2418, Loss: 0.387349933385849, Final Batch Loss: 0.19635966420173645\n",
      "Epoch 2419, Loss: 0.30971474945545197, Final Batch Loss: 0.16969668865203857\n",
      "Epoch 2420, Loss: 0.32037530839443207, Final Batch Loss: 0.14873820543289185\n",
      "Epoch 2421, Loss: 0.35678422451019287, Final Batch Loss: 0.16574421525001526\n",
      "Epoch 2422, Loss: 0.33415284752845764, Final Batch Loss: 0.16099722683429718\n",
      "Epoch 2423, Loss: 0.34037573635578156, Final Batch Loss: 0.16502784192562103\n",
      "Epoch 2424, Loss: 0.3658003956079483, Final Batch Loss: 0.18344196677207947\n",
      "Epoch 2425, Loss: 0.300165057182312, Final Batch Loss: 0.13951696455478668\n",
      "Epoch 2426, Loss: 0.33782702684402466, Final Batch Loss: 0.17645195126533508\n",
      "Epoch 2427, Loss: 0.3259638547897339, Final Batch Loss: 0.16954109072685242\n",
      "Epoch 2428, Loss: 0.2975897267460823, Final Batch Loss: 0.12066700309515\n",
      "Epoch 2429, Loss: 0.3045419007539749, Final Batch Loss: 0.15372025966644287\n",
      "Epoch 2430, Loss: 0.32489238679409027, Final Batch Loss: 0.15057553350925446\n",
      "Epoch 2431, Loss: 0.3338536471128464, Final Batch Loss: 0.1480274200439453\n",
      "Epoch 2432, Loss: 0.36155618727207184, Final Batch Loss: 0.20150649547576904\n",
      "Epoch 2433, Loss: 0.39320456981658936, Final Batch Loss: 0.18392963707447052\n",
      "Epoch 2434, Loss: 0.32545194029808044, Final Batch Loss: 0.161781907081604\n",
      "Epoch 2435, Loss: 0.33409029245376587, Final Batch Loss: 0.1688912808895111\n",
      "Epoch 2436, Loss: 0.35893233120441437, Final Batch Loss: 0.16199490427970886\n",
      "Epoch 2437, Loss: 0.41062217950820923, Final Batch Loss: 0.23825833201408386\n",
      "Epoch 2438, Loss: 0.2924594581127167, Final Batch Loss: 0.1211315244436264\n",
      "Epoch 2439, Loss: 0.34438206255435944, Final Batch Loss: 0.16220566630363464\n",
      "Epoch 2440, Loss: 0.3150670677423477, Final Batch Loss: 0.14935587346553802\n",
      "Epoch 2441, Loss: 0.34506379067897797, Final Batch Loss: 0.19260162115097046\n",
      "Epoch 2442, Loss: 0.37953153252601624, Final Batch Loss: 0.16512860357761383\n",
      "Epoch 2443, Loss: 0.3185913413763046, Final Batch Loss: 0.1408294439315796\n",
      "Epoch 2444, Loss: 0.4037167727947235, Final Batch Loss: 0.22039656341075897\n",
      "Epoch 2445, Loss: 0.3473306745290756, Final Batch Loss: 0.13934770226478577\n",
      "Epoch 2446, Loss: 0.37308134138584137, Final Batch Loss: 0.20800651609897614\n",
      "Epoch 2447, Loss: 0.3154413104057312, Final Batch Loss: 0.12442615628242493\n",
      "Epoch 2448, Loss: 0.3682964891195297, Final Batch Loss: 0.2301250547170639\n",
      "Epoch 2449, Loss: 0.3141692578792572, Final Batch Loss: 0.18450190126895905\n",
      "Epoch 2450, Loss: 0.34213609993457794, Final Batch Loss: 0.15732493996620178\n",
      "Epoch 2451, Loss: 0.2766406685113907, Final Batch Loss: 0.13691754639148712\n",
      "Epoch 2452, Loss: 0.2902118116617203, Final Batch Loss: 0.13861492276191711\n",
      "Epoch 2453, Loss: 0.3270544409751892, Final Batch Loss: 0.15485964715480804\n",
      "Epoch 2454, Loss: 0.33361251652240753, Final Batch Loss: 0.17756545543670654\n",
      "Epoch 2455, Loss: 0.31908726692199707, Final Batch Loss: 0.184475839138031\n",
      "Epoch 2456, Loss: 0.3359873443841934, Final Batch Loss: 0.19962374866008759\n",
      "Epoch 2457, Loss: 0.3086085468530655, Final Batch Loss: 0.17120368778705597\n",
      "Epoch 2458, Loss: 0.35613854229450226, Final Batch Loss: 0.15623143315315247\n",
      "Epoch 2459, Loss: 0.33614037930965424, Final Batch Loss: 0.13653068244457245\n",
      "Epoch 2460, Loss: 0.3904039263725281, Final Batch Loss: 0.24335864186286926\n",
      "Epoch 2461, Loss: 0.3053559735417366, Final Batch Loss: 0.11728546768426895\n",
      "Epoch 2462, Loss: 0.35359613597393036, Final Batch Loss: 0.1677151918411255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2463, Loss: 0.27137425541877747, Final Batch Loss: 0.1440722644329071\n",
      "Epoch 2464, Loss: 0.32025623321533203, Final Batch Loss: 0.18804505467414856\n",
      "Epoch 2465, Loss: 0.313435897231102, Final Batch Loss: 0.16560892760753632\n",
      "Epoch 2466, Loss: 0.31536149978637695, Final Batch Loss: 0.16929438710212708\n",
      "Epoch 2467, Loss: 0.31002502143383026, Final Batch Loss: 0.17599549889564514\n",
      "Epoch 2468, Loss: 0.3268507272005081, Final Batch Loss: 0.17057408392429352\n",
      "Epoch 2469, Loss: 0.3159407675266266, Final Batch Loss: 0.1311635971069336\n",
      "Epoch 2470, Loss: 0.3298506587743759, Final Batch Loss: 0.1693846732378006\n",
      "Epoch 2471, Loss: 0.2619054317474365, Final Batch Loss: 0.147894024848938\n",
      "Epoch 2472, Loss: 0.24860607087612152, Final Batch Loss: 0.13303761184215546\n",
      "Epoch 2473, Loss: 0.2940843254327774, Final Batch Loss: 0.15897014737129211\n",
      "Epoch 2474, Loss: 0.3991139680147171, Final Batch Loss: 0.210858553647995\n",
      "Epoch 2475, Loss: 0.2969474047422409, Final Batch Loss: 0.1428428441286087\n",
      "Epoch 2476, Loss: 0.2984764352440834, Final Batch Loss: 0.1741946041584015\n",
      "Epoch 2477, Loss: 0.3008077144622803, Final Batch Loss: 0.14832231402397156\n",
      "Epoch 2478, Loss: 0.26221420615911484, Final Batch Loss: 0.11050065606832504\n",
      "Epoch 2479, Loss: 0.2911839634180069, Final Batch Loss: 0.13692054152488708\n",
      "Epoch 2480, Loss: 0.3252936899662018, Final Batch Loss: 0.14569173753261566\n",
      "Epoch 2481, Loss: 0.24083025753498077, Final Batch Loss: 0.07547944784164429\n",
      "Epoch 2482, Loss: 0.30613332986831665, Final Batch Loss: 0.14744463562965393\n",
      "Epoch 2483, Loss: 0.3549906015396118, Final Batch Loss: 0.15449939668178558\n",
      "Epoch 2484, Loss: 0.3164054602384567, Final Batch Loss: 0.19579698145389557\n",
      "Epoch 2485, Loss: 0.3593727648258209, Final Batch Loss: 0.18317897617816925\n",
      "Epoch 2486, Loss: 0.2697165757417679, Final Batch Loss: 0.10067306458950043\n",
      "Epoch 2487, Loss: 0.28932660073041916, Final Batch Loss: 0.1246802881360054\n",
      "Epoch 2488, Loss: 0.4036265015602112, Final Batch Loss: 0.22796042263507843\n",
      "Epoch 2489, Loss: 0.4473544657230377, Final Batch Loss: 0.22565266489982605\n",
      "Epoch 2490, Loss: 0.3243650794029236, Final Batch Loss: 0.14782197773456573\n",
      "Epoch 2491, Loss: 0.3407863527536392, Final Batch Loss: 0.1665751338005066\n",
      "Epoch 2492, Loss: 0.36042939126491547, Final Batch Loss: 0.17071498930454254\n",
      "Epoch 2493, Loss: 0.303112268447876, Final Batch Loss: 0.16130195558071136\n",
      "Epoch 2494, Loss: 0.36580778658390045, Final Batch Loss: 0.18564383685588837\n",
      "Epoch 2495, Loss: 0.290853351354599, Final Batch Loss: 0.13773201406002045\n",
      "Epoch 2496, Loss: 0.3371904045343399, Final Batch Loss: 0.15748687088489532\n",
      "Epoch 2497, Loss: 0.31816257536411285, Final Batch Loss: 0.14160673320293427\n",
      "Epoch 2498, Loss: 0.3484226018190384, Final Batch Loss: 0.12415054440498352\n",
      "Epoch 2499, Loss: 0.2972387969493866, Final Batch Loss: 0.16123373806476593\n",
      "Epoch 2500, Loss: 0.33014680445194244, Final Batch Loss: 0.1751106083393097\n",
      "Epoch 2501, Loss: 0.4160107225179672, Final Batch Loss: 0.15007390081882477\n",
      "Epoch 2502, Loss: 0.31675589084625244, Final Batch Loss: 0.15361109375953674\n",
      "Epoch 2503, Loss: 0.3274836540222168, Final Batch Loss: 0.19917307794094086\n",
      "Epoch 2504, Loss: 0.3505907505750656, Final Batch Loss: 0.18144507706165314\n",
      "Epoch 2505, Loss: 0.31707537174224854, Final Batch Loss: 0.14331649243831635\n",
      "Epoch 2506, Loss: 0.316612109541893, Final Batch Loss: 0.18682290613651276\n",
      "Epoch 2507, Loss: 0.30941803753376007, Final Batch Loss: 0.1467985361814499\n",
      "Epoch 2508, Loss: 0.35115738213062286, Final Batch Loss: 0.1886041909456253\n",
      "Epoch 2509, Loss: 0.3129049986600876, Final Batch Loss: 0.1427229940891266\n",
      "Epoch 2510, Loss: 0.3756007254123688, Final Batch Loss: 0.20566698908805847\n",
      "Epoch 2511, Loss: 0.3427804559469223, Final Batch Loss: 0.18575812876224518\n",
      "Epoch 2512, Loss: 0.3417866677045822, Final Batch Loss: 0.14143897593021393\n",
      "Epoch 2513, Loss: 0.2762830927968025, Final Batch Loss: 0.15806050598621368\n",
      "Epoch 2514, Loss: 0.3483542650938034, Final Batch Loss: 0.201452374458313\n",
      "Epoch 2515, Loss: 0.31623636186122894, Final Batch Loss: 0.1273653358221054\n",
      "Epoch 2516, Loss: 0.3930928260087967, Final Batch Loss: 0.21339881420135498\n",
      "Epoch 2517, Loss: 0.3439773619174957, Final Batch Loss: 0.1786159873008728\n",
      "Epoch 2518, Loss: 0.346971333026886, Final Batch Loss: 0.18707267940044403\n",
      "Epoch 2519, Loss: 0.33538229763507843, Final Batch Loss: 0.1601497083902359\n",
      "Epoch 2520, Loss: 0.32225334644317627, Final Batch Loss: 0.1837252974510193\n",
      "Epoch 2521, Loss: 0.2754790782928467, Final Batch Loss: 0.12805473804473877\n",
      "Epoch 2522, Loss: 0.29149821400642395, Final Batch Loss: 0.13229219615459442\n",
      "Epoch 2523, Loss: 0.37655025720596313, Final Batch Loss: 0.22209954261779785\n",
      "Epoch 2524, Loss: 0.3351393938064575, Final Batch Loss: 0.18356412649154663\n",
      "Epoch 2525, Loss: 0.30929046869277954, Final Batch Loss: 0.14301297068595886\n",
      "Epoch 2526, Loss: 0.4588896930217743, Final Batch Loss: 0.3137173652648926\n",
      "Epoch 2527, Loss: 0.3428311049938202, Final Batch Loss: 0.16565099358558655\n",
      "Epoch 2528, Loss: 0.34794218838214874, Final Batch Loss: 0.17785139381885529\n",
      "Epoch 2529, Loss: 0.2815328538417816, Final Batch Loss: 0.13299795985221863\n",
      "Epoch 2530, Loss: 0.3162659704685211, Final Batch Loss: 0.14575792849063873\n",
      "Epoch 2531, Loss: 0.3250897079706192, Final Batch Loss: 0.16815386712551117\n",
      "Epoch 2532, Loss: 0.38073496520519257, Final Batch Loss: 0.24080780148506165\n",
      "Epoch 2533, Loss: 0.34710070490837097, Final Batch Loss: 0.18898458778858185\n",
      "Epoch 2534, Loss: 0.3505249321460724, Final Batch Loss: 0.1498003453016281\n",
      "Epoch 2535, Loss: 0.3863827884197235, Final Batch Loss: 0.15197604894638062\n",
      "Epoch 2536, Loss: 0.33206750452518463, Final Batch Loss: 0.19174017012119293\n",
      "Epoch 2537, Loss: 0.3213900923728943, Final Batch Loss: 0.15209954977035522\n",
      "Epoch 2538, Loss: 0.3171224743127823, Final Batch Loss: 0.16040128469467163\n",
      "Epoch 2539, Loss: 0.3222882151603699, Final Batch Loss: 0.1382020264863968\n",
      "Epoch 2540, Loss: 0.2890756130218506, Final Batch Loss: 0.16128461062908173\n",
      "Epoch 2541, Loss: 0.2636830657720566, Final Batch Loss: 0.15220370888710022\n",
      "Epoch 2542, Loss: 0.3070392608642578, Final Batch Loss: 0.164724200963974\n",
      "Epoch 2543, Loss: 0.3261546790599823, Final Batch Loss: 0.1520380824804306\n",
      "Epoch 2544, Loss: 0.3479871302843094, Final Batch Loss: 0.2055661380290985\n",
      "Epoch 2545, Loss: 0.2792975530028343, Final Batch Loss: 0.11749423295259476\n",
      "Epoch 2546, Loss: 0.3821224570274353, Final Batch Loss: 0.20765021443367004\n",
      "Epoch 2547, Loss: 0.283934623003006, Final Batch Loss: 0.13468287885189056\n",
      "Epoch 2548, Loss: 0.3219590336084366, Final Batch Loss: 0.17815430462360382\n",
      "Epoch 2549, Loss: 0.28248948603868484, Final Batch Loss: 0.16163469851016998\n",
      "Epoch 2550, Loss: 0.4091791808605194, Final Batch Loss: 0.22047361731529236\n",
      "Epoch 2551, Loss: 0.33783358335494995, Final Batch Loss: 0.1857161819934845\n",
      "Epoch 2552, Loss: 0.3369569629430771, Final Batch Loss: 0.14661461114883423\n",
      "Epoch 2553, Loss: 0.33847495913505554, Final Batch Loss: 0.16491851210594177\n",
      "Epoch 2554, Loss: 0.29242241382598877, Final Batch Loss: 0.1497633010149002\n",
      "Epoch 2555, Loss: 0.33592498302459717, Final Batch Loss: 0.16471873223781586\n",
      "Epoch 2556, Loss: 0.3000986725091934, Final Batch Loss: 0.1429745852947235\n",
      "Epoch 2557, Loss: 0.3381398022174835, Final Batch Loss: 0.1391288787126541\n",
      "Epoch 2558, Loss: 0.36950506269931793, Final Batch Loss: 0.16353996098041534\n",
      "Epoch 2559, Loss: 0.2909620329737663, Final Batch Loss: 0.12371938675642014\n",
      "Epoch 2560, Loss: 0.2808542549610138, Final Batch Loss: 0.17382468283176422\n",
      "Epoch 2561, Loss: 0.32229049503803253, Final Batch Loss: 0.1810636967420578\n",
      "Epoch 2562, Loss: 0.2771434783935547, Final Batch Loss: 0.14032083749771118\n",
      "Epoch 2563, Loss: 0.3553467392921448, Final Batch Loss: 0.1883440762758255\n",
      "Epoch 2564, Loss: 0.35006438195705414, Final Batch Loss: 0.20143555104732513\n",
      "Epoch 2565, Loss: 0.3493364751338959, Final Batch Loss: 0.16204379498958588\n",
      "Epoch 2566, Loss: 0.3006450831890106, Final Batch Loss: 0.1584860384464264\n",
      "Epoch 2567, Loss: 0.3414541780948639, Final Batch Loss: 0.19209207594394684\n",
      "Epoch 2568, Loss: 0.34394434094429016, Final Batch Loss: 0.17224545776844025\n",
      "Epoch 2569, Loss: 0.3225530982017517, Final Batch Loss: 0.16045796871185303\n",
      "Epoch 2570, Loss: 0.32780857384204865, Final Batch Loss: 0.17260783910751343\n",
      "Epoch 2571, Loss: 0.339830607175827, Final Batch Loss: 0.17423652112483978\n",
      "Epoch 2572, Loss: 0.2829538583755493, Final Batch Loss: 0.11773891746997833\n",
      "Epoch 2573, Loss: 0.35527533292770386, Final Batch Loss: 0.16238832473754883\n",
      "Epoch 2574, Loss: 0.306686207652092, Final Batch Loss: 0.15896877646446228\n",
      "Epoch 2575, Loss: 0.36003468930721283, Final Batch Loss: 0.17439939081668854\n",
      "Epoch 2576, Loss: 0.275828093290329, Final Batch Loss: 0.14132541418075562\n",
      "Epoch 2577, Loss: 0.36836859583854675, Final Batch Loss: 0.18868622183799744\n",
      "Epoch 2578, Loss: 0.2742104083299637, Final Batch Loss: 0.13070982694625854\n",
      "Epoch 2579, Loss: 0.31472381949424744, Final Batch Loss: 0.1782207041978836\n",
      "Epoch 2580, Loss: 0.3181426078081131, Final Batch Loss: 0.16824883222579956\n",
      "Epoch 2581, Loss: 0.28002069890499115, Final Batch Loss: 0.12334488332271576\n",
      "Epoch 2582, Loss: 0.30177098512649536, Final Batch Loss: 0.1560390740633011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2583, Loss: 0.3148351460695267, Final Batch Loss: 0.1610681265592575\n",
      "Epoch 2584, Loss: 0.32262399792671204, Final Batch Loss: 0.17333142459392548\n",
      "Epoch 2585, Loss: 0.3395500183105469, Final Batch Loss: 0.19041350483894348\n",
      "Epoch 2586, Loss: 0.33573199808597565, Final Batch Loss: 0.16213105618953705\n",
      "Epoch 2587, Loss: 0.3140607327222824, Final Batch Loss: 0.14018884301185608\n",
      "Epoch 2588, Loss: 0.37542420625686646, Final Batch Loss: 0.1829260289669037\n",
      "Epoch 2589, Loss: 0.2868063896894455, Final Batch Loss: 0.14470461010932922\n",
      "Epoch 2590, Loss: 0.26620102673768997, Final Batch Loss: 0.14257687330245972\n",
      "Epoch 2591, Loss: 0.33677299320697784, Final Batch Loss: 0.1628713607788086\n",
      "Epoch 2592, Loss: 0.29003408551216125, Final Batch Loss: 0.15422961115837097\n",
      "Epoch 2593, Loss: 0.3128805160522461, Final Batch Loss: 0.12827782332897186\n",
      "Epoch 2594, Loss: 0.3377680778503418, Final Batch Loss: 0.1827479898929596\n",
      "Epoch 2595, Loss: 0.3149251788854599, Final Batch Loss: 0.15229225158691406\n",
      "Epoch 2596, Loss: 0.3284755349159241, Final Batch Loss: 0.18147024512290955\n",
      "Epoch 2597, Loss: 0.33778874576091766, Final Batch Loss: 0.20116518437862396\n",
      "Epoch 2598, Loss: 0.3007306307554245, Final Batch Loss: 0.1546461433172226\n",
      "Epoch 2599, Loss: 0.3129922151565552, Final Batch Loss: 0.14676667749881744\n",
      "Epoch 2600, Loss: 0.34907233715057373, Final Batch Loss: 0.1681155264377594\n",
      "Epoch 2601, Loss: 0.28075940907001495, Final Batch Loss: 0.1435943990945816\n",
      "Epoch 2602, Loss: 0.31003785133361816, Final Batch Loss: 0.14083677530288696\n",
      "Epoch 2603, Loss: 0.33581143617630005, Final Batch Loss: 0.19039316475391388\n",
      "Epoch 2604, Loss: 0.26047183573246, Final Batch Loss: 0.12695445120334625\n",
      "Epoch 2605, Loss: 0.2910270541906357, Final Batch Loss: 0.14367347955703735\n",
      "Epoch 2606, Loss: 0.3431140184402466, Final Batch Loss: 0.19028271734714508\n",
      "Epoch 2607, Loss: 0.2753741890192032, Final Batch Loss: 0.12014661729335785\n",
      "Epoch 2608, Loss: 0.31099842488765717, Final Batch Loss: 0.17172163724899292\n",
      "Epoch 2609, Loss: 0.33767133951187134, Final Batch Loss: 0.22170206904411316\n",
      "Epoch 2610, Loss: 0.3245081752538681, Final Batch Loss: 0.17400161921977997\n",
      "Epoch 2611, Loss: 0.2434762939810753, Final Batch Loss: 0.11818362027406693\n",
      "Epoch 2612, Loss: 0.3285141885280609, Final Batch Loss: 0.18012650310993195\n",
      "Epoch 2613, Loss: 0.31080152839422226, Final Batch Loss: 0.11420530825853348\n",
      "Epoch 2614, Loss: 0.24177230894565582, Final Batch Loss: 0.11221419274806976\n",
      "Epoch 2615, Loss: 0.3235131651163101, Final Batch Loss: 0.17145459353923798\n",
      "Epoch 2616, Loss: 0.2917690873146057, Final Batch Loss: 0.1367134153842926\n",
      "Epoch 2617, Loss: 0.26486262679100037, Final Batch Loss: 0.13922707736492157\n",
      "Epoch 2618, Loss: 0.2924286276102066, Final Batch Loss: 0.1353551149368286\n",
      "Epoch 2619, Loss: 0.3655707687139511, Final Batch Loss: 0.2054697871208191\n",
      "Epoch 2620, Loss: 0.41892966628074646, Final Batch Loss: 0.24110546708106995\n",
      "Epoch 2621, Loss: 0.3794345110654831, Final Batch Loss: 0.20373979210853577\n",
      "Epoch 2622, Loss: 0.3071152865886688, Final Batch Loss: 0.1399042010307312\n",
      "Epoch 2623, Loss: 0.35330045223236084, Final Batch Loss: 0.17916838824748993\n",
      "Epoch 2624, Loss: 0.25981520116329193, Final Batch Loss: 0.13153955340385437\n",
      "Epoch 2625, Loss: 0.3342873752117157, Final Batch Loss: 0.13222193717956543\n",
      "Epoch 2626, Loss: 0.3580597788095474, Final Batch Loss: 0.21772852540016174\n",
      "Epoch 2627, Loss: 0.2803030014038086, Final Batch Loss: 0.12798389792442322\n",
      "Epoch 2628, Loss: 0.28359153866767883, Final Batch Loss: 0.13785842061042786\n",
      "Epoch 2629, Loss: 0.3366372436285019, Final Batch Loss: 0.1768311858177185\n",
      "Epoch 2630, Loss: 0.33514533936977386, Final Batch Loss: 0.188654825091362\n",
      "Epoch 2631, Loss: 0.3425356447696686, Final Batch Loss: 0.17067767679691315\n",
      "Epoch 2632, Loss: 0.2565015181899071, Final Batch Loss: 0.11313129216432571\n",
      "Epoch 2633, Loss: 0.3029772788286209, Final Batch Loss: 0.1398361772298813\n",
      "Epoch 2634, Loss: 0.3938578814268112, Final Batch Loss: 0.2614891231060028\n",
      "Epoch 2635, Loss: 0.31928692758083344, Final Batch Loss: 0.16450347006320953\n",
      "Epoch 2636, Loss: 0.2522675469517708, Final Batch Loss: 0.0812942162156105\n",
      "Epoch 2637, Loss: 0.28693827241659164, Final Batch Loss: 0.1660739779472351\n",
      "Epoch 2638, Loss: 0.2652597948908806, Final Batch Loss: 0.11475358158349991\n",
      "Epoch 2639, Loss: 0.2630561962723732, Final Batch Loss: 0.108143649995327\n",
      "Epoch 2640, Loss: 0.3364364355802536, Final Batch Loss: 0.20859390497207642\n",
      "Epoch 2641, Loss: 0.3609396517276764, Final Batch Loss: 0.15572555363178253\n",
      "Epoch 2642, Loss: 0.31710387766361237, Final Batch Loss: 0.1488741785287857\n",
      "Epoch 2643, Loss: 0.3179114907979965, Final Batch Loss: 0.13925763964653015\n",
      "Epoch 2644, Loss: 0.3370915353298187, Final Batch Loss: 0.1324811577796936\n",
      "Epoch 2645, Loss: 0.26197802275419235, Final Batch Loss: 0.1205262616276741\n",
      "Epoch 2646, Loss: 0.3253947049379349, Final Batch Loss: 0.13512541353702545\n",
      "Epoch 2647, Loss: 0.3053855895996094, Final Batch Loss: 0.15039168298244476\n",
      "Epoch 2648, Loss: 0.28967489302158356, Final Batch Loss: 0.113409623503685\n",
      "Epoch 2649, Loss: 0.28230196237564087, Final Batch Loss: 0.12281525135040283\n",
      "Epoch 2650, Loss: 0.31489843130111694, Final Batch Loss: 0.17979082465171814\n",
      "Epoch 2651, Loss: 0.3570419251918793, Final Batch Loss: 0.17885640263557434\n",
      "Epoch 2652, Loss: 0.28335801512002945, Final Batch Loss: 0.12295114248991013\n",
      "Epoch 2653, Loss: 0.2647509202361107, Final Batch Loss: 0.14683061838150024\n",
      "Epoch 2654, Loss: 0.31719981133937836, Final Batch Loss: 0.1671823263168335\n",
      "Epoch 2655, Loss: 0.35065264999866486, Final Batch Loss: 0.20423680543899536\n",
      "Epoch 2656, Loss: 0.3346899151802063, Final Batch Loss: 0.16440677642822266\n",
      "Epoch 2657, Loss: 0.3252408057451248, Final Batch Loss: 0.12983664870262146\n",
      "Epoch 2658, Loss: 0.26853275299072266, Final Batch Loss: 0.1130807101726532\n",
      "Epoch 2659, Loss: 0.30673278868198395, Final Batch Loss: 0.16982720792293549\n",
      "Epoch 2660, Loss: 0.26515696942806244, Final Batch Loss: 0.14961230754852295\n",
      "Epoch 2661, Loss: 0.29832297563552856, Final Batch Loss: 0.1388217955827713\n",
      "Epoch 2662, Loss: 0.3807150721549988, Final Batch Loss: 0.24114002287387848\n",
      "Epoch 2663, Loss: 0.3286200016736984, Final Batch Loss: 0.14516793191432953\n",
      "Epoch 2664, Loss: 0.27078013867139816, Final Batch Loss: 0.08543454855680466\n",
      "Epoch 2665, Loss: 0.3109994977712631, Final Batch Loss: 0.17531757056713104\n",
      "Epoch 2666, Loss: 0.3398456424474716, Final Batch Loss: 0.17501495778560638\n",
      "Epoch 2667, Loss: 0.2972167283296585, Final Batch Loss: 0.1216435432434082\n",
      "Epoch 2668, Loss: 0.2848076671361923, Final Batch Loss: 0.15585997700691223\n",
      "Epoch 2669, Loss: 0.31489504873752594, Final Batch Loss: 0.1609821766614914\n",
      "Epoch 2670, Loss: 0.3206246495246887, Final Batch Loss: 0.13641901314258575\n",
      "Epoch 2671, Loss: 0.2550401985645294, Final Batch Loss: 0.12822794914245605\n",
      "Epoch 2672, Loss: 0.38571274280548096, Final Batch Loss: 0.22916467487812042\n",
      "Epoch 2673, Loss: 0.3485027998685837, Final Batch Loss: 0.1647760570049286\n",
      "Epoch 2674, Loss: 0.307389572262764, Final Batch Loss: 0.11724106967449188\n",
      "Epoch 2675, Loss: 0.3409044146537781, Final Batch Loss: 0.19810189306735992\n",
      "Epoch 2676, Loss: 0.2930748835206032, Final Batch Loss: 0.16883055865764618\n",
      "Epoch 2677, Loss: 0.3109913319349289, Final Batch Loss: 0.1391706019639969\n",
      "Epoch 2678, Loss: 0.29197025299072266, Final Batch Loss: 0.13684038817882538\n",
      "Epoch 2679, Loss: 0.4084753394126892, Final Batch Loss: 0.2122780680656433\n",
      "Epoch 2680, Loss: 0.2913070619106293, Final Batch Loss: 0.14508770406246185\n",
      "Epoch 2681, Loss: 0.3287197947502136, Final Batch Loss: 0.16726742684841156\n",
      "Epoch 2682, Loss: 0.2905949503183365, Final Batch Loss: 0.15685494244098663\n",
      "Epoch 2683, Loss: 0.2747073322534561, Final Batch Loss: 0.14258086681365967\n",
      "Epoch 2684, Loss: 0.3314928114414215, Final Batch Loss: 0.13996031880378723\n",
      "Epoch 2685, Loss: 0.3004556894302368, Final Batch Loss: 0.16722731292247772\n",
      "Epoch 2686, Loss: 0.2632592096924782, Final Batch Loss: 0.15335625410079956\n",
      "Epoch 2687, Loss: 0.2763682156801224, Final Batch Loss: 0.10894674062728882\n",
      "Epoch 2688, Loss: 0.22990047931671143, Final Batch Loss: 0.12244309484958649\n",
      "Epoch 2689, Loss: 0.27859417349100113, Final Batch Loss: 0.16267456114292145\n",
      "Epoch 2690, Loss: 0.3084615468978882, Final Batch Loss: 0.15614691376686096\n",
      "Epoch 2691, Loss: 0.26691005378961563, Final Batch Loss: 0.15377721190452576\n",
      "Epoch 2692, Loss: 0.3088461458683014, Final Batch Loss: 0.14560435712337494\n",
      "Epoch 2693, Loss: 0.3140386790037155, Final Batch Loss: 0.1561427265405655\n",
      "Epoch 2694, Loss: 0.26582252979278564, Final Batch Loss: 0.15314310789108276\n",
      "Epoch 2695, Loss: 0.31598692387342453, Final Batch Loss: 0.12085305899381638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2696, Loss: 0.32218916714191437, Final Batch Loss: 0.1637023240327835\n",
      "Epoch 2697, Loss: 0.33906933665275574, Final Batch Loss: 0.18705998361110687\n",
      "Epoch 2698, Loss: 0.32525216042995453, Final Batch Loss: 0.1521495133638382\n",
      "Epoch 2699, Loss: 0.3474580943584442, Final Batch Loss: 0.17506401240825653\n",
      "Epoch 2700, Loss: 0.23214726895093918, Final Batch Loss: 0.09477924555540085\n",
      "Epoch 2701, Loss: 0.2937299460172653, Final Batch Loss: 0.13504792749881744\n",
      "Epoch 2702, Loss: 0.34489019215106964, Final Batch Loss: 0.19692352414131165\n",
      "Epoch 2703, Loss: 0.2699655666947365, Final Batch Loss: 0.16256225109100342\n",
      "Epoch 2704, Loss: 0.2833142355084419, Final Batch Loss: 0.12062003463506699\n",
      "Epoch 2705, Loss: 0.23616793751716614, Final Batch Loss: 0.10672329366207123\n",
      "Epoch 2706, Loss: 0.28763297945261, Final Batch Loss: 0.12284626811742783\n",
      "Epoch 2707, Loss: 0.2868139371275902, Final Batch Loss: 0.11055325716733932\n",
      "Epoch 2708, Loss: 0.3240007683634758, Final Batch Loss: 0.2017485797405243\n",
      "Epoch 2709, Loss: 0.2966424971818924, Final Batch Loss: 0.1567106544971466\n",
      "Epoch 2710, Loss: 0.2578049600124359, Final Batch Loss: 0.11964696645736694\n",
      "Epoch 2711, Loss: 0.2781965434551239, Final Batch Loss: 0.13376623392105103\n",
      "Epoch 2712, Loss: 0.26841220259666443, Final Batch Loss: 0.12462067604064941\n",
      "Epoch 2713, Loss: 0.3582022786140442, Final Batch Loss: 0.22357209026813507\n",
      "Epoch 2714, Loss: 0.32599177956581116, Final Batch Loss: 0.17401625216007233\n",
      "Epoch 2715, Loss: 0.2924138456583023, Final Batch Loss: 0.13289831578731537\n",
      "Epoch 2716, Loss: 0.33889807760715485, Final Batch Loss: 0.1893230378627777\n",
      "Epoch 2717, Loss: 0.27029113471508026, Final Batch Loss: 0.14384746551513672\n",
      "Epoch 2718, Loss: 0.26407555490732193, Final Batch Loss: 0.12439379841089249\n",
      "Epoch 2719, Loss: 0.2739304006099701, Final Batch Loss: 0.10285355150699615\n",
      "Epoch 2720, Loss: 0.25293774902820587, Final Batch Loss: 0.10841350257396698\n",
      "Epoch 2721, Loss: 0.28831538558006287, Final Batch Loss: 0.13402119278907776\n",
      "Epoch 2722, Loss: 0.3760424703359604, Final Batch Loss: 0.2062923163175583\n",
      "Epoch 2723, Loss: 0.2035277709364891, Final Batch Loss: 0.06679802387952805\n",
      "Epoch 2724, Loss: 0.27530549466609955, Final Batch Loss: 0.11731527745723724\n",
      "Epoch 2725, Loss: 0.2466861456632614, Final Batch Loss: 0.11644241213798523\n",
      "Epoch 2726, Loss: 0.29856663942337036, Final Batch Loss: 0.1570131480693817\n",
      "Epoch 2727, Loss: 0.24709056317806244, Final Batch Loss: 0.12206844985485077\n",
      "Epoch 2728, Loss: 0.2915099263191223, Final Batch Loss: 0.16040119528770447\n",
      "Epoch 2729, Loss: 0.32143624126911163, Final Batch Loss: 0.17824411392211914\n",
      "Epoch 2730, Loss: 0.27158258110284805, Final Batch Loss: 0.15231779217720032\n",
      "Epoch 2731, Loss: 0.30801571905612946, Final Batch Loss: 0.16966049373149872\n",
      "Epoch 2732, Loss: 0.35064515471458435, Final Batch Loss: 0.16202634572982788\n",
      "Epoch 2733, Loss: 0.3400374799966812, Final Batch Loss: 0.22696906328201294\n",
      "Epoch 2734, Loss: 0.3018682673573494, Final Batch Loss: 0.12153785675764084\n",
      "Epoch 2735, Loss: 0.32233527302742004, Final Batch Loss: 0.16083472967147827\n",
      "Epoch 2736, Loss: 0.2493746355175972, Final Batch Loss: 0.12330298870801926\n",
      "Epoch 2737, Loss: 0.33020997047424316, Final Batch Loss: 0.12981277704238892\n",
      "Epoch 2738, Loss: 0.2521793320775032, Final Batch Loss: 0.13254894316196442\n",
      "Epoch 2739, Loss: 0.2840021401643753, Final Batch Loss: 0.14174103736877441\n",
      "Epoch 2740, Loss: 0.36710038781166077, Final Batch Loss: 0.23080657422542572\n",
      "Epoch 2741, Loss: 0.29648222029209137, Final Batch Loss: 0.1422230452299118\n",
      "Epoch 2742, Loss: 0.2882908210158348, Final Batch Loss: 0.16682976484298706\n",
      "Epoch 2743, Loss: 0.34353527426719666, Final Batch Loss: 0.18412786722183228\n",
      "Epoch 2744, Loss: 0.2930770665407181, Final Batch Loss: 0.13625440001487732\n",
      "Epoch 2745, Loss: 0.3450276255607605, Final Batch Loss: 0.1639731079339981\n",
      "Epoch 2746, Loss: 0.21004358679056168, Final Batch Loss: 0.08700340986251831\n",
      "Epoch 2747, Loss: 0.2961562052369118, Final Batch Loss: 0.193106010556221\n",
      "Epoch 2748, Loss: 0.3089742958545685, Final Batch Loss: 0.15455833077430725\n",
      "Epoch 2749, Loss: 0.2910628244280815, Final Batch Loss: 0.18996131420135498\n",
      "Epoch 2750, Loss: 0.2627594396471977, Final Batch Loss: 0.16504435241222382\n",
      "Epoch 2751, Loss: 0.30233487486839294, Final Batch Loss: 0.17039822041988373\n",
      "Epoch 2752, Loss: 0.30197973549366, Final Batch Loss: 0.1704704612493515\n",
      "Epoch 2753, Loss: 0.29276350140571594, Final Batch Loss: 0.14079385995864868\n",
      "Epoch 2754, Loss: 0.3493338078260422, Final Batch Loss: 0.18740113079547882\n",
      "Epoch 2755, Loss: 0.27265122532844543, Final Batch Loss: 0.11642768979072571\n",
      "Epoch 2756, Loss: 0.28301580995321274, Final Batch Loss: 0.16341830790042877\n",
      "Epoch 2757, Loss: 0.30825190246105194, Final Batch Loss: 0.13700072467327118\n",
      "Epoch 2758, Loss: 0.28295227885246277, Final Batch Loss: 0.14545519649982452\n",
      "Epoch 2759, Loss: 0.3837086707353592, Final Batch Loss: 0.20152157545089722\n",
      "Epoch 2760, Loss: 0.2549290731549263, Final Batch Loss: 0.09751086682081223\n",
      "Epoch 2761, Loss: 0.2874637395143509, Final Batch Loss: 0.1317196637392044\n",
      "Epoch 2762, Loss: 0.3425595089793205, Final Batch Loss: 0.12035206705331802\n",
      "Epoch 2763, Loss: 0.30693310499191284, Final Batch Loss: 0.14457868039608002\n",
      "Epoch 2764, Loss: 0.35741397738456726, Final Batch Loss: 0.20247817039489746\n",
      "Epoch 2765, Loss: 0.3441077470779419, Final Batch Loss: 0.17034457623958588\n",
      "Epoch 2766, Loss: 0.28662481904029846, Final Batch Loss: 0.1264340877532959\n",
      "Epoch 2767, Loss: 0.26430708169937134, Final Batch Loss: 0.10948403179645538\n",
      "Epoch 2768, Loss: 0.2988229840993881, Final Batch Loss: 0.1263735145330429\n",
      "Epoch 2769, Loss: 0.31483329832553864, Final Batch Loss: 0.15875355899333954\n",
      "Epoch 2770, Loss: 0.32161134481430054, Final Batch Loss: 0.15113504230976105\n",
      "Epoch 2771, Loss: 0.3215547055006027, Final Batch Loss: 0.1463802307844162\n",
      "Epoch 2772, Loss: 0.3351924866437912, Final Batch Loss: 0.18440662324428558\n",
      "Epoch 2773, Loss: 0.2871953397989273, Final Batch Loss: 0.14411289989948273\n",
      "Epoch 2774, Loss: 0.24886418133974075, Final Batch Loss: 0.11497210711240768\n",
      "Epoch 2775, Loss: 0.24017147719860077, Final Batch Loss: 0.13132350146770477\n",
      "Epoch 2776, Loss: 0.23920457065105438, Final Batch Loss: 0.11859752982854843\n",
      "Epoch 2777, Loss: 0.2869777977466583, Final Batch Loss: 0.13417485356330872\n",
      "Epoch 2778, Loss: 0.2689051106572151, Final Batch Loss: 0.1496419906616211\n",
      "Epoch 2779, Loss: 0.30107441544532776, Final Batch Loss: 0.1609046906232834\n",
      "Epoch 2780, Loss: 0.26204243302345276, Final Batch Loss: 0.14322417974472046\n",
      "Epoch 2781, Loss: 0.31080667674541473, Final Batch Loss: 0.13031399250030518\n",
      "Epoch 2782, Loss: 0.3404105603694916, Final Batch Loss: 0.19178089499473572\n",
      "Epoch 2783, Loss: 0.3771945387125015, Final Batch Loss: 0.23051773011684418\n",
      "Epoch 2784, Loss: 0.2263840138912201, Final Batch Loss: 0.10575942695140839\n",
      "Epoch 2785, Loss: 0.2766222506761551, Final Batch Loss: 0.11248795688152313\n",
      "Epoch 2786, Loss: 0.26131950318813324, Final Batch Loss: 0.1357855498790741\n",
      "Epoch 2787, Loss: 0.2984619587659836, Final Batch Loss: 0.13907764852046967\n",
      "Epoch 2788, Loss: 0.23791828751564026, Final Batch Loss: 0.13010135293006897\n",
      "Epoch 2789, Loss: 0.3124663084745407, Final Batch Loss: 0.18539179861545563\n",
      "Epoch 2790, Loss: 0.3905435651540756, Final Batch Loss: 0.20180216431617737\n",
      "Epoch 2791, Loss: 0.2914176732301712, Final Batch Loss: 0.13613593578338623\n",
      "Epoch 2792, Loss: 0.2751896232366562, Final Batch Loss: 0.136398583650589\n",
      "Epoch 2793, Loss: 0.25321468710899353, Final Batch Loss: 0.11351020634174347\n",
      "Epoch 2794, Loss: 0.2528490275144577, Final Batch Loss: 0.09883260726928711\n",
      "Epoch 2795, Loss: 0.26455287635326385, Final Batch Loss: 0.15306822955608368\n",
      "Epoch 2796, Loss: 0.30571572482585907, Final Batch Loss: 0.1346595287322998\n",
      "Epoch 2797, Loss: 0.3001207113265991, Final Batch Loss: 0.1424855887889862\n",
      "Epoch 2798, Loss: 0.317709282040596, Final Batch Loss: 0.11805064976215363\n",
      "Epoch 2799, Loss: 0.24590039253234863, Final Batch Loss: 0.14742492139339447\n",
      "Epoch 2800, Loss: 0.262462854385376, Final Batch Loss: 0.11665540933609009\n",
      "Epoch 2801, Loss: 0.3007354140281677, Final Batch Loss: 0.13098955154418945\n",
      "Epoch 2802, Loss: 0.29072581231594086, Final Batch Loss: 0.15956434607505798\n",
      "Epoch 2803, Loss: 0.34001588821411133, Final Batch Loss: 0.19711636006832123\n",
      "Epoch 2804, Loss: 0.27792448550462723, Final Batch Loss: 0.1564885973930359\n",
      "Epoch 2805, Loss: 0.2456464245915413, Final Batch Loss: 0.1327943056821823\n",
      "Epoch 2806, Loss: 0.30125968158245087, Final Batch Loss: 0.12772761285305023\n",
      "Epoch 2807, Loss: 0.30197250843048096, Final Batch Loss: 0.16750536859035492\n",
      "Epoch 2808, Loss: 0.34179404377937317, Final Batch Loss: 0.20173379778862\n",
      "Epoch 2809, Loss: 0.23750711232423782, Final Batch Loss: 0.1271255761384964\n",
      "Epoch 2810, Loss: 0.30433934926986694, Final Batch Loss: 0.13071240484714508\n",
      "Epoch 2811, Loss: 0.2726178616285324, Final Batch Loss: 0.11121426522731781\n",
      "Epoch 2812, Loss: 0.2863890528678894, Final Batch Loss: 0.15443125367164612\n",
      "Epoch 2813, Loss: 0.4189399778842926, Final Batch Loss: 0.20670315623283386\n",
      "Epoch 2814, Loss: 0.254186250269413, Final Batch Loss: 0.10909301787614822\n",
      "Epoch 2815, Loss: 0.2970545142889023, Final Batch Loss: 0.1431136578321457\n",
      "Epoch 2816, Loss: 0.33153554052114487, Final Batch Loss: 0.2177123874425888\n",
      "Epoch 2817, Loss: 0.2972659766674042, Final Batch Loss: 0.16183294355869293\n",
      "Epoch 2818, Loss: 0.28615300357341766, Final Batch Loss: 0.15924131870269775\n",
      "Epoch 2819, Loss: 0.3173501640558243, Final Batch Loss: 0.1890355497598648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2820, Loss: 0.24011152237653732, Final Batch Loss: 0.10581428557634354\n",
      "Epoch 2821, Loss: 0.2523676007986069, Final Batch Loss: 0.13071630895137787\n",
      "Epoch 2822, Loss: 0.27369174361228943, Final Batch Loss: 0.1478782743215561\n",
      "Epoch 2823, Loss: 0.3106552064418793, Final Batch Loss: 0.12645882368087769\n",
      "Epoch 2824, Loss: 0.322612926363945, Final Batch Loss: 0.14319762587547302\n",
      "Epoch 2825, Loss: 0.31037409603595734, Final Batch Loss: 0.15091872215270996\n",
      "Epoch 2826, Loss: 0.2923032343387604, Final Batch Loss: 0.13861116766929626\n",
      "Epoch 2827, Loss: 0.31812135875225067, Final Batch Loss: 0.13315355777740479\n",
      "Epoch 2828, Loss: 0.26699167490005493, Final Batch Loss: 0.14475345611572266\n",
      "Epoch 2829, Loss: 0.2536832466721535, Final Batch Loss: 0.11894678324460983\n",
      "Epoch 2830, Loss: 0.34553103148937225, Final Batch Loss: 0.19363194704055786\n",
      "Epoch 2831, Loss: 0.29294729232788086, Final Batch Loss: 0.15726874768733978\n",
      "Epoch 2832, Loss: 0.2858637571334839, Final Batch Loss: 0.13604994118213654\n",
      "Epoch 2833, Loss: 0.2792665958404541, Final Batch Loss: 0.1513325273990631\n",
      "Epoch 2834, Loss: 0.2456139549612999, Final Batch Loss: 0.10381249338388443\n",
      "Epoch 2835, Loss: 0.28154614567756653, Final Batch Loss: 0.13025745749473572\n",
      "Epoch 2836, Loss: 0.29404497146606445, Final Batch Loss: 0.1393875628709793\n",
      "Epoch 2837, Loss: 0.2560484781861305, Final Batch Loss: 0.09891919046640396\n",
      "Epoch 2838, Loss: 0.26602746546268463, Final Batch Loss: 0.1340058445930481\n",
      "Epoch 2839, Loss: 0.2595730572938919, Final Batch Loss: 0.13509705662727356\n",
      "Epoch 2840, Loss: 0.32297705113887787, Final Batch Loss: 0.1659766435623169\n",
      "Epoch 2841, Loss: 0.3000425100326538, Final Batch Loss: 0.1410619169473648\n",
      "Epoch 2842, Loss: 0.2955098897218704, Final Batch Loss: 0.12590481340885162\n",
      "Epoch 2843, Loss: 0.33138836920261383, Final Batch Loss: 0.17960046231746674\n",
      "Epoch 2844, Loss: 0.26348673552274704, Final Batch Loss: 0.10929354280233383\n",
      "Epoch 2845, Loss: 0.30617909133434296, Final Batch Loss: 0.16028930246829987\n",
      "Epoch 2846, Loss: 0.31682129204273224, Final Batch Loss: 0.14673133194446564\n",
      "Epoch 2847, Loss: 0.34972646832466125, Final Batch Loss: 0.20965896546840668\n",
      "Epoch 2848, Loss: 0.2531249299645424, Final Batch Loss: 0.10300783067941666\n",
      "Epoch 2849, Loss: 0.2351878583431244, Final Batch Loss: 0.11690376698970795\n",
      "Epoch 2850, Loss: 0.27794963121414185, Final Batch Loss: 0.12361016869544983\n",
      "Epoch 2851, Loss: 0.30315186083316803, Final Batch Loss: 0.1765570044517517\n",
      "Epoch 2852, Loss: 0.27853091061115265, Final Batch Loss: 0.15505553781986237\n",
      "Epoch 2853, Loss: 0.33645056188106537, Final Batch Loss: 0.18975967168807983\n",
      "Epoch 2854, Loss: 0.29103393107652664, Final Batch Loss: 0.12018465250730515\n",
      "Epoch 2855, Loss: 0.3076120913028717, Final Batch Loss: 0.14467176795005798\n",
      "Epoch 2856, Loss: 0.2786342352628708, Final Batch Loss: 0.1335899829864502\n",
      "Epoch 2857, Loss: 0.29463255405426025, Final Batch Loss: 0.1487428843975067\n",
      "Epoch 2858, Loss: 0.3050597906112671, Final Batch Loss: 0.14876379072666168\n",
      "Epoch 2859, Loss: 0.32055386900901794, Final Batch Loss: 0.16629695892333984\n",
      "Epoch 2860, Loss: 0.33167725801467896, Final Batch Loss: 0.16812936961650848\n",
      "Epoch 2861, Loss: 0.29032573103904724, Final Batch Loss: 0.15317974984645844\n",
      "Epoch 2862, Loss: 0.2750551104545593, Final Batch Loss: 0.12698517739772797\n",
      "Epoch 2863, Loss: 0.31459156423807144, Final Batch Loss: 0.19059646129608154\n",
      "Epoch 2864, Loss: 0.18007076159119606, Final Batch Loss: 0.05791854485869408\n",
      "Epoch 2865, Loss: 0.23609007894992828, Final Batch Loss: 0.10446327924728394\n",
      "Epoch 2866, Loss: 0.2788696438074112, Final Batch Loss: 0.12522979080677032\n",
      "Epoch 2867, Loss: 0.31357747316360474, Final Batch Loss: 0.160550057888031\n",
      "Epoch 2868, Loss: 0.28773196041584015, Final Batch Loss: 0.14688533544540405\n",
      "Epoch 2869, Loss: 0.30885230004787445, Final Batch Loss: 0.14419551193714142\n",
      "Epoch 2870, Loss: 0.3202402740716934, Final Batch Loss: 0.14763587713241577\n",
      "Epoch 2871, Loss: 0.2387773096561432, Final Batch Loss: 0.08509048819541931\n",
      "Epoch 2872, Loss: 0.2933036983013153, Final Batch Loss: 0.11234946548938751\n",
      "Epoch 2873, Loss: 0.2865039259195328, Final Batch Loss: 0.14596958458423615\n",
      "Epoch 2874, Loss: 0.36683325469493866, Final Batch Loss: 0.1748666912317276\n",
      "Epoch 2875, Loss: 0.346232607960701, Final Batch Loss: 0.17512591183185577\n",
      "Epoch 2876, Loss: 0.35124577581882477, Final Batch Loss: 0.2049359679222107\n",
      "Epoch 2877, Loss: 0.23505110293626785, Final Batch Loss: 0.08069593459367752\n",
      "Epoch 2878, Loss: 0.3213541507720947, Final Batch Loss: 0.143549844622612\n",
      "Epoch 2879, Loss: 0.28765933960676193, Final Batch Loss: 0.11682923883199692\n",
      "Epoch 2880, Loss: 0.2557815760374069, Final Batch Loss: 0.1072625070810318\n",
      "Epoch 2881, Loss: 0.29881079494953156, Final Batch Loss: 0.15373340249061584\n",
      "Epoch 2882, Loss: 0.2678341642022133, Final Batch Loss: 0.11527713388204575\n",
      "Epoch 2883, Loss: 0.311633363366127, Final Batch Loss: 0.14805974066257477\n",
      "Epoch 2884, Loss: 0.2710762917995453, Final Batch Loss: 0.14824062585830688\n",
      "Epoch 2885, Loss: 0.27599214762449265, Final Batch Loss: 0.12461169809103012\n",
      "Epoch 2886, Loss: 0.27467820048332214, Final Batch Loss: 0.14433597028255463\n",
      "Epoch 2887, Loss: 0.3257354199886322, Final Batch Loss: 0.1641099750995636\n",
      "Epoch 2888, Loss: 0.25341086089611053, Final Batch Loss: 0.11595673859119415\n",
      "Epoch 2889, Loss: 0.31734034419059753, Final Batch Loss: 0.1438896358013153\n",
      "Epoch 2890, Loss: 0.2746381461620331, Final Batch Loss: 0.1381571888923645\n",
      "Epoch 2891, Loss: 0.2887280434370041, Final Batch Loss: 0.16080056130886078\n",
      "Epoch 2892, Loss: 0.26988227665424347, Final Batch Loss: 0.13343864679336548\n",
      "Epoch 2893, Loss: 0.2962873578071594, Final Batch Loss: 0.16814857721328735\n",
      "Epoch 2894, Loss: 0.28955917060375214, Final Batch Loss: 0.13789482414722443\n",
      "Epoch 2895, Loss: 0.28358739614486694, Final Batch Loss: 0.15712961554527283\n",
      "Epoch 2896, Loss: 0.28053900599479675, Final Batch Loss: 0.1414250135421753\n",
      "Epoch 2897, Loss: 0.25818677246570587, Final Batch Loss: 0.1231352835893631\n",
      "Epoch 2898, Loss: 0.34160442650318146, Final Batch Loss: 0.1761454939842224\n",
      "Epoch 2899, Loss: 0.2914521098136902, Final Batch Loss: 0.1562846153974533\n",
      "Epoch 2900, Loss: 0.2439868450164795, Final Batch Loss: 0.13064852356910706\n",
      "Epoch 2901, Loss: 0.31305427849292755, Final Batch Loss: 0.20386631786823273\n",
      "Epoch 2902, Loss: 0.2700125575065613, Final Batch Loss: 0.15840700268745422\n",
      "Epoch 2903, Loss: 0.35616424679756165, Final Batch Loss: 0.15853536128997803\n",
      "Epoch 2904, Loss: 0.40468089282512665, Final Batch Loss: 0.2555866241455078\n",
      "Epoch 2905, Loss: 0.2540135085582733, Final Batch Loss: 0.10910707712173462\n",
      "Epoch 2906, Loss: 0.2602134719491005, Final Batch Loss: 0.1035415306687355\n",
      "Epoch 2907, Loss: 0.2841695249080658, Final Batch Loss: 0.1553131341934204\n",
      "Epoch 2908, Loss: 0.31216612458229065, Final Batch Loss: 0.11516474187374115\n",
      "Epoch 2909, Loss: 0.2919035851955414, Final Batch Loss: 0.1281275749206543\n",
      "Epoch 2910, Loss: 0.3023664057254791, Final Batch Loss: 0.14727987349033356\n",
      "Epoch 2911, Loss: 0.2942393720149994, Final Batch Loss: 0.15743176639080048\n",
      "Epoch 2912, Loss: 0.3486868590116501, Final Batch Loss: 0.20634569227695465\n",
      "Epoch 2913, Loss: 0.24660051614046097, Final Batch Loss: 0.1352611929178238\n",
      "Epoch 2914, Loss: 0.32811853289604187, Final Batch Loss: 0.1889585703611374\n",
      "Epoch 2915, Loss: 0.29487909376621246, Final Batch Loss: 0.1358945071697235\n",
      "Epoch 2916, Loss: 0.3179949223995209, Final Batch Loss: 0.13626974821090698\n",
      "Epoch 2917, Loss: 0.38823410868644714, Final Batch Loss: 0.18296286463737488\n",
      "Epoch 2918, Loss: 0.29014281928539276, Final Batch Loss: 0.16632801294326782\n",
      "Epoch 2919, Loss: 0.30529412627220154, Final Batch Loss: 0.14288312196731567\n",
      "Epoch 2920, Loss: 0.19848662614822388, Final Batch Loss: 0.09804655611515045\n",
      "Epoch 2921, Loss: 0.2836969196796417, Final Batch Loss: 0.1311766505241394\n",
      "Epoch 2922, Loss: 0.24569889158010483, Final Batch Loss: 0.1040167585015297\n",
      "Epoch 2923, Loss: 0.2903527244925499, Final Batch Loss: 0.17803727090358734\n",
      "Epoch 2924, Loss: 0.260688416659832, Final Batch Loss: 0.11991599947214127\n",
      "Epoch 2925, Loss: 0.32608599960803986, Final Batch Loss: 0.1688275784254074\n",
      "Epoch 2926, Loss: 0.2083725556731224, Final Batch Loss: 0.0778341218829155\n",
      "Epoch 2927, Loss: 0.2912340834736824, Final Batch Loss: 0.17289797961711884\n",
      "Epoch 2928, Loss: 0.3303672894835472, Final Batch Loss: 0.21332809329032898\n",
      "Epoch 2929, Loss: 0.32893192768096924, Final Batch Loss: 0.11403240263462067\n",
      "Epoch 2930, Loss: 0.30838170647621155, Final Batch Loss: 0.13968303799629211\n",
      "Epoch 2931, Loss: 0.29831987619400024, Final Batch Loss: 0.1453746259212494\n",
      "Epoch 2932, Loss: 0.29191648960113525, Final Batch Loss: 0.12608347833156586\n",
      "Epoch 2933, Loss: 0.29390572756528854, Final Batch Loss: 0.17407134175300598\n",
      "Epoch 2934, Loss: 0.30211399495601654, Final Batch Loss: 0.14685556292533875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2935, Loss: 0.3947855532169342, Final Batch Loss: 0.23434768617153168\n",
      "Epoch 2936, Loss: 0.282067209482193, Final Batch Loss: 0.1415945142507553\n",
      "Epoch 2937, Loss: 0.2722758501768112, Final Batch Loss: 0.1319628208875656\n",
      "Epoch 2938, Loss: 0.3022604361176491, Final Batch Loss: 0.0979425385594368\n",
      "Epoch 2939, Loss: 0.23870548605918884, Final Batch Loss: 0.13119712471961975\n",
      "Epoch 2940, Loss: 0.34587207436561584, Final Batch Loss: 0.192689910531044\n",
      "Epoch 2941, Loss: 0.3079540878534317, Final Batch Loss: 0.14379054307937622\n",
      "Epoch 2942, Loss: 0.29003097116947174, Final Batch Loss: 0.13024428486824036\n",
      "Epoch 2943, Loss: 0.2871644198894501, Final Batch Loss: 0.13845935463905334\n",
      "Epoch 2944, Loss: 0.2657703012228012, Final Batch Loss: 0.12824831902980804\n",
      "Epoch 2945, Loss: 0.3110858201980591, Final Batch Loss: 0.1733075976371765\n",
      "Epoch 2946, Loss: 0.23083922266960144, Final Batch Loss: 0.12487701326608658\n",
      "Epoch 2947, Loss: 0.3075394257903099, Final Batch Loss: 0.18852019309997559\n",
      "Epoch 2948, Loss: 0.3295244425535202, Final Batch Loss: 0.18607890605926514\n",
      "Epoch 2949, Loss: 0.29979661107063293, Final Batch Loss: 0.16313467919826508\n",
      "Epoch 2950, Loss: 0.2742636427283287, Final Batch Loss: 0.11161445826292038\n",
      "Epoch 2951, Loss: 0.27174295485019684, Final Batch Loss: 0.14299413561820984\n",
      "Epoch 2952, Loss: 0.25959615409374237, Final Batch Loss: 0.12552112340927124\n",
      "Epoch 2953, Loss: 0.24040063470602036, Final Batch Loss: 0.10590914636850357\n",
      "Epoch 2954, Loss: 0.28680089116096497, Final Batch Loss: 0.15136486291885376\n",
      "Epoch 2955, Loss: 0.32658885419368744, Final Batch Loss: 0.16024336218833923\n",
      "Epoch 2956, Loss: 0.3115166425704956, Final Batch Loss: 0.16423101723194122\n",
      "Epoch 2957, Loss: 0.3413724899291992, Final Batch Loss: 0.15564297139644623\n",
      "Epoch 2958, Loss: 0.26616765558719635, Final Batch Loss: 0.1306110918521881\n",
      "Epoch 2959, Loss: 0.2999193072319031, Final Batch Loss: 0.16318821907043457\n",
      "Epoch 2960, Loss: 0.28174953162670135, Final Batch Loss: 0.1523156762123108\n",
      "Epoch 2961, Loss: 0.27155424654483795, Final Batch Loss: 0.12112952768802643\n",
      "Epoch 2962, Loss: 0.30199573189020157, Final Batch Loss: 0.18732130527496338\n",
      "Epoch 2963, Loss: 0.2578427493572235, Final Batch Loss: 0.10177013278007507\n",
      "Epoch 2964, Loss: 0.24552063643932343, Final Batch Loss: 0.125543475151062\n",
      "Epoch 2965, Loss: 0.28828585147857666, Final Batch Loss: 0.13902102410793304\n",
      "Epoch 2966, Loss: 0.2933187484741211, Final Batch Loss: 0.139889195561409\n",
      "Epoch 2967, Loss: 0.21727219223976135, Final Batch Loss: 0.09238835424184799\n",
      "Epoch 2968, Loss: 0.2879876494407654, Final Batch Loss: 0.15419180691242218\n",
      "Epoch 2969, Loss: 0.30599160492420197, Final Batch Loss: 0.1634206473827362\n",
      "Epoch 2970, Loss: 0.2192479893565178, Final Batch Loss: 0.11875128746032715\n",
      "Epoch 2971, Loss: 0.2655632421374321, Final Batch Loss: 0.11340466886758804\n",
      "Epoch 2972, Loss: 0.3104882761836052, Final Batch Loss: 0.18744780123233795\n",
      "Epoch 2973, Loss: 0.261727049946785, Final Batch Loss: 0.1630023568868637\n",
      "Epoch 2974, Loss: 0.3218560814857483, Final Batch Loss: 0.1876416802406311\n",
      "Epoch 2975, Loss: 0.31225304305553436, Final Batch Loss: 0.21031539142131805\n",
      "Epoch 2976, Loss: 0.3412158936262131, Final Batch Loss: 0.1745816320180893\n",
      "Epoch 2977, Loss: 0.3020629733800888, Final Batch Loss: 0.1644292175769806\n",
      "Epoch 2978, Loss: 0.31325629353523254, Final Batch Loss: 0.14211687445640564\n",
      "Epoch 2979, Loss: 0.2553998678922653, Final Batch Loss: 0.11997029185295105\n",
      "Epoch 2980, Loss: 0.2967776134610176, Final Batch Loss: 0.17364102602005005\n",
      "Epoch 2981, Loss: 0.2762031555175781, Final Batch Loss: 0.1430085152387619\n",
      "Epoch 2982, Loss: 0.28034990280866623, Final Batch Loss: 0.1175428107380867\n",
      "Epoch 2983, Loss: 0.2898084372282028, Final Batch Loss: 0.15840370953083038\n",
      "Epoch 2984, Loss: 0.3143565282225609, Final Batch Loss: 0.1929401457309723\n",
      "Epoch 2985, Loss: 0.2751326262950897, Final Batch Loss: 0.1658923327922821\n",
      "Epoch 2986, Loss: 0.2626838982105255, Final Batch Loss: 0.1554955095052719\n",
      "Epoch 2987, Loss: 0.2862221524119377, Final Batch Loss: 0.12328586727380753\n",
      "Epoch 2988, Loss: 0.2609662413597107, Final Batch Loss: 0.13341926038265228\n",
      "Epoch 2989, Loss: 0.31057965755462646, Final Batch Loss: 0.1648159772157669\n",
      "Epoch 2990, Loss: 0.2620694041252136, Final Batch Loss: 0.11316005885601044\n",
      "Epoch 2991, Loss: 0.3066100627183914, Final Batch Loss: 0.1580139547586441\n",
      "Epoch 2992, Loss: 0.27826501429080963, Final Batch Loss: 0.13148029148578644\n",
      "Epoch 2993, Loss: 0.26781077682971954, Final Batch Loss: 0.1276853233575821\n",
      "Epoch 2994, Loss: 0.28333738446235657, Final Batch Loss: 0.16603484749794006\n",
      "Epoch 2995, Loss: 0.3315965235233307, Final Batch Loss: 0.194551482796669\n",
      "Epoch 2996, Loss: 0.2784384936094284, Final Batch Loss: 0.16792082786560059\n",
      "Epoch 2997, Loss: 0.3479628264904022, Final Batch Loss: 0.19300271570682526\n",
      "Epoch 2998, Loss: 0.26595786213874817, Final Batch Loss: 0.10287685692310333\n",
      "Epoch 2999, Loss: 0.27931883186101913, Final Batch Loss: 0.08583088964223862\n",
      "Epoch 3000, Loss: 0.26850517094135284, Final Batch Loss: 0.10945308208465576\n",
      "Epoch 3001, Loss: 0.3199222534894943, Final Batch Loss: 0.19362106919288635\n",
      "Epoch 3002, Loss: 0.2457193210721016, Final Batch Loss: 0.10629192739725113\n",
      "Epoch 3003, Loss: 0.2723632752895355, Final Batch Loss: 0.1390789896249771\n",
      "Epoch 3004, Loss: 0.23713625222444534, Final Batch Loss: 0.09926780313253403\n",
      "Epoch 3005, Loss: 0.3041166216135025, Final Batch Loss: 0.1665913313627243\n",
      "Epoch 3006, Loss: 0.2548433914780617, Final Batch Loss: 0.11205316334962845\n",
      "Epoch 3007, Loss: 0.26574110239744186, Final Batch Loss: 0.11638156324625015\n",
      "Epoch 3008, Loss: 0.3033812791109085, Final Batch Loss: 0.16398784518241882\n",
      "Epoch 3009, Loss: 0.27764029055833817, Final Batch Loss: 0.1644553542137146\n",
      "Epoch 3010, Loss: 0.3067575842142105, Final Batch Loss: 0.1711689829826355\n",
      "Epoch 3011, Loss: 0.25109076499938965, Final Batch Loss: 0.11602947115898132\n",
      "Epoch 3012, Loss: 0.3079879730939865, Final Batch Loss: 0.14721643924713135\n",
      "Epoch 3013, Loss: 0.36621204018592834, Final Batch Loss: 0.18946079909801483\n",
      "Epoch 3014, Loss: 0.2963668704032898, Final Batch Loss: 0.14208054542541504\n",
      "Epoch 3015, Loss: 0.23914039880037308, Final Batch Loss: 0.10904394835233688\n",
      "Epoch 3016, Loss: 0.2982887029647827, Final Batch Loss: 0.18727736175060272\n",
      "Epoch 3017, Loss: 0.295839786529541, Final Batch Loss: 0.1396140158176422\n",
      "Epoch 3018, Loss: 0.25255216658115387, Final Batch Loss: 0.1187460720539093\n",
      "Epoch 3019, Loss: 0.3088422194123268, Final Batch Loss: 0.1951896846294403\n",
      "Epoch 3020, Loss: 0.27000652253627777, Final Batch Loss: 0.10797974467277527\n",
      "Epoch 3021, Loss: 0.24049316346645355, Final Batch Loss: 0.12580019235610962\n",
      "Epoch 3022, Loss: 0.31673990190029144, Final Batch Loss: 0.21085305511951447\n",
      "Epoch 3023, Loss: 0.2929416000843048, Final Batch Loss: 0.15822957456111908\n",
      "Epoch 3024, Loss: 0.22755705565214157, Final Batch Loss: 0.0853695496916771\n",
      "Epoch 3025, Loss: 0.3404523432254791, Final Batch Loss: 0.20633834600448608\n",
      "Epoch 3026, Loss: 0.21395861357450485, Final Batch Loss: 0.09204555302858353\n",
      "Epoch 3027, Loss: 0.25703417509794235, Final Batch Loss: 0.09633549302816391\n",
      "Epoch 3028, Loss: 0.24558839201927185, Final Batch Loss: 0.13674698770046234\n",
      "Epoch 3029, Loss: 0.21550308167934418, Final Batch Loss: 0.11870355159044266\n",
      "Epoch 3030, Loss: 0.2818111702799797, Final Batch Loss: 0.12399279326200485\n",
      "Epoch 3031, Loss: 0.2650696709752083, Final Batch Loss: 0.14798086881637573\n",
      "Epoch 3032, Loss: 0.2543855160474777, Final Batch Loss: 0.11913082003593445\n",
      "Epoch 3033, Loss: 0.2739216685295105, Final Batch Loss: 0.1363440454006195\n",
      "Epoch 3034, Loss: 0.33521072566509247, Final Batch Loss: 0.1541702300310135\n",
      "Epoch 3035, Loss: 0.3371584415435791, Final Batch Loss: 0.1418684720993042\n",
      "Epoch 3036, Loss: 0.21532906591892242, Final Batch Loss: 0.1050763726234436\n",
      "Epoch 3037, Loss: 0.29330378770828247, Final Batch Loss: 0.14112238585948944\n",
      "Epoch 3038, Loss: 0.25319843739271164, Final Batch Loss: 0.10391218215227127\n",
      "Epoch 3039, Loss: 0.31092092394828796, Final Batch Loss: 0.13336549699306488\n",
      "Epoch 3040, Loss: 0.238735631108284, Final Batch Loss: 0.11084625124931335\n",
      "Epoch 3041, Loss: 0.23148717731237411, Final Batch Loss: 0.0987376943230629\n",
      "Epoch 3042, Loss: 0.2953244000673294, Final Batch Loss: 0.15958234667778015\n",
      "Epoch 3043, Loss: 0.3358427435159683, Final Batch Loss: 0.1614164113998413\n",
      "Epoch 3044, Loss: 0.3050129860639572, Final Batch Loss: 0.15804411470890045\n",
      "Epoch 3045, Loss: 0.27350766956806183, Final Batch Loss: 0.14130030572414398\n",
      "Epoch 3046, Loss: 0.23906933516263962, Final Batch Loss: 0.10080484300851822\n",
      "Epoch 3047, Loss: 0.3086903840303421, Final Batch Loss: 0.13115708529949188\n",
      "Epoch 3048, Loss: 0.270693838596344, Final Batch Loss: 0.1304382085800171\n",
      "Epoch 3049, Loss: 0.33079151809215546, Final Batch Loss: 0.1660797894001007\n",
      "Epoch 3050, Loss: 0.24397726356983185, Final Batch Loss: 0.13770361244678497\n",
      "Epoch 3051, Loss: 0.24950945377349854, Final Batch Loss: 0.1303453892469406\n",
      "Epoch 3052, Loss: 0.2019171044230461, Final Batch Loss: 0.07234842330217361\n",
      "Epoch 3053, Loss: 0.3440070003271103, Final Batch Loss: 0.17251859605312347\n",
      "Epoch 3054, Loss: 0.24760476499795914, Final Batch Loss: 0.0888710543513298\n",
      "Epoch 3055, Loss: 0.26054226607084274, Final Batch Loss: 0.11362125724554062\n",
      "Epoch 3056, Loss: 0.2879633605480194, Final Batch Loss: 0.12512868642807007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3057, Loss: 0.27020321786403656, Final Batch Loss: 0.13854674994945526\n",
      "Epoch 3058, Loss: 0.246720090508461, Final Batch Loss: 0.11744135618209839\n",
      "Epoch 3059, Loss: 0.24412591010332108, Final Batch Loss: 0.0991469994187355\n",
      "Epoch 3060, Loss: 0.3015042394399643, Final Batch Loss: 0.1405508667230606\n",
      "Epoch 3061, Loss: 0.26537322998046875, Final Batch Loss: 0.14881035685539246\n",
      "Epoch 3062, Loss: 0.2946301996707916, Final Batch Loss: 0.14672806859016418\n",
      "Epoch 3063, Loss: 0.2399691492319107, Final Batch Loss: 0.1275467425584793\n",
      "Epoch 3064, Loss: 0.23657319694757462, Final Batch Loss: 0.11751998960971832\n",
      "Epoch 3065, Loss: 0.3172968327999115, Final Batch Loss: 0.18117573857307434\n",
      "Epoch 3066, Loss: 0.20339203625917435, Final Batch Loss: 0.10647992044687271\n",
      "Epoch 3067, Loss: 0.3248213082551956, Final Batch Loss: 0.19276157021522522\n",
      "Epoch 3068, Loss: 0.3108282685279846, Final Batch Loss: 0.1728052943944931\n",
      "Epoch 3069, Loss: 0.25278329849243164, Final Batch Loss: 0.12896233797073364\n",
      "Epoch 3070, Loss: 0.22443398088216782, Final Batch Loss: 0.11486836522817612\n",
      "Epoch 3071, Loss: 0.2943604812026024, Final Batch Loss: 0.18250562250614166\n",
      "Epoch 3072, Loss: 0.27111683785915375, Final Batch Loss: 0.12065546214580536\n",
      "Epoch 3073, Loss: 0.32129210233688354, Final Batch Loss: 0.1525818109512329\n",
      "Epoch 3074, Loss: 0.21986548602581024, Final Batch Loss: 0.08660194277763367\n",
      "Epoch 3075, Loss: 0.22750354558229446, Final Batch Loss: 0.14846260845661163\n",
      "Epoch 3076, Loss: 0.2742222398519516, Final Batch Loss: 0.14596179127693176\n",
      "Epoch 3077, Loss: 0.2558141127228737, Final Batch Loss: 0.10564211755990982\n",
      "Epoch 3078, Loss: 0.2902566120028496, Final Batch Loss: 0.10978307574987411\n",
      "Epoch 3079, Loss: 0.2277073785662651, Final Batch Loss: 0.1196458488702774\n",
      "Epoch 3080, Loss: 0.42392899096012115, Final Batch Loss: 0.17175988852977753\n",
      "Epoch 3081, Loss: 0.25553837418556213, Final Batch Loss: 0.15323658287525177\n",
      "Epoch 3082, Loss: 0.25767751038074493, Final Batch Loss: 0.15264005959033966\n",
      "Epoch 3083, Loss: 0.2601776719093323, Final Batch Loss: 0.13036994636058807\n",
      "Epoch 3084, Loss: 0.24707220494747162, Final Batch Loss: 0.11246858537197113\n",
      "Epoch 3085, Loss: 0.36798255145549774, Final Batch Loss: 0.24841322004795074\n",
      "Epoch 3086, Loss: 0.24855349212884903, Final Batch Loss: 0.09264055639505386\n",
      "Epoch 3087, Loss: 0.3228875994682312, Final Batch Loss: 0.144524484872818\n",
      "Epoch 3088, Loss: 0.2961147278547287, Final Batch Loss: 0.1556064635515213\n",
      "Epoch 3089, Loss: 0.2885873317718506, Final Batch Loss: 0.16521243751049042\n",
      "Epoch 3090, Loss: 0.37451237440109253, Final Batch Loss: 0.23385505378246307\n",
      "Epoch 3091, Loss: 0.269264817237854, Final Batch Loss: 0.1627112329006195\n",
      "Epoch 3092, Loss: 0.280789814889431, Final Batch Loss: 0.18374963104724884\n",
      "Epoch 3093, Loss: 0.2501569390296936, Final Batch Loss: 0.1257864236831665\n",
      "Epoch 3094, Loss: 0.24064461886882782, Final Batch Loss: 0.09732533991336823\n",
      "Epoch 3095, Loss: 0.3044085055589676, Final Batch Loss: 0.17080406844615936\n",
      "Epoch 3096, Loss: 0.42741450667381287, Final Batch Loss: 0.2335004210472107\n",
      "Epoch 3097, Loss: 0.30861370265483856, Final Batch Loss: 0.16274714469909668\n",
      "Epoch 3098, Loss: 0.35157768428325653, Final Batch Loss: 0.17933252453804016\n",
      "Epoch 3099, Loss: 0.27574993669986725, Final Batch Loss: 0.1315481811761856\n",
      "Epoch 3100, Loss: 0.3344127759337425, Final Batch Loss: 0.23845864832401276\n",
      "Epoch 3101, Loss: 0.2587907165288925, Final Batch Loss: 0.14315363764762878\n",
      "Epoch 3102, Loss: 0.3150486648082733, Final Batch Loss: 0.15773801505565643\n",
      "Epoch 3103, Loss: 0.23202530294656754, Final Batch Loss: 0.11315220594406128\n",
      "Epoch 3104, Loss: 0.23839347809553146, Final Batch Loss: 0.11026006191968918\n",
      "Epoch 3105, Loss: 0.25045096129179, Final Batch Loss: 0.13862542808055878\n",
      "Epoch 3106, Loss: 0.2683461755514145, Final Batch Loss: 0.1325216442346573\n",
      "Epoch 3107, Loss: 0.2549879550933838, Final Batch Loss: 0.17183607816696167\n",
      "Epoch 3108, Loss: 0.3348088264465332, Final Batch Loss: 0.166055366396904\n",
      "Epoch 3109, Loss: 0.21455468982458115, Final Batch Loss: 0.0853867158293724\n",
      "Epoch 3110, Loss: 0.2911502718925476, Final Batch Loss: 0.13752524554729462\n",
      "Epoch 3111, Loss: 0.27069801092147827, Final Batch Loss: 0.1619182974100113\n",
      "Epoch 3112, Loss: 0.29018737375736237, Final Batch Loss: 0.16256895661354065\n",
      "Epoch 3113, Loss: 0.31688106060028076, Final Batch Loss: 0.14585615694522858\n",
      "Epoch 3114, Loss: 0.28808144479990005, Final Batch Loss: 0.16718420386314392\n",
      "Epoch 3115, Loss: 0.2684694081544876, Final Batch Loss: 0.12913863360881805\n",
      "Epoch 3116, Loss: 0.2831178605556488, Final Batch Loss: 0.157377228140831\n",
      "Epoch 3117, Loss: 0.25558852404356003, Final Batch Loss: 0.15988698601722717\n",
      "Epoch 3118, Loss: 0.21959315240383148, Final Batch Loss: 0.09059754014015198\n",
      "Epoch 3119, Loss: 0.2332923337817192, Final Batch Loss: 0.11027918010950089\n",
      "Epoch 3120, Loss: 0.24261613935232162, Final Batch Loss: 0.12987478077411652\n",
      "Epoch 3121, Loss: 0.26153548806905746, Final Batch Loss: 0.16517546772956848\n",
      "Epoch 3122, Loss: 0.2898576855659485, Final Batch Loss: 0.16082227230072021\n",
      "Epoch 3123, Loss: 0.26572373509407043, Final Batch Loss: 0.12848185002803802\n",
      "Epoch 3124, Loss: 0.2764541506767273, Final Batch Loss: 0.1152694970369339\n",
      "Epoch 3125, Loss: 0.2790623903274536, Final Batch Loss: 0.12875962257385254\n",
      "Epoch 3126, Loss: 0.2472475990653038, Final Batch Loss: 0.11303316801786423\n",
      "Epoch 3127, Loss: 0.28008677065372467, Final Batch Loss: 0.14613722264766693\n",
      "Epoch 3128, Loss: 0.24582885950803757, Final Batch Loss: 0.12122540920972824\n",
      "Epoch 3129, Loss: 0.2956436723470688, Final Batch Loss: 0.15214581787586212\n",
      "Epoch 3130, Loss: 0.2601057440042496, Final Batch Loss: 0.1290920078754425\n",
      "Epoch 3131, Loss: 0.3069167733192444, Final Batch Loss: 0.11141641438007355\n",
      "Epoch 3132, Loss: 0.2610424607992172, Final Batch Loss: 0.13876202702522278\n",
      "Epoch 3133, Loss: 0.23311935365200043, Final Batch Loss: 0.1227642223238945\n",
      "Epoch 3134, Loss: 0.3422173708677292, Final Batch Loss: 0.167603999376297\n",
      "Epoch 3135, Loss: 0.269687294960022, Final Batch Loss: 0.143560990691185\n",
      "Epoch 3136, Loss: 0.4202815592288971, Final Batch Loss: 0.2618637681007385\n",
      "Epoch 3137, Loss: 0.27259112894535065, Final Batch Loss: 0.10772398114204407\n",
      "Epoch 3138, Loss: 0.330946609377861, Final Batch Loss: 0.18619081377983093\n",
      "Epoch 3139, Loss: 0.23700223118066788, Final Batch Loss: 0.10174877196550369\n",
      "Epoch 3140, Loss: 0.2772214412689209, Final Batch Loss: 0.17463955283164978\n",
      "Epoch 3141, Loss: 0.3141609728336334, Final Batch Loss: 0.15569578111171722\n",
      "Epoch 3142, Loss: 0.2631576359272003, Final Batch Loss: 0.146432027220726\n",
      "Epoch 3143, Loss: 0.31220610439777374, Final Batch Loss: 0.1540137380361557\n",
      "Epoch 3144, Loss: 0.2514856234192848, Final Batch Loss: 0.1508251577615738\n",
      "Epoch 3145, Loss: 0.27897635102272034, Final Batch Loss: 0.15236671268939972\n",
      "Epoch 3146, Loss: 0.24160662293434143, Final Batch Loss: 0.13839471340179443\n",
      "Epoch 3147, Loss: 0.2578001171350479, Final Batch Loss: 0.14540719985961914\n",
      "Epoch 3148, Loss: 0.26964765787124634, Final Batch Loss: 0.1342504471540451\n",
      "Epoch 3149, Loss: 0.27509841322898865, Final Batch Loss: 0.1419297158718109\n",
      "Epoch 3150, Loss: 0.343131959438324, Final Batch Loss: 0.1800307184457779\n",
      "Epoch 3151, Loss: 0.29239651560783386, Final Batch Loss: 0.15988841652870178\n",
      "Epoch 3152, Loss: 0.2910543158650398, Final Batch Loss: 0.11080556362867355\n",
      "Epoch 3153, Loss: 0.3349791318178177, Final Batch Loss: 0.12975598871707916\n",
      "Epoch 3154, Loss: 0.24184488505125046, Final Batch Loss: 0.12490564584732056\n",
      "Epoch 3155, Loss: 0.2536817267537117, Final Batch Loss: 0.13331308960914612\n",
      "Epoch 3156, Loss: 0.3119724839925766, Final Batch Loss: 0.16137194633483887\n",
      "Epoch 3157, Loss: 0.3062007948756218, Final Batch Loss: 0.20374447107315063\n",
      "Epoch 3158, Loss: 0.2669551223516464, Final Batch Loss: 0.15023407340049744\n",
      "Epoch 3159, Loss: 0.2824178636074066, Final Batch Loss: 0.1416938155889511\n",
      "Epoch 3160, Loss: 0.3643491715192795, Final Batch Loss: 0.1853625476360321\n",
      "Epoch 3161, Loss: 0.28228316456079483, Final Batch Loss: 0.16704203188419342\n",
      "Epoch 3162, Loss: 0.271956130862236, Final Batch Loss: 0.16332510113716125\n",
      "Epoch 3163, Loss: 0.23811869323253632, Final Batch Loss: 0.1131572425365448\n",
      "Epoch 3164, Loss: 0.26393933594226837, Final Batch Loss: 0.13774612545967102\n",
      "Epoch 3165, Loss: 0.23258696496486664, Final Batch Loss: 0.07519207894802094\n",
      "Epoch 3166, Loss: 0.2653636634349823, Final Batch Loss: 0.10794824361801147\n",
      "Epoch 3167, Loss: 0.3077132999897003, Final Batch Loss: 0.1407882571220398\n",
      "Epoch 3168, Loss: 0.2944522053003311, Final Batch Loss: 0.16599038243293762\n",
      "Epoch 3169, Loss: 0.3211338371038437, Final Batch Loss: 0.18769565224647522\n",
      "Epoch 3170, Loss: 0.26233550906181335, Final Batch Loss: 0.15094754099845886\n",
      "Epoch 3171, Loss: 0.23138441890478134, Final Batch Loss: 0.0801762118935585\n",
      "Epoch 3172, Loss: 0.24510904401540756, Final Batch Loss: 0.0964595302939415\n",
      "Epoch 3173, Loss: 0.24523460119962692, Final Batch Loss: 0.14005382359027863\n",
      "Epoch 3174, Loss: 0.2566532492637634, Final Batch Loss: 0.14690247178077698\n",
      "Epoch 3175, Loss: 0.2752314358949661, Final Batch Loss: 0.16144560277462006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3176, Loss: 0.2950325310230255, Final Batch Loss: 0.16672557592391968\n",
      "Epoch 3177, Loss: 0.2613904997706413, Final Batch Loss: 0.13917063176631927\n",
      "Epoch 3178, Loss: 0.29461444914340973, Final Batch Loss: 0.1599850058555603\n",
      "Epoch 3179, Loss: 0.30027393996715546, Final Batch Loss: 0.15356476604938507\n",
      "Epoch 3180, Loss: 0.31956715881824493, Final Batch Loss: 0.17763090133666992\n",
      "Epoch 3181, Loss: 0.2666046768426895, Final Batch Loss: 0.1334528625011444\n",
      "Epoch 3182, Loss: 0.22369636595249176, Final Batch Loss: 0.10140451788902283\n",
      "Epoch 3183, Loss: 0.26616138964891434, Final Batch Loss: 0.11413728445768356\n",
      "Epoch 3184, Loss: 0.2795071452856064, Final Batch Loss: 0.18495282530784607\n",
      "Epoch 3185, Loss: 0.2656165063381195, Final Batch Loss: 0.14281955361366272\n",
      "Epoch 3186, Loss: 0.2988852560520172, Final Batch Loss: 0.18732322752475739\n",
      "Epoch 3187, Loss: 0.27451087534427643, Final Batch Loss: 0.11083976924419403\n",
      "Epoch 3188, Loss: 0.29661886394023895, Final Batch Loss: 0.12210319936275482\n",
      "Epoch 3189, Loss: 0.261616051197052, Final Batch Loss: 0.1260632425546646\n",
      "Epoch 3190, Loss: 0.27072224766016006, Final Batch Loss: 0.07747986167669296\n",
      "Epoch 3191, Loss: 0.3004513829946518, Final Batch Loss: 0.18493013083934784\n",
      "Epoch 3192, Loss: 0.23966354876756668, Final Batch Loss: 0.09770870953798294\n",
      "Epoch 3193, Loss: 0.26573263108730316, Final Batch Loss: 0.09987170994281769\n",
      "Epoch 3194, Loss: 0.27179187536239624, Final Batch Loss: 0.12492005527019501\n",
      "Epoch 3195, Loss: 0.22264191508293152, Final Batch Loss: 0.11228986829519272\n",
      "Epoch 3196, Loss: 0.2323821559548378, Final Batch Loss: 0.11034925282001495\n",
      "Epoch 3197, Loss: 0.22532415390014648, Final Batch Loss: 0.0893707424402237\n",
      "Epoch 3198, Loss: 0.2844686061143875, Final Batch Loss: 0.1468043327331543\n",
      "Epoch 3199, Loss: 0.18551382422447205, Final Batch Loss: 0.08844386786222458\n",
      "Epoch 3200, Loss: 0.2320067659020424, Final Batch Loss: 0.09147273749113083\n",
      "Epoch 3201, Loss: 0.3005846291780472, Final Batch Loss: 0.17751885950565338\n",
      "Epoch 3202, Loss: 0.2568705379962921, Final Batch Loss: 0.13301217555999756\n",
      "Epoch 3203, Loss: 0.24560688436031342, Final Batch Loss: 0.1297590434551239\n",
      "Epoch 3204, Loss: 0.23966530710458755, Final Batch Loss: 0.12816239893436432\n",
      "Epoch 3205, Loss: 0.2641061246395111, Final Batch Loss: 0.15556249022483826\n",
      "Epoch 3206, Loss: 0.25298553705215454, Final Batch Loss: 0.1347058117389679\n",
      "Epoch 3207, Loss: 0.239017553627491, Final Batch Loss: 0.11754804104566574\n",
      "Epoch 3208, Loss: 0.24494387209415436, Final Batch Loss: 0.12915971875190735\n",
      "Epoch 3209, Loss: 0.2748313844203949, Final Batch Loss: 0.14978352189064026\n",
      "Epoch 3210, Loss: 0.24252527207136154, Final Batch Loss: 0.10552146285772324\n",
      "Epoch 3211, Loss: 0.24306654930114746, Final Batch Loss: 0.12185516953468323\n",
      "Epoch 3212, Loss: 0.23917005211114883, Final Batch Loss: 0.09853579849004745\n",
      "Epoch 3213, Loss: 0.23304661363363266, Final Batch Loss: 0.09288123995065689\n",
      "Epoch 3214, Loss: 0.23806088417768478, Final Batch Loss: 0.14688740670681\n",
      "Epoch 3215, Loss: 0.3810858130455017, Final Batch Loss: 0.19592684507369995\n",
      "Epoch 3216, Loss: 0.26362747699022293, Final Batch Loss: 0.14480304718017578\n",
      "Epoch 3217, Loss: 0.2604389190673828, Final Batch Loss: 0.17440611124038696\n",
      "Epoch 3218, Loss: 0.2909797504544258, Final Batch Loss: 0.1901550143957138\n",
      "Epoch 3219, Loss: 0.2472185268998146, Final Batch Loss: 0.10540982335805893\n",
      "Epoch 3220, Loss: 0.2544669136404991, Final Batch Loss: 0.09956451505422592\n",
      "Epoch 3221, Loss: 0.26088956743478775, Final Batch Loss: 0.11310646682977676\n",
      "Epoch 3222, Loss: 0.2730003297328949, Final Batch Loss: 0.19751589000225067\n",
      "Epoch 3223, Loss: 0.25304003804922104, Final Batch Loss: 0.11403708904981613\n",
      "Epoch 3224, Loss: 0.27352504432201385, Final Batch Loss: 0.1551535427570343\n",
      "Epoch 3225, Loss: 0.2963731437921524, Final Batch Loss: 0.128301739692688\n",
      "Epoch 3226, Loss: 0.3337753862142563, Final Batch Loss: 0.17578962445259094\n",
      "Epoch 3227, Loss: 0.2438981682062149, Final Batch Loss: 0.11745499074459076\n",
      "Epoch 3228, Loss: 0.31210121512413025, Final Batch Loss: 0.13857758045196533\n",
      "Epoch 3229, Loss: 0.2899567037820816, Final Batch Loss: 0.1597215086221695\n",
      "Epoch 3230, Loss: 0.3038860112428665, Final Batch Loss: 0.1226278692483902\n",
      "Epoch 3231, Loss: 0.30678150057792664, Final Batch Loss: 0.1572045087814331\n",
      "Epoch 3232, Loss: 0.3000211864709854, Final Batch Loss: 0.14954471588134766\n",
      "Epoch 3233, Loss: 0.2788987159729004, Final Batch Loss: 0.15819989144802094\n",
      "Epoch 3234, Loss: 0.2738534212112427, Final Batch Loss: 0.13537076115608215\n",
      "Epoch 3235, Loss: 0.227031908929348, Final Batch Loss: 0.12399085611104965\n",
      "Epoch 3236, Loss: 0.28178535401821136, Final Batch Loss: 0.14765043556690216\n",
      "Epoch 3237, Loss: 0.24158941954374313, Final Batch Loss: 0.1085723266005516\n",
      "Epoch 3238, Loss: 0.285376638174057, Final Batch Loss: 0.16108214855194092\n",
      "Epoch 3239, Loss: 0.22460676729679108, Final Batch Loss: 0.11527982354164124\n",
      "Epoch 3240, Loss: 0.29051797091960907, Final Batch Loss: 0.15123575925827026\n",
      "Epoch 3241, Loss: 0.28955671191215515, Final Batch Loss: 0.12767185270786285\n",
      "Epoch 3242, Loss: 0.2808569446206093, Final Batch Loss: 0.12212613970041275\n",
      "Epoch 3243, Loss: 0.26370398700237274, Final Batch Loss: 0.11983698606491089\n",
      "Epoch 3244, Loss: 0.34470366686582565, Final Batch Loss: 0.0975487008690834\n",
      "Epoch 3245, Loss: 0.30061014741659164, Final Batch Loss: 0.17953559756278992\n",
      "Epoch 3246, Loss: 0.2667043134570122, Final Batch Loss: 0.14516204595565796\n",
      "Epoch 3247, Loss: 0.2352389544248581, Final Batch Loss: 0.114990234375\n",
      "Epoch 3248, Loss: 0.2507174015045166, Final Batch Loss: 0.1607096642255783\n",
      "Epoch 3249, Loss: 0.22768433392047882, Final Batch Loss: 0.10403037816286087\n",
      "Epoch 3250, Loss: 0.27686768025159836, Final Batch Loss: 0.18237733840942383\n",
      "Epoch 3251, Loss: 0.3538585305213928, Final Batch Loss: 0.2182263880968094\n",
      "Epoch 3252, Loss: 0.29399681091308594, Final Batch Loss: 0.14777952432632446\n",
      "Epoch 3253, Loss: 0.24478184431791306, Final Batch Loss: 0.11587736755609512\n",
      "Epoch 3254, Loss: 0.31079133599996567, Final Batch Loss: 0.19595491886138916\n",
      "Epoch 3255, Loss: 0.25545143336057663, Final Batch Loss: 0.10767639428377151\n",
      "Epoch 3256, Loss: 0.24977868795394897, Final Batch Loss: 0.1414547711610794\n",
      "Epoch 3257, Loss: 0.2401936650276184, Final Batch Loss: 0.11361391842365265\n",
      "Epoch 3258, Loss: 0.2526782378554344, Final Batch Loss: 0.1458355188369751\n",
      "Epoch 3259, Loss: 0.26670342683792114, Final Batch Loss: 0.1348496526479721\n",
      "Epoch 3260, Loss: 0.31648020446300507, Final Batch Loss: 0.12919634580612183\n",
      "Epoch 3261, Loss: 0.23037083446979523, Final Batch Loss: 0.10275676846504211\n",
      "Epoch 3262, Loss: 0.22148428857326508, Final Batch Loss: 0.13285909593105316\n",
      "Epoch 3263, Loss: 0.261163130402565, Final Batch Loss: 0.12787939608097076\n",
      "Epoch 3264, Loss: 0.27979233115911484, Final Batch Loss: 0.1569562554359436\n",
      "Epoch 3265, Loss: 0.3158865123987198, Final Batch Loss: 0.1563543677330017\n",
      "Epoch 3266, Loss: 0.2652950584888458, Final Batch Loss: 0.14778093993663788\n",
      "Epoch 3267, Loss: 0.22547746449708939, Final Batch Loss: 0.09173164516687393\n",
      "Epoch 3268, Loss: 0.25325820595026016, Final Batch Loss: 0.13551191985607147\n",
      "Epoch 3269, Loss: 0.25996115803718567, Final Batch Loss: 0.1256192922592163\n",
      "Epoch 3270, Loss: 0.2120947316288948, Final Batch Loss: 0.09720788896083832\n",
      "Epoch 3271, Loss: 0.26672378182411194, Final Batch Loss: 0.11407230794429779\n",
      "Epoch 3272, Loss: 0.22715634107589722, Final Batch Loss: 0.116329625248909\n",
      "Epoch 3273, Loss: 0.35045862197875977, Final Batch Loss: 0.19910171627998352\n",
      "Epoch 3274, Loss: 0.27128228545188904, Final Batch Loss: 0.1062099039554596\n",
      "Epoch 3275, Loss: 0.27276184409856796, Final Batch Loss: 0.17576630413532257\n",
      "Epoch 3276, Loss: 0.2649024873971939, Final Batch Loss: 0.13930243253707886\n",
      "Epoch 3277, Loss: 0.27460993081331253, Final Batch Loss: 0.11952660232782364\n",
      "Epoch 3278, Loss: 0.20604486018419266, Final Batch Loss: 0.09166406095027924\n",
      "Epoch 3279, Loss: 0.25773727148771286, Final Batch Loss: 0.13910293579101562\n",
      "Epoch 3280, Loss: 0.3021869510412216, Final Batch Loss: 0.13152512907981873\n",
      "Epoch 3281, Loss: 0.2550230994820595, Final Batch Loss: 0.1314975470304489\n",
      "Epoch 3282, Loss: 0.2295537143945694, Final Batch Loss: 0.10881657153367996\n",
      "Epoch 3283, Loss: 0.232601560652256, Final Batch Loss: 0.1461368054151535\n",
      "Epoch 3284, Loss: 0.30214419960975647, Final Batch Loss: 0.14307519793510437\n",
      "Epoch 3285, Loss: 0.32452182471752167, Final Batch Loss: 0.18820998072624207\n",
      "Epoch 3286, Loss: 0.28825055062770844, Final Batch Loss: 0.17411117255687714\n",
      "Epoch 3287, Loss: 0.2728745713829994, Final Batch Loss: 0.12249656766653061\n",
      "Epoch 3288, Loss: 0.23064131289720535, Final Batch Loss: 0.1178864911198616\n",
      "Epoch 3289, Loss: 0.24112912267446518, Final Batch Loss: 0.11791806668043137\n",
      "Epoch 3290, Loss: 0.2543049678206444, Final Batch Loss: 0.1324121057987213\n",
      "Epoch 3291, Loss: 0.2508852928876877, Final Batch Loss: 0.11000886559486389\n",
      "Epoch 3292, Loss: 0.2994173765182495, Final Batch Loss: 0.12763144075870514\n",
      "Epoch 3293, Loss: 0.2538866028189659, Final Batch Loss: 0.10316357761621475\n",
      "Epoch 3294, Loss: 0.25976405292749405, Final Batch Loss: 0.12410049885511398\n",
      "Epoch 3295, Loss: 0.25833039730787277, Final Batch Loss: 0.1192193552851677\n",
      "Epoch 3296, Loss: 0.298855222761631, Final Batch Loss: 0.11811160296201706\n",
      "Epoch 3297, Loss: 0.263778381049633, Final Batch Loss: 0.11864101141691208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3298, Loss: 0.29230304062366486, Final Batch Loss: 0.15153175592422485\n",
      "Epoch 3299, Loss: 0.2818280905485153, Final Batch Loss: 0.12945201992988586\n",
      "Epoch 3300, Loss: 0.2916889488697052, Final Batch Loss: 0.16527646780014038\n",
      "Epoch 3301, Loss: 0.26470252871513367, Final Batch Loss: 0.12965987622737885\n",
      "Epoch 3302, Loss: 0.25410331785678864, Final Batch Loss: 0.11352865397930145\n",
      "Epoch 3303, Loss: 0.2631179615855217, Final Batch Loss: 0.12237963825464249\n",
      "Epoch 3304, Loss: 0.21806961297988892, Final Batch Loss: 0.10585174709558487\n",
      "Epoch 3305, Loss: 0.30005818605422974, Final Batch Loss: 0.12049466371536255\n",
      "Epoch 3306, Loss: 0.2850927412509918, Final Batch Loss: 0.13852377235889435\n",
      "Epoch 3307, Loss: 0.22886063158512115, Final Batch Loss: 0.119779571890831\n",
      "Epoch 3308, Loss: 0.265906922519207, Final Batch Loss: 0.11667286604642868\n",
      "Epoch 3309, Loss: 0.2532772421836853, Final Batch Loss: 0.14886943995952606\n",
      "Epoch 3310, Loss: 0.2495049238204956, Final Batch Loss: 0.08624179661273956\n",
      "Epoch 3311, Loss: 0.2772955149412155, Final Batch Loss: 0.17858968675136566\n",
      "Epoch 3312, Loss: 0.2454630434513092, Final Batch Loss: 0.11839847266674042\n",
      "Epoch 3313, Loss: 0.358119398355484, Final Batch Loss: 0.2095606029033661\n",
      "Epoch 3314, Loss: 0.2713198810815811, Final Batch Loss: 0.10165716707706451\n",
      "Epoch 3315, Loss: 0.29042187333106995, Final Batch Loss: 0.13216611742973328\n",
      "Epoch 3316, Loss: 0.3475649207830429, Final Batch Loss: 0.23257626593112946\n",
      "Epoch 3317, Loss: 0.3229568302631378, Final Batch Loss: 0.1728101670742035\n",
      "Epoch 3318, Loss: 0.1868385151028633, Final Batch Loss: 0.07609840482473373\n",
      "Epoch 3319, Loss: 0.3047066330909729, Final Batch Loss: 0.14251846075057983\n",
      "Epoch 3320, Loss: 0.2232516035437584, Final Batch Loss: 0.11409647017717361\n",
      "Epoch 3321, Loss: 0.2744915932416916, Final Batch Loss: 0.13247495889663696\n",
      "Epoch 3322, Loss: 0.3468685448169708, Final Batch Loss: 0.17627587914466858\n",
      "Epoch 3323, Loss: 0.26453477144241333, Final Batch Loss: 0.12044037878513336\n",
      "Epoch 3324, Loss: 0.27500665187835693, Final Batch Loss: 0.18284212052822113\n",
      "Epoch 3325, Loss: 0.28217969834804535, Final Batch Loss: 0.1121104508638382\n",
      "Epoch 3326, Loss: 0.1925954893231392, Final Batch Loss: 0.07529203593730927\n",
      "Epoch 3327, Loss: 0.23694146424531937, Final Batch Loss: 0.10288407653570175\n",
      "Epoch 3328, Loss: 0.25111527740955353, Final Batch Loss: 0.13892862200737\n",
      "Epoch 3329, Loss: 0.2895985096693039, Final Batch Loss: 0.16412992775440216\n",
      "Epoch 3330, Loss: 0.22476497292518616, Final Batch Loss: 0.09088453650474548\n",
      "Epoch 3331, Loss: 0.21819164603948593, Final Batch Loss: 0.10681173205375671\n",
      "Epoch 3332, Loss: 0.2024303525686264, Final Batch Loss: 0.08838988840579987\n",
      "Epoch 3333, Loss: 0.24998340010643005, Final Batch Loss: 0.10250911116600037\n",
      "Epoch 3334, Loss: 0.250347338616848, Final Batch Loss: 0.10505224019289017\n",
      "Epoch 3335, Loss: 0.25290558487176895, Final Batch Loss: 0.1197822317481041\n",
      "Epoch 3336, Loss: 0.2575399875640869, Final Batch Loss: 0.1287321299314499\n",
      "Epoch 3337, Loss: 0.3210698664188385, Final Batch Loss: 0.20857980847358704\n",
      "Epoch 3338, Loss: 0.24031136184930801, Final Batch Loss: 0.147734135389328\n",
      "Epoch 3339, Loss: 0.2627278119325638, Final Batch Loss: 0.14007630944252014\n",
      "Epoch 3340, Loss: 0.2555813193321228, Final Batch Loss: 0.14106251299381256\n",
      "Epoch 3341, Loss: 0.26826731860637665, Final Batch Loss: 0.13592080771923065\n",
      "Epoch 3342, Loss: 0.27628107368946075, Final Batch Loss: 0.15979737043380737\n",
      "Epoch 3343, Loss: 0.2455901801586151, Final Batch Loss: 0.10188910365104675\n",
      "Epoch 3344, Loss: 0.3132112920284271, Final Batch Loss: 0.15719819068908691\n",
      "Epoch 3345, Loss: 0.23829609155654907, Final Batch Loss: 0.13592754304409027\n",
      "Epoch 3346, Loss: 0.2828618437051773, Final Batch Loss: 0.18086513876914978\n",
      "Epoch 3347, Loss: 0.25894009321928024, Final Batch Loss: 0.12135962396860123\n",
      "Epoch 3348, Loss: 0.33066345751285553, Final Batch Loss: 0.1876361072063446\n",
      "Epoch 3349, Loss: 0.2304297611117363, Final Batch Loss: 0.07274984568357468\n",
      "Epoch 3350, Loss: 0.2999713569879532, Final Batch Loss: 0.17036229372024536\n",
      "Epoch 3351, Loss: 0.23151080310344696, Final Batch Loss: 0.07937873899936676\n",
      "Epoch 3352, Loss: 0.21816745400428772, Final Batch Loss: 0.11790945380926132\n",
      "Epoch 3353, Loss: 0.20851946622133255, Final Batch Loss: 0.11934413760900497\n",
      "Epoch 3354, Loss: 0.25539860874414444, Final Batch Loss: 0.14423449337482452\n",
      "Epoch 3355, Loss: 0.2575303912162781, Final Batch Loss: 0.12874071300029755\n",
      "Epoch 3356, Loss: 0.2503371685743332, Final Batch Loss: 0.1202428787946701\n",
      "Epoch 3357, Loss: 0.23216532915830612, Final Batch Loss: 0.11018375307321548\n",
      "Epoch 3358, Loss: 0.23409971594810486, Final Batch Loss: 0.10785715281963348\n",
      "Epoch 3359, Loss: 0.22045131027698517, Final Batch Loss: 0.11954592913389206\n",
      "Epoch 3360, Loss: 0.20396853238344193, Final Batch Loss: 0.09295333921909332\n",
      "Epoch 3361, Loss: 0.29418718814849854, Final Batch Loss: 0.15900073945522308\n",
      "Epoch 3362, Loss: 0.24041161686182022, Final Batch Loss: 0.10325352102518082\n",
      "Epoch 3363, Loss: 0.33277907967567444, Final Batch Loss: 0.13631783425807953\n",
      "Epoch 3364, Loss: 0.20548248291015625, Final Batch Loss: 0.10664801299571991\n",
      "Epoch 3365, Loss: 0.30542468279600143, Final Batch Loss: 0.1131553128361702\n",
      "Epoch 3366, Loss: 0.2856748476624489, Final Batch Loss: 0.16897739470005035\n",
      "Epoch 3367, Loss: 0.26599037647247314, Final Batch Loss: 0.16347931325435638\n",
      "Epoch 3368, Loss: 0.24120495468378067, Final Batch Loss: 0.14580219984054565\n",
      "Epoch 3369, Loss: 0.24207434058189392, Final Batch Loss: 0.11662673950195312\n",
      "Epoch 3370, Loss: 0.22765762358903885, Final Batch Loss: 0.09639003127813339\n",
      "Epoch 3371, Loss: 0.2571524679660797, Final Batch Loss: 0.1322881281375885\n",
      "Epoch 3372, Loss: 0.2739269286394119, Final Batch Loss: 0.13652494549751282\n",
      "Epoch 3373, Loss: 0.21905095875263214, Final Batch Loss: 0.09118615090847015\n",
      "Epoch 3374, Loss: 0.2577420398592949, Final Batch Loss: 0.08935298770666122\n",
      "Epoch 3375, Loss: 0.2918589115142822, Final Batch Loss: 0.14120163023471832\n",
      "Epoch 3376, Loss: 0.2342822551727295, Final Batch Loss: 0.1129596084356308\n",
      "Epoch 3377, Loss: 0.3138592094182968, Final Batch Loss: 0.14797088503837585\n",
      "Epoch 3378, Loss: 0.226826973259449, Final Batch Loss: 0.1354876309633255\n",
      "Epoch 3379, Loss: 0.2963293790817261, Final Batch Loss: 0.1422751247882843\n",
      "Epoch 3380, Loss: 0.21604248881340027, Final Batch Loss: 0.12600603699684143\n",
      "Epoch 3381, Loss: 0.25090305507183075, Final Batch Loss: 0.11066828668117523\n",
      "Epoch 3382, Loss: 0.23654258996248245, Final Batch Loss: 0.11622186750173569\n",
      "Epoch 3383, Loss: 0.27494189143180847, Final Batch Loss: 0.14970959722995758\n",
      "Epoch 3384, Loss: 0.28642454743385315, Final Batch Loss: 0.13617771863937378\n",
      "Epoch 3385, Loss: 0.1996798738837242, Final Batch Loss: 0.0734434649348259\n",
      "Epoch 3386, Loss: 0.24100730568170547, Final Batch Loss: 0.13351432979106903\n",
      "Epoch 3387, Loss: 0.2733796760439873, Final Batch Loss: 0.10925666242837906\n",
      "Epoch 3388, Loss: 0.25949154049158096, Final Batch Loss: 0.14797233045101166\n",
      "Epoch 3389, Loss: 0.23430675268173218, Final Batch Loss: 0.12371005117893219\n",
      "Epoch 3390, Loss: 0.19942273199558258, Final Batch Loss: 0.09536164253950119\n",
      "Epoch 3391, Loss: 0.2763017863035202, Final Batch Loss: 0.1180720329284668\n",
      "Epoch 3392, Loss: 0.22045649588108063, Final Batch Loss: 0.09796730428934097\n",
      "Epoch 3393, Loss: 0.29167190194129944, Final Batch Loss: 0.15173378586769104\n",
      "Epoch 3394, Loss: 0.2527197375893593, Final Batch Loss: 0.1332124024629593\n",
      "Epoch 3395, Loss: 0.25639808177948, Final Batch Loss: 0.1485425978899002\n",
      "Epoch 3396, Loss: 0.27328869700431824, Final Batch Loss: 0.13878917694091797\n",
      "Epoch 3397, Loss: 0.23721391707658768, Final Batch Loss: 0.1315183788537979\n",
      "Epoch 3398, Loss: 0.20165371894836426, Final Batch Loss: 0.08935245126485825\n",
      "Epoch 3399, Loss: 0.28946395218372345, Final Batch Loss: 0.12787896394729614\n",
      "Epoch 3400, Loss: 0.2597736790776253, Final Batch Loss: 0.1365983784198761\n",
      "Epoch 3401, Loss: 0.27464862912893295, Final Batch Loss: 0.16644659638404846\n",
      "Epoch 3402, Loss: 0.20890690386295319, Final Batch Loss: 0.08799828588962555\n",
      "Epoch 3403, Loss: 0.26263897120952606, Final Batch Loss: 0.17018091678619385\n",
      "Epoch 3404, Loss: 0.23930773884058, Final Batch Loss: 0.13433484733104706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3405, Loss: 0.2555248290300369, Final Batch Loss: 0.13941901922225952\n",
      "Epoch 3406, Loss: 0.2320733293890953, Final Batch Loss: 0.13640552759170532\n",
      "Epoch 3407, Loss: 0.2651449292898178, Final Batch Loss: 0.14125578105449677\n",
      "Epoch 3408, Loss: 0.21424254775047302, Final Batch Loss: 0.08261577785015106\n",
      "Epoch 3409, Loss: 0.20900865644216537, Final Batch Loss: 0.11650077253580093\n",
      "Epoch 3410, Loss: 0.2609308585524559, Final Batch Loss: 0.10896659642457962\n",
      "Epoch 3411, Loss: 0.22255855053663254, Final Batch Loss: 0.11023178696632385\n",
      "Epoch 3412, Loss: 0.2467348948121071, Final Batch Loss: 0.130624458193779\n",
      "Epoch 3413, Loss: 0.32367388904094696, Final Batch Loss: 0.17952807247638702\n",
      "Epoch 3414, Loss: 0.2528829425573349, Final Batch Loss: 0.1274038404226303\n",
      "Epoch 3415, Loss: 0.18772534281015396, Final Batch Loss: 0.09991630911827087\n",
      "Epoch 3416, Loss: 0.23566102236509323, Final Batch Loss: 0.10539152473211288\n",
      "Epoch 3417, Loss: 0.2545621246099472, Final Batch Loss: 0.13130347430706024\n",
      "Epoch 3418, Loss: 0.271200492978096, Final Batch Loss: 0.1232011616230011\n",
      "Epoch 3419, Loss: 0.2953133136034012, Final Batch Loss: 0.15221978724002838\n",
      "Epoch 3420, Loss: 0.2055763602256775, Final Batch Loss: 0.1158212274312973\n",
      "Epoch 3421, Loss: 0.21948154270648956, Final Batch Loss: 0.10202721506357193\n",
      "Epoch 3422, Loss: 0.24155815690755844, Final Batch Loss: 0.1324186623096466\n",
      "Epoch 3423, Loss: 0.32998423278331757, Final Batch Loss: 0.18808157742023468\n",
      "Epoch 3424, Loss: 0.24510610103607178, Final Batch Loss: 0.1120649129152298\n",
      "Epoch 3425, Loss: 0.23384805768728256, Final Batch Loss: 0.10378838330507278\n",
      "Epoch 3426, Loss: 0.25533269345760345, Final Batch Loss: 0.11558140814304352\n",
      "Epoch 3427, Loss: 0.20941203832626343, Final Batch Loss: 0.08716569095849991\n",
      "Epoch 3428, Loss: 0.23771552741527557, Final Batch Loss: 0.11934038251638412\n",
      "Epoch 3429, Loss: 0.242025688290596, Final Batch Loss: 0.07959769666194916\n",
      "Epoch 3430, Loss: 0.2427579089999199, Final Batch Loss: 0.11415330320596695\n",
      "Epoch 3431, Loss: 0.25238627195358276, Final Batch Loss: 0.14028863608837128\n",
      "Epoch 3432, Loss: 0.2292931005358696, Final Batch Loss: 0.13329535722732544\n",
      "Epoch 3433, Loss: 0.24349922686815262, Final Batch Loss: 0.1113768145442009\n",
      "Epoch 3434, Loss: 0.2501858174800873, Final Batch Loss: 0.13244491815567017\n",
      "Epoch 3435, Loss: 0.227821946144104, Final Batch Loss: 0.09397706389427185\n",
      "Epoch 3436, Loss: 0.2664647027850151, Final Batch Loss: 0.1217205598950386\n",
      "Epoch 3437, Loss: 0.24055739492177963, Final Batch Loss: 0.09913580864667892\n",
      "Epoch 3438, Loss: 0.2863895446062088, Final Batch Loss: 0.15107981860637665\n",
      "Epoch 3439, Loss: 0.22672465443611145, Final Batch Loss: 0.10193990916013718\n",
      "Epoch 3440, Loss: 0.24216872453689575, Final Batch Loss: 0.1430397480726242\n",
      "Epoch 3441, Loss: 0.22934170067310333, Final Batch Loss: 0.10285860300064087\n",
      "Epoch 3442, Loss: 0.2308865189552307, Final Batch Loss: 0.09937535226345062\n",
      "Epoch 3443, Loss: 0.2655201032757759, Final Batch Loss: 0.10783090442419052\n",
      "Epoch 3444, Loss: 0.29792695492506027, Final Batch Loss: 0.19503317773342133\n",
      "Epoch 3445, Loss: 0.22584253549575806, Final Batch Loss: 0.11724457889795303\n",
      "Epoch 3446, Loss: 0.2733779177069664, Final Batch Loss: 0.15192878246307373\n",
      "Epoch 3447, Loss: 0.2516256421804428, Final Batch Loss: 0.13613587617874146\n",
      "Epoch 3448, Loss: 0.20878470689058304, Final Batch Loss: 0.10919144749641418\n",
      "Epoch 3449, Loss: 0.22840367257595062, Final Batch Loss: 0.10549188405275345\n",
      "Epoch 3450, Loss: 0.24781004339456558, Final Batch Loss: 0.11906615644693375\n",
      "Epoch 3451, Loss: 0.2528531029820442, Final Batch Loss: 0.11177840083837509\n",
      "Epoch 3452, Loss: 0.35613739490509033, Final Batch Loss: 0.18896812200546265\n",
      "Epoch 3453, Loss: 0.22071436047554016, Final Batch Loss: 0.09366729855537415\n",
      "Epoch 3454, Loss: 0.2561711445450783, Final Batch Loss: 0.1389700025320053\n",
      "Epoch 3455, Loss: 0.2285356968641281, Final Batch Loss: 0.11126985400915146\n",
      "Epoch 3456, Loss: 0.27086593955755234, Final Batch Loss: 0.15270157158374786\n",
      "Epoch 3457, Loss: 0.20027730613946915, Final Batch Loss: 0.09145011007785797\n",
      "Epoch 3458, Loss: 0.26955828070640564, Final Batch Loss: 0.1388099193572998\n",
      "Epoch 3459, Loss: 0.25724103301763535, Final Batch Loss: 0.12472055107355118\n",
      "Epoch 3460, Loss: 0.2759154438972473, Final Batch Loss: 0.11552566289901733\n",
      "Epoch 3461, Loss: 0.2445957437157631, Final Batch Loss: 0.09715116769075394\n",
      "Epoch 3462, Loss: 0.2809613347053528, Final Batch Loss: 0.14276358485221863\n",
      "Epoch 3463, Loss: 0.19942810386419296, Final Batch Loss: 0.07753840833902359\n",
      "Epoch 3464, Loss: 0.31637655198574066, Final Batch Loss: 0.15503385663032532\n",
      "Epoch 3465, Loss: 0.23728731274604797, Final Batch Loss: 0.14512963593006134\n",
      "Epoch 3466, Loss: 0.250291608273983, Final Batch Loss: 0.11368223279714584\n",
      "Epoch 3467, Loss: 0.3030610680580139, Final Batch Loss: 0.12065014243125916\n",
      "Epoch 3468, Loss: 0.2276783511042595, Final Batch Loss: 0.11303870379924774\n",
      "Epoch 3469, Loss: 0.25905924290418625, Final Batch Loss: 0.16530945897102356\n",
      "Epoch 3470, Loss: 0.1934027224779129, Final Batch Loss: 0.11293421685695648\n",
      "Epoch 3471, Loss: 0.23865385353565216, Final Batch Loss: 0.10104484856128693\n",
      "Epoch 3472, Loss: 0.22716186940670013, Final Batch Loss: 0.12530167400836945\n",
      "Epoch 3473, Loss: 0.24092482030391693, Final Batch Loss: 0.07336553931236267\n",
      "Epoch 3474, Loss: 0.21970334649085999, Final Batch Loss: 0.1046425849199295\n",
      "Epoch 3475, Loss: 0.3194926530122757, Final Batch Loss: 0.18678408861160278\n",
      "Epoch 3476, Loss: 0.2604716196656227, Final Batch Loss: 0.12418479472398758\n",
      "Epoch 3477, Loss: 0.23271950334310532, Final Batch Loss: 0.10215242952108383\n",
      "Epoch 3478, Loss: 0.22035331279039383, Final Batch Loss: 0.14438049495220184\n",
      "Epoch 3479, Loss: 0.24518035352230072, Final Batch Loss: 0.09860427677631378\n",
      "Epoch 3480, Loss: 0.2500298395752907, Final Batch Loss: 0.1263909488916397\n",
      "Epoch 3481, Loss: 0.2188240885734558, Final Batch Loss: 0.08703398704528809\n",
      "Epoch 3482, Loss: 0.3076791614294052, Final Batch Loss: 0.1438007801771164\n",
      "Epoch 3483, Loss: 0.242652028799057, Final Batch Loss: 0.10435198247432709\n",
      "Epoch 3484, Loss: 0.34394069015979767, Final Batch Loss: 0.12839384377002716\n",
      "Epoch 3485, Loss: 0.27784573286771774, Final Batch Loss: 0.1617467999458313\n",
      "Epoch 3486, Loss: 0.2244381234049797, Final Batch Loss: 0.110479936003685\n",
      "Epoch 3487, Loss: 0.2567881792783737, Final Batch Loss: 0.13008657097816467\n",
      "Epoch 3488, Loss: 0.26247382909059525, Final Batch Loss: 0.10592346638441086\n",
      "Epoch 3489, Loss: 0.23785342276096344, Final Batch Loss: 0.10184986889362335\n",
      "Epoch 3490, Loss: 0.24754677712917328, Final Batch Loss: 0.12745895981788635\n",
      "Epoch 3491, Loss: 0.2089225798845291, Final Batch Loss: 0.10934257507324219\n",
      "Epoch 3492, Loss: 0.23965895920991898, Final Batch Loss: 0.13884089887142181\n",
      "Epoch 3493, Loss: 0.25262124836444855, Final Batch Loss: 0.1118800938129425\n",
      "Epoch 3494, Loss: 0.1779799684882164, Final Batch Loss: 0.08109459280967712\n",
      "Epoch 3495, Loss: 0.2844972312450409, Final Batch Loss: 0.11572819948196411\n",
      "Epoch 3496, Loss: 0.23194044083356857, Final Batch Loss: 0.10022785514593124\n",
      "Epoch 3497, Loss: 0.250724658370018, Final Batch Loss: 0.12781989574432373\n",
      "Epoch 3498, Loss: 0.2605493664741516, Final Batch Loss: 0.1482817679643631\n",
      "Epoch 3499, Loss: 0.2004995420575142, Final Batch Loss: 0.08124717324972153\n",
      "Epoch 3500, Loss: 0.2124066799879074, Final Batch Loss: 0.09582345187664032\n",
      "Epoch 3501, Loss: 0.20639386028051376, Final Batch Loss: 0.09919086843729019\n",
      "Epoch 3502, Loss: 0.21070992946624756, Final Batch Loss: 0.09718497842550278\n",
      "Epoch 3503, Loss: 0.22881171107292175, Final Batch Loss: 0.11630254983901978\n",
      "Epoch 3504, Loss: 0.30249591171741486, Final Batch Loss: 0.16438573598861694\n",
      "Epoch 3505, Loss: 0.24886608868837357, Final Batch Loss: 0.16001076996326447\n",
      "Epoch 3506, Loss: 0.3359435349702835, Final Batch Loss: 0.21733978390693665\n",
      "Epoch 3507, Loss: 0.2224198281764984, Final Batch Loss: 0.11912377923727036\n",
      "Epoch 3508, Loss: 0.3113647848367691, Final Batch Loss: 0.14990293979644775\n",
      "Epoch 3509, Loss: 0.2890363112092018, Final Batch Loss: 0.12178745120763779\n",
      "Epoch 3510, Loss: 0.2325839251279831, Final Batch Loss: 0.08664612472057343\n",
      "Epoch 3511, Loss: 0.25522686541080475, Final Batch Loss: 0.13963985443115234\n",
      "Epoch 3512, Loss: 0.17643265426158905, Final Batch Loss: 0.09630671888589859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3513, Loss: 0.25336988270282745, Final Batch Loss: 0.15016257762908936\n",
      "Epoch 3514, Loss: 0.1884915754199028, Final Batch Loss: 0.07589790225028992\n",
      "Epoch 3515, Loss: 0.27597709000110626, Final Batch Loss: 0.12440471351146698\n",
      "Epoch 3516, Loss: 0.2509786784648895, Final Batch Loss: 0.11537906527519226\n",
      "Epoch 3517, Loss: 0.16292638331651688, Final Batch Loss: 0.06651446223258972\n",
      "Epoch 3518, Loss: 0.21292316913604736, Final Batch Loss: 0.10944149643182755\n",
      "Epoch 3519, Loss: 0.1970391944050789, Final Batch Loss: 0.09885772317647934\n",
      "Epoch 3520, Loss: 0.21822547167539597, Final Batch Loss: 0.1268085092306137\n",
      "Epoch 3521, Loss: 0.22429513931274414, Final Batch Loss: 0.0834459513425827\n",
      "Epoch 3522, Loss: 0.22259989380836487, Final Batch Loss: 0.12530137598514557\n",
      "Epoch 3523, Loss: 0.23746182769536972, Final Batch Loss: 0.12812764942646027\n",
      "Epoch 3524, Loss: 0.2575959488749504, Final Batch Loss: 0.08774460107088089\n",
      "Epoch 3525, Loss: 0.22722730785608292, Final Batch Loss: 0.12583506107330322\n",
      "Epoch 3526, Loss: 0.28118564188480377, Final Batch Loss: 0.14724381268024445\n",
      "Epoch 3527, Loss: 0.2430347129702568, Final Batch Loss: 0.10889389365911484\n",
      "Epoch 3528, Loss: 0.21137624979019165, Final Batch Loss: 0.07661977410316467\n",
      "Epoch 3529, Loss: 0.19642934203147888, Final Batch Loss: 0.0892280787229538\n",
      "Epoch 3530, Loss: 0.2736336886882782, Final Batch Loss: 0.12956397235393524\n",
      "Epoch 3531, Loss: 0.2077929601073265, Final Batch Loss: 0.08300299942493439\n",
      "Epoch 3532, Loss: 0.264246441423893, Final Batch Loss: 0.11226718872785568\n",
      "Epoch 3533, Loss: 0.296238511800766, Final Batch Loss: 0.17910201847553253\n",
      "Epoch 3534, Loss: 0.37521131336688995, Final Batch Loss: 0.2104317545890808\n",
      "Epoch 3535, Loss: 0.2149452567100525, Final Batch Loss: 0.11744571477174759\n",
      "Epoch 3536, Loss: 0.2493567243218422, Final Batch Loss: 0.13345758616924286\n",
      "Epoch 3537, Loss: 0.18107673525810242, Final Batch Loss: 0.10258211195468903\n",
      "Epoch 3538, Loss: 0.21297643333673477, Final Batch Loss: 0.10172935575246811\n",
      "Epoch 3539, Loss: 0.18980717658996582, Final Batch Loss: 0.104777492582798\n",
      "Epoch 3540, Loss: 0.18867851048707962, Final Batch Loss: 0.06605354696512222\n",
      "Epoch 3541, Loss: 0.23237939924001694, Final Batch Loss: 0.10383600741624832\n",
      "Epoch 3542, Loss: 0.28591209650039673, Final Batch Loss: 0.1572074294090271\n",
      "Epoch 3543, Loss: 0.2647170051932335, Final Batch Loss: 0.18984360992908478\n",
      "Epoch 3544, Loss: 0.2576461136341095, Final Batch Loss: 0.15027764439582825\n",
      "Epoch 3545, Loss: 0.27265508472919464, Final Batch Loss: 0.13751064240932465\n",
      "Epoch 3546, Loss: 0.24511755257844925, Final Batch Loss: 0.11118561774492264\n",
      "Epoch 3547, Loss: 0.3536909520626068, Final Batch Loss: 0.2228534072637558\n",
      "Epoch 3548, Loss: 0.24515660107135773, Final Batch Loss: 0.16281543672084808\n",
      "Epoch 3549, Loss: 0.24432773143053055, Final Batch Loss: 0.11091490834951401\n",
      "Epoch 3550, Loss: 0.19921532273292542, Final Batch Loss: 0.0748092383146286\n",
      "Epoch 3551, Loss: 0.33625437319278717, Final Batch Loss: 0.2018464356660843\n",
      "Epoch 3552, Loss: 0.32805652916431427, Final Batch Loss: 0.1948416829109192\n",
      "Epoch 3553, Loss: 0.2576415240764618, Final Batch Loss: 0.16391097009181976\n",
      "Epoch 3554, Loss: 0.25196849554777145, Final Batch Loss: 0.09189067035913467\n",
      "Epoch 3555, Loss: 0.2534768283367157, Final Batch Loss: 0.1403180956840515\n",
      "Epoch 3556, Loss: 0.365857258439064, Final Batch Loss: 0.22843357920646667\n",
      "Epoch 3557, Loss: 0.22205336391925812, Final Batch Loss: 0.08843259513378143\n",
      "Epoch 3558, Loss: 0.26127420365810394, Final Batch Loss: 0.14928779006004333\n",
      "Epoch 3559, Loss: 0.2529821991920471, Final Batch Loss: 0.15328671038150787\n",
      "Epoch 3560, Loss: 0.20298434048891068, Final Batch Loss: 0.0998929962515831\n",
      "Epoch 3561, Loss: 0.22431062161922455, Final Batch Loss: 0.10630464553833008\n",
      "Epoch 3562, Loss: 0.1973181962966919, Final Batch Loss: 0.0885406881570816\n",
      "Epoch 3563, Loss: 0.24515391886234283, Final Batch Loss: 0.13940681517124176\n",
      "Epoch 3564, Loss: 0.2734370827674866, Final Batch Loss: 0.109856978058815\n",
      "Epoch 3565, Loss: 0.23786316066980362, Final Batch Loss: 0.1281706988811493\n",
      "Epoch 3566, Loss: 0.24718281626701355, Final Batch Loss: 0.1378221958875656\n",
      "Epoch 3567, Loss: 0.287201851606369, Final Batch Loss: 0.1443406492471695\n",
      "Epoch 3568, Loss: 0.23694075644016266, Final Batch Loss: 0.1468309909105301\n",
      "Epoch 3569, Loss: 0.25616201013326645, Final Batch Loss: 0.16368962824344635\n",
      "Epoch 3570, Loss: 0.287443183362484, Final Batch Loss: 0.18449215590953827\n",
      "Epoch 3571, Loss: 0.26265543699264526, Final Batch Loss: 0.1015520691871643\n",
      "Epoch 3572, Loss: 0.25454026460647583, Final Batch Loss: 0.08931557834148407\n",
      "Epoch 3573, Loss: 0.228960819542408, Final Batch Loss: 0.11574455350637436\n",
      "Epoch 3574, Loss: 0.23680300265550613, Final Batch Loss: 0.11960563063621521\n",
      "Epoch 3575, Loss: 0.2507906034588814, Final Batch Loss: 0.13679230213165283\n",
      "Epoch 3576, Loss: 0.24194670468568802, Final Batch Loss: 0.11897449195384979\n",
      "Epoch 3577, Loss: 0.23394586145877838, Final Batch Loss: 0.11660392582416534\n",
      "Epoch 3578, Loss: 0.23849061131477356, Final Batch Loss: 0.12006498128175735\n",
      "Epoch 3579, Loss: 0.19575238972902298, Final Batch Loss: 0.07377573102712631\n",
      "Epoch 3580, Loss: 0.21053892374038696, Final Batch Loss: 0.05956047773361206\n",
      "Epoch 3581, Loss: 0.33168143033981323, Final Batch Loss: 0.16500145196914673\n",
      "Epoch 3582, Loss: 0.228998102247715, Final Batch Loss: 0.10256391018629074\n",
      "Epoch 3583, Loss: 0.2682437002658844, Final Batch Loss: 0.11726762354373932\n",
      "Epoch 3584, Loss: 0.24822991341352463, Final Batch Loss: 0.14986032247543335\n",
      "Epoch 3585, Loss: 0.2680060639977455, Final Batch Loss: 0.11885107308626175\n",
      "Epoch 3586, Loss: 0.22584594041109085, Final Batch Loss: 0.09344228357076645\n",
      "Epoch 3587, Loss: 0.19766098260879517, Final Batch Loss: 0.09586241096258163\n",
      "Epoch 3588, Loss: 0.21629267930984497, Final Batch Loss: 0.1061902642250061\n",
      "Epoch 3589, Loss: 0.2526533454656601, Final Batch Loss: 0.12607279419898987\n",
      "Epoch 3590, Loss: 0.25401852279901505, Final Batch Loss: 0.14367260038852692\n",
      "Epoch 3591, Loss: 0.1996249333024025, Final Batch Loss: 0.09505194425582886\n",
      "Epoch 3592, Loss: 0.21772152185440063, Final Batch Loss: 0.10717537999153137\n",
      "Epoch 3593, Loss: 0.2494070902466774, Final Batch Loss: 0.12333493679761887\n",
      "Epoch 3594, Loss: 0.2341793030500412, Final Batch Loss: 0.11723518371582031\n",
      "Epoch 3595, Loss: 0.2521219104528427, Final Batch Loss: 0.12628410756587982\n",
      "Epoch 3596, Loss: 0.23753474652767181, Final Batch Loss: 0.10404783487319946\n",
      "Epoch 3597, Loss: 0.22339985519647598, Final Batch Loss: 0.11802613735198975\n",
      "Epoch 3598, Loss: 0.23169256746768951, Final Batch Loss: 0.09022893011569977\n",
      "Epoch 3599, Loss: 0.22618970274925232, Final Batch Loss: 0.10704915970563889\n",
      "Epoch 3600, Loss: 0.2379889339208603, Final Batch Loss: 0.10964874923229218\n",
      "Epoch 3601, Loss: 0.18962344527244568, Final Batch Loss: 0.08088915050029755\n",
      "Epoch 3602, Loss: 0.23076729476451874, Final Batch Loss: 0.09376256167888641\n",
      "Epoch 3603, Loss: 0.23776984959840775, Final Batch Loss: 0.12288358062505722\n",
      "Epoch 3604, Loss: 0.2179371416568756, Final Batch Loss: 0.09822992235422134\n",
      "Epoch 3605, Loss: 0.3096461594104767, Final Batch Loss: 0.12509550154209137\n",
      "Epoch 3606, Loss: 0.23150231689214706, Final Batch Loss: 0.08759037405252457\n",
      "Epoch 3607, Loss: 0.23895853757858276, Final Batch Loss: 0.11272159218788147\n",
      "Epoch 3608, Loss: 0.2593458741903305, Final Batch Loss: 0.12889410555362701\n",
      "Epoch 3609, Loss: 0.29973241686820984, Final Batch Loss: 0.19846977293491364\n",
      "Epoch 3610, Loss: 0.2795545160770416, Final Batch Loss: 0.145751953125\n",
      "Epoch 3611, Loss: 0.22507137060165405, Final Batch Loss: 0.10590478777885437\n",
      "Epoch 3612, Loss: 0.21670591831207275, Final Batch Loss: 0.12246891856193542\n",
      "Epoch 3613, Loss: 0.24876032024621964, Final Batch Loss: 0.09756802767515182\n",
      "Epoch 3614, Loss: 0.20288749784231186, Final Batch Loss: 0.10022502392530441\n",
      "Epoch 3615, Loss: 0.22936787456274033, Final Batch Loss: 0.10988717526197433\n",
      "Epoch 3616, Loss: 0.3264980688691139, Final Batch Loss: 0.10868699103593826\n",
      "Epoch 3617, Loss: 0.3111276924610138, Final Batch Loss: 0.19166293740272522\n",
      "Epoch 3618, Loss: 0.23496758192777634, Final Batch Loss: 0.1131092831492424\n",
      "Epoch 3619, Loss: 0.16905713081359863, Final Batch Loss: 0.0912071019411087\n",
      "Epoch 3620, Loss: 0.2444225326180458, Final Batch Loss: 0.07279611378908157\n",
      "Epoch 3621, Loss: 0.2556770443916321, Final Batch Loss: 0.15026257932186127\n",
      "Epoch 3622, Loss: 0.23936330527067184, Final Batch Loss: 0.13812372088432312\n",
      "Epoch 3623, Loss: 0.21003474295139313, Final Batch Loss: 0.07985909283161163\n",
      "Epoch 3624, Loss: 0.2650190517306328, Final Batch Loss: 0.14986354112625122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3625, Loss: 0.19028105586767197, Final Batch Loss: 0.08237283676862717\n",
      "Epoch 3626, Loss: 0.2082080915570259, Final Batch Loss: 0.10748766362667084\n",
      "Epoch 3627, Loss: 0.20977279543876648, Final Batch Loss: 0.10520840436220169\n",
      "Epoch 3628, Loss: 0.23255855590105057, Final Batch Loss: 0.1174999549984932\n",
      "Epoch 3629, Loss: 0.2678162008523941, Final Batch Loss: 0.1390243023633957\n",
      "Epoch 3630, Loss: 0.2536500170826912, Final Batch Loss: 0.17779797315597534\n",
      "Epoch 3631, Loss: 0.25505875796079636, Final Batch Loss: 0.13537399470806122\n",
      "Epoch 3632, Loss: 0.21255328506231308, Final Batch Loss: 0.10258840769529343\n",
      "Epoch 3633, Loss: 0.24011698365211487, Final Batch Loss: 0.11919885873794556\n",
      "Epoch 3634, Loss: 0.19937396049499512, Final Batch Loss: 0.08137021958827972\n",
      "Epoch 3635, Loss: 0.2901086360216141, Final Batch Loss: 0.15096254646778107\n",
      "Epoch 3636, Loss: 0.2902596741914749, Final Batch Loss: 0.1591004580259323\n",
      "Epoch 3637, Loss: 0.2336224839091301, Final Batch Loss: 0.09581456333398819\n",
      "Epoch 3638, Loss: 0.21398013830184937, Final Batch Loss: 0.10135264694690704\n",
      "Epoch 3639, Loss: 0.27385323494672775, Final Batch Loss: 0.15912309288978577\n",
      "Epoch 3640, Loss: 0.2698243260383606, Final Batch Loss: 0.13958166539669037\n",
      "Epoch 3641, Loss: 0.24681774526834488, Final Batch Loss: 0.15092986822128296\n",
      "Epoch 3642, Loss: 0.30732743442058563, Final Batch Loss: 0.19577421247959137\n",
      "Epoch 3643, Loss: 0.2906947657465935, Final Batch Loss: 0.18095366656780243\n",
      "Epoch 3644, Loss: 0.23601316660642624, Final Batch Loss: 0.12277399748563766\n",
      "Epoch 3645, Loss: 0.24407196044921875, Final Batch Loss: 0.1799435019493103\n",
      "Epoch 3646, Loss: 0.2320985421538353, Final Batch Loss: 0.11653663218021393\n",
      "Epoch 3647, Loss: 0.2372017800807953, Final Batch Loss: 0.12385503202676773\n",
      "Epoch 3648, Loss: 0.2504594400525093, Final Batch Loss: 0.11794779449701309\n",
      "Epoch 3649, Loss: 0.29558685421943665, Final Batch Loss: 0.17050965130329132\n",
      "Epoch 3650, Loss: 0.25104226917028427, Final Batch Loss: 0.11513824015855789\n",
      "Epoch 3651, Loss: 0.34534725546836853, Final Batch Loss: 0.2228284478187561\n",
      "Epoch 3652, Loss: 0.21769168227910995, Final Batch Loss: 0.08610168844461441\n",
      "Epoch 3653, Loss: 0.18505404889583588, Final Batch Loss: 0.08443328738212585\n",
      "Epoch 3654, Loss: 0.2220633700489998, Final Batch Loss: 0.08271107822656631\n",
      "Epoch 3655, Loss: 0.22239094972610474, Final Batch Loss: 0.10271026939153671\n",
      "Epoch 3656, Loss: 0.2378314658999443, Final Batch Loss: 0.15235431492328644\n",
      "Epoch 3657, Loss: 0.23880426585674286, Final Batch Loss: 0.12340831011533737\n",
      "Epoch 3658, Loss: 0.2574632912874222, Final Batch Loss: 0.1268797069787979\n",
      "Epoch 3659, Loss: 0.24141564965248108, Final Batch Loss: 0.0976925939321518\n",
      "Epoch 3660, Loss: 0.2084195241332054, Final Batch Loss: 0.08532027900218964\n",
      "Epoch 3661, Loss: 0.20438851416110992, Final Batch Loss: 0.08408856391906738\n",
      "Epoch 3662, Loss: 0.23507405072450638, Final Batch Loss: 0.09377176314592361\n",
      "Epoch 3663, Loss: 0.24846824258565903, Final Batch Loss: 0.130869060754776\n",
      "Epoch 3664, Loss: 0.2026689574122429, Final Batch Loss: 0.08255030959844589\n",
      "Epoch 3665, Loss: 0.2506050020456314, Final Batch Loss: 0.13654300570487976\n",
      "Epoch 3666, Loss: 0.21239810436964035, Final Batch Loss: 0.10143911838531494\n",
      "Epoch 3667, Loss: 0.1787647381424904, Final Batch Loss: 0.08055821806192398\n",
      "Epoch 3668, Loss: 0.2030329704284668, Final Batch Loss: 0.11848992854356766\n",
      "Epoch 3669, Loss: 0.21307997405529022, Final Batch Loss: 0.1121196448802948\n",
      "Epoch 3670, Loss: 0.2136170119047165, Final Batch Loss: 0.08372893929481506\n",
      "Epoch 3671, Loss: 0.20780455321073532, Final Batch Loss: 0.09424469619989395\n",
      "Epoch 3672, Loss: 0.18550117313861847, Final Batch Loss: 0.08136482536792755\n",
      "Epoch 3673, Loss: 0.1592702865600586, Final Batch Loss: 0.05947379022836685\n",
      "Epoch 3674, Loss: 0.2150668352842331, Final Batch Loss: 0.10874732583761215\n",
      "Epoch 3675, Loss: 0.2010006532073021, Final Batch Loss: 0.08223549276590347\n",
      "Epoch 3676, Loss: 0.23101388663053513, Final Batch Loss: 0.10629259794950485\n",
      "Epoch 3677, Loss: 0.20659389346837997, Final Batch Loss: 0.10995768010616302\n",
      "Epoch 3678, Loss: 0.24602160602808, Final Batch Loss: 0.08344835788011551\n",
      "Epoch 3679, Loss: 0.2366262599825859, Final Batch Loss: 0.14216065406799316\n",
      "Epoch 3680, Loss: 0.20724929124116898, Final Batch Loss: 0.08913872390985489\n",
      "Epoch 3681, Loss: 0.26964209973812103, Final Batch Loss: 0.12845106422901154\n",
      "Epoch 3682, Loss: 0.2553980201482773, Final Batch Loss: 0.13732129335403442\n",
      "Epoch 3683, Loss: 0.3464803248643875, Final Batch Loss: 0.22531916201114655\n",
      "Epoch 3684, Loss: 0.20296244323253632, Final Batch Loss: 0.08342255651950836\n",
      "Epoch 3685, Loss: 0.18951892852783203, Final Batch Loss: 0.08038659393787384\n",
      "Epoch 3686, Loss: 0.19144921004772186, Final Batch Loss: 0.0852525383234024\n",
      "Epoch 3687, Loss: 0.21113210916519165, Final Batch Loss: 0.09563206136226654\n",
      "Epoch 3688, Loss: 0.19354895502328873, Final Batch Loss: 0.11155282706022263\n",
      "Epoch 3689, Loss: 0.2024705708026886, Final Batch Loss: 0.08775467425584793\n",
      "Epoch 3690, Loss: 0.30587226152420044, Final Batch Loss: 0.16654562950134277\n",
      "Epoch 3691, Loss: 0.20974101126194, Final Batch Loss: 0.09320211410522461\n",
      "Epoch 3692, Loss: 0.21448669582605362, Final Batch Loss: 0.1240389496088028\n",
      "Epoch 3693, Loss: 0.17298877611756325, Final Batch Loss: 0.05352642014622688\n",
      "Epoch 3694, Loss: 0.2625824734568596, Final Batch Loss: 0.1192314401268959\n",
      "Epoch 3695, Loss: 0.21183522790670395, Final Batch Loss: 0.12235616147518158\n",
      "Epoch 3696, Loss: 0.20183741301298141, Final Batch Loss: 0.09801815450191498\n",
      "Epoch 3697, Loss: 0.21413637697696686, Final Batch Loss: 0.09137599170207977\n",
      "Epoch 3698, Loss: 0.2470579668879509, Final Batch Loss: 0.10369711369276047\n",
      "Epoch 3699, Loss: 0.2565193623304367, Final Batch Loss: 0.13826784491539001\n",
      "Epoch 3700, Loss: 0.20819221436977386, Final Batch Loss: 0.12044350802898407\n",
      "Epoch 3701, Loss: 0.2030506581068039, Final Batch Loss: 0.1197822168469429\n",
      "Epoch 3702, Loss: 0.2424136996269226, Final Batch Loss: 0.09071996808052063\n",
      "Epoch 3703, Loss: 0.23599614948034286, Final Batch Loss: 0.13596905767917633\n",
      "Epoch 3704, Loss: 0.2243998870253563, Final Batch Loss: 0.11786167323589325\n",
      "Epoch 3705, Loss: 0.22124730050563812, Final Batch Loss: 0.12252230942249298\n",
      "Epoch 3706, Loss: 0.22700349986553192, Final Batch Loss: 0.09956492483615875\n",
      "Epoch 3707, Loss: 0.2536635398864746, Final Batch Loss: 0.13371233642101288\n",
      "Epoch 3708, Loss: 0.24583733826875687, Final Batch Loss: 0.13355699181556702\n",
      "Epoch 3709, Loss: 0.2041894495487213, Final Batch Loss: 0.09315907955169678\n",
      "Epoch 3710, Loss: 0.24982470273971558, Final Batch Loss: 0.15379396080970764\n",
      "Epoch 3711, Loss: 0.2552978917956352, Final Batch Loss: 0.17037896811962128\n",
      "Epoch 3712, Loss: 0.2544247508049011, Final Batch Loss: 0.09893977642059326\n",
      "Epoch 3713, Loss: 0.2518954500555992, Final Batch Loss: 0.12757883965969086\n",
      "Epoch 3714, Loss: 0.2907688766717911, Final Batch Loss: 0.13040012121200562\n",
      "Epoch 3715, Loss: 0.21294201165437698, Final Batch Loss: 0.0891767144203186\n",
      "Epoch 3716, Loss: 0.21663552522659302, Final Batch Loss: 0.12859787046909332\n",
      "Epoch 3717, Loss: 0.24934636056423187, Final Batch Loss: 0.10147038102149963\n",
      "Epoch 3718, Loss: 0.19751597940921783, Final Batch Loss: 0.09418884664773941\n",
      "Epoch 3719, Loss: 0.24962961673736572, Final Batch Loss: 0.13699764013290405\n",
      "Epoch 3720, Loss: 0.3526345267891884, Final Batch Loss: 0.24271127581596375\n",
      "Epoch 3721, Loss: 0.21861010789871216, Final Batch Loss: 0.0922330766916275\n",
      "Epoch 3722, Loss: 0.2623435780405998, Final Batch Loss: 0.12495749443769455\n",
      "Epoch 3723, Loss: 0.19605965912342072, Final Batch Loss: 0.09724927693605423\n",
      "Epoch 3724, Loss: 0.22483707964420319, Final Batch Loss: 0.10700951516628265\n",
      "Epoch 3725, Loss: 0.19795884937047958, Final Batch Loss: 0.11193621903657913\n",
      "Epoch 3726, Loss: 0.25347375124692917, Final Batch Loss: 0.14763334393501282\n",
      "Epoch 3727, Loss: 0.2095993533730507, Final Batch Loss: 0.08826138079166412\n",
      "Epoch 3728, Loss: 0.21624786406755447, Final Batch Loss: 0.08503391593694687\n",
      "Epoch 3729, Loss: 0.23431778699159622, Final Batch Loss: 0.10212596505880356\n",
      "Epoch 3730, Loss: 0.2575213387608528, Final Batch Loss: 0.1422722488641739\n",
      "Epoch 3731, Loss: 0.21201583743095398, Final Batch Loss: 0.08815347403287888\n",
      "Epoch 3732, Loss: 0.21255136281251907, Final Batch Loss: 0.11989685148000717\n",
      "Epoch 3733, Loss: 0.16946789622306824, Final Batch Loss: 0.07270722091197968\n",
      "Epoch 3734, Loss: 0.1663227453827858, Final Batch Loss: 0.07881185412406921\n",
      "Epoch 3735, Loss: 0.19115790724754333, Final Batch Loss: 0.07752039283514023\n",
      "Epoch 3736, Loss: 0.24514269083738327, Final Batch Loss: 0.15800558030605316\n",
      "Epoch 3737, Loss: 0.20458431541919708, Final Batch Loss: 0.10915859788656235\n",
      "Epoch 3738, Loss: 0.2717214822769165, Final Batch Loss: 0.1441604048013687\n",
      "Epoch 3739, Loss: 0.17374039441347122, Final Batch Loss: 0.08591151982545853\n",
      "Epoch 3740, Loss: 0.2677694857120514, Final Batch Loss: 0.14744508266448975\n",
      "Epoch 3741, Loss: 0.24786823987960815, Final Batch Loss: 0.10893584787845612\n",
      "Epoch 3742, Loss: 0.23705410212278366, Final Batch Loss: 0.09482473880052567\n",
      "Epoch 3743, Loss: 0.23150786012411118, Final Batch Loss: 0.07161053270101547\n",
      "Epoch 3744, Loss: 0.2203180491924286, Final Batch Loss: 0.10617506504058838\n",
      "Epoch 3745, Loss: 0.16847050935029984, Final Batch Loss: 0.06931453198194504\n",
      "Epoch 3746, Loss: 0.19552084058523178, Final Batch Loss: 0.06290330737829208\n",
      "Epoch 3747, Loss: 0.1897730603814125, Final Batch Loss: 0.10231909900903702\n",
      "Epoch 3748, Loss: 0.19152617454528809, Final Batch Loss: 0.1068686917424202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3749, Loss: 0.2212706357240677, Final Batch Loss: 0.1359652727842331\n",
      "Epoch 3750, Loss: 0.23038644343614578, Final Batch Loss: 0.12236715108156204\n",
      "Epoch 3751, Loss: 0.23728150874376297, Final Batch Loss: 0.10076338797807693\n",
      "Epoch 3752, Loss: 0.19993915408849716, Final Batch Loss: 0.08760283142328262\n",
      "Epoch 3753, Loss: 0.22969349473714828, Final Batch Loss: 0.08584686368703842\n",
      "Epoch 3754, Loss: 0.20658200979232788, Final Batch Loss: 0.11851940304040909\n",
      "Epoch 3755, Loss: 0.238549143075943, Final Batch Loss: 0.11674851924180984\n",
      "Epoch 3756, Loss: 0.21016136556863785, Final Batch Loss: 0.09243854880332947\n",
      "Epoch 3757, Loss: 0.2683146223425865, Final Batch Loss: 0.16269612312316895\n",
      "Epoch 3758, Loss: 0.1752791777253151, Final Batch Loss: 0.07444266974925995\n",
      "Epoch 3759, Loss: 0.24884480983018875, Final Batch Loss: 0.11034854501485825\n",
      "Epoch 3760, Loss: 0.16846131533384323, Final Batch Loss: 0.09582629054784775\n",
      "Epoch 3761, Loss: 0.20426460355520248, Final Batch Loss: 0.11314968764781952\n",
      "Epoch 3762, Loss: 0.25091806799173355, Final Batch Loss: 0.13513702154159546\n",
      "Epoch 3763, Loss: 0.18666021525859833, Final Batch Loss: 0.07733198255300522\n",
      "Epoch 3764, Loss: 0.20297054201364517, Final Batch Loss: 0.0986528992652893\n",
      "Epoch 3765, Loss: 0.20131447911262512, Final Batch Loss: 0.0905136987566948\n",
      "Epoch 3766, Loss: 0.24606387317180634, Final Batch Loss: 0.10880367457866669\n",
      "Epoch 3767, Loss: 0.1609429195523262, Final Batch Loss: 0.08403252065181732\n",
      "Epoch 3768, Loss: 0.2514177933335304, Final Batch Loss: 0.15413931012153625\n",
      "Epoch 3769, Loss: 0.2593321055173874, Final Batch Loss: 0.11676754057407379\n",
      "Epoch 3770, Loss: 0.27762749791145325, Final Batch Loss: 0.18978269398212433\n",
      "Epoch 3771, Loss: 0.28659655153751373, Final Batch Loss: 0.11525881290435791\n",
      "Epoch 3772, Loss: 0.22041472792625427, Final Batch Loss: 0.09434644877910614\n",
      "Epoch 3773, Loss: 0.29270292818546295, Final Batch Loss: 0.14809885621070862\n",
      "Epoch 3774, Loss: 0.21873388066887856, Final Batch Loss: 0.060163576155900955\n",
      "Epoch 3775, Loss: 0.18032130599021912, Final Batch Loss: 0.09464020282030106\n",
      "Epoch 3776, Loss: 0.19967443495988846, Final Batch Loss: 0.08882371336221695\n",
      "Epoch 3777, Loss: 0.23455698788166046, Final Batch Loss: 0.11537620425224304\n",
      "Epoch 3778, Loss: 0.24067912995815277, Final Batch Loss: 0.16173438727855682\n",
      "Epoch 3779, Loss: 0.15989569574594498, Final Batch Loss: 0.07028722763061523\n",
      "Epoch 3780, Loss: 0.25605247914791107, Final Batch Loss: 0.1304900050163269\n",
      "Epoch 3781, Loss: 0.2133249267935753, Final Batch Loss: 0.11385182291269302\n",
      "Epoch 3782, Loss: 0.272868275642395, Final Batch Loss: 0.12926995754241943\n",
      "Epoch 3783, Loss: 0.1625327169895172, Final Batch Loss: 0.07063663005828857\n",
      "Epoch 3784, Loss: 0.18066608905792236, Final Batch Loss: 0.11667167395353317\n",
      "Epoch 3785, Loss: 0.21009019762277603, Final Batch Loss: 0.11604175716638565\n",
      "Epoch 3786, Loss: 0.23087512701749802, Final Batch Loss: 0.10424145311117172\n",
      "Epoch 3787, Loss: 0.2118445485830307, Final Batch Loss: 0.12162905931472778\n",
      "Epoch 3788, Loss: 0.20316123962402344, Final Batch Loss: 0.10858539491891861\n",
      "Epoch 3789, Loss: 0.1844893991947174, Final Batch Loss: 0.08987542986869812\n",
      "Epoch 3790, Loss: 0.21716339886188507, Final Batch Loss: 0.12483616173267365\n",
      "Epoch 3791, Loss: 0.23109950125217438, Final Batch Loss: 0.11476875096559525\n",
      "Epoch 3792, Loss: 0.2696329429745674, Final Batch Loss: 0.15311433374881744\n",
      "Epoch 3793, Loss: 0.2439010739326477, Final Batch Loss: 0.12197377532720566\n",
      "Epoch 3794, Loss: 0.1890094056725502, Final Batch Loss: 0.0926688015460968\n",
      "Epoch 3795, Loss: 0.20106109976768494, Final Batch Loss: 0.10928191244602203\n",
      "Epoch 3796, Loss: 0.22467488795518875, Final Batch Loss: 0.09363017231225967\n",
      "Epoch 3797, Loss: 0.2887555882334709, Final Batch Loss: 0.17617616057395935\n",
      "Epoch 3798, Loss: 0.2064570039510727, Final Batch Loss: 0.1165013313293457\n",
      "Epoch 3799, Loss: 0.26833610236644745, Final Batch Loss: 0.14164143800735474\n",
      "Epoch 3800, Loss: 0.22441183030605316, Final Batch Loss: 0.08195963501930237\n",
      "Epoch 3801, Loss: 0.23383047431707382, Final Batch Loss: 0.13922683894634247\n",
      "Epoch 3802, Loss: 0.2630520835518837, Final Batch Loss: 0.10969527810811996\n",
      "Epoch 3803, Loss: 0.2215314581990242, Final Batch Loss: 0.09765248745679855\n",
      "Epoch 3804, Loss: 0.2553345337510109, Final Batch Loss: 0.1362178474664688\n",
      "Epoch 3805, Loss: 0.28584785759449005, Final Batch Loss: 0.13658103346824646\n",
      "Epoch 3806, Loss: 0.17420517653226852, Final Batch Loss: 0.08418995141983032\n",
      "Epoch 3807, Loss: 0.2020486518740654, Final Batch Loss: 0.07388357073068619\n",
      "Epoch 3808, Loss: 0.2638295441865921, Final Batch Loss: 0.13602431118488312\n",
      "Epoch 3809, Loss: 0.17225011438131332, Final Batch Loss: 0.07226559519767761\n",
      "Epoch 3810, Loss: 0.23067591339349747, Final Batch Loss: 0.1145583912730217\n",
      "Epoch 3811, Loss: 0.218281090259552, Final Batch Loss: 0.10027055442333221\n",
      "Epoch 3812, Loss: 0.22066717594861984, Final Batch Loss: 0.12339185178279877\n",
      "Epoch 3813, Loss: 0.2332439050078392, Final Batch Loss: 0.1315723955631256\n",
      "Epoch 3814, Loss: 0.29517366737127304, Final Batch Loss: 0.17472957074642181\n",
      "Epoch 3815, Loss: 0.20266517251729965, Final Batch Loss: 0.10832387208938599\n",
      "Epoch 3816, Loss: 0.2537925988435745, Final Batch Loss: 0.12818562984466553\n",
      "Epoch 3817, Loss: 0.23156701773405075, Final Batch Loss: 0.11327704787254333\n",
      "Epoch 3818, Loss: 0.31218715012073517, Final Batch Loss: 0.16691917181015015\n",
      "Epoch 3819, Loss: 0.20826585590839386, Final Batch Loss: 0.09403994679450989\n",
      "Epoch 3820, Loss: 0.2655293047428131, Final Batch Loss: 0.1153697520494461\n",
      "Epoch 3821, Loss: 0.27820103615522385, Final Batch Loss: 0.15879788994789124\n",
      "Epoch 3822, Loss: 0.22282429039478302, Final Batch Loss: 0.12502922117710114\n",
      "Epoch 3823, Loss: 0.20385416597127914, Final Batch Loss: 0.13030175864696503\n",
      "Epoch 3824, Loss: 0.24594180285930634, Final Batch Loss: 0.12625065445899963\n",
      "Epoch 3825, Loss: 0.22954609245061874, Final Batch Loss: 0.12312964349985123\n",
      "Epoch 3826, Loss: 0.1883535087108612, Final Batch Loss: 0.09154118597507477\n",
      "Epoch 3827, Loss: 0.2892225980758667, Final Batch Loss: 0.138217955827713\n",
      "Epoch 3828, Loss: 0.23492469638586044, Final Batch Loss: 0.13320471346378326\n",
      "Epoch 3829, Loss: 0.24950063228607178, Final Batch Loss: 0.12171405553817749\n",
      "Epoch 3830, Loss: 0.27087973058223724, Final Batch Loss: 0.10766930878162384\n",
      "Epoch 3831, Loss: 0.2198965847492218, Final Batch Loss: 0.12065190076828003\n",
      "Epoch 3832, Loss: 0.2854895815253258, Final Batch Loss: 0.09951146692037582\n",
      "Epoch 3833, Loss: 0.2428131327033043, Final Batch Loss: 0.11315558105707169\n",
      "Epoch 3834, Loss: 0.17592839896678925, Final Batch Loss: 0.06752821803092957\n",
      "Epoch 3835, Loss: 0.24230433255434036, Final Batch Loss: 0.09099388867616653\n",
      "Epoch 3836, Loss: 0.24803858995437622, Final Batch Loss: 0.14405012130737305\n",
      "Epoch 3837, Loss: 0.26231788843870163, Final Batch Loss: 0.10462238639593124\n",
      "Epoch 3838, Loss: 0.21216455847024918, Final Batch Loss: 0.08321363478899002\n",
      "Epoch 3839, Loss: 0.2223423272371292, Final Batch Loss: 0.11195255070924759\n",
      "Epoch 3840, Loss: 0.22948023676872253, Final Batch Loss: 0.11747284978628159\n",
      "Epoch 3841, Loss: 0.23035844415426254, Final Batch Loss: 0.10994715243577957\n",
      "Epoch 3842, Loss: 0.24582898616790771, Final Batch Loss: 0.15377061069011688\n",
      "Epoch 3843, Loss: 0.2742522433400154, Final Batch Loss: 0.15981732308864594\n",
      "Epoch 3844, Loss: 0.30280372500419617, Final Batch Loss: 0.15152640640735626\n",
      "Epoch 3845, Loss: 0.2773824632167816, Final Batch Loss: 0.11304545402526855\n",
      "Epoch 3846, Loss: 0.2111130654811859, Final Batch Loss: 0.09914562106132507\n",
      "Epoch 3847, Loss: 0.28589819371700287, Final Batch Loss: 0.17117789387702942\n",
      "Epoch 3848, Loss: 0.2676282897591591, Final Batch Loss: 0.12019375711679459\n",
      "Epoch 3849, Loss: 0.1544409766793251, Final Batch Loss: 0.06295745819807053\n",
      "Epoch 3850, Loss: 0.217015840113163, Final Batch Loss: 0.09411220252513885\n",
      "Epoch 3851, Loss: 0.25535115599632263, Final Batch Loss: 0.12540365755558014\n",
      "Epoch 3852, Loss: 0.2757316157221794, Final Batch Loss: 0.16075684130191803\n",
      "Epoch 3853, Loss: 0.24147967249155045, Final Batch Loss: 0.13424402475357056\n",
      "Epoch 3854, Loss: 0.2983534410595894, Final Batch Loss: 0.19411437213420868\n",
      "Epoch 3855, Loss: 0.2595461457967758, Final Batch Loss: 0.09414565563201904\n",
      "Epoch 3856, Loss: 0.19639300554990768, Final Batch Loss: 0.08199762552976608\n",
      "Epoch 3857, Loss: 0.2104520946741104, Final Batch Loss: 0.10938727110624313\n",
      "Epoch 3858, Loss: 0.30612562596797943, Final Batch Loss: 0.15742678940296173\n",
      "Epoch 3859, Loss: 0.200227290391922, Final Batch Loss: 0.10062648355960846\n",
      "Epoch 3860, Loss: 0.3042420297861099, Final Batch Loss: 0.21900787949562073\n",
      "Epoch 3861, Loss: 0.3085801973938942, Final Batch Loss: 0.18631432950496674\n",
      "Epoch 3862, Loss: 0.19517280161380768, Final Batch Loss: 0.07964944839477539\n",
      "Epoch 3863, Loss: 0.20969072729349136, Final Batch Loss: 0.08633240312337875\n",
      "Epoch 3864, Loss: 0.18153414875268936, Final Batch Loss: 0.07459204643964767\n",
      "Epoch 3865, Loss: 0.22283964604139328, Final Batch Loss: 0.13735990226268768\n",
      "Epoch 3866, Loss: 0.19908300042152405, Final Batch Loss: 0.12928958237171173\n",
      "Epoch 3867, Loss: 0.17564678192138672, Final Batch Loss: 0.06930215656757355\n",
      "Epoch 3868, Loss: 0.22553700953722, Final Batch Loss: 0.12740477919578552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3869, Loss: 0.3315660282969475, Final Batch Loss: 0.22475653886795044\n",
      "Epoch 3870, Loss: 0.22700019925832748, Final Batch Loss: 0.09380466490983963\n",
      "Epoch 3871, Loss: 0.27513861656188965, Final Batch Loss: 0.12418030202388763\n",
      "Epoch 3872, Loss: 0.21343855559825897, Final Batch Loss: 0.13233265280723572\n",
      "Epoch 3873, Loss: 0.19621071964502335, Final Batch Loss: 0.10470352321863174\n",
      "Epoch 3874, Loss: 0.20628321915864944, Final Batch Loss: 0.09210680425167084\n",
      "Epoch 3875, Loss: 0.21778561174869537, Final Batch Loss: 0.12745869159698486\n",
      "Epoch 3876, Loss: 0.2243582233786583, Final Batch Loss: 0.10286276042461395\n",
      "Epoch 3877, Loss: 0.22659986466169357, Final Batch Loss: 0.11367300152778625\n",
      "Epoch 3878, Loss: 0.23179815709590912, Final Batch Loss: 0.13665252923965454\n",
      "Epoch 3879, Loss: 0.22088495641946793, Final Batch Loss: 0.10313014686107635\n",
      "Epoch 3880, Loss: 0.25287093967199326, Final Batch Loss: 0.13980773091316223\n",
      "Epoch 3881, Loss: 0.23671311885118484, Final Batch Loss: 0.1200854480266571\n",
      "Epoch 3882, Loss: 0.1771954596042633, Final Batch Loss: 0.08537166565656662\n",
      "Epoch 3883, Loss: 0.24689126759767532, Final Batch Loss: 0.1543571949005127\n",
      "Epoch 3884, Loss: 0.21006373316049576, Final Batch Loss: 0.1354963481426239\n",
      "Epoch 3885, Loss: 0.2321961149573326, Final Batch Loss: 0.09448397904634476\n",
      "Epoch 3886, Loss: 0.1974882110953331, Final Batch Loss: 0.08667480945587158\n",
      "Epoch 3887, Loss: 0.24455102533102036, Final Batch Loss: 0.13436618447303772\n",
      "Epoch 3888, Loss: 0.19100940227508545, Final Batch Loss: 0.0759098157286644\n",
      "Epoch 3889, Loss: 0.24881180375814438, Final Batch Loss: 0.14653444290161133\n",
      "Epoch 3890, Loss: 0.2401644065976143, Final Batch Loss: 0.11538045853376389\n",
      "Epoch 3891, Loss: 0.22752675414085388, Final Batch Loss: 0.1159922257065773\n",
      "Epoch 3892, Loss: 0.22018920630216599, Final Batch Loss: 0.1223834902048111\n",
      "Epoch 3893, Loss: 0.227816641330719, Final Batch Loss: 0.10789964348077774\n",
      "Epoch 3894, Loss: 0.24811865389347076, Final Batch Loss: 0.1352282166481018\n",
      "Epoch 3895, Loss: 0.1925746649503708, Final Batch Loss: 0.08885855227708817\n",
      "Epoch 3896, Loss: 0.17254403233528137, Final Batch Loss: 0.07664815336465836\n",
      "Epoch 3897, Loss: 0.20133547484874725, Final Batch Loss: 0.12695927917957306\n",
      "Epoch 3898, Loss: 0.2338612899184227, Final Batch Loss: 0.12420530617237091\n",
      "Epoch 3899, Loss: 0.22363613545894623, Final Batch Loss: 0.13876916468143463\n",
      "Epoch 3900, Loss: 0.2191082164645195, Final Batch Loss: 0.10056644678115845\n",
      "Epoch 3901, Loss: 0.248797707259655, Final Batch Loss: 0.1605294793844223\n",
      "Epoch 3902, Loss: 0.16868840530514717, Final Batch Loss: 0.06019153818488121\n",
      "Epoch 3903, Loss: 0.2079523578286171, Final Batch Loss: 0.08948241919279099\n",
      "Epoch 3904, Loss: 0.2842266112565994, Final Batch Loss: 0.14011958241462708\n",
      "Epoch 3905, Loss: 0.24628912657499313, Final Batch Loss: 0.0950910672545433\n",
      "Epoch 3906, Loss: 0.2043253555893898, Final Batch Loss: 0.10665733367204666\n",
      "Epoch 3907, Loss: 0.21001267433166504, Final Batch Loss: 0.10529719293117523\n",
      "Epoch 3908, Loss: 0.29943089187145233, Final Batch Loss: 0.175480455160141\n",
      "Epoch 3909, Loss: 0.28856413066387177, Final Batch Loss: 0.16329512000083923\n",
      "Epoch 3910, Loss: 0.27933360636234283, Final Batch Loss: 0.12814976274967194\n",
      "Epoch 3911, Loss: 0.2164849042892456, Final Batch Loss: 0.12732714414596558\n",
      "Epoch 3912, Loss: 0.2429252192378044, Final Batch Loss: 0.12903350591659546\n",
      "Epoch 3913, Loss: 0.2502624914050102, Final Batch Loss: 0.15709903836250305\n",
      "Epoch 3914, Loss: 0.23248740285634995, Final Batch Loss: 0.1246764063835144\n",
      "Epoch 3915, Loss: 0.20759780704975128, Final Batch Loss: 0.10061623156070709\n",
      "Epoch 3916, Loss: 0.19212939590215683, Final Batch Loss: 0.10143976658582687\n",
      "Epoch 3917, Loss: 0.1644166186451912, Final Batch Loss: 0.06882792711257935\n",
      "Epoch 3918, Loss: 0.19998416304588318, Final Batch Loss: 0.06678642332553864\n",
      "Epoch 3919, Loss: 0.21829144656658173, Final Batch Loss: 0.0788210779428482\n",
      "Epoch 3920, Loss: 0.21486278623342514, Final Batch Loss: 0.10414914041757584\n",
      "Epoch 3921, Loss: 0.24409965425729752, Final Batch Loss: 0.12237478047609329\n",
      "Epoch 3922, Loss: 0.2245582789182663, Final Batch Loss: 0.09696324169635773\n",
      "Epoch 3923, Loss: 0.26685769110918045, Final Batch Loss: 0.11280641704797745\n",
      "Epoch 3924, Loss: 0.2412094995379448, Final Batch Loss: 0.12351242452859879\n",
      "Epoch 3925, Loss: 0.18249112367630005, Final Batch Loss: 0.09856446087360382\n",
      "Epoch 3926, Loss: 0.19869917631149292, Final Batch Loss: 0.08030811697244644\n",
      "Epoch 3927, Loss: 0.15501276403665543, Final Batch Loss: 0.07103796303272247\n",
      "Epoch 3928, Loss: 0.1878788024187088, Final Batch Loss: 0.04868699610233307\n",
      "Epoch 3929, Loss: 0.22146443277597427, Final Batch Loss: 0.08868498355150223\n",
      "Epoch 3930, Loss: 0.2241622507572174, Final Batch Loss: 0.13793052732944489\n",
      "Epoch 3931, Loss: 0.21430661529302597, Final Batch Loss: 0.10296361893415451\n",
      "Epoch 3932, Loss: 0.19501721113920212, Final Batch Loss: 0.08192937076091766\n",
      "Epoch 3933, Loss: 0.19980337470769882, Final Batch Loss: 0.10694774240255356\n",
      "Epoch 3934, Loss: 0.25141894817352295, Final Batch Loss: 0.15023155510425568\n",
      "Epoch 3935, Loss: 0.20021699368953705, Final Batch Loss: 0.11719977855682373\n",
      "Epoch 3936, Loss: 0.21850669384002686, Final Batch Loss: 0.09604298323392868\n",
      "Epoch 3937, Loss: 0.21710184961557388, Final Batch Loss: 0.10387876629829407\n",
      "Epoch 3938, Loss: 0.21302567422389984, Final Batch Loss: 0.10694842040538788\n",
      "Epoch 3939, Loss: 0.24131779372692108, Final Batch Loss: 0.14385509490966797\n",
      "Epoch 3940, Loss: 0.2713482081890106, Final Batch Loss: 0.11981263756752014\n",
      "Epoch 3941, Loss: 0.2583150565624237, Final Batch Loss: 0.12787362933158875\n",
      "Epoch 3942, Loss: 0.2423243746161461, Final Batch Loss: 0.12054754048585892\n",
      "Epoch 3943, Loss: 0.18971656262874603, Final Batch Loss: 0.09545597434043884\n",
      "Epoch 3944, Loss: 0.22822464257478714, Final Batch Loss: 0.12325289845466614\n",
      "Epoch 3945, Loss: 0.20519429445266724, Final Batch Loss: 0.10941211134195328\n",
      "Epoch 3946, Loss: 0.22730139642953873, Final Batch Loss: 0.12400399148464203\n",
      "Epoch 3947, Loss: 0.1995782107114792, Final Batch Loss: 0.10373244434595108\n",
      "Epoch 3948, Loss: 0.23525966703891754, Final Batch Loss: 0.1527879685163498\n",
      "Epoch 3949, Loss: 0.20988135039806366, Final Batch Loss: 0.125897616147995\n",
      "Epoch 3950, Loss: 0.20472180843353271, Final Batch Loss: 0.09276750683784485\n",
      "Epoch 3951, Loss: 0.2283552885055542, Final Batch Loss: 0.12371606379747391\n",
      "Epoch 3952, Loss: 0.25481776893138885, Final Batch Loss: 0.15066517889499664\n",
      "Epoch 3953, Loss: 0.18457526713609695, Final Batch Loss: 0.07756384462118149\n",
      "Epoch 3954, Loss: 0.1778179258108139, Final Batch Loss: 0.10162503272294998\n",
      "Epoch 3955, Loss: 0.20796030014753342, Final Batch Loss: 0.09493044018745422\n",
      "Epoch 3956, Loss: 0.23952822387218475, Final Batch Loss: 0.09946241974830627\n",
      "Epoch 3957, Loss: 0.1887952908873558, Final Batch Loss: 0.11322780698537827\n",
      "Epoch 3958, Loss: 0.22271352261304855, Final Batch Loss: 0.12302309274673462\n",
      "Epoch 3959, Loss: 0.19787514954805374, Final Batch Loss: 0.09528248012065887\n",
      "Epoch 3960, Loss: 0.21928322315216064, Final Batch Loss: 0.10710981488227844\n",
      "Epoch 3961, Loss: 0.20510242134332657, Final Batch Loss: 0.10348919034004211\n",
      "Epoch 3962, Loss: 0.207565076649189, Final Batch Loss: 0.07841355353593826\n",
      "Epoch 3963, Loss: 0.19195857644081116, Final Batch Loss: 0.09489146620035172\n",
      "Epoch 3964, Loss: 0.21676070243120193, Final Batch Loss: 0.12585169076919556\n",
      "Epoch 3965, Loss: 0.20713359117507935, Final Batch Loss: 0.09693047404289246\n",
      "Epoch 3966, Loss: 0.21105118095874786, Final Batch Loss: 0.13409847021102905\n",
      "Epoch 3967, Loss: 0.18595296144485474, Final Batch Loss: 0.08159774541854858\n",
      "Epoch 3968, Loss: 0.2810963839292526, Final Batch Loss: 0.049532726407051086\n",
      "Epoch 3969, Loss: 0.22527742385864258, Final Batch Loss: 0.12135414034128189\n",
      "Epoch 3970, Loss: 0.23767656087875366, Final Batch Loss: 0.10582937300205231\n",
      "Epoch 3971, Loss: 0.25955748558044434, Final Batch Loss: 0.1703711748123169\n",
      "Epoch 3972, Loss: 0.19752216339111328, Final Batch Loss: 0.07233227789402008\n",
      "Epoch 3973, Loss: 0.18750111013650894, Final Batch Loss: 0.07192753255367279\n",
      "Epoch 3974, Loss: 0.191093809902668, Final Batch Loss: 0.0941372662782669\n",
      "Epoch 3975, Loss: 0.18847361952066422, Final Batch Loss: 0.08411058783531189\n",
      "Epoch 3976, Loss: 0.18839167803525925, Final Batch Loss: 0.10499811172485352\n",
      "Epoch 3977, Loss: 0.1600954383611679, Final Batch Loss: 0.055581942200660706\n",
      "Epoch 3978, Loss: 0.22709620743989944, Final Batch Loss: 0.13149577379226685\n",
      "Epoch 3979, Loss: 0.2066180780529976, Final Batch Loss: 0.09670835733413696\n",
      "Epoch 3980, Loss: 0.26494956761598587, Final Batch Loss: 0.17896273732185364\n",
      "Epoch 3981, Loss: 0.2463647499680519, Final Batch Loss: 0.11287247389554977\n",
      "Epoch 3982, Loss: 0.24730437248945236, Final Batch Loss: 0.12721699476242065\n",
      "Epoch 3983, Loss: 0.2407991588115692, Final Batch Loss: 0.08614115417003632\n",
      "Epoch 3984, Loss: 0.1984734833240509, Final Batch Loss: 0.10190068185329437\n",
      "Epoch 3985, Loss: 0.19943998008966446, Final Batch Loss: 0.09244699031114578\n",
      "Epoch 3986, Loss: 0.2559916079044342, Final Batch Loss: 0.16655151546001434\n",
      "Epoch 3987, Loss: 0.24508348107337952, Final Batch Loss: 0.1367875188589096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3988, Loss: 0.34635405242443085, Final Batch Loss: 0.2123924195766449\n",
      "Epoch 3989, Loss: 0.2536851614713669, Final Batch Loss: 0.14697517454624176\n",
      "Epoch 3990, Loss: 0.23171288520097733, Final Batch Loss: 0.09479091316461563\n",
      "Epoch 3991, Loss: 0.2632893770933151, Final Batch Loss: 0.1179775595664978\n",
      "Epoch 3992, Loss: 0.18256869912147522, Final Batch Loss: 0.09330305457115173\n",
      "Epoch 3993, Loss: 0.2278490662574768, Final Batch Loss: 0.08740416169166565\n",
      "Epoch 3994, Loss: 0.1926685944199562, Final Batch Loss: 0.09454485774040222\n",
      "Epoch 3995, Loss: 0.26009534299373627, Final Batch Loss: 0.14075741171836853\n",
      "Epoch 3996, Loss: 0.22164708375930786, Final Batch Loss: 0.113413505256176\n",
      "Epoch 3997, Loss: 0.22218362987041473, Final Batch Loss: 0.12101471424102783\n",
      "Epoch 3998, Loss: 0.21984734386205673, Final Batch Loss: 0.1266297549009323\n",
      "Epoch 3999, Loss: 0.1435726098716259, Final Batch Loss: 0.055574189871549606\n",
      "Epoch 4000, Loss: 0.2753497436642647, Final Batch Loss: 0.16034258902072906\n",
      "Epoch 4001, Loss: 0.26228024810552597, Final Batch Loss: 0.1637430191040039\n",
      "Epoch 4002, Loss: 0.2712490037083626, Final Batch Loss: 0.15283016860485077\n",
      "Epoch 4003, Loss: 0.26521485298871994, Final Batch Loss: 0.1591610610485077\n",
      "Epoch 4004, Loss: 0.24454431980848312, Final Batch Loss: 0.12830108404159546\n",
      "Epoch 4005, Loss: 0.24699239432811737, Final Batch Loss: 0.11975347995758057\n",
      "Epoch 4006, Loss: 0.2132590264081955, Final Batch Loss: 0.11131981760263443\n",
      "Epoch 4007, Loss: 0.16208086907863617, Final Batch Loss: 0.06817237287759781\n",
      "Epoch 4008, Loss: 0.24682848155498505, Final Batch Loss: 0.10802604258060455\n",
      "Epoch 4009, Loss: 0.20412105694413185, Final Batch Loss: 0.055796924978494644\n",
      "Epoch 4010, Loss: 0.22135088592767715, Final Batch Loss: 0.08251536637544632\n",
      "Epoch 4011, Loss: 0.2435988336801529, Final Batch Loss: 0.1438174694776535\n",
      "Epoch 4012, Loss: 0.24105795472860336, Final Batch Loss: 0.12553666532039642\n",
      "Epoch 4013, Loss: 0.2713618874549866, Final Batch Loss: 0.12749840319156647\n",
      "Epoch 4014, Loss: 0.2332889661192894, Final Batch Loss: 0.11627571284770966\n",
      "Epoch 4015, Loss: 0.233986958861351, Final Batch Loss: 0.14210642874240875\n",
      "Epoch 4016, Loss: 0.2230621576309204, Final Batch Loss: 0.10951199382543564\n",
      "Epoch 4017, Loss: 0.20352662354707718, Final Batch Loss: 0.06276343017816544\n",
      "Epoch 4018, Loss: 0.26048409193754196, Final Batch Loss: 0.14322444796562195\n",
      "Epoch 4019, Loss: 0.22973577678203583, Final Batch Loss: 0.1215004026889801\n",
      "Epoch 4020, Loss: 0.235018789768219, Final Batch Loss: 0.13173003494739532\n",
      "Epoch 4021, Loss: 0.22137179970741272, Final Batch Loss: 0.10423256456851959\n",
      "Epoch 4022, Loss: 0.24066802114248276, Final Batch Loss: 0.10607031732797623\n",
      "Epoch 4023, Loss: 0.21538729220628738, Final Batch Loss: 0.08830035477876663\n",
      "Epoch 4024, Loss: 0.1878146380186081, Final Batch Loss: 0.07000786066055298\n",
      "Epoch 4025, Loss: 0.23717737942934036, Final Batch Loss: 0.11531640589237213\n",
      "Epoch 4026, Loss: 0.1871771588921547, Final Batch Loss: 0.10357194393873215\n",
      "Epoch 4027, Loss: 0.158783458173275, Final Batch Loss: 0.07779259234666824\n",
      "Epoch 4028, Loss: 0.21222451329231262, Final Batch Loss: 0.12081192433834076\n",
      "Epoch 4029, Loss: 0.24672088772058487, Final Batch Loss: 0.12393632531166077\n",
      "Epoch 4030, Loss: 0.2034682258963585, Final Batch Loss: 0.07798034697771072\n",
      "Epoch 4031, Loss: 0.1975247636437416, Final Batch Loss: 0.07842301577329636\n",
      "Epoch 4032, Loss: 0.1856575831770897, Final Batch Loss: 0.06484071165323257\n",
      "Epoch 4033, Loss: 0.2176012322306633, Final Batch Loss: 0.1177770346403122\n",
      "Epoch 4034, Loss: 0.2874624505639076, Final Batch Loss: 0.17055249214172363\n",
      "Epoch 4035, Loss: 0.24210402369499207, Final Batch Loss: 0.1458120346069336\n",
      "Epoch 4036, Loss: 0.15493881329894066, Final Batch Loss: 0.04977865889668465\n",
      "Epoch 4037, Loss: 0.3350194990634918, Final Batch Loss: 0.18117623031139374\n",
      "Epoch 4038, Loss: 0.18894340842962265, Final Batch Loss: 0.11754180490970612\n",
      "Epoch 4039, Loss: 0.20820319652557373, Final Batch Loss: 0.08729958534240723\n",
      "Epoch 4040, Loss: 0.26630859076976776, Final Batch Loss: 0.17347756028175354\n",
      "Epoch 4041, Loss: 0.29274891316890717, Final Batch Loss: 0.16542598605155945\n",
      "Epoch 4042, Loss: 0.20481917262077332, Final Batch Loss: 0.09858296066522598\n",
      "Epoch 4043, Loss: 0.27597303688526154, Final Batch Loss: 0.09191983938217163\n",
      "Epoch 4044, Loss: 0.19795367866754532, Final Batch Loss: 0.10257299244403839\n",
      "Epoch 4045, Loss: 0.17223454266786575, Final Batch Loss: 0.10221702605485916\n",
      "Epoch 4046, Loss: 0.22977885603904724, Final Batch Loss: 0.12197906523942947\n",
      "Epoch 4047, Loss: 0.296557255089283, Final Batch Loss: 0.21444174647331238\n",
      "Epoch 4048, Loss: 0.17074283212423325, Final Batch Loss: 0.07319573312997818\n",
      "Epoch 4049, Loss: 0.21700791269540787, Final Batch Loss: 0.11108136177062988\n",
      "Epoch 4050, Loss: 0.22287175059318542, Final Batch Loss: 0.11046845465898514\n",
      "Epoch 4051, Loss: 0.20914003252983093, Final Batch Loss: 0.09410542249679565\n",
      "Epoch 4052, Loss: 0.1698097288608551, Final Batch Loss: 0.09152907878160477\n",
      "Epoch 4053, Loss: 0.28949183225631714, Final Batch Loss: 0.18989916145801544\n",
      "Epoch 4054, Loss: 0.2748539000749588, Final Batch Loss: 0.14600487053394318\n",
      "Epoch 4055, Loss: 0.15527355670928955, Final Batch Loss: 0.09420771151781082\n",
      "Epoch 4056, Loss: 0.22270386666059494, Final Batch Loss: 0.0862344428896904\n",
      "Epoch 4057, Loss: 0.1661055125296116, Final Batch Loss: 0.036543238908052444\n",
      "Epoch 4058, Loss: 0.19266250729560852, Final Batch Loss: 0.10503660887479782\n",
      "Epoch 4059, Loss: 0.17824732512235641, Final Batch Loss: 0.09074573218822479\n",
      "Epoch 4060, Loss: 0.15818526968359947, Final Batch Loss: 0.054924461990594864\n",
      "Epoch 4061, Loss: 0.20707591623067856, Final Batch Loss: 0.09616430848836899\n",
      "Epoch 4062, Loss: 0.2645881325006485, Final Batch Loss: 0.18482598662376404\n",
      "Epoch 4063, Loss: 0.20036131888628006, Final Batch Loss: 0.12220169603824615\n",
      "Epoch 4064, Loss: 0.20291105657815933, Final Batch Loss: 0.12710696458816528\n",
      "Epoch 4065, Loss: 0.18786896765232086, Final Batch Loss: 0.10566839575767517\n",
      "Epoch 4066, Loss: 0.19746075570583344, Final Batch Loss: 0.07697948068380356\n",
      "Epoch 4067, Loss: 0.1871568039059639, Final Batch Loss: 0.08187274634838104\n",
      "Epoch 4068, Loss: 0.19333983212709427, Final Batch Loss: 0.07357115298509598\n",
      "Epoch 4069, Loss: 0.2099297195672989, Final Batch Loss: 0.107755646109581\n",
      "Epoch 4070, Loss: 0.2253967747092247, Final Batch Loss: 0.15212535858154297\n",
      "Epoch 4071, Loss: 0.21889816224575043, Final Batch Loss: 0.09286986291408539\n",
      "Epoch 4072, Loss: 0.20178474485874176, Final Batch Loss: 0.09197437018156052\n",
      "Epoch 4073, Loss: 0.18635481595993042, Final Batch Loss: 0.07344779372215271\n",
      "Epoch 4074, Loss: 0.23680482059717178, Final Batch Loss: 0.1452309787273407\n",
      "Epoch 4075, Loss: 0.27220335602760315, Final Batch Loss: 0.16636666655540466\n",
      "Epoch 4076, Loss: 0.15686926245689392, Final Batch Loss: 0.06979124993085861\n",
      "Epoch 4077, Loss: 0.16534001380205154, Final Batch Loss: 0.08798210322856903\n",
      "Epoch 4078, Loss: 0.2031070962548256, Final Batch Loss: 0.11789395660161972\n",
      "Epoch 4079, Loss: 0.16317494213581085, Final Batch Loss: 0.0755024254322052\n",
      "Epoch 4080, Loss: 0.24504289776086807, Final Batch Loss: 0.13987743854522705\n",
      "Epoch 4081, Loss: 0.19815251231193542, Final Batch Loss: 0.1076802983880043\n",
      "Epoch 4082, Loss: 0.2122935876250267, Final Batch Loss: 0.07985001057386398\n",
      "Epoch 4083, Loss: 0.2348891720175743, Final Batch Loss: 0.13317139446735382\n",
      "Epoch 4084, Loss: 0.20002342760562897, Final Batch Loss: 0.07287687063217163\n",
      "Epoch 4085, Loss: 0.2910720780491829, Final Batch Loss: 0.17606134712696075\n",
      "Epoch 4086, Loss: 0.2967759445309639, Final Batch Loss: 0.20448100566864014\n",
      "Epoch 4087, Loss: 0.1900167539715767, Final Batch Loss: 0.08393636345863342\n",
      "Epoch 4088, Loss: 0.20639841258525848, Final Batch Loss: 0.0909048318862915\n",
      "Epoch 4089, Loss: 0.2259020358324051, Final Batch Loss: 0.07135772705078125\n",
      "Epoch 4090, Loss: 0.22759941220283508, Final Batch Loss: 0.10177619755268097\n",
      "Epoch 4091, Loss: 0.26089728623628616, Final Batch Loss: 0.11383263021707535\n",
      "Epoch 4092, Loss: 0.26445846259593964, Final Batch Loss: 0.17943236231803894\n",
      "Epoch 4093, Loss: 0.21642859280109406, Final Batch Loss: 0.10275180637836456\n",
      "Epoch 4094, Loss: 0.18965478986501694, Final Batch Loss: 0.095831960439682\n",
      "Epoch 4095, Loss: 0.2153613641858101, Final Batch Loss: 0.08874281495809555\n",
      "Epoch 4096, Loss: 0.22981562465429306, Final Batch Loss: 0.12234917283058167\n",
      "Epoch 4097, Loss: 0.2109466716647148, Final Batch Loss: 0.12130957096815109\n",
      "Epoch 4098, Loss: 0.22658798843622208, Final Batch Loss: 0.11562303453683853\n",
      "Epoch 4099, Loss: 0.21122027933597565, Final Batch Loss: 0.07933533191680908\n",
      "Epoch 4100, Loss: 0.212245874106884, Final Batch Loss: 0.0845644548535347\n",
      "Epoch 4101, Loss: 0.22169038653373718, Final Batch Loss: 0.15235017240047455\n",
      "Epoch 4102, Loss: 0.1763056293129921, Final Batch Loss: 0.07379987090826035\n",
      "Epoch 4103, Loss: 0.27289676666259766, Final Batch Loss: 0.13638858497142792\n",
      "Epoch 4104, Loss: 0.22502242028713226, Final Batch Loss: 0.13238641619682312\n",
      "Epoch 4105, Loss: 0.22472215443849564, Final Batch Loss: 0.12374663352966309\n",
      "Epoch 4106, Loss: 0.1506488397717476, Final Batch Loss: 0.08553709834814072\n",
      "Epoch 4107, Loss: 0.22969885170459747, Final Batch Loss: 0.1150670126080513\n",
      "Epoch 4108, Loss: 0.2020954191684723, Final Batch Loss: 0.1180669516324997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4109, Loss: 0.18376462161540985, Final Batch Loss: 0.0626399889588356\n",
      "Epoch 4110, Loss: 0.22502758353948593, Final Batch Loss: 0.1132829487323761\n",
      "Epoch 4111, Loss: 0.1655493527650833, Final Batch Loss: 0.09217686951160431\n",
      "Epoch 4112, Loss: 0.20597060024738312, Final Batch Loss: 0.10111793875694275\n",
      "Epoch 4113, Loss: 0.26813966780900955, Final Batch Loss: 0.1444205492734909\n",
      "Epoch 4114, Loss: 0.20864956825971603, Final Batch Loss: 0.1059909537434578\n",
      "Epoch 4115, Loss: 0.23123805224895477, Final Batch Loss: 0.14459727704524994\n",
      "Epoch 4116, Loss: 0.22098665684461594, Final Batch Loss: 0.11882084608078003\n",
      "Epoch 4117, Loss: 0.1575431004166603, Final Batch Loss: 0.06964095681905746\n",
      "Epoch 4118, Loss: 0.16294600069522858, Final Batch Loss: 0.07208535075187683\n",
      "Epoch 4119, Loss: 0.18582196533679962, Final Batch Loss: 0.12170074880123138\n",
      "Epoch 4120, Loss: 0.18547961860895157, Final Batch Loss: 0.0668383538722992\n",
      "Epoch 4121, Loss: 0.18513088673353195, Final Batch Loss: 0.0816621258854866\n",
      "Epoch 4122, Loss: 0.2488156482577324, Final Batch Loss: 0.101895771920681\n",
      "Epoch 4123, Loss: 0.23069729655981064, Final Batch Loss: 0.10040263086557388\n",
      "Epoch 4124, Loss: 0.1995762214064598, Final Batch Loss: 0.10606584697961807\n",
      "Epoch 4125, Loss: 0.20035696774721146, Final Batch Loss: 0.06450920552015305\n",
      "Epoch 4126, Loss: 0.19641458243131638, Final Batch Loss: 0.07255058735609055\n",
      "Epoch 4127, Loss: 0.20807330310344696, Final Batch Loss: 0.1148395836353302\n",
      "Epoch 4128, Loss: 0.2229781225323677, Final Batch Loss: 0.12540310621261597\n",
      "Epoch 4129, Loss: 0.23481398075819016, Final Batch Loss: 0.1262802928686142\n",
      "Epoch 4130, Loss: 0.21068833768367767, Final Batch Loss: 0.09355159848928452\n",
      "Epoch 4131, Loss: 0.1411895900964737, Final Batch Loss: 0.05475153028964996\n",
      "Epoch 4132, Loss: 0.1845007762312889, Final Batch Loss: 0.11989708989858627\n",
      "Epoch 4133, Loss: 0.18379361927509308, Final Batch Loss: 0.09491561353206635\n",
      "Epoch 4134, Loss: 0.22767377644777298, Final Batch Loss: 0.09188396483659744\n",
      "Epoch 4135, Loss: 0.17907609045505524, Final Batch Loss: 0.07079251855611801\n",
      "Epoch 4136, Loss: 0.16479529440402985, Final Batch Loss: 0.06300868839025497\n",
      "Epoch 4137, Loss: 0.24729201197624207, Final Batch Loss: 0.1298719197511673\n",
      "Epoch 4138, Loss: 0.25569407641887665, Final Batch Loss: 0.11325491964817047\n",
      "Epoch 4139, Loss: 0.19007273763418198, Final Batch Loss: 0.09069924056529999\n",
      "Epoch 4140, Loss: 0.27001309394836426, Final Batch Loss: 0.12706974148750305\n",
      "Epoch 4141, Loss: 0.21732055395841599, Final Batch Loss: 0.13791868090629578\n",
      "Epoch 4142, Loss: 0.21262351423501968, Final Batch Loss: 0.08527443557977676\n",
      "Epoch 4143, Loss: 0.1926766335964203, Final Batch Loss: 0.08596066385507584\n",
      "Epoch 4144, Loss: 0.1992785409092903, Final Batch Loss: 0.10738421231508255\n",
      "Epoch 4145, Loss: 0.22568528354167938, Final Batch Loss: 0.10134396702051163\n",
      "Epoch 4146, Loss: 0.21231526136398315, Final Batch Loss: 0.11620334535837173\n",
      "Epoch 4147, Loss: 0.15885049849748611, Final Batch Loss: 0.08133510500192642\n",
      "Epoch 4148, Loss: 0.2395879551768303, Final Batch Loss: 0.12102639675140381\n",
      "Epoch 4149, Loss: 0.16303861141204834, Final Batch Loss: 0.07161594182252884\n",
      "Epoch 4150, Loss: 0.175403892993927, Final Batch Loss: 0.09069088101387024\n",
      "Epoch 4151, Loss: 0.21365103870630264, Final Batch Loss: 0.07520183175802231\n",
      "Epoch 4152, Loss: 0.166420616209507, Final Batch Loss: 0.06868040561676025\n",
      "Epoch 4153, Loss: 0.24167586117982864, Final Batch Loss: 0.1120947077870369\n",
      "Epoch 4154, Loss: 0.21144311875104904, Final Batch Loss: 0.08245082944631577\n",
      "Epoch 4155, Loss: 0.18255511671304703, Final Batch Loss: 0.06955254822969437\n",
      "Epoch 4156, Loss: 0.24056091159582138, Final Batch Loss: 0.11189780384302139\n",
      "Epoch 4157, Loss: 0.19687256962060928, Final Batch Loss: 0.11795704066753387\n",
      "Epoch 4158, Loss: 0.20604030787944794, Final Batch Loss: 0.12235856056213379\n",
      "Epoch 4159, Loss: 0.2491307482123375, Final Batch Loss: 0.12843146920204163\n",
      "Epoch 4160, Loss: 0.250498004257679, Final Batch Loss: 0.08117444068193436\n",
      "Epoch 4161, Loss: 0.21449341624975204, Final Batch Loss: 0.09963098913431168\n",
      "Epoch 4162, Loss: 0.20701982825994492, Final Batch Loss: 0.11533695459365845\n",
      "Epoch 4163, Loss: 0.22142035514116287, Final Batch Loss: 0.12408030778169632\n",
      "Epoch 4164, Loss: 0.20917217433452606, Final Batch Loss: 0.10456491261720657\n",
      "Epoch 4165, Loss: 0.24731670320034027, Final Batch Loss: 0.1345522403717041\n",
      "Epoch 4166, Loss: 0.19396356493234634, Final Batch Loss: 0.08614754676818848\n",
      "Epoch 4167, Loss: 0.3419974446296692, Final Batch Loss: 0.15584446489810944\n",
      "Epoch 4168, Loss: 0.16215283423662186, Final Batch Loss: 0.06690364331007004\n",
      "Epoch 4169, Loss: 0.1716170608997345, Final Batch Loss: 0.10155073553323746\n",
      "Epoch 4170, Loss: 0.22222354263067245, Final Batch Loss: 0.098947674036026\n",
      "Epoch 4171, Loss: 0.17450733482837677, Final Batch Loss: 0.09454744309186935\n",
      "Epoch 4172, Loss: 0.1618260219693184, Final Batch Loss: 0.09425714612007141\n",
      "Epoch 4173, Loss: 0.25842277705669403, Final Batch Loss: 0.12627007067203522\n",
      "Epoch 4174, Loss: 0.21654173731803894, Final Batch Loss: 0.09242724627256393\n",
      "Epoch 4175, Loss: 0.19788021966814995, Final Batch Loss: 0.06149878725409508\n",
      "Epoch 4176, Loss: 0.1972016841173172, Final Batch Loss: 0.08518152683973312\n",
      "Epoch 4177, Loss: 0.16666381806135178, Final Batch Loss: 0.07468418776988983\n",
      "Epoch 4178, Loss: 0.2470649555325508, Final Batch Loss: 0.16687439382076263\n",
      "Epoch 4179, Loss: 0.2144286409020424, Final Batch Loss: 0.09696686267852783\n",
      "Epoch 4180, Loss: 0.1369975544512272, Final Batch Loss: 0.0770675539970398\n",
      "Epoch 4181, Loss: 0.161131352186203, Final Batch Loss: 0.07939253002405167\n",
      "Epoch 4182, Loss: 0.21473001688718796, Final Batch Loss: 0.10124650597572327\n",
      "Epoch 4183, Loss: 0.17561030387878418, Final Batch Loss: 0.0702243372797966\n",
      "Epoch 4184, Loss: 0.23106607049703598, Final Batch Loss: 0.07815367728471756\n",
      "Epoch 4185, Loss: 0.2123153805732727, Final Batch Loss: 0.12537036836147308\n",
      "Epoch 4186, Loss: 0.23204649984836578, Final Batch Loss: 0.1401558369398117\n",
      "Epoch 4187, Loss: 0.1891137957572937, Final Batch Loss: 0.08551831543445587\n",
      "Epoch 4188, Loss: 0.2567017003893852, Final Batch Loss: 0.1520596593618393\n",
      "Epoch 4189, Loss: 0.19806135445833206, Final Batch Loss: 0.09357678890228271\n",
      "Epoch 4190, Loss: 0.22402840852737427, Final Batch Loss: 0.09864780306816101\n",
      "Epoch 4191, Loss: 0.17076049000024796, Final Batch Loss: 0.08169376850128174\n",
      "Epoch 4192, Loss: 0.19351059198379517, Final Batch Loss: 0.10754363238811493\n",
      "Epoch 4193, Loss: 0.2195821776986122, Final Batch Loss: 0.11681278049945831\n",
      "Epoch 4194, Loss: 0.15609002858400345, Final Batch Loss: 0.05275283753871918\n",
      "Epoch 4195, Loss: 0.20606957376003265, Final Batch Loss: 0.12509828805923462\n",
      "Epoch 4196, Loss: 0.22893205285072327, Final Batch Loss: 0.12657153606414795\n",
      "Epoch 4197, Loss: 0.2109530344605446, Final Batch Loss: 0.08703238517045975\n",
      "Epoch 4198, Loss: 0.16037216037511826, Final Batch Loss: 0.07744891941547394\n",
      "Epoch 4199, Loss: 0.16425802558660507, Final Batch Loss: 0.07624707370996475\n",
      "Epoch 4200, Loss: 0.22708594799041748, Final Batch Loss: 0.15658147633075714\n",
      "Epoch 4201, Loss: 0.22043057531118393, Final Batch Loss: 0.11505488306283951\n",
      "Epoch 4202, Loss: 0.15852691233158112, Final Batch Loss: 0.06929055601358414\n",
      "Epoch 4203, Loss: 0.17018037289381027, Final Batch Loss: 0.06873255223035812\n",
      "Epoch 4204, Loss: 0.16021428257226944, Final Batch Loss: 0.07841917127370834\n",
      "Epoch 4205, Loss: 0.18364086747169495, Final Batch Loss: 0.08704306930303574\n",
      "Epoch 4206, Loss: 0.17956383153796196, Final Batch Loss: 0.059667300432920456\n",
      "Epoch 4207, Loss: 0.1905924156308174, Final Batch Loss: 0.09050431102514267\n",
      "Epoch 4208, Loss: 0.1738060712814331, Final Batch Loss: 0.10395459085702896\n",
      "Epoch 4209, Loss: 0.20241693407297134, Final Batch Loss: 0.08309267461299896\n",
      "Epoch 4210, Loss: 0.13659939169883728, Final Batch Loss: 0.07367506623268127\n",
      "Epoch 4211, Loss: 0.18599551171064377, Final Batch Loss: 0.08396358042955399\n",
      "Epoch 4212, Loss: 0.1536872461438179, Final Batch Loss: 0.05808015167713165\n",
      "Epoch 4213, Loss: 0.20703859627246857, Final Batch Loss: 0.142507866024971\n",
      "Epoch 4214, Loss: 0.18422874063253403, Final Batch Loss: 0.06780553609132767\n",
      "Epoch 4215, Loss: 0.25401638448238373, Final Batch Loss: 0.12492325901985168\n",
      "Epoch 4216, Loss: 0.15895767137408257, Final Batch Loss: 0.058954160660505295\n",
      "Epoch 4217, Loss: 0.2159833163022995, Final Batch Loss: 0.12209129333496094\n",
      "Epoch 4218, Loss: 0.29806627333164215, Final Batch Loss: 0.13884438574314117\n",
      "Epoch 4219, Loss: 0.1578678898513317, Final Batch Loss: 0.054186929017305374\n",
      "Epoch 4220, Loss: 0.20367834717035294, Final Batch Loss: 0.0994427502155304\n",
      "Epoch 4221, Loss: 0.19767751544713974, Final Batch Loss: 0.10688507556915283\n",
      "Epoch 4222, Loss: 0.2774268537759781, Final Batch Loss: 0.1885518729686737\n",
      "Epoch 4223, Loss: 0.23141543194651604, Final Batch Loss: 0.17505966126918793\n",
      "Epoch 4224, Loss: 0.232598714530468, Final Batch Loss: 0.06411241739988327\n",
      "Epoch 4225, Loss: 0.2819111570715904, Final Batch Loss: 0.19120542705059052\n",
      "Epoch 4226, Loss: 0.14656508713960648, Final Batch Loss: 0.06868443638086319\n",
      "Epoch 4227, Loss: 0.18157381564378738, Final Batch Loss: 0.09035561233758926\n",
      "Epoch 4228, Loss: 0.18385692685842514, Final Batch Loss: 0.089508056640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4229, Loss: 0.19897646456956863, Final Batch Loss: 0.08016534894704819\n",
      "Epoch 4230, Loss: 0.2299753874540329, Final Batch Loss: 0.09604579210281372\n",
      "Epoch 4231, Loss: 0.2528863251209259, Final Batch Loss: 0.15885081887245178\n",
      "Epoch 4232, Loss: 0.1767919883131981, Final Batch Loss: 0.07530675828456879\n",
      "Epoch 4233, Loss: 0.24337495863437653, Final Batch Loss: 0.15037408471107483\n",
      "Epoch 4234, Loss: 0.1894301325082779, Final Batch Loss: 0.10122988373041153\n",
      "Epoch 4235, Loss: 0.24207350611686707, Final Batch Loss: 0.1288607269525528\n",
      "Epoch 4236, Loss: 0.14891833066940308, Final Batch Loss: 0.06872662901878357\n",
      "Epoch 4237, Loss: 0.22509845346212387, Final Batch Loss: 0.11616701632738113\n",
      "Epoch 4238, Loss: 0.23943201452493668, Final Batch Loss: 0.1378311663866043\n",
      "Epoch 4239, Loss: 0.2042217180132866, Final Batch Loss: 0.11906388401985168\n",
      "Epoch 4240, Loss: 0.20547274500131607, Final Batch Loss: 0.09688572585582733\n",
      "Epoch 4241, Loss: 0.20151466876268387, Final Batch Loss: 0.08988194167613983\n",
      "Epoch 4242, Loss: 0.21412431448698044, Final Batch Loss: 0.11780504882335663\n",
      "Epoch 4243, Loss: 0.1770690456032753, Final Batch Loss: 0.10429754853248596\n",
      "Epoch 4244, Loss: 0.21432943642139435, Final Batch Loss: 0.12534618377685547\n",
      "Epoch 4245, Loss: 0.1792241483926773, Final Batch Loss: 0.09693095833063126\n",
      "Epoch 4246, Loss: 0.2177862599492073, Final Batch Loss: 0.10975013673305511\n",
      "Epoch 4247, Loss: 0.16858401894569397, Final Batch Loss: 0.09238483011722565\n",
      "Epoch 4248, Loss: 0.22979405522346497, Final Batch Loss: 0.12334637343883514\n",
      "Epoch 4249, Loss: 0.20354921743273735, Final Batch Loss: 0.05628390237689018\n",
      "Epoch 4250, Loss: 0.2053932398557663, Final Batch Loss: 0.07771971821784973\n",
      "Epoch 4251, Loss: 0.2078421413898468, Final Batch Loss: 0.1372765153646469\n",
      "Epoch 4252, Loss: 0.1776164546608925, Final Batch Loss: 0.06869209557771683\n",
      "Epoch 4253, Loss: 0.13970626145601273, Final Batch Loss: 0.0524667352437973\n",
      "Epoch 4254, Loss: 0.24245068430900574, Final Batch Loss: 0.17878977954387665\n",
      "Epoch 4255, Loss: 0.1922018676996231, Final Batch Loss: 0.10394827276468277\n",
      "Epoch 4256, Loss: 0.2196066677570343, Final Batch Loss: 0.1483640968799591\n",
      "Epoch 4257, Loss: 0.18827036023139954, Final Batch Loss: 0.08124341815710068\n",
      "Epoch 4258, Loss: 0.19638021290302277, Final Batch Loss: 0.09388566017150879\n",
      "Epoch 4259, Loss: 0.16959355026483536, Final Batch Loss: 0.09208786487579346\n",
      "Epoch 4260, Loss: 0.13949594274163246, Final Batch Loss: 0.04610694572329521\n",
      "Epoch 4261, Loss: 0.2155631110072136, Final Batch Loss: 0.10516282916069031\n",
      "Epoch 4262, Loss: 0.2291022390127182, Final Batch Loss: 0.11095236986875534\n",
      "Epoch 4263, Loss: 0.2592306584119797, Final Batch Loss: 0.19519460201263428\n",
      "Epoch 4264, Loss: 0.18913424015045166, Final Batch Loss: 0.08651704341173172\n",
      "Epoch 4265, Loss: 0.19730927050113678, Final Batch Loss: 0.132831871509552\n",
      "Epoch 4266, Loss: 0.16388744115829468, Final Batch Loss: 0.07463255524635315\n",
      "Epoch 4267, Loss: 0.18332882225513458, Final Batch Loss: 0.08781848102807999\n",
      "Epoch 4268, Loss: 0.20053548365831375, Final Batch Loss: 0.10556096583604813\n",
      "Epoch 4269, Loss: 0.237741120159626, Final Batch Loss: 0.13114877045154572\n",
      "Epoch 4270, Loss: 0.23198817670345306, Final Batch Loss: 0.12564116716384888\n",
      "Epoch 4271, Loss: 0.16006004437804222, Final Batch Loss: 0.10414063185453415\n",
      "Epoch 4272, Loss: 0.15447751432657242, Final Batch Loss: 0.08274378627538681\n",
      "Epoch 4273, Loss: 0.16254699975252151, Final Batch Loss: 0.07830628007650375\n",
      "Epoch 4274, Loss: 0.21112558990716934, Final Batch Loss: 0.10734478384256363\n",
      "Epoch 4275, Loss: 0.2775203660130501, Final Batch Loss: 0.20420807600021362\n",
      "Epoch 4276, Loss: 0.18870356678962708, Final Batch Loss: 0.07744459062814713\n",
      "Epoch 4277, Loss: 0.1582600474357605, Final Batch Loss: 0.0711326152086258\n",
      "Epoch 4278, Loss: 0.21774431318044662, Final Batch Loss: 0.11624915897846222\n",
      "Epoch 4279, Loss: 0.19968687742948532, Final Batch Loss: 0.136428564786911\n",
      "Epoch 4280, Loss: 0.1984391212463379, Final Batch Loss: 0.09819690138101578\n",
      "Epoch 4281, Loss: 0.21293333172798157, Final Batch Loss: 0.12937934696674347\n",
      "Epoch 4282, Loss: 0.18743891268968582, Final Batch Loss: 0.09148460626602173\n",
      "Epoch 4283, Loss: 0.18996836245059967, Final Batch Loss: 0.10038136690855026\n",
      "Epoch 4284, Loss: 0.15755826979875565, Final Batch Loss: 0.06834331899881363\n",
      "Epoch 4285, Loss: 0.14771530777215958, Final Batch Loss: 0.06357665359973907\n",
      "Epoch 4286, Loss: 0.34070418775081635, Final Batch Loss: 0.2604190707206726\n",
      "Epoch 4287, Loss: 0.21961066126823425, Final Batch Loss: 0.13434858620166779\n",
      "Epoch 4288, Loss: 0.2390953004360199, Final Batch Loss: 0.0719093531370163\n",
      "Epoch 4289, Loss: 0.13938813656568527, Final Batch Loss: 0.07059290260076523\n",
      "Epoch 4290, Loss: 0.315369077026844, Final Batch Loss: 0.19589057564735413\n",
      "Epoch 4291, Loss: 0.18071269243955612, Final Batch Loss: 0.0853850245475769\n",
      "Epoch 4292, Loss: 0.1650310829281807, Final Batch Loss: 0.05435492843389511\n",
      "Epoch 4293, Loss: 0.14513326436281204, Final Batch Loss: 0.0741415023803711\n",
      "Epoch 4294, Loss: 0.15536653995513916, Final Batch Loss: 0.06952697038650513\n",
      "Epoch 4295, Loss: 0.19528275728225708, Final Batch Loss: 0.09991126507520676\n",
      "Epoch 4296, Loss: 0.18022964894771576, Final Batch Loss: 0.0790724977850914\n",
      "Epoch 4297, Loss: 0.22445333749055862, Final Batch Loss: 0.1235598549246788\n",
      "Epoch 4298, Loss: 0.18092018365859985, Final Batch Loss: 0.07693617045879364\n",
      "Epoch 4299, Loss: 0.18541717529296875, Final Batch Loss: 0.09242647886276245\n",
      "Epoch 4300, Loss: 0.19162360578775406, Final Batch Loss: 0.09591453522443771\n",
      "Epoch 4301, Loss: 0.18957438319921494, Final Batch Loss: 0.09465059638023376\n",
      "Epoch 4302, Loss: 0.22732672840356827, Final Batch Loss: 0.11146456748247147\n",
      "Epoch 4303, Loss: 0.1897154077887535, Final Batch Loss: 0.12330421060323715\n",
      "Epoch 4304, Loss: 0.20341041684150696, Final Batch Loss: 0.06377162039279938\n",
      "Epoch 4305, Loss: 0.23432957381010056, Final Batch Loss: 0.13300928473472595\n",
      "Epoch 4306, Loss: 0.21575713902711868, Final Batch Loss: 0.09044892340898514\n",
      "Epoch 4307, Loss: 0.1937103569507599, Final Batch Loss: 0.09828273952007294\n",
      "Epoch 4308, Loss: 0.24128081649541855, Final Batch Loss: 0.14812831580638885\n",
      "Epoch 4309, Loss: 0.14083228632807732, Final Batch Loss: 0.04630174860358238\n",
      "Epoch 4310, Loss: 0.22030404210090637, Final Batch Loss: 0.12236790359020233\n",
      "Epoch 4311, Loss: 0.20207763463258743, Final Batch Loss: 0.11433637887239456\n",
      "Epoch 4312, Loss: 0.21255576610565186, Final Batch Loss: 0.0956205353140831\n",
      "Epoch 4313, Loss: 0.14698651432991028, Final Batch Loss: 0.06911735981702805\n",
      "Epoch 4314, Loss: 0.14753979817032814, Final Batch Loss: 0.0597851537168026\n",
      "Epoch 4315, Loss: 0.1960732564330101, Final Batch Loss: 0.10300556570291519\n",
      "Epoch 4316, Loss: 0.1609126403927803, Final Batch Loss: 0.09107019007205963\n",
      "Epoch 4317, Loss: 0.16600215435028076, Final Batch Loss: 0.0899152010679245\n",
      "Epoch 4318, Loss: 0.1796407327055931, Final Batch Loss: 0.07102149724960327\n",
      "Epoch 4319, Loss: 0.16657916456460953, Final Batch Loss: 0.07950242608785629\n",
      "Epoch 4320, Loss: 0.17936864495277405, Final Batch Loss: 0.10335410386323929\n",
      "Epoch 4321, Loss: 0.15148727595806122, Final Batch Loss: 0.0855642557144165\n",
      "Epoch 4322, Loss: 0.19654414802789688, Final Batch Loss: 0.1262437105178833\n",
      "Epoch 4323, Loss: 0.13366342708468437, Final Batch Loss: 0.040068935602903366\n",
      "Epoch 4324, Loss: 0.16526304185390472, Final Batch Loss: 0.07285085320472717\n",
      "Epoch 4325, Loss: 0.23727422207593918, Final Batch Loss: 0.11450967937707901\n",
      "Epoch 4326, Loss: 0.17697419971227646, Final Batch Loss: 0.0924859493970871\n",
      "Epoch 4327, Loss: 0.20734542608261108, Final Batch Loss: 0.10436443239450455\n",
      "Epoch 4328, Loss: 0.22472573816776276, Final Batch Loss: 0.10287918150424957\n",
      "Epoch 4329, Loss: 0.2165490910410881, Final Batch Loss: 0.11740469932556152\n",
      "Epoch 4330, Loss: 0.231853149831295, Final Batch Loss: 0.13834278285503387\n",
      "Epoch 4331, Loss: 0.16385462880134583, Final Batch Loss: 0.09892638772726059\n",
      "Epoch 4332, Loss: 0.16877227276563644, Final Batch Loss: 0.10069967061281204\n",
      "Epoch 4333, Loss: 0.1648688092827797, Final Batch Loss: 0.0759604424238205\n",
      "Epoch 4334, Loss: 0.18058324605226517, Final Batch Loss: 0.0763736143708229\n",
      "Epoch 4335, Loss: 0.20844192057847977, Final Batch Loss: 0.11253610253334045\n",
      "Epoch 4336, Loss: 0.14968562126159668, Final Batch Loss: 0.050088055431842804\n",
      "Epoch 4337, Loss: 0.16626997664570808, Final Batch Loss: 0.10607809573411942\n",
      "Epoch 4338, Loss: 0.14629001915454865, Final Batch Loss: 0.07748960703611374\n",
      "Epoch 4339, Loss: 0.18895626440644264, Final Batch Loss: 0.059866730123758316\n",
      "Epoch 4340, Loss: 0.1427927017211914, Final Batch Loss: 0.05763297528028488\n",
      "Epoch 4341, Loss: 0.16483551263809204, Final Batch Loss: 0.08579502999782562\n",
      "Epoch 4342, Loss: 0.17506831139326096, Final Batch Loss: 0.08752001076936722\n",
      "Epoch 4343, Loss: 0.15641144663095474, Final Batch Loss: 0.05361983925104141\n",
      "Epoch 4344, Loss: 0.14522621780633926, Final Batch Loss: 0.07991814613342285\n",
      "Epoch 4345, Loss: 0.17170653119683266, Final Batch Loss: 0.05697816237807274\n",
      "Epoch 4346, Loss: 0.20930971950292587, Final Batch Loss: 0.11547736078500748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4347, Loss: 0.18403127789497375, Final Batch Loss: 0.11845853179693222\n",
      "Epoch 4348, Loss: 0.20511389523744583, Final Batch Loss: 0.10487956553697586\n",
      "Epoch 4349, Loss: 0.15142559260129929, Final Batch Loss: 0.07245717942714691\n",
      "Epoch 4350, Loss: 0.2059871405363083, Final Batch Loss: 0.13433989882469177\n",
      "Epoch 4351, Loss: 0.1971224769949913, Final Batch Loss: 0.08312885463237762\n",
      "Epoch 4352, Loss: 0.22472146898508072, Final Batch Loss: 0.12805230915546417\n",
      "Epoch 4353, Loss: 0.1661200150847435, Final Batch Loss: 0.0706174448132515\n",
      "Epoch 4354, Loss: 0.18379484862089157, Final Batch Loss: 0.07207800447940826\n",
      "Epoch 4355, Loss: 0.17158367484807968, Final Batch Loss: 0.08145029097795486\n",
      "Epoch 4356, Loss: 0.16013529151678085, Final Batch Loss: 0.05839219689369202\n",
      "Epoch 4357, Loss: 0.1862914264202118, Final Batch Loss: 0.10263539850711823\n",
      "Epoch 4358, Loss: 0.17131655663251877, Final Batch Loss: 0.09247051924467087\n",
      "Epoch 4359, Loss: 0.21070294082164764, Final Batch Loss: 0.09431193768978119\n",
      "Epoch 4360, Loss: 0.15889453887939453, Final Batch Loss: 0.07091058045625687\n",
      "Epoch 4361, Loss: 0.17339716851711273, Final Batch Loss: 0.0895644798874855\n",
      "Epoch 4362, Loss: 0.25479142367839813, Final Batch Loss: 0.15887758135795593\n",
      "Epoch 4363, Loss: 0.1832990124821663, Final Batch Loss: 0.07941417396068573\n",
      "Epoch 4364, Loss: 0.17841631174087524, Final Batch Loss: 0.085255466401577\n",
      "Epoch 4365, Loss: 0.1801643967628479, Final Batch Loss: 0.10843665897846222\n",
      "Epoch 4366, Loss: 0.14646122604608536, Final Batch Loss: 0.0693243220448494\n",
      "Epoch 4367, Loss: 0.19332240521907806, Final Batch Loss: 0.11243975162506104\n",
      "Epoch 4368, Loss: 0.18182461708784103, Final Batch Loss: 0.08088656514883041\n",
      "Epoch 4369, Loss: 0.1479114554822445, Final Batch Loss: 0.055089499801397324\n",
      "Epoch 4370, Loss: 0.28441721200942993, Final Batch Loss: 0.20554298162460327\n",
      "Epoch 4371, Loss: 0.12586474418640137, Final Batch Loss: 0.04058276116847992\n",
      "Epoch 4372, Loss: 0.17680233716964722, Final Batch Loss: 0.09456367790699005\n",
      "Epoch 4373, Loss: 0.2748497724533081, Final Batch Loss: 0.15702572464942932\n",
      "Epoch 4374, Loss: 0.15868692845106125, Final Batch Loss: 0.0700601190328598\n",
      "Epoch 4375, Loss: 0.1801328808069229, Final Batch Loss: 0.09398448467254639\n",
      "Epoch 4376, Loss: 0.14358282834291458, Final Batch Loss: 0.07384148240089417\n",
      "Epoch 4377, Loss: 0.20141887664794922, Final Batch Loss: 0.08074953407049179\n",
      "Epoch 4378, Loss: 0.17877863347530365, Final Batch Loss: 0.08226422220468521\n",
      "Epoch 4379, Loss: 0.15067825466394424, Final Batch Loss: 0.0848783403635025\n",
      "Epoch 4380, Loss: 0.19901751354336739, Final Batch Loss: 0.14461278915405273\n",
      "Epoch 4381, Loss: 0.15993095189332962, Final Batch Loss: 0.09114885330200195\n",
      "Epoch 4382, Loss: 0.18673060089349747, Final Batch Loss: 0.08792154490947723\n",
      "Epoch 4383, Loss: 0.18328223377466202, Final Batch Loss: 0.08239629119634628\n",
      "Epoch 4384, Loss: 0.19036947935819626, Final Batch Loss: 0.07495620846748352\n",
      "Epoch 4385, Loss: 0.17711904644966125, Final Batch Loss: 0.0820389986038208\n",
      "Epoch 4386, Loss: 0.18781369924545288, Final Batch Loss: 0.08803772181272507\n",
      "Epoch 4387, Loss: 0.25671258568763733, Final Batch Loss: 0.15832650661468506\n",
      "Epoch 4388, Loss: 0.17090186476707458, Final Batch Loss: 0.08303169161081314\n",
      "Epoch 4389, Loss: 0.2643035277724266, Final Batch Loss: 0.08641118556261063\n",
      "Epoch 4390, Loss: 0.1605185717344284, Final Batch Loss: 0.07978209108114243\n",
      "Epoch 4391, Loss: 0.17720134183764458, Final Batch Loss: 0.04952169582247734\n",
      "Epoch 4392, Loss: 0.22543762624263763, Final Batch Loss: 0.10981736332178116\n",
      "Epoch 4393, Loss: 0.14342506229877472, Final Batch Loss: 0.062373608350753784\n",
      "Epoch 4394, Loss: 0.183006189763546, Final Batch Loss: 0.10843256115913391\n",
      "Epoch 4395, Loss: 0.2086140215396881, Final Batch Loss: 0.07440498471260071\n",
      "Epoch 4396, Loss: 0.15983841940760612, Final Batch Loss: 0.05469106510281563\n",
      "Epoch 4397, Loss: 0.14188678562641144, Final Batch Loss: 0.0809144452214241\n",
      "Epoch 4398, Loss: 0.23134499043226242, Final Batch Loss: 0.07374788075685501\n",
      "Epoch 4399, Loss: 0.2567763924598694, Final Batch Loss: 0.12876224517822266\n",
      "Epoch 4400, Loss: 0.19382796436548233, Final Batch Loss: 0.0849088504910469\n",
      "Epoch 4401, Loss: 0.11650758981704712, Final Batch Loss: 0.056418146938085556\n",
      "Epoch 4402, Loss: 0.24812065809965134, Final Batch Loss: 0.17874263226985931\n",
      "Epoch 4403, Loss: 0.20910115540027618, Final Batch Loss: 0.10825854539871216\n",
      "Epoch 4404, Loss: 0.15195134282112122, Final Batch Loss: 0.06929430365562439\n",
      "Epoch 4405, Loss: 0.21775274723768234, Final Batch Loss: 0.07135749608278275\n",
      "Epoch 4406, Loss: 0.20446665585041046, Final Batch Loss: 0.11023484915494919\n",
      "Epoch 4407, Loss: 0.22213560342788696, Final Batch Loss: 0.07668580114841461\n",
      "Epoch 4408, Loss: 0.1705477088689804, Final Batch Loss: 0.08081647753715515\n",
      "Epoch 4409, Loss: 0.17456865310668945, Final Batch Loss: 0.10752227157354355\n",
      "Epoch 4410, Loss: 0.23413794487714767, Final Batch Loss: 0.1410188376903534\n",
      "Epoch 4411, Loss: 0.1752898097038269, Final Batch Loss: 0.08857754617929459\n",
      "Epoch 4412, Loss: 0.16615751385688782, Final Batch Loss: 0.0867125615477562\n",
      "Epoch 4413, Loss: 0.1440424546599388, Final Batch Loss: 0.04705728590488434\n",
      "Epoch 4414, Loss: 0.182284414768219, Final Batch Loss: 0.10056920349597931\n",
      "Epoch 4415, Loss: 0.1687840223312378, Final Batch Loss: 0.07914905250072479\n",
      "Epoch 4416, Loss: 0.2108965963125229, Final Batch Loss: 0.1220923364162445\n",
      "Epoch 4417, Loss: 0.16306187212467194, Final Batch Loss: 0.07346588373184204\n",
      "Epoch 4418, Loss: 0.1494358777999878, Final Batch Loss: 0.06970598548650742\n",
      "Epoch 4419, Loss: 0.19617468118667603, Final Batch Loss: 0.09464758634567261\n",
      "Epoch 4420, Loss: 0.20632488280534744, Final Batch Loss: 0.1283980756998062\n",
      "Epoch 4421, Loss: 0.20583544671535492, Final Batch Loss: 0.08345115184783936\n",
      "Epoch 4422, Loss: 0.20988094806671143, Final Batch Loss: 0.10052832216024399\n",
      "Epoch 4423, Loss: 0.10649027675390244, Final Batch Loss: 0.05612299591302872\n",
      "Epoch 4424, Loss: 0.15516282618045807, Final Batch Loss: 0.07333230972290039\n",
      "Epoch 4425, Loss: 0.19596225023269653, Final Batch Loss: 0.10046710819005966\n",
      "Epoch 4426, Loss: 0.17196062952280045, Final Batch Loss: 0.06090046465396881\n",
      "Epoch 4427, Loss: 0.18124306946992874, Final Batch Loss: 0.08155371993780136\n",
      "Epoch 4428, Loss: 0.19434647262096405, Final Batch Loss: 0.07849666476249695\n",
      "Epoch 4429, Loss: 0.25878071784973145, Final Batch Loss: 0.19286029040813446\n",
      "Epoch 4430, Loss: 0.23650844395160675, Final Batch Loss: 0.12545302510261536\n",
      "Epoch 4431, Loss: 0.17368265241384506, Final Batch Loss: 0.0959116593003273\n",
      "Epoch 4432, Loss: 0.17242038995027542, Final Batch Loss: 0.07271512597799301\n",
      "Epoch 4433, Loss: 0.1856071650981903, Final Batch Loss: 0.07621057331562042\n",
      "Epoch 4434, Loss: 0.1710219830274582, Final Batch Loss: 0.09319304674863815\n",
      "Epoch 4435, Loss: 0.14994152635335922, Final Batch Loss: 0.06696231663227081\n",
      "Epoch 4436, Loss: 0.16352517902851105, Final Batch Loss: 0.08703360706567764\n",
      "Epoch 4437, Loss: 0.19014260172843933, Final Batch Loss: 0.06930164247751236\n",
      "Epoch 4438, Loss: 0.17384596914052963, Final Batch Loss: 0.06621937453746796\n",
      "Epoch 4439, Loss: 0.24454060196876526, Final Batch Loss: 0.13336673378944397\n",
      "Epoch 4440, Loss: 0.23153439164161682, Final Batch Loss: 0.07052242755889893\n",
      "Epoch 4441, Loss: 0.22610563784837723, Final Batch Loss: 0.09899982064962387\n",
      "Epoch 4442, Loss: 0.19088422507047653, Final Batch Loss: 0.08577778190374374\n",
      "Epoch 4443, Loss: 0.1774202585220337, Final Batch Loss: 0.08263737708330154\n",
      "Epoch 4444, Loss: 0.17315296083688736, Final Batch Loss: 0.10805300623178482\n",
      "Epoch 4445, Loss: 0.0968291275203228, Final Batch Loss: 0.036018531769514084\n",
      "Epoch 4446, Loss: 0.2246399223804474, Final Batch Loss: 0.13133087754249573\n",
      "Epoch 4447, Loss: 0.18092405796051025, Final Batch Loss: 0.08941805362701416\n",
      "Epoch 4448, Loss: 0.19411268085241318, Final Batch Loss: 0.07271560281515121\n",
      "Epoch 4449, Loss: 0.1337558999657631, Final Batch Loss: 0.06550521403551102\n",
      "Epoch 4450, Loss: 0.13952773809432983, Final Batch Loss: 0.05773721635341644\n",
      "Epoch 4451, Loss: 0.1486295461654663, Final Batch Loss: 0.049841880798339844\n",
      "Epoch 4452, Loss: 0.16260384023189545, Final Batch Loss: 0.07403462380170822\n",
      "Epoch 4453, Loss: 0.1478983834385872, Final Batch Loss: 0.07312080264091492\n",
      "Epoch 4454, Loss: 0.17998507991433144, Final Batch Loss: 0.12197646498680115\n",
      "Epoch 4455, Loss: 0.14607448875904083, Final Batch Loss: 0.06717337667942047\n",
      "Epoch 4456, Loss: 0.12131495028734207, Final Batch Loss: 0.04908955842256546\n",
      "Epoch 4457, Loss: 0.14054149016737938, Final Batch Loss: 0.08205443620681763\n",
      "Epoch 4458, Loss: 0.17018400877714157, Final Batch Loss: 0.09607160091400146\n",
      "Epoch 4459, Loss: 0.2537868842482567, Final Batch Loss: 0.12899033725261688\n",
      "Epoch 4460, Loss: 0.1302071437239647, Final Batch Loss: 0.06593724340200424\n",
      "Epoch 4461, Loss: 0.1506429985165596, Final Batch Loss: 0.08132186532020569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4462, Loss: 0.17044974863529205, Final Batch Loss: 0.07707370072603226\n",
      "Epoch 4463, Loss: 0.2298913598060608, Final Batch Loss: 0.12779761850833893\n",
      "Epoch 4464, Loss: 0.15403633564710617, Final Batch Loss: 0.06893526762723923\n",
      "Epoch 4465, Loss: 0.17327313870191574, Final Batch Loss: 0.07498576492071152\n",
      "Epoch 4466, Loss: 0.19435298442840576, Final Batch Loss: 0.08587487041950226\n",
      "Epoch 4467, Loss: 0.18718895316123962, Final Batch Loss: 0.08481767028570175\n",
      "Epoch 4468, Loss: 0.14994372427463531, Final Batch Loss: 0.0781322568655014\n",
      "Epoch 4469, Loss: 0.16714207082986832, Final Batch Loss: 0.07693852484226227\n",
      "Epoch 4470, Loss: 0.1737757995724678, Final Batch Loss: 0.09015879034996033\n",
      "Epoch 4471, Loss: 0.184747152030468, Final Batch Loss: 0.0430726483464241\n",
      "Epoch 4472, Loss: 0.13884233683347702, Final Batch Loss: 0.08376255631446838\n",
      "Epoch 4473, Loss: 0.18178822845220566, Final Batch Loss: 0.08353054523468018\n",
      "Epoch 4474, Loss: 0.1798786073923111, Final Batch Loss: 0.08766775578260422\n",
      "Epoch 4475, Loss: 0.20993369072675705, Final Batch Loss: 0.0923704206943512\n",
      "Epoch 4476, Loss: 0.14429695159196854, Final Batch Loss: 0.07483363151550293\n",
      "Epoch 4477, Loss: 0.189038448035717, Final Batch Loss: 0.08885697275400162\n",
      "Epoch 4478, Loss: 0.15932585299015045, Final Batch Loss: 0.0813681110739708\n",
      "Epoch 4479, Loss: 0.14338642731308937, Final Batch Loss: 0.10203030705451965\n",
      "Epoch 4480, Loss: 0.23389160633087158, Final Batch Loss: 0.08110949397087097\n",
      "Epoch 4481, Loss: 0.2039889097213745, Final Batch Loss: 0.06550396978855133\n",
      "Epoch 4482, Loss: 0.178938589990139, Final Batch Loss: 0.08654419332742691\n",
      "Epoch 4483, Loss: 0.21652548760175705, Final Batch Loss: 0.11195247620344162\n",
      "Epoch 4484, Loss: 0.1925913244485855, Final Batch Loss: 0.12175904959440231\n",
      "Epoch 4485, Loss: 0.17099205404520035, Final Batch Loss: 0.07888507097959518\n",
      "Epoch 4486, Loss: 0.15872704982757568, Final Batch Loss: 0.08266577124595642\n",
      "Epoch 4487, Loss: 0.17186444252729416, Final Batch Loss: 0.08585620671510696\n",
      "Epoch 4488, Loss: 0.20764602720737457, Final Batch Loss: 0.12079649418592453\n",
      "Epoch 4489, Loss: 0.18904464691877365, Final Batch Loss: 0.11148373037576675\n",
      "Epoch 4490, Loss: 0.16865596175193787, Final Batch Loss: 0.08421912044286728\n",
      "Epoch 4491, Loss: 0.251369833946228, Final Batch Loss: 0.1445477306842804\n",
      "Epoch 4492, Loss: 0.21591982245445251, Final Batch Loss: 0.10662902891635895\n",
      "Epoch 4493, Loss: 0.16428080946207047, Final Batch Loss: 0.06283516436815262\n",
      "Epoch 4494, Loss: 0.17791859060525894, Final Batch Loss: 0.0691804438829422\n",
      "Epoch 4495, Loss: 0.18556798249483109, Final Batch Loss: 0.10389336943626404\n",
      "Epoch 4496, Loss: 0.1356702223420143, Final Batch Loss: 0.06946275383234024\n",
      "Epoch 4497, Loss: 0.19875160604715347, Final Batch Loss: 0.07576757669448853\n",
      "Epoch 4498, Loss: 0.15395785123109818, Final Batch Loss: 0.06787187606096268\n",
      "Epoch 4499, Loss: 0.1614580675959587, Final Batch Loss: 0.08307025581598282\n",
      "Epoch 4500, Loss: 0.13716187700629234, Final Batch Loss: 0.05597379431128502\n",
      "Epoch 4501, Loss: 0.1522217094898224, Final Batch Loss: 0.05048462003469467\n",
      "Epoch 4502, Loss: 0.20525045692920685, Final Batch Loss: 0.142431378364563\n",
      "Epoch 4503, Loss: 0.14883258193731308, Final Batch Loss: 0.08045577257871628\n",
      "Epoch 4504, Loss: 0.24750782549381256, Final Batch Loss: 0.1358298808336258\n",
      "Epoch 4505, Loss: 0.2072271704673767, Final Batch Loss: 0.08085420727729797\n",
      "Epoch 4506, Loss: 0.23554106056690216, Final Batch Loss: 0.12853968143463135\n",
      "Epoch 4507, Loss: 0.22971535474061966, Final Batch Loss: 0.13650621473789215\n",
      "Epoch 4508, Loss: 0.15935342013835907, Final Batch Loss: 0.09101451188325882\n",
      "Epoch 4509, Loss: 0.19594653695821762, Final Batch Loss: 0.05801389366388321\n",
      "Epoch 4510, Loss: 0.16883444041013718, Final Batch Loss: 0.08239921182394028\n",
      "Epoch 4511, Loss: 0.1680392324924469, Final Batch Loss: 0.0927150547504425\n",
      "Epoch 4512, Loss: 0.2107139453291893, Final Batch Loss: 0.14566263556480408\n",
      "Epoch 4513, Loss: 0.24809621274471283, Final Batch Loss: 0.12788455188274384\n",
      "Epoch 4514, Loss: 0.1606191173195839, Final Batch Loss: 0.09006506204605103\n",
      "Epoch 4515, Loss: 0.17720568925142288, Final Batch Loss: 0.11610442399978638\n",
      "Epoch 4516, Loss: 0.1772511601448059, Final Batch Loss: 0.0811321884393692\n",
      "Epoch 4517, Loss: 0.1362924873828888, Final Batch Loss: 0.0536850169301033\n",
      "Epoch 4518, Loss: 0.21683558076620102, Final Batch Loss: 0.11096259951591492\n",
      "Epoch 4519, Loss: 0.13688239455223083, Final Batch Loss: 0.07137497514486313\n",
      "Epoch 4520, Loss: 0.20656857639551163, Final Batch Loss: 0.1225476935505867\n",
      "Epoch 4521, Loss: 0.1543060764670372, Final Batch Loss: 0.10066042840480804\n",
      "Epoch 4522, Loss: 0.1649230420589447, Final Batch Loss: 0.07481671124696732\n",
      "Epoch 4523, Loss: 0.1428103744983673, Final Batch Loss: 0.07307132333517075\n",
      "Epoch 4524, Loss: 0.21373645216226578, Final Batch Loss: 0.1006896123290062\n",
      "Epoch 4525, Loss: 0.14698784798383713, Final Batch Loss: 0.06406927108764648\n",
      "Epoch 4526, Loss: 0.14987581223249435, Final Batch Loss: 0.04897052049636841\n",
      "Epoch 4527, Loss: 0.2546192407608032, Final Batch Loss: 0.13075213134288788\n",
      "Epoch 4528, Loss: 0.19420992210507393, Final Batch Loss: 0.06198706105351448\n",
      "Epoch 4529, Loss: 0.14745823293924332, Final Batch Loss: 0.047447435557842255\n",
      "Epoch 4530, Loss: 0.20384986698627472, Final Batch Loss: 0.10553395003080368\n",
      "Epoch 4531, Loss: 0.14211924374103546, Final Batch Loss: 0.04241696000099182\n",
      "Epoch 4532, Loss: 0.2043255716562271, Final Batch Loss: 0.10576514899730682\n",
      "Epoch 4533, Loss: 0.16065028309822083, Final Batch Loss: 0.06658028811216354\n",
      "Epoch 4534, Loss: 0.1321425437927246, Final Batch Loss: 0.08554589748382568\n",
      "Epoch 4535, Loss: 0.1843821257352829, Final Batch Loss: 0.07670027017593384\n",
      "Epoch 4536, Loss: 0.21124780178070068, Final Batch Loss: 0.13324350118637085\n",
      "Epoch 4537, Loss: 0.13508427888154984, Final Batch Loss: 0.06793084740638733\n",
      "Epoch 4538, Loss: 0.19420480728149414, Final Batch Loss: 0.08806651830673218\n",
      "Epoch 4539, Loss: 0.13121987879276276, Final Batch Loss: 0.06736788898706436\n",
      "Epoch 4540, Loss: 0.12768593803048134, Final Batch Loss: 0.060955848544836044\n",
      "Epoch 4541, Loss: 0.2060142159461975, Final Batch Loss: 0.08740416169166565\n",
      "Epoch 4542, Loss: 0.15563078969717026, Final Batch Loss: 0.06771901249885559\n",
      "Epoch 4543, Loss: 0.17720022425055504, Final Batch Loss: 0.11990835517644882\n",
      "Epoch 4544, Loss: 0.16816166788339615, Final Batch Loss: 0.06829740107059479\n",
      "Epoch 4545, Loss: 0.16041778028011322, Final Batch Loss: 0.0724983736872673\n",
      "Epoch 4546, Loss: 0.17687880247831345, Final Batch Loss: 0.08620354533195496\n",
      "Epoch 4547, Loss: 0.23193462193012238, Final Batch Loss: 0.09306040406227112\n",
      "Epoch 4548, Loss: 0.12999650835990906, Final Batch Loss: 0.05384261906147003\n",
      "Epoch 4549, Loss: 0.1253727674484253, Final Batch Loss: 0.05324950069189072\n",
      "Epoch 4550, Loss: 0.19463439285755157, Final Batch Loss: 0.05428782105445862\n",
      "Epoch 4551, Loss: 0.18514751642942429, Final Batch Loss: 0.09079387784004211\n",
      "Epoch 4552, Loss: 0.2127787172794342, Final Batch Loss: 0.10350264608860016\n",
      "Epoch 4553, Loss: 0.20319830626249313, Final Batch Loss: 0.11181282997131348\n",
      "Epoch 4554, Loss: 0.17441941797733307, Final Batch Loss: 0.11183526366949081\n",
      "Epoch 4555, Loss: 0.1567666083574295, Final Batch Loss: 0.06171724945306778\n",
      "Epoch 4556, Loss: 0.16727257519960403, Final Batch Loss: 0.12184058129787445\n",
      "Epoch 4557, Loss: 0.14922979474067688, Final Batch Loss: 0.054511286318302155\n",
      "Epoch 4558, Loss: 0.12008455023169518, Final Batch Loss: 0.04933805391192436\n",
      "Epoch 4559, Loss: 0.18504297733306885, Final Batch Loss: 0.07926640659570694\n",
      "Epoch 4560, Loss: 0.1321382224559784, Final Batch Loss: 0.05714346468448639\n",
      "Epoch 4561, Loss: 0.18700964003801346, Final Batch Loss: 0.087840236723423\n",
      "Epoch 4562, Loss: 0.17777027562260628, Final Batch Loss: 0.054252464324235916\n",
      "Epoch 4563, Loss: 0.17541775852441788, Final Batch Loss: 0.08800681680440903\n",
      "Epoch 4564, Loss: 0.18618042767047882, Final Batch Loss: 0.08071201294660568\n",
      "Epoch 4565, Loss: 0.16065151989459991, Final Batch Loss: 0.08418790996074677\n",
      "Epoch 4566, Loss: 0.1792002171278, Final Batch Loss: 0.09277040511369705\n",
      "Epoch 4567, Loss: 0.14568392932415009, Final Batch Loss: 0.07139917463064194\n",
      "Epoch 4568, Loss: 0.1466117724776268, Final Batch Loss: 0.09607408940792084\n",
      "Epoch 4569, Loss: 0.1909763440489769, Final Batch Loss: 0.07542134076356888\n",
      "Epoch 4570, Loss: 0.15912985056638718, Final Batch Loss: 0.08057450503110886\n",
      "Epoch 4571, Loss: 0.2410474345088005, Final Batch Loss: 0.07795091718435287\n",
      "Epoch 4572, Loss: 0.15764158219099045, Final Batch Loss: 0.06350643187761307\n",
      "Epoch 4573, Loss: 0.24972894042730331, Final Batch Loss: 0.1252349615097046\n",
      "Epoch 4574, Loss: 0.1427786871790886, Final Batch Loss: 0.06081254780292511\n",
      "Epoch 4575, Loss: 0.18200280889868736, Final Batch Loss: 0.05826140567660332\n",
      "Epoch 4576, Loss: 0.16650976985692978, Final Batch Loss: 0.07898817956447601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4577, Loss: 0.16821114718914032, Final Batch Loss: 0.10929964482784271\n",
      "Epoch 4578, Loss: 0.16587566956877708, Final Batch Loss: 0.10608568042516708\n",
      "Epoch 4579, Loss: 0.17296696454286575, Final Batch Loss: 0.07190762460231781\n",
      "Epoch 4580, Loss: 0.19494377449154854, Final Batch Loss: 0.1330099105834961\n",
      "Epoch 4581, Loss: 0.12216994538903236, Final Batch Loss: 0.06651654839515686\n",
      "Epoch 4582, Loss: 0.18768221139907837, Final Batch Loss: 0.07889657467603683\n",
      "Epoch 4583, Loss: 0.1927974447607994, Final Batch Loss: 0.09599413722753525\n",
      "Epoch 4584, Loss: 0.13162988051772118, Final Batch Loss: 0.06039200350642204\n",
      "Epoch 4585, Loss: 0.1815585196018219, Final Batch Loss: 0.0915856808423996\n",
      "Epoch 4586, Loss: 0.15755566954612732, Final Batch Loss: 0.048180654644966125\n",
      "Epoch 4587, Loss: 0.19065963476896286, Final Batch Loss: 0.09961128979921341\n",
      "Epoch 4588, Loss: 0.13533172011375427, Final Batch Loss: 0.07207424193620682\n",
      "Epoch 4589, Loss: 0.11726943403482437, Final Batch Loss: 0.03701608628034592\n",
      "Epoch 4590, Loss: 0.1422220952808857, Final Batch Loss: 0.061222318559885025\n",
      "Epoch 4591, Loss: 0.1789253056049347, Final Batch Loss: 0.06590498983860016\n",
      "Epoch 4592, Loss: 0.1444329097867012, Final Batch Loss: 0.07454994320869446\n",
      "Epoch 4593, Loss: 0.17390788346529007, Final Batch Loss: 0.10320058465003967\n",
      "Epoch 4594, Loss: 0.15883054584264755, Final Batch Loss: 0.10224355757236481\n",
      "Epoch 4595, Loss: 0.12951960042119026, Final Batch Loss: 0.05010104551911354\n",
      "Epoch 4596, Loss: 0.19081641733646393, Final Batch Loss: 0.09812790155410767\n",
      "Epoch 4597, Loss: 0.19116473942995071, Final Batch Loss: 0.10303881019353867\n",
      "Epoch 4598, Loss: 0.1691293567419052, Final Batch Loss: 0.06031240522861481\n",
      "Epoch 4599, Loss: 0.10861014947295189, Final Batch Loss: 0.04206177964806557\n",
      "Epoch 4600, Loss: 0.16050999611616135, Final Batch Loss: 0.08464693278074265\n",
      "Epoch 4601, Loss: 0.15104567632079124, Final Batch Loss: 0.09120939671993256\n",
      "Epoch 4602, Loss: 0.16608738899230957, Final Batch Loss: 0.08610451221466064\n",
      "Epoch 4603, Loss: 0.16381264477968216, Final Batch Loss: 0.07633089274168015\n",
      "Epoch 4604, Loss: 0.17341769486665726, Final Batch Loss: 0.09715050458908081\n",
      "Epoch 4605, Loss: 0.1726417876780033, Final Batch Loss: 0.1146981343626976\n",
      "Epoch 4606, Loss: 0.19369825720787048, Final Batch Loss: 0.11815931648015976\n",
      "Epoch 4607, Loss: 0.1483418121933937, Final Batch Loss: 0.07664461433887482\n",
      "Epoch 4608, Loss: 0.169684000313282, Final Batch Loss: 0.06930287927389145\n",
      "Epoch 4609, Loss: 0.26468416303396225, Final Batch Loss: 0.11923270672559738\n",
      "Epoch 4610, Loss: 0.13391456753015518, Final Batch Loss: 0.06923461705446243\n",
      "Epoch 4611, Loss: 0.23081006854772568, Final Batch Loss: 0.1339111626148224\n",
      "Epoch 4612, Loss: 0.1899762973189354, Final Batch Loss: 0.08152176439762115\n",
      "Epoch 4613, Loss: 0.13752934336662292, Final Batch Loss: 0.0662798285484314\n",
      "Epoch 4614, Loss: 0.13547376915812492, Final Batch Loss: 0.05601584538817406\n",
      "Epoch 4615, Loss: 0.16173835471272469, Final Batch Loss: 0.05617068335413933\n",
      "Epoch 4616, Loss: 0.17536140233278275, Final Batch Loss: 0.08930891752243042\n",
      "Epoch 4617, Loss: 0.20788969099521637, Final Batch Loss: 0.1166871041059494\n",
      "Epoch 4618, Loss: 0.1872299611568451, Final Batch Loss: 0.1062023714184761\n",
      "Epoch 4619, Loss: 0.14223213866353035, Final Batch Loss: 0.049848366528749466\n",
      "Epoch 4620, Loss: 0.19872577488422394, Final Batch Loss: 0.11079905182123184\n",
      "Epoch 4621, Loss: 0.1512388102710247, Final Batch Loss: 0.0458257831633091\n",
      "Epoch 4622, Loss: 0.10880769044160843, Final Batch Loss: 0.05724649876356125\n",
      "Epoch 4623, Loss: 0.16917773336172104, Final Batch Loss: 0.07534749805927277\n",
      "Epoch 4624, Loss: 0.14810890704393387, Final Batch Loss: 0.09248577058315277\n",
      "Epoch 4625, Loss: 0.13488247618079185, Final Batch Loss: 0.07592649012804031\n",
      "Epoch 4626, Loss: 0.22457176446914673, Final Batch Loss: 0.08255986869335175\n",
      "Epoch 4627, Loss: 0.18742220103740692, Final Batch Loss: 0.1075979545712471\n",
      "Epoch 4628, Loss: 0.1517028883099556, Final Batch Loss: 0.08554092049598694\n",
      "Epoch 4629, Loss: 0.1454923003911972, Final Batch Loss: 0.05996452271938324\n",
      "Epoch 4630, Loss: 0.1836852952837944, Final Batch Loss: 0.11211258172988892\n",
      "Epoch 4631, Loss: 0.14739326760172844, Final Batch Loss: 0.09313926100730896\n",
      "Epoch 4632, Loss: 0.16866907477378845, Final Batch Loss: 0.09976530075073242\n",
      "Epoch 4633, Loss: 0.1402846910059452, Final Batch Loss: 0.09616530686616898\n",
      "Epoch 4634, Loss: 0.11301231384277344, Final Batch Loss: 0.04751139134168625\n",
      "Epoch 4635, Loss: 0.20787577331066132, Final Batch Loss: 0.06840267777442932\n",
      "Epoch 4636, Loss: 0.1552925854921341, Final Batch Loss: 0.06294457614421844\n",
      "Epoch 4637, Loss: 0.15113194286823273, Final Batch Loss: 0.088129423558712\n",
      "Epoch 4638, Loss: 0.12593750655651093, Final Batch Loss: 0.04131436347961426\n",
      "Epoch 4639, Loss: 0.23265618085861206, Final Batch Loss: 0.15297922492027283\n",
      "Epoch 4640, Loss: 0.1891387328505516, Final Batch Loss: 0.09073014557361603\n",
      "Epoch 4641, Loss: 0.11711452901363373, Final Batch Loss: 0.04898221790790558\n",
      "Epoch 4642, Loss: 0.1444620117545128, Final Batch Loss: 0.055536359548568726\n",
      "Epoch 4643, Loss: 0.1640184298157692, Final Batch Loss: 0.0779457613825798\n",
      "Epoch 4644, Loss: 0.21562596410512924, Final Batch Loss: 0.09408047050237656\n",
      "Epoch 4645, Loss: 0.2031961753964424, Final Batch Loss: 0.11692457646131516\n",
      "Epoch 4646, Loss: 0.18132547289133072, Final Batch Loss: 0.07836843281984329\n",
      "Epoch 4647, Loss: 0.17275124043226242, Final Batch Loss: 0.0754162073135376\n",
      "Epoch 4648, Loss: 0.15486550703644753, Final Batch Loss: 0.10616637766361237\n",
      "Epoch 4649, Loss: 0.15967730432748795, Final Batch Loss: 0.08281641453504562\n",
      "Epoch 4650, Loss: 0.14888222515583038, Final Batch Loss: 0.0633988231420517\n",
      "Epoch 4651, Loss: 0.1543327122926712, Final Batch Loss: 0.06317906081676483\n",
      "Epoch 4652, Loss: 0.19770825654268265, Final Batch Loss: 0.11047110706567764\n",
      "Epoch 4653, Loss: 0.2090046927332878, Final Batch Loss: 0.13683047890663147\n",
      "Epoch 4654, Loss: 0.23890453577041626, Final Batch Loss: 0.15886175632476807\n",
      "Epoch 4655, Loss: 0.15697485953569412, Final Batch Loss: 0.07710893452167511\n",
      "Epoch 4656, Loss: 0.21013674139976501, Final Batch Loss: 0.09475334733724594\n",
      "Epoch 4657, Loss: 0.1697131097316742, Final Batch Loss: 0.0494861975312233\n",
      "Epoch 4658, Loss: 0.14374680072069168, Final Batch Loss: 0.07302482426166534\n",
      "Epoch 4659, Loss: 0.20184359699487686, Final Batch Loss: 0.09986071288585663\n",
      "Epoch 4660, Loss: 0.1545119285583496, Final Batch Loss: 0.08574140816926956\n",
      "Epoch 4661, Loss: 0.18072228878736496, Final Batch Loss: 0.06807871907949448\n",
      "Epoch 4662, Loss: 0.11486094817519188, Final Batch Loss: 0.05146962031722069\n",
      "Epoch 4663, Loss: 0.1813066229224205, Final Batch Loss: 0.08070489019155502\n",
      "Epoch 4664, Loss: 0.144203782081604, Final Batch Loss: 0.062094494700431824\n",
      "Epoch 4665, Loss: 0.13595961034297943, Final Batch Loss: 0.050158075988292694\n",
      "Epoch 4666, Loss: 0.1620081663131714, Final Batch Loss: 0.09370047599077225\n",
      "Epoch 4667, Loss: 0.13793589919805527, Final Batch Loss: 0.059919968247413635\n",
      "Epoch 4668, Loss: 0.1767847090959549, Final Batch Loss: 0.08801089972257614\n",
      "Epoch 4669, Loss: 0.17928814142942429, Final Batch Loss: 0.08887846022844315\n",
      "Epoch 4670, Loss: 0.1523757427930832, Final Batch Loss: 0.08209122717380524\n",
      "Epoch 4671, Loss: 0.13119176775217056, Final Batch Loss: 0.05872587859630585\n",
      "Epoch 4672, Loss: 0.13423827290534973, Final Batch Loss: 0.0794689729809761\n",
      "Epoch 4673, Loss: 0.18823177367448807, Final Batch Loss: 0.0853225365281105\n",
      "Epoch 4674, Loss: 0.2231682911515236, Final Batch Loss: 0.12812578678131104\n",
      "Epoch 4675, Loss: 0.15370580926537514, Final Batch Loss: 0.061956148594617844\n",
      "Epoch 4676, Loss: 0.21584110707044601, Final Batch Loss: 0.10918976366519928\n",
      "Epoch 4677, Loss: 0.1646624654531479, Final Batch Loss: 0.0736144557595253\n",
      "Epoch 4678, Loss: 0.2820191830396652, Final Batch Loss: 0.10188433527946472\n",
      "Epoch 4679, Loss: 0.17764727771282196, Final Batch Loss: 0.06508231163024902\n",
      "Epoch 4680, Loss: 0.1399144008755684, Final Batch Loss: 0.06666775047779083\n",
      "Epoch 4681, Loss: 0.17835604399442673, Final Batch Loss: 0.1039666086435318\n",
      "Epoch 4682, Loss: 0.15567008778452873, Final Batch Loss: 0.059829775243997574\n",
      "Epoch 4683, Loss: 0.19999554753303528, Final Batch Loss: 0.07934439182281494\n",
      "Epoch 4684, Loss: 0.22506290674209595, Final Batch Loss: 0.09985633194446564\n",
      "Epoch 4685, Loss: 0.15975675731897354, Final Batch Loss: 0.0684952512383461\n",
      "Epoch 4686, Loss: 0.14687814563512802, Final Batch Loss: 0.06409673392772675\n",
      "Epoch 4687, Loss: 0.14115700870752335, Final Batch Loss: 0.06217224895954132\n",
      "Epoch 4688, Loss: 0.2264474555850029, Final Batch Loss: 0.11642225086688995\n",
      "Epoch 4689, Loss: 0.16393882781267166, Final Batch Loss: 0.09522201120853424\n",
      "Epoch 4690, Loss: 0.11402970552444458, Final Batch Loss: 0.05867096409201622\n",
      "Epoch 4691, Loss: 0.20386210829019547, Final Batch Loss: 0.07261496037244797\n",
      "Epoch 4692, Loss: 0.15345891937613487, Final Batch Loss: 0.056733157485723495\n",
      "Epoch 4693, Loss: 0.2356114313006401, Final Batch Loss: 0.08419431000947952\n",
      "Epoch 4694, Loss: 0.14743689820170403, Final Batch Loss: 0.09128683060407639\n",
      "Epoch 4695, Loss: 0.20600466430187225, Final Batch Loss: 0.11243662238121033\n",
      "Epoch 4696, Loss: 0.1462438702583313, Final Batch Loss: 0.06332583725452423\n",
      "Epoch 4697, Loss: 0.15274494141340256, Final Batch Loss: 0.0778818428516388\n",
      "Epoch 4698, Loss: 0.09922029450535774, Final Batch Loss: 0.04292451590299606\n",
      "Epoch 4699, Loss: 0.21376854181289673, Final Batch Loss: 0.10812853276729584\n",
      "Epoch 4700, Loss: 0.16766852885484695, Final Batch Loss: 0.09718794375658035\n",
      "Epoch 4701, Loss: 0.1910809800028801, Final Batch Loss: 0.10515505820512772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4702, Loss: 0.1643076427280903, Final Batch Loss: 0.1105375662446022\n",
      "Epoch 4703, Loss: 0.18300504982471466, Final Batch Loss: 0.09288619458675385\n",
      "Epoch 4704, Loss: 0.20212916284799576, Final Batch Loss: 0.10797663778066635\n",
      "Epoch 4705, Loss: 0.18979110568761826, Final Batch Loss: 0.09451813995838165\n",
      "Epoch 4706, Loss: 0.1341625079512596, Final Batch Loss: 0.0701378881931305\n",
      "Epoch 4707, Loss: 0.1298065036535263, Final Batch Loss: 0.044420063495635986\n",
      "Epoch 4708, Loss: 0.1282118782401085, Final Batch Loss: 0.06889662146568298\n",
      "Epoch 4709, Loss: 0.12160596624016762, Final Batch Loss: 0.07159682363271713\n",
      "Epoch 4710, Loss: 0.20704934000968933, Final Batch Loss: 0.1374811977148056\n",
      "Epoch 4711, Loss: 0.13496509194374084, Final Batch Loss: 0.0687471553683281\n",
      "Epoch 4712, Loss: 0.14490610361099243, Final Batch Loss: 0.05774006247520447\n",
      "Epoch 4713, Loss: 0.14596158266067505, Final Batch Loss: 0.07491839677095413\n",
      "Epoch 4714, Loss: 0.18809399753808975, Final Batch Loss: 0.0831986740231514\n",
      "Epoch 4715, Loss: 0.1380360648036003, Final Batch Loss: 0.06983473151922226\n",
      "Epoch 4716, Loss: 0.1849198192358017, Final Batch Loss: 0.12303072214126587\n",
      "Epoch 4717, Loss: 0.15755726397037506, Final Batch Loss: 0.07427659630775452\n",
      "Epoch 4718, Loss: 0.15829455107450485, Final Batch Loss: 0.07429690659046173\n",
      "Epoch 4719, Loss: 0.12837942317128181, Final Batch Loss: 0.07461027055978775\n",
      "Epoch 4720, Loss: 0.1680813804268837, Final Batch Loss: 0.11452090740203857\n",
      "Epoch 4721, Loss: 0.13301324471831322, Final Batch Loss: 0.058186355978250504\n",
      "Epoch 4722, Loss: 0.13509608060121536, Final Batch Loss: 0.07052742689847946\n",
      "Epoch 4723, Loss: 0.18645596504211426, Final Batch Loss: 0.09066043049097061\n",
      "Epoch 4724, Loss: 0.18144507706165314, Final Batch Loss: 0.06981571018695831\n",
      "Epoch 4725, Loss: 0.17114802449941635, Final Batch Loss: 0.08597292751073837\n",
      "Epoch 4726, Loss: 0.18657684326171875, Final Batch Loss: 0.08207710087299347\n",
      "Epoch 4727, Loss: 0.1391528807580471, Final Batch Loss: 0.04877283051609993\n",
      "Epoch 4728, Loss: 0.14947455003857613, Final Batch Loss: 0.05055893585085869\n",
      "Epoch 4729, Loss: 0.18459807336330414, Final Batch Loss: 0.07948530465364456\n",
      "Epoch 4730, Loss: 0.14410517364740372, Final Batch Loss: 0.07969589531421661\n",
      "Epoch 4731, Loss: 0.17215372622013092, Final Batch Loss: 0.08002615720033646\n",
      "Epoch 4732, Loss: 0.18057461082935333, Final Batch Loss: 0.11641177535057068\n",
      "Epoch 4733, Loss: 0.14694562554359436, Final Batch Loss: 0.08198417723178864\n",
      "Epoch 4734, Loss: 0.16053449362516403, Final Batch Loss: 0.11764252185821533\n",
      "Epoch 4735, Loss: 0.15744175016880035, Final Batch Loss: 0.09244833886623383\n",
      "Epoch 4736, Loss: 0.1903844252228737, Final Batch Loss: 0.10643984377384186\n",
      "Epoch 4737, Loss: 0.15219561010599136, Final Batch Loss: 0.08321481198072433\n",
      "Epoch 4738, Loss: 0.1519409641623497, Final Batch Loss: 0.0669030025601387\n",
      "Epoch 4739, Loss: 0.1759902536869049, Final Batch Loss: 0.08308626711368561\n",
      "Epoch 4740, Loss: 0.1254614219069481, Final Batch Loss: 0.059069447219371796\n",
      "Epoch 4741, Loss: 0.16604653745889664, Final Batch Loss: 0.07583677768707275\n",
      "Epoch 4742, Loss: 0.14103030413389206, Final Batch Loss: 0.047889381647109985\n",
      "Epoch 4743, Loss: 0.12666798755526543, Final Batch Loss: 0.044007088989019394\n",
      "Epoch 4744, Loss: 0.20029378682374954, Final Batch Loss: 0.12127766013145447\n",
      "Epoch 4745, Loss: 0.152238667011261, Final Batch Loss: 0.06409841030836105\n",
      "Epoch 4746, Loss: 0.219272643327713, Final Batch Loss: 0.15102680027484894\n",
      "Epoch 4747, Loss: 0.15376149863004684, Final Batch Loss: 0.05959097295999527\n",
      "Epoch 4748, Loss: 0.23683734983205795, Final Batch Loss: 0.14646682143211365\n",
      "Epoch 4749, Loss: 0.17986377701163292, Final Batch Loss: 0.1181940957903862\n",
      "Epoch 4750, Loss: 0.11154112964868546, Final Batch Loss: 0.03852523863315582\n",
      "Epoch 4751, Loss: 0.16107573732733727, Final Batch Loss: 0.05049214884638786\n",
      "Epoch 4752, Loss: 0.181355819106102, Final Batch Loss: 0.08881635963916779\n",
      "Epoch 4753, Loss: 0.11753439903259277, Final Batch Loss: 0.050276979804039\n",
      "Epoch 4754, Loss: 0.1385122425854206, Final Batch Loss: 0.09219979494810104\n",
      "Epoch 4755, Loss: 0.13074424862861633, Final Batch Loss: 0.06177204102277756\n",
      "Epoch 4756, Loss: 0.19046305865049362, Final Batch Loss: 0.12232933938503265\n",
      "Epoch 4757, Loss: 0.2060161605477333, Final Batch Loss: 0.11190898716449738\n",
      "Epoch 4758, Loss: 0.17052897065877914, Final Batch Loss: 0.101504385471344\n",
      "Epoch 4759, Loss: 0.10519595071673393, Final Batch Loss: 0.03332656994462013\n",
      "Epoch 4760, Loss: 0.10349410399794579, Final Batch Loss: 0.035704370588064194\n",
      "Epoch 4761, Loss: 0.18690136820077896, Final Batch Loss: 0.10128527879714966\n",
      "Epoch 4762, Loss: 0.10196937993168831, Final Batch Loss: 0.037353482097387314\n",
      "Epoch 4763, Loss: 0.12064354121685028, Final Batch Loss: 0.06585118919610977\n",
      "Epoch 4764, Loss: 0.19685447961091995, Final Batch Loss: 0.10521095246076584\n",
      "Epoch 4765, Loss: 0.139720119535923, Final Batch Loss: 0.07542996853590012\n",
      "Epoch 4766, Loss: 0.12959504127502441, Final Batch Loss: 0.06042829900979996\n",
      "Epoch 4767, Loss: 0.181118942797184, Final Batch Loss: 0.10104182362556458\n",
      "Epoch 4768, Loss: 0.14395838975906372, Final Batch Loss: 0.07698757946491241\n",
      "Epoch 4769, Loss: 0.13799982517957687, Final Batch Loss: 0.0774516761302948\n",
      "Epoch 4770, Loss: 0.1970250979065895, Final Batch Loss: 0.07799524813890457\n",
      "Epoch 4771, Loss: 0.2487781047821045, Final Batch Loss: 0.18067245185375214\n",
      "Epoch 4772, Loss: 0.1805468425154686, Final Batch Loss: 0.08990544080734253\n",
      "Epoch 4773, Loss: 0.14390908926725388, Final Batch Loss: 0.08328384906053543\n",
      "Epoch 4774, Loss: 0.12888811901211739, Final Batch Loss: 0.06008261814713478\n",
      "Epoch 4775, Loss: 0.15661247819662094, Final Batch Loss: 0.07779807597398758\n",
      "Epoch 4776, Loss: 0.14677245542407036, Final Batch Loss: 0.049608368426561356\n",
      "Epoch 4777, Loss: 0.2567693591117859, Final Batch Loss: 0.14136993885040283\n",
      "Epoch 4778, Loss: 0.1180221252143383, Final Batch Loss: 0.045908037573099136\n",
      "Epoch 4779, Loss: 0.19233956933021545, Final Batch Loss: 0.10504145920276642\n",
      "Epoch 4780, Loss: 0.18074629455804825, Final Batch Loss: 0.11470296233892441\n",
      "Epoch 4781, Loss: 0.24535658955574036, Final Batch Loss: 0.1379251480102539\n",
      "Epoch 4782, Loss: 0.1376834698021412, Final Batch Loss: 0.08398710191249847\n",
      "Epoch 4783, Loss: 0.21523858606815338, Final Batch Loss: 0.08802187442779541\n",
      "Epoch 4784, Loss: 0.13524865731596947, Final Batch Loss: 0.10209763795137405\n",
      "Epoch 4785, Loss: 0.14282943680882454, Final Batch Loss: 0.04774134233593941\n",
      "Epoch 4786, Loss: 0.11759137362241745, Final Batch Loss: 0.06760085374116898\n",
      "Epoch 4787, Loss: 0.1837562769651413, Final Batch Loss: 0.11866097897291183\n",
      "Epoch 4788, Loss: 0.17031291127204895, Final Batch Loss: 0.10232701897621155\n",
      "Epoch 4789, Loss: 0.1947154700756073, Final Batch Loss: 0.13086159527301788\n",
      "Epoch 4790, Loss: 0.22669261693954468, Final Batch Loss: 0.11315406113862991\n",
      "Epoch 4791, Loss: 0.2078276127576828, Final Batch Loss: 0.07077933847904205\n",
      "Epoch 4792, Loss: 0.13210075721144676, Final Batch Loss: 0.04940931871533394\n",
      "Epoch 4793, Loss: 0.17993279546499252, Final Batch Loss: 0.10947153717279434\n",
      "Epoch 4794, Loss: 0.16584919393062592, Final Batch Loss: 0.07796920090913773\n",
      "Epoch 4795, Loss: 0.17830480635166168, Final Batch Loss: 0.08636589348316193\n",
      "Epoch 4796, Loss: 0.1510397493839264, Final Batch Loss: 0.06498242169618607\n",
      "Epoch 4797, Loss: 0.12864777818322182, Final Batch Loss: 0.07245344668626785\n",
      "Epoch 4798, Loss: 0.15752776712179184, Final Batch Loss: 0.05967279523611069\n",
      "Epoch 4799, Loss: 0.08798081055283546, Final Batch Loss: 0.038829997181892395\n",
      "Epoch 4800, Loss: 0.15929894149303436, Final Batch Loss: 0.0870426595211029\n",
      "Epoch 4801, Loss: 0.22040054202079773, Final Batch Loss: 0.09748855978250504\n",
      "Epoch 4802, Loss: 0.14023229107260704, Final Batch Loss: 0.05436882749199867\n",
      "Epoch 4803, Loss: 0.13030259311199188, Final Batch Loss: 0.07477686554193497\n",
      "Epoch 4804, Loss: 0.17369694262742996, Final Batch Loss: 0.07706276327371597\n",
      "Epoch 4805, Loss: 0.14982625097036362, Final Batch Loss: 0.05152429640293121\n",
      "Epoch 4806, Loss: 0.14210588857531548, Final Batch Loss: 0.08719757944345474\n",
      "Epoch 4807, Loss: 0.16891685873270035, Final Batch Loss: 0.08631222695112228\n",
      "Epoch 4808, Loss: 0.19490999728441238, Final Batch Loss: 0.09521287679672241\n",
      "Epoch 4809, Loss: 0.17939968407154083, Final Batch Loss: 0.1046631932258606\n",
      "Epoch 4810, Loss: 0.15622834861278534, Final Batch Loss: 0.08609897643327713\n",
      "Epoch 4811, Loss: 0.13773882389068604, Final Batch Loss: 0.07347801327705383\n",
      "Epoch 4812, Loss: 0.20172109454870224, Final Batch Loss: 0.08438730239868164\n",
      "Epoch 4813, Loss: 0.17171459645032883, Final Batch Loss: 0.07998661696910858\n",
      "Epoch 4814, Loss: 0.12315549701452255, Final Batch Loss: 0.060717180371284485\n",
      "Epoch 4815, Loss: 0.17318154126405716, Final Batch Loss: 0.07377015054225922\n",
      "Epoch 4816, Loss: 0.17959892004728317, Final Batch Loss: 0.09826011210680008\n",
      "Epoch 4817, Loss: 0.20030764117836952, Final Batch Loss: 0.05731779709458351\n",
      "Epoch 4818, Loss: 0.13054149597883224, Final Batch Loss: 0.03760676085948944\n",
      "Epoch 4819, Loss: 0.19641303271055222, Final Batch Loss: 0.08288915455341339\n",
      "Epoch 4820, Loss: 0.19701546430587769, Final Batch Loss: 0.09575413167476654\n",
      "Epoch 4821, Loss: 0.1438695415854454, Final Batch Loss: 0.06832912564277649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4822, Loss: 0.1525910571217537, Final Batch Loss: 0.0785343274474144\n",
      "Epoch 4823, Loss: 0.17387930303812027, Final Batch Loss: 0.09627722203731537\n",
      "Epoch 4824, Loss: 0.14759214594960213, Final Batch Loss: 0.05649534985423088\n",
      "Epoch 4825, Loss: 0.17713019251823425, Final Batch Loss: 0.09233337640762329\n",
      "Epoch 4826, Loss: 0.340451717376709, Final Batch Loss: 0.26562219858169556\n",
      "Epoch 4827, Loss: 0.11711988598108292, Final Batch Loss: 0.03761615604162216\n",
      "Epoch 4828, Loss: 0.18639381974935532, Final Batch Loss: 0.10415305197238922\n",
      "Epoch 4829, Loss: 0.11145796626806259, Final Batch Loss: 0.040942564606666565\n",
      "Epoch 4830, Loss: 0.19571950286626816, Final Batch Loss: 0.11650720983743668\n",
      "Epoch 4831, Loss: 0.13001370057463646, Final Batch Loss: 0.08287064731121063\n",
      "Epoch 4832, Loss: 0.1822708621621132, Final Batch Loss: 0.11607160419225693\n",
      "Epoch 4833, Loss: 0.14289269968867302, Final Batch Loss: 0.08186275511980057\n",
      "Epoch 4834, Loss: 0.2083548754453659, Final Batch Loss: 0.08166810870170593\n",
      "Epoch 4835, Loss: 0.2422129362821579, Final Batch Loss: 0.0659269243478775\n",
      "Epoch 4836, Loss: 0.17344605177640915, Final Batch Loss: 0.10077737271785736\n",
      "Epoch 4837, Loss: 0.1792452409863472, Final Batch Loss: 0.08071549981832504\n",
      "Epoch 4838, Loss: 0.11899722367525101, Final Batch Loss: 0.04912569373846054\n",
      "Epoch 4839, Loss: 0.11622332781553268, Final Batch Loss: 0.04239107668399811\n",
      "Epoch 4840, Loss: 0.22835637629032135, Final Batch Loss: 0.108830027282238\n",
      "Epoch 4841, Loss: 0.17236538231372833, Final Batch Loss: 0.11056805402040482\n",
      "Epoch 4842, Loss: 0.16970432549715042, Final Batch Loss: 0.08761268109083176\n",
      "Epoch 4843, Loss: 0.14994729682803154, Final Batch Loss: 0.094502292573452\n",
      "Epoch 4844, Loss: 0.20106521993875504, Final Batch Loss: 0.127212792634964\n",
      "Epoch 4845, Loss: 0.1767742782831192, Final Batch Loss: 0.10428129881620407\n",
      "Epoch 4846, Loss: 0.1375964805483818, Final Batch Loss: 0.05267731845378876\n",
      "Epoch 4847, Loss: 0.1212143823504448, Final Batch Loss: 0.04525122791528702\n",
      "Epoch 4848, Loss: 0.0945298969745636, Final Batch Loss: 0.047995638102293015\n",
      "Epoch 4849, Loss: 0.1442754790186882, Final Batch Loss: 0.0723828375339508\n",
      "Epoch 4850, Loss: 0.2053881213068962, Final Batch Loss: 0.12160541862249374\n",
      "Epoch 4851, Loss: 0.1879853531718254, Final Batch Loss: 0.09330902993679047\n",
      "Epoch 4852, Loss: 0.10788260400295258, Final Batch Loss: 0.06297820806503296\n",
      "Epoch 4853, Loss: 0.17343053966760635, Final Batch Loss: 0.07398606836795807\n",
      "Epoch 4854, Loss: 0.14722710102796555, Final Batch Loss: 0.08780116587877274\n",
      "Epoch 4855, Loss: 0.1689130812883377, Final Batch Loss: 0.08371245115995407\n",
      "Epoch 4856, Loss: 0.14908218383789062, Final Batch Loss: 0.08946205675601959\n",
      "Epoch 4857, Loss: 0.1663632094860077, Final Batch Loss: 0.06975830346345901\n",
      "Epoch 4858, Loss: 0.12870316579937935, Final Batch Loss: 0.054841648787260056\n",
      "Epoch 4859, Loss: 0.12485568597912788, Final Batch Loss: 0.05570756271481514\n",
      "Epoch 4860, Loss: 0.16098905354738235, Final Batch Loss: 0.08244916796684265\n",
      "Epoch 4861, Loss: 0.13719850778579712, Final Batch Loss: 0.04393325746059418\n",
      "Epoch 4862, Loss: 0.15879938751459122, Final Batch Loss: 0.0749666690826416\n",
      "Epoch 4863, Loss: 0.13508455827832222, Final Batch Loss: 0.05711087957024574\n",
      "Epoch 4864, Loss: 0.1680636703968048, Final Batch Loss: 0.11287405341863632\n",
      "Epoch 4865, Loss: 0.1571623757481575, Final Batch Loss: 0.09478864073753357\n",
      "Epoch 4866, Loss: 0.24192775785923004, Final Batch Loss: 0.1759382039308548\n",
      "Epoch 4867, Loss: 0.17867335677146912, Final Batch Loss: 0.11689221113920212\n",
      "Epoch 4868, Loss: 0.10059091448783875, Final Batch Loss: 0.056543972343206406\n",
      "Epoch 4869, Loss: 0.1592894196510315, Final Batch Loss: 0.10181114822626114\n",
      "Epoch 4870, Loss: 0.14266006648540497, Final Batch Loss: 0.05896229296922684\n",
      "Epoch 4871, Loss: 0.12532323971390724, Final Batch Loss: 0.04844462499022484\n",
      "Epoch 4872, Loss: 0.16310013085603714, Final Batch Loss: 0.09035299718379974\n",
      "Epoch 4873, Loss: 0.17291782051324844, Final Batch Loss: 0.11345431953668594\n",
      "Epoch 4874, Loss: 0.1700776368379593, Final Batch Loss: 0.10681802034378052\n",
      "Epoch 4875, Loss: 0.17530852556228638, Final Batch Loss: 0.07989602535963058\n",
      "Epoch 4876, Loss: 0.16634264960885048, Final Batch Loss: 0.11626965552568436\n",
      "Epoch 4877, Loss: 0.15518643707036972, Final Batch Loss: 0.07368709146976471\n",
      "Epoch 4878, Loss: 0.1528518721461296, Final Batch Loss: 0.08226093649864197\n",
      "Epoch 4879, Loss: 0.2034260705113411, Final Batch Loss: 0.07197857648134232\n",
      "Epoch 4880, Loss: 0.2268628254532814, Final Batch Loss: 0.10527051240205765\n",
      "Epoch 4881, Loss: 0.1498650722205639, Final Batch Loss: 0.06120210513472557\n",
      "Epoch 4882, Loss: 0.18739016354084015, Final Batch Loss: 0.0793459415435791\n",
      "Epoch 4883, Loss: 0.1887960359454155, Final Batch Loss: 0.09611960500478745\n",
      "Epoch 4884, Loss: 0.11304322257637978, Final Batch Loss: 0.05647031590342522\n",
      "Epoch 4885, Loss: 0.1558058150112629, Final Batch Loss: 0.059342335909605026\n",
      "Epoch 4886, Loss: 0.21005956828594208, Final Batch Loss: 0.09875896573066711\n",
      "Epoch 4887, Loss: 0.1654948890209198, Final Batch Loss: 0.08666901290416718\n",
      "Epoch 4888, Loss: 0.18940285593271255, Final Batch Loss: 0.1033986508846283\n",
      "Epoch 4889, Loss: 0.1966848373413086, Final Batch Loss: 0.12740033864974976\n",
      "Epoch 4890, Loss: 0.21572256088256836, Final Batch Loss: 0.08849534392356873\n",
      "Epoch 4891, Loss: 0.18132305145263672, Final Batch Loss: 0.08722641319036484\n",
      "Epoch 4892, Loss: 0.12401281297206879, Final Batch Loss: 0.047855667769908905\n",
      "Epoch 4893, Loss: 0.15432457998394966, Final Batch Loss: 0.060559723526239395\n",
      "Epoch 4894, Loss: 0.14479684457182884, Final Batch Loss: 0.042795661836862564\n",
      "Epoch 4895, Loss: 0.15268393605947495, Final Batch Loss: 0.07090283185243607\n",
      "Epoch 4896, Loss: 0.11008770018815994, Final Batch Loss: 0.041918858885765076\n",
      "Epoch 4897, Loss: 0.16201340407133102, Final Batch Loss: 0.06525462865829468\n",
      "Epoch 4898, Loss: 0.14430446922779083, Final Batch Loss: 0.07807288318872452\n",
      "Epoch 4899, Loss: 0.17113639786839485, Final Batch Loss: 0.05583016201853752\n",
      "Epoch 4900, Loss: 0.1420852243900299, Final Batch Loss: 0.09140542894601822\n",
      "Epoch 4901, Loss: 0.11820945143699646, Final Batch Loss: 0.044386573135852814\n",
      "Epoch 4902, Loss: 0.19535840302705765, Final Batch Loss: 0.1003408432006836\n",
      "Epoch 4903, Loss: 0.14269007369875908, Final Batch Loss: 0.061100494116544724\n",
      "Epoch 4904, Loss: 0.12623705342411995, Final Batch Loss: 0.06123431399464607\n",
      "Epoch 4905, Loss: 0.13914253562688828, Final Batch Loss: 0.08803382515907288\n",
      "Epoch 4906, Loss: 0.18302135542035103, Final Batch Loss: 0.13224944472312927\n",
      "Epoch 4907, Loss: 0.1297206822782755, Final Batch Loss: 0.030326606705784798\n",
      "Epoch 4908, Loss: 0.1648172065615654, Final Batch Loss: 0.08625423163175583\n",
      "Epoch 4909, Loss: 0.11979271471500397, Final Batch Loss: 0.05040203034877777\n",
      "Epoch 4910, Loss: 0.15092897415161133, Final Batch Loss: 0.07684315741062164\n",
      "Epoch 4911, Loss: 0.18495702743530273, Final Batch Loss: 0.0745234414935112\n",
      "Epoch 4912, Loss: 0.14689185842871666, Final Batch Loss: 0.08530405163764954\n",
      "Epoch 4913, Loss: 0.14598911255598068, Final Batch Loss: 0.05026044696569443\n",
      "Epoch 4914, Loss: 0.14355041459202766, Final Batch Loss: 0.056331735104322433\n",
      "Epoch 4915, Loss: 0.18271727859973907, Final Batch Loss: 0.10572178661823273\n",
      "Epoch 4916, Loss: 0.17616870254278183, Final Batch Loss: 0.08434507995843887\n",
      "Epoch 4917, Loss: 0.10298092290759087, Final Batch Loss: 0.04680035263299942\n",
      "Epoch 4918, Loss: 0.17412817478179932, Final Batch Loss: 0.0997830256819725\n",
      "Epoch 4919, Loss: 0.188835509121418, Final Batch Loss: 0.1250205934047699\n",
      "Epoch 4920, Loss: 0.09864432364702225, Final Batch Loss: 0.03700222074985504\n",
      "Epoch 4921, Loss: 0.17471236735582352, Final Batch Loss: 0.08650321513414383\n",
      "Epoch 4922, Loss: 0.16102228313684464, Final Batch Loss: 0.08795146644115448\n",
      "Epoch 4923, Loss: 0.2279408723115921, Final Batch Loss: 0.13394618034362793\n",
      "Epoch 4924, Loss: 0.1798795685172081, Final Batch Loss: 0.05342084914445877\n",
      "Epoch 4925, Loss: 0.15988365560770035, Final Batch Loss: 0.0698317140340805\n",
      "Epoch 4926, Loss: 0.14455924928188324, Final Batch Loss: 0.047562889754772186\n",
      "Epoch 4927, Loss: 0.17241884768009186, Final Batch Loss: 0.07330476492643356\n",
      "Epoch 4928, Loss: 0.14242685586214066, Final Batch Loss: 0.058182790875434875\n",
      "Epoch 4929, Loss: 0.11047188192605972, Final Batch Loss: 0.0653972327709198\n",
      "Epoch 4930, Loss: 0.17266210168600082, Final Batch Loss: 0.08147139847278595\n",
      "Epoch 4931, Loss: 0.14868653565645218, Final Batch Loss: 0.08588441461324692\n",
      "Epoch 4932, Loss: 0.16363035887479782, Final Batch Loss: 0.09720559418201447\n",
      "Epoch 4933, Loss: 0.1755773387849331, Final Batch Loss: 0.11826583743095398\n",
      "Epoch 4934, Loss: 0.14812270551919937, Final Batch Loss: 0.07046283781528473\n",
      "Epoch 4935, Loss: 0.19897299259901047, Final Batch Loss: 0.07292451709508896\n",
      "Epoch 4936, Loss: 0.16993243247270584, Final Batch Loss: 0.08751299977302551\n",
      "Epoch 4937, Loss: 0.1201409175992012, Final Batch Loss: 0.05591953545808792\n",
      "Epoch 4938, Loss: 0.210605651140213, Final Batch Loss: 0.10610485821962357\n",
      "Epoch 4939, Loss: 0.12856382504105568, Final Batch Loss: 0.057101454585790634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4940, Loss: 0.21118063479661942, Final Batch Loss: 0.10851456969976425\n",
      "Epoch 4941, Loss: 0.18527807295322418, Final Batch Loss: 0.10312674194574356\n",
      "Epoch 4942, Loss: 0.10827742144465446, Final Batch Loss: 0.05189569666981697\n",
      "Epoch 4943, Loss: 0.1392400749027729, Final Batch Loss: 0.04420914873480797\n",
      "Epoch 4944, Loss: 0.17862534523010254, Final Batch Loss: 0.10103666037321091\n",
      "Epoch 4945, Loss: 0.2070951610803604, Final Batch Loss: 0.11169922351837158\n",
      "Epoch 4946, Loss: 0.13848775625228882, Final Batch Loss: 0.0644582211971283\n",
      "Epoch 4947, Loss: 0.11258869990706444, Final Batch Loss: 0.07015331834554672\n",
      "Epoch 4948, Loss: 0.1309063583612442, Final Batch Loss: 0.06216906011104584\n",
      "Epoch 4949, Loss: 0.14048971235752106, Final Batch Loss: 0.06715201586484909\n",
      "Epoch 4950, Loss: 0.19230031967163086, Final Batch Loss: 0.09181056171655655\n",
      "Epoch 4951, Loss: 0.19080150872468948, Final Batch Loss: 0.07962097972631454\n",
      "Epoch 4952, Loss: 0.15174728631973267, Final Batch Loss: 0.07575929164886475\n",
      "Epoch 4953, Loss: 0.14125892519950867, Final Batch Loss: 0.04681995511054993\n",
      "Epoch 4954, Loss: 0.23640283942222595, Final Batch Loss: 0.15218183398246765\n",
      "Epoch 4955, Loss: 0.138225257396698, Final Batch Loss: 0.07094702869653702\n",
      "Epoch 4956, Loss: 0.12933571264147758, Final Batch Loss: 0.07750304788351059\n",
      "Epoch 4957, Loss: 0.12410323321819305, Final Batch Loss: 0.052635520696640015\n",
      "Epoch 4958, Loss: 0.17021526768803596, Final Batch Loss: 0.061485666781663895\n",
      "Epoch 4959, Loss: 0.13036351650953293, Final Batch Loss: 0.06876074522733688\n",
      "Epoch 4960, Loss: 0.15156199038028717, Final Batch Loss: 0.06339825689792633\n",
      "Epoch 4961, Loss: 0.1264670342206955, Final Batch Loss: 0.05849819630384445\n",
      "Epoch 4962, Loss: 0.14400260150432587, Final Batch Loss: 0.06868994981050491\n",
      "Epoch 4963, Loss: 0.16290654242038727, Final Batch Loss: 0.1179526150226593\n",
      "Epoch 4964, Loss: 0.13500866293907166, Final Batch Loss: 0.057942405343055725\n",
      "Epoch 4965, Loss: 0.14840490743517876, Final Batch Loss: 0.04832875356078148\n",
      "Epoch 4966, Loss: 0.15332868695259094, Final Batch Loss: 0.06710975617170334\n",
      "Epoch 4967, Loss: 0.2119818702340126, Final Batch Loss: 0.11172527819871902\n",
      "Epoch 4968, Loss: 0.15864740312099457, Final Batch Loss: 0.06634197384119034\n",
      "Epoch 4969, Loss: 0.13806626200675964, Final Batch Loss: 0.05052062124013901\n",
      "Epoch 4970, Loss: 0.12859474122524261, Final Batch Loss: 0.044921502470970154\n",
      "Epoch 4971, Loss: 0.14583109319210052, Final Batch Loss: 0.06896165013313293\n",
      "Epoch 4972, Loss: 0.1894063875079155, Final Batch Loss: 0.09120619297027588\n",
      "Epoch 4973, Loss: 0.16675888001918793, Final Batch Loss: 0.1015947014093399\n",
      "Epoch 4974, Loss: 0.17356348037719727, Final Batch Loss: 0.07466009259223938\n",
      "Epoch 4975, Loss: 0.2838255539536476, Final Batch Loss: 0.08687487989664078\n",
      "Epoch 4976, Loss: 0.1677560806274414, Final Batch Loss: 0.09654086083173752\n",
      "Epoch 4977, Loss: 0.11489462107419968, Final Batch Loss: 0.06510678678750992\n",
      "Epoch 4978, Loss: 0.1569478064775467, Final Batch Loss: 0.08388182520866394\n",
      "Epoch 4979, Loss: 0.18241728842258453, Final Batch Loss: 0.10182180255651474\n",
      "Epoch 4980, Loss: 0.16026413440704346, Final Batch Loss: 0.07708745449781418\n",
      "Epoch 4981, Loss: 0.18807346373796463, Final Batch Loss: 0.11281678825616837\n",
      "Epoch 4982, Loss: 0.15899166092276573, Final Batch Loss: 0.05882839486002922\n",
      "Epoch 4983, Loss: 0.1275445558130741, Final Batch Loss: 0.06644956767559052\n",
      "Epoch 4984, Loss: 0.14542458951473236, Final Batch Loss: 0.09555620700120926\n",
      "Epoch 4985, Loss: 0.1707097701728344, Final Batch Loss: 0.1132831946015358\n",
      "Epoch 4986, Loss: 0.20820626989006996, Final Batch Loss: 0.05656634643673897\n",
      "Epoch 4987, Loss: 0.11735639348626137, Final Batch Loss: 0.04452471062541008\n",
      "Epoch 4988, Loss: 0.139134019613266, Final Batch Loss: 0.05980799347162247\n",
      "Epoch 4989, Loss: 0.1923438385128975, Final Batch Loss: 0.09653838723897934\n",
      "Epoch 4990, Loss: 0.13131878525018692, Final Batch Loss: 0.03684011846780777\n",
      "Epoch 4991, Loss: 0.15889126062393188, Final Batch Loss: 0.07975928485393524\n",
      "Epoch 4992, Loss: 0.17096036672592163, Final Batch Loss: 0.09425492584705353\n",
      "Epoch 4993, Loss: 0.16561812907457352, Final Batch Loss: 0.06684541702270508\n",
      "Epoch 4994, Loss: 0.17198029905557632, Final Batch Loss: 0.08733978867530823\n",
      "Epoch 4995, Loss: 0.11193231120705605, Final Batch Loss: 0.05081842467188835\n",
      "Epoch 4996, Loss: 0.14101584255695343, Final Batch Loss: 0.08298401534557343\n",
      "Epoch 4997, Loss: 0.105082206428051, Final Batch Loss: 0.034996241331100464\n",
      "Epoch 4998, Loss: 0.16154291480779648, Final Batch Loss: 0.09593227505683899\n",
      "Epoch 4999, Loss: 0.1441381275653839, Final Batch Loss: 0.07034259289503098\n",
      "Epoch 5000, Loss: 0.1466975212097168, Final Batch Loss: 0.06863696128129959\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0 13  0  0  0  0  0  1]\n",
      " [ 0  0  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  1  0  0 11  0  0  1]\n",
      " [ 0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000        10\n",
      "           2    0.92857   0.92857   0.92857        14\n",
      "           3    1.00000   1.00000   1.00000        11\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   0.84615   0.91667        13\n",
      "           6    1.00000   1.00000   1.00000        13\n",
      "           7    1.00000   1.00000   1.00000         6\n",
      "           8    0.87500   1.00000   0.93333        14\n",
      "\n",
      "    accuracy                        0.96907        97\n",
      "   macro avg    0.97817   0.97497   0.97540        97\n",
      "weighted avg    0.97165   0.96907   0.96890        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_Group_3_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 3)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "\n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0 14  0  0  0  0  0  2]\n",
      " [ 0  0  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  0  0  0  0]\n",
      " [ 0  0  0  0  0 11  0  0  0]\n",
      " [ 0  0  1  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  1  0  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        16\n",
      "           1    1.00000   1.00000   1.00000        10\n",
      "           2    0.93333   0.87500   0.90323        16\n",
      "           3    1.00000   1.00000   1.00000        13\n",
      "           4    1.00000   1.00000   1.00000         6\n",
      "           5    0.91667   1.00000   0.95652        11\n",
      "           6    1.00000   0.85714   0.92308         7\n",
      "           7    1.00000   1.00000   1.00000         7\n",
      "           8    0.83333   0.90909   0.86957        11\n",
      "\n",
      "    accuracy                        0.95876        97\n",
      "   macro avg    0.96481   0.96014   0.96138        97\n",
      "weighted avg    0.96065   0.95876   0.95876        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
