{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 15) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A0 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_1 = gen(to_gen).detach().numpy()\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A1 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_2 = gen(to_gen).detach().numpy()\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A2 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_3 = gen(to_gen).detach().numpy()\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A0 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_4 = gen(to_gen).detach().numpy()\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A1 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_5 = gen(to_gen).detach().numpy()\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A2 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_6 = gen(to_gen).detach().numpy()\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A0 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_7 = gen(to_gen).detach().numpy()\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A1 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_8 = gen(to_gen).detach().numpy()\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A2 Excluded Group 3_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_9 = gen(to_gen).detach().numpy()\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_test = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_test = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [14, 15, 17]\n",
    "X_train, y_train = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.416366338729858, Final Batch Loss: 2.2094311714172363\n",
      "Epoch 2, Loss: 4.411351680755615, Final Batch Loss: 2.2092018127441406\n",
      "Epoch 3, Loss: 4.409950256347656, Final Batch Loss: 2.205338716506958\n",
      "Epoch 4, Loss: 4.405186176300049, Final Batch Loss: 2.2035961151123047\n",
      "Epoch 5, Loss: 4.405171155929565, Final Batch Loss: 2.207744598388672\n",
      "Epoch 6, Loss: 4.406302213668823, Final Batch Loss: 2.201408624649048\n",
      "Epoch 7, Loss: 4.402022361755371, Final Batch Loss: 2.2073593139648438\n",
      "Epoch 8, Loss: 4.394659757614136, Final Batch Loss: 2.18928599357605\n",
      "Epoch 9, Loss: 4.403120994567871, Final Batch Loss: 2.201702833175659\n",
      "Epoch 10, Loss: 4.396866083145142, Final Batch Loss: 2.203338623046875\n",
      "Epoch 11, Loss: 4.389291048049927, Final Batch Loss: 2.1896309852600098\n",
      "Epoch 12, Loss: 4.383824348449707, Final Batch Loss: 2.1918842792510986\n",
      "Epoch 13, Loss: 4.378310441970825, Final Batch Loss: 2.190980911254883\n",
      "Epoch 14, Loss: 4.367805004119873, Final Batch Loss: 2.1798012256622314\n",
      "Epoch 15, Loss: 4.36975359916687, Final Batch Loss: 2.181169271469116\n",
      "Epoch 16, Loss: 4.362472772598267, Final Batch Loss: 2.1814608573913574\n",
      "Epoch 17, Loss: 4.338005781173706, Final Batch Loss: 2.172006130218506\n",
      "Epoch 18, Loss: 4.340042591094971, Final Batch Loss: 2.164891481399536\n",
      "Epoch 19, Loss: 4.3231751918792725, Final Batch Loss: 2.141897201538086\n",
      "Epoch 20, Loss: 4.314562559127808, Final Batch Loss: 2.1501965522766113\n",
      "Epoch 21, Loss: 4.2988481521606445, Final Batch Loss: 2.123155355453491\n",
      "Epoch 22, Loss: 4.2806806564331055, Final Batch Loss: 2.1440954208374023\n",
      "Epoch 23, Loss: 4.254770755767822, Final Batch Loss: 2.126997232437134\n",
      "Epoch 24, Loss: 4.25675106048584, Final Batch Loss: 2.1292009353637695\n",
      "Epoch 25, Loss: 4.211501121520996, Final Batch Loss: 2.097763776779175\n",
      "Epoch 26, Loss: 4.202367305755615, Final Batch Loss: 2.10899019241333\n",
      "Epoch 27, Loss: 4.180461645126343, Final Batch Loss: 2.1037163734436035\n",
      "Epoch 28, Loss: 4.172828912734985, Final Batch Loss: 2.1176812648773193\n",
      "Epoch 29, Loss: 4.122645616531372, Final Batch Loss: 2.04105544090271\n",
      "Epoch 30, Loss: 4.105215549468994, Final Batch Loss: 2.0405514240264893\n",
      "Epoch 31, Loss: 4.056314706802368, Final Batch Loss: 2.0253512859344482\n",
      "Epoch 32, Loss: 4.0390520095825195, Final Batch Loss: 2.0022053718566895\n",
      "Epoch 33, Loss: 3.9894927740097046, Final Batch Loss: 2.0101895332336426\n",
      "Epoch 34, Loss: 3.9921010732650757, Final Batch Loss: 1.9760063886642456\n",
      "Epoch 35, Loss: 3.9058690071105957, Final Batch Loss: 1.9521631002426147\n",
      "Epoch 36, Loss: 3.878057599067688, Final Batch Loss: 1.937887191772461\n",
      "Epoch 37, Loss: 3.8910038471221924, Final Batch Loss: 1.9586639404296875\n",
      "Epoch 38, Loss: 3.863737463951111, Final Batch Loss: 1.9199090003967285\n",
      "Epoch 39, Loss: 3.7688534259796143, Final Batch Loss: 1.901012659072876\n",
      "Epoch 40, Loss: 3.7335630655288696, Final Batch Loss: 1.8490631580352783\n",
      "Epoch 41, Loss: 3.688657522201538, Final Batch Loss: 1.8295743465423584\n",
      "Epoch 42, Loss: 3.7272555828094482, Final Batch Loss: 1.8276393413543701\n",
      "Epoch 43, Loss: 3.633660316467285, Final Batch Loss: 1.820274829864502\n",
      "Epoch 44, Loss: 3.5896668434143066, Final Batch Loss: 1.7592668533325195\n",
      "Epoch 45, Loss: 3.5789730548858643, Final Batch Loss: 1.7706044912338257\n",
      "Epoch 46, Loss: 3.511904001235962, Final Batch Loss: 1.7783339023590088\n",
      "Epoch 47, Loss: 3.5244109630584717, Final Batch Loss: 1.7739211320877075\n",
      "Epoch 48, Loss: 3.410957098007202, Final Batch Loss: 1.6622915267944336\n",
      "Epoch 49, Loss: 3.4154080152511597, Final Batch Loss: 1.7144356966018677\n",
      "Epoch 50, Loss: 3.269504189491272, Final Batch Loss: 1.6504896879196167\n",
      "Epoch 51, Loss: 3.313005566596985, Final Batch Loss: 1.6580852270126343\n",
      "Epoch 52, Loss: 3.3254841566085815, Final Batch Loss: 1.6976157426834106\n",
      "Epoch 53, Loss: 3.1914228200912476, Final Batch Loss: 1.5726999044418335\n",
      "Epoch 54, Loss: 3.17670738697052, Final Batch Loss: 1.5875532627105713\n",
      "Epoch 55, Loss: 3.14916455745697, Final Batch Loss: 1.6057292222976685\n",
      "Epoch 56, Loss: 3.036384701728821, Final Batch Loss: 1.5124787092208862\n",
      "Epoch 57, Loss: 3.004682779312134, Final Batch Loss: 1.4895038604736328\n",
      "Epoch 58, Loss: 3.0256186723709106, Final Batch Loss: 1.5001122951507568\n",
      "Epoch 59, Loss: 2.918910026550293, Final Batch Loss: 1.4648053646087646\n",
      "Epoch 60, Loss: 2.943194270133972, Final Batch Loss: 1.4756816625595093\n",
      "Epoch 61, Loss: 2.9430261850357056, Final Batch Loss: 1.4766790866851807\n",
      "Epoch 62, Loss: 2.8528648614883423, Final Batch Loss: 1.42471444606781\n",
      "Epoch 63, Loss: 2.778558850288391, Final Batch Loss: 1.402988314628601\n",
      "Epoch 64, Loss: 2.7231966257095337, Final Batch Loss: 1.343437671661377\n",
      "Epoch 65, Loss: 2.800734519958496, Final Batch Loss: 1.3881622552871704\n",
      "Epoch 66, Loss: 2.6768676042556763, Final Batch Loss: 1.2853480577468872\n",
      "Epoch 67, Loss: 2.6769996881484985, Final Batch Loss: 1.3312273025512695\n",
      "Epoch 68, Loss: 2.6071897745132446, Final Batch Loss: 1.2999204397201538\n",
      "Epoch 69, Loss: 2.6615806818008423, Final Batch Loss: 1.290068507194519\n",
      "Epoch 70, Loss: 2.528319478034973, Final Batch Loss: 1.2242326736450195\n",
      "Epoch 71, Loss: 2.513203978538513, Final Batch Loss: 1.2498890161514282\n",
      "Epoch 72, Loss: 2.461409091949463, Final Batch Loss: 1.2940281629562378\n",
      "Epoch 73, Loss: 2.5361058712005615, Final Batch Loss: 1.2231985330581665\n",
      "Epoch 74, Loss: 2.364816427230835, Final Batch Loss: 1.1952942609786987\n",
      "Epoch 75, Loss: 2.3240387439727783, Final Batch Loss: 1.1760355234146118\n",
      "Epoch 76, Loss: 2.405462145805359, Final Batch Loss: 1.1551820039749146\n",
      "Epoch 77, Loss: 2.300044059753418, Final Batch Loss: 1.1500383615493774\n",
      "Epoch 78, Loss: 2.3171664476394653, Final Batch Loss: 1.132800817489624\n",
      "Epoch 79, Loss: 2.2447952032089233, Final Batch Loss: 1.1044774055480957\n",
      "Epoch 80, Loss: 2.231815457344055, Final Batch Loss: 1.0861115455627441\n",
      "Epoch 81, Loss: 2.18474280834198, Final Batch Loss: 1.1243196725845337\n",
      "Epoch 82, Loss: 2.2059555053710938, Final Batch Loss: 1.1120232343673706\n",
      "Epoch 83, Loss: 2.1952234506607056, Final Batch Loss: 1.1194002628326416\n",
      "Epoch 84, Loss: 2.179566740989685, Final Batch Loss: 1.1015148162841797\n",
      "Epoch 85, Loss: 2.1644465923309326, Final Batch Loss: 1.0547912120819092\n",
      "Epoch 86, Loss: 2.1463412046432495, Final Batch Loss: 1.1224533319473267\n",
      "Epoch 87, Loss: 2.0999561548233032, Final Batch Loss: 1.0276776552200317\n",
      "Epoch 88, Loss: 2.1154136061668396, Final Batch Loss: 0.9973128437995911\n",
      "Epoch 89, Loss: 2.0818156599998474, Final Batch Loss: 0.9901940226554871\n",
      "Epoch 90, Loss: 2.0739749670028687, Final Batch Loss: 1.0594574213027954\n",
      "Epoch 91, Loss: 1.9921659231185913, Final Batch Loss: 1.018125057220459\n",
      "Epoch 92, Loss: 2.005656361579895, Final Batch Loss: 0.9489610195159912\n",
      "Epoch 93, Loss: 1.9330705404281616, Final Batch Loss: 0.8898812532424927\n",
      "Epoch 94, Loss: 1.9172698855400085, Final Batch Loss: 0.9458838701248169\n",
      "Epoch 95, Loss: 1.9359793066978455, Final Batch Loss: 0.9566916227340698\n",
      "Epoch 96, Loss: 1.8932318687438965, Final Batch Loss: 0.9194344282150269\n",
      "Epoch 97, Loss: 1.9356203079223633, Final Batch Loss: 0.9788647890090942\n",
      "Epoch 98, Loss: 1.944193720817566, Final Batch Loss: 0.9841911196708679\n",
      "Epoch 99, Loss: 1.9504408836364746, Final Batch Loss: 1.0221232175827026\n",
      "Epoch 100, Loss: 1.8513363599777222, Final Batch Loss: 0.949347734451294\n",
      "Epoch 101, Loss: 1.8722313046455383, Final Batch Loss: 0.93770831823349\n",
      "Epoch 102, Loss: 1.9250520467758179, Final Batch Loss: 0.9597840905189514\n",
      "Epoch 103, Loss: 1.8040006160736084, Final Batch Loss: 0.8849360942840576\n",
      "Epoch 104, Loss: 1.8616686463356018, Final Batch Loss: 0.8912212252616882\n",
      "Epoch 105, Loss: 1.8246795535087585, Final Batch Loss: 0.9084547162055969\n",
      "Epoch 106, Loss: 1.8577331900596619, Final Batch Loss: 0.9468146562576294\n",
      "Epoch 107, Loss: 1.7842813730239868, Final Batch Loss: 0.9365629553794861\n",
      "Epoch 108, Loss: 1.8279733061790466, Final Batch Loss: 0.9266119599342346\n",
      "Epoch 109, Loss: 1.7433935403823853, Final Batch Loss: 0.8823687434196472\n",
      "Epoch 110, Loss: 1.73203045129776, Final Batch Loss: 0.8418982028961182\n",
      "Epoch 111, Loss: 1.7364166378974915, Final Batch Loss: 0.8694937229156494\n",
      "Epoch 112, Loss: 1.772301197052002, Final Batch Loss: 0.8481521010398865\n",
      "Epoch 113, Loss: 1.7401250004768372, Final Batch Loss: 0.917347252368927\n",
      "Epoch 114, Loss: 1.7752853035926819, Final Batch Loss: 0.8890048265457153\n",
      "Epoch 115, Loss: 1.7231491208076477, Final Batch Loss: 0.8593729138374329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, Loss: 1.7267680764198303, Final Batch Loss: 0.8837472200393677\n",
      "Epoch 117, Loss: 1.6997599601745605, Final Batch Loss: 0.8845036029815674\n",
      "Epoch 118, Loss: 1.7007607817649841, Final Batch Loss: 0.8236299157142639\n",
      "Epoch 119, Loss: 1.7552987337112427, Final Batch Loss: 0.8754104971885681\n",
      "Epoch 120, Loss: 1.7257468700408936, Final Batch Loss: 0.8550285696983337\n",
      "Epoch 121, Loss: 1.660182774066925, Final Batch Loss: 0.8348121047019958\n",
      "Epoch 122, Loss: 1.700490951538086, Final Batch Loss: 0.859285831451416\n",
      "Epoch 123, Loss: 1.7341635823249817, Final Batch Loss: 0.8709236979484558\n",
      "Epoch 124, Loss: 1.641904056072235, Final Batch Loss: 0.8366488814353943\n",
      "Epoch 125, Loss: 1.694413423538208, Final Batch Loss: 0.8769045472145081\n",
      "Epoch 126, Loss: 1.6317333579063416, Final Batch Loss: 0.8234081864356995\n",
      "Epoch 127, Loss: 1.6487082839012146, Final Batch Loss: 0.8902210593223572\n",
      "Epoch 128, Loss: 1.6593859791755676, Final Batch Loss: 0.763741135597229\n",
      "Epoch 129, Loss: 1.6277579069137573, Final Batch Loss: 0.8144447803497314\n",
      "Epoch 130, Loss: 1.7132123708724976, Final Batch Loss: 0.8477651476860046\n",
      "Epoch 131, Loss: 1.6475412845611572, Final Batch Loss: 0.8850021958351135\n",
      "Epoch 132, Loss: 1.730512022972107, Final Batch Loss: 0.8349835276603699\n",
      "Epoch 133, Loss: 1.6227015256881714, Final Batch Loss: 0.805513322353363\n",
      "Epoch 134, Loss: 1.6222782731056213, Final Batch Loss: 0.7464583516120911\n",
      "Epoch 135, Loss: 1.621606707572937, Final Batch Loss: 0.835465669631958\n",
      "Epoch 136, Loss: 1.6217013001441956, Final Batch Loss: 0.8403826951980591\n",
      "Epoch 137, Loss: 1.6323915719985962, Final Batch Loss: 0.8211937546730042\n",
      "Epoch 138, Loss: 1.5728272199630737, Final Batch Loss: 0.8076248168945312\n",
      "Epoch 139, Loss: 1.602502703666687, Final Batch Loss: 0.8080319762229919\n",
      "Epoch 140, Loss: 1.5534247756004333, Final Batch Loss: 0.8316067457199097\n",
      "Epoch 141, Loss: 1.508711338043213, Final Batch Loss: 0.7412818670272827\n",
      "Epoch 142, Loss: 1.5550258159637451, Final Batch Loss: 0.8004140853881836\n",
      "Epoch 143, Loss: 1.5704766511917114, Final Batch Loss: 0.8014727830886841\n",
      "Epoch 144, Loss: 1.5701833367347717, Final Batch Loss: 0.7401556968688965\n",
      "Epoch 145, Loss: 1.5786489844322205, Final Batch Loss: 0.8078134655952454\n",
      "Epoch 146, Loss: 1.4879306554794312, Final Batch Loss: 0.7920058965682983\n",
      "Epoch 147, Loss: 1.586397409439087, Final Batch Loss: 0.8061488270759583\n",
      "Epoch 148, Loss: 1.4887260794639587, Final Batch Loss: 0.7631586790084839\n",
      "Epoch 149, Loss: 1.475438117980957, Final Batch Loss: 0.7540086507797241\n",
      "Epoch 150, Loss: 1.5246030688285828, Final Batch Loss: 0.7420812249183655\n",
      "Epoch 151, Loss: 1.530828595161438, Final Batch Loss: 0.7730476260185242\n",
      "Epoch 152, Loss: 1.5794404745101929, Final Batch Loss: 0.7901889681816101\n",
      "Epoch 153, Loss: 1.496837854385376, Final Batch Loss: 0.7536271810531616\n",
      "Epoch 154, Loss: 1.4473992586135864, Final Batch Loss: 0.6908583641052246\n",
      "Epoch 155, Loss: 1.4696638584136963, Final Batch Loss: 0.7583203315734863\n",
      "Epoch 156, Loss: 1.4653058052062988, Final Batch Loss: 0.7647519111633301\n",
      "Epoch 157, Loss: 1.5623130202293396, Final Batch Loss: 0.7580339908599854\n",
      "Epoch 158, Loss: 1.4974703788757324, Final Batch Loss: 0.7145103216171265\n",
      "Epoch 159, Loss: 1.424134075641632, Final Batch Loss: 0.6992334127426147\n",
      "Epoch 160, Loss: 1.4057737588882446, Final Batch Loss: 0.660519540309906\n",
      "Epoch 161, Loss: 1.4491705894470215, Final Batch Loss: 0.7670227885246277\n",
      "Epoch 162, Loss: 1.3655037879943848, Final Batch Loss: 0.6705014109611511\n",
      "Epoch 163, Loss: 1.4465813636779785, Final Batch Loss: 0.7052950263023376\n",
      "Epoch 164, Loss: 1.4818987250328064, Final Batch Loss: 0.7360069751739502\n",
      "Epoch 165, Loss: 1.3905079364776611, Final Batch Loss: 0.6952789425849915\n",
      "Epoch 166, Loss: 1.3953499794006348, Final Batch Loss: 0.6866832375526428\n",
      "Epoch 167, Loss: 1.4651944637298584, Final Batch Loss: 0.7870343923568726\n",
      "Epoch 168, Loss: 1.415607511997223, Final Batch Loss: 0.7238281965255737\n",
      "Epoch 169, Loss: 1.4526076316833496, Final Batch Loss: 0.7469788193702698\n",
      "Epoch 170, Loss: 1.4781705141067505, Final Batch Loss: 0.714431881904602\n",
      "Epoch 171, Loss: 1.326139211654663, Final Batch Loss: 0.6890073418617249\n",
      "Epoch 172, Loss: 1.3865694403648376, Final Batch Loss: 0.643814742565155\n",
      "Epoch 173, Loss: 1.3982362151145935, Final Batch Loss: 0.7211366891860962\n",
      "Epoch 174, Loss: 1.3428995609283447, Final Batch Loss: 0.687879204750061\n",
      "Epoch 175, Loss: 1.4085506796836853, Final Batch Loss: 0.6831735372543335\n",
      "Epoch 176, Loss: 1.3924294710159302, Final Batch Loss: 0.6717267036437988\n",
      "Epoch 177, Loss: 1.4239331483840942, Final Batch Loss: 0.6730960011482239\n",
      "Epoch 178, Loss: 1.3744881749153137, Final Batch Loss: 0.7306810617446899\n",
      "Epoch 179, Loss: 1.3164172172546387, Final Batch Loss: 0.6585811972618103\n",
      "Epoch 180, Loss: 1.3416937589645386, Final Batch Loss: 0.6272948980331421\n",
      "Epoch 181, Loss: 1.3299750685691833, Final Batch Loss: 0.6754919290542603\n",
      "Epoch 182, Loss: 1.3908633589744568, Final Batch Loss: 0.7028985023498535\n",
      "Epoch 183, Loss: 1.295719027519226, Final Batch Loss: 0.6554878354072571\n",
      "Epoch 184, Loss: 1.4468314051628113, Final Batch Loss: 0.7286723256111145\n",
      "Epoch 185, Loss: 1.3832550644874573, Final Batch Loss: 0.6609511375427246\n",
      "Epoch 186, Loss: 1.3006752133369446, Final Batch Loss: 0.683746337890625\n",
      "Epoch 187, Loss: 1.3300561308860779, Final Batch Loss: 0.6836960911750793\n",
      "Epoch 188, Loss: 1.3119818568229675, Final Batch Loss: 0.6432258486747742\n",
      "Epoch 189, Loss: 1.3126131892204285, Final Batch Loss: 0.6531151533126831\n",
      "Epoch 190, Loss: 1.3436460494995117, Final Batch Loss: 0.6989241242408752\n",
      "Epoch 191, Loss: 1.2895013093948364, Final Batch Loss: 0.6603454351425171\n",
      "Epoch 192, Loss: 1.3119754195213318, Final Batch Loss: 0.6224877238273621\n",
      "Epoch 193, Loss: 1.3042840361595154, Final Batch Loss: 0.6595731973648071\n",
      "Epoch 194, Loss: 1.3968930840492249, Final Batch Loss: 0.7074570059776306\n",
      "Epoch 195, Loss: 1.3664212226867676, Final Batch Loss: 0.6551584005355835\n",
      "Epoch 196, Loss: 1.316621482372284, Final Batch Loss: 0.6532096266746521\n",
      "Epoch 197, Loss: 1.2476701736450195, Final Batch Loss: 0.6448429822921753\n",
      "Epoch 198, Loss: 1.289259910583496, Final Batch Loss: 0.6521778702735901\n",
      "Epoch 199, Loss: 1.2970288395881653, Final Batch Loss: 0.6528538465499878\n",
      "Epoch 200, Loss: 1.2629949450492859, Final Batch Loss: 0.6495029926300049\n",
      "Epoch 201, Loss: 1.270516812801361, Final Batch Loss: 0.6674478650093079\n",
      "Epoch 202, Loss: 1.3371924757957458, Final Batch Loss: 0.6430919170379639\n",
      "Epoch 203, Loss: 1.283769130706787, Final Batch Loss: 0.6689051985740662\n",
      "Epoch 204, Loss: 1.1705524921417236, Final Batch Loss: 0.5545433163642883\n",
      "Epoch 205, Loss: 1.257560908794403, Final Batch Loss: 0.6397501230239868\n",
      "Epoch 206, Loss: 1.2708665132522583, Final Batch Loss: 0.6738796234130859\n",
      "Epoch 207, Loss: 1.269895076751709, Final Batch Loss: 0.6361523866653442\n",
      "Epoch 208, Loss: 1.252386450767517, Final Batch Loss: 0.6185404062271118\n",
      "Epoch 209, Loss: 1.264476239681244, Final Batch Loss: 0.6301504373550415\n",
      "Epoch 210, Loss: 1.2282466292381287, Final Batch Loss: 0.6637223362922668\n",
      "Epoch 211, Loss: 1.2320602536201477, Final Batch Loss: 0.6183457970619202\n",
      "Epoch 212, Loss: 1.2415399551391602, Final Batch Loss: 0.630560040473938\n",
      "Epoch 213, Loss: 1.1585173606872559, Final Batch Loss: 0.5788364410400391\n",
      "Epoch 214, Loss: 1.2669775485992432, Final Batch Loss: 0.6264796257019043\n",
      "Epoch 215, Loss: 1.2149601578712463, Final Batch Loss: 0.6107087135314941\n",
      "Epoch 216, Loss: 1.2860937118530273, Final Batch Loss: 0.6765484809875488\n",
      "Epoch 217, Loss: 1.2362772822380066, Final Batch Loss: 0.6928226947784424\n",
      "Epoch 218, Loss: 1.2579070329666138, Final Batch Loss: 0.6472209095954895\n",
      "Epoch 219, Loss: 1.1810502409934998, Final Batch Loss: 0.6320409774780273\n",
      "Epoch 220, Loss: 1.1452282667160034, Final Batch Loss: 0.5432450771331787\n",
      "Epoch 221, Loss: 1.2126713991165161, Final Batch Loss: 0.6104231476783752\n",
      "Epoch 222, Loss: 1.199424147605896, Final Batch Loss: 0.5801772475242615\n",
      "Epoch 223, Loss: 1.2196093797683716, Final Batch Loss: 0.5330399870872498\n",
      "Epoch 224, Loss: 1.2124695181846619, Final Batch Loss: 0.5782048106193542\n",
      "Epoch 225, Loss: 1.1924989819526672, Final Batch Loss: 0.5748878717422485\n",
      "Epoch 226, Loss: 1.258065104484558, Final Batch Loss: 0.6101958155632019\n",
      "Epoch 227, Loss: 1.1800497174263, Final Batch Loss: 0.5603867769241333\n",
      "Epoch 228, Loss: 1.2773578763008118, Final Batch Loss: 0.6665242314338684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229, Loss: 1.0802149772644043, Final Batch Loss: 0.5463061928749084\n",
      "Epoch 230, Loss: 1.1767644882202148, Final Batch Loss: 0.5765054225921631\n",
      "Epoch 231, Loss: 1.184739887714386, Final Batch Loss: 0.5794706344604492\n",
      "Epoch 232, Loss: 1.1449797749519348, Final Batch Loss: 0.5859734416007996\n",
      "Epoch 233, Loss: 1.2442774176597595, Final Batch Loss: 0.6298320889472961\n",
      "Epoch 234, Loss: 1.1977728605270386, Final Batch Loss: 0.6553043723106384\n",
      "Epoch 235, Loss: 1.261401116847992, Final Batch Loss: 0.6346442103385925\n",
      "Epoch 236, Loss: 1.3025575280189514, Final Batch Loss: 0.6316947340965271\n",
      "Epoch 237, Loss: 1.2060908675193787, Final Batch Loss: 0.5721485614776611\n",
      "Epoch 238, Loss: 1.1996639966964722, Final Batch Loss: 0.6309945583343506\n",
      "Epoch 239, Loss: 1.2007994651794434, Final Batch Loss: 0.599519670009613\n",
      "Epoch 240, Loss: 1.192088782787323, Final Batch Loss: 0.5737956166267395\n",
      "Epoch 241, Loss: 1.1515591740608215, Final Batch Loss: 0.5313878059387207\n",
      "Epoch 242, Loss: 1.148260772228241, Final Batch Loss: 0.6213505864143372\n",
      "Epoch 243, Loss: 1.171068549156189, Final Batch Loss: 0.583075761795044\n",
      "Epoch 244, Loss: 1.133458435535431, Final Batch Loss: 0.5643086433410645\n",
      "Epoch 245, Loss: 1.1458825469017029, Final Batch Loss: 0.577582061290741\n",
      "Epoch 246, Loss: 1.1524564027786255, Final Batch Loss: 0.5646587014198303\n",
      "Epoch 247, Loss: 1.1978399753570557, Final Batch Loss: 0.602063000202179\n",
      "Epoch 248, Loss: 1.178036093711853, Final Batch Loss: 0.6496603488922119\n",
      "Epoch 249, Loss: 1.1789115071296692, Final Batch Loss: 0.5232377052307129\n",
      "Epoch 250, Loss: 1.138824999332428, Final Batch Loss: 0.5342915654182434\n",
      "Epoch 251, Loss: 1.1617270112037659, Final Batch Loss: 0.5395500063896179\n",
      "Epoch 252, Loss: 1.1100281476974487, Final Batch Loss: 0.5588675737380981\n",
      "Epoch 253, Loss: 1.1468695402145386, Final Batch Loss: 0.5252802968025208\n",
      "Epoch 254, Loss: 1.199720025062561, Final Batch Loss: 0.6226750612258911\n",
      "Epoch 255, Loss: 1.1765151619911194, Final Batch Loss: 0.6017446517944336\n",
      "Epoch 256, Loss: 1.1256733536720276, Final Batch Loss: 0.5986943244934082\n",
      "Epoch 257, Loss: 1.1083242893218994, Final Batch Loss: 0.5420101881027222\n",
      "Epoch 258, Loss: 1.1631162762641907, Final Batch Loss: 0.625480055809021\n",
      "Epoch 259, Loss: 1.1147319674491882, Final Batch Loss: 0.5645231008529663\n",
      "Epoch 260, Loss: 1.1045247912406921, Final Batch Loss: 0.5345349907875061\n",
      "Epoch 261, Loss: 1.1578505039215088, Final Batch Loss: 0.5445789694786072\n",
      "Epoch 262, Loss: 1.0793227553367615, Final Batch Loss: 0.5775265693664551\n",
      "Epoch 263, Loss: 1.1634259819984436, Final Batch Loss: 0.6246727705001831\n",
      "Epoch 264, Loss: 1.135985016822815, Final Batch Loss: 0.6233587265014648\n",
      "Epoch 265, Loss: 1.130548119544983, Final Batch Loss: 0.5494725108146667\n",
      "Epoch 266, Loss: 1.1713347434997559, Final Batch Loss: 0.6238653659820557\n",
      "Epoch 267, Loss: 1.1256730556488037, Final Batch Loss: 0.5245273113250732\n",
      "Epoch 268, Loss: 1.0172872245311737, Final Batch Loss: 0.566435694694519\n",
      "Epoch 269, Loss: 1.0985414385795593, Final Batch Loss: 0.5453193783760071\n",
      "Epoch 270, Loss: 1.1011818647384644, Final Batch Loss: 0.6020090579986572\n",
      "Epoch 271, Loss: 1.047160565853119, Final Batch Loss: 0.5082327723503113\n",
      "Epoch 272, Loss: 1.1015244722366333, Final Batch Loss: 0.5677637457847595\n",
      "Epoch 273, Loss: 1.1230624318122864, Final Batch Loss: 0.5333325862884521\n",
      "Epoch 274, Loss: 1.0638350248336792, Final Batch Loss: 0.564997673034668\n",
      "Epoch 275, Loss: 1.1300223469734192, Final Batch Loss: 0.5209871530532837\n",
      "Epoch 276, Loss: 1.158538281917572, Final Batch Loss: 0.6088063716888428\n",
      "Epoch 277, Loss: 1.1441991925239563, Final Batch Loss: 0.6649630069732666\n",
      "Epoch 278, Loss: 1.0829269289970398, Final Batch Loss: 0.5820883512496948\n",
      "Epoch 279, Loss: 1.0438790917396545, Final Batch Loss: 0.5103716850280762\n",
      "Epoch 280, Loss: 1.0664665699005127, Final Batch Loss: 0.532785952091217\n",
      "Epoch 281, Loss: 1.048259198665619, Final Batch Loss: 0.5105909109115601\n",
      "Epoch 282, Loss: 1.1054036617279053, Final Batch Loss: 0.5575471520423889\n",
      "Epoch 283, Loss: 1.1063525676727295, Final Batch Loss: 0.5479987263679504\n",
      "Epoch 284, Loss: 1.0366690456867218, Final Batch Loss: 0.554836630821228\n",
      "Epoch 285, Loss: 1.0403867363929749, Final Batch Loss: 0.5034639835357666\n",
      "Epoch 286, Loss: 1.0574907064437866, Final Batch Loss: 0.5151253938674927\n",
      "Epoch 287, Loss: 1.0709989070892334, Final Batch Loss: 0.5110207796096802\n",
      "Epoch 288, Loss: 1.0555140972137451, Final Batch Loss: 0.5325795412063599\n",
      "Epoch 289, Loss: 1.1070581078529358, Final Batch Loss: 0.5617605447769165\n",
      "Epoch 290, Loss: 1.052461326122284, Final Batch Loss: 0.5501672029495239\n",
      "Epoch 291, Loss: 0.9903624951839447, Final Batch Loss: 0.515155553817749\n",
      "Epoch 292, Loss: 1.0475124716758728, Final Batch Loss: 0.5223915576934814\n",
      "Epoch 293, Loss: 1.0846456289291382, Final Batch Loss: 0.5548263192176819\n",
      "Epoch 294, Loss: 1.026475876569748, Final Batch Loss: 0.5397493243217468\n",
      "Epoch 295, Loss: 1.023410975933075, Final Batch Loss: 0.512982964515686\n",
      "Epoch 296, Loss: 0.989210844039917, Final Batch Loss: 0.4439430236816406\n",
      "Epoch 297, Loss: 0.9634937644004822, Final Batch Loss: 0.46736767888069153\n",
      "Epoch 298, Loss: 0.9996770322322845, Final Batch Loss: 0.5177473425865173\n",
      "Epoch 299, Loss: 1.0135290324687958, Final Batch Loss: 0.46829232573509216\n",
      "Epoch 300, Loss: 1.015140861272812, Final Batch Loss: 0.5236804485321045\n",
      "Epoch 301, Loss: 1.1448241472244263, Final Batch Loss: 0.6054957509040833\n",
      "Epoch 302, Loss: 0.9980443120002747, Final Batch Loss: 0.49664467573165894\n",
      "Epoch 303, Loss: 1.0139229893684387, Final Batch Loss: 0.5400875806808472\n",
      "Epoch 304, Loss: 0.9940418004989624, Final Batch Loss: 0.4845576286315918\n",
      "Epoch 305, Loss: 1.0333772897720337, Final Batch Loss: 0.5096724629402161\n",
      "Epoch 306, Loss: 1.0159981846809387, Final Batch Loss: 0.5530438423156738\n",
      "Epoch 307, Loss: 0.9788342118263245, Final Batch Loss: 0.4712146520614624\n",
      "Epoch 308, Loss: 1.0708236694335938, Final Batch Loss: 0.5474706292152405\n",
      "Epoch 309, Loss: 1.0256491899490356, Final Batch Loss: 0.5238507986068726\n",
      "Epoch 310, Loss: 1.0269564986228943, Final Batch Loss: 0.5050796270370483\n",
      "Epoch 311, Loss: 1.0242252945899963, Final Batch Loss: 0.5061801075935364\n",
      "Epoch 312, Loss: 0.9941408336162567, Final Batch Loss: 0.4991843104362488\n",
      "Epoch 313, Loss: 0.9543525874614716, Final Batch Loss: 0.4622768759727478\n",
      "Epoch 314, Loss: 0.9574591815471649, Final Batch Loss: 0.4844096302986145\n",
      "Epoch 315, Loss: 1.0620979070663452, Final Batch Loss: 0.518494725227356\n",
      "Epoch 316, Loss: 0.9361793100833893, Final Batch Loss: 0.45718327164649963\n",
      "Epoch 317, Loss: 0.9609057605266571, Final Batch Loss: 0.4847396910190582\n",
      "Epoch 318, Loss: 0.9684264659881592, Final Batch Loss: 0.4595641493797302\n",
      "Epoch 319, Loss: 0.9659600257873535, Final Batch Loss: 0.5264480113983154\n",
      "Epoch 320, Loss: 0.9625884294509888, Final Batch Loss: 0.4659966230392456\n",
      "Epoch 321, Loss: 0.996428906917572, Final Batch Loss: 0.5255671739578247\n",
      "Epoch 322, Loss: 0.9295525550842285, Final Batch Loss: 0.4625902473926544\n",
      "Epoch 323, Loss: 0.9674293398857117, Final Batch Loss: 0.4392979145050049\n",
      "Epoch 324, Loss: 0.962801605463028, Final Batch Loss: 0.5066482424736023\n",
      "Epoch 325, Loss: 1.0748738050460815, Final Batch Loss: 0.5648083090782166\n",
      "Epoch 326, Loss: 0.9782634377479553, Final Batch Loss: 0.5066683292388916\n",
      "Epoch 327, Loss: 0.9892581403255463, Final Batch Loss: 0.46210864186286926\n",
      "Epoch 328, Loss: 0.9885241091251373, Final Batch Loss: 0.4960553050041199\n",
      "Epoch 329, Loss: 0.9139639139175415, Final Batch Loss: 0.46829599142074585\n",
      "Epoch 330, Loss: 0.9333061277866364, Final Batch Loss: 0.4828280210494995\n",
      "Epoch 331, Loss: 0.9532772600650787, Final Batch Loss: 0.4471707046031952\n",
      "Epoch 332, Loss: 0.9606684148311615, Final Batch Loss: 0.4782847762107849\n",
      "Epoch 333, Loss: 1.0053159296512604, Final Batch Loss: 0.4525916278362274\n",
      "Epoch 334, Loss: 0.9430638551712036, Final Batch Loss: 0.4160330295562744\n",
      "Epoch 335, Loss: 1.0220086872577667, Final Batch Loss: 0.5317703485488892\n",
      "Epoch 336, Loss: 0.980527400970459, Final Batch Loss: 0.47549372911453247\n",
      "Epoch 337, Loss: 0.9328494369983673, Final Batch Loss: 0.4367707669734955\n",
      "Epoch 338, Loss: 0.9722223877906799, Final Batch Loss: 0.5044345855712891\n",
      "Epoch 339, Loss: 0.9079912602901459, Final Batch Loss: 0.4178760051727295\n",
      "Epoch 340, Loss: 0.9636849164962769, Final Batch Loss: 0.4761458933353424\n",
      "Epoch 341, Loss: 0.9459672272205353, Final Batch Loss: 0.46893468499183655\n",
      "Epoch 342, Loss: 0.8925961852073669, Final Batch Loss: 0.4284306764602661\n",
      "Epoch 343, Loss: 0.95929816365242, Final Batch Loss: 0.4421745836734772\n",
      "Epoch 344, Loss: 1.0247196853160858, Final Batch Loss: 0.4877447187900543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345, Loss: 0.9472845494747162, Final Batch Loss: 0.4523678719997406\n",
      "Epoch 346, Loss: 0.9488269686698914, Final Batch Loss: 0.4019750952720642\n",
      "Epoch 347, Loss: 0.9326653480529785, Final Batch Loss: 0.4767075181007385\n",
      "Epoch 348, Loss: 0.9024737775325775, Final Batch Loss: 0.45132988691329956\n",
      "Epoch 349, Loss: 0.9439535737037659, Final Batch Loss: 0.4558892250061035\n",
      "Epoch 350, Loss: 0.8831825852394104, Final Batch Loss: 0.4448608458042145\n",
      "Epoch 351, Loss: 0.9624952077865601, Final Batch Loss: 0.4695427417755127\n",
      "Epoch 352, Loss: 0.9785950779914856, Final Batch Loss: 0.46334338188171387\n",
      "Epoch 353, Loss: 1.0059814751148224, Final Batch Loss: 0.5302107334136963\n",
      "Epoch 354, Loss: 0.9335408508777618, Final Batch Loss: 0.48099878430366516\n",
      "Epoch 355, Loss: 0.9427607953548431, Final Batch Loss: 0.483333945274353\n",
      "Epoch 356, Loss: 0.9451277852058411, Final Batch Loss: 0.48197221755981445\n",
      "Epoch 357, Loss: 0.9107107222080231, Final Batch Loss: 0.4175978899002075\n",
      "Epoch 358, Loss: 0.9195401668548584, Final Batch Loss: 0.4549782872200012\n",
      "Epoch 359, Loss: 0.8808937966823578, Final Batch Loss: 0.44750967621803284\n",
      "Epoch 360, Loss: 0.9470556676387787, Final Batch Loss: 0.4852001368999481\n",
      "Epoch 361, Loss: 0.8994399011135101, Final Batch Loss: 0.4858555495738983\n",
      "Epoch 362, Loss: 0.9357323348522186, Final Batch Loss: 0.47055718302726746\n",
      "Epoch 363, Loss: 0.9292681217193604, Final Batch Loss: 0.46205654740333557\n",
      "Epoch 364, Loss: 0.863496869802475, Final Batch Loss: 0.41115471720695496\n",
      "Epoch 365, Loss: 0.9233455061912537, Final Batch Loss: 0.4125218391418457\n",
      "Epoch 366, Loss: 0.8743085861206055, Final Batch Loss: 0.43250584602355957\n",
      "Epoch 367, Loss: 0.938936173915863, Final Batch Loss: 0.46424171328544617\n",
      "Epoch 368, Loss: 0.9017319977283478, Final Batch Loss: 0.42507925629615784\n",
      "Epoch 369, Loss: 0.9808398187160492, Final Batch Loss: 0.47863486409187317\n",
      "Epoch 370, Loss: 0.9626058638095856, Final Batch Loss: 0.4712182283401489\n",
      "Epoch 371, Loss: 0.8951260149478912, Final Batch Loss: 0.46287792921066284\n",
      "Epoch 372, Loss: 0.912169337272644, Final Batch Loss: 0.4213632047176361\n",
      "Epoch 373, Loss: 0.9120860397815704, Final Batch Loss: 0.42856845259666443\n",
      "Epoch 374, Loss: 0.9272766709327698, Final Batch Loss: 0.5069966316223145\n",
      "Epoch 375, Loss: 0.9491751790046692, Final Batch Loss: 0.5040885210037231\n",
      "Epoch 376, Loss: 0.8834545612335205, Final Batch Loss: 0.4316791594028473\n",
      "Epoch 377, Loss: 0.9147107303142548, Final Batch Loss: 0.40799257159233093\n",
      "Epoch 378, Loss: 0.9728871583938599, Final Batch Loss: 0.5278227925300598\n",
      "Epoch 379, Loss: 0.9177598655223846, Final Batch Loss: 0.4364326596260071\n",
      "Epoch 380, Loss: 0.9412356317043304, Final Batch Loss: 0.493022084236145\n",
      "Epoch 381, Loss: 0.8762341439723969, Final Batch Loss: 0.4663826823234558\n",
      "Epoch 382, Loss: 0.8932468891143799, Final Batch Loss: 0.4298560917377472\n",
      "Epoch 383, Loss: 0.9254928827285767, Final Batch Loss: 0.4525548219680786\n",
      "Epoch 384, Loss: 0.9380077719688416, Final Batch Loss: 0.5099612474441528\n",
      "Epoch 385, Loss: 0.9130550920963287, Final Batch Loss: 0.43715885281562805\n",
      "Epoch 386, Loss: 0.966177374124527, Final Batch Loss: 0.4775885045528412\n",
      "Epoch 387, Loss: 0.8759926855564117, Final Batch Loss: 0.39699381589889526\n",
      "Epoch 388, Loss: 0.9185792207717896, Final Batch Loss: 0.4683281481266022\n",
      "Epoch 389, Loss: 0.921847939491272, Final Batch Loss: 0.4587794244289398\n",
      "Epoch 390, Loss: 0.9488728642463684, Final Batch Loss: 0.4602905213832855\n",
      "Epoch 391, Loss: 0.908843606710434, Final Batch Loss: 0.5023893117904663\n",
      "Epoch 392, Loss: 0.8616728484630585, Final Batch Loss: 0.43794262409210205\n",
      "Epoch 393, Loss: 0.9446559846401215, Final Batch Loss: 0.4417501389980316\n",
      "Epoch 394, Loss: 0.8746341168880463, Final Batch Loss: 0.44800126552581787\n",
      "Epoch 395, Loss: 0.9261647462844849, Final Batch Loss: 0.42972275614738464\n",
      "Epoch 396, Loss: 0.9163669049739838, Final Batch Loss: 0.45440346002578735\n",
      "Epoch 397, Loss: 0.9051525592803955, Final Batch Loss: 0.4147346019744873\n",
      "Epoch 398, Loss: 0.8542983531951904, Final Batch Loss: 0.3945779502391815\n",
      "Epoch 399, Loss: 0.9492620229721069, Final Batch Loss: 0.5246196389198303\n",
      "Epoch 400, Loss: 0.877983033657074, Final Batch Loss: 0.42214199900627136\n",
      "Epoch 401, Loss: 0.9713093638420105, Final Batch Loss: 0.4698413610458374\n",
      "Epoch 402, Loss: 0.8835298717021942, Final Batch Loss: 0.4830072224140167\n",
      "Epoch 403, Loss: 0.8891235589981079, Final Batch Loss: 0.42685627937316895\n",
      "Epoch 404, Loss: 0.8855441510677338, Final Batch Loss: 0.4540538489818573\n",
      "Epoch 405, Loss: 0.9547672867774963, Final Batch Loss: 0.48574063181877136\n",
      "Epoch 406, Loss: 0.8554169535636902, Final Batch Loss: 0.39853763580322266\n",
      "Epoch 407, Loss: 0.8745543956756592, Final Batch Loss: 0.3770446181297302\n",
      "Epoch 408, Loss: 0.9404011368751526, Final Batch Loss: 0.4776895046234131\n",
      "Epoch 409, Loss: 0.9508334100246429, Final Batch Loss: 0.5094293355941772\n",
      "Epoch 410, Loss: 0.9250454902648926, Final Batch Loss: 0.42408066987991333\n",
      "Epoch 411, Loss: 0.835936963558197, Final Batch Loss: 0.3827075660228729\n",
      "Epoch 412, Loss: 0.8869202435016632, Final Batch Loss: 0.4162198603153229\n",
      "Epoch 413, Loss: 0.8763667643070221, Final Batch Loss: 0.42397913336753845\n",
      "Epoch 414, Loss: 0.9061534702777863, Final Batch Loss: 0.3833633363246918\n",
      "Epoch 415, Loss: 0.8861969113349915, Final Batch Loss: 0.41867709159851074\n",
      "Epoch 416, Loss: 0.939991295337677, Final Batch Loss: 0.48764467239379883\n",
      "Epoch 417, Loss: 0.900648832321167, Final Batch Loss: 0.49418801069259644\n",
      "Epoch 418, Loss: 0.9147448241710663, Final Batch Loss: 0.478593111038208\n",
      "Epoch 419, Loss: 0.8709674775600433, Final Batch Loss: 0.40715402364730835\n",
      "Epoch 420, Loss: 0.8934653401374817, Final Batch Loss: 0.3892253637313843\n",
      "Epoch 421, Loss: 0.89051952958107, Final Batch Loss: 0.43743008375167847\n",
      "Epoch 422, Loss: 0.8589534163475037, Final Batch Loss: 0.39431577920913696\n",
      "Epoch 423, Loss: 0.862703412771225, Final Batch Loss: 0.439387708902359\n",
      "Epoch 424, Loss: 0.9126644730567932, Final Batch Loss: 0.44114142656326294\n",
      "Epoch 425, Loss: 0.8952350914478302, Final Batch Loss: 0.4626915454864502\n",
      "Epoch 426, Loss: 0.8270388245582581, Final Batch Loss: 0.43480637669563293\n",
      "Epoch 427, Loss: 0.8854177296161652, Final Batch Loss: 0.4390409290790558\n",
      "Epoch 428, Loss: 0.9062798917293549, Final Batch Loss: 0.45155060291290283\n",
      "Epoch 429, Loss: 0.8873507082462311, Final Batch Loss: 0.4118836224079132\n",
      "Epoch 430, Loss: 0.8818991780281067, Final Batch Loss: 0.4264598488807678\n",
      "Epoch 431, Loss: 0.8770826458930969, Final Batch Loss: 0.46257856488227844\n",
      "Epoch 432, Loss: 0.8490428924560547, Final Batch Loss: 0.4363420009613037\n",
      "Epoch 433, Loss: 0.8104161620140076, Final Batch Loss: 0.3813740611076355\n",
      "Epoch 434, Loss: 0.8189722895622253, Final Batch Loss: 0.42843306064605713\n",
      "Epoch 435, Loss: 0.8762799799442291, Final Batch Loss: 0.46424490213394165\n",
      "Epoch 436, Loss: 0.8057393431663513, Final Batch Loss: 0.38566747307777405\n",
      "Epoch 437, Loss: 0.8574223220348358, Final Batch Loss: 0.4483594298362732\n",
      "Epoch 438, Loss: 0.8445678055286407, Final Batch Loss: 0.37494897842407227\n",
      "Epoch 439, Loss: 0.9391187429428101, Final Batch Loss: 0.472443550825119\n",
      "Epoch 440, Loss: 0.8342682421207428, Final Batch Loss: 0.38804352283477783\n",
      "Epoch 441, Loss: 0.8509469032287598, Final Batch Loss: 0.4522986114025116\n",
      "Epoch 442, Loss: 0.8667562007904053, Final Batch Loss: 0.45017847418785095\n",
      "Epoch 443, Loss: 0.857894241809845, Final Batch Loss: 0.48127099871635437\n",
      "Epoch 444, Loss: 0.8553801476955414, Final Batch Loss: 0.37126535177230835\n",
      "Epoch 445, Loss: 0.8242661654949188, Final Batch Loss: 0.46131619811058044\n",
      "Epoch 446, Loss: 0.9480625987052917, Final Batch Loss: 0.5702563524246216\n",
      "Epoch 447, Loss: 0.868272215127945, Final Batch Loss: 0.4380461573600769\n",
      "Epoch 448, Loss: 0.839706540107727, Final Batch Loss: 0.4250146448612213\n",
      "Epoch 449, Loss: 0.8591025769710541, Final Batch Loss: 0.4247669577598572\n",
      "Epoch 450, Loss: 0.8720313310623169, Final Batch Loss: 0.41680800914764404\n",
      "Epoch 451, Loss: 0.8723347187042236, Final Batch Loss: 0.44055038690567017\n",
      "Epoch 452, Loss: 0.7909692227840424, Final Batch Loss: 0.4388577342033386\n",
      "Epoch 453, Loss: 0.8823956251144409, Final Batch Loss: 0.4155476987361908\n",
      "Epoch 454, Loss: 0.837596982717514, Final Batch Loss: 0.42194467782974243\n",
      "Epoch 455, Loss: 0.859999418258667, Final Batch Loss: 0.48324841260910034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456, Loss: 0.8712477087974548, Final Batch Loss: 0.3694760203361511\n",
      "Epoch 457, Loss: 0.8873794078826904, Final Batch Loss: 0.4510103762149811\n",
      "Epoch 458, Loss: 0.8729475736618042, Final Batch Loss: 0.47833919525146484\n",
      "Epoch 459, Loss: 0.8523648083209991, Final Batch Loss: 0.4594773054122925\n",
      "Epoch 460, Loss: 0.8919640779495239, Final Batch Loss: 0.40815186500549316\n",
      "Epoch 461, Loss: 0.8211559355258942, Final Batch Loss: 0.4459083080291748\n",
      "Epoch 462, Loss: 0.8648236393928528, Final Batch Loss: 0.4072718918323517\n",
      "Epoch 463, Loss: 0.9499256014823914, Final Batch Loss: 0.5183457136154175\n",
      "Epoch 464, Loss: 0.8210845291614532, Final Batch Loss: 0.42006590962409973\n",
      "Epoch 465, Loss: 0.8412117660045624, Final Batch Loss: 0.3903234302997589\n",
      "Epoch 466, Loss: 0.8532694876194, Final Batch Loss: 0.37877193093299866\n",
      "Epoch 467, Loss: 0.8174645304679871, Final Batch Loss: 0.42556747794151306\n",
      "Epoch 468, Loss: 0.8040016293525696, Final Batch Loss: 0.4000927805900574\n",
      "Epoch 469, Loss: 0.8697860538959503, Final Batch Loss: 0.4124405086040497\n",
      "Epoch 470, Loss: 0.8092494606971741, Final Batch Loss: 0.4469582736492157\n",
      "Epoch 471, Loss: 0.8488239645957947, Final Batch Loss: 0.41370394825935364\n",
      "Epoch 472, Loss: 0.9318138062953949, Final Batch Loss: 0.4195381700992584\n",
      "Epoch 473, Loss: 0.8320994079113007, Final Batch Loss: 0.3705795705318451\n",
      "Epoch 474, Loss: 0.7810465395450592, Final Batch Loss: 0.3951855003833771\n",
      "Epoch 475, Loss: 0.8271386027336121, Final Batch Loss: 0.4132348597049713\n",
      "Epoch 476, Loss: 0.8325026631355286, Final Batch Loss: 0.41649556159973145\n",
      "Epoch 477, Loss: 0.8924971222877502, Final Batch Loss: 0.4429403841495514\n",
      "Epoch 478, Loss: 0.7997582256793976, Final Batch Loss: 0.38158881664276123\n",
      "Epoch 479, Loss: 0.8136484622955322, Final Batch Loss: 0.4192119836807251\n",
      "Epoch 480, Loss: 0.8643163740634918, Final Batch Loss: 0.46166539192199707\n",
      "Epoch 481, Loss: 0.7989305853843689, Final Batch Loss: 0.3976749777793884\n",
      "Epoch 482, Loss: 0.8081536591053009, Final Batch Loss: 0.4360724091529846\n",
      "Epoch 483, Loss: 0.8063597083091736, Final Batch Loss: 0.42057129740715027\n",
      "Epoch 484, Loss: 0.8557556569576263, Final Batch Loss: 0.4732092320919037\n",
      "Epoch 485, Loss: 0.809248149394989, Final Batch Loss: 0.40864166617393494\n",
      "Epoch 486, Loss: 0.8178190886974335, Final Batch Loss: 0.39330390095710754\n",
      "Epoch 487, Loss: 0.8115182518959045, Final Batch Loss: 0.4296611547470093\n",
      "Epoch 488, Loss: 0.8263208866119385, Final Batch Loss: 0.4484506845474243\n",
      "Epoch 489, Loss: 0.8904489278793335, Final Batch Loss: 0.40919768810272217\n",
      "Epoch 490, Loss: 0.8097253441810608, Final Batch Loss: 0.4312252104282379\n",
      "Epoch 491, Loss: 0.9102440774440765, Final Batch Loss: 0.4408949911594391\n",
      "Epoch 492, Loss: 0.8018330037593842, Final Batch Loss: 0.4326518177986145\n",
      "Epoch 493, Loss: 0.8270121812820435, Final Batch Loss: 0.380859911441803\n",
      "Epoch 494, Loss: 0.7580297589302063, Final Batch Loss: 0.38790974020957947\n",
      "Epoch 495, Loss: 0.7927567064762115, Final Batch Loss: 0.3764949142932892\n",
      "Epoch 496, Loss: 0.775725781917572, Final Batch Loss: 0.3960743248462677\n",
      "Epoch 497, Loss: 0.8160305917263031, Final Batch Loss: 0.40089651942253113\n",
      "Epoch 498, Loss: 0.9374699890613556, Final Batch Loss: 0.40278688073158264\n",
      "Epoch 499, Loss: 0.8334177136421204, Final Batch Loss: 0.3852524757385254\n",
      "Epoch 500, Loss: 0.8430167138576508, Final Batch Loss: 0.4520660936832428\n",
      "Epoch 501, Loss: 0.8750393092632294, Final Batch Loss: 0.4582599699497223\n",
      "Epoch 502, Loss: 0.8071107864379883, Final Batch Loss: 0.3946419954299927\n",
      "Epoch 503, Loss: 0.8129213154315948, Final Batch Loss: 0.40536439418792725\n",
      "Epoch 504, Loss: 0.8002620935440063, Final Batch Loss: 0.400724858045578\n",
      "Epoch 505, Loss: 0.8587177395820618, Final Batch Loss: 0.4586963951587677\n",
      "Epoch 506, Loss: 0.8081181049346924, Final Batch Loss: 0.3964749872684479\n",
      "Epoch 507, Loss: 0.8542045950889587, Final Batch Loss: 0.42228859663009644\n",
      "Epoch 508, Loss: 0.8090609014034271, Final Batch Loss: 0.4043080508708954\n",
      "Epoch 509, Loss: 0.8061372637748718, Final Batch Loss: 0.40175461769104004\n",
      "Epoch 510, Loss: 0.773745059967041, Final Batch Loss: 0.4043304920196533\n",
      "Epoch 511, Loss: 0.7687529623508453, Final Batch Loss: 0.3428705036640167\n",
      "Epoch 512, Loss: 0.7741550207138062, Final Batch Loss: 0.31596487760543823\n",
      "Epoch 513, Loss: 0.821075439453125, Final Batch Loss: 0.40975120663642883\n",
      "Epoch 514, Loss: 0.7651664614677429, Final Batch Loss: 0.340996116399765\n",
      "Epoch 515, Loss: 0.8080137372016907, Final Batch Loss: 0.40362414717674255\n",
      "Epoch 516, Loss: 0.7413153946399689, Final Batch Loss: 0.35331183671951294\n",
      "Epoch 517, Loss: 0.8004243075847626, Final Batch Loss: 0.38199687004089355\n",
      "Epoch 518, Loss: 0.798915445804596, Final Batch Loss: 0.40491271018981934\n",
      "Epoch 519, Loss: 0.804651528596878, Final Batch Loss: 0.4213378429412842\n",
      "Epoch 520, Loss: 0.8287229537963867, Final Batch Loss: 0.4172699451446533\n",
      "Epoch 521, Loss: 0.8074487149715424, Final Batch Loss: 0.4311034381389618\n",
      "Epoch 522, Loss: 0.791849821805954, Final Batch Loss: 0.45461276173591614\n",
      "Epoch 523, Loss: 0.7817142605781555, Final Batch Loss: 0.4192294478416443\n",
      "Epoch 524, Loss: 0.7645179033279419, Final Batch Loss: 0.3750050961971283\n",
      "Epoch 525, Loss: 0.7848394215106964, Final Batch Loss: 0.3749906122684479\n",
      "Epoch 526, Loss: 0.8642779886722565, Final Batch Loss: 0.467519611120224\n",
      "Epoch 527, Loss: 0.7819848656654358, Final Batch Loss: 0.3919387459754944\n",
      "Epoch 528, Loss: 0.811788022518158, Final Batch Loss: 0.341879278421402\n",
      "Epoch 529, Loss: 0.8269877135753632, Final Batch Loss: 0.389871209859848\n",
      "Epoch 530, Loss: 0.8564050793647766, Final Batch Loss: 0.4713941514492035\n",
      "Epoch 531, Loss: 0.8478834927082062, Final Batch Loss: 0.3979427218437195\n",
      "Epoch 532, Loss: 0.8173640370368958, Final Batch Loss: 0.43153393268585205\n",
      "Epoch 533, Loss: 0.7962648272514343, Final Batch Loss: 0.3876827657222748\n",
      "Epoch 534, Loss: 0.7626700699329376, Final Batch Loss: 0.3710780739784241\n",
      "Epoch 535, Loss: 0.758966475725174, Final Batch Loss: 0.3805777132511139\n",
      "Epoch 536, Loss: 0.754713237285614, Final Batch Loss: 0.3890477120876312\n",
      "Epoch 537, Loss: 0.8241053521633148, Final Batch Loss: 0.42889752984046936\n",
      "Epoch 538, Loss: 0.77081298828125, Final Batch Loss: 0.45331165194511414\n",
      "Epoch 539, Loss: 0.7651002407073975, Final Batch Loss: 0.39726686477661133\n",
      "Epoch 540, Loss: 0.7958201766014099, Final Batch Loss: 0.3584012985229492\n",
      "Epoch 541, Loss: 0.7452724575996399, Final Batch Loss: 0.3832748234272003\n",
      "Epoch 542, Loss: 0.7822022140026093, Final Batch Loss: 0.3806281089782715\n",
      "Epoch 543, Loss: 0.8030710518360138, Final Batch Loss: 0.34734413027763367\n",
      "Epoch 544, Loss: 0.7365078926086426, Final Batch Loss: 0.3517366647720337\n",
      "Epoch 545, Loss: 0.884958028793335, Final Batch Loss: 0.45087218284606934\n",
      "Epoch 546, Loss: 0.8033714294433594, Final Batch Loss: 0.3992004096508026\n",
      "Epoch 547, Loss: 0.7683035433292389, Final Batch Loss: 0.40152788162231445\n",
      "Epoch 548, Loss: 0.7990324199199677, Final Batch Loss: 0.3602279722690582\n",
      "Epoch 549, Loss: 0.7697672843933105, Final Batch Loss: 0.39708825945854187\n",
      "Epoch 550, Loss: 0.8218076825141907, Final Batch Loss: 0.40987735986709595\n",
      "Epoch 551, Loss: 0.7976210713386536, Final Batch Loss: 0.40060704946517944\n",
      "Epoch 552, Loss: 0.7661934196949005, Final Batch Loss: 0.38727325201034546\n",
      "Epoch 553, Loss: 0.8064172565937042, Final Batch Loss: 0.4280947148799896\n",
      "Epoch 554, Loss: 0.7929786443710327, Final Batch Loss: 0.413003146648407\n",
      "Epoch 555, Loss: 0.7580875754356384, Final Batch Loss: 0.38204503059387207\n",
      "Epoch 556, Loss: 0.8256947994232178, Final Batch Loss: 0.3364672064781189\n",
      "Epoch 557, Loss: 0.7385606169700623, Final Batch Loss: 0.3513485789299011\n",
      "Epoch 558, Loss: 0.8027157783508301, Final Batch Loss: 0.40364494919776917\n",
      "Epoch 559, Loss: 0.7619082033634186, Final Batch Loss: 0.34389907121658325\n",
      "Epoch 560, Loss: 0.7636082172393799, Final Batch Loss: 0.38591063022613525\n",
      "Epoch 561, Loss: 0.8393968641757965, Final Batch Loss: 0.4193580150604248\n",
      "Epoch 562, Loss: 0.8626421689987183, Final Batch Loss: 0.4237464964389801\n",
      "Epoch 563, Loss: 0.7800962030887604, Final Batch Loss: 0.40331223607063293\n",
      "Epoch 564, Loss: 0.908996969461441, Final Batch Loss: 0.48310473561286926\n",
      "Epoch 565, Loss: 0.7847906947135925, Final Batch Loss: 0.3882942199707031\n",
      "Epoch 566, Loss: 0.7981698215007782, Final Batch Loss: 0.40485379099845886\n",
      "Epoch 567, Loss: 0.7693594098091125, Final Batch Loss: 0.406842976808548\n",
      "Epoch 568, Loss: 0.8965746164321899, Final Batch Loss: 0.44766828417778015\n",
      "Epoch 569, Loss: 0.7442724406719208, Final Batch Loss: 0.358023464679718\n",
      "Epoch 570, Loss: 0.7495420575141907, Final Batch Loss: 0.4194396138191223\n",
      "Epoch 571, Loss: 0.7715697288513184, Final Batch Loss: 0.3735223114490509\n",
      "Epoch 572, Loss: 0.791961133480072, Final Batch Loss: 0.4129960536956787\n",
      "Epoch 573, Loss: 0.7959132790565491, Final Batch Loss: 0.39326152205467224\n",
      "Epoch 574, Loss: 0.7719192206859589, Final Batch Loss: 0.38832154870033264\n",
      "Epoch 575, Loss: 0.7953230440616608, Final Batch Loss: 0.40960246324539185\n",
      "Epoch 576, Loss: 0.7482969462871552, Final Batch Loss: 0.35286879539489746\n",
      "Epoch 577, Loss: 0.8047315776348114, Final Batch Loss: 0.4115463197231293\n",
      "Epoch 578, Loss: 0.7842390239238739, Final Batch Loss: 0.3472084403038025\n",
      "Epoch 579, Loss: 0.7782687246799469, Final Batch Loss: 0.38504475355148315\n",
      "Epoch 580, Loss: 0.7406645715236664, Final Batch Loss: 0.3691147565841675\n",
      "Epoch 581, Loss: 0.8144374489784241, Final Batch Loss: 0.39972400665283203\n",
      "Epoch 582, Loss: 0.7522949874401093, Final Batch Loss: 0.3871626853942871\n",
      "Epoch 583, Loss: 0.7766419649124146, Final Batch Loss: 0.40671446919441223\n",
      "Epoch 584, Loss: 0.7546821534633636, Final Batch Loss: 0.394910603761673\n",
      "Epoch 585, Loss: 0.7774066030979156, Final Batch Loss: 0.36396247148513794\n",
      "Epoch 586, Loss: 0.7591970860958099, Final Batch Loss: 0.3391702175140381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 587, Loss: 0.7876240015029907, Final Batch Loss: 0.376316636800766\n",
      "Epoch 588, Loss: 0.8008505702018738, Final Batch Loss: 0.3884661793708801\n",
      "Epoch 589, Loss: 0.7669525444507599, Final Batch Loss: 0.3941470980644226\n",
      "Epoch 590, Loss: 0.7263866662979126, Final Batch Loss: 0.3422265350818634\n",
      "Epoch 591, Loss: 0.727202445268631, Final Batch Loss: 0.3614685535430908\n",
      "Epoch 592, Loss: 0.7745302319526672, Final Batch Loss: 0.39068910479545593\n",
      "Epoch 593, Loss: 0.7737047672271729, Final Batch Loss: 0.38115864992141724\n",
      "Epoch 594, Loss: 0.7635727822780609, Final Batch Loss: 0.3662186563014984\n",
      "Epoch 595, Loss: 0.7879249453544617, Final Batch Loss: 0.37633025646209717\n",
      "Epoch 596, Loss: 0.7689136862754822, Final Batch Loss: 0.3952175974845886\n",
      "Epoch 597, Loss: 0.7917901575565338, Final Batch Loss: 0.3996948301792145\n",
      "Epoch 598, Loss: 0.7338975965976715, Final Batch Loss: 0.3514767587184906\n",
      "Epoch 599, Loss: 0.7701934278011322, Final Batch Loss: 0.3614126741886139\n",
      "Epoch 600, Loss: 0.7852209210395813, Final Batch Loss: 0.3910321295261383\n",
      "Epoch 601, Loss: 0.7216306328773499, Final Batch Loss: 0.3899056017398834\n",
      "Epoch 602, Loss: 0.7830781638622284, Final Batch Loss: 0.38300561904907227\n",
      "Epoch 603, Loss: 0.7610098421573639, Final Batch Loss: 0.3587869703769684\n",
      "Epoch 604, Loss: 0.734905332326889, Final Batch Loss: 0.38682040572166443\n",
      "Epoch 605, Loss: 0.8046715557575226, Final Batch Loss: 0.3717975318431854\n",
      "Epoch 606, Loss: 0.7511118948459625, Final Batch Loss: 0.36659765243530273\n",
      "Epoch 607, Loss: 0.785404622554779, Final Batch Loss: 0.41266515851020813\n",
      "Epoch 608, Loss: 0.7471927404403687, Final Batch Loss: 0.3861537575721741\n",
      "Epoch 609, Loss: 0.7521992027759552, Final Batch Loss: 0.42146578431129456\n",
      "Epoch 610, Loss: 0.7848986387252808, Final Batch Loss: 0.3914240896701813\n",
      "Epoch 611, Loss: 0.7389052510261536, Final Batch Loss: 0.3649381995201111\n",
      "Epoch 612, Loss: 0.8275672793388367, Final Batch Loss: 0.4452441334724426\n",
      "Epoch 613, Loss: 0.7253832519054413, Final Batch Loss: 0.3307356834411621\n",
      "Epoch 614, Loss: 0.7089026272296906, Final Batch Loss: 0.3557252585887909\n",
      "Epoch 615, Loss: 0.7889745831489563, Final Batch Loss: 0.3510550856590271\n",
      "Epoch 616, Loss: 0.7513886988162994, Final Batch Loss: 0.3377774655818939\n",
      "Epoch 617, Loss: 0.7292744517326355, Final Batch Loss: 0.3451857268810272\n",
      "Epoch 618, Loss: 0.7280870676040649, Final Batch Loss: 0.3541732728481293\n",
      "Epoch 619, Loss: 0.7299339771270752, Final Batch Loss: 0.385023832321167\n",
      "Epoch 620, Loss: 0.7159053087234497, Final Batch Loss: 0.38105711340904236\n",
      "Epoch 621, Loss: 0.716072291135788, Final Batch Loss: 0.36568906903266907\n",
      "Epoch 622, Loss: 0.7344416975975037, Final Batch Loss: 0.38337528705596924\n",
      "Epoch 623, Loss: 0.7950135171413422, Final Batch Loss: 0.39559033513069153\n",
      "Epoch 624, Loss: 0.7802042663097382, Final Batch Loss: 0.4218093156814575\n",
      "Epoch 625, Loss: 0.7868604958057404, Final Batch Loss: 0.3475528061389923\n",
      "Epoch 626, Loss: 0.7009606957435608, Final Batch Loss: 0.33006876707077026\n",
      "Epoch 627, Loss: 0.7338910102844238, Final Batch Loss: 0.385451078414917\n",
      "Epoch 628, Loss: 0.7850826382637024, Final Batch Loss: 0.4274170994758606\n",
      "Epoch 629, Loss: 0.7640504837036133, Final Batch Loss: 0.36474794149398804\n",
      "Epoch 630, Loss: 0.7555882930755615, Final Batch Loss: 0.3900023102760315\n",
      "Epoch 631, Loss: 0.7391595244407654, Final Batch Loss: 0.3954463601112366\n",
      "Epoch 632, Loss: 0.748168021440506, Final Batch Loss: 0.33661627769470215\n",
      "Epoch 633, Loss: 0.7177550494670868, Final Batch Loss: 0.3292626142501831\n",
      "Epoch 634, Loss: 0.7247629761695862, Final Batch Loss: 0.34806305170059204\n",
      "Epoch 635, Loss: 0.7395511567592621, Final Batch Loss: 0.3701571226119995\n",
      "Epoch 636, Loss: 0.8132262825965881, Final Batch Loss: 0.4100848138332367\n",
      "Epoch 637, Loss: 0.7028776705265045, Final Batch Loss: 0.3484712541103363\n",
      "Epoch 638, Loss: 0.750901997089386, Final Batch Loss: 0.37843087315559387\n",
      "Epoch 639, Loss: 0.8051837682723999, Final Batch Loss: 0.3800806403160095\n",
      "Epoch 640, Loss: 0.7567666172981262, Final Batch Loss: 0.3778893053531647\n",
      "Epoch 641, Loss: 0.7232784628868103, Final Batch Loss: 0.3571372330188751\n",
      "Epoch 642, Loss: 0.7324813008308411, Final Batch Loss: 0.35788145661354065\n",
      "Epoch 643, Loss: 0.7384330034255981, Final Batch Loss: 0.3620765805244446\n",
      "Epoch 644, Loss: 0.776405394077301, Final Batch Loss: 0.3782264292240143\n",
      "Epoch 645, Loss: 0.7418906986713409, Final Batch Loss: 0.31482619047164917\n",
      "Epoch 646, Loss: 0.7195289731025696, Final Batch Loss: 0.33696919679641724\n",
      "Epoch 647, Loss: 0.7202192842960358, Final Batch Loss: 0.3836492598056793\n",
      "Epoch 648, Loss: 0.7620497643947601, Final Batch Loss: 0.33998024463653564\n",
      "Epoch 649, Loss: 0.7205174267292023, Final Batch Loss: 0.35861727595329285\n",
      "Epoch 650, Loss: 0.7841340899467468, Final Batch Loss: 0.3906845450401306\n",
      "Epoch 651, Loss: 0.7845781445503235, Final Batch Loss: 0.42952999472618103\n",
      "Epoch 652, Loss: 0.7073881328105927, Final Batch Loss: 0.3259608745574951\n",
      "Epoch 653, Loss: 0.7629249095916748, Final Batch Loss: 0.3514668643474579\n",
      "Epoch 654, Loss: 0.7534038126468658, Final Batch Loss: 0.3704344630241394\n",
      "Epoch 655, Loss: 0.7461173236370087, Final Batch Loss: 0.35707536339759827\n",
      "Epoch 656, Loss: 0.7418884932994843, Final Batch Loss: 0.3917987644672394\n",
      "Epoch 657, Loss: 0.7184930145740509, Final Batch Loss: 0.3294961750507355\n",
      "Epoch 658, Loss: 0.7488751411437988, Final Batch Loss: 0.37094053626060486\n",
      "Epoch 659, Loss: 0.7385863661766052, Final Batch Loss: 0.3411608636379242\n",
      "Epoch 660, Loss: 0.7550713419914246, Final Batch Loss: 0.34922167658805847\n",
      "Epoch 661, Loss: 0.7531028389930725, Final Batch Loss: 0.40003281831741333\n",
      "Epoch 662, Loss: 0.7657735049724579, Final Batch Loss: 0.3744053542613983\n",
      "Epoch 663, Loss: 0.7247912883758545, Final Batch Loss: 0.39059802889823914\n",
      "Epoch 664, Loss: 0.7408702671527863, Final Batch Loss: 0.356173574924469\n",
      "Epoch 665, Loss: 0.8009064793586731, Final Batch Loss: 0.4221605658531189\n",
      "Epoch 666, Loss: 0.7988793253898621, Final Batch Loss: 0.356601357460022\n",
      "Epoch 667, Loss: 0.760889321565628, Final Batch Loss: 0.3739473819732666\n",
      "Epoch 668, Loss: 0.7427042424678802, Final Batch Loss: 0.37168559432029724\n",
      "Epoch 669, Loss: 0.8533704876899719, Final Batch Loss: 0.41789478063583374\n",
      "Epoch 670, Loss: 0.7425145208835602, Final Batch Loss: 0.3636665344238281\n",
      "Epoch 671, Loss: 0.7182800471782684, Final Batch Loss: 0.3450130820274353\n",
      "Epoch 672, Loss: 0.7985515892505646, Final Batch Loss: 0.40456971526145935\n",
      "Epoch 673, Loss: 0.7709575295448303, Final Batch Loss: 0.3848784565925598\n",
      "Epoch 674, Loss: 0.7628398239612579, Final Batch Loss: 0.4185044765472412\n",
      "Epoch 675, Loss: 0.7591298818588257, Final Batch Loss: 0.3972652852535248\n",
      "Epoch 676, Loss: 0.7395731806755066, Final Batch Loss: 0.37983238697052\n",
      "Epoch 677, Loss: 0.7120887935161591, Final Batch Loss: 0.37407785654067993\n",
      "Epoch 678, Loss: 0.7253052592277527, Final Batch Loss: 0.3592783212661743\n",
      "Epoch 679, Loss: 0.736956000328064, Final Batch Loss: 0.3091869652271271\n",
      "Epoch 680, Loss: 0.7857846617698669, Final Batch Loss: 0.4244093596935272\n",
      "Epoch 681, Loss: 0.7789129018783569, Final Batch Loss: 0.35593047738075256\n",
      "Epoch 682, Loss: 0.7728524506092072, Final Batch Loss: 0.4054112136363983\n",
      "Epoch 683, Loss: 0.7700700461864471, Final Batch Loss: 0.39042410254478455\n",
      "Epoch 684, Loss: 0.7087623178958893, Final Batch Loss: 0.34362101554870605\n",
      "Epoch 685, Loss: 0.7453576028347015, Final Batch Loss: 0.38862770795822144\n",
      "Epoch 686, Loss: 0.6871043741703033, Final Batch Loss: 0.33200573921203613\n",
      "Epoch 687, Loss: 0.7733679413795471, Final Batch Loss: 0.4130512475967407\n",
      "Epoch 688, Loss: 0.6907918751239777, Final Batch Loss: 0.3139708340167999\n",
      "Epoch 689, Loss: 0.7147930264472961, Final Batch Loss: 0.39451679587364197\n",
      "Epoch 690, Loss: 0.7616381645202637, Final Batch Loss: 0.349823534488678\n",
      "Epoch 691, Loss: 0.7453673481941223, Final Batch Loss: 0.38243192434310913\n",
      "Epoch 692, Loss: 0.7610389590263367, Final Batch Loss: 0.3631433844566345\n",
      "Epoch 693, Loss: 0.7279932200908661, Final Batch Loss: 0.3372027277946472\n",
      "Epoch 694, Loss: 0.7446427941322327, Final Batch Loss: 0.3809221088886261\n",
      "Epoch 695, Loss: 0.782820463180542, Final Batch Loss: 0.40614429116249084\n",
      "Epoch 696, Loss: 0.747501015663147, Final Batch Loss: 0.3672640919685364\n",
      "Epoch 697, Loss: 0.7522647082805634, Final Batch Loss: 0.3759104311466217\n",
      "Epoch 698, Loss: 0.7082855701446533, Final Batch Loss: 0.37599384784698486\n",
      "Epoch 699, Loss: 0.6988465189933777, Final Batch Loss: 0.37403687834739685\n",
      "Epoch 700, Loss: 0.6784364879131317, Final Batch Loss: 0.3310404419898987\n",
      "Epoch 701, Loss: 0.7034248113632202, Final Batch Loss: 0.31280022859573364\n",
      "Epoch 702, Loss: 0.7011826634407043, Final Batch Loss: 0.37874314188957214\n",
      "Epoch 703, Loss: 0.7862133085727692, Final Batch Loss: 0.3616013526916504\n",
      "Epoch 704, Loss: 0.7272398769855499, Final Batch Loss: 0.3661175072193146\n",
      "Epoch 705, Loss: 0.8147586584091187, Final Batch Loss: 0.4678174555301666\n",
      "Epoch 706, Loss: 0.7754011750221252, Final Batch Loss: 0.40015846490859985\n",
      "Epoch 707, Loss: 0.7168035507202148, Final Batch Loss: 0.3667086958885193\n",
      "Epoch 708, Loss: 0.7324880659580231, Final Batch Loss: 0.40071552991867065\n",
      "Epoch 709, Loss: 0.7170071601867676, Final Batch Loss: 0.3210277259349823\n",
      "Epoch 710, Loss: 0.7614227831363678, Final Batch Loss: 0.37803900241851807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 711, Loss: 0.7518884241580963, Final Batch Loss: 0.41434386372566223\n",
      "Epoch 712, Loss: 0.7499022781848907, Final Batch Loss: 0.3749363124370575\n",
      "Epoch 713, Loss: 0.7717345356941223, Final Batch Loss: 0.4224427044391632\n",
      "Epoch 714, Loss: 0.7215046882629395, Final Batch Loss: 0.39270687103271484\n",
      "Epoch 715, Loss: 0.6995816826820374, Final Batch Loss: 0.3195198178291321\n",
      "Epoch 716, Loss: 0.7250649631023407, Final Batch Loss: 0.3840171694755554\n",
      "Epoch 717, Loss: 0.7579807341098785, Final Batch Loss: 0.36201727390289307\n",
      "Epoch 718, Loss: 0.7117013037204742, Final Batch Loss: 0.3410777747631073\n",
      "Epoch 719, Loss: 0.7310970723628998, Final Batch Loss: 0.3761294484138489\n",
      "Epoch 720, Loss: 0.702181875705719, Final Batch Loss: 0.35345959663391113\n",
      "Epoch 721, Loss: 0.7444546222686768, Final Batch Loss: 0.43617016077041626\n",
      "Epoch 722, Loss: 0.7375493347644806, Final Batch Loss: 0.3955649435520172\n",
      "Epoch 723, Loss: 0.6972117722034454, Final Batch Loss: 0.3187221884727478\n",
      "Epoch 724, Loss: 0.7235954403877258, Final Batch Loss: 0.3237435817718506\n",
      "Epoch 725, Loss: 0.7583162188529968, Final Batch Loss: 0.37806639075279236\n",
      "Epoch 726, Loss: 0.6935434639453888, Final Batch Loss: 0.3045627772808075\n",
      "Epoch 727, Loss: 0.718770295381546, Final Batch Loss: 0.3535953164100647\n",
      "Epoch 728, Loss: 0.7377628684043884, Final Batch Loss: 0.3878194987773895\n",
      "Epoch 729, Loss: 0.7078388333320618, Final Batch Loss: 0.3172968626022339\n",
      "Epoch 730, Loss: 0.7054935693740845, Final Batch Loss: 0.3412189781665802\n",
      "Epoch 731, Loss: 0.7551127970218658, Final Batch Loss: 0.3686686158180237\n",
      "Epoch 732, Loss: 0.7313700914382935, Final Batch Loss: 0.3360496163368225\n",
      "Epoch 733, Loss: 0.6896818578243256, Final Batch Loss: 0.3513103127479553\n",
      "Epoch 734, Loss: 0.7554097473621368, Final Batch Loss: 0.3801661729812622\n",
      "Epoch 735, Loss: 0.6842994391918182, Final Batch Loss: 0.3420061469078064\n",
      "Epoch 736, Loss: 0.75540691614151, Final Batch Loss: 0.33079370856285095\n",
      "Epoch 737, Loss: 0.7133128941059113, Final Batch Loss: 0.3436641991138458\n",
      "Epoch 738, Loss: 0.7589883208274841, Final Batch Loss: 0.38433969020843506\n",
      "Epoch 739, Loss: 0.6961861252784729, Final Batch Loss: 0.354111909866333\n",
      "Epoch 740, Loss: 0.7682275474071503, Final Batch Loss: 0.40215855836868286\n",
      "Epoch 741, Loss: 0.7175972759723663, Final Batch Loss: 0.4047257900238037\n",
      "Epoch 742, Loss: 0.684712678194046, Final Batch Loss: 0.3390072286128998\n",
      "Epoch 743, Loss: 0.7370543479919434, Final Batch Loss: 0.38136541843414307\n",
      "Epoch 744, Loss: 0.7153261005878448, Final Batch Loss: 0.3260711133480072\n",
      "Epoch 745, Loss: 0.7126100063323975, Final Batch Loss: 0.3469311594963074\n",
      "Epoch 746, Loss: 0.6756002306938171, Final Batch Loss: 0.3060528337955475\n",
      "Epoch 747, Loss: 0.7141165435314178, Final Batch Loss: 0.34112051129341125\n",
      "Epoch 748, Loss: 0.7592252194881439, Final Batch Loss: 0.3873617947101593\n",
      "Epoch 749, Loss: 0.7400460839271545, Final Batch Loss: 0.3823620080947876\n",
      "Epoch 750, Loss: 0.7159544825553894, Final Batch Loss: 0.3050888180732727\n",
      "Epoch 751, Loss: 0.7125193476676941, Final Batch Loss: 0.42335277795791626\n",
      "Epoch 752, Loss: 0.6952540278434753, Final Batch Loss: 0.3384665548801422\n",
      "Epoch 753, Loss: 0.7136633992195129, Final Batch Loss: 0.3870578408241272\n",
      "Epoch 754, Loss: 0.6920477449893951, Final Batch Loss: 0.3812025487422943\n",
      "Epoch 755, Loss: 0.7193850576877594, Final Batch Loss: 0.350742906332016\n",
      "Epoch 756, Loss: 0.7887958586215973, Final Batch Loss: 0.39052021503448486\n",
      "Epoch 757, Loss: 0.6557248830795288, Final Batch Loss: 0.3064470589160919\n",
      "Epoch 758, Loss: 0.6911877691745758, Final Batch Loss: 0.3616545796394348\n",
      "Epoch 759, Loss: 0.7686091661453247, Final Batch Loss: 0.44263070821762085\n",
      "Epoch 760, Loss: 0.6814243197441101, Final Batch Loss: 0.3072669804096222\n",
      "Epoch 761, Loss: 0.7689217925071716, Final Batch Loss: 0.3933000862598419\n",
      "Epoch 762, Loss: 0.7061305642127991, Final Batch Loss: 0.39591118693351746\n",
      "Epoch 763, Loss: 0.7066810727119446, Final Batch Loss: 0.33158937096595764\n",
      "Epoch 764, Loss: 0.7130613625049591, Final Batch Loss: 0.34972095489501953\n",
      "Epoch 765, Loss: 0.739313006401062, Final Batch Loss: 0.3788999021053314\n",
      "Epoch 766, Loss: 0.7227174639701843, Final Batch Loss: 0.37503328919410706\n",
      "Epoch 767, Loss: 0.7109908163547516, Final Batch Loss: 0.4007527530193329\n",
      "Epoch 768, Loss: 0.6793612241744995, Final Batch Loss: 0.32883232831954956\n",
      "Epoch 769, Loss: 0.7653974890708923, Final Batch Loss: 0.41024500131607056\n",
      "Epoch 770, Loss: 0.7071304321289062, Final Batch Loss: 0.3532096743583679\n",
      "Epoch 771, Loss: 0.7737683951854706, Final Batch Loss: 0.38467639684677124\n",
      "Epoch 772, Loss: 0.6944654881954193, Final Batch Loss: 0.3095216155052185\n",
      "Epoch 773, Loss: 0.6458505690097809, Final Batch Loss: 0.3438173830509186\n",
      "Epoch 774, Loss: 0.6845613420009613, Final Batch Loss: 0.307834267616272\n",
      "Epoch 775, Loss: 0.7212618887424469, Final Batch Loss: 0.38349461555480957\n",
      "Epoch 776, Loss: 0.6719527244567871, Final Batch Loss: 0.3041563332080841\n",
      "Epoch 777, Loss: 0.7191210389137268, Final Batch Loss: 0.3746032118797302\n",
      "Epoch 778, Loss: 0.6984268128871918, Final Batch Loss: 0.3343553841114044\n",
      "Epoch 779, Loss: 0.7107831835746765, Final Batch Loss: 0.3644675612449646\n",
      "Epoch 780, Loss: 0.656160444021225, Final Batch Loss: 0.3521910011768341\n",
      "Epoch 781, Loss: 0.7044905126094818, Final Batch Loss: 0.365021288394928\n",
      "Epoch 782, Loss: 0.7029275596141815, Final Batch Loss: 0.38771024346351624\n",
      "Epoch 783, Loss: 0.6946235299110413, Final Batch Loss: 0.31444409489631653\n",
      "Epoch 784, Loss: 0.6847800016403198, Final Batch Loss: 0.2940901815891266\n",
      "Epoch 785, Loss: 0.717413604259491, Final Batch Loss: 0.3432689607143402\n",
      "Epoch 786, Loss: 0.7423668205738068, Final Batch Loss: 0.39585086703300476\n",
      "Epoch 787, Loss: 0.7358468174934387, Final Batch Loss: 0.3385898768901825\n",
      "Epoch 788, Loss: 0.7047033607959747, Final Batch Loss: 0.3160427212715149\n",
      "Epoch 789, Loss: 0.7300843894481659, Final Batch Loss: 0.36613762378692627\n",
      "Epoch 790, Loss: 0.7141571938991547, Final Batch Loss: 0.3436679542064667\n",
      "Epoch 791, Loss: 0.7005544900894165, Final Batch Loss: 0.3501913249492645\n",
      "Epoch 792, Loss: 0.6587043404579163, Final Batch Loss: 0.30078595876693726\n",
      "Epoch 793, Loss: 0.6968615651130676, Final Batch Loss: 0.3179648816585541\n",
      "Epoch 794, Loss: 0.7061845362186432, Final Batch Loss: 0.38166260719299316\n",
      "Epoch 795, Loss: 0.6975888609886169, Final Batch Loss: 0.34426409006118774\n",
      "Epoch 796, Loss: 0.6967863738536835, Final Batch Loss: 0.36905744671821594\n",
      "Epoch 797, Loss: 0.694252222776413, Final Batch Loss: 0.3350684344768524\n",
      "Epoch 798, Loss: 0.7213812172412872, Final Batch Loss: 0.33979418873786926\n",
      "Epoch 799, Loss: 0.6610359251499176, Final Batch Loss: 0.35894641280174255\n",
      "Epoch 800, Loss: 0.6910918056964874, Final Batch Loss: 0.33178335428237915\n",
      "Epoch 801, Loss: 0.7114169597625732, Final Batch Loss: 0.3810959756374359\n",
      "Epoch 802, Loss: 0.732231080532074, Final Batch Loss: 0.37440410256385803\n",
      "Epoch 803, Loss: 0.7016915082931519, Final Batch Loss: 0.33726799488067627\n",
      "Epoch 804, Loss: 0.7530394196510315, Final Batch Loss: 0.36711809039115906\n",
      "Epoch 805, Loss: 0.6428584158420563, Final Batch Loss: 0.3150160014629364\n",
      "Epoch 806, Loss: 0.6930631399154663, Final Batch Loss: 0.36227670311927795\n",
      "Epoch 807, Loss: 0.6598401665687561, Final Batch Loss: 0.3205668032169342\n",
      "Epoch 808, Loss: 0.7005415260791779, Final Batch Loss: 0.369931697845459\n",
      "Epoch 809, Loss: 0.6346262991428375, Final Batch Loss: 0.2984030842781067\n",
      "Epoch 810, Loss: 0.6784655451774597, Final Batch Loss: 0.3577529788017273\n",
      "Epoch 811, Loss: 0.6952276527881622, Final Batch Loss: 0.3719208836555481\n",
      "Epoch 812, Loss: 0.7095949053764343, Final Batch Loss: 0.3721309304237366\n",
      "Epoch 813, Loss: 0.7527616918087006, Final Batch Loss: 0.39624130725860596\n",
      "Epoch 814, Loss: 0.696219801902771, Final Batch Loss: 0.32600754499435425\n",
      "Epoch 815, Loss: 0.6775814890861511, Final Batch Loss: 0.3626663386821747\n",
      "Epoch 816, Loss: 0.7609396874904633, Final Batch Loss: 0.39178019762039185\n",
      "Epoch 817, Loss: 0.7310585677623749, Final Batch Loss: 0.38012582063674927\n",
      "Epoch 818, Loss: 0.7122830450534821, Final Batch Loss: 0.3935694694519043\n",
      "Epoch 819, Loss: 0.7105658054351807, Final Batch Loss: 0.3393147885799408\n",
      "Epoch 820, Loss: 0.702775627374649, Final Batch Loss: 0.3988303542137146\n",
      "Epoch 821, Loss: 0.7515504360198975, Final Batch Loss: 0.4061972200870514\n",
      "Epoch 822, Loss: 0.7063621878623962, Final Batch Loss: 0.3362027108669281\n",
      "Epoch 823, Loss: 0.6635717153549194, Final Batch Loss: 0.30624833703041077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 824, Loss: 0.6683849990367889, Final Batch Loss: 0.3186374008655548\n",
      "Epoch 825, Loss: 0.6583245098590851, Final Batch Loss: 0.31034713983535767\n",
      "Epoch 826, Loss: 0.6768603920936584, Final Batch Loss: 0.3231925368309021\n",
      "Epoch 827, Loss: 0.6867288053035736, Final Batch Loss: 0.3755737245082855\n",
      "Epoch 828, Loss: 0.6304280161857605, Final Batch Loss: 0.3264952301979065\n",
      "Epoch 829, Loss: 0.735447496175766, Final Batch Loss: 0.40274375677108765\n",
      "Epoch 830, Loss: 0.6653017103672028, Final Batch Loss: 0.35290291905403137\n",
      "Epoch 831, Loss: 0.7085677683353424, Final Batch Loss: 0.39466187357902527\n",
      "Epoch 832, Loss: 0.703946590423584, Final Batch Loss: 0.30630919337272644\n",
      "Epoch 833, Loss: 0.6680479049682617, Final Batch Loss: 0.3122423589229584\n",
      "Epoch 834, Loss: 0.7000282108783722, Final Batch Loss: 0.3900827169418335\n",
      "Epoch 835, Loss: 0.7007641792297363, Final Batch Loss: 0.3188948631286621\n",
      "Epoch 836, Loss: 0.7383315861225128, Final Batch Loss: 0.32923486828804016\n",
      "Epoch 837, Loss: 0.6898826360702515, Final Batch Loss: 0.32567039132118225\n",
      "Epoch 838, Loss: 0.682006448507309, Final Batch Loss: 0.31791678071022034\n",
      "Epoch 839, Loss: 0.74554044008255, Final Batch Loss: 0.3684847950935364\n",
      "Epoch 840, Loss: 0.6639525294303894, Final Batch Loss: 0.29462888836860657\n",
      "Epoch 841, Loss: 0.6646952331066132, Final Batch Loss: 0.3542906939983368\n",
      "Epoch 842, Loss: 0.6865569949150085, Final Batch Loss: 0.3529782295227051\n",
      "Epoch 843, Loss: 0.6703994274139404, Final Batch Loss: 0.3504425585269928\n",
      "Epoch 844, Loss: 0.6831050515174866, Final Batch Loss: 0.36458849906921387\n",
      "Epoch 845, Loss: 0.7312561273574829, Final Batch Loss: 0.34711384773254395\n",
      "Epoch 846, Loss: 0.6922596991062164, Final Batch Loss: 0.33893582224845886\n",
      "Epoch 847, Loss: 0.6353555917739868, Final Batch Loss: 0.34028616547584534\n",
      "Epoch 848, Loss: 0.695804238319397, Final Batch Loss: 0.38869988918304443\n",
      "Epoch 849, Loss: 0.7521679699420929, Final Batch Loss: 0.4210233688354492\n",
      "Epoch 850, Loss: 0.7492761015892029, Final Batch Loss: 0.3701421320438385\n",
      "Epoch 851, Loss: 0.6989206671714783, Final Batch Loss: 0.3446881175041199\n",
      "Epoch 852, Loss: 0.6699750125408173, Final Batch Loss: 0.3595829904079437\n",
      "Epoch 853, Loss: 0.6831629276275635, Final Batch Loss: 0.3135375678539276\n",
      "Epoch 854, Loss: 0.6741826832294464, Final Batch Loss: 0.33526450395584106\n",
      "Epoch 855, Loss: 0.7073078453540802, Final Batch Loss: 0.3378371298313141\n",
      "Epoch 856, Loss: 0.7314405739307404, Final Batch Loss: 0.33749040961265564\n",
      "Epoch 857, Loss: 0.7175682187080383, Final Batch Loss: 0.3568372428417206\n",
      "Epoch 858, Loss: 0.6495802402496338, Final Batch Loss: 0.32291772961616516\n",
      "Epoch 859, Loss: 0.6792992651462555, Final Batch Loss: 0.33352112770080566\n",
      "Epoch 860, Loss: 0.6362574696540833, Final Batch Loss: 0.34316885471343994\n",
      "Epoch 861, Loss: 0.6867993474006653, Final Batch Loss: 0.3288439214229584\n",
      "Epoch 862, Loss: 0.6715786457061768, Final Batch Loss: 0.36515378952026367\n",
      "Epoch 863, Loss: 0.7027763426303864, Final Batch Loss: 0.3457967936992645\n",
      "Epoch 864, Loss: 0.7058468461036682, Final Batch Loss: 0.3201444149017334\n",
      "Epoch 865, Loss: 0.6743307709693909, Final Batch Loss: 0.2981834411621094\n",
      "Epoch 866, Loss: 0.6893370151519775, Final Batch Loss: 0.353066623210907\n",
      "Epoch 867, Loss: 0.727337658405304, Final Batch Loss: 0.32480859756469727\n",
      "Epoch 868, Loss: 0.7144701480865479, Final Batch Loss: 0.3705943524837494\n",
      "Epoch 869, Loss: 0.7020616233348846, Final Batch Loss: 0.3858676552772522\n",
      "Epoch 870, Loss: 0.7089878022670746, Final Batch Loss: 0.38309144973754883\n",
      "Epoch 871, Loss: 0.6529321074485779, Final Batch Loss: 0.3046196401119232\n",
      "Epoch 872, Loss: 0.707394152879715, Final Batch Loss: 0.3704862594604492\n",
      "Epoch 873, Loss: 0.7045658230781555, Final Batch Loss: 0.3635476231575012\n",
      "Epoch 874, Loss: 0.6438817083835602, Final Batch Loss: 0.33615565299987793\n",
      "Epoch 875, Loss: 0.7141726315021515, Final Batch Loss: 0.38526442646980286\n",
      "Epoch 876, Loss: 0.6437630653381348, Final Batch Loss: 0.3131926655769348\n",
      "Epoch 877, Loss: 0.6481640636920929, Final Batch Loss: 0.314301073551178\n",
      "Epoch 878, Loss: 0.6673153042793274, Final Batch Loss: 0.31817108392715454\n",
      "Epoch 879, Loss: 0.6487071216106415, Final Batch Loss: 0.3180026710033417\n",
      "Epoch 880, Loss: 0.6625479757785797, Final Batch Loss: 0.34433692693710327\n",
      "Epoch 881, Loss: 0.7107833027839661, Final Batch Loss: 0.36643946170806885\n",
      "Epoch 882, Loss: 0.6388494670391083, Final Batch Loss: 0.3389612138271332\n",
      "Epoch 883, Loss: 0.673799455165863, Final Batch Loss: 0.3513320982456207\n",
      "Epoch 884, Loss: 0.7156544625759125, Final Batch Loss: 0.36307770013809204\n",
      "Epoch 885, Loss: 0.6853547692298889, Final Batch Loss: 0.32935795187950134\n",
      "Epoch 886, Loss: 0.692654550075531, Final Batch Loss: 0.31333744525909424\n",
      "Epoch 887, Loss: 0.6718067824840546, Final Batch Loss: 0.3450518250465393\n",
      "Epoch 888, Loss: 0.7135989665985107, Final Batch Loss: 0.370362251996994\n",
      "Epoch 889, Loss: 0.6713395714759827, Final Batch Loss: 0.3307764530181885\n",
      "Epoch 890, Loss: 0.6623488962650299, Final Batch Loss: 0.3355328142642975\n",
      "Epoch 891, Loss: 0.7296271324157715, Final Batch Loss: 0.31482699513435364\n",
      "Epoch 892, Loss: 0.6683472096920013, Final Batch Loss: 0.33750978112220764\n",
      "Epoch 893, Loss: 0.6888371706008911, Final Batch Loss: 0.3449433445930481\n",
      "Epoch 894, Loss: 0.6536865234375, Final Batch Loss: 0.2874644994735718\n",
      "Epoch 895, Loss: 0.6818931698799133, Final Batch Loss: 0.3179108798503876\n",
      "Epoch 896, Loss: 0.6716936826705933, Final Batch Loss: 0.31988540291786194\n",
      "Epoch 897, Loss: 0.6505921185016632, Final Batch Loss: 0.30720362067222595\n",
      "Epoch 898, Loss: 0.690178632736206, Final Batch Loss: 0.3340190351009369\n",
      "Epoch 899, Loss: 0.6869570910930634, Final Batch Loss: 0.3132999539375305\n",
      "Epoch 900, Loss: 0.6756666004657745, Final Batch Loss: 0.3441154658794403\n",
      "Epoch 901, Loss: 0.7229967713356018, Final Batch Loss: 0.3834903836250305\n",
      "Epoch 902, Loss: 0.6698213815689087, Final Batch Loss: 0.34240493178367615\n",
      "Epoch 903, Loss: 0.7501390874385834, Final Batch Loss: 0.38565391302108765\n",
      "Epoch 904, Loss: 0.6788859665393829, Final Batch Loss: 0.33467182517051697\n",
      "Epoch 905, Loss: 0.6598404943943024, Final Batch Loss: 0.3434038758277893\n",
      "Epoch 906, Loss: 0.652956634759903, Final Batch Loss: 0.3057577311992645\n",
      "Epoch 907, Loss: 0.6982589066028595, Final Batch Loss: 0.35779502987861633\n",
      "Epoch 908, Loss: 0.6922931373119354, Final Batch Loss: 0.37948736548423767\n",
      "Epoch 909, Loss: 0.7181527018547058, Final Batch Loss: 0.3767204284667969\n",
      "Epoch 910, Loss: 0.7268155813217163, Final Batch Loss: 0.38479816913604736\n",
      "Epoch 911, Loss: 0.6761436760425568, Final Batch Loss: 0.3233063519001007\n",
      "Epoch 912, Loss: 0.6620347499847412, Final Batch Loss: 0.32448849081993103\n",
      "Epoch 913, Loss: 0.7210201323032379, Final Batch Loss: 0.3487057387828827\n",
      "Epoch 914, Loss: 0.6895961761474609, Final Batch Loss: 0.3707762360572815\n",
      "Epoch 915, Loss: 0.730429083108902, Final Batch Loss: 0.34755489230155945\n",
      "Epoch 916, Loss: 0.7186641693115234, Final Batch Loss: 0.34823378920555115\n",
      "Epoch 917, Loss: 0.6683357954025269, Final Batch Loss: 0.34628862142562866\n",
      "Epoch 918, Loss: 0.6736563146114349, Final Batch Loss: 0.33298975229263306\n",
      "Epoch 919, Loss: 0.6612331569194794, Final Batch Loss: 0.3414556384086609\n",
      "Epoch 920, Loss: 0.6844916045665741, Final Batch Loss: 0.37720537185668945\n",
      "Epoch 921, Loss: 0.6978088319301605, Final Batch Loss: 0.3365410566329956\n",
      "Epoch 922, Loss: 0.665254145860672, Final Batch Loss: 0.36382055282592773\n",
      "Epoch 923, Loss: 0.664311945438385, Final Batch Loss: 0.3116028904914856\n",
      "Epoch 924, Loss: 0.6278942823410034, Final Batch Loss: 0.3372025489807129\n",
      "Epoch 925, Loss: 0.645986407995224, Final Batch Loss: 0.2929213345050812\n",
      "Epoch 926, Loss: 0.6578995585441589, Final Batch Loss: 0.3258820176124573\n",
      "Epoch 927, Loss: 0.6558701694011688, Final Batch Loss: 0.3053181767463684\n",
      "Epoch 928, Loss: 0.7245311141014099, Final Batch Loss: 0.33251211047172546\n",
      "Epoch 929, Loss: 0.6826985776424408, Final Batch Loss: 0.26334840059280396\n",
      "Epoch 930, Loss: 0.6993003487586975, Final Batch Loss: 0.35842788219451904\n",
      "Epoch 931, Loss: 0.6741412281990051, Final Batch Loss: 0.3342273533344269\n",
      "Epoch 932, Loss: 0.7130913436412811, Final Batch Loss: 0.33721643686294556\n",
      "Epoch 933, Loss: 0.6751705408096313, Final Batch Loss: 0.3139726221561432\n",
      "Epoch 934, Loss: 0.6372827291488647, Final Batch Loss: 0.311076819896698\n",
      "Epoch 935, Loss: 0.6222935020923615, Final Batch Loss: 0.3062627613544464\n",
      "Epoch 936, Loss: 0.660920113325119, Final Batch Loss: 0.29658323526382446\n",
      "Epoch 937, Loss: 0.6754432618618011, Final Batch Loss: 0.33525756001472473\n",
      "Epoch 938, Loss: 0.7172870635986328, Final Batch Loss: 0.38362088799476624\n",
      "Epoch 939, Loss: 0.6826609969139099, Final Batch Loss: 0.33168652653694153\n",
      "Epoch 940, Loss: 0.6673833727836609, Final Batch Loss: 0.31280940771102905\n",
      "Epoch 941, Loss: 0.7441446781158447, Final Batch Loss: 0.3763576149940491\n",
      "Epoch 942, Loss: 0.7289164066314697, Final Batch Loss: 0.3770174980163574\n",
      "Epoch 943, Loss: 0.6601723730564117, Final Batch Loss: 0.33326414227485657\n",
      "Epoch 944, Loss: 0.6650147140026093, Final Batch Loss: 0.3375864624977112\n",
      "Epoch 945, Loss: 0.7452422380447388, Final Batch Loss: 0.3544946014881134\n",
      "Epoch 946, Loss: 0.694195419549942, Final Batch Loss: 0.34096598625183105\n",
      "Epoch 947, Loss: 0.6980045735836029, Final Batch Loss: 0.377652645111084\n",
      "Epoch 948, Loss: 0.6744950115680695, Final Batch Loss: 0.31919360160827637\n",
      "Epoch 949, Loss: 0.6380288600921631, Final Batch Loss: 0.3038914203643799\n",
      "Epoch 950, Loss: 0.658665806055069, Final Batch Loss: 0.3153321444988251\n",
      "Epoch 951, Loss: 0.6546914577484131, Final Batch Loss: 0.38686859607696533\n",
      "Epoch 952, Loss: 0.6722358167171478, Final Batch Loss: 0.348685085773468\n",
      "Epoch 953, Loss: 0.6742480397224426, Final Batch Loss: 0.38077035546302795\n",
      "Epoch 954, Loss: 0.6779085993766785, Final Batch Loss: 0.351633220911026\n",
      "Epoch 955, Loss: 0.6766335368156433, Final Batch Loss: 0.3506869971752167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 956, Loss: 0.7369642853736877, Final Batch Loss: 0.41576534509658813\n",
      "Epoch 957, Loss: 0.679140716791153, Final Batch Loss: 0.4030260145664215\n",
      "Epoch 958, Loss: 0.6157627403736115, Final Batch Loss: 0.30845513939857483\n",
      "Epoch 959, Loss: 0.7338462173938751, Final Batch Loss: 0.37964165210723877\n",
      "Epoch 960, Loss: 0.6454042196273804, Final Batch Loss: 0.29177325963974\n",
      "Epoch 961, Loss: 0.6482941806316376, Final Batch Loss: 0.30913591384887695\n",
      "Epoch 962, Loss: 0.6526205241680145, Final Batch Loss: 0.3081534206867218\n",
      "Epoch 963, Loss: 0.6754910945892334, Final Batch Loss: 0.3036169409751892\n",
      "Epoch 964, Loss: 0.6664107739925385, Final Batch Loss: 0.37468162178993225\n",
      "Epoch 965, Loss: 0.6308768689632416, Final Batch Loss: 0.29936179518699646\n",
      "Epoch 966, Loss: 0.707822859287262, Final Batch Loss: 0.3733268678188324\n",
      "Epoch 967, Loss: 0.6812463998794556, Final Batch Loss: 0.3065835237503052\n",
      "Epoch 968, Loss: 0.6606520712375641, Final Batch Loss: 0.343986451625824\n",
      "Epoch 969, Loss: 0.6558511257171631, Final Batch Loss: 0.33991894125938416\n",
      "Epoch 970, Loss: 0.6297277510166168, Final Batch Loss: 0.24845212697982788\n",
      "Epoch 971, Loss: 0.734613299369812, Final Batch Loss: 0.3961452841758728\n",
      "Epoch 972, Loss: 0.6440839171409607, Final Batch Loss: 0.3338613510131836\n",
      "Epoch 973, Loss: 0.6298477947711945, Final Batch Loss: 0.3181694746017456\n",
      "Epoch 974, Loss: 0.6875617504119873, Final Batch Loss: 0.3779512047767639\n",
      "Epoch 975, Loss: 0.6712191104888916, Final Batch Loss: 0.31107598543167114\n",
      "Epoch 976, Loss: 0.6676201224327087, Final Batch Loss: 0.3755026161670685\n",
      "Epoch 977, Loss: 0.6596552133560181, Final Batch Loss: 0.27611035108566284\n",
      "Epoch 978, Loss: 0.7008503675460815, Final Batch Loss: 0.38003700971603394\n",
      "Epoch 979, Loss: 0.6339744627475739, Final Batch Loss: 0.30826836824417114\n",
      "Epoch 980, Loss: 0.670005202293396, Final Batch Loss: 0.3288738429546356\n",
      "Epoch 981, Loss: 0.6551631987094879, Final Batch Loss: 0.3297518193721771\n",
      "Epoch 982, Loss: 0.6587989032268524, Final Batch Loss: 0.3104924261569977\n",
      "Epoch 983, Loss: 0.6462536156177521, Final Batch Loss: 0.3040725886821747\n",
      "Epoch 984, Loss: 0.6776831448078156, Final Batch Loss: 0.32653266191482544\n",
      "Epoch 985, Loss: 0.6301915347576141, Final Batch Loss: 0.2847103774547577\n",
      "Epoch 986, Loss: 0.6799326241016388, Final Batch Loss: 0.31600475311279297\n",
      "Epoch 987, Loss: 0.6697368621826172, Final Batch Loss: 0.3730579614639282\n",
      "Epoch 988, Loss: 0.6521914303302765, Final Batch Loss: 0.3192419707775116\n",
      "Epoch 989, Loss: 0.6610714495182037, Final Batch Loss: 0.30859410762786865\n",
      "Epoch 990, Loss: 0.6630008220672607, Final Batch Loss: 0.2957684397697449\n",
      "Epoch 991, Loss: 0.6825349926948547, Final Batch Loss: 0.34742745757102966\n",
      "Epoch 992, Loss: 0.6955105662345886, Final Batch Loss: 0.3579613268375397\n",
      "Epoch 993, Loss: 0.669320285320282, Final Batch Loss: 0.3403385579586029\n",
      "Epoch 994, Loss: 0.648144006729126, Final Batch Loss: 0.29431384801864624\n",
      "Epoch 995, Loss: 0.6381899118423462, Final Batch Loss: 0.32097384333610535\n",
      "Epoch 996, Loss: 0.6800293922424316, Final Batch Loss: 0.28715938329696655\n",
      "Epoch 997, Loss: 0.6855850219726562, Final Batch Loss: 0.33035656809806824\n",
      "Epoch 998, Loss: 0.6808642148971558, Final Batch Loss: 0.3500131666660309\n",
      "Epoch 999, Loss: 0.6581862568855286, Final Batch Loss: 0.30100277066230774\n",
      "Epoch 1000, Loss: 0.6822325587272644, Final Batch Loss: 0.3422921299934387\n",
      "Epoch 1001, Loss: 0.6605288982391357, Final Batch Loss: 0.31474199891090393\n",
      "Epoch 1002, Loss: 0.6240756213665009, Final Batch Loss: 0.33122169971466064\n",
      "Epoch 1003, Loss: 0.6377418041229248, Final Batch Loss: 0.34003308415412903\n",
      "Epoch 1004, Loss: 0.6389848589897156, Final Batch Loss: 0.312111496925354\n",
      "Epoch 1005, Loss: 0.6295494437217712, Final Batch Loss: 0.2955525815486908\n",
      "Epoch 1006, Loss: 0.6181686520576477, Final Batch Loss: 0.31467512249946594\n",
      "Epoch 1007, Loss: 0.6354579031467438, Final Batch Loss: 0.32123661041259766\n",
      "Epoch 1008, Loss: 0.6764736175537109, Final Batch Loss: 0.3221794068813324\n",
      "Epoch 1009, Loss: 0.6382295489311218, Final Batch Loss: 0.35081756114959717\n",
      "Epoch 1010, Loss: 0.6702024638652802, Final Batch Loss: 0.3675142228603363\n",
      "Epoch 1011, Loss: 0.653982937335968, Final Batch Loss: 0.2915741503238678\n",
      "Epoch 1012, Loss: 0.6439897418022156, Final Batch Loss: 0.3379097878932953\n",
      "Epoch 1013, Loss: 0.6163091659545898, Final Batch Loss: 0.2969221770763397\n",
      "Epoch 1014, Loss: 0.6431801319122314, Final Batch Loss: 0.3145115077495575\n",
      "Epoch 1015, Loss: 0.6143815517425537, Final Batch Loss: 0.28724390268325806\n",
      "Epoch 1016, Loss: 0.6187836825847626, Final Batch Loss: 0.31169265508651733\n",
      "Epoch 1017, Loss: 0.7357488572597504, Final Batch Loss: 0.31979724764823914\n",
      "Epoch 1018, Loss: 0.6333789527416229, Final Batch Loss: 0.33131036162376404\n",
      "Epoch 1019, Loss: 0.6765695214271545, Final Batch Loss: 0.3155684173107147\n",
      "Epoch 1020, Loss: 0.6771574318408966, Final Batch Loss: 0.3198522925376892\n",
      "Epoch 1021, Loss: 0.6624478995800018, Final Batch Loss: 0.3537690341472626\n",
      "Epoch 1022, Loss: 0.6276368200778961, Final Batch Loss: 0.3357159197330475\n",
      "Epoch 1023, Loss: 0.6421939134597778, Final Batch Loss: 0.3323553502559662\n",
      "Epoch 1024, Loss: 0.6532370448112488, Final Batch Loss: 0.3096170723438263\n",
      "Epoch 1025, Loss: 0.6066976487636566, Final Batch Loss: 0.30758219957351685\n",
      "Epoch 1026, Loss: 0.788808286190033, Final Batch Loss: 0.44087472558021545\n",
      "Epoch 1027, Loss: 0.6869231462478638, Final Batch Loss: 0.37916263937950134\n",
      "Epoch 1028, Loss: 0.7047602534294128, Final Batch Loss: 0.34402087330818176\n",
      "Epoch 1029, Loss: 0.6934817433357239, Final Batch Loss: 0.38466712832450867\n",
      "Epoch 1030, Loss: 0.6551780998706818, Final Batch Loss: 0.3210556209087372\n",
      "Epoch 1031, Loss: 0.6448254287242889, Final Batch Loss: 0.3257787227630615\n",
      "Epoch 1032, Loss: 0.6186741292476654, Final Batch Loss: 0.3777633011341095\n",
      "Epoch 1033, Loss: 0.6814828813076019, Final Batch Loss: 0.32779791951179504\n",
      "Epoch 1034, Loss: 0.6378685235977173, Final Batch Loss: 0.3249487578868866\n",
      "Epoch 1035, Loss: 0.666055291891098, Final Batch Loss: 0.32263535261154175\n",
      "Epoch 1036, Loss: 0.6375806033611298, Final Batch Loss: 0.32739681005477905\n",
      "Epoch 1037, Loss: 0.6455481946468353, Final Batch Loss: 0.3231634199619293\n",
      "Epoch 1038, Loss: 0.6372020542621613, Final Batch Loss: 0.3088470995426178\n",
      "Epoch 1039, Loss: 0.6249296367168427, Final Batch Loss: 0.2746615707874298\n",
      "Epoch 1040, Loss: 0.6402338743209839, Final Batch Loss: 0.334329754114151\n",
      "Epoch 1041, Loss: 0.6586442291736603, Final Batch Loss: 0.3127964437007904\n",
      "Epoch 1042, Loss: 0.6701838374137878, Final Batch Loss: 0.33035510778427124\n",
      "Epoch 1043, Loss: 0.6548091173171997, Final Batch Loss: 0.367586225271225\n",
      "Epoch 1044, Loss: 0.6080206334590912, Final Batch Loss: 0.2830222547054291\n",
      "Epoch 1045, Loss: 0.6119753122329712, Final Batch Loss: 0.3271852135658264\n",
      "Epoch 1046, Loss: 0.6429964005947113, Final Batch Loss: 0.3463088572025299\n",
      "Epoch 1047, Loss: 0.6850621700286865, Final Batch Loss: 0.34548714756965637\n",
      "Epoch 1048, Loss: 0.6378259658813477, Final Batch Loss: 0.32062849402427673\n",
      "Epoch 1049, Loss: 0.6749118268489838, Final Batch Loss: 0.32001596689224243\n",
      "Epoch 1050, Loss: 0.6342118978500366, Final Batch Loss: 0.34444600343704224\n",
      "Epoch 1051, Loss: 0.6467479765415192, Final Batch Loss: 0.3216482102870941\n",
      "Epoch 1052, Loss: 0.6845099925994873, Final Batch Loss: 0.3111736476421356\n",
      "Epoch 1053, Loss: 0.6254526376724243, Final Batch Loss: 0.3125663101673126\n",
      "Epoch 1054, Loss: 0.6788816154003143, Final Batch Loss: 0.3572034537792206\n",
      "Epoch 1055, Loss: 0.6320239901542664, Final Batch Loss: 0.3087937533855438\n",
      "Epoch 1056, Loss: 0.6600923240184784, Final Batch Loss: 0.33292442560195923\n",
      "Epoch 1057, Loss: 0.6511757969856262, Final Batch Loss: 0.3598216474056244\n",
      "Epoch 1058, Loss: 0.687517374753952, Final Batch Loss: 0.32158562541007996\n",
      "Epoch 1059, Loss: 0.6864242255687714, Final Batch Loss: 0.36422568559646606\n",
      "Epoch 1060, Loss: 0.6654874682426453, Final Batch Loss: 0.32968297600746155\n",
      "Epoch 1061, Loss: 0.5983297228813171, Final Batch Loss: 0.2844492197036743\n",
      "Epoch 1062, Loss: 0.6549495458602905, Final Batch Loss: 0.3449348509311676\n",
      "Epoch 1063, Loss: 0.6195272505283356, Final Batch Loss: 0.3122319281101227\n",
      "Epoch 1064, Loss: 0.6398012340068817, Final Batch Loss: 0.30672481656074524\n",
      "Epoch 1065, Loss: 0.6611621379852295, Final Batch Loss: 0.32715535163879395\n",
      "Epoch 1066, Loss: 0.6876365542411804, Final Batch Loss: 0.3172158896923065\n",
      "Epoch 1067, Loss: 0.6828502416610718, Final Batch Loss: 0.2940436005592346\n",
      "Epoch 1068, Loss: 0.6184540092945099, Final Batch Loss: 0.3074129521846771\n",
      "Epoch 1069, Loss: 0.6624581217765808, Final Batch Loss: 0.34063300490379333\n",
      "Epoch 1070, Loss: 0.6432486176490784, Final Batch Loss: 0.32384297251701355\n",
      "Epoch 1071, Loss: 0.6557069718837738, Final Batch Loss: 0.35417604446411133\n",
      "Epoch 1072, Loss: 0.6719911992549896, Final Batch Loss: 0.37737590074539185\n",
      "Epoch 1073, Loss: 0.6526531875133514, Final Batch Loss: 0.3642175495624542\n",
      "Epoch 1074, Loss: 0.6419134736061096, Final Batch Loss: 0.31177258491516113\n",
      "Epoch 1075, Loss: 0.6215687692165375, Final Batch Loss: 0.32267075777053833\n",
      "Epoch 1076, Loss: 0.6399333775043488, Final Batch Loss: 0.31493431329727173\n",
      "Epoch 1077, Loss: 0.657572329044342, Final Batch Loss: 0.2830318212509155\n",
      "Epoch 1078, Loss: 0.6438993811607361, Final Batch Loss: 0.2850453853607178\n",
      "Epoch 1079, Loss: 0.6478652954101562, Final Batch Loss: 0.2799229025840759\n",
      "Epoch 1080, Loss: 0.6313129961490631, Final Batch Loss: 0.3414061367511749\n",
      "Epoch 1081, Loss: 0.61646369099617, Final Batch Loss: 0.289291113615036\n",
      "Epoch 1082, Loss: 0.6642962396144867, Final Batch Loss: 0.29873088002204895\n",
      "Epoch 1083, Loss: 0.6541038751602173, Final Batch Loss: 0.32709386944770813\n",
      "Epoch 1084, Loss: 0.6437203884124756, Final Batch Loss: 0.324106365442276\n",
      "Epoch 1085, Loss: 0.6837013065814972, Final Batch Loss: 0.392037957906723\n",
      "Epoch 1086, Loss: 0.6647631824016571, Final Batch Loss: 0.33553948998451233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1087, Loss: 0.6348538994789124, Final Batch Loss: 0.3115122318267822\n",
      "Epoch 1088, Loss: 0.6583690941333771, Final Batch Loss: 0.326369047164917\n",
      "Epoch 1089, Loss: 0.6295858025550842, Final Batch Loss: 0.3304881751537323\n",
      "Epoch 1090, Loss: 0.6579726040363312, Final Batch Loss: 0.28796306252479553\n",
      "Epoch 1091, Loss: 0.6290731430053711, Final Batch Loss: 0.30915990471839905\n",
      "Epoch 1092, Loss: 0.6023750603199005, Final Batch Loss: 0.2896338999271393\n",
      "Epoch 1093, Loss: 0.6981474459171295, Final Batch Loss: 0.3865053355693817\n",
      "Epoch 1094, Loss: 0.6518333852291107, Final Batch Loss: 0.3794398605823517\n",
      "Epoch 1095, Loss: 0.6305162906646729, Final Batch Loss: 0.3128609359264374\n",
      "Epoch 1096, Loss: 0.6543874442577362, Final Batch Loss: 0.342817485332489\n",
      "Epoch 1097, Loss: 0.6168364882469177, Final Batch Loss: 0.3099265396595001\n",
      "Epoch 1098, Loss: 0.6591495871543884, Final Batch Loss: 0.3287143111228943\n",
      "Epoch 1099, Loss: 0.6092624366283417, Final Batch Loss: 0.29815319180488586\n",
      "Epoch 1100, Loss: 0.603217214345932, Final Batch Loss: 0.2856053411960602\n",
      "Epoch 1101, Loss: 0.6358284950256348, Final Batch Loss: 0.3214738965034485\n",
      "Epoch 1102, Loss: 0.6321342885494232, Final Batch Loss: 0.2633199393749237\n",
      "Epoch 1103, Loss: 0.6115395724773407, Final Batch Loss: 0.3010953962802887\n",
      "Epoch 1104, Loss: 0.6732385456562042, Final Batch Loss: 0.3084169030189514\n",
      "Epoch 1105, Loss: 0.625163733959198, Final Batch Loss: 0.3009110391139984\n",
      "Epoch 1106, Loss: 0.6695558130741119, Final Batch Loss: 0.28241419792175293\n",
      "Epoch 1107, Loss: 0.6782627403736115, Final Batch Loss: 0.36830028891563416\n",
      "Epoch 1108, Loss: 0.6956900060176849, Final Batch Loss: 0.3424983024597168\n",
      "Epoch 1109, Loss: 0.6400138139724731, Final Batch Loss: 0.3233879506587982\n",
      "Epoch 1110, Loss: 0.619541734457016, Final Batch Loss: 0.31442153453826904\n",
      "Epoch 1111, Loss: 0.6272405385971069, Final Batch Loss: 0.33033478260040283\n",
      "Epoch 1112, Loss: 0.6220051348209381, Final Batch Loss: 0.27933329343795776\n",
      "Epoch 1113, Loss: 0.6574373543262482, Final Batch Loss: 0.3203152120113373\n",
      "Epoch 1114, Loss: 0.6301721036434174, Final Batch Loss: 0.34396886825561523\n",
      "Epoch 1115, Loss: 0.6537726819515228, Final Batch Loss: 0.32628655433654785\n",
      "Epoch 1116, Loss: 0.6710235774517059, Final Batch Loss: 0.3641256093978882\n",
      "Epoch 1117, Loss: 0.6611071228981018, Final Batch Loss: 0.33316799998283386\n",
      "Epoch 1118, Loss: 0.6962569952011108, Final Batch Loss: 0.35063138604164124\n",
      "Epoch 1119, Loss: 0.6227640211582184, Final Batch Loss: 0.29829171299934387\n",
      "Epoch 1120, Loss: 0.6284196376800537, Final Batch Loss: 0.328753799200058\n",
      "Epoch 1121, Loss: 0.6224784851074219, Final Batch Loss: 0.30426856875419617\n",
      "Epoch 1122, Loss: 0.6511657536029816, Final Batch Loss: 0.28187400102615356\n",
      "Epoch 1123, Loss: 0.6701591908931732, Final Batch Loss: 0.35032951831817627\n",
      "Epoch 1124, Loss: 0.6165952384471893, Final Batch Loss: 0.27696356177330017\n",
      "Epoch 1125, Loss: 0.7147067189216614, Final Batch Loss: 0.377376526594162\n",
      "Epoch 1126, Loss: 0.6790591478347778, Final Batch Loss: 0.38860034942626953\n",
      "Epoch 1127, Loss: 0.6243525147438049, Final Batch Loss: 0.2979729473590851\n",
      "Epoch 1128, Loss: 0.654783695936203, Final Batch Loss: 0.3693482279777527\n",
      "Epoch 1129, Loss: 0.6498372554779053, Final Batch Loss: 0.30135613679885864\n",
      "Epoch 1130, Loss: 0.6518483459949493, Final Batch Loss: 0.3374217450618744\n",
      "Epoch 1131, Loss: 0.6932345926761627, Final Batch Loss: 0.36129623651504517\n",
      "Epoch 1132, Loss: 0.6530327200889587, Final Batch Loss: 0.359573632478714\n",
      "Epoch 1133, Loss: 0.6482818722724915, Final Batch Loss: 0.2805589437484741\n",
      "Epoch 1134, Loss: 0.6405883133411407, Final Batch Loss: 0.2817628085613251\n",
      "Epoch 1135, Loss: 0.6581811010837555, Final Batch Loss: 0.3418700695037842\n",
      "Epoch 1136, Loss: 0.6093532741069794, Final Batch Loss: 0.3028925955295563\n",
      "Epoch 1137, Loss: 0.6049548089504242, Final Batch Loss: 0.28851452469825745\n",
      "Epoch 1138, Loss: 0.6663784086704254, Final Batch Loss: 0.3066309690475464\n",
      "Epoch 1139, Loss: 0.6348219811916351, Final Batch Loss: 0.3359769880771637\n",
      "Epoch 1140, Loss: 0.6522195935249329, Final Batch Loss: 0.3056425452232361\n",
      "Epoch 1141, Loss: 0.6562018096446991, Final Batch Loss: 0.3051481544971466\n",
      "Epoch 1142, Loss: 0.6624032855033875, Final Batch Loss: 0.29316946864128113\n",
      "Epoch 1143, Loss: 0.6301535964012146, Final Batch Loss: 0.3428157866001129\n",
      "Epoch 1144, Loss: 0.6686214506626129, Final Batch Loss: 0.3062436878681183\n",
      "Epoch 1145, Loss: 0.6278063058853149, Final Batch Loss: 0.3297586739063263\n",
      "Epoch 1146, Loss: 0.6191225051879883, Final Batch Loss: 0.3276311457157135\n",
      "Epoch 1147, Loss: 0.6251476109027863, Final Batch Loss: 0.30192840099334717\n",
      "Epoch 1148, Loss: 0.6196055710315704, Final Batch Loss: 0.2919721007347107\n",
      "Epoch 1149, Loss: 0.6302133798599243, Final Batch Loss: 0.3009961247444153\n",
      "Epoch 1150, Loss: 0.6142041087150574, Final Batch Loss: 0.27703744173049927\n",
      "Epoch 1151, Loss: 0.6700472831726074, Final Batch Loss: 0.314973920583725\n",
      "Epoch 1152, Loss: 0.6406652331352234, Final Batch Loss: 0.31597211956977844\n",
      "Epoch 1153, Loss: 0.6343011856079102, Final Batch Loss: 0.31236252188682556\n",
      "Epoch 1154, Loss: 0.6415091753005981, Final Batch Loss: 0.31848448514938354\n",
      "Epoch 1155, Loss: 0.5871120095252991, Final Batch Loss: 0.3129288852214813\n",
      "Epoch 1156, Loss: 0.6450929939746857, Final Batch Loss: 0.288449764251709\n",
      "Epoch 1157, Loss: 0.6570880115032196, Final Batch Loss: 0.3512190878391266\n",
      "Epoch 1158, Loss: 0.6726087331771851, Final Batch Loss: 0.33219486474990845\n",
      "Epoch 1159, Loss: 0.6381887197494507, Final Batch Loss: 0.29334360361099243\n",
      "Epoch 1160, Loss: 0.6438165307044983, Final Batch Loss: 0.35560253262519836\n",
      "Epoch 1161, Loss: 0.654489666223526, Final Batch Loss: 0.38495391607284546\n",
      "Epoch 1162, Loss: 0.5805491507053375, Final Batch Loss: 0.2508860230445862\n",
      "Epoch 1163, Loss: 0.669012039899826, Final Batch Loss: 0.3348751962184906\n",
      "Epoch 1164, Loss: 0.6486355066299438, Final Batch Loss: 0.3049370348453522\n",
      "Epoch 1165, Loss: 0.6250376105308533, Final Batch Loss: 0.29833391308784485\n",
      "Epoch 1166, Loss: 0.6231480240821838, Final Batch Loss: 0.2864898145198822\n",
      "Epoch 1167, Loss: 0.6386314928531647, Final Batch Loss: 0.31442758440971375\n",
      "Epoch 1168, Loss: 0.6283506751060486, Final Batch Loss: 0.31923091411590576\n",
      "Epoch 1169, Loss: 0.6007320880889893, Final Batch Loss: 0.3375420868396759\n",
      "Epoch 1170, Loss: 0.6101883947849274, Final Batch Loss: 0.30578646063804626\n",
      "Epoch 1171, Loss: 0.6751129031181335, Final Batch Loss: 0.3307896554470062\n",
      "Epoch 1172, Loss: 0.5911023914813995, Final Batch Loss: 0.2748090922832489\n",
      "Epoch 1173, Loss: 0.6024028956890106, Final Batch Loss: 0.2923326790332794\n",
      "Epoch 1174, Loss: 0.7535774111747742, Final Batch Loss: 0.39949703216552734\n",
      "Epoch 1175, Loss: 0.6506249904632568, Final Batch Loss: 0.322468638420105\n",
      "Epoch 1176, Loss: 0.6370761394500732, Final Batch Loss: 0.3000209629535675\n",
      "Epoch 1177, Loss: 0.6381886601448059, Final Batch Loss: 0.3249477744102478\n",
      "Epoch 1178, Loss: 0.6356159448623657, Final Batch Loss: 0.3607366383075714\n",
      "Epoch 1179, Loss: 0.661624014377594, Final Batch Loss: 0.35888180136680603\n",
      "Epoch 1180, Loss: 0.6644017398357391, Final Batch Loss: 0.375179260969162\n",
      "Epoch 1181, Loss: 0.6053906977176666, Final Batch Loss: 0.30009040236473083\n",
      "Epoch 1182, Loss: 0.6337506771087646, Final Batch Loss: 0.3246355652809143\n",
      "Epoch 1183, Loss: 0.6808647215366364, Final Batch Loss: 0.31761035323143005\n",
      "Epoch 1184, Loss: 0.6052349507808685, Final Batch Loss: 0.27968695759773254\n",
      "Epoch 1185, Loss: 0.6175131499767303, Final Batch Loss: 0.3155607283115387\n",
      "Epoch 1186, Loss: 0.6284604072570801, Final Batch Loss: 0.32547125220298767\n",
      "Epoch 1187, Loss: 0.6283033490180969, Final Batch Loss: 0.31315287947654724\n",
      "Epoch 1188, Loss: 0.6439689099788666, Final Batch Loss: 0.3135063946247101\n",
      "Epoch 1189, Loss: 0.6118481159210205, Final Batch Loss: 0.29143595695495605\n",
      "Epoch 1190, Loss: 0.6565941274166107, Final Batch Loss: 0.31637364625930786\n",
      "Epoch 1191, Loss: 0.7060030400753021, Final Batch Loss: 0.2951105535030365\n",
      "Epoch 1192, Loss: 0.6378795206546783, Final Batch Loss: 0.2890240252017975\n",
      "Epoch 1193, Loss: 0.6370299756526947, Final Batch Loss: 0.31226903200149536\n",
      "Epoch 1194, Loss: 0.6309324502944946, Final Batch Loss: 0.31973904371261597\n",
      "Epoch 1195, Loss: 0.6102960407733917, Final Batch Loss: 0.26332905888557434\n",
      "Epoch 1196, Loss: 0.6219467520713806, Final Batch Loss: 0.2971992492675781\n",
      "Epoch 1197, Loss: 0.6603964865207672, Final Batch Loss: 0.32684382796287537\n",
      "Epoch 1198, Loss: 0.6319842636585236, Final Batch Loss: 0.27889665961265564\n",
      "Epoch 1199, Loss: 0.663611650466919, Final Batch Loss: 0.3224427402019501\n",
      "Epoch 1200, Loss: 0.6315290629863739, Final Batch Loss: 0.3276216983795166\n",
      "Epoch 1201, Loss: 0.6341251730918884, Final Batch Loss: 0.31376683712005615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1202, Loss: 0.592680424451828, Final Batch Loss: 0.28229257464408875\n",
      "Epoch 1203, Loss: 0.6521769165992737, Final Batch Loss: 0.3143467903137207\n",
      "Epoch 1204, Loss: 0.6280007064342499, Final Batch Loss: 0.3264809846878052\n",
      "Epoch 1205, Loss: 0.6690889298915863, Final Batch Loss: 0.3157460689544678\n",
      "Epoch 1206, Loss: 0.6207642555236816, Final Batch Loss: 0.31196582317352295\n",
      "Epoch 1207, Loss: 0.605883002281189, Final Batch Loss: 0.3225035071372986\n",
      "Epoch 1208, Loss: 0.6637058854103088, Final Batch Loss: 0.38449394702911377\n",
      "Epoch 1209, Loss: 0.648880124092102, Final Batch Loss: 0.3081113398075104\n",
      "Epoch 1210, Loss: 0.6778276264667511, Final Batch Loss: 0.3086787164211273\n",
      "Epoch 1211, Loss: 0.5917911529541016, Final Batch Loss: 0.26703062653541565\n",
      "Epoch 1212, Loss: 0.698961615562439, Final Batch Loss: 0.3541681468486786\n",
      "Epoch 1213, Loss: 0.6381109952926636, Final Batch Loss: 0.3464815318584442\n",
      "Epoch 1214, Loss: 0.6590149998664856, Final Batch Loss: 0.2857514023780823\n",
      "Epoch 1215, Loss: 0.5969224572181702, Final Batch Loss: 0.2770327925682068\n",
      "Epoch 1216, Loss: 0.6516401171684265, Final Batch Loss: 0.34164050221443176\n",
      "Epoch 1217, Loss: 0.6290920674800873, Final Batch Loss: 0.3255249857902527\n",
      "Epoch 1218, Loss: 0.6418609917163849, Final Batch Loss: 0.29329243302345276\n",
      "Epoch 1219, Loss: 0.6180922091007233, Final Batch Loss: 0.2975839674472809\n",
      "Epoch 1220, Loss: 0.6494889259338379, Final Batch Loss: 0.3472723662853241\n",
      "Epoch 1221, Loss: 0.6328487992286682, Final Batch Loss: 0.34908923506736755\n",
      "Epoch 1222, Loss: 0.6185948252677917, Final Batch Loss: 0.3090009093284607\n",
      "Epoch 1223, Loss: 0.619658887386322, Final Batch Loss: 0.32709649205207825\n",
      "Epoch 1224, Loss: 0.6741442978382111, Final Batch Loss: 0.3688834309577942\n",
      "Epoch 1225, Loss: 0.6137155294418335, Final Batch Loss: 0.32582947611808777\n",
      "Epoch 1226, Loss: 0.6160936653614044, Final Batch Loss: 0.3035818934440613\n",
      "Epoch 1227, Loss: 0.6049941182136536, Final Batch Loss: 0.3117121756076813\n",
      "Epoch 1228, Loss: 0.6129368543624878, Final Batch Loss: 0.31512656807899475\n",
      "Epoch 1229, Loss: 0.628358781337738, Final Batch Loss: 0.28832733631134033\n",
      "Epoch 1230, Loss: 0.6396097242832184, Final Batch Loss: 0.2907339632511139\n",
      "Epoch 1231, Loss: 0.6176247596740723, Final Batch Loss: 0.3093836009502411\n",
      "Epoch 1232, Loss: 0.5881180465221405, Final Batch Loss: 0.3257467746734619\n",
      "Epoch 1233, Loss: 0.6027736961841583, Final Batch Loss: 0.3463817536830902\n",
      "Epoch 1234, Loss: 0.6418418884277344, Final Batch Loss: 0.33402204513549805\n",
      "Epoch 1235, Loss: 0.6262200176715851, Final Batch Loss: 0.3030393123626709\n",
      "Epoch 1236, Loss: 0.6106769144535065, Final Batch Loss: 0.3088931739330292\n",
      "Epoch 1237, Loss: 0.6316264271736145, Final Batch Loss: 0.2671982944011688\n",
      "Epoch 1238, Loss: 0.7336999177932739, Final Batch Loss: 0.33109351992607117\n",
      "Epoch 1239, Loss: 0.7178344130516052, Final Batch Loss: 0.34309396147727966\n",
      "Epoch 1240, Loss: 0.6228877305984497, Final Batch Loss: 0.29911884665489197\n",
      "Epoch 1241, Loss: 0.6514163315296173, Final Batch Loss: 0.306382417678833\n",
      "Epoch 1242, Loss: 0.5918486416339874, Final Batch Loss: 0.2681731879711151\n",
      "Epoch 1243, Loss: 0.6312889754772186, Final Batch Loss: 0.2856821119785309\n",
      "Epoch 1244, Loss: 0.6243231892585754, Final Batch Loss: 0.2657884955406189\n",
      "Epoch 1245, Loss: 0.6178746819496155, Final Batch Loss: 0.3057754635810852\n",
      "Epoch 1246, Loss: 0.6338074505329132, Final Batch Loss: 0.3063795268535614\n",
      "Epoch 1247, Loss: 0.71833136677742, Final Batch Loss: 0.4076995253562927\n",
      "Epoch 1248, Loss: 0.5950576961040497, Final Batch Loss: 0.3003111183643341\n",
      "Epoch 1249, Loss: 0.6180438995361328, Final Batch Loss: 0.3092283606529236\n",
      "Epoch 1250, Loss: 0.6406739950180054, Final Batch Loss: 0.2778611183166504\n",
      "Epoch 1251, Loss: 0.5922500193119049, Final Batch Loss: 0.30176568031311035\n",
      "Epoch 1252, Loss: 0.621276319026947, Final Batch Loss: 0.3101067543029785\n",
      "Epoch 1253, Loss: 0.6549575328826904, Final Batch Loss: 0.30957263708114624\n",
      "Epoch 1254, Loss: 0.6726985573768616, Final Batch Loss: 0.2972397208213806\n",
      "Epoch 1255, Loss: 0.6163819134235382, Final Batch Loss: 0.2865629494190216\n",
      "Epoch 1256, Loss: 0.6689545214176178, Final Batch Loss: 0.3298489451408386\n",
      "Epoch 1257, Loss: 0.6724891066551208, Final Batch Loss: 0.299998939037323\n",
      "Epoch 1258, Loss: 0.6234880983829498, Final Batch Loss: 0.30539244413375854\n",
      "Epoch 1259, Loss: 0.590319812297821, Final Batch Loss: 0.3119654953479767\n",
      "Epoch 1260, Loss: 0.60302734375, Final Batch Loss: 0.2747713327407837\n",
      "Epoch 1261, Loss: 0.6078442633152008, Final Batch Loss: 0.3088403046131134\n",
      "Epoch 1262, Loss: 0.6411789953708649, Final Batch Loss: 0.3144146502017975\n",
      "Epoch 1263, Loss: 0.6229825019836426, Final Batch Loss: 0.3141435980796814\n",
      "Epoch 1264, Loss: 0.6324184536933899, Final Batch Loss: 0.3300926387310028\n",
      "Epoch 1265, Loss: 0.6189200580120087, Final Batch Loss: 0.3021214008331299\n",
      "Epoch 1266, Loss: 0.5804719626903534, Final Batch Loss: 0.2851004898548126\n",
      "Epoch 1267, Loss: 0.5998398661613464, Final Batch Loss: 0.3197614550590515\n",
      "Epoch 1268, Loss: 0.6122548878192902, Final Batch Loss: 0.3095550835132599\n",
      "Epoch 1269, Loss: 0.6135967969894409, Final Batch Loss: 0.3261156678199768\n",
      "Epoch 1270, Loss: 0.5951521098613739, Final Batch Loss: 0.27107489109039307\n",
      "Epoch 1271, Loss: 0.6471661329269409, Final Batch Loss: 0.31422147154808044\n",
      "Epoch 1272, Loss: 0.6929642856121063, Final Batch Loss: 0.36962413787841797\n",
      "Epoch 1273, Loss: 0.5932382941246033, Final Batch Loss: 0.33585575222969055\n",
      "Epoch 1274, Loss: 0.6236873865127563, Final Batch Loss: 0.2644709050655365\n",
      "Epoch 1275, Loss: 0.6589236557483673, Final Batch Loss: 0.30933499336242676\n",
      "Epoch 1276, Loss: 0.6128025949001312, Final Batch Loss: 0.30357882380485535\n",
      "Epoch 1277, Loss: 0.6472671031951904, Final Batch Loss: 0.3054700493812561\n",
      "Epoch 1278, Loss: 0.6087527275085449, Final Batch Loss: 0.2747417986392975\n",
      "Epoch 1279, Loss: 0.6960223615169525, Final Batch Loss: 0.3320879340171814\n",
      "Epoch 1280, Loss: 0.621338278055191, Final Batch Loss: 0.2794005572795868\n",
      "Epoch 1281, Loss: 0.6222690343856812, Final Batch Loss: 0.31145724654197693\n",
      "Epoch 1282, Loss: 0.6343394219875336, Final Batch Loss: 0.35806983709335327\n",
      "Epoch 1283, Loss: 0.6398963034152985, Final Batch Loss: 0.3045900762081146\n",
      "Epoch 1284, Loss: 0.6080492436885834, Final Batch Loss: 0.30464938282966614\n",
      "Epoch 1285, Loss: 0.5965167880058289, Final Batch Loss: 0.30228495597839355\n",
      "Epoch 1286, Loss: 0.6612048447132111, Final Batch Loss: 0.3601487874984741\n",
      "Epoch 1287, Loss: 0.6714776456356049, Final Batch Loss: 0.32252344489097595\n",
      "Epoch 1288, Loss: 0.6028812825679779, Final Batch Loss: 0.2812659740447998\n",
      "Epoch 1289, Loss: 0.6857894062995911, Final Batch Loss: 0.36893942952156067\n",
      "Epoch 1290, Loss: 0.6059184074401855, Final Batch Loss: 0.3410522937774658\n",
      "Epoch 1291, Loss: 0.6024426519870758, Final Batch Loss: 0.28720390796661377\n",
      "Epoch 1292, Loss: 0.6436408162117004, Final Batch Loss: 0.30646905303001404\n",
      "Epoch 1293, Loss: 0.6170032024383545, Final Batch Loss: 0.28370100259780884\n",
      "Epoch 1294, Loss: 0.5977279841899872, Final Batch Loss: 0.3094860017299652\n",
      "Epoch 1295, Loss: 0.5984990000724792, Final Batch Loss: 0.33298736810684204\n",
      "Epoch 1296, Loss: 0.6113615036010742, Final Batch Loss: 0.31538793444633484\n",
      "Epoch 1297, Loss: 0.7134884893894196, Final Batch Loss: 0.3735530972480774\n",
      "Epoch 1298, Loss: 0.6304757297039032, Final Batch Loss: 0.3453980088233948\n",
      "Epoch 1299, Loss: 0.63185915350914, Final Batch Loss: 0.3320848047733307\n",
      "Epoch 1300, Loss: 0.649025559425354, Final Batch Loss: 0.32597312331199646\n",
      "Epoch 1301, Loss: 0.6548814475536346, Final Batch Loss: 0.32806867361068726\n",
      "Epoch 1302, Loss: 0.6152979731559753, Final Batch Loss: 0.29400157928466797\n",
      "Epoch 1303, Loss: 0.6711635291576385, Final Batch Loss: 0.29401150345802307\n",
      "Epoch 1304, Loss: 0.6075764894485474, Final Batch Loss: 0.303153395652771\n",
      "Epoch 1305, Loss: 0.619038999080658, Final Batch Loss: 0.2890976369380951\n",
      "Epoch 1306, Loss: 0.5929024517536163, Final Batch Loss: 0.257343053817749\n",
      "Epoch 1307, Loss: 0.6149707138538361, Final Batch Loss: 0.26864758133888245\n",
      "Epoch 1308, Loss: 0.6279658377170563, Final Batch Loss: 0.306903213262558\n",
      "Epoch 1309, Loss: 0.5999593734741211, Final Batch Loss: 0.30926910042762756\n",
      "Epoch 1310, Loss: 0.5815152227878571, Final Batch Loss: 0.28757017850875854\n",
      "Epoch 1311, Loss: 0.6793542802333832, Final Batch Loss: 0.29761484265327454\n",
      "Epoch 1312, Loss: 0.6633026003837585, Final Batch Loss: 0.321277916431427\n",
      "Epoch 1313, Loss: 0.6348272860050201, Final Batch Loss: 0.3166504502296448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1314, Loss: 0.6220013797283173, Final Batch Loss: 0.33042049407958984\n",
      "Epoch 1315, Loss: 0.5918013155460358, Final Batch Loss: 0.30167362093925476\n",
      "Epoch 1316, Loss: 0.6530163884162903, Final Batch Loss: 0.3014693260192871\n",
      "Epoch 1317, Loss: 0.658118486404419, Final Batch Loss: 0.2627376914024353\n",
      "Epoch 1318, Loss: 0.6264335513114929, Final Batch Loss: 0.28985846042633057\n",
      "Epoch 1319, Loss: 0.6304240524768829, Final Batch Loss: 0.30368638038635254\n",
      "Epoch 1320, Loss: 0.6441396772861481, Final Batch Loss: 0.33468905091285706\n",
      "Epoch 1321, Loss: 0.6138401031494141, Final Batch Loss: 0.31977954506874084\n",
      "Epoch 1322, Loss: 0.6470356285572052, Final Batch Loss: 0.3262180984020233\n",
      "Epoch 1323, Loss: 0.6039383113384247, Final Batch Loss: 0.28012973070144653\n",
      "Epoch 1324, Loss: 0.6201312839984894, Final Batch Loss: 0.30573028326034546\n",
      "Epoch 1325, Loss: 0.6222895979881287, Final Batch Loss: 0.3349344730377197\n",
      "Epoch 1326, Loss: 0.6294664442539215, Final Batch Loss: 0.34017834067344666\n",
      "Epoch 1327, Loss: 0.6320082247257233, Final Batch Loss: 0.30286288261413574\n",
      "Epoch 1328, Loss: 0.6183592677116394, Final Batch Loss: 0.28502416610717773\n",
      "Epoch 1329, Loss: 0.675521045923233, Final Batch Loss: 0.3317573368549347\n",
      "Epoch 1330, Loss: 0.6520097851753235, Final Batch Loss: 0.36440977454185486\n",
      "Epoch 1331, Loss: 0.619378387928009, Final Batch Loss: 0.2616155743598938\n",
      "Epoch 1332, Loss: 0.6202493011951447, Final Batch Loss: 0.28361976146698\n",
      "Epoch 1333, Loss: 0.6084929406642914, Final Batch Loss: 0.28168585896492004\n",
      "Epoch 1334, Loss: 0.6380454003810883, Final Batch Loss: 0.3268294334411621\n",
      "Epoch 1335, Loss: 0.6271916031837463, Final Batch Loss: 0.3229951858520508\n",
      "Epoch 1336, Loss: 0.5615547895431519, Final Batch Loss: 0.2499154806137085\n",
      "Epoch 1337, Loss: 0.5765330046415329, Final Batch Loss: 0.33415865898132324\n",
      "Epoch 1338, Loss: 0.6224206387996674, Final Batch Loss: 0.3280108571052551\n",
      "Epoch 1339, Loss: 0.633921355009079, Final Batch Loss: 0.2928497791290283\n",
      "Epoch 1340, Loss: 0.6252013146877289, Final Batch Loss: 0.30506616830825806\n",
      "Epoch 1341, Loss: 0.5994656682014465, Final Batch Loss: 0.2954498827457428\n",
      "Epoch 1342, Loss: 0.6210666596889496, Final Batch Loss: 0.27698734402656555\n",
      "Epoch 1343, Loss: 0.6009635627269745, Final Batch Loss: 0.2985691726207733\n",
      "Epoch 1344, Loss: 0.6171131730079651, Final Batch Loss: 0.31764763593673706\n",
      "Epoch 1345, Loss: 0.5946442484855652, Final Batch Loss: 0.30781289935112\n",
      "Epoch 1346, Loss: 0.6157412230968475, Final Batch Loss: 0.32935747504234314\n",
      "Epoch 1347, Loss: 0.6090893149375916, Final Batch Loss: 0.2919745445251465\n",
      "Epoch 1348, Loss: 0.64392951130867, Final Batch Loss: 0.3311665952205658\n",
      "Epoch 1349, Loss: 0.5926313698291779, Final Batch Loss: 0.3098289668560028\n",
      "Epoch 1350, Loss: 0.6150889098644257, Final Batch Loss: 0.3427983224391937\n",
      "Epoch 1351, Loss: 0.5981979370117188, Final Batch Loss: 0.3278806507587433\n",
      "Epoch 1352, Loss: 0.5877781510353088, Final Batch Loss: 0.2737036347389221\n",
      "Epoch 1353, Loss: 0.6208314597606659, Final Batch Loss: 0.3068746030330658\n",
      "Epoch 1354, Loss: 0.5936570763587952, Final Batch Loss: 0.3061034679412842\n",
      "Epoch 1355, Loss: 0.6154695749282837, Final Batch Loss: 0.2730065882205963\n",
      "Epoch 1356, Loss: 0.6344479024410248, Final Batch Loss: 0.3538729250431061\n",
      "Epoch 1357, Loss: 0.677537351846695, Final Batch Loss: 0.3302863538265228\n",
      "Epoch 1358, Loss: 0.6040522754192352, Final Batch Loss: 0.25993114709854126\n",
      "Epoch 1359, Loss: 0.6189282834529877, Final Batch Loss: 0.27880093455314636\n",
      "Epoch 1360, Loss: 0.6575706303119659, Final Batch Loss: 0.33297568559646606\n",
      "Epoch 1361, Loss: 0.6133118271827698, Final Batch Loss: 0.289558082818985\n",
      "Epoch 1362, Loss: 0.637253075838089, Final Batch Loss: 0.32394787669181824\n",
      "Epoch 1363, Loss: 0.6290875673294067, Final Batch Loss: 0.32026979327201843\n",
      "Epoch 1364, Loss: 0.6083005666732788, Final Batch Loss: 0.27142563462257385\n",
      "Epoch 1365, Loss: 0.6009519100189209, Final Batch Loss: 0.29668867588043213\n",
      "Epoch 1366, Loss: 0.596619039773941, Final Batch Loss: 0.30133533477783203\n",
      "Epoch 1367, Loss: 0.5915320217609406, Final Batch Loss: 0.30218565464019775\n",
      "Epoch 1368, Loss: 0.6065577268600464, Final Batch Loss: 0.31594568490982056\n",
      "Epoch 1369, Loss: 0.5978432595729828, Final Batch Loss: 0.27480584383010864\n",
      "Epoch 1370, Loss: 0.5973004400730133, Final Batch Loss: 0.29401326179504395\n",
      "Epoch 1371, Loss: 0.5827976614236832, Final Batch Loss: 0.33770132064819336\n",
      "Epoch 1372, Loss: 0.6147656142711639, Final Batch Loss: 0.31052660942077637\n",
      "Epoch 1373, Loss: 0.6086486876010895, Final Batch Loss: 0.32329705357551575\n",
      "Epoch 1374, Loss: 0.5719745755195618, Final Batch Loss: 0.29100510478019714\n",
      "Epoch 1375, Loss: 0.6001061797142029, Final Batch Loss: 0.3069183826446533\n",
      "Epoch 1376, Loss: 0.6092044711112976, Final Batch Loss: 0.29185014963150024\n",
      "Epoch 1377, Loss: 0.6278132498264313, Final Batch Loss: 0.33513179421424866\n",
      "Epoch 1378, Loss: 0.6258329153060913, Final Batch Loss: 0.2870078682899475\n",
      "Epoch 1379, Loss: 0.5973748564720154, Final Batch Loss: 0.28619250655174255\n",
      "Epoch 1380, Loss: 0.5935791432857513, Final Batch Loss: 0.2592089772224426\n",
      "Epoch 1381, Loss: 0.6941234767436981, Final Batch Loss: 0.38776928186416626\n",
      "Epoch 1382, Loss: 0.5908565819263458, Final Batch Loss: 0.28043580055236816\n",
      "Epoch 1383, Loss: 0.6390651762485504, Final Batch Loss: 0.2900441288948059\n",
      "Epoch 1384, Loss: 0.5991775095462799, Final Batch Loss: 0.32576510310173035\n",
      "Epoch 1385, Loss: 0.61219522356987, Final Batch Loss: 0.3325573801994324\n",
      "Epoch 1386, Loss: 0.6059890389442444, Final Batch Loss: 0.3207300901412964\n",
      "Epoch 1387, Loss: 0.6316690742969513, Final Batch Loss: 0.3104929029941559\n",
      "Epoch 1388, Loss: 0.6646968722343445, Final Batch Loss: 0.37460920214653015\n",
      "Epoch 1389, Loss: 0.6495573222637177, Final Batch Loss: 0.29001691937446594\n",
      "Epoch 1390, Loss: 0.6073849499225616, Final Batch Loss: 0.3010454475879669\n",
      "Epoch 1391, Loss: 0.5858010649681091, Final Batch Loss: 0.31182000041007996\n",
      "Epoch 1392, Loss: 0.6109640896320343, Final Batch Loss: 0.3106951117515564\n",
      "Epoch 1393, Loss: 0.5832334756851196, Final Batch Loss: 0.30601966381073\n",
      "Epoch 1394, Loss: 0.5769549608230591, Final Batch Loss: 0.28046560287475586\n",
      "Epoch 1395, Loss: 0.5767031311988831, Final Batch Loss: 0.2717323899269104\n",
      "Epoch 1396, Loss: 0.6132518947124481, Final Batch Loss: 0.2903120219707489\n",
      "Epoch 1397, Loss: 0.5825211256742477, Final Batch Loss: 0.3383011519908905\n",
      "Epoch 1398, Loss: 0.6162750720977783, Final Batch Loss: 0.31887489557266235\n",
      "Epoch 1399, Loss: 0.5926505029201508, Final Batch Loss: 0.29586347937583923\n",
      "Epoch 1400, Loss: 0.6205207705497742, Final Batch Loss: 0.3157427906990051\n",
      "Epoch 1401, Loss: 0.6091298758983612, Final Batch Loss: 0.3066544830799103\n",
      "Epoch 1402, Loss: 0.5910635888576508, Final Batch Loss: 0.2735491096973419\n",
      "Epoch 1403, Loss: 0.6040117740631104, Final Batch Loss: 0.31038108468055725\n",
      "Epoch 1404, Loss: 0.7069381475448608, Final Batch Loss: 0.31679028272628784\n",
      "Epoch 1405, Loss: 0.6226090788841248, Final Batch Loss: 0.3331944942474365\n",
      "Epoch 1406, Loss: 0.618599146604538, Final Batch Loss: 0.3388093411922455\n",
      "Epoch 1407, Loss: 0.6199518144130707, Final Batch Loss: 0.2975628077983856\n",
      "Epoch 1408, Loss: 0.5700254142284393, Final Batch Loss: 0.2838577926158905\n",
      "Epoch 1409, Loss: 0.6609407961368561, Final Batch Loss: 0.3838057816028595\n",
      "Epoch 1410, Loss: 0.6561100482940674, Final Batch Loss: 0.3407188355922699\n",
      "Epoch 1411, Loss: 0.5911478996276855, Final Batch Loss: 0.27348843216896057\n",
      "Epoch 1412, Loss: 0.5852606892585754, Final Batch Loss: 0.29680272936820984\n",
      "Epoch 1413, Loss: 0.5926722884178162, Final Batch Loss: 0.25408732891082764\n",
      "Epoch 1414, Loss: 0.584241509437561, Final Batch Loss: 0.29212337732315063\n",
      "Epoch 1415, Loss: 0.590800017118454, Final Batch Loss: 0.25154736638069153\n",
      "Epoch 1416, Loss: 0.5975208878517151, Final Batch Loss: 0.27416902780532837\n",
      "Epoch 1417, Loss: 0.5699519217014313, Final Batch Loss: 0.2676888406276703\n",
      "Epoch 1418, Loss: 0.5773104429244995, Final Batch Loss: 0.2765340209007263\n",
      "Epoch 1419, Loss: 0.5840930342674255, Final Batch Loss: 0.27570840716362\n",
      "Epoch 1420, Loss: 0.5767761617898941, Final Batch Loss: 0.24279700219631195\n",
      "Epoch 1421, Loss: 0.6386323273181915, Final Batch Loss: 0.3270623981952667\n",
      "Epoch 1422, Loss: 0.6596166789531708, Final Batch Loss: 0.322400838136673\n",
      "Epoch 1423, Loss: 0.5735985636711121, Final Batch Loss: 0.2624199688434601\n",
      "Epoch 1424, Loss: 0.5945973992347717, Final Batch Loss: 0.26311758160591125\n",
      "Epoch 1425, Loss: 0.5688841640949249, Final Batch Loss: 0.30983415246009827\n",
      "Epoch 1426, Loss: 0.660085141658783, Final Batch Loss: 0.30807948112487793\n",
      "Epoch 1427, Loss: 0.632540762424469, Final Batch Loss: 0.3273340165615082\n",
      "Epoch 1428, Loss: 0.6309922933578491, Final Batch Loss: 0.3192744553089142\n",
      "Epoch 1429, Loss: 0.5635793805122375, Final Batch Loss: 0.2680715024471283\n",
      "Epoch 1430, Loss: 0.6096787750720978, Final Batch Loss: 0.29767364263534546\n",
      "Epoch 1431, Loss: 0.5658216178417206, Final Batch Loss: 0.2886181175708771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1432, Loss: 0.5900959670543671, Final Batch Loss: 0.3195103704929352\n",
      "Epoch 1433, Loss: 0.6340261399745941, Final Batch Loss: 0.2850273549556732\n",
      "Epoch 1434, Loss: 0.5973538458347321, Final Batch Loss: 0.30410727858543396\n",
      "Epoch 1435, Loss: 0.6324630379676819, Final Batch Loss: 0.3194074034690857\n",
      "Epoch 1436, Loss: 0.5837688744068146, Final Batch Loss: 0.2821061611175537\n",
      "Epoch 1437, Loss: 0.6394743323326111, Final Batch Loss: 0.2599359154701233\n",
      "Epoch 1438, Loss: 0.5823619961738586, Final Batch Loss: 0.27044257521629333\n",
      "Epoch 1439, Loss: 0.6070964932441711, Final Batch Loss: 0.3246012032032013\n",
      "Epoch 1440, Loss: 0.5757171511650085, Final Batch Loss: 0.2912958562374115\n",
      "Epoch 1441, Loss: 0.6475447118282318, Final Batch Loss: 0.3537295460700989\n",
      "Epoch 1442, Loss: 0.584572970867157, Final Batch Loss: 0.3098030388355255\n",
      "Epoch 1443, Loss: 0.5748603343963623, Final Batch Loss: 0.2722972631454468\n",
      "Epoch 1444, Loss: 0.5888644456863403, Final Batch Loss: 0.28069019317626953\n",
      "Epoch 1445, Loss: 0.6235342621803284, Final Batch Loss: 0.29439347982406616\n",
      "Epoch 1446, Loss: 0.63820980489254, Final Batch Loss: 0.3902532756328583\n",
      "Epoch 1447, Loss: 0.5727112293243408, Final Batch Loss: 0.2550193667411804\n",
      "Epoch 1448, Loss: 0.5986238718032837, Final Batch Loss: 0.27786916494369507\n",
      "Epoch 1449, Loss: 0.5944413542747498, Final Batch Loss: 0.3468059301376343\n",
      "Epoch 1450, Loss: 0.5858162641525269, Final Batch Loss: 0.29826435446739197\n",
      "Epoch 1451, Loss: 0.6328209340572357, Final Batch Loss: 0.3565326929092407\n",
      "Epoch 1452, Loss: 0.5588589012622833, Final Batch Loss: 0.2557689845561981\n",
      "Epoch 1453, Loss: 0.5907434821128845, Final Batch Loss: 0.2879928648471832\n",
      "Epoch 1454, Loss: 0.5568093210458755, Final Batch Loss: 0.24000541865825653\n",
      "Epoch 1455, Loss: 0.6492165327072144, Final Batch Loss: 0.285249799489975\n",
      "Epoch 1456, Loss: 0.5660721957683563, Final Batch Loss: 0.2662798762321472\n",
      "Epoch 1457, Loss: 0.5889618992805481, Final Batch Loss: 0.3275991380214691\n",
      "Epoch 1458, Loss: 0.6189268827438354, Final Batch Loss: 0.32361745834350586\n",
      "Epoch 1459, Loss: 0.5603452920913696, Final Batch Loss: 0.293940007686615\n",
      "Epoch 1460, Loss: 0.5869336128234863, Final Batch Loss: 0.31495702266693115\n",
      "Epoch 1461, Loss: 0.6087940335273743, Final Batch Loss: 0.3103349208831787\n",
      "Epoch 1462, Loss: 0.609869658946991, Final Batch Loss: 0.3466126620769501\n",
      "Epoch 1463, Loss: 0.5976068675518036, Final Batch Loss: 0.298160582780838\n",
      "Epoch 1464, Loss: 0.5701806545257568, Final Batch Loss: 0.25606977939605713\n",
      "Epoch 1465, Loss: 0.5917352437973022, Final Batch Loss: 0.2637125551700592\n",
      "Epoch 1466, Loss: 0.57896488904953, Final Batch Loss: 0.2836974263191223\n",
      "Epoch 1467, Loss: 0.6209137141704559, Final Batch Loss: 0.33418527245521545\n",
      "Epoch 1468, Loss: 0.5862542688846588, Final Batch Loss: 0.29346272349357605\n",
      "Epoch 1469, Loss: 0.5713333487510681, Final Batch Loss: 0.27520275115966797\n",
      "Epoch 1470, Loss: 0.5991872251033783, Final Batch Loss: 0.3286183476448059\n",
      "Epoch 1471, Loss: 0.5795886218547821, Final Batch Loss: 0.25719571113586426\n",
      "Epoch 1472, Loss: 0.6322501301765442, Final Batch Loss: 0.3225729167461395\n",
      "Epoch 1473, Loss: 0.6004869937896729, Final Batch Loss: 0.3342790901660919\n",
      "Epoch 1474, Loss: 0.5892670154571533, Final Batch Loss: 0.3022814989089966\n",
      "Epoch 1475, Loss: 0.5561420321464539, Final Batch Loss: 0.26938244700431824\n",
      "Epoch 1476, Loss: 0.5522379279136658, Final Batch Loss: 0.26623284816741943\n",
      "Epoch 1477, Loss: 0.6039023995399475, Final Batch Loss: 0.29002395272254944\n",
      "Epoch 1478, Loss: 0.5812419056892395, Final Batch Loss: 0.2936880588531494\n",
      "Epoch 1479, Loss: 0.5857529044151306, Final Batch Loss: 0.2945488393306732\n",
      "Epoch 1480, Loss: 0.576121062040329, Final Batch Loss: 0.31313174962997437\n",
      "Epoch 1481, Loss: 0.5581395030021667, Final Batch Loss: 0.27419689297676086\n",
      "Epoch 1482, Loss: 0.5845537185668945, Final Batch Loss: 0.3127070963382721\n",
      "Epoch 1483, Loss: 0.6961987614631653, Final Batch Loss: 0.40306416153907776\n",
      "Epoch 1484, Loss: 0.6221472918987274, Final Batch Loss: 0.27055105566978455\n",
      "Epoch 1485, Loss: 0.5790735185146332, Final Batch Loss: 0.27886727452278137\n",
      "Epoch 1486, Loss: 0.5859878063201904, Final Batch Loss: 0.27985671162605286\n",
      "Epoch 1487, Loss: 0.6006783545017242, Final Batch Loss: 0.2795357406139374\n",
      "Epoch 1488, Loss: 0.5802630484104156, Final Batch Loss: 0.27036887407302856\n",
      "Epoch 1489, Loss: 0.5635148584842682, Final Batch Loss: 0.3031257688999176\n",
      "Epoch 1490, Loss: 0.608978658914566, Final Batch Loss: 0.2685658633708954\n",
      "Epoch 1491, Loss: 0.5533209443092346, Final Batch Loss: 0.27825406193733215\n",
      "Epoch 1492, Loss: 0.5850200057029724, Final Batch Loss: 0.26679959893226624\n",
      "Epoch 1493, Loss: 0.6055837571620941, Final Batch Loss: 0.2726881504058838\n",
      "Epoch 1494, Loss: 0.5727050006389618, Final Batch Loss: 0.2787933349609375\n",
      "Epoch 1495, Loss: 0.6316868960857391, Final Batch Loss: 0.2865697741508484\n",
      "Epoch 1496, Loss: 0.545756459236145, Final Batch Loss: 0.2517428994178772\n",
      "Epoch 1497, Loss: 0.5652478337287903, Final Batch Loss: 0.2774752676486969\n",
      "Epoch 1498, Loss: 0.5754555463790894, Final Batch Loss: 0.30939722061157227\n",
      "Epoch 1499, Loss: 0.6712110042572021, Final Batch Loss: 0.3408978581428528\n",
      "Epoch 1500, Loss: 0.55827996134758, Final Batch Loss: 0.2943718731403351\n",
      "Epoch 1501, Loss: 0.5877752900123596, Final Batch Loss: 0.27517592906951904\n",
      "Epoch 1502, Loss: 0.5428198873996735, Final Batch Loss: 0.2717401683330536\n",
      "Epoch 1503, Loss: 0.5759587287902832, Final Batch Loss: 0.3084237277507782\n",
      "Epoch 1504, Loss: 0.5741875469684601, Final Batch Loss: 0.30319744348526\n",
      "Epoch 1505, Loss: 0.5676256865262985, Final Batch Loss: 0.32338225841522217\n",
      "Epoch 1506, Loss: 0.5780886709690094, Final Batch Loss: 0.28652194142341614\n",
      "Epoch 1507, Loss: 0.5692266225814819, Final Batch Loss: 0.3115502893924713\n",
      "Epoch 1508, Loss: 0.5450464487075806, Final Batch Loss: 0.26658451557159424\n",
      "Epoch 1509, Loss: 0.5610671639442444, Final Batch Loss: 0.2650718092918396\n",
      "Epoch 1510, Loss: 0.5853285491466522, Final Batch Loss: 0.30861857533454895\n",
      "Epoch 1511, Loss: 0.5777064263820648, Final Batch Loss: 0.2756454348564148\n",
      "Epoch 1512, Loss: 0.6107946038246155, Final Batch Loss: 0.2928004860877991\n",
      "Epoch 1513, Loss: 0.5790897905826569, Final Batch Loss: 0.31256920099258423\n",
      "Epoch 1514, Loss: 0.5975316762924194, Final Batch Loss: 0.32175663113594055\n",
      "Epoch 1515, Loss: 0.5623769164085388, Final Batch Loss: 0.27053096890449524\n",
      "Epoch 1516, Loss: 0.5488534718751907, Final Batch Loss: 0.3139124810695648\n",
      "Epoch 1517, Loss: 0.5602442026138306, Final Batch Loss: 0.28878653049468994\n",
      "Epoch 1518, Loss: 0.6383249461650848, Final Batch Loss: 0.3143273890018463\n",
      "Epoch 1519, Loss: 0.6054238379001617, Final Batch Loss: 0.3143397569656372\n",
      "Epoch 1520, Loss: 0.6728493869304657, Final Batch Loss: 0.3120865225791931\n",
      "Epoch 1521, Loss: 0.5707280337810516, Final Batch Loss: 0.25116896629333496\n",
      "Epoch 1522, Loss: 0.5640610754489899, Final Batch Loss: 0.2861664891242981\n",
      "Epoch 1523, Loss: 0.6123663783073425, Final Batch Loss: 0.28799086809158325\n",
      "Epoch 1524, Loss: 0.5620574653148651, Final Batch Loss: 0.27367913722991943\n",
      "Epoch 1525, Loss: 0.5598917603492737, Final Batch Loss: 0.26430585980415344\n",
      "Epoch 1526, Loss: 0.6072657406330109, Final Batch Loss: 0.3037642538547516\n",
      "Epoch 1527, Loss: 0.5575392097234726, Final Batch Loss: 0.3106026351451874\n",
      "Epoch 1528, Loss: 0.576159805059433, Final Batch Loss: 0.288181871175766\n",
      "Epoch 1529, Loss: 0.5630979537963867, Final Batch Loss: 0.25145187973976135\n",
      "Epoch 1530, Loss: 0.5577796250581741, Final Batch Loss: 0.2467205971479416\n",
      "Epoch 1531, Loss: 0.5847225487232208, Final Batch Loss: 0.2876189649105072\n",
      "Epoch 1532, Loss: 0.5359610319137573, Final Batch Loss: 0.2852036654949188\n",
      "Epoch 1533, Loss: 0.5740355253219604, Final Batch Loss: 0.3150548040866852\n",
      "Epoch 1534, Loss: 0.5885023176670074, Final Batch Loss: 0.29188036918640137\n",
      "Epoch 1535, Loss: 0.6182166635990143, Final Batch Loss: 0.3035465478897095\n",
      "Epoch 1536, Loss: 0.5860317051410675, Final Batch Loss: 0.33792737126350403\n",
      "Epoch 1537, Loss: 0.5549654215574265, Final Batch Loss: 0.30521494150161743\n",
      "Epoch 1538, Loss: 0.5513193011283875, Final Batch Loss: 0.2566662132740021\n",
      "Epoch 1539, Loss: 0.5301614105701447, Final Batch Loss: 0.25753363966941833\n",
      "Epoch 1540, Loss: 0.5526694357395172, Final Batch Loss: 0.2921889126300812\n",
      "Epoch 1541, Loss: 0.5692901909351349, Final Batch Loss: 0.2874338924884796\n",
      "Epoch 1542, Loss: 0.5863397121429443, Final Batch Loss: 0.31860703229904175\n",
      "Epoch 1543, Loss: 0.578055202960968, Final Batch Loss: 0.3154904544353485\n",
      "Epoch 1544, Loss: 0.5382236540317535, Final Batch Loss: 0.28237709403038025\n",
      "Epoch 1545, Loss: 0.5662007033824921, Final Batch Loss: 0.2767966687679291\n",
      "Epoch 1546, Loss: 0.6037745177745819, Final Batch Loss: 0.3444000780582428\n",
      "Epoch 1547, Loss: 0.5724399089813232, Final Batch Loss: 0.3041754961013794\n",
      "Epoch 1548, Loss: 0.5578231662511826, Final Batch Loss: 0.3089345693588257\n",
      "Epoch 1549, Loss: 0.5816168189048767, Final Batch Loss: 0.2754735052585602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1550, Loss: 0.5656390190124512, Final Batch Loss: 0.26519736647605896\n",
      "Epoch 1551, Loss: 0.5856438875198364, Final Batch Loss: 0.3015594780445099\n",
      "Epoch 1552, Loss: 0.5240154713392258, Final Batch Loss: 0.2407522350549698\n",
      "Epoch 1553, Loss: 0.604189544916153, Final Batch Loss: 0.3085954785346985\n",
      "Epoch 1554, Loss: 0.5892358422279358, Final Batch Loss: 0.3301379978656769\n",
      "Epoch 1555, Loss: 0.55417400598526, Final Batch Loss: 0.2612365782260895\n",
      "Epoch 1556, Loss: 0.6757630407810211, Final Batch Loss: 0.3569455146789551\n",
      "Epoch 1557, Loss: 0.5577771961688995, Final Batch Loss: 0.30126872658729553\n",
      "Epoch 1558, Loss: 0.5571879744529724, Final Batch Loss: 0.253396600484848\n",
      "Epoch 1559, Loss: 0.5283842980861664, Final Batch Loss: 0.25557371973991394\n",
      "Epoch 1560, Loss: 0.5239381790161133, Final Batch Loss: 0.24387970566749573\n",
      "Epoch 1561, Loss: 0.5621079802513123, Final Batch Loss: 0.26788651943206787\n",
      "Epoch 1562, Loss: 0.5412940233945847, Final Batch Loss: 0.2464170902967453\n",
      "Epoch 1563, Loss: 0.5171894431114197, Final Batch Loss: 0.25168049335479736\n",
      "Epoch 1564, Loss: 0.561444103717804, Final Batch Loss: 0.2600644826889038\n",
      "Epoch 1565, Loss: 0.5518912076950073, Final Batch Loss: 0.2696821689605713\n",
      "Epoch 1566, Loss: 0.5643659234046936, Final Batch Loss: 0.2684544324874878\n",
      "Epoch 1567, Loss: 0.587520569562912, Final Batch Loss: 0.3073180019855499\n",
      "Epoch 1568, Loss: 0.5650734305381775, Final Batch Loss: 0.29804256558418274\n",
      "Epoch 1569, Loss: 0.5588089525699615, Final Batch Loss: 0.2661731541156769\n",
      "Epoch 1570, Loss: 0.5960738956928253, Final Batch Loss: 0.26860806345939636\n",
      "Epoch 1571, Loss: 0.5545893609523773, Final Batch Loss: 0.29172536730766296\n",
      "Epoch 1572, Loss: 0.5853413194417953, Final Batch Loss: 0.34544283151626587\n",
      "Epoch 1573, Loss: 0.5366911590099335, Final Batch Loss: 0.25909602642059326\n",
      "Epoch 1574, Loss: 0.5356768369674683, Final Batch Loss: 0.2656375765800476\n",
      "Epoch 1575, Loss: 0.562736988067627, Final Batch Loss: 0.27873823046684265\n",
      "Epoch 1576, Loss: 0.5243983119726181, Final Batch Loss: 0.28991976380348206\n",
      "Epoch 1577, Loss: 0.5521986484527588, Final Batch Loss: 0.2985124886035919\n",
      "Epoch 1578, Loss: 0.5592575073242188, Final Batch Loss: 0.2920748293399811\n",
      "Epoch 1579, Loss: 0.5670439451932907, Final Batch Loss: 0.23049108684062958\n",
      "Epoch 1580, Loss: 0.5466889441013336, Final Batch Loss: 0.275260865688324\n",
      "Epoch 1581, Loss: 0.5759404003620148, Final Batch Loss: 0.26690927147865295\n",
      "Epoch 1582, Loss: 0.5574153065681458, Final Batch Loss: 0.27043038606643677\n",
      "Epoch 1583, Loss: 0.5400802493095398, Final Batch Loss: 0.3086630702018738\n",
      "Epoch 1584, Loss: 0.4878849387168884, Final Batch Loss: 0.22963234782218933\n",
      "Epoch 1585, Loss: 0.5679591298103333, Final Batch Loss: 0.2807918190956116\n",
      "Epoch 1586, Loss: 0.5723120272159576, Final Batch Loss: 0.2674872875213623\n",
      "Epoch 1587, Loss: 0.5656818151473999, Final Batch Loss: 0.27720704674720764\n",
      "Epoch 1588, Loss: 0.5983923673629761, Final Batch Loss: 0.3310771584510803\n",
      "Epoch 1589, Loss: 0.576275646686554, Final Batch Loss: 0.2840554714202881\n",
      "Epoch 1590, Loss: 0.5626484155654907, Final Batch Loss: 0.27880027890205383\n",
      "Epoch 1591, Loss: 0.5540446788072586, Final Batch Loss: 0.30827420949935913\n",
      "Epoch 1592, Loss: 0.5446316301822662, Final Batch Loss: 0.26700547337532043\n",
      "Epoch 1593, Loss: 0.5628074705600739, Final Batch Loss: 0.27231037616729736\n",
      "Epoch 1594, Loss: 0.5347089171409607, Final Batch Loss: 0.26137208938598633\n",
      "Epoch 1595, Loss: 0.6013393402099609, Final Batch Loss: 0.3266369104385376\n",
      "Epoch 1596, Loss: 0.5331075191497803, Final Batch Loss: 0.27858462929725647\n",
      "Epoch 1597, Loss: 0.5809982717037201, Final Batch Loss: 0.3382762670516968\n",
      "Epoch 1598, Loss: 0.5773115456104279, Final Batch Loss: 0.29597046971321106\n",
      "Epoch 1599, Loss: 0.5327804684638977, Final Batch Loss: 0.2651281952857971\n",
      "Epoch 1600, Loss: 0.5241926312446594, Final Batch Loss: 0.26561087369918823\n",
      "Epoch 1601, Loss: 0.53131003677845, Final Batch Loss: 0.2956847846508026\n",
      "Epoch 1602, Loss: 0.5457249581813812, Final Batch Loss: 0.2936324179172516\n",
      "Epoch 1603, Loss: 0.6491373479366302, Final Batch Loss: 0.27428683638572693\n",
      "Epoch 1604, Loss: 0.5723779201507568, Final Batch Loss: 0.28576675057411194\n",
      "Epoch 1605, Loss: 0.5476649701595306, Final Batch Loss: 0.259572833776474\n",
      "Epoch 1606, Loss: 0.5789407789707184, Final Batch Loss: 0.24029743671417236\n",
      "Epoch 1607, Loss: 0.5614655315876007, Final Batch Loss: 0.2885293662548065\n",
      "Epoch 1608, Loss: 0.5358813107013702, Final Batch Loss: 0.25180456042289734\n",
      "Epoch 1609, Loss: 0.5558425933122635, Final Batch Loss: 0.24435700476169586\n",
      "Epoch 1610, Loss: 0.5463329553604126, Final Batch Loss: 0.26428472995758057\n",
      "Epoch 1611, Loss: 0.5441275537014008, Final Batch Loss: 0.2607690095901489\n",
      "Epoch 1612, Loss: 0.6341150403022766, Final Batch Loss: 0.2998103201389313\n",
      "Epoch 1613, Loss: 0.5313883125782013, Final Batch Loss: 0.2659235894680023\n",
      "Epoch 1614, Loss: 0.555457353591919, Final Batch Loss: 0.25212374329566956\n",
      "Epoch 1615, Loss: 0.5368764400482178, Final Batch Loss: 0.25653478503227234\n",
      "Epoch 1616, Loss: 0.5918034315109253, Final Batch Loss: 0.33830565214157104\n",
      "Epoch 1617, Loss: 0.49768687784671783, Final Batch Loss: 0.2124244123697281\n",
      "Epoch 1618, Loss: 0.5059059262275696, Final Batch Loss: 0.23172920942306519\n",
      "Epoch 1619, Loss: 0.6052724719047546, Final Batch Loss: 0.29749831557273865\n",
      "Epoch 1620, Loss: 0.566695511341095, Final Batch Loss: 0.2981092929840088\n",
      "Epoch 1621, Loss: 0.6072109639644623, Final Batch Loss: 0.2960507571697235\n",
      "Epoch 1622, Loss: 0.5203261226415634, Final Batch Loss: 0.22874601185321808\n",
      "Epoch 1623, Loss: 0.527207687497139, Final Batch Loss: 0.23104847967624664\n",
      "Epoch 1624, Loss: 0.4957851469516754, Final Batch Loss: 0.22659683227539062\n",
      "Epoch 1625, Loss: 0.5339076966047287, Final Batch Loss: 0.2878830134868622\n",
      "Epoch 1626, Loss: 0.5766628980636597, Final Batch Loss: 0.3165910243988037\n",
      "Epoch 1627, Loss: 0.6185473501682281, Final Batch Loss: 0.3095893859863281\n",
      "Epoch 1628, Loss: 0.481382817029953, Final Batch Loss: 0.24016110599040985\n",
      "Epoch 1629, Loss: 0.5718851983547211, Final Batch Loss: 0.28567081689834595\n",
      "Epoch 1630, Loss: 0.5368682146072388, Final Batch Loss: 0.28566697239875793\n",
      "Epoch 1631, Loss: 0.5466678440570831, Final Batch Loss: 0.27274221181869507\n",
      "Epoch 1632, Loss: 0.5658358037471771, Final Batch Loss: 0.31735312938690186\n",
      "Epoch 1633, Loss: 0.545080155134201, Final Batch Loss: 0.268073707818985\n",
      "Epoch 1634, Loss: 0.5660827159881592, Final Batch Loss: 0.2630993723869324\n",
      "Epoch 1635, Loss: 0.5737633109092712, Final Batch Loss: 0.27827057242393494\n",
      "Epoch 1636, Loss: 0.5976590812206268, Final Batch Loss: 0.36612606048583984\n",
      "Epoch 1637, Loss: 0.5625383257865906, Final Batch Loss: 0.28139835596084595\n",
      "Epoch 1638, Loss: 0.5736357867717743, Final Batch Loss: 0.271981418132782\n",
      "Epoch 1639, Loss: 0.5330274403095245, Final Batch Loss: 0.2683262526988983\n",
      "Epoch 1640, Loss: 0.5378304421901703, Final Batch Loss: 0.2849128544330597\n",
      "Epoch 1641, Loss: 0.6009606420993805, Final Batch Loss: 0.2599301338195801\n",
      "Epoch 1642, Loss: 0.5184046626091003, Final Batch Loss: 0.26136747002601624\n",
      "Epoch 1643, Loss: 0.5671488046646118, Final Batch Loss: 0.2968652844429016\n",
      "Epoch 1644, Loss: 0.546027421951294, Final Batch Loss: 0.285828173160553\n",
      "Epoch 1645, Loss: 0.5497055053710938, Final Batch Loss: 0.30737975239753723\n",
      "Epoch 1646, Loss: 0.5252529978752136, Final Batch Loss: 0.2580067217350006\n",
      "Epoch 1647, Loss: 0.5261786878108978, Final Batch Loss: 0.2686043679714203\n",
      "Epoch 1648, Loss: 0.4978247880935669, Final Batch Loss: 0.23569703102111816\n",
      "Epoch 1649, Loss: 0.5391989052295685, Final Batch Loss: 0.2811235189437866\n",
      "Epoch 1650, Loss: 0.5516406893730164, Final Batch Loss: 0.2847965359687805\n",
      "Epoch 1651, Loss: 0.5829289555549622, Final Batch Loss: 0.29182106256484985\n",
      "Epoch 1652, Loss: 0.5268743634223938, Final Batch Loss: 0.2787613570690155\n",
      "Epoch 1653, Loss: 0.5582668334245682, Final Batch Loss: 0.23093722760677338\n",
      "Epoch 1654, Loss: 0.5547560751438141, Final Batch Loss: 0.2726578116416931\n",
      "Epoch 1655, Loss: 0.4998474270105362, Final Batch Loss: 0.25913748145103455\n",
      "Epoch 1656, Loss: 0.5042250454425812, Final Batch Loss: 0.2395084798336029\n",
      "Epoch 1657, Loss: 0.5379442274570465, Final Batch Loss: 0.25224751234054565\n",
      "Epoch 1658, Loss: 0.5082826912403107, Final Batch Loss: 0.2537626624107361\n",
      "Epoch 1659, Loss: 0.538575679063797, Final Batch Loss: 0.28162622451782227\n",
      "Epoch 1660, Loss: 0.5221978724002838, Final Batch Loss: 0.26052185893058777\n",
      "Epoch 1661, Loss: 0.541894257068634, Final Batch Loss: 0.26877561211586\n",
      "Epoch 1662, Loss: 0.5796642303466797, Final Batch Loss: 0.29587212204933167\n",
      "Epoch 1663, Loss: 0.5256776809692383, Final Batch Loss: 0.2689189910888672\n",
      "Epoch 1664, Loss: 0.5351623892784119, Final Batch Loss: 0.2584069073200226\n",
      "Epoch 1665, Loss: 0.524250403046608, Final Batch Loss: 0.3155786097049713\n",
      "Epoch 1666, Loss: 0.5125581324100494, Final Batch Loss: 0.27716147899627686\n",
      "Epoch 1667, Loss: 0.5162008106708527, Final Batch Loss: 0.24152076244354248\n",
      "Epoch 1668, Loss: 0.5225633233785629, Final Batch Loss: 0.2493109554052353\n",
      "Epoch 1669, Loss: 0.528693288564682, Final Batch Loss: 0.2558823227882385\n",
      "Epoch 1670, Loss: 0.5006217211484909, Final Batch Loss: 0.2659538686275482\n",
      "Epoch 1671, Loss: 0.531673476099968, Final Batch Loss: 0.2946711480617523\n",
      "Epoch 1672, Loss: 0.514380007982254, Final Batch Loss: 0.2568979859352112\n",
      "Epoch 1673, Loss: 0.5473849326372147, Final Batch Loss: 0.30098751187324524\n",
      "Epoch 1674, Loss: 0.5764165818691254, Final Batch Loss: 0.26386594772338867\n",
      "Epoch 1675, Loss: 0.5368685722351074, Final Batch Loss: 0.24450364708900452\n",
      "Epoch 1676, Loss: 0.5223144888877869, Final Batch Loss: 0.2521043121814728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1677, Loss: 0.505224883556366, Final Batch Loss: 0.2557576298713684\n",
      "Epoch 1678, Loss: 0.5346814841032028, Final Batch Loss: 0.2326444536447525\n",
      "Epoch 1679, Loss: 0.5055554509162903, Final Batch Loss: 0.2327299416065216\n",
      "Epoch 1680, Loss: 0.5899609625339508, Final Batch Loss: 0.30890515446662903\n",
      "Epoch 1681, Loss: 0.5606237053871155, Final Batch Loss: 0.3030625283718109\n",
      "Epoch 1682, Loss: 0.505341961979866, Final Batch Loss: 0.23498113453388214\n",
      "Epoch 1683, Loss: 0.5265625268220901, Final Batch Loss: 0.2869703471660614\n",
      "Epoch 1684, Loss: 0.5487750768661499, Final Batch Loss: 0.27961888909339905\n",
      "Epoch 1685, Loss: 0.5239098966121674, Final Batch Loss: 0.26642662286758423\n",
      "Epoch 1686, Loss: 0.4760894924402237, Final Batch Loss: 0.2114875465631485\n",
      "Epoch 1687, Loss: 0.5321552008390427, Final Batch Loss: 0.21684832870960236\n",
      "Epoch 1688, Loss: 0.5407984852790833, Final Batch Loss: 0.23389005661010742\n",
      "Epoch 1689, Loss: 0.5134532898664474, Final Batch Loss: 0.27633345127105713\n",
      "Epoch 1690, Loss: 0.4990382045507431, Final Batch Loss: 0.23932786285877228\n",
      "Epoch 1691, Loss: 0.5140334069728851, Final Batch Loss: 0.2683996260166168\n",
      "Epoch 1692, Loss: 0.5511954128742218, Final Batch Loss: 0.28182339668273926\n",
      "Epoch 1693, Loss: 0.5019332617521286, Final Batch Loss: 0.24432148039340973\n",
      "Epoch 1694, Loss: 0.507004052400589, Final Batch Loss: 0.2363925576210022\n",
      "Epoch 1695, Loss: 0.60700523853302, Final Batch Loss: 0.2985704839229584\n",
      "Epoch 1696, Loss: 0.48470474779605865, Final Batch Loss: 0.21933536231517792\n",
      "Epoch 1697, Loss: 0.584131121635437, Final Batch Loss: 0.2799023389816284\n",
      "Epoch 1698, Loss: 0.5010490566492081, Final Batch Loss: 0.2527581751346588\n",
      "Epoch 1699, Loss: 0.5731649100780487, Final Batch Loss: 0.3126516044139862\n",
      "Epoch 1700, Loss: 0.5856077373027802, Final Batch Loss: 0.30689141154289246\n",
      "Epoch 1701, Loss: 0.5585468113422394, Final Batch Loss: 0.2672751247882843\n",
      "Epoch 1702, Loss: 0.5376846790313721, Final Batch Loss: 0.25746628642082214\n",
      "Epoch 1703, Loss: 0.5115770101547241, Final Batch Loss: 0.2452702522277832\n",
      "Epoch 1704, Loss: 0.49842676520347595, Final Batch Loss: 0.24319350719451904\n",
      "Epoch 1705, Loss: 0.5375603437423706, Final Batch Loss: 0.2681367099285126\n",
      "Epoch 1706, Loss: 0.5165776610374451, Final Batch Loss: 0.26386821269989014\n",
      "Epoch 1707, Loss: 0.46564531326293945, Final Batch Loss: 0.23194758594036102\n",
      "Epoch 1708, Loss: 0.5189190208911896, Final Batch Loss: 0.23596125841140747\n",
      "Epoch 1709, Loss: 0.5520173013210297, Final Batch Loss: 0.29353469610214233\n",
      "Epoch 1710, Loss: 0.5218581557273865, Final Batch Loss: 0.289645791053772\n",
      "Epoch 1711, Loss: 0.5357390344142914, Final Batch Loss: 0.278938889503479\n",
      "Epoch 1712, Loss: 0.49506932497024536, Final Batch Loss: 0.23356682062149048\n",
      "Epoch 1713, Loss: 0.4780978262424469, Final Batch Loss: 0.21683728694915771\n",
      "Epoch 1714, Loss: 0.5582196414470673, Final Batch Loss: 0.21935811638832092\n",
      "Epoch 1715, Loss: 0.5059985965490341, Final Batch Loss: 0.2840825617313385\n",
      "Epoch 1716, Loss: 0.4889303594827652, Final Batch Loss: 0.2494571954011917\n",
      "Epoch 1717, Loss: 0.4912943094968796, Final Batch Loss: 0.22184740006923676\n",
      "Epoch 1718, Loss: 0.5719271898269653, Final Batch Loss: 0.2813323438167572\n",
      "Epoch 1719, Loss: 0.5117911100387573, Final Batch Loss: 0.2658745348453522\n",
      "Epoch 1720, Loss: 0.48797500133514404, Final Batch Loss: 0.2370644211769104\n",
      "Epoch 1721, Loss: 0.5069232881069183, Final Batch Loss: 0.23444387316703796\n",
      "Epoch 1722, Loss: 0.5359872430562973, Final Batch Loss: 0.28795957565307617\n",
      "Epoch 1723, Loss: 0.5018491595983505, Final Batch Loss: 0.26322975754737854\n",
      "Epoch 1724, Loss: 0.5482672899961472, Final Batch Loss: 0.3063746988773346\n",
      "Epoch 1725, Loss: 0.509178876876831, Final Batch Loss: 0.23135197162628174\n",
      "Epoch 1726, Loss: 0.5680215954780579, Final Batch Loss: 0.318276584148407\n",
      "Epoch 1727, Loss: 0.49894383549690247, Final Batch Loss: 0.2643337845802307\n",
      "Epoch 1728, Loss: 0.5113916993141174, Final Batch Loss: 0.2500971853733063\n",
      "Epoch 1729, Loss: 0.4888745844364166, Final Batch Loss: 0.2540037930011749\n",
      "Epoch 1730, Loss: 0.5481980890035629, Final Batch Loss: 0.3119259476661682\n",
      "Epoch 1731, Loss: 0.5033481568098068, Final Batch Loss: 0.25768205523490906\n",
      "Epoch 1732, Loss: 0.49797220528125763, Final Batch Loss: 0.28447288274765015\n",
      "Epoch 1733, Loss: 0.47900184988975525, Final Batch Loss: 0.22672945261001587\n",
      "Epoch 1734, Loss: 0.5101179778575897, Final Batch Loss: 0.23350301384925842\n",
      "Epoch 1735, Loss: 0.5442976653575897, Final Batch Loss: 0.31591612100601196\n",
      "Epoch 1736, Loss: 0.5235762000083923, Final Batch Loss: 0.28192466497421265\n",
      "Epoch 1737, Loss: 0.4916810393333435, Final Batch Loss: 0.2197265625\n",
      "Epoch 1738, Loss: 0.5036611109972, Final Batch Loss: 0.23556531965732574\n",
      "Epoch 1739, Loss: 0.4987149238586426, Final Batch Loss: 0.23933345079421997\n",
      "Epoch 1740, Loss: 0.49994391202926636, Final Batch Loss: 0.24955880641937256\n",
      "Epoch 1741, Loss: 0.48667049407958984, Final Batch Loss: 0.2231699526309967\n",
      "Epoch 1742, Loss: 0.5260958671569824, Final Batch Loss: 0.2408934235572815\n",
      "Epoch 1743, Loss: 0.5263859033584595, Final Batch Loss: 0.2505069971084595\n",
      "Epoch 1744, Loss: 0.4994959384202957, Final Batch Loss: 0.2591457962989807\n",
      "Epoch 1745, Loss: 0.5731734335422516, Final Batch Loss: 0.2993612587451935\n",
      "Epoch 1746, Loss: 0.4722251445055008, Final Batch Loss: 0.24222706258296967\n",
      "Epoch 1747, Loss: 0.47736866772174835, Final Batch Loss: 0.24105580151081085\n",
      "Epoch 1748, Loss: 0.5078202486038208, Final Batch Loss: 0.27463170886039734\n",
      "Epoch 1749, Loss: 0.534382164478302, Final Batch Loss: 0.25298547744750977\n",
      "Epoch 1750, Loss: 0.4532480090856552, Final Batch Loss: 0.2204686403274536\n",
      "Epoch 1751, Loss: 0.5103790313005447, Final Batch Loss: 0.2274366170167923\n",
      "Epoch 1752, Loss: 0.5127084702253342, Final Batch Loss: 0.23313163220882416\n",
      "Epoch 1753, Loss: 0.4985704720020294, Final Batch Loss: 0.26052233576774597\n",
      "Epoch 1754, Loss: 0.5071616768836975, Final Batch Loss: 0.2805720865726471\n",
      "Epoch 1755, Loss: 0.5233335793018341, Final Batch Loss: 0.2603822946548462\n",
      "Epoch 1756, Loss: 0.485293373465538, Final Batch Loss: 0.2318863719701767\n",
      "Epoch 1757, Loss: 0.4924004226922989, Final Batch Loss: 0.237471804022789\n",
      "Epoch 1758, Loss: 0.5032665729522705, Final Batch Loss: 0.24441790580749512\n",
      "Epoch 1759, Loss: 0.4914267212152481, Final Batch Loss: 0.2195400446653366\n",
      "Epoch 1760, Loss: 0.5031348764896393, Final Batch Loss: 0.2568930387496948\n",
      "Epoch 1761, Loss: 0.5111430436372757, Final Batch Loss: 0.29160135984420776\n",
      "Epoch 1762, Loss: 0.48663321137428284, Final Batch Loss: 0.25083568692207336\n",
      "Epoch 1763, Loss: 0.4918987601995468, Final Batch Loss: 0.22255219519138336\n",
      "Epoch 1764, Loss: 0.47856438159942627, Final Batch Loss: 0.2116282880306244\n",
      "Epoch 1765, Loss: 0.4724464416503906, Final Batch Loss: 0.22126001119613647\n",
      "Epoch 1766, Loss: 0.4603663682937622, Final Batch Loss: 0.22136034071445465\n",
      "Epoch 1767, Loss: 0.4907433092594147, Final Batch Loss: 0.22509720921516418\n",
      "Epoch 1768, Loss: 0.4935297220945358, Final Batch Loss: 0.2713165581226349\n",
      "Epoch 1769, Loss: 0.5162480473518372, Final Batch Loss: 0.27432236075401306\n",
      "Epoch 1770, Loss: 0.5230477750301361, Final Batch Loss: 0.2637816071510315\n",
      "Epoch 1771, Loss: 0.5374269187450409, Final Batch Loss: 0.2408360242843628\n",
      "Epoch 1772, Loss: 0.48806169629096985, Final Batch Loss: 0.20685532689094543\n",
      "Epoch 1773, Loss: 0.5125601589679718, Final Batch Loss: 0.28892531991004944\n",
      "Epoch 1774, Loss: 0.46636874973773956, Final Batch Loss: 0.2299797236919403\n",
      "Epoch 1775, Loss: 0.48808753490448, Final Batch Loss: 0.2571515440940857\n",
      "Epoch 1776, Loss: 0.471974179148674, Final Batch Loss: 0.24454255402088165\n",
      "Epoch 1777, Loss: 0.5504129528999329, Final Batch Loss: 0.30700287222862244\n",
      "Epoch 1778, Loss: 0.43887029588222504, Final Batch Loss: 0.21242459118366241\n",
      "Epoch 1779, Loss: 0.4721307158470154, Final Batch Loss: 0.2004237174987793\n",
      "Epoch 1780, Loss: 0.45245157182216644, Final Batch Loss: 0.22926847636699677\n",
      "Epoch 1781, Loss: 0.5083851367235184, Final Batch Loss: 0.2255730777978897\n",
      "Epoch 1782, Loss: 0.4615258127450943, Final Batch Loss: 0.2082408219575882\n",
      "Epoch 1783, Loss: 0.5097355544567108, Final Batch Loss: 0.2817647457122803\n",
      "Epoch 1784, Loss: 0.46358203887939453, Final Batch Loss: 0.2298658937215805\n",
      "Epoch 1785, Loss: 0.4793366491794586, Final Batch Loss: 0.22965164482593536\n",
      "Epoch 1786, Loss: 0.5006863921880722, Final Batch Loss: 0.21954409778118134\n",
      "Epoch 1787, Loss: 0.5183697938919067, Final Batch Loss: 0.24879276752471924\n",
      "Epoch 1788, Loss: 0.4861961007118225, Final Batch Loss: 0.26559290289878845\n",
      "Epoch 1789, Loss: 0.5210144817829132, Final Batch Loss: 0.303650438785553\n",
      "Epoch 1790, Loss: 0.49545058608055115, Final Batch Loss: 0.2533918023109436\n",
      "Epoch 1791, Loss: 0.4562881588935852, Final Batch Loss: 0.1886681318283081\n",
      "Epoch 1792, Loss: 0.5126533210277557, Final Batch Loss: 0.21492338180541992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1793, Loss: 0.49722304940223694, Final Batch Loss: 0.2294151484966278\n",
      "Epoch 1794, Loss: 0.4907337576150894, Final Batch Loss: 0.26493164896965027\n",
      "Epoch 1795, Loss: 0.5576324462890625, Final Batch Loss: 0.2771926522254944\n",
      "Epoch 1796, Loss: 0.46022936701774597, Final Batch Loss: 0.21663762629032135\n",
      "Epoch 1797, Loss: 0.49588705599308014, Final Batch Loss: 0.2832165062427521\n",
      "Epoch 1798, Loss: 0.4833044856786728, Final Batch Loss: 0.23412200808525085\n",
      "Epoch 1799, Loss: 0.45767757296562195, Final Batch Loss: 0.2522721290588379\n",
      "Epoch 1800, Loss: 0.5386051386594772, Final Batch Loss: 0.31565871834754944\n",
      "Epoch 1801, Loss: 0.4433378279209137, Final Batch Loss: 0.1941486895084381\n",
      "Epoch 1802, Loss: 0.4918854981660843, Final Batch Loss: 0.23437146842479706\n",
      "Epoch 1803, Loss: 0.48650507628917694, Final Batch Loss: 0.21077866852283478\n",
      "Epoch 1804, Loss: 0.48060864210128784, Final Batch Loss: 0.21946385502815247\n",
      "Epoch 1805, Loss: 0.4575466066598892, Final Batch Loss: 0.20848174393177032\n",
      "Epoch 1806, Loss: 0.4449223279953003, Final Batch Loss: 0.19251546263694763\n",
      "Epoch 1807, Loss: 0.4632273018360138, Final Batch Loss: 0.24548999965190887\n",
      "Epoch 1808, Loss: 0.47273632884025574, Final Batch Loss: 0.229094460606575\n",
      "Epoch 1809, Loss: 0.47530999779701233, Final Batch Loss: 0.24463605880737305\n",
      "Epoch 1810, Loss: 0.5289512276649475, Final Batch Loss: 0.20349282026290894\n",
      "Epoch 1811, Loss: 0.5436305552721024, Final Batch Loss: 0.3055114448070526\n",
      "Epoch 1812, Loss: 0.49236829578876495, Final Batch Loss: 0.20096473395824432\n",
      "Epoch 1813, Loss: 0.4479178935289383, Final Batch Loss: 0.21347370743751526\n",
      "Epoch 1814, Loss: 0.507128968834877, Final Batch Loss: 0.24607954919338226\n",
      "Epoch 1815, Loss: 0.48445238173007965, Final Batch Loss: 0.26421353220939636\n",
      "Epoch 1816, Loss: 0.4327617734670639, Final Batch Loss: 0.22714205086231232\n",
      "Epoch 1817, Loss: 0.5319786667823792, Final Batch Loss: 0.2618662416934967\n",
      "Epoch 1818, Loss: 0.49462418258190155, Final Batch Loss: 0.2582690715789795\n",
      "Epoch 1819, Loss: 0.46957990527153015, Final Batch Loss: 0.23782378435134888\n",
      "Epoch 1820, Loss: 0.4968241900205612, Final Batch Loss: 0.23236428201198578\n",
      "Epoch 1821, Loss: 0.5363017916679382, Final Batch Loss: 0.259346067905426\n",
      "Epoch 1822, Loss: 0.4860541522502899, Final Batch Loss: 0.19548943638801575\n",
      "Epoch 1823, Loss: 0.5633944422006607, Final Batch Loss: 0.3239402174949646\n",
      "Epoch 1824, Loss: 0.46854130923748016, Final Batch Loss: 0.22390346229076385\n",
      "Epoch 1825, Loss: 0.48920394480228424, Final Batch Loss: 0.27439919114112854\n",
      "Epoch 1826, Loss: 0.4244280159473419, Final Batch Loss: 0.22447825968265533\n",
      "Epoch 1827, Loss: 0.49548208713531494, Final Batch Loss: 0.2595384418964386\n",
      "Epoch 1828, Loss: 0.493566557765007, Final Batch Loss: 0.1947290152311325\n",
      "Epoch 1829, Loss: 0.543827623128891, Final Batch Loss: 0.2527197599411011\n",
      "Epoch 1830, Loss: 0.5061495304107666, Final Batch Loss: 0.24572527408599854\n",
      "Epoch 1831, Loss: 0.4683174639940262, Final Batch Loss: 0.20110853016376495\n",
      "Epoch 1832, Loss: 0.5070479214191437, Final Batch Loss: 0.24947789311408997\n",
      "Epoch 1833, Loss: 0.5280773937702179, Final Batch Loss: 0.3116459846496582\n",
      "Epoch 1834, Loss: 0.48104844987392426, Final Batch Loss: 0.24070584774017334\n",
      "Epoch 1835, Loss: 0.5152308344841003, Final Batch Loss: 0.2833549380302429\n",
      "Epoch 1836, Loss: 0.48775333166122437, Final Batch Loss: 0.21319696307182312\n",
      "Epoch 1837, Loss: 0.4787183254957199, Final Batch Loss: 0.23743832111358643\n",
      "Epoch 1838, Loss: 0.4389379769563675, Final Batch Loss: 0.22236685454845428\n",
      "Epoch 1839, Loss: 0.47291944921016693, Final Batch Loss: 0.21973685920238495\n",
      "Epoch 1840, Loss: 0.5262231826782227, Final Batch Loss: 0.27453309297561646\n",
      "Epoch 1841, Loss: 0.46051105856895447, Final Batch Loss: 0.24600321054458618\n",
      "Epoch 1842, Loss: 0.48048487305641174, Final Batch Loss: 0.21811553835868835\n",
      "Epoch 1843, Loss: 0.4759989529848099, Final Batch Loss: 0.2607196271419525\n",
      "Epoch 1844, Loss: 0.5311069041490555, Final Batch Loss: 0.3078099489212036\n",
      "Epoch 1845, Loss: 0.5269283801317215, Final Batch Loss: 0.28880593180656433\n",
      "Epoch 1846, Loss: 0.448093518614769, Final Batch Loss: 0.20854265987873077\n",
      "Epoch 1847, Loss: 0.49607889354228973, Final Batch Loss: 0.2750268876552582\n",
      "Epoch 1848, Loss: 0.4462146610021591, Final Batch Loss: 0.2507711946964264\n",
      "Epoch 1849, Loss: 0.4828570783138275, Final Batch Loss: 0.24228844046592712\n",
      "Epoch 1850, Loss: 0.4397464990615845, Final Batch Loss: 0.22170309722423553\n",
      "Epoch 1851, Loss: 0.46247659623622894, Final Batch Loss: 0.23004473745822906\n",
      "Epoch 1852, Loss: 0.4715442508459091, Final Batch Loss: 0.24121078848838806\n",
      "Epoch 1853, Loss: 0.47373242676258087, Final Batch Loss: 0.24729683995246887\n",
      "Epoch 1854, Loss: 0.48989951610565186, Final Batch Loss: 0.2532351315021515\n",
      "Epoch 1855, Loss: 0.5061163902282715, Final Batch Loss: 0.28209447860717773\n",
      "Epoch 1856, Loss: 0.4040248692035675, Final Batch Loss: 0.19804829359054565\n",
      "Epoch 1857, Loss: 0.5492618978023529, Final Batch Loss: 0.23481661081314087\n",
      "Epoch 1858, Loss: 0.46940502524375916, Final Batch Loss: 0.21974055469036102\n",
      "Epoch 1859, Loss: 0.49045585095882416, Final Batch Loss: 0.264889121055603\n",
      "Epoch 1860, Loss: 0.47414161264896393, Final Batch Loss: 0.20853321254253387\n",
      "Epoch 1861, Loss: 0.46979089081287384, Final Batch Loss: 0.2599177658557892\n",
      "Epoch 1862, Loss: 0.5226619690656662, Final Batch Loss: 0.2458401769399643\n",
      "Epoch 1863, Loss: 0.472691148519516, Final Batch Loss: 0.269892156124115\n",
      "Epoch 1864, Loss: 0.4816588908433914, Final Batch Loss: 0.2528092861175537\n",
      "Epoch 1865, Loss: 0.4994119703769684, Final Batch Loss: 0.26744601130485535\n",
      "Epoch 1866, Loss: 0.4376351982355118, Final Batch Loss: 0.20505550503730774\n",
      "Epoch 1867, Loss: 0.48066967725753784, Final Batch Loss: 0.19942381978034973\n",
      "Epoch 1868, Loss: 0.5495344400405884, Final Batch Loss: 0.3132023513317108\n",
      "Epoch 1869, Loss: 0.4715535044670105, Final Batch Loss: 0.23148494958877563\n",
      "Epoch 1870, Loss: 0.46165570616722107, Final Batch Loss: 0.245818093419075\n",
      "Epoch 1871, Loss: 0.4565563350915909, Final Batch Loss: 0.22552251815795898\n",
      "Epoch 1872, Loss: 0.50271175801754, Final Batch Loss: 0.26465117931365967\n",
      "Epoch 1873, Loss: 0.46334323287010193, Final Batch Loss: 0.22368590533733368\n",
      "Epoch 1874, Loss: 0.4383288472890854, Final Batch Loss: 0.22740034759044647\n",
      "Epoch 1875, Loss: 0.4791157990694046, Final Batch Loss: 0.22850041091442108\n",
      "Epoch 1876, Loss: 0.48258034884929657, Final Batch Loss: 0.2322022169828415\n",
      "Epoch 1877, Loss: 0.5078794807195663, Final Batch Loss: 0.26912328600883484\n",
      "Epoch 1878, Loss: 0.48569531738758087, Final Batch Loss: 0.2265491932630539\n",
      "Epoch 1879, Loss: 0.4666623920202255, Final Batch Loss: 0.20773403346538544\n",
      "Epoch 1880, Loss: 0.5215713679790497, Final Batch Loss: 0.23939049243927002\n",
      "Epoch 1881, Loss: 0.4653206020593643, Final Batch Loss: 0.22783133387565613\n",
      "Epoch 1882, Loss: 0.5014651715755463, Final Batch Loss: 0.2731950283050537\n",
      "Epoch 1883, Loss: 0.48517003655433655, Final Batch Loss: 0.29932141304016113\n",
      "Epoch 1884, Loss: 0.4766000807285309, Final Batch Loss: 0.2321961224079132\n",
      "Epoch 1885, Loss: 0.4519254118204117, Final Batch Loss: 0.20044396817684174\n",
      "Epoch 1886, Loss: 0.4554991126060486, Final Batch Loss: 0.22425255179405212\n",
      "Epoch 1887, Loss: 0.4704088717699051, Final Batch Loss: 0.2632351219654083\n",
      "Epoch 1888, Loss: 0.465221643447876, Final Batch Loss: 0.2660643458366394\n",
      "Epoch 1889, Loss: 0.4509951174259186, Final Batch Loss: 0.2265155166387558\n",
      "Epoch 1890, Loss: 0.46049317717552185, Final Batch Loss: 0.24684543907642365\n",
      "Epoch 1891, Loss: 0.48806698620319366, Final Batch Loss: 0.24611084163188934\n",
      "Epoch 1892, Loss: 0.4949832558631897, Final Batch Loss: 0.24312946200370789\n",
      "Epoch 1893, Loss: 0.49593181908130646, Final Batch Loss: 0.23527781665325165\n",
      "Epoch 1894, Loss: 0.4580124318599701, Final Batch Loss: 0.25594231486320496\n",
      "Epoch 1895, Loss: 0.4963260442018509, Final Batch Loss: 0.22935016453266144\n",
      "Epoch 1896, Loss: 0.4512327015399933, Final Batch Loss: 0.2351740449666977\n",
      "Epoch 1897, Loss: 0.4101092517375946, Final Batch Loss: 0.18183906376361847\n",
      "Epoch 1898, Loss: 0.49618658423423767, Final Batch Loss: 0.28916099667549133\n",
      "Epoch 1899, Loss: 0.5489928424358368, Final Batch Loss: 0.28743335604667664\n",
      "Epoch 1900, Loss: 0.4854762703180313, Final Batch Loss: 0.23496641218662262\n",
      "Epoch 1901, Loss: 0.44425900280475616, Final Batch Loss: 0.23254482448101044\n",
      "Epoch 1902, Loss: 0.513953298330307, Final Batch Loss: 0.25336596369743347\n",
      "Epoch 1903, Loss: 0.4711802452802658, Final Batch Loss: 0.2400846630334854\n",
      "Epoch 1904, Loss: 0.49551378190517426, Final Batch Loss: 0.20006756484508514\n",
      "Epoch 1905, Loss: 0.4803859293460846, Final Batch Loss: 0.2897837162017822\n",
      "Epoch 1906, Loss: 0.4740098714828491, Final Batch Loss: 0.24549221992492676\n",
      "Epoch 1907, Loss: 0.4660068601369858, Final Batch Loss: 0.23646238446235657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1908, Loss: 0.45192116498947144, Final Batch Loss: 0.20190250873565674\n",
      "Epoch 1909, Loss: 0.4187060594558716, Final Batch Loss: 0.2165239155292511\n",
      "Epoch 1910, Loss: 0.45236730575561523, Final Batch Loss: 0.2406315952539444\n",
      "Epoch 1911, Loss: 0.4387156665325165, Final Batch Loss: 0.24224041402339935\n",
      "Epoch 1912, Loss: 0.5116056799888611, Final Batch Loss: 0.25007325410842896\n",
      "Epoch 1913, Loss: 0.4749782234430313, Final Batch Loss: 0.23699258267879486\n",
      "Epoch 1914, Loss: 0.4481073468923569, Final Batch Loss: 0.24319365620613098\n",
      "Epoch 1915, Loss: 0.47577543556690216, Final Batch Loss: 0.2824192941188812\n",
      "Epoch 1916, Loss: 0.47161853313446045, Final Batch Loss: 0.27422434091567993\n",
      "Epoch 1917, Loss: 0.4429471790790558, Final Batch Loss: 0.20170117914676666\n",
      "Epoch 1918, Loss: 0.4096544086933136, Final Batch Loss: 0.2231413573026657\n",
      "Epoch 1919, Loss: 0.4470902383327484, Final Batch Loss: 0.19975905120372772\n",
      "Epoch 1920, Loss: 0.4735931158065796, Final Batch Loss: 0.24353760480880737\n",
      "Epoch 1921, Loss: 0.5031141340732574, Final Batch Loss: 0.254235178232193\n",
      "Epoch 1922, Loss: 0.4287971556186676, Final Batch Loss: 0.22566793859004974\n",
      "Epoch 1923, Loss: 0.5361116379499435, Final Batch Loss: 0.20384512841701508\n",
      "Epoch 1924, Loss: 0.49389709532260895, Final Batch Loss: 0.18479694426059723\n",
      "Epoch 1925, Loss: 0.5037520676851273, Final Batch Loss: 0.22478531301021576\n",
      "Epoch 1926, Loss: 0.44817300140857697, Final Batch Loss: 0.23125475645065308\n",
      "Epoch 1927, Loss: 0.5326716601848602, Final Batch Loss: 0.2992812991142273\n",
      "Epoch 1928, Loss: 0.43221573531627655, Final Batch Loss: 0.197860985994339\n",
      "Epoch 1929, Loss: 0.440669909119606, Final Batch Loss: 0.22165104746818542\n",
      "Epoch 1930, Loss: 0.43239839375019073, Final Batch Loss: 0.24626365303993225\n",
      "Epoch 1931, Loss: 0.4670327305793762, Final Batch Loss: 0.2367638349533081\n",
      "Epoch 1932, Loss: 0.40841197967529297, Final Batch Loss: 0.19220946729183197\n",
      "Epoch 1933, Loss: 0.43731480836868286, Final Batch Loss: 0.2000967115163803\n",
      "Epoch 1934, Loss: 0.4470730721950531, Final Batch Loss: 0.24126560986042023\n",
      "Epoch 1935, Loss: 0.5433416962623596, Final Batch Loss: 0.29131215810775757\n",
      "Epoch 1936, Loss: 0.4938473254442215, Final Batch Loss: 0.3101315498352051\n",
      "Epoch 1937, Loss: 0.4508288502693176, Final Batch Loss: 0.2001035213470459\n",
      "Epoch 1938, Loss: 0.4513501971960068, Final Batch Loss: 0.20912329852581024\n",
      "Epoch 1939, Loss: 0.4492240697145462, Final Batch Loss: 0.2295749932527542\n",
      "Epoch 1940, Loss: 0.4356748014688492, Final Batch Loss: 0.26040273904800415\n",
      "Epoch 1941, Loss: 0.4332881569862366, Final Batch Loss: 0.21708369255065918\n",
      "Epoch 1942, Loss: 0.5427922308444977, Final Batch Loss: 0.27550366520881653\n",
      "Epoch 1943, Loss: 0.4467207342386246, Final Batch Loss: 0.2365693897008896\n",
      "Epoch 1944, Loss: 0.45943765342235565, Final Batch Loss: 0.23559659719467163\n",
      "Epoch 1945, Loss: 0.4417535811662674, Final Batch Loss: 0.19910204410552979\n",
      "Epoch 1946, Loss: 0.4447762370109558, Final Batch Loss: 0.2228386104106903\n",
      "Epoch 1947, Loss: 0.47653016448020935, Final Batch Loss: 0.2607954740524292\n",
      "Epoch 1948, Loss: 0.4834240525960922, Final Batch Loss: 0.2629318833351135\n",
      "Epoch 1949, Loss: 0.504079595208168, Final Batch Loss: 0.2317722886800766\n",
      "Epoch 1950, Loss: 0.4709833413362503, Final Batch Loss: 0.19333790242671967\n",
      "Epoch 1951, Loss: 0.44051116704940796, Final Batch Loss: 0.22970199584960938\n",
      "Epoch 1952, Loss: 0.4604176878929138, Final Batch Loss: 0.2500974237918854\n",
      "Epoch 1953, Loss: 0.4366234987974167, Final Batch Loss: 0.21488797664642334\n",
      "Epoch 1954, Loss: 0.4304663687944412, Final Batch Loss: 0.21826738119125366\n",
      "Epoch 1955, Loss: 0.46922270953655243, Final Batch Loss: 0.2128753513097763\n",
      "Epoch 1956, Loss: 0.46685872972011566, Final Batch Loss: 0.20979748666286469\n",
      "Epoch 1957, Loss: 0.44278641045093536, Final Batch Loss: 0.20128077268600464\n",
      "Epoch 1958, Loss: 0.43746404349803925, Final Batch Loss: 0.21383818984031677\n",
      "Epoch 1959, Loss: 0.47162067890167236, Final Batch Loss: 0.19363313913345337\n",
      "Epoch 1960, Loss: 0.48274317383766174, Final Batch Loss: 0.2927975058555603\n",
      "Epoch 1961, Loss: 0.43095968663692474, Final Batch Loss: 0.20134063065052032\n",
      "Epoch 1962, Loss: 0.46241702139377594, Final Batch Loss: 0.21246731281280518\n",
      "Epoch 1963, Loss: 0.45126262307167053, Final Batch Loss: 0.20949001610279083\n",
      "Epoch 1964, Loss: 0.41530677676200867, Final Batch Loss: 0.20336882770061493\n",
      "Epoch 1965, Loss: 0.41950009763240814, Final Batch Loss: 0.19077952206134796\n",
      "Epoch 1966, Loss: 0.46821945905685425, Final Batch Loss: 0.21378028392791748\n",
      "Epoch 1967, Loss: 0.48866473138332367, Final Batch Loss: 0.22367040812969208\n",
      "Epoch 1968, Loss: 0.503293976187706, Final Batch Loss: 0.28073182702064514\n",
      "Epoch 1969, Loss: 0.4754040539264679, Final Batch Loss: 0.23720483481884003\n",
      "Epoch 1970, Loss: 0.5099225342273712, Final Batch Loss: 0.25932496786117554\n",
      "Epoch 1971, Loss: 0.4278955012559891, Final Batch Loss: 0.20530985295772552\n",
      "Epoch 1972, Loss: 0.4669782221317291, Final Batch Loss: 0.26586413383483887\n",
      "Epoch 1973, Loss: 0.45898696780204773, Final Batch Loss: 0.24445217847824097\n",
      "Epoch 1974, Loss: 0.4576408714056015, Final Batch Loss: 0.2236332893371582\n",
      "Epoch 1975, Loss: 0.41135746240615845, Final Batch Loss: 0.2047686129808426\n",
      "Epoch 1976, Loss: 0.5111611187458038, Final Batch Loss: 0.2745552957057953\n",
      "Epoch 1977, Loss: 0.49036887288093567, Final Batch Loss: 0.27279654145240784\n",
      "Epoch 1978, Loss: 0.49158139526844025, Final Batch Loss: 0.3021273910999298\n",
      "Epoch 1979, Loss: 0.4419660121202469, Final Batch Loss: 0.2464195042848587\n",
      "Epoch 1980, Loss: 0.4635852575302124, Final Batch Loss: 0.22151075303554535\n",
      "Epoch 1981, Loss: 0.4330017417669296, Final Batch Loss: 0.22384318709373474\n",
      "Epoch 1982, Loss: 0.38671891391277313, Final Batch Loss: 0.2042546421289444\n",
      "Epoch 1983, Loss: 0.4272501766681671, Final Batch Loss: 0.2232847511768341\n",
      "Epoch 1984, Loss: 0.4439609795808792, Final Batch Loss: 0.1971648931503296\n",
      "Epoch 1985, Loss: 0.45297250151634216, Final Batch Loss: 0.22539262473583221\n",
      "Epoch 1986, Loss: 0.4609677344560623, Final Batch Loss: 0.2308983951807022\n",
      "Epoch 1987, Loss: 0.4385340213775635, Final Batch Loss: 0.25450506806373596\n",
      "Epoch 1988, Loss: 0.4229058474302292, Final Batch Loss: 0.2212146520614624\n",
      "Epoch 1989, Loss: 0.4245896488428116, Final Batch Loss: 0.1971570998430252\n",
      "Epoch 1990, Loss: 0.46861760318279266, Final Batch Loss: 0.23342013359069824\n",
      "Epoch 1991, Loss: 0.4810912609100342, Final Batch Loss: 0.23601152002811432\n",
      "Epoch 1992, Loss: 0.42153747379779816, Final Batch Loss: 0.21178819239139557\n",
      "Epoch 1993, Loss: 0.3959362208843231, Final Batch Loss: 0.17535899579524994\n",
      "Epoch 1994, Loss: 0.46011342108249664, Final Batch Loss: 0.22163383662700653\n",
      "Epoch 1995, Loss: 0.4459966570138931, Final Batch Loss: 0.24113954603672028\n",
      "Epoch 1996, Loss: 0.44064487516880035, Final Batch Loss: 0.18196509778499603\n",
      "Epoch 1997, Loss: 0.4058452844619751, Final Batch Loss: 0.2517305314540863\n",
      "Epoch 1998, Loss: 0.4108629375696182, Final Batch Loss: 0.22053074836730957\n",
      "Epoch 1999, Loss: 0.5393926203250885, Final Batch Loss: 0.2528938949108124\n",
      "Epoch 2000, Loss: 0.4964277148246765, Final Batch Loss: 0.26853498816490173\n",
      "Epoch 2001, Loss: 0.4798782765865326, Final Batch Loss: 0.2268953025341034\n",
      "Epoch 2002, Loss: 0.4177614897489548, Final Batch Loss: 0.1993759423494339\n",
      "Epoch 2003, Loss: 0.4313938021659851, Final Batch Loss: 0.21511295437812805\n",
      "Epoch 2004, Loss: 0.41907818615436554, Final Batch Loss: 0.22814038395881653\n",
      "Epoch 2005, Loss: 0.462570384144783, Final Batch Loss: 0.28003019094467163\n",
      "Epoch 2006, Loss: 0.4117441028356552, Final Batch Loss: 0.1964191496372223\n",
      "Epoch 2007, Loss: 0.38813768327236176, Final Batch Loss: 0.1946152150630951\n",
      "Epoch 2008, Loss: 0.4291737377643585, Final Batch Loss: 0.18073509633541107\n",
      "Epoch 2009, Loss: 0.4539032280445099, Final Batch Loss: 0.2272048145532608\n",
      "Epoch 2010, Loss: 0.4483099728822708, Final Batch Loss: 0.21428559720516205\n",
      "Epoch 2011, Loss: 0.4503229409456253, Final Batch Loss: 0.2245251089334488\n",
      "Epoch 2012, Loss: 0.4781929701566696, Final Batch Loss: 0.21490810811519623\n",
      "Epoch 2013, Loss: 0.47184039652347565, Final Batch Loss: 0.23158127069473267\n",
      "Epoch 2014, Loss: 0.4115598648786545, Final Batch Loss: 0.20484760403633118\n",
      "Epoch 2015, Loss: 0.427408367395401, Final Batch Loss: 0.2238026112318039\n",
      "Epoch 2016, Loss: 0.44033433496952057, Final Batch Loss: 0.2006990760564804\n",
      "Epoch 2017, Loss: 0.3881300836801529, Final Batch Loss: 0.18579639494419098\n",
      "Epoch 2018, Loss: 0.45096655189991, Final Batch Loss: 0.22532136738300323\n",
      "Epoch 2019, Loss: 0.42205810546875, Final Batch Loss: 0.19931285083293915\n",
      "Epoch 2020, Loss: 0.400742769241333, Final Batch Loss: 0.182984858751297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2021, Loss: 0.4764888137578964, Final Batch Loss: 0.22940023243427277\n",
      "Epoch 2022, Loss: 0.4340171366930008, Final Batch Loss: 0.19512788951396942\n",
      "Epoch 2023, Loss: 0.4194621294736862, Final Batch Loss: 0.1878281980752945\n",
      "Epoch 2024, Loss: 0.4491480588912964, Final Batch Loss: 0.21709878742694855\n",
      "Epoch 2025, Loss: 0.43195395171642303, Final Batch Loss: 0.22634248435497284\n",
      "Epoch 2026, Loss: 0.3937441259622574, Final Batch Loss: 0.21352820098400116\n",
      "Epoch 2027, Loss: 0.48254306614398956, Final Batch Loss: 0.20668341219425201\n",
      "Epoch 2028, Loss: 0.40721002221107483, Final Batch Loss: 0.1785590797662735\n",
      "Epoch 2029, Loss: 0.4254777431488037, Final Batch Loss: 0.20927788317203522\n",
      "Epoch 2030, Loss: 0.38664665818214417, Final Batch Loss: 0.21331687271595\n",
      "Epoch 2031, Loss: 0.42174696922302246, Final Batch Loss: 0.18956996500492096\n",
      "Epoch 2032, Loss: 0.46224333345890045, Final Batch Loss: 0.23540784418582916\n",
      "Epoch 2033, Loss: 0.40239062905311584, Final Batch Loss: 0.19992651045322418\n",
      "Epoch 2034, Loss: 0.4363371729850769, Final Batch Loss: 0.18286874890327454\n",
      "Epoch 2035, Loss: 0.41985318064689636, Final Batch Loss: 0.17769458889961243\n",
      "Epoch 2036, Loss: 0.40761131048202515, Final Batch Loss: 0.22608427703380585\n",
      "Epoch 2037, Loss: 0.4683196246623993, Final Batch Loss: 0.2389868199825287\n",
      "Epoch 2038, Loss: 0.4710724651813507, Final Batch Loss: 0.22331292927265167\n",
      "Epoch 2039, Loss: 0.49105189740657806, Final Batch Loss: 0.22348059713840485\n",
      "Epoch 2040, Loss: 0.4501999169588089, Final Batch Loss: 0.2347998023033142\n",
      "Epoch 2041, Loss: 0.4204024374485016, Final Batch Loss: 0.19955582916736603\n",
      "Epoch 2042, Loss: 0.3439343422651291, Final Batch Loss: 0.1629062443971634\n",
      "Epoch 2043, Loss: 0.4873955547809601, Final Batch Loss: 0.268647700548172\n",
      "Epoch 2044, Loss: 0.44021062552928925, Final Batch Loss: 0.2011037915945053\n",
      "Epoch 2045, Loss: 0.3928912878036499, Final Batch Loss: 0.19224883615970612\n",
      "Epoch 2046, Loss: 0.44430817663669586, Final Batch Loss: 0.22728045284748077\n",
      "Epoch 2047, Loss: 0.47431233525276184, Final Batch Loss: 0.2709091007709503\n",
      "Epoch 2048, Loss: 0.42496173083782196, Final Batch Loss: 0.20666538178920746\n",
      "Epoch 2049, Loss: 0.4270232766866684, Final Batch Loss: 0.17987824976444244\n",
      "Epoch 2050, Loss: 0.41903185844421387, Final Batch Loss: 0.2145693004131317\n",
      "Epoch 2051, Loss: 0.41784678399562836, Final Batch Loss: 0.16043107211589813\n",
      "Epoch 2052, Loss: 0.40125441551208496, Final Batch Loss: 0.20496363937854767\n",
      "Epoch 2053, Loss: 0.43864937126636505, Final Batch Loss: 0.23632685840129852\n",
      "Epoch 2054, Loss: 0.5012944489717484, Final Batch Loss: 0.20369739830493927\n",
      "Epoch 2055, Loss: 0.4170549511909485, Final Batch Loss: 0.18126173317432404\n",
      "Epoch 2056, Loss: 0.4051206260919571, Final Batch Loss: 0.17225578427314758\n",
      "Epoch 2057, Loss: 0.4259780943393707, Final Batch Loss: 0.17949241399765015\n",
      "Epoch 2058, Loss: 0.5087694376707077, Final Batch Loss: 0.22737912833690643\n",
      "Epoch 2059, Loss: 0.4633084684610367, Final Batch Loss: 0.2803947627544403\n",
      "Epoch 2060, Loss: 0.4005487859249115, Final Batch Loss: 0.22399687767028809\n",
      "Epoch 2061, Loss: 0.41143275797367096, Final Batch Loss: 0.20007865130901337\n",
      "Epoch 2062, Loss: 0.4258767068386078, Final Batch Loss: 0.22838014364242554\n",
      "Epoch 2063, Loss: 0.3784118592739105, Final Batch Loss: 0.17325301468372345\n",
      "Epoch 2064, Loss: 0.41276203095912933, Final Batch Loss: 0.16879414021968842\n",
      "Epoch 2065, Loss: 0.45186126232147217, Final Batch Loss: 0.2236832231283188\n",
      "Epoch 2066, Loss: 0.4511824697256088, Final Batch Loss: 0.24158929288387299\n",
      "Epoch 2067, Loss: 0.5187240093946457, Final Batch Loss: 0.3153603971004486\n",
      "Epoch 2068, Loss: 0.43554845452308655, Final Batch Loss: 0.23739804327487946\n",
      "Epoch 2069, Loss: 0.4377557039260864, Final Batch Loss: 0.26567330956459045\n",
      "Epoch 2070, Loss: 0.4756413996219635, Final Batch Loss: 0.20442506670951843\n",
      "Epoch 2071, Loss: 0.39689572155475616, Final Batch Loss: 0.21327708661556244\n",
      "Epoch 2072, Loss: 0.44470056891441345, Final Batch Loss: 0.22555387020111084\n",
      "Epoch 2073, Loss: 0.46834561228752136, Final Batch Loss: 0.2703970670700073\n",
      "Epoch 2074, Loss: 0.43867209553718567, Final Batch Loss: 0.23728497326374054\n",
      "Epoch 2075, Loss: 0.3701816350221634, Final Batch Loss: 0.19391454756259918\n",
      "Epoch 2076, Loss: 0.4231236129999161, Final Batch Loss: 0.23740491271018982\n",
      "Epoch 2077, Loss: 0.4115173667669296, Final Batch Loss: 0.21636256575584412\n",
      "Epoch 2078, Loss: 0.4077833741903305, Final Batch Loss: 0.2259833812713623\n",
      "Epoch 2079, Loss: 0.3895135074853897, Final Batch Loss: 0.2275560349225998\n",
      "Epoch 2080, Loss: 0.47911427915096283, Final Batch Loss: 0.252909779548645\n",
      "Epoch 2081, Loss: 0.48881223797798157, Final Batch Loss: 0.3046441674232483\n",
      "Epoch 2082, Loss: 0.5084892213344574, Final Batch Loss: 0.25513386726379395\n",
      "Epoch 2083, Loss: 0.4019686132669449, Final Batch Loss: 0.18378165364265442\n",
      "Epoch 2084, Loss: 0.42983338236808777, Final Batch Loss: 0.20615917444229126\n",
      "Epoch 2085, Loss: 0.44639503955841064, Final Batch Loss: 0.21947845816612244\n",
      "Epoch 2086, Loss: 0.4090184271335602, Final Batch Loss: 0.1865561604499817\n",
      "Epoch 2087, Loss: 0.4959886521100998, Final Batch Loss: 0.2505071461200714\n",
      "Epoch 2088, Loss: 0.4593276083469391, Final Batch Loss: 0.24409469962120056\n",
      "Epoch 2089, Loss: 0.4774412661790848, Final Batch Loss: 0.23712937533855438\n",
      "Epoch 2090, Loss: 0.39709433913230896, Final Batch Loss: 0.1962578296661377\n",
      "Epoch 2091, Loss: 0.40844617784023285, Final Batch Loss: 0.18914470076560974\n",
      "Epoch 2092, Loss: 0.41712786257267, Final Batch Loss: 0.2462267130613327\n",
      "Epoch 2093, Loss: 0.44162094593048096, Final Batch Loss: 0.1841132640838623\n",
      "Epoch 2094, Loss: 0.4137883484363556, Final Batch Loss: 0.20462244749069214\n",
      "Epoch 2095, Loss: 0.5299511104822159, Final Batch Loss: 0.294247567653656\n",
      "Epoch 2096, Loss: 0.45369940996170044, Final Batch Loss: 0.2490387260913849\n",
      "Epoch 2097, Loss: 0.4053361862897873, Final Batch Loss: 0.1941399723291397\n",
      "Epoch 2098, Loss: 0.39772574603557587, Final Batch Loss: 0.18285223841667175\n",
      "Epoch 2099, Loss: 0.49388930201530457, Final Batch Loss: 0.22401195764541626\n",
      "Epoch 2100, Loss: 0.38923002779483795, Final Batch Loss: 0.16601133346557617\n",
      "Epoch 2101, Loss: 0.3988830894231796, Final Batch Loss: 0.2114870846271515\n",
      "Epoch 2102, Loss: 0.4376313090324402, Final Batch Loss: 0.2110883742570877\n",
      "Epoch 2103, Loss: 0.42644505202770233, Final Batch Loss: 0.2546924650669098\n",
      "Epoch 2104, Loss: 0.4403381943702698, Final Batch Loss: 0.23942235112190247\n",
      "Epoch 2105, Loss: 0.43780745565891266, Final Batch Loss: 0.24126169085502625\n",
      "Epoch 2106, Loss: 0.45842453837394714, Final Batch Loss: 0.24382153153419495\n",
      "Epoch 2107, Loss: 0.4534880369901657, Final Batch Loss: 0.22754770517349243\n",
      "Epoch 2108, Loss: 0.4243699610233307, Final Batch Loss: 0.21332961320877075\n",
      "Epoch 2109, Loss: 0.38090458512306213, Final Batch Loss: 0.19501760601997375\n",
      "Epoch 2110, Loss: 0.45474302768707275, Final Batch Loss: 0.22063440084457397\n",
      "Epoch 2111, Loss: 0.42573443055152893, Final Batch Loss: 0.24648921191692352\n",
      "Epoch 2112, Loss: 0.44575972855091095, Final Batch Loss: 0.2560674846172333\n",
      "Epoch 2113, Loss: 0.410210520029068, Final Batch Loss: 0.19548042118549347\n",
      "Epoch 2114, Loss: 0.4478244185447693, Final Batch Loss: 0.2323431670665741\n",
      "Epoch 2115, Loss: 0.38943323493003845, Final Batch Loss: 0.16270393133163452\n",
      "Epoch 2116, Loss: 0.42195285856723785, Final Batch Loss: 0.22299666702747345\n",
      "Epoch 2117, Loss: 0.4386478066444397, Final Batch Loss: 0.22136743366718292\n",
      "Epoch 2118, Loss: 0.3845398873090744, Final Batch Loss: 0.1731332540512085\n",
      "Epoch 2119, Loss: 0.415094792842865, Final Batch Loss: 0.20294858515262604\n",
      "Epoch 2120, Loss: 0.393721804022789, Final Batch Loss: 0.18785877525806427\n",
      "Epoch 2121, Loss: 0.39262041449546814, Final Batch Loss: 0.20887607336044312\n",
      "Epoch 2122, Loss: 0.3657311350107193, Final Batch Loss: 0.1571160852909088\n",
      "Epoch 2123, Loss: 0.3778262436389923, Final Batch Loss: 0.20647701621055603\n",
      "Epoch 2124, Loss: 0.4454171359539032, Final Batch Loss: 0.2595180869102478\n",
      "Epoch 2125, Loss: 0.41618606448173523, Final Batch Loss: 0.20911096036434174\n",
      "Epoch 2126, Loss: 0.41700106859207153, Final Batch Loss: 0.18084366619586945\n",
      "Epoch 2127, Loss: 0.40462759137153625, Final Batch Loss: 0.19040822982788086\n",
      "Epoch 2128, Loss: 0.4779294729232788, Final Batch Loss: 0.2615896463394165\n",
      "Epoch 2129, Loss: 0.38853128254413605, Final Batch Loss: 0.22012487053871155\n",
      "Epoch 2130, Loss: 0.42332108318805695, Final Batch Loss: 0.19059596955776215\n",
      "Epoch 2131, Loss: 0.38663628697395325, Final Batch Loss: 0.19924971461296082\n",
      "Epoch 2132, Loss: 0.4844827800989151, Final Batch Loss: 0.23882779479026794\n",
      "Epoch 2133, Loss: 0.4860459268093109, Final Batch Loss: 0.2553879916667938\n",
      "Epoch 2134, Loss: 0.4031183123588562, Final Batch Loss: 0.19013313949108124\n",
      "Epoch 2135, Loss: 0.39909160137176514, Final Batch Loss: 0.16451750695705414\n",
      "Epoch 2136, Loss: 0.4398607760667801, Final Batch Loss: 0.23060230910778046\n",
      "Epoch 2137, Loss: 0.3864160478115082, Final Batch Loss: 0.2074967622756958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2138, Loss: 0.44723354279994965, Final Batch Loss: 0.2515788972377777\n",
      "Epoch 2139, Loss: 0.43091981112957, Final Batch Loss: 0.24388161301612854\n",
      "Epoch 2140, Loss: 0.4083593040704727, Final Batch Loss: 0.27963098883628845\n",
      "Epoch 2141, Loss: 0.36830464005470276, Final Batch Loss: 0.19082041084766388\n",
      "Epoch 2142, Loss: 0.4326210469007492, Final Batch Loss: 0.25502631068229675\n",
      "Epoch 2143, Loss: 0.4071581959724426, Final Batch Loss: 0.20842035114765167\n",
      "Epoch 2144, Loss: 0.4271252602338791, Final Batch Loss: 0.2391536980867386\n",
      "Epoch 2145, Loss: 0.4724033772945404, Final Batch Loss: 0.22861798107624054\n",
      "Epoch 2146, Loss: 0.417070597410202, Final Batch Loss: 0.23108810186386108\n",
      "Epoch 2147, Loss: 0.4270736277103424, Final Batch Loss: 0.22061224281787872\n",
      "Epoch 2148, Loss: 0.43547065556049347, Final Batch Loss: 0.1936292201280594\n",
      "Epoch 2149, Loss: 0.39129239320755005, Final Batch Loss: 0.22275064885616302\n",
      "Epoch 2150, Loss: 0.45605479180812836, Final Batch Loss: 0.22970135509967804\n",
      "Epoch 2151, Loss: 0.3451479524374008, Final Batch Loss: 0.17999908328056335\n",
      "Epoch 2152, Loss: 0.423296183347702, Final Batch Loss: 0.2364072948694229\n",
      "Epoch 2153, Loss: 0.4200086146593094, Final Batch Loss: 0.2499406784772873\n",
      "Epoch 2154, Loss: 0.47072598338127136, Final Batch Loss: 0.2254306823015213\n",
      "Epoch 2155, Loss: 0.3883122205734253, Final Batch Loss: 0.16961602866649628\n",
      "Epoch 2156, Loss: 0.42733605206012726, Final Batch Loss: 0.19976817071437836\n",
      "Epoch 2157, Loss: 0.3981408327817917, Final Batch Loss: 0.20199908316135406\n",
      "Epoch 2158, Loss: 0.3584025651216507, Final Batch Loss: 0.18919922411441803\n",
      "Epoch 2159, Loss: 0.38253921270370483, Final Batch Loss: 0.17164501547813416\n",
      "Epoch 2160, Loss: 0.45228566229343414, Final Batch Loss: 0.204864040017128\n",
      "Epoch 2161, Loss: 0.4202881008386612, Final Batch Loss: 0.2107054740190506\n",
      "Epoch 2162, Loss: 0.413912758231163, Final Batch Loss: 0.1791936457157135\n",
      "Epoch 2163, Loss: 0.43047285079956055, Final Batch Loss: 0.19762203097343445\n",
      "Epoch 2164, Loss: 0.4174472987651825, Final Batch Loss: 0.19645656645298004\n",
      "Epoch 2165, Loss: 0.3568127751350403, Final Batch Loss: 0.18018174171447754\n",
      "Epoch 2166, Loss: 0.3888463228940964, Final Batch Loss: 0.1682613343000412\n",
      "Epoch 2167, Loss: 0.3915715217590332, Final Batch Loss: 0.22012700140476227\n",
      "Epoch 2168, Loss: 0.4278288334608078, Final Batch Loss: 0.20351825654506683\n",
      "Epoch 2169, Loss: 0.4065481275320053, Final Batch Loss: 0.2153918445110321\n",
      "Epoch 2170, Loss: 0.3883841782808304, Final Batch Loss: 0.18139296770095825\n",
      "Epoch 2171, Loss: 0.42245593667030334, Final Batch Loss: 0.19150832295417786\n",
      "Epoch 2172, Loss: 0.3803279399871826, Final Batch Loss: 0.19223521649837494\n",
      "Epoch 2173, Loss: 0.4026608169078827, Final Batch Loss: 0.19377577304840088\n",
      "Epoch 2174, Loss: 0.41062207520008087, Final Batch Loss: 0.23261360824108124\n",
      "Epoch 2175, Loss: 0.40238527953624725, Final Batch Loss: 0.20802843570709229\n",
      "Epoch 2176, Loss: 0.44744521379470825, Final Batch Loss: 0.22214795649051666\n",
      "Epoch 2177, Loss: 0.5356873869895935, Final Batch Loss: 0.22823086380958557\n",
      "Epoch 2178, Loss: 0.3909793198108673, Final Batch Loss: 0.20482760667800903\n",
      "Epoch 2179, Loss: 0.42475543916225433, Final Batch Loss: 0.20979925990104675\n",
      "Epoch 2180, Loss: 0.38001179695129395, Final Batch Loss: 0.18675373494625092\n",
      "Epoch 2181, Loss: 0.3807292878627777, Final Batch Loss: 0.1962968409061432\n",
      "Epoch 2182, Loss: 0.45553891360759735, Final Batch Loss: 0.16374842822551727\n",
      "Epoch 2183, Loss: 0.42193709313869476, Final Batch Loss: 0.21269232034683228\n",
      "Epoch 2184, Loss: 0.3737553507089615, Final Batch Loss: 0.22573961317539215\n",
      "Epoch 2185, Loss: 0.36341698467731476, Final Batch Loss: 0.17399069666862488\n",
      "Epoch 2186, Loss: 0.5171559453010559, Final Batch Loss: 0.2246025800704956\n",
      "Epoch 2187, Loss: 0.4215008616447449, Final Batch Loss: 0.17073088884353638\n",
      "Epoch 2188, Loss: 0.4027721434831619, Final Batch Loss: 0.2450602799654007\n",
      "Epoch 2189, Loss: 0.4201020300388336, Final Batch Loss: 0.21675868332386017\n",
      "Epoch 2190, Loss: 0.37519729137420654, Final Batch Loss: 0.18688517808914185\n",
      "Epoch 2191, Loss: 0.4139302968978882, Final Batch Loss: 0.2034371793270111\n",
      "Epoch 2192, Loss: 0.3580104410648346, Final Batch Loss: 0.16872163116931915\n",
      "Epoch 2193, Loss: 0.3486115485429764, Final Batch Loss: 0.16933633387088776\n",
      "Epoch 2194, Loss: 0.43745391070842743, Final Batch Loss: 0.20354467630386353\n",
      "Epoch 2195, Loss: 0.45462383329868317, Final Batch Loss: 0.19350798428058624\n",
      "Epoch 2196, Loss: 0.38790711760520935, Final Batch Loss: 0.20241013169288635\n",
      "Epoch 2197, Loss: 0.37126466631889343, Final Batch Loss: 0.2105543166399002\n",
      "Epoch 2198, Loss: 0.3573978841304779, Final Batch Loss: 0.16568726301193237\n",
      "Epoch 2199, Loss: 0.38801607489585876, Final Batch Loss: 0.1838936060667038\n",
      "Epoch 2200, Loss: 0.38578732311725616, Final Batch Loss: 0.22561559081077576\n",
      "Epoch 2201, Loss: 0.3539412021636963, Final Batch Loss: 0.17366397380828857\n",
      "Epoch 2202, Loss: 0.4075164496898651, Final Batch Loss: 0.24718326330184937\n",
      "Epoch 2203, Loss: 0.4525355398654938, Final Batch Loss: 0.19479039311408997\n",
      "Epoch 2204, Loss: 0.367063969373703, Final Batch Loss: 0.17835241556167603\n",
      "Epoch 2205, Loss: 0.395088329911232, Final Batch Loss: 0.19627711176872253\n",
      "Epoch 2206, Loss: 0.40488313138484955, Final Batch Loss: 0.24121318757534027\n",
      "Epoch 2207, Loss: 0.4254220575094223, Final Batch Loss: 0.20418092608451843\n",
      "Epoch 2208, Loss: 0.3673698455095291, Final Batch Loss: 0.188587948679924\n",
      "Epoch 2209, Loss: 0.38348057866096497, Final Batch Loss: 0.20682071149349213\n",
      "Epoch 2210, Loss: 0.38824842870235443, Final Batch Loss: 0.18983830511569977\n",
      "Epoch 2211, Loss: 0.3802771121263504, Final Batch Loss: 0.17670489847660065\n",
      "Epoch 2212, Loss: 0.4220702052116394, Final Batch Loss: 0.1870052069425583\n",
      "Epoch 2213, Loss: 0.4137597680091858, Final Batch Loss: 0.21688981354236603\n",
      "Epoch 2214, Loss: 0.40193553268909454, Final Batch Loss: 0.20145577192306519\n",
      "Epoch 2215, Loss: 0.36942414939403534, Final Batch Loss: 0.1830340474843979\n",
      "Epoch 2216, Loss: 0.41980281472206116, Final Batch Loss: 0.19613079726696014\n",
      "Epoch 2217, Loss: 0.40600036084651947, Final Batch Loss: 0.21619419753551483\n",
      "Epoch 2218, Loss: 0.4178176075220108, Final Batch Loss: 0.17134186625480652\n",
      "Epoch 2219, Loss: 0.3778846263885498, Final Batch Loss: 0.19533699750900269\n",
      "Epoch 2220, Loss: 0.3959692269563675, Final Batch Loss: 0.1491076946258545\n",
      "Epoch 2221, Loss: 0.3449400067329407, Final Batch Loss: 0.1478634923696518\n",
      "Epoch 2222, Loss: 0.39865437150001526, Final Batch Loss: 0.19340279698371887\n",
      "Epoch 2223, Loss: 0.42172402143478394, Final Batch Loss: 0.2127692550420761\n",
      "Epoch 2224, Loss: 0.4046412259340286, Final Batch Loss: 0.18509216606616974\n",
      "Epoch 2225, Loss: 0.3741897791624069, Final Batch Loss: 0.2125580608844757\n",
      "Epoch 2226, Loss: 0.3971603065729141, Final Batch Loss: 0.16847799718379974\n",
      "Epoch 2227, Loss: 0.4395248740911484, Final Batch Loss: 0.21051740646362305\n",
      "Epoch 2228, Loss: 0.3617004007101059, Final Batch Loss: 0.17673246562480927\n",
      "Epoch 2229, Loss: 0.3727095276117325, Final Batch Loss: 0.19200459122657776\n",
      "Epoch 2230, Loss: 0.4106839895248413, Final Batch Loss: 0.23325392603874207\n",
      "Epoch 2231, Loss: 0.3906574547290802, Final Batch Loss: 0.16301266849040985\n",
      "Epoch 2232, Loss: 0.37621966004371643, Final Batch Loss: 0.17232166230678558\n",
      "Epoch 2233, Loss: 0.38480277359485626, Final Batch Loss: 0.1939375251531601\n",
      "Epoch 2234, Loss: 0.3720533847808838, Final Batch Loss: 0.17438149452209473\n",
      "Epoch 2235, Loss: 0.3708171099424362, Final Batch Loss: 0.1405128389596939\n",
      "Epoch 2236, Loss: 0.4822438061237335, Final Batch Loss: 0.27192971110343933\n",
      "Epoch 2237, Loss: 0.3435107171535492, Final Batch Loss: 0.19111116230487823\n",
      "Epoch 2238, Loss: 0.4040736109018326, Final Batch Loss: 0.2071608006954193\n",
      "Epoch 2239, Loss: 0.3873957544565201, Final Batch Loss: 0.17174698412418365\n",
      "Epoch 2240, Loss: 0.34379222989082336, Final Batch Loss: 0.13837899267673492\n",
      "Epoch 2241, Loss: 0.44627974927425385, Final Batch Loss: 0.21471555531024933\n",
      "Epoch 2242, Loss: 0.3834160566329956, Final Batch Loss: 0.21584872901439667\n",
      "Epoch 2243, Loss: 0.37060263752937317, Final Batch Loss: 0.21227477490901947\n",
      "Epoch 2244, Loss: 0.4245496392250061, Final Batch Loss: 0.1949661821126938\n",
      "Epoch 2245, Loss: 0.41758909821510315, Final Batch Loss: 0.20007717609405518\n",
      "Epoch 2246, Loss: 0.37272705137729645, Final Batch Loss: 0.18275555968284607\n",
      "Epoch 2247, Loss: 0.3758484721183777, Final Batch Loss: 0.17233745753765106\n",
      "Epoch 2248, Loss: 0.4389440566301346, Final Batch Loss: 0.2247135490179062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2249, Loss: 0.39492538571357727, Final Batch Loss: 0.1781361699104309\n",
      "Epoch 2250, Loss: 0.4054865539073944, Final Batch Loss: 0.20441509783267975\n",
      "Epoch 2251, Loss: 0.3778167963027954, Final Batch Loss: 0.1874784529209137\n",
      "Epoch 2252, Loss: 0.38659000396728516, Final Batch Loss: 0.19242601096630096\n",
      "Epoch 2253, Loss: 0.4056212455034256, Final Batch Loss: 0.22955311834812164\n",
      "Epoch 2254, Loss: 0.4426833838224411, Final Batch Loss: 0.24657925963401794\n",
      "Epoch 2255, Loss: 0.36706016957759857, Final Batch Loss: 0.15804849565029144\n",
      "Epoch 2256, Loss: 0.4464879781007767, Final Batch Loss: 0.21063677966594696\n",
      "Epoch 2257, Loss: 0.40465815365314484, Final Batch Loss: 0.24397575855255127\n",
      "Epoch 2258, Loss: 0.37150900065898895, Final Batch Loss: 0.14556193351745605\n",
      "Epoch 2259, Loss: 0.3390222191810608, Final Batch Loss: 0.16937224566936493\n",
      "Epoch 2260, Loss: 0.414779856801033, Final Batch Loss: 0.2235064059495926\n",
      "Epoch 2261, Loss: 0.40428730845451355, Final Batch Loss: 0.210567444562912\n",
      "Epoch 2262, Loss: 0.44433385133743286, Final Batch Loss: 0.20820197463035583\n",
      "Epoch 2263, Loss: 0.4241402745246887, Final Batch Loss: 0.20689347386360168\n",
      "Epoch 2264, Loss: 0.35020798444747925, Final Batch Loss: 0.18234354257583618\n",
      "Epoch 2265, Loss: 0.46521854400634766, Final Batch Loss: 0.263541579246521\n",
      "Epoch 2266, Loss: 0.35232438147068024, Final Batch Loss: 0.1683240681886673\n",
      "Epoch 2267, Loss: 0.4012768715620041, Final Batch Loss: 0.2048373520374298\n",
      "Epoch 2268, Loss: 0.4111671894788742, Final Batch Loss: 0.1948396861553192\n",
      "Epoch 2269, Loss: 0.3432377725839615, Final Batch Loss: 0.14857275784015656\n",
      "Epoch 2270, Loss: 0.3624519407749176, Final Batch Loss: 0.16641195118427277\n",
      "Epoch 2271, Loss: 0.34005822241306305, Final Batch Loss: 0.18096797168254852\n",
      "Epoch 2272, Loss: 0.35057662427425385, Final Batch Loss: 0.17281441390514374\n",
      "Epoch 2273, Loss: 0.36746932566165924, Final Batch Loss: 0.17758628726005554\n",
      "Epoch 2274, Loss: 0.41534706950187683, Final Batch Loss: 0.21715892851352692\n",
      "Epoch 2275, Loss: 0.40924447774887085, Final Batch Loss: 0.2051023244857788\n",
      "Epoch 2276, Loss: 0.37116214632987976, Final Batch Loss: 0.179803729057312\n",
      "Epoch 2277, Loss: 0.38053302466869354, Final Batch Loss: 0.18903858959674835\n",
      "Epoch 2278, Loss: 0.4152712821960449, Final Batch Loss: 0.2539055645465851\n",
      "Epoch 2279, Loss: 0.3621519207954407, Final Batch Loss: 0.17793570458889008\n",
      "Epoch 2280, Loss: 0.4017709195613861, Final Batch Loss: 0.20424236357212067\n",
      "Epoch 2281, Loss: 0.34858499467372894, Final Batch Loss: 0.22005027532577515\n",
      "Epoch 2282, Loss: 0.360930472612381, Final Batch Loss: 0.1739296168088913\n",
      "Epoch 2283, Loss: 0.3624122887849808, Final Batch Loss: 0.2082560807466507\n",
      "Epoch 2284, Loss: 0.39838258922100067, Final Batch Loss: 0.22487685084342957\n",
      "Epoch 2285, Loss: 0.39533254504203796, Final Batch Loss: 0.18203046917915344\n",
      "Epoch 2286, Loss: 0.4004197120666504, Final Batch Loss: 0.1987733691930771\n",
      "Epoch 2287, Loss: 0.3891136795282364, Final Batch Loss: 0.19753435254096985\n",
      "Epoch 2288, Loss: 0.3712993264198303, Final Batch Loss: 0.20605109632015228\n",
      "Epoch 2289, Loss: 0.40997327864170074, Final Batch Loss: 0.24417594075202942\n",
      "Epoch 2290, Loss: 0.3466494381427765, Final Batch Loss: 0.14149166643619537\n",
      "Epoch 2291, Loss: 0.40602248907089233, Final Batch Loss: 0.13783439993858337\n",
      "Epoch 2292, Loss: 0.40811391174793243, Final Batch Loss: 0.19372667372226715\n",
      "Epoch 2293, Loss: 0.39512521028518677, Final Batch Loss: 0.19762256741523743\n",
      "Epoch 2294, Loss: 0.36261869966983795, Final Batch Loss: 0.17882810533046722\n",
      "Epoch 2295, Loss: 0.3615366816520691, Final Batch Loss: 0.18500521779060364\n",
      "Epoch 2296, Loss: 0.3951287716627121, Final Batch Loss: 0.21198226511478424\n",
      "Epoch 2297, Loss: 0.4415050745010376, Final Batch Loss: 0.23939281702041626\n",
      "Epoch 2298, Loss: 0.42390429973602295, Final Batch Loss: 0.19847752153873444\n",
      "Epoch 2299, Loss: 0.4387473613023758, Final Batch Loss: 0.19537118077278137\n",
      "Epoch 2300, Loss: 0.3654400259256363, Final Batch Loss: 0.1967543363571167\n",
      "Epoch 2301, Loss: 0.39256586134433746, Final Batch Loss: 0.1994006186723709\n",
      "Epoch 2302, Loss: 0.33843374252319336, Final Batch Loss: 0.1598592847585678\n",
      "Epoch 2303, Loss: 0.3942307233810425, Final Batch Loss: 0.17998407781124115\n",
      "Epoch 2304, Loss: 0.3370501846075058, Final Batch Loss: 0.14645327627658844\n",
      "Epoch 2305, Loss: 0.36399489641189575, Final Batch Loss: 0.1844937950372696\n",
      "Epoch 2306, Loss: 0.3808583915233612, Final Batch Loss: 0.1949007511138916\n",
      "Epoch 2307, Loss: 0.37079155445098877, Final Batch Loss: 0.18810200691223145\n",
      "Epoch 2308, Loss: 0.409427285194397, Final Batch Loss: 0.20365430414676666\n",
      "Epoch 2309, Loss: 0.42284417152404785, Final Batch Loss: 0.17621372640132904\n",
      "Epoch 2310, Loss: 0.32962578535079956, Final Batch Loss: 0.14371243119239807\n",
      "Epoch 2311, Loss: 0.39649735391139984, Final Batch Loss: 0.21312206983566284\n",
      "Epoch 2312, Loss: 0.4058302193880081, Final Batch Loss: 0.20907679200172424\n",
      "Epoch 2313, Loss: 0.4023667871952057, Final Batch Loss: 0.18811921775341034\n",
      "Epoch 2314, Loss: 0.3635517954826355, Final Batch Loss: 0.17830109596252441\n",
      "Epoch 2315, Loss: 0.3883717954158783, Final Batch Loss: 0.19690413773059845\n",
      "Epoch 2316, Loss: 0.2974739372730255, Final Batch Loss: 0.13684672117233276\n",
      "Epoch 2317, Loss: 0.3940952718257904, Final Batch Loss: 0.20535306632518768\n",
      "Epoch 2318, Loss: 0.37894418835639954, Final Batch Loss: 0.18123474717140198\n",
      "Epoch 2319, Loss: 0.3569321185350418, Final Batch Loss: 0.1579345017671585\n",
      "Epoch 2320, Loss: 0.3308912217617035, Final Batch Loss: 0.153274267911911\n",
      "Epoch 2321, Loss: 0.35864226520061493, Final Batch Loss: 0.16875286400318146\n",
      "Epoch 2322, Loss: 0.4041542261838913, Final Batch Loss: 0.22683702409267426\n",
      "Epoch 2323, Loss: 0.3447924554347992, Final Batch Loss: 0.1794525682926178\n",
      "Epoch 2324, Loss: 0.44098761677742004, Final Batch Loss: 0.23854604363441467\n",
      "Epoch 2325, Loss: 0.3595924377441406, Final Batch Loss: 0.19044984877109528\n",
      "Epoch 2326, Loss: 0.35733336210250854, Final Batch Loss: 0.19582499563694\n",
      "Epoch 2327, Loss: 0.4273037612438202, Final Batch Loss: 0.1979609578847885\n",
      "Epoch 2328, Loss: 0.38432030379772186, Final Batch Loss: 0.19462771713733673\n",
      "Epoch 2329, Loss: 0.39390118420124054, Final Batch Loss: 0.20266930758953094\n",
      "Epoch 2330, Loss: 0.4263913184404373, Final Batch Loss: 0.22715425491333008\n",
      "Epoch 2331, Loss: 0.32533030211925507, Final Batch Loss: 0.1617884635925293\n",
      "Epoch 2332, Loss: 0.29704661667346954, Final Batch Loss: 0.14564955234527588\n",
      "Epoch 2333, Loss: 0.38769590854644775, Final Batch Loss: 0.20817700028419495\n",
      "Epoch 2334, Loss: 0.4414609223604202, Final Batch Loss: 0.22541804611682892\n",
      "Epoch 2335, Loss: 0.3277958333492279, Final Batch Loss: 0.18126806616783142\n",
      "Epoch 2336, Loss: 0.3898441940546036, Final Batch Loss: 0.20149113237857819\n",
      "Epoch 2337, Loss: 0.3434545546770096, Final Batch Loss: 0.16104140877723694\n",
      "Epoch 2338, Loss: 0.38031405210494995, Final Batch Loss: 0.15848664939403534\n",
      "Epoch 2339, Loss: 0.391314834356308, Final Batch Loss: 0.18388885259628296\n",
      "Epoch 2340, Loss: 0.3652821183204651, Final Batch Loss: 0.183970645070076\n",
      "Epoch 2341, Loss: 0.3665151298046112, Final Batch Loss: 0.16525113582611084\n",
      "Epoch 2342, Loss: 0.41568027436733246, Final Batch Loss: 0.16761630773544312\n",
      "Epoch 2343, Loss: 0.34164491295814514, Final Batch Loss: 0.18799333274364471\n",
      "Epoch 2344, Loss: 0.36257484555244446, Final Batch Loss: 0.1633489429950714\n",
      "Epoch 2345, Loss: 0.35778962075710297, Final Batch Loss: 0.14662504196166992\n",
      "Epoch 2346, Loss: 0.4141300469636917, Final Batch Loss: 0.16431425511837006\n",
      "Epoch 2347, Loss: 0.3609292507171631, Final Batch Loss: 0.1856347769498825\n",
      "Epoch 2348, Loss: 0.3633529394865036, Final Batch Loss: 0.19426952302455902\n",
      "Epoch 2349, Loss: 0.3945259749889374, Final Batch Loss: 0.213272362947464\n",
      "Epoch 2350, Loss: 0.4169599115848541, Final Batch Loss: 0.24280881881713867\n",
      "Epoch 2351, Loss: 0.3576151430606842, Final Batch Loss: 0.188728928565979\n",
      "Epoch 2352, Loss: 0.33785194158554077, Final Batch Loss: 0.17538337409496307\n",
      "Epoch 2353, Loss: 0.388492226600647, Final Batch Loss: 0.1932787001132965\n",
      "Epoch 2354, Loss: 0.39385010302066803, Final Batch Loss: 0.2051931917667389\n",
      "Epoch 2355, Loss: 0.3732598274946213, Final Batch Loss: 0.1584346443414688\n",
      "Epoch 2356, Loss: 0.37618371844291687, Final Batch Loss: 0.1765613704919815\n",
      "Epoch 2357, Loss: 0.38905391097068787, Final Batch Loss: 0.1873631477355957\n",
      "Epoch 2358, Loss: 0.3649301528930664, Final Batch Loss: 0.2194218933582306\n",
      "Epoch 2359, Loss: 0.340921550989151, Final Batch Loss: 0.15415889024734497\n",
      "Epoch 2360, Loss: 0.4191342741250992, Final Batch Loss: 0.2210935354232788\n",
      "Epoch 2361, Loss: 0.430629163980484, Final Batch Loss: 0.16468483209609985\n",
      "Epoch 2362, Loss: 0.399151012301445, Final Batch Loss: 0.1771661788225174\n",
      "Epoch 2363, Loss: 0.3146122545003891, Final Batch Loss: 0.17836037278175354\n",
      "Epoch 2364, Loss: 0.33669613301754, Final Batch Loss: 0.145885169506073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2365, Loss: 0.4030376970767975, Final Batch Loss: 0.22214166820049286\n",
      "Epoch 2366, Loss: 0.46477679908275604, Final Batch Loss: 0.2638775706291199\n",
      "Epoch 2367, Loss: 0.40770484507083893, Final Batch Loss: 0.1958509236574173\n",
      "Epoch 2368, Loss: 0.38513483107089996, Final Batch Loss: 0.1838829517364502\n",
      "Epoch 2369, Loss: 0.34280209243297577, Final Batch Loss: 0.1895027607679367\n",
      "Epoch 2370, Loss: 0.3659297227859497, Final Batch Loss: 0.18183545768260956\n",
      "Epoch 2371, Loss: 0.4249420315027237, Final Batch Loss: 0.26182442903518677\n",
      "Epoch 2372, Loss: 0.3715474605560303, Final Batch Loss: 0.20348568260669708\n",
      "Epoch 2373, Loss: 0.36037424206733704, Final Batch Loss: 0.1878751963376999\n",
      "Epoch 2374, Loss: 0.39685823023319244, Final Batch Loss: 0.17707517743110657\n",
      "Epoch 2375, Loss: 0.4131740927696228, Final Batch Loss: 0.23965519666671753\n",
      "Epoch 2376, Loss: 0.4373796582221985, Final Batch Loss: 0.2625510096549988\n",
      "Epoch 2377, Loss: 0.36774277687072754, Final Batch Loss: 0.18365691602230072\n",
      "Epoch 2378, Loss: 0.36798861622810364, Final Batch Loss: 0.16245341300964355\n",
      "Epoch 2379, Loss: 0.3886271119117737, Final Batch Loss: 0.17957259714603424\n",
      "Epoch 2380, Loss: 0.40246713161468506, Final Batch Loss: 0.15897692739963531\n",
      "Epoch 2381, Loss: 0.36554108560085297, Final Batch Loss: 0.20426374673843384\n",
      "Epoch 2382, Loss: 0.3941619396209717, Final Batch Loss: 0.19901402294635773\n",
      "Epoch 2383, Loss: 0.3400891572237015, Final Batch Loss: 0.16801433265209198\n",
      "Epoch 2384, Loss: 0.4280199408531189, Final Batch Loss: 0.20067977905273438\n",
      "Epoch 2385, Loss: 0.33930812776088715, Final Batch Loss: 0.1650809794664383\n",
      "Epoch 2386, Loss: 0.43204261362552643, Final Batch Loss: 0.20785555243492126\n",
      "Epoch 2387, Loss: 0.41645874083042145, Final Batch Loss: 0.21752265095710754\n",
      "Epoch 2388, Loss: 0.3619206100702286, Final Batch Loss: 0.1690203845500946\n",
      "Epoch 2389, Loss: 0.3561414033174515, Final Batch Loss: 0.1747507005929947\n",
      "Epoch 2390, Loss: 0.3767209053039551, Final Batch Loss: 0.1767088770866394\n",
      "Epoch 2391, Loss: 0.3408919870853424, Final Batch Loss: 0.1702105700969696\n",
      "Epoch 2392, Loss: 0.38186030089855194, Final Batch Loss: 0.19412678480148315\n",
      "Epoch 2393, Loss: 0.29884253442287445, Final Batch Loss: 0.14626452326774597\n",
      "Epoch 2394, Loss: 0.30679452419281006, Final Batch Loss: 0.13973429799079895\n",
      "Epoch 2395, Loss: 0.3711552768945694, Final Batch Loss: 0.1806066334247589\n",
      "Epoch 2396, Loss: 0.3424629718065262, Final Batch Loss: 0.14431923627853394\n",
      "Epoch 2397, Loss: 0.4239026755094528, Final Batch Loss: 0.19752483069896698\n",
      "Epoch 2398, Loss: 0.3657611757516861, Final Batch Loss: 0.18272168934345245\n",
      "Epoch 2399, Loss: 0.3428954929113388, Final Batch Loss: 0.15981122851371765\n",
      "Epoch 2400, Loss: 0.33016806840896606, Final Batch Loss: 0.17012712359428406\n",
      "Epoch 2401, Loss: 0.34189197421073914, Final Batch Loss: 0.1485699862241745\n",
      "Epoch 2402, Loss: 0.3912974148988724, Final Batch Loss: 0.1636059433221817\n",
      "Epoch 2403, Loss: 0.3742738515138626, Final Batch Loss: 0.17230816185474396\n",
      "Epoch 2404, Loss: 0.3304087966680527, Final Batch Loss: 0.16066807508468628\n",
      "Epoch 2405, Loss: 0.36211802065372467, Final Batch Loss: 0.17100019752979279\n",
      "Epoch 2406, Loss: 0.38562776148319244, Final Batch Loss: 0.15364541113376617\n",
      "Epoch 2407, Loss: 0.322248637676239, Final Batch Loss: 0.14730064570903778\n",
      "Epoch 2408, Loss: 0.350505530834198, Final Batch Loss: 0.2225150465965271\n",
      "Epoch 2409, Loss: 0.3294006288051605, Final Batch Loss: 0.195979505777359\n",
      "Epoch 2410, Loss: 0.4189039617776871, Final Batch Loss: 0.26425701379776\n",
      "Epoch 2411, Loss: 0.37607529759407043, Final Batch Loss: 0.19609203934669495\n",
      "Epoch 2412, Loss: 0.37669816613197327, Final Batch Loss: 0.1859593689441681\n",
      "Epoch 2413, Loss: 0.3516538292169571, Final Batch Loss: 0.18774832785129547\n",
      "Epoch 2414, Loss: 0.3023463785648346, Final Batch Loss: 0.14061246812343597\n",
      "Epoch 2415, Loss: 0.3605165481567383, Final Batch Loss: 0.19522632658481598\n",
      "Epoch 2416, Loss: 0.37349243462085724, Final Batch Loss: 0.15776485204696655\n",
      "Epoch 2417, Loss: 0.41547878086566925, Final Batch Loss: 0.22913561761379242\n",
      "Epoch 2418, Loss: 0.33615899085998535, Final Batch Loss: 0.15955141186714172\n",
      "Epoch 2419, Loss: 0.3500436395406723, Final Batch Loss: 0.1843036562204361\n",
      "Epoch 2420, Loss: 0.3628917634487152, Final Batch Loss: 0.19941739737987518\n",
      "Epoch 2421, Loss: 0.379646435379982, Final Batch Loss: 0.16378241777420044\n",
      "Epoch 2422, Loss: 0.3798031806945801, Final Batch Loss: 0.19560182094573975\n",
      "Epoch 2423, Loss: 0.3652769923210144, Final Batch Loss: 0.18252205848693848\n",
      "Epoch 2424, Loss: 0.4246409386396408, Final Batch Loss: 0.25699204206466675\n",
      "Epoch 2425, Loss: 0.37815579771995544, Final Batch Loss: 0.18487825989723206\n",
      "Epoch 2426, Loss: 0.38809528946876526, Final Batch Loss: 0.20451931655406952\n",
      "Epoch 2427, Loss: 0.40160971879959106, Final Batch Loss: 0.233488991856575\n",
      "Epoch 2428, Loss: 0.4194034934043884, Final Batch Loss: 0.23825858533382416\n",
      "Epoch 2429, Loss: 0.359636515378952, Final Batch Loss: 0.16818004846572876\n",
      "Epoch 2430, Loss: 0.39909133315086365, Final Batch Loss: 0.19749344885349274\n",
      "Epoch 2431, Loss: 0.33291031420230865, Final Batch Loss: 0.1554981768131256\n",
      "Epoch 2432, Loss: 0.39771217107772827, Final Batch Loss: 0.17360343039035797\n",
      "Epoch 2433, Loss: 0.3981069177389145, Final Batch Loss: 0.18129822611808777\n",
      "Epoch 2434, Loss: 0.3950835168361664, Final Batch Loss: 0.20360149443149567\n",
      "Epoch 2435, Loss: 0.421991765499115, Final Batch Loss: 0.16415351629257202\n",
      "Epoch 2436, Loss: 0.36427851021289825, Final Batch Loss: 0.1962043195962906\n",
      "Epoch 2437, Loss: 0.34873074293136597, Final Batch Loss: 0.16805337369441986\n",
      "Epoch 2438, Loss: 0.4205898642539978, Final Batch Loss: 0.21353931725025177\n",
      "Epoch 2439, Loss: 0.38351693749427795, Final Batch Loss: 0.17829535901546478\n",
      "Epoch 2440, Loss: 0.37310691177845, Final Batch Loss: 0.20801521837711334\n",
      "Epoch 2441, Loss: 0.3376849889755249, Final Batch Loss: 0.1994200497865677\n",
      "Epoch 2442, Loss: 0.35941898822784424, Final Batch Loss: 0.18523313105106354\n",
      "Epoch 2443, Loss: 0.3698981702327728, Final Batch Loss: 0.21351532638072968\n",
      "Epoch 2444, Loss: 0.41867077350616455, Final Batch Loss: 0.19059476256370544\n",
      "Epoch 2445, Loss: 0.36301635205745697, Final Batch Loss: 0.1657179445028305\n",
      "Epoch 2446, Loss: 0.388087198138237, Final Batch Loss: 0.22212553024291992\n",
      "Epoch 2447, Loss: 0.3014623820781708, Final Batch Loss: 0.1305198222398758\n",
      "Epoch 2448, Loss: 0.3803059309720993, Final Batch Loss: 0.16975927352905273\n",
      "Epoch 2449, Loss: 0.37557975947856903, Final Batch Loss: 0.18693284690380096\n",
      "Epoch 2450, Loss: 0.3478008508682251, Final Batch Loss: 0.16798676550388336\n",
      "Epoch 2451, Loss: 0.3806535601615906, Final Batch Loss: 0.18441897630691528\n",
      "Epoch 2452, Loss: 0.3958584815263748, Final Batch Loss: 0.2091006189584732\n",
      "Epoch 2453, Loss: 0.34137019515037537, Final Batch Loss: 0.20642776787281036\n",
      "Epoch 2454, Loss: 0.33654001355171204, Final Batch Loss: 0.165251225233078\n",
      "Epoch 2455, Loss: 0.3395938128232956, Final Batch Loss: 0.16686271131038666\n",
      "Epoch 2456, Loss: 0.39390453696250916, Final Batch Loss: 0.16069857776165009\n",
      "Epoch 2457, Loss: 0.35680094361305237, Final Batch Loss: 0.1731952279806137\n",
      "Epoch 2458, Loss: 0.4050113707780838, Final Batch Loss: 0.19930656254291534\n",
      "Epoch 2459, Loss: 0.43716177344322205, Final Batch Loss: 0.24160124361515045\n",
      "Epoch 2460, Loss: 0.32343393564224243, Final Batch Loss: 0.1950458586215973\n",
      "Epoch 2461, Loss: 0.44001658260822296, Final Batch Loss: 0.22992411255836487\n",
      "Epoch 2462, Loss: 0.37504592537879944, Final Batch Loss: 0.14600659906864166\n",
      "Epoch 2463, Loss: 0.3626967668533325, Final Batch Loss: 0.17772133648395538\n",
      "Epoch 2464, Loss: 0.36122168600559235, Final Batch Loss: 0.20271918177604675\n",
      "Epoch 2465, Loss: 0.38174016773700714, Final Batch Loss: 0.18709377944469452\n",
      "Epoch 2466, Loss: 0.33582229912281036, Final Batch Loss: 0.1594059020280838\n",
      "Epoch 2467, Loss: 0.3077313154935837, Final Batch Loss: 0.17719046771526337\n",
      "Epoch 2468, Loss: 0.3535415232181549, Final Batch Loss: 0.19254162907600403\n",
      "Epoch 2469, Loss: 0.3576224595308304, Final Batch Loss: 0.21041461825370789\n",
      "Epoch 2470, Loss: 0.30536312609910965, Final Batch Loss: 0.12321939319372177\n",
      "Epoch 2471, Loss: 0.37652088701725006, Final Batch Loss: 0.2166079729795456\n",
      "Epoch 2472, Loss: 0.4039316773414612, Final Batch Loss: 0.23225222527980804\n",
      "Epoch 2473, Loss: 0.2998795509338379, Final Batch Loss: 0.15336620807647705\n",
      "Epoch 2474, Loss: 0.37611024081707, Final Batch Loss: 0.1878267526626587\n",
      "Epoch 2475, Loss: 0.3427942991256714, Final Batch Loss: 0.1419772356748581\n",
      "Epoch 2476, Loss: 0.39985693991184235, Final Batch Loss: 0.17596177756786346\n",
      "Epoch 2477, Loss: 0.45174555480480194, Final Batch Loss: 0.23801189661026\n",
      "Epoch 2478, Loss: 0.3520991951227188, Final Batch Loss: 0.1625666320323944\n",
      "Epoch 2479, Loss: 0.32672610878944397, Final Batch Loss: 0.18139581382274628\n",
      "Epoch 2480, Loss: 0.3711392730474472, Final Batch Loss: 0.1732589155435562\n",
      "Epoch 2481, Loss: 0.3154730498790741, Final Batch Loss: 0.17729967832565308\n",
      "Epoch 2482, Loss: 0.3757519870996475, Final Batch Loss: 0.17683595418930054\n",
      "Epoch 2483, Loss: 0.3484466075897217, Final Batch Loss: 0.22028763592243195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2484, Loss: 0.35512208938598633, Final Batch Loss: 0.1638847291469574\n",
      "Epoch 2485, Loss: 0.4096771627664566, Final Batch Loss: 0.1752110719680786\n",
      "Epoch 2486, Loss: 0.3664657175540924, Final Batch Loss: 0.1854916363954544\n",
      "Epoch 2487, Loss: 0.3442942798137665, Final Batch Loss: 0.16222137212753296\n",
      "Epoch 2488, Loss: 0.4123608469963074, Final Batch Loss: 0.2114529013633728\n",
      "Epoch 2489, Loss: 0.3335421681404114, Final Batch Loss: 0.1369047313928604\n",
      "Epoch 2490, Loss: 0.3166521042585373, Final Batch Loss: 0.1665111631155014\n",
      "Epoch 2491, Loss: 0.35917772352695465, Final Batch Loss: 0.16881069540977478\n",
      "Epoch 2492, Loss: 0.3086736649274826, Final Batch Loss: 0.12820398807525635\n",
      "Epoch 2493, Loss: 0.31909511983394623, Final Batch Loss: 0.1472058743238449\n",
      "Epoch 2494, Loss: 0.36977407336235046, Final Batch Loss: 0.1857854425907135\n",
      "Epoch 2495, Loss: 0.39745742082595825, Final Batch Loss: 0.14593493938446045\n",
      "Epoch 2496, Loss: 0.34836599230766296, Final Batch Loss: 0.18551206588745117\n",
      "Epoch 2497, Loss: 0.33099475502967834, Final Batch Loss: 0.14581230282783508\n",
      "Epoch 2498, Loss: 0.37828776240348816, Final Batch Loss: 0.18899518251419067\n",
      "Epoch 2499, Loss: 0.34757785499095917, Final Batch Loss: 0.15258890390396118\n",
      "Epoch 2500, Loss: 0.4073010981082916, Final Batch Loss: 0.21213145554065704\n",
      "Epoch 2501, Loss: 0.2890048027038574, Final Batch Loss: 0.15746891498565674\n",
      "Epoch 2502, Loss: 0.334111288189888, Final Batch Loss: 0.17538954317569733\n",
      "Epoch 2503, Loss: 0.40134397149086, Final Batch Loss: 0.23957134783267975\n",
      "Epoch 2504, Loss: 0.42119309306144714, Final Batch Loss: 0.21344734728336334\n",
      "Epoch 2505, Loss: 0.3152831196784973, Final Batch Loss: 0.15303537249565125\n",
      "Epoch 2506, Loss: 0.3354189246892929, Final Batch Loss: 0.18343108892440796\n",
      "Epoch 2507, Loss: 0.34894120693206787, Final Batch Loss: 0.19121675193309784\n",
      "Epoch 2508, Loss: 0.38305485248565674, Final Batch Loss: 0.20364505052566528\n",
      "Epoch 2509, Loss: 0.37795490026474, Final Batch Loss: 0.19129884243011475\n",
      "Epoch 2510, Loss: 0.4214169681072235, Final Batch Loss: 0.20189784467220306\n",
      "Epoch 2511, Loss: 0.3486435115337372, Final Batch Loss: 0.18927828967571259\n",
      "Epoch 2512, Loss: 0.35597115755081177, Final Batch Loss: 0.17819306254386902\n",
      "Epoch 2513, Loss: 0.3197629228234291, Final Batch Loss: 0.20230086147785187\n",
      "Epoch 2514, Loss: 0.41400621831417084, Final Batch Loss: 0.20088188350200653\n",
      "Epoch 2515, Loss: 0.3632969260215759, Final Batch Loss: 0.212199404835701\n",
      "Epoch 2516, Loss: 0.28540681302547455, Final Batch Loss: 0.14620669186115265\n",
      "Epoch 2517, Loss: 0.39923515915870667, Final Batch Loss: 0.181150421500206\n",
      "Epoch 2518, Loss: 0.384474441409111, Final Batch Loss: 0.23037627339363098\n",
      "Epoch 2519, Loss: 0.34516365826129913, Final Batch Loss: 0.176337331533432\n",
      "Epoch 2520, Loss: 0.4047471433877945, Final Batch Loss: 0.18030135333538055\n",
      "Epoch 2521, Loss: 0.33801354467868805, Final Batch Loss: 0.19028431177139282\n",
      "Epoch 2522, Loss: 0.3307144343852997, Final Batch Loss: 0.1864934116601944\n",
      "Epoch 2523, Loss: 0.32649749517440796, Final Batch Loss: 0.16564688086509705\n",
      "Epoch 2524, Loss: 0.35656823217868805, Final Batch Loss: 0.17833612859249115\n",
      "Epoch 2525, Loss: 0.37421004474163055, Final Batch Loss: 0.18867851793766022\n",
      "Epoch 2526, Loss: 0.41506603360176086, Final Batch Loss: 0.22766132652759552\n",
      "Epoch 2527, Loss: 0.4001413434743881, Final Batch Loss: 0.21679449081420898\n",
      "Epoch 2528, Loss: 0.3014809489250183, Final Batch Loss: 0.14731445908546448\n",
      "Epoch 2529, Loss: 0.3330240398645401, Final Batch Loss: 0.1605454534292221\n",
      "Epoch 2530, Loss: 0.42442625761032104, Final Batch Loss: 0.18815341591835022\n",
      "Epoch 2531, Loss: 0.3546954393386841, Final Batch Loss: 0.19600611925125122\n",
      "Epoch 2532, Loss: 0.2989780753850937, Final Batch Loss: 0.15082240104675293\n",
      "Epoch 2533, Loss: 0.34902432560920715, Final Batch Loss: 0.16079671680927277\n",
      "Epoch 2534, Loss: 0.3920648843050003, Final Batch Loss: 0.1716052144765854\n",
      "Epoch 2535, Loss: 0.3750555217266083, Final Batch Loss: 0.2265181541442871\n",
      "Epoch 2536, Loss: 0.33498167991638184, Final Batch Loss: 0.19770129024982452\n",
      "Epoch 2537, Loss: 0.3204762637615204, Final Batch Loss: 0.15262728929519653\n",
      "Epoch 2538, Loss: 0.3408622443675995, Final Batch Loss: 0.17589929699897766\n",
      "Epoch 2539, Loss: 0.36714984476566315, Final Batch Loss: 0.1722220778465271\n",
      "Epoch 2540, Loss: 0.4309411346912384, Final Batch Loss: 0.2603416442871094\n",
      "Epoch 2541, Loss: 0.3234116733074188, Final Batch Loss: 0.18932995200157166\n",
      "Epoch 2542, Loss: 0.3372069001197815, Final Batch Loss: 0.13826264441013336\n",
      "Epoch 2543, Loss: 0.33663831651210785, Final Batch Loss: 0.18790864944458008\n",
      "Epoch 2544, Loss: 0.30765901505947113, Final Batch Loss: 0.15936219692230225\n",
      "Epoch 2545, Loss: 0.325191468000412, Final Batch Loss: 0.18042220175266266\n",
      "Epoch 2546, Loss: 0.3285435289144516, Final Batch Loss: 0.16359470784664154\n",
      "Epoch 2547, Loss: 0.40099993348121643, Final Batch Loss: 0.2095036506652832\n",
      "Epoch 2548, Loss: 0.3755231648683548, Final Batch Loss: 0.21225664019584656\n",
      "Epoch 2549, Loss: 0.30494165420532227, Final Batch Loss: 0.15508514642715454\n",
      "Epoch 2550, Loss: 0.31681106984615326, Final Batch Loss: 0.15738484263420105\n",
      "Epoch 2551, Loss: 0.3585897535085678, Final Batch Loss: 0.2079731971025467\n",
      "Epoch 2552, Loss: 0.32445840537548065, Final Batch Loss: 0.15590065717697144\n",
      "Epoch 2553, Loss: 0.3614788204431534, Final Batch Loss: 0.19325345754623413\n",
      "Epoch 2554, Loss: 0.35385584831237793, Final Batch Loss: 0.21376848220825195\n",
      "Epoch 2555, Loss: 0.32710961997509, Final Batch Loss: 0.14993835985660553\n",
      "Epoch 2556, Loss: 0.3292785733938217, Final Batch Loss: 0.18956604599952698\n",
      "Epoch 2557, Loss: 0.366472989320755, Final Batch Loss: 0.195143461227417\n",
      "Epoch 2558, Loss: 0.33176374435424805, Final Batch Loss: 0.1279110610485077\n",
      "Epoch 2559, Loss: 0.3296627253293991, Final Batch Loss: 0.18058384954929352\n",
      "Epoch 2560, Loss: 0.325466588139534, Final Batch Loss: 0.16505801677703857\n",
      "Epoch 2561, Loss: 0.3352652043104172, Final Batch Loss: 0.125288724899292\n",
      "Epoch 2562, Loss: 0.34922976791858673, Final Batch Loss: 0.17139001190662384\n",
      "Epoch 2563, Loss: 0.32467077672481537, Final Batch Loss: 0.14781881868839264\n",
      "Epoch 2564, Loss: 0.3961358815431595, Final Batch Loss: 0.23535731434822083\n",
      "Epoch 2565, Loss: 0.4118556082248688, Final Batch Loss: 0.22677497565746307\n",
      "Epoch 2566, Loss: 0.33932386338710785, Final Batch Loss: 0.18579815328121185\n",
      "Epoch 2567, Loss: 0.3526526391506195, Final Batch Loss: 0.17101751267910004\n",
      "Epoch 2568, Loss: 0.31660042703151703, Final Batch Loss: 0.1284031718969345\n",
      "Epoch 2569, Loss: 0.33264440298080444, Final Batch Loss: 0.1619836986064911\n",
      "Epoch 2570, Loss: 0.33583322167396545, Final Batch Loss: 0.1699250042438507\n",
      "Epoch 2571, Loss: 0.34932075440883636, Final Batch Loss: 0.15571220219135284\n",
      "Epoch 2572, Loss: 0.35986508429050446, Final Batch Loss: 0.17426732182502747\n",
      "Epoch 2573, Loss: 0.3835524916648865, Final Batch Loss: 0.21010911464691162\n",
      "Epoch 2574, Loss: 0.3136971592903137, Final Batch Loss: 0.14265850186347961\n",
      "Epoch 2575, Loss: 0.37140554189682007, Final Batch Loss: 0.17543579638004303\n",
      "Epoch 2576, Loss: 0.34666022658348083, Final Batch Loss: 0.17995521426200867\n",
      "Epoch 2577, Loss: 0.3146512061357498, Final Batch Loss: 0.1381177455186844\n",
      "Epoch 2578, Loss: 0.35495124757289886, Final Batch Loss: 0.1766880452632904\n",
      "Epoch 2579, Loss: 0.41957277059555054, Final Batch Loss: 0.2099878489971161\n",
      "Epoch 2580, Loss: 0.3586357682943344, Final Batch Loss: 0.1787242442369461\n",
      "Epoch 2581, Loss: 0.34319430589675903, Final Batch Loss: 0.14805381000041962\n",
      "Epoch 2582, Loss: 0.2954951673746109, Final Batch Loss: 0.13730430603027344\n",
      "Epoch 2583, Loss: 0.3718659281730652, Final Batch Loss: 0.18608811497688293\n",
      "Epoch 2584, Loss: 0.35851433873176575, Final Batch Loss: 0.1599445790052414\n",
      "Epoch 2585, Loss: 0.32466618716716766, Final Batch Loss: 0.16070899367332458\n",
      "Epoch 2586, Loss: 0.3753371238708496, Final Batch Loss: 0.1713007092475891\n",
      "Epoch 2587, Loss: 0.30473071336746216, Final Batch Loss: 0.15369968116283417\n",
      "Epoch 2588, Loss: 0.3682161718606949, Final Batch Loss: 0.1637623906135559\n",
      "Epoch 2589, Loss: 0.3294915407896042, Final Batch Loss: 0.19211791455745697\n",
      "Epoch 2590, Loss: 0.32044582068920135, Final Batch Loss: 0.14853933453559875\n",
      "Epoch 2591, Loss: 0.3560062050819397, Final Batch Loss: 0.20874662697315216\n",
      "Epoch 2592, Loss: 0.3326687067747116, Final Batch Loss: 0.1753074675798416\n",
      "Epoch 2593, Loss: 0.3351625055074692, Final Batch Loss: 0.16401223838329315\n",
      "Epoch 2594, Loss: 0.30889660120010376, Final Batch Loss: 0.15803521871566772\n",
      "Epoch 2595, Loss: 0.37081027030944824, Final Batch Loss: 0.13922280073165894\n",
      "Epoch 2596, Loss: 0.322900652885437, Final Batch Loss: 0.15846213698387146\n",
      "Epoch 2597, Loss: 0.3686683773994446, Final Batch Loss: 0.1400444656610489\n",
      "Epoch 2598, Loss: 0.3461065739393234, Final Batch Loss: 0.14919959008693695\n",
      "Epoch 2599, Loss: 0.3571205735206604, Final Batch Loss: 0.15583427250385284\n",
      "Epoch 2600, Loss: 0.3481359928846359, Final Batch Loss: 0.158813014626503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2601, Loss: 0.30984972417354584, Final Batch Loss: 0.17553362250328064\n",
      "Epoch 2602, Loss: 0.32930703461170197, Final Batch Loss: 0.1641845405101776\n",
      "Epoch 2603, Loss: 0.3137015402317047, Final Batch Loss: 0.17789475619792938\n",
      "Epoch 2604, Loss: 0.40843240916728973, Final Batch Loss: 0.24102602899074554\n",
      "Epoch 2605, Loss: 0.3696846663951874, Final Batch Loss: 0.1958465278148651\n",
      "Epoch 2606, Loss: 0.3454463928937912, Final Batch Loss: 0.17257054150104523\n",
      "Epoch 2607, Loss: 0.3294798880815506, Final Batch Loss: 0.18134166300296783\n",
      "Epoch 2608, Loss: 0.2860058695077896, Final Batch Loss: 0.11761133372783661\n",
      "Epoch 2609, Loss: 0.37164999544620514, Final Batch Loss: 0.1725388616323471\n",
      "Epoch 2610, Loss: 0.3451463580131531, Final Batch Loss: 0.1838388293981552\n",
      "Epoch 2611, Loss: 0.33608144521713257, Final Batch Loss: 0.14998884499073029\n",
      "Epoch 2612, Loss: 0.36770835518836975, Final Batch Loss: 0.1724516600370407\n",
      "Epoch 2613, Loss: 0.2930239289999008, Final Batch Loss: 0.17114701867103577\n",
      "Epoch 2614, Loss: 0.27768324315547943, Final Batch Loss: 0.14817769825458527\n",
      "Epoch 2615, Loss: 0.3565405607223511, Final Batch Loss: 0.18107426166534424\n",
      "Epoch 2616, Loss: 0.3219304382801056, Final Batch Loss: 0.19026066362857819\n",
      "Epoch 2617, Loss: 0.3299672454595566, Final Batch Loss: 0.1530168503522873\n",
      "Epoch 2618, Loss: 0.3629338890314102, Final Batch Loss: 0.17327557504177094\n",
      "Epoch 2619, Loss: 0.30919237434864044, Final Batch Loss: 0.15038864314556122\n",
      "Epoch 2620, Loss: 0.3076557070016861, Final Batch Loss: 0.1663951426744461\n",
      "Epoch 2621, Loss: 0.3368068337440491, Final Batch Loss: 0.19428198039531708\n",
      "Epoch 2622, Loss: 0.3033967614173889, Final Batch Loss: 0.1400117576122284\n",
      "Epoch 2623, Loss: 0.28314080834388733, Final Batch Loss: 0.12290017306804657\n",
      "Epoch 2624, Loss: 0.3274304270744324, Final Batch Loss: 0.15231136977672577\n",
      "Epoch 2625, Loss: 0.37953391671180725, Final Batch Loss: 0.209876149892807\n",
      "Epoch 2626, Loss: 0.3586127609014511, Final Batch Loss: 0.1602579653263092\n",
      "Epoch 2627, Loss: 0.3477732688188553, Final Batch Loss: 0.18156124651432037\n",
      "Epoch 2628, Loss: 0.30663594603538513, Final Batch Loss: 0.1699078232049942\n",
      "Epoch 2629, Loss: 0.2813432812690735, Final Batch Loss: 0.15357419848442078\n",
      "Epoch 2630, Loss: 0.343860000371933, Final Batch Loss: 0.15583442151546478\n",
      "Epoch 2631, Loss: 0.30402952432632446, Final Batch Loss: 0.1303788423538208\n",
      "Epoch 2632, Loss: 0.38036535680294037, Final Batch Loss: 0.1473078429698944\n",
      "Epoch 2633, Loss: 0.30311204493045807, Final Batch Loss: 0.15030735731124878\n",
      "Epoch 2634, Loss: 0.3324177861213684, Final Batch Loss: 0.16397912800312042\n",
      "Epoch 2635, Loss: 0.284604087471962, Final Batch Loss: 0.13610899448394775\n",
      "Epoch 2636, Loss: 0.3092050552368164, Final Batch Loss: 0.13763201236724854\n",
      "Epoch 2637, Loss: 0.3533616214990616, Final Batch Loss: 0.1594957411289215\n",
      "Epoch 2638, Loss: 0.354840412735939, Final Batch Loss: 0.1394128054380417\n",
      "Epoch 2639, Loss: 0.3578824996948242, Final Batch Loss: 0.16096127033233643\n",
      "Epoch 2640, Loss: 0.33660875260829926, Final Batch Loss: 0.17540214955806732\n",
      "Epoch 2641, Loss: 0.31492049992084503, Final Batch Loss: 0.1617339700460434\n",
      "Epoch 2642, Loss: 0.3140876740217209, Final Batch Loss: 0.16782844066619873\n",
      "Epoch 2643, Loss: 0.3025122508406639, Final Batch Loss: 0.18027392029762268\n",
      "Epoch 2644, Loss: 0.3445171117782593, Final Batch Loss: 0.14281344413757324\n",
      "Epoch 2645, Loss: 0.34073321521282196, Final Batch Loss: 0.18182280659675598\n",
      "Epoch 2646, Loss: 0.34313035011291504, Final Batch Loss: 0.16959352791309357\n",
      "Epoch 2647, Loss: 0.35139551758766174, Final Batch Loss: 0.16654908657073975\n",
      "Epoch 2648, Loss: 0.38250088691711426, Final Batch Loss: 0.22164158523082733\n",
      "Epoch 2649, Loss: 0.28878653049468994, Final Batch Loss: 0.12741944193840027\n",
      "Epoch 2650, Loss: 0.32312050461769104, Final Batch Loss: 0.13598471879959106\n",
      "Epoch 2651, Loss: 0.3017923980951309, Final Batch Loss: 0.13655060529708862\n",
      "Epoch 2652, Loss: 0.3668994754552841, Final Batch Loss: 0.20288877189159393\n",
      "Epoch 2653, Loss: 0.28559716045856476, Final Batch Loss: 0.13340000808238983\n",
      "Epoch 2654, Loss: 0.38468484580516815, Final Batch Loss: 0.19807569682598114\n",
      "Epoch 2655, Loss: 0.32440419495105743, Final Batch Loss: 0.1602303385734558\n",
      "Epoch 2656, Loss: 0.4086747169494629, Final Batch Loss: 0.18241912126541138\n",
      "Epoch 2657, Loss: 0.3041846454143524, Final Batch Loss: 0.14841051399707794\n",
      "Epoch 2658, Loss: 0.3243112564086914, Final Batch Loss: 0.17988702654838562\n",
      "Epoch 2659, Loss: 0.39853741228580475, Final Batch Loss: 0.23273083567619324\n",
      "Epoch 2660, Loss: 0.34527841210365295, Final Batch Loss: 0.20009826123714447\n",
      "Epoch 2661, Loss: 0.3429064303636551, Final Batch Loss: 0.19912582635879517\n",
      "Epoch 2662, Loss: 0.2812667489051819, Final Batch Loss: 0.14096738398075104\n",
      "Epoch 2663, Loss: 0.349129855632782, Final Batch Loss: 0.1453736573457718\n",
      "Epoch 2664, Loss: 0.3149840980768204, Final Batch Loss: 0.15763413906097412\n",
      "Epoch 2665, Loss: 0.3287949562072754, Final Batch Loss: 0.1760162115097046\n",
      "Epoch 2666, Loss: 0.3352668732404709, Final Batch Loss: 0.2063354104757309\n",
      "Epoch 2667, Loss: 0.3150984197854996, Final Batch Loss: 0.13389278948307037\n",
      "Epoch 2668, Loss: 0.33837559819221497, Final Batch Loss: 0.16905882954597473\n",
      "Epoch 2669, Loss: 0.32635101675987244, Final Batch Loss: 0.19192008674144745\n",
      "Epoch 2670, Loss: 0.38094662129879, Final Batch Loss: 0.20226167142391205\n",
      "Epoch 2671, Loss: 0.3523162454366684, Final Batch Loss: 0.1774739921092987\n",
      "Epoch 2672, Loss: 0.3034440279006958, Final Batch Loss: 0.1470937877893448\n",
      "Epoch 2673, Loss: 0.3294338136911392, Final Batch Loss: 0.14788001775741577\n",
      "Epoch 2674, Loss: 0.33838529884815216, Final Batch Loss: 0.1841728240251541\n",
      "Epoch 2675, Loss: 0.30846107006073, Final Batch Loss: 0.17192938923835754\n",
      "Epoch 2676, Loss: 0.37124690413475037, Final Batch Loss: 0.18937459588050842\n",
      "Epoch 2677, Loss: 0.3184991776943207, Final Batch Loss: 0.1504267156124115\n",
      "Epoch 2678, Loss: 0.2975200414657593, Final Batch Loss: 0.13547833263874054\n",
      "Epoch 2679, Loss: 0.3397502601146698, Final Batch Loss: 0.15259802341461182\n",
      "Epoch 2680, Loss: 0.3529447913169861, Final Batch Loss: 0.21047239005565643\n",
      "Epoch 2681, Loss: 0.30684511363506317, Final Batch Loss: 0.18021997809410095\n",
      "Epoch 2682, Loss: 0.34142008423805237, Final Batch Loss: 0.14653213322162628\n",
      "Epoch 2683, Loss: 0.35213717818260193, Final Batch Loss: 0.19754959642887115\n",
      "Epoch 2684, Loss: 0.3749479204416275, Final Batch Loss: 0.20571497082710266\n",
      "Epoch 2685, Loss: 0.2926529198884964, Final Batch Loss: 0.1467743217945099\n",
      "Epoch 2686, Loss: 0.28422847390174866, Final Batch Loss: 0.13405126333236694\n",
      "Epoch 2687, Loss: 0.3174707889556885, Final Batch Loss: 0.1377287060022354\n",
      "Epoch 2688, Loss: 0.32994242012500763, Final Batch Loss: 0.18048496544361115\n",
      "Epoch 2689, Loss: 0.3100602254271507, Final Batch Loss: 0.19396904110908508\n",
      "Epoch 2690, Loss: 0.3189424127340317, Final Batch Loss: 0.17226935923099518\n",
      "Epoch 2691, Loss: 0.31935296952724457, Final Batch Loss: 0.15832702815532684\n",
      "Epoch 2692, Loss: 0.30121392011642456, Final Batch Loss: 0.1452886015176773\n",
      "Epoch 2693, Loss: 0.3318689167499542, Final Batch Loss: 0.1611977219581604\n",
      "Epoch 2694, Loss: 0.31508420407772064, Final Batch Loss: 0.15510043501853943\n",
      "Epoch 2695, Loss: 0.328666016459465, Final Batch Loss: 0.1600898951292038\n",
      "Epoch 2696, Loss: 0.2837781310081482, Final Batch Loss: 0.13903376460075378\n",
      "Epoch 2697, Loss: 0.337681844830513, Final Batch Loss: 0.15377448499202728\n",
      "Epoch 2698, Loss: 0.4094751179218292, Final Batch Loss: 0.19801609218120575\n",
      "Epoch 2699, Loss: 0.33942224085330963, Final Batch Loss: 0.16021208465099335\n",
      "Epoch 2700, Loss: 0.33447185158729553, Final Batch Loss: 0.17067837715148926\n",
      "Epoch 2701, Loss: 0.27884936332702637, Final Batch Loss: 0.12501457333564758\n",
      "Epoch 2702, Loss: 0.33729639649391174, Final Batch Loss: 0.16017678380012512\n",
      "Epoch 2703, Loss: 0.3366111069917679, Final Batch Loss: 0.1839907020330429\n",
      "Epoch 2704, Loss: 0.30970726907253265, Final Batch Loss: 0.1281101554632187\n",
      "Epoch 2705, Loss: 0.29789847135543823, Final Batch Loss: 0.1751147210597992\n",
      "Epoch 2706, Loss: 0.3152206987142563, Final Batch Loss: 0.18570183217525482\n",
      "Epoch 2707, Loss: 0.3305179327726364, Final Batch Loss: 0.1673535853624344\n",
      "Epoch 2708, Loss: 0.37047258019447327, Final Batch Loss: 0.1839563101530075\n",
      "Epoch 2709, Loss: 0.3188804090023041, Final Batch Loss: 0.1710846722126007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2710, Loss: 0.3219348043203354, Final Batch Loss: 0.16526202857494354\n",
      "Epoch 2711, Loss: 0.33657000958919525, Final Batch Loss: 0.13355588912963867\n",
      "Epoch 2712, Loss: 0.22760255634784698, Final Batch Loss: 0.10173113644123077\n",
      "Epoch 2713, Loss: 0.322772815823555, Final Batch Loss: 0.17345288395881653\n",
      "Epoch 2714, Loss: 0.30757205188274384, Final Batch Loss: 0.1680006980895996\n",
      "Epoch 2715, Loss: 0.31033071875572205, Final Batch Loss: 0.14780442416667938\n",
      "Epoch 2716, Loss: 0.3099079877138138, Final Batch Loss: 0.14866821467876434\n",
      "Epoch 2717, Loss: 0.3593807518482208, Final Batch Loss: 0.17868919670581818\n",
      "Epoch 2718, Loss: 0.32566553354263306, Final Batch Loss: 0.16345731914043427\n",
      "Epoch 2719, Loss: 0.26265351474285126, Final Batch Loss: 0.10738202929496765\n",
      "Epoch 2720, Loss: 0.2709784209728241, Final Batch Loss: 0.14749561250209808\n",
      "Epoch 2721, Loss: 0.28793518245220184, Final Batch Loss: 0.15437854826450348\n",
      "Epoch 2722, Loss: 0.3121684640645981, Final Batch Loss: 0.16367268562316895\n",
      "Epoch 2723, Loss: 0.33877032995224, Final Batch Loss: 0.16813302040100098\n",
      "Epoch 2724, Loss: 0.3465212732553482, Final Batch Loss: 0.17844896018505096\n",
      "Epoch 2725, Loss: 0.3107650429010391, Final Batch Loss: 0.13543115556240082\n",
      "Epoch 2726, Loss: 0.262633815407753, Final Batch Loss: 0.12703923881053925\n",
      "Epoch 2727, Loss: 0.2943388298153877, Final Batch Loss: 0.1073513850569725\n",
      "Epoch 2728, Loss: 0.35216224193573, Final Batch Loss: 0.16614949703216553\n",
      "Epoch 2729, Loss: 0.3114462196826935, Final Batch Loss: 0.17888623476028442\n",
      "Epoch 2730, Loss: 0.27047179639339447, Final Batch Loss: 0.12726396322250366\n",
      "Epoch 2731, Loss: 0.3678487688302994, Final Batch Loss: 0.20483513176441193\n",
      "Epoch 2732, Loss: 0.31826552748680115, Final Batch Loss: 0.14143668115139008\n",
      "Epoch 2733, Loss: 0.27860040962696075, Final Batch Loss: 0.12817785143852234\n",
      "Epoch 2734, Loss: 0.4058636575937271, Final Batch Loss: 0.25687021017074585\n",
      "Epoch 2735, Loss: 0.38956300914287567, Final Batch Loss: 0.1850018948316574\n",
      "Epoch 2736, Loss: 0.3762146383523941, Final Batch Loss: 0.16819937527179718\n",
      "Epoch 2737, Loss: 0.2733013331890106, Final Batch Loss: 0.13364636898040771\n",
      "Epoch 2738, Loss: 0.3445337563753128, Final Batch Loss: 0.19001789391040802\n",
      "Epoch 2739, Loss: 0.3541420251131058, Final Batch Loss: 0.12421296536922455\n",
      "Epoch 2740, Loss: 0.2787647098302841, Final Batch Loss: 0.15726210176944733\n",
      "Epoch 2741, Loss: 0.3255918025970459, Final Batch Loss: 0.17237913608551025\n",
      "Epoch 2742, Loss: 0.3461758494377136, Final Batch Loss: 0.17764045298099518\n",
      "Epoch 2743, Loss: 0.3614773452281952, Final Batch Loss: 0.1744726449251175\n",
      "Epoch 2744, Loss: 0.30380380153656006, Final Batch Loss: 0.17391793429851532\n",
      "Epoch 2745, Loss: 0.29995734989643097, Final Batch Loss: 0.1315014660358429\n",
      "Epoch 2746, Loss: 0.2666536122560501, Final Batch Loss: 0.08904065191745758\n",
      "Epoch 2747, Loss: 0.2645266354084015, Final Batch Loss: 0.09897178411483765\n",
      "Epoch 2748, Loss: 0.27060801535844803, Final Batch Loss: 0.1081189289689064\n",
      "Epoch 2749, Loss: 0.33079861104488373, Final Batch Loss: 0.18817202746868134\n",
      "Epoch 2750, Loss: 0.32108819484710693, Final Batch Loss: 0.15680159628391266\n",
      "Epoch 2751, Loss: 0.3377145826816559, Final Batch Loss: 0.18506956100463867\n",
      "Epoch 2752, Loss: 0.2765329107642174, Final Batch Loss: 0.16816262900829315\n",
      "Epoch 2753, Loss: 0.3107270523905754, Final Batch Loss: 0.18600092828273773\n",
      "Epoch 2754, Loss: 0.34674856066703796, Final Batch Loss: 0.16992135345935822\n",
      "Epoch 2755, Loss: 0.4000560939311981, Final Batch Loss: 0.19271531701087952\n",
      "Epoch 2756, Loss: 0.3453458994626999, Final Batch Loss: 0.1763470470905304\n",
      "Epoch 2757, Loss: 0.299333855509758, Final Batch Loss: 0.15643417835235596\n",
      "Epoch 2758, Loss: 0.26033493876457214, Final Batch Loss: 0.1287449598312378\n",
      "Epoch 2759, Loss: 0.36949992179870605, Final Batch Loss: 0.1532682329416275\n",
      "Epoch 2760, Loss: 0.3672238141298294, Final Batch Loss: 0.18807868659496307\n",
      "Epoch 2761, Loss: 0.3056444972753525, Final Batch Loss: 0.13117733597755432\n",
      "Epoch 2762, Loss: 0.33658142387866974, Final Batch Loss: 0.18934448063373566\n",
      "Epoch 2763, Loss: 0.34333282709121704, Final Batch Loss: 0.16684196889400482\n",
      "Epoch 2764, Loss: 0.3046352416276932, Final Batch Loss: 0.15090537071228027\n",
      "Epoch 2765, Loss: 0.34869568049907684, Final Batch Loss: 0.18870271742343903\n",
      "Epoch 2766, Loss: 0.2876158654689789, Final Batch Loss: 0.13768453896045685\n",
      "Epoch 2767, Loss: 0.3342832177877426, Final Batch Loss: 0.12890970706939697\n",
      "Epoch 2768, Loss: 0.2993038594722748, Final Batch Loss: 0.14957430958747864\n",
      "Epoch 2769, Loss: 0.35172179341316223, Final Batch Loss: 0.16242589056491852\n",
      "Epoch 2770, Loss: 0.3138589859008789, Final Batch Loss: 0.19177502393722534\n",
      "Epoch 2771, Loss: 0.2768491432070732, Final Batch Loss: 0.15292075276374817\n",
      "Epoch 2772, Loss: 0.26866093277931213, Final Batch Loss: 0.13289523124694824\n",
      "Epoch 2773, Loss: 0.28168773651123047, Final Batch Loss: 0.1512582004070282\n",
      "Epoch 2774, Loss: 0.33191320300102234, Final Batch Loss: 0.15610477328300476\n",
      "Epoch 2775, Loss: 0.3117402493953705, Final Batch Loss: 0.1260145604610443\n",
      "Epoch 2776, Loss: 0.32647448033094406, Final Batch Loss: 0.21183612942695618\n",
      "Epoch 2777, Loss: 0.34982500970363617, Final Batch Loss: 0.1409883350133896\n",
      "Epoch 2778, Loss: 0.35889191925525665, Final Batch Loss: 0.22599254548549652\n",
      "Epoch 2779, Loss: 0.40075908601284027, Final Batch Loss: 0.20100384950637817\n",
      "Epoch 2780, Loss: 0.3038557320833206, Final Batch Loss: 0.18064549565315247\n",
      "Epoch 2781, Loss: 0.26130039244890213, Final Batch Loss: 0.09913263469934464\n",
      "Epoch 2782, Loss: 0.2900034412741661, Final Batch Loss: 0.10202860087156296\n",
      "Epoch 2783, Loss: 0.2576891630887985, Final Batch Loss: 0.12781016528606415\n",
      "Epoch 2784, Loss: 0.318502739071846, Final Batch Loss: 0.12673988938331604\n",
      "Epoch 2785, Loss: 0.36750927567481995, Final Batch Loss: 0.18556822836399078\n",
      "Epoch 2786, Loss: 0.3316841274499893, Final Batch Loss: 0.17195984721183777\n",
      "Epoch 2787, Loss: 0.3153013288974762, Final Batch Loss: 0.1720549613237381\n",
      "Epoch 2788, Loss: 0.3024742156267166, Final Batch Loss: 0.15436488389968872\n",
      "Epoch 2789, Loss: 0.3092305064201355, Final Batch Loss: 0.17320959270000458\n",
      "Epoch 2790, Loss: 0.303123340010643, Final Batch Loss: 0.1666400283575058\n",
      "Epoch 2791, Loss: 0.2952880784869194, Final Batch Loss: 0.12368009239435196\n",
      "Epoch 2792, Loss: 0.36496680974960327, Final Batch Loss: 0.21599772572517395\n",
      "Epoch 2793, Loss: 0.2455596625804901, Final Batch Loss: 0.116315096616745\n",
      "Epoch 2794, Loss: 0.28327494859695435, Final Batch Loss: 0.14318087697029114\n",
      "Epoch 2795, Loss: 0.31082117557525635, Final Batch Loss: 0.1481625735759735\n",
      "Epoch 2796, Loss: 0.29795141518116, Final Batch Loss: 0.13360857963562012\n",
      "Epoch 2797, Loss: 0.25963419675827026, Final Batch Loss: 0.15690426528453827\n",
      "Epoch 2798, Loss: 0.24971922487020493, Final Batch Loss: 0.10474538058042526\n",
      "Epoch 2799, Loss: 0.264356791973114, Final Batch Loss: 0.1246539056301117\n",
      "Epoch 2800, Loss: 0.2645387351512909, Final Batch Loss: 0.122732013463974\n",
      "Epoch 2801, Loss: 0.24560484290122986, Final Batch Loss: 0.14284849166870117\n",
      "Epoch 2802, Loss: 0.3112451285123825, Final Batch Loss: 0.15075065195560455\n",
      "Epoch 2803, Loss: 0.34668196737766266, Final Batch Loss: 0.20241640508174896\n",
      "Epoch 2804, Loss: 0.34457828104496, Final Batch Loss: 0.18037699162960052\n",
      "Epoch 2805, Loss: 0.2957502007484436, Final Batch Loss: 0.14107148349285126\n",
      "Epoch 2806, Loss: 0.3471512347459793, Final Batch Loss: 0.1826643943786621\n",
      "Epoch 2807, Loss: 0.3090232014656067, Final Batch Loss: 0.18267112970352173\n",
      "Epoch 2808, Loss: 0.3086196780204773, Final Batch Loss: 0.1373397260904312\n",
      "Epoch 2809, Loss: 0.3054317533969879, Final Batch Loss: 0.14014941453933716\n",
      "Epoch 2810, Loss: 0.28133754432201385, Final Batch Loss: 0.14651286602020264\n",
      "Epoch 2811, Loss: 0.26648660749197006, Final Batch Loss: 0.14274761080741882\n",
      "Epoch 2812, Loss: 0.3354979604482651, Final Batch Loss: 0.124479740858078\n",
      "Epoch 2813, Loss: 0.2967895567417145, Final Batch Loss: 0.1760730892419815\n",
      "Epoch 2814, Loss: 0.3423614799976349, Final Batch Loss: 0.18002821505069733\n",
      "Epoch 2815, Loss: 0.2779853567481041, Final Batch Loss: 0.11901014298200607\n",
      "Epoch 2816, Loss: 0.36875924468040466, Final Batch Loss: 0.2193344384431839\n",
      "Epoch 2817, Loss: 0.37659065425395966, Final Batch Loss: 0.20479094982147217\n",
      "Epoch 2818, Loss: 0.2700914442539215, Final Batch Loss: 0.1267865002155304\n",
      "Epoch 2819, Loss: 0.29680009186267853, Final Batch Loss: 0.13838422298431396\n",
      "Epoch 2820, Loss: 0.24269844591617584, Final Batch Loss: 0.11535757780075073\n",
      "Epoch 2821, Loss: 0.2389931008219719, Final Batch Loss: 0.12030699104070663\n",
      "Epoch 2822, Loss: 0.32431481778621674, Final Batch Loss: 0.1927367001771927\n",
      "Epoch 2823, Loss: 0.29573867470026016, Final Batch Loss: 0.11675430089235306\n",
      "Epoch 2824, Loss: 0.25956781953573227, Final Batch Loss: 0.10387394577264786\n",
      "Epoch 2825, Loss: 0.27979472279548645, Final Batch Loss: 0.1390897035598755\n",
      "Epoch 2826, Loss: 0.2697686105966568, Final Batch Loss: 0.13094286620616913\n",
      "Epoch 2827, Loss: 0.25540921092033386, Final Batch Loss: 0.11398577690124512\n",
      "Epoch 2828, Loss: 0.32593248784542084, Final Batch Loss: 0.18746206164360046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2829, Loss: 0.29368825256824493, Final Batch Loss: 0.14893467724323273\n",
      "Epoch 2830, Loss: 0.37185753881931305, Final Batch Loss: 0.20948316156864166\n",
      "Epoch 2831, Loss: 0.27458885312080383, Final Batch Loss: 0.14102551341056824\n",
      "Epoch 2832, Loss: 0.334799587726593, Final Batch Loss: 0.19044142961502075\n",
      "Epoch 2833, Loss: 0.29590047895908356, Final Batch Loss: 0.14269354939460754\n",
      "Epoch 2834, Loss: 0.2737469971179962, Final Batch Loss: 0.14368891716003418\n",
      "Epoch 2835, Loss: 0.29789040982723236, Final Batch Loss: 0.1359919309616089\n",
      "Epoch 2836, Loss: 0.32449401915073395, Final Batch Loss: 0.17385412752628326\n",
      "Epoch 2837, Loss: 0.2319634035229683, Final Batch Loss: 0.12569041550159454\n",
      "Epoch 2838, Loss: 0.26697684824466705, Final Batch Loss: 0.11950978636741638\n",
      "Epoch 2839, Loss: 0.2400697022676468, Final Batch Loss: 0.120969757437706\n",
      "Epoch 2840, Loss: 0.2885020971298218, Final Batch Loss: 0.14997754991054535\n",
      "Epoch 2841, Loss: 0.2766175642609596, Final Batch Loss: 0.10984211415052414\n",
      "Epoch 2842, Loss: 0.32399463653564453, Final Batch Loss: 0.1158401370048523\n",
      "Epoch 2843, Loss: 0.2978360056877136, Final Batch Loss: 0.16119161248207092\n",
      "Epoch 2844, Loss: 0.2936776280403137, Final Batch Loss: 0.1614251285791397\n",
      "Epoch 2845, Loss: 0.31393034011125565, Final Batch Loss: 0.19238154590129852\n",
      "Epoch 2846, Loss: 0.283739373087883, Final Batch Loss: 0.1410234123468399\n",
      "Epoch 2847, Loss: 0.32726018130779266, Final Batch Loss: 0.17838969826698303\n",
      "Epoch 2848, Loss: 0.27831633388996124, Final Batch Loss: 0.13296054303646088\n",
      "Epoch 2849, Loss: 0.28901296854019165, Final Batch Loss: 0.13335363566875458\n",
      "Epoch 2850, Loss: 0.3009869009256363, Final Batch Loss: 0.1289939433336258\n",
      "Epoch 2851, Loss: 0.3024885803461075, Final Batch Loss: 0.11808967590332031\n",
      "Epoch 2852, Loss: 0.24626248329877853, Final Batch Loss: 0.1405450850725174\n",
      "Epoch 2853, Loss: 0.2568853050470352, Final Batch Loss: 0.15093626081943512\n",
      "Epoch 2854, Loss: 0.34686775505542755, Final Batch Loss: 0.2102963924407959\n",
      "Epoch 2855, Loss: 0.2825600206851959, Final Batch Loss: 0.1378932148218155\n",
      "Epoch 2856, Loss: 0.2882586866617203, Final Batch Loss: 0.16118939220905304\n",
      "Epoch 2857, Loss: 0.25494620203971863, Final Batch Loss: 0.13505582511425018\n",
      "Epoch 2858, Loss: 0.3023977726697922, Final Batch Loss: 0.164858877658844\n",
      "Epoch 2859, Loss: 0.3800739198923111, Final Batch Loss: 0.23409616947174072\n",
      "Epoch 2860, Loss: 0.2643375098705292, Final Batch Loss: 0.11666563153266907\n",
      "Epoch 2861, Loss: 0.23958728462457657, Final Batch Loss: 0.12913550436496735\n",
      "Epoch 2862, Loss: 0.28641077876091003, Final Batch Loss: 0.11242614686489105\n",
      "Epoch 2863, Loss: 0.3007889688014984, Final Batch Loss: 0.1366298645734787\n",
      "Epoch 2864, Loss: 0.24803826957941055, Final Batch Loss: 0.12027797847986221\n",
      "Epoch 2865, Loss: 0.24955230206251144, Final Batch Loss: 0.13988128304481506\n",
      "Epoch 2866, Loss: 0.27205975353717804, Final Batch Loss: 0.14067335426807404\n",
      "Epoch 2867, Loss: 0.2973081022500992, Final Batch Loss: 0.1234305202960968\n",
      "Epoch 2868, Loss: 0.2463323250412941, Final Batch Loss: 0.07787808030843735\n",
      "Epoch 2869, Loss: 0.26540669053792953, Final Batch Loss: 0.14648054540157318\n",
      "Epoch 2870, Loss: 0.3273828327655792, Final Batch Loss: 0.17826947569847107\n",
      "Epoch 2871, Loss: 0.29807939380407333, Final Batch Loss: 0.11831333488225937\n",
      "Epoch 2872, Loss: 0.23748676478862762, Final Batch Loss: 0.10669595003128052\n",
      "Epoch 2873, Loss: 0.31394076347351074, Final Batch Loss: 0.09545326232910156\n",
      "Epoch 2874, Loss: 0.29755379259586334, Final Batch Loss: 0.14447267353534698\n",
      "Epoch 2875, Loss: 0.35980530083179474, Final Batch Loss: 0.22836223244667053\n",
      "Epoch 2876, Loss: 0.2974839210510254, Final Batch Loss: 0.1434720754623413\n",
      "Epoch 2877, Loss: 0.28781557083129883, Final Batch Loss: 0.15057072043418884\n",
      "Epoch 2878, Loss: 0.3209385722875595, Final Batch Loss: 0.16010019183158875\n",
      "Epoch 2879, Loss: 0.26034868508577347, Final Batch Loss: 0.13765259087085724\n",
      "Epoch 2880, Loss: 0.2876432240009308, Final Batch Loss: 0.13958530128002167\n",
      "Epoch 2881, Loss: 0.29123012721538544, Final Batch Loss: 0.12789368629455566\n",
      "Epoch 2882, Loss: 0.3495694547891617, Final Batch Loss: 0.152958944439888\n",
      "Epoch 2883, Loss: 0.3360515534877777, Final Batch Loss: 0.18354438245296478\n",
      "Epoch 2884, Loss: 0.26943980902433395, Final Batch Loss: 0.11387983709573746\n",
      "Epoch 2885, Loss: 0.3022763282060623, Final Batch Loss: 0.1286890208721161\n",
      "Epoch 2886, Loss: 0.2226105034351349, Final Batch Loss: 0.10090856999158859\n",
      "Epoch 2887, Loss: 0.26766686141490936, Final Batch Loss: 0.13435834646224976\n",
      "Epoch 2888, Loss: 0.2811288833618164, Final Batch Loss: 0.14269651472568512\n",
      "Epoch 2889, Loss: 0.2555982694029808, Final Batch Loss: 0.10243620723485947\n",
      "Epoch 2890, Loss: 0.24625415354967117, Final Batch Loss: 0.1403713822364807\n",
      "Epoch 2891, Loss: 0.25503338873386383, Final Batch Loss: 0.12705731391906738\n",
      "Epoch 2892, Loss: 0.25307202339172363, Final Batch Loss: 0.12730693817138672\n",
      "Epoch 2893, Loss: 0.21441149711608887, Final Batch Loss: 0.11620750278234482\n",
      "Epoch 2894, Loss: 0.27415604889392853, Final Batch Loss: 0.16506843268871307\n",
      "Epoch 2895, Loss: 0.27780681848526, Final Batch Loss: 0.1482478380203247\n",
      "Epoch 2896, Loss: 0.3033805638551712, Final Batch Loss: 0.12462140619754791\n",
      "Epoch 2897, Loss: 0.3226785361766815, Final Batch Loss: 0.14598919451236725\n",
      "Epoch 2898, Loss: 0.31697525829076767, Final Batch Loss: 0.10823570936918259\n",
      "Epoch 2899, Loss: 0.32814282178878784, Final Batch Loss: 0.1230972409248352\n",
      "Epoch 2900, Loss: 0.31141427159309387, Final Batch Loss: 0.18250679969787598\n",
      "Epoch 2901, Loss: 0.37235307693481445, Final Batch Loss: 0.19119980931282043\n",
      "Epoch 2902, Loss: 0.2717284485697746, Final Batch Loss: 0.15266241133213043\n",
      "Epoch 2903, Loss: 0.32538945972919464, Final Batch Loss: 0.125907301902771\n",
      "Epoch 2904, Loss: 0.3344092518091202, Final Batch Loss: 0.1953587830066681\n",
      "Epoch 2905, Loss: 0.23839570581912994, Final Batch Loss: 0.09574873745441437\n",
      "Epoch 2906, Loss: 0.3083885759115219, Final Batch Loss: 0.12450069189071655\n",
      "Epoch 2907, Loss: 0.265827514231205, Final Batch Loss: 0.16515424847602844\n",
      "Epoch 2908, Loss: 0.25384151190519333, Final Batch Loss: 0.11998654156923294\n",
      "Epoch 2909, Loss: 0.27125710248947144, Final Batch Loss: 0.1107059121131897\n",
      "Epoch 2910, Loss: 0.29651306569576263, Final Batch Loss: 0.16132879257202148\n",
      "Epoch 2911, Loss: 0.24962635338306427, Final Batch Loss: 0.12115581333637238\n",
      "Epoch 2912, Loss: 0.27444712817668915, Final Batch Loss: 0.14486479759216309\n",
      "Epoch 2913, Loss: 0.3414415419101715, Final Batch Loss: 0.15322037041187286\n",
      "Epoch 2914, Loss: 0.27817101031541824, Final Batch Loss: 0.12057977169752121\n",
      "Epoch 2915, Loss: 0.25993648171424866, Final Batch Loss: 0.14125092327594757\n",
      "Epoch 2916, Loss: 0.260249525308609, Final Batch Loss: 0.1298210322856903\n",
      "Epoch 2917, Loss: 0.2889641374349594, Final Batch Loss: 0.11661723256111145\n",
      "Epoch 2918, Loss: 0.2773720622062683, Final Batch Loss: 0.1327873021364212\n",
      "Epoch 2919, Loss: 0.30802683532238007, Final Batch Loss: 0.1751573234796524\n",
      "Epoch 2920, Loss: 0.24932632595300674, Final Batch Loss: 0.12110989540815353\n",
      "Epoch 2921, Loss: 0.2837191969156265, Final Batch Loss: 0.1550702303647995\n",
      "Epoch 2922, Loss: 0.2955572307109833, Final Batch Loss: 0.15060164034366608\n",
      "Epoch 2923, Loss: 0.33109281957149506, Final Batch Loss: 0.13461428880691528\n",
      "Epoch 2924, Loss: 0.28253668546676636, Final Batch Loss: 0.16238193213939667\n",
      "Epoch 2925, Loss: 0.239701509475708, Final Batch Loss: 0.13532596826553345\n",
      "Epoch 2926, Loss: 0.31106604635715485, Final Batch Loss: 0.16893865168094635\n",
      "Epoch 2927, Loss: 0.3105996251106262, Final Batch Loss: 0.14601914584636688\n",
      "Epoch 2928, Loss: 0.27361175417900085, Final Batch Loss: 0.1468580663204193\n",
      "Epoch 2929, Loss: 0.2981002479791641, Final Batch Loss: 0.14254236221313477\n",
      "Epoch 2930, Loss: 0.26757175475358963, Final Batch Loss: 0.1175665631890297\n",
      "Epoch 2931, Loss: 0.302878275513649, Final Batch Loss: 0.13933439552783966\n",
      "Epoch 2932, Loss: 0.26458151638507843, Final Batch Loss: 0.11237893998622894\n",
      "Epoch 2933, Loss: 0.341269388794899, Final Batch Loss: 0.11559009552001953\n",
      "Epoch 2934, Loss: 0.2635357901453972, Final Batch Loss: 0.14047500491142273\n",
      "Epoch 2935, Loss: 0.3216865509748459, Final Batch Loss: 0.19231881201267242\n",
      "Epoch 2936, Loss: 0.23583117127418518, Final Batch Loss: 0.11125583201646805\n",
      "Epoch 2937, Loss: 0.26969894766807556, Final Batch Loss: 0.15468309819698334\n",
      "Epoch 2938, Loss: 0.28893549740314484, Final Batch Loss: 0.15035682916641235\n",
      "Epoch 2939, Loss: 0.35987839102745056, Final Batch Loss: 0.13466547429561615\n",
      "Epoch 2940, Loss: 0.2725069671869278, Final Batch Loss: 0.1443902850151062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2941, Loss: 0.27531054615974426, Final Batch Loss: 0.13300321996212006\n",
      "Epoch 2942, Loss: 0.2415756955742836, Final Batch Loss: 0.11042939871549606\n",
      "Epoch 2943, Loss: 0.2584339529275894, Final Batch Loss: 0.13088852167129517\n",
      "Epoch 2944, Loss: 0.2536357343196869, Final Batch Loss: 0.11452348530292511\n",
      "Epoch 2945, Loss: 0.2672565132379532, Final Batch Loss: 0.12258200347423553\n",
      "Epoch 2946, Loss: 0.2753102704882622, Final Batch Loss: 0.15047475695610046\n",
      "Epoch 2947, Loss: 0.32160189747810364, Final Batch Loss: 0.17607691884040833\n",
      "Epoch 2948, Loss: 0.27594947814941406, Final Batch Loss: 0.1494140625\n",
      "Epoch 2949, Loss: 0.2517463192343712, Final Batch Loss: 0.12974615395069122\n",
      "Epoch 2950, Loss: 0.2803422659635544, Final Batch Loss: 0.15215198695659637\n",
      "Epoch 2951, Loss: 0.2564931958913803, Final Batch Loss: 0.11989524960517883\n",
      "Epoch 2952, Loss: 0.3263271898031235, Final Batch Loss: 0.1479196697473526\n",
      "Epoch 2953, Loss: 0.26090720295906067, Final Batch Loss: 0.10790465772151947\n",
      "Epoch 2954, Loss: 0.2689661681652069, Final Batch Loss: 0.1338435411453247\n",
      "Epoch 2955, Loss: 0.2657041400671005, Final Batch Loss: 0.10562008619308472\n",
      "Epoch 2956, Loss: 0.26042357087135315, Final Batch Loss: 0.1269931048154831\n",
      "Epoch 2957, Loss: 0.32266008853912354, Final Batch Loss: 0.18669317662715912\n",
      "Epoch 2958, Loss: 0.32432132959365845, Final Batch Loss: 0.1827847957611084\n",
      "Epoch 2959, Loss: 0.3343786597251892, Final Batch Loss: 0.1923362761735916\n",
      "Epoch 2960, Loss: 0.27036257088184357, Final Batch Loss: 0.11285129189491272\n",
      "Epoch 2961, Loss: 0.262475922703743, Final Batch Loss: 0.10258908569812775\n",
      "Epoch 2962, Loss: 0.2813086211681366, Final Batch Loss: 0.1352609097957611\n",
      "Epoch 2963, Loss: 0.3197821229696274, Final Batch Loss: 0.166281059384346\n",
      "Epoch 2964, Loss: 0.23035229742527008, Final Batch Loss: 0.10641918331384659\n",
      "Epoch 2965, Loss: 0.2803524062037468, Final Batch Loss: 0.16717852652072906\n",
      "Epoch 2966, Loss: 0.25346313416957855, Final Batch Loss: 0.1300072818994522\n",
      "Epoch 2967, Loss: 0.2889079302549362, Final Batch Loss: 0.13009129464626312\n",
      "Epoch 2968, Loss: 0.2838393896818161, Final Batch Loss: 0.15113325417041779\n",
      "Epoch 2969, Loss: 0.2854876071214676, Final Batch Loss: 0.14313556253910065\n",
      "Epoch 2970, Loss: 0.3150892108678818, Final Batch Loss: 0.17266102135181427\n",
      "Epoch 2971, Loss: 0.30412329733371735, Final Batch Loss: 0.15583764016628265\n",
      "Epoch 2972, Loss: 0.24545209109783173, Final Batch Loss: 0.13479791581630707\n",
      "Epoch 2973, Loss: 0.2840774580836296, Final Batch Loss: 0.16783854365348816\n",
      "Epoch 2974, Loss: 0.22224314510822296, Final Batch Loss: 0.10404009371995926\n",
      "Epoch 2975, Loss: 0.22144195437431335, Final Batch Loss: 0.08463910222053528\n",
      "Epoch 2976, Loss: 0.24238023161888123, Final Batch Loss: 0.12242306768894196\n",
      "Epoch 2977, Loss: 0.22849766165018082, Final Batch Loss: 0.10661391913890839\n",
      "Epoch 2978, Loss: 0.2734859436750412, Final Batch Loss: 0.14503717422485352\n",
      "Epoch 2979, Loss: 0.3174589201807976, Final Batch Loss: 0.19577038288116455\n",
      "Epoch 2980, Loss: 0.350492924451828, Final Batch Loss: 0.16579453647136688\n",
      "Epoch 2981, Loss: 0.33371543884277344, Final Batch Loss: 0.2082672268152237\n",
      "Epoch 2982, Loss: 0.2677586227655411, Final Batch Loss: 0.14087925851345062\n",
      "Epoch 2983, Loss: 0.3449965566396713, Final Batch Loss: 0.16696441173553467\n",
      "Epoch 2984, Loss: 0.2825622037053108, Final Batch Loss: 0.1582958996295929\n",
      "Epoch 2985, Loss: 0.294580414891243, Final Batch Loss: 0.14175020158290863\n",
      "Epoch 2986, Loss: 0.276918388903141, Final Batch Loss: 0.10906907171010971\n",
      "Epoch 2987, Loss: 0.2825154513120651, Final Batch Loss: 0.15264767408370972\n",
      "Epoch 2988, Loss: 0.23286842554807663, Final Batch Loss: 0.11242032796144485\n",
      "Epoch 2989, Loss: 0.2811613529920578, Final Batch Loss: 0.15134553611278534\n",
      "Epoch 2990, Loss: 0.3189225494861603, Final Batch Loss: 0.16364063322544098\n",
      "Epoch 2991, Loss: 0.29942579567432404, Final Batch Loss: 0.17001532018184662\n",
      "Epoch 2992, Loss: 0.26849816739559174, Final Batch Loss: 0.12876193225383759\n",
      "Epoch 2993, Loss: 0.24730586260557175, Final Batch Loss: 0.14043208956718445\n",
      "Epoch 2994, Loss: 0.2501184269785881, Final Batch Loss: 0.1378641575574875\n",
      "Epoch 2995, Loss: 0.2420380562543869, Final Batch Loss: 0.1239059716463089\n",
      "Epoch 2996, Loss: 0.26241594552993774, Final Batch Loss: 0.1324985921382904\n",
      "Epoch 2997, Loss: 0.2525717094540596, Final Batch Loss: 0.11874806135892868\n",
      "Epoch 2998, Loss: 0.23806613683700562, Final Batch Loss: 0.1342180073261261\n",
      "Epoch 2999, Loss: 0.27762553095817566, Final Batch Loss: 0.13141493499279022\n",
      "Epoch 3000, Loss: 0.23350349813699722, Final Batch Loss: 0.10289003700017929\n",
      "Epoch 3001, Loss: 0.320644348859787, Final Batch Loss: 0.14117968082427979\n",
      "Epoch 3002, Loss: 0.2846214473247528, Final Batch Loss: 0.15771882236003876\n",
      "Epoch 3003, Loss: 0.25121403485536575, Final Batch Loss: 0.11836271733045578\n",
      "Epoch 3004, Loss: 0.2390373796224594, Final Batch Loss: 0.12666547298431396\n",
      "Epoch 3005, Loss: 0.2754880487918854, Final Batch Loss: 0.13887059688568115\n",
      "Epoch 3006, Loss: 0.24109843373298645, Final Batch Loss: 0.11129091680049896\n",
      "Epoch 3007, Loss: 0.267501637339592, Final Batch Loss: 0.16939695179462433\n",
      "Epoch 3008, Loss: 0.2390911728143692, Final Batch Loss: 0.12782728672027588\n",
      "Epoch 3009, Loss: 0.3512502461671829, Final Batch Loss: 0.15356914699077606\n",
      "Epoch 3010, Loss: 0.2681315243244171, Final Batch Loss: 0.12833774089813232\n",
      "Epoch 3011, Loss: 0.25222785770893097, Final Batch Loss: 0.13495716452598572\n",
      "Epoch 3012, Loss: 0.19547941535711288, Final Batch Loss: 0.10038239508867264\n",
      "Epoch 3013, Loss: 0.27807052433490753, Final Batch Loss: 0.09568336606025696\n",
      "Epoch 3014, Loss: 0.20537608861923218, Final Batch Loss: 0.09264626353979111\n",
      "Epoch 3015, Loss: 0.25850464403629303, Final Batch Loss: 0.13966572284698486\n",
      "Epoch 3016, Loss: 0.2604032829403877, Final Batch Loss: 0.15340377390384674\n",
      "Epoch 3017, Loss: 0.23160580545663834, Final Batch Loss: 0.09405451267957687\n",
      "Epoch 3018, Loss: 0.2551318258047104, Final Batch Loss: 0.11974059045314789\n",
      "Epoch 3019, Loss: 0.26901714503765106, Final Batch Loss: 0.12565384805202484\n",
      "Epoch 3020, Loss: 0.2919842526316643, Final Batch Loss: 0.11923245340585709\n",
      "Epoch 3021, Loss: 0.27428601682186127, Final Batch Loss: 0.15181802213191986\n",
      "Epoch 3022, Loss: 0.22707007080316544, Final Batch Loss: 0.11224261671304703\n",
      "Epoch 3023, Loss: 0.2989051789045334, Final Batch Loss: 0.1699962615966797\n",
      "Epoch 3024, Loss: 0.2848217785358429, Final Batch Loss: 0.13514924049377441\n",
      "Epoch 3025, Loss: 0.2510857582092285, Final Batch Loss: 0.10418833792209625\n",
      "Epoch 3026, Loss: 0.24296748638153076, Final Batch Loss: 0.123990997672081\n",
      "Epoch 3027, Loss: 0.23281287401914597, Final Batch Loss: 0.1300591081380844\n",
      "Epoch 3028, Loss: 0.2794056385755539, Final Batch Loss: 0.14581432938575745\n",
      "Epoch 3029, Loss: 0.2246759682893753, Final Batch Loss: 0.11055261641740799\n",
      "Epoch 3030, Loss: 0.2987724244594574, Final Batch Loss: 0.15417583286762238\n",
      "Epoch 3031, Loss: 0.2785578966140747, Final Batch Loss: 0.17012856900691986\n",
      "Epoch 3032, Loss: 0.26829521358013153, Final Batch Loss: 0.15296050906181335\n",
      "Epoch 3033, Loss: 0.30164700746536255, Final Batch Loss: 0.1668720692396164\n",
      "Epoch 3034, Loss: 0.2528684437274933, Final Batch Loss: 0.10802821815013885\n",
      "Epoch 3035, Loss: 0.3142940402030945, Final Batch Loss: 0.15199562907218933\n",
      "Epoch 3036, Loss: 0.26691269874572754, Final Batch Loss: 0.11596325039863586\n",
      "Epoch 3037, Loss: 0.26571911573410034, Final Batch Loss: 0.1521255224943161\n",
      "Epoch 3038, Loss: 0.2321673482656479, Final Batch Loss: 0.10459795594215393\n",
      "Epoch 3039, Loss: 0.26500821858644485, Final Batch Loss: 0.1129540279507637\n",
      "Epoch 3040, Loss: 0.2904634475708008, Final Batch Loss: 0.13865824043750763\n",
      "Epoch 3041, Loss: 0.29362034797668457, Final Batch Loss: 0.1542884260416031\n",
      "Epoch 3042, Loss: 0.2666241079568863, Final Batch Loss: 0.13761158287525177\n",
      "Epoch 3043, Loss: 0.2197015881538391, Final Batch Loss: 0.10713279992341995\n",
      "Epoch 3044, Loss: 0.3206409811973572, Final Batch Loss: 0.193003848195076\n",
      "Epoch 3045, Loss: 0.2507150173187256, Final Batch Loss: 0.11346122622489929\n",
      "Epoch 3046, Loss: 0.24983447790145874, Final Batch Loss: 0.13426904380321503\n",
      "Epoch 3047, Loss: 0.24120614677667618, Final Batch Loss: 0.10040389746427536\n",
      "Epoch 3048, Loss: 0.2643915116786957, Final Batch Loss: 0.11736650764942169\n",
      "Epoch 3049, Loss: 0.25409138202667236, Final Batch Loss: 0.13012439012527466\n",
      "Epoch 3050, Loss: 0.26070111989974976, Final Batch Loss: 0.1306341588497162\n",
      "Epoch 3051, Loss: 0.23342110216617584, Final Batch Loss: 0.11493272334337234\n",
      "Epoch 3052, Loss: 0.3191521316766739, Final Batch Loss: 0.1667753905057907\n",
      "Epoch 3053, Loss: 0.241382896900177, Final Batch Loss: 0.15315327048301697\n",
      "Epoch 3054, Loss: 0.2619687095284462, Final Batch Loss: 0.1398555487394333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3055, Loss: 0.25663425773382187, Final Batch Loss: 0.11988646537065506\n",
      "Epoch 3056, Loss: 0.25931699573993683, Final Batch Loss: 0.13292989134788513\n",
      "Epoch 3057, Loss: 0.24461684376001358, Final Batch Loss: 0.1156928613781929\n",
      "Epoch 3058, Loss: 0.20086725056171417, Final Batch Loss: 0.07732496410608292\n",
      "Epoch 3059, Loss: 0.2829454392194748, Final Batch Loss: 0.1624232977628708\n",
      "Epoch 3060, Loss: 0.23341216146945953, Final Batch Loss: 0.1347348988056183\n",
      "Epoch 3061, Loss: 0.2557794898748398, Final Batch Loss: 0.1276957243680954\n",
      "Epoch 3062, Loss: 0.23266521841287613, Final Batch Loss: 0.10039081424474716\n",
      "Epoch 3063, Loss: 0.315780371427536, Final Batch Loss: 0.1973438709974289\n",
      "Epoch 3064, Loss: 0.29794906079769135, Final Batch Loss: 0.1710905134677887\n",
      "Epoch 3065, Loss: 0.3037613779306412, Final Batch Loss: 0.18597738444805145\n",
      "Epoch 3066, Loss: 0.3237769305706024, Final Batch Loss: 0.15768282115459442\n",
      "Epoch 3067, Loss: 0.2349582314491272, Final Batch Loss: 0.11093570291996002\n",
      "Epoch 3068, Loss: 0.24617121368646622, Final Batch Loss: 0.12780146300792694\n",
      "Epoch 3069, Loss: 0.3139835000038147, Final Batch Loss: 0.14672259986400604\n",
      "Epoch 3070, Loss: 0.242656372487545, Final Batch Loss: 0.12860213220119476\n",
      "Epoch 3071, Loss: 0.21406656503677368, Final Batch Loss: 0.10933821648359299\n",
      "Epoch 3072, Loss: 0.22004134207963943, Final Batch Loss: 0.1017228290438652\n",
      "Epoch 3073, Loss: 0.215874083340168, Final Batch Loss: 0.10810569673776627\n",
      "Epoch 3074, Loss: 0.2601063549518585, Final Batch Loss: 0.14280401170253754\n",
      "Epoch 3075, Loss: 0.22898025810718536, Final Batch Loss: 0.100749671459198\n",
      "Epoch 3076, Loss: 0.26052936911582947, Final Batch Loss: 0.1639929562807083\n",
      "Epoch 3077, Loss: 0.31006471812725067, Final Batch Loss: 0.16794897615909576\n",
      "Epoch 3078, Loss: 0.2500482574105263, Final Batch Loss: 0.10453230887651443\n",
      "Epoch 3079, Loss: 0.2606322839856148, Final Batch Loss: 0.12120041996240616\n",
      "Epoch 3080, Loss: 0.2594483941793442, Final Batch Loss: 0.11345569789409637\n",
      "Epoch 3081, Loss: 0.24072246253490448, Final Batch Loss: 0.10221801698207855\n",
      "Epoch 3082, Loss: 0.22011969983577728, Final Batch Loss: 0.10632418841123581\n",
      "Epoch 3083, Loss: 0.27678003162145615, Final Batch Loss: 0.15777587890625\n",
      "Epoch 3084, Loss: 0.26864074915647507, Final Batch Loss: 0.15986588597297668\n",
      "Epoch 3085, Loss: 0.27392104268074036, Final Batch Loss: 0.12563522160053253\n",
      "Epoch 3086, Loss: 0.2220381572842598, Final Batch Loss: 0.11985945701599121\n",
      "Epoch 3087, Loss: 0.27559569478034973, Final Batch Loss: 0.1487511396408081\n",
      "Epoch 3088, Loss: 0.23725171387195587, Final Batch Loss: 0.10096167027950287\n",
      "Epoch 3089, Loss: 0.2118006944656372, Final Batch Loss: 0.09064748883247375\n",
      "Epoch 3090, Loss: 0.19185011088848114, Final Batch Loss: 0.09046607464551926\n",
      "Epoch 3091, Loss: 0.2597116306424141, Final Batch Loss: 0.12055737525224686\n",
      "Epoch 3092, Loss: 0.24740806967020035, Final Batch Loss: 0.1356022208929062\n",
      "Epoch 3093, Loss: 0.28911976516246796, Final Batch Loss: 0.17381276190280914\n",
      "Epoch 3094, Loss: 0.24361538141965866, Final Batch Loss: 0.10561724752187729\n",
      "Epoch 3095, Loss: 0.30166204273700714, Final Batch Loss: 0.19560787081718445\n",
      "Epoch 3096, Loss: 0.28088848292827606, Final Batch Loss: 0.11999937891960144\n",
      "Epoch 3097, Loss: 0.21469951421022415, Final Batch Loss: 0.10434065014123917\n",
      "Epoch 3098, Loss: 0.2514074370265007, Final Batch Loss: 0.13674069941043854\n",
      "Epoch 3099, Loss: 0.25310448557138443, Final Batch Loss: 0.1345483511686325\n",
      "Epoch 3100, Loss: 0.26464226841926575, Final Batch Loss: 0.14433662593364716\n",
      "Epoch 3101, Loss: 0.27291981875896454, Final Batch Loss: 0.11415110528469086\n",
      "Epoch 3102, Loss: 0.21474190801382065, Final Batch Loss: 0.11484548449516296\n",
      "Epoch 3103, Loss: 0.2525320276618004, Final Batch Loss: 0.12323325127363205\n",
      "Epoch 3104, Loss: 0.23434723913669586, Final Batch Loss: 0.14053712785243988\n",
      "Epoch 3105, Loss: 0.2512558475136757, Final Batch Loss: 0.12127020210027695\n",
      "Epoch 3106, Loss: 0.20702769607305527, Final Batch Loss: 0.11163520067930222\n",
      "Epoch 3107, Loss: 0.26173506677150726, Final Batch Loss: 0.10113684833049774\n",
      "Epoch 3108, Loss: 0.3106817975640297, Final Batch Loss: 0.12334110587835312\n",
      "Epoch 3109, Loss: 0.2289596125483513, Final Batch Loss: 0.11530175060033798\n",
      "Epoch 3110, Loss: 0.23998697102069855, Final Batch Loss: 0.13624802231788635\n",
      "Epoch 3111, Loss: 0.2463824674487114, Final Batch Loss: 0.13028748333454132\n",
      "Epoch 3112, Loss: 0.24812497943639755, Final Batch Loss: 0.135051891207695\n",
      "Epoch 3113, Loss: 0.24669168144464493, Final Batch Loss: 0.12942570447921753\n",
      "Epoch 3114, Loss: 0.22362859547138214, Final Batch Loss: 0.116036556661129\n",
      "Epoch 3115, Loss: 0.23300348222255707, Final Batch Loss: 0.1000540554523468\n",
      "Epoch 3116, Loss: 0.27634406089782715, Final Batch Loss: 0.130320206284523\n",
      "Epoch 3117, Loss: 0.239972323179245, Final Batch Loss: 0.11959831416606903\n",
      "Epoch 3118, Loss: 0.26432549953460693, Final Batch Loss: 0.12293578684329987\n",
      "Epoch 3119, Loss: 0.2042572945356369, Final Batch Loss: 0.10503019392490387\n",
      "Epoch 3120, Loss: 0.25592228025197983, Final Batch Loss: 0.13872507214546204\n",
      "Epoch 3121, Loss: 0.3033628761768341, Final Batch Loss: 0.15126657485961914\n",
      "Epoch 3122, Loss: 0.27559198439121246, Final Batch Loss: 0.1376732438802719\n",
      "Epoch 3123, Loss: 0.24175651371479034, Final Batch Loss: 0.0937754213809967\n",
      "Epoch 3124, Loss: 0.26732345670461655, Final Batch Loss: 0.11692031472921371\n",
      "Epoch 3125, Loss: 0.23137343674898148, Final Batch Loss: 0.11661049723625183\n",
      "Epoch 3126, Loss: 0.23171748965978622, Final Batch Loss: 0.14402753114700317\n",
      "Epoch 3127, Loss: 0.2226886749267578, Final Batch Loss: 0.1290726214647293\n",
      "Epoch 3128, Loss: 0.24436786770820618, Final Batch Loss: 0.12247226387262344\n",
      "Epoch 3129, Loss: 0.3030889630317688, Final Batch Loss: 0.15427735447883606\n",
      "Epoch 3130, Loss: 0.2875353768467903, Final Batch Loss: 0.16879130899906158\n",
      "Epoch 3131, Loss: 0.2610512673854828, Final Batch Loss: 0.16853152215480804\n",
      "Epoch 3132, Loss: 0.2832541763782501, Final Batch Loss: 0.1637355238199234\n",
      "Epoch 3133, Loss: 0.28392963111400604, Final Batch Loss: 0.1479741483926773\n",
      "Epoch 3134, Loss: 0.2204284742474556, Final Batch Loss: 0.11255428194999695\n",
      "Epoch 3135, Loss: 0.23492344468832016, Final Batch Loss: 0.126376211643219\n",
      "Epoch 3136, Loss: 0.26120156794786453, Final Batch Loss: 0.09926167875528336\n",
      "Epoch 3137, Loss: 0.2522941157221794, Final Batch Loss: 0.1062285378575325\n",
      "Epoch 3138, Loss: 0.21992873400449753, Final Batch Loss: 0.10972489416599274\n",
      "Epoch 3139, Loss: 0.259892001748085, Final Batch Loss: 0.13086742162704468\n",
      "Epoch 3140, Loss: 0.27554232627153397, Final Batch Loss: 0.09819569438695908\n",
      "Epoch 3141, Loss: 0.2594708949327469, Final Batch Loss: 0.14216631650924683\n",
      "Epoch 3142, Loss: 0.2567225247621536, Final Batch Loss: 0.12114846706390381\n",
      "Epoch 3143, Loss: 0.2346363291144371, Final Batch Loss: 0.11114559322595596\n",
      "Epoch 3144, Loss: 0.3075174391269684, Final Batch Loss: 0.09102672338485718\n",
      "Epoch 3145, Loss: 0.2620927095413208, Final Batch Loss: 0.17616434395313263\n",
      "Epoch 3146, Loss: 0.24691975861787796, Final Batch Loss: 0.15030501782894135\n",
      "Epoch 3147, Loss: 0.2598774656653404, Final Batch Loss: 0.10606539994478226\n",
      "Epoch 3148, Loss: 0.26535895466804504, Final Batch Loss: 0.13953249156475067\n",
      "Epoch 3149, Loss: 0.2564944922924042, Final Batch Loss: 0.13711746037006378\n",
      "Epoch 3150, Loss: 0.2040380910038948, Final Batch Loss: 0.0997033417224884\n",
      "Epoch 3151, Loss: 0.25924256443977356, Final Batch Loss: 0.12719513475894928\n",
      "Epoch 3152, Loss: 0.22836071997880936, Final Batch Loss: 0.12470109015703201\n",
      "Epoch 3153, Loss: 0.27814190089702606, Final Batch Loss: 0.13215379416942596\n",
      "Epoch 3154, Loss: 0.28172746300697327, Final Batch Loss: 0.17811191082000732\n",
      "Epoch 3155, Loss: 0.17098630592226982, Final Batch Loss: 0.11447110027074814\n",
      "Epoch 3156, Loss: 0.28215382993221283, Final Batch Loss: 0.1214037537574768\n",
      "Epoch 3157, Loss: 0.34527361392974854, Final Batch Loss: 0.17014957964420319\n",
      "Epoch 3158, Loss: 0.24110354483127594, Final Batch Loss: 0.08113420009613037\n",
      "Epoch 3159, Loss: 0.22267872095108032, Final Batch Loss: 0.13638687133789062\n",
      "Epoch 3160, Loss: 0.2492011860013008, Final Batch Loss: 0.12672890722751617\n",
      "Epoch 3161, Loss: 0.21952415257692337, Final Batch Loss: 0.10371404886245728\n",
      "Epoch 3162, Loss: 0.20413616299629211, Final Batch Loss: 0.10220104455947876\n",
      "Epoch 3163, Loss: 0.23173322528600693, Final Batch Loss: 0.11838279664516449\n",
      "Epoch 3164, Loss: 0.31553521752357483, Final Batch Loss: 0.13866840302944183\n",
      "Epoch 3165, Loss: 0.23872678726911545, Final Batch Loss: 0.1502808779478073\n",
      "Epoch 3166, Loss: 0.25322557240724564, Final Batch Loss: 0.12143131345510483\n",
      "Epoch 3167, Loss: 0.22412264347076416, Final Batch Loss: 0.10189446061849594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3168, Loss: 0.231692373752594, Final Batch Loss: 0.11035335808992386\n",
      "Epoch 3169, Loss: 0.2958460599184036, Final Batch Loss: 0.1722799837589264\n",
      "Epoch 3170, Loss: 0.3100575953722, Final Batch Loss: 0.13305996358394623\n",
      "Epoch 3171, Loss: 0.31870877742767334, Final Batch Loss: 0.13448520004749298\n",
      "Epoch 3172, Loss: 0.26697060465812683, Final Batch Loss: 0.14608260989189148\n",
      "Epoch 3173, Loss: 0.27420511841773987, Final Batch Loss: 0.1786196231842041\n",
      "Epoch 3174, Loss: 0.23654527962207794, Final Batch Loss: 0.1116870790719986\n",
      "Epoch 3175, Loss: 0.19772615283727646, Final Batch Loss: 0.09619926661252975\n",
      "Epoch 3176, Loss: 0.23852083832025528, Final Batch Loss: 0.1452159881591797\n",
      "Epoch 3177, Loss: 0.23642101883888245, Final Batch Loss: 0.11634572595357895\n",
      "Epoch 3178, Loss: 0.2551434636116028, Final Batch Loss: 0.12962287664413452\n",
      "Epoch 3179, Loss: 0.23250796645879745, Final Batch Loss: 0.1375587433576584\n",
      "Epoch 3180, Loss: 0.2745288759469986, Final Batch Loss: 0.14732423424720764\n",
      "Epoch 3181, Loss: 0.23293683677911758, Final Batch Loss: 0.11060801148414612\n",
      "Epoch 3182, Loss: 0.2825684994459152, Final Batch Loss: 0.16087447106838226\n",
      "Epoch 3183, Loss: 0.3217156380414963, Final Batch Loss: 0.25477778911590576\n",
      "Epoch 3184, Loss: 0.18938729912042618, Final Batch Loss: 0.10934862494468689\n",
      "Epoch 3185, Loss: 0.24982628226280212, Final Batch Loss: 0.10342797636985779\n",
      "Epoch 3186, Loss: 0.2949637621641159, Final Batch Loss: 0.13913913071155548\n",
      "Epoch 3187, Loss: 0.244881771504879, Final Batch Loss: 0.10647334903478622\n",
      "Epoch 3188, Loss: 0.3518730252981186, Final Batch Loss: 0.18594956398010254\n",
      "Epoch 3189, Loss: 0.2370988205075264, Final Batch Loss: 0.10154903680086136\n",
      "Epoch 3190, Loss: 0.2787449359893799, Final Batch Loss: 0.14180505275726318\n",
      "Epoch 3191, Loss: 0.2611372023820877, Final Batch Loss: 0.15450811386108398\n",
      "Epoch 3192, Loss: 0.31191402673721313, Final Batch Loss: 0.12933090329170227\n",
      "Epoch 3193, Loss: 0.25852634012699127, Final Batch Loss: 0.09935863316059113\n",
      "Epoch 3194, Loss: 0.22391030192375183, Final Batch Loss: 0.11647383868694305\n",
      "Epoch 3195, Loss: 0.22998008131980896, Final Batch Loss: 0.14219236373901367\n",
      "Epoch 3196, Loss: 0.2989218309521675, Final Batch Loss: 0.17446818947792053\n",
      "Epoch 3197, Loss: 0.23207440972328186, Final Batch Loss: 0.11286820471286774\n",
      "Epoch 3198, Loss: 0.24601688981056213, Final Batch Loss: 0.1382012665271759\n",
      "Epoch 3199, Loss: 0.3333713263273239, Final Batch Loss: 0.13587334752082825\n",
      "Epoch 3200, Loss: 0.2726915329694748, Final Batch Loss: 0.14764882624149323\n",
      "Epoch 3201, Loss: 0.24270804226398468, Final Batch Loss: 0.09897848963737488\n",
      "Epoch 3202, Loss: 0.28460830450057983, Final Batch Loss: 0.1477438062429428\n",
      "Epoch 3203, Loss: 0.23018057644367218, Final Batch Loss: 0.12238813936710358\n",
      "Epoch 3204, Loss: 0.29682500660419464, Final Batch Loss: 0.12892846763134003\n",
      "Epoch 3205, Loss: 0.21996288001537323, Final Batch Loss: 0.12323951721191406\n",
      "Epoch 3206, Loss: 0.23691413551568985, Final Batch Loss: 0.11612164229154587\n",
      "Epoch 3207, Loss: 0.23341701179742813, Final Batch Loss: 0.11304475367069244\n",
      "Epoch 3208, Loss: 0.3953954949975014, Final Batch Loss: 0.1231069341301918\n",
      "Epoch 3209, Loss: 0.25057076662778854, Final Batch Loss: 0.1193452998995781\n",
      "Epoch 3210, Loss: 0.2080821692943573, Final Batch Loss: 0.09616941213607788\n",
      "Epoch 3211, Loss: 0.21098433434963226, Final Batch Loss: 0.09224867075681686\n",
      "Epoch 3212, Loss: 0.24909498542547226, Final Batch Loss: 0.10685387998819351\n",
      "Epoch 3213, Loss: 0.2365127056837082, Final Batch Loss: 0.11644130200147629\n",
      "Epoch 3214, Loss: 0.26915784180164337, Final Batch Loss: 0.13848266005516052\n",
      "Epoch 3215, Loss: 0.28591038286685944, Final Batch Loss: 0.15290579199790955\n",
      "Epoch 3216, Loss: 0.3176170140504837, Final Batch Loss: 0.18532705307006836\n",
      "Epoch 3217, Loss: 0.2380259931087494, Final Batch Loss: 0.1214836984872818\n",
      "Epoch 3218, Loss: 0.20478785783052444, Final Batch Loss: 0.08467315137386322\n",
      "Epoch 3219, Loss: 0.279656782746315, Final Batch Loss: 0.16448360681533813\n",
      "Epoch 3220, Loss: 0.23513450473546982, Final Batch Loss: 0.11438378691673279\n",
      "Epoch 3221, Loss: 0.21085289120674133, Final Batch Loss: 0.0965438038110733\n",
      "Epoch 3222, Loss: 0.21307337284088135, Final Batch Loss: 0.11417338997125626\n",
      "Epoch 3223, Loss: 0.246240496635437, Final Batch Loss: 0.11770524084568024\n",
      "Epoch 3224, Loss: 0.2999110370874405, Final Batch Loss: 0.12289918959140778\n",
      "Epoch 3225, Loss: 0.24657966941595078, Final Batch Loss: 0.11605473607778549\n",
      "Epoch 3226, Loss: 0.33556103706359863, Final Batch Loss: 0.14015251398086548\n",
      "Epoch 3227, Loss: 0.27999719232320786, Final Batch Loss: 0.1682717502117157\n",
      "Epoch 3228, Loss: 0.21710120141506195, Final Batch Loss: 0.08990246057510376\n",
      "Epoch 3229, Loss: 0.2562812492251396, Final Batch Loss: 0.1324089765548706\n",
      "Epoch 3230, Loss: 0.2800232023000717, Final Batch Loss: 0.11149126291275024\n",
      "Epoch 3231, Loss: 0.22220555692911148, Final Batch Loss: 0.11904826015233994\n",
      "Epoch 3232, Loss: 0.23271820694208145, Final Batch Loss: 0.12498033046722412\n",
      "Epoch 3233, Loss: 0.2581590935587883, Final Batch Loss: 0.1495358645915985\n",
      "Epoch 3234, Loss: 0.23874301463365555, Final Batch Loss: 0.09415624290704727\n",
      "Epoch 3235, Loss: 0.2612454816699028, Final Batch Loss: 0.11886578053236008\n",
      "Epoch 3236, Loss: 0.1947450488805771, Final Batch Loss: 0.08511821180582047\n",
      "Epoch 3237, Loss: 0.22285302728414536, Final Batch Loss: 0.10649918764829636\n",
      "Epoch 3238, Loss: 0.2018999084830284, Final Batch Loss: 0.0813525840640068\n",
      "Epoch 3239, Loss: 0.19871626049280167, Final Batch Loss: 0.07084091752767563\n",
      "Epoch 3240, Loss: 0.18263038992881775, Final Batch Loss: 0.10341767221689224\n",
      "Epoch 3241, Loss: 0.1701125055551529, Final Batch Loss: 0.09755930304527283\n",
      "Epoch 3242, Loss: 0.22637387365102768, Final Batch Loss: 0.12935468554496765\n",
      "Epoch 3243, Loss: 0.267763614654541, Final Batch Loss: 0.14393198490142822\n",
      "Epoch 3244, Loss: 0.18730251491069794, Final Batch Loss: 0.0697721466422081\n",
      "Epoch 3245, Loss: 0.2809901684522629, Final Batch Loss: 0.0913911908864975\n",
      "Epoch 3246, Loss: 0.2024061158299446, Final Batch Loss: 0.10948439687490463\n",
      "Epoch 3247, Loss: 0.2644796594977379, Final Batch Loss: 0.1582161784172058\n",
      "Epoch 3248, Loss: 0.276286281645298, Final Batch Loss: 0.17041082680225372\n",
      "Epoch 3249, Loss: 0.28965847194194794, Final Batch Loss: 0.15518374741077423\n",
      "Epoch 3250, Loss: 0.2568397670984268, Final Batch Loss: 0.16956208646297455\n",
      "Epoch 3251, Loss: 0.22913622856140137, Final Batch Loss: 0.1206497848033905\n",
      "Epoch 3252, Loss: 0.22766033560037613, Final Batch Loss: 0.13675673305988312\n",
      "Epoch 3253, Loss: 0.23304273933172226, Final Batch Loss: 0.11923383921384811\n",
      "Epoch 3254, Loss: 0.26810283958911896, Final Batch Loss: 0.12858635187149048\n",
      "Epoch 3255, Loss: 0.2738730311393738, Final Batch Loss: 0.10602748394012451\n",
      "Epoch 3256, Loss: 0.24715052545070648, Final Batch Loss: 0.10939823091030121\n",
      "Epoch 3257, Loss: 0.25067324936389923, Final Batch Loss: 0.11249616742134094\n",
      "Epoch 3258, Loss: 0.2074616402387619, Final Batch Loss: 0.10215742886066437\n",
      "Epoch 3259, Loss: 0.2358621284365654, Final Batch Loss: 0.13475604355335236\n",
      "Epoch 3260, Loss: 0.21156319975852966, Final Batch Loss: 0.10529966652393341\n",
      "Epoch 3261, Loss: 0.2657531127333641, Final Batch Loss: 0.15180425345897675\n",
      "Epoch 3262, Loss: 0.22940672934055328, Final Batch Loss: 0.12738563120365143\n",
      "Epoch 3263, Loss: 0.18442538380622864, Final Batch Loss: 0.08280707150697708\n",
      "Epoch 3264, Loss: 0.2520175352692604, Final Batch Loss: 0.14095701277256012\n",
      "Epoch 3265, Loss: 0.31997059285640717, Final Batch Loss: 0.22595146298408508\n",
      "Epoch 3266, Loss: 0.1955694481730461, Final Batch Loss: 0.11257457733154297\n",
      "Epoch 3267, Loss: 0.18308698385953903, Final Batch Loss: 0.09754043072462082\n",
      "Epoch 3268, Loss: 0.2768353894352913, Final Batch Loss: 0.1894071102142334\n",
      "Epoch 3269, Loss: 0.26893334090709686, Final Batch Loss: 0.1594158113002777\n",
      "Epoch 3270, Loss: 0.26023024320602417, Final Batch Loss: 0.1523474156856537\n",
      "Epoch 3271, Loss: 0.27995194494724274, Final Batch Loss: 0.14106622338294983\n",
      "Epoch 3272, Loss: 0.273394376039505, Final Batch Loss: 0.12226125597953796\n",
      "Epoch 3273, Loss: 0.22275185585021973, Final Batch Loss: 0.08613605797290802\n",
      "Epoch 3274, Loss: 0.18577909469604492, Final Batch Loss: 0.08869796991348267\n",
      "Epoch 3275, Loss: 0.2063109576702118, Final Batch Loss: 0.12184746563434601\n",
      "Epoch 3276, Loss: 0.2238636314868927, Final Batch Loss: 0.07944062352180481\n",
      "Epoch 3277, Loss: 0.23829616606235504, Final Batch Loss: 0.10133235156536102\n",
      "Epoch 3278, Loss: 0.26251402497291565, Final Batch Loss: 0.13636431097984314\n",
      "Epoch 3279, Loss: 0.22239824384450912, Final Batch Loss: 0.11929288506507874\n",
      "Epoch 3280, Loss: 0.26571421325206757, Final Batch Loss: 0.1394294798374176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3281, Loss: 0.19294049590826035, Final Batch Loss: 0.10458780080080032\n",
      "Epoch 3282, Loss: 0.2651631236076355, Final Batch Loss: 0.1673101931810379\n",
      "Epoch 3283, Loss: 0.22351347655057907, Final Batch Loss: 0.11083122342824936\n",
      "Epoch 3284, Loss: 0.21428222954273224, Final Batch Loss: 0.11241860687732697\n",
      "Epoch 3285, Loss: 0.24809180200099945, Final Batch Loss: 0.12320641428232193\n",
      "Epoch 3286, Loss: 0.2427075430750847, Final Batch Loss: 0.1302342563867569\n",
      "Epoch 3287, Loss: 0.23457053303718567, Final Batch Loss: 0.12800776958465576\n",
      "Epoch 3288, Loss: 0.22861889749765396, Final Batch Loss: 0.13318903744220734\n",
      "Epoch 3289, Loss: 0.23289451003074646, Final Batch Loss: 0.13257835805416107\n",
      "Epoch 3290, Loss: 0.1975705772638321, Final Batch Loss: 0.09614372998476028\n",
      "Epoch 3291, Loss: 0.2621590718626976, Final Batch Loss: 0.1540008783340454\n",
      "Epoch 3292, Loss: 0.2538788616657257, Final Batch Loss: 0.10027565062046051\n",
      "Epoch 3293, Loss: 0.15611211210489273, Final Batch Loss: 0.07102494686841965\n",
      "Epoch 3294, Loss: 0.19514977931976318, Final Batch Loss: 0.08587200939655304\n",
      "Epoch 3295, Loss: 0.295059934258461, Final Batch Loss: 0.18450449407100677\n",
      "Epoch 3296, Loss: 0.18265597522258759, Final Batch Loss: 0.10693361610174179\n",
      "Epoch 3297, Loss: 0.23552270978689194, Final Batch Loss: 0.09990449994802475\n",
      "Epoch 3298, Loss: 0.2116149738430977, Final Batch Loss: 0.08067392557859421\n",
      "Epoch 3299, Loss: 0.1986190527677536, Final Batch Loss: 0.10585146397352219\n",
      "Epoch 3300, Loss: 0.27741214632987976, Final Batch Loss: 0.1410030573606491\n",
      "Epoch 3301, Loss: 0.26861758530139923, Final Batch Loss: 0.09841698408126831\n",
      "Epoch 3302, Loss: 0.21283167600631714, Final Batch Loss: 0.11914394795894623\n",
      "Epoch 3303, Loss: 0.22473301738500595, Final Batch Loss: 0.12705115973949432\n",
      "Epoch 3304, Loss: 0.25202227383852005, Final Batch Loss: 0.11501092463731766\n",
      "Epoch 3305, Loss: 0.29899007081985474, Final Batch Loss: 0.16187767684459686\n",
      "Epoch 3306, Loss: 0.22604618966579437, Final Batch Loss: 0.11022917181253433\n",
      "Epoch 3307, Loss: 0.25754139572381973, Final Batch Loss: 0.11351453512907028\n",
      "Epoch 3308, Loss: 0.2064293771982193, Final Batch Loss: 0.10035226494073868\n",
      "Epoch 3309, Loss: 0.23753342777490616, Final Batch Loss: 0.12607257068157196\n",
      "Epoch 3310, Loss: 0.22183573991060257, Final Batch Loss: 0.08817710727453232\n",
      "Epoch 3311, Loss: 0.20350565016269684, Final Batch Loss: 0.09833673387765884\n",
      "Epoch 3312, Loss: 0.2246905043721199, Final Batch Loss: 0.10108473151922226\n",
      "Epoch 3313, Loss: 0.23007500171661377, Final Batch Loss: 0.06505334377288818\n",
      "Epoch 3314, Loss: 0.23227420449256897, Final Batch Loss: 0.12889143824577332\n",
      "Epoch 3315, Loss: 0.23785224556922913, Final Batch Loss: 0.0791553258895874\n",
      "Epoch 3316, Loss: 0.21473435312509537, Final Batch Loss: 0.06354422122240067\n",
      "Epoch 3317, Loss: 0.1666519194841385, Final Batch Loss: 0.08117973804473877\n",
      "Epoch 3318, Loss: 0.24379872530698776, Final Batch Loss: 0.12727582454681396\n",
      "Epoch 3319, Loss: 0.21416658908128738, Final Batch Loss: 0.13289521634578705\n",
      "Epoch 3320, Loss: 0.1997719258069992, Final Batch Loss: 0.11431042850017548\n",
      "Epoch 3321, Loss: 0.21552510559558868, Final Batch Loss: 0.08254081010818481\n",
      "Epoch 3322, Loss: 0.21541088074445724, Final Batch Loss: 0.07255040854215622\n",
      "Epoch 3323, Loss: 0.20542044192552567, Final Batch Loss: 0.10840717703104019\n",
      "Epoch 3324, Loss: 0.2150416150689125, Final Batch Loss: 0.12148048728704453\n",
      "Epoch 3325, Loss: 0.26644405722618103, Final Batch Loss: 0.1397457718849182\n",
      "Epoch 3326, Loss: 0.1991143822669983, Final Batch Loss: 0.07914124429225922\n",
      "Epoch 3327, Loss: 0.2276572808623314, Final Batch Loss: 0.11451301723718643\n",
      "Epoch 3328, Loss: 0.23293334245681763, Final Batch Loss: 0.11904550343751907\n",
      "Epoch 3329, Loss: 0.22509458661079407, Final Batch Loss: 0.10730505734682083\n",
      "Epoch 3330, Loss: 0.1859055906534195, Final Batch Loss: 0.09417568892240524\n",
      "Epoch 3331, Loss: 0.22062750160694122, Final Batch Loss: 0.09227615594863892\n",
      "Epoch 3332, Loss: 0.22511101514101028, Final Batch Loss: 0.08421777933835983\n",
      "Epoch 3333, Loss: 0.25372011959552765, Final Batch Loss: 0.12439391016960144\n",
      "Epoch 3334, Loss: 0.15692397207021713, Final Batch Loss: 0.09416364133358002\n",
      "Epoch 3335, Loss: 0.184393472969532, Final Batch Loss: 0.08791244775056839\n",
      "Epoch 3336, Loss: 0.24713443964719772, Final Batch Loss: 0.12265972048044205\n",
      "Epoch 3337, Loss: 0.26947730779647827, Final Batch Loss: 0.16275542974472046\n",
      "Epoch 3338, Loss: 0.22276200354099274, Final Batch Loss: 0.1139693334698677\n",
      "Epoch 3339, Loss: 0.3112112954258919, Final Batch Loss: 0.19288672506809235\n",
      "Epoch 3340, Loss: 0.19234073162078857, Final Batch Loss: 0.08908671885728836\n",
      "Epoch 3341, Loss: 0.2776634320616722, Final Batch Loss: 0.16958770155906677\n",
      "Epoch 3342, Loss: 0.29405443370342255, Final Batch Loss: 0.16541527211666107\n",
      "Epoch 3343, Loss: 0.1816800832748413, Final Batch Loss: 0.0770384892821312\n",
      "Epoch 3344, Loss: 0.20351102203130722, Final Batch Loss: 0.10561805963516235\n",
      "Epoch 3345, Loss: 0.26906169950962067, Final Batch Loss: 0.151699036359787\n",
      "Epoch 3346, Loss: 0.23980335146188736, Final Batch Loss: 0.10403173416852951\n",
      "Epoch 3347, Loss: 0.17048659920692444, Final Batch Loss: 0.07621525228023529\n",
      "Epoch 3348, Loss: 0.21390732377767563, Final Batch Loss: 0.11226020753383636\n",
      "Epoch 3349, Loss: 0.19844001531600952, Final Batch Loss: 0.08638627082109451\n",
      "Epoch 3350, Loss: 0.2428603321313858, Final Batch Loss: 0.1143157035112381\n",
      "Epoch 3351, Loss: 0.21622347086668015, Final Batch Loss: 0.12026381492614746\n",
      "Epoch 3352, Loss: 0.2707769572734833, Final Batch Loss: 0.112822026014328\n",
      "Epoch 3353, Loss: 0.2589888796210289, Final Batch Loss: 0.11813316494226456\n",
      "Epoch 3354, Loss: 0.19241084903478622, Final Batch Loss: 0.08557863533496857\n",
      "Epoch 3355, Loss: 0.220050610601902, Final Batch Loss: 0.09109138697385788\n",
      "Epoch 3356, Loss: 0.24781230092048645, Final Batch Loss: 0.1177874207496643\n",
      "Epoch 3357, Loss: 0.2062496766448021, Final Batch Loss: 0.12018629908561707\n",
      "Epoch 3358, Loss: 0.2503695711493492, Final Batch Loss: 0.12659461796283722\n",
      "Epoch 3359, Loss: 0.2446487918496132, Final Batch Loss: 0.1282402127981186\n",
      "Epoch 3360, Loss: 0.23343929648399353, Final Batch Loss: 0.1076212227344513\n",
      "Epoch 3361, Loss: 0.26537709683179855, Final Batch Loss: 0.15429313480854034\n",
      "Epoch 3362, Loss: 0.22061510384082794, Final Batch Loss: 0.1102805808186531\n",
      "Epoch 3363, Loss: 0.21499618887901306, Final Batch Loss: 0.12247671186923981\n",
      "Epoch 3364, Loss: 0.24470674246549606, Final Batch Loss: 0.12571702897548676\n",
      "Epoch 3365, Loss: 0.2295389249920845, Final Batch Loss: 0.09165366739034653\n",
      "Epoch 3366, Loss: 0.20271043479442596, Final Batch Loss: 0.1029413491487503\n",
      "Epoch 3367, Loss: 0.20665626227855682, Final Batch Loss: 0.09025441110134125\n",
      "Epoch 3368, Loss: 0.28351980447769165, Final Batch Loss: 0.14640742540359497\n",
      "Epoch 3369, Loss: 0.23308216780424118, Final Batch Loss: 0.12333264201879501\n",
      "Epoch 3370, Loss: 0.2967292368412018, Final Batch Loss: 0.1544518768787384\n",
      "Epoch 3371, Loss: 0.20853211730718613, Final Batch Loss: 0.09958390146493912\n",
      "Epoch 3372, Loss: 0.18074161559343338, Final Batch Loss: 0.09595149010419846\n",
      "Epoch 3373, Loss: 0.26182296872138977, Final Batch Loss: 0.14969055354595184\n",
      "Epoch 3374, Loss: 0.1514807939529419, Final Batch Loss: 0.06837794929742813\n",
      "Epoch 3375, Loss: 0.23293620347976685, Final Batch Loss: 0.1355440467596054\n",
      "Epoch 3376, Loss: 0.25226323306560516, Final Batch Loss: 0.10148356854915619\n",
      "Epoch 3377, Loss: 0.3259231820702553, Final Batch Loss: 0.12213394790887833\n",
      "Epoch 3378, Loss: 0.21428152173757553, Final Batch Loss: 0.10914231091737747\n",
      "Epoch 3379, Loss: 0.1819840595126152, Final Batch Loss: 0.11311593651771545\n",
      "Epoch 3380, Loss: 0.1931530237197876, Final Batch Loss: 0.09674245864152908\n",
      "Epoch 3381, Loss: 0.2897973507642746, Final Batch Loss: 0.12146376073360443\n",
      "Epoch 3382, Loss: 0.24498995393514633, Final Batch Loss: 0.1429605334997177\n",
      "Epoch 3383, Loss: 0.2328568920493126, Final Batch Loss: 0.1324833631515503\n",
      "Epoch 3384, Loss: 0.23012878745794296, Final Batch Loss: 0.08645182102918625\n",
      "Epoch 3385, Loss: 0.23884928971529007, Final Batch Loss: 0.1271526962518692\n",
      "Epoch 3386, Loss: 0.2615443468093872, Final Batch Loss: 0.12182097136974335\n",
      "Epoch 3387, Loss: 0.26506319642066956, Final Batch Loss: 0.1316637247800827\n",
      "Epoch 3388, Loss: 0.18092907965183258, Final Batch Loss: 0.08414148539304733\n",
      "Epoch 3389, Loss: 0.2862560823559761, Final Batch Loss: 0.1658332496881485\n",
      "Epoch 3390, Loss: 0.25706715136766434, Final Batch Loss: 0.1705871820449829\n",
      "Epoch 3391, Loss: 0.22005656361579895, Final Batch Loss: 0.11975152790546417\n",
      "Epoch 3392, Loss: 0.18105100095272064, Final Batch Loss: 0.07168076187372208\n",
      "Epoch 3393, Loss: 0.34517133235931396, Final Batch Loss: 0.17892847955226898\n",
      "Epoch 3394, Loss: 0.22642138600349426, Final Batch Loss: 0.1295386701822281\n",
      "Epoch 3395, Loss: 0.18289589881896973, Final Batch Loss: 0.08224239200353622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3396, Loss: 0.21565697342157364, Final Batch Loss: 0.12322662770748138\n",
      "Epoch 3397, Loss: 0.20491176843643188, Final Batch Loss: 0.11909838765859604\n",
      "Epoch 3398, Loss: 0.20354457944631577, Final Batch Loss: 0.10677904635667801\n",
      "Epoch 3399, Loss: 0.22240768373012543, Final Batch Loss: 0.10241824388504028\n",
      "Epoch 3400, Loss: 0.19781478494405746, Final Batch Loss: 0.08037254959344864\n",
      "Epoch 3401, Loss: 0.2691691145300865, Final Batch Loss: 0.09933789819478989\n",
      "Epoch 3402, Loss: 0.24409721791744232, Final Batch Loss: 0.13260698318481445\n",
      "Epoch 3403, Loss: 0.18551678955554962, Final Batch Loss: 0.10279827564954758\n",
      "Epoch 3404, Loss: 0.17876719683408737, Final Batch Loss: 0.09825454652309418\n",
      "Epoch 3405, Loss: 0.18147484958171844, Final Batch Loss: 0.10767345130443573\n",
      "Epoch 3406, Loss: 0.18185677379369736, Final Batch Loss: 0.10164478421211243\n",
      "Epoch 3407, Loss: 0.2072872593998909, Final Batch Loss: 0.1136351004242897\n",
      "Epoch 3408, Loss: 0.16574732959270477, Final Batch Loss: 0.084244005382061\n",
      "Epoch 3409, Loss: 0.23354673385620117, Final Batch Loss: 0.09879554808139801\n",
      "Epoch 3410, Loss: 0.18495281040668488, Final Batch Loss: 0.09002470970153809\n",
      "Epoch 3411, Loss: 0.1337178722023964, Final Batch Loss: 0.0764436200261116\n",
      "Epoch 3412, Loss: 0.21688803285360336, Final Batch Loss: 0.11274223029613495\n",
      "Epoch 3413, Loss: 0.20553234964609146, Final Batch Loss: 0.12177567929029465\n",
      "Epoch 3414, Loss: 0.23537158221006393, Final Batch Loss: 0.14136944711208344\n",
      "Epoch 3415, Loss: 0.2349650263786316, Final Batch Loss: 0.12952707707881927\n",
      "Epoch 3416, Loss: 0.19894377887248993, Final Batch Loss: 0.09515875577926636\n",
      "Epoch 3417, Loss: 0.19707610458135605, Final Batch Loss: 0.08435238897800446\n",
      "Epoch 3418, Loss: 0.22762667387723923, Final Batch Loss: 0.08637427538633347\n",
      "Epoch 3419, Loss: 0.17600828409194946, Final Batch Loss: 0.08368042856454849\n",
      "Epoch 3420, Loss: 0.20837070792913437, Final Batch Loss: 0.11168986558914185\n",
      "Epoch 3421, Loss: 0.2005392387509346, Final Batch Loss: 0.09742260724306107\n",
      "Epoch 3422, Loss: 0.2320433109998703, Final Batch Loss: 0.11627498269081116\n",
      "Epoch 3423, Loss: 0.25823818892240524, Final Batch Loss: 0.10356570035219193\n",
      "Epoch 3424, Loss: 0.23363005369901657, Final Batch Loss: 0.10937178879976273\n",
      "Epoch 3425, Loss: 0.18217749893665314, Final Batch Loss: 0.09285464137792587\n",
      "Epoch 3426, Loss: 0.19308993220329285, Final Batch Loss: 0.07059276849031448\n",
      "Epoch 3427, Loss: 0.24913446605205536, Final Batch Loss: 0.1283300518989563\n",
      "Epoch 3428, Loss: 0.21567358821630478, Final Batch Loss: 0.08222002536058426\n",
      "Epoch 3429, Loss: 0.18857617676258087, Final Batch Loss: 0.07965732365846634\n",
      "Epoch 3430, Loss: 0.2013825699687004, Final Batch Loss: 0.08299464732408524\n",
      "Epoch 3431, Loss: 0.3066566735506058, Final Batch Loss: 0.15564122796058655\n",
      "Epoch 3432, Loss: 0.21924765408039093, Final Batch Loss: 0.10536357015371323\n",
      "Epoch 3433, Loss: 0.2183271422982216, Final Batch Loss: 0.11122526973485947\n",
      "Epoch 3434, Loss: 0.17428715527057648, Final Batch Loss: 0.08925094455480576\n",
      "Epoch 3435, Loss: 0.24966903030872345, Final Batch Loss: 0.1452329158782959\n",
      "Epoch 3436, Loss: 0.18975135684013367, Final Batch Loss: 0.10596319288015366\n",
      "Epoch 3437, Loss: 0.20547138154506683, Final Batch Loss: 0.10556987673044205\n",
      "Epoch 3438, Loss: 0.2011372223496437, Final Batch Loss: 0.11512161791324615\n",
      "Epoch 3439, Loss: 0.23711492121219635, Final Batch Loss: 0.1138230413198471\n",
      "Epoch 3440, Loss: 0.21241791546344757, Final Batch Loss: 0.10736517608165741\n",
      "Epoch 3441, Loss: 0.23053137212991714, Final Batch Loss: 0.11855115741491318\n",
      "Epoch 3442, Loss: 0.17321119457483292, Final Batch Loss: 0.06841226667165756\n",
      "Epoch 3443, Loss: 0.17809737473726273, Final Batch Loss: 0.09094395488500595\n",
      "Epoch 3444, Loss: 0.21195098012685776, Final Batch Loss: 0.11544741690158844\n",
      "Epoch 3445, Loss: 0.20260094106197357, Final Batch Loss: 0.12791182100772858\n",
      "Epoch 3446, Loss: 0.22212732583284378, Final Batch Loss: 0.0810028687119484\n",
      "Epoch 3447, Loss: 0.1767084151506424, Final Batch Loss: 0.10098303854465485\n",
      "Epoch 3448, Loss: 0.1960606873035431, Final Batch Loss: 0.0803033635020256\n",
      "Epoch 3449, Loss: 0.1566186398267746, Final Batch Loss: 0.07910393923521042\n",
      "Epoch 3450, Loss: 0.23240934312343597, Final Batch Loss: 0.11404106765985489\n",
      "Epoch 3451, Loss: 0.19828208535909653, Final Batch Loss: 0.1141788586974144\n",
      "Epoch 3452, Loss: 0.26288963854312897, Final Batch Loss: 0.11506013572216034\n",
      "Epoch 3453, Loss: 0.2850745767354965, Final Batch Loss: 0.15009963512420654\n",
      "Epoch 3454, Loss: 0.24506866931915283, Final Batch Loss: 0.09207029640674591\n",
      "Epoch 3455, Loss: 0.1994684562087059, Final Batch Loss: 0.09186290949583054\n",
      "Epoch 3456, Loss: 0.13963472098112106, Final Batch Loss: 0.08027151972055435\n",
      "Epoch 3457, Loss: 0.2678030878305435, Final Batch Loss: 0.15432392060756683\n",
      "Epoch 3458, Loss: 0.22441359609365463, Final Batch Loss: 0.10370632261037827\n",
      "Epoch 3459, Loss: 0.1774398386478424, Final Batch Loss: 0.09963414072990417\n",
      "Epoch 3460, Loss: 0.21142355352640152, Final Batch Loss: 0.11062727123498917\n",
      "Epoch 3461, Loss: 0.1674240231513977, Final Batch Loss: 0.09330091625452042\n",
      "Epoch 3462, Loss: 0.21943216770887375, Final Batch Loss: 0.0995509922504425\n",
      "Epoch 3463, Loss: 0.18523618578910828, Final Batch Loss: 0.07994155585765839\n",
      "Epoch 3464, Loss: 0.27534154057502747, Final Batch Loss: 0.14554616808891296\n",
      "Epoch 3465, Loss: 0.21498310565948486, Final Batch Loss: 0.10941261053085327\n",
      "Epoch 3466, Loss: 0.22429091483354568, Final Batch Loss: 0.0809575542807579\n",
      "Epoch 3467, Loss: 0.20799677819013596, Final Batch Loss: 0.12113302946090698\n",
      "Epoch 3468, Loss: 0.21860577911138535, Final Batch Loss: 0.10292038321495056\n",
      "Epoch 3469, Loss: 0.24382363259792328, Final Batch Loss: 0.11534334719181061\n",
      "Epoch 3470, Loss: 0.1970890536904335, Final Batch Loss: 0.1317933052778244\n",
      "Epoch 3471, Loss: 0.18196309357881546, Final Batch Loss: 0.0922367125749588\n",
      "Epoch 3472, Loss: 0.2231271043419838, Final Batch Loss: 0.10892775654792786\n",
      "Epoch 3473, Loss: 0.21849309653043747, Final Batch Loss: 0.10656474530696869\n",
      "Epoch 3474, Loss: 0.161039337515831, Final Batch Loss: 0.060211002826690674\n",
      "Epoch 3475, Loss: 0.2210826426744461, Final Batch Loss: 0.1079358235001564\n",
      "Epoch 3476, Loss: 0.24747677892446518, Final Batch Loss: 0.15590542554855347\n",
      "Epoch 3477, Loss: 0.22119316458702087, Final Batch Loss: 0.09390270709991455\n",
      "Epoch 3478, Loss: 0.2295539826154709, Final Batch Loss: 0.1444086730480194\n",
      "Epoch 3479, Loss: 0.17753680050373077, Final Batch Loss: 0.08749121427536011\n",
      "Epoch 3480, Loss: 0.1816006600856781, Final Batch Loss: 0.0949719250202179\n",
      "Epoch 3481, Loss: 0.2056746855378151, Final Batch Loss: 0.09977434575557709\n",
      "Epoch 3482, Loss: 0.1683770939707756, Final Batch Loss: 0.09244267642498016\n",
      "Epoch 3483, Loss: 0.13978437706828117, Final Batch Loss: 0.09519414603710175\n",
      "Epoch 3484, Loss: 0.1744147688150406, Final Batch Loss: 0.0812317430973053\n",
      "Epoch 3485, Loss: 0.1878216192126274, Final Batch Loss: 0.08596985042095184\n",
      "Epoch 3486, Loss: 0.17343967407941818, Final Batch Loss: 0.0899917483329773\n",
      "Epoch 3487, Loss: 0.1894153282046318, Final Batch Loss: 0.09114661067724228\n",
      "Epoch 3488, Loss: 0.18993063271045685, Final Batch Loss: 0.07583566755056381\n",
      "Epoch 3489, Loss: 0.25654757022857666, Final Batch Loss: 0.12507401406764984\n",
      "Epoch 3490, Loss: 0.19168058782815933, Final Batch Loss: 0.09003389626741409\n",
      "Epoch 3491, Loss: 0.2751575633883476, Final Batch Loss: 0.08721772581338882\n",
      "Epoch 3492, Loss: 0.17255064845085144, Final Batch Loss: 0.08110233396291733\n",
      "Epoch 3493, Loss: 0.20327871292829514, Final Batch Loss: 0.11457418650388718\n",
      "Epoch 3494, Loss: 0.18746019154787064, Final Batch Loss: 0.10635697841644287\n",
      "Epoch 3495, Loss: 0.20121996104717255, Final Batch Loss: 0.1277388334274292\n",
      "Epoch 3496, Loss: 0.16855571419000626, Final Batch Loss: 0.09348875284194946\n",
      "Epoch 3497, Loss: 0.24836566299200058, Final Batch Loss: 0.1178148165345192\n",
      "Epoch 3498, Loss: 0.20280489325523376, Final Batch Loss: 0.1114472821354866\n",
      "Epoch 3499, Loss: 0.2011270970106125, Final Batch Loss: 0.11276891827583313\n",
      "Epoch 3500, Loss: 0.20288140326738358, Final Batch Loss: 0.09761455655097961\n",
      "Epoch 3501, Loss: 0.15164953470230103, Final Batch Loss: 0.07699224352836609\n",
      "Epoch 3502, Loss: 0.25524304062128067, Final Batch Loss: 0.1448826640844345\n",
      "Epoch 3503, Loss: 0.17802200466394424, Final Batch Loss: 0.09773320704698563\n",
      "Epoch 3504, Loss: 0.23239277303218842, Final Batch Loss: 0.0841352641582489\n",
      "Epoch 3505, Loss: 0.2172488197684288, Final Batch Loss: 0.08783993870019913\n",
      "Epoch 3506, Loss: 0.20477396249771118, Final Batch Loss: 0.1030694767832756\n",
      "Epoch 3507, Loss: 0.2356877401471138, Final Batch Loss: 0.12626947462558746\n",
      "Epoch 3508, Loss: 0.18040210008621216, Final Batch Loss: 0.07478273659944534\n",
      "Epoch 3509, Loss: 0.22752388566732407, Final Batch Loss: 0.1461566686630249\n",
      "Epoch 3510, Loss: 0.203621506690979, Final Batch Loss: 0.11509471386671066\n",
      "Epoch 3511, Loss: 0.2677533030509949, Final Batch Loss: 0.15586157143115997\n",
      "Epoch 3512, Loss: 0.14818577468395233, Final Batch Loss: 0.07682132720947266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3513, Loss: 0.18381118029356003, Final Batch Loss: 0.10313008725643158\n",
      "Epoch 3514, Loss: 0.24239343404769897, Final Batch Loss: 0.1393684297800064\n",
      "Epoch 3515, Loss: 0.1775430142879486, Final Batch Loss: 0.09257100522518158\n",
      "Epoch 3516, Loss: 0.18227816745638847, Final Batch Loss: 0.12515263259410858\n",
      "Epoch 3517, Loss: 0.2271857187151909, Final Batch Loss: 0.0900900736451149\n",
      "Epoch 3518, Loss: 0.18848847597837448, Final Batch Loss: 0.09170104563236237\n",
      "Epoch 3519, Loss: 0.1839398294687271, Final Batch Loss: 0.06763923168182373\n",
      "Epoch 3520, Loss: 0.18207665532827377, Final Batch Loss: 0.08623369038105011\n",
      "Epoch 3521, Loss: 0.1571810320019722, Final Batch Loss: 0.07856583595275879\n",
      "Epoch 3522, Loss: 0.1560167670249939, Final Batch Loss: 0.0682971253991127\n",
      "Epoch 3523, Loss: 0.2132868990302086, Final Batch Loss: 0.07825791090726852\n",
      "Epoch 3524, Loss: 0.1968635767698288, Final Batch Loss: 0.10666124522686005\n",
      "Epoch 3525, Loss: 0.21960634738206863, Final Batch Loss: 0.10613028705120087\n",
      "Epoch 3526, Loss: 0.15766434744000435, Final Batch Loss: 0.09926317632198334\n",
      "Epoch 3527, Loss: 0.14929693192243576, Final Batch Loss: 0.07343801110982895\n",
      "Epoch 3528, Loss: 0.14456308633089066, Final Batch Loss: 0.06917805224657059\n",
      "Epoch 3529, Loss: 0.2563013806939125, Final Batch Loss: 0.10461702197790146\n",
      "Epoch 3530, Loss: 0.28713707625865936, Final Batch Loss: 0.16288447380065918\n",
      "Epoch 3531, Loss: 0.1779501810669899, Final Batch Loss: 0.090231753885746\n",
      "Epoch 3532, Loss: 0.24074158817529678, Final Batch Loss: 0.13128705322742462\n",
      "Epoch 3533, Loss: 0.1928142085671425, Final Batch Loss: 0.11040667444467545\n",
      "Epoch 3534, Loss: 0.2669658958911896, Final Batch Loss: 0.11974932253360748\n",
      "Epoch 3535, Loss: 0.21315797418355942, Final Batch Loss: 0.13172754645347595\n",
      "Epoch 3536, Loss: 0.16326933354139328, Final Batch Loss: 0.07317043095827103\n",
      "Epoch 3537, Loss: 0.2153024971485138, Final Batch Loss: 0.07791425287723541\n",
      "Epoch 3538, Loss: 0.16268867999315262, Final Batch Loss: 0.06448320299386978\n",
      "Epoch 3539, Loss: 0.19534660130739212, Final Batch Loss: 0.08622155338525772\n",
      "Epoch 3540, Loss: 0.21767139434814453, Final Batch Loss: 0.13362708687782288\n",
      "Epoch 3541, Loss: 0.21511141955852509, Final Batch Loss: 0.1174979954957962\n",
      "Epoch 3542, Loss: 0.18042152374982834, Final Batch Loss: 0.07579393684864044\n",
      "Epoch 3543, Loss: 0.12622641026973724, Final Batch Loss: 0.07120221108198166\n",
      "Epoch 3544, Loss: 0.1387339048087597, Final Batch Loss: 0.0779799371957779\n",
      "Epoch 3545, Loss: 0.18020808696746826, Final Batch Loss: 0.10081455111503601\n",
      "Epoch 3546, Loss: 0.17560140788555145, Final Batch Loss: 0.10268673300743103\n",
      "Epoch 3547, Loss: 0.14145908504724503, Final Batch Loss: 0.09154289960861206\n",
      "Epoch 3548, Loss: 0.18473053723573685, Final Batch Loss: 0.07926236093044281\n",
      "Epoch 3549, Loss: 0.2290189117193222, Final Batch Loss: 0.14387544989585876\n",
      "Epoch 3550, Loss: 0.1500839963555336, Final Batch Loss: 0.08541817218065262\n",
      "Epoch 3551, Loss: 0.23064393550157547, Final Batch Loss: 0.12790633738040924\n",
      "Epoch 3552, Loss: 0.16365157440304756, Final Batch Loss: 0.10307627171278\n",
      "Epoch 3553, Loss: 0.16975563019514084, Final Batch Loss: 0.08208328485488892\n",
      "Epoch 3554, Loss: 0.22639267146587372, Final Batch Loss: 0.11507216095924377\n",
      "Epoch 3555, Loss: 0.16807585209608078, Final Batch Loss: 0.07407929003238678\n",
      "Epoch 3556, Loss: 0.19452860951423645, Final Batch Loss: 0.10729105025529861\n",
      "Epoch 3557, Loss: 0.2000860497355461, Final Batch Loss: 0.1134423315525055\n",
      "Epoch 3558, Loss: 0.20905335247516632, Final Batch Loss: 0.10511970520019531\n",
      "Epoch 3559, Loss: 0.14650193601846695, Final Batch Loss: 0.08196751028299332\n",
      "Epoch 3560, Loss: 0.21241240203380585, Final Batch Loss: 0.10300854593515396\n",
      "Epoch 3561, Loss: 0.3268078863620758, Final Batch Loss: 0.19348786771297455\n",
      "Epoch 3562, Loss: 0.178573876619339, Final Batch Loss: 0.09842886030673981\n",
      "Epoch 3563, Loss: 0.17700015008449554, Final Batch Loss: 0.08822781592607498\n",
      "Epoch 3564, Loss: 0.20270254462957382, Final Batch Loss: 0.1255870759487152\n",
      "Epoch 3565, Loss: 0.22344108670949936, Final Batch Loss: 0.08890994638204575\n",
      "Epoch 3566, Loss: 0.2579648047685623, Final Batch Loss: 0.11204484105110168\n",
      "Epoch 3567, Loss: 0.14630065113306046, Final Batch Loss: 0.06449120491743088\n",
      "Epoch 3568, Loss: 0.20820803940296173, Final Batch Loss: 0.09951362758874893\n",
      "Epoch 3569, Loss: 0.22724905610084534, Final Batch Loss: 0.10914434492588043\n",
      "Epoch 3570, Loss: 0.24547947943210602, Final Batch Loss: 0.16257569193840027\n",
      "Epoch 3571, Loss: 0.22247665375471115, Final Batch Loss: 0.14739251136779785\n",
      "Epoch 3572, Loss: 0.21204570680856705, Final Batch Loss: 0.1109292283654213\n",
      "Epoch 3573, Loss: 0.17772723734378815, Final Batch Loss: 0.07023905962705612\n",
      "Epoch 3574, Loss: 0.194710835814476, Final Batch Loss: 0.09727030247449875\n",
      "Epoch 3575, Loss: 0.2499515265226364, Final Batch Loss: 0.12207528948783875\n",
      "Epoch 3576, Loss: 0.2024349570274353, Final Batch Loss: 0.10037898272275925\n",
      "Epoch 3577, Loss: 0.16245412826538086, Final Batch Loss: 0.06335247308015823\n",
      "Epoch 3578, Loss: 0.23611819744110107, Final Batch Loss: 0.1073974221944809\n",
      "Epoch 3579, Loss: 0.1600201055407524, Final Batch Loss: 0.083078533411026\n",
      "Epoch 3580, Loss: 0.18487713485956192, Final Batch Loss: 0.10341477394104004\n",
      "Epoch 3581, Loss: 0.20210976898670197, Final Batch Loss: 0.10728278011083603\n",
      "Epoch 3582, Loss: 0.2028176113963127, Final Batch Loss: 0.09498228132724762\n",
      "Epoch 3583, Loss: 0.16859076917171478, Final Batch Loss: 0.05661828815937042\n",
      "Epoch 3584, Loss: 0.1961948126554489, Final Batch Loss: 0.12679670751094818\n",
      "Epoch 3585, Loss: 0.1595330387353897, Final Batch Loss: 0.05989988148212433\n",
      "Epoch 3586, Loss: 0.22004302591085434, Final Batch Loss: 0.10088781267404556\n",
      "Epoch 3587, Loss: 0.20831437408924103, Final Batch Loss: 0.12500816583633423\n",
      "Epoch 3588, Loss: 0.13665256649255753, Final Batch Loss: 0.0524979829788208\n",
      "Epoch 3589, Loss: 0.16522879153490067, Final Batch Loss: 0.06555427610874176\n",
      "Epoch 3590, Loss: 0.21041975170373917, Final Batch Loss: 0.11029965430498123\n",
      "Epoch 3591, Loss: 0.14499332010746002, Final Batch Loss: 0.06778819113969803\n",
      "Epoch 3592, Loss: 0.17683860659599304, Final Batch Loss: 0.0742749497294426\n",
      "Epoch 3593, Loss: 0.16696255654096603, Final Batch Loss: 0.08964921534061432\n",
      "Epoch 3594, Loss: 0.14203237742185593, Final Batch Loss: 0.07379034906625748\n",
      "Epoch 3595, Loss: 0.22774866968393326, Final Batch Loss: 0.12419839948415756\n",
      "Epoch 3596, Loss: 0.18592632561922073, Final Batch Loss: 0.08862694352865219\n",
      "Epoch 3597, Loss: 0.17544103413820267, Final Batch Loss: 0.09015991538763046\n",
      "Epoch 3598, Loss: 0.1987333744764328, Final Batch Loss: 0.09768079221248627\n",
      "Epoch 3599, Loss: 0.2148696929216385, Final Batch Loss: 0.11424525082111359\n",
      "Epoch 3600, Loss: 0.16872739791870117, Final Batch Loss: 0.08929295837879181\n",
      "Epoch 3601, Loss: 0.16931653022766113, Final Batch Loss: 0.1000530794262886\n",
      "Epoch 3602, Loss: 0.20878787338733673, Final Batch Loss: 0.12383188307285309\n",
      "Epoch 3603, Loss: 0.22198883444070816, Final Batch Loss: 0.12863995134830475\n",
      "Epoch 3604, Loss: 0.24244032055139542, Final Batch Loss: 0.15265102684497833\n",
      "Epoch 3605, Loss: 0.1310536116361618, Final Batch Loss: 0.06477035582065582\n",
      "Epoch 3606, Loss: 0.21364876627922058, Final Batch Loss: 0.11339583992958069\n",
      "Epoch 3607, Loss: 0.18434782326221466, Final Batch Loss: 0.07978350669145584\n",
      "Epoch 3608, Loss: 0.1542270928621292, Final Batch Loss: 0.09795425087213516\n",
      "Epoch 3609, Loss: 0.15926654636859894, Final Batch Loss: 0.08366034924983978\n",
      "Epoch 3610, Loss: 0.1765396073460579, Final Batch Loss: 0.06978359073400497\n",
      "Epoch 3611, Loss: 0.2310056909918785, Final Batch Loss: 0.11016152799129486\n",
      "Epoch 3612, Loss: 0.17654459178447723, Final Batch Loss: 0.10326257348060608\n",
      "Epoch 3613, Loss: 0.19438118487596512, Final Batch Loss: 0.11085270345211029\n",
      "Epoch 3614, Loss: 0.19744963943958282, Final Batch Loss: 0.06840530037879944\n",
      "Epoch 3615, Loss: 0.1921544075012207, Final Batch Loss: 0.09280449151992798\n",
      "Epoch 3616, Loss: 0.2037132903933525, Final Batch Loss: 0.090369313955307\n",
      "Epoch 3617, Loss: 0.1464555636048317, Final Batch Loss: 0.0815863087773323\n",
      "Epoch 3618, Loss: 0.23934461176395416, Final Batch Loss: 0.13628093898296356\n",
      "Epoch 3619, Loss: 0.18508461862802505, Final Batch Loss: 0.12317823618650436\n",
      "Epoch 3620, Loss: 0.1745057925581932, Final Batch Loss: 0.08806399255990982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3621, Loss: 0.19751283526420593, Final Batch Loss: 0.08478428423404694\n",
      "Epoch 3622, Loss: 0.2160792350769043, Final Batch Loss: 0.1331697702407837\n",
      "Epoch 3623, Loss: 0.1698780059814453, Final Batch Loss: 0.07726394385099411\n",
      "Epoch 3624, Loss: 0.19666150212287903, Final Batch Loss: 0.10298893600702286\n",
      "Epoch 3625, Loss: 0.12761228159070015, Final Batch Loss: 0.03677685931324959\n",
      "Epoch 3626, Loss: 0.2506802976131439, Final Batch Loss: 0.12537282705307007\n",
      "Epoch 3627, Loss: 0.2153736874461174, Final Batch Loss: 0.09239325672388077\n",
      "Epoch 3628, Loss: 0.16110694780945778, Final Batch Loss: 0.11429416388273239\n",
      "Epoch 3629, Loss: 0.2302710935473442, Final Batch Loss: 0.12493523210287094\n",
      "Epoch 3630, Loss: 0.18192056566476822, Final Batch Loss: 0.07072263211011887\n",
      "Epoch 3631, Loss: 0.2616523429751396, Final Batch Loss: 0.07052696496248245\n",
      "Epoch 3632, Loss: 0.24821975827217102, Final Batch Loss: 0.11307089030742645\n",
      "Epoch 3633, Loss: 0.18279818445444107, Final Batch Loss: 0.1279907375574112\n",
      "Epoch 3634, Loss: 0.18823493272066116, Final Batch Loss: 0.10730848461389542\n",
      "Epoch 3635, Loss: 0.21511586755514145, Final Batch Loss: 0.09328281879425049\n",
      "Epoch 3636, Loss: 0.2177126184105873, Final Batch Loss: 0.1103823333978653\n",
      "Epoch 3637, Loss: 0.18155154958367348, Final Batch Loss: 0.13167133927345276\n",
      "Epoch 3638, Loss: 0.21226081997156143, Final Batch Loss: 0.12533393502235413\n",
      "Epoch 3639, Loss: 0.17434966564178467, Final Batch Loss: 0.09462321549654007\n",
      "Epoch 3640, Loss: 0.21606505662202835, Final Batch Loss: 0.10789018124341965\n",
      "Epoch 3641, Loss: 0.2218749299645424, Final Batch Loss: 0.11361294984817505\n",
      "Epoch 3642, Loss: 0.15157640725374222, Final Batch Loss: 0.07080624252557755\n",
      "Epoch 3643, Loss: 0.19483911246061325, Final Batch Loss: 0.10518412292003632\n",
      "Epoch 3644, Loss: 0.21528444439172745, Final Batch Loss: 0.09496443718671799\n",
      "Epoch 3645, Loss: 0.2027415707707405, Final Batch Loss: 0.0647776648402214\n",
      "Epoch 3646, Loss: 0.23584310710430145, Final Batch Loss: 0.13047045469284058\n",
      "Epoch 3647, Loss: 0.20015830546617508, Final Batch Loss: 0.07675375044345856\n",
      "Epoch 3648, Loss: 0.22225463390350342, Final Batch Loss: 0.09699398279190063\n",
      "Epoch 3649, Loss: 0.17135457694530487, Final Batch Loss: 0.08280287683010101\n",
      "Epoch 3650, Loss: 0.4794415980577469, Final Batch Loss: 0.10414667427539825\n",
      "Epoch 3651, Loss: 0.13883334770798683, Final Batch Loss: 0.09117632359266281\n",
      "Epoch 3652, Loss: 0.20419378578662872, Final Batch Loss: 0.08630917221307755\n",
      "Epoch 3653, Loss: 0.14447861909866333, Final Batch Loss: 0.07751531898975372\n",
      "Epoch 3654, Loss: 0.212175615131855, Final Batch Loss: 0.08464648574590683\n",
      "Epoch 3655, Loss: 0.14420420676469803, Final Batch Loss: 0.08475466817617416\n",
      "Epoch 3656, Loss: 0.1878693699836731, Final Batch Loss: 0.06367535889148712\n",
      "Epoch 3657, Loss: 0.16220723092556, Final Batch Loss: 0.087653249502182\n",
      "Epoch 3658, Loss: 0.14987649768590927, Final Batch Loss: 0.08212514221668243\n",
      "Epoch 3659, Loss: 0.23470571637153625, Final Batch Loss: 0.08706106245517731\n",
      "Epoch 3660, Loss: 0.19590258598327637, Final Batch Loss: 0.06776373088359833\n",
      "Epoch 3661, Loss: 0.1887977123260498, Final Batch Loss: 0.10563408583402634\n",
      "Epoch 3662, Loss: 0.13137323409318924, Final Batch Loss: 0.057865969836711884\n",
      "Epoch 3663, Loss: 0.16090931743383408, Final Batch Loss: 0.04575321078300476\n",
      "Epoch 3664, Loss: 0.24040865153074265, Final Batch Loss: 0.11336026340723038\n",
      "Epoch 3665, Loss: 0.20746026933193207, Final Batch Loss: 0.12001457065343857\n",
      "Epoch 3666, Loss: 0.17778755724430084, Final Batch Loss: 0.10405410826206207\n",
      "Epoch 3667, Loss: 0.1956288069486618, Final Batch Loss: 0.09547271579504013\n",
      "Epoch 3668, Loss: 0.19214028120040894, Final Batch Loss: 0.08548258244991302\n",
      "Epoch 3669, Loss: 0.12855900824069977, Final Batch Loss: 0.05801260471343994\n",
      "Epoch 3670, Loss: 0.15117597579956055, Final Batch Loss: 0.06983649730682373\n",
      "Epoch 3671, Loss: 0.19839196652173996, Final Batch Loss: 0.1060498058795929\n",
      "Epoch 3672, Loss: 0.16897747665643692, Final Batch Loss: 0.08308494836091995\n",
      "Epoch 3673, Loss: 0.19278104603290558, Final Batch Loss: 0.10994374006986618\n",
      "Epoch 3674, Loss: 0.16682633012533188, Final Batch Loss: 0.090837761759758\n",
      "Epoch 3675, Loss: 0.22856704145669937, Final Batch Loss: 0.12012927234172821\n",
      "Epoch 3676, Loss: 0.12631190940737724, Final Batch Loss: 0.06624478101730347\n",
      "Epoch 3677, Loss: 0.1690637245774269, Final Batch Loss: 0.10217802971601486\n",
      "Epoch 3678, Loss: 0.28008225560188293, Final Batch Loss: 0.10005609691143036\n",
      "Epoch 3679, Loss: 0.20436101406812668, Final Batch Loss: 0.10750127583742142\n",
      "Epoch 3680, Loss: 0.189163938164711, Final Batch Loss: 0.10518380254507065\n",
      "Epoch 3681, Loss: 0.21348240226507187, Final Batch Loss: 0.12776389718055725\n",
      "Epoch 3682, Loss: 0.17466969043016434, Final Batch Loss: 0.08417201042175293\n",
      "Epoch 3683, Loss: 0.16423413157463074, Final Batch Loss: 0.08314061164855957\n",
      "Epoch 3684, Loss: 0.1874527484178543, Final Batch Loss: 0.11123975366353989\n",
      "Epoch 3685, Loss: 0.18121402710676193, Final Batch Loss: 0.0901029035449028\n",
      "Epoch 3686, Loss: 0.20935675501823425, Final Batch Loss: 0.08972906321287155\n",
      "Epoch 3687, Loss: 0.20368532836437225, Final Batch Loss: 0.1264255940914154\n",
      "Epoch 3688, Loss: 0.26962055265903473, Final Batch Loss: 0.15843619406223297\n",
      "Epoch 3689, Loss: 0.1635061502456665, Final Batch Loss: 0.06328611075878143\n",
      "Epoch 3690, Loss: 0.20673713833093643, Final Batch Loss: 0.1149202510714531\n",
      "Epoch 3691, Loss: 0.2555982135236263, Final Batch Loss: 0.19465257227420807\n",
      "Epoch 3692, Loss: 0.21257787942886353, Final Batch Loss: 0.08689621090888977\n",
      "Epoch 3693, Loss: 0.1951223462820053, Final Batch Loss: 0.10699792206287384\n",
      "Epoch 3694, Loss: 0.22926164418458939, Final Batch Loss: 0.10815811902284622\n",
      "Epoch 3695, Loss: 0.20877676457166672, Final Batch Loss: 0.1021140068769455\n",
      "Epoch 3696, Loss: 0.17673557996749878, Final Batch Loss: 0.09036044776439667\n",
      "Epoch 3697, Loss: 0.14526024460792542, Final Batch Loss: 0.06251022219657898\n",
      "Epoch 3698, Loss: 0.19138435274362564, Final Batch Loss: 0.0914636179804802\n",
      "Epoch 3699, Loss: 0.2064589112997055, Final Batch Loss: 0.10779067128896713\n",
      "Epoch 3700, Loss: 0.1775149703025818, Final Batch Loss: 0.07287508994340897\n",
      "Epoch 3701, Loss: 0.16863089054822922, Final Batch Loss: 0.05160544067621231\n",
      "Epoch 3702, Loss: 0.1849694848060608, Final Batch Loss: 0.08909161388874054\n",
      "Epoch 3703, Loss: 0.15718837827444077, Final Batch Loss: 0.07845045626163483\n",
      "Epoch 3704, Loss: 0.1475014165043831, Final Batch Loss: 0.0719330683350563\n",
      "Epoch 3705, Loss: 0.1878514587879181, Final Batch Loss: 0.10513138771057129\n",
      "Epoch 3706, Loss: 0.21720827370882034, Final Batch Loss: 0.10227431356906891\n",
      "Epoch 3707, Loss: 0.24473721534013748, Final Batch Loss: 0.13361278176307678\n",
      "Epoch 3708, Loss: 0.189342699944973, Final Batch Loss: 0.11208094656467438\n",
      "Epoch 3709, Loss: 0.1478976532816887, Final Batch Loss: 0.07703981548547745\n",
      "Epoch 3710, Loss: 0.19466674327850342, Final Batch Loss: 0.06114327907562256\n",
      "Epoch 3711, Loss: 0.2202168107032776, Final Batch Loss: 0.11474329233169556\n",
      "Epoch 3712, Loss: 0.19043317064642906, Final Batch Loss: 0.13865338265895844\n",
      "Epoch 3713, Loss: 0.16179588809609413, Final Batch Loss: 0.060802560299634933\n",
      "Epoch 3714, Loss: 0.1575215384364128, Final Batch Loss: 0.08440588414669037\n",
      "Epoch 3715, Loss: 0.15124638378620148, Final Batch Loss: 0.07742404192686081\n",
      "Epoch 3716, Loss: 0.11054949089884758, Final Batch Loss: 0.04572802409529686\n",
      "Epoch 3717, Loss: 0.24955877661705017, Final Batch Loss: 0.12025564908981323\n",
      "Epoch 3718, Loss: 0.14328139275312424, Final Batch Loss: 0.07196585834026337\n",
      "Epoch 3719, Loss: 0.1705702543258667, Final Batch Loss: 0.07352777570486069\n",
      "Epoch 3720, Loss: 0.18530841171741486, Final Batch Loss: 0.08226235210895538\n",
      "Epoch 3721, Loss: 0.15186947584152222, Final Batch Loss: 0.08606374263763428\n",
      "Epoch 3722, Loss: 0.17230098322033882, Final Batch Loss: 0.11267212778329849\n",
      "Epoch 3723, Loss: 0.1720612533390522, Final Batch Loss: 0.11129995435476303\n",
      "Epoch 3724, Loss: 0.20418867468833923, Final Batch Loss: 0.09992604702711105\n",
      "Epoch 3725, Loss: 0.18310454487800598, Final Batch Loss: 0.11485394090414047\n",
      "Epoch 3726, Loss: 0.18197688460350037, Final Batch Loss: 0.0932137668132782\n",
      "Epoch 3727, Loss: 0.152514036744833, Final Batch Loss: 0.09120693057775497\n",
      "Epoch 3728, Loss: 0.19197000563144684, Final Batch Loss: 0.09905875474214554\n",
      "Epoch 3729, Loss: 0.16311287134885788, Final Batch Loss: 0.09176251292228699\n",
      "Epoch 3730, Loss: 0.20323093235492706, Final Batch Loss: 0.09531497955322266\n",
      "Epoch 3731, Loss: 0.16191362589597702, Final Batch Loss: 0.07589701563119888\n",
      "Epoch 3732, Loss: 0.19827258586883545, Final Batch Loss: 0.07195164263248444\n",
      "Epoch 3733, Loss: 0.22950167208909988, Final Batch Loss: 0.10756547749042511\n",
      "Epoch 3734, Loss: 0.21503615379333496, Final Batch Loss: 0.1202387735247612\n",
      "Epoch 3735, Loss: 0.18406717479228973, Final Batch Loss: 0.07249080389738083\n",
      "Epoch 3736, Loss: 0.2536045238375664, Final Batch Loss: 0.14581623673439026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3737, Loss: 0.1794416531920433, Final Batch Loss: 0.09733636677265167\n",
      "Epoch 3738, Loss: 0.22539451718330383, Final Batch Loss: 0.10481936484575272\n",
      "Epoch 3739, Loss: 0.1393393613398075, Final Batch Loss: 0.09457678347826004\n",
      "Epoch 3740, Loss: 0.13470788672566414, Final Batch Loss: 0.07291103899478912\n",
      "Epoch 3741, Loss: 0.18956201523542404, Final Batch Loss: 0.08184516429901123\n",
      "Epoch 3742, Loss: 0.24648207426071167, Final Batch Loss: 0.11212660372257233\n",
      "Epoch 3743, Loss: 0.1474243700504303, Final Batch Loss: 0.07953833788633347\n",
      "Epoch 3744, Loss: 0.2025369629263878, Final Batch Loss: 0.09835866093635559\n",
      "Epoch 3745, Loss: 0.22950458526611328, Final Batch Loss: 0.1198636069893837\n",
      "Epoch 3746, Loss: 0.15204058587551117, Final Batch Loss: 0.07964952290058136\n",
      "Epoch 3747, Loss: 0.12995488196611404, Final Batch Loss: 0.06913531571626663\n",
      "Epoch 3748, Loss: 0.16002459824085236, Final Batch Loss: 0.07224656641483307\n",
      "Epoch 3749, Loss: 0.1754406988620758, Final Batch Loss: 0.11149609088897705\n",
      "Epoch 3750, Loss: 0.18674860522150993, Final Batch Loss: 0.1295018494129181\n",
      "Epoch 3751, Loss: 0.16267651319503784, Final Batch Loss: 0.10447143763303757\n",
      "Epoch 3752, Loss: 0.15968036651611328, Final Batch Loss: 0.06836194545030594\n",
      "Epoch 3753, Loss: 0.17483383417129517, Final Batch Loss: 0.06552712619304657\n",
      "Epoch 3754, Loss: 0.17795607447624207, Final Batch Loss: 0.07409662008285522\n",
      "Epoch 3755, Loss: 0.1648126095533371, Final Batch Loss: 0.09133128076791763\n",
      "Epoch 3756, Loss: 0.11147170141339302, Final Batch Loss: 0.05197770521044731\n",
      "Epoch 3757, Loss: 0.2164713367819786, Final Batch Loss: 0.11773031949996948\n",
      "Epoch 3758, Loss: 0.16701160371303558, Final Batch Loss: 0.08630308508872986\n",
      "Epoch 3759, Loss: 0.15054479986429214, Final Batch Loss: 0.07437789440155029\n",
      "Epoch 3760, Loss: 0.15672367811203003, Final Batch Loss: 0.08957405388355255\n",
      "Epoch 3761, Loss: 0.17903704941272736, Final Batch Loss: 0.10194533318281174\n",
      "Epoch 3762, Loss: 0.168668232858181, Final Batch Loss: 0.10625921189785004\n",
      "Epoch 3763, Loss: 0.16244225203990936, Final Batch Loss: 0.09146612882614136\n",
      "Epoch 3764, Loss: 0.13615625351667404, Final Batch Loss: 0.06778068095445633\n",
      "Epoch 3765, Loss: 0.1723884791135788, Final Batch Loss: 0.06905408948659897\n",
      "Epoch 3766, Loss: 0.14895664900541306, Final Batch Loss: 0.0834539383649826\n",
      "Epoch 3767, Loss: 0.1595778688788414, Final Batch Loss: 0.07000640779733658\n",
      "Epoch 3768, Loss: 0.22177287936210632, Final Batch Loss: 0.10895567387342453\n",
      "Epoch 3769, Loss: 0.23125432431697845, Final Batch Loss: 0.10596531629562378\n",
      "Epoch 3770, Loss: 0.11908617615699768, Final Batch Loss: 0.053140997886657715\n",
      "Epoch 3771, Loss: 0.1653587445616722, Final Batch Loss: 0.07317543029785156\n",
      "Epoch 3772, Loss: 0.11858401075005531, Final Batch Loss: 0.054493051022291183\n",
      "Epoch 3773, Loss: 0.17296375334262848, Final Batch Loss: 0.06362644582986832\n",
      "Epoch 3774, Loss: 0.1734636425971985, Final Batch Loss: 0.08635842055082321\n",
      "Epoch 3775, Loss: 0.16258715093135834, Final Batch Loss: 0.06986377388238907\n",
      "Epoch 3776, Loss: 0.19888468831777573, Final Batch Loss: 0.10335949063301086\n",
      "Epoch 3777, Loss: 0.2521988973021507, Final Batch Loss: 0.1297490894794464\n",
      "Epoch 3778, Loss: 0.1766979768872261, Final Batch Loss: 0.0644909143447876\n",
      "Epoch 3779, Loss: 0.16669336706399918, Final Batch Loss: 0.08760914951562881\n",
      "Epoch 3780, Loss: 0.17781663686037064, Final Batch Loss: 0.09912620484828949\n",
      "Epoch 3781, Loss: 0.1535307914018631, Final Batch Loss: 0.07107771933078766\n",
      "Epoch 3782, Loss: 0.18399525433778763, Final Batch Loss: 0.09845708310604095\n",
      "Epoch 3783, Loss: 0.15125435590744019, Final Batch Loss: 0.07788322865962982\n",
      "Epoch 3784, Loss: 0.18806719034910202, Final Batch Loss: 0.10493823140859604\n",
      "Epoch 3785, Loss: 0.25560616701841354, Final Batch Loss: 0.11493467539548874\n",
      "Epoch 3786, Loss: 0.1819823905825615, Final Batch Loss: 0.10958171635866165\n",
      "Epoch 3787, Loss: 0.15162327513098717, Final Batch Loss: 0.0913417637348175\n",
      "Epoch 3788, Loss: 0.17564382776618004, Final Batch Loss: 0.05651135370135307\n",
      "Epoch 3789, Loss: 0.2196611873805523, Final Batch Loss: 0.15854008495807648\n",
      "Epoch 3790, Loss: 0.2370467558503151, Final Batch Loss: 0.10890992730855942\n",
      "Epoch 3791, Loss: 0.17628885805606842, Final Batch Loss: 0.09920492023229599\n",
      "Epoch 3792, Loss: 0.1712951883673668, Final Batch Loss: 0.08486539125442505\n",
      "Epoch 3793, Loss: 0.13456154614686966, Final Batch Loss: 0.06517051160335541\n",
      "Epoch 3794, Loss: 0.1936958059668541, Final Batch Loss: 0.12364710122346878\n",
      "Epoch 3795, Loss: 0.18538283556699753, Final Batch Loss: 0.08230330795049667\n",
      "Epoch 3796, Loss: 0.20143793523311615, Final Batch Loss: 0.1105460524559021\n",
      "Epoch 3797, Loss: 0.16745999082922935, Final Batch Loss: 0.060610581189394\n",
      "Epoch 3798, Loss: 0.1880842000246048, Final Batch Loss: 0.08282368630170822\n",
      "Epoch 3799, Loss: 0.23560504615306854, Final Batch Loss: 0.11460771411657333\n",
      "Epoch 3800, Loss: 0.2016659453511238, Final Batch Loss: 0.13230359554290771\n",
      "Epoch 3801, Loss: 0.13662062212824821, Final Batch Loss: 0.04687328264117241\n",
      "Epoch 3802, Loss: 0.1725192815065384, Final Batch Loss: 0.10089126229286194\n",
      "Epoch 3803, Loss: 0.2651187777519226, Final Batch Loss: 0.13938289880752563\n",
      "Epoch 3804, Loss: 0.163958340883255, Final Batch Loss: 0.09980349242687225\n",
      "Epoch 3805, Loss: 0.17530040442943573, Final Batch Loss: 0.09734585881233215\n",
      "Epoch 3806, Loss: 0.17661436647176743, Final Batch Loss: 0.09289328753948212\n",
      "Epoch 3807, Loss: 0.23054178804159164, Final Batch Loss: 0.12074656039476395\n",
      "Epoch 3808, Loss: 0.1794714331626892, Final Batch Loss: 0.08805716037750244\n",
      "Epoch 3809, Loss: 0.17379271239042282, Final Batch Loss: 0.07830925285816193\n",
      "Epoch 3810, Loss: 0.15301212668418884, Final Batch Loss: 0.07448936998844147\n",
      "Epoch 3811, Loss: 0.20698033273220062, Final Batch Loss: 0.10402282327413559\n",
      "Epoch 3812, Loss: 0.18537035584449768, Final Batch Loss: 0.09541745483875275\n",
      "Epoch 3813, Loss: 0.22423354536294937, Final Batch Loss: 0.12383868545293808\n",
      "Epoch 3814, Loss: 0.23046664148569107, Final Batch Loss: 0.11307939141988754\n",
      "Epoch 3815, Loss: 0.16136683151125908, Final Batch Loss: 0.04961499944329262\n",
      "Epoch 3816, Loss: 0.1546860858798027, Final Batch Loss: 0.06694474816322327\n",
      "Epoch 3817, Loss: 0.2572091817855835, Final Batch Loss: 0.16940999031066895\n",
      "Epoch 3818, Loss: 0.1417602226138115, Final Batch Loss: 0.07155104726552963\n",
      "Epoch 3819, Loss: 0.20119545608758926, Final Batch Loss: 0.10678065568208694\n",
      "Epoch 3820, Loss: 0.1975935474038124, Final Batch Loss: 0.1234242171049118\n",
      "Epoch 3821, Loss: 0.1631404384970665, Final Batch Loss: 0.0712985023856163\n",
      "Epoch 3822, Loss: 0.21941639482975006, Final Batch Loss: 0.08517935872077942\n",
      "Epoch 3823, Loss: 0.1920292228460312, Final Batch Loss: 0.10301008820533752\n",
      "Epoch 3824, Loss: 0.22252535074949265, Final Batch Loss: 0.0786469504237175\n",
      "Epoch 3825, Loss: 0.15748009830713272, Final Batch Loss: 0.09213124215602875\n",
      "Epoch 3826, Loss: 0.15802433714270592, Final Batch Loss: 0.04890679940581322\n",
      "Epoch 3827, Loss: 0.15775851905345917, Final Batch Loss: 0.07452955096960068\n",
      "Epoch 3828, Loss: 0.18428028374910355, Final Batch Loss: 0.11536018550395966\n",
      "Epoch 3829, Loss: 0.15693914890289307, Final Batch Loss: 0.08554572612047195\n",
      "Epoch 3830, Loss: 0.1654111072421074, Final Batch Loss: 0.06484587490558624\n",
      "Epoch 3831, Loss: 0.1399502120912075, Final Batch Loss: 0.08094677329063416\n",
      "Epoch 3832, Loss: 0.16738759726285934, Final Batch Loss: 0.08822225779294968\n",
      "Epoch 3833, Loss: 0.19693993777036667, Final Batch Loss: 0.101434625685215\n",
      "Epoch 3834, Loss: 0.2196763977408409, Final Batch Loss: 0.11924131214618683\n",
      "Epoch 3835, Loss: 0.2000451236963272, Final Batch Loss: 0.08328865468502045\n",
      "Epoch 3836, Loss: 0.22335943579673767, Final Batch Loss: 0.08806227147579193\n",
      "Epoch 3837, Loss: 0.1527295522391796, Final Batch Loss: 0.047729913145303726\n",
      "Epoch 3838, Loss: 0.1596525013446808, Final Batch Loss: 0.06757418811321259\n",
      "Epoch 3839, Loss: 0.13484645634889603, Final Batch Loss: 0.07636886835098267\n",
      "Epoch 3840, Loss: 0.158110611140728, Final Batch Loss: 0.09772180020809174\n",
      "Epoch 3841, Loss: 0.15567438304424286, Final Batch Loss: 0.0850195437669754\n",
      "Epoch 3842, Loss: 0.15257307887077332, Final Batch Loss: 0.09241370856761932\n",
      "Epoch 3843, Loss: 0.12927623093128204, Final Batch Loss: 0.06649818271398544\n",
      "Epoch 3844, Loss: 0.16982990503311157, Final Batch Loss: 0.09646430611610413\n",
      "Epoch 3845, Loss: 0.15556561946868896, Final Batch Loss: 0.06919935345649719\n",
      "Epoch 3846, Loss: 0.16140345484018326, Final Batch Loss: 0.08549661189317703\n",
      "Epoch 3847, Loss: 0.1509290114045143, Final Batch Loss: 0.07222927361726761\n",
      "Epoch 3848, Loss: 0.11937525495886803, Final Batch Loss: 0.06223388761281967\n",
      "Epoch 3849, Loss: 0.22159021347761154, Final Batch Loss: 0.09750955551862717\n",
      "Epoch 3850, Loss: 0.11059242486953735, Final Batch Loss: 0.048959728330373764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3851, Loss: 0.18165472894906998, Final Batch Loss: 0.08480338007211685\n",
      "Epoch 3852, Loss: 0.2882225923240185, Final Batch Loss: 0.05649922415614128\n",
      "Epoch 3853, Loss: 0.2153834104537964, Final Batch Loss: 0.09228196740150452\n",
      "Epoch 3854, Loss: 0.1389888897538185, Final Batch Loss: 0.060050152242183685\n",
      "Epoch 3855, Loss: 0.2092892825603485, Final Batch Loss: 0.08945044130086899\n",
      "Epoch 3856, Loss: 0.15440690144896507, Final Batch Loss: 0.05966304615139961\n",
      "Epoch 3857, Loss: 0.1634162738919258, Final Batch Loss: 0.08093485981225967\n",
      "Epoch 3858, Loss: 0.20690233260393143, Final Batch Loss: 0.11017211526632309\n",
      "Epoch 3859, Loss: 0.12944848090410233, Final Batch Loss: 0.062403663992881775\n",
      "Epoch 3860, Loss: 0.11503838747739792, Final Batch Loss: 0.06804997473955154\n",
      "Epoch 3861, Loss: 0.11343676969408989, Final Batch Loss: 0.05861833691596985\n",
      "Epoch 3862, Loss: 0.18543999642133713, Final Batch Loss: 0.08856405317783356\n",
      "Epoch 3863, Loss: 0.16382211819291115, Final Batch Loss: 0.10361261665821075\n",
      "Epoch 3864, Loss: 0.20754101127386093, Final Batch Loss: 0.10192493349313736\n",
      "Epoch 3865, Loss: 0.1635439395904541, Final Batch Loss: 0.06777318567037582\n",
      "Epoch 3866, Loss: 0.14642292261123657, Final Batch Loss: 0.08507348597049713\n",
      "Epoch 3867, Loss: 0.17635204643011093, Final Batch Loss: 0.07160898298025131\n",
      "Epoch 3868, Loss: 0.1367594674229622, Final Batch Loss: 0.07563904672861099\n",
      "Epoch 3869, Loss: 0.1579439714550972, Final Batch Loss: 0.05141118913888931\n",
      "Epoch 3870, Loss: 0.09576043114066124, Final Batch Loss: 0.04920956864953041\n",
      "Epoch 3871, Loss: 0.1713142991065979, Final Batch Loss: 0.09932967275381088\n",
      "Epoch 3872, Loss: 0.12575247883796692, Final Batch Loss: 0.07094522565603256\n",
      "Epoch 3873, Loss: 0.1414414830505848, Final Batch Loss: 0.05885186418890953\n",
      "Epoch 3874, Loss: 0.19255363196134567, Final Batch Loss: 0.11105763167142868\n",
      "Epoch 3875, Loss: 0.13146071881055832, Final Batch Loss: 0.05048469454050064\n",
      "Epoch 3876, Loss: 0.13624336197972298, Final Batch Loss: 0.043082427233457565\n",
      "Epoch 3877, Loss: 0.1243845634162426, Final Batch Loss: 0.05256335809826851\n",
      "Epoch 3878, Loss: 0.13887730240821838, Final Batch Loss: 0.07305558770895004\n",
      "Epoch 3879, Loss: 0.1614702045917511, Final Batch Loss: 0.07459495961666107\n",
      "Epoch 3880, Loss: 0.17790203541517258, Final Batch Loss: 0.09910598397254944\n",
      "Epoch 3881, Loss: 0.12686199322342873, Final Batch Loss: 0.05923067405819893\n",
      "Epoch 3882, Loss: 0.14039238542318344, Final Batch Loss: 0.06484795361757278\n",
      "Epoch 3883, Loss: 0.17743054777383804, Final Batch Loss: 0.10708918422460556\n",
      "Epoch 3884, Loss: 0.18011552095413208, Final Batch Loss: 0.09707948565483093\n",
      "Epoch 3885, Loss: 0.14276131242513657, Final Batch Loss: 0.08060053735971451\n",
      "Epoch 3886, Loss: 0.17050699144601822, Final Batch Loss: 0.07428152859210968\n",
      "Epoch 3887, Loss: 0.21614883095026016, Final Batch Loss: 0.1182652935385704\n",
      "Epoch 3888, Loss: 0.18647801131010056, Final Batch Loss: 0.08621535450220108\n",
      "Epoch 3889, Loss: 0.21729665249586105, Final Batch Loss: 0.12288516014814377\n",
      "Epoch 3890, Loss: 0.1921115294098854, Final Batch Loss: 0.10403529554605484\n",
      "Epoch 3891, Loss: 0.12934453785419464, Final Batch Loss: 0.06526979058980942\n",
      "Epoch 3892, Loss: 0.18148952722549438, Final Batch Loss: 0.09096056967973709\n",
      "Epoch 3893, Loss: 0.21175622195005417, Final Batch Loss: 0.10475850850343704\n",
      "Epoch 3894, Loss: 0.20380913466215134, Final Batch Loss: 0.08895963430404663\n",
      "Epoch 3895, Loss: 0.20530780404806137, Final Batch Loss: 0.11604586243629456\n",
      "Epoch 3896, Loss: 0.185954712331295, Final Batch Loss: 0.0957384705543518\n",
      "Epoch 3897, Loss: 0.156462624669075, Final Batch Loss: 0.07796834409236908\n",
      "Epoch 3898, Loss: 0.25462016463279724, Final Batch Loss: 0.10793985426425934\n",
      "Epoch 3899, Loss: 0.15600460022687912, Final Batch Loss: 0.08011452853679657\n",
      "Epoch 3900, Loss: 0.15705277025699615, Final Batch Loss: 0.06317249685525894\n",
      "Epoch 3901, Loss: 0.15612385421991348, Final Batch Loss: 0.08309409022331238\n",
      "Epoch 3902, Loss: 0.12363146618008614, Final Batch Loss: 0.05267314985394478\n",
      "Epoch 3903, Loss: 0.20458054542541504, Final Batch Loss: 0.12582416832447052\n",
      "Epoch 3904, Loss: 0.1563931331038475, Final Batch Loss: 0.06622033566236496\n",
      "Epoch 3905, Loss: 0.1206190474331379, Final Batch Loss: 0.042284514755010605\n",
      "Epoch 3906, Loss: 0.162053681910038, Final Batch Loss: 0.0707181766629219\n",
      "Epoch 3907, Loss: 0.15153318271040916, Final Batch Loss: 0.0515001155436039\n",
      "Epoch 3908, Loss: 0.19519779831171036, Final Batch Loss: 0.10013347119092941\n",
      "Epoch 3909, Loss: 0.165285162627697, Final Batch Loss: 0.08436793833971024\n",
      "Epoch 3910, Loss: 0.2450239658355713, Final Batch Loss: 0.1607460081577301\n",
      "Epoch 3911, Loss: 0.18447088077664375, Final Batch Loss: 0.127004936337471\n",
      "Epoch 3912, Loss: 0.13108015805482864, Final Batch Loss: 0.07129030674695969\n",
      "Epoch 3913, Loss: 0.17848969995975494, Final Batch Loss: 0.10348711907863617\n",
      "Epoch 3914, Loss: 0.17610222846269608, Final Batch Loss: 0.07603050768375397\n",
      "Epoch 3915, Loss: 0.15763232856988907, Final Batch Loss: 0.0723930150270462\n",
      "Epoch 3916, Loss: 0.22268210351467133, Final Batch Loss: 0.15038743615150452\n",
      "Epoch 3917, Loss: 0.14017487689852715, Final Batch Loss: 0.04928943142294884\n",
      "Epoch 3918, Loss: 0.15734610706567764, Final Batch Loss: 0.08803360164165497\n",
      "Epoch 3919, Loss: 0.093862634152174, Final Batch Loss: 0.038922518491744995\n",
      "Epoch 3920, Loss: 0.13957368209958076, Final Batch Loss: 0.05602790042757988\n",
      "Epoch 3921, Loss: 0.1744513139128685, Final Batch Loss: 0.0869690328836441\n",
      "Epoch 3922, Loss: 0.11046602576971054, Final Batch Loss: 0.05551249906420708\n",
      "Epoch 3923, Loss: 0.20743770897388458, Final Batch Loss: 0.11248329281806946\n",
      "Epoch 3924, Loss: 0.14226575940847397, Final Batch Loss: 0.07479188591241837\n",
      "Epoch 3925, Loss: 0.1779005527496338, Final Batch Loss: 0.09603944420814514\n",
      "Epoch 3926, Loss: 0.15902847051620483, Final Batch Loss: 0.07803244143724442\n",
      "Epoch 3927, Loss: 0.13687728345394135, Final Batch Loss: 0.0622958168387413\n",
      "Epoch 3928, Loss: 0.1305054984986782, Final Batch Loss: 0.04715492203831673\n",
      "Epoch 3929, Loss: 0.14544422179460526, Final Batch Loss: 0.08188090473413467\n",
      "Epoch 3930, Loss: 0.18451399356126785, Final Batch Loss: 0.09352463483810425\n",
      "Epoch 3931, Loss: 0.1489531397819519, Final Batch Loss: 0.07653601467609406\n",
      "Epoch 3932, Loss: 0.18019330501556396, Final Batch Loss: 0.06886294484138489\n",
      "Epoch 3933, Loss: 0.17312294989824295, Final Batch Loss: 0.0915462002158165\n",
      "Epoch 3934, Loss: 0.147867850959301, Final Batch Loss: 0.07722099870443344\n",
      "Epoch 3935, Loss: 0.12220161780714989, Final Batch Loss: 0.04589005187153816\n",
      "Epoch 3936, Loss: 0.17227905243635178, Final Batch Loss: 0.10676463693380356\n",
      "Epoch 3937, Loss: 0.15352005511522293, Final Batch Loss: 0.07902272790670395\n",
      "Epoch 3938, Loss: 0.1796126887202263, Final Batch Loss: 0.09940298646688461\n",
      "Epoch 3939, Loss: 0.1867847591638565, Final Batch Loss: 0.07728306204080582\n",
      "Epoch 3940, Loss: 0.28255657106637955, Final Batch Loss: 0.08935653418302536\n",
      "Epoch 3941, Loss: 0.1454126462340355, Final Batch Loss: 0.058256104588508606\n",
      "Epoch 3942, Loss: 0.15146122127771378, Final Batch Loss: 0.07139410078525543\n",
      "Epoch 3943, Loss: 0.1390712894499302, Final Batch Loss: 0.04817705973982811\n",
      "Epoch 3944, Loss: 0.19571233540773392, Final Batch Loss: 0.09290838986635208\n",
      "Epoch 3945, Loss: 0.1732877641916275, Final Batch Loss: 0.08558080345392227\n",
      "Epoch 3946, Loss: 0.12269008159637451, Final Batch Loss: 0.07542029023170471\n",
      "Epoch 3947, Loss: 0.14752256870269775, Final Batch Loss: 0.0644148513674736\n",
      "Epoch 3948, Loss: 0.17674370110034943, Final Batch Loss: 0.08778302371501923\n",
      "Epoch 3949, Loss: 0.13387193158268929, Final Batch Loss: 0.07186707109212875\n",
      "Epoch 3950, Loss: 0.17401086539030075, Final Batch Loss: 0.08308939635753632\n",
      "Epoch 3951, Loss: 0.15460791438817978, Final Batch Loss: 0.0820932611823082\n",
      "Epoch 3952, Loss: 0.22315368801355362, Final Batch Loss: 0.06926435977220535\n",
      "Epoch 3953, Loss: 0.1079115942120552, Final Batch Loss: 0.069337397813797\n",
      "Epoch 3954, Loss: 0.1451202817261219, Final Batch Loss: 0.061701271682977676\n",
      "Epoch 3955, Loss: 0.14272314310073853, Final Batch Loss: 0.06423608958721161\n",
      "Epoch 3956, Loss: 0.19581183046102524, Final Batch Loss: 0.08262462168931961\n",
      "Epoch 3957, Loss: 0.1594805121421814, Final Batch Loss: 0.08654869347810745\n",
      "Epoch 3958, Loss: 0.09659568220376968, Final Batch Loss: 0.04731561616063118\n",
      "Epoch 3959, Loss: 0.18217942118644714, Final Batch Loss: 0.08071307837963104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3960, Loss: 0.10830501094460487, Final Batch Loss: 0.03978921100497246\n",
      "Epoch 3961, Loss: 0.14434702321887016, Final Batch Loss: 0.0495586059987545\n",
      "Epoch 3962, Loss: 0.30131152272224426, Final Batch Loss: 0.13304883241653442\n",
      "Epoch 3963, Loss: 0.14182263985276222, Final Batch Loss: 0.06032021716237068\n",
      "Epoch 3964, Loss: 0.1373736672103405, Final Batch Loss: 0.0591365285217762\n",
      "Epoch 3965, Loss: 0.14476355910301208, Final Batch Loss: 0.08307220786809921\n",
      "Epoch 3966, Loss: 0.1306879185140133, Final Batch Loss: 0.07451508194208145\n",
      "Epoch 3967, Loss: 0.1391507238149643, Final Batch Loss: 0.07670245319604874\n",
      "Epoch 3968, Loss: 0.13637364655733109, Final Batch Loss: 0.07591907680034637\n",
      "Epoch 3969, Loss: 0.18704157322645187, Final Batch Loss: 0.11005229502916336\n",
      "Epoch 3970, Loss: 0.12112192437052727, Final Batch Loss: 0.05649825558066368\n",
      "Epoch 3971, Loss: 0.1945372149348259, Final Batch Loss: 0.10132855176925659\n",
      "Epoch 3972, Loss: 0.14994163066148758, Final Batch Loss: 0.0733112320303917\n",
      "Epoch 3973, Loss: 0.11771770194172859, Final Batch Loss: 0.05593966692686081\n",
      "Epoch 3974, Loss: 0.20383569598197937, Final Batch Loss: 0.13580913841724396\n",
      "Epoch 3975, Loss: 0.20679933577775955, Final Batch Loss: 0.08277329802513123\n",
      "Epoch 3976, Loss: 0.1391649767756462, Final Batch Loss: 0.07281747460365295\n",
      "Epoch 3977, Loss: 0.1476241797208786, Final Batch Loss: 0.09044992923736572\n",
      "Epoch 3978, Loss: 0.14188798889517784, Final Batch Loss: 0.10152005404233932\n",
      "Epoch 3979, Loss: 0.16758856177330017, Final Batch Loss: 0.06289032101631165\n",
      "Epoch 3980, Loss: 0.14941072091460228, Final Batch Loss: 0.058117251843214035\n",
      "Epoch 3981, Loss: 0.14064240083098412, Final Batch Loss: 0.055846940726041794\n",
      "Epoch 3982, Loss: 0.16158746182918549, Final Batch Loss: 0.08197157084941864\n",
      "Epoch 3983, Loss: 0.18451067805290222, Final Batch Loss: 0.08030830323696136\n",
      "Epoch 3984, Loss: 0.1146232858300209, Final Batch Loss: 0.05338360369205475\n",
      "Epoch 3985, Loss: 0.11152646690607071, Final Batch Loss: 0.06091197580099106\n",
      "Epoch 3986, Loss: 0.15423864871263504, Final Batch Loss: 0.06275151669979095\n",
      "Epoch 3987, Loss: 0.11413821205496788, Final Batch Loss: 0.05809096246957779\n",
      "Epoch 3988, Loss: 0.18210579454898834, Final Batch Loss: 0.0822591558098793\n",
      "Epoch 3989, Loss: 0.17897959798574448, Final Batch Loss: 0.09243911504745483\n",
      "Epoch 3990, Loss: 0.10800152644515038, Final Batch Loss: 0.04377846047282219\n",
      "Epoch 3991, Loss: 0.17195073887705803, Final Batch Loss: 0.12350897490978241\n",
      "Epoch 3992, Loss: 0.1681543104350567, Final Batch Loss: 0.10735595971345901\n",
      "Epoch 3993, Loss: 0.1601896658539772, Final Batch Loss: 0.06346398591995239\n",
      "Epoch 3994, Loss: 0.1882845237851143, Final Batch Loss: 0.089389868080616\n",
      "Epoch 3995, Loss: 0.14225655049085617, Final Batch Loss: 0.07343924790620804\n",
      "Epoch 3996, Loss: 0.18434815853834152, Final Batch Loss: 0.11198718845844269\n",
      "Epoch 3997, Loss: 0.13244247436523438, Final Batch Loss: 0.0636456161737442\n",
      "Epoch 3998, Loss: 0.27254266291856766, Final Batch Loss: 0.11648119240999222\n",
      "Epoch 3999, Loss: 0.18952379375696182, Final Batch Loss: 0.09335903078317642\n",
      "Epoch 4000, Loss: 0.20397446304559708, Final Batch Loss: 0.08997993916273117\n",
      "Epoch 4001, Loss: 0.20857905596494675, Final Batch Loss: 0.11554623395204544\n",
      "Epoch 4002, Loss: 0.14699135720729828, Final Batch Loss: 0.07874337583780289\n",
      "Epoch 4003, Loss: 0.170568086206913, Final Batch Loss: 0.07364243268966675\n",
      "Epoch 4004, Loss: 0.12665139883756638, Final Batch Loss: 0.04984592646360397\n",
      "Epoch 4005, Loss: 0.16517745703458786, Final Batch Loss: 0.09964735805988312\n",
      "Epoch 4006, Loss: 0.172689538449049, Final Batch Loss: 0.0624220035970211\n",
      "Epoch 4007, Loss: 0.1513344682753086, Final Batch Loss: 0.09621553122997284\n",
      "Epoch 4008, Loss: 0.1544179990887642, Final Batch Loss: 0.04492628574371338\n",
      "Epoch 4009, Loss: 0.22897478938102722, Final Batch Loss: 0.08978676795959473\n",
      "Epoch 4010, Loss: 0.16889763623476028, Final Batch Loss: 0.08693782240152359\n",
      "Epoch 4011, Loss: 0.16850817948579788, Final Batch Loss: 0.07162495702505112\n",
      "Epoch 4012, Loss: 0.14048077166080475, Final Batch Loss: 0.06438691914081573\n",
      "Epoch 4013, Loss: 0.14795155078172684, Final Batch Loss: 0.09272616356611252\n",
      "Epoch 4014, Loss: 0.1495121344923973, Final Batch Loss: 0.09674207866191864\n",
      "Epoch 4015, Loss: 0.2163773775100708, Final Batch Loss: 0.12394437193870544\n",
      "Epoch 4016, Loss: 0.13160710409283638, Final Batch Loss: 0.07538949698209763\n",
      "Epoch 4017, Loss: 0.1330988109111786, Final Batch Loss: 0.07714004069566727\n",
      "Epoch 4018, Loss: 0.15744763612747192, Final Batch Loss: 0.09618136286735535\n",
      "Epoch 4019, Loss: 0.1673273742198944, Final Batch Loss: 0.08055836707353592\n",
      "Epoch 4020, Loss: 0.20803768932819366, Final Batch Loss: 0.104815773665905\n",
      "Epoch 4021, Loss: 0.1598464883863926, Final Batch Loss: 0.05637485161423683\n",
      "Epoch 4022, Loss: 0.16170033067464828, Final Batch Loss: 0.058034032583236694\n",
      "Epoch 4023, Loss: 0.13480450212955475, Final Batch Loss: 0.07818800210952759\n",
      "Epoch 4024, Loss: 0.15522326529026031, Final Batch Loss: 0.060684338212013245\n",
      "Epoch 4025, Loss: 0.14177683740854263, Final Batch Loss: 0.08782123029232025\n",
      "Epoch 4026, Loss: 0.12769664824008942, Final Batch Loss: 0.06323514878749847\n",
      "Epoch 4027, Loss: 0.18077952787280083, Final Batch Loss: 0.05953230336308479\n",
      "Epoch 4028, Loss: 0.1287272870540619, Final Batch Loss: 0.08675246685743332\n",
      "Epoch 4029, Loss: 0.12507152557373047, Final Batch Loss: 0.06171759217977524\n",
      "Epoch 4030, Loss: 0.11425448954105377, Final Batch Loss: 0.04573403298854828\n",
      "Epoch 4031, Loss: 0.1744745373725891, Final Batch Loss: 0.10717375576496124\n",
      "Epoch 4032, Loss: 0.1981649436056614, Final Batch Loss: 0.1383214145898819\n",
      "Epoch 4033, Loss: 0.1631413921713829, Final Batch Loss: 0.09335856139659882\n",
      "Epoch 4034, Loss: 0.14057893306016922, Final Batch Loss: 0.07297731190919876\n",
      "Epoch 4035, Loss: 0.14669761061668396, Final Batch Loss: 0.058734484016895294\n",
      "Epoch 4036, Loss: 0.13151691108942032, Final Batch Loss: 0.06545181572437286\n",
      "Epoch 4037, Loss: 0.1422581598162651, Final Batch Loss: 0.04941230267286301\n",
      "Epoch 4038, Loss: 0.19960452616214752, Final Batch Loss: 0.0912240743637085\n",
      "Epoch 4039, Loss: 0.12615570798516273, Final Batch Loss: 0.07407989352941513\n",
      "Epoch 4040, Loss: 0.13801468536257744, Final Batch Loss: 0.05792755261063576\n",
      "Epoch 4041, Loss: 0.1534155234694481, Final Batch Loss: 0.07414870709180832\n",
      "Epoch 4042, Loss: 0.1637345477938652, Final Batch Loss: 0.0913010686635971\n",
      "Epoch 4043, Loss: 0.12134653702378273, Final Batch Loss: 0.06739809364080429\n",
      "Epoch 4044, Loss: 0.14524677395820618, Final Batch Loss: 0.05369541049003601\n",
      "Epoch 4045, Loss: 0.1313961036503315, Final Batch Loss: 0.05516447499394417\n",
      "Epoch 4046, Loss: 0.13328561186790466, Final Batch Loss: 0.07798758149147034\n",
      "Epoch 4047, Loss: 0.18960172683000565, Final Batch Loss: 0.08968781679868698\n",
      "Epoch 4048, Loss: 0.17508430033922195, Final Batch Loss: 0.0836082473397255\n",
      "Epoch 4049, Loss: 0.15929196774959564, Final Batch Loss: 0.07793945074081421\n",
      "Epoch 4050, Loss: 0.12144806981086731, Final Batch Loss: 0.07152394950389862\n",
      "Epoch 4051, Loss: 0.10679975897073746, Final Batch Loss: 0.06777974963188171\n",
      "Epoch 4052, Loss: 0.15154145658016205, Final Batch Loss: 0.07933434844017029\n",
      "Epoch 4053, Loss: 0.1622692346572876, Final Batch Loss: 0.07382375746965408\n",
      "Epoch 4054, Loss: 0.12335780262947083, Final Batch Loss: 0.072969950735569\n",
      "Epoch 4055, Loss: 0.15917785465717316, Final Batch Loss: 0.111412912607193\n",
      "Epoch 4056, Loss: 0.15199082344770432, Final Batch Loss: 0.054263994097709656\n",
      "Epoch 4057, Loss: 0.19458316266536713, Final Batch Loss: 0.08049819618463516\n",
      "Epoch 4058, Loss: 0.16845932230353355, Final Batch Loss: 0.060029011219739914\n",
      "Epoch 4059, Loss: 0.13510746508836746, Final Batch Loss: 0.07607868313789368\n",
      "Epoch 4060, Loss: 0.22106853872537613, Final Batch Loss: 0.14216288924217224\n",
      "Epoch 4061, Loss: 0.13650675863027573, Final Batch Loss: 0.07238634675741196\n",
      "Epoch 4062, Loss: 0.15616938471794128, Final Batch Loss: 0.0742466002702713\n",
      "Epoch 4063, Loss: 0.13801344484090805, Final Batch Loss: 0.07940592616796494\n",
      "Epoch 4064, Loss: 0.11839832738041878, Final Batch Loss: 0.05442643538117409\n",
      "Epoch 4065, Loss: 0.12987146899104118, Final Batch Loss: 0.0755978599190712\n",
      "Epoch 4066, Loss: 0.1540212333202362, Final Batch Loss: 0.08727570623159409\n",
      "Epoch 4067, Loss: 0.12380317226052284, Final Batch Loss: 0.06592025607824326\n",
      "Epoch 4068, Loss: 0.1460568681359291, Final Batch Loss: 0.07866999506950378\n",
      "Epoch 4069, Loss: 0.12189378961920738, Final Batch Loss: 0.04662894085049629\n",
      "Epoch 4070, Loss: 0.17056884616613388, Final Batch Loss: 0.09372957050800323\n",
      "Epoch 4071, Loss: 0.1488073691725731, Final Batch Loss: 0.07108379900455475\n",
      "Epoch 4072, Loss: 0.09003011509776115, Final Batch Loss: 0.05109222233295441\n",
      "Epoch 4073, Loss: 0.1576540246605873, Final Batch Loss: 0.11357980221509933\n",
      "Epoch 4074, Loss: 0.1709311157464981, Final Batch Loss: 0.09397067129611969\n",
      "Epoch 4075, Loss: 0.13023313134908676, Final Batch Loss: 0.04694952815771103\n",
      "Epoch 4076, Loss: 0.16263140365481377, Final Batch Loss: 0.04691537097096443\n",
      "Epoch 4077, Loss: 0.15529507398605347, Final Batch Loss: 0.08531319350004196\n",
      "Epoch 4078, Loss: 0.13226519525051117, Final Batch Loss: 0.06055767089128494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4079, Loss: 0.152794249355793, Final Batch Loss: 0.07600530982017517\n",
      "Epoch 4080, Loss: 0.1680867150425911, Final Batch Loss: 0.10198014974594116\n",
      "Epoch 4081, Loss: 0.14127758890390396, Final Batch Loss: 0.04887350648641586\n",
      "Epoch 4082, Loss: 0.0980667881667614, Final Batch Loss: 0.03732963651418686\n",
      "Epoch 4083, Loss: 0.1572589948773384, Final Batch Loss: 0.08076726645231247\n",
      "Epoch 4084, Loss: 0.2548675686120987, Final Batch Loss: 0.12812921404838562\n",
      "Epoch 4085, Loss: 0.17644642293453217, Final Batch Loss: 0.07667328417301178\n",
      "Epoch 4086, Loss: 0.14883118867874146, Final Batch Loss: 0.06826050579547882\n",
      "Epoch 4087, Loss: 0.20297318696975708, Final Batch Loss: 0.10588536411523819\n",
      "Epoch 4088, Loss: 0.17449834197759628, Final Batch Loss: 0.09982188045978546\n",
      "Epoch 4089, Loss: 0.19064822793006897, Final Batch Loss: 0.08030204474925995\n",
      "Epoch 4090, Loss: 0.16117334365844727, Final Batch Loss: 0.07550089806318283\n",
      "Epoch 4091, Loss: 0.177784264087677, Final Batch Loss: 0.10994714498519897\n",
      "Epoch 4092, Loss: 0.1531321108341217, Final Batch Loss: 0.0825432687997818\n",
      "Epoch 4093, Loss: 0.1913103237748146, Final Batch Loss: 0.10196921974420547\n",
      "Epoch 4094, Loss: 0.14735905826091766, Final Batch Loss: 0.07720588147640228\n",
      "Epoch 4095, Loss: 0.19186024367809296, Final Batch Loss: 0.09711886942386627\n",
      "Epoch 4096, Loss: 0.1960073933005333, Final Batch Loss: 0.12766113877296448\n",
      "Epoch 4097, Loss: 0.14435729756951332, Final Batch Loss: 0.08206497132778168\n",
      "Epoch 4098, Loss: 0.1698603704571724, Final Batch Loss: 0.1015646681189537\n",
      "Epoch 4099, Loss: 0.15786854922771454, Final Batch Loss: 0.07555967569351196\n",
      "Epoch 4100, Loss: 0.2144155390560627, Final Batch Loss: 0.05552547052502632\n",
      "Epoch 4101, Loss: 0.14496910944581032, Final Batch Loss: 0.051120463758707047\n",
      "Epoch 4102, Loss: 0.20678983628749847, Final Batch Loss: 0.12006419152021408\n",
      "Epoch 4103, Loss: 0.16844195127487183, Final Batch Loss: 0.07026892155408859\n",
      "Epoch 4104, Loss: 0.16472532600164413, Final Batch Loss: 0.07028871774673462\n",
      "Epoch 4105, Loss: 0.12387403473258018, Final Batch Loss: 0.07820377498865128\n",
      "Epoch 4106, Loss: 0.23859621584415436, Final Batch Loss: 0.0973171591758728\n",
      "Epoch 4107, Loss: 0.134408600628376, Final Batch Loss: 0.06945271044969559\n",
      "Epoch 4108, Loss: 0.14168984070420265, Final Batch Loss: 0.05698032304644585\n",
      "Epoch 4109, Loss: 0.16496505215764046, Final Batch Loss: 0.10662217438220978\n",
      "Epoch 4110, Loss: 0.09332326427102089, Final Batch Loss: 0.06177201122045517\n",
      "Epoch 4111, Loss: 0.23995473235845566, Final Batch Loss: 0.17964844405651093\n",
      "Epoch 4112, Loss: 0.17297320812940598, Final Batch Loss: 0.09444325417280197\n",
      "Epoch 4113, Loss: 0.17209065705537796, Final Batch Loss: 0.1147872731089592\n",
      "Epoch 4114, Loss: 0.18363018333911896, Final Batch Loss: 0.10262003540992737\n",
      "Epoch 4115, Loss: 0.12377584353089333, Final Batch Loss: 0.05107877030968666\n",
      "Epoch 4116, Loss: 0.1667276695370674, Final Batch Loss: 0.08466757833957672\n",
      "Epoch 4117, Loss: 0.16131222993135452, Final Batch Loss: 0.06920783221721649\n",
      "Epoch 4118, Loss: 0.1594042107462883, Final Batch Loss: 0.08107521384954453\n",
      "Epoch 4119, Loss: 0.1427922248840332, Final Batch Loss: 0.11046908795833588\n",
      "Epoch 4120, Loss: 0.2290855348110199, Final Batch Loss: 0.10891325026750565\n",
      "Epoch 4121, Loss: 0.13581689447164536, Final Batch Loss: 0.08143243193626404\n",
      "Epoch 4122, Loss: 0.11565034091472626, Final Batch Loss: 0.05429024621844292\n",
      "Epoch 4123, Loss: 0.16140426695346832, Final Batch Loss: 0.06867613643407822\n",
      "Epoch 4124, Loss: 0.14387279003858566, Final Batch Loss: 0.07073492556810379\n",
      "Epoch 4125, Loss: 0.14050175994634628, Final Batch Loss: 0.06772810965776443\n",
      "Epoch 4126, Loss: 0.1515558958053589, Final Batch Loss: 0.06839537620544434\n",
      "Epoch 4127, Loss: 0.19344881922006607, Final Batch Loss: 0.08837489783763885\n",
      "Epoch 4128, Loss: 0.1824592649936676, Final Batch Loss: 0.09764400124549866\n",
      "Epoch 4129, Loss: 0.12339362874627113, Final Batch Loss: 0.06502445042133331\n",
      "Epoch 4130, Loss: 0.12761853635311127, Final Batch Loss: 0.04674647003412247\n",
      "Epoch 4131, Loss: 0.1882372945547104, Final Batch Loss: 0.07628845423460007\n",
      "Epoch 4132, Loss: 0.12972566857933998, Final Batch Loss: 0.055617671459913254\n",
      "Epoch 4133, Loss: 0.14099173992872238, Final Batch Loss: 0.07405564934015274\n",
      "Epoch 4134, Loss: 0.2204245701432228, Final Batch Loss: 0.13318674266338348\n",
      "Epoch 4135, Loss: 0.15050137042999268, Final Batch Loss: 0.07568272948265076\n",
      "Epoch 4136, Loss: 0.12329141795635223, Final Batch Loss: 0.05517182499170303\n",
      "Epoch 4137, Loss: 0.18060407042503357, Final Batch Loss: 0.089588463306427\n",
      "Epoch 4138, Loss: 0.18047882616519928, Final Batch Loss: 0.10551171749830246\n",
      "Epoch 4139, Loss: 0.14764916896820068, Final Batch Loss: 0.07263129204511642\n",
      "Epoch 4140, Loss: 0.13738621026277542, Final Batch Loss: 0.0680515393614769\n",
      "Epoch 4141, Loss: 0.162741057574749, Final Batch Loss: 0.09119059890508652\n",
      "Epoch 4142, Loss: 0.15219654887914658, Final Batch Loss: 0.06777919828891754\n",
      "Epoch 4143, Loss: 0.15882380679249763, Final Batch Loss: 0.10067373514175415\n",
      "Epoch 4144, Loss: 0.2187550589442253, Final Batch Loss: 0.09332815557718277\n",
      "Epoch 4145, Loss: 0.19587481021881104, Final Batch Loss: 0.07758886367082596\n",
      "Epoch 4146, Loss: 0.12678592279553413, Final Batch Loss: 0.07707876712083817\n",
      "Epoch 4147, Loss: 0.10309800878167152, Final Batch Loss: 0.05841441452503204\n",
      "Epoch 4148, Loss: 0.12233291938900948, Final Batch Loss: 0.0875752717256546\n",
      "Epoch 4149, Loss: 0.13149337098002434, Final Batch Loss: 0.07769893854856491\n",
      "Epoch 4150, Loss: 0.15932876989245415, Final Batch Loss: 0.10033480823040009\n",
      "Epoch 4151, Loss: 0.1201094537973404, Final Batch Loss: 0.06083871051669121\n",
      "Epoch 4152, Loss: 0.17894533276557922, Final Batch Loss: 0.06000378727912903\n",
      "Epoch 4153, Loss: 0.1915176808834076, Final Batch Loss: 0.12179560214281082\n",
      "Epoch 4154, Loss: 0.16730159521102905, Final Batch Loss: 0.069113589823246\n",
      "Epoch 4155, Loss: 0.19701383262872696, Final Batch Loss: 0.10971179604530334\n",
      "Epoch 4156, Loss: 0.2586749270558357, Final Batch Loss: 0.06657692044973373\n",
      "Epoch 4157, Loss: 0.17292039841413498, Final Batch Loss: 0.0964510515332222\n",
      "Epoch 4158, Loss: 0.12405083701014519, Final Batch Loss: 0.05935712531208992\n",
      "Epoch 4159, Loss: 0.13469931483268738, Final Batch Loss: 0.06437333673238754\n",
      "Epoch 4160, Loss: 0.12860509008169174, Final Batch Loss: 0.06669812649488449\n",
      "Epoch 4161, Loss: 0.16264089196920395, Final Batch Loss: 0.06731787323951721\n",
      "Epoch 4162, Loss: 0.1443045809864998, Final Batch Loss: 0.0733342319726944\n",
      "Epoch 4163, Loss: 0.12254813313484192, Final Batch Loss: 0.08490709215402603\n",
      "Epoch 4164, Loss: 0.16496817022562027, Final Batch Loss: 0.08403598517179489\n",
      "Epoch 4165, Loss: 0.27801408246159554, Final Batch Loss: 0.2159089893102646\n",
      "Epoch 4166, Loss: 0.1537027582526207, Final Batch Loss: 0.06840565800666809\n",
      "Epoch 4167, Loss: 0.19861257076263428, Final Batch Loss: 0.05710861086845398\n",
      "Epoch 4168, Loss: 0.15534836053848267, Final Batch Loss: 0.08519916981458664\n",
      "Epoch 4169, Loss: 0.17861763387918472, Final Batch Loss: 0.10207506269216537\n",
      "Epoch 4170, Loss: 0.2636306807398796, Final Batch Loss: 0.17609301209449768\n",
      "Epoch 4171, Loss: 0.1277402527630329, Final Batch Loss: 0.07995879650115967\n",
      "Epoch 4172, Loss: 0.1676526926457882, Final Batch Loss: 0.11815408617258072\n",
      "Epoch 4173, Loss: 0.15672238916158676, Final Batch Loss: 0.07451298087835312\n",
      "Epoch 4174, Loss: 0.14186545833945274, Final Batch Loss: 0.07996933162212372\n",
      "Epoch 4175, Loss: 0.19575195014476776, Final Batch Loss: 0.0851210206747055\n",
      "Epoch 4176, Loss: 0.14418495073914528, Final Batch Loss: 0.09259897470474243\n",
      "Epoch 4177, Loss: 0.11814649030566216, Final Batch Loss: 0.04744968190789223\n",
      "Epoch 4178, Loss: 0.15873638540506363, Final Batch Loss: 0.08538302034139633\n",
      "Epoch 4179, Loss: 0.13339730352163315, Final Batch Loss: 0.05852166563272476\n",
      "Epoch 4180, Loss: 0.1568981632590294, Final Batch Loss: 0.10384376347064972\n",
      "Epoch 4181, Loss: 0.11711807176470757, Final Batch Loss: 0.05880247429013252\n",
      "Epoch 4182, Loss: 0.16331801563501358, Final Batch Loss: 0.08656048774719238\n",
      "Epoch 4183, Loss: 0.15725719928741455, Final Batch Loss: 0.0840790718793869\n",
      "Epoch 4184, Loss: 0.1741422340273857, Final Batch Loss: 0.07923298329114914\n",
      "Epoch 4185, Loss: 0.15121974050998688, Final Batch Loss: 0.07908618450164795\n",
      "Epoch 4186, Loss: 0.1486210972070694, Final Batch Loss: 0.05590134859085083\n",
      "Epoch 4187, Loss: 0.13632278516888618, Final Batch Loss: 0.05014133080840111\n",
      "Epoch 4188, Loss: 0.1349972039461136, Final Batch Loss: 0.046346455812454224\n",
      "Epoch 4189, Loss: 0.1393572837114334, Final Batch Loss: 0.05746935307979584\n",
      "Epoch 4190, Loss: 0.10555550828576088, Final Batch Loss: 0.058553729206323624\n",
      "Epoch 4191, Loss: 0.13590270280838013, Final Batch Loss: 0.06977809965610504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4192, Loss: 0.1925654076039791, Final Batch Loss: 0.13550080358982086\n",
      "Epoch 4193, Loss: 0.13624625653028488, Final Batch Loss: 0.0651637613773346\n",
      "Epoch 4194, Loss: 0.132065087556839, Final Batch Loss: 0.051606908440589905\n",
      "Epoch 4195, Loss: 0.13173333927989006, Final Batch Loss: 0.05771840736269951\n",
      "Epoch 4196, Loss: 0.17024290561676025, Final Batch Loss: 0.06928762793540955\n",
      "Epoch 4197, Loss: 0.1454000510275364, Final Batch Loss: 0.08552499860525131\n",
      "Epoch 4198, Loss: 0.16856300085783005, Final Batch Loss: 0.0789242684841156\n",
      "Epoch 4199, Loss: 0.15659035369753838, Final Batch Loss: 0.11349284648895264\n",
      "Epoch 4200, Loss: 0.203714519739151, Final Batch Loss: 0.07104508578777313\n",
      "Epoch 4201, Loss: 0.19542540609836578, Final Batch Loss: 0.07548350840806961\n",
      "Epoch 4202, Loss: 0.1387496329843998, Final Batch Loss: 0.05174350365996361\n",
      "Epoch 4203, Loss: 0.11523973569273949, Final Batch Loss: 0.0776480883359909\n",
      "Epoch 4204, Loss: 0.20036178082227707, Final Batch Loss: 0.1245289072394371\n",
      "Epoch 4205, Loss: 0.1309014931321144, Final Batch Loss: 0.06406456232070923\n",
      "Epoch 4206, Loss: 0.10279332101345062, Final Batch Loss: 0.04111647233366966\n",
      "Epoch 4207, Loss: 0.16093116998672485, Final Batch Loss: 0.08029235899448395\n",
      "Epoch 4208, Loss: 0.12952838838100433, Final Batch Loss: 0.049058280885219574\n",
      "Epoch 4209, Loss: 0.1311509795486927, Final Batch Loss: 0.08157026022672653\n",
      "Epoch 4210, Loss: 0.13471171632409096, Final Batch Loss: 0.09326755255460739\n",
      "Epoch 4211, Loss: 0.11726247146725655, Final Batch Loss: 0.04219049587845802\n",
      "Epoch 4212, Loss: 0.10400179401040077, Final Batch Loss: 0.05238781124353409\n",
      "Epoch 4213, Loss: 0.18870612978935242, Final Batch Loss: 0.1286470890045166\n",
      "Epoch 4214, Loss: 0.11595864593982697, Final Batch Loss: 0.056009627878665924\n",
      "Epoch 4215, Loss: 0.13050426170229912, Final Batch Loss: 0.06883765012025833\n",
      "Epoch 4216, Loss: 0.19112277776002884, Final Batch Loss: 0.10545191168785095\n",
      "Epoch 4217, Loss: 0.1551947370171547, Final Batch Loss: 0.07264525443315506\n",
      "Epoch 4218, Loss: 0.09751413390040398, Final Batch Loss: 0.037504393607378006\n",
      "Epoch 4219, Loss: 0.1737072840332985, Final Batch Loss: 0.07807383686304092\n",
      "Epoch 4220, Loss: 0.22417398542165756, Final Batch Loss: 0.11491168290376663\n",
      "Epoch 4221, Loss: 0.12207616120576859, Final Batch Loss: 0.057793520390987396\n",
      "Epoch 4222, Loss: 0.11713745817542076, Final Batch Loss: 0.050528306514024734\n",
      "Epoch 4223, Loss: 0.15064562112092972, Final Batch Loss: 0.05990660935640335\n",
      "Epoch 4224, Loss: 0.16613880544900894, Final Batch Loss: 0.09917840361595154\n",
      "Epoch 4225, Loss: 0.2680201232433319, Final Batch Loss: 0.12514512240886688\n",
      "Epoch 4226, Loss: 0.24476251751184464, Final Batch Loss: 0.12737691402435303\n",
      "Epoch 4227, Loss: 0.15289194136857986, Final Batch Loss: 0.08282557874917984\n",
      "Epoch 4228, Loss: 0.15073838084936142, Final Batch Loss: 0.11978684365749359\n",
      "Epoch 4229, Loss: 0.12275492027401924, Final Batch Loss: 0.06352028995752335\n",
      "Epoch 4230, Loss: 0.13571104034781456, Final Batch Loss: 0.08094325661659241\n",
      "Epoch 4231, Loss: 0.12423510476946831, Final Batch Loss: 0.06051310524344444\n",
      "Epoch 4232, Loss: 0.13065890595316887, Final Batch Loss: 0.05765390768647194\n",
      "Epoch 4233, Loss: 0.19994939863681793, Final Batch Loss: 0.10199661552906036\n",
      "Epoch 4234, Loss: 0.12506957352161407, Final Batch Loss: 0.0647689625620842\n",
      "Epoch 4235, Loss: 0.18144027143716812, Final Batch Loss: 0.0971604585647583\n",
      "Epoch 4236, Loss: 0.14483029767870903, Final Batch Loss: 0.06064825877547264\n",
      "Epoch 4237, Loss: 0.1274733766913414, Final Batch Loss: 0.043480247259140015\n",
      "Epoch 4238, Loss: 0.12620893865823746, Final Batch Loss: 0.06159602105617523\n",
      "Epoch 4239, Loss: 0.14879252016544342, Final Batch Loss: 0.05633491277694702\n",
      "Epoch 4240, Loss: 0.13955935463309288, Final Batch Loss: 0.055424559861421585\n",
      "Epoch 4241, Loss: 0.12546982988715172, Final Batch Loss: 0.08234379440546036\n",
      "Epoch 4242, Loss: 0.1567813642323017, Final Batch Loss: 0.10548798739910126\n",
      "Epoch 4243, Loss: 0.21352972090244293, Final Batch Loss: 0.12598168849945068\n",
      "Epoch 4244, Loss: 0.20980750024318695, Final Batch Loss: 0.11718237400054932\n",
      "Epoch 4245, Loss: 0.09535638615489006, Final Batch Loss: 0.06536354124546051\n",
      "Epoch 4246, Loss: 0.15023817121982574, Final Batch Loss: 0.07763049751520157\n",
      "Epoch 4247, Loss: 0.16270487755537033, Final Batch Loss: 0.06349460035562515\n",
      "Epoch 4248, Loss: 0.15463387966156006, Final Batch Loss: 0.07563556730747223\n",
      "Epoch 4249, Loss: 0.17396774888038635, Final Batch Loss: 0.09719382226467133\n",
      "Epoch 4250, Loss: 0.12842927500605583, Final Batch Loss: 0.058395978063344955\n",
      "Epoch 4251, Loss: 0.1310378648340702, Final Batch Loss: 0.03910918906331062\n",
      "Epoch 4252, Loss: 0.18052295595407486, Final Batch Loss: 0.10533998161554337\n",
      "Epoch 4253, Loss: 0.1918962374329567, Final Batch Loss: 0.09318354725837708\n",
      "Epoch 4254, Loss: 0.16167277842760086, Final Batch Loss: 0.06149592995643616\n",
      "Epoch 4255, Loss: 0.14819453656673431, Final Batch Loss: 0.060282617807388306\n",
      "Epoch 4256, Loss: 0.27389511466026306, Final Batch Loss: 0.11039803922176361\n",
      "Epoch 4257, Loss: 0.1864609718322754, Final Batch Loss: 0.09672409296035767\n",
      "Epoch 4258, Loss: 0.15054331719875336, Final Batch Loss: 0.08921613544225693\n",
      "Epoch 4259, Loss: 0.15887487679719925, Final Batch Loss: 0.08997754007577896\n",
      "Epoch 4260, Loss: 0.1796506568789482, Final Batch Loss: 0.11672002822160721\n",
      "Epoch 4261, Loss: 0.13638140633702278, Final Batch Loss: 0.06144021078944206\n",
      "Epoch 4262, Loss: 0.14452531933784485, Final Batch Loss: 0.08143544942140579\n",
      "Epoch 4263, Loss: 0.1285085268318653, Final Batch Loss: 0.049365829676389694\n",
      "Epoch 4264, Loss: 0.14749739319086075, Final Batch Loss: 0.07851754873991013\n",
      "Epoch 4265, Loss: 0.13771723210811615, Final Batch Loss: 0.08785731345415115\n",
      "Epoch 4266, Loss: 0.09515643492341042, Final Batch Loss: 0.04329841956496239\n",
      "Epoch 4267, Loss: 0.2090524584054947, Final Batch Loss: 0.07716715335845947\n",
      "Epoch 4268, Loss: 0.23124312609434128, Final Batch Loss: 0.12841393053531647\n",
      "Epoch 4269, Loss: 0.11749763041734695, Final Batch Loss: 0.07578632980585098\n",
      "Epoch 4270, Loss: 0.14947205036878586, Final Batch Loss: 0.0736009031534195\n",
      "Epoch 4271, Loss: 0.1601090133190155, Final Batch Loss: 0.08389924466609955\n",
      "Epoch 4272, Loss: 0.12293564155697823, Final Batch Loss: 0.05663083866238594\n",
      "Epoch 4273, Loss: 0.1368466317653656, Final Batch Loss: 0.06626082211732864\n",
      "Epoch 4274, Loss: 0.11887333914637566, Final Batch Loss: 0.04424299672245979\n",
      "Epoch 4275, Loss: 0.17772622406482697, Final Batch Loss: 0.09165812283754349\n",
      "Epoch 4276, Loss: 0.22567953169345856, Final Batch Loss: 0.12542416155338287\n",
      "Epoch 4277, Loss: 0.09435125440359116, Final Batch Loss: 0.03925791010260582\n",
      "Epoch 4278, Loss: 0.11932643130421638, Final Batch Loss: 0.07092772424221039\n",
      "Epoch 4279, Loss: 0.17203933000564575, Final Batch Loss: 0.0821790099143982\n",
      "Epoch 4280, Loss: 0.14116834104061127, Final Batch Loss: 0.07331544905900955\n",
      "Epoch 4281, Loss: 0.13778556138277054, Final Batch Loss: 0.09146137535572052\n",
      "Epoch 4282, Loss: 0.1307957023382187, Final Batch Loss: 0.0640772357583046\n",
      "Epoch 4283, Loss: 0.13975229859352112, Final Batch Loss: 0.05102711170911789\n",
      "Epoch 4284, Loss: 0.14549895375967026, Final Batch Loss: 0.07066435366868973\n",
      "Epoch 4285, Loss: 0.10797831416130066, Final Batch Loss: 0.06507668644189835\n",
      "Epoch 4286, Loss: 0.1103530153632164, Final Batch Loss: 0.06975303590297699\n",
      "Epoch 4287, Loss: 0.11222760751843452, Final Batch Loss: 0.05827772244811058\n",
      "Epoch 4288, Loss: 0.17842993885278702, Final Batch Loss: 0.08652647584676743\n",
      "Epoch 4289, Loss: 0.11920864135026932, Final Batch Loss: 0.05193831026554108\n",
      "Epoch 4290, Loss: 0.19032394140958786, Final Batch Loss: 0.10032255947589874\n",
      "Epoch 4291, Loss: 0.13424593210220337, Final Batch Loss: 0.06848111003637314\n",
      "Epoch 4292, Loss: 0.11603041738271713, Final Batch Loss: 0.06298243999481201\n",
      "Epoch 4293, Loss: 0.13869009912014008, Final Batch Loss: 0.05593239516019821\n",
      "Epoch 4294, Loss: 0.22684691101312637, Final Batch Loss: 0.12224937975406647\n",
      "Epoch 4295, Loss: 0.16943632066249847, Final Batch Loss: 0.07769066840410233\n",
      "Epoch 4296, Loss: 0.12746112048625946, Final Batch Loss: 0.0633220300078392\n",
      "Epoch 4297, Loss: 0.1385534256696701, Final Batch Loss: 0.0691424310207367\n",
      "Epoch 4298, Loss: 0.14259030669927597, Final Batch Loss: 0.06516461819410324\n",
      "Epoch 4299, Loss: 0.1178479827940464, Final Batch Loss: 0.06472830474376678\n",
      "Epoch 4300, Loss: 0.09662917256355286, Final Batch Loss: 0.048745959997177124\n",
      "Epoch 4301, Loss: 0.1752997413277626, Final Batch Loss: 0.06492976099252701\n",
      "Epoch 4302, Loss: 0.14262323826551437, Final Batch Loss: 0.07512803375720978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4303, Loss: 0.1751803308725357, Final Batch Loss: 0.07079461216926575\n",
      "Epoch 4304, Loss: 0.13795393332839012, Final Batch Loss: 0.04957718774676323\n",
      "Epoch 4305, Loss: 0.13607271015644073, Final Batch Loss: 0.08057131618261337\n",
      "Epoch 4306, Loss: 0.11633249372243881, Final Batch Loss: 0.05131077021360397\n",
      "Epoch 4307, Loss: 0.12092574685811996, Final Batch Loss: 0.08023758977651596\n",
      "Epoch 4308, Loss: 0.15717309713363647, Final Batch Loss: 0.07955906540155411\n",
      "Epoch 4309, Loss: 0.17581384629011154, Final Batch Loss: 0.10625849664211273\n",
      "Epoch 4310, Loss: 0.13341130316257477, Final Batch Loss: 0.06788928061723709\n",
      "Epoch 4311, Loss: 0.1597011759877205, Final Batch Loss: 0.0935487374663353\n",
      "Epoch 4312, Loss: 0.1282833069562912, Final Batch Loss: 0.0867888480424881\n",
      "Epoch 4313, Loss: 0.11863397434353828, Final Batch Loss: 0.05418306216597557\n",
      "Epoch 4314, Loss: 0.08519366383552551, Final Batch Loss: 0.03210458159446716\n",
      "Epoch 4315, Loss: 0.10244712233543396, Final Batch Loss: 0.04205704107880592\n",
      "Epoch 4316, Loss: 0.09567329287528992, Final Batch Loss: 0.03644019737839699\n",
      "Epoch 4317, Loss: 0.12629897519946098, Final Batch Loss: 0.0667695626616478\n",
      "Epoch 4318, Loss: 0.1628800481557846, Final Batch Loss: 0.057192668318748474\n",
      "Epoch 4319, Loss: 0.11590558290481567, Final Batch Loss: 0.06318362802267075\n",
      "Epoch 4320, Loss: 0.14131580665707588, Final Batch Loss: 0.04886746034026146\n",
      "Epoch 4321, Loss: 0.17127050459384918, Final Batch Loss: 0.0844455435872078\n",
      "Epoch 4322, Loss: 0.15395697206258774, Final Batch Loss: 0.08085475862026215\n",
      "Epoch 4323, Loss: 0.12807796150445938, Final Batch Loss: 0.07538887858390808\n",
      "Epoch 4324, Loss: 0.11572999507188797, Final Batch Loss: 0.06336238980293274\n",
      "Epoch 4325, Loss: 0.14922283962368965, Final Batch Loss: 0.09083302319049835\n",
      "Epoch 4326, Loss: 0.14898863807320595, Final Batch Loss: 0.061554040759801865\n",
      "Epoch 4327, Loss: 0.1229536160826683, Final Batch Loss: 0.06616095453500748\n",
      "Epoch 4328, Loss: 0.1324949748814106, Final Batch Loss: 0.044440072029829025\n",
      "Epoch 4329, Loss: 0.17013172060251236, Final Batch Loss: 0.08771081268787384\n",
      "Epoch 4330, Loss: 0.11325322836637497, Final Batch Loss: 0.060576874762773514\n",
      "Epoch 4331, Loss: 0.16720064729452133, Final Batch Loss: 0.06605532765388489\n",
      "Epoch 4332, Loss: 0.18363792449235916, Final Batch Loss: 0.06519337743520737\n",
      "Epoch 4333, Loss: 0.1778670847415924, Final Batch Loss: 0.07284270226955414\n",
      "Epoch 4334, Loss: 0.1313474178314209, Final Batch Loss: 0.06410280615091324\n",
      "Epoch 4335, Loss: 0.18811319768428802, Final Batch Loss: 0.11067572981119156\n",
      "Epoch 4336, Loss: 0.167096309363842, Final Batch Loss: 0.0860678106546402\n",
      "Epoch 4337, Loss: 0.13286485150456429, Final Batch Loss: 0.08031431585550308\n",
      "Epoch 4338, Loss: 0.12293620780110359, Final Batch Loss: 0.04254679009318352\n",
      "Epoch 4339, Loss: 0.12840677052736282, Final Batch Loss: 0.058042652904987335\n",
      "Epoch 4340, Loss: 0.1221090629696846, Final Batch Loss: 0.06967916339635849\n",
      "Epoch 4341, Loss: 0.16508720815181732, Final Batch Loss: 0.09005168080329895\n",
      "Epoch 4342, Loss: 0.10349395871162415, Final Batch Loss: 0.04527755081653595\n",
      "Epoch 4343, Loss: 0.17142456024885178, Final Batch Loss: 0.12535709142684937\n",
      "Epoch 4344, Loss: 0.1307709589600563, Final Batch Loss: 0.0665888637304306\n",
      "Epoch 4345, Loss: 0.166806161403656, Final Batch Loss: 0.09441609680652618\n",
      "Epoch 4346, Loss: 0.13561134040355682, Final Batch Loss: 0.04810427874326706\n",
      "Epoch 4347, Loss: 0.13556308299303055, Final Batch Loss: 0.07028539478778839\n",
      "Epoch 4348, Loss: 0.09158621728420258, Final Batch Loss: 0.057564422488212585\n",
      "Epoch 4349, Loss: 0.10870912298560143, Final Batch Loss: 0.05700415372848511\n",
      "Epoch 4350, Loss: 0.1419833078980446, Final Batch Loss: 0.09222403913736343\n",
      "Epoch 4351, Loss: 0.1566189005970955, Final Batch Loss: 0.0711832270026207\n",
      "Epoch 4352, Loss: 0.15687568485736847, Final Batch Loss: 0.07495048642158508\n",
      "Epoch 4353, Loss: 0.14134567230939865, Final Batch Loss: 0.07686702162027359\n",
      "Epoch 4354, Loss: 0.14995213970541954, Final Batch Loss: 0.06029250845313072\n",
      "Epoch 4355, Loss: 0.1258450411260128, Final Batch Loss: 0.06386161595582962\n",
      "Epoch 4356, Loss: 0.14279869198799133, Final Batch Loss: 0.06755702197551727\n",
      "Epoch 4357, Loss: 0.11708414927124977, Final Batch Loss: 0.05110948160290718\n",
      "Epoch 4358, Loss: 0.1540273241698742, Final Batch Loss: 0.09246011078357697\n",
      "Epoch 4359, Loss: 0.12941284850239754, Final Batch Loss: 0.04517655447125435\n",
      "Epoch 4360, Loss: 0.12199137732386589, Final Batch Loss: 0.08109506219625473\n",
      "Epoch 4361, Loss: 0.13793619722127914, Final Batch Loss: 0.0672752782702446\n",
      "Epoch 4362, Loss: 0.16183829307556152, Final Batch Loss: 0.0738886147737503\n",
      "Epoch 4363, Loss: 0.17718320339918137, Final Batch Loss: 0.08347184211015701\n",
      "Epoch 4364, Loss: 0.12218139320611954, Final Batch Loss: 0.07284930348396301\n",
      "Epoch 4365, Loss: 0.1801694631576538, Final Batch Loss: 0.08442321419715881\n",
      "Epoch 4366, Loss: 0.2468637079000473, Final Batch Loss: 0.08863906562328339\n",
      "Epoch 4367, Loss: 0.19848095625638962, Final Batch Loss: 0.09274774044752121\n",
      "Epoch 4368, Loss: 0.16273456811904907, Final Batch Loss: 0.08043607324361801\n",
      "Epoch 4369, Loss: 0.11630358919501305, Final Batch Loss: 0.060009125620126724\n",
      "Epoch 4370, Loss: 0.12298431620001793, Final Batch Loss: 0.06931911408901215\n",
      "Epoch 4371, Loss: 0.10558124259114265, Final Batch Loss: 0.04380669444799423\n",
      "Epoch 4372, Loss: 0.12164877355098724, Final Batch Loss: 0.06327445060014725\n",
      "Epoch 4373, Loss: 0.12333105877041817, Final Batch Loss: 0.08432205021381378\n",
      "Epoch 4374, Loss: 0.158199705183506, Final Batch Loss: 0.0554036945104599\n",
      "Epoch 4375, Loss: 0.11200419068336487, Final Batch Loss: 0.04522201418876648\n",
      "Epoch 4376, Loss: 0.11035271733999252, Final Batch Loss: 0.054171569645404816\n",
      "Epoch 4377, Loss: 0.10364918410778046, Final Batch Loss: 0.05318130552768707\n",
      "Epoch 4378, Loss: 0.13709239289164543, Final Batch Loss: 0.06024324521422386\n",
      "Epoch 4379, Loss: 0.1832582727074623, Final Batch Loss: 0.07158788293600082\n",
      "Epoch 4380, Loss: 0.10736902989447117, Final Batch Loss: 0.02463364042341709\n",
      "Epoch 4381, Loss: 0.18079763278365135, Final Batch Loss: 0.12168583273887634\n",
      "Epoch 4382, Loss: 0.15684979408979416, Final Batch Loss: 0.08287931978702545\n",
      "Epoch 4383, Loss: 0.16189168393611908, Final Batch Loss: 0.0778525099158287\n",
      "Epoch 4384, Loss: 0.12724127620458603, Final Batch Loss: 0.0660034641623497\n",
      "Epoch 4385, Loss: 0.1220707930624485, Final Batch Loss: 0.05867573991417885\n",
      "Epoch 4386, Loss: 0.100707296282053, Final Batch Loss: 0.0371234305202961\n",
      "Epoch 4387, Loss: 0.12320693582296371, Final Batch Loss: 0.05328597128391266\n",
      "Epoch 4388, Loss: 0.12303061038255692, Final Batch Loss: 0.07331405580043793\n",
      "Epoch 4389, Loss: 0.14558470249176025, Final Batch Loss: 0.05695231258869171\n",
      "Epoch 4390, Loss: 0.11709127947688103, Final Batch Loss: 0.04006047174334526\n",
      "Epoch 4391, Loss: 0.12734068930149078, Final Batch Loss: 0.07883241772651672\n",
      "Epoch 4392, Loss: 0.08277453482151031, Final Batch Loss: 0.03180505707859993\n",
      "Epoch 4393, Loss: 0.15079426765441895, Final Batch Loss: 0.06988505274057388\n",
      "Epoch 4394, Loss: 0.19359049946069717, Final Batch Loss: 0.11204379796981812\n",
      "Epoch 4395, Loss: 0.1637232042849064, Final Batch Loss: 0.0570591501891613\n",
      "Epoch 4396, Loss: 0.1736622452735901, Final Batch Loss: 0.07663439214229584\n",
      "Epoch 4397, Loss: 0.13408679515123367, Final Batch Loss: 0.05108851194381714\n",
      "Epoch 4398, Loss: 0.14585208147764206, Final Batch Loss: 0.08937343955039978\n",
      "Epoch 4399, Loss: 0.11230092495679855, Final Batch Loss: 0.05462540686130524\n",
      "Epoch 4400, Loss: 0.1764073148369789, Final Batch Loss: 0.10714425146579742\n",
      "Epoch 4401, Loss: 0.1737268567085266, Final Batch Loss: 0.09452672302722931\n",
      "Epoch 4402, Loss: 0.1538735218346119, Final Batch Loss: 0.09486965090036392\n",
      "Epoch 4403, Loss: 0.22187341004610062, Final Batch Loss: 0.08774638921022415\n",
      "Epoch 4404, Loss: 0.1532713547348976, Final Batch Loss: 0.07767101377248764\n",
      "Epoch 4405, Loss: 0.21605730801820755, Final Batch Loss: 0.09548218548297882\n",
      "Epoch 4406, Loss: 0.1544034779071808, Final Batch Loss: 0.08085724711418152\n",
      "Epoch 4407, Loss: 0.16265550255775452, Final Batch Loss: 0.07291141897439957\n",
      "Epoch 4408, Loss: 0.15553653985261917, Final Batch Loss: 0.09214489907026291\n",
      "Epoch 4409, Loss: 0.1783711090683937, Final Batch Loss: 0.1032734140753746\n",
      "Epoch 4410, Loss: 0.1410154215991497, Final Batch Loss: 0.08561109751462936\n",
      "Epoch 4411, Loss: 0.189823716878891, Final Batch Loss: 0.10820572078227997\n",
      "Epoch 4412, Loss: 0.13026560097932816, Final Batch Loss: 0.05038920044898987\n",
      "Epoch 4413, Loss: 0.16494736075401306, Final Batch Loss: 0.128157377243042\n",
      "Epoch 4414, Loss: 0.12059375643730164, Final Batch Loss: 0.045819856226444244\n",
      "Epoch 4415, Loss: 0.12550166249275208, Final Batch Loss: 0.053364112973213196\n",
      "Epoch 4416, Loss: 0.101495411247015, Final Batch Loss: 0.048528313636779785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4417, Loss: 0.11778221651911736, Final Batch Loss: 0.05421197786927223\n",
      "Epoch 4418, Loss: 0.24508003145456314, Final Batch Loss: 0.1284554898738861\n",
      "Epoch 4419, Loss: 0.09907113388180733, Final Batch Loss: 0.05187360197305679\n",
      "Epoch 4420, Loss: 0.10872434079647064, Final Batch Loss: 0.05232406035065651\n",
      "Epoch 4421, Loss: 0.11762193962931633, Final Batch Loss: 0.05511549487709999\n",
      "Epoch 4422, Loss: 0.15161960572004318, Final Batch Loss: 0.0804542675614357\n",
      "Epoch 4423, Loss: 0.16761768609285355, Final Batch Loss: 0.09486882388591766\n",
      "Epoch 4424, Loss: 0.17785801738500595, Final Batch Loss: 0.09913717210292816\n",
      "Epoch 4425, Loss: 0.11360673606395721, Final Batch Loss: 0.07423003017902374\n",
      "Epoch 4426, Loss: 0.189649797976017, Final Batch Loss: 0.0920620933175087\n",
      "Epoch 4427, Loss: 0.13731031119823456, Final Batch Loss: 0.05443424731492996\n",
      "Epoch 4428, Loss: 0.1704905778169632, Final Batch Loss: 0.10231668502092361\n",
      "Epoch 4429, Loss: 0.14038078486919403, Final Batch Loss: 0.09380636364221573\n",
      "Epoch 4430, Loss: 0.15394078195095062, Final Batch Loss: 0.0881601870059967\n",
      "Epoch 4431, Loss: 0.1908288076519966, Final Batch Loss: 0.04495102912187576\n",
      "Epoch 4432, Loss: 0.16299139335751534, Final Batch Loss: 0.11737129837274551\n",
      "Epoch 4433, Loss: 0.15519362688064575, Final Batch Loss: 0.07352611422538757\n",
      "Epoch 4434, Loss: 0.10778108984231949, Final Batch Loss: 0.047303952276706696\n",
      "Epoch 4435, Loss: 0.14033950865268707, Final Batch Loss: 0.07243409752845764\n",
      "Epoch 4436, Loss: 0.12108511477708817, Final Batch Loss: 0.07165732234716415\n",
      "Epoch 4437, Loss: 0.142335195094347, Final Batch Loss: 0.08044663816690445\n",
      "Epoch 4438, Loss: 0.12708871439099312, Final Batch Loss: 0.058799128979444504\n",
      "Epoch 4439, Loss: 0.14848649129271507, Final Batch Loss: 0.058724287897348404\n",
      "Epoch 4440, Loss: 0.17608486115932465, Final Batch Loss: 0.141307532787323\n",
      "Epoch 4441, Loss: 0.14381113648414612, Final Batch Loss: 0.09846898168325424\n",
      "Epoch 4442, Loss: 0.12150207906961441, Final Batch Loss: 0.03916333615779877\n",
      "Epoch 4443, Loss: 0.17006705701351166, Final Batch Loss: 0.1051534116268158\n",
      "Epoch 4444, Loss: 0.17058105021715164, Final Batch Loss: 0.06710362434387207\n",
      "Epoch 4445, Loss: 0.12343784794211388, Final Batch Loss: 0.07342110574245453\n",
      "Epoch 4446, Loss: 0.14701631665229797, Final Batch Loss: 0.08395124226808548\n",
      "Epoch 4447, Loss: 0.09118640795350075, Final Batch Loss: 0.04427415505051613\n",
      "Epoch 4448, Loss: 0.1798045113682747, Final Batch Loss: 0.0725918710231781\n",
      "Epoch 4449, Loss: 0.12702876701951027, Final Batch Loss: 0.05272718146443367\n",
      "Epoch 4450, Loss: 0.143427062779665, Final Batch Loss: 0.08941159397363663\n",
      "Epoch 4451, Loss: 0.1395871713757515, Final Batch Loss: 0.07970556616783142\n",
      "Epoch 4452, Loss: 0.13848910480737686, Final Batch Loss: 0.08060766756534576\n",
      "Epoch 4453, Loss: 0.1536182016134262, Final Batch Loss: 0.08489180356264114\n",
      "Epoch 4454, Loss: 0.11474339663982391, Final Batch Loss: 0.06424342840909958\n",
      "Epoch 4455, Loss: 0.11566995456814766, Final Batch Loss: 0.06042348966002464\n",
      "Epoch 4456, Loss: 0.13098926097154617, Final Batch Loss: 0.03580528497695923\n",
      "Epoch 4457, Loss: 0.13955390825867653, Final Batch Loss: 0.09905654191970825\n",
      "Epoch 4458, Loss: 0.14388764649629593, Final Batch Loss: 0.07958224415779114\n",
      "Epoch 4459, Loss: 0.19716525077819824, Final Batch Loss: 0.10086067020893097\n",
      "Epoch 4460, Loss: 0.1740138679742813, Final Batch Loss: 0.07945744693279266\n",
      "Epoch 4461, Loss: 0.12168481200933456, Final Batch Loss: 0.06998028606176376\n",
      "Epoch 4462, Loss: 0.10351739823818207, Final Batch Loss: 0.05493919923901558\n",
      "Epoch 4463, Loss: 0.20330702513456345, Final Batch Loss: 0.12214286625385284\n",
      "Epoch 4464, Loss: 0.15317126363515854, Final Batch Loss: 0.07352738827466965\n",
      "Epoch 4465, Loss: 0.12647204473614693, Final Batch Loss: 0.059289220720529556\n",
      "Epoch 4466, Loss: 0.11412014067173004, Final Batch Loss: 0.06701508164405823\n",
      "Epoch 4467, Loss: 0.12094109505414963, Final Batch Loss: 0.05120856314897537\n",
      "Epoch 4468, Loss: 0.16727720946073532, Final Batch Loss: 0.06506209820508957\n",
      "Epoch 4469, Loss: 0.14735272154211998, Final Batch Loss: 0.05194352939724922\n",
      "Epoch 4470, Loss: 0.20035216212272644, Final Batch Loss: 0.11961666494607925\n",
      "Epoch 4471, Loss: 0.16120904311537743, Final Batch Loss: 0.11594909429550171\n",
      "Epoch 4472, Loss: 0.15346136689186096, Final Batch Loss: 0.07396695017814636\n",
      "Epoch 4473, Loss: 0.13973797112703323, Final Batch Loss: 0.06745193153619766\n",
      "Epoch 4474, Loss: 0.1477506384253502, Final Batch Loss: 0.06690821796655655\n",
      "Epoch 4475, Loss: 0.14410238340497017, Final Batch Loss: 0.09175852686166763\n",
      "Epoch 4476, Loss: 0.15108074992895126, Final Batch Loss: 0.07197972387075424\n",
      "Epoch 4477, Loss: 0.1500500626862049, Final Batch Loss: 0.05932316556572914\n",
      "Epoch 4478, Loss: 0.11822029948234558, Final Batch Loss: 0.06545914709568024\n",
      "Epoch 4479, Loss: 0.09114529564976692, Final Batch Loss: 0.05486184358596802\n",
      "Epoch 4480, Loss: 0.1395578682422638, Final Batch Loss: 0.07934218645095825\n",
      "Epoch 4481, Loss: 0.1330445632338524, Final Batch Loss: 0.07027596980333328\n",
      "Epoch 4482, Loss: 0.11493171751499176, Final Batch Loss: 0.06397632509469986\n",
      "Epoch 4483, Loss: 0.14935437589883804, Final Batch Loss: 0.06526371836662292\n",
      "Epoch 4484, Loss: 0.08889155089855194, Final Batch Loss: 0.057627756148576736\n",
      "Epoch 4485, Loss: 0.20163623243570328, Final Batch Loss: 0.12714453041553497\n",
      "Epoch 4486, Loss: 0.09994371607899666, Final Batch Loss: 0.05521603673696518\n",
      "Epoch 4487, Loss: 0.1558079570531845, Final Batch Loss: 0.07747592031955719\n",
      "Epoch 4488, Loss: 0.15759268403053284, Final Batch Loss: 0.09694232791662216\n",
      "Epoch 4489, Loss: 0.197505421936512, Final Batch Loss: 0.04342556744813919\n",
      "Epoch 4490, Loss: 0.16152160987257957, Final Batch Loss: 0.05915720388293266\n",
      "Epoch 4491, Loss: 0.1812572330236435, Final Batch Loss: 0.09627454727888107\n",
      "Epoch 4492, Loss: 0.12057527899742126, Final Batch Loss: 0.06051519140601158\n",
      "Epoch 4493, Loss: 0.11633806675672531, Final Batch Loss: 0.06481107324361801\n",
      "Epoch 4494, Loss: 0.10156631469726562, Final Batch Loss: 0.04488654062151909\n",
      "Epoch 4495, Loss: 0.1586637683212757, Final Batch Loss: 0.09960877895355225\n",
      "Epoch 4496, Loss: 0.09549229592084885, Final Batch Loss: 0.02312939614057541\n",
      "Epoch 4497, Loss: 0.1287120170891285, Final Batch Loss: 0.05920608714222908\n",
      "Epoch 4498, Loss: 0.1305767558515072, Final Batch Loss: 0.0685691311955452\n",
      "Epoch 4499, Loss: 0.1327919065952301, Final Batch Loss: 0.07277096807956696\n",
      "Epoch 4500, Loss: 0.14161739870905876, Final Batch Loss: 0.08598805218935013\n",
      "Epoch 4501, Loss: 0.13171325623989105, Final Batch Loss: 0.08229527622461319\n",
      "Epoch 4502, Loss: 0.11548381671309471, Final Batch Loss: 0.06318436563014984\n",
      "Epoch 4503, Loss: 0.08373329788446426, Final Batch Loss: 0.053861916065216064\n",
      "Epoch 4504, Loss: 0.11359363794326782, Final Batch Loss: 0.06542443484067917\n",
      "Epoch 4505, Loss: 0.13791221007704735, Final Batch Loss: 0.0912916362285614\n",
      "Epoch 4506, Loss: 0.08739707246422768, Final Batch Loss: 0.03529861196875572\n",
      "Epoch 4507, Loss: 0.14982178807258606, Final Batch Loss: 0.08422211557626724\n",
      "Epoch 4508, Loss: 0.1091137919574976, Final Batch Loss: 0.025659779086709023\n",
      "Epoch 4509, Loss: 0.12653998285531998, Final Batch Loss: 0.06491284817457199\n",
      "Epoch 4510, Loss: 0.1637927070260048, Final Batch Loss: 0.06013021618127823\n",
      "Epoch 4511, Loss: 0.09326637908816338, Final Batch Loss: 0.035524602979421616\n",
      "Epoch 4512, Loss: 0.13884882628917694, Final Batch Loss: 0.06852997094392776\n",
      "Epoch 4513, Loss: 0.10371166467666626, Final Batch Loss: 0.04627693071961403\n",
      "Epoch 4514, Loss: 0.07186049968004227, Final Batch Loss: 0.029429636895656586\n",
      "Epoch 4515, Loss: 0.14107394963502884, Final Batch Loss: 0.04511415213346481\n",
      "Epoch 4516, Loss: 0.198092520236969, Final Batch Loss: 0.08823300898075104\n",
      "Epoch 4517, Loss: 0.15904784947633743, Final Batch Loss: 0.07025890052318573\n",
      "Epoch 4518, Loss: 0.1715354472398758, Final Batch Loss: 0.10039505362510681\n",
      "Epoch 4519, Loss: 0.09528060257434845, Final Batch Loss: 0.053182534873485565\n",
      "Epoch 4520, Loss: 0.15546267479658127, Final Batch Loss: 0.08089333027601242\n",
      "Epoch 4521, Loss: 0.13735803589224815, Final Batch Loss: 0.08370278775691986\n",
      "Epoch 4522, Loss: 0.12889927253127098, Final Batch Loss: 0.07519693672657013\n",
      "Epoch 4523, Loss: 0.09974298812448978, Final Batch Loss: 0.0707930475473404\n",
      "Epoch 4524, Loss: 0.08623610436916351, Final Batch Loss: 0.036764271557331085\n",
      "Epoch 4525, Loss: 0.1086467020213604, Final Batch Loss: 0.051718808710575104\n",
      "Epoch 4526, Loss: 0.15808507055044174, Final Batch Loss: 0.09013497084379196\n",
      "Epoch 4527, Loss: 0.12442895770072937, Final Batch Loss: 0.06386742740869522\n",
      "Epoch 4528, Loss: 0.16287974268198013, Final Batch Loss: 0.08349734544754028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4529, Loss: 0.13483154401183128, Final Batch Loss: 0.07579442858695984\n",
      "Epoch 4530, Loss: 0.12805141881108284, Final Batch Loss: 0.08050937205553055\n",
      "Epoch 4531, Loss: 0.14644381776452065, Final Batch Loss: 0.08424212038516998\n",
      "Epoch 4532, Loss: 0.13631830364465714, Final Batch Loss: 0.08257730305194855\n",
      "Epoch 4533, Loss: 0.11885594949126244, Final Batch Loss: 0.06859140843153\n",
      "Epoch 4534, Loss: 0.13387836515903473, Final Batch Loss: 0.08387482166290283\n",
      "Epoch 4535, Loss: 0.10020343214273453, Final Batch Loss: 0.05672154575586319\n",
      "Epoch 4536, Loss: 0.18393444269895554, Final Batch Loss: 0.09243210405111313\n",
      "Epoch 4537, Loss: 0.11783010885119438, Final Batch Loss: 0.0481075681746006\n",
      "Epoch 4538, Loss: 0.16371850669384003, Final Batch Loss: 0.05665694177150726\n",
      "Epoch 4539, Loss: 0.18955054879188538, Final Batch Loss: 0.0675576776266098\n",
      "Epoch 4540, Loss: 0.09941750764846802, Final Batch Loss: 0.04616176709532738\n",
      "Epoch 4541, Loss: 0.13583776354789734, Final Batch Loss: 0.08152443170547485\n",
      "Epoch 4542, Loss: 0.16210004687309265, Final Batch Loss: 0.09397865831851959\n",
      "Epoch 4543, Loss: 0.11851716786623001, Final Batch Loss: 0.031024448573589325\n",
      "Epoch 4544, Loss: 0.105790164321661, Final Batch Loss: 0.0513186901807785\n",
      "Epoch 4545, Loss: 0.17213697731494904, Final Batch Loss: 0.09472545981407166\n",
      "Epoch 4546, Loss: 0.14585205912590027, Final Batch Loss: 0.07753054797649384\n",
      "Epoch 4547, Loss: 0.13322540000081062, Final Batch Loss: 0.07983383536338806\n",
      "Epoch 4548, Loss: 0.10880489274859428, Final Batch Loss: 0.0511704757809639\n",
      "Epoch 4549, Loss: 0.1358308084309101, Final Batch Loss: 0.05841968581080437\n",
      "Epoch 4550, Loss: 0.09024494513869286, Final Batch Loss: 0.04007301107048988\n",
      "Epoch 4551, Loss: 0.11694258078932762, Final Batch Loss: 0.03345252200961113\n",
      "Epoch 4552, Loss: 0.0891859345138073, Final Batch Loss: 0.04759994149208069\n",
      "Epoch 4553, Loss: 0.13312844187021255, Final Batch Loss: 0.06738366186618805\n",
      "Epoch 4554, Loss: 0.12685666605830193, Final Batch Loss: 0.0716453567147255\n",
      "Epoch 4555, Loss: 0.22206304967403412, Final Batch Loss: 0.12320435047149658\n",
      "Epoch 4556, Loss: 0.17543140798807144, Final Batch Loss: 0.10106682777404785\n",
      "Epoch 4557, Loss: 0.1400996297597885, Final Batch Loss: 0.05753234773874283\n",
      "Epoch 4558, Loss: 0.1346411183476448, Final Batch Loss: 0.07184173166751862\n",
      "Epoch 4559, Loss: 0.22412237524986267, Final Batch Loss: 0.10249441862106323\n",
      "Epoch 4560, Loss: 0.14057183265686035, Final Batch Loss: 0.05792227387428284\n",
      "Epoch 4561, Loss: 0.11043465882539749, Final Batch Loss: 0.05128281190991402\n",
      "Epoch 4562, Loss: 0.13711118698120117, Final Batch Loss: 0.058628737926483154\n",
      "Epoch 4563, Loss: 0.1341456063091755, Final Batch Loss: 0.07322656363248825\n",
      "Epoch 4564, Loss: 0.11377643048763275, Final Batch Loss: 0.07902460545301437\n",
      "Epoch 4565, Loss: 0.16592901572585106, Final Batch Loss: 0.05882624164223671\n",
      "Epoch 4566, Loss: 0.12856796011328697, Final Batch Loss: 0.07273288816213608\n",
      "Epoch 4567, Loss: 0.1742943935096264, Final Batch Loss: 0.05539899691939354\n",
      "Epoch 4568, Loss: 0.12311050295829773, Final Batch Loss: 0.05607294291257858\n",
      "Epoch 4569, Loss: 0.08043064922094345, Final Batch Loss: 0.037681788206100464\n",
      "Epoch 4570, Loss: 0.23751042783260345, Final Batch Loss: 0.06368578970432281\n",
      "Epoch 4571, Loss: 0.1439010053873062, Final Batch Loss: 0.07001844048500061\n",
      "Epoch 4572, Loss: 0.09396392479538918, Final Batch Loss: 0.05109461024403572\n",
      "Epoch 4573, Loss: 0.12225661799311638, Final Batch Loss: 0.06170658767223358\n",
      "Epoch 4574, Loss: 0.12592970207333565, Final Batch Loss: 0.06179449334740639\n",
      "Epoch 4575, Loss: 0.11826887354254723, Final Batch Loss: 0.041938576847314835\n",
      "Epoch 4576, Loss: 0.13999876007437706, Final Batch Loss: 0.08456537127494812\n",
      "Epoch 4577, Loss: 0.11331754177808762, Final Batch Loss: 0.06282147765159607\n",
      "Epoch 4578, Loss: 0.1473786048591137, Final Batch Loss: 0.08757984638214111\n",
      "Epoch 4579, Loss: 0.1877579465508461, Final Batch Loss: 0.13336239755153656\n",
      "Epoch 4580, Loss: 0.1547788605093956, Final Batch Loss: 0.09923183917999268\n",
      "Epoch 4581, Loss: 0.17841386795043945, Final Batch Loss: 0.08034371584653854\n",
      "Epoch 4582, Loss: 0.10247058793902397, Final Batch Loss: 0.06517976522445679\n",
      "Epoch 4583, Loss: 0.08205699548125267, Final Batch Loss: 0.04180561751127243\n",
      "Epoch 4584, Loss: 0.12107507884502411, Final Batch Loss: 0.058261439204216\n",
      "Epoch 4585, Loss: 0.11574889346957207, Final Batch Loss: 0.038591522723436356\n",
      "Epoch 4586, Loss: 0.14520704001188278, Final Batch Loss: 0.09016676247119904\n",
      "Epoch 4587, Loss: 0.1622878648340702, Final Batch Loss: 0.05232244357466698\n",
      "Epoch 4588, Loss: 0.1392425335943699, Final Batch Loss: 0.08691208809614182\n",
      "Epoch 4589, Loss: 0.13211122155189514, Final Batch Loss: 0.060140423476696014\n",
      "Epoch 4590, Loss: 0.17809488624334335, Final Batch Loss: 0.08259408921003342\n",
      "Epoch 4591, Loss: 0.16431600227952003, Final Batch Loss: 0.12047050893306732\n",
      "Epoch 4592, Loss: 0.10982422530651093, Final Batch Loss: 0.066712386906147\n",
      "Epoch 4593, Loss: 0.11539820954203606, Final Batch Loss: 0.05526289716362953\n",
      "Epoch 4594, Loss: 0.15423058718442917, Final Batch Loss: 0.09511919319629669\n",
      "Epoch 4595, Loss: 0.13727685436606407, Final Batch Loss: 0.05575479939579964\n",
      "Epoch 4596, Loss: 0.1701764240860939, Final Batch Loss: 0.049847424030303955\n",
      "Epoch 4597, Loss: 0.15601114183664322, Final Batch Loss: 0.08218957483768463\n",
      "Epoch 4598, Loss: 0.14640291780233383, Final Batch Loss: 0.0753980204463005\n",
      "Epoch 4599, Loss: 0.10573258623480797, Final Batch Loss: 0.057831328362226486\n",
      "Epoch 4600, Loss: 0.14339711517095566, Final Batch Loss: 0.06886447221040726\n",
      "Epoch 4601, Loss: 0.11739887297153473, Final Batch Loss: 0.06606542319059372\n",
      "Epoch 4602, Loss: 0.0751016866415739, Final Batch Loss: 0.02644704468548298\n",
      "Epoch 4603, Loss: 0.11858705058693886, Final Batch Loss: 0.04497121647000313\n",
      "Epoch 4604, Loss: 0.10615229606628418, Final Batch Loss: 0.04858862981200218\n",
      "Epoch 4605, Loss: 0.1097254678606987, Final Batch Loss: 0.04926512390375137\n",
      "Epoch 4606, Loss: 0.1426747962832451, Final Batch Loss: 0.07665054500102997\n",
      "Epoch 4607, Loss: 0.15609696507453918, Final Batch Loss: 0.09265205264091492\n",
      "Epoch 4608, Loss: 0.0903855487704277, Final Batch Loss: 0.03599841520190239\n",
      "Epoch 4609, Loss: 0.1409129686653614, Final Batch Loss: 0.08231235295534134\n",
      "Epoch 4610, Loss: 0.11657959222793579, Final Batch Loss: 0.07608829438686371\n",
      "Epoch 4611, Loss: 0.14975911378860474, Final Batch Loss: 0.0853172019124031\n",
      "Epoch 4612, Loss: 0.13247983902692795, Final Batch Loss: 0.0663742646574974\n",
      "Epoch 4613, Loss: 0.10141885653138161, Final Batch Loss: 0.04233122989535332\n",
      "Epoch 4614, Loss: 0.13125377893447876, Final Batch Loss: 0.06532789021730423\n",
      "Epoch 4615, Loss: 0.07177506014704704, Final Batch Loss: 0.03795338049530983\n",
      "Epoch 4616, Loss: 0.11586596816778183, Final Batch Loss: 0.0645856112241745\n",
      "Epoch 4617, Loss: 0.10566994547843933, Final Batch Loss: 0.05300455167889595\n",
      "Epoch 4618, Loss: 0.1502174474298954, Final Batch Loss: 0.06215544417500496\n",
      "Epoch 4619, Loss: 0.17507892847061157, Final Batch Loss: 0.10587245225906372\n",
      "Epoch 4620, Loss: 0.15494568645954132, Final Batch Loss: 0.0714106410741806\n",
      "Epoch 4621, Loss: 0.07050500996410847, Final Batch Loss: 0.04081254452466965\n",
      "Epoch 4622, Loss: 0.10763026773929596, Final Batch Loss: 0.06192583218216896\n",
      "Epoch 4623, Loss: 0.15534311532974243, Final Batch Loss: 0.10378304868936539\n",
      "Epoch 4624, Loss: 0.14464887976646423, Final Batch Loss: 0.0407070592045784\n",
      "Epoch 4625, Loss: 0.14046109095215797, Final Batch Loss: 0.04833901301026344\n",
      "Epoch 4626, Loss: 0.0923951007425785, Final Batch Loss: 0.057425230741500854\n",
      "Epoch 4627, Loss: 0.10674156993627548, Final Batch Loss: 0.06681700795888901\n",
      "Epoch 4628, Loss: 0.1987266167998314, Final Batch Loss: 0.10231838375329971\n",
      "Epoch 4629, Loss: 0.16488274931907654, Final Batch Loss: 0.089411161839962\n",
      "Epoch 4630, Loss: 0.1316266693174839, Final Batch Loss: 0.044307682663202286\n",
      "Epoch 4631, Loss: 0.10822413116693497, Final Batch Loss: 0.045027077198028564\n",
      "Epoch 4632, Loss: 0.13218355178833008, Final Batch Loss: 0.06626015901565552\n",
      "Epoch 4633, Loss: 0.1523151397705078, Final Batch Loss: 0.057930029928684235\n",
      "Epoch 4634, Loss: 0.113881915807724, Final Batch Loss: 0.038860492408275604\n",
      "Epoch 4635, Loss: 0.15707673132419586, Final Batch Loss: 0.06904477626085281\n",
      "Epoch 4636, Loss: 0.13322358950972557, Final Batch Loss: 0.05286988243460655\n",
      "Epoch 4637, Loss: 0.16777395457029343, Final Batch Loss: 0.06020741164684296\n",
      "Epoch 4638, Loss: 0.12660394236445427, Final Batch Loss: 0.0660780668258667\n",
      "Epoch 4639, Loss: 0.11106431111693382, Final Batch Loss: 0.06080165505409241\n",
      "Epoch 4640, Loss: 0.06711158342659473, Final Batch Loss: 0.02703113667666912\n",
      "Epoch 4641, Loss: 0.16603178530931473, Final Batch Loss: 0.08272659033536911\n",
      "Epoch 4642, Loss: 0.18662229180335999, Final Batch Loss: 0.14696349203586578\n",
      "Epoch 4643, Loss: 0.18042928725481033, Final Batch Loss: 0.07188510149717331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4644, Loss: 0.09400278329849243, Final Batch Loss: 0.04936987906694412\n",
      "Epoch 4645, Loss: 0.13532926887273788, Final Batch Loss: 0.06064945459365845\n",
      "Epoch 4646, Loss: 0.1240498535335064, Final Batch Loss: 0.0630328431725502\n",
      "Epoch 4647, Loss: 0.1348361000418663, Final Batch Loss: 0.0432954803109169\n",
      "Epoch 4648, Loss: 0.16613999754190445, Final Batch Loss: 0.08598588407039642\n",
      "Epoch 4649, Loss: 0.09510697796940804, Final Batch Loss: 0.05455735698342323\n",
      "Epoch 4650, Loss: 0.10416391864418983, Final Batch Loss: 0.038030724972486496\n",
      "Epoch 4651, Loss: 0.13451865315437317, Final Batch Loss: 0.06042011082172394\n",
      "Epoch 4652, Loss: 0.11054152995347977, Final Batch Loss: 0.043644070625305176\n",
      "Epoch 4653, Loss: 0.10965598747134209, Final Batch Loss: 0.056436218321323395\n",
      "Epoch 4654, Loss: 0.09706661105155945, Final Batch Loss: 0.059950657188892365\n",
      "Epoch 4655, Loss: 0.10431493446230888, Final Batch Loss: 0.050308555364608765\n",
      "Epoch 4656, Loss: 0.17980553954839706, Final Batch Loss: 0.10355529934167862\n",
      "Epoch 4657, Loss: 0.13123856112360954, Final Batch Loss: 0.08751458674669266\n",
      "Epoch 4658, Loss: 0.16194574907422066, Final Batch Loss: 0.11344302445650101\n",
      "Epoch 4659, Loss: 0.1266910694539547, Final Batch Loss: 0.06844210624694824\n",
      "Epoch 4660, Loss: 0.13373347371816635, Final Batch Loss: 0.08013790845870972\n",
      "Epoch 4661, Loss: 0.10339569300413132, Final Batch Loss: 0.04793025553226471\n",
      "Epoch 4662, Loss: 0.0846707820892334, Final Batch Loss: 0.04145840182900429\n",
      "Epoch 4663, Loss: 0.14678475633263588, Final Batch Loss: 0.08556675910949707\n",
      "Epoch 4664, Loss: 0.12281795591115952, Final Batch Loss: 0.061851274222135544\n",
      "Epoch 4665, Loss: 0.05966305173933506, Final Batch Loss: 0.023842284455895424\n",
      "Epoch 4666, Loss: 0.11249177530407906, Final Batch Loss: 0.05625344067811966\n",
      "Epoch 4667, Loss: 0.15938758850097656, Final Batch Loss: 0.08439000695943832\n",
      "Epoch 4668, Loss: 0.10423040017485619, Final Batch Loss: 0.07129368185997009\n",
      "Epoch 4669, Loss: 0.12982597202062607, Final Batch Loss: 0.08641508221626282\n",
      "Epoch 4670, Loss: 0.1803632155060768, Final Batch Loss: 0.11748377233743668\n",
      "Epoch 4671, Loss: 0.1379675418138504, Final Batch Loss: 0.07471273839473724\n",
      "Epoch 4672, Loss: 0.13865316286683083, Final Batch Loss: 0.05465729907155037\n",
      "Epoch 4673, Loss: 0.08197730779647827, Final Batch Loss: 0.04934665933251381\n",
      "Epoch 4674, Loss: 0.19011911749839783, Final Batch Loss: 0.06563473492860794\n",
      "Epoch 4675, Loss: 0.1561451144516468, Final Batch Loss: 0.0975307896733284\n",
      "Epoch 4676, Loss: 0.1421690210700035, Final Batch Loss: 0.0763665959239006\n",
      "Epoch 4677, Loss: 0.14019529148936272, Final Batch Loss: 0.08842319995164871\n",
      "Epoch 4678, Loss: 0.10122890770435333, Final Batch Loss: 0.03782103955745697\n",
      "Epoch 4679, Loss: 0.10154133662581444, Final Batch Loss: 0.04969015344977379\n",
      "Epoch 4680, Loss: 0.1708194687962532, Final Batch Loss: 0.10800312459468842\n",
      "Epoch 4681, Loss: 0.10911109298467636, Final Batch Loss: 0.06434110552072525\n",
      "Epoch 4682, Loss: 0.08060777187347412, Final Batch Loss: 0.047743625938892365\n",
      "Epoch 4683, Loss: 0.134676244109869, Final Batch Loss: 0.060553211718797684\n",
      "Epoch 4684, Loss: 0.1298314332962036, Final Batch Loss: 0.06151276081800461\n",
      "Epoch 4685, Loss: 0.13513236865401268, Final Batch Loss: 0.05212574824690819\n",
      "Epoch 4686, Loss: 0.1699172705411911, Final Batch Loss: 0.06617039442062378\n",
      "Epoch 4687, Loss: 0.18166905641555786, Final Batch Loss: 0.10112293064594269\n",
      "Epoch 4688, Loss: 0.14638305827975273, Final Batch Loss: 0.05091813579201698\n",
      "Epoch 4689, Loss: 0.14957762509584427, Final Batch Loss: 0.06940679997205734\n",
      "Epoch 4690, Loss: 0.1239958107471466, Final Batch Loss: 0.049200788140296936\n",
      "Epoch 4691, Loss: 0.11472625285387039, Final Batch Loss: 0.06266118586063385\n",
      "Epoch 4692, Loss: 0.10182126611471176, Final Batch Loss: 0.02418595552444458\n",
      "Epoch 4693, Loss: 0.12122363224625587, Final Batch Loss: 0.060108184814453125\n",
      "Epoch 4694, Loss: 0.10610142722725868, Final Batch Loss: 0.0676393210887909\n",
      "Epoch 4695, Loss: 0.18088895082473755, Final Batch Loss: 0.08622664958238602\n",
      "Epoch 4696, Loss: 0.09845528937876225, Final Batch Loss: 0.026488566771149635\n",
      "Epoch 4697, Loss: 0.19124453514814377, Final Batch Loss: 0.09402191638946533\n",
      "Epoch 4698, Loss: 0.18954551964998245, Final Batch Loss: 0.09106887131929398\n",
      "Epoch 4699, Loss: 0.10716602951288223, Final Batch Loss: 0.0660599023103714\n",
      "Epoch 4700, Loss: 0.13535217940807343, Final Batch Loss: 0.06568553298711777\n",
      "Epoch 4701, Loss: 0.13433262705802917, Final Batch Loss: 0.06686761230230331\n",
      "Epoch 4702, Loss: 0.17046740651130676, Final Batch Loss: 0.061285704374313354\n",
      "Epoch 4703, Loss: 0.09732666984200478, Final Batch Loss: 0.07001479715108871\n",
      "Epoch 4704, Loss: 0.14653022587299347, Final Batch Loss: 0.05915366858243942\n",
      "Epoch 4705, Loss: 0.15725992619991302, Final Batch Loss: 0.07985953241586685\n",
      "Epoch 4706, Loss: 0.15140486136078835, Final Batch Loss: 0.09012261778116226\n",
      "Epoch 4707, Loss: 0.10904157906770706, Final Batch Loss: 0.055809903889894485\n",
      "Epoch 4708, Loss: 0.11400523409247398, Final Batch Loss: 0.067615807056427\n",
      "Epoch 4709, Loss: 0.15939296036958694, Final Batch Loss: 0.0887054055929184\n",
      "Epoch 4710, Loss: 0.14089230820536613, Final Batch Loss: 0.0397527702152729\n",
      "Epoch 4711, Loss: 0.11615531519055367, Final Batch Loss: 0.062168169766664505\n",
      "Epoch 4712, Loss: 0.18485993146896362, Final Batch Loss: 0.09102088212966919\n",
      "Epoch 4713, Loss: 0.16385987401008606, Final Batch Loss: 0.06167960911989212\n",
      "Epoch 4714, Loss: 0.13367959856987, Final Batch Loss: 0.048268288373947144\n",
      "Epoch 4715, Loss: 0.11076000705361366, Final Batch Loss: 0.05160204693675041\n",
      "Epoch 4716, Loss: 0.15314556285738945, Final Batch Loss: 0.11010454595088959\n",
      "Epoch 4717, Loss: 0.1154509000480175, Final Batch Loss: 0.05736563354730606\n",
      "Epoch 4718, Loss: 0.14196433126926422, Final Batch Loss: 0.08103390783071518\n",
      "Epoch 4719, Loss: 0.15052186325192451, Final Batch Loss: 0.09510470181703568\n",
      "Epoch 4720, Loss: 0.12694347277283669, Final Batch Loss: 0.07217416167259216\n",
      "Epoch 4721, Loss: 0.12788110971450806, Final Batch Loss: 0.0702451840043068\n",
      "Epoch 4722, Loss: 0.13291077315807343, Final Batch Loss: 0.08703181147575378\n",
      "Epoch 4723, Loss: 0.17417822033166885, Final Batch Loss: 0.11244595050811768\n",
      "Epoch 4724, Loss: 0.13038834929466248, Final Batch Loss: 0.048675619065761566\n",
      "Epoch 4725, Loss: 0.1248355470597744, Final Batch Loss: 0.07321900874376297\n",
      "Epoch 4726, Loss: 0.12083330750465393, Final Batch Loss: 0.05995025485754013\n",
      "Epoch 4727, Loss: 0.1366986148059368, Final Batch Loss: 0.08676714450120926\n",
      "Epoch 4728, Loss: 0.1453835517168045, Final Batch Loss: 0.08904705941677094\n",
      "Epoch 4729, Loss: 0.11680489405989647, Final Batch Loss: 0.060436613857746124\n",
      "Epoch 4730, Loss: 0.12449989467859268, Final Batch Loss: 0.06805732101202011\n",
      "Epoch 4731, Loss: 0.12981390953063965, Final Batch Loss: 0.05936528742313385\n",
      "Epoch 4732, Loss: 0.11221758276224136, Final Batch Loss: 0.043748460710048676\n",
      "Epoch 4733, Loss: 0.10285306721925735, Final Batch Loss: 0.05034980550408363\n",
      "Epoch 4734, Loss: 0.09690011665225029, Final Batch Loss: 0.04534820467233658\n",
      "Epoch 4735, Loss: 0.14100299403071404, Final Batch Loss: 0.05461480841040611\n",
      "Epoch 4736, Loss: 0.21242991089820862, Final Batch Loss: 0.09178528189659119\n",
      "Epoch 4737, Loss: 0.16120167821645737, Final Batch Loss: 0.06349965929985046\n",
      "Epoch 4738, Loss: 0.18020879477262497, Final Batch Loss: 0.058475010097026825\n",
      "Epoch 4739, Loss: 0.15176637470722198, Final Batch Loss: 0.05320972204208374\n",
      "Epoch 4740, Loss: 0.1729370653629303, Final Batch Loss: 0.08438362926244736\n",
      "Epoch 4741, Loss: 0.0881010964512825, Final Batch Loss: 0.052031368017196655\n",
      "Epoch 4742, Loss: 0.19595711678266525, Final Batch Loss: 0.0708695575594902\n",
      "Epoch 4743, Loss: 0.09584718570113182, Final Batch Loss: 0.04826362431049347\n",
      "Epoch 4744, Loss: 0.09102281928062439, Final Batch Loss: 0.041433632373809814\n",
      "Epoch 4745, Loss: 0.07666932046413422, Final Batch Loss: 0.04976537078619003\n",
      "Epoch 4746, Loss: 0.09212647005915642, Final Batch Loss: 0.050970207899808884\n",
      "Epoch 4747, Loss: 0.12246638163924217, Final Batch Loss: 0.044762250036001205\n",
      "Epoch 4748, Loss: 0.23863042891025543, Final Batch Loss: 0.08603350818157196\n",
      "Epoch 4749, Loss: 0.16146494448184967, Final Batch Loss: 0.08960993587970734\n",
      "Epoch 4750, Loss: 0.0912855677306652, Final Batch Loss: 0.04608829692006111\n",
      "Epoch 4751, Loss: 0.11817568726837635, Final Batch Loss: 0.08749673515558243\n",
      "Epoch 4752, Loss: 0.192330963909626, Final Batch Loss: 0.10196326673030853\n",
      "Epoch 4753, Loss: 0.16804670542478561, Final Batch Loss: 0.0792572870850563\n",
      "Epoch 4754, Loss: 0.11050640791654587, Final Batch Loss: 0.046499624848365784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4755, Loss: 0.10263919830322266, Final Batch Loss: 0.05932891368865967\n",
      "Epoch 4756, Loss: 0.13700227066874504, Final Batch Loss: 0.07648409157991409\n",
      "Epoch 4757, Loss: 0.0776349138468504, Final Batch Loss: 0.05451839044690132\n",
      "Epoch 4758, Loss: 0.14625797793269157, Final Batch Loss: 0.04895472154021263\n",
      "Epoch 4759, Loss: 0.09771542623639107, Final Batch Loss: 0.055487100034952164\n",
      "Epoch 4760, Loss: 0.10458711162209511, Final Batch Loss: 0.05832797661423683\n",
      "Epoch 4761, Loss: 0.1455635391175747, Final Batch Loss: 0.05639495328068733\n",
      "Epoch 4762, Loss: 0.11982619017362595, Final Batch Loss: 0.08385822176933289\n",
      "Epoch 4763, Loss: 0.13226095587015152, Final Batch Loss: 0.06649895757436752\n",
      "Epoch 4764, Loss: 0.10910915210843086, Final Batch Loss: 0.05383291840553284\n",
      "Epoch 4765, Loss: 0.1168702095746994, Final Batch Loss: 0.050903476774692535\n",
      "Epoch 4766, Loss: 0.14261089637875557, Final Batch Loss: 0.08013202250003815\n",
      "Epoch 4767, Loss: 0.1574331745505333, Final Batch Loss: 0.10883758217096329\n",
      "Epoch 4768, Loss: 0.1349601373076439, Final Batch Loss: 0.08948121219873428\n",
      "Epoch 4769, Loss: 0.15095370262861252, Final Batch Loss: 0.07095298916101456\n",
      "Epoch 4770, Loss: 0.11650659888982773, Final Batch Loss: 0.0821029469370842\n",
      "Epoch 4771, Loss: 0.1199692003428936, Final Batch Loss: 0.06003997102379799\n",
      "Epoch 4772, Loss: 0.12992627173662186, Final Batch Loss: 0.0582241490483284\n",
      "Epoch 4773, Loss: 0.1408851183950901, Final Batch Loss: 0.07933331280946732\n",
      "Epoch 4774, Loss: 0.11114120483398438, Final Batch Loss: 0.052835773676633835\n",
      "Epoch 4775, Loss: 0.16008960455656052, Final Batch Loss: 0.08258271217346191\n",
      "Epoch 4776, Loss: 0.16911442577838898, Final Batch Loss: 0.08445841073989868\n",
      "Epoch 4777, Loss: 0.08532477170228958, Final Batch Loss: 0.034023892134428024\n",
      "Epoch 4778, Loss: 0.1435265690088272, Final Batch Loss: 0.06846483796834946\n",
      "Epoch 4779, Loss: 0.09930014982819557, Final Batch Loss: 0.040803249925374985\n",
      "Epoch 4780, Loss: 0.1228158213198185, Final Batch Loss: 0.054433587938547134\n",
      "Epoch 4781, Loss: 0.1397487297654152, Final Batch Loss: 0.06397424638271332\n",
      "Epoch 4782, Loss: 0.11660581454634666, Final Batch Loss: 0.05987346172332764\n",
      "Epoch 4783, Loss: 0.17277687788009644, Final Batch Loss: 0.07429075986146927\n",
      "Epoch 4784, Loss: 0.11727182567119598, Final Batch Loss: 0.06587537378072739\n",
      "Epoch 4785, Loss: 0.16477583348751068, Final Batch Loss: 0.1087372750043869\n",
      "Epoch 4786, Loss: 0.11156048253178596, Final Batch Loss: 0.06654193997383118\n",
      "Epoch 4787, Loss: 0.06277964822947979, Final Batch Loss: 0.03978361189365387\n",
      "Epoch 4788, Loss: 0.10972559079527855, Final Batch Loss: 0.06964293867349625\n",
      "Epoch 4789, Loss: 0.09733666107058525, Final Batch Loss: 0.02506181225180626\n",
      "Epoch 4790, Loss: 0.11389778554439545, Final Batch Loss: 0.06335332244634628\n",
      "Epoch 4791, Loss: 0.15120867267251015, Final Batch Loss: 0.051675695925951004\n",
      "Epoch 4792, Loss: 0.1522277407348156, Final Batch Loss: 0.10329093039035797\n",
      "Epoch 4793, Loss: 0.20857814326882362, Final Batch Loss: 0.1549793928861618\n",
      "Epoch 4794, Loss: 0.10205529257655144, Final Batch Loss: 0.05465732514858246\n",
      "Epoch 4795, Loss: 0.12591272220015526, Final Batch Loss: 0.05489585921168327\n",
      "Epoch 4796, Loss: 0.08754974976181984, Final Batch Loss: 0.05301831290125847\n",
      "Epoch 4797, Loss: 0.07225705496966839, Final Batch Loss: 0.043899115175008774\n",
      "Epoch 4798, Loss: 0.13349392637610435, Final Batch Loss: 0.07920584827661514\n",
      "Epoch 4799, Loss: 0.14250297844409943, Final Batch Loss: 0.07607679814100266\n",
      "Epoch 4800, Loss: 0.16659941524267197, Final Batch Loss: 0.08061700314283371\n",
      "Epoch 4801, Loss: 0.0869678407907486, Final Batch Loss: 0.03188958391547203\n",
      "Epoch 4802, Loss: 0.14822221547365189, Final Batch Loss: 0.07518181204795837\n",
      "Epoch 4803, Loss: 0.1324743926525116, Final Batch Loss: 0.06679839640855789\n",
      "Epoch 4804, Loss: 0.11722993850708008, Final Batch Loss: 0.058252036571502686\n",
      "Epoch 4805, Loss: 0.0987870953977108, Final Batch Loss: 0.045934874564409256\n",
      "Epoch 4806, Loss: 0.13082723319530487, Final Batch Loss: 0.07348892092704773\n",
      "Epoch 4807, Loss: 0.08515945076942444, Final Batch Loss: 0.03815797343850136\n",
      "Epoch 4808, Loss: 0.12965430319309235, Final Batch Loss: 0.06309445202350616\n",
      "Epoch 4809, Loss: 0.10634323582053185, Final Batch Loss: 0.056476324796676636\n",
      "Epoch 4810, Loss: 0.15781281888484955, Final Batch Loss: 0.10351596027612686\n",
      "Epoch 4811, Loss: 0.11785142496228218, Final Batch Loss: 0.04911434277892113\n",
      "Epoch 4812, Loss: 0.10685482248663902, Final Batch Loss: 0.0511251837015152\n",
      "Epoch 4813, Loss: 0.11716736480593681, Final Batch Loss: 0.04803784564137459\n",
      "Epoch 4814, Loss: 0.2274974174797535, Final Batch Loss: 0.050326187163591385\n",
      "Epoch 4815, Loss: 0.10806608945131302, Final Batch Loss: 0.041971080005168915\n",
      "Epoch 4816, Loss: 0.10991151630878448, Final Batch Loss: 0.054676737636327744\n",
      "Epoch 4817, Loss: 0.09210218489170074, Final Batch Loss: 0.034051310271024704\n",
      "Epoch 4818, Loss: 0.12667768821120262, Final Batch Loss: 0.0514962263405323\n",
      "Epoch 4819, Loss: 0.1356598138809204, Final Batch Loss: 0.08562888205051422\n",
      "Epoch 4820, Loss: 0.1357724368572235, Final Batch Loss: 0.09084067493677139\n",
      "Epoch 4821, Loss: 0.07803066447377205, Final Batch Loss: 0.05075817555189133\n",
      "Epoch 4822, Loss: 0.099362563341856, Final Batch Loss: 0.07195795327425003\n",
      "Epoch 4823, Loss: 0.11885833740234375, Final Batch Loss: 0.051503151655197144\n",
      "Epoch 4824, Loss: 0.09585870802402496, Final Batch Loss: 0.05337069183588028\n",
      "Epoch 4825, Loss: 0.17372943460941315, Final Batch Loss: 0.045125171542167664\n",
      "Epoch 4826, Loss: 0.10075471550226212, Final Batch Loss: 0.06083294749259949\n",
      "Epoch 4827, Loss: 0.1065247431397438, Final Batch Loss: 0.06189447268843651\n",
      "Epoch 4828, Loss: 0.11157670989632607, Final Batch Loss: 0.061007309705019\n",
      "Epoch 4829, Loss: 0.12079501897096634, Final Batch Loss: 0.08675755560398102\n",
      "Epoch 4830, Loss: 0.13419537991285324, Final Batch Loss: 0.06929844617843628\n",
      "Epoch 4831, Loss: 0.1278533786535263, Final Batch Loss: 0.07374341040849686\n",
      "Epoch 4832, Loss: 0.15484384447336197, Final Batch Loss: 0.056560590863227844\n",
      "Epoch 4833, Loss: 0.16796918213367462, Final Batch Loss: 0.02962583303451538\n",
      "Epoch 4834, Loss: 0.08459477871656418, Final Batch Loss: 0.04478945583105087\n",
      "Epoch 4835, Loss: 0.1528020054101944, Final Batch Loss: 0.09993796050548553\n",
      "Epoch 4836, Loss: 0.10790550708770752, Final Batch Loss: 0.053006257861852646\n",
      "Epoch 4837, Loss: 0.12685613706707954, Final Batch Loss: 0.056859057396650314\n",
      "Epoch 4838, Loss: 0.10904503986239433, Final Batch Loss: 0.04438569024205208\n",
      "Epoch 4839, Loss: 0.07403665408492088, Final Batch Loss: 0.03614785894751549\n",
      "Epoch 4840, Loss: 0.11300817877054214, Final Batch Loss: 0.042740598320961\n",
      "Epoch 4841, Loss: 0.1559765301644802, Final Batch Loss: 0.04816170409321785\n",
      "Epoch 4842, Loss: 0.15633582323789597, Final Batch Loss: 0.07096512615680695\n",
      "Epoch 4843, Loss: 0.09140214323997498, Final Batch Loss: 0.04191553592681885\n",
      "Epoch 4844, Loss: 0.09283354505896568, Final Batch Loss: 0.04289610683917999\n",
      "Epoch 4845, Loss: 0.15571347624063492, Final Batch Loss: 0.08061514049768448\n",
      "Epoch 4846, Loss: 0.07842898368835449, Final Batch Loss: 0.041984591633081436\n",
      "Epoch 4847, Loss: 0.2370646670460701, Final Batch Loss: 0.12919865548610687\n",
      "Epoch 4848, Loss: 0.13419318944215775, Final Batch Loss: 0.06233315169811249\n",
      "Epoch 4849, Loss: 0.12484756857156754, Final Batch Loss: 0.04382173717021942\n",
      "Epoch 4850, Loss: 0.16928540170192719, Final Batch Loss: 0.08187448233366013\n",
      "Epoch 4851, Loss: 0.08759548142552376, Final Batch Loss: 0.05340296030044556\n",
      "Epoch 4852, Loss: 0.16895169764757156, Final Batch Loss: 0.0963713526725769\n",
      "Epoch 4853, Loss: 0.07325802370905876, Final Batch Loss: 0.04698445647954941\n",
      "Epoch 4854, Loss: 0.11005835235118866, Final Batch Loss: 0.043237440288066864\n",
      "Epoch 4855, Loss: 0.11775369197130203, Final Batch Loss: 0.06340005993843079\n",
      "Epoch 4856, Loss: 0.19954529404640198, Final Batch Loss: 0.08510339260101318\n",
      "Epoch 4857, Loss: 0.13457590341567993, Final Batch Loss: 0.07213328778743744\n",
      "Epoch 4858, Loss: 0.12320137768983841, Final Batch Loss: 0.0915173888206482\n",
      "Epoch 4859, Loss: 0.14433645457029343, Final Batch Loss: 0.06500314176082611\n",
      "Epoch 4860, Loss: 0.14392810314893723, Final Batch Loss: 0.07063470035791397\n",
      "Epoch 4861, Loss: 0.18019882589578629, Final Batch Loss: 0.0974651575088501\n",
      "Epoch 4862, Loss: 0.15719323605298996, Final Batch Loss: 0.09057417511940002\n",
      "Epoch 4863, Loss: 0.13281658291816711, Final Batch Loss: 0.04143232852220535\n",
      "Epoch 4864, Loss: 0.09359452500939369, Final Batch Loss: 0.0525548979640007\n",
      "Epoch 4865, Loss: 0.12384385243058205, Final Batch Loss: 0.05013149604201317\n",
      "Epoch 4866, Loss: 0.11664033308625221, Final Batch Loss: 0.07156039774417877\n",
      "Epoch 4867, Loss: 0.17057529836893082, Final Batch Loss: 0.10187087208032608\n",
      "Epoch 4868, Loss: 0.16252915188670158, Final Batch Loss: 0.061960864812135696\n",
      "Epoch 4869, Loss: 0.11126455292105675, Final Batch Loss: 0.06059633567929268\n",
      "Epoch 4870, Loss: 0.11263162270188332, Final Batch Loss: 0.058368999511003494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4871, Loss: 0.16072913259267807, Final Batch Loss: 0.07457903027534485\n",
      "Epoch 4872, Loss: 0.11051236093044281, Final Batch Loss: 0.03611409664154053\n",
      "Epoch 4873, Loss: 0.07866564020514488, Final Batch Loss: 0.03608334809541702\n",
      "Epoch 4874, Loss: 0.1271159127354622, Final Batch Loss: 0.08436872810125351\n",
      "Epoch 4875, Loss: 0.1145530454814434, Final Batch Loss: 0.06910238415002823\n",
      "Epoch 4876, Loss: 0.19749878346920013, Final Batch Loss: 0.061747729778289795\n",
      "Epoch 4877, Loss: 0.1164025217294693, Final Batch Loss: 0.05958864465355873\n",
      "Epoch 4878, Loss: 0.14302335679531097, Final Batch Loss: 0.054524146020412445\n",
      "Epoch 4879, Loss: 0.15578358620405197, Final Batch Loss: 0.09798116981983185\n",
      "Epoch 4880, Loss: 0.1275239996612072, Final Batch Loss: 0.08095454424619675\n",
      "Epoch 4881, Loss: 0.13412423431873322, Final Batch Loss: 0.04992229491472244\n",
      "Epoch 4882, Loss: 0.14895852655172348, Final Batch Loss: 0.04605540633201599\n",
      "Epoch 4883, Loss: 0.12574884667992592, Final Batch Loss: 0.08635839819908142\n",
      "Epoch 4884, Loss: 0.1305636130273342, Final Batch Loss: 0.07895869761705399\n",
      "Epoch 4885, Loss: 0.0723979789763689, Final Batch Loss: 0.024235421791672707\n",
      "Epoch 4886, Loss: 0.08333389833569527, Final Batch Loss: 0.032308973371982574\n",
      "Epoch 4887, Loss: 0.09508048743009567, Final Batch Loss: 0.05307532846927643\n",
      "Epoch 4888, Loss: 0.08665533736348152, Final Batch Loss: 0.04561067372560501\n",
      "Epoch 4889, Loss: 0.16751167923212051, Final Batch Loss: 0.07227513939142227\n",
      "Epoch 4890, Loss: 0.1270773820579052, Final Batch Loss: 0.07510431855916977\n",
      "Epoch 4891, Loss: 0.12123461067676544, Final Batch Loss: 0.07049661874771118\n",
      "Epoch 4892, Loss: 0.14424553513526917, Final Batch Loss: 0.07798159867525101\n",
      "Epoch 4893, Loss: 0.13842936977744102, Final Batch Loss: 0.0469050295650959\n",
      "Epoch 4894, Loss: 0.1441844403743744, Final Batch Loss: 0.07583566755056381\n",
      "Epoch 4895, Loss: 0.1646858975291252, Final Batch Loss: 0.09180210530757904\n",
      "Epoch 4896, Loss: 0.18171241134405136, Final Batch Loss: 0.14897045493125916\n",
      "Epoch 4897, Loss: 0.28035780787467957, Final Batch Loss: 0.07418207824230194\n",
      "Epoch 4898, Loss: 0.10442738234996796, Final Batch Loss: 0.06572340428829193\n",
      "Epoch 4899, Loss: 0.12873844802379608, Final Batch Loss: 0.05606845021247864\n",
      "Epoch 4900, Loss: 0.10694729909300804, Final Batch Loss: 0.04523115232586861\n",
      "Epoch 4901, Loss: 0.12072324007749557, Final Batch Loss: 0.05487444996833801\n",
      "Epoch 4902, Loss: 0.1449739634990692, Final Batch Loss: 0.06775092333555222\n",
      "Epoch 4903, Loss: 0.12674270197749138, Final Batch Loss: 0.060010116547346115\n",
      "Epoch 4904, Loss: 0.10731035098433495, Final Batch Loss: 0.05020551010966301\n",
      "Epoch 4905, Loss: 0.11301307007670403, Final Batch Loss: 0.050130296498537064\n",
      "Epoch 4906, Loss: 0.14297904446721077, Final Batch Loss: 0.08316054195165634\n",
      "Epoch 4907, Loss: 0.10259626060724258, Final Batch Loss: 0.052076976746320724\n",
      "Epoch 4908, Loss: 0.12811335921287537, Final Batch Loss: 0.06362628936767578\n",
      "Epoch 4909, Loss: 0.1346752792596817, Final Batch Loss: 0.06649255752563477\n",
      "Epoch 4910, Loss: 0.1544155515730381, Final Batch Loss: 0.032117318361997604\n",
      "Epoch 4911, Loss: 0.11775337532162666, Final Batch Loss: 0.0692114382982254\n",
      "Epoch 4912, Loss: 0.07407288998365402, Final Batch Loss: 0.03773971274495125\n",
      "Epoch 4913, Loss: 0.17653409391641617, Final Batch Loss: 0.10533217340707779\n",
      "Epoch 4914, Loss: 0.10266970470547676, Final Batch Loss: 0.04083845764398575\n",
      "Epoch 4915, Loss: 0.18715230375528336, Final Batch Loss: 0.10089469701051712\n",
      "Epoch 4916, Loss: 0.08551359176635742, Final Batch Loss: 0.032038357108831406\n",
      "Epoch 4917, Loss: 0.13431205227971077, Final Batch Loss: 0.08201061934232712\n",
      "Epoch 4918, Loss: 0.11665863171219826, Final Batch Loss: 0.06932108849287033\n",
      "Epoch 4919, Loss: 0.09834343567490578, Final Batch Loss: 0.033194441348314285\n",
      "Epoch 4920, Loss: 0.1336042657494545, Final Batch Loss: 0.06014905124902725\n",
      "Epoch 4921, Loss: 0.12115585431456566, Final Batch Loss: 0.04936646297574043\n",
      "Epoch 4922, Loss: 0.13554848358035088, Final Batch Loss: 0.05056202784180641\n",
      "Epoch 4923, Loss: 0.13223103433847427, Final Batch Loss: 0.07232581824064255\n",
      "Epoch 4924, Loss: 0.11413989961147308, Final Batch Loss: 0.054047729820013046\n",
      "Epoch 4925, Loss: 0.110056571662426, Final Batch Loss: 0.053653836250305176\n",
      "Epoch 4926, Loss: 0.17692315578460693, Final Batch Loss: 0.0899280309677124\n",
      "Epoch 4927, Loss: 0.07596120610833168, Final Batch Loss: 0.046571649610996246\n",
      "Epoch 4928, Loss: 0.19388435781002045, Final Batch Loss: 0.08595158159732819\n",
      "Epoch 4929, Loss: 0.14153935760259628, Final Batch Loss: 0.07413636893033981\n",
      "Epoch 4930, Loss: 0.11724565550684929, Final Batch Loss: 0.05540243536233902\n",
      "Epoch 4931, Loss: 0.09340580180287361, Final Batch Loss: 0.03889055177569389\n",
      "Epoch 4932, Loss: 0.10046958923339844, Final Batch Loss: 0.04704844206571579\n",
      "Epoch 4933, Loss: 0.17251529544591904, Final Batch Loss: 0.08223825693130493\n",
      "Epoch 4934, Loss: 0.1069810651242733, Final Batch Loss: 0.07093613594770432\n",
      "Epoch 4935, Loss: 0.1336938962340355, Final Batch Loss: 0.08717420697212219\n",
      "Epoch 4936, Loss: 0.15391755849123, Final Batch Loss: 0.07689283788204193\n",
      "Epoch 4937, Loss: 0.08056769147515297, Final Batch Loss: 0.049062613397836685\n",
      "Epoch 4938, Loss: 0.13406965509057045, Final Batch Loss: 0.03890034928917885\n",
      "Epoch 4939, Loss: 0.08751285076141357, Final Batch Loss: 0.05504808947443962\n",
      "Epoch 4940, Loss: 0.16224299371242523, Final Batch Loss: 0.10191687196493149\n",
      "Epoch 4941, Loss: 0.11021403223276138, Final Batch Loss: 0.06529056280851364\n",
      "Epoch 4942, Loss: 0.10513991490006447, Final Batch Loss: 0.04467986151576042\n",
      "Epoch 4943, Loss: 0.10201549343764782, Final Batch Loss: 0.027597768232226372\n",
      "Epoch 4944, Loss: 0.12176654487848282, Final Batch Loss: 0.08152799308300018\n",
      "Epoch 4945, Loss: 0.11247170343995094, Final Batch Loss: 0.0743727907538414\n",
      "Epoch 4946, Loss: 0.15128037333488464, Final Batch Loss: 0.085391566157341\n",
      "Epoch 4947, Loss: 0.13690422475337982, Final Batch Loss: 0.06776702404022217\n",
      "Epoch 4948, Loss: 0.08628681302070618, Final Batch Loss: 0.040218234062194824\n",
      "Epoch 4949, Loss: 0.10055716708302498, Final Batch Loss: 0.06407497823238373\n",
      "Epoch 4950, Loss: 0.11372415721416473, Final Batch Loss: 0.0826626569032669\n",
      "Epoch 4951, Loss: 0.11772274225950241, Final Batch Loss: 0.0652647614479065\n",
      "Epoch 4952, Loss: 0.08527340739965439, Final Batch Loss: 0.0206160768866539\n",
      "Epoch 4953, Loss: 0.06514543294906616, Final Batch Loss: 0.04052850604057312\n",
      "Epoch 4954, Loss: 0.12407805025577545, Final Batch Loss: 0.07222163677215576\n",
      "Epoch 4955, Loss: 0.12459947541356087, Final Batch Loss: 0.06438829004764557\n",
      "Epoch 4956, Loss: 0.12745677307248116, Final Batch Loss: 0.06616951525211334\n",
      "Epoch 4957, Loss: 0.0870603621006012, Final Batch Loss: 0.033509302884340286\n",
      "Epoch 4958, Loss: 0.10275779664516449, Final Batch Loss: 0.058659255504608154\n",
      "Epoch 4959, Loss: 0.14696189761161804, Final Batch Loss: 0.09218405932188034\n",
      "Epoch 4960, Loss: 0.10468955710530281, Final Batch Loss: 0.06102287024259567\n",
      "Epoch 4961, Loss: 0.11399220302700996, Final Batch Loss: 0.045403119176626205\n",
      "Epoch 4962, Loss: 0.11705520749092102, Final Batch Loss: 0.09137444943189621\n",
      "Epoch 4963, Loss: 0.09037305042147636, Final Batch Loss: 0.05251554027199745\n",
      "Epoch 4964, Loss: 0.11381679028272629, Final Batch Loss: 0.05427243188023567\n",
      "Epoch 4965, Loss: 0.10151973739266396, Final Batch Loss: 0.035448092967271805\n",
      "Epoch 4966, Loss: 0.11096705123782158, Final Batch Loss: 0.059225793927907944\n",
      "Epoch 4967, Loss: 0.14365727081894875, Final Batch Loss: 0.08292770385742188\n",
      "Epoch 4968, Loss: 0.1358688920736313, Final Batch Loss: 0.05509699136018753\n",
      "Epoch 4969, Loss: 0.11237821355462074, Final Batch Loss: 0.052117131650447845\n",
      "Epoch 4970, Loss: 0.11324618011713028, Final Batch Loss: 0.07516012340784073\n",
      "Epoch 4971, Loss: 0.1734073869884014, Final Batch Loss: 0.056354422122240067\n",
      "Epoch 4972, Loss: 0.08428096957504749, Final Batch Loss: 0.02855571173131466\n",
      "Epoch 4973, Loss: 0.08489597216248512, Final Batch Loss: 0.045120399445295334\n",
      "Epoch 4974, Loss: 0.08056879416108131, Final Batch Loss: 0.04505191743373871\n",
      "Epoch 4975, Loss: 0.09148328006267548, Final Batch Loss: 0.028959862887859344\n",
      "Epoch 4976, Loss: 0.13792157545685768, Final Batch Loss: 0.05261976644396782\n",
      "Epoch 4977, Loss: 0.12387677654623985, Final Batch Loss: 0.047452401369810104\n",
      "Epoch 4978, Loss: 0.1305750161409378, Final Batch Loss: 0.07555617392063141\n",
      "Epoch 4979, Loss: 0.12215520814061165, Final Batch Loss: 0.06773047894239426\n",
      "Epoch 4980, Loss: 0.109370656311512, Final Batch Loss: 0.061309829354286194\n",
      "Epoch 4981, Loss: 0.09354507178068161, Final Batch Loss: 0.03771110251545906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4982, Loss: 0.09860795736312866, Final Batch Loss: 0.05375821143388748\n",
      "Epoch 4983, Loss: 0.181132934987545, Final Batch Loss: 0.0914762020111084\n",
      "Epoch 4984, Loss: 0.11567419022321701, Final Batch Loss: 0.05343172699213028\n",
      "Epoch 4985, Loss: 0.0884178951382637, Final Batch Loss: 0.029726438224315643\n",
      "Epoch 4986, Loss: 0.10470554418861866, Final Batch Loss: 0.02894890494644642\n",
      "Epoch 4987, Loss: 0.17364854738116264, Final Batch Loss: 0.05629913881421089\n",
      "Epoch 4988, Loss: 0.14501137286424637, Final Batch Loss: 0.058561891317367554\n",
      "Epoch 4989, Loss: 0.13223803043365479, Final Batch Loss: 0.03754442185163498\n",
      "Epoch 4990, Loss: 0.1268911175429821, Final Batch Loss: 0.06956856697797775\n",
      "Epoch 4991, Loss: 0.20192745327949524, Final Batch Loss: 0.12435861676931381\n",
      "Epoch 4992, Loss: 0.09834598377346992, Final Batch Loss: 0.047008465975522995\n",
      "Epoch 4993, Loss: 0.09896162152290344, Final Batch Loss: 0.03966379165649414\n",
      "Epoch 4994, Loss: 0.10618475452065468, Final Batch Loss: 0.05082065612077713\n",
      "Epoch 4995, Loss: 0.10776245221495628, Final Batch Loss: 0.07004716992378235\n",
      "Epoch 4996, Loss: 0.11252686753869057, Final Batch Loss: 0.06743285059928894\n",
      "Epoch 4997, Loss: 0.11306197568774223, Final Batch Loss: 0.06840173900127411\n",
      "Epoch 4998, Loss: 0.19744595140218735, Final Batch Loss: 0.10218343883752823\n",
      "Epoch 4999, Loss: 0.12151677533984184, Final Batch Loss: 0.05762125924229622\n",
      "Epoch 5000, Loss: 0.12563081085681915, Final Batch Loss: 0.032419316470623016\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 35  0  0  0  0  0]\n",
      " [ 0 34  0  0  0  0  1  0  0]\n",
      " [ 5  0 29  0  0  0  0  0  1]\n",
      " [ 0  0  0 22  0  0  6  7  0]\n",
      " [ 0  0  0  0 35  0  0  0  0]\n",
      " [ 0  0  3  0  0 32  0  0  0]\n",
      " [ 0  0  0  0  0  0 35  0  0]\n",
      " [ 0  7  0  0  0  0  3 25  0]\n",
      " [ 0  0  3  0  0  3  0  0 29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.00000   0.00000   0.00000        35\n",
      "         1.0    0.82927   0.97143   0.89474        35\n",
      "         2.0    0.82857   0.82857   0.82857        35\n",
      "         3.0    0.38596   0.62857   0.47826        35\n",
      "         4.0    1.00000   1.00000   1.00000        35\n",
      "         5.0    0.91429   0.91429   0.91429        35\n",
      "         6.0    0.77778   1.00000   0.87500        35\n",
      "         7.0    0.78125   0.71429   0.74627        35\n",
      "         8.0    0.96667   0.82857   0.89231        35\n",
      "\n",
      "    accuracy                        0.76508       315\n",
      "   macro avg    0.72042   0.76508   0.73660       315\n",
      "weighted avg    0.72042   0.76508   0.73660       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8180933333333335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET RID OF 2, 5, 8 for JUST DYNAMIC\n",
    "(0+0.89474+0.47826+1+0.91429+0.875+0.74627)/6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
