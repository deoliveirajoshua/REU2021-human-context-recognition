{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [23, 25, 27]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.4347217082977295, Final Batch Loss: 2.2140936851501465\n",
      "Epoch 2, Loss: 4.434186220169067, Final Batch Loss: 2.2177114486694336\n",
      "Epoch 3, Loss: 4.431804656982422, Final Batch Loss: 2.2137868404388428\n",
      "Epoch 4, Loss: 4.428043603897095, Final Batch Loss: 2.2165467739105225\n",
      "Epoch 5, Loss: 4.425544500350952, Final Batch Loss: 2.204549551010132\n",
      "Epoch 6, Loss: 4.42397403717041, Final Batch Loss: 2.2126262187957764\n",
      "Epoch 7, Loss: 4.415851354598999, Final Batch Loss: 2.201073169708252\n",
      "Epoch 8, Loss: 4.414262294769287, Final Batch Loss: 2.20589280128479\n",
      "Epoch 9, Loss: 4.414660215377808, Final Batch Loss: 2.205204963684082\n",
      "Epoch 10, Loss: 4.407128572463989, Final Batch Loss: 2.202409505844116\n",
      "Epoch 11, Loss: 4.401142597198486, Final Batch Loss: 2.1981611251831055\n",
      "Epoch 12, Loss: 4.400351285934448, Final Batch Loss: 2.2069504261016846\n",
      "Epoch 13, Loss: 4.388543367385864, Final Batch Loss: 2.1843740940093994\n",
      "Epoch 14, Loss: 4.3819968700408936, Final Batch Loss: 2.1880555152893066\n",
      "Epoch 15, Loss: 4.376621246337891, Final Batch Loss: 2.1895875930786133\n",
      "Epoch 16, Loss: 4.36804723739624, Final Batch Loss: 2.187617778778076\n",
      "Epoch 17, Loss: 4.346318483352661, Final Batch Loss: 2.169849157333374\n",
      "Epoch 18, Loss: 4.328166246414185, Final Batch Loss: 2.159905195236206\n",
      "Epoch 19, Loss: 4.313233852386475, Final Batch Loss: 2.1567742824554443\n",
      "Epoch 20, Loss: 4.303555011749268, Final Batch Loss: 2.1510796546936035\n",
      "Epoch 21, Loss: 4.2884111404418945, Final Batch Loss: 2.1495068073272705\n",
      "Epoch 22, Loss: 4.269688606262207, Final Batch Loss: 2.1394405364990234\n",
      "Epoch 23, Loss: 4.239108085632324, Final Batch Loss: 2.1018309593200684\n",
      "Epoch 24, Loss: 4.207825183868408, Final Batch Loss: 2.1040849685668945\n",
      "Epoch 25, Loss: 4.189074993133545, Final Batch Loss: 2.089092254638672\n",
      "Epoch 26, Loss: 4.179430246353149, Final Batch Loss: 2.0940935611724854\n",
      "Epoch 27, Loss: 4.151238441467285, Final Batch Loss: 2.0811476707458496\n",
      "Epoch 28, Loss: 4.0936338901519775, Final Batch Loss: 2.0461223125457764\n",
      "Epoch 29, Loss: 4.0491673946380615, Final Batch Loss: 2.01016902923584\n",
      "Epoch 30, Loss: 4.017829179763794, Final Batch Loss: 1.9810540676116943\n",
      "Epoch 31, Loss: 3.9933446645736694, Final Batch Loss: 1.9813300371170044\n",
      "Epoch 32, Loss: 3.968940019607544, Final Batch Loss: 1.9787644147872925\n",
      "Epoch 33, Loss: 3.9126524925231934, Final Batch Loss: 1.925642728805542\n",
      "Epoch 34, Loss: 3.8910924196243286, Final Batch Loss: 1.943435549736023\n",
      "Epoch 35, Loss: 3.885917901992798, Final Batch Loss: 1.988330364227295\n",
      "Epoch 36, Loss: 3.839627981185913, Final Batch Loss: 1.958022952079773\n",
      "Epoch 37, Loss: 3.8019585609436035, Final Batch Loss: 1.9325544834136963\n",
      "Epoch 38, Loss: 3.739747405052185, Final Batch Loss: 1.8873029947280884\n",
      "Epoch 39, Loss: 3.7420806884765625, Final Batch Loss: 1.8961232900619507\n",
      "Epoch 40, Loss: 3.6570866107940674, Final Batch Loss: 1.8057122230529785\n",
      "Epoch 41, Loss: 3.63242244720459, Final Batch Loss: 1.7928258180618286\n",
      "Epoch 42, Loss: 3.602024793624878, Final Batch Loss: 1.802206039428711\n",
      "Epoch 43, Loss: 3.4949933290481567, Final Batch Loss: 1.7272027730941772\n",
      "Epoch 44, Loss: 3.593543767929077, Final Batch Loss: 1.8317725658416748\n",
      "Epoch 45, Loss: 3.4619921445846558, Final Batch Loss: 1.731387972831726\n",
      "Epoch 46, Loss: 3.4485955238342285, Final Batch Loss: 1.7532424926757812\n",
      "Epoch 47, Loss: 3.3578537702560425, Final Batch Loss: 1.6774938106536865\n",
      "Epoch 48, Loss: 3.298861026763916, Final Batch Loss: 1.6326448917388916\n",
      "Epoch 49, Loss: 3.216807007789612, Final Batch Loss: 1.6069244146347046\n",
      "Epoch 50, Loss: 3.1814661026000977, Final Batch Loss: 1.6211497783660889\n",
      "Epoch 51, Loss: 3.19171142578125, Final Batch Loss: 1.6420074701309204\n",
      "Epoch 52, Loss: 3.1310641765594482, Final Batch Loss: 1.545547604560852\n",
      "Epoch 53, Loss: 3.0203187465667725, Final Batch Loss: 1.5083622932434082\n",
      "Epoch 54, Loss: 2.950402021408081, Final Batch Loss: 1.4438667297363281\n",
      "Epoch 55, Loss: 2.9231313467025757, Final Batch Loss: 1.4522590637207031\n",
      "Epoch 56, Loss: 2.882372260093689, Final Batch Loss: 1.4076896905899048\n",
      "Epoch 57, Loss: 2.7890151739120483, Final Batch Loss: 1.4085407257080078\n",
      "Epoch 58, Loss: 2.6652984619140625, Final Batch Loss: 1.3122490644454956\n",
      "Epoch 59, Loss: 2.7211846113204956, Final Batch Loss: 1.364332675933838\n",
      "Epoch 60, Loss: 2.6182334423065186, Final Batch Loss: 1.2651458978652954\n",
      "Epoch 61, Loss: 2.6742513179779053, Final Batch Loss: 1.3252559900283813\n",
      "Epoch 62, Loss: 2.6282923221588135, Final Batch Loss: 1.3684358596801758\n",
      "Epoch 63, Loss: 2.5399712324142456, Final Batch Loss: 1.2066071033477783\n",
      "Epoch 64, Loss: 2.480584502220154, Final Batch Loss: 1.275248646736145\n",
      "Epoch 65, Loss: 2.5167609453201294, Final Batch Loss: 1.2614349126815796\n",
      "Epoch 66, Loss: 2.418804407119751, Final Batch Loss: 1.160848617553711\n",
      "Epoch 67, Loss: 2.413609504699707, Final Batch Loss: 1.2205898761749268\n",
      "Epoch 68, Loss: 2.50165331363678, Final Batch Loss: 1.2583487033843994\n",
      "Epoch 69, Loss: 2.325091600418091, Final Batch Loss: 1.1957106590270996\n",
      "Epoch 70, Loss: 2.444638967514038, Final Batch Loss: 1.2225620746612549\n",
      "Epoch 71, Loss: 2.3554331064224243, Final Batch Loss: 1.2025892734527588\n",
      "Epoch 72, Loss: 2.279522180557251, Final Batch Loss: 1.1118661165237427\n",
      "Epoch 73, Loss: 2.346627712249756, Final Batch Loss: 1.1438913345336914\n",
      "Epoch 74, Loss: 2.1988033056259155, Final Batch Loss: 1.0588443279266357\n",
      "Epoch 75, Loss: 2.224091649055481, Final Batch Loss: 1.0748355388641357\n",
      "Epoch 76, Loss: 2.184317469596863, Final Batch Loss: 1.0929456949234009\n",
      "Epoch 77, Loss: 2.167152762413025, Final Batch Loss: 1.1112215518951416\n",
      "Epoch 78, Loss: 2.151819586753845, Final Batch Loss: 1.0400532484054565\n",
      "Epoch 79, Loss: 2.2181737422943115, Final Batch Loss: 1.121849775314331\n",
      "Epoch 80, Loss: 2.180743098258972, Final Batch Loss: 1.0561254024505615\n",
      "Epoch 81, Loss: 2.185973048210144, Final Batch Loss: 1.1406859159469604\n",
      "Epoch 82, Loss: 2.1151793003082275, Final Batch Loss: 1.0181806087493896\n",
      "Epoch 83, Loss: 2.167380213737488, Final Batch Loss: 1.0781599283218384\n",
      "Epoch 84, Loss: 2.133510410785675, Final Batch Loss: 1.145943522453308\n",
      "Epoch 85, Loss: 2.2385770082473755, Final Batch Loss: 1.1393582820892334\n",
      "Epoch 86, Loss: 2.103086233139038, Final Batch Loss: 1.0376266241073608\n",
      "Epoch 87, Loss: 2.145633339881897, Final Batch Loss: 1.0813173055648804\n",
      "Epoch 88, Loss: 2.1383752822875977, Final Batch Loss: 1.088189721107483\n",
      "Epoch 89, Loss: 2.2116936445236206, Final Batch Loss: 1.1523383855819702\n",
      "Epoch 90, Loss: 2.0285873413085938, Final Batch Loss: 1.0518734455108643\n",
      "Epoch 91, Loss: 2.0726685523986816, Final Batch Loss: 1.0389535427093506\n",
      "Epoch 92, Loss: 2.0427507162094116, Final Batch Loss: 0.9648492336273193\n",
      "Epoch 93, Loss: 2.0927703380584717, Final Batch Loss: 1.0408822298049927\n",
      "Epoch 94, Loss: 2.006026804447174, Final Batch Loss: 0.9830568432807922\n",
      "Epoch 95, Loss: 2.012099266052246, Final Batch Loss: 1.0078749656677246\n",
      "Epoch 96, Loss: 1.999827802181244, Final Batch Loss: 0.979748547077179\n",
      "Epoch 97, Loss: 2.0470234155654907, Final Batch Loss: 1.0073586702346802\n",
      "Epoch 98, Loss: 2.103898048400879, Final Batch Loss: 1.047495722770691\n",
      "Epoch 99, Loss: 2.0008144974708557, Final Batch Loss: 1.025381326675415\n",
      "Epoch 100, Loss: 1.9900603294372559, Final Batch Loss: 1.0062862634658813\n",
      "Epoch 101, Loss: 1.967314898967743, Final Batch Loss: 1.0341205596923828\n",
      "Epoch 102, Loss: 2.0056400299072266, Final Batch Loss: 0.966332197189331\n",
      "Epoch 103, Loss: 1.963245451450348, Final Batch Loss: 0.9693660140037537\n",
      "Epoch 104, Loss: 1.9610885381698608, Final Batch Loss: 0.9827855825424194\n",
      "Epoch 105, Loss: 1.9963030219078064, Final Batch Loss: 0.9680296778678894\n",
      "Epoch 106, Loss: 1.938897728919983, Final Batch Loss: 0.9527818560600281\n",
      "Epoch 107, Loss: 1.9573637247085571, Final Batch Loss: 0.934996485710144\n",
      "Epoch 108, Loss: 1.9534675478935242, Final Batch Loss: 0.9628540277481079\n",
      "Epoch 109, Loss: 1.9903043508529663, Final Batch Loss: 0.9672956466674805\n",
      "Epoch 110, Loss: 1.9059982895851135, Final Batch Loss: 0.9650006294250488\n",
      "Epoch 111, Loss: 1.9322721362113953, Final Batch Loss: 0.951107919216156\n",
      "Epoch 112, Loss: 1.880327820777893, Final Batch Loss: 0.8906461000442505\n",
      "Epoch 113, Loss: 1.95896577835083, Final Batch Loss: 0.9910387396812439\n",
      "Epoch 114, Loss: 1.9317221641540527, Final Batch Loss: 0.956622838973999\n",
      "Epoch 115, Loss: 1.8681243658065796, Final Batch Loss: 0.91578608751297\n",
      "Epoch 116, Loss: 1.8521036505699158, Final Batch Loss: 0.9387177228927612\n",
      "Epoch 117, Loss: 1.9016161561012268, Final Batch Loss: 0.9830203652381897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 1.8835492730140686, Final Batch Loss: 0.9459570646286011\n",
      "Epoch 119, Loss: 1.8992271423339844, Final Batch Loss: 0.9814133644104004\n",
      "Epoch 120, Loss: 1.9106677770614624, Final Batch Loss: 0.9000521898269653\n",
      "Epoch 121, Loss: 1.864456295967102, Final Batch Loss: 0.9402604103088379\n",
      "Epoch 122, Loss: 1.9069982171058655, Final Batch Loss: 0.9170746803283691\n",
      "Epoch 123, Loss: 1.8609440922737122, Final Batch Loss: 0.9239597916603088\n",
      "Epoch 124, Loss: 1.8266807198524475, Final Batch Loss: 0.9040545225143433\n",
      "Epoch 125, Loss: 1.8396018147468567, Final Batch Loss: 0.9317435026168823\n",
      "Epoch 126, Loss: 1.798188865184784, Final Batch Loss: 0.8641582727432251\n",
      "Epoch 127, Loss: 1.8265233635902405, Final Batch Loss: 0.9104364514350891\n",
      "Epoch 128, Loss: 1.7593640685081482, Final Batch Loss: 0.8292063474655151\n",
      "Epoch 129, Loss: 1.7262585163116455, Final Batch Loss: 0.8276712894439697\n",
      "Epoch 130, Loss: 1.7611960768699646, Final Batch Loss: 0.861130952835083\n",
      "Epoch 131, Loss: 1.8241919875144958, Final Batch Loss: 0.9453170299530029\n",
      "Epoch 132, Loss: 1.797527551651001, Final Batch Loss: 0.9003192782402039\n",
      "Epoch 133, Loss: 1.7918953895568848, Final Batch Loss: 0.8907572627067566\n",
      "Epoch 134, Loss: 1.7498100996017456, Final Batch Loss: 0.8612568378448486\n",
      "Epoch 135, Loss: 1.7428853511810303, Final Batch Loss: 0.8793092966079712\n",
      "Epoch 136, Loss: 1.6866785287857056, Final Batch Loss: 0.8436770439147949\n",
      "Epoch 137, Loss: 1.7652164101600647, Final Batch Loss: 0.8994561433792114\n",
      "Epoch 138, Loss: 1.7846220135688782, Final Batch Loss: 0.959999144077301\n",
      "Epoch 139, Loss: 1.6752814650535583, Final Batch Loss: 0.8643592000007629\n",
      "Epoch 140, Loss: 1.717835247516632, Final Batch Loss: 0.869037389755249\n",
      "Epoch 141, Loss: 1.690485656261444, Final Batch Loss: 0.8261294364929199\n",
      "Epoch 142, Loss: 1.7725995779037476, Final Batch Loss: 0.9168035984039307\n",
      "Epoch 143, Loss: 1.7643740773200989, Final Batch Loss: 0.883429765701294\n",
      "Epoch 144, Loss: 1.6627570390701294, Final Batch Loss: 0.8442311882972717\n",
      "Epoch 145, Loss: 1.7303531169891357, Final Batch Loss: 0.8770235180854797\n",
      "Epoch 146, Loss: 1.6749957203865051, Final Batch Loss: 0.8615956902503967\n",
      "Epoch 147, Loss: 1.7145876288414001, Final Batch Loss: 0.8275021314620972\n",
      "Epoch 148, Loss: 1.647400140762329, Final Batch Loss: 0.7868804931640625\n",
      "Epoch 149, Loss: 1.663804054260254, Final Batch Loss: 0.8198025822639465\n",
      "Epoch 150, Loss: 1.734928548336029, Final Batch Loss: 0.8552246689796448\n",
      "Epoch 151, Loss: 1.6829679608345032, Final Batch Loss: 0.8254387974739075\n",
      "Epoch 152, Loss: 1.6902655363082886, Final Batch Loss: 0.7943083643913269\n",
      "Epoch 153, Loss: 1.7295791506767273, Final Batch Loss: 0.8552437424659729\n",
      "Epoch 154, Loss: 1.6633585095405579, Final Batch Loss: 0.8383483290672302\n",
      "Epoch 155, Loss: 1.6531128287315369, Final Batch Loss: 0.7515397667884827\n",
      "Epoch 156, Loss: 1.6880138516426086, Final Batch Loss: 0.8163264989852905\n",
      "Epoch 157, Loss: 1.6352462768554688, Final Batch Loss: 0.8104725480079651\n",
      "Epoch 158, Loss: 1.648712694644928, Final Batch Loss: 0.8071560859680176\n",
      "Epoch 159, Loss: 1.6984007954597473, Final Batch Loss: 0.8673897981643677\n",
      "Epoch 160, Loss: 1.536617398262024, Final Batch Loss: 0.7401567697525024\n",
      "Epoch 161, Loss: 1.6507981419563293, Final Batch Loss: 0.8444879651069641\n",
      "Epoch 162, Loss: 1.6295320391654968, Final Batch Loss: 0.8172196745872498\n",
      "Epoch 163, Loss: 1.6771056652069092, Final Batch Loss: 0.8396568298339844\n",
      "Epoch 164, Loss: 1.6667602062225342, Final Batch Loss: 0.8111555576324463\n",
      "Epoch 165, Loss: 1.6063263416290283, Final Batch Loss: 0.7952802777290344\n",
      "Epoch 166, Loss: 1.6198163628578186, Final Batch Loss: 0.8467077612876892\n",
      "Epoch 167, Loss: 1.5951008200645447, Final Batch Loss: 0.785180926322937\n",
      "Epoch 168, Loss: 1.5883410573005676, Final Batch Loss: 0.8015467524528503\n",
      "Epoch 169, Loss: 1.6735196709632874, Final Batch Loss: 0.8481839895248413\n",
      "Epoch 170, Loss: 1.575645089149475, Final Batch Loss: 0.7365698218345642\n",
      "Epoch 171, Loss: 1.589697003364563, Final Batch Loss: 0.7753104567527771\n",
      "Epoch 172, Loss: 1.6151544451713562, Final Batch Loss: 0.8411481976509094\n",
      "Epoch 173, Loss: 1.5636946558952332, Final Batch Loss: 0.7595567107200623\n",
      "Epoch 174, Loss: 1.6093506813049316, Final Batch Loss: 0.8448117971420288\n",
      "Epoch 175, Loss: 1.5619964599609375, Final Batch Loss: 0.752270519733429\n",
      "Epoch 176, Loss: 1.594753384590149, Final Batch Loss: 0.7471860647201538\n",
      "Epoch 177, Loss: 1.5699032545089722, Final Batch Loss: 0.8279528617858887\n",
      "Epoch 178, Loss: 1.6133789420127869, Final Batch Loss: 0.8092585206031799\n",
      "Epoch 179, Loss: 1.5333513617515564, Final Batch Loss: 0.7647340893745422\n",
      "Epoch 180, Loss: 1.540539562702179, Final Batch Loss: 0.7291673421859741\n",
      "Epoch 181, Loss: 1.5748862624168396, Final Batch Loss: 0.809175431728363\n",
      "Epoch 182, Loss: 1.5927541255950928, Final Batch Loss: 0.8085989356040955\n",
      "Epoch 183, Loss: 1.5799935460090637, Final Batch Loss: 0.841549813747406\n",
      "Epoch 184, Loss: 1.4986725449562073, Final Batch Loss: 0.7537530064582825\n",
      "Epoch 185, Loss: 1.5300223231315613, Final Batch Loss: 0.7178659439086914\n",
      "Epoch 186, Loss: 1.5337905883789062, Final Batch Loss: 0.7545270323753357\n",
      "Epoch 187, Loss: 1.5860072374343872, Final Batch Loss: 0.7862179279327393\n",
      "Epoch 188, Loss: 1.5639403462409973, Final Batch Loss: 0.7832345366477966\n",
      "Epoch 189, Loss: 1.5276173949241638, Final Batch Loss: 0.7239838242530823\n",
      "Epoch 190, Loss: 1.4956330060958862, Final Batch Loss: 0.7412883043289185\n",
      "Epoch 191, Loss: 1.55697101354599, Final Batch Loss: 0.8156524300575256\n",
      "Epoch 192, Loss: 1.5537686944007874, Final Batch Loss: 0.8215581774711609\n",
      "Epoch 193, Loss: 1.5125256180763245, Final Batch Loss: 0.740984320640564\n",
      "Epoch 194, Loss: 1.545185625553131, Final Batch Loss: 0.8400501608848572\n",
      "Epoch 195, Loss: 1.520121157169342, Final Batch Loss: 0.7766237854957581\n",
      "Epoch 196, Loss: 1.5906910300254822, Final Batch Loss: 0.7982523441314697\n",
      "Epoch 197, Loss: 1.5117279291152954, Final Batch Loss: 0.7303528189659119\n",
      "Epoch 198, Loss: 1.4367987513542175, Final Batch Loss: 0.676571786403656\n",
      "Epoch 199, Loss: 1.5247641205787659, Final Batch Loss: 0.7749491930007935\n",
      "Epoch 200, Loss: 1.5253387093544006, Final Batch Loss: 0.7533995509147644\n",
      "Epoch 201, Loss: 1.5497206449508667, Final Batch Loss: 0.8377790451049805\n",
      "Epoch 202, Loss: 1.5267205238342285, Final Batch Loss: 0.7580828666687012\n",
      "Epoch 203, Loss: 1.5484800338745117, Final Batch Loss: 0.8116077184677124\n",
      "Epoch 204, Loss: 1.4844692945480347, Final Batch Loss: 0.7667052149772644\n",
      "Epoch 205, Loss: 1.5087568163871765, Final Batch Loss: 0.6595951318740845\n",
      "Epoch 206, Loss: 1.4697719812393188, Final Batch Loss: 0.7820147275924683\n",
      "Epoch 207, Loss: 1.4388175010681152, Final Batch Loss: 0.730871856212616\n",
      "Epoch 208, Loss: 1.4362457394599915, Final Batch Loss: 0.7086482048034668\n",
      "Epoch 209, Loss: 1.4510291814804077, Final Batch Loss: 0.7533925175666809\n",
      "Epoch 210, Loss: 1.4770792126655579, Final Batch Loss: 0.6880132555961609\n",
      "Epoch 211, Loss: 1.4744038581848145, Final Batch Loss: 0.7489847540855408\n",
      "Epoch 212, Loss: 1.4548620581626892, Final Batch Loss: 0.6801273226737976\n",
      "Epoch 213, Loss: 1.4768391251564026, Final Batch Loss: 0.7616357803344727\n",
      "Epoch 214, Loss: 1.4838246703147888, Final Batch Loss: 0.7881149053573608\n",
      "Epoch 215, Loss: 1.3557555675506592, Final Batch Loss: 0.6738138198852539\n",
      "Epoch 216, Loss: 1.4430745244026184, Final Batch Loss: 0.6818673014640808\n",
      "Epoch 217, Loss: 1.4116998314857483, Final Batch Loss: 0.6689351201057434\n",
      "Epoch 218, Loss: 1.4803360104560852, Final Batch Loss: 0.7331638336181641\n",
      "Epoch 219, Loss: 1.4804490804672241, Final Batch Loss: 0.7509884238243103\n",
      "Epoch 220, Loss: 1.3832327127456665, Final Batch Loss: 0.6413296461105347\n",
      "Epoch 221, Loss: 1.4526031017303467, Final Batch Loss: 0.7122461199760437\n",
      "Epoch 222, Loss: 1.4537211060523987, Final Batch Loss: 0.7063750624656677\n",
      "Epoch 223, Loss: 1.475862205028534, Final Batch Loss: 0.7486613988876343\n",
      "Epoch 224, Loss: 1.3901053667068481, Final Batch Loss: 0.6788427829742432\n",
      "Epoch 225, Loss: 1.4307929277420044, Final Batch Loss: 0.6943201422691345\n",
      "Epoch 226, Loss: 1.3598012924194336, Final Batch Loss: 0.6758277416229248\n",
      "Epoch 227, Loss: 1.429272472858429, Final Batch Loss: 0.7055634260177612\n",
      "Epoch 228, Loss: 1.4416547417640686, Final Batch Loss: 0.6830628514289856\n",
      "Epoch 229, Loss: 1.418153464794159, Final Batch Loss: 0.6630679368972778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230, Loss: 1.405604064464569, Final Batch Loss: 0.7253932952880859\n",
      "Epoch 231, Loss: 1.3772156238555908, Final Batch Loss: 0.6603706479072571\n",
      "Epoch 232, Loss: 1.4119746088981628, Final Batch Loss: 0.7103503346443176\n",
      "Epoch 233, Loss: 1.3666326999664307, Final Batch Loss: 0.6779454946517944\n",
      "Epoch 234, Loss: 1.3745115399360657, Final Batch Loss: 0.6249181032180786\n",
      "Epoch 235, Loss: 1.3957756161689758, Final Batch Loss: 0.7481343746185303\n",
      "Epoch 236, Loss: 1.4064461588859558, Final Batch Loss: 0.7402979731559753\n",
      "Epoch 237, Loss: 1.4284409880638123, Final Batch Loss: 0.7040667533874512\n",
      "Epoch 238, Loss: 1.415938138961792, Final Batch Loss: 0.7446808218955994\n",
      "Epoch 239, Loss: 1.363386869430542, Final Batch Loss: 0.6713137626647949\n",
      "Epoch 240, Loss: 1.4204379320144653, Final Batch Loss: 0.6914213299751282\n",
      "Epoch 241, Loss: 1.416352927684784, Final Batch Loss: 0.7498769760131836\n",
      "Epoch 242, Loss: 1.4178376197814941, Final Batch Loss: 0.7733689546585083\n",
      "Epoch 243, Loss: 1.3667928576469421, Final Batch Loss: 0.6978936791419983\n",
      "Epoch 244, Loss: 1.3918683528900146, Final Batch Loss: 0.6783099174499512\n",
      "Epoch 245, Loss: 1.3136039972305298, Final Batch Loss: 0.665984034538269\n",
      "Epoch 246, Loss: 1.3817238211631775, Final Batch Loss: 0.7144672274589539\n",
      "Epoch 247, Loss: 1.3124579191207886, Final Batch Loss: 0.6444636583328247\n",
      "Epoch 248, Loss: 1.3890252113342285, Final Batch Loss: 0.7075712084770203\n",
      "Epoch 249, Loss: 1.433230459690094, Final Batch Loss: 0.6839448809623718\n",
      "Epoch 250, Loss: 1.3470628261566162, Final Batch Loss: 0.6589720249176025\n",
      "Epoch 251, Loss: 1.3943647742271423, Final Batch Loss: 0.7147736549377441\n",
      "Epoch 252, Loss: 1.3640087246894836, Final Batch Loss: 0.7345430254936218\n",
      "Epoch 253, Loss: 1.3216455578804016, Final Batch Loss: 0.6414564251899719\n",
      "Epoch 254, Loss: 1.2175105810165405, Final Batch Loss: 0.5725098252296448\n",
      "Epoch 255, Loss: 1.3674067258834839, Final Batch Loss: 0.6644400358200073\n",
      "Epoch 256, Loss: 1.3454799056053162, Final Batch Loss: 0.6461964845657349\n",
      "Epoch 257, Loss: 1.2779341340065002, Final Batch Loss: 0.6365590691566467\n",
      "Epoch 258, Loss: 1.3408696055412292, Final Batch Loss: 0.690552294254303\n",
      "Epoch 259, Loss: 1.2942841053009033, Final Batch Loss: 0.6894813776016235\n",
      "Epoch 260, Loss: 1.2874343991279602, Final Batch Loss: 0.6107056736946106\n",
      "Epoch 261, Loss: 1.3754730224609375, Final Batch Loss: 0.663761556148529\n",
      "Epoch 262, Loss: 1.3922781348228455, Final Batch Loss: 0.7097625732421875\n",
      "Epoch 263, Loss: 1.3308915495872498, Final Batch Loss: 0.7034347653388977\n",
      "Epoch 264, Loss: 1.2994332909584045, Final Batch Loss: 0.6860886216163635\n",
      "Epoch 265, Loss: 1.329013705253601, Final Batch Loss: 0.6368659734725952\n",
      "Epoch 266, Loss: 1.3270615339279175, Final Batch Loss: 0.649705708026886\n",
      "Epoch 267, Loss: 1.3497045040130615, Final Batch Loss: 0.6642192602157593\n",
      "Epoch 268, Loss: 1.220611333847046, Final Batch Loss: 0.6343222856521606\n",
      "Epoch 269, Loss: 1.2780511975288391, Final Batch Loss: 0.6573687195777893\n",
      "Epoch 270, Loss: 1.2755382657051086, Final Batch Loss: 0.7215020060539246\n",
      "Epoch 271, Loss: 1.2527565360069275, Final Batch Loss: 0.5856915712356567\n",
      "Epoch 272, Loss: 1.3235155940055847, Final Batch Loss: 0.6903208494186401\n",
      "Epoch 273, Loss: 1.2851774096488953, Final Batch Loss: 0.6295225620269775\n",
      "Epoch 274, Loss: 1.2736754417419434, Final Batch Loss: 0.6782689690589905\n",
      "Epoch 275, Loss: 1.2748262882232666, Final Batch Loss: 0.6072679758071899\n",
      "Epoch 276, Loss: 1.2155713438987732, Final Batch Loss: 0.562182605266571\n",
      "Epoch 277, Loss: 1.2772088646888733, Final Batch Loss: 0.6528381705284119\n",
      "Epoch 278, Loss: 1.2023476958274841, Final Batch Loss: 0.5465710163116455\n",
      "Epoch 279, Loss: 1.2734577655792236, Final Batch Loss: 0.6076768040657043\n",
      "Epoch 280, Loss: 1.3198015093803406, Final Batch Loss: 0.6822313070297241\n",
      "Epoch 281, Loss: 1.2494189143180847, Final Batch Loss: 0.6446352601051331\n",
      "Epoch 282, Loss: 1.2683929800987244, Final Batch Loss: 0.6324842572212219\n",
      "Epoch 283, Loss: 1.2899271845817566, Final Batch Loss: 0.6384530663490295\n",
      "Epoch 284, Loss: 1.1906766891479492, Final Batch Loss: 0.5885818600654602\n",
      "Epoch 285, Loss: 1.1918137669563293, Final Batch Loss: 0.5915783047676086\n",
      "Epoch 286, Loss: 1.207715094089508, Final Batch Loss: 0.5800185799598694\n",
      "Epoch 287, Loss: 1.3180838227272034, Final Batch Loss: 0.6526621580123901\n",
      "Epoch 288, Loss: 1.269297480583191, Final Batch Loss: 0.6300068497657776\n",
      "Epoch 289, Loss: 1.194770336151123, Final Batch Loss: 0.5724949240684509\n",
      "Epoch 290, Loss: 1.2023860812187195, Final Batch Loss: 0.612472653388977\n",
      "Epoch 291, Loss: 1.2366869449615479, Final Batch Loss: 0.6053693890571594\n",
      "Epoch 292, Loss: 1.2225475311279297, Final Batch Loss: 0.5962930917739868\n",
      "Epoch 293, Loss: 1.2452163100242615, Final Batch Loss: 0.6299954652786255\n",
      "Epoch 294, Loss: 1.1825628280639648, Final Batch Loss: 0.638857364654541\n",
      "Epoch 295, Loss: 1.2408749461174011, Final Batch Loss: 0.6143819689750671\n",
      "Epoch 296, Loss: 1.2545363903045654, Final Batch Loss: 0.6146680116653442\n",
      "Epoch 297, Loss: 1.2143442034721375, Final Batch Loss: 0.6198790073394775\n",
      "Epoch 298, Loss: 1.2367355227470398, Final Batch Loss: 0.6764440536499023\n",
      "Epoch 299, Loss: 1.2189240455627441, Final Batch Loss: 0.59110027551651\n",
      "Epoch 300, Loss: 1.2510710954666138, Final Batch Loss: 0.6187682151794434\n",
      "Epoch 301, Loss: 1.1364797949790955, Final Batch Loss: 0.5286905169487\n",
      "Epoch 302, Loss: 1.2292700409889221, Final Batch Loss: 0.6524899005889893\n",
      "Epoch 303, Loss: 1.2860855460166931, Final Batch Loss: 0.662817120552063\n",
      "Epoch 304, Loss: 1.2129733562469482, Final Batch Loss: 0.6339846253395081\n",
      "Epoch 305, Loss: 1.301992952823639, Final Batch Loss: 0.7148469090461731\n",
      "Epoch 306, Loss: 1.2301146984100342, Final Batch Loss: 0.6379377841949463\n",
      "Epoch 307, Loss: 1.2298229932785034, Final Batch Loss: 0.5606438517570496\n",
      "Epoch 308, Loss: 1.1936838626861572, Final Batch Loss: 0.5629133582115173\n",
      "Epoch 309, Loss: 1.1892326474189758, Final Batch Loss: 0.6268758177757263\n",
      "Epoch 310, Loss: 1.187212884426117, Final Batch Loss: 0.543071448802948\n",
      "Epoch 311, Loss: 1.2668110132217407, Final Batch Loss: 0.6596530675888062\n",
      "Epoch 312, Loss: 1.1733207702636719, Final Batch Loss: 0.5813382267951965\n",
      "Epoch 313, Loss: 1.215575933456421, Final Batch Loss: 0.5779293775558472\n",
      "Epoch 314, Loss: 1.1945099234580994, Final Batch Loss: 0.6408072113990784\n",
      "Epoch 315, Loss: 1.1893791556358337, Final Batch Loss: 0.6068654656410217\n",
      "Epoch 316, Loss: 1.1419035196304321, Final Batch Loss: 0.5417901277542114\n",
      "Epoch 317, Loss: 1.1941807866096497, Final Batch Loss: 0.5718062520027161\n",
      "Epoch 318, Loss: 1.2052923440933228, Final Batch Loss: 0.5711702704429626\n",
      "Epoch 319, Loss: 1.1665961742401123, Final Batch Loss: 0.5603386759757996\n",
      "Epoch 320, Loss: 1.1638743877410889, Final Batch Loss: 0.5507153868675232\n",
      "Epoch 321, Loss: 1.1807655096054077, Final Batch Loss: 0.591644287109375\n",
      "Epoch 322, Loss: 1.1781790852546692, Final Batch Loss: 0.6431600451469421\n",
      "Epoch 323, Loss: 1.157005250453949, Final Batch Loss: 0.5706632137298584\n",
      "Epoch 324, Loss: 1.1242143511772156, Final Batch Loss: 0.5539013743400574\n",
      "Epoch 325, Loss: 1.1920501589775085, Final Batch Loss: 0.5940461158752441\n",
      "Epoch 326, Loss: 1.2685519456863403, Final Batch Loss: 0.6236021518707275\n",
      "Epoch 327, Loss: 1.1531134843826294, Final Batch Loss: 0.5721784830093384\n",
      "Epoch 328, Loss: 1.1537402868270874, Final Batch Loss: 0.5996943712234497\n",
      "Epoch 329, Loss: 1.1655850410461426, Final Batch Loss: 0.552207350730896\n",
      "Epoch 330, Loss: 1.1689636707305908, Final Batch Loss: 0.5737501978874207\n",
      "Epoch 331, Loss: 1.1634021997451782, Final Batch Loss: 0.605013906955719\n",
      "Epoch 332, Loss: 1.1347687244415283, Final Batch Loss: 0.5544514060020447\n",
      "Epoch 333, Loss: 1.1602365374565125, Final Batch Loss: 0.5839568376541138\n",
      "Epoch 334, Loss: 1.199243426322937, Final Batch Loss: 0.5846635103225708\n",
      "Epoch 335, Loss: 1.1203175783157349, Final Batch Loss: 0.5226901173591614\n",
      "Epoch 336, Loss: 1.1412854194641113, Final Batch Loss: 0.5689693689346313\n",
      "Epoch 337, Loss: 1.2444297075271606, Final Batch Loss: 0.6358131170272827\n",
      "Epoch 338, Loss: 1.0674009323120117, Final Batch Loss: 0.5573457479476929\n",
      "Epoch 339, Loss: 1.179078996181488, Final Batch Loss: 0.573503851890564\n",
      "Epoch 340, Loss: 1.1410609483718872, Final Batch Loss: 0.5524394512176514\n",
      "Epoch 341, Loss: 1.1005280017852783, Final Batch Loss: 0.5584805011749268\n",
      "Epoch 342, Loss: 1.1058965921401978, Final Batch Loss: 0.5798748731613159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343, Loss: 1.1426265835762024, Final Batch Loss: 0.5714884400367737\n",
      "Epoch 344, Loss: 1.132542073726654, Final Batch Loss: 0.5799058675765991\n",
      "Epoch 345, Loss: 1.1451201438903809, Final Batch Loss: 0.6200394630432129\n",
      "Epoch 346, Loss: 1.1506258249282837, Final Batch Loss: 0.5716219544410706\n",
      "Epoch 347, Loss: 1.1486912965774536, Final Batch Loss: 0.5651829242706299\n",
      "Epoch 348, Loss: 1.2254668474197388, Final Batch Loss: 0.6368802785873413\n",
      "Epoch 349, Loss: 1.185440182685852, Final Batch Loss: 0.540125846862793\n",
      "Epoch 350, Loss: 1.1319569945335388, Final Batch Loss: 0.5789538025856018\n",
      "Epoch 351, Loss: 1.166386365890503, Final Batch Loss: 0.6014363765716553\n",
      "Epoch 352, Loss: 1.1302145719528198, Final Batch Loss: 0.5474064946174622\n",
      "Epoch 353, Loss: 1.1184534430503845, Final Batch Loss: 0.4858499765396118\n",
      "Epoch 354, Loss: 1.0803090333938599, Final Batch Loss: 0.5391020774841309\n",
      "Epoch 355, Loss: 1.157444179058075, Final Batch Loss: 0.5571220517158508\n",
      "Epoch 356, Loss: 1.1863152980804443, Final Batch Loss: 0.6078737378120422\n",
      "Epoch 357, Loss: 1.1516972184181213, Final Batch Loss: 0.5789397358894348\n",
      "Epoch 358, Loss: 1.167984664440155, Final Batch Loss: 0.5186258554458618\n",
      "Epoch 359, Loss: 1.2245638966560364, Final Batch Loss: 0.5763062238693237\n",
      "Epoch 360, Loss: 1.0952198207378387, Final Batch Loss: 0.4917198121547699\n",
      "Epoch 361, Loss: 1.1115972399711609, Final Batch Loss: 0.5186065435409546\n",
      "Epoch 362, Loss: 1.084395945072174, Final Batch Loss: 0.5293342471122742\n",
      "Epoch 363, Loss: 1.1447092294692993, Final Batch Loss: 0.5746371746063232\n",
      "Epoch 364, Loss: 1.212464451789856, Final Batch Loss: 0.6539509892463684\n",
      "Epoch 365, Loss: 1.1343930959701538, Final Batch Loss: 0.5613688826560974\n",
      "Epoch 366, Loss: 1.0832536220550537, Final Batch Loss: 0.5089511275291443\n",
      "Epoch 367, Loss: 1.1302149295806885, Final Batch Loss: 0.5666177868843079\n",
      "Epoch 368, Loss: 1.1583278179168701, Final Batch Loss: 0.6522834300994873\n",
      "Epoch 369, Loss: 1.1211415529251099, Final Batch Loss: 0.5597819685935974\n",
      "Epoch 370, Loss: 1.0643793940544128, Final Batch Loss: 0.4992135763168335\n",
      "Epoch 371, Loss: 1.0884000062942505, Final Batch Loss: 0.5336395502090454\n",
      "Epoch 372, Loss: 1.1398723721504211, Final Batch Loss: 0.5816582441329956\n",
      "Epoch 373, Loss: 1.0975781679153442, Final Batch Loss: 0.5339605808258057\n",
      "Epoch 374, Loss: 1.1680421233177185, Final Batch Loss: 0.62262362241745\n",
      "Epoch 375, Loss: 1.0683573484420776, Final Batch Loss: 0.5375545620918274\n",
      "Epoch 376, Loss: 1.0643093585968018, Final Batch Loss: 0.5331860780715942\n",
      "Epoch 377, Loss: 1.1097826957702637, Final Batch Loss: 0.5399536490440369\n",
      "Epoch 378, Loss: 1.0846213698387146, Final Batch Loss: 0.509888768196106\n",
      "Epoch 379, Loss: 1.132784128189087, Final Batch Loss: 0.5724084973335266\n",
      "Epoch 380, Loss: 1.0633960962295532, Final Batch Loss: 0.5379287004470825\n",
      "Epoch 381, Loss: 1.1217289566993713, Final Batch Loss: 0.529657244682312\n",
      "Epoch 382, Loss: 1.1406769156455994, Final Batch Loss: 0.5863606333732605\n",
      "Epoch 383, Loss: 1.0895148515701294, Final Batch Loss: 0.5338190793991089\n",
      "Epoch 384, Loss: 1.0900935530662537, Final Batch Loss: 0.5930353999137878\n",
      "Epoch 385, Loss: 1.0609619617462158, Final Batch Loss: 0.5276704430580139\n",
      "Epoch 386, Loss: 1.109358012676239, Final Batch Loss: 0.5117275714874268\n",
      "Epoch 387, Loss: 1.0678607821464539, Final Batch Loss: 0.5538932085037231\n",
      "Epoch 388, Loss: 1.11004638671875, Final Batch Loss: 0.5678027868270874\n",
      "Epoch 389, Loss: 1.1451522707939148, Final Batch Loss: 0.5980412364006042\n",
      "Epoch 390, Loss: 1.146441400051117, Final Batch Loss: 0.5805222392082214\n",
      "Epoch 391, Loss: 1.0408065915107727, Final Batch Loss: 0.5120022296905518\n",
      "Epoch 392, Loss: 1.0798691511154175, Final Batch Loss: 0.5433905720710754\n",
      "Epoch 393, Loss: 1.0647000670433044, Final Batch Loss: 0.46326184272766113\n",
      "Epoch 394, Loss: 1.041384607553482, Final Batch Loss: 0.48870059847831726\n",
      "Epoch 395, Loss: 1.0738605856895447, Final Batch Loss: 0.5271263122558594\n",
      "Epoch 396, Loss: 1.1121452450752258, Final Batch Loss: 0.5727337002754211\n",
      "Epoch 397, Loss: 1.1905238628387451, Final Batch Loss: 0.6312233805656433\n",
      "Epoch 398, Loss: 1.0169512331485748, Final Batch Loss: 0.5240087509155273\n",
      "Epoch 399, Loss: 1.0460330843925476, Final Batch Loss: 0.47735893726348877\n",
      "Epoch 400, Loss: 1.0732308626174927, Final Batch Loss: 0.5030406713485718\n",
      "Epoch 401, Loss: 1.0840190052986145, Final Batch Loss: 0.565132737159729\n",
      "Epoch 402, Loss: 1.1040133833885193, Final Batch Loss: 0.5702741146087646\n",
      "Epoch 403, Loss: 1.0875428318977356, Final Batch Loss: 0.5159647464752197\n",
      "Epoch 404, Loss: 1.0639106631278992, Final Batch Loss: 0.5633032917976379\n",
      "Epoch 405, Loss: 1.0969383716583252, Final Batch Loss: 0.539608895778656\n",
      "Epoch 406, Loss: 1.0778786540031433, Final Batch Loss: 0.5338290929794312\n",
      "Epoch 407, Loss: 1.1190151572227478, Final Batch Loss: 0.5882329344749451\n",
      "Epoch 408, Loss: 1.1034443974494934, Final Batch Loss: 0.5524675846099854\n",
      "Epoch 409, Loss: 1.053748905658722, Final Batch Loss: 0.5187534689903259\n",
      "Epoch 410, Loss: 1.065915822982788, Final Batch Loss: 0.5441997647285461\n",
      "Epoch 411, Loss: 1.0973228216171265, Final Batch Loss: 0.5497249960899353\n",
      "Epoch 412, Loss: 1.0372392535209656, Final Batch Loss: 0.52590411901474\n",
      "Epoch 413, Loss: 1.0564139485359192, Final Batch Loss: 0.5250933766365051\n",
      "Epoch 414, Loss: 1.00912544131279, Final Batch Loss: 0.5365875363349915\n",
      "Epoch 415, Loss: 1.1077841520309448, Final Batch Loss: 0.5920504331588745\n",
      "Epoch 416, Loss: 1.0695554614067078, Final Batch Loss: 0.5459536910057068\n",
      "Epoch 417, Loss: 1.0609848499298096, Final Batch Loss: 0.532749593257904\n",
      "Epoch 418, Loss: 1.0359518229961395, Final Batch Loss: 0.5662454962730408\n",
      "Epoch 419, Loss: 1.1144846677780151, Final Batch Loss: 0.5336704850196838\n",
      "Epoch 420, Loss: 1.0672082304954529, Final Batch Loss: 0.5169919729232788\n",
      "Epoch 421, Loss: 1.053452491760254, Final Batch Loss: 0.5231830477714539\n",
      "Epoch 422, Loss: 1.0939499139785767, Final Batch Loss: 0.5512966513633728\n",
      "Epoch 423, Loss: 1.0427860617637634, Final Batch Loss: 0.5059528946876526\n",
      "Epoch 424, Loss: 1.0649697184562683, Final Batch Loss: 0.5277411937713623\n",
      "Epoch 425, Loss: 1.0846853852272034, Final Batch Loss: 0.5535870790481567\n",
      "Epoch 426, Loss: 1.035128891468048, Final Batch Loss: 0.5347571969032288\n",
      "Epoch 427, Loss: 1.1399570107460022, Final Batch Loss: 0.5651819705963135\n",
      "Epoch 428, Loss: 1.0348125398159027, Final Batch Loss: 0.49788060784339905\n",
      "Epoch 429, Loss: 1.0701881647109985, Final Batch Loss: 0.5200168490409851\n",
      "Epoch 430, Loss: 1.0404051542282104, Final Batch Loss: 0.490156352519989\n",
      "Epoch 431, Loss: 1.0835304260253906, Final Batch Loss: 0.5848792195320129\n",
      "Epoch 432, Loss: 1.0915354490280151, Final Batch Loss: 0.5686329007148743\n",
      "Epoch 433, Loss: 1.0217853784561157, Final Batch Loss: 0.5016165971755981\n",
      "Epoch 434, Loss: 1.051197350025177, Final Batch Loss: 0.5561961531639099\n",
      "Epoch 435, Loss: 0.9867100417613983, Final Batch Loss: 0.4436936676502228\n",
      "Epoch 436, Loss: 1.1056426167488098, Final Batch Loss: 0.5837019681930542\n",
      "Epoch 437, Loss: 1.072124719619751, Final Batch Loss: 0.5591830015182495\n",
      "Epoch 438, Loss: 1.0366145968437195, Final Batch Loss: 0.4943247437477112\n",
      "Epoch 439, Loss: 1.0996957421302795, Final Batch Loss: 0.5451133251190186\n",
      "Epoch 440, Loss: 1.103849709033966, Final Batch Loss: 0.5778926610946655\n",
      "Epoch 441, Loss: 1.0654510855674744, Final Batch Loss: 0.5085970759391785\n",
      "Epoch 442, Loss: 1.0942500531673431, Final Batch Loss: 0.48920896649360657\n",
      "Epoch 443, Loss: 1.138932228088379, Final Batch Loss: 0.5914148688316345\n",
      "Epoch 444, Loss: 1.0979160070419312, Final Batch Loss: 0.5542439818382263\n",
      "Epoch 445, Loss: 1.0091922283172607, Final Batch Loss: 0.5117914080619812\n",
      "Epoch 446, Loss: 1.070217102766037, Final Batch Loss: 0.5951998829841614\n",
      "Epoch 447, Loss: 1.0265119075775146, Final Batch Loss: 0.5213426351547241\n",
      "Epoch 448, Loss: 1.0280434489250183, Final Batch Loss: 0.4871567487716675\n",
      "Epoch 449, Loss: 1.0415623188018799, Final Batch Loss: 0.5216183066368103\n",
      "Epoch 450, Loss: 1.055004894733429, Final Batch Loss: 0.5269407033920288\n",
      "Epoch 451, Loss: 1.0097123980522156, Final Batch Loss: 0.514085054397583\n",
      "Epoch 452, Loss: 0.9929206669330597, Final Batch Loss: 0.5030242204666138\n",
      "Epoch 453, Loss: 1.0731585025787354, Final Batch Loss: 0.5496562123298645\n",
      "Epoch 454, Loss: 1.0730990171432495, Final Batch Loss: 0.5676822066307068\n",
      "Epoch 455, Loss: 1.0767903327941895, Final Batch Loss: 0.5563383102416992\n",
      "Epoch 456, Loss: 1.0451876521110535, Final Batch Loss: 0.4960688352584839\n",
      "Epoch 457, Loss: 1.0186134576797485, Final Batch Loss: 0.5104623436927795\n",
      "Epoch 458, Loss: 1.0508151054382324, Final Batch Loss: 0.5291177034378052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459, Loss: 1.0346530973911285, Final Batch Loss: 0.4808882176876068\n",
      "Epoch 460, Loss: 1.1069361567497253, Final Batch Loss: 0.5701172351837158\n",
      "Epoch 461, Loss: 1.021798551082611, Final Batch Loss: 0.5048655271530151\n",
      "Epoch 462, Loss: 1.0283113718032837, Final Batch Loss: 0.5346570014953613\n",
      "Epoch 463, Loss: 1.1227824687957764, Final Batch Loss: 0.5576217770576477\n",
      "Epoch 464, Loss: 1.0181264579296112, Final Batch Loss: 0.46232661604881287\n",
      "Epoch 465, Loss: 0.9888880252838135, Final Batch Loss: 0.44032835960388184\n",
      "Epoch 466, Loss: 1.065086841583252, Final Batch Loss: 0.548753023147583\n",
      "Epoch 467, Loss: 1.0036338865756989, Final Batch Loss: 0.48790374398231506\n",
      "Epoch 468, Loss: 1.0241032242774963, Final Batch Loss: 0.4475485682487488\n",
      "Epoch 469, Loss: 1.05765038728714, Final Batch Loss: 0.5312442183494568\n",
      "Epoch 470, Loss: 1.0422505140304565, Final Batch Loss: 0.563431978225708\n",
      "Epoch 471, Loss: 1.0117138028144836, Final Batch Loss: 0.5194341540336609\n",
      "Epoch 472, Loss: 1.0044430792331696, Final Batch Loss: 0.5689458847045898\n",
      "Epoch 473, Loss: 1.0488099455833435, Final Batch Loss: 0.5065130591392517\n",
      "Epoch 474, Loss: 1.0269078016281128, Final Batch Loss: 0.517616331577301\n",
      "Epoch 475, Loss: 1.0182662904262543, Final Batch Loss: 0.5279538035392761\n",
      "Epoch 476, Loss: 1.048498511314392, Final Batch Loss: 0.5078603029251099\n",
      "Epoch 477, Loss: 0.9967766106128693, Final Batch Loss: 0.5057777762413025\n",
      "Epoch 478, Loss: 1.0482728481292725, Final Batch Loss: 0.5090265274047852\n",
      "Epoch 479, Loss: 1.1028019189834595, Final Batch Loss: 0.5470815300941467\n",
      "Epoch 480, Loss: 1.0092955827713013, Final Batch Loss: 0.5283317565917969\n",
      "Epoch 481, Loss: 1.005305677652359, Final Batch Loss: 0.48184338212013245\n",
      "Epoch 482, Loss: 1.0323485136032104, Final Batch Loss: 0.5213814377784729\n",
      "Epoch 483, Loss: 1.0055347681045532, Final Batch Loss: 0.48363709449768066\n",
      "Epoch 484, Loss: 0.9999766647815704, Final Batch Loss: 0.4988549053668976\n",
      "Epoch 485, Loss: 1.0244728028774261, Final Batch Loss: 0.5405417084693909\n",
      "Epoch 486, Loss: 1.0551714897155762, Final Batch Loss: 0.5173344612121582\n",
      "Epoch 487, Loss: 1.0095272064208984, Final Batch Loss: 0.5158045291900635\n",
      "Epoch 488, Loss: 1.0282807350158691, Final Batch Loss: 0.5145850777626038\n",
      "Epoch 489, Loss: 1.02936190366745, Final Batch Loss: 0.5073836445808411\n",
      "Epoch 490, Loss: 0.977406769990921, Final Batch Loss: 0.5129746794700623\n",
      "Epoch 491, Loss: 1.0287266075611115, Final Batch Loss: 0.5423105359077454\n",
      "Epoch 492, Loss: 1.0051507651805878, Final Batch Loss: 0.513245701789856\n",
      "Epoch 493, Loss: 1.0660123825073242, Final Batch Loss: 0.5134044289588928\n",
      "Epoch 494, Loss: 1.015265792608261, Final Batch Loss: 0.551563024520874\n",
      "Epoch 495, Loss: 1.031277060508728, Final Batch Loss: 0.508897066116333\n",
      "Epoch 496, Loss: 1.060372233390808, Final Batch Loss: 0.5190332531929016\n",
      "Epoch 497, Loss: 0.9723994433879852, Final Batch Loss: 0.4665888249874115\n",
      "Epoch 498, Loss: 0.9516229629516602, Final Batch Loss: 0.45801815390586853\n",
      "Epoch 499, Loss: 1.0338236391544342, Final Batch Loss: 0.5611853003501892\n",
      "Epoch 500, Loss: 1.0526497960090637, Final Batch Loss: 0.5594745874404907\n",
      "Epoch 501, Loss: 1.013249695301056, Final Batch Loss: 0.5053790211677551\n",
      "Epoch 502, Loss: 0.98247230052948, Final Batch Loss: 0.5140334367752075\n",
      "Epoch 503, Loss: 1.040197730064392, Final Batch Loss: 0.5158671736717224\n",
      "Epoch 504, Loss: 1.0192362070083618, Final Batch Loss: 0.4963333010673523\n",
      "Epoch 505, Loss: 1.0038259029388428, Final Batch Loss: 0.47453874349594116\n",
      "Epoch 506, Loss: 1.0181587040424347, Final Batch Loss: 0.5731381177902222\n",
      "Epoch 507, Loss: 1.0903424620628357, Final Batch Loss: 0.5588480830192566\n",
      "Epoch 508, Loss: 0.9779854714870453, Final Batch Loss: 0.4812398850917816\n",
      "Epoch 509, Loss: 1.0235744416713715, Final Batch Loss: 0.5310498476028442\n",
      "Epoch 510, Loss: 1.0164758265018463, Final Batch Loss: 0.5452112555503845\n",
      "Epoch 511, Loss: 0.9725524485111237, Final Batch Loss: 0.4885704815387726\n",
      "Epoch 512, Loss: 0.9266637563705444, Final Batch Loss: 0.4127717614173889\n",
      "Epoch 513, Loss: 1.0712806582450867, Final Batch Loss: 0.5366286635398865\n",
      "Epoch 514, Loss: 1.0728875696659088, Final Batch Loss: 0.5977102518081665\n",
      "Epoch 515, Loss: 0.9674757421016693, Final Batch Loss: 0.469413161277771\n",
      "Epoch 516, Loss: 0.9698373973369598, Final Batch Loss: 0.5174239873886108\n",
      "Epoch 517, Loss: 1.0148983597755432, Final Batch Loss: 0.547199010848999\n",
      "Epoch 518, Loss: 0.9867278933525085, Final Batch Loss: 0.5069872140884399\n",
      "Epoch 519, Loss: 1.0169574618339539, Final Batch Loss: 0.4364476799964905\n",
      "Epoch 520, Loss: 0.9938655495643616, Final Batch Loss: 0.4927818775177002\n",
      "Epoch 521, Loss: 1.0080169439315796, Final Batch Loss: 0.48873287439346313\n",
      "Epoch 522, Loss: 1.0693708658218384, Final Batch Loss: 0.539935827255249\n",
      "Epoch 523, Loss: 1.009282112121582, Final Batch Loss: 0.5113330483436584\n",
      "Epoch 524, Loss: 1.008300542831421, Final Batch Loss: 0.5052489638328552\n",
      "Epoch 525, Loss: 1.0356691479682922, Final Batch Loss: 0.5264483094215393\n",
      "Epoch 526, Loss: 1.0320776104927063, Final Batch Loss: 0.5287777781486511\n",
      "Epoch 527, Loss: 1.0397933721542358, Final Batch Loss: 0.5667660236358643\n",
      "Epoch 528, Loss: 0.9440389275550842, Final Batch Loss: 0.4837273955345154\n",
      "Epoch 529, Loss: 0.9772210419178009, Final Batch Loss: 0.47234275937080383\n",
      "Epoch 530, Loss: 0.9549132287502289, Final Batch Loss: 0.5185602903366089\n",
      "Epoch 531, Loss: 1.0185114741325378, Final Batch Loss: 0.5353510975837708\n",
      "Epoch 532, Loss: 1.0360288619995117, Final Batch Loss: 0.5317625999450684\n",
      "Epoch 533, Loss: 0.928611159324646, Final Batch Loss: 0.43676215410232544\n",
      "Epoch 534, Loss: 1.0051100850105286, Final Batch Loss: 0.46944504976272583\n",
      "Epoch 535, Loss: 0.99423548579216, Final Batch Loss: 0.5238541960716248\n",
      "Epoch 536, Loss: 0.985266923904419, Final Batch Loss: 0.5114867091178894\n",
      "Epoch 537, Loss: 1.0287982523441315, Final Batch Loss: 0.4954630434513092\n",
      "Epoch 538, Loss: 1.0103152394294739, Final Batch Loss: 0.4992607831954956\n",
      "Epoch 539, Loss: 1.0193946361541748, Final Batch Loss: 0.5136979818344116\n",
      "Epoch 540, Loss: 1.0240109860897064, Final Batch Loss: 0.5449400544166565\n",
      "Epoch 541, Loss: 1.0492236614227295, Final Batch Loss: 0.5159127116203308\n",
      "Epoch 542, Loss: 0.9523102045059204, Final Batch Loss: 0.49104043841362\n",
      "Epoch 543, Loss: 0.9636456072330475, Final Batch Loss: 0.5123645663261414\n",
      "Epoch 544, Loss: 1.0179141759872437, Final Batch Loss: 0.5083577632904053\n",
      "Epoch 545, Loss: 1.0730476081371307, Final Batch Loss: 0.5859665870666504\n",
      "Epoch 546, Loss: 0.9791862666606903, Final Batch Loss: 0.48860934376716614\n",
      "Epoch 547, Loss: 0.997819185256958, Final Batch Loss: 0.5068642497062683\n",
      "Epoch 548, Loss: 1.0092763602733612, Final Batch Loss: 0.5380404591560364\n",
      "Epoch 549, Loss: 1.0026851892471313, Final Batch Loss: 0.5239640474319458\n",
      "Epoch 550, Loss: 1.0028665363788605, Final Batch Loss: 0.5257852673530579\n",
      "Epoch 551, Loss: 1.0461680889129639, Final Batch Loss: 0.5450206398963928\n",
      "Epoch 552, Loss: 1.0173497796058655, Final Batch Loss: 0.4971877336502075\n",
      "Epoch 553, Loss: 1.0202656984329224, Final Batch Loss: 0.5092077255249023\n",
      "Epoch 554, Loss: 0.9214937388896942, Final Batch Loss: 0.42846301198005676\n",
      "Epoch 555, Loss: 0.9846226274967194, Final Batch Loss: 0.43940845131874084\n",
      "Epoch 556, Loss: 0.9920576810836792, Final Batch Loss: 0.5012962818145752\n",
      "Epoch 557, Loss: 0.9289351999759674, Final Batch Loss: 0.44896286725997925\n",
      "Epoch 558, Loss: 1.0217849910259247, Final Batch Loss: 0.4878794252872467\n",
      "Epoch 559, Loss: 0.9708096981048584, Final Batch Loss: 0.4734032154083252\n",
      "Epoch 560, Loss: 0.9644947946071625, Final Batch Loss: 0.47358155250549316\n",
      "Epoch 561, Loss: 0.9452467262744904, Final Batch Loss: 0.4496425986289978\n",
      "Epoch 562, Loss: 0.9730914831161499, Final Batch Loss: 0.5203540325164795\n",
      "Epoch 563, Loss: 0.9182906746864319, Final Batch Loss: 0.4534251391887665\n",
      "Epoch 564, Loss: 1.0442359447479248, Final Batch Loss: 0.4760274291038513\n",
      "Epoch 565, Loss: 0.9318886697292328, Final Batch Loss: 0.43106773495674133\n",
      "Epoch 566, Loss: 0.9610835611820221, Final Batch Loss: 0.48382943868637085\n",
      "Epoch 567, Loss: 1.0080979466438293, Final Batch Loss: 0.5603969693183899\n",
      "Epoch 568, Loss: 1.0053097009658813, Final Batch Loss: 0.5050008296966553\n",
      "Epoch 569, Loss: 0.9620887935161591, Final Batch Loss: 0.5041214823722839\n",
      "Epoch 570, Loss: 0.9619071781635284, Final Batch Loss: 0.4889182448387146\n",
      "Epoch 571, Loss: 1.0427580773830414, Final Batch Loss: 0.5460289120674133\n",
      "Epoch 572, Loss: 0.9294010102748871, Final Batch Loss: 0.44432225823402405\n",
      "Epoch 573, Loss: 0.965710461139679, Final Batch Loss: 0.4580460786819458\n",
      "Epoch 574, Loss: 0.9796697795391083, Final Batch Loss: 0.48443469405174255\n",
      "Epoch 575, Loss: 0.9987097978591919, Final Batch Loss: 0.4758375883102417\n",
      "Epoch 576, Loss: 1.0368948578834534, Final Batch Loss: 0.567617654800415\n",
      "Epoch 577, Loss: 1.0625896453857422, Final Batch Loss: 0.5168706774711609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 578, Loss: 0.9650138914585114, Final Batch Loss: 0.45631179213523865\n",
      "Epoch 579, Loss: 0.9650839269161224, Final Batch Loss: 0.5197252035140991\n",
      "Epoch 580, Loss: 0.9999946653842926, Final Batch Loss: 0.5064185261726379\n",
      "Epoch 581, Loss: 0.956196516752243, Final Batch Loss: 0.4646238386631012\n",
      "Epoch 582, Loss: 0.9251269698143005, Final Batch Loss: 0.43854549527168274\n",
      "Epoch 583, Loss: 0.9819791913032532, Final Batch Loss: 0.5095769762992859\n",
      "Epoch 584, Loss: 1.048805981874466, Final Batch Loss: 0.572364091873169\n",
      "Epoch 585, Loss: 0.9845568835735321, Final Batch Loss: 0.5186105370521545\n",
      "Epoch 586, Loss: 1.0501866340637207, Final Batch Loss: 0.5449257493019104\n",
      "Epoch 587, Loss: 0.9368450045585632, Final Batch Loss: 0.4661185145378113\n",
      "Epoch 588, Loss: 0.9505071341991425, Final Batch Loss: 0.45205047726631165\n",
      "Epoch 589, Loss: 0.9957376718521118, Final Batch Loss: 0.5134995579719543\n",
      "Epoch 590, Loss: 0.9587345719337463, Final Batch Loss: 0.49012595415115356\n",
      "Epoch 591, Loss: 0.9941258728504181, Final Batch Loss: 0.540157675743103\n",
      "Epoch 592, Loss: 0.9857002198696136, Final Batch Loss: 0.44924530386924744\n",
      "Epoch 593, Loss: 0.9447514414787292, Final Batch Loss: 0.4795253574848175\n",
      "Epoch 594, Loss: 0.9997852742671967, Final Batch Loss: 0.4749676287174225\n",
      "Epoch 595, Loss: 0.9967800378799438, Final Batch Loss: 0.5031769871711731\n",
      "Epoch 596, Loss: 1.0057462751865387, Final Batch Loss: 0.4815863072872162\n",
      "Epoch 597, Loss: 0.954864114522934, Final Batch Loss: 0.46344470977783203\n",
      "Epoch 598, Loss: 0.9862024188041687, Final Batch Loss: 0.5285654067993164\n",
      "Epoch 599, Loss: 0.9669869840145111, Final Batch Loss: 0.46652504801750183\n",
      "Epoch 600, Loss: 0.9880115985870361, Final Batch Loss: 0.5244162082672119\n",
      "Epoch 601, Loss: 0.9953544437885284, Final Batch Loss: 0.4726286828517914\n",
      "Epoch 602, Loss: 0.9693272411823273, Final Batch Loss: 0.4814333915710449\n",
      "Epoch 603, Loss: 0.9450454711914062, Final Batch Loss: 0.4612584710121155\n",
      "Epoch 604, Loss: 0.9448988139629364, Final Batch Loss: 0.4692224860191345\n",
      "Epoch 605, Loss: 0.9804888665676117, Final Batch Loss: 0.4812176823616028\n",
      "Epoch 606, Loss: 1.1034972667694092, Final Batch Loss: 0.5984712839126587\n",
      "Epoch 607, Loss: 0.94896399974823, Final Batch Loss: 0.4316539764404297\n",
      "Epoch 608, Loss: 0.9484654068946838, Final Batch Loss: 0.4737890660762787\n",
      "Epoch 609, Loss: 1.048419713973999, Final Batch Loss: 0.6051287651062012\n",
      "Epoch 610, Loss: 0.965102344751358, Final Batch Loss: 0.47096267342567444\n",
      "Epoch 611, Loss: 0.9744693040847778, Final Batch Loss: 0.5160150527954102\n",
      "Epoch 612, Loss: 0.9748619198799133, Final Batch Loss: 0.4920046627521515\n",
      "Epoch 613, Loss: 0.9551040828227997, Final Batch Loss: 0.5018123388290405\n",
      "Epoch 614, Loss: 0.9460223913192749, Final Batch Loss: 0.4335123300552368\n",
      "Epoch 615, Loss: 0.9366975128650665, Final Batch Loss: 0.48704633116722107\n",
      "Epoch 616, Loss: 1.0354046523571014, Final Batch Loss: 0.4987630546092987\n",
      "Epoch 617, Loss: 0.9810045063495636, Final Batch Loss: 0.5223351716995239\n",
      "Epoch 618, Loss: 0.9875237047672272, Final Batch Loss: 0.5743352770805359\n",
      "Epoch 619, Loss: 0.9709153771400452, Final Batch Loss: 0.45299094915390015\n",
      "Epoch 620, Loss: 0.9535937607288361, Final Batch Loss: 0.5099466443061829\n",
      "Epoch 621, Loss: 0.9576662480831146, Final Batch Loss: 0.47524645924568176\n",
      "Epoch 622, Loss: 0.9458139538764954, Final Batch Loss: 0.4534398317337036\n",
      "Epoch 623, Loss: 0.9347866475582123, Final Batch Loss: 0.4844922125339508\n",
      "Epoch 624, Loss: 0.9754127264022827, Final Batch Loss: 0.5178584456443787\n",
      "Epoch 625, Loss: 0.9507887065410614, Final Batch Loss: 0.4436085522174835\n",
      "Epoch 626, Loss: 0.9857925176620483, Final Batch Loss: 0.49453192949295044\n",
      "Epoch 627, Loss: 1.022126317024231, Final Batch Loss: 0.5607075095176697\n",
      "Epoch 628, Loss: 0.9513820707798004, Final Batch Loss: 0.4448682963848114\n",
      "Epoch 629, Loss: 0.9774671792984009, Final Batch Loss: 0.465451180934906\n",
      "Epoch 630, Loss: 0.9735846519470215, Final Batch Loss: 0.47145628929138184\n",
      "Epoch 631, Loss: 0.9649029672145844, Final Batch Loss: 0.47258245944976807\n",
      "Epoch 632, Loss: 0.9712820053100586, Final Batch Loss: 0.4883051812648773\n",
      "Epoch 633, Loss: 1.006059318780899, Final Batch Loss: 0.5098856687545776\n",
      "Epoch 634, Loss: 0.9615396559238434, Final Batch Loss: 0.494632363319397\n",
      "Epoch 635, Loss: 0.9660444557666779, Final Batch Loss: 0.46214404702186584\n",
      "Epoch 636, Loss: 0.9182183742523193, Final Batch Loss: 0.44378092885017395\n",
      "Epoch 637, Loss: 0.9688949584960938, Final Batch Loss: 0.509718656539917\n",
      "Epoch 638, Loss: 0.9798022508621216, Final Batch Loss: 0.5017969012260437\n",
      "Epoch 639, Loss: 0.924591451883316, Final Batch Loss: 0.4748336970806122\n",
      "Epoch 640, Loss: 0.9100857377052307, Final Batch Loss: 0.43150031566619873\n",
      "Epoch 641, Loss: 0.97954460978508, Final Batch Loss: 0.47865548729896545\n",
      "Epoch 642, Loss: 0.9193693399429321, Final Batch Loss: 0.44557562470436096\n",
      "Epoch 643, Loss: 0.9359133243560791, Final Batch Loss: 0.522328794002533\n",
      "Epoch 644, Loss: 0.9227771759033203, Final Batch Loss: 0.4420626163482666\n",
      "Epoch 645, Loss: 0.946428507566452, Final Batch Loss: 0.4733734130859375\n",
      "Epoch 646, Loss: 0.9387161135673523, Final Batch Loss: 0.4757131338119507\n",
      "Epoch 647, Loss: 0.9155008494853973, Final Batch Loss: 0.4672558903694153\n",
      "Epoch 648, Loss: 0.9277916550636292, Final Batch Loss: 0.43566668033599854\n",
      "Epoch 649, Loss: 0.9475599527359009, Final Batch Loss: 0.45735087990760803\n",
      "Epoch 650, Loss: 0.9822406768798828, Final Batch Loss: 0.4912737011909485\n",
      "Epoch 651, Loss: 1.0445398688316345, Final Batch Loss: 0.5142519474029541\n",
      "Epoch 652, Loss: 0.9372464418411255, Final Batch Loss: 0.46321457624435425\n",
      "Epoch 653, Loss: 0.9613611698150635, Final Batch Loss: 0.475784033536911\n",
      "Epoch 654, Loss: 0.9522909820079803, Final Batch Loss: 0.48519518971443176\n",
      "Epoch 655, Loss: 0.9572876691818237, Final Batch Loss: 0.4986322522163391\n",
      "Epoch 656, Loss: 0.9797395765781403, Final Batch Loss: 0.5005362033843994\n",
      "Epoch 657, Loss: 0.9800218641757965, Final Batch Loss: 0.49332574009895325\n",
      "Epoch 658, Loss: 1.0079539120197296, Final Batch Loss: 0.4971962869167328\n",
      "Epoch 659, Loss: 0.9546631276607513, Final Batch Loss: 0.4963679909706116\n",
      "Epoch 660, Loss: 0.9975279271602631, Final Batch Loss: 0.5613759756088257\n",
      "Epoch 661, Loss: 0.9730028510093689, Final Batch Loss: 0.5024574995040894\n",
      "Epoch 662, Loss: 0.9785766899585724, Final Batch Loss: 0.5073456764221191\n",
      "Epoch 663, Loss: 0.9889592230319977, Final Batch Loss: 0.5174123048782349\n",
      "Epoch 664, Loss: 0.9634886980056763, Final Batch Loss: 0.49821656942367554\n",
      "Epoch 665, Loss: 0.9533305168151855, Final Batch Loss: 0.4966041147708893\n",
      "Epoch 666, Loss: 0.9439080655574799, Final Batch Loss: 0.5219976305961609\n",
      "Epoch 667, Loss: 0.9461182653903961, Final Batch Loss: 0.4776657223701477\n",
      "Epoch 668, Loss: 0.9320578873157501, Final Batch Loss: 0.4818248152732849\n",
      "Epoch 669, Loss: 0.9507429897785187, Final Batch Loss: 0.47600120306015015\n",
      "Epoch 670, Loss: 0.9312783777713776, Final Batch Loss: 0.4687317907810211\n",
      "Epoch 671, Loss: 0.9536568224430084, Final Batch Loss: 0.4283996522426605\n",
      "Epoch 672, Loss: 0.8952919542789459, Final Batch Loss: 0.4679696261882782\n",
      "Epoch 673, Loss: 0.9154272675514221, Final Batch Loss: 0.4702402651309967\n",
      "Epoch 674, Loss: 0.94228595495224, Final Batch Loss: 0.5090124607086182\n",
      "Epoch 675, Loss: 0.8924621641635895, Final Batch Loss: 0.4197356700897217\n",
      "Epoch 676, Loss: 0.9447813332080841, Final Batch Loss: 0.48165321350097656\n",
      "Epoch 677, Loss: 0.9586063325405121, Final Batch Loss: 0.4691592752933502\n",
      "Epoch 678, Loss: 0.9787985682487488, Final Batch Loss: 0.4868573844432831\n",
      "Epoch 679, Loss: 0.9625204801559448, Final Batch Loss: 0.4561907649040222\n",
      "Epoch 680, Loss: 0.8875435292720795, Final Batch Loss: 0.4163108170032501\n",
      "Epoch 681, Loss: 0.9360735416412354, Final Batch Loss: 0.40896159410476685\n",
      "Epoch 682, Loss: 0.9179502427577972, Final Batch Loss: 0.5064605474472046\n",
      "Epoch 683, Loss: 0.8922454416751862, Final Batch Loss: 0.4599680006504059\n",
      "Epoch 684, Loss: 0.9302361011505127, Final Batch Loss: 0.46377959847450256\n",
      "Epoch 685, Loss: 0.9350453019142151, Final Batch Loss: 0.45913663506507874\n",
      "Epoch 686, Loss: 0.9316692352294922, Final Batch Loss: 0.4675581157207489\n",
      "Epoch 687, Loss: 0.907787024974823, Final Batch Loss: 0.42304089665412903\n",
      "Epoch 688, Loss: 0.9300220012664795, Final Batch Loss: 0.46265357732772827\n",
      "Epoch 689, Loss: 0.9686191082000732, Final Batch Loss: 0.4622209072113037\n",
      "Epoch 690, Loss: 0.9614892899990082, Final Batch Loss: 0.5233221054077148\n",
      "Epoch 691, Loss: 0.9707672297954559, Final Batch Loss: 0.456766277551651\n",
      "Epoch 692, Loss: 0.9401267468929291, Final Batch Loss: 0.4934358596801758\n",
      "Epoch 693, Loss: 0.8938073813915253, Final Batch Loss: 0.4060373902320862\n",
      "Epoch 694, Loss: 0.9434634745121002, Final Batch Loss: 0.4581708610057831\n",
      "Epoch 695, Loss: 0.9278765916824341, Final Batch Loss: 0.4324546158313751\n",
      "Epoch 696, Loss: 0.9295415282249451, Final Batch Loss: 0.486777663230896\n",
      "Epoch 697, Loss: 0.9209837317466736, Final Batch Loss: 0.44726458191871643\n",
      "Epoch 698, Loss: 0.9066250920295715, Final Batch Loss: 0.44352245330810547\n",
      "Epoch 699, Loss: 0.9571948051452637, Final Batch Loss: 0.5084983706474304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700, Loss: 0.951145201921463, Final Batch Loss: 0.5283325910568237\n",
      "Epoch 701, Loss: 0.9443357884883881, Final Batch Loss: 0.4941025674343109\n",
      "Epoch 702, Loss: 0.9445434212684631, Final Batch Loss: 0.48556607961654663\n",
      "Epoch 703, Loss: 0.8788318037986755, Final Batch Loss: 0.3862670361995697\n",
      "Epoch 704, Loss: 0.9679847657680511, Final Batch Loss: 0.4864865243434906\n",
      "Epoch 705, Loss: 0.9391191303730011, Final Batch Loss: 0.4757363796234131\n",
      "Epoch 706, Loss: 0.9361394941806793, Final Batch Loss: 0.44278019666671753\n",
      "Epoch 707, Loss: 0.8944839239120483, Final Batch Loss: 0.43849489092826843\n",
      "Epoch 708, Loss: 0.963970422744751, Final Batch Loss: 0.47492775321006775\n",
      "Epoch 709, Loss: 0.8970241844654083, Final Batch Loss: 0.4691097140312195\n",
      "Epoch 710, Loss: 0.8975041210651398, Final Batch Loss: 0.4436049163341522\n",
      "Epoch 711, Loss: 0.93348628282547, Final Batch Loss: 0.48155835270881653\n",
      "Epoch 712, Loss: 0.9165236353874207, Final Batch Loss: 0.4664384424686432\n",
      "Epoch 713, Loss: 0.9672332108020782, Final Batch Loss: 0.4643356502056122\n",
      "Epoch 714, Loss: 0.9386729001998901, Final Batch Loss: 0.4940319061279297\n",
      "Epoch 715, Loss: 0.9867434203624725, Final Batch Loss: 0.5103598833084106\n",
      "Epoch 716, Loss: 0.9040213525295258, Final Batch Loss: 0.443377286195755\n",
      "Epoch 717, Loss: 0.9744769036769867, Final Batch Loss: 0.47750943899154663\n",
      "Epoch 718, Loss: 0.9634343385696411, Final Batch Loss: 0.48448696732521057\n",
      "Epoch 719, Loss: 0.905176043510437, Final Batch Loss: 0.4121716320514679\n",
      "Epoch 720, Loss: 0.9277946352958679, Final Batch Loss: 0.4847038686275482\n",
      "Epoch 721, Loss: 0.974445104598999, Final Batch Loss: 0.5571720004081726\n",
      "Epoch 722, Loss: 0.9365756809711456, Final Batch Loss: 0.461515873670578\n",
      "Epoch 723, Loss: 0.9483583569526672, Final Batch Loss: 0.48013997077941895\n",
      "Epoch 724, Loss: 0.9546991884708405, Final Batch Loss: 0.4495292007923126\n",
      "Epoch 725, Loss: 0.8968007564544678, Final Batch Loss: 0.44199225306510925\n",
      "Epoch 726, Loss: 0.9487879872322083, Final Batch Loss: 0.5234996676445007\n",
      "Epoch 727, Loss: 0.9231411814689636, Final Batch Loss: 0.46064943075180054\n",
      "Epoch 728, Loss: 0.9306207597255707, Final Batch Loss: 0.45898982882499695\n",
      "Epoch 729, Loss: 0.9264435470104218, Final Batch Loss: 0.4956868886947632\n",
      "Epoch 730, Loss: 0.9184058606624603, Final Batch Loss: 0.45306840538978577\n",
      "Epoch 731, Loss: 0.9368657469749451, Final Batch Loss: 0.47669893503189087\n",
      "Epoch 732, Loss: 0.919910192489624, Final Batch Loss: 0.45276007056236267\n",
      "Epoch 733, Loss: 0.9206981360912323, Final Batch Loss: 0.446890264749527\n",
      "Epoch 734, Loss: 0.907182902097702, Final Batch Loss: 0.43710237741470337\n",
      "Epoch 735, Loss: 0.9003417193889618, Final Batch Loss: 0.4264192581176758\n",
      "Epoch 736, Loss: 0.913860410451889, Final Batch Loss: 0.48539191484451294\n",
      "Epoch 737, Loss: 0.9623242318630219, Final Batch Loss: 0.4769750237464905\n",
      "Epoch 738, Loss: 0.9299776554107666, Final Batch Loss: 0.4949837028980255\n",
      "Epoch 739, Loss: 0.9212845861911774, Final Batch Loss: 0.43962448835372925\n",
      "Epoch 740, Loss: 0.9088131487369537, Final Batch Loss: 0.43174663186073303\n",
      "Epoch 741, Loss: 0.9530976414680481, Final Batch Loss: 0.46987470984458923\n",
      "Epoch 742, Loss: 0.9400853216648102, Final Batch Loss: 0.45951616764068604\n",
      "Epoch 743, Loss: 0.9195359647274017, Final Batch Loss: 0.475047767162323\n",
      "Epoch 744, Loss: 0.9416283071041107, Final Batch Loss: 0.4704536497592926\n",
      "Epoch 745, Loss: 0.9150682091712952, Final Batch Loss: 0.4708506762981415\n",
      "Epoch 746, Loss: 0.9174644351005554, Final Batch Loss: 0.44998878240585327\n",
      "Epoch 747, Loss: 0.8509190082550049, Final Batch Loss: 0.4147636294364929\n",
      "Epoch 748, Loss: 0.8862815201282501, Final Batch Loss: 0.3858937919139862\n",
      "Epoch 749, Loss: 0.9983482956886292, Final Batch Loss: 0.5263512134552002\n",
      "Epoch 750, Loss: 0.9026675224304199, Final Batch Loss: 0.47386422753334045\n",
      "Epoch 751, Loss: 0.9173513352870941, Final Batch Loss: 0.4786759912967682\n",
      "Epoch 752, Loss: 0.937309592962265, Final Batch Loss: 0.4672340154647827\n",
      "Epoch 753, Loss: 0.9134020805358887, Final Batch Loss: 0.46374890208244324\n",
      "Epoch 754, Loss: 0.8867642879486084, Final Batch Loss: 0.4026138484477997\n",
      "Epoch 755, Loss: 0.932918131351471, Final Batch Loss: 0.4855797588825226\n",
      "Epoch 756, Loss: 0.9442232549190521, Final Batch Loss: 0.5033289194107056\n",
      "Epoch 757, Loss: 0.9215196967124939, Final Batch Loss: 0.4946940541267395\n",
      "Epoch 758, Loss: 0.924015611410141, Final Batch Loss: 0.460573673248291\n",
      "Epoch 759, Loss: 0.9175634384155273, Final Batch Loss: 0.42925316095352173\n",
      "Epoch 760, Loss: 0.8975644409656525, Final Batch Loss: 0.42624086141586304\n",
      "Epoch 761, Loss: 0.9385888576507568, Final Batch Loss: 0.4665084481239319\n",
      "Epoch 762, Loss: 0.8902042806148529, Final Batch Loss: 0.44448649883270264\n",
      "Epoch 763, Loss: 0.8754451870918274, Final Batch Loss: 0.41790392994880676\n",
      "Epoch 764, Loss: 0.9688398241996765, Final Batch Loss: 0.447501540184021\n",
      "Epoch 765, Loss: 0.9152736663818359, Final Batch Loss: 0.41818153858184814\n",
      "Epoch 766, Loss: 0.8846711814403534, Final Batch Loss: 0.44588425755500793\n",
      "Epoch 767, Loss: 0.8738493919372559, Final Batch Loss: 0.4050949811935425\n",
      "Epoch 768, Loss: 0.9306928813457489, Final Batch Loss: 0.5016558170318604\n",
      "Epoch 769, Loss: 0.8745340704917908, Final Batch Loss: 0.3895561099052429\n",
      "Epoch 770, Loss: 0.9127123653888702, Final Batch Loss: 0.42493507266044617\n",
      "Epoch 771, Loss: 0.9548267722129822, Final Batch Loss: 0.47934356331825256\n",
      "Epoch 772, Loss: 0.8873320519924164, Final Batch Loss: 0.40288370847702026\n",
      "Epoch 773, Loss: 0.8745863735675812, Final Batch Loss: 0.41991525888442993\n",
      "Epoch 774, Loss: 0.8967345356941223, Final Batch Loss: 0.4287479817867279\n",
      "Epoch 775, Loss: 0.9311843812465668, Final Batch Loss: 0.43109986186027527\n",
      "Epoch 776, Loss: 0.9096803963184357, Final Batch Loss: 0.45322009921073914\n",
      "Epoch 777, Loss: 0.9093723893165588, Final Batch Loss: 0.436984658241272\n",
      "Epoch 778, Loss: 0.8510720729827881, Final Batch Loss: 0.37084150314331055\n",
      "Epoch 779, Loss: 0.8892292678356171, Final Batch Loss: 0.4092630445957184\n",
      "Epoch 780, Loss: 0.9491565823554993, Final Batch Loss: 0.5002457499504089\n",
      "Epoch 781, Loss: 0.8831146955490112, Final Batch Loss: 0.44814324378967285\n",
      "Epoch 782, Loss: 0.9072778522968292, Final Batch Loss: 0.47898736596107483\n",
      "Epoch 783, Loss: 0.9126710891723633, Final Batch Loss: 0.4722830653190613\n",
      "Epoch 784, Loss: 0.9758535325527191, Final Batch Loss: 0.5461178421974182\n",
      "Epoch 785, Loss: 0.8965460658073425, Final Batch Loss: 0.4866099953651428\n",
      "Epoch 786, Loss: 0.91376593708992, Final Batch Loss: 0.4620673656463623\n",
      "Epoch 787, Loss: 0.9551064670085907, Final Batch Loss: 0.5100250244140625\n",
      "Epoch 788, Loss: 0.8736886382102966, Final Batch Loss: 0.43126726150512695\n",
      "Epoch 789, Loss: 0.9121692776679993, Final Batch Loss: 0.46722912788391113\n",
      "Epoch 790, Loss: 0.9318382143974304, Final Batch Loss: 0.4775935709476471\n",
      "Epoch 791, Loss: 0.9385804235935211, Final Batch Loss: 0.4466780126094818\n",
      "Epoch 792, Loss: 0.8734127581119537, Final Batch Loss: 0.44377121329307556\n",
      "Epoch 793, Loss: 0.9131330251693726, Final Batch Loss: 0.48851174116134644\n",
      "Epoch 794, Loss: 0.8859824240207672, Final Batch Loss: 0.45890185236930847\n",
      "Epoch 795, Loss: 0.874030590057373, Final Batch Loss: 0.4161440432071686\n",
      "Epoch 796, Loss: 0.9001730680465698, Final Batch Loss: 0.4690457284450531\n",
      "Epoch 797, Loss: 0.9260893762111664, Final Batch Loss: 0.5050007104873657\n",
      "Epoch 798, Loss: 0.8887540400028229, Final Batch Loss: 0.48151037096977234\n",
      "Epoch 799, Loss: 0.9040331840515137, Final Batch Loss: 0.47324681282043457\n",
      "Epoch 800, Loss: 0.9171250760555267, Final Batch Loss: 0.47875097393989563\n",
      "Epoch 801, Loss: 0.8555895984172821, Final Batch Loss: 0.4130754768848419\n",
      "Epoch 802, Loss: 0.8692400753498077, Final Batch Loss: 0.42033568024635315\n",
      "Epoch 803, Loss: 0.8815777003765106, Final Batch Loss: 0.46864214539527893\n",
      "Epoch 804, Loss: 0.8937044441699982, Final Batch Loss: 0.42583706974983215\n",
      "Epoch 805, Loss: 0.9529224038124084, Final Batch Loss: 0.513340413570404\n",
      "Epoch 806, Loss: 0.9049921035766602, Final Batch Loss: 0.41500696539878845\n",
      "Epoch 807, Loss: 0.9009285569190979, Final Batch Loss: 0.42249593138694763\n",
      "Epoch 808, Loss: 0.9184855818748474, Final Batch Loss: 0.4930794835090637\n",
      "Epoch 809, Loss: 0.8954252302646637, Final Batch Loss: 0.41222572326660156\n",
      "Epoch 810, Loss: 0.8795715868473053, Final Batch Loss: 0.4004950523376465\n",
      "Epoch 811, Loss: 0.8968456983566284, Final Batch Loss: 0.47846001386642456\n",
      "Epoch 812, Loss: 0.9245306849479675, Final Batch Loss: 0.43434086441993713\n",
      "Epoch 813, Loss: 0.9242135286331177, Final Batch Loss: 0.4645666778087616\n",
      "Epoch 814, Loss: 0.9270093142986298, Final Batch Loss: 0.4683782160282135\n",
      "Epoch 815, Loss: 0.9013782739639282, Final Batch Loss: 0.4738682210445404\n",
      "Epoch 816, Loss: 0.8464834988117218, Final Batch Loss: 0.41077330708503723\n",
      "Epoch 817, Loss: 0.8819331526756287, Final Batch Loss: 0.4240138828754425\n",
      "Epoch 818, Loss: 0.9106498062610626, Final Batch Loss: 0.4519292414188385\n",
      "Epoch 819, Loss: 0.8524118065834045, Final Batch Loss: 0.4060311019420624\n",
      "Epoch 820, Loss: 0.8955434560775757, Final Batch Loss: 0.44775626063346863\n",
      "Epoch 821, Loss: 0.9122608602046967, Final Batch Loss: 0.4217354953289032\n",
      "Epoch 822, Loss: 0.8933184146881104, Final Batch Loss: 0.4265320897102356\n",
      "Epoch 823, Loss: 0.8633107542991638, Final Batch Loss: 0.3992186486721039\n",
      "Epoch 824, Loss: 0.9055738151073456, Final Batch Loss: 0.38942524790763855\n",
      "Epoch 825, Loss: 0.8994992971420288, Final Batch Loss: 0.43658241629600525\n",
      "Epoch 826, Loss: 0.8746543824672699, Final Batch Loss: 0.41856643557548523\n",
      "Epoch 827, Loss: 0.8649619817733765, Final Batch Loss: 0.41037046909332275\n",
      "Epoch 828, Loss: 0.9462070465087891, Final Batch Loss: 0.4997000992298126\n",
      "Epoch 829, Loss: 0.8545742630958557, Final Batch Loss: 0.3685767352581024\n",
      "Epoch 830, Loss: 0.9395565390586853, Final Batch Loss: 0.4939834773540497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 831, Loss: 0.9368621706962585, Final Batch Loss: 0.5173978209495544\n",
      "Epoch 832, Loss: 0.8868185579776764, Final Batch Loss: 0.47392600774765015\n",
      "Epoch 833, Loss: 0.8848235607147217, Final Batch Loss: 0.4105037450790405\n",
      "Epoch 834, Loss: 0.867054283618927, Final Batch Loss: 0.42798489332199097\n",
      "Epoch 835, Loss: 0.91917285323143, Final Batch Loss: 0.479828804731369\n",
      "Epoch 836, Loss: 0.9554231762886047, Final Batch Loss: 0.5108548998832703\n",
      "Epoch 837, Loss: 0.8961870968341827, Final Batch Loss: 0.42457762360572815\n",
      "Epoch 838, Loss: 0.9200645983219147, Final Batch Loss: 0.4932178258895874\n",
      "Epoch 839, Loss: 0.9191626608371735, Final Batch Loss: 0.4799892008304596\n",
      "Epoch 840, Loss: 0.8955626487731934, Final Batch Loss: 0.43445321917533875\n",
      "Epoch 841, Loss: 0.9256335198879242, Final Batch Loss: 0.4701683521270752\n",
      "Epoch 842, Loss: 0.885389894247055, Final Batch Loss: 0.4496075212955475\n",
      "Epoch 843, Loss: 0.939372181892395, Final Batch Loss: 0.5257600545883179\n",
      "Epoch 844, Loss: 0.9656926095485687, Final Batch Loss: 0.5231217741966248\n",
      "Epoch 845, Loss: 0.9096060395240784, Final Batch Loss: 0.48928990960121155\n",
      "Epoch 846, Loss: 0.891810417175293, Final Batch Loss: 0.396597683429718\n",
      "Epoch 847, Loss: 0.9565848112106323, Final Batch Loss: 0.515887975692749\n",
      "Epoch 848, Loss: 0.8329567611217499, Final Batch Loss: 0.4150053560733795\n",
      "Epoch 849, Loss: 0.8684898912906647, Final Batch Loss: 0.4045403003692627\n",
      "Epoch 850, Loss: 0.9258348941802979, Final Batch Loss: 0.4870688319206238\n",
      "Epoch 851, Loss: 0.8664512038230896, Final Batch Loss: 0.4281851053237915\n",
      "Epoch 852, Loss: 0.90476855635643, Final Batch Loss: 0.4371221661567688\n",
      "Epoch 853, Loss: 0.8593351542949677, Final Batch Loss: 0.4081231951713562\n",
      "Epoch 854, Loss: 0.9406216740608215, Final Batch Loss: 0.5124001502990723\n",
      "Epoch 855, Loss: 0.8722239434719086, Final Batch Loss: 0.4401409327983856\n",
      "Epoch 856, Loss: 0.8986515998840332, Final Batch Loss: 0.4532613158226013\n",
      "Epoch 857, Loss: 0.8862848281860352, Final Batch Loss: 0.472624808549881\n",
      "Epoch 858, Loss: 0.9162258207798004, Final Batch Loss: 0.4579944610595703\n",
      "Epoch 859, Loss: 0.8890128135681152, Final Batch Loss: 0.45825663208961487\n",
      "Epoch 860, Loss: 0.8752121031284332, Final Batch Loss: 0.47122442722320557\n",
      "Epoch 861, Loss: 0.873803973197937, Final Batch Loss: 0.4025469124317169\n",
      "Epoch 862, Loss: 0.8894615173339844, Final Batch Loss: 0.4535876512527466\n",
      "Epoch 863, Loss: 0.8786908388137817, Final Batch Loss: 0.4017902612686157\n",
      "Epoch 864, Loss: 0.9703839123249054, Final Batch Loss: 0.47076335549354553\n",
      "Epoch 865, Loss: 0.9505741000175476, Final Batch Loss: 0.4417334794998169\n",
      "Epoch 866, Loss: 0.91193887591362, Final Batch Loss: 0.4361449182033539\n",
      "Epoch 867, Loss: 0.9489035904407501, Final Batch Loss: 0.4332835376262665\n",
      "Epoch 868, Loss: 0.8423429131507874, Final Batch Loss: 0.4361783266067505\n",
      "Epoch 869, Loss: 0.8687658607959747, Final Batch Loss: 0.48190298676490784\n",
      "Epoch 870, Loss: 0.945145457983017, Final Batch Loss: 0.4940918982028961\n",
      "Epoch 871, Loss: 0.8494454324245453, Final Batch Loss: 0.4737047851085663\n",
      "Epoch 872, Loss: 0.8965019285678864, Final Batch Loss: 0.45575040578842163\n",
      "Epoch 873, Loss: 0.9303039312362671, Final Batch Loss: 0.4548177123069763\n",
      "Epoch 874, Loss: 0.8814680874347687, Final Batch Loss: 0.4436916410923004\n",
      "Epoch 875, Loss: 0.8741155564785004, Final Batch Loss: 0.43766263127326965\n",
      "Epoch 876, Loss: 0.8683620393276215, Final Batch Loss: 0.43739354610443115\n",
      "Epoch 877, Loss: 0.958037406206131, Final Batch Loss: 0.51410973072052\n",
      "Epoch 878, Loss: 0.9734155535697937, Final Batch Loss: 0.4548768401145935\n",
      "Epoch 879, Loss: 0.9087215960025787, Final Batch Loss: 0.44191399216651917\n",
      "Epoch 880, Loss: 0.8964041173458099, Final Batch Loss: 0.4892362654209137\n",
      "Epoch 881, Loss: 0.9309239685535431, Final Batch Loss: 0.5324288606643677\n",
      "Epoch 882, Loss: 0.9003503322601318, Final Batch Loss: 0.43215474486351013\n",
      "Epoch 883, Loss: 0.8781652450561523, Final Batch Loss: 0.425232470035553\n",
      "Epoch 884, Loss: 0.8817477226257324, Final Batch Loss: 0.4340387284755707\n",
      "Epoch 885, Loss: 0.896123081445694, Final Batch Loss: 0.45456328988075256\n",
      "Epoch 886, Loss: 0.8596378564834595, Final Batch Loss: 0.41110798716545105\n",
      "Epoch 887, Loss: 0.8820054233074188, Final Batch Loss: 0.48867225646972656\n",
      "Epoch 888, Loss: 0.8996018171310425, Final Batch Loss: 0.4653841555118561\n",
      "Epoch 889, Loss: 0.9064182341098785, Final Batch Loss: 0.424782931804657\n",
      "Epoch 890, Loss: 0.8808862268924713, Final Batch Loss: 0.4654741585254669\n",
      "Epoch 891, Loss: 0.8669456243515015, Final Batch Loss: 0.40607449412345886\n",
      "Epoch 892, Loss: 0.9332187175750732, Final Batch Loss: 0.5061578750610352\n",
      "Epoch 893, Loss: 0.8878129720687866, Final Batch Loss: 0.39927080273628235\n",
      "Epoch 894, Loss: 0.884246438741684, Final Batch Loss: 0.4801352322101593\n",
      "Epoch 895, Loss: 0.9138542711734772, Final Batch Loss: 0.4577271044254303\n",
      "Epoch 896, Loss: 0.8958760201931, Final Batch Loss: 0.46055421233177185\n",
      "Epoch 897, Loss: 0.9215308427810669, Final Batch Loss: 0.4925006628036499\n",
      "Epoch 898, Loss: 0.8598561882972717, Final Batch Loss: 0.40223565697669983\n",
      "Epoch 899, Loss: 0.9111775159835815, Final Batch Loss: 0.4712505340576172\n",
      "Epoch 900, Loss: 0.8725140690803528, Final Batch Loss: 0.3929862380027771\n",
      "Epoch 901, Loss: 0.8939862847328186, Final Batch Loss: 0.4544665515422821\n",
      "Epoch 902, Loss: 0.8621852397918701, Final Batch Loss: 0.42954492568969727\n",
      "Epoch 903, Loss: 0.8622323870658875, Final Batch Loss: 0.3749024271965027\n",
      "Epoch 904, Loss: 0.8743939697742462, Final Batch Loss: 0.4533928632736206\n",
      "Epoch 905, Loss: 0.8598466515541077, Final Batch Loss: 0.45155447721481323\n",
      "Epoch 906, Loss: 0.8772667348384857, Final Batch Loss: 0.43920013308525085\n",
      "Epoch 907, Loss: 0.8520927131175995, Final Batch Loss: 0.3914574980735779\n",
      "Epoch 908, Loss: 0.928977757692337, Final Batch Loss: 0.4419073760509491\n",
      "Epoch 909, Loss: 0.8692832291126251, Final Batch Loss: 0.3893468677997589\n",
      "Epoch 910, Loss: 0.9122083485126495, Final Batch Loss: 0.40865984559059143\n",
      "Epoch 911, Loss: 0.9078027606010437, Final Batch Loss: 0.47408556938171387\n",
      "Epoch 912, Loss: 0.8756901919841766, Final Batch Loss: 0.39026495814323425\n",
      "Epoch 913, Loss: 0.8453093767166138, Final Batch Loss: 0.40452659130096436\n",
      "Epoch 914, Loss: 0.9058470726013184, Final Batch Loss: 0.44575217366218567\n",
      "Epoch 915, Loss: 0.8843143582344055, Final Batch Loss: 0.42020565271377563\n",
      "Epoch 916, Loss: 0.858052670955658, Final Batch Loss: 0.4445244073867798\n",
      "Epoch 917, Loss: 0.8866280317306519, Final Batch Loss: 0.46962958574295044\n",
      "Epoch 918, Loss: 0.9115419983863831, Final Batch Loss: 0.4067516326904297\n",
      "Epoch 919, Loss: 0.9089093208312988, Final Batch Loss: 0.46891990303993225\n",
      "Epoch 920, Loss: 0.869061678647995, Final Batch Loss: 0.4306979775428772\n",
      "Epoch 921, Loss: 0.89670729637146, Final Batch Loss: 0.4550442695617676\n",
      "Epoch 922, Loss: 0.8735104501247406, Final Batch Loss: 0.4148787558078766\n",
      "Epoch 923, Loss: 0.8476582765579224, Final Batch Loss: 0.3905700147151947\n",
      "Epoch 924, Loss: 0.8588418364524841, Final Batch Loss: 0.446158230304718\n",
      "Epoch 925, Loss: 0.8952131867408752, Final Batch Loss: 0.4675692617893219\n",
      "Epoch 926, Loss: 0.9295749068260193, Final Batch Loss: 0.48679012060165405\n",
      "Epoch 927, Loss: 0.8400915265083313, Final Batch Loss: 0.4316547214984894\n",
      "Epoch 928, Loss: 0.9646778106689453, Final Batch Loss: 0.48608094453811646\n",
      "Epoch 929, Loss: 0.9520978331565857, Final Batch Loss: 0.4702706038951874\n",
      "Epoch 930, Loss: 0.8445700407028198, Final Batch Loss: 0.39522701501846313\n",
      "Epoch 931, Loss: 0.8493327498435974, Final Batch Loss: 0.39790526032447815\n",
      "Epoch 932, Loss: 0.9375898241996765, Final Batch Loss: 0.536594808101654\n",
      "Epoch 933, Loss: 0.8969545066356659, Final Batch Loss: 0.47799327969551086\n",
      "Epoch 934, Loss: 0.8490460515022278, Final Batch Loss: 0.4109688401222229\n",
      "Epoch 935, Loss: 0.9501502215862274, Final Batch Loss: 0.5098992586135864\n",
      "Epoch 936, Loss: 0.84848353266716, Final Batch Loss: 0.3966115415096283\n",
      "Epoch 937, Loss: 0.8465463817119598, Final Batch Loss: 0.41607025265693665\n",
      "Epoch 938, Loss: 0.8410000503063202, Final Batch Loss: 0.39498671889305115\n",
      "Epoch 939, Loss: 0.9546396732330322, Final Batch Loss: 0.5536516308784485\n",
      "Epoch 940, Loss: 0.8575705587863922, Final Batch Loss: 0.47005531191825867\n",
      "Epoch 941, Loss: 0.8546587228775024, Final Batch Loss: 0.4291602373123169\n",
      "Epoch 942, Loss: 0.8528744876384735, Final Batch Loss: 0.4215727746486664\n",
      "Epoch 943, Loss: 0.844795435667038, Final Batch Loss: 0.4262332022190094\n",
      "Epoch 944, Loss: 0.8484220802783966, Final Batch Loss: 0.43035653233528137\n",
      "Epoch 945, Loss: 0.9460335969924927, Final Batch Loss: 0.5063835978507996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946, Loss: 0.846623569726944, Final Batch Loss: 0.46125906705856323\n",
      "Epoch 947, Loss: 0.8505944311618805, Final Batch Loss: 0.4464559257030487\n",
      "Epoch 948, Loss: 0.9331512451171875, Final Batch Loss: 0.4851239323616028\n",
      "Epoch 949, Loss: 0.8931310474872589, Final Batch Loss: 0.4470585286617279\n",
      "Epoch 950, Loss: 0.8747761249542236, Final Batch Loss: 0.47166144847869873\n",
      "Epoch 951, Loss: 0.8040561974048615, Final Batch Loss: 0.39753609895706177\n",
      "Epoch 952, Loss: 0.876770406961441, Final Batch Loss: 0.41840001940727234\n",
      "Epoch 953, Loss: 0.8635372817516327, Final Batch Loss: 0.43474888801574707\n",
      "Epoch 954, Loss: 0.8225297629833221, Final Batch Loss: 0.39754223823547363\n",
      "Epoch 955, Loss: 0.8740580379962921, Final Batch Loss: 0.4720953404903412\n",
      "Epoch 956, Loss: 0.8719193637371063, Final Batch Loss: 0.43423891067504883\n",
      "Epoch 957, Loss: 0.8782240450382233, Final Batch Loss: 0.4829672873020172\n",
      "Epoch 958, Loss: 0.8892644941806793, Final Batch Loss: 0.4452162981033325\n",
      "Epoch 959, Loss: 0.8886274993419647, Final Batch Loss: 0.4481349289417267\n",
      "Epoch 960, Loss: 0.8742741048336029, Final Batch Loss: 0.39851436018943787\n",
      "Epoch 961, Loss: 0.8943426311016083, Final Batch Loss: 0.48628363013267517\n",
      "Epoch 962, Loss: 0.904430627822876, Final Batch Loss: 0.43856552243232727\n",
      "Epoch 963, Loss: 0.8338602185249329, Final Batch Loss: 0.43922507762908936\n",
      "Epoch 964, Loss: 0.9120859205722809, Final Batch Loss: 0.51725172996521\n",
      "Epoch 965, Loss: 0.8336821496486664, Final Batch Loss: 0.43121644854545593\n",
      "Epoch 966, Loss: 0.8430856764316559, Final Batch Loss: 0.40250998735427856\n",
      "Epoch 967, Loss: 0.8156614303588867, Final Batch Loss: 0.40671834349632263\n",
      "Epoch 968, Loss: 0.9024814069271088, Final Batch Loss: 0.46567922830581665\n",
      "Epoch 969, Loss: 0.869476318359375, Final Batch Loss: 0.41048741340637207\n",
      "Epoch 970, Loss: 0.8751299381256104, Final Batch Loss: 0.4296565055847168\n",
      "Epoch 971, Loss: 0.8952687084674835, Final Batch Loss: 0.48255136609077454\n",
      "Epoch 972, Loss: 0.8824511766433716, Final Batch Loss: 0.4335339665412903\n",
      "Epoch 973, Loss: 0.9100487232208252, Final Batch Loss: 0.48722267150878906\n",
      "Epoch 974, Loss: 0.8782187402248383, Final Batch Loss: 0.42673537135124207\n",
      "Epoch 975, Loss: 0.8651637136936188, Final Batch Loss: 0.49113228917121887\n",
      "Epoch 976, Loss: 0.9120666980743408, Final Batch Loss: 0.3985709547996521\n",
      "Epoch 977, Loss: 0.8637531101703644, Final Batch Loss: 0.43728214502334595\n",
      "Epoch 978, Loss: 0.8529916405677795, Final Batch Loss: 0.3829169273376465\n",
      "Epoch 979, Loss: 0.8578878343105316, Final Batch Loss: 0.4519382119178772\n",
      "Epoch 980, Loss: 0.854738712310791, Final Batch Loss: 0.4902544319629669\n",
      "Epoch 981, Loss: 0.8959375619888306, Final Batch Loss: 0.4863075911998749\n",
      "Epoch 982, Loss: 0.8843261897563934, Final Batch Loss: 0.45392563939094543\n",
      "Epoch 983, Loss: 0.8937748372554779, Final Batch Loss: 0.4787188470363617\n",
      "Epoch 984, Loss: 0.9187130331993103, Final Batch Loss: 0.43565571308135986\n",
      "Epoch 985, Loss: 0.8535144925117493, Final Batch Loss: 0.41707083582878113\n",
      "Epoch 986, Loss: 0.8480300009250641, Final Batch Loss: 0.40820035338401794\n",
      "Epoch 987, Loss: 0.847040981054306, Final Batch Loss: 0.44396787881851196\n",
      "Epoch 988, Loss: 0.8698026537895203, Final Batch Loss: 0.4657965302467346\n",
      "Epoch 989, Loss: 0.872295081615448, Final Batch Loss: 0.49136656522750854\n",
      "Epoch 990, Loss: 0.8573096990585327, Final Batch Loss: 0.41066455841064453\n",
      "Epoch 991, Loss: 0.8527234792709351, Final Batch Loss: 0.3953683078289032\n",
      "Epoch 992, Loss: 0.8424243927001953, Final Batch Loss: 0.419937402009964\n",
      "Epoch 993, Loss: 0.8404867351055145, Final Batch Loss: 0.4014783203601837\n",
      "Epoch 994, Loss: 0.8498058915138245, Final Batch Loss: 0.4362797439098358\n",
      "Epoch 995, Loss: 0.9542441368103027, Final Batch Loss: 0.47036007046699524\n",
      "Epoch 996, Loss: 0.9071145951747894, Final Batch Loss: 0.42652392387390137\n",
      "Epoch 997, Loss: 0.9318223297595978, Final Batch Loss: 0.43303900957107544\n",
      "Epoch 998, Loss: 0.8625891804695129, Final Batch Loss: 0.4343119263648987\n",
      "Epoch 999, Loss: 0.8247083723545074, Final Batch Loss: 0.38690072298049927\n",
      "Epoch 1000, Loss: 0.8345610201358795, Final Batch Loss: 0.42706558108329773\n",
      "Epoch 1001, Loss: 0.8238934576511383, Final Batch Loss: 0.35681742429733276\n",
      "Epoch 1002, Loss: 0.8693208992481232, Final Batch Loss: 0.49917036294937134\n",
      "Epoch 1003, Loss: 0.8151755630970001, Final Batch Loss: 0.419527530670166\n",
      "Epoch 1004, Loss: 0.8530980348587036, Final Batch Loss: 0.40704086422920227\n",
      "Epoch 1005, Loss: 0.8491950035095215, Final Batch Loss: 0.36662304401397705\n",
      "Epoch 1006, Loss: 0.7780535519123077, Final Batch Loss: 0.30814096331596375\n",
      "Epoch 1007, Loss: 0.8353308141231537, Final Batch Loss: 0.38560283184051514\n",
      "Epoch 1008, Loss: 0.8426302075386047, Final Batch Loss: 0.46094465255737305\n",
      "Epoch 1009, Loss: 0.8703750669956207, Final Batch Loss: 0.44738712906837463\n",
      "Epoch 1010, Loss: 0.8818977177143097, Final Batch Loss: 0.44334232807159424\n",
      "Epoch 1011, Loss: 0.8778010904788971, Final Batch Loss: 0.45639219880104065\n",
      "Epoch 1012, Loss: 0.8647385537624359, Final Batch Loss: 0.4516962170600891\n",
      "Epoch 1013, Loss: 0.837043434381485, Final Batch Loss: 0.43838557600975037\n",
      "Epoch 1014, Loss: 0.8759455978870392, Final Batch Loss: 0.45045825839042664\n",
      "Epoch 1015, Loss: 0.8771390020847321, Final Batch Loss: 0.4199477434158325\n",
      "Epoch 1016, Loss: 0.8462018966674805, Final Batch Loss: 0.42727118730545044\n",
      "Epoch 1017, Loss: 0.8383155167102814, Final Batch Loss: 0.37869158387184143\n",
      "Epoch 1018, Loss: 0.8957376182079315, Final Batch Loss: 0.47170305252075195\n",
      "Epoch 1019, Loss: 0.8607451617717743, Final Batch Loss: 0.42361801862716675\n",
      "Epoch 1020, Loss: 0.8714611828327179, Final Batch Loss: 0.45461708307266235\n",
      "Epoch 1021, Loss: 0.8160376250743866, Final Batch Loss: 0.41166937351226807\n",
      "Epoch 1022, Loss: 0.8932050466537476, Final Batch Loss: 0.4252198338508606\n",
      "Epoch 1023, Loss: 0.8411952257156372, Final Batch Loss: 0.4314559996128082\n",
      "Epoch 1024, Loss: 0.8942579627037048, Final Batch Loss: 0.4169028401374817\n",
      "Epoch 1025, Loss: 0.8109335899353027, Final Batch Loss: 0.3689543306827545\n",
      "Epoch 1026, Loss: 0.8533191084861755, Final Batch Loss: 0.4073948860168457\n",
      "Epoch 1027, Loss: 0.8238747715950012, Final Batch Loss: 0.35054299235343933\n",
      "Epoch 1028, Loss: 0.8423985540866852, Final Batch Loss: 0.455706387758255\n",
      "Epoch 1029, Loss: 0.9105612337589264, Final Batch Loss: 0.4613185226917267\n",
      "Epoch 1030, Loss: 0.8339663445949554, Final Batch Loss: 0.3926846981048584\n",
      "Epoch 1031, Loss: 0.8361796438694, Final Batch Loss: 0.4231730103492737\n",
      "Epoch 1032, Loss: 0.8599298894405365, Final Batch Loss: 0.4556167721748352\n",
      "Epoch 1033, Loss: 0.8558908104896545, Final Batch Loss: 0.4475935995578766\n",
      "Epoch 1034, Loss: 0.8553823232650757, Final Batch Loss: 0.4640369415283203\n",
      "Epoch 1035, Loss: 0.8817131817340851, Final Batch Loss: 0.4575570821762085\n",
      "Epoch 1036, Loss: 0.8046039342880249, Final Batch Loss: 0.41360554099082947\n",
      "Epoch 1037, Loss: 0.8934547901153564, Final Batch Loss: 0.45145460963249207\n",
      "Epoch 1038, Loss: 0.8315398693084717, Final Batch Loss: 0.37425827980041504\n",
      "Epoch 1039, Loss: 0.8836950361728668, Final Batch Loss: 0.4701783061027527\n",
      "Epoch 1040, Loss: 0.8584417998790741, Final Batch Loss: 0.45953595638275146\n",
      "Epoch 1041, Loss: 0.804407000541687, Final Batch Loss: 0.3483261466026306\n",
      "Epoch 1042, Loss: 0.8452107310295105, Final Batch Loss: 0.3968595564365387\n",
      "Epoch 1043, Loss: 0.8698902130126953, Final Batch Loss: 0.4351239502429962\n",
      "Epoch 1044, Loss: 0.8407855033874512, Final Batch Loss: 0.45873621106147766\n",
      "Epoch 1045, Loss: 0.853621631860733, Final Batch Loss: 0.40266153216362\n",
      "Epoch 1046, Loss: 0.8626404404640198, Final Batch Loss: 0.40535682439804077\n",
      "Epoch 1047, Loss: 0.8667791187763214, Final Batch Loss: 0.4055871367454529\n",
      "Epoch 1048, Loss: 0.8214100003242493, Final Batch Loss: 0.41798990964889526\n",
      "Epoch 1049, Loss: 0.8269850313663483, Final Batch Loss: 0.3720414638519287\n",
      "Epoch 1050, Loss: 0.817243218421936, Final Batch Loss: 0.38107040524482727\n",
      "Epoch 1051, Loss: 0.845085471868515, Final Batch Loss: 0.45216870307922363\n",
      "Epoch 1052, Loss: 0.8390320837497711, Final Batch Loss: 0.41720885038375854\n",
      "Epoch 1053, Loss: 0.8320397436618805, Final Batch Loss: 0.4195161759853363\n",
      "Epoch 1054, Loss: 0.8417027592658997, Final Batch Loss: 0.4596182703971863\n",
      "Epoch 1055, Loss: 0.8353150486946106, Final Batch Loss: 0.4527415931224823\n",
      "Epoch 1056, Loss: 0.8334904611110687, Final Batch Loss: 0.4250347316265106\n",
      "Epoch 1057, Loss: 0.7992073595523834, Final Batch Loss: 0.3629944920539856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1058, Loss: 0.7999459505081177, Final Batch Loss: 0.37998121976852417\n",
      "Epoch 1059, Loss: 0.8521669507026672, Final Batch Loss: 0.45395344495773315\n",
      "Epoch 1060, Loss: 0.8596025109291077, Final Batch Loss: 0.4137673079967499\n",
      "Epoch 1061, Loss: 0.8110800087451935, Final Batch Loss: 0.39699986577033997\n",
      "Epoch 1062, Loss: 0.8515969812870026, Final Batch Loss: 0.43865182995796204\n",
      "Epoch 1063, Loss: 0.8531939089298248, Final Batch Loss: 0.40260472893714905\n",
      "Epoch 1064, Loss: 0.8227992355823517, Final Batch Loss: 0.4112482964992523\n",
      "Epoch 1065, Loss: 0.8644045293331146, Final Batch Loss: 0.473330557346344\n",
      "Epoch 1066, Loss: 0.8241322338581085, Final Batch Loss: 0.4614160656929016\n",
      "Epoch 1067, Loss: 0.7909850478172302, Final Batch Loss: 0.3640984892845154\n",
      "Epoch 1068, Loss: 0.8094419538974762, Final Batch Loss: 0.3840811848640442\n",
      "Epoch 1069, Loss: 0.853210985660553, Final Batch Loss: 0.4000411629676819\n",
      "Epoch 1070, Loss: 0.8522390127182007, Final Batch Loss: 0.40570002794265747\n",
      "Epoch 1071, Loss: 0.869253396987915, Final Batch Loss: 0.44341588020324707\n",
      "Epoch 1072, Loss: 0.8644542992115021, Final Batch Loss: 0.5074402093887329\n",
      "Epoch 1073, Loss: 0.8602806329727173, Final Batch Loss: 0.43946573138237\n",
      "Epoch 1074, Loss: 0.8502242267131805, Final Batch Loss: 0.41609877347946167\n",
      "Epoch 1075, Loss: 0.8073244392871857, Final Batch Loss: 0.4145108461380005\n",
      "Epoch 1076, Loss: 0.812353104352951, Final Batch Loss: 0.43617409467697144\n",
      "Epoch 1077, Loss: 0.8614079654216766, Final Batch Loss: 0.46375590562820435\n",
      "Epoch 1078, Loss: 0.8497395515441895, Final Batch Loss: 0.3504582643508911\n",
      "Epoch 1079, Loss: 0.7932240068912506, Final Batch Loss: 0.41325196623802185\n",
      "Epoch 1080, Loss: 0.8169003427028656, Final Batch Loss: 0.366830974817276\n",
      "Epoch 1081, Loss: 0.8130598962306976, Final Batch Loss: 0.4024573862552643\n",
      "Epoch 1082, Loss: 0.8498085737228394, Final Batch Loss: 0.45590388774871826\n",
      "Epoch 1083, Loss: 0.8303423225879669, Final Batch Loss: 0.3900931477546692\n",
      "Epoch 1084, Loss: 0.8391078412532806, Final Batch Loss: 0.49533969163894653\n",
      "Epoch 1085, Loss: 0.8429722785949707, Final Batch Loss: 0.4604339599609375\n",
      "Epoch 1086, Loss: 0.820305198431015, Final Batch Loss: 0.42496681213378906\n",
      "Epoch 1087, Loss: 0.830014556646347, Final Batch Loss: 0.3903663158416748\n",
      "Epoch 1088, Loss: 0.8118876218795776, Final Batch Loss: 0.3600063920021057\n",
      "Epoch 1089, Loss: 0.8320910632610321, Final Batch Loss: 0.44395992159843445\n",
      "Epoch 1090, Loss: 0.8289042115211487, Final Batch Loss: 0.45505374670028687\n",
      "Epoch 1091, Loss: 0.8411933481693268, Final Batch Loss: 0.39464113116264343\n",
      "Epoch 1092, Loss: 0.8171832859516144, Final Batch Loss: 0.4116877019405365\n",
      "Epoch 1093, Loss: 0.8435932695865631, Final Batch Loss: 0.4647044539451599\n",
      "Epoch 1094, Loss: 0.786866307258606, Final Batch Loss: 0.32770097255706787\n",
      "Epoch 1095, Loss: 0.8129372894763947, Final Batch Loss: 0.4035198390483856\n",
      "Epoch 1096, Loss: 0.8561567962169647, Final Batch Loss: 0.45863351225852966\n",
      "Epoch 1097, Loss: 0.8506247401237488, Final Batch Loss: 0.43279698491096497\n",
      "Epoch 1098, Loss: 0.8028029203414917, Final Batch Loss: 0.46689489483833313\n",
      "Epoch 1099, Loss: 0.8388500213623047, Final Batch Loss: 0.40654391050338745\n",
      "Epoch 1100, Loss: 0.8318340182304382, Final Batch Loss: 0.41859421133995056\n",
      "Epoch 1101, Loss: 0.8402955234050751, Final Batch Loss: 0.45553532242774963\n",
      "Epoch 1102, Loss: 0.8177989721298218, Final Batch Loss: 0.4498104453086853\n",
      "Epoch 1103, Loss: 0.8577907979488373, Final Batch Loss: 0.437796026468277\n",
      "Epoch 1104, Loss: 0.925105482339859, Final Batch Loss: 0.45365670323371887\n",
      "Epoch 1105, Loss: 0.8646821677684784, Final Batch Loss: 0.46332812309265137\n",
      "Epoch 1106, Loss: 0.7979414761066437, Final Batch Loss: 0.38752517104148865\n",
      "Epoch 1107, Loss: 0.8162231147289276, Final Batch Loss: 0.3644005060195923\n",
      "Epoch 1108, Loss: 0.8296376466751099, Final Batch Loss: 0.43155089020729065\n",
      "Epoch 1109, Loss: 0.8338135480880737, Final Batch Loss: 0.39140379428863525\n",
      "Epoch 1110, Loss: 0.7920517325401306, Final Batch Loss: 0.36268362402915955\n",
      "Epoch 1111, Loss: 0.8328255116939545, Final Batch Loss: 0.42220407724380493\n",
      "Epoch 1112, Loss: 0.8186582624912262, Final Batch Loss: 0.45000070333480835\n",
      "Epoch 1113, Loss: 0.8303931057453156, Final Batch Loss: 0.4141121804714203\n",
      "Epoch 1114, Loss: 0.8107267022132874, Final Batch Loss: 0.4119355082511902\n",
      "Epoch 1115, Loss: 0.881491094827652, Final Batch Loss: 0.4465058445930481\n",
      "Epoch 1116, Loss: 0.781532347202301, Final Batch Loss: 0.3934953212738037\n",
      "Epoch 1117, Loss: 0.8616530001163483, Final Batch Loss: 0.43060460686683655\n",
      "Epoch 1118, Loss: 0.8103777766227722, Final Batch Loss: 0.40246501564979553\n",
      "Epoch 1119, Loss: 0.8615244030952454, Final Batch Loss: 0.3852163255214691\n",
      "Epoch 1120, Loss: 0.8493503630161285, Final Batch Loss: 0.4118264317512512\n",
      "Epoch 1121, Loss: 0.8177303671836853, Final Batch Loss: 0.4045957028865814\n",
      "Epoch 1122, Loss: 0.8106259405612946, Final Batch Loss: 0.37876051664352417\n",
      "Epoch 1123, Loss: 0.8645772933959961, Final Batch Loss: 0.49160173535346985\n",
      "Epoch 1124, Loss: 0.7884663045406342, Final Batch Loss: 0.3389016091823578\n",
      "Epoch 1125, Loss: 0.8111695051193237, Final Batch Loss: 0.42771637439727783\n",
      "Epoch 1126, Loss: 0.8352300524711609, Final Batch Loss: 0.4267713725566864\n",
      "Epoch 1127, Loss: 0.8386111557483673, Final Batch Loss: 0.4111749827861786\n",
      "Epoch 1128, Loss: 0.8250619173049927, Final Batch Loss: 0.41560739278793335\n",
      "Epoch 1129, Loss: 0.874971479177475, Final Batch Loss: 0.43842142820358276\n",
      "Epoch 1130, Loss: 0.8224086761474609, Final Batch Loss: 0.4449646472930908\n",
      "Epoch 1131, Loss: 0.8612920939922333, Final Batch Loss: 0.4304974377155304\n",
      "Epoch 1132, Loss: 0.772579163312912, Final Batch Loss: 0.41601166129112244\n",
      "Epoch 1133, Loss: 0.7818653285503387, Final Batch Loss: 0.418468177318573\n",
      "Epoch 1134, Loss: 0.8200760781764984, Final Batch Loss: 0.4183971583843231\n",
      "Epoch 1135, Loss: 0.8243514597415924, Final Batch Loss: 0.41460978984832764\n",
      "Epoch 1136, Loss: 0.8091203272342682, Final Batch Loss: 0.4143649637699127\n",
      "Epoch 1137, Loss: 0.8991395235061646, Final Batch Loss: 0.4654358923435211\n",
      "Epoch 1138, Loss: 0.7717053294181824, Final Batch Loss: 0.36285629868507385\n",
      "Epoch 1139, Loss: 0.7941834926605225, Final Batch Loss: 0.3527139723300934\n",
      "Epoch 1140, Loss: 0.8273383975028992, Final Batch Loss: 0.39553171396255493\n",
      "Epoch 1141, Loss: 0.8431783020496368, Final Batch Loss: 0.38587722182273865\n",
      "Epoch 1142, Loss: 0.8041565120220184, Final Batch Loss: 0.4186490476131439\n",
      "Epoch 1143, Loss: 0.7867944538593292, Final Batch Loss: 0.418674111366272\n",
      "Epoch 1144, Loss: 0.7693709135055542, Final Batch Loss: 0.3495553135871887\n",
      "Epoch 1145, Loss: 0.7716944515705109, Final Batch Loss: 0.4048461616039276\n",
      "Epoch 1146, Loss: 0.7804476618766785, Final Batch Loss: 0.3885045051574707\n",
      "Epoch 1147, Loss: 0.8329456448554993, Final Batch Loss: 0.4498343765735626\n",
      "Epoch 1148, Loss: 0.8231837749481201, Final Batch Loss: 0.4251353442668915\n",
      "Epoch 1149, Loss: 0.8334278464317322, Final Batch Loss: 0.4725317060947418\n",
      "Epoch 1150, Loss: 0.8048995733261108, Final Batch Loss: 0.41647273302078247\n",
      "Epoch 1151, Loss: 0.8554129898548126, Final Batch Loss: 0.4610517919063568\n",
      "Epoch 1152, Loss: 0.8010391294956207, Final Batch Loss: 0.3975304961204529\n",
      "Epoch 1153, Loss: 0.7329288125038147, Final Batch Loss: 0.33205679059028625\n",
      "Epoch 1154, Loss: 0.8019970953464508, Final Batch Loss: 0.38750138878822327\n",
      "Epoch 1155, Loss: 0.8212354481220245, Final Batch Loss: 0.4151427745819092\n",
      "Epoch 1156, Loss: 0.7641985714435577, Final Batch Loss: 0.37072864174842834\n",
      "Epoch 1157, Loss: 0.7618128061294556, Final Batch Loss: 0.3224410116672516\n",
      "Epoch 1158, Loss: 0.8033472001552582, Final Batch Loss: 0.39731746912002563\n",
      "Epoch 1159, Loss: 0.7824848592281342, Final Batch Loss: 0.3733372092247009\n",
      "Epoch 1160, Loss: 0.8344061076641083, Final Batch Loss: 0.4022800624370575\n",
      "Epoch 1161, Loss: 0.7536962032318115, Final Batch Loss: 0.3813001811504364\n",
      "Epoch 1162, Loss: 0.8199575543403625, Final Batch Loss: 0.42425891757011414\n",
      "Epoch 1163, Loss: 0.8061718344688416, Final Batch Loss: 0.38943472504615784\n",
      "Epoch 1164, Loss: 0.8593757450580597, Final Batch Loss: 0.4175070822238922\n",
      "Epoch 1165, Loss: 0.8240019679069519, Final Batch Loss: 0.396255761384964\n",
      "Epoch 1166, Loss: 0.7591123878955841, Final Batch Loss: 0.4172624945640564\n",
      "Epoch 1167, Loss: 0.8329703211784363, Final Batch Loss: 0.35699546337127686\n",
      "Epoch 1168, Loss: 0.8115558922290802, Final Batch Loss: 0.41867148876190186\n",
      "Epoch 1169, Loss: 0.8017372786998749, Final Batch Loss: 0.403504341840744\n",
      "Epoch 1170, Loss: 0.8513880372047424, Final Batch Loss: 0.4149430990219116\n",
      "Epoch 1171, Loss: 0.8228714466094971, Final Batch Loss: 0.44902166724205017\n",
      "Epoch 1172, Loss: 0.808944582939148, Final Batch Loss: 0.350931316614151\n",
      "Epoch 1173, Loss: 0.7997800409793854, Final Batch Loss: 0.3858766555786133\n",
      "Epoch 1174, Loss: 0.7743536531925201, Final Batch Loss: 0.3814338743686676\n",
      "Epoch 1175, Loss: 0.7386462986469269, Final Batch Loss: 0.37291088700294495\n",
      "Epoch 1176, Loss: 0.8069667518138885, Final Batch Loss: 0.42014065384864807\n",
      "Epoch 1177, Loss: 0.7686195373535156, Final Batch Loss: 0.37702029943466187\n",
      "Epoch 1178, Loss: 0.8566524684429169, Final Batch Loss: 0.48761677742004395\n",
      "Epoch 1179, Loss: 0.7833495438098907, Final Batch Loss: 0.38580939173698425\n",
      "Epoch 1180, Loss: 0.7671762406826019, Final Batch Loss: 0.361724317073822\n",
      "Epoch 1181, Loss: 0.8190726339817047, Final Batch Loss: 0.41705700755119324\n",
      "Epoch 1182, Loss: 0.8055812418460846, Final Batch Loss: 0.3782380521297455\n",
      "Epoch 1183, Loss: 0.8258407115936279, Final Batch Loss: 0.43638041615486145\n",
      "Epoch 1184, Loss: 0.7869893908500671, Final Batch Loss: 0.43084418773651123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1185, Loss: 0.8195270597934723, Final Batch Loss: 0.45329031348228455\n",
      "Epoch 1186, Loss: 0.7858833968639374, Final Batch Loss: 0.39022135734558105\n",
      "Epoch 1187, Loss: 0.7988756895065308, Final Batch Loss: 0.3693808615207672\n",
      "Epoch 1188, Loss: 0.7557695209980011, Final Batch Loss: 0.39509323239326477\n",
      "Epoch 1189, Loss: 0.7377635538578033, Final Batch Loss: 0.35750114917755127\n",
      "Epoch 1190, Loss: 0.7937860488891602, Final Batch Loss: 0.4287346601486206\n",
      "Epoch 1191, Loss: 0.7849185764789581, Final Batch Loss: 0.39136505126953125\n",
      "Epoch 1192, Loss: 0.7439090609550476, Final Batch Loss: 0.35598036646842957\n",
      "Epoch 1193, Loss: 0.7786115109920502, Final Batch Loss: 0.4045124650001526\n",
      "Epoch 1194, Loss: 0.7793894708156586, Final Batch Loss: 0.36873698234558105\n",
      "Epoch 1195, Loss: 0.8156425356864929, Final Batch Loss: 0.4051853120326996\n",
      "Epoch 1196, Loss: 0.7689138948917389, Final Batch Loss: 0.3940616548061371\n",
      "Epoch 1197, Loss: 0.8276629149913788, Final Batch Loss: 0.41168710589408875\n",
      "Epoch 1198, Loss: 0.7449465095996857, Final Batch Loss: 0.3476180136203766\n",
      "Epoch 1199, Loss: 0.7781663835048676, Final Batch Loss: 0.3814587891101837\n",
      "Epoch 1200, Loss: 0.7508895695209503, Final Batch Loss: 0.39457011222839355\n",
      "Epoch 1201, Loss: 0.8576737344264984, Final Batch Loss: 0.4183429181575775\n",
      "Epoch 1202, Loss: 0.7816628217697144, Final Batch Loss: 0.3710974156856537\n",
      "Epoch 1203, Loss: 0.7990240156650543, Final Batch Loss: 0.38929328322410583\n",
      "Epoch 1204, Loss: 0.7790050208568573, Final Batch Loss: 0.4061141908168793\n",
      "Epoch 1205, Loss: 0.8632566332817078, Final Batch Loss: 0.4294898509979248\n",
      "Epoch 1206, Loss: 0.7806660234928131, Final Batch Loss: 0.40087586641311646\n",
      "Epoch 1207, Loss: 0.7763214111328125, Final Batch Loss: 0.41920655965805054\n",
      "Epoch 1208, Loss: 0.777231752872467, Final Batch Loss: 0.4098427891731262\n",
      "Epoch 1209, Loss: 0.7388832867145538, Final Batch Loss: 0.3772800862789154\n",
      "Epoch 1210, Loss: 0.765944093465805, Final Batch Loss: 0.3907233476638794\n",
      "Epoch 1211, Loss: 0.7744794487953186, Final Batch Loss: 0.39780393242836\n",
      "Epoch 1212, Loss: 0.8356836438179016, Final Batch Loss: 0.48816612362861633\n",
      "Epoch 1213, Loss: 0.7535251080989838, Final Batch Loss: 0.37961456179618835\n",
      "Epoch 1214, Loss: 0.7707405984401703, Final Batch Loss: 0.3528878092765808\n",
      "Epoch 1215, Loss: 0.7954070270061493, Final Batch Loss: 0.3899071514606476\n",
      "Epoch 1216, Loss: 0.7702212035655975, Final Batch Loss: 0.36626237630844116\n",
      "Epoch 1217, Loss: 0.8314192295074463, Final Batch Loss: 0.44148778915405273\n",
      "Epoch 1218, Loss: 0.7100749313831329, Final Batch Loss: 0.3323476016521454\n",
      "Epoch 1219, Loss: 0.7270760536193848, Final Batch Loss: 0.3205256164073944\n",
      "Epoch 1220, Loss: 0.8043673634529114, Final Batch Loss: 0.3979630172252655\n",
      "Epoch 1221, Loss: 0.7754398286342621, Final Batch Loss: 0.35179901123046875\n",
      "Epoch 1222, Loss: 0.7465022504329681, Final Batch Loss: 0.382449209690094\n",
      "Epoch 1223, Loss: 0.7311908602714539, Final Batch Loss: 0.3703703284263611\n",
      "Epoch 1224, Loss: 0.7669258117675781, Final Batch Loss: 0.35873448848724365\n",
      "Epoch 1225, Loss: 0.7103211879730225, Final Batch Loss: 0.3360557556152344\n",
      "Epoch 1226, Loss: 0.7514045238494873, Final Batch Loss: 0.36674514412879944\n",
      "Epoch 1227, Loss: 0.7862218618392944, Final Batch Loss: 0.3965902626514435\n",
      "Epoch 1228, Loss: 0.7263736426830292, Final Batch Loss: 0.4249974489212036\n",
      "Epoch 1229, Loss: 0.7911828756332397, Final Batch Loss: 0.36912015080451965\n",
      "Epoch 1230, Loss: 0.7844561636447906, Final Batch Loss: 0.426266610622406\n",
      "Epoch 1231, Loss: 0.7577200829982758, Final Batch Loss: 0.4182273745536804\n",
      "Epoch 1232, Loss: 0.7358609437942505, Final Batch Loss: 0.41231516003608704\n",
      "Epoch 1233, Loss: 0.7232473492622375, Final Batch Loss: 0.33415839076042175\n",
      "Epoch 1234, Loss: 0.7535105347633362, Final Batch Loss: 0.33202603459358215\n",
      "Epoch 1235, Loss: 0.7522226870059967, Final Batch Loss: 0.3582385778427124\n",
      "Epoch 1236, Loss: 0.7634082138538361, Final Batch Loss: 0.35530054569244385\n",
      "Epoch 1237, Loss: 0.7362067401409149, Final Batch Loss: 0.39453229308128357\n",
      "Epoch 1238, Loss: 0.778645247220993, Final Batch Loss: 0.4010525941848755\n",
      "Epoch 1239, Loss: 0.7727963626384735, Final Batch Loss: 0.39518865942955017\n",
      "Epoch 1240, Loss: 0.7551546394824982, Final Batch Loss: 0.3812318742275238\n",
      "Epoch 1241, Loss: 0.7342789471149445, Final Batch Loss: 0.35879456996917725\n",
      "Epoch 1242, Loss: 0.7303353548049927, Final Batch Loss: 0.38713622093200684\n",
      "Epoch 1243, Loss: 0.7354824542999268, Final Batch Loss: 0.3606439530849457\n",
      "Epoch 1244, Loss: 0.7767176330089569, Final Batch Loss: 0.3258410394191742\n",
      "Epoch 1245, Loss: 0.7080752849578857, Final Batch Loss: 0.30211251974105835\n",
      "Epoch 1246, Loss: 0.7186979055404663, Final Batch Loss: 0.33263757824897766\n",
      "Epoch 1247, Loss: 0.7306182384490967, Final Batch Loss: 0.3354478180408478\n",
      "Epoch 1248, Loss: 0.7302180826663971, Final Batch Loss: 0.3125053346157074\n",
      "Epoch 1249, Loss: 0.7073841094970703, Final Batch Loss: 0.31574729084968567\n",
      "Epoch 1250, Loss: 0.7033467590808868, Final Batch Loss: 0.3170209527015686\n",
      "Epoch 1251, Loss: 0.7641628384590149, Final Batch Loss: 0.35234659910202026\n",
      "Epoch 1252, Loss: 0.6973616480827332, Final Batch Loss: 0.35741889476776123\n",
      "Epoch 1253, Loss: 0.6861028671264648, Final Batch Loss: 0.31221163272857666\n",
      "Epoch 1254, Loss: 0.7370516359806061, Final Batch Loss: 0.35787880420684814\n",
      "Epoch 1255, Loss: 0.7197400033473969, Final Batch Loss: 0.37661391496658325\n",
      "Epoch 1256, Loss: 0.7786142230033875, Final Batch Loss: 0.41416919231414795\n",
      "Epoch 1257, Loss: 0.7712777256965637, Final Batch Loss: 0.37600308656692505\n",
      "Epoch 1258, Loss: 0.7386714518070221, Final Batch Loss: 0.4004512429237366\n",
      "Epoch 1259, Loss: 0.7327011823654175, Final Batch Loss: 0.38410627841949463\n",
      "Epoch 1260, Loss: 0.7537400722503662, Final Batch Loss: 0.40653112530708313\n",
      "Epoch 1261, Loss: 0.7056900262832642, Final Batch Loss: 0.3543279767036438\n",
      "Epoch 1262, Loss: 0.7142328023910522, Final Batch Loss: 0.37432152032852173\n",
      "Epoch 1263, Loss: 0.7258306443691254, Final Batch Loss: 0.3520860970020294\n",
      "Epoch 1264, Loss: 0.6842225790023804, Final Batch Loss: 0.37602949142456055\n",
      "Epoch 1265, Loss: 0.6687279343605042, Final Batch Loss: 0.3378509283065796\n",
      "Epoch 1266, Loss: 0.6992591619491577, Final Batch Loss: 0.3510759174823761\n",
      "Epoch 1267, Loss: 0.7882035374641418, Final Batch Loss: 0.38419482111930847\n",
      "Epoch 1268, Loss: 0.7376196682453156, Final Batch Loss: 0.3242834806442261\n",
      "Epoch 1269, Loss: 0.7160307466983795, Final Batch Loss: 0.30316871404647827\n",
      "Epoch 1270, Loss: 0.7772932350635529, Final Batch Loss: 0.42225486040115356\n",
      "Epoch 1271, Loss: 0.6814157962799072, Final Batch Loss: 0.35491159558296204\n",
      "Epoch 1272, Loss: 0.7712608277797699, Final Batch Loss: 0.3787400722503662\n",
      "Epoch 1273, Loss: 0.7797711491584778, Final Batch Loss: 0.41770249605178833\n",
      "Epoch 1274, Loss: 0.7573865354061127, Final Batch Loss: 0.4003351330757141\n",
      "Epoch 1275, Loss: 0.7114964127540588, Final Batch Loss: 0.38063284754753113\n",
      "Epoch 1276, Loss: 0.7649988532066345, Final Batch Loss: 0.4109807014465332\n",
      "Epoch 1277, Loss: 0.7333594560623169, Final Batch Loss: 0.38586410880088806\n",
      "Epoch 1278, Loss: 0.7952075004577637, Final Batch Loss: 0.35701504349708557\n",
      "Epoch 1279, Loss: 0.8263852596282959, Final Batch Loss: 0.47578534483909607\n",
      "Epoch 1280, Loss: 0.72037473320961, Final Batch Loss: 0.3280806541442871\n",
      "Epoch 1281, Loss: 0.7058229446411133, Final Batch Loss: 0.37081775069236755\n",
      "Epoch 1282, Loss: 0.7166900336742401, Final Batch Loss: 0.34057751297950745\n",
      "Epoch 1283, Loss: 0.6879220902919769, Final Batch Loss: 0.3598755896091461\n",
      "Epoch 1284, Loss: 0.7674410343170166, Final Batch Loss: 0.41622084379196167\n",
      "Epoch 1285, Loss: 0.7054084539413452, Final Batch Loss: 0.3500954508781433\n",
      "Epoch 1286, Loss: 0.7216695547103882, Final Batch Loss: 0.29685741662979126\n",
      "Epoch 1287, Loss: 0.7237457633018494, Final Batch Loss: 0.40409722924232483\n",
      "Epoch 1288, Loss: 0.7211216390132904, Final Batch Loss: 0.3221406638622284\n",
      "Epoch 1289, Loss: 0.6948350369930267, Final Batch Loss: 0.35800623893737793\n",
      "Epoch 1290, Loss: 0.6866582036018372, Final Batch Loss: 0.35212284326553345\n",
      "Epoch 1291, Loss: 0.7283026576042175, Final Batch Loss: 0.35690510272979736\n",
      "Epoch 1292, Loss: 0.7141832113265991, Final Batch Loss: 0.364071249961853\n",
      "Epoch 1293, Loss: 0.7682636380195618, Final Batch Loss: 0.41441163420677185\n",
      "Epoch 1294, Loss: 0.6734598577022552, Final Batch Loss: 0.307210236787796\n",
      "Epoch 1295, Loss: 0.7768834233283997, Final Batch Loss: 0.4189006984233856\n",
      "Epoch 1296, Loss: 0.6561181545257568, Final Batch Loss: 0.325470507144928\n",
      "Epoch 1297, Loss: 0.6872217953205109, Final Batch Loss: 0.35788601636886597\n",
      "Epoch 1298, Loss: 0.6861562430858612, Final Batch Loss: 0.33795392513275146\n",
      "Epoch 1299, Loss: 0.6998792886734009, Final Batch Loss: 0.32623088359832764\n",
      "Epoch 1300, Loss: 0.7314021587371826, Final Batch Loss: 0.32926127314567566\n",
      "Epoch 1301, Loss: 0.7318193912506104, Final Batch Loss: 0.3639936149120331\n",
      "Epoch 1302, Loss: 0.7400304079055786, Final Batch Loss: 0.3765648603439331\n",
      "Epoch 1303, Loss: 0.7122791707515717, Final Batch Loss: 0.38792315125465393\n",
      "Epoch 1304, Loss: 0.6685560941696167, Final Batch Loss: 0.3071662187576294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1305, Loss: 0.6990532279014587, Final Batch Loss: 0.3519827127456665\n",
      "Epoch 1306, Loss: 0.7171840369701385, Final Batch Loss: 0.37032458186149597\n",
      "Epoch 1307, Loss: 0.6788603067398071, Final Batch Loss: 0.3072018623352051\n",
      "Epoch 1308, Loss: 0.6615522503852844, Final Batch Loss: 0.37977534532546997\n",
      "Epoch 1309, Loss: 0.7525725066661835, Final Batch Loss: 0.4004192352294922\n",
      "Epoch 1310, Loss: 0.6713333129882812, Final Batch Loss: 0.34299060702323914\n",
      "Epoch 1311, Loss: 0.7110249698162079, Final Batch Loss: 0.42690151929855347\n",
      "Epoch 1312, Loss: 0.694010853767395, Final Batch Loss: 0.34537526965141296\n",
      "Epoch 1313, Loss: 0.7336895763874054, Final Batch Loss: 0.3841935694217682\n",
      "Epoch 1314, Loss: 0.6744944453239441, Final Batch Loss: 0.3029901087284088\n",
      "Epoch 1315, Loss: 0.643651694059372, Final Batch Loss: 0.2554340064525604\n",
      "Epoch 1316, Loss: 0.7194489538669586, Final Batch Loss: 0.3832975924015045\n",
      "Epoch 1317, Loss: 0.7571033835411072, Final Batch Loss: 0.4365217685699463\n",
      "Epoch 1318, Loss: 0.7371441423892975, Final Batch Loss: 0.3955318331718445\n",
      "Epoch 1319, Loss: 0.6655944585800171, Final Batch Loss: 0.3220774233341217\n",
      "Epoch 1320, Loss: 0.6640007495880127, Final Batch Loss: 0.33846616744995117\n",
      "Epoch 1321, Loss: 0.6556296050548553, Final Batch Loss: 0.34667330980300903\n",
      "Epoch 1322, Loss: 0.6490516066551208, Final Batch Loss: 0.34204310178756714\n",
      "Epoch 1323, Loss: 0.6870359182357788, Final Batch Loss: 0.347840815782547\n",
      "Epoch 1324, Loss: 0.6354592442512512, Final Batch Loss: 0.33341994881629944\n",
      "Epoch 1325, Loss: 0.756471574306488, Final Batch Loss: 0.3691461682319641\n",
      "Epoch 1326, Loss: 0.6945106983184814, Final Batch Loss: 0.3233093321323395\n",
      "Epoch 1327, Loss: 0.6278547048568726, Final Batch Loss: 0.2703114449977875\n",
      "Epoch 1328, Loss: 0.6586290001869202, Final Batch Loss: 0.35982128977775574\n",
      "Epoch 1329, Loss: 0.6609830558300018, Final Batch Loss: 0.317795991897583\n",
      "Epoch 1330, Loss: 0.6545139253139496, Final Batch Loss: 0.3463154435157776\n",
      "Epoch 1331, Loss: 0.6919543147087097, Final Batch Loss: 0.34127306938171387\n",
      "Epoch 1332, Loss: 0.6826781034469604, Final Batch Loss: 0.3085518181324005\n",
      "Epoch 1333, Loss: 0.6549994647502899, Final Batch Loss: 0.3171430230140686\n",
      "Epoch 1334, Loss: 0.7055187523365021, Final Batch Loss: 0.3712652325630188\n",
      "Epoch 1335, Loss: 0.6743913292884827, Final Batch Loss: 0.35965877771377563\n",
      "Epoch 1336, Loss: 0.6834672391414642, Final Batch Loss: 0.3484664559364319\n",
      "Epoch 1337, Loss: 0.637010782957077, Final Batch Loss: 0.3550287187099457\n",
      "Epoch 1338, Loss: 0.6857308447360992, Final Batch Loss: 0.32185062766075134\n",
      "Epoch 1339, Loss: 0.6575709283351898, Final Batch Loss: 0.3797603249549866\n",
      "Epoch 1340, Loss: 0.6895380616188049, Final Batch Loss: 0.31912392377853394\n",
      "Epoch 1341, Loss: 0.6672150492668152, Final Batch Loss: 0.3627416491508484\n",
      "Epoch 1342, Loss: 0.6351353526115417, Final Batch Loss: 0.36149999499320984\n",
      "Epoch 1343, Loss: 0.6776799857616425, Final Batch Loss: 0.3819164037704468\n",
      "Epoch 1344, Loss: 0.7649295330047607, Final Batch Loss: 0.46533820033073425\n",
      "Epoch 1345, Loss: 0.7194053828716278, Final Batch Loss: 0.41742101311683655\n",
      "Epoch 1346, Loss: 0.6915408670902252, Final Batch Loss: 0.32238271832466125\n",
      "Epoch 1347, Loss: 0.6829090416431427, Final Batch Loss: 0.3459261953830719\n",
      "Epoch 1348, Loss: 0.6720392107963562, Final Batch Loss: 0.3324938118457794\n",
      "Epoch 1349, Loss: 0.6701211333274841, Final Batch Loss: 0.340218186378479\n",
      "Epoch 1350, Loss: 0.684515506029129, Final Batch Loss: 0.3479691743850708\n",
      "Epoch 1351, Loss: 0.6399670541286469, Final Batch Loss: 0.3288925886154175\n",
      "Epoch 1352, Loss: 0.6307597756385803, Final Batch Loss: 0.3252872824668884\n",
      "Epoch 1353, Loss: 0.7320001125335693, Final Batch Loss: 0.36330223083496094\n",
      "Epoch 1354, Loss: 0.6683029532432556, Final Batch Loss: 0.3250123858451843\n",
      "Epoch 1355, Loss: 0.579770028591156, Final Batch Loss: 0.2635020613670349\n",
      "Epoch 1356, Loss: 0.6340075731277466, Final Batch Loss: 0.31509846448898315\n",
      "Epoch 1357, Loss: 0.671989917755127, Final Batch Loss: 0.39066293835639954\n",
      "Epoch 1358, Loss: 0.6808745265007019, Final Batch Loss: 0.3206455409526825\n",
      "Epoch 1359, Loss: 0.719183623790741, Final Batch Loss: 0.4071459174156189\n",
      "Epoch 1360, Loss: 0.6169934570789337, Final Batch Loss: 0.24486342072486877\n",
      "Epoch 1361, Loss: 0.6336671411991119, Final Batch Loss: 0.2887016236782074\n",
      "Epoch 1362, Loss: 0.6915317177772522, Final Batch Loss: 0.33155307173728943\n",
      "Epoch 1363, Loss: 0.7236299216747284, Final Batch Loss: 0.3826938271522522\n",
      "Epoch 1364, Loss: 0.6787135303020477, Final Batch Loss: 0.38650503754615784\n",
      "Epoch 1365, Loss: 0.6617672443389893, Final Batch Loss: 0.33045494556427\n",
      "Epoch 1366, Loss: 0.6421968340873718, Final Batch Loss: 0.33954837918281555\n",
      "Epoch 1367, Loss: 0.6370537579059601, Final Batch Loss: 0.28936585783958435\n",
      "Epoch 1368, Loss: 0.6249199509620667, Final Batch Loss: 0.3104700446128845\n",
      "Epoch 1369, Loss: 0.6684511303901672, Final Batch Loss: 0.30116787552833557\n",
      "Epoch 1370, Loss: 0.6230436563491821, Final Batch Loss: 0.3441198468208313\n",
      "Epoch 1371, Loss: 0.6271246373653412, Final Batch Loss: 0.29308873414993286\n",
      "Epoch 1372, Loss: 0.6035043001174927, Final Batch Loss: 0.3161797821521759\n",
      "Epoch 1373, Loss: 0.6705237925052643, Final Batch Loss: 0.3503052592277527\n",
      "Epoch 1374, Loss: 0.6473336219787598, Final Batch Loss: 0.31511279940605164\n",
      "Epoch 1375, Loss: 0.6977396607398987, Final Batch Loss: 0.36861544847488403\n",
      "Epoch 1376, Loss: 0.6557056307792664, Final Batch Loss: 0.351936936378479\n",
      "Epoch 1377, Loss: 0.6258006989955902, Final Batch Loss: 0.31024041771888733\n",
      "Epoch 1378, Loss: 0.6775704920291901, Final Batch Loss: 0.3401581346988678\n",
      "Epoch 1379, Loss: 0.6411676704883575, Final Batch Loss: 0.35970279574394226\n",
      "Epoch 1380, Loss: 0.6278367340564728, Final Batch Loss: 0.338955819606781\n",
      "Epoch 1381, Loss: 0.7084844410419464, Final Batch Loss: 0.338332861661911\n",
      "Epoch 1382, Loss: 0.5859144330024719, Final Batch Loss: 0.28667378425598145\n",
      "Epoch 1383, Loss: 0.631629079580307, Final Batch Loss: 0.30206120014190674\n",
      "Epoch 1384, Loss: 0.6046374440193176, Final Batch Loss: 0.31008920073509216\n",
      "Epoch 1385, Loss: 0.5844533443450928, Final Batch Loss: 0.2869254946708679\n",
      "Epoch 1386, Loss: 0.6557763516902924, Final Batch Loss: 0.31728479266166687\n",
      "Epoch 1387, Loss: 0.6172863841056824, Final Batch Loss: 0.3195837736129761\n",
      "Epoch 1388, Loss: 0.6537219882011414, Final Batch Loss: 0.3027818500995636\n",
      "Epoch 1389, Loss: 0.6000156700611115, Final Batch Loss: 0.34061169624328613\n",
      "Epoch 1390, Loss: 0.64261195063591, Final Batch Loss: 0.2971683442592621\n",
      "Epoch 1391, Loss: 0.6024553775787354, Final Batch Loss: 0.32888850569725037\n",
      "Epoch 1392, Loss: 0.6510768830776215, Final Batch Loss: 0.2992306351661682\n",
      "Epoch 1393, Loss: 0.5964087545871735, Final Batch Loss: 0.3004867732524872\n",
      "Epoch 1394, Loss: 0.6447089910507202, Final Batch Loss: 0.34052255749702454\n",
      "Epoch 1395, Loss: 0.6491103172302246, Final Batch Loss: 0.33777281641960144\n",
      "Epoch 1396, Loss: 0.6294862627983093, Final Batch Loss: 0.2966741621494293\n",
      "Epoch 1397, Loss: 0.635516494512558, Final Batch Loss: 0.30634596943855286\n",
      "Epoch 1398, Loss: 0.5904463529586792, Final Batch Loss: 0.28773921728134155\n",
      "Epoch 1399, Loss: 0.632447361946106, Final Batch Loss: 0.3250356614589691\n",
      "Epoch 1400, Loss: 0.6092098355293274, Final Batch Loss: 0.32266223430633545\n",
      "Epoch 1401, Loss: 0.643132746219635, Final Batch Loss: 0.35296109318733215\n",
      "Epoch 1402, Loss: 0.608640044927597, Final Batch Loss: 0.2958531081676483\n",
      "Epoch 1403, Loss: 0.6281757354736328, Final Batch Loss: 0.30875661969184875\n",
      "Epoch 1404, Loss: 0.6496662199497223, Final Batch Loss: 0.2912146747112274\n",
      "Epoch 1405, Loss: 0.6404604017734528, Final Batch Loss: 0.315493643283844\n",
      "Epoch 1406, Loss: 0.6132990419864655, Final Batch Loss: 0.3200927972793579\n",
      "Epoch 1407, Loss: 0.5978194922208786, Final Batch Loss: 0.35359400510787964\n",
      "Epoch 1408, Loss: 0.6851361691951752, Final Batch Loss: 0.3297167420387268\n",
      "Epoch 1409, Loss: 0.6403726935386658, Final Batch Loss: 0.32912617921829224\n",
      "Epoch 1410, Loss: 0.5975115597248077, Final Batch Loss: 0.2503354847431183\n",
      "Epoch 1411, Loss: 0.6114896237850189, Final Batch Loss: 0.3006725609302521\n",
      "Epoch 1412, Loss: 0.6508141458034515, Final Batch Loss: 0.31811827421188354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1413, Loss: 0.6460095047950745, Final Batch Loss: 0.3357464075088501\n",
      "Epoch 1414, Loss: 0.646404892206192, Final Batch Loss: 0.3063051104545593\n",
      "Epoch 1415, Loss: 0.6426662802696228, Final Batch Loss: 0.3594186007976532\n",
      "Epoch 1416, Loss: 0.6393798291683197, Final Batch Loss: 0.32427147030830383\n",
      "Epoch 1417, Loss: 0.6104864180088043, Final Batch Loss: 0.34287387132644653\n",
      "Epoch 1418, Loss: 0.6564003527164459, Final Batch Loss: 0.35124778747558594\n",
      "Epoch 1419, Loss: 0.6469003558158875, Final Batch Loss: 0.2972210943698883\n",
      "Epoch 1420, Loss: 0.5859563052654266, Final Batch Loss: 0.30709803104400635\n",
      "Epoch 1421, Loss: 0.5765958726406097, Final Batch Loss: 0.23258551955223083\n",
      "Epoch 1422, Loss: 0.6179653108119965, Final Batch Loss: 0.29201042652130127\n",
      "Epoch 1423, Loss: 0.6178170442581177, Final Batch Loss: 0.30705633759498596\n",
      "Epoch 1424, Loss: 0.62253737449646, Final Batch Loss: 0.30036142468452454\n",
      "Epoch 1425, Loss: 0.7356891632080078, Final Batch Loss: 0.43014630675315857\n",
      "Epoch 1426, Loss: 0.6219702064990997, Final Batch Loss: 0.3216012120246887\n",
      "Epoch 1427, Loss: 0.6138966977596283, Final Batch Loss: 0.3142390847206116\n",
      "Epoch 1428, Loss: 0.5961812734603882, Final Batch Loss: 0.3325621485710144\n",
      "Epoch 1429, Loss: 0.591431051492691, Final Batch Loss: 0.3304760158061981\n",
      "Epoch 1430, Loss: 0.587391197681427, Final Batch Loss: 0.327714204788208\n",
      "Epoch 1431, Loss: 0.6353165209293365, Final Batch Loss: 0.3383413553237915\n",
      "Epoch 1432, Loss: 0.6193869411945343, Final Batch Loss: 0.31301310658454895\n",
      "Epoch 1433, Loss: 0.5675100088119507, Final Batch Loss: 0.25144362449645996\n",
      "Epoch 1434, Loss: 0.6487893760204315, Final Batch Loss: 0.3253686726093292\n",
      "Epoch 1435, Loss: 0.5788574516773224, Final Batch Loss: 0.29952582716941833\n",
      "Epoch 1436, Loss: 0.5629094541072845, Final Batch Loss: 0.27788448333740234\n",
      "Epoch 1437, Loss: 0.6076720356941223, Final Batch Loss: 0.29259032011032104\n",
      "Epoch 1438, Loss: 0.6123168170452118, Final Batch Loss: 0.3008648753166199\n",
      "Epoch 1439, Loss: 0.5758150517940521, Final Batch Loss: 0.30844512581825256\n",
      "Epoch 1440, Loss: 0.5621598064899445, Final Batch Loss: 0.2724863588809967\n",
      "Epoch 1441, Loss: 0.5590256750583649, Final Batch Loss: 0.28730303049087524\n",
      "Epoch 1442, Loss: 0.5909398794174194, Final Batch Loss: 0.34087496995925903\n",
      "Epoch 1443, Loss: 0.6175942718982697, Final Batch Loss: 0.27308860421180725\n",
      "Epoch 1444, Loss: 0.5966732501983643, Final Batch Loss: 0.3052624464035034\n",
      "Epoch 1445, Loss: 0.63346728682518, Final Batch Loss: 0.3387705385684967\n",
      "Epoch 1446, Loss: 0.6100473403930664, Final Batch Loss: 0.3165421187877655\n",
      "Epoch 1447, Loss: 0.6653969287872314, Final Batch Loss: 0.3412260115146637\n",
      "Epoch 1448, Loss: 0.5760255753993988, Final Batch Loss: 0.2844177782535553\n",
      "Epoch 1449, Loss: 0.6351886987686157, Final Batch Loss: 0.3295007050037384\n",
      "Epoch 1450, Loss: 0.6271397173404694, Final Batch Loss: 0.2956681549549103\n",
      "Epoch 1451, Loss: 0.5358694791793823, Final Batch Loss: 0.2279166579246521\n",
      "Epoch 1452, Loss: 0.6142705380916595, Final Batch Loss: 0.2757563292980194\n",
      "Epoch 1453, Loss: 0.5661981105804443, Final Batch Loss: 0.28867462277412415\n",
      "Epoch 1454, Loss: 0.5855757743120193, Final Batch Loss: 0.24269913136959076\n",
      "Epoch 1455, Loss: 0.6787603795528412, Final Batch Loss: 0.3123619854450226\n",
      "Epoch 1456, Loss: 0.6004667580127716, Final Batch Loss: 0.2915912866592407\n",
      "Epoch 1457, Loss: 0.5866585671901703, Final Batch Loss: 0.2888038754463196\n",
      "Epoch 1458, Loss: 0.61668261885643, Final Batch Loss: 0.37060660123825073\n",
      "Epoch 1459, Loss: 0.5846759080886841, Final Batch Loss: 0.2593865394592285\n",
      "Epoch 1460, Loss: 0.6345297992229462, Final Batch Loss: 0.2939048409461975\n",
      "Epoch 1461, Loss: 0.5990900099277496, Final Batch Loss: 0.3264816105365753\n",
      "Epoch 1462, Loss: 0.5926887094974518, Final Batch Loss: 0.3083789348602295\n",
      "Epoch 1463, Loss: 0.5505716800689697, Final Batch Loss: 0.2673492133617401\n",
      "Epoch 1464, Loss: 0.5316204875707626, Final Batch Loss: 0.24592410027980804\n",
      "Epoch 1465, Loss: 0.6141259372234344, Final Batch Loss: 0.30379000306129456\n",
      "Epoch 1466, Loss: 0.5881487429141998, Final Batch Loss: 0.24923449754714966\n",
      "Epoch 1467, Loss: 0.6333965361118317, Final Batch Loss: 0.35633984208106995\n",
      "Epoch 1468, Loss: 0.6369844675064087, Final Batch Loss: 0.3259749710559845\n",
      "Epoch 1469, Loss: 0.5690940618515015, Final Batch Loss: 0.2772972881793976\n",
      "Epoch 1470, Loss: 0.5763465762138367, Final Batch Loss: 0.3058265745639801\n",
      "Epoch 1471, Loss: 0.5970914959907532, Final Batch Loss: 0.2522555887699127\n",
      "Epoch 1472, Loss: 0.6233606040477753, Final Batch Loss: 0.2781652510166168\n",
      "Epoch 1473, Loss: 0.5919901132583618, Final Batch Loss: 0.3455381393432617\n",
      "Epoch 1474, Loss: 0.57708939909935, Final Batch Loss: 0.2695811986923218\n",
      "Epoch 1475, Loss: 0.5579197406768799, Final Batch Loss: 0.2952965795993805\n",
      "Epoch 1476, Loss: 0.5989335775375366, Final Batch Loss: 0.3222053647041321\n",
      "Epoch 1477, Loss: 0.5363835096359253, Final Batch Loss: 0.25826495885849\n",
      "Epoch 1478, Loss: 0.5449093282222748, Final Batch Loss: 0.26775798201560974\n",
      "Epoch 1479, Loss: 0.6394020020961761, Final Batch Loss: 0.32088929414749146\n",
      "Epoch 1480, Loss: 0.58814737200737, Final Batch Loss: 0.3176290988922119\n",
      "Epoch 1481, Loss: 0.6179623007774353, Final Batch Loss: 0.30911532044410706\n",
      "Epoch 1482, Loss: 0.6289831846952438, Final Batch Loss: 0.24014295637607574\n",
      "Epoch 1483, Loss: 0.5786716192960739, Final Batch Loss: 0.3385266959667206\n",
      "Epoch 1484, Loss: 0.5913479626178741, Final Batch Loss: 0.3046782910823822\n",
      "Epoch 1485, Loss: 0.5499144196510315, Final Batch Loss: 0.27671489119529724\n",
      "Epoch 1486, Loss: 0.6029497683048248, Final Batch Loss: 0.3403520882129669\n",
      "Epoch 1487, Loss: 0.5561869740486145, Final Batch Loss: 0.2508907616138458\n",
      "Epoch 1488, Loss: 0.5683530867099762, Final Batch Loss: 0.29755163192749023\n",
      "Epoch 1489, Loss: 0.571370929479599, Final Batch Loss: 0.2813744843006134\n",
      "Epoch 1490, Loss: 0.5386912524700165, Final Batch Loss: 0.29640302062034607\n",
      "Epoch 1491, Loss: 0.5958011150360107, Final Batch Loss: 0.3138614892959595\n",
      "Epoch 1492, Loss: 0.5709712207317352, Final Batch Loss: 0.2668650150299072\n",
      "Epoch 1493, Loss: 0.552986353635788, Final Batch Loss: 0.2568342685699463\n",
      "Epoch 1494, Loss: 0.5952965617179871, Final Batch Loss: 0.2942619025707245\n",
      "Epoch 1495, Loss: 0.5723260939121246, Final Batch Loss: 0.276142954826355\n",
      "Epoch 1496, Loss: 0.5278213024139404, Final Batch Loss: 0.27032411098480225\n",
      "Epoch 1497, Loss: 0.5569109320640564, Final Batch Loss: 0.2749136984348297\n",
      "Epoch 1498, Loss: 0.5778475403785706, Final Batch Loss: 0.24440976977348328\n",
      "Epoch 1499, Loss: 0.6107871532440186, Final Batch Loss: 0.30062419176101685\n",
      "Epoch 1500, Loss: 0.6089560091495514, Final Batch Loss: 0.3092387020587921\n",
      "Epoch 1501, Loss: 0.5121972858905792, Final Batch Loss: 0.284609854221344\n",
      "Epoch 1502, Loss: 0.6173975765705109, Final Batch Loss: 0.3330913484096527\n",
      "Epoch 1503, Loss: 0.574050784111023, Final Batch Loss: 0.25091150403022766\n",
      "Epoch 1504, Loss: 0.5390595495700836, Final Batch Loss: 0.28313833475112915\n",
      "Epoch 1505, Loss: 0.5623338520526886, Final Batch Loss: 0.27395015954971313\n",
      "Epoch 1506, Loss: 0.5466813445091248, Final Batch Loss: 0.25419408082962036\n",
      "Epoch 1507, Loss: 0.5607379078865051, Final Batch Loss: 0.2727588415145874\n",
      "Epoch 1508, Loss: 0.523202434182167, Final Batch Loss: 0.23522870242595673\n",
      "Epoch 1509, Loss: 0.5456233620643616, Final Batch Loss: 0.2808774709701538\n",
      "Epoch 1510, Loss: 0.5530305206775665, Final Batch Loss: 0.26374581456184387\n",
      "Epoch 1511, Loss: 0.6368368864059448, Final Batch Loss: 0.3234332799911499\n",
      "Epoch 1512, Loss: 0.5685206353664398, Final Batch Loss: 0.26938775181770325\n",
      "Epoch 1513, Loss: 0.536332368850708, Final Batch Loss: 0.2723948061466217\n",
      "Epoch 1514, Loss: 0.5732870697975159, Final Batch Loss: 0.2611207365989685\n",
      "Epoch 1515, Loss: 0.603106826543808, Final Batch Loss: 0.2947564721107483\n",
      "Epoch 1516, Loss: 0.5229986757040024, Final Batch Loss: 0.24665729701519012\n",
      "Epoch 1517, Loss: 0.5570303499698639, Final Batch Loss: 0.27225229144096375\n",
      "Epoch 1518, Loss: 0.5879978239536285, Final Batch Loss: 0.3083040416240692\n",
      "Epoch 1519, Loss: 0.5986991822719574, Final Batch Loss: 0.27762332558631897\n",
      "Epoch 1520, Loss: 0.5100592374801636, Final Batch Loss: 0.226732075214386\n",
      "Epoch 1521, Loss: 0.5577690005302429, Final Batch Loss: 0.30498483777046204\n",
      "Epoch 1522, Loss: 0.5483223497867584, Final Batch Loss: 0.25640028715133667\n",
      "Epoch 1523, Loss: 0.5505190789699554, Final Batch Loss: 0.255097895860672\n",
      "Epoch 1524, Loss: 0.5240619331598282, Final Batch Loss: 0.22899751365184784\n",
      "Epoch 1525, Loss: 0.5872049331665039, Final Batch Loss: 0.3571653962135315\n",
      "Epoch 1526, Loss: 0.5704104602336884, Final Batch Loss: 0.33243900537490845\n",
      "Epoch 1527, Loss: 0.5985195636749268, Final Batch Loss: 0.31774187088012695\n",
      "Epoch 1528, Loss: 0.6211643517017365, Final Batch Loss: 0.2627853751182556\n",
      "Epoch 1529, Loss: 0.5446920394897461, Final Batch Loss: 0.2602140009403229\n",
      "Epoch 1530, Loss: 0.5260021984577179, Final Batch Loss: 0.27533042430877686\n",
      "Epoch 1531, Loss: 0.6060676872730255, Final Batch Loss: 0.31713050603866577\n",
      "Epoch 1532, Loss: 0.544489175081253, Final Batch Loss: 0.2911049723625183\n",
      "Epoch 1533, Loss: 0.4973795861005783, Final Batch Loss: 0.2511442005634308\n",
      "Epoch 1534, Loss: 0.5604980885982513, Final Batch Loss: 0.2856975495815277\n",
      "Epoch 1535, Loss: 0.536949023604393, Final Batch Loss: 0.23547492921352386\n",
      "Epoch 1536, Loss: 0.5404084324836731, Final Batch Loss: 0.2686813473701477\n",
      "Epoch 1537, Loss: 0.5838613510131836, Final Batch Loss: 0.29683932662010193\n",
      "Epoch 1538, Loss: 0.5430105626583099, Final Batch Loss: 0.28101152181625366\n",
      "Epoch 1539, Loss: 0.5560324490070343, Final Batch Loss: 0.34875017404556274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1540, Loss: 0.5903926491737366, Final Batch Loss: 0.33672305941581726\n",
      "Epoch 1541, Loss: 0.5126388818025589, Final Batch Loss: 0.26361772418022156\n",
      "Epoch 1542, Loss: 0.5485425144433975, Final Batch Loss: 0.3037891983985901\n",
      "Epoch 1543, Loss: 0.5574195086956024, Final Batch Loss: 0.26282474398612976\n",
      "Epoch 1544, Loss: 0.48063863813877106, Final Batch Loss: 0.22793345153331757\n",
      "Epoch 1545, Loss: 0.4899429529905319, Final Batch Loss: 0.23794938623905182\n",
      "Epoch 1546, Loss: 0.5554207265377045, Final Batch Loss: 0.2983272075653076\n",
      "Epoch 1547, Loss: 0.5871217250823975, Final Batch Loss: 0.31169575452804565\n",
      "Epoch 1548, Loss: 0.5244801044464111, Final Batch Loss: 0.26761531829833984\n",
      "Epoch 1549, Loss: 0.5698402523994446, Final Batch Loss: 0.26019155979156494\n",
      "Epoch 1550, Loss: 0.49310095608234406, Final Batch Loss: 0.2578199803829193\n",
      "Epoch 1551, Loss: 0.5533915758132935, Final Batch Loss: 0.2713926136493683\n",
      "Epoch 1552, Loss: 0.5208463817834854, Final Batch Loss: 0.24280114471912384\n",
      "Epoch 1553, Loss: 0.5070505291223526, Final Batch Loss: 0.28749769926071167\n",
      "Epoch 1554, Loss: 0.5516852140426636, Final Batch Loss: 0.2657804787158966\n",
      "Epoch 1555, Loss: 0.5195291340351105, Final Batch Loss: 0.2232954502105713\n",
      "Epoch 1556, Loss: 0.5535577237606049, Final Batch Loss: 0.2654370367527008\n",
      "Epoch 1557, Loss: 0.5146965086460114, Final Batch Loss: 0.2606387138366699\n",
      "Epoch 1558, Loss: 0.5049222856760025, Final Batch Loss: 0.239816352725029\n",
      "Epoch 1559, Loss: 0.4787692725658417, Final Batch Loss: 0.24357298016548157\n",
      "Epoch 1560, Loss: 0.5140732824802399, Final Batch Loss: 0.2555510401725769\n",
      "Epoch 1561, Loss: 0.5753087997436523, Final Batch Loss: 0.300377756357193\n",
      "Epoch 1562, Loss: 0.5796711444854736, Final Batch Loss: 0.31293314695358276\n",
      "Epoch 1563, Loss: 0.5625093877315521, Final Batch Loss: 0.28497397899627686\n",
      "Epoch 1564, Loss: 0.4762301445007324, Final Batch Loss: 0.23643551766872406\n",
      "Epoch 1565, Loss: 0.5309809148311615, Final Batch Loss: 0.27491700649261475\n",
      "Epoch 1566, Loss: 0.49310314655303955, Final Batch Loss: 0.20136544108390808\n",
      "Epoch 1567, Loss: 0.5932883322238922, Final Batch Loss: 0.33164292573928833\n",
      "Epoch 1568, Loss: 0.5388607680797577, Final Batch Loss: 0.2610117495059967\n",
      "Epoch 1569, Loss: 0.5369802862405777, Final Batch Loss: 0.2480391412973404\n",
      "Epoch 1570, Loss: 0.5257398784160614, Final Batch Loss: 0.2504193186759949\n",
      "Epoch 1571, Loss: 0.44719311594963074, Final Batch Loss: 0.23944079875946045\n",
      "Epoch 1572, Loss: 0.5393106639385223, Final Batch Loss: 0.2706800103187561\n",
      "Epoch 1573, Loss: 0.5444844216108322, Final Batch Loss: 0.31506404280662537\n",
      "Epoch 1574, Loss: 0.586065798997879, Final Batch Loss: 0.23634937405586243\n",
      "Epoch 1575, Loss: 0.4716346561908722, Final Batch Loss: 0.24666813015937805\n",
      "Epoch 1576, Loss: 0.5294744968414307, Final Batch Loss: 0.2588557004928589\n",
      "Epoch 1577, Loss: 0.5628993809223175, Final Batch Loss: 0.30028656125068665\n",
      "Epoch 1578, Loss: 0.5491814911365509, Final Batch Loss: 0.2734648287296295\n",
      "Epoch 1579, Loss: 0.5029621720314026, Final Batch Loss: 0.21110928058624268\n",
      "Epoch 1580, Loss: 0.5460399091243744, Final Batch Loss: 0.26969417929649353\n",
      "Epoch 1581, Loss: 0.5469156205654144, Final Batch Loss: 0.26603108644485474\n",
      "Epoch 1582, Loss: 0.5193672627210617, Final Batch Loss: 0.29235830903053284\n",
      "Epoch 1583, Loss: 0.46910636126995087, Final Batch Loss: 0.25279638171195984\n",
      "Epoch 1584, Loss: 0.5051921904087067, Final Batch Loss: 0.2578308880329132\n",
      "Epoch 1585, Loss: 0.5369694232940674, Final Batch Loss: 0.3056243062019348\n",
      "Epoch 1586, Loss: 0.4695054441690445, Final Batch Loss: 0.19816555082798004\n",
      "Epoch 1587, Loss: 0.5050374865531921, Final Batch Loss: 0.2572137117385864\n",
      "Epoch 1588, Loss: 0.5157427191734314, Final Batch Loss: 0.26219189167022705\n",
      "Epoch 1589, Loss: 0.5246106386184692, Final Batch Loss: 0.26843103766441345\n",
      "Epoch 1590, Loss: 0.5224233716726303, Final Batch Loss: 0.275892436504364\n",
      "Epoch 1591, Loss: 0.5023895651102066, Final Batch Loss: 0.2375994473695755\n",
      "Epoch 1592, Loss: 0.48132357001304626, Final Batch Loss: 0.24781310558319092\n",
      "Epoch 1593, Loss: 0.5377888381481171, Final Batch Loss: 0.25352221727371216\n",
      "Epoch 1594, Loss: 0.5852626264095306, Final Batch Loss: 0.29030418395996094\n",
      "Epoch 1595, Loss: 0.5213983654975891, Final Batch Loss: 0.26522719860076904\n",
      "Epoch 1596, Loss: 0.5058523267507553, Final Batch Loss: 0.23701845109462738\n",
      "Epoch 1597, Loss: 0.5049828439950943, Final Batch Loss: 0.24100671708583832\n",
      "Epoch 1598, Loss: 0.5728853642940521, Final Batch Loss: 0.2587626874446869\n",
      "Epoch 1599, Loss: 0.537327378988266, Final Batch Loss: 0.23603785037994385\n",
      "Epoch 1600, Loss: 0.4974755793809891, Final Batch Loss: 0.2442941814661026\n",
      "Epoch 1601, Loss: 0.5065404325723648, Final Batch Loss: 0.21283142268657684\n",
      "Epoch 1602, Loss: 0.4961431175470352, Final Batch Loss: 0.23332549631595612\n",
      "Epoch 1603, Loss: 0.526475116610527, Final Batch Loss: 0.23710592091083527\n",
      "Epoch 1604, Loss: 0.5466429889202118, Final Batch Loss: 0.3211738169193268\n",
      "Epoch 1605, Loss: 0.49792779982089996, Final Batch Loss: 0.2534227669239044\n",
      "Epoch 1606, Loss: 0.5436402261257172, Final Batch Loss: 0.2688594460487366\n",
      "Epoch 1607, Loss: 0.5999561250209808, Final Batch Loss: 0.3189326822757721\n",
      "Epoch 1608, Loss: 0.5523480772972107, Final Batch Loss: 0.3302616775035858\n",
      "Epoch 1609, Loss: 0.5343210250139236, Final Batch Loss: 0.2938539683818817\n",
      "Epoch 1610, Loss: 0.5101467967033386, Final Batch Loss: 0.21718177199363708\n",
      "Epoch 1611, Loss: 0.5040981769561768, Final Batch Loss: 0.2861086428165436\n",
      "Epoch 1612, Loss: 0.5635784715414047, Final Batch Loss: 0.3235933184623718\n",
      "Epoch 1613, Loss: 0.4874880760908127, Final Batch Loss: 0.25383633375167847\n",
      "Epoch 1614, Loss: 0.4939333200454712, Final Batch Loss: 0.24929113686084747\n",
      "Epoch 1615, Loss: 0.5012139230966568, Final Batch Loss: 0.23818685114383698\n",
      "Epoch 1616, Loss: 0.5149552971124649, Final Batch Loss: 0.293266624212265\n",
      "Epoch 1617, Loss: 0.49222372472286224, Final Batch Loss: 0.258543461561203\n",
      "Epoch 1618, Loss: 0.5476875305175781, Final Batch Loss: 0.2700435519218445\n",
      "Epoch 1619, Loss: 0.518934041261673, Final Batch Loss: 0.25983256101608276\n",
      "Epoch 1620, Loss: 0.4767788499593735, Final Batch Loss: 0.22467045485973358\n",
      "Epoch 1621, Loss: 0.5568476617336273, Final Batch Loss: 0.302926242351532\n",
      "Epoch 1622, Loss: 0.5207060128450394, Final Batch Loss: 0.28026384115219116\n",
      "Epoch 1623, Loss: 0.5077995508909225, Final Batch Loss: 0.23484249413013458\n",
      "Epoch 1624, Loss: 0.5148015022277832, Final Batch Loss: 0.23943790793418884\n",
      "Epoch 1625, Loss: 0.4948499798774719, Final Batch Loss: 0.2237425446510315\n",
      "Epoch 1626, Loss: 0.47966253757476807, Final Batch Loss: 0.21156561374664307\n",
      "Epoch 1627, Loss: 0.543432891368866, Final Batch Loss: 0.2447626292705536\n",
      "Epoch 1628, Loss: 0.5004717707633972, Final Batch Loss: 0.26421234011650085\n",
      "Epoch 1629, Loss: 0.47603021562099457, Final Batch Loss: 0.23198601603507996\n",
      "Epoch 1630, Loss: 0.5023229122161865, Final Batch Loss: 0.20115062594413757\n",
      "Epoch 1631, Loss: 0.49290062487125397, Final Batch Loss: 0.2819778323173523\n",
      "Epoch 1632, Loss: 0.48044033348560333, Final Batch Loss: 0.22647811472415924\n",
      "Epoch 1633, Loss: 0.48522965610027313, Final Batch Loss: 0.2536405920982361\n",
      "Epoch 1634, Loss: 0.4506409913301468, Final Batch Loss: 0.2096264362335205\n",
      "Epoch 1635, Loss: 0.4567064642906189, Final Batch Loss: 0.23610152304172516\n",
      "Epoch 1636, Loss: 0.5246726870536804, Final Batch Loss: 0.2987046539783478\n",
      "Epoch 1637, Loss: 0.520483136177063, Final Batch Loss: 0.2555219531059265\n",
      "Epoch 1638, Loss: 0.5151508152484894, Final Batch Loss: 0.26670634746551514\n",
      "Epoch 1639, Loss: 0.4832393229007721, Final Batch Loss: 0.23660686612129211\n",
      "Epoch 1640, Loss: 0.4760754704475403, Final Batch Loss: 0.2816638648509979\n",
      "Epoch 1641, Loss: 0.5286758542060852, Final Batch Loss: 0.27366477251052856\n",
      "Epoch 1642, Loss: 0.6273352205753326, Final Batch Loss: 0.3701280951499939\n",
      "Epoch 1643, Loss: 0.5186026245355606, Final Batch Loss: 0.2179882675409317\n",
      "Epoch 1644, Loss: 0.47243547439575195, Final Batch Loss: 0.2196841537952423\n",
      "Epoch 1645, Loss: 0.4979318082332611, Final Batch Loss: 0.2537822425365448\n",
      "Epoch 1646, Loss: 0.4602442681789398, Final Batch Loss: 0.22597086429595947\n",
      "Epoch 1647, Loss: 0.539758712053299, Final Batch Loss: 0.25753775238990784\n",
      "Epoch 1648, Loss: 0.5001267492771149, Final Batch Loss: 0.29003477096557617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1649, Loss: 0.5049407184123993, Final Batch Loss: 0.2694763243198395\n",
      "Epoch 1650, Loss: 0.5003756731748581, Final Batch Loss: 0.2900152802467346\n",
      "Epoch 1651, Loss: 0.5050694644451141, Final Batch Loss: 0.24867117404937744\n",
      "Epoch 1652, Loss: 0.43264441192150116, Final Batch Loss: 0.19025872647762299\n",
      "Epoch 1653, Loss: 0.48315446078777313, Final Batch Loss: 0.2344372719526291\n",
      "Epoch 1654, Loss: 0.5303737074136734, Final Batch Loss: 0.3206881880760193\n",
      "Epoch 1655, Loss: 0.5216801166534424, Final Batch Loss: 0.25083428621292114\n",
      "Epoch 1656, Loss: 0.4772535264492035, Final Batch Loss: 0.24702009558677673\n",
      "Epoch 1657, Loss: 0.48235437273979187, Final Batch Loss: 0.2075178325176239\n",
      "Epoch 1658, Loss: 0.49474598467350006, Final Batch Loss: 0.23672975599765778\n",
      "Epoch 1659, Loss: 0.47523723542690277, Final Batch Loss: 0.22026194632053375\n",
      "Epoch 1660, Loss: 0.44228340685367584, Final Batch Loss: 0.25710850954055786\n",
      "Epoch 1661, Loss: 0.45962245762348175, Final Batch Loss: 0.24061745405197144\n",
      "Epoch 1662, Loss: 0.46692347526550293, Final Batch Loss: 0.25337347388267517\n",
      "Epoch 1663, Loss: 0.4648428410291672, Final Batch Loss: 0.2082914561033249\n",
      "Epoch 1664, Loss: 0.47475817799568176, Final Batch Loss: 0.22328677773475647\n",
      "Epoch 1665, Loss: 0.5402528345584869, Final Batch Loss: 0.2653602063655853\n",
      "Epoch 1666, Loss: 0.4530368149280548, Final Batch Loss: 0.21997934579849243\n",
      "Epoch 1667, Loss: 0.46433648467063904, Final Batch Loss: 0.22461482882499695\n",
      "Epoch 1668, Loss: 0.47417427599430084, Final Batch Loss: 0.235035702586174\n",
      "Epoch 1669, Loss: 0.5315598547458649, Final Batch Loss: 0.28576773405075073\n",
      "Epoch 1670, Loss: 0.4612449109554291, Final Batch Loss: 0.24468933045864105\n",
      "Epoch 1671, Loss: 0.46528753638267517, Final Batch Loss: 0.2045959234237671\n",
      "Epoch 1672, Loss: 0.5084282159805298, Final Batch Loss: 0.25641459226608276\n",
      "Epoch 1673, Loss: 0.4371192902326584, Final Batch Loss: 0.2316635400056839\n",
      "Epoch 1674, Loss: 0.4557134658098221, Final Batch Loss: 0.20081891119480133\n",
      "Epoch 1675, Loss: 0.48979464173316956, Final Batch Loss: 0.24686990678310394\n",
      "Epoch 1676, Loss: 0.4859936684370041, Final Batch Loss: 0.2504671812057495\n",
      "Epoch 1677, Loss: 0.5863634347915649, Final Batch Loss: 0.3121090829372406\n",
      "Epoch 1678, Loss: 0.5543129444122314, Final Batch Loss: 0.2544953525066376\n",
      "Epoch 1679, Loss: 0.5445003509521484, Final Batch Loss: 0.2967437505722046\n",
      "Epoch 1680, Loss: 0.5032542198896408, Final Batch Loss: 0.2652789056301117\n",
      "Epoch 1681, Loss: 0.4828225076198578, Final Batch Loss: 0.2486371397972107\n",
      "Epoch 1682, Loss: 0.506737545132637, Final Batch Loss: 0.274844765663147\n",
      "Epoch 1683, Loss: 0.4006861597299576, Final Batch Loss: 0.1855771690607071\n",
      "Epoch 1684, Loss: 0.5301705747842789, Final Batch Loss: 0.28864148259162903\n",
      "Epoch 1685, Loss: 0.5073727667331696, Final Batch Loss: 0.27231287956237793\n",
      "Epoch 1686, Loss: 0.41725774109363556, Final Batch Loss: 0.17353272438049316\n",
      "Epoch 1687, Loss: 0.4426407516002655, Final Batch Loss: 0.22741387784481049\n",
      "Epoch 1688, Loss: 0.4534596651792526, Final Batch Loss: 0.2270578294992447\n",
      "Epoch 1689, Loss: 0.42512619495391846, Final Batch Loss: 0.2127097249031067\n",
      "Epoch 1690, Loss: 0.4737559109926224, Final Batch Loss: 0.23790638148784637\n",
      "Epoch 1691, Loss: 0.45380331575870514, Final Batch Loss: 0.22938241064548492\n",
      "Epoch 1692, Loss: 0.4917050451040268, Final Batch Loss: 0.2535511255264282\n",
      "Epoch 1693, Loss: 0.5141577869653702, Final Batch Loss: 0.2863289713859558\n",
      "Epoch 1694, Loss: 0.4414381682872772, Final Batch Loss: 0.25047561526298523\n",
      "Epoch 1695, Loss: 0.4371098279953003, Final Batch Loss: 0.21194036304950714\n",
      "Epoch 1696, Loss: 0.4918956309556961, Final Batch Loss: 0.2357945293188095\n",
      "Epoch 1697, Loss: 0.4547968655824661, Final Batch Loss: 0.2654946446418762\n",
      "Epoch 1698, Loss: 0.5083319693803787, Final Batch Loss: 0.2704896926879883\n",
      "Epoch 1699, Loss: 0.5737583637237549, Final Batch Loss: 0.31563666462898254\n",
      "Epoch 1700, Loss: 0.4495649188756943, Final Batch Loss: 0.21991856396198273\n",
      "Epoch 1701, Loss: 0.43908731639385223, Final Batch Loss: 0.22024981677532196\n",
      "Epoch 1702, Loss: 0.48604877293109894, Final Batch Loss: 0.2211095243692398\n",
      "Epoch 1703, Loss: 0.46697069704532623, Final Batch Loss: 0.24053725600242615\n",
      "Epoch 1704, Loss: 0.46522727608680725, Final Batch Loss: 0.23515474796295166\n",
      "Epoch 1705, Loss: 0.4826253056526184, Final Batch Loss: 0.2348899096250534\n",
      "Epoch 1706, Loss: 0.457367941737175, Final Batch Loss: 0.24038167297840118\n",
      "Epoch 1707, Loss: 0.4870889484882355, Final Batch Loss: 0.21508759260177612\n",
      "Epoch 1708, Loss: 0.45457786321640015, Final Batch Loss: 0.2499847561120987\n",
      "Epoch 1709, Loss: 0.446097269654274, Final Batch Loss: 0.20996280014514923\n",
      "Epoch 1710, Loss: 0.493016853928566, Final Batch Loss: 0.26647046208381653\n",
      "Epoch 1711, Loss: 0.4651167392730713, Final Batch Loss: 0.2380325347185135\n",
      "Epoch 1712, Loss: 0.4745437204837799, Final Batch Loss: 0.22883397340774536\n",
      "Epoch 1713, Loss: 0.37817078828811646, Final Batch Loss: 0.20791342854499817\n",
      "Epoch 1714, Loss: 0.5271820574998856, Final Batch Loss: 0.3355337977409363\n",
      "Epoch 1715, Loss: 0.43101800978183746, Final Batch Loss: 0.19396208226680756\n",
      "Epoch 1716, Loss: 0.3923207074403763, Final Batch Loss: 0.16047337651252747\n",
      "Epoch 1717, Loss: 0.4203004986047745, Final Batch Loss: 0.22844794392585754\n",
      "Epoch 1718, Loss: 0.44416971504688263, Final Batch Loss: 0.24000701308250427\n",
      "Epoch 1719, Loss: 0.48437029123306274, Final Batch Loss: 0.23237290978431702\n",
      "Epoch 1720, Loss: 0.4129066616296768, Final Batch Loss: 0.18692238628864288\n",
      "Epoch 1721, Loss: 0.4522264897823334, Final Batch Loss: 0.2395639568567276\n",
      "Epoch 1722, Loss: 0.39005888998508453, Final Batch Loss: 0.16573791205883026\n",
      "Epoch 1723, Loss: 0.4661835879087448, Final Batch Loss: 0.23041477799415588\n",
      "Epoch 1724, Loss: 0.5260182768106461, Final Batch Loss: 0.28328776359558105\n",
      "Epoch 1725, Loss: 0.49366533756256104, Final Batch Loss: 0.2551417350769043\n",
      "Epoch 1726, Loss: 0.45006580650806427, Final Batch Loss: 0.23311066627502441\n",
      "Epoch 1727, Loss: 0.5059371739625931, Final Batch Loss: 0.29311391711235046\n",
      "Epoch 1728, Loss: 0.48396527767181396, Final Batch Loss: 0.1690163016319275\n",
      "Epoch 1729, Loss: 0.4272492080926895, Final Batch Loss: 0.2125016450881958\n",
      "Epoch 1730, Loss: 0.4526953548192978, Final Batch Loss: 0.21420036256313324\n",
      "Epoch 1731, Loss: 0.48078499734401703, Final Batch Loss: 0.2497609704732895\n",
      "Epoch 1732, Loss: 0.5013852268457413, Final Batch Loss: 0.2549539804458618\n",
      "Epoch 1733, Loss: 0.46116700768470764, Final Batch Loss: 0.2222323715686798\n",
      "Epoch 1734, Loss: 0.5097646713256836, Final Batch Loss: 0.27493417263031006\n",
      "Epoch 1735, Loss: 0.5079222172498703, Final Batch Loss: 0.2382577806711197\n",
      "Epoch 1736, Loss: 0.5133559554815292, Final Batch Loss: 0.27498820424079895\n",
      "Epoch 1737, Loss: 0.5149481445550919, Final Batch Loss: 0.27411705255508423\n",
      "Epoch 1738, Loss: 0.43627357482910156, Final Batch Loss: 0.20391379296779633\n",
      "Epoch 1739, Loss: 0.43789052963256836, Final Batch Loss: 0.21465416252613068\n",
      "Epoch 1740, Loss: 0.4835710674524307, Final Batch Loss: 0.2882727086544037\n",
      "Epoch 1741, Loss: 0.5337640941143036, Final Batch Loss: 0.32974883913993835\n",
      "Epoch 1742, Loss: 0.4347004145383835, Final Batch Loss: 0.22991104423999786\n",
      "Epoch 1743, Loss: 0.4430707097053528, Final Batch Loss: 0.2025694102048874\n",
      "Epoch 1744, Loss: 0.4560844749212265, Final Batch Loss: 0.24426008760929108\n",
      "Epoch 1745, Loss: 0.4397146552801132, Final Batch Loss: 0.2378598153591156\n",
      "Epoch 1746, Loss: 0.4586328715085983, Final Batch Loss: 0.2462390959262848\n",
      "Epoch 1747, Loss: 0.504385232925415, Final Batch Loss: 0.25670644640922546\n",
      "Epoch 1748, Loss: 0.5174389034509659, Final Batch Loss: 0.2978931665420532\n",
      "Epoch 1749, Loss: 0.41722674667835236, Final Batch Loss: 0.2028348594903946\n",
      "Epoch 1750, Loss: 0.42044804990291595, Final Batch Loss: 0.2298625111579895\n",
      "Epoch 1751, Loss: 0.4072277396917343, Final Batch Loss: 0.21822015941143036\n",
      "Epoch 1752, Loss: 0.38850706815719604, Final Batch Loss: 0.19437028467655182\n",
      "Epoch 1753, Loss: 0.46435196697711945, Final Batch Loss: 0.2665693163871765\n",
      "Epoch 1754, Loss: 0.42284075915813446, Final Batch Loss: 0.19378656148910522\n",
      "Epoch 1755, Loss: 0.4615812301635742, Final Batch Loss: 0.2523573637008667\n",
      "Epoch 1756, Loss: 0.4643463045358658, Final Batch Loss: 0.2198193073272705\n",
      "Epoch 1757, Loss: 0.4457673579454422, Final Batch Loss: 0.2256762534379959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1758, Loss: 0.40645238757133484, Final Batch Loss: 0.20250365138053894\n",
      "Epoch 1759, Loss: 0.42980286478996277, Final Batch Loss: 0.23459728062152863\n",
      "Epoch 1760, Loss: 0.4146769791841507, Final Batch Loss: 0.19496384263038635\n",
      "Epoch 1761, Loss: 0.4451049864292145, Final Batch Loss: 0.2051524668931961\n",
      "Epoch 1762, Loss: 0.4632774293422699, Final Batch Loss: 0.2543140649795532\n",
      "Epoch 1763, Loss: 0.4642820507287979, Final Batch Loss: 0.23178893327713013\n",
      "Epoch 1764, Loss: 0.457131564617157, Final Batch Loss: 0.21146371960639954\n",
      "Epoch 1765, Loss: 0.3691595643758774, Final Batch Loss: 0.16819708049297333\n",
      "Epoch 1766, Loss: 0.4709843099117279, Final Batch Loss: 0.26160627603530884\n",
      "Epoch 1767, Loss: 0.4826182574033737, Final Batch Loss: 0.26275959610939026\n",
      "Epoch 1768, Loss: 0.5064598768949509, Final Batch Loss: 0.2704145014286041\n",
      "Epoch 1769, Loss: 0.46642808616161346, Final Batch Loss: 0.23360928893089294\n",
      "Epoch 1770, Loss: 0.4570803642272949, Final Batch Loss: 0.2094111293554306\n",
      "Epoch 1771, Loss: 0.4028344005346298, Final Batch Loss: 0.23297786712646484\n",
      "Epoch 1772, Loss: 0.45415838062763214, Final Batch Loss: 0.19593720138072968\n",
      "Epoch 1773, Loss: 0.4199281334877014, Final Batch Loss: 0.21188415586948395\n",
      "Epoch 1774, Loss: 0.4481837898492813, Final Batch Loss: 0.2092306911945343\n",
      "Epoch 1775, Loss: 0.4297848045825958, Final Batch Loss: 0.2527455985546112\n",
      "Epoch 1776, Loss: 0.42613929510116577, Final Batch Loss: 0.1829453557729721\n",
      "Epoch 1777, Loss: 0.412626177072525, Final Batch Loss: 0.22856146097183228\n",
      "Epoch 1778, Loss: 0.4595460593700409, Final Batch Loss: 0.252359002828598\n",
      "Epoch 1779, Loss: 0.4722462147474289, Final Batch Loss: 0.26167207956314087\n",
      "Epoch 1780, Loss: 0.44369712471961975, Final Batch Loss: 0.23234587907791138\n",
      "Epoch 1781, Loss: 0.40152329206466675, Final Batch Loss: 0.16626881062984467\n",
      "Epoch 1782, Loss: 0.48840460181236267, Final Batch Loss: 0.26169005036354065\n",
      "Epoch 1783, Loss: 0.47363507747650146, Final Batch Loss: 0.24850846827030182\n",
      "Epoch 1784, Loss: 0.4521847516298294, Final Batch Loss: 0.22327616810798645\n",
      "Epoch 1785, Loss: 0.45079362392425537, Final Batch Loss: 0.23709674179553986\n",
      "Epoch 1786, Loss: 0.504235714673996, Final Batch Loss: 0.271027535200119\n",
      "Epoch 1787, Loss: 0.3943284898996353, Final Batch Loss: 0.20062966644763947\n",
      "Epoch 1788, Loss: 0.4461805820465088, Final Batch Loss: 0.2309572994709015\n",
      "Epoch 1789, Loss: 0.38349877297878265, Final Batch Loss: 0.15094086527824402\n",
      "Epoch 1790, Loss: 0.5179775208234787, Final Batch Loss: 0.2827300727367401\n",
      "Epoch 1791, Loss: 0.3784915655851364, Final Batch Loss: 0.17646130919456482\n",
      "Epoch 1792, Loss: 0.40784241259098053, Final Batch Loss: 0.20981961488723755\n",
      "Epoch 1793, Loss: 0.5095097124576569, Final Batch Loss: 0.2550574541091919\n",
      "Epoch 1794, Loss: 0.39230507612228394, Final Batch Loss: 0.2027280479669571\n",
      "Epoch 1795, Loss: 0.44883404672145844, Final Batch Loss: 0.19378428161144257\n",
      "Epoch 1796, Loss: 0.4403395801782608, Final Batch Loss: 0.24261201918125153\n",
      "Epoch 1797, Loss: 0.42360593378543854, Final Batch Loss: 0.21162663400173187\n",
      "Epoch 1798, Loss: 0.42900441586971283, Final Batch Loss: 0.21846458315849304\n",
      "Epoch 1799, Loss: 0.42150136828422546, Final Batch Loss: 0.2115584909915924\n",
      "Epoch 1800, Loss: 0.4091772586107254, Final Batch Loss: 0.18208757042884827\n",
      "Epoch 1801, Loss: 0.47305601835250854, Final Batch Loss: 0.2206435203552246\n",
      "Epoch 1802, Loss: 0.4044284522533417, Final Batch Loss: 0.1973946988582611\n",
      "Epoch 1803, Loss: 0.3903133124113083, Final Batch Loss: 0.19467060267925262\n",
      "Epoch 1804, Loss: 0.4015851765871048, Final Batch Loss: 0.20631402730941772\n",
      "Epoch 1805, Loss: 0.45598120987415314, Final Batch Loss: 0.18591468036174774\n",
      "Epoch 1806, Loss: 0.45358990132808685, Final Batch Loss: 0.2451091706752777\n",
      "Epoch 1807, Loss: 0.47454895079135895, Final Batch Loss: 0.24302075803279877\n",
      "Epoch 1808, Loss: 0.45844949781894684, Final Batch Loss: 0.25611943006515503\n",
      "Epoch 1809, Loss: 0.4561045318841934, Final Batch Loss: 0.22580888867378235\n",
      "Epoch 1810, Loss: 0.4207797050476074, Final Batch Loss: 0.17944929003715515\n",
      "Epoch 1811, Loss: 0.4086064398288727, Final Batch Loss: 0.19103862345218658\n",
      "Epoch 1812, Loss: 0.46769875288009644, Final Batch Loss: 0.25656160712242126\n",
      "Epoch 1813, Loss: 0.45945100486278534, Final Batch Loss: 0.23848974704742432\n",
      "Epoch 1814, Loss: 0.4262631833553314, Final Batch Loss: 0.22290290892124176\n",
      "Epoch 1815, Loss: 0.40033964812755585, Final Batch Loss: 0.18156713247299194\n",
      "Epoch 1816, Loss: 0.41826561093330383, Final Batch Loss: 0.21350346505641937\n",
      "Epoch 1817, Loss: 0.4728734493255615, Final Batch Loss: 0.2568431496620178\n",
      "Epoch 1818, Loss: 0.44132566452026367, Final Batch Loss: 0.22207508981227875\n",
      "Epoch 1819, Loss: 0.4300408214330673, Final Batch Loss: 0.2820374369621277\n",
      "Epoch 1820, Loss: 0.4068482220172882, Final Batch Loss: 0.22420302033424377\n",
      "Epoch 1821, Loss: 0.4244852513074875, Final Batch Loss: 0.20904263854026794\n",
      "Epoch 1822, Loss: 0.4199536144733429, Final Batch Loss: 0.19312097132205963\n",
      "Epoch 1823, Loss: 0.3886990547180176, Final Batch Loss: 0.19291594624519348\n",
      "Epoch 1824, Loss: 0.41123606264591217, Final Batch Loss: 0.2004794329404831\n",
      "Epoch 1825, Loss: 0.38041798770427704, Final Batch Loss: 0.19483935832977295\n",
      "Epoch 1826, Loss: 0.36458835005760193, Final Batch Loss: 0.15284883975982666\n",
      "Epoch 1827, Loss: 0.4090813398361206, Final Batch Loss: 0.18840283155441284\n",
      "Epoch 1828, Loss: 0.46529437601566315, Final Batch Loss: 0.24240241944789886\n",
      "Epoch 1829, Loss: 0.42189277708530426, Final Batch Loss: 0.2122666835784912\n",
      "Epoch 1830, Loss: 0.4244668036699295, Final Batch Loss: 0.2232002317905426\n",
      "Epoch 1831, Loss: 0.42011187970638275, Final Batch Loss: 0.23682379722595215\n",
      "Epoch 1832, Loss: 0.4122720956802368, Final Batch Loss: 0.24791446328163147\n",
      "Epoch 1833, Loss: 0.45817501842975616, Final Batch Loss: 0.23942843079566956\n",
      "Epoch 1834, Loss: 0.36656780540943146, Final Batch Loss: 0.14363659918308258\n",
      "Epoch 1835, Loss: 0.43009932339191437, Final Batch Loss: 0.19050267338752747\n",
      "Epoch 1836, Loss: 0.42892031371593475, Final Batch Loss: 0.21937935054302216\n",
      "Epoch 1837, Loss: 0.4485418498516083, Final Batch Loss: 0.24200649559497833\n",
      "Epoch 1838, Loss: 0.3872363716363907, Final Batch Loss: 0.18911582231521606\n",
      "Epoch 1839, Loss: 0.39931072294712067, Final Batch Loss: 0.17446337640285492\n",
      "Epoch 1840, Loss: 0.3773222714662552, Final Batch Loss: 0.21417884528636932\n",
      "Epoch 1841, Loss: 0.4016807824373245, Final Batch Loss: 0.1810779720544815\n",
      "Epoch 1842, Loss: 0.37490297853946686, Final Batch Loss: 0.18164785206317902\n",
      "Epoch 1843, Loss: 0.41949836909770966, Final Batch Loss: 0.17079485952854156\n",
      "Epoch 1844, Loss: 0.47613154351711273, Final Batch Loss: 0.24060821533203125\n",
      "Epoch 1845, Loss: 0.4136851727962494, Final Batch Loss: 0.19673076272010803\n",
      "Epoch 1846, Loss: 0.5407208651304245, Final Batch Loss: 0.3024396002292633\n",
      "Epoch 1847, Loss: 0.38953082263469696, Final Batch Loss: 0.21310696005821228\n",
      "Epoch 1848, Loss: 0.3806777894496918, Final Batch Loss: 0.19941329956054688\n",
      "Epoch 1849, Loss: 0.4414856880903244, Final Batch Loss: 0.26639384031295776\n",
      "Epoch 1850, Loss: 0.37641800940036774, Final Batch Loss: 0.18248338997364044\n",
      "Epoch 1851, Loss: 0.3543780446052551, Final Batch Loss: 0.19549639523029327\n",
      "Epoch 1852, Loss: 0.43245215713977814, Final Batch Loss: 0.21187196671962738\n",
      "Epoch 1853, Loss: 0.4619944840669632, Final Batch Loss: 0.1502453237771988\n",
      "Epoch 1854, Loss: 0.4425625801086426, Final Batch Loss: 0.20387056469917297\n",
      "Epoch 1855, Loss: 0.37380868196487427, Final Batch Loss: 0.20389507710933685\n",
      "Epoch 1856, Loss: 0.4312157779932022, Final Batch Loss: 0.2508794367313385\n",
      "Epoch 1857, Loss: 0.4246435761451721, Final Batch Loss: 0.19224423170089722\n",
      "Epoch 1858, Loss: 0.3914034962654114, Final Batch Loss: 0.22472764551639557\n",
      "Epoch 1859, Loss: 0.3825487792491913, Final Batch Loss: 0.20169483125209808\n",
      "Epoch 1860, Loss: 0.400261253118515, Final Batch Loss: 0.19203253090381622\n",
      "Epoch 1861, Loss: 0.40300969779491425, Final Batch Loss: 0.19189760088920593\n",
      "Epoch 1862, Loss: 0.3672282099723816, Final Batch Loss: 0.18608446419239044\n",
      "Epoch 1863, Loss: 0.40400315821170807, Final Batch Loss: 0.2047610580921173\n",
      "Epoch 1864, Loss: 0.39613910019397736, Final Batch Loss: 0.22974629700183868\n",
      "Epoch 1865, Loss: 0.453997865319252, Final Batch Loss: 0.2081262320280075\n",
      "Epoch 1866, Loss: 0.39825059473514557, Final Batch Loss: 0.18550743162631989\n",
      "Epoch 1867, Loss: 0.39280641078948975, Final Batch Loss: 0.16399963200092316\n",
      "Epoch 1868, Loss: 0.37625260651111603, Final Batch Loss: 0.20371867716312408\n",
      "Epoch 1869, Loss: 0.38707002997398376, Final Batch Loss: 0.23595774173736572\n",
      "Epoch 1870, Loss: 0.4295926094055176, Final Batch Loss: 0.21872800588607788\n",
      "Epoch 1871, Loss: 0.36771203577518463, Final Batch Loss: 0.21940796077251434\n",
      "Epoch 1872, Loss: 0.38382379710674286, Final Batch Loss: 0.21455256640911102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1873, Loss: 0.39068350195884705, Final Batch Loss: 0.17281705141067505\n",
      "Epoch 1874, Loss: 0.38341978192329407, Final Batch Loss: 0.18114544451236725\n",
      "Epoch 1875, Loss: 0.3666466176509857, Final Batch Loss: 0.20363503694534302\n",
      "Epoch 1876, Loss: 0.3914007395505905, Final Batch Loss: 0.19687426090240479\n",
      "Epoch 1877, Loss: 0.4404821842908859, Final Batch Loss: 0.19334891438484192\n",
      "Epoch 1878, Loss: 0.39115476608276367, Final Batch Loss: 0.18240942060947418\n",
      "Epoch 1879, Loss: 0.36731159687042236, Final Batch Loss: 0.2006596177816391\n",
      "Epoch 1880, Loss: 0.31648923456668854, Final Batch Loss: 0.14139175415039062\n",
      "Epoch 1881, Loss: 0.38745054602622986, Final Batch Loss: 0.15664508938789368\n",
      "Epoch 1882, Loss: 0.4156138002872467, Final Batch Loss: 0.20454302430152893\n",
      "Epoch 1883, Loss: 0.40171973407268524, Final Batch Loss: 0.19051599502563477\n",
      "Epoch 1884, Loss: 0.3692709505558014, Final Batch Loss: 0.19068795442581177\n",
      "Epoch 1885, Loss: 0.3920047879219055, Final Batch Loss: 0.19078290462493896\n",
      "Epoch 1886, Loss: 0.4629621356725693, Final Batch Loss: 0.2469712644815445\n",
      "Epoch 1887, Loss: 0.3901306986808777, Final Batch Loss: 0.16604390740394592\n",
      "Epoch 1888, Loss: 0.43594348430633545, Final Batch Loss: 0.22793415188789368\n",
      "Epoch 1889, Loss: 0.436187207698822, Final Batch Loss: 0.19038932025432587\n",
      "Epoch 1890, Loss: 0.3647918552160263, Final Batch Loss: 0.18290925025939941\n",
      "Epoch 1891, Loss: 0.4185221940279007, Final Batch Loss: 0.19867165386676788\n",
      "Epoch 1892, Loss: 0.4136117100715637, Final Batch Loss: 0.16355985403060913\n",
      "Epoch 1893, Loss: 0.39004726707935333, Final Batch Loss: 0.15416620671749115\n",
      "Epoch 1894, Loss: 0.42369744181632996, Final Batch Loss: 0.22233858704566956\n",
      "Epoch 1895, Loss: 0.4231231063604355, Final Batch Loss: 0.21385790407657623\n",
      "Epoch 1896, Loss: 0.41417108476161957, Final Batch Loss: 0.23377454280853271\n",
      "Epoch 1897, Loss: 0.416194349527359, Final Batch Loss: 0.16973471641540527\n",
      "Epoch 1898, Loss: 0.34686049818992615, Final Batch Loss: 0.15985319018363953\n",
      "Epoch 1899, Loss: 0.42038998007774353, Final Batch Loss: 0.24707342684268951\n",
      "Epoch 1900, Loss: 0.363998681306839, Final Batch Loss: 0.20505309104919434\n",
      "Epoch 1901, Loss: 0.3857308179140091, Final Batch Loss: 0.16097266972064972\n",
      "Epoch 1902, Loss: 0.34037843346595764, Final Batch Loss: 0.14170388877391815\n",
      "Epoch 1903, Loss: 0.37447234988212585, Final Batch Loss: 0.17912998795509338\n",
      "Epoch 1904, Loss: 0.49287860095500946, Final Batch Loss: 0.2592288553714752\n",
      "Epoch 1905, Loss: 0.43125292658805847, Final Batch Loss: 0.1743447482585907\n",
      "Epoch 1906, Loss: 0.43997469544410706, Final Batch Loss: 0.2138468474149704\n",
      "Epoch 1907, Loss: 0.3414905220270157, Final Batch Loss: 0.16222092509269714\n",
      "Epoch 1908, Loss: 0.3608279824256897, Final Batch Loss: 0.18346019089221954\n",
      "Epoch 1909, Loss: 0.38609579205513, Final Batch Loss: 0.2225096970796585\n",
      "Epoch 1910, Loss: 0.36985188722610474, Final Batch Loss: 0.19093500077724457\n",
      "Epoch 1911, Loss: 0.4174692779779434, Final Batch Loss: 0.2204003930091858\n",
      "Epoch 1912, Loss: 0.40376928448677063, Final Batch Loss: 0.1939595341682434\n",
      "Epoch 1913, Loss: 0.4006166309118271, Final Batch Loss: 0.2084421068429947\n",
      "Epoch 1914, Loss: 0.382002130150795, Final Batch Loss: 0.17490610480308533\n",
      "Epoch 1915, Loss: 0.3513595014810562, Final Batch Loss: 0.17700211703777313\n",
      "Epoch 1916, Loss: 0.3719373494386673, Final Batch Loss: 0.20136365294456482\n",
      "Epoch 1917, Loss: 0.3217795193195343, Final Batch Loss: 0.16674312949180603\n",
      "Epoch 1918, Loss: 0.37029726803302765, Final Batch Loss: 0.1617758870124817\n",
      "Epoch 1919, Loss: 0.351472869515419, Final Batch Loss: 0.15657414495944977\n",
      "Epoch 1920, Loss: 0.44940894842147827, Final Batch Loss: 0.20244193077087402\n",
      "Epoch 1921, Loss: 0.4250103086233139, Final Batch Loss: 0.23225951194763184\n",
      "Epoch 1922, Loss: 0.42181994020938873, Final Batch Loss: 0.21972616016864777\n",
      "Epoch 1923, Loss: 0.39828550815582275, Final Batch Loss: 0.1777108758687973\n",
      "Epoch 1924, Loss: 0.4359568953514099, Final Batch Loss: 0.24071794748306274\n",
      "Epoch 1925, Loss: 0.3805244117975235, Final Batch Loss: 0.16445502638816833\n",
      "Epoch 1926, Loss: 0.3904169648885727, Final Batch Loss: 0.1872871369123459\n",
      "Epoch 1927, Loss: 0.3943319320678711, Final Batch Loss: 0.188187837600708\n",
      "Epoch 1928, Loss: 0.3926621675491333, Final Batch Loss: 0.19440202414989471\n",
      "Epoch 1929, Loss: 0.4516809433698654, Final Batch Loss: 0.29174283146858215\n",
      "Epoch 1930, Loss: 0.4019661098718643, Final Batch Loss: 0.17557761073112488\n",
      "Epoch 1931, Loss: 0.37671537697315216, Final Batch Loss: 0.19344079494476318\n",
      "Epoch 1932, Loss: 0.3756226897239685, Final Batch Loss: 0.15230245888233185\n",
      "Epoch 1933, Loss: 0.33715158700942993, Final Batch Loss: 0.15535445511341095\n",
      "Epoch 1934, Loss: 0.3888593316078186, Final Batch Loss: 0.1557469218969345\n",
      "Epoch 1935, Loss: 0.36973775923252106, Final Batch Loss: 0.22341246902942657\n",
      "Epoch 1936, Loss: 0.4378565400838852, Final Batch Loss: 0.20070137083530426\n",
      "Epoch 1937, Loss: 0.36708368360996246, Final Batch Loss: 0.185832217335701\n",
      "Epoch 1938, Loss: 0.364752933382988, Final Batch Loss: 0.17481516301631927\n",
      "Epoch 1939, Loss: 0.37814609706401825, Final Batch Loss: 0.18988904356956482\n",
      "Epoch 1940, Loss: 0.4127017706632614, Final Batch Loss: 0.18412019312381744\n",
      "Epoch 1941, Loss: 0.3692304491996765, Final Batch Loss: 0.20961615443229675\n",
      "Epoch 1942, Loss: 0.37566937506198883, Final Batch Loss: 0.17394091188907623\n",
      "Epoch 1943, Loss: 0.41140738129615784, Final Batch Loss: 0.23761343955993652\n",
      "Epoch 1944, Loss: 0.388911709189415, Final Batch Loss: 0.22595976293087006\n",
      "Epoch 1945, Loss: 0.37437310814857483, Final Batch Loss: 0.13997213542461395\n",
      "Epoch 1946, Loss: 0.37367647886276245, Final Batch Loss: 0.15507836639881134\n",
      "Epoch 1947, Loss: 0.36727628111839294, Final Batch Loss: 0.16317997872829437\n",
      "Epoch 1948, Loss: 0.44107572734355927, Final Batch Loss: 0.23476943373680115\n",
      "Epoch 1949, Loss: 0.40282441675662994, Final Batch Loss: 0.19770091772079468\n",
      "Epoch 1950, Loss: 0.36573179066181183, Final Batch Loss: 0.22217920422554016\n",
      "Epoch 1951, Loss: 0.391262486577034, Final Batch Loss: 0.16808192431926727\n",
      "Epoch 1952, Loss: 0.3916941434144974, Final Batch Loss: 0.21677756309509277\n",
      "Epoch 1953, Loss: 0.37810198962688446, Final Batch Loss: 0.19634316861629486\n",
      "Epoch 1954, Loss: 0.32591046392917633, Final Batch Loss: 0.14265801012516022\n",
      "Epoch 1955, Loss: 0.3490306884050369, Final Batch Loss: 0.1606566309928894\n",
      "Epoch 1956, Loss: 0.33583755791187286, Final Batch Loss: 0.18972966074943542\n",
      "Epoch 1957, Loss: 0.38173265755176544, Final Batch Loss: 0.23737074434757233\n",
      "Epoch 1958, Loss: 0.34198926389217377, Final Batch Loss: 0.16732364892959595\n",
      "Epoch 1959, Loss: 0.39358313381671906, Final Batch Loss: 0.23210269212722778\n",
      "Epoch 1960, Loss: 0.356306329369545, Final Batch Loss: 0.18473336100578308\n",
      "Epoch 1961, Loss: 0.338699534535408, Final Batch Loss: 0.14572520554065704\n",
      "Epoch 1962, Loss: 0.3628058433532715, Final Batch Loss: 0.14181791245937347\n",
      "Epoch 1963, Loss: 0.3856799006462097, Final Batch Loss: 0.1894683837890625\n",
      "Epoch 1964, Loss: 0.3665250390768051, Final Batch Loss: 0.19157208502292633\n",
      "Epoch 1965, Loss: 0.3553764969110489, Final Batch Loss: 0.1487104594707489\n",
      "Epoch 1966, Loss: 0.36414797604084015, Final Batch Loss: 0.21629275381565094\n",
      "Epoch 1967, Loss: 0.3943418711423874, Final Batch Loss: 0.21665537357330322\n",
      "Epoch 1968, Loss: 0.35922516882419586, Final Batch Loss: 0.2036852091550827\n",
      "Epoch 1969, Loss: 0.35164354741573334, Final Batch Loss: 0.17164909839630127\n",
      "Epoch 1970, Loss: 0.39683687686920166, Final Batch Loss: 0.186063751578331\n",
      "Epoch 1971, Loss: 0.35218052566051483, Final Batch Loss: 0.2026400864124298\n",
      "Epoch 1972, Loss: 0.33749721944332123, Final Batch Loss: 0.16526265442371368\n",
      "Epoch 1973, Loss: 0.3667890131473541, Final Batch Loss: 0.13347631692886353\n",
      "Epoch 1974, Loss: 0.346875861287117, Final Batch Loss: 0.16143135726451874\n",
      "Epoch 1975, Loss: 0.3534078449010849, Final Batch Loss: 0.15982353687286377\n",
      "Epoch 1976, Loss: 0.3559648394584656, Final Batch Loss: 0.20339423418045044\n",
      "Epoch 1977, Loss: 0.369943767786026, Final Batch Loss: 0.18660090863704681\n",
      "Epoch 1978, Loss: 0.39762546122074127, Final Batch Loss: 0.16681985557079315\n",
      "Epoch 1979, Loss: 0.40210315585136414, Final Batch Loss: 0.184526264667511\n",
      "Epoch 1980, Loss: 0.30831320583820343, Final Batch Loss: 0.12490397691726685\n",
      "Epoch 1981, Loss: 0.39644214510917664, Final Batch Loss: 0.17636604607105255\n",
      "Epoch 1982, Loss: 0.4368920475244522, Final Batch Loss: 0.2677161991596222\n",
      "Epoch 1983, Loss: 0.41353969275951385, Final Batch Loss: 0.1648569405078888\n",
      "Epoch 1984, Loss: 0.40694358944892883, Final Batch Loss: 0.23809662461280823\n",
      "Epoch 1985, Loss: 0.37549056112766266, Final Batch Loss: 0.13958673179149628\n",
      "Epoch 1986, Loss: 0.3771824389696121, Final Batch Loss: 0.16553854942321777\n",
      "Epoch 1987, Loss: 0.342498317360878, Final Batch Loss: 0.16375279426574707\n",
      "Epoch 1988, Loss: 0.3993614912033081, Final Batch Loss: 0.21627065539360046\n",
      "Epoch 1989, Loss: 0.38854534924030304, Final Batch Loss: 0.16688326001167297\n",
      "Epoch 1990, Loss: 0.37652258574962616, Final Batch Loss: 0.18891935050487518\n",
      "Epoch 1991, Loss: 0.3645596206188202, Final Batch Loss: 0.16265197098255157\n",
      "Epoch 1992, Loss: 0.3469238430261612, Final Batch Loss: 0.16194546222686768\n",
      "Epoch 1993, Loss: 0.32268716394901276, Final Batch Loss: 0.13253594934940338\n",
      "Epoch 1994, Loss: 0.36759425699710846, Final Batch Loss: 0.17078307271003723\n",
      "Epoch 1995, Loss: 0.4059869796037674, Final Batch Loss: 0.21073883771896362\n",
      "Epoch 1996, Loss: 0.3539421111345291, Final Batch Loss: 0.17198683321475983\n",
      "Epoch 1997, Loss: 0.3980500251054764, Final Batch Loss: 0.20035472512245178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1998, Loss: 0.35554489493370056, Final Batch Loss: 0.15957964956760406\n",
      "Epoch 1999, Loss: 0.36628489196300507, Final Batch Loss: 0.17046670615673065\n",
      "Epoch 2000, Loss: 0.32498013973236084, Final Batch Loss: 0.16363948583602905\n",
      "Epoch 2001, Loss: 0.4020327627658844, Final Batch Loss: 0.21869108080863953\n",
      "Epoch 2002, Loss: 0.3728075325489044, Final Batch Loss: 0.22929559648036957\n",
      "Epoch 2003, Loss: 0.3953516334295273, Final Batch Loss: 0.16059088706970215\n",
      "Epoch 2004, Loss: 0.381160169839859, Final Batch Loss: 0.17120930552482605\n",
      "Epoch 2005, Loss: 0.4549185335636139, Final Batch Loss: 0.20121610164642334\n",
      "Epoch 2006, Loss: 0.447173073887825, Final Batch Loss: 0.23950181901454926\n",
      "Epoch 2007, Loss: 0.31133581697940826, Final Batch Loss: 0.14737410843372345\n",
      "Epoch 2008, Loss: 0.3995978385210037, Final Batch Loss: 0.19522212445735931\n",
      "Epoch 2009, Loss: 0.3611751198768616, Final Batch Loss: 0.16325901448726654\n",
      "Epoch 2010, Loss: 0.34992317855358124, Final Batch Loss: 0.2100743055343628\n",
      "Epoch 2011, Loss: 0.3268357962369919, Final Batch Loss: 0.1677185297012329\n",
      "Epoch 2012, Loss: 0.3600911498069763, Final Batch Loss: 0.17608672380447388\n",
      "Epoch 2013, Loss: 0.39685599505901337, Final Batch Loss: 0.24141663312911987\n",
      "Epoch 2014, Loss: 0.311938613653183, Final Batch Loss: 0.14812763035297394\n",
      "Epoch 2015, Loss: 0.3369248956441879, Final Batch Loss: 0.17221353948116302\n",
      "Epoch 2016, Loss: 0.3484206944704056, Final Batch Loss: 0.15346506237983704\n",
      "Epoch 2017, Loss: 0.3807288855314255, Final Batch Loss: 0.17553403973579407\n",
      "Epoch 2018, Loss: 0.3770567923784256, Final Batch Loss: 0.1926962286233902\n",
      "Epoch 2019, Loss: 0.45186033844947815, Final Batch Loss: 0.24280162155628204\n",
      "Epoch 2020, Loss: 0.3118091970682144, Final Batch Loss: 0.15788176655769348\n",
      "Epoch 2021, Loss: 0.3708435744047165, Final Batch Loss: 0.18129625916481018\n",
      "Epoch 2022, Loss: 0.4031965285539627, Final Batch Loss: 0.21623672544956207\n",
      "Epoch 2023, Loss: 0.4411775469779968, Final Batch Loss: 0.20602872967720032\n",
      "Epoch 2024, Loss: 0.33177410066127777, Final Batch Loss: 0.1913231909275055\n",
      "Epoch 2025, Loss: 0.3855573534965515, Final Batch Loss: 0.20123665034770966\n",
      "Epoch 2026, Loss: 0.3258470892906189, Final Batch Loss: 0.14008603990077972\n",
      "Epoch 2027, Loss: 0.39432279765605927, Final Batch Loss: 0.21892978250980377\n",
      "Epoch 2028, Loss: 0.3555472493171692, Final Batch Loss: 0.18526475131511688\n",
      "Epoch 2029, Loss: 0.4608642905950546, Final Batch Loss: 0.22615255415439606\n",
      "Epoch 2030, Loss: 0.36236824095249176, Final Batch Loss: 0.19253067672252655\n",
      "Epoch 2031, Loss: 0.3795449584722519, Final Batch Loss: 0.2266675978899002\n",
      "Epoch 2032, Loss: 0.3035922273993492, Final Batch Loss: 0.1805507242679596\n",
      "Epoch 2033, Loss: 0.36899006366729736, Final Batch Loss: 0.15841835737228394\n",
      "Epoch 2034, Loss: 0.3823346644639969, Final Batch Loss: 0.18945397436618805\n",
      "Epoch 2035, Loss: 0.36707867681980133, Final Batch Loss: 0.20144352316856384\n",
      "Epoch 2036, Loss: 0.3403661698102951, Final Batch Loss: 0.1488473117351532\n",
      "Epoch 2037, Loss: 0.4104452580213547, Final Batch Loss: 0.20889675617218018\n",
      "Epoch 2038, Loss: 0.32884474098682404, Final Batch Loss: 0.1700783222913742\n",
      "Epoch 2039, Loss: 0.38534000515937805, Final Batch Loss: 0.155243381857872\n",
      "Epoch 2040, Loss: 0.4285619556903839, Final Batch Loss: 0.2019583135843277\n",
      "Epoch 2041, Loss: 0.32650206983089447, Final Batch Loss: 0.15008696913719177\n",
      "Epoch 2042, Loss: 0.3377232998609543, Final Batch Loss: 0.16279274225234985\n",
      "Epoch 2043, Loss: 0.32193537056446075, Final Batch Loss: 0.1433357447385788\n",
      "Epoch 2044, Loss: 0.3850148618221283, Final Batch Loss: 0.21035873889923096\n",
      "Epoch 2045, Loss: 0.38920997083187103, Final Batch Loss: 0.1995328664779663\n",
      "Epoch 2046, Loss: 0.3406696915626526, Final Batch Loss: 0.1621745526790619\n",
      "Epoch 2047, Loss: 0.3601527810096741, Final Batch Loss: 0.19505317509174347\n",
      "Epoch 2048, Loss: 0.35301995277404785, Final Batch Loss: 0.1455933153629303\n",
      "Epoch 2049, Loss: 0.3885340243577957, Final Batch Loss: 0.2173236906528473\n",
      "Epoch 2050, Loss: 0.3572738915681839, Final Batch Loss: 0.19255520403385162\n",
      "Epoch 2051, Loss: 0.3298366516828537, Final Batch Loss: 0.16346964240074158\n",
      "Epoch 2052, Loss: 0.3805340379476547, Final Batch Loss: 0.20252588391304016\n",
      "Epoch 2053, Loss: 0.30702224373817444, Final Batch Loss: 0.14636124670505524\n",
      "Epoch 2054, Loss: 0.4381352663040161, Final Batch Loss: 0.23699061572551727\n",
      "Epoch 2055, Loss: 0.32552194595336914, Final Batch Loss: 0.1322019249200821\n",
      "Epoch 2056, Loss: 0.36695361137390137, Final Batch Loss: 0.18115928769111633\n",
      "Epoch 2057, Loss: 0.37533918023109436, Final Batch Loss: 0.17400845885276794\n",
      "Epoch 2058, Loss: 0.4284271150827408, Final Batch Loss: 0.2473134845495224\n",
      "Epoch 2059, Loss: 0.41389214992523193, Final Batch Loss: 0.22014513611793518\n",
      "Epoch 2060, Loss: 0.37880946695804596, Final Batch Loss: 0.19722945988178253\n",
      "Epoch 2061, Loss: 0.3739618808031082, Final Batch Loss: 0.20887871086597443\n",
      "Epoch 2062, Loss: 0.37384307384490967, Final Batch Loss: 0.17245489358901978\n",
      "Epoch 2063, Loss: 0.35072119534015656, Final Batch Loss: 0.13394670188426971\n",
      "Epoch 2064, Loss: 0.3601391762495041, Final Batch Loss: 0.16840170323848724\n",
      "Epoch 2065, Loss: 0.31609904766082764, Final Batch Loss: 0.150599405169487\n",
      "Epoch 2066, Loss: 0.32135624438524246, Final Batch Loss: 0.11958111077547073\n",
      "Epoch 2067, Loss: 0.3396838903427124, Final Batch Loss: 0.15521347522735596\n",
      "Epoch 2068, Loss: 0.3924344480037689, Final Batch Loss: 0.2021172195672989\n",
      "Epoch 2069, Loss: 0.3169972449541092, Final Batch Loss: 0.1662590056657791\n",
      "Epoch 2070, Loss: 0.3656911253929138, Final Batch Loss: 0.1749243140220642\n",
      "Epoch 2071, Loss: 0.3203897327184677, Final Batch Loss: 0.16623695194721222\n",
      "Epoch 2072, Loss: 0.3327561169862747, Final Batch Loss: 0.16676099598407745\n",
      "Epoch 2073, Loss: 0.355161190032959, Final Batch Loss: 0.18008512258529663\n",
      "Epoch 2074, Loss: 0.3039565682411194, Final Batch Loss: 0.1497754007577896\n",
      "Epoch 2075, Loss: 0.4202522337436676, Final Batch Loss: 0.21084703505039215\n",
      "Epoch 2076, Loss: 0.32248975336551666, Final Batch Loss: 0.15984728932380676\n",
      "Epoch 2077, Loss: 0.32252295315265656, Final Batch Loss: 0.17369315028190613\n",
      "Epoch 2078, Loss: 0.3625940531492233, Final Batch Loss: 0.1768234521150589\n",
      "Epoch 2079, Loss: 0.29750819504261017, Final Batch Loss: 0.1450638473033905\n",
      "Epoch 2080, Loss: 0.3220777362585068, Final Batch Loss: 0.16788773238658905\n",
      "Epoch 2081, Loss: 0.30811214447021484, Final Batch Loss: 0.13542096316814423\n",
      "Epoch 2082, Loss: 0.45306602120399475, Final Batch Loss: 0.20566996932029724\n",
      "Epoch 2083, Loss: 0.35886943340301514, Final Batch Loss: 0.1663150191307068\n",
      "Epoch 2084, Loss: 0.32197780907154083, Final Batch Loss: 0.1692156046628952\n",
      "Epoch 2085, Loss: 0.354447603225708, Final Batch Loss: 0.1801404058933258\n",
      "Epoch 2086, Loss: 0.3284786492586136, Final Batch Loss: 0.14738060534000397\n",
      "Epoch 2087, Loss: 0.45222826302051544, Final Batch Loss: 0.26690778136253357\n",
      "Epoch 2088, Loss: 0.43775132298469543, Final Batch Loss: 0.2074267715215683\n",
      "Epoch 2089, Loss: 0.37515537440776825, Final Batch Loss: 0.1633688360452652\n",
      "Epoch 2090, Loss: 0.29463131725788116, Final Batch Loss: 0.13827453553676605\n",
      "Epoch 2091, Loss: 0.2927939295768738, Final Batch Loss: 0.12964418530464172\n",
      "Epoch 2092, Loss: 0.36810024082660675, Final Batch Loss: 0.17621582746505737\n",
      "Epoch 2093, Loss: 0.3394593745470047, Final Batch Loss: 0.1874588429927826\n",
      "Epoch 2094, Loss: 0.2689075320959091, Final Batch Loss: 0.14596468210220337\n",
      "Epoch 2095, Loss: 0.30569030344486237, Final Batch Loss: 0.1665162295103073\n",
      "Epoch 2096, Loss: 0.3605705797672272, Final Batch Loss: 0.18744534254074097\n",
      "Epoch 2097, Loss: 0.3254929631948471, Final Batch Loss: 0.13187748193740845\n",
      "Epoch 2098, Loss: 0.4086785465478897, Final Batch Loss: 0.16815346479415894\n",
      "Epoch 2099, Loss: 0.42785288393497467, Final Batch Loss: 0.22708624601364136\n",
      "Epoch 2100, Loss: 0.40560780465602875, Final Batch Loss: 0.1889706403017044\n",
      "Epoch 2101, Loss: 0.33360736072063446, Final Batch Loss: 0.15096674859523773\n",
      "Epoch 2102, Loss: 0.31768907606601715, Final Batch Loss: 0.1658506691455841\n",
      "Epoch 2103, Loss: 0.31939440965652466, Final Batch Loss: 0.1568876951932907\n",
      "Epoch 2104, Loss: 0.31289976835250854, Final Batch Loss: 0.15726101398468018\n",
      "Epoch 2105, Loss: 0.34819889068603516, Final Batch Loss: 0.1658688336610794\n",
      "Epoch 2106, Loss: 0.3175420016050339, Final Batch Loss: 0.17295442521572113\n",
      "Epoch 2107, Loss: 0.29732777178287506, Final Batch Loss: 0.16949957609176636\n",
      "Epoch 2108, Loss: 0.3478962332010269, Final Batch Loss: 0.18984609842300415\n",
      "Epoch 2109, Loss: 0.3884294778108597, Final Batch Loss: 0.19776351749897003\n",
      "Epoch 2110, Loss: 0.36394599080085754, Final Batch Loss: 0.19351989030838013\n",
      "Epoch 2111, Loss: 0.3276686668395996, Final Batch Loss: 0.14081142842769623\n",
      "Epoch 2112, Loss: 0.3537532240152359, Final Batch Loss: 0.1481403410434723\n",
      "Epoch 2113, Loss: 0.3089659735560417, Final Batch Loss: 0.11610131710767746\n",
      "Epoch 2114, Loss: 0.3442613333463669, Final Batch Loss: 0.1767275184392929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2115, Loss: 0.38124674558639526, Final Batch Loss: 0.2189292460680008\n",
      "Epoch 2116, Loss: 0.48139356076717377, Final Batch Loss: 0.1754416972398758\n",
      "Epoch 2117, Loss: 0.30998755991458893, Final Batch Loss: 0.17128963768482208\n",
      "Epoch 2118, Loss: 0.33444245159626007, Final Batch Loss: 0.17718318104743958\n",
      "Epoch 2119, Loss: 0.31027428805828094, Final Batch Loss: 0.13500796258449554\n",
      "Epoch 2120, Loss: 0.3546968400478363, Final Batch Loss: 0.126021608710289\n",
      "Epoch 2121, Loss: 0.349385529756546, Final Batch Loss: 0.21798084676265717\n",
      "Epoch 2122, Loss: 0.3649463653564453, Final Batch Loss: 0.17422911524772644\n",
      "Epoch 2123, Loss: 0.30433689057826996, Final Batch Loss: 0.17135199904441833\n",
      "Epoch 2124, Loss: 0.3403942734003067, Final Batch Loss: 0.1938803493976593\n",
      "Epoch 2125, Loss: 0.31455086171627045, Final Batch Loss: 0.15838377177715302\n",
      "Epoch 2126, Loss: 0.4378015995025635, Final Batch Loss: 0.2515045702457428\n",
      "Epoch 2127, Loss: 0.37282004952430725, Final Batch Loss: 0.15864251554012299\n",
      "Epoch 2128, Loss: 0.3076225370168686, Final Batch Loss: 0.18589402735233307\n",
      "Epoch 2129, Loss: 0.3389207571744919, Final Batch Loss: 0.19071784615516663\n",
      "Epoch 2130, Loss: 0.3327164649963379, Final Batch Loss: 0.17437776923179626\n",
      "Epoch 2131, Loss: 0.30709192156791687, Final Batch Loss: 0.15521614253520966\n",
      "Epoch 2132, Loss: 0.31869836151599884, Final Batch Loss: 0.13091668486595154\n",
      "Epoch 2133, Loss: 0.32085804641246796, Final Batch Loss: 0.13348707556724548\n",
      "Epoch 2134, Loss: 0.296051561832428, Final Batch Loss: 0.1391465663909912\n",
      "Epoch 2135, Loss: 0.3285960704088211, Final Batch Loss: 0.15634118020534515\n",
      "Epoch 2136, Loss: 0.31111402809619904, Final Batch Loss: 0.1469629555940628\n",
      "Epoch 2137, Loss: 0.34084221720695496, Final Batch Loss: 0.1528887301683426\n",
      "Epoch 2138, Loss: 0.37313637137413025, Final Batch Loss: 0.20440058410167694\n",
      "Epoch 2139, Loss: 0.33204062283039093, Final Batch Loss: 0.18619367480278015\n",
      "Epoch 2140, Loss: 0.2969873696565628, Final Batch Loss: 0.12897077202796936\n",
      "Epoch 2141, Loss: 0.31037385761737823, Final Batch Loss: 0.14601829648017883\n",
      "Epoch 2142, Loss: 0.28879210352897644, Final Batch Loss: 0.1555897295475006\n",
      "Epoch 2143, Loss: 0.29468462616205215, Final Batch Loss: 0.10391499847173691\n",
      "Epoch 2144, Loss: 0.3452032506465912, Final Batch Loss: 0.16323037445545197\n",
      "Epoch 2145, Loss: 0.3113187998533249, Final Batch Loss: 0.18002787232398987\n",
      "Epoch 2146, Loss: 0.3654070496559143, Final Batch Loss: 0.20240850746631622\n",
      "Epoch 2147, Loss: 0.34719009697437286, Final Batch Loss: 0.20724093914031982\n",
      "Epoch 2148, Loss: 0.3103398382663727, Final Batch Loss: 0.1588650494813919\n",
      "Epoch 2149, Loss: 0.38100190460681915, Final Batch Loss: 0.12508155405521393\n",
      "Epoch 2150, Loss: 0.3077428489923477, Final Batch Loss: 0.1549185961484909\n",
      "Epoch 2151, Loss: 0.34595055878162384, Final Batch Loss: 0.1947791874408722\n",
      "Epoch 2152, Loss: 0.3037942498922348, Final Batch Loss: 0.13539697229862213\n",
      "Epoch 2153, Loss: 0.33728381991386414, Final Batch Loss: 0.12794262170791626\n",
      "Epoch 2154, Loss: 0.3007892370223999, Final Batch Loss: 0.15941168367862701\n",
      "Epoch 2155, Loss: 0.308491475880146, Final Batch Loss: 0.11431027203798294\n",
      "Epoch 2156, Loss: 0.3693203181028366, Final Batch Loss: 0.1515524834394455\n",
      "Epoch 2157, Loss: 0.29587166011333466, Final Batch Loss: 0.13979284465312958\n",
      "Epoch 2158, Loss: 0.31730373203754425, Final Batch Loss: 0.160623237490654\n",
      "Epoch 2159, Loss: 0.28964853286743164, Final Batch Loss: 0.14126378297805786\n",
      "Epoch 2160, Loss: 0.35317574441432953, Final Batch Loss: 0.1427324414253235\n",
      "Epoch 2161, Loss: 0.41438598930835724, Final Batch Loss: 0.18172380328178406\n",
      "Epoch 2162, Loss: 0.31661446392536163, Final Batch Loss: 0.15940843522548676\n",
      "Epoch 2163, Loss: 0.4075312316417694, Final Batch Loss: 0.24582409858703613\n",
      "Epoch 2164, Loss: 0.3376746028661728, Final Batch Loss: 0.20319581031799316\n",
      "Epoch 2165, Loss: 0.41514572501182556, Final Batch Loss: 0.19501763582229614\n",
      "Epoch 2166, Loss: 0.2621588557958603, Final Batch Loss: 0.12762846052646637\n",
      "Epoch 2167, Loss: 0.3068246841430664, Final Batch Loss: 0.1749977022409439\n",
      "Epoch 2168, Loss: 0.33900201320648193, Final Batch Loss: 0.16633093357086182\n",
      "Epoch 2169, Loss: 0.3046201169490814, Final Batch Loss: 0.15133950114250183\n",
      "Epoch 2170, Loss: 0.3491542786359787, Final Batch Loss: 0.18924281001091003\n",
      "Epoch 2171, Loss: 0.3638199269771576, Final Batch Loss: 0.1922394335269928\n",
      "Epoch 2172, Loss: 0.34691257774829865, Final Batch Loss: 0.1946280151605606\n",
      "Epoch 2173, Loss: 0.3993050754070282, Final Batch Loss: 0.26407596468925476\n",
      "Epoch 2174, Loss: 0.349169060587883, Final Batch Loss: 0.17216581106185913\n",
      "Epoch 2175, Loss: 0.33204932510852814, Final Batch Loss: 0.18212617933750153\n",
      "Epoch 2176, Loss: 0.3188161104917526, Final Batch Loss: 0.12598195672035217\n",
      "Epoch 2177, Loss: 0.3034026771783829, Final Batch Loss: 0.14085619151592255\n",
      "Epoch 2178, Loss: 0.31545110046863556, Final Batch Loss: 0.16142264008522034\n",
      "Epoch 2179, Loss: 0.2989947497844696, Final Batch Loss: 0.14844052493572235\n",
      "Epoch 2180, Loss: 0.3425111025571823, Final Batch Loss: 0.15495191514492035\n",
      "Epoch 2181, Loss: 0.3420647233724594, Final Batch Loss: 0.15402135252952576\n",
      "Epoch 2182, Loss: 0.3614766001701355, Final Batch Loss: 0.19445067644119263\n",
      "Epoch 2183, Loss: 0.3684486150741577, Final Batch Loss: 0.1928972750902176\n",
      "Epoch 2184, Loss: 0.35143110156059265, Final Batch Loss: 0.1648101657629013\n",
      "Epoch 2185, Loss: 0.2944502830505371, Final Batch Loss: 0.15185357630252838\n",
      "Epoch 2186, Loss: 0.3131883889436722, Final Batch Loss: 0.12892352044582367\n",
      "Epoch 2187, Loss: 0.3063744679093361, Final Batch Loss: 0.11358361691236496\n",
      "Epoch 2188, Loss: 0.3004218488931656, Final Batch Loss: 0.13487198948860168\n",
      "Epoch 2189, Loss: 0.2981395721435547, Final Batch Loss: 0.13258062303066254\n",
      "Epoch 2190, Loss: 0.3976840227842331, Final Batch Loss: 0.2522323429584503\n",
      "Epoch 2191, Loss: 0.31826426088809967, Final Batch Loss: 0.15771949291229248\n",
      "Epoch 2192, Loss: 0.38196584582328796, Final Batch Loss: 0.19132846593856812\n",
      "Epoch 2193, Loss: 0.2866766005754471, Final Batch Loss: 0.14798744022846222\n",
      "Epoch 2194, Loss: 0.3215259611606598, Final Batch Loss: 0.16623085737228394\n",
      "Epoch 2195, Loss: 0.2932485193014145, Final Batch Loss: 0.14134466648101807\n",
      "Epoch 2196, Loss: 0.30388763546943665, Final Batch Loss: 0.16599030792713165\n",
      "Epoch 2197, Loss: 0.33969901502132416, Final Batch Loss: 0.1658264398574829\n",
      "Epoch 2198, Loss: 0.306286096572876, Final Batch Loss: 0.17678140103816986\n",
      "Epoch 2199, Loss: 0.32144874334335327, Final Batch Loss: 0.18216034770011902\n",
      "Epoch 2200, Loss: 0.2807953655719757, Final Batch Loss: 0.16239304840564728\n",
      "Epoch 2201, Loss: 0.3045509085059166, Final Batch Loss: 0.11728670448064804\n",
      "Epoch 2202, Loss: 0.289101779460907, Final Batch Loss: 0.13453315198421478\n",
      "Epoch 2203, Loss: 0.3647981286048889, Final Batch Loss: 0.19636882841587067\n",
      "Epoch 2204, Loss: 0.4136277437210083, Final Batch Loss: 0.1577666997909546\n",
      "Epoch 2205, Loss: 0.2789934128522873, Final Batch Loss: 0.13989706337451935\n",
      "Epoch 2206, Loss: 0.27327969670295715, Final Batch Loss: 0.14425458014011383\n",
      "Epoch 2207, Loss: 0.3311621695756912, Final Batch Loss: 0.18706901371479034\n",
      "Epoch 2208, Loss: 0.31262050569057465, Final Batch Loss: 0.16195820271968842\n",
      "Epoch 2209, Loss: 0.27499426901340485, Final Batch Loss: 0.10920420289039612\n",
      "Epoch 2210, Loss: 0.3752724826335907, Final Batch Loss: 0.21349945664405823\n",
      "Epoch 2211, Loss: 0.3354286849498749, Final Batch Loss: 0.1840604543685913\n",
      "Epoch 2212, Loss: 0.35680729150772095, Final Batch Loss: 0.15761929750442505\n",
      "Epoch 2213, Loss: 0.3431081175804138, Final Batch Loss: 0.18054664134979248\n",
      "Epoch 2214, Loss: 0.32839466631412506, Final Batch Loss: 0.15970785915851593\n",
      "Epoch 2215, Loss: 0.2642489820718765, Final Batch Loss: 0.15393054485321045\n",
      "Epoch 2216, Loss: 0.32358862459659576, Final Batch Loss: 0.1684120148420334\n",
      "Epoch 2217, Loss: 0.27780377864837646, Final Batch Loss: 0.13939571380615234\n",
      "Epoch 2218, Loss: 0.27628906071186066, Final Batch Loss: 0.13148799538612366\n",
      "Epoch 2219, Loss: 0.3090001195669174, Final Batch Loss: 0.14565931260585785\n",
      "Epoch 2220, Loss: 0.31475014984607697, Final Batch Loss: 0.13475137948989868\n",
      "Epoch 2221, Loss: 0.31003765761852264, Final Batch Loss: 0.1425790935754776\n",
      "Epoch 2222, Loss: 0.3941230773925781, Final Batch Loss: 0.20168587565422058\n",
      "Epoch 2223, Loss: 0.3210943043231964, Final Batch Loss: 0.17161519825458527\n",
      "Epoch 2224, Loss: 0.3552173376083374, Final Batch Loss: 0.20289066433906555\n",
      "Epoch 2225, Loss: 0.38040220737457275, Final Batch Loss: 0.18192729353904724\n",
      "Epoch 2226, Loss: 0.3494705855846405, Final Batch Loss: 0.17466215789318085\n",
      "Epoch 2227, Loss: 0.2613554373383522, Final Batch Loss: 0.11725089699029922\n",
      "Epoch 2228, Loss: 0.296189546585083, Final Batch Loss: 0.13752493262290955\n",
      "Epoch 2229, Loss: 0.2770298793911934, Final Batch Loss: 0.1713380366563797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2230, Loss: 0.3591791242361069, Final Batch Loss: 0.18068334460258484\n",
      "Epoch 2231, Loss: 0.40184386074543, Final Batch Loss: 0.16694194078445435\n",
      "Epoch 2232, Loss: 0.3460044860839844, Final Batch Loss: 0.18020948767662048\n",
      "Epoch 2233, Loss: 0.31628313660621643, Final Batch Loss: 0.13573963940143585\n",
      "Epoch 2234, Loss: 0.39409828186035156, Final Batch Loss: 0.18898509442806244\n",
      "Epoch 2235, Loss: 0.28986550867557526, Final Batch Loss: 0.12784597277641296\n",
      "Epoch 2236, Loss: 0.2933871001005173, Final Batch Loss: 0.15364861488342285\n",
      "Epoch 2237, Loss: 0.2666090354323387, Final Batch Loss: 0.10704945772886276\n",
      "Epoch 2238, Loss: 0.32370026409626007, Final Batch Loss: 0.19872824847698212\n",
      "Epoch 2239, Loss: 0.30847059190273285, Final Batch Loss: 0.16575594246387482\n",
      "Epoch 2240, Loss: 0.35049211978912354, Final Batch Loss: 0.16368158161640167\n",
      "Epoch 2241, Loss: 0.25823886692523956, Final Batch Loss: 0.1128586083650589\n",
      "Epoch 2242, Loss: 0.2848290354013443, Final Batch Loss: 0.13072386384010315\n",
      "Epoch 2243, Loss: 0.2978748232126236, Final Batch Loss: 0.1256636381149292\n",
      "Epoch 2244, Loss: 0.3325813263654709, Final Batch Loss: 0.1377667486667633\n",
      "Epoch 2245, Loss: 0.2775919884443283, Final Batch Loss: 0.13143275678157806\n",
      "Epoch 2246, Loss: 0.31721173226833344, Final Batch Loss: 0.12860339879989624\n",
      "Epoch 2247, Loss: 0.29305023699998856, Final Batch Loss: 0.11937170475721359\n",
      "Epoch 2248, Loss: 0.28453874588012695, Final Batch Loss: 0.12978290021419525\n",
      "Epoch 2249, Loss: 0.29735782742500305, Final Batch Loss: 0.16132789850234985\n",
      "Epoch 2250, Loss: 0.26217927038669586, Final Batch Loss: 0.1292596310377121\n",
      "Epoch 2251, Loss: 0.3515060693025589, Final Batch Loss: 0.17035987973213196\n",
      "Epoch 2252, Loss: 0.2826012372970581, Final Batch Loss: 0.12543438374996185\n",
      "Epoch 2253, Loss: 0.29262465238571167, Final Batch Loss: 0.1589192897081375\n",
      "Epoch 2254, Loss: 0.2841375172138214, Final Batch Loss: 0.12270987033843994\n",
      "Epoch 2255, Loss: 0.2689577341079712, Final Batch Loss: 0.14746877551078796\n",
      "Epoch 2256, Loss: 0.32257096469402313, Final Batch Loss: 0.15580829977989197\n",
      "Epoch 2257, Loss: 0.319724902510643, Final Batch Loss: 0.16951610147953033\n",
      "Epoch 2258, Loss: 0.28743212670087814, Final Batch Loss: 0.17258477210998535\n",
      "Epoch 2259, Loss: 0.2834482714533806, Final Batch Loss: 0.12349740415811539\n",
      "Epoch 2260, Loss: 0.3098694831132889, Final Batch Loss: 0.17087002098560333\n",
      "Epoch 2261, Loss: 0.3020695596933365, Final Batch Loss: 0.15422461926937103\n",
      "Epoch 2262, Loss: 0.2601170912384987, Final Batch Loss: 0.09858868271112442\n",
      "Epoch 2263, Loss: 0.31020502001047134, Final Batch Loss: 0.1162450984120369\n",
      "Epoch 2264, Loss: 0.3383370041847229, Final Batch Loss: 0.21901418268680573\n",
      "Epoch 2265, Loss: 0.2995605915784836, Final Batch Loss: 0.15205688774585724\n",
      "Epoch 2266, Loss: 0.265177883207798, Final Batch Loss: 0.12286188453435898\n",
      "Epoch 2267, Loss: 0.2946150451898575, Final Batch Loss: 0.17163319885730743\n",
      "Epoch 2268, Loss: 0.3876384496688843, Final Batch Loss: 0.22724635899066925\n",
      "Epoch 2269, Loss: 0.28666310012340546, Final Batch Loss: 0.14890366792678833\n",
      "Epoch 2270, Loss: 0.27026304602622986, Final Batch Loss: 0.1294671595096588\n",
      "Epoch 2271, Loss: 0.3127429932355881, Final Batch Loss: 0.16349472105503082\n",
      "Epoch 2272, Loss: 0.3534950613975525, Final Batch Loss: 0.18616360425949097\n",
      "Epoch 2273, Loss: 0.3042585253715515, Final Batch Loss: 0.17106349766254425\n",
      "Epoch 2274, Loss: 0.3381134569644928, Final Batch Loss: 0.19961082935333252\n",
      "Epoch 2275, Loss: 0.32012657821178436, Final Batch Loss: 0.1592792272567749\n",
      "Epoch 2276, Loss: 0.3022865355014801, Final Batch Loss: 0.1650438904762268\n",
      "Epoch 2277, Loss: 0.38770778477191925, Final Batch Loss: 0.20338128507137299\n",
      "Epoch 2278, Loss: 0.29760487377643585, Final Batch Loss: 0.1464264690876007\n",
      "Epoch 2279, Loss: 0.3318717032670975, Final Batch Loss: 0.18348467350006104\n",
      "Epoch 2280, Loss: 0.3209245055913925, Final Batch Loss: 0.18514439463615417\n",
      "Epoch 2281, Loss: 0.30404627323150635, Final Batch Loss: 0.15449212491512299\n",
      "Epoch 2282, Loss: 0.2950093746185303, Final Batch Loss: 0.1570805013179779\n",
      "Epoch 2283, Loss: 0.35500727593898773, Final Batch Loss: 0.19818639755249023\n",
      "Epoch 2284, Loss: 0.27835613489151, Final Batch Loss: 0.11598396301269531\n",
      "Epoch 2285, Loss: 0.37728072702884674, Final Batch Loss: 0.17427124083042145\n",
      "Epoch 2286, Loss: 0.31085067987442017, Final Batch Loss: 0.17522765696048737\n",
      "Epoch 2287, Loss: 0.35181915760040283, Final Batch Loss: 0.16474148631095886\n",
      "Epoch 2288, Loss: 0.3428025245666504, Final Batch Loss: 0.17082886397838593\n",
      "Epoch 2289, Loss: 0.336435467004776, Final Batch Loss: 0.16884639859199524\n",
      "Epoch 2290, Loss: 0.34163127839565277, Final Batch Loss: 0.18401207029819489\n",
      "Epoch 2291, Loss: 0.322777658700943, Final Batch Loss: 0.15221473574638367\n",
      "Epoch 2292, Loss: 0.2900993973016739, Final Batch Loss: 0.1332319974899292\n",
      "Epoch 2293, Loss: 0.2360990345478058, Final Batch Loss: 0.11652141064405441\n",
      "Epoch 2294, Loss: 0.30990901589393616, Final Batch Loss: 0.1821555495262146\n",
      "Epoch 2295, Loss: 0.3378397822380066, Final Batch Loss: 0.15577386319637299\n",
      "Epoch 2296, Loss: 0.29500509053468704, Final Batch Loss: 0.12045571953058243\n",
      "Epoch 2297, Loss: 0.30160845816135406, Final Batch Loss: 0.1595892608165741\n",
      "Epoch 2298, Loss: 0.2867482155561447, Final Batch Loss: 0.15831786394119263\n",
      "Epoch 2299, Loss: 0.31343287229537964, Final Batch Loss: 0.17660486698150635\n",
      "Epoch 2300, Loss: 0.24950675666332245, Final Batch Loss: 0.12038816511631012\n",
      "Epoch 2301, Loss: 0.35438914597034454, Final Batch Loss: 0.18515938520431519\n",
      "Epoch 2302, Loss: 0.2720269411802292, Final Batch Loss: 0.11549670994281769\n",
      "Epoch 2303, Loss: 0.2875138819217682, Final Batch Loss: 0.14810901880264282\n",
      "Epoch 2304, Loss: 0.3185221552848816, Final Batch Loss: 0.14563004672527313\n",
      "Epoch 2305, Loss: 0.2786574214696884, Final Batch Loss: 0.13542696833610535\n",
      "Epoch 2306, Loss: 0.3402053117752075, Final Batch Loss: 0.15228351950645447\n",
      "Epoch 2307, Loss: 0.27263904362916946, Final Batch Loss: 0.09198377281427383\n",
      "Epoch 2308, Loss: 0.2996997833251953, Final Batch Loss: 0.18474745750427246\n",
      "Epoch 2309, Loss: 0.34094467759132385, Final Batch Loss: 0.1770177185535431\n",
      "Epoch 2310, Loss: 0.30224551260471344, Final Batch Loss: 0.160795196890831\n",
      "Epoch 2311, Loss: 0.31148968636989594, Final Batch Loss: 0.13789042830467224\n",
      "Epoch 2312, Loss: 0.29551099240779877, Final Batch Loss: 0.1591457724571228\n",
      "Epoch 2313, Loss: 0.3239811360836029, Final Batch Loss: 0.185190349817276\n",
      "Epoch 2314, Loss: 0.31899920105934143, Final Batch Loss: 0.17533287405967712\n",
      "Epoch 2315, Loss: 0.3054477423429489, Final Batch Loss: 0.11047793924808502\n",
      "Epoch 2316, Loss: 0.2644580826163292, Final Batch Loss: 0.10595574229955673\n",
      "Epoch 2317, Loss: 0.30716075003147125, Final Batch Loss: 0.1437520831823349\n",
      "Epoch 2318, Loss: 0.2862198054790497, Final Batch Loss: 0.15458175539970398\n",
      "Epoch 2319, Loss: 0.34616512060165405, Final Batch Loss: 0.17097108066082\n",
      "Epoch 2320, Loss: 0.3290172815322876, Final Batch Loss: 0.16124378144741058\n",
      "Epoch 2321, Loss: 0.3063233196735382, Final Batch Loss: 0.15736010670661926\n",
      "Epoch 2322, Loss: 0.32234804332256317, Final Batch Loss: 0.1484852284193039\n",
      "Epoch 2323, Loss: 0.27784179151058197, Final Batch Loss: 0.12713229656219482\n",
      "Epoch 2324, Loss: 0.31697314977645874, Final Batch Loss: 0.15466798841953278\n",
      "Epoch 2325, Loss: 0.28635717928409576, Final Batch Loss: 0.13285325467586517\n",
      "Epoch 2326, Loss: 0.31386882066726685, Final Batch Loss: 0.17120791971683502\n",
      "Epoch 2327, Loss: 0.33165663480758667, Final Batch Loss: 0.150162011384964\n",
      "Epoch 2328, Loss: 0.2449803724884987, Final Batch Loss: 0.11971081048250198\n",
      "Epoch 2329, Loss: 0.3166620433330536, Final Batch Loss: 0.15883412957191467\n",
      "Epoch 2330, Loss: 0.3180600553750992, Final Batch Loss: 0.17898383736610413\n",
      "Epoch 2331, Loss: 0.2679791897535324, Final Batch Loss: 0.1208563894033432\n",
      "Epoch 2332, Loss: 0.2767922282218933, Final Batch Loss: 0.16003087162971497\n",
      "Epoch 2333, Loss: 0.354118674993515, Final Batch Loss: 0.19037514925003052\n",
      "Epoch 2334, Loss: 0.26574913412332535, Final Batch Loss: 0.11882112175226212\n",
      "Epoch 2335, Loss: 0.3654613345861435, Final Batch Loss: 0.206297367811203\n",
      "Epoch 2336, Loss: 0.3616161048412323, Final Batch Loss: 0.15777398645877838\n",
      "Epoch 2337, Loss: 0.3347703069448471, Final Batch Loss: 0.1745869666337967\n",
      "Epoch 2338, Loss: 0.2596246302127838, Final Batch Loss: 0.12310169637203217\n",
      "Epoch 2339, Loss: 0.2974778264760971, Final Batch Loss: 0.145456463098526\n",
      "Epoch 2340, Loss: 0.25022047758102417, Final Batch Loss: 0.14393533766269684\n",
      "Epoch 2341, Loss: 0.2882087454199791, Final Batch Loss: 0.10064605623483658\n",
      "Epoch 2342, Loss: 0.2770034819841385, Final Batch Loss: 0.1270303875207901\n",
      "Epoch 2343, Loss: 0.29044589400291443, Final Batch Loss: 0.14438575506210327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2344, Loss: 0.3137648403644562, Final Batch Loss: 0.1556965410709381\n",
      "Epoch 2345, Loss: 0.32961931824684143, Final Batch Loss: 0.16213688254356384\n",
      "Epoch 2346, Loss: 0.2551518529653549, Final Batch Loss: 0.11805212497711182\n",
      "Epoch 2347, Loss: 0.3154066652059555, Final Batch Loss: 0.16858215630054474\n",
      "Epoch 2348, Loss: 0.3143683224916458, Final Batch Loss: 0.1454908847808838\n",
      "Epoch 2349, Loss: 0.25947149097919464, Final Batch Loss: 0.12013532221317291\n",
      "Epoch 2350, Loss: 0.277680441737175, Final Batch Loss: 0.17348740994930267\n",
      "Epoch 2351, Loss: 0.3515040874481201, Final Batch Loss: 0.16321508586406708\n",
      "Epoch 2352, Loss: 0.34751854836940765, Final Batch Loss: 0.1525532454252243\n",
      "Epoch 2353, Loss: 0.25786853581666946, Final Batch Loss: 0.1193251833319664\n",
      "Epoch 2354, Loss: 0.30370333790779114, Final Batch Loss: 0.15430030226707458\n",
      "Epoch 2355, Loss: 0.25364288687705994, Final Batch Loss: 0.13055367767810822\n",
      "Epoch 2356, Loss: 0.2986489534378052, Final Batch Loss: 0.15150706470012665\n",
      "Epoch 2357, Loss: 0.2897605672478676, Final Batch Loss: 0.10283591598272324\n",
      "Epoch 2358, Loss: 0.31876637786626816, Final Batch Loss: 0.1090221181511879\n",
      "Epoch 2359, Loss: 0.2664540708065033, Final Batch Loss: 0.14358797669410706\n",
      "Epoch 2360, Loss: 0.27543649077415466, Final Batch Loss: 0.14852677285671234\n",
      "Epoch 2361, Loss: 0.2568247467279434, Final Batch Loss: 0.12968949973583221\n",
      "Epoch 2362, Loss: 0.29431504756212234, Final Batch Loss: 0.17239466309547424\n",
      "Epoch 2363, Loss: 0.32191501557826996, Final Batch Loss: 0.17208190262317657\n",
      "Epoch 2364, Loss: 0.25027594715356827, Final Batch Loss: 0.11013411730527878\n",
      "Epoch 2365, Loss: 0.3083038255572319, Final Batch Loss: 0.19574041664600372\n",
      "Epoch 2366, Loss: 0.25978146493434906, Final Batch Loss: 0.11675018072128296\n",
      "Epoch 2367, Loss: 0.34372490644454956, Final Batch Loss: 0.16912740468978882\n",
      "Epoch 2368, Loss: 0.2493232637643814, Final Batch Loss: 0.10068446397781372\n",
      "Epoch 2369, Loss: 0.31920382380485535, Final Batch Loss: 0.17252549529075623\n",
      "Epoch 2370, Loss: 0.2667238935828209, Final Batch Loss: 0.14472199976444244\n",
      "Epoch 2371, Loss: 0.3223313242197037, Final Batch Loss: 0.18813692033290863\n",
      "Epoch 2372, Loss: 0.2618957385420799, Final Batch Loss: 0.11953449994325638\n",
      "Epoch 2373, Loss: 0.2766970247030258, Final Batch Loss: 0.10965991020202637\n",
      "Epoch 2374, Loss: 0.3059266656637192, Final Batch Loss: 0.17483635246753693\n",
      "Epoch 2375, Loss: 0.2879638969898224, Final Batch Loss: 0.1344340741634369\n",
      "Epoch 2376, Loss: 0.287778802216053, Final Batch Loss: 0.17350433766841888\n",
      "Epoch 2377, Loss: 0.3027120679616928, Final Batch Loss: 0.12700100243091583\n",
      "Epoch 2378, Loss: 0.31728191673755646, Final Batch Loss: 0.15614460408687592\n",
      "Epoch 2379, Loss: 0.2746787443757057, Final Batch Loss: 0.11228909343481064\n",
      "Epoch 2380, Loss: 0.24809430539608002, Final Batch Loss: 0.10806794464588165\n",
      "Epoch 2381, Loss: 0.2674848139286041, Final Batch Loss: 0.12743157148361206\n",
      "Epoch 2382, Loss: 0.3054451197385788, Final Batch Loss: 0.1291383057832718\n",
      "Epoch 2383, Loss: 0.2369537353515625, Final Batch Loss: 0.12288981676101685\n",
      "Epoch 2384, Loss: 0.31474485993385315, Final Batch Loss: 0.12643912434577942\n",
      "Epoch 2385, Loss: 0.27415959537029266, Final Batch Loss: 0.1181461364030838\n",
      "Epoch 2386, Loss: 0.26415496319532394, Final Batch Loss: 0.1191181167960167\n",
      "Epoch 2387, Loss: 0.3095724582672119, Final Batch Loss: 0.14957062900066376\n",
      "Epoch 2388, Loss: 0.32958342134952545, Final Batch Loss: 0.14069603383541107\n",
      "Epoch 2389, Loss: 0.317654088139534, Final Batch Loss: 0.13158106803894043\n",
      "Epoch 2390, Loss: 0.26786012202501297, Final Batch Loss: 0.14549683034420013\n",
      "Epoch 2391, Loss: 0.3273215591907501, Final Batch Loss: 0.14144545793533325\n",
      "Epoch 2392, Loss: 0.32367317378520966, Final Batch Loss: 0.1560864895582199\n",
      "Epoch 2393, Loss: 0.3755265325307846, Final Batch Loss: 0.2236463576555252\n",
      "Epoch 2394, Loss: 0.28224362432956696, Final Batch Loss: 0.13435374200344086\n",
      "Epoch 2395, Loss: 0.39397476613521576, Final Batch Loss: 0.2114083617925644\n",
      "Epoch 2396, Loss: 0.3398449420928955, Final Batch Loss: 0.19411008059978485\n",
      "Epoch 2397, Loss: 0.3258426934480667, Final Batch Loss: 0.13893382251262665\n",
      "Epoch 2398, Loss: 0.33831319212913513, Final Batch Loss: 0.15335001051425934\n",
      "Epoch 2399, Loss: 0.30985766649246216, Final Batch Loss: 0.17473049461841583\n",
      "Epoch 2400, Loss: 0.2676475793123245, Final Batch Loss: 0.11307249963283539\n",
      "Epoch 2401, Loss: 0.29162247478961945, Final Batch Loss: 0.13887061178684235\n",
      "Epoch 2402, Loss: 0.2743421196937561, Final Batch Loss: 0.13100260496139526\n",
      "Epoch 2403, Loss: 0.3041330575942993, Final Batch Loss: 0.10731497406959534\n",
      "Epoch 2404, Loss: 0.3836391419172287, Final Batch Loss: 0.22067712247371674\n",
      "Epoch 2405, Loss: 0.26102694869041443, Final Batch Loss: 0.1252915859222412\n",
      "Epoch 2406, Loss: 0.25493374466896057, Final Batch Loss: 0.14162732660770416\n",
      "Epoch 2407, Loss: 0.2524613291025162, Final Batch Loss: 0.10875652730464935\n",
      "Epoch 2408, Loss: 0.2701156437397003, Final Batch Loss: 0.1330857127904892\n",
      "Epoch 2409, Loss: 0.26489420235157013, Final Batch Loss: 0.1324169933795929\n",
      "Epoch 2410, Loss: 0.2628594785928726, Final Batch Loss: 0.1353713870048523\n",
      "Epoch 2411, Loss: 0.28941623866558075, Final Batch Loss: 0.12816762924194336\n",
      "Epoch 2412, Loss: 0.29477277398109436, Final Batch Loss: 0.1477563977241516\n",
      "Epoch 2413, Loss: 0.26594436168670654, Final Batch Loss: 0.14579452574253082\n",
      "Epoch 2414, Loss: 0.22724340856075287, Final Batch Loss: 0.12180643528699875\n",
      "Epoch 2415, Loss: 0.22389710694551468, Final Batch Loss: 0.09746008366346359\n",
      "Epoch 2416, Loss: 0.26532596349716187, Final Batch Loss: 0.1373414397239685\n",
      "Epoch 2417, Loss: 0.2629820927977562, Final Batch Loss: 0.09804343432188034\n",
      "Epoch 2418, Loss: 0.2627124637365341, Final Batch Loss: 0.14540107548236847\n",
      "Epoch 2419, Loss: 0.31043849885463715, Final Batch Loss: 0.15564872324466705\n",
      "Epoch 2420, Loss: 0.2608748525381088, Final Batch Loss: 0.12476678192615509\n",
      "Epoch 2421, Loss: 0.30766114592552185, Final Batch Loss: 0.14455968141555786\n",
      "Epoch 2422, Loss: 0.25645893812179565, Final Batch Loss: 0.16007740795612335\n",
      "Epoch 2423, Loss: 0.2571312263607979, Final Batch Loss: 0.09762809425592422\n",
      "Epoch 2424, Loss: 0.29725758731365204, Final Batch Loss: 0.17487630248069763\n",
      "Epoch 2425, Loss: 0.2657542824745178, Final Batch Loss: 0.14491978287696838\n",
      "Epoch 2426, Loss: 0.2821906805038452, Final Batch Loss: 0.11972092092037201\n",
      "Epoch 2427, Loss: 0.28665579855442047, Final Batch Loss: 0.13849125802516937\n",
      "Epoch 2428, Loss: 0.2542465478181839, Final Batch Loss: 0.15334650874137878\n",
      "Epoch 2429, Loss: 0.24007270485162735, Final Batch Loss: 0.0952020063996315\n",
      "Epoch 2430, Loss: 0.23266997933387756, Final Batch Loss: 0.11556733399629593\n",
      "Epoch 2431, Loss: 0.37269777059555054, Final Batch Loss: 0.17605085670948029\n",
      "Epoch 2432, Loss: 0.34321078658103943, Final Batch Loss: 0.18229390680789948\n",
      "Epoch 2433, Loss: 0.2992078959941864, Final Batch Loss: 0.16610082983970642\n",
      "Epoch 2434, Loss: 0.22944600135087967, Final Batch Loss: 0.0868431106209755\n",
      "Epoch 2435, Loss: 0.26125553995370865, Final Batch Loss: 0.11904080957174301\n",
      "Epoch 2436, Loss: 0.3617662638425827, Final Batch Loss: 0.19652990996837616\n",
      "Epoch 2437, Loss: 0.3372171223163605, Final Batch Loss: 0.16506321728229523\n",
      "Epoch 2438, Loss: 0.28517767041921616, Final Batch Loss: 0.10107161849737167\n",
      "Epoch 2439, Loss: 0.3002440929412842, Final Batch Loss: 0.1585075408220291\n",
      "Epoch 2440, Loss: 0.33733344078063965, Final Batch Loss: 0.16255462169647217\n",
      "Epoch 2441, Loss: 0.2508849799633026, Final Batch Loss: 0.1199587881565094\n",
      "Epoch 2442, Loss: 0.33057035505771637, Final Batch Loss: 0.15079355239868164\n",
      "Epoch 2443, Loss: 0.2441328763961792, Final Batch Loss: 0.1101333349943161\n",
      "Epoch 2444, Loss: 0.3021187335252762, Final Batch Loss: 0.14230579137802124\n",
      "Epoch 2445, Loss: 0.2543671503663063, Final Batch Loss: 0.13152600824832916\n",
      "Epoch 2446, Loss: 0.3134271502494812, Final Batch Loss: 0.1488327980041504\n",
      "Epoch 2447, Loss: 0.31682024896144867, Final Batch Loss: 0.17814335227012634\n",
      "Epoch 2448, Loss: 0.26368945837020874, Final Batch Loss: 0.10802726447582245\n",
      "Epoch 2449, Loss: 0.30093367397785187, Final Batch Loss: 0.19100108742713928\n",
      "Epoch 2450, Loss: 0.3178006559610367, Final Batch Loss: 0.16729913651943207\n",
      "Epoch 2451, Loss: 0.3220997005701065, Final Batch Loss: 0.15838462114334106\n",
      "Epoch 2452, Loss: 0.2596936300396919, Final Batch Loss: 0.10914764553308487\n",
      "Epoch 2453, Loss: 0.26423951238393784, Final Batch Loss: 0.12002114206552505\n",
      "Epoch 2454, Loss: 0.2585299462080002, Final Batch Loss: 0.09720116853713989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2455, Loss: 0.24922535568475723, Final Batch Loss: 0.12882764637470245\n",
      "Epoch 2456, Loss: 0.3508489429950714, Final Batch Loss: 0.2156427949666977\n",
      "Epoch 2457, Loss: 0.24882785230875015, Final Batch Loss: 0.12533794343471527\n",
      "Epoch 2458, Loss: 0.34534840285778046, Final Batch Loss: 0.21271279454231262\n",
      "Epoch 2459, Loss: 0.22790684551000595, Final Batch Loss: 0.11353923380374908\n",
      "Epoch 2460, Loss: 0.2659911811351776, Final Batch Loss: 0.10252301394939423\n",
      "Epoch 2461, Loss: 0.33985635638237, Final Batch Loss: 0.15207898616790771\n",
      "Epoch 2462, Loss: 0.2719385474920273, Final Batch Loss: 0.16152533888816833\n",
      "Epoch 2463, Loss: 0.2832953706383705, Final Batch Loss: 0.19224925339221954\n",
      "Epoch 2464, Loss: 0.2482798621058464, Final Batch Loss: 0.13236141204833984\n",
      "Epoch 2465, Loss: 0.31646572053432465, Final Batch Loss: 0.16632327437400818\n",
      "Epoch 2466, Loss: 0.37459075450897217, Final Batch Loss: 0.12585508823394775\n",
      "Epoch 2467, Loss: 0.2860066592693329, Final Batch Loss: 0.14594011008739471\n",
      "Epoch 2468, Loss: 0.2769206538796425, Final Batch Loss: 0.11702760308980942\n",
      "Epoch 2469, Loss: 0.28809820115566254, Final Batch Loss: 0.15521705150604248\n",
      "Epoch 2470, Loss: 0.25894665718078613, Final Batch Loss: 0.13650089502334595\n",
      "Epoch 2471, Loss: 0.323216512799263, Final Batch Loss: 0.1434699296951294\n",
      "Epoch 2472, Loss: 0.24556958675384521, Final Batch Loss: 0.16473375260829926\n",
      "Epoch 2473, Loss: 0.24864869564771652, Final Batch Loss: 0.09820429235696793\n",
      "Epoch 2474, Loss: 0.23076920211315155, Final Batch Loss: 0.10510140657424927\n",
      "Epoch 2475, Loss: 0.24581100046634674, Final Batch Loss: 0.1371157467365265\n",
      "Epoch 2476, Loss: 0.29484449326992035, Final Batch Loss: 0.1715308129787445\n",
      "Epoch 2477, Loss: 0.23360246419906616, Final Batch Loss: 0.13695929944515228\n",
      "Epoch 2478, Loss: 0.28718267381191254, Final Batch Loss: 0.14096492528915405\n",
      "Epoch 2479, Loss: 0.31945815682411194, Final Batch Loss: 0.1819143295288086\n",
      "Epoch 2480, Loss: 0.27763688564300537, Final Batch Loss: 0.1410444974899292\n",
      "Epoch 2481, Loss: 0.3015441596508026, Final Batch Loss: 0.13488124310970306\n",
      "Epoch 2482, Loss: 0.2969535291194916, Final Batch Loss: 0.15816059708595276\n",
      "Epoch 2483, Loss: 0.2956855446100235, Final Batch Loss: 0.16043274104595184\n",
      "Epoch 2484, Loss: 0.33341315388679504, Final Batch Loss: 0.1635468751192093\n",
      "Epoch 2485, Loss: 0.2979942262172699, Final Batch Loss: 0.15985606610774994\n",
      "Epoch 2486, Loss: 0.2696323022246361, Final Batch Loss: 0.12024927884340286\n",
      "Epoch 2487, Loss: 0.29963747411966324, Final Batch Loss: 0.18536286056041718\n",
      "Epoch 2488, Loss: 0.2977600246667862, Final Batch Loss: 0.14967185258865356\n",
      "Epoch 2489, Loss: 0.2944723516702652, Final Batch Loss: 0.15076880156993866\n",
      "Epoch 2490, Loss: 0.283451646566391, Final Batch Loss: 0.14024218916893005\n",
      "Epoch 2491, Loss: 0.1889413595199585, Final Batch Loss: 0.10026832669973373\n",
      "Epoch 2492, Loss: 0.28971296548843384, Final Batch Loss: 0.14158517122268677\n",
      "Epoch 2493, Loss: 0.3201211094856262, Final Batch Loss: 0.18843209743499756\n",
      "Epoch 2494, Loss: 0.3249781131744385, Final Batch Loss: 0.17093238234519958\n",
      "Epoch 2495, Loss: 0.3011334240436554, Final Batch Loss: 0.15548068284988403\n",
      "Epoch 2496, Loss: 0.28610582649707794, Final Batch Loss: 0.11901186406612396\n",
      "Epoch 2497, Loss: 0.3313211053609848, Final Batch Loss: 0.16603538393974304\n",
      "Epoch 2498, Loss: 0.3424775153398514, Final Batch Loss: 0.14591892063617706\n",
      "Epoch 2499, Loss: 0.30725154280662537, Final Batch Loss: 0.15139275789260864\n",
      "Epoch 2500, Loss: 0.3509582132101059, Final Batch Loss: 0.1967000961303711\n",
      "Epoch 2501, Loss: 0.3099699169397354, Final Batch Loss: 0.16168279945850372\n",
      "Epoch 2502, Loss: 0.31801916658878326, Final Batch Loss: 0.1623050570487976\n",
      "Epoch 2503, Loss: 0.30355246365070343, Final Batch Loss: 0.17006628215312958\n",
      "Epoch 2504, Loss: 0.299068421125412, Final Batch Loss: 0.1343686878681183\n",
      "Epoch 2505, Loss: 0.2968600392341614, Final Batch Loss: 0.1286981701850891\n",
      "Epoch 2506, Loss: 0.22029713541269302, Final Batch Loss: 0.09040337055921555\n",
      "Epoch 2507, Loss: 0.27295759320259094, Final Batch Loss: 0.13558706641197205\n",
      "Epoch 2508, Loss: 0.29304589331150055, Final Batch Loss: 0.12966369092464447\n",
      "Epoch 2509, Loss: 0.2696032226085663, Final Batch Loss: 0.12947838008403778\n",
      "Epoch 2510, Loss: 0.3123507499694824, Final Batch Loss: 0.15468598902225494\n",
      "Epoch 2511, Loss: 0.26114311069250107, Final Batch Loss: 0.11489715427160263\n",
      "Epoch 2512, Loss: 0.31427334249019623, Final Batch Loss: 0.17302413284778595\n",
      "Epoch 2513, Loss: 0.3171352595090866, Final Batch Loss: 0.1503574401140213\n",
      "Epoch 2514, Loss: 0.26215456426143646, Final Batch Loss: 0.10340087115764618\n",
      "Epoch 2515, Loss: 0.2342429906129837, Final Batch Loss: 0.07911218702793121\n",
      "Epoch 2516, Loss: 0.27028799057006836, Final Batch Loss: 0.1540418118238449\n",
      "Epoch 2517, Loss: 0.2984762042760849, Final Batch Loss: 0.15674856305122375\n",
      "Epoch 2518, Loss: 0.2326209619641304, Final Batch Loss: 0.12640538811683655\n",
      "Epoch 2519, Loss: 0.287514328956604, Final Batch Loss: 0.13960954546928406\n",
      "Epoch 2520, Loss: 0.2802652269601822, Final Batch Loss: 0.12310762703418732\n",
      "Epoch 2521, Loss: 0.24399539083242416, Final Batch Loss: 0.14078085124492645\n",
      "Epoch 2522, Loss: 0.2795148640871048, Final Batch Loss: 0.1264648139476776\n",
      "Epoch 2523, Loss: 0.2709540128707886, Final Batch Loss: 0.14486993849277496\n",
      "Epoch 2524, Loss: 0.2703993171453476, Final Batch Loss: 0.12136568129062653\n",
      "Epoch 2525, Loss: 0.25797899812459946, Final Batch Loss: 0.13838599622249603\n",
      "Epoch 2526, Loss: 0.27194859832525253, Final Batch Loss: 0.15238267183303833\n",
      "Epoch 2527, Loss: 0.28847426176071167, Final Batch Loss: 0.16134203970432281\n",
      "Epoch 2528, Loss: 0.2848474755883217, Final Batch Loss: 0.10494618862867355\n",
      "Epoch 2529, Loss: 0.2803913801908493, Final Batch Loss: 0.14514563977718353\n",
      "Epoch 2530, Loss: 0.2662053778767586, Final Batch Loss: 0.14249561727046967\n",
      "Epoch 2531, Loss: 0.2807459980249405, Final Batch Loss: 0.14548420906066895\n",
      "Epoch 2532, Loss: 0.27370187640190125, Final Batch Loss: 0.1336715668439865\n",
      "Epoch 2533, Loss: 0.252993680536747, Final Batch Loss: 0.15707704424858093\n",
      "Epoch 2534, Loss: 0.30925609171390533, Final Batch Loss: 0.18088121712207794\n",
      "Epoch 2535, Loss: 0.26467087119817734, Final Batch Loss: 0.12009876221418381\n",
      "Epoch 2536, Loss: 0.3149983137845993, Final Batch Loss: 0.1711599975824356\n",
      "Epoch 2537, Loss: 0.30308689177036285, Final Batch Loss: 0.16809986531734467\n",
      "Epoch 2538, Loss: 0.3142038434743881, Final Batch Loss: 0.18345725536346436\n",
      "Epoch 2539, Loss: 0.2297874316573143, Final Batch Loss: 0.10959190130233765\n",
      "Epoch 2540, Loss: 0.24597149342298508, Final Batch Loss: 0.12681540846824646\n",
      "Epoch 2541, Loss: 0.21334891021251678, Final Batch Loss: 0.10560009628534317\n",
      "Epoch 2542, Loss: 0.31034334003925323, Final Batch Loss: 0.14858131110668182\n",
      "Epoch 2543, Loss: 0.3111339956521988, Final Batch Loss: 0.14083682000637054\n",
      "Epoch 2544, Loss: 0.3068629652261734, Final Batch Loss: 0.14395244419574738\n",
      "Epoch 2545, Loss: 0.2482905164361, Final Batch Loss: 0.11942946165800095\n",
      "Epoch 2546, Loss: 0.2516671270132065, Final Batch Loss: 0.1361384093761444\n",
      "Epoch 2547, Loss: 0.279869481921196, Final Batch Loss: 0.14681056141853333\n",
      "Epoch 2548, Loss: 0.2505272403359413, Final Batch Loss: 0.134605273604393\n",
      "Epoch 2549, Loss: 0.28034570068120956, Final Batch Loss: 0.11520596593618393\n",
      "Epoch 2550, Loss: 0.28358112275600433, Final Batch Loss: 0.14517506957054138\n",
      "Epoch 2551, Loss: 0.2721339762210846, Final Batch Loss: 0.12071928381919861\n",
      "Epoch 2552, Loss: 0.2363990917801857, Final Batch Loss: 0.13413967192173004\n",
      "Epoch 2553, Loss: 0.2724977806210518, Final Batch Loss: 0.08795806020498276\n",
      "Epoch 2554, Loss: 0.3178333193063736, Final Batch Loss: 0.147806778550148\n",
      "Epoch 2555, Loss: 0.2452705278992653, Final Batch Loss: 0.12551796436309814\n",
      "Epoch 2556, Loss: 0.26935338973999023, Final Batch Loss: 0.14408735930919647\n",
      "Epoch 2557, Loss: 0.25102247297763824, Final Batch Loss: 0.11946587264537811\n",
      "Epoch 2558, Loss: 0.3522077649831772, Final Batch Loss: 0.19456152617931366\n",
      "Epoch 2559, Loss: 0.26114001125097275, Final Batch Loss: 0.15437668561935425\n",
      "Epoch 2560, Loss: 0.24337834119796753, Final Batch Loss: 0.11510096490383148\n",
      "Epoch 2561, Loss: 0.2091079205274582, Final Batch Loss: 0.10535655915737152\n",
      "Epoch 2562, Loss: 0.2533637210726738, Final Batch Loss: 0.15385599434375763\n",
      "Epoch 2563, Loss: 0.3325134813785553, Final Batch Loss: 0.15331068634986877\n",
      "Epoch 2564, Loss: 0.2932533621788025, Final Batch Loss: 0.16128672659397125\n",
      "Epoch 2565, Loss: 0.24314545840024948, Final Batch Loss: 0.11505123227834702\n",
      "Epoch 2566, Loss: 0.28278933465480804, Final Batch Loss: 0.12518636882305145\n",
      "Epoch 2567, Loss: 0.26650939881801605, Final Batch Loss: 0.16633251309394836\n",
      "Epoch 2568, Loss: 0.2675657719373703, Final Batch Loss: 0.14807948470115662\n",
      "Epoch 2569, Loss: 0.27850575745105743, Final Batch Loss: 0.10423494875431061\n",
      "Epoch 2570, Loss: 0.2741916850209236, Final Batch Loss: 0.09156734496355057\n",
      "Epoch 2571, Loss: 0.3020147830247879, Final Batch Loss: 0.16320902109146118\n",
      "Epoch 2572, Loss: 0.27626945078372955, Final Batch Loss: 0.12839993834495544\n",
      "Epoch 2573, Loss: 0.27321823686361313, Final Batch Loss: 0.16203078627586365\n",
      "Epoch 2574, Loss: 0.29719314724206924, Final Batch Loss: 0.11456615477800369\n",
      "Epoch 2575, Loss: 0.2764121890068054, Final Batch Loss: 0.15617850422859192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2576, Loss: 0.2852214276790619, Final Batch Loss: 0.1472363919019699\n",
      "Epoch 2577, Loss: 0.22488413751125336, Final Batch Loss: 0.11024400591850281\n",
      "Epoch 2578, Loss: 0.32884007692337036, Final Batch Loss: 0.19231227040290833\n",
      "Epoch 2579, Loss: 0.23160229623317719, Final Batch Loss: 0.12208515405654907\n",
      "Epoch 2580, Loss: 0.300278440117836, Final Batch Loss: 0.1483682245016098\n",
      "Epoch 2581, Loss: 0.35130636394023895, Final Batch Loss: 0.14285661280155182\n",
      "Epoch 2582, Loss: 0.29613617062568665, Final Batch Loss: 0.13832226395606995\n",
      "Epoch 2583, Loss: 0.25353845953941345, Final Batch Loss: 0.11117474734783173\n",
      "Epoch 2584, Loss: 0.24922733753919601, Final Batch Loss: 0.13123144209384918\n",
      "Epoch 2585, Loss: 0.2550303786993027, Final Batch Loss: 0.12689833343029022\n",
      "Epoch 2586, Loss: 0.31138868629932404, Final Batch Loss: 0.1552589237689972\n",
      "Epoch 2587, Loss: 0.2638462707400322, Final Batch Loss: 0.12332993000745773\n",
      "Epoch 2588, Loss: 0.2630380615592003, Final Batch Loss: 0.14667202532291412\n",
      "Epoch 2589, Loss: 0.24664882570505142, Final Batch Loss: 0.10499956458806992\n",
      "Epoch 2590, Loss: 0.25374817848205566, Final Batch Loss: 0.1055263876914978\n",
      "Epoch 2591, Loss: 0.2603708505630493, Final Batch Loss: 0.12713168561458588\n",
      "Epoch 2592, Loss: 0.24955125898122787, Final Batch Loss: 0.09678993374109268\n",
      "Epoch 2593, Loss: 0.19189320504665375, Final Batch Loss: 0.10592295229434967\n",
      "Epoch 2594, Loss: 0.2536076158285141, Final Batch Loss: 0.11926059424877167\n",
      "Epoch 2595, Loss: 0.353751465678215, Final Batch Loss: 0.18024279177188873\n",
      "Epoch 2596, Loss: 0.26609645783901215, Final Batch Loss: 0.13427741825580597\n",
      "Epoch 2597, Loss: 0.265683613717556, Final Batch Loss: 0.14135535061359406\n",
      "Epoch 2598, Loss: 0.2926654890179634, Final Batch Loss: 0.12487214058637619\n",
      "Epoch 2599, Loss: 0.27026907354593277, Final Batch Loss: 0.16190791130065918\n",
      "Epoch 2600, Loss: 0.21188589185476303, Final Batch Loss: 0.09835916012525558\n",
      "Epoch 2601, Loss: 0.22008410841226578, Final Batch Loss: 0.11835590749979019\n",
      "Epoch 2602, Loss: 0.27343733608722687, Final Batch Loss: 0.1459827721118927\n",
      "Epoch 2603, Loss: 0.2729655057191849, Final Batch Loss: 0.14286988973617554\n",
      "Epoch 2604, Loss: 0.3073585256934166, Final Batch Loss: 0.20148241519927979\n",
      "Epoch 2605, Loss: 0.2980378717184067, Final Batch Loss: 0.16155469417572021\n",
      "Epoch 2606, Loss: 0.28910453617572784, Final Batch Loss: 0.14108934998512268\n",
      "Epoch 2607, Loss: 0.2921890467405319, Final Batch Loss: 0.14950987696647644\n",
      "Epoch 2608, Loss: 0.25945692509412766, Final Batch Loss: 0.11973699182271957\n",
      "Epoch 2609, Loss: 0.25208088755607605, Final Batch Loss: 0.11838597059249878\n",
      "Epoch 2610, Loss: 0.31240539252758026, Final Batch Loss: 0.12967629730701447\n",
      "Epoch 2611, Loss: 0.24628663063049316, Final Batch Loss: 0.13444870710372925\n",
      "Epoch 2612, Loss: 0.27639181911945343, Final Batch Loss: 0.13314998149871826\n",
      "Epoch 2613, Loss: 0.30571387708187103, Final Batch Loss: 0.1561552882194519\n",
      "Epoch 2614, Loss: 0.24845996499061584, Final Batch Loss: 0.14002947509288788\n",
      "Epoch 2615, Loss: 0.2697870433330536, Final Batch Loss: 0.12266425788402557\n",
      "Epoch 2616, Loss: 0.21379368752241135, Final Batch Loss: 0.08644136041402817\n",
      "Epoch 2617, Loss: 0.3063076287508011, Final Batch Loss: 0.14363178610801697\n",
      "Epoch 2618, Loss: 0.25619448721408844, Final Batch Loss: 0.11401994526386261\n",
      "Epoch 2619, Loss: 0.24414097517728806, Final Batch Loss: 0.13656874001026154\n",
      "Epoch 2620, Loss: 0.24898113310337067, Final Batch Loss: 0.14534373581409454\n",
      "Epoch 2621, Loss: 0.23627983778715134, Final Batch Loss: 0.10934541374444962\n",
      "Epoch 2622, Loss: 0.2520614340901375, Final Batch Loss: 0.13442584872245789\n",
      "Epoch 2623, Loss: 0.29951266944408417, Final Batch Loss: 0.15842053294181824\n",
      "Epoch 2624, Loss: 0.23208896070718765, Final Batch Loss: 0.1231844574213028\n",
      "Epoch 2625, Loss: 0.26867755502462387, Final Batch Loss: 0.16564221680164337\n",
      "Epoch 2626, Loss: 0.2495342567563057, Final Batch Loss: 0.09360886365175247\n",
      "Epoch 2627, Loss: 0.31233374774456024, Final Batch Loss: 0.134441077709198\n",
      "Epoch 2628, Loss: 0.2229355126619339, Final Batch Loss: 0.09518760442733765\n",
      "Epoch 2629, Loss: 0.27480338513851166, Final Batch Loss: 0.12620821595191956\n",
      "Epoch 2630, Loss: 0.2716117426753044, Final Batch Loss: 0.16304293274879456\n",
      "Epoch 2631, Loss: 0.2290383279323578, Final Batch Loss: 0.11061196029186249\n",
      "Epoch 2632, Loss: 0.23460295051336288, Final Batch Loss: 0.10295898467302322\n",
      "Epoch 2633, Loss: 0.24213546514511108, Final Batch Loss: 0.12605763971805573\n",
      "Epoch 2634, Loss: 0.27929139882326126, Final Batch Loss: 0.11217821389436722\n",
      "Epoch 2635, Loss: 0.2500568553805351, Final Batch Loss: 0.13801933825016022\n",
      "Epoch 2636, Loss: 0.2631617486476898, Final Batch Loss: 0.13591891527175903\n",
      "Epoch 2637, Loss: 0.33569347858428955, Final Batch Loss: 0.15204283595085144\n",
      "Epoch 2638, Loss: 0.24376605451107025, Final Batch Loss: 0.1241501122713089\n",
      "Epoch 2639, Loss: 0.2837580665946007, Final Batch Loss: 0.158916175365448\n",
      "Epoch 2640, Loss: 0.27610206604003906, Final Batch Loss: 0.1259511411190033\n",
      "Epoch 2641, Loss: 0.3120175153017044, Final Batch Loss: 0.1556677371263504\n",
      "Epoch 2642, Loss: 0.18030407279729843, Final Batch Loss: 0.10256131738424301\n",
      "Epoch 2643, Loss: 0.2792262136936188, Final Batch Loss: 0.14559394121170044\n",
      "Epoch 2644, Loss: 0.31002381443977356, Final Batch Loss: 0.15703043341636658\n",
      "Epoch 2645, Loss: 0.3177778720855713, Final Batch Loss: 0.16107848286628723\n",
      "Epoch 2646, Loss: 0.24575432389974594, Final Batch Loss: 0.128537118434906\n",
      "Epoch 2647, Loss: 0.2981780022382736, Final Batch Loss: 0.14544081687927246\n",
      "Epoch 2648, Loss: 0.29741960763931274, Final Batch Loss: 0.12704896926879883\n",
      "Epoch 2649, Loss: 0.29026413708925247, Final Batch Loss: 0.11885400861501694\n",
      "Epoch 2650, Loss: 0.2874496132135391, Final Batch Loss: 0.15004433691501617\n",
      "Epoch 2651, Loss: 0.20099718868732452, Final Batch Loss: 0.0975421592593193\n",
      "Epoch 2652, Loss: 0.2529333159327507, Final Batch Loss: 0.12318352609872818\n",
      "Epoch 2653, Loss: 0.24644124507904053, Final Batch Loss: 0.14069706201553345\n",
      "Epoch 2654, Loss: 0.2242933288216591, Final Batch Loss: 0.10518789291381836\n",
      "Epoch 2655, Loss: 0.24705662578344345, Final Batch Loss: 0.1550751030445099\n",
      "Epoch 2656, Loss: 0.30431491136550903, Final Batch Loss: 0.16660833358764648\n",
      "Epoch 2657, Loss: 0.31109024584293365, Final Batch Loss: 0.18295224010944366\n",
      "Epoch 2658, Loss: 0.31723901629447937, Final Batch Loss: 0.15078824758529663\n",
      "Epoch 2659, Loss: 0.2214832380414009, Final Batch Loss: 0.10036394745111465\n",
      "Epoch 2660, Loss: 0.3070106953382492, Final Batch Loss: 0.19190295040607452\n",
      "Epoch 2661, Loss: 0.2349572330713272, Final Batch Loss: 0.11012749373912811\n",
      "Epoch 2662, Loss: 0.2875370383262634, Final Batch Loss: 0.15819990634918213\n",
      "Epoch 2663, Loss: 0.23058533668518066, Final Batch Loss: 0.11271917074918747\n",
      "Epoch 2664, Loss: 0.22620458900928497, Final Batch Loss: 0.12402389943599701\n",
      "Epoch 2665, Loss: 0.2685563564300537, Final Batch Loss: 0.09365753829479218\n",
      "Epoch 2666, Loss: 0.2423688843846321, Final Batch Loss: 0.1128106489777565\n",
      "Epoch 2667, Loss: 0.32086313515901566, Final Batch Loss: 0.12327780574560165\n",
      "Epoch 2668, Loss: 0.24361267685890198, Final Batch Loss: 0.15627950429916382\n",
      "Epoch 2669, Loss: 0.22542482614517212, Final Batch Loss: 0.11803413927555084\n",
      "Epoch 2670, Loss: 0.26136939227581024, Final Batch Loss: 0.13140776753425598\n",
      "Epoch 2671, Loss: 0.2363634556531906, Final Batch Loss: 0.130974680185318\n",
      "Epoch 2672, Loss: 0.24022726714611053, Final Batch Loss: 0.12074366211891174\n",
      "Epoch 2673, Loss: 0.21169260144233704, Final Batch Loss: 0.08800632506608963\n",
      "Epoch 2674, Loss: 0.23144657164812088, Final Batch Loss: 0.1322077363729477\n",
      "Epoch 2675, Loss: 0.43737372756004333, Final Batch Loss: 0.23297004401683807\n",
      "Epoch 2676, Loss: 0.2684534192085266, Final Batch Loss: 0.13161733746528625\n",
      "Epoch 2677, Loss: 0.2546006292104721, Final Batch Loss: 0.11396293342113495\n",
      "Epoch 2678, Loss: 0.2224113941192627, Final Batch Loss: 0.11193640530109406\n",
      "Epoch 2679, Loss: 0.2542131319642067, Final Batch Loss: 0.1331014633178711\n",
      "Epoch 2680, Loss: 0.24439598619937897, Final Batch Loss: 0.08124013245105743\n",
      "Epoch 2681, Loss: 0.28026869893074036, Final Batch Loss: 0.13958698511123657\n",
      "Epoch 2682, Loss: 0.2564956918358803, Final Batch Loss: 0.11945699900388718\n",
      "Epoch 2683, Loss: 0.32588163018226624, Final Batch Loss: 0.18785296380519867\n",
      "Epoch 2684, Loss: 0.2808413729071617, Final Batch Loss: 0.19722263514995575\n",
      "Epoch 2685, Loss: 0.23045726120471954, Final Batch Loss: 0.10426823794841766\n",
      "Epoch 2686, Loss: 0.2907910495996475, Final Batch Loss: 0.14237381517887115\n",
      "Epoch 2687, Loss: 0.26320404559373856, Final Batch Loss: 0.14663052558898926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2688, Loss: 0.2899929881095886, Final Batch Loss: 0.13722863793373108\n",
      "Epoch 2689, Loss: 0.27388864755630493, Final Batch Loss: 0.13200122117996216\n",
      "Epoch 2690, Loss: 0.41089534759521484, Final Batch Loss: 0.23150531947612762\n",
      "Epoch 2691, Loss: 0.2538324296474457, Final Batch Loss: 0.10128358006477356\n",
      "Epoch 2692, Loss: 0.22430196404457092, Final Batch Loss: 0.11032316088676453\n",
      "Epoch 2693, Loss: 0.33116209506988525, Final Batch Loss: 0.15977789461612701\n",
      "Epoch 2694, Loss: 0.2699233666062355, Final Batch Loss: 0.11964190751314163\n",
      "Epoch 2695, Loss: 0.3321460038423538, Final Batch Loss: 0.19643591344356537\n",
      "Epoch 2696, Loss: 0.2652489021420479, Final Batch Loss: 0.14163804054260254\n",
      "Epoch 2697, Loss: 0.2685747891664505, Final Batch Loss: 0.10212621092796326\n",
      "Epoch 2698, Loss: 0.24051887542009354, Final Batch Loss: 0.11547672003507614\n",
      "Epoch 2699, Loss: 0.2254823073744774, Final Batch Loss: 0.11491533368825912\n",
      "Epoch 2700, Loss: 0.2879718244075775, Final Batch Loss: 0.1542058289051056\n",
      "Epoch 2701, Loss: 0.27417926490306854, Final Batch Loss: 0.15962746739387512\n",
      "Epoch 2702, Loss: 0.21453583240509033, Final Batch Loss: 0.10149591416120529\n",
      "Epoch 2703, Loss: 0.26014677435159683, Final Batch Loss: 0.10164772719144821\n",
      "Epoch 2704, Loss: 0.2039891555905342, Final Batch Loss: 0.09555133432149887\n",
      "Epoch 2705, Loss: 0.22919557243585587, Final Batch Loss: 0.11771628260612488\n",
      "Epoch 2706, Loss: 0.21839312463998795, Final Batch Loss: 0.10351970046758652\n",
      "Epoch 2707, Loss: 0.30483880639076233, Final Batch Loss: 0.15791410207748413\n",
      "Epoch 2708, Loss: 0.25390172749757767, Final Batch Loss: 0.10228637605905533\n",
      "Epoch 2709, Loss: 0.24344605952501297, Final Batch Loss: 0.11094116419553757\n",
      "Epoch 2710, Loss: 0.25686899572610855, Final Batch Loss: 0.15786835551261902\n",
      "Epoch 2711, Loss: 0.2566567063331604, Final Batch Loss: 0.12369631230831146\n",
      "Epoch 2712, Loss: 0.2927775979042053, Final Batch Loss: 0.15332390367984772\n",
      "Epoch 2713, Loss: 0.2559748664498329, Final Batch Loss: 0.11459621042013168\n",
      "Epoch 2714, Loss: 0.2843653932213783, Final Batch Loss: 0.12370377033948898\n",
      "Epoch 2715, Loss: 0.2687487155199051, Final Batch Loss: 0.14195893704891205\n",
      "Epoch 2716, Loss: 0.2693290188908577, Final Batch Loss: 0.11455089598894119\n",
      "Epoch 2717, Loss: 0.28905048966407776, Final Batch Loss: 0.14366060495376587\n",
      "Epoch 2718, Loss: 0.244279682636261, Final Batch Loss: 0.13035911321640015\n",
      "Epoch 2719, Loss: 0.1932908445596695, Final Batch Loss: 0.11767598241567612\n",
      "Epoch 2720, Loss: 0.30571070313453674, Final Batch Loss: 0.17085902392864227\n",
      "Epoch 2721, Loss: 0.27233364433050156, Final Batch Loss: 0.16192211210727692\n",
      "Epoch 2722, Loss: 0.23432200402021408, Final Batch Loss: 0.12525668740272522\n",
      "Epoch 2723, Loss: 0.21947962045669556, Final Batch Loss: 0.11677143722772598\n",
      "Epoch 2724, Loss: 0.2749304920434952, Final Batch Loss: 0.12556587159633636\n",
      "Epoch 2725, Loss: 0.284966379404068, Final Batch Loss: 0.17386013269424438\n",
      "Epoch 2726, Loss: 0.25587187707424164, Final Batch Loss: 0.11957481503486633\n",
      "Epoch 2727, Loss: 0.2501751482486725, Final Batch Loss: 0.12487004697322845\n",
      "Epoch 2728, Loss: 0.26553843170404434, Final Batch Loss: 0.1455267071723938\n",
      "Epoch 2729, Loss: 0.22501290589571, Final Batch Loss: 0.10793156176805496\n",
      "Epoch 2730, Loss: 0.31718942523002625, Final Batch Loss: 0.2112777978181839\n",
      "Epoch 2731, Loss: 0.26609431207180023, Final Batch Loss: 0.13448937237262726\n",
      "Epoch 2732, Loss: 0.2801048457622528, Final Batch Loss: 0.15246036648750305\n",
      "Epoch 2733, Loss: 0.22296138107776642, Final Batch Loss: 0.12301473319530487\n",
      "Epoch 2734, Loss: 0.27174316346645355, Final Batch Loss: 0.15152248740196228\n",
      "Epoch 2735, Loss: 0.2641891986131668, Final Batch Loss: 0.1251227855682373\n",
      "Epoch 2736, Loss: 0.2849951982498169, Final Batch Loss: 0.15264296531677246\n",
      "Epoch 2737, Loss: 0.2369879111647606, Final Batch Loss: 0.13408643007278442\n",
      "Epoch 2738, Loss: 0.23717621713876724, Final Batch Loss: 0.0878550335764885\n",
      "Epoch 2739, Loss: 0.25994595885276794, Final Batch Loss: 0.08926303684711456\n",
      "Epoch 2740, Loss: 0.20662885904312134, Final Batch Loss: 0.10379789024591446\n",
      "Epoch 2741, Loss: 0.26711662113666534, Final Batch Loss: 0.11235234141349792\n",
      "Epoch 2742, Loss: 0.22257592529058456, Final Batch Loss: 0.12096871435642242\n",
      "Epoch 2743, Loss: 0.24847833067178726, Final Batch Loss: 0.14062350988388062\n",
      "Epoch 2744, Loss: 0.30093951523303986, Final Batch Loss: 0.15670627355575562\n",
      "Epoch 2745, Loss: 0.2226557359099388, Final Batch Loss: 0.11549468338489532\n",
      "Epoch 2746, Loss: 0.2897539585828781, Final Batch Loss: 0.13621768355369568\n",
      "Epoch 2747, Loss: 0.2724180370569229, Final Batch Loss: 0.1460200548171997\n",
      "Epoch 2748, Loss: 0.30370359867811203, Final Batch Loss: 0.19989074766635895\n",
      "Epoch 2749, Loss: 0.20750603079795837, Final Batch Loss: 0.10165401548147202\n",
      "Epoch 2750, Loss: 0.2525193840265274, Final Batch Loss: 0.11471648514270782\n",
      "Epoch 2751, Loss: 0.21830347180366516, Final Batch Loss: 0.09885626286268234\n",
      "Epoch 2752, Loss: 0.20372439920902252, Final Batch Loss: 0.07674486935138702\n",
      "Epoch 2753, Loss: 0.19386927783489227, Final Batch Loss: 0.07467813789844513\n",
      "Epoch 2754, Loss: 0.25578490644693375, Final Batch Loss: 0.07623205333948135\n",
      "Epoch 2755, Loss: 0.245606929063797, Final Batch Loss: 0.11753888428211212\n",
      "Epoch 2756, Loss: 0.30564185231924057, Final Batch Loss: 0.19383399188518524\n",
      "Epoch 2757, Loss: 0.22977767884731293, Final Batch Loss: 0.10657649487257004\n",
      "Epoch 2758, Loss: 0.23713448643684387, Final Batch Loss: 0.1041935533285141\n",
      "Epoch 2759, Loss: 0.3271423578262329, Final Batch Loss: 0.20635467767715454\n",
      "Epoch 2760, Loss: 0.27976518869400024, Final Batch Loss: 0.14580994844436646\n",
      "Epoch 2761, Loss: 0.2589051350951195, Final Batch Loss: 0.1431673914194107\n",
      "Epoch 2762, Loss: 0.25021497160196304, Final Batch Loss: 0.14502845704555511\n",
      "Epoch 2763, Loss: 0.25691303610801697, Final Batch Loss: 0.13052354753017426\n",
      "Epoch 2764, Loss: 0.2841566503047943, Final Batch Loss: 0.15523242950439453\n",
      "Epoch 2765, Loss: 0.21889802813529968, Final Batch Loss: 0.11088092625141144\n",
      "Epoch 2766, Loss: 0.3176129385828972, Final Batch Loss: 0.195306196808815\n",
      "Epoch 2767, Loss: 0.22290176898241043, Final Batch Loss: 0.10469242930412292\n",
      "Epoch 2768, Loss: 0.2101377546787262, Final Batch Loss: 0.08889675885438919\n",
      "Epoch 2769, Loss: 0.3044239431619644, Final Batch Loss: 0.16337808966636658\n",
      "Epoch 2770, Loss: 0.310056671500206, Final Batch Loss: 0.16616596281528473\n",
      "Epoch 2771, Loss: 0.32085853815078735, Final Batch Loss: 0.14469583332538605\n",
      "Epoch 2772, Loss: 0.26236972212791443, Final Batch Loss: 0.11456261575222015\n",
      "Epoch 2773, Loss: 0.26944494992494583, Final Batch Loss: 0.14904068410396576\n",
      "Epoch 2774, Loss: 0.27976125478744507, Final Batch Loss: 0.10552068054676056\n",
      "Epoch 2775, Loss: 0.2535390332341194, Final Batch Loss: 0.14475226402282715\n",
      "Epoch 2776, Loss: 0.2815987467765808, Final Batch Loss: 0.1438717246055603\n",
      "Epoch 2777, Loss: 0.32794851064682007, Final Batch Loss: 0.18571506440639496\n",
      "Epoch 2778, Loss: 0.2397814244031906, Final Batch Loss: 0.08129197359085083\n",
      "Epoch 2779, Loss: 0.18586450815200806, Final Batch Loss: 0.07203566282987595\n",
      "Epoch 2780, Loss: 0.3464808613061905, Final Batch Loss: 0.15819226205348969\n",
      "Epoch 2781, Loss: 0.3039073199033737, Final Batch Loss: 0.15841813385486603\n",
      "Epoch 2782, Loss: 0.330615371465683, Final Batch Loss: 0.17287588119506836\n",
      "Epoch 2783, Loss: 0.2754865437746048, Final Batch Loss: 0.1364472210407257\n",
      "Epoch 2784, Loss: 0.23784243315458298, Final Batch Loss: 0.08600180596113205\n",
      "Epoch 2785, Loss: 0.22754456102848053, Final Batch Loss: 0.12438205629587173\n",
      "Epoch 2786, Loss: 0.22378280758857727, Final Batch Loss: 0.0941849946975708\n",
      "Epoch 2787, Loss: 0.3007113188505173, Final Batch Loss: 0.10616202652454376\n",
      "Epoch 2788, Loss: 0.28695473819971085, Final Batch Loss: 0.18354804813861847\n",
      "Epoch 2789, Loss: 0.289332777261734, Final Batch Loss: 0.12027692794799805\n",
      "Epoch 2790, Loss: 0.2834075689315796, Final Batch Loss: 0.11351104080677032\n",
      "Epoch 2791, Loss: 0.2355300709605217, Final Batch Loss: 0.10921292752027512\n",
      "Epoch 2792, Loss: 0.2419963702559471, Final Batch Loss: 0.11052977293729782\n",
      "Epoch 2793, Loss: 0.2014915943145752, Final Batch Loss: 0.11779377609491348\n",
      "Epoch 2794, Loss: 0.2474243864417076, Final Batch Loss: 0.15420249104499817\n",
      "Epoch 2795, Loss: 0.2656828984618187, Final Batch Loss: 0.14874786138534546\n",
      "Epoch 2796, Loss: 0.2490522339940071, Final Batch Loss: 0.12853564321994781\n",
      "Epoch 2797, Loss: 0.2562681883573532, Final Batch Loss: 0.11466126143932343\n",
      "Epoch 2798, Loss: 0.303072914481163, Final Batch Loss: 0.1382065862417221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2799, Loss: 0.27218349277973175, Final Batch Loss: 0.15477699041366577\n",
      "Epoch 2800, Loss: 0.25159019231796265, Final Batch Loss: 0.12409549951553345\n",
      "Epoch 2801, Loss: 0.22027871012687683, Final Batch Loss: 0.0798395574092865\n",
      "Epoch 2802, Loss: 0.24414902180433273, Final Batch Loss: 0.15042290091514587\n",
      "Epoch 2803, Loss: 0.2278411164879799, Final Batch Loss: 0.10339109599590302\n",
      "Epoch 2804, Loss: 0.24356873333454132, Final Batch Loss: 0.11378626525402069\n",
      "Epoch 2805, Loss: 0.2436143085360527, Final Batch Loss: 0.13319110870361328\n",
      "Epoch 2806, Loss: 0.2237779051065445, Final Batch Loss: 0.11940672248601913\n",
      "Epoch 2807, Loss: 0.2587878480553627, Final Batch Loss: 0.11160881072282791\n",
      "Epoch 2808, Loss: 0.22631032764911652, Final Batch Loss: 0.09198327362537384\n",
      "Epoch 2809, Loss: 0.2180468663573265, Final Batch Loss: 0.07958569377660751\n",
      "Epoch 2810, Loss: 0.270100362598896, Final Batch Loss: 0.10273555666208267\n",
      "Epoch 2811, Loss: 0.26664792746305466, Final Batch Loss: 0.11702042073011398\n",
      "Epoch 2812, Loss: 0.2651275768876076, Final Batch Loss: 0.15762624144554138\n",
      "Epoch 2813, Loss: 0.28232255578041077, Final Batch Loss: 0.11312493681907654\n",
      "Epoch 2814, Loss: 0.261765718460083, Final Batch Loss: 0.15095265209674835\n",
      "Epoch 2815, Loss: 0.30788688361644745, Final Batch Loss: 0.12796550989151\n",
      "Epoch 2816, Loss: 0.2467249259352684, Final Batch Loss: 0.1321561485528946\n",
      "Epoch 2817, Loss: 0.2419566810131073, Final Batch Loss: 0.1290881186723709\n",
      "Epoch 2818, Loss: 0.2595577910542488, Final Batch Loss: 0.1539495587348938\n",
      "Epoch 2819, Loss: 0.22117548435926437, Final Batch Loss: 0.10673371702432632\n",
      "Epoch 2820, Loss: 0.2148076295852661, Final Batch Loss: 0.08199061453342438\n",
      "Epoch 2821, Loss: 0.24998369067907333, Final Batch Loss: 0.12465300410985947\n",
      "Epoch 2822, Loss: 0.25586435198783875, Final Batch Loss: 0.12587805092334747\n",
      "Epoch 2823, Loss: 0.2789628729224205, Final Batch Loss: 0.11804338544607162\n",
      "Epoch 2824, Loss: 0.2655339390039444, Final Batch Loss: 0.14790166914463043\n",
      "Epoch 2825, Loss: 0.2068178355693817, Final Batch Loss: 0.07588046789169312\n",
      "Epoch 2826, Loss: 0.2845306098461151, Final Batch Loss: 0.16222935914993286\n",
      "Epoch 2827, Loss: 0.20373093336820602, Final Batch Loss: 0.08390811830759048\n",
      "Epoch 2828, Loss: 0.34046612679958344, Final Batch Loss: 0.15621237456798553\n",
      "Epoch 2829, Loss: 0.2663061320781708, Final Batch Loss: 0.16223418712615967\n",
      "Epoch 2830, Loss: 0.24789928644895554, Final Batch Loss: 0.10340399295091629\n",
      "Epoch 2831, Loss: 0.25722433626651764, Final Batch Loss: 0.11988548934459686\n",
      "Epoch 2832, Loss: 0.28050989657640457, Final Batch Loss: 0.12477365881204605\n",
      "Epoch 2833, Loss: 0.24855825304985046, Final Batch Loss: 0.14211228489875793\n",
      "Epoch 2834, Loss: 0.22903943806886673, Final Batch Loss: 0.08081182092428207\n",
      "Epoch 2835, Loss: 0.27572283148765564, Final Batch Loss: 0.1688937395811081\n",
      "Epoch 2836, Loss: 0.24092954397201538, Final Batch Loss: 0.09679144620895386\n",
      "Epoch 2837, Loss: 0.21873628348112106, Final Batch Loss: 0.11559073626995087\n",
      "Epoch 2838, Loss: 0.2435455620288849, Final Batch Loss: 0.11857875436544418\n",
      "Epoch 2839, Loss: 0.3031871020793915, Final Batch Loss: 0.1671818196773529\n",
      "Epoch 2840, Loss: 0.2838197499513626, Final Batch Loss: 0.12712793052196503\n",
      "Epoch 2841, Loss: 0.3261672854423523, Final Batch Loss: 0.16363908350467682\n",
      "Epoch 2842, Loss: 0.24855808168649673, Final Batch Loss: 0.13412705063819885\n",
      "Epoch 2843, Loss: 0.2700681760907173, Final Batch Loss: 0.10898139327764511\n",
      "Epoch 2844, Loss: 0.2694590836763382, Final Batch Loss: 0.13785964250564575\n",
      "Epoch 2845, Loss: 0.2818705514073372, Final Batch Loss: 0.1706324964761734\n",
      "Epoch 2846, Loss: 0.39977097511291504, Final Batch Loss: 0.18883925676345825\n",
      "Epoch 2847, Loss: 0.2196061909198761, Final Batch Loss: 0.12908931076526642\n",
      "Epoch 2848, Loss: 0.24399462342262268, Final Batch Loss: 0.11353093385696411\n",
      "Epoch 2849, Loss: 0.2606199085712433, Final Batch Loss: 0.12506483495235443\n",
      "Epoch 2850, Loss: 0.20610255002975464, Final Batch Loss: 0.08821810036897659\n",
      "Epoch 2851, Loss: 0.22456295788288116, Final Batch Loss: 0.10312231630086899\n",
      "Epoch 2852, Loss: 0.26707639545202255, Final Batch Loss: 0.11746899038553238\n",
      "Epoch 2853, Loss: 0.21788066625595093, Final Batch Loss: 0.11835138499736786\n",
      "Epoch 2854, Loss: 0.2408473715186119, Final Batch Loss: 0.10894962400197983\n",
      "Epoch 2855, Loss: 0.25307299941778183, Final Batch Loss: 0.14300379157066345\n",
      "Epoch 2856, Loss: 0.23106183111667633, Final Batch Loss: 0.12630631029605865\n",
      "Epoch 2857, Loss: 0.25382478535175323, Final Batch Loss: 0.12849973142147064\n",
      "Epoch 2858, Loss: 0.2442416250705719, Final Batch Loss: 0.13140666484832764\n",
      "Epoch 2859, Loss: 0.33490969240665436, Final Batch Loss: 0.15193453431129456\n",
      "Epoch 2860, Loss: 0.2633073329925537, Final Batch Loss: 0.08989749848842621\n",
      "Epoch 2861, Loss: 0.2781180515885353, Final Batch Loss: 0.11510659009218216\n",
      "Epoch 2862, Loss: 0.2635241150856018, Final Batch Loss: 0.15268409252166748\n",
      "Epoch 2863, Loss: 0.2280980497598648, Final Batch Loss: 0.133752703666687\n",
      "Epoch 2864, Loss: 0.25228574126958847, Final Batch Loss: 0.15592171251773834\n",
      "Epoch 2865, Loss: 0.23824001103639603, Final Batch Loss: 0.10812186449766159\n",
      "Epoch 2866, Loss: 0.25780337303876877, Final Batch Loss: 0.14288635551929474\n",
      "Epoch 2867, Loss: 0.24677065014839172, Final Batch Loss: 0.12287364900112152\n",
      "Epoch 2868, Loss: 0.238571897149086, Final Batch Loss: 0.12652207911014557\n",
      "Epoch 2869, Loss: 0.2801889702677727, Final Batch Loss: 0.11502280086278915\n",
      "Epoch 2870, Loss: 0.21967043727636337, Final Batch Loss: 0.13684828579425812\n",
      "Epoch 2871, Loss: 0.28635552525520325, Final Batch Loss: 0.10995283722877502\n",
      "Epoch 2872, Loss: 0.28164713829755783, Final Batch Loss: 0.19153055548667908\n",
      "Epoch 2873, Loss: 0.20659369975328445, Final Batch Loss: 0.09997770190238953\n",
      "Epoch 2874, Loss: 0.19967584311962128, Final Batch Loss: 0.1124073788523674\n",
      "Epoch 2875, Loss: 0.2525278329849243, Final Batch Loss: 0.11952206492424011\n",
      "Epoch 2876, Loss: 0.24909620732069016, Final Batch Loss: 0.11769459396600723\n",
      "Epoch 2877, Loss: 0.2436913549900055, Final Batch Loss: 0.13217632472515106\n",
      "Epoch 2878, Loss: 0.22369197010993958, Final Batch Loss: 0.11320634186267853\n",
      "Epoch 2879, Loss: 0.2896683067083359, Final Batch Loss: 0.15815748274326324\n",
      "Epoch 2880, Loss: 0.22596748173236847, Final Batch Loss: 0.11762239784002304\n",
      "Epoch 2881, Loss: 0.24619737267494202, Final Batch Loss: 0.12746937572956085\n",
      "Epoch 2882, Loss: 0.26203011721372604, Final Batch Loss: 0.09253191202878952\n",
      "Epoch 2883, Loss: 0.2628973424434662, Final Batch Loss: 0.13047169148921967\n",
      "Epoch 2884, Loss: 0.22808465361595154, Final Batch Loss: 0.12836526334285736\n",
      "Epoch 2885, Loss: 0.27067920565605164, Final Batch Loss: 0.1330963522195816\n",
      "Epoch 2886, Loss: 0.2201150357723236, Final Batch Loss: 0.10370898246765137\n",
      "Epoch 2887, Loss: 0.2804849222302437, Final Batch Loss: 0.11045042425394058\n",
      "Epoch 2888, Loss: 0.256880484521389, Final Batch Loss: 0.1547912210226059\n",
      "Epoch 2889, Loss: 0.21769596636295319, Final Batch Loss: 0.1052115187048912\n",
      "Epoch 2890, Loss: 0.20517637580633163, Final Batch Loss: 0.1155015155673027\n",
      "Epoch 2891, Loss: 0.28438813984394073, Final Batch Loss: 0.18461734056472778\n",
      "Epoch 2892, Loss: 0.18128927797079086, Final Batch Loss: 0.07202082872390747\n",
      "Epoch 2893, Loss: 0.27355627715587616, Final Batch Loss: 0.13030676543712616\n",
      "Epoch 2894, Loss: 0.21314726769924164, Final Batch Loss: 0.09565795212984085\n",
      "Epoch 2895, Loss: 0.2826278805732727, Final Batch Loss: 0.16438411176204681\n",
      "Epoch 2896, Loss: 0.22215450555086136, Final Batch Loss: 0.11117089539766312\n",
      "Epoch 2897, Loss: 0.23907619714736938, Final Batch Loss: 0.10008974373340607\n",
      "Epoch 2898, Loss: 0.21472813934087753, Final Batch Loss: 0.09831315279006958\n",
      "Epoch 2899, Loss: 0.2732943594455719, Final Batch Loss: 0.12618862092494965\n",
      "Epoch 2900, Loss: 0.23809361457824707, Final Batch Loss: 0.10509826242923737\n",
      "Epoch 2901, Loss: 0.22906801104545593, Final Batch Loss: 0.11848247796297073\n",
      "Epoch 2902, Loss: 0.2874396741390228, Final Batch Loss: 0.14243389666080475\n",
      "Epoch 2903, Loss: 0.2729634791612625, Final Batch Loss: 0.1605113446712494\n",
      "Epoch 2904, Loss: 0.2609603554010391, Final Batch Loss: 0.13719123601913452\n",
      "Epoch 2905, Loss: 0.2236088067293167, Final Batch Loss: 0.09935543686151505\n",
      "Epoch 2906, Loss: 0.2202306166291237, Final Batch Loss: 0.09824869781732559\n",
      "Epoch 2907, Loss: 0.219229057431221, Final Batch Loss: 0.08817051351070404\n",
      "Epoch 2908, Loss: 0.2819112092256546, Final Batch Loss: 0.16457872092723846\n",
      "Epoch 2909, Loss: 0.2293616235256195, Final Batch Loss: 0.1476592868566513\n",
      "Epoch 2910, Loss: 0.2569420784711838, Final Batch Loss: 0.14947743713855743\n",
      "Epoch 2911, Loss: 0.27416279166936874, Final Batch Loss: 0.15997040271759033\n",
      "Epoch 2912, Loss: 0.2548884376883507, Final Batch Loss: 0.13381458818912506\n",
      "Epoch 2913, Loss: 0.244864322245121, Final Batch Loss: 0.12584899365901947\n",
      "Epoch 2914, Loss: 0.2727612257003784, Final Batch Loss: 0.10875855386257172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2915, Loss: 0.22127970308065414, Final Batch Loss: 0.10204876214265823\n",
      "Epoch 2916, Loss: 0.2179369404911995, Final Batch Loss: 0.10087397694587708\n",
      "Epoch 2917, Loss: 0.2637737989425659, Final Batch Loss: 0.10655046999454498\n",
      "Epoch 2918, Loss: 0.2630844861268997, Final Batch Loss: 0.13290543854236603\n",
      "Epoch 2919, Loss: 0.3176860064268112, Final Batch Loss: 0.15852724015712738\n",
      "Epoch 2920, Loss: 0.27918073534965515, Final Batch Loss: 0.1441691517829895\n",
      "Epoch 2921, Loss: 0.27569419145584106, Final Batch Loss: 0.12515190243721008\n",
      "Epoch 2922, Loss: 0.23905934393405914, Final Batch Loss: 0.13635756075382233\n",
      "Epoch 2923, Loss: 0.22440333664417267, Final Batch Loss: 0.11608131974935532\n",
      "Epoch 2924, Loss: 0.23216886073350906, Final Batch Loss: 0.10724730044603348\n",
      "Epoch 2925, Loss: 0.25912078469991684, Final Batch Loss: 0.13734984397888184\n",
      "Epoch 2926, Loss: 0.3036545664072037, Final Batch Loss: 0.1444447636604309\n",
      "Epoch 2927, Loss: 0.20143283158540726, Final Batch Loss: 0.09656176716089249\n",
      "Epoch 2928, Loss: 0.23565933108329773, Final Batch Loss: 0.11530818790197372\n",
      "Epoch 2929, Loss: 0.26525329053401947, Final Batch Loss: 0.1378173828125\n",
      "Epoch 2930, Loss: 0.23035187274217606, Final Batch Loss: 0.09863569587469101\n",
      "Epoch 2931, Loss: 0.3010057210922241, Final Batch Loss: 0.14976921677589417\n",
      "Epoch 2932, Loss: 0.23813162744045258, Final Batch Loss: 0.12025228887796402\n",
      "Epoch 2933, Loss: 0.22532369196414948, Final Batch Loss: 0.11109426617622375\n",
      "Epoch 2934, Loss: 0.22839394956827164, Final Batch Loss: 0.09332778304815292\n",
      "Epoch 2935, Loss: 0.22567956149578094, Final Batch Loss: 0.11995336413383484\n",
      "Epoch 2936, Loss: 0.2687106132507324, Final Batch Loss: 0.14493079483509064\n",
      "Epoch 2937, Loss: 0.2254410982131958, Final Batch Loss: 0.12366454303264618\n",
      "Epoch 2938, Loss: 0.1938667744398117, Final Batch Loss: 0.08481225371360779\n",
      "Epoch 2939, Loss: 0.2740423157811165, Final Batch Loss: 0.1567625254392624\n",
      "Epoch 2940, Loss: 0.2634090259671211, Final Batch Loss: 0.139434814453125\n",
      "Epoch 2941, Loss: 0.22267909348011017, Final Batch Loss: 0.12027381360530853\n",
      "Epoch 2942, Loss: 0.24852479994297028, Final Batch Loss: 0.13249994814395905\n",
      "Epoch 2943, Loss: 0.2664164826273918, Final Batch Loss: 0.14751510322093964\n",
      "Epoch 2944, Loss: 0.42924074828624725, Final Batch Loss: 0.3125433027744293\n",
      "Epoch 2945, Loss: 0.289859801530838, Final Batch Loss: 0.13151879608631134\n",
      "Epoch 2946, Loss: 0.21438506245613098, Final Batch Loss: 0.09613727033138275\n",
      "Epoch 2947, Loss: 0.2739308625459671, Final Batch Loss: 0.1436292678117752\n",
      "Epoch 2948, Loss: 0.28459540009498596, Final Batch Loss: 0.19257859885692596\n",
      "Epoch 2949, Loss: 0.20981159061193466, Final Batch Loss: 0.0997641533613205\n",
      "Epoch 2950, Loss: 0.22608767449855804, Final Batch Loss: 0.11589032411575317\n",
      "Epoch 2951, Loss: 0.29581038653850555, Final Batch Loss: 0.16020116209983826\n",
      "Epoch 2952, Loss: 0.26570357382297516, Final Batch Loss: 0.12323763966560364\n",
      "Epoch 2953, Loss: 0.2565438896417618, Final Batch Loss: 0.15843896567821503\n",
      "Epoch 2954, Loss: 0.2762141078710556, Final Batch Loss: 0.150446355342865\n",
      "Epoch 2955, Loss: 0.22549567371606827, Final Batch Loss: 0.12516094744205475\n",
      "Epoch 2956, Loss: 0.24276874959468842, Final Batch Loss: 0.1097734272480011\n",
      "Epoch 2957, Loss: 0.23958897590637207, Final Batch Loss: 0.11122700572013855\n",
      "Epoch 2958, Loss: 0.17631738632917404, Final Batch Loss: 0.09492036700248718\n",
      "Epoch 2959, Loss: 0.25695713609457016, Final Batch Loss: 0.1654433161020279\n",
      "Epoch 2960, Loss: 0.28675729036331177, Final Batch Loss: 0.1537897288799286\n",
      "Epoch 2961, Loss: 0.24111881107091904, Final Batch Loss: 0.12065734714269638\n",
      "Epoch 2962, Loss: 0.2231711596250534, Final Batch Loss: 0.12133591622114182\n",
      "Epoch 2963, Loss: 0.2165094092488289, Final Batch Loss: 0.08657362312078476\n",
      "Epoch 2964, Loss: 0.2317337542772293, Final Batch Loss: 0.12670956552028656\n",
      "Epoch 2965, Loss: 0.21667125076055527, Final Batch Loss: 0.12295827269554138\n",
      "Epoch 2966, Loss: 0.18759960681200027, Final Batch Loss: 0.09136658161878586\n",
      "Epoch 2967, Loss: 0.19879360496997833, Final Batch Loss: 0.1139676421880722\n",
      "Epoch 2968, Loss: 0.25607482343912125, Final Batch Loss: 0.1213197186589241\n",
      "Epoch 2969, Loss: 0.27951231598854065, Final Batch Loss: 0.1603083461523056\n",
      "Epoch 2970, Loss: 0.25086556375026703, Final Batch Loss: 0.11575841903686523\n",
      "Epoch 2971, Loss: 0.2701012045145035, Final Batch Loss: 0.12579910457134247\n",
      "Epoch 2972, Loss: 0.20053881406784058, Final Batch Loss: 0.11707130074501038\n",
      "Epoch 2973, Loss: 0.2554994896054268, Final Batch Loss: 0.09340628236532211\n",
      "Epoch 2974, Loss: 0.2771926373243332, Final Batch Loss: 0.14698806405067444\n",
      "Epoch 2975, Loss: 0.22981390357017517, Final Batch Loss: 0.08561208844184875\n",
      "Epoch 2976, Loss: 0.2675594687461853, Final Batch Loss: 0.13417650759220123\n",
      "Epoch 2977, Loss: 0.20046519488096237, Final Batch Loss: 0.070156030356884\n",
      "Epoch 2978, Loss: 0.19413820654153824, Final Batch Loss: 0.06529086083173752\n",
      "Epoch 2979, Loss: 0.2240237444639206, Final Batch Loss: 0.10062207281589508\n",
      "Epoch 2980, Loss: 0.24352923035621643, Final Batch Loss: 0.12021323293447495\n",
      "Epoch 2981, Loss: 0.2551371082663536, Final Batch Loss: 0.11551082879304886\n",
      "Epoch 2982, Loss: 0.22303874045610428, Final Batch Loss: 0.09277909249067307\n",
      "Epoch 2983, Loss: 0.2410459890961647, Final Batch Loss: 0.09423886984586716\n",
      "Epoch 2984, Loss: 0.24397344887256622, Final Batch Loss: 0.13703760504722595\n",
      "Epoch 2985, Loss: 0.25830022245645523, Final Batch Loss: 0.14645987749099731\n",
      "Epoch 2986, Loss: 0.24516545236110687, Final Batch Loss: 0.14036765694618225\n",
      "Epoch 2987, Loss: 0.19406411796808243, Final Batch Loss: 0.0716388151049614\n",
      "Epoch 2988, Loss: 0.20952900499105453, Final Batch Loss: 0.07573951035737991\n",
      "Epoch 2989, Loss: 0.20133011788129807, Final Batch Loss: 0.12446253001689911\n",
      "Epoch 2990, Loss: 0.2831839546561241, Final Batch Loss: 0.17173591256141663\n",
      "Epoch 2991, Loss: 0.2152870073914528, Final Batch Loss: 0.10422411561012268\n",
      "Epoch 2992, Loss: 0.2018958106637001, Final Batch Loss: 0.10320018231868744\n",
      "Epoch 2993, Loss: 0.25066012144088745, Final Batch Loss: 0.142339289188385\n",
      "Epoch 2994, Loss: 0.22541313618421555, Final Batch Loss: 0.1193653792142868\n",
      "Epoch 2995, Loss: 0.29577140510082245, Final Batch Loss: 0.15326061844825745\n",
      "Epoch 2996, Loss: 0.2242756113409996, Final Batch Loss: 0.11490869522094727\n",
      "Epoch 2997, Loss: 0.25991422683000565, Final Batch Loss: 0.11337023228406906\n",
      "Epoch 2998, Loss: 0.25421611964702606, Final Batch Loss: 0.1275016814470291\n",
      "Epoch 2999, Loss: 0.24675776064395905, Final Batch Loss: 0.138995960354805\n",
      "Epoch 3000, Loss: 0.27888182550668716, Final Batch Loss: 0.1802036166191101\n",
      "Epoch 3001, Loss: 0.24475513398647308, Final Batch Loss: 0.13226717710494995\n",
      "Epoch 3002, Loss: 0.2936854958534241, Final Batch Loss: 0.1357867270708084\n",
      "Epoch 3003, Loss: 0.2280237078666687, Final Batch Loss: 0.11869211494922638\n",
      "Epoch 3004, Loss: 0.26652324199676514, Final Batch Loss: 0.08579248189926147\n",
      "Epoch 3005, Loss: 0.3109624683856964, Final Batch Loss: 0.1466602385044098\n",
      "Epoch 3006, Loss: 0.19842682033777237, Final Batch Loss: 0.08444226533174515\n",
      "Epoch 3007, Loss: 0.2513495609164238, Final Batch Loss: 0.14484156668186188\n",
      "Epoch 3008, Loss: 0.3066888451576233, Final Batch Loss: 0.10126228630542755\n",
      "Epoch 3009, Loss: 0.25443459302186966, Final Batch Loss: 0.15881095826625824\n",
      "Epoch 3010, Loss: 0.20608586072921753, Final Batch Loss: 0.09465134143829346\n",
      "Epoch 3011, Loss: 0.3230648338794708, Final Batch Loss: 0.10811899602413177\n",
      "Epoch 3012, Loss: 0.2079290896654129, Final Batch Loss: 0.11314503103494644\n",
      "Epoch 3013, Loss: 0.21199247241020203, Final Batch Loss: 0.09551935642957687\n",
      "Epoch 3014, Loss: 0.21503208577632904, Final Batch Loss: 0.08577781915664673\n",
      "Epoch 3015, Loss: 0.2955935448408127, Final Batch Loss: 0.16718637943267822\n",
      "Epoch 3016, Loss: 0.2832988426089287, Final Batch Loss: 0.16631484031677246\n",
      "Epoch 3017, Loss: 0.24641811102628708, Final Batch Loss: 0.09420200437307358\n",
      "Epoch 3018, Loss: 0.1956775113940239, Final Batch Loss: 0.09222665429115295\n",
      "Epoch 3019, Loss: 0.24673911184072495, Final Batch Loss: 0.15148642659187317\n",
      "Epoch 3020, Loss: 0.2245909422636032, Final Batch Loss: 0.11332475394010544\n",
      "Epoch 3021, Loss: 0.21487051248550415, Final Batch Loss: 0.11334909498691559\n",
      "Epoch 3022, Loss: 0.23166269063949585, Final Batch Loss: 0.11698426306247711\n",
      "Epoch 3023, Loss: 0.2587776631116867, Final Batch Loss: 0.10645405948162079\n",
      "Epoch 3024, Loss: 0.266673281788826, Final Batch Loss: 0.11848753690719604\n",
      "Epoch 3025, Loss: 0.2156781405210495, Final Batch Loss: 0.10157424956560135\n",
      "Epoch 3026, Loss: 0.28766700625419617, Final Batch Loss: 0.15672758221626282\n",
      "Epoch 3027, Loss: 0.22451846301555634, Final Batch Loss: 0.09869296848773956\n",
      "Epoch 3028, Loss: 0.2838689163327217, Final Batch Loss: 0.16604383289813995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3029, Loss: 0.2105524241924286, Final Batch Loss: 0.10556483268737793\n",
      "Epoch 3030, Loss: 0.22648869454860687, Final Batch Loss: 0.10251808166503906\n",
      "Epoch 3031, Loss: 0.28478003293275833, Final Batch Loss: 0.16808290779590607\n",
      "Epoch 3032, Loss: 0.21981295198202133, Final Batch Loss: 0.10890394449234009\n",
      "Epoch 3033, Loss: 0.24079860001802444, Final Batch Loss: 0.12256354838609695\n",
      "Epoch 3034, Loss: 0.21883874386548996, Final Batch Loss: 0.11945636570453644\n",
      "Epoch 3035, Loss: 0.2599254697561264, Final Batch Loss: 0.13207602500915527\n",
      "Epoch 3036, Loss: 0.26208260655403137, Final Batch Loss: 0.1416587382555008\n",
      "Epoch 3037, Loss: 0.23679452389478683, Final Batch Loss: 0.1163991391658783\n",
      "Epoch 3038, Loss: 0.28000298887491226, Final Batch Loss: 0.09915951639413834\n",
      "Epoch 3039, Loss: 0.27213384956121445, Final Batch Loss: 0.14824818074703217\n",
      "Epoch 3040, Loss: 0.4012550562620163, Final Batch Loss: 0.18587027490139008\n",
      "Epoch 3041, Loss: 0.25698254257440567, Final Batch Loss: 0.1489938348531723\n",
      "Epoch 3042, Loss: 0.22559776157140732, Final Batch Loss: 0.12828725576400757\n",
      "Epoch 3043, Loss: 0.23771825432777405, Final Batch Loss: 0.1360272467136383\n",
      "Epoch 3044, Loss: 0.2542402893304825, Final Batch Loss: 0.12572379410266876\n",
      "Epoch 3045, Loss: 0.264202319085598, Final Batch Loss: 0.17563480138778687\n",
      "Epoch 3046, Loss: 0.25328229367733, Final Batch Loss: 0.130680114030838\n",
      "Epoch 3047, Loss: 0.2461291402578354, Final Batch Loss: 0.1044364720582962\n",
      "Epoch 3048, Loss: 0.2974017560482025, Final Batch Loss: 0.15494883060455322\n",
      "Epoch 3049, Loss: 0.23054979741573334, Final Batch Loss: 0.10032853484153748\n",
      "Epoch 3050, Loss: 0.2815508097410202, Final Batch Loss: 0.12205298244953156\n",
      "Epoch 3051, Loss: 0.24640005081892014, Final Batch Loss: 0.11975418776273727\n",
      "Epoch 3052, Loss: 0.19505701959133148, Final Batch Loss: 0.08910013735294342\n",
      "Epoch 3053, Loss: 0.22515805065631866, Final Batch Loss: 0.10102826356887817\n",
      "Epoch 3054, Loss: 0.2962346747517586, Final Batch Loss: 0.11161927133798599\n",
      "Epoch 3055, Loss: 0.2313474342226982, Final Batch Loss: 0.1236349418759346\n",
      "Epoch 3056, Loss: 0.26407040655612946, Final Batch Loss: 0.08377888798713684\n",
      "Epoch 3057, Loss: 0.23680022358894348, Final Batch Loss: 0.12828628718852997\n",
      "Epoch 3058, Loss: 0.23960640281438828, Final Batch Loss: 0.10574982315301895\n",
      "Epoch 3059, Loss: 0.23704370856285095, Final Batch Loss: 0.12594321370124817\n",
      "Epoch 3060, Loss: 0.29045775532722473, Final Batch Loss: 0.1677047610282898\n",
      "Epoch 3061, Loss: 0.2867771089076996, Final Batch Loss: 0.18140383064746857\n",
      "Epoch 3062, Loss: 0.18822360038757324, Final Batch Loss: 0.07026112824678421\n",
      "Epoch 3063, Loss: 0.24184809625148773, Final Batch Loss: 0.13270217180252075\n",
      "Epoch 3064, Loss: 0.22672169655561447, Final Batch Loss: 0.09373300522565842\n",
      "Epoch 3065, Loss: 0.25769660621881485, Final Batch Loss: 0.15789449214935303\n",
      "Epoch 3066, Loss: 0.2207510769367218, Final Batch Loss: 0.1339406669139862\n",
      "Epoch 3067, Loss: 0.20684067904949188, Final Batch Loss: 0.11719843000173569\n",
      "Epoch 3068, Loss: 0.17966409027576447, Final Batch Loss: 0.1010202169418335\n",
      "Epoch 3069, Loss: 0.23974954336881638, Final Batch Loss: 0.09914534538984299\n",
      "Epoch 3070, Loss: 0.2640400752425194, Final Batch Loss: 0.14436107873916626\n",
      "Epoch 3071, Loss: 0.296239510178566, Final Batch Loss: 0.14035522937774658\n",
      "Epoch 3072, Loss: 0.22373248636722565, Final Batch Loss: 0.13825774192810059\n",
      "Epoch 3073, Loss: 0.23104438185691833, Final Batch Loss: 0.14983060956001282\n",
      "Epoch 3074, Loss: 0.19950652122497559, Final Batch Loss: 0.08996590971946716\n",
      "Epoch 3075, Loss: 0.24616165459156036, Final Batch Loss: 0.11119942367076874\n",
      "Epoch 3076, Loss: 0.2148200049996376, Final Batch Loss: 0.09186698496341705\n",
      "Epoch 3077, Loss: 0.25382623076438904, Final Batch Loss: 0.13116878271102905\n",
      "Epoch 3078, Loss: 0.2510293796658516, Final Batch Loss: 0.11637648195028305\n",
      "Epoch 3079, Loss: 0.2588927000761032, Final Batch Loss: 0.1285010278224945\n",
      "Epoch 3080, Loss: 0.24256166070699692, Final Batch Loss: 0.12070015072822571\n",
      "Epoch 3081, Loss: 0.20163409411907196, Final Batch Loss: 0.10286582261323929\n",
      "Epoch 3082, Loss: 0.19486281275749207, Final Batch Loss: 0.08800797909498215\n",
      "Epoch 3083, Loss: 0.2533240467309952, Final Batch Loss: 0.12197770178318024\n",
      "Epoch 3084, Loss: 0.21532322466373444, Final Batch Loss: 0.10020514577627182\n",
      "Epoch 3085, Loss: 0.2746470794081688, Final Batch Loss: 0.15437614917755127\n",
      "Epoch 3086, Loss: 0.3112616240978241, Final Batch Loss: 0.16734881699085236\n",
      "Epoch 3087, Loss: 0.23140573501586914, Final Batch Loss: 0.10934828966856003\n",
      "Epoch 3088, Loss: 0.2409757599234581, Final Batch Loss: 0.1074269488453865\n",
      "Epoch 3089, Loss: 0.22860417515039444, Final Batch Loss: 0.12662041187286377\n",
      "Epoch 3090, Loss: 0.24557462334632874, Final Batch Loss: 0.1185704916715622\n",
      "Epoch 3091, Loss: 0.23514509201049805, Final Batch Loss: 0.10348847508430481\n",
      "Epoch 3092, Loss: 0.24004650115966797, Final Batch Loss: 0.11704056710004807\n",
      "Epoch 3093, Loss: 0.2429071143269539, Final Batch Loss: 0.12429997324943542\n",
      "Epoch 3094, Loss: 0.22756794095039368, Final Batch Loss: 0.0730859786272049\n",
      "Epoch 3095, Loss: 0.3079061955213547, Final Batch Loss: 0.1443251222372055\n",
      "Epoch 3096, Loss: 0.23950394988059998, Final Batch Loss: 0.1447731852531433\n",
      "Epoch 3097, Loss: 0.21923696249723434, Final Batch Loss: 0.1025407686829567\n",
      "Epoch 3098, Loss: 0.1850692555308342, Final Batch Loss: 0.07575054466724396\n",
      "Epoch 3099, Loss: 0.22551872581243515, Final Batch Loss: 0.1026872992515564\n",
      "Epoch 3100, Loss: 0.23488754034042358, Final Batch Loss: 0.09816403687000275\n",
      "Epoch 3101, Loss: 0.22810813039541245, Final Batch Loss: 0.1046694666147232\n",
      "Epoch 3102, Loss: 0.22363294661045074, Final Batch Loss: 0.11623194813728333\n",
      "Epoch 3103, Loss: 0.21512747555971146, Final Batch Loss: 0.09690159559249878\n",
      "Epoch 3104, Loss: 0.294310063123703, Final Batch Loss: 0.10424283146858215\n",
      "Epoch 3105, Loss: 0.19806940108537674, Final Batch Loss: 0.10487259924411774\n",
      "Epoch 3106, Loss: 0.2086326852440834, Final Batch Loss: 0.10709746181964874\n",
      "Epoch 3107, Loss: 0.23950950056314468, Final Batch Loss: 0.10641803592443466\n",
      "Epoch 3108, Loss: 0.22798021882772446, Final Batch Loss: 0.09976338595151901\n",
      "Epoch 3109, Loss: 0.25276604294776917, Final Batch Loss: 0.1025618463754654\n",
      "Epoch 3110, Loss: 0.2685026526451111, Final Batch Loss: 0.13140179216861725\n",
      "Epoch 3111, Loss: 0.28047017753124237, Final Batch Loss: 0.08856409788131714\n",
      "Epoch 3112, Loss: 0.2538731023669243, Final Batch Loss: 0.09341011196374893\n",
      "Epoch 3113, Loss: 0.252646267414093, Final Batch Loss: 0.13075916469097137\n",
      "Epoch 3114, Loss: 0.2611806318163872, Final Batch Loss: 0.14478275179862976\n",
      "Epoch 3115, Loss: 0.21386028081178665, Final Batch Loss: 0.12432844936847687\n",
      "Epoch 3116, Loss: 0.18472761660814285, Final Batch Loss: 0.08663422614336014\n",
      "Epoch 3117, Loss: 0.19873400032520294, Final Batch Loss: 0.08767609298229218\n",
      "Epoch 3118, Loss: 0.24709375202655792, Final Batch Loss: 0.1293829381465912\n",
      "Epoch 3119, Loss: 0.20692316442728043, Final Batch Loss: 0.11017956584692001\n",
      "Epoch 3120, Loss: 0.2371995449066162, Final Batch Loss: 0.11948385834693909\n",
      "Epoch 3121, Loss: 0.26690249145030975, Final Batch Loss: 0.15749526023864746\n",
      "Epoch 3122, Loss: 0.235435388982296, Final Batch Loss: 0.10515693575143814\n",
      "Epoch 3123, Loss: 0.20938709378242493, Final Batch Loss: 0.10291381180286407\n",
      "Epoch 3124, Loss: 0.23820578306913376, Final Batch Loss: 0.11412937194108963\n",
      "Epoch 3125, Loss: 0.2097310572862625, Final Batch Loss: 0.09848526865243912\n",
      "Epoch 3126, Loss: 0.25475531816482544, Final Batch Loss: 0.14627602696418762\n",
      "Epoch 3127, Loss: 0.2817957028746605, Final Batch Loss: 0.16326789557933807\n",
      "Epoch 3128, Loss: 0.25021861493587494, Final Batch Loss: 0.11796583235263824\n",
      "Epoch 3129, Loss: 0.21902645379304886, Final Batch Loss: 0.10255566239356995\n",
      "Epoch 3130, Loss: 0.25336354225873947, Final Batch Loss: 0.11877068132162094\n",
      "Epoch 3131, Loss: 0.2761895954608917, Final Batch Loss: 0.152653768658638\n",
      "Epoch 3132, Loss: 0.23148126155138016, Final Batch Loss: 0.10198908299207687\n",
      "Epoch 3133, Loss: 0.2348472997546196, Final Batch Loss: 0.10441244393587112\n",
      "Epoch 3134, Loss: 0.25634224712848663, Final Batch Loss: 0.14201906323432922\n",
      "Epoch 3135, Loss: 0.22296010702848434, Final Batch Loss: 0.11883889883756638\n",
      "Epoch 3136, Loss: 0.19812364131212234, Final Batch Loss: 0.10844430327415466\n",
      "Epoch 3137, Loss: 0.2886362820863724, Final Batch Loss: 0.16333091259002686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3138, Loss: 0.2714981883764267, Final Batch Loss: 0.1430426836013794\n",
      "Epoch 3139, Loss: 0.2349041923880577, Final Batch Loss: 0.11948197335004807\n",
      "Epoch 3140, Loss: 0.2390609160065651, Final Batch Loss: 0.09130669385194778\n",
      "Epoch 3141, Loss: 0.22786714881658554, Final Batch Loss: 0.10900534689426422\n",
      "Epoch 3142, Loss: 0.2273126021027565, Final Batch Loss: 0.09965609759092331\n",
      "Epoch 3143, Loss: 0.23661485314369202, Final Batch Loss: 0.13947878777980804\n",
      "Epoch 3144, Loss: 0.26333199441432953, Final Batch Loss: 0.13466747105121613\n",
      "Epoch 3145, Loss: 0.2106342911720276, Final Batch Loss: 0.1008063405752182\n",
      "Epoch 3146, Loss: 0.18573486804962158, Final Batch Loss: 0.0776372104883194\n",
      "Epoch 3147, Loss: 0.2579740360379219, Final Batch Loss: 0.11697874218225479\n",
      "Epoch 3148, Loss: 0.3026135712862015, Final Batch Loss: 0.17315611243247986\n",
      "Epoch 3149, Loss: 0.23582520335912704, Final Batch Loss: 0.11438228189945221\n",
      "Epoch 3150, Loss: 0.215752474963665, Final Batch Loss: 0.11783147603273392\n",
      "Epoch 3151, Loss: 0.25747082382440567, Final Batch Loss: 0.12465166300535202\n",
      "Epoch 3152, Loss: 0.2896142601966858, Final Batch Loss: 0.16801735758781433\n",
      "Epoch 3153, Loss: 0.2084864154458046, Final Batch Loss: 0.07276815921068192\n",
      "Epoch 3154, Loss: 0.18941421061754227, Final Batch Loss: 0.07290738821029663\n",
      "Epoch 3155, Loss: 0.24567370116710663, Final Batch Loss: 0.11731241643428802\n",
      "Epoch 3156, Loss: 0.23321270197629929, Final Batch Loss: 0.089446060359478\n",
      "Epoch 3157, Loss: 0.20531224459409714, Final Batch Loss: 0.10359354317188263\n",
      "Epoch 3158, Loss: 0.2045678272843361, Final Batch Loss: 0.09708519279956818\n",
      "Epoch 3159, Loss: 0.23653192818164825, Final Batch Loss: 0.09763644635677338\n",
      "Epoch 3160, Loss: 0.251391239464283, Final Batch Loss: 0.13435803353786469\n",
      "Epoch 3161, Loss: 0.25483914464712143, Final Batch Loss: 0.17161399126052856\n",
      "Epoch 3162, Loss: 0.30756471306085587, Final Batch Loss: 0.2068982571363449\n",
      "Epoch 3163, Loss: 0.2583504393696785, Final Batch Loss: 0.16151247918605804\n",
      "Epoch 3164, Loss: 0.2232694998383522, Final Batch Loss: 0.08185755461454391\n",
      "Epoch 3165, Loss: 0.2590094730257988, Final Batch Loss: 0.14932988584041595\n",
      "Epoch 3166, Loss: 0.28642672300338745, Final Batch Loss: 0.1306450515985489\n",
      "Epoch 3167, Loss: 0.2493288442492485, Final Batch Loss: 0.15052489936351776\n",
      "Epoch 3168, Loss: 0.25648852437734604, Final Batch Loss: 0.13176031410694122\n",
      "Epoch 3169, Loss: 0.28231048583984375, Final Batch Loss: 0.1499965339899063\n",
      "Epoch 3170, Loss: 0.24070686101913452, Final Batch Loss: 0.11689254641532898\n",
      "Epoch 3171, Loss: 0.2392132803797722, Final Batch Loss: 0.1222417950630188\n",
      "Epoch 3172, Loss: 0.19380833208560944, Final Batch Loss: 0.09015402942895889\n",
      "Epoch 3173, Loss: 0.2736576646566391, Final Batch Loss: 0.14158634841442108\n",
      "Epoch 3174, Loss: 0.26429908722639084, Final Batch Loss: 0.09858227521181107\n",
      "Epoch 3175, Loss: 0.23070217669010162, Final Batch Loss: 0.12938056886196136\n",
      "Epoch 3176, Loss: 0.223528154194355, Final Batch Loss: 0.11449270695447922\n",
      "Epoch 3177, Loss: 0.21969328075647354, Final Batch Loss: 0.08849536627531052\n",
      "Epoch 3178, Loss: 0.2004944458603859, Final Batch Loss: 0.0961105227470398\n",
      "Epoch 3179, Loss: 0.17499719560146332, Final Batch Loss: 0.08610285073518753\n",
      "Epoch 3180, Loss: 0.27204304933547974, Final Batch Loss: 0.1447940617799759\n",
      "Epoch 3181, Loss: 0.2407473847270012, Final Batch Loss: 0.13833220303058624\n",
      "Epoch 3182, Loss: 0.29747529327869415, Final Batch Loss: 0.14689888060092926\n",
      "Epoch 3183, Loss: 0.24372165650129318, Final Batch Loss: 0.1496969312429428\n",
      "Epoch 3184, Loss: 0.22594306617975235, Final Batch Loss: 0.09089667350053787\n",
      "Epoch 3185, Loss: 0.21719689667224884, Final Batch Loss: 0.0761142373085022\n",
      "Epoch 3186, Loss: 0.24953543394804, Final Batch Loss: 0.10919369012117386\n",
      "Epoch 3187, Loss: 0.2712804675102234, Final Batch Loss: 0.16886480152606964\n",
      "Epoch 3188, Loss: 0.28763098269701004, Final Batch Loss: 0.12214133888483047\n",
      "Epoch 3189, Loss: 0.21895167976617813, Final Batch Loss: 0.08862421661615372\n",
      "Epoch 3190, Loss: 0.24021125584840775, Final Batch Loss: 0.10851957648992538\n",
      "Epoch 3191, Loss: 0.28357715904712677, Final Batch Loss: 0.1383274793624878\n",
      "Epoch 3192, Loss: 0.2581195905804634, Final Batch Loss: 0.14277973771095276\n",
      "Epoch 3193, Loss: 0.22395998984575272, Final Batch Loss: 0.1565122902393341\n",
      "Epoch 3194, Loss: 0.22455764561891556, Final Batch Loss: 0.11096290498971939\n",
      "Epoch 3195, Loss: 0.2543819472193718, Final Batch Loss: 0.12251793593168259\n",
      "Epoch 3196, Loss: 0.22725745290517807, Final Batch Loss: 0.1073412299156189\n",
      "Epoch 3197, Loss: 0.255218006670475, Final Batch Loss: 0.1341756135225296\n",
      "Epoch 3198, Loss: 0.26225267350673676, Final Batch Loss: 0.12344607710838318\n",
      "Epoch 3199, Loss: 0.24002204090356827, Final Batch Loss: 0.10754520446062088\n",
      "Epoch 3200, Loss: 0.22593434900045395, Final Batch Loss: 0.09827352315187454\n",
      "Epoch 3201, Loss: 0.19110723584890366, Final Batch Loss: 0.09100907295942307\n",
      "Epoch 3202, Loss: 0.24549706280231476, Final Batch Loss: 0.10620005428791046\n",
      "Epoch 3203, Loss: 0.2450212761759758, Final Batch Loss: 0.15037740767002106\n",
      "Epoch 3204, Loss: 0.18236983567476273, Final Batch Loss: 0.08945803344249725\n",
      "Epoch 3205, Loss: 0.2991822510957718, Final Batch Loss: 0.15438179671764374\n",
      "Epoch 3206, Loss: 0.2156359776854515, Final Batch Loss: 0.10535886883735657\n",
      "Epoch 3207, Loss: 0.2358507141470909, Final Batch Loss: 0.08701954036951065\n",
      "Epoch 3208, Loss: 0.23354514688253403, Final Batch Loss: 0.11478552967309952\n",
      "Epoch 3209, Loss: 0.17916569113731384, Final Batch Loss: 0.08496079593896866\n",
      "Epoch 3210, Loss: 0.29080040752887726, Final Batch Loss: 0.15955953299999237\n",
      "Epoch 3211, Loss: 0.199598491191864, Final Batch Loss: 0.09838941693305969\n",
      "Epoch 3212, Loss: 0.22185275703668594, Final Batch Loss: 0.13291122019290924\n",
      "Epoch 3213, Loss: 0.18295813351869583, Final Batch Loss: 0.07773229479789734\n",
      "Epoch 3214, Loss: 0.25671155750751495, Final Batch Loss: 0.11717180907726288\n",
      "Epoch 3215, Loss: 0.1932763308286667, Final Batch Loss: 0.06692072749137878\n",
      "Epoch 3216, Loss: 0.32737548649311066, Final Batch Loss: 0.15680572390556335\n",
      "Epoch 3217, Loss: 0.24926410615444183, Final Batch Loss: 0.11620137095451355\n",
      "Epoch 3218, Loss: 0.20333191007375717, Final Batch Loss: 0.08795008808374405\n",
      "Epoch 3219, Loss: 0.1917519047856331, Final Batch Loss: 0.10044596344232559\n",
      "Epoch 3220, Loss: 0.2296510636806488, Final Batch Loss: 0.1253122091293335\n",
      "Epoch 3221, Loss: 0.2110358029603958, Final Batch Loss: 0.10394737869501114\n",
      "Epoch 3222, Loss: 0.2389289140701294, Final Batch Loss: 0.11357885599136353\n",
      "Epoch 3223, Loss: 0.2063671424984932, Final Batch Loss: 0.10787465423345566\n",
      "Epoch 3224, Loss: 0.18618260324001312, Final Batch Loss: 0.1191992461681366\n",
      "Epoch 3225, Loss: 0.2709740847349167, Final Batch Loss: 0.12446649372577667\n",
      "Epoch 3226, Loss: 0.2500147223472595, Final Batch Loss: 0.12904316186904907\n",
      "Epoch 3227, Loss: 0.24069547653198242, Final Batch Loss: 0.12954029440879822\n",
      "Epoch 3228, Loss: 0.22801777720451355, Final Batch Loss: 0.0866909921169281\n",
      "Epoch 3229, Loss: 0.20643173903226852, Final Batch Loss: 0.10893717408180237\n",
      "Epoch 3230, Loss: 0.22356721013784409, Final Batch Loss: 0.09658915549516678\n",
      "Epoch 3231, Loss: 0.22857412695884705, Final Batch Loss: 0.12309133261442184\n",
      "Epoch 3232, Loss: 0.2062031477689743, Final Batch Loss: 0.0954941064119339\n",
      "Epoch 3233, Loss: 0.2552875131368637, Final Batch Loss: 0.10511907935142517\n",
      "Epoch 3234, Loss: 0.22769442945718765, Final Batch Loss: 0.10692325234413147\n",
      "Epoch 3235, Loss: 0.23986930400133133, Final Batch Loss: 0.11895996332168579\n",
      "Epoch 3236, Loss: 0.2314193993806839, Final Batch Loss: 0.10935510694980621\n",
      "Epoch 3237, Loss: 0.23123803734779358, Final Batch Loss: 0.13115106523036957\n",
      "Epoch 3238, Loss: 0.27772746980190277, Final Batch Loss: 0.14618468284606934\n",
      "Epoch 3239, Loss: 0.25679948180913925, Final Batch Loss: 0.11407072097063065\n",
      "Epoch 3240, Loss: 0.30147936940193176, Final Batch Loss: 0.13199320435523987\n",
      "Epoch 3241, Loss: 0.21281731128692627, Final Batch Loss: 0.09710242599248886\n",
      "Epoch 3242, Loss: 0.2383541241288185, Final Batch Loss: 0.10385329276323318\n",
      "Epoch 3243, Loss: 0.22706659883260727, Final Batch Loss: 0.11471526324748993\n",
      "Epoch 3244, Loss: 0.26513325423002243, Final Batch Loss: 0.12072090059518814\n",
      "Epoch 3245, Loss: 0.26195673644542694, Final Batch Loss: 0.1178114265203476\n",
      "Epoch 3246, Loss: 0.2525191828608513, Final Batch Loss: 0.12391787022352219\n",
      "Epoch 3247, Loss: 0.27207815647125244, Final Batch Loss: 0.15665386617183685\n",
      "Epoch 3248, Loss: 0.25662849843502045, Final Batch Loss: 0.15724262595176697\n",
      "Epoch 3249, Loss: 0.27521007508039474, Final Batch Loss: 0.1535692811012268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3250, Loss: 0.20236290991306305, Final Batch Loss: 0.10052436590194702\n",
      "Epoch 3251, Loss: 0.18508126586675644, Final Batch Loss: 0.09053251147270203\n",
      "Epoch 3252, Loss: 0.2424556016921997, Final Batch Loss: 0.10744135081768036\n",
      "Epoch 3253, Loss: 0.23383646458387375, Final Batch Loss: 0.06699300557374954\n",
      "Epoch 3254, Loss: 0.22137143462896347, Final Batch Loss: 0.09179873019456863\n",
      "Epoch 3255, Loss: 0.22791509330272675, Final Batch Loss: 0.11630207300186157\n",
      "Epoch 3256, Loss: 0.23354802280664444, Final Batch Loss: 0.10000339895486832\n",
      "Epoch 3257, Loss: 0.2775302082300186, Final Batch Loss: 0.09841981530189514\n",
      "Epoch 3258, Loss: 0.22235605120658875, Final Batch Loss: 0.12480638176202774\n",
      "Epoch 3259, Loss: 0.21337151527404785, Final Batch Loss: 0.11698401719331741\n",
      "Epoch 3260, Loss: 0.21403437107801437, Final Batch Loss: 0.11320208758115768\n",
      "Epoch 3261, Loss: 0.2443612515926361, Final Batch Loss: 0.09725040197372437\n",
      "Epoch 3262, Loss: 0.2674379199743271, Final Batch Loss: 0.15160097181797028\n",
      "Epoch 3263, Loss: 0.2267593815922737, Final Batch Loss: 0.12090671062469482\n",
      "Epoch 3264, Loss: 0.20418740808963776, Final Batch Loss: 0.09033843129873276\n",
      "Epoch 3265, Loss: 0.20152081549167633, Final Batch Loss: 0.0992775559425354\n",
      "Epoch 3266, Loss: 0.2103256657719612, Final Batch Loss: 0.08947458118200302\n",
      "Epoch 3267, Loss: 0.2193361297249794, Final Batch Loss: 0.12686149775981903\n",
      "Epoch 3268, Loss: 0.17523083090782166, Final Batch Loss: 0.06567798554897308\n",
      "Epoch 3269, Loss: 0.24383287131786346, Final Batch Loss: 0.08350542187690735\n",
      "Epoch 3270, Loss: 0.2693912386894226, Final Batch Loss: 0.16058926284313202\n",
      "Epoch 3271, Loss: 0.23899083584547043, Final Batch Loss: 0.12302449345588684\n",
      "Epoch 3272, Loss: 0.2120763510465622, Final Batch Loss: 0.10516977310180664\n",
      "Epoch 3273, Loss: 0.2554985135793686, Final Batch Loss: 0.11909684538841248\n",
      "Epoch 3274, Loss: 0.19573653489351273, Final Batch Loss: 0.08128362149000168\n",
      "Epoch 3275, Loss: 0.2822355628013611, Final Batch Loss: 0.16651804745197296\n",
      "Epoch 3276, Loss: 0.24459154903888702, Final Batch Loss: 0.10794122517108917\n",
      "Epoch 3277, Loss: 0.2497168481349945, Final Batch Loss: 0.13646073639392853\n",
      "Epoch 3278, Loss: 0.29293788969516754, Final Batch Loss: 0.16776564717292786\n",
      "Epoch 3279, Loss: 0.22949706763029099, Final Batch Loss: 0.13094399869441986\n",
      "Epoch 3280, Loss: 0.18793370574712753, Final Batch Loss: 0.10004408657550812\n",
      "Epoch 3281, Loss: 0.22914566844701767, Final Batch Loss: 0.1244082972407341\n",
      "Epoch 3282, Loss: 0.24181485176086426, Final Batch Loss: 0.11047092080116272\n",
      "Epoch 3283, Loss: 0.2582535296678543, Final Batch Loss: 0.1313856989145279\n",
      "Epoch 3284, Loss: 0.2567816376686096, Final Batch Loss: 0.14193999767303467\n",
      "Epoch 3285, Loss: 0.2334694042801857, Final Batch Loss: 0.10770734399557114\n",
      "Epoch 3286, Loss: 0.18495288491249084, Final Batch Loss: 0.09060509502887726\n",
      "Epoch 3287, Loss: 0.22516357898712158, Final Batch Loss: 0.10822570323944092\n",
      "Epoch 3288, Loss: 0.22494830936193466, Final Batch Loss: 0.10796026885509491\n",
      "Epoch 3289, Loss: 0.19511763006448746, Final Batch Loss: 0.10036571323871613\n",
      "Epoch 3290, Loss: 0.2917722016572952, Final Batch Loss: 0.12689432501792908\n",
      "Epoch 3291, Loss: 0.23679911345243454, Final Batch Loss: 0.125276118516922\n",
      "Epoch 3292, Loss: 0.17869044095277786, Final Batch Loss: 0.10064893215894699\n",
      "Epoch 3293, Loss: 0.2113051787018776, Final Batch Loss: 0.09481728821992874\n",
      "Epoch 3294, Loss: 0.21446673572063446, Final Batch Loss: 0.10245772451162338\n",
      "Epoch 3295, Loss: 0.21503376960754395, Final Batch Loss: 0.07694059610366821\n",
      "Epoch 3296, Loss: 0.23815350979566574, Final Batch Loss: 0.15508992969989777\n",
      "Epoch 3297, Loss: 0.18106167018413544, Final Batch Loss: 0.10511694103479385\n",
      "Epoch 3298, Loss: 0.24862036108970642, Final Batch Loss: 0.16353939473628998\n",
      "Epoch 3299, Loss: 0.2205035611987114, Final Batch Loss: 0.09464988857507706\n",
      "Epoch 3300, Loss: 0.22974557429552078, Final Batch Loss: 0.12275562435388565\n",
      "Epoch 3301, Loss: 0.186250239610672, Final Batch Loss: 0.07861927151679993\n",
      "Epoch 3302, Loss: 0.19282463937997818, Final Batch Loss: 0.0893232524394989\n",
      "Epoch 3303, Loss: 0.28010857850313187, Final Batch Loss: 0.09865479916334152\n",
      "Epoch 3304, Loss: 0.26725926995277405, Final Batch Loss: 0.14100022614002228\n",
      "Epoch 3305, Loss: 0.2404918372631073, Final Batch Loss: 0.13134928047657013\n",
      "Epoch 3306, Loss: 0.2519127056002617, Final Batch Loss: 0.11080684512853622\n",
      "Epoch 3307, Loss: 0.19236887246370316, Final Batch Loss: 0.06855873763561249\n",
      "Epoch 3308, Loss: 0.23662441223859787, Final Batch Loss: 0.12340240180492401\n",
      "Epoch 3309, Loss: 0.20927035063505173, Final Batch Loss: 0.06887654215097427\n",
      "Epoch 3310, Loss: 0.18617011606693268, Final Batch Loss: 0.07572951167821884\n",
      "Epoch 3311, Loss: 0.2451896369457245, Final Batch Loss: 0.1325862556695938\n",
      "Epoch 3312, Loss: 0.20928820222616196, Final Batch Loss: 0.1137976199388504\n",
      "Epoch 3313, Loss: 0.23322732746601105, Final Batch Loss: 0.13208948075771332\n",
      "Epoch 3314, Loss: 0.2504749298095703, Final Batch Loss: 0.15687735378742218\n",
      "Epoch 3315, Loss: 0.3118352144956589, Final Batch Loss: 0.11341764032840729\n",
      "Epoch 3316, Loss: 0.22316408902406693, Final Batch Loss: 0.12071800231933594\n",
      "Epoch 3317, Loss: 0.24887388944625854, Final Batch Loss: 0.13263872265815735\n",
      "Epoch 3318, Loss: 0.1874045729637146, Final Batch Loss: 0.09225218743085861\n",
      "Epoch 3319, Loss: 0.18379652500152588, Final Batch Loss: 0.08448876440525055\n",
      "Epoch 3320, Loss: 0.2379251793026924, Final Batch Loss: 0.1130988821387291\n",
      "Epoch 3321, Loss: 0.17251735925674438, Final Batch Loss: 0.09378193318843842\n",
      "Epoch 3322, Loss: 0.21782677620649338, Final Batch Loss: 0.10503464937210083\n",
      "Epoch 3323, Loss: 0.2121938318014145, Final Batch Loss: 0.12058593332767487\n",
      "Epoch 3324, Loss: 0.256099171936512, Final Batch Loss: 0.1650901734828949\n",
      "Epoch 3325, Loss: 0.2851303964853287, Final Batch Loss: 0.13375413417816162\n",
      "Epoch 3326, Loss: 0.2381310909986496, Final Batch Loss: 0.1034785658121109\n",
      "Epoch 3327, Loss: 0.20229749381542206, Final Batch Loss: 0.11626598984003067\n",
      "Epoch 3328, Loss: 0.2525235041975975, Final Batch Loss: 0.13465113937854767\n",
      "Epoch 3329, Loss: 0.2177221029996872, Final Batch Loss: 0.13277506828308105\n",
      "Epoch 3330, Loss: 0.21125076711177826, Final Batch Loss: 0.10884912312030792\n",
      "Epoch 3331, Loss: 0.23283304274082184, Final Batch Loss: 0.1315171867609024\n",
      "Epoch 3332, Loss: 0.21705276519060135, Final Batch Loss: 0.13358068466186523\n",
      "Epoch 3333, Loss: 0.2553843632340431, Final Batch Loss: 0.11778641492128372\n",
      "Epoch 3334, Loss: 0.21709215641021729, Final Batch Loss: 0.09028281271457672\n",
      "Epoch 3335, Loss: 0.2976515144109726, Final Batch Loss: 0.13655345141887665\n",
      "Epoch 3336, Loss: 0.25222646445035934, Final Batch Loss: 0.14473497867584229\n",
      "Epoch 3337, Loss: 0.23596090823411942, Final Batch Loss: 0.11818139255046844\n",
      "Epoch 3338, Loss: 0.21925543248653412, Final Batch Loss: 0.12498199939727783\n",
      "Epoch 3339, Loss: 0.25419235974550247, Final Batch Loss: 0.16032086312770844\n",
      "Epoch 3340, Loss: 0.2329982966184616, Final Batch Loss: 0.14046235382556915\n",
      "Epoch 3341, Loss: 0.16620060801506042, Final Batch Loss: 0.08124098926782608\n",
      "Epoch 3342, Loss: 0.18285343050956726, Final Batch Loss: 0.10462154448032379\n",
      "Epoch 3343, Loss: 0.23583324253559113, Final Batch Loss: 0.11151458323001862\n",
      "Epoch 3344, Loss: 0.21208106726408005, Final Batch Loss: 0.10111916810274124\n",
      "Epoch 3345, Loss: 0.21033554524183273, Final Batch Loss: 0.08421964198350906\n",
      "Epoch 3346, Loss: 0.19167517125606537, Final Batch Loss: 0.07522761076688766\n",
      "Epoch 3347, Loss: 0.23490475863218307, Final Batch Loss: 0.11700277775526047\n",
      "Epoch 3348, Loss: 0.21082130074501038, Final Batch Loss: 0.10338094085454941\n",
      "Epoch 3349, Loss: 0.23636215180158615, Final Batch Loss: 0.0900919958949089\n",
      "Epoch 3350, Loss: 0.22082702070474625, Final Batch Loss: 0.12504473328590393\n",
      "Epoch 3351, Loss: 0.2303621768951416, Final Batch Loss: 0.14448252320289612\n",
      "Epoch 3352, Loss: 0.23788520693778992, Final Batch Loss: 0.13568951189517975\n",
      "Epoch 3353, Loss: 0.2302444726228714, Final Batch Loss: 0.14020396769046783\n",
      "Epoch 3354, Loss: 0.2096499353647232, Final Batch Loss: 0.11148545891046524\n",
      "Epoch 3355, Loss: 0.21979516744613647, Final Batch Loss: 0.09076711535453796\n",
      "Epoch 3356, Loss: 0.15279094129800797, Final Batch Loss: 0.06585893034934998\n",
      "Epoch 3357, Loss: 0.2738427147269249, Final Batch Loss: 0.11317745596170425\n",
      "Epoch 3358, Loss: 0.21669792383909225, Final Batch Loss: 0.12189905345439911\n",
      "Epoch 3359, Loss: 0.2508154436945915, Final Batch Loss: 0.13765046000480652\n",
      "Epoch 3360, Loss: 0.22050169855356216, Final Batch Loss: 0.098647341132164\n",
      "Epoch 3361, Loss: 0.17772893607616425, Final Batch Loss: 0.08309564739465714\n",
      "Epoch 3362, Loss: 0.21468167006969452, Final Batch Loss: 0.10560032725334167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3363, Loss: 0.20526525378227234, Final Batch Loss: 0.08744468539953232\n",
      "Epoch 3364, Loss: 0.2975437119603157, Final Batch Loss: 0.19236916303634644\n",
      "Epoch 3365, Loss: 0.2798569053411484, Final Batch Loss: 0.11027009785175323\n",
      "Epoch 3366, Loss: 0.22525806725025177, Final Batch Loss: 0.12296514958143234\n",
      "Epoch 3367, Loss: 0.24065414816141129, Final Batch Loss: 0.13166053593158722\n",
      "Epoch 3368, Loss: 0.2162443995475769, Final Batch Loss: 0.11194241046905518\n",
      "Epoch 3369, Loss: 0.24679017812013626, Final Batch Loss: 0.1271936446428299\n",
      "Epoch 3370, Loss: 0.24417119473218918, Final Batch Loss: 0.1383381485939026\n",
      "Epoch 3371, Loss: 0.264143630862236, Final Batch Loss: 0.14385396242141724\n",
      "Epoch 3372, Loss: 0.287056639790535, Final Batch Loss: 0.12443973124027252\n",
      "Epoch 3373, Loss: 0.26306329667568207, Final Batch Loss: 0.14549648761749268\n",
      "Epoch 3374, Loss: 0.2090723216533661, Final Batch Loss: 0.1018805205821991\n",
      "Epoch 3375, Loss: 0.26320379972457886, Final Batch Loss: 0.12030045688152313\n",
      "Epoch 3376, Loss: 0.264590248465538, Final Batch Loss: 0.13273362815380096\n",
      "Epoch 3377, Loss: 0.26722995191812515, Final Batch Loss: 0.15124282240867615\n",
      "Epoch 3378, Loss: 0.2032763808965683, Final Batch Loss: 0.11972789466381073\n",
      "Epoch 3379, Loss: 0.20792336761951447, Final Batch Loss: 0.09793229401111603\n",
      "Epoch 3380, Loss: 0.19558291137218475, Final Batch Loss: 0.09709785133600235\n",
      "Epoch 3381, Loss: 0.26688502728939056, Final Batch Loss: 0.13695687055587769\n",
      "Epoch 3382, Loss: 0.22314849495887756, Final Batch Loss: 0.13224157691001892\n",
      "Epoch 3383, Loss: 0.2515897899866104, Final Batch Loss: 0.1420280486345291\n",
      "Epoch 3384, Loss: 0.1706806644797325, Final Batch Loss: 0.06330429762601852\n",
      "Epoch 3385, Loss: 0.2766069173812866, Final Batch Loss: 0.1006288081407547\n",
      "Epoch 3386, Loss: 0.1992601528763771, Final Batch Loss: 0.08257089555263519\n",
      "Epoch 3387, Loss: 0.270698681473732, Final Batch Loss: 0.1281176209449768\n",
      "Epoch 3388, Loss: 0.26132597029209137, Final Batch Loss: 0.1456952691078186\n",
      "Epoch 3389, Loss: 0.21370062232017517, Final Batch Loss: 0.1200302243232727\n",
      "Epoch 3390, Loss: 0.26847711205482483, Final Batch Loss: 0.1387455314397812\n",
      "Epoch 3391, Loss: 0.24864185601472855, Final Batch Loss: 0.09698953479528427\n",
      "Epoch 3392, Loss: 0.23724620044231415, Final Batch Loss: 0.1028469055891037\n",
      "Epoch 3393, Loss: 0.21437819302082062, Final Batch Loss: 0.11238673329353333\n",
      "Epoch 3394, Loss: 0.21652928739786148, Final Batch Loss: 0.11921717971563339\n",
      "Epoch 3395, Loss: 0.24278363585472107, Final Batch Loss: 0.14220237731933594\n",
      "Epoch 3396, Loss: 0.22720932960510254, Final Batch Loss: 0.08504728972911835\n",
      "Epoch 3397, Loss: 0.20437611639499664, Final Batch Loss: 0.08155427873134613\n",
      "Epoch 3398, Loss: 0.28400036692619324, Final Batch Loss: 0.1419225037097931\n",
      "Epoch 3399, Loss: 0.20928020775318146, Final Batch Loss: 0.09218978136777878\n",
      "Epoch 3400, Loss: 0.19229726493358612, Final Batch Loss: 0.07289973646402359\n",
      "Epoch 3401, Loss: 0.22656653076410294, Final Batch Loss: 0.09039182215929031\n",
      "Epoch 3402, Loss: 0.23396950960159302, Final Batch Loss: 0.1425113081932068\n",
      "Epoch 3403, Loss: 0.2306964471936226, Final Batch Loss: 0.1552700400352478\n",
      "Epoch 3404, Loss: 0.1729426234960556, Final Batch Loss: 0.08806390315294266\n",
      "Epoch 3405, Loss: 0.22418509423732758, Final Batch Loss: 0.1272910237312317\n",
      "Epoch 3406, Loss: 0.17620856314897537, Final Batch Loss: 0.06627929955720901\n",
      "Epoch 3407, Loss: 0.27510929107666016, Final Batch Loss: 0.12550680339336395\n",
      "Epoch 3408, Loss: 0.35019903630018234, Final Batch Loss: 0.11773275583982468\n",
      "Epoch 3409, Loss: 0.20620492100715637, Final Batch Loss: 0.08607959002256393\n",
      "Epoch 3410, Loss: 0.23291446268558502, Final Batch Loss: 0.08687025308609009\n",
      "Epoch 3411, Loss: 0.2310548573732376, Final Batch Loss: 0.11817457526922226\n",
      "Epoch 3412, Loss: 0.21947389096021652, Final Batch Loss: 0.10537827759981155\n",
      "Epoch 3413, Loss: 0.26509731262922287, Final Batch Loss: 0.11707756668329239\n",
      "Epoch 3414, Loss: 0.26745662093162537, Final Batch Loss: 0.13788831233978271\n",
      "Epoch 3415, Loss: 0.23493555188179016, Final Batch Loss: 0.11725745350122452\n",
      "Epoch 3416, Loss: 0.19851219654083252, Final Batch Loss: 0.10763654112815857\n",
      "Epoch 3417, Loss: 0.22641118615865707, Final Batch Loss: 0.10434231162071228\n",
      "Epoch 3418, Loss: 0.20423106849193573, Final Batch Loss: 0.10445431619882584\n",
      "Epoch 3419, Loss: 0.22651240974664688, Final Batch Loss: 0.10096173733472824\n",
      "Epoch 3420, Loss: 0.21885841339826584, Final Batch Loss: 0.08659373968839645\n",
      "Epoch 3421, Loss: 0.21988967806100845, Final Batch Loss: 0.11180834472179413\n",
      "Epoch 3422, Loss: 0.24746371805667877, Final Batch Loss: 0.0942121297121048\n",
      "Epoch 3423, Loss: 0.23061316460371017, Final Batch Loss: 0.09911184757947922\n",
      "Epoch 3424, Loss: 0.218797467648983, Final Batch Loss: 0.10489783436059952\n",
      "Epoch 3425, Loss: 0.2518341392278671, Final Batch Loss: 0.12167912721633911\n",
      "Epoch 3426, Loss: 0.24574221670627594, Final Batch Loss: 0.10743512213230133\n",
      "Epoch 3427, Loss: 0.20439990609884262, Final Batch Loss: 0.08510887622833252\n",
      "Epoch 3428, Loss: 0.20361940562725067, Final Batch Loss: 0.077829509973526\n",
      "Epoch 3429, Loss: 0.23737281560897827, Final Batch Loss: 0.08359017968177795\n",
      "Epoch 3430, Loss: 0.21804285794496536, Final Batch Loss: 0.09629123657941818\n",
      "Epoch 3431, Loss: 0.22311878949403763, Final Batch Loss: 0.1308387815952301\n",
      "Epoch 3432, Loss: 0.2308894544839859, Final Batch Loss: 0.14320704340934753\n",
      "Epoch 3433, Loss: 0.2075129821896553, Final Batch Loss: 0.11837749183177948\n",
      "Epoch 3434, Loss: 0.19985148310661316, Final Batch Loss: 0.08382566273212433\n",
      "Epoch 3435, Loss: 0.2421780750155449, Final Batch Loss: 0.15298818051815033\n",
      "Epoch 3436, Loss: 0.23155218362808228, Final Batch Loss: 0.12630116939544678\n",
      "Epoch 3437, Loss: 0.2501291483640671, Final Batch Loss: 0.14297834038734436\n",
      "Epoch 3438, Loss: 0.25389161705970764, Final Batch Loss: 0.12401728332042694\n",
      "Epoch 3439, Loss: 0.18436424434185028, Final Batch Loss: 0.09831687808036804\n",
      "Epoch 3440, Loss: 0.17970644682645798, Final Batch Loss: 0.09892750531435013\n",
      "Epoch 3441, Loss: 0.22533012181520462, Final Batch Loss: 0.1319321095943451\n",
      "Epoch 3442, Loss: 0.23297975212335587, Final Batch Loss: 0.14097730815410614\n",
      "Epoch 3443, Loss: 0.21700768917798996, Final Batch Loss: 0.07130426913499832\n",
      "Epoch 3444, Loss: 0.24472253769636154, Final Batch Loss: 0.14786411821842194\n",
      "Epoch 3445, Loss: 0.2644936516880989, Final Batch Loss: 0.11827022582292557\n",
      "Epoch 3446, Loss: 0.28739675134420395, Final Batch Loss: 0.17831051349639893\n",
      "Epoch 3447, Loss: 0.2913014739751816, Final Batch Loss: 0.15825100243091583\n",
      "Epoch 3448, Loss: 0.27170202136039734, Final Batch Loss: 0.1643276959657669\n",
      "Epoch 3449, Loss: 0.21129073947668076, Final Batch Loss: 0.09841318428516388\n",
      "Epoch 3450, Loss: 0.25472673028707504, Final Batch Loss: 0.11971383541822433\n",
      "Epoch 3451, Loss: 0.2794879674911499, Final Batch Loss: 0.15405456721782684\n",
      "Epoch 3452, Loss: 0.2890254035592079, Final Batch Loss: 0.19183382391929626\n",
      "Epoch 3453, Loss: 0.2070392370223999, Final Batch Loss: 0.08488555997610092\n",
      "Epoch 3454, Loss: 0.20921877771615982, Final Batch Loss: 0.1008472740650177\n",
      "Epoch 3455, Loss: 0.192767933011055, Final Batch Loss: 0.10882778465747833\n",
      "Epoch 3456, Loss: 0.20662947744131088, Final Batch Loss: 0.11293334513902664\n",
      "Epoch 3457, Loss: 0.2580963149666786, Final Batch Loss: 0.10846949368715286\n",
      "Epoch 3458, Loss: 0.21986040472984314, Final Batch Loss: 0.11732977628707886\n",
      "Epoch 3459, Loss: 0.18412374705076218, Final Batch Loss: 0.08717866241931915\n",
      "Epoch 3460, Loss: 0.23931040614843369, Final Batch Loss: 0.11385313421487808\n",
      "Epoch 3461, Loss: 0.17170938849449158, Final Batch Loss: 0.09388077259063721\n",
      "Epoch 3462, Loss: 0.26681073755025864, Final Batch Loss: 0.12144052237272263\n",
      "Epoch 3463, Loss: 0.2541923299431801, Final Batch Loss: 0.13657434284687042\n",
      "Epoch 3464, Loss: 0.24992380291223526, Final Batch Loss: 0.14163772761821747\n",
      "Epoch 3465, Loss: 0.26640912890434265, Final Batch Loss: 0.1251451075077057\n",
      "Epoch 3466, Loss: 0.23219002783298492, Final Batch Loss: 0.12126179784536362\n",
      "Epoch 3467, Loss: 0.2414439544081688, Final Batch Loss: 0.1331302672624588\n",
      "Epoch 3468, Loss: 0.25905250012874603, Final Batch Loss: 0.1329994946718216\n",
      "Epoch 3469, Loss: 0.20873983949422836, Final Batch Loss: 0.11498228460550308\n",
      "Epoch 3470, Loss: 0.30678126215934753, Final Batch Loss: 0.20378990471363068\n",
      "Epoch 3471, Loss: 0.1880491077899933, Final Batch Loss: 0.09092690050601959\n",
      "Epoch 3472, Loss: 0.20955341309309006, Final Batch Loss: 0.10104718059301376\n",
      "Epoch 3473, Loss: 0.2763778939843178, Final Batch Loss: 0.16150586307048798\n",
      "Epoch 3474, Loss: 0.2392512410879135, Final Batch Loss: 0.1479955017566681\n",
      "Epoch 3475, Loss: 0.20461803674697876, Final Batch Loss: 0.11196176707744598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3476, Loss: 0.215321883559227, Final Batch Loss: 0.1210714653134346\n",
      "Epoch 3477, Loss: 0.2564636543393135, Final Batch Loss: 0.14498312771320343\n",
      "Epoch 3478, Loss: 0.22634176909923553, Final Batch Loss: 0.09826509654521942\n",
      "Epoch 3479, Loss: 0.2627779245376587, Final Batch Loss: 0.11581757664680481\n",
      "Epoch 3480, Loss: 0.2566814497113228, Final Batch Loss: 0.12077780812978745\n",
      "Epoch 3481, Loss: 0.27078964561223984, Final Batch Loss: 0.1644687056541443\n",
      "Epoch 3482, Loss: 0.18961891531944275, Final Batch Loss: 0.09616480767726898\n",
      "Epoch 3483, Loss: 0.2172459214925766, Final Batch Loss: 0.0719815194606781\n",
      "Epoch 3484, Loss: 0.23943208158016205, Final Batch Loss: 0.11054527759552002\n",
      "Epoch 3485, Loss: 0.2201155200600624, Final Batch Loss: 0.12244484573602676\n",
      "Epoch 3486, Loss: 0.2076953500509262, Final Batch Loss: 0.08746041357517242\n",
      "Epoch 3487, Loss: 0.25947244465351105, Final Batch Loss: 0.1441468894481659\n",
      "Epoch 3488, Loss: 0.21604245901107788, Final Batch Loss: 0.08997085690498352\n",
      "Epoch 3489, Loss: 0.194233238697052, Final Batch Loss: 0.1107666864991188\n",
      "Epoch 3490, Loss: 0.21480649709701538, Final Batch Loss: 0.12425875663757324\n",
      "Epoch 3491, Loss: 0.2334558591246605, Final Batch Loss: 0.09506284445524216\n",
      "Epoch 3492, Loss: 0.20645124465227127, Final Batch Loss: 0.11324692517518997\n",
      "Epoch 3493, Loss: 0.22924090176820755, Final Batch Loss: 0.11439406871795654\n",
      "Epoch 3494, Loss: 0.22771301120519638, Final Batch Loss: 0.12530706822872162\n",
      "Epoch 3495, Loss: 0.1938985213637352, Final Batch Loss: 0.09341797232627869\n",
      "Epoch 3496, Loss: 0.2536180540919304, Final Batch Loss: 0.1387527883052826\n",
      "Epoch 3497, Loss: 0.23093180358409882, Final Batch Loss: 0.08713585138320923\n",
      "Epoch 3498, Loss: 0.22185616195201874, Final Batch Loss: 0.11495327949523926\n",
      "Epoch 3499, Loss: 0.18025580048561096, Final Batch Loss: 0.07684126496315002\n",
      "Epoch 3500, Loss: 0.18035531789064407, Final Batch Loss: 0.10122273862361908\n",
      "Epoch 3501, Loss: 0.17741046845912933, Final Batch Loss: 0.099248006939888\n",
      "Epoch 3502, Loss: 0.22379959374666214, Final Batch Loss: 0.10249567776918411\n",
      "Epoch 3503, Loss: 0.26363711059093475, Final Batch Loss: 0.14232851564884186\n",
      "Epoch 3504, Loss: 0.2784395217895508, Final Batch Loss: 0.1825716495513916\n",
      "Epoch 3505, Loss: 0.17762373387813568, Final Batch Loss: 0.08486692607402802\n",
      "Epoch 3506, Loss: 0.2697467505931854, Final Batch Loss: 0.14538486301898956\n",
      "Epoch 3507, Loss: 0.21196069568395615, Final Batch Loss: 0.07161661237478256\n",
      "Epoch 3508, Loss: 0.23267902433872223, Final Batch Loss: 0.12551924586296082\n",
      "Epoch 3509, Loss: 0.2375231608748436, Final Batch Loss: 0.10947973281145096\n",
      "Epoch 3510, Loss: 0.21788166463375092, Final Batch Loss: 0.1049758568406105\n",
      "Epoch 3511, Loss: 0.23178621381521225, Final Batch Loss: 0.10770773142576218\n",
      "Epoch 3512, Loss: 0.20271828025579453, Final Batch Loss: 0.10870958864688873\n",
      "Epoch 3513, Loss: 0.20690693706274033, Final Batch Loss: 0.09280585497617722\n",
      "Epoch 3514, Loss: 0.2512695789337158, Final Batch Loss: 0.15802732110023499\n",
      "Epoch 3515, Loss: 0.23134734481573105, Final Batch Loss: 0.06177765876054764\n",
      "Epoch 3516, Loss: 0.18465807288885117, Final Batch Loss: 0.092950738966465\n",
      "Epoch 3517, Loss: 0.316532701253891, Final Batch Loss: 0.15321870148181915\n",
      "Epoch 3518, Loss: 0.2717296779155731, Final Batch Loss: 0.13151594996452332\n",
      "Epoch 3519, Loss: 0.18873337656259537, Final Batch Loss: 0.07765284180641174\n",
      "Epoch 3520, Loss: 0.20352499186992645, Final Batch Loss: 0.10020527243614197\n",
      "Epoch 3521, Loss: 0.2533973902463913, Final Batch Loss: 0.12283627688884735\n",
      "Epoch 3522, Loss: 0.2903096377849579, Final Batch Loss: 0.16415144503116608\n",
      "Epoch 3523, Loss: 0.28343960642814636, Final Batch Loss: 0.1289956122636795\n",
      "Epoch 3524, Loss: 0.26832112669944763, Final Batch Loss: 0.14389556646347046\n",
      "Epoch 3525, Loss: 0.19410382956266403, Final Batch Loss: 0.10161861777305603\n",
      "Epoch 3526, Loss: 0.20195705443620682, Final Batch Loss: 0.10104450583457947\n",
      "Epoch 3527, Loss: 0.18596556782722473, Final Batch Loss: 0.07851073145866394\n",
      "Epoch 3528, Loss: 0.23071495443582535, Final Batch Loss: 0.15361574292182922\n",
      "Epoch 3529, Loss: 0.1638852208852768, Final Batch Loss: 0.0765615925192833\n",
      "Epoch 3530, Loss: 0.2263840213418007, Final Batch Loss: 0.09715273231267929\n",
      "Epoch 3531, Loss: 0.22199933230876923, Final Batch Loss: 0.1076282411813736\n",
      "Epoch 3532, Loss: 0.1866154745221138, Final Batch Loss: 0.08562934398651123\n",
      "Epoch 3533, Loss: 0.24627234786748886, Final Batch Loss: 0.15228550136089325\n",
      "Epoch 3534, Loss: 0.18004444241523743, Final Batch Loss: 0.07806755602359772\n",
      "Epoch 3535, Loss: 0.23243629932403564, Final Batch Loss: 0.1126568615436554\n",
      "Epoch 3536, Loss: 0.22509445250034332, Final Batch Loss: 0.1058163195848465\n",
      "Epoch 3537, Loss: 0.25216153264045715, Final Batch Loss: 0.13788215816020966\n",
      "Epoch 3538, Loss: 0.23088795691728592, Final Batch Loss: 0.1453007012605667\n",
      "Epoch 3539, Loss: 0.19592230767011642, Final Batch Loss: 0.08403702080249786\n",
      "Epoch 3540, Loss: 0.20455601066350937, Final Batch Loss: 0.11259253323078156\n",
      "Epoch 3541, Loss: 0.2652495503425598, Final Batch Loss: 0.11028176546096802\n",
      "Epoch 3542, Loss: 0.26645030826330185, Final Batch Loss: 0.10151412338018417\n",
      "Epoch 3543, Loss: 0.24275501817464828, Final Batch Loss: 0.11222691088914871\n",
      "Epoch 3544, Loss: 0.2575636953115463, Final Batch Loss: 0.12967611849308014\n",
      "Epoch 3545, Loss: 0.22201980650424957, Final Batch Loss: 0.12966124713420868\n",
      "Epoch 3546, Loss: 0.19834046810865402, Final Batch Loss: 0.0967259556055069\n",
      "Epoch 3547, Loss: 0.21048186719417572, Final Batch Loss: 0.1311466544866562\n",
      "Epoch 3548, Loss: 0.2014407217502594, Final Batch Loss: 0.10184792429208755\n",
      "Epoch 3549, Loss: 0.21781640499830246, Final Batch Loss: 0.0914478525519371\n",
      "Epoch 3550, Loss: 0.2360444739460945, Final Batch Loss: 0.16011887788772583\n",
      "Epoch 3551, Loss: 0.2745676562190056, Final Batch Loss: 0.15257605910301208\n",
      "Epoch 3552, Loss: 0.27127908915281296, Final Batch Loss: 0.15070028603076935\n",
      "Epoch 3553, Loss: 0.20957066863775253, Final Batch Loss: 0.10002105683088303\n",
      "Epoch 3554, Loss: 0.2407888099551201, Final Batch Loss: 0.09018067270517349\n",
      "Epoch 3555, Loss: 0.18477313220500946, Final Batch Loss: 0.07993966341018677\n",
      "Epoch 3556, Loss: 0.20899539440870285, Final Batch Loss: 0.14098624885082245\n",
      "Epoch 3557, Loss: 0.19894812256097794, Final Batch Loss: 0.07951110601425171\n",
      "Epoch 3558, Loss: 0.2193264216184616, Final Batch Loss: 0.11881531774997711\n",
      "Epoch 3559, Loss: 0.2437896877527237, Final Batch Loss: 0.10575222969055176\n",
      "Epoch 3560, Loss: 0.21112029999494553, Final Batch Loss: 0.08178266137838364\n",
      "Epoch 3561, Loss: 0.19066600501537323, Final Batch Loss: 0.08947070688009262\n",
      "Epoch 3562, Loss: 0.18666303157806396, Final Batch Loss: 0.09620476514101028\n",
      "Epoch 3563, Loss: 0.20598649978637695, Final Batch Loss: 0.08929704874753952\n",
      "Epoch 3564, Loss: 0.231148399412632, Final Batch Loss: 0.121612548828125\n",
      "Epoch 3565, Loss: 0.2023836374282837, Final Batch Loss: 0.1073748767375946\n",
      "Epoch 3566, Loss: 0.23697223514318466, Final Batch Loss: 0.11383344978094101\n",
      "Epoch 3567, Loss: 0.19053210318088531, Final Batch Loss: 0.08612789958715439\n",
      "Epoch 3568, Loss: 0.23562267422676086, Final Batch Loss: 0.13152220845222473\n",
      "Epoch 3569, Loss: 0.20162750035524368, Final Batch Loss: 0.10985098779201508\n",
      "Epoch 3570, Loss: 0.15221785008907318, Final Batch Loss: 0.06508851796388626\n",
      "Epoch 3571, Loss: 0.17073510587215424, Final Batch Loss: 0.08455225825309753\n",
      "Epoch 3572, Loss: 0.19685576856136322, Final Batch Loss: 0.09971938282251358\n",
      "Epoch 3573, Loss: 0.2582142725586891, Final Batch Loss: 0.11589527875185013\n",
      "Epoch 3574, Loss: 0.2712586894631386, Final Batch Loss: 0.1597454994916916\n",
      "Epoch 3575, Loss: 0.3026515394449234, Final Batch Loss: 0.089259073138237\n",
      "Epoch 3576, Loss: 0.25540270656347275, Final Batch Loss: 0.11454454809427261\n",
      "Epoch 3577, Loss: 0.19039616733789444, Final Batch Loss: 0.10620085150003433\n",
      "Epoch 3578, Loss: 0.23697122931480408, Final Batch Loss: 0.11942196637392044\n",
      "Epoch 3579, Loss: 0.1828562542796135, Final Batch Loss: 0.08395007997751236\n",
      "Epoch 3580, Loss: 0.2964724600315094, Final Batch Loss: 0.1429494470357895\n",
      "Epoch 3581, Loss: 0.2594020962715149, Final Batch Loss: 0.13552971184253693\n",
      "Epoch 3582, Loss: 0.32942289859056473, Final Batch Loss: 0.2168823480606079\n",
      "Epoch 3583, Loss: 0.1797439083456993, Final Batch Loss: 0.08908437192440033\n",
      "Epoch 3584, Loss: 0.16724899411201477, Final Batch Loss: 0.06703145056962967\n",
      "Epoch 3585, Loss: 0.2702035680413246, Final Batch Loss: 0.16342400014400482\n",
      "Epoch 3586, Loss: 0.19668003171682358, Final Batch Loss: 0.08831914514303207\n",
      "Epoch 3587, Loss: 0.20510463416576385, Final Batch Loss: 0.09070494025945663\n",
      "Epoch 3588, Loss: 0.2257661372423172, Final Batch Loss: 0.11696598678827286\n",
      "Epoch 3589, Loss: 0.20178329944610596, Final Batch Loss: 0.13080233335494995\n",
      "Epoch 3590, Loss: 0.2381065934896469, Final Batch Loss: 0.14110518991947174\n",
      "Epoch 3591, Loss: 0.2179505005478859, Final Batch Loss: 0.13003048300743103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3592, Loss: 0.29694129526615143, Final Batch Loss: 0.16195295751094818\n",
      "Epoch 3593, Loss: 0.21316228061914444, Final Batch Loss: 0.09563028067350388\n",
      "Epoch 3594, Loss: 0.21453631669282913, Final Batch Loss: 0.12083178013563156\n",
      "Epoch 3595, Loss: 0.2187388464808464, Final Batch Loss: 0.10962849855422974\n",
      "Epoch 3596, Loss: 0.19431065768003464, Final Batch Loss: 0.09050393849611282\n",
      "Epoch 3597, Loss: 0.2132871001958847, Final Batch Loss: 0.08900380879640579\n",
      "Epoch 3598, Loss: 0.16801823675632477, Final Batch Loss: 0.06616024672985077\n",
      "Epoch 3599, Loss: 0.2879345268011093, Final Batch Loss: 0.12595823407173157\n",
      "Epoch 3600, Loss: 0.22750310599803925, Final Batch Loss: 0.10313680768013\n",
      "Epoch 3601, Loss: 0.2709273062646389, Final Batch Loss: 0.21302413940429688\n",
      "Epoch 3602, Loss: 0.19984447583556175, Final Batch Loss: 0.060775768011808395\n",
      "Epoch 3603, Loss: 0.2220560535788536, Final Batch Loss: 0.1263604313135147\n",
      "Epoch 3604, Loss: 0.1865561679005623, Final Batch Loss: 0.07996935397386551\n",
      "Epoch 3605, Loss: 0.22916244715452194, Final Batch Loss: 0.14430762827396393\n",
      "Epoch 3606, Loss: 0.2349715232849121, Final Batch Loss: 0.08503495156764984\n",
      "Epoch 3607, Loss: 0.1958814561367035, Final Batch Loss: 0.10184512287378311\n",
      "Epoch 3608, Loss: 0.3048613518476486, Final Batch Loss: 0.1895650178194046\n",
      "Epoch 3609, Loss: 0.20059803128242493, Final Batch Loss: 0.0861920639872551\n",
      "Epoch 3610, Loss: 0.18286043405532837, Final Batch Loss: 0.07615593075752258\n",
      "Epoch 3611, Loss: 0.2297172024846077, Final Batch Loss: 0.12564940750598907\n",
      "Epoch 3612, Loss: 0.24125760048627853, Final Batch Loss: 0.1097722128033638\n",
      "Epoch 3613, Loss: 0.23828624933958054, Final Batch Loss: 0.14694160223007202\n",
      "Epoch 3614, Loss: 0.23136383295059204, Final Batch Loss: 0.13668739795684814\n",
      "Epoch 3615, Loss: 0.2088363766670227, Final Batch Loss: 0.12509112060070038\n",
      "Epoch 3616, Loss: 0.1728358194231987, Final Batch Loss: 0.0902409553527832\n",
      "Epoch 3617, Loss: 0.19801799207925797, Final Batch Loss: 0.09835218638181686\n",
      "Epoch 3618, Loss: 0.21728818118572235, Final Batch Loss: 0.11697433888912201\n",
      "Epoch 3619, Loss: 0.1987593024969101, Final Batch Loss: 0.10806257277727127\n",
      "Epoch 3620, Loss: 0.17655719071626663, Final Batch Loss: 0.06757587939500809\n",
      "Epoch 3621, Loss: 0.19002822786569595, Final Batch Loss: 0.09835775196552277\n",
      "Epoch 3622, Loss: 0.25279680639505386, Final Batch Loss: 0.11654477566480637\n",
      "Epoch 3623, Loss: 0.19293074309825897, Final Batch Loss: 0.10902538150548935\n",
      "Epoch 3624, Loss: 0.20047211647033691, Final Batch Loss: 0.10233546048402786\n",
      "Epoch 3625, Loss: 0.18513107299804688, Final Batch Loss: 0.11152953654527664\n",
      "Epoch 3626, Loss: 0.14194360375404358, Final Batch Loss: 0.08565978705883026\n",
      "Epoch 3627, Loss: 0.24397777765989304, Final Batch Loss: 0.10501756519079208\n",
      "Epoch 3628, Loss: 0.214880108833313, Final Batch Loss: 0.09405899792909622\n",
      "Epoch 3629, Loss: 0.17583075165748596, Final Batch Loss: 0.06372442841529846\n",
      "Epoch 3630, Loss: 0.24929817765951157, Final Batch Loss: 0.13698209822177887\n",
      "Epoch 3631, Loss: 0.18005048483610153, Final Batch Loss: 0.1039409264922142\n",
      "Epoch 3632, Loss: 0.22420424222946167, Final Batch Loss: 0.10717155039310455\n",
      "Epoch 3633, Loss: 0.20856702327728271, Final Batch Loss: 0.10025154799222946\n",
      "Epoch 3634, Loss: 0.2658592015504837, Final Batch Loss: 0.13123206794261932\n",
      "Epoch 3635, Loss: 0.21747983247041702, Final Batch Loss: 0.1420496255159378\n",
      "Epoch 3636, Loss: 0.24125485867261887, Final Batch Loss: 0.13545888662338257\n",
      "Epoch 3637, Loss: 0.30257707089185715, Final Batch Loss: 0.08805667608976364\n",
      "Epoch 3638, Loss: 0.23158074170351028, Final Batch Loss: 0.09131243079900742\n",
      "Epoch 3639, Loss: 0.22663532942533493, Final Batch Loss: 0.11386694759130478\n",
      "Epoch 3640, Loss: 0.27512820065021515, Final Batch Loss: 0.14632709324359894\n",
      "Epoch 3641, Loss: 0.18126143515110016, Final Batch Loss: 0.09513251483440399\n",
      "Epoch 3642, Loss: 0.2014390528202057, Final Batch Loss: 0.0956699550151825\n",
      "Epoch 3643, Loss: 0.2168867439031601, Final Batch Loss: 0.10091106593608856\n",
      "Epoch 3644, Loss: 0.18885155022144318, Final Batch Loss: 0.1094532236456871\n",
      "Epoch 3645, Loss: 0.17427875101566315, Final Batch Loss: 0.07539445161819458\n",
      "Epoch 3646, Loss: 0.1953107789158821, Final Batch Loss: 0.07045862823724747\n",
      "Epoch 3647, Loss: 0.2416817769408226, Final Batch Loss: 0.13168686628341675\n",
      "Epoch 3648, Loss: 0.19601726531982422, Final Batch Loss: 0.09647738933563232\n",
      "Epoch 3649, Loss: 0.2478099763393402, Final Batch Loss: 0.10361286997795105\n",
      "Epoch 3650, Loss: 0.20161860436201096, Final Batch Loss: 0.1101761907339096\n",
      "Epoch 3651, Loss: 0.23583342880010605, Final Batch Loss: 0.12212798744440079\n",
      "Epoch 3652, Loss: 0.18927698582410812, Final Batch Loss: 0.07669396698474884\n",
      "Epoch 3653, Loss: 0.25185754150152206, Final Batch Loss: 0.1495932787656784\n",
      "Epoch 3654, Loss: 0.33359870314598083, Final Batch Loss: 0.1757199466228485\n",
      "Epoch 3655, Loss: 0.2842242121696472, Final Batch Loss: 0.12077566981315613\n",
      "Epoch 3656, Loss: 0.2376396581530571, Final Batch Loss: 0.12430762499570847\n",
      "Epoch 3657, Loss: 0.19507993757724762, Final Batch Loss: 0.10244657099246979\n",
      "Epoch 3658, Loss: 0.3136460557579994, Final Batch Loss: 0.19293902814388275\n",
      "Epoch 3659, Loss: 0.21929728984832764, Final Batch Loss: 0.10543669760227203\n",
      "Epoch 3660, Loss: 0.24005278199911118, Final Batch Loss: 0.1303826868534088\n",
      "Epoch 3661, Loss: 0.20781094580888748, Final Batch Loss: 0.10324136167764664\n",
      "Epoch 3662, Loss: 0.2354615032672882, Final Batch Loss: 0.10044285655021667\n",
      "Epoch 3663, Loss: 0.20778819918632507, Final Batch Loss: 0.10629753768444061\n",
      "Epoch 3664, Loss: 0.20561356842517853, Final Batch Loss: 0.08640815317630768\n",
      "Epoch 3665, Loss: 0.24365748465061188, Final Batch Loss: 0.13282401859760284\n",
      "Epoch 3666, Loss: 0.19402165710926056, Final Batch Loss: 0.10455496609210968\n",
      "Epoch 3667, Loss: 0.27570727467536926, Final Batch Loss: 0.09315595030784607\n",
      "Epoch 3668, Loss: 0.20915386825799942, Final Batch Loss: 0.12743602693080902\n",
      "Epoch 3669, Loss: 0.23249007016420364, Final Batch Loss: 0.08160219341516495\n",
      "Epoch 3670, Loss: 0.2198805958032608, Final Batch Loss: 0.11481856554746628\n",
      "Epoch 3671, Loss: 0.2303808331489563, Final Batch Loss: 0.1127203106880188\n",
      "Epoch 3672, Loss: 0.25104739516973495, Final Batch Loss: 0.12678179144859314\n",
      "Epoch 3673, Loss: 0.1937975212931633, Final Batch Loss: 0.10931003093719482\n",
      "Epoch 3674, Loss: 0.20635992288589478, Final Batch Loss: 0.11945129185914993\n",
      "Epoch 3675, Loss: 0.23707375675439835, Final Batch Loss: 0.14989426732063293\n",
      "Epoch 3676, Loss: 0.18567190319299698, Final Batch Loss: 0.0875738337635994\n",
      "Epoch 3677, Loss: 0.1915963515639305, Final Batch Loss: 0.10358405858278275\n",
      "Epoch 3678, Loss: 0.217465378344059, Final Batch Loss: 0.10542169958353043\n",
      "Epoch 3679, Loss: 0.19372621178627014, Final Batch Loss: 0.11197815090417862\n",
      "Epoch 3680, Loss: 0.20596101880073547, Final Batch Loss: 0.07241901755332947\n",
      "Epoch 3681, Loss: 0.2027602344751358, Final Batch Loss: 0.08637795597314835\n",
      "Epoch 3682, Loss: 0.19995270669460297, Final Batch Loss: 0.12875519692897797\n",
      "Epoch 3683, Loss: 0.24718567728996277, Final Batch Loss: 0.10756233334541321\n",
      "Epoch 3684, Loss: 0.26162148267030716, Final Batch Loss: 0.13932138681411743\n",
      "Epoch 3685, Loss: 0.16551853716373444, Final Batch Loss: 0.07399775832891464\n",
      "Epoch 3686, Loss: 0.21342813968658447, Final Batch Loss: 0.10187450051307678\n",
      "Epoch 3687, Loss: 0.1954595446586609, Final Batch Loss: 0.09735796600580215\n",
      "Epoch 3688, Loss: 0.19893227517604828, Final Batch Loss: 0.10731901973485947\n",
      "Epoch 3689, Loss: 0.22581905126571655, Final Batch Loss: 0.11036741733551025\n",
      "Epoch 3690, Loss: 0.21469978988170624, Final Batch Loss: 0.10497448593378067\n",
      "Epoch 3691, Loss: 0.19942957162857056, Final Batch Loss: 0.08471647650003433\n",
      "Epoch 3692, Loss: 0.1976296603679657, Final Batch Loss: 0.10006283968687057\n",
      "Epoch 3693, Loss: 0.22641315311193466, Final Batch Loss: 0.10522884875535965\n",
      "Epoch 3694, Loss: 0.26201488077640533, Final Batch Loss: 0.16036473214626312\n",
      "Epoch 3695, Loss: 0.19040130078792572, Final Batch Loss: 0.07976033538579941\n",
      "Epoch 3696, Loss: 0.20154696702957153, Final Batch Loss: 0.09214448928833008\n",
      "Epoch 3697, Loss: 0.23150508850812912, Final Batch Loss: 0.08911895006895065\n",
      "Epoch 3698, Loss: 0.19905389845371246, Final Batch Loss: 0.09823278337717056\n",
      "Epoch 3699, Loss: 0.2785085439682007, Final Batch Loss: 0.18470890820026398\n",
      "Epoch 3700, Loss: 0.16021867096424103, Final Batch Loss: 0.07164530456066132\n",
      "Epoch 3701, Loss: 0.239311583340168, Final Batch Loss: 0.12634502351284027\n",
      "Epoch 3702, Loss: 0.22935491800308228, Final Batch Loss: 0.10279552638530731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3703, Loss: 0.21398115158081055, Final Batch Loss: 0.10963359475135803\n",
      "Epoch 3704, Loss: 0.20962174981832504, Final Batch Loss: 0.11910473555326462\n",
      "Epoch 3705, Loss: 0.2123376429080963, Final Batch Loss: 0.08072984218597412\n",
      "Epoch 3706, Loss: 0.16990721225738525, Final Batch Loss: 0.09262620657682419\n",
      "Epoch 3707, Loss: 0.1820158064365387, Final Batch Loss: 0.07511475682258606\n",
      "Epoch 3708, Loss: 0.19137389212846756, Final Batch Loss: 0.10522691160440445\n",
      "Epoch 3709, Loss: 0.19603734463453293, Final Batch Loss: 0.11275309324264526\n",
      "Epoch 3710, Loss: 0.21267540752887726, Final Batch Loss: 0.10929439961910248\n",
      "Epoch 3711, Loss: 0.2265162616968155, Final Batch Loss: 0.10812250524759293\n",
      "Epoch 3712, Loss: 0.21641825139522552, Final Batch Loss: 0.12213499844074249\n",
      "Epoch 3713, Loss: 0.3059885501861572, Final Batch Loss: 0.1822478324174881\n",
      "Epoch 3714, Loss: 0.21583443880081177, Final Batch Loss: 0.1083621084690094\n",
      "Epoch 3715, Loss: 0.2315906658768654, Final Batch Loss: 0.0890347883105278\n",
      "Epoch 3716, Loss: 0.3247237503528595, Final Batch Loss: 0.14071884751319885\n",
      "Epoch 3717, Loss: 0.29647866636514664, Final Batch Loss: 0.07971619814634323\n",
      "Epoch 3718, Loss: 0.2570127099752426, Final Batch Loss: 0.12135079503059387\n",
      "Epoch 3719, Loss: 0.31314073503017426, Final Batch Loss: 0.18467479944229126\n",
      "Epoch 3720, Loss: 0.2506733685731888, Final Batch Loss: 0.12532110512256622\n",
      "Epoch 3721, Loss: 0.20398330688476562, Final Batch Loss: 0.0768427848815918\n",
      "Epoch 3722, Loss: 0.24102258682250977, Final Batch Loss: 0.11007900536060333\n",
      "Epoch 3723, Loss: 0.2694879621267319, Final Batch Loss: 0.17198659479618073\n",
      "Epoch 3724, Loss: 0.2307715341448784, Final Batch Loss: 0.13183829188346863\n",
      "Epoch 3725, Loss: 0.22662688791751862, Final Batch Loss: 0.10769904404878616\n",
      "Epoch 3726, Loss: 0.25885117053985596, Final Batch Loss: 0.13110272586345673\n",
      "Epoch 3727, Loss: 0.32640257477760315, Final Batch Loss: 0.19431234896183014\n",
      "Epoch 3728, Loss: 0.16838222742080688, Final Batch Loss: 0.092108815908432\n",
      "Epoch 3729, Loss: 0.252668596804142, Final Batch Loss: 0.1416151225566864\n",
      "Epoch 3730, Loss: 0.25595831125974655, Final Batch Loss: 0.13490869104862213\n",
      "Epoch 3731, Loss: 0.1777428314089775, Final Batch Loss: 0.08708467334508896\n",
      "Epoch 3732, Loss: 0.23515699058771133, Final Batch Loss: 0.10228347033262253\n",
      "Epoch 3733, Loss: 0.2898818626999855, Final Batch Loss: 0.19274947047233582\n",
      "Epoch 3734, Loss: 0.21218375861644745, Final Batch Loss: 0.11815657466650009\n",
      "Epoch 3735, Loss: 0.20492389798164368, Final Batch Loss: 0.09201864898204803\n",
      "Epoch 3736, Loss: 0.2371847853064537, Final Batch Loss: 0.12630189955234528\n",
      "Epoch 3737, Loss: 0.21820315718650818, Final Batch Loss: 0.10432249307632446\n",
      "Epoch 3738, Loss: 0.2348261922597885, Final Batch Loss: 0.1409570872783661\n",
      "Epoch 3739, Loss: 0.1902005448937416, Final Batch Loss: 0.0950385108590126\n",
      "Epoch 3740, Loss: 0.2116025760769844, Final Batch Loss: 0.12349345535039902\n",
      "Epoch 3741, Loss: 0.20570901036262512, Final Batch Loss: 0.1137208491563797\n",
      "Epoch 3742, Loss: 0.28210218250751495, Final Batch Loss: 0.15145988762378693\n",
      "Epoch 3743, Loss: 0.23409327864646912, Final Batch Loss: 0.1226743683218956\n",
      "Epoch 3744, Loss: 0.18903092294931412, Final Batch Loss: 0.10231442004442215\n",
      "Epoch 3745, Loss: 0.196319080889225, Final Batch Loss: 0.08288288116455078\n",
      "Epoch 3746, Loss: 0.27293507009744644, Final Batch Loss: 0.12090074270963669\n",
      "Epoch 3747, Loss: 0.2583288922905922, Final Batch Loss: 0.146250918507576\n",
      "Epoch 3748, Loss: 0.2747413069009781, Final Batch Loss: 0.149095356464386\n",
      "Epoch 3749, Loss: 0.18215332180261612, Final Batch Loss: 0.08855515718460083\n",
      "Epoch 3750, Loss: 0.1870374158024788, Final Batch Loss: 0.10306324064731598\n",
      "Epoch 3751, Loss: 0.19867069274187088, Final Batch Loss: 0.08374340087175369\n",
      "Epoch 3752, Loss: 0.16540833562612534, Final Batch Loss: 0.10086902230978012\n",
      "Epoch 3753, Loss: 0.20383089035749435, Final Batch Loss: 0.1007111594080925\n",
      "Epoch 3754, Loss: 0.1970265805721283, Final Batch Loss: 0.09948363155126572\n",
      "Epoch 3755, Loss: 0.20541806519031525, Final Batch Loss: 0.118759386241436\n",
      "Epoch 3756, Loss: 0.21441540867090225, Final Batch Loss: 0.09430032223463058\n",
      "Epoch 3757, Loss: 0.18624232709407806, Final Batch Loss: 0.09475098550319672\n",
      "Epoch 3758, Loss: 0.24035236984491348, Final Batch Loss: 0.12559020519256592\n",
      "Epoch 3759, Loss: 0.21617010235786438, Final Batch Loss: 0.10478993505239487\n",
      "Epoch 3760, Loss: 0.2367907613515854, Final Batch Loss: 0.11879989504814148\n",
      "Epoch 3761, Loss: 0.26646190136671066, Final Batch Loss: 0.1415686160326004\n",
      "Epoch 3762, Loss: 0.24225851893424988, Final Batch Loss: 0.1527126133441925\n",
      "Epoch 3763, Loss: 0.18389376252889633, Final Batch Loss: 0.08156026899814606\n",
      "Epoch 3764, Loss: 0.17760418355464935, Final Batch Loss: 0.07819359749555588\n",
      "Epoch 3765, Loss: 0.21995779871940613, Final Batch Loss: 0.10178790241479874\n",
      "Epoch 3766, Loss: 0.25171567499637604, Final Batch Loss: 0.1436888724565506\n",
      "Epoch 3767, Loss: 0.19954606890678406, Final Batch Loss: 0.11713320761919022\n",
      "Epoch 3768, Loss: 0.17092777788639069, Final Batch Loss: 0.06958241760730743\n",
      "Epoch 3769, Loss: 0.18451795727014542, Final Batch Loss: 0.06333868205547333\n",
      "Epoch 3770, Loss: 0.18661625683307648, Final Batch Loss: 0.1233913004398346\n",
      "Epoch 3771, Loss: 0.2314331829547882, Final Batch Loss: 0.12143188714981079\n",
      "Epoch 3772, Loss: 0.18651488423347473, Final Batch Loss: 0.09464429318904877\n",
      "Epoch 3773, Loss: 0.2299962043762207, Final Batch Loss: 0.1368538737297058\n",
      "Epoch 3774, Loss: 0.2022380605340004, Final Batch Loss: 0.10861222445964813\n",
      "Epoch 3775, Loss: 0.2315972000360489, Final Batch Loss: 0.09444358944892883\n",
      "Epoch 3776, Loss: 0.19562740623950958, Final Batch Loss: 0.10395320504903793\n",
      "Epoch 3777, Loss: 0.1986883208155632, Final Batch Loss: 0.09174321591854095\n",
      "Epoch 3778, Loss: 0.17650623619556427, Final Batch Loss: 0.09554187953472137\n",
      "Epoch 3779, Loss: 0.2275417372584343, Final Batch Loss: 0.1164330244064331\n",
      "Epoch 3780, Loss: 0.17151549458503723, Final Batch Loss: 0.08015516400337219\n",
      "Epoch 3781, Loss: 0.19440767914056778, Final Batch Loss: 0.09780828654766083\n",
      "Epoch 3782, Loss: 0.25401771068573, Final Batch Loss: 0.10345384478569031\n",
      "Epoch 3783, Loss: 0.2244000807404518, Final Batch Loss: 0.1347535401582718\n",
      "Epoch 3784, Loss: 0.2622113600373268, Final Batch Loss: 0.18754760921001434\n",
      "Epoch 3785, Loss: 0.16250359266996384, Final Batch Loss: 0.07232407480478287\n",
      "Epoch 3786, Loss: 0.19270703196525574, Final Batch Loss: 0.09283246099948883\n",
      "Epoch 3787, Loss: 0.24929558485746384, Final Batch Loss: 0.120876245200634\n",
      "Epoch 3788, Loss: 0.2362159788608551, Final Batch Loss: 0.08723841607570648\n",
      "Epoch 3789, Loss: 0.18072908371686935, Final Batch Loss: 0.09317443519830704\n",
      "Epoch 3790, Loss: 0.255452997982502, Final Batch Loss: 0.11360365897417068\n",
      "Epoch 3791, Loss: 0.23493961244821548, Final Batch Loss: 0.14422528445720673\n",
      "Epoch 3792, Loss: 0.17821858823299408, Final Batch Loss: 0.08483631163835526\n",
      "Epoch 3793, Loss: 0.21579287201166153, Final Batch Loss: 0.12212616205215454\n",
      "Epoch 3794, Loss: 0.21489376574754715, Final Batch Loss: 0.0975695326924324\n",
      "Epoch 3795, Loss: 0.18607471883296967, Final Batch Loss: 0.10462252050638199\n",
      "Epoch 3796, Loss: 0.20318889617919922, Final Batch Loss: 0.10280857235193253\n",
      "Epoch 3797, Loss: 0.1808491349220276, Final Batch Loss: 0.0816904753446579\n",
      "Epoch 3798, Loss: 0.25195255875587463, Final Batch Loss: 0.14738038182258606\n",
      "Epoch 3799, Loss: 0.1769818812608719, Final Batch Loss: 0.08474408835172653\n",
      "Epoch 3800, Loss: 0.23815666139125824, Final Batch Loss: 0.1385921835899353\n",
      "Epoch 3801, Loss: 0.2678328678011894, Final Batch Loss: 0.1162586584687233\n",
      "Epoch 3802, Loss: 0.24863731861114502, Final Batch Loss: 0.16540589928627014\n",
      "Epoch 3803, Loss: 0.27169328182935715, Final Batch Loss: 0.1532938927412033\n",
      "Epoch 3804, Loss: 0.18824930489063263, Final Batch Loss: 0.08951384574174881\n",
      "Epoch 3805, Loss: 0.21950142830610275, Final Batch Loss: 0.14018918573856354\n",
      "Epoch 3806, Loss: 0.3113672062754631, Final Batch Loss: 0.23218736052513123\n",
      "Epoch 3807, Loss: 0.1931830793619156, Final Batch Loss: 0.11956756561994553\n",
      "Epoch 3808, Loss: 0.2839757576584816, Final Batch Loss: 0.0978107675909996\n",
      "Epoch 3809, Loss: 0.2106136530637741, Final Batch Loss: 0.11565297096967697\n",
      "Epoch 3810, Loss: 0.17392253130674362, Final Batch Loss: 0.11047215014696121\n",
      "Epoch 3811, Loss: 0.23925141617655754, Final Batch Loss: 0.058849941939115524\n",
      "Epoch 3812, Loss: 0.17528888583183289, Final Batch Loss: 0.09274890273809433\n",
      "Epoch 3813, Loss: 0.17751675844192505, Final Batch Loss: 0.10018428415060043\n",
      "Epoch 3814, Loss: 0.2033308669924736, Final Batch Loss: 0.11819367110729218\n",
      "Epoch 3815, Loss: 0.18949604034423828, Final Batch Loss: 0.09851889312267303\n",
      "Epoch 3816, Loss: 0.25765322893857956, Final Batch Loss: 0.14598917961120605\n",
      "Epoch 3817, Loss: 0.20432516187429428, Final Batch Loss: 0.13091561198234558\n",
      "Epoch 3818, Loss: 0.26098115742206573, Final Batch Loss: 0.19383440911769867\n",
      "Epoch 3819, Loss: 0.2044798955321312, Final Batch Loss: 0.11281034350395203\n",
      "Epoch 3820, Loss: 0.20106567442417145, Final Batch Loss: 0.08110298961400986\n",
      "Epoch 3821, Loss: 0.1807825267314911, Final Batch Loss: 0.09658010303974152\n",
      "Epoch 3822, Loss: 0.23240543901920319, Final Batch Loss: 0.08978280425071716\n",
      "Epoch 3823, Loss: 0.2210569605231285, Final Batch Loss: 0.13713857531547546\n",
      "Epoch 3824, Loss: 0.2029130905866623, Final Batch Loss: 0.09762899577617645\n",
      "Epoch 3825, Loss: 0.1987893059849739, Final Batch Loss: 0.09484418481588364\n",
      "Epoch 3826, Loss: 0.27317382395267487, Final Batch Loss: 0.13376769423484802\n",
      "Epoch 3827, Loss: 0.2138388715684414, Final Batch Loss: 0.05908651277422905\n",
      "Epoch 3828, Loss: 0.2817694619297981, Final Batch Loss: 0.1641807109117508\n",
      "Epoch 3829, Loss: 0.23078934848308563, Final Batch Loss: 0.08673782646656036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3830, Loss: 0.2837041765451431, Final Batch Loss: 0.15073162317276\n",
      "Epoch 3831, Loss: 0.20267264544963837, Final Batch Loss: 0.14086106419563293\n",
      "Epoch 3832, Loss: 0.18061362206935883, Final Batch Loss: 0.09632886946201324\n",
      "Epoch 3833, Loss: 0.26652873307466507, Final Batch Loss: 0.17396663129329681\n",
      "Epoch 3834, Loss: 0.2345290258526802, Final Batch Loss: 0.10042452067136765\n",
      "Epoch 3835, Loss: 0.21630462259054184, Final Batch Loss: 0.09685155749320984\n",
      "Epoch 3836, Loss: 0.23485197871923447, Final Batch Loss: 0.09112515300512314\n",
      "Epoch 3837, Loss: 0.23576293885707855, Final Batch Loss: 0.1300175040960312\n",
      "Epoch 3838, Loss: 0.22019027173519135, Final Batch Loss: 0.10791532695293427\n",
      "Epoch 3839, Loss: 0.2640102803707123, Final Batch Loss: 0.1455094963312149\n",
      "Epoch 3840, Loss: 0.17792188376188278, Final Batch Loss: 0.06164361536502838\n",
      "Epoch 3841, Loss: 0.23686788976192474, Final Batch Loss: 0.09610363841056824\n",
      "Epoch 3842, Loss: 0.1985326111316681, Final Batch Loss: 0.10557525604963303\n",
      "Epoch 3843, Loss: 0.21842333674430847, Final Batch Loss: 0.13185445964336395\n",
      "Epoch 3844, Loss: 0.20708821713924408, Final Batch Loss: 0.0911833867430687\n",
      "Epoch 3845, Loss: 0.23128823190927505, Final Batch Loss: 0.13689696788787842\n",
      "Epoch 3846, Loss: 0.1992097720503807, Final Batch Loss: 0.08890735357999802\n",
      "Epoch 3847, Loss: 0.2469639852643013, Final Batch Loss: 0.11217977851629257\n",
      "Epoch 3848, Loss: 0.3315180093050003, Final Batch Loss: 0.166314959526062\n",
      "Epoch 3849, Loss: 0.20228911191225052, Final Batch Loss: 0.11812085658311844\n",
      "Epoch 3850, Loss: 0.21207094192504883, Final Batch Loss: 0.13693587481975555\n",
      "Epoch 3851, Loss: 0.23613979667425156, Final Batch Loss: 0.11246836930513382\n",
      "Epoch 3852, Loss: 0.20472699403762817, Final Batch Loss: 0.12949644029140472\n",
      "Epoch 3853, Loss: 0.22930419445037842, Final Batch Loss: 0.12196419388055801\n",
      "Epoch 3854, Loss: 0.25159285217523575, Final Batch Loss: 0.1093883141875267\n",
      "Epoch 3855, Loss: 0.22835001349449158, Final Batch Loss: 0.12586353719234467\n",
      "Epoch 3856, Loss: 0.2357965037226677, Final Batch Loss: 0.1203952431678772\n",
      "Epoch 3857, Loss: 0.20876359939575195, Final Batch Loss: 0.1091381087899208\n",
      "Epoch 3858, Loss: 0.22997233271598816, Final Batch Loss: 0.12409303337335587\n",
      "Epoch 3859, Loss: 0.26405662298202515, Final Batch Loss: 0.13457506895065308\n",
      "Epoch 3860, Loss: 0.2560936212539673, Final Batch Loss: 0.10085825622081757\n",
      "Epoch 3861, Loss: 0.18823027610778809, Final Batch Loss: 0.08949695527553558\n",
      "Epoch 3862, Loss: 0.2019285038113594, Final Batch Loss: 0.10995487123727798\n",
      "Epoch 3863, Loss: 0.226412832736969, Final Batch Loss: 0.11284802854061127\n",
      "Epoch 3864, Loss: 0.20977094024419785, Final Batch Loss: 0.12077271938323975\n",
      "Epoch 3865, Loss: 0.23575935512781143, Final Batch Loss: 0.1409333050251007\n",
      "Epoch 3866, Loss: 0.2033071294426918, Final Batch Loss: 0.09897386282682419\n",
      "Epoch 3867, Loss: 0.2520512416958809, Final Batch Loss: 0.13177642226219177\n",
      "Epoch 3868, Loss: 0.23910443484783173, Final Batch Loss: 0.12904687225818634\n",
      "Epoch 3869, Loss: 0.26256678253412247, Final Batch Loss: 0.12155187875032425\n",
      "Epoch 3870, Loss: 0.212687648832798, Final Batch Loss: 0.10856664925813675\n",
      "Epoch 3871, Loss: 0.2238779440522194, Final Batch Loss: 0.13959138095378876\n",
      "Epoch 3872, Loss: 0.2758290097117424, Final Batch Loss: 0.16675569117069244\n",
      "Epoch 3873, Loss: 0.2009389027953148, Final Batch Loss: 0.1127760112285614\n",
      "Epoch 3874, Loss: 0.22724781185388565, Final Batch Loss: 0.14074617624282837\n",
      "Epoch 3875, Loss: 0.2479982152581215, Final Batch Loss: 0.14281834661960602\n",
      "Epoch 3876, Loss: 0.19444917887449265, Final Batch Loss: 0.10264880210161209\n",
      "Epoch 3877, Loss: 0.20950733125209808, Final Batch Loss: 0.08713989704847336\n",
      "Epoch 3878, Loss: 0.22569618374109268, Final Batch Loss: 0.09768485277891159\n",
      "Epoch 3879, Loss: 0.23317503929138184, Final Batch Loss: 0.11048122495412827\n",
      "Epoch 3880, Loss: 0.1756221204996109, Final Batch Loss: 0.08556271344423294\n",
      "Epoch 3881, Loss: 0.21995244920253754, Final Batch Loss: 0.11191337555646896\n",
      "Epoch 3882, Loss: 0.22968805581331253, Final Batch Loss: 0.11990872025489807\n",
      "Epoch 3883, Loss: 0.23638483881950378, Final Batch Loss: 0.13597455620765686\n",
      "Epoch 3884, Loss: 0.2185949757695198, Final Batch Loss: 0.11455049365758896\n",
      "Epoch 3885, Loss: 0.19584430754184723, Final Batch Loss: 0.1267928183078766\n",
      "Epoch 3886, Loss: 0.218478761613369, Final Batch Loss: 0.13233916461467743\n",
      "Epoch 3887, Loss: 0.20870336145162582, Final Batch Loss: 0.10037951916456223\n",
      "Epoch 3888, Loss: 0.19450398534536362, Final Batch Loss: 0.1025758758187294\n",
      "Epoch 3889, Loss: 0.23603160679340363, Final Batch Loss: 0.07745598256587982\n",
      "Epoch 3890, Loss: 0.20314732939004898, Final Batch Loss: 0.09177514910697937\n",
      "Epoch 3891, Loss: 0.18728681653738022, Final Batch Loss: 0.11427372694015503\n",
      "Epoch 3892, Loss: 0.20951835066080093, Final Batch Loss: 0.12736216187477112\n",
      "Epoch 3893, Loss: 0.2160782590508461, Final Batch Loss: 0.10033734887838364\n",
      "Epoch 3894, Loss: 0.18005873262882233, Final Batch Loss: 0.10062684118747711\n",
      "Epoch 3895, Loss: 0.19381768256425858, Final Batch Loss: 0.09495031833648682\n",
      "Epoch 3896, Loss: 0.21903616189956665, Final Batch Loss: 0.10066390782594681\n",
      "Epoch 3897, Loss: 0.18556220829486847, Final Batch Loss: 0.0651230663061142\n",
      "Epoch 3898, Loss: 0.17687685042619705, Final Batch Loss: 0.10959898680448532\n",
      "Epoch 3899, Loss: 0.24197860062122345, Final Batch Loss: 0.11660538613796234\n",
      "Epoch 3900, Loss: 0.216450497508049, Final Batch Loss: 0.09065568447113037\n",
      "Epoch 3901, Loss: 0.17054271697998047, Final Batch Loss: 0.08488786965608597\n",
      "Epoch 3902, Loss: 0.19483337551355362, Final Batch Loss: 0.09335292875766754\n",
      "Epoch 3903, Loss: 0.2056535705924034, Final Batch Loss: 0.09722533822059631\n",
      "Epoch 3904, Loss: 0.19449233263731003, Final Batch Loss: 0.07690609246492386\n",
      "Epoch 3905, Loss: 0.2472802959382534, Final Batch Loss: 0.19272978603839874\n",
      "Epoch 3906, Loss: 0.17808816581964493, Final Batch Loss: 0.08604026585817337\n",
      "Epoch 3907, Loss: 0.1326121725142002, Final Batch Loss: 0.04536024108529091\n",
      "Epoch 3908, Loss: 0.3138926476240158, Final Batch Loss: 0.23769457638263702\n",
      "Epoch 3909, Loss: 0.19287554919719696, Final Batch Loss: 0.09710810333490372\n",
      "Epoch 3910, Loss: 0.1860937401652336, Final Batch Loss: 0.08546724170446396\n",
      "Epoch 3911, Loss: 0.1631511226296425, Final Batch Loss: 0.07234510779380798\n",
      "Epoch 3912, Loss: 0.20043600350618362, Final Batch Loss: 0.08507522940635681\n",
      "Epoch 3913, Loss: 0.20590704679489136, Final Batch Loss: 0.1123955100774765\n",
      "Epoch 3914, Loss: 0.1940152496099472, Final Batch Loss: 0.07737071812152863\n",
      "Epoch 3915, Loss: 0.21338491141796112, Final Batch Loss: 0.10786566883325577\n",
      "Epoch 3916, Loss: 0.19483084976673126, Final Batch Loss: 0.09011726826429367\n",
      "Epoch 3917, Loss: 0.20237721502780914, Final Batch Loss: 0.11417961865663528\n",
      "Epoch 3918, Loss: 0.15392109006643295, Final Batch Loss: 0.08417046815156937\n",
      "Epoch 3919, Loss: 0.16434704512357712, Final Batch Loss: 0.07639119774103165\n",
      "Epoch 3920, Loss: 0.2483922466635704, Final Batch Loss: 0.10899769514799118\n",
      "Epoch 3921, Loss: 0.16276264935731888, Final Batch Loss: 0.08978160470724106\n",
      "Epoch 3922, Loss: 0.1688838005065918, Final Batch Loss: 0.08690767735242844\n",
      "Epoch 3923, Loss: 0.1931554600596428, Final Batch Loss: 0.08828021585941315\n",
      "Epoch 3924, Loss: 0.18381065875291824, Final Batch Loss: 0.08618929982185364\n",
      "Epoch 3925, Loss: 0.21299464255571365, Final Batch Loss: 0.09931377321481705\n",
      "Epoch 3926, Loss: 0.21768918633460999, Final Batch Loss: 0.1242242157459259\n",
      "Epoch 3927, Loss: 0.1501348540186882, Final Batch Loss: 0.0707419142127037\n",
      "Epoch 3928, Loss: 0.20134473592042923, Final Batch Loss: 0.1071428656578064\n",
      "Epoch 3929, Loss: 0.3223209083080292, Final Batch Loss: 0.14451457560062408\n",
      "Epoch 3930, Loss: 0.16344813257455826, Final Batch Loss: 0.09336835891008377\n",
      "Epoch 3931, Loss: 0.17276782542467117, Final Batch Loss: 0.0861295685172081\n",
      "Epoch 3932, Loss: 0.20038312673568726, Final Batch Loss: 0.05500410497188568\n",
      "Epoch 3933, Loss: 0.18208733201026917, Final Batch Loss: 0.06755277514457703\n",
      "Epoch 3934, Loss: 0.2162541076540947, Final Batch Loss: 0.1036081537604332\n",
      "Epoch 3935, Loss: 0.15148869529366493, Final Batch Loss: 0.09130643308162689\n",
      "Epoch 3936, Loss: 0.15437143668532372, Final Batch Loss: 0.061639729887247086\n",
      "Epoch 3937, Loss: 0.18129871040582657, Final Batch Loss: 0.0820423811674118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3938, Loss: 0.18611066043376923, Final Batch Loss: 0.08621921390295029\n",
      "Epoch 3939, Loss: 0.16932176053524017, Final Batch Loss: 0.08211973309516907\n",
      "Epoch 3940, Loss: 0.15699893981218338, Final Batch Loss: 0.0910646840929985\n",
      "Epoch 3941, Loss: 0.23553314059972763, Final Batch Loss: 0.07877988368272781\n",
      "Epoch 3942, Loss: 0.20998472720384598, Final Batch Loss: 0.10634835809469223\n",
      "Epoch 3943, Loss: 0.1420009881258011, Final Batch Loss: 0.07759107649326324\n",
      "Epoch 3944, Loss: 0.2124883085489273, Final Batch Loss: 0.10061952471733093\n",
      "Epoch 3945, Loss: 0.2771241217851639, Final Batch Loss: 0.14515724778175354\n",
      "Epoch 3946, Loss: 0.2383321225643158, Final Batch Loss: 0.09552685916423798\n",
      "Epoch 3947, Loss: 0.18522564321756363, Final Batch Loss: 0.07894061505794525\n",
      "Epoch 3948, Loss: 0.21687740087509155, Final Batch Loss: 0.09744006395339966\n",
      "Epoch 3949, Loss: 0.19229572266340256, Final Batch Loss: 0.09637434035539627\n",
      "Epoch 3950, Loss: 0.22999095916748047, Final Batch Loss: 0.09629261493682861\n",
      "Epoch 3951, Loss: 0.210995651781559, Final Batch Loss: 0.08691567927598953\n",
      "Epoch 3952, Loss: 0.21572107821702957, Final Batch Loss: 0.08749819546937943\n",
      "Epoch 3953, Loss: 0.2093045711517334, Final Batch Loss: 0.11636625975370407\n",
      "Epoch 3954, Loss: 0.18722686916589737, Final Batch Loss: 0.0878298431634903\n",
      "Epoch 3955, Loss: 0.20805830508470535, Final Batch Loss: 0.10631506145000458\n",
      "Epoch 3956, Loss: 0.21812214702367783, Final Batch Loss: 0.0910545215010643\n",
      "Epoch 3957, Loss: 0.32112690806388855, Final Batch Loss: 0.19820460677146912\n",
      "Epoch 3958, Loss: 0.21462388336658478, Final Batch Loss: 0.11467107385396957\n",
      "Epoch 3959, Loss: 0.18417027592658997, Final Batch Loss: 0.11021696031093597\n",
      "Epoch 3960, Loss: 0.20310033857822418, Final Batch Loss: 0.09798926115036011\n",
      "Epoch 3961, Loss: 0.19111136347055435, Final Batch Loss: 0.09838695824146271\n",
      "Epoch 3962, Loss: 0.21408440172672272, Final Batch Loss: 0.09478440880775452\n",
      "Epoch 3963, Loss: 0.17432943731546402, Final Batch Loss: 0.09426514804363251\n",
      "Epoch 3964, Loss: 0.16412106901407242, Final Batch Loss: 0.0801267996430397\n",
      "Epoch 3965, Loss: 0.1908312365412712, Final Batch Loss: 0.05147039145231247\n",
      "Epoch 3966, Loss: 0.2737084776163101, Final Batch Loss: 0.12504489719867706\n",
      "Epoch 3967, Loss: 0.21908769011497498, Final Batch Loss: 0.11718111485242844\n",
      "Epoch 3968, Loss: 0.1887480616569519, Final Batch Loss: 0.1077154353260994\n",
      "Epoch 3969, Loss: 0.2542150542140007, Final Batch Loss: 0.11429622024297714\n",
      "Epoch 3970, Loss: 0.16661366075277328, Final Batch Loss: 0.08006506413221359\n",
      "Epoch 3971, Loss: 0.23319070786237717, Final Batch Loss: 0.11233927309513092\n",
      "Epoch 3972, Loss: 0.16976015269756317, Final Batch Loss: 0.08676202595233917\n",
      "Epoch 3973, Loss: 0.2256382778286934, Final Batch Loss: 0.126215860247612\n",
      "Epoch 3974, Loss: 0.1817091777920723, Final Batch Loss: 0.07623478025197983\n",
      "Epoch 3975, Loss: 0.2341013103723526, Final Batch Loss: 0.13118338584899902\n",
      "Epoch 3976, Loss: 0.28619129955768585, Final Batch Loss: 0.18094509840011597\n",
      "Epoch 3977, Loss: 0.18742402642965317, Final Batch Loss: 0.09979042410850525\n",
      "Epoch 3978, Loss: 0.19410397857427597, Final Batch Loss: 0.09680704772472382\n",
      "Epoch 3979, Loss: 0.22185161709785461, Final Batch Loss: 0.08986169099807739\n",
      "Epoch 3980, Loss: 0.2562151923775673, Final Batch Loss: 0.13202884793281555\n",
      "Epoch 3981, Loss: 0.2302684187889099, Final Batch Loss: 0.11803150922060013\n",
      "Epoch 3982, Loss: 0.19358158856630325, Final Batch Loss: 0.07598607987165451\n",
      "Epoch 3983, Loss: 0.1883218139410019, Final Batch Loss: 0.10250543802976608\n",
      "Epoch 3984, Loss: 0.19177597761154175, Final Batch Loss: 0.11048641055822372\n",
      "Epoch 3985, Loss: 0.24323320388793945, Final Batch Loss: 0.11987414956092834\n",
      "Epoch 3986, Loss: 0.181217759847641, Final Batch Loss: 0.08407880365848541\n",
      "Epoch 3987, Loss: 0.18578898161649704, Final Batch Loss: 0.09598170965909958\n",
      "Epoch 3988, Loss: 0.20383446663618088, Final Batch Loss: 0.09648185968399048\n",
      "Epoch 3989, Loss: 0.20256106555461884, Final Batch Loss: 0.0857124924659729\n",
      "Epoch 3990, Loss: 0.1993275284767151, Final Batch Loss: 0.11280041933059692\n",
      "Epoch 3991, Loss: 0.1718985065817833, Final Batch Loss: 0.07447359710931778\n",
      "Epoch 3992, Loss: 0.23362649232149124, Final Batch Loss: 0.1448124796152115\n",
      "Epoch 3993, Loss: 0.2297651693224907, Final Batch Loss: 0.07809986919164658\n",
      "Epoch 3994, Loss: 0.21891090273857117, Final Batch Loss: 0.10277361422777176\n",
      "Epoch 3995, Loss: 0.21552633494138718, Final Batch Loss: 0.09892521053552628\n",
      "Epoch 3996, Loss: 0.16205675154924393, Final Batch Loss: 0.08215676993131638\n",
      "Epoch 3997, Loss: 0.23717612028121948, Final Batch Loss: 0.11120840907096863\n",
      "Epoch 3998, Loss: 0.19671431928873062, Final Batch Loss: 0.06598689407110214\n",
      "Epoch 3999, Loss: 0.20981670171022415, Final Batch Loss: 0.11113601922988892\n",
      "Epoch 4000, Loss: 0.2141834795475006, Final Batch Loss: 0.08980385959148407\n",
      "Epoch 4001, Loss: 0.21824049204587936, Final Batch Loss: 0.09293582290410995\n",
      "Epoch 4002, Loss: 0.1961238831281662, Final Batch Loss: 0.10095883160829544\n",
      "Epoch 4003, Loss: 0.19929800927639008, Final Batch Loss: 0.11438557505607605\n",
      "Epoch 4004, Loss: 0.22103261947631836, Final Batch Loss: 0.09031155705451965\n",
      "Epoch 4005, Loss: 0.21146928519010544, Final Batch Loss: 0.10314624756574631\n",
      "Epoch 4006, Loss: 0.21923739463090897, Final Batch Loss: 0.1073351576924324\n",
      "Epoch 4007, Loss: 0.22333445399999619, Final Batch Loss: 0.08795063942670822\n",
      "Epoch 4008, Loss: 0.2052047699689865, Final Batch Loss: 0.1138128861784935\n",
      "Epoch 4009, Loss: 0.2938632369041443, Final Batch Loss: 0.1225946694612503\n",
      "Epoch 4010, Loss: 0.2288685292005539, Final Batch Loss: 0.119191013276577\n",
      "Epoch 4011, Loss: 0.25949156284332275, Final Batch Loss: 0.11985784769058228\n",
      "Epoch 4012, Loss: 0.19445130228996277, Final Batch Loss: 0.09001456201076508\n",
      "Epoch 4013, Loss: 0.20676305890083313, Final Batch Loss: 0.09919417649507523\n",
      "Epoch 4014, Loss: 0.18487047404050827, Final Batch Loss: 0.09435912221670151\n",
      "Epoch 4015, Loss: 0.2215501070022583, Final Batch Loss: 0.1258886158466339\n",
      "Epoch 4016, Loss: 0.23867328464984894, Final Batch Loss: 0.10528627038002014\n",
      "Epoch 4017, Loss: 0.25045399367809296, Final Batch Loss: 0.12704242765903473\n",
      "Epoch 4018, Loss: 0.2219531536102295, Final Batch Loss: 0.12183782458305359\n",
      "Epoch 4019, Loss: 0.2324419990181923, Final Batch Loss: 0.15191367268562317\n",
      "Epoch 4020, Loss: 0.1759885922074318, Final Batch Loss: 0.10535595566034317\n",
      "Epoch 4021, Loss: 0.19945697486400604, Final Batch Loss: 0.08005828410387039\n",
      "Epoch 4022, Loss: 0.21598003804683685, Final Batch Loss: 0.13601820170879364\n",
      "Epoch 4023, Loss: 0.24010422080755234, Final Batch Loss: 0.12251902371644974\n",
      "Epoch 4024, Loss: 0.20087308436632156, Final Batch Loss: 0.10022024810314178\n",
      "Epoch 4025, Loss: 0.1758372262120247, Final Batch Loss: 0.08156149089336395\n",
      "Epoch 4026, Loss: 0.17797137796878815, Final Batch Loss: 0.07159896194934845\n",
      "Epoch 4027, Loss: 0.23710322380065918, Final Batch Loss: 0.10007752478122711\n",
      "Epoch 4028, Loss: 0.18918424099683762, Final Batch Loss: 0.10326360166072845\n",
      "Epoch 4029, Loss: 0.19128172099590302, Final Batch Loss: 0.09420900791883469\n",
      "Epoch 4030, Loss: 0.2467806339263916, Final Batch Loss: 0.10457845032215118\n",
      "Epoch 4031, Loss: 0.1506650075316429, Final Batch Loss: 0.06866737455129623\n",
      "Epoch 4032, Loss: 0.19339962303638458, Final Batch Loss: 0.08702434599399567\n",
      "Epoch 4033, Loss: 0.16973139345645905, Final Batch Loss: 0.05661175400018692\n",
      "Epoch 4034, Loss: 0.24458074569702148, Final Batch Loss: 0.1133304089307785\n",
      "Epoch 4035, Loss: 0.2592489793896675, Final Batch Loss: 0.15714430809020996\n",
      "Epoch 4036, Loss: 0.16451112180948257, Final Batch Loss: 0.04973519593477249\n",
      "Epoch 4037, Loss: 0.20742028206586838, Final Batch Loss: 0.08535997569561005\n",
      "Epoch 4038, Loss: 0.22190620750188828, Final Batch Loss: 0.10740991681814194\n",
      "Epoch 4039, Loss: 0.1927708312869072, Final Batch Loss: 0.11193110048770905\n",
      "Epoch 4040, Loss: 0.24622391164302826, Final Batch Loss: 0.08304911851882935\n",
      "Epoch 4041, Loss: 0.2227194383740425, Final Batch Loss: 0.12299925833940506\n",
      "Epoch 4042, Loss: 0.2317790985107422, Final Batch Loss: 0.13817013800144196\n",
      "Epoch 4043, Loss: 0.22137151658535004, Final Batch Loss: 0.11702389270067215\n",
      "Epoch 4044, Loss: 0.22300195693969727, Final Batch Loss: 0.11917238682508469\n",
      "Epoch 4045, Loss: 0.28826355189085007, Final Batch Loss: 0.17270460724830627\n",
      "Epoch 4046, Loss: 0.2674228772521019, Final Batch Loss: 0.15365639328956604\n",
      "Epoch 4047, Loss: 0.21636775881052017, Final Batch Loss: 0.137619748711586\n",
      "Epoch 4048, Loss: 0.17359234392642975, Final Batch Loss: 0.08162884414196014\n",
      "Epoch 4049, Loss: 0.18346983194351196, Final Batch Loss: 0.06707949191331863\n",
      "Epoch 4050, Loss: 0.2724234387278557, Final Batch Loss: 0.10202931612730026\n",
      "Epoch 4051, Loss: 0.2115311473608017, Final Batch Loss: 0.11248443275690079\n",
      "Epoch 4052, Loss: 0.2324221059679985, Final Batch Loss: 0.07867274433374405\n",
      "Epoch 4053, Loss: 0.2385084182024002, Final Batch Loss: 0.13011614978313446\n",
      "Epoch 4054, Loss: 0.2282278686761856, Final Batch Loss: 0.09935219585895538\n",
      "Epoch 4055, Loss: 0.19982131570577621, Final Batch Loss: 0.08860445022583008\n",
      "Epoch 4056, Loss: 0.20165298879146576, Final Batch Loss: 0.10365603119134903\n",
      "Epoch 4057, Loss: 0.29448410868644714, Final Batch Loss: 0.17977650463581085\n",
      "Epoch 4058, Loss: 0.2907557040452957, Final Batch Loss: 0.14973928034305573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4059, Loss: 0.2695607915520668, Final Batch Loss: 0.17719420790672302\n",
      "Epoch 4060, Loss: 0.2686605006456375, Final Batch Loss: 0.15724815428256989\n",
      "Epoch 4061, Loss: 0.2287389636039734, Final Batch Loss: 0.1481473296880722\n",
      "Epoch 4062, Loss: 0.17802376300096512, Final Batch Loss: 0.09066269546747208\n",
      "Epoch 4063, Loss: 0.20113815367221832, Final Batch Loss: 0.10961273312568665\n",
      "Epoch 4064, Loss: 0.18393423408269882, Final Batch Loss: 0.07369038462638855\n",
      "Epoch 4065, Loss: 0.23120646178722382, Final Batch Loss: 0.10716444998979568\n",
      "Epoch 4066, Loss: 0.24738386273384094, Final Batch Loss: 0.13927064836025238\n",
      "Epoch 4067, Loss: 0.2331554815173149, Final Batch Loss: 0.09651792794466019\n",
      "Epoch 4068, Loss: 0.19352492690086365, Final Batch Loss: 0.10599546134471893\n",
      "Epoch 4069, Loss: 0.22993237525224686, Final Batch Loss: 0.13149435818195343\n",
      "Epoch 4070, Loss: 0.1956857442855835, Final Batch Loss: 0.10666351020336151\n",
      "Epoch 4071, Loss: 0.24116861075162888, Final Batch Loss: 0.12660491466522217\n",
      "Epoch 4072, Loss: 0.15114066004753113, Final Batch Loss: 0.07181240618228912\n",
      "Epoch 4073, Loss: 0.2988153323531151, Final Batch Loss: 0.18511326611042023\n",
      "Epoch 4074, Loss: 0.18393820524215698, Final Batch Loss: 0.09897015988826752\n",
      "Epoch 4075, Loss: 0.15774282813072205, Final Batch Loss: 0.0939507856965065\n",
      "Epoch 4076, Loss: 0.19963371753692627, Final Batch Loss: 0.10554581135511398\n",
      "Epoch 4077, Loss: 0.24259691685438156, Final Batch Loss: 0.1428116261959076\n",
      "Epoch 4078, Loss: 0.21217184513807297, Final Batch Loss: 0.09228518605232239\n",
      "Epoch 4079, Loss: 0.14406811445951462, Final Batch Loss: 0.07009129971265793\n",
      "Epoch 4080, Loss: 0.22780190408229828, Final Batch Loss: 0.09646658599376678\n",
      "Epoch 4081, Loss: 0.19410310685634613, Final Batch Loss: 0.09041833877563477\n",
      "Epoch 4082, Loss: 0.19661811739206314, Final Batch Loss: 0.11655949056148529\n",
      "Epoch 4083, Loss: 0.21786876022815704, Final Batch Loss: 0.1169661283493042\n",
      "Epoch 4084, Loss: 0.21761952340602875, Final Batch Loss: 0.09355354309082031\n",
      "Epoch 4085, Loss: 0.18372497707605362, Final Batch Loss: 0.09652216732501984\n",
      "Epoch 4086, Loss: 0.26434004306793213, Final Batch Loss: 0.13031546771526337\n",
      "Epoch 4087, Loss: 0.20543164014816284, Final Batch Loss: 0.06962563097476959\n",
      "Epoch 4088, Loss: 0.18295225873589516, Final Batch Loss: 0.06201900169253349\n",
      "Epoch 4089, Loss: 0.19238203763961792, Final Batch Loss: 0.11243993788957596\n",
      "Epoch 4090, Loss: 0.18400877714157104, Final Batch Loss: 0.07644733786582947\n",
      "Epoch 4091, Loss: 0.167374886572361, Final Batch Loss: 0.0693146362900734\n",
      "Epoch 4092, Loss: 0.1963680312037468, Final Batch Loss: 0.11290544271469116\n",
      "Epoch 4093, Loss: 0.21235311776399612, Final Batch Loss: 0.10193093121051788\n",
      "Epoch 4094, Loss: 0.2435818910598755, Final Batch Loss: 0.13795596361160278\n",
      "Epoch 4095, Loss: 0.18398397415876389, Final Batch Loss: 0.09955086559057236\n",
      "Epoch 4096, Loss: 0.23060647398233414, Final Batch Loss: 0.11494573205709457\n",
      "Epoch 4097, Loss: 0.20886173844337463, Final Batch Loss: 0.09308809041976929\n",
      "Epoch 4098, Loss: 0.18556968867778778, Final Batch Loss: 0.07213733345270157\n",
      "Epoch 4099, Loss: 0.20648273080587387, Final Batch Loss: 0.10559771955013275\n",
      "Epoch 4100, Loss: 0.1844819188117981, Final Batch Loss: 0.10433512181043625\n",
      "Epoch 4101, Loss: 0.18713155388832092, Final Batch Loss: 0.07729936391115189\n",
      "Epoch 4102, Loss: 0.1928114891052246, Final Batch Loss: 0.0892469733953476\n",
      "Epoch 4103, Loss: 0.19044522941112518, Final Batch Loss: 0.08674861490726471\n",
      "Epoch 4104, Loss: 0.18956802040338516, Final Batch Loss: 0.10329677164554596\n",
      "Epoch 4105, Loss: 0.16486964374780655, Final Batch Loss: 0.07548993080854416\n",
      "Epoch 4106, Loss: 0.22227444499731064, Final Batch Loss: 0.137497678399086\n",
      "Epoch 4107, Loss: 0.14952034503221512, Final Batch Loss: 0.05734831839799881\n",
      "Epoch 4108, Loss: 0.20795902609825134, Final Batch Loss: 0.11069745570421219\n",
      "Epoch 4109, Loss: 0.17834854125976562, Final Batch Loss: 0.07860486209392548\n",
      "Epoch 4110, Loss: 0.195542111992836, Final Batch Loss: 0.0997922345995903\n",
      "Epoch 4111, Loss: 0.24284417182207108, Final Batch Loss: 0.14668914675712585\n",
      "Epoch 4112, Loss: 0.20666777342557907, Final Batch Loss: 0.06757832318544388\n",
      "Epoch 4113, Loss: 0.16159551590681076, Final Batch Loss: 0.06309003382921219\n",
      "Epoch 4114, Loss: 0.1630544811487198, Final Batch Loss: 0.09030977636575699\n",
      "Epoch 4115, Loss: 0.1961756870150566, Final Batch Loss: 0.08724403381347656\n",
      "Epoch 4116, Loss: 0.18996349722146988, Final Batch Loss: 0.08895569294691086\n",
      "Epoch 4117, Loss: 0.2256888449192047, Final Batch Loss: 0.11069352924823761\n",
      "Epoch 4118, Loss: 0.19864599406719208, Final Batch Loss: 0.11020302027463913\n",
      "Epoch 4119, Loss: 0.18888839334249496, Final Batch Loss: 0.06437968462705612\n",
      "Epoch 4120, Loss: 0.2182730808854103, Final Batch Loss: 0.09319557994604111\n",
      "Epoch 4121, Loss: 0.20093847066164017, Final Batch Loss: 0.06942019611597061\n",
      "Epoch 4122, Loss: 0.14938192442059517, Final Batch Loss: 0.062272388488054276\n",
      "Epoch 4123, Loss: 0.16476835310459137, Final Batch Loss: 0.06460090726613998\n",
      "Epoch 4124, Loss: 0.2194923460483551, Final Batch Loss: 0.1474333554506302\n",
      "Epoch 4125, Loss: 0.25346455723047256, Final Batch Loss: 0.12226537615060806\n",
      "Epoch 4126, Loss: 0.20933546870946884, Final Batch Loss: 0.10885833948850632\n",
      "Epoch 4127, Loss: 0.18525247275829315, Final Batch Loss: 0.1041022315621376\n",
      "Epoch 4128, Loss: 0.16547488421201706, Final Batch Loss: 0.08830135315656662\n",
      "Epoch 4129, Loss: 0.1488281786441803, Final Batch Loss: 0.06247576326131821\n",
      "Epoch 4130, Loss: 0.2594381794333458, Final Batch Loss: 0.12170500308275223\n",
      "Epoch 4131, Loss: 0.2713964581489563, Final Batch Loss: 0.15792883932590485\n",
      "Epoch 4132, Loss: 0.20582681894302368, Final Batch Loss: 0.14315088093280792\n",
      "Epoch 4133, Loss: 0.17713363468647003, Final Batch Loss: 0.10999953746795654\n",
      "Epoch 4134, Loss: 0.17248063534498215, Final Batch Loss: 0.09979564696550369\n",
      "Epoch 4135, Loss: 0.1885438859462738, Final Batch Loss: 0.10454593598842621\n",
      "Epoch 4136, Loss: 0.22917498648166656, Final Batch Loss: 0.07917489111423492\n",
      "Epoch 4137, Loss: 0.18435929715633392, Final Batch Loss: 0.12280285358428955\n",
      "Epoch 4138, Loss: 0.2116067335009575, Final Batch Loss: 0.07786337286233902\n",
      "Epoch 4139, Loss: 0.21726133674383163, Final Batch Loss: 0.09767819195985794\n",
      "Epoch 4140, Loss: 0.22541026026010513, Final Batch Loss: 0.09100057929754257\n",
      "Epoch 4141, Loss: 0.19265767186880112, Final Batch Loss: 0.08638298511505127\n",
      "Epoch 4142, Loss: 0.17337475717067719, Final Batch Loss: 0.08312886208295822\n",
      "Epoch 4143, Loss: 0.1959013268351555, Final Batch Loss: 0.0798400267958641\n",
      "Epoch 4144, Loss: 0.1623627319931984, Final Batch Loss: 0.073927141726017\n",
      "Epoch 4145, Loss: 0.24878961592912674, Final Batch Loss: 0.09467227011919022\n",
      "Epoch 4146, Loss: 0.20108918845653534, Final Batch Loss: 0.08895879983901978\n",
      "Epoch 4147, Loss: 0.31699907034635544, Final Batch Loss: 0.19555680453777313\n",
      "Epoch 4148, Loss: 0.2005307972431183, Final Batch Loss: 0.10282684117555618\n",
      "Epoch 4149, Loss: 0.2419431284070015, Final Batch Loss: 0.11733178049325943\n",
      "Epoch 4150, Loss: 0.29177479445934296, Final Batch Loss: 0.11161889135837555\n",
      "Epoch 4151, Loss: 0.21023869514465332, Final Batch Loss: 0.13820883631706238\n",
      "Epoch 4152, Loss: 0.2250835821032524, Final Batch Loss: 0.1217140331864357\n",
      "Epoch 4153, Loss: 0.20489338040351868, Final Batch Loss: 0.0910380408167839\n",
      "Epoch 4154, Loss: 0.22801973670721054, Final Batch Loss: 0.11922869831323624\n",
      "Epoch 4155, Loss: 0.1971014067530632, Final Batch Loss: 0.1247616857290268\n",
      "Epoch 4156, Loss: 0.24365848302841187, Final Batch Loss: 0.12960004806518555\n",
      "Epoch 4157, Loss: 0.24576839804649353, Final Batch Loss: 0.13543277978897095\n",
      "Epoch 4158, Loss: 0.1760346218943596, Final Batch Loss: 0.10328975319862366\n",
      "Epoch 4159, Loss: 0.2128918617963791, Final Batch Loss: 0.11980060487985611\n",
      "Epoch 4160, Loss: 0.19300372898578644, Final Batch Loss: 0.11890330910682678\n",
      "Epoch 4161, Loss: 0.18059417605400085, Final Batch Loss: 0.09393500536680222\n",
      "Epoch 4162, Loss: 0.2634813040494919, Final Batch Loss: 0.09608960151672363\n",
      "Epoch 4163, Loss: 0.19588974863290787, Final Batch Loss: 0.08601706475019455\n",
      "Epoch 4164, Loss: 0.1903461292386055, Final Batch Loss: 0.09569178521633148\n",
      "Epoch 4165, Loss: 0.2465449571609497, Final Batch Loss: 0.12982702255249023\n",
      "Epoch 4166, Loss: 0.19519906491041183, Final Batch Loss: 0.07625890523195267\n",
      "Epoch 4167, Loss: 0.2626650333404541, Final Batch Loss: 0.12892945110797882\n",
      "Epoch 4168, Loss: 0.229323111474514, Final Batch Loss: 0.14044396579265594\n",
      "Epoch 4169, Loss: 0.2384527400135994, Final Batch Loss: 0.11530745774507523\n",
      "Epoch 4170, Loss: 0.24483483284711838, Final Batch Loss: 0.11649493128061295\n",
      "Epoch 4171, Loss: 0.22070696204900742, Final Batch Loss: 0.08845312148332596\n",
      "Epoch 4172, Loss: 0.23991727828979492, Final Batch Loss: 0.1120995283126831\n",
      "Epoch 4173, Loss: 0.26093775779008865, Final Batch Loss: 0.13727211952209473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4174, Loss: 0.1601208820939064, Final Batch Loss: 0.08479275554418564\n",
      "Epoch 4175, Loss: 0.1752643957734108, Final Batch Loss: 0.07767137140035629\n",
      "Epoch 4176, Loss: 0.2727896720170975, Final Batch Loss: 0.14381225407123566\n",
      "Epoch 4177, Loss: 0.2428603544831276, Final Batch Loss: 0.08818113058805466\n",
      "Epoch 4178, Loss: 0.18952252715826035, Final Batch Loss: 0.10394711047410965\n",
      "Epoch 4179, Loss: 0.20984070003032684, Final Batch Loss: 0.08500539511442184\n",
      "Epoch 4180, Loss: 0.23521418869495392, Final Batch Loss: 0.10250137746334076\n",
      "Epoch 4181, Loss: 0.2500186413526535, Final Batch Loss: 0.1003512293100357\n",
      "Epoch 4182, Loss: 0.191641204059124, Final Batch Loss: 0.08798442780971527\n",
      "Epoch 4183, Loss: 0.15086696296930313, Final Batch Loss: 0.07019005715847015\n",
      "Epoch 4184, Loss: 0.22051449120044708, Final Batch Loss: 0.1128244623541832\n",
      "Epoch 4185, Loss: 0.24119746685028076, Final Batch Loss: 0.13216009736061096\n",
      "Epoch 4186, Loss: 0.23921848088502884, Final Batch Loss: 0.10873889178037643\n",
      "Epoch 4187, Loss: 0.18954256922006607, Final Batch Loss: 0.08140654116868973\n",
      "Epoch 4188, Loss: 0.16905048489570618, Final Batch Loss: 0.09632540494203568\n",
      "Epoch 4189, Loss: 0.2362646386027336, Final Batch Loss: 0.16970179975032806\n",
      "Epoch 4190, Loss: 0.20412132889032364, Final Batch Loss: 0.09848091006278992\n",
      "Epoch 4191, Loss: 0.26269152760505676, Final Batch Loss: 0.11035566031932831\n",
      "Epoch 4192, Loss: 0.17348003387451172, Final Batch Loss: 0.07801906019449234\n",
      "Epoch 4193, Loss: 0.2826041430234909, Final Batch Loss: 0.16904670000076294\n",
      "Epoch 4194, Loss: 0.1689533069729805, Final Batch Loss: 0.09897806495428085\n",
      "Epoch 4195, Loss: 0.17281978577375412, Final Batch Loss: 0.08454372733831406\n",
      "Epoch 4196, Loss: 0.23389994353055954, Final Batch Loss: 0.11246515065431595\n",
      "Epoch 4197, Loss: 0.20498451590538025, Final Batch Loss: 0.07750028371810913\n",
      "Epoch 4198, Loss: 0.17010273039340973, Final Batch Loss: 0.06544409692287445\n",
      "Epoch 4199, Loss: 0.28218793869018555, Final Batch Loss: 0.12169606983661652\n",
      "Epoch 4200, Loss: 0.2445666864514351, Final Batch Loss: 0.10801567882299423\n",
      "Epoch 4201, Loss: 0.2270057052373886, Final Batch Loss: 0.1211857795715332\n",
      "Epoch 4202, Loss: 0.20564109832048416, Final Batch Loss: 0.10734438896179199\n",
      "Epoch 4203, Loss: 0.1789543330669403, Final Batch Loss: 0.08711826801300049\n",
      "Epoch 4204, Loss: 0.17840592563152313, Final Batch Loss: 0.07499684393405914\n",
      "Epoch 4205, Loss: 0.2277737408876419, Final Batch Loss: 0.11590548604726791\n",
      "Epoch 4206, Loss: 0.17159713059663773, Final Batch Loss: 0.0932043269276619\n",
      "Epoch 4207, Loss: 0.1730620563030243, Final Batch Loss: 0.0842169001698494\n",
      "Epoch 4208, Loss: 0.1786351352930069, Final Batch Loss: 0.08077321946620941\n",
      "Epoch 4209, Loss: 0.1973532810807228, Final Batch Loss: 0.11303539574146271\n",
      "Epoch 4210, Loss: 0.17229358851909637, Final Batch Loss: 0.0811539739370346\n",
      "Epoch 4211, Loss: 0.20065266638994217, Final Batch Loss: 0.07652381807565689\n",
      "Epoch 4212, Loss: 0.2614392265677452, Final Batch Loss: 0.14515772461891174\n",
      "Epoch 4213, Loss: 0.169536255300045, Final Batch Loss: 0.0734371766448021\n",
      "Epoch 4214, Loss: 0.24665648490190506, Final Batch Loss: 0.12062828987836838\n",
      "Epoch 4215, Loss: 0.21698644757270813, Final Batch Loss: 0.10867596417665482\n",
      "Epoch 4216, Loss: 0.2244253158569336, Final Batch Loss: 0.11130864918231964\n",
      "Epoch 4217, Loss: 0.19430557638406754, Final Batch Loss: 0.10442731529474258\n",
      "Epoch 4218, Loss: 0.21729225665330887, Final Batch Loss: 0.10824479162693024\n",
      "Epoch 4219, Loss: 0.17617281526327133, Final Batch Loss: 0.0938565656542778\n",
      "Epoch 4220, Loss: 0.17166151106357574, Final Batch Loss: 0.08075011521577835\n",
      "Epoch 4221, Loss: 0.18409063667058945, Final Batch Loss: 0.10297616571187973\n",
      "Epoch 4222, Loss: 0.17087942361831665, Final Batch Loss: 0.08442612737417221\n",
      "Epoch 4223, Loss: 0.16605355590581894, Final Batch Loss: 0.0885857343673706\n",
      "Epoch 4224, Loss: 0.20396603643894196, Final Batch Loss: 0.09612848609685898\n",
      "Epoch 4225, Loss: 0.20053522288799286, Final Batch Loss: 0.11946713179349899\n",
      "Epoch 4226, Loss: 0.17758562415838242, Final Batch Loss: 0.09702175855636597\n",
      "Epoch 4227, Loss: 0.22003234177827835, Final Batch Loss: 0.09541846811771393\n",
      "Epoch 4228, Loss: 0.15982282161712646, Final Batch Loss: 0.08487388491630554\n",
      "Epoch 4229, Loss: 0.19287676364183426, Final Batch Loss: 0.11790371686220169\n",
      "Epoch 4230, Loss: 0.21441006660461426, Final Batch Loss: 0.1127641499042511\n",
      "Epoch 4231, Loss: 0.21123717725276947, Final Batch Loss: 0.13641834259033203\n",
      "Epoch 4232, Loss: 0.19364919513463974, Final Batch Loss: 0.09941098839044571\n",
      "Epoch 4233, Loss: 0.2101411670446396, Final Batch Loss: 0.11454585194587708\n",
      "Epoch 4234, Loss: 0.2109125256538391, Final Batch Loss: 0.11368409544229507\n",
      "Epoch 4235, Loss: 0.18633978068828583, Final Batch Loss: 0.09561145305633545\n",
      "Epoch 4236, Loss: 0.17123306542634964, Final Batch Loss: 0.05537977069616318\n",
      "Epoch 4237, Loss: 0.2213025912642479, Final Batch Loss: 0.10419879108667374\n",
      "Epoch 4238, Loss: 0.3203236162662506, Final Batch Loss: 0.1605810970067978\n",
      "Epoch 4239, Loss: 0.19161570072174072, Final Batch Loss: 0.10350284725427628\n",
      "Epoch 4240, Loss: 0.22275716066360474, Final Batch Loss: 0.12810423970222473\n",
      "Epoch 4241, Loss: 0.1736171543598175, Final Batch Loss: 0.09312774240970612\n",
      "Epoch 4242, Loss: 0.18007000535726547, Final Batch Loss: 0.08593425154685974\n",
      "Epoch 4243, Loss: 0.25691550970077515, Final Batch Loss: 0.11571837961673737\n",
      "Epoch 4244, Loss: 0.24963415414094925, Final Batch Loss: 0.09792772680521011\n",
      "Epoch 4245, Loss: 0.22237087041139603, Final Batch Loss: 0.08703301101922989\n",
      "Epoch 4246, Loss: 0.20023543387651443, Final Batch Loss: 0.08078703284263611\n",
      "Epoch 4247, Loss: 0.23915301263332367, Final Batch Loss: 0.08211424946784973\n",
      "Epoch 4248, Loss: 0.20073361694812775, Final Batch Loss: 0.09811589121818542\n",
      "Epoch 4249, Loss: 0.19611195474863052, Final Batch Loss: 0.1021830141544342\n",
      "Epoch 4250, Loss: 0.271840363740921, Final Batch Loss: 0.15428423881530762\n",
      "Epoch 4251, Loss: 0.16924303025007248, Final Batch Loss: 0.09431283921003342\n",
      "Epoch 4252, Loss: 0.23897572606801987, Final Batch Loss: 0.12154584378004074\n",
      "Epoch 4253, Loss: 0.20133419334888458, Final Batch Loss: 0.10253375768661499\n",
      "Epoch 4254, Loss: 0.20673233270645142, Final Batch Loss: 0.10128826647996902\n",
      "Epoch 4255, Loss: 0.19113530963659286, Final Batch Loss: 0.10408765822649002\n",
      "Epoch 4256, Loss: 0.1563180387020111, Final Batch Loss: 0.06317824125289917\n",
      "Epoch 4257, Loss: 0.29246800392866135, Final Batch Loss: 0.1919509470462799\n",
      "Epoch 4258, Loss: 0.1837574765086174, Final Batch Loss: 0.0839158147573471\n",
      "Epoch 4259, Loss: 0.22195393592119217, Final Batch Loss: 0.13461360335350037\n",
      "Epoch 4260, Loss: 0.1769179031252861, Final Batch Loss: 0.07655129581689835\n",
      "Epoch 4261, Loss: 0.1630016416311264, Final Batch Loss: 0.08104323595762253\n",
      "Epoch 4262, Loss: 0.19680988788604736, Final Batch Loss: 0.08957689255475998\n",
      "Epoch 4263, Loss: 0.24406220763921738, Final Batch Loss: 0.16111144423484802\n",
      "Epoch 4264, Loss: 0.20121347904205322, Final Batch Loss: 0.13376513123512268\n",
      "Epoch 4265, Loss: 0.19961480051279068, Final Batch Loss: 0.13019070029258728\n",
      "Epoch 4266, Loss: 0.2253701388835907, Final Batch Loss: 0.09158225357532501\n",
      "Epoch 4267, Loss: 0.22527874261140823, Final Batch Loss: 0.12765835225582123\n",
      "Epoch 4268, Loss: 0.18137908726930618, Final Batch Loss: 0.09684327244758606\n",
      "Epoch 4269, Loss: 0.16650081425905228, Final Batch Loss: 0.08421000093221664\n",
      "Epoch 4270, Loss: 0.215962216258049, Final Batch Loss: 0.10651539266109467\n",
      "Epoch 4271, Loss: 0.19237056374549866, Final Batch Loss: 0.11980985850095749\n",
      "Epoch 4272, Loss: 0.19256839156150818, Final Batch Loss: 0.10951761901378632\n",
      "Epoch 4273, Loss: 0.22088727355003357, Final Batch Loss: 0.13033223152160645\n",
      "Epoch 4274, Loss: 0.20977282524108887, Final Batch Loss: 0.11186477541923523\n",
      "Epoch 4275, Loss: 0.22941066324710846, Final Batch Loss: 0.11978355050086975\n",
      "Epoch 4276, Loss: 0.20886589586734772, Final Batch Loss: 0.1176389753818512\n",
      "Epoch 4277, Loss: 0.16244690120220184, Final Batch Loss: 0.08038811385631561\n",
      "Epoch 4278, Loss: 0.1777568832039833, Final Batch Loss: 0.09108597040176392\n",
      "Epoch 4279, Loss: 0.17653826624155045, Final Batch Loss: 0.07271283119916916\n",
      "Epoch 4280, Loss: 0.19269949197769165, Final Batch Loss: 0.08309269696474075\n",
      "Epoch 4281, Loss: 0.16909854859113693, Final Batch Loss: 0.07795275002717972\n",
      "Epoch 4282, Loss: 0.23281999677419662, Final Batch Loss: 0.13561619818210602\n",
      "Epoch 4283, Loss: 0.15770657360553741, Final Batch Loss: 0.09272364526987076\n",
      "Epoch 4284, Loss: 0.22847244888544083, Final Batch Loss: 0.11928721517324448\n",
      "Epoch 4285, Loss: 0.22429227083921432, Final Batch Loss: 0.154861181974411\n",
      "Epoch 4286, Loss: 0.18921532481908798, Final Batch Loss: 0.0930996760725975\n",
      "Epoch 4287, Loss: 0.23849070072174072, Final Batch Loss: 0.10880483686923981\n",
      "Epoch 4288, Loss: 0.17589343339204788, Final Batch Loss: 0.09281138330698013\n",
      "Epoch 4289, Loss: 0.2311205342411995, Final Batch Loss: 0.14219418168067932\n",
      "Epoch 4290, Loss: 0.19622591137886047, Final Batch Loss: 0.1275988668203354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4291, Loss: 0.18921218067407608, Final Batch Loss: 0.08995375037193298\n",
      "Epoch 4292, Loss: 0.2741275578737259, Final Batch Loss: 0.13523197174072266\n",
      "Epoch 4293, Loss: 0.2255922257900238, Final Batch Loss: 0.12094948440790176\n",
      "Epoch 4294, Loss: 0.20385073125362396, Final Batch Loss: 0.09876798093318939\n",
      "Epoch 4295, Loss: 0.168556809425354, Final Batch Loss: 0.06298995763063431\n",
      "Epoch 4296, Loss: 0.1868097335100174, Final Batch Loss: 0.07261110097169876\n",
      "Epoch 4297, Loss: 0.2101231887936592, Final Batch Loss: 0.09629864990711212\n",
      "Epoch 4298, Loss: 0.19019784033298492, Final Batch Loss: 0.09074543416500092\n",
      "Epoch 4299, Loss: 0.2026345133781433, Final Batch Loss: 0.12210948765277863\n",
      "Epoch 4300, Loss: 0.19204805046319962, Final Batch Loss: 0.08751161396503448\n",
      "Epoch 4301, Loss: 0.19696424156427383, Final Batch Loss: 0.06950251013040543\n",
      "Epoch 4302, Loss: 0.2054816037416458, Final Batch Loss: 0.11923743784427643\n",
      "Epoch 4303, Loss: 0.20923857390880585, Final Batch Loss: 0.114964559674263\n",
      "Epoch 4304, Loss: 0.18148411065340042, Final Batch Loss: 0.08705312758684158\n",
      "Epoch 4305, Loss: 0.21103515475988388, Final Batch Loss: 0.08093885332345963\n",
      "Epoch 4306, Loss: 0.2016621008515358, Final Batch Loss: 0.1083950400352478\n",
      "Epoch 4307, Loss: 0.18701615929603577, Final Batch Loss: 0.09067684412002563\n",
      "Epoch 4308, Loss: 0.1996413767337799, Final Batch Loss: 0.09808997809886932\n",
      "Epoch 4309, Loss: 0.17815948277711868, Final Batch Loss: 0.09247506409883499\n",
      "Epoch 4310, Loss: 0.2097945734858513, Final Batch Loss: 0.0968649759888649\n",
      "Epoch 4311, Loss: 0.2356909140944481, Final Batch Loss: 0.10212992876768112\n",
      "Epoch 4312, Loss: 0.18385891616344452, Final Batch Loss: 0.07849098742008209\n",
      "Epoch 4313, Loss: 0.20680493116378784, Final Batch Loss: 0.0974353775382042\n",
      "Epoch 4314, Loss: 0.17491531372070312, Final Batch Loss: 0.07682735472917557\n",
      "Epoch 4315, Loss: 0.21517948061227798, Final Batch Loss: 0.104324109852314\n",
      "Epoch 4316, Loss: 0.1948777362704277, Final Batch Loss: 0.1051488071680069\n",
      "Epoch 4317, Loss: 0.16792837530374527, Final Batch Loss: 0.08151209354400635\n",
      "Epoch 4318, Loss: 0.17366226017475128, Final Batch Loss: 0.08857886493206024\n",
      "Epoch 4319, Loss: 0.18175305426120758, Final Batch Loss: 0.07408934086561203\n",
      "Epoch 4320, Loss: 0.1964060217142105, Final Batch Loss: 0.0932142436504364\n",
      "Epoch 4321, Loss: 0.16580037772655487, Final Batch Loss: 0.054716795682907104\n",
      "Epoch 4322, Loss: 0.1947600096464157, Final Batch Loss: 0.07713787257671356\n",
      "Epoch 4323, Loss: 0.19722574204206467, Final Batch Loss: 0.09713347256183624\n",
      "Epoch 4324, Loss: 0.28356926143169403, Final Batch Loss: 0.1429995447397232\n",
      "Epoch 4325, Loss: 0.20149988681077957, Final Batch Loss: 0.10861841589212418\n",
      "Epoch 4326, Loss: 0.2535500004887581, Final Batch Loss: 0.10190374404191971\n",
      "Epoch 4327, Loss: 0.16704219579696655, Final Batch Loss: 0.07037796825170517\n",
      "Epoch 4328, Loss: 0.16930131614208221, Final Batch Loss: 0.10002856701612473\n",
      "Epoch 4329, Loss: 0.26613112539052963, Final Batch Loss: 0.12257952243089676\n",
      "Epoch 4330, Loss: 0.20106758922338486, Final Batch Loss: 0.11595143377780914\n",
      "Epoch 4331, Loss: 0.16089677065610886, Final Batch Loss: 0.07098043709993362\n",
      "Epoch 4332, Loss: 0.14347460865974426, Final Batch Loss: 0.07594408094882965\n",
      "Epoch 4333, Loss: 0.21986035257577896, Final Batch Loss: 0.14209458231925964\n",
      "Epoch 4334, Loss: 0.18222059309482574, Final Batch Loss: 0.10637624561786652\n",
      "Epoch 4335, Loss: 0.18350734561681747, Final Batch Loss: 0.11194214969873428\n",
      "Epoch 4336, Loss: 0.17814305424690247, Final Batch Loss: 0.10103131830692291\n",
      "Epoch 4337, Loss: 0.23018484562635422, Final Batch Loss: 0.0740029439330101\n",
      "Epoch 4338, Loss: 0.2063533514738083, Final Batch Loss: 0.13281576335430145\n",
      "Epoch 4339, Loss: 0.21217361092567444, Final Batch Loss: 0.08578591048717499\n",
      "Epoch 4340, Loss: 0.1643705517053604, Final Batch Loss: 0.06717163324356079\n",
      "Epoch 4341, Loss: 0.17303908616304398, Final Batch Loss: 0.08450417220592499\n",
      "Epoch 4342, Loss: 0.22598673403263092, Final Batch Loss: 0.1470988690853119\n",
      "Epoch 4343, Loss: 0.17682261765003204, Final Batch Loss: 0.06400085240602493\n",
      "Epoch 4344, Loss: 0.19015982747077942, Final Batch Loss: 0.08355820178985596\n",
      "Epoch 4345, Loss: 0.16971010714769363, Final Batch Loss: 0.07883866131305695\n",
      "Epoch 4346, Loss: 0.18650350719690323, Final Batch Loss: 0.10855113714933395\n",
      "Epoch 4347, Loss: 0.15982753038406372, Final Batch Loss: 0.08243915438652039\n",
      "Epoch 4348, Loss: 0.22826170176267624, Final Batch Loss: 0.081643246114254\n",
      "Epoch 4349, Loss: 0.2238256335258484, Final Batch Loss: 0.07982009649276733\n",
      "Epoch 4350, Loss: 0.16903936117887497, Final Batch Loss: 0.09827315807342529\n",
      "Epoch 4351, Loss: 0.17256682366132736, Final Batch Loss: 0.10244868695735931\n",
      "Epoch 4352, Loss: 0.2109404057264328, Final Batch Loss: 0.08648935705423355\n",
      "Epoch 4353, Loss: 0.2704150900244713, Final Batch Loss: 0.1688564270734787\n",
      "Epoch 4354, Loss: 0.17389874905347824, Final Batch Loss: 0.07244021445512772\n",
      "Epoch 4355, Loss: 0.16404269263148308, Final Batch Loss: 0.10564032942056656\n",
      "Epoch 4356, Loss: 0.19917140901088715, Final Batch Loss: 0.0956159383058548\n",
      "Epoch 4357, Loss: 0.18391788750886917, Final Batch Loss: 0.06999342888593674\n",
      "Epoch 4358, Loss: 0.2361830547451973, Final Batch Loss: 0.08732525259256363\n",
      "Epoch 4359, Loss: 0.355757474899292, Final Batch Loss: 0.24203179776668549\n",
      "Epoch 4360, Loss: 0.22903725504875183, Final Batch Loss: 0.10689210891723633\n",
      "Epoch 4361, Loss: 0.2000134363770485, Final Batch Loss: 0.0929710641503334\n",
      "Epoch 4362, Loss: 0.21400584280490875, Final Batch Loss: 0.10481453686952591\n",
      "Epoch 4363, Loss: 0.2050669565796852, Final Batch Loss: 0.08661706000566483\n",
      "Epoch 4364, Loss: 0.20227168500423431, Final Batch Loss: 0.11567328870296478\n",
      "Epoch 4365, Loss: 0.19997264444828033, Final Batch Loss: 0.11213619261980057\n",
      "Epoch 4366, Loss: 0.22738897800445557, Final Batch Loss: 0.1375703066587448\n",
      "Epoch 4367, Loss: 0.16617468744516373, Final Batch Loss: 0.09319795668125153\n",
      "Epoch 4368, Loss: 0.2151639312505722, Final Batch Loss: 0.09054355323314667\n",
      "Epoch 4369, Loss: 0.2230251431465149, Final Batch Loss: 0.11324506998062134\n",
      "Epoch 4370, Loss: 0.19374661892652512, Final Batch Loss: 0.11344439536333084\n",
      "Epoch 4371, Loss: 0.21131296455860138, Final Batch Loss: 0.1312938779592514\n",
      "Epoch 4372, Loss: 0.2378823533654213, Final Batch Loss: 0.10385368019342422\n",
      "Epoch 4373, Loss: 0.2088337019085884, Final Batch Loss: 0.08634760230779648\n",
      "Epoch 4374, Loss: 0.20664270967245102, Final Batch Loss: 0.0947871133685112\n",
      "Epoch 4375, Loss: 0.2401595115661621, Final Batch Loss: 0.11003315448760986\n",
      "Epoch 4376, Loss: 0.19680865854024887, Final Batch Loss: 0.09797526150941849\n",
      "Epoch 4377, Loss: 0.20598720014095306, Final Batch Loss: 0.09385579079389572\n",
      "Epoch 4378, Loss: 0.17822737991809845, Final Batch Loss: 0.07847955077886581\n",
      "Epoch 4379, Loss: 0.16456032544374466, Final Batch Loss: 0.09058958292007446\n",
      "Epoch 4380, Loss: 0.1662478893995285, Final Batch Loss: 0.09174278378486633\n",
      "Epoch 4381, Loss: 0.20068144798278809, Final Batch Loss: 0.08126827329397202\n",
      "Epoch 4382, Loss: 0.20273710042238235, Final Batch Loss: 0.11351059377193451\n",
      "Epoch 4383, Loss: 0.16404223442077637, Final Batch Loss: 0.0845983549952507\n",
      "Epoch 4384, Loss: 0.2172037959098816, Final Batch Loss: 0.14346972107887268\n",
      "Epoch 4385, Loss: 0.20201483368873596, Final Batch Loss: 0.08731555193662643\n",
      "Epoch 4386, Loss: 0.1735847368836403, Final Batch Loss: 0.08729542046785355\n",
      "Epoch 4387, Loss: 0.22712133824825287, Final Batch Loss: 0.13409863412380219\n",
      "Epoch 4388, Loss: 0.18182643502950668, Final Batch Loss: 0.09507261961698532\n",
      "Epoch 4389, Loss: 0.19456485658884048, Final Batch Loss: 0.09070081263780594\n",
      "Epoch 4390, Loss: 0.1922527626156807, Final Batch Loss: 0.1024768054485321\n",
      "Epoch 4391, Loss: 0.17578336596488953, Final Batch Loss: 0.08109698444604874\n",
      "Epoch 4392, Loss: 0.21270611137151718, Final Batch Loss: 0.111861452460289\n",
      "Epoch 4393, Loss: 0.18677987903356552, Final Batch Loss: 0.1140386164188385\n",
      "Epoch 4394, Loss: 0.20346108824014664, Final Batch Loss: 0.11152283847332001\n",
      "Epoch 4395, Loss: 0.19272547215223312, Final Batch Loss: 0.10797999054193497\n",
      "Epoch 4396, Loss: 0.19039686024188995, Final Batch Loss: 0.11839184165000916\n",
      "Epoch 4397, Loss: 0.17381184548139572, Final Batch Loss: 0.0888710618019104\n",
      "Epoch 4398, Loss: 0.1852768212556839, Final Batch Loss: 0.0998033732175827\n",
      "Epoch 4399, Loss: 0.19569212198257446, Final Batch Loss: 0.1123851090669632\n",
      "Epoch 4400, Loss: 0.21465791761875153, Final Batch Loss: 0.12120351940393448\n",
      "Epoch 4401, Loss: 0.1449926272034645, Final Batch Loss: 0.06683652102947235\n",
      "Epoch 4402, Loss: 0.24922548979520798, Final Batch Loss: 0.11329721659421921\n",
      "Epoch 4403, Loss: 0.2520512714982033, Final Batch Loss: 0.1513797789812088\n",
      "Epoch 4404, Loss: 0.1943424753844738, Final Batch Loss: 0.13396292924880981\n",
      "Epoch 4405, Loss: 0.1856543868780136, Final Batch Loss: 0.10531783849000931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4406, Loss: 0.2139115110039711, Final Batch Loss: 0.12842020392417908\n",
      "Epoch 4407, Loss: 0.20871637761592865, Final Batch Loss: 0.08181199431419373\n",
      "Epoch 4408, Loss: 0.20539335906505585, Final Batch Loss: 0.09237633645534515\n",
      "Epoch 4409, Loss: 0.22394836694002151, Final Batch Loss: 0.10509897023439407\n",
      "Epoch 4410, Loss: 0.15431901812553406, Final Batch Loss: 0.06785985827445984\n",
      "Epoch 4411, Loss: 0.1882471777498722, Final Batch Loss: 0.057287994772195816\n",
      "Epoch 4412, Loss: 0.2417389452457428, Final Batch Loss: 0.09803387522697449\n",
      "Epoch 4413, Loss: 0.2197195142507553, Final Batch Loss: 0.11218562722206116\n",
      "Epoch 4414, Loss: 0.21553116291761398, Final Batch Loss: 0.09660731256008148\n",
      "Epoch 4415, Loss: 0.20945563167333603, Final Batch Loss: 0.09657447785139084\n",
      "Epoch 4416, Loss: 0.20624800026416779, Final Batch Loss: 0.1251566857099533\n",
      "Epoch 4417, Loss: 0.2597530260682106, Final Batch Loss: 0.16237476468086243\n",
      "Epoch 4418, Loss: 0.17576471716165543, Final Batch Loss: 0.07011570781469345\n",
      "Epoch 4419, Loss: 0.23216629773378372, Final Batch Loss: 0.1176508218050003\n",
      "Epoch 4420, Loss: 0.20419730246067047, Final Batch Loss: 0.11661084741353989\n",
      "Epoch 4421, Loss: 0.1886848285794258, Final Batch Loss: 0.08401784300804138\n",
      "Epoch 4422, Loss: 0.16662468016147614, Final Batch Loss: 0.06422826647758484\n",
      "Epoch 4423, Loss: 0.14663363248109818, Final Batch Loss: 0.07689327001571655\n",
      "Epoch 4424, Loss: 0.14256948232650757, Final Batch Loss: 0.06962757557630539\n",
      "Epoch 4425, Loss: 0.21783367544412613, Final Batch Loss: 0.11284178495407104\n",
      "Epoch 4426, Loss: 0.17309297621250153, Final Batch Loss: 0.09573061764240265\n",
      "Epoch 4427, Loss: 0.16980710625648499, Final Batch Loss: 0.07596611231565475\n",
      "Epoch 4428, Loss: 0.20039362460374832, Final Batch Loss: 0.10122013837099075\n",
      "Epoch 4429, Loss: 0.21940773725509644, Final Batch Loss: 0.06241196393966675\n",
      "Epoch 4430, Loss: 0.16272477060556412, Final Batch Loss: 0.08430787175893784\n",
      "Epoch 4431, Loss: 0.16663921624422073, Final Batch Loss: 0.09261879324913025\n",
      "Epoch 4432, Loss: 0.1835329458117485, Final Batch Loss: 0.0795300081372261\n",
      "Epoch 4433, Loss: 0.18591274321079254, Final Batch Loss: 0.11105260998010635\n",
      "Epoch 4434, Loss: 0.17997631430625916, Final Batch Loss: 0.0670694038271904\n",
      "Epoch 4435, Loss: 0.17671027034521103, Final Batch Loss: 0.09366248548030853\n",
      "Epoch 4436, Loss: 0.18317876011133194, Final Batch Loss: 0.10779383033514023\n",
      "Epoch 4437, Loss: 0.23660504072904587, Final Batch Loss: 0.11048094183206558\n",
      "Epoch 4438, Loss: 0.15507715195417404, Final Batch Loss: 0.06695283949375153\n",
      "Epoch 4439, Loss: 0.2991866320371628, Final Batch Loss: 0.18382138013839722\n",
      "Epoch 4440, Loss: 0.23200824111700058, Final Batch Loss: 0.1425042301416397\n",
      "Epoch 4441, Loss: 0.20234400033950806, Final Batch Loss: 0.11227140575647354\n",
      "Epoch 4442, Loss: 0.16822536289691925, Final Batch Loss: 0.06700392812490463\n",
      "Epoch 4443, Loss: 0.20947383344173431, Final Batch Loss: 0.09093970060348511\n",
      "Epoch 4444, Loss: 0.1913261115550995, Final Batch Loss: 0.0989082083106041\n",
      "Epoch 4445, Loss: 0.24047452956438065, Final Batch Loss: 0.09377724677324295\n",
      "Epoch 4446, Loss: 0.15082353726029396, Final Batch Loss: 0.09128706902265549\n",
      "Epoch 4447, Loss: 0.1884172111749649, Final Batch Loss: 0.07268959283828735\n",
      "Epoch 4448, Loss: 0.2354004979133606, Final Batch Loss: 0.1449728161096573\n",
      "Epoch 4449, Loss: 0.14624769613146782, Final Batch Loss: 0.05365612730383873\n",
      "Epoch 4450, Loss: 0.15749984234571457, Final Batch Loss: 0.07768061012029648\n",
      "Epoch 4451, Loss: 0.20264258980751038, Final Batch Loss: 0.0877889022231102\n",
      "Epoch 4452, Loss: 0.21514892578125, Final Batch Loss: 0.11923948675394058\n",
      "Epoch 4453, Loss: 0.19716228544712067, Final Batch Loss: 0.09886842221021652\n",
      "Epoch 4454, Loss: 0.19269585609436035, Final Batch Loss: 0.08782802522182465\n",
      "Epoch 4455, Loss: 0.2354954183101654, Final Batch Loss: 0.0926114022731781\n",
      "Epoch 4456, Loss: 0.1503339521586895, Final Batch Loss: 0.061238761991262436\n",
      "Epoch 4457, Loss: 0.23763225972652435, Final Batch Loss: 0.0991557389497757\n",
      "Epoch 4458, Loss: 0.25948502123355865, Final Batch Loss: 0.14917947351932526\n",
      "Epoch 4459, Loss: 0.18399471044540405, Final Batch Loss: 0.09189701825380325\n",
      "Epoch 4460, Loss: 0.18991796672344208, Final Batch Loss: 0.08632463961839676\n",
      "Epoch 4461, Loss: 0.20414619147777557, Final Batch Loss: 0.12325423955917358\n",
      "Epoch 4462, Loss: 0.19382385164499283, Final Batch Loss: 0.09234652668237686\n",
      "Epoch 4463, Loss: 0.16704678535461426, Final Batch Loss: 0.10273704677820206\n",
      "Epoch 4464, Loss: 0.21970981359481812, Final Batch Loss: 0.12383164465427399\n",
      "Epoch 4465, Loss: 0.2231326475739479, Final Batch Loss: 0.08278844505548477\n",
      "Epoch 4466, Loss: 0.18717364966869354, Final Batch Loss: 0.09616300463676453\n",
      "Epoch 4467, Loss: 0.2019665390253067, Final Batch Loss: 0.11299573630094528\n",
      "Epoch 4468, Loss: 0.20199085026979446, Final Batch Loss: 0.08603701740503311\n",
      "Epoch 4469, Loss: 0.16055169701576233, Final Batch Loss: 0.07331764698028564\n",
      "Epoch 4470, Loss: 0.1832357496023178, Final Batch Loss: 0.09413229674100876\n",
      "Epoch 4471, Loss: 0.17037351429462433, Final Batch Loss: 0.07537531852722168\n",
      "Epoch 4472, Loss: 0.19152283668518066, Final Batch Loss: 0.08483864367008209\n",
      "Epoch 4473, Loss: 0.198879212141037, Final Batch Loss: 0.10039278119802475\n",
      "Epoch 4474, Loss: 0.18931537866592407, Final Batch Loss: 0.09660530090332031\n",
      "Epoch 4475, Loss: 0.20958054810762405, Final Batch Loss: 0.13766323029994965\n",
      "Epoch 4476, Loss: 0.1450277790427208, Final Batch Loss: 0.06092480570077896\n",
      "Epoch 4477, Loss: 0.2002229541540146, Final Batch Loss: 0.1213749349117279\n",
      "Epoch 4478, Loss: 0.17288464307785034, Final Batch Loss: 0.09791337698698044\n",
      "Epoch 4479, Loss: 0.19407550990581512, Final Batch Loss: 0.08859027922153473\n",
      "Epoch 4480, Loss: 0.20065855979919434, Final Batch Loss: 0.09565895050764084\n",
      "Epoch 4481, Loss: 0.18742632120847702, Final Batch Loss: 0.09017263352870941\n",
      "Epoch 4482, Loss: 0.17464174330234528, Final Batch Loss: 0.09756962954998016\n",
      "Epoch 4483, Loss: 0.1748199388384819, Final Batch Loss: 0.08126246929168701\n",
      "Epoch 4484, Loss: 0.23899400979280472, Final Batch Loss: 0.15255917608737946\n",
      "Epoch 4485, Loss: 0.18762564659118652, Final Batch Loss: 0.06732313334941864\n",
      "Epoch 4486, Loss: 0.15494468808174133, Final Batch Loss: 0.08431001007556915\n",
      "Epoch 4487, Loss: 0.1936442255973816, Final Batch Loss: 0.11879346519708633\n",
      "Epoch 4488, Loss: 0.20549600571393967, Final Batch Loss: 0.09884197264909744\n",
      "Epoch 4489, Loss: 0.20533108711242676, Final Batch Loss: 0.10239556431770325\n",
      "Epoch 4490, Loss: 0.21195756644010544, Final Batch Loss: 0.09692760556936264\n",
      "Epoch 4491, Loss: 0.19955181330442429, Final Batch Loss: 0.11543800681829453\n",
      "Epoch 4492, Loss: 0.15772543102502823, Final Batch Loss: 0.07487504184246063\n",
      "Epoch 4493, Loss: 0.20716140419244766, Final Batch Loss: 0.09789731353521347\n",
      "Epoch 4494, Loss: 0.12665436044335365, Final Batch Loss: 0.036176566034555435\n",
      "Epoch 4495, Loss: 0.1949186697602272, Final Batch Loss: 0.10142877697944641\n",
      "Epoch 4496, Loss: 0.19576174020767212, Final Batch Loss: 0.10629371553659439\n",
      "Epoch 4497, Loss: 0.2735053673386574, Final Batch Loss: 0.12426859885454178\n",
      "Epoch 4498, Loss: 0.20379552245140076, Final Batch Loss: 0.09668023884296417\n",
      "Epoch 4499, Loss: 0.17864231765270233, Final Batch Loss: 0.11772525310516357\n",
      "Epoch 4500, Loss: 0.1888645887374878, Final Batch Loss: 0.12542307376861572\n",
      "Epoch 4501, Loss: 0.2052510604262352, Final Batch Loss: 0.11304568499326706\n",
      "Epoch 4502, Loss: 0.2333366796374321, Final Batch Loss: 0.1140802875161171\n",
      "Epoch 4503, Loss: 0.18951623886823654, Final Batch Loss: 0.09138340502977371\n",
      "Epoch 4504, Loss: 0.20053190737962723, Final Batch Loss: 0.10873610526323318\n",
      "Epoch 4505, Loss: 0.2435186430811882, Final Batch Loss: 0.113919697701931\n",
      "Epoch 4506, Loss: 0.229768306016922, Final Batch Loss: 0.13188479840755463\n",
      "Epoch 4507, Loss: 0.2224944829940796, Final Batch Loss: 0.08530078828334808\n",
      "Epoch 4508, Loss: 0.15729208290576935, Final Batch Loss: 0.07366926223039627\n",
      "Epoch 4509, Loss: 0.20183078944683075, Final Batch Loss: 0.1092347726225853\n",
      "Epoch 4510, Loss: 0.2758224904537201, Final Batch Loss: 0.15640617907047272\n",
      "Epoch 4511, Loss: 0.18210533261299133, Final Batch Loss: 0.08866138756275177\n",
      "Epoch 4512, Loss: 0.16319134086370468, Final Batch Loss: 0.0743342861533165\n",
      "Epoch 4513, Loss: 0.16989606618881226, Final Batch Loss: 0.07038470357656479\n",
      "Epoch 4514, Loss: 0.19204800575971603, Final Batch Loss: 0.10473934561014175\n",
      "Epoch 4515, Loss: 0.17838746309280396, Final Batch Loss: 0.11125744879245758\n",
      "Epoch 4516, Loss: 0.2430562824010849, Final Batch Loss: 0.11176678538322449\n",
      "Epoch 4517, Loss: 0.21858128160238266, Final Batch Loss: 0.09431830793619156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4518, Loss: 0.1668444164097309, Final Batch Loss: 0.1100020632147789\n",
      "Epoch 4519, Loss: 0.2059701830148697, Final Batch Loss: 0.08603398501873016\n",
      "Epoch 4520, Loss: 0.23608415573835373, Final Batch Loss: 0.09993945807218552\n",
      "Epoch 4521, Loss: 0.23288236558437347, Final Batch Loss: 0.09875084459781647\n",
      "Epoch 4522, Loss: 0.17573243379592896, Final Batch Loss: 0.09130340814590454\n",
      "Epoch 4523, Loss: 0.18571529537439346, Final Batch Loss: 0.08968599140644073\n",
      "Epoch 4524, Loss: 0.19986506551504135, Final Batch Loss: 0.0968710407614708\n",
      "Epoch 4525, Loss: 0.20462344586849213, Final Batch Loss: 0.10711982101202011\n",
      "Epoch 4526, Loss: 0.2676045298576355, Final Batch Loss: 0.13236522674560547\n",
      "Epoch 4527, Loss: 0.17843003571033478, Final Batch Loss: 0.07257962971925735\n",
      "Epoch 4528, Loss: 0.20055659860372543, Final Batch Loss: 0.10855059325695038\n",
      "Epoch 4529, Loss: 0.20439647883176804, Final Batch Loss: 0.11513368785381317\n",
      "Epoch 4530, Loss: 0.20000071078538895, Final Batch Loss: 0.08150676637887955\n",
      "Epoch 4531, Loss: 0.20145490020513535, Final Batch Loss: 0.11917851120233536\n",
      "Epoch 4532, Loss: 0.175196073949337, Final Batch Loss: 0.09337466955184937\n",
      "Epoch 4533, Loss: 0.1741526946425438, Final Batch Loss: 0.09841471165418625\n",
      "Epoch 4534, Loss: 0.1779501810669899, Final Batch Loss: 0.10233136266469955\n",
      "Epoch 4535, Loss: 0.20799369364976883, Final Batch Loss: 0.07204217463731766\n",
      "Epoch 4536, Loss: 0.18873082101345062, Final Batch Loss: 0.08837264776229858\n",
      "Epoch 4537, Loss: 0.220805324614048, Final Batch Loss: 0.10585597157478333\n",
      "Epoch 4538, Loss: 0.1449318379163742, Final Batch Loss: 0.08101613819599152\n",
      "Epoch 4539, Loss: 0.1722181737422943, Final Batch Loss: 0.0796995684504509\n",
      "Epoch 4540, Loss: 0.16924594342708588, Final Batch Loss: 0.10625699907541275\n",
      "Epoch 4541, Loss: 0.16341878473758698, Final Batch Loss: 0.0757405012845993\n",
      "Epoch 4542, Loss: 0.1617256999015808, Final Batch Loss: 0.08465531468391418\n",
      "Epoch 4543, Loss: 0.20834015309810638, Final Batch Loss: 0.11962886899709702\n",
      "Epoch 4544, Loss: 0.2080433890223503, Final Batch Loss: 0.10427821427583694\n",
      "Epoch 4545, Loss: 0.18057838827371597, Final Batch Loss: 0.08525315672159195\n",
      "Epoch 4546, Loss: 0.22636990994215012, Final Batch Loss: 0.12663547694683075\n",
      "Epoch 4547, Loss: 0.2018965780735016, Final Batch Loss: 0.10397964715957642\n",
      "Epoch 4548, Loss: 0.18968436121940613, Final Batch Loss: 0.09642413258552551\n",
      "Epoch 4549, Loss: 0.2311883345246315, Final Batch Loss: 0.08098600059747696\n",
      "Epoch 4550, Loss: 0.13871192187070847, Final Batch Loss: 0.06804299354553223\n",
      "Epoch 4551, Loss: 0.1622365042567253, Final Batch Loss: 0.06636941432952881\n",
      "Epoch 4552, Loss: 0.1890885978937149, Final Batch Loss: 0.08326893299818039\n",
      "Epoch 4553, Loss: 0.1564745381474495, Final Batch Loss: 0.06966089457273483\n",
      "Epoch 4554, Loss: 0.2213405817747116, Final Batch Loss: 0.07972241938114166\n",
      "Epoch 4555, Loss: 0.2180754616856575, Final Batch Loss: 0.09512672573328018\n",
      "Epoch 4556, Loss: 0.1781005635857582, Final Batch Loss: 0.1015411838889122\n",
      "Epoch 4557, Loss: 0.17651212215423584, Final Batch Loss: 0.09710187464952469\n",
      "Epoch 4558, Loss: 0.18078572675585747, Final Batch Loss: 0.12291482090950012\n",
      "Epoch 4559, Loss: 0.22692187130451202, Final Batch Loss: 0.10854177176952362\n",
      "Epoch 4560, Loss: 0.15854942798614502, Final Batch Loss: 0.06490471959114075\n",
      "Epoch 4561, Loss: 0.1785869151353836, Final Batch Loss: 0.082952119410038\n",
      "Epoch 4562, Loss: 0.16860615462064743, Final Batch Loss: 0.1000872403383255\n",
      "Epoch 4563, Loss: 0.17223675549030304, Final Batch Loss: 0.08982516080141068\n",
      "Epoch 4564, Loss: 0.20319519937038422, Final Batch Loss: 0.10791736096143723\n",
      "Epoch 4565, Loss: 0.19723978638648987, Final Batch Loss: 0.10931603610515594\n",
      "Epoch 4566, Loss: 0.1758151277899742, Final Batch Loss: 0.08310515433549881\n",
      "Epoch 4567, Loss: 0.14841481298208237, Final Batch Loss: 0.07311440259218216\n",
      "Epoch 4568, Loss: 0.22164339572191238, Final Batch Loss: 0.10813470929861069\n",
      "Epoch 4569, Loss: 0.198724627494812, Final Batch Loss: 0.0884578675031662\n",
      "Epoch 4570, Loss: 0.1545962318778038, Final Batch Loss: 0.06795386970043182\n",
      "Epoch 4571, Loss: 0.16706915944814682, Final Batch Loss: 0.08569267392158508\n",
      "Epoch 4572, Loss: 0.17032411694526672, Final Batch Loss: 0.07214462757110596\n",
      "Epoch 4573, Loss: 0.1535806506872177, Final Batch Loss: 0.06718795001506805\n",
      "Epoch 4574, Loss: 0.16184311360120773, Final Batch Loss: 0.0807155966758728\n",
      "Epoch 4575, Loss: 0.16183529794216156, Final Batch Loss: 0.07816917449235916\n",
      "Epoch 4576, Loss: 0.20040728151798248, Final Batch Loss: 0.08788842707872391\n",
      "Epoch 4577, Loss: 0.14637228474020958, Final Batch Loss: 0.08617375791072845\n",
      "Epoch 4578, Loss: 0.20990269631147385, Final Batch Loss: 0.08822349458932877\n",
      "Epoch 4579, Loss: 0.14022164046764374, Final Batch Loss: 0.04161748290061951\n",
      "Epoch 4580, Loss: 0.19273870438337326, Final Batch Loss: 0.09908946603536606\n",
      "Epoch 4581, Loss: 0.17908522486686707, Final Batch Loss: 0.11383246630430222\n",
      "Epoch 4582, Loss: 0.2213503122329712, Final Batch Loss: 0.13650260865688324\n",
      "Epoch 4583, Loss: 0.20165148377418518, Final Batch Loss: 0.09408172965049744\n",
      "Epoch 4584, Loss: 0.1550421118736267, Final Batch Loss: 0.06386806070804596\n",
      "Epoch 4585, Loss: 0.1511375904083252, Final Batch Loss: 0.0853821262717247\n",
      "Epoch 4586, Loss: 0.2070852890610695, Final Batch Loss: 0.09159953147172928\n",
      "Epoch 4587, Loss: 0.20156151801347733, Final Batch Loss: 0.080241858959198\n",
      "Epoch 4588, Loss: 0.16535872220993042, Final Batch Loss: 0.06087889522314072\n",
      "Epoch 4589, Loss: 0.1402086541056633, Final Batch Loss: 0.07241184264421463\n",
      "Epoch 4590, Loss: 0.20237185060977936, Final Batch Loss: 0.07822848856449127\n",
      "Epoch 4591, Loss: 0.2501571550965309, Final Batch Loss: 0.14796024560928345\n",
      "Epoch 4592, Loss: 0.1491839960217476, Final Batch Loss: 0.07314422726631165\n",
      "Epoch 4593, Loss: 0.20645971596240997, Final Batch Loss: 0.09423165768384933\n",
      "Epoch 4594, Loss: 0.17603431642055511, Final Batch Loss: 0.09030495584011078\n",
      "Epoch 4595, Loss: 0.1869608238339424, Final Batch Loss: 0.09147028625011444\n",
      "Epoch 4596, Loss: 0.23516399413347244, Final Batch Loss: 0.12293741852045059\n",
      "Epoch 4597, Loss: 0.2662762925028801, Final Batch Loss: 0.1629309505224228\n",
      "Epoch 4598, Loss: 0.2547270879149437, Final Batch Loss: 0.1490200012922287\n",
      "Epoch 4599, Loss: 0.2312052994966507, Final Batch Loss: 0.13742414116859436\n",
      "Epoch 4600, Loss: 0.19242919981479645, Final Batch Loss: 0.10726457089185715\n",
      "Epoch 4601, Loss: 0.2139105200767517, Final Batch Loss: 0.09818755090236664\n",
      "Epoch 4602, Loss: 0.19393455237150192, Final Batch Loss: 0.08945950865745544\n",
      "Epoch 4603, Loss: 0.2048310935497284, Final Batch Loss: 0.08251727372407913\n",
      "Epoch 4604, Loss: 0.20073451101779938, Final Batch Loss: 0.1040562242269516\n",
      "Epoch 4605, Loss: 0.1688762903213501, Final Batch Loss: 0.08023004978895187\n",
      "Epoch 4606, Loss: 0.2162681519985199, Final Batch Loss: 0.12125568091869354\n",
      "Epoch 4607, Loss: 0.28076496720314026, Final Batch Loss: 0.09235970675945282\n",
      "Epoch 4608, Loss: 0.2008793130517006, Final Batch Loss: 0.1299295723438263\n",
      "Epoch 4609, Loss: 0.15992113202810287, Final Batch Loss: 0.07161762565374374\n",
      "Epoch 4610, Loss: 0.19031793624162674, Final Batch Loss: 0.10588310658931732\n",
      "Epoch 4611, Loss: 0.16038911789655685, Final Batch Loss: 0.08390079438686371\n",
      "Epoch 4612, Loss: 0.21570273488759995, Final Batch Loss: 0.13270717859268188\n",
      "Epoch 4613, Loss: 0.22880669683218002, Final Batch Loss: 0.12355075031518936\n",
      "Epoch 4614, Loss: 0.20781394839286804, Final Batch Loss: 0.10475927591323853\n",
      "Epoch 4615, Loss: 0.19422394782304764, Final Batch Loss: 0.07266325503587723\n",
      "Epoch 4616, Loss: 0.23028042912483215, Final Batch Loss: 0.11607813090085983\n",
      "Epoch 4617, Loss: 0.1863698735833168, Final Batch Loss: 0.07600689679384232\n",
      "Epoch 4618, Loss: 0.18586253747344017, Final Batch Loss: 0.058431487530469894\n",
      "Epoch 4619, Loss: 0.17578398436307907, Final Batch Loss: 0.09447683393955231\n",
      "Epoch 4620, Loss: 0.26453909277915955, Final Batch Loss: 0.13457128405570984\n",
      "Epoch 4621, Loss: 0.1776513233780861, Final Batch Loss: 0.07340768724679947\n",
      "Epoch 4622, Loss: 0.16864842176437378, Final Batch Loss: 0.0824187770485878\n",
      "Epoch 4623, Loss: 0.2032320871949196, Final Batch Loss: 0.127871572971344\n",
      "Epoch 4624, Loss: 0.19032664597034454, Final Batch Loss: 0.07320484519004822\n",
      "Epoch 4625, Loss: 0.18848294764757156, Final Batch Loss: 0.12100684642791748\n",
      "Epoch 4626, Loss: 0.18727993965148926, Final Batch Loss: 0.09405511617660522\n",
      "Epoch 4627, Loss: 0.16613994166254997, Final Batch Loss: 0.0598411001265049\n",
      "Epoch 4628, Loss: 0.18158075958490372, Final Batch Loss: 0.08801952749490738\n",
      "Epoch 4629, Loss: 0.16613735631108284, Final Batch Loss: 0.05401087924838066\n",
      "Epoch 4630, Loss: 0.18336407840251923, Final Batch Loss: 0.09064048528671265\n",
      "Epoch 4631, Loss: 0.17816082388162613, Final Batch Loss: 0.09634712338447571\n",
      "Epoch 4632, Loss: 0.24527566134929657, Final Batch Loss: 0.12796665728092194\n",
      "Epoch 4633, Loss: 0.1805632933974266, Final Batch Loss: 0.10567876696586609\n",
      "Epoch 4634, Loss: 0.1790654957294464, Final Batch Loss: 0.0982380285859108\n",
      "Epoch 4635, Loss: 0.2247808575630188, Final Batch Loss: 0.11161823570728302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4636, Loss: 0.15238391235470772, Final Batch Loss: 0.09009139984846115\n",
      "Epoch 4637, Loss: 0.20141932368278503, Final Batch Loss: 0.10314254462718964\n",
      "Epoch 4638, Loss: 0.18422629684209824, Final Batch Loss: 0.09022717922925949\n",
      "Epoch 4639, Loss: 0.20870241522789001, Final Batch Loss: 0.08962642401456833\n",
      "Epoch 4640, Loss: 0.19989404827356339, Final Batch Loss: 0.09231524914503098\n",
      "Epoch 4641, Loss: 0.19651976227760315, Final Batch Loss: 0.112610824406147\n",
      "Epoch 4642, Loss: 0.18813510239124298, Final Batch Loss: 0.10565947741270065\n",
      "Epoch 4643, Loss: 0.20834166556596756, Final Batch Loss: 0.0881551131606102\n",
      "Epoch 4644, Loss: 0.17047658562660217, Final Batch Loss: 0.09241954982280731\n",
      "Epoch 4645, Loss: 0.2607981264591217, Final Batch Loss: 0.1185552328824997\n",
      "Epoch 4646, Loss: 0.2322218418121338, Final Batch Loss: 0.11372315138578415\n",
      "Epoch 4647, Loss: 0.16266833990812302, Final Batch Loss: 0.07645126432180405\n",
      "Epoch 4648, Loss: 0.15770956873893738, Final Batch Loss: 0.08314591646194458\n",
      "Epoch 4649, Loss: 0.22223786264657974, Final Batch Loss: 0.10685912519693375\n",
      "Epoch 4650, Loss: 0.21787463128566742, Final Batch Loss: 0.12017346918582916\n",
      "Epoch 4651, Loss: 0.22854075580835342, Final Batch Loss: 0.14643451571464539\n",
      "Epoch 4652, Loss: 0.21906771510839462, Final Batch Loss: 0.10670540481805801\n",
      "Epoch 4653, Loss: 0.24409444630146027, Final Batch Loss: 0.13337960839271545\n",
      "Epoch 4654, Loss: 0.19821345061063766, Final Batch Loss: 0.11994317173957825\n",
      "Epoch 4655, Loss: 0.20902172476053238, Final Batch Loss: 0.13546930253505707\n",
      "Epoch 4656, Loss: 0.1831338182091713, Final Batch Loss: 0.13757196068763733\n",
      "Epoch 4657, Loss: 0.19144728779792786, Final Batch Loss: 0.07579679787158966\n",
      "Epoch 4658, Loss: 0.19624201208353043, Final Batch Loss: 0.10005544871091843\n",
      "Epoch 4659, Loss: 0.2308410182595253, Final Batch Loss: 0.15987516939640045\n",
      "Epoch 4660, Loss: 0.25728363543748856, Final Batch Loss: 0.10740341991186142\n",
      "Epoch 4661, Loss: 0.1675618663430214, Final Batch Loss: 0.08006000518798828\n",
      "Epoch 4662, Loss: 0.18030666559934616, Final Batch Loss: 0.08963505178689957\n",
      "Epoch 4663, Loss: 0.1612878292798996, Final Batch Loss: 0.06048322468996048\n",
      "Epoch 4664, Loss: 0.15811901539564133, Final Batch Loss: 0.0902966782450676\n",
      "Epoch 4665, Loss: 0.19811087846755981, Final Batch Loss: 0.10719142854213715\n",
      "Epoch 4666, Loss: 0.15079240500926971, Final Batch Loss: 0.07870171964168549\n",
      "Epoch 4667, Loss: 0.16486603766679764, Final Batch Loss: 0.08641954511404037\n",
      "Epoch 4668, Loss: 0.2541842833161354, Final Batch Loss: 0.16158358752727509\n",
      "Epoch 4669, Loss: 0.2057827040553093, Final Batch Loss: 0.09279433637857437\n",
      "Epoch 4670, Loss: 0.2723870128393173, Final Batch Loss: 0.14483413100242615\n",
      "Epoch 4671, Loss: 0.21571606397628784, Final Batch Loss: 0.10467318445444107\n",
      "Epoch 4672, Loss: 0.20025397092103958, Final Batch Loss: 0.09555770456790924\n",
      "Epoch 4673, Loss: 0.19096406549215317, Final Batch Loss: 0.12001875042915344\n",
      "Epoch 4674, Loss: 0.24760520458221436, Final Batch Loss: 0.10482366383075714\n",
      "Epoch 4675, Loss: 0.24405623972415924, Final Batch Loss: 0.09566518664360046\n",
      "Epoch 4676, Loss: 0.20101099461317062, Final Batch Loss: 0.10274530202150345\n",
      "Epoch 4677, Loss: 0.18718454986810684, Final Batch Loss: 0.08386179059743881\n",
      "Epoch 4678, Loss: 0.2693078964948654, Final Batch Loss: 0.12741994857788086\n",
      "Epoch 4679, Loss: 0.1957433670759201, Final Batch Loss: 0.07452496886253357\n",
      "Epoch 4680, Loss: 0.1963777020573616, Final Batch Loss: 0.07112877815961838\n",
      "Epoch 4681, Loss: 0.27359767258167267, Final Batch Loss: 0.1351994425058365\n",
      "Epoch 4682, Loss: 0.2142975553870201, Final Batch Loss: 0.08954756706953049\n",
      "Epoch 4683, Loss: 0.19388305395841599, Final Batch Loss: 0.089115209877491\n",
      "Epoch 4684, Loss: 0.21990030258893967, Final Batch Loss: 0.06365976482629776\n",
      "Epoch 4685, Loss: 0.1579480990767479, Final Batch Loss: 0.08890443295240402\n",
      "Epoch 4686, Loss: 0.23406701534986496, Final Batch Loss: 0.11167215555906296\n",
      "Epoch 4687, Loss: 0.20518434792757034, Final Batch Loss: 0.09479545056819916\n",
      "Epoch 4688, Loss: 0.16017474234104156, Final Batch Loss: 0.0873945951461792\n",
      "Epoch 4689, Loss: 0.15617761760950089, Final Batch Loss: 0.07462852448225021\n",
      "Epoch 4690, Loss: 0.24610983580350876, Final Batch Loss: 0.10512096434831619\n",
      "Epoch 4691, Loss: 0.16902578622102737, Final Batch Loss: 0.07879236340522766\n",
      "Epoch 4692, Loss: 0.2131122723221779, Final Batch Loss: 0.13174784183502197\n",
      "Epoch 4693, Loss: 0.14584795385599136, Final Batch Loss: 0.08130107820034027\n",
      "Epoch 4694, Loss: 0.20175710320472717, Final Batch Loss: 0.10726602375507355\n",
      "Epoch 4695, Loss: 0.18153740093111992, Final Batch Loss: 0.06119426712393761\n",
      "Epoch 4696, Loss: 0.19770878553390503, Final Batch Loss: 0.09260997921228409\n",
      "Epoch 4697, Loss: 0.18708083033561707, Final Batch Loss: 0.11474356800317764\n",
      "Epoch 4698, Loss: 0.17283407598733902, Final Batch Loss: 0.0842624306678772\n",
      "Epoch 4699, Loss: 0.19033585488796234, Final Batch Loss: 0.11998463422060013\n",
      "Epoch 4700, Loss: 0.18735310435295105, Final Batch Loss: 0.07723715156316757\n",
      "Epoch 4701, Loss: 0.22767324000597, Final Batch Loss: 0.09637711197137833\n",
      "Epoch 4702, Loss: 0.1857893243432045, Final Batch Loss: 0.09609826654195786\n",
      "Epoch 4703, Loss: 0.17779063433408737, Final Batch Loss: 0.10551514476537704\n",
      "Epoch 4704, Loss: 0.17778023332357407, Final Batch Loss: 0.10969150066375732\n",
      "Epoch 4705, Loss: 0.20342827588319778, Final Batch Loss: 0.10977456718683243\n",
      "Epoch 4706, Loss: 0.1987977847456932, Final Batch Loss: 0.08810754865407944\n",
      "Epoch 4707, Loss: 0.18214531242847443, Final Batch Loss: 0.07794090360403061\n",
      "Epoch 4708, Loss: 0.19428642839193344, Final Batch Loss: 0.10615704208612442\n",
      "Epoch 4709, Loss: 0.17232224345207214, Final Batch Loss: 0.06800530105829239\n",
      "Epoch 4710, Loss: 0.15468823909759521, Final Batch Loss: 0.0660729929804802\n",
      "Epoch 4711, Loss: 0.1708768531680107, Final Batch Loss: 0.0685386136174202\n",
      "Epoch 4712, Loss: 0.17518389970064163, Final Batch Loss: 0.06811881065368652\n",
      "Epoch 4713, Loss: 0.2423812448978424, Final Batch Loss: 0.12880048155784607\n",
      "Epoch 4714, Loss: 0.19041823595762253, Final Batch Loss: 0.09961782395839691\n",
      "Epoch 4715, Loss: 0.19055093824863434, Final Batch Loss: 0.1012890413403511\n",
      "Epoch 4716, Loss: 0.18536930531263351, Final Batch Loss: 0.07640045881271362\n",
      "Epoch 4717, Loss: 0.20125561207532883, Final Batch Loss: 0.10056347399950027\n",
      "Epoch 4718, Loss: 0.16662206500768661, Final Batch Loss: 0.08450083434581757\n",
      "Epoch 4719, Loss: 0.1975933089852333, Final Batch Loss: 0.13287609815597534\n",
      "Epoch 4720, Loss: 0.24195944517850876, Final Batch Loss: 0.16953925788402557\n",
      "Epoch 4721, Loss: 0.1705569475889206, Final Batch Loss: 0.09111528843641281\n",
      "Epoch 4722, Loss: 0.14976407960057259, Final Batch Loss: 0.08743563294410706\n",
      "Epoch 4723, Loss: 0.21766524016857147, Final Batch Loss: 0.09437441825866699\n",
      "Epoch 4724, Loss: 0.17808836698532104, Final Batch Loss: 0.07767004519701004\n",
      "Epoch 4725, Loss: 0.16190730035305023, Final Batch Loss: 0.08468374609947205\n",
      "Epoch 4726, Loss: 0.17535019665956497, Final Batch Loss: 0.09544149786233902\n",
      "Epoch 4727, Loss: 0.19991479814052582, Final Batch Loss: 0.11991334706544876\n",
      "Epoch 4728, Loss: 0.1642620489001274, Final Batch Loss: 0.09795211255550385\n",
      "Epoch 4729, Loss: 0.17643281817436218, Final Batch Loss: 0.07422008365392685\n",
      "Epoch 4730, Loss: 0.188054621219635, Final Batch Loss: 0.1011539101600647\n",
      "Epoch 4731, Loss: 0.2044154703617096, Final Batch Loss: 0.1297065168619156\n",
      "Epoch 4732, Loss: 0.18919092416763306, Final Batch Loss: 0.0878078043460846\n",
      "Epoch 4733, Loss: 0.1410668045282364, Final Batch Loss: 0.07584226131439209\n",
      "Epoch 4734, Loss: 0.22975687682628632, Final Batch Loss: 0.08952610194683075\n",
      "Epoch 4735, Loss: 0.1929289698600769, Final Batch Loss: 0.10458576679229736\n",
      "Epoch 4736, Loss: 0.14881212264299393, Final Batch Loss: 0.05169188231229782\n",
      "Epoch 4737, Loss: 0.1563844010233879, Final Batch Loss: 0.07773686200380325\n",
      "Epoch 4738, Loss: 0.19298535585403442, Final Batch Loss: 0.10963520407676697\n",
      "Epoch 4739, Loss: 0.23237530887126923, Final Batch Loss: 0.12694895267486572\n",
      "Epoch 4740, Loss: 0.27707505226135254, Final Batch Loss: 0.17111338675022125\n",
      "Epoch 4741, Loss: 0.22184138745069504, Final Batch Loss: 0.11430428177118301\n",
      "Epoch 4742, Loss: 0.25592388957738876, Final Batch Loss: 0.1202123835682869\n",
      "Epoch 4743, Loss: 0.2284703478217125, Final Batch Loss: 0.1099245697259903\n",
      "Epoch 4744, Loss: 0.3326457738876343, Final Batch Loss: 0.18922021985054016\n",
      "Epoch 4745, Loss: 0.20555240660905838, Final Batch Loss: 0.1277172565460205\n",
      "Epoch 4746, Loss: 0.21793537586927414, Final Batch Loss: 0.10128550231456757\n",
      "Epoch 4747, Loss: 0.27071695774793625, Final Batch Loss: 0.19239352643489838\n",
      "Epoch 4748, Loss: 0.25407955795526505, Final Batch Loss: 0.1647203266620636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4749, Loss: 0.28399017453193665, Final Batch Loss: 0.15336918830871582\n",
      "Epoch 4750, Loss: 0.23474711179733276, Final Batch Loss: 0.13463762402534485\n",
      "Epoch 4751, Loss: 0.2307855263352394, Final Batch Loss: 0.1300041228532791\n",
      "Epoch 4752, Loss: 0.26670345664024353, Final Batch Loss: 0.17347675561904907\n",
      "Epoch 4753, Loss: 0.26941056549549103, Final Batch Loss: 0.12826482951641083\n",
      "Epoch 4754, Loss: 0.22752974927425385, Final Batch Loss: 0.12028849869966507\n",
      "Epoch 4755, Loss: 0.20894590020179749, Final Batch Loss: 0.09654133766889572\n",
      "Epoch 4756, Loss: 0.20990563929080963, Final Batch Loss: 0.09953456372022629\n",
      "Epoch 4757, Loss: 0.19756988435983658, Final Batch Loss: 0.09372326731681824\n",
      "Epoch 4758, Loss: 0.1957097202539444, Final Batch Loss: 0.11456121504306793\n",
      "Epoch 4759, Loss: 0.20837201923131943, Final Batch Loss: 0.11520858108997345\n",
      "Epoch 4760, Loss: 0.19560855627059937, Final Batch Loss: 0.11533891409635544\n",
      "Epoch 4761, Loss: 0.15203744918107986, Final Batch Loss: 0.07326235622167587\n",
      "Epoch 4762, Loss: 0.24305622279644012, Final Batch Loss: 0.1566576063632965\n",
      "Epoch 4763, Loss: 0.2139884978532791, Final Batch Loss: 0.11027934402227402\n",
      "Epoch 4764, Loss: 0.2404906153678894, Final Batch Loss: 0.12356699258089066\n",
      "Epoch 4765, Loss: 0.17051565647125244, Final Batch Loss: 0.06268403679132462\n",
      "Epoch 4766, Loss: 0.1985563114285469, Final Batch Loss: 0.10786379128694534\n",
      "Epoch 4767, Loss: 0.21810751408338547, Final Batch Loss: 0.1110539436340332\n",
      "Epoch 4768, Loss: 0.2054307907819748, Final Batch Loss: 0.08927175402641296\n",
      "Epoch 4769, Loss: 0.23118730634450912, Final Batch Loss: 0.1430719494819641\n",
      "Epoch 4770, Loss: 0.26791512221097946, Final Batch Loss: 0.16842392086982727\n",
      "Epoch 4771, Loss: 0.1870683878660202, Final Batch Loss: 0.08374716341495514\n",
      "Epoch 4772, Loss: 0.2255530282855034, Final Batch Loss: 0.1351567655801773\n",
      "Epoch 4773, Loss: 0.19242922961711884, Final Batch Loss: 0.08295385539531708\n",
      "Epoch 4774, Loss: 0.1785023733973503, Final Batch Loss: 0.09165828675031662\n",
      "Epoch 4775, Loss: 0.19526655226945877, Final Batch Loss: 0.09850647300481796\n",
      "Epoch 4776, Loss: 0.18877868354320526, Final Batch Loss: 0.10231445729732513\n",
      "Epoch 4777, Loss: 0.1885380893945694, Final Batch Loss: 0.08825859427452087\n",
      "Epoch 4778, Loss: 0.15781498700380325, Final Batch Loss: 0.09108245372772217\n",
      "Epoch 4779, Loss: 0.18607285618782043, Final Batch Loss: 0.11714790016412735\n",
      "Epoch 4780, Loss: 0.1507878638803959, Final Batch Loss: 0.06094556674361229\n",
      "Epoch 4781, Loss: 0.19346794858574867, Final Batch Loss: 0.055364225059747696\n",
      "Epoch 4782, Loss: 0.20450956374406815, Final Batch Loss: 0.08003237098455429\n",
      "Epoch 4783, Loss: 0.23294568806886673, Final Batch Loss: 0.14186763763427734\n",
      "Epoch 4784, Loss: 0.16458725929260254, Final Batch Loss: 0.09738563001155853\n",
      "Epoch 4785, Loss: 0.1637406349182129, Final Batch Loss: 0.08656558394432068\n",
      "Epoch 4786, Loss: 0.18283840268850327, Final Batch Loss: 0.09915788471698761\n",
      "Epoch 4787, Loss: 0.13605565577745438, Final Batch Loss: 0.0694519579410553\n",
      "Epoch 4788, Loss: 0.25527404248714447, Final Batch Loss: 0.11641198396682739\n",
      "Epoch 4789, Loss: 0.16751381754875183, Final Batch Loss: 0.09078594297170639\n",
      "Epoch 4790, Loss: 0.21070648729801178, Final Batch Loss: 0.08954407274723053\n",
      "Epoch 4791, Loss: 0.17017631232738495, Final Batch Loss: 0.0985562652349472\n",
      "Epoch 4792, Loss: 0.22604111582040787, Final Batch Loss: 0.11594501882791519\n",
      "Epoch 4793, Loss: 0.13156552240252495, Final Batch Loss: 0.05620741471648216\n",
      "Epoch 4794, Loss: 0.25352615118026733, Final Batch Loss: 0.07895307242870331\n",
      "Epoch 4795, Loss: 0.18248792737722397, Final Batch Loss: 0.08435340225696564\n",
      "Epoch 4796, Loss: 0.18923068046569824, Final Batch Loss: 0.07023333013057709\n",
      "Epoch 4797, Loss: 0.16810663044452667, Final Batch Loss: 0.0674491748213768\n",
      "Epoch 4798, Loss: 0.18741846829652786, Final Batch Loss: 0.0747930258512497\n",
      "Epoch 4799, Loss: 0.18365520983934402, Final Batch Loss: 0.07479985803365707\n",
      "Epoch 4800, Loss: 0.16816319152712822, Final Batch Loss: 0.06145342066884041\n",
      "Epoch 4801, Loss: 0.1719704605638981, Final Batch Loss: 0.05386841669678688\n",
      "Epoch 4802, Loss: 0.19212540239095688, Final Batch Loss: 0.12276248633861542\n",
      "Epoch 4803, Loss: 0.15936657786369324, Final Batch Loss: 0.0874878540635109\n",
      "Epoch 4804, Loss: 0.1743573546409607, Final Batch Loss: 0.09829794615507126\n",
      "Epoch 4805, Loss: 0.15455754101276398, Final Batch Loss: 0.07876868546009064\n",
      "Epoch 4806, Loss: 0.15010841190814972, Final Batch Loss: 0.07634734362363815\n",
      "Epoch 4807, Loss: 0.17376843094825745, Final Batch Loss: 0.06330351531505585\n",
      "Epoch 4808, Loss: 0.2400255724787712, Final Batch Loss: 0.14037930965423584\n",
      "Epoch 4809, Loss: 0.22367074340581894, Final Batch Loss: 0.10255509614944458\n",
      "Epoch 4810, Loss: 0.19587668031454086, Final Batch Loss: 0.10479021817445755\n",
      "Epoch 4811, Loss: 0.20080000162124634, Final Batch Loss: 0.10373170673847198\n",
      "Epoch 4812, Loss: 0.17163287103176117, Final Batch Loss: 0.07160133123397827\n",
      "Epoch 4813, Loss: 0.22146794199943542, Final Batch Loss: 0.12027177214622498\n",
      "Epoch 4814, Loss: 0.1829366758465767, Final Batch Loss: 0.0824684351682663\n",
      "Epoch 4815, Loss: 0.21568918228149414, Final Batch Loss: 0.10153555124998093\n",
      "Epoch 4816, Loss: 0.15113233774900436, Final Batch Loss: 0.06274972856044769\n",
      "Epoch 4817, Loss: 0.19222299382090569, Final Batch Loss: 0.05597500130534172\n",
      "Epoch 4818, Loss: 0.16094999015331268, Final Batch Loss: 0.07457160204648972\n",
      "Epoch 4819, Loss: 0.14774249494075775, Final Batch Loss: 0.06998112797737122\n",
      "Epoch 4820, Loss: 0.15464498847723007, Final Batch Loss: 0.06640133261680603\n",
      "Epoch 4821, Loss: 0.1879713386297226, Final Batch Loss: 0.10649649798870087\n",
      "Epoch 4822, Loss: 0.21814241260290146, Final Batch Loss: 0.08027546852827072\n",
      "Epoch 4823, Loss: 0.1362510472536087, Final Batch Loss: 0.0474124476313591\n",
      "Epoch 4824, Loss: 0.1709911972284317, Final Batch Loss: 0.0737830251455307\n",
      "Epoch 4825, Loss: 0.15483823418617249, Final Batch Loss: 0.07369363307952881\n",
      "Epoch 4826, Loss: 0.18765857815742493, Final Batch Loss: 0.10751169174909592\n",
      "Epoch 4827, Loss: 0.17612160742282867, Final Batch Loss: 0.07982612401247025\n",
      "Epoch 4828, Loss: 0.16521697491407394, Final Batch Loss: 0.07069136202335358\n",
      "Epoch 4829, Loss: 0.1853892281651497, Final Batch Loss: 0.1018078625202179\n",
      "Epoch 4830, Loss: 0.17064625769853592, Final Batch Loss: 0.08122208714485168\n",
      "Epoch 4831, Loss: 0.22776763141155243, Final Batch Loss: 0.15086781978607178\n",
      "Epoch 4832, Loss: 0.2041918858885765, Final Batch Loss: 0.11657840758562088\n",
      "Epoch 4833, Loss: 0.18241536617279053, Final Batch Loss: 0.09142591804265976\n",
      "Epoch 4834, Loss: 0.1865333467721939, Final Batch Loss: 0.10145730525255203\n",
      "Epoch 4835, Loss: 0.2442426234483719, Final Batch Loss: 0.08245198428630829\n",
      "Epoch 4836, Loss: 0.177273727953434, Final Batch Loss: 0.09338908642530441\n",
      "Epoch 4837, Loss: 0.17671670764684677, Final Batch Loss: 0.08376167714595795\n",
      "Epoch 4838, Loss: 0.17810020595788956, Final Batch Loss: 0.08120424300432205\n",
      "Epoch 4839, Loss: 0.171711303293705, Final Batch Loss: 0.08710665255784988\n",
      "Epoch 4840, Loss: 0.23847707360982895, Final Batch Loss: 0.09614694863557816\n",
      "Epoch 4841, Loss: 0.1714797094464302, Final Batch Loss: 0.08534186333417892\n",
      "Epoch 4842, Loss: 0.19901470839977264, Final Batch Loss: 0.12883268296718597\n",
      "Epoch 4843, Loss: 0.20080839097499847, Final Batch Loss: 0.10748845338821411\n",
      "Epoch 4844, Loss: 0.17035770416259766, Final Batch Loss: 0.09058940410614014\n",
      "Epoch 4845, Loss: 0.23053550720214844, Final Batch Loss: 0.12322638928890228\n",
      "Epoch 4846, Loss: 0.2532423138618469, Final Batch Loss: 0.09790642559528351\n",
      "Epoch 4847, Loss: 0.21998731791973114, Final Batch Loss: 0.1404871642589569\n",
      "Epoch 4848, Loss: 0.22085313498973846, Final Batch Loss: 0.13613419234752655\n",
      "Epoch 4849, Loss: 0.27624158561229706, Final Batch Loss: 0.13812607526779175\n",
      "Epoch 4850, Loss: 0.19012518972158432, Final Batch Loss: 0.11496137827634811\n",
      "Epoch 4851, Loss: 0.19135192036628723, Final Batch Loss: 0.08576980233192444\n",
      "Epoch 4852, Loss: 0.1731303483247757, Final Batch Loss: 0.08117729425430298\n",
      "Epoch 4853, Loss: 0.3077182173728943, Final Batch Loss: 0.21433547139167786\n",
      "Epoch 4854, Loss: 0.26340949535369873, Final Batch Loss: 0.12329335510730743\n",
      "Epoch 4855, Loss: 0.20552969723939896, Final Batch Loss: 0.07390628010034561\n",
      "Epoch 4856, Loss: 0.1804492101073265, Final Batch Loss: 0.10510188341140747\n",
      "Epoch 4857, Loss: 0.21258074045181274, Final Batch Loss: 0.11521995067596436\n",
      "Epoch 4858, Loss: 0.24508600682020187, Final Batch Loss: 0.10248967260122299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4859, Loss: 0.26455050706863403, Final Batch Loss: 0.1138475239276886\n",
      "Epoch 4860, Loss: 0.2599454000592232, Final Batch Loss: 0.12197015434503555\n",
      "Epoch 4861, Loss: 0.19420651346445084, Final Batch Loss: 0.08276506513357162\n",
      "Epoch 4862, Loss: 0.15331781655550003, Final Batch Loss: 0.06389104574918747\n",
      "Epoch 4863, Loss: 0.177162803709507, Final Batch Loss: 0.08785537630319595\n",
      "Epoch 4864, Loss: 0.22552762925624847, Final Batch Loss: 0.13588951528072357\n",
      "Epoch 4865, Loss: 0.1968330442905426, Final Batch Loss: 0.08307918906211853\n",
      "Epoch 4866, Loss: 0.19885173439979553, Final Batch Loss: 0.0743386521935463\n",
      "Epoch 4867, Loss: 0.33716531842947006, Final Batch Loss: 0.12305449694395065\n",
      "Epoch 4868, Loss: 0.23019421845674515, Final Batch Loss: 0.12749667465686798\n",
      "Epoch 4869, Loss: 0.2020956501364708, Final Batch Loss: 0.11966279149055481\n",
      "Epoch 4870, Loss: 0.17093275487422943, Final Batch Loss: 0.08728935569524765\n",
      "Epoch 4871, Loss: 0.20697277784347534, Final Batch Loss: 0.11963632702827454\n",
      "Epoch 4872, Loss: 0.21327253431081772, Final Batch Loss: 0.11416417360305786\n",
      "Epoch 4873, Loss: 0.19875410199165344, Final Batch Loss: 0.11744058132171631\n",
      "Epoch 4874, Loss: 0.16045638173818588, Final Batch Loss: 0.0656127855181694\n",
      "Epoch 4875, Loss: 0.17271534353494644, Final Batch Loss: 0.10368575900793076\n",
      "Epoch 4876, Loss: 0.16456346213817596, Final Batch Loss: 0.06460465490818024\n",
      "Epoch 4877, Loss: 0.17728684097528458, Final Batch Loss: 0.09276662766933441\n",
      "Epoch 4878, Loss: 0.13854625076055527, Final Batch Loss: 0.07010670006275177\n",
      "Epoch 4879, Loss: 0.19954289495944977, Final Batch Loss: 0.10161684453487396\n",
      "Epoch 4880, Loss: 0.1468791738152504, Final Batch Loss: 0.08018887042999268\n",
      "Epoch 4881, Loss: 0.1712654009461403, Final Batch Loss: 0.06995897740125656\n",
      "Epoch 4882, Loss: 0.19397646933794022, Final Batch Loss: 0.07930120080709457\n",
      "Epoch 4883, Loss: 0.17212120443582535, Final Batch Loss: 0.06938005238771439\n",
      "Epoch 4884, Loss: 0.13995987176895142, Final Batch Loss: 0.07385782897472382\n",
      "Epoch 4885, Loss: 0.1916077807545662, Final Batch Loss: 0.09385836869478226\n",
      "Epoch 4886, Loss: 0.1707025170326233, Final Batch Loss: 0.0914631262421608\n",
      "Epoch 4887, Loss: 0.15755733847618103, Final Batch Loss: 0.06885340809822083\n",
      "Epoch 4888, Loss: 0.19737624377012253, Final Batch Loss: 0.1003950908780098\n",
      "Epoch 4889, Loss: 0.14311335235834122, Final Batch Loss: 0.06539708375930786\n",
      "Epoch 4890, Loss: 0.17595230787992477, Final Batch Loss: 0.09347818046808243\n",
      "Epoch 4891, Loss: 0.2013506144285202, Final Batch Loss: 0.0744868665933609\n",
      "Epoch 4892, Loss: 0.15818674117326736, Final Batch Loss: 0.07097041606903076\n",
      "Epoch 4893, Loss: 0.175462543964386, Final Batch Loss: 0.07424204051494598\n",
      "Epoch 4894, Loss: 0.17420727759599686, Final Batch Loss: 0.08951415121555328\n",
      "Epoch 4895, Loss: 0.19609375298023224, Final Batch Loss: 0.0900353416800499\n",
      "Epoch 4896, Loss: 0.17329145595431328, Final Batch Loss: 0.11621315032243729\n",
      "Epoch 4897, Loss: 0.159717358648777, Final Batch Loss: 0.07239606976509094\n",
      "Epoch 4898, Loss: 0.19462137669324875, Final Batch Loss: 0.08377433568239212\n",
      "Epoch 4899, Loss: 0.21804216504096985, Final Batch Loss: 0.12934795022010803\n",
      "Epoch 4900, Loss: 0.22057528048753738, Final Batch Loss: 0.08109351247549057\n",
      "Epoch 4901, Loss: 0.20725397020578384, Final Batch Loss: 0.12257752567529678\n",
      "Epoch 4902, Loss: 0.2142229825258255, Final Batch Loss: 0.12031674385070801\n",
      "Epoch 4903, Loss: 0.1498207375407219, Final Batch Loss: 0.06354334205389023\n",
      "Epoch 4904, Loss: 0.1676206961274147, Final Batch Loss: 0.0893334373831749\n",
      "Epoch 4905, Loss: 0.1931927353143692, Final Batch Loss: 0.09066479653120041\n",
      "Epoch 4906, Loss: 0.14881430566310883, Final Batch Loss: 0.06032510846853256\n",
      "Epoch 4907, Loss: 0.171979121863842, Final Batch Loss: 0.07176010310649872\n",
      "Epoch 4908, Loss: 0.19000469148159027, Final Batch Loss: 0.09170924872159958\n",
      "Epoch 4909, Loss: 0.15840069204568863, Final Batch Loss: 0.09055975079536438\n",
      "Epoch 4910, Loss: 0.23797836154699326, Final Batch Loss: 0.12555141746997833\n",
      "Epoch 4911, Loss: 0.18952391296625137, Final Batch Loss: 0.06465303152799606\n",
      "Epoch 4912, Loss: 0.13789206743240356, Final Batch Loss: 0.07003415375947952\n",
      "Epoch 4913, Loss: 0.1766735464334488, Final Batch Loss: 0.09391864389181137\n",
      "Epoch 4914, Loss: 0.14665939286351204, Final Batch Loss: 0.04998496547341347\n",
      "Epoch 4915, Loss: 0.22079744189977646, Final Batch Loss: 0.09257686883211136\n",
      "Epoch 4916, Loss: 0.19373248517513275, Final Batch Loss: 0.09999474883079529\n",
      "Epoch 4917, Loss: 0.22892194986343384, Final Batch Loss: 0.10478594154119492\n",
      "Epoch 4918, Loss: 0.17670408636331558, Final Batch Loss: 0.08929384499788284\n",
      "Epoch 4919, Loss: 0.14767137169837952, Final Batch Loss: 0.07008717209100723\n",
      "Epoch 4920, Loss: 0.19811153411865234, Final Batch Loss: 0.10779611021280289\n",
      "Epoch 4921, Loss: 0.16939741373062134, Final Batch Loss: 0.08760116994380951\n",
      "Epoch 4922, Loss: 0.2296559140086174, Final Batch Loss: 0.1013917401432991\n",
      "Epoch 4923, Loss: 0.15517369657754898, Final Batch Loss: 0.05031739920377731\n",
      "Epoch 4924, Loss: 0.2018452286720276, Final Batch Loss: 0.09903096407651901\n",
      "Epoch 4925, Loss: 0.16873130947351456, Final Batch Loss: 0.10368124395608902\n",
      "Epoch 4926, Loss: 0.20305392891168594, Final Batch Loss: 0.07619632035493851\n",
      "Epoch 4927, Loss: 0.19327465444803238, Final Batch Loss: 0.09022578597068787\n",
      "Epoch 4928, Loss: 0.21598736941814423, Final Batch Loss: 0.1045127883553505\n",
      "Epoch 4929, Loss: 0.19968494027853012, Final Batch Loss: 0.09684024751186371\n",
      "Epoch 4930, Loss: 0.17760063335299492, Final Batch Loss: 0.059856560081243515\n",
      "Epoch 4931, Loss: 0.22422001138329506, Final Batch Loss: 0.06155203655362129\n",
      "Epoch 4932, Loss: 0.16469980776309967, Final Batch Loss: 0.08174318075180054\n",
      "Epoch 4933, Loss: 0.20270714908838272, Final Batch Loss: 0.11088502407073975\n",
      "Epoch 4934, Loss: 0.23389189690351486, Final Batch Loss: 0.10653238743543625\n",
      "Epoch 4935, Loss: 0.17663482576608658, Final Batch Loss: 0.07382693886756897\n",
      "Epoch 4936, Loss: 0.1697220280766487, Final Batch Loss: 0.07805588841438293\n",
      "Epoch 4937, Loss: 0.19713427871465683, Final Batch Loss: 0.1082225888967514\n",
      "Epoch 4938, Loss: 0.2165176272392273, Final Batch Loss: 0.1288583129644394\n",
      "Epoch 4939, Loss: 0.30194689333438873, Final Batch Loss: 0.217801034450531\n",
      "Epoch 4940, Loss: 0.14693089574575424, Final Batch Loss: 0.05160735547542572\n",
      "Epoch 4941, Loss: 0.18634263426065445, Final Batch Loss: 0.09560955315828323\n",
      "Epoch 4942, Loss: 0.234202079474926, Final Batch Loss: 0.1392107605934143\n",
      "Epoch 4943, Loss: 0.18469954282045364, Final Batch Loss: 0.1289619654417038\n",
      "Epoch 4944, Loss: 0.1846497654914856, Final Batch Loss: 0.12089056521654129\n",
      "Epoch 4945, Loss: 0.23760250210762024, Final Batch Loss: 0.11388202756643295\n",
      "Epoch 4946, Loss: 0.23282932490110397, Final Batch Loss: 0.11683933436870575\n",
      "Epoch 4947, Loss: 0.20116326957941055, Final Batch Loss: 0.07072097808122635\n",
      "Epoch 4948, Loss: 0.24747058004140854, Final Batch Loss: 0.09382256120443344\n",
      "Epoch 4949, Loss: 0.1846136674284935, Final Batch Loss: 0.10066864639520645\n",
      "Epoch 4950, Loss: 0.18773969262838364, Final Batch Loss: 0.06708384305238724\n",
      "Epoch 4951, Loss: 0.18828461319208145, Final Batch Loss: 0.10973186790943146\n",
      "Epoch 4952, Loss: 0.18055938929319382, Final Batch Loss: 0.09634863585233688\n",
      "Epoch 4953, Loss: 0.15418295189738274, Final Batch Loss: 0.06184082105755806\n",
      "Epoch 4954, Loss: 0.21374663710594177, Final Batch Loss: 0.11807897686958313\n",
      "Epoch 4955, Loss: 0.20686229318380356, Final Batch Loss: 0.09446106106042862\n",
      "Epoch 4956, Loss: 0.14780159294605255, Final Batch Loss: 0.06483173370361328\n",
      "Epoch 4957, Loss: 0.25452667474746704, Final Batch Loss: 0.10707680881023407\n",
      "Epoch 4958, Loss: 0.19548173248767853, Final Batch Loss: 0.08679503947496414\n",
      "Epoch 4959, Loss: 0.15715430676937103, Final Batch Loss: 0.1050397977232933\n",
      "Epoch 4960, Loss: 0.1677522286772728, Final Batch Loss: 0.08062141388654709\n",
      "Epoch 4961, Loss: 0.22905060648918152, Final Batch Loss: 0.14470721781253815\n",
      "Epoch 4962, Loss: 0.16703610122203827, Final Batch Loss: 0.09941025078296661\n",
      "Epoch 4963, Loss: 0.2463308572769165, Final Batch Loss: 0.10216003656387329\n",
      "Epoch 4964, Loss: 0.21156425774097443, Final Batch Loss: 0.09909435361623764\n",
      "Epoch 4965, Loss: 0.22578447312116623, Final Batch Loss: 0.0698542520403862\n",
      "Epoch 4966, Loss: 0.18599475175142288, Final Batch Loss: 0.1008627638220787\n",
      "Epoch 4967, Loss: 0.18060234934091568, Final Batch Loss: 0.10236376523971558\n",
      "Epoch 4968, Loss: 0.2500293627381325, Final Batch Loss: 0.10417962819337845\n",
      "Epoch 4969, Loss: 0.2647170275449753, Final Batch Loss: 0.15438266098499298\n",
      "Epoch 4970, Loss: 0.2123057320713997, Final Batch Loss: 0.11383535712957382\n",
      "Epoch 4971, Loss: 0.21393902599811554, Final Batch Loss: 0.09815901517868042\n",
      "Epoch 4972, Loss: 0.24235065281391144, Final Batch Loss: 0.1116856187582016\n",
      "Epoch 4973, Loss: 0.23906797170639038, Final Batch Loss: 0.12699581682682037\n",
      "Epoch 4974, Loss: 0.1643056720495224, Final Batch Loss: 0.09175240248441696\n",
      "Epoch 4975, Loss: 0.22871780395507812, Final Batch Loss: 0.11204596608877182\n",
      "Epoch 4976, Loss: 0.2532331049442291, Final Batch Loss: 0.08625611662864685\n",
      "Epoch 4977, Loss: 0.15861352533102036, Final Batch Loss: 0.07037784904241562\n",
      "Epoch 4978, Loss: 0.2712792158126831, Final Batch Loss: 0.12115965783596039\n",
      "Epoch 4979, Loss: 0.17252705246210098, Final Batch Loss: 0.07973536849021912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4980, Loss: 0.2877914607524872, Final Batch Loss: 0.10279883444309235\n",
      "Epoch 4981, Loss: 0.20472519099712372, Final Batch Loss: 0.10892688482999802\n",
      "Epoch 4982, Loss: 0.1781422197818756, Final Batch Loss: 0.1073606088757515\n",
      "Epoch 4983, Loss: 0.21320762485265732, Final Batch Loss: 0.11308258026838303\n",
      "Epoch 4984, Loss: 0.23650283366441727, Final Batch Loss: 0.12185713648796082\n",
      "Epoch 4985, Loss: 0.1839320883154869, Final Batch Loss: 0.09868413954973221\n",
      "Epoch 4986, Loss: 0.15097910165786743, Final Batch Loss: 0.07208126038312912\n",
      "Epoch 4987, Loss: 0.25510965287685394, Final Batch Loss: 0.12887461483478546\n",
      "Epoch 4988, Loss: 0.2018863782286644, Final Batch Loss: 0.10436046123504639\n",
      "Epoch 4989, Loss: 0.18235743790864944, Final Batch Loss: 0.10025032609701157\n",
      "Epoch 4990, Loss: 0.21616791188716888, Final Batch Loss: 0.10076625645160675\n",
      "Epoch 4991, Loss: 0.1842346414923668, Final Batch Loss: 0.07780791819095612\n",
      "Epoch 4992, Loss: 0.17827627062797546, Final Batch Loss: 0.07654419541358948\n",
      "Epoch 4993, Loss: 0.19024048745632172, Final Batch Loss: 0.07803789526224136\n",
      "Epoch 4994, Loss: 0.14538658782839775, Final Batch Loss: 0.04665294662117958\n",
      "Epoch 4995, Loss: 0.18257560580968857, Final Batch Loss: 0.10685793310403824\n",
      "Epoch 4996, Loss: 0.20773402601480484, Final Batch Loss: 0.10393313318490982\n",
      "Epoch 4997, Loss: 0.19820386916399002, Final Batch Loss: 0.0717092826962471\n",
      "Epoch 4998, Loss: 0.21829576790332794, Final Batch Loss: 0.1173185408115387\n",
      "Epoch 4999, Loss: 0.16693435609340668, Final Batch Loss: 0.055377207696437836\n",
      "Epoch 5000, Loss: 0.2072770744562149, Final Batch Loss: 0.09538155794143677\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 16  0  0  0  0]\n",
      " [ 0  0  3  0  0  8  0  0  1]\n",
      " [ 0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  1  0  0  1  0  0 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        15\n",
      "           1    1.00000   1.00000   1.00000         6\n",
      "           2    0.71429   1.00000   0.83333        10\n",
      "           3    1.00000   1.00000   1.00000        16\n",
      "           4    1.00000   1.00000   1.00000        16\n",
      "           5    0.88889   0.66667   0.76190        12\n",
      "           6    1.00000   1.00000   1.00000         7\n",
      "           7    1.00000   1.00000   1.00000        11\n",
      "           8    0.93750   0.88235   0.90909        17\n",
      "\n",
      "    accuracy                        0.94545       110\n",
      "   macro avg    0.94896   0.94989   0.94493       110\n",
      "weighted avg    0.95225   0.94545   0.94482       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_Group_5_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 3)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "\n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0  0  0  0  0  0  0]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0 15  1  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  2]\n",
      " [ 0  0  0  0  1  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        13\n",
      "           1    1.00000   1.00000   1.00000        19\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   0.93750   0.96774        16\n",
      "           4    0.77778   1.00000   0.87500         7\n",
      "           5    1.00000   0.83333   0.90909        12\n",
      "           6    1.00000   0.91667   0.95652        12\n",
      "           7    1.00000   1.00000   1.00000        13\n",
      "           8    0.83333   1.00000   0.90909        10\n",
      "\n",
      "    accuracy                        0.96364       110\n",
      "   macro avg    0.95679   0.96528   0.95749       110\n",
      "weighted avg    0.97071   0.96364   0.96443       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
