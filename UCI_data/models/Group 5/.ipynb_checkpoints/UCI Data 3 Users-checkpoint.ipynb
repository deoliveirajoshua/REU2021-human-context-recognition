{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                 -0.976623              -0.976353  ...   \n",
       "1                 -0.989046              -0.989038  ...   \n",
       "2                 -0.993552              -0.994122  ...   \n",
       "3                 -0.992407              -0.993142  ...   \n",
       "4                 -0.992378              -0.992542  ...   \n",
       "...                     ...                    ...  ...   \n",
       "7347               0.084878               0.065142  ...   \n",
       "7348               0.098249               0.091791  ...   \n",
       "7349               0.185902               0.170686  ...   \n",
       "7350               0.190360               0.178939  ...   \n",
       "7351               0.022216              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                    -0.998285        1         5  \n",
       "1                    -0.999472        1         5  \n",
       "2                    -0.999807        1         5  \n",
       "3                    -0.999770        1         5  \n",
       "4                    -0.999873        1         5  \n",
       "...                        ...      ...       ...  \n",
       "7347                 -0.584282       30         2  \n",
       "7348                 -0.632536       30         2  \n",
       "7349                 -0.641170       30         2  \n",
       "7350                 -0.663579       30         2  \n",
       "7351                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([19, 21, 22])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([19, 21, 22])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 19:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 21:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2003588676452637, Final Batch Loss: 1.0999960899353027\n",
      "Epoch 2, Loss: 2.1994292736053467, Final Batch Loss: 1.1053839921951294\n",
      "Epoch 3, Loss: 2.19439160823822, Final Batch Loss: 1.096760630607605\n",
      "Epoch 4, Loss: 2.1958166360855103, Final Batch Loss: 1.0987480878829956\n",
      "Epoch 5, Loss: 2.189229130744934, Final Batch Loss: 1.0882147550582886\n",
      "Epoch 6, Loss: 2.1894664764404297, Final Batch Loss: 1.0939445495605469\n",
      "Epoch 7, Loss: 2.187725305557251, Final Batch Loss: 1.0956408977508545\n",
      "Epoch 8, Loss: 2.18701171875, Final Batch Loss: 1.0951769351959229\n",
      "Epoch 9, Loss: 2.1832849979400635, Final Batch Loss: 1.0900864601135254\n",
      "Epoch 10, Loss: 2.182266354560852, Final Batch Loss: 1.0925668478012085\n",
      "Epoch 11, Loss: 2.1776556968688965, Final Batch Loss: 1.0911921262741089\n",
      "Epoch 12, Loss: 2.1767996549606323, Final Batch Loss: 1.0925865173339844\n",
      "Epoch 13, Loss: 2.172520875930786, Final Batch Loss: 1.0839452743530273\n",
      "Epoch 14, Loss: 2.166998267173767, Final Batch Loss: 1.07906174659729\n",
      "Epoch 15, Loss: 2.1697582006454468, Final Batch Loss: 1.0874632596969604\n",
      "Epoch 16, Loss: 2.1584763526916504, Final Batch Loss: 1.075973629951477\n",
      "Epoch 17, Loss: 2.1642986536026, Final Batch Loss: 1.0838526487350464\n",
      "Epoch 18, Loss: 2.154292345046997, Final Batch Loss: 1.081407070159912\n",
      "Epoch 19, Loss: 2.1433284282684326, Final Batch Loss: 1.0745257139205933\n",
      "Epoch 20, Loss: 2.1385916471481323, Final Batch Loss: 1.0684993267059326\n",
      "Epoch 21, Loss: 2.128713369369507, Final Batch Loss: 1.060682773590088\n",
      "Epoch 22, Loss: 2.117650032043457, Final Batch Loss: 1.0506445169448853\n",
      "Epoch 23, Loss: 2.1186996698379517, Final Batch Loss: 1.0605363845825195\n",
      "Epoch 24, Loss: 2.1213200092315674, Final Batch Loss: 1.0712767839431763\n",
      "Epoch 25, Loss: 2.0961824655532837, Final Batch Loss: 1.0538320541381836\n",
      "Epoch 26, Loss: 2.0769747495651245, Final Batch Loss: 1.0300195217132568\n",
      "Epoch 27, Loss: 2.0685538053512573, Final Batch Loss: 1.0419461727142334\n",
      "Epoch 28, Loss: 2.0396729707717896, Final Batch Loss: 1.006107211112976\n",
      "Epoch 29, Loss: 2.036074995994568, Final Batch Loss: 1.012601613998413\n",
      "Epoch 30, Loss: 2.0127379298210144, Final Batch Loss: 0.9931004643440247\n",
      "Epoch 31, Loss: 1.9794930219650269, Final Batch Loss: 1.0007437467575073\n",
      "Epoch 32, Loss: 1.9672395586967468, Final Batch Loss: 0.9620302319526672\n",
      "Epoch 33, Loss: 1.9102817177772522, Final Batch Loss: 0.9229982495307922\n",
      "Epoch 34, Loss: 1.9223725199699402, Final Batch Loss: 0.979995608329773\n",
      "Epoch 35, Loss: 1.894857406616211, Final Batch Loss: 0.9539205431938171\n",
      "Epoch 36, Loss: 1.851150393486023, Final Batch Loss: 0.8880815505981445\n",
      "Epoch 37, Loss: 1.8528817296028137, Final Batch Loss: 0.9229934811592102\n",
      "Epoch 38, Loss: 1.824133038520813, Final Batch Loss: 0.9160422086715698\n",
      "Epoch 39, Loss: 1.8068355917930603, Final Batch Loss: 0.9278304576873779\n",
      "Epoch 40, Loss: 1.770734429359436, Final Batch Loss: 0.8685023188591003\n",
      "Epoch 41, Loss: 1.7267134189605713, Final Batch Loss: 0.8531734347343445\n",
      "Epoch 42, Loss: 1.7035619020462036, Final Batch Loss: 0.8283084034919739\n",
      "Epoch 43, Loss: 1.6271430850028992, Final Batch Loss: 0.7705446481704712\n",
      "Epoch 44, Loss: 1.6543781161308289, Final Batch Loss: 0.8413411974906921\n",
      "Epoch 45, Loss: 1.6012664437294006, Final Batch Loss: 0.8015226721763611\n",
      "Epoch 46, Loss: 1.543412685394287, Final Batch Loss: 0.7517063021659851\n",
      "Epoch 47, Loss: 1.5646058320999146, Final Batch Loss: 0.7766454219818115\n",
      "Epoch 48, Loss: 1.5120897889137268, Final Batch Loss: 0.760840892791748\n",
      "Epoch 49, Loss: 1.5184993147850037, Final Batch Loss: 0.7809160351753235\n",
      "Epoch 50, Loss: 1.5094433426856995, Final Batch Loss: 0.7744215130805969\n",
      "Epoch 51, Loss: 1.438628911972046, Final Batch Loss: 0.6971676349639893\n",
      "Epoch 52, Loss: 1.4054720401763916, Final Batch Loss: 0.6672788858413696\n",
      "Epoch 53, Loss: 1.402438461780548, Final Batch Loss: 0.701575756072998\n",
      "Epoch 54, Loss: 1.3318543434143066, Final Batch Loss: 0.6943789124488831\n",
      "Epoch 55, Loss: 1.3150300979614258, Final Batch Loss: 0.6160410046577454\n",
      "Epoch 56, Loss: 1.3145391345024109, Final Batch Loss: 0.6242078542709351\n",
      "Epoch 57, Loss: 1.2937031388282776, Final Batch Loss: 0.6355340480804443\n",
      "Epoch 58, Loss: 1.2312801480293274, Final Batch Loss: 0.5536894202232361\n",
      "Epoch 59, Loss: 1.2739312052726746, Final Batch Loss: 0.6260499358177185\n",
      "Epoch 60, Loss: 1.267352283000946, Final Batch Loss: 0.6814176440238953\n",
      "Epoch 61, Loss: 1.1620903611183167, Final Batch Loss: 0.5917523503303528\n",
      "Epoch 62, Loss: 1.172141194343567, Final Batch Loss: 0.5521594285964966\n",
      "Epoch 63, Loss: 1.1865785717964172, Final Batch Loss: 0.575416088104248\n",
      "Epoch 64, Loss: 1.1184234619140625, Final Batch Loss: 0.48601871728897095\n",
      "Epoch 65, Loss: 1.1349129676818848, Final Batch Loss: 0.5592106580734253\n",
      "Epoch 66, Loss: 1.0964012145996094, Final Batch Loss: 0.5454016923904419\n",
      "Epoch 67, Loss: 1.0895752906799316, Final Batch Loss: 0.5667871832847595\n",
      "Epoch 68, Loss: 1.0710428357124329, Final Batch Loss: 0.5102260112762451\n",
      "Epoch 69, Loss: 1.0828099250793457, Final Batch Loss: 0.5505526065826416\n",
      "Epoch 70, Loss: 1.0508163571357727, Final Batch Loss: 0.48857903480529785\n",
      "Epoch 71, Loss: 0.9999008476734161, Final Batch Loss: 0.48138120770454407\n",
      "Epoch 72, Loss: 1.0134506225585938, Final Batch Loss: 0.5077740550041199\n",
      "Epoch 73, Loss: 1.0452937483787537, Final Batch Loss: 0.5484205484390259\n",
      "Epoch 74, Loss: 0.9727621078491211, Final Batch Loss: 0.4345173239707947\n",
      "Epoch 75, Loss: 1.0151458978652954, Final Batch Loss: 0.47110432386398315\n",
      "Epoch 76, Loss: 0.9175453782081604, Final Batch Loss: 0.42106035351753235\n",
      "Epoch 77, Loss: 0.920972466468811, Final Batch Loss: 0.3786408305168152\n",
      "Epoch 78, Loss: 0.9763242602348328, Final Batch Loss: 0.48546770215034485\n",
      "Epoch 79, Loss: 0.9415308237075806, Final Batch Loss: 0.4863399267196655\n",
      "Epoch 80, Loss: 0.9255672693252563, Final Batch Loss: 0.43767887353897095\n",
      "Epoch 81, Loss: 0.9196466505527496, Final Batch Loss: 0.4808596968650818\n",
      "Epoch 82, Loss: 0.9898296296596527, Final Batch Loss: 0.4653138220310211\n",
      "Epoch 83, Loss: 0.9067892134189606, Final Batch Loss: 0.46806198358535767\n",
      "Epoch 84, Loss: 0.934762179851532, Final Batch Loss: 0.4552033543586731\n",
      "Epoch 85, Loss: 0.8810884952545166, Final Batch Loss: 0.4222545921802521\n",
      "Epoch 86, Loss: 0.8898990750312805, Final Batch Loss: 0.4257366359233856\n",
      "Epoch 87, Loss: 0.8671785891056061, Final Batch Loss: 0.44893380999565125\n",
      "Epoch 88, Loss: 0.8473240733146667, Final Batch Loss: 0.40017420053482056\n",
      "Epoch 89, Loss: 0.880897730588913, Final Batch Loss: 0.4388445317745209\n",
      "Epoch 90, Loss: 0.8361929357051849, Final Batch Loss: 0.39529332518577576\n",
      "Epoch 91, Loss: 0.764084666967392, Final Batch Loss: 0.38406357169151306\n",
      "Epoch 92, Loss: 0.8789573907852173, Final Batch Loss: 0.4256064295768738\n",
      "Epoch 93, Loss: 0.7892995178699493, Final Batch Loss: 0.377750039100647\n",
      "Epoch 94, Loss: 0.7726080119609833, Final Batch Loss: 0.3753978908061981\n",
      "Epoch 95, Loss: 0.7648459672927856, Final Batch Loss: 0.33808550238609314\n",
      "Epoch 96, Loss: 0.800599604845047, Final Batch Loss: 0.40392303466796875\n",
      "Epoch 97, Loss: 0.7660718262195587, Final Batch Loss: 0.3208814859390259\n",
      "Epoch 98, Loss: 0.8039683103561401, Final Batch Loss: 0.40830445289611816\n",
      "Epoch 99, Loss: 0.792810320854187, Final Batch Loss: 0.43638184666633606\n",
      "Epoch 100, Loss: 0.7583347856998444, Final Batch Loss: 0.4106888473033905\n",
      "Epoch 101, Loss: 0.7549391984939575, Final Batch Loss: 0.39029550552368164\n",
      "Epoch 102, Loss: 0.7408739328384399, Final Batch Loss: 0.3459243178367615\n",
      "Epoch 103, Loss: 0.7410054802894592, Final Batch Loss: 0.35677778720855713\n",
      "Epoch 104, Loss: 0.7440103590488434, Final Batch Loss: 0.376914918422699\n",
      "Epoch 105, Loss: 0.7571713626384735, Final Batch Loss: 0.3494032323360443\n",
      "Epoch 106, Loss: 0.6752623915672302, Final Batch Loss: 0.32340875267982483\n",
      "Epoch 107, Loss: 0.7173499166965485, Final Batch Loss: 0.3812197148799896\n",
      "Epoch 108, Loss: 0.7134749889373779, Final Batch Loss: 0.3738536238670349\n",
      "Epoch 109, Loss: 0.7489876747131348, Final Batch Loss: 0.39733555912971497\n",
      "Epoch 110, Loss: 0.7617699801921844, Final Batch Loss: 0.39915168285369873\n",
      "Epoch 111, Loss: 0.6896984577178955, Final Batch Loss: 0.37695690989494324\n",
      "Epoch 112, Loss: 0.7209000885486603, Final Batch Loss: 0.4008844196796417\n",
      "Epoch 113, Loss: 0.6473667919635773, Final Batch Loss: 0.3490249812602997\n",
      "Epoch 114, Loss: 0.6742412447929382, Final Batch Loss: 0.3061891794204712\n",
      "Epoch 115, Loss: 0.709891676902771, Final Batch Loss: 0.3475325107574463\n",
      "Epoch 116, Loss: 0.7128322422504425, Final Batch Loss: 0.36219850182533264\n",
      "Epoch 117, Loss: 0.6200181841850281, Final Batch Loss: 0.29581719636917114\n",
      "Epoch 118, Loss: 0.6578024625778198, Final Batch Loss: 0.3223817050457001\n",
      "Epoch 119, Loss: 0.6289404630661011, Final Batch Loss: 0.3342144787311554\n",
      "Epoch 120, Loss: 0.5987441539764404, Final Batch Loss: 0.2896209955215454\n",
      "Epoch 121, Loss: 0.5687014609575272, Final Batch Loss: 0.238877072930336\n",
      "Epoch 122, Loss: 0.597081869840622, Final Batch Loss: 0.28436771035194397\n",
      "Epoch 123, Loss: 0.6055416762828827, Final Batch Loss: 0.31979337334632874\n",
      "Epoch 124, Loss: 0.6306194961071014, Final Batch Loss: 0.31556814908981323\n",
      "Epoch 125, Loss: 0.56780706346035, Final Batch Loss: 0.240600124001503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: 0.6732995808124542, Final Batch Loss: 0.28126072883605957\n",
      "Epoch 127, Loss: 0.6296215951442719, Final Batch Loss: 0.35200491547584534\n",
      "Epoch 128, Loss: 0.5865400731563568, Final Batch Loss: 0.3169030249118805\n",
      "Epoch 129, Loss: 0.5656230747699738, Final Batch Loss: 0.2630401849746704\n",
      "Epoch 130, Loss: 0.5829218029975891, Final Batch Loss: 0.28471145033836365\n",
      "Epoch 131, Loss: 0.553909182548523, Final Batch Loss: 0.2751828730106354\n",
      "Epoch 132, Loss: 0.5901913642883301, Final Batch Loss: 0.3417213261127472\n",
      "Epoch 133, Loss: 0.591858983039856, Final Batch Loss: 0.282649427652359\n",
      "Epoch 134, Loss: 0.5861338376998901, Final Batch Loss: 0.31079408526420593\n",
      "Epoch 135, Loss: 0.5595206022262573, Final Batch Loss: 0.276577353477478\n",
      "Epoch 136, Loss: 0.5625419318675995, Final Batch Loss: 0.2540321946144104\n",
      "Epoch 137, Loss: 0.5719084739685059, Final Batch Loss: 0.28371375799179077\n",
      "Epoch 138, Loss: 0.5396347939968109, Final Batch Loss: 0.2744341790676117\n",
      "Epoch 139, Loss: 0.5682620108127594, Final Batch Loss: 0.33715566992759705\n",
      "Epoch 140, Loss: 0.5378600209951401, Final Batch Loss: 0.24697451293468475\n",
      "Epoch 141, Loss: 0.552084431052208, Final Batch Loss: 0.22908245027065277\n",
      "Epoch 142, Loss: 0.5676291882991791, Final Batch Loss: 0.28728148341178894\n",
      "Epoch 143, Loss: 0.547051876783371, Final Batch Loss: 0.26848888397216797\n",
      "Epoch 144, Loss: 0.5384445339441299, Final Batch Loss: 0.29088279604911804\n",
      "Epoch 145, Loss: 0.5477308630943298, Final Batch Loss: 0.2705088257789612\n",
      "Epoch 146, Loss: 0.463487908244133, Final Batch Loss: 0.2291252613067627\n",
      "Epoch 147, Loss: 0.5192911326885223, Final Batch Loss: 0.2596874535083771\n",
      "Epoch 148, Loss: 0.5414583683013916, Final Batch Loss: 0.249428391456604\n",
      "Epoch 149, Loss: 0.5486999154090881, Final Batch Loss: 0.29912877082824707\n",
      "Epoch 150, Loss: 0.502004474401474, Final Batch Loss: 0.26520606875419617\n",
      "Epoch 151, Loss: 0.49028079211711884, Final Batch Loss: 0.2182200402021408\n",
      "Epoch 152, Loss: 0.4712155759334564, Final Batch Loss: 0.19029518961906433\n",
      "Epoch 153, Loss: 0.46897347271442413, Final Batch Loss: 0.26703494787216187\n",
      "Epoch 154, Loss: 0.4661344587802887, Final Batch Loss: 0.22330226004123688\n",
      "Epoch 155, Loss: 0.5126343816518784, Final Batch Loss: 0.3212313652038574\n",
      "Epoch 156, Loss: 0.47906018793582916, Final Batch Loss: 0.23931139707565308\n",
      "Epoch 157, Loss: 0.46183376014232635, Final Batch Loss: 0.21914274990558624\n",
      "Epoch 158, Loss: 0.4683140069246292, Final Batch Loss: 0.2658156752586365\n",
      "Epoch 159, Loss: 0.4302673190832138, Final Batch Loss: 0.2278463989496231\n",
      "Epoch 160, Loss: 0.49779605865478516, Final Batch Loss: 0.23957151174545288\n",
      "Epoch 161, Loss: 0.43496929109096527, Final Batch Loss: 0.20851856470108032\n",
      "Epoch 162, Loss: 0.49525706470012665, Final Batch Loss: 0.28874415159225464\n",
      "Epoch 163, Loss: 0.4353756457567215, Final Batch Loss: 0.23468415439128876\n",
      "Epoch 164, Loss: 0.5012319087982178, Final Batch Loss: 0.2630937993526459\n",
      "Epoch 165, Loss: 0.4555627405643463, Final Batch Loss: 0.21356157958507538\n",
      "Epoch 166, Loss: 0.47639670968055725, Final Batch Loss: 0.19143268465995789\n",
      "Epoch 167, Loss: 0.47710295021533966, Final Batch Loss: 0.2560165226459503\n",
      "Epoch 168, Loss: 0.4909699559211731, Final Batch Loss: 0.24208886921405792\n",
      "Epoch 169, Loss: 0.4622870236635208, Final Batch Loss: 0.22120824456214905\n",
      "Epoch 170, Loss: 0.47503775358200073, Final Batch Loss: 0.23159946501255035\n",
      "Epoch 171, Loss: 0.4248521029949188, Final Batch Loss: 0.20396173000335693\n",
      "Epoch 172, Loss: 0.4857134521007538, Final Batch Loss: 0.27136334776878357\n",
      "Epoch 173, Loss: 0.4521116316318512, Final Batch Loss: 0.2133934050798416\n",
      "Epoch 174, Loss: 0.43772946298122406, Final Batch Loss: 0.23055817186832428\n",
      "Epoch 175, Loss: 0.5046969205141068, Final Batch Loss: 0.2675841152667999\n",
      "Epoch 176, Loss: 0.459488183259964, Final Batch Loss: 0.22231940925121307\n",
      "Epoch 177, Loss: 0.4818924069404602, Final Batch Loss: 0.2589925229549408\n",
      "Epoch 178, Loss: 0.44383157789707184, Final Batch Loss: 0.23279422521591187\n",
      "Epoch 179, Loss: 0.42151518166065216, Final Batch Loss: 0.18765246868133545\n",
      "Epoch 180, Loss: 0.40961724519729614, Final Batch Loss: 0.2259145975112915\n",
      "Epoch 181, Loss: 0.46921104192733765, Final Batch Loss: 0.21081721782684326\n",
      "Epoch 182, Loss: 0.46526242792606354, Final Batch Loss: 0.24202091991901398\n",
      "Epoch 183, Loss: 0.4960889369249344, Final Batch Loss: 0.23831872642040253\n",
      "Epoch 184, Loss: 0.44101524353027344, Final Batch Loss: 0.2486746907234192\n",
      "Epoch 185, Loss: 0.46454206109046936, Final Batch Loss: 0.20368748903274536\n",
      "Epoch 186, Loss: 0.4138137400150299, Final Batch Loss: 0.194402277469635\n",
      "Epoch 187, Loss: 0.4493723660707474, Final Batch Loss: 0.20823508501052856\n",
      "Epoch 188, Loss: 0.4331074059009552, Final Batch Loss: 0.2616293132305145\n",
      "Epoch 189, Loss: 0.45012103021144867, Final Batch Loss: 0.2654581665992737\n",
      "Epoch 190, Loss: 0.4157984107732773, Final Batch Loss: 0.19427728652954102\n",
      "Epoch 191, Loss: 0.45616622269153595, Final Batch Loss: 0.21698667109012604\n",
      "Epoch 192, Loss: 0.41501350700855255, Final Batch Loss: 0.21803772449493408\n",
      "Epoch 193, Loss: 0.4575231671333313, Final Batch Loss: 0.20250624418258667\n",
      "Epoch 194, Loss: 0.41246679425239563, Final Batch Loss: 0.2367745190858841\n",
      "Epoch 195, Loss: 0.45651179552078247, Final Batch Loss: 0.22429847717285156\n",
      "Epoch 196, Loss: 0.3789675235748291, Final Batch Loss: 0.188691645860672\n",
      "Epoch 197, Loss: 0.38785530626773834, Final Batch Loss: 0.18367981910705566\n",
      "Epoch 198, Loss: 0.452133908867836, Final Batch Loss: 0.22744514048099518\n",
      "Epoch 199, Loss: 0.4133298248052597, Final Batch Loss: 0.20520584285259247\n",
      "Epoch 200, Loss: 0.3942536413669586, Final Batch Loss: 0.20764002203941345\n",
      "Epoch 201, Loss: 0.4394936263561249, Final Batch Loss: 0.19906581938266754\n",
      "Epoch 202, Loss: 0.4078753739595413, Final Batch Loss: 0.23054581880569458\n",
      "Epoch 203, Loss: 0.41421838104724884, Final Batch Loss: 0.21641336381435394\n",
      "Epoch 204, Loss: 0.4497141093015671, Final Batch Loss: 0.1823609620332718\n",
      "Epoch 205, Loss: 0.3695240765810013, Final Batch Loss: 0.15837503969669342\n",
      "Epoch 206, Loss: 0.45345941185951233, Final Batch Loss: 0.26735028624534607\n",
      "Epoch 207, Loss: 0.4174950420856476, Final Batch Loss: 0.23417069017887115\n",
      "Epoch 208, Loss: 0.4085346907377243, Final Batch Loss: 0.2136814445257187\n",
      "Epoch 209, Loss: 0.3830648213624954, Final Batch Loss: 0.1777307689189911\n",
      "Epoch 210, Loss: 0.48068834841251373, Final Batch Loss: 0.27962884306907654\n",
      "Epoch 211, Loss: 0.3831208348274231, Final Batch Loss: 0.1578393429517746\n",
      "Epoch 212, Loss: 0.5031209737062454, Final Batch Loss: 0.2082088440656662\n",
      "Epoch 213, Loss: 0.41657431423664093, Final Batch Loss: 0.2296488881111145\n",
      "Epoch 214, Loss: 0.4622286707162857, Final Batch Loss: 0.19629134237766266\n",
      "Epoch 215, Loss: 0.39555972814559937, Final Batch Loss: 0.1890876740217209\n",
      "Epoch 216, Loss: 0.46771126985549927, Final Batch Loss: 0.26180583238601685\n",
      "Epoch 217, Loss: 0.43804705142974854, Final Batch Loss: 0.25329387187957764\n",
      "Epoch 218, Loss: 0.3927184194326401, Final Batch Loss: 0.15472574532032013\n",
      "Epoch 219, Loss: 0.4114953875541687, Final Batch Loss: 0.2193109691143036\n",
      "Epoch 220, Loss: 0.42081962525844574, Final Batch Loss: 0.22866761684417725\n",
      "Epoch 221, Loss: 0.39929987490177155, Final Batch Loss: 0.1995687186717987\n",
      "Epoch 222, Loss: 0.4437527656555176, Final Batch Loss: 0.270565390586853\n",
      "Epoch 223, Loss: 0.3480445295572281, Final Batch Loss: 0.16275568306446075\n",
      "Epoch 224, Loss: 0.3869560658931732, Final Batch Loss: 0.1885296255350113\n",
      "Epoch 225, Loss: 0.433585986495018, Final Batch Loss: 0.21428512036800385\n",
      "Epoch 226, Loss: 0.40263159573078156, Final Batch Loss: 0.21588431298732758\n",
      "Epoch 227, Loss: 0.36985352635383606, Final Batch Loss: 0.18462173640727997\n",
      "Epoch 228, Loss: 0.4004799425601959, Final Batch Loss: 0.207576721906662\n",
      "Epoch 229, Loss: 0.3992641270160675, Final Batch Loss: 0.1947321593761444\n",
      "Epoch 230, Loss: 0.3650659769773483, Final Batch Loss: 0.1457878202199936\n",
      "Epoch 231, Loss: 0.414789080619812, Final Batch Loss: 0.23167939484119415\n",
      "Epoch 232, Loss: 0.3586563318967819, Final Batch Loss: 0.1556830257177353\n",
      "Epoch 233, Loss: 0.39361752569675446, Final Batch Loss: 0.22543153166770935\n",
      "Epoch 234, Loss: 0.3611472100019455, Final Batch Loss: 0.1656379997730255\n",
      "Epoch 235, Loss: 0.4428389072418213, Final Batch Loss: 0.25050288438796997\n",
      "Epoch 236, Loss: 0.399135947227478, Final Batch Loss: 0.17808100581169128\n",
      "Epoch 237, Loss: 0.3534236252307892, Final Batch Loss: 0.17267365753650665\n",
      "Epoch 238, Loss: 0.3670078366994858, Final Batch Loss: 0.15307173132896423\n",
      "Epoch 239, Loss: 0.3749034255743027, Final Batch Loss: 0.23068676888942719\n",
      "Epoch 240, Loss: 0.3402527868747711, Final Batch Loss: 0.1778964251279831\n",
      "Epoch 241, Loss: 0.35160528123378754, Final Batch Loss: 0.18459336459636688\n",
      "Epoch 242, Loss: 0.44867776334285736, Final Batch Loss: 0.23094487190246582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243, Loss: 0.402204692363739, Final Batch Loss: 0.18423844873905182\n",
      "Epoch 244, Loss: 0.35942454636096954, Final Batch Loss: 0.16750817000865936\n",
      "Epoch 245, Loss: 0.37558500468730927, Final Batch Loss: 0.21854430437088013\n",
      "Epoch 246, Loss: 0.38058042526245117, Final Batch Loss: 0.1478143036365509\n",
      "Epoch 247, Loss: 0.3796877861022949, Final Batch Loss: 0.1734185665845871\n",
      "Epoch 248, Loss: 0.3555893003940582, Final Batch Loss: 0.2090868204832077\n",
      "Epoch 249, Loss: 0.4038201868534088, Final Batch Loss: 0.18530097603797913\n",
      "Epoch 250, Loss: 0.35575641691684723, Final Batch Loss: 0.18729208409786224\n",
      "Epoch 251, Loss: 0.352025106549263, Final Batch Loss: 0.16681253910064697\n",
      "Epoch 252, Loss: 0.33851440250873566, Final Batch Loss: 0.16107377409934998\n",
      "Epoch 253, Loss: 0.3881606012582779, Final Batch Loss: 0.20757512748241425\n",
      "Epoch 254, Loss: 0.3616553843021393, Final Batch Loss: 0.20709525048732758\n",
      "Epoch 255, Loss: 0.3697455823421478, Final Batch Loss: 0.2112428843975067\n",
      "Epoch 256, Loss: 0.3612717092037201, Final Batch Loss: 0.17662696540355682\n",
      "Epoch 257, Loss: 0.358317032456398, Final Batch Loss: 0.18166689574718475\n",
      "Epoch 258, Loss: 0.30947035551071167, Final Batch Loss: 0.16029773652553558\n",
      "Epoch 259, Loss: 0.3713807612657547, Final Batch Loss: 0.23265279829502106\n",
      "Epoch 260, Loss: 0.3640200346708298, Final Batch Loss: 0.20153309404850006\n",
      "Epoch 261, Loss: 0.41618263721466064, Final Batch Loss: 0.25373226404190063\n",
      "Epoch 262, Loss: 0.35662657022476196, Final Batch Loss: 0.15671230852603912\n",
      "Epoch 263, Loss: 0.37948334217071533, Final Batch Loss: 0.1998196393251419\n",
      "Epoch 264, Loss: 0.433219313621521, Final Batch Loss: 0.23603953421115875\n",
      "Epoch 265, Loss: 0.4039373844861984, Final Batch Loss: 0.1821099817752838\n",
      "Epoch 266, Loss: 0.39680320024490356, Final Batch Loss: 0.2022613137960434\n",
      "Epoch 267, Loss: 0.33108997344970703, Final Batch Loss: 0.13908454775810242\n",
      "Epoch 268, Loss: 0.29447339475154877, Final Batch Loss: 0.16624072194099426\n",
      "Epoch 269, Loss: 0.36528876423835754, Final Batch Loss: 0.16205745935440063\n",
      "Epoch 270, Loss: 0.3143208771944046, Final Batch Loss: 0.14240150153636932\n",
      "Epoch 271, Loss: 0.3301076889038086, Final Batch Loss: 0.15345776081085205\n",
      "Epoch 272, Loss: 0.3192882388830185, Final Batch Loss: 0.14128360152244568\n",
      "Epoch 273, Loss: 0.3477795720100403, Final Batch Loss: 0.2051815390586853\n",
      "Epoch 274, Loss: 0.30819568037986755, Final Batch Loss: 0.1347002536058426\n",
      "Epoch 275, Loss: 0.34859296679496765, Final Batch Loss: 0.18605445325374603\n",
      "Epoch 276, Loss: 0.3461453914642334, Final Batch Loss: 0.1632826328277588\n",
      "Epoch 277, Loss: 0.3396182954311371, Final Batch Loss: 0.16661059856414795\n",
      "Epoch 278, Loss: 0.2852330952882767, Final Batch Loss: 0.13424891233444214\n",
      "Epoch 279, Loss: 0.3402065485715866, Final Batch Loss: 0.1546439677476883\n",
      "Epoch 280, Loss: 0.3200303763151169, Final Batch Loss: 0.1539250612258911\n",
      "Epoch 281, Loss: 0.300907626748085, Final Batch Loss: 0.13056440651416779\n",
      "Epoch 282, Loss: 0.35747939348220825, Final Batch Loss: 0.15988945960998535\n",
      "Epoch 283, Loss: 0.3279564827680588, Final Batch Loss: 0.17251837253570557\n",
      "Epoch 284, Loss: 0.28655673563480377, Final Batch Loss: 0.11992806196212769\n",
      "Epoch 285, Loss: 0.31359095871448517, Final Batch Loss: 0.16138850152492523\n",
      "Epoch 286, Loss: 0.2801797389984131, Final Batch Loss: 0.11456505954265594\n",
      "Epoch 287, Loss: 0.2912069633603096, Final Batch Loss: 0.18575815856456757\n",
      "Epoch 288, Loss: 0.3458050638437271, Final Batch Loss: 0.1996469795703888\n",
      "Epoch 289, Loss: 0.3200778663158417, Final Batch Loss: 0.19199535250663757\n",
      "Epoch 290, Loss: 0.3232039585709572, Final Batch Loss: 0.19841398298740387\n",
      "Epoch 291, Loss: 0.3409564271569252, Final Batch Loss: 0.2172955721616745\n",
      "Epoch 292, Loss: 0.32571378350257874, Final Batch Loss: 0.1734711229801178\n",
      "Epoch 293, Loss: 0.3259521573781967, Final Batch Loss: 0.1900131106376648\n",
      "Epoch 294, Loss: 0.2988628000020981, Final Batch Loss: 0.15263184905052185\n",
      "Epoch 295, Loss: 0.31776162981987, Final Batch Loss: 0.16740845143795013\n",
      "Epoch 296, Loss: 0.34090153872966766, Final Batch Loss: 0.1794847846031189\n",
      "Epoch 297, Loss: 0.32510408759117126, Final Batch Loss: 0.16877049207687378\n",
      "Epoch 298, Loss: 0.30974559485912323, Final Batch Loss: 0.12077441811561584\n",
      "Epoch 299, Loss: 0.25794393569231033, Final Batch Loss: 0.11735496670007706\n",
      "Epoch 300, Loss: 0.2805843651294708, Final Batch Loss: 0.11707058548927307\n",
      "Epoch 301, Loss: 0.29886139184236526, Final Batch Loss: 0.11257676035165787\n",
      "Epoch 302, Loss: 0.3603239357471466, Final Batch Loss: 0.2167174518108368\n",
      "Epoch 303, Loss: 0.2974633574485779, Final Batch Loss: 0.16091947257518768\n",
      "Epoch 304, Loss: 0.30369360744953156, Final Batch Loss: 0.12703891098499298\n",
      "Epoch 305, Loss: 0.34484556317329407, Final Batch Loss: 0.1966831088066101\n",
      "Epoch 306, Loss: 0.29688818752765656, Final Batch Loss: 0.14584608376026154\n",
      "Epoch 307, Loss: 0.3471386432647705, Final Batch Loss: 0.2040233165025711\n",
      "Epoch 308, Loss: 0.33030152320861816, Final Batch Loss: 0.15781690180301666\n",
      "Epoch 309, Loss: 0.288937509059906, Final Batch Loss: 0.1195424348115921\n",
      "Epoch 310, Loss: 0.3362271636724472, Final Batch Loss: 0.14533740282058716\n",
      "Epoch 311, Loss: 0.2913832664489746, Final Batch Loss: 0.15383221209049225\n",
      "Epoch 312, Loss: 0.3281915932893753, Final Batch Loss: 0.17591680586338043\n",
      "Epoch 313, Loss: 0.3030630499124527, Final Batch Loss: 0.15786083042621613\n",
      "Epoch 314, Loss: 0.3195170909166336, Final Batch Loss: 0.18577474355697632\n",
      "Epoch 315, Loss: 0.31675902009010315, Final Batch Loss: 0.14965929090976715\n",
      "Epoch 316, Loss: 0.21563159674406052, Final Batch Loss: 0.09875132888555527\n",
      "Epoch 317, Loss: 0.3400600254535675, Final Batch Loss: 0.17716935276985168\n",
      "Epoch 318, Loss: 0.2732148692011833, Final Batch Loss: 0.15686967968940735\n",
      "Epoch 319, Loss: 0.2705892324447632, Final Batch Loss: 0.10834261775016785\n",
      "Epoch 320, Loss: 0.23566392809152603, Final Batch Loss: 0.11389507353305817\n",
      "Epoch 321, Loss: 0.28481268882751465, Final Batch Loss: 0.161871999502182\n",
      "Epoch 322, Loss: 0.27027714252471924, Final Batch Loss: 0.11228114366531372\n",
      "Epoch 323, Loss: 0.23344573378562927, Final Batch Loss: 0.1160985454916954\n",
      "Epoch 324, Loss: 0.32998035848140717, Final Batch Loss: 0.1923474371433258\n",
      "Epoch 325, Loss: 0.28678618371486664, Final Batch Loss: 0.12600208818912506\n",
      "Epoch 326, Loss: 0.26268450915813446, Final Batch Loss: 0.126426100730896\n",
      "Epoch 327, Loss: 0.3068614602088928, Final Batch Loss: 0.13335470855236053\n",
      "Epoch 328, Loss: 0.308964341878891, Final Batch Loss: 0.16515697538852692\n",
      "Epoch 329, Loss: 0.35816019773483276, Final Batch Loss: 0.20279429852962494\n",
      "Epoch 330, Loss: 0.2543950453400612, Final Batch Loss: 0.1409969925880432\n",
      "Epoch 331, Loss: 0.27274564653635025, Final Batch Loss: 0.1041698232293129\n",
      "Epoch 332, Loss: 0.32270530611276627, Final Batch Loss: 0.19851496815681458\n",
      "Epoch 333, Loss: 0.28909559547901154, Final Batch Loss: 0.14797112345695496\n",
      "Epoch 334, Loss: 0.33674436807632446, Final Batch Loss: 0.17988669872283936\n",
      "Epoch 335, Loss: 0.3774431645870209, Final Batch Loss: 0.2202654480934143\n",
      "Epoch 336, Loss: 0.3425745666027069, Final Batch Loss: 0.19116747379302979\n",
      "Epoch 337, Loss: 0.3174396902322769, Final Batch Loss: 0.17033268511295319\n",
      "Epoch 338, Loss: 0.3169028013944626, Final Batch Loss: 0.18233256042003632\n",
      "Epoch 339, Loss: 0.3423328846693039, Final Batch Loss: 0.1448710560798645\n",
      "Epoch 340, Loss: 0.3131726011633873, Final Batch Loss: 0.09658894687891006\n",
      "Epoch 341, Loss: 0.31232157349586487, Final Batch Loss: 0.14726746082305908\n",
      "Epoch 342, Loss: 0.33873823285102844, Final Batch Loss: 0.1695822775363922\n",
      "Epoch 343, Loss: 0.29388508200645447, Final Batch Loss: 0.15778227150440216\n",
      "Epoch 344, Loss: 0.2680710107088089, Final Batch Loss: 0.11689335107803345\n",
      "Epoch 345, Loss: 0.3363843262195587, Final Batch Loss: 0.16699670255184174\n",
      "Epoch 346, Loss: 0.248864583671093, Final Batch Loss: 0.12377514690160751\n",
      "Epoch 347, Loss: 0.26265398412942886, Final Batch Loss: 0.12386197596788406\n",
      "Epoch 348, Loss: 0.2809770405292511, Final Batch Loss: 0.14374332129955292\n",
      "Epoch 349, Loss: 0.2844345271587372, Final Batch Loss: 0.12104575335979462\n",
      "Epoch 350, Loss: 0.3071530759334564, Final Batch Loss: 0.1707315742969513\n",
      "Epoch 351, Loss: 0.23810647428035736, Final Batch Loss: 0.10553434491157532\n",
      "Epoch 352, Loss: 0.3121517151594162, Final Batch Loss: 0.16891787946224213\n",
      "Epoch 353, Loss: 0.29547251760959625, Final Batch Loss: 0.17058993875980377\n",
      "Epoch 354, Loss: 0.2271762192249298, Final Batch Loss: 0.1158931702375412\n",
      "Epoch 355, Loss: 0.2573803514242172, Final Batch Loss: 0.12144489586353302\n",
      "Epoch 356, Loss: 0.242792047560215, Final Batch Loss: 0.1102740541100502\n",
      "Epoch 357, Loss: 0.305333212018013, Final Batch Loss: 0.1372867077589035\n",
      "Epoch 358, Loss: 0.2739069536328316, Final Batch Loss: 0.1502535194158554\n",
      "Epoch 359, Loss: 0.2865656763315201, Final Batch Loss: 0.15050296485424042\n",
      "Epoch 360, Loss: 0.2558947131037712, Final Batch Loss: 0.15842165052890778\n",
      "Epoch 361, Loss: 0.22828291356563568, Final Batch Loss: 0.13515184819698334\n",
      "Epoch 362, Loss: 0.25997771322727203, Final Batch Loss: 0.12697891891002655\n",
      "Epoch 363, Loss: 0.19164183735847473, Final Batch Loss: 0.10068000853061676\n",
      "Epoch 364, Loss: 0.2287558913230896, Final Batch Loss: 0.12179311364889145\n",
      "Epoch 365, Loss: 0.26827558875083923, Final Batch Loss: 0.14230895042419434\n",
      "Epoch 366, Loss: 0.2794957011938095, Final Batch Loss: 0.1446135938167572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367, Loss: 0.2451411560177803, Final Batch Loss: 0.10712439566850662\n",
      "Epoch 368, Loss: 0.20692430436611176, Final Batch Loss: 0.09551474452018738\n",
      "Epoch 369, Loss: 0.2228720337152481, Final Batch Loss: 0.10310327261686325\n",
      "Epoch 370, Loss: 0.222728930413723, Final Batch Loss: 0.08412680774927139\n",
      "Epoch 371, Loss: 0.23374947905540466, Final Batch Loss: 0.09347891807556152\n",
      "Epoch 372, Loss: 0.21512342989444733, Final Batch Loss: 0.09418755769729614\n",
      "Epoch 373, Loss: 0.23860137164592743, Final Batch Loss: 0.10366669297218323\n",
      "Epoch 374, Loss: 0.2071124017238617, Final Batch Loss: 0.08205176889896393\n",
      "Epoch 375, Loss: 0.23964745551347733, Final Batch Loss: 0.12059686332941055\n",
      "Epoch 376, Loss: 0.2297155037522316, Final Batch Loss: 0.09309592097997665\n",
      "Epoch 377, Loss: 0.26352090388536453, Final Batch Loss: 0.11978700011968613\n",
      "Epoch 378, Loss: 0.24831225723028183, Final Batch Loss: 0.12226178497076035\n",
      "Epoch 379, Loss: 0.26608848571777344, Final Batch Loss: 0.127240851521492\n",
      "Epoch 380, Loss: 0.2159983441233635, Final Batch Loss: 0.08812790364027023\n",
      "Epoch 381, Loss: 0.2271665707230568, Final Batch Loss: 0.11169158667325974\n",
      "Epoch 382, Loss: 0.26403388381004333, Final Batch Loss: 0.13787126541137695\n",
      "Epoch 383, Loss: 0.24075214564800262, Final Batch Loss: 0.14546677470207214\n",
      "Epoch 384, Loss: 0.2763361185789108, Final Batch Loss: 0.14658993482589722\n",
      "Epoch 385, Loss: 0.27066828310489655, Final Batch Loss: 0.11431483924388885\n",
      "Epoch 386, Loss: 0.16602817922830582, Final Batch Loss: 0.08248449116945267\n",
      "Epoch 387, Loss: 0.22476376593112946, Final Batch Loss: 0.11680950969457626\n",
      "Epoch 388, Loss: 0.264160692691803, Final Batch Loss: 0.14905737340450287\n",
      "Epoch 389, Loss: 0.2317507416009903, Final Batch Loss: 0.07556119561195374\n",
      "Epoch 390, Loss: 0.26557885855436325, Final Batch Loss: 0.11067026108503342\n",
      "Epoch 391, Loss: 0.2703370824456215, Final Batch Loss: 0.17989307641983032\n",
      "Epoch 392, Loss: 0.21451031416654587, Final Batch Loss: 0.1061461791396141\n",
      "Epoch 393, Loss: 0.2938525974750519, Final Batch Loss: 0.1721445918083191\n",
      "Epoch 394, Loss: 0.20902012288570404, Final Batch Loss: 0.11876428127288818\n",
      "Epoch 395, Loss: 0.22168604284524918, Final Batch Loss: 0.1126624196767807\n",
      "Epoch 396, Loss: 0.2664162218570709, Final Batch Loss: 0.137242391705513\n",
      "Epoch 397, Loss: 0.20920374244451523, Final Batch Loss: 0.10583359003067017\n",
      "Epoch 398, Loss: 0.22276119887828827, Final Batch Loss: 0.0997224748134613\n",
      "Epoch 399, Loss: 0.34767746925354004, Final Batch Loss: 0.1708267629146576\n",
      "Epoch 400, Loss: 0.2840429097414017, Final Batch Loss: 0.1438797414302826\n",
      "Epoch 401, Loss: 0.248511902987957, Final Batch Loss: 0.10549376159906387\n",
      "Epoch 402, Loss: 0.24228890240192413, Final Batch Loss: 0.10856673121452332\n",
      "Epoch 403, Loss: 0.19271346181631088, Final Batch Loss: 0.06620814651250839\n",
      "Epoch 404, Loss: 0.24328847229480743, Final Batch Loss: 0.1412508487701416\n",
      "Epoch 405, Loss: 0.23518069088459015, Final Batch Loss: 0.1181534081697464\n",
      "Epoch 406, Loss: 0.20990348607301712, Final Batch Loss: 0.11439601331949234\n",
      "Epoch 407, Loss: 0.21359319239854813, Final Batch Loss: 0.09988108277320862\n",
      "Epoch 408, Loss: 0.19890352338552475, Final Batch Loss: 0.10421373695135117\n",
      "Epoch 409, Loss: 0.2647918537259102, Final Batch Loss: 0.15882118046283722\n",
      "Epoch 410, Loss: 0.2419191300868988, Final Batch Loss: 0.11207406222820282\n",
      "Epoch 411, Loss: 0.22579295188188553, Final Batch Loss: 0.14531998336315155\n",
      "Epoch 412, Loss: 0.25446218252182007, Final Batch Loss: 0.15745580196380615\n",
      "Epoch 413, Loss: 0.16773412376642227, Final Batch Loss: 0.07996994256973267\n",
      "Epoch 414, Loss: 0.19479137659072876, Final Batch Loss: 0.08429974317550659\n",
      "Epoch 415, Loss: 0.21803008764982224, Final Batch Loss: 0.11827059835195541\n",
      "Epoch 416, Loss: 0.22095520049333572, Final Batch Loss: 0.08210449665784836\n",
      "Epoch 417, Loss: 0.20817917585372925, Final Batch Loss: 0.11753887683153152\n",
      "Epoch 418, Loss: 0.23723506182432175, Final Batch Loss: 0.13554003834724426\n",
      "Epoch 419, Loss: 0.2185959741473198, Final Batch Loss: 0.10492191463708878\n",
      "Epoch 420, Loss: 0.2050747349858284, Final Batch Loss: 0.11660260707139969\n",
      "Epoch 421, Loss: 0.18351095914840698, Final Batch Loss: 0.0967317745089531\n",
      "Epoch 422, Loss: 0.18540383875370026, Final Batch Loss: 0.09498235583305359\n",
      "Epoch 423, Loss: 0.23263727873563766, Final Batch Loss: 0.12864087522029877\n",
      "Epoch 424, Loss: 0.201617531478405, Final Batch Loss: 0.06954429298639297\n",
      "Epoch 425, Loss: 0.2157774195075035, Final Batch Loss: 0.12315124273300171\n",
      "Epoch 426, Loss: 0.19067783653736115, Final Batch Loss: 0.0799366682767868\n",
      "Epoch 427, Loss: 0.18516626209020615, Final Batch Loss: 0.12048591673374176\n",
      "Epoch 428, Loss: 0.25307149440050125, Final Batch Loss: 0.15341314673423767\n",
      "Epoch 429, Loss: 0.2512238770723343, Final Batch Loss: 0.1313464194536209\n",
      "Epoch 430, Loss: 0.17708740383386612, Final Batch Loss: 0.07470900565385818\n",
      "Epoch 431, Loss: 0.18283116817474365, Final Batch Loss: 0.08653485029935837\n",
      "Epoch 432, Loss: 0.1740122139453888, Final Batch Loss: 0.061979494988918304\n",
      "Epoch 433, Loss: 0.2351820170879364, Final Batch Loss: 0.11664225906133652\n",
      "Epoch 434, Loss: 0.19195837527513504, Final Batch Loss: 0.09568753093481064\n",
      "Epoch 435, Loss: 0.17177130281925201, Final Batch Loss: 0.058724962174892426\n",
      "Epoch 436, Loss: 0.22261002659797668, Final Batch Loss: 0.1261919140815735\n",
      "Epoch 437, Loss: 0.21080370992422104, Final Batch Loss: 0.07113563269376755\n",
      "Epoch 438, Loss: 0.27008238434791565, Final Batch Loss: 0.1366477608680725\n",
      "Epoch 439, Loss: 0.22961147874593735, Final Batch Loss: 0.1073848307132721\n",
      "Epoch 440, Loss: 0.2218027040362358, Final Batch Loss: 0.15289098024368286\n",
      "Epoch 441, Loss: 0.2160099297761917, Final Batch Loss: 0.13413149118423462\n",
      "Epoch 442, Loss: 0.20350155234336853, Final Batch Loss: 0.08559071272611618\n",
      "Epoch 443, Loss: 0.1901128813624382, Final Batch Loss: 0.10220666974782944\n",
      "Epoch 444, Loss: 0.1810350865125656, Final Batch Loss: 0.08179259300231934\n",
      "Epoch 445, Loss: 0.2164180800318718, Final Batch Loss: 0.11947428435087204\n",
      "Epoch 446, Loss: 0.19170498847961426, Final Batch Loss: 0.08573220670223236\n",
      "Epoch 447, Loss: 0.22557656466960907, Final Batch Loss: 0.12139788269996643\n",
      "Epoch 448, Loss: 0.20316992700099945, Final Batch Loss: 0.11322174221277237\n",
      "Epoch 449, Loss: 0.21652249991893768, Final Batch Loss: 0.13119153678417206\n",
      "Epoch 450, Loss: 0.24827151745557785, Final Batch Loss: 0.13854119181632996\n",
      "Epoch 451, Loss: 0.1961049661040306, Final Batch Loss: 0.09341788291931152\n",
      "Epoch 452, Loss: 0.16932997107505798, Final Batch Loss: 0.08015991002321243\n",
      "Epoch 453, Loss: 0.22315473854541779, Final Batch Loss: 0.07683148980140686\n",
      "Epoch 454, Loss: 0.23238113522529602, Final Batch Loss: 0.12277378141880035\n",
      "Epoch 455, Loss: 0.19466355443000793, Final Batch Loss: 0.10550657659769058\n",
      "Epoch 456, Loss: 0.19772478938102722, Final Batch Loss: 0.0999416932463646\n",
      "Epoch 457, Loss: 0.20704728364944458, Final Batch Loss: 0.10221006721258163\n",
      "Epoch 458, Loss: 0.18188176304101944, Final Batch Loss: 0.10101640224456787\n",
      "Epoch 459, Loss: 0.21036671847105026, Final Batch Loss: 0.11886755377054214\n",
      "Epoch 460, Loss: 0.22724097222089767, Final Batch Loss: 0.13339172303676605\n",
      "Epoch 461, Loss: 0.19193950295448303, Final Batch Loss: 0.08075625449419022\n",
      "Epoch 462, Loss: 0.18762798607349396, Final Batch Loss: 0.11449970304965973\n",
      "Epoch 463, Loss: 0.14576075971126556, Final Batch Loss: 0.0728859007358551\n",
      "Epoch 464, Loss: 0.212125264108181, Final Batch Loss: 0.13125629723072052\n",
      "Epoch 465, Loss: 0.18463221192359924, Final Batch Loss: 0.07909899204969406\n",
      "Epoch 466, Loss: 0.23752940446138382, Final Batch Loss: 0.1302766501903534\n",
      "Epoch 467, Loss: 0.20783981680870056, Final Batch Loss: 0.11064226925373077\n",
      "Epoch 468, Loss: 0.1922195851802826, Final Batch Loss: 0.10407770425081253\n",
      "Epoch 469, Loss: 0.19032374024391174, Final Batch Loss: 0.07482939213514328\n",
      "Epoch 470, Loss: 0.23506640642881393, Final Batch Loss: 0.11304601281881332\n",
      "Epoch 471, Loss: 0.2139594405889511, Final Batch Loss: 0.112808458507061\n",
      "Epoch 472, Loss: 0.2176981344819069, Final Batch Loss: 0.10689627379179001\n",
      "Epoch 473, Loss: 0.21254875510931015, Final Batch Loss: 0.11583337187767029\n",
      "Epoch 474, Loss: 0.2164241001009941, Final Batch Loss: 0.11956685036420822\n",
      "Epoch 475, Loss: 0.18815376609563828, Final Batch Loss: 0.09831863641738892\n",
      "Epoch 476, Loss: 0.2903332784771919, Final Batch Loss: 0.17770975828170776\n",
      "Epoch 477, Loss: 0.2138296291232109, Final Batch Loss: 0.10831983387470245\n",
      "Epoch 478, Loss: 0.19149664789438248, Final Batch Loss: 0.11496613919734955\n",
      "Epoch 479, Loss: 0.20117442309856415, Final Batch Loss: 0.08035905659198761\n",
      "Epoch 480, Loss: 0.2055635303258896, Final Batch Loss: 0.10154543817043304\n",
      "Epoch 481, Loss: 0.21040933579206467, Final Batch Loss: 0.1083393543958664\n",
      "Epoch 482, Loss: 0.18328719586133957, Final Batch Loss: 0.08255030959844589\n",
      "Epoch 483, Loss: 0.18504636734724045, Final Batch Loss: 0.08109615743160248\n",
      "Epoch 484, Loss: 0.22345536947250366, Final Batch Loss: 0.09396393597126007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485, Loss: 0.16518035903573036, Final Batch Loss: 0.06237613782286644\n",
      "Epoch 486, Loss: 0.1671856790781021, Final Batch Loss: 0.0929429903626442\n",
      "Epoch 487, Loss: 0.1836858168244362, Final Batch Loss: 0.125568687915802\n",
      "Epoch 488, Loss: 0.188290037214756, Final Batch Loss: 0.11821720749139786\n",
      "Epoch 489, Loss: 0.17927901446819305, Final Batch Loss: 0.08968974649906158\n",
      "Epoch 490, Loss: 0.16516728699207306, Final Batch Loss: 0.08043450117111206\n",
      "Epoch 491, Loss: 0.1352311260998249, Final Batch Loss: 0.07744066417217255\n",
      "Epoch 492, Loss: 0.1928943693637848, Final Batch Loss: 0.08520188182592392\n",
      "Epoch 493, Loss: 0.16799410432577133, Final Batch Loss: 0.07185228168964386\n",
      "Epoch 494, Loss: 0.15530460327863693, Final Batch Loss: 0.07344461977481842\n",
      "Epoch 495, Loss: 0.16371747478842735, Final Batch Loss: 0.05141126736998558\n",
      "Epoch 496, Loss: 0.19738838076591492, Final Batch Loss: 0.10065075010061264\n",
      "Epoch 497, Loss: 0.1837451159954071, Final Batch Loss: 0.0961671993136406\n",
      "Epoch 498, Loss: 0.1695573925971985, Final Batch Loss: 0.07886338233947754\n",
      "Epoch 499, Loss: 0.16925781965255737, Final Batch Loss: 0.09404777735471725\n",
      "Epoch 500, Loss: 0.15806316584348679, Final Batch Loss: 0.07909808307886124\n",
      "Epoch 501, Loss: 0.16316045075654984, Final Batch Loss: 0.08861462026834488\n",
      "Epoch 502, Loss: 0.20674771443009377, Final Batch Loss: 0.05546972528100014\n",
      "Epoch 503, Loss: 0.17235888540744781, Final Batch Loss: 0.06402111798524857\n",
      "Epoch 504, Loss: 0.16274119541049004, Final Batch Loss: 0.049950819462537766\n",
      "Epoch 505, Loss: 0.13663210719823837, Final Batch Loss: 0.06754469126462936\n",
      "Epoch 506, Loss: 0.20123131573200226, Final Batch Loss: 0.11163503676652908\n",
      "Epoch 507, Loss: 0.16792181879281998, Final Batch Loss: 0.0958869680762291\n",
      "Epoch 508, Loss: 0.15374662727117538, Final Batch Loss: 0.0730787143111229\n",
      "Epoch 509, Loss: 0.13519178703427315, Final Batch Loss: 0.04096895828843117\n",
      "Epoch 510, Loss: 0.13888371735811234, Final Batch Loss: 0.08185714483261108\n",
      "Epoch 511, Loss: 0.17701686173677444, Final Batch Loss: 0.0867089331150055\n",
      "Epoch 512, Loss: 0.162336565554142, Final Batch Loss: 0.08678436279296875\n",
      "Epoch 513, Loss: 0.19920220971107483, Final Batch Loss: 0.12142270803451538\n",
      "Epoch 514, Loss: 0.21641619503498077, Final Batch Loss: 0.13338147103786469\n",
      "Epoch 515, Loss: 0.1873801350593567, Final Batch Loss: 0.12410832941532135\n",
      "Epoch 516, Loss: 0.1671535149216652, Final Batch Loss: 0.07514184713363647\n",
      "Epoch 517, Loss: 0.1826968789100647, Final Batch Loss: 0.09899087995290756\n",
      "Epoch 518, Loss: 0.1806362383067608, Final Batch Loss: 0.1208280399441719\n",
      "Epoch 519, Loss: 0.14981013536453247, Final Batch Loss: 0.062266044318675995\n",
      "Epoch 520, Loss: 0.17118769884109497, Final Batch Loss: 0.08057112246751785\n",
      "Epoch 521, Loss: 0.1447964608669281, Final Batch Loss: 0.07127325981855392\n",
      "Epoch 522, Loss: 0.14908476918935776, Final Batch Loss: 0.08019212633371353\n",
      "Epoch 523, Loss: 0.15644504874944687, Final Batch Loss: 0.07226575165987015\n",
      "Epoch 524, Loss: 0.16308827698230743, Final Batch Loss: 0.07080736011266708\n",
      "Epoch 525, Loss: 0.14749178290367126, Final Batch Loss: 0.07562880963087082\n",
      "Epoch 526, Loss: 0.14176778495311737, Final Batch Loss: 0.08015179634094238\n",
      "Epoch 527, Loss: 0.13396180421113968, Final Batch Loss: 0.0713716670870781\n",
      "Epoch 528, Loss: 0.16992861032485962, Final Batch Loss: 0.07077326625585556\n",
      "Epoch 529, Loss: 0.177080437541008, Final Batch Loss: 0.07820652425289154\n",
      "Epoch 530, Loss: 0.22334901243448257, Final Batch Loss: 0.15188324451446533\n",
      "Epoch 531, Loss: 0.13813159242272377, Final Batch Loss: 0.05166953429579735\n",
      "Epoch 532, Loss: 0.13988017290830612, Final Batch Loss: 0.051372647285461426\n",
      "Epoch 533, Loss: 0.17089059948921204, Final Batch Loss: 0.10473690181970596\n",
      "Epoch 534, Loss: 0.20195329189300537, Final Batch Loss: 0.11918164789676666\n",
      "Epoch 535, Loss: 0.1680225431919098, Final Batch Loss: 0.09257572144269943\n",
      "Epoch 536, Loss: 0.18204007297754288, Final Batch Loss: 0.09912031143903732\n",
      "Epoch 537, Loss: 0.16395056247711182, Final Batch Loss: 0.07649937272071838\n",
      "Epoch 538, Loss: 0.09632734581828117, Final Batch Loss: 0.04427869990468025\n",
      "Epoch 539, Loss: 0.1464000567793846, Final Batch Loss: 0.056329332292079926\n",
      "Epoch 540, Loss: 0.2000867873430252, Final Batch Loss: 0.12927378714084625\n",
      "Epoch 541, Loss: 0.16386021301150322, Final Batch Loss: 0.05716581270098686\n",
      "Epoch 542, Loss: 0.1938353329896927, Final Batch Loss: 0.08660131692886353\n",
      "Epoch 543, Loss: 0.13924510031938553, Final Batch Loss: 0.06888783723115921\n",
      "Epoch 544, Loss: 0.16333184391260147, Final Batch Loss: 0.0661710724234581\n",
      "Epoch 545, Loss: 0.18848209828138351, Final Batch Loss: 0.08113916963338852\n",
      "Epoch 546, Loss: 0.17529170960187912, Final Batch Loss: 0.09713829308748245\n",
      "Epoch 547, Loss: 0.1847422868013382, Final Batch Loss: 0.07766521722078323\n",
      "Epoch 548, Loss: 0.12757772579789162, Final Batch Loss: 0.06745510548353195\n",
      "Epoch 549, Loss: 0.20413228124380112, Final Batch Loss: 0.10840892791748047\n",
      "Epoch 550, Loss: 0.12949897721409798, Final Batch Loss: 0.05052819475531578\n",
      "Epoch 551, Loss: 0.15350330248475075, Final Batch Loss: 0.059739429503679276\n",
      "Epoch 552, Loss: 0.14108680933713913, Final Batch Loss: 0.06356178969144821\n",
      "Epoch 553, Loss: 0.16321543604135513, Final Batch Loss: 0.10028116405010223\n",
      "Epoch 554, Loss: 0.1511804722249508, Final Batch Loss: 0.054747868329286575\n",
      "Epoch 555, Loss: 0.14970330893993378, Final Batch Loss: 0.05782105028629303\n",
      "Epoch 556, Loss: 0.17801540344953537, Final Batch Loss: 0.050782836973667145\n",
      "Epoch 557, Loss: 0.1677391305565834, Final Batch Loss: 0.1002005934715271\n",
      "Epoch 558, Loss: 0.13583899661898613, Final Batch Loss: 0.08795096725225449\n",
      "Epoch 559, Loss: 0.13241466134786606, Final Batch Loss: 0.06679647415876389\n",
      "Epoch 560, Loss: 0.12915322184562683, Final Batch Loss: 0.058944955468177795\n",
      "Epoch 561, Loss: 0.16144749894738197, Final Batch Loss: 0.10100463777780533\n",
      "Epoch 562, Loss: 0.16168811917304993, Final Batch Loss: 0.08290962129831314\n",
      "Epoch 563, Loss: 0.13089188188314438, Final Batch Loss: 0.0641864463686943\n",
      "Epoch 564, Loss: 0.14825551211833954, Final Batch Loss: 0.08470865339040756\n",
      "Epoch 565, Loss: 0.1462208405137062, Final Batch Loss: 0.054951488971710205\n",
      "Epoch 566, Loss: 0.1405603103339672, Final Batch Loss: 0.08121646195650101\n",
      "Epoch 567, Loss: 0.1708519235253334, Final Batch Loss: 0.0927942618727684\n",
      "Epoch 568, Loss: 0.10327371209859848, Final Batch Loss: 0.05309826508164406\n",
      "Epoch 569, Loss: 0.12840864062309265, Final Batch Loss: 0.05379868298768997\n",
      "Epoch 570, Loss: 0.14660778641700745, Final Batch Loss: 0.07945924997329712\n",
      "Epoch 571, Loss: 0.13321258127689362, Final Batch Loss: 0.09027320146560669\n",
      "Epoch 572, Loss: 0.13614287227392197, Final Batch Loss: 0.07587135583162308\n",
      "Epoch 573, Loss: 0.149862639605999, Final Batch Loss: 0.09173474460840225\n",
      "Epoch 574, Loss: 0.12015349417924881, Final Batch Loss: 0.05755586922168732\n",
      "Epoch 575, Loss: 0.13588467612862587, Final Batch Loss: 0.08322259783744812\n",
      "Epoch 576, Loss: 0.15767302364110947, Final Batch Loss: 0.05444478988647461\n",
      "Epoch 577, Loss: 0.1773691177368164, Final Batch Loss: 0.08032400906085968\n",
      "Epoch 578, Loss: 0.10484715923666954, Final Batch Loss: 0.04870015010237694\n",
      "Epoch 579, Loss: 0.13705377280712128, Final Batch Loss: 0.0726672038435936\n",
      "Epoch 580, Loss: 0.14774607121944427, Final Batch Loss: 0.0720069631934166\n",
      "Epoch 581, Loss: 0.12112350761890411, Final Batch Loss: 0.06325691193342209\n",
      "Epoch 582, Loss: 0.1493442878127098, Final Batch Loss: 0.06600596755743027\n",
      "Epoch 583, Loss: 0.16294722631573677, Final Batch Loss: 0.10869782418012619\n",
      "Epoch 584, Loss: 0.15528909116983414, Final Batch Loss: 0.09200038015842438\n",
      "Epoch 585, Loss: 0.15279673039913177, Final Batch Loss: 0.07285032421350479\n",
      "Epoch 586, Loss: 0.15256081521511078, Final Batch Loss: 0.05279967188835144\n",
      "Epoch 587, Loss: 0.08926444873213768, Final Batch Loss: 0.04037419706583023\n",
      "Epoch 588, Loss: 0.11294686421751976, Final Batch Loss: 0.05922381952404976\n",
      "Epoch 589, Loss: 0.1443960815668106, Final Batch Loss: 0.07826735824346542\n",
      "Epoch 590, Loss: 0.13493642956018448, Final Batch Loss: 0.0830017551779747\n",
      "Epoch 591, Loss: 0.1359652616083622, Final Batch Loss: 0.09583248943090439\n",
      "Epoch 592, Loss: 0.17675135284662247, Final Batch Loss: 0.10421290248632431\n",
      "Epoch 593, Loss: 0.13534856587648392, Final Batch Loss: 0.07218512892723083\n",
      "Epoch 594, Loss: 0.1303669661283493, Final Batch Loss: 0.06406160444021225\n",
      "Epoch 595, Loss: 0.14254513382911682, Final Batch Loss: 0.08104581385850906\n",
      "Epoch 596, Loss: 0.1966169998049736, Final Batch Loss: 0.09492988139390945\n",
      "Epoch 597, Loss: 0.14578492939472198, Final Batch Loss: 0.06774931401014328\n",
      "Epoch 598, Loss: 0.1124018169939518, Final Batch Loss: 0.05762895941734314\n",
      "Epoch 599, Loss: 0.16519437357783318, Final Batch Loss: 0.1123686209321022\n",
      "Epoch 600, Loss: 0.12873823195695877, Final Batch Loss: 0.04911760985851288\n",
      "Epoch 601, Loss: 0.15293678641319275, Final Batch Loss: 0.09640375524759293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 602, Loss: 0.1301519088447094, Final Batch Loss: 0.0844714418053627\n",
      "Epoch 603, Loss: 0.1356750689446926, Final Batch Loss: 0.07715442031621933\n",
      "Epoch 604, Loss: 0.15296537429094315, Final Batch Loss: 0.07188499718904495\n",
      "Epoch 605, Loss: 0.14099107310175896, Final Batch Loss: 0.04956958815455437\n",
      "Epoch 606, Loss: 0.12435352802276611, Final Batch Loss: 0.061768755316734314\n",
      "Epoch 607, Loss: 0.09832977503538132, Final Batch Loss: 0.036284416913986206\n",
      "Epoch 608, Loss: 0.11818502843379974, Final Batch Loss: 0.06361633539199829\n",
      "Epoch 609, Loss: 0.16283360123634338, Final Batch Loss: 0.10202551633119583\n",
      "Epoch 610, Loss: 0.13391626626253128, Final Batch Loss: 0.07511066645383835\n",
      "Epoch 611, Loss: 0.12429802119731903, Final Batch Loss: 0.05885721743106842\n",
      "Epoch 612, Loss: 0.12253148294985294, Final Batch Loss: 0.09274107962846756\n",
      "Epoch 613, Loss: 0.13618948310613632, Final Batch Loss: 0.07814999669790268\n",
      "Epoch 614, Loss: 0.14204946160316467, Final Batch Loss: 0.07710310071706772\n",
      "Epoch 615, Loss: 0.1434754729270935, Final Batch Loss: 0.07892333716154099\n",
      "Epoch 616, Loss: 0.10481201857328415, Final Batch Loss: 0.03967174142599106\n",
      "Epoch 617, Loss: 0.11617491394281387, Final Batch Loss: 0.02866116166114807\n",
      "Epoch 618, Loss: 0.11162962019443512, Final Batch Loss: 0.038144707679748535\n",
      "Epoch 619, Loss: 0.13259174302220345, Final Batch Loss: 0.07168681174516678\n",
      "Epoch 620, Loss: 0.1231459379196167, Final Batch Loss: 0.06372750550508499\n",
      "Epoch 621, Loss: 0.11858638375997543, Final Batch Loss: 0.0399293377995491\n",
      "Epoch 622, Loss: 0.14237314090132713, Final Batch Loss: 0.08361472189426422\n",
      "Epoch 623, Loss: 0.12990958988666534, Final Batch Loss: 0.054637983441352844\n",
      "Epoch 624, Loss: 0.09554250165820122, Final Batch Loss: 0.044765032827854156\n",
      "Epoch 625, Loss: 0.1157366894185543, Final Batch Loss: 0.05966080352663994\n",
      "Epoch 626, Loss: 0.11780916154384613, Final Batch Loss: 0.05037473887205124\n",
      "Epoch 627, Loss: 0.11645419895648956, Final Batch Loss: 0.047883957624435425\n",
      "Epoch 628, Loss: 0.15460509434342384, Final Batch Loss: 0.10080723464488983\n",
      "Epoch 629, Loss: 0.11325627192854881, Final Batch Loss: 0.0403258390724659\n",
      "Epoch 630, Loss: 0.17548854649066925, Final Batch Loss: 0.07627834379673004\n",
      "Epoch 631, Loss: 0.12710250169038773, Final Batch Loss: 0.05408480018377304\n",
      "Epoch 632, Loss: 0.0956588163971901, Final Batch Loss: 0.01634734869003296\n",
      "Epoch 633, Loss: 0.1620083600282669, Final Batch Loss: 0.07490155100822449\n",
      "Epoch 634, Loss: 0.09431126341223717, Final Batch Loss: 0.0447518527507782\n",
      "Epoch 635, Loss: 0.10317113995552063, Final Batch Loss: 0.0494544543325901\n",
      "Epoch 636, Loss: 0.11944549530744553, Final Batch Loss: 0.054526835680007935\n",
      "Epoch 637, Loss: 0.10952616855502129, Final Batch Loss: 0.03859279677271843\n",
      "Epoch 638, Loss: 0.09499534964561462, Final Batch Loss: 0.06108488515019417\n",
      "Epoch 639, Loss: 0.0957309678196907, Final Batch Loss: 0.0373443178832531\n",
      "Epoch 640, Loss: 0.098936527967453, Final Batch Loss: 0.04341434687376022\n",
      "Epoch 641, Loss: 0.11471590399742126, Final Batch Loss: 0.06962428987026215\n",
      "Epoch 642, Loss: 0.08963614702224731, Final Batch Loss: 0.03167666122317314\n",
      "Epoch 643, Loss: 0.12724708020687103, Final Batch Loss: 0.04821495711803436\n",
      "Epoch 644, Loss: 0.13731352239847183, Final Batch Loss: 0.07609459012746811\n",
      "Epoch 645, Loss: 0.11816133186221123, Final Batch Loss: 0.06695745140314102\n",
      "Epoch 646, Loss: 0.11207715794444084, Final Batch Loss: 0.05108152702450752\n",
      "Epoch 647, Loss: 0.0974566824734211, Final Batch Loss: 0.04152090847492218\n",
      "Epoch 648, Loss: 0.13668560609221458, Final Batch Loss: 0.08917441964149475\n",
      "Epoch 649, Loss: 0.1354164481163025, Final Batch Loss: 0.10348795354366302\n",
      "Epoch 650, Loss: 0.1356014683842659, Final Batch Loss: 0.08883309364318848\n",
      "Epoch 651, Loss: 0.14354103058576584, Final Batch Loss: 0.08034170418977737\n",
      "Epoch 652, Loss: 0.15606722608208656, Final Batch Loss: 0.09562098979949951\n",
      "Epoch 653, Loss: 0.12386483699083328, Final Batch Loss: 0.048048898577690125\n",
      "Epoch 654, Loss: 0.12179843336343765, Final Batch Loss: 0.041772179305553436\n",
      "Epoch 655, Loss: 0.13266746699810028, Final Batch Loss: 0.0646200031042099\n",
      "Epoch 656, Loss: 0.12578396871685982, Final Batch Loss: 0.0322611965239048\n",
      "Epoch 657, Loss: 0.17840643227100372, Final Batch Loss: 0.10907026380300522\n",
      "Epoch 658, Loss: 0.12108155712485313, Final Batch Loss: 0.04951887205243111\n",
      "Epoch 659, Loss: 0.1011420376598835, Final Batch Loss: 0.05195259302854538\n",
      "Epoch 660, Loss: 0.11147383973002434, Final Batch Loss: 0.038161639124155045\n",
      "Epoch 661, Loss: 0.114041056483984, Final Batch Loss: 0.05340666323900223\n",
      "Epoch 662, Loss: 0.10507342964410782, Final Batch Loss: 0.06642698496580124\n",
      "Epoch 663, Loss: 0.07909292355179787, Final Batch Loss: 0.044338658452034\n",
      "Epoch 664, Loss: 0.11295143514871597, Final Batch Loss: 0.043227337300777435\n",
      "Epoch 665, Loss: 0.15505918860435486, Final Batch Loss: 0.0840245932340622\n",
      "Epoch 666, Loss: 0.10576898977160454, Final Batch Loss: 0.04799235984683037\n",
      "Epoch 667, Loss: 0.11848076805472374, Final Batch Loss: 0.057446256279945374\n",
      "Epoch 668, Loss: 0.1537681147456169, Final Batch Loss: 0.11210630089044571\n",
      "Epoch 669, Loss: 0.1309593878686428, Final Batch Loss: 0.048630353063344955\n",
      "Epoch 670, Loss: 0.07842306420207024, Final Batch Loss: 0.04022330418229103\n",
      "Epoch 671, Loss: 0.10370292142033577, Final Batch Loss: 0.056998711079359055\n",
      "Epoch 672, Loss: 0.12004100158810616, Final Batch Loss: 0.0533795990049839\n",
      "Epoch 673, Loss: 0.10917351394891739, Final Batch Loss: 0.07778947055339813\n",
      "Epoch 674, Loss: 0.12873850017786026, Final Batch Loss: 0.06516613811254501\n",
      "Epoch 675, Loss: 0.13443875312805176, Final Batch Loss: 0.0702321007847786\n",
      "Epoch 676, Loss: 0.08944470435380936, Final Batch Loss: 0.02866709604859352\n",
      "Epoch 677, Loss: 0.11084426566958427, Final Batch Loss: 0.0662955567240715\n",
      "Epoch 678, Loss: 0.13818753883242607, Final Batch Loss: 0.08411598205566406\n",
      "Epoch 679, Loss: 0.11161007359623909, Final Batch Loss: 0.05685395374894142\n",
      "Epoch 680, Loss: 0.10782699659466743, Final Batch Loss: 0.045353859663009644\n",
      "Epoch 681, Loss: 0.10547437518835068, Final Batch Loss: 0.06248283386230469\n",
      "Epoch 682, Loss: 0.12504935264587402, Final Batch Loss: 0.07071150839328766\n",
      "Epoch 683, Loss: 0.0777862686663866, Final Batch Loss: 0.04953647032380104\n",
      "Epoch 684, Loss: 0.13174663484096527, Final Batch Loss: 0.09027215093374252\n",
      "Epoch 685, Loss: 0.1078278012573719, Final Batch Loss: 0.0465623214840889\n",
      "Epoch 686, Loss: 0.14589466154575348, Final Batch Loss: 0.0731431171298027\n",
      "Epoch 687, Loss: 0.11584782600402832, Final Batch Loss: 0.06963449716567993\n",
      "Epoch 688, Loss: 0.15496202558279037, Final Batch Loss: 0.08607510477304459\n",
      "Epoch 689, Loss: 0.09378806315362453, Final Batch Loss: 0.07008133083581924\n",
      "Epoch 690, Loss: 0.11451620236039162, Final Batch Loss: 0.04825100675225258\n",
      "Epoch 691, Loss: 0.1243635043501854, Final Batch Loss: 0.0675513818860054\n",
      "Epoch 692, Loss: 0.12230376899242401, Final Batch Loss: 0.043684713542461395\n",
      "Epoch 693, Loss: 0.11311540007591248, Final Batch Loss: 0.04509611427783966\n",
      "Epoch 694, Loss: 0.07012883760035038, Final Batch Loss: 0.04371131956577301\n",
      "Epoch 695, Loss: 0.08548595011234283, Final Batch Loss: 0.04566938430070877\n",
      "Epoch 696, Loss: 0.10947629064321518, Final Batch Loss: 0.05662303790450096\n",
      "Epoch 697, Loss: 0.11926950141787529, Final Batch Loss: 0.06500132381916046\n",
      "Epoch 698, Loss: 0.16190870851278305, Final Batch Loss: 0.08865443617105484\n",
      "Epoch 699, Loss: 0.15082856267690659, Final Batch Loss: 0.0750771164894104\n",
      "Epoch 700, Loss: 0.143910214304924, Final Batch Loss: 0.0914195328950882\n",
      "Epoch 701, Loss: 0.13759443908929825, Final Batch Loss: 0.06965052336454391\n",
      "Epoch 702, Loss: 0.14081576094031334, Final Batch Loss: 0.04284915700554848\n",
      "Epoch 703, Loss: 0.11625199764966965, Final Batch Loss: 0.0700492262840271\n",
      "Epoch 704, Loss: 0.09745682403445244, Final Batch Loss: 0.040716879069805145\n",
      "Epoch 705, Loss: 0.1543259397149086, Final Batch Loss: 0.08694273233413696\n",
      "Epoch 706, Loss: 0.1088968999683857, Final Batch Loss: 0.04772365093231201\n",
      "Epoch 707, Loss: 0.13171713426709175, Final Batch Loss: 0.04407856985926628\n",
      "Epoch 708, Loss: 0.09826765395700932, Final Batch Loss: 0.030462035909295082\n",
      "Epoch 709, Loss: 0.14853877574205399, Final Batch Loss: 0.07095030695199966\n",
      "Epoch 710, Loss: 0.14197225123643875, Final Batch Loss: 0.06086675822734833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 711, Loss: 0.07328391261398792, Final Batch Loss: 0.026432698592543602\n",
      "Epoch 712, Loss: 0.09474064037203789, Final Batch Loss: 0.037214573472738266\n",
      "Epoch 713, Loss: 0.12611470371484756, Final Batch Loss: 0.03713855892419815\n",
      "Epoch 714, Loss: 0.0951501913368702, Final Batch Loss: 0.05357188358902931\n",
      "Epoch 715, Loss: 0.0959216058254242, Final Batch Loss: 0.02514582872390747\n",
      "Epoch 716, Loss: 0.1619560867547989, Final Batch Loss: 0.10631518810987473\n",
      "Epoch 717, Loss: 0.14821044728159904, Final Batch Loss: 0.08663132786750793\n",
      "Epoch 718, Loss: 0.12516216188669205, Final Batch Loss: 0.07493236660957336\n",
      "Epoch 719, Loss: 0.1902797669172287, Final Batch Loss: 0.10001685470342636\n",
      "Epoch 720, Loss: 0.23965001851320267, Final Batch Loss: 0.10570838302373886\n",
      "Epoch 721, Loss: 0.18660684674978256, Final Batch Loss: 0.10805089771747589\n",
      "Epoch 722, Loss: 0.1311873495578766, Final Batch Loss: 0.035063691437244415\n",
      "Epoch 723, Loss: 0.17349379509687424, Final Batch Loss: 0.07142016291618347\n",
      "Epoch 724, Loss: 0.11021452769637108, Final Batch Loss: 0.06339111924171448\n",
      "Epoch 725, Loss: 0.10852557420730591, Final Batch Loss: 0.042880550026893616\n",
      "Epoch 726, Loss: 0.11813405901193619, Final Batch Loss: 0.06389059871435165\n",
      "Epoch 727, Loss: 0.14541148021817207, Final Batch Loss: 0.040245939046144485\n",
      "Epoch 728, Loss: 0.14285292476415634, Final Batch Loss: 0.0712374821305275\n",
      "Epoch 729, Loss: 0.08557284250855446, Final Batch Loss: 0.05183419957756996\n",
      "Epoch 730, Loss: 0.09183866158127785, Final Batch Loss: 0.04739001393318176\n",
      "Epoch 731, Loss: 0.09479881823062897, Final Batch Loss: 0.037899479269981384\n",
      "Epoch 732, Loss: 0.17029304057359695, Final Batch Loss: 0.11310779303312302\n",
      "Epoch 733, Loss: 0.0795302465558052, Final Batch Loss: 0.04722544178366661\n",
      "Epoch 734, Loss: 0.15307710319757462, Final Batch Loss: 0.06666707992553711\n",
      "Epoch 735, Loss: 0.11439388990402222, Final Batch Loss: 0.0689975917339325\n",
      "Epoch 736, Loss: 0.13439497351646423, Final Batch Loss: 0.07153434306383133\n",
      "Epoch 737, Loss: 0.1579344943165779, Final Batch Loss: 0.08490408211946487\n",
      "Epoch 738, Loss: 0.10187562927603722, Final Batch Loss: 0.0651315376162529\n",
      "Epoch 739, Loss: 0.15957579016685486, Final Batch Loss: 0.07055862993001938\n",
      "Epoch 740, Loss: 0.09984783455729485, Final Batch Loss: 0.04523735120892525\n",
      "Epoch 741, Loss: 0.16691379249095917, Final Batch Loss: 0.06533215194940567\n",
      "Epoch 742, Loss: 0.14418556541204453, Final Batch Loss: 0.049489349126815796\n",
      "Epoch 743, Loss: 0.14427056908607483, Final Batch Loss: 0.09928995370864868\n",
      "Epoch 744, Loss: 0.09051384404301643, Final Batch Loss: 0.055842507630586624\n",
      "Epoch 745, Loss: 0.07964797504246235, Final Batch Loss: 0.028209278360009193\n",
      "Epoch 746, Loss: 0.10483961179852486, Final Batch Loss: 0.07281099259853363\n",
      "Epoch 747, Loss: 0.08175576850771904, Final Batch Loss: 0.0551091730594635\n",
      "Epoch 748, Loss: 0.07436476461589336, Final Batch Loss: 0.020975759252905846\n",
      "Epoch 749, Loss: 0.09308096766471863, Final Batch Loss: 0.05058411508798599\n",
      "Epoch 750, Loss: 0.09286831319332123, Final Batch Loss: 0.040869906544685364\n",
      "Epoch 751, Loss: 0.10261113569140434, Final Batch Loss: 0.05858774483203888\n",
      "Epoch 752, Loss: 0.12646784633398056, Final Batch Loss: 0.061960041522979736\n",
      "Epoch 753, Loss: 0.09828992560505867, Final Batch Loss: 0.04760296270251274\n",
      "Epoch 754, Loss: 0.09077954664826393, Final Batch Loss: 0.04977145791053772\n",
      "Epoch 755, Loss: 0.07167290151119232, Final Batch Loss: 0.03656218573451042\n",
      "Epoch 756, Loss: 0.10887522622942924, Final Batch Loss: 0.061611078679561615\n",
      "Epoch 757, Loss: 0.11144940182566643, Final Batch Loss: 0.043428000062704086\n",
      "Epoch 758, Loss: 0.051504114642739296, Final Batch Loss: 0.027095502242445946\n",
      "Epoch 759, Loss: 0.09738180041313171, Final Batch Loss: 0.06110144406557083\n",
      "Epoch 760, Loss: 0.12790405005216599, Final Batch Loss: 0.07601716369390488\n",
      "Epoch 761, Loss: 0.06119058281183243, Final Batch Loss: 0.024283871054649353\n",
      "Epoch 762, Loss: 0.06848155707120895, Final Batch Loss: 0.0339471735060215\n",
      "Epoch 763, Loss: 0.15807157382369041, Final Batch Loss: 0.047302115708589554\n",
      "Epoch 764, Loss: 0.11331001669168472, Final Batch Loss: 0.0745818018913269\n",
      "Epoch 765, Loss: 0.08953483402729034, Final Batch Loss: 0.05064951628446579\n",
      "Epoch 766, Loss: 0.121011171489954, Final Batch Loss: 0.10379275679588318\n",
      "Epoch 767, Loss: 0.07736089453101158, Final Batch Loss: 0.03941972553730011\n",
      "Epoch 768, Loss: 0.12761760875582695, Final Batch Loss: 0.05106157436966896\n",
      "Epoch 769, Loss: 0.13891849294304848, Final Batch Loss: 0.05547093227505684\n",
      "Epoch 770, Loss: 0.15332011505961418, Final Batch Loss: 0.057747457176446915\n",
      "Epoch 771, Loss: 0.11647211015224457, Final Batch Loss: 0.05713421478867531\n",
      "Epoch 772, Loss: 0.11384691670536995, Final Batch Loss: 0.05160825699567795\n",
      "Epoch 773, Loss: 0.13215036317706108, Final Batch Loss: 0.10431554168462753\n",
      "Epoch 774, Loss: 0.0739982146769762, Final Batch Loss: 0.04542992264032364\n",
      "Epoch 775, Loss: 0.13560203835368156, Final Batch Loss: 0.0879831612110138\n",
      "Epoch 776, Loss: 0.10721615329384804, Final Batch Loss: 0.05888108164072037\n",
      "Epoch 777, Loss: 0.1479676216840744, Final Batch Loss: 0.0952221155166626\n",
      "Epoch 778, Loss: 0.06977704726159573, Final Batch Loss: 0.029938263818621635\n",
      "Epoch 779, Loss: 0.14131802320480347, Final Batch Loss: 0.0885695368051529\n",
      "Epoch 780, Loss: 0.09248093515634537, Final Batch Loss: 0.044537726789712906\n",
      "Epoch 781, Loss: 0.15666555240750313, Final Batch Loss: 0.11300566047430038\n",
      "Epoch 782, Loss: 0.15948501974344254, Final Batch Loss: 0.06495095789432526\n",
      "Epoch 783, Loss: 0.15250195935368538, Final Batch Loss: 0.09970080107450485\n",
      "Epoch 784, Loss: 0.150246050208807, Final Batch Loss: 0.04643930867314339\n",
      "Epoch 785, Loss: 0.12002214789390564, Final Batch Loss: 0.06633102148771286\n",
      "Epoch 786, Loss: 0.12137703970074654, Final Batch Loss: 0.05542329326272011\n",
      "Epoch 787, Loss: 0.07502800785005093, Final Batch Loss: 0.02777809463441372\n",
      "Epoch 788, Loss: 0.10640183836221695, Final Batch Loss: 0.04876790568232536\n",
      "Epoch 789, Loss: 0.10173780098557472, Final Batch Loss: 0.05679606273770332\n",
      "Epoch 790, Loss: 0.08187559805810452, Final Batch Loss: 0.029513252899050713\n",
      "Epoch 791, Loss: 0.09373519010841846, Final Batch Loss: 0.01813465543091297\n",
      "Epoch 792, Loss: 0.07927003689110279, Final Batch Loss: 0.026521602645516396\n",
      "Epoch 793, Loss: 0.13216315023601055, Final Batch Loss: 0.027074670419096947\n",
      "Epoch 794, Loss: 0.08473077602684498, Final Batch Loss: 0.05662661790847778\n",
      "Epoch 795, Loss: 0.1458652764558792, Final Batch Loss: 0.06316445767879486\n",
      "Epoch 796, Loss: 0.06268724426627159, Final Batch Loss: 0.02760479599237442\n",
      "Epoch 797, Loss: 0.09148134663701057, Final Batch Loss: 0.04057534039020538\n",
      "Epoch 798, Loss: 0.09684149362146854, Final Batch Loss: 0.024181963875889778\n",
      "Epoch 799, Loss: 0.17474986612796783, Final Batch Loss: 0.09457064419984818\n",
      "Epoch 800, Loss: 0.08439305052161217, Final Batch Loss: 0.028690654784440994\n",
      "Epoch 801, Loss: 0.10246201232075691, Final Batch Loss: 0.03794017806649208\n",
      "Epoch 802, Loss: 0.09283578023314476, Final Batch Loss: 0.0347292497754097\n",
      "Epoch 803, Loss: 0.10283609479665756, Final Batch Loss: 0.05328061804175377\n",
      "Epoch 804, Loss: 0.09036009758710861, Final Batch Loss: 0.034523461014032364\n",
      "Epoch 805, Loss: 0.05315163731575012, Final Batch Loss: 0.02688695676624775\n",
      "Epoch 806, Loss: 0.10679977759718895, Final Batch Loss: 0.07169153541326523\n",
      "Epoch 807, Loss: 0.10568538680672646, Final Batch Loss: 0.06407789140939713\n",
      "Epoch 808, Loss: 0.07364755868911743, Final Batch Loss: 0.037054482847452164\n",
      "Epoch 809, Loss: 0.06802786886692047, Final Batch Loss: 0.027674414217472076\n",
      "Epoch 810, Loss: 0.1044466644525528, Final Batch Loss: 0.05334213376045227\n",
      "Epoch 811, Loss: 0.09142164140939713, Final Batch Loss: 0.022808894515037537\n",
      "Epoch 812, Loss: 0.12468980625271797, Final Batch Loss: 0.07547862082719803\n",
      "Epoch 813, Loss: 0.1123817190527916, Final Batch Loss: 0.04170450568199158\n",
      "Epoch 814, Loss: 0.08861951902508736, Final Batch Loss: 0.03886711597442627\n",
      "Epoch 815, Loss: 0.09169250726699829, Final Batch Loss: 0.036875877529382706\n",
      "Epoch 816, Loss: 0.10293081402778625, Final Batch Loss: 0.06788232177495956\n",
      "Epoch 817, Loss: 0.1371171772480011, Final Batch Loss: 0.05302123725414276\n",
      "Epoch 818, Loss: 0.07568864151835442, Final Batch Loss: 0.026122529059648514\n",
      "Epoch 819, Loss: 0.08298306539654732, Final Batch Loss: 0.03263723850250244\n",
      "Epoch 820, Loss: 0.125069759786129, Final Batch Loss: 0.06808042526245117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 821, Loss: 0.18093132972717285, Final Batch Loss: 0.13456305861473083\n",
      "Epoch 822, Loss: 0.10433811694383621, Final Batch Loss: 0.05489395186305046\n",
      "Epoch 823, Loss: 0.08416111394762993, Final Batch Loss: 0.04276690259575844\n",
      "Epoch 824, Loss: 0.08207908272743225, Final Batch Loss: 0.03792336955666542\n",
      "Epoch 825, Loss: 0.13238004967570305, Final Batch Loss: 0.09400708973407745\n",
      "Epoch 826, Loss: 0.08164878003299236, Final Batch Loss: 0.024505259469151497\n",
      "Epoch 827, Loss: 0.11798657849431038, Final Batch Loss: 0.06786144524812698\n",
      "Epoch 828, Loss: 0.11057789623737335, Final Batch Loss: 0.0391395166516304\n",
      "Epoch 829, Loss: 0.09877397492527962, Final Batch Loss: 0.06362426280975342\n",
      "Epoch 830, Loss: 0.08398149162530899, Final Batch Loss: 0.027042731642723083\n",
      "Epoch 831, Loss: 0.06391477212309837, Final Batch Loss: 0.026684574782848358\n",
      "Epoch 832, Loss: 0.101656224578619, Final Batch Loss: 0.03812430426478386\n",
      "Epoch 833, Loss: 0.0783064067363739, Final Batch Loss: 0.043545544147491455\n",
      "Epoch 834, Loss: 0.04956045746803284, Final Batch Loss: 0.027526918798685074\n",
      "Epoch 835, Loss: 0.07107505947351456, Final Batch Loss: 0.03760222718119621\n",
      "Epoch 836, Loss: 0.08145833387970924, Final Batch Loss: 0.04932357743382454\n",
      "Epoch 837, Loss: 0.07804499566555023, Final Batch Loss: 0.0497591458261013\n",
      "Epoch 838, Loss: 0.09372248128056526, Final Batch Loss: 0.05805189535021782\n",
      "Epoch 839, Loss: 0.07465813308954239, Final Batch Loss: 0.03316822648048401\n",
      "Epoch 840, Loss: 0.06189733184874058, Final Batch Loss: 0.029671816155314445\n",
      "Epoch 841, Loss: 0.09952801838517189, Final Batch Loss: 0.058810412883758545\n",
      "Epoch 842, Loss: 0.08330314978957176, Final Batch Loss: 0.03614259511232376\n",
      "Epoch 843, Loss: 0.07969602197408676, Final Batch Loss: 0.03739328682422638\n",
      "Epoch 844, Loss: 0.06390763446688652, Final Batch Loss: 0.025931600481271744\n",
      "Epoch 845, Loss: 0.07262920215725899, Final Batch Loss: 0.04877762496471405\n",
      "Epoch 846, Loss: 0.11012411490082741, Final Batch Loss: 0.08425625413656235\n",
      "Epoch 847, Loss: 0.06912977620959282, Final Batch Loss: 0.048385851085186005\n",
      "Epoch 848, Loss: 0.14838139712810516, Final Batch Loss: 0.08626527339220047\n",
      "Epoch 849, Loss: 0.07953115366399288, Final Batch Loss: 0.018442077562212944\n",
      "Epoch 850, Loss: 0.11443956382572651, Final Batch Loss: 0.08798814564943314\n",
      "Epoch 851, Loss: 0.09045993536710739, Final Batch Loss: 0.049665726721286774\n",
      "Epoch 852, Loss: 0.07982433959841728, Final Batch Loss: 0.05107581615447998\n",
      "Epoch 853, Loss: 0.07302701845765114, Final Batch Loss: 0.03810614347457886\n",
      "Epoch 854, Loss: 0.06404196843504906, Final Batch Loss: 0.024746406823396683\n",
      "Epoch 855, Loss: 0.16849753633141518, Final Batch Loss: 0.11423013359308243\n",
      "Epoch 856, Loss: 0.08981238305568695, Final Batch Loss: 0.042870406061410904\n",
      "Epoch 857, Loss: 0.21181799471378326, Final Batch Loss: 0.1671372652053833\n",
      "Epoch 858, Loss: 0.10606763511896133, Final Batch Loss: 0.06600822508335114\n",
      "Epoch 859, Loss: 0.319484680891037, Final Batch Loss: 0.05124396085739136\n",
      "Epoch 860, Loss: 0.10411037504673004, Final Batch Loss: 0.06342148780822754\n",
      "Epoch 861, Loss: 0.17579477280378342, Final Batch Loss: 0.13845257461071014\n",
      "Epoch 862, Loss: 0.05807394161820412, Final Batch Loss: 0.032742004841566086\n",
      "Epoch 863, Loss: 0.2883712351322174, Final Batch Loss: 0.09628719091415405\n",
      "Epoch 864, Loss: 0.10380180180072784, Final Batch Loss: 0.02654968947172165\n",
      "Epoch 865, Loss: 0.12084581330418587, Final Batch Loss: 0.0473441518843174\n",
      "Epoch 866, Loss: 0.20668482780456543, Final Batch Loss: 0.11010269075632095\n",
      "Epoch 867, Loss: 0.135582085698843, Final Batch Loss: 0.07750329375267029\n",
      "Epoch 868, Loss: 0.1505763903260231, Final Batch Loss: 0.0746786817908287\n",
      "Epoch 869, Loss: 0.10627692937850952, Final Batch Loss: 0.0395682156085968\n",
      "Epoch 870, Loss: 0.1014951840043068, Final Batch Loss: 0.04858972877264023\n",
      "Epoch 871, Loss: 0.10395235195755959, Final Batch Loss: 0.04465026408433914\n",
      "Epoch 872, Loss: 0.12018031999468803, Final Batch Loss: 0.057802923023700714\n",
      "Epoch 873, Loss: 0.1400686651468277, Final Batch Loss: 0.07443802058696747\n",
      "Epoch 874, Loss: 0.09476260840892792, Final Batch Loss: 0.05581843852996826\n",
      "Epoch 875, Loss: 0.10211325064301491, Final Batch Loss: 0.040724288672208786\n",
      "Epoch 876, Loss: 0.08025782182812691, Final Batch Loss: 0.03748803213238716\n",
      "Epoch 877, Loss: 0.10057886689901352, Final Batch Loss: 0.040897540748119354\n",
      "Epoch 878, Loss: 0.1272534653544426, Final Batch Loss: 0.06414306908845901\n",
      "Epoch 879, Loss: 0.0485762283205986, Final Batch Loss: 0.01844308152794838\n",
      "Epoch 880, Loss: 0.0862232856452465, Final Batch Loss: 0.038814179599285126\n",
      "Epoch 881, Loss: 0.08030573651194572, Final Batch Loss: 0.027257677167654037\n",
      "Epoch 882, Loss: 0.10425482876598835, Final Batch Loss: 0.07541153579950333\n",
      "Epoch 883, Loss: 0.07687293365597725, Final Batch Loss: 0.019067008048295975\n",
      "Epoch 884, Loss: 0.08392573706805706, Final Batch Loss: 0.028230464085936546\n",
      "Epoch 885, Loss: 0.10609356686472893, Final Batch Loss: 0.04189588502049446\n",
      "Epoch 886, Loss: 0.06523224711418152, Final Batch Loss: 0.03139502555131912\n",
      "Epoch 887, Loss: 0.07207694929093122, Final Batch Loss: 0.059558019042015076\n",
      "Epoch 888, Loss: 0.08390768617391586, Final Batch Loss: 0.04036538675427437\n",
      "Epoch 889, Loss: 0.09128035604953766, Final Batch Loss: 0.053573112934827805\n",
      "Epoch 890, Loss: 0.06625705398619175, Final Batch Loss: 0.02602403797209263\n",
      "Epoch 891, Loss: 0.06662854924798012, Final Batch Loss: 0.03141816332936287\n",
      "Epoch 892, Loss: 0.10818077251315117, Final Batch Loss: 0.0469241701066494\n",
      "Epoch 893, Loss: 0.06176765076816082, Final Batch Loss: 0.03635613992810249\n",
      "Epoch 894, Loss: 0.09844173491001129, Final Batch Loss: 0.05543077364563942\n",
      "Epoch 895, Loss: 0.08888332173228264, Final Batch Loss: 0.04566183313727379\n",
      "Epoch 896, Loss: 0.10151109844446182, Final Batch Loss: 0.057269226759672165\n",
      "Epoch 897, Loss: 0.08633698895573616, Final Batch Loss: 0.04178003966808319\n",
      "Epoch 898, Loss: 0.08026529848575592, Final Batch Loss: 0.02313898876309395\n",
      "Epoch 899, Loss: 0.05553246848285198, Final Batch Loss: 0.020302722230553627\n",
      "Epoch 900, Loss: 0.09711917117238045, Final Batch Loss: 0.057132720947265625\n",
      "Epoch 901, Loss: 0.12429254502058029, Final Batch Loss: 0.03146545588970184\n",
      "Epoch 902, Loss: 0.060219546779990196, Final Batch Loss: 0.023986632004380226\n",
      "Epoch 903, Loss: 0.07393249869346619, Final Batch Loss: 0.01868613436818123\n",
      "Epoch 904, Loss: 0.13452816009521484, Final Batch Loss: 0.08301055431365967\n",
      "Epoch 905, Loss: 0.06775102391839027, Final Batch Loss: 0.021835118532180786\n",
      "Epoch 906, Loss: 0.04502307064831257, Final Batch Loss: 0.021661454811692238\n",
      "Epoch 907, Loss: 0.051152678206562996, Final Batch Loss: 0.021415352821350098\n",
      "Epoch 908, Loss: 0.10563210025429726, Final Batch Loss: 0.06056511402130127\n",
      "Epoch 909, Loss: 0.10690541937947273, Final Batch Loss: 0.07512778043746948\n",
      "Epoch 910, Loss: 0.07663306221365929, Final Batch Loss: 0.03752676770091057\n",
      "Epoch 911, Loss: 0.06754602305591106, Final Batch Loss: 0.04686826094985008\n",
      "Epoch 912, Loss: 0.12244608625769615, Final Batch Loss: 0.07585803419351578\n",
      "Epoch 913, Loss: 0.08985776081681252, Final Batch Loss: 0.05505222827196121\n",
      "Epoch 914, Loss: 0.07962169125676155, Final Batch Loss: 0.015265557914972305\n",
      "Epoch 915, Loss: 0.11816906183958054, Final Batch Loss: 0.05371643602848053\n",
      "Epoch 916, Loss: 0.07281984388828278, Final Batch Loss: 0.03558129444718361\n",
      "Epoch 917, Loss: 0.056521352380514145, Final Batch Loss: 0.034357354044914246\n",
      "Epoch 918, Loss: 0.057255351915955544, Final Batch Loss: 0.028891880065202713\n",
      "Epoch 919, Loss: 0.0707150511443615, Final Batch Loss: 0.03889631852507591\n",
      "Epoch 920, Loss: 0.08190476521849632, Final Batch Loss: 0.06108000874519348\n",
      "Epoch 921, Loss: 0.062465159222483635, Final Batch Loss: 0.025755776092410088\n",
      "Epoch 922, Loss: 0.03648583684116602, Final Batch Loss: 0.011699440889060497\n",
      "Epoch 923, Loss: 0.07966004684567451, Final Batch Loss: 0.049659356474876404\n",
      "Epoch 924, Loss: 0.058709800243377686, Final Batch Loss: 0.03222031891345978\n",
      "Epoch 925, Loss: 0.055703556165099144, Final Batch Loss: 0.039786890149116516\n",
      "Epoch 926, Loss: 0.10628429800271988, Final Batch Loss: 0.06714823096990585\n",
      "Epoch 927, Loss: 0.09460880979895592, Final Batch Loss: 0.04890253394842148\n",
      "Epoch 928, Loss: 0.10258555971086025, Final Batch Loss: 0.07343801856040955\n",
      "Epoch 929, Loss: 0.07565528713166714, Final Batch Loss: 0.05143001675605774\n",
      "Epoch 930, Loss: 0.08812355622649193, Final Batch Loss: 0.03625410422682762\n",
      "Epoch 931, Loss: 0.14009623229503632, Final Batch Loss: 0.08839108794927597\n",
      "Epoch 932, Loss: 0.04926951788365841, Final Batch Loss: 0.02134709246456623\n",
      "Epoch 933, Loss: 0.07207401469349861, Final Batch Loss: 0.04941132664680481\n",
      "Epoch 934, Loss: 0.11982311680912971, Final Batch Loss: 0.06484238803386688\n",
      "Epoch 935, Loss: 0.07673048600554466, Final Batch Loss: 0.05447118729352951\n",
      "Epoch 936, Loss: 0.12849825993180275, Final Batch Loss: 0.05706724897027016\n",
      "Epoch 937, Loss: 0.1036459244787693, Final Batch Loss: 0.04629850760102272\n",
      "Epoch 938, Loss: 0.12348762899637222, Final Batch Loss: 0.06208523362874985\n",
      "Epoch 939, Loss: 0.04741260036826134, Final Batch Loss: 0.02143046446144581\n",
      "Epoch 940, Loss: 0.07764814794063568, Final Batch Loss: 0.04255412146449089\n",
      "Epoch 941, Loss: 0.0739190187305212, Final Batch Loss: 0.025989854708313942\n",
      "Epoch 942, Loss: 0.08712411858141422, Final Batch Loss: 0.022457199171185493\n",
      "Epoch 943, Loss: 0.07245619222521782, Final Batch Loss: 0.039828069508075714\n",
      "Epoch 944, Loss: 0.06789719685912132, Final Batch Loss: 0.0500548779964447\n",
      "Epoch 945, Loss: 0.06828417256474495, Final Batch Loss: 0.03236968070268631\n",
      "Epoch 946, Loss: 0.06062335800379515, Final Batch Loss: 0.01478214468806982\n",
      "Epoch 947, Loss: 0.07877499610185623, Final Batch Loss: 0.03495899587869644\n",
      "Epoch 948, Loss: 0.07314295042306185, Final Batch Loss: 0.013641581870615482\n",
      "Epoch 949, Loss: 0.07056223042309284, Final Batch Loss: 0.025011660531163216\n",
      "Epoch 950, Loss: 0.07943308539688587, Final Batch Loss: 0.029690323397517204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951, Loss: 0.07349523156881332, Final Batch Loss: 0.03318411484360695\n",
      "Epoch 952, Loss: 0.05434233881533146, Final Batch Loss: 0.01149226538836956\n",
      "Epoch 953, Loss: 0.05865021049976349, Final Batch Loss: 0.02444840967655182\n",
      "Epoch 954, Loss: 0.07392188906669617, Final Batch Loss: 0.04882863536477089\n",
      "Epoch 955, Loss: 0.04197705537080765, Final Batch Loss: 0.022008905187249184\n",
      "Epoch 956, Loss: 0.06602325662970543, Final Batch Loss: 0.02756728231906891\n",
      "Epoch 957, Loss: 0.08002640306949615, Final Batch Loss: 0.04377347230911255\n",
      "Epoch 958, Loss: 0.06563158147037029, Final Batch Loss: 0.016615891829133034\n",
      "Epoch 959, Loss: 0.08897875621914864, Final Batch Loss: 0.03639576584100723\n",
      "Epoch 960, Loss: 0.04976171813905239, Final Batch Loss: 0.02508290484547615\n",
      "Epoch 961, Loss: 0.04827874153852463, Final Batch Loss: 0.031412847340106964\n",
      "Epoch 962, Loss: 0.11069158837199211, Final Batch Loss: 0.05549674853682518\n",
      "Epoch 963, Loss: 0.08948609605431557, Final Batch Loss: 0.03748146817088127\n",
      "Epoch 964, Loss: 0.05317309778183699, Final Batch Loss: 0.038818635046482086\n",
      "Epoch 965, Loss: 0.08294174633920193, Final Batch Loss: 0.025644032284617424\n",
      "Epoch 966, Loss: 0.06972112506628036, Final Batch Loss: 0.04197923094034195\n",
      "Epoch 967, Loss: 0.06838909722864628, Final Batch Loss: 0.03821477293968201\n",
      "Epoch 968, Loss: 0.06773537397384644, Final Batch Loss: 0.03514088690280914\n",
      "Epoch 969, Loss: 0.051072241738438606, Final Batch Loss: 0.026484133675694466\n",
      "Epoch 970, Loss: 0.03838722221553326, Final Batch Loss: 0.0223456472158432\n",
      "Epoch 971, Loss: 0.06583657674491405, Final Batch Loss: 0.03922475501894951\n",
      "Epoch 972, Loss: 0.04549014288932085, Final Batch Loss: 0.01099396962672472\n",
      "Epoch 973, Loss: 0.09436495788395405, Final Batch Loss: 0.07473739981651306\n",
      "Epoch 974, Loss: 0.0735819935798645, Final Batch Loss: 0.033314041793346405\n",
      "Epoch 975, Loss: 0.05036164075136185, Final Batch Loss: 0.02451195754110813\n",
      "Epoch 976, Loss: 0.0500648058950901, Final Batch Loss: 0.03042633831501007\n",
      "Epoch 977, Loss: 0.07336107641458511, Final Batch Loss: 0.05730763450264931\n",
      "Epoch 978, Loss: 0.05746627040207386, Final Batch Loss: 0.027742397040128708\n",
      "Epoch 979, Loss: 0.0738922469317913, Final Batch Loss: 0.040461037307977676\n",
      "Epoch 980, Loss: 0.06942031159996986, Final Batch Loss: 0.04880882054567337\n",
      "Epoch 981, Loss: 0.055632637813687325, Final Batch Loss: 0.03458837419748306\n",
      "Epoch 982, Loss: 0.06811061501502991, Final Batch Loss: 0.04886048659682274\n",
      "Epoch 983, Loss: 0.07392401061952114, Final Batch Loss: 0.05139503255486488\n",
      "Epoch 984, Loss: 0.08710077032446861, Final Batch Loss: 0.04709530621767044\n",
      "Epoch 985, Loss: 0.08363774418830872, Final Batch Loss: 0.043843455612659454\n",
      "Epoch 986, Loss: 0.06788429245352745, Final Batch Loss: 0.033883124589920044\n",
      "Epoch 987, Loss: 0.06681486032903194, Final Batch Loss: 0.04516880586743355\n",
      "Epoch 988, Loss: 0.08860569074749947, Final Batch Loss: 0.040907613933086395\n",
      "Epoch 989, Loss: 0.045151159167289734, Final Batch Loss: 0.02362125553190708\n",
      "Epoch 990, Loss: 0.06246631406247616, Final Batch Loss: 0.03776177391409874\n",
      "Epoch 991, Loss: 0.07461708411574364, Final Batch Loss: 0.05817265063524246\n",
      "Epoch 992, Loss: 0.09807050228118896, Final Batch Loss: 0.06951092928647995\n",
      "Epoch 993, Loss: 0.07341201789677143, Final Batch Loss: 0.04352930188179016\n",
      "Epoch 994, Loss: 0.05885042157024145, Final Batch Loss: 0.010735410265624523\n",
      "Epoch 995, Loss: 0.12992702051997185, Final Batch Loss: 0.07896144688129425\n",
      "Epoch 996, Loss: 0.0558881675824523, Final Batch Loss: 0.04036799818277359\n",
      "Epoch 997, Loss: 0.09930593147873878, Final Batch Loss: 0.04228203371167183\n",
      "Epoch 998, Loss: 0.09552706591784954, Final Batch Loss: 0.07483033090829849\n",
      "Epoch 999, Loss: 0.0760808177292347, Final Batch Loss: 0.04130471870303154\n",
      "Epoch 1000, Loss: 0.09637629985809326, Final Batch Loss: 0.0669666975736618\n",
      "Epoch 1001, Loss: 0.0595631655305624, Final Batch Loss: 0.029445897787809372\n",
      "Epoch 1002, Loss: 0.11247150972485542, Final Batch Loss: 0.06899233907461166\n",
      "Epoch 1003, Loss: 0.08967837318778038, Final Batch Loss: 0.038281477987766266\n",
      "Epoch 1004, Loss: 0.06482037529349327, Final Batch Loss: 0.0300900898873806\n",
      "Epoch 1005, Loss: 0.05933051556348801, Final Batch Loss: 0.0333264134824276\n",
      "Epoch 1006, Loss: 0.04846925474703312, Final Batch Loss: 0.019997134804725647\n",
      "Epoch 1007, Loss: 0.07284624129533768, Final Batch Loss: 0.03382880613207817\n",
      "Epoch 1008, Loss: 0.0670856423676014, Final Batch Loss: 0.03905213996767998\n",
      "Epoch 1009, Loss: 0.10710175707936287, Final Batch Loss: 0.07962971925735474\n",
      "Epoch 1010, Loss: 0.0679994747042656, Final Batch Loss: 0.03737042099237442\n",
      "Epoch 1011, Loss: 0.12199871614575386, Final Batch Loss: 0.0622251071035862\n",
      "Epoch 1012, Loss: 0.04531222768127918, Final Batch Loss: 0.02029610052704811\n",
      "Epoch 1013, Loss: 0.11315889656543732, Final Batch Loss: 0.018235795199871063\n",
      "Epoch 1014, Loss: 0.06197233498096466, Final Batch Loss: 0.02342398837208748\n",
      "Epoch 1015, Loss: 0.07661860436201096, Final Batch Loss: 0.03190630301833153\n",
      "Epoch 1016, Loss: 0.05192550644278526, Final Batch Loss: 0.015648365020751953\n",
      "Epoch 1017, Loss: 0.05916091613471508, Final Batch Loss: 0.015626704320311546\n",
      "Epoch 1018, Loss: 0.08078425098210573, Final Batch Loss: 0.009396401233971119\n",
      "Epoch 1019, Loss: 0.06754652410745621, Final Batch Loss: 0.02604670077562332\n",
      "Epoch 1020, Loss: 0.13988080620765686, Final Batch Loss: 0.07322897017002106\n",
      "Epoch 1021, Loss: 0.033988408744335175, Final Batch Loss: 0.01421058364212513\n",
      "Epoch 1022, Loss: 0.12106993794441223, Final Batch Loss: 0.035470232367515564\n",
      "Epoch 1023, Loss: 0.07344217225909233, Final Batch Loss: 0.04756265506148338\n",
      "Epoch 1024, Loss: 0.07771443389356136, Final Batch Loss: 0.051954902708530426\n",
      "Epoch 1025, Loss: 0.04436468333005905, Final Batch Loss: 0.016727665439248085\n",
      "Epoch 1026, Loss: 0.06550188735127449, Final Batch Loss: 0.0298960842192173\n",
      "Epoch 1027, Loss: 0.07779747620224953, Final Batch Loss: 0.04114539921283722\n",
      "Epoch 1028, Loss: 0.03779640980064869, Final Batch Loss: 0.016242006793618202\n",
      "Epoch 1029, Loss: 0.06524598225951195, Final Batch Loss: 0.036759644746780396\n",
      "Epoch 1030, Loss: 0.05002208426594734, Final Batch Loss: 0.02979460172355175\n",
      "Epoch 1031, Loss: 0.08705239370465279, Final Batch Loss: 0.06445643305778503\n",
      "Epoch 1032, Loss: 0.05655192770063877, Final Batch Loss: 0.029136138036847115\n",
      "Epoch 1033, Loss: 0.04955955222249031, Final Batch Loss: 0.017268139868974686\n",
      "Epoch 1034, Loss: 0.11149177327752113, Final Batch Loss: 0.064937062561512\n",
      "Epoch 1035, Loss: 0.06698609329760075, Final Batch Loss: 0.03977666422724724\n",
      "Epoch 1036, Loss: 0.05159595422446728, Final Batch Loss: 0.016083577647805214\n",
      "Epoch 1037, Loss: 0.1141374483704567, Final Batch Loss: 0.05650448054075241\n",
      "Epoch 1038, Loss: 0.04528403282165527, Final Batch Loss: 0.017054768279194832\n",
      "Epoch 1039, Loss: 0.05208044871687889, Final Batch Loss: 0.033283401280641556\n",
      "Epoch 1040, Loss: 0.049106509424746037, Final Batch Loss: 0.015145027078688145\n",
      "Epoch 1041, Loss: 0.04889644868671894, Final Batch Loss: 0.02867872826755047\n",
      "Epoch 1042, Loss: 0.06189727131277323, Final Batch Loss: 0.04975486546754837\n",
      "Epoch 1043, Loss: 0.02766779623925686, Final Batch Loss: 0.012240762822329998\n",
      "Epoch 1044, Loss: 0.08087506890296936, Final Batch Loss: 0.038119420409202576\n",
      "Epoch 1045, Loss: 0.06425534188747406, Final Batch Loss: 0.04494049772620201\n",
      "Epoch 1046, Loss: 0.060586895793676376, Final Batch Loss: 0.03102649562060833\n",
      "Epoch 1047, Loss: 0.0650839526206255, Final Batch Loss: 0.04357827082276344\n",
      "Epoch 1048, Loss: 0.0936734788119793, Final Batch Loss: 0.05044937878847122\n",
      "Epoch 1049, Loss: 0.0525885010138154, Final Batch Loss: 0.008363489992916584\n",
      "Epoch 1050, Loss: 0.13489223830401897, Final Batch Loss: 0.021682588383555412\n",
      "Epoch 1051, Loss: 0.13110556453466415, Final Batch Loss: 0.046150803565979004\n",
      "Epoch 1052, Loss: 0.04211060144007206, Final Batch Loss: 0.017152145504951477\n",
      "Epoch 1053, Loss: 0.07992606610059738, Final Batch Loss: 0.044935520738363266\n",
      "Epoch 1054, Loss: 0.06319781020283699, Final Batch Loss: 0.025498956441879272\n",
      "Epoch 1055, Loss: 0.1039421409368515, Final Batch Loss: 0.040949851274490356\n",
      "Epoch 1056, Loss: 0.03262457624077797, Final Batch Loss: 0.023089218884706497\n",
      "Epoch 1057, Loss: 0.0547797167673707, Final Batch Loss: 0.04077625647187233\n",
      "Epoch 1058, Loss: 0.08627407625317574, Final Batch Loss: 0.05413030833005905\n",
      "Epoch 1059, Loss: 0.046155137941241264, Final Batch Loss: 0.010677089914679527\n",
      "Epoch 1060, Loss: 0.0350375110283494, Final Batch Loss: 0.015133802779018879\n",
      "Epoch 1061, Loss: 0.035677894949913025, Final Batch Loss: 0.021274084225296974\n",
      "Epoch 1062, Loss: 0.07625923305749893, Final Batch Loss: 0.05092134326696396\n",
      "Epoch 1063, Loss: 0.06098809279501438, Final Batch Loss: 0.022670498117804527\n",
      "Epoch 1064, Loss: 0.047419752925634384, Final Batch Loss: 0.028789402917027473\n",
      "Epoch 1065, Loss: 0.07609369792044163, Final Batch Loss: 0.05909881368279457\n",
      "Epoch 1066, Loss: 0.08535171672701836, Final Batch Loss: 0.04706668108701706\n",
      "Epoch 1067, Loss: 0.13150398805737495, Final Batch Loss: 0.08688998967409134\n",
      "Epoch 1068, Loss: 0.0677611231803894, Final Batch Loss: 0.01894988864660263\n",
      "Epoch 1069, Loss: 0.10416621901094913, Final Batch Loss: 0.08034142851829529\n",
      "Epoch 1070, Loss: 0.03951336909085512, Final Batch Loss: 0.027692001312971115\n",
      "Epoch 1071, Loss: 0.05936337076127529, Final Batch Loss: 0.03764048591256142\n",
      "Epoch 1072, Loss: 0.08310988917946815, Final Batch Loss: 0.056271955370903015\n",
      "Epoch 1073, Loss: 0.07665351778268814, Final Batch Loss: 0.04642881453037262\n",
      "Epoch 1074, Loss: 0.08452436700463295, Final Batch Loss: 0.04807678610086441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1075, Loss: 0.07400637865066528, Final Batch Loss: 0.0264161117374897\n",
      "Epoch 1076, Loss: 0.04136552941054106, Final Batch Loss: 0.013708745129406452\n",
      "Epoch 1077, Loss: 0.09971904009580612, Final Batch Loss: 0.05743257701396942\n",
      "Epoch 1078, Loss: 0.06311654858291149, Final Batch Loss: 0.034390781074762344\n",
      "Epoch 1079, Loss: 0.07708058692514896, Final Batch Loss: 0.0515914186835289\n",
      "Epoch 1080, Loss: 0.043101800605654716, Final Batch Loss: 0.014304913580417633\n",
      "Epoch 1081, Loss: 0.03701676148921251, Final Batch Loss: 0.012387328781187534\n",
      "Epoch 1082, Loss: 0.07110300287604332, Final Batch Loss: 0.044331494718790054\n",
      "Epoch 1083, Loss: 0.05277036502957344, Final Batch Loss: 0.02156585268676281\n",
      "Epoch 1084, Loss: 0.05642234906554222, Final Batch Loss: 0.017004292458295822\n",
      "Epoch 1085, Loss: 0.0641075000166893, Final Batch Loss: 0.032726455479860306\n",
      "Epoch 1086, Loss: 0.06381469592452049, Final Batch Loss: 0.018536914139986038\n",
      "Epoch 1087, Loss: 0.0391544196754694, Final Batch Loss: 0.011035464704036713\n",
      "Epoch 1088, Loss: 0.07495806366205215, Final Batch Loss: 0.018972843885421753\n",
      "Epoch 1089, Loss: 0.04999536648392677, Final Batch Loss: 0.016375400125980377\n",
      "Epoch 1090, Loss: 0.08315680548548698, Final Batch Loss: 0.042881082743406296\n",
      "Epoch 1091, Loss: 0.04057248216122389, Final Batch Loss: 0.010756135918200016\n",
      "Epoch 1092, Loss: 0.061096581630408764, Final Batch Loss: 0.013493704609572887\n",
      "Epoch 1093, Loss: 0.05999954231083393, Final Batch Loss: 0.023662144318223\n",
      "Epoch 1094, Loss: 0.056961849331855774, Final Batch Loss: 0.019557297229766846\n",
      "Epoch 1095, Loss: 0.041274793446063995, Final Batch Loss: 0.02307765930891037\n",
      "Epoch 1096, Loss: 0.09796133451163769, Final Batch Loss: 0.025012439116835594\n",
      "Epoch 1097, Loss: 0.051194602623581886, Final Batch Loss: 0.023926645517349243\n",
      "Epoch 1098, Loss: 0.047349127009510994, Final Batch Loss: 0.022894591093063354\n",
      "Epoch 1099, Loss: 0.04764920845627785, Final Batch Loss: 0.025565816089510918\n",
      "Epoch 1100, Loss: 0.07011385262012482, Final Batch Loss: 0.02427496388554573\n",
      "Epoch 1101, Loss: 0.04632064513862133, Final Batch Loss: 0.03393910825252533\n",
      "Epoch 1102, Loss: 0.06930656731128693, Final Batch Loss: 0.032735396176576614\n",
      "Epoch 1103, Loss: 0.1168733723461628, Final Batch Loss: 0.066861592233181\n",
      "Epoch 1104, Loss: 0.035229504108428955, Final Batch Loss: 0.015030313283205032\n",
      "Epoch 1105, Loss: 0.07607923354953527, Final Batch Loss: 0.0640745759010315\n",
      "Epoch 1106, Loss: 0.0527438223361969, Final Batch Loss: 0.035260383039712906\n",
      "Epoch 1107, Loss: 0.05851362086832523, Final Batch Loss: 0.028827689588069916\n",
      "Epoch 1108, Loss: 0.09146084450185299, Final Batch Loss: 0.0637635812163353\n",
      "Epoch 1109, Loss: 0.07740376610308886, Final Batch Loss: 0.015507065691053867\n",
      "Epoch 1110, Loss: 0.05909022316336632, Final Batch Loss: 0.041440412402153015\n",
      "Epoch 1111, Loss: 0.07834183983504772, Final Batch Loss: 0.024669313803315163\n",
      "Epoch 1112, Loss: 0.13129962608218193, Final Batch Loss: 0.07993413507938385\n",
      "Epoch 1113, Loss: 0.05777608975768089, Final Batch Loss: 0.029215887188911438\n",
      "Epoch 1114, Loss: 0.09083056077361107, Final Batch Loss: 0.06795153766870499\n",
      "Epoch 1115, Loss: 0.11418446153402328, Final Batch Loss: 0.023172996938228607\n",
      "Epoch 1116, Loss: 0.0823749452829361, Final Batch Loss: 0.06436068564653397\n",
      "Epoch 1117, Loss: 0.05693052522838116, Final Batch Loss: 0.015031864866614342\n",
      "Epoch 1118, Loss: 0.06353439763188362, Final Batch Loss: 0.03136640042066574\n",
      "Epoch 1119, Loss: 0.06434598937630653, Final Batch Loss: 0.0244448222219944\n",
      "Epoch 1120, Loss: 0.09149884432554245, Final Batch Loss: 0.035831961780786514\n",
      "Epoch 1121, Loss: 0.0914001390337944, Final Batch Loss: 0.020876578986644745\n",
      "Epoch 1122, Loss: 0.046061960980296135, Final Batch Loss: 0.021361416205763817\n",
      "Epoch 1123, Loss: 0.10251407697796822, Final Batch Loss: 0.03987054154276848\n",
      "Epoch 1124, Loss: 0.08341998234391212, Final Batch Loss: 0.03500538691878319\n",
      "Epoch 1125, Loss: 0.06669583544135094, Final Batch Loss: 0.02955905720591545\n",
      "Epoch 1126, Loss: 0.031839859671890736, Final Batch Loss: 0.013578795827925205\n",
      "Epoch 1127, Loss: 0.10526576265692711, Final Batch Loss: 0.06006275862455368\n",
      "Epoch 1128, Loss: 0.10822372138500214, Final Batch Loss: 0.07316723465919495\n",
      "Epoch 1129, Loss: 0.03861590847373009, Final Batch Loss: 0.01540195383131504\n",
      "Epoch 1130, Loss: 0.19174741953611374, Final Batch Loss: 0.07091465592384338\n",
      "Epoch 1131, Loss: 0.07097497954964638, Final Batch Loss: 0.043622810393571854\n",
      "Epoch 1132, Loss: 0.11434120312333107, Final Batch Loss: 0.042636264115571976\n",
      "Epoch 1133, Loss: 0.12436244450509548, Final Batch Loss: 0.10156469792127609\n",
      "Epoch 1134, Loss: 0.07404474914073944, Final Batch Loss: 0.014532089233398438\n",
      "Epoch 1135, Loss: 0.054328556172549725, Final Batch Loss: 0.01412488054484129\n",
      "Epoch 1136, Loss: 0.053175526671111584, Final Batch Loss: 0.03924264758825302\n",
      "Epoch 1137, Loss: 0.04631048999726772, Final Batch Loss: 0.021383265033364296\n",
      "Epoch 1138, Loss: 0.05185745004564524, Final Batch Loss: 0.015595161356031895\n",
      "Epoch 1139, Loss: 0.059052991680800915, Final Batch Loss: 0.009699988178908825\n",
      "Epoch 1140, Loss: 0.05272567272186279, Final Batch Loss: 0.03086291439831257\n",
      "Epoch 1141, Loss: 0.07496803347021341, Final Batch Loss: 0.012591485865414143\n",
      "Epoch 1142, Loss: 0.044805364683270454, Final Batch Loss: 0.02826860174536705\n",
      "Epoch 1143, Loss: 0.07237750105559826, Final Batch Loss: 0.05418774485588074\n",
      "Epoch 1144, Loss: 0.02806716598570347, Final Batch Loss: 0.015900058671832085\n",
      "Epoch 1145, Loss: 0.06674124952405691, Final Batch Loss: 0.013381796889007092\n",
      "Epoch 1146, Loss: 0.1019156314432621, Final Batch Loss: 0.05595190078020096\n",
      "Epoch 1147, Loss: 0.021544805727899075, Final Batch Loss: 0.011349895037710667\n",
      "Epoch 1148, Loss: 0.03337668813765049, Final Batch Loss: 0.016916578635573387\n",
      "Epoch 1149, Loss: 0.08092176541686058, Final Batch Loss: 0.03449224680662155\n",
      "Epoch 1150, Loss: 0.059471672400832176, Final Batch Loss: 0.03677346184849739\n",
      "Epoch 1151, Loss: 0.02903655916452408, Final Batch Loss: 0.013849200680851936\n",
      "Epoch 1152, Loss: 0.039973409846425056, Final Batch Loss: 0.022719474509358406\n",
      "Epoch 1153, Loss: 0.06378268264234066, Final Batch Loss: 0.020355673506855965\n",
      "Epoch 1154, Loss: 0.05917062982916832, Final Batch Loss: 0.02038789913058281\n",
      "Epoch 1155, Loss: 0.02838216070085764, Final Batch Loss: 0.013669522479176521\n",
      "Epoch 1156, Loss: 0.1272755153477192, Final Batch Loss: 0.09268450736999512\n",
      "Epoch 1157, Loss: 0.1033822763711214, Final Batch Loss: 0.09094254672527313\n",
      "Epoch 1158, Loss: 0.08617895469069481, Final Batch Loss: 0.026107072830200195\n",
      "Epoch 1159, Loss: 0.051888760179281235, Final Batch Loss: 0.016420692205429077\n",
      "Epoch 1160, Loss: 0.10069335252046585, Final Batch Loss: 0.07230670750141144\n",
      "Epoch 1161, Loss: 0.0515950508415699, Final Batch Loss: 0.03501904010772705\n",
      "Epoch 1162, Loss: 0.09349068999290466, Final Batch Loss: 0.027224242687225342\n",
      "Epoch 1163, Loss: 0.06932033970952034, Final Batch Loss: 0.02346936985850334\n",
      "Epoch 1164, Loss: 0.1300899162888527, Final Batch Loss: 0.08726038783788681\n",
      "Epoch 1165, Loss: 0.10480573028326035, Final Batch Loss: 0.06664267182350159\n",
      "Epoch 1166, Loss: 0.049222296103835106, Final Batch Loss: 0.017730211839079857\n",
      "Epoch 1167, Loss: 0.09089458361268044, Final Batch Loss: 0.04705830290913582\n",
      "Epoch 1168, Loss: 0.03518062084913254, Final Batch Loss: 0.016179369762539864\n",
      "Epoch 1169, Loss: 0.08342079818248749, Final Batch Loss: 0.03296307846903801\n",
      "Epoch 1170, Loss: 0.06166639365255833, Final Batch Loss: 0.01775205321609974\n",
      "Epoch 1171, Loss: 0.06627689301967621, Final Batch Loss: 0.03871023654937744\n",
      "Epoch 1172, Loss: 0.05164181813597679, Final Batch Loss: 0.02440265566110611\n",
      "Epoch 1173, Loss: 0.04851539805531502, Final Batch Loss: 0.020142871886491776\n",
      "Epoch 1174, Loss: 0.0726796593517065, Final Batch Loss: 0.022017674520611763\n",
      "Epoch 1175, Loss: 0.0445435456931591, Final Batch Loss: 0.0221162810921669\n",
      "Epoch 1176, Loss: 0.10881377197802067, Final Batch Loss: 0.08259468525648117\n",
      "Epoch 1177, Loss: 0.0635907594114542, Final Batch Loss: 0.03757506608963013\n",
      "Epoch 1178, Loss: 0.029340384528040886, Final Batch Loss: 0.013272751122713089\n",
      "Epoch 1179, Loss: 0.10594557225704193, Final Batch Loss: 0.05888962373137474\n",
      "Epoch 1180, Loss: 0.03414872754365206, Final Batch Loss: 0.020687386393547058\n",
      "Epoch 1181, Loss: 0.11401057429611683, Final Batch Loss: 0.0990545004606247\n",
      "Epoch 1182, Loss: 0.04132391698658466, Final Batch Loss: 0.013370741158723831\n",
      "Epoch 1183, Loss: 0.1200341209769249, Final Batch Loss: 0.08770602941513062\n",
      "Epoch 1184, Loss: 0.09220763109624386, Final Batch Loss: 0.01778637431561947\n",
      "Epoch 1185, Loss: 0.1669258177280426, Final Batch Loss: 0.14220069348812103\n",
      "Epoch 1186, Loss: 0.055816249921917915, Final Batch Loss: 0.039304498583078384\n",
      "Epoch 1187, Loss: 0.09251853451132774, Final Batch Loss: 0.03125913441181183\n",
      "Epoch 1188, Loss: 0.08533762395381927, Final Batch Loss: 0.025917615741491318\n",
      "Epoch 1189, Loss: 0.048782819882035255, Final Batch Loss: 0.02214191108942032\n",
      "Epoch 1190, Loss: 0.09642637521028519, Final Batch Loss: 0.06229548156261444\n",
      "Epoch 1191, Loss: 0.035093726590275764, Final Batch Loss: 0.012707328423857689\n",
      "Epoch 1192, Loss: 0.03572533652186394, Final Batch Loss: 0.01039528101682663\n",
      "Epoch 1193, Loss: 0.12135934829711914, Final Batch Loss: 0.07756780833005905\n",
      "Epoch 1194, Loss: 0.07372415065765381, Final Batch Loss: 0.04638107493519783\n",
      "Epoch 1195, Loss: 0.10052932240068913, Final Batch Loss: 0.024635234847664833\n",
      "Epoch 1196, Loss: 0.10712362453341484, Final Batch Loss: 0.04291951283812523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1197, Loss: 0.05655076168477535, Final Batch Loss: 0.04283561557531357\n",
      "Epoch 1198, Loss: 0.059231679886579514, Final Batch Loss: 0.03903134539723396\n",
      "Epoch 1199, Loss: 0.08241330459713936, Final Batch Loss: 0.03787807375192642\n",
      "Epoch 1200, Loss: 0.05335462465882301, Final Batch Loss: 0.026232458651065826\n",
      "Epoch 1201, Loss: 0.056775810196995735, Final Batch Loss: 0.037786275148391724\n",
      "Epoch 1202, Loss: 0.0734932329505682, Final Batch Loss: 0.043350689113140106\n",
      "Epoch 1203, Loss: 0.041514662094414234, Final Batch Loss: 0.012660934589803219\n",
      "Epoch 1204, Loss: 0.047255102545022964, Final Batch Loss: 0.03263917192816734\n",
      "Epoch 1205, Loss: 0.07792790792882442, Final Batch Loss: 0.022986898198723793\n",
      "Epoch 1206, Loss: 0.06317054480314255, Final Batch Loss: 0.018688395619392395\n",
      "Epoch 1207, Loss: 0.06574499979615211, Final Batch Loss: 0.023302994668483734\n",
      "Epoch 1208, Loss: 0.09677458927035332, Final Batch Loss: 0.07219526916742325\n",
      "Epoch 1209, Loss: 0.059296365827322006, Final Batch Loss: 0.02480161562561989\n",
      "Epoch 1210, Loss: 0.07368440181016922, Final Batch Loss: 0.04682350531220436\n",
      "Epoch 1211, Loss: 0.06183623801916838, Final Batch Loss: 0.05081739276647568\n",
      "Epoch 1212, Loss: 0.031537264585494995, Final Batch Loss: 0.01616165041923523\n",
      "Epoch 1213, Loss: 0.044131552800536156, Final Batch Loss: 0.018945101648569107\n",
      "Epoch 1214, Loss: 0.06202142685651779, Final Batch Loss: 0.02957986295223236\n",
      "Epoch 1215, Loss: 0.06487569212913513, Final Batch Loss: 0.03601812571287155\n",
      "Epoch 1216, Loss: 0.0867619439959526, Final Batch Loss: 0.05323825404047966\n",
      "Epoch 1217, Loss: 0.03834646753966808, Final Batch Loss: 0.01866653747856617\n",
      "Epoch 1218, Loss: 0.06522608362138271, Final Batch Loss: 0.02962319739162922\n",
      "Epoch 1219, Loss: 0.03296503610908985, Final Batch Loss: 0.012698285281658173\n",
      "Epoch 1220, Loss: 0.08957130834460258, Final Batch Loss: 0.05766536667943001\n",
      "Epoch 1221, Loss: 0.05711407959461212, Final Batch Loss: 0.030176909640431404\n",
      "Epoch 1222, Loss: 0.06576007977128029, Final Batch Loss: 0.04333054646849632\n",
      "Epoch 1223, Loss: 0.02767991367727518, Final Batch Loss: 0.014361225068569183\n",
      "Epoch 1224, Loss: 0.0354049876332283, Final Batch Loss: 0.016563618555665016\n",
      "Epoch 1225, Loss: 0.03518839180469513, Final Batch Loss: 0.012166174128651619\n",
      "Epoch 1226, Loss: 0.04075310658663511, Final Batch Loss: 0.012121145613491535\n",
      "Epoch 1227, Loss: 0.06683095917105675, Final Batch Loss: 0.03441493958234787\n",
      "Epoch 1228, Loss: 0.05220338888466358, Final Batch Loss: 0.03621120750904083\n",
      "Epoch 1229, Loss: 0.030003282241523266, Final Batch Loss: 0.016042253002524376\n",
      "Epoch 1230, Loss: 0.06893494911491871, Final Batch Loss: 0.028598731383681297\n",
      "Epoch 1231, Loss: 0.058648332953453064, Final Batch Loss: 0.039569396525621414\n",
      "Epoch 1232, Loss: 0.023306189104914665, Final Batch Loss: 0.007035169750452042\n",
      "Epoch 1233, Loss: 0.03847766853868961, Final Batch Loss: 0.018494166433811188\n",
      "Epoch 1234, Loss: 0.0967458188533783, Final Batch Loss: 0.0688706561923027\n",
      "Epoch 1235, Loss: 0.035640571266412735, Final Batch Loss: 0.019522590562701225\n",
      "Epoch 1236, Loss: 0.03490633983165026, Final Batch Loss: 0.009719639085233212\n",
      "Epoch 1237, Loss: 0.02904321253299713, Final Batch Loss: 0.015099192038178444\n",
      "Epoch 1238, Loss: 0.06257675960659981, Final Batch Loss: 0.03909025341272354\n",
      "Epoch 1239, Loss: 0.05404542200267315, Final Batch Loss: 0.03356354311108589\n",
      "Epoch 1240, Loss: 0.06574310548603535, Final Batch Loss: 0.024998938664793968\n",
      "Epoch 1241, Loss: 0.052821310237050056, Final Batch Loss: 0.03468210622668266\n",
      "Epoch 1242, Loss: 0.02247590944170952, Final Batch Loss: 0.014893831685185432\n",
      "Epoch 1243, Loss: 0.06214919872581959, Final Batch Loss: 0.025061873719096184\n",
      "Epoch 1244, Loss: 0.020568739622831345, Final Batch Loss: 0.014882465824484825\n",
      "Epoch 1245, Loss: 0.055514903739094734, Final Batch Loss: 0.008466651663184166\n",
      "Epoch 1246, Loss: 0.03192534204572439, Final Batch Loss: 0.007913344539701939\n",
      "Epoch 1247, Loss: 0.058084108866751194, Final Batch Loss: 0.009825804270803928\n",
      "Epoch 1248, Loss: 0.06806348823010921, Final Batch Loss: 0.04392900690436363\n",
      "Epoch 1249, Loss: 0.055954596027731895, Final Batch Loss: 0.025455903261899948\n",
      "Epoch 1250, Loss: 0.041323985904455185, Final Batch Loss: 0.016360217705368996\n",
      "Epoch 1251, Loss: 0.05276593565940857, Final Batch Loss: 0.024878263473510742\n",
      "Epoch 1252, Loss: 0.03360759932547808, Final Batch Loss: 0.023424923419952393\n",
      "Epoch 1253, Loss: 0.041601840406656265, Final Batch Loss: 0.024806039407849312\n",
      "Epoch 1254, Loss: 0.05501489993184805, Final Batch Loss: 0.005760013125836849\n",
      "Epoch 1255, Loss: 0.03520573675632477, Final Batch Loss: 0.017787283286452293\n",
      "Epoch 1256, Loss: 0.04020248353481293, Final Batch Loss: 0.021663490682840347\n",
      "Epoch 1257, Loss: 0.048742031678557396, Final Batch Loss: 0.010174302384257317\n",
      "Epoch 1258, Loss: 0.05177692137658596, Final Batch Loss: 0.032517388463020325\n",
      "Epoch 1259, Loss: 0.05692256987094879, Final Batch Loss: 0.008811701089143753\n",
      "Epoch 1260, Loss: 0.044008065946400166, Final Batch Loss: 0.0145217040553689\n",
      "Epoch 1261, Loss: 0.040509335696697235, Final Batch Loss: 0.011741600930690765\n",
      "Epoch 1262, Loss: 0.060265304520726204, Final Batch Loss: 0.04762337729334831\n",
      "Epoch 1263, Loss: 0.05598053149878979, Final Batch Loss: 0.026956934481859207\n",
      "Epoch 1264, Loss: 0.060149967670440674, Final Batch Loss: 0.019381243735551834\n",
      "Epoch 1265, Loss: 0.031549119390547276, Final Batch Loss: 0.014336337335407734\n",
      "Epoch 1266, Loss: 0.04565442260354757, Final Batch Loss: 0.013378354720771313\n",
      "Epoch 1267, Loss: 0.017413318157196045, Final Batch Loss: 0.008353780955076218\n",
      "Epoch 1268, Loss: 0.057851579040288925, Final Batch Loss: 0.021092578768730164\n",
      "Epoch 1269, Loss: 0.040633780881762505, Final Batch Loss: 0.009259259328246117\n",
      "Epoch 1270, Loss: 0.09317685291171074, Final Batch Loss: 0.046861954033374786\n",
      "Epoch 1271, Loss: 0.06049365922808647, Final Batch Loss: 0.03623855113983154\n",
      "Epoch 1272, Loss: 0.031085669994354248, Final Batch Loss: 0.017370231449604034\n",
      "Epoch 1273, Loss: 0.04116260167211294, Final Batch Loss: 0.02692805975675583\n",
      "Epoch 1274, Loss: 0.08819051086902618, Final Batch Loss: 0.05658750236034393\n",
      "Epoch 1275, Loss: 0.09709719195961952, Final Batch Loss: 0.03626979887485504\n",
      "Epoch 1276, Loss: 0.030182565562427044, Final Batch Loss: 0.015004755929112434\n",
      "Epoch 1277, Loss: 0.057081423699855804, Final Batch Loss: 0.019783243536949158\n",
      "Epoch 1278, Loss: 0.05528016947209835, Final Batch Loss: 0.016931166872382164\n",
      "Epoch 1279, Loss: 0.11074629332870245, Final Batch Loss: 0.09814278781414032\n",
      "Epoch 1280, Loss: 0.038902753964066505, Final Batch Loss: 0.02953203022480011\n",
      "Epoch 1281, Loss: 0.05639417842030525, Final Batch Loss: 0.03242788091301918\n",
      "Epoch 1282, Loss: 0.08038990572094917, Final Batch Loss: 0.030727624893188477\n",
      "Epoch 1283, Loss: 0.07222852483391762, Final Batch Loss: 0.03385203704237938\n",
      "Epoch 1284, Loss: 0.103345587849617, Final Batch Loss: 0.052341219037771225\n",
      "Epoch 1285, Loss: 0.039518666453659534, Final Batch Loss: 0.013443914242088795\n",
      "Epoch 1286, Loss: 0.05697876401245594, Final Batch Loss: 0.030991502106189728\n",
      "Epoch 1287, Loss: 0.06622407212853432, Final Batch Loss: 0.04006127640604973\n",
      "Epoch 1288, Loss: 0.058910105377435684, Final Batch Loss: 0.022783104330301285\n",
      "Epoch 1289, Loss: 0.06547673419117928, Final Batch Loss: 0.03069159761071205\n",
      "Epoch 1290, Loss: 0.053523341193795204, Final Batch Loss: 0.03566047549247742\n",
      "Epoch 1291, Loss: 0.05781933106482029, Final Batch Loss: 0.03560005500912666\n",
      "Epoch 1292, Loss: 0.11086034029722214, Final Batch Loss: 0.07167300581932068\n",
      "Epoch 1293, Loss: 0.09233657270669937, Final Batch Loss: 0.054495133459568024\n",
      "Epoch 1294, Loss: 0.05205756053328514, Final Batch Loss: 0.02394760586321354\n",
      "Epoch 1295, Loss: 0.05700190644711256, Final Batch Loss: 0.04385846480727196\n",
      "Epoch 1296, Loss: 0.08665576204657555, Final Batch Loss: 0.051610078662633896\n",
      "Epoch 1297, Loss: 0.03026633244007826, Final Batch Loss: 0.021634381264448166\n",
      "Epoch 1298, Loss: 0.06659604609012604, Final Batch Loss: 0.023462262004613876\n",
      "Epoch 1299, Loss: 0.058424413204193115, Final Batch Loss: 0.044710006564855576\n",
      "Epoch 1300, Loss: 0.05216200649738312, Final Batch Loss: 0.021027591079473495\n",
      "Epoch 1301, Loss: 0.0727184358984232, Final Batch Loss: 0.016881035640835762\n",
      "Epoch 1302, Loss: 0.08964349702000618, Final Batch Loss: 0.046753913164138794\n",
      "Epoch 1303, Loss: 0.04462622106075287, Final Batch Loss: 0.014261065050959587\n",
      "Epoch 1304, Loss: 0.032611528411507607, Final Batch Loss: 0.00858418457210064\n",
      "Epoch 1305, Loss: 0.06469154916703701, Final Batch Loss: 0.016578668728470802\n",
      "Epoch 1306, Loss: 0.04671187698841095, Final Batch Loss: 0.012221969664096832\n",
      "Epoch 1307, Loss: 0.07932545058429241, Final Batch Loss: 0.02075210027396679\n",
      "Epoch 1308, Loss: 0.09923844411969185, Final Batch Loss: 0.05434698238968849\n",
      "Epoch 1309, Loss: 0.030340607278048992, Final Batch Loss: 0.015528570860624313\n",
      "Epoch 1310, Loss: 0.044296919368207455, Final Batch Loss: 0.012629850767552853\n",
      "Epoch 1311, Loss: 0.047630028799176216, Final Batch Loss: 0.025813313201069832\n",
      "Epoch 1312, Loss: 0.09047668799757957, Final Batch Loss: 0.05378631129860878\n",
      "Epoch 1313, Loss: 0.0318982508033514, Final Batch Loss: 0.010400278493762016\n",
      "Epoch 1314, Loss: 0.04097294993698597, Final Batch Loss: 0.015589268878102303\n",
      "Epoch 1315, Loss: 0.03049019630998373, Final Batch Loss: 0.015531651675701141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1316, Loss: 0.05381244048476219, Final Batch Loss: 0.030854081735014915\n",
      "Epoch 1317, Loss: 0.040805425494909286, Final Batch Loss: 0.011477656662464142\n",
      "Epoch 1318, Loss: 0.03942827507853508, Final Batch Loss: 0.030471304431557655\n",
      "Epoch 1319, Loss: 0.07319596409797668, Final Batch Loss: 0.04754424840211868\n",
      "Epoch 1320, Loss: 0.03735745884478092, Final Batch Loss: 0.023868514224886894\n",
      "Epoch 1321, Loss: 0.028335182927548885, Final Batch Loss: 0.0031799813732504845\n",
      "Epoch 1322, Loss: 0.04597637429833412, Final Batch Loss: 0.022128378972411156\n",
      "Epoch 1323, Loss: 0.07617060653865337, Final Batch Loss: 0.05489388480782509\n",
      "Epoch 1324, Loss: 0.034567276015877724, Final Batch Loss: 0.026390213519334793\n",
      "Epoch 1325, Loss: 0.04155777022242546, Final Batch Loss: 0.019284771755337715\n",
      "Epoch 1326, Loss: 0.03748318087309599, Final Batch Loss: 0.02281814254820347\n",
      "Epoch 1327, Loss: 0.0566690918058157, Final Batch Loss: 0.02867395058274269\n",
      "Epoch 1328, Loss: 0.05375927221029997, Final Batch Loss: 0.04432867839932442\n",
      "Epoch 1329, Loss: 0.04017426259815693, Final Batch Loss: 0.02031262032687664\n",
      "Epoch 1330, Loss: 0.04019472934305668, Final Batch Loss: 0.022855285555124283\n",
      "Epoch 1331, Loss: 0.0890355659648776, Final Batch Loss: 0.07448676973581314\n",
      "Epoch 1332, Loss: 0.05864932481199503, Final Batch Loss: 0.007960674352943897\n",
      "Epoch 1333, Loss: 0.060010310262441635, Final Batch Loss: 0.010546036064624786\n",
      "Epoch 1334, Loss: 0.028056022711098194, Final Batch Loss: 0.014705059118568897\n",
      "Epoch 1335, Loss: 0.07646510004997253, Final Batch Loss: 0.04429209604859352\n",
      "Epoch 1336, Loss: 0.05641457065939903, Final Batch Loss: 0.026864701882004738\n",
      "Epoch 1337, Loss: 0.05719560943543911, Final Batch Loss: 0.03966366872191429\n",
      "Epoch 1338, Loss: 0.11193394288420677, Final Batch Loss: 0.05030978471040726\n",
      "Epoch 1339, Loss: 0.04700111038982868, Final Batch Loss: 0.009378274902701378\n",
      "Epoch 1340, Loss: 0.07026155851781368, Final Batch Loss: 0.05082394555211067\n",
      "Epoch 1341, Loss: 0.03902306407690048, Final Batch Loss: 0.02879442647099495\n",
      "Epoch 1342, Loss: 0.08019569516181946, Final Batch Loss: 0.01718056946992874\n",
      "Epoch 1343, Loss: 0.12374351918697357, Final Batch Loss: 0.0900857225060463\n",
      "Epoch 1344, Loss: 0.06674863211810589, Final Batch Loss: 0.022837849333882332\n",
      "Epoch 1345, Loss: 0.05604432988911867, Final Batch Loss: 0.04124562069773674\n",
      "Epoch 1346, Loss: 0.0892896493896842, Final Batch Loss: 0.013054310344159603\n",
      "Epoch 1347, Loss: 0.04230313654989004, Final Batch Loss: 0.029731426388025284\n",
      "Epoch 1348, Loss: 0.036271034739911556, Final Batch Loss: 0.02320716343820095\n",
      "Epoch 1349, Loss: 0.060195788741111755, Final Batch Loss: 0.022597528994083405\n",
      "Epoch 1350, Loss: 0.05628735385835171, Final Batch Loss: 0.016282537952065468\n",
      "Epoch 1351, Loss: 0.07014582678675652, Final Batch Loss: 0.03043542057275772\n",
      "Epoch 1352, Loss: 0.06515170447528362, Final Batch Loss: 0.04215623065829277\n",
      "Epoch 1353, Loss: 0.0619464423507452, Final Batch Loss: 0.011065853759646416\n",
      "Epoch 1354, Loss: 0.039604149758815765, Final Batch Loss: 0.028543921187520027\n",
      "Epoch 1355, Loss: 0.029563594609498978, Final Batch Loss: 0.018594393506646156\n",
      "Epoch 1356, Loss: 0.050230054184794426, Final Batch Loss: 0.034677904099226\n",
      "Epoch 1357, Loss: 0.044876535423099995, Final Batch Loss: 0.03411609306931496\n",
      "Epoch 1358, Loss: 0.044493502005934715, Final Batch Loss: 0.03035759925842285\n",
      "Epoch 1359, Loss: 0.04828576743602753, Final Batch Loss: 0.011860299855470657\n",
      "Epoch 1360, Loss: 0.0477629080414772, Final Batch Loss: 0.024177880957722664\n",
      "Epoch 1361, Loss: 0.054497478529810905, Final Batch Loss: 0.031685058027505875\n",
      "Epoch 1362, Loss: 0.047796785831451416, Final Batch Loss: 0.042215872555971146\n",
      "Epoch 1363, Loss: 0.04074358381330967, Final Batch Loss: 0.014080286026000977\n",
      "Epoch 1364, Loss: 0.06149943545460701, Final Batch Loss: 0.051916494965553284\n",
      "Epoch 1365, Loss: 0.0323594668880105, Final Batch Loss: 0.010847375728189945\n",
      "Epoch 1366, Loss: 0.04089934937655926, Final Batch Loss: 0.022740190848708153\n",
      "Epoch 1367, Loss: 0.08506930619478226, Final Batch Loss: 0.020212426781654358\n",
      "Epoch 1368, Loss: 0.06076575070619583, Final Batch Loss: 0.04385797679424286\n",
      "Epoch 1369, Loss: 0.07452039048075676, Final Batch Loss: 0.06409375369548798\n",
      "Epoch 1370, Loss: 0.0644189715385437, Final Batch Loss: 0.03735681623220444\n",
      "Epoch 1371, Loss: 0.07133445329964161, Final Batch Loss: 0.04704517126083374\n",
      "Epoch 1372, Loss: 0.04280707985162735, Final Batch Loss: 0.02248707041144371\n",
      "Epoch 1373, Loss: 0.04616772010922432, Final Batch Loss: 0.025034099817276\n",
      "Epoch 1374, Loss: 0.04359706491231918, Final Batch Loss: 0.027560679242014885\n",
      "Epoch 1375, Loss: 0.05329693481326103, Final Batch Loss: 0.03878018632531166\n",
      "Epoch 1376, Loss: 0.020649696234613657, Final Batch Loss: 0.007667266298085451\n",
      "Epoch 1377, Loss: 0.049615765921771526, Final Batch Loss: 0.03508656099438667\n",
      "Epoch 1378, Loss: 0.022312243469059467, Final Batch Loss: 0.009919839911162853\n",
      "Epoch 1379, Loss: 0.027323896065354347, Final Batch Loss: 0.017472701147198677\n",
      "Epoch 1380, Loss: 0.055445365607738495, Final Batch Loss: 0.032222773879766464\n",
      "Epoch 1381, Loss: 0.16907239705324173, Final Batch Loss: 0.1226397380232811\n",
      "Epoch 1382, Loss: 0.047389449551701546, Final Batch Loss: 0.01777810789644718\n",
      "Epoch 1383, Loss: 0.08967939391732216, Final Batch Loss: 0.014332246035337448\n",
      "Epoch 1384, Loss: 0.0809016041457653, Final Batch Loss: 0.03137381002306938\n",
      "Epoch 1385, Loss: 0.065733902156353, Final Batch Loss: 0.04170101135969162\n",
      "Epoch 1386, Loss: 0.05473901703953743, Final Batch Loss: 0.009247176349163055\n",
      "Epoch 1387, Loss: 0.0794550497084856, Final Batch Loss: 0.05310681462287903\n",
      "Epoch 1388, Loss: 0.052792586386203766, Final Batch Loss: 0.0248890183866024\n",
      "Epoch 1389, Loss: 0.039431571029126644, Final Batch Loss: 0.014158840291202068\n",
      "Epoch 1390, Loss: 0.06863844580948353, Final Batch Loss: 0.02354813553392887\n",
      "Epoch 1391, Loss: 0.09985102340579033, Final Batch Loss: 0.06317086517810822\n",
      "Epoch 1392, Loss: 0.029791290871798992, Final Batch Loss: 0.01807108335196972\n",
      "Epoch 1393, Loss: 0.038928329944610596, Final Batch Loss: 0.0167773999273777\n",
      "Epoch 1394, Loss: 0.05225740559399128, Final Batch Loss: 0.031793080270290375\n",
      "Epoch 1395, Loss: 0.029944904148578644, Final Batch Loss: 0.01569197326898575\n",
      "Epoch 1396, Loss: 0.014108652248978615, Final Batch Loss: 0.008613877929747105\n",
      "Epoch 1397, Loss: 0.05131768248975277, Final Batch Loss: 0.027367087081074715\n",
      "Epoch 1398, Loss: 0.07010122202336788, Final Batch Loss: 0.022948486730456352\n",
      "Epoch 1399, Loss: 0.038318829610943794, Final Batch Loss: 0.015667788684368134\n",
      "Epoch 1400, Loss: 0.02569541335105896, Final Batch Loss: 0.012002137489616871\n",
      "Epoch 1401, Loss: 0.021772675216197968, Final Batch Loss: 0.005399396643042564\n",
      "Epoch 1402, Loss: 0.033332709223032, Final Batch Loss: 0.0058881789445877075\n",
      "Epoch 1403, Loss: 0.06383177265524864, Final Batch Loss: 0.039241645485162735\n",
      "Epoch 1404, Loss: 0.08542025368660688, Final Batch Loss: 0.07030346989631653\n",
      "Epoch 1405, Loss: 0.035011506639420986, Final Batch Loss: 0.010959903709590435\n",
      "Epoch 1406, Loss: 0.022263912484049797, Final Batch Loss: 0.010247799567878246\n",
      "Epoch 1407, Loss: 0.053790975362062454, Final Batch Loss: 0.025856034830212593\n",
      "Epoch 1408, Loss: 0.03806067770346999, Final Batch Loss: 0.030886979773640633\n",
      "Epoch 1409, Loss: 0.07642802782356739, Final Batch Loss: 0.04774487018585205\n",
      "Epoch 1410, Loss: 0.04805454798042774, Final Batch Loss: 0.022695349529385567\n",
      "Epoch 1411, Loss: 0.10949048399925232, Final Batch Loss: 0.0670727789402008\n",
      "Epoch 1412, Loss: 0.041386643424630165, Final Batch Loss: 0.032062627375125885\n",
      "Epoch 1413, Loss: 0.06845265626907349, Final Batch Loss: 0.02247467264533043\n",
      "Epoch 1414, Loss: 0.04826357774436474, Final Batch Loss: 0.025459179654717445\n",
      "Epoch 1415, Loss: 0.04016538430005312, Final Batch Loss: 0.010274377651512623\n",
      "Epoch 1416, Loss: 0.05577613227069378, Final Batch Loss: 0.029217783361673355\n",
      "Epoch 1417, Loss: 0.02793741412460804, Final Batch Loss: 0.011138081550598145\n",
      "Epoch 1418, Loss: 0.03728492558002472, Final Batch Loss: 0.01957109570503235\n",
      "Epoch 1419, Loss: 0.029685987159609795, Final Batch Loss: 0.016914652660489082\n",
      "Epoch 1420, Loss: 0.06284922361373901, Final Batch Loss: 0.030003588646650314\n",
      "Epoch 1421, Loss: 0.046918995678424835, Final Batch Loss: 0.02896294742822647\n",
      "Epoch 1422, Loss: 0.032337137032300234, Final Batch Loss: 0.005152775440365076\n",
      "Epoch 1423, Loss: 0.021156150847673416, Final Batch Loss: 0.0145040862262249\n",
      "Epoch 1424, Loss: 0.024910488165915012, Final Batch Loss: 0.008486838079988956\n",
      "Epoch 1425, Loss: 0.058720314875245094, Final Batch Loss: 0.02447826974093914\n",
      "Epoch 1426, Loss: 0.025917927734553814, Final Batch Loss: 0.015211634337902069\n",
      "Epoch 1427, Loss: 0.035729605704545975, Final Batch Loss: 0.017842162400484085\n",
      "Epoch 1428, Loss: 0.04928944446146488, Final Batch Loss: 0.02289223112165928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1429, Loss: 0.0465347645804286, Final Batch Loss: 0.03789729252457619\n",
      "Epoch 1430, Loss: 0.05299793416634202, Final Batch Loss: 0.007312251720577478\n",
      "Epoch 1431, Loss: 0.03096824884414673, Final Batch Loss: 0.020416337996721268\n",
      "Epoch 1432, Loss: 0.024331956170499325, Final Batch Loss: 0.006940365768969059\n",
      "Epoch 1433, Loss: 0.058634208515286446, Final Batch Loss: 0.022857116535305977\n",
      "Epoch 1434, Loss: 0.03634115029126406, Final Batch Loss: 0.021227555349469185\n",
      "Epoch 1435, Loss: 0.07337412238121033, Final Batch Loss: 0.02449280023574829\n",
      "Epoch 1436, Loss: 0.014339622110128403, Final Batch Loss: 0.007547573186457157\n",
      "Epoch 1437, Loss: 0.03767295554280281, Final Batch Loss: 0.026239553466439247\n",
      "Epoch 1438, Loss: 0.07360710948705673, Final Batch Loss: 0.03547065705060959\n",
      "Epoch 1439, Loss: 0.03347859485074878, Final Batch Loss: 0.02714228257536888\n",
      "Epoch 1440, Loss: 0.04639643244445324, Final Batch Loss: 0.02334223873913288\n",
      "Epoch 1441, Loss: 0.04461892135441303, Final Batch Loss: 0.025305205956101418\n",
      "Epoch 1442, Loss: 0.035016476176679134, Final Batch Loss: 0.02124023251235485\n",
      "Epoch 1443, Loss: 0.0744301788508892, Final Batch Loss: 0.039223019033670425\n",
      "Epoch 1444, Loss: 0.0426561813801527, Final Batch Loss: 0.02602909877896309\n",
      "Epoch 1445, Loss: 0.038529531098902225, Final Batch Loss: 0.02716839499771595\n",
      "Epoch 1446, Loss: 0.03649033419787884, Final Batch Loss: 0.01648232340812683\n",
      "Epoch 1447, Loss: 0.027258689049631357, Final Batch Loss: 0.00762943597510457\n",
      "Epoch 1448, Loss: 0.06846568547189236, Final Batch Loss: 0.019707849249243736\n",
      "Epoch 1449, Loss: 0.06218278221786022, Final Batch Loss: 0.04961533844470978\n",
      "Epoch 1450, Loss: 0.046477191150188446, Final Batch Loss: 0.025196611881256104\n",
      "Epoch 1451, Loss: 0.05463943723589182, Final Batch Loss: 0.039578214287757874\n",
      "Epoch 1452, Loss: 0.08323240280151367, Final Batch Loss: 0.05212809517979622\n",
      "Epoch 1453, Loss: 0.04335430637001991, Final Batch Loss: 0.02924259379506111\n",
      "Epoch 1454, Loss: 0.059235306456685066, Final Batch Loss: 0.01936526410281658\n",
      "Epoch 1455, Loss: 0.049542803317308426, Final Batch Loss: 0.03372649848461151\n",
      "Epoch 1456, Loss: 0.03287504706531763, Final Batch Loss: 0.011353016830980778\n",
      "Epoch 1457, Loss: 0.04446902126073837, Final Batch Loss: 0.020799433812499046\n",
      "Epoch 1458, Loss: 0.04694185592234135, Final Batch Loss: 0.022701216861605644\n",
      "Epoch 1459, Loss: 0.01992565207183361, Final Batch Loss: 0.009109320119023323\n",
      "Epoch 1460, Loss: 0.0714993579313159, Final Batch Loss: 0.012955612502992153\n",
      "Epoch 1461, Loss: 0.06540261954069138, Final Batch Loss: 0.04139574617147446\n",
      "Epoch 1462, Loss: 0.0670335590839386, Final Batch Loss: 0.03365971893072128\n",
      "Epoch 1463, Loss: 0.029896531719714403, Final Batch Loss: 0.023288609459996223\n",
      "Epoch 1464, Loss: 0.09391774982213974, Final Batch Loss: 0.055025748908519745\n",
      "Epoch 1465, Loss: 0.039741694927215576, Final Batch Loss: 0.02172473631799221\n",
      "Epoch 1466, Loss: 0.07010732218623161, Final Batch Loss: 0.05862703546881676\n",
      "Epoch 1467, Loss: 0.041005996987223625, Final Batch Loss: 0.0047441404312849045\n",
      "Epoch 1468, Loss: 0.0642481166869402, Final Batch Loss: 0.04049462452530861\n",
      "Epoch 1469, Loss: 0.06792033463716507, Final Batch Loss: 0.023087292909622192\n",
      "Epoch 1470, Loss: 0.05802660435438156, Final Batch Loss: 0.038302257657051086\n",
      "Epoch 1471, Loss: 0.05912225507199764, Final Batch Loss: 0.02779652737081051\n",
      "Epoch 1472, Loss: 0.033737823367118835, Final Batch Loss: 0.022579822689294815\n",
      "Epoch 1473, Loss: 0.10708576627075672, Final Batch Loss: 0.08089499175548553\n",
      "Epoch 1474, Loss: 0.1085130013525486, Final Batch Loss: 0.022359009832143784\n",
      "Epoch 1475, Loss: 0.035288832150399685, Final Batch Loss: 0.02829977683722973\n",
      "Epoch 1476, Loss: 0.09266412630677223, Final Batch Loss: 0.05122825875878334\n",
      "Epoch 1477, Loss: 0.035426768474280834, Final Batch Loss: 0.02029678411781788\n",
      "Epoch 1478, Loss: 0.05585828796029091, Final Batch Loss: 0.012847095727920532\n",
      "Epoch 1479, Loss: 0.08197610452771187, Final Batch Loss: 0.029683329164981842\n",
      "Epoch 1480, Loss: 0.033023959025740623, Final Batch Loss: 0.012233566492795944\n",
      "Epoch 1481, Loss: 0.03695942461490631, Final Batch Loss: 0.020112451165914536\n",
      "Epoch 1482, Loss: 0.044021524488925934, Final Batch Loss: 0.01723279431462288\n",
      "Epoch 1483, Loss: 0.07155756652355194, Final Batch Loss: 0.03131663799285889\n",
      "Epoch 1484, Loss: 0.02895138179883361, Final Batch Loss: 0.006650123279541731\n",
      "Epoch 1485, Loss: 0.073580302298069, Final Batch Loss: 0.024071045219898224\n",
      "Epoch 1486, Loss: 0.05352340638637543, Final Batch Loss: 0.0369061641395092\n",
      "Epoch 1487, Loss: 0.09148801118135452, Final Batch Loss: 0.059755418449640274\n",
      "Epoch 1488, Loss: 0.059787796810269356, Final Batch Loss: 0.025979163125157356\n",
      "Epoch 1489, Loss: 0.03931103739887476, Final Batch Loss: 0.01343283150345087\n",
      "Epoch 1490, Loss: 0.05342105217278004, Final Batch Loss: 0.04372787103056908\n",
      "Epoch 1491, Loss: 0.029604376293718815, Final Batch Loss: 0.0096195163205266\n",
      "Epoch 1492, Loss: 0.047800577245652676, Final Batch Loss: 0.033551864326000214\n",
      "Epoch 1493, Loss: 0.05021798424422741, Final Batch Loss: 0.016676975414156914\n",
      "Epoch 1494, Loss: 0.02989995013922453, Final Batch Loss: 0.011851808987557888\n",
      "Epoch 1495, Loss: 0.02972122374922037, Final Batch Loss: 0.011078852228820324\n",
      "Epoch 1496, Loss: 0.025247158482670784, Final Batch Loss: 0.01555509865283966\n",
      "Epoch 1497, Loss: 0.07049632538110018, Final Batch Loss: 0.012916130013763905\n",
      "Epoch 1498, Loss: 0.05381619744002819, Final Batch Loss: 0.03217223659157753\n",
      "Epoch 1499, Loss: 0.08224411308765411, Final Batch Loss: 0.04328029602766037\n",
      "Epoch 1500, Loss: 0.029783759266138077, Final Batch Loss: 0.010942798107862473\n",
      "Epoch 1501, Loss: 0.05940500646829605, Final Batch Loss: 0.01901666447520256\n",
      "Epoch 1502, Loss: 0.053859745152294636, Final Batch Loss: 0.01240758690983057\n",
      "Epoch 1503, Loss: 0.025843781419098377, Final Batch Loss: 0.008617612533271313\n",
      "Epoch 1504, Loss: 0.040904052555561066, Final Batch Loss: 0.015874072909355164\n",
      "Epoch 1505, Loss: 0.06160806678235531, Final Batch Loss: 0.052446283400058746\n",
      "Epoch 1506, Loss: 0.031081926077604294, Final Batch Loss: 0.012752469629049301\n",
      "Epoch 1507, Loss: 0.043633343651890755, Final Batch Loss: 0.035670071840286255\n",
      "Epoch 1508, Loss: 0.02951765339821577, Final Batch Loss: 0.01430058479309082\n",
      "Epoch 1509, Loss: 0.035994911566376686, Final Batch Loss: 0.015818201005458832\n",
      "Epoch 1510, Loss: 0.04446716606616974, Final Batch Loss: 0.027135387063026428\n",
      "Epoch 1511, Loss: 0.04138604924082756, Final Batch Loss: 0.024355167523026466\n",
      "Epoch 1512, Loss: 0.02459159679710865, Final Batch Loss: 0.015499507077038288\n",
      "Epoch 1513, Loss: 0.09505538269877434, Final Batch Loss: 0.02848934754729271\n",
      "Epoch 1514, Loss: 0.07621853239834309, Final Batch Loss: 0.020291363820433617\n",
      "Epoch 1515, Loss: 0.06239401362836361, Final Batch Loss: 0.05069638416171074\n",
      "Epoch 1516, Loss: 0.07618952449411154, Final Batch Loss: 0.007514297030866146\n",
      "Epoch 1517, Loss: 0.06023862212896347, Final Batch Loss: 0.04099582880735397\n",
      "Epoch 1518, Loss: 0.06599228549748659, Final Batch Loss: 0.013766472227871418\n",
      "Epoch 1519, Loss: 0.11946733295917511, Final Batch Loss: 0.08296744525432587\n",
      "Epoch 1520, Loss: 0.06390959396958351, Final Batch Loss: 0.04550408199429512\n",
      "Epoch 1521, Loss: 0.08378439396619797, Final Batch Loss: 0.049528155475854874\n",
      "Epoch 1522, Loss: 0.06423201411962509, Final Batch Loss: 0.024391047656536102\n",
      "Epoch 1523, Loss: 0.033659633714705706, Final Batch Loss: 0.007599022705107927\n",
      "Epoch 1524, Loss: 0.03667762316763401, Final Batch Loss: 0.016283372417092323\n",
      "Epoch 1525, Loss: 0.08160902559757233, Final Batch Loss: 0.046561092138290405\n",
      "Epoch 1526, Loss: 0.026904482394456863, Final Batch Loss: 0.015022709965705872\n",
      "Epoch 1527, Loss: 0.028911960311233997, Final Batch Loss: 0.015294955112040043\n",
      "Epoch 1528, Loss: 0.06009025312960148, Final Batch Loss: 0.030931584537029266\n",
      "Epoch 1529, Loss: 0.0316263223066926, Final Batch Loss: 0.005887395702302456\n",
      "Epoch 1530, Loss: 0.07396985217928886, Final Batch Loss: 0.03265560790896416\n",
      "Epoch 1531, Loss: 0.07082816027104855, Final Batch Loss: 0.009328583255410194\n",
      "Epoch 1532, Loss: 0.05211343429982662, Final Batch Loss: 0.03264598548412323\n",
      "Epoch 1533, Loss: 0.0477902851998806, Final Batch Loss: 0.017861833795905113\n",
      "Epoch 1534, Loss: 0.021930241025984287, Final Batch Loss: 0.016584405675530434\n",
      "Epoch 1535, Loss: 0.05058513302356005, Final Batch Loss: 0.01247923169285059\n",
      "Epoch 1536, Loss: 0.04732142202556133, Final Batch Loss: 0.03421662375330925\n",
      "Epoch 1537, Loss: 0.04777849651873112, Final Batch Loss: 0.012168383225798607\n",
      "Epoch 1538, Loss: 0.04082849808037281, Final Batch Loss: 0.01751634292304516\n",
      "Epoch 1539, Loss: 0.04999659862369299, Final Batch Loss: 0.012962478213012218\n",
      "Epoch 1540, Loss: 0.06347891315817833, Final Batch Loss: 0.016528014093637466\n",
      "Epoch 1541, Loss: 0.04267730936408043, Final Batch Loss: 0.010482538491487503\n",
      "Epoch 1542, Loss: 0.042718532495200634, Final Batch Loss: 0.008222616277635098\n",
      "Epoch 1543, Loss: 0.035479553043842316, Final Batch Loss: 0.01040513627231121\n",
      "Epoch 1544, Loss: 0.07928855158388615, Final Batch Loss: 0.018361127004027367\n",
      "Epoch 1545, Loss: 0.040292439982295036, Final Batch Loss: 0.0160115584731102\n",
      "Epoch 1546, Loss: 0.035314030945301056, Final Batch Loss: 0.009691823273897171\n",
      "Epoch 1547, Loss: 0.04719295911490917, Final Batch Loss: 0.022458868101239204\n",
      "Epoch 1548, Loss: 0.049563340842723846, Final Batch Loss: 0.025692997500300407\n",
      "Epoch 1549, Loss: 0.021079767495393753, Final Batch Loss: 0.00999434757977724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1550, Loss: 0.036344519816339016, Final Batch Loss: 0.02441314421594143\n",
      "Epoch 1551, Loss: 0.07972320169210434, Final Batch Loss: 0.031714167445898056\n",
      "Epoch 1552, Loss: 0.07021781802177429, Final Batch Loss: 0.030398711562156677\n",
      "Epoch 1553, Loss: 0.05979379266500473, Final Batch Loss: 0.026058614253997803\n",
      "Epoch 1554, Loss: 0.1049804836511612, Final Batch Loss: 0.06703092157840729\n",
      "Epoch 1555, Loss: 0.07989104278385639, Final Batch Loss: 0.04890010505914688\n",
      "Epoch 1556, Loss: 0.10754718445241451, Final Batch Loss: 0.08530906587839127\n",
      "Epoch 1557, Loss: 0.08127810433506966, Final Batch Loss: 0.038320139050483704\n",
      "Epoch 1558, Loss: 0.04188178572803736, Final Batch Loss: 0.02813900262117386\n",
      "Epoch 1559, Loss: 0.060372259467840195, Final Batch Loss: 0.025822486728429794\n",
      "Epoch 1560, Loss: 0.09071319550275803, Final Batch Loss: 0.03134715557098389\n",
      "Epoch 1561, Loss: 0.05636529251933098, Final Batch Loss: 0.009351026266813278\n",
      "Epoch 1562, Loss: 0.04056085180491209, Final Batch Loss: 0.030496396124362946\n",
      "Epoch 1563, Loss: 0.05179140344262123, Final Batch Loss: 0.029852846637368202\n",
      "Epoch 1564, Loss: 0.046708736568689346, Final Batch Loss: 0.022859200835227966\n",
      "Epoch 1565, Loss: 0.06234043184667826, Final Batch Loss: 0.04910992458462715\n",
      "Epoch 1566, Loss: 0.027272348292171955, Final Batch Loss: 0.011838697828352451\n",
      "Epoch 1567, Loss: 0.051180051639676094, Final Batch Loss: 0.03583277761936188\n",
      "Epoch 1568, Loss: 0.04449211247265339, Final Batch Loss: 0.036219608038663864\n",
      "Epoch 1569, Loss: 0.033143870532512665, Final Batch Loss: 0.020865315571427345\n",
      "Epoch 1570, Loss: 0.060511454939842224, Final Batch Loss: 0.04122740030288696\n",
      "Epoch 1571, Loss: 0.0520236911252141, Final Batch Loss: 0.01526407990604639\n",
      "Epoch 1572, Loss: 0.033394841477274895, Final Batch Loss: 0.018350237980484962\n",
      "Epoch 1573, Loss: 0.05568962171673775, Final Batch Loss: 0.036528851836919785\n",
      "Epoch 1574, Loss: 0.09246435016393661, Final Batch Loss: 0.03370053321123123\n",
      "Epoch 1575, Loss: 0.04033991089090705, Final Batch Loss: 0.03294206038117409\n",
      "Epoch 1576, Loss: 0.012637559324502945, Final Batch Loss: 0.004719586111605167\n",
      "Epoch 1577, Loss: 0.03198734484612942, Final Batch Loss: 0.016851378604769707\n",
      "Epoch 1578, Loss: 0.061192214488983154, Final Batch Loss: 0.04155495762825012\n",
      "Epoch 1579, Loss: 0.07392602227628231, Final Batch Loss: 0.0546121820807457\n",
      "Epoch 1580, Loss: 0.0436359029263258, Final Batch Loss: 0.03152895346283913\n",
      "Epoch 1581, Loss: 0.0767739750444889, Final Batch Loss: 0.022971458733081818\n",
      "Epoch 1582, Loss: 0.013189226388931274, Final Batch Loss: 0.005155240185558796\n",
      "Epoch 1583, Loss: 0.16331851109862328, Final Batch Loss: 0.12738525867462158\n",
      "Epoch 1584, Loss: 0.026324260979890823, Final Batch Loss: 0.015364548191428185\n",
      "Epoch 1585, Loss: 0.08841600827872753, Final Batch Loss: 0.06569322943687439\n",
      "Epoch 1586, Loss: 0.046481527388095856, Final Batch Loss: 0.03780509904026985\n",
      "Epoch 1587, Loss: 0.06954596750438213, Final Batch Loss: 0.026011725887656212\n",
      "Epoch 1588, Loss: 0.07911215163767338, Final Batch Loss: 0.053364552557468414\n",
      "Epoch 1589, Loss: 0.05336957238614559, Final Batch Loss: 0.026391856372356415\n",
      "Epoch 1590, Loss: 0.07862589508295059, Final Batch Loss: 0.03465353697538376\n",
      "Epoch 1591, Loss: 0.04210162162780762, Final Batch Loss: 0.020137926563620567\n",
      "Epoch 1592, Loss: 0.03198551945388317, Final Batch Loss: 0.008148841559886932\n",
      "Epoch 1593, Loss: 0.06242172047495842, Final Batch Loss: 0.016667921096086502\n",
      "Epoch 1594, Loss: 0.02945022750645876, Final Batch Loss: 0.008702474646270275\n",
      "Epoch 1595, Loss: 0.02842158079147339, Final Batch Loss: 0.015348865650594234\n",
      "Epoch 1596, Loss: 0.08873383700847626, Final Batch Loss: 0.059502582997083664\n",
      "Epoch 1597, Loss: 0.07212561927735806, Final Batch Loss: 0.047477807849645615\n",
      "Epoch 1598, Loss: 0.01649515377357602, Final Batch Loss: 0.003437431063503027\n",
      "Epoch 1599, Loss: 0.03787733614444733, Final Batch Loss: 0.009244659915566444\n",
      "Epoch 1600, Loss: 0.03406558185815811, Final Batch Loss: 0.008812444284558296\n",
      "Epoch 1601, Loss: 0.033057138323783875, Final Batch Loss: 0.009703898802399635\n",
      "Epoch 1602, Loss: 0.03315223567187786, Final Batch Loss: 0.021846728399395943\n",
      "Epoch 1603, Loss: 0.052740663290023804, Final Batch Loss: 0.01736747846007347\n",
      "Epoch 1604, Loss: 0.10719024762511253, Final Batch Loss: 0.05054767057299614\n",
      "Epoch 1605, Loss: 0.03306465037167072, Final Batch Loss: 0.006191899999976158\n",
      "Epoch 1606, Loss: 0.044140728656202555, Final Batch Loss: 0.007673064712435007\n",
      "Epoch 1607, Loss: 0.044069875963032246, Final Batch Loss: 0.01442000363022089\n",
      "Epoch 1608, Loss: 0.03656357619911432, Final Batch Loss: 0.029020588845014572\n",
      "Epoch 1609, Loss: 0.051759907975792885, Final Batch Loss: 0.01991920731961727\n",
      "Epoch 1610, Loss: 0.0873621329665184, Final Batch Loss: 0.07487662881612778\n",
      "Epoch 1611, Loss: 0.05644653597846627, Final Batch Loss: 0.04891975224018097\n",
      "Epoch 1612, Loss: 0.12007802538573742, Final Batch Loss: 0.03090572915971279\n",
      "Epoch 1613, Loss: 0.06751182861626148, Final Batch Loss: 0.04781278222799301\n",
      "Epoch 1614, Loss: 0.050575677771121264, Final Batch Loss: 0.00522697763517499\n",
      "Epoch 1615, Loss: 0.07064324244856834, Final Batch Loss: 0.03378042206168175\n",
      "Epoch 1616, Loss: 0.1271454207599163, Final Batch Loss: 0.04076702520251274\n",
      "Epoch 1617, Loss: 0.025370551273226738, Final Batch Loss: 0.014249920845031738\n",
      "Epoch 1618, Loss: 0.06667541339993477, Final Batch Loss: 0.011203475296497345\n",
      "Epoch 1619, Loss: 0.021029594354331493, Final Batch Loss: 0.007403232157230377\n",
      "Epoch 1620, Loss: 0.09959820657968521, Final Batch Loss: 0.029802724719047546\n",
      "Epoch 1621, Loss: 0.08366362564265728, Final Batch Loss: 0.05347904562950134\n",
      "Epoch 1622, Loss: 0.06015065498650074, Final Batch Loss: 0.03165007755160332\n",
      "Epoch 1623, Loss: 0.0713079608976841, Final Batch Loss: 0.039229363203048706\n",
      "Epoch 1624, Loss: 0.037721545435488224, Final Batch Loss: 0.026185641065239906\n",
      "Epoch 1625, Loss: 0.061185574159026146, Final Batch Loss: 0.03311238810420036\n",
      "Epoch 1626, Loss: 0.08294201456010342, Final Batch Loss: 0.0659409686923027\n",
      "Epoch 1627, Loss: 0.023324979469180107, Final Batch Loss: 0.014112191274762154\n",
      "Epoch 1628, Loss: 0.05715675465762615, Final Batch Loss: 0.032809119671583176\n",
      "Epoch 1629, Loss: 0.02525109238922596, Final Batch Loss: 0.0075387489050626755\n",
      "Epoch 1630, Loss: 0.03658757172524929, Final Batch Loss: 0.011473840102553368\n",
      "Epoch 1631, Loss: 0.10061701759696007, Final Batch Loss: 0.06914101541042328\n",
      "Epoch 1632, Loss: 0.06122281029820442, Final Batch Loss: 0.04539097845554352\n",
      "Epoch 1633, Loss: 0.05720004253089428, Final Batch Loss: 0.0380072258412838\n",
      "Epoch 1634, Loss: 0.09148025512695312, Final Batch Loss: 0.02166086435317993\n",
      "Epoch 1635, Loss: 0.07600917108356953, Final Batch Loss: 0.05074580758810043\n",
      "Epoch 1636, Loss: 0.0779501385986805, Final Batch Loss: 0.04582930728793144\n",
      "Epoch 1637, Loss: 0.060779910534620285, Final Batch Loss: 0.029152605682611465\n",
      "Epoch 1638, Loss: 0.06627990305423737, Final Batch Loss: 0.034137941896915436\n",
      "Epoch 1639, Loss: 0.042074453085660934, Final Batch Loss: 0.015886850655078888\n",
      "Epoch 1640, Loss: 0.08848227374255657, Final Batch Loss: 0.029615672305226326\n",
      "Epoch 1641, Loss: 0.06678541749715805, Final Batch Loss: 0.02292831614613533\n",
      "Epoch 1642, Loss: 0.057745231315493584, Final Batch Loss: 0.03309989720582962\n",
      "Epoch 1643, Loss: 0.05591299198567867, Final Batch Loss: 0.014286475256085396\n",
      "Epoch 1644, Loss: 0.07309253700077534, Final Batch Loss: 0.06080443784594536\n",
      "Epoch 1645, Loss: 0.04107136372476816, Final Batch Loss: 0.010564214549958706\n",
      "Epoch 1646, Loss: 0.049106523394584656, Final Batch Loss: 0.032719116657972336\n",
      "Epoch 1647, Loss: 0.05982411280274391, Final Batch Loss: 0.038858067244291306\n",
      "Epoch 1648, Loss: 0.06771530210971832, Final Batch Loss: 0.03337215259671211\n",
      "Epoch 1649, Loss: 0.03990268800407648, Final Batch Loss: 0.014466661028563976\n",
      "Epoch 1650, Loss: 0.058125074952840805, Final Batch Loss: 0.04724932089447975\n",
      "Epoch 1651, Loss: 0.12306002900004387, Final Batch Loss: 0.11131425946950912\n",
      "Epoch 1652, Loss: 0.03246214799582958, Final Batch Loss: 0.01642601378262043\n",
      "Epoch 1653, Loss: 0.08622798323631287, Final Batch Loss: 0.04555152356624603\n",
      "Epoch 1654, Loss: 0.04767224378883839, Final Batch Loss: 0.012709559872746468\n",
      "Epoch 1655, Loss: 0.05859011318534613, Final Batch Loss: 0.047932110726833344\n",
      "Epoch 1656, Loss: 0.030420808121562004, Final Batch Loss: 0.006355199962854385\n",
      "Epoch 1657, Loss: 0.03586583584547043, Final Batch Loss: 0.016105378046631813\n",
      "Epoch 1658, Loss: 0.030558230355381966, Final Batch Loss: 0.010365618392825127\n",
      "Epoch 1659, Loss: 0.07528126239776611, Final Batch Loss: 0.03312993794679642\n",
      "Epoch 1660, Loss: 0.064154296182096, Final Batch Loss: 0.013897652737796307\n",
      "Epoch 1661, Loss: 0.058753824327141047, Final Batch Loss: 0.051118504256010056\n",
      "Epoch 1662, Loss: 0.060001928359270096, Final Batch Loss: 0.04803307726979256\n",
      "Epoch 1663, Loss: 0.06171225570142269, Final Batch Loss: 0.021750448271632195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1664, Loss: 0.0862607080489397, Final Batch Loss: 0.06717758625745773\n",
      "Epoch 1665, Loss: 0.0759602403268218, Final Batch Loss: 0.006745814345777035\n",
      "Epoch 1666, Loss: 0.03249599598348141, Final Batch Loss: 0.015822602435946465\n",
      "Epoch 1667, Loss: 0.06680764630436897, Final Batch Loss: 0.013763591647148132\n",
      "Epoch 1668, Loss: 0.09313928708434105, Final Batch Loss: 0.0472351498901844\n",
      "Epoch 1669, Loss: 0.05187737196683884, Final Batch Loss: 0.021226858720183372\n",
      "Epoch 1670, Loss: 0.040497008711099625, Final Batch Loss: 0.013082072138786316\n",
      "Epoch 1671, Loss: 0.044446623884141445, Final Batch Loss: 0.015525023452937603\n",
      "Epoch 1672, Loss: 0.07188964635133743, Final Batch Loss: 0.04691430926322937\n",
      "Epoch 1673, Loss: 0.049513560719788074, Final Batch Loss: 0.03991258144378662\n",
      "Epoch 1674, Loss: 0.05849255062639713, Final Batch Loss: 0.027027389034628868\n",
      "Epoch 1675, Loss: 0.04705044161528349, Final Batch Loss: 0.007928810082376003\n",
      "Epoch 1676, Loss: 0.06776389479637146, Final Batch Loss: 0.03462086245417595\n",
      "Epoch 1677, Loss: 0.0465857544913888, Final Batch Loss: 0.013795611448585987\n",
      "Epoch 1678, Loss: 0.04551788233220577, Final Batch Loss: 0.013221023604273796\n",
      "Epoch 1679, Loss: 0.07508473470807076, Final Batch Loss: 0.04545004665851593\n",
      "Epoch 1680, Loss: 0.044191169552505016, Final Batch Loss: 0.011606450192630291\n",
      "Epoch 1681, Loss: 0.02236153371632099, Final Batch Loss: 0.009099948219954967\n",
      "Epoch 1682, Loss: 0.0435765627771616, Final Batch Loss: 0.025937477126717567\n",
      "Epoch 1683, Loss: 0.08295603469014168, Final Batch Loss: 0.049571968615055084\n",
      "Epoch 1684, Loss: 0.06600964814424515, Final Batch Loss: 0.03513895347714424\n",
      "Epoch 1685, Loss: 0.043523430824279785, Final Batch Loss: 0.03371370956301689\n",
      "Epoch 1686, Loss: 0.029148458503186703, Final Batch Loss: 0.021268213167786598\n",
      "Epoch 1687, Loss: 0.06663094274699688, Final Batch Loss: 0.02339593507349491\n",
      "Epoch 1688, Loss: 0.043957119807600975, Final Batch Loss: 0.03510105609893799\n",
      "Epoch 1689, Loss: 0.028422588482499123, Final Batch Loss: 0.010442165657877922\n",
      "Epoch 1690, Loss: 0.06625027768313885, Final Batch Loss: 0.027225462719798088\n",
      "Epoch 1691, Loss: 0.029418363235890865, Final Batch Loss: 0.013030714355409145\n",
      "Epoch 1692, Loss: 0.04143738001585007, Final Batch Loss: 0.02892219088971615\n",
      "Epoch 1693, Loss: 0.0564127080142498, Final Batch Loss: 0.025838879868388176\n",
      "Epoch 1694, Loss: 0.025494382716715336, Final Batch Loss: 0.004632697440683842\n",
      "Epoch 1695, Loss: 0.03536873031407595, Final Batch Loss: 0.026431094855070114\n",
      "Epoch 1696, Loss: 0.055740002542734146, Final Batch Loss: 0.02198721468448639\n",
      "Epoch 1697, Loss: 0.030641944613307714, Final Batch Loss: 0.0056562344543635845\n",
      "Epoch 1698, Loss: 0.059353276155889034, Final Batch Loss: 0.04819890111684799\n",
      "Epoch 1699, Loss: 0.033546787686645985, Final Batch Loss: 0.012649999000132084\n",
      "Epoch 1700, Loss: 0.035628029610961676, Final Batch Loss: 0.006156811024993658\n",
      "Epoch 1701, Loss: 0.025802995543926954, Final Batch Loss: 0.01924760825932026\n",
      "Epoch 1702, Loss: 0.027823654003441334, Final Batch Loss: 0.003964615054428577\n",
      "Epoch 1703, Loss: 0.020753683056682348, Final Batch Loss: 0.005987839307636023\n",
      "Epoch 1704, Loss: 0.032773735001683235, Final Batch Loss: 0.01700432598590851\n",
      "Epoch 1705, Loss: 0.04879788402467966, Final Batch Loss: 0.03637551888823509\n",
      "Epoch 1706, Loss: 0.046782735735177994, Final Batch Loss: 0.03521233797073364\n",
      "Epoch 1707, Loss: 0.023276757448911667, Final Batch Loss: 0.01311195082962513\n",
      "Epoch 1708, Loss: 0.014928982593119144, Final Batch Loss: 0.006079257465898991\n",
      "Epoch 1709, Loss: 0.07094598095864058, Final Batch Loss: 0.014886713586747646\n",
      "Epoch 1710, Loss: 0.06067011132836342, Final Batch Loss: 0.028052281588315964\n",
      "Epoch 1711, Loss: 0.029677677899599075, Final Batch Loss: 0.012414379045367241\n",
      "Epoch 1712, Loss: 0.10828148992732167, Final Batch Loss: 0.10170716047286987\n",
      "Epoch 1713, Loss: 0.029534067027270794, Final Batch Loss: 0.018628109246492386\n",
      "Epoch 1714, Loss: 0.05215216614305973, Final Batch Loss: 0.017908433452248573\n",
      "Epoch 1715, Loss: 0.05320249870419502, Final Batch Loss: 0.03354392573237419\n",
      "Epoch 1716, Loss: 0.04025613283738494, Final Batch Loss: 0.03437414765357971\n",
      "Epoch 1717, Loss: 0.02709200419485569, Final Batch Loss: 0.004921033978462219\n",
      "Epoch 1718, Loss: 0.05225098505616188, Final Batch Loss: 0.028065284714102745\n",
      "Epoch 1719, Loss: 0.041571779642254114, Final Batch Loss: 0.03760742396116257\n",
      "Epoch 1720, Loss: 0.021081585437059402, Final Batch Loss: 0.012854804284870625\n",
      "Epoch 1721, Loss: 0.04221676615998149, Final Batch Loss: 0.03549007698893547\n",
      "Epoch 1722, Loss: 0.03784421272575855, Final Batch Loss: 0.021118611097335815\n",
      "Epoch 1723, Loss: 0.036349750123918056, Final Batch Loss: 0.025096049532294273\n",
      "Epoch 1724, Loss: 0.05226029455661774, Final Batch Loss: 0.008806992322206497\n",
      "Epoch 1725, Loss: 0.027311822399497032, Final Batch Loss: 0.013434463180601597\n",
      "Epoch 1726, Loss: 0.05701604951173067, Final Batch Loss: 0.04461842402815819\n",
      "Epoch 1727, Loss: 0.06523734517395496, Final Batch Loss: 0.011558862403035164\n",
      "Epoch 1728, Loss: 0.03171371482312679, Final Batch Loss: 0.021352602168917656\n",
      "Epoch 1729, Loss: 0.04374176822602749, Final Batch Loss: 0.018579082563519478\n",
      "Epoch 1730, Loss: 0.056002940982580185, Final Batch Loss: 0.03344571590423584\n",
      "Epoch 1731, Loss: 0.05425601452589035, Final Batch Loss: 0.026501698419451714\n",
      "Epoch 1732, Loss: 0.0713027436286211, Final Batch Loss: 0.047705475240945816\n",
      "Epoch 1733, Loss: 0.0619584284722805, Final Batch Loss: 0.0210927352309227\n",
      "Epoch 1734, Loss: 0.09880263730883598, Final Batch Loss: 0.06686364859342575\n",
      "Epoch 1735, Loss: 0.037546396255493164, Final Batch Loss: 0.017708683386445045\n",
      "Epoch 1736, Loss: 0.01642721425741911, Final Batch Loss: 0.010414309799671173\n",
      "Epoch 1737, Loss: 0.0770617127418518, Final Batch Loss: 0.05802977830171585\n",
      "Epoch 1738, Loss: 0.04106776416301727, Final Batch Loss: 0.026372559368610382\n",
      "Epoch 1739, Loss: 0.054668808821588755, Final Batch Loss: 0.04888680949807167\n",
      "Epoch 1740, Loss: 0.0648261858150363, Final Batch Loss: 0.059401754289865494\n",
      "Epoch 1741, Loss: 0.05348328314721584, Final Batch Loss: 0.03335190564393997\n",
      "Epoch 1742, Loss: 0.03143424354493618, Final Batch Loss: 0.014975128695368767\n",
      "Epoch 1743, Loss: 0.04111272469162941, Final Batch Loss: 0.024153651669621468\n",
      "Epoch 1744, Loss: 0.056509796530008316, Final Batch Loss: 0.03500725328922272\n",
      "Epoch 1745, Loss: 0.06650268286466599, Final Batch Loss: 0.02574605494737625\n",
      "Epoch 1746, Loss: 0.04154791170731187, Final Batch Loss: 0.03623608127236366\n",
      "Epoch 1747, Loss: 0.03438443876802921, Final Batch Loss: 0.006690260022878647\n",
      "Epoch 1748, Loss: 0.07135321944952011, Final Batch Loss: 0.044657934457063675\n",
      "Epoch 1749, Loss: 0.05454598367214203, Final Batch Loss: 0.027312178164720535\n",
      "Epoch 1750, Loss: 0.04080839175730944, Final Batch Loss: 0.026384569704532623\n",
      "Epoch 1751, Loss: 0.020860645454376936, Final Batch Loss: 0.005437124986201525\n",
      "Epoch 1752, Loss: 0.07470597513020039, Final Batch Loss: 0.06013305112719536\n",
      "Epoch 1753, Loss: 0.07077533565461636, Final Batch Loss: 0.01930423267185688\n",
      "Epoch 1754, Loss: 0.050384458154439926, Final Batch Loss: 0.03390417620539665\n",
      "Epoch 1755, Loss: 0.09043633192777634, Final Batch Loss: 0.032443851232528687\n",
      "Epoch 1756, Loss: 0.023804688826203346, Final Batch Loss: 0.01303937379270792\n",
      "Epoch 1757, Loss: 0.029803684912621975, Final Batch Loss: 0.011009817011654377\n",
      "Epoch 1758, Loss: 0.05908938683569431, Final Batch Loss: 0.038170091807842255\n",
      "Epoch 1759, Loss: 0.026437715627253056, Final Batch Loss: 0.006472296081483364\n",
      "Epoch 1760, Loss: 0.02866291720420122, Final Batch Loss: 0.011361387558281422\n",
      "Epoch 1761, Loss: 0.04610404372215271, Final Batch Loss: 0.02188895456492901\n",
      "Epoch 1762, Loss: 0.046656977385282516, Final Batch Loss: 0.030991559848189354\n",
      "Epoch 1763, Loss: 0.02372698113322258, Final Batch Loss: 0.010116375051438808\n",
      "Epoch 1764, Loss: 0.03407280705869198, Final Batch Loss: 0.016751235350966454\n",
      "Epoch 1765, Loss: 0.05209095496684313, Final Batch Loss: 0.040335170924663544\n",
      "Epoch 1766, Loss: 0.02644707914441824, Final Batch Loss: 0.012772629037499428\n",
      "Epoch 1767, Loss: 0.02203504228964448, Final Batch Loss: 0.005974639672785997\n",
      "Epoch 1768, Loss: 0.056797731667757034, Final Batch Loss: 0.03342925012111664\n",
      "Epoch 1769, Loss: 0.09878569468855858, Final Batch Loss: 0.07927438616752625\n",
      "Epoch 1770, Loss: 0.03270502761006355, Final Batch Loss: 0.02211436629295349\n",
      "Epoch 1771, Loss: 0.04126029461622238, Final Batch Loss: 0.024327432736754417\n",
      "Epoch 1772, Loss: 0.035538798198103905, Final Batch Loss: 0.01671665720641613\n",
      "Epoch 1773, Loss: 0.04747552238404751, Final Batch Loss: 0.026292672380805016\n",
      "Epoch 1774, Loss: 0.07155550457537174, Final Batch Loss: 0.05288376286625862\n",
      "Epoch 1775, Loss: 0.05106315389275551, Final Batch Loss: 0.011472579091787338\n",
      "Epoch 1776, Loss: 0.03800936695188284, Final Batch Loss: 0.029002947732806206\n",
      "Epoch 1777, Loss: 0.04636596329510212, Final Batch Loss: 0.01639622449874878\n",
      "Epoch 1778, Loss: 0.03426503110677004, Final Batch Loss: 0.012688632123172283\n",
      "Epoch 1779, Loss: 0.07847764156758785, Final Batch Loss: 0.0600552000105381\n",
      "Epoch 1780, Loss: 0.041137492284178734, Final Batch Loss: 0.016427000984549522\n",
      "Epoch 1781, Loss: 0.05701574916020036, Final Batch Loss: 0.005637014750391245\n",
      "Epoch 1782, Loss: 0.03509751334786415, Final Batch Loss: 0.02211025170981884\n",
      "Epoch 1783, Loss: 0.027977539226412773, Final Batch Loss: 0.015176174230873585\n",
      "Epoch 1784, Loss: 0.036331760697066784, Final Batch Loss: 0.02841184474527836\n",
      "Epoch 1785, Loss: 0.05721386894583702, Final Batch Loss: 0.016729731112718582\n",
      "Epoch 1786, Loss: 0.030649414286017418, Final Batch Loss: 0.02001073583960533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1787, Loss: 0.056044161319732666, Final Batch Loss: 0.03076733462512493\n",
      "Epoch 1788, Loss: 0.0176812456920743, Final Batch Loss: 0.009215284138917923\n",
      "Epoch 1789, Loss: 0.024690779857337475, Final Batch Loss: 0.015807202085852623\n",
      "Epoch 1790, Loss: 0.0916117150336504, Final Batch Loss: 0.02561512030661106\n",
      "Epoch 1791, Loss: 0.04064899682998657, Final Batch Loss: 0.016568869352340698\n",
      "Epoch 1792, Loss: 0.09342272393405437, Final Batch Loss: 0.06691975891590118\n",
      "Epoch 1793, Loss: 0.054658740758895874, Final Batch Loss: 0.033400893211364746\n",
      "Epoch 1794, Loss: 0.1132008247077465, Final Batch Loss: 0.07307905703783035\n",
      "Epoch 1795, Loss: 0.04715414997190237, Final Batch Loss: 0.03523402661085129\n",
      "Epoch 1796, Loss: 0.05877150222659111, Final Batch Loss: 0.009508494287729263\n",
      "Epoch 1797, Loss: 0.04145784303545952, Final Batch Loss: 0.009106945246458054\n",
      "Epoch 1798, Loss: 0.13224756345152855, Final Batch Loss: 0.025403674691915512\n",
      "Epoch 1799, Loss: 0.01286576734855771, Final Batch Loss: 0.007353710010647774\n",
      "Epoch 1800, Loss: 0.08685997501015663, Final Batch Loss: 0.06006249412894249\n",
      "Epoch 1801, Loss: 0.04915216006338596, Final Batch Loss: 0.019068803638219833\n",
      "Epoch 1802, Loss: 0.10886905808001757, Final Batch Loss: 0.09770944714546204\n",
      "Epoch 1803, Loss: 0.03440652787685394, Final Batch Loss: 0.02510135807096958\n",
      "Epoch 1804, Loss: 0.20978017151355743, Final Batch Loss: 0.1405039280653\n",
      "Epoch 1805, Loss: 0.037398796528577805, Final Batch Loss: 0.030672265216708183\n",
      "Epoch 1806, Loss: 0.13105682283639908, Final Batch Loss: 0.053689636290073395\n",
      "Epoch 1807, Loss: 0.05816544033586979, Final Batch Loss: 0.05125516280531883\n",
      "Epoch 1808, Loss: 0.17070499807596207, Final Batch Loss: 0.07290273904800415\n",
      "Epoch 1809, Loss: 0.17940369248390198, Final Batch Loss: 0.005853533744812012\n",
      "Epoch 1810, Loss: 0.08353852014988661, Final Batch Loss: 0.07029746472835541\n",
      "Epoch 1811, Loss: 0.09210941940546036, Final Batch Loss: 0.04022804647684097\n",
      "Epoch 1812, Loss: 0.081801388412714, Final Batch Loss: 0.05976910889148712\n",
      "Epoch 1813, Loss: 0.030453613959252834, Final Batch Loss: 0.017934124916791916\n",
      "Epoch 1814, Loss: 0.033769525587558746, Final Batch Loss: 0.018646936863660812\n",
      "Epoch 1815, Loss: 0.03729430679231882, Final Batch Loss: 0.008890322409570217\n",
      "Epoch 1816, Loss: 0.06711016409099102, Final Batch Loss: 0.022838326171040535\n",
      "Epoch 1817, Loss: 0.12997815571725368, Final Batch Loss: 0.11538630723953247\n",
      "Epoch 1818, Loss: 0.06317921355366707, Final Batch Loss: 0.02397928386926651\n",
      "Epoch 1819, Loss: 0.07659757789224386, Final Batch Loss: 0.00721540953963995\n",
      "Epoch 1820, Loss: 0.052552077919244766, Final Batch Loss: 0.024280469864606857\n",
      "Epoch 1821, Loss: 0.02525881165638566, Final Batch Loss: 0.0077040367759764194\n",
      "Epoch 1822, Loss: 0.11659455671906471, Final Batch Loss: 0.07639393210411072\n",
      "Epoch 1823, Loss: 0.03782385680824518, Final Batch Loss: 0.0061063701286911964\n",
      "Epoch 1824, Loss: 0.07979442924261093, Final Batch Loss: 0.039329081773757935\n",
      "Epoch 1825, Loss: 0.1390576809644699, Final Batch Loss: 0.06863025575876236\n",
      "Epoch 1826, Loss: 0.03745459113270044, Final Batch Loss: 0.013150705955922604\n",
      "Epoch 1827, Loss: 0.028050221502780914, Final Batch Loss: 0.01303701102733612\n",
      "Epoch 1828, Loss: 0.04444150160998106, Final Batch Loss: 0.010348892770707607\n",
      "Epoch 1829, Loss: 0.0558248208835721, Final Batch Loss: 0.010555882938206196\n",
      "Epoch 1830, Loss: 0.0394366355612874, Final Batch Loss: 0.008697164244949818\n",
      "Epoch 1831, Loss: 0.05814184062182903, Final Batch Loss: 0.033202964812517166\n",
      "Epoch 1832, Loss: 0.04630507156252861, Final Batch Loss: 0.027778184041380882\n",
      "Epoch 1833, Loss: 0.0416974313557148, Final Batch Loss: 0.013967553153634071\n",
      "Epoch 1834, Loss: 0.08060872182250023, Final Batch Loss: 0.0656685084104538\n",
      "Epoch 1835, Loss: 0.092860983684659, Final Batch Loss: 0.06657739728689194\n",
      "Epoch 1836, Loss: 0.040553102269768715, Final Batch Loss: 0.02573506534099579\n",
      "Epoch 1837, Loss: 0.02682653721421957, Final Batch Loss: 0.014035087078809738\n",
      "Epoch 1838, Loss: 0.058637578040361404, Final Batch Loss: 0.034908682107925415\n",
      "Epoch 1839, Loss: 0.07737402617931366, Final Batch Loss: 0.041668426245450974\n",
      "Epoch 1840, Loss: 0.04859452974051237, Final Batch Loss: 0.03697557374835014\n",
      "Epoch 1841, Loss: 0.09532391652464867, Final Batch Loss: 0.029818017035722733\n",
      "Epoch 1842, Loss: 0.04467381536960602, Final Batch Loss: 0.02273399382829666\n",
      "Epoch 1843, Loss: 0.03518249001353979, Final Batch Loss: 0.011914492584764957\n",
      "Epoch 1844, Loss: 0.047351766377687454, Final Batch Loss: 0.027077918872237206\n",
      "Epoch 1845, Loss: 0.011373179033398628, Final Batch Loss: 0.004140364937484264\n",
      "Epoch 1846, Loss: 0.06608244311064482, Final Batch Loss: 0.010235882364213467\n",
      "Epoch 1847, Loss: 0.06761921662837267, Final Batch Loss: 0.056615158915519714\n",
      "Epoch 1848, Loss: 0.06096761301159859, Final Batch Loss: 0.018958784639835358\n",
      "Epoch 1849, Loss: 0.047381442040205, Final Batch Loss: 0.020710226148366928\n",
      "Epoch 1850, Loss: 0.02801963873207569, Final Batch Loss: 0.011978920549154282\n",
      "Epoch 1851, Loss: 0.04053885582834482, Final Batch Loss: 0.029106399044394493\n",
      "Epoch 1852, Loss: 0.023866640403866768, Final Batch Loss: 0.015110275708138943\n",
      "Epoch 1853, Loss: 0.052940504625439644, Final Batch Loss: 0.03253377974033356\n",
      "Epoch 1854, Loss: 0.05496903508901596, Final Batch Loss: 0.017582453787326813\n",
      "Epoch 1855, Loss: 0.04752263240516186, Final Batch Loss: 0.01746246963739395\n",
      "Epoch 1856, Loss: 0.021817296743392944, Final Batch Loss: 0.014955981634557247\n",
      "Epoch 1857, Loss: 0.024995163083076477, Final Batch Loss: 0.014698311686515808\n",
      "Epoch 1858, Loss: 0.06406743451952934, Final Batch Loss: 0.05246419832110405\n",
      "Epoch 1859, Loss: 0.04681817442178726, Final Batch Loss: 0.012980114668607712\n",
      "Epoch 1860, Loss: 0.03010517545044422, Final Batch Loss: 0.020331580191850662\n",
      "Epoch 1861, Loss: 0.048120190389454365, Final Batch Loss: 0.010974233038723469\n",
      "Epoch 1862, Loss: 0.132821187376976, Final Batch Loss: 0.09135694801807404\n",
      "Epoch 1863, Loss: 0.07854153960943222, Final Batch Loss: 0.03328122943639755\n",
      "Epoch 1864, Loss: 0.030208290554583073, Final Batch Loss: 0.00989763718098402\n",
      "Epoch 1865, Loss: 0.03644164651632309, Final Batch Loss: 0.009093662723898888\n",
      "Epoch 1866, Loss: 0.06144579732790589, Final Batch Loss: 0.007576285395771265\n",
      "Epoch 1867, Loss: 0.07244973629713058, Final Batch Loss: 0.030504651367664337\n",
      "Epoch 1868, Loss: 0.1336274640634656, Final Batch Loss: 0.12179868668317795\n",
      "Epoch 1869, Loss: 0.08862713351845741, Final Batch Loss: 0.056742772459983826\n",
      "Epoch 1870, Loss: 0.08671045862138271, Final Batch Loss: 0.027774667367339134\n",
      "Epoch 1871, Loss: 0.07842054590582848, Final Batch Loss: 0.0434943288564682\n",
      "Epoch 1872, Loss: 0.036643872037529945, Final Batch Loss: 0.011743517592549324\n",
      "Epoch 1873, Loss: 0.06399259343743324, Final Batch Loss: 0.02867111563682556\n",
      "Epoch 1874, Loss: 0.07065089792013168, Final Batch Loss: 0.031427472829818726\n",
      "Epoch 1875, Loss: 0.06820430606603622, Final Batch Loss: 0.04797707498073578\n",
      "Epoch 1876, Loss: 0.030872788280248642, Final Batch Loss: 0.006234664469957352\n",
      "Epoch 1877, Loss: 0.03587981127202511, Final Batch Loss: 0.018418580293655396\n",
      "Epoch 1878, Loss: 0.04037544596940279, Final Batch Loss: 0.011444720439612865\n",
      "Epoch 1879, Loss: 0.06304511614143848, Final Batch Loss: 0.026281123980879784\n",
      "Epoch 1880, Loss: 0.045376554131507874, Final Batch Loss: 0.01809384673833847\n",
      "Epoch 1881, Loss: 0.03388755489140749, Final Batch Loss: 0.013306482695043087\n",
      "Epoch 1882, Loss: 0.027763735502958298, Final Batch Loss: 0.017970113083720207\n",
      "Epoch 1883, Loss: 0.021721716038882732, Final Batch Loss: 0.01491714920848608\n",
      "Epoch 1884, Loss: 0.0390463606454432, Final Batch Loss: 0.006935013923794031\n",
      "Epoch 1885, Loss: 0.01858454430475831, Final Batch Loss: 0.004835416097193956\n",
      "Epoch 1886, Loss: 0.04150819033384323, Final Batch Loss: 0.009654484689235687\n",
      "Epoch 1887, Loss: 0.029728423804044724, Final Batch Loss: 0.006716925650835037\n",
      "Epoch 1888, Loss: 0.014119431376457214, Final Batch Loss: 0.007199632935225964\n",
      "Epoch 1889, Loss: 0.028501836583018303, Final Batch Loss: 0.01791672222316265\n",
      "Epoch 1890, Loss: 0.06627273187041283, Final Batch Loss: 0.05713579058647156\n",
      "Epoch 1891, Loss: 0.026212268508970737, Final Batch Loss: 0.017652535811066628\n",
      "Epoch 1892, Loss: 0.020349552854895592, Final Batch Loss: 0.009409362450242043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1893, Loss: 0.033582736272364855, Final Batch Loss: 0.007475151214748621\n",
      "Epoch 1894, Loss: 0.03116279374808073, Final Batch Loss: 0.01546595897525549\n",
      "Epoch 1895, Loss: 0.04059465974569321, Final Batch Loss: 0.031228872016072273\n",
      "Epoch 1896, Loss: 0.036331845447421074, Final Batch Loss: 0.016283199191093445\n",
      "Epoch 1897, Loss: 0.018320119474083185, Final Batch Loss: 0.007482216227799654\n",
      "Epoch 1898, Loss: 0.017677420284599066, Final Batch Loss: 0.012580065988004208\n",
      "Epoch 1899, Loss: 0.03438073070719838, Final Batch Loss: 0.0061857146210968494\n",
      "Epoch 1900, Loss: 0.03137475298717618, Final Batch Loss: 0.026206159964203835\n",
      "Epoch 1901, Loss: 0.04091409593820572, Final Batch Loss: 0.03436208888888359\n",
      "Epoch 1902, Loss: 0.0655628852546215, Final Batch Loss: 0.04308805242180824\n",
      "Epoch 1903, Loss: 0.059225499629974365, Final Batch Loss: 0.010272644460201263\n",
      "Epoch 1904, Loss: 0.05787972919642925, Final Batch Loss: 0.025218045338988304\n",
      "Epoch 1905, Loss: 0.017240147106349468, Final Batch Loss: 0.009427925571799278\n",
      "Epoch 1906, Loss: 0.03430125489830971, Final Batch Loss: 0.016446253284811974\n",
      "Epoch 1907, Loss: 0.03148909565061331, Final Batch Loss: 0.01211437676101923\n",
      "Epoch 1908, Loss: 0.025993595365434885, Final Batch Loss: 0.00653446139767766\n",
      "Epoch 1909, Loss: 0.02920115552842617, Final Batch Loss: 0.015719298273324966\n",
      "Epoch 1910, Loss: 0.015753896441310644, Final Batch Loss: 0.007237739395350218\n",
      "Epoch 1911, Loss: 0.045694601722061634, Final Batch Loss: 0.014575068838894367\n",
      "Epoch 1912, Loss: 0.034953526221215725, Final Batch Loss: 0.008893229998648167\n",
      "Epoch 1913, Loss: 0.01722763665020466, Final Batch Loss: 0.005299745127558708\n",
      "Epoch 1914, Loss: 0.039630454033613205, Final Batch Loss: 0.02815104089677334\n",
      "Epoch 1915, Loss: 0.028161740861833096, Final Batch Loss: 0.0079226428642869\n",
      "Epoch 1916, Loss: 0.0563691221177578, Final Batch Loss: 0.014156799763441086\n",
      "Epoch 1917, Loss: 0.01968732662498951, Final Batch Loss: 0.007963445037603378\n",
      "Epoch 1918, Loss: 0.029752413742244244, Final Batch Loss: 0.014894741587340832\n",
      "Epoch 1919, Loss: 0.02574666030704975, Final Batch Loss: 0.017800940200686455\n",
      "Epoch 1920, Loss: 0.04102905094623566, Final Batch Loss: 0.01049002818763256\n",
      "Epoch 1921, Loss: 0.03275810228660703, Final Batch Loss: 0.006446604151278734\n",
      "Epoch 1922, Loss: 0.019477101042866707, Final Batch Loss: 0.007754351012408733\n",
      "Epoch 1923, Loss: 0.06043359078466892, Final Batch Loss: 0.045035094022750854\n",
      "Epoch 1924, Loss: 0.040663253515958786, Final Batch Loss: 0.03345777466893196\n",
      "Epoch 1925, Loss: 0.08220822736620903, Final Batch Loss: 0.061068207025527954\n",
      "Epoch 1926, Loss: 0.093782102689147, Final Batch Loss: 0.06364195048809052\n",
      "Epoch 1927, Loss: 0.049717807210981846, Final Batch Loss: 0.011161440052092075\n",
      "Epoch 1928, Loss: 0.05073716538026929, Final Batch Loss: 0.0045121838338673115\n",
      "Epoch 1929, Loss: 0.040924156550318, Final Batch Loss: 0.006709190551191568\n",
      "Epoch 1930, Loss: 0.016920013818889856, Final Batch Loss: 0.006158839445561171\n",
      "Epoch 1931, Loss: 0.02037164568901062, Final Batch Loss: 0.010606763884425163\n",
      "Epoch 1932, Loss: 0.04534299857914448, Final Batch Loss: 0.021017158403992653\n",
      "Epoch 1933, Loss: 0.056462936103343964, Final Batch Loss: 0.01737203821539879\n",
      "Epoch 1934, Loss: 0.046848002821207047, Final Batch Loss: 0.006698109209537506\n",
      "Epoch 1935, Loss: 0.07564632594585419, Final Batch Loss: 0.051552917808294296\n",
      "Epoch 1936, Loss: 0.01510291825979948, Final Batch Loss: 0.005737241357564926\n",
      "Epoch 1937, Loss: 0.04723856784403324, Final Batch Loss: 0.009753050282597542\n",
      "Epoch 1938, Loss: 0.013878158293664455, Final Batch Loss: 0.006553041748702526\n",
      "Epoch 1939, Loss: 0.03348581679165363, Final Batch Loss: 0.007843989878892899\n",
      "Epoch 1940, Loss: 0.03682259004563093, Final Batch Loss: 0.012950497679412365\n",
      "Epoch 1941, Loss: 0.05798264034092426, Final Batch Loss: 0.05203402042388916\n",
      "Epoch 1942, Loss: 0.01193333463743329, Final Batch Loss: 0.006961355917155743\n",
      "Epoch 1943, Loss: 0.01967327343299985, Final Batch Loss: 0.007088650483638048\n",
      "Epoch 1944, Loss: 0.02912468370050192, Final Batch Loss: 0.02147662825882435\n",
      "Epoch 1945, Loss: 0.01993315014988184, Final Batch Loss: 0.007246858440339565\n",
      "Epoch 1946, Loss: 0.04431278444826603, Final Batch Loss: 0.017514249309897423\n",
      "Epoch 1947, Loss: 0.02301386184990406, Final Batch Loss: 0.015070030465722084\n",
      "Epoch 1948, Loss: 0.036631543189287186, Final Batch Loss: 0.015315772965550423\n",
      "Epoch 1949, Loss: 0.02251951675862074, Final Batch Loss: 0.0038246186450123787\n",
      "Epoch 1950, Loss: 0.04224566277116537, Final Batch Loss: 0.037477098405361176\n",
      "Epoch 1951, Loss: 0.04639764502644539, Final Batch Loss: 0.015701336786150932\n",
      "Epoch 1952, Loss: 0.04315291345119476, Final Batch Loss: 0.010796435177326202\n",
      "Epoch 1953, Loss: 0.040354836732149124, Final Batch Loss: 0.020113345235586166\n",
      "Epoch 1954, Loss: 0.04964062198996544, Final Batch Loss: 0.04091838747262955\n",
      "Epoch 1955, Loss: 0.05697672115638852, Final Batch Loss: 0.005884021986275911\n",
      "Epoch 1956, Loss: 0.04054341837763786, Final Batch Loss: 0.024530235677957535\n",
      "Epoch 1957, Loss: 0.008534545544534922, Final Batch Loss: 0.004448055289685726\n",
      "Epoch 1958, Loss: 0.07079754397273064, Final Batch Loss: 0.05157945305109024\n",
      "Epoch 1959, Loss: 0.03649073000997305, Final Batch Loss: 0.007445159368216991\n",
      "Epoch 1960, Loss: 0.07814461831003428, Final Batch Loss: 0.06784071773290634\n",
      "Epoch 1961, Loss: 0.05179562233388424, Final Batch Loss: 0.00968187116086483\n",
      "Epoch 1962, Loss: 0.04168689902871847, Final Batch Loss: 0.014577425085008144\n",
      "Epoch 1963, Loss: 0.06763134710490704, Final Batch Loss: 0.016507619991898537\n",
      "Epoch 1964, Loss: 0.028035318478941917, Final Batch Loss: 0.021471573039889336\n",
      "Epoch 1965, Loss: 0.03343783691525459, Final Batch Loss: 0.010237758979201317\n",
      "Epoch 1966, Loss: 0.05507557839155197, Final Batch Loss: 0.04215613752603531\n",
      "Epoch 1967, Loss: 0.028920887038111687, Final Batch Loss: 0.0071617718786001205\n",
      "Epoch 1968, Loss: 0.01956837112084031, Final Batch Loss: 0.012728805653750896\n",
      "Epoch 1969, Loss: 0.0184834823012352, Final Batch Loss: 0.005159489810466766\n",
      "Epoch 1970, Loss: 0.014918309869244695, Final Batch Loss: 0.003301974618807435\n",
      "Epoch 1971, Loss: 0.04197527840733528, Final Batch Loss: 0.025513723492622375\n",
      "Epoch 1972, Loss: 0.020659644156694412, Final Batch Loss: 0.008252509869635105\n",
      "Epoch 1973, Loss: 0.1151435524225235, Final Batch Loss: 0.10171651095151901\n",
      "Epoch 1974, Loss: 0.009900036733597517, Final Batch Loss: 0.005105023737996817\n",
      "Epoch 1975, Loss: 0.04332196153700352, Final Batch Loss: 0.01608324609696865\n",
      "Epoch 1976, Loss: 0.0699595008045435, Final Batch Loss: 0.01222030259668827\n",
      "Epoch 1977, Loss: 0.11788737680763006, Final Batch Loss: 0.10948967933654785\n",
      "Epoch 1978, Loss: 0.09885511547327042, Final Batch Loss: 0.06741474568843842\n",
      "Epoch 1979, Loss: 0.03972055343911052, Final Batch Loss: 0.007148392032831907\n",
      "Epoch 1980, Loss: 0.10694598779082298, Final Batch Loss: 0.08979348093271255\n",
      "Epoch 1981, Loss: 0.05464586615562439, Final Batch Loss: 0.007898837327957153\n",
      "Epoch 1982, Loss: 0.0395918944850564, Final Batch Loss: 0.010351953096687794\n",
      "Epoch 1983, Loss: 0.014492873102426529, Final Batch Loss: 0.006983827333897352\n",
      "Epoch 1984, Loss: 0.09100055880844593, Final Batch Loss: 0.060610827058553696\n",
      "Epoch 1985, Loss: 0.021092722192406654, Final Batch Loss: 0.00833949912339449\n",
      "Epoch 1986, Loss: 0.04339590109884739, Final Batch Loss: 0.02167278528213501\n",
      "Epoch 1987, Loss: 0.018439325504004955, Final Batch Loss: 0.011430642567574978\n",
      "Epoch 1988, Loss: 0.035447769332677126, Final Batch Loss: 0.03142205625772476\n",
      "Epoch 1989, Loss: 0.05875200033187866, Final Batch Loss: 0.024836726486682892\n",
      "Epoch 1990, Loss: 0.04569748975336552, Final Batch Loss: 0.032873157411813736\n",
      "Epoch 1991, Loss: 0.05734796915203333, Final Batch Loss: 0.00733818206936121\n",
      "Epoch 1992, Loss: 0.02387777017429471, Final Batch Loss: 0.019695168361067772\n",
      "Epoch 1993, Loss: 0.07496599480509758, Final Batch Loss: 0.032519496977329254\n",
      "Epoch 1994, Loss: 0.06505810376256704, Final Batch Loss: 0.05508407950401306\n",
      "Epoch 1995, Loss: 0.01935385772958398, Final Batch Loss: 0.007653419394046068\n",
      "Epoch 1996, Loss: 0.029337004758417606, Final Batch Loss: 0.015439968556165695\n",
      "Epoch 1997, Loss: 0.012262214440852404, Final Batch Loss: 0.0047623454593122005\n",
      "Epoch 1998, Loss: 0.030091041699051857, Final Batch Loss: 0.0042016711086034775\n",
      "Epoch 1999, Loss: 0.03044507442973554, Final Batch Loss: 0.003783289110288024\n",
      "Epoch 2000, Loss: 0.041291638277471066, Final Batch Loss: 0.031546495854854584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2001, Loss: 0.03222768474370241, Final Batch Loss: 0.0030069267377257347\n",
      "Epoch 2002, Loss: 0.013002500403672457, Final Batch Loss: 0.008075774647295475\n",
      "Epoch 2003, Loss: 0.044788993895053864, Final Batch Loss: 0.03303169459104538\n",
      "Epoch 2004, Loss: 0.028230994939804077, Final Batch Loss: 0.016492491587996483\n",
      "Epoch 2005, Loss: 0.040618401020765305, Final Batch Loss: 0.008623242378234863\n",
      "Epoch 2006, Loss: 0.03153006825596094, Final Batch Loss: 0.02484537847340107\n",
      "Epoch 2007, Loss: 0.041180490516126156, Final Batch Loss: 0.010128294117748737\n",
      "Epoch 2008, Loss: 0.033238111063838005, Final Batch Loss: 0.015963388606905937\n",
      "Epoch 2009, Loss: 0.04548673238605261, Final Batch Loss: 0.009515671990811825\n",
      "Epoch 2010, Loss: 0.046876628883183, Final Batch Loss: 0.03253829479217529\n",
      "Epoch 2011, Loss: 0.028626381885260344, Final Batch Loss: 0.023901205509901047\n",
      "Epoch 2012, Loss: 0.028638717718422413, Final Batch Loss: 0.020495155826210976\n",
      "Epoch 2013, Loss: 0.038981824181973934, Final Batch Loss: 0.005770414136350155\n",
      "Epoch 2014, Loss: 0.018724589608609676, Final Batch Loss: 0.008864645846188068\n",
      "Epoch 2015, Loss: 0.034263236448168755, Final Batch Loss: 0.024984043091535568\n",
      "Epoch 2016, Loss: 0.07672793231904507, Final Batch Loss: 0.055471550673246384\n",
      "Epoch 2017, Loss: 0.02692044246941805, Final Batch Loss: 0.02001366764307022\n",
      "Epoch 2018, Loss: 0.02198171056807041, Final Batch Loss: 0.009510330855846405\n",
      "Epoch 2019, Loss: 0.08779099211096764, Final Batch Loss: 0.05050353705883026\n",
      "Epoch 2020, Loss: 0.04147754469886422, Final Batch Loss: 0.03506161645054817\n",
      "Epoch 2021, Loss: 0.07876934856176376, Final Batch Loss: 0.017929434776306152\n",
      "Epoch 2022, Loss: 0.08955216035246849, Final Batch Loss: 0.05526924878358841\n",
      "Epoch 2023, Loss: 0.04502012487500906, Final Batch Loss: 0.008932325057685375\n",
      "Epoch 2024, Loss: 0.015774451196193695, Final Batch Loss: 0.007829191163182259\n",
      "Epoch 2025, Loss: 0.09668618813157082, Final Batch Loss: 0.0793551653623581\n",
      "Epoch 2026, Loss: 0.024917280301451683, Final Batch Loss: 0.013976041227579117\n",
      "Epoch 2027, Loss: 0.05650407448410988, Final Batch Loss: 0.00861252099275589\n",
      "Epoch 2028, Loss: 0.02450884599238634, Final Batch Loss: 0.011552478186786175\n",
      "Epoch 2029, Loss: 0.04258394334465265, Final Batch Loss: 0.011679275892674923\n",
      "Epoch 2030, Loss: 0.07078423537313938, Final Batch Loss: 0.05649814382195473\n",
      "Epoch 2031, Loss: 0.03476489149034023, Final Batch Loss: 0.011300167068839073\n",
      "Epoch 2032, Loss: 0.018198559060692787, Final Batch Loss: 0.0038524940609931946\n",
      "Epoch 2033, Loss: 0.017266526352614164, Final Batch Loss: 0.0096605708822608\n",
      "Epoch 2034, Loss: 0.016716074664145708, Final Batch Loss: 0.009980711154639721\n",
      "Epoch 2035, Loss: 0.01989575754851103, Final Batch Loss: 0.013930761255323887\n",
      "Epoch 2036, Loss: 0.025693503208458424, Final Batch Loss: 0.017680194228887558\n",
      "Epoch 2037, Loss: 0.053708454594016075, Final Batch Loss: 0.01685159094631672\n",
      "Epoch 2038, Loss: 0.020348764024674892, Final Batch Loss: 0.00978253223001957\n",
      "Epoch 2039, Loss: 0.04575200751423836, Final Batch Loss: 0.016975944861769676\n",
      "Epoch 2040, Loss: 0.02559721004217863, Final Batch Loss: 0.015048633329570293\n",
      "Epoch 2041, Loss: 0.07279033213853836, Final Batch Loss: 0.051269881427288055\n",
      "Epoch 2042, Loss: 0.07081876322627068, Final Batch Loss: 0.03979683294892311\n",
      "Epoch 2043, Loss: 0.08963380753993988, Final Batch Loss: 0.07165826112031937\n",
      "Epoch 2044, Loss: 0.045174021273851395, Final Batch Loss: 0.04204948619008064\n",
      "Epoch 2045, Loss: 0.07798365037888288, Final Batch Loss: 0.07268139719963074\n",
      "Epoch 2046, Loss: 0.07020996045321226, Final Batch Loss: 0.05547700449824333\n",
      "Epoch 2047, Loss: 0.060584986582398415, Final Batch Loss: 0.04747961089015007\n",
      "Epoch 2048, Loss: 0.11791162472218275, Final Batch Loss: 0.10588697344064713\n",
      "Epoch 2049, Loss: 0.10918772593140602, Final Batch Loss: 0.0245874784886837\n",
      "Epoch 2050, Loss: 0.21897589415311813, Final Batch Loss: 0.12986958026885986\n",
      "Epoch 2051, Loss: 0.15366104617714882, Final Batch Loss: 0.09406527876853943\n",
      "Epoch 2052, Loss: 0.17447873950004578, Final Batch Loss: 0.0820297822356224\n",
      "Epoch 2053, Loss: 0.08932094648480415, Final Batch Loss: 0.04500540345907211\n",
      "Epoch 2054, Loss: 0.07329266145825386, Final Batch Loss: 0.03329755738377571\n",
      "Epoch 2055, Loss: 0.052612834610044956, Final Batch Loss: 0.012067616917192936\n",
      "Epoch 2056, Loss: 0.05474705435335636, Final Batch Loss: 0.022872870787978172\n",
      "Epoch 2057, Loss: 0.06712295673787594, Final Batch Loss: 0.03619080409407616\n",
      "Epoch 2058, Loss: 0.027709276415407658, Final Batch Loss: 0.007952873595058918\n",
      "Epoch 2059, Loss: 0.13871819898486137, Final Batch Loss: 0.09549442678689957\n",
      "Epoch 2060, Loss: 0.05994381196796894, Final Batch Loss: 0.037201590836048126\n",
      "Epoch 2061, Loss: 0.08166145719587803, Final Batch Loss: 0.05767063423991203\n",
      "Epoch 2062, Loss: 0.024974873289465904, Final Batch Loss: 0.015296156518161297\n",
      "Epoch 2063, Loss: 0.056680671870708466, Final Batch Loss: 0.031085744500160217\n",
      "Epoch 2064, Loss: 0.048418207094073296, Final Batch Loss: 0.019773080945014954\n",
      "Epoch 2065, Loss: 0.06341938301920891, Final Batch Loss: 0.026125945150852203\n",
      "Epoch 2066, Loss: 0.03197856992483139, Final Batch Loss: 0.017708932980895042\n",
      "Epoch 2067, Loss: 0.06844212859869003, Final Batch Loss: 0.017329350113868713\n",
      "Epoch 2068, Loss: 0.036161513067781925, Final Batch Loss: 0.008075135760009289\n",
      "Epoch 2069, Loss: 0.042843399569392204, Final Batch Loss: 0.006284361705183983\n",
      "Epoch 2070, Loss: 0.024016073904931545, Final Batch Loss: 0.010481089353561401\n",
      "Epoch 2071, Loss: 0.06186419166624546, Final Batch Loss: 0.01932785101234913\n",
      "Epoch 2072, Loss: 0.06460010632872581, Final Batch Loss: 0.01790788024663925\n",
      "Epoch 2073, Loss: 0.03427420649677515, Final Batch Loss: 0.01374520268291235\n",
      "Epoch 2074, Loss: 0.04753858968615532, Final Batch Loss: 0.01665934920310974\n",
      "Epoch 2075, Loss: 0.07116327807307243, Final Batch Loss: 0.03633909299969673\n",
      "Epoch 2076, Loss: 0.027383223176002502, Final Batch Loss: 0.00915493257343769\n",
      "Epoch 2077, Loss: 0.07747157663106918, Final Batch Loss: 0.056696221232414246\n",
      "Epoch 2078, Loss: 0.03480389155447483, Final Batch Loss: 0.01525270938873291\n",
      "Epoch 2079, Loss: 0.1410016119480133, Final Batch Loss: 0.055604487657547\n",
      "Epoch 2080, Loss: 0.03519466333091259, Final Batch Loss: 0.017541920766234398\n",
      "Epoch 2081, Loss: 0.024181322194635868, Final Batch Loss: 0.012136944569647312\n",
      "Epoch 2082, Loss: 0.0455278605222702, Final Batch Loss: 0.00911274179816246\n",
      "Epoch 2083, Loss: 0.041983528062701225, Final Batch Loss: 0.030028050765395164\n",
      "Epoch 2084, Loss: 0.08400411531329155, Final Batch Loss: 0.03866419568657875\n",
      "Epoch 2085, Loss: 0.05813928786665201, Final Batch Loss: 0.012979359365999699\n",
      "Epoch 2086, Loss: 0.029233011417090893, Final Batch Loss: 0.01729452982544899\n",
      "Epoch 2087, Loss: 0.0562925860285759, Final Batch Loss: 0.01867855340242386\n",
      "Epoch 2088, Loss: 0.027713405899703503, Final Batch Loss: 0.01575515978038311\n",
      "Epoch 2089, Loss: 0.04991209879517555, Final Batch Loss: 0.017853587865829468\n",
      "Epoch 2090, Loss: 0.02469947189092636, Final Batch Loss: 0.013053875416517258\n",
      "Epoch 2091, Loss: 0.09886849671602249, Final Batch Loss: 0.05533541366457939\n",
      "Epoch 2092, Loss: 0.061732249334454536, Final Batch Loss: 0.03602670878171921\n",
      "Epoch 2093, Loss: 0.05685650743544102, Final Batch Loss: 0.0341390036046505\n",
      "Epoch 2094, Loss: 0.048501089215278625, Final Batch Loss: 0.01602747291326523\n",
      "Epoch 2095, Loss: 0.05004877224564552, Final Batch Loss: 0.017051201313734055\n",
      "Epoch 2096, Loss: 0.0714398380368948, Final Batch Loss: 0.02969265542924404\n",
      "Epoch 2097, Loss: 0.07351326197385788, Final Batch Loss: 0.04745953157544136\n",
      "Epoch 2098, Loss: 0.019914157688617706, Final Batch Loss: 0.008261923678219318\n",
      "Epoch 2099, Loss: 0.025771288201212883, Final Batch Loss: 0.01022874005138874\n",
      "Epoch 2100, Loss: 0.024602776393294334, Final Batch Loss: 0.005125140771269798\n",
      "Epoch 2101, Loss: 0.055090270936489105, Final Batch Loss: 0.017034966498613358\n",
      "Epoch 2102, Loss: 0.0765487402677536, Final Batch Loss: 0.034340228885412216\n",
      "Epoch 2103, Loss: 0.020386830903589725, Final Batch Loss: 0.008513818494975567\n",
      "Epoch 2104, Loss: 0.028208951465785503, Final Batch Loss: 0.022801198065280914\n",
      "Epoch 2105, Loss: 0.036672418005764484, Final Batch Loss: 0.012353534810245037\n",
      "Epoch 2106, Loss: 0.012294463813304901, Final Batch Loss: 0.006021474022418261\n",
      "Epoch 2107, Loss: 0.042274920269846916, Final Batch Loss: 0.02194949798285961\n",
      "Epoch 2108, Loss: 0.06133630592375994, Final Batch Loss: 0.015197576023638248\n",
      "Epoch 2109, Loss: 0.025325605180114508, Final Batch Loss: 0.005270720925182104\n",
      "Epoch 2110, Loss: 0.03593612555414438, Final Batch Loss: 0.011797052808105946\n",
      "Epoch 2111, Loss: 0.05135218799114227, Final Batch Loss: 0.03817986696958542\n",
      "Epoch 2112, Loss: 0.03942778054624796, Final Batch Loss: 0.012884796597063541\n",
      "Epoch 2113, Loss: 0.021346215158700943, Final Batch Loss: 0.011072484776377678\n",
      "Epoch 2114, Loss: 0.021845803130418062, Final Batch Loss: 0.01734906993806362\n",
      "Epoch 2115, Loss: 0.05713278893381357, Final Batch Loss: 0.013555091805756092\n",
      "Epoch 2116, Loss: 0.062040758319199085, Final Batch Loss: 0.048875417560338974\n",
      "Epoch 2117, Loss: 0.05776906851679087, Final Batch Loss: 0.004207742400467396\n",
      "Epoch 2118, Loss: 0.08078395202755928, Final Batch Loss: 0.06345930695533752\n",
      "Epoch 2119, Loss: 0.05674920976161957, Final Batch Loss: 0.018881462514400482\n",
      "Epoch 2120, Loss: 0.15390989929437637, Final Batch Loss: 0.05781000852584839\n",
      "Epoch 2121, Loss: 0.1151488684117794, Final Batch Loss: 0.0919974222779274\n",
      "Epoch 2122, Loss: 0.08220367692410946, Final Batch Loss: 0.015341570600867271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2123, Loss: 0.06274321675300598, Final Batch Loss: 0.04579230397939682\n",
      "Epoch 2124, Loss: 0.05526178330183029, Final Batch Loss: 0.022253043949604034\n",
      "Epoch 2125, Loss: 0.0537895355373621, Final Batch Loss: 0.010397261008620262\n",
      "Epoch 2126, Loss: 0.06530264858156443, Final Batch Loss: 0.05285726860165596\n",
      "Epoch 2127, Loss: 0.04955595452338457, Final Batch Loss: 0.040120240300893784\n",
      "Epoch 2128, Loss: 0.028451050631701946, Final Batch Loss: 0.014787626452744007\n",
      "Epoch 2129, Loss: 0.028965857811272144, Final Batch Loss: 0.012826948426663876\n",
      "Epoch 2130, Loss: 0.041366759687662125, Final Batch Loss: 0.021171817556023598\n",
      "Epoch 2131, Loss: 0.04489216487854719, Final Batch Loss: 0.008641100488603115\n",
      "Epoch 2132, Loss: 0.09665515273809433, Final Batch Loss: 0.0616992749273777\n",
      "Epoch 2133, Loss: 0.03265091963112354, Final Batch Loss: 0.00877576507627964\n",
      "Epoch 2134, Loss: 0.031632390804588795, Final Batch Loss: 0.020451122894883156\n",
      "Epoch 2135, Loss: 0.04952199384570122, Final Batch Loss: 0.024100909009575844\n",
      "Epoch 2136, Loss: 0.04670155607163906, Final Batch Loss: 0.008056463673710823\n",
      "Epoch 2137, Loss: 0.04468880221247673, Final Batch Loss: 0.014137357473373413\n",
      "Epoch 2138, Loss: 0.06285545602440834, Final Batch Loss: 0.010606557130813599\n",
      "Epoch 2139, Loss: 0.05019599571824074, Final Batch Loss: 0.012847531586885452\n",
      "Epoch 2140, Loss: 0.06771835684776306, Final Batch Loss: 0.03155463561415672\n",
      "Epoch 2141, Loss: 0.025024736300110817, Final Batch Loss: 0.01242043450474739\n",
      "Epoch 2142, Loss: 0.06405697483569384, Final Batch Loss: 0.053652096539735794\n",
      "Epoch 2143, Loss: 0.0524513591080904, Final Batch Loss: 0.03156828135251999\n",
      "Epoch 2144, Loss: 0.07420920580625534, Final Batch Loss: 0.058368388563394547\n",
      "Epoch 2145, Loss: 0.022585498169064522, Final Batch Loss: 0.014528298750519753\n",
      "Epoch 2146, Loss: 0.050622690469026566, Final Batch Loss: 0.01776309311389923\n",
      "Epoch 2147, Loss: 0.04597865277901292, Final Batch Loss: 0.003916418645530939\n",
      "Epoch 2148, Loss: 0.022513761185109615, Final Batch Loss: 0.011512779630720615\n",
      "Epoch 2149, Loss: 0.029708484187722206, Final Batch Loss: 0.011444922536611557\n",
      "Epoch 2150, Loss: 0.024887909181416035, Final Batch Loss: 0.007397749461233616\n",
      "Epoch 2151, Loss: 0.02707437053322792, Final Batch Loss: 0.009260239079594612\n",
      "Epoch 2152, Loss: 0.030933810397982597, Final Batch Loss: 0.021631037816405296\n",
      "Epoch 2153, Loss: 0.07035528868436813, Final Batch Loss: 0.020153652876615524\n",
      "Epoch 2154, Loss: 0.0399659126996994, Final Batch Loss: 0.028779003769159317\n",
      "Epoch 2155, Loss: 0.026375808753073215, Final Batch Loss: 0.005040097050368786\n",
      "Epoch 2156, Loss: 0.03520573861896992, Final Batch Loss: 0.019338080659508705\n",
      "Epoch 2157, Loss: 0.04502895660698414, Final Batch Loss: 0.024284416809678078\n",
      "Epoch 2158, Loss: 0.057008556090295315, Final Batch Loss: 0.00941516738384962\n",
      "Epoch 2159, Loss: 0.03658716194331646, Final Batch Loss: 0.01757422275841236\n",
      "Epoch 2160, Loss: 0.05557059310376644, Final Batch Loss: 0.010303949937224388\n",
      "Epoch 2161, Loss: 0.024624519981443882, Final Batch Loss: 0.012410089373588562\n",
      "Epoch 2162, Loss: 0.048448506742715836, Final Batch Loss: 0.016901083290576935\n",
      "Epoch 2163, Loss: 0.04120730888098478, Final Batch Loss: 0.009680411778390408\n",
      "Epoch 2164, Loss: 0.06243237294256687, Final Batch Loss: 0.024019276723265648\n",
      "Epoch 2165, Loss: 0.01855703489854932, Final Batch Loss: 0.005538528319448233\n",
      "Epoch 2166, Loss: 0.05927609046921134, Final Batch Loss: 0.055128488689661026\n",
      "Epoch 2167, Loss: 0.030774282291531563, Final Batch Loss: 0.025631841272115707\n",
      "Epoch 2168, Loss: 0.04476826637983322, Final Batch Loss: 0.0289030559360981\n",
      "Epoch 2169, Loss: 0.05396207608282566, Final Batch Loss: 0.026751145720481873\n",
      "Epoch 2170, Loss: 0.07201334461569786, Final Batch Loss: 0.033258404582738876\n",
      "Epoch 2171, Loss: 0.024736764142289758, Final Batch Loss: 0.02182551845908165\n",
      "Epoch 2172, Loss: 0.06448986567556858, Final Batch Loss: 0.016948776319622993\n",
      "Epoch 2173, Loss: 0.02903904765844345, Final Batch Loss: 0.022544771432876587\n",
      "Epoch 2174, Loss: 0.0227122672367841, Final Batch Loss: 0.0033411059994250536\n",
      "Epoch 2175, Loss: 0.026004329323768616, Final Batch Loss: 0.014893966726958752\n",
      "Epoch 2176, Loss: 0.027694091200828552, Final Batch Loss: 0.021069394424557686\n",
      "Epoch 2177, Loss: 0.038042571395635605, Final Batch Loss: 0.02414914220571518\n",
      "Epoch 2178, Loss: 0.05075574293732643, Final Batch Loss: 0.018355537205934525\n",
      "Epoch 2179, Loss: 0.02361297979950905, Final Batch Loss: 0.015262520872056484\n",
      "Epoch 2180, Loss: 0.0519854798913002, Final Batch Loss: 0.036659400910139084\n",
      "Epoch 2181, Loss: 0.03615614864975214, Final Batch Loss: 0.007060053758323193\n",
      "Epoch 2182, Loss: 0.03217596933245659, Final Batch Loss: 0.016518397256731987\n",
      "Epoch 2183, Loss: 0.05868241051211953, Final Batch Loss: 0.007372720632702112\n",
      "Epoch 2184, Loss: 0.02896399237215519, Final Batch Loss: 0.018493855372071266\n",
      "Epoch 2185, Loss: 0.0433534518815577, Final Batch Loss: 0.03920462727546692\n",
      "Epoch 2186, Loss: 0.020296975038945675, Final Batch Loss: 0.010784548707306385\n",
      "Epoch 2187, Loss: 0.026877284049987793, Final Batch Loss: 0.005882853642106056\n",
      "Epoch 2188, Loss: 0.0746711753308773, Final Batch Loss: 0.0337698757648468\n",
      "Epoch 2189, Loss: 0.0748774588573724, Final Batch Loss: 0.07177626341581345\n",
      "Epoch 2190, Loss: 0.05710876267403364, Final Batch Loss: 0.013627267442643642\n",
      "Epoch 2191, Loss: 0.025110776536166668, Final Batch Loss: 0.005185124464333057\n",
      "Epoch 2192, Loss: 0.0453071566298604, Final Batch Loss: 0.009135146625339985\n",
      "Epoch 2193, Loss: 0.022572264540940523, Final Batch Loss: 0.01702423021197319\n",
      "Epoch 2194, Loss: 0.054089032113552094, Final Batch Loss: 0.005053568631410599\n",
      "Epoch 2195, Loss: 0.0698418915271759, Final Batch Loss: 0.03843005746603012\n",
      "Epoch 2196, Loss: 0.04252787493169308, Final Batch Loss: 0.02357165329158306\n",
      "Epoch 2197, Loss: 0.062349215149879456, Final Batch Loss: 0.019639715552330017\n",
      "Epoch 2198, Loss: 0.03862678352743387, Final Batch Loss: 0.024650488048791885\n",
      "Epoch 2199, Loss: 0.0381822120398283, Final Batch Loss: 0.01460375264286995\n",
      "Epoch 2200, Loss: 0.025267726741731167, Final Batch Loss: 0.014790975488722324\n",
      "Epoch 2201, Loss: 0.030466324649751186, Final Batch Loss: 0.01719370298087597\n",
      "Epoch 2202, Loss: 0.035075802356004715, Final Batch Loss: 0.019852416589856148\n",
      "Epoch 2203, Loss: 0.041961781680583954, Final Batch Loss: 0.015609590336680412\n",
      "Epoch 2204, Loss: 0.0401535090059042, Final Batch Loss: 0.02222677506506443\n",
      "Epoch 2205, Loss: 0.026789328083395958, Final Batch Loss: 0.004857530817389488\n",
      "Epoch 2206, Loss: 0.09643341228365898, Final Batch Loss: 0.05663971230387688\n",
      "Epoch 2207, Loss: 0.047250385861843824, Final Batch Loss: 0.041450727730989456\n",
      "Epoch 2208, Loss: 0.02613003458827734, Final Batch Loss: 0.0044900206848979\n",
      "Epoch 2209, Loss: 0.029361196793615818, Final Batch Loss: 0.009019059129059315\n",
      "Epoch 2210, Loss: 0.052539950236678123, Final Batch Loss: 0.02125507779419422\n",
      "Epoch 2211, Loss: 0.11984515190124512, Final Batch Loss: 0.0470595583319664\n",
      "Epoch 2212, Loss: 0.02299379650503397, Final Batch Loss: 0.00887466873973608\n",
      "Epoch 2213, Loss: 0.01659583766013384, Final Batch Loss: 0.01171898003667593\n",
      "Epoch 2214, Loss: 0.017142461147159338, Final Batch Loss: 0.007573029492050409\n",
      "Epoch 2215, Loss: 0.057054731994867325, Final Batch Loss: 0.03533986210823059\n",
      "Epoch 2216, Loss: 0.1651252992451191, Final Batch Loss: 0.10978209227323532\n",
      "Epoch 2217, Loss: 0.030502593144774437, Final Batch Loss: 0.017722822725772858\n",
      "Epoch 2218, Loss: 0.01729406649246812, Final Batch Loss: 0.005256454925984144\n",
      "Epoch 2219, Loss: 0.042965371161699295, Final Batch Loss: 0.025881638750433922\n",
      "Epoch 2220, Loss: 0.048600440844893456, Final Batch Loss: 0.011686155572533607\n",
      "Epoch 2221, Loss: 0.037120907101780176, Final Batch Loss: 0.003827387932687998\n",
      "Epoch 2222, Loss: 0.15244314074516296, Final Batch Loss: 0.10349635779857635\n",
      "Epoch 2223, Loss: 0.05703252926468849, Final Batch Loss: 0.026320457458496094\n",
      "Epoch 2224, Loss: 0.056640240363776684, Final Batch Loss: 0.04606899991631508\n",
      "Epoch 2225, Loss: 0.09733279794454575, Final Batch Loss: 0.04661964625120163\n",
      "Epoch 2226, Loss: 0.06905946508049965, Final Batch Loss: 0.02070550248026848\n",
      "Epoch 2227, Loss: 0.029588764533400536, Final Batch Loss: 0.007892059162259102\n",
      "Epoch 2228, Loss: 0.0785056259483099, Final Batch Loss: 0.053256843239068985\n",
      "Epoch 2229, Loss: 0.034412247128784657, Final Batch Loss: 0.024336248636245728\n",
      "Epoch 2230, Loss: 0.02636727597564459, Final Batch Loss: 0.010982654057443142\n",
      "Epoch 2231, Loss: 0.0809682235121727, Final Batch Loss: 0.04157247394323349\n",
      "Epoch 2232, Loss: 0.06504196859896183, Final Batch Loss: 0.04748905450105667\n",
      "Epoch 2233, Loss: 0.05516831949353218, Final Batch Loss: 0.018061213195323944\n",
      "Epoch 2234, Loss: 0.047822557389736176, Final Batch Loss: 0.02921605110168457\n",
      "Epoch 2235, Loss: 0.03476522769778967, Final Batch Loss: 0.011081012897193432\n",
      "Epoch 2236, Loss: 0.07464968040585518, Final Batch Loss: 0.05111919716000557\n",
      "Epoch 2237, Loss: 0.06835735775530338, Final Batch Loss: 0.03943537548184395\n",
      "Epoch 2238, Loss: 0.06817086413502693, Final Batch Loss: 0.02484121173620224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2239, Loss: 0.038109635934233665, Final Batch Loss: 0.01309506967663765\n",
      "Epoch 2240, Loss: 0.09605991840362549, Final Batch Loss: 0.04061008617281914\n",
      "Epoch 2241, Loss: 0.04665728285908699, Final Batch Loss: 0.0258196834474802\n",
      "Epoch 2242, Loss: 0.034365280997008085, Final Batch Loss: 0.007250962313264608\n",
      "Epoch 2243, Loss: 0.06490446254611015, Final Batch Loss: 0.047141753137111664\n",
      "Epoch 2244, Loss: 0.07258412055671215, Final Batch Loss: 0.05269082635641098\n",
      "Epoch 2245, Loss: 0.04026026511564851, Final Batch Loss: 0.007172024343162775\n",
      "Epoch 2246, Loss: 0.034800685942173004, Final Batch Loss: 0.021634042263031006\n",
      "Epoch 2247, Loss: 0.02968063671141863, Final Batch Loss: 0.012779698707163334\n",
      "Epoch 2248, Loss: 0.04397863708436489, Final Batch Loss: 0.024806763976812363\n",
      "Epoch 2249, Loss: 0.07205215841531754, Final Batch Loss: 0.021886523813009262\n",
      "Epoch 2250, Loss: 0.020474710036069155, Final Batch Loss: 0.006046920549124479\n",
      "Epoch 2251, Loss: 0.030493331607431173, Final Batch Loss: 0.0076245809905231\n",
      "Epoch 2252, Loss: 0.028065907768905163, Final Batch Loss: 0.02075793407857418\n",
      "Epoch 2253, Loss: 0.01795151364058256, Final Batch Loss: 0.003766232170164585\n",
      "Epoch 2254, Loss: 0.036909133195877075, Final Batch Loss: 0.02558501623570919\n",
      "Epoch 2255, Loss: 0.038885834626853466, Final Batch Loss: 0.026679135859012604\n",
      "Epoch 2256, Loss: 0.023236272856593132, Final Batch Loss: 0.008103647269308567\n",
      "Epoch 2257, Loss: 0.041307711973786354, Final Batch Loss: 0.03614560142159462\n",
      "Epoch 2258, Loss: 0.025762486271560192, Final Batch Loss: 0.009888515807688236\n",
      "Epoch 2259, Loss: 0.04517119564116001, Final Batch Loss: 0.030614160001277924\n",
      "Epoch 2260, Loss: 0.021820559166371822, Final Batch Loss: 0.009139271453022957\n",
      "Epoch 2261, Loss: 0.030316723510622978, Final Batch Loss: 0.01661764085292816\n",
      "Epoch 2262, Loss: 0.05144905298948288, Final Batch Loss: 0.018269602209329605\n",
      "Epoch 2263, Loss: 0.05734156910330057, Final Batch Loss: 0.0422457717359066\n",
      "Epoch 2264, Loss: 0.03794964961707592, Final Batch Loss: 0.010157961398363113\n",
      "Epoch 2265, Loss: 0.04635465331375599, Final Batch Loss: 0.02028719149529934\n",
      "Epoch 2266, Loss: 0.015542685054242611, Final Batch Loss: 0.009323876351118088\n",
      "Epoch 2267, Loss: 0.06781117431819439, Final Batch Loss: 0.054723117500543594\n",
      "Epoch 2268, Loss: 0.017998509109020233, Final Batch Loss: 0.008713400922715664\n",
      "Epoch 2269, Loss: 0.022730732802301645, Final Batch Loss: 0.005826775450259447\n",
      "Epoch 2270, Loss: 0.036991676315665245, Final Batch Loss: 0.01892438717186451\n",
      "Epoch 2271, Loss: 0.030332266353070736, Final Batch Loss: 0.007480482570827007\n",
      "Epoch 2272, Loss: 0.02529967436566949, Final Batch Loss: 0.021425575017929077\n",
      "Epoch 2273, Loss: 0.03768657520413399, Final Batch Loss: 0.020637158304452896\n",
      "Epoch 2274, Loss: 0.0692994762212038, Final Batch Loss: 0.0570281445980072\n",
      "Epoch 2275, Loss: 0.016441908199340105, Final Batch Loss: 0.009800643660128117\n",
      "Epoch 2276, Loss: 0.019673428498208523, Final Batch Loss: 0.00877134595066309\n",
      "Epoch 2277, Loss: 0.028876975178718567, Final Batch Loss: 0.019874047487974167\n",
      "Epoch 2278, Loss: 0.020834448281675577, Final Batch Loss: 0.014209623448550701\n",
      "Epoch 2279, Loss: 0.034824141301214695, Final Batch Loss: 0.028024859726428986\n",
      "Epoch 2280, Loss: 0.01757262321189046, Final Batch Loss: 0.004683008883148432\n",
      "Epoch 2281, Loss: 0.04853898659348488, Final Batch Loss: 0.020132770761847496\n",
      "Epoch 2282, Loss: 0.03615495655685663, Final Batch Loss: 0.005466495640575886\n",
      "Epoch 2283, Loss: 0.021865819115191698, Final Batch Loss: 0.015182510949671268\n",
      "Epoch 2284, Loss: 0.03482956252992153, Final Batch Loss: 0.019636301323771477\n",
      "Epoch 2285, Loss: 0.0327672865241766, Final Batch Loss: 0.006566803902387619\n",
      "Epoch 2286, Loss: 0.07229859754443169, Final Batch Loss: 0.036937166005373\n",
      "Epoch 2287, Loss: 0.07564051449298859, Final Batch Loss: 0.04092499613761902\n",
      "Epoch 2288, Loss: 0.059944527223706245, Final Batch Loss: 0.04162276163697243\n",
      "Epoch 2289, Loss: 0.10443554073572159, Final Batch Loss: 0.05983924865722656\n",
      "Epoch 2290, Loss: 0.03623815253376961, Final Batch Loss: 0.026165375486016273\n",
      "Epoch 2291, Loss: 0.04907967709004879, Final Batch Loss: 0.026279553771018982\n",
      "Epoch 2292, Loss: 0.06300721410661936, Final Batch Loss: 0.012100585736334324\n",
      "Epoch 2293, Loss: 0.02610808378085494, Final Batch Loss: 0.0031891274265944958\n",
      "Epoch 2294, Loss: 0.06173368962481618, Final Batch Loss: 0.05530732497572899\n",
      "Epoch 2295, Loss: 0.050613343715667725, Final Batch Loss: 0.04151943325996399\n",
      "Epoch 2296, Loss: 0.08555283769965172, Final Batch Loss: 0.0677027702331543\n",
      "Epoch 2297, Loss: 0.07301527913659811, Final Batch Loss: 0.01231865119189024\n",
      "Epoch 2298, Loss: 0.04664712632074952, Final Batch Loss: 0.040999170392751694\n",
      "Epoch 2299, Loss: 0.05709339678287506, Final Batch Loss: 0.022776886820793152\n",
      "Epoch 2300, Loss: 0.07233262713998556, Final Batch Loss: 0.05857861042022705\n",
      "Epoch 2301, Loss: 0.033797443844377995, Final Batch Loss: 0.023794416338205338\n",
      "Epoch 2302, Loss: 0.10435071587562561, Final Batch Loss: 0.06624376028776169\n",
      "Epoch 2303, Loss: 0.07739183120429516, Final Batch Loss: 0.05942196398973465\n",
      "Epoch 2304, Loss: 0.021937886252999306, Final Batch Loss: 0.0050618089735507965\n",
      "Epoch 2305, Loss: 0.012349271215498447, Final Batch Loss: 0.003150797449052334\n",
      "Epoch 2306, Loss: 0.04253282956779003, Final Batch Loss: 0.025611761957406998\n",
      "Epoch 2307, Loss: 0.037537215277552605, Final Batch Loss: 0.013559229671955109\n",
      "Epoch 2308, Loss: 0.029565487056970596, Final Batch Loss: 0.00431344099342823\n",
      "Epoch 2309, Loss: 0.07117791473865509, Final Batch Loss: 0.044236134737730026\n",
      "Epoch 2310, Loss: 0.04575391998514533, Final Batch Loss: 0.007599217351526022\n",
      "Epoch 2311, Loss: 0.06781502906233072, Final Batch Loss: 0.05654226988554001\n",
      "Epoch 2312, Loss: 0.05785303749144077, Final Batch Loss: 0.01914718560874462\n",
      "Epoch 2313, Loss: 0.033026170916855335, Final Batch Loss: 0.011323475278913975\n",
      "Epoch 2314, Loss: 0.035988378804177046, Final Batch Loss: 0.007221355568617582\n",
      "Epoch 2315, Loss: 0.018376118503510952, Final Batch Loss: 0.00869268923997879\n",
      "Epoch 2316, Loss: 0.01545735727995634, Final Batch Loss: 0.0038553178310394287\n",
      "Epoch 2317, Loss: 0.016348582692444324, Final Batch Loss: 0.0052948445081710815\n",
      "Epoch 2318, Loss: 0.020252542570233345, Final Batch Loss: 0.009192283265292645\n",
      "Epoch 2319, Loss: 0.014741951134055853, Final Batch Loss: 0.005878683645278215\n",
      "Epoch 2320, Loss: 0.023500102572143078, Final Batch Loss: 0.009526442736387253\n",
      "Epoch 2321, Loss: 0.012958993669599295, Final Batch Loss: 0.006431497633457184\n",
      "Epoch 2322, Loss: 0.05590311158448458, Final Batch Loss: 0.043227799236774445\n",
      "Epoch 2323, Loss: 0.07419506832957268, Final Batch Loss: 0.03247961774468422\n",
      "Epoch 2324, Loss: 0.061689937487244606, Final Batch Loss: 0.022919191047549248\n",
      "Epoch 2325, Loss: 0.031952742487192154, Final Batch Loss: 0.010232498869299889\n",
      "Epoch 2326, Loss: 0.020755326375365257, Final Batch Loss: 0.011446516960859299\n",
      "Epoch 2327, Loss: 0.025326035916805267, Final Batch Loss: 0.011620214208960533\n",
      "Epoch 2328, Loss: 0.031320932786911726, Final Batch Loss: 0.006392765324562788\n",
      "Epoch 2329, Loss: 0.04194286372512579, Final Batch Loss: 0.013755806721746922\n",
      "Epoch 2330, Loss: 0.015960915246978402, Final Batch Loss: 0.003707167925313115\n",
      "Epoch 2331, Loss: 0.007838734658434987, Final Batch Loss: 0.0028583284001797438\n",
      "Epoch 2332, Loss: 0.008987783454358578, Final Batch Loss: 0.004241324029862881\n",
      "Epoch 2333, Loss: 0.017993734683841467, Final Batch Loss: 0.010274916887283325\n",
      "Epoch 2334, Loss: 0.02754467399790883, Final Batch Loss: 0.005677024368196726\n",
      "Epoch 2335, Loss: 0.025254250969737768, Final Batch Loss: 0.005933867301791906\n",
      "Epoch 2336, Loss: 0.03652246296405792, Final Batch Loss: 0.021079804748296738\n",
      "Epoch 2337, Loss: 0.04934086836874485, Final Batch Loss: 0.032850153744220734\n",
      "Epoch 2338, Loss: 0.017805502749979496, Final Batch Loss: 0.004146197810769081\n",
      "Epoch 2339, Loss: 0.016243704594671726, Final Batch Loss: 0.002474116161465645\n",
      "Epoch 2340, Loss: 0.03490951377898455, Final Batch Loss: 0.010756448842585087\n",
      "Epoch 2341, Loss: 0.014645599760115147, Final Batch Loss: 0.0053476011380553246\n",
      "Epoch 2342, Loss: 0.06201545428484678, Final Batch Loss: 0.046590033918619156\n",
      "Epoch 2343, Loss: 0.018271908164024353, Final Batch Loss: 0.005092372186481953\n",
      "Epoch 2344, Loss: 0.017967370338737965, Final Batch Loss: 0.004915421828627586\n",
      "Epoch 2345, Loss: 0.034942240454256535, Final Batch Loss: 0.02409983053803444\n",
      "Epoch 2346, Loss: 0.03501406218856573, Final Batch Loss: 0.011887985281646252\n",
      "Epoch 2347, Loss: 0.01849258504807949, Final Batch Loss: 0.009540029801428318\n",
      "Epoch 2348, Loss: 0.01621086709201336, Final Batch Loss: 0.010767982341349125\n",
      "Epoch 2349, Loss: 0.03822960192337632, Final Batch Loss: 0.032429855316877365\n",
      "Epoch 2350, Loss: 0.07988966628909111, Final Batch Loss: 0.041617803275585175\n",
      "Epoch 2351, Loss: 0.04208173416554928, Final Batch Loss: 0.028471214696764946\n",
      "Epoch 2352, Loss: 0.05206060782074928, Final Batch Loss: 0.0332234725356102\n",
      "Epoch 2353, Loss: 0.055328210815787315, Final Batch Loss: 0.030197499319911003\n",
      "Epoch 2354, Loss: 0.023734922520816326, Final Batch Loss: 0.01013289112597704\n",
      "Epoch 2355, Loss: 0.02724185260012746, Final Batch Loss: 0.02061188779771328\n",
      "Epoch 2356, Loss: 0.09313292056322098, Final Batch Loss: 0.013054259121418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2357, Loss: 0.026844663079828024, Final Batch Loss: 0.007188861723989248\n",
      "Epoch 2358, Loss: 0.05359483230859041, Final Batch Loss: 0.009234153665602207\n",
      "Epoch 2359, Loss: 0.02685229666531086, Final Batch Loss: 0.006601547822356224\n",
      "Epoch 2360, Loss: 0.029974620789289474, Final Batch Loss: 0.02165503241121769\n",
      "Epoch 2361, Loss: 0.01638584164902568, Final Batch Loss: 0.011600620113313198\n",
      "Epoch 2362, Loss: 0.030691263265907764, Final Batch Loss: 0.011203096248209476\n",
      "Epoch 2363, Loss: 0.03873978275805712, Final Batch Loss: 0.029350729659199715\n",
      "Epoch 2364, Loss: 0.031597958877682686, Final Batch Loss: 0.014398595318198204\n",
      "Epoch 2365, Loss: 0.02487798035144806, Final Batch Loss: 0.016203012317419052\n",
      "Epoch 2366, Loss: 0.03560586832463741, Final Batch Loss: 0.025858333334326744\n",
      "Epoch 2367, Loss: 0.015079732984304428, Final Batch Loss: 0.0065389592200517654\n",
      "Epoch 2368, Loss: 0.011833921074867249, Final Batch Loss: 0.00832979753613472\n",
      "Epoch 2369, Loss: 0.03956586308777332, Final Batch Loss: 0.02316264994442463\n",
      "Epoch 2370, Loss: 0.03469641227275133, Final Batch Loss: 0.007732699625194073\n",
      "Epoch 2371, Loss: 0.028950602747499943, Final Batch Loss: 0.01161250565201044\n",
      "Epoch 2372, Loss: 0.09690909460186958, Final Batch Loss: 0.03380445018410683\n",
      "Epoch 2373, Loss: 0.03733575250953436, Final Batch Loss: 0.0061782849952578545\n",
      "Epoch 2374, Loss: 0.0146760200150311, Final Batch Loss: 0.00576218543574214\n",
      "Epoch 2375, Loss: 0.03506835177540779, Final Batch Loss: 0.020638877525925636\n",
      "Epoch 2376, Loss: 0.04092797636985779, Final Batch Loss: 0.019024532288312912\n",
      "Epoch 2377, Loss: 0.03338852524757385, Final Batch Loss: 0.013400295749306679\n",
      "Epoch 2378, Loss: 0.02689504763111472, Final Batch Loss: 0.0069756112061440945\n",
      "Epoch 2379, Loss: 0.02307940786704421, Final Batch Loss: 0.005547845270484686\n",
      "Epoch 2380, Loss: 0.025589972734451294, Final Batch Loss: 0.012065702117979527\n",
      "Epoch 2381, Loss: 0.032249001786112785, Final Batch Loss: 0.023356055840849876\n",
      "Epoch 2382, Loss: 0.024775524623692036, Final Batch Loss: 0.01519041508436203\n",
      "Epoch 2383, Loss: 0.05079720634967089, Final Batch Loss: 0.01186914462596178\n",
      "Epoch 2384, Loss: 0.01188739133067429, Final Batch Loss: 0.003831762121990323\n",
      "Epoch 2385, Loss: 0.03384673688560724, Final Batch Loss: 0.003639654256403446\n",
      "Epoch 2386, Loss: 0.10122559312731028, Final Batch Loss: 0.09162956476211548\n",
      "Epoch 2387, Loss: 0.02655818359926343, Final Batch Loss: 0.02113262377679348\n",
      "Epoch 2388, Loss: 0.03542603552341461, Final Batch Loss: 0.03125295788049698\n",
      "Epoch 2389, Loss: 0.02096816897392273, Final Batch Loss: 0.01173988077789545\n",
      "Epoch 2390, Loss: 0.0452633798122406, Final Batch Loss: 0.014787605032324791\n",
      "Epoch 2391, Loss: 0.028913785703480244, Final Batch Loss: 0.007992918603122234\n",
      "Epoch 2392, Loss: 0.05118911527097225, Final Batch Loss: 0.018809394910931587\n",
      "Epoch 2393, Loss: 0.03399694990366697, Final Batch Loss: 0.013085256330668926\n",
      "Epoch 2394, Loss: 0.051812395453453064, Final Batch Loss: 0.024182792752981186\n",
      "Epoch 2395, Loss: 0.03719469113275409, Final Batch Loss: 0.0038932966999709606\n",
      "Epoch 2396, Loss: 0.04253639746457338, Final Batch Loss: 0.014143683947622776\n",
      "Epoch 2397, Loss: 0.08305149152874947, Final Batch Loss: 0.061149004846811295\n",
      "Epoch 2398, Loss: 0.021332571050152183, Final Batch Loss: 0.003056827699765563\n",
      "Epoch 2399, Loss: 0.017588449642062187, Final Batch Loss: 0.004125921055674553\n",
      "Epoch 2400, Loss: 0.03043059166520834, Final Batch Loss: 0.020285017788410187\n",
      "Epoch 2401, Loss: 0.079714166931808, Final Batch Loss: 0.07112564891576767\n",
      "Epoch 2402, Loss: 0.04070555977523327, Final Batch Loss: 0.016323411837220192\n",
      "Epoch 2403, Loss: 0.029980433639138937, Final Batch Loss: 0.025003429502248764\n",
      "Epoch 2404, Loss: 0.008965389337390661, Final Batch Loss: 0.0017099319957196712\n",
      "Epoch 2405, Loss: 0.04180731624364853, Final Batch Loss: 0.017973458394408226\n",
      "Epoch 2406, Loss: 0.024712922517210245, Final Batch Loss: 0.0042860438115894794\n",
      "Epoch 2407, Loss: 0.08294455334544182, Final Batch Loss: 0.044989779591560364\n",
      "Epoch 2408, Loss: 0.03871993627399206, Final Batch Loss: 0.02770637534558773\n",
      "Epoch 2409, Loss: 0.016857863403856754, Final Batch Loss: 0.007584655657410622\n",
      "Epoch 2410, Loss: 0.008072803961113095, Final Batch Loss: 0.004664327949285507\n",
      "Epoch 2411, Loss: 0.017430108040571213, Final Batch Loss: 0.006801570765674114\n",
      "Epoch 2412, Loss: 0.045884668827056885, Final Batch Loss: 0.03649745136499405\n",
      "Epoch 2413, Loss: 0.10852622846141458, Final Batch Loss: 0.1016223207116127\n",
      "Epoch 2414, Loss: 0.026363929733633995, Final Batch Loss: 0.0037757549434900284\n",
      "Epoch 2415, Loss: 0.01162783452309668, Final Batch Loss: 0.007942236959934235\n",
      "Epoch 2416, Loss: 0.017038426361978054, Final Batch Loss: 0.009030994027853012\n",
      "Epoch 2417, Loss: 0.02649483922868967, Final Batch Loss: 0.021822204813361168\n",
      "Epoch 2418, Loss: 0.0304060117341578, Final Batch Loss: 0.006529411766678095\n",
      "Epoch 2419, Loss: 0.03457052493467927, Final Batch Loss: 0.00568009028211236\n",
      "Epoch 2420, Loss: 0.022131352219730616, Final Batch Loss: 0.01498518604785204\n",
      "Epoch 2421, Loss: 0.02469791006296873, Final Batch Loss: 0.01620270311832428\n",
      "Epoch 2422, Loss: 0.08722853567451239, Final Batch Loss: 0.006404264830052853\n",
      "Epoch 2423, Loss: 0.044454099610447884, Final Batch Loss: 0.027232173830270767\n",
      "Epoch 2424, Loss: 0.051092805340886116, Final Batch Loss: 0.0206561628729105\n",
      "Epoch 2425, Loss: 0.028338349424302578, Final Batch Loss: 0.00439689215272665\n",
      "Epoch 2426, Loss: 0.07329360954463482, Final Batch Loss: 0.04753146320581436\n",
      "Epoch 2427, Loss: 0.04891347512602806, Final Batch Loss: 0.015876196324825287\n",
      "Epoch 2428, Loss: 0.008519230643287301, Final Batch Loss: 0.002388169290497899\n",
      "Epoch 2429, Loss: 0.04271236341446638, Final Batch Loss: 0.030978823080658913\n",
      "Epoch 2430, Loss: 0.03693010192364454, Final Batch Loss: 0.01382234413176775\n",
      "Epoch 2431, Loss: 0.06272333487868309, Final Batch Loss: 0.017774038016796112\n",
      "Epoch 2432, Loss: 0.07332961447536945, Final Batch Loss: 0.04430224001407623\n",
      "Epoch 2433, Loss: 0.05454947613179684, Final Batch Loss: 0.02014331705868244\n",
      "Epoch 2434, Loss: 0.08543699607253075, Final Batch Loss: 0.04745032265782356\n",
      "Epoch 2435, Loss: 0.06329410523176193, Final Batch Loss: 0.045059870928525925\n",
      "Epoch 2436, Loss: 0.057395944371819496, Final Batch Loss: 0.04103674367070198\n",
      "Epoch 2437, Loss: 0.1356663666665554, Final Batch Loss: 0.09531551599502563\n",
      "Epoch 2438, Loss: 0.047569381073117256, Final Batch Loss: 0.011079119518399239\n",
      "Epoch 2439, Loss: 0.02076093479990959, Final Batch Loss: 0.008553496561944485\n",
      "Epoch 2440, Loss: 0.08195638842880726, Final Batch Loss: 0.02127951942384243\n",
      "Epoch 2441, Loss: 0.06036987528204918, Final Batch Loss: 0.030388588085770607\n",
      "Epoch 2442, Loss: 0.06449904106557369, Final Batch Loss: 0.04672931507229805\n",
      "Epoch 2443, Loss: 0.09973523020744324, Final Batch Loss: 0.08129537850618362\n",
      "Epoch 2444, Loss: 0.048863125033676624, Final Batch Loss: 0.033831190317869186\n",
      "Epoch 2445, Loss: 0.049444111064076424, Final Batch Loss: 0.028752528131008148\n",
      "Epoch 2446, Loss: 0.01006506523117423, Final Batch Loss: 0.004759251605719328\n",
      "Epoch 2447, Loss: 0.025856340304017067, Final Batch Loss: 0.007866673171520233\n",
      "Epoch 2448, Loss: 0.020122252870351076, Final Batch Loss: 0.006830520462244749\n",
      "Epoch 2449, Loss: 0.03649357333779335, Final Batch Loss: 0.014158310368657112\n",
      "Epoch 2450, Loss: 0.05868642032146454, Final Batch Loss: 0.03208611533045769\n",
      "Epoch 2451, Loss: 0.02233472838997841, Final Batch Loss: 0.010318155400454998\n",
      "Epoch 2452, Loss: 0.033354765735566616, Final Batch Loss: 0.021389521658420563\n",
      "Epoch 2453, Loss: 0.03709905780851841, Final Batch Loss: 0.017062125727534294\n",
      "Epoch 2454, Loss: 0.014367778785526752, Final Batch Loss: 0.007865242660045624\n",
      "Epoch 2455, Loss: 0.02776579884812236, Final Batch Loss: 0.007359263952821493\n",
      "Epoch 2456, Loss: 0.03246038127690554, Final Batch Loss: 0.009959940798580647\n",
      "Epoch 2457, Loss: 0.05716738849878311, Final Batch Loss: 0.025455020368099213\n",
      "Epoch 2458, Loss: 0.018534092232584953, Final Batch Loss: 0.00791211612522602\n",
      "Epoch 2459, Loss: 0.04798999987542629, Final Batch Loss: 0.029628688469529152\n",
      "Epoch 2460, Loss: 0.00840281811542809, Final Batch Loss: 0.005489317234605551\n",
      "Epoch 2461, Loss: 0.008763621328398585, Final Batch Loss: 0.003852298716083169\n",
      "Epoch 2462, Loss: 0.02641182579100132, Final Batch Loss: 0.020634405314922333\n",
      "Epoch 2463, Loss: 0.043131159618496895, Final Batch Loss: 0.028161562979221344\n",
      "Epoch 2464, Loss: 0.04261839855462313, Final Batch Loss: 0.010115866549313068\n",
      "Epoch 2465, Loss: 0.021829751320183277, Final Batch Loss: 0.009198561310768127\n",
      "Epoch 2466, Loss: 0.036560979671776295, Final Batch Loss: 0.022001344710588455\n",
      "Epoch 2467, Loss: 0.07083433121442795, Final Batch Loss: 0.03360780328512192\n",
      "Epoch 2468, Loss: 0.05793095380067825, Final Batch Loss: 0.025931496173143387\n",
      "Epoch 2469, Loss: 0.053476182743906975, Final Batch Loss: 0.01841423474252224\n",
      "Epoch 2470, Loss: 0.08164520002901554, Final Batch Loss: 0.05621911957859993\n",
      "Epoch 2471, Loss: 0.025088278576731682, Final Batch Loss: 0.01174016110599041\n",
      "Epoch 2472, Loss: 0.018175800796598196, Final Batch Loss: 0.007331135217100382\n",
      "Epoch 2473, Loss: 0.027417216915637255, Final Batch Loss: 0.005010104272514582\n",
      "Epoch 2474, Loss: 0.02666276739910245, Final Batch Loss: 0.01945132203400135\n",
      "Epoch 2475, Loss: 0.017355992924422026, Final Batch Loss: 0.011437810026109219\n",
      "Epoch 2476, Loss: 0.05565827386453748, Final Batch Loss: 0.04877150058746338\n",
      "Epoch 2477, Loss: 0.022342314012348652, Final Batch Loss: 0.007822005078196526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2478, Loss: 0.037823967169970274, Final Batch Loss: 0.0037021669559180737\n",
      "Epoch 2479, Loss: 0.03812255337834358, Final Batch Loss: 0.012792497873306274\n",
      "Epoch 2480, Loss: 0.025470728054642677, Final Batch Loss: 0.0039206091314554214\n",
      "Epoch 2481, Loss: 0.03246148303151131, Final Batch Loss: 0.013994261622428894\n",
      "Epoch 2482, Loss: 0.04332403186708689, Final Batch Loss: 0.028293732553720474\n",
      "Epoch 2483, Loss: 0.08495405782014132, Final Batch Loss: 0.006898020394146442\n",
      "Epoch 2484, Loss: 0.055696435272693634, Final Batch Loss: 0.0341748371720314\n",
      "Epoch 2485, Loss: 0.04732305742800236, Final Batch Loss: 0.013817353174090385\n",
      "Epoch 2486, Loss: 0.05054728966206312, Final Batch Loss: 0.006275719963014126\n",
      "Epoch 2487, Loss: 0.020630910992622375, Final Batch Loss: 0.008222502656280994\n",
      "Epoch 2488, Loss: 0.03283122833818197, Final Batch Loss: 0.0146194351837039\n",
      "Epoch 2489, Loss: 0.011259128339588642, Final Batch Loss: 0.004154496360570192\n",
      "Epoch 2490, Loss: 0.047595392912626266, Final Batch Loss: 0.008469689637422562\n",
      "Epoch 2491, Loss: 0.013413958251476288, Final Batch Loss: 0.004114789888262749\n",
      "Epoch 2492, Loss: 0.05360695580020547, Final Batch Loss: 0.04648703709244728\n",
      "Epoch 2493, Loss: 0.02117192931473255, Final Batch Loss: 0.017570698633790016\n",
      "Epoch 2494, Loss: 0.02398819476366043, Final Batch Loss: 0.009730490855872631\n",
      "Epoch 2495, Loss: 0.05824874900281429, Final Batch Loss: 0.03636030852794647\n",
      "Epoch 2496, Loss: 0.010107445530593395, Final Batch Loss: 0.004298040177673101\n",
      "Epoch 2497, Loss: 0.029324709437787533, Final Batch Loss: 0.01801980286836624\n",
      "Epoch 2498, Loss: 0.034758225083351135, Final Batch Loss: 0.01687009446322918\n",
      "Epoch 2499, Loss: 0.04020485281944275, Final Batch Loss: 0.005379386246204376\n",
      "Epoch 2500, Loss: 0.051782709546387196, Final Batch Loss: 0.01399185974150896\n",
      "Epoch 2501, Loss: 0.02618835773319006, Final Batch Loss: 0.002970372326672077\n",
      "Epoch 2502, Loss: 0.06207918468862772, Final Batch Loss: 0.012318057008087635\n",
      "Epoch 2503, Loss: 0.018024839460849762, Final Batch Loss: 0.01254297886043787\n",
      "Epoch 2504, Loss: 0.04577628802508116, Final Batch Loss: 0.04130655899643898\n",
      "Epoch 2505, Loss: 0.0175229930318892, Final Batch Loss: 0.006415336858481169\n",
      "Epoch 2506, Loss: 0.033425155095756054, Final Batch Loss: 0.009975726716220379\n",
      "Epoch 2507, Loss: 0.012513401918113232, Final Batch Loss: 0.007781967520713806\n",
      "Epoch 2508, Loss: 0.021919038612395525, Final Batch Loss: 0.004715729970484972\n",
      "Epoch 2509, Loss: 0.04262329172343016, Final Batch Loss: 0.030161894857883453\n",
      "Epoch 2510, Loss: 0.017475559376180172, Final Batch Loss: 0.0032860497012734413\n",
      "Epoch 2511, Loss: 0.0366738592274487, Final Batch Loss: 0.0057947696186602116\n",
      "Epoch 2512, Loss: 0.08534381166100502, Final Batch Loss: 0.06367813050746918\n",
      "Epoch 2513, Loss: 0.040412384551018476, Final Batch Loss: 0.005159064661711454\n",
      "Epoch 2514, Loss: 0.035532908514142036, Final Batch Loss: 0.01908455602824688\n",
      "Epoch 2515, Loss: 0.030995049513876438, Final Batch Loss: 0.007956628687679768\n",
      "Epoch 2516, Loss: 0.012186428066343069, Final Batch Loss: 0.008124320767819881\n",
      "Epoch 2517, Loss: 0.045684048905968666, Final Batch Loss: 0.02379395067691803\n",
      "Epoch 2518, Loss: 0.03117340710014105, Final Batch Loss: 0.02221449837088585\n",
      "Epoch 2519, Loss: 0.02403629245236516, Final Batch Loss: 0.004214309621602297\n",
      "Epoch 2520, Loss: 0.0471463818103075, Final Batch Loss: 0.013569874688982964\n",
      "Epoch 2521, Loss: 0.04186758166179061, Final Batch Loss: 0.03684031963348389\n",
      "Epoch 2522, Loss: 0.07322735246270895, Final Batch Loss: 0.009124300442636013\n",
      "Epoch 2523, Loss: 0.042816659435629845, Final Batch Loss: 0.021290961652994156\n",
      "Epoch 2524, Loss: 0.03582008299417794, Final Batch Loss: 0.002642632694914937\n",
      "Epoch 2525, Loss: 0.06369651854038239, Final Batch Loss: 0.009593240916728973\n",
      "Epoch 2526, Loss: 0.019238158827647567, Final Batch Loss: 0.003629577113315463\n",
      "Epoch 2527, Loss: 0.04411578783765435, Final Batch Loss: 0.0029060677625238895\n",
      "Epoch 2528, Loss: 0.012364777969196439, Final Batch Loss: 0.00901471171528101\n",
      "Epoch 2529, Loss: 0.024533245712518692, Final Batch Loss: 0.004346935078501701\n",
      "Epoch 2530, Loss: 0.024079746566712856, Final Batch Loss: 0.016938533633947372\n",
      "Epoch 2531, Loss: 0.011981088668107986, Final Batch Loss: 0.004043188877403736\n",
      "Epoch 2532, Loss: 0.052090040990151465, Final Batch Loss: 0.0018812265479937196\n",
      "Epoch 2533, Loss: 0.03774946369230747, Final Batch Loss: 0.0032147448509931564\n",
      "Epoch 2534, Loss: 0.0176355903968215, Final Batch Loss: 0.0036012763157486916\n",
      "Epoch 2535, Loss: 0.011493935948237777, Final Batch Loss: 0.007727664429694414\n",
      "Epoch 2536, Loss: 0.013199636712670326, Final Batch Loss: 0.006884710397571325\n",
      "Epoch 2537, Loss: 0.023230280727148056, Final Batch Loss: 0.016590308398008347\n",
      "Epoch 2538, Loss: 0.012742577819153666, Final Batch Loss: 0.003694305894896388\n",
      "Epoch 2539, Loss: 0.02885614987462759, Final Batch Loss: 0.007802109234035015\n",
      "Epoch 2540, Loss: 0.02062519360333681, Final Batch Loss: 0.006220291368663311\n",
      "Epoch 2541, Loss: 0.01593669061549008, Final Batch Loss: 0.0024531243834644556\n",
      "Epoch 2542, Loss: 0.036856683902442455, Final Batch Loss: 0.027038441970944405\n",
      "Epoch 2543, Loss: 0.03662703977897763, Final Batch Loss: 0.03163176029920578\n",
      "Epoch 2544, Loss: 0.036046210676431656, Final Batch Loss: 0.019993195310235023\n",
      "Epoch 2545, Loss: 0.012478198390454054, Final Batch Loss: 0.007594116497784853\n",
      "Epoch 2546, Loss: 0.05458777816966176, Final Batch Loss: 0.04762154072523117\n",
      "Epoch 2547, Loss: 0.014883204363286495, Final Batch Loss: 0.007899116724729538\n",
      "Epoch 2548, Loss: 0.04491049610078335, Final Batch Loss: 0.011024454608559608\n",
      "Epoch 2549, Loss: 0.012856058310717344, Final Batch Loss: 0.008997371420264244\n",
      "Epoch 2550, Loss: 0.025829315185546875, Final Batch Loss: 0.011767743155360222\n",
      "Epoch 2551, Loss: 0.08361201081424952, Final Batch Loss: 0.07550235837697983\n",
      "Epoch 2552, Loss: 0.10555313993245363, Final Batch Loss: 0.09298659861087799\n",
      "Epoch 2553, Loss: 0.01369347469881177, Final Batch Loss: 0.009783015586435795\n",
      "Epoch 2554, Loss: 0.02742778731044382, Final Batch Loss: 0.0017784259980544448\n",
      "Epoch 2555, Loss: 0.07440542802214622, Final Batch Loss: 0.0526738241314888\n",
      "Epoch 2556, Loss: 0.039985597133636475, Final Batch Loss: 0.01012328453361988\n",
      "Epoch 2557, Loss: 0.0608588345348835, Final Batch Loss: 0.012475162744522095\n",
      "Epoch 2558, Loss: 0.03475158102810383, Final Batch Loss: 0.017736751586198807\n",
      "Epoch 2559, Loss: 0.028670010156929493, Final Batch Loss: 0.01728377491235733\n",
      "Epoch 2560, Loss: 0.05948483757674694, Final Batch Loss: 0.01611066795885563\n",
      "Epoch 2561, Loss: 0.05399603024125099, Final Batch Loss: 0.019625887274742126\n",
      "Epoch 2562, Loss: 0.11391502618789673, Final Batch Loss: 0.03776898980140686\n",
      "Epoch 2563, Loss: 0.052150149596855044, Final Batch Loss: 0.049389395862817764\n",
      "Epoch 2564, Loss: 0.04449982801452279, Final Batch Loss: 0.037318672984838486\n",
      "Epoch 2565, Loss: 0.023599707521498203, Final Batch Loss: 0.006858608685433865\n",
      "Epoch 2566, Loss: 0.0728306882083416, Final Batch Loss: 0.03063477948307991\n",
      "Epoch 2567, Loss: 0.017044041771441698, Final Batch Loss: 0.006684365216642618\n",
      "Epoch 2568, Loss: 0.04298255778849125, Final Batch Loss: 0.019944822415709496\n",
      "Epoch 2569, Loss: 0.07343344925902784, Final Batch Loss: 0.002755602588877082\n",
      "Epoch 2570, Loss: 0.030793312238529325, Final Batch Loss: 0.0026770990807563066\n",
      "Epoch 2571, Loss: 0.0506597850471735, Final Batch Loss: 0.046718329191207886\n",
      "Epoch 2572, Loss: 0.04226650111377239, Final Batch Loss: 0.0247150007635355\n",
      "Epoch 2573, Loss: 0.04928038269281387, Final Batch Loss: 0.010470002889633179\n",
      "Epoch 2574, Loss: 0.04237067233771086, Final Batch Loss: 0.007221738807857037\n",
      "Epoch 2575, Loss: 0.013419575057923794, Final Batch Loss: 0.00797615759074688\n",
      "Epoch 2576, Loss: 0.03947001043707132, Final Batch Loss: 0.005706106312572956\n",
      "Epoch 2577, Loss: 0.01182070467621088, Final Batch Loss: 0.007587100379168987\n",
      "Epoch 2578, Loss: 0.030130506493151188, Final Batch Loss: 0.013060302473604679\n",
      "Epoch 2579, Loss: 0.019816895481199026, Final Batch Loss: 0.00428828364238143\n",
      "Epoch 2580, Loss: 0.06087334267795086, Final Batch Loss: 0.019561218097805977\n",
      "Epoch 2581, Loss: 0.033196969889104366, Final Batch Loss: 0.01962578482925892\n",
      "Epoch 2582, Loss: 0.029778565280139446, Final Batch Loss: 0.015782713890075684\n",
      "Epoch 2583, Loss: 0.01609451835975051, Final Batch Loss: 0.007751400116831064\n",
      "Epoch 2584, Loss: 0.023696075193583965, Final Batch Loss: 0.005680692382156849\n",
      "Epoch 2585, Loss: 0.03522940445691347, Final Batch Loss: 0.026672136038541794\n",
      "Epoch 2586, Loss: 0.021648913621902466, Final Batch Loss: 0.006640235893428326\n",
      "Epoch 2587, Loss: 0.014270546846091747, Final Batch Loss: 0.00513797253370285\n",
      "Epoch 2588, Loss: 0.025530537590384483, Final Batch Loss: 0.0141835268586874\n",
      "Epoch 2589, Loss: 0.060163021087646484, Final Batch Loss: 0.04069346562027931\n",
      "Epoch 2590, Loss: 0.031465234234929085, Final Batch Loss: 0.021027550101280212\n",
      "Epoch 2591, Loss: 0.019414422567933798, Final Batch Loss: 0.015375945717096329\n",
      "Epoch 2592, Loss: 0.023978546261787415, Final Batch Loss: 0.012597332708537579\n",
      "Epoch 2593, Loss: 0.019669162575155497, Final Batch Loss: 0.0042863464914262295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2594, Loss: 0.03682689415290952, Final Batch Loss: 0.031163018196821213\n",
      "Epoch 2595, Loss: 0.02669464936479926, Final Batch Loss: 0.007615334820002317\n",
      "Epoch 2596, Loss: 0.021928129252046347, Final Batch Loss: 0.015434336848556995\n",
      "Epoch 2597, Loss: 0.0137376650236547, Final Batch Loss: 0.008361286483705044\n",
      "Epoch 2598, Loss: 0.062226552516222, Final Batch Loss: 0.03178677335381508\n",
      "Epoch 2599, Loss: 0.0072430893778800964, Final Batch Loss: 0.0031262137927114964\n",
      "Epoch 2600, Loss: 0.01753264549188316, Final Batch Loss: 0.0034757989924401045\n",
      "Epoch 2601, Loss: 0.016210522036999464, Final Batch Loss: 0.00989485066384077\n",
      "Epoch 2602, Loss: 0.03825587220489979, Final Batch Loss: 0.030148623511195183\n",
      "Epoch 2603, Loss: 0.04827386140823364, Final Batch Loss: 0.00997747853398323\n",
      "Epoch 2604, Loss: 0.012886146316304803, Final Batch Loss: 0.003317914204671979\n",
      "Epoch 2605, Loss: 0.012582734692841768, Final Batch Loss: 0.004838414955884218\n",
      "Epoch 2606, Loss: 0.01645089266821742, Final Batch Loss: 0.005324638914316893\n",
      "Epoch 2607, Loss: 0.029268978163599968, Final Batch Loss: 0.01389244943857193\n",
      "Epoch 2608, Loss: 0.044242940843105316, Final Batch Loss: 0.03854665160179138\n",
      "Epoch 2609, Loss: 0.036964986473321915, Final Batch Loss: 0.017054397612810135\n",
      "Epoch 2610, Loss: 0.02082521701231599, Final Batch Loss: 0.014046700671315193\n",
      "Epoch 2611, Loss: 0.021577492356300354, Final Batch Loss: 0.013611748814582825\n",
      "Epoch 2612, Loss: 0.012431962881237268, Final Batch Loss: 0.005301984492689371\n",
      "Epoch 2613, Loss: 0.06680203415453434, Final Batch Loss: 0.009210368618369102\n",
      "Epoch 2614, Loss: 0.01177396485581994, Final Batch Loss: 0.004252253100275993\n",
      "Epoch 2615, Loss: 0.014261703006923199, Final Batch Loss: 0.0037632109597325325\n",
      "Epoch 2616, Loss: 0.031864640302956104, Final Batch Loss: 0.017031725496053696\n",
      "Epoch 2617, Loss: 0.0161271458491683, Final Batch Loss: 0.011737597174942493\n",
      "Epoch 2618, Loss: 0.021062176674604416, Final Batch Loss: 0.0049262698739767075\n",
      "Epoch 2619, Loss: 0.02463886234909296, Final Batch Loss: 0.00456602219492197\n",
      "Epoch 2620, Loss: 0.02363484725356102, Final Batch Loss: 0.012214424088597298\n",
      "Epoch 2621, Loss: 0.02456754306331277, Final Batch Loss: 0.006577643100172281\n",
      "Epoch 2622, Loss: 0.020475835539400578, Final Batch Loss: 0.00902489572763443\n",
      "Epoch 2623, Loss: 0.018215499818325043, Final Batch Loss: 0.0043207695707678795\n",
      "Epoch 2624, Loss: 0.01856451714411378, Final Batch Loss: 0.003626026678830385\n",
      "Epoch 2625, Loss: 0.021034891717135906, Final Batch Loss: 0.012731067836284637\n",
      "Epoch 2626, Loss: 0.022346759331412613, Final Batch Loss: 0.0012191013665869832\n",
      "Epoch 2627, Loss: 0.02342736627906561, Final Batch Loss: 0.005295685492455959\n",
      "Epoch 2628, Loss: 0.025785677134990692, Final Batch Loss: 0.018337935209274292\n",
      "Epoch 2629, Loss: 0.01193236606195569, Final Batch Loss: 0.0026542642153799534\n",
      "Epoch 2630, Loss: 0.021812729071825743, Final Batch Loss: 0.004419249948114157\n",
      "Epoch 2631, Loss: 0.012373290723189712, Final Batch Loss: 0.0035157667007297277\n",
      "Epoch 2632, Loss: 0.031243671663105488, Final Batch Loss: 0.013727988116443157\n",
      "Epoch 2633, Loss: 0.03054545260965824, Final Batch Loss: 0.005888177081942558\n",
      "Epoch 2634, Loss: 0.11341138742864132, Final Batch Loss: 0.01632808707654476\n",
      "Epoch 2635, Loss: 0.042217278853058815, Final Batch Loss: 0.02347971312701702\n",
      "Epoch 2636, Loss: 0.015959170181304216, Final Batch Loss: 0.005320367868989706\n",
      "Epoch 2637, Loss: 0.03398806229233742, Final Batch Loss: 0.02299337089061737\n",
      "Epoch 2638, Loss: 0.041530551854521036, Final Batch Loss: 0.006534295622259378\n",
      "Epoch 2639, Loss: 0.018176688812673092, Final Batch Loss: 0.008923506364226341\n",
      "Epoch 2640, Loss: 0.021021868102252483, Final Batch Loss: 0.004572534002363682\n",
      "Epoch 2641, Loss: 0.029930620454251766, Final Batch Loss: 0.01611044816672802\n",
      "Epoch 2642, Loss: 0.011709396727383137, Final Batch Loss: 0.00464908080175519\n",
      "Epoch 2643, Loss: 0.0189516544342041, Final Batch Loss: 0.012979901395738125\n",
      "Epoch 2644, Loss: 0.07744467072188854, Final Batch Loss: 0.046910591423511505\n",
      "Epoch 2645, Loss: 0.03199068270623684, Final Batch Loss: 0.009294267743825912\n",
      "Epoch 2646, Loss: 0.01704895356670022, Final Batch Loss: 0.004567313473671675\n",
      "Epoch 2647, Loss: 0.016996163874864578, Final Batch Loss: 0.012208975851535797\n",
      "Epoch 2648, Loss: 0.041923245415091515, Final Batch Loss: 0.02384881302714348\n",
      "Epoch 2649, Loss: 0.027117779478430748, Final Batch Loss: 0.016423044726252556\n",
      "Epoch 2650, Loss: 0.012488733511418104, Final Batch Loss: 0.00768817774951458\n",
      "Epoch 2651, Loss: 0.031463246792554855, Final Batch Loss: 0.009993501007556915\n",
      "Epoch 2652, Loss: 0.026384033728390932, Final Batch Loss: 0.019403913989663124\n",
      "Epoch 2653, Loss: 0.04958490468561649, Final Batch Loss: 0.019408948719501495\n",
      "Epoch 2654, Loss: 0.05111313238739967, Final Batch Loss: 0.030183585360646248\n",
      "Epoch 2655, Loss: 0.0383271723985672, Final Batch Loss: 0.021477095782756805\n",
      "Epoch 2656, Loss: 0.012153966818004847, Final Batch Loss: 0.005118304397910833\n",
      "Epoch 2657, Loss: 0.016713404562324286, Final Batch Loss: 0.010104435496032238\n",
      "Epoch 2658, Loss: 0.016821402590721846, Final Batch Loss: 0.005059553775936365\n",
      "Epoch 2659, Loss: 0.010957125574350357, Final Batch Loss: 0.005156261846423149\n",
      "Epoch 2660, Loss: 0.02717402298003435, Final Batch Loss: 0.004819552414119244\n",
      "Epoch 2661, Loss: 0.013452637940645218, Final Batch Loss: 0.003970852121710777\n",
      "Epoch 2662, Loss: 0.02691793628036976, Final Batch Loss: 0.022506998851895332\n",
      "Epoch 2663, Loss: 0.023377683479338884, Final Batch Loss: 0.015658099204301834\n",
      "Epoch 2664, Loss: 0.014107795432209969, Final Batch Loss: 0.007269434165209532\n",
      "Epoch 2665, Loss: 0.029924329835921526, Final Batch Loss: 0.023993683978915215\n",
      "Epoch 2666, Loss: 0.06142897065728903, Final Batch Loss: 0.00719971489161253\n",
      "Epoch 2667, Loss: 0.017037166748195887, Final Batch Loss: 0.010783521458506584\n",
      "Epoch 2668, Loss: 0.1242460161447525, Final Batch Loss: 0.10164161771535873\n",
      "Epoch 2669, Loss: 0.05172338709235191, Final Batch Loss: 0.04752231761813164\n",
      "Epoch 2670, Loss: 0.09474496357142925, Final Batch Loss: 0.06899186968803406\n",
      "Epoch 2671, Loss: 0.030542441178113222, Final Batch Loss: 0.006168922875076532\n",
      "Epoch 2672, Loss: 0.1301405243575573, Final Batch Loss: 0.09461254626512527\n",
      "Epoch 2673, Loss: 0.06620749086141586, Final Batch Loss: 0.026233989745378494\n",
      "Epoch 2674, Loss: 0.008820653893053532, Final Batch Loss: 0.004200457129627466\n",
      "Epoch 2675, Loss: 0.08341715484857559, Final Batch Loss: 0.030471235513687134\n",
      "Epoch 2676, Loss: 0.06886111758649349, Final Batch Loss: 0.05000757426023483\n",
      "Epoch 2677, Loss: 0.24102295190095901, Final Batch Loss: 0.14556992053985596\n",
      "Epoch 2678, Loss: 0.09606223367154598, Final Batch Loss: 0.08107706159353256\n",
      "Epoch 2679, Loss: 0.5025935918092728, Final Batch Loss: 0.13378824293613434\n",
      "Epoch 2680, Loss: 0.11635707691311836, Final Batch Loss: 0.03399203345179558\n",
      "Epoch 2681, Loss: 0.19543633610010147, Final Batch Loss: 0.11546206474304199\n",
      "Epoch 2682, Loss: 0.1487431451678276, Final Batch Loss: 0.06352195888757706\n",
      "Epoch 2683, Loss: 0.15138313546776772, Final Batch Loss: 0.04708635434508324\n",
      "Epoch 2684, Loss: 0.08090968430042267, Final Batch Loss: 0.03500404953956604\n",
      "Epoch 2685, Loss: 0.09622426889836788, Final Batch Loss: 0.030995404347777367\n",
      "Epoch 2686, Loss: 0.16619762778282166, Final Batch Loss: 0.09153646975755692\n",
      "Epoch 2687, Loss: 0.02810294576920569, Final Batch Loss: 0.0037810245994478464\n",
      "Epoch 2688, Loss: 0.15388022735714912, Final Batch Loss: 0.11303240060806274\n",
      "Epoch 2689, Loss: 0.07438660599291325, Final Batch Loss: 0.01697632484138012\n",
      "Epoch 2690, Loss: 0.08920046966522932, Final Batch Loss: 0.012253965251147747\n",
      "Epoch 2691, Loss: 0.05707571841776371, Final Batch Loss: 0.02203923650085926\n",
      "Epoch 2692, Loss: 0.08297505602240562, Final Batch Loss: 0.06621269136667252\n",
      "Epoch 2693, Loss: 0.07035253755748272, Final Batch Loss: 0.058849673718214035\n",
      "Epoch 2694, Loss: 0.031490265391767025, Final Batch Loss: 0.015233521349728107\n",
      "Epoch 2695, Loss: 0.045965058729052544, Final Batch Loss: 0.012379081919789314\n",
      "Epoch 2696, Loss: 0.06080393213778734, Final Batch Loss: 0.051366522908210754\n",
      "Epoch 2697, Loss: 0.053610945120453835, Final Batch Loss: 0.019100790843367577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2698, Loss: 0.08363895490765572, Final Batch Loss: 0.05770207196474075\n",
      "Epoch 2699, Loss: 0.034017172176390886, Final Batch Loss: 0.00730171101167798\n",
      "Epoch 2700, Loss: 0.06721422076225281, Final Batch Loss: 0.03138824179768562\n",
      "Epoch 2701, Loss: 0.025444969069212675, Final Batch Loss: 0.01887870579957962\n",
      "Epoch 2702, Loss: 0.02015894651412964, Final Batch Loss: 0.011590610258281231\n",
      "Epoch 2703, Loss: 0.04016048088669777, Final Batch Loss: 0.030006080865859985\n",
      "Epoch 2704, Loss: 0.012323966715484858, Final Batch Loss: 0.004848681855946779\n",
      "Epoch 2705, Loss: 0.023579860106110573, Final Batch Loss: 0.01586172729730606\n",
      "Epoch 2706, Loss: 0.02523114625364542, Final Batch Loss: 0.004198662005364895\n",
      "Epoch 2707, Loss: 0.0311443954706192, Final Batch Loss: 0.019267555326223373\n",
      "Epoch 2708, Loss: 0.06775346770882607, Final Batch Loss: 0.03650837764143944\n",
      "Epoch 2709, Loss: 0.0344031723216176, Final Batch Loss: 0.02519410289824009\n",
      "Epoch 2710, Loss: 0.08400358632206917, Final Batch Loss: 0.06417947262525558\n",
      "Epoch 2711, Loss: 0.08242669189348817, Final Batch Loss: 0.006806033197790384\n",
      "Epoch 2712, Loss: 0.06207934767007828, Final Batch Loss: 0.030251014977693558\n",
      "Epoch 2713, Loss: 0.05586280580610037, Final Batch Loss: 0.01448731403797865\n",
      "Epoch 2714, Loss: 0.039403341710567474, Final Batch Loss: 0.017790131270885468\n",
      "Epoch 2715, Loss: 0.02627380471676588, Final Batch Loss: 0.02124282345175743\n",
      "Epoch 2716, Loss: 0.0237285690382123, Final Batch Loss: 0.016583295539021492\n",
      "Epoch 2717, Loss: 0.06040072441101074, Final Batch Loss: 0.02262953296303749\n",
      "Epoch 2718, Loss: 0.01804505567997694, Final Batch Loss: 0.007005857303738594\n",
      "Epoch 2719, Loss: 0.05668673850595951, Final Batch Loss: 0.022933920845389366\n",
      "Epoch 2720, Loss: 0.050229186192154884, Final Batch Loss: 0.04012751951813698\n",
      "Epoch 2721, Loss: 0.0401141420006752, Final Batch Loss: 0.021653244271874428\n",
      "Epoch 2722, Loss: 0.01469583110883832, Final Batch Loss: 0.005430025514215231\n",
      "Epoch 2723, Loss: 0.02941147657111287, Final Batch Loss: 0.007138772401958704\n",
      "Epoch 2724, Loss: 0.040284350514411926, Final Batch Loss: 0.02769980952143669\n",
      "Epoch 2725, Loss: 0.019525631330907345, Final Batch Loss: 0.006504097022116184\n",
      "Epoch 2726, Loss: 0.0694115711376071, Final Batch Loss: 0.054185740649700165\n",
      "Epoch 2727, Loss: 0.013836277183145285, Final Batch Loss: 0.005814173724502325\n",
      "Epoch 2728, Loss: 0.038404034450650215, Final Batch Loss: 0.019857393577694893\n",
      "Epoch 2729, Loss: 0.028556992299854755, Final Batch Loss: 0.010788868181407452\n",
      "Epoch 2730, Loss: 0.04005224257707596, Final Batch Loss: 0.026921622455120087\n",
      "Epoch 2731, Loss: 0.02312034834176302, Final Batch Loss: 0.00793416053056717\n",
      "Epoch 2732, Loss: 0.011560747399926186, Final Batch Loss: 0.005231781397014856\n",
      "Epoch 2733, Loss: 0.032848868519067764, Final Batch Loss: 0.007988996803760529\n",
      "Epoch 2734, Loss: 0.025866772048175335, Final Batch Loss: 0.013765531592071056\n",
      "Epoch 2735, Loss: 0.027166337706148624, Final Batch Loss: 0.015039561316370964\n",
      "Epoch 2736, Loss: 0.022218123078346252, Final Batch Loss: 0.012517738156020641\n",
      "Epoch 2737, Loss: 0.024946600198745728, Final Batch Loss: 0.014455504715442657\n",
      "Epoch 2738, Loss: 0.0319727947935462, Final Batch Loss: 0.015151056461036205\n",
      "Epoch 2739, Loss: 0.050217537209391594, Final Batch Loss: 0.03619421645998955\n",
      "Epoch 2740, Loss: 0.015411731321364641, Final Batch Loss: 0.006159413140267134\n",
      "Epoch 2741, Loss: 0.013556078542023897, Final Batch Loss: 0.00588848115876317\n",
      "Epoch 2742, Loss: 0.08724893163889647, Final Batch Loss: 0.07487740367650986\n",
      "Epoch 2743, Loss: 0.05736026354134083, Final Batch Loss: 0.03511069342494011\n",
      "Epoch 2744, Loss: 0.031088748015463352, Final Batch Loss: 0.007526417262852192\n",
      "Epoch 2745, Loss: 0.035100690089166164, Final Batch Loss: 0.02668614313006401\n",
      "Epoch 2746, Loss: 0.023802790325134993, Final Batch Loss: 0.016857029870152473\n",
      "Epoch 2747, Loss: 0.020138531923294067, Final Batch Loss: 0.003151543438434601\n",
      "Epoch 2748, Loss: 0.1224641241133213, Final Batch Loss: 0.08383556455373764\n",
      "Epoch 2749, Loss: 0.04235463961958885, Final Batch Loss: 0.03387240320444107\n",
      "Epoch 2750, Loss: 0.02550212573260069, Final Batch Loss: 0.019133254885673523\n",
      "Epoch 2751, Loss: 0.010781403165310621, Final Batch Loss: 0.005370886996388435\n",
      "Epoch 2752, Loss: 0.017338658217340708, Final Batch Loss: 0.005972057115286589\n",
      "Epoch 2753, Loss: 0.03959005419164896, Final Batch Loss: 0.008030601777136326\n",
      "Epoch 2754, Loss: 0.02663293294608593, Final Batch Loss: 0.013592066243290901\n",
      "Epoch 2755, Loss: 0.020685742143541574, Final Batch Loss: 0.0065754917450249195\n",
      "Epoch 2756, Loss: 0.0236716503277421, Final Batch Loss: 0.01482529379427433\n",
      "Epoch 2757, Loss: 0.027726764790713787, Final Batch Loss: 0.017032966017723083\n",
      "Epoch 2758, Loss: 0.05557895265519619, Final Batch Loss: 0.026178983971476555\n",
      "Epoch 2759, Loss: 0.021475248970091343, Final Batch Loss: 0.001981985755264759\n",
      "Epoch 2760, Loss: 0.05164290592074394, Final Batch Loss: 0.03883682191371918\n",
      "Epoch 2761, Loss: 0.008881894405931234, Final Batch Loss: 0.005213461816310883\n",
      "Epoch 2762, Loss: 0.024769948329776525, Final Batch Loss: 0.00571354990825057\n",
      "Epoch 2763, Loss: 0.01615664968267083, Final Batch Loss: 0.009278401732444763\n",
      "Epoch 2764, Loss: 0.010461108526214957, Final Batch Loss: 0.0038955409545451403\n",
      "Epoch 2765, Loss: 0.013414588756859303, Final Batch Loss: 0.007407008204609156\n",
      "Epoch 2766, Loss: 0.06337768025696278, Final Batch Loss: 0.03368525579571724\n",
      "Epoch 2767, Loss: 0.08474404364824295, Final Batch Loss: 0.0637705847620964\n",
      "Epoch 2768, Loss: 0.0057361735962331295, Final Batch Loss: 0.002846113173291087\n",
      "Epoch 2769, Loss: 0.05227578990161419, Final Batch Loss: 0.031686823815107346\n",
      "Epoch 2770, Loss: 0.014182072598487139, Final Batch Loss: 0.007611841429024935\n",
      "Epoch 2771, Loss: 0.027267908910289407, Final Batch Loss: 0.023731615394353867\n",
      "Epoch 2772, Loss: 0.0674548801034689, Final Batch Loss: 0.04180222377181053\n",
      "Epoch 2773, Loss: 0.020675793290138245, Final Batch Loss: 0.009504811838269234\n",
      "Epoch 2774, Loss: 0.022152765654027462, Final Batch Loss: 0.003822523169219494\n",
      "Epoch 2775, Loss: 0.017395473085343838, Final Batch Loss: 0.013614644296467304\n",
      "Epoch 2776, Loss: 0.03467828826978803, Final Batch Loss: 0.027145707979798317\n",
      "Epoch 2777, Loss: 0.03281858377158642, Final Batch Loss: 0.0037429500371217728\n",
      "Epoch 2778, Loss: 0.009446947369724512, Final Batch Loss: 0.006324957124888897\n",
      "Epoch 2779, Loss: 0.02366226352751255, Final Batch Loss: 0.019151220098137856\n",
      "Epoch 2780, Loss: 0.01798229943960905, Final Batch Loss: 0.008339975029230118\n",
      "Epoch 2781, Loss: 0.025111103430390358, Final Batch Loss: 0.02152024768292904\n",
      "Epoch 2782, Loss: 0.022980251000262797, Final Batch Loss: 0.0015676120528951287\n",
      "Epoch 2783, Loss: 0.016505584586411715, Final Batch Loss: 0.005232615862041712\n",
      "Epoch 2784, Loss: 0.02163190394639969, Final Batch Loss: 0.01449914462864399\n",
      "Epoch 2785, Loss: 0.008927227463573217, Final Batch Loss: 0.0045589059591293335\n",
      "Epoch 2786, Loss: 0.011438982095569372, Final Batch Loss: 0.004482213407754898\n",
      "Epoch 2787, Loss: 0.05173002555966377, Final Batch Loss: 0.011531263589859009\n",
      "Epoch 2788, Loss: 0.01241609314456582, Final Batch Loss: 0.00876760296523571\n",
      "Epoch 2789, Loss: 0.046223029494285583, Final Batch Loss: 0.017347371205687523\n",
      "Epoch 2790, Loss: 0.08715399913489819, Final Batch Loss: 0.062493544071912766\n",
      "Epoch 2791, Loss: 0.1255483776330948, Final Batch Loss: 0.09044083952903748\n",
      "Epoch 2792, Loss: 0.290329460054636, Final Batch Loss: 0.059839654713869095\n",
      "Epoch 2793, Loss: 0.11311031505465508, Final Batch Loss: 0.04990427568554878\n",
      "Epoch 2794, Loss: 0.12204284965991974, Final Batch Loss: 0.08395431935787201\n",
      "Epoch 2795, Loss: 0.07369455695152283, Final Batch Loss: 0.028461407870054245\n",
      "Epoch 2796, Loss: 0.1251860223710537, Final Batch Loss: 0.11196558177471161\n",
      "Epoch 2797, Loss: 0.08097242005169392, Final Batch Loss: 0.01819305308163166\n",
      "Epoch 2798, Loss: 0.02949862787500024, Final Batch Loss: 0.023063326254487038\n",
      "Epoch 2799, Loss: 0.06180434115231037, Final Batch Loss: 0.010839583352208138\n",
      "Epoch 2800, Loss: 0.054410653188824654, Final Batch Loss: 0.029683951288461685\n",
      "Epoch 2801, Loss: 0.032700920943170786, Final Batch Loss: 0.025587882846593857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2802, Loss: 0.07370377331972122, Final Batch Loss: 0.05431750416755676\n",
      "Epoch 2803, Loss: 0.0620739571750164, Final Batch Loss: 0.04568850249052048\n",
      "Epoch 2804, Loss: 0.016832332592457533, Final Batch Loss: 0.006724535021930933\n",
      "Epoch 2805, Loss: 0.02946986025199294, Final Batch Loss: 0.024861112236976624\n",
      "Epoch 2806, Loss: 0.01376319583505392, Final Batch Loss: 0.0077500119805336\n",
      "Epoch 2807, Loss: 0.05224751867353916, Final Batch Loss: 0.025029411539435387\n",
      "Epoch 2808, Loss: 0.0490235835313797, Final Batch Loss: 0.034678295254707336\n",
      "Epoch 2809, Loss: 0.024737888015806675, Final Batch Loss: 0.019878698512911797\n",
      "Epoch 2810, Loss: 0.05717398226261139, Final Batch Loss: 0.02025790512561798\n",
      "Epoch 2811, Loss: 0.011596839409321547, Final Batch Loss: 0.006059876177459955\n",
      "Epoch 2812, Loss: 0.0381257776170969, Final Batch Loss: 0.03128676116466522\n",
      "Epoch 2813, Loss: 0.021142887882888317, Final Batch Loss: 0.01743464358150959\n",
      "Epoch 2814, Loss: 0.02846010960638523, Final Batch Loss: 0.005921021103858948\n",
      "Epoch 2815, Loss: 0.06438218802213669, Final Batch Loss: 0.04116545617580414\n",
      "Epoch 2816, Loss: 0.012487982399761677, Final Batch Loss: 0.005640081595629454\n",
      "Epoch 2817, Loss: 0.023056797683238983, Final Batch Loss: 0.010308659635484219\n",
      "Epoch 2818, Loss: 0.014027335215359926, Final Batch Loss: 0.004636271391063929\n",
      "Epoch 2819, Loss: 0.03358999267220497, Final Batch Loss: 0.022063948214054108\n",
      "Epoch 2820, Loss: 0.06825814209878445, Final Batch Loss: 0.04100396856665611\n",
      "Epoch 2821, Loss: 0.026215415447950363, Final Batch Loss: 0.010635118000209332\n",
      "Epoch 2822, Loss: 0.036783517338335514, Final Batch Loss: 0.024545451626181602\n",
      "Epoch 2823, Loss: 0.020708010299131274, Final Batch Loss: 0.0034764434676617384\n",
      "Epoch 2824, Loss: 0.08320971950888634, Final Batch Loss: 0.05579754337668419\n",
      "Epoch 2825, Loss: 0.014619209337979555, Final Batch Loss: 0.006403431762009859\n",
      "Epoch 2826, Loss: 0.022799108177423477, Final Batch Loss: 0.017443591728806496\n",
      "Epoch 2827, Loss: 0.0271925984416157, Final Batch Loss: 0.0033269173000007868\n",
      "Epoch 2828, Loss: 0.026466338895261288, Final Batch Loss: 0.011228007264435291\n",
      "Epoch 2829, Loss: 0.031377182342112064, Final Batch Loss: 0.009295648895204067\n",
      "Epoch 2830, Loss: 0.04328443156555295, Final Batch Loss: 0.03605946898460388\n",
      "Epoch 2831, Loss: 0.04453488625586033, Final Batch Loss: 0.011557700112462044\n",
      "Epoch 2832, Loss: 0.043648323277011514, Final Batch Loss: 0.003889108309522271\n",
      "Epoch 2833, Loss: 0.03040516935288906, Final Batch Loss: 0.0084913931787014\n",
      "Epoch 2834, Loss: 0.11737704835832119, Final Batch Loss: 0.11085671186447144\n",
      "Epoch 2835, Loss: 0.024626308120787144, Final Batch Loss: 0.012737814337015152\n",
      "Epoch 2836, Loss: 0.0321893529035151, Final Batch Loss: 0.00484555633738637\n",
      "Epoch 2837, Loss: 0.021213604137301445, Final Batch Loss: 0.01391257718205452\n",
      "Epoch 2838, Loss: 0.07941045146435499, Final Batch Loss: 0.06740503758192062\n",
      "Epoch 2839, Loss: 0.04449650412425399, Final Batch Loss: 0.003612601663917303\n",
      "Epoch 2840, Loss: 0.0413654213771224, Final Batch Loss: 0.028300713747739792\n",
      "Epoch 2841, Loss: 0.04216889198869467, Final Batch Loss: 0.03331228718161583\n",
      "Epoch 2842, Loss: 0.028794362442567945, Final Batch Loss: 0.0037846353370696306\n",
      "Epoch 2843, Loss: 0.021802383475005627, Final Batch Loss: 0.01667596586048603\n",
      "Epoch 2844, Loss: 0.019554488360881805, Final Batch Loss: 0.007336885668337345\n",
      "Epoch 2845, Loss: 0.023282102774828672, Final Batch Loss: 0.0051492671482264996\n",
      "Epoch 2846, Loss: 0.04812883771955967, Final Batch Loss: 0.019502924755215645\n",
      "Epoch 2847, Loss: 0.05297404620796442, Final Batch Loss: 0.008951847441494465\n",
      "Epoch 2848, Loss: 0.05096070282161236, Final Batch Loss: 0.03778822347521782\n",
      "Epoch 2849, Loss: 0.028142175637185574, Final Batch Loss: 0.01733921654522419\n",
      "Epoch 2850, Loss: 0.05656744726002216, Final Batch Loss: 0.029550090432167053\n",
      "Epoch 2851, Loss: 0.043115176260471344, Final Batch Loss: 0.012122776359319687\n",
      "Epoch 2852, Loss: 0.07540156692266464, Final Batch Loss: 0.043610829859972\n",
      "Epoch 2853, Loss: 0.015702828764915466, Final Batch Loss: 0.009636902250349522\n",
      "Epoch 2854, Loss: 0.07997233234345913, Final Batch Loss: 0.005993572995066643\n",
      "Epoch 2855, Loss: 0.03575253672897816, Final Batch Loss: 0.025900378823280334\n",
      "Epoch 2856, Loss: 0.03534333594143391, Final Batch Loss: 0.007094951346516609\n",
      "Epoch 2857, Loss: 0.010983708780258894, Final Batch Loss: 0.006314410362392664\n",
      "Epoch 2858, Loss: 0.03762861713767052, Final Batch Loss: 0.02019537426531315\n",
      "Epoch 2859, Loss: 0.034881217405200005, Final Batch Loss: 0.028611866757273674\n",
      "Epoch 2860, Loss: 0.05767929553985596, Final Batch Loss: 0.014718487858772278\n",
      "Epoch 2861, Loss: 0.038873436860740185, Final Batch Loss: 0.025167999789118767\n",
      "Epoch 2862, Loss: 0.047972981818020344, Final Batch Loss: 0.04154829680919647\n",
      "Epoch 2863, Loss: 0.016902818344533443, Final Batch Loss: 0.0035803616046905518\n",
      "Epoch 2864, Loss: 0.02087247557938099, Final Batch Loss: 0.004646960645914078\n",
      "Epoch 2865, Loss: 0.06906390935182571, Final Batch Loss: 0.0382101833820343\n",
      "Epoch 2866, Loss: 0.022392285987734795, Final Batch Loss: 0.01195899210870266\n",
      "Epoch 2867, Loss: 0.027601715177297592, Final Batch Loss: 0.010714102536439896\n",
      "Epoch 2868, Loss: 0.06409959075972438, Final Batch Loss: 0.004396296571940184\n",
      "Epoch 2869, Loss: 0.0291952402330935, Final Batch Loss: 0.006720720324665308\n",
      "Epoch 2870, Loss: 0.031173001043498516, Final Batch Loss: 0.015440455637872219\n",
      "Epoch 2871, Loss: 0.009940291289240122, Final Batch Loss: 0.004489182028919458\n",
      "Epoch 2872, Loss: 0.12078038975596428, Final Batch Loss: 0.0493936650454998\n",
      "Epoch 2873, Loss: 0.06649906933307648, Final Batch Loss: 0.022148020565509796\n",
      "Epoch 2874, Loss: 0.017112698405981064, Final Batch Loss: 0.006290234625339508\n",
      "Epoch 2875, Loss: 0.08908024337142706, Final Batch Loss: 0.07822861522436142\n",
      "Epoch 2876, Loss: 0.049113402143120766, Final Batch Loss: 0.03790103644132614\n",
      "Epoch 2877, Loss: 0.038984809536486864, Final Batch Loss: 0.006650101859122515\n",
      "Epoch 2878, Loss: 0.050795840099453926, Final Batch Loss: 0.01853877864778042\n",
      "Epoch 2879, Loss: 0.01809313939884305, Final Batch Loss: 0.004938023630529642\n",
      "Epoch 2880, Loss: 0.11391918361186981, Final Batch Loss: 0.019458763301372528\n",
      "Epoch 2881, Loss: 0.02438670489937067, Final Batch Loss: 0.01632753200829029\n",
      "Epoch 2882, Loss: 0.027882162481546402, Final Batch Loss: 0.013053025119006634\n",
      "Epoch 2883, Loss: 0.025012610480189323, Final Batch Loss: 0.021854886785149574\n",
      "Epoch 2884, Loss: 0.0472716735675931, Final Batch Loss: 0.03861137107014656\n",
      "Epoch 2885, Loss: 0.030671875458210707, Final Batch Loss: 0.022933386266231537\n",
      "Epoch 2886, Loss: 0.04685181751847267, Final Batch Loss: 0.02164030633866787\n",
      "Epoch 2887, Loss: 0.02314246050082147, Final Batch Loss: 0.0037361138965934515\n",
      "Epoch 2888, Loss: 0.013912655878812075, Final Batch Loss: 0.005299063865095377\n",
      "Epoch 2889, Loss: 0.0225309943780303, Final Batch Loss: 0.005839924328029156\n",
      "Epoch 2890, Loss: 0.03053772309795022, Final Batch Loss: 0.022930266335606575\n",
      "Epoch 2891, Loss: 0.04735104739665985, Final Batch Loss: 0.02784915454685688\n",
      "Epoch 2892, Loss: 0.05009838240221143, Final Batch Loss: 0.04611729085445404\n",
      "Epoch 2893, Loss: 0.024663759395480156, Final Batch Loss: 0.007936151698231697\n",
      "Epoch 2894, Loss: 0.03568102326244116, Final Batch Loss: 0.02735750563442707\n",
      "Epoch 2895, Loss: 0.06407118728384376, Final Batch Loss: 0.05924038216471672\n",
      "Epoch 2896, Loss: 0.02604981977492571, Final Batch Loss: 0.015039880760014057\n",
      "Epoch 2897, Loss: 0.01918488834053278, Final Batch Loss: 0.008145026862621307\n",
      "Epoch 2898, Loss: 0.06572500057518482, Final Batch Loss: 0.031026730313897133\n",
      "Epoch 2899, Loss: 0.022330110892653465, Final Batch Loss: 0.008029095828533173\n",
      "Epoch 2900, Loss: 0.01698189228773117, Final Batch Loss: 0.009637074545025826\n",
      "Epoch 2901, Loss: 0.012476654257625341, Final Batch Loss: 0.008463083766400814\n",
      "Epoch 2902, Loss: 0.020808341912925243, Final Batch Loss: 0.006243931129574776\n",
      "Epoch 2903, Loss: 0.030700810253620148, Final Batch Loss: 0.02183612994849682\n",
      "Epoch 2904, Loss: 0.06516805663704872, Final Batch Loss: 0.011896464973688126\n",
      "Epoch 2905, Loss: 0.01989205344580114, Final Batch Loss: 0.003673679893836379\n",
      "Epoch 2906, Loss: 0.02004967234097421, Final Batch Loss: 0.0033700631465762854\n",
      "Epoch 2907, Loss: 0.01394124049693346, Final Batch Loss: 0.007303671445697546\n",
      "Epoch 2908, Loss: 0.026762136723846197, Final Batch Loss: 0.021292181685566902\n",
      "Epoch 2909, Loss: 0.013617784017696977, Final Batch Loss: 0.003830553265288472\n",
      "Epoch 2910, Loss: 0.03044879622757435, Final Batch Loss: 0.013539647683501244\n",
      "Epoch 2911, Loss: 0.0199937978759408, Final Batch Loss: 0.005222523584961891\n",
      "Epoch 2912, Loss: 0.02533537894487381, Final Batch Loss: 0.01679072342813015\n",
      "Epoch 2913, Loss: 0.009418866131454706, Final Batch Loss: 0.005925721954554319\n",
      "Epoch 2914, Loss: 0.056096707470715046, Final Batch Loss: 0.047516610473394394\n",
      "Epoch 2915, Loss: 0.010581126436591148, Final Batch Loss: 0.005380759481340647\n",
      "Epoch 2916, Loss: 0.04293656721711159, Final Batch Loss: 0.019116433337330818\n",
      "Epoch 2917, Loss: 0.024682678980752826, Final Batch Loss: 0.003253212897107005\n",
      "Epoch 2918, Loss: 0.0327515103854239, Final Batch Loss: 0.006751833949238062\n",
      "Epoch 2919, Loss: 0.031024040654301643, Final Batch Loss: 0.01589353196322918\n",
      "Epoch 2920, Loss: 0.013721808791160583, Final Batch Loss: 0.00438571535050869\n",
      "Epoch 2921, Loss: 0.017415867652744055, Final Batch Loss: 0.012836769223213196\n",
      "Epoch 2922, Loss: 0.013127242214977741, Final Batch Loss: 0.007920105941593647\n",
      "Epoch 2923, Loss: 0.05815443769097328, Final Batch Loss: 0.008398525416851044\n",
      "Epoch 2924, Loss: 0.02763071283698082, Final Batch Loss: 0.007312599569559097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2925, Loss: 0.03408971335738897, Final Batch Loss: 0.008380665443837643\n",
      "Epoch 2926, Loss: 0.020899163093417883, Final Batch Loss: 0.006702119950205088\n",
      "Epoch 2927, Loss: 0.03829011879861355, Final Batch Loss: 0.018176274374127388\n",
      "Epoch 2928, Loss: 0.021870301105082035, Final Batch Loss: 0.01400897279381752\n",
      "Epoch 2929, Loss: 0.019083330407738686, Final Batch Loss: 0.011755130253732204\n",
      "Epoch 2930, Loss: 0.007622154196724296, Final Batch Loss: 0.00418578926473856\n",
      "Epoch 2931, Loss: 0.03480814956128597, Final Batch Loss: 0.007115021347999573\n",
      "Epoch 2932, Loss: 0.03748109005391598, Final Batch Loss: 0.012025807052850723\n",
      "Epoch 2933, Loss: 0.027490468695759773, Final Batch Loss: 0.015597634948790073\n",
      "Epoch 2934, Loss: 0.017097626347094774, Final Batch Loss: 0.006158156786113977\n",
      "Epoch 2935, Loss: 0.01149294339120388, Final Batch Loss: 0.001663094386458397\n",
      "Epoch 2936, Loss: 0.024246840737760067, Final Batch Loss: 0.005834939889609814\n",
      "Epoch 2937, Loss: 0.04308790573850274, Final Batch Loss: 0.0059307836927473545\n",
      "Epoch 2938, Loss: 0.006514945533126593, Final Batch Loss: 0.0024221669882535934\n",
      "Epoch 2939, Loss: 0.029089685063809156, Final Batch Loss: 0.02206854335963726\n",
      "Epoch 2940, Loss: 0.04190736962482333, Final Batch Loss: 0.004304345231503248\n",
      "Epoch 2941, Loss: 0.05722756218165159, Final Batch Loss: 0.0127074820920825\n",
      "Epoch 2942, Loss: 0.01865124376490712, Final Batch Loss: 0.00427387235686183\n",
      "Epoch 2943, Loss: 0.030142602510750294, Final Batch Loss: 0.025618255138397217\n",
      "Epoch 2944, Loss: 0.025749921798706055, Final Batch Loss: 0.011033128947019577\n",
      "Epoch 2945, Loss: 0.05357138905674219, Final Batch Loss: 0.008655690588057041\n",
      "Epoch 2946, Loss: 0.014821344055235386, Final Batch Loss: 0.002936549484729767\n",
      "Epoch 2947, Loss: 0.028582301922142506, Final Batch Loss: 0.00801254902034998\n",
      "Epoch 2948, Loss: 0.04310540296137333, Final Batch Loss: 0.02591763064265251\n",
      "Epoch 2949, Loss: 0.007203964050859213, Final Batch Loss: 0.0030151838436722755\n",
      "Epoch 2950, Loss: 0.04194601904600859, Final Batch Loss: 0.03641682490706444\n",
      "Epoch 2951, Loss: 0.022455747239291668, Final Batch Loss: 0.011450630612671375\n",
      "Epoch 2952, Loss: 0.009166239760816097, Final Batch Loss: 0.00545296398922801\n",
      "Epoch 2953, Loss: 0.010642990935593843, Final Batch Loss: 0.003920061979442835\n",
      "Epoch 2954, Loss: 0.039517506025731564, Final Batch Loss: 0.03094378672540188\n",
      "Epoch 2955, Loss: 0.02937065064907074, Final Batch Loss: 0.008723972365260124\n",
      "Epoch 2956, Loss: 0.04665896203368902, Final Batch Loss: 0.01107837911695242\n",
      "Epoch 2957, Loss: 0.05569476634263992, Final Batch Loss: 0.03516162559390068\n",
      "Epoch 2958, Loss: 0.034962470177561045, Final Batch Loss: 0.03220997750759125\n",
      "Epoch 2959, Loss: 0.024544384330511093, Final Batch Loss: 0.01079817395657301\n",
      "Epoch 2960, Loss: 0.03750649467110634, Final Batch Loss: 0.024492619559168816\n",
      "Epoch 2961, Loss: 0.03143499791622162, Final Batch Loss: 0.010244708508253098\n",
      "Epoch 2962, Loss: 0.018041398841887712, Final Batch Loss: 0.004098187666386366\n",
      "Epoch 2963, Loss: 0.015074105001986027, Final Batch Loss: 0.006069942377507687\n",
      "Epoch 2964, Loss: 0.04439959488809109, Final Batch Loss: 0.03454767167568207\n",
      "Epoch 2965, Loss: 0.02839289139956236, Final Batch Loss: 0.010856295935809612\n",
      "Epoch 2966, Loss: 0.009765155613422394, Final Batch Loss: 0.005554170813411474\n",
      "Epoch 2967, Loss: 0.014166941866278648, Final Batch Loss: 0.007587048690766096\n",
      "Epoch 2968, Loss: 0.061522869393229485, Final Batch Loss: 0.04991351440548897\n",
      "Epoch 2969, Loss: 0.02227705530822277, Final Batch Loss: 0.009213541634380817\n",
      "Epoch 2970, Loss: 0.07487676665186882, Final Batch Loss: 0.06398902833461761\n",
      "Epoch 2971, Loss: 0.01822941517457366, Final Batch Loss: 0.004510157275944948\n",
      "Epoch 2972, Loss: 0.01502086129039526, Final Batch Loss: 0.003886464051902294\n",
      "Epoch 2973, Loss: 0.03202999010682106, Final Batch Loss: 0.00928705558180809\n",
      "Epoch 2974, Loss: 0.028605344239622355, Final Batch Loss: 0.006381612736731768\n",
      "Epoch 2975, Loss: 0.012229217099957168, Final Batch Loss: 0.0017711900873109698\n",
      "Epoch 2976, Loss: 0.019555299077183008, Final Batch Loss: 0.013884772546589375\n",
      "Epoch 2977, Loss: 0.024966632947325706, Final Batch Loss: 0.016553856432437897\n",
      "Epoch 2978, Loss: 0.05468939617276192, Final Batch Loss: 0.031019216403365135\n",
      "Epoch 2979, Loss: 0.03531610034406185, Final Batch Loss: 0.02364511229097843\n",
      "Epoch 2980, Loss: 0.020361008122563362, Final Batch Loss: 0.016363749280571938\n",
      "Epoch 2981, Loss: 0.01964752748608589, Final Batch Loss: 0.010645640082657337\n",
      "Epoch 2982, Loss: 0.020058888476341963, Final Batch Loss: 0.0024940217845141888\n",
      "Epoch 2983, Loss: 0.02117153163999319, Final Batch Loss: 0.008152440190315247\n",
      "Epoch 2984, Loss: 0.09822142496705055, Final Batch Loss: 0.061427172273397446\n",
      "Epoch 2985, Loss: 0.011435923166573048, Final Batch Loss: 0.004636017605662346\n",
      "Epoch 2986, Loss: 0.03707975707948208, Final Batch Loss: 0.018568361178040504\n",
      "Epoch 2987, Loss: 0.019573704805225134, Final Batch Loss: 0.005941358860582113\n",
      "Epoch 2988, Loss: 0.015997481998056173, Final Batch Loss: 0.009563324972987175\n",
      "Epoch 2989, Loss: 0.047471702098846436, Final Batch Loss: 0.0191457848995924\n",
      "Epoch 2990, Loss: 0.037063565105199814, Final Batch Loss: 0.008090732619166374\n",
      "Epoch 2991, Loss: 0.00953916972503066, Final Batch Loss: 0.00447915680706501\n",
      "Epoch 2992, Loss: 0.02275685640051961, Final Batch Loss: 0.018458135426044464\n",
      "Epoch 2993, Loss: 0.048448771238327026, Final Batch Loss: 0.024047797545790672\n",
      "Epoch 2994, Loss: 0.01337111135944724, Final Batch Loss: 0.0038477187044918537\n",
      "Epoch 2995, Loss: 0.031072908081114292, Final Batch Loss: 0.023657463490962982\n",
      "Epoch 2996, Loss: 0.038615341298282146, Final Batch Loss: 0.024939674884080887\n",
      "Epoch 2997, Loss: 0.013795019593089819, Final Batch Loss: 0.007983767427504063\n",
      "Epoch 2998, Loss: 0.055086092092096806, Final Batch Loss: 0.040525514632463455\n",
      "Epoch 2999, Loss: 0.04969684034585953, Final Batch Loss: 0.02913411520421505\n",
      "Epoch 3000, Loss: 0.02700989437289536, Final Batch Loss: 0.0037788127083331347\n",
      "Epoch 3001, Loss: 0.024071295745670795, Final Batch Loss: 0.005866614170372486\n",
      "Epoch 3002, Loss: 0.05533998366445303, Final Batch Loss: 0.015526033006608486\n",
      "Epoch 3003, Loss: 0.018462717533111572, Final Batch Loss: 0.010492035187780857\n",
      "Epoch 3004, Loss: 0.0876889294013381, Final Batch Loss: 0.01060913410037756\n",
      "Epoch 3005, Loss: 0.07437381893396378, Final Batch Loss: 0.021523907780647278\n",
      "Epoch 3006, Loss: 0.028904717415571213, Final Batch Loss: 0.016988158226013184\n",
      "Epoch 3007, Loss: 0.020870591513812542, Final Batch Loss: 0.00887833908200264\n",
      "Epoch 3008, Loss: 0.02566032763570547, Final Batch Loss: 0.014988303184509277\n",
      "Epoch 3009, Loss: 0.02689772774465382, Final Batch Loss: 0.00297567923553288\n",
      "Epoch 3010, Loss: 0.0153995705768466, Final Batch Loss: 0.00932387262582779\n",
      "Epoch 3011, Loss: 0.04264990845695138, Final Batch Loss: 0.004713392350822687\n",
      "Epoch 3012, Loss: 0.021810008212924004, Final Batch Loss: 0.012871678918600082\n",
      "Epoch 3013, Loss: 0.015695591922849417, Final Batch Loss: 0.006991754751652479\n",
      "Epoch 3014, Loss: 0.05060688406229019, Final Batch Loss: 0.03202933073043823\n",
      "Epoch 3015, Loss: 0.02135975449346006, Final Batch Loss: 0.0037267261650413275\n",
      "Epoch 3016, Loss: 0.022103669121861458, Final Batch Loss: 0.009748134762048721\n",
      "Epoch 3017, Loss: 0.012950608972460032, Final Batch Loss: 0.004536991473287344\n",
      "Epoch 3018, Loss: 0.02073844103142619, Final Batch Loss: 0.00608282582834363\n",
      "Epoch 3019, Loss: 0.012103091110475361, Final Batch Loss: 0.0015338017838075757\n",
      "Epoch 3020, Loss: 0.00953413499519229, Final Batch Loss: 0.006631164811551571\n",
      "Epoch 3021, Loss: 0.03530964069068432, Final Batch Loss: 0.019525451585650444\n",
      "Epoch 3022, Loss: 0.018465754576027393, Final Batch Loss: 0.0026817889884114265\n",
      "Epoch 3023, Loss: 0.04723425954580307, Final Batch Loss: 0.01732541434466839\n",
      "Epoch 3024, Loss: 0.01643472071737051, Final Batch Loss: 0.004890625365078449\n",
      "Epoch 3025, Loss: 0.019468057435005903, Final Batch Loss: 0.015534669160842896\n",
      "Epoch 3026, Loss: 0.01050912356004119, Final Batch Loss: 0.0050101070664823055\n",
      "Epoch 3027, Loss: 0.013239362742751837, Final Batch Loss: 0.007836014963686466\n",
      "Epoch 3028, Loss: 0.042042556684464216, Final Batch Loss: 0.004193633329123259\n",
      "Epoch 3029, Loss: 0.009501693770289421, Final Batch Loss: 0.005261532496660948\n",
      "Epoch 3030, Loss: 0.009743395261466503, Final Batch Loss: 0.004039721097797155\n",
      "Epoch 3031, Loss: 0.03658968326635659, Final Batch Loss: 0.03310070186853409\n",
      "Epoch 3032, Loss: 0.03247618721798062, Final Batch Loss: 0.004015136975795031\n",
      "Epoch 3033, Loss: 0.08704683184623718, Final Batch Loss: 0.03919538855552673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3034, Loss: 0.02472764253616333, Final Batch Loss: 0.008081717416644096\n",
      "Epoch 3035, Loss: 0.026352127781137824, Final Batch Loss: 0.024682633578777313\n",
      "Epoch 3036, Loss: 0.012052332982420921, Final Batch Loss: 0.0037400834262371063\n",
      "Epoch 3037, Loss: 0.0639589955098927, Final Batch Loss: 0.05777723342180252\n",
      "Epoch 3038, Loss: 0.09051536023616791, Final Batch Loss: 0.023627959191799164\n",
      "Epoch 3039, Loss: 0.07867932692170143, Final Batch Loss: 0.07074034959077835\n",
      "Epoch 3040, Loss: 0.05851753614842892, Final Batch Loss: 0.03505954146385193\n",
      "Epoch 3041, Loss: 0.035085185430943966, Final Batch Loss: 0.0148078678175807\n",
      "Epoch 3042, Loss: 0.06494098715484142, Final Batch Loss: 0.03499331697821617\n",
      "Epoch 3043, Loss: 0.032045451924204826, Final Batch Loss: 0.01596127636730671\n",
      "Epoch 3044, Loss: 0.05011284723877907, Final Batch Loss: 0.02784710004925728\n",
      "Epoch 3045, Loss: 0.0670071616768837, Final Batch Loss: 0.03339099884033203\n",
      "Epoch 3046, Loss: 0.03843902796506882, Final Batch Loss: 0.019589446485042572\n",
      "Epoch 3047, Loss: 0.05072639510035515, Final Batch Loss: 0.03249981626868248\n",
      "Epoch 3048, Loss: 0.030074412003159523, Final Batch Loss: 0.020418843254446983\n",
      "Epoch 3049, Loss: 0.025792334228754044, Final Batch Loss: 0.008696295320987701\n",
      "Epoch 3050, Loss: 0.01535556185990572, Final Batch Loss: 0.003604932688176632\n",
      "Epoch 3051, Loss: 0.033232265152037144, Final Batch Loss: 0.008577941916882992\n",
      "Epoch 3052, Loss: 0.05779244750738144, Final Batch Loss: 0.050008442252874374\n",
      "Epoch 3053, Loss: 0.05261016637086868, Final Batch Loss: 0.02248724177479744\n",
      "Epoch 3054, Loss: 0.06672938633710146, Final Batch Loss: 0.06125844269990921\n",
      "Epoch 3055, Loss: 0.07620434695854783, Final Batch Loss: 0.06862100213766098\n",
      "Epoch 3056, Loss: 0.010650062467902899, Final Batch Loss: 0.004868526943027973\n",
      "Epoch 3057, Loss: 0.037681025452911854, Final Batch Loss: 0.015246677212417126\n",
      "Epoch 3058, Loss: 0.04250895790755749, Final Batch Loss: 0.015941137447953224\n",
      "Epoch 3059, Loss: 0.02056968305259943, Final Batch Loss: 0.006644511595368385\n",
      "Epoch 3060, Loss: 0.05520494282245636, Final Batch Loss: 0.024276454001665115\n",
      "Epoch 3061, Loss: 0.0344023248180747, Final Batch Loss: 0.020714491605758667\n",
      "Epoch 3062, Loss: 0.01543490868061781, Final Batch Loss: 0.008868059143424034\n",
      "Epoch 3063, Loss: 0.0717061273753643, Final Batch Loss: 0.032015133649110794\n",
      "Epoch 3064, Loss: 0.02083274256438017, Final Batch Loss: 0.009052706882357597\n",
      "Epoch 3065, Loss: 0.03749545384198427, Final Batch Loss: 0.005649571307003498\n",
      "Epoch 3066, Loss: 0.023361808620393276, Final Batch Loss: 0.01090867817401886\n",
      "Epoch 3067, Loss: 0.024431481957435608, Final Batch Loss: 0.014122388325631618\n",
      "Epoch 3068, Loss: 0.03078729286789894, Final Batch Loss: 0.006667952984571457\n",
      "Epoch 3069, Loss: 0.06094789318740368, Final Batch Loss: 0.01863080821931362\n",
      "Epoch 3070, Loss: 0.035247158259153366, Final Batch Loss: 0.030789751559495926\n",
      "Epoch 3071, Loss: 0.011727078352123499, Final Batch Loss: 0.006827208679169416\n",
      "Epoch 3072, Loss: 0.02577816601842642, Final Batch Loss: 0.010063228197395802\n",
      "Epoch 3073, Loss: 0.014978090301156044, Final Batch Loss: 0.006522681564092636\n",
      "Epoch 3074, Loss: 0.08159108925610781, Final Batch Loss: 0.0726904571056366\n",
      "Epoch 3075, Loss: 0.0401327982544899, Final Batch Loss: 0.009823732078075409\n",
      "Epoch 3076, Loss: 0.04250988503918052, Final Batch Loss: 0.034992776811122894\n",
      "Epoch 3077, Loss: 0.06768728420138359, Final Batch Loss: 0.020527184009552002\n",
      "Epoch 3078, Loss: 0.042762347497045994, Final Batch Loss: 0.015525405295193195\n",
      "Epoch 3079, Loss: 0.025849301367998123, Final Batch Loss: 0.006298394873738289\n",
      "Epoch 3080, Loss: 0.015679643023759127, Final Batch Loss: 0.006301878485828638\n",
      "Epoch 3081, Loss: 0.009203026071190834, Final Batch Loss: 0.004691556096076965\n",
      "Epoch 3082, Loss: 0.02514851000159979, Final Batch Loss: 0.007206630893051624\n",
      "Epoch 3083, Loss: 0.02638970874249935, Final Batch Loss: 0.018626486882567406\n",
      "Epoch 3084, Loss: 0.021976131945848465, Final Batch Loss: 0.010664350353181362\n",
      "Epoch 3085, Loss: 0.02225423790514469, Final Batch Loss: 0.014609649777412415\n",
      "Epoch 3086, Loss: 0.025093933567404747, Final Batch Loss: 0.016364462673664093\n",
      "Epoch 3087, Loss: 0.01645444380119443, Final Batch Loss: 0.011989305727183819\n",
      "Epoch 3088, Loss: 0.028099944815039635, Final Batch Loss: 0.01714414730668068\n",
      "Epoch 3089, Loss: 0.0289241224527359, Final Batch Loss: 0.00801672600209713\n",
      "Epoch 3090, Loss: 0.03235234320163727, Final Batch Loss: 0.02436465211212635\n",
      "Epoch 3091, Loss: 0.03713016118854284, Final Batch Loss: 0.02597133442759514\n",
      "Epoch 3092, Loss: 0.04474148922599852, Final Batch Loss: 0.04157518967986107\n",
      "Epoch 3093, Loss: 0.02610470075160265, Final Batch Loss: 0.009470104239881039\n",
      "Epoch 3094, Loss: 0.017459087539464235, Final Batch Loss: 0.005582374054938555\n",
      "Epoch 3095, Loss: 0.045015652664005756, Final Batch Loss: 0.03688254579901695\n",
      "Epoch 3096, Loss: 0.01710375864058733, Final Batch Loss: 0.00829146895557642\n",
      "Epoch 3097, Loss: 0.03387835342437029, Final Batch Loss: 0.022079220041632652\n",
      "Epoch 3098, Loss: 0.03621715912595391, Final Batch Loss: 0.006499890703707933\n",
      "Epoch 3099, Loss: 0.0576324500143528, Final Batch Loss: 0.030363401398062706\n",
      "Epoch 3100, Loss: 0.03054293105378747, Final Batch Loss: 0.005373569671064615\n",
      "Epoch 3101, Loss: 0.04632074758410454, Final Batch Loss: 0.041466500610113144\n",
      "Epoch 3102, Loss: 0.020477730315178633, Final Batch Loss: 0.016767045482993126\n",
      "Epoch 3103, Loss: 0.034068210050463676, Final Batch Loss: 0.016777262091636658\n",
      "Epoch 3104, Loss: 0.016641457565128803, Final Batch Loss: 0.004928675480186939\n",
      "Epoch 3105, Loss: 0.03484204784035683, Final Batch Loss: 0.029626915231347084\n",
      "Epoch 3106, Loss: 0.023380527505651116, Final Batch Loss: 0.0034363886807113886\n",
      "Epoch 3107, Loss: 0.016723268665373325, Final Batch Loss: 0.011045047082006931\n",
      "Epoch 3108, Loss: 0.13047000020742416, Final Batch Loss: 0.10665132850408554\n",
      "Epoch 3109, Loss: 0.04833637364208698, Final Batch Loss: 0.04399920627474785\n",
      "Epoch 3110, Loss: 0.018668872769922018, Final Batch Loss: 0.0015334817580878735\n",
      "Epoch 3111, Loss: 0.03549465350806713, Final Batch Loss: 0.02100338786840439\n",
      "Epoch 3112, Loss: 0.006402390543371439, Final Batch Loss: 0.0024236105382442474\n",
      "Epoch 3113, Loss: 0.057227956131100655, Final Batch Loss: 0.04360738769173622\n",
      "Epoch 3114, Loss: 0.05461594182997942, Final Batch Loss: 0.045597754418849945\n",
      "Epoch 3115, Loss: 0.05694566387683153, Final Batch Loss: 0.010928773321211338\n",
      "Epoch 3116, Loss: 0.035169451497495174, Final Batch Loss: 0.021556217223405838\n",
      "Epoch 3117, Loss: 0.1020444049499929, Final Batch Loss: 0.09772899001836777\n",
      "Epoch 3118, Loss: 0.03509095078334212, Final Batch Loss: 0.006057589780539274\n",
      "Epoch 3119, Loss: 0.009710432030260563, Final Batch Loss: 0.004303809255361557\n",
      "Epoch 3120, Loss: 0.032965159974992275, Final Batch Loss: 0.02154022455215454\n",
      "Epoch 3121, Loss: 0.034989283652976155, Final Batch Loss: 0.003152706893160939\n",
      "Epoch 3122, Loss: 0.024427092634141445, Final Batch Loss: 0.00914221815764904\n",
      "Epoch 3123, Loss: 0.03792238608002663, Final Batch Loss: 0.029801618307828903\n",
      "Epoch 3124, Loss: 0.03288743272423744, Final Batch Loss: 0.018592052161693573\n",
      "Epoch 3125, Loss: 0.028323034290224314, Final Batch Loss: 0.0032167197205126286\n",
      "Epoch 3126, Loss: 0.033644815208390355, Final Batch Loss: 0.002507466124370694\n",
      "Epoch 3127, Loss: 0.022482212632894516, Final Batch Loss: 0.01182652823626995\n",
      "Epoch 3128, Loss: 0.028019064106047153, Final Batch Loss: 0.017755204811692238\n",
      "Epoch 3129, Loss: 0.03218787908554077, Final Batch Loss: 0.014725986868143082\n",
      "Epoch 3130, Loss: 0.041342057287693024, Final Batch Loss: 0.02970561943948269\n",
      "Epoch 3131, Loss: 0.018786665983498096, Final Batch Loss: 0.007119639776647091\n",
      "Epoch 3132, Loss: 0.05634745582938194, Final Batch Loss: 0.03105013258755207\n",
      "Epoch 3133, Loss: 0.015838440507650375, Final Batch Loss: 0.005496256053447723\n",
      "Epoch 3134, Loss: 0.023276074789464474, Final Batch Loss: 0.015118018724024296\n",
      "Epoch 3135, Loss: 0.008983112405985594, Final Batch Loss: 0.00458069471642375\n",
      "Epoch 3136, Loss: 0.013822456821799278, Final Batch Loss: 0.0021000467240810394\n",
      "Epoch 3137, Loss: 0.016862953081727028, Final Batch Loss: 0.00927931722253561\n",
      "Epoch 3138, Loss: 0.019687545485794544, Final Batch Loss: 0.00477280467748642\n",
      "Epoch 3139, Loss: 0.0312631637789309, Final Batch Loss: 0.026454661041498184\n",
      "Epoch 3140, Loss: 0.040137805975973606, Final Batch Loss: 0.006574527360498905\n",
      "Epoch 3141, Loss: 0.01981719397008419, Final Batch Loss: 0.0036053434014320374\n",
      "Epoch 3142, Loss: 0.0077561435755342245, Final Batch Loss: 0.0036152673419564962\n",
      "Epoch 3143, Loss: 0.02570970682427287, Final Batch Loss: 0.00754702789708972\n",
      "Epoch 3144, Loss: 0.030872255330905318, Final Batch Loss: 0.027638953179121017\n",
      "Epoch 3145, Loss: 0.00664147362112999, Final Batch Loss: 0.001583135686814785\n",
      "Epoch 3146, Loss: 0.022093394305557013, Final Batch Loss: 0.0028055659495294094\n",
      "Epoch 3147, Loss: 0.01521596172824502, Final Batch Loss: 0.004909947048872709\n",
      "Epoch 3148, Loss: 0.0158517814707011, Final Batch Loss: 0.01228638831526041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3149, Loss: 0.10130283422768116, Final Batch Loss: 0.028278635814785957\n",
      "Epoch 3150, Loss: 0.016815022099763155, Final Batch Loss: 0.011048663407564163\n",
      "Epoch 3151, Loss: 0.025987882167100906, Final Batch Loss: 0.014215742237865925\n",
      "Epoch 3152, Loss: 0.04880168358795345, Final Batch Loss: 0.0028729743789881468\n",
      "Epoch 3153, Loss: 0.0163095248863101, Final Batch Loss: 0.0063558220863342285\n",
      "Epoch 3154, Loss: 0.08560903184115887, Final Batch Loss: 0.0647902861237526\n",
      "Epoch 3155, Loss: 0.04670725390315056, Final Batch Loss: 0.03430192172527313\n",
      "Epoch 3156, Loss: 0.15343818068504333, Final Batch Loss: 0.07559475302696228\n",
      "Epoch 3157, Loss: 0.0375679200515151, Final Batch Loss: 0.013336305506527424\n",
      "Epoch 3158, Loss: 0.16190749034285545, Final Batch Loss: 0.045231644064188004\n",
      "Epoch 3159, Loss: 0.04211936146020889, Final Batch Loss: 0.024725528433918953\n",
      "Epoch 3160, Loss: 0.14788847044110298, Final Batch Loss: 0.05031934008002281\n",
      "Epoch 3161, Loss: 0.020767181180417538, Final Batch Loss: 0.0087425010278821\n",
      "Epoch 3162, Loss: 0.05796443950384855, Final Batch Loss: 0.043418385088443756\n",
      "Epoch 3163, Loss: 0.054468030110001564, Final Batch Loss: 0.03449476137757301\n",
      "Epoch 3164, Loss: 0.27810656279325485, Final Batch Loss: 0.11119354516267776\n",
      "Epoch 3165, Loss: 0.05599688459187746, Final Batch Loss: 0.04745441675186157\n",
      "Epoch 3166, Loss: 0.13666641898453236, Final Batch Loss: 0.02101968415081501\n",
      "Epoch 3167, Loss: 0.032845442183315754, Final Batch Loss: 0.018513614311814308\n",
      "Epoch 3168, Loss: 0.03868699073791504, Final Batch Loss: 0.013281852006912231\n",
      "Epoch 3169, Loss: 0.029464944265782833, Final Batch Loss: 0.01047119963914156\n",
      "Epoch 3170, Loss: 0.03767602704465389, Final Batch Loss: 0.005015747621655464\n",
      "Epoch 3171, Loss: 0.023469359148293734, Final Batch Loss: 0.01849650964140892\n",
      "Epoch 3172, Loss: 0.021215809509158134, Final Batch Loss: 0.0032384321093559265\n",
      "Epoch 3173, Loss: 0.03488807566463947, Final Batch Loss: 0.01154770515859127\n",
      "Epoch 3174, Loss: 0.02032909309491515, Final Batch Loss: 0.01562570221722126\n",
      "Epoch 3175, Loss: 0.057813060469925404, Final Batch Loss: 0.049477383494377136\n",
      "Epoch 3176, Loss: 0.020586777478456497, Final Batch Loss: 0.003709344193339348\n",
      "Epoch 3177, Loss: 0.05242481082677841, Final Batch Loss: 0.03571150824427605\n",
      "Epoch 3178, Loss: 0.055609145667403936, Final Batch Loss: 0.007163004484027624\n",
      "Epoch 3179, Loss: 0.014664636459201574, Final Batch Loss: 0.007258603349328041\n",
      "Epoch 3180, Loss: 0.04882977716624737, Final Batch Loss: 0.017316019162535667\n",
      "Epoch 3181, Loss: 0.01955475937575102, Final Batch Loss: 0.009668709710240364\n",
      "Epoch 3182, Loss: 0.03742585889995098, Final Batch Loss: 0.014550000429153442\n",
      "Epoch 3183, Loss: 0.037445168010890484, Final Batch Loss: 0.008121940307319164\n",
      "Epoch 3184, Loss: 0.08750600041821599, Final Batch Loss: 0.08441748470067978\n",
      "Epoch 3185, Loss: 0.02577708289027214, Final Batch Loss: 0.013042633421719074\n",
      "Epoch 3186, Loss: 0.03970694541931152, Final Batch Loss: 0.025540167465806007\n",
      "Epoch 3187, Loss: 0.0069782971404492855, Final Batch Loss: 0.0037167484406381845\n",
      "Epoch 3188, Loss: 0.012202038429677486, Final Batch Loss: 0.005705790128558874\n",
      "Epoch 3189, Loss: 0.05380111373960972, Final Batch Loss: 0.039497457444667816\n",
      "Epoch 3190, Loss: 0.018727314658463, Final Batch Loss: 0.00869187992066145\n",
      "Epoch 3191, Loss: 0.06885617133229971, Final Batch Loss: 0.0560959056019783\n",
      "Epoch 3192, Loss: 0.05456892820075154, Final Batch Loss: 0.004957031924277544\n",
      "Epoch 3193, Loss: 0.01373505499213934, Final Batch Loss: 0.007579656317830086\n",
      "Epoch 3194, Loss: 0.0519411563873291, Final Batch Loss: 0.029305340722203255\n",
      "Epoch 3195, Loss: 0.042695026844739914, Final Batch Loss: 0.0162680484354496\n",
      "Epoch 3196, Loss: 0.02676244592294097, Final Batch Loss: 0.021732881665229797\n",
      "Epoch 3197, Loss: 0.04319367557764053, Final Batch Loss: 0.023083264008164406\n",
      "Epoch 3198, Loss: 0.055104777216911316, Final Batch Loss: 0.030818790197372437\n",
      "Epoch 3199, Loss: 0.011213343357667327, Final Batch Loss: 0.0035105824936181307\n",
      "Epoch 3200, Loss: 0.08225993067026138, Final Batch Loss: 0.07428967952728271\n",
      "Epoch 3201, Loss: 0.13574974238872528, Final Batch Loss: 0.10497993230819702\n",
      "Epoch 3202, Loss: 0.14765718206763268, Final Batch Loss: 0.015838246792554855\n",
      "Epoch 3203, Loss: 0.18696611374616623, Final Batch Loss: 0.09830214828252792\n",
      "Epoch 3204, Loss: 0.017552941106259823, Final Batch Loss: 0.003984690643846989\n",
      "Epoch 3205, Loss: 0.03315171506255865, Final Batch Loss: 0.019311580806970596\n",
      "Epoch 3206, Loss: 0.12719230074435472, Final Batch Loss: 0.01462660450488329\n",
      "Epoch 3207, Loss: 0.04330728901550174, Final Batch Loss: 0.004298872780054808\n",
      "Epoch 3208, Loss: 0.06338060926645994, Final Batch Loss: 0.009537453763186932\n",
      "Epoch 3209, Loss: 0.052450791001319885, Final Batch Loss: 0.03023463860154152\n",
      "Epoch 3210, Loss: 0.034590712282806635, Final Batch Loss: 0.004305290523916483\n",
      "Epoch 3211, Loss: 0.04503295198082924, Final Batch Loss: 0.025822140276432037\n",
      "Epoch 3212, Loss: 0.03263430669903755, Final Batch Loss: 0.016086192801594734\n",
      "Epoch 3213, Loss: 0.019101985730230808, Final Batch Loss: 0.010461583733558655\n",
      "Epoch 3214, Loss: 0.040012426674366, Final Batch Loss: 0.01601424068212509\n",
      "Epoch 3215, Loss: 0.024500325322151184, Final Batch Loss: 0.010753442533314228\n",
      "Epoch 3216, Loss: 0.023251773323863745, Final Batch Loss: 0.016257736831903458\n",
      "Epoch 3217, Loss: 0.022056168410927057, Final Batch Loss: 0.0074731153436005116\n",
      "Epoch 3218, Loss: 0.022254669573158026, Final Batch Loss: 0.01495274156332016\n",
      "Epoch 3219, Loss: 0.0398715827614069, Final Batch Loss: 0.008746732026338577\n",
      "Epoch 3220, Loss: 0.03217209503054619, Final Batch Loss: 0.024703893810510635\n",
      "Epoch 3221, Loss: 0.06207762658596039, Final Batch Loss: 0.02482118457555771\n",
      "Epoch 3222, Loss: 0.03359645325690508, Final Batch Loss: 0.028741613030433655\n",
      "Epoch 3223, Loss: 0.016022554598748684, Final Batch Loss: 0.010921865701675415\n",
      "Epoch 3224, Loss: 0.026860307436436415, Final Batch Loss: 0.020445628091692924\n",
      "Epoch 3225, Loss: 0.009924306534230709, Final Batch Loss: 0.004835488740354776\n",
      "Epoch 3226, Loss: 0.03215815592557192, Final Batch Loss: 0.02085375227034092\n",
      "Epoch 3227, Loss: 0.017131815664470196, Final Batch Loss: 0.005157541483640671\n",
      "Epoch 3228, Loss: 0.011800563428550959, Final Batch Loss: 0.003978708293288946\n",
      "Epoch 3229, Loss: 0.03982712421566248, Final Batch Loss: 0.03291623666882515\n",
      "Epoch 3230, Loss: 0.010444765910506248, Final Batch Loss: 0.0028542145155370235\n",
      "Epoch 3231, Loss: 0.1210578503087163, Final Batch Loss: 0.11550791561603546\n",
      "Epoch 3232, Loss: 0.006688742432743311, Final Batch Loss: 0.005158318672329187\n",
      "Epoch 3233, Loss: 0.03813381493091583, Final Batch Loss: 0.014858683571219444\n",
      "Epoch 3234, Loss: 0.01146163372322917, Final Batch Loss: 0.004724330734461546\n",
      "Epoch 3235, Loss: 0.03817147761583328, Final Batch Loss: 0.02164490707218647\n",
      "Epoch 3236, Loss: 0.03376551950350404, Final Batch Loss: 0.027173584327101707\n",
      "Epoch 3237, Loss: 0.1340136155486107, Final Batch Loss: 0.06893517822027206\n",
      "Epoch 3238, Loss: 0.06634210050106049, Final Batch Loss: 0.029926173388957977\n",
      "Epoch 3239, Loss: 0.13317633420228958, Final Batch Loss: 0.08734921365976334\n",
      "Epoch 3240, Loss: 0.05037752725183964, Final Batch Loss: 0.018272100016474724\n",
      "Epoch 3241, Loss: 0.038804758340120316, Final Batch Loss: 0.021387606859207153\n",
      "Epoch 3242, Loss: 0.09000410325825214, Final Batch Loss: 0.07017754763364792\n",
      "Epoch 3243, Loss: 0.05202660709619522, Final Batch Loss: 0.018537286669015884\n",
      "Epoch 3244, Loss: 0.04860663693398237, Final Batch Loss: 0.03358595818281174\n",
      "Epoch 3245, Loss: 0.08027085848152637, Final Batch Loss: 0.029744179919362068\n",
      "Epoch 3246, Loss: 0.06763215735554695, Final Batch Loss: 0.0516648106276989\n",
      "Epoch 3247, Loss: 0.05118683818727732, Final Batch Loss: 0.041807182133197784\n",
      "Epoch 3248, Loss: 0.05314607918262482, Final Batch Loss: 0.03203107416629791\n",
      "Epoch 3249, Loss: 0.012593128252774477, Final Batch Loss: 0.0025359862484037876\n",
      "Epoch 3250, Loss: 0.04474067874252796, Final Batch Loss: 0.023648761212825775\n",
      "Epoch 3251, Loss: 0.08824114874005318, Final Batch Loss: 0.0427357442677021\n",
      "Epoch 3252, Loss: 0.02253165701404214, Final Batch Loss: 0.006863350514322519\n",
      "Epoch 3253, Loss: 0.11828839033842087, Final Batch Loss: 0.0565447062253952\n",
      "Epoch 3254, Loss: 0.021846849471330643, Final Batch Loss: 0.01426626555621624\n",
      "Epoch 3255, Loss: 0.04876395408064127, Final Batch Loss: 0.037659645080566406\n",
      "Epoch 3256, Loss: 0.10907164961099625, Final Batch Loss: 0.0985005646944046\n",
      "Epoch 3257, Loss: 0.05513697676360607, Final Batch Loss: 0.031682807952165604\n",
      "Epoch 3258, Loss: 0.039455339312553406, Final Batch Loss: 0.010662566870450974\n",
      "Epoch 3259, Loss: 0.03361692372709513, Final Batch Loss: 0.020987721160054207\n",
      "Epoch 3260, Loss: 0.024310979526489973, Final Batch Loss: 0.006099417340010405\n",
      "Epoch 3261, Loss: 0.01969316927716136, Final Batch Loss: 0.006626105401664972\n",
      "Epoch 3262, Loss: 0.05009649693965912, Final Batch Loss: 0.018245581537485123\n",
      "Epoch 3263, Loss: 0.01256002951413393, Final Batch Loss: 0.00788810569792986\n",
      "Epoch 3264, Loss: 0.009340640855953097, Final Batch Loss: 0.005448598880320787\n",
      "Epoch 3265, Loss: 0.014556546695530415, Final Batch Loss: 0.0068154772743582726\n",
      "Epoch 3266, Loss: 0.021592723205685616, Final Batch Loss: 0.010518014430999756\n",
      "Epoch 3267, Loss: 0.010474827839061618, Final Batch Loss: 0.0030840688850730658\n",
      "Epoch 3268, Loss: 0.043134309351444244, Final Batch Loss: 0.03304547816514969\n",
      "Epoch 3269, Loss: 0.04068443085998297, Final Batch Loss: 0.027129966765642166\n",
      "Epoch 3270, Loss: 0.13306359760463238, Final Batch Loss: 0.019623978063464165\n",
      "Epoch 3271, Loss: 0.01486348221078515, Final Batch Loss: 0.0039923167787492275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3272, Loss: 0.028378601651638746, Final Batch Loss: 0.0067163570784032345\n",
      "Epoch 3273, Loss: 0.033277212642133236, Final Batch Loss: 0.02712959796190262\n",
      "Epoch 3274, Loss: 0.08191236108541489, Final Batch Loss: 0.029677148908376694\n",
      "Epoch 3275, Loss: 0.011030110064893961, Final Batch Loss: 0.0048928945325315\n",
      "Epoch 3276, Loss: 0.012190980836749077, Final Batch Loss: 0.004148650914430618\n",
      "Epoch 3277, Loss: 0.08106811344623566, Final Batch Loss: 0.03764207288622856\n",
      "Epoch 3278, Loss: 0.04257997218519449, Final Batch Loss: 0.027134476229548454\n",
      "Epoch 3279, Loss: 0.017224150709807873, Final Batch Loss: 0.003641837276518345\n",
      "Epoch 3280, Loss: 0.025388332549482584, Final Batch Loss: 0.007601127494126558\n",
      "Epoch 3281, Loss: 0.026712195947766304, Final Batch Loss: 0.021682009100914\n",
      "Epoch 3282, Loss: 0.04283310053870082, Final Batch Loss: 0.004808760713785887\n",
      "Epoch 3283, Loss: 0.034184886142611504, Final Batch Loss: 0.010356158018112183\n",
      "Epoch 3284, Loss: 0.03727327659726143, Final Batch Loss: 0.01648121513426304\n",
      "Epoch 3285, Loss: 0.03063005581498146, Final Batch Loss: 0.011830992996692657\n",
      "Epoch 3286, Loss: 0.029961542692035437, Final Batch Loss: 0.003243101295083761\n",
      "Epoch 3287, Loss: 0.0166197856888175, Final Batch Loss: 0.008496144786477089\n",
      "Epoch 3288, Loss: 0.01742395618930459, Final Batch Loss: 0.007533453870564699\n",
      "Epoch 3289, Loss: 0.0304329008795321, Final Batch Loss: 0.023418262600898743\n",
      "Epoch 3290, Loss: 0.03408362343907356, Final Batch Loss: 0.015114465728402138\n",
      "Epoch 3291, Loss: 0.02614000765606761, Final Batch Loss: 0.018703795969486237\n",
      "Epoch 3292, Loss: 0.009386011864989996, Final Batch Loss: 0.0041917008347809315\n",
      "Epoch 3293, Loss: 0.012187649495899677, Final Batch Loss: 0.006642045918852091\n",
      "Epoch 3294, Loss: 0.011841814033687115, Final Batch Loss: 0.005063887219876051\n",
      "Epoch 3295, Loss: 0.034209733828902245, Final Batch Loss: 0.021482260897755623\n",
      "Epoch 3296, Loss: 0.029541511088609695, Final Batch Loss: 0.019987046718597412\n",
      "Epoch 3297, Loss: 0.014009432401508093, Final Batch Loss: 0.007368673570454121\n",
      "Epoch 3298, Loss: 0.023725145030766726, Final Batch Loss: 0.004502395633608103\n",
      "Epoch 3299, Loss: 0.012579372152686119, Final Batch Loss: 0.008179424330592155\n",
      "Epoch 3300, Loss: 0.009627679828554392, Final Batch Loss: 0.004661861807107925\n",
      "Epoch 3301, Loss: 0.030929782427847385, Final Batch Loss: 0.018990950658917427\n",
      "Epoch 3302, Loss: 0.013731821905821562, Final Batch Loss: 0.007703030481934547\n",
      "Epoch 3303, Loss: 0.029269500635564327, Final Batch Loss: 0.019385825842618942\n",
      "Epoch 3304, Loss: 0.024679446825757623, Final Batch Loss: 0.0028280543629080057\n",
      "Epoch 3305, Loss: 0.016851609107106924, Final Batch Loss: 0.004935333970934153\n",
      "Epoch 3306, Loss: 0.03169397637248039, Final Batch Loss: 0.003936080262064934\n",
      "Epoch 3307, Loss: 0.011421363335102797, Final Batch Loss: 0.005435492377728224\n",
      "Epoch 3308, Loss: 0.00877991714514792, Final Batch Loss: 0.003686825977638364\n",
      "Epoch 3309, Loss: 0.03523187851533294, Final Batch Loss: 0.0024898997507989407\n",
      "Epoch 3310, Loss: 0.011773733422160149, Final Batch Loss: 0.005203884560614824\n",
      "Epoch 3311, Loss: 0.010745973093435168, Final Batch Loss: 0.007622784934937954\n",
      "Epoch 3312, Loss: 0.008553550695069134, Final Batch Loss: 0.0015697177732363343\n",
      "Epoch 3313, Loss: 0.02955271489918232, Final Batch Loss: 0.010258622467517853\n",
      "Epoch 3314, Loss: 0.009196075145155191, Final Batch Loss: 0.0049950433894991875\n",
      "Epoch 3315, Loss: 0.010184551123529673, Final Batch Loss: 0.006171553395688534\n",
      "Epoch 3316, Loss: 0.014132311102002859, Final Batch Loss: 0.0028235535137355328\n",
      "Epoch 3317, Loss: 0.08213135227560997, Final Batch Loss: 0.018047410994768143\n",
      "Epoch 3318, Loss: 0.05337697081267834, Final Batch Loss: 0.04254430532455444\n",
      "Epoch 3319, Loss: 0.024392723804339767, Final Batch Loss: 0.003517294069752097\n",
      "Epoch 3320, Loss: 0.016843613469973207, Final Batch Loss: 0.0037273068446666002\n",
      "Epoch 3321, Loss: 0.011547037400305271, Final Batch Loss: 0.005878852214664221\n",
      "Epoch 3322, Loss: 0.01813492295332253, Final Batch Loss: 0.0029361278284341097\n",
      "Epoch 3323, Loss: 0.0090627227909863, Final Batch Loss: 0.0038212225772440434\n",
      "Epoch 3324, Loss: 0.01459213113412261, Final Batch Loss: 0.0034194947220385075\n",
      "Epoch 3325, Loss: 0.048429423943161964, Final Batch Loss: 0.033815912902355194\n",
      "Epoch 3326, Loss: 0.024404937401413918, Final Batch Loss: 0.0116505678743124\n",
      "Epoch 3327, Loss: 0.042804205790162086, Final Batch Loss: 0.03496613726019859\n",
      "Epoch 3328, Loss: 0.06782223656773567, Final Batch Loss: 0.022111859172582626\n",
      "Epoch 3329, Loss: 0.029980347957462072, Final Batch Loss: 0.02461431734263897\n",
      "Epoch 3330, Loss: 0.01772493775933981, Final Batch Loss: 0.010851040482521057\n",
      "Epoch 3331, Loss: 0.0478630606085062, Final Batch Loss: 0.024890339002013206\n",
      "Epoch 3332, Loss: 0.021545888856053352, Final Batch Loss: 0.016412340104579926\n",
      "Epoch 3333, Loss: 0.05564134009182453, Final Batch Loss: 0.03729749843478203\n",
      "Epoch 3334, Loss: 0.02580456668511033, Final Batch Loss: 0.0044972882606089115\n",
      "Epoch 3335, Loss: 0.02962595969438553, Final Batch Loss: 0.011764701455831528\n",
      "Epoch 3336, Loss: 0.015194662380963564, Final Batch Loss: 0.0034057642333209515\n",
      "Epoch 3337, Loss: 0.03314548567868769, Final Batch Loss: 0.003180606523528695\n",
      "Epoch 3338, Loss: 0.049989352002739906, Final Batch Loss: 0.014390984550118446\n",
      "Epoch 3339, Loss: 0.05937931500375271, Final Batch Loss: 0.03288425877690315\n",
      "Epoch 3340, Loss: 0.026113488245755434, Final Batch Loss: 0.003957014996558428\n",
      "Epoch 3341, Loss: 0.011789447627961636, Final Batch Loss: 0.009794973768293858\n",
      "Epoch 3342, Loss: 0.04038941953331232, Final Batch Loss: 0.032980531454086304\n",
      "Epoch 3343, Loss: 0.019058125093579292, Final Batch Loss: 0.015659460797905922\n",
      "Epoch 3344, Loss: 0.0704472993966192, Final Batch Loss: 0.06774214655160904\n",
      "Epoch 3345, Loss: 0.036487908102571964, Final Batch Loss: 0.022524747997522354\n",
      "Epoch 3346, Loss: 0.04113161494024098, Final Batch Loss: 0.03747308626770973\n",
      "Epoch 3347, Loss: 0.013238645857200027, Final Batch Loss: 0.0034248882438987494\n",
      "Epoch 3348, Loss: 0.020539610646665096, Final Batch Loss: 0.013770775869488716\n",
      "Epoch 3349, Loss: 0.05042168125510216, Final Batch Loss: 0.03476017713546753\n",
      "Epoch 3350, Loss: 0.014408105053007603, Final Batch Loss: 0.007682311814278364\n",
      "Epoch 3351, Loss: 0.015758291818201542, Final Batch Loss: 0.006456805393099785\n",
      "Epoch 3352, Loss: 0.021207505371421576, Final Batch Loss: 0.002703550737351179\n",
      "Epoch 3353, Loss: 0.02266229595988989, Final Batch Loss: 0.014206607826054096\n",
      "Epoch 3354, Loss: 0.010232242289930582, Final Batch Loss: 0.004083579406142235\n",
      "Epoch 3355, Loss: 0.006870119133964181, Final Batch Loss: 0.004612390883266926\n",
      "Epoch 3356, Loss: 0.04360517719760537, Final Batch Loss: 0.0415438637137413\n",
      "Epoch 3357, Loss: 0.016496941912919283, Final Batch Loss: 0.0022774715907871723\n",
      "Epoch 3358, Loss: 0.01199517329223454, Final Batch Loss: 0.0082533685490489\n",
      "Epoch 3359, Loss: 0.031479005701839924, Final Batch Loss: 0.009049571119248867\n",
      "Epoch 3360, Loss: 0.008542951894924045, Final Batch Loss: 0.0036418556701391935\n",
      "Epoch 3361, Loss: 0.014532729517668486, Final Batch Loss: 0.00445558549836278\n",
      "Epoch 3362, Loss: 0.017067410983145237, Final Batch Loss: 0.012786086648702621\n",
      "Epoch 3363, Loss: 0.048006586730480194, Final Batch Loss: 0.018039625138044357\n",
      "Epoch 3364, Loss: 0.006800247821956873, Final Batch Loss: 0.0028114323504269123\n",
      "Epoch 3365, Loss: 0.06056144740432501, Final Batch Loss: 0.05694442614912987\n",
      "Epoch 3366, Loss: 0.010178702417761087, Final Batch Loss: 0.0031194137409329414\n",
      "Epoch 3367, Loss: 0.016580760944634676, Final Batch Loss: 0.00709774112328887\n",
      "Epoch 3368, Loss: 0.005890043103136122, Final Batch Loss: 0.001007145387120545\n",
      "Epoch 3369, Loss: 0.00846498878672719, Final Batch Loss: 0.005380935501307249\n",
      "Epoch 3370, Loss: 0.027734242379665375, Final Batch Loss: 0.013677486218512058\n",
      "Epoch 3371, Loss: 0.01437642378732562, Final Batch Loss: 0.004665991757065058\n",
      "Epoch 3372, Loss: 0.008135716198012233, Final Batch Loss: 0.003803650615736842\n",
      "Epoch 3373, Loss: 0.011058434378355742, Final Batch Loss: 0.003474549390375614\n",
      "Epoch 3374, Loss: 0.012183536309748888, Final Batch Loss: 0.003924625460058451\n",
      "Epoch 3375, Loss: 0.006139180390164256, Final Batch Loss: 0.0039799995720386505\n",
      "Epoch 3376, Loss: 0.042309390380978584, Final Batch Loss: 0.03197912871837616\n",
      "Epoch 3377, Loss: 0.02318137465044856, Final Batch Loss: 0.005876007955521345\n",
      "Epoch 3378, Loss: 0.020939012989401817, Final Batch Loss: 0.01304392796009779\n",
      "Epoch 3379, Loss: 0.016750558745115995, Final Batch Loss: 0.014898435212671757\n",
      "Epoch 3380, Loss: 0.014252313179895282, Final Batch Loss: 0.01132295187562704\n",
      "Epoch 3381, Loss: 0.05211804620921612, Final Batch Loss: 0.01311022974550724\n",
      "Epoch 3382, Loss: 0.012293903389945626, Final Batch Loss: 0.0025592867750674486\n",
      "Epoch 3383, Loss: 0.02861112169921398, Final Batch Loss: 0.016488388180732727\n",
      "Epoch 3384, Loss: 0.008936202619224787, Final Batch Loss: 0.006545559968799353\n",
      "Epoch 3385, Loss: 0.03117459313943982, Final Batch Loss: 0.0042904396541416645\n",
      "Epoch 3386, Loss: 0.008528945967555046, Final Batch Loss: 0.00395722221583128\n",
      "Epoch 3387, Loss: 0.035278466530144215, Final Batch Loss: 0.014190572313964367\n",
      "Epoch 3388, Loss: 0.010954180732369423, Final Batch Loss: 0.003970038611441851\n",
      "Epoch 3389, Loss: 0.03556197416037321, Final Batch Loss: 0.02208746410906315\n",
      "Epoch 3390, Loss: 0.007932117441669106, Final Batch Loss: 0.00438376609236002\n",
      "Epoch 3391, Loss: 0.018032415071502328, Final Batch Loss: 0.002191834384575486\n",
      "Epoch 3392, Loss: 0.03562415391206741, Final Batch Loss: 0.02039283700287342\n",
      "Epoch 3393, Loss: 0.021076928824186325, Final Batch Loss: 0.008184234611690044\n",
      "Epoch 3394, Loss: 0.02861284837126732, Final Batch Loss: 0.00525379553437233\n",
      "Epoch 3395, Loss: 0.015029880683869123, Final Batch Loss: 0.00659858388826251\n",
      "Epoch 3396, Loss: 0.012628189520910382, Final Batch Loss: 0.0036068253684788942\n",
      "Epoch 3397, Loss: 0.010615141363814473, Final Batch Loss: 0.003764556022360921\n",
      "Epoch 3398, Loss: 0.013352335430681705, Final Batch Loss: 0.011301456019282341\n",
      "Epoch 3399, Loss: 0.015394296497106552, Final Batch Loss: 0.006041131913661957\n",
      "Epoch 3400, Loss: 0.017913438845425844, Final Batch Loss: 0.005919378716498613\n",
      "Epoch 3401, Loss: 0.020327931735664606, Final Batch Loss: 0.0043706209398806095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3402, Loss: 0.014048249227926135, Final Batch Loss: 0.0020758879836648703\n",
      "Epoch 3403, Loss: 0.040223442018032074, Final Batch Loss: 0.03338918834924698\n",
      "Epoch 3404, Loss: 0.0066858259961009026, Final Batch Loss: 0.0034663735423237085\n",
      "Epoch 3405, Loss: 0.030349645763635635, Final Batch Loss: 0.013550540432333946\n",
      "Epoch 3406, Loss: 0.09764646738767624, Final Batch Loss: 0.0644131600856781\n",
      "Epoch 3407, Loss: 0.019498773850500584, Final Batch Loss: 0.0053583718836307526\n",
      "Epoch 3408, Loss: 0.013708198675885797, Final Batch Loss: 0.0031781357247382402\n",
      "Epoch 3409, Loss: 0.058841750491410494, Final Batch Loss: 0.05266665294766426\n",
      "Epoch 3410, Loss: 0.0050596515648067, Final Batch Loss: 0.0028077049646526575\n",
      "Epoch 3411, Loss: 0.06185128912329674, Final Batch Loss: 0.04178088158369064\n",
      "Epoch 3412, Loss: 0.02516852831467986, Final Batch Loss: 0.004122236277908087\n",
      "Epoch 3413, Loss: 0.019733977504074574, Final Batch Loss: 0.004990323446691036\n",
      "Epoch 3414, Loss: 0.00911683333106339, Final Batch Loss: 0.001238674158230424\n",
      "Epoch 3415, Loss: 0.032518362160772085, Final Batch Loss: 0.0022357101552188396\n",
      "Epoch 3416, Loss: 0.015251552686095238, Final Batch Loss: 0.006222336553037167\n",
      "Epoch 3417, Loss: 0.02271033707074821, Final Batch Loss: 0.021400105208158493\n",
      "Epoch 3418, Loss: 0.020046283258125186, Final Batch Loss: 0.017604229971766472\n",
      "Epoch 3419, Loss: 0.01794984540902078, Final Batch Loss: 0.001986890798434615\n",
      "Epoch 3420, Loss: 0.048268143087625504, Final Batch Loss: 0.004732582718133926\n",
      "Epoch 3421, Loss: 0.016539097297936678, Final Batch Loss: 0.007171241100877523\n",
      "Epoch 3422, Loss: 0.005302367266267538, Final Batch Loss: 0.002148639177903533\n",
      "Epoch 3423, Loss: 0.00851383781991899, Final Batch Loss: 0.0061026946641504765\n",
      "Epoch 3424, Loss: 0.005728546413592994, Final Batch Loss: 0.0017228451324626803\n",
      "Epoch 3425, Loss: 0.020183158572763205, Final Batch Loss: 0.006079032551497221\n",
      "Epoch 3426, Loss: 0.02138984017074108, Final Batch Loss: 0.013160337693989277\n",
      "Epoch 3427, Loss: 0.015567227732390165, Final Batch Loss: 0.010413159616291523\n",
      "Epoch 3428, Loss: 0.029325325042009354, Final Batch Loss: 0.015317029319703579\n",
      "Epoch 3429, Loss: 0.02016043895855546, Final Batch Loss: 0.01773468591272831\n",
      "Epoch 3430, Loss: 0.00887936307117343, Final Batch Loss: 0.0015292498283088207\n",
      "Epoch 3431, Loss: 0.00979667087085545, Final Batch Loss: 0.0016879665199667215\n",
      "Epoch 3432, Loss: 0.02894475730136037, Final Batch Loss: 0.00651557045057416\n",
      "Epoch 3433, Loss: 0.02975062746554613, Final Batch Loss: 0.010905847884714603\n",
      "Epoch 3434, Loss: 0.05516695976257324, Final Batch Loss: 0.003275681287050247\n",
      "Epoch 3435, Loss: 0.011077074334025383, Final Batch Loss: 0.00370651762932539\n",
      "Epoch 3436, Loss: 0.06155101768672466, Final Batch Loss: 0.028158867731690407\n",
      "Epoch 3437, Loss: 0.00607799191493541, Final Batch Loss: 0.0013481035130098462\n",
      "Epoch 3438, Loss: 0.09940820001065731, Final Batch Loss: 0.07823646068572998\n",
      "Epoch 3439, Loss: 0.0207732398994267, Final Batch Loss: 0.003650153521448374\n",
      "Epoch 3440, Loss: 0.018427370814606547, Final Batch Loss: 0.0029984682332724333\n",
      "Epoch 3441, Loss: 0.03601076453924179, Final Batch Loss: 0.018585896119475365\n",
      "Epoch 3442, Loss: 0.0162614225409925, Final Batch Loss: 0.0015269885770976543\n",
      "Epoch 3443, Loss: 0.009329660097137094, Final Batch Loss: 0.006048952229321003\n",
      "Epoch 3444, Loss: 0.02109920885413885, Final Batch Loss: 0.007630803622305393\n",
      "Epoch 3445, Loss: 0.012231722939759493, Final Batch Loss: 0.004979523364454508\n",
      "Epoch 3446, Loss: 0.037610131315886974, Final Batch Loss: 0.023753413930535316\n",
      "Epoch 3447, Loss: 0.009895347757264972, Final Batch Loss: 0.007322082296013832\n",
      "Epoch 3448, Loss: 0.061830117367208004, Final Batch Loss: 0.014503098092973232\n",
      "Epoch 3449, Loss: 0.07099458016455173, Final Batch Loss: 0.01178872399032116\n",
      "Epoch 3450, Loss: 0.018508074339479208, Final Batch Loss: 0.013714170083403587\n",
      "Epoch 3451, Loss: 0.020145184360444546, Final Batch Loss: 0.013402549549937248\n",
      "Epoch 3452, Loss: 0.01682338654063642, Final Batch Loss: 0.013754110783338547\n",
      "Epoch 3453, Loss: 0.025861657690256834, Final Batch Loss: 0.023749392479658127\n",
      "Epoch 3454, Loss: 0.015001427615061402, Final Batch Loss: 0.011184495873749256\n",
      "Epoch 3455, Loss: 0.007637253263965249, Final Batch Loss: 0.0024601586628705263\n",
      "Epoch 3456, Loss: 0.010392563417553902, Final Batch Loss: 0.0028337230905890465\n",
      "Epoch 3457, Loss: 0.008190677966922522, Final Batch Loss: 0.0034664729610085487\n",
      "Epoch 3458, Loss: 0.011742950417101383, Final Batch Loss: 0.00285501591861248\n",
      "Epoch 3459, Loss: 0.029226779006421566, Final Batch Loss: 0.016431203112006187\n",
      "Epoch 3460, Loss: 0.004457836039364338, Final Batch Loss: 0.002194694709032774\n",
      "Epoch 3461, Loss: 0.018952289363369346, Final Batch Loss: 0.015607615932822227\n",
      "Epoch 3462, Loss: 0.023643739987164736, Final Batch Loss: 0.02172975242137909\n",
      "Epoch 3463, Loss: 0.01758416136726737, Final Batch Loss: 0.005320808384567499\n",
      "Epoch 3464, Loss: 0.008448187494650483, Final Batch Loss: 0.0062494887970387936\n",
      "Epoch 3465, Loss: 0.00800229562446475, Final Batch Loss: 0.0033614812418818474\n",
      "Epoch 3466, Loss: 0.029755035415291786, Final Batch Loss: 0.026302196085453033\n",
      "Epoch 3467, Loss: 0.011541250860318542, Final Batch Loss: 0.002200640505179763\n",
      "Epoch 3468, Loss: 0.00799340452067554, Final Batch Loss: 0.004223603289574385\n",
      "Epoch 3469, Loss: 0.02882569981738925, Final Batch Loss: 0.023717720061540604\n",
      "Epoch 3470, Loss: 0.026797576807439327, Final Batch Loss: 0.010199136100709438\n",
      "Epoch 3471, Loss: 0.009225919377058744, Final Batch Loss: 0.003530232235789299\n",
      "Epoch 3472, Loss: 0.00669427658431232, Final Batch Loss: 0.004489053040742874\n",
      "Epoch 3473, Loss: 0.06892313994467258, Final Batch Loss: 0.04202301427721977\n",
      "Epoch 3474, Loss: 0.015478687826544046, Final Batch Loss: 0.009718191809952259\n",
      "Epoch 3475, Loss: 0.011740004061721265, Final Batch Loss: 0.010012821294367313\n",
      "Epoch 3476, Loss: 0.008998566307127476, Final Batch Loss: 0.004450047854334116\n",
      "Epoch 3477, Loss: 0.03786221030168235, Final Batch Loss: 0.0033086228650063276\n",
      "Epoch 3478, Loss: 0.006886894814670086, Final Batch Loss: 0.0024303272366523743\n",
      "Epoch 3479, Loss: 0.03172960318624973, Final Batch Loss: 0.016994629055261612\n",
      "Epoch 3480, Loss: 0.007166886702179909, Final Batch Loss: 0.003421300556510687\n",
      "Epoch 3481, Loss: 0.03973958734422922, Final Batch Loss: 0.006689880974590778\n",
      "Epoch 3482, Loss: 0.004785147262737155, Final Batch Loss: 0.0028281945269554853\n",
      "Epoch 3483, Loss: 0.005201564636081457, Final Batch Loss: 0.003538798075169325\n",
      "Epoch 3484, Loss: 0.00410242669750005, Final Batch Loss: 0.0012401911662891507\n",
      "Epoch 3485, Loss: 0.029463566839694977, Final Batch Loss: 0.026467492803931236\n",
      "Epoch 3486, Loss: 0.04627794318366796, Final Batch Loss: 0.0018713950412347913\n",
      "Epoch 3487, Loss: 0.003470053314231336, Final Batch Loss: 0.0017404138343408704\n",
      "Epoch 3488, Loss: 0.0040524391224607825, Final Batch Loss: 0.0025816052220761776\n",
      "Epoch 3489, Loss: 0.01531911175698042, Final Batch Loss: 0.0037905294448137283\n",
      "Epoch 3490, Loss: 0.026279345620423555, Final Batch Loss: 0.006560247857123613\n",
      "Epoch 3491, Loss: 0.007344231475144625, Final Batch Loss: 0.004244047682732344\n",
      "Epoch 3492, Loss: 0.03397439233958721, Final Batch Loss: 0.023491941392421722\n",
      "Epoch 3493, Loss: 0.010180273558944464, Final Batch Loss: 0.00611089589074254\n",
      "Epoch 3494, Loss: 0.01793799363076687, Final Batch Loss: 0.01506487000733614\n",
      "Epoch 3495, Loss: 0.014126281021162868, Final Batch Loss: 0.003511493792757392\n",
      "Epoch 3496, Loss: 0.02822323562577367, Final Batch Loss: 0.0065353331156075\n",
      "Epoch 3497, Loss: 0.01802247390151024, Final Batch Loss: 0.0048427339643239975\n",
      "Epoch 3498, Loss: 0.006814625347033143, Final Batch Loss: 0.003933179657906294\n",
      "Epoch 3499, Loss: 0.025462147779762745, Final Batch Loss: 0.011960207484662533\n",
      "Epoch 3500, Loss: 0.011055196868255734, Final Batch Loss: 0.0036966593470424414\n",
      "Epoch 3501, Loss: 0.01519980258308351, Final Batch Loss: 0.0011252553667873144\n",
      "Epoch 3502, Loss: 0.012575872940942645, Final Batch Loss: 0.003154702251777053\n",
      "Epoch 3503, Loss: 0.017935110721737146, Final Batch Loss: 0.01570865511894226\n",
      "Epoch 3504, Loss: 0.018229249864816666, Final Batch Loss: 0.00585675984621048\n",
      "Epoch 3505, Loss: 0.06302085611969233, Final Batch Loss: 0.053295064717531204\n",
      "Epoch 3506, Loss: 0.029864010866731405, Final Batch Loss: 0.02213524840772152\n",
      "Epoch 3507, Loss: 0.013223775196820498, Final Batch Loss: 0.0062868632376194\n",
      "Epoch 3508, Loss: 0.022388585843145847, Final Batch Loss: 0.01822574995458126\n",
      "Epoch 3509, Loss: 0.00673068268224597, Final Batch Loss: 0.003185098757967353\n",
      "Epoch 3510, Loss: 0.0192958174739033, Final Batch Loss: 0.01664694957435131\n",
      "Epoch 3511, Loss: 0.008302884874865413, Final Batch Loss: 0.0012400054838508368\n",
      "Epoch 3512, Loss: 0.011548008769750595, Final Batch Loss: 0.008539621718227863\n",
      "Epoch 3513, Loss: 0.010146498680114746, Final Batch Loss: 0.003750863950699568\n",
      "Epoch 3514, Loss: 0.015957776457071304, Final Batch Loss: 0.004695633426308632\n",
      "Epoch 3515, Loss: 0.008546521072275937, Final Batch Loss: 0.0013209945755079389\n",
      "Epoch 3516, Loss: 0.047666809521615505, Final Batch Loss: 0.04120180383324623\n",
      "Epoch 3517, Loss: 0.017065584659576416, Final Batch Loss: 0.011539256200194359\n",
      "Epoch 3518, Loss: 0.008478497853502631, Final Batch Loss: 0.003350556595250964\n",
      "Epoch 3519, Loss: 0.028741762973368168, Final Batch Loss: 0.018332917243242264\n",
      "Epoch 3520, Loss: 0.011033897288143635, Final Batch Loss: 0.005725114140659571\n",
      "Epoch 3521, Loss: 0.0235318667255342, Final Batch Loss: 0.004719469230622053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3522, Loss: 0.013157608918845654, Final Batch Loss: 0.006455203052610159\n",
      "Epoch 3523, Loss: 0.030377278104424477, Final Batch Loss: 0.01420312188565731\n",
      "Epoch 3524, Loss: 0.026790187694132328, Final Batch Loss: 0.015443392097949982\n",
      "Epoch 3525, Loss: 0.009331199806183577, Final Batch Loss: 0.002280739601701498\n",
      "Epoch 3526, Loss: 0.005309735890477896, Final Batch Loss: 0.002960706129670143\n",
      "Epoch 3527, Loss: 0.01875681569799781, Final Batch Loss: 0.0033767507411539555\n",
      "Epoch 3528, Loss: 0.021667612716555595, Final Batch Loss: 0.00879725068807602\n",
      "Epoch 3529, Loss: 0.010076332604512572, Final Batch Loss: 0.0024221206549555063\n",
      "Epoch 3530, Loss: 0.012148280162364244, Final Batch Loss: 0.0025348239578306675\n",
      "Epoch 3531, Loss: 0.010981646832078695, Final Batch Loss: 0.004279742017388344\n",
      "Epoch 3532, Loss: 0.06921882252208889, Final Batch Loss: 0.06535165756940842\n",
      "Epoch 3533, Loss: 0.03799934312701225, Final Batch Loss: 0.02539178542792797\n",
      "Epoch 3534, Loss: 0.06344045349396765, Final Batch Loss: 0.0023226107005029917\n",
      "Epoch 3535, Loss: 0.012368601281195879, Final Batch Loss: 0.006579367443919182\n",
      "Epoch 3536, Loss: 0.01515574986115098, Final Batch Loss: 0.005741697270423174\n",
      "Epoch 3537, Loss: 0.014285436365753412, Final Batch Loss: 0.010156127624213696\n",
      "Epoch 3538, Loss: 0.030057760886847973, Final Batch Loss: 0.021529505029320717\n",
      "Epoch 3539, Loss: 0.013312302180565894, Final Batch Loss: 0.0011127159232273698\n",
      "Epoch 3540, Loss: 0.04156443802639842, Final Batch Loss: 0.035786788910627365\n",
      "Epoch 3541, Loss: 0.04304342716932297, Final Batch Loss: 0.0364212803542614\n",
      "Epoch 3542, Loss: 0.044171045534312725, Final Batch Loss: 0.0021352311596274376\n",
      "Epoch 3543, Loss: 0.008432638365775347, Final Batch Loss: 0.0043401638977229595\n",
      "Epoch 3544, Loss: 0.12940655648708344, Final Batch Loss: 0.1128176599740982\n",
      "Epoch 3545, Loss: 0.023807242512702942, Final Batch Loss: 0.015295728109776974\n",
      "Epoch 3546, Loss: 0.04401722177863121, Final Batch Loss: 0.026811473071575165\n",
      "Epoch 3547, Loss: 0.02419059770181775, Final Batch Loss: 0.01844920963048935\n",
      "Epoch 3548, Loss: 0.029904862865805626, Final Batch Loss: 0.014836978167295456\n",
      "Epoch 3549, Loss: 0.029691684991121292, Final Batch Loss: 0.008223121985793114\n",
      "Epoch 3550, Loss: 0.02802176307886839, Final Batch Loss: 0.018668925389647484\n",
      "Epoch 3551, Loss: 0.0747053325176239, Final Batch Loss: 0.04814916476607323\n",
      "Epoch 3552, Loss: 0.07069671293720603, Final Batch Loss: 0.004214623477309942\n",
      "Epoch 3553, Loss: 0.05333237908780575, Final Batch Loss: 0.023082586005330086\n",
      "Epoch 3554, Loss: 0.10549969598650932, Final Batch Loss: 0.02389461174607277\n",
      "Epoch 3555, Loss: 0.030148212797939777, Final Batch Loss: 0.0017191870138049126\n",
      "Epoch 3556, Loss: 0.014996801270172, Final Batch Loss: 0.0027070946525782347\n",
      "Epoch 3557, Loss: 0.020177785656414926, Final Batch Loss: 0.018693769350647926\n",
      "Epoch 3558, Loss: 0.05731226271018386, Final Batch Loss: 0.002202217932790518\n",
      "Epoch 3559, Loss: 0.012542410288006067, Final Batch Loss: 0.005181347951292992\n",
      "Epoch 3560, Loss: 0.022914784029126167, Final Batch Loss: 0.003765743225812912\n",
      "Epoch 3561, Loss: 0.07723021972924471, Final Batch Loss: 0.06327134370803833\n",
      "Epoch 3562, Loss: 0.005657597212120891, Final Batch Loss: 0.0024318720679730177\n",
      "Epoch 3563, Loss: 0.04359038034453988, Final Batch Loss: 0.03812443092465401\n",
      "Epoch 3564, Loss: 0.02516201790422201, Final Batch Loss: 0.018609387800097466\n",
      "Epoch 3565, Loss: 0.027391791809350252, Final Batch Loss: 0.022978726774454117\n",
      "Epoch 3566, Loss: 0.047621702775359154, Final Batch Loss: 0.024509841576218605\n",
      "Epoch 3567, Loss: 0.01652544317767024, Final Batch Loss: 0.006220599170774221\n",
      "Epoch 3568, Loss: 0.03792958706617355, Final Batch Loss: 0.025671551004052162\n",
      "Epoch 3569, Loss: 0.023249241057783365, Final Batch Loss: 0.016994783654808998\n",
      "Epoch 3570, Loss: 0.030021999788004905, Final Batch Loss: 0.0009008076158352196\n",
      "Epoch 3571, Loss: 0.009657555492594838, Final Batch Loss: 0.006549799349159002\n",
      "Epoch 3572, Loss: 0.015018829610198736, Final Batch Loss: 0.00463389465585351\n",
      "Epoch 3573, Loss: 0.008702291874215007, Final Batch Loss: 0.005124241579324007\n",
      "Epoch 3574, Loss: 0.006428738124668598, Final Batch Loss: 0.00209015142172575\n",
      "Epoch 3575, Loss: 0.06490255100652575, Final Batch Loss: 0.06257224828004837\n",
      "Epoch 3576, Loss: 0.037722984328866005, Final Batch Loss: 0.0157963577657938\n",
      "Epoch 3577, Loss: 0.01608393294736743, Final Batch Loss: 0.004407990258187056\n",
      "Epoch 3578, Loss: 0.022682259790599346, Final Batch Loss: 0.014152778312563896\n",
      "Epoch 3579, Loss: 0.010565423406660557, Final Batch Loss: 0.0040405988693237305\n",
      "Epoch 3580, Loss: 0.011314981151372194, Final Batch Loss: 0.0028972853906452656\n",
      "Epoch 3581, Loss: 0.019251885125413537, Final Batch Loss: 0.01776067353785038\n",
      "Epoch 3582, Loss: 0.012913177255541086, Final Batch Loss: 0.0090815220028162\n",
      "Epoch 3583, Loss: 0.005743802525103092, Final Batch Loss: 0.001869159983471036\n",
      "Epoch 3584, Loss: 0.03089142357930541, Final Batch Loss: 0.025763919577002525\n",
      "Epoch 3585, Loss: 0.018558782525360584, Final Batch Loss: 0.004561816342175007\n",
      "Epoch 3586, Loss: 0.007895175134763122, Final Batch Loss: 0.0036208529490977526\n",
      "Epoch 3587, Loss: 0.010260027600452304, Final Batch Loss: 0.007486915215849876\n",
      "Epoch 3588, Loss: 0.00801442377269268, Final Batch Loss: 0.00554662523791194\n",
      "Epoch 3589, Loss: 0.01113087183330208, Final Batch Loss: 0.0018867604667320848\n",
      "Epoch 3590, Loss: 0.0770590202882886, Final Batch Loss: 0.07304996252059937\n",
      "Epoch 3591, Loss: 0.03187151998281479, Final Batch Loss: 0.015202919021248817\n",
      "Epoch 3592, Loss: 0.035996961407363415, Final Batch Loss: 0.023071272298693657\n",
      "Epoch 3593, Loss: 0.022239912068471313, Final Batch Loss: 0.0038855166640132666\n",
      "Epoch 3594, Loss: 0.06446973979473114, Final Batch Loss: 0.02596094459295273\n",
      "Epoch 3595, Loss: 0.018295807298272848, Final Batch Loss: 0.006694556679576635\n",
      "Epoch 3596, Loss: 0.019856940489262342, Final Batch Loss: 0.015704479068517685\n",
      "Epoch 3597, Loss: 0.031167794950306416, Final Batch Loss: 0.014374260790646076\n",
      "Epoch 3598, Loss: 0.012844559270888567, Final Batch Loss: 0.008176886476576328\n",
      "Epoch 3599, Loss: 0.03594580199569464, Final Batch Loss: 0.015016243793070316\n",
      "Epoch 3600, Loss: 0.05027442052960396, Final Batch Loss: 0.028772613033652306\n",
      "Epoch 3601, Loss: 0.01826943038031459, Final Batch Loss: 0.007510582450777292\n",
      "Epoch 3602, Loss: 0.01665150048211217, Final Batch Loss: 0.013631357811391354\n",
      "Epoch 3603, Loss: 0.1609571895096451, Final Batch Loss: 0.1576198786497116\n",
      "Epoch 3604, Loss: 0.025495968759059906, Final Batch Loss: 0.01431046798825264\n",
      "Epoch 3605, Loss: 0.0737237948924303, Final Batch Loss: 0.00772627629339695\n",
      "Epoch 3606, Loss: 0.018993466161191463, Final Batch Loss: 0.00504278764128685\n",
      "Epoch 3607, Loss: 0.015209665056318045, Final Batch Loss: 0.005249842535704374\n",
      "Epoch 3608, Loss: 0.03145941346883774, Final Batch Loss: 0.026644304394721985\n",
      "Epoch 3609, Loss: 0.02190883131697774, Final Batch Loss: 0.005700005684047937\n",
      "Epoch 3610, Loss: 0.008715452393516898, Final Batch Loss: 0.006167089566588402\n",
      "Epoch 3611, Loss: 0.031792301684617996, Final Batch Loss: 0.023609749972820282\n",
      "Epoch 3612, Loss: 0.011186360381543636, Final Batch Loss: 0.006605126895010471\n",
      "Epoch 3613, Loss: 0.09413008205592632, Final Batch Loss: 0.06871525943279266\n",
      "Epoch 3614, Loss: 0.0038469821447506547, Final Batch Loss: 0.0018400553381070495\n",
      "Epoch 3615, Loss: 0.030048652552068233, Final Batch Loss: 0.018240736797451973\n",
      "Epoch 3616, Loss: 0.016012059524655342, Final Batch Loss: 0.0029966961592435837\n",
      "Epoch 3617, Loss: 0.007790310541167855, Final Batch Loss: 0.00387594779022038\n",
      "Epoch 3618, Loss: 0.0098807904869318, Final Batch Loss: 0.004918474704027176\n",
      "Epoch 3619, Loss: 0.02388950064778328, Final Batch Loss: 0.008571476675570011\n",
      "Epoch 3620, Loss: 0.053315671160817146, Final Batch Loss: 0.03223390877246857\n",
      "Epoch 3621, Loss: 0.024952521547675133, Final Batch Loss: 0.018201349303126335\n",
      "Epoch 3622, Loss: 0.010330882389098406, Final Batch Loss: 0.004319517407566309\n",
      "Epoch 3623, Loss: 0.02636404847726226, Final Batch Loss: 0.0038120276294648647\n",
      "Epoch 3624, Loss: 0.009117111330851912, Final Batch Loss: 0.005388596095144749\n",
      "Epoch 3625, Loss: 0.0063892664620652795, Final Batch Loss: 0.0011015288764610887\n",
      "Epoch 3626, Loss: 0.03277645702473819, Final Batch Loss: 0.030001237988471985\n",
      "Epoch 3627, Loss: 0.011972547508776188, Final Batch Loss: 0.006832201965153217\n",
      "Epoch 3628, Loss: 0.032813445664942265, Final Batch Loss: 0.0037843557074666023\n",
      "Epoch 3629, Loss: 0.014203747734427452, Final Batch Loss: 0.011527519673109055\n",
      "Epoch 3630, Loss: 0.03598023997619748, Final Batch Loss: 0.03209982067346573\n",
      "Epoch 3631, Loss: 0.019965145271271467, Final Batch Loss: 0.013357777148485184\n",
      "Epoch 3632, Loss: 0.024977990426123142, Final Batch Loss: 0.017426831647753716\n",
      "Epoch 3633, Loss: 0.025165742728859186, Final Batch Loss: 0.01963387243449688\n",
      "Epoch 3634, Loss: 0.039104038849473, Final Batch Loss: 0.030002154409885406\n",
      "Epoch 3635, Loss: 0.010751968016847968, Final Batch Loss: 0.0020401503425091505\n",
      "Epoch 3636, Loss: 0.011277080047875643, Final Batch Loss: 0.004576264414936304\n",
      "Epoch 3637, Loss: 0.01512764859944582, Final Batch Loss: 0.0016058003529906273\n",
      "Epoch 3638, Loss: 0.008238534908741713, Final Batch Loss: 0.004032567609101534\n",
      "Epoch 3639, Loss: 0.03361308458261192, Final Batch Loss: 0.0023610021453350782\n",
      "Epoch 3640, Loss: 0.03503669623751193, Final Batch Loss: 0.0011564624728634953\n",
      "Epoch 3641, Loss: 0.0211928803473711, Final Batch Loss: 0.018317651003599167\n",
      "Epoch 3642, Loss: 0.03144324291497469, Final Batch Loss: 0.013820110820233822\n",
      "Epoch 3643, Loss: 0.010496529284864664, Final Batch Loss: 0.001917629037052393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3644, Loss: 0.008181478129699826, Final Batch Loss: 0.0051251864060759544\n",
      "Epoch 3645, Loss: 0.021455520763993263, Final Batch Loss: 0.013233067467808723\n",
      "Epoch 3646, Loss: 0.010036278050392866, Final Batch Loss: 0.006065598223358393\n",
      "Epoch 3647, Loss: 0.021623421926051378, Final Batch Loss: 0.006186455022543669\n",
      "Epoch 3648, Loss: 0.005939629510976374, Final Batch Loss: 0.001299262628890574\n",
      "Epoch 3649, Loss: 0.019621169427409768, Final Batch Loss: 0.01654922030866146\n",
      "Epoch 3650, Loss: 0.017287034075707197, Final Batch Loss: 0.00780452461913228\n",
      "Epoch 3651, Loss: 0.028311329893767834, Final Batch Loss: 0.013796183280646801\n",
      "Epoch 3652, Loss: 0.011229120194911957, Final Batch Loss: 0.0034611602313816547\n",
      "Epoch 3653, Loss: 0.007933652261272073, Final Batch Loss: 0.0018671115394681692\n",
      "Epoch 3654, Loss: 0.009029427077621222, Final Batch Loss: 0.0030431319028139114\n",
      "Epoch 3655, Loss: 0.0291024181060493, Final Batch Loss: 0.02464877814054489\n",
      "Epoch 3656, Loss: 0.01653039176017046, Final Batch Loss: 0.006672084331512451\n",
      "Epoch 3657, Loss: 0.010182453668676317, Final Batch Loss: 0.00925495382398367\n",
      "Epoch 3658, Loss: 0.0505053224042058, Final Batch Loss: 0.008582604117691517\n",
      "Epoch 3659, Loss: 0.021342027932405472, Final Batch Loss: 0.015990084037184715\n",
      "Epoch 3660, Loss: 0.005759412772022188, Final Batch Loss: 0.0014564291341230273\n",
      "Epoch 3661, Loss: 0.04709763824939728, Final Batch Loss: 0.02556556463241577\n",
      "Epoch 3662, Loss: 0.025552047649398446, Final Batch Loss: 0.021711096167564392\n",
      "Epoch 3663, Loss: 0.050693133380264044, Final Batch Loss: 0.004025218542665243\n",
      "Epoch 3664, Loss: 0.037772849667817354, Final Batch Loss: 0.03548545017838478\n",
      "Epoch 3665, Loss: 0.03845216520130634, Final Batch Loss: 0.01810448430478573\n",
      "Epoch 3666, Loss: 0.020163567271083593, Final Batch Loss: 0.016826501116156578\n",
      "Epoch 3667, Loss: 0.11135873384773731, Final Batch Loss: 0.018402772024273872\n",
      "Epoch 3668, Loss: 0.07420637272298336, Final Batch Loss: 0.028549151495099068\n",
      "Epoch 3669, Loss: 0.013437808025628328, Final Batch Loss: 0.010180857963860035\n",
      "Epoch 3670, Loss: 0.04436056734994054, Final Batch Loss: 0.006670099217444658\n",
      "Epoch 3671, Loss: 0.03400832507759333, Final Batch Loss: 0.021827155724167824\n",
      "Epoch 3672, Loss: 0.022739372681826353, Final Batch Loss: 0.00659737316891551\n",
      "Epoch 3673, Loss: 0.049042098224163055, Final Batch Loss: 0.03685181215405464\n",
      "Epoch 3674, Loss: 0.04068693891167641, Final Batch Loss: 0.02034149318933487\n",
      "Epoch 3675, Loss: 0.012409154791384935, Final Batch Loss: 0.009393801912665367\n",
      "Epoch 3676, Loss: 0.06197254918515682, Final Batch Loss: 0.04512554407119751\n",
      "Epoch 3677, Loss: 0.01747299637645483, Final Batch Loss: 0.013587131164968014\n",
      "Epoch 3678, Loss: 0.013234644196927547, Final Batch Loss: 0.009825728833675385\n",
      "Epoch 3679, Loss: 0.06611096113920212, Final Batch Loss: 0.03828645497560501\n",
      "Epoch 3680, Loss: 0.017248958349227905, Final Batch Loss: 0.0073779672384262085\n",
      "Epoch 3681, Loss: 0.009657422546297312, Final Batch Loss: 0.0032809083350002766\n",
      "Epoch 3682, Loss: 0.049428753554821014, Final Batch Loss: 0.008019383996725082\n",
      "Epoch 3683, Loss: 0.03327052900567651, Final Batch Loss: 0.0051873777993023396\n",
      "Epoch 3684, Loss: 0.01651344122365117, Final Batch Loss: 0.0034428066574037075\n",
      "Epoch 3685, Loss: 0.026306177489459515, Final Batch Loss: 0.02266218513250351\n",
      "Epoch 3686, Loss: 0.0634571872651577, Final Batch Loss: 0.02776174619793892\n",
      "Epoch 3687, Loss: 0.017614767886698246, Final Batch Loss: 0.004927159287035465\n",
      "Epoch 3688, Loss: 0.03897366486489773, Final Batch Loss: 0.015844160690903664\n",
      "Epoch 3689, Loss: 0.029203570913523436, Final Batch Loss: 0.022060932591557503\n",
      "Epoch 3690, Loss: 0.010556858498603106, Final Batch Loss: 0.004731381312012672\n",
      "Epoch 3691, Loss: 0.007176298648118973, Final Batch Loss: 0.004633429925888777\n",
      "Epoch 3692, Loss: 0.021325065754354, Final Batch Loss: 0.011957184411585331\n",
      "Epoch 3693, Loss: 0.018941064830869436, Final Batch Loss: 0.003960378002375364\n",
      "Epoch 3694, Loss: 0.022790318354964256, Final Batch Loss: 0.009252958931028843\n",
      "Epoch 3695, Loss: 0.009128710720688105, Final Batch Loss: 0.005343738943338394\n",
      "Epoch 3696, Loss: 0.00889763399027288, Final Batch Loss: 0.006820020731538534\n",
      "Epoch 3697, Loss: 0.0231608422473073, Final Batch Loss: 0.021689696237444878\n",
      "Epoch 3698, Loss: 0.01586794783361256, Final Batch Loss: 0.012712857685983181\n",
      "Epoch 3699, Loss: 0.007761470274999738, Final Batch Loss: 0.006251371931284666\n",
      "Epoch 3700, Loss: 0.0060887411236763, Final Batch Loss: 0.004093743395060301\n",
      "Epoch 3701, Loss: 0.010205317987129092, Final Batch Loss: 0.0020054138731211424\n",
      "Epoch 3702, Loss: 0.018266982166096568, Final Batch Loss: 0.003026077290996909\n",
      "Epoch 3703, Loss: 0.02063423953950405, Final Batch Loss: 0.016026299446821213\n",
      "Epoch 3704, Loss: 0.0078961793333292, Final Batch Loss: 0.0026064561679959297\n",
      "Epoch 3705, Loss: 0.00845862994901836, Final Batch Loss: 0.006770498119294643\n",
      "Epoch 3706, Loss: 0.00989106623455882, Final Batch Loss: 0.00595257431268692\n",
      "Epoch 3707, Loss: 0.029400461819022894, Final Batch Loss: 0.02435232140123844\n",
      "Epoch 3708, Loss: 0.03257338795810938, Final Batch Loss: 0.01420223992317915\n",
      "Epoch 3709, Loss: 0.016437030863016844, Final Batch Loss: 0.014541307464241982\n",
      "Epoch 3710, Loss: 0.05413866601884365, Final Batch Loss: 0.006339939311146736\n",
      "Epoch 3711, Loss: 0.009450451005250216, Final Batch Loss: 0.00642284844070673\n",
      "Epoch 3712, Loss: 0.005601149518042803, Final Batch Loss: 0.002682080026715994\n",
      "Epoch 3713, Loss: 0.0441159806214273, Final Batch Loss: 0.006839183624833822\n",
      "Epoch 3714, Loss: 0.06581472232937813, Final Batch Loss: 0.007978703826665878\n",
      "Epoch 3715, Loss: 0.008235443383455276, Final Batch Loss: 0.0058532701805233955\n",
      "Epoch 3716, Loss: 0.10316453129053116, Final Batch Loss: 0.0675339549779892\n",
      "Epoch 3717, Loss: 0.01053414843045175, Final Batch Loss: 0.007442357949912548\n",
      "Epoch 3718, Loss: 0.013570989016443491, Final Batch Loss: 0.007428471464663744\n",
      "Epoch 3719, Loss: 0.0428065937012434, Final Batch Loss: 0.023896753787994385\n",
      "Epoch 3720, Loss: 0.04244345426559448, Final Batch Loss: 0.0062266141176223755\n",
      "Epoch 3721, Loss: 0.015057360287755728, Final Batch Loss: 0.00259708845987916\n",
      "Epoch 3722, Loss: 0.014486053958535194, Final Batch Loss: 0.010214630514383316\n",
      "Epoch 3723, Loss: 0.024595197290182114, Final Batch Loss: 0.009192712604999542\n",
      "Epoch 3724, Loss: 0.10001439321786165, Final Batch Loss: 0.08642661571502686\n",
      "Epoch 3725, Loss: 0.02950626937672496, Final Batch Loss: 0.006807893048971891\n",
      "Epoch 3726, Loss: 0.06313901953399181, Final Batch Loss: 0.02533947117626667\n",
      "Epoch 3727, Loss: 0.017913992749527097, Final Batch Loss: 0.002436236711218953\n",
      "Epoch 3728, Loss: 0.043779512867331505, Final Batch Loss: 0.002266181632876396\n",
      "Epoch 3729, Loss: 0.010659312829375267, Final Batch Loss: 0.005656232591718435\n",
      "Epoch 3730, Loss: 0.022596651688218117, Final Batch Loss: 0.012102106586098671\n",
      "Epoch 3731, Loss: 0.040315307676792145, Final Batch Loss: 0.02018861658871174\n",
      "Epoch 3732, Loss: 0.03210295643657446, Final Batch Loss: 0.011876457370817661\n",
      "Epoch 3733, Loss: 0.005933455191552639, Final Batch Loss: 0.0031508798711001873\n",
      "Epoch 3734, Loss: 0.007239705417305231, Final Batch Loss: 0.0031783003360033035\n",
      "Epoch 3735, Loss: 0.021759350784122944, Final Batch Loss: 0.0114801786839962\n",
      "Epoch 3736, Loss: 0.02096981555223465, Final Batch Loss: 0.006093671545386314\n",
      "Epoch 3737, Loss: 0.012493118410930037, Final Batch Loss: 0.010131091810762882\n",
      "Epoch 3738, Loss: 0.009311892790719867, Final Batch Loss: 0.0070291971787810326\n",
      "Epoch 3739, Loss: 0.03689195169135928, Final Batch Loss: 0.005425974261015654\n",
      "Epoch 3740, Loss: 0.011874717893078923, Final Batch Loss: 0.0029422801453620195\n",
      "Epoch 3741, Loss: 0.025653560645878315, Final Batch Loss: 0.003257286734879017\n",
      "Epoch 3742, Loss: 0.012300129048526287, Final Batch Loss: 0.008833606727421284\n",
      "Epoch 3743, Loss: 0.02703706710599363, Final Batch Loss: 0.024134043604135513\n",
      "Epoch 3744, Loss: 0.03429296053946018, Final Batch Loss: 0.004383280873298645\n",
      "Epoch 3745, Loss: 0.016028682701289654, Final Batch Loss: 0.003936947323381901\n",
      "Epoch 3746, Loss: 0.012169786030426621, Final Batch Loss: 0.00904027372598648\n",
      "Epoch 3747, Loss: 0.011518395971506834, Final Batch Loss: 0.005694277584552765\n",
      "Epoch 3748, Loss: 0.010265714954584837, Final Batch Loss: 0.005044615361839533\n",
      "Epoch 3749, Loss: 0.016871686559170485, Final Batch Loss: 0.004680176731199026\n",
      "Epoch 3750, Loss: 0.013415495632216334, Final Batch Loss: 0.0030035253148525953\n",
      "Epoch 3751, Loss: 0.006056144135072827, Final Batch Loss: 0.0025696808006614447\n",
      "Epoch 3752, Loss: 0.011962700868025422, Final Batch Loss: 0.003093019360676408\n",
      "Epoch 3753, Loss: 0.006289853015914559, Final Batch Loss: 0.0020375053863972425\n",
      "Epoch 3754, Loss: 0.037201310973614454, Final Batch Loss: 0.029527299106121063\n",
      "Epoch 3755, Loss: 0.02410598937422037, Final Batch Loss: 0.019921544939279556\n",
      "Epoch 3756, Loss: 0.021952536655589938, Final Batch Loss: 0.019410783424973488\n",
      "Epoch 3757, Loss: 0.037937555462121964, Final Batch Loss: 0.025108326226472855\n",
      "Epoch 3758, Loss: 0.029494558461010456, Final Batch Loss: 0.020513901486992836\n",
      "Epoch 3759, Loss: 0.07560565695166588, Final Batch Loss: 0.045758843421936035\n",
      "Epoch 3760, Loss: 0.12447196245193481, Final Batch Loss: 0.0714663565158844\n",
      "Epoch 3761, Loss: 0.11919048056006432, Final Batch Loss: 0.10087919980287552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3762, Loss: 0.11437887977808714, Final Batch Loss: 0.013565863482654095\n",
      "Epoch 3763, Loss: 0.043898727744817734, Final Batch Loss: 0.017781298607587814\n",
      "Epoch 3764, Loss: 0.05736729875206947, Final Batch Loss: 0.02178391069173813\n",
      "Epoch 3765, Loss: 0.039737594313919544, Final Batch Loss: 0.011001038365066051\n",
      "Epoch 3766, Loss: 0.11729305610060692, Final Batch Loss: 0.0624990239739418\n",
      "Epoch 3767, Loss: 0.04326382023282349, Final Batch Loss: 0.0033078279811888933\n",
      "Epoch 3768, Loss: 0.025073916651308537, Final Batch Loss: 0.01336266752332449\n",
      "Epoch 3769, Loss: 0.11577362567186356, Final Batch Loss: 0.038787998259067535\n",
      "Epoch 3770, Loss: 0.07706429250538349, Final Batch Loss: 0.06829895824193954\n",
      "Epoch 3771, Loss: 0.06719892844557762, Final Batch Loss: 0.032638344913721085\n",
      "Epoch 3772, Loss: 0.011144254356622696, Final Batch Loss: 0.006930036004632711\n",
      "Epoch 3773, Loss: 0.040253923274576664, Final Batch Loss: 0.012325660325586796\n",
      "Epoch 3774, Loss: 0.048884425312280655, Final Batch Loss: 0.01848435215651989\n",
      "Epoch 3775, Loss: 0.03747338801622391, Final Batch Loss: 0.00915892980992794\n",
      "Epoch 3776, Loss: 0.07090874947607517, Final Batch Loss: 0.06317435950040817\n",
      "Epoch 3777, Loss: 0.018705228343605995, Final Batch Loss: 0.005952510051429272\n",
      "Epoch 3778, Loss: 0.06264430843293667, Final Batch Loss: 0.05034326761960983\n",
      "Epoch 3779, Loss: 0.029157621785998344, Final Batch Loss: 0.00482533685863018\n",
      "Epoch 3780, Loss: 0.02051544259302318, Final Batch Loss: 0.0037061560433357954\n",
      "Epoch 3781, Loss: 0.028759080218151212, Final Batch Loss: 0.003615348832681775\n",
      "Epoch 3782, Loss: 0.03603209042921662, Final Batch Loss: 0.02883070334792137\n",
      "Epoch 3783, Loss: 0.011401887517422438, Final Batch Loss: 0.004568096715956926\n",
      "Epoch 3784, Loss: 0.029461327474564314, Final Batch Loss: 0.004010848235338926\n",
      "Epoch 3785, Loss: 0.03998698526993394, Final Batch Loss: 0.006700052414089441\n",
      "Epoch 3786, Loss: 0.08088895957916975, Final Batch Loss: 0.06702230870723724\n",
      "Epoch 3787, Loss: 0.027252317871898413, Final Batch Loss: 0.020350640639662743\n",
      "Epoch 3788, Loss: 0.04449662286788225, Final Batch Loss: 0.03567807748913765\n",
      "Epoch 3789, Loss: 0.06060909666121006, Final Batch Loss: 0.0508812814950943\n",
      "Epoch 3790, Loss: 0.04257271904498339, Final Batch Loss: 0.013983127661049366\n",
      "Epoch 3791, Loss: 0.009740399662405252, Final Batch Loss: 0.004029153846204281\n",
      "Epoch 3792, Loss: 0.017963594757020473, Final Batch Loss: 0.014812403358519077\n",
      "Epoch 3793, Loss: 0.021384583320468664, Final Batch Loss: 0.006272226106375456\n",
      "Epoch 3794, Loss: 0.03902031481266022, Final Batch Loss: 0.02325088530778885\n",
      "Epoch 3795, Loss: 0.010189216583967209, Final Batch Loss: 0.00446886382997036\n",
      "Epoch 3796, Loss: 0.01835862477310002, Final Batch Loss: 0.00389292580075562\n",
      "Epoch 3797, Loss: 0.010080967098474503, Final Batch Loss: 0.004415582865476608\n",
      "Epoch 3798, Loss: 0.010036303661763668, Final Batch Loss: 0.004864710383117199\n",
      "Epoch 3799, Loss: 0.024302193894982338, Final Batch Loss: 0.012089557014405727\n",
      "Epoch 3800, Loss: 0.012428931659087539, Final Batch Loss: 0.003466182155534625\n",
      "Epoch 3801, Loss: 0.02077043103054166, Final Batch Loss: 0.015034827403724194\n",
      "Epoch 3802, Loss: 0.022033493500202894, Final Batch Loss: 0.004209951963275671\n",
      "Epoch 3803, Loss: 0.016892442014068365, Final Batch Loss: 0.004945445340126753\n",
      "Epoch 3804, Loss: 0.01443444611504674, Final Batch Loss: 0.006696899887174368\n",
      "Epoch 3805, Loss: 0.03961419407278299, Final Batch Loss: 0.0060688192024827\n",
      "Epoch 3806, Loss: 0.08644886314868927, Final Batch Loss: 0.054987091571092606\n",
      "Epoch 3807, Loss: 0.011684521567076445, Final Batch Loss: 0.0059695071540772915\n",
      "Epoch 3808, Loss: 0.007057409966364503, Final Batch Loss: 0.002900077262893319\n",
      "Epoch 3809, Loss: 0.028170909732580185, Final Batch Loss: 0.01340087503194809\n",
      "Epoch 3810, Loss: 0.02120905602350831, Final Batch Loss: 0.007587079424411058\n",
      "Epoch 3811, Loss: 0.017311161383986473, Final Batch Loss: 0.0030648522078990936\n",
      "Epoch 3812, Loss: 0.012595617212355137, Final Batch Loss: 0.00503080990165472\n",
      "Epoch 3813, Loss: 0.048586832359433174, Final Batch Loss: 0.03831225261092186\n",
      "Epoch 3814, Loss: 0.017075468320399523, Final Batch Loss: 0.004866836126893759\n",
      "Epoch 3815, Loss: 0.006208690931089222, Final Batch Loss: 0.004928732290863991\n",
      "Epoch 3816, Loss: 0.02293018437922001, Final Batch Loss: 0.007977722212672234\n",
      "Epoch 3817, Loss: 0.024040664546191692, Final Batch Loss: 0.01168628316372633\n",
      "Epoch 3818, Loss: 0.011159166926518083, Final Batch Loss: 0.007265298627316952\n",
      "Epoch 3819, Loss: 0.01320931687951088, Final Batch Loss: 0.006894988473504782\n",
      "Epoch 3820, Loss: 0.03742009587585926, Final Batch Loss: 0.011708509176969528\n",
      "Epoch 3821, Loss: 0.014209665823727846, Final Batch Loss: 0.00957033783197403\n",
      "Epoch 3822, Loss: 0.01025932957418263, Final Batch Loss: 0.0030211645644158125\n",
      "Epoch 3823, Loss: 0.019925187807530165, Final Batch Loss: 0.00436695059761405\n",
      "Epoch 3824, Loss: 0.006369157927110791, Final Batch Loss: 0.0039983526803553104\n",
      "Epoch 3825, Loss: 0.04820346459746361, Final Batch Loss: 0.021646816283464432\n",
      "Epoch 3826, Loss: 0.03934756061062217, Final Batch Loss: 0.03572608903050423\n",
      "Epoch 3827, Loss: 0.01764620002359152, Final Batch Loss: 0.00836998037993908\n",
      "Epoch 3828, Loss: 0.018868484534323215, Final Batch Loss: 0.0040419697761535645\n",
      "Epoch 3829, Loss: 0.03932463238015771, Final Batch Loss: 0.0037765861488878727\n",
      "Epoch 3830, Loss: 0.009642157470807433, Final Batch Loss: 0.007593892980366945\n",
      "Epoch 3831, Loss: 0.026097259484231472, Final Batch Loss: 0.015133945271372795\n",
      "Epoch 3832, Loss: 0.022086794953793287, Final Batch Loss: 0.019331809133291245\n",
      "Epoch 3833, Loss: 0.03869369812309742, Final Batch Loss: 0.01846485212445259\n",
      "Epoch 3834, Loss: 0.005857418058440089, Final Batch Loss: 0.002749023027718067\n",
      "Epoch 3835, Loss: 0.006148208398371935, Final Batch Loss: 0.0024675456807017326\n",
      "Epoch 3836, Loss: 0.008726336294785142, Final Batch Loss: 0.005708690732717514\n",
      "Epoch 3837, Loss: 0.03674576780758798, Final Batch Loss: 0.0034478341694921255\n",
      "Epoch 3838, Loss: 0.018829676788300276, Final Batch Loss: 0.006773788016289473\n",
      "Epoch 3839, Loss: 0.019216338638216257, Final Batch Loss: 0.0031943568028509617\n",
      "Epoch 3840, Loss: 0.019115953473374248, Final Batch Loss: 0.016854997724294662\n",
      "Epoch 3841, Loss: 0.05863251769915223, Final Batch Loss: 0.004686335567384958\n",
      "Epoch 3842, Loss: 0.0117815260309726, Final Batch Loss: 0.008331820368766785\n",
      "Epoch 3843, Loss: 0.019691459834575653, Final Batch Loss: 0.01649850606918335\n",
      "Epoch 3844, Loss: 0.006094607524573803, Final Batch Loss: 0.002866826020181179\n",
      "Epoch 3845, Loss: 0.01663192454725504, Final Batch Loss: 0.0037566330283880234\n",
      "Epoch 3846, Loss: 0.2459755316376686, Final Batch Loss: 0.1858026385307312\n",
      "Epoch 3847, Loss: 0.024652784690260887, Final Batch Loss: 0.009477443993091583\n",
      "Epoch 3848, Loss: 0.05648556537926197, Final Batch Loss: 0.028762606903910637\n",
      "Epoch 3849, Loss: 0.017881205305457115, Final Batch Loss: 0.002826274372637272\n",
      "Epoch 3850, Loss: 0.032925096806138754, Final Batch Loss: 0.02941930666565895\n",
      "Epoch 3851, Loss: 0.042063331231474876, Final Batch Loss: 0.020387792959809303\n",
      "Epoch 3852, Loss: 0.006454637972638011, Final Batch Loss: 0.002971500623971224\n",
      "Epoch 3853, Loss: 0.05849364027380943, Final Batch Loss: 0.01951071247458458\n",
      "Epoch 3854, Loss: 0.015092612244188786, Final Batch Loss: 0.005968257784843445\n",
      "Epoch 3855, Loss: 0.037115559447556734, Final Batch Loss: 0.029314696788787842\n",
      "Epoch 3856, Loss: 0.019431871362030506, Final Batch Loss: 0.00938186701387167\n",
      "Epoch 3857, Loss: 0.05833993526175618, Final Batch Loss: 0.007767525035887957\n",
      "Epoch 3858, Loss: 0.025569378398358822, Final Batch Loss: 0.004301571287214756\n",
      "Epoch 3859, Loss: 0.02452193619683385, Final Batch Loss: 0.0022452143020927906\n",
      "Epoch 3860, Loss: 0.017864231951534748, Final Batch Loss: 0.013993820175528526\n",
      "Epoch 3861, Loss: 0.01791964378207922, Final Batch Loss: 0.004331426694989204\n",
      "Epoch 3862, Loss: 0.019601905718445778, Final Batch Loss: 0.008541582152247429\n",
      "Epoch 3863, Loss: 0.07274516299366951, Final Batch Loss: 0.037220247089862823\n",
      "Epoch 3864, Loss: 0.028588927816599607, Final Batch Loss: 0.02297138422727585\n",
      "Epoch 3865, Loss: 0.015439440496265888, Final Batch Loss: 0.0067728012800216675\n",
      "Epoch 3866, Loss: 0.06296153925359249, Final Batch Loss: 0.04137749224901199\n",
      "Epoch 3867, Loss: 0.02645787689834833, Final Batch Loss: 0.013695300556719303\n",
      "Epoch 3868, Loss: 0.00921360356733203, Final Batch Loss: 0.005481963511556387\n",
      "Epoch 3869, Loss: 0.06297583132982254, Final Batch Loss: 0.03104526177048683\n",
      "Epoch 3870, Loss: 0.05545989237725735, Final Batch Loss: 0.013019831851124763\n",
      "Epoch 3871, Loss: 0.01047108555212617, Final Batch Loss: 0.0032985429279506207\n",
      "Epoch 3872, Loss: 0.030966044403612614, Final Batch Loss: 0.027788108214735985\n",
      "Epoch 3873, Loss: 0.025040929205715656, Final Batch Loss: 0.013943794183433056\n",
      "Epoch 3874, Loss: 0.07263514026999474, Final Batch Loss: 0.012583281844854355\n",
      "Epoch 3875, Loss: 0.02954639494419098, Final Batch Loss: 0.014326266944408417\n",
      "Epoch 3876, Loss: 0.022604200057685375, Final Batch Loss: 0.016277020797133446\n",
      "Epoch 3877, Loss: 0.027824108488857746, Final Batch Loss: 0.019251765683293343\n",
      "Epoch 3878, Loss: 0.02019348368048668, Final Batch Loss: 0.012196398340165615\n",
      "Epoch 3879, Loss: 0.04109457693994045, Final Batch Loss: 0.02056867443025112\n",
      "Epoch 3880, Loss: 0.05595944821834564, Final Batch Loss: 0.018700040876865387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3881, Loss: 0.025968283880501986, Final Batch Loss: 0.007508593145757914\n",
      "Epoch 3882, Loss: 0.04081816226243973, Final Batch Loss: 0.01950547844171524\n",
      "Epoch 3883, Loss: 0.02979082684032619, Final Batch Loss: 0.0020095694344490767\n",
      "Epoch 3884, Loss: 0.0383780871052295, Final Batch Loss: 0.035528961569070816\n",
      "Epoch 3885, Loss: 0.02221527206711471, Final Batch Loss: 0.003472544951364398\n",
      "Epoch 3886, Loss: 0.02605687582399696, Final Batch Loss: 0.0019096651813015342\n",
      "Epoch 3887, Loss: 0.030551073141396046, Final Batch Loss: 0.011859389953315258\n",
      "Epoch 3888, Loss: 0.006168270250782371, Final Batch Loss: 0.004698488861322403\n",
      "Epoch 3889, Loss: 0.010464439168572426, Final Batch Loss: 0.004665211774408817\n",
      "Epoch 3890, Loss: 0.026703133597038686, Final Batch Loss: 0.025403954088687897\n",
      "Epoch 3891, Loss: 0.009022877085953951, Final Batch Loss: 0.005805845372378826\n",
      "Epoch 3892, Loss: 0.06546973064541817, Final Batch Loss: 0.03010111302137375\n",
      "Epoch 3893, Loss: 0.027476133313030005, Final Batch Loss: 0.020766356959939003\n",
      "Epoch 3894, Loss: 0.04279557894915342, Final Batch Loss: 0.03466609865427017\n",
      "Epoch 3895, Loss: 0.02018861402757466, Final Batch Loss: 0.0176395233720541\n",
      "Epoch 3896, Loss: 0.008474561618641019, Final Batch Loss: 0.005188945680856705\n",
      "Epoch 3897, Loss: 0.010486240964382887, Final Batch Loss: 0.006850661244243383\n",
      "Epoch 3898, Loss: 0.015654170769266784, Final Batch Loss: 0.0017091705230996013\n",
      "Epoch 3899, Loss: 0.019120305310934782, Final Batch Loss: 0.014790809713304043\n",
      "Epoch 3900, Loss: 0.006385666783899069, Final Batch Loss: 0.004128458444029093\n",
      "Epoch 3901, Loss: 0.05805962206795812, Final Batch Loss: 0.004521798808127642\n",
      "Epoch 3902, Loss: 0.011206308612599969, Final Batch Loss: 0.009313776157796383\n",
      "Epoch 3903, Loss: 0.00931602343916893, Final Batch Loss: 0.005184412468224764\n",
      "Epoch 3904, Loss: 0.014831582549959421, Final Batch Loss: 0.0075052776373922825\n",
      "Epoch 3905, Loss: 0.007198439911007881, Final Batch Loss: 0.003869216423481703\n",
      "Epoch 3906, Loss: 0.009144392097368836, Final Batch Loss: 0.00637654447928071\n",
      "Epoch 3907, Loss: 0.030607846565544605, Final Batch Loss: 0.018236691132187843\n",
      "Epoch 3908, Loss: 0.052629651967436075, Final Batch Loss: 0.04773898795247078\n",
      "Epoch 3909, Loss: 0.006503641838207841, Final Batch Loss: 0.003623519791290164\n",
      "Epoch 3910, Loss: 0.01602453109808266, Final Batch Loss: 0.0020705258939415216\n",
      "Epoch 3911, Loss: 0.015682585770264268, Final Batch Loss: 0.011799981817603111\n",
      "Epoch 3912, Loss: 0.02472408814355731, Final Batch Loss: 0.018096348270773888\n",
      "Epoch 3913, Loss: 0.01697113085538149, Final Batch Loss: 0.00924256443977356\n",
      "Epoch 3914, Loss: 0.014753743540495634, Final Batch Loss: 0.004926633555442095\n",
      "Epoch 3915, Loss: 0.018503095023334026, Final Batch Loss: 0.0017536478117108345\n",
      "Epoch 3916, Loss: 0.009030641056597233, Final Batch Loss: 0.005049849394708872\n",
      "Epoch 3917, Loss: 0.004945273045450449, Final Batch Loss: 0.001764738466590643\n",
      "Epoch 3918, Loss: 0.010905447416007519, Final Batch Loss: 0.0027526356279850006\n",
      "Epoch 3919, Loss: 0.017348342575132847, Final Batch Loss: 0.008176544681191444\n",
      "Epoch 3920, Loss: 0.006163661601021886, Final Batch Loss: 0.0032215409446507692\n",
      "Epoch 3921, Loss: 0.010920303408056498, Final Batch Loss: 0.0032608965411782265\n",
      "Epoch 3922, Loss: 0.018965459894388914, Final Batch Loss: 0.014975634403526783\n",
      "Epoch 3923, Loss: 0.040882349479943514, Final Batch Loss: 0.03476996719837189\n",
      "Epoch 3924, Loss: 0.009393875079695135, Final Batch Loss: 0.0009725965210236609\n",
      "Epoch 3925, Loss: 0.01694567478261888, Final Batch Loss: 0.014118576422333717\n",
      "Epoch 3926, Loss: 0.007535335200373083, Final Batch Loss: 0.006708378437906504\n",
      "Epoch 3927, Loss: 0.058611735701560974, Final Batch Loss: 0.02039312943816185\n",
      "Epoch 3928, Loss: 0.005015447968617082, Final Batch Loss: 0.0020015195477753878\n",
      "Epoch 3929, Loss: 0.01604513358324766, Final Batch Loss: 0.0022224755957722664\n",
      "Epoch 3930, Loss: 0.023255681851878762, Final Batch Loss: 0.02024908736348152\n",
      "Epoch 3931, Loss: 0.016758662881329656, Final Batch Loss: 0.0031473927665501833\n",
      "Epoch 3932, Loss: 0.004412113688886166, Final Batch Loss: 0.002767085563391447\n",
      "Epoch 3933, Loss: 0.009721892944071442, Final Batch Loss: 0.0007389728561975062\n",
      "Epoch 3934, Loss: 0.06120183505117893, Final Batch Loss: 0.02199597842991352\n",
      "Epoch 3935, Loss: 0.015624506399035454, Final Batch Loss: 0.0027372660115361214\n",
      "Epoch 3936, Loss: 0.020071168895810843, Final Batch Loss: 0.017612921074032784\n",
      "Epoch 3937, Loss: 0.010349090676754713, Final Batch Loss: 0.006286473013460636\n",
      "Epoch 3938, Loss: 0.01834117923863232, Final Batch Loss: 0.002377129392698407\n",
      "Epoch 3939, Loss: 0.015569429378956556, Final Batch Loss: 0.003877062816172838\n",
      "Epoch 3940, Loss: 0.025623546913266182, Final Batch Loss: 0.013018663972616196\n",
      "Epoch 3941, Loss: 0.016905919881537557, Final Batch Loss: 0.013811903074383736\n",
      "Epoch 3942, Loss: 0.008905332535505295, Final Batch Loss: 0.004509092308580875\n",
      "Epoch 3943, Loss: 0.022955648135393858, Final Batch Loss: 0.0037561762146651745\n",
      "Epoch 3944, Loss: 0.03482582699507475, Final Batch Loss: 0.01374667789787054\n",
      "Epoch 3945, Loss: 0.03245952073484659, Final Batch Loss: 0.020670611411333084\n",
      "Epoch 3946, Loss: 0.02155676018446684, Final Batch Loss: 0.01685200445353985\n",
      "Epoch 3947, Loss: 0.004407183732837439, Final Batch Loss: 0.0017028264701366425\n",
      "Epoch 3948, Loss: 0.00816554226912558, Final Batch Loss: 0.002608184004202485\n",
      "Epoch 3949, Loss: 0.005605045240372419, Final Batch Loss: 0.0031387528870254755\n",
      "Epoch 3950, Loss: 0.011159936431795359, Final Batch Loss: 0.004487209487706423\n",
      "Epoch 3951, Loss: 0.013498834508936852, Final Batch Loss: 0.0007530853035859764\n",
      "Epoch 3952, Loss: 0.0332587631419301, Final Batch Loss: 0.004348614253103733\n",
      "Epoch 3953, Loss: 0.00619818433187902, Final Batch Loss: 0.0008277457673102617\n",
      "Epoch 3954, Loss: 0.00767751457169652, Final Batch Loss: 0.002854251768440008\n",
      "Epoch 3955, Loss: 0.020376801025122404, Final Batch Loss: 0.015469061210751534\n",
      "Epoch 3956, Loss: 0.025779014453291893, Final Batch Loss: 0.0025536194443702698\n",
      "Epoch 3957, Loss: 0.010588271776214242, Final Batch Loss: 0.0033788203727453947\n",
      "Epoch 3958, Loss: 0.009643872268497944, Final Batch Loss: 0.006338363513350487\n",
      "Epoch 3959, Loss: 0.013527930248528719, Final Batch Loss: 0.008528236299753189\n",
      "Epoch 3960, Loss: 0.0024401668342761695, Final Batch Loss: 0.0006911195232532918\n",
      "Epoch 3961, Loss: 0.020351550541818142, Final Batch Loss: 0.00395102147012949\n",
      "Epoch 3962, Loss: 0.00876657571643591, Final Batch Loss: 0.004136989824473858\n",
      "Epoch 3963, Loss: 0.00422871217597276, Final Batch Loss: 0.0015104362973943353\n",
      "Epoch 3964, Loss: 0.011348525993525982, Final Batch Loss: 0.0024627316743135452\n",
      "Epoch 3965, Loss: 0.004900115658529103, Final Batch Loss: 0.0032861530780792236\n",
      "Epoch 3966, Loss: 0.015008049318566918, Final Batch Loss: 0.011215714737772942\n",
      "Epoch 3967, Loss: 0.030547152273356915, Final Batch Loss: 0.0034010978415608406\n",
      "Epoch 3968, Loss: 0.02324153366498649, Final Batch Loss: 0.020020393654704094\n",
      "Epoch 3969, Loss: 0.007502792403101921, Final Batch Loss: 0.0041686356998980045\n",
      "Epoch 3970, Loss: 0.0639392901211977, Final Batch Loss: 0.01949319802224636\n",
      "Epoch 3971, Loss: 0.004433188354596496, Final Batch Loss: 0.001660466892644763\n",
      "Epoch 3972, Loss: 0.019515372812747955, Final Batch Loss: 0.0025211144238710403\n",
      "Epoch 3973, Loss: 0.014317813329398632, Final Batch Loss: 0.0013742214068770409\n",
      "Epoch 3974, Loss: 0.015068377833813429, Final Batch Loss: 0.0020373263396322727\n",
      "Epoch 3975, Loss: 0.1253575726877898, Final Batch Loss: 0.0020381256472319365\n",
      "Epoch 3976, Loss: 0.027828054269775748, Final Batch Loss: 0.025791797786951065\n",
      "Epoch 3977, Loss: 0.02745547820813954, Final Batch Loss: 0.0029949669260531664\n",
      "Epoch 3978, Loss: 0.04073446267284453, Final Batch Loss: 0.003296159440651536\n",
      "Epoch 3979, Loss: 0.020438971929252148, Final Batch Loss: 0.008698083460330963\n",
      "Epoch 3980, Loss: 0.021561272093094885, Final Batch Loss: 0.0013113763416185975\n",
      "Epoch 3981, Loss: 0.029196823947131634, Final Batch Loss: 0.021323686465620995\n",
      "Epoch 3982, Loss: 0.11382106691598892, Final Batch Loss: 0.02297890931367874\n",
      "Epoch 3983, Loss: 0.020010889042168856, Final Batch Loss: 0.0184037946164608\n",
      "Epoch 3984, Loss: 0.015225604642182589, Final Batch Loss: 0.003403823357075453\n",
      "Epoch 3985, Loss: 0.005169997806660831, Final Batch Loss: 0.0016835321439430118\n",
      "Epoch 3986, Loss: 0.022502432111650705, Final Batch Loss: 0.001920855138450861\n",
      "Epoch 3987, Loss: 0.014156205113977194, Final Batch Loss: 0.003622040618211031\n",
      "Epoch 3988, Loss: 0.023884156718850136, Final Batch Loss: 0.01062176562845707\n",
      "Epoch 3989, Loss: 0.025274641811847687, Final Batch Loss: 0.02100982330739498\n",
      "Epoch 3990, Loss: 0.05302546499297023, Final Batch Loss: 0.05076991394162178\n",
      "Epoch 3991, Loss: 0.03778722649440169, Final Batch Loss: 0.03603311628103256\n",
      "Epoch 3992, Loss: 0.020682761445641518, Final Batch Loss: 0.012560688890516758\n",
      "Epoch 3993, Loss: 0.06576343718916178, Final Batch Loss: 0.012725059874355793\n",
      "Epoch 3994, Loss: 0.033798038959503174, Final Batch Loss: 0.01743745617568493\n",
      "Epoch 3995, Loss: 0.019904127344489098, Final Batch Loss: 0.008239638060331345\n",
      "Epoch 3996, Loss: 0.024082822259515524, Final Batch Loss: 0.018646037206053734\n",
      "Epoch 3997, Loss: 0.016271871980279684, Final Batch Loss: 0.006087382789701223\n",
      "Epoch 3998, Loss: 0.01216479018330574, Final Batch Loss: 0.007783297915011644\n",
      "Epoch 3999, Loss: 0.04956942843273282, Final Batch Loss: 0.04523830488324165\n",
      "Epoch 4000, Loss: 0.048128293827176094, Final Batch Loss: 0.031264517456293106\n",
      "Epoch 4001, Loss: 0.04302237695083022, Final Batch Loss: 0.03823430836200714\n",
      "Epoch 4002, Loss: 0.048115107230842113, Final Batch Loss: 0.011002964340150356\n",
      "Epoch 4003, Loss: 0.03452528268098831, Final Batch Loss: 0.023947695270180702\n",
      "Epoch 4004, Loss: 0.015984342200681567, Final Batch Loss: 0.012614650651812553\n",
      "Epoch 4005, Loss: 0.016709305346012115, Final Batch Loss: 0.00875631533563137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4006, Loss: 0.0820519020780921, Final Batch Loss: 0.0722513496875763\n",
      "Epoch 4007, Loss: 0.008312314748764038, Final Batch Loss: 0.0065252771601080894\n",
      "Epoch 4008, Loss: 0.0036115418188273907, Final Batch Loss: 0.0019248792668804526\n",
      "Epoch 4009, Loss: 0.019582592882215977, Final Batch Loss: 0.011119537986814976\n",
      "Epoch 4010, Loss: 0.025164364837110043, Final Batch Loss: 0.009375362657010555\n",
      "Epoch 4011, Loss: 0.031207330524921417, Final Batch Loss: 0.0066075511276721954\n",
      "Epoch 4012, Loss: 0.01823435351252556, Final Batch Loss: 0.013668002560734749\n",
      "Epoch 4013, Loss: 0.011225837282836437, Final Batch Loss: 0.004581815097481012\n",
      "Epoch 4014, Loss: 0.038163254503160715, Final Batch Loss: 0.006955245975404978\n",
      "Epoch 4015, Loss: 0.01749144261702895, Final Batch Loss: 0.002674445975571871\n",
      "Epoch 4016, Loss: 0.019879461731761694, Final Batch Loss: 0.01340652909129858\n",
      "Epoch 4017, Loss: 0.030103588476777077, Final Batch Loss: 0.016550881788134575\n",
      "Epoch 4018, Loss: 0.02449904754757881, Final Batch Loss: 0.013882184401154518\n",
      "Epoch 4019, Loss: 0.04080371651798487, Final Batch Loss: 0.012302997522056103\n",
      "Epoch 4020, Loss: 0.025517931208014488, Final Batch Loss: 0.0058660563081502914\n",
      "Epoch 4021, Loss: 0.06472589261829853, Final Batch Loss: 0.03397788852453232\n",
      "Epoch 4022, Loss: 0.03756442619487643, Final Batch Loss: 0.004944283049553633\n",
      "Epoch 4023, Loss: 0.023392552509903908, Final Batch Loss: 0.0182508397847414\n",
      "Epoch 4024, Loss: 0.029761867597699165, Final Batch Loss: 0.025489522144198418\n",
      "Epoch 4025, Loss: 0.009714122163131833, Final Batch Loss: 0.006463371217250824\n",
      "Epoch 4026, Loss: 0.029537146911025047, Final Batch Loss: 0.011745734140276909\n",
      "Epoch 4027, Loss: 0.03205501846969128, Final Batch Loss: 0.0022870227694511414\n",
      "Epoch 4028, Loss: 0.04855806194245815, Final Batch Loss: 0.022364776581525803\n",
      "Epoch 4029, Loss: 0.005335210124030709, Final Batch Loss: 0.0025649280287325382\n",
      "Epoch 4030, Loss: 0.029893212020397186, Final Batch Loss: 0.002314966171979904\n",
      "Epoch 4031, Loss: 0.024883861653506756, Final Batch Loss: 0.014662985689938068\n",
      "Epoch 4032, Loss: 0.028549891081638634, Final Batch Loss: 0.001246133935637772\n",
      "Epoch 4033, Loss: 0.008999057579785585, Final Batch Loss: 0.004371478222310543\n",
      "Epoch 4034, Loss: 0.027600117959082127, Final Batch Loss: 0.011753835715353489\n",
      "Epoch 4035, Loss: 0.008940691128373146, Final Batch Loss: 0.004596454091370106\n",
      "Epoch 4036, Loss: 0.09673843905329704, Final Batch Loss: 0.03506464138627052\n",
      "Epoch 4037, Loss: 0.053721577394753695, Final Batch Loss: 0.051477137953042984\n",
      "Epoch 4038, Loss: 0.019070581533014774, Final Batch Loss: 0.008075612597167492\n",
      "Epoch 4039, Loss: 0.023117676842957735, Final Batch Loss: 0.01697184145450592\n",
      "Epoch 4040, Loss: 0.04801821429282427, Final Batch Loss: 0.01333190593868494\n",
      "Epoch 4041, Loss: 0.024407858727499843, Final Batch Loss: 0.0025374225806444883\n",
      "Epoch 4042, Loss: 0.01748288795351982, Final Batch Loss: 0.008250126615166664\n",
      "Epoch 4043, Loss: 0.007814074400812387, Final Batch Loss: 0.002540836576372385\n",
      "Epoch 4044, Loss: 0.04574583936482668, Final Batch Loss: 0.009870958514511585\n",
      "Epoch 4045, Loss: 0.010618521366268396, Final Batch Loss: 0.0064865099266171455\n",
      "Epoch 4046, Loss: 0.030110868625342846, Final Batch Loss: 0.023356879130005836\n",
      "Epoch 4047, Loss: 0.005768596660345793, Final Batch Loss: 0.002150467364117503\n",
      "Epoch 4048, Loss: 0.008742181584239006, Final Batch Loss: 0.0040489980019629\n",
      "Epoch 4049, Loss: 0.02951581636443734, Final Batch Loss: 0.0023082573898136616\n",
      "Epoch 4050, Loss: 0.010414350312203169, Final Batch Loss: 0.004504370037466288\n",
      "Epoch 4051, Loss: 0.04286816343665123, Final Batch Loss: 0.029406828805804253\n",
      "Epoch 4052, Loss: 0.015102993231266737, Final Batch Loss: 0.005556759890168905\n",
      "Epoch 4053, Loss: 0.04267198406159878, Final Batch Loss: 0.0208723321557045\n",
      "Epoch 4054, Loss: 0.03668322484008968, Final Batch Loss: 0.002763190073892474\n",
      "Epoch 4055, Loss: 0.01735732122324407, Final Batch Loss: 0.0143126854673028\n",
      "Epoch 4056, Loss: 0.015295353485271335, Final Batch Loss: 0.011564096435904503\n",
      "Epoch 4057, Loss: 0.015242165653035045, Final Batch Loss: 0.012910080142319202\n",
      "Epoch 4058, Loss: 0.036211047787219286, Final Batch Loss: 0.007785507012158632\n",
      "Epoch 4059, Loss: 0.016792974434792995, Final Batch Loss: 0.002946188673377037\n",
      "Epoch 4060, Loss: 0.03316169511526823, Final Batch Loss: 0.015468073077499866\n",
      "Epoch 4061, Loss: 0.021139345015399158, Final Batch Loss: 0.0018874042434617877\n",
      "Epoch 4062, Loss: 0.012448116904124618, Final Batch Loss: 0.0034053928684443235\n",
      "Epoch 4063, Loss: 0.008751435554586351, Final Batch Loss: 0.0014121461426839232\n",
      "Epoch 4064, Loss: 0.005198472645133734, Final Batch Loss: 0.002745238598436117\n",
      "Epoch 4065, Loss: 0.012103333603590727, Final Batch Loss: 0.005958269815891981\n",
      "Epoch 4066, Loss: 0.006726789986714721, Final Batch Loss: 0.002106999745592475\n",
      "Epoch 4067, Loss: 0.02296072943136096, Final Batch Loss: 0.004185819532722235\n",
      "Epoch 4068, Loss: 0.004544348921626806, Final Batch Loss: 0.0020925579592585564\n",
      "Epoch 4069, Loss: 0.010294820182025433, Final Batch Loss: 0.005518725607544184\n",
      "Epoch 4070, Loss: 0.012632073601707816, Final Batch Loss: 0.010100362822413445\n",
      "Epoch 4071, Loss: 0.04794531501829624, Final Batch Loss: 0.021232113242149353\n",
      "Epoch 4072, Loss: 0.005218817386776209, Final Batch Loss: 0.0028318888507783413\n",
      "Epoch 4073, Loss: 0.00903399265371263, Final Batch Loss: 0.0030321560334414244\n",
      "Epoch 4074, Loss: 0.01599802658893168, Final Batch Loss: 0.003637876594439149\n",
      "Epoch 4075, Loss: 0.03254030738025904, Final Batch Loss: 0.01455665659159422\n",
      "Epoch 4076, Loss: 0.024284442188218236, Final Batch Loss: 0.02118740789592266\n",
      "Epoch 4077, Loss: 0.035399447195231915, Final Batch Loss: 0.005837361328303814\n",
      "Epoch 4078, Loss: 0.04021639283746481, Final Batch Loss: 0.03751155361533165\n",
      "Epoch 4079, Loss: 0.005750925512984395, Final Batch Loss: 0.0029354316648095846\n",
      "Epoch 4080, Loss: 0.017266797134652734, Final Batch Loss: 0.013494962826371193\n",
      "Epoch 4081, Loss: 0.006273420760408044, Final Batch Loss: 0.002283278154209256\n",
      "Epoch 4082, Loss: 0.026033164467662573, Final Batch Loss: 0.003921761643141508\n",
      "Epoch 4083, Loss: 0.020816356176510453, Final Batch Loss: 0.017573975026607513\n",
      "Epoch 4084, Loss: 0.014880529837682843, Final Batch Loss: 0.011560497805476189\n",
      "Epoch 4085, Loss: 0.0403865659609437, Final Batch Loss: 0.0013559618964791298\n",
      "Epoch 4086, Loss: 0.03121577901765704, Final Batch Loss: 0.00513206934556365\n",
      "Epoch 4087, Loss: 0.023635495454072952, Final Batch Loss: 0.01737745851278305\n",
      "Epoch 4088, Loss: 0.10968335345387459, Final Batch Loss: 0.024407070130109787\n",
      "Epoch 4089, Loss: 0.08211143687367439, Final Batch Loss: 0.043720364570617676\n",
      "Epoch 4090, Loss: 0.13512898609042168, Final Batch Loss: 0.004122551530599594\n",
      "Epoch 4091, Loss: 0.004113447503186762, Final Batch Loss: 0.001932831364683807\n",
      "Epoch 4092, Loss: 0.006956996861845255, Final Batch Loss: 0.0028735417872667313\n",
      "Epoch 4093, Loss: 0.10455838218331337, Final Batch Loss: 0.04115920886397362\n",
      "Epoch 4094, Loss: 0.055748989805579185, Final Batch Loss: 0.013340631499886513\n",
      "Epoch 4095, Loss: 0.08722442295402288, Final Batch Loss: 0.07191023975610733\n",
      "Epoch 4096, Loss: 0.041615650057792664, Final Batch Loss: 0.014137739315629005\n",
      "Epoch 4097, Loss: 0.02287645125761628, Final Batch Loss: 0.018138613551855087\n",
      "Epoch 4098, Loss: 0.023744021076709032, Final Batch Loss: 0.018949659541249275\n",
      "Epoch 4099, Loss: 0.03140984941273928, Final Batch Loss: 0.011598183773458004\n",
      "Epoch 4100, Loss: 0.05873429123312235, Final Batch Loss: 0.049436844885349274\n",
      "Epoch 4101, Loss: 0.046668226132169366, Final Batch Loss: 0.003573986003175378\n",
      "Epoch 4102, Loss: 0.007745755836367607, Final Batch Loss: 0.0030533610843122005\n",
      "Epoch 4103, Loss: 0.019797992892563343, Final Batch Loss: 0.00683987233787775\n",
      "Epoch 4104, Loss: 0.042988541536033154, Final Batch Loss: 0.03133454918861389\n",
      "Epoch 4105, Loss: 0.06460123881697655, Final Batch Loss: 0.03603395074605942\n",
      "Epoch 4106, Loss: 0.1593257039785385, Final Batch Loss: 0.09321863949298859\n",
      "Epoch 4107, Loss: 0.0772973969578743, Final Batch Loss: 0.03129158169031143\n",
      "Epoch 4108, Loss: 0.056195635348558426, Final Batch Loss: 0.006050821393728256\n",
      "Epoch 4109, Loss: 0.043909266125410795, Final Batch Loss: 0.0059487405233085155\n",
      "Epoch 4110, Loss: 0.032538809813559055, Final Batch Loss: 0.012411090545356274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4111, Loss: 0.047387074679136276, Final Batch Loss: 0.004270672798156738\n",
      "Epoch 4112, Loss: 0.04326278064399958, Final Batch Loss: 0.012799321673810482\n",
      "Epoch 4113, Loss: 0.09238940477371216, Final Batch Loss: 0.07057269662618637\n",
      "Epoch 4114, Loss: 0.035752649419009686, Final Batch Loss: 0.004136870615184307\n",
      "Epoch 4115, Loss: 0.02074090950191021, Final Batch Loss: 0.014845467172563076\n",
      "Epoch 4116, Loss: 0.09311490878462791, Final Batch Loss: 0.00965704396367073\n",
      "Epoch 4117, Loss: 0.09979207068681717, Final Batch Loss: 0.06332128494977951\n",
      "Epoch 4118, Loss: 0.010883513372391462, Final Batch Loss: 0.0062833428382873535\n",
      "Epoch 4119, Loss: 0.03546311520040035, Final Batch Loss: 0.01900205947458744\n",
      "Epoch 4120, Loss: 0.0892288088798523, Final Batch Loss: 0.053599122911691666\n",
      "Epoch 4121, Loss: 0.020940319169312716, Final Batch Loss: 0.007474518846720457\n",
      "Epoch 4122, Loss: 0.06633383873850107, Final Batch Loss: 0.05232800543308258\n",
      "Epoch 4123, Loss: 0.10179708153009415, Final Batch Loss: 0.07959961891174316\n",
      "Epoch 4124, Loss: 0.043809193186461926, Final Batch Loss: 0.038383565843105316\n",
      "Epoch 4125, Loss: 0.16218192130327225, Final Batch Loss: 0.09183643758296967\n",
      "Epoch 4126, Loss: 0.0551427211612463, Final Batch Loss: 0.009909460321068764\n",
      "Epoch 4127, Loss: 0.06471079215407372, Final Batch Loss: 0.03235001862049103\n",
      "Epoch 4128, Loss: 0.11388969980180264, Final Batch Loss: 0.09749717265367508\n",
      "Epoch 4129, Loss: 0.02890555839985609, Final Batch Loss: 0.01484416052699089\n",
      "Epoch 4130, Loss: 0.06537017971277237, Final Batch Loss: 0.01900184154510498\n",
      "Epoch 4131, Loss: 0.07036369107663631, Final Batch Loss: 0.04174497723579407\n",
      "Epoch 4132, Loss: 0.025758862495422363, Final Batch Loss: 0.014511225745081902\n",
      "Epoch 4133, Loss: 0.045294348150491714, Final Batch Loss: 0.028072839602828026\n",
      "Epoch 4134, Loss: 0.04533587209880352, Final Batch Loss: 0.024594523012638092\n",
      "Epoch 4135, Loss: 0.010890509700402617, Final Batch Loss: 0.0037167922127991915\n",
      "Epoch 4136, Loss: 0.055653102695941925, Final Batch Loss: 0.03858437016606331\n",
      "Epoch 4137, Loss: 0.021256680600345135, Final Batch Loss: 0.012589245103299618\n",
      "Epoch 4138, Loss: 0.03913024114444852, Final Batch Loss: 0.006358020473271608\n",
      "Epoch 4139, Loss: 0.022266517393290997, Final Batch Loss: 0.014069848693907261\n",
      "Epoch 4140, Loss: 0.017765604192391038, Final Batch Loss: 0.003749883035197854\n",
      "Epoch 4141, Loss: 0.02955210441723466, Final Batch Loss: 0.0034546353854238987\n",
      "Epoch 4142, Loss: 0.010153281036764383, Final Batch Loss: 0.005105107091367245\n",
      "Epoch 4143, Loss: 0.0164550868794322, Final Batch Loss: 0.005944037809967995\n",
      "Epoch 4144, Loss: 0.04506426211446524, Final Batch Loss: 0.035010505467653275\n",
      "Epoch 4145, Loss: 0.01257961755618453, Final Batch Loss: 0.00711826654151082\n",
      "Epoch 4146, Loss: 0.01689708954654634, Final Batch Loss: 0.003191157942637801\n",
      "Epoch 4147, Loss: 0.013639611657708883, Final Batch Loss: 0.008504487574100494\n",
      "Epoch 4148, Loss: 0.013296893564984202, Final Batch Loss: 0.009607259184122086\n",
      "Epoch 4149, Loss: 0.022146077826619148, Final Batch Loss: 0.014291289262473583\n",
      "Epoch 4150, Loss: 0.0064708085265010595, Final Batch Loss: 0.002694273367524147\n",
      "Epoch 4151, Loss: 0.011922983918339014, Final Batch Loss: 0.007822775281965733\n",
      "Epoch 4152, Loss: 0.019589039031416178, Final Batch Loss: 0.017408736050128937\n",
      "Epoch 4153, Loss: 0.007965677184984088, Final Batch Loss: 0.0027467485051602125\n",
      "Epoch 4154, Loss: 0.00832794071175158, Final Batch Loss: 0.002486748853698373\n",
      "Epoch 4155, Loss: 0.016676653642207384, Final Batch Loss: 0.00265170494094491\n",
      "Epoch 4156, Loss: 0.02034720464143902, Final Batch Loss: 0.0015711082378402352\n",
      "Epoch 4157, Loss: 0.008819281123578548, Final Batch Loss: 0.0015330202877521515\n",
      "Epoch 4158, Loss: 0.01853387476876378, Final Batch Loss: 0.014594830572605133\n",
      "Epoch 4159, Loss: 0.009315254166722298, Final Batch Loss: 0.004387497436255217\n",
      "Epoch 4160, Loss: 0.012527965707704425, Final Batch Loss: 0.003425771137699485\n",
      "Epoch 4161, Loss: 0.01660319697111845, Final Batch Loss: 0.0061523569747805595\n",
      "Epoch 4162, Loss: 0.0051926125306636095, Final Batch Loss: 0.002601437969133258\n",
      "Epoch 4163, Loss: 0.03833595663309097, Final Batch Loss: 0.015591830015182495\n",
      "Epoch 4164, Loss: 0.06206292356364429, Final Batch Loss: 0.06007568538188934\n",
      "Epoch 4165, Loss: 0.012586256954818964, Final Batch Loss: 0.006568034179508686\n",
      "Epoch 4166, Loss: 0.023542471695691347, Final Batch Loss: 0.005570840556174517\n",
      "Epoch 4167, Loss: 0.05748028447851539, Final Batch Loss: 0.051955584436655045\n",
      "Epoch 4168, Loss: 0.02031621802598238, Final Batch Loss: 0.01380050927400589\n",
      "Epoch 4169, Loss: 0.01004825159907341, Final Batch Loss: 0.0049651190638542175\n",
      "Epoch 4170, Loss: 0.03079595835879445, Final Batch Loss: 0.004913576412945986\n",
      "Epoch 4171, Loss: 0.031178345903754234, Final Batch Loss: 0.01833479106426239\n",
      "Epoch 4172, Loss: 0.012144473847001791, Final Batch Loss: 0.006541025824844837\n",
      "Epoch 4173, Loss: 0.011180098168551922, Final Batch Loss: 0.004536672029644251\n",
      "Epoch 4174, Loss: 0.016114985570311546, Final Batch Loss: 0.013452474027872086\n",
      "Epoch 4175, Loss: 0.006357674021273851, Final Batch Loss: 0.002047207672148943\n",
      "Epoch 4176, Loss: 0.015270860865712166, Final Batch Loss: 0.004022255539894104\n",
      "Epoch 4177, Loss: 0.022983625531196594, Final Batch Loss: 0.012223699130117893\n",
      "Epoch 4178, Loss: 0.043350393418222666, Final Batch Loss: 0.03907753899693489\n",
      "Epoch 4179, Loss: 0.04648495465517044, Final Batch Loss: 0.012195110321044922\n",
      "Epoch 4180, Loss: 0.030340401455760002, Final Batch Loss: 0.013972491025924683\n",
      "Epoch 4181, Loss: 0.01761942310258746, Final Batch Loss: 0.014708396978676319\n",
      "Epoch 4182, Loss: 0.02892073057591915, Final Batch Loss: 0.016189182177186012\n",
      "Epoch 4183, Loss: 0.00998409849125892, Final Batch Loss: 0.0016252315836027265\n",
      "Epoch 4184, Loss: 0.014842066913843155, Final Batch Loss: 0.006334537640213966\n",
      "Epoch 4185, Loss: 0.01811875030398369, Final Batch Loss: 0.006480647251009941\n",
      "Epoch 4186, Loss: 0.013893308117985725, Final Batch Loss: 0.0028720013797283173\n",
      "Epoch 4187, Loss: 0.0066357499454170465, Final Batch Loss: 0.0028880920726805925\n",
      "Epoch 4188, Loss: 0.016492582857608795, Final Batch Loss: 0.004505828954279423\n",
      "Epoch 4189, Loss: 0.015012513380497694, Final Batch Loss: 0.007380873430520296\n",
      "Epoch 4190, Loss: 0.010887681506574154, Final Batch Loss: 0.004192453343421221\n",
      "Epoch 4191, Loss: 0.010490364395081997, Final Batch Loss: 0.006553693674504757\n",
      "Epoch 4192, Loss: 0.011616992298513651, Final Batch Loss: 0.006160827353596687\n",
      "Epoch 4193, Loss: 0.01841276348568499, Final Batch Loss: 0.002248311648145318\n",
      "Epoch 4194, Loss: 0.01285605039447546, Final Batch Loss: 0.002851300872862339\n",
      "Epoch 4195, Loss: 0.021900635911151767, Final Batch Loss: 0.003639592556282878\n",
      "Epoch 4196, Loss: 0.009666303754784167, Final Batch Loss: 0.0018153224373236299\n",
      "Epoch 4197, Loss: 0.012227068655192852, Final Batch Loss: 0.006562128197401762\n",
      "Epoch 4198, Loss: 0.014449796872213483, Final Batch Loss: 0.0023117728997021914\n",
      "Epoch 4199, Loss: 0.009821896441280842, Final Batch Loss: 0.0028120814822614193\n",
      "Epoch 4200, Loss: 0.025451928842812777, Final Batch Loss: 0.00408259266987443\n",
      "Epoch 4201, Loss: 0.009585395106114447, Final Batch Loss: 0.0016829647356644273\n",
      "Epoch 4202, Loss: 0.032738763839006424, Final Batch Loss: 0.017160706222057343\n",
      "Epoch 4203, Loss: 0.009861703030765057, Final Batch Loss: 0.004246633965522051\n",
      "Epoch 4204, Loss: 0.00580985308624804, Final Batch Loss: 0.004088422283530235\n",
      "Epoch 4205, Loss: 0.03212366160005331, Final Batch Loss: 0.002585015259683132\n",
      "Epoch 4206, Loss: 0.024789893999695778, Final Batch Loss: 0.013881932944059372\n",
      "Epoch 4207, Loss: 0.023986055050045252, Final Batch Loss: 0.01940251886844635\n",
      "Epoch 4208, Loss: 0.04646923276595771, Final Batch Loss: 0.0013875465374439955\n",
      "Epoch 4209, Loss: 0.004661303013563156, Final Batch Loss: 0.0025409164372831583\n",
      "Epoch 4210, Loss: 0.005242286832071841, Final Batch Loss: 0.0035095219500362873\n",
      "Epoch 4211, Loss: 0.017067395616322756, Final Batch Loss: 0.0035736938007175922\n",
      "Epoch 4212, Loss: 0.02465610671788454, Final Batch Loss: 0.016666708514094353\n",
      "Epoch 4213, Loss: 0.004825395648367703, Final Batch Loss: 0.0031515229493379593\n",
      "Epoch 4214, Loss: 0.006701877689920366, Final Batch Loss: 0.0015147844096645713\n",
      "Epoch 4215, Loss: 0.011299194768071175, Final Batch Loss: 0.00894059706479311\n",
      "Epoch 4216, Loss: 0.016986885108053684, Final Batch Loss: 0.012959124520421028\n",
      "Epoch 4217, Loss: 0.07018590439110994, Final Batch Loss: 0.05731513723731041\n",
      "Epoch 4218, Loss: 0.010174295864999294, Final Batch Loss: 0.005515512079000473\n",
      "Epoch 4219, Loss: 0.009686452569440007, Final Batch Loss: 0.0031729477923363447\n",
      "Epoch 4220, Loss: 0.00808498845435679, Final Batch Loss: 0.006036901846528053\n",
      "Epoch 4221, Loss: 0.004874344449490309, Final Batch Loss: 0.002563932677730918\n",
      "Epoch 4222, Loss: 0.025912864599376917, Final Batch Loss: 0.005044663790613413\n",
      "Epoch 4223, Loss: 0.036564416252076626, Final Batch Loss: 0.028037751093506813\n",
      "Epoch 4224, Loss: 0.05106595437973738, Final Batch Loss: 0.004199362359941006\n",
      "Epoch 4225, Loss: 0.02580066490918398, Final Batch Loss: 0.01952064037322998\n",
      "Epoch 4226, Loss: 0.010094091296195984, Final Batch Loss: 0.004214484244585037\n",
      "Epoch 4227, Loss: 0.02625069161877036, Final Batch Loss: 0.005695037078112364\n",
      "Epoch 4228, Loss: 0.013484681490808725, Final Batch Loss: 0.004047051537781954\n",
      "Epoch 4229, Loss: 0.012463209684938192, Final Batch Loss: 0.0021177963353693485\n",
      "Epoch 4230, Loss: 0.014654156286269426, Final Batch Loss: 0.003164821770042181\n",
      "Epoch 4231, Loss: 0.03437550086528063, Final Batch Loss: 0.02071540616452694\n",
      "Epoch 4232, Loss: 0.008753989124670625, Final Batch Loss: 0.006348774302750826\n",
      "Epoch 4233, Loss: 0.012274544918909669, Final Batch Loss: 0.009297232143580914\n",
      "Epoch 4234, Loss: 0.026134485029615462, Final Batch Loss: 0.0011631936067715287\n",
      "Epoch 4235, Loss: 0.04235381027683616, Final Batch Loss: 0.004498699214309454\n",
      "Epoch 4236, Loss: 0.052665723487734795, Final Batch Loss: 0.009687723591923714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4237, Loss: 0.005684050498530269, Final Batch Loss: 0.002962717553600669\n",
      "Epoch 4238, Loss: 0.004490130813792348, Final Batch Loss: 0.0023178844712674618\n",
      "Epoch 4239, Loss: 0.0095315664075315, Final Batch Loss: 0.005894572474062443\n",
      "Epoch 4240, Loss: 0.01261792704463005, Final Batch Loss: 0.007954821921885014\n",
      "Epoch 4241, Loss: 0.004233010811731219, Final Batch Loss: 0.0029913189355283976\n",
      "Epoch 4242, Loss: 0.04372890992090106, Final Batch Loss: 0.0009840275160968304\n",
      "Epoch 4243, Loss: 0.004620245541445911, Final Batch Loss: 0.0017594854580238461\n",
      "Epoch 4244, Loss: 0.013783097267150879, Final Batch Loss: 0.006675019860267639\n",
      "Epoch 4245, Loss: 0.0032043124083429575, Final Batch Loss: 0.0011546900495886803\n",
      "Epoch 4246, Loss: 0.008230566745623946, Final Batch Loss: 0.002333664568141103\n",
      "Epoch 4247, Loss: 0.005246261483989656, Final Batch Loss: 0.0014352191938087344\n",
      "Epoch 4248, Loss: 0.004317749757319689, Final Batch Loss: 0.001721804030239582\n",
      "Epoch 4249, Loss: 0.008352794451639056, Final Batch Loss: 0.002053581876680255\n",
      "Epoch 4250, Loss: 0.009231219883076847, Final Batch Loss: 0.0015092323301360011\n",
      "Epoch 4251, Loss: 0.016105645801872015, Final Batch Loss: 0.0047502960078418255\n",
      "Epoch 4252, Loss: 0.004008851014077663, Final Batch Loss: 0.0017561707645654678\n",
      "Epoch 4253, Loss: 0.019833411555737257, Final Batch Loss: 0.016428159549832344\n",
      "Epoch 4254, Loss: 0.012345403898507357, Final Batch Loss: 0.0026188907213509083\n",
      "Epoch 4255, Loss: 0.047040699515491724, Final Batch Loss: 0.0032820203341543674\n",
      "Epoch 4256, Loss: 0.12427164427936077, Final Batch Loss: 0.016900738701224327\n",
      "Epoch 4257, Loss: 0.0385582409799099, Final Batch Loss: 0.019695870578289032\n",
      "Epoch 4258, Loss: 0.03451452869921923, Final Batch Loss: 0.021328449249267578\n",
      "Epoch 4259, Loss: 0.03509812615811825, Final Batch Loss: 0.011005323380231857\n",
      "Epoch 4260, Loss: 0.0427194160874933, Final Batch Loss: 0.03952633962035179\n",
      "Epoch 4261, Loss: 0.01724133756943047, Final Batch Loss: 0.0036161208990961313\n",
      "Epoch 4262, Loss: 0.018959388602524996, Final Batch Loss: 0.012543652206659317\n",
      "Epoch 4263, Loss: 0.07022625161334872, Final Batch Loss: 0.005927644204348326\n",
      "Epoch 4264, Loss: 0.048075743950903416, Final Batch Loss: 0.043403081595897675\n",
      "Epoch 4265, Loss: 0.030973159708082676, Final Batch Loss: 0.010680028237402439\n",
      "Epoch 4266, Loss: 0.06460232101380825, Final Batch Loss: 0.02780965156853199\n",
      "Epoch 4267, Loss: 0.019279838306829333, Final Batch Loss: 0.016276545822620392\n",
      "Epoch 4268, Loss: 0.02779422700405121, Final Batch Loss: 0.010434683412313461\n",
      "Epoch 4269, Loss: 0.021832636324688792, Final Batch Loss: 0.0029342162888497114\n",
      "Epoch 4270, Loss: 0.010365826077759266, Final Batch Loss: 0.006996135227382183\n",
      "Epoch 4271, Loss: 0.02436485094949603, Final Batch Loss: 0.016903696581721306\n",
      "Epoch 4272, Loss: 0.020928585436195135, Final Batch Loss: 0.0024918965063989162\n",
      "Epoch 4273, Loss: 0.007748711388558149, Final Batch Loss: 0.00557304173707962\n",
      "Epoch 4274, Loss: 0.014149342896416783, Final Batch Loss: 0.0028609943110495806\n",
      "Epoch 4275, Loss: 0.012844303040765226, Final Batch Loss: 0.001552413566969335\n",
      "Epoch 4276, Loss: 0.013636595103889704, Final Batch Loss: 0.002380580175668001\n",
      "Epoch 4277, Loss: 0.006387685425579548, Final Batch Loss: 0.0017894171178340912\n",
      "Epoch 4278, Loss: 0.04279705509543419, Final Batch Loss: 0.027064772322773933\n",
      "Epoch 4279, Loss: 0.0056004689540714025, Final Batch Loss: 0.0015369977336376905\n",
      "Epoch 4280, Loss: 0.013263943488709629, Final Batch Loss: 0.0015431576175615191\n",
      "Epoch 4281, Loss: 0.019161825999617577, Final Batch Loss: 0.011155473999679089\n",
      "Epoch 4282, Loss: 0.01129108713939786, Final Batch Loss: 0.007742970250546932\n",
      "Epoch 4283, Loss: 0.02851400466170162, Final Batch Loss: 0.001488007022999227\n",
      "Epoch 4284, Loss: 0.03537075314670801, Final Batch Loss: 0.008169068954885006\n",
      "Epoch 4285, Loss: 0.06037211697548628, Final Batch Loss: 0.04637858271598816\n",
      "Epoch 4286, Loss: 0.07840513065457344, Final Batch Loss: 0.033111073076725006\n",
      "Epoch 4287, Loss: 0.12284744973294437, Final Batch Loss: 0.0026599138509482145\n",
      "Epoch 4288, Loss: 0.06716886954382062, Final Batch Loss: 0.06195149943232536\n",
      "Epoch 4289, Loss: 0.06017831526696682, Final Batch Loss: 0.023616952821612358\n",
      "Epoch 4290, Loss: 0.028869980480521917, Final Batch Loss: 0.004647772293537855\n",
      "Epoch 4291, Loss: 0.00889998348429799, Final Batch Loss: 0.004332621581852436\n",
      "Epoch 4292, Loss: 0.06303265877068043, Final Batch Loss: 0.046657562255859375\n",
      "Epoch 4293, Loss: 0.03301365417428315, Final Batch Loss: 0.0022341471631079912\n",
      "Epoch 4294, Loss: 0.020934816915541887, Final Batch Loss: 0.005049968603998423\n",
      "Epoch 4295, Loss: 0.07306953147053719, Final Batch Loss: 0.03593970090150833\n",
      "Epoch 4296, Loss: 0.019655881449580193, Final Batch Loss: 0.011539955623447895\n",
      "Epoch 4297, Loss: 0.03696503955870867, Final Batch Loss: 0.006654319353401661\n",
      "Epoch 4298, Loss: 0.06941912695765495, Final Batch Loss: 0.01848338544368744\n",
      "Epoch 4299, Loss: 0.01933885831385851, Final Batch Loss: 0.005313645116984844\n",
      "Epoch 4300, Loss: 0.035223270766437054, Final Batch Loss: 0.006125104613602161\n",
      "Epoch 4301, Loss: 0.025975277181714773, Final Batch Loss: 0.004669086541980505\n",
      "Epoch 4302, Loss: 0.016476280987262726, Final Batch Loss: 0.006996337324380875\n",
      "Epoch 4303, Loss: 0.015927312895655632, Final Batch Loss: 0.0038519203662872314\n",
      "Epoch 4304, Loss: 0.006110603921115398, Final Batch Loss: 0.002257219748571515\n",
      "Epoch 4305, Loss: 0.045994846150279045, Final Batch Loss: 0.03358210250735283\n",
      "Epoch 4306, Loss: 0.023282580077648163, Final Batch Loss: 0.019880518317222595\n",
      "Epoch 4307, Loss: 0.014657455729320645, Final Batch Loss: 0.00214608502574265\n",
      "Epoch 4308, Loss: 0.014618797809816897, Final Batch Loss: 0.0017826069379225373\n",
      "Epoch 4309, Loss: 0.16194871254265308, Final Batch Loss: 0.13204094767570496\n",
      "Epoch 4310, Loss: 0.07115840865299106, Final Batch Loss: 0.06804259121417999\n",
      "Epoch 4311, Loss: 0.07352293841540813, Final Batch Loss: 0.009817717596888542\n",
      "Epoch 4312, Loss: 0.03818698972463608, Final Batch Loss: 0.006906893104314804\n",
      "Epoch 4313, Loss: 0.009352091932669282, Final Batch Loss: 0.006108706351369619\n",
      "Epoch 4314, Loss: 0.11244447901844978, Final Batch Loss: 0.070144422352314\n",
      "Epoch 4315, Loss: 0.05603846162557602, Final Batch Loss: 0.03023560345172882\n",
      "Epoch 4316, Loss: 0.147552952170372, Final Batch Loss: 0.07895825803279877\n",
      "Epoch 4317, Loss: 0.019596649333834648, Final Batch Loss: 0.010071637108922005\n",
      "Epoch 4318, Loss: 0.1361859031021595, Final Batch Loss: 0.09887049347162247\n",
      "Epoch 4319, Loss: 0.23148637637495995, Final Batch Loss: 0.028026755899190903\n",
      "Epoch 4320, Loss: 0.18302425742149353, Final Batch Loss: 0.08061274886131287\n",
      "Epoch 4321, Loss: 0.04525699373334646, Final Batch Loss: 0.03831479698419571\n",
      "Epoch 4322, Loss: 0.07263601757586002, Final Batch Loss: 0.059518709778785706\n",
      "Epoch 4323, Loss: 0.08247005939483643, Final Batch Loss: 0.01809345930814743\n",
      "Epoch 4324, Loss: 0.023510579019784927, Final Batch Loss: 0.010395948775112629\n",
      "Epoch 4325, Loss: 0.05465422570705414, Final Batch Loss: 0.010786879807710648\n",
      "Epoch 4326, Loss: 0.03761900030076504, Final Batch Loss: 0.02465137280523777\n",
      "Epoch 4327, Loss: 0.0470285271294415, Final Batch Loss: 0.006821580696851015\n",
      "Epoch 4328, Loss: 0.022867247462272644, Final Batch Loss: 0.008241310715675354\n",
      "Epoch 4329, Loss: 0.09990614838898182, Final Batch Loss: 0.030928829684853554\n",
      "Epoch 4330, Loss: 0.07017304375767708, Final Batch Loss: 0.02952176332473755\n",
      "Epoch 4331, Loss: 0.060650698840618134, Final Batch Loss: 0.04489048570394516\n",
      "Epoch 4332, Loss: 0.05207194946706295, Final Batch Loss: 0.03248850256204605\n",
      "Epoch 4333, Loss: 0.029915478080511093, Final Batch Loss: 0.01845586486160755\n",
      "Epoch 4334, Loss: 0.0165084688924253, Final Batch Loss: 0.009568501263856888\n",
      "Epoch 4335, Loss: 0.015599221922457218, Final Batch Loss: 0.008266876451671124\n",
      "Epoch 4336, Loss: 0.02634686278179288, Final Batch Loss: 0.02241252176463604\n",
      "Epoch 4337, Loss: 0.02333230711519718, Final Batch Loss: 0.013419614173471928\n",
      "Epoch 4338, Loss: 0.02946229837834835, Final Batch Loss: 0.015030307695269585\n",
      "Epoch 4339, Loss: 0.017709523672237992, Final Batch Loss: 0.014623424969613552\n",
      "Epoch 4340, Loss: 0.03928631264716387, Final Batch Loss: 0.031505100429058075\n",
      "Epoch 4341, Loss: 0.023659437894821167, Final Batch Loss: 0.005149319767951965\n",
      "Epoch 4342, Loss: 0.05187508836388588, Final Batch Loss: 0.01975991204380989\n",
      "Epoch 4343, Loss: 0.012717584613710642, Final Batch Loss: 0.005948864854872227\n",
      "Epoch 4344, Loss: 0.010829378850758076, Final Batch Loss: 0.004763217642903328\n",
      "Epoch 4345, Loss: 0.008554585743695498, Final Batch Loss: 0.0054982248693704605\n",
      "Epoch 4346, Loss: 0.03548243502154946, Final Batch Loss: 0.004429294262081385\n",
      "Epoch 4347, Loss: 0.04840677231550217, Final Batch Loss: 0.03131194785237312\n",
      "Epoch 4348, Loss: 0.014240695629268885, Final Batch Loss: 0.005764532368630171\n",
      "Epoch 4349, Loss: 0.04207377415150404, Final Batch Loss: 0.0022038882598280907\n",
      "Epoch 4350, Loss: 0.01780393486842513, Final Batch Loss: 0.004453322384506464\n",
      "Epoch 4351, Loss: 0.03470051661133766, Final Batch Loss: 0.0030594654381275177\n",
      "Epoch 4352, Loss: 0.01618667831644416, Final Batch Loss: 0.004648297559469938\n",
      "Epoch 4353, Loss: 0.024507964961230755, Final Batch Loss: 0.014859265647828579\n",
      "Epoch 4354, Loss: 0.018795344280079007, Final Batch Loss: 0.00260154833085835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4355, Loss: 0.011095181107521057, Final Batch Loss: 0.0050602625124156475\n",
      "Epoch 4356, Loss: 0.0433759605512023, Final Batch Loss: 0.03142617270350456\n",
      "Epoch 4357, Loss: 0.010284642223268747, Final Batch Loss: 0.004347447771579027\n",
      "Epoch 4358, Loss: 0.020212205592542887, Final Batch Loss: 0.004909818526357412\n",
      "Epoch 4359, Loss: 0.006198344519361854, Final Batch Loss: 0.0015389660838991404\n",
      "Epoch 4360, Loss: 0.02566128969192505, Final Batch Loss: 0.012941792607307434\n",
      "Epoch 4361, Loss: 0.038775211200118065, Final Batch Loss: 0.018551144748926163\n",
      "Epoch 4362, Loss: 0.023651568684726954, Final Batch Loss: 0.004101878497749567\n",
      "Epoch 4363, Loss: 0.030832595191895962, Final Batch Loss: 0.020805135369300842\n",
      "Epoch 4364, Loss: 0.019144084537401795, Final Batch Loss: 0.002249328652396798\n",
      "Epoch 4365, Loss: 0.010059278225526214, Final Batch Loss: 0.0015614551957696676\n",
      "Epoch 4366, Loss: 0.046994274482131004, Final Batch Loss: 0.017729129642248154\n",
      "Epoch 4367, Loss: 0.03233125293627381, Final Batch Loss: 0.007074000779539347\n",
      "Epoch 4368, Loss: 0.009087714366614819, Final Batch Loss: 0.004464682191610336\n",
      "Epoch 4369, Loss: 0.02770283166319132, Final Batch Loss: 0.004401284269988537\n",
      "Epoch 4370, Loss: 0.054937943816185, Final Batch Loss: 0.01851440966129303\n",
      "Epoch 4371, Loss: 0.012184930499643087, Final Batch Loss: 0.0051389653235673904\n",
      "Epoch 4372, Loss: 0.02152788615785539, Final Batch Loss: 0.002519074594601989\n",
      "Epoch 4373, Loss: 0.03477730602025986, Final Batch Loss: 0.02317602001130581\n",
      "Epoch 4374, Loss: 0.023864129791036248, Final Batch Loss: 0.003363833064213395\n",
      "Epoch 4375, Loss: 0.025914625264704227, Final Batch Loss: 0.016673529520630836\n",
      "Epoch 4376, Loss: 0.04618014022707939, Final Batch Loss: 0.03146820142865181\n",
      "Epoch 4377, Loss: 0.0488918200135231, Final Batch Loss: 0.040281765162944794\n",
      "Epoch 4378, Loss: 0.038246940821409225, Final Batch Loss: 0.011131491512060165\n",
      "Epoch 4379, Loss: 0.028287050779908895, Final Batch Loss: 0.007667671423405409\n",
      "Epoch 4380, Loss: 0.015178818255662918, Final Batch Loss: 0.005165041424334049\n",
      "Epoch 4381, Loss: 0.008452407084405422, Final Batch Loss: 0.004074611701071262\n",
      "Epoch 4382, Loss: 0.027183960424736142, Final Batch Loss: 0.0038867269176989794\n",
      "Epoch 4383, Loss: 0.014138050377368927, Final Batch Loss: 0.010311407037079334\n",
      "Epoch 4384, Loss: 0.01031910115852952, Final Batch Loss: 0.00310726510360837\n",
      "Epoch 4385, Loss: 0.014480465091764927, Final Batch Loss: 0.0024813730269670486\n",
      "Epoch 4386, Loss: 0.024001083336770535, Final Batch Loss: 0.01691095158457756\n",
      "Epoch 4387, Loss: 0.014163727406412363, Final Batch Loss: 0.010713933035731316\n",
      "Epoch 4388, Loss: 0.06013871356844902, Final Batch Loss: 0.017313681542873383\n",
      "Epoch 4389, Loss: 0.014307079836726189, Final Batch Loss: 0.0025332970544695854\n",
      "Epoch 4390, Loss: 0.017478603636845946, Final Batch Loss: 0.013653047382831573\n",
      "Epoch 4391, Loss: 0.010295243933796883, Final Batch Loss: 0.005299558397382498\n",
      "Epoch 4392, Loss: 0.05453205527737737, Final Batch Loss: 0.05048606917262077\n",
      "Epoch 4393, Loss: 0.05388335511088371, Final Batch Loss: 0.04757395014166832\n",
      "Epoch 4394, Loss: 0.012052578385919333, Final Batch Loss: 0.005752711556851864\n",
      "Epoch 4395, Loss: 0.05287562683224678, Final Batch Loss: 0.03452843800187111\n",
      "Epoch 4396, Loss: 0.010563346091657877, Final Batch Loss: 0.005364657379686832\n",
      "Epoch 4397, Loss: 0.07848858088254929, Final Batch Loss: 0.055805351585149765\n",
      "Epoch 4398, Loss: 0.01467967638745904, Final Batch Loss: 0.007822912186384201\n",
      "Epoch 4399, Loss: 0.08578525204211473, Final Batch Loss: 0.07105693966150284\n",
      "Epoch 4400, Loss: 0.16762541234493256, Final Batch Loss: 0.10893421620130539\n",
      "Epoch 4401, Loss: 0.01019572839140892, Final Batch Loss: 0.002788919024169445\n",
      "Epoch 4402, Loss: 0.053654017858207226, Final Batch Loss: 0.04599889740347862\n",
      "Epoch 4403, Loss: 0.15278325229883194, Final Batch Loss: 0.0722137913107872\n",
      "Epoch 4404, Loss: 0.12407336384057999, Final Batch Loss: 0.07339341193437576\n",
      "Epoch 4405, Loss: 0.09782716259360313, Final Batch Loss: 0.059057045727968216\n",
      "Epoch 4406, Loss: 0.0903446301817894, Final Batch Loss: 0.03139502927660942\n",
      "Epoch 4407, Loss: 0.03511174162849784, Final Batch Loss: 0.029515251517295837\n",
      "Epoch 4408, Loss: 0.04278938286006451, Final Batch Loss: 0.020944537594914436\n",
      "Epoch 4409, Loss: 0.044221870601177216, Final Batch Loss: 0.010065332055091858\n",
      "Epoch 4410, Loss: 0.045231616124510765, Final Batch Loss: 0.02038581483066082\n",
      "Epoch 4411, Loss: 0.04875195166096091, Final Batch Loss: 0.04137105122208595\n",
      "Epoch 4412, Loss: 0.03188225533813238, Final Batch Loss: 0.017667299136519432\n",
      "Epoch 4413, Loss: 0.015150237828493118, Final Batch Loss: 0.005693117156624794\n",
      "Epoch 4414, Loss: 0.02530320454388857, Final Batch Loss: 0.009040388278663158\n",
      "Epoch 4415, Loss: 0.04143163189291954, Final Batch Loss: 0.007123325020074844\n",
      "Epoch 4416, Loss: 0.01438539708033204, Final Batch Loss: 0.006088534835726023\n",
      "Epoch 4417, Loss: 0.03540980722755194, Final Batch Loss: 0.008631431497633457\n",
      "Epoch 4418, Loss: 0.04983179271221161, Final Batch Loss: 0.028798209503293037\n",
      "Epoch 4419, Loss: 0.020374251063913107, Final Batch Loss: 0.006127601023763418\n",
      "Epoch 4420, Loss: 0.03511903062462807, Final Batch Loss: 0.011260053142905235\n",
      "Epoch 4421, Loss: 0.01702985353767872, Final Batch Loss: 0.01107448898255825\n",
      "Epoch 4422, Loss: 0.025434832787141204, Final Batch Loss: 0.0034213510807603598\n",
      "Epoch 4423, Loss: 0.08275888115167618, Final Batch Loss: 0.03394688665866852\n",
      "Epoch 4424, Loss: 0.04253842867910862, Final Batch Loss: 0.020084623247385025\n",
      "Epoch 4425, Loss: 0.02093628654256463, Final Batch Loss: 0.006781968753784895\n",
      "Epoch 4426, Loss: 0.014931656885892153, Final Batch Loss: 0.007051889318972826\n",
      "Epoch 4427, Loss: 0.01353420875966549, Final Batch Loss: 0.007792961318045855\n",
      "Epoch 4428, Loss: 0.024032294750213623, Final Batch Loss: 0.012318885885179043\n",
      "Epoch 4429, Loss: 0.035929061472415924, Final Batch Loss: 0.01822466216981411\n",
      "Epoch 4430, Loss: 0.01996970036998391, Final Batch Loss: 0.00496820779517293\n",
      "Epoch 4431, Loss: 0.028268364258110523, Final Batch Loss: 0.02473769150674343\n",
      "Epoch 4432, Loss: 0.016535583417862654, Final Batch Loss: 0.009196286089718342\n",
      "Epoch 4433, Loss: 0.008820838993415236, Final Batch Loss: 0.0026369013357907534\n",
      "Epoch 4434, Loss: 0.007936750538647175, Final Batch Loss: 0.002293661702424288\n",
      "Epoch 4435, Loss: 0.015617373399436474, Final Batch Loss: 0.010496173985302448\n",
      "Epoch 4436, Loss: 0.03148835618048906, Final Batch Loss: 0.022411581128835678\n",
      "Epoch 4437, Loss: 0.015468405093997717, Final Batch Loss: 0.005340422037988901\n",
      "Epoch 4438, Loss: 0.019930175971239805, Final Batch Loss: 0.016506265848875046\n",
      "Epoch 4439, Loss: 0.03341160202398896, Final Batch Loss: 0.001779275480657816\n",
      "Epoch 4440, Loss: 0.007293293485417962, Final Batch Loss: 0.002526284893974662\n",
      "Epoch 4441, Loss: 0.016085903625935316, Final Batch Loss: 0.0052140033803880215\n",
      "Epoch 4442, Loss: 0.021780463866889477, Final Batch Loss: 0.015025585889816284\n",
      "Epoch 4443, Loss: 0.03922624047845602, Final Batch Loss: 0.03308591991662979\n",
      "Epoch 4444, Loss: 0.04412010405212641, Final Batch Loss: 0.01042229775339365\n",
      "Epoch 4445, Loss: 0.07039235904812813, Final Batch Loss: 0.051154155284166336\n",
      "Epoch 4446, Loss: 0.022465028334409, Final Batch Loss: 0.01850387640297413\n",
      "Epoch 4447, Loss: 0.11613784730434418, Final Batch Loss: 0.04766774922609329\n",
      "Epoch 4448, Loss: 0.06171171925961971, Final Batch Loss: 0.019635429605841637\n",
      "Epoch 4449, Loss: 0.016943699680268764, Final Batch Loss: 0.0147910350933671\n",
      "Epoch 4450, Loss: 0.02380310371518135, Final Batch Loss: 0.015747034922242165\n",
      "Epoch 4451, Loss: 0.027529258280992508, Final Batch Loss: 0.008624417707324028\n",
      "Epoch 4452, Loss: 0.03895827382802963, Final Batch Loss: 0.020035728812217712\n",
      "Epoch 4453, Loss: 0.055688487365841866, Final Batch Loss: 0.02979908511042595\n",
      "Epoch 4454, Loss: 0.012470058863982558, Final Batch Loss: 0.0038515168707817793\n",
      "Epoch 4455, Loss: 0.06277102790772915, Final Batch Loss: 0.0234499704092741\n",
      "Epoch 4456, Loss: 0.018276721704751253, Final Batch Loss: 0.003955565858632326\n",
      "Epoch 4457, Loss: 0.031809646636247635, Final Batch Loss: 0.011119252070784569\n",
      "Epoch 4458, Loss: 0.038118595257401466, Final Batch Loss: 0.025436105206608772\n",
      "Epoch 4459, Loss: 0.09020071476697922, Final Batch Loss: 0.07157161086797714\n",
      "Epoch 4460, Loss: 0.030145260505378246, Final Batch Loss: 0.016023479402065277\n",
      "Epoch 4461, Loss: 0.0250078272074461, Final Batch Loss: 0.005006114020943642\n",
      "Epoch 4462, Loss: 0.024843602441251278, Final Batch Loss: 0.017781350761651993\n",
      "Epoch 4463, Loss: 0.014110162388533354, Final Batch Loss: 0.003980167675763369\n",
      "Epoch 4464, Loss: 0.11972610466182232, Final Batch Loss: 0.11142797023057938\n",
      "Epoch 4465, Loss: 0.011005754116922617, Final Batch Loss: 0.003974173218011856\n",
      "Epoch 4466, Loss: 0.057502771727740765, Final Batch Loss: 0.04542861506342888\n",
      "Epoch 4467, Loss: 0.05213672108948231, Final Batch Loss: 0.035542428493499756\n",
      "Epoch 4468, Loss: 0.05917404335923493, Final Batch Loss: 0.003486455651000142\n",
      "Epoch 4469, Loss: 0.0540531650185585, Final Batch Loss: 0.03358842805027962\n",
      "Epoch 4470, Loss: 0.04861117713153362, Final Batch Loss: 0.03732376918196678\n",
      "Epoch 4471, Loss: 0.025544556323438883, Final Batch Loss: 0.0067528714425861835\n",
      "Epoch 4472, Loss: 0.024377685505896807, Final Batch Loss: 0.0065341223962605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4473, Loss: 0.030138025991618633, Final Batch Loss: 0.020185960456728935\n",
      "Epoch 4474, Loss: 0.015249538235366344, Final Batch Loss: 0.004841804504394531\n",
      "Epoch 4475, Loss: 0.01846630545333028, Final Batch Loss: 0.004093173425644636\n",
      "Epoch 4476, Loss: 0.008664073189720511, Final Batch Loss: 0.005029486026614904\n",
      "Epoch 4477, Loss: 0.06762995384633541, Final Batch Loss: 0.0508313812315464\n",
      "Epoch 4478, Loss: 0.032563683576881886, Final Batch Loss: 0.01845628209412098\n",
      "Epoch 4479, Loss: 0.007364220917224884, Final Batch Loss: 0.003656880697235465\n",
      "Epoch 4480, Loss: 0.11179516464471817, Final Batch Loss: 0.07859985530376434\n",
      "Epoch 4481, Loss: 0.012350973673164845, Final Batch Loss: 0.008401728235185146\n",
      "Epoch 4482, Loss: 0.0530222337692976, Final Batch Loss: 0.03525516018271446\n",
      "Epoch 4483, Loss: 0.01994090573862195, Final Batch Loss: 0.007201293017715216\n",
      "Epoch 4484, Loss: 0.028582262806594372, Final Batch Loss: 0.008991210721433163\n",
      "Epoch 4485, Loss: 0.019799375208094716, Final Batch Loss: 0.0027472053188830614\n",
      "Epoch 4486, Loss: 0.06805873662233353, Final Batch Loss: 0.03337793052196503\n",
      "Epoch 4487, Loss: 0.027384126093238592, Final Batch Loss: 0.0072003561072051525\n",
      "Epoch 4488, Loss: 0.018539006356149912, Final Batch Loss: 0.006038955878466368\n",
      "Epoch 4489, Loss: 0.04479199321940541, Final Batch Loss: 0.004641458857804537\n",
      "Epoch 4490, Loss: 0.06213797442615032, Final Batch Loss: 0.047049589455127716\n",
      "Epoch 4491, Loss: 0.041375440545380116, Final Batch Loss: 0.03610942140221596\n",
      "Epoch 4492, Loss: 0.036738911643624306, Final Batch Loss: 0.016513580456376076\n",
      "Epoch 4493, Loss: 0.04026245325803757, Final Batch Loss: 0.010062893852591515\n",
      "Epoch 4494, Loss: 0.03114245180040598, Final Batch Loss: 0.012361637316644192\n",
      "Epoch 4495, Loss: 0.06027703732252121, Final Batch Loss: 0.0059776753187179565\n",
      "Epoch 4496, Loss: 0.02997731603682041, Final Batch Loss: 0.018556147813796997\n",
      "Epoch 4497, Loss: 0.04318417049944401, Final Batch Loss: 0.022292256355285645\n",
      "Epoch 4498, Loss: 0.0068696190137416124, Final Batch Loss: 0.002848466159775853\n",
      "Epoch 4499, Loss: 0.025700771249830723, Final Batch Loss: 0.014424124732613564\n",
      "Epoch 4500, Loss: 0.08311664499342442, Final Batch Loss: 0.06797541677951813\n",
      "Epoch 4501, Loss: 0.028130984865128994, Final Batch Loss: 0.0199113842099905\n",
      "Epoch 4502, Loss: 0.030446229502558708, Final Batch Loss: 0.01908951997756958\n",
      "Epoch 4503, Loss: 0.026503812288865447, Final Batch Loss: 0.0027069968637079\n",
      "Epoch 4504, Loss: 0.01879080943763256, Final Batch Loss: 0.008016142062842846\n",
      "Epoch 4505, Loss: 0.04922015778720379, Final Batch Loss: 0.040484242141246796\n",
      "Epoch 4506, Loss: 0.004309117910452187, Final Batch Loss: 0.0019252655329182744\n",
      "Epoch 4507, Loss: 0.020947661716490984, Final Batch Loss: 0.0044961594976484776\n",
      "Epoch 4508, Loss: 0.01859659468755126, Final Batch Loss: 0.012279095128178596\n",
      "Epoch 4509, Loss: 0.02084259851835668, Final Batch Loss: 0.0018387751188129187\n",
      "Epoch 4510, Loss: 0.02770464587956667, Final Batch Loss: 0.01959814876317978\n",
      "Epoch 4511, Loss: 0.02209552749991417, Final Batch Loss: 0.010516205802559853\n",
      "Epoch 4512, Loss: 0.01909781526774168, Final Batch Loss: 0.01595696620643139\n",
      "Epoch 4513, Loss: 0.021825413219630718, Final Batch Loss: 0.011185498908162117\n",
      "Epoch 4514, Loss: 0.03264131676405668, Final Batch Loss: 0.014335705898702145\n",
      "Epoch 4515, Loss: 0.03658016864210367, Final Batch Loss: 0.029535681009292603\n",
      "Epoch 4516, Loss: 0.028948428109288216, Final Batch Loss: 0.016428662464022636\n",
      "Epoch 4517, Loss: 0.02292629238218069, Final Batch Loss: 0.007956882938742638\n",
      "Epoch 4518, Loss: 0.056160176172852516, Final Batch Loss: 0.03858286514878273\n",
      "Epoch 4519, Loss: 0.03049680730327964, Final Batch Loss: 0.024685241281986237\n",
      "Epoch 4520, Loss: 0.03405397292226553, Final Batch Loss: 0.024614281952381134\n",
      "Epoch 4521, Loss: 0.02814009808935225, Final Batch Loss: 0.0038253015372902155\n",
      "Epoch 4522, Loss: 0.012957153609022498, Final Batch Loss: 0.009568429552018642\n",
      "Epoch 4523, Loss: 0.0443272702395916, Final Batch Loss: 0.0157987829297781\n",
      "Epoch 4524, Loss: 0.08167382702231407, Final Batch Loss: 0.07096659392118454\n",
      "Epoch 4525, Loss: 0.01516188308596611, Final Batch Loss: 0.00682944618165493\n",
      "Epoch 4526, Loss: 0.039403798058629036, Final Batch Loss: 0.014424474909901619\n",
      "Epoch 4527, Loss: 0.027428610948845744, Final Batch Loss: 0.0037336430978029966\n",
      "Epoch 4528, Loss: 0.005937229143455625, Final Batch Loss: 0.002218510489910841\n",
      "Epoch 4529, Loss: 0.024219497106969357, Final Batch Loss: 0.015265218913555145\n",
      "Epoch 4530, Loss: 0.022139686159789562, Final Batch Loss: 0.0025011757388710976\n",
      "Epoch 4531, Loss: 0.02322519663721323, Final Batch Loss: 0.020379645749926567\n",
      "Epoch 4532, Loss: 0.036885941401124, Final Batch Loss: 0.01700124889612198\n",
      "Epoch 4533, Loss: 0.01018035807646811, Final Batch Loss: 0.0026655651163309813\n",
      "Epoch 4534, Loss: 0.0412829490378499, Final Batch Loss: 0.03665359690785408\n",
      "Epoch 4535, Loss: 0.04816918261349201, Final Batch Loss: 0.03316380828619003\n",
      "Epoch 4536, Loss: 0.027864699717611074, Final Batch Loss: 0.007301914971321821\n",
      "Epoch 4537, Loss: 0.00857566180638969, Final Batch Loss: 0.0056899418123066425\n",
      "Epoch 4538, Loss: 0.017748489044606686, Final Batch Loss: 0.00597709696739912\n",
      "Epoch 4539, Loss: 0.0207617599517107, Final Batch Loss: 0.009528180584311485\n",
      "Epoch 4540, Loss: 0.02285651955753565, Final Batch Loss: 0.011370670050382614\n",
      "Epoch 4541, Loss: 0.0289054736495018, Final Batch Loss: 0.016011303290724754\n",
      "Epoch 4542, Loss: 0.03269314765930176, Final Batch Loss: 0.024805448949337006\n",
      "Epoch 4543, Loss: 0.06468713656067848, Final Batch Loss: 0.02818811684846878\n",
      "Epoch 4544, Loss: 0.04915756359696388, Final Batch Loss: 0.012752827256917953\n",
      "Epoch 4545, Loss: 0.05785064212977886, Final Batch Loss: 0.03585003688931465\n",
      "Epoch 4546, Loss: 0.03948372742161155, Final Batch Loss: 0.00595371937379241\n",
      "Epoch 4547, Loss: 0.05196730047464371, Final Batch Loss: 0.017862901091575623\n",
      "Epoch 4548, Loss: 0.03648242074996233, Final Batch Loss: 0.008554437197744846\n",
      "Epoch 4549, Loss: 0.02777167223393917, Final Batch Loss: 0.021109119057655334\n",
      "Epoch 4550, Loss: 0.028903480619192123, Final Batch Loss: 0.011545289307832718\n",
      "Epoch 4551, Loss: 0.020726073998957872, Final Batch Loss: 0.00396713288500905\n",
      "Epoch 4552, Loss: 0.036085815634578466, Final Batch Loss: 0.029532765969634056\n",
      "Epoch 4553, Loss: 0.03493135701864958, Final Batch Loss: 0.029101451858878136\n",
      "Epoch 4554, Loss: 0.04013381898403168, Final Batch Loss: 0.02787891961634159\n",
      "Epoch 4555, Loss: 0.011212336597964168, Final Batch Loss: 0.002204296877607703\n",
      "Epoch 4556, Loss: 0.011475817300379276, Final Batch Loss: 0.007964727468788624\n",
      "Epoch 4557, Loss: 0.027504885103553534, Final Batch Loss: 0.021876482293009758\n",
      "Epoch 4558, Loss: 0.03759262152016163, Final Batch Loss: 0.017885198816657066\n",
      "Epoch 4559, Loss: 0.09701041132211685, Final Batch Loss: 0.07445283979177475\n",
      "Epoch 4560, Loss: 0.04311643820255995, Final Batch Loss: 0.01066416036337614\n",
      "Epoch 4561, Loss: 0.03980923257768154, Final Batch Loss: 0.013466741889715195\n",
      "Epoch 4562, Loss: 0.015914655290544033, Final Batch Loss: 0.009628483094274998\n",
      "Epoch 4563, Loss: 0.010966353351250291, Final Batch Loss: 0.0032999103423208\n",
      "Epoch 4564, Loss: 0.025948847644031048, Final Batch Loss: 0.003776007331907749\n",
      "Epoch 4565, Loss: 0.012860957998782396, Final Batch Loss: 0.00519469054415822\n",
      "Epoch 4566, Loss: 0.05337348487228155, Final Batch Loss: 0.04634415730834007\n",
      "Epoch 4567, Loss: 0.009936357848346233, Final Batch Loss: 0.0025715678930282593\n",
      "Epoch 4568, Loss: 0.008554075844585896, Final Batch Loss: 0.006359700579196215\n",
      "Epoch 4569, Loss: 0.017518495675176382, Final Batch Loss: 0.011227053590118885\n",
      "Epoch 4570, Loss: 0.043708483688533306, Final Batch Loss: 0.031128736212849617\n",
      "Epoch 4571, Loss: 0.03411975037306547, Final Batch Loss: 0.013354749418795109\n",
      "Epoch 4572, Loss: 0.033578083384782076, Final Batch Loss: 0.004712254274636507\n",
      "Epoch 4573, Loss: 0.04274949058890343, Final Batch Loss: 0.013402767479419708\n",
      "Epoch 4574, Loss: 0.02857037854846567, Final Batch Loss: 0.0019228126620873809\n",
      "Epoch 4575, Loss: 0.011758753098547459, Final Batch Loss: 0.006167584098875523\n",
      "Epoch 4576, Loss: 0.03649755381047726, Final Batch Loss: 0.016074253246188164\n",
      "Epoch 4577, Loss: 0.02430912060663104, Final Batch Loss: 0.017875483259558678\n",
      "Epoch 4578, Loss: 0.04706851299852133, Final Batch Loss: 0.031538818031549454\n",
      "Epoch 4579, Loss: 0.023227890953421593, Final Batch Loss: 0.010639894753694534\n",
      "Epoch 4580, Loss: 0.01573781529441476, Final Batch Loss: 0.004666969645768404\n",
      "Epoch 4581, Loss: 0.018193486146628857, Final Batch Loss: 0.009161404334008694\n",
      "Epoch 4582, Loss: 0.016827565152198076, Final Batch Loss: 0.012114174664020538\n",
      "Epoch 4583, Loss: 0.01919303461909294, Final Batch Loss: 0.005156888626515865\n",
      "Epoch 4584, Loss: 0.01152264210395515, Final Batch Loss: 0.003793868934735656\n",
      "Epoch 4585, Loss: 0.049878066405653954, Final Batch Loss: 0.029380304738879204\n",
      "Epoch 4586, Loss: 0.018810448702424765, Final Batch Loss: 0.005586516577750444\n",
      "Epoch 4587, Loss: 0.017848909366875887, Final Batch Loss: 0.005622569937258959\n",
      "Epoch 4588, Loss: 0.045286182314157486, Final Batch Loss: 0.027655595913529396\n",
      "Epoch 4589, Loss: 0.009570124209858477, Final Batch Loss: 0.007852680049836636\n",
      "Epoch 4590, Loss: 0.013169096782803535, Final Batch Loss: 0.004612528719007969\n",
      "Epoch 4591, Loss: 0.10511643998324871, Final Batch Loss: 0.07857093960046768\n",
      "Epoch 4592, Loss: 0.027098732069134712, Final Batch Loss: 0.018292659893631935\n",
      "Epoch 4593, Loss: 0.008843954652547836, Final Batch Loss: 0.0051316265016794205\n",
      "Epoch 4594, Loss: 0.012895964086055756, Final Batch Loss: 0.005143174435943365\n",
      "Epoch 4595, Loss: 0.08640675200149417, Final Batch Loss: 0.00558501435443759\n",
      "Epoch 4596, Loss: 0.05959249660372734, Final Batch Loss: 0.051395710557699203\n",
      "Epoch 4597, Loss: 0.09546003490686417, Final Batch Loss: 0.022128649055957794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4598, Loss: 0.063616874627769, Final Batch Loss: 0.05462637171149254\n",
      "Epoch 4599, Loss: 0.10336916148662567, Final Batch Loss: 0.010512851178646088\n",
      "Epoch 4600, Loss: 0.06655282527208328, Final Batch Loss: 0.0208854041993618\n",
      "Epoch 4601, Loss: 0.06546097621321678, Final Batch Loss: 0.012753210961818695\n",
      "Epoch 4602, Loss: 0.11526373401284218, Final Batch Loss: 0.055241990834474564\n",
      "Epoch 4603, Loss: 0.01715981331653893, Final Batch Loss: 0.014242694713175297\n",
      "Epoch 4604, Loss: 0.03786125406622887, Final Batch Loss: 0.00579255074262619\n",
      "Epoch 4605, Loss: 0.03291110089048743, Final Batch Loss: 0.006176344584673643\n",
      "Epoch 4606, Loss: 0.029088818468153477, Final Batch Loss: 0.010909073986113071\n",
      "Epoch 4607, Loss: 0.056345170363783836, Final Batch Loss: 0.024671727791428566\n",
      "Epoch 4608, Loss: 0.014151988551020622, Final Batch Loss: 0.009920994751155376\n",
      "Epoch 4609, Loss: 0.016405750531703234, Final Batch Loss: 0.011386745609343052\n",
      "Epoch 4610, Loss: 0.02964345458894968, Final Batch Loss: 0.008947261609137058\n",
      "Epoch 4611, Loss: 0.05030165147036314, Final Batch Loss: 0.0039459457620978355\n",
      "Epoch 4612, Loss: 0.08889704942703247, Final Batch Loss: 0.0627257376909256\n",
      "Epoch 4613, Loss: 0.03443397395312786, Final Batch Loss: 0.016547055914998055\n",
      "Epoch 4614, Loss: 0.024422117974609137, Final Batch Loss: 0.007199590560048819\n",
      "Epoch 4615, Loss: 0.03035811148583889, Final Batch Loss: 0.02265361323952675\n",
      "Epoch 4616, Loss: 0.01743699237704277, Final Batch Loss: 0.005132634192705154\n",
      "Epoch 4617, Loss: 0.038192365784198046, Final Batch Loss: 0.007660387549549341\n",
      "Epoch 4618, Loss: 0.01861240016296506, Final Batch Loss: 0.005011019762605429\n",
      "Epoch 4619, Loss: 0.02456929162144661, Final Batch Loss: 0.019403813406825066\n",
      "Epoch 4620, Loss: 0.01075967331416905, Final Batch Loss: 0.007081793155521154\n",
      "Epoch 4621, Loss: 0.015181154012680054, Final Batch Loss: 0.004802988842129707\n",
      "Epoch 4622, Loss: 0.010637641418725252, Final Batch Loss: 0.0051565105095505714\n",
      "Epoch 4623, Loss: 0.05422019422985613, Final Batch Loss: 0.00246764556504786\n",
      "Epoch 4624, Loss: 0.044916270300745964, Final Batch Loss: 0.019622862339019775\n",
      "Epoch 4625, Loss: 0.04158060159534216, Final Batch Loss: 0.03372081741690636\n",
      "Epoch 4626, Loss: 0.01650542882271111, Final Batch Loss: 0.0015962461475282907\n",
      "Epoch 4627, Loss: 0.016475501703098416, Final Batch Loss: 0.0035360718611627817\n",
      "Epoch 4628, Loss: 0.02285199286416173, Final Batch Loss: 0.004591331351548433\n",
      "Epoch 4629, Loss: 0.042365246219560504, Final Batch Loss: 0.002771650208160281\n",
      "Epoch 4630, Loss: 0.028897469863295555, Final Batch Loss: 0.010094491764903069\n",
      "Epoch 4631, Loss: 0.005562403704971075, Final Batch Loss: 0.0021591149270534515\n",
      "Epoch 4632, Loss: 0.015146756544709206, Final Batch Loss: 0.0074103036895394325\n",
      "Epoch 4633, Loss: 0.02321179024875164, Final Batch Loss: 0.015069352462887764\n",
      "Epoch 4634, Loss: 0.017408259212970734, Final Batch Loss: 0.004506373777985573\n",
      "Epoch 4635, Loss: 0.024057836271822453, Final Batch Loss: 0.010652301833033562\n",
      "Epoch 4636, Loss: 0.0369161032140255, Final Batch Loss: 0.02197200618684292\n",
      "Epoch 4637, Loss: 0.015556453727185726, Final Batch Loss: 0.0044687651097774506\n",
      "Epoch 4638, Loss: 0.04831200931221247, Final Batch Loss: 0.006459248252213001\n",
      "Epoch 4639, Loss: 0.008540557697415352, Final Batch Loss: 0.005070030689239502\n",
      "Epoch 4640, Loss: 0.02347831125371158, Final Batch Loss: 0.002853530226275325\n",
      "Epoch 4641, Loss: 0.00846374873071909, Final Batch Loss: 0.0033854516223073006\n",
      "Epoch 4642, Loss: 0.018287149257957935, Final Batch Loss: 0.010573171079158783\n",
      "Epoch 4643, Loss: 0.0445229415781796, Final Batch Loss: 0.0032875542528927326\n",
      "Epoch 4644, Loss: 0.043064975179731846, Final Batch Loss: 0.011021123267710209\n",
      "Epoch 4645, Loss: 0.02058961521834135, Final Batch Loss: 0.015002420172095299\n",
      "Epoch 4646, Loss: 0.05473385541699827, Final Batch Loss: 0.0024555714335292578\n",
      "Epoch 4647, Loss: 0.01626467565074563, Final Batch Loss: 0.0028711804188787937\n",
      "Epoch 4648, Loss: 0.011136265937238932, Final Batch Loss: 0.0057429117150604725\n",
      "Epoch 4649, Loss: 0.012303397292271256, Final Batch Loss: 0.008958284743130207\n",
      "Epoch 4650, Loss: 0.018490132875740528, Final Batch Loss: 0.005906159058213234\n",
      "Epoch 4651, Loss: 0.010920343222096562, Final Batch Loss: 0.008159465156495571\n",
      "Epoch 4652, Loss: 0.020205148495733738, Final Batch Loss: 0.012353762052953243\n",
      "Epoch 4653, Loss: 0.020345325814560056, Final Batch Loss: 0.01757710985839367\n",
      "Epoch 4654, Loss: 0.08200373128056526, Final Batch Loss: 0.03969607874751091\n",
      "Epoch 4655, Loss: 0.012829981511458755, Final Batch Loss: 0.0033128063660115004\n",
      "Epoch 4656, Loss: 0.035763274412602186, Final Batch Loss: 0.004342507105320692\n",
      "Epoch 4657, Loss: 0.007011521840468049, Final Batch Loss: 0.0032363394275307655\n",
      "Epoch 4658, Loss: 0.029889738652855158, Final Batch Loss: 0.024298958480358124\n",
      "Epoch 4659, Loss: 0.06517620198428631, Final Batch Loss: 0.0034163203090429306\n",
      "Epoch 4660, Loss: 0.04155036620795727, Final Batch Loss: 0.03653968498110771\n",
      "Epoch 4661, Loss: 0.017118837218731642, Final Batch Loss: 0.0036150715313851833\n",
      "Epoch 4662, Loss: 0.005797981983050704, Final Batch Loss: 0.0027710096910595894\n",
      "Epoch 4663, Loss: 0.0087612250354141, Final Batch Loss: 0.0033664049115031958\n",
      "Epoch 4664, Loss: 0.019335835240781307, Final Batch Loss: 0.010532093234360218\n",
      "Epoch 4665, Loss: 0.008361829444766045, Final Batch Loss: 0.005945667624473572\n",
      "Epoch 4666, Loss: 0.01236322894692421, Final Batch Loss: 0.008394709788262844\n",
      "Epoch 4667, Loss: 0.006609604344703257, Final Batch Loss: 0.0011692099506035447\n",
      "Epoch 4668, Loss: 0.01671258546411991, Final Batch Loss: 0.010221236385405064\n",
      "Epoch 4669, Loss: 0.011660341173410416, Final Batch Loss: 0.00560014508664608\n",
      "Epoch 4670, Loss: 0.010481198783963919, Final Batch Loss: 0.007136609870940447\n",
      "Epoch 4671, Loss: 0.010791297536343336, Final Batch Loss: 0.007697537075728178\n",
      "Epoch 4672, Loss: 0.009344348916783929, Final Batch Loss: 0.007058860268443823\n",
      "Epoch 4673, Loss: 0.0072882031090557575, Final Batch Loss: 0.0050718835555016994\n",
      "Epoch 4674, Loss: 0.006019639200530946, Final Batch Loss: 0.0017595592653378844\n",
      "Epoch 4675, Loss: 0.05031127901747823, Final Batch Loss: 0.003909100312739611\n",
      "Epoch 4676, Loss: 0.04505331814289093, Final Batch Loss: 0.021878737956285477\n",
      "Epoch 4677, Loss: 0.0159698654897511, Final Batch Loss: 0.0038082157261669636\n",
      "Epoch 4678, Loss: 0.024544942658394575, Final Batch Loss: 0.019028617069125175\n",
      "Epoch 4679, Loss: 0.048513210378587246, Final Batch Loss: 0.04171564057469368\n",
      "Epoch 4680, Loss: 0.008146794280037284, Final Batch Loss: 0.0033569124061614275\n",
      "Epoch 4681, Loss: 0.014777206117287278, Final Batch Loss: 0.003004414262250066\n",
      "Epoch 4682, Loss: 0.02623305469751358, Final Batch Loss: 0.021166842430830002\n",
      "Epoch 4683, Loss: 0.019871043507009745, Final Batch Loss: 0.0025806990452110767\n",
      "Epoch 4684, Loss: 0.003587564220651984, Final Batch Loss: 0.001607462763786316\n",
      "Epoch 4685, Loss: 0.010976392077282071, Final Batch Loss: 0.008412827737629414\n",
      "Epoch 4686, Loss: 0.005231125513091683, Final Batch Loss: 0.0020033421460539103\n",
      "Epoch 4687, Loss: 0.01837104093283415, Final Batch Loss: 0.014249748550355434\n",
      "Epoch 4688, Loss: 0.012505152728408575, Final Batch Loss: 0.008141393773257732\n",
      "Epoch 4689, Loss: 0.011656345333904028, Final Batch Loss: 0.004291218239814043\n",
      "Epoch 4690, Loss: 0.028145752381533384, Final Batch Loss: 0.022387944161891937\n",
      "Epoch 4691, Loss: 0.02492146845906973, Final Batch Loss: 0.008809116668999195\n",
      "Epoch 4692, Loss: 0.03411952964961529, Final Batch Loss: 0.009220585227012634\n",
      "Epoch 4693, Loss: 0.02766527864150703, Final Batch Loss: 0.02474437654018402\n",
      "Epoch 4694, Loss: 0.06405386608093977, Final Batch Loss: 0.05741849169135094\n",
      "Epoch 4695, Loss: 0.008849047357216477, Final Batch Loss: 0.005503373686224222\n",
      "Epoch 4696, Loss: 0.04376524593681097, Final Batch Loss: 0.033291153609752655\n",
      "Epoch 4697, Loss: 0.01650784257799387, Final Batch Loss: 0.004843550734221935\n",
      "Epoch 4698, Loss: 0.026025393279269338, Final Batch Loss: 0.002629905240610242\n",
      "Epoch 4699, Loss: 0.062133679166436195, Final Batch Loss: 0.029953451827168465\n",
      "Epoch 4700, Loss: 0.024646016769111156, Final Batch Loss: 0.015131738036870956\n",
      "Epoch 4701, Loss: 0.026446398813277483, Final Batch Loss: 0.022167369723320007\n",
      "Epoch 4702, Loss: 0.022445169277489185, Final Batch Loss: 0.008876313455402851\n",
      "Epoch 4703, Loss: 0.01135397981852293, Final Batch Loss: 0.004964890424162149\n",
      "Epoch 4704, Loss: 0.014822485391050577, Final Batch Loss: 0.003752583172172308\n",
      "Epoch 4705, Loss: 0.02296693390235305, Final Batch Loss: 0.004064697306603193\n",
      "Epoch 4706, Loss: 0.0721517177298665, Final Batch Loss: 0.06741595268249512\n",
      "Epoch 4707, Loss: 0.05172492191195488, Final Batch Loss: 0.031432367861270905\n",
      "Epoch 4708, Loss: 0.023107423447072506, Final Batch Loss: 0.007113258354365826\n",
      "Epoch 4709, Loss: 0.07797543052583933, Final Batch Loss: 0.07211417704820633\n",
      "Epoch 4710, Loss: 0.029169621877372265, Final Batch Loss: 0.013770443387329578\n",
      "Epoch 4711, Loss: 0.016091697383672, Final Batch Loss: 0.008958576247096062\n",
      "Epoch 4712, Loss: 0.04713069088757038, Final Batch Loss: 0.02835778519511223\n",
      "Epoch 4713, Loss: 0.017312390264123678, Final Batch Loss: 0.005002770107239485\n",
      "Epoch 4714, Loss: 0.010449116816744208, Final Batch Loss: 0.001462175277993083\n",
      "Epoch 4715, Loss: 0.04022081242874265, Final Batch Loss: 0.004662219900637865\n",
      "Epoch 4716, Loss: 0.02244534343481064, Final Batch Loss: 0.013233447447419167\n",
      "Epoch 4717, Loss: 0.017669651424512267, Final Batch Loss: 0.0028372991364449263\n",
      "Epoch 4718, Loss: 0.016846651677042246, Final Batch Loss: 0.01209399662911892\n",
      "Epoch 4719, Loss: 0.009105297736823559, Final Batch Loss: 0.00481291813775897\n",
      "Epoch 4720, Loss: 0.013285825029015541, Final Batch Loss: 0.005895599722862244\n",
      "Epoch 4721, Loss: 0.017101211473345757, Final Batch Loss: 0.005082940682768822\n",
      "Epoch 4722, Loss: 0.03737192042171955, Final Batch Loss: 0.006164655089378357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4723, Loss: 0.057359592989087105, Final Batch Loss: 0.007936159148812294\n",
      "Epoch 4724, Loss: 0.01967672724276781, Final Batch Loss: 0.013173623010516167\n",
      "Epoch 4725, Loss: 0.025855367071926594, Final Batch Loss: 0.01585930772125721\n",
      "Epoch 4726, Loss: 0.009006657637655735, Final Batch Loss: 0.004093334078788757\n",
      "Epoch 4727, Loss: 0.03137711621820927, Final Batch Loss: 0.02063664048910141\n",
      "Epoch 4728, Loss: 0.1124661322683096, Final Batch Loss: 0.10755203664302826\n",
      "Epoch 4729, Loss: 0.07952805608510971, Final Batch Loss: 0.07043824344873428\n",
      "Epoch 4730, Loss: 0.00820845109410584, Final Batch Loss: 0.002793670864775777\n",
      "Epoch 4731, Loss: 0.017331672133877873, Final Batch Loss: 0.0029684400651603937\n",
      "Epoch 4732, Loss: 0.0497893956489861, Final Batch Loss: 0.04354887828230858\n",
      "Epoch 4733, Loss: 0.0334261879324913, Final Batch Loss: 0.015341546386480331\n",
      "Epoch 4734, Loss: 0.004487044992856681, Final Batch Loss: 0.0017105467850342393\n",
      "Epoch 4735, Loss: 0.011901323916390538, Final Batch Loss: 0.0026354354340583086\n",
      "Epoch 4736, Loss: 0.028261651750653982, Final Batch Loss: 0.023429129272699356\n",
      "Epoch 4737, Loss: 0.030221521854400635, Final Batch Loss: 0.022652581334114075\n",
      "Epoch 4738, Loss: 0.026087412610650063, Final Batch Loss: 0.004538793116807938\n",
      "Epoch 4739, Loss: 0.030631180852651596, Final Batch Loss: 0.009175928309559822\n",
      "Epoch 4740, Loss: 0.04071441572159529, Final Batch Loss: 0.027475357055664062\n",
      "Epoch 4741, Loss: 0.08824445307254791, Final Batch Loss: 0.025753822177648544\n",
      "Epoch 4742, Loss: 0.018688754178583622, Final Batch Loss: 0.014555465430021286\n",
      "Epoch 4743, Loss: 0.019130610395222902, Final Batch Loss: 0.0066420636139810085\n",
      "Epoch 4744, Loss: 0.039989606477320194, Final Batch Loss: 0.027163589373230934\n",
      "Epoch 4745, Loss: 0.06421903893351555, Final Batch Loss: 0.03259963169693947\n",
      "Epoch 4746, Loss: 0.014841634780168533, Final Batch Loss: 0.0082843117415905\n",
      "Epoch 4747, Loss: 0.06572496704757214, Final Batch Loss: 0.043972235172986984\n",
      "Epoch 4748, Loss: 0.010085396002978086, Final Batch Loss: 0.0064917136915028095\n",
      "Epoch 4749, Loss: 0.0074425829807296395, Final Batch Loss: 0.0018201136263087392\n",
      "Epoch 4750, Loss: 0.015716183930635452, Final Batch Loss: 0.006420314311981201\n",
      "Epoch 4751, Loss: 0.0352445337921381, Final Batch Loss: 0.010443497449159622\n",
      "Epoch 4752, Loss: 0.026370263658463955, Final Batch Loss: 0.0037665804848074913\n",
      "Epoch 4753, Loss: 0.019477348774671555, Final Batch Loss: 0.0120866559445858\n",
      "Epoch 4754, Loss: 0.014623654074966908, Final Batch Loss: 0.004486536607146263\n",
      "Epoch 4755, Loss: 0.009540703147649765, Final Batch Loss: 0.002165178768336773\n",
      "Epoch 4756, Loss: 0.030585732776671648, Final Batch Loss: 0.027381138876080513\n",
      "Epoch 4757, Loss: 0.02227752562612295, Final Batch Loss: 0.015848873183131218\n",
      "Epoch 4758, Loss: 0.014825926627963781, Final Batch Loss: 0.01029356662184\n",
      "Epoch 4759, Loss: 0.029834379442036152, Final Batch Loss: 0.013090462423861027\n",
      "Epoch 4760, Loss: 0.02418870758265257, Final Batch Loss: 0.007517793215811253\n",
      "Epoch 4761, Loss: 0.02006813883781433, Final Batch Loss: 0.00795607641339302\n",
      "Epoch 4762, Loss: 0.013232636963948607, Final Batch Loss: 0.0020603674929589033\n",
      "Epoch 4763, Loss: 0.01807023398578167, Final Batch Loss: 0.006086321547627449\n",
      "Epoch 4764, Loss: 0.02242750208824873, Final Batch Loss: 0.012442633509635925\n",
      "Epoch 4765, Loss: 0.013958033174276352, Final Batch Loss: 0.0022177277132868767\n",
      "Epoch 4766, Loss: 0.0064036843832582235, Final Batch Loss: 0.00363967870362103\n",
      "Epoch 4767, Loss: 0.017400624230504036, Final Batch Loss: 0.00497698038816452\n",
      "Epoch 4768, Loss: 0.017816864419728518, Final Batch Loss: 0.0020109866745769978\n",
      "Epoch 4769, Loss: 0.01824616501107812, Final Batch Loss: 0.004669648129492998\n",
      "Epoch 4770, Loss: 0.009294673334807158, Final Batch Loss: 0.005324881989508867\n",
      "Epoch 4771, Loss: 0.004328647744841874, Final Batch Loss: 0.0017210278892889619\n",
      "Epoch 4772, Loss: 0.010920464294031262, Final Batch Loss: 0.007853769697248936\n",
      "Epoch 4773, Loss: 0.019605220295488834, Final Batch Loss: 0.011844813823699951\n",
      "Epoch 4774, Loss: 0.03739599045366049, Final Batch Loss: 0.0341825969517231\n",
      "Epoch 4775, Loss: 0.009403112810105085, Final Batch Loss: 0.0037627913989126682\n",
      "Epoch 4776, Loss: 0.023581386543810368, Final Batch Loss: 0.017533738166093826\n",
      "Epoch 4777, Loss: 0.03198849596083164, Final Batch Loss: 0.013727320358157158\n",
      "Epoch 4778, Loss: 0.020871490007266402, Final Batch Loss: 0.002280372893437743\n",
      "Epoch 4779, Loss: 0.01519015314988792, Final Batch Loss: 0.012437715195119381\n",
      "Epoch 4780, Loss: 0.03328504040837288, Final Batch Loss: 0.009161492809653282\n",
      "Epoch 4781, Loss: 0.04031296540051699, Final Batch Loss: 0.007028990425169468\n",
      "Epoch 4782, Loss: 0.02854982390999794, Final Batch Loss: 0.002249043434858322\n",
      "Epoch 4783, Loss: 0.02899027056992054, Final Batch Loss: 0.021636582911014557\n",
      "Epoch 4784, Loss: 0.047028396278619766, Final Batch Loss: 0.02822684310376644\n",
      "Epoch 4785, Loss: 0.009652857668697834, Final Batch Loss: 0.0033404980786144733\n",
      "Epoch 4786, Loss: 0.044447469525039196, Final Batch Loss: 0.032409489154815674\n",
      "Epoch 4787, Loss: 0.024188098963350058, Final Batch Loss: 0.002902632113546133\n",
      "Epoch 4788, Loss: 0.019578067120164633, Final Batch Loss: 0.0028081084601581097\n",
      "Epoch 4789, Loss: 0.010253727901726961, Final Batch Loss: 0.008006712421774864\n",
      "Epoch 4790, Loss: 0.01163080905098468, Final Batch Loss: 0.0015248433919623494\n",
      "Epoch 4791, Loss: 0.006252749590203166, Final Batch Loss: 0.002062238985672593\n",
      "Epoch 4792, Loss: 0.006845122203230858, Final Batch Loss: 0.002716958988457918\n",
      "Epoch 4793, Loss: 0.009444199502468109, Final Batch Loss: 0.006213102489709854\n",
      "Epoch 4794, Loss: 0.014848885126411915, Final Batch Loss: 0.005378554575145245\n",
      "Epoch 4795, Loss: 0.006626601098105311, Final Batch Loss: 0.0020653672982007265\n",
      "Epoch 4796, Loss: 0.025526251643896103, Final Batch Loss: 0.018389170989394188\n",
      "Epoch 4797, Loss: 0.024821370840072632, Final Batch Loss: 0.023240482434630394\n",
      "Epoch 4798, Loss: 0.026262931525707245, Final Batch Loss: 0.010706701315939426\n",
      "Epoch 4799, Loss: 0.010995423421263695, Final Batch Loss: 0.00798565149307251\n",
      "Epoch 4800, Loss: 0.019049864262342453, Final Batch Loss: 0.008292526938021183\n",
      "Epoch 4801, Loss: 0.02606166247278452, Final Batch Loss: 0.015526803210377693\n",
      "Epoch 4802, Loss: 0.026288049295544624, Final Batch Loss: 0.015642601996660233\n",
      "Epoch 4803, Loss: 0.03865776630118489, Final Batch Loss: 0.0042974879033863544\n",
      "Epoch 4804, Loss: 0.02505413955077529, Final Batch Loss: 0.0025474573485553265\n",
      "Epoch 4805, Loss: 0.08551639318466187, Final Batch Loss: 0.040853288024663925\n",
      "Epoch 4806, Loss: 0.011037329910323024, Final Batch Loss: 0.009191801771521568\n",
      "Epoch 4807, Loss: 0.0313873877748847, Final Batch Loss: 0.020178020000457764\n",
      "Epoch 4808, Loss: 0.015838917810469866, Final Batch Loss: 0.005196103360503912\n",
      "Epoch 4809, Loss: 0.05009859101846814, Final Batch Loss: 0.003970284480601549\n",
      "Epoch 4810, Loss: 0.03339143004268408, Final Batch Loss: 0.005305035971105099\n",
      "Epoch 4811, Loss: 0.10120492428541183, Final Batch Loss: 0.052637290209531784\n",
      "Epoch 4812, Loss: 0.02390252286568284, Final Batch Loss: 0.017006361857056618\n",
      "Epoch 4813, Loss: 0.03899255394935608, Final Batch Loss: 0.017377687618136406\n",
      "Epoch 4814, Loss: 0.14057697472162545, Final Batch Loss: 0.002388523193076253\n",
      "Epoch 4815, Loss: 0.080675283446908, Final Batch Loss: 0.022782159969210625\n",
      "Epoch 4816, Loss: 0.03269694885239005, Final Batch Loss: 0.00521507253870368\n",
      "Epoch 4817, Loss: 0.06246429122984409, Final Batch Loss: 0.019331326708197594\n",
      "Epoch 4818, Loss: 0.0643914183601737, Final Batch Loss: 0.010273232124745846\n",
      "Epoch 4819, Loss: 0.042208629194647074, Final Batch Loss: 0.007238606456667185\n",
      "Epoch 4820, Loss: 0.01975556043908, Final Batch Loss: 0.007208735216408968\n",
      "Epoch 4821, Loss: 0.039095318876206875, Final Batch Loss: 0.013685242272913456\n",
      "Epoch 4822, Loss: 0.019754525274038315, Final Batch Loss: 0.010747428052127361\n",
      "Epoch 4823, Loss: 0.04312014579772949, Final Batch Loss: 0.01582072302699089\n",
      "Epoch 4824, Loss: 0.0242910860106349, Final Batch Loss: 0.003665502183139324\n",
      "Epoch 4825, Loss: 0.014417917467653751, Final Batch Loss: 0.005215398035943508\n",
      "Epoch 4826, Loss: 0.028489221818745136, Final Batch Loss: 0.02070685103535652\n",
      "Epoch 4827, Loss: 0.005920378258451819, Final Batch Loss: 0.0020606578327715397\n",
      "Epoch 4828, Loss: 0.008798855822533369, Final Batch Loss: 0.005886547267436981\n",
      "Epoch 4829, Loss: 0.011177144246175885, Final Batch Loss: 0.0028892208356410265\n",
      "Epoch 4830, Loss: 0.010991170536726713, Final Batch Loss: 0.006763170938938856\n",
      "Epoch 4831, Loss: 0.020986177027225494, Final Batch Loss: 0.009518085047602654\n",
      "Epoch 4832, Loss: 0.01756544830277562, Final Batch Loss: 0.012378832325339317\n",
      "Epoch 4833, Loss: 0.017585802357643843, Final Batch Loss: 0.005017212126404047\n",
      "Epoch 4834, Loss: 0.08148408308625221, Final Batch Loss: 0.021860376000404358\n",
      "Epoch 4835, Loss: 0.034957761876285076, Final Batch Loss: 0.010626194067299366\n",
      "Epoch 4836, Loss: 0.053510235622525215, Final Batch Loss: 0.024609869346022606\n",
      "Epoch 4837, Loss: 0.008770460728555918, Final Batch Loss: 0.003453643526881933\n",
      "Epoch 4838, Loss: 0.015153610613197088, Final Batch Loss: 0.010922509245574474\n",
      "Epoch 4839, Loss: 0.07050660811364651, Final Batch Loss: 0.05569889023900032\n",
      "Epoch 4840, Loss: 0.02985250111669302, Final Batch Loss: 0.018027696758508682\n",
      "Epoch 4841, Loss: 0.024878522381186485, Final Batch Loss: 0.01305394433438778\n",
      "Epoch 4842, Loss: 0.02015057811513543, Final Batch Loss: 0.01608721725642681\n",
      "Epoch 4843, Loss: 0.01404885295778513, Final Batch Loss: 0.003825758583843708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4844, Loss: 0.016179464757442474, Final Batch Loss: 0.0057753510773181915\n",
      "Epoch 4845, Loss: 0.01162352692335844, Final Batch Loss: 0.00707958685234189\n",
      "Epoch 4846, Loss: 0.07094476197380573, Final Batch Loss: 0.00164565269369632\n",
      "Epoch 4847, Loss: 0.03703708900138736, Final Batch Loss: 0.029898526147007942\n",
      "Epoch 4848, Loss: 0.018672370817512274, Final Batch Loss: 0.0017302553169429302\n",
      "Epoch 4849, Loss: 0.03604961186647415, Final Batch Loss: 0.013528505340218544\n",
      "Epoch 4850, Loss: 0.01821946632117033, Final Batch Loss: 0.004183119162917137\n",
      "Epoch 4851, Loss: 0.011093729175627232, Final Batch Loss: 0.006621915381401777\n",
      "Epoch 4852, Loss: 0.017495765117928386, Final Batch Loss: 0.01505464781075716\n",
      "Epoch 4853, Loss: 0.011880476726219058, Final Batch Loss: 0.008817687630653381\n",
      "Epoch 4854, Loss: 0.007603705860674381, Final Batch Loss: 0.0022142278030514717\n",
      "Epoch 4855, Loss: 0.012303211726248264, Final Batch Loss: 0.0030105970799922943\n",
      "Epoch 4856, Loss: 0.03155473340302706, Final Batch Loss: 0.021315336227416992\n",
      "Epoch 4857, Loss: 0.017737353453412652, Final Batch Loss: 0.0016223883721977472\n",
      "Epoch 4858, Loss: 0.03303517191670835, Final Batch Loss: 0.0034204383846372366\n",
      "Epoch 4859, Loss: 0.06082241237163544, Final Batch Loss: 0.028263133019208908\n",
      "Epoch 4860, Loss: 0.009985166601836681, Final Batch Loss: 0.0045044454745948315\n",
      "Epoch 4861, Loss: 0.013021018821746111, Final Batch Loss: 0.009935693815350533\n",
      "Epoch 4862, Loss: 0.02661489648744464, Final Batch Loss: 0.006855910178273916\n",
      "Epoch 4863, Loss: 0.010238118004053831, Final Batch Loss: 0.004354740492999554\n",
      "Epoch 4864, Loss: 0.022242071572691202, Final Batch Loss: 0.014666554518043995\n",
      "Epoch 4865, Loss: 0.026033574948087335, Final Batch Loss: 0.0028296506498008966\n",
      "Epoch 4866, Loss: 0.013915842864662409, Final Batch Loss: 0.011780383065342903\n",
      "Epoch 4867, Loss: 0.009410888655111194, Final Batch Loss: 0.0035640366841107607\n",
      "Epoch 4868, Loss: 0.007118163863196969, Final Batch Loss: 0.0011654032859951258\n",
      "Epoch 4869, Loss: 0.012730080401524901, Final Batch Loss: 0.003623110009357333\n",
      "Epoch 4870, Loss: 0.0388008002191782, Final Batch Loss: 0.0341215580701828\n",
      "Epoch 4871, Loss: 0.02155158855021, Final Batch Loss: 0.007853960618376732\n",
      "Epoch 4872, Loss: 0.025711368769407272, Final Batch Loss: 0.008806394413113594\n",
      "Epoch 4873, Loss: 0.007907697465270758, Final Batch Loss: 0.0019867452792823315\n",
      "Epoch 4874, Loss: 0.006214934401214123, Final Batch Loss: 0.002619659062474966\n",
      "Epoch 4875, Loss: 0.010964824235998094, Final Batch Loss: 0.0016784857725724578\n",
      "Epoch 4876, Loss: 0.00953693431802094, Final Batch Loss: 0.0031532093416899443\n",
      "Epoch 4877, Loss: 0.008647310547530651, Final Batch Loss: 0.0029731309041380882\n",
      "Epoch 4878, Loss: 0.009462679037824273, Final Batch Loss: 0.0029606164898723364\n",
      "Epoch 4879, Loss: 0.02147449692711234, Final Batch Loss: 0.016125623136758804\n",
      "Epoch 4880, Loss: 0.005242150509729981, Final Batch Loss: 0.0026296686846762896\n",
      "Epoch 4881, Loss: 0.006993318675085902, Final Batch Loss: 0.0013505641836673021\n",
      "Epoch 4882, Loss: 0.006286620395258069, Final Batch Loss: 0.0030893192160874605\n",
      "Epoch 4883, Loss: 0.0037415881524793804, Final Batch Loss: 0.000916267919819802\n",
      "Epoch 4884, Loss: 0.004298124113120139, Final Batch Loss: 0.0029791430570185184\n",
      "Epoch 4885, Loss: 0.009091806365177035, Final Batch Loss: 0.0021480799186974764\n",
      "Epoch 4886, Loss: 0.0033675723243504763, Final Batch Loss: 0.001313242595642805\n",
      "Epoch 4887, Loss: 0.028696313267573714, Final Batch Loss: 0.002296615159139037\n",
      "Epoch 4888, Loss: 0.022331690648570657, Final Batch Loss: 0.0198859591037035\n",
      "Epoch 4889, Loss: 0.01585736428387463, Final Batch Loss: 0.0032998130191117525\n",
      "Epoch 4890, Loss: 0.07063462864607573, Final Batch Loss: 0.012732363305985928\n",
      "Epoch 4891, Loss: 0.012375677470117807, Final Batch Loss: 0.0023299786262214184\n",
      "Epoch 4892, Loss: 0.05293544940650463, Final Batch Loss: 0.03377416729927063\n",
      "Epoch 4893, Loss: 0.016245674807578325, Final Batch Loss: 0.005176322069019079\n",
      "Epoch 4894, Loss: 0.020277407951653004, Final Batch Loss: 0.005527466535568237\n",
      "Epoch 4895, Loss: 0.04328016098588705, Final Batch Loss: 0.03874124214053154\n",
      "Epoch 4896, Loss: 0.006671516690403223, Final Batch Loss: 0.0043457080610096455\n",
      "Epoch 4897, Loss: 0.02900185575708747, Final Batch Loss: 0.022696686908602715\n",
      "Epoch 4898, Loss: 0.09440410952083766, Final Batch Loss: 0.09149755537509918\n",
      "Epoch 4899, Loss: 0.07828584196977317, Final Batch Loss: 0.07574273645877838\n",
      "Epoch 4900, Loss: 0.22090391255915165, Final Batch Loss: 0.030959809198975563\n",
      "Epoch 4901, Loss: 0.07865064218640327, Final Batch Loss: 0.04682038351893425\n",
      "Epoch 4902, Loss: 0.08190963044762611, Final Batch Loss: 0.03366721794009209\n",
      "Epoch 4903, Loss: 0.06864479929208755, Final Batch Loss: 0.05982222035527229\n",
      "Epoch 4904, Loss: 0.04012934770435095, Final Batch Loss: 0.010562124662101269\n",
      "Epoch 4905, Loss: 0.01699997461400926, Final Batch Loss: 0.014582890085875988\n",
      "Epoch 4906, Loss: 0.029088910669088364, Final Batch Loss: 0.007067373022437096\n",
      "Epoch 4907, Loss: 0.01368018938228488, Final Batch Loss: 0.009102564305067062\n",
      "Epoch 4908, Loss: 0.05421667778864503, Final Batch Loss: 0.04886360093951225\n",
      "Epoch 4909, Loss: 0.10590355843305588, Final Batch Loss: 0.08854623883962631\n",
      "Epoch 4910, Loss: 0.04114143177866936, Final Batch Loss: 0.023595664650201797\n",
      "Epoch 4911, Loss: 0.04102782811969519, Final Batch Loss: 0.03062565065920353\n",
      "Epoch 4912, Loss: 0.0206089629791677, Final Batch Loss: 0.016291087493300438\n",
      "Epoch 4913, Loss: 0.02593380957841873, Final Batch Loss: 0.009466206654906273\n",
      "Epoch 4914, Loss: 0.08964302949607372, Final Batch Loss: 0.08044879138469696\n",
      "Epoch 4915, Loss: 0.01760018663480878, Final Batch Loss: 0.011260670609772205\n",
      "Epoch 4916, Loss: 0.032048864755779505, Final Batch Loss: 0.027594882994890213\n",
      "Epoch 4917, Loss: 0.025798017159104347, Final Batch Loss: 0.006810160353779793\n",
      "Epoch 4918, Loss: 0.0458508781157434, Final Batch Loss: 0.004805014934390783\n",
      "Epoch 4919, Loss: 0.03065075119957328, Final Batch Loss: 0.007569267880171537\n",
      "Epoch 4920, Loss: 0.02369933808222413, Final Batch Loss: 0.0042891246266663074\n",
      "Epoch 4921, Loss: 0.036978090181946754, Final Batch Loss: 0.013017158955335617\n",
      "Epoch 4922, Loss: 0.009571489877998829, Final Batch Loss: 0.004703786689788103\n",
      "Epoch 4923, Loss: 0.007606471190229058, Final Batch Loss: 0.0026930056046694517\n",
      "Epoch 4924, Loss: 0.020171612268313766, Final Batch Loss: 0.0165686197578907\n",
      "Epoch 4925, Loss: 0.03106547985225916, Final Batch Loss: 0.020609457045793533\n",
      "Epoch 4926, Loss: 0.03008247073739767, Final Batch Loss: 0.01698331907391548\n",
      "Epoch 4927, Loss: 0.027698594145476818, Final Batch Loss: 0.00963751319795847\n",
      "Epoch 4928, Loss: 0.02739123720675707, Final Batch Loss: 0.005220896564424038\n",
      "Epoch 4929, Loss: 0.03393223416060209, Final Batch Loss: 0.015087713487446308\n",
      "Epoch 4930, Loss: 0.020426016068086028, Final Batch Loss: 0.002341616665944457\n",
      "Epoch 4931, Loss: 0.05802363529801369, Final Batch Loss: 0.0401778519153595\n",
      "Epoch 4932, Loss: 0.01330310245975852, Final Batch Loss: 0.007218134123831987\n",
      "Epoch 4933, Loss: 0.006441034842282534, Final Batch Loss: 0.003176686353981495\n",
      "Epoch 4934, Loss: 0.02197507955133915, Final Batch Loss: 0.01448084693402052\n",
      "Epoch 4935, Loss: 0.043665217235684395, Final Batch Loss: 0.02885265275835991\n",
      "Epoch 4936, Loss: 0.021971495356410742, Final Batch Loss: 0.0058076954446733\n",
      "Epoch 4937, Loss: 0.04225894948467612, Final Batch Loss: 0.03828686848282814\n",
      "Epoch 4938, Loss: 0.028467507101595402, Final Batch Loss: 0.011300330050289631\n",
      "Epoch 4939, Loss: 0.0122255589812994, Final Batch Loss: 0.010058838874101639\n",
      "Epoch 4940, Loss: 0.020505913998931646, Final Batch Loss: 0.018183032050728798\n",
      "Epoch 4941, Loss: 0.01730598951689899, Final Batch Loss: 0.015035906806588173\n",
      "Epoch 4942, Loss: 0.01731789985205978, Final Batch Loss: 0.001724533154629171\n",
      "Epoch 4943, Loss: 0.03714086953550577, Final Batch Loss: 0.01410858053714037\n",
      "Epoch 4944, Loss: 0.023418091470375657, Final Batch Loss: 0.002713326597586274\n",
      "Epoch 4945, Loss: 0.010567052522674203, Final Batch Loss: 0.0031495795119553804\n",
      "Epoch 4946, Loss: 0.01801801985129714, Final Batch Loss: 0.014539201743900776\n",
      "Epoch 4947, Loss: 0.004252155544236302, Final Batch Loss: 0.002140897326171398\n",
      "Epoch 4948, Loss: 0.009092440363019705, Final Batch Loss: 0.00583401694893837\n",
      "Epoch 4949, Loss: 0.02738131955265999, Final Batch Loss: 0.01357041671872139\n",
      "Epoch 4950, Loss: 0.009354516165331006, Final Batch Loss: 0.0023042873945087194\n",
      "Epoch 4951, Loss: 0.02294530300423503, Final Batch Loss: 0.01712212711572647\n",
      "Epoch 4952, Loss: 0.04273068020120263, Final Batch Loss: 0.0376373790204525\n",
      "Epoch 4953, Loss: 0.0543357003480196, Final Batch Loss: 0.03654767945408821\n",
      "Epoch 4954, Loss: 0.008746276143938303, Final Batch Loss: 0.004335229285061359\n",
      "Epoch 4955, Loss: 0.01613094424828887, Final Batch Loss: 0.0047879633493721485\n",
      "Epoch 4956, Loss: 0.010917361360043287, Final Batch Loss: 0.006172582507133484\n",
      "Epoch 4957, Loss: 0.05847838753834367, Final Batch Loss: 0.006524863187223673\n",
      "Epoch 4958, Loss: 0.014051622012630105, Final Batch Loss: 0.011281577870249748\n",
      "Epoch 4959, Loss: 0.055231109261512756, Final Batch Loss: 0.018643300980329514\n",
      "Epoch 4960, Loss: 0.07573330588638783, Final Batch Loss: 0.06761898845434189\n",
      "Epoch 4961, Loss: 0.12897443771362305, Final Batch Loss: 0.09360287338495255\n",
      "Epoch 4962, Loss: 0.17016103118658066, Final Batch Loss: 0.05110374093055725\n",
      "Epoch 4963, Loss: 0.07234426029026508, Final Batch Loss: 0.05628250911831856\n",
      "Epoch 4964, Loss: 0.0999048724770546, Final Batch Loss: 0.04120316356420517\n",
      "Epoch 4965, Loss: 0.029275313019752502, Final Batch Loss: 0.015758464112877846\n",
      "Epoch 4966, Loss: 0.1347685568034649, Final Batch Loss: 0.06240331754088402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4967, Loss: 0.04475412517786026, Final Batch Loss: 0.02059166505932808\n",
      "Epoch 4968, Loss: 0.06786718219518661, Final Batch Loss: 0.05173718184232712\n",
      "Epoch 4969, Loss: 0.054748996160924435, Final Batch Loss: 0.04892411828041077\n",
      "Epoch 4970, Loss: 0.06857279501855373, Final Batch Loss: 0.03973695635795593\n",
      "Epoch 4971, Loss: 0.05079968087375164, Final Batch Loss: 0.01336967758834362\n",
      "Epoch 4972, Loss: 0.03657454252243042, Final Batch Loss: 0.015670843422412872\n",
      "Epoch 4973, Loss: 0.03190093953162432, Final Batch Loss: 0.0049051279202103615\n",
      "Epoch 4974, Loss: 0.028979696333408356, Final Batch Loss: 0.003946732729673386\n",
      "Epoch 4975, Loss: 0.028884625993669033, Final Batch Loss: 0.0021521979942917824\n",
      "Epoch 4976, Loss: 0.021472369320690632, Final Batch Loss: 0.006732812151312828\n",
      "Epoch 4977, Loss: 0.013217403553426266, Final Batch Loss: 0.007955926470458508\n",
      "Epoch 4978, Loss: 0.027043498121201992, Final Batch Loss: 0.020828645676374435\n",
      "Epoch 4979, Loss: 0.03483706689439714, Final Batch Loss: 0.0033116971608251333\n",
      "Epoch 4980, Loss: 0.05712296534329653, Final Batch Loss: 0.014990069903433323\n",
      "Epoch 4981, Loss: 0.053560206200927496, Final Batch Loss: 0.04771757125854492\n",
      "Epoch 4982, Loss: 0.03338597994297743, Final Batch Loss: 0.024467026814818382\n",
      "Epoch 4983, Loss: 0.004237496759742498, Final Batch Loss: 0.002840588102117181\n",
      "Epoch 4984, Loss: 0.013776607811450958, Final Batch Loss: 0.006051147356629372\n",
      "Epoch 4985, Loss: 0.019681286998093128, Final Batch Loss: 0.00822343211621046\n",
      "Epoch 4986, Loss: 0.026728529715910554, Final Batch Loss: 0.0028970318380743265\n",
      "Epoch 4987, Loss: 0.0474223718047142, Final Batch Loss: 0.041371334344148636\n",
      "Epoch 4988, Loss: 0.05789623595774174, Final Batch Loss: 0.047219231724739075\n",
      "Epoch 4989, Loss: 0.030740021727979183, Final Batch Loss: 0.004082183353602886\n",
      "Epoch 4990, Loss: 0.052057765424251556, Final Batch Loss: 0.018478766083717346\n",
      "Epoch 4991, Loss: 0.04596969950944185, Final Batch Loss: 0.035691626369953156\n",
      "Epoch 4992, Loss: 0.007311891298741102, Final Batch Loss: 0.003116240259259939\n",
      "Epoch 4993, Loss: 0.052503946237266064, Final Batch Loss: 0.048498447984457016\n",
      "Epoch 4994, Loss: 0.019011130090802908, Final Batch Loss: 0.005137198138982058\n",
      "Epoch 4995, Loss: 0.02937833871692419, Final Batch Loss: 0.014962071552872658\n",
      "Epoch 4996, Loss: 0.024442907888442278, Final Batch Loss: 0.006708382163196802\n",
      "Epoch 4997, Loss: 0.015865341760218143, Final Batch Loss: 0.00622422993183136\n",
      "Epoch 4998, Loss: 0.03366266284137964, Final Batch Loss: 0.024525102227926254\n",
      "Epoch 4999, Loss: 0.057378880213946104, Final Batch Loss: 0.006905231159180403\n",
      "Epoch 5000, Loss: 0.035729740746319294, Final Batch Loss: 0.025813525542616844\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  0  0]\n",
      " [ 0 25  0]\n",
      " [ 0  0 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        27\n",
      "           1      1.000     1.000     1.000        25\n",
      "           2      1.000     1.000     1.000        22\n",
      "\n",
      "    accuracy                          1.000        74\n",
      "   macro avg      1.000     1.000     1.000        74\n",
      "weighted avg      1.000     1.000     1.000        74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../saved_models/UCI 3 User Classifier Group 3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
