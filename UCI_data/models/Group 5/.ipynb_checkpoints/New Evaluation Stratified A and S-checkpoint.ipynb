{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [23, 25, 27]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.417480230331421, Final Batch Loss: 2.203195095062256\n",
      "Epoch 2, Loss: 4.4155333042144775, Final Batch Loss: 2.214625835418701\n",
      "Epoch 3, Loss: 4.410343408584595, Final Batch Loss: 2.1888551712036133\n",
      "Epoch 4, Loss: 4.402981758117676, Final Batch Loss: 2.189234972000122\n",
      "Epoch 5, Loss: 4.395047903060913, Final Batch Loss: 2.2000443935394287\n",
      "Epoch 6, Loss: 4.392462730407715, Final Batch Loss: 2.189796209335327\n",
      "Epoch 7, Loss: 4.3924713134765625, Final Batch Loss: 2.191723108291626\n",
      "Epoch 8, Loss: 4.386497497558594, Final Batch Loss: 2.1944785118103027\n",
      "Epoch 9, Loss: 4.378002882003784, Final Batch Loss: 2.1860768795013428\n",
      "Epoch 10, Loss: 4.372597932815552, Final Batch Loss: 2.187197685241699\n",
      "Epoch 11, Loss: 4.366967439651489, Final Batch Loss: 2.158565044403076\n",
      "Epoch 12, Loss: 4.362393617630005, Final Batch Loss: 2.18190598487854\n",
      "Epoch 13, Loss: 4.357719898223877, Final Batch Loss: 2.1699423789978027\n",
      "Epoch 14, Loss: 4.340387582778931, Final Batch Loss: 2.166548728942871\n",
      "Epoch 15, Loss: 4.335768938064575, Final Batch Loss: 2.156073570251465\n",
      "Epoch 16, Loss: 4.327857255935669, Final Batch Loss: 2.1711721420288086\n",
      "Epoch 17, Loss: 4.308269262313843, Final Batch Loss: 2.1403839588165283\n",
      "Epoch 18, Loss: 4.302649974822998, Final Batch Loss: 2.1594765186309814\n",
      "Epoch 19, Loss: 4.281061172485352, Final Batch Loss: 2.136962652206421\n",
      "Epoch 20, Loss: 4.2629714012146, Final Batch Loss: 2.127764940261841\n",
      "Epoch 21, Loss: 4.240600347518921, Final Batch Loss: 2.104642629623413\n",
      "Epoch 22, Loss: 4.227513551712036, Final Batch Loss: 2.1125354766845703\n",
      "Epoch 23, Loss: 4.1938910484313965, Final Batch Loss: 2.0932395458221436\n",
      "Epoch 24, Loss: 4.1559271812438965, Final Batch Loss: 2.08480167388916\n",
      "Epoch 25, Loss: 4.132643461227417, Final Batch Loss: 2.0692942142486572\n",
      "Epoch 26, Loss: 4.105259895324707, Final Batch Loss: 2.0668718814849854\n",
      "Epoch 27, Loss: 4.03827178478241, Final Batch Loss: 1.9691370725631714\n",
      "Epoch 28, Loss: 4.003926157951355, Final Batch Loss: 1.9987787008285522\n",
      "Epoch 29, Loss: 3.950062394142151, Final Batch Loss: 1.972050428390503\n",
      "Epoch 30, Loss: 3.90364146232605, Final Batch Loss: 1.9500398635864258\n",
      "Epoch 31, Loss: 3.8498032093048096, Final Batch Loss: 1.9226393699645996\n",
      "Epoch 32, Loss: 3.781030058860779, Final Batch Loss: 1.9016014337539673\n",
      "Epoch 33, Loss: 3.755250096321106, Final Batch Loss: 1.875736117362976\n",
      "Epoch 34, Loss: 3.681925654411316, Final Batch Loss: 1.8644897937774658\n",
      "Epoch 35, Loss: 3.6191576719284058, Final Batch Loss: 1.76019287109375\n",
      "Epoch 36, Loss: 3.5152478218078613, Final Batch Loss: 1.7257089614868164\n",
      "Epoch 37, Loss: 3.505333423614502, Final Batch Loss: 1.7186766862869263\n",
      "Epoch 38, Loss: 3.411846399307251, Final Batch Loss: 1.6747134923934937\n",
      "Epoch 39, Loss: 3.4244627952575684, Final Batch Loss: 1.6925220489501953\n",
      "Epoch 40, Loss: 3.321870446205139, Final Batch Loss: 1.701303482055664\n",
      "Epoch 41, Loss: 3.3146930932998657, Final Batch Loss: 1.6459964513778687\n",
      "Epoch 42, Loss: 3.316691279411316, Final Batch Loss: 1.6806070804595947\n",
      "Epoch 43, Loss: 3.2214406728744507, Final Batch Loss: 1.6393663883209229\n",
      "Epoch 44, Loss: 3.128393054008484, Final Batch Loss: 1.563099980354309\n",
      "Epoch 45, Loss: 3.142175078392029, Final Batch Loss: 1.5068647861480713\n",
      "Epoch 46, Loss: 3.141162872314453, Final Batch Loss: 1.5666135549545288\n",
      "Epoch 47, Loss: 3.099764823913574, Final Batch Loss: 1.5418567657470703\n",
      "Epoch 48, Loss: 2.9680198431015015, Final Batch Loss: 1.4662576913833618\n",
      "Epoch 49, Loss: 3.0251288414001465, Final Batch Loss: 1.5367244482040405\n",
      "Epoch 50, Loss: 2.9946634769439697, Final Batch Loss: 1.5279369354248047\n",
      "Epoch 51, Loss: 2.915831446647644, Final Batch Loss: 1.4320811033248901\n",
      "Epoch 52, Loss: 2.8941290378570557, Final Batch Loss: 1.4253818988800049\n",
      "Epoch 53, Loss: 2.812156915664673, Final Batch Loss: 1.3562886714935303\n",
      "Epoch 54, Loss: 2.8554937839508057, Final Batch Loss: 1.420506238937378\n",
      "Epoch 55, Loss: 2.922115921974182, Final Batch Loss: 1.4641340970993042\n",
      "Epoch 56, Loss: 2.770554542541504, Final Batch Loss: 1.4126927852630615\n",
      "Epoch 57, Loss: 2.772269129753113, Final Batch Loss: 1.3526705503463745\n",
      "Epoch 58, Loss: 2.7295161485671997, Final Batch Loss: 1.3392008543014526\n",
      "Epoch 59, Loss: 2.7054989337921143, Final Batch Loss: 1.3396084308624268\n",
      "Epoch 60, Loss: 2.752714514732361, Final Batch Loss: 1.3869044780731201\n",
      "Epoch 61, Loss: 2.691251516342163, Final Batch Loss: 1.3399245738983154\n",
      "Epoch 62, Loss: 2.635887861251831, Final Batch Loss: 1.3227616548538208\n",
      "Epoch 63, Loss: 2.6576846837997437, Final Batch Loss: 1.3238399028778076\n",
      "Epoch 64, Loss: 2.7182916402816772, Final Batch Loss: 1.391272783279419\n",
      "Epoch 65, Loss: 2.6748546361923218, Final Batch Loss: 1.350161075592041\n",
      "Epoch 66, Loss: 2.576966404914856, Final Batch Loss: 1.2917827367782593\n",
      "Epoch 67, Loss: 2.599403738975525, Final Batch Loss: 1.3026472330093384\n",
      "Epoch 68, Loss: 2.647585153579712, Final Batch Loss: 1.3083670139312744\n",
      "Epoch 69, Loss: 2.666896104812622, Final Batch Loss: 1.361019253730774\n",
      "Epoch 70, Loss: 2.647428274154663, Final Batch Loss: 1.3416873216629028\n",
      "Epoch 71, Loss: 2.5033780336380005, Final Batch Loss: 1.2484391927719116\n",
      "Epoch 72, Loss: 2.4958993196487427, Final Batch Loss: 1.2605394124984741\n",
      "Epoch 73, Loss: 2.546053647994995, Final Batch Loss: 1.2862638235092163\n",
      "Epoch 74, Loss: 2.4858816862106323, Final Batch Loss: 1.2472177743911743\n",
      "Epoch 75, Loss: 2.520631790161133, Final Batch Loss: 1.2294450998306274\n",
      "Epoch 76, Loss: 2.465532898902893, Final Batch Loss: 1.231481909751892\n",
      "Epoch 77, Loss: 2.4955400228500366, Final Batch Loss: 1.2055050134658813\n",
      "Epoch 78, Loss: 2.5105066299438477, Final Batch Loss: 1.2524112462997437\n",
      "Epoch 79, Loss: 2.462657332420349, Final Batch Loss: 1.2439827919006348\n",
      "Epoch 80, Loss: 2.407285213470459, Final Batch Loss: 1.1890778541564941\n",
      "Epoch 81, Loss: 2.4017995595932007, Final Batch Loss: 1.1929486989974976\n",
      "Epoch 82, Loss: 2.3712512254714966, Final Batch Loss: 1.143137812614441\n",
      "Epoch 83, Loss: 2.3972089290618896, Final Batch Loss: 1.19135320186615\n",
      "Epoch 84, Loss: 2.4579514265060425, Final Batch Loss: 1.2023355960845947\n",
      "Epoch 85, Loss: 2.442586898803711, Final Batch Loss: 1.2563353776931763\n",
      "Epoch 86, Loss: 2.2898815870285034, Final Batch Loss: 1.1470035314559937\n",
      "Epoch 87, Loss: 2.3784300088882446, Final Batch Loss: 1.174680233001709\n",
      "Epoch 88, Loss: 2.3115761280059814, Final Batch Loss: 1.175462245941162\n",
      "Epoch 89, Loss: 2.4569497108459473, Final Batch Loss: 1.2289366722106934\n",
      "Epoch 90, Loss: 2.370099425315857, Final Batch Loss: 1.1653387546539307\n",
      "Epoch 91, Loss: 2.2936148643493652, Final Batch Loss: 1.1403374671936035\n",
      "Epoch 92, Loss: 2.26186740398407, Final Batch Loss: 1.1328321695327759\n",
      "Epoch 93, Loss: 2.300890803337097, Final Batch Loss: 1.1575380563735962\n",
      "Epoch 94, Loss: 2.3096091747283936, Final Batch Loss: 1.1287392377853394\n",
      "Epoch 95, Loss: 2.308579921722412, Final Batch Loss: 1.1072041988372803\n",
      "Epoch 96, Loss: 2.320113778114319, Final Batch Loss: 1.128544807434082\n",
      "Epoch 97, Loss: 2.290811777114868, Final Batch Loss: 1.1528431177139282\n",
      "Epoch 98, Loss: 2.2676790952682495, Final Batch Loss: 1.1385544538497925\n",
      "Epoch 99, Loss: 2.34748637676239, Final Batch Loss: 1.1906832456588745\n",
      "Epoch 100, Loss: 2.298590064048767, Final Batch Loss: 1.1277209520339966\n",
      "Epoch 101, Loss: 2.268578052520752, Final Batch Loss: 1.1038432121276855\n",
      "Epoch 102, Loss: 2.1477328538894653, Final Batch Loss: 1.0561896562576294\n",
      "Epoch 103, Loss: 2.215332508087158, Final Batch Loss: 1.094894528388977\n",
      "Epoch 104, Loss: 2.268645763397217, Final Batch Loss: 1.1673872470855713\n",
      "Epoch 105, Loss: 2.2049185037612915, Final Batch Loss: 1.0898966789245605\n",
      "Epoch 106, Loss: 2.2134929895401, Final Batch Loss: 1.105156660079956\n",
      "Epoch 107, Loss: 2.2561336755752563, Final Batch Loss: 1.128171443939209\n",
      "Epoch 108, Loss: 2.189650297164917, Final Batch Loss: 1.1218234300613403\n",
      "Epoch 109, Loss: 2.191620349884033, Final Batch Loss: 1.0971277952194214\n",
      "Epoch 110, Loss: 2.180221438407898, Final Batch Loss: 1.0752990245819092\n",
      "Epoch 111, Loss: 2.124406099319458, Final Batch Loss: 1.026059627532959\n",
      "Epoch 112, Loss: 2.1399420499801636, Final Batch Loss: 1.0948524475097656\n",
      "Epoch 113, Loss: 2.2033275365829468, Final Batch Loss: 1.13181734085083\n",
      "Epoch 114, Loss: 2.21846342086792, Final Batch Loss: 1.0843755006790161\n",
      "Epoch 115, Loss: 2.1520618200302124, Final Batch Loss: 1.0614880323410034\n",
      "Epoch 116, Loss: 2.1750813722610474, Final Batch Loss: 1.0992380380630493\n",
      "Epoch 117, Loss: 2.10222327709198, Final Batch Loss: 1.0464365482330322\n",
      "Epoch 118, Loss: 2.17058002948761, Final Batch Loss: 1.0858418941497803\n",
      "Epoch 119, Loss: 2.102170944213867, Final Batch Loss: 1.0821547508239746\n",
      "Epoch 120, Loss: 2.1714857816696167, Final Batch Loss: 1.064635157585144\n",
      "Epoch 121, Loss: 2.0966020822525024, Final Batch Loss: 1.0234171152114868\n",
      "Epoch 122, Loss: 2.156820297241211, Final Batch Loss: 1.0701671838760376\n",
      "Epoch 123, Loss: 2.133072853088379, Final Batch Loss: 1.06754732131958\n",
      "Epoch 124, Loss: 2.202685236930847, Final Batch Loss: 1.1250884532928467\n",
      "Epoch 125, Loss: 2.096747040748596, Final Batch Loss: 1.0708290338516235\n",
      "Epoch 126, Loss: 2.0880777835845947, Final Batch Loss: 1.060410976409912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: 2.0656813383102417, Final Batch Loss: 0.9936341047286987\n",
      "Epoch 128, Loss: 2.101752281188965, Final Batch Loss: 1.0237901210784912\n",
      "Epoch 129, Loss: 2.0289372205734253, Final Batch Loss: 1.007084608078003\n",
      "Epoch 130, Loss: 2.110980272293091, Final Batch Loss: 1.0902070999145508\n",
      "Epoch 131, Loss: 2.1069328784942627, Final Batch Loss: 1.0688987970352173\n",
      "Epoch 132, Loss: 2.0414472818374634, Final Batch Loss: 1.0075476169586182\n",
      "Epoch 133, Loss: 2.0814130306243896, Final Batch Loss: 1.066582441329956\n",
      "Epoch 134, Loss: 2.093071937561035, Final Batch Loss: 1.050696611404419\n",
      "Epoch 135, Loss: 2.0031570196151733, Final Batch Loss: 0.9957283735275269\n",
      "Epoch 136, Loss: 2.065882444381714, Final Batch Loss: 1.0061935186386108\n",
      "Epoch 137, Loss: 2.0617074966430664, Final Batch Loss: 1.0420551300048828\n",
      "Epoch 138, Loss: 2.0017667412757874, Final Batch Loss: 0.9853231310844421\n",
      "Epoch 139, Loss: 2.0015493631362915, Final Batch Loss: 1.0143135786056519\n",
      "Epoch 140, Loss: 2.1015809774398804, Final Batch Loss: 1.0455478429794312\n",
      "Epoch 141, Loss: 2.085116386413574, Final Batch Loss: 1.0525895357131958\n",
      "Epoch 142, Loss: 2.054858922958374, Final Batch Loss: 1.0618702173233032\n",
      "Epoch 143, Loss: 2.029398202896118, Final Batch Loss: 1.0213221311569214\n",
      "Epoch 144, Loss: 1.9944472312927246, Final Batch Loss: 0.9920879602432251\n",
      "Epoch 145, Loss: 1.9989674091339111, Final Batch Loss: 0.996903657913208\n",
      "Epoch 146, Loss: 2.0261930227279663, Final Batch Loss: 1.029531717300415\n",
      "Epoch 147, Loss: 1.9631099700927734, Final Batch Loss: 1.004188895225525\n",
      "Epoch 148, Loss: 1.9722502827644348, Final Batch Loss: 0.9865606427192688\n",
      "Epoch 149, Loss: 1.968906819820404, Final Batch Loss: 0.9981411695480347\n",
      "Epoch 150, Loss: 1.9704284071922302, Final Batch Loss: 0.9730552434921265\n",
      "Epoch 151, Loss: 1.9759016036987305, Final Batch Loss: 1.0185048580169678\n",
      "Epoch 152, Loss: 1.9496292471885681, Final Batch Loss: 0.9474270939826965\n",
      "Epoch 153, Loss: 1.9551568627357483, Final Batch Loss: 0.9927911758422852\n",
      "Epoch 154, Loss: 1.9307671785354614, Final Batch Loss: 0.9193515777587891\n",
      "Epoch 155, Loss: 2.013242542743683, Final Batch Loss: 1.024295449256897\n",
      "Epoch 156, Loss: 1.9794647097587585, Final Batch Loss: 1.0182338953018188\n",
      "Epoch 157, Loss: 1.9671015739440918, Final Batch Loss: 0.9916187524795532\n",
      "Epoch 158, Loss: 1.9793826937675476, Final Batch Loss: 0.9907453656196594\n",
      "Epoch 159, Loss: 1.9894878268241882, Final Batch Loss: 1.0402429103851318\n",
      "Epoch 160, Loss: 2.001459062099457, Final Batch Loss: 1.0171072483062744\n",
      "Epoch 161, Loss: 1.98460453748703, Final Batch Loss: 1.0101995468139648\n",
      "Epoch 162, Loss: 1.9574124813079834, Final Batch Loss: 0.9741103649139404\n",
      "Epoch 163, Loss: 1.955049216747284, Final Batch Loss: 0.9841004014015198\n",
      "Epoch 164, Loss: 1.9283807277679443, Final Batch Loss: 0.9224008321762085\n",
      "Epoch 165, Loss: 1.9517517685890198, Final Batch Loss: 0.9329271912574768\n",
      "Epoch 166, Loss: 1.9276084303855896, Final Batch Loss: 0.9640580415725708\n",
      "Epoch 167, Loss: 1.903929352760315, Final Batch Loss: 0.9226111173629761\n",
      "Epoch 168, Loss: 1.9287446737289429, Final Batch Loss: 0.9980103969573975\n",
      "Epoch 169, Loss: 1.9358347654342651, Final Batch Loss: 0.96390700340271\n",
      "Epoch 170, Loss: 1.8910674452781677, Final Batch Loss: 0.9432478547096252\n",
      "Epoch 171, Loss: 1.9148882031440735, Final Batch Loss: 0.9621415138244629\n",
      "Epoch 172, Loss: 1.9212641716003418, Final Batch Loss: 0.9633688926696777\n",
      "Epoch 173, Loss: 1.8727497458457947, Final Batch Loss: 0.9504169225692749\n",
      "Epoch 174, Loss: 1.9838007092475891, Final Batch Loss: 0.9592387080192566\n",
      "Epoch 175, Loss: 1.9162888526916504, Final Batch Loss: 0.9374237656593323\n",
      "Epoch 176, Loss: 1.8676049709320068, Final Batch Loss: 0.9002982378005981\n",
      "Epoch 177, Loss: 1.9053402543067932, Final Batch Loss: 0.9588358998298645\n",
      "Epoch 178, Loss: 1.8421711325645447, Final Batch Loss: 0.9119328856468201\n",
      "Epoch 179, Loss: 1.8997209072113037, Final Batch Loss: 1.0100314617156982\n",
      "Epoch 180, Loss: 1.8809396028518677, Final Batch Loss: 0.8976093530654907\n",
      "Epoch 181, Loss: 1.906567633152008, Final Batch Loss: 0.9522201418876648\n",
      "Epoch 182, Loss: 1.8336840867996216, Final Batch Loss: 0.9163861274719238\n",
      "Epoch 183, Loss: 1.8458802103996277, Final Batch Loss: 0.9013146162033081\n",
      "Epoch 184, Loss: 1.809316098690033, Final Batch Loss: 0.8872405290603638\n",
      "Epoch 185, Loss: 1.8564112186431885, Final Batch Loss: 0.9216309189796448\n",
      "Epoch 186, Loss: 1.8178915977478027, Final Batch Loss: 0.9151856899261475\n",
      "Epoch 187, Loss: 1.8234765529632568, Final Batch Loss: 0.9247488379478455\n",
      "Epoch 188, Loss: 1.819368302822113, Final Batch Loss: 0.892273485660553\n",
      "Epoch 189, Loss: 1.806610882282257, Final Batch Loss: 0.9007933139801025\n",
      "Epoch 190, Loss: 1.8425132632255554, Final Batch Loss: 0.9553582072257996\n",
      "Epoch 191, Loss: 1.8038819432258606, Final Batch Loss: 0.8881921768188477\n",
      "Epoch 192, Loss: 1.8130003213882446, Final Batch Loss: 0.9096707105636597\n",
      "Epoch 193, Loss: 1.7799431085586548, Final Batch Loss: 0.8338145017623901\n",
      "Epoch 194, Loss: 1.7742564678192139, Final Batch Loss: 0.8783655166625977\n",
      "Epoch 195, Loss: 1.7978537678718567, Final Batch Loss: 0.8866636753082275\n",
      "Epoch 196, Loss: 1.7344673871994019, Final Batch Loss: 0.8596338629722595\n",
      "Epoch 197, Loss: 1.691582202911377, Final Batch Loss: 0.8474016785621643\n",
      "Epoch 198, Loss: 1.7345223426818848, Final Batch Loss: 0.8607500791549683\n",
      "Epoch 199, Loss: 1.7419710755348206, Final Batch Loss: 0.8702124357223511\n",
      "Epoch 200, Loss: 1.742168664932251, Final Batch Loss: 0.859390914440155\n",
      "Epoch 201, Loss: 1.7147108912467957, Final Batch Loss: 0.8370321989059448\n",
      "Epoch 202, Loss: 1.6968950033187866, Final Batch Loss: 0.8221092224121094\n",
      "Epoch 203, Loss: 1.6723508834838867, Final Batch Loss: 0.8373934626579285\n",
      "Epoch 204, Loss: 1.6431648135185242, Final Batch Loss: 0.8141438961029053\n",
      "Epoch 205, Loss: 1.6853851079940796, Final Batch Loss: 0.8518522381782532\n",
      "Epoch 206, Loss: 1.6689271926879883, Final Batch Loss: 0.8500125408172607\n",
      "Epoch 207, Loss: 1.5943142175674438, Final Batch Loss: 0.7816482782363892\n",
      "Epoch 208, Loss: 1.650486409664154, Final Batch Loss: 0.8697676062583923\n",
      "Epoch 209, Loss: 1.6222466826438904, Final Batch Loss: 0.8113915324211121\n",
      "Epoch 210, Loss: 1.6170977354049683, Final Batch Loss: 0.8065679669380188\n",
      "Epoch 211, Loss: 1.6666305661201477, Final Batch Loss: 0.8266462087631226\n",
      "Epoch 212, Loss: 1.614075779914856, Final Batch Loss: 0.8039278984069824\n",
      "Epoch 213, Loss: 1.6275579333305359, Final Batch Loss: 0.8191906809806824\n",
      "Epoch 214, Loss: 1.613798439502716, Final Batch Loss: 0.8206454515457153\n",
      "Epoch 215, Loss: 1.5967822074890137, Final Batch Loss: 0.7489657998085022\n",
      "Epoch 216, Loss: 1.541189968585968, Final Batch Loss: 0.7714473009109497\n",
      "Epoch 217, Loss: 1.4879828691482544, Final Batch Loss: 0.7559122443199158\n",
      "Epoch 218, Loss: 1.5382898449897766, Final Batch Loss: 0.7497461438179016\n",
      "Epoch 219, Loss: 1.4381315112113953, Final Batch Loss: 0.6665053367614746\n",
      "Epoch 220, Loss: 1.4927135705947876, Final Batch Loss: 0.7114551067352295\n",
      "Epoch 221, Loss: 1.455836832523346, Final Batch Loss: 0.7210280895233154\n",
      "Epoch 222, Loss: 1.5185307264328003, Final Batch Loss: 0.8170158863067627\n",
      "Epoch 223, Loss: 1.4742876291275024, Final Batch Loss: 0.7368953824043274\n",
      "Epoch 224, Loss: 1.471112608909607, Final Batch Loss: 0.7520473599433899\n",
      "Epoch 225, Loss: 1.4942535161972046, Final Batch Loss: 0.731184720993042\n",
      "Epoch 226, Loss: 1.4831881523132324, Final Batch Loss: 0.7334699630737305\n",
      "Epoch 227, Loss: 1.4803407192230225, Final Batch Loss: 0.7664902210235596\n",
      "Epoch 228, Loss: 1.5245710015296936, Final Batch Loss: 0.7964963316917419\n",
      "Epoch 229, Loss: 1.4003183245658875, Final Batch Loss: 0.7202210426330566\n",
      "Epoch 230, Loss: 1.4883073568344116, Final Batch Loss: 0.7161694765090942\n",
      "Epoch 231, Loss: 1.428187906742096, Final Batch Loss: 0.7116890549659729\n",
      "Epoch 232, Loss: 1.4154963493347168, Final Batch Loss: 0.7230722904205322\n",
      "Epoch 233, Loss: 1.4211608171463013, Final Batch Loss: 0.6665063500404358\n",
      "Epoch 234, Loss: 1.453431248664856, Final Batch Loss: 0.7032096982002258\n",
      "Epoch 235, Loss: 1.4819273948669434, Final Batch Loss: 0.7496389746665955\n",
      "Epoch 236, Loss: 1.4636138081550598, Final Batch Loss: 0.7593984603881836\n",
      "Epoch 237, Loss: 1.3519331216812134, Final Batch Loss: 0.7060537934303284\n",
      "Epoch 238, Loss: 1.3901566863059998, Final Batch Loss: 0.6773442029953003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239, Loss: 1.3689220547676086, Final Batch Loss: 0.6552038788795471\n",
      "Epoch 240, Loss: 1.3317305445671082, Final Batch Loss: 0.6627016067504883\n",
      "Epoch 241, Loss: 1.339768409729004, Final Batch Loss: 0.626476526260376\n",
      "Epoch 242, Loss: 1.379750669002533, Final Batch Loss: 0.6745806336402893\n",
      "Epoch 243, Loss: 1.4434533715248108, Final Batch Loss: 0.7930467128753662\n",
      "Epoch 244, Loss: 1.3598390817642212, Final Batch Loss: 0.711434006690979\n",
      "Epoch 245, Loss: 1.4444997906684875, Final Batch Loss: 0.6954709887504578\n",
      "Epoch 246, Loss: 1.3558059334754944, Final Batch Loss: 0.6632834076881409\n",
      "Epoch 247, Loss: 1.2709822058677673, Final Batch Loss: 0.6138315796852112\n",
      "Epoch 248, Loss: 1.3396541476249695, Final Batch Loss: 0.6736400723457336\n",
      "Epoch 249, Loss: 1.3123886585235596, Final Batch Loss: 0.6411944627761841\n",
      "Epoch 250, Loss: 1.3880318999290466, Final Batch Loss: 0.6797888875007629\n",
      "Epoch 251, Loss: 1.3222402930259705, Final Batch Loss: 0.6054357886314392\n",
      "Epoch 252, Loss: 1.2667637467384338, Final Batch Loss: 0.6055265665054321\n",
      "Epoch 253, Loss: 1.3619272112846375, Final Batch Loss: 0.6562548279762268\n",
      "Epoch 254, Loss: 1.375925600528717, Final Batch Loss: 0.6461533308029175\n",
      "Epoch 255, Loss: 1.3784341216087341, Final Batch Loss: 0.6409024596214294\n",
      "Epoch 256, Loss: 1.2568650245666504, Final Batch Loss: 0.623149573802948\n",
      "Epoch 257, Loss: 1.3561958074569702, Final Batch Loss: 0.6495158076286316\n",
      "Epoch 258, Loss: 1.2588691711425781, Final Batch Loss: 0.5954937934875488\n",
      "Epoch 259, Loss: 1.3556172847747803, Final Batch Loss: 0.6792559623718262\n",
      "Epoch 260, Loss: 1.2276655435562134, Final Batch Loss: 0.597226083278656\n",
      "Epoch 261, Loss: 1.3244121074676514, Final Batch Loss: 0.6213710904121399\n",
      "Epoch 262, Loss: 1.3081369996070862, Final Batch Loss: 0.6311601996421814\n",
      "Epoch 263, Loss: 1.382996916770935, Final Batch Loss: 0.6926801800727844\n",
      "Epoch 264, Loss: 1.2827712893486023, Final Batch Loss: 0.5907021760940552\n",
      "Epoch 265, Loss: 1.3081432580947876, Final Batch Loss: 0.6719498634338379\n",
      "Epoch 266, Loss: 1.3259721398353577, Final Batch Loss: 0.6520493626594543\n",
      "Epoch 267, Loss: 1.3036776781082153, Final Batch Loss: 0.6627481579780579\n",
      "Epoch 268, Loss: 1.3197213411331177, Final Batch Loss: 0.6742290258407593\n",
      "Epoch 269, Loss: 1.3201225996017456, Final Batch Loss: 0.6450958847999573\n",
      "Epoch 270, Loss: 1.3157020211219788, Final Batch Loss: 0.6862647533416748\n",
      "Epoch 271, Loss: 1.1728971600532532, Final Batch Loss: 0.6189571022987366\n",
      "Epoch 272, Loss: 1.2469446063041687, Final Batch Loss: 0.6447768211364746\n",
      "Epoch 273, Loss: 1.3310420513153076, Final Batch Loss: 0.7019132375717163\n",
      "Epoch 274, Loss: 1.2638044357299805, Final Batch Loss: 0.6013405323028564\n",
      "Epoch 275, Loss: 1.2336714267730713, Final Batch Loss: 0.623704731464386\n",
      "Epoch 276, Loss: 1.2422918677330017, Final Batch Loss: 0.6027150750160217\n",
      "Epoch 277, Loss: 1.2776658535003662, Final Batch Loss: 0.6572248935699463\n",
      "Epoch 278, Loss: 1.2726727724075317, Final Batch Loss: 0.6070560812950134\n",
      "Epoch 279, Loss: 1.2887145280838013, Final Batch Loss: 0.6592390537261963\n",
      "Epoch 280, Loss: 1.313498318195343, Final Batch Loss: 0.679631769657135\n",
      "Epoch 281, Loss: 1.2327577471733093, Final Batch Loss: 0.5738587379455566\n",
      "Epoch 282, Loss: 1.2110981941223145, Final Batch Loss: 0.5756060481071472\n",
      "Epoch 283, Loss: 1.2255223989486694, Final Batch Loss: 0.6145530939102173\n",
      "Epoch 284, Loss: 1.2492696046829224, Final Batch Loss: 0.6309443712234497\n",
      "Epoch 285, Loss: 1.3246591091156006, Final Batch Loss: 0.6541622281074524\n",
      "Epoch 286, Loss: 1.2389763593673706, Final Batch Loss: 0.6034315824508667\n",
      "Epoch 287, Loss: 1.2082490921020508, Final Batch Loss: 0.6698298454284668\n",
      "Epoch 288, Loss: 1.230717420578003, Final Batch Loss: 0.6510469913482666\n",
      "Epoch 289, Loss: 1.2006849646568298, Final Batch Loss: 0.6004003286361694\n",
      "Epoch 290, Loss: 1.2270941138267517, Final Batch Loss: 0.5558829307556152\n",
      "Epoch 291, Loss: 1.1538615226745605, Final Batch Loss: 0.5330368280410767\n",
      "Epoch 292, Loss: 1.1818047165870667, Final Batch Loss: 0.5486940145492554\n",
      "Epoch 293, Loss: 1.215972900390625, Final Batch Loss: 0.5595448613166809\n",
      "Epoch 294, Loss: 1.2427517175674438, Final Batch Loss: 0.5951240658760071\n",
      "Epoch 295, Loss: 1.168756127357483, Final Batch Loss: 0.5614107847213745\n",
      "Epoch 296, Loss: 1.252144455909729, Final Batch Loss: 0.6070197820663452\n",
      "Epoch 297, Loss: 1.180866777896881, Final Batch Loss: 0.6007888913154602\n",
      "Epoch 298, Loss: 1.1900426745414734, Final Batch Loss: 0.5878428220748901\n",
      "Epoch 299, Loss: 1.133016586303711, Final Batch Loss: 0.5686705112457275\n",
      "Epoch 300, Loss: 1.196674883365631, Final Batch Loss: 0.5861080288887024\n",
      "Epoch 301, Loss: 1.1943021416664124, Final Batch Loss: 0.6339529156684875\n",
      "Epoch 302, Loss: 1.221472978591919, Final Batch Loss: 0.5867735147476196\n",
      "Epoch 303, Loss: 1.178341805934906, Final Batch Loss: 0.5561681985855103\n",
      "Epoch 304, Loss: 1.199297845363617, Final Batch Loss: 0.6501412391662598\n",
      "Epoch 305, Loss: 1.1997272968292236, Final Batch Loss: 0.621349036693573\n",
      "Epoch 306, Loss: 1.1607596278190613, Final Batch Loss: 0.5512012839317322\n",
      "Epoch 307, Loss: 1.2028955221176147, Final Batch Loss: 0.6267197132110596\n",
      "Epoch 308, Loss: 1.1176100969314575, Final Batch Loss: 0.5758650898933411\n",
      "Epoch 309, Loss: 1.2153438925743103, Final Batch Loss: 0.6011269092559814\n",
      "Epoch 310, Loss: 1.1011967062950134, Final Batch Loss: 0.6105518937110901\n",
      "Epoch 311, Loss: 1.1741397380828857, Final Batch Loss: 0.5558276176452637\n",
      "Epoch 312, Loss: 1.1632742285728455, Final Batch Loss: 0.5870205760002136\n",
      "Epoch 313, Loss: 1.2332907915115356, Final Batch Loss: 0.6433423757553101\n",
      "Epoch 314, Loss: 1.216903567314148, Final Batch Loss: 0.5898360013961792\n",
      "Epoch 315, Loss: 1.194247543811798, Final Batch Loss: 0.596581757068634\n",
      "Epoch 316, Loss: 1.255128264427185, Final Batch Loss: 0.5817640423774719\n",
      "Epoch 317, Loss: 1.1981087923049927, Final Batch Loss: 0.6339673399925232\n",
      "Epoch 318, Loss: 1.2001210451126099, Final Batch Loss: 0.6664220094680786\n",
      "Epoch 319, Loss: 1.1344091296195984, Final Batch Loss: 0.540227472782135\n",
      "Epoch 320, Loss: 1.1293679475784302, Final Batch Loss: 0.5667562484741211\n",
      "Epoch 321, Loss: 1.2182061672210693, Final Batch Loss: 0.6142845153808594\n",
      "Epoch 322, Loss: 1.2306670546531677, Final Batch Loss: 0.570167064666748\n",
      "Epoch 323, Loss: 1.1890671253204346, Final Batch Loss: 0.5582762360572815\n",
      "Epoch 324, Loss: 1.1575916409492493, Final Batch Loss: 0.5408892631530762\n",
      "Epoch 325, Loss: 1.126117467880249, Final Batch Loss: 0.6053207516670227\n",
      "Epoch 326, Loss: 1.183969497680664, Final Batch Loss: 0.5629303455352783\n",
      "Epoch 327, Loss: 1.108424425125122, Final Batch Loss: 0.5309662818908691\n",
      "Epoch 328, Loss: 1.2230294346809387, Final Batch Loss: 0.6067624688148499\n",
      "Epoch 329, Loss: 1.0941227078437805, Final Batch Loss: 0.5471562147140503\n",
      "Epoch 330, Loss: 1.1170445084571838, Final Batch Loss: 0.5238363146781921\n",
      "Epoch 331, Loss: 1.2271934151649475, Final Batch Loss: 0.6302509903907776\n",
      "Epoch 332, Loss: 1.0511773228645325, Final Batch Loss: 0.5424638986587524\n",
      "Epoch 333, Loss: 1.1900086998939514, Final Batch Loss: 0.5890411138534546\n",
      "Epoch 334, Loss: 1.146957516670227, Final Batch Loss: 0.5715619325637817\n",
      "Epoch 335, Loss: 1.0695871114730835, Final Batch Loss: 0.5503620505332947\n",
      "Epoch 336, Loss: 1.2131539583206177, Final Batch Loss: 0.5770142674446106\n",
      "Epoch 337, Loss: 1.1229159832000732, Final Batch Loss: 0.5794972777366638\n",
      "Epoch 338, Loss: 1.1431790590286255, Final Batch Loss: 0.563541054725647\n",
      "Epoch 339, Loss: 1.1623838543891907, Final Batch Loss: 0.6004794239997864\n",
      "Epoch 340, Loss: 1.1351165771484375, Final Batch Loss: 0.5872039198875427\n",
      "Epoch 341, Loss: 1.103045105934143, Final Batch Loss: 0.5924492478370667\n",
      "Epoch 342, Loss: 1.101908266544342, Final Batch Loss: 0.5851851105690002\n",
      "Epoch 343, Loss: 1.156356692314148, Final Batch Loss: 0.6033790707588196\n",
      "Epoch 344, Loss: 1.1016852259635925, Final Batch Loss: 0.5404565930366516\n",
      "Epoch 345, Loss: 1.1576133370399475, Final Batch Loss: 0.6336362957954407\n",
      "Epoch 346, Loss: 1.101303219795227, Final Batch Loss: 0.5185437202453613\n",
      "Epoch 347, Loss: 1.2013619542121887, Final Batch Loss: 0.6392269730567932\n",
      "Epoch 348, Loss: 1.0709892511367798, Final Batch Loss: 0.5511772632598877\n",
      "Epoch 349, Loss: 1.0997014045715332, Final Batch Loss: 0.567389190196991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, Loss: 1.045074701309204, Final Batch Loss: 0.5305483937263489\n",
      "Epoch 351, Loss: 1.0662768483161926, Final Batch Loss: 0.5579493045806885\n",
      "Epoch 352, Loss: 1.097410798072815, Final Batch Loss: 0.5760346055030823\n",
      "Epoch 353, Loss: 1.087701678276062, Final Batch Loss: 0.5352252721786499\n",
      "Epoch 354, Loss: 1.021436870098114, Final Batch Loss: 0.50831139087677\n",
      "Epoch 355, Loss: 1.1161799430847168, Final Batch Loss: 0.5912328362464905\n",
      "Epoch 356, Loss: 1.1757263541221619, Final Batch Loss: 0.5586245656013489\n",
      "Epoch 357, Loss: 1.1771308779716492, Final Batch Loss: 0.6009085774421692\n",
      "Epoch 358, Loss: 1.1748434901237488, Final Batch Loss: 0.6184618473052979\n",
      "Epoch 359, Loss: 1.0931630730628967, Final Batch Loss: 0.5713986754417419\n",
      "Epoch 360, Loss: 1.0408830642700195, Final Batch Loss: 0.4773048162460327\n",
      "Epoch 361, Loss: 1.1155487298965454, Final Batch Loss: 0.5545969009399414\n",
      "Epoch 362, Loss: 1.11543208360672, Final Batch Loss: 0.5703341960906982\n",
      "Epoch 363, Loss: 1.1309600472450256, Final Batch Loss: 0.6016879081726074\n",
      "Epoch 364, Loss: 1.112113356590271, Final Batch Loss: 0.5760951042175293\n",
      "Epoch 365, Loss: 1.0847650170326233, Final Batch Loss: 0.5360767841339111\n",
      "Epoch 366, Loss: 1.1630873084068298, Final Batch Loss: 0.6016281247138977\n",
      "Epoch 367, Loss: 1.142349123954773, Final Batch Loss: 0.545534610748291\n",
      "Epoch 368, Loss: 1.1670717000961304, Final Batch Loss: 0.5711312294006348\n",
      "Epoch 369, Loss: 1.1008518636226654, Final Batch Loss: 0.4983406960964203\n",
      "Epoch 370, Loss: 1.1211005449295044, Final Batch Loss: 0.5781124234199524\n",
      "Epoch 371, Loss: 1.1484813690185547, Final Batch Loss: 0.548533022403717\n",
      "Epoch 372, Loss: 1.0352391004562378, Final Batch Loss: 0.5218566656112671\n",
      "Epoch 373, Loss: 1.12474125623703, Final Batch Loss: 0.5560569763183594\n",
      "Epoch 374, Loss: 1.0783477425575256, Final Batch Loss: 0.5099409222602844\n",
      "Epoch 375, Loss: 1.11882084608078, Final Batch Loss: 0.5456357598304749\n",
      "Epoch 376, Loss: 1.096623957157135, Final Batch Loss: 0.5309560894966125\n",
      "Epoch 377, Loss: 1.0586817860603333, Final Batch Loss: 0.5194773077964783\n",
      "Epoch 378, Loss: 1.2004939913749695, Final Batch Loss: 0.6341753602027893\n",
      "Epoch 379, Loss: 1.0783665776252747, Final Batch Loss: 0.5025295615196228\n",
      "Epoch 380, Loss: 1.1060269474983215, Final Batch Loss: 0.5411165952682495\n",
      "Epoch 381, Loss: 1.0883104801177979, Final Batch Loss: 0.5103442072868347\n",
      "Epoch 382, Loss: 1.0986112356185913, Final Batch Loss: 0.5189388990402222\n",
      "Epoch 383, Loss: 1.0826357007026672, Final Batch Loss: 0.5801836252212524\n",
      "Epoch 384, Loss: 1.076827049255371, Final Batch Loss: 0.5242004990577698\n",
      "Epoch 385, Loss: 1.1235488653182983, Final Batch Loss: 0.5686122179031372\n",
      "Epoch 386, Loss: 1.0631846189498901, Final Batch Loss: 0.5234236121177673\n",
      "Epoch 387, Loss: 1.0374892354011536, Final Batch Loss: 0.4885064959526062\n",
      "Epoch 388, Loss: 1.0612971186637878, Final Batch Loss: 0.48303037881851196\n",
      "Epoch 389, Loss: 1.0303486287593842, Final Batch Loss: 0.5553046464920044\n",
      "Epoch 390, Loss: 1.0267512798309326, Final Batch Loss: 0.46831172704696655\n",
      "Epoch 391, Loss: 1.0697013139724731, Final Batch Loss: 0.49988389015197754\n",
      "Epoch 392, Loss: 1.0494118332862854, Final Batch Loss: 0.5108835101127625\n",
      "Epoch 393, Loss: 1.1498876810073853, Final Batch Loss: 0.623961329460144\n",
      "Epoch 394, Loss: 0.9937674403190613, Final Batch Loss: 0.5098562836647034\n",
      "Epoch 395, Loss: 1.0692890584468842, Final Batch Loss: 0.5849223732948303\n",
      "Epoch 396, Loss: 1.1192204356193542, Final Batch Loss: 0.5490589141845703\n",
      "Epoch 397, Loss: 1.0569283962249756, Final Batch Loss: 0.5312989354133606\n",
      "Epoch 398, Loss: 1.0146260261535645, Final Batch Loss: 0.5271212458610535\n",
      "Epoch 399, Loss: 1.0424124002456665, Final Batch Loss: 0.5081673860549927\n",
      "Epoch 400, Loss: 1.0500394701957703, Final Batch Loss: 0.509807288646698\n",
      "Epoch 401, Loss: 1.0394495129585266, Final Batch Loss: 0.47411632537841797\n",
      "Epoch 402, Loss: 1.1471885442733765, Final Batch Loss: 0.5853908061981201\n",
      "Epoch 403, Loss: 1.1357925534248352, Final Batch Loss: 0.6106137633323669\n",
      "Epoch 404, Loss: 1.0399138629436493, Final Batch Loss: 0.5573750138282776\n",
      "Epoch 405, Loss: 1.0232810974121094, Final Batch Loss: 0.5121572017669678\n",
      "Epoch 406, Loss: 1.054263949394226, Final Batch Loss: 0.53269362449646\n",
      "Epoch 407, Loss: 1.0204238295555115, Final Batch Loss: 0.5453033447265625\n",
      "Epoch 408, Loss: 1.0612511038780212, Final Batch Loss: 0.5335595011711121\n",
      "Epoch 409, Loss: 1.0323526859283447, Final Batch Loss: 0.49594032764434814\n",
      "Epoch 410, Loss: 1.0394023060798645, Final Batch Loss: 0.5614645481109619\n",
      "Epoch 411, Loss: 1.0244580805301666, Final Batch Loss: 0.49347713589668274\n",
      "Epoch 412, Loss: 1.1016107201576233, Final Batch Loss: 0.5739683508872986\n",
      "Epoch 413, Loss: 1.0965989232063293, Final Batch Loss: 0.5460782051086426\n",
      "Epoch 414, Loss: 1.0381806790828705, Final Batch Loss: 0.5669532418251038\n",
      "Epoch 415, Loss: 0.9979235231876373, Final Batch Loss: 0.5124459266662598\n",
      "Epoch 416, Loss: 1.083422839641571, Final Batch Loss: 0.5784499645233154\n",
      "Epoch 417, Loss: 1.0175552666187286, Final Batch Loss: 0.4891130030155182\n",
      "Epoch 418, Loss: 1.0300166606903076, Final Batch Loss: 0.4832097291946411\n",
      "Epoch 419, Loss: 0.9832669794559479, Final Batch Loss: 0.5009360909461975\n",
      "Epoch 420, Loss: 1.0219803750514984, Final Batch Loss: 0.44957491755485535\n",
      "Epoch 421, Loss: 1.0718244314193726, Final Batch Loss: 0.5569807291030884\n",
      "Epoch 422, Loss: 1.0329433679580688, Final Batch Loss: 0.5150062441825867\n",
      "Epoch 423, Loss: 1.054008424282074, Final Batch Loss: 0.5453692674636841\n",
      "Epoch 424, Loss: 1.0063839554786682, Final Batch Loss: 0.5051314234733582\n",
      "Epoch 425, Loss: 1.013201355934143, Final Batch Loss: 0.5001915097236633\n",
      "Epoch 426, Loss: 1.0453132390975952, Final Batch Loss: 0.4817470908164978\n",
      "Epoch 427, Loss: 0.9906264543533325, Final Batch Loss: 0.512224555015564\n",
      "Epoch 428, Loss: 1.0094431340694427, Final Batch Loss: 0.5574735403060913\n",
      "Epoch 429, Loss: 1.0560143291950226, Final Batch Loss: 0.4624633491039276\n",
      "Epoch 430, Loss: 1.029496669769287, Final Batch Loss: 0.508034884929657\n",
      "Epoch 431, Loss: 1.0336390435695648, Final Batch Loss: 0.5460026860237122\n",
      "Epoch 432, Loss: 0.9770312011241913, Final Batch Loss: 0.47358372807502747\n",
      "Epoch 433, Loss: 1.0613440871238708, Final Batch Loss: 0.5665035843849182\n",
      "Epoch 434, Loss: 1.0220107436180115, Final Batch Loss: 0.5323718786239624\n",
      "Epoch 435, Loss: 1.0510579943656921, Final Batch Loss: 0.5312168002128601\n",
      "Epoch 436, Loss: 0.9864596426486969, Final Batch Loss: 0.47000929713249207\n",
      "Epoch 437, Loss: 1.0112779140472412, Final Batch Loss: 0.47930479049682617\n",
      "Epoch 438, Loss: 0.9992097020149231, Final Batch Loss: 0.5240588188171387\n",
      "Epoch 439, Loss: 0.9688040912151337, Final Batch Loss: 0.42118319869041443\n",
      "Epoch 440, Loss: 1.038916677236557, Final Batch Loss: 0.4979972541332245\n",
      "Epoch 441, Loss: 1.0081576704978943, Final Batch Loss: 0.5069113969802856\n",
      "Epoch 442, Loss: 1.0680699348449707, Final Batch Loss: 0.5418018102645874\n",
      "Epoch 443, Loss: 1.1037647724151611, Final Batch Loss: 0.5643896460533142\n",
      "Epoch 444, Loss: 0.9995786845684052, Final Batch Loss: 0.5018817186355591\n",
      "Epoch 445, Loss: 1.087266206741333, Final Batch Loss: 0.5469180941581726\n",
      "Epoch 446, Loss: 1.011338472366333, Final Batch Loss: 0.508099377155304\n",
      "Epoch 447, Loss: 0.9953697323799133, Final Batch Loss: 0.4706707000732422\n",
      "Epoch 448, Loss: 1.0676815807819366, Final Batch Loss: 0.5881330966949463\n",
      "Epoch 449, Loss: 1.0240966081619263, Final Batch Loss: 0.5175759196281433\n",
      "Epoch 450, Loss: 0.9846436977386475, Final Batch Loss: 0.45971977710723877\n",
      "Epoch 451, Loss: 1.0500125885009766, Final Batch Loss: 0.5071398019790649\n",
      "Epoch 452, Loss: 1.059934377670288, Final Batch Loss: 0.5268872380256653\n",
      "Epoch 453, Loss: 0.9681813716888428, Final Batch Loss: 0.46947115659713745\n",
      "Epoch 454, Loss: 0.9170300960540771, Final Batch Loss: 0.4419468939304352\n",
      "Epoch 455, Loss: 1.0452831983566284, Final Batch Loss: 0.53257155418396\n",
      "Epoch 456, Loss: 1.015647977590561, Final Batch Loss: 0.47126099467277527\n",
      "Epoch 457, Loss: 0.9963213205337524, Final Batch Loss: 0.526378333568573\n",
      "Epoch 458, Loss: 1.005216121673584, Final Batch Loss: 0.5068532824516296\n",
      "Epoch 459, Loss: 0.966464102268219, Final Batch Loss: 0.4469575881958008\n",
      "Epoch 460, Loss: 1.0336061716079712, Final Batch Loss: 0.48325157165527344\n",
      "Epoch 461, Loss: 1.0041343569755554, Final Batch Loss: 0.5210524201393127\n",
      "Epoch 462, Loss: 0.9569935202598572, Final Batch Loss: 0.49608170986175537\n",
      "Epoch 463, Loss: 1.008848875761032, Final Batch Loss: 0.5107651352882385\n",
      "Epoch 464, Loss: 1.0197186768054962, Final Batch Loss: 0.4788692891597748\n",
      "Epoch 465, Loss: 1.017649620771408, Final Batch Loss: 0.44486597180366516\n",
      "Epoch 466, Loss: 0.9585976600646973, Final Batch Loss: 0.446682333946228\n",
      "Epoch 467, Loss: 0.956610918045044, Final Batch Loss: 0.48312443494796753\n",
      "Epoch 468, Loss: 0.9677295088768005, Final Batch Loss: 0.4602043032646179\n",
      "Epoch 469, Loss: 1.0493552088737488, Final Batch Loss: 0.5259210467338562\n",
      "Epoch 470, Loss: 0.9625775814056396, Final Batch Loss: 0.4948788285255432\n",
      "Epoch 471, Loss: 0.9754016101360321, Final Batch Loss: 0.46083053946495056\n",
      "Epoch 472, Loss: 1.0110691487789154, Final Batch Loss: 0.49137046933174133\n",
      "Epoch 473, Loss: 0.9318754076957703, Final Batch Loss: 0.44176775217056274\n",
      "Epoch 474, Loss: 0.9688302874565125, Final Batch Loss: 0.433505654335022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475, Loss: 0.9678649008274078, Final Batch Loss: 0.46565255522727966\n",
      "Epoch 476, Loss: 0.9565528631210327, Final Batch Loss: 0.4426443576812744\n",
      "Epoch 477, Loss: 1.0456750690937042, Final Batch Loss: 0.48799750208854675\n",
      "Epoch 478, Loss: 1.0082806646823883, Final Batch Loss: 0.5316212773323059\n",
      "Epoch 479, Loss: 1.006066918373108, Final Batch Loss: 0.5073798298835754\n",
      "Epoch 480, Loss: 0.9936303198337555, Final Batch Loss: 0.524957537651062\n",
      "Epoch 481, Loss: 0.976846843957901, Final Batch Loss: 0.48921477794647217\n",
      "Epoch 482, Loss: 1.0118446350097656, Final Batch Loss: 0.5194612741470337\n",
      "Epoch 483, Loss: 1.0436438918113708, Final Batch Loss: 0.5208855271339417\n",
      "Epoch 484, Loss: 1.0604056417942047, Final Batch Loss: 0.5613831281661987\n",
      "Epoch 485, Loss: 0.9543739855289459, Final Batch Loss: 0.49662744998931885\n",
      "Epoch 486, Loss: 0.9880968332290649, Final Batch Loss: 0.5320672988891602\n",
      "Epoch 487, Loss: 1.0062196254730225, Final Batch Loss: 0.534880518913269\n",
      "Epoch 488, Loss: 1.0334275364875793, Final Batch Loss: 0.5241377949714661\n",
      "Epoch 489, Loss: 0.9637718498706818, Final Batch Loss: 0.5257740616798401\n",
      "Epoch 490, Loss: 0.9149501621723175, Final Batch Loss: 0.44017651677131653\n",
      "Epoch 491, Loss: 0.9639234840869904, Final Batch Loss: 0.4659683406352997\n",
      "Epoch 492, Loss: 0.9355285167694092, Final Batch Loss: 0.4680650234222412\n",
      "Epoch 493, Loss: 0.9896170496940613, Final Batch Loss: 0.5015670657157898\n",
      "Epoch 494, Loss: 1.0283189713954926, Final Batch Loss: 0.5416430234909058\n",
      "Epoch 495, Loss: 1.0000176131725311, Final Batch Loss: 0.5108686685562134\n",
      "Epoch 496, Loss: 1.0019252598285675, Final Batch Loss: 0.5372809171676636\n",
      "Epoch 497, Loss: 0.9820852279663086, Final Batch Loss: 0.5161880254745483\n",
      "Epoch 498, Loss: 0.9271606802940369, Final Batch Loss: 0.4920617938041687\n",
      "Epoch 499, Loss: 0.944316953420639, Final Batch Loss: 0.5039486289024353\n",
      "Epoch 500, Loss: 1.0468300580978394, Final Batch Loss: 0.5325928330421448\n",
      "Epoch 501, Loss: 0.9467400312423706, Final Batch Loss: 0.4251706600189209\n",
      "Epoch 502, Loss: 0.9266470670700073, Final Batch Loss: 0.47232791781425476\n",
      "Epoch 503, Loss: 0.96431964635849, Final Batch Loss: 0.45766323804855347\n",
      "Epoch 504, Loss: 0.9747723340988159, Final Batch Loss: 0.4786178469657898\n",
      "Epoch 505, Loss: 0.9962550103664398, Final Batch Loss: 0.4723258316516876\n",
      "Epoch 506, Loss: 0.9447261095046997, Final Batch Loss: 0.524742841720581\n",
      "Epoch 507, Loss: 0.9209686517715454, Final Batch Loss: 0.40816575288772583\n",
      "Epoch 508, Loss: 0.9799280166625977, Final Batch Loss: 0.44227099418640137\n",
      "Epoch 509, Loss: 0.9533819556236267, Final Batch Loss: 0.45813560485839844\n",
      "Epoch 510, Loss: 1.025873601436615, Final Batch Loss: 0.49914413690567017\n",
      "Epoch 511, Loss: 0.9225645661354065, Final Batch Loss: 0.4570585787296295\n",
      "Epoch 512, Loss: 0.9604015648365021, Final Batch Loss: 0.4842219650745392\n",
      "Epoch 513, Loss: 1.0441795587539673, Final Batch Loss: 0.5394583940505981\n",
      "Epoch 514, Loss: 0.9632553160190582, Final Batch Loss: 0.5308084487915039\n",
      "Epoch 515, Loss: 0.9626690149307251, Final Batch Loss: 0.5151322484016418\n",
      "Epoch 516, Loss: 0.9116611480712891, Final Batch Loss: 0.4290653467178345\n",
      "Epoch 517, Loss: 0.9238416254520416, Final Batch Loss: 0.37750741839408875\n",
      "Epoch 518, Loss: 0.8918431997299194, Final Batch Loss: 0.4449969232082367\n",
      "Epoch 519, Loss: 0.9504102766513824, Final Batch Loss: 0.48701411485671997\n",
      "Epoch 520, Loss: 0.9989655315876007, Final Batch Loss: 0.48156866431236267\n",
      "Epoch 521, Loss: 0.9516755938529968, Final Batch Loss: 0.485728919506073\n",
      "Epoch 522, Loss: 0.9342186450958252, Final Batch Loss: 0.43761059641838074\n",
      "Epoch 523, Loss: 0.9595881998538971, Final Batch Loss: 0.47239387035369873\n",
      "Epoch 524, Loss: 0.9992591142654419, Final Batch Loss: 0.5269567966461182\n",
      "Epoch 525, Loss: 0.9320721328258514, Final Batch Loss: 0.49472862482070923\n",
      "Epoch 526, Loss: 0.9935943186283112, Final Batch Loss: 0.4942663013935089\n",
      "Epoch 527, Loss: 0.9863195717334747, Final Batch Loss: 0.5095493197441101\n",
      "Epoch 528, Loss: 0.9887821078300476, Final Batch Loss: 0.5257799029350281\n",
      "Epoch 529, Loss: 0.9880291521549225, Final Batch Loss: 0.5108932256698608\n",
      "Epoch 530, Loss: 0.981259673833847, Final Batch Loss: 0.5126875042915344\n",
      "Epoch 531, Loss: 0.8841898143291473, Final Batch Loss: 0.45822009444236755\n",
      "Epoch 532, Loss: 0.9361131489276886, Final Batch Loss: 0.46090686321258545\n",
      "Epoch 533, Loss: 0.9649040997028351, Final Batch Loss: 0.456966370344162\n",
      "Epoch 534, Loss: 0.9975903034210205, Final Batch Loss: 0.5183836221694946\n",
      "Epoch 535, Loss: 0.9974826574325562, Final Batch Loss: 0.516350269317627\n",
      "Epoch 536, Loss: 0.9531148076057434, Final Batch Loss: 0.46255457401275635\n",
      "Epoch 537, Loss: 0.9741576015949249, Final Batch Loss: 0.5395598411560059\n",
      "Epoch 538, Loss: 0.965870201587677, Final Batch Loss: 0.5253105163574219\n",
      "Epoch 539, Loss: 0.9468929767608643, Final Batch Loss: 0.5020735263824463\n",
      "Epoch 540, Loss: 1.0165584981441498, Final Batch Loss: 0.55328768491745\n",
      "Epoch 541, Loss: 0.9174063801765442, Final Batch Loss: 0.4470338821411133\n",
      "Epoch 542, Loss: 0.9727510809898376, Final Batch Loss: 0.555306077003479\n",
      "Epoch 543, Loss: 0.9011532366275787, Final Batch Loss: 0.4490172266960144\n",
      "Epoch 544, Loss: 0.990463376045227, Final Batch Loss: 0.44392555952072144\n",
      "Epoch 545, Loss: 0.9448685348033905, Final Batch Loss: 0.4750816226005554\n",
      "Epoch 546, Loss: 0.9738864898681641, Final Batch Loss: 0.45473766326904297\n",
      "Epoch 547, Loss: 0.9262441396713257, Final Batch Loss: 0.43552953004837036\n",
      "Epoch 548, Loss: 0.980262279510498, Final Batch Loss: 0.4396204352378845\n",
      "Epoch 549, Loss: 0.9786720275878906, Final Batch Loss: 0.4974868595600128\n",
      "Epoch 550, Loss: 0.9611815512180328, Final Batch Loss: 0.5076270699501038\n",
      "Epoch 551, Loss: 0.9428791105747223, Final Batch Loss: 0.4754514694213867\n",
      "Epoch 552, Loss: 0.9359742105007172, Final Batch Loss: 0.4210977256298065\n",
      "Epoch 553, Loss: 0.9651467204093933, Final Batch Loss: 0.509311318397522\n",
      "Epoch 554, Loss: 0.9253217577934265, Final Batch Loss: 0.4325947165489197\n",
      "Epoch 555, Loss: 0.9663402140140533, Final Batch Loss: 0.4936738610267639\n",
      "Epoch 556, Loss: 0.9855227172374725, Final Batch Loss: 0.4790342152118683\n",
      "Epoch 557, Loss: 1.0215218663215637, Final Batch Loss: 0.5586991906166077\n",
      "Epoch 558, Loss: 0.8983640670776367, Final Batch Loss: 0.38987791538238525\n",
      "Epoch 559, Loss: 0.9440217912197113, Final Batch Loss: 0.48401162028312683\n",
      "Epoch 560, Loss: 0.9137415885925293, Final Batch Loss: 0.47918978333473206\n",
      "Epoch 561, Loss: 0.9042034447193146, Final Batch Loss: 0.45005401968955994\n",
      "Epoch 562, Loss: 0.9343198835849762, Final Batch Loss: 0.45995602011680603\n",
      "Epoch 563, Loss: 0.926285982131958, Final Batch Loss: 0.4655061364173889\n",
      "Epoch 564, Loss: 0.9876305460929871, Final Batch Loss: 0.4953995943069458\n",
      "Epoch 565, Loss: 0.8838061392307281, Final Batch Loss: 0.3860390782356262\n",
      "Epoch 566, Loss: 0.9459360539913177, Final Batch Loss: 0.4250243604183197\n",
      "Epoch 567, Loss: 0.93964684009552, Final Batch Loss: 0.46522799134254456\n",
      "Epoch 568, Loss: 0.8976688385009766, Final Batch Loss: 0.47886747121810913\n",
      "Epoch 569, Loss: 0.9874747097492218, Final Batch Loss: 0.5083533525466919\n",
      "Epoch 570, Loss: 0.9505323767662048, Final Batch Loss: 0.44015079736709595\n",
      "Epoch 571, Loss: 0.9207540154457092, Final Batch Loss: 0.5150852799415588\n",
      "Epoch 572, Loss: 0.9316672086715698, Final Batch Loss: 0.475006103515625\n",
      "Epoch 573, Loss: 0.9753294289112091, Final Batch Loss: 0.4787783622741699\n",
      "Epoch 574, Loss: 0.9366699159145355, Final Batch Loss: 0.47980931401252747\n",
      "Epoch 575, Loss: 0.9521974921226501, Final Batch Loss: 0.46809402108192444\n",
      "Epoch 576, Loss: 0.9209405183792114, Final Batch Loss: 0.44442886114120483\n",
      "Epoch 577, Loss: 0.8716334700584412, Final Batch Loss: 0.4601319432258606\n",
      "Epoch 578, Loss: 0.97634357213974, Final Batch Loss: 0.5026246905326843\n",
      "Epoch 579, Loss: 0.9332676827907562, Final Batch Loss: 0.4700362980365753\n",
      "Epoch 580, Loss: 0.9752551317214966, Final Batch Loss: 0.42912358045578003\n",
      "Epoch 581, Loss: 0.9162020981311798, Final Batch Loss: 0.43323367834091187\n",
      "Epoch 582, Loss: 0.9992969036102295, Final Batch Loss: 0.5005715489387512\n",
      "Epoch 583, Loss: 0.9040099084377289, Final Batch Loss: 0.447012335062027\n",
      "Epoch 584, Loss: 0.9211155772209167, Final Batch Loss: 0.4516533315181732\n",
      "Epoch 585, Loss: 0.9735393822193146, Final Batch Loss: 0.5262698531150818\n",
      "Epoch 586, Loss: 0.9777353703975677, Final Batch Loss: 0.5232513546943665\n",
      "Epoch 587, Loss: 1.005541980266571, Final Batch Loss: 0.5092765688896179\n",
      "Epoch 588, Loss: 0.9813267290592194, Final Batch Loss: 0.5045182704925537\n",
      "Epoch 589, Loss: 0.9070725440979004, Final Batch Loss: 0.4402719736099243\n",
      "Epoch 590, Loss: 0.9174216389656067, Final Batch Loss: 0.4710457921028137\n",
      "Epoch 591, Loss: 0.9827450215816498, Final Batch Loss: 0.48311832547187805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592, Loss: 0.9964599609375, Final Batch Loss: 0.5054903626441956\n",
      "Epoch 593, Loss: 0.9208477437496185, Final Batch Loss: 0.48669227957725525\n",
      "Epoch 594, Loss: 0.9415829479694366, Final Batch Loss: 0.49263638257980347\n",
      "Epoch 595, Loss: 0.8646248579025269, Final Batch Loss: 0.4146944582462311\n",
      "Epoch 596, Loss: 0.9470462501049042, Final Batch Loss: 0.5313454866409302\n",
      "Epoch 597, Loss: 0.944146066904068, Final Batch Loss: 0.45504269003868103\n",
      "Epoch 598, Loss: 0.9335022568702698, Final Batch Loss: 0.41171789169311523\n",
      "Epoch 599, Loss: 0.9511077105998993, Final Batch Loss: 0.5145350098609924\n",
      "Epoch 600, Loss: 0.9237573146820068, Final Batch Loss: 0.48407962918281555\n",
      "Epoch 601, Loss: 0.956861287355423, Final Batch Loss: 0.4631538689136505\n",
      "Epoch 602, Loss: 0.9213549494743347, Final Batch Loss: 0.4094812273979187\n",
      "Epoch 603, Loss: 0.8641766011714935, Final Batch Loss: 0.4139324426651001\n",
      "Epoch 604, Loss: 0.8805693686008453, Final Batch Loss: 0.42063117027282715\n",
      "Epoch 605, Loss: 0.9274593889713287, Final Batch Loss: 0.4838689863681793\n",
      "Epoch 606, Loss: 0.9956276416778564, Final Batch Loss: 0.5021688938140869\n",
      "Epoch 607, Loss: 0.909816563129425, Final Batch Loss: 0.4494353234767914\n",
      "Epoch 608, Loss: 0.9738230407238007, Final Batch Loss: 0.4761435389518738\n",
      "Epoch 609, Loss: 0.9534463882446289, Final Batch Loss: 0.47134262323379517\n",
      "Epoch 610, Loss: 0.9447605311870575, Final Batch Loss: 0.4455871284008026\n",
      "Epoch 611, Loss: 0.8723089098930359, Final Batch Loss: 0.4133148193359375\n",
      "Epoch 612, Loss: 0.8740317821502686, Final Batch Loss: 0.37186092138290405\n",
      "Epoch 613, Loss: 0.9001474976539612, Final Batch Loss: 0.46386784315109253\n",
      "Epoch 614, Loss: 0.9538000822067261, Final Batch Loss: 0.48041877150535583\n",
      "Epoch 615, Loss: 0.9400312006473541, Final Batch Loss: 0.43409958481788635\n",
      "Epoch 616, Loss: 0.9048173725605011, Final Batch Loss: 0.4246505796909332\n",
      "Epoch 617, Loss: 0.9631739854812622, Final Batch Loss: 0.4654865860939026\n",
      "Epoch 618, Loss: 0.9216532707214355, Final Batch Loss: 0.43840292096138\n",
      "Epoch 619, Loss: 0.9401423037052155, Final Batch Loss: 0.5220560431480408\n",
      "Epoch 620, Loss: 0.8513419926166534, Final Batch Loss: 0.4075736105442047\n",
      "Epoch 621, Loss: 0.9108029901981354, Final Batch Loss: 0.42129600048065186\n",
      "Epoch 622, Loss: 0.9131983518600464, Final Batch Loss: 0.4041675925254822\n",
      "Epoch 623, Loss: 0.9065321683883667, Final Batch Loss: 0.42102301120758057\n",
      "Epoch 624, Loss: 0.9221508800983429, Final Batch Loss: 0.43571287393569946\n",
      "Epoch 625, Loss: 0.9319310486316681, Final Batch Loss: 0.4504263997077942\n",
      "Epoch 626, Loss: 0.912989467382431, Final Batch Loss: 0.44958996772766113\n",
      "Epoch 627, Loss: 0.9926857352256775, Final Batch Loss: 0.4996810853481293\n",
      "Epoch 628, Loss: 0.9059917330741882, Final Batch Loss: 0.45876070857048035\n",
      "Epoch 629, Loss: 0.8683124780654907, Final Batch Loss: 0.42151448130607605\n",
      "Epoch 630, Loss: 0.8913314044475555, Final Batch Loss: 0.48374584317207336\n",
      "Epoch 631, Loss: 0.9289180636405945, Final Batch Loss: 0.4606853127479553\n",
      "Epoch 632, Loss: 0.8796877264976501, Final Batch Loss: 0.38731998205184937\n",
      "Epoch 633, Loss: 0.8862471580505371, Final Batch Loss: 0.3883261978626251\n",
      "Epoch 634, Loss: 0.9081631004810333, Final Batch Loss: 0.42671114206314087\n",
      "Epoch 635, Loss: 0.8901859521865845, Final Batch Loss: 0.45941662788391113\n",
      "Epoch 636, Loss: 0.8924326002597809, Final Batch Loss: 0.481466144323349\n",
      "Epoch 637, Loss: 0.9485237300395966, Final Batch Loss: 0.4687831401824951\n",
      "Epoch 638, Loss: 0.9129272997379303, Final Batch Loss: 0.4812055826187134\n",
      "Epoch 639, Loss: 0.8964363634586334, Final Batch Loss: 0.4467231333255768\n",
      "Epoch 640, Loss: 0.9132174551486969, Final Batch Loss: 0.426279753446579\n",
      "Epoch 641, Loss: 0.9639021754264832, Final Batch Loss: 0.5125427842140198\n",
      "Epoch 642, Loss: 0.8895243406295776, Final Batch Loss: 0.4648514688014984\n",
      "Epoch 643, Loss: 0.9565228223800659, Final Batch Loss: 0.45587295293807983\n",
      "Epoch 644, Loss: 0.9627601504325867, Final Batch Loss: 0.46362951397895813\n",
      "Epoch 645, Loss: 0.9821369051933289, Final Batch Loss: 0.4733397960662842\n",
      "Epoch 646, Loss: 0.8984789848327637, Final Batch Loss: 0.45553329586982727\n",
      "Epoch 647, Loss: 0.907638669013977, Final Batch Loss: 0.44768771529197693\n",
      "Epoch 648, Loss: 0.9645559787750244, Final Batch Loss: 0.45850807428359985\n",
      "Epoch 649, Loss: 0.9043970704078674, Final Batch Loss: 0.441287636756897\n",
      "Epoch 650, Loss: 0.9645622372627258, Final Batch Loss: 0.5036068558692932\n",
      "Epoch 651, Loss: 0.9611033201217651, Final Batch Loss: 0.490410178899765\n",
      "Epoch 652, Loss: 0.929186999797821, Final Batch Loss: 0.46385395526885986\n",
      "Epoch 653, Loss: 0.854939728975296, Final Batch Loss: 0.4094260334968567\n",
      "Epoch 654, Loss: 0.9216965436935425, Final Batch Loss: 0.4738694131374359\n",
      "Epoch 655, Loss: 0.8992815315723419, Final Batch Loss: 0.5054163932800293\n",
      "Epoch 656, Loss: 0.9184031188488007, Final Batch Loss: 0.48502427339553833\n",
      "Epoch 657, Loss: 0.8555381298065186, Final Batch Loss: 0.34741902351379395\n",
      "Epoch 658, Loss: 0.8710460364818573, Final Batch Loss: 0.40004676580429077\n",
      "Epoch 659, Loss: 0.9381594061851501, Final Batch Loss: 0.5061367154121399\n",
      "Epoch 660, Loss: 0.9446997046470642, Final Batch Loss: 0.3931252360343933\n",
      "Epoch 661, Loss: 0.9331835210323334, Final Batch Loss: 0.4759203791618347\n",
      "Epoch 662, Loss: 0.9170829653739929, Final Batch Loss: 0.49894779920578003\n",
      "Epoch 663, Loss: 0.9241607487201691, Final Batch Loss: 0.48100200295448303\n",
      "Epoch 664, Loss: 0.9732809364795685, Final Batch Loss: 0.4910673499107361\n",
      "Epoch 665, Loss: 0.8991931974887848, Final Batch Loss: 0.47029659152030945\n",
      "Epoch 666, Loss: 0.9280199408531189, Final Batch Loss: 0.4511796534061432\n",
      "Epoch 667, Loss: 0.9553318023681641, Final Batch Loss: 0.47522062063217163\n",
      "Epoch 668, Loss: 0.9319295585155487, Final Batch Loss: 0.49482932686805725\n",
      "Epoch 669, Loss: 0.8509942889213562, Final Batch Loss: 0.3940645158290863\n",
      "Epoch 670, Loss: 0.9190115630626678, Final Batch Loss: 0.43269291520118713\n",
      "Epoch 671, Loss: 0.8748873174190521, Final Batch Loss: 0.4530837833881378\n",
      "Epoch 672, Loss: 0.9193271398544312, Final Batch Loss: 0.4512574076652527\n",
      "Epoch 673, Loss: 0.9049819707870483, Final Batch Loss: 0.42005565762519836\n",
      "Epoch 674, Loss: 0.9787232577800751, Final Batch Loss: 0.5418314337730408\n",
      "Epoch 675, Loss: 0.8631808161735535, Final Batch Loss: 0.4562230706214905\n",
      "Epoch 676, Loss: 0.9507941007614136, Final Batch Loss: 0.48478105664253235\n",
      "Epoch 677, Loss: 0.9184631705284119, Final Batch Loss: 0.4587194323539734\n",
      "Epoch 678, Loss: 0.8777551651000977, Final Batch Loss: 0.38165101408958435\n",
      "Epoch 679, Loss: 0.8816590011119843, Final Batch Loss: 0.4113714098930359\n",
      "Epoch 680, Loss: 0.8946542739868164, Final Batch Loss: 0.47239479422569275\n",
      "Epoch 681, Loss: 0.955748975276947, Final Batch Loss: 0.5358361005783081\n",
      "Epoch 682, Loss: 0.884121835231781, Final Batch Loss: 0.46945855021476746\n",
      "Epoch 683, Loss: 0.8845440149307251, Final Batch Loss: 0.43179720640182495\n",
      "Epoch 684, Loss: 0.9504330158233643, Final Batch Loss: 0.49901801347732544\n",
      "Epoch 685, Loss: 0.9122109115123749, Final Batch Loss: 0.4739167094230652\n",
      "Epoch 686, Loss: 0.9206931591033936, Final Batch Loss: 0.45605334639549255\n",
      "Epoch 687, Loss: 0.8889504969120026, Final Batch Loss: 0.43221309781074524\n",
      "Epoch 688, Loss: 0.8957916498184204, Final Batch Loss: 0.4642975926399231\n",
      "Epoch 689, Loss: 0.8962717652320862, Final Batch Loss: 0.44994375109672546\n",
      "Epoch 690, Loss: 0.8638794422149658, Final Batch Loss: 0.41457274556159973\n",
      "Epoch 691, Loss: 0.8621242046356201, Final Batch Loss: 0.3851627707481384\n",
      "Epoch 692, Loss: 0.9060433804988861, Final Batch Loss: 0.40830525755882263\n",
      "Epoch 693, Loss: 0.8933746814727783, Final Batch Loss: 0.4366205930709839\n",
      "Epoch 694, Loss: 0.9061839580535889, Final Batch Loss: 0.4910150468349457\n",
      "Epoch 695, Loss: 0.9042057693004608, Final Batch Loss: 0.4696105420589447\n",
      "Epoch 696, Loss: 0.8994632661342621, Final Batch Loss: 0.46624332666397095\n",
      "Epoch 697, Loss: 0.8982201218605042, Final Batch Loss: 0.4154926538467407\n",
      "Epoch 698, Loss: 0.9068963825702667, Final Batch Loss: 0.4325641989707947\n",
      "Epoch 699, Loss: 0.8929111659526825, Final Batch Loss: 0.40092089772224426\n",
      "Epoch 700, Loss: 0.8353977203369141, Final Batch Loss: 0.4169212579727173\n",
      "Epoch 701, Loss: 0.8998968005180359, Final Batch Loss: 0.4656038284301758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 702, Loss: 0.8683772385120392, Final Batch Loss: 0.4397163987159729\n",
      "Epoch 703, Loss: 0.8957070708274841, Final Batch Loss: 0.44667476415634155\n",
      "Epoch 704, Loss: 0.9033637046813965, Final Batch Loss: 0.39502793550491333\n",
      "Epoch 705, Loss: 0.9270159006118774, Final Batch Loss: 0.4574538469314575\n",
      "Epoch 706, Loss: 0.9098430573940277, Final Batch Loss: 0.43897485733032227\n",
      "Epoch 707, Loss: 0.8951475024223328, Final Batch Loss: 0.43122023344039917\n",
      "Epoch 708, Loss: 0.8793597519397736, Final Batch Loss: 0.42478546500205994\n",
      "Epoch 709, Loss: 0.8486822545528412, Final Batch Loss: 0.42374199628829956\n",
      "Epoch 710, Loss: 0.9119121432304382, Final Batch Loss: 0.43450015783309937\n",
      "Epoch 711, Loss: 0.8840434551239014, Final Batch Loss: 0.4601157307624817\n",
      "Epoch 712, Loss: 0.9418776333332062, Final Batch Loss: 0.46260741353034973\n",
      "Epoch 713, Loss: 0.9324363768100739, Final Batch Loss: 0.4943886697292328\n",
      "Epoch 714, Loss: 0.8460571765899658, Final Batch Loss: 0.4281569719314575\n",
      "Epoch 715, Loss: 0.8986649215221405, Final Batch Loss: 0.41213661432266235\n",
      "Epoch 716, Loss: 0.8618145883083344, Final Batch Loss: 0.3771376311779022\n",
      "Epoch 717, Loss: 0.9217245280742645, Final Batch Loss: 0.5176769495010376\n",
      "Epoch 718, Loss: 0.8609176576137543, Final Batch Loss: 0.43610692024230957\n",
      "Epoch 719, Loss: 0.9266918003559113, Final Batch Loss: 0.452408105134964\n",
      "Epoch 720, Loss: 0.905214250087738, Final Batch Loss: 0.45416808128356934\n",
      "Epoch 721, Loss: 0.8672637343406677, Final Batch Loss: 0.4489782750606537\n",
      "Epoch 722, Loss: 0.9391885697841644, Final Batch Loss: 0.472734272480011\n",
      "Epoch 723, Loss: 0.9578688740730286, Final Batch Loss: 0.47168847918510437\n",
      "Epoch 724, Loss: 0.8921659588813782, Final Batch Loss: 0.38634324073791504\n",
      "Epoch 725, Loss: 0.8899440765380859, Final Batch Loss: 0.3882867693901062\n",
      "Epoch 726, Loss: 0.8885293304920197, Final Batch Loss: 0.41687077283859253\n",
      "Epoch 727, Loss: 0.8692471981048584, Final Batch Loss: 0.43341460824012756\n",
      "Epoch 728, Loss: 0.8517274856567383, Final Batch Loss: 0.4504076838493347\n",
      "Epoch 729, Loss: 0.9060358703136444, Final Batch Loss: 0.47881534695625305\n",
      "Epoch 730, Loss: 0.864342600107193, Final Batch Loss: 0.41684582829475403\n",
      "Epoch 731, Loss: 0.9557012319564819, Final Batch Loss: 0.46663957834243774\n",
      "Epoch 732, Loss: 0.9208373129367828, Final Batch Loss: 0.5150451064109802\n",
      "Epoch 733, Loss: 0.9357518255710602, Final Batch Loss: 0.49616512656211853\n",
      "Epoch 734, Loss: 0.8441244065761566, Final Batch Loss: 0.44356614351272583\n",
      "Epoch 735, Loss: 0.9000726640224457, Final Batch Loss: 0.509215235710144\n",
      "Epoch 736, Loss: 0.8734886050224304, Final Batch Loss: 0.47135233879089355\n",
      "Epoch 737, Loss: 0.8528253138065338, Final Batch Loss: 0.4066711366176605\n",
      "Epoch 738, Loss: 0.8842126429080963, Final Batch Loss: 0.39522209763526917\n",
      "Epoch 739, Loss: 0.89964759349823, Final Batch Loss: 0.46227580308914185\n",
      "Epoch 740, Loss: 0.9052712917327881, Final Batch Loss: 0.4559643566608429\n",
      "Epoch 741, Loss: 0.8928780853748322, Final Batch Loss: 0.38426652550697327\n",
      "Epoch 742, Loss: 0.8908637762069702, Final Batch Loss: 0.4546697437763214\n",
      "Epoch 743, Loss: 0.8691235184669495, Final Batch Loss: 0.44569429755210876\n",
      "Epoch 744, Loss: 0.8956376314163208, Final Batch Loss: 0.43872129917144775\n",
      "Epoch 745, Loss: 0.8466944992542267, Final Batch Loss: 0.42475593090057373\n",
      "Epoch 746, Loss: 0.8493092060089111, Final Batch Loss: 0.3794696033000946\n",
      "Epoch 747, Loss: 0.8808351457118988, Final Batch Loss: 0.47592949867248535\n",
      "Epoch 748, Loss: 0.9074910581111908, Final Batch Loss: 0.4983707368373871\n",
      "Epoch 749, Loss: 0.8845473229885101, Final Batch Loss: 0.49178189039230347\n",
      "Epoch 750, Loss: 0.897176057100296, Final Batch Loss: 0.459422767162323\n",
      "Epoch 751, Loss: 0.8945631384849548, Final Batch Loss: 0.48704349994659424\n",
      "Epoch 752, Loss: 0.9742178618907928, Final Batch Loss: 0.49300500750541687\n",
      "Epoch 753, Loss: 0.8926535248756409, Final Batch Loss: 0.5182518362998962\n",
      "Epoch 754, Loss: 0.9304190576076508, Final Batch Loss: 0.5024401545524597\n",
      "Epoch 755, Loss: 0.9399092495441437, Final Batch Loss: 0.4631012976169586\n",
      "Epoch 756, Loss: 0.9077644944190979, Final Batch Loss: 0.4712856113910675\n",
      "Epoch 757, Loss: 0.8930672109127045, Final Batch Loss: 0.4475724995136261\n",
      "Epoch 758, Loss: 0.9209178984165192, Final Batch Loss: 0.4393920600414276\n",
      "Epoch 759, Loss: 0.851269394159317, Final Batch Loss: 0.416864275932312\n",
      "Epoch 760, Loss: 0.8520711958408356, Final Batch Loss: 0.39916613698005676\n",
      "Epoch 761, Loss: 0.8315570950508118, Final Batch Loss: 0.371067076921463\n",
      "Epoch 762, Loss: 0.8702743351459503, Final Batch Loss: 0.41167789697647095\n",
      "Epoch 763, Loss: 0.8463334441184998, Final Batch Loss: 0.4204108715057373\n",
      "Epoch 764, Loss: 0.9320749044418335, Final Batch Loss: 0.5053642988204956\n",
      "Epoch 765, Loss: 0.8574636280536652, Final Batch Loss: 0.38127827644348145\n",
      "Epoch 766, Loss: 0.8294156491756439, Final Batch Loss: 0.41554537415504456\n",
      "Epoch 767, Loss: 0.8394157886505127, Final Batch Loss: 0.45785343647003174\n",
      "Epoch 768, Loss: 0.8407394886016846, Final Batch Loss: 0.3725886940956116\n",
      "Epoch 769, Loss: 0.9227761924266815, Final Batch Loss: 0.42644649744033813\n",
      "Epoch 770, Loss: 0.8870221674442291, Final Batch Loss: 0.45635339617729187\n",
      "Epoch 771, Loss: 0.8324894309043884, Final Batch Loss: 0.39495155215263367\n",
      "Epoch 772, Loss: 0.7979922294616699, Final Batch Loss: 0.36478590965270996\n",
      "Epoch 773, Loss: 0.8677087128162384, Final Batch Loss: 0.3792901337146759\n",
      "Epoch 774, Loss: 0.8223987519741058, Final Batch Loss: 0.42955121397972107\n",
      "Epoch 775, Loss: 0.8951922357082367, Final Batch Loss: 0.4665476083755493\n",
      "Epoch 776, Loss: 0.8874351680278778, Final Batch Loss: 0.41563156247138977\n",
      "Epoch 777, Loss: 0.8739343583583832, Final Batch Loss: 0.43654391169548035\n",
      "Epoch 778, Loss: 0.8533616960048676, Final Batch Loss: 0.42469993233680725\n",
      "Epoch 779, Loss: 0.9062498211860657, Final Batch Loss: 0.44932830333709717\n",
      "Epoch 780, Loss: 0.8570837080478668, Final Batch Loss: 0.41576364636421204\n",
      "Epoch 781, Loss: 0.8393230438232422, Final Batch Loss: 0.4371657967567444\n",
      "Epoch 782, Loss: 0.8543142080307007, Final Batch Loss: 0.4062817394733429\n",
      "Epoch 783, Loss: 0.8345266580581665, Final Batch Loss: 0.42212074995040894\n",
      "Epoch 784, Loss: 0.9029196202754974, Final Batch Loss: 0.4377170503139496\n",
      "Epoch 785, Loss: 0.8463307023048401, Final Batch Loss: 0.381457656621933\n",
      "Epoch 786, Loss: 0.870241641998291, Final Batch Loss: 0.39305171370506287\n",
      "Epoch 787, Loss: 0.871046245098114, Final Batch Loss: 0.3815814256668091\n",
      "Epoch 788, Loss: 0.870191752910614, Final Batch Loss: 0.4075614809989929\n",
      "Epoch 789, Loss: 0.877922773361206, Final Batch Loss: 0.4415508806705475\n",
      "Epoch 790, Loss: 0.9044325351715088, Final Batch Loss: 0.47233566641807556\n",
      "Epoch 791, Loss: 0.886307418346405, Final Batch Loss: 0.4233090877532959\n",
      "Epoch 792, Loss: 0.8644983172416687, Final Batch Loss: 0.4482572078704834\n",
      "Epoch 793, Loss: 0.8950594067573547, Final Batch Loss: 0.39393937587738037\n",
      "Epoch 794, Loss: 0.8996539413928986, Final Batch Loss: 0.4885913133621216\n",
      "Epoch 795, Loss: 0.8520965874195099, Final Batch Loss: 0.44843247532844543\n",
      "Epoch 796, Loss: 0.8886951208114624, Final Batch Loss: 0.4541640877723694\n",
      "Epoch 797, Loss: 0.8757846653461456, Final Batch Loss: 0.48756325244903564\n",
      "Epoch 798, Loss: 0.8389487266540527, Final Batch Loss: 0.40960508584976196\n",
      "Epoch 799, Loss: 0.8610968291759491, Final Batch Loss: 0.42948028445243835\n",
      "Epoch 800, Loss: 0.8490521311759949, Final Batch Loss: 0.4331740140914917\n",
      "Epoch 801, Loss: 0.8290062248706818, Final Batch Loss: 0.42126524448394775\n",
      "Epoch 802, Loss: 0.909333735704422, Final Batch Loss: 0.484535813331604\n",
      "Epoch 803, Loss: 0.8910587430000305, Final Batch Loss: 0.41785332560539246\n",
      "Epoch 804, Loss: 0.8518867194652557, Final Batch Loss: 0.4191574454307556\n",
      "Epoch 805, Loss: 0.8622961044311523, Final Batch Loss: 0.4401578903198242\n",
      "Epoch 806, Loss: 0.8546404540538788, Final Batch Loss: 0.4414350986480713\n",
      "Epoch 807, Loss: 0.8203265368938446, Final Batch Loss: 0.380318820476532\n",
      "Epoch 808, Loss: 0.8451803624629974, Final Batch Loss: 0.43134570121765137\n",
      "Epoch 809, Loss: 0.9084135591983795, Final Batch Loss: 0.45663461089134216\n",
      "Epoch 810, Loss: 0.8847062587738037, Final Batch Loss: 0.48609060049057007\n",
      "Epoch 811, Loss: 0.869397759437561, Final Batch Loss: 0.4079749584197998\n",
      "Epoch 812, Loss: 0.8346686363220215, Final Batch Loss: 0.43538451194763184\n",
      "Epoch 813, Loss: 0.895623117685318, Final Batch Loss: 0.4666561484336853\n",
      "Epoch 814, Loss: 0.8529196679592133, Final Batch Loss: 0.3802419900894165\n",
      "Epoch 815, Loss: 0.9037437438964844, Final Batch Loss: 0.4826383590698242\n",
      "Epoch 816, Loss: 0.8366871476173401, Final Batch Loss: 0.39392396807670593\n",
      "Epoch 817, Loss: 0.8366406559944153, Final Batch Loss: 0.4176369309425354\n",
      "Epoch 818, Loss: 0.8780051469802856, Final Batch Loss: 0.4162253439426422\n",
      "Epoch 819, Loss: 0.8486005961894989, Final Batch Loss: 0.44558247923851013\n",
      "Epoch 820, Loss: 0.8584182858467102, Final Batch Loss: 0.49268969893455505\n",
      "Epoch 821, Loss: 0.8446646332740784, Final Batch Loss: 0.41552969813346863\n",
      "Epoch 822, Loss: 0.8896364569664001, Final Batch Loss: 0.36695539951324463\n",
      "Epoch 823, Loss: 0.8603348731994629, Final Batch Loss: 0.446900337934494\n",
      "Epoch 824, Loss: 0.8166559636592865, Final Batch Loss: 0.38577568531036377\n",
      "Epoch 825, Loss: 0.8900924623012543, Final Batch Loss: 0.4569869637489319\n",
      "Epoch 826, Loss: 0.8325431942939758, Final Batch Loss: 0.4000733494758606\n",
      "Epoch 827, Loss: 0.8168470859527588, Final Batch Loss: 0.42518356442451477\n",
      "Epoch 828, Loss: 0.8614682555198669, Final Batch Loss: 0.3971284329891205\n",
      "Epoch 829, Loss: 0.8327379524707794, Final Batch Loss: 0.4039403200149536\n",
      "Epoch 830, Loss: 0.8557143211364746, Final Batch Loss: 0.3874165713787079\n",
      "Epoch 831, Loss: 0.8540477752685547, Final Batch Loss: 0.44133466482162476\n",
      "Epoch 832, Loss: 0.8889757394790649, Final Batch Loss: 0.4576907157897949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 833, Loss: 0.8914288878440857, Final Batch Loss: 0.43980395793914795\n",
      "Epoch 834, Loss: 0.8634907007217407, Final Batch Loss: 0.45396924018859863\n",
      "Epoch 835, Loss: 0.9424755275249481, Final Batch Loss: 0.5004832148551941\n",
      "Epoch 836, Loss: 0.854905515909195, Final Batch Loss: 0.4331631064414978\n",
      "Epoch 837, Loss: 0.8857055306434631, Final Batch Loss: 0.4825432598590851\n",
      "Epoch 838, Loss: 0.7998948395252228, Final Batch Loss: 0.37640324234962463\n",
      "Epoch 839, Loss: 0.8852412104606628, Final Batch Loss: 0.40659791231155396\n",
      "Epoch 840, Loss: 0.9106811583042145, Final Batch Loss: 0.4465855658054352\n",
      "Epoch 841, Loss: 0.8348996937274933, Final Batch Loss: 0.4098266065120697\n",
      "Epoch 842, Loss: 0.8383592963218689, Final Batch Loss: 0.38448649644851685\n",
      "Epoch 843, Loss: 0.89548259973526, Final Batch Loss: 0.483213871717453\n",
      "Epoch 844, Loss: 0.8906817436218262, Final Batch Loss: 0.42323774099349976\n",
      "Epoch 845, Loss: 0.808971494436264, Final Batch Loss: 0.43154072761535645\n",
      "Epoch 846, Loss: 0.8220786452293396, Final Batch Loss: 0.3761535584926605\n",
      "Epoch 847, Loss: 0.8657078146934509, Final Batch Loss: 0.4374731481075287\n",
      "Epoch 848, Loss: 0.8398455679416656, Final Batch Loss: 0.4128054678440094\n",
      "Epoch 849, Loss: 0.891024112701416, Final Batch Loss: 0.4696401357650757\n",
      "Epoch 850, Loss: 0.8614273965358734, Final Batch Loss: 0.4326619803905487\n",
      "Epoch 851, Loss: 0.8332237303256989, Final Batch Loss: 0.3928336203098297\n",
      "Epoch 852, Loss: 0.8092457056045532, Final Batch Loss: 0.3937554657459259\n",
      "Epoch 853, Loss: 0.8825635015964508, Final Batch Loss: 0.47642582654953003\n",
      "Epoch 854, Loss: 0.8328263163566589, Final Batch Loss: 0.4289918541908264\n",
      "Epoch 855, Loss: 0.7976151704788208, Final Batch Loss: 0.3871752619743347\n",
      "Epoch 856, Loss: 0.8822101354598999, Final Batch Loss: 0.45873188972473145\n",
      "Epoch 857, Loss: 0.8924858570098877, Final Batch Loss: 0.45117953419685364\n",
      "Epoch 858, Loss: 0.8327497243881226, Final Batch Loss: 0.39527231454849243\n",
      "Epoch 859, Loss: 0.8854182362556458, Final Batch Loss: 0.4542408287525177\n",
      "Epoch 860, Loss: 0.8323371410369873, Final Batch Loss: 0.3756878674030304\n",
      "Epoch 861, Loss: 0.824616938829422, Final Batch Loss: 0.3946494460105896\n",
      "Epoch 862, Loss: 0.7998839020729065, Final Batch Loss: 0.36889395117759705\n",
      "Epoch 863, Loss: 0.8805677592754364, Final Batch Loss: 0.46378961205482483\n",
      "Epoch 864, Loss: 0.8914322555065155, Final Batch Loss: 0.4335583746433258\n",
      "Epoch 865, Loss: 0.8642292022705078, Final Batch Loss: 0.389908105134964\n",
      "Epoch 866, Loss: 0.8553664684295654, Final Batch Loss: 0.41533637046813965\n",
      "Epoch 867, Loss: 0.8548958897590637, Final Batch Loss: 0.48063069581985474\n",
      "Epoch 868, Loss: 0.8529428243637085, Final Batch Loss: 0.36929067969322205\n",
      "Epoch 869, Loss: 0.8981902599334717, Final Batch Loss: 0.46009257435798645\n",
      "Epoch 870, Loss: 0.8171460926532745, Final Batch Loss: 0.41454917192459106\n",
      "Epoch 871, Loss: 0.8357822597026825, Final Batch Loss: 0.44990622997283936\n",
      "Epoch 872, Loss: 0.8280099034309387, Final Batch Loss: 0.41577979922294617\n",
      "Epoch 873, Loss: 0.8997332751750946, Final Batch Loss: 0.45006516575813293\n",
      "Epoch 874, Loss: 0.8630391359329224, Final Batch Loss: 0.4481383264064789\n",
      "Epoch 875, Loss: 0.8334582149982452, Final Batch Loss: 0.41345423460006714\n",
      "Epoch 876, Loss: 0.8354688584804535, Final Batch Loss: 0.42117124795913696\n",
      "Epoch 877, Loss: 0.892162024974823, Final Batch Loss: 0.41755786538124084\n",
      "Epoch 878, Loss: 0.8488571345806122, Final Batch Loss: 0.38934385776519775\n",
      "Epoch 879, Loss: 0.8195725679397583, Final Batch Loss: 0.4361579418182373\n",
      "Epoch 880, Loss: 0.8382297456264496, Final Batch Loss: 0.44362756609916687\n",
      "Epoch 881, Loss: 0.8071375489234924, Final Batch Loss: 0.4119197428226471\n",
      "Epoch 882, Loss: 0.8339285850524902, Final Batch Loss: 0.3997279703617096\n",
      "Epoch 883, Loss: 0.8985165059566498, Final Batch Loss: 0.45898908376693726\n",
      "Epoch 884, Loss: 0.8485503792762756, Final Batch Loss: 0.40574023127555847\n",
      "Epoch 885, Loss: 0.8304327726364136, Final Batch Loss: 0.46020761132240295\n",
      "Epoch 886, Loss: 0.8008276522159576, Final Batch Loss: 0.33664244413375854\n",
      "Epoch 887, Loss: 0.8383429646492004, Final Batch Loss: 0.37932252883911133\n",
      "Epoch 888, Loss: 0.8941264748573303, Final Batch Loss: 0.44327235221862793\n",
      "Epoch 889, Loss: 0.8362878859043121, Final Batch Loss: 0.405741810798645\n",
      "Epoch 890, Loss: 0.8386964797973633, Final Batch Loss: 0.40412619709968567\n",
      "Epoch 891, Loss: 0.8301166892051697, Final Batch Loss: 0.4158942699432373\n",
      "Epoch 892, Loss: 0.8124969899654388, Final Batch Loss: 0.3939775824546814\n",
      "Epoch 893, Loss: 0.83146733045578, Final Batch Loss: 0.42194175720214844\n",
      "Epoch 894, Loss: 0.7994681596755981, Final Batch Loss: 0.4083426594734192\n",
      "Epoch 895, Loss: 0.8271301984786987, Final Batch Loss: 0.38816478848457336\n",
      "Epoch 896, Loss: 0.863601416349411, Final Batch Loss: 0.437478631734848\n",
      "Epoch 897, Loss: 0.8554546535015106, Final Batch Loss: 0.39182353019714355\n",
      "Epoch 898, Loss: 0.8333496153354645, Final Batch Loss: 0.43685370683670044\n",
      "Epoch 899, Loss: 0.8506746590137482, Final Batch Loss: 0.42579618096351624\n",
      "Epoch 900, Loss: 0.8302391767501831, Final Batch Loss: 0.43094441294670105\n",
      "Epoch 901, Loss: 0.8532953858375549, Final Batch Loss: 0.40952828526496887\n",
      "Epoch 902, Loss: 0.7814950942993164, Final Batch Loss: 0.4124867618083954\n",
      "Epoch 903, Loss: 0.8607502579689026, Final Batch Loss: 0.40120425820350647\n",
      "Epoch 904, Loss: 0.8279882967472076, Final Batch Loss: 0.4380168318748474\n",
      "Epoch 905, Loss: 0.8140668869018555, Final Batch Loss: 0.4239319860935211\n",
      "Epoch 906, Loss: 0.8734306395053864, Final Batch Loss: 0.40202271938323975\n",
      "Epoch 907, Loss: 0.8498546183109283, Final Batch Loss: 0.48022669553756714\n",
      "Epoch 908, Loss: 0.8906941413879395, Final Batch Loss: 0.4471088647842407\n",
      "Epoch 909, Loss: 0.8504090011119843, Final Batch Loss: 0.4495978355407715\n",
      "Epoch 910, Loss: 0.8695833683013916, Final Batch Loss: 0.42440223693847656\n",
      "Epoch 911, Loss: 0.8757050335407257, Final Batch Loss: 0.4637795090675354\n",
      "Epoch 912, Loss: 0.8583961427211761, Final Batch Loss: 0.4186546802520752\n",
      "Epoch 913, Loss: 0.7978892028331757, Final Batch Loss: 0.4070729911327362\n",
      "Epoch 914, Loss: 0.84650719165802, Final Batch Loss: 0.4540306329727173\n",
      "Epoch 915, Loss: 0.8087217807769775, Final Batch Loss: 0.38546136021614075\n",
      "Epoch 916, Loss: 0.8528797328472137, Final Batch Loss: 0.4546334445476532\n",
      "Epoch 917, Loss: 0.8415778577327728, Final Batch Loss: 0.3859189450740814\n",
      "Epoch 918, Loss: 0.8231680393218994, Final Batch Loss: 0.40740400552749634\n",
      "Epoch 919, Loss: 0.8283781707286835, Final Batch Loss: 0.39299410581588745\n",
      "Epoch 920, Loss: 0.8578363656997681, Final Batch Loss: 0.3921532928943634\n",
      "Epoch 921, Loss: 0.8385004103183746, Final Batch Loss: 0.4160783290863037\n",
      "Epoch 922, Loss: 0.8155376017093658, Final Batch Loss: 0.40858301520347595\n",
      "Epoch 923, Loss: 0.8364825546741486, Final Batch Loss: 0.4171162247657776\n",
      "Epoch 924, Loss: 0.8248442709445953, Final Batch Loss: 0.3923466205596924\n",
      "Epoch 925, Loss: 0.8483019471168518, Final Batch Loss: 0.481627494096756\n",
      "Epoch 926, Loss: 0.8549126982688904, Final Batch Loss: 0.4284855127334595\n",
      "Epoch 927, Loss: 0.8422202467918396, Final Batch Loss: 0.4387635588645935\n",
      "Epoch 928, Loss: 0.8417691588401794, Final Batch Loss: 0.43591979146003723\n",
      "Epoch 929, Loss: 0.7922776341438293, Final Batch Loss: 0.402759850025177\n",
      "Epoch 930, Loss: 0.8353545069694519, Final Batch Loss: 0.46051836013793945\n",
      "Epoch 931, Loss: 0.8112226724624634, Final Batch Loss: 0.4088882803916931\n",
      "Epoch 932, Loss: 0.8406901955604553, Final Batch Loss: 0.41497430205345154\n",
      "Epoch 933, Loss: 0.7985561192035675, Final Batch Loss: 0.39288967847824097\n",
      "Epoch 934, Loss: 0.7814889252185822, Final Batch Loss: 0.35579121112823486\n",
      "Epoch 935, Loss: 0.8309920132160187, Final Batch Loss: 0.40774717926979065\n",
      "Epoch 936, Loss: 0.7735469937324524, Final Batch Loss: 0.35127225518226624\n",
      "Epoch 937, Loss: 0.7813319265842438, Final Batch Loss: 0.36379823088645935\n",
      "Epoch 938, Loss: 0.8374007046222687, Final Batch Loss: 0.3947247564792633\n",
      "Epoch 939, Loss: 0.8167648017406464, Final Batch Loss: 0.3986161947250366\n",
      "Epoch 940, Loss: 0.8228394687175751, Final Batch Loss: 0.4082466959953308\n",
      "Epoch 941, Loss: 0.8391343653202057, Final Batch Loss: 0.40785059332847595\n",
      "Epoch 942, Loss: 0.7949007153511047, Final Batch Loss: 0.39875486493110657\n",
      "Epoch 943, Loss: 0.8824571669101715, Final Batch Loss: 0.4372376799583435\n",
      "Epoch 944, Loss: 0.8206042945384979, Final Batch Loss: 0.4371429979801178\n",
      "Epoch 945, Loss: 0.8135777413845062, Final Batch Loss: 0.37902581691741943\n",
      "Epoch 946, Loss: 0.8406639397144318, Final Batch Loss: 0.38622137904167175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947, Loss: 0.8511689007282257, Final Batch Loss: 0.43692752718925476\n",
      "Epoch 948, Loss: 0.9012284874916077, Final Batch Loss: 0.45106109976768494\n",
      "Epoch 949, Loss: 0.7926271259784698, Final Batch Loss: 0.3997502028942108\n",
      "Epoch 950, Loss: 0.7854094505310059, Final Batch Loss: 0.37706154584884644\n",
      "Epoch 951, Loss: 0.8242017924785614, Final Batch Loss: 0.41606706380844116\n",
      "Epoch 952, Loss: 0.7830191552639008, Final Batch Loss: 0.3961077928543091\n",
      "Epoch 953, Loss: 0.8034426271915436, Final Batch Loss: 0.39187783002853394\n",
      "Epoch 954, Loss: 0.8605064153671265, Final Batch Loss: 0.4704357385635376\n",
      "Epoch 955, Loss: 0.786747395992279, Final Batch Loss: 0.39382174611091614\n",
      "Epoch 956, Loss: 0.8270225524902344, Final Batch Loss: 0.36690545082092285\n",
      "Epoch 957, Loss: 0.8089015185832977, Final Batch Loss: 0.3928777575492859\n",
      "Epoch 958, Loss: 0.7867761850357056, Final Batch Loss: 0.3605116903781891\n",
      "Epoch 959, Loss: 0.8140745162963867, Final Batch Loss: 0.4058776795864105\n",
      "Epoch 960, Loss: 0.8053479790687561, Final Batch Loss: 0.3668645918369293\n",
      "Epoch 961, Loss: 0.8224842250347137, Final Batch Loss: 0.4198421835899353\n",
      "Epoch 962, Loss: 0.8058570325374603, Final Batch Loss: 0.41113701462745667\n",
      "Epoch 963, Loss: 0.884720504283905, Final Batch Loss: 0.4078017473220825\n",
      "Epoch 964, Loss: 0.841496080160141, Final Batch Loss: 0.42307496070861816\n",
      "Epoch 965, Loss: 0.8207257390022278, Final Batch Loss: 0.40601226687431335\n",
      "Epoch 966, Loss: 0.8160824477672577, Final Batch Loss: 0.3749690353870392\n",
      "Epoch 967, Loss: 0.845542848110199, Final Batch Loss: 0.41129568219184875\n",
      "Epoch 968, Loss: 0.8084515035152435, Final Batch Loss: 0.41105973720550537\n",
      "Epoch 969, Loss: 0.8537572622299194, Final Batch Loss: 0.43607333302497864\n",
      "Epoch 970, Loss: 0.806599348783493, Final Batch Loss: 0.39683109521865845\n",
      "Epoch 971, Loss: 0.8067773282527924, Final Batch Loss: 0.4664027988910675\n",
      "Epoch 972, Loss: 0.8130303621292114, Final Batch Loss: 0.35090261697769165\n",
      "Epoch 973, Loss: 0.7538506090641022, Final Batch Loss: 0.3103574216365814\n",
      "Epoch 974, Loss: 0.8171719908714294, Final Batch Loss: 0.42133286595344543\n",
      "Epoch 975, Loss: 0.8071355819702148, Final Batch Loss: 0.3605315685272217\n",
      "Epoch 976, Loss: 0.8742379546165466, Final Batch Loss: 0.3908558189868927\n",
      "Epoch 977, Loss: 0.8201550543308258, Final Batch Loss: 0.41983702778816223\n",
      "Epoch 978, Loss: 0.8604591190814972, Final Batch Loss: 0.46179449558258057\n",
      "Epoch 979, Loss: 0.8391919732093811, Final Batch Loss: 0.40745019912719727\n",
      "Epoch 980, Loss: 0.8316170871257782, Final Batch Loss: 0.41624680161476135\n",
      "Epoch 981, Loss: 0.8080389499664307, Final Batch Loss: 0.45990538597106934\n",
      "Epoch 982, Loss: 0.8012730479240417, Final Batch Loss: 0.34814831614494324\n",
      "Epoch 983, Loss: 0.8421173095703125, Final Batch Loss: 0.48374292254447937\n",
      "Epoch 984, Loss: 0.8601058125495911, Final Batch Loss: 0.45107942819595337\n",
      "Epoch 985, Loss: 0.7728345990180969, Final Batch Loss: 0.33722156286239624\n",
      "Epoch 986, Loss: 0.8283692598342896, Final Batch Loss: 0.3871418237686157\n",
      "Epoch 987, Loss: 0.8128760457038879, Final Batch Loss: 0.39290550351142883\n",
      "Epoch 988, Loss: 0.8158740699291229, Final Batch Loss: 0.4188591241836548\n",
      "Epoch 989, Loss: 0.9172492623329163, Final Batch Loss: 0.46323347091674805\n",
      "Epoch 990, Loss: 0.805997759103775, Final Batch Loss: 0.4089309871196747\n",
      "Epoch 991, Loss: 0.7838943600654602, Final Batch Loss: 0.3878360688686371\n",
      "Epoch 992, Loss: 0.7661901116371155, Final Batch Loss: 0.3860355019569397\n",
      "Epoch 993, Loss: 0.8662598133087158, Final Batch Loss: 0.4583407938480377\n",
      "Epoch 994, Loss: 0.8608202636241913, Final Batch Loss: 0.45233699679374695\n",
      "Epoch 995, Loss: 0.9020143151283264, Final Batch Loss: 0.44925618171691895\n",
      "Epoch 996, Loss: 0.7871312499046326, Final Batch Loss: 0.37724417448043823\n",
      "Epoch 997, Loss: 0.8103555738925934, Final Batch Loss: 0.3598102331161499\n",
      "Epoch 998, Loss: 0.8291284143924713, Final Batch Loss: 0.46134358644485474\n",
      "Epoch 999, Loss: 0.8199810981750488, Final Batch Loss: 0.38057610392570496\n",
      "Epoch 1000, Loss: 0.8513137698173523, Final Batch Loss: 0.4231317639350891\n",
      "Epoch 1001, Loss: 0.809890866279602, Final Batch Loss: 0.38118797540664673\n",
      "Epoch 1002, Loss: 0.805370956659317, Final Batch Loss: 0.44501110911369324\n",
      "Epoch 1003, Loss: 0.794957309961319, Final Batch Loss: 0.3648746609687805\n",
      "Epoch 1004, Loss: 0.7877602577209473, Final Batch Loss: 0.34863272309303284\n",
      "Epoch 1005, Loss: 0.8297203779220581, Final Batch Loss: 0.4659406840801239\n",
      "Epoch 1006, Loss: 0.7849819958209991, Final Batch Loss: 0.36400410532951355\n",
      "Epoch 1007, Loss: 0.8175649046897888, Final Batch Loss: 0.4046502411365509\n",
      "Epoch 1008, Loss: 0.8541270196437836, Final Batch Loss: 0.44546088576316833\n",
      "Epoch 1009, Loss: 0.841991126537323, Final Batch Loss: 0.43125492334365845\n",
      "Epoch 1010, Loss: 0.8133331835269928, Final Batch Loss: 0.34059983491897583\n",
      "Epoch 1011, Loss: 0.7889823317527771, Final Batch Loss: 0.4036235511302948\n",
      "Epoch 1012, Loss: 0.8789396286010742, Final Batch Loss: 0.43623489141464233\n",
      "Epoch 1013, Loss: 0.8335906565189362, Final Batch Loss: 0.40214642882347107\n",
      "Epoch 1014, Loss: 0.860031396150589, Final Batch Loss: 0.40377622842788696\n",
      "Epoch 1015, Loss: 0.846179336309433, Final Batch Loss: 0.4344382584095001\n",
      "Epoch 1016, Loss: 0.827764481306076, Final Batch Loss: 0.393174946308136\n",
      "Epoch 1017, Loss: 0.8113051652908325, Final Batch Loss: 0.4034687280654907\n",
      "Epoch 1018, Loss: 0.7855595648288727, Final Batch Loss: 0.40157264471054077\n",
      "Epoch 1019, Loss: 0.9140307605266571, Final Batch Loss: 0.46325069665908813\n",
      "Epoch 1020, Loss: 0.8202199935913086, Final Batch Loss: 0.45120304822921753\n",
      "Epoch 1021, Loss: 0.8339466154575348, Final Batch Loss: 0.43848535418510437\n",
      "Epoch 1022, Loss: 0.821873664855957, Final Batch Loss: 0.44989994168281555\n",
      "Epoch 1023, Loss: 0.8312249183654785, Final Batch Loss: 0.4118390679359436\n",
      "Epoch 1024, Loss: 0.7783711850643158, Final Batch Loss: 0.3339129388332367\n",
      "Epoch 1025, Loss: 0.886493057012558, Final Batch Loss: 0.45667046308517456\n",
      "Epoch 1026, Loss: 0.8082617223262787, Final Batch Loss: 0.43422698974609375\n",
      "Epoch 1027, Loss: 0.8259897828102112, Final Batch Loss: 0.42312026023864746\n",
      "Epoch 1028, Loss: 0.7789924740791321, Final Batch Loss: 0.3691692054271698\n",
      "Epoch 1029, Loss: 0.8570235669612885, Final Batch Loss: 0.41357001662254333\n",
      "Epoch 1030, Loss: 0.8355672955513, Final Batch Loss: 0.5001415610313416\n",
      "Epoch 1031, Loss: 0.8593830168247223, Final Batch Loss: 0.4540953040122986\n",
      "Epoch 1032, Loss: 0.8062699437141418, Final Batch Loss: 0.3648887574672699\n",
      "Epoch 1033, Loss: 0.8094517886638641, Final Batch Loss: 0.3724493980407715\n",
      "Epoch 1034, Loss: 0.7923520505428314, Final Batch Loss: 0.43372365832328796\n",
      "Epoch 1035, Loss: 0.8136661648750305, Final Batch Loss: 0.37940284609794617\n",
      "Epoch 1036, Loss: 0.8176874816417694, Final Batch Loss: 0.41681969165802\n",
      "Epoch 1037, Loss: 0.8601546287536621, Final Batch Loss: 0.44481348991394043\n",
      "Epoch 1038, Loss: 0.8084086179733276, Final Batch Loss: 0.43181681632995605\n",
      "Epoch 1039, Loss: 0.7938688695430756, Final Batch Loss: 0.39639973640441895\n",
      "Epoch 1040, Loss: 0.8610338866710663, Final Batch Loss: 0.3998890221118927\n",
      "Epoch 1041, Loss: 0.8103798925876617, Final Batch Loss: 0.4126584529876709\n",
      "Epoch 1042, Loss: 0.7863000333309174, Final Batch Loss: 0.407352477312088\n",
      "Epoch 1043, Loss: 0.7703137993812561, Final Batch Loss: 0.3732527494430542\n",
      "Epoch 1044, Loss: 0.7983352839946747, Final Batch Loss: 0.4145621359348297\n",
      "Epoch 1045, Loss: 0.8097805976867676, Final Batch Loss: 0.40928876399993896\n",
      "Epoch 1046, Loss: 0.8063707947731018, Final Batch Loss: 0.3811935484409332\n",
      "Epoch 1047, Loss: 0.8039963245391846, Final Batch Loss: 0.4218006134033203\n",
      "Epoch 1048, Loss: 0.8269718587398529, Final Batch Loss: 0.41535210609436035\n",
      "Epoch 1049, Loss: 0.8226243555545807, Final Batch Loss: 0.4285154938697815\n",
      "Epoch 1050, Loss: 0.8651072382926941, Final Batch Loss: 0.4761419892311096\n",
      "Epoch 1051, Loss: 0.8053177893161774, Final Batch Loss: 0.36280858516693115\n",
      "Epoch 1052, Loss: 0.7443740367889404, Final Batch Loss: 0.3444364070892334\n",
      "Epoch 1053, Loss: 0.8135907649993896, Final Batch Loss: 0.4643845856189728\n",
      "Epoch 1054, Loss: 0.7861742377281189, Final Batch Loss: 0.3583270013332367\n",
      "Epoch 1055, Loss: 0.8163037598133087, Final Batch Loss: 0.4129973351955414\n",
      "Epoch 1056, Loss: 0.7969534695148468, Final Batch Loss: 0.39185917377471924\n",
      "Epoch 1057, Loss: 0.7804470956325531, Final Batch Loss: 0.3785548508167267\n",
      "Epoch 1058, Loss: 0.7676926851272583, Final Batch Loss: 0.375559538602829\n",
      "Epoch 1059, Loss: 0.8149514198303223, Final Batch Loss: 0.39988023042678833\n",
      "Epoch 1060, Loss: 0.8591471910476685, Final Batch Loss: 0.4092177450656891\n",
      "Epoch 1061, Loss: 0.8094808757305145, Final Batch Loss: 0.47200238704681396\n",
      "Epoch 1062, Loss: 0.8551627695560455, Final Batch Loss: 0.4729383885860443\n",
      "Epoch 1063, Loss: 0.8333413600921631, Final Batch Loss: 0.41384753584861755\n",
      "Epoch 1064, Loss: 0.8105280101299286, Final Batch Loss: 0.4421924352645874\n",
      "Epoch 1065, Loss: 0.7670368850231171, Final Batch Loss: 0.35966241359710693\n",
      "Epoch 1066, Loss: 0.7843145728111267, Final Batch Loss: 0.3886658251285553\n",
      "Epoch 1067, Loss: 0.794058084487915, Final Batch Loss: 0.39081141352653503\n",
      "Epoch 1068, Loss: 0.7661486566066742, Final Batch Loss: 0.3347823917865753\n",
      "Epoch 1069, Loss: 0.7792263925075531, Final Batch Loss: 0.38337090611457825\n",
      "Epoch 1070, Loss: 0.7834676504135132, Final Batch Loss: 0.3933005630970001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1071, Loss: 0.7875319123268127, Final Batch Loss: 0.4061829149723053\n",
      "Epoch 1072, Loss: 0.8306678235530853, Final Batch Loss: 0.4313695728778839\n",
      "Epoch 1073, Loss: 0.8093723654747009, Final Batch Loss: 0.43518295884132385\n",
      "Epoch 1074, Loss: 0.8346525430679321, Final Batch Loss: 0.4545522928237915\n",
      "Epoch 1075, Loss: 0.7445974946022034, Final Batch Loss: 0.32428163290023804\n",
      "Epoch 1076, Loss: 0.7875992953777313, Final Batch Loss: 0.38178107142448425\n",
      "Epoch 1077, Loss: 0.7625885009765625, Final Batch Loss: 0.37745675444602966\n",
      "Epoch 1078, Loss: 0.8488878607749939, Final Batch Loss: 0.4296974241733551\n",
      "Epoch 1079, Loss: 0.818575382232666, Final Batch Loss: 0.4258412718772888\n",
      "Epoch 1080, Loss: 0.8068414926528931, Final Batch Loss: 0.43360230326652527\n",
      "Epoch 1081, Loss: 0.8470478057861328, Final Batch Loss: 0.44962653517723083\n",
      "Epoch 1082, Loss: 0.8255018591880798, Final Batch Loss: 0.4798811972141266\n",
      "Epoch 1083, Loss: 0.799907773733139, Final Batch Loss: 0.4346306324005127\n",
      "Epoch 1084, Loss: 0.8066903948783875, Final Batch Loss: 0.4415040612220764\n",
      "Epoch 1085, Loss: 0.8181905448436737, Final Batch Loss: 0.41269198060035706\n",
      "Epoch 1086, Loss: 0.7848247587680817, Final Batch Loss: 0.36220139265060425\n",
      "Epoch 1087, Loss: 0.8186370730400085, Final Batch Loss: 0.3936227560043335\n",
      "Epoch 1088, Loss: 0.81037437915802, Final Batch Loss: 0.3433486819267273\n",
      "Epoch 1089, Loss: 0.8005924820899963, Final Batch Loss: 0.39662793278694153\n",
      "Epoch 1090, Loss: 0.8031132519245148, Final Batch Loss: 0.38286304473876953\n",
      "Epoch 1091, Loss: 0.8112806081771851, Final Batch Loss: 0.3983505964279175\n",
      "Epoch 1092, Loss: 0.7808489203453064, Final Batch Loss: 0.38164472579956055\n",
      "Epoch 1093, Loss: 0.8230470418930054, Final Batch Loss: 0.4470338821411133\n",
      "Epoch 1094, Loss: 0.8478412926197052, Final Batch Loss: 0.4779813587665558\n",
      "Epoch 1095, Loss: 0.7766091525554657, Final Batch Loss: 0.41292640566825867\n",
      "Epoch 1096, Loss: 0.8035531640052795, Final Batch Loss: 0.39976248145103455\n",
      "Epoch 1097, Loss: 0.8128003776073456, Final Batch Loss: 0.4186227023601532\n",
      "Epoch 1098, Loss: 0.8051940202713013, Final Batch Loss: 0.4373214840888977\n",
      "Epoch 1099, Loss: 0.770470917224884, Final Batch Loss: 0.3976667821407318\n",
      "Epoch 1100, Loss: 0.8177313208580017, Final Batch Loss: 0.4218668043613434\n",
      "Epoch 1101, Loss: 0.8399344086647034, Final Batch Loss: 0.42041638493537903\n",
      "Epoch 1102, Loss: 0.8884139657020569, Final Batch Loss: 0.46027445793151855\n",
      "Epoch 1103, Loss: 0.7992800772190094, Final Batch Loss: 0.4125753343105316\n",
      "Epoch 1104, Loss: 0.8160192668437958, Final Batch Loss: 0.41053563356399536\n",
      "Epoch 1105, Loss: 0.8276698589324951, Final Batch Loss: 0.41385892033576965\n",
      "Epoch 1106, Loss: 0.765556275844574, Final Batch Loss: 0.3560265898704529\n",
      "Epoch 1107, Loss: 0.76291224360466, Final Batch Loss: 0.3857492208480835\n",
      "Epoch 1108, Loss: 0.8299335241317749, Final Batch Loss: 0.40943190455436707\n",
      "Epoch 1109, Loss: 0.7943115532398224, Final Batch Loss: 0.3663519322872162\n",
      "Epoch 1110, Loss: 0.7615334093570709, Final Batch Loss: 0.33460697531700134\n",
      "Epoch 1111, Loss: 0.876587837934494, Final Batch Loss: 0.45296043157577515\n",
      "Epoch 1112, Loss: 0.7692460417747498, Final Batch Loss: 0.3966456353664398\n",
      "Epoch 1113, Loss: 0.8261574804782867, Final Batch Loss: 0.39710119366645813\n",
      "Epoch 1114, Loss: 0.8361119031906128, Final Batch Loss: 0.35526028275489807\n",
      "Epoch 1115, Loss: 0.8355976045131683, Final Batch Loss: 0.45135247707366943\n",
      "Epoch 1116, Loss: 0.7895394265651703, Final Batch Loss: 0.39980143308639526\n",
      "Epoch 1117, Loss: 0.8522232174873352, Final Batch Loss: 0.4887314736843109\n",
      "Epoch 1118, Loss: 0.8280815780162811, Final Batch Loss: 0.46198874711990356\n",
      "Epoch 1119, Loss: 0.8120294511318207, Final Batch Loss: 0.43258869647979736\n",
      "Epoch 1120, Loss: 0.7825905084609985, Final Batch Loss: 0.42878958582878113\n",
      "Epoch 1121, Loss: 0.7429062128067017, Final Batch Loss: 0.3954436182975769\n",
      "Epoch 1122, Loss: 0.8138571679592133, Final Batch Loss: 0.39932486414909363\n",
      "Epoch 1123, Loss: 0.8287422358989716, Final Batch Loss: 0.4780578315258026\n",
      "Epoch 1124, Loss: 0.7957359254360199, Final Batch Loss: 0.3914062976837158\n",
      "Epoch 1125, Loss: 0.8346525132656097, Final Batch Loss: 0.44732987880706787\n",
      "Epoch 1126, Loss: 0.8025957643985748, Final Batch Loss: 0.41028285026550293\n",
      "Epoch 1127, Loss: 0.8438814878463745, Final Batch Loss: 0.41996973752975464\n",
      "Epoch 1128, Loss: 0.8343031108379364, Final Batch Loss: 0.4724145233631134\n",
      "Epoch 1129, Loss: 0.8446415364742279, Final Batch Loss: 0.4699513614177704\n",
      "Epoch 1130, Loss: 0.8073915541172028, Final Batch Loss: 0.37836456298828125\n",
      "Epoch 1131, Loss: 0.8129913806915283, Final Batch Loss: 0.4028858542442322\n",
      "Epoch 1132, Loss: 0.8089487552642822, Final Batch Loss: 0.37717047333717346\n",
      "Epoch 1133, Loss: 0.8385817110538483, Final Batch Loss: 0.45255061984062195\n",
      "Epoch 1134, Loss: 0.7907195091247559, Final Batch Loss: 0.3487955331802368\n",
      "Epoch 1135, Loss: 0.8121320903301239, Final Batch Loss: 0.3955923318862915\n",
      "Epoch 1136, Loss: 0.8052490949630737, Final Batch Loss: 0.38973963260650635\n",
      "Epoch 1137, Loss: 0.8322823941707611, Final Batch Loss: 0.4500463306903839\n",
      "Epoch 1138, Loss: 0.7783650755882263, Final Batch Loss: 0.37226808071136475\n",
      "Epoch 1139, Loss: 0.7305622696876526, Final Batch Loss: 0.3469959497451782\n",
      "Epoch 1140, Loss: 0.8185289800167084, Final Batch Loss: 0.383439838886261\n",
      "Epoch 1141, Loss: 0.8169808387756348, Final Batch Loss: 0.36057865619659424\n",
      "Epoch 1142, Loss: 0.7687025964260101, Final Batch Loss: 0.372799277305603\n",
      "Epoch 1143, Loss: 0.76251420378685, Final Batch Loss: 0.3661513030529022\n",
      "Epoch 1144, Loss: 0.7600823044776917, Final Batch Loss: 0.36836227774620056\n",
      "Epoch 1145, Loss: 0.7809177935123444, Final Batch Loss: 0.37074220180511475\n",
      "Epoch 1146, Loss: 0.7764738500118256, Final Batch Loss: 0.3987260162830353\n",
      "Epoch 1147, Loss: 0.7678644955158234, Final Batch Loss: 0.3742857873439789\n",
      "Epoch 1148, Loss: 0.8433340787887573, Final Batch Loss: 0.4498380720615387\n",
      "Epoch 1149, Loss: 0.8031066358089447, Final Batch Loss: 0.42179974913597107\n",
      "Epoch 1150, Loss: 0.7717079222202301, Final Batch Loss: 0.37545332312583923\n",
      "Epoch 1151, Loss: 0.7983171939849854, Final Batch Loss: 0.4108882546424866\n",
      "Epoch 1152, Loss: 0.788544774055481, Final Batch Loss: 0.4099559187889099\n",
      "Epoch 1153, Loss: 0.7689004242420197, Final Batch Loss: 0.3689545691013336\n",
      "Epoch 1154, Loss: 0.7709557116031647, Final Batch Loss: 0.3415323793888092\n",
      "Epoch 1155, Loss: 0.8469433188438416, Final Batch Loss: 0.42208877205848694\n",
      "Epoch 1156, Loss: 0.7809142768383026, Final Batch Loss: 0.32629597187042236\n",
      "Epoch 1157, Loss: 0.7708680927753448, Final Batch Loss: 0.3692573606967926\n",
      "Epoch 1158, Loss: 0.7998383343219757, Final Batch Loss: 0.43154191970825195\n",
      "Epoch 1159, Loss: 0.8045845925807953, Final Batch Loss: 0.36767661571502686\n",
      "Epoch 1160, Loss: 0.7366837859153748, Final Batch Loss: 0.29282569885253906\n",
      "Epoch 1161, Loss: 0.7307023108005524, Final Batch Loss: 0.35727477073669434\n",
      "Epoch 1162, Loss: 0.7888667285442352, Final Batch Loss: 0.40506014227867126\n",
      "Epoch 1163, Loss: 0.779682070016861, Final Batch Loss: 0.38239333033561707\n",
      "Epoch 1164, Loss: 0.7846476137638092, Final Batch Loss: 0.39620882272720337\n",
      "Epoch 1165, Loss: 0.7931191027164459, Final Batch Loss: 0.385958194732666\n",
      "Epoch 1166, Loss: 0.8088433146476746, Final Batch Loss: 0.38805821537971497\n",
      "Epoch 1167, Loss: 0.7930897772312164, Final Batch Loss: 0.4383622407913208\n",
      "Epoch 1168, Loss: 0.7652900815010071, Final Batch Loss: 0.35973241925239563\n",
      "Epoch 1169, Loss: 0.8053781688213348, Final Batch Loss: 0.3589039146900177\n",
      "Epoch 1170, Loss: 0.9408097267150879, Final Batch Loss: 0.5383792519569397\n",
      "Epoch 1171, Loss: 0.8904312551021576, Final Batch Loss: 0.5133211612701416\n",
      "Epoch 1172, Loss: 0.7808863818645477, Final Batch Loss: 0.35550037026405334\n",
      "Epoch 1173, Loss: 0.7896120250225067, Final Batch Loss: 0.358659565448761\n",
      "Epoch 1174, Loss: 0.7877883315086365, Final Batch Loss: 0.4019743502140045\n",
      "Epoch 1175, Loss: 0.7809286415576935, Final Batch Loss: 0.37725594639778137\n",
      "Epoch 1176, Loss: 0.7973074316978455, Final Batch Loss: 0.3977319598197937\n",
      "Epoch 1177, Loss: 0.783006876707077, Final Batch Loss: 0.43663400411605835\n",
      "Epoch 1178, Loss: 0.7798675894737244, Final Batch Loss: 0.3974749445915222\n",
      "Epoch 1179, Loss: 0.7649789452552795, Final Batch Loss: 0.3995262086391449\n",
      "Epoch 1180, Loss: 0.8123273551464081, Final Batch Loss: 0.408631831407547\n",
      "Epoch 1181, Loss: 0.7964576184749603, Final Batch Loss: 0.38875797390937805\n",
      "Epoch 1182, Loss: 0.802006185054779, Final Batch Loss: 0.4297858476638794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1183, Loss: 0.7533906400203705, Final Batch Loss: 0.29742586612701416\n",
      "Epoch 1184, Loss: 0.786393791437149, Final Batch Loss: 0.4091012477874756\n",
      "Epoch 1185, Loss: 0.7843860685825348, Final Batch Loss: 0.37837764620780945\n",
      "Epoch 1186, Loss: 0.7785367071628571, Final Batch Loss: 0.39612171053886414\n",
      "Epoch 1187, Loss: 0.7852270901203156, Final Batch Loss: 0.3966948986053467\n",
      "Epoch 1188, Loss: 0.7788385450839996, Final Batch Loss: 0.408953994512558\n",
      "Epoch 1189, Loss: 0.7867602705955505, Final Batch Loss: 0.409271776676178\n",
      "Epoch 1190, Loss: 0.7537384927272797, Final Batch Loss: 0.3801635801792145\n",
      "Epoch 1191, Loss: 0.7303411066532135, Final Batch Loss: 0.29248595237731934\n",
      "Epoch 1192, Loss: 0.7277356386184692, Final Batch Loss: 0.3342595398426056\n",
      "Epoch 1193, Loss: 0.8242290914058685, Final Batch Loss: 0.38055843114852905\n",
      "Epoch 1194, Loss: 0.778380960226059, Final Batch Loss: 0.37695232033729553\n",
      "Epoch 1195, Loss: 0.7659035623073578, Final Batch Loss: 0.3902861475944519\n",
      "Epoch 1196, Loss: 0.7559270262718201, Final Batch Loss: 0.3914136588573456\n",
      "Epoch 1197, Loss: 0.8058463633060455, Final Batch Loss: 0.42811062932014465\n",
      "Epoch 1198, Loss: 0.7999761402606964, Final Batch Loss: 0.40633565187454224\n",
      "Epoch 1199, Loss: 0.7856038510799408, Final Batch Loss: 0.3809293210506439\n",
      "Epoch 1200, Loss: 0.791408121585846, Final Batch Loss: 0.4322287440299988\n",
      "Epoch 1201, Loss: 0.8003197908401489, Final Batch Loss: 0.3879280388355255\n",
      "Epoch 1202, Loss: 0.7490116059780121, Final Batch Loss: 0.34316450357437134\n",
      "Epoch 1203, Loss: 0.7963690161705017, Final Batch Loss: 0.40090450644493103\n",
      "Epoch 1204, Loss: 0.8064451813697815, Final Batch Loss: 0.37221041321754456\n",
      "Epoch 1205, Loss: 0.7943889200687408, Final Batch Loss: 0.3881683647632599\n",
      "Epoch 1206, Loss: 0.8266844749450684, Final Batch Loss: 0.44260165095329285\n",
      "Epoch 1207, Loss: 0.7820281386375427, Final Batch Loss: 0.4125657379627228\n",
      "Epoch 1208, Loss: 0.831624299287796, Final Batch Loss: 0.41999900341033936\n",
      "Epoch 1209, Loss: 0.8054457902908325, Final Batch Loss: 0.4015316963195801\n",
      "Epoch 1210, Loss: 0.7682072818279266, Final Batch Loss: 0.37932494282722473\n",
      "Epoch 1211, Loss: 0.7615998983383179, Final Batch Loss: 0.3601347506046295\n",
      "Epoch 1212, Loss: 0.7483501136302948, Final Batch Loss: 0.35828956961631775\n",
      "Epoch 1213, Loss: 0.7831192314624786, Final Batch Loss: 0.4196845293045044\n",
      "Epoch 1214, Loss: 0.763205498456955, Final Batch Loss: 0.2859719693660736\n",
      "Epoch 1215, Loss: 0.7542121112346649, Final Batch Loss: 0.3552795946598053\n",
      "Epoch 1216, Loss: 0.7728314101696014, Final Batch Loss: 0.4155319035053253\n",
      "Epoch 1217, Loss: 0.783185750246048, Final Batch Loss: 0.3984774947166443\n",
      "Epoch 1218, Loss: 0.7575555145740509, Final Batch Loss: 0.38806581497192383\n",
      "Epoch 1219, Loss: 0.7821992039680481, Final Batch Loss: 0.4582265615463257\n",
      "Epoch 1220, Loss: 0.8233173787593842, Final Batch Loss: 0.3897761106491089\n",
      "Epoch 1221, Loss: 0.8255711495876312, Final Batch Loss: 0.4256865978240967\n",
      "Epoch 1222, Loss: 0.7626871466636658, Final Batch Loss: 0.3555670380592346\n",
      "Epoch 1223, Loss: 0.7817502617835999, Final Batch Loss: 0.37919002771377563\n",
      "Epoch 1224, Loss: 0.7747840881347656, Final Batch Loss: 0.37583646178245544\n",
      "Epoch 1225, Loss: 0.7366040647029877, Final Batch Loss: 0.36643990874290466\n",
      "Epoch 1226, Loss: 0.7366633415222168, Final Batch Loss: 0.3541671633720398\n",
      "Epoch 1227, Loss: 0.7954361736774445, Final Batch Loss: 0.4110366702079773\n",
      "Epoch 1228, Loss: 0.7528143525123596, Final Batch Loss: 0.37922248244285583\n",
      "Epoch 1229, Loss: 0.8459840714931488, Final Batch Loss: 0.4258752763271332\n",
      "Epoch 1230, Loss: 0.7786775529384613, Final Batch Loss: 0.4279816746711731\n",
      "Epoch 1231, Loss: 0.7705722749233246, Final Batch Loss: 0.3947528302669525\n",
      "Epoch 1232, Loss: 0.7933511137962341, Final Batch Loss: 0.4058416187763214\n",
      "Epoch 1233, Loss: 0.7707545459270477, Final Batch Loss: 0.39596158266067505\n",
      "Epoch 1234, Loss: 0.7597722411155701, Final Batch Loss: 0.3488559126853943\n",
      "Epoch 1235, Loss: 0.7763596773147583, Final Batch Loss: 0.353189617395401\n",
      "Epoch 1236, Loss: 0.7774706780910492, Final Batch Loss: 0.36970725655555725\n",
      "Epoch 1237, Loss: 0.7826748490333557, Final Batch Loss: 0.403085321187973\n",
      "Epoch 1238, Loss: 0.7633153200149536, Final Batch Loss: 0.36732053756713867\n",
      "Epoch 1239, Loss: 0.7454644739627838, Final Batch Loss: 0.3530190587043762\n",
      "Epoch 1240, Loss: 0.7591027915477753, Final Batch Loss: 0.32721197605133057\n",
      "Epoch 1241, Loss: 0.7670191526412964, Final Batch Loss: 0.38952696323394775\n",
      "Epoch 1242, Loss: 0.7364651560783386, Final Batch Loss: 0.34026485681533813\n",
      "Epoch 1243, Loss: 0.7707108557224274, Final Batch Loss: 0.41558972001075745\n",
      "Epoch 1244, Loss: 0.7664052248001099, Final Batch Loss: 0.3537828028202057\n",
      "Epoch 1245, Loss: 0.7336239516735077, Final Batch Loss: 0.3546198010444641\n",
      "Epoch 1246, Loss: 0.841756671667099, Final Batch Loss: 0.40722817182540894\n",
      "Epoch 1247, Loss: 0.7558156251907349, Final Batch Loss: 0.366155743598938\n",
      "Epoch 1248, Loss: 0.7641884386539459, Final Batch Loss: 0.4243520498275757\n",
      "Epoch 1249, Loss: 0.7540407776832581, Final Batch Loss: 0.3776026666164398\n",
      "Epoch 1250, Loss: 0.7739680707454681, Final Batch Loss: 0.36710259318351746\n",
      "Epoch 1251, Loss: 0.8069665431976318, Final Batch Loss: 0.3787996768951416\n",
      "Epoch 1252, Loss: 0.7561847269535065, Final Batch Loss: 0.37236112356185913\n",
      "Epoch 1253, Loss: 0.7403759360313416, Final Batch Loss: 0.39512568712234497\n",
      "Epoch 1254, Loss: 0.795819491147995, Final Batch Loss: 0.43639883399009705\n",
      "Epoch 1255, Loss: 0.74753138422966, Final Batch Loss: 0.3805892765522003\n",
      "Epoch 1256, Loss: 0.7423143982887268, Final Batch Loss: 0.34625598788261414\n",
      "Epoch 1257, Loss: 0.7606936693191528, Final Batch Loss: 0.392553448677063\n",
      "Epoch 1258, Loss: 0.7646694183349609, Final Batch Loss: 0.3837635815143585\n",
      "Epoch 1259, Loss: 0.764350026845932, Final Batch Loss: 0.38888320326805115\n",
      "Epoch 1260, Loss: 0.7508020997047424, Final Batch Loss: 0.381542831659317\n",
      "Epoch 1261, Loss: 0.7789603769779205, Final Batch Loss: 0.4167942702770233\n",
      "Epoch 1262, Loss: 0.797954648733139, Final Batch Loss: 0.40389472246170044\n",
      "Epoch 1263, Loss: 0.8135218918323517, Final Batch Loss: 0.41764265298843384\n",
      "Epoch 1264, Loss: 0.788975328207016, Final Batch Loss: 0.40193185210227966\n",
      "Epoch 1265, Loss: 0.7584410011768341, Final Batch Loss: 0.3794015645980835\n",
      "Epoch 1266, Loss: 0.8192860186100006, Final Batch Loss: 0.43530571460723877\n",
      "Epoch 1267, Loss: 0.7963682413101196, Final Batch Loss: 0.381795734167099\n",
      "Epoch 1268, Loss: 0.756177693605423, Final Batch Loss: 0.3513343036174774\n",
      "Epoch 1269, Loss: 0.743264228105545, Final Batch Loss: 0.35958245396614075\n",
      "Epoch 1270, Loss: 0.7499913275241852, Final Batch Loss: 0.3372022807598114\n",
      "Epoch 1271, Loss: 0.7802855670452118, Final Batch Loss: 0.3780786097049713\n",
      "Epoch 1272, Loss: 0.8390645086765289, Final Batch Loss: 0.41177302598953247\n",
      "Epoch 1273, Loss: 0.7578551769256592, Final Batch Loss: 0.35716450214385986\n",
      "Epoch 1274, Loss: 0.7706019580364227, Final Batch Loss: 0.38118013739585876\n",
      "Epoch 1275, Loss: 0.833460658788681, Final Batch Loss: 0.43037667870521545\n",
      "Epoch 1276, Loss: 0.7611128389835358, Final Batch Loss: 0.3571660816669464\n",
      "Epoch 1277, Loss: 0.7609924077987671, Final Batch Loss: 0.3575310707092285\n",
      "Epoch 1278, Loss: 0.7531408667564392, Final Batch Loss: 0.36145490407943726\n",
      "Epoch 1279, Loss: 0.7848057150840759, Final Batch Loss: 0.3731817901134491\n",
      "Epoch 1280, Loss: 0.7864574491977692, Final Batch Loss: 0.40767404437065125\n",
      "Epoch 1281, Loss: 0.7956392765045166, Final Batch Loss: 0.4048089385032654\n",
      "Epoch 1282, Loss: 0.7756829857826233, Final Batch Loss: 0.4135710299015045\n",
      "Epoch 1283, Loss: 0.8298721313476562, Final Batch Loss: 0.4360942542552948\n",
      "Epoch 1284, Loss: 0.748749703168869, Final Batch Loss: 0.3225809931755066\n",
      "Epoch 1285, Loss: 0.8402268588542938, Final Batch Loss: 0.48292747139930725\n",
      "Epoch 1286, Loss: 0.852472722530365, Final Batch Loss: 0.5052868723869324\n",
      "Epoch 1287, Loss: 0.8125836849212646, Final Batch Loss: 0.39926478266716003\n",
      "Epoch 1288, Loss: 0.739147961139679, Final Batch Loss: 0.3417729437351227\n",
      "Epoch 1289, Loss: 0.7373790144920349, Final Batch Loss: 0.37910062074661255\n",
      "Epoch 1290, Loss: 0.7847088873386383, Final Batch Loss: 0.38930413126945496\n",
      "Epoch 1291, Loss: 0.7255946695804596, Final Batch Loss: 0.35653626918792725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1292, Loss: 0.7561070919036865, Final Batch Loss: 0.3586002290248871\n",
      "Epoch 1293, Loss: 0.7921251058578491, Final Batch Loss: 0.4596693217754364\n",
      "Epoch 1294, Loss: 0.8422756195068359, Final Batch Loss: 0.49193984270095825\n",
      "Epoch 1295, Loss: 0.7580999732017517, Final Batch Loss: 0.34557661414146423\n",
      "Epoch 1296, Loss: 0.760858565568924, Final Batch Loss: 0.37037479877471924\n",
      "Epoch 1297, Loss: 0.7398614883422852, Final Batch Loss: 0.3641327917575836\n",
      "Epoch 1298, Loss: 0.7924895584583282, Final Batch Loss: 0.41174182295799255\n",
      "Epoch 1299, Loss: 0.708983838558197, Final Batch Loss: 0.36481672525405884\n",
      "Epoch 1300, Loss: 0.7495105564594269, Final Batch Loss: 0.33906418085098267\n",
      "Epoch 1301, Loss: 0.7802510857582092, Final Batch Loss: 0.38325628638267517\n",
      "Epoch 1302, Loss: 0.7358950078487396, Final Batch Loss: 0.41022980213165283\n",
      "Epoch 1303, Loss: 0.8081143200397491, Final Batch Loss: 0.41556185483932495\n",
      "Epoch 1304, Loss: 0.7688681781291962, Final Batch Loss: 0.39269402623176575\n",
      "Epoch 1305, Loss: 0.7641215324401855, Final Batch Loss: 0.37546539306640625\n",
      "Epoch 1306, Loss: 0.8043633699417114, Final Batch Loss: 0.42137226462364197\n",
      "Epoch 1307, Loss: 0.7528708875179291, Final Batch Loss: 0.3859991729259491\n",
      "Epoch 1308, Loss: 0.7465033233165741, Final Batch Loss: 0.35112887620925903\n",
      "Epoch 1309, Loss: 0.698775053024292, Final Batch Loss: 0.33954155445098877\n",
      "Epoch 1310, Loss: 0.7630771696567535, Final Batch Loss: 0.3596108555793762\n",
      "Epoch 1311, Loss: 0.8016738295555115, Final Batch Loss: 0.4454011023044586\n",
      "Epoch 1312, Loss: 0.7218959331512451, Final Batch Loss: 0.35319408774375916\n",
      "Epoch 1313, Loss: 0.7425362467765808, Final Batch Loss: 0.40578827261924744\n",
      "Epoch 1314, Loss: 0.7507545351982117, Final Batch Loss: 0.3629181981086731\n",
      "Epoch 1315, Loss: 0.8231399357318878, Final Batch Loss: 0.4644622504711151\n",
      "Epoch 1316, Loss: 0.7418976128101349, Final Batch Loss: 0.3278767764568329\n",
      "Epoch 1317, Loss: 0.7820626497268677, Final Batch Loss: 0.41557976603507996\n",
      "Epoch 1318, Loss: 0.7395910024642944, Final Batch Loss: 0.3885328769683838\n",
      "Epoch 1319, Loss: 0.7643844783306122, Final Batch Loss: 0.3937762975692749\n",
      "Epoch 1320, Loss: 0.7648809850215912, Final Batch Loss: 0.43768399953842163\n",
      "Epoch 1321, Loss: 0.7426781356334686, Final Batch Loss: 0.39776894450187683\n",
      "Epoch 1322, Loss: 0.788417398929596, Final Batch Loss: 0.4659324884414673\n",
      "Epoch 1323, Loss: 0.757879227399826, Final Batch Loss: 0.398820698261261\n",
      "Epoch 1324, Loss: 0.7370359897613525, Final Batch Loss: 0.3613589107990265\n",
      "Epoch 1325, Loss: 0.7452846765518188, Final Batch Loss: 0.3934972584247589\n",
      "Epoch 1326, Loss: 0.7544441521167755, Final Batch Loss: 0.40862613916397095\n",
      "Epoch 1327, Loss: 0.7800717353820801, Final Batch Loss: 0.37655898928642273\n",
      "Epoch 1328, Loss: 0.7678726017475128, Final Batch Loss: 0.3732796609401703\n",
      "Epoch 1329, Loss: 0.7392362356185913, Final Batch Loss: 0.3622508645057678\n",
      "Epoch 1330, Loss: 0.7765369415283203, Final Batch Loss: 0.4549958109855652\n",
      "Epoch 1331, Loss: 0.7691569328308105, Final Batch Loss: 0.3948521316051483\n",
      "Epoch 1332, Loss: 0.7776884436607361, Final Batch Loss: 0.39777901768684387\n",
      "Epoch 1333, Loss: 0.7646873593330383, Final Batch Loss: 0.39233821630477905\n",
      "Epoch 1334, Loss: 0.7474550306797028, Final Batch Loss: 0.32912710309028625\n",
      "Epoch 1335, Loss: 0.7629803717136383, Final Batch Loss: 0.3580368161201477\n",
      "Epoch 1336, Loss: 0.7794872522354126, Final Batch Loss: 0.37401577830314636\n",
      "Epoch 1337, Loss: 0.7478559613227844, Final Batch Loss: 0.36628958582878113\n",
      "Epoch 1338, Loss: 0.7910820543766022, Final Batch Loss: 0.40077561140060425\n",
      "Epoch 1339, Loss: 0.7310651242733002, Final Batch Loss: 0.38012373447418213\n",
      "Epoch 1340, Loss: 0.749602198600769, Final Batch Loss: 0.3605894446372986\n",
      "Epoch 1341, Loss: 0.7270388305187225, Final Batch Loss: 0.35912030935287476\n",
      "Epoch 1342, Loss: 0.7887300252914429, Final Batch Loss: 0.4147171676158905\n",
      "Epoch 1343, Loss: 0.7312963604927063, Final Batch Loss: 0.3624676465988159\n",
      "Epoch 1344, Loss: 0.7582352459430695, Final Batch Loss: 0.387557715177536\n",
      "Epoch 1345, Loss: 0.7712108194828033, Final Batch Loss: 0.3978544771671295\n",
      "Epoch 1346, Loss: 0.7539171576499939, Final Batch Loss: 0.3503433167934418\n",
      "Epoch 1347, Loss: 0.7584726214408875, Final Batch Loss: 0.36866623163223267\n",
      "Epoch 1348, Loss: 0.7509784996509552, Final Batch Loss: 0.39419886469841003\n",
      "Epoch 1349, Loss: 0.7253097593784332, Final Batch Loss: 0.34446850419044495\n",
      "Epoch 1350, Loss: 0.7135026454925537, Final Batch Loss: 0.34433794021606445\n",
      "Epoch 1351, Loss: 0.7329148352146149, Final Batch Loss: 0.3544774651527405\n",
      "Epoch 1352, Loss: 0.7250739932060242, Final Batch Loss: 0.3533744513988495\n",
      "Epoch 1353, Loss: 0.7543697953224182, Final Batch Loss: 0.3552974760532379\n",
      "Epoch 1354, Loss: 0.6978350579738617, Final Batch Loss: 0.34722456336021423\n",
      "Epoch 1355, Loss: 0.7527992427349091, Final Batch Loss: 0.38301631808280945\n",
      "Epoch 1356, Loss: 0.7426358759403229, Final Batch Loss: 0.33796820044517517\n",
      "Epoch 1357, Loss: 0.8403003513813019, Final Batch Loss: 0.4539165198802948\n",
      "Epoch 1358, Loss: 0.7665724456310272, Final Batch Loss: 0.36038655042648315\n",
      "Epoch 1359, Loss: 0.7408009767532349, Final Batch Loss: 0.3551519215106964\n",
      "Epoch 1360, Loss: 0.7740689218044281, Final Batch Loss: 0.42565789818763733\n",
      "Epoch 1361, Loss: 0.7234492897987366, Final Batch Loss: 0.3504217267036438\n",
      "Epoch 1362, Loss: 0.7832452356815338, Final Batch Loss: 0.4049736261367798\n",
      "Epoch 1363, Loss: 0.7482957243919373, Final Batch Loss: 0.37293604016304016\n",
      "Epoch 1364, Loss: 0.7566995620727539, Final Batch Loss: 0.3282370865345001\n",
      "Epoch 1365, Loss: 0.749221533536911, Final Batch Loss: 0.370760440826416\n",
      "Epoch 1366, Loss: 0.7511147260665894, Final Batch Loss: 0.3735490143299103\n",
      "Epoch 1367, Loss: 0.7550469040870667, Final Batch Loss: 0.3720908463001251\n",
      "Epoch 1368, Loss: 0.7615256607532501, Final Batch Loss: 0.3411845862865448\n",
      "Epoch 1369, Loss: 0.7724741697311401, Final Batch Loss: 0.37207818031311035\n",
      "Epoch 1370, Loss: 0.7754101157188416, Final Batch Loss: 0.42487218976020813\n",
      "Epoch 1371, Loss: 0.7967696189880371, Final Batch Loss: 0.35071617364883423\n",
      "Epoch 1372, Loss: 0.8133746087551117, Final Batch Loss: 0.42312872409820557\n",
      "Epoch 1373, Loss: 0.7312494516372681, Final Batch Loss: 0.39028260111808777\n",
      "Epoch 1374, Loss: 0.7633480429649353, Final Batch Loss: 0.4036555886268616\n",
      "Epoch 1375, Loss: 0.7188967764377594, Final Batch Loss: 0.32179272174835205\n",
      "Epoch 1376, Loss: 0.7234507501125336, Final Batch Loss: 0.3578324019908905\n",
      "Epoch 1377, Loss: 0.783737450838089, Final Batch Loss: 0.37253066897392273\n",
      "Epoch 1378, Loss: 0.8180362582206726, Final Batch Loss: 0.39697813987731934\n",
      "Epoch 1379, Loss: 0.7500552237033844, Final Batch Loss: 0.38290995359420776\n",
      "Epoch 1380, Loss: 0.7254074215888977, Final Batch Loss: 0.3776669502258301\n",
      "Epoch 1381, Loss: 0.7962469756603241, Final Batch Loss: 0.4390024244785309\n",
      "Epoch 1382, Loss: 0.73775315284729, Final Batch Loss: 0.397330641746521\n",
      "Epoch 1383, Loss: 0.7152339816093445, Final Batch Loss: 0.32969680428504944\n",
      "Epoch 1384, Loss: 0.7509448826313019, Final Batch Loss: 0.369181364774704\n",
      "Epoch 1385, Loss: 0.7260725200176239, Final Batch Loss: 0.36903518438339233\n",
      "Epoch 1386, Loss: 0.7629095017910004, Final Batch Loss: 0.3894897401332855\n",
      "Epoch 1387, Loss: 0.7697593867778778, Final Batch Loss: 0.4363977611064911\n",
      "Epoch 1388, Loss: 0.7147789299488068, Final Batch Loss: 0.3508445620536804\n",
      "Epoch 1389, Loss: 0.7908951342105865, Final Batch Loss: 0.4289841055870056\n",
      "Epoch 1390, Loss: 0.7533282935619354, Final Batch Loss: 0.37952715158462524\n",
      "Epoch 1391, Loss: 0.771111249923706, Final Batch Loss: 0.35871952772140503\n",
      "Epoch 1392, Loss: 0.7568988800048828, Final Batch Loss: 0.40251868963241577\n",
      "Epoch 1393, Loss: 0.7538027167320251, Final Batch Loss: 0.3518276810646057\n",
      "Epoch 1394, Loss: 0.7939165234565735, Final Batch Loss: 0.4001142382621765\n",
      "Epoch 1395, Loss: 0.7403257489204407, Final Batch Loss: 0.36519482731819153\n",
      "Epoch 1396, Loss: 0.744560182094574, Final Batch Loss: 0.38074254989624023\n",
      "Epoch 1397, Loss: 0.7394998073577881, Final Batch Loss: 0.3436398506164551\n",
      "Epoch 1398, Loss: 0.7665269672870636, Final Batch Loss: 0.3999537229537964\n",
      "Epoch 1399, Loss: 0.7566395699977875, Final Batch Loss: 0.3802531361579895\n",
      "Epoch 1400, Loss: 0.7657364308834076, Final Batch Loss: 0.39471060037612915\n",
      "Epoch 1401, Loss: 0.7575125396251678, Final Batch Loss: 0.35369011759757996\n",
      "Epoch 1402, Loss: 0.7460457980632782, Final Batch Loss: 0.3881322145462036\n",
      "Epoch 1403, Loss: 0.7405439615249634, Final Batch Loss: 0.3760714530944824\n",
      "Epoch 1404, Loss: 0.7296180427074432, Final Batch Loss: 0.3737384080886841\n",
      "Epoch 1405, Loss: 0.7286526262760162, Final Batch Loss: 0.3816715478897095\n",
      "Epoch 1406, Loss: 0.7418403327465057, Final Batch Loss: 0.3834584057331085\n",
      "Epoch 1407, Loss: 0.7465384304523468, Final Batch Loss: 0.3641956150531769\n",
      "Epoch 1408, Loss: 0.7441504597663879, Final Batch Loss: 0.3580795228481293\n",
      "Epoch 1409, Loss: 0.7373255491256714, Final Batch Loss: 0.3531096577644348\n",
      "Epoch 1410, Loss: 0.7159782648086548, Final Batch Loss: 0.3152802586555481\n",
      "Epoch 1411, Loss: 0.6821393072605133, Final Batch Loss: 0.29416441917419434\n",
      "Epoch 1412, Loss: 0.7208833694458008, Final Batch Loss: 0.32878661155700684\n",
      "Epoch 1413, Loss: 0.7740979492664337, Final Batch Loss: 0.42302802205085754\n",
      "Epoch 1414, Loss: 0.6962068676948547, Final Batch Loss: 0.3230437636375427\n",
      "Epoch 1415, Loss: 0.7646067440509796, Final Batch Loss: 0.39296332001686096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1416, Loss: 0.759179949760437, Final Batch Loss: 0.4045148193836212\n",
      "Epoch 1417, Loss: 0.7513059377670288, Final Batch Loss: 0.38177406787872314\n",
      "Epoch 1418, Loss: 0.7091723680496216, Final Batch Loss: 0.3562444746494293\n",
      "Epoch 1419, Loss: 0.7681780755519867, Final Batch Loss: 0.38585761189460754\n",
      "Epoch 1420, Loss: 0.7346093058586121, Final Batch Loss: 0.4071681797504425\n",
      "Epoch 1421, Loss: 0.7296797037124634, Final Batch Loss: 0.36566823720932007\n",
      "Epoch 1422, Loss: 0.7306902408599854, Final Batch Loss: 0.3916981518268585\n",
      "Epoch 1423, Loss: 0.7834492623806, Final Batch Loss: 0.46971917152404785\n",
      "Epoch 1424, Loss: 0.6968826353549957, Final Batch Loss: 0.298239529132843\n",
      "Epoch 1425, Loss: 0.7025770545005798, Final Batch Loss: 0.32035118341445923\n",
      "Epoch 1426, Loss: 0.7334963381290436, Final Batch Loss: 0.35253846645355225\n",
      "Epoch 1427, Loss: 0.7733479142189026, Final Batch Loss: 0.4015810191631317\n",
      "Epoch 1428, Loss: 0.7142795920372009, Final Batch Loss: 0.36961686611175537\n",
      "Epoch 1429, Loss: 0.7225249409675598, Final Batch Loss: 0.36490732431411743\n",
      "Epoch 1430, Loss: 0.707001805305481, Final Batch Loss: 0.33729392290115356\n",
      "Epoch 1431, Loss: 0.7457996904850006, Final Batch Loss: 0.42110463976860046\n",
      "Epoch 1432, Loss: 0.7328294813632965, Final Batch Loss: 0.3849034011363983\n",
      "Epoch 1433, Loss: 0.7138243019580841, Final Batch Loss: 0.3164273500442505\n",
      "Epoch 1434, Loss: 0.7140712738037109, Final Batch Loss: 0.37213075160980225\n",
      "Epoch 1435, Loss: 0.6997054815292358, Final Batch Loss: 0.3386542797088623\n",
      "Epoch 1436, Loss: 0.774404376745224, Final Batch Loss: 0.4205721914768219\n",
      "Epoch 1437, Loss: 0.7217777073383331, Final Batch Loss: 0.38454470038414\n",
      "Epoch 1438, Loss: 0.7388848066329956, Final Batch Loss: 0.39414718747138977\n",
      "Epoch 1439, Loss: 0.7053402364253998, Final Batch Loss: 0.35138705372810364\n",
      "Epoch 1440, Loss: 0.6930038332939148, Final Batch Loss: 0.2984064519405365\n",
      "Epoch 1441, Loss: 0.7596668004989624, Final Batch Loss: 0.38072147965431213\n",
      "Epoch 1442, Loss: 0.6873027682304382, Final Batch Loss: 0.35037365555763245\n",
      "Epoch 1443, Loss: 0.6796025335788727, Final Batch Loss: 0.3140075206756592\n",
      "Epoch 1444, Loss: 0.7694906294345856, Final Batch Loss: 0.3371690511703491\n",
      "Epoch 1445, Loss: 0.7203220129013062, Final Batch Loss: 0.35953488945961\n",
      "Epoch 1446, Loss: 0.707627534866333, Final Batch Loss: 0.3054448366165161\n",
      "Epoch 1447, Loss: 0.7414121329784393, Final Batch Loss: 0.41455206274986267\n",
      "Epoch 1448, Loss: 0.7245826125144958, Final Batch Loss: 0.3367007374763489\n",
      "Epoch 1449, Loss: 0.7475048899650574, Final Batch Loss: 0.40758800506591797\n",
      "Epoch 1450, Loss: 0.7275727689266205, Final Batch Loss: 0.3519758880138397\n",
      "Epoch 1451, Loss: 0.7768973410129547, Final Batch Loss: 0.44047558307647705\n",
      "Epoch 1452, Loss: 0.7091733515262604, Final Batch Loss: 0.3803238570690155\n",
      "Epoch 1453, Loss: 0.7342603504657745, Final Batch Loss: 0.37621474266052246\n",
      "Epoch 1454, Loss: 0.6864734590053558, Final Batch Loss: 0.2996641993522644\n",
      "Epoch 1455, Loss: 0.7298805415630341, Final Batch Loss: 0.3782941699028015\n",
      "Epoch 1456, Loss: 0.7432369887828827, Final Batch Loss: 0.3671763241291046\n",
      "Epoch 1457, Loss: 0.7210301756858826, Final Batch Loss: 0.36217761039733887\n",
      "Epoch 1458, Loss: 0.6987911462783813, Final Batch Loss: 0.3647560179233551\n",
      "Epoch 1459, Loss: 0.733838826417923, Final Batch Loss: 0.3388143479824066\n",
      "Epoch 1460, Loss: 0.7251498699188232, Final Batch Loss: 0.3804902136325836\n",
      "Epoch 1461, Loss: 0.720882922410965, Final Batch Loss: 0.34698858857154846\n",
      "Epoch 1462, Loss: 0.7117745280265808, Final Batch Loss: 0.36479130387306213\n",
      "Epoch 1463, Loss: 0.7925162315368652, Final Batch Loss: 0.36461442708969116\n",
      "Epoch 1464, Loss: 0.7378172278404236, Final Batch Loss: 0.3390178382396698\n",
      "Epoch 1465, Loss: 0.7147305607795715, Final Batch Loss: 0.35545527935028076\n",
      "Epoch 1466, Loss: 0.7448121905326843, Final Batch Loss: 0.35168522596359253\n",
      "Epoch 1467, Loss: 0.7365354597568512, Final Batch Loss: 0.3912929892539978\n",
      "Epoch 1468, Loss: 0.761446088552475, Final Batch Loss: 0.45335060358047485\n",
      "Epoch 1469, Loss: 0.6992473602294922, Final Batch Loss: 0.3503323197364807\n",
      "Epoch 1470, Loss: 0.7253326177597046, Final Batch Loss: 0.37675172090530396\n",
      "Epoch 1471, Loss: 0.698161393404007, Final Batch Loss: 0.376941055059433\n",
      "Epoch 1472, Loss: 0.7002765834331512, Final Batch Loss: 0.3546144664287567\n",
      "Epoch 1473, Loss: 0.6948950886726379, Final Batch Loss: 0.3206501603126526\n",
      "Epoch 1474, Loss: 0.7606544494628906, Final Batch Loss: 0.378865510225296\n",
      "Epoch 1475, Loss: 0.7205561995506287, Final Batch Loss: 0.3394882380962372\n",
      "Epoch 1476, Loss: 0.7101744115352631, Final Batch Loss: 0.3431594669818878\n",
      "Epoch 1477, Loss: 0.6803086698055267, Final Batch Loss: 0.32014381885528564\n",
      "Epoch 1478, Loss: 0.6887979507446289, Final Batch Loss: 0.3447941839694977\n",
      "Epoch 1479, Loss: 0.7381649911403656, Final Batch Loss: 0.38006725907325745\n",
      "Epoch 1480, Loss: 0.7373224198818207, Final Batch Loss: 0.3782687783241272\n",
      "Epoch 1481, Loss: 0.6907852590084076, Final Batch Loss: 0.3498241603374481\n",
      "Epoch 1482, Loss: 0.7158187627792358, Final Batch Loss: 0.3650698661804199\n",
      "Epoch 1483, Loss: 0.6889562606811523, Final Batch Loss: 0.3538939356803894\n",
      "Epoch 1484, Loss: 0.7323341965675354, Final Batch Loss: 0.4019199013710022\n",
      "Epoch 1485, Loss: 0.7359941601753235, Final Batch Loss: 0.36891302466392517\n",
      "Epoch 1486, Loss: 0.6665170788764954, Final Batch Loss: 0.33791372179985046\n",
      "Epoch 1487, Loss: 0.7456635534763336, Final Batch Loss: 0.4073670506477356\n",
      "Epoch 1488, Loss: 0.6903630793094635, Final Batch Loss: 0.31208929419517517\n",
      "Epoch 1489, Loss: 0.7023420333862305, Final Batch Loss: 0.3911304473876953\n",
      "Epoch 1490, Loss: 0.6862680613994598, Final Batch Loss: 0.33653372526168823\n",
      "Epoch 1491, Loss: 0.6543682515621185, Final Batch Loss: 0.3025895655155182\n",
      "Epoch 1492, Loss: 0.6870771944522858, Final Batch Loss: 0.3147903382778168\n",
      "Epoch 1493, Loss: 0.8009844422340393, Final Batch Loss: 0.45452210307121277\n",
      "Epoch 1494, Loss: 0.6937102675437927, Final Batch Loss: 0.38044121861457825\n",
      "Epoch 1495, Loss: 0.6680735647678375, Final Batch Loss: 0.3487904369831085\n",
      "Epoch 1496, Loss: 0.7229078114032745, Final Batch Loss: 0.34523963928222656\n",
      "Epoch 1497, Loss: 0.693267434835434, Final Batch Loss: 0.3645702600479126\n",
      "Epoch 1498, Loss: 0.7389680445194244, Final Batch Loss: 0.38634979724884033\n",
      "Epoch 1499, Loss: 0.6835007667541504, Final Batch Loss: 0.3780626058578491\n",
      "Epoch 1500, Loss: 0.6646353006362915, Final Batch Loss: 0.34231245517730713\n",
      "Epoch 1501, Loss: 0.7069869935512543, Final Batch Loss: 0.37571781873703003\n",
      "Epoch 1502, Loss: 0.7231273949146271, Final Batch Loss: 0.37486734986305237\n",
      "Epoch 1503, Loss: 0.7028377950191498, Final Batch Loss: 0.3678625226020813\n",
      "Epoch 1504, Loss: 0.7278898358345032, Final Batch Loss: 0.38545891642570496\n",
      "Epoch 1505, Loss: 0.6727319061756134, Final Batch Loss: 0.3419415056705475\n",
      "Epoch 1506, Loss: 0.7081207036972046, Final Batch Loss: 0.35577133297920227\n",
      "Epoch 1507, Loss: 0.6935072541236877, Final Batch Loss: 0.3297913372516632\n",
      "Epoch 1508, Loss: 0.6803481876850128, Final Batch Loss: 0.3763652443885803\n",
      "Epoch 1509, Loss: 0.6908708214759827, Final Batch Loss: 0.36865222454071045\n",
      "Epoch 1510, Loss: 0.663571685552597, Final Batch Loss: 0.3125137388706207\n",
      "Epoch 1511, Loss: 0.6915131211280823, Final Batch Loss: 0.32722708582878113\n",
      "Epoch 1512, Loss: 0.7399530410766602, Final Batch Loss: 0.4176725447177887\n",
      "Epoch 1513, Loss: 0.6888005435466766, Final Batch Loss: 0.35041069984436035\n",
      "Epoch 1514, Loss: 0.6733781695365906, Final Batch Loss: 0.3200801610946655\n",
      "Epoch 1515, Loss: 0.6770709156990051, Final Batch Loss: 0.30787795782089233\n",
      "Epoch 1516, Loss: 0.7148716747760773, Final Batch Loss: 0.33432266116142273\n",
      "Epoch 1517, Loss: 0.6602188050746918, Final Batch Loss: 0.29223760962486267\n",
      "Epoch 1518, Loss: 0.6842969357967377, Final Batch Loss: 0.3620975613594055\n",
      "Epoch 1519, Loss: 0.670220136642456, Final Batch Loss: 0.32522204518318176\n",
      "Epoch 1520, Loss: 0.6304546296596527, Final Batch Loss: 0.25690028071403503\n",
      "Epoch 1521, Loss: 0.6874714195728302, Final Batch Loss: 0.40078118443489075\n",
      "Epoch 1522, Loss: 0.7266521155834198, Final Batch Loss: 0.3725113570690155\n",
      "Epoch 1523, Loss: 0.6888357698917389, Final Batch Loss: 0.3500543534755707\n",
      "Epoch 1524, Loss: 0.6590054333209991, Final Batch Loss: 0.3635324239730835\n",
      "Epoch 1525, Loss: 0.6289006769657135, Final Batch Loss: 0.3285006284713745\n",
      "Epoch 1526, Loss: 0.6564821302890778, Final Batch Loss: 0.32662415504455566\n",
      "Epoch 1527, Loss: 0.6196788847446442, Final Batch Loss: 0.28657329082489014\n",
      "Epoch 1528, Loss: 0.6432887017726898, Final Batch Loss: 0.2899170219898224\n",
      "Epoch 1529, Loss: 0.7022256255149841, Final Batch Loss: 0.35285046696662903\n",
      "Epoch 1530, Loss: 0.6526561081409454, Final Batch Loss: 0.2942817807197571\n",
      "Epoch 1531, Loss: 0.7063274681568146, Final Batch Loss: 0.33419427275657654\n",
      "Epoch 1532, Loss: 0.6607081592082977, Final Batch Loss: 0.337147980928421\n",
      "Epoch 1533, Loss: 0.6685814559459686, Final Batch Loss: 0.29066184163093567\n",
      "Epoch 1534, Loss: 0.6785484850406647, Final Batch Loss: 0.3050191104412079\n",
      "Epoch 1535, Loss: 0.7015932500362396, Final Batch Loss: 0.32344990968704224\n",
      "Epoch 1536, Loss: 0.6373617053031921, Final Batch Loss: 0.3195057511329651\n",
      "Epoch 1537, Loss: 0.638215184211731, Final Batch Loss: 0.292996346950531\n",
      "Epoch 1538, Loss: 0.6715878248214722, Final Batch Loss: 0.352175235748291\n",
      "Epoch 1539, Loss: 0.6652541756629944, Final Batch Loss: 0.3226511478424072\n",
      "Epoch 1540, Loss: 0.6567852795124054, Final Batch Loss: 0.3311750888824463\n",
      "Epoch 1541, Loss: 0.657440721988678, Final Batch Loss: 0.3398931324481964\n",
      "Epoch 1542, Loss: 0.662731409072876, Final Batch Loss: 0.30710625648498535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1543, Loss: 0.6965512335300446, Final Batch Loss: 0.3316822350025177\n",
      "Epoch 1544, Loss: 0.6859894096851349, Final Batch Loss: 0.3792097568511963\n",
      "Epoch 1545, Loss: 0.6531157195568085, Final Batch Loss: 0.3291102945804596\n",
      "Epoch 1546, Loss: 0.7392226159572601, Final Batch Loss: 0.38973531126976013\n",
      "Epoch 1547, Loss: 0.6695745587348938, Final Batch Loss: 0.3238912522792816\n",
      "Epoch 1548, Loss: 0.667339950799942, Final Batch Loss: 0.357989639043808\n",
      "Epoch 1549, Loss: 0.6876653730869293, Final Batch Loss: 0.33934247493743896\n",
      "Epoch 1550, Loss: 0.6761853992938995, Final Batch Loss: 0.31527215242385864\n",
      "Epoch 1551, Loss: 0.6528392136096954, Final Batch Loss: 0.30773141980171204\n",
      "Epoch 1552, Loss: 0.6699754297733307, Final Batch Loss: 0.3224957287311554\n",
      "Epoch 1553, Loss: 0.7341691553592682, Final Batch Loss: 0.4235040843486786\n",
      "Epoch 1554, Loss: 0.6608387231826782, Final Batch Loss: 0.36421889066696167\n",
      "Epoch 1555, Loss: 0.6470719277858734, Final Batch Loss: 0.3130102753639221\n",
      "Epoch 1556, Loss: 0.6266738474369049, Final Batch Loss: 0.3450985550880432\n",
      "Epoch 1557, Loss: 0.6781412661075592, Final Batch Loss: 0.36423859000205994\n",
      "Epoch 1558, Loss: 0.6846356391906738, Final Batch Loss: 0.3706345856189728\n",
      "Epoch 1559, Loss: 0.6547567844390869, Final Batch Loss: 0.3507508635520935\n",
      "Epoch 1560, Loss: 0.6515447795391083, Final Batch Loss: 0.31354743242263794\n",
      "Epoch 1561, Loss: 0.6480994522571564, Final Batch Loss: 0.3201148509979248\n",
      "Epoch 1562, Loss: 0.6990582942962646, Final Batch Loss: 0.357450932264328\n",
      "Epoch 1563, Loss: 0.6160243153572083, Final Batch Loss: 0.35256171226501465\n",
      "Epoch 1564, Loss: 0.6163052320480347, Final Batch Loss: 0.2901493012905121\n",
      "Epoch 1565, Loss: 0.6590262949466705, Final Batch Loss: 0.35574600100517273\n",
      "Epoch 1566, Loss: 0.6538336873054504, Final Batch Loss: 0.2922455668449402\n",
      "Epoch 1567, Loss: 0.743426650762558, Final Batch Loss: 0.42403608560562134\n",
      "Epoch 1568, Loss: 0.622820109128952, Final Batch Loss: 0.27201882004737854\n",
      "Epoch 1569, Loss: 0.6015716195106506, Final Batch Loss: 0.29079383611679077\n",
      "Epoch 1570, Loss: 0.6998806595802307, Final Batch Loss: 0.3775649964809418\n",
      "Epoch 1571, Loss: 0.6735523045063019, Final Batch Loss: 0.35036271810531616\n",
      "Epoch 1572, Loss: 0.6638628840446472, Final Batch Loss: 0.3085705637931824\n",
      "Epoch 1573, Loss: 0.6614364385604858, Final Batch Loss: 0.3735042214393616\n",
      "Epoch 1574, Loss: 0.6296383738517761, Final Batch Loss: 0.2793509066104889\n",
      "Epoch 1575, Loss: 0.61506187915802, Final Batch Loss: 0.27740904688835144\n",
      "Epoch 1576, Loss: 0.633267879486084, Final Batch Loss: 0.29358041286468506\n",
      "Epoch 1577, Loss: 0.6590089499950409, Final Batch Loss: 0.36220741271972656\n",
      "Epoch 1578, Loss: 0.6237211227416992, Final Batch Loss: 0.2839772403240204\n",
      "Epoch 1579, Loss: 0.6113516390323639, Final Batch Loss: 0.27545076608657837\n",
      "Epoch 1580, Loss: 0.6401677131652832, Final Batch Loss: 0.3247942328453064\n",
      "Epoch 1581, Loss: 0.6435522437095642, Final Batch Loss: 0.3415016233921051\n",
      "Epoch 1582, Loss: 0.671109139919281, Final Batch Loss: 0.3217678368091583\n",
      "Epoch 1583, Loss: 0.6488813161849976, Final Batch Loss: 0.3580631911754608\n",
      "Epoch 1584, Loss: 0.6780285835266113, Final Batch Loss: 0.3166198432445526\n",
      "Epoch 1585, Loss: 0.6478903889656067, Final Batch Loss: 0.29244479537010193\n",
      "Epoch 1586, Loss: 0.6497562229633331, Final Batch Loss: 0.35754454135894775\n",
      "Epoch 1587, Loss: 0.6281532943248749, Final Batch Loss: 0.35832467675209045\n",
      "Epoch 1588, Loss: 0.6321865022182465, Final Batch Loss: 0.27976030111312866\n",
      "Epoch 1589, Loss: 0.64289391040802, Final Batch Loss: 0.3059654235839844\n",
      "Epoch 1590, Loss: 0.6140751242637634, Final Batch Loss: 0.2972148656845093\n",
      "Epoch 1591, Loss: 0.5973328053951263, Final Batch Loss: 0.31084364652633667\n",
      "Epoch 1592, Loss: 0.6376814246177673, Final Batch Loss: 0.3137526214122772\n",
      "Epoch 1593, Loss: 0.6463056206703186, Final Batch Loss: 0.3181094527244568\n",
      "Epoch 1594, Loss: 0.6781778633594513, Final Batch Loss: 0.3809897303581238\n",
      "Epoch 1595, Loss: 0.6573175489902496, Final Batch Loss: 0.33228954672813416\n",
      "Epoch 1596, Loss: 0.5966568887233734, Final Batch Loss: 0.3633723556995392\n",
      "Epoch 1597, Loss: 0.6253563463687897, Final Batch Loss: 0.3251936733722687\n",
      "Epoch 1598, Loss: 0.6441362202167511, Final Batch Loss: 0.3426033854484558\n",
      "Epoch 1599, Loss: 0.6560175120830536, Final Batch Loss: 0.3360292315483093\n",
      "Epoch 1600, Loss: 0.6611999869346619, Final Batch Loss: 0.33414366841316223\n",
      "Epoch 1601, Loss: 0.645028293132782, Final Batch Loss: 0.33171403408050537\n",
      "Epoch 1602, Loss: 0.6491186320781708, Final Batch Loss: 0.2990871071815491\n",
      "Epoch 1603, Loss: 0.7231070399284363, Final Batch Loss: 0.3655426502227783\n",
      "Epoch 1604, Loss: 0.676631510257721, Final Batch Loss: 0.3479732871055603\n",
      "Epoch 1605, Loss: 0.6677435636520386, Final Batch Loss: 0.3533138632774353\n",
      "Epoch 1606, Loss: 0.6725208163261414, Final Batch Loss: 0.32280221581459045\n",
      "Epoch 1607, Loss: 0.6212763488292694, Final Batch Loss: 0.33592021465301514\n",
      "Epoch 1608, Loss: 0.6244987845420837, Final Batch Loss: 0.31180423498153687\n",
      "Epoch 1609, Loss: 0.6116558611392975, Final Batch Loss: 0.3137187361717224\n",
      "Epoch 1610, Loss: 0.6245640218257904, Final Batch Loss: 0.3270181119441986\n",
      "Epoch 1611, Loss: 0.6422603130340576, Final Batch Loss: 0.3130491375923157\n",
      "Epoch 1612, Loss: 0.6415624022483826, Final Batch Loss: 0.3250476121902466\n",
      "Epoch 1613, Loss: 0.5776768028736115, Final Batch Loss: 0.2646748721599579\n",
      "Epoch 1614, Loss: 0.6791801750659943, Final Batch Loss: 0.3425443470478058\n",
      "Epoch 1615, Loss: 0.6349370777606964, Final Batch Loss: 0.31169813871383667\n",
      "Epoch 1616, Loss: 0.6268264651298523, Final Batch Loss: 0.3639654815196991\n",
      "Epoch 1617, Loss: 0.6166976094245911, Final Batch Loss: 0.26920560002326965\n",
      "Epoch 1618, Loss: 0.6469572186470032, Final Batch Loss: 0.3200870156288147\n",
      "Epoch 1619, Loss: 0.6486307978630066, Final Batch Loss: 0.303469717502594\n",
      "Epoch 1620, Loss: 0.6284368634223938, Final Batch Loss: 0.35708677768707275\n",
      "Epoch 1621, Loss: 0.6394077837467194, Final Batch Loss: 0.2667214870452881\n",
      "Epoch 1622, Loss: 0.6673860847949982, Final Batch Loss: 0.33844903111457825\n",
      "Epoch 1623, Loss: 0.6415577232837677, Final Batch Loss: 0.3695628345012665\n",
      "Epoch 1624, Loss: 0.6487442553043365, Final Batch Loss: 0.3149870038032532\n",
      "Epoch 1625, Loss: 0.6507725417613983, Final Batch Loss: 0.3173479437828064\n",
      "Epoch 1626, Loss: 0.6463637351989746, Final Batch Loss: 0.3518255054950714\n",
      "Epoch 1627, Loss: 0.654211699962616, Final Batch Loss: 0.35172343254089355\n",
      "Epoch 1628, Loss: 0.5761370062828064, Final Batch Loss: 0.28487637639045715\n",
      "Epoch 1629, Loss: 0.6122823059558868, Final Batch Loss: 0.33923354744911194\n",
      "Epoch 1630, Loss: 0.598668098449707, Final Batch Loss: 0.3176034986972809\n",
      "Epoch 1631, Loss: 0.5927634239196777, Final Batch Loss: 0.24445655941963196\n",
      "Epoch 1632, Loss: 0.6121731698513031, Final Batch Loss: 0.34776318073272705\n",
      "Epoch 1633, Loss: 0.652046799659729, Final Batch Loss: 0.3540535271167755\n",
      "Epoch 1634, Loss: 0.6357797980308533, Final Batch Loss: 0.29895105957984924\n",
      "Epoch 1635, Loss: 0.6298927664756775, Final Batch Loss: 0.32320475578308105\n",
      "Epoch 1636, Loss: 0.6189574599266052, Final Batch Loss: 0.31626662611961365\n",
      "Epoch 1637, Loss: 0.6366152465343475, Final Batch Loss: 0.33433687686920166\n",
      "Epoch 1638, Loss: 0.6359650492668152, Final Batch Loss: 0.35463616251945496\n",
      "Epoch 1639, Loss: 0.6312842667102814, Final Batch Loss: 0.3002975881099701\n",
      "Epoch 1640, Loss: 0.6073428392410278, Final Batch Loss: 0.33082225918769836\n",
      "Epoch 1641, Loss: 0.5642125010490417, Final Batch Loss: 0.28595107793807983\n",
      "Epoch 1642, Loss: 0.6242526769638062, Final Batch Loss: 0.2979927957057953\n",
      "Epoch 1643, Loss: 0.6110731065273285, Final Batch Loss: 0.3295559883117676\n",
      "Epoch 1644, Loss: 0.630072146654129, Final Batch Loss: 0.2854780852794647\n",
      "Epoch 1645, Loss: 0.6130711436271667, Final Batch Loss: 0.30284634232521057\n",
      "Epoch 1646, Loss: 0.6462644636631012, Final Batch Loss: 0.3670736253261566\n",
      "Epoch 1647, Loss: 0.6136090755462646, Final Batch Loss: 0.3042202293872833\n",
      "Epoch 1648, Loss: 0.6867917478084564, Final Batch Loss: 0.4094897508621216\n",
      "Epoch 1649, Loss: 0.6035783290863037, Final Batch Loss: 0.3057231903076172\n",
      "Epoch 1650, Loss: 0.5857883095741272, Final Batch Loss: 0.27811816334724426\n",
      "Epoch 1651, Loss: 0.6577197909355164, Final Batch Loss: 0.294003427028656\n",
      "Epoch 1652, Loss: 0.6005066335201263, Final Batch Loss: 0.30546221137046814\n",
      "Epoch 1653, Loss: 0.6195469200611115, Final Batch Loss: 0.29568684101104736\n",
      "Epoch 1654, Loss: 0.6209812462329865, Final Batch Loss: 0.2750784754753113\n",
      "Epoch 1655, Loss: 0.5995070338249207, Final Batch Loss: 0.31186822056770325\n",
      "Epoch 1656, Loss: 0.6064363420009613, Final Batch Loss: 0.3062000870704651\n",
      "Epoch 1657, Loss: 0.6759832799434662, Final Batch Loss: 0.2946234345436096\n",
      "Epoch 1658, Loss: 0.636473149061203, Final Batch Loss: 0.29773879051208496\n",
      "Epoch 1659, Loss: 0.5637460649013519, Final Batch Loss: 0.3157787024974823\n",
      "Epoch 1660, Loss: 0.6082649528980255, Final Batch Loss: 0.3265557587146759\n",
      "Epoch 1661, Loss: 0.6120023727416992, Final Batch Loss: 0.31356632709503174\n",
      "Epoch 1662, Loss: 0.6102104485034943, Final Batch Loss: 0.3389267325401306\n",
      "Epoch 1663, Loss: 0.6195531785488129, Final Batch Loss: 0.32327011227607727\n",
      "Epoch 1664, Loss: 0.5890064537525177, Final Batch Loss: 0.2647286355495453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1665, Loss: 0.6610592603683472, Final Batch Loss: 0.33306336402893066\n",
      "Epoch 1666, Loss: 0.6005101799964905, Final Batch Loss: 0.27492469549179077\n",
      "Epoch 1667, Loss: 0.6074405312538147, Final Batch Loss: 0.288720965385437\n",
      "Epoch 1668, Loss: 0.5738751888275146, Final Batch Loss: 0.2627490758895874\n",
      "Epoch 1669, Loss: 0.6323758959770203, Final Batch Loss: 0.3085244596004486\n",
      "Epoch 1670, Loss: 0.6544090211391449, Final Batch Loss: 0.36149707436561584\n",
      "Epoch 1671, Loss: 0.6507938802242279, Final Batch Loss: 0.3504161834716797\n",
      "Epoch 1672, Loss: 0.6149555742740631, Final Batch Loss: 0.3269836902618408\n",
      "Epoch 1673, Loss: 0.6389859020709991, Final Batch Loss: 0.35572510957717896\n",
      "Epoch 1674, Loss: 0.6047567427158356, Final Batch Loss: 0.3196813464164734\n",
      "Epoch 1675, Loss: 0.6170796751976013, Final Batch Loss: 0.3169427216053009\n",
      "Epoch 1676, Loss: 0.5641895532608032, Final Batch Loss: 0.2679881453514099\n",
      "Epoch 1677, Loss: 0.5930326879024506, Final Batch Loss: 0.29154130816459656\n",
      "Epoch 1678, Loss: 0.6203935146331787, Final Batch Loss: 0.3224236071109772\n",
      "Epoch 1679, Loss: 0.6204307973384857, Final Batch Loss: 0.3122204840183258\n",
      "Epoch 1680, Loss: 0.5657786428928375, Final Batch Loss: 0.28645262122154236\n",
      "Epoch 1681, Loss: 0.6033887565135956, Final Batch Loss: 0.26305899024009705\n",
      "Epoch 1682, Loss: 0.6328200101852417, Final Batch Loss: 0.3121905028820038\n",
      "Epoch 1683, Loss: 0.5810562968254089, Final Batch Loss: 0.27295154333114624\n",
      "Epoch 1684, Loss: 0.6067704558372498, Final Batch Loss: 0.2866465151309967\n",
      "Epoch 1685, Loss: 0.5991802513599396, Final Batch Loss: 0.3060346245765686\n",
      "Epoch 1686, Loss: 0.607243150472641, Final Batch Loss: 0.321074903011322\n",
      "Epoch 1687, Loss: 0.6010619401931763, Final Batch Loss: 0.34494614601135254\n",
      "Epoch 1688, Loss: 0.6057686507701874, Final Batch Loss: 0.26556381583213806\n",
      "Epoch 1689, Loss: 0.6180832386016846, Final Batch Loss: 0.2945954501628876\n",
      "Epoch 1690, Loss: 0.5987290740013123, Final Batch Loss: 0.28885698318481445\n",
      "Epoch 1691, Loss: 0.6208901405334473, Final Batch Loss: 0.3239690959453583\n",
      "Epoch 1692, Loss: 0.5720334649085999, Final Batch Loss: 0.28202229738235474\n",
      "Epoch 1693, Loss: 0.6151112020015717, Final Batch Loss: 0.3051820397377014\n",
      "Epoch 1694, Loss: 0.5984773635864258, Final Batch Loss: 0.2614002227783203\n",
      "Epoch 1695, Loss: 0.6089796125888824, Final Batch Loss: 0.3038914203643799\n",
      "Epoch 1696, Loss: 0.621514081954956, Final Batch Loss: 0.31481000781059265\n",
      "Epoch 1697, Loss: 0.6025179028511047, Final Batch Loss: 0.3216846287250519\n",
      "Epoch 1698, Loss: 0.6292044222354889, Final Batch Loss: 0.3194858431816101\n",
      "Epoch 1699, Loss: 0.5746590197086334, Final Batch Loss: 0.27093252539634705\n",
      "Epoch 1700, Loss: 0.616948276758194, Final Batch Loss: 0.3051547408103943\n",
      "Epoch 1701, Loss: 0.5870808064937592, Final Batch Loss: 0.32750681042671204\n",
      "Epoch 1702, Loss: 0.5642761290073395, Final Batch Loss: 0.25463005900382996\n",
      "Epoch 1703, Loss: 0.6052474677562714, Final Batch Loss: 0.3233358860015869\n",
      "Epoch 1704, Loss: 0.6003442406654358, Final Batch Loss: 0.3397361636161804\n",
      "Epoch 1705, Loss: 0.6326052248477936, Final Batch Loss: 0.3150884211063385\n",
      "Epoch 1706, Loss: 0.5875035226345062, Final Batch Loss: 0.29282763600349426\n",
      "Epoch 1707, Loss: 0.5182537138462067, Final Batch Loss: 0.25061407685279846\n",
      "Epoch 1708, Loss: 0.6212436258792877, Final Batch Loss: 0.2771526873111725\n",
      "Epoch 1709, Loss: 0.6154392659664154, Final Batch Loss: 0.31540441513061523\n",
      "Epoch 1710, Loss: 0.5864337384700775, Final Batch Loss: 0.28702670335769653\n",
      "Epoch 1711, Loss: 0.6060513257980347, Final Batch Loss: 0.2979719936847687\n",
      "Epoch 1712, Loss: 0.5907078087329865, Final Batch Loss: 0.2875981330871582\n",
      "Epoch 1713, Loss: 0.6351827085018158, Final Batch Loss: 0.2979739010334015\n",
      "Epoch 1714, Loss: 0.5826653838157654, Final Batch Loss: 0.2775714099407196\n",
      "Epoch 1715, Loss: 0.6171134114265442, Final Batch Loss: 0.31792446970939636\n",
      "Epoch 1716, Loss: 0.6234163343906403, Final Batch Loss: 0.3128401041030884\n",
      "Epoch 1717, Loss: 0.5657879710197449, Final Batch Loss: 0.28004366159439087\n",
      "Epoch 1718, Loss: 0.5957464575767517, Final Batch Loss: 0.2935733497142792\n",
      "Epoch 1719, Loss: 0.5903792381286621, Final Batch Loss: 0.28903406858444214\n",
      "Epoch 1720, Loss: 0.5933528244495392, Final Batch Loss: 0.3213203251361847\n",
      "Epoch 1721, Loss: 0.5414249300956726, Final Batch Loss: 0.24767714738845825\n",
      "Epoch 1722, Loss: 0.6063344180583954, Final Batch Loss: 0.29567989706993103\n",
      "Epoch 1723, Loss: 0.6657093465328217, Final Batch Loss: 0.3561365008354187\n",
      "Epoch 1724, Loss: 0.6361907720565796, Final Batch Loss: 0.31466802954673767\n",
      "Epoch 1725, Loss: 0.6184464991092682, Final Batch Loss: 0.3351047933101654\n",
      "Epoch 1726, Loss: 0.6065404117107391, Final Batch Loss: 0.29147058725357056\n",
      "Epoch 1727, Loss: 0.5872502028942108, Final Batch Loss: 0.25734132528305054\n",
      "Epoch 1728, Loss: 0.6123300790786743, Final Batch Loss: 0.2854360342025757\n",
      "Epoch 1729, Loss: 0.6357064843177795, Final Batch Loss: 0.305929958820343\n",
      "Epoch 1730, Loss: 0.5944039225578308, Final Batch Loss: 0.3001542091369629\n",
      "Epoch 1731, Loss: 0.5915220081806183, Final Batch Loss: 0.28545087575912476\n",
      "Epoch 1732, Loss: 0.6203835606575012, Final Batch Loss: 0.32296720147132874\n",
      "Epoch 1733, Loss: 0.5640202462673187, Final Batch Loss: 0.31190261244773865\n",
      "Epoch 1734, Loss: 0.6377881169319153, Final Batch Loss: 0.33894893527030945\n",
      "Epoch 1735, Loss: 0.5977651476860046, Final Batch Loss: 0.32083067297935486\n",
      "Epoch 1736, Loss: 0.5964832603931427, Final Batch Loss: 0.26809781789779663\n",
      "Epoch 1737, Loss: 0.6009542644023895, Final Batch Loss: 0.3115123212337494\n",
      "Epoch 1738, Loss: 0.6002225875854492, Final Batch Loss: 0.31220048666000366\n",
      "Epoch 1739, Loss: 0.5980180203914642, Final Batch Loss: 0.29330408573150635\n",
      "Epoch 1740, Loss: 0.655487596988678, Final Batch Loss: 0.3186640739440918\n",
      "Epoch 1741, Loss: 0.613006055355072, Final Batch Loss: 0.23371392488479614\n",
      "Epoch 1742, Loss: 0.5696564614772797, Final Batch Loss: 0.28464511036872864\n",
      "Epoch 1743, Loss: 0.6154455542564392, Final Batch Loss: 0.3160991966724396\n",
      "Epoch 1744, Loss: 0.5449986755847931, Final Batch Loss: 0.2497168779373169\n",
      "Epoch 1745, Loss: 0.5919508635997772, Final Batch Loss: 0.27174732089042664\n",
      "Epoch 1746, Loss: 0.5753768384456635, Final Batch Loss: 0.2840607762336731\n",
      "Epoch 1747, Loss: 0.6039368808269501, Final Batch Loss: 0.3487520217895508\n",
      "Epoch 1748, Loss: 0.6150906383991241, Final Batch Loss: 0.2989868223667145\n",
      "Epoch 1749, Loss: 0.5648270547389984, Final Batch Loss: 0.27852553129196167\n",
      "Epoch 1750, Loss: 0.5811675190925598, Final Batch Loss: 0.29392099380493164\n",
      "Epoch 1751, Loss: 0.5639388263225555, Final Batch Loss: 0.2773248553276062\n",
      "Epoch 1752, Loss: 0.5499120056629181, Final Batch Loss: 0.26002123951911926\n",
      "Epoch 1753, Loss: 0.5991696119308472, Final Batch Loss: 0.3058602809906006\n",
      "Epoch 1754, Loss: 0.5624865889549255, Final Batch Loss: 0.27014827728271484\n",
      "Epoch 1755, Loss: 0.5716760158538818, Final Batch Loss: 0.25897476077079773\n",
      "Epoch 1756, Loss: 0.578187882900238, Final Batch Loss: 0.3091107904911041\n",
      "Epoch 1757, Loss: 0.5567809641361237, Final Batch Loss: 0.28512588143348694\n",
      "Epoch 1758, Loss: 0.5852180123329163, Final Batch Loss: 0.30674028396606445\n",
      "Epoch 1759, Loss: 0.5273354053497314, Final Batch Loss: 0.23285213112831116\n",
      "Epoch 1760, Loss: 0.5749795734882355, Final Batch Loss: 0.2801072597503662\n",
      "Epoch 1761, Loss: 0.544552206993103, Final Batch Loss: 0.2591744065284729\n",
      "Epoch 1762, Loss: 0.5859275460243225, Final Batch Loss: 0.28193920850753784\n",
      "Epoch 1763, Loss: 0.5431515872478485, Final Batch Loss: 0.24194729328155518\n",
      "Epoch 1764, Loss: 0.5700928270816803, Final Batch Loss: 0.27799031138420105\n",
      "Epoch 1765, Loss: 0.5932791233062744, Final Batch Loss: 0.2895064055919647\n",
      "Epoch 1766, Loss: 0.5362851321697235, Final Batch Loss: 0.2754363715648651\n",
      "Epoch 1767, Loss: 0.5822331309318542, Final Batch Loss: 0.2903122007846832\n",
      "Epoch 1768, Loss: 0.5720022171735764, Final Batch Loss: 0.23457024991512299\n",
      "Epoch 1769, Loss: 0.5902961492538452, Final Batch Loss: 0.302381306886673\n",
      "Epoch 1770, Loss: 0.5949756801128387, Final Batch Loss: 0.28105923533439636\n",
      "Epoch 1771, Loss: 0.5460786819458008, Final Batch Loss: 0.28776371479034424\n",
      "Epoch 1772, Loss: 0.5629785060882568, Final Batch Loss: 0.30443274974823\n",
      "Epoch 1773, Loss: 0.5789982378482819, Final Batch Loss: 0.2955385148525238\n",
      "Epoch 1774, Loss: 0.5636516213417053, Final Batch Loss: 0.29969313740730286\n",
      "Epoch 1775, Loss: 0.5409596562385559, Final Batch Loss: 0.2733970284461975\n",
      "Epoch 1776, Loss: 0.5673458576202393, Final Batch Loss: 0.27425187826156616\n",
      "Epoch 1777, Loss: 0.5830588042736053, Final Batch Loss: 0.29821398854255676\n",
      "Epoch 1778, Loss: 0.5690445303916931, Final Batch Loss: 0.27403995394706726\n",
      "Epoch 1779, Loss: 0.5659990906715393, Final Batch Loss: 0.2701505422592163\n",
      "Epoch 1780, Loss: 0.5812512636184692, Final Batch Loss: 0.26193520426750183\n",
      "Epoch 1781, Loss: 0.6268052756786346, Final Batch Loss: 0.3727492094039917\n",
      "Epoch 1782, Loss: 0.5875428318977356, Final Batch Loss: 0.29178738594055176\n",
      "Epoch 1783, Loss: 0.6418108344078064, Final Batch Loss: 0.33191001415252686\n",
      "Epoch 1784, Loss: 0.5595430135726929, Final Batch Loss: 0.2953829765319824\n",
      "Epoch 1785, Loss: 0.5341517627239227, Final Batch Loss: 0.2691633999347687\n",
      "Epoch 1786, Loss: 0.5943566858768463, Final Batch Loss: 0.28741079568862915\n",
      "Epoch 1787, Loss: 0.5924268662929535, Final Batch Loss: 0.33004194498062134\n",
      "Epoch 1788, Loss: 0.6085422039031982, Final Batch Loss: 0.31338104605674744\n",
      "Epoch 1789, Loss: 0.5726262480020523, Final Batch Loss: 0.3241417407989502\n",
      "Epoch 1790, Loss: 0.6293608248233795, Final Batch Loss: 0.3451136350631714\n",
      "Epoch 1791, Loss: 0.7066588699817657, Final Batch Loss: 0.35120677947998047\n",
      "Epoch 1792, Loss: 0.5730337500572205, Final Batch Loss: 0.2702215313911438\n",
      "Epoch 1793, Loss: 0.5638626515865326, Final Batch Loss: 0.3054119050502777\n",
      "Epoch 1794, Loss: 0.5859985947608948, Final Batch Loss: 0.30415216088294983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1795, Loss: 0.5534245371818542, Final Batch Loss: 0.2668911814689636\n",
      "Epoch 1796, Loss: 0.6001852452754974, Final Batch Loss: 0.31253543496131897\n",
      "Epoch 1797, Loss: 0.5377655327320099, Final Batch Loss: 0.27593371272087097\n",
      "Epoch 1798, Loss: 0.5858421921730042, Final Batch Loss: 0.25289374589920044\n",
      "Epoch 1799, Loss: 0.5872189402580261, Final Batch Loss: 0.329233855009079\n",
      "Epoch 1800, Loss: 0.5844343304634094, Final Batch Loss: 0.25597143173217773\n",
      "Epoch 1801, Loss: 0.5446053445339203, Final Batch Loss: 0.2696358859539032\n",
      "Epoch 1802, Loss: 0.5967082530260086, Final Batch Loss: 0.24212850630283356\n",
      "Epoch 1803, Loss: 0.5879172682762146, Final Batch Loss: 0.3070129156112671\n",
      "Epoch 1804, Loss: 0.5926941633224487, Final Batch Loss: 0.2943403422832489\n",
      "Epoch 1805, Loss: 0.5256939679384232, Final Batch Loss: 0.21778301894664764\n",
      "Epoch 1806, Loss: 0.5917845666408539, Final Batch Loss: 0.3089536726474762\n",
      "Epoch 1807, Loss: 0.570397138595581, Final Batch Loss: 0.2821124196052551\n",
      "Epoch 1808, Loss: 0.5328510403633118, Final Batch Loss: 0.27702438831329346\n",
      "Epoch 1809, Loss: 0.5994856357574463, Final Batch Loss: 0.3015364110469818\n",
      "Epoch 1810, Loss: 0.5338034331798553, Final Batch Loss: 0.22883012890815735\n",
      "Epoch 1811, Loss: 0.610309898853302, Final Batch Loss: 0.30857840180397034\n",
      "Epoch 1812, Loss: 0.5724213421344757, Final Batch Loss: 0.2963581085205078\n",
      "Epoch 1813, Loss: 0.5481523275375366, Final Batch Loss: 0.27298638224601746\n",
      "Epoch 1814, Loss: 0.5424032807350159, Final Batch Loss: 0.30958497524261475\n",
      "Epoch 1815, Loss: 0.5443645715713501, Final Batch Loss: 0.3057645857334137\n",
      "Epoch 1816, Loss: 0.5240028649568558, Final Batch Loss: 0.2242649644613266\n",
      "Epoch 1817, Loss: 0.5260085612535477, Final Batch Loss: 0.2458348423242569\n",
      "Epoch 1818, Loss: 0.5390612483024597, Final Batch Loss: 0.2732115387916565\n",
      "Epoch 1819, Loss: 0.5702622532844543, Final Batch Loss: 0.28914788365364075\n",
      "Epoch 1820, Loss: 0.5114498883485794, Final Batch Loss: 0.24669592082500458\n",
      "Epoch 1821, Loss: 0.5650793313980103, Final Batch Loss: 0.28186744451522827\n",
      "Epoch 1822, Loss: 0.6000221371650696, Final Batch Loss: 0.31035763025283813\n",
      "Epoch 1823, Loss: 0.5448911488056183, Final Batch Loss: 0.28644120693206787\n",
      "Epoch 1824, Loss: 0.5282688438892365, Final Batch Loss: 0.276901513338089\n",
      "Epoch 1825, Loss: 0.6390344798564911, Final Batch Loss: 0.34490200877189636\n",
      "Epoch 1826, Loss: 0.5590176582336426, Final Batch Loss: 0.27987155318260193\n",
      "Epoch 1827, Loss: 0.6096982359886169, Final Batch Loss: 0.356479674577713\n",
      "Epoch 1828, Loss: 0.5472829639911652, Final Batch Loss: 0.24011972546577454\n",
      "Epoch 1829, Loss: 0.549203485250473, Final Batch Loss: 0.2681182324886322\n",
      "Epoch 1830, Loss: 0.5943941175937653, Final Batch Loss: 0.29642146825790405\n",
      "Epoch 1831, Loss: 0.5143924206495285, Final Batch Loss: 0.2396181970834732\n",
      "Epoch 1832, Loss: 0.550029844045639, Final Batch Loss: 0.2767556607723236\n",
      "Epoch 1833, Loss: 0.519147515296936, Final Batch Loss: 0.2221754789352417\n",
      "Epoch 1834, Loss: 0.5558402240276337, Final Batch Loss: 0.2944237291812897\n",
      "Epoch 1835, Loss: 0.5234279930591583, Final Batch Loss: 0.22654452919960022\n",
      "Epoch 1836, Loss: 0.5866119414567947, Final Batch Loss: 0.22443030774593353\n",
      "Epoch 1837, Loss: 0.5433784127235413, Final Batch Loss: 0.3067271411418915\n",
      "Epoch 1838, Loss: 0.5952675640583038, Final Batch Loss: 0.2989969551563263\n",
      "Epoch 1839, Loss: 0.5457146465778351, Final Batch Loss: 0.24587005376815796\n",
      "Epoch 1840, Loss: 0.5642637312412262, Final Batch Loss: 0.3312860429286957\n",
      "Epoch 1841, Loss: 0.5487114489078522, Final Batch Loss: 0.2533656060695648\n",
      "Epoch 1842, Loss: 0.6480861008167267, Final Batch Loss: 0.3110118508338928\n",
      "Epoch 1843, Loss: 0.5301357954740524, Final Batch Loss: 0.24426142871379852\n",
      "Epoch 1844, Loss: 0.5589658915996552, Final Batch Loss: 0.3179546892642975\n",
      "Epoch 1845, Loss: 0.5994691252708435, Final Batch Loss: 0.25591087341308594\n",
      "Epoch 1846, Loss: 0.538366973400116, Final Batch Loss: 0.25654980540275574\n",
      "Epoch 1847, Loss: 0.5486053973436356, Final Batch Loss: 0.3161133825778961\n",
      "Epoch 1848, Loss: 0.5826611220836639, Final Batch Loss: 0.2967018187046051\n",
      "Epoch 1849, Loss: 0.5417777597904205, Final Batch Loss: 0.28165560960769653\n",
      "Epoch 1850, Loss: 0.5862168967723846, Final Batch Loss: 0.29449039697647095\n",
      "Epoch 1851, Loss: 0.5521740317344666, Final Batch Loss: 0.2723608613014221\n",
      "Epoch 1852, Loss: 0.55339714884758, Final Batch Loss: 0.29159095883369446\n",
      "Epoch 1853, Loss: 0.5822839438915253, Final Batch Loss: 0.3467179834842682\n",
      "Epoch 1854, Loss: 0.5523679554462433, Final Batch Loss: 0.28197500109672546\n",
      "Epoch 1855, Loss: 0.5722435414791107, Final Batch Loss: 0.2574952244758606\n",
      "Epoch 1856, Loss: 0.5530619621276855, Final Batch Loss: 0.2780502438545227\n",
      "Epoch 1857, Loss: 0.5777635276317596, Final Batch Loss: 0.27269577980041504\n",
      "Epoch 1858, Loss: 0.5156342089176178, Final Batch Loss: 0.23397013545036316\n",
      "Epoch 1859, Loss: 0.5231766402721405, Final Batch Loss: 0.2840817868709564\n",
      "Epoch 1860, Loss: 0.5485278069972992, Final Batch Loss: 0.28042131662368774\n",
      "Epoch 1861, Loss: 0.5448243021965027, Final Batch Loss: 0.2946421504020691\n",
      "Epoch 1862, Loss: 0.5745209753513336, Final Batch Loss: 0.319397509098053\n",
      "Epoch 1863, Loss: 0.5803901851177216, Final Batch Loss: 0.30458757281303406\n",
      "Epoch 1864, Loss: 0.5694617033004761, Final Batch Loss: 0.27155378460884094\n",
      "Epoch 1865, Loss: 0.5350504666566849, Final Batch Loss: 0.2873043417930603\n",
      "Epoch 1866, Loss: 0.5331586599349976, Final Batch Loss: 0.22625863552093506\n",
      "Epoch 1867, Loss: 0.5775196254253387, Final Batch Loss: 0.31624582409858704\n",
      "Epoch 1868, Loss: 0.5216262191534042, Final Batch Loss: 0.28797972202301025\n",
      "Epoch 1869, Loss: 0.5756945908069611, Final Batch Loss: 0.2912953197956085\n",
      "Epoch 1870, Loss: 0.5594982206821442, Final Batch Loss: 0.30170348286628723\n",
      "Epoch 1871, Loss: 0.6155497431755066, Final Batch Loss: 0.2746240794658661\n",
      "Epoch 1872, Loss: 0.5404499620199203, Final Batch Loss: 0.29917052388191223\n",
      "Epoch 1873, Loss: 0.5563411116600037, Final Batch Loss: 0.28873011469841003\n",
      "Epoch 1874, Loss: 0.595302402973175, Final Batch Loss: 0.3049144446849823\n",
      "Epoch 1875, Loss: 0.5562858879566193, Final Batch Loss: 0.27144473791122437\n",
      "Epoch 1876, Loss: 0.5424448996782303, Final Batch Loss: 0.32183122634887695\n",
      "Epoch 1877, Loss: 0.5082046389579773, Final Batch Loss: 0.24807682633399963\n",
      "Epoch 1878, Loss: 0.5510683208703995, Final Batch Loss: 0.3253186345100403\n",
      "Epoch 1879, Loss: 0.5797181129455566, Final Batch Loss: 0.2850130796432495\n",
      "Epoch 1880, Loss: 0.48389868438243866, Final Batch Loss: 0.24228718876838684\n",
      "Epoch 1881, Loss: 0.5617097318172455, Final Batch Loss: 0.28290855884552\n",
      "Epoch 1882, Loss: 0.5343657284975052, Final Batch Loss: 0.24304865300655365\n",
      "Epoch 1883, Loss: 0.5228253602981567, Final Batch Loss: 0.2335437834262848\n",
      "Epoch 1884, Loss: 0.538817286491394, Final Batch Loss: 0.3126603066921234\n",
      "Epoch 1885, Loss: 0.555724561214447, Final Batch Loss: 0.2805939018726349\n",
      "Epoch 1886, Loss: 0.5090266466140747, Final Batch Loss: 0.2570195198059082\n",
      "Epoch 1887, Loss: 0.5761531591415405, Final Batch Loss: 0.2980065643787384\n",
      "Epoch 1888, Loss: 0.5158092528581619, Final Batch Loss: 0.24205143749713898\n",
      "Epoch 1889, Loss: 0.5658113360404968, Final Batch Loss: 0.3128948509693146\n",
      "Epoch 1890, Loss: 0.57023486495018, Final Batch Loss: 0.25892820954322815\n",
      "Epoch 1891, Loss: 0.581907719373703, Final Batch Loss: 0.2750081717967987\n",
      "Epoch 1892, Loss: 0.541903555393219, Final Batch Loss: 0.3069336414337158\n",
      "Epoch 1893, Loss: 0.5831001400947571, Final Batch Loss: 0.3116563558578491\n",
      "Epoch 1894, Loss: 0.5683114528656006, Final Batch Loss: 0.3395792245864868\n",
      "Epoch 1895, Loss: 0.591822475194931, Final Batch Loss: 0.2903392016887665\n",
      "Epoch 1896, Loss: 0.5484267175197601, Final Batch Loss: 0.23782843351364136\n",
      "Epoch 1897, Loss: 0.5474584996700287, Final Batch Loss: 0.2551876902580261\n",
      "Epoch 1898, Loss: 0.5037664622068405, Final Batch Loss: 0.27428048849105835\n",
      "Epoch 1899, Loss: 0.5185961276292801, Final Batch Loss: 0.2458718866109848\n",
      "Epoch 1900, Loss: 0.5424210429191589, Final Batch Loss: 0.2773638665676117\n",
      "Epoch 1901, Loss: 0.5505509227514267, Final Batch Loss: 0.32163646817207336\n",
      "Epoch 1902, Loss: 0.5440850257873535, Final Batch Loss: 0.26543667912483215\n",
      "Epoch 1903, Loss: 0.5430488884449005, Final Batch Loss: 0.2541312277317047\n",
      "Epoch 1904, Loss: 0.6052290201187134, Final Batch Loss: 0.3193730115890503\n",
      "Epoch 1905, Loss: 0.508187472820282, Final Batch Loss: 0.24618512392044067\n",
      "Epoch 1906, Loss: 0.5154182612895966, Final Batch Loss: 0.2644311189651489\n",
      "Epoch 1907, Loss: 0.545787513256073, Final Batch Loss: 0.2671374976634979\n",
      "Epoch 1908, Loss: 0.584549605846405, Final Batch Loss: 0.3132407069206238\n",
      "Epoch 1909, Loss: 0.5350306332111359, Final Batch Loss: 0.25527602434158325\n",
      "Epoch 1910, Loss: 0.5602522492408752, Final Batch Loss: 0.30415961146354675\n",
      "Epoch 1911, Loss: 0.5469483137130737, Final Batch Loss: 0.25113993883132935\n",
      "Epoch 1912, Loss: 0.5271909534931183, Final Batch Loss: 0.27409112453460693\n",
      "Epoch 1913, Loss: 0.517485722899437, Final Batch Loss: 0.22615168988704681\n",
      "Epoch 1914, Loss: 0.5967692136764526, Final Batch Loss: 0.3224499523639679\n",
      "Epoch 1915, Loss: 0.5676980018615723, Final Batch Loss: 0.28282272815704346\n",
      "Epoch 1916, Loss: 0.5835115909576416, Final Batch Loss: 0.2940855920314789\n",
      "Epoch 1917, Loss: 0.5410435497760773, Final Batch Loss: 0.25940394401550293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1918, Loss: 0.6329462826251984, Final Batch Loss: 0.34046047925949097\n",
      "Epoch 1919, Loss: 0.5615999698638916, Final Batch Loss: 0.2661501467227936\n",
      "Epoch 1920, Loss: 0.5406284928321838, Final Batch Loss: 0.26506152749061584\n",
      "Epoch 1921, Loss: 0.5073528438806534, Final Batch Loss: 0.21249403059482574\n",
      "Epoch 1922, Loss: 0.544223964214325, Final Batch Loss: 0.2793329358100891\n",
      "Epoch 1923, Loss: 0.5648421347141266, Final Batch Loss: 0.2596193850040436\n",
      "Epoch 1924, Loss: 0.5593540668487549, Final Batch Loss: 0.2926405966281891\n",
      "Epoch 1925, Loss: 0.49357233941555023, Final Batch Loss: 0.23772300779819489\n",
      "Epoch 1926, Loss: 0.5301487445831299, Final Batch Loss: 0.227277934551239\n",
      "Epoch 1927, Loss: 0.4858066439628601, Final Batch Loss: 0.2294507920742035\n",
      "Epoch 1928, Loss: 0.6031230092048645, Final Batch Loss: 0.32532286643981934\n",
      "Epoch 1929, Loss: 0.5219604074954987, Final Batch Loss: 0.21832537651062012\n",
      "Epoch 1930, Loss: 0.52635657787323, Final Batch Loss: 0.27329006791114807\n",
      "Epoch 1931, Loss: 0.4818578064441681, Final Batch Loss: 0.23087343573570251\n",
      "Epoch 1932, Loss: 0.6105328500270844, Final Batch Loss: 0.32419753074645996\n",
      "Epoch 1933, Loss: 0.5698756277561188, Final Batch Loss: 0.25633299350738525\n",
      "Epoch 1934, Loss: 0.5443189144134521, Final Batch Loss: 0.2727464735507965\n",
      "Epoch 1935, Loss: 0.557026207447052, Final Batch Loss: 0.2666085362434387\n",
      "Epoch 1936, Loss: 0.48402151465415955, Final Batch Loss: 0.26686567068099976\n",
      "Epoch 1937, Loss: 0.5305547714233398, Final Batch Loss: 0.24252420663833618\n",
      "Epoch 1938, Loss: 0.48526546359062195, Final Batch Loss: 0.26570653915405273\n",
      "Epoch 1939, Loss: 0.5038488358259201, Final Batch Loss: 0.24253211915493011\n",
      "Epoch 1940, Loss: 0.5567036867141724, Final Batch Loss: 0.29939597845077515\n",
      "Epoch 1941, Loss: 0.5559343695640564, Final Batch Loss: 0.26005125045776367\n",
      "Epoch 1942, Loss: 0.4530460387468338, Final Batch Loss: 0.19847498834133148\n",
      "Epoch 1943, Loss: 0.4964836537837982, Final Batch Loss: 0.29606157541275024\n",
      "Epoch 1944, Loss: 0.5074485838413239, Final Batch Loss: 0.2532023787498474\n",
      "Epoch 1945, Loss: 0.5686069428920746, Final Batch Loss: 0.28456950187683105\n",
      "Epoch 1946, Loss: 0.4872884303331375, Final Batch Loss: 0.20528779923915863\n",
      "Epoch 1947, Loss: 0.49090026319026947, Final Batch Loss: 0.25193947553634644\n",
      "Epoch 1948, Loss: 0.5729456096887589, Final Batch Loss: 0.23065151274204254\n",
      "Epoch 1949, Loss: 0.502802774310112, Final Batch Loss: 0.2847815155982971\n",
      "Epoch 1950, Loss: 0.5212499648332596, Final Batch Loss: 0.29643887281417847\n",
      "Epoch 1951, Loss: 0.48356159031391144, Final Batch Loss: 0.24823430180549622\n",
      "Epoch 1952, Loss: 0.499435231089592, Final Batch Loss: 0.24165205657482147\n",
      "Epoch 1953, Loss: 0.5376920402050018, Final Batch Loss: 0.28245142102241516\n",
      "Epoch 1954, Loss: 0.5511482059955597, Final Batch Loss: 0.26168859004974365\n",
      "Epoch 1955, Loss: 0.5145481079816818, Final Batch Loss: 0.20348761975765228\n",
      "Epoch 1956, Loss: 0.49985361099243164, Final Batch Loss: 0.2438221275806427\n",
      "Epoch 1957, Loss: 0.5383563339710236, Final Batch Loss: 0.22565990686416626\n",
      "Epoch 1958, Loss: 0.4902239441871643, Final Batch Loss: 0.2237463891506195\n",
      "Epoch 1959, Loss: 0.5581537485122681, Final Batch Loss: 0.2729480564594269\n",
      "Epoch 1960, Loss: 0.5567536950111389, Final Batch Loss: 0.30483922362327576\n",
      "Epoch 1961, Loss: 0.5451129823923111, Final Batch Loss: 0.24051712453365326\n",
      "Epoch 1962, Loss: 0.5180508494377136, Final Batch Loss: 0.2583743631839752\n",
      "Epoch 1963, Loss: 0.6133102178573608, Final Batch Loss: 0.30737215280532837\n",
      "Epoch 1964, Loss: 0.46997083723545074, Final Batch Loss: 0.20233853161334991\n",
      "Epoch 1965, Loss: 0.5139613747596741, Final Batch Loss: 0.25771287083625793\n",
      "Epoch 1966, Loss: 0.5474501550197601, Final Batch Loss: 0.25971144437789917\n",
      "Epoch 1967, Loss: 0.4631706178188324, Final Batch Loss: 0.20681095123291016\n",
      "Epoch 1968, Loss: 0.4874623864889145, Final Batch Loss: 0.23191256821155548\n",
      "Epoch 1969, Loss: 0.5517297983169556, Final Batch Loss: 0.2993353307247162\n",
      "Epoch 1970, Loss: 0.49804723262786865, Final Batch Loss: 0.24626374244689941\n",
      "Epoch 1971, Loss: 0.5273729711771011, Final Batch Loss: 0.28355878591537476\n",
      "Epoch 1972, Loss: 0.5270998179912567, Final Batch Loss: 0.26291152834892273\n",
      "Epoch 1973, Loss: 0.5496886372566223, Final Batch Loss: 0.2858593463897705\n",
      "Epoch 1974, Loss: 0.5384397655725479, Final Batch Loss: 0.2402656227350235\n",
      "Epoch 1975, Loss: 0.5445472896099091, Final Batch Loss: 0.29225441813468933\n",
      "Epoch 1976, Loss: 0.553321897983551, Final Batch Loss: 0.26664841175079346\n",
      "Epoch 1977, Loss: 0.5062929093837738, Final Batch Loss: 0.20352515578269958\n",
      "Epoch 1978, Loss: 0.523118868470192, Final Batch Loss: 0.28324273228645325\n",
      "Epoch 1979, Loss: 0.502395361661911, Final Batch Loss: 0.25494101643562317\n",
      "Epoch 1980, Loss: 0.5334654003381729, Final Batch Loss: 0.236778125166893\n",
      "Epoch 1981, Loss: 0.48259851336479187, Final Batch Loss: 0.2245333194732666\n",
      "Epoch 1982, Loss: 0.5137067586183548, Final Batch Loss: 0.2303193062543869\n",
      "Epoch 1983, Loss: 0.5227527916431427, Final Batch Loss: 0.2535519599914551\n",
      "Epoch 1984, Loss: 0.5082469284534454, Final Batch Loss: 0.2796204090118408\n",
      "Epoch 1985, Loss: 0.5048129856586456, Final Batch Loss: 0.30140599608421326\n",
      "Epoch 1986, Loss: 0.5323599278926849, Final Batch Loss: 0.25051018595695496\n",
      "Epoch 1987, Loss: 0.5643604695796967, Final Batch Loss: 0.2583945393562317\n",
      "Epoch 1988, Loss: 0.51102514564991, Final Batch Loss: 0.2724912464618683\n",
      "Epoch 1989, Loss: 0.4797607809305191, Final Batch Loss: 0.20388929545879364\n",
      "Epoch 1990, Loss: 0.5786624848842621, Final Batch Loss: 0.327539324760437\n",
      "Epoch 1991, Loss: 0.47183071076869965, Final Batch Loss: 0.25411921739578247\n",
      "Epoch 1992, Loss: 0.5310903638601303, Final Batch Loss: 0.24676229059696198\n",
      "Epoch 1993, Loss: 0.593526303768158, Final Batch Loss: 0.33681514859199524\n",
      "Epoch 1994, Loss: 0.5146202147006989, Final Batch Loss: 0.2568652629852295\n",
      "Epoch 1995, Loss: 0.5193031430244446, Final Batch Loss: 0.2513531744480133\n",
      "Epoch 1996, Loss: 0.518704280257225, Final Batch Loss: 0.2443772703409195\n",
      "Epoch 1997, Loss: 0.5137138962745667, Final Batch Loss: 0.25870612263679504\n",
      "Epoch 1998, Loss: 0.5502427071332932, Final Batch Loss: 0.30275383591651917\n",
      "Epoch 1999, Loss: 0.5191086232662201, Final Batch Loss: 0.2667756974697113\n",
      "Epoch 2000, Loss: 0.5290869176387787, Final Batch Loss: 0.2699052691459656\n",
      "Epoch 2001, Loss: 0.4822753369808197, Final Batch Loss: 0.19528979063034058\n",
      "Epoch 2002, Loss: 0.5007514953613281, Final Batch Loss: 0.24524274468421936\n",
      "Epoch 2003, Loss: 0.511833518743515, Final Batch Loss: 0.28279805183410645\n",
      "Epoch 2004, Loss: 0.4959975630044937, Final Batch Loss: 0.2598690986633301\n",
      "Epoch 2005, Loss: 0.5015222877264023, Final Batch Loss: 0.27680516242980957\n",
      "Epoch 2006, Loss: 0.5441761314868927, Final Batch Loss: 0.304739385843277\n",
      "Epoch 2007, Loss: 0.510105311870575, Final Batch Loss: 0.23766982555389404\n",
      "Epoch 2008, Loss: 0.4994806498289108, Final Batch Loss: 0.2680760324001312\n",
      "Epoch 2009, Loss: 0.5163905620574951, Final Batch Loss: 0.26412150263786316\n",
      "Epoch 2010, Loss: 0.5723921358585358, Final Batch Loss: 0.2770802974700928\n",
      "Epoch 2011, Loss: 0.5194859504699707, Final Batch Loss: 0.26137056946754456\n",
      "Epoch 2012, Loss: 0.571628749370575, Final Batch Loss: 0.29135045409202576\n",
      "Epoch 2013, Loss: 0.4987807869911194, Final Batch Loss: 0.24983814358711243\n",
      "Epoch 2014, Loss: 0.5107169151306152, Final Batch Loss: 0.2382543385028839\n",
      "Epoch 2015, Loss: 0.5143884569406509, Final Batch Loss: 0.2713972330093384\n",
      "Epoch 2016, Loss: 0.48528625071048737, Final Batch Loss: 0.20195399224758148\n",
      "Epoch 2017, Loss: 0.4911438226699829, Final Batch Loss: 0.2525162994861603\n",
      "Epoch 2018, Loss: 0.4785812795162201, Final Batch Loss: 0.19339871406555176\n",
      "Epoch 2019, Loss: 0.5456999838352203, Final Batch Loss: 0.26524484157562256\n",
      "Epoch 2020, Loss: 0.5042640864849091, Final Batch Loss: 0.2586601972579956\n",
      "Epoch 2021, Loss: 0.5335372388362885, Final Batch Loss: 0.30741241574287415\n",
      "Epoch 2022, Loss: 0.4998161792755127, Final Batch Loss: 0.25681644678115845\n",
      "Epoch 2023, Loss: 0.5834055244922638, Final Batch Loss: 0.3192290961742401\n",
      "Epoch 2024, Loss: 0.5078563839197159, Final Batch Loss: 0.24269430339336395\n",
      "Epoch 2025, Loss: 0.5021263360977173, Final Batch Loss: 0.3069557547569275\n",
      "Epoch 2026, Loss: 0.5038903951644897, Final Batch Loss: 0.24171283841133118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2027, Loss: 0.5281335711479187, Final Batch Loss: 0.2438783347606659\n",
      "Epoch 2028, Loss: 0.5910314321517944, Final Batch Loss: 0.29716062545776367\n",
      "Epoch 2029, Loss: 0.5215987414121628, Final Batch Loss: 0.2489054650068283\n",
      "Epoch 2030, Loss: 0.4786332845687866, Final Batch Loss: 0.26501792669296265\n",
      "Epoch 2031, Loss: 0.5569951832294464, Final Batch Loss: 0.27309441566467285\n",
      "Epoch 2032, Loss: 0.4834195673465729, Final Batch Loss: 0.23137512803077698\n",
      "Epoch 2033, Loss: 0.47035469114780426, Final Batch Loss: 0.19477327167987823\n",
      "Epoch 2034, Loss: 0.4378074109554291, Final Batch Loss: 0.14902114868164062\n",
      "Epoch 2035, Loss: 0.4965488165616989, Final Batch Loss: 0.21520863473415375\n",
      "Epoch 2036, Loss: 0.5111709237098694, Final Batch Loss: 0.2117222547531128\n",
      "Epoch 2037, Loss: 0.4679861217737198, Final Batch Loss: 0.2298397719860077\n",
      "Epoch 2038, Loss: 0.5462064445018768, Final Batch Loss: 0.257707417011261\n",
      "Epoch 2039, Loss: 0.5151943117380142, Final Batch Loss: 0.2404586523771286\n",
      "Epoch 2040, Loss: 0.530490905046463, Final Batch Loss: 0.27895018458366394\n",
      "Epoch 2041, Loss: 0.5673593282699585, Final Batch Loss: 0.29404887557029724\n",
      "Epoch 2042, Loss: 0.5223400145769119, Final Batch Loss: 0.21992488205432892\n",
      "Epoch 2043, Loss: 0.5030301511287689, Final Batch Loss: 0.23679760098457336\n",
      "Epoch 2044, Loss: 0.5109799206256866, Final Batch Loss: 0.26398181915283203\n",
      "Epoch 2045, Loss: 0.5316238850355148, Final Batch Loss: 0.29418230056762695\n",
      "Epoch 2046, Loss: 0.5148868411779404, Final Batch Loss: 0.27165091037750244\n",
      "Epoch 2047, Loss: 0.5249582380056381, Final Batch Loss: 0.20538662374019623\n",
      "Epoch 2048, Loss: 0.5378901660442352, Final Batch Loss: 0.28947094082832336\n",
      "Epoch 2049, Loss: 0.5349803864955902, Final Batch Loss: 0.2777016758918762\n",
      "Epoch 2050, Loss: 0.48377634584903717, Final Batch Loss: 0.2383827120065689\n",
      "Epoch 2051, Loss: 0.5160914063453674, Final Batch Loss: 0.2795054614543915\n",
      "Epoch 2052, Loss: 0.5664674043655396, Final Batch Loss: 0.29045554995536804\n",
      "Epoch 2053, Loss: 0.45620371401309967, Final Batch Loss: 0.25867971777915955\n",
      "Epoch 2054, Loss: 0.5393621921539307, Final Batch Loss: 0.2657514810562134\n",
      "Epoch 2055, Loss: 0.4753911942243576, Final Batch Loss: 0.21238826215267181\n",
      "Epoch 2056, Loss: 0.5563256591558456, Final Batch Loss: 0.3359103500843048\n",
      "Epoch 2057, Loss: 0.4652436524629593, Final Batch Loss: 0.21971271932125092\n",
      "Epoch 2058, Loss: 0.5035658031702042, Final Batch Loss: 0.27323344349861145\n",
      "Epoch 2059, Loss: 0.4496880769729614, Final Batch Loss: 0.2504430413246155\n",
      "Epoch 2060, Loss: 0.47364041209220886, Final Batch Loss: 0.2156377136707306\n",
      "Epoch 2061, Loss: 0.6191989183425903, Final Batch Loss: 0.3089534044265747\n",
      "Epoch 2062, Loss: 0.4552418887615204, Final Batch Loss: 0.21396596729755402\n",
      "Epoch 2063, Loss: 0.5253722071647644, Final Batch Loss: 0.26698124408721924\n",
      "Epoch 2064, Loss: 0.5074989795684814, Final Batch Loss: 0.28126388788223267\n",
      "Epoch 2065, Loss: 0.5616205930709839, Final Batch Loss: 0.2982556223869324\n",
      "Epoch 2066, Loss: 0.4645776152610779, Final Batch Loss: 0.24741101264953613\n",
      "Epoch 2067, Loss: 0.4797298461198807, Final Batch Loss: 0.19527192413806915\n",
      "Epoch 2068, Loss: 0.5265582948923111, Final Batch Loss: 0.2958511710166931\n",
      "Epoch 2069, Loss: 0.4618184268474579, Final Batch Loss: 0.21129107475280762\n",
      "Epoch 2070, Loss: 0.4821415990591049, Final Batch Loss: 0.2570759057998657\n",
      "Epoch 2071, Loss: 0.5060686469078064, Final Batch Loss: 0.26257210969924927\n",
      "Epoch 2072, Loss: 0.5357459485530853, Final Batch Loss: 0.29057034850120544\n",
      "Epoch 2073, Loss: 0.5116112977266312, Final Batch Loss: 0.3018171787261963\n",
      "Epoch 2074, Loss: 0.456540510058403, Final Batch Loss: 0.19363848865032196\n",
      "Epoch 2075, Loss: 0.4456457495689392, Final Batch Loss: 0.230271577835083\n",
      "Epoch 2076, Loss: 0.4643558859825134, Final Batch Loss: 0.23337167501449585\n",
      "Epoch 2077, Loss: 0.4467054307460785, Final Batch Loss: 0.20192284882068634\n",
      "Epoch 2078, Loss: 0.46277229487895966, Final Batch Loss: 0.20881913602352142\n",
      "Epoch 2079, Loss: 0.5135804116725922, Final Batch Loss: 0.2601224184036255\n",
      "Epoch 2080, Loss: 0.48367229104042053, Final Batch Loss: 0.21462732553482056\n",
      "Epoch 2081, Loss: 0.5297819226980209, Final Batch Loss: 0.29362982511520386\n",
      "Epoch 2082, Loss: 0.5205729305744171, Final Batch Loss: 0.2694776952266693\n",
      "Epoch 2083, Loss: 0.43037503957748413, Final Batch Loss: 0.21022820472717285\n",
      "Epoch 2084, Loss: 0.4859750419855118, Final Batch Loss: 0.27832460403442383\n",
      "Epoch 2085, Loss: 0.44387340545654297, Final Batch Loss: 0.2121671885251999\n",
      "Epoch 2086, Loss: 0.47532808780670166, Final Batch Loss: 0.23492473363876343\n",
      "Epoch 2087, Loss: 0.4602743834257126, Final Batch Loss: 0.18867866694927216\n",
      "Epoch 2088, Loss: 0.4686940461397171, Final Batch Loss: 0.2016991525888443\n",
      "Epoch 2089, Loss: 0.45635145902633667, Final Batch Loss: 0.20409682393074036\n",
      "Epoch 2090, Loss: 0.4829327315092087, Final Batch Loss: 0.25639402866363525\n",
      "Epoch 2091, Loss: 0.5113012343645096, Final Batch Loss: 0.27270936965942383\n",
      "Epoch 2092, Loss: 0.5460122227668762, Final Batch Loss: 0.2691902220249176\n",
      "Epoch 2093, Loss: 0.4869609624147415, Final Batch Loss: 0.22189252078533173\n",
      "Epoch 2094, Loss: 0.5367191135883331, Final Batch Loss: 0.316127747297287\n",
      "Epoch 2095, Loss: 0.47380587458610535, Final Batch Loss: 0.27857640385627747\n",
      "Epoch 2096, Loss: 0.44936224818229675, Final Batch Loss: 0.25678086280822754\n",
      "Epoch 2097, Loss: 0.4876123368740082, Final Batch Loss: 0.2669805586338043\n",
      "Epoch 2098, Loss: 0.49055156111717224, Final Batch Loss: 0.25110918283462524\n",
      "Epoch 2099, Loss: 0.4900732487440109, Final Batch Loss: 0.23621554672718048\n",
      "Epoch 2100, Loss: 0.4749988317489624, Final Batch Loss: 0.27430370450019836\n",
      "Epoch 2101, Loss: 0.45368990302085876, Final Batch Loss: 0.22388112545013428\n",
      "Epoch 2102, Loss: 0.4491830915212631, Final Batch Loss: 0.21317149698734283\n",
      "Epoch 2103, Loss: 0.5207634717226028, Final Batch Loss: 0.2848759591579437\n",
      "Epoch 2104, Loss: 0.4910123497247696, Final Batch Loss: 0.28681981563568115\n",
      "Epoch 2105, Loss: 0.4534490406513214, Final Batch Loss: 0.2634921669960022\n",
      "Epoch 2106, Loss: 0.4900987148284912, Final Batch Loss: 0.21599557995796204\n",
      "Epoch 2107, Loss: 0.48367223143577576, Final Batch Loss: 0.24104726314544678\n",
      "Epoch 2108, Loss: 0.5443636178970337, Final Batch Loss: 0.2651248872280121\n",
      "Epoch 2109, Loss: 0.48012183606624603, Final Batch Loss: 0.2598908543586731\n",
      "Epoch 2110, Loss: 0.4518532305955887, Final Batch Loss: 0.2518629729747772\n",
      "Epoch 2111, Loss: 0.5586928427219391, Final Batch Loss: 0.3127017021179199\n",
      "Epoch 2112, Loss: 0.5411556661128998, Final Batch Loss: 0.2934710681438446\n",
      "Epoch 2113, Loss: 0.48052428662776947, Final Batch Loss: 0.2582157850265503\n",
      "Epoch 2114, Loss: 0.43392322957515717, Final Batch Loss: 0.22761951386928558\n",
      "Epoch 2115, Loss: 0.5110836327075958, Final Batch Loss: 0.26107215881347656\n",
      "Epoch 2116, Loss: 0.43609364330768585, Final Batch Loss: 0.2245420664548874\n",
      "Epoch 2117, Loss: 0.48921750485897064, Final Batch Loss: 0.2578493654727936\n",
      "Epoch 2118, Loss: 0.4762255400419235, Final Batch Loss: 0.2273101955652237\n",
      "Epoch 2119, Loss: 0.49863530695438385, Final Batch Loss: 0.28486117720603943\n",
      "Epoch 2120, Loss: 0.48722319304943085, Final Batch Loss: 0.25898078083992004\n",
      "Epoch 2121, Loss: 0.43155041337013245, Final Batch Loss: 0.20028795301914215\n",
      "Epoch 2122, Loss: 0.4607047885656357, Final Batch Loss: 0.23216573894023895\n",
      "Epoch 2123, Loss: 0.4661407619714737, Final Batch Loss: 0.22409991919994354\n",
      "Epoch 2124, Loss: 0.584321066737175, Final Batch Loss: 0.3557784855365753\n",
      "Epoch 2125, Loss: 0.4561677575111389, Final Batch Loss: 0.1999695599079132\n",
      "Epoch 2126, Loss: 0.5135035663843155, Final Batch Loss: 0.2886394262313843\n",
      "Epoch 2127, Loss: 0.46938011050224304, Final Batch Loss: 0.226201593875885\n",
      "Epoch 2128, Loss: 0.5053112804889679, Final Batch Loss: 0.2694503366947174\n",
      "Epoch 2129, Loss: 0.44944562017917633, Final Batch Loss: 0.18749438226222992\n",
      "Epoch 2130, Loss: 0.4779341518878937, Final Batch Loss: 0.22037991881370544\n",
      "Epoch 2131, Loss: 0.5062995702028275, Final Batch Loss: 0.24260573089122772\n",
      "Epoch 2132, Loss: 0.4510917663574219, Final Batch Loss: 0.19926854968070984\n",
      "Epoch 2133, Loss: 0.47276438772678375, Final Batch Loss: 0.2259567230939865\n",
      "Epoch 2134, Loss: 0.47420574724674225, Final Batch Loss: 0.24667929112911224\n",
      "Epoch 2135, Loss: 0.46595489978790283, Final Batch Loss: 0.20040950179100037\n",
      "Epoch 2136, Loss: 0.4465193599462509, Final Batch Loss: 0.20390552282333374\n",
      "Epoch 2137, Loss: 0.4603322148323059, Final Batch Loss: 0.22667257487773895\n",
      "Epoch 2138, Loss: 0.46316494047641754, Final Batch Loss: 0.2126997858285904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2139, Loss: 0.4609755426645279, Final Batch Loss: 0.19796188175678253\n",
      "Epoch 2140, Loss: 0.472157821059227, Final Batch Loss: 0.24916371703147888\n",
      "Epoch 2141, Loss: 0.5577340722084045, Final Batch Loss: 0.2381160855293274\n",
      "Epoch 2142, Loss: 0.5507075786590576, Final Batch Loss: 0.22765129804611206\n",
      "Epoch 2143, Loss: 0.4513859748840332, Final Batch Loss: 0.2214438021183014\n",
      "Epoch 2144, Loss: 0.4475932866334915, Final Batch Loss: 0.22165049612522125\n",
      "Epoch 2145, Loss: 0.4448683261871338, Final Batch Loss: 0.1705230176448822\n",
      "Epoch 2146, Loss: 0.4750248044729233, Final Batch Loss: 0.2283218801021576\n",
      "Epoch 2147, Loss: 0.4682650715112686, Final Batch Loss: 0.21823416650295258\n",
      "Epoch 2148, Loss: 0.47001464664936066, Final Batch Loss: 0.25065556168556213\n",
      "Epoch 2149, Loss: 0.4804232120513916, Final Batch Loss: 0.2462829053401947\n",
      "Epoch 2150, Loss: 0.451062873005867, Final Batch Loss: 0.24961040914058685\n",
      "Epoch 2151, Loss: 0.46442684531211853, Final Batch Loss: 0.24517196416854858\n",
      "Epoch 2152, Loss: 0.4398098438978195, Final Batch Loss: 0.20134912431240082\n",
      "Epoch 2153, Loss: 0.4785638451576233, Final Batch Loss: 0.26736336946487427\n",
      "Epoch 2154, Loss: 0.44716309010982513, Final Batch Loss: 0.22239618003368378\n",
      "Epoch 2155, Loss: 0.47380368411540985, Final Batch Loss: 0.20921041071414948\n",
      "Epoch 2156, Loss: 0.4787933975458145, Final Batch Loss: 0.21619592607021332\n",
      "Epoch 2157, Loss: 0.440959170460701, Final Batch Loss: 0.23555000126361847\n",
      "Epoch 2158, Loss: 0.5674712657928467, Final Batch Loss: 0.28658363223075867\n",
      "Epoch 2159, Loss: 0.47397221624851227, Final Batch Loss: 0.20180733501911163\n",
      "Epoch 2160, Loss: 0.5154118835926056, Final Batch Loss: 0.2740289270877838\n",
      "Epoch 2161, Loss: 0.4776059538125992, Final Batch Loss: 0.22857055068016052\n",
      "Epoch 2162, Loss: 0.49802732467651367, Final Batch Loss: 0.20700162649154663\n",
      "Epoch 2163, Loss: 0.4647279679775238, Final Batch Loss: 0.2427627593278885\n",
      "Epoch 2164, Loss: 0.4877825379371643, Final Batch Loss: 0.24458906054496765\n",
      "Epoch 2165, Loss: 0.48936982452869415, Final Batch Loss: 0.23590229451656342\n",
      "Epoch 2166, Loss: 0.4235708564519882, Final Batch Loss: 0.1881955862045288\n",
      "Epoch 2167, Loss: 0.5059012770652771, Final Batch Loss: 0.28807005286216736\n",
      "Epoch 2168, Loss: 0.47528211772441864, Final Batch Loss: 0.23326237499713898\n",
      "Epoch 2169, Loss: 0.5711949467658997, Final Batch Loss: 0.2822820544242859\n",
      "Epoch 2170, Loss: 0.5058626532554626, Final Batch Loss: 0.2632838189601898\n",
      "Epoch 2171, Loss: 0.48871268332004547, Final Batch Loss: 0.24917900562286377\n",
      "Epoch 2172, Loss: 0.4124998599290848, Final Batch Loss: 0.2008543312549591\n",
      "Epoch 2173, Loss: 0.4621741771697998, Final Batch Loss: 0.22198525071144104\n",
      "Epoch 2174, Loss: 0.4666050523519516, Final Batch Loss: 0.19291247427463531\n",
      "Epoch 2175, Loss: 0.5189586728811264, Final Batch Loss: 0.2999182343482971\n",
      "Epoch 2176, Loss: 0.46608561277389526, Final Batch Loss: 0.1917344331741333\n",
      "Epoch 2177, Loss: 0.5106185972690582, Final Batch Loss: 0.2956337034702301\n",
      "Epoch 2178, Loss: 0.4430694133043289, Final Batch Loss: 0.20295916497707367\n",
      "Epoch 2179, Loss: 0.39419420063495636, Final Batch Loss: 0.21158377826213837\n",
      "Epoch 2180, Loss: 0.5039965510368347, Final Batch Loss: 0.27127301692962646\n",
      "Epoch 2181, Loss: 0.48364628851413727, Final Batch Loss: 0.2454843521118164\n",
      "Epoch 2182, Loss: 0.4405982494354248, Final Batch Loss: 0.24341994524002075\n",
      "Epoch 2183, Loss: 0.5175641477108002, Final Batch Loss: 0.2341119945049286\n",
      "Epoch 2184, Loss: 0.5031318217515945, Final Batch Loss: 0.2287634164094925\n",
      "Epoch 2185, Loss: 0.426100492477417, Final Batch Loss: 0.21017573773860931\n",
      "Epoch 2186, Loss: 0.5027225315570831, Final Batch Loss: 0.2574094831943512\n",
      "Epoch 2187, Loss: 0.4595724493265152, Final Batch Loss: 0.2156456857919693\n",
      "Epoch 2188, Loss: 0.4489406645298004, Final Batch Loss: 0.22449293732643127\n",
      "Epoch 2189, Loss: 0.4828777015209198, Final Batch Loss: 0.27740973234176636\n",
      "Epoch 2190, Loss: 0.5046675950288773, Final Batch Loss: 0.2592006325721741\n",
      "Epoch 2191, Loss: 0.4326464533805847, Final Batch Loss: 0.24189917743206024\n",
      "Epoch 2192, Loss: 0.49484987556934357, Final Batch Loss: 0.22975535690784454\n",
      "Epoch 2193, Loss: 0.4872564226388931, Final Batch Loss: 0.22687284648418427\n",
      "Epoch 2194, Loss: 0.5546761900186539, Final Batch Loss: 0.306866854429245\n",
      "Epoch 2195, Loss: 0.42374369502067566, Final Batch Loss: 0.24202413856983185\n",
      "Epoch 2196, Loss: 0.41967540979385376, Final Batch Loss: 0.19326674938201904\n",
      "Epoch 2197, Loss: 0.47662825882434845, Final Batch Loss: 0.24906009435653687\n",
      "Epoch 2198, Loss: 0.4847200959920883, Final Batch Loss: 0.21211247146129608\n",
      "Epoch 2199, Loss: 0.45323440432548523, Final Batch Loss: 0.23465029895305634\n",
      "Epoch 2200, Loss: 0.45472030341625214, Final Batch Loss: 0.2194107472896576\n",
      "Epoch 2201, Loss: 0.4496181607246399, Final Batch Loss: 0.21300971508026123\n",
      "Epoch 2202, Loss: 0.5015478283166885, Final Batch Loss: 0.25847554206848145\n",
      "Epoch 2203, Loss: 0.5121231973171234, Final Batch Loss: 0.25235679745674133\n",
      "Epoch 2204, Loss: 0.45319710671901703, Final Batch Loss: 0.23131264746189117\n",
      "Epoch 2205, Loss: 0.4684387296438217, Final Batch Loss: 0.21811841428279877\n",
      "Epoch 2206, Loss: 0.4648341089487076, Final Batch Loss: 0.2419087439775467\n",
      "Epoch 2207, Loss: 0.45885390043258667, Final Batch Loss: 0.22659410536289215\n",
      "Epoch 2208, Loss: 0.48075170814991, Final Batch Loss: 0.29050347208976746\n",
      "Epoch 2209, Loss: 0.4932318925857544, Final Batch Loss: 0.29349902272224426\n",
      "Epoch 2210, Loss: 0.4863131493330002, Final Batch Loss: 0.20660953223705292\n",
      "Epoch 2211, Loss: 0.5008817464113235, Final Batch Loss: 0.23470105230808258\n",
      "Epoch 2212, Loss: 0.4929409921169281, Final Batch Loss: 0.2636469304561615\n",
      "Epoch 2213, Loss: 0.508566826581955, Final Batch Loss: 0.2511408030986786\n",
      "Epoch 2214, Loss: 0.39720383286476135, Final Batch Loss: 0.20743559300899506\n",
      "Epoch 2215, Loss: 0.5612770915031433, Final Batch Loss: 0.29776737093925476\n",
      "Epoch 2216, Loss: 0.4329448342323303, Final Batch Loss: 0.20181472599506378\n",
      "Epoch 2217, Loss: 0.4824857711791992, Final Batch Loss: 0.25653448700904846\n",
      "Epoch 2218, Loss: 0.48493848741054535, Final Batch Loss: 0.1813327819108963\n",
      "Epoch 2219, Loss: 0.5035162419080734, Final Batch Loss: 0.23046360909938812\n",
      "Epoch 2220, Loss: 0.5352124571800232, Final Batch Loss: 0.2669980823993683\n",
      "Epoch 2221, Loss: 0.5105586498975754, Final Batch Loss: 0.2907223701477051\n",
      "Epoch 2222, Loss: 0.465788334608078, Final Batch Loss: 0.22407680749893188\n",
      "Epoch 2223, Loss: 0.44267725944519043, Final Batch Loss: 0.20690861344337463\n",
      "Epoch 2224, Loss: 0.45271773636341095, Final Batch Loss: 0.24098126590251923\n",
      "Epoch 2225, Loss: 0.5044811964035034, Final Batch Loss: 0.2508111298084259\n",
      "Epoch 2226, Loss: 0.46435388922691345, Final Batch Loss: 0.21750406920909882\n",
      "Epoch 2227, Loss: 0.5237827599048615, Final Batch Loss: 0.26896408200263977\n",
      "Epoch 2228, Loss: 0.4846193343400955, Final Batch Loss: 0.23319153487682343\n",
      "Epoch 2229, Loss: 0.4392135292291641, Final Batch Loss: 0.27723824977874756\n",
      "Epoch 2230, Loss: 0.47355253994464874, Final Batch Loss: 0.20053191483020782\n",
      "Epoch 2231, Loss: 0.5181603133678436, Final Batch Loss: 0.25276440382003784\n",
      "Epoch 2232, Loss: 0.5476024448871613, Final Batch Loss: 0.2676764726638794\n",
      "Epoch 2233, Loss: 0.4492943435907364, Final Batch Loss: 0.22500313818454742\n",
      "Epoch 2234, Loss: 0.4343198835849762, Final Batch Loss: 0.17882263660430908\n",
      "Epoch 2235, Loss: 0.5058436393737793, Final Batch Loss: 0.2658187448978424\n",
      "Epoch 2236, Loss: 0.5047889649868011, Final Batch Loss: 0.2710416615009308\n",
      "Epoch 2237, Loss: 0.46817657351493835, Final Batch Loss: 0.25599274039268494\n",
      "Epoch 2238, Loss: 0.5333724617958069, Final Batch Loss: 0.2964659035205841\n",
      "Epoch 2239, Loss: 0.42406678199768066, Final Batch Loss: 0.16800621151924133\n",
      "Epoch 2240, Loss: 0.4661562740802765, Final Batch Loss: 0.2660726010799408\n",
      "Epoch 2241, Loss: 0.46579182147979736, Final Batch Loss: 0.23048727214336395\n",
      "Epoch 2242, Loss: 0.45503273606300354, Final Batch Loss: 0.22993691265583038\n",
      "Epoch 2243, Loss: 0.5316016674041748, Final Batch Loss: 0.27514681220054626\n",
      "Epoch 2244, Loss: 0.43985515832901, Final Batch Loss: 0.2653232514858246\n",
      "Epoch 2245, Loss: 0.4253527522087097, Final Batch Loss: 0.21647979319095612\n",
      "Epoch 2246, Loss: 0.4688114523887634, Final Batch Loss: 0.21867626905441284\n",
      "Epoch 2247, Loss: 0.47566550970077515, Final Batch Loss: 0.2591260075569153\n",
      "Epoch 2248, Loss: 0.4100734293460846, Final Batch Loss: 0.16918300092220306\n",
      "Epoch 2249, Loss: 0.4322471171617508, Final Batch Loss: 0.22300374507904053\n",
      "Epoch 2250, Loss: 0.46763452887535095, Final Batch Loss: 0.22159512341022491\n",
      "Epoch 2251, Loss: 0.48716333508491516, Final Batch Loss: 0.16809698939323425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2252, Loss: 0.45317719876766205, Final Batch Loss: 0.18675987422466278\n",
      "Epoch 2253, Loss: 0.4569626748561859, Final Batch Loss: 0.22568506002426147\n",
      "Epoch 2254, Loss: 0.49610206484794617, Final Batch Loss: 0.22844281792640686\n",
      "Epoch 2255, Loss: 0.4312593340873718, Final Batch Loss: 0.20301344990730286\n",
      "Epoch 2256, Loss: 0.49744051694869995, Final Batch Loss: 0.2185516357421875\n",
      "Epoch 2257, Loss: 0.5371948480606079, Final Batch Loss: 0.2830008268356323\n",
      "Epoch 2258, Loss: 0.5042024850845337, Final Batch Loss: 0.28719353675842285\n",
      "Epoch 2259, Loss: 0.4175056219100952, Final Batch Loss: 0.19010107219219208\n",
      "Epoch 2260, Loss: 0.43499739468097687, Final Batch Loss: 0.20731818675994873\n",
      "Epoch 2261, Loss: 0.494815468788147, Final Batch Loss: 0.23986020684242249\n",
      "Epoch 2262, Loss: 0.5004490464925766, Final Batch Loss: 0.2215500921010971\n",
      "Epoch 2263, Loss: 0.4011430889368057, Final Batch Loss: 0.17440184950828552\n",
      "Epoch 2264, Loss: 0.4232802540063858, Final Batch Loss: 0.23299811780452728\n",
      "Epoch 2265, Loss: 0.41726814210414886, Final Batch Loss: 0.20044410228729248\n",
      "Epoch 2266, Loss: 0.4120538532733917, Final Batch Loss: 0.1935017853975296\n",
      "Epoch 2267, Loss: 0.4883662611246109, Final Batch Loss: 0.27947118878364563\n",
      "Epoch 2268, Loss: 0.4545130729675293, Final Batch Loss: 0.21187667548656464\n",
      "Epoch 2269, Loss: 0.4333404004573822, Final Batch Loss: 0.2062702476978302\n",
      "Epoch 2270, Loss: 0.47630107402801514, Final Batch Loss: 0.28319233655929565\n",
      "Epoch 2271, Loss: 0.46624310314655304, Final Batch Loss: 0.2585204243659973\n",
      "Epoch 2272, Loss: 0.4706709533929825, Final Batch Loss: 0.2592408061027527\n",
      "Epoch 2273, Loss: 0.5075878202915192, Final Batch Loss: 0.30944177508354187\n",
      "Epoch 2274, Loss: 0.4323261082172394, Final Batch Loss: 0.21557368338108063\n",
      "Epoch 2275, Loss: 0.45356664061546326, Final Batch Loss: 0.2506294548511505\n",
      "Epoch 2276, Loss: 0.42029543220996857, Final Batch Loss: 0.2228248417377472\n",
      "Epoch 2277, Loss: 0.4710070788860321, Final Batch Loss: 0.24772845208644867\n",
      "Epoch 2278, Loss: 0.46340706944465637, Final Batch Loss: 0.22518914937973022\n",
      "Epoch 2279, Loss: 0.45041361451148987, Final Batch Loss: 0.2606748044490814\n",
      "Epoch 2280, Loss: 0.4292505830526352, Final Batch Loss: 0.19293975830078125\n",
      "Epoch 2281, Loss: 0.430241197347641, Final Batch Loss: 0.20341096818447113\n",
      "Epoch 2282, Loss: 0.41094253957271576, Final Batch Loss: 0.17346853017807007\n",
      "Epoch 2283, Loss: 0.46378524601459503, Final Batch Loss: 0.2502792477607727\n",
      "Epoch 2284, Loss: 0.44880440831184387, Final Batch Loss: 0.2496073842048645\n",
      "Epoch 2285, Loss: 0.46425414085388184, Final Batch Loss: 0.22159285843372345\n",
      "Epoch 2286, Loss: 0.4231148362159729, Final Batch Loss: 0.21516308188438416\n",
      "Epoch 2287, Loss: 0.521764874458313, Final Batch Loss: 0.26114365458488464\n",
      "Epoch 2288, Loss: 0.4228522330522537, Final Batch Loss: 0.21839597821235657\n",
      "Epoch 2289, Loss: 0.48744943737983704, Final Batch Loss: 0.2777443826198578\n",
      "Epoch 2290, Loss: 0.4583050161600113, Final Batch Loss: 0.2158946543931961\n",
      "Epoch 2291, Loss: 0.5291538834571838, Final Batch Loss: 0.2501600682735443\n",
      "Epoch 2292, Loss: 0.41507674753665924, Final Batch Loss: 0.24376560747623444\n",
      "Epoch 2293, Loss: 0.4154413044452667, Final Batch Loss: 0.22706693410873413\n",
      "Epoch 2294, Loss: 0.4256879538297653, Final Batch Loss: 0.18154366314411163\n",
      "Epoch 2295, Loss: 0.42122742533683777, Final Batch Loss: 0.22688914835453033\n",
      "Epoch 2296, Loss: 0.4686778336763382, Final Batch Loss: 0.28374233841896057\n",
      "Epoch 2297, Loss: 0.47853440046310425, Final Batch Loss: 0.218571275472641\n",
      "Epoch 2298, Loss: 0.4884386211633682, Final Batch Loss: 0.24481193721294403\n",
      "Epoch 2299, Loss: 0.4660843759775162, Final Batch Loss: 0.23965948820114136\n",
      "Epoch 2300, Loss: 0.45592494308948517, Final Batch Loss: 0.2224632352590561\n",
      "Epoch 2301, Loss: 0.48672187328338623, Final Batch Loss: 0.24118126928806305\n",
      "Epoch 2302, Loss: 0.4326140135526657, Final Batch Loss: 0.22755391895771027\n",
      "Epoch 2303, Loss: 0.48023195564746857, Final Batch Loss: 0.2250785380601883\n",
      "Epoch 2304, Loss: 0.4587589204311371, Final Batch Loss: 0.2324206680059433\n",
      "Epoch 2305, Loss: 0.49214717745780945, Final Batch Loss: 0.23275431990623474\n",
      "Epoch 2306, Loss: 0.483756348490715, Final Batch Loss: 0.260537326335907\n",
      "Epoch 2307, Loss: 0.46193259954452515, Final Batch Loss: 0.28311747312545776\n",
      "Epoch 2308, Loss: 0.45355500280857086, Final Batch Loss: 0.21378201246261597\n",
      "Epoch 2309, Loss: 0.44484037160873413, Final Batch Loss: 0.21426615118980408\n",
      "Epoch 2310, Loss: 0.4464479684829712, Final Batch Loss: 0.22018347680568695\n",
      "Epoch 2311, Loss: 0.45764559507369995, Final Batch Loss: 0.22567351162433624\n",
      "Epoch 2312, Loss: 0.5223881900310516, Final Batch Loss: 0.24082747101783752\n",
      "Epoch 2313, Loss: 0.44352683424949646, Final Batch Loss: 0.20721691846847534\n",
      "Epoch 2314, Loss: 0.45064082741737366, Final Batch Loss: 0.20419590175151825\n",
      "Epoch 2315, Loss: 0.4999825060367584, Final Batch Loss: 0.26752448081970215\n",
      "Epoch 2316, Loss: 0.45315833389759064, Final Batch Loss: 0.2226632535457611\n",
      "Epoch 2317, Loss: 0.4638848751783371, Final Batch Loss: 0.2129419893026352\n",
      "Epoch 2318, Loss: 0.45241712033748627, Final Batch Loss: 0.24324281513690948\n",
      "Epoch 2319, Loss: 0.4978991597890854, Final Batch Loss: 0.2624649405479431\n",
      "Epoch 2320, Loss: 0.4703851193189621, Final Batch Loss: 0.18858860433101654\n",
      "Epoch 2321, Loss: 0.46221958100795746, Final Batch Loss: 0.23864348232746124\n",
      "Epoch 2322, Loss: 0.4297816902399063, Final Batch Loss: 0.20717531442642212\n",
      "Epoch 2323, Loss: 0.446196511387825, Final Batch Loss: 0.24550507962703705\n",
      "Epoch 2324, Loss: 0.4827531725168228, Final Batch Loss: 0.227561816573143\n",
      "Epoch 2325, Loss: 0.47205451130867004, Final Batch Loss: 0.2671647071838379\n",
      "Epoch 2326, Loss: 0.45520393550395966, Final Batch Loss: 0.24360117316246033\n",
      "Epoch 2327, Loss: 0.47428129613399506, Final Batch Loss: 0.23428192734718323\n",
      "Epoch 2328, Loss: 0.4468933790922165, Final Batch Loss: 0.23672005534172058\n",
      "Epoch 2329, Loss: 0.39804333448410034, Final Batch Loss: 0.1836024522781372\n",
      "Epoch 2330, Loss: 0.47167129814624786, Final Batch Loss: 0.2543843686580658\n",
      "Epoch 2331, Loss: 0.4093240946531296, Final Batch Loss: 0.20926587283611298\n",
      "Epoch 2332, Loss: 0.40400804579257965, Final Batch Loss: 0.2067086398601532\n",
      "Epoch 2333, Loss: 0.5047621130943298, Final Batch Loss: 0.27127137780189514\n",
      "Epoch 2334, Loss: 0.41815733909606934, Final Batch Loss: 0.23014472424983978\n",
      "Epoch 2335, Loss: 0.38993580639362335, Final Batch Loss: 0.20398609340190887\n",
      "Epoch 2336, Loss: 0.5287743806838989, Final Batch Loss: 0.2735150158405304\n",
      "Epoch 2337, Loss: 0.42480650544166565, Final Batch Loss: 0.2176404893398285\n",
      "Epoch 2338, Loss: 0.46336857974529266, Final Batch Loss: 0.25704917311668396\n",
      "Epoch 2339, Loss: 0.36906909942626953, Final Batch Loss: 0.1568208783864975\n",
      "Epoch 2340, Loss: 0.4372903108596802, Final Batch Loss: 0.2273537814617157\n",
      "Epoch 2341, Loss: 0.4476715177297592, Final Batch Loss: 0.19968481361865997\n",
      "Epoch 2342, Loss: 0.5173896849155426, Final Batch Loss: 0.2642744481563568\n",
      "Epoch 2343, Loss: 0.4727989733219147, Final Batch Loss: 0.25678154826164246\n",
      "Epoch 2344, Loss: 0.46817971765995026, Final Batch Loss: 0.2514320909976959\n",
      "Epoch 2345, Loss: 0.4279956817626953, Final Batch Loss: 0.19025257229804993\n",
      "Epoch 2346, Loss: 0.402473047375679, Final Batch Loss: 0.20550908148288727\n",
      "Epoch 2347, Loss: 0.437360018491745, Final Batch Loss: 0.24744535982608795\n",
      "Epoch 2348, Loss: 0.4766026735305786, Final Batch Loss: 0.26477161049842834\n",
      "Epoch 2349, Loss: 0.4416252672672272, Final Batch Loss: 0.2440207600593567\n",
      "Epoch 2350, Loss: 0.4076423794031143, Final Batch Loss: 0.1846083402633667\n",
      "Epoch 2351, Loss: 0.45840033888816833, Final Batch Loss: 0.23623748123645782\n",
      "Epoch 2352, Loss: 0.37693794071674347, Final Batch Loss: 0.16602368652820587\n",
      "Epoch 2353, Loss: 0.5274436622858047, Final Batch Loss: 0.2847850024700165\n",
      "Epoch 2354, Loss: 0.4439274072647095, Final Batch Loss: 0.22592531144618988\n",
      "Epoch 2355, Loss: 0.49377837777137756, Final Batch Loss: 0.25926879048347473\n",
      "Epoch 2356, Loss: 0.4001491963863373, Final Batch Loss: 0.22986973822116852\n",
      "Epoch 2357, Loss: 0.429227277636528, Final Batch Loss: 0.21084032952785492\n",
      "Epoch 2358, Loss: 0.465059831738472, Final Batch Loss: 0.24289730191230774\n",
      "Epoch 2359, Loss: 0.41908368468284607, Final Batch Loss: 0.2009049654006958\n",
      "Epoch 2360, Loss: 0.5236523449420929, Final Batch Loss: 0.2276545763015747\n",
      "Epoch 2361, Loss: 0.4435413032770157, Final Batch Loss: 0.19891847670078278\n",
      "Epoch 2362, Loss: 0.48313598334789276, Final Batch Loss: 0.2189672738313675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2363, Loss: 0.4334170073270798, Final Batch Loss: 0.1824631243944168\n",
      "Epoch 2364, Loss: 0.40726085007190704, Final Batch Loss: 0.1723867803812027\n",
      "Epoch 2365, Loss: 0.5310187637805939, Final Batch Loss: 0.2538418173789978\n",
      "Epoch 2366, Loss: 0.4207251965999603, Final Batch Loss: 0.23260977864265442\n",
      "Epoch 2367, Loss: 0.4521259665489197, Final Batch Loss: 0.23213604092597961\n",
      "Epoch 2368, Loss: 0.5090796053409576, Final Batch Loss: 0.2793722450733185\n",
      "Epoch 2369, Loss: 0.4844909608364105, Final Batch Loss: 0.2769140601158142\n",
      "Epoch 2370, Loss: 0.4044284522533417, Final Batch Loss: 0.19016161561012268\n",
      "Epoch 2371, Loss: 0.4288988709449768, Final Batch Loss: 0.22964903712272644\n",
      "Epoch 2372, Loss: 0.48155778646469116, Final Batch Loss: 0.21061450242996216\n",
      "Epoch 2373, Loss: 0.4893234521150589, Final Batch Loss: 0.2370394915342331\n",
      "Epoch 2374, Loss: 0.4343017488718033, Final Batch Loss: 0.24110621213912964\n",
      "Epoch 2375, Loss: 0.3921734392642975, Final Batch Loss: 0.1783570647239685\n",
      "Epoch 2376, Loss: 0.34391312301158905, Final Batch Loss: 0.18349233269691467\n",
      "Epoch 2377, Loss: 0.44038306176662445, Final Batch Loss: 0.21452657878398895\n",
      "Epoch 2378, Loss: 0.4607016444206238, Final Batch Loss: 0.2411365658044815\n",
      "Epoch 2379, Loss: 0.4273829907178879, Final Batch Loss: 0.23263618350028992\n",
      "Epoch 2380, Loss: 0.4435500502586365, Final Batch Loss: 0.22966498136520386\n",
      "Epoch 2381, Loss: 0.42700663208961487, Final Batch Loss: 0.18264000117778778\n",
      "Epoch 2382, Loss: 0.40263573825359344, Final Batch Loss: 0.1853325068950653\n",
      "Epoch 2383, Loss: 0.42139554023742676, Final Batch Loss: 0.21139268577098846\n",
      "Epoch 2384, Loss: 0.43246062099933624, Final Batch Loss: 0.19091984629631042\n",
      "Epoch 2385, Loss: 0.3980255126953125, Final Batch Loss: 0.17422713339328766\n",
      "Epoch 2386, Loss: 0.5241669416427612, Final Batch Loss: 0.2586548328399658\n",
      "Epoch 2387, Loss: 0.4909019023180008, Final Batch Loss: 0.28530454635620117\n",
      "Epoch 2388, Loss: 0.40087921917438507, Final Batch Loss: 0.20383960008621216\n",
      "Epoch 2389, Loss: 0.40096892416477203, Final Batch Loss: 0.18491391837596893\n",
      "Epoch 2390, Loss: 0.4623989015817642, Final Batch Loss: 0.21417748928070068\n",
      "Epoch 2391, Loss: 0.4437993913888931, Final Batch Loss: 0.20894983410835266\n",
      "Epoch 2392, Loss: 0.48033519089221954, Final Batch Loss: 0.2166576236486435\n",
      "Epoch 2393, Loss: 0.4357014447450638, Final Batch Loss: 0.1916191130876541\n",
      "Epoch 2394, Loss: 0.5242371112108231, Final Batch Loss: 0.29789188504219055\n",
      "Epoch 2395, Loss: 0.40036454796791077, Final Batch Loss: 0.26493626832962036\n",
      "Epoch 2396, Loss: 0.4557760953903198, Final Batch Loss: 0.26148727536201477\n",
      "Epoch 2397, Loss: 0.5001574903726578, Final Batch Loss: 0.2743907868862152\n",
      "Epoch 2398, Loss: 0.4732031673192978, Final Batch Loss: 0.25257182121276855\n",
      "Epoch 2399, Loss: 0.41279104351997375, Final Batch Loss: 0.203733891248703\n",
      "Epoch 2400, Loss: 0.4009184092283249, Final Batch Loss: 0.19584722816944122\n",
      "Epoch 2401, Loss: 0.4083006978034973, Final Batch Loss: 0.23568002879619598\n",
      "Epoch 2402, Loss: 0.4431813657283783, Final Batch Loss: 0.17787829041481018\n",
      "Epoch 2403, Loss: 0.4217977225780487, Final Batch Loss: 0.19377969205379486\n",
      "Epoch 2404, Loss: 0.4424341171979904, Final Batch Loss: 0.19900661706924438\n",
      "Epoch 2405, Loss: 0.4261598587036133, Final Batch Loss: 0.2119005173444748\n",
      "Epoch 2406, Loss: 0.45863521099090576, Final Batch Loss: 0.23864847421646118\n",
      "Epoch 2407, Loss: 0.5023511648178101, Final Batch Loss: 0.24120163917541504\n",
      "Epoch 2408, Loss: 0.4000939577817917, Final Batch Loss: 0.16595588624477386\n",
      "Epoch 2409, Loss: 0.4249575436115265, Final Batch Loss: 0.2264275997877121\n",
      "Epoch 2410, Loss: 0.43300798535346985, Final Batch Loss: 0.20963281393051147\n",
      "Epoch 2411, Loss: 0.3892679065465927, Final Batch Loss: 0.22148092091083527\n",
      "Epoch 2412, Loss: 0.4264703691005707, Final Batch Loss: 0.19643235206604004\n",
      "Epoch 2413, Loss: 0.4366140216588974, Final Batch Loss: 0.20809613168239594\n",
      "Epoch 2414, Loss: 0.4750368595123291, Final Batch Loss: 0.26261597871780396\n",
      "Epoch 2415, Loss: 0.4163103401660919, Final Batch Loss: 0.20226392149925232\n",
      "Epoch 2416, Loss: 0.5097219794988632, Final Batch Loss: 0.24667303264141083\n",
      "Epoch 2417, Loss: 0.43154163658618927, Final Batch Loss: 0.256405770778656\n",
      "Epoch 2418, Loss: 0.38700810074806213, Final Batch Loss: 0.16843372583389282\n",
      "Epoch 2419, Loss: 0.3932243138551712, Final Batch Loss: 0.21053388714790344\n",
      "Epoch 2420, Loss: 0.4568524658679962, Final Batch Loss: 0.2367989867925644\n",
      "Epoch 2421, Loss: 0.49875980615615845, Final Batch Loss: 0.2603091895580292\n",
      "Epoch 2422, Loss: 0.4934357851743698, Final Batch Loss: 0.2713984251022339\n",
      "Epoch 2423, Loss: 0.3985580801963806, Final Batch Loss: 0.16684550046920776\n",
      "Epoch 2424, Loss: 0.5346659719944, Final Batch Loss: 0.26140356063842773\n",
      "Epoch 2425, Loss: 0.3909809738397598, Final Batch Loss: 0.20997174084186554\n",
      "Epoch 2426, Loss: 0.47579173743724823, Final Batch Loss: 0.2231101244688034\n",
      "Epoch 2427, Loss: 0.39896850287914276, Final Batch Loss: 0.17726652324199677\n",
      "Epoch 2428, Loss: 0.4330693185329437, Final Batch Loss: 0.2597391605377197\n",
      "Epoch 2429, Loss: 0.4706040918827057, Final Batch Loss: 0.20886462926864624\n",
      "Epoch 2430, Loss: 0.4452805817127228, Final Batch Loss: 0.21809427440166473\n",
      "Epoch 2431, Loss: 0.4252503216266632, Final Batch Loss: 0.16966617107391357\n",
      "Epoch 2432, Loss: 0.47178708016872406, Final Batch Loss: 0.21408586204051971\n",
      "Epoch 2433, Loss: 0.4214572161436081, Final Batch Loss: 0.22623249888420105\n",
      "Epoch 2434, Loss: 0.39407603442668915, Final Batch Loss: 0.1836710125207901\n",
      "Epoch 2435, Loss: 0.44816336035728455, Final Batch Loss: 0.20762553811073303\n",
      "Epoch 2436, Loss: 0.37209536135196686, Final Batch Loss: 0.15412098169326782\n",
      "Epoch 2437, Loss: 0.46840880811214447, Final Batch Loss: 0.2689664363861084\n",
      "Epoch 2438, Loss: 0.45673227310180664, Final Batch Loss: 0.2228795886039734\n",
      "Epoch 2439, Loss: 0.3979266434907913, Final Batch Loss: 0.18695010244846344\n",
      "Epoch 2440, Loss: 0.3844570070505142, Final Batch Loss: 0.17736947536468506\n",
      "Epoch 2441, Loss: 0.47877389192581177, Final Batch Loss: 0.2581785023212433\n",
      "Epoch 2442, Loss: 0.34883230924606323, Final Batch Loss: 0.16880403459072113\n",
      "Epoch 2443, Loss: 0.42541809380054474, Final Batch Loss: 0.16540326178073883\n",
      "Epoch 2444, Loss: 0.5146127343177795, Final Batch Loss: 0.24848073720932007\n",
      "Epoch 2445, Loss: 0.46559031307697296, Final Batch Loss: 0.23408013582229614\n",
      "Epoch 2446, Loss: 0.460579976439476, Final Batch Loss: 0.18246857821941376\n",
      "Epoch 2447, Loss: 0.47664661705493927, Final Batch Loss: 0.25157156586647034\n",
      "Epoch 2448, Loss: 0.3949180245399475, Final Batch Loss: 0.2081826627254486\n",
      "Epoch 2449, Loss: 0.4694824516773224, Final Batch Loss: 0.22526335716247559\n",
      "Epoch 2450, Loss: 0.4876735210418701, Final Batch Loss: 0.20778173208236694\n",
      "Epoch 2451, Loss: 0.4066913425922394, Final Batch Loss: 0.16951699554920197\n",
      "Epoch 2452, Loss: 0.5036973059177399, Final Batch Loss: 0.2746598720550537\n",
      "Epoch 2453, Loss: 0.4227816164493561, Final Batch Loss: 0.22218552231788635\n",
      "Epoch 2454, Loss: 0.44523464143276215, Final Batch Loss: 0.229176327586174\n",
      "Epoch 2455, Loss: 0.45798322558403015, Final Batch Loss: 0.22713448107242584\n",
      "Epoch 2456, Loss: 0.5744386613368988, Final Batch Loss: 0.33304110169410706\n",
      "Epoch 2457, Loss: 0.41765885055065155, Final Batch Loss: 0.23688752949237823\n",
      "Epoch 2458, Loss: 0.4033457040786743, Final Batch Loss: 0.19273677468299866\n",
      "Epoch 2459, Loss: 0.5205753892660141, Final Batch Loss: 0.237516388297081\n",
      "Epoch 2460, Loss: 0.4775098115205765, Final Batch Loss: 0.20832990109920502\n",
      "Epoch 2461, Loss: 0.4704377204179764, Final Batch Loss: 0.23331047594547272\n",
      "Epoch 2462, Loss: 0.532476931810379, Final Batch Loss: 0.2801945209503174\n",
      "Epoch 2463, Loss: 0.49776601791381836, Final Batch Loss: 0.22305166721343994\n",
      "Epoch 2464, Loss: 0.404742956161499, Final Batch Loss: 0.22711864113807678\n",
      "Epoch 2465, Loss: 0.48641568422317505, Final Batch Loss: 0.2379579246044159\n",
      "Epoch 2466, Loss: 0.4453586935997009, Final Batch Loss: 0.23276416957378387\n",
      "Epoch 2467, Loss: 0.47873541712760925, Final Batch Loss: 0.19107556343078613\n",
      "Epoch 2468, Loss: 0.44726498425006866, Final Batch Loss: 0.23388226330280304\n",
      "Epoch 2469, Loss: 0.4529015123844147, Final Batch Loss: 0.25140878558158875\n",
      "Epoch 2470, Loss: 0.4204097241163254, Final Batch Loss: 0.2175760269165039\n",
      "Epoch 2471, Loss: 0.42705845832824707, Final Batch Loss: 0.2484709769487381\n",
      "Epoch 2472, Loss: 0.4259060472249985, Final Batch Loss: 0.2047007977962494\n",
      "Epoch 2473, Loss: 0.4316462427377701, Final Batch Loss: 0.22726069390773773\n",
      "Epoch 2474, Loss: 0.42892803251743317, Final Batch Loss: 0.17587445676326752\n",
      "Epoch 2475, Loss: 0.407881036400795, Final Batch Loss: 0.202392116189003\n",
      "Epoch 2476, Loss: 0.4421449452638626, Final Batch Loss: 0.24889768660068512\n",
      "Epoch 2477, Loss: 0.4247748404741287, Final Batch Loss: 0.21567212045192719\n",
      "Epoch 2478, Loss: 0.4595072865486145, Final Batch Loss: 0.2525478005409241\n",
      "Epoch 2479, Loss: 0.4174109399318695, Final Batch Loss: 0.23516355454921722\n",
      "Epoch 2480, Loss: 0.4383084774017334, Final Batch Loss: 0.19582615792751312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2481, Loss: 0.4202609360218048, Final Batch Loss: 0.2271510511636734\n",
      "Epoch 2482, Loss: 0.40200673043727875, Final Batch Loss: 0.19617655873298645\n",
      "Epoch 2483, Loss: 0.4061192274093628, Final Batch Loss: 0.17276565730571747\n",
      "Epoch 2484, Loss: 0.3993462771177292, Final Batch Loss: 0.1906738132238388\n",
      "Epoch 2485, Loss: 0.4465017169713974, Final Batch Loss: 0.22105172276496887\n",
      "Epoch 2486, Loss: 0.4388455003499985, Final Batch Loss: 0.23085738718509674\n",
      "Epoch 2487, Loss: 0.442910373210907, Final Batch Loss: 0.22953000664710999\n",
      "Epoch 2488, Loss: 0.4200623631477356, Final Batch Loss: 0.19470734894275665\n",
      "Epoch 2489, Loss: 0.40672767162323, Final Batch Loss: 0.1988677978515625\n",
      "Epoch 2490, Loss: 0.4735855758190155, Final Batch Loss: 0.23844163119792938\n",
      "Epoch 2491, Loss: 0.4261390119791031, Final Batch Loss: 0.24905554950237274\n",
      "Epoch 2492, Loss: 0.4589342772960663, Final Batch Loss: 0.24648895859718323\n",
      "Epoch 2493, Loss: 0.43222853541374207, Final Batch Loss: 0.2177797108888626\n",
      "Epoch 2494, Loss: 0.5597391426563263, Final Batch Loss: 0.32199349999427795\n",
      "Epoch 2495, Loss: 0.4001873880624771, Final Batch Loss: 0.18303035199642181\n",
      "Epoch 2496, Loss: 0.3898674100637436, Final Batch Loss: 0.1930718868970871\n",
      "Epoch 2497, Loss: 0.43179914355278015, Final Batch Loss: 0.21442300081253052\n",
      "Epoch 2498, Loss: 0.4648822993040085, Final Batch Loss: 0.24174150824546814\n",
      "Epoch 2499, Loss: 0.47288429737091064, Final Batch Loss: 0.28628304600715637\n",
      "Epoch 2500, Loss: 0.4092917889356613, Final Batch Loss: 0.2236930876970291\n",
      "Epoch 2501, Loss: 0.3770265579223633, Final Batch Loss: 0.13341672718524933\n",
      "Epoch 2502, Loss: 0.451465368270874, Final Batch Loss: 0.250364750623703\n",
      "Epoch 2503, Loss: 0.41679441928863525, Final Batch Loss: 0.19353342056274414\n",
      "Epoch 2504, Loss: 0.44368986785411835, Final Batch Loss: 0.19492097198963165\n",
      "Epoch 2505, Loss: 0.4646914452314377, Final Batch Loss: 0.26927974820137024\n",
      "Epoch 2506, Loss: 0.4398694187402725, Final Batch Loss: 0.1807260662317276\n",
      "Epoch 2507, Loss: 0.3966599553823471, Final Batch Loss: 0.21784619987010956\n",
      "Epoch 2508, Loss: 0.46576274931430817, Final Batch Loss: 0.25016361474990845\n",
      "Epoch 2509, Loss: 0.4119329899549484, Final Batch Loss: 0.1903584599494934\n",
      "Epoch 2510, Loss: 0.36948569118976593, Final Batch Loss: 0.16312353312969208\n",
      "Epoch 2511, Loss: 0.44379913806915283, Final Batch Loss: 0.20654095709323883\n",
      "Epoch 2512, Loss: 0.4859226644039154, Final Batch Loss: 0.24568624794483185\n",
      "Epoch 2513, Loss: 0.41068926453590393, Final Batch Loss: 0.2178400456905365\n",
      "Epoch 2514, Loss: 0.4195020794868469, Final Batch Loss: 0.25199222564697266\n",
      "Epoch 2515, Loss: 0.4264248162508011, Final Batch Loss: 0.19322331249713898\n",
      "Epoch 2516, Loss: 0.41345179080963135, Final Batch Loss: 0.20795230567455292\n",
      "Epoch 2517, Loss: 0.43179745972156525, Final Batch Loss: 0.19215704500675201\n",
      "Epoch 2518, Loss: 0.45474444329738617, Final Batch Loss: 0.24690434336662292\n",
      "Epoch 2519, Loss: 0.43195173144340515, Final Batch Loss: 0.17992538213729858\n",
      "Epoch 2520, Loss: 0.4564017504453659, Final Batch Loss: 0.2455994337797165\n",
      "Epoch 2521, Loss: 0.414773166179657, Final Batch Loss: 0.16388526558876038\n",
      "Epoch 2522, Loss: 0.4295288920402527, Final Batch Loss: 0.24325191974639893\n",
      "Epoch 2523, Loss: 0.3974621742963791, Final Batch Loss: 0.21901963651180267\n",
      "Epoch 2524, Loss: 0.42807452380657196, Final Batch Loss: 0.22757799923419952\n",
      "Epoch 2525, Loss: 0.5419173836708069, Final Batch Loss: 0.28776416182518005\n",
      "Epoch 2526, Loss: 0.41060182452201843, Final Batch Loss: 0.19107681512832642\n",
      "Epoch 2527, Loss: 0.46695925295352936, Final Batch Loss: 0.27143359184265137\n",
      "Epoch 2528, Loss: 0.4882524907588959, Final Batch Loss: 0.22827300429344177\n",
      "Epoch 2529, Loss: 0.43336522579193115, Final Batch Loss: 0.2482791543006897\n",
      "Epoch 2530, Loss: 0.3510070890188217, Final Batch Loss: 0.14935266971588135\n",
      "Epoch 2531, Loss: 0.43800222873687744, Final Batch Loss: 0.24585369229316711\n",
      "Epoch 2532, Loss: 0.44879963994026184, Final Batch Loss: 0.2270248681306839\n",
      "Epoch 2533, Loss: 0.38971273601055145, Final Batch Loss: 0.19577060639858246\n",
      "Epoch 2534, Loss: 0.3861321061849594, Final Batch Loss: 0.16696923971176147\n",
      "Epoch 2535, Loss: 0.40388375520706177, Final Batch Loss: 0.2071036696434021\n",
      "Epoch 2536, Loss: 0.39203397929668427, Final Batch Loss: 0.19799838960170746\n",
      "Epoch 2537, Loss: 0.43294309079647064, Final Batch Loss: 0.21855495870113373\n",
      "Epoch 2538, Loss: 0.3966616094112396, Final Batch Loss: 0.21822801232337952\n",
      "Epoch 2539, Loss: 0.42696191370487213, Final Batch Loss: 0.22619383037090302\n",
      "Epoch 2540, Loss: 0.47910986840724945, Final Batch Loss: 0.27386417984962463\n",
      "Epoch 2541, Loss: 0.46169479191303253, Final Batch Loss: 0.2375900000333786\n",
      "Epoch 2542, Loss: 0.4666273444890976, Final Batch Loss: 0.25304877758026123\n",
      "Epoch 2543, Loss: 0.47588546574115753, Final Batch Loss: 0.23372958600521088\n",
      "Epoch 2544, Loss: 0.3749080300331116, Final Batch Loss: 0.15992741286754608\n",
      "Epoch 2545, Loss: 0.4434681087732315, Final Batch Loss: 0.25801196694374084\n",
      "Epoch 2546, Loss: 0.40828488767147064, Final Batch Loss: 0.21426904201507568\n",
      "Epoch 2547, Loss: 0.38588741421699524, Final Batch Loss: 0.21582025289535522\n",
      "Epoch 2548, Loss: 0.4334622025489807, Final Batch Loss: 0.2333158254623413\n",
      "Epoch 2549, Loss: 0.44587959349155426, Final Batch Loss: 0.21751797199249268\n",
      "Epoch 2550, Loss: 0.41028836369514465, Final Batch Loss: 0.19310332834720612\n",
      "Epoch 2551, Loss: 0.4459105581045151, Final Batch Loss: 0.1982513964176178\n",
      "Epoch 2552, Loss: 0.39681242406368256, Final Batch Loss: 0.1755870282649994\n",
      "Epoch 2553, Loss: 0.4459661394357681, Final Batch Loss: 0.21947917342185974\n",
      "Epoch 2554, Loss: 0.40867583453655243, Final Batch Loss: 0.18190200626850128\n",
      "Epoch 2555, Loss: 0.4422300308942795, Final Batch Loss: 0.26126134395599365\n",
      "Epoch 2556, Loss: 0.426393985748291, Final Batch Loss: 0.26998937129974365\n",
      "Epoch 2557, Loss: 0.40244312584400177, Final Batch Loss: 0.202706977725029\n",
      "Epoch 2558, Loss: 0.43300147354602814, Final Batch Loss: 0.23428383469581604\n",
      "Epoch 2559, Loss: 0.4112800657749176, Final Batch Loss: 0.2462238073348999\n",
      "Epoch 2560, Loss: 0.45593081414699554, Final Batch Loss: 0.21076500415802002\n",
      "Epoch 2561, Loss: 0.43881092965602875, Final Batch Loss: 0.22085584700107574\n",
      "Epoch 2562, Loss: 0.44844384491443634, Final Batch Loss: 0.24978262186050415\n",
      "Epoch 2563, Loss: 0.4036340117454529, Final Batch Loss: 0.2272280603647232\n",
      "Epoch 2564, Loss: 0.387503519654274, Final Batch Loss: 0.19975514709949493\n",
      "Epoch 2565, Loss: 0.4005964398384094, Final Batch Loss: 0.19978582859039307\n",
      "Epoch 2566, Loss: 0.4448504000902176, Final Batch Loss: 0.23831553757190704\n",
      "Epoch 2567, Loss: 0.39750875532627106, Final Batch Loss: 0.19136886298656464\n",
      "Epoch 2568, Loss: 0.4444252699613571, Final Batch Loss: 0.2390810251235962\n",
      "Epoch 2569, Loss: 0.36360351741313934, Final Batch Loss: 0.18246518075466156\n",
      "Epoch 2570, Loss: 0.4569029361009598, Final Batch Loss: 0.22952724993228912\n",
      "Epoch 2571, Loss: 0.4069046080112457, Final Batch Loss: 0.22427597641944885\n",
      "Epoch 2572, Loss: 0.4022520184516907, Final Batch Loss: 0.23794929683208466\n",
      "Epoch 2573, Loss: 0.4462178498506546, Final Batch Loss: 0.19606514275074005\n",
      "Epoch 2574, Loss: 0.3917638808488846, Final Batch Loss: 0.20323513448238373\n",
      "Epoch 2575, Loss: 0.4167104959487915, Final Batch Loss: 0.17466241121292114\n",
      "Epoch 2576, Loss: 0.4224466383457184, Final Batch Loss: 0.1908019483089447\n",
      "Epoch 2577, Loss: 0.4343220591545105, Final Batch Loss: 0.23572978377342224\n",
      "Epoch 2578, Loss: 0.42205798625946045, Final Batch Loss: 0.1964794397354126\n",
      "Epoch 2579, Loss: 0.42639292776584625, Final Batch Loss: 0.1906334012746811\n",
      "Epoch 2580, Loss: 0.3703599274158478, Final Batch Loss: 0.15574687719345093\n",
      "Epoch 2581, Loss: 0.38264797627925873, Final Batch Loss: 0.16575391590595245\n",
      "Epoch 2582, Loss: 0.358192503452301, Final Batch Loss: 0.17650102078914642\n",
      "Epoch 2583, Loss: 0.3937361240386963, Final Batch Loss: 0.20772837102413177\n",
      "Epoch 2584, Loss: 0.46222466230392456, Final Batch Loss: 0.21108034253120422\n",
      "Epoch 2585, Loss: 0.4416680783033371, Final Batch Loss: 0.20636340975761414\n",
      "Epoch 2586, Loss: 0.4395867586135864, Final Batch Loss: 0.259441077709198\n",
      "Epoch 2587, Loss: 0.3934551328420639, Final Batch Loss: 0.20974424481391907\n",
      "Epoch 2588, Loss: 0.4097185283899307, Final Batch Loss: 0.2141185998916626\n",
      "Epoch 2589, Loss: 0.4215126484632492, Final Batch Loss: 0.18890032172203064\n",
      "Epoch 2590, Loss: 0.3844088912010193, Final Batch Loss: 0.20201492309570312\n",
      "Epoch 2591, Loss: 0.4632333517074585, Final Batch Loss: 0.2386104166507721\n",
      "Epoch 2592, Loss: 0.43744514882564545, Final Batch Loss: 0.2535426914691925\n",
      "Epoch 2593, Loss: 0.4257979989051819, Final Batch Loss: 0.2151646912097931\n",
      "Epoch 2594, Loss: 0.43079473078250885, Final Batch Loss: 0.21346044540405273\n",
      "Epoch 2595, Loss: 0.4365605115890503, Final Batch Loss: 0.20569854974746704\n",
      "Epoch 2596, Loss: 0.41014648973941803, Final Batch Loss: 0.20829853415489197\n",
      "Epoch 2597, Loss: 0.436048686504364, Final Batch Loss: 0.26873040199279785\n",
      "Epoch 2598, Loss: 0.40028753876686096, Final Batch Loss: 0.21506771445274353\n",
      "Epoch 2599, Loss: 0.4872901439666748, Final Batch Loss: 0.21087121963500977\n",
      "Epoch 2600, Loss: 0.4429890662431717, Final Batch Loss: 0.22122550010681152\n",
      "Epoch 2601, Loss: 0.43649858236312866, Final Batch Loss: 0.21225278079509735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2602, Loss: 0.3875046819448471, Final Batch Loss: 0.18286262452602386\n",
      "Epoch 2603, Loss: 0.38916830718517303, Final Batch Loss: 0.17437982559204102\n",
      "Epoch 2604, Loss: 0.36014023423194885, Final Batch Loss: 0.14251010119915009\n",
      "Epoch 2605, Loss: 0.4060968905687332, Final Batch Loss: 0.21058878302574158\n",
      "Epoch 2606, Loss: 0.4141727238893509, Final Batch Loss: 0.24091880023479462\n",
      "Epoch 2607, Loss: 0.47323866188526154, Final Batch Loss: 0.3078843057155609\n",
      "Epoch 2608, Loss: 0.3913873881101608, Final Batch Loss: 0.16197288036346436\n",
      "Epoch 2609, Loss: 0.34411394596099854, Final Batch Loss: 0.15888303518295288\n",
      "Epoch 2610, Loss: 0.36951276659965515, Final Batch Loss: 0.18189193308353424\n",
      "Epoch 2611, Loss: 0.3984931707382202, Final Batch Loss: 0.2084895372390747\n",
      "Epoch 2612, Loss: 0.478681743144989, Final Batch Loss: 0.24917522072792053\n",
      "Epoch 2613, Loss: 0.3673122376203537, Final Batch Loss: 0.17781950533390045\n",
      "Epoch 2614, Loss: 0.40842053294181824, Final Batch Loss: 0.22397039830684662\n",
      "Epoch 2615, Loss: 0.5251140594482422, Final Batch Loss: 0.32335415482521057\n",
      "Epoch 2616, Loss: 0.3991112560033798, Final Batch Loss: 0.16503141820430756\n",
      "Epoch 2617, Loss: 0.4096659719944, Final Batch Loss: 0.20706109702587128\n",
      "Epoch 2618, Loss: 0.44294923543930054, Final Batch Loss: 0.19245648384094238\n",
      "Epoch 2619, Loss: 0.4524580240249634, Final Batch Loss: 0.22708909213542938\n",
      "Epoch 2620, Loss: 0.42838890850543976, Final Batch Loss: 0.1690596491098404\n",
      "Epoch 2621, Loss: 0.46653617918491364, Final Batch Loss: 0.23968687653541565\n",
      "Epoch 2622, Loss: 0.40170568227767944, Final Batch Loss: 0.1804639995098114\n",
      "Epoch 2623, Loss: 0.3836975544691086, Final Batch Loss: 0.18854264914989471\n",
      "Epoch 2624, Loss: 0.36199064552783966, Final Batch Loss: 0.20032928884029388\n",
      "Epoch 2625, Loss: 0.4156411439180374, Final Batch Loss: 0.20328179001808167\n",
      "Epoch 2626, Loss: 0.3965306133031845, Final Batch Loss: 0.18482543528079987\n",
      "Epoch 2627, Loss: 0.4363781809806824, Final Batch Loss: 0.21277554333209991\n",
      "Epoch 2628, Loss: 0.4175039529800415, Final Batch Loss: 0.21449387073516846\n",
      "Epoch 2629, Loss: 0.38566599786281586, Final Batch Loss: 0.20708535611629486\n",
      "Epoch 2630, Loss: 0.4360019117593765, Final Batch Loss: 0.23777201771736145\n",
      "Epoch 2631, Loss: 0.3964824229478836, Final Batch Loss: 0.1873384714126587\n",
      "Epoch 2632, Loss: 0.35102084279060364, Final Batch Loss: 0.140950545668602\n",
      "Epoch 2633, Loss: 0.46829694509506226, Final Batch Loss: 0.22057820856571198\n",
      "Epoch 2634, Loss: 0.40601831674575806, Final Batch Loss: 0.24426165223121643\n",
      "Epoch 2635, Loss: 0.4074364900588989, Final Batch Loss: 0.22174794971942902\n",
      "Epoch 2636, Loss: 0.3854654133319855, Final Batch Loss: 0.1949901431798935\n",
      "Epoch 2637, Loss: 0.5214971601963043, Final Batch Loss: 0.26250314712524414\n",
      "Epoch 2638, Loss: 0.4453302323818207, Final Batch Loss: 0.22029277682304382\n",
      "Epoch 2639, Loss: 0.3938106745481491, Final Batch Loss: 0.19791530072689056\n",
      "Epoch 2640, Loss: 0.46865007281303406, Final Batch Loss: 0.25676771998405457\n",
      "Epoch 2641, Loss: 0.4692983478307724, Final Batch Loss: 0.23644289374351501\n",
      "Epoch 2642, Loss: 0.49477963149547577, Final Batch Loss: 0.22846619784832\n",
      "Epoch 2643, Loss: 0.38965097069740295, Final Batch Loss: 0.17586831748485565\n",
      "Epoch 2644, Loss: 0.4181651175022125, Final Batch Loss: 0.2012551873922348\n",
      "Epoch 2645, Loss: 0.4182289093732834, Final Batch Loss: 0.21460382640361786\n",
      "Epoch 2646, Loss: 0.42765821516513824, Final Batch Loss: 0.20027267932891846\n",
      "Epoch 2647, Loss: 0.4619157612323761, Final Batch Loss: 0.2742663621902466\n",
      "Epoch 2648, Loss: 0.38964833319187164, Final Batch Loss: 0.18072402477264404\n",
      "Epoch 2649, Loss: 0.38100649416446686, Final Batch Loss: 0.20618152618408203\n",
      "Epoch 2650, Loss: 0.32851460576057434, Final Batch Loss: 0.16112329065799713\n",
      "Epoch 2651, Loss: 0.45448918640613556, Final Batch Loss: 0.21892444789409637\n",
      "Epoch 2652, Loss: 0.46567103266716003, Final Batch Loss: 0.21261408925056458\n",
      "Epoch 2653, Loss: 0.40640445053577423, Final Batch Loss: 0.21824954450130463\n",
      "Epoch 2654, Loss: 0.4389399439096451, Final Batch Loss: 0.22736015915870667\n",
      "Epoch 2655, Loss: 0.4396540969610214, Final Batch Loss: 0.21215160191059113\n",
      "Epoch 2656, Loss: 0.3853401690721512, Final Batch Loss: 0.22048138082027435\n",
      "Epoch 2657, Loss: 0.35685305297374725, Final Batch Loss: 0.1718665212392807\n",
      "Epoch 2658, Loss: 0.34618183970451355, Final Batch Loss: 0.1553371548652649\n",
      "Epoch 2659, Loss: 0.38856810331344604, Final Batch Loss: 0.22076807916164398\n",
      "Epoch 2660, Loss: 0.38854771852493286, Final Batch Loss: 0.19258391857147217\n",
      "Epoch 2661, Loss: 0.4297788739204407, Final Batch Loss: 0.24546624720096588\n",
      "Epoch 2662, Loss: 0.41781386733055115, Final Batch Loss: 0.23413503170013428\n",
      "Epoch 2663, Loss: 0.40195707976818085, Final Batch Loss: 0.1928471177816391\n",
      "Epoch 2664, Loss: 0.4520847201347351, Final Batch Loss: 0.2733837962150574\n",
      "Epoch 2665, Loss: 0.39070481061935425, Final Batch Loss: 0.18034911155700684\n",
      "Epoch 2666, Loss: 0.4008101224899292, Final Batch Loss: 0.20158766210079193\n",
      "Epoch 2667, Loss: 0.3944924622774124, Final Batch Loss: 0.20386208593845367\n",
      "Epoch 2668, Loss: 0.4822901636362076, Final Batch Loss: 0.24154281616210938\n",
      "Epoch 2669, Loss: 0.42379045486450195, Final Batch Loss: 0.20687445998191833\n",
      "Epoch 2670, Loss: 0.46132107079029083, Final Batch Loss: 0.267922043800354\n",
      "Epoch 2671, Loss: 0.40148235857486725, Final Batch Loss: 0.1895143687725067\n",
      "Epoch 2672, Loss: 0.433009535074234, Final Batch Loss: 0.2524300217628479\n",
      "Epoch 2673, Loss: 0.41297121345996857, Final Batch Loss: 0.20894214510917664\n",
      "Epoch 2674, Loss: 0.3965568393468857, Final Batch Loss: 0.1958247423171997\n",
      "Epoch 2675, Loss: 0.3805297911167145, Final Batch Loss: 0.1746169626712799\n",
      "Epoch 2676, Loss: 0.37943029403686523, Final Batch Loss: 0.22558638453483582\n",
      "Epoch 2677, Loss: 0.42768873274326324, Final Batch Loss: 0.23013374209403992\n",
      "Epoch 2678, Loss: 0.4060712307691574, Final Batch Loss: 0.22518788278102875\n",
      "Epoch 2679, Loss: 0.3899602144956589, Final Batch Loss: 0.18643830716609955\n",
      "Epoch 2680, Loss: 0.43983806669712067, Final Batch Loss: 0.20717200636863708\n",
      "Epoch 2681, Loss: 0.37511569261550903, Final Batch Loss: 0.20987096428871155\n",
      "Epoch 2682, Loss: 0.4611496925354004, Final Batch Loss: 0.24790862202644348\n",
      "Epoch 2683, Loss: 0.38972628116607666, Final Batch Loss: 0.16659791767597198\n",
      "Epoch 2684, Loss: 0.4590303897857666, Final Batch Loss: 0.27161186933517456\n",
      "Epoch 2685, Loss: 0.375415101647377, Final Batch Loss: 0.1254555881023407\n",
      "Epoch 2686, Loss: 0.47670362889766693, Final Batch Loss: 0.23446905612945557\n",
      "Epoch 2687, Loss: 0.4282746762037277, Final Batch Loss: 0.23245666921138763\n",
      "Epoch 2688, Loss: 0.38827966153621674, Final Batch Loss: 0.18633228540420532\n",
      "Epoch 2689, Loss: 0.44557468593120575, Final Batch Loss: 0.21653702855110168\n",
      "Epoch 2690, Loss: 0.3620155453681946, Final Batch Loss: 0.1794002652168274\n",
      "Epoch 2691, Loss: 0.3874157965183258, Final Batch Loss: 0.2111876904964447\n",
      "Epoch 2692, Loss: 0.40060126781463623, Final Batch Loss: 0.2013566792011261\n",
      "Epoch 2693, Loss: 0.4459252953529358, Final Batch Loss: 0.22508108615875244\n",
      "Epoch 2694, Loss: 0.4387698769569397, Final Batch Loss: 0.2206820696592331\n",
      "Epoch 2695, Loss: 0.3683861047029495, Final Batch Loss: 0.19650469720363617\n",
      "Epoch 2696, Loss: 0.5266077518463135, Final Batch Loss: 0.2658979296684265\n",
      "Epoch 2697, Loss: 0.4024864137172699, Final Batch Loss: 0.1704447865486145\n",
      "Epoch 2698, Loss: 0.3838716596364975, Final Batch Loss: 0.1546819508075714\n",
      "Epoch 2699, Loss: 0.49358828365802765, Final Batch Loss: 0.24994626641273499\n",
      "Epoch 2700, Loss: 0.44351036846637726, Final Batch Loss: 0.2443106323480606\n",
      "Epoch 2701, Loss: 0.45285850763320923, Final Batch Loss: 0.25459474325180054\n",
      "Epoch 2702, Loss: 0.40176187455654144, Final Batch Loss: 0.19080734252929688\n",
      "Epoch 2703, Loss: 0.48266758024692535, Final Batch Loss: 0.24217930436134338\n",
      "Epoch 2704, Loss: 0.4065624326467514, Final Batch Loss: 0.18949629366397858\n",
      "Epoch 2705, Loss: 0.38382601737976074, Final Batch Loss: 0.18466141819953918\n",
      "Epoch 2706, Loss: 0.42991994321346283, Final Batch Loss: 0.2228281944990158\n",
      "Epoch 2707, Loss: 0.40236344933509827, Final Batch Loss: 0.22549758851528168\n",
      "Epoch 2708, Loss: 0.40909427404403687, Final Batch Loss: 0.22602719068527222\n",
      "Epoch 2709, Loss: 0.3965335935354233, Final Batch Loss: 0.1880548894405365\n",
      "Epoch 2710, Loss: 0.4056268483400345, Final Batch Loss: 0.19296269118785858\n",
      "Epoch 2711, Loss: 0.3974996507167816, Final Batch Loss: 0.19958849251270294\n",
      "Epoch 2712, Loss: 0.3902592062950134, Final Batch Loss: 0.18944348394870758\n",
      "Epoch 2713, Loss: 0.36263346672058105, Final Batch Loss: 0.1824764907360077\n",
      "Epoch 2714, Loss: 0.38972240686416626, Final Batch Loss: 0.19619625806808472\n",
      "Epoch 2715, Loss: 0.3432013839483261, Final Batch Loss: 0.18802562355995178\n",
      "Epoch 2716, Loss: 0.4282955229282379, Final Batch Loss: 0.1956934630870819\n",
      "Epoch 2717, Loss: 0.42224904894828796, Final Batch Loss: 0.25137966871261597\n",
      "Epoch 2718, Loss: 0.4255952835083008, Final Batch Loss: 0.1466766595840454\n",
      "Epoch 2719, Loss: 0.45386141538619995, Final Batch Loss: 0.2354065626859665\n",
      "Epoch 2720, Loss: 0.37134861946105957, Final Batch Loss: 0.14913903176784515\n",
      "Epoch 2721, Loss: 0.4112890213727951, Final Batch Loss: 0.2510058581829071\n",
      "Epoch 2722, Loss: 0.39272381365299225, Final Batch Loss: 0.19189241528511047\n",
      "Epoch 2723, Loss: 0.4415297210216522, Final Batch Loss: 0.26685306429862976\n",
      "Epoch 2724, Loss: 0.3755357563495636, Final Batch Loss: 0.18829748034477234\n",
      "Epoch 2725, Loss: 0.39012089371681213, Final Batch Loss: 0.22072561085224152\n",
      "Epoch 2726, Loss: 0.37667398154735565, Final Batch Loss: 0.18412770330905914\n",
      "Epoch 2727, Loss: 0.43926721811294556, Final Batch Loss: 0.18760526180267334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2728, Loss: 0.5000201761722565, Final Batch Loss: 0.26216596364974976\n",
      "Epoch 2729, Loss: 0.42085306346416473, Final Batch Loss: 0.26010438799858093\n",
      "Epoch 2730, Loss: 0.3929736614227295, Final Batch Loss: 0.19274194538593292\n",
      "Epoch 2731, Loss: 0.41355548799037933, Final Batch Loss: 0.15180687606334686\n",
      "Epoch 2732, Loss: 0.3506082445383072, Final Batch Loss: 0.19029894471168518\n",
      "Epoch 2733, Loss: 0.4724418967962265, Final Batch Loss: 0.27394336462020874\n",
      "Epoch 2734, Loss: 0.4041444659233093, Final Batch Loss: 0.20906323194503784\n",
      "Epoch 2735, Loss: 0.40153002738952637, Final Batch Loss: 0.24470873177051544\n",
      "Epoch 2736, Loss: 0.3477336913347244, Final Batch Loss: 0.1760299652814865\n",
      "Epoch 2737, Loss: 0.402291402220726, Final Batch Loss: 0.22124958038330078\n",
      "Epoch 2738, Loss: 0.4558728486299515, Final Batch Loss: 0.19095878303050995\n",
      "Epoch 2739, Loss: 0.4458897262811661, Final Batch Loss: 0.21723408997058868\n",
      "Epoch 2740, Loss: 0.41807883977890015, Final Batch Loss: 0.23683443665504456\n",
      "Epoch 2741, Loss: 0.35092538595199585, Final Batch Loss: 0.17111821472644806\n",
      "Epoch 2742, Loss: 0.41148926317691803, Final Batch Loss: 0.23138414323329926\n",
      "Epoch 2743, Loss: 0.4439350515604019, Final Batch Loss: 0.23721900582313538\n",
      "Epoch 2744, Loss: 0.45047545433044434, Final Batch Loss: 0.2486962229013443\n",
      "Epoch 2745, Loss: 0.47121164202690125, Final Batch Loss: 0.2778574526309967\n",
      "Epoch 2746, Loss: 0.37586140632629395, Final Batch Loss: 0.21424752473831177\n",
      "Epoch 2747, Loss: 0.43965059518814087, Final Batch Loss: 0.243180513381958\n",
      "Epoch 2748, Loss: 0.40274859964847565, Final Batch Loss: 0.13928069174289703\n",
      "Epoch 2749, Loss: 0.4231262654066086, Final Batch Loss: 0.21831336617469788\n",
      "Epoch 2750, Loss: 0.37841935455799103, Final Batch Loss: 0.17380711436271667\n",
      "Epoch 2751, Loss: 0.40463700890541077, Final Batch Loss: 0.20284941792488098\n",
      "Epoch 2752, Loss: 0.44685521721839905, Final Batch Loss: 0.24158760905265808\n",
      "Epoch 2753, Loss: 0.39061784744262695, Final Batch Loss: 0.18237440288066864\n",
      "Epoch 2754, Loss: 0.3284275084733963, Final Batch Loss: 0.1501842439174652\n",
      "Epoch 2755, Loss: 0.3873402923345566, Final Batch Loss: 0.17352978885173798\n",
      "Epoch 2756, Loss: 0.3681657463312149, Final Batch Loss: 0.15397325158119202\n",
      "Epoch 2757, Loss: 0.3964548259973526, Final Batch Loss: 0.2116924524307251\n",
      "Epoch 2758, Loss: 0.3711507022380829, Final Batch Loss: 0.18218685686588287\n",
      "Epoch 2759, Loss: 0.40941208600997925, Final Batch Loss: 0.17311041057109833\n",
      "Epoch 2760, Loss: 0.37731748819351196, Final Batch Loss: 0.16567851603031158\n",
      "Epoch 2761, Loss: 0.3834056556224823, Final Batch Loss: 0.1782824546098709\n",
      "Epoch 2762, Loss: 0.45767608284950256, Final Batch Loss: 0.19659405946731567\n",
      "Epoch 2763, Loss: 0.3809507489204407, Final Batch Loss: 0.15552812814712524\n",
      "Epoch 2764, Loss: 0.31190958619117737, Final Batch Loss: 0.15768228471279144\n",
      "Epoch 2765, Loss: 0.3847315013408661, Final Batch Loss: 0.21939781308174133\n",
      "Epoch 2766, Loss: 0.4788120985031128, Final Batch Loss: 0.23929904401302338\n",
      "Epoch 2767, Loss: 0.39122752845287323, Final Batch Loss: 0.16448740661144257\n",
      "Epoch 2768, Loss: 0.4091748893260956, Final Batch Loss: 0.1877213716506958\n",
      "Epoch 2769, Loss: 0.39526621997356415, Final Batch Loss: 0.1658400148153305\n",
      "Epoch 2770, Loss: 0.4163607656955719, Final Batch Loss: 0.22096188366413116\n",
      "Epoch 2771, Loss: 0.43157413601875305, Final Batch Loss: 0.21352644264698029\n",
      "Epoch 2772, Loss: 0.390164777636528, Final Batch Loss: 0.21999774873256683\n",
      "Epoch 2773, Loss: 0.38596102595329285, Final Batch Loss: 0.1968439221382141\n",
      "Epoch 2774, Loss: 0.33533692359924316, Final Batch Loss: 0.17190654575824738\n",
      "Epoch 2775, Loss: 0.4248792976140976, Final Batch Loss: 0.25662851333618164\n",
      "Epoch 2776, Loss: 0.3240150213241577, Final Batch Loss: 0.1430133581161499\n",
      "Epoch 2777, Loss: 0.43962039053440094, Final Batch Loss: 0.2533177435398102\n",
      "Epoch 2778, Loss: 0.4090501070022583, Final Batch Loss: 0.19318269193172455\n",
      "Epoch 2779, Loss: 0.36971141397953033, Final Batch Loss: 0.18176600337028503\n",
      "Epoch 2780, Loss: 0.3321041911840439, Final Batch Loss: 0.15640556812286377\n",
      "Epoch 2781, Loss: 0.32552042603492737, Final Batch Loss: 0.16098691523075104\n",
      "Epoch 2782, Loss: 0.3980814963579178, Final Batch Loss: 0.21228797733783722\n",
      "Epoch 2783, Loss: 0.3507239371538162, Final Batch Loss: 0.18869952857494354\n",
      "Epoch 2784, Loss: 0.37307676672935486, Final Batch Loss: 0.19935669004917145\n",
      "Epoch 2785, Loss: 0.42703859508037567, Final Batch Loss: 0.21698474884033203\n",
      "Epoch 2786, Loss: 0.41075386106967926, Final Batch Loss: 0.19179874658584595\n",
      "Epoch 2787, Loss: 0.4493807256221771, Final Batch Loss: 0.2820805609226227\n",
      "Epoch 2788, Loss: 0.4112584888935089, Final Batch Loss: 0.20939995348453522\n",
      "Epoch 2789, Loss: 0.37031178176403046, Final Batch Loss: 0.17403413355350494\n",
      "Epoch 2790, Loss: 0.4327622354030609, Final Batch Loss: 0.21875198185443878\n",
      "Epoch 2791, Loss: 0.428534597158432, Final Batch Loss: 0.248977392911911\n",
      "Epoch 2792, Loss: 0.37409090995788574, Final Batch Loss: 0.1912165880203247\n",
      "Epoch 2793, Loss: 0.38745924830436707, Final Batch Loss: 0.19226205348968506\n",
      "Epoch 2794, Loss: 0.3471999913454056, Final Batch Loss: 0.164231076836586\n",
      "Epoch 2795, Loss: 0.42736247181892395, Final Batch Loss: 0.24646976590156555\n",
      "Epoch 2796, Loss: 0.32802753150463104, Final Batch Loss: 0.18327754735946655\n",
      "Epoch 2797, Loss: 0.39099015295505524, Final Batch Loss: 0.20285317301750183\n",
      "Epoch 2798, Loss: 0.40356864035129547, Final Batch Loss: 0.22986584901809692\n",
      "Epoch 2799, Loss: 0.4167923033237457, Final Batch Loss: 0.18851400911808014\n",
      "Epoch 2800, Loss: 0.41760145127773285, Final Batch Loss: 0.23371803760528564\n",
      "Epoch 2801, Loss: 0.4845597445964813, Final Batch Loss: 0.3085872232913971\n",
      "Epoch 2802, Loss: 0.37378472089767456, Final Batch Loss: 0.1999385505914688\n",
      "Epoch 2803, Loss: 0.398101270198822, Final Batch Loss: 0.19444844126701355\n",
      "Epoch 2804, Loss: 0.44445304572582245, Final Batch Loss: 0.23129279911518097\n",
      "Epoch 2805, Loss: 0.3980976492166519, Final Batch Loss: 0.21731169521808624\n",
      "Epoch 2806, Loss: 0.32538506388664246, Final Batch Loss: 0.1547638475894928\n",
      "Epoch 2807, Loss: 0.37706027925014496, Final Batch Loss: 0.1969127207994461\n",
      "Epoch 2808, Loss: 0.4662471413612366, Final Batch Loss: 0.19680365920066833\n",
      "Epoch 2809, Loss: 0.43287262320518494, Final Batch Loss: 0.19504250586032867\n",
      "Epoch 2810, Loss: 0.4141746610403061, Final Batch Loss: 0.21416202187538147\n",
      "Epoch 2811, Loss: 0.41191041469573975, Final Batch Loss: 0.20665746927261353\n",
      "Epoch 2812, Loss: 0.3875321298837662, Final Batch Loss: 0.15693160891532898\n",
      "Epoch 2813, Loss: 0.3514404147863388, Final Batch Loss: 0.14520572125911713\n",
      "Epoch 2814, Loss: 0.38745950162410736, Final Batch Loss: 0.19179990887641907\n",
      "Epoch 2815, Loss: 0.41235101222991943, Final Batch Loss: 0.21515080332756042\n",
      "Epoch 2816, Loss: 0.35357216000556946, Final Batch Loss: 0.18183794617652893\n",
      "Epoch 2817, Loss: 0.43750181794166565, Final Batch Loss: 0.2441672682762146\n",
      "Epoch 2818, Loss: 0.38815726339817047, Final Batch Loss: 0.18273738026618958\n",
      "Epoch 2819, Loss: 0.44000954926013947, Final Batch Loss: 0.19762107729911804\n",
      "Epoch 2820, Loss: 0.35497821867465973, Final Batch Loss: 0.15529705584049225\n",
      "Epoch 2821, Loss: 0.5325158387422562, Final Batch Loss: 0.221735879778862\n",
      "Epoch 2822, Loss: 0.36250269412994385, Final Batch Loss: 0.129775732755661\n",
      "Epoch 2823, Loss: 0.36851659417152405, Final Batch Loss: 0.19905085861682892\n",
      "Epoch 2824, Loss: 0.4265608787536621, Final Batch Loss: 0.23468247056007385\n",
      "Epoch 2825, Loss: 0.3795931935310364, Final Batch Loss: 0.17361438274383545\n",
      "Epoch 2826, Loss: 0.37579502165317535, Final Batch Loss: 0.198307603597641\n",
      "Epoch 2827, Loss: 0.3259292244911194, Final Batch Loss: 0.13074609637260437\n",
      "Epoch 2828, Loss: 0.4595801830291748, Final Batch Loss: 0.23392482101917267\n",
      "Epoch 2829, Loss: 0.4271513819694519, Final Batch Loss: 0.22938957810401917\n",
      "Epoch 2830, Loss: 0.3894519507884979, Final Batch Loss: 0.16471706330776215\n",
      "Epoch 2831, Loss: 0.3564009815454483, Final Batch Loss: 0.17896823585033417\n",
      "Epoch 2832, Loss: 0.38274912536144257, Final Batch Loss: 0.1748068481683731\n",
      "Epoch 2833, Loss: 0.35830509662628174, Final Batch Loss: 0.17866931855678558\n",
      "Epoch 2834, Loss: 0.353182390332222, Final Batch Loss: 0.15683291852474213\n",
      "Epoch 2835, Loss: 0.33271624147892, Final Batch Loss: 0.179054394364357\n",
      "Epoch 2836, Loss: 0.34059901535511017, Final Batch Loss: 0.18402047455310822\n",
      "Epoch 2837, Loss: 0.4423312544822693, Final Batch Loss: 0.24962787330150604\n",
      "Epoch 2838, Loss: 0.45196114480495453, Final Batch Loss: 0.232758030295372\n",
      "Epoch 2839, Loss: 0.3503754884004593, Final Batch Loss: 0.18310829997062683\n",
      "Epoch 2840, Loss: 0.43881046772003174, Final Batch Loss: 0.25188779830932617\n",
      "Epoch 2841, Loss: 0.36196082830429077, Final Batch Loss: 0.200814887881279\n",
      "Epoch 2842, Loss: 0.46432361006736755, Final Batch Loss: 0.23532550036907196\n",
      "Epoch 2843, Loss: 0.4152098596096039, Final Batch Loss: 0.2443229705095291\n",
      "Epoch 2844, Loss: 0.39034129679203033, Final Batch Loss: 0.20399829745292664\n",
      "Epoch 2845, Loss: 0.4141766279935837, Final Batch Loss: 0.2228851169347763\n",
      "Epoch 2846, Loss: 0.4324598163366318, Final Batch Loss: 0.20783711969852448\n",
      "Epoch 2847, Loss: 0.3834493011236191, Final Batch Loss: 0.1646435707807541\n",
      "Epoch 2848, Loss: 0.379236176609993, Final Batch Loss: 0.18459877371788025\n",
      "Epoch 2849, Loss: 0.40333564579486847, Final Batch Loss: 0.189031183719635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2850, Loss: 0.3677048087120056, Final Batch Loss: 0.15536990761756897\n",
      "Epoch 2851, Loss: 0.3546796143054962, Final Batch Loss: 0.14139078557491302\n",
      "Epoch 2852, Loss: 0.33580242097377777, Final Batch Loss: 0.16083049774169922\n",
      "Epoch 2853, Loss: 0.37409698963165283, Final Batch Loss: 0.1835060864686966\n",
      "Epoch 2854, Loss: 0.3463785648345947, Final Batch Loss: 0.16569830477237701\n",
      "Epoch 2855, Loss: 0.34394845366477966, Final Batch Loss: 0.15238720178604126\n",
      "Epoch 2856, Loss: 0.38727298378944397, Final Batch Loss: 0.22578948736190796\n",
      "Epoch 2857, Loss: 0.37098297476768494, Final Batch Loss: 0.14778901636600494\n",
      "Epoch 2858, Loss: 0.31308604776859283, Final Batch Loss: 0.15719468891620636\n",
      "Epoch 2859, Loss: 0.34581053256988525, Final Batch Loss: 0.15520764887332916\n",
      "Epoch 2860, Loss: 0.40368877351284027, Final Batch Loss: 0.20338860154151917\n",
      "Epoch 2861, Loss: 0.41132836043834686, Final Batch Loss: 0.2035851925611496\n",
      "Epoch 2862, Loss: 0.4149703234434128, Final Batch Loss: 0.2094898372888565\n",
      "Epoch 2863, Loss: 0.35941702127456665, Final Batch Loss: 0.18140441179275513\n",
      "Epoch 2864, Loss: 0.34095968306064606, Final Batch Loss: 0.18992775678634644\n",
      "Epoch 2865, Loss: 0.35527685284614563, Final Batch Loss: 0.18494471907615662\n",
      "Epoch 2866, Loss: 0.34695254266262054, Final Batch Loss: 0.1851208657026291\n",
      "Epoch 2867, Loss: 0.3898364305496216, Final Batch Loss: 0.21638186275959015\n",
      "Epoch 2868, Loss: 0.36897291243076324, Final Batch Loss: 0.17661939561367035\n",
      "Epoch 2869, Loss: 0.39214202761650085, Final Batch Loss: 0.21403621137142181\n",
      "Epoch 2870, Loss: 0.3642210513353348, Final Batch Loss: 0.18730711936950684\n",
      "Epoch 2871, Loss: 0.38839924335479736, Final Batch Loss: 0.23089398443698883\n",
      "Epoch 2872, Loss: 0.3742036819458008, Final Batch Loss: 0.17437079548835754\n",
      "Epoch 2873, Loss: 0.3938164860010147, Final Batch Loss: 0.1831531971693039\n",
      "Epoch 2874, Loss: 0.3904547393321991, Final Batch Loss: 0.19103799760341644\n",
      "Epoch 2875, Loss: 0.39632612466812134, Final Batch Loss: 0.22851239144802094\n",
      "Epoch 2876, Loss: 0.404141366481781, Final Batch Loss: 0.2232692688703537\n",
      "Epoch 2877, Loss: 0.43914470076560974, Final Batch Loss: 0.2203330546617508\n",
      "Epoch 2878, Loss: 0.3410271108150482, Final Batch Loss: 0.18717700242996216\n",
      "Epoch 2879, Loss: 0.37614330649375916, Final Batch Loss: 0.1945028156042099\n",
      "Epoch 2880, Loss: 0.4354191720485687, Final Batch Loss: 0.20113246142864227\n",
      "Epoch 2881, Loss: 0.41505350172519684, Final Batch Loss: 0.20353484153747559\n",
      "Epoch 2882, Loss: 0.37542369961738586, Final Batch Loss: 0.210075244307518\n",
      "Epoch 2883, Loss: 0.41531309485435486, Final Batch Loss: 0.1675003319978714\n",
      "Epoch 2884, Loss: 0.38528968393802643, Final Batch Loss: 0.19106332957744598\n",
      "Epoch 2885, Loss: 0.4229365885257721, Final Batch Loss: 0.17684030532836914\n",
      "Epoch 2886, Loss: 0.38116055727005005, Final Batch Loss: 0.19090835750102997\n",
      "Epoch 2887, Loss: 0.4320419877767563, Final Batch Loss: 0.19353099167346954\n",
      "Epoch 2888, Loss: 0.3408816456794739, Final Batch Loss: 0.149791419506073\n",
      "Epoch 2889, Loss: 0.38899460434913635, Final Batch Loss: 0.19582994282245636\n",
      "Epoch 2890, Loss: 0.33597463369369507, Final Batch Loss: 0.15950143337249756\n",
      "Epoch 2891, Loss: 0.3636508733034134, Final Batch Loss: 0.14840582013130188\n",
      "Epoch 2892, Loss: 0.40509870648384094, Final Batch Loss: 0.19076623022556305\n",
      "Epoch 2893, Loss: 0.36587877571582794, Final Batch Loss: 0.1632850021123886\n",
      "Epoch 2894, Loss: 0.36991041898727417, Final Batch Loss: 0.19622443616390228\n",
      "Epoch 2895, Loss: 0.40660908818244934, Final Batch Loss: 0.18022291362285614\n",
      "Epoch 2896, Loss: 0.39751341938972473, Final Batch Loss: 0.18899565935134888\n",
      "Epoch 2897, Loss: 0.3469913601875305, Final Batch Loss: 0.14562445878982544\n",
      "Epoch 2898, Loss: 0.4105512499809265, Final Batch Loss: 0.19115936756134033\n",
      "Epoch 2899, Loss: 0.39741791784763336, Final Batch Loss: 0.21779312193393707\n",
      "Epoch 2900, Loss: 0.388666108250618, Final Batch Loss: 0.18119123578071594\n",
      "Epoch 2901, Loss: 0.34444355964660645, Final Batch Loss: 0.1579262614250183\n",
      "Epoch 2902, Loss: 0.3921821266412735, Final Batch Loss: 0.20126332342624664\n",
      "Epoch 2903, Loss: 0.4028233140707016, Final Batch Loss: 0.22154076397418976\n",
      "Epoch 2904, Loss: 0.35591746866703033, Final Batch Loss: 0.1938939392566681\n",
      "Epoch 2905, Loss: 0.3731153607368469, Final Batch Loss: 0.19259408116340637\n",
      "Epoch 2906, Loss: 0.3784828931093216, Final Batch Loss: 0.18948735296726227\n",
      "Epoch 2907, Loss: 0.3936571031808853, Final Batch Loss: 0.21237261593341827\n",
      "Epoch 2908, Loss: 0.4047376364469528, Final Batch Loss: 0.1596609503030777\n",
      "Epoch 2909, Loss: 0.40228724479675293, Final Batch Loss: 0.17886346578598022\n",
      "Epoch 2910, Loss: 0.33412735164165497, Final Batch Loss: 0.17272235453128815\n",
      "Epoch 2911, Loss: 0.38648612797260284, Final Batch Loss: 0.2029741257429123\n",
      "Epoch 2912, Loss: 0.44159381091594696, Final Batch Loss: 0.22606679797172546\n",
      "Epoch 2913, Loss: 0.39956943690776825, Final Batch Loss: 0.18244381248950958\n",
      "Epoch 2914, Loss: 0.39248619973659515, Final Batch Loss: 0.17379295825958252\n",
      "Epoch 2915, Loss: 0.3386617451906204, Final Batch Loss: 0.1727803349494934\n",
      "Epoch 2916, Loss: 0.34572622179985046, Final Batch Loss: 0.14508101344108582\n",
      "Epoch 2917, Loss: 0.34173694252967834, Final Batch Loss: 0.17574025690555573\n",
      "Epoch 2918, Loss: 0.4417352229356766, Final Batch Loss: 0.21861301362514496\n",
      "Epoch 2919, Loss: 0.4330046623945236, Final Batch Loss: 0.25211459398269653\n",
      "Epoch 2920, Loss: 0.3797595649957657, Final Batch Loss: 0.18956510722637177\n",
      "Epoch 2921, Loss: 0.3837420344352722, Final Batch Loss: 0.2264079600572586\n",
      "Epoch 2922, Loss: 0.39804746210575104, Final Batch Loss: 0.18240129947662354\n",
      "Epoch 2923, Loss: 0.36462782323360443, Final Batch Loss: 0.2199413776397705\n",
      "Epoch 2924, Loss: 0.3503471314907074, Final Batch Loss: 0.1773470938205719\n",
      "Epoch 2925, Loss: 0.3328985273838043, Final Batch Loss: 0.16074347496032715\n",
      "Epoch 2926, Loss: 0.39243383705616, Final Batch Loss: 0.20002864301204681\n",
      "Epoch 2927, Loss: 0.38663363456726074, Final Batch Loss: 0.17278096079826355\n",
      "Epoch 2928, Loss: 0.3803750276565552, Final Batch Loss: 0.20315580070018768\n",
      "Epoch 2929, Loss: 0.34706689417362213, Final Batch Loss: 0.16787289083003998\n",
      "Epoch 2930, Loss: 0.38034315407276154, Final Batch Loss: 0.19074945151805878\n",
      "Epoch 2931, Loss: 0.33155620098114014, Final Batch Loss: 0.19046272337436676\n",
      "Epoch 2932, Loss: 0.3954775780439377, Final Batch Loss: 0.17922210693359375\n",
      "Epoch 2933, Loss: 0.3474762737751007, Final Batch Loss: 0.1586281806230545\n",
      "Epoch 2934, Loss: 0.3350796699523926, Final Batch Loss: 0.18802843987941742\n",
      "Epoch 2935, Loss: 0.3690279871225357, Final Batch Loss: 0.1714392751455307\n",
      "Epoch 2936, Loss: 0.37204739451408386, Final Batch Loss: 0.12856319546699524\n",
      "Epoch 2937, Loss: 0.40528343617916107, Final Batch Loss: 0.23048804700374603\n",
      "Epoch 2938, Loss: 0.36090509593486786, Final Batch Loss: 0.1526833325624466\n",
      "Epoch 2939, Loss: 0.37456412613391876, Final Batch Loss: 0.19552919268608093\n",
      "Epoch 2940, Loss: 0.3779062181711197, Final Batch Loss: 0.19760076701641083\n",
      "Epoch 2941, Loss: 0.38640737533569336, Final Batch Loss: 0.20610511302947998\n",
      "Epoch 2942, Loss: 0.41562798619270325, Final Batch Loss: 0.22819668054580688\n",
      "Epoch 2943, Loss: 0.395007461309433, Final Batch Loss: 0.18563449382781982\n",
      "Epoch 2944, Loss: 0.4236372262239456, Final Batch Loss: 0.2144080400466919\n",
      "Epoch 2945, Loss: 0.3851689249277115, Final Batch Loss: 0.16518253087997437\n",
      "Epoch 2946, Loss: 0.34614168107509613, Final Batch Loss: 0.15267758071422577\n",
      "Epoch 2947, Loss: 0.3449417054653168, Final Batch Loss: 0.16711954772472382\n",
      "Epoch 2948, Loss: 0.3523746132850647, Final Batch Loss: 0.22083881497383118\n",
      "Epoch 2949, Loss: 0.3514593839645386, Final Batch Loss: 0.14308664202690125\n",
      "Epoch 2950, Loss: 0.36781275272369385, Final Batch Loss: 0.15256854891777039\n",
      "Epoch 2951, Loss: 0.3168216198682785, Final Batch Loss: 0.16341324150562286\n",
      "Epoch 2952, Loss: 0.36463530361652374, Final Batch Loss: 0.20469817519187927\n",
      "Epoch 2953, Loss: 0.3798629194498062, Final Batch Loss: 0.18332704901695251\n",
      "Epoch 2954, Loss: 0.3999759256839752, Final Batch Loss: 0.216848224401474\n",
      "Epoch 2955, Loss: 0.39466919004917145, Final Batch Loss: 0.19832903146743774\n",
      "Epoch 2956, Loss: 0.3649025708436966, Final Batch Loss: 0.2130565494298935\n",
      "Epoch 2957, Loss: 0.44174377620220184, Final Batch Loss: 0.2084646224975586\n",
      "Epoch 2958, Loss: 0.3057233989238739, Final Batch Loss: 0.11773544549942017\n",
      "Epoch 2959, Loss: 0.3545345813035965, Final Batch Loss: 0.21088330447673798\n",
      "Epoch 2960, Loss: 0.3429040014743805, Final Batch Loss: 0.13269223272800446\n",
      "Epoch 2961, Loss: 0.37585918605327606, Final Batch Loss: 0.21086932718753815\n",
      "Epoch 2962, Loss: 0.3966953307390213, Final Batch Loss: 0.22693172097206116\n",
      "Epoch 2963, Loss: 0.3748999387025833, Final Batch Loss: 0.2207055538892746\n",
      "Epoch 2964, Loss: 0.33003726601600647, Final Batch Loss: 0.17749328911304474\n",
      "Epoch 2965, Loss: 0.344710573554039, Final Batch Loss: 0.15029767155647278\n",
      "Epoch 2966, Loss: 0.359387069940567, Final Batch Loss: 0.17636503279209137\n",
      "Epoch 2967, Loss: 0.32931916415691376, Final Batch Loss: 0.16644629836082458\n",
      "Epoch 2968, Loss: 0.3429633229970932, Final Batch Loss: 0.15695007145404816\n",
      "Epoch 2969, Loss: 0.38327960669994354, Final Batch Loss: 0.19446870684623718\n",
      "Epoch 2970, Loss: 0.3846849054098129, Final Batch Loss: 0.20384840667247772\n",
      "Epoch 2971, Loss: 0.40710535645484924, Final Batch Loss: 0.22726309299468994\n",
      "Epoch 2972, Loss: 0.3685337156057358, Final Batch Loss: 0.19499239325523376\n",
      "Epoch 2973, Loss: 0.42532189190387726, Final Batch Loss: 0.20500659942626953\n",
      "Epoch 2974, Loss: 0.3750757575035095, Final Batch Loss: 0.20294497907161713\n",
      "Epoch 2975, Loss: 0.357895165681839, Final Batch Loss: 0.14740192890167236\n",
      "Epoch 2976, Loss: 0.38052046298980713, Final Batch Loss: 0.1851935237646103\n",
      "Epoch 2977, Loss: 0.377314954996109, Final Batch Loss: 0.20874124765396118\n",
      "Epoch 2978, Loss: 0.3644346445798874, Final Batch Loss: 0.17763398587703705\n",
      "Epoch 2979, Loss: 0.33215947449207306, Final Batch Loss: 0.1725303828716278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2980, Loss: 0.342079758644104, Final Batch Loss: 0.16947337985038757\n",
      "Epoch 2981, Loss: 0.40617720782756805, Final Batch Loss: 0.21506652235984802\n",
      "Epoch 2982, Loss: 0.3818426877260208, Final Batch Loss: 0.19186469912528992\n",
      "Epoch 2983, Loss: 0.32651621103286743, Final Batch Loss: 0.18261560797691345\n",
      "Epoch 2984, Loss: 0.3103102594614029, Final Batch Loss: 0.12922824919223785\n",
      "Epoch 2985, Loss: 0.38587281107902527, Final Batch Loss: 0.19813857972621918\n",
      "Epoch 2986, Loss: 0.3815692514181137, Final Batch Loss: 0.21455861628055573\n",
      "Epoch 2987, Loss: 0.40832658112049103, Final Batch Loss: 0.21903014183044434\n",
      "Epoch 2988, Loss: 0.3056415840983391, Final Batch Loss: 0.1210162565112114\n",
      "Epoch 2989, Loss: 0.38242121040821075, Final Batch Loss: 0.19541727006435394\n",
      "Epoch 2990, Loss: 0.3846690207719803, Final Batch Loss: 0.18585152924060822\n",
      "Epoch 2991, Loss: 0.3877671808004379, Final Batch Loss: 0.17730176448822021\n",
      "Epoch 2992, Loss: 0.39864201843738556, Final Batch Loss: 0.19737233221530914\n",
      "Epoch 2993, Loss: 0.4067026972770691, Final Batch Loss: 0.20474174618721008\n",
      "Epoch 2994, Loss: 0.3427310138940811, Final Batch Loss: 0.1676775962114334\n",
      "Epoch 2995, Loss: 0.28284740447998047, Final Batch Loss: 0.139835387468338\n",
      "Epoch 2996, Loss: 0.33638718724250793, Final Batch Loss: 0.15065480768680573\n",
      "Epoch 2997, Loss: 0.31360405683517456, Final Batch Loss: 0.13095839321613312\n",
      "Epoch 2998, Loss: 0.3561101406812668, Final Batch Loss: 0.16360434889793396\n",
      "Epoch 2999, Loss: 0.37018100917339325, Final Batch Loss: 0.1777428388595581\n",
      "Epoch 3000, Loss: 0.32706423103809357, Final Batch Loss: 0.14570124447345734\n",
      "Epoch 3001, Loss: 0.35938532650470734, Final Batch Loss: 0.16473352909088135\n",
      "Epoch 3002, Loss: 0.4050423204898834, Final Batch Loss: 0.2382044494152069\n",
      "Epoch 3003, Loss: 0.4184940308332443, Final Batch Loss: 0.1971139907836914\n",
      "Epoch 3004, Loss: 0.37114304304122925, Final Batch Loss: 0.17566348612308502\n",
      "Epoch 3005, Loss: 0.4215128570795059, Final Batch Loss: 0.21044673025608063\n",
      "Epoch 3006, Loss: 0.3974200189113617, Final Batch Loss: 0.1964029222726822\n",
      "Epoch 3007, Loss: 0.4824868440628052, Final Batch Loss: 0.30331605672836304\n",
      "Epoch 3008, Loss: 0.33062201738357544, Final Batch Loss: 0.15182727575302124\n",
      "Epoch 3009, Loss: 0.33758874237537384, Final Batch Loss: 0.17667286098003387\n",
      "Epoch 3010, Loss: 0.3792016804218292, Final Batch Loss: 0.19035303592681885\n",
      "Epoch 3011, Loss: 0.3662675768136978, Final Batch Loss: 0.1663406789302826\n",
      "Epoch 3012, Loss: 0.3034840524196625, Final Batch Loss: 0.1371624618768692\n",
      "Epoch 3013, Loss: 0.35619744658470154, Final Batch Loss: 0.21007202565670013\n",
      "Epoch 3014, Loss: 0.3062959164381027, Final Batch Loss: 0.13261307775974274\n",
      "Epoch 3015, Loss: 0.36572401225566864, Final Batch Loss: 0.1904863864183426\n",
      "Epoch 3016, Loss: 0.35840825736522675, Final Batch Loss: 0.19329015910625458\n",
      "Epoch 3017, Loss: 0.32066184282302856, Final Batch Loss: 0.15523961186408997\n",
      "Epoch 3018, Loss: 0.35910098254680634, Final Batch Loss: 0.16373756527900696\n",
      "Epoch 3019, Loss: 0.37522199749946594, Final Batch Loss: 0.18376819789409637\n",
      "Epoch 3020, Loss: 0.3401811867952347, Final Batch Loss: 0.12777911126613617\n",
      "Epoch 3021, Loss: 0.39567704498767853, Final Batch Loss: 0.21348081529140472\n",
      "Epoch 3022, Loss: 0.4114152044057846, Final Batch Loss: 0.2032327651977539\n",
      "Epoch 3023, Loss: 0.4426807463169098, Final Batch Loss: 0.24575737118721008\n",
      "Epoch 3024, Loss: 0.34600479900836945, Final Batch Loss: 0.189568892121315\n",
      "Epoch 3025, Loss: 0.3512866497039795, Final Batch Loss: 0.1576114296913147\n",
      "Epoch 3026, Loss: 0.3791928291320801, Final Batch Loss: 0.20631185173988342\n",
      "Epoch 3027, Loss: 0.3680269867181778, Final Batch Loss: 0.2089761346578598\n",
      "Epoch 3028, Loss: 0.3958495408296585, Final Batch Loss: 0.1751653552055359\n",
      "Epoch 3029, Loss: 0.36506393551826477, Final Batch Loss: 0.18316242098808289\n",
      "Epoch 3030, Loss: 0.3809058964252472, Final Batch Loss: 0.2139376550912857\n",
      "Epoch 3031, Loss: 0.34519925713539124, Final Batch Loss: 0.1976420134305954\n",
      "Epoch 3032, Loss: 0.36466728150844574, Final Batch Loss: 0.16549232602119446\n",
      "Epoch 3033, Loss: 0.42773690819740295, Final Batch Loss: 0.17896585166454315\n",
      "Epoch 3034, Loss: 0.3879636526107788, Final Batch Loss: 0.2443220317363739\n",
      "Epoch 3035, Loss: 0.38197161257267, Final Batch Loss: 0.2189611792564392\n",
      "Epoch 3036, Loss: 0.35286611318588257, Final Batch Loss: 0.19026309251785278\n",
      "Epoch 3037, Loss: 0.356061726808548, Final Batch Loss: 0.19145721197128296\n",
      "Epoch 3038, Loss: 0.3324032872915268, Final Batch Loss: 0.12709127366542816\n",
      "Epoch 3039, Loss: 0.3346274793148041, Final Batch Loss: 0.13746283948421478\n",
      "Epoch 3040, Loss: 0.41995956003665924, Final Batch Loss: 0.21122758090496063\n",
      "Epoch 3041, Loss: 0.31966595351696014, Final Batch Loss: 0.15138840675354004\n",
      "Epoch 3042, Loss: 0.31570497155189514, Final Batch Loss: 0.19020836055278778\n",
      "Epoch 3043, Loss: 0.3428441733121872, Final Batch Loss: 0.1609964370727539\n",
      "Epoch 3044, Loss: 0.3523784875869751, Final Batch Loss: 0.1401815116405487\n",
      "Epoch 3045, Loss: 0.37623168528079987, Final Batch Loss: 0.2014547437429428\n",
      "Epoch 3046, Loss: 0.31305699050426483, Final Batch Loss: 0.15489688515663147\n",
      "Epoch 3047, Loss: 0.34369538724422455, Final Batch Loss: 0.20210008323192596\n",
      "Epoch 3048, Loss: 0.39627979695796967, Final Batch Loss: 0.21482893824577332\n",
      "Epoch 3049, Loss: 0.3818211853504181, Final Batch Loss: 0.22914870083332062\n",
      "Epoch 3050, Loss: 0.4482440799474716, Final Batch Loss: 0.2500375509262085\n",
      "Epoch 3051, Loss: 0.4000942260026932, Final Batch Loss: 0.19530713558197021\n",
      "Epoch 3052, Loss: 0.3765913397073746, Final Batch Loss: 0.16147874295711517\n",
      "Epoch 3053, Loss: 0.36440224945545197, Final Batch Loss: 0.1831045150756836\n",
      "Epoch 3054, Loss: 0.31092508137226105, Final Batch Loss: 0.1391684114933014\n",
      "Epoch 3055, Loss: 0.3435243219137192, Final Batch Loss: 0.18880951404571533\n",
      "Epoch 3056, Loss: 0.3981325030326843, Final Batch Loss: 0.19432251155376434\n",
      "Epoch 3057, Loss: 0.2984000891447067, Final Batch Loss: 0.1504821926355362\n",
      "Epoch 3058, Loss: 0.38630299270153046, Final Batch Loss: 0.2216196358203888\n",
      "Epoch 3059, Loss: 0.41835036873817444, Final Batch Loss: 0.2112734615802765\n",
      "Epoch 3060, Loss: 0.38657744228839874, Final Batch Loss: 0.14973387122154236\n",
      "Epoch 3061, Loss: 0.36872461438179016, Final Batch Loss: 0.13432778418064117\n",
      "Epoch 3062, Loss: 0.3587682396173477, Final Batch Loss: 0.17047961056232452\n",
      "Epoch 3063, Loss: 0.3394213616847992, Final Batch Loss: 0.1934124231338501\n",
      "Epoch 3064, Loss: 0.38429005444049835, Final Batch Loss: 0.2153748869895935\n",
      "Epoch 3065, Loss: 0.34727779030799866, Final Batch Loss: 0.20424678921699524\n",
      "Epoch 3066, Loss: 0.38726137578487396, Final Batch Loss: 0.1647227257490158\n",
      "Epoch 3067, Loss: 0.3364976644515991, Final Batch Loss: 0.16339077055454254\n",
      "Epoch 3068, Loss: 0.3942095786333084, Final Batch Loss: 0.1654224395751953\n",
      "Epoch 3069, Loss: 0.42521561682224274, Final Batch Loss: 0.25691333413124084\n",
      "Epoch 3070, Loss: 0.3978431075811386, Final Batch Loss: 0.2210177332162857\n",
      "Epoch 3071, Loss: 0.3784606456756592, Final Batch Loss: 0.16699114441871643\n",
      "Epoch 3072, Loss: 0.3568902462720871, Final Batch Loss: 0.12920841574668884\n",
      "Epoch 3073, Loss: 0.3652782142162323, Final Batch Loss: 0.20127001404762268\n",
      "Epoch 3074, Loss: 0.372358113527298, Final Batch Loss: 0.1393868774175644\n",
      "Epoch 3075, Loss: 0.41920559108257294, Final Batch Loss: 0.20687836408615112\n",
      "Epoch 3076, Loss: 0.364937886595726, Final Batch Loss: 0.20557470619678497\n",
      "Epoch 3077, Loss: 0.35899223387241364, Final Batch Loss: 0.18147478997707367\n",
      "Epoch 3078, Loss: 0.3410244733095169, Final Batch Loss: 0.14838182926177979\n",
      "Epoch 3079, Loss: 0.3026845008134842, Final Batch Loss: 0.14666417241096497\n",
      "Epoch 3080, Loss: 0.2895433232188225, Final Batch Loss: 0.09194397181272507\n",
      "Epoch 3081, Loss: 0.3181615173816681, Final Batch Loss: 0.14730030298233032\n",
      "Epoch 3082, Loss: 0.32723285257816315, Final Batch Loss: 0.151997372508049\n",
      "Epoch 3083, Loss: 0.3889520913362503, Final Batch Loss: 0.1738411784172058\n",
      "Epoch 3084, Loss: 0.4035198390483856, Final Batch Loss: 0.21589268743991852\n",
      "Epoch 3085, Loss: 0.3623708486557007, Final Batch Loss: 0.2170526683330536\n",
      "Epoch 3086, Loss: 0.32374075055122375, Final Batch Loss: 0.15862704813480377\n",
      "Epoch 3087, Loss: 0.4076553136110306, Final Batch Loss: 0.21979349851608276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3088, Loss: 0.4220515340566635, Final Batch Loss: 0.19096972048282623\n",
      "Epoch 3089, Loss: 0.37151049077510834, Final Batch Loss: 0.1588984876871109\n",
      "Epoch 3090, Loss: 0.3468841463327408, Final Batch Loss: 0.1497279703617096\n",
      "Epoch 3091, Loss: 0.3689580708742142, Final Batch Loss: 0.19499605894088745\n",
      "Epoch 3092, Loss: 0.42715977132320404, Final Batch Loss: 0.17483653128147125\n",
      "Epoch 3093, Loss: 0.3475668728351593, Final Batch Loss: 0.17920608818531036\n",
      "Epoch 3094, Loss: 0.3550696074962616, Final Batch Loss: 0.19992321729660034\n",
      "Epoch 3095, Loss: 0.37946708500385284, Final Batch Loss: 0.1910441368818283\n",
      "Epoch 3096, Loss: 0.3738597333431244, Final Batch Loss: 0.1826067566871643\n",
      "Epoch 3097, Loss: 0.3814311623573303, Final Batch Loss: 0.20770671963691711\n",
      "Epoch 3098, Loss: 0.3777753859758377, Final Batch Loss: 0.16739147901535034\n",
      "Epoch 3099, Loss: 0.4063786268234253, Final Batch Loss: 0.21514447033405304\n",
      "Epoch 3100, Loss: 0.3417397141456604, Final Batch Loss: 0.1719626933336258\n",
      "Epoch 3101, Loss: 0.322838693857193, Final Batch Loss: 0.1589333415031433\n",
      "Epoch 3102, Loss: 0.3273736536502838, Final Batch Loss: 0.15810808539390564\n",
      "Epoch 3103, Loss: 0.39050331711769104, Final Batch Loss: 0.21300989389419556\n",
      "Epoch 3104, Loss: 0.37177062034606934, Final Batch Loss: 0.20323197543621063\n",
      "Epoch 3105, Loss: 0.37084056437015533, Final Batch Loss: 0.15406794846057892\n",
      "Epoch 3106, Loss: 0.4073203057050705, Final Batch Loss: 0.23505975306034088\n",
      "Epoch 3107, Loss: 0.40242183208465576, Final Batch Loss: 0.19029629230499268\n",
      "Epoch 3108, Loss: 0.35696224868297577, Final Batch Loss: 0.18122994899749756\n",
      "Epoch 3109, Loss: 0.32322394847869873, Final Batch Loss: 0.16555918753147125\n",
      "Epoch 3110, Loss: 0.34240788221359253, Final Batch Loss: 0.23044368624687195\n",
      "Epoch 3111, Loss: 0.4469466656446457, Final Batch Loss: 0.20181483030319214\n",
      "Epoch 3112, Loss: 0.363613024353981, Final Batch Loss: 0.2079809457063675\n",
      "Epoch 3113, Loss: 0.3153066337108612, Final Batch Loss: 0.13676685094833374\n",
      "Epoch 3114, Loss: 0.39649635553359985, Final Batch Loss: 0.17342141270637512\n",
      "Epoch 3115, Loss: 0.3204411566257477, Final Batch Loss: 0.1508507877588272\n",
      "Epoch 3116, Loss: 0.3641122728586197, Final Batch Loss: 0.16553142666816711\n",
      "Epoch 3117, Loss: 0.39522382616996765, Final Batch Loss: 0.18345709145069122\n",
      "Epoch 3118, Loss: 0.4500803053379059, Final Batch Loss: 0.2136763334274292\n",
      "Epoch 3119, Loss: 0.3575838953256607, Final Batch Loss: 0.17015133798122406\n",
      "Epoch 3120, Loss: 0.39219434559345245, Final Batch Loss: 0.157377690076828\n",
      "Epoch 3121, Loss: 0.2790130451321602, Final Batch Loss: 0.11944863945245743\n",
      "Epoch 3122, Loss: 0.36478807032108307, Final Batch Loss: 0.14591984450817108\n",
      "Epoch 3123, Loss: 0.30139827728271484, Final Batch Loss: 0.12001784145832062\n",
      "Epoch 3124, Loss: 0.3678204417228699, Final Batch Loss: 0.20175762474536896\n",
      "Epoch 3125, Loss: 0.3419039398431778, Final Batch Loss: 0.17365863919258118\n",
      "Epoch 3126, Loss: 0.4064600169658661, Final Batch Loss: 0.24567940831184387\n",
      "Epoch 3127, Loss: 0.2877674847841263, Final Batch Loss: 0.1303226500749588\n",
      "Epoch 3128, Loss: 0.38803088665008545, Final Batch Loss: 0.1991061419248581\n",
      "Epoch 3129, Loss: 0.40647101402282715, Final Batch Loss: 0.19313447177410126\n",
      "Epoch 3130, Loss: 0.4089132994413376, Final Batch Loss: 0.22811415791511536\n",
      "Epoch 3131, Loss: 0.3999616950750351, Final Batch Loss: 0.24063953757286072\n",
      "Epoch 3132, Loss: 0.38381896913051605, Final Batch Loss: 0.19507382810115814\n",
      "Epoch 3133, Loss: 0.35735905170440674, Final Batch Loss: 0.19307351112365723\n",
      "Epoch 3134, Loss: 0.38447923958301544, Final Batch Loss: 0.19311915338039398\n",
      "Epoch 3135, Loss: 0.38296718895435333, Final Batch Loss: 0.19154857099056244\n",
      "Epoch 3136, Loss: 0.34694693982601166, Final Batch Loss: 0.17437893152236938\n",
      "Epoch 3137, Loss: 0.3932736963033676, Final Batch Loss: 0.1711861789226532\n",
      "Epoch 3138, Loss: 0.3829928934574127, Final Batch Loss: 0.2167184054851532\n",
      "Epoch 3139, Loss: 0.4031378924846649, Final Batch Loss: 0.24864065647125244\n",
      "Epoch 3140, Loss: 0.3210703134536743, Final Batch Loss: 0.1281518042087555\n",
      "Epoch 3141, Loss: 0.3053564131259918, Final Batch Loss: 0.14702816307544708\n",
      "Epoch 3142, Loss: 0.3096315711736679, Final Batch Loss: 0.16320723295211792\n",
      "Epoch 3143, Loss: 0.40155717730522156, Final Batch Loss: 0.19991691410541534\n",
      "Epoch 3144, Loss: 0.3777565509080887, Final Batch Loss: 0.1977066993713379\n",
      "Epoch 3145, Loss: 0.3466435819864273, Final Batch Loss: 0.19051377475261688\n",
      "Epoch 3146, Loss: 0.3628176748752594, Final Batch Loss: 0.2001105546951294\n",
      "Epoch 3147, Loss: 0.35352644324302673, Final Batch Loss: 0.1300356090068817\n",
      "Epoch 3148, Loss: 0.35068488121032715, Final Batch Loss: 0.1736827790737152\n",
      "Epoch 3149, Loss: 0.33488985896110535, Final Batch Loss: 0.15842421352863312\n",
      "Epoch 3150, Loss: 0.3741600215435028, Final Batch Loss: 0.19588424265384674\n",
      "Epoch 3151, Loss: 0.3687543123960495, Final Batch Loss: 0.17964714765548706\n",
      "Epoch 3152, Loss: 0.37089604139328003, Final Batch Loss: 0.19583703577518463\n",
      "Epoch 3153, Loss: 0.3585447072982788, Final Batch Loss: 0.2142917662858963\n",
      "Epoch 3154, Loss: 0.3944136053323746, Final Batch Loss: 0.22339938580989838\n",
      "Epoch 3155, Loss: 0.41668573021888733, Final Batch Loss: 0.22935618460178375\n",
      "Epoch 3156, Loss: 0.3886905312538147, Final Batch Loss: 0.20557637512683868\n",
      "Epoch 3157, Loss: 0.39403529465198517, Final Batch Loss: 0.19655562937259674\n",
      "Epoch 3158, Loss: 0.38896360993385315, Final Batch Loss: 0.20838066935539246\n",
      "Epoch 3159, Loss: 0.35673923790454865, Final Batch Loss: 0.14465245604515076\n",
      "Epoch 3160, Loss: 0.40898212790489197, Final Batch Loss: 0.21331816911697388\n",
      "Epoch 3161, Loss: 0.3426337093114853, Final Batch Loss: 0.14223088324069977\n",
      "Epoch 3162, Loss: 0.35435569286346436, Final Batch Loss: 0.15718258917331696\n",
      "Epoch 3163, Loss: 0.3671523779630661, Final Batch Loss: 0.19215169548988342\n",
      "Epoch 3164, Loss: 0.31598617136478424, Final Batch Loss: 0.17848235368728638\n",
      "Epoch 3165, Loss: 0.3150044232606888, Final Batch Loss: 0.1688128113746643\n",
      "Epoch 3166, Loss: 0.36289751529693604, Final Batch Loss: 0.18726742267608643\n",
      "Epoch 3167, Loss: 0.3633681386709213, Final Batch Loss: 0.21001361310482025\n",
      "Epoch 3168, Loss: 0.3332708030939102, Final Batch Loss: 0.17414315044879913\n",
      "Epoch 3169, Loss: 0.3422618806362152, Final Batch Loss: 0.15589790046215057\n",
      "Epoch 3170, Loss: 0.37633416056632996, Final Batch Loss: 0.14580725133419037\n",
      "Epoch 3171, Loss: 0.33510857820510864, Final Batch Loss: 0.17454026639461517\n",
      "Epoch 3172, Loss: 0.3535451591014862, Final Batch Loss: 0.16549108922481537\n",
      "Epoch 3173, Loss: 0.33307936787605286, Final Batch Loss: 0.19839917123317719\n",
      "Epoch 3174, Loss: 0.36238037049770355, Final Batch Loss: 0.18711629509925842\n",
      "Epoch 3175, Loss: 0.3749771863222122, Final Batch Loss: 0.20936061441898346\n",
      "Epoch 3176, Loss: 0.3825051039457321, Final Batch Loss: 0.21714679896831512\n",
      "Epoch 3177, Loss: 0.32201793789863586, Final Batch Loss: 0.13317163288593292\n",
      "Epoch 3178, Loss: 0.3690538555383682, Final Batch Loss: 0.14820754528045654\n",
      "Epoch 3179, Loss: 0.32234008610248566, Final Batch Loss: 0.1746324747800827\n",
      "Epoch 3180, Loss: 0.3748687207698822, Final Batch Loss: 0.18841736018657684\n",
      "Epoch 3181, Loss: 0.3351497948169708, Final Batch Loss: 0.17310650646686554\n",
      "Epoch 3182, Loss: 0.27888868749141693, Final Batch Loss: 0.16704687476158142\n",
      "Epoch 3183, Loss: 0.3720799684524536, Final Batch Loss: 0.17238189280033112\n",
      "Epoch 3184, Loss: 0.33534443378448486, Final Batch Loss: 0.1592698097229004\n",
      "Epoch 3185, Loss: 0.3325088173151016, Final Batch Loss: 0.1559274047613144\n",
      "Epoch 3186, Loss: 0.37901104986667633, Final Batch Loss: 0.20172375440597534\n",
      "Epoch 3187, Loss: 0.29106995463371277, Final Batch Loss: 0.14409902691841125\n",
      "Epoch 3188, Loss: 0.3156798630952835, Final Batch Loss: 0.15535378456115723\n",
      "Epoch 3189, Loss: 0.3788641542196274, Final Batch Loss: 0.15773820877075195\n",
      "Epoch 3190, Loss: 0.38443760573863983, Final Batch Loss: 0.19599881768226624\n",
      "Epoch 3191, Loss: 0.3265591859817505, Final Batch Loss: 0.16952279210090637\n",
      "Epoch 3192, Loss: 0.33211424946784973, Final Batch Loss: 0.19666746258735657\n",
      "Epoch 3193, Loss: 0.31393007934093475, Final Batch Loss: 0.14286908507347107\n",
      "Epoch 3194, Loss: 0.36528030037879944, Final Batch Loss: 0.2091795653104782\n",
      "Epoch 3195, Loss: 0.2928883731365204, Final Batch Loss: 0.15856823325157166\n",
      "Epoch 3196, Loss: 0.32072097063064575, Final Batch Loss: 0.1644624024629593\n",
      "Epoch 3197, Loss: 0.40297169983386993, Final Batch Loss: 0.22095364332199097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3198, Loss: 0.3880292475223541, Final Batch Loss: 0.20506572723388672\n",
      "Epoch 3199, Loss: 0.3211994469165802, Final Batch Loss: 0.17796626687049866\n",
      "Epoch 3200, Loss: 0.41898418962955475, Final Batch Loss: 0.24634318053722382\n",
      "Epoch 3201, Loss: 0.3500342518091202, Final Batch Loss: 0.18109585344791412\n",
      "Epoch 3202, Loss: 0.3398909270763397, Final Batch Loss: 0.1606331616640091\n",
      "Epoch 3203, Loss: 0.37623216211795807, Final Batch Loss: 0.15654537081718445\n",
      "Epoch 3204, Loss: 0.3167679160833359, Final Batch Loss: 0.15208037197589874\n",
      "Epoch 3205, Loss: 0.3018214553594589, Final Batch Loss: 0.17851321399211884\n",
      "Epoch 3206, Loss: 0.3500656187534332, Final Batch Loss: 0.1375807821750641\n",
      "Epoch 3207, Loss: 0.34865976870059967, Final Batch Loss: 0.1912136822938919\n",
      "Epoch 3208, Loss: 0.3278631493449211, Final Batch Loss: 0.12063372880220413\n",
      "Epoch 3209, Loss: 0.30652035772800446, Final Batch Loss: 0.13380230963230133\n",
      "Epoch 3210, Loss: 0.33487001061439514, Final Batch Loss: 0.15269380807876587\n",
      "Epoch 3211, Loss: 0.302972674369812, Final Batch Loss: 0.1157384067773819\n",
      "Epoch 3212, Loss: 0.3639018386602402, Final Batch Loss: 0.18986068665981293\n",
      "Epoch 3213, Loss: 0.37600672245025635, Final Batch Loss: 0.2190084457397461\n",
      "Epoch 3214, Loss: 0.34420663118362427, Final Batch Loss: 0.18403415381908417\n",
      "Epoch 3215, Loss: 0.31082021445035934, Final Batch Loss: 0.1896435022354126\n",
      "Epoch 3216, Loss: 0.36108408868312836, Final Batch Loss: 0.16198597848415375\n",
      "Epoch 3217, Loss: 0.3439033478498459, Final Batch Loss: 0.1789092868566513\n",
      "Epoch 3218, Loss: 0.36017823219299316, Final Batch Loss: 0.22912055253982544\n",
      "Epoch 3219, Loss: 0.3860466331243515, Final Batch Loss: 0.2504866123199463\n",
      "Epoch 3220, Loss: 0.31318652629852295, Final Batch Loss: 0.15169812738895416\n",
      "Epoch 3221, Loss: 0.3406773507595062, Final Batch Loss: 0.18766194581985474\n",
      "Epoch 3222, Loss: 0.4052273780107498, Final Batch Loss: 0.2059624046087265\n",
      "Epoch 3223, Loss: 0.35270872712135315, Final Batch Loss: 0.18016552925109863\n",
      "Epoch 3224, Loss: 0.3216961473226547, Final Batch Loss: 0.1535937786102295\n",
      "Epoch 3225, Loss: 0.3191075176000595, Final Batch Loss: 0.14320053160190582\n",
      "Epoch 3226, Loss: 0.4045799523591995, Final Batch Loss: 0.22525013983249664\n",
      "Epoch 3227, Loss: 0.33264415711164474, Final Batch Loss: 0.12027353793382645\n",
      "Epoch 3228, Loss: 0.32112714648246765, Final Batch Loss: 0.16156154870986938\n",
      "Epoch 3229, Loss: 0.2883281111717224, Final Batch Loss: 0.16308563947677612\n",
      "Epoch 3230, Loss: 0.28943145275115967, Final Batch Loss: 0.13516421616077423\n",
      "Epoch 3231, Loss: 0.3167172074317932, Final Batch Loss: 0.12239281833171844\n",
      "Epoch 3232, Loss: 0.44179578125476837, Final Batch Loss: 0.2154662311077118\n",
      "Epoch 3233, Loss: 0.3974599093198776, Final Batch Loss: 0.19047866761684418\n",
      "Epoch 3234, Loss: 0.3605107516050339, Final Batch Loss: 0.17672844231128693\n",
      "Epoch 3235, Loss: 0.4427664875984192, Final Batch Loss: 0.2152550369501114\n",
      "Epoch 3236, Loss: 0.34171320497989655, Final Batch Loss: 0.16413509845733643\n",
      "Epoch 3237, Loss: 0.34005293250083923, Final Batch Loss: 0.16031774878501892\n",
      "Epoch 3238, Loss: 0.3632795959711075, Final Batch Loss: 0.13516564667224884\n",
      "Epoch 3239, Loss: 0.31482715904712677, Final Batch Loss: 0.15327471494674683\n",
      "Epoch 3240, Loss: 0.33278684318065643, Final Batch Loss: 0.1716565489768982\n",
      "Epoch 3241, Loss: 0.37541593611240387, Final Batch Loss: 0.21305902302265167\n",
      "Epoch 3242, Loss: 0.33034904301166534, Final Batch Loss: 0.18553099036216736\n",
      "Epoch 3243, Loss: 0.41242392361164093, Final Batch Loss: 0.18682172894477844\n",
      "Epoch 3244, Loss: 0.32376645505428314, Final Batch Loss: 0.17892250418663025\n",
      "Epoch 3245, Loss: 0.45076633989810944, Final Batch Loss: 0.23662197589874268\n",
      "Epoch 3246, Loss: 0.39502182602882385, Final Batch Loss: 0.17378248274326324\n",
      "Epoch 3247, Loss: 0.38745032250881195, Final Batch Loss: 0.22042201459407806\n",
      "Epoch 3248, Loss: 0.31253471970558167, Final Batch Loss: 0.15089161694049835\n",
      "Epoch 3249, Loss: 0.3511384427547455, Final Batch Loss: 0.14459620416164398\n",
      "Epoch 3250, Loss: 0.30863451957702637, Final Batch Loss: 0.14781011641025543\n",
      "Epoch 3251, Loss: 0.35632462799549103, Final Batch Loss: 0.21564854681491852\n",
      "Epoch 3252, Loss: 0.37040503323078156, Final Batch Loss: 0.21602408587932587\n",
      "Epoch 3253, Loss: 0.41519686579704285, Final Batch Loss: 0.17978885769844055\n",
      "Epoch 3254, Loss: 0.3563620299100876, Final Batch Loss: 0.20155614614486694\n",
      "Epoch 3255, Loss: 0.3356820046901703, Final Batch Loss: 0.15967632830142975\n",
      "Epoch 3256, Loss: 0.3330509215593338, Final Batch Loss: 0.1769278645515442\n",
      "Epoch 3257, Loss: 0.3237404078245163, Final Batch Loss: 0.17211855947971344\n",
      "Epoch 3258, Loss: 0.3265012204647064, Final Batch Loss: 0.14521212875843048\n",
      "Epoch 3259, Loss: 0.36623698472976685, Final Batch Loss: 0.1930403709411621\n",
      "Epoch 3260, Loss: 0.3463313579559326, Final Batch Loss: 0.19180193543434143\n",
      "Epoch 3261, Loss: 0.3537495881319046, Final Batch Loss: 0.1271669566631317\n",
      "Epoch 3262, Loss: 0.35222169756889343, Final Batch Loss: 0.18675996363162994\n",
      "Epoch 3263, Loss: 0.3834194540977478, Final Batch Loss: 0.18552689254283905\n",
      "Epoch 3264, Loss: 0.42949235439300537, Final Batch Loss: 0.2213691622018814\n",
      "Epoch 3265, Loss: 0.27819913625717163, Final Batch Loss: 0.11429522931575775\n",
      "Epoch 3266, Loss: 0.3755713254213333, Final Batch Loss: 0.17211873829364777\n",
      "Epoch 3267, Loss: 0.3770950585603714, Final Batch Loss: 0.2516361474990845\n",
      "Epoch 3268, Loss: 0.30387720465660095, Final Batch Loss: 0.17254555225372314\n",
      "Epoch 3269, Loss: 0.33665814995765686, Final Batch Loss: 0.1530175358057022\n",
      "Epoch 3270, Loss: 0.3067637234926224, Final Batch Loss: 0.1368621289730072\n",
      "Epoch 3271, Loss: 0.3760940581560135, Final Batch Loss: 0.16611742973327637\n",
      "Epoch 3272, Loss: 0.3010222762823105, Final Batch Loss: 0.14689162373542786\n",
      "Epoch 3273, Loss: 0.3176548629999161, Final Batch Loss: 0.15522372722625732\n",
      "Epoch 3274, Loss: 0.3310212194919586, Final Batch Loss: 0.14109866321086884\n",
      "Epoch 3275, Loss: 0.40378881990909576, Final Batch Loss: 0.2033899873495102\n",
      "Epoch 3276, Loss: 0.4020102620124817, Final Batch Loss: 0.21017087996006012\n",
      "Epoch 3277, Loss: 0.3295307755470276, Final Batch Loss: 0.1254868507385254\n",
      "Epoch 3278, Loss: 0.33250510692596436, Final Batch Loss: 0.15642674267292023\n",
      "Epoch 3279, Loss: 0.37257125973701477, Final Batch Loss: 0.13472045958042145\n",
      "Epoch 3280, Loss: 0.3168320506811142, Final Batch Loss: 0.13494116067886353\n",
      "Epoch 3281, Loss: 0.3900328427553177, Final Batch Loss: 0.1788484901189804\n",
      "Epoch 3282, Loss: 0.3620358556509018, Final Batch Loss: 0.17488530278205872\n",
      "Epoch 3283, Loss: 0.3799387365579605, Final Batch Loss: 0.22233439981937408\n",
      "Epoch 3284, Loss: 0.3937346786260605, Final Batch Loss: 0.24233263731002808\n",
      "Epoch 3285, Loss: 0.3773600310087204, Final Batch Loss: 0.18297652900218964\n",
      "Epoch 3286, Loss: 0.34822867810726166, Final Batch Loss: 0.17285498976707458\n",
      "Epoch 3287, Loss: 0.30300532281398773, Final Batch Loss: 0.12352485954761505\n",
      "Epoch 3288, Loss: 0.45680075883865356, Final Batch Loss: 0.22422151267528534\n",
      "Epoch 3289, Loss: 0.3409532755613327, Final Batch Loss: 0.172530397772789\n",
      "Epoch 3290, Loss: 0.3202708214521408, Final Batch Loss: 0.1557643860578537\n",
      "Epoch 3291, Loss: 0.3854367285966873, Final Batch Loss: 0.23451021313667297\n",
      "Epoch 3292, Loss: 0.2977503389120102, Final Batch Loss: 0.14300675690174103\n",
      "Epoch 3293, Loss: 0.35491883754730225, Final Batch Loss: 0.17518866062164307\n",
      "Epoch 3294, Loss: 0.31498342752456665, Final Batch Loss: 0.16743911802768707\n",
      "Epoch 3295, Loss: 0.3488953560590744, Final Batch Loss: 0.1878511756658554\n",
      "Epoch 3296, Loss: 0.3804788887500763, Final Batch Loss: 0.1581404209136963\n",
      "Epoch 3297, Loss: 0.31648287177085876, Final Batch Loss: 0.13833463191986084\n",
      "Epoch 3298, Loss: 0.3180004954338074, Final Batch Loss: 0.13668940961360931\n",
      "Epoch 3299, Loss: 0.35437092185020447, Final Batch Loss: 0.19008418917655945\n",
      "Epoch 3300, Loss: 0.3191661685705185, Final Batch Loss: 0.18665844202041626\n",
      "Epoch 3301, Loss: 0.3273015022277832, Final Batch Loss: 0.14945422112941742\n",
      "Epoch 3302, Loss: 0.3871844559907913, Final Batch Loss: 0.18546655774116516\n",
      "Epoch 3303, Loss: 0.3475465178489685, Final Batch Loss: 0.1275557577610016\n",
      "Epoch 3304, Loss: 0.31453466415405273, Final Batch Loss: 0.14239658415317535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3305, Loss: 0.29430772364139557, Final Batch Loss: 0.15264669060707092\n",
      "Epoch 3306, Loss: 0.3496188372373581, Final Batch Loss: 0.17834483087062836\n",
      "Epoch 3307, Loss: 0.3517194986343384, Final Batch Loss: 0.19779400527477264\n",
      "Epoch 3308, Loss: 0.37556782364845276, Final Batch Loss: 0.22231356799602509\n",
      "Epoch 3309, Loss: 0.39094221591949463, Final Batch Loss: 0.21630661189556122\n",
      "Epoch 3310, Loss: 0.39572393894195557, Final Batch Loss: 0.24079261720180511\n",
      "Epoch 3311, Loss: 0.3327966630458832, Final Batch Loss: 0.14933648705482483\n",
      "Epoch 3312, Loss: 0.33435045182704926, Final Batch Loss: 0.1743633896112442\n",
      "Epoch 3313, Loss: 0.3352974206209183, Final Batch Loss: 0.1694129854440689\n",
      "Epoch 3314, Loss: 0.29868996888399124, Final Batch Loss: 0.1085786297917366\n",
      "Epoch 3315, Loss: 0.4183674603700638, Final Batch Loss: 0.1417710930109024\n",
      "Epoch 3316, Loss: 0.33679112792015076, Final Batch Loss: 0.16640214622020721\n",
      "Epoch 3317, Loss: 0.3631228506565094, Final Batch Loss: 0.15634740889072418\n",
      "Epoch 3318, Loss: 0.2890963703393936, Final Batch Loss: 0.14965282380580902\n",
      "Epoch 3319, Loss: 0.340155228972435, Final Batch Loss: 0.19814813137054443\n",
      "Epoch 3320, Loss: 0.3232998549938202, Final Batch Loss: 0.15785811841487885\n",
      "Epoch 3321, Loss: 0.33662550151348114, Final Batch Loss: 0.16826611757278442\n",
      "Epoch 3322, Loss: 0.31495844572782516, Final Batch Loss: 0.09340693801641464\n",
      "Epoch 3323, Loss: 0.33111572265625, Final Batch Loss: 0.1702762246131897\n",
      "Epoch 3324, Loss: 0.27857791632413864, Final Batch Loss: 0.16433149576187134\n",
      "Epoch 3325, Loss: 0.36286675930023193, Final Batch Loss: 0.2020403891801834\n",
      "Epoch 3326, Loss: 0.31299881637096405, Final Batch Loss: 0.15320034325122833\n",
      "Epoch 3327, Loss: 0.28180017322301865, Final Batch Loss: 0.17502634227275848\n",
      "Epoch 3328, Loss: 0.33500850200653076, Final Batch Loss: 0.16656161844730377\n",
      "Epoch 3329, Loss: 0.3242143839597702, Final Batch Loss: 0.18082769215106964\n",
      "Epoch 3330, Loss: 0.32958319783210754, Final Batch Loss: 0.20523303747177124\n",
      "Epoch 3331, Loss: 0.30577610433101654, Final Batch Loss: 0.14945082366466522\n",
      "Epoch 3332, Loss: 0.30530279874801636, Final Batch Loss: 0.1644192934036255\n",
      "Epoch 3333, Loss: 0.29475872218608856, Final Batch Loss: 0.1542944312095642\n",
      "Epoch 3334, Loss: 0.30681660771369934, Final Batch Loss: 0.16723571717739105\n",
      "Epoch 3335, Loss: 0.3210983872413635, Final Batch Loss: 0.17003051936626434\n",
      "Epoch 3336, Loss: 0.4173249304294586, Final Batch Loss: 0.22843648493289948\n",
      "Epoch 3337, Loss: 0.342039555311203, Final Batch Loss: 0.19278950989246368\n",
      "Epoch 3338, Loss: 0.3743516355752945, Final Batch Loss: 0.18322405219078064\n",
      "Epoch 3339, Loss: 0.3701988309621811, Final Batch Loss: 0.17661155760288239\n",
      "Epoch 3340, Loss: 0.31229883432388306, Final Batch Loss: 0.1491895169019699\n",
      "Epoch 3341, Loss: 0.2980080246925354, Final Batch Loss: 0.16783557832241058\n",
      "Epoch 3342, Loss: 0.30012278258800507, Final Batch Loss: 0.1286679357290268\n",
      "Epoch 3343, Loss: 0.31962864100933075, Final Batch Loss: 0.14492374658584595\n",
      "Epoch 3344, Loss: 0.33095139265060425, Final Batch Loss: 0.17637398838996887\n",
      "Epoch 3345, Loss: 0.3538595139980316, Final Batch Loss: 0.16084572672843933\n",
      "Epoch 3346, Loss: 0.2890545576810837, Final Batch Loss: 0.119826540350914\n",
      "Epoch 3347, Loss: 0.27215516567230225, Final Batch Loss: 0.12371814250946045\n",
      "Epoch 3348, Loss: 0.31216955184936523, Final Batch Loss: 0.16248291730880737\n",
      "Epoch 3349, Loss: 0.37791192531585693, Final Batch Loss: 0.20631100237369537\n",
      "Epoch 3350, Loss: 0.32280904054641724, Final Batch Loss: 0.14399933815002441\n",
      "Epoch 3351, Loss: 0.3572985678911209, Final Batch Loss: 0.1637483686208725\n",
      "Epoch 3352, Loss: 0.36005598306655884, Final Batch Loss: 0.145102396607399\n",
      "Epoch 3353, Loss: 0.25182177126407623, Final Batch Loss: 0.1177436113357544\n",
      "Epoch 3354, Loss: 0.34481386840343475, Final Batch Loss: 0.15827392041683197\n",
      "Epoch 3355, Loss: 0.27429476380348206, Final Batch Loss: 0.1358937919139862\n",
      "Epoch 3356, Loss: 0.3206302598118782, Final Batch Loss: 0.12443500012159348\n",
      "Epoch 3357, Loss: 0.3482331484556198, Final Batch Loss: 0.20808489620685577\n",
      "Epoch 3358, Loss: 0.37772858142852783, Final Batch Loss: 0.2247011363506317\n",
      "Epoch 3359, Loss: 0.36789846420288086, Final Batch Loss: 0.19792526960372925\n",
      "Epoch 3360, Loss: 0.35807305574417114, Final Batch Loss: 0.19505202770233154\n",
      "Epoch 3361, Loss: 0.3076782375574112, Final Batch Loss: 0.15356218814849854\n",
      "Epoch 3362, Loss: 0.28509948402643204, Final Batch Loss: 0.12238698452711105\n",
      "Epoch 3363, Loss: 0.3754666745662689, Final Batch Loss: 0.16290776431560516\n",
      "Epoch 3364, Loss: 0.311637744307518, Final Batch Loss: 0.13305558264255524\n",
      "Epoch 3365, Loss: 0.3167143613100052, Final Batch Loss: 0.15740977227687836\n",
      "Epoch 3366, Loss: 0.34940753877162933, Final Batch Loss: 0.15089790523052216\n",
      "Epoch 3367, Loss: 0.39391645789146423, Final Batch Loss: 0.19535668194293976\n",
      "Epoch 3368, Loss: 0.3246871680021286, Final Batch Loss: 0.19259655475616455\n",
      "Epoch 3369, Loss: 0.3189428150653839, Final Batch Loss: 0.14530524611473083\n",
      "Epoch 3370, Loss: 0.3840864598751068, Final Batch Loss: 0.19193314015865326\n",
      "Epoch 3371, Loss: 0.35219593346118927, Final Batch Loss: 0.18838055431842804\n",
      "Epoch 3372, Loss: 0.2818033844232559, Final Batch Loss: 0.1371636837720871\n",
      "Epoch 3373, Loss: 0.3080441728234291, Final Batch Loss: 0.10682997852563858\n",
      "Epoch 3374, Loss: 0.3339088261127472, Final Batch Loss: 0.178275927901268\n",
      "Epoch 3375, Loss: 0.2919391691684723, Final Batch Loss: 0.12747544050216675\n",
      "Epoch 3376, Loss: 0.3051155060529709, Final Batch Loss: 0.13704395294189453\n",
      "Epoch 3377, Loss: 0.28603656589984894, Final Batch Loss: 0.14735683798789978\n",
      "Epoch 3378, Loss: 0.31346648931503296, Final Batch Loss: 0.1802678406238556\n",
      "Epoch 3379, Loss: 0.3157379627227783, Final Batch Loss: 0.1570272147655487\n",
      "Epoch 3380, Loss: 0.3193947374820709, Final Batch Loss: 0.1538257598876953\n",
      "Epoch 3381, Loss: 0.3357735127210617, Final Batch Loss: 0.15393371880054474\n",
      "Epoch 3382, Loss: 0.315895676612854, Final Batch Loss: 0.1568891406059265\n",
      "Epoch 3383, Loss: 0.3605616092681885, Final Batch Loss: 0.19739048182964325\n",
      "Epoch 3384, Loss: 0.33274830877780914, Final Batch Loss: 0.18971888720989227\n",
      "Epoch 3385, Loss: 0.3853466063737869, Final Batch Loss: 0.22131673991680145\n",
      "Epoch 3386, Loss: 0.35448668897151947, Final Batch Loss: 0.15637366473674774\n",
      "Epoch 3387, Loss: 0.3511766642332077, Final Batch Loss: 0.13318763673305511\n",
      "Epoch 3388, Loss: 0.3243693858385086, Final Batch Loss: 0.12958738207817078\n",
      "Epoch 3389, Loss: 0.3131116181612015, Final Batch Loss: 0.1268087774515152\n",
      "Epoch 3390, Loss: 0.33999691158533096, Final Batch Loss: 0.22040745615959167\n",
      "Epoch 3391, Loss: 0.3814875930547714, Final Batch Loss: 0.19492273032665253\n",
      "Epoch 3392, Loss: 0.3592444360256195, Final Batch Loss: 0.13822241127490997\n",
      "Epoch 3393, Loss: 0.33004796504974365, Final Batch Loss: 0.15615490078926086\n",
      "Epoch 3394, Loss: 0.3507467359304428, Final Batch Loss: 0.17115890979766846\n",
      "Epoch 3395, Loss: 0.32855193316936493, Final Batch Loss: 0.19497932493686676\n",
      "Epoch 3396, Loss: 0.34476611018180847, Final Batch Loss: 0.1622370481491089\n",
      "Epoch 3397, Loss: 0.32664138078689575, Final Batch Loss: 0.14998790621757507\n",
      "Epoch 3398, Loss: 0.2919757217168808, Final Batch Loss: 0.15939927101135254\n",
      "Epoch 3399, Loss: 0.3670717775821686, Final Batch Loss: 0.2066144347190857\n",
      "Epoch 3400, Loss: 0.35076281428337097, Final Batch Loss: 0.18971878290176392\n",
      "Epoch 3401, Loss: 0.37711529433727264, Final Batch Loss: 0.21443164348602295\n",
      "Epoch 3402, Loss: 0.3135126084089279, Final Batch Loss: 0.15526723861694336\n",
      "Epoch 3403, Loss: 0.3041222542524338, Final Batch Loss: 0.1514497846364975\n",
      "Epoch 3404, Loss: 0.30021536350250244, Final Batch Loss: 0.140125572681427\n",
      "Epoch 3405, Loss: 0.3688046932220459, Final Batch Loss: 0.19563695788383484\n",
      "Epoch 3406, Loss: 0.32291558384895325, Final Batch Loss: 0.1548594981431961\n",
      "Epoch 3407, Loss: 0.366327241063118, Final Batch Loss: 0.22758013010025024\n",
      "Epoch 3408, Loss: 0.34341320395469666, Final Batch Loss: 0.17767125368118286\n",
      "Epoch 3409, Loss: 0.3499199151992798, Final Batch Loss: 0.15307492017745972\n",
      "Epoch 3410, Loss: 0.286147877573967, Final Batch Loss: 0.1334056705236435\n",
      "Epoch 3411, Loss: 0.31833668053150177, Final Batch Loss: 0.20288752019405365\n",
      "Epoch 3412, Loss: 0.27219779789447784, Final Batch Loss: 0.12188717722892761\n",
      "Epoch 3413, Loss: 0.3608725666999817, Final Batch Loss: 0.18345898389816284\n",
      "Epoch 3414, Loss: 0.35680297017097473, Final Batch Loss: 0.19410905241966248\n",
      "Epoch 3415, Loss: 0.3956049084663391, Final Batch Loss: 0.248264878988266\n",
      "Epoch 3416, Loss: 0.36728833615779877, Final Batch Loss: 0.18564197421073914\n",
      "Epoch 3417, Loss: 0.3814118653535843, Final Batch Loss: 0.20176579058170319\n",
      "Epoch 3418, Loss: 0.2901462912559509, Final Batch Loss: 0.15375865995883942\n",
      "Epoch 3419, Loss: 0.3723516911268234, Final Batch Loss: 0.1644367128610611\n",
      "Epoch 3420, Loss: 0.32639725506305695, Final Batch Loss: 0.18570750951766968\n",
      "Epoch 3421, Loss: 0.3315024822950363, Final Batch Loss: 0.2044041007757187\n",
      "Epoch 3422, Loss: 0.36251571774482727, Final Batch Loss: 0.21233543753623962\n",
      "Epoch 3423, Loss: 0.3264036327600479, Final Batch Loss: 0.16105230152606964\n",
      "Epoch 3424, Loss: 0.34567582607269287, Final Batch Loss: 0.16373619437217712\n",
      "Epoch 3425, Loss: 0.34299954771995544, Final Batch Loss: 0.20605988800525665\n",
      "Epoch 3426, Loss: 0.29396532475948334, Final Batch Loss: 0.1525048315525055\n",
      "Epoch 3427, Loss: 0.3354419469833374, Final Batch Loss: 0.14041092991828918\n",
      "Epoch 3428, Loss: 0.3114894926548004, Final Batch Loss: 0.16532401740550995\n",
      "Epoch 3429, Loss: 0.31218232214450836, Final Batch Loss: 0.16899056732654572\n",
      "Epoch 3430, Loss: 0.3503216430544853, Final Batch Loss: 0.2381860464811325\n",
      "Epoch 3431, Loss: 0.3591305911540985, Final Batch Loss: 0.2016339749097824\n",
      "Epoch 3432, Loss: 0.3642672747373581, Final Batch Loss: 0.17893992364406586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3433, Loss: 0.3201547712087631, Final Batch Loss: 0.1265667974948883\n",
      "Epoch 3434, Loss: 0.3526402860879898, Final Batch Loss: 0.17469367384910583\n",
      "Epoch 3435, Loss: 0.35643045604228973, Final Batch Loss: 0.165312260389328\n",
      "Epoch 3436, Loss: 0.3064040094614029, Final Batch Loss: 0.14439284801483154\n",
      "Epoch 3437, Loss: 0.4029373973608017, Final Batch Loss: 0.23125891387462616\n",
      "Epoch 3438, Loss: 0.2934548705816269, Final Batch Loss: 0.14731311798095703\n",
      "Epoch 3439, Loss: 0.3358560800552368, Final Batch Loss: 0.19338294863700867\n",
      "Epoch 3440, Loss: 0.3179769814014435, Final Batch Loss: 0.16365207731723785\n",
      "Epoch 3441, Loss: 0.32891225814819336, Final Batch Loss: 0.17110128700733185\n",
      "Epoch 3442, Loss: 0.2901060879230499, Final Batch Loss: 0.15359847247600555\n",
      "Epoch 3443, Loss: 0.38177672028541565, Final Batch Loss: 0.17964594066143036\n",
      "Epoch 3444, Loss: 0.3326404094696045, Final Batch Loss: 0.13507448136806488\n",
      "Epoch 3445, Loss: 0.3919513523578644, Final Batch Loss: 0.22576040029525757\n",
      "Epoch 3446, Loss: 0.32369497418403625, Final Batch Loss: 0.14991448819637299\n",
      "Epoch 3447, Loss: 0.38736817240715027, Final Batch Loss: 0.18235242366790771\n",
      "Epoch 3448, Loss: 0.35972271859645844, Final Batch Loss: 0.1587899625301361\n",
      "Epoch 3449, Loss: 0.32692818343639374, Final Batch Loss: 0.1759614646434784\n",
      "Epoch 3450, Loss: 0.33899566531181335, Final Batch Loss: 0.18486297130584717\n",
      "Epoch 3451, Loss: 0.36513644456863403, Final Batch Loss: 0.2315945029258728\n",
      "Epoch 3452, Loss: 0.2847105711698532, Final Batch Loss: 0.142933189868927\n",
      "Epoch 3453, Loss: 0.423784077167511, Final Batch Loss: 0.2199375033378601\n",
      "Epoch 3454, Loss: 0.38299307227134705, Final Batch Loss: 0.21134331822395325\n",
      "Epoch 3455, Loss: 0.34677980840206146, Final Batch Loss: 0.17108581960201263\n",
      "Epoch 3456, Loss: 0.31225569546222687, Final Batch Loss: 0.14510494470596313\n",
      "Epoch 3457, Loss: 0.28194093704223633, Final Batch Loss: 0.13829746842384338\n",
      "Epoch 3458, Loss: 0.28630654513835907, Final Batch Loss: 0.13642802834510803\n",
      "Epoch 3459, Loss: 0.3381691873073578, Final Batch Loss: 0.14917007088661194\n",
      "Epoch 3460, Loss: 0.3258843868970871, Final Batch Loss: 0.15679922699928284\n",
      "Epoch 3461, Loss: 0.3585473597049713, Final Batch Loss: 0.20011787116527557\n",
      "Epoch 3462, Loss: 0.37073616683483124, Final Batch Loss: 0.16806860268115997\n",
      "Epoch 3463, Loss: 0.29190750420093536, Final Batch Loss: 0.1367909461259842\n",
      "Epoch 3464, Loss: 0.38105421513319016, Final Batch Loss: 0.11967232078313828\n",
      "Epoch 3465, Loss: 0.30437564849853516, Final Batch Loss: 0.15137894451618195\n",
      "Epoch 3466, Loss: 0.3558521121740341, Final Batch Loss: 0.16594679653644562\n",
      "Epoch 3467, Loss: 0.28924980759620667, Final Batch Loss: 0.13575299084186554\n",
      "Epoch 3468, Loss: 0.3267330378293991, Final Batch Loss: 0.18360908329486847\n",
      "Epoch 3469, Loss: 0.38008633255958557, Final Batch Loss: 0.18894556164741516\n",
      "Epoch 3470, Loss: 0.27116386592388153, Final Batch Loss: 0.13234977424144745\n",
      "Epoch 3471, Loss: 0.36249250173568726, Final Batch Loss: 0.1749839037656784\n",
      "Epoch 3472, Loss: 0.37403804063796997, Final Batch Loss: 0.18131712079048157\n",
      "Epoch 3473, Loss: 0.3554523289203644, Final Batch Loss: 0.12561361491680145\n",
      "Epoch 3474, Loss: 0.3087685853242874, Final Batch Loss: 0.14295628666877747\n",
      "Epoch 3475, Loss: 0.32610298693180084, Final Batch Loss: 0.1596744954586029\n",
      "Epoch 3476, Loss: 0.3227880150079727, Final Batch Loss: 0.1379733681678772\n",
      "Epoch 3477, Loss: 0.3000536262989044, Final Batch Loss: 0.13728433847427368\n",
      "Epoch 3478, Loss: 0.35036784410476685, Final Batch Loss: 0.19911132752895355\n",
      "Epoch 3479, Loss: 0.32741953432559967, Final Batch Loss: 0.1608303040266037\n",
      "Epoch 3480, Loss: 0.2838292717933655, Final Batch Loss: 0.15899665653705597\n",
      "Epoch 3481, Loss: 0.3528461456298828, Final Batch Loss: 0.14804698526859283\n",
      "Epoch 3482, Loss: 0.3617469072341919, Final Batch Loss: 0.17730000615119934\n",
      "Epoch 3483, Loss: 0.34845636785030365, Final Batch Loss: 0.1621510237455368\n",
      "Epoch 3484, Loss: 0.33744971454143524, Final Batch Loss: 0.1422785073518753\n",
      "Epoch 3485, Loss: 0.31908200681209564, Final Batch Loss: 0.12275101244449615\n",
      "Epoch 3486, Loss: 0.36028487980365753, Final Batch Loss: 0.14917686581611633\n",
      "Epoch 3487, Loss: 0.40844544768333435, Final Batch Loss: 0.25749316811561584\n",
      "Epoch 3488, Loss: 0.3474578261375427, Final Batch Loss: 0.18675734102725983\n",
      "Epoch 3489, Loss: 0.4038090705871582, Final Batch Loss: 0.192805677652359\n",
      "Epoch 3490, Loss: 0.3132821023464203, Final Batch Loss: 0.17171673476696014\n",
      "Epoch 3491, Loss: 0.3957880139350891, Final Batch Loss: 0.1997661143541336\n",
      "Epoch 3492, Loss: 0.3470069617033005, Final Batch Loss: 0.18075327575206757\n",
      "Epoch 3493, Loss: 0.33003444969654083, Final Batch Loss: 0.17411546409130096\n",
      "Epoch 3494, Loss: 0.2940724194049835, Final Batch Loss: 0.15880821645259857\n",
      "Epoch 3495, Loss: 0.3129071742296219, Final Batch Loss: 0.16803893446922302\n",
      "Epoch 3496, Loss: 0.32062235474586487, Final Batch Loss: 0.17117851972579956\n",
      "Epoch 3497, Loss: 0.3495814651250839, Final Batch Loss: 0.2016679048538208\n",
      "Epoch 3498, Loss: 0.315484881401062, Final Batch Loss: 0.12005339562892914\n",
      "Epoch 3499, Loss: 0.38388437032699585, Final Batch Loss: 0.17634817957878113\n",
      "Epoch 3500, Loss: 0.34231652319431305, Final Batch Loss: 0.1633746176958084\n",
      "Epoch 3501, Loss: 0.314493864774704, Final Batch Loss: 0.13118976354599\n",
      "Epoch 3502, Loss: 0.29926542937755585, Final Batch Loss: 0.16082604229450226\n",
      "Epoch 3503, Loss: 0.30101683735847473, Final Batch Loss: 0.1616583913564682\n",
      "Epoch 3504, Loss: 0.2836838737130165, Final Batch Loss: 0.12483444064855576\n",
      "Epoch 3505, Loss: 0.2952826917171478, Final Batch Loss: 0.16492122411727905\n",
      "Epoch 3506, Loss: 0.277418777346611, Final Batch Loss: 0.12653878331184387\n",
      "Epoch 3507, Loss: 0.33411705493927, Final Batch Loss: 0.15621209144592285\n",
      "Epoch 3508, Loss: 0.29878874123096466, Final Batch Loss: 0.12335190176963806\n",
      "Epoch 3509, Loss: 0.25565580278635025, Final Batch Loss: 0.13675834238529205\n",
      "Epoch 3510, Loss: 0.3179114758968353, Final Batch Loss: 0.16508981585502625\n",
      "Epoch 3511, Loss: 0.2987980544567108, Final Batch Loss: 0.13722939789295197\n",
      "Epoch 3512, Loss: 0.29121870547533035, Final Batch Loss: 0.12020493298768997\n",
      "Epoch 3513, Loss: 0.36003781855106354, Final Batch Loss: 0.19723546504974365\n",
      "Epoch 3514, Loss: 0.3030707836151123, Final Batch Loss: 0.1651340126991272\n",
      "Epoch 3515, Loss: 0.34938983619213104, Final Batch Loss: 0.16667714715003967\n",
      "Epoch 3516, Loss: 0.3614555150270462, Final Batch Loss: 0.14709551632404327\n",
      "Epoch 3517, Loss: 0.3638559430837631, Final Batch Loss: 0.16693159937858582\n",
      "Epoch 3518, Loss: 0.30258816480636597, Final Batch Loss: 0.13600046932697296\n",
      "Epoch 3519, Loss: 0.36212296783924103, Final Batch Loss: 0.19376391172409058\n",
      "Epoch 3520, Loss: 0.3289044350385666, Final Batch Loss: 0.17734134197235107\n",
      "Epoch 3521, Loss: 0.32719556987285614, Final Batch Loss: 0.14856268465518951\n",
      "Epoch 3522, Loss: 0.40039893984794617, Final Batch Loss: 0.20456673204898834\n",
      "Epoch 3523, Loss: 0.3130970150232315, Final Batch Loss: 0.1807156503200531\n",
      "Epoch 3524, Loss: 0.304227277636528, Final Batch Loss: 0.16664759814739227\n",
      "Epoch 3525, Loss: 0.3398537188768387, Final Batch Loss: 0.13294684886932373\n",
      "Epoch 3526, Loss: 0.3608103394508362, Final Batch Loss: 0.1828329712152481\n",
      "Epoch 3527, Loss: 0.33258524537086487, Final Batch Loss: 0.14031155407428741\n",
      "Epoch 3528, Loss: 0.32973164319992065, Final Batch Loss: 0.13397495448589325\n",
      "Epoch 3529, Loss: 0.2759843170642853, Final Batch Loss: 0.13823102414608002\n",
      "Epoch 3530, Loss: 0.28601132333278656, Final Batch Loss: 0.09924560785293579\n",
      "Epoch 3531, Loss: 0.34908680617809296, Final Batch Loss: 0.1548534631729126\n",
      "Epoch 3532, Loss: 0.2779546231031418, Final Batch Loss: 0.14225143194198608\n",
      "Epoch 3533, Loss: 0.31330282986164093, Final Batch Loss: 0.1370462328195572\n",
      "Epoch 3534, Loss: 0.33952298760414124, Final Batch Loss: 0.16747038066387177\n",
      "Epoch 3535, Loss: 0.34504155814647675, Final Batch Loss: 0.22686494886875153\n",
      "Epoch 3536, Loss: 0.3806426078081131, Final Batch Loss: 0.2045910507440567\n",
      "Epoch 3537, Loss: 0.2767032980918884, Final Batch Loss: 0.14937236905097961\n",
      "Epoch 3538, Loss: 0.3265058845281601, Final Batch Loss: 0.157377228140831\n",
      "Epoch 3539, Loss: 0.3232489824295044, Final Batch Loss: 0.15935565531253815\n",
      "Epoch 3540, Loss: 0.3931680768728256, Final Batch Loss: 0.20281697809696198\n",
      "Epoch 3541, Loss: 0.34544724225997925, Final Batch Loss: 0.16033992171287537\n",
      "Epoch 3542, Loss: 0.33844687044620514, Final Batch Loss: 0.1977228820323944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3543, Loss: 0.30995118618011475, Final Batch Loss: 0.17164111137390137\n",
      "Epoch 3544, Loss: 0.30319055914878845, Final Batch Loss: 0.1567772924900055\n",
      "Epoch 3545, Loss: 0.3537270128726959, Final Batch Loss: 0.19380679726600647\n",
      "Epoch 3546, Loss: 0.30533871054649353, Final Batch Loss: 0.15089178085327148\n",
      "Epoch 3547, Loss: 0.31916068494319916, Final Batch Loss: 0.13555602729320526\n",
      "Epoch 3548, Loss: 0.3010792136192322, Final Batch Loss: 0.1701352596282959\n",
      "Epoch 3549, Loss: 0.34005266427993774, Final Batch Loss: 0.2066781371831894\n",
      "Epoch 3550, Loss: 0.2869138941168785, Final Batch Loss: 0.16497543454170227\n",
      "Epoch 3551, Loss: 0.24901223927736282, Final Batch Loss: 0.1053270623087883\n",
      "Epoch 3552, Loss: 0.35258930921554565, Final Batch Loss: 0.23392091691493988\n",
      "Epoch 3553, Loss: 0.26362593472003937, Final Batch Loss: 0.1135173887014389\n",
      "Epoch 3554, Loss: 0.36165234446525574, Final Batch Loss: 0.22649134695529938\n",
      "Epoch 3555, Loss: 0.3478178083896637, Final Batch Loss: 0.1641106754541397\n",
      "Epoch 3556, Loss: 0.31205570697784424, Final Batch Loss: 0.129322350025177\n",
      "Epoch 3557, Loss: 0.342265784740448, Final Batch Loss: 0.14215651154518127\n",
      "Epoch 3558, Loss: 0.3443531394004822, Final Batch Loss: 0.19622382521629333\n",
      "Epoch 3559, Loss: 0.3018605709075928, Final Batch Loss: 0.1501903384923935\n",
      "Epoch 3560, Loss: 0.323808953166008, Final Batch Loss: 0.11035566031932831\n",
      "Epoch 3561, Loss: 0.3246302008628845, Final Batch Loss: 0.1812182366847992\n",
      "Epoch 3562, Loss: 0.2876402884721756, Final Batch Loss: 0.14961843192577362\n",
      "Epoch 3563, Loss: 0.3542926013469696, Final Batch Loss: 0.1759319007396698\n",
      "Epoch 3564, Loss: 0.3401138037443161, Final Batch Loss: 0.19578523933887482\n",
      "Epoch 3565, Loss: 0.31743646413087845, Final Batch Loss: 0.1947438269853592\n",
      "Epoch 3566, Loss: 0.33919501304626465, Final Batch Loss: 0.1410464197397232\n",
      "Epoch 3567, Loss: 0.3192180097103119, Final Batch Loss: 0.15740686655044556\n",
      "Epoch 3568, Loss: 0.3126452714204788, Final Batch Loss: 0.13843563199043274\n",
      "Epoch 3569, Loss: 0.362231582403183, Final Batch Loss: 0.19854940474033356\n",
      "Epoch 3570, Loss: 0.3625716120004654, Final Batch Loss: 0.17049327492713928\n",
      "Epoch 3571, Loss: 0.3493122458457947, Final Batch Loss: 0.1697332262992859\n",
      "Epoch 3572, Loss: 0.3707931786775589, Final Batch Loss: 0.19173653423786163\n",
      "Epoch 3573, Loss: 0.30903691053390503, Final Batch Loss: 0.14460062980651855\n",
      "Epoch 3574, Loss: 0.3714846819639206, Final Batch Loss: 0.17958161234855652\n",
      "Epoch 3575, Loss: 0.38745978474617004, Final Batch Loss: 0.2360144704580307\n",
      "Epoch 3576, Loss: 0.31991641223430634, Final Batch Loss: 0.1513199359178543\n",
      "Epoch 3577, Loss: 0.3744802922010422, Final Batch Loss: 0.20583325624465942\n",
      "Epoch 3578, Loss: 0.3505308926105499, Final Batch Loss: 0.18832829594612122\n",
      "Epoch 3579, Loss: 0.3609785735607147, Final Batch Loss: 0.17991426587104797\n",
      "Epoch 3580, Loss: 0.37319688498973846, Final Batch Loss: 0.18726132810115814\n",
      "Epoch 3581, Loss: 0.33434389531612396, Final Batch Loss: 0.14757175743579865\n",
      "Epoch 3582, Loss: 0.35184597969055176, Final Batch Loss: 0.14493867754936218\n",
      "Epoch 3583, Loss: 0.3511221557855606, Final Batch Loss: 0.16289952397346497\n",
      "Epoch 3584, Loss: 0.32721081376075745, Final Batch Loss: 0.16862227022647858\n",
      "Epoch 3585, Loss: 0.2900325059890747, Final Batch Loss: 0.13503418862819672\n",
      "Epoch 3586, Loss: 0.3290400505065918, Final Batch Loss: 0.15431731939315796\n",
      "Epoch 3587, Loss: 0.3090456873178482, Final Batch Loss: 0.15765859186649323\n",
      "Epoch 3588, Loss: 0.3170526623725891, Final Batch Loss: 0.1811559945344925\n",
      "Epoch 3589, Loss: 0.30008257925510406, Final Batch Loss: 0.15188471972942352\n",
      "Epoch 3590, Loss: 0.27975060790777206, Final Batch Loss: 0.15477900207042694\n",
      "Epoch 3591, Loss: 0.3774554878473282, Final Batch Loss: 0.21172353625297546\n",
      "Epoch 3592, Loss: 0.34833918511867523, Final Batch Loss: 0.18495680391788483\n",
      "Epoch 3593, Loss: 0.34533172845840454, Final Batch Loss: 0.19567108154296875\n",
      "Epoch 3594, Loss: 0.2968598008155823, Final Batch Loss: 0.16535253822803497\n",
      "Epoch 3595, Loss: 0.26622723042964935, Final Batch Loss: 0.12513698637485504\n",
      "Epoch 3596, Loss: 0.3882185220718384, Final Batch Loss: 0.1397145390510559\n",
      "Epoch 3597, Loss: 0.3561336100101471, Final Batch Loss: 0.17513468861579895\n",
      "Epoch 3598, Loss: 0.2890401780605316, Final Batch Loss: 0.1538468450307846\n",
      "Epoch 3599, Loss: 0.32556480169296265, Final Batch Loss: 0.1849755346775055\n",
      "Epoch 3600, Loss: 0.31808599829673767, Final Batch Loss: 0.16213130950927734\n",
      "Epoch 3601, Loss: 0.31310081481933594, Final Batch Loss: 0.18416601419448853\n",
      "Epoch 3602, Loss: 0.3405483365058899, Final Batch Loss: 0.1685289591550827\n",
      "Epoch 3603, Loss: 0.33058302104473114, Final Batch Loss: 0.19730030000209808\n",
      "Epoch 3604, Loss: 0.3319584131240845, Final Batch Loss: 0.16788940131664276\n",
      "Epoch 3605, Loss: 0.32548047602176666, Final Batch Loss: 0.16441693902015686\n",
      "Epoch 3606, Loss: 0.3400771915912628, Final Batch Loss: 0.16173291206359863\n",
      "Epoch 3607, Loss: 0.33253610134124756, Final Batch Loss: 0.19587849080562592\n",
      "Epoch 3608, Loss: 0.3179764300584793, Final Batch Loss: 0.18897372484207153\n",
      "Epoch 3609, Loss: 0.3007354438304901, Final Batch Loss: 0.11286373436450958\n",
      "Epoch 3610, Loss: 0.34236791729927063, Final Batch Loss: 0.1576075404882431\n",
      "Epoch 3611, Loss: 0.2992098182439804, Final Batch Loss: 0.1424022614955902\n",
      "Epoch 3612, Loss: 0.3258948475122452, Final Batch Loss: 0.1468101292848587\n",
      "Epoch 3613, Loss: 0.3564741760492325, Final Batch Loss: 0.16509273648262024\n",
      "Epoch 3614, Loss: 0.34034232795238495, Final Batch Loss: 0.14678126573562622\n",
      "Epoch 3615, Loss: 0.26949965953826904, Final Batch Loss: 0.15620329976081848\n",
      "Epoch 3616, Loss: 0.24209903180599213, Final Batch Loss: 0.1176425889134407\n",
      "Epoch 3617, Loss: 0.27276791632175446, Final Batch Loss: 0.09830468893051147\n",
      "Epoch 3618, Loss: 0.3516618609428406, Final Batch Loss: 0.18151262402534485\n",
      "Epoch 3619, Loss: 0.2610095292329788, Final Batch Loss: 0.13361184298992157\n",
      "Epoch 3620, Loss: 0.3602731376886368, Final Batch Loss: 0.20919497311115265\n",
      "Epoch 3621, Loss: 0.31994929909706116, Final Batch Loss: 0.15377214550971985\n",
      "Epoch 3622, Loss: 0.3355350196361542, Final Batch Loss: 0.1840417981147766\n",
      "Epoch 3623, Loss: 0.34288913011550903, Final Batch Loss: 0.192803755402565\n",
      "Epoch 3624, Loss: 0.23690854012966156, Final Batch Loss: 0.11145952343940735\n",
      "Epoch 3625, Loss: 0.3227747231721878, Final Batch Loss: 0.1667865663766861\n",
      "Epoch 3626, Loss: 0.3310360014438629, Final Batch Loss: 0.19894586503505707\n",
      "Epoch 3627, Loss: 0.3279563784599304, Final Batch Loss: 0.15385237336158752\n",
      "Epoch 3628, Loss: 0.3269484341144562, Final Batch Loss: 0.1594722419977188\n",
      "Epoch 3629, Loss: 0.34155572950839996, Final Batch Loss: 0.1611732542514801\n",
      "Epoch 3630, Loss: 0.28181132674217224, Final Batch Loss: 0.15185335278511047\n",
      "Epoch 3631, Loss: 0.3200829029083252, Final Batch Loss: 0.17618152499198914\n",
      "Epoch 3632, Loss: 0.3044794574379921, Final Batch Loss: 0.11979638785123825\n",
      "Epoch 3633, Loss: 0.3281954973936081, Final Batch Loss: 0.1448984146118164\n",
      "Epoch 3634, Loss: 0.2720434218645096, Final Batch Loss: 0.11590276658535004\n",
      "Epoch 3635, Loss: 0.31428416073322296, Final Batch Loss: 0.13999627530574799\n",
      "Epoch 3636, Loss: 0.33646678924560547, Final Batch Loss: 0.18985265493392944\n",
      "Epoch 3637, Loss: 0.31952841579914093, Final Batch Loss: 0.15155869722366333\n",
      "Epoch 3638, Loss: 0.33650903403759, Final Batch Loss: 0.17653752863407135\n",
      "Epoch 3639, Loss: 0.27385207265615463, Final Batch Loss: 0.15669609606266022\n",
      "Epoch 3640, Loss: 0.3622698038816452, Final Batch Loss: 0.19948458671569824\n",
      "Epoch 3641, Loss: 0.30397121608257294, Final Batch Loss: 0.15734724700450897\n",
      "Epoch 3642, Loss: 0.27661946415901184, Final Batch Loss: 0.1344681680202484\n",
      "Epoch 3643, Loss: 0.28678148239851, Final Batch Loss: 0.11373970657587051\n",
      "Epoch 3644, Loss: 0.33173535764217377, Final Batch Loss: 0.16167967021465302\n",
      "Epoch 3645, Loss: 0.37440265715122223, Final Batch Loss: 0.21132299304008484\n",
      "Epoch 3646, Loss: 0.29198989272117615, Final Batch Loss: 0.13722968101501465\n",
      "Epoch 3647, Loss: 0.27821920812129974, Final Batch Loss: 0.12993909418582916\n",
      "Epoch 3648, Loss: 0.3586866855621338, Final Batch Loss: 0.19641809165477753\n",
      "Epoch 3649, Loss: 0.307227298617363, Final Batch Loss: 0.13617011904716492\n",
      "Epoch 3650, Loss: 0.3394467979669571, Final Batch Loss: 0.17740735411643982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3651, Loss: 0.3449907451868057, Final Batch Loss: 0.16137176752090454\n",
      "Epoch 3652, Loss: 0.31215720623731613, Final Batch Loss: 0.1906139999628067\n",
      "Epoch 3653, Loss: 0.29454492032527924, Final Batch Loss: 0.15521705150604248\n",
      "Epoch 3654, Loss: 0.3355490267276764, Final Batch Loss: 0.16604924201965332\n",
      "Epoch 3655, Loss: 0.3156236708164215, Final Batch Loss: 0.17005835473537445\n",
      "Epoch 3656, Loss: 0.28640158474445343, Final Batch Loss: 0.1252765655517578\n",
      "Epoch 3657, Loss: 0.29266172647476196, Final Batch Loss: 0.16271530091762543\n",
      "Epoch 3658, Loss: 0.261931337416172, Final Batch Loss: 0.11431895941495895\n",
      "Epoch 3659, Loss: 0.29243896901607513, Final Batch Loss: 0.14556358754634857\n",
      "Epoch 3660, Loss: 0.31824490427970886, Final Batch Loss: 0.17386159300804138\n",
      "Epoch 3661, Loss: 0.30908316373825073, Final Batch Loss: 0.16189385950565338\n",
      "Epoch 3662, Loss: 0.3768727034330368, Final Batch Loss: 0.19904759526252747\n",
      "Epoch 3663, Loss: 0.30446256697177887, Final Batch Loss: 0.15853309631347656\n",
      "Epoch 3664, Loss: 0.29052336513996124, Final Batch Loss: 0.14459317922592163\n",
      "Epoch 3665, Loss: 0.3031258285045624, Final Batch Loss: 0.13792885839939117\n",
      "Epoch 3666, Loss: 0.32334721088409424, Final Batch Loss: 0.19541195034980774\n",
      "Epoch 3667, Loss: 0.3244350552558899, Final Batch Loss: 0.14085212349891663\n",
      "Epoch 3668, Loss: 0.33740633726119995, Final Batch Loss: 0.21775799989700317\n",
      "Epoch 3669, Loss: 0.280329667031765, Final Batch Loss: 0.12136545032262802\n",
      "Epoch 3670, Loss: 0.30555248260498047, Final Batch Loss: 0.16366618871688843\n",
      "Epoch 3671, Loss: 0.3236713111400604, Final Batch Loss: 0.1623758226633072\n",
      "Epoch 3672, Loss: 0.3430839627981186, Final Batch Loss: 0.1893717646598816\n",
      "Epoch 3673, Loss: 0.36109577119350433, Final Batch Loss: 0.1936870515346527\n",
      "Epoch 3674, Loss: 0.3122367411851883, Final Batch Loss: 0.17826072871685028\n",
      "Epoch 3675, Loss: 0.3103755861520767, Final Batch Loss: 0.15097224712371826\n",
      "Epoch 3676, Loss: 0.3060939162969589, Final Batch Loss: 0.1580830216407776\n",
      "Epoch 3677, Loss: 0.31472383439540863, Final Batch Loss: 0.13478627800941467\n",
      "Epoch 3678, Loss: 0.32522110641002655, Final Batch Loss: 0.18141476809978485\n",
      "Epoch 3679, Loss: 0.34233471751213074, Final Batch Loss: 0.16122430562973022\n",
      "Epoch 3680, Loss: 0.24906786531209946, Final Batch Loss: 0.09858416765928268\n",
      "Epoch 3681, Loss: 0.2960645407438278, Final Batch Loss: 0.1370168775320053\n",
      "Epoch 3682, Loss: 0.30114972591400146, Final Batch Loss: 0.1546480506658554\n",
      "Epoch 3683, Loss: 0.32007724046707153, Final Batch Loss: 0.16899684071540833\n",
      "Epoch 3684, Loss: 0.281054750084877, Final Batch Loss: 0.15407203137874603\n",
      "Epoch 3685, Loss: 0.31458038091659546, Final Batch Loss: 0.17708911001682281\n",
      "Epoch 3686, Loss: 0.2889970690011978, Final Batch Loss: 0.1638423502445221\n",
      "Epoch 3687, Loss: 0.3079800307750702, Final Batch Loss: 0.14365684986114502\n",
      "Epoch 3688, Loss: 0.3086988478899002, Final Batch Loss: 0.16307519376277924\n",
      "Epoch 3689, Loss: 0.3560599386692047, Final Batch Loss: 0.20094354450702667\n",
      "Epoch 3690, Loss: 0.3144836723804474, Final Batch Loss: 0.14070142805576324\n",
      "Epoch 3691, Loss: 0.3430538475513458, Final Batch Loss: 0.1308494359254837\n",
      "Epoch 3692, Loss: 0.31298837065696716, Final Batch Loss: 0.1558905392885208\n",
      "Epoch 3693, Loss: 0.3016064912080765, Final Batch Loss: 0.1680976152420044\n",
      "Epoch 3694, Loss: 0.262808658182621, Final Batch Loss: 0.1579706072807312\n",
      "Epoch 3695, Loss: 0.29462240636348724, Final Batch Loss: 0.13433563709259033\n",
      "Epoch 3696, Loss: 0.3456036150455475, Final Batch Loss: 0.14775411784648895\n",
      "Epoch 3697, Loss: 0.33727768063545227, Final Batch Loss: 0.16960257291793823\n",
      "Epoch 3698, Loss: 0.2607971280813217, Final Batch Loss: 0.12828411161899567\n",
      "Epoch 3699, Loss: 0.29702386260032654, Final Batch Loss: 0.1686793714761734\n",
      "Epoch 3700, Loss: 0.2814968228340149, Final Batch Loss: 0.1491357535123825\n",
      "Epoch 3701, Loss: 0.33840087056159973, Final Batch Loss: 0.17163950204849243\n",
      "Epoch 3702, Loss: 0.31162846088409424, Final Batch Loss: 0.12455333769321442\n",
      "Epoch 3703, Loss: 0.2778703570365906, Final Batch Loss: 0.12341958284378052\n",
      "Epoch 3704, Loss: 0.28251470625400543, Final Batch Loss: 0.1205957680940628\n",
      "Epoch 3705, Loss: 0.29399473965168, Final Batch Loss: 0.14948110282421112\n",
      "Epoch 3706, Loss: 0.2922838032245636, Final Batch Loss: 0.16154778003692627\n",
      "Epoch 3707, Loss: 0.30297085642814636, Final Batch Loss: 0.11958739161491394\n",
      "Epoch 3708, Loss: 0.31054577231407166, Final Batch Loss: 0.1622084528207779\n",
      "Epoch 3709, Loss: 0.32525375485420227, Final Batch Loss: 0.19072751700878143\n",
      "Epoch 3710, Loss: 0.31558290123939514, Final Batch Loss: 0.17608697712421417\n",
      "Epoch 3711, Loss: 0.37988682091236115, Final Batch Loss: 0.23348966240882874\n",
      "Epoch 3712, Loss: 0.3295428901910782, Final Batch Loss: 0.13853223621845245\n",
      "Epoch 3713, Loss: 0.333772748708725, Final Batch Loss: 0.16791434586048126\n",
      "Epoch 3714, Loss: 0.2844986766576767, Final Batch Loss: 0.14820775389671326\n",
      "Epoch 3715, Loss: 0.3315468430519104, Final Batch Loss: 0.14981137216091156\n",
      "Epoch 3716, Loss: 0.2877151668071747, Final Batch Loss: 0.12219594419002533\n",
      "Epoch 3717, Loss: 0.278630830347538, Final Batch Loss: 0.11952973157167435\n",
      "Epoch 3718, Loss: 0.23518728464841843, Final Batch Loss: 0.0811137929558754\n",
      "Epoch 3719, Loss: 0.4075281322002411, Final Batch Loss: 0.23775526881217957\n",
      "Epoch 3720, Loss: 0.3540923744440079, Final Batch Loss: 0.1722692996263504\n",
      "Epoch 3721, Loss: 0.3505651503801346, Final Batch Loss: 0.17825882136821747\n",
      "Epoch 3722, Loss: 0.33517318218946457, Final Batch Loss: 0.11866248399019241\n",
      "Epoch 3723, Loss: 0.3496423214673996, Final Batch Loss: 0.1754973828792572\n",
      "Epoch 3724, Loss: 0.28426967561244965, Final Batch Loss: 0.12962785363197327\n",
      "Epoch 3725, Loss: 0.36426883935928345, Final Batch Loss: 0.17997854948043823\n",
      "Epoch 3726, Loss: 0.2873876690864563, Final Batch Loss: 0.15339896082878113\n",
      "Epoch 3727, Loss: 0.35025352239608765, Final Batch Loss: 0.19003638625144958\n",
      "Epoch 3728, Loss: 0.3348692059516907, Final Batch Loss: 0.15983521938323975\n",
      "Epoch 3729, Loss: 0.2823192775249481, Final Batch Loss: 0.12663967907428741\n",
      "Epoch 3730, Loss: 0.3178379684686661, Final Batch Loss: 0.19597284495830536\n",
      "Epoch 3731, Loss: 0.24099566042423248, Final Batch Loss: 0.11059911549091339\n",
      "Epoch 3732, Loss: 0.2737363427877426, Final Batch Loss: 0.14881698787212372\n",
      "Epoch 3733, Loss: 0.3586570620536804, Final Batch Loss: 0.21095885336399078\n",
      "Epoch 3734, Loss: 0.32076026499271393, Final Batch Loss: 0.1359315663576126\n",
      "Epoch 3735, Loss: 0.33592215180397034, Final Batch Loss: 0.1907358318567276\n",
      "Epoch 3736, Loss: 0.33579646050930023, Final Batch Loss: 0.2290651500225067\n",
      "Epoch 3737, Loss: 0.31735971570014954, Final Batch Loss: 0.16458727419376373\n",
      "Epoch 3738, Loss: 0.2931101620197296, Final Batch Loss: 0.11134229600429535\n",
      "Epoch 3739, Loss: 0.3716229498386383, Final Batch Loss: 0.24430517852306366\n",
      "Epoch 3740, Loss: 0.36119626462459564, Final Batch Loss: 0.19770902395248413\n",
      "Epoch 3741, Loss: 0.2838580012321472, Final Batch Loss: 0.13333961367607117\n",
      "Epoch 3742, Loss: 0.3380105793476105, Final Batch Loss: 0.16033433377742767\n",
      "Epoch 3743, Loss: 0.3464968204498291, Final Batch Loss: 0.16016089916229248\n",
      "Epoch 3744, Loss: 0.298404261469841, Final Batch Loss: 0.1274602711200714\n",
      "Epoch 3745, Loss: 0.3406602144241333, Final Batch Loss: 0.17988954484462738\n",
      "Epoch 3746, Loss: 0.3328157216310501, Final Batch Loss: 0.1817496418952942\n",
      "Epoch 3747, Loss: 0.26398713886737823, Final Batch Loss: 0.11288493871688843\n",
      "Epoch 3748, Loss: 0.2990555912256241, Final Batch Loss: 0.17143219709396362\n",
      "Epoch 3749, Loss: 0.2786354348063469, Final Batch Loss: 0.16284385323524475\n",
      "Epoch 3750, Loss: 0.32008975744247437, Final Batch Loss: 0.1913016438484192\n",
      "Epoch 3751, Loss: 0.299737885594368, Final Batch Loss: 0.12909629940986633\n",
      "Epoch 3752, Loss: 0.26973941177129745, Final Batch Loss: 0.11673016101121902\n",
      "Epoch 3753, Loss: 0.3570854365825653, Final Batch Loss: 0.2116742879152298\n",
      "Epoch 3754, Loss: 0.33052101731300354, Final Batch Loss: 0.1624477207660675\n",
      "Epoch 3755, Loss: 0.27552324533462524, Final Batch Loss: 0.1360948085784912\n",
      "Epoch 3756, Loss: 0.3315739929676056, Final Batch Loss: 0.1771431416273117\n",
      "Epoch 3757, Loss: 0.26590732485055923, Final Batch Loss: 0.14828115701675415\n",
      "Epoch 3758, Loss: 0.33460886776447296, Final Batch Loss: 0.16302797198295593\n",
      "Epoch 3759, Loss: 0.26477257162332535, Final Batch Loss: 0.11737493425607681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3760, Loss: 0.3635687977075577, Final Batch Loss: 0.15843404829502106\n",
      "Epoch 3761, Loss: 0.3637356013059616, Final Batch Loss: 0.2138291597366333\n",
      "Epoch 3762, Loss: 0.296703964471817, Final Batch Loss: 0.14357306063175201\n",
      "Epoch 3763, Loss: 0.28192080557346344, Final Batch Loss: 0.12392528355121613\n",
      "Epoch 3764, Loss: 0.2785687893629074, Final Batch Loss: 0.15677368640899658\n",
      "Epoch 3765, Loss: 0.3018973767757416, Final Batch Loss: 0.14482887089252472\n",
      "Epoch 3766, Loss: 0.3172854781150818, Final Batch Loss: 0.1752670556306839\n",
      "Epoch 3767, Loss: 0.28334084153175354, Final Batch Loss: 0.15419231355190277\n",
      "Epoch 3768, Loss: 0.36124663054943085, Final Batch Loss: 0.21572504937648773\n",
      "Epoch 3769, Loss: 0.28358177840709686, Final Batch Loss: 0.13300721347332\n",
      "Epoch 3770, Loss: 0.26809513568878174, Final Batch Loss: 0.12814323604106903\n",
      "Epoch 3771, Loss: 0.2816650718450546, Final Batch Loss: 0.11720892786979675\n",
      "Epoch 3772, Loss: 0.28798484802246094, Final Batch Loss: 0.17073450982570648\n",
      "Epoch 3773, Loss: 0.31000884622335434, Final Batch Loss: 0.19186030328273773\n",
      "Epoch 3774, Loss: 0.27119171619415283, Final Batch Loss: 0.14918014407157898\n",
      "Epoch 3775, Loss: 0.30222900211811066, Final Batch Loss: 0.13023759424686432\n",
      "Epoch 3776, Loss: 0.298934668302536, Final Batch Loss: 0.14581400156021118\n",
      "Epoch 3777, Loss: 0.3296731263399124, Final Batch Loss: 0.19164524972438812\n",
      "Epoch 3778, Loss: 0.2789597138762474, Final Batch Loss: 0.15731044113636017\n",
      "Epoch 3779, Loss: 0.2660171687602997, Final Batch Loss: 0.10150608420372009\n",
      "Epoch 3780, Loss: 0.30216550827026367, Final Batch Loss: 0.1600826233625412\n",
      "Epoch 3781, Loss: 0.3108767718076706, Final Batch Loss: 0.17030023038387299\n",
      "Epoch 3782, Loss: 0.32090483605861664, Final Batch Loss: 0.1552804559469223\n",
      "Epoch 3783, Loss: 0.2784474194049835, Final Batch Loss: 0.1395554393529892\n",
      "Epoch 3784, Loss: 0.2987254410982132, Final Batch Loss: 0.16209839284420013\n",
      "Epoch 3785, Loss: 0.25987614691257477, Final Batch Loss: 0.13753606379032135\n",
      "Epoch 3786, Loss: 0.3240482211112976, Final Batch Loss: 0.19315531849861145\n",
      "Epoch 3787, Loss: 0.28598134219646454, Final Batch Loss: 0.1417175531387329\n",
      "Epoch 3788, Loss: 0.3757183998823166, Final Batch Loss: 0.20993243157863617\n",
      "Epoch 3789, Loss: 0.36178267002105713, Final Batch Loss: 0.2427440881729126\n",
      "Epoch 3790, Loss: 0.26258329302072525, Final Batch Loss: 0.14861902594566345\n",
      "Epoch 3791, Loss: 0.30463707447052, Final Batch Loss: 0.13992655277252197\n",
      "Epoch 3792, Loss: 0.3055180087685585, Final Batch Loss: 0.20145319402217865\n",
      "Epoch 3793, Loss: 0.33160673826932907, Final Batch Loss: 0.10806400328874588\n",
      "Epoch 3794, Loss: 0.25876012444496155, Final Batch Loss: 0.10634259879589081\n",
      "Epoch 3795, Loss: 0.3298758566379547, Final Batch Loss: 0.18137571215629578\n",
      "Epoch 3796, Loss: 0.3137456327676773, Final Batch Loss: 0.14723503589630127\n",
      "Epoch 3797, Loss: 0.25531770288944244, Final Batch Loss: 0.12668317556381226\n",
      "Epoch 3798, Loss: 0.31878961622714996, Final Batch Loss: 0.172135129570961\n",
      "Epoch 3799, Loss: 0.2743946313858032, Final Batch Loss: 0.13612665235996246\n",
      "Epoch 3800, Loss: 0.31517165899276733, Final Batch Loss: 0.18469099700450897\n",
      "Epoch 3801, Loss: 0.32565511763095856, Final Batch Loss: 0.16116929054260254\n",
      "Epoch 3802, Loss: 0.3058507889509201, Final Batch Loss: 0.1581394225358963\n",
      "Epoch 3803, Loss: 0.2809150144457817, Final Batch Loss: 0.12442243844270706\n",
      "Epoch 3804, Loss: 0.29658351838588715, Final Batch Loss: 0.16677848994731903\n",
      "Epoch 3805, Loss: 0.29588672518730164, Final Batch Loss: 0.187774658203125\n",
      "Epoch 3806, Loss: 0.2660480737686157, Final Batch Loss: 0.13262175023555756\n",
      "Epoch 3807, Loss: 0.3274623900651932, Final Batch Loss: 0.15181918442249298\n",
      "Epoch 3808, Loss: 0.3035131096839905, Final Batch Loss: 0.15919066965579987\n",
      "Epoch 3809, Loss: 0.2737683355808258, Final Batch Loss: 0.1456523835659027\n",
      "Epoch 3810, Loss: 0.28537197411060333, Final Batch Loss: 0.14402078092098236\n",
      "Epoch 3811, Loss: 0.3021532669663429, Final Batch Loss: 0.11389908939599991\n",
      "Epoch 3812, Loss: 0.28725577890872955, Final Batch Loss: 0.15385204553604126\n",
      "Epoch 3813, Loss: 0.34013234078884125, Final Batch Loss: 0.17877429723739624\n",
      "Epoch 3814, Loss: 0.2742370441555977, Final Batch Loss: 0.1084320917725563\n",
      "Epoch 3815, Loss: 0.27392256259918213, Final Batch Loss: 0.1463351845741272\n",
      "Epoch 3816, Loss: 0.3461466580629349, Final Batch Loss: 0.18424081802368164\n",
      "Epoch 3817, Loss: 0.31029418110847473, Final Batch Loss: 0.15338127315044403\n",
      "Epoch 3818, Loss: 0.362109437584877, Final Batch Loss: 0.1528630405664444\n",
      "Epoch 3819, Loss: 0.3294079452753067, Final Batch Loss: 0.15667593479156494\n",
      "Epoch 3820, Loss: 0.25246611982584, Final Batch Loss: 0.10693582147359848\n",
      "Epoch 3821, Loss: 0.2959195524454117, Final Batch Loss: 0.1362466812133789\n",
      "Epoch 3822, Loss: 0.2687317654490471, Final Batch Loss: 0.09449482709169388\n",
      "Epoch 3823, Loss: 0.29766637086868286, Final Batch Loss: 0.15634050965309143\n",
      "Epoch 3824, Loss: 0.3357556015253067, Final Batch Loss: 0.1898438185453415\n",
      "Epoch 3825, Loss: 0.3506620079278946, Final Batch Loss: 0.18951505422592163\n",
      "Epoch 3826, Loss: 0.34856458008289337, Final Batch Loss: 0.1525559276342392\n",
      "Epoch 3827, Loss: 0.32137665152549744, Final Batch Loss: 0.18886634707450867\n",
      "Epoch 3828, Loss: 0.3377838730812073, Final Batch Loss: 0.12549804151058197\n",
      "Epoch 3829, Loss: 0.28755801171064377, Final Batch Loss: 0.16521543264389038\n",
      "Epoch 3830, Loss: 0.277226522564888, Final Batch Loss: 0.0832885205745697\n",
      "Epoch 3831, Loss: 0.2572646588087082, Final Batch Loss: 0.14883330464363098\n",
      "Epoch 3832, Loss: 0.3088781088590622, Final Batch Loss: 0.14652010798454285\n",
      "Epoch 3833, Loss: 0.336692675948143, Final Batch Loss: 0.17595987021923065\n",
      "Epoch 3834, Loss: 0.28474385291337967, Final Batch Loss: 0.10834818333387375\n",
      "Epoch 3835, Loss: 0.2802327126264572, Final Batch Loss: 0.1599297672510147\n",
      "Epoch 3836, Loss: 0.3043450564146042, Final Batch Loss: 0.13406196236610413\n",
      "Epoch 3837, Loss: 0.28005677461624146, Final Batch Loss: 0.12855744361877441\n",
      "Epoch 3838, Loss: 0.2822950705885887, Final Batch Loss: 0.10705330222845078\n",
      "Epoch 3839, Loss: 0.25674252212047577, Final Batch Loss: 0.12797002494335175\n",
      "Epoch 3840, Loss: 0.2908930778503418, Final Batch Loss: 0.14174990355968475\n",
      "Epoch 3841, Loss: 0.28920888900756836, Final Batch Loss: 0.1110653281211853\n",
      "Epoch 3842, Loss: 0.3167656362056732, Final Batch Loss: 0.1432640552520752\n",
      "Epoch 3843, Loss: 0.26497893035411835, Final Batch Loss: 0.13229893147945404\n",
      "Epoch 3844, Loss: 0.31124502420425415, Final Batch Loss: 0.16338402032852173\n",
      "Epoch 3845, Loss: 0.32767869532108307, Final Batch Loss: 0.14630015194416046\n",
      "Epoch 3846, Loss: 0.31927720457315445, Final Batch Loss: 0.12269899994134903\n",
      "Epoch 3847, Loss: 0.27846167236566544, Final Batch Loss: 0.12165234237909317\n",
      "Epoch 3848, Loss: 0.32006262242794037, Final Batch Loss: 0.15597523748874664\n",
      "Epoch 3849, Loss: 0.3855011314153671, Final Batch Loss: 0.1523413509130478\n",
      "Epoch 3850, Loss: 0.30034807324409485, Final Batch Loss: 0.15221619606018066\n",
      "Epoch 3851, Loss: 0.29475392401218414, Final Batch Loss: 0.14385291934013367\n",
      "Epoch 3852, Loss: 0.3412502259016037, Final Batch Loss: 0.18453605473041534\n",
      "Epoch 3853, Loss: 0.24114783108234406, Final Batch Loss: 0.11505906283855438\n",
      "Epoch 3854, Loss: 0.34604334831237793, Final Batch Loss: 0.21755217015743256\n",
      "Epoch 3855, Loss: 0.27618400752544403, Final Batch Loss: 0.13306741416454315\n",
      "Epoch 3856, Loss: 0.30463463068008423, Final Batch Loss: 0.18305422365665436\n",
      "Epoch 3857, Loss: 0.26807913184165955, Final Batch Loss: 0.13772350549697876\n",
      "Epoch 3858, Loss: 0.26766353100538254, Final Batch Loss: 0.1151425763964653\n",
      "Epoch 3859, Loss: 0.2762786000967026, Final Batch Loss: 0.12239408493041992\n",
      "Epoch 3860, Loss: 0.30183015763759613, Final Batch Loss: 0.15449824929237366\n",
      "Epoch 3861, Loss: 0.31166671216487885, Final Batch Loss: 0.14224915206432343\n",
      "Epoch 3862, Loss: 0.28050360083580017, Final Batch Loss: 0.13756653666496277\n",
      "Epoch 3863, Loss: 0.2620619460940361, Final Batch Loss: 0.09420944005250931\n",
      "Epoch 3864, Loss: 0.3122878074645996, Final Batch Loss: 0.14060042798519135\n",
      "Epoch 3865, Loss: 0.28470373153686523, Final Batch Loss: 0.15049605071544647\n",
      "Epoch 3866, Loss: 0.2685195207595825, Final Batch Loss: 0.1324343979358673\n",
      "Epoch 3867, Loss: 0.2710534930229187, Final Batch Loss: 0.14333747327327728\n",
      "Epoch 3868, Loss: 0.30486758053302765, Final Batch Loss: 0.13880813121795654\n",
      "Epoch 3869, Loss: 0.2969310134649277, Final Batch Loss: 0.14380422234535217\n",
      "Epoch 3870, Loss: 0.3430664986371994, Final Batch Loss: 0.1533031463623047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3871, Loss: 0.2574436739087105, Final Batch Loss: 0.12145043164491653\n",
      "Epoch 3872, Loss: 0.28666746616363525, Final Batch Loss: 0.14284826815128326\n",
      "Epoch 3873, Loss: 0.32508543133735657, Final Batch Loss: 0.18598629534244537\n",
      "Epoch 3874, Loss: 0.27683351933956146, Final Batch Loss: 0.13818992674350739\n",
      "Epoch 3875, Loss: 0.23667779564857483, Final Batch Loss: 0.12402623146772385\n",
      "Epoch 3876, Loss: 0.27083656191825867, Final Batch Loss: 0.15164825320243835\n",
      "Epoch 3877, Loss: 0.2698434889316559, Final Batch Loss: 0.13208165764808655\n",
      "Epoch 3878, Loss: 0.27561408281326294, Final Batch Loss: 0.1154647022485733\n",
      "Epoch 3879, Loss: 0.2916637137532234, Final Batch Loss: 0.11802484840154648\n",
      "Epoch 3880, Loss: 0.2884367108345032, Final Batch Loss: 0.1581030786037445\n",
      "Epoch 3881, Loss: 0.27898193895816803, Final Batch Loss: 0.1256169229745865\n",
      "Epoch 3882, Loss: 0.2671836316585541, Final Batch Loss: 0.13008816540241241\n",
      "Epoch 3883, Loss: 0.2766730561852455, Final Batch Loss: 0.15558560192584991\n",
      "Epoch 3884, Loss: 0.3111911565065384, Final Batch Loss: 0.15556780993938446\n",
      "Epoch 3885, Loss: 0.34563663601875305, Final Batch Loss: 0.12728223204612732\n",
      "Epoch 3886, Loss: 0.2535923719406128, Final Batch Loss: 0.10834358632564545\n",
      "Epoch 3887, Loss: 0.3178125098347664, Final Batch Loss: 0.2058791071176529\n",
      "Epoch 3888, Loss: 0.335695743560791, Final Batch Loss: 0.19341523945331573\n",
      "Epoch 3889, Loss: 0.2857437953352928, Final Batch Loss: 0.117103211581707\n",
      "Epoch 3890, Loss: 0.25998347252607346, Final Batch Loss: 0.14588423073291779\n",
      "Epoch 3891, Loss: 0.2529883459210396, Final Batch Loss: 0.11529464274644852\n",
      "Epoch 3892, Loss: 0.28155115246772766, Final Batch Loss: 0.11788196861743927\n",
      "Epoch 3893, Loss: 0.28396938741207123, Final Batch Loss: 0.12232694029808044\n",
      "Epoch 3894, Loss: 0.2538546621799469, Final Batch Loss: 0.10079433023929596\n",
      "Epoch 3895, Loss: 0.2911335676908493, Final Batch Loss: 0.15185946226119995\n",
      "Epoch 3896, Loss: 0.31104789674282074, Final Batch Loss: 0.13537383079528809\n",
      "Epoch 3897, Loss: 0.3090837597846985, Final Batch Loss: 0.15811479091644287\n",
      "Epoch 3898, Loss: 0.3057495653629303, Final Batch Loss: 0.14511042833328247\n",
      "Epoch 3899, Loss: 0.30340491980314255, Final Batch Loss: 0.1193004623055458\n",
      "Epoch 3900, Loss: 0.3555579334497452, Final Batch Loss: 0.14365750551223755\n",
      "Epoch 3901, Loss: 0.24998551607131958, Final Batch Loss: 0.1271580308675766\n",
      "Epoch 3902, Loss: 0.2785976529121399, Final Batch Loss: 0.1469205766916275\n",
      "Epoch 3903, Loss: 0.2987423315644264, Final Batch Loss: 0.19541920721530914\n",
      "Epoch 3904, Loss: 0.32074151933193207, Final Batch Loss: 0.15795807540416718\n",
      "Epoch 3905, Loss: 0.2640102952718735, Final Batch Loss: 0.14443130791187286\n",
      "Epoch 3906, Loss: 0.3762059062719345, Final Batch Loss: 0.15957055985927582\n",
      "Epoch 3907, Loss: 0.33565714955329895, Final Batch Loss: 0.1616121083498001\n",
      "Epoch 3908, Loss: 0.2633223831653595, Final Batch Loss: 0.13480257987976074\n",
      "Epoch 3909, Loss: 0.2787882089614868, Final Batch Loss: 0.13957816362380981\n",
      "Epoch 3910, Loss: 0.33658988773822784, Final Batch Loss: 0.17836931347846985\n",
      "Epoch 3911, Loss: 0.34769558906555176, Final Batch Loss: 0.1970912516117096\n",
      "Epoch 3912, Loss: 0.24067998677492142, Final Batch Loss: 0.1202855110168457\n",
      "Epoch 3913, Loss: 0.24874908477067947, Final Batch Loss: 0.12111552804708481\n",
      "Epoch 3914, Loss: 0.24942069500684738, Final Batch Loss: 0.11859025806188583\n",
      "Epoch 3915, Loss: 0.3084893524646759, Final Batch Loss: 0.08606228232383728\n",
      "Epoch 3916, Loss: 0.3125658631324768, Final Batch Loss: 0.13351231813430786\n",
      "Epoch 3917, Loss: 0.25125738978385925, Final Batch Loss: 0.11730945110321045\n",
      "Epoch 3918, Loss: 0.326517716050148, Final Batch Loss: 0.19740192592144012\n",
      "Epoch 3919, Loss: 0.2608829736709595, Final Batch Loss: 0.12813901901245117\n",
      "Epoch 3920, Loss: 0.3200462684035301, Final Batch Loss: 0.10644235461950302\n",
      "Epoch 3921, Loss: 0.2806021422147751, Final Batch Loss: 0.13701635599136353\n",
      "Epoch 3922, Loss: 0.24463390558958054, Final Batch Loss: 0.11796476691961288\n",
      "Epoch 3923, Loss: 0.2896169275045395, Final Batch Loss: 0.1588640809059143\n",
      "Epoch 3924, Loss: 0.31002144515514374, Final Batch Loss: 0.1441478729248047\n",
      "Epoch 3925, Loss: 0.2513314485549927, Final Batch Loss: 0.10280762612819672\n",
      "Epoch 3926, Loss: 0.2530820220708847, Final Batch Loss: 0.12198741734027863\n",
      "Epoch 3927, Loss: 0.2282222881913185, Final Batch Loss: 0.10964278876781464\n",
      "Epoch 3928, Loss: 0.2684890031814575, Final Batch Loss: 0.13464507460594177\n",
      "Epoch 3929, Loss: 0.2908451706171036, Final Batch Loss: 0.14435507357120514\n",
      "Epoch 3930, Loss: 0.3134309649467468, Final Batch Loss: 0.15219922363758087\n",
      "Epoch 3931, Loss: 0.3122888505458832, Final Batch Loss: 0.17039881646633148\n",
      "Epoch 3932, Loss: 0.3110859990119934, Final Batch Loss: 0.13258729875087738\n",
      "Epoch 3933, Loss: 0.2604661211371422, Final Batch Loss: 0.12343985587358475\n",
      "Epoch 3934, Loss: 0.2753061503171921, Final Batch Loss: 0.11908648908138275\n",
      "Epoch 3935, Loss: 0.3128311038017273, Final Batch Loss: 0.15352296829223633\n",
      "Epoch 3936, Loss: 0.29585715383291245, Final Batch Loss: 0.09600462764501572\n",
      "Epoch 3937, Loss: 0.3090033233165741, Final Batch Loss: 0.13175331056118011\n",
      "Epoch 3938, Loss: 0.32000632584095, Final Batch Loss: 0.16161169111728668\n",
      "Epoch 3939, Loss: 0.3233626186847687, Final Batch Loss: 0.16686944663524628\n",
      "Epoch 3940, Loss: 0.27448081970214844, Final Batch Loss: 0.10505566000938416\n",
      "Epoch 3941, Loss: 0.31562092155218124, Final Batch Loss: 0.1226297989487648\n",
      "Epoch 3942, Loss: 0.26431064307689667, Final Batch Loss: 0.11064307391643524\n",
      "Epoch 3943, Loss: 0.2677993029356003, Final Batch Loss: 0.13436515629291534\n",
      "Epoch 3944, Loss: 0.33452238142490387, Final Batch Loss: 0.14217208325862885\n",
      "Epoch 3945, Loss: 0.3755147308111191, Final Batch Loss: 0.2367558628320694\n",
      "Epoch 3946, Loss: 0.2712705433368683, Final Batch Loss: 0.14054740965366364\n",
      "Epoch 3947, Loss: 0.24859122931957245, Final Batch Loss: 0.09315456449985504\n",
      "Epoch 3948, Loss: 0.3641539663076401, Final Batch Loss: 0.16787272691726685\n",
      "Epoch 3949, Loss: 0.3191094696521759, Final Batch Loss: 0.167661651968956\n",
      "Epoch 3950, Loss: 0.31553496420383453, Final Batch Loss: 0.14595474302768707\n",
      "Epoch 3951, Loss: 0.27713219821453094, Final Batch Loss: 0.09980827569961548\n",
      "Epoch 3952, Loss: 0.28542274236679077, Final Batch Loss: 0.14764092862606049\n",
      "Epoch 3953, Loss: 0.27048373222351074, Final Batch Loss: 0.14189432561397552\n",
      "Epoch 3954, Loss: 0.3170047104358673, Final Batch Loss: 0.19302630424499512\n",
      "Epoch 3955, Loss: 0.2525446116924286, Final Batch Loss: 0.1392975151538849\n",
      "Epoch 3956, Loss: 0.26944486796855927, Final Batch Loss: 0.12140919268131256\n",
      "Epoch 3957, Loss: 0.30737875401973724, Final Batch Loss: 0.17378893494606018\n",
      "Epoch 3958, Loss: 0.2668130695819855, Final Batch Loss: 0.13752448558807373\n",
      "Epoch 3959, Loss: 0.28451935946941376, Final Batch Loss: 0.11326488852500916\n",
      "Epoch 3960, Loss: 0.29536062479019165, Final Batch Loss: 0.15470853447914124\n",
      "Epoch 3961, Loss: 0.27846188843250275, Final Batch Loss: 0.15479542315006256\n",
      "Epoch 3962, Loss: 0.2915714308619499, Final Batch Loss: 0.17299875617027283\n",
      "Epoch 3963, Loss: 0.324201300740242, Final Batch Loss: 0.15287548303604126\n",
      "Epoch 3964, Loss: 0.32369624078273773, Final Batch Loss: 0.17123323678970337\n",
      "Epoch 3965, Loss: 0.2727292776107788, Final Batch Loss: 0.13101814687252045\n",
      "Epoch 3966, Loss: 0.2616199105978012, Final Batch Loss: 0.16783496737480164\n",
      "Epoch 3967, Loss: 0.3221992999315262, Final Batch Loss: 0.13091789186000824\n",
      "Epoch 3968, Loss: 0.25158391892910004, Final Batch Loss: 0.10664339363574982\n",
      "Epoch 3969, Loss: 0.277530238032341, Final Batch Loss: 0.11114655435085297\n",
      "Epoch 3970, Loss: 0.37420816719532013, Final Batch Loss: 0.22154659032821655\n",
      "Epoch 3971, Loss: 0.26189206540584564, Final Batch Loss: 0.12194280326366425\n",
      "Epoch 3972, Loss: 0.25229959189891815, Final Batch Loss: 0.13188482820987701\n",
      "Epoch 3973, Loss: 0.33270055055618286, Final Batch Loss: 0.17603106796741486\n",
      "Epoch 3974, Loss: 0.2154376432299614, Final Batch Loss: 0.09133671224117279\n",
      "Epoch 3975, Loss: 0.285964772105217, Final Batch Loss: 0.13755935430526733\n",
      "Epoch 3976, Loss: 0.2782333791255951, Final Batch Loss: 0.1297132819890976\n",
      "Epoch 3977, Loss: 0.27964313328266144, Final Batch Loss: 0.14587676525115967\n",
      "Epoch 3978, Loss: 0.26168620586395264, Final Batch Loss: 0.12645915150642395\n",
      "Epoch 3979, Loss: 0.33018143475055695, Final Batch Loss: 0.17396171391010284\n",
      "Epoch 3980, Loss: 0.34455280005931854, Final Batch Loss: 0.16921952366828918\n",
      "Epoch 3981, Loss: 0.30339376628398895, Final Batch Loss: 0.1699666678905487\n",
      "Epoch 3982, Loss: 0.3269546926021576, Final Batch Loss: 0.18646514415740967\n",
      "Epoch 3983, Loss: 0.2645261287689209, Final Batch Loss: 0.11487407982349396\n",
      "Epoch 3984, Loss: 0.2799764722585678, Final Batch Loss: 0.14578960835933685\n",
      "Epoch 3985, Loss: 0.31398534774780273, Final Batch Loss: 0.15578947961330414\n",
      "Epoch 3986, Loss: 0.2852502167224884, Final Batch Loss: 0.12159287929534912\n",
      "Epoch 3987, Loss: 0.30904002487659454, Final Batch Loss: 0.15274353325366974\n",
      "Epoch 3988, Loss: 0.28997965157032013, Final Batch Loss: 0.1378621608018875\n",
      "Epoch 3989, Loss: 0.3009068965911865, Final Batch Loss: 0.12186150252819061\n",
      "Epoch 3990, Loss: 0.26497968286275864, Final Batch Loss: 0.1071079894900322\n",
      "Epoch 3991, Loss: 0.30067839473485947, Final Batch Loss: 0.12249975651502609\n",
      "Epoch 3992, Loss: 0.2802393138408661, Final Batch Loss: 0.14482471346855164\n",
      "Epoch 3993, Loss: 0.3037358373403549, Final Batch Loss: 0.16036470234394073\n",
      "Epoch 3994, Loss: 0.2950732707977295, Final Batch Loss: 0.163631871342659\n",
      "Epoch 3995, Loss: 0.40562795102596283, Final Batch Loss: 0.24267305433750153\n",
      "Epoch 3996, Loss: 0.24643057584762573, Final Batch Loss: 0.13066384196281433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3997, Loss: 0.3032265305519104, Final Batch Loss: 0.15915006399154663\n",
      "Epoch 3998, Loss: 0.23785730451345444, Final Batch Loss: 0.12476140260696411\n",
      "Epoch 3999, Loss: 0.2706385850906372, Final Batch Loss: 0.1332503855228424\n",
      "Epoch 4000, Loss: 0.2729640528559685, Final Batch Loss: 0.12205732613801956\n",
      "Epoch 4001, Loss: 0.2720266133546829, Final Batch Loss: 0.14444488286972046\n",
      "Epoch 4002, Loss: 0.2948639392852783, Final Batch Loss: 0.16339685022830963\n",
      "Epoch 4003, Loss: 0.2925166189670563, Final Batch Loss: 0.13583140075206757\n",
      "Epoch 4004, Loss: 0.2233157828450203, Final Batch Loss: 0.09992460906505585\n",
      "Epoch 4005, Loss: 0.26283782720565796, Final Batch Loss: 0.11553366482257843\n",
      "Epoch 4006, Loss: 0.25716373324394226, Final Batch Loss: 0.12813487648963928\n",
      "Epoch 4007, Loss: 0.34427572786808014, Final Batch Loss: 0.19505934417247772\n",
      "Epoch 4008, Loss: 0.2991483211517334, Final Batch Loss: 0.11585946381092072\n",
      "Epoch 4009, Loss: 0.29337121546268463, Final Batch Loss: 0.17296339571475983\n",
      "Epoch 4010, Loss: 0.2322544828057289, Final Batch Loss: 0.11901578307151794\n",
      "Epoch 4011, Loss: 0.24296733736991882, Final Batch Loss: 0.11185911297798157\n",
      "Epoch 4012, Loss: 0.27984338998794556, Final Batch Loss: 0.14092935621738434\n",
      "Epoch 4013, Loss: 0.3017483949661255, Final Batch Loss: 0.11210718750953674\n",
      "Epoch 4014, Loss: 0.2702958434820175, Final Batch Loss: 0.14386187493801117\n",
      "Epoch 4015, Loss: 0.3167206346988678, Final Batch Loss: 0.12325271964073181\n",
      "Epoch 4016, Loss: 0.24114477634429932, Final Batch Loss: 0.12989264726638794\n",
      "Epoch 4017, Loss: 0.26805639266967773, Final Batch Loss: 0.14477550983428955\n",
      "Epoch 4018, Loss: 0.30239322781562805, Final Batch Loss: 0.16415643692016602\n",
      "Epoch 4019, Loss: 0.2853645831346512, Final Batch Loss: 0.15444916486740112\n",
      "Epoch 4020, Loss: 0.2908671349287033, Final Batch Loss: 0.12589804828166962\n",
      "Epoch 4021, Loss: 0.3545955717563629, Final Batch Loss: 0.1936509609222412\n",
      "Epoch 4022, Loss: 0.2621082216501236, Final Batch Loss: 0.15897521376609802\n",
      "Epoch 4023, Loss: 0.2630812078714371, Final Batch Loss: 0.1278579831123352\n",
      "Epoch 4024, Loss: 0.2925039380788803, Final Batch Loss: 0.16102948784828186\n",
      "Epoch 4025, Loss: 0.3050859719514847, Final Batch Loss: 0.16453982889652252\n",
      "Epoch 4026, Loss: 0.26116546988487244, Final Batch Loss: 0.12344804406166077\n",
      "Epoch 4027, Loss: 0.30436675250530243, Final Batch Loss: 0.1418292373418808\n",
      "Epoch 4028, Loss: 0.2601788341999054, Final Batch Loss: 0.1125066876411438\n",
      "Epoch 4029, Loss: 0.31392376124858856, Final Batch Loss: 0.12426745891571045\n",
      "Epoch 4030, Loss: 0.23526832461357117, Final Batch Loss: 0.13722005486488342\n",
      "Epoch 4031, Loss: 0.29964832961559296, Final Batch Loss: 0.15511326491832733\n",
      "Epoch 4032, Loss: 0.2972872108221054, Final Batch Loss: 0.14399048686027527\n",
      "Epoch 4033, Loss: 0.24919665604829788, Final Batch Loss: 0.13332803547382355\n",
      "Epoch 4034, Loss: 0.25344258546829224, Final Batch Loss: 0.10385853052139282\n",
      "Epoch 4035, Loss: 0.3123846799135208, Final Batch Loss: 0.17999006807804108\n",
      "Epoch 4036, Loss: 0.3392660468816757, Final Batch Loss: 0.19380615651607513\n",
      "Epoch 4037, Loss: 0.35728059709072113, Final Batch Loss: 0.22863200306892395\n",
      "Epoch 4038, Loss: 0.338091105222702, Final Batch Loss: 0.143148735165596\n",
      "Epoch 4039, Loss: 0.2841254696249962, Final Batch Loss: 0.16958822309970856\n",
      "Epoch 4040, Loss: 0.26764727383852005, Final Batch Loss: 0.12420742958784103\n",
      "Epoch 4041, Loss: 0.2740710526704788, Final Batch Loss: 0.10607628524303436\n",
      "Epoch 4042, Loss: 0.2714150995016098, Final Batch Loss: 0.12747426331043243\n",
      "Epoch 4043, Loss: 0.25023053586483, Final Batch Loss: 0.1169305145740509\n",
      "Epoch 4044, Loss: 0.2852494865655899, Final Batch Loss: 0.10609224438667297\n",
      "Epoch 4045, Loss: 0.2797608748078346, Final Batch Loss: 0.16627033054828644\n",
      "Epoch 4046, Loss: 0.26487650722265244, Final Batch Loss: 0.11359541863203049\n",
      "Epoch 4047, Loss: 0.35442622005939484, Final Batch Loss: 0.1936722993850708\n",
      "Epoch 4048, Loss: 0.37038134038448334, Final Batch Loss: 0.2553008496761322\n",
      "Epoch 4049, Loss: 0.24279968440532684, Final Batch Loss: 0.14160683751106262\n",
      "Epoch 4050, Loss: 0.32630405575037, Final Batch Loss: 0.12078414112329483\n",
      "Epoch 4051, Loss: 0.30657637119293213, Final Batch Loss: 0.1343773603439331\n",
      "Epoch 4052, Loss: 0.2751206085085869, Final Batch Loss: 0.11495185643434525\n",
      "Epoch 4053, Loss: 0.3342435359954834, Final Batch Loss: 0.19390258193016052\n",
      "Epoch 4054, Loss: 0.2930481731891632, Final Batch Loss: 0.17847637832164764\n",
      "Epoch 4055, Loss: 0.23486612737178802, Final Batch Loss: 0.0834779441356659\n",
      "Epoch 4056, Loss: 0.2767502889037132, Final Batch Loss: 0.16568432748317719\n",
      "Epoch 4057, Loss: 0.29835107177495956, Final Batch Loss: 0.12232957035303116\n",
      "Epoch 4058, Loss: 0.32769693434238434, Final Batch Loss: 0.16573838889598846\n",
      "Epoch 4059, Loss: 0.2879752814769745, Final Batch Loss: 0.13607190549373627\n",
      "Epoch 4060, Loss: 0.31729407608509064, Final Batch Loss: 0.17094118893146515\n",
      "Epoch 4061, Loss: 0.2822716385126114, Final Batch Loss: 0.12656135857105255\n",
      "Epoch 4062, Loss: 0.27067896723747253, Final Batch Loss: 0.18089960515499115\n",
      "Epoch 4063, Loss: 0.3151693791151047, Final Batch Loss: 0.1609770506620407\n",
      "Epoch 4064, Loss: 0.27731919288635254, Final Batch Loss: 0.12905144691467285\n",
      "Epoch 4065, Loss: 0.25563640147447586, Final Batch Loss: 0.15138384699821472\n",
      "Epoch 4066, Loss: 0.2754480168223381, Final Batch Loss: 0.15682677924633026\n",
      "Epoch 4067, Loss: 0.2829597145318985, Final Batch Loss: 0.13945932686328888\n",
      "Epoch 4068, Loss: 0.32316848635673523, Final Batch Loss: 0.1519942283630371\n",
      "Epoch 4069, Loss: 0.25464335829019547, Final Batch Loss: 0.13692022860050201\n",
      "Epoch 4070, Loss: 0.2691182866692543, Final Batch Loss: 0.12280561774969101\n",
      "Epoch 4071, Loss: 0.2596183195710182, Final Batch Loss: 0.1596890538930893\n",
      "Epoch 4072, Loss: 0.22590060532093048, Final Batch Loss: 0.10826244205236435\n",
      "Epoch 4073, Loss: 0.23356235027313232, Final Batch Loss: 0.09467534720897675\n",
      "Epoch 4074, Loss: 0.27046384662389755, Final Batch Loss: 0.15331651270389557\n",
      "Epoch 4075, Loss: 0.23438184708356857, Final Batch Loss: 0.1302279382944107\n",
      "Epoch 4076, Loss: 0.27414656430482864, Final Batch Loss: 0.15207865834236145\n",
      "Epoch 4077, Loss: 0.2790265679359436, Final Batch Loss: 0.16272924840450287\n",
      "Epoch 4078, Loss: 0.30866818130016327, Final Batch Loss: 0.15138141810894012\n",
      "Epoch 4079, Loss: 0.24814818054437637, Final Batch Loss: 0.14101248979568481\n",
      "Epoch 4080, Loss: 0.2821384444832802, Final Batch Loss: 0.17618994414806366\n",
      "Epoch 4081, Loss: 0.23693802952766418, Final Batch Loss: 0.12782242894172668\n",
      "Epoch 4082, Loss: 0.3745592385530472, Final Batch Loss: 0.18658176064491272\n",
      "Epoch 4083, Loss: 0.3041449934244156, Final Batch Loss: 0.1567644327878952\n",
      "Epoch 4084, Loss: 0.2575739026069641, Final Batch Loss: 0.13000592589378357\n",
      "Epoch 4085, Loss: 0.30317021906375885, Final Batch Loss: 0.17060236632823944\n",
      "Epoch 4086, Loss: 0.33485883474349976, Final Batch Loss: 0.2059975415468216\n",
      "Epoch 4087, Loss: 0.2602769136428833, Final Batch Loss: 0.1271599680185318\n",
      "Epoch 4088, Loss: 0.30094797909259796, Final Batch Loss: 0.12902714312076569\n",
      "Epoch 4089, Loss: 0.2489161491394043, Final Batch Loss: 0.12130492925643921\n",
      "Epoch 4090, Loss: 0.3563432991504669, Final Batch Loss: 0.19842411577701569\n",
      "Epoch 4091, Loss: 0.2444107010960579, Final Batch Loss: 0.12051568925380707\n",
      "Epoch 4092, Loss: 0.31852562725543976, Final Batch Loss: 0.16674697399139404\n",
      "Epoch 4093, Loss: 0.2767547518014908, Final Batch Loss: 0.14828817546367645\n",
      "Epoch 4094, Loss: 0.3018893077969551, Final Batch Loss: 0.11629707366228104\n",
      "Epoch 4095, Loss: 0.26916106045246124, Final Batch Loss: 0.1379275619983673\n",
      "Epoch 4096, Loss: 0.3391924798488617, Final Batch Loss: 0.1645752489566803\n",
      "Epoch 4097, Loss: 0.2284781113266945, Final Batch Loss: 0.10618449747562408\n",
      "Epoch 4098, Loss: 0.29841451346874237, Final Batch Loss: 0.13968883454799652\n",
      "Epoch 4099, Loss: 0.2628975361585617, Final Batch Loss: 0.12184549868106842\n",
      "Epoch 4100, Loss: 0.27205827832221985, Final Batch Loss: 0.12918543815612793\n",
      "Epoch 4101, Loss: 0.27759692072868347, Final Batch Loss: 0.12466233968734741\n",
      "Epoch 4102, Loss: 0.23389138281345367, Final Batch Loss: 0.09885625541210175\n",
      "Epoch 4103, Loss: 0.28585799038410187, Final Batch Loss: 0.15284888446331024\n",
      "Epoch 4104, Loss: 0.29697272181510925, Final Batch Loss: 0.16759924590587616\n",
      "Epoch 4105, Loss: 0.31207437813282013, Final Batch Loss: 0.1731516271829605\n",
      "Epoch 4106, Loss: 0.27738969773054123, Final Batch Loss: 0.11421649903059006\n",
      "Epoch 4107, Loss: 0.2983732372522354, Final Batch Loss: 0.14588777720928192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4108, Loss: 0.2816849797964096, Final Batch Loss: 0.13620856404304504\n",
      "Epoch 4109, Loss: 0.29208536446094513, Final Batch Loss: 0.14214850962162018\n",
      "Epoch 4110, Loss: 0.31052085757255554, Final Batch Loss: 0.1401212513446808\n",
      "Epoch 4111, Loss: 0.24005171656608582, Final Batch Loss: 0.10690338909626007\n",
      "Epoch 4112, Loss: 0.2456885203719139, Final Batch Loss: 0.12001652270555496\n",
      "Epoch 4113, Loss: 0.24438778311014175, Final Batch Loss: 0.1447858214378357\n",
      "Epoch 4114, Loss: 0.2725715935230255, Final Batch Loss: 0.09855884313583374\n",
      "Epoch 4115, Loss: 0.25299401581287384, Final Batch Loss: 0.14559273421764374\n",
      "Epoch 4116, Loss: 0.2637050598859787, Final Batch Loss: 0.11924909055233002\n",
      "Epoch 4117, Loss: 0.23839577287435532, Final Batch Loss: 0.11060499399900436\n",
      "Epoch 4118, Loss: 0.27761927247047424, Final Batch Loss: 0.13753780722618103\n",
      "Epoch 4119, Loss: 0.3087266832590103, Final Batch Loss: 0.14522744715213776\n",
      "Epoch 4120, Loss: 0.26088665425777435, Final Batch Loss: 0.15195070207118988\n",
      "Epoch 4121, Loss: 0.3070967197418213, Final Batch Loss: 0.1760990172624588\n",
      "Epoch 4122, Loss: 0.2795114517211914, Final Batch Loss: 0.1422407180070877\n",
      "Epoch 4123, Loss: 0.2773417681455612, Final Batch Loss: 0.13437075912952423\n",
      "Epoch 4124, Loss: 0.2877710908651352, Final Batch Loss: 0.15141499042510986\n",
      "Epoch 4125, Loss: 0.3459826707839966, Final Batch Loss: 0.1866985261440277\n",
      "Epoch 4126, Loss: 0.25986216962337494, Final Batch Loss: 0.14033222198486328\n",
      "Epoch 4127, Loss: 0.25468841940164566, Final Batch Loss: 0.13264219462871552\n",
      "Epoch 4128, Loss: 0.22546076029539108, Final Batch Loss: 0.10496441274881363\n",
      "Epoch 4129, Loss: 0.3300678879022598, Final Batch Loss: 0.14292356371879578\n",
      "Epoch 4130, Loss: 0.2835463881492615, Final Batch Loss: 0.13006161153316498\n",
      "Epoch 4131, Loss: 0.221940316259861, Final Batch Loss: 0.11575872451066971\n",
      "Epoch 4132, Loss: 0.24424975365400314, Final Batch Loss: 0.08973122388124466\n",
      "Epoch 4133, Loss: 0.29734988510608673, Final Batch Loss: 0.1480826735496521\n",
      "Epoch 4134, Loss: 0.2693150192499161, Final Batch Loss: 0.1133960634469986\n",
      "Epoch 4135, Loss: 0.3271949589252472, Final Batch Loss: 0.18149669468402863\n",
      "Epoch 4136, Loss: 0.2545306906104088, Final Batch Loss: 0.11616700142621994\n",
      "Epoch 4137, Loss: 0.2815992683172226, Final Batch Loss: 0.1373300999403\n",
      "Epoch 4138, Loss: 0.27581875771284103, Final Batch Loss: 0.1249409094452858\n",
      "Epoch 4139, Loss: 0.28381357342004776, Final Batch Loss: 0.12423167377710342\n",
      "Epoch 4140, Loss: 0.27101416885852814, Final Batch Loss: 0.1356661021709442\n",
      "Epoch 4141, Loss: 0.28680621832609177, Final Batch Loss: 0.1116175577044487\n",
      "Epoch 4142, Loss: 0.2857864648103714, Final Batch Loss: 0.12503956258296967\n",
      "Epoch 4143, Loss: 0.2900064140558243, Final Batch Loss: 0.12021918594837189\n",
      "Epoch 4144, Loss: 0.27129675447940826, Final Batch Loss: 0.14783111214637756\n",
      "Epoch 4145, Loss: 0.26387108117341995, Final Batch Loss: 0.16049526631832123\n",
      "Epoch 4146, Loss: 0.22665616869926453, Final Batch Loss: 0.11094199120998383\n",
      "Epoch 4147, Loss: 0.2782860994338989, Final Batch Loss: 0.16106782853603363\n",
      "Epoch 4148, Loss: 0.2650829702615738, Final Batch Loss: 0.1394548863172531\n",
      "Epoch 4149, Loss: 0.22827254980802536, Final Batch Loss: 0.08852872997522354\n",
      "Epoch 4150, Loss: 0.2596943974494934, Final Batch Loss: 0.12687155604362488\n",
      "Epoch 4151, Loss: 0.23452407866716385, Final Batch Loss: 0.09890670329332352\n",
      "Epoch 4152, Loss: 0.256538949906826, Final Batch Loss: 0.10893156379461288\n",
      "Epoch 4153, Loss: 0.2428266853094101, Final Batch Loss: 0.10239501297473907\n",
      "Epoch 4154, Loss: 0.2873898595571518, Final Batch Loss: 0.15207751095294952\n",
      "Epoch 4155, Loss: 0.32744739949703217, Final Batch Loss: 0.15657757222652435\n",
      "Epoch 4156, Loss: 0.2635517716407776, Final Batch Loss: 0.12745314836502075\n",
      "Epoch 4157, Loss: 0.30052492022514343, Final Batch Loss: 0.14723877608776093\n",
      "Epoch 4158, Loss: 0.29085031151771545, Final Batch Loss: 0.14676439762115479\n",
      "Epoch 4159, Loss: 0.2804933190345764, Final Batch Loss: 0.1470068395137787\n",
      "Epoch 4160, Loss: 0.33640390634536743, Final Batch Loss: 0.14994332194328308\n",
      "Epoch 4161, Loss: 0.2646697461605072, Final Batch Loss: 0.1333262026309967\n",
      "Epoch 4162, Loss: 0.3688119053840637, Final Batch Loss: 0.21633006632328033\n",
      "Epoch 4163, Loss: 0.26621322333812714, Final Batch Loss: 0.1404503583908081\n",
      "Epoch 4164, Loss: 0.26410409063100815, Final Batch Loss: 0.15035495162010193\n",
      "Epoch 4165, Loss: 0.2933129593729973, Final Batch Loss: 0.10227604955434799\n",
      "Epoch 4166, Loss: 0.24978691339492798, Final Batch Loss: 0.15677566826343536\n",
      "Epoch 4167, Loss: 0.2586798593401909, Final Batch Loss: 0.12274452298879623\n",
      "Epoch 4168, Loss: 0.24048016220331192, Final Batch Loss: 0.12812526524066925\n",
      "Epoch 4169, Loss: 0.30165569484233856, Final Batch Loss: 0.14793835580348969\n",
      "Epoch 4170, Loss: 0.24811066687107086, Final Batch Loss: 0.11969900131225586\n",
      "Epoch 4171, Loss: 0.22372771799564362, Final Batch Loss: 0.10371190309524536\n",
      "Epoch 4172, Loss: 0.2768314406275749, Final Batch Loss: 0.11560525745153427\n",
      "Epoch 4173, Loss: 0.2629900127649307, Final Batch Loss: 0.13635537028312683\n",
      "Epoch 4174, Loss: 0.2669386565685272, Final Batch Loss: 0.14542120695114136\n",
      "Epoch 4175, Loss: 0.251025453209877, Final Batch Loss: 0.11013543605804443\n",
      "Epoch 4176, Loss: 0.268166646361351, Final Batch Loss: 0.12805704772472382\n",
      "Epoch 4177, Loss: 0.3217426910996437, Final Batch Loss: 0.10969162732362747\n",
      "Epoch 4178, Loss: 0.21869555115699768, Final Batch Loss: 0.1303548514842987\n",
      "Epoch 4179, Loss: 0.23015054315328598, Final Batch Loss: 0.12293940037488937\n",
      "Epoch 4180, Loss: 0.23569228500127792, Final Batch Loss: 0.12975941598415375\n",
      "Epoch 4181, Loss: 0.2901299148797989, Final Batch Loss: 0.16112889349460602\n",
      "Epoch 4182, Loss: 0.2546750456094742, Final Batch Loss: 0.1241479367017746\n",
      "Epoch 4183, Loss: 0.25767266750335693, Final Batch Loss: 0.12864285707473755\n",
      "Epoch 4184, Loss: 0.2882010340690613, Final Batch Loss: 0.11512462794780731\n",
      "Epoch 4185, Loss: 0.2438913881778717, Final Batch Loss: 0.13228414952754974\n",
      "Epoch 4186, Loss: 0.3134041875600815, Final Batch Loss: 0.1534600555896759\n",
      "Epoch 4187, Loss: 0.2760789841413498, Final Batch Loss: 0.12057165801525116\n",
      "Epoch 4188, Loss: 0.2881280183792114, Final Batch Loss: 0.1610482633113861\n",
      "Epoch 4189, Loss: 0.24283022433519363, Final Batch Loss: 0.11093773692846298\n",
      "Epoch 4190, Loss: 0.28585074841976166, Final Batch Loss: 0.13986057043075562\n",
      "Epoch 4191, Loss: 0.3432212322950363, Final Batch Loss: 0.13904108107089996\n",
      "Epoch 4192, Loss: 0.343791127204895, Final Batch Loss: 0.1963360756635666\n",
      "Epoch 4193, Loss: 0.30514657497406006, Final Batch Loss: 0.11934739351272583\n",
      "Epoch 4194, Loss: 0.24743770062923431, Final Batch Loss: 0.11634646356105804\n",
      "Epoch 4195, Loss: 0.3130457103252411, Final Batch Loss: 0.18366456031799316\n",
      "Epoch 4196, Loss: 0.2515806555747986, Final Batch Loss: 0.12068088352680206\n",
      "Epoch 4197, Loss: 0.27916882932186127, Final Batch Loss: 0.17879843711853027\n",
      "Epoch 4198, Loss: 0.3008057028055191, Final Batch Loss: 0.14905981719493866\n",
      "Epoch 4199, Loss: 0.2837498039007187, Final Batch Loss: 0.13885532319545746\n",
      "Epoch 4200, Loss: 0.24751393496990204, Final Batch Loss: 0.16622059047222137\n",
      "Epoch 4201, Loss: 0.2538333013653755, Final Batch Loss: 0.13770270347595215\n",
      "Epoch 4202, Loss: 0.31349150836467743, Final Batch Loss: 0.16473299264907837\n",
      "Epoch 4203, Loss: 0.30092884600162506, Final Batch Loss: 0.1472233086824417\n",
      "Epoch 4204, Loss: 0.3142395615577698, Final Batch Loss: 0.1599971204996109\n",
      "Epoch 4205, Loss: 0.25132742524147034, Final Batch Loss: 0.12467452883720398\n",
      "Epoch 4206, Loss: 0.2913093566894531, Final Batch Loss: 0.1329408884048462\n",
      "Epoch 4207, Loss: 0.23465726524591446, Final Batch Loss: 0.11896204948425293\n",
      "Epoch 4208, Loss: 0.2591477707028389, Final Batch Loss: 0.12140818685293198\n",
      "Epoch 4209, Loss: 0.2919987514615059, Final Batch Loss: 0.17143970727920532\n",
      "Epoch 4210, Loss: 0.3243922144174576, Final Batch Loss: 0.1322874277830124\n",
      "Epoch 4211, Loss: 0.2701341211795807, Final Batch Loss: 0.13572350144386292\n",
      "Epoch 4212, Loss: 0.3228234499692917, Final Batch Loss: 0.17968612909317017\n",
      "Epoch 4213, Loss: 0.24243563413619995, Final Batch Loss: 0.1011216789484024\n",
      "Epoch 4214, Loss: 0.273319274187088, Final Batch Loss: 0.12827245891094208\n",
      "Epoch 4215, Loss: 0.26551374793052673, Final Batch Loss: 0.13211405277252197\n",
      "Epoch 4216, Loss: 0.2612062692642212, Final Batch Loss: 0.13672614097595215\n",
      "Epoch 4217, Loss: 0.2663620859384537, Final Batch Loss: 0.12419678270816803\n",
      "Epoch 4218, Loss: 0.31647340953350067, Final Batch Loss: 0.12883450090885162\n",
      "Epoch 4219, Loss: 0.28832513093948364, Final Batch Loss: 0.1584288626909256\n",
      "Epoch 4220, Loss: 0.31897545605897903, Final Batch Loss: 0.20460599660873413\n",
      "Epoch 4221, Loss: 0.3045651614665985, Final Batch Loss: 0.16577550768852234\n",
      "Epoch 4222, Loss: 0.34591367840766907, Final Batch Loss: 0.11935894191265106\n",
      "Epoch 4223, Loss: 0.2508062720298767, Final Batch Loss: 0.14335887134075165\n",
      "Epoch 4224, Loss: 0.2918684184551239, Final Batch Loss: 0.15370111167430878\n",
      "Epoch 4225, Loss: 0.28422975540161133, Final Batch Loss: 0.15485510230064392\n",
      "Epoch 4226, Loss: 0.2402857169508934, Final Batch Loss: 0.10193929821252823\n",
      "Epoch 4227, Loss: 0.2925800681114197, Final Batch Loss: 0.16864654421806335\n",
      "Epoch 4228, Loss: 0.2368055060505867, Final Batch Loss: 0.10432048887014389\n",
      "Epoch 4229, Loss: 0.2885202467441559, Final Batch Loss: 0.13607144355773926\n",
      "Epoch 4230, Loss: 0.21807751059532166, Final Batch Loss: 0.09201052784919739\n",
      "Epoch 4231, Loss: 0.32989858090877533, Final Batch Loss: 0.15838532149791718\n",
      "Epoch 4232, Loss: 0.22850697487592697, Final Batch Loss: 0.10734421759843826\n",
      "Epoch 4233, Loss: 0.2779865860939026, Final Batch Loss: 0.12560641765594482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4234, Loss: 0.2493324801325798, Final Batch Loss: 0.14871786534786224\n",
      "Epoch 4235, Loss: 0.3236313909292221, Final Batch Loss: 0.1679326295852661\n",
      "Epoch 4236, Loss: 0.267110675573349, Final Batch Loss: 0.13950324058532715\n",
      "Epoch 4237, Loss: 0.24184782058000565, Final Batch Loss: 0.09427999705076218\n",
      "Epoch 4238, Loss: 0.3103204518556595, Final Batch Loss: 0.13674435019493103\n",
      "Epoch 4239, Loss: 0.24827199429273605, Final Batch Loss: 0.13926206529140472\n",
      "Epoch 4240, Loss: 0.26623284816741943, Final Batch Loss: 0.1344798505306244\n",
      "Epoch 4241, Loss: 0.21867340803146362, Final Batch Loss: 0.11615785211324692\n",
      "Epoch 4242, Loss: 0.2614290565252304, Final Batch Loss: 0.14438040554523468\n",
      "Epoch 4243, Loss: 0.2638642489910126, Final Batch Loss: 0.12780612707138062\n",
      "Epoch 4244, Loss: 0.23648589104413986, Final Batch Loss: 0.09146467596292496\n",
      "Epoch 4245, Loss: 0.261210098862648, Final Batch Loss: 0.1414923220872879\n",
      "Epoch 4246, Loss: 0.25885073840618134, Final Batch Loss: 0.10850086808204651\n",
      "Epoch 4247, Loss: 0.2214323952794075, Final Batch Loss: 0.1127815917134285\n",
      "Epoch 4248, Loss: 0.2537035718560219, Final Batch Loss: 0.11926940828561783\n",
      "Epoch 4249, Loss: 0.2778901904821396, Final Batch Loss: 0.1356082558631897\n",
      "Epoch 4250, Loss: 0.26037830114364624, Final Batch Loss: 0.17265652120113373\n",
      "Epoch 4251, Loss: 0.25133975595235825, Final Batch Loss: 0.13546697795391083\n",
      "Epoch 4252, Loss: 0.2723560780286789, Final Batch Loss: 0.1437513828277588\n",
      "Epoch 4253, Loss: 0.2742003947496414, Final Batch Loss: 0.1173630803823471\n",
      "Epoch 4254, Loss: 0.29895447194576263, Final Batch Loss: 0.1522606462240219\n",
      "Epoch 4255, Loss: 0.2920398414134979, Final Batch Loss: 0.14517426490783691\n",
      "Epoch 4256, Loss: 0.26948100328445435, Final Batch Loss: 0.1288485825061798\n",
      "Epoch 4257, Loss: 0.3111316114664078, Final Batch Loss: 0.15592943131923676\n",
      "Epoch 4258, Loss: 0.36696237325668335, Final Batch Loss: 0.1626022607088089\n",
      "Epoch 4259, Loss: 0.30047596991062164, Final Batch Loss: 0.14740267395973206\n",
      "Epoch 4260, Loss: 0.29592905938625336, Final Batch Loss: 0.13463781774044037\n",
      "Epoch 4261, Loss: 0.2779780477285385, Final Batch Loss: 0.12281090021133423\n",
      "Epoch 4262, Loss: 0.30091215670108795, Final Batch Loss: 0.11985297501087189\n",
      "Epoch 4263, Loss: 0.29224463552236557, Final Batch Loss: 0.16963043808937073\n",
      "Epoch 4264, Loss: 0.27641263604164124, Final Batch Loss: 0.14528368413448334\n",
      "Epoch 4265, Loss: 0.26113981753587723, Final Batch Loss: 0.14514189958572388\n",
      "Epoch 4266, Loss: 0.2799008712172508, Final Batch Loss: 0.12276632338762283\n",
      "Epoch 4267, Loss: 0.29827553033828735, Final Batch Loss: 0.16998562216758728\n",
      "Epoch 4268, Loss: 0.2433268204331398, Final Batch Loss: 0.1533348113298416\n",
      "Epoch 4269, Loss: 0.22000563889741898, Final Batch Loss: 0.11249493062496185\n",
      "Epoch 4270, Loss: 0.37975770235061646, Final Batch Loss: 0.17113113403320312\n",
      "Epoch 4271, Loss: 0.29143422096967697, Final Batch Loss: 0.17277908325195312\n",
      "Epoch 4272, Loss: 0.24783917516469955, Final Batch Loss: 0.10910574346780777\n",
      "Epoch 4273, Loss: 0.2539753168821335, Final Batch Loss: 0.14848405122756958\n",
      "Epoch 4274, Loss: 0.21305155009031296, Final Batch Loss: 0.084416963160038\n",
      "Epoch 4275, Loss: 0.2790079265832901, Final Batch Loss: 0.16297200322151184\n",
      "Epoch 4276, Loss: 0.3233433812856674, Final Batch Loss: 0.14413544535636902\n",
      "Epoch 4277, Loss: 0.28358569741249084, Final Batch Loss: 0.1542554348707199\n",
      "Epoch 4278, Loss: 0.27810336649417877, Final Batch Loss: 0.14665432274341583\n",
      "Epoch 4279, Loss: 0.2818150371313095, Final Batch Loss: 0.13931888341903687\n",
      "Epoch 4280, Loss: 0.24908410757780075, Final Batch Loss: 0.11603165417909622\n",
      "Epoch 4281, Loss: 0.2553071677684784, Final Batch Loss: 0.13025598227977753\n",
      "Epoch 4282, Loss: 0.2652255669236183, Final Batch Loss: 0.11091197282075882\n",
      "Epoch 4283, Loss: 0.3127894252538681, Final Batch Loss: 0.17266935110092163\n",
      "Epoch 4284, Loss: 0.31814901530742645, Final Batch Loss: 0.14245305955410004\n",
      "Epoch 4285, Loss: 0.2618274390697479, Final Batch Loss: 0.13526718318462372\n",
      "Epoch 4286, Loss: 0.24050568789243698, Final Batch Loss: 0.10770963877439499\n",
      "Epoch 4287, Loss: 0.22585226595401764, Final Batch Loss: 0.08690039813518524\n",
      "Epoch 4288, Loss: 0.2659130394458771, Final Batch Loss: 0.13055242598056793\n",
      "Epoch 4289, Loss: 0.27314966917037964, Final Batch Loss: 0.14880725741386414\n",
      "Epoch 4290, Loss: 0.30266422033309937, Final Batch Loss: 0.18337728083133698\n",
      "Epoch 4291, Loss: 0.22526997327804565, Final Batch Loss: 0.10550188273191452\n",
      "Epoch 4292, Loss: 0.2580985352396965, Final Batch Loss: 0.10350847989320755\n",
      "Epoch 4293, Loss: 0.2537109851837158, Final Batch Loss: 0.14744062721729279\n",
      "Epoch 4294, Loss: 0.2583444267511368, Final Batch Loss: 0.09857858717441559\n",
      "Epoch 4295, Loss: 0.2800392284989357, Final Batch Loss: 0.1562100499868393\n",
      "Epoch 4296, Loss: 0.2321215569972992, Final Batch Loss: 0.10113838315010071\n",
      "Epoch 4297, Loss: 0.28840966522693634, Final Batch Loss: 0.12064215540885925\n",
      "Epoch 4298, Loss: 0.28063810616731644, Final Batch Loss: 0.18184281885623932\n",
      "Epoch 4299, Loss: 0.24985849112272263, Final Batch Loss: 0.15635286271572113\n",
      "Epoch 4300, Loss: 0.24726948887109756, Final Batch Loss: 0.1501043438911438\n",
      "Epoch 4301, Loss: 0.30663081258535385, Final Batch Loss: 0.2123592495918274\n",
      "Epoch 4302, Loss: 0.27463750541210175, Final Batch Loss: 0.1693589687347412\n",
      "Epoch 4303, Loss: 0.22924232482910156, Final Batch Loss: 0.1295798122882843\n",
      "Epoch 4304, Loss: 0.24167171120643616, Final Batch Loss: 0.12331672757863998\n",
      "Epoch 4305, Loss: 0.2145819514989853, Final Batch Loss: 0.10902691632509232\n",
      "Epoch 4306, Loss: 0.2529425695538521, Final Batch Loss: 0.13485197722911835\n",
      "Epoch 4307, Loss: 0.3103494793176651, Final Batch Loss: 0.14704564213752747\n",
      "Epoch 4308, Loss: 0.24786833673715591, Final Batch Loss: 0.09405725449323654\n",
      "Epoch 4309, Loss: 0.2562938332557678, Final Batch Loss: 0.13189561665058136\n",
      "Epoch 4310, Loss: 0.2747224345803261, Final Batch Loss: 0.15380041301250458\n",
      "Epoch 4311, Loss: 0.21930234879255295, Final Batch Loss: 0.15876683592796326\n",
      "Epoch 4312, Loss: 0.22746817767620087, Final Batch Loss: 0.12014929205179214\n",
      "Epoch 4313, Loss: 0.21443058550357819, Final Batch Loss: 0.08167630434036255\n",
      "Epoch 4314, Loss: 0.2315661385655403, Final Batch Loss: 0.11448396742343903\n",
      "Epoch 4315, Loss: 0.2947585880756378, Final Batch Loss: 0.15376339852809906\n",
      "Epoch 4316, Loss: 0.26945093274116516, Final Batch Loss: 0.14855898916721344\n",
      "Epoch 4317, Loss: 0.28646207600831985, Final Batch Loss: 0.16683129966259003\n",
      "Epoch 4318, Loss: 0.22752408683300018, Final Batch Loss: 0.1248844712972641\n",
      "Epoch 4319, Loss: 0.2461075261235237, Final Batch Loss: 0.1293422281742096\n",
      "Epoch 4320, Loss: 0.2860591858625412, Final Batch Loss: 0.10373672842979431\n",
      "Epoch 4321, Loss: 0.26184943318367004, Final Batch Loss: 0.13027243316173553\n",
      "Epoch 4322, Loss: 0.33876919746398926, Final Batch Loss: 0.1484675258398056\n",
      "Epoch 4323, Loss: 0.23578768223524094, Final Batch Loss: 0.09382820874452591\n",
      "Epoch 4324, Loss: 0.3832812011241913, Final Batch Loss: 0.16562259197235107\n",
      "Epoch 4325, Loss: 0.22307558357715607, Final Batch Loss: 0.09972924739122391\n",
      "Epoch 4326, Loss: 0.2759401202201843, Final Batch Loss: 0.14569209516048431\n",
      "Epoch 4327, Loss: 0.24054190516471863, Final Batch Loss: 0.132095605134964\n",
      "Epoch 4328, Loss: 0.34464170038700104, Final Batch Loss: 0.21383194625377655\n",
      "Epoch 4329, Loss: 0.30599458515644073, Final Batch Loss: 0.1633470207452774\n",
      "Epoch 4330, Loss: 0.29782330989837646, Final Batch Loss: 0.15510545670986176\n",
      "Epoch 4331, Loss: 0.28076091408729553, Final Batch Loss: 0.12499593198299408\n",
      "Epoch 4332, Loss: 0.34624773263931274, Final Batch Loss: 0.15232935547828674\n",
      "Epoch 4333, Loss: 0.231924407184124, Final Batch Loss: 0.09019655734300613\n",
      "Epoch 4334, Loss: 0.21928084641695023, Final Batch Loss: 0.10123036801815033\n",
      "Epoch 4335, Loss: 0.28489941358566284, Final Batch Loss: 0.14394408464431763\n",
      "Epoch 4336, Loss: 0.2575419843196869, Final Batch Loss: 0.11366410553455353\n",
      "Epoch 4337, Loss: 0.2789228558540344, Final Batch Loss: 0.14922641217708588\n",
      "Epoch 4338, Loss: 0.23924520611763, Final Batch Loss: 0.14001482725143433\n",
      "Epoch 4339, Loss: 0.2597409337759018, Final Batch Loss: 0.13911880552768707\n",
      "Epoch 4340, Loss: 0.2808874845504761, Final Batch Loss: 0.16674408316612244\n",
      "Epoch 4341, Loss: 0.2592310830950737, Final Batch Loss: 0.15638166666030884\n",
      "Epoch 4342, Loss: 0.20543785393238068, Final Batch Loss: 0.09887205809354782\n",
      "Epoch 4343, Loss: 0.25489285588264465, Final Batch Loss: 0.13193605840206146\n",
      "Epoch 4344, Loss: 0.2288191020488739, Final Batch Loss: 0.09542499482631683\n",
      "Epoch 4345, Loss: 0.35105961561203003, Final Batch Loss: 0.11934620141983032\n",
      "Epoch 4346, Loss: 0.2620600238442421, Final Batch Loss: 0.1444578766822815\n",
      "Epoch 4347, Loss: 0.32118284702301025, Final Batch Loss: 0.13501739501953125\n",
      "Epoch 4348, Loss: 0.24296965450048447, Final Batch Loss: 0.1295997053384781\n",
      "Epoch 4349, Loss: 0.21086740493774414, Final Batch Loss: 0.10989391803741455\n",
      "Epoch 4350, Loss: 0.2845965027809143, Final Batch Loss: 0.14897656440734863\n",
      "Epoch 4351, Loss: 0.28278520703315735, Final Batch Loss: 0.1341995894908905\n",
      "Epoch 4352, Loss: 0.27066630125045776, Final Batch Loss: 0.1517333835363388\n",
      "Epoch 4353, Loss: 0.24565216898918152, Final Batch Loss: 0.11401332914829254\n",
      "Epoch 4354, Loss: 0.3077996224164963, Final Batch Loss: 0.16263756155967712\n",
      "Epoch 4355, Loss: 0.2345326691865921, Final Batch Loss: 0.16018910706043243\n",
      "Epoch 4356, Loss: 0.25942474603652954, Final Batch Loss: 0.1307077705860138\n",
      "Epoch 4357, Loss: 0.2597009465098381, Final Batch Loss: 0.15115030109882355\n",
      "Epoch 4358, Loss: 0.30374132096767426, Final Batch Loss: 0.15064553916454315\n",
      "Epoch 4359, Loss: 0.2793940603733063, Final Batch Loss: 0.1461292803287506\n",
      "Epoch 4360, Loss: 0.2799242362380028, Final Batch Loss: 0.12473411113023758\n",
      "Epoch 4361, Loss: 0.2412765920162201, Final Batch Loss: 0.1351204365491867\n",
      "Epoch 4362, Loss: 0.27949413657188416, Final Batch Loss: 0.12631602585315704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4363, Loss: 0.2644004449248314, Final Batch Loss: 0.14078685641288757\n",
      "Epoch 4364, Loss: 0.2568585276603699, Final Batch Loss: 0.10940596461296082\n",
      "Epoch 4365, Loss: 0.23999356478452682, Final Batch Loss: 0.130018949508667\n",
      "Epoch 4366, Loss: 0.2766202390193939, Final Batch Loss: 0.15139339864253998\n",
      "Epoch 4367, Loss: 0.22391971945762634, Final Batch Loss: 0.10574936121702194\n",
      "Epoch 4368, Loss: 0.294962540268898, Final Batch Loss: 0.10854077339172363\n",
      "Epoch 4369, Loss: 0.2694990485906601, Final Batch Loss: 0.1335059106349945\n",
      "Epoch 4370, Loss: 0.2584048956632614, Final Batch Loss: 0.07501140236854553\n",
      "Epoch 4371, Loss: 0.27124307304620743, Final Batch Loss: 0.11490433663129807\n",
      "Epoch 4372, Loss: 0.365776002407074, Final Batch Loss: 0.18527933955192566\n",
      "Epoch 4373, Loss: 0.2833031862974167, Final Batch Loss: 0.11941741406917572\n",
      "Epoch 4374, Loss: 0.25634876638650894, Final Batch Loss: 0.11048123985528946\n",
      "Epoch 4375, Loss: 0.24197863787412643, Final Batch Loss: 0.09353914111852646\n",
      "Epoch 4376, Loss: 0.24063507467508316, Final Batch Loss: 0.12343427538871765\n",
      "Epoch 4377, Loss: 0.2588697373867035, Final Batch Loss: 0.12623274326324463\n",
      "Epoch 4378, Loss: 0.27238988876342773, Final Batch Loss: 0.135111466050148\n",
      "Epoch 4379, Loss: 0.3235294967889786, Final Batch Loss: 0.15011057257652283\n",
      "Epoch 4380, Loss: 0.22294993698596954, Final Batch Loss: 0.09815257787704468\n",
      "Epoch 4381, Loss: 0.26741791516542435, Final Batch Loss: 0.11657809466123581\n",
      "Epoch 4382, Loss: 0.25259431451559067, Final Batch Loss: 0.1360820233821869\n",
      "Epoch 4383, Loss: 0.2597544267773628, Final Batch Loss: 0.14252091944217682\n",
      "Epoch 4384, Loss: 0.261379711329937, Final Batch Loss: 0.12024953216314316\n",
      "Epoch 4385, Loss: 0.3180574029684067, Final Batch Loss: 0.1876220554113388\n",
      "Epoch 4386, Loss: 0.23608096688985825, Final Batch Loss: 0.10996856540441513\n",
      "Epoch 4387, Loss: 0.27228041738271713, Final Batch Loss: 0.0951913520693779\n",
      "Epoch 4388, Loss: 0.2642843872308731, Final Batch Loss: 0.1747736781835556\n",
      "Epoch 4389, Loss: 0.24630732089281082, Final Batch Loss: 0.09927495568990707\n",
      "Epoch 4390, Loss: 0.24695241451263428, Final Batch Loss: 0.11715017259120941\n",
      "Epoch 4391, Loss: 0.28872590512037277, Final Batch Loss: 0.16469469666481018\n",
      "Epoch 4392, Loss: 0.2901647537946701, Final Batch Loss: 0.16657748818397522\n",
      "Epoch 4393, Loss: 0.304895281791687, Final Batch Loss: 0.14049667119979858\n",
      "Epoch 4394, Loss: 0.3692125529050827, Final Batch Loss: 0.20230425894260406\n",
      "Epoch 4395, Loss: 0.25678110122680664, Final Batch Loss: 0.14668476581573486\n",
      "Epoch 4396, Loss: 0.25824880599975586, Final Batch Loss: 0.15248863399028778\n",
      "Epoch 4397, Loss: 0.2526996433734894, Final Batch Loss: 0.12437129020690918\n",
      "Epoch 4398, Loss: 0.30762071907520294, Final Batch Loss: 0.16409097611904144\n",
      "Epoch 4399, Loss: 0.25806373357772827, Final Batch Loss: 0.1598225086927414\n",
      "Epoch 4400, Loss: 0.3947286605834961, Final Batch Loss: 0.22869649529457092\n",
      "Epoch 4401, Loss: 0.29238754510879517, Final Batch Loss: 0.16145509481430054\n",
      "Epoch 4402, Loss: 0.2573359087109566, Final Batch Loss: 0.15025211870670319\n",
      "Epoch 4403, Loss: 0.2532738596200943, Final Batch Loss: 0.1252819299697876\n",
      "Epoch 4404, Loss: 0.28632406890392303, Final Batch Loss: 0.16001097857952118\n",
      "Epoch 4405, Loss: 0.2931460365653038, Final Batch Loss: 0.1778852641582489\n",
      "Epoch 4406, Loss: 0.23263892531394958, Final Batch Loss: 0.11413140594959259\n",
      "Epoch 4407, Loss: 0.23802818357944489, Final Batch Loss: 0.11246949434280396\n",
      "Epoch 4408, Loss: 0.24440822005271912, Final Batch Loss: 0.1364128440618515\n",
      "Epoch 4409, Loss: 0.2627343386411667, Final Batch Loss: 0.151023268699646\n",
      "Epoch 4410, Loss: 0.3018294423818588, Final Batch Loss: 0.1323150098323822\n",
      "Epoch 4411, Loss: 0.30473703145980835, Final Batch Loss: 0.15574128925800323\n",
      "Epoch 4412, Loss: 0.28545013815164566, Final Batch Loss: 0.16246061027050018\n",
      "Epoch 4413, Loss: 0.28298255801200867, Final Batch Loss: 0.13139942288398743\n",
      "Epoch 4414, Loss: 0.23584317415952682, Final Batch Loss: 0.1106312945485115\n",
      "Epoch 4415, Loss: 0.298178032040596, Final Batch Loss: 0.15268464386463165\n",
      "Epoch 4416, Loss: 0.2560911476612091, Final Batch Loss: 0.13702185451984406\n",
      "Epoch 4417, Loss: 0.19438065588474274, Final Batch Loss: 0.10590124130249023\n",
      "Epoch 4418, Loss: 0.30123765766620636, Final Batch Loss: 0.1255849152803421\n",
      "Epoch 4419, Loss: 0.27037136256694794, Final Batch Loss: 0.10816898941993713\n",
      "Epoch 4420, Loss: 0.20821373164653778, Final Batch Loss: 0.08969445526599884\n",
      "Epoch 4421, Loss: 0.23881328105926514, Final Batch Loss: 0.13228188455104828\n",
      "Epoch 4422, Loss: 0.23855675756931305, Final Batch Loss: 0.1148383617401123\n",
      "Epoch 4423, Loss: 0.2679044231772423, Final Batch Loss: 0.10364145785570145\n",
      "Epoch 4424, Loss: 0.2265099361538887, Final Batch Loss: 0.11206686496734619\n",
      "Epoch 4425, Loss: 0.26449520140886307, Final Batch Loss: 0.15593533217906952\n",
      "Epoch 4426, Loss: 0.3065783679485321, Final Batch Loss: 0.14342455565929413\n",
      "Epoch 4427, Loss: 0.2513497695326805, Final Batch Loss: 0.1216990277171135\n",
      "Epoch 4428, Loss: 0.2697056233882904, Final Batch Loss: 0.13379351794719696\n",
      "Epoch 4429, Loss: 0.2639569714665413, Final Batch Loss: 0.1596575528383255\n",
      "Epoch 4430, Loss: 0.30676236748695374, Final Batch Loss: 0.14185316860675812\n",
      "Epoch 4431, Loss: 0.24775031208992004, Final Batch Loss: 0.12119030952453613\n",
      "Epoch 4432, Loss: 0.23269299417734146, Final Batch Loss: 0.13749554753303528\n",
      "Epoch 4433, Loss: 0.2422032728791237, Final Batch Loss: 0.11754446476697922\n",
      "Epoch 4434, Loss: 0.2821708247065544, Final Batch Loss: 0.1674005389213562\n",
      "Epoch 4435, Loss: 0.2466627135872841, Final Batch Loss: 0.12073006480932236\n",
      "Epoch 4436, Loss: 0.2788783237338066, Final Batch Loss: 0.15600250661373138\n",
      "Epoch 4437, Loss: 0.26283716410398483, Final Batch Loss: 0.14231422543525696\n",
      "Epoch 4438, Loss: 0.30179348587989807, Final Batch Loss: 0.15029554069042206\n",
      "Epoch 4439, Loss: 0.26665277034044266, Final Batch Loss: 0.14698131382465363\n",
      "Epoch 4440, Loss: 0.2534407079219818, Final Batch Loss: 0.14070671796798706\n",
      "Epoch 4441, Loss: 0.2326090857386589, Final Batch Loss: 0.1469644457101822\n",
      "Epoch 4442, Loss: 0.20430953800678253, Final Batch Loss: 0.1081545427441597\n",
      "Epoch 4443, Loss: 0.22875920683145523, Final Batch Loss: 0.1141437441110611\n",
      "Epoch 4444, Loss: 0.2716377004981041, Final Batch Loss: 0.15760239958763123\n",
      "Epoch 4445, Loss: 0.27503468841314316, Final Batch Loss: 0.11400453001260757\n",
      "Epoch 4446, Loss: 0.24662712216377258, Final Batch Loss: 0.14354808628559113\n",
      "Epoch 4447, Loss: 0.23902235180139542, Final Batch Loss: 0.13243024051189423\n",
      "Epoch 4448, Loss: 0.27710476517677307, Final Batch Loss: 0.17229843139648438\n",
      "Epoch 4449, Loss: 0.2626216262578964, Final Batch Loss: 0.1274895966053009\n",
      "Epoch 4450, Loss: 0.212294802069664, Final Batch Loss: 0.10319843888282776\n",
      "Epoch 4451, Loss: 0.33177603781223297, Final Batch Loss: 0.12841123342514038\n",
      "Epoch 4452, Loss: 0.20940012484788895, Final Batch Loss: 0.10373039543628693\n",
      "Epoch 4453, Loss: 0.21092645078897476, Final Batch Loss: 0.1085352897644043\n",
      "Epoch 4454, Loss: 0.26183905452489853, Final Batch Loss: 0.10502972453832626\n",
      "Epoch 4455, Loss: 0.25195374339818954, Final Batch Loss: 0.11972970515489578\n",
      "Epoch 4456, Loss: 0.2649382948875427, Final Batch Loss: 0.16821226477622986\n",
      "Epoch 4457, Loss: 0.26812461018562317, Final Batch Loss: 0.1326901763677597\n",
      "Epoch 4458, Loss: 0.267728790640831, Final Batch Loss: 0.13973581790924072\n",
      "Epoch 4459, Loss: 0.21404149383306503, Final Batch Loss: 0.11713457852602005\n",
      "Epoch 4460, Loss: 0.24990801513195038, Final Batch Loss: 0.12403412163257599\n",
      "Epoch 4461, Loss: 0.2639891281723976, Final Batch Loss: 0.16146063804626465\n",
      "Epoch 4462, Loss: 0.20796234905719757, Final Batch Loss: 0.08677602559328079\n",
      "Epoch 4463, Loss: 0.21387270092964172, Final Batch Loss: 0.09530135244131088\n",
      "Epoch 4464, Loss: 0.2629895657300949, Final Batch Loss: 0.12188191711902618\n",
      "Epoch 4465, Loss: 0.2507506385445595, Final Batch Loss: 0.12360524386167526\n",
      "Epoch 4466, Loss: 0.22659776359796524, Final Batch Loss: 0.11008299142122269\n",
      "Epoch 4467, Loss: 0.1977119818329811, Final Batch Loss: 0.09340666979551315\n",
      "Epoch 4468, Loss: 0.25518178194761276, Final Batch Loss: 0.11029829829931259\n",
      "Epoch 4469, Loss: 0.23437993228435516, Final Batch Loss: 0.12129843235015869\n",
      "Epoch 4470, Loss: 0.23009204864501953, Final Batch Loss: 0.12039408832788467\n",
      "Epoch 4471, Loss: 0.24100866168737411, Final Batch Loss: 0.12089963257312775\n",
      "Epoch 4472, Loss: 0.23953192681074142, Final Batch Loss: 0.15549281239509583\n",
      "Epoch 4473, Loss: 0.2699373736977577, Final Batch Loss: 0.10683280974626541\n",
      "Epoch 4474, Loss: 0.20680782943964005, Final Batch Loss: 0.09314456582069397\n",
      "Epoch 4475, Loss: 0.25535406917333603, Final Batch Loss: 0.10329239815473557\n",
      "Epoch 4476, Loss: 0.27834437042474747, Final Batch Loss: 0.15711882710456848\n",
      "Epoch 4477, Loss: 0.31623007357120514, Final Batch Loss: 0.13317745923995972\n",
      "Epoch 4478, Loss: 0.2439580336213112, Final Batch Loss: 0.10782233625650406\n",
      "Epoch 4479, Loss: 0.3015301823616028, Final Batch Loss: 0.15461251139640808\n",
      "Epoch 4480, Loss: 0.2505451366305351, Final Batch Loss: 0.12073101848363876\n",
      "Epoch 4481, Loss: 0.19159932434558868, Final Batch Loss: 0.07184120267629623\n",
      "Epoch 4482, Loss: 0.30844343453645706, Final Batch Loss: 0.20027318596839905\n",
      "Epoch 4483, Loss: 0.33346086740493774, Final Batch Loss: 0.13672524690628052\n",
      "Epoch 4484, Loss: 0.21453308314085007, Final Batch Loss: 0.10395899415016174\n",
      "Epoch 4485, Loss: 0.22703076899051666, Final Batch Loss: 0.08550471067428589\n",
      "Epoch 4486, Loss: 0.33772653341293335, Final Batch Loss: 0.18066783249378204\n",
      "Epoch 4487, Loss: 0.22038880735635757, Final Batch Loss: 0.09862102568149567\n",
      "Epoch 4488, Loss: 0.2528347671031952, Final Batch Loss: 0.14656466245651245\n",
      "Epoch 4489, Loss: 0.24152766168117523, Final Batch Loss: 0.14381708204746246\n",
      "Epoch 4490, Loss: 0.2186976671218872, Final Batch Loss: 0.08869533240795135\n",
      "Epoch 4491, Loss: 0.25881408154964447, Final Batch Loss: 0.1299317330121994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4492, Loss: 0.26691487431526184, Final Batch Loss: 0.1579217165708542\n",
      "Epoch 4493, Loss: 0.25695835798978806, Final Batch Loss: 0.10259883850812912\n",
      "Epoch 4494, Loss: 0.2525462731719017, Final Batch Loss: 0.14780956506729126\n",
      "Epoch 4495, Loss: 0.24839933961629868, Final Batch Loss: 0.13707910478115082\n",
      "Epoch 4496, Loss: 0.270177960395813, Final Batch Loss: 0.10742707550525665\n",
      "Epoch 4497, Loss: 0.24816831201314926, Final Batch Loss: 0.14438849687576294\n",
      "Epoch 4498, Loss: 0.20302674919366837, Final Batch Loss: 0.11275661736726761\n",
      "Epoch 4499, Loss: 0.2334495633840561, Final Batch Loss: 0.12861327826976776\n",
      "Epoch 4500, Loss: 0.2551723048090935, Final Batch Loss: 0.1312924474477768\n",
      "Epoch 4501, Loss: 0.2627154216170311, Final Batch Loss: 0.1484857052564621\n",
      "Epoch 4502, Loss: 0.26359523087739944, Final Batch Loss: 0.11172489076852798\n",
      "Epoch 4503, Loss: 0.23417264223098755, Final Batch Loss: 0.12858310341835022\n",
      "Epoch 4504, Loss: 0.22400157898664474, Final Batch Loss: 0.10580305010080338\n",
      "Epoch 4505, Loss: 0.23052267730236053, Final Batch Loss: 0.11030567437410355\n",
      "Epoch 4506, Loss: 0.2753940522670746, Final Batch Loss: 0.14092853665351868\n",
      "Epoch 4507, Loss: 0.2174902781844139, Final Batch Loss: 0.08875716477632523\n",
      "Epoch 4508, Loss: 0.24653314054012299, Final Batch Loss: 0.1632549911737442\n",
      "Epoch 4509, Loss: 0.3051866441965103, Final Batch Loss: 0.18977367877960205\n",
      "Epoch 4510, Loss: 0.21139465272426605, Final Batch Loss: 0.10089273750782013\n",
      "Epoch 4511, Loss: 0.22544147074222565, Final Batch Loss: 0.11818888783454895\n",
      "Epoch 4512, Loss: 0.22571099549531937, Final Batch Loss: 0.10949636250734329\n",
      "Epoch 4513, Loss: 0.2278197556734085, Final Batch Loss: 0.1112019270658493\n",
      "Epoch 4514, Loss: 0.23988835513591766, Final Batch Loss: 0.11026974022388458\n",
      "Epoch 4515, Loss: 0.23617642372846603, Final Batch Loss: 0.12568475306034088\n",
      "Epoch 4516, Loss: 0.24464572966098785, Final Batch Loss: 0.10630425810813904\n",
      "Epoch 4517, Loss: 0.18261893838644028, Final Batch Loss: 0.09034452587366104\n",
      "Epoch 4518, Loss: 0.21295683085918427, Final Batch Loss: 0.1223115548491478\n",
      "Epoch 4519, Loss: 0.2565508782863617, Final Batch Loss: 0.14215539395809174\n",
      "Epoch 4520, Loss: 0.2832951694726944, Final Batch Loss: 0.1443777084350586\n",
      "Epoch 4521, Loss: 0.25990529358386993, Final Batch Loss: 0.11365517973899841\n",
      "Epoch 4522, Loss: 0.21663880348205566, Final Batch Loss: 0.11993350833654404\n",
      "Epoch 4523, Loss: 0.20272652432322502, Final Batch Loss: 0.056633006781339645\n",
      "Epoch 4524, Loss: 0.23922141641378403, Final Batch Loss: 0.1051207110285759\n",
      "Epoch 4525, Loss: 0.24295951426029205, Final Batch Loss: 0.12658798694610596\n",
      "Epoch 4526, Loss: 0.24031906574964523, Final Batch Loss: 0.11804817616939545\n",
      "Epoch 4527, Loss: 0.2460411861538887, Final Batch Loss: 0.13781480491161346\n",
      "Epoch 4528, Loss: 0.2510443925857544, Final Batch Loss: 0.10865861177444458\n",
      "Epoch 4529, Loss: 0.28277426958084106, Final Batch Loss: 0.14062120020389557\n",
      "Epoch 4530, Loss: 0.3145563006401062, Final Batch Loss: 0.1408083736896515\n",
      "Epoch 4531, Loss: 0.2475580871105194, Final Batch Loss: 0.12386211007833481\n",
      "Epoch 4532, Loss: 0.29516051709651947, Final Batch Loss: 0.14681163430213928\n",
      "Epoch 4533, Loss: 0.28285419940948486, Final Batch Loss: 0.16260546445846558\n",
      "Epoch 4534, Loss: 0.27681417763233185, Final Batch Loss: 0.17872221767902374\n",
      "Epoch 4535, Loss: 0.275781974196434, Final Batch Loss: 0.13718178868293762\n",
      "Epoch 4536, Loss: 0.2712778151035309, Final Batch Loss: 0.13052210211753845\n",
      "Epoch 4537, Loss: 0.24867190420627594, Final Batch Loss: 0.11709955334663391\n",
      "Epoch 4538, Loss: 0.22175930440425873, Final Batch Loss: 0.1080847978591919\n",
      "Epoch 4539, Loss: 0.2743314281105995, Final Batch Loss: 0.16499999165534973\n",
      "Epoch 4540, Loss: 0.2636459022760391, Final Batch Loss: 0.1317138522863388\n",
      "Epoch 4541, Loss: 0.21427496522665024, Final Batch Loss: 0.11185575276613235\n",
      "Epoch 4542, Loss: 0.29747694730758667, Final Batch Loss: 0.14824442565441132\n",
      "Epoch 4543, Loss: 0.2831927686929703, Final Batch Loss: 0.13321365416049957\n",
      "Epoch 4544, Loss: 0.2779892608523369, Final Batch Loss: 0.15941855311393738\n",
      "Epoch 4545, Loss: 0.2524234279990196, Final Batch Loss: 0.1278592199087143\n",
      "Epoch 4546, Loss: 0.2560335695743561, Final Batch Loss: 0.12654803693294525\n",
      "Epoch 4547, Loss: 0.3100511133670807, Final Batch Loss: 0.1829611361026764\n",
      "Epoch 4548, Loss: 0.2402205690741539, Final Batch Loss: 0.11863068491220474\n",
      "Epoch 4549, Loss: 0.2258036509156227, Final Batch Loss: 0.11695858836174011\n",
      "Epoch 4550, Loss: 0.24333563446998596, Final Batch Loss: 0.1295124590396881\n",
      "Epoch 4551, Loss: 0.25143568217754364, Final Batch Loss: 0.13115505874156952\n",
      "Epoch 4552, Loss: 0.21541918814182281, Final Batch Loss: 0.12228021025657654\n",
      "Epoch 4553, Loss: 0.22939670830965042, Final Batch Loss: 0.10201243311166763\n",
      "Epoch 4554, Loss: 0.2985066622495651, Final Batch Loss: 0.11123085021972656\n",
      "Epoch 4555, Loss: 0.26188062131404877, Final Batch Loss: 0.13609068095684052\n",
      "Epoch 4556, Loss: 0.25337932258844376, Final Batch Loss: 0.1453380435705185\n",
      "Epoch 4557, Loss: 0.2747078388929367, Final Batch Loss: 0.1057756096124649\n",
      "Epoch 4558, Loss: 0.22904162853956223, Final Batch Loss: 0.11395253241062164\n",
      "Epoch 4559, Loss: 0.23580799251794815, Final Batch Loss: 0.1276513636112213\n",
      "Epoch 4560, Loss: 0.2811131328344345, Final Batch Loss: 0.1068325936794281\n",
      "Epoch 4561, Loss: 0.2648848816752434, Final Batch Loss: 0.15225932002067566\n",
      "Epoch 4562, Loss: 0.2674170136451721, Final Batch Loss: 0.15253858268260956\n",
      "Epoch 4563, Loss: 0.2521809861063957, Final Batch Loss: 0.10536041110754013\n",
      "Epoch 4564, Loss: 0.267930768430233, Final Batch Loss: 0.15349839627742767\n",
      "Epoch 4565, Loss: 0.20807477831840515, Final Batch Loss: 0.11415489763021469\n",
      "Epoch 4566, Loss: 0.2299540787935257, Final Batch Loss: 0.11603078991174698\n",
      "Epoch 4567, Loss: 0.21261192858219147, Final Batch Loss: 0.11039220541715622\n",
      "Epoch 4568, Loss: 0.2973650395870209, Final Batch Loss: 0.12961043417453766\n",
      "Epoch 4569, Loss: 0.28223133832216263, Final Batch Loss: 0.158815398812294\n",
      "Epoch 4570, Loss: 0.2340729832649231, Final Batch Loss: 0.11207093298435211\n",
      "Epoch 4571, Loss: 0.2308051586151123, Final Batch Loss: 0.13611344993114471\n",
      "Epoch 4572, Loss: 0.2836514115333557, Final Batch Loss: 0.12827454507350922\n",
      "Epoch 4573, Loss: 0.2843032553792, Final Batch Loss: 0.09000516682863235\n",
      "Epoch 4574, Loss: 0.2983311042189598, Final Batch Loss: 0.17655472457408905\n",
      "Epoch 4575, Loss: 0.3336638957262039, Final Batch Loss: 0.18763794004917145\n",
      "Epoch 4576, Loss: 0.2607002779841423, Final Batch Loss: 0.15472151339054108\n",
      "Epoch 4577, Loss: 0.25782310962677, Final Batch Loss: 0.09583723545074463\n",
      "Epoch 4578, Loss: 0.2656002789735794, Final Batch Loss: 0.1316225379705429\n",
      "Epoch 4579, Loss: 0.23337730020284653, Final Batch Loss: 0.10698216408491135\n",
      "Epoch 4580, Loss: 0.26029518991708755, Final Batch Loss: 0.09389450401067734\n",
      "Epoch 4581, Loss: 0.2584914043545723, Final Batch Loss: 0.16601312160491943\n",
      "Epoch 4582, Loss: 0.25300852209329605, Final Batch Loss: 0.12916094064712524\n",
      "Epoch 4583, Loss: 0.2970134988427162, Final Batch Loss: 0.09376179426908493\n",
      "Epoch 4584, Loss: 0.2283070906996727, Final Batch Loss: 0.1267855167388916\n",
      "Epoch 4585, Loss: 0.26878759264945984, Final Batch Loss: 0.13565631210803986\n",
      "Epoch 4586, Loss: 0.21574275940656662, Final Batch Loss: 0.08951135724782944\n",
      "Epoch 4587, Loss: 0.2739584743976593, Final Batch Loss: 0.1358223557472229\n",
      "Epoch 4588, Loss: 0.24948478490114212, Final Batch Loss: 0.1186329796910286\n",
      "Epoch 4589, Loss: 0.25684045255184174, Final Batch Loss: 0.11713521182537079\n",
      "Epoch 4590, Loss: 0.29545406997203827, Final Batch Loss: 0.11159311234951019\n",
      "Epoch 4591, Loss: 0.28598804771900177, Final Batch Loss: 0.14864519238471985\n",
      "Epoch 4592, Loss: 0.20626171678304672, Final Batch Loss: 0.10552966594696045\n",
      "Epoch 4593, Loss: 0.31186604499816895, Final Batch Loss: 0.19499936699867249\n",
      "Epoch 4594, Loss: 0.29082124680280685, Final Batch Loss: 0.16993513703346252\n",
      "Epoch 4595, Loss: 0.2960541993379593, Final Batch Loss: 0.13019250333309174\n",
      "Epoch 4596, Loss: 0.1968880146741867, Final Batch Loss: 0.11124223470687866\n",
      "Epoch 4597, Loss: 0.22074439376592636, Final Batch Loss: 0.11248753219842911\n",
      "Epoch 4598, Loss: 0.26238127052783966, Final Batch Loss: 0.12684103846549988\n",
      "Epoch 4599, Loss: 0.2390967234969139, Final Batch Loss: 0.09206611663103104\n",
      "Epoch 4600, Loss: 0.3289583548903465, Final Batch Loss: 0.12392366677522659\n",
      "Epoch 4601, Loss: 0.253681942820549, Final Batch Loss: 0.14584164321422577\n",
      "Epoch 4602, Loss: 0.2753574997186661, Final Batch Loss: 0.10891583561897278\n",
      "Epoch 4603, Loss: 0.24192439764738083, Final Batch Loss: 0.10875832289457321\n",
      "Epoch 4604, Loss: 0.262370765209198, Final Batch Loss: 0.13095006346702576\n",
      "Epoch 4605, Loss: 0.32612521946430206, Final Batch Loss: 0.20218737423419952\n",
      "Epoch 4606, Loss: 0.2665991485118866, Final Batch Loss: 0.14546209573745728\n",
      "Epoch 4607, Loss: 0.23073896765708923, Final Batch Loss: 0.11314865201711655\n",
      "Epoch 4608, Loss: 0.23529919236898422, Final Batch Loss: 0.11148612201213837\n",
      "Epoch 4609, Loss: 0.29777780175209045, Final Batch Loss: 0.1295185089111328\n",
      "Epoch 4610, Loss: 0.25777772068977356, Final Batch Loss: 0.12833546102046967\n",
      "Epoch 4611, Loss: 0.23314787447452545, Final Batch Loss: 0.12610599398612976\n",
      "Epoch 4612, Loss: 0.21549894660711288, Final Batch Loss: 0.0922970324754715\n",
      "Epoch 4613, Loss: 0.2243824377655983, Final Batch Loss: 0.11854001879692078\n",
      "Epoch 4614, Loss: 0.26071225106716156, Final Batch Loss: 0.1374596357345581\n",
      "Epoch 4615, Loss: 0.22154103964567184, Final Batch Loss: 0.08695926517248154\n",
      "Epoch 4616, Loss: 0.22581305354833603, Final Batch Loss: 0.11222072690725327\n",
      "Epoch 4617, Loss: 0.1995162069797516, Final Batch Loss: 0.09925352782011032\n",
      "Epoch 4618, Loss: 0.2553926855325699, Final Batch Loss: 0.09536780416965485\n",
      "Epoch 4619, Loss: 0.23889003694057465, Final Batch Loss: 0.08863990008831024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4620, Loss: 0.26130450516939163, Final Batch Loss: 0.14325366914272308\n",
      "Epoch 4621, Loss: 0.26717659085989, Final Batch Loss: 0.11905733495950699\n",
      "Epoch 4622, Loss: 0.2677677720785141, Final Batch Loss: 0.13607630133628845\n",
      "Epoch 4623, Loss: 0.27481282502412796, Final Batch Loss: 0.15628190338611603\n",
      "Epoch 4624, Loss: 0.22912582755088806, Final Batch Loss: 0.14277979731559753\n",
      "Epoch 4625, Loss: 0.23068930953741074, Final Batch Loss: 0.1096893772482872\n",
      "Epoch 4626, Loss: 0.27216773480176926, Final Batch Loss: 0.17172546684741974\n",
      "Epoch 4627, Loss: 0.18607056140899658, Final Batch Loss: 0.08690241724252701\n",
      "Epoch 4628, Loss: 0.2641955614089966, Final Batch Loss: 0.16254067420959473\n",
      "Epoch 4629, Loss: 0.2660805732011795, Final Batch Loss: 0.09813196957111359\n",
      "Epoch 4630, Loss: 0.21956531703472137, Final Batch Loss: 0.11818527430295944\n",
      "Epoch 4631, Loss: 0.27598198503255844, Final Batch Loss: 0.1233542338013649\n",
      "Epoch 4632, Loss: 0.26472635567188263, Final Batch Loss: 0.1281430721282959\n",
      "Epoch 4633, Loss: 0.23326734453439713, Final Batch Loss: 0.12223303318023682\n",
      "Epoch 4634, Loss: 0.2675069645047188, Final Batch Loss: 0.15706510841846466\n",
      "Epoch 4635, Loss: 0.21007923781871796, Final Batch Loss: 0.10113517194986343\n",
      "Epoch 4636, Loss: 0.23667264729738235, Final Batch Loss: 0.13669830560684204\n",
      "Epoch 4637, Loss: 0.2528080716729164, Final Batch Loss: 0.1298326551914215\n",
      "Epoch 4638, Loss: 0.23639706522226334, Final Batch Loss: 0.13516195118427277\n",
      "Epoch 4639, Loss: 0.3061961233615875, Final Batch Loss: 0.17539864778518677\n",
      "Epoch 4640, Loss: 0.25140734016895294, Final Batch Loss: 0.12224572896957397\n",
      "Epoch 4641, Loss: 0.2713426500558853, Final Batch Loss: 0.16089394688606262\n",
      "Epoch 4642, Loss: 0.2178288847208023, Final Batch Loss: 0.10876841098070145\n",
      "Epoch 4643, Loss: 0.22246655076742172, Final Batch Loss: 0.08256595581769943\n",
      "Epoch 4644, Loss: 0.3291240558028221, Final Batch Loss: 0.23772269487380981\n",
      "Epoch 4645, Loss: 0.2947287857532501, Final Batch Loss: 0.15055562555789948\n",
      "Epoch 4646, Loss: 0.22311074286699295, Final Batch Loss: 0.11416999250650406\n",
      "Epoch 4647, Loss: 0.2689351812005043, Final Batch Loss: 0.14976198971271515\n",
      "Epoch 4648, Loss: 0.2267373651266098, Final Batch Loss: 0.1386457085609436\n",
      "Epoch 4649, Loss: 0.2422938346862793, Final Batch Loss: 0.11406178772449493\n",
      "Epoch 4650, Loss: 0.25237634778022766, Final Batch Loss: 0.11118236184120178\n",
      "Epoch 4651, Loss: 0.2690250426530838, Final Batch Loss: 0.12384460866451263\n",
      "Epoch 4652, Loss: 0.33021974563598633, Final Batch Loss: 0.18815875053405762\n",
      "Epoch 4653, Loss: 0.2596730887889862, Final Batch Loss: 0.15161482989788055\n",
      "Epoch 4654, Loss: 0.26913226395845413, Final Batch Loss: 0.1622479408979416\n",
      "Epoch 4655, Loss: 0.27903537452220917, Final Batch Loss: 0.16231948137283325\n",
      "Epoch 4656, Loss: 0.24513787776231766, Final Batch Loss: 0.09348573535680771\n",
      "Epoch 4657, Loss: 0.2445371374487877, Final Batch Loss: 0.11870384961366653\n",
      "Epoch 4658, Loss: 0.22595543414354324, Final Batch Loss: 0.13218912482261658\n",
      "Epoch 4659, Loss: 0.20891128480434418, Final Batch Loss: 0.08743671327829361\n",
      "Epoch 4660, Loss: 0.23377933353185654, Final Batch Loss: 0.11985760927200317\n",
      "Epoch 4661, Loss: 0.2482088953256607, Final Batch Loss: 0.13614824414253235\n",
      "Epoch 4662, Loss: 0.2866952419281006, Final Batch Loss: 0.18101799488067627\n",
      "Epoch 4663, Loss: 0.24832987040281296, Final Batch Loss: 0.10349909216165543\n",
      "Epoch 4664, Loss: 0.2573639377951622, Final Batch Loss: 0.15832780301570892\n",
      "Epoch 4665, Loss: 0.2881922200322151, Final Batch Loss: 0.11077501624822617\n",
      "Epoch 4666, Loss: 0.3034719377756119, Final Batch Loss: 0.15568575263023376\n",
      "Epoch 4667, Loss: 0.2443605214357376, Final Batch Loss: 0.12605975568294525\n",
      "Epoch 4668, Loss: 0.24212905764579773, Final Batch Loss: 0.10266926884651184\n",
      "Epoch 4669, Loss: 0.23904160410165787, Final Batch Loss: 0.1091219112277031\n",
      "Epoch 4670, Loss: 0.2847854048013687, Final Batch Loss: 0.13868221640586853\n",
      "Epoch 4671, Loss: 0.27012887597084045, Final Batch Loss: 0.11518658697605133\n",
      "Epoch 4672, Loss: 0.23259595781564713, Final Batch Loss: 0.126079261302948\n",
      "Epoch 4673, Loss: 0.23232611268758774, Final Batch Loss: 0.11639939993619919\n",
      "Epoch 4674, Loss: 0.22347155213356018, Final Batch Loss: 0.12181490659713745\n",
      "Epoch 4675, Loss: 0.24092372506856918, Final Batch Loss: 0.10947228223085403\n",
      "Epoch 4676, Loss: 0.2282445728778839, Final Batch Loss: 0.09647320210933685\n",
      "Epoch 4677, Loss: 0.21824757009744644, Final Batch Loss: 0.12836989760398865\n",
      "Epoch 4678, Loss: 0.21727974712848663, Final Batch Loss: 0.10199978947639465\n",
      "Epoch 4679, Loss: 0.278334841132164, Final Batch Loss: 0.12787890434265137\n",
      "Epoch 4680, Loss: 0.23785033077001572, Final Batch Loss: 0.09928777068853378\n",
      "Epoch 4681, Loss: 0.29330798983573914, Final Batch Loss: 0.13722096383571625\n",
      "Epoch 4682, Loss: 0.234647735953331, Final Batch Loss: 0.09496571123600006\n",
      "Epoch 4683, Loss: 0.2526778057217598, Final Batch Loss: 0.1124366894364357\n",
      "Epoch 4684, Loss: 0.26580725610256195, Final Batch Loss: 0.13755373656749725\n",
      "Epoch 4685, Loss: 0.2703873813152313, Final Batch Loss: 0.09376651048660278\n",
      "Epoch 4686, Loss: 0.24007432907819748, Final Batch Loss: 0.12863829731941223\n",
      "Epoch 4687, Loss: 0.21217571198940277, Final Batch Loss: 0.09894461929798126\n",
      "Epoch 4688, Loss: 0.24612031877040863, Final Batch Loss: 0.10251973569393158\n",
      "Epoch 4689, Loss: 0.2385203018784523, Final Batch Loss: 0.09593363851308823\n",
      "Epoch 4690, Loss: 0.20231914520263672, Final Batch Loss: 0.12298406660556793\n",
      "Epoch 4691, Loss: 0.2954232543706894, Final Batch Loss: 0.15190595388412476\n",
      "Epoch 4692, Loss: 0.22796890139579773, Final Batch Loss: 0.11047026515007019\n",
      "Epoch 4693, Loss: 0.27105043083429337, Final Batch Loss: 0.1672930121421814\n",
      "Epoch 4694, Loss: 0.2235734462738037, Final Batch Loss: 0.089027538895607\n",
      "Epoch 4695, Loss: 0.2664010301232338, Final Batch Loss: 0.06890716403722763\n",
      "Epoch 4696, Loss: 0.22847697883844376, Final Batch Loss: 0.08762481063604355\n",
      "Epoch 4697, Loss: 0.2712428644299507, Final Batch Loss: 0.1490771770477295\n",
      "Epoch 4698, Loss: 0.24160883575677872, Final Batch Loss: 0.11480583995580673\n",
      "Epoch 4699, Loss: 0.21312806010246277, Final Batch Loss: 0.13075169920921326\n",
      "Epoch 4700, Loss: 0.2308925837278366, Final Batch Loss: 0.14051996171474457\n",
      "Epoch 4701, Loss: 0.23857948929071426, Final Batch Loss: 0.15257102251052856\n",
      "Epoch 4702, Loss: 0.29714876413345337, Final Batch Loss: 0.15898574888706207\n",
      "Epoch 4703, Loss: 0.2390826791524887, Final Batch Loss: 0.13031114637851715\n",
      "Epoch 4704, Loss: 0.23679231107234955, Final Batch Loss: 0.14416787028312683\n",
      "Epoch 4705, Loss: 0.30473391711711884, Final Batch Loss: 0.10817143321037292\n",
      "Epoch 4706, Loss: 0.22454607486724854, Final Batch Loss: 0.08152276277542114\n",
      "Epoch 4707, Loss: 0.23346726596355438, Final Batch Loss: 0.12524361908435822\n",
      "Epoch 4708, Loss: 0.2186064049601555, Final Batch Loss: 0.06393048912286758\n",
      "Epoch 4709, Loss: 0.2209109142422676, Final Batch Loss: 0.09606168419122696\n",
      "Epoch 4710, Loss: 0.26089300215244293, Final Batch Loss: 0.15107528865337372\n",
      "Epoch 4711, Loss: 0.2457929104566574, Final Batch Loss: 0.1339375078678131\n",
      "Epoch 4712, Loss: 0.25718751549720764, Final Batch Loss: 0.14929839968681335\n",
      "Epoch 4713, Loss: 0.2827753722667694, Final Batch Loss: 0.13159537315368652\n",
      "Epoch 4714, Loss: 0.18787679821252823, Final Batch Loss: 0.08869413286447525\n",
      "Epoch 4715, Loss: 0.22588718682527542, Final Batch Loss: 0.13644550740718842\n",
      "Epoch 4716, Loss: 0.21952763199806213, Final Batch Loss: 0.0917297750711441\n",
      "Epoch 4717, Loss: 0.21442469209432602, Final Batch Loss: 0.10385049879550934\n",
      "Epoch 4718, Loss: 0.2860949710011482, Final Batch Loss: 0.19867852330207825\n",
      "Epoch 4719, Loss: 0.21563328802585602, Final Batch Loss: 0.11392530798912048\n",
      "Epoch 4720, Loss: 0.2530406787991524, Final Batch Loss: 0.09917379170656204\n",
      "Epoch 4721, Loss: 0.2372025027871132, Final Batch Loss: 0.11384924501180649\n",
      "Epoch 4722, Loss: 0.23716972768306732, Final Batch Loss: 0.11647415906190872\n",
      "Epoch 4723, Loss: 0.22098764777183533, Final Batch Loss: 0.10163062065839767\n",
      "Epoch 4724, Loss: 0.2021399363875389, Final Batch Loss: 0.1038346067070961\n",
      "Epoch 4725, Loss: 0.20957854390144348, Final Batch Loss: 0.09422097355127335\n",
      "Epoch 4726, Loss: 0.20942994952201843, Final Batch Loss: 0.11808216571807861\n",
      "Epoch 4727, Loss: 0.26925697177648544, Final Batch Loss: 0.10273557156324387\n",
      "Epoch 4728, Loss: 0.22945507615804672, Final Batch Loss: 0.0908130332827568\n",
      "Epoch 4729, Loss: 0.2111472636461258, Final Batch Loss: 0.07632584869861603\n",
      "Epoch 4730, Loss: 0.2127229943871498, Final Batch Loss: 0.09143603593111038\n",
      "Epoch 4731, Loss: 0.22796699404716492, Final Batch Loss: 0.08955265581607819\n",
      "Epoch 4732, Loss: 0.25462158769369125, Final Batch Loss: 0.13706795871257782\n",
      "Epoch 4733, Loss: 0.22140702605247498, Final Batch Loss: 0.11882318556308746\n",
      "Epoch 4734, Loss: 0.2548113390803337, Final Batch Loss: 0.1343553364276886\n",
      "Epoch 4735, Loss: 0.30418136715888977, Final Batch Loss: 0.11877529323101044\n",
      "Epoch 4736, Loss: 0.256006695330143, Final Batch Loss: 0.1420048177242279\n",
      "Epoch 4737, Loss: 0.23283708840608597, Final Batch Loss: 0.11705783754587173\n",
      "Epoch 4738, Loss: 0.234803706407547, Final Batch Loss: 0.10146912932395935\n",
      "Epoch 4739, Loss: 0.24026411026716232, Final Batch Loss: 0.11616524308919907\n",
      "Epoch 4740, Loss: 0.25535670667886734, Final Batch Loss: 0.1450730264186859\n",
      "Epoch 4741, Loss: 0.25669002532958984, Final Batch Loss: 0.11738850176334381\n",
      "Epoch 4742, Loss: 0.2560948058962822, Final Batch Loss: 0.12440129369497299\n",
      "Epoch 4743, Loss: 0.29543986171483994, Final Batch Loss: 0.18762712180614471\n",
      "Epoch 4744, Loss: 0.2373596727848053, Final Batch Loss: 0.1275443732738495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4745, Loss: 0.29780347645282745, Final Batch Loss: 0.1721975952386856\n",
      "Epoch 4746, Loss: 0.28279344737529755, Final Batch Loss: 0.1440158486366272\n",
      "Epoch 4747, Loss: 0.23226813226938248, Final Batch Loss: 0.09748228639364243\n",
      "Epoch 4748, Loss: 0.2142656370997429, Final Batch Loss: 0.09301290661096573\n",
      "Epoch 4749, Loss: 0.20414021611213684, Final Batch Loss: 0.11511198431253433\n",
      "Epoch 4750, Loss: 0.31166674196720123, Final Batch Loss: 0.16019044816493988\n",
      "Epoch 4751, Loss: 0.2231101542711258, Final Batch Loss: 0.10999378561973572\n",
      "Epoch 4752, Loss: 0.23045466095209122, Final Batch Loss: 0.14454886317253113\n",
      "Epoch 4753, Loss: 0.24986495077610016, Final Batch Loss: 0.14386174082756042\n",
      "Epoch 4754, Loss: 0.23263638466596603, Final Batch Loss: 0.13943159580230713\n",
      "Epoch 4755, Loss: 0.22811077535152435, Final Batch Loss: 0.11035322397947311\n",
      "Epoch 4756, Loss: 0.20254459977149963, Final Batch Loss: 0.09834203869104385\n",
      "Epoch 4757, Loss: 0.2524235099554062, Final Batch Loss: 0.10390472412109375\n",
      "Epoch 4758, Loss: 0.2783791869878769, Final Batch Loss: 0.17789258062839508\n",
      "Epoch 4759, Loss: 0.23685501515865326, Final Batch Loss: 0.12483133375644684\n",
      "Epoch 4760, Loss: 0.2340434417128563, Final Batch Loss: 0.10884752124547958\n",
      "Epoch 4761, Loss: 0.2719750851392746, Final Batch Loss: 0.14666865766048431\n",
      "Epoch 4762, Loss: 0.24776457995176315, Final Batch Loss: 0.09310146421194077\n",
      "Epoch 4763, Loss: 0.2809882313013077, Final Batch Loss: 0.14830981194972992\n",
      "Epoch 4764, Loss: 0.27831877768039703, Final Batch Loss: 0.08705280721187592\n",
      "Epoch 4765, Loss: 0.2430945336818695, Final Batch Loss: 0.13026921451091766\n",
      "Epoch 4766, Loss: 0.28631091862916946, Final Batch Loss: 0.16848058998584747\n",
      "Epoch 4767, Loss: 0.2177957221865654, Final Batch Loss: 0.10197653621435165\n",
      "Epoch 4768, Loss: 0.2725609391927719, Final Batch Loss: 0.15013781189918518\n",
      "Epoch 4769, Loss: 0.23076993972063065, Final Batch Loss: 0.13205763697624207\n",
      "Epoch 4770, Loss: 0.2740035057067871, Final Batch Loss: 0.15566906332969666\n",
      "Epoch 4771, Loss: 0.2525039538741112, Final Batch Loss: 0.10525762289762497\n",
      "Epoch 4772, Loss: 0.22616443037986755, Final Batch Loss: 0.10543039441108704\n",
      "Epoch 4773, Loss: 0.24785873293876648, Final Batch Loss: 0.14598219096660614\n",
      "Epoch 4774, Loss: 0.31145113706588745, Final Batch Loss: 0.1709960699081421\n",
      "Epoch 4775, Loss: 0.19128333777189255, Final Batch Loss: 0.07952813059091568\n",
      "Epoch 4776, Loss: 0.24341513216495514, Final Batch Loss: 0.11232608556747437\n",
      "Epoch 4777, Loss: 0.24875043332576752, Final Batch Loss: 0.1193239837884903\n",
      "Epoch 4778, Loss: 0.23629572242498398, Final Batch Loss: 0.09778711944818497\n",
      "Epoch 4779, Loss: 0.2743739187717438, Final Batch Loss: 0.1280709058046341\n",
      "Epoch 4780, Loss: 0.25188547372817993, Final Batch Loss: 0.12242139875888824\n",
      "Epoch 4781, Loss: 0.22236767411231995, Final Batch Loss: 0.08873164653778076\n",
      "Epoch 4782, Loss: 0.25306084752082825, Final Batch Loss: 0.1192215234041214\n",
      "Epoch 4783, Loss: 0.22005821019411087, Final Batch Loss: 0.11814791709184647\n",
      "Epoch 4784, Loss: 0.2444903701543808, Final Batch Loss: 0.12779606878757477\n",
      "Epoch 4785, Loss: 0.23803824186325073, Final Batch Loss: 0.09330561757087708\n",
      "Epoch 4786, Loss: 0.24788223206996918, Final Batch Loss: 0.14179156720638275\n",
      "Epoch 4787, Loss: 0.23221513628959656, Final Batch Loss: 0.11511864513158798\n",
      "Epoch 4788, Loss: 0.2029842808842659, Final Batch Loss: 0.12311156094074249\n",
      "Epoch 4789, Loss: 0.24093860387802124, Final Batch Loss: 0.09826639294624329\n",
      "Epoch 4790, Loss: 0.27846525609493256, Final Batch Loss: 0.13611234724521637\n",
      "Epoch 4791, Loss: 0.23983024060726166, Final Batch Loss: 0.15143395960330963\n",
      "Epoch 4792, Loss: 0.24123510718345642, Final Batch Loss: 0.12706248462200165\n",
      "Epoch 4793, Loss: 0.23647452890872955, Final Batch Loss: 0.09743337333202362\n",
      "Epoch 4794, Loss: 0.27175716310739517, Final Batch Loss: 0.11513072997331619\n",
      "Epoch 4795, Loss: 0.20641359686851501, Final Batch Loss: 0.0903264731168747\n",
      "Epoch 4796, Loss: 0.22709456086158752, Final Batch Loss: 0.11431360989809036\n",
      "Epoch 4797, Loss: 0.26305927336215973, Final Batch Loss: 0.14591358602046967\n",
      "Epoch 4798, Loss: 0.21373295038938522, Final Batch Loss: 0.0884796753525734\n",
      "Epoch 4799, Loss: 0.2406507283449173, Final Batch Loss: 0.09512655436992645\n",
      "Epoch 4800, Loss: 0.22263050824403763, Final Batch Loss: 0.10530596226453781\n",
      "Epoch 4801, Loss: 0.22337139397859573, Final Batch Loss: 0.07936234027147293\n",
      "Epoch 4802, Loss: 0.223789282143116, Final Batch Loss: 0.102478988468647\n",
      "Epoch 4803, Loss: 0.21617884933948517, Final Batch Loss: 0.09339916706085205\n",
      "Epoch 4804, Loss: 0.22790556401014328, Final Batch Loss: 0.11924262344837189\n",
      "Epoch 4805, Loss: 0.27543337643146515, Final Batch Loss: 0.1464749574661255\n",
      "Epoch 4806, Loss: 0.22340382635593414, Final Batch Loss: 0.1041475459933281\n",
      "Epoch 4807, Loss: 0.20303083211183548, Final Batch Loss: 0.10214199125766754\n",
      "Epoch 4808, Loss: 0.2456430196762085, Final Batch Loss: 0.11596938967704773\n",
      "Epoch 4809, Loss: 0.2895081341266632, Final Batch Loss: 0.12774775922298431\n",
      "Epoch 4810, Loss: 0.23905041068792343, Final Batch Loss: 0.12426184862852097\n",
      "Epoch 4811, Loss: 0.26892878860235214, Final Batch Loss: 0.17376092076301575\n",
      "Epoch 4812, Loss: 0.26512616872787476, Final Batch Loss: 0.12693136930465698\n",
      "Epoch 4813, Loss: 0.18271002918481827, Final Batch Loss: 0.08882465958595276\n",
      "Epoch 4814, Loss: 0.28883885592222214, Final Batch Loss: 0.18963982164859772\n",
      "Epoch 4815, Loss: 0.2047530561685562, Final Batch Loss: 0.10258836299180984\n",
      "Epoch 4816, Loss: 0.23283474147319794, Final Batch Loss: 0.08593334257602692\n",
      "Epoch 4817, Loss: 0.2297532930970192, Final Batch Loss: 0.10852079093456268\n",
      "Epoch 4818, Loss: 0.21847014129161835, Final Batch Loss: 0.08732336759567261\n",
      "Epoch 4819, Loss: 0.25787353515625, Final Batch Loss: 0.13239578902721405\n",
      "Epoch 4820, Loss: 0.25116077065467834, Final Batch Loss: 0.11766655743122101\n",
      "Epoch 4821, Loss: 0.25817374885082245, Final Batch Loss: 0.12927494943141937\n",
      "Epoch 4822, Loss: 0.28206513822078705, Final Batch Loss: 0.08964723348617554\n",
      "Epoch 4823, Loss: 0.2371293231844902, Final Batch Loss: 0.12961187958717346\n",
      "Epoch 4824, Loss: 0.25716280192136765, Final Batch Loss: 0.14469347894191742\n",
      "Epoch 4825, Loss: 0.26160257309675217, Final Batch Loss: 0.16481710970401764\n",
      "Epoch 4826, Loss: 0.24711482226848602, Final Batch Loss: 0.13824476301670074\n",
      "Epoch 4827, Loss: 0.24030093103647232, Final Batch Loss: 0.12291025370359421\n",
      "Epoch 4828, Loss: 0.2117108404636383, Final Batch Loss: 0.09924200177192688\n",
      "Epoch 4829, Loss: 0.2738495171070099, Final Batch Loss: 0.14860057830810547\n",
      "Epoch 4830, Loss: 0.23018022626638412, Final Batch Loss: 0.09676540642976761\n",
      "Epoch 4831, Loss: 0.28020697832107544, Final Batch Loss: 0.13900649547576904\n",
      "Epoch 4832, Loss: 0.2582986429333687, Final Batch Loss: 0.14496882259845734\n",
      "Epoch 4833, Loss: 0.21064386516809464, Final Batch Loss: 0.12925918400287628\n",
      "Epoch 4834, Loss: 0.18975984305143356, Final Batch Loss: 0.07430966943502426\n",
      "Epoch 4835, Loss: 0.20362350344657898, Final Batch Loss: 0.09355605393648148\n",
      "Epoch 4836, Loss: 0.217265747487545, Final Batch Loss: 0.10121812671422958\n",
      "Epoch 4837, Loss: 0.2827203571796417, Final Batch Loss: 0.14845232665538788\n",
      "Epoch 4838, Loss: 0.2223120629787445, Final Batch Loss: 0.13274382054805756\n",
      "Epoch 4839, Loss: 0.23423974215984344, Final Batch Loss: 0.10504677891731262\n",
      "Epoch 4840, Loss: 0.2251097857952118, Final Batch Loss: 0.1262463927268982\n",
      "Epoch 4841, Loss: 0.22202541679143906, Final Batch Loss: 0.08561583608388901\n",
      "Epoch 4842, Loss: 0.24040623009204865, Final Batch Loss: 0.13694681227207184\n",
      "Epoch 4843, Loss: 0.2732851579785347, Final Batch Loss: 0.1534048467874527\n",
      "Epoch 4844, Loss: 0.23768477141857147, Final Batch Loss: 0.11813864856958389\n",
      "Epoch 4845, Loss: 0.20819055289030075, Final Batch Loss: 0.10978465527296066\n",
      "Epoch 4846, Loss: 0.2439672201871872, Final Batch Loss: 0.1553480178117752\n",
      "Epoch 4847, Loss: 0.21843138337135315, Final Batch Loss: 0.13758151233196259\n",
      "Epoch 4848, Loss: 0.23982911556959152, Final Batch Loss: 0.149298757314682\n",
      "Epoch 4849, Loss: 0.25049956142902374, Final Batch Loss: 0.13200756907463074\n",
      "Epoch 4850, Loss: 0.24994423985481262, Final Batch Loss: 0.11712150275707245\n",
      "Epoch 4851, Loss: 0.21095652878284454, Final Batch Loss: 0.08354364335536957\n",
      "Epoch 4852, Loss: 0.2796749696135521, Final Batch Loss: 0.17099392414093018\n",
      "Epoch 4853, Loss: 0.2943287640810013, Final Batch Loss: 0.13729621469974518\n",
      "Epoch 4854, Loss: 0.21853233873844147, Final Batch Loss: 0.11302509158849716\n",
      "Epoch 4855, Loss: 0.24585823714733124, Final Batch Loss: 0.14400814473628998\n",
      "Epoch 4856, Loss: 0.21300607919692993, Final Batch Loss: 0.09718006104230881\n",
      "Epoch 4857, Loss: 0.31002312898635864, Final Batch Loss: 0.1759280264377594\n",
      "Epoch 4858, Loss: 0.2091449648141861, Final Batch Loss: 0.11115612834692001\n",
      "Epoch 4859, Loss: 0.22397703677415848, Final Batch Loss: 0.10579829663038254\n",
      "Epoch 4860, Loss: 0.24257151037454605, Final Batch Loss: 0.12663204967975616\n",
      "Epoch 4861, Loss: 0.22463919967412949, Final Batch Loss: 0.11476536095142365\n",
      "Epoch 4862, Loss: 0.27070558816194534, Final Batch Loss: 0.10637185722589493\n",
      "Epoch 4863, Loss: 0.247859887778759, Final Batch Loss: 0.12996551394462585\n",
      "Epoch 4864, Loss: 0.2770709991455078, Final Batch Loss: 0.12828005850315094\n",
      "Epoch 4865, Loss: 0.1965332254767418, Final Batch Loss: 0.10299572348594666\n",
      "Epoch 4866, Loss: 0.22503114491701126, Final Batch Loss: 0.11354890465736389\n",
      "Epoch 4867, Loss: 0.2435639724135399, Final Batch Loss: 0.08208183199167252\n",
      "Epoch 4868, Loss: 0.22814064472913742, Final Batch Loss: 0.12608754634857178\n",
      "Epoch 4869, Loss: 0.22646799683570862, Final Batch Loss: 0.08840896189212799\n",
      "Epoch 4870, Loss: 0.2223432958126068, Final Batch Loss: 0.0940077155828476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4871, Loss: 0.28348373621702194, Final Batch Loss: 0.17529547214508057\n",
      "Epoch 4872, Loss: 0.2103041484951973, Final Batch Loss: 0.10014725476503372\n",
      "Epoch 4873, Loss: 0.24602264910936356, Final Batch Loss: 0.1293765902519226\n",
      "Epoch 4874, Loss: 0.21447765827178955, Final Batch Loss: 0.09731850028038025\n",
      "Epoch 4875, Loss: 0.1875864341855049, Final Batch Loss: 0.06942155957221985\n",
      "Epoch 4876, Loss: 0.22279765456914902, Final Batch Loss: 0.13438118994235992\n",
      "Epoch 4877, Loss: 0.24967603385448456, Final Batch Loss: 0.11691330373287201\n",
      "Epoch 4878, Loss: 0.2489570528268814, Final Batch Loss: 0.165549635887146\n",
      "Epoch 4879, Loss: 0.17011752724647522, Final Batch Loss: 0.08679133653640747\n",
      "Epoch 4880, Loss: 0.27698399126529694, Final Batch Loss: 0.1440456062555313\n",
      "Epoch 4881, Loss: 0.24322015047073364, Final Batch Loss: 0.11607219278812408\n",
      "Epoch 4882, Loss: 0.24388515204191208, Final Batch Loss: 0.156395822763443\n",
      "Epoch 4883, Loss: 0.22779599577188492, Final Batch Loss: 0.12780757248401642\n",
      "Epoch 4884, Loss: 0.231627956032753, Final Batch Loss: 0.12216772139072418\n",
      "Epoch 4885, Loss: 0.25100962817668915, Final Batch Loss: 0.12402842938899994\n",
      "Epoch 4886, Loss: 0.26438485085964203, Final Batch Loss: 0.11761471629142761\n",
      "Epoch 4887, Loss: 0.26179924607276917, Final Batch Loss: 0.10915808379650116\n",
      "Epoch 4888, Loss: 0.26315121352672577, Final Batch Loss: 0.1599685400724411\n",
      "Epoch 4889, Loss: 0.23980743438005447, Final Batch Loss: 0.1301443725824356\n",
      "Epoch 4890, Loss: 0.22720301896333694, Final Batch Loss: 0.10414893180131912\n",
      "Epoch 4891, Loss: 0.18450982123613358, Final Batch Loss: 0.07185781747102737\n",
      "Epoch 4892, Loss: 0.2521379142999649, Final Batch Loss: 0.11961142718791962\n",
      "Epoch 4893, Loss: 0.2401825562119484, Final Batch Loss: 0.13737761974334717\n",
      "Epoch 4894, Loss: 0.202412411570549, Final Batch Loss: 0.09102267026901245\n",
      "Epoch 4895, Loss: 0.26072657853364944, Final Batch Loss: 0.12482541054487228\n",
      "Epoch 4896, Loss: 0.21906837821006775, Final Batch Loss: 0.1125544086098671\n",
      "Epoch 4897, Loss: 0.24763331562280655, Final Batch Loss: 0.13098037242889404\n",
      "Epoch 4898, Loss: 0.27711211144924164, Final Batch Loss: 0.15528106689453125\n",
      "Epoch 4899, Loss: 0.21392060071229935, Final Batch Loss: 0.08307977765798569\n",
      "Epoch 4900, Loss: 0.20682509988546371, Final Batch Loss: 0.09766968339681625\n",
      "Epoch 4901, Loss: 0.2616465240716934, Final Batch Loss: 0.1603834480047226\n",
      "Epoch 4902, Loss: 0.25587042421102524, Final Batch Loss: 0.08184819668531418\n",
      "Epoch 4903, Loss: 0.264241062104702, Final Batch Loss: 0.18387365341186523\n",
      "Epoch 4904, Loss: 0.24853505939245224, Final Batch Loss: 0.11945942789316177\n",
      "Epoch 4905, Loss: 0.31932859122753143, Final Batch Loss: 0.15850256383419037\n",
      "Epoch 4906, Loss: 0.2928771898150444, Final Batch Loss: 0.1684385985136032\n",
      "Epoch 4907, Loss: 0.22566667199134827, Final Batch Loss: 0.11786139756441116\n",
      "Epoch 4908, Loss: 0.2107856124639511, Final Batch Loss: 0.11257220804691315\n",
      "Epoch 4909, Loss: 0.26045946776866913, Final Batch Loss: 0.14027681946754456\n",
      "Epoch 4910, Loss: 0.19254043698310852, Final Batch Loss: 0.08459632843732834\n",
      "Epoch 4911, Loss: 0.27650775760412216, Final Batch Loss: 0.17615662515163422\n",
      "Epoch 4912, Loss: 0.22151386737823486, Final Batch Loss: 0.14526839554309845\n",
      "Epoch 4913, Loss: 0.20132581889629364, Final Batch Loss: 0.10123424232006073\n",
      "Epoch 4914, Loss: 0.2454478070139885, Final Batch Loss: 0.12012874335050583\n",
      "Epoch 4915, Loss: 0.24141693860292435, Final Batch Loss: 0.10978125780820847\n",
      "Epoch 4916, Loss: 0.34111618995666504, Final Batch Loss: 0.23300005495548248\n",
      "Epoch 4917, Loss: 0.2309853732585907, Final Batch Loss: 0.1224181279540062\n",
      "Epoch 4918, Loss: 0.2152319699525833, Final Batch Loss: 0.1243833601474762\n",
      "Epoch 4919, Loss: 0.20388516783714294, Final Batch Loss: 0.08633877336978912\n",
      "Epoch 4920, Loss: 0.21880501508712769, Final Batch Loss: 0.14118000864982605\n",
      "Epoch 4921, Loss: 0.21832333505153656, Final Batch Loss: 0.10441350191831589\n",
      "Epoch 4922, Loss: 0.20717504620552063, Final Batch Loss: 0.09968660026788712\n",
      "Epoch 4923, Loss: 0.2249363735318184, Final Batch Loss: 0.14246100187301636\n",
      "Epoch 4924, Loss: 0.22653838992118835, Final Batch Loss: 0.08522956073284149\n",
      "Epoch 4925, Loss: 0.18728971481323242, Final Batch Loss: 0.09179867058992386\n",
      "Epoch 4926, Loss: 0.2125760242342949, Final Batch Loss: 0.08755231648683548\n",
      "Epoch 4927, Loss: 0.25288474559783936, Final Batch Loss: 0.1148311197757721\n",
      "Epoch 4928, Loss: 0.21470294147729874, Final Batch Loss: 0.10477194935083389\n",
      "Epoch 4929, Loss: 0.1968059167265892, Final Batch Loss: 0.10944350063800812\n",
      "Epoch 4930, Loss: 0.23034197837114334, Final Batch Loss: 0.08832097798585892\n",
      "Epoch 4931, Loss: 0.21294596046209335, Final Batch Loss: 0.07499312609434128\n",
      "Epoch 4932, Loss: 0.22162383049726486, Final Batch Loss: 0.11828549951314926\n",
      "Epoch 4933, Loss: 0.23760256171226501, Final Batch Loss: 0.09654758870601654\n",
      "Epoch 4934, Loss: 0.2763710618019104, Final Batch Loss: 0.1456555426120758\n",
      "Epoch 4935, Loss: 0.2094251960515976, Final Batch Loss: 0.09991654008626938\n",
      "Epoch 4936, Loss: 0.23736391216516495, Final Batch Loss: 0.11937557905912399\n",
      "Epoch 4937, Loss: 0.17807640880346298, Final Batch Loss: 0.08856838196516037\n",
      "Epoch 4938, Loss: 0.2332666665315628, Final Batch Loss: 0.1397070437669754\n",
      "Epoch 4939, Loss: 0.21715939790010452, Final Batch Loss: 0.12615154683589935\n",
      "Epoch 4940, Loss: 0.22639134526252747, Final Batch Loss: 0.1260293424129486\n",
      "Epoch 4941, Loss: 0.2392839789390564, Final Batch Loss: 0.12680374085903168\n",
      "Epoch 4942, Loss: 0.34751756489276886, Final Batch Loss: 0.1372511237859726\n",
      "Epoch 4943, Loss: 0.28732719272375107, Final Batch Loss: 0.19176574051380157\n",
      "Epoch 4944, Loss: 0.20807045698165894, Final Batch Loss: 0.11497916281223297\n",
      "Epoch 4945, Loss: 0.2055564969778061, Final Batch Loss: 0.08600140362977982\n",
      "Epoch 4946, Loss: 0.19357406347990036, Final Batch Loss: 0.08283629268407822\n",
      "Epoch 4947, Loss: 0.28670287132263184, Final Batch Loss: 0.1711709350347519\n",
      "Epoch 4948, Loss: 0.23696186393499374, Final Batch Loss: 0.09082642942667007\n",
      "Epoch 4949, Loss: 0.2767239212989807, Final Batch Loss: 0.1557612270116806\n",
      "Epoch 4950, Loss: 0.22548523545265198, Final Batch Loss: 0.12058546394109726\n",
      "Epoch 4951, Loss: 0.21642277389764786, Final Batch Loss: 0.09300332516431808\n",
      "Epoch 4952, Loss: 0.20175450295209885, Final Batch Loss: 0.0804559588432312\n",
      "Epoch 4953, Loss: 0.21782219409942627, Final Batch Loss: 0.10312973707914352\n",
      "Epoch 4954, Loss: 0.17932621389627457, Final Batch Loss: 0.08581331372261047\n",
      "Epoch 4955, Loss: 0.24493050575256348, Final Batch Loss: 0.14534303545951843\n",
      "Epoch 4956, Loss: 0.2308747097849846, Final Batch Loss: 0.08452863246202469\n",
      "Epoch 4957, Loss: 0.2148578017950058, Final Batch Loss: 0.09843367338180542\n",
      "Epoch 4958, Loss: 0.20701914280653, Final Batch Loss: 0.08565077185630798\n",
      "Epoch 4959, Loss: 0.20419912040233612, Final Batch Loss: 0.10828544944524765\n",
      "Epoch 4960, Loss: 0.22944969683885574, Final Batch Loss: 0.1070791557431221\n",
      "Epoch 4961, Loss: 0.30335133522748947, Final Batch Loss: 0.10387132316827774\n",
      "Epoch 4962, Loss: 0.2503386437892914, Final Batch Loss: 0.13860708475112915\n",
      "Epoch 4963, Loss: 0.25011778622865677, Final Batch Loss: 0.1146315410733223\n",
      "Epoch 4964, Loss: 0.21074074506759644, Final Batch Loss: 0.09164300560951233\n",
      "Epoch 4965, Loss: 0.26212192326784134, Final Batch Loss: 0.120119608938694\n",
      "Epoch 4966, Loss: 0.22937684506177902, Final Batch Loss: 0.11282234638929367\n",
      "Epoch 4967, Loss: 0.22328080981969833, Final Batch Loss: 0.12898027896881104\n",
      "Epoch 4968, Loss: 0.24478084594011307, Final Batch Loss: 0.14607493579387665\n",
      "Epoch 4969, Loss: 0.25057803839445114, Final Batch Loss: 0.1301952600479126\n",
      "Epoch 4970, Loss: 0.23694487661123276, Final Batch Loss: 0.13529103994369507\n",
      "Epoch 4971, Loss: 0.23682980239391327, Final Batch Loss: 0.1268421709537506\n",
      "Epoch 4972, Loss: 0.22870639711618423, Final Batch Loss: 0.12931084632873535\n",
      "Epoch 4973, Loss: 0.23445511609315872, Final Batch Loss: 0.07193078845739365\n",
      "Epoch 4974, Loss: 0.2663399428129196, Final Batch Loss: 0.136981800198555\n",
      "Epoch 4975, Loss: 0.21536067873239517, Final Batch Loss: 0.09429466724395752\n",
      "Epoch 4976, Loss: 0.21300733089447021, Final Batch Loss: 0.10287346690893173\n",
      "Epoch 4977, Loss: 0.24276117980480194, Final Batch Loss: 0.10834315419197083\n",
      "Epoch 4978, Loss: 0.2777399569749832, Final Batch Loss: 0.14541830122470856\n",
      "Epoch 4979, Loss: 0.2597913444042206, Final Batch Loss: 0.13779805600643158\n",
      "Epoch 4980, Loss: 0.2632882669568062, Final Batch Loss: 0.16161806881427765\n",
      "Epoch 4981, Loss: 0.21799763292074203, Final Batch Loss: 0.08399762958288193\n",
      "Epoch 4982, Loss: 0.20793190598487854, Final Batch Loss: 0.11040652543306351\n",
      "Epoch 4983, Loss: 0.21260026842355728, Final Batch Loss: 0.10304144024848938\n",
      "Epoch 4984, Loss: 0.2248377874493599, Final Batch Loss: 0.07804941385984421\n",
      "Epoch 4985, Loss: 0.21511950343847275, Final Batch Loss: 0.13004975020885468\n",
      "Epoch 4986, Loss: 0.2143741473555565, Final Batch Loss: 0.10558948665857315\n",
      "Epoch 4987, Loss: 0.2088681161403656, Final Batch Loss: 0.10522443056106567\n",
      "Epoch 4988, Loss: 0.2002272754907608, Final Batch Loss: 0.09763166308403015\n",
      "Epoch 4989, Loss: 0.26125291734933853, Final Batch Loss: 0.09217449277639389\n",
      "Epoch 4990, Loss: 0.21538672596216202, Final Batch Loss: 0.11915598809719086\n",
      "Epoch 4991, Loss: 0.18894881010055542, Final Batch Loss: 0.10103210806846619\n",
      "Epoch 4992, Loss: 0.26317716389894485, Final Batch Loss: 0.11943637579679489\n",
      "Epoch 4993, Loss: 0.21895113587379456, Final Batch Loss: 0.10418031364679337\n",
      "Epoch 4994, Loss: 0.19701823592185974, Final Batch Loss: 0.1244632750749588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4995, Loss: 0.21826045215129852, Final Batch Loss: 0.10295503586530685\n",
      "Epoch 4996, Loss: 0.29214294254779816, Final Batch Loss: 0.17903666198253632\n",
      "Epoch 4997, Loss: 0.23645202070474625, Final Batch Loss: 0.09775232523679733\n",
      "Epoch 4998, Loss: 0.22122252732515335, Final Batch Loss: 0.11259970813989639\n",
      "Epoch 4999, Loss: 0.2610060125589371, Final Batch Loss: 0.158168762922287\n",
      "Epoch 5000, Loss: 0.2605656087398529, Final Batch Loss: 0.13585281372070312\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  1  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  5  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  1  0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        12\n",
      "           1    1.00000   1.00000   1.00000        11\n",
      "           2    0.70588   0.92308   0.80000        13\n",
      "           3    1.00000   1.00000   1.00000        13\n",
      "           4    1.00000   1.00000   1.00000        13\n",
      "           5    0.85714   0.70588   0.77419        17\n",
      "           6    1.00000   1.00000   1.00000        11\n",
      "           7    1.00000   1.00000   1.00000         6\n",
      "           8    1.00000   0.92857   0.96296        14\n",
      "\n",
      "    accuracy                        0.93636       110\n",
      "   macro avg    0.95145   0.95084   0.94857       110\n",
      "weighted avg    0.94316   0.93636   0.93675       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 30\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Group 5_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  0  2  0  0  2  0  0]\n",
      " [ 0 30  0  0  0  0  0  0  0]\n",
      " [ 0  0 29  0  0  1  0  0  0]\n",
      " [ 0  0  0 29  1  0  0  0  0]\n",
      " [ 0  0  0  0 23  0  0  7  0]\n",
      " [ 0  0  6  0  0 19  0  0  5]\n",
      " [ 3  1  0  0  0  0 26  0  0]\n",
      " [ 0  0  0  0  0  0  0 30  0]\n",
      " [ 0  0 11  0  0  1  0  0 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.89655   0.86667   0.88136        30\n",
      "         1.0    0.96774   1.00000   0.98361        30\n",
      "         2.0    0.63043   0.96667   0.76316        30\n",
      "         3.0    0.93548   0.96667   0.95082        30\n",
      "         4.0    0.95833   0.76667   0.85185        30\n",
      "         5.0    0.90476   0.63333   0.74510        30\n",
      "         6.0    0.92857   0.86667   0.89655        30\n",
      "         7.0    0.81081   1.00000   0.89552        30\n",
      "         8.0    0.78261   0.60000   0.67925        30\n",
      "\n",
      "    accuracy                        0.85185       270\n",
      "   macro avg    0.86837   0.85185   0.84969       270\n",
      "weighted avg    0.86837   0.85185   0.84969       270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
