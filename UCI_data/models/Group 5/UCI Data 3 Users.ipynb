{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                 -0.976623              -0.976353  ...   \n",
       "1                 -0.989046              -0.989038  ...   \n",
       "2                 -0.993552              -0.994122  ...   \n",
       "3                 -0.992407              -0.993142  ...   \n",
       "4                 -0.992378              -0.992542  ...   \n",
       "...                     ...                    ...  ...   \n",
       "7347               0.084878               0.065142  ...   \n",
       "7348               0.098249               0.091791  ...   \n",
       "7349               0.185902               0.170686  ...   \n",
       "7350               0.190360               0.178939  ...   \n",
       "7351               0.022216              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                    -0.998285        1         5  \n",
       "1                    -0.999472        1         5  \n",
       "2                    -0.999807        1         5  \n",
       "3                    -0.999770        1         5  \n",
       "4                    -0.999873        1         5  \n",
       "...                        ...      ...       ...  \n",
       "7347                 -0.584282       30         2  \n",
       "7348                 -0.632536       30         2  \n",
       "7349                 -0.641170       30         2  \n",
       "7350                 -0.663579       30         2  \n",
       "7351                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([23, 25, 27])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([23, 25, 27])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 23:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 25:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.200658440589905, Final Batch Loss: 1.1042265892028809\n",
      "Epoch 2, Loss: 2.1990095376968384, Final Batch Loss: 1.100662350654602\n",
      "Epoch 3, Loss: 2.198748469352722, Final Batch Loss: 1.0911118984222412\n",
      "Epoch 4, Loss: 2.19314444065094, Final Batch Loss: 1.0966066122055054\n",
      "Epoch 5, Loss: 2.1949328184127808, Final Batch Loss: 1.1040284633636475\n",
      "Epoch 6, Loss: 2.1907354593276978, Final Batch Loss: 1.0917129516601562\n",
      "Epoch 7, Loss: 2.185514211654663, Final Batch Loss: 1.0970133543014526\n",
      "Epoch 8, Loss: 2.185429096221924, Final Batch Loss: 1.087180733680725\n",
      "Epoch 9, Loss: 2.181128978729248, Final Batch Loss: 1.0907343626022339\n",
      "Epoch 10, Loss: 2.1741623878479004, Final Batch Loss: 1.0794941186904907\n",
      "Epoch 11, Loss: 2.1692488193511963, Final Batch Loss: 1.084101676940918\n",
      "Epoch 12, Loss: 2.165552854537964, Final Batch Loss: 1.0829293727874756\n",
      "Epoch 13, Loss: 2.1675493717193604, Final Batch Loss: 1.0833817720413208\n",
      "Epoch 14, Loss: 2.1630783081054688, Final Batch Loss: 1.0770153999328613\n",
      "Epoch 15, Loss: 2.1525927782058716, Final Batch Loss: 1.0659584999084473\n",
      "Epoch 16, Loss: 2.169279456138611, Final Batch Loss: 1.0792170763015747\n",
      "Epoch 17, Loss: 2.155744791030884, Final Batch Loss: 1.0808069705963135\n",
      "Epoch 18, Loss: 2.1430256366729736, Final Batch Loss: 1.0776922702789307\n",
      "Epoch 19, Loss: 2.1439199447631836, Final Batch Loss: 1.0641807317733765\n",
      "Epoch 20, Loss: 2.142877697944641, Final Batch Loss: 1.069124698638916\n",
      "Epoch 21, Loss: 2.124879240989685, Final Batch Loss: 1.0743438005447388\n",
      "Epoch 22, Loss: 2.137728214263916, Final Batch Loss: 1.0749021768569946\n",
      "Epoch 23, Loss: 2.1165740489959717, Final Batch Loss: 1.0409208536148071\n",
      "Epoch 24, Loss: 2.117299437522888, Final Batch Loss: 1.0617189407348633\n",
      "Epoch 25, Loss: 2.1223695278167725, Final Batch Loss: 1.0486418008804321\n",
      "Epoch 26, Loss: 2.1034836769104004, Final Batch Loss: 1.0747992992401123\n",
      "Epoch 27, Loss: 2.0936930179595947, Final Batch Loss: 1.0461689233779907\n",
      "Epoch 28, Loss: 2.095393180847168, Final Batch Loss: 1.0493357181549072\n",
      "Epoch 29, Loss: 2.084598660469055, Final Batch Loss: 1.0569610595703125\n",
      "Epoch 30, Loss: 2.0689966678619385, Final Batch Loss: 1.041183590888977\n",
      "Epoch 31, Loss: 2.0420291423797607, Final Batch Loss: 1.0245471000671387\n",
      "Epoch 32, Loss: 2.066064715385437, Final Batch Loss: 1.015067458152771\n",
      "Epoch 33, Loss: 2.046277403831482, Final Batch Loss: 1.0231693983078003\n",
      "Epoch 34, Loss: 2.0427849292755127, Final Batch Loss: 1.0169531106948853\n",
      "Epoch 35, Loss: 2.0154480934143066, Final Batch Loss: 1.014077067375183\n",
      "Epoch 36, Loss: 1.9756725430488586, Final Batch Loss: 0.980646550655365\n",
      "Epoch 37, Loss: 1.9859642386436462, Final Batch Loss: 1.0226022005081177\n",
      "Epoch 38, Loss: 1.961578607559204, Final Batch Loss: 0.9909125566482544\n",
      "Epoch 39, Loss: 1.9513436555862427, Final Batch Loss: 0.9510438442230225\n",
      "Epoch 40, Loss: 1.924472153186798, Final Batch Loss: 0.9911391735076904\n",
      "Epoch 41, Loss: 1.9218512177467346, Final Batch Loss: 0.9551534056663513\n",
      "Epoch 42, Loss: 1.8952110409736633, Final Batch Loss: 0.9451707005500793\n",
      "Epoch 43, Loss: 1.860550045967102, Final Batch Loss: 0.9364818334579468\n",
      "Epoch 44, Loss: 1.8345946669578552, Final Batch Loss: 0.9134208559989929\n",
      "Epoch 45, Loss: 1.8090219497680664, Final Batch Loss: 0.9173922538757324\n",
      "Epoch 46, Loss: 1.808713674545288, Final Batch Loss: 0.9251371026039124\n",
      "Epoch 47, Loss: 1.7506878972053528, Final Batch Loss: 0.8913025856018066\n",
      "Epoch 48, Loss: 1.6979058384895325, Final Batch Loss: 0.8482890129089355\n",
      "Epoch 49, Loss: 1.7013382315635681, Final Batch Loss: 0.840221643447876\n",
      "Epoch 50, Loss: 1.6341053247451782, Final Batch Loss: 0.8004640936851501\n",
      "Epoch 51, Loss: 1.5904919505119324, Final Batch Loss: 0.7841939330101013\n",
      "Epoch 52, Loss: 1.5198351740837097, Final Batch Loss: 0.7517021298408508\n",
      "Epoch 53, Loss: 1.5403882265090942, Final Batch Loss: 0.8039534091949463\n",
      "Epoch 54, Loss: 1.458187460899353, Final Batch Loss: 0.7126941084861755\n",
      "Epoch 55, Loss: 1.4510332345962524, Final Batch Loss: 0.723302960395813\n",
      "Epoch 56, Loss: 1.3795342445373535, Final Batch Loss: 0.6581275463104248\n",
      "Epoch 57, Loss: 1.3844982385635376, Final Batch Loss: 0.7085533738136292\n",
      "Epoch 58, Loss: 1.3600875735282898, Final Batch Loss: 0.7065817713737488\n",
      "Epoch 59, Loss: 1.2705670595169067, Final Batch Loss: 0.577182948589325\n",
      "Epoch 60, Loss: 1.247494876384735, Final Batch Loss: 0.6476979851722717\n",
      "Epoch 61, Loss: 1.2439938187599182, Final Batch Loss: 0.582040011882782\n",
      "Epoch 62, Loss: 1.260353684425354, Final Batch Loss: 0.6610144376754761\n",
      "Epoch 63, Loss: 1.2347936034202576, Final Batch Loss: 0.6451205611228943\n",
      "Epoch 64, Loss: 1.1653165817260742, Final Batch Loss: 0.5918351411819458\n",
      "Epoch 65, Loss: 1.1228855848312378, Final Batch Loss: 0.5476550459861755\n",
      "Epoch 66, Loss: 1.1501925587654114, Final Batch Loss: 0.5809994339942932\n",
      "Epoch 67, Loss: 1.1151793003082275, Final Batch Loss: 0.5196050405502319\n",
      "Epoch 68, Loss: 1.1472135186195374, Final Batch Loss: 0.5490214824676514\n",
      "Epoch 69, Loss: 1.0996246337890625, Final Batch Loss: 0.5572018027305603\n",
      "Epoch 70, Loss: 1.1415603756904602, Final Batch Loss: 0.5138353109359741\n",
      "Epoch 71, Loss: 1.0838427543640137, Final Batch Loss: 0.5749430060386658\n",
      "Epoch 72, Loss: 1.0889028310775757, Final Batch Loss: 0.5252032279968262\n",
      "Epoch 73, Loss: 1.0024119317531586, Final Batch Loss: 0.465452641248703\n",
      "Epoch 74, Loss: 1.0120832324028015, Final Batch Loss: 0.49811381101608276\n",
      "Epoch 75, Loss: 1.0382339358329773, Final Batch Loss: 0.5388425588607788\n",
      "Epoch 76, Loss: 1.025188386440277, Final Batch Loss: 0.5345595479011536\n",
      "Epoch 77, Loss: 1.0458067059516907, Final Batch Loss: 0.5205832719802856\n",
      "Epoch 78, Loss: 0.9862406849861145, Final Batch Loss: 0.4727558493614197\n",
      "Epoch 79, Loss: 0.9868528544902802, Final Batch Loss: 0.4989011287689209\n",
      "Epoch 80, Loss: 0.9840108156204224, Final Batch Loss: 0.4742303490638733\n",
      "Epoch 81, Loss: 0.9772422611713409, Final Batch Loss: 0.47848236560821533\n",
      "Epoch 82, Loss: 0.9456391930580139, Final Batch Loss: 0.4836072623729706\n",
      "Epoch 83, Loss: 0.9584046602249146, Final Batch Loss: 0.4944313168525696\n",
      "Epoch 84, Loss: 0.9546979367733002, Final Batch Loss: 0.48956266045570374\n",
      "Epoch 85, Loss: 0.9675213694572449, Final Batch Loss: 0.44965916872024536\n",
      "Epoch 86, Loss: 0.9396827220916748, Final Batch Loss: 0.5234358310699463\n",
      "Epoch 87, Loss: 0.9050489068031311, Final Batch Loss: 0.45991000533103943\n",
      "Epoch 88, Loss: 0.9048956036567688, Final Batch Loss: 0.4120868146419525\n",
      "Epoch 89, Loss: 0.9461803138256073, Final Batch Loss: 0.4791784882545471\n",
      "Epoch 90, Loss: 0.9208637177944183, Final Batch Loss: 0.47714492678642273\n",
      "Epoch 91, Loss: 0.9226252138614655, Final Batch Loss: 0.5063662528991699\n",
      "Epoch 92, Loss: 0.9666826128959656, Final Batch Loss: 0.44122010469436646\n",
      "Epoch 93, Loss: 0.9029734432697296, Final Batch Loss: 0.47770336270332336\n",
      "Epoch 94, Loss: 0.889294445514679, Final Batch Loss: 0.43959879875183105\n",
      "Epoch 95, Loss: 0.9365410804748535, Final Batch Loss: 0.4676942825317383\n",
      "Epoch 96, Loss: 0.8818920850753784, Final Batch Loss: 0.4692990779876709\n",
      "Epoch 97, Loss: 0.9145333766937256, Final Batch Loss: 0.46387699246406555\n",
      "Epoch 98, Loss: 0.8753557801246643, Final Batch Loss: 0.4438779354095459\n",
      "Epoch 99, Loss: 0.881217896938324, Final Batch Loss: 0.41684490442276\n",
      "Epoch 100, Loss: 0.8969064652919769, Final Batch Loss: 0.43416139483451843\n",
      "Epoch 101, Loss: 0.8876161873340607, Final Batch Loss: 0.4349019229412079\n",
      "Epoch 102, Loss: 0.8592629730701447, Final Batch Loss: 0.4066495895385742\n",
      "Epoch 103, Loss: 0.8771391212940216, Final Batch Loss: 0.43416112661361694\n",
      "Epoch 104, Loss: 0.8385201692581177, Final Batch Loss: 0.41082262992858887\n",
      "Epoch 105, Loss: 0.8592098355293274, Final Batch Loss: 0.4269072711467743\n",
      "Epoch 106, Loss: 0.8537531793117523, Final Batch Loss: 0.41071054339408875\n",
      "Epoch 107, Loss: 0.8486894369125366, Final Batch Loss: 0.4119848906993866\n",
      "Epoch 108, Loss: 0.8610045313835144, Final Batch Loss: 0.4540601968765259\n",
      "Epoch 109, Loss: 0.8698645830154419, Final Batch Loss: 0.433280348777771\n",
      "Epoch 110, Loss: 0.8512651026248932, Final Batch Loss: 0.40991121530532837\n",
      "Epoch 111, Loss: 0.7775005400180817, Final Batch Loss: 0.35554373264312744\n",
      "Epoch 112, Loss: 0.8660198152065277, Final Batch Loss: 0.41388964653015137\n",
      "Epoch 113, Loss: 0.8045006394386292, Final Batch Loss: 0.38888871669769287\n",
      "Epoch 114, Loss: 0.7987955212593079, Final Batch Loss: 0.377006858587265\n",
      "Epoch 115, Loss: 0.7758092284202576, Final Batch Loss: 0.4084359407424927\n",
      "Epoch 116, Loss: 0.8188931047916412, Final Batch Loss: 0.43519365787506104\n",
      "Epoch 117, Loss: 0.7939611375331879, Final Batch Loss: 0.3900076150894165\n",
      "Epoch 118, Loss: 0.7845594882965088, Final Batch Loss: 0.35484275221824646\n",
      "Epoch 119, Loss: 0.7810603678226471, Final Batch Loss: 0.3452552855014801\n",
      "Epoch 120, Loss: 0.7446699738502502, Final Batch Loss: 0.3915901184082031\n",
      "Epoch 121, Loss: 0.756460040807724, Final Batch Loss: 0.3903229236602783\n",
      "Epoch 122, Loss: 0.7825140357017517, Final Batch Loss: 0.37929585576057434\n",
      "Epoch 123, Loss: 0.7785591185092926, Final Batch Loss: 0.40779897570610046\n",
      "Epoch 124, Loss: 0.7418098747730255, Final Batch Loss: 0.4048803150653839\n",
      "Epoch 125, Loss: 0.7615731954574585, Final Batch Loss: 0.3123205304145813\n",
      "Epoch 126, Loss: 0.7781630158424377, Final Batch Loss: 0.3984178304672241\n",
      "Epoch 127, Loss: 0.753277450799942, Final Batch Loss: 0.35518378019332886\n",
      "Epoch 128, Loss: 0.7207453548908234, Final Batch Loss: 0.3566879630088806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Loss: 0.731876790523529, Final Batch Loss: 0.3712015450000763\n",
      "Epoch 130, Loss: 0.7313864231109619, Final Batch Loss: 0.3525184392929077\n",
      "Epoch 131, Loss: 0.7490906119346619, Final Batch Loss: 0.3946406841278076\n",
      "Epoch 132, Loss: 0.7248943746089935, Final Batch Loss: 0.36279693245887756\n",
      "Epoch 133, Loss: 0.7012345790863037, Final Batch Loss: 0.3616919219493866\n",
      "Epoch 134, Loss: 0.7593818306922913, Final Batch Loss: 0.3756953775882721\n",
      "Epoch 135, Loss: 0.6773301362991333, Final Batch Loss: 0.33659034967422485\n",
      "Epoch 136, Loss: 0.7259250283241272, Final Batch Loss: 0.32686081528663635\n",
      "Epoch 137, Loss: 0.7551421225070953, Final Batch Loss: 0.4033716022968292\n",
      "Epoch 138, Loss: 0.703225314617157, Final Batch Loss: 0.3385199308395386\n",
      "Epoch 139, Loss: 0.7513655722141266, Final Batch Loss: 0.4098201394081116\n",
      "Epoch 140, Loss: 0.7268100380897522, Final Batch Loss: 0.37051641941070557\n",
      "Epoch 141, Loss: 0.6814877390861511, Final Batch Loss: 0.32825106382369995\n",
      "Epoch 142, Loss: 0.6732190251350403, Final Batch Loss: 0.3033510446548462\n",
      "Epoch 143, Loss: 0.6763825714588165, Final Batch Loss: 0.3572395443916321\n",
      "Epoch 144, Loss: 0.6869201958179474, Final Batch Loss: 0.3365858495235443\n",
      "Epoch 145, Loss: 0.6871143579483032, Final Batch Loss: 0.3371618092060089\n",
      "Epoch 146, Loss: 0.7134946286678314, Final Batch Loss: 0.34320950508117676\n",
      "Epoch 147, Loss: 0.6514050364494324, Final Batch Loss: 0.3130374848842621\n",
      "Epoch 148, Loss: 0.6581575572490692, Final Batch Loss: 0.31768813729286194\n",
      "Epoch 149, Loss: 0.6816355586051941, Final Batch Loss: 0.3253325819969177\n",
      "Epoch 150, Loss: 0.6599816679954529, Final Batch Loss: 0.3150956928730011\n",
      "Epoch 151, Loss: 0.6460519433021545, Final Batch Loss: 0.32351791858673096\n",
      "Epoch 152, Loss: 0.6680698990821838, Final Batch Loss: 0.32918569445610046\n",
      "Epoch 153, Loss: 0.6746631562709808, Final Batch Loss: 0.3574969470500946\n",
      "Epoch 154, Loss: 0.640093207359314, Final Batch Loss: 0.3056490421295166\n",
      "Epoch 155, Loss: 0.6746636629104614, Final Batch Loss: 0.3359566032886505\n",
      "Epoch 156, Loss: 0.6747041642665863, Final Batch Loss: 0.3479524552822113\n",
      "Epoch 157, Loss: 0.6425322592258453, Final Batch Loss: 0.3079896867275238\n",
      "Epoch 158, Loss: 0.6487149894237518, Final Batch Loss: 0.34512507915496826\n",
      "Epoch 159, Loss: 0.6267245709896088, Final Batch Loss: 0.29179438948631287\n",
      "Epoch 160, Loss: 0.6596381664276123, Final Batch Loss: 0.30464836955070496\n",
      "Epoch 161, Loss: 0.6384669542312622, Final Batch Loss: 0.33672431111335754\n",
      "Epoch 162, Loss: 0.6298847794532776, Final Batch Loss: 0.32286256551742554\n",
      "Epoch 163, Loss: 0.6387040317058563, Final Batch Loss: 0.3468164801597595\n",
      "Epoch 164, Loss: 0.6041897535324097, Final Batch Loss: 0.29730549454689026\n",
      "Epoch 165, Loss: 0.6237311661243439, Final Batch Loss: 0.2917523682117462\n",
      "Epoch 166, Loss: 0.6193751394748688, Final Batch Loss: 0.3215210735797882\n",
      "Epoch 167, Loss: 0.5877826511859894, Final Batch Loss: 0.2976774573326111\n",
      "Epoch 168, Loss: 0.6078265607357025, Final Batch Loss: 0.28749880194664\n",
      "Epoch 169, Loss: 0.6315281987190247, Final Batch Loss: 0.3059913218021393\n",
      "Epoch 170, Loss: 0.5585128366947174, Final Batch Loss: 0.2671394348144531\n",
      "Epoch 171, Loss: 0.5809402763843536, Final Batch Loss: 0.2816615700721741\n",
      "Epoch 172, Loss: 0.5817684531211853, Final Batch Loss: 0.3067249655723572\n",
      "Epoch 173, Loss: 0.5585121810436249, Final Batch Loss: 0.2854922413825989\n",
      "Epoch 174, Loss: 0.5951516628265381, Final Batch Loss: 0.27391117811203003\n",
      "Epoch 175, Loss: 0.5633364021778107, Final Batch Loss: 0.3207223415374756\n",
      "Epoch 176, Loss: 0.5708085000514984, Final Batch Loss: 0.26930227875709534\n",
      "Epoch 177, Loss: 0.5539305210113525, Final Batch Loss: 0.283838152885437\n",
      "Epoch 178, Loss: 0.5538988411426544, Final Batch Loss: 0.296151727437973\n",
      "Epoch 179, Loss: 0.5387197136878967, Final Batch Loss: 0.2319963574409485\n",
      "Epoch 180, Loss: 0.5061829090118408, Final Batch Loss: 0.23709526658058167\n",
      "Epoch 181, Loss: 0.5029891431331635, Final Batch Loss: 0.2801411747932434\n",
      "Epoch 182, Loss: 0.519774317741394, Final Batch Loss: 0.28400224447250366\n",
      "Epoch 183, Loss: 0.529386356472969, Final Batch Loss: 0.2472502738237381\n",
      "Epoch 184, Loss: 0.5048016607761383, Final Batch Loss: 0.2609347105026245\n",
      "Epoch 185, Loss: 0.5273095667362213, Final Batch Loss: 0.30268150568008423\n",
      "Epoch 186, Loss: 0.532889723777771, Final Batch Loss: 0.25006240606307983\n",
      "Epoch 187, Loss: 0.45156601071357727, Final Batch Loss: 0.24712897837162018\n",
      "Epoch 188, Loss: 0.5052787065505981, Final Batch Loss: 0.26230794191360474\n",
      "Epoch 189, Loss: 0.5220970511436462, Final Batch Loss: 0.25901517271995544\n",
      "Epoch 190, Loss: 0.47642146050930023, Final Batch Loss: 0.23954826593399048\n",
      "Epoch 191, Loss: 0.5191390812397003, Final Batch Loss: 0.24502485990524292\n",
      "Epoch 192, Loss: 0.49240659177303314, Final Batch Loss: 0.26507142186164856\n",
      "Epoch 193, Loss: 0.516451820731163, Final Batch Loss: 0.27529099583625793\n",
      "Epoch 194, Loss: 0.4959600418806076, Final Batch Loss: 0.31297487020492554\n",
      "Epoch 195, Loss: 0.47919484972953796, Final Batch Loss: 0.18717727065086365\n",
      "Epoch 196, Loss: 0.4851790815591812, Final Batch Loss: 0.24628697335720062\n",
      "Epoch 197, Loss: 0.535191535949707, Final Batch Loss: 0.236953467130661\n",
      "Epoch 198, Loss: 0.46625925600528717, Final Batch Loss: 0.25042086839675903\n",
      "Epoch 199, Loss: 0.4860183447599411, Final Batch Loss: 0.26784706115722656\n",
      "Epoch 200, Loss: 0.47449342906475067, Final Batch Loss: 0.2100210040807724\n",
      "Epoch 201, Loss: 0.4607798606157303, Final Batch Loss: 0.2436216175556183\n",
      "Epoch 202, Loss: 0.41915175318717957, Final Batch Loss: 0.20159080624580383\n",
      "Epoch 203, Loss: 0.4804627150297165, Final Batch Loss: 0.26574259996414185\n",
      "Epoch 204, Loss: 0.44015155732631683, Final Batch Loss: 0.20704394578933716\n",
      "Epoch 205, Loss: 0.49202990531921387, Final Batch Loss: 0.2675645053386688\n",
      "Epoch 206, Loss: 0.39955124258995056, Final Batch Loss: 0.19389010965824127\n",
      "Epoch 207, Loss: 0.4532320350408554, Final Batch Loss: 0.1966610699892044\n",
      "Epoch 208, Loss: 0.47009801864624023, Final Batch Loss: 0.2412257343530655\n",
      "Epoch 209, Loss: 0.4576711058616638, Final Batch Loss: 0.19297221302986145\n",
      "Epoch 210, Loss: 0.41756656765937805, Final Batch Loss: 0.20407915115356445\n",
      "Epoch 211, Loss: 0.45268614590168, Final Batch Loss: 0.19752280414104462\n",
      "Epoch 212, Loss: 0.49617883563041687, Final Batch Loss: 0.23595568537712097\n",
      "Epoch 213, Loss: 0.47081123292446136, Final Batch Loss: 0.2415204644203186\n",
      "Epoch 214, Loss: 0.4081219732761383, Final Batch Loss: 0.16724830865859985\n",
      "Epoch 215, Loss: 0.4671318084001541, Final Batch Loss: 0.194440558552742\n",
      "Epoch 216, Loss: 0.4334399551153183, Final Batch Loss: 0.20848095417022705\n",
      "Epoch 217, Loss: 0.4325646609067917, Final Batch Loss: 0.1885267049074173\n",
      "Epoch 218, Loss: 0.43844033777713776, Final Batch Loss: 0.22369180619716644\n",
      "Epoch 219, Loss: 0.4444587230682373, Final Batch Loss: 0.2036387175321579\n",
      "Epoch 220, Loss: 0.4102883040904999, Final Batch Loss: 0.20989105105400085\n",
      "Epoch 221, Loss: 0.420412540435791, Final Batch Loss: 0.23357638716697693\n",
      "Epoch 222, Loss: 0.3954967111349106, Final Batch Loss: 0.24413110315799713\n",
      "Epoch 223, Loss: 0.42825476825237274, Final Batch Loss: 0.2254813015460968\n",
      "Epoch 224, Loss: 0.4339246153831482, Final Batch Loss: 0.22222614288330078\n",
      "Epoch 225, Loss: 0.4000498652458191, Final Batch Loss: 0.21089135110378265\n",
      "Epoch 226, Loss: 0.4070696383714676, Final Batch Loss: 0.1922268122434616\n",
      "Epoch 227, Loss: 0.41135773062705994, Final Batch Loss: 0.21759024262428284\n",
      "Epoch 228, Loss: 0.4146419167518616, Final Batch Loss: 0.18952009081840515\n",
      "Epoch 229, Loss: 0.3915191739797592, Final Batch Loss: 0.18390581011772156\n",
      "Epoch 230, Loss: 0.3718041032552719, Final Batch Loss: 0.17615848779678345\n",
      "Epoch 231, Loss: 0.3792400658130646, Final Batch Loss: 0.16788209974765778\n",
      "Epoch 232, Loss: 0.4097750186920166, Final Batch Loss: 0.18796946108341217\n",
      "Epoch 233, Loss: 0.3745273947715759, Final Batch Loss: 0.2121446430683136\n",
      "Epoch 234, Loss: 0.39140522480010986, Final Batch Loss: 0.2224208563566208\n",
      "Epoch 235, Loss: 0.37952573597431183, Final Batch Loss: 0.17947639524936676\n",
      "Epoch 236, Loss: 0.3379492536187172, Final Batch Loss: 0.12425831705331802\n",
      "Epoch 237, Loss: 0.36474867165088654, Final Batch Loss: 0.1846584975719452\n",
      "Epoch 238, Loss: 0.4336356371641159, Final Batch Loss: 0.24506595730781555\n",
      "Epoch 239, Loss: 0.42291414737701416, Final Batch Loss: 0.21353913843631744\n",
      "Epoch 240, Loss: 0.37094223499298096, Final Batch Loss: 0.19024576246738434\n",
      "Epoch 241, Loss: 0.3931535929441452, Final Batch Loss: 0.21435649693012238\n",
      "Epoch 242, Loss: 0.3767983019351959, Final Batch Loss: 0.15300598740577698\n",
      "Epoch 243, Loss: 0.39099428057670593, Final Batch Loss: 0.20429016649723053\n",
      "Epoch 244, Loss: 0.35423171520233154, Final Batch Loss: 0.19443179666996002\n",
      "Epoch 245, Loss: 0.4010680466890335, Final Batch Loss: 0.17000259459018707\n",
      "Epoch 246, Loss: 0.3513374328613281, Final Batch Loss: 0.1770925670862198\n",
      "Epoch 247, Loss: 0.4279615432024002, Final Batch Loss: 0.21584700047969818\n",
      "Epoch 248, Loss: 0.37984345853328705, Final Batch Loss: 0.2246667742729187\n",
      "Epoch 249, Loss: 0.33361704647541046, Final Batch Loss: 0.16627448797225952\n",
      "Epoch 250, Loss: 0.37212710082530975, Final Batch Loss: 0.2014416605234146\n",
      "Epoch 251, Loss: 0.37728361785411835, Final Batch Loss: 0.20346035063266754\n",
      "Epoch 252, Loss: 0.32146479189395905, Final Batch Loss: 0.18536145985126495\n",
      "Epoch 253, Loss: 0.3746054321527481, Final Batch Loss: 0.19876785576343536\n",
      "Epoch 254, Loss: 0.32857294380664825, Final Batch Loss: 0.16241027414798737\n",
      "Epoch 255, Loss: 0.356772780418396, Final Batch Loss: 0.13893486559391022\n",
      "Epoch 256, Loss: 0.37504400312900543, Final Batch Loss: 0.14601309597492218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257, Loss: 0.3420219421386719, Final Batch Loss: 0.17479227483272552\n",
      "Epoch 258, Loss: 0.39007890224456787, Final Batch Loss: 0.19752056896686554\n",
      "Epoch 259, Loss: 0.32746900618076324, Final Batch Loss: 0.15346844494342804\n",
      "Epoch 260, Loss: 0.3709951639175415, Final Batch Loss: 0.17709693312644958\n",
      "Epoch 261, Loss: 0.34327125549316406, Final Batch Loss: 0.15668916702270508\n",
      "Epoch 262, Loss: 0.34984245896339417, Final Batch Loss: 0.16241663694381714\n",
      "Epoch 263, Loss: 0.3459702134132385, Final Batch Loss: 0.16481828689575195\n",
      "Epoch 264, Loss: 0.38070589303970337, Final Batch Loss: 0.20421697199344635\n",
      "Epoch 265, Loss: 0.3620879352092743, Final Batch Loss: 0.17505447566509247\n",
      "Epoch 266, Loss: 0.3446718454360962, Final Batch Loss: 0.17603595554828644\n",
      "Epoch 267, Loss: 0.49461911618709564, Final Batch Loss: 0.23765088617801666\n",
      "Epoch 268, Loss: 0.2944657951593399, Final Batch Loss: 0.12160278856754303\n",
      "Epoch 269, Loss: 0.3202725201845169, Final Batch Loss: 0.14914697408676147\n",
      "Epoch 270, Loss: 0.40166859328746796, Final Batch Loss: 0.15501925349235535\n",
      "Epoch 271, Loss: 0.3402297794818878, Final Batch Loss: 0.1515042930841446\n",
      "Epoch 272, Loss: 0.34278059005737305, Final Batch Loss: 0.17127960920333862\n",
      "Epoch 273, Loss: 0.35561761260032654, Final Batch Loss: 0.1757412552833557\n",
      "Epoch 274, Loss: 0.36929790675640106, Final Batch Loss: 0.19018305838108063\n",
      "Epoch 275, Loss: 0.3785153329372406, Final Batch Loss: 0.24087384343147278\n",
      "Epoch 276, Loss: 0.38779063522815704, Final Batch Loss: 0.22247762978076935\n",
      "Epoch 277, Loss: 0.3511872738599777, Final Batch Loss: 0.19322697818279266\n",
      "Epoch 278, Loss: 0.3950805217027664, Final Batch Loss: 0.20355087518692017\n",
      "Epoch 279, Loss: 0.2791450321674347, Final Batch Loss: 0.09245777130126953\n",
      "Epoch 280, Loss: 0.3074169307947159, Final Batch Loss: 0.17065754532814026\n",
      "Epoch 281, Loss: 0.3264095038175583, Final Batch Loss: 0.17499154806137085\n",
      "Epoch 282, Loss: 0.31230907142162323, Final Batch Loss: 0.1296011358499527\n",
      "Epoch 283, Loss: 0.34021250903606415, Final Batch Loss: 0.18945838510990143\n",
      "Epoch 284, Loss: 0.2930697500705719, Final Batch Loss: 0.13721241056919098\n",
      "Epoch 285, Loss: 0.418296679854393, Final Batch Loss: 0.18553589284420013\n",
      "Epoch 286, Loss: 0.3162388652563095, Final Batch Loss: 0.17909106612205505\n",
      "Epoch 287, Loss: 0.329687163233757, Final Batch Loss: 0.14037084579467773\n",
      "Epoch 288, Loss: 0.3289213478565216, Final Batch Loss: 0.1423763483762741\n",
      "Epoch 289, Loss: 0.3630371689796448, Final Batch Loss: 0.19594411551952362\n",
      "Epoch 290, Loss: 0.27139126509428024, Final Batch Loss: 0.12154442816972733\n",
      "Epoch 291, Loss: 0.3004585951566696, Final Batch Loss: 0.16304649412631989\n",
      "Epoch 292, Loss: 0.2558676451444626, Final Batch Loss: 0.11945442855358124\n",
      "Epoch 293, Loss: 0.33359430730342865, Final Batch Loss: 0.17639176547527313\n",
      "Epoch 294, Loss: 0.330401748418808, Final Batch Loss: 0.19212865829467773\n",
      "Epoch 295, Loss: 0.35500097274780273, Final Batch Loss: 0.17863495647907257\n",
      "Epoch 296, Loss: 0.27293752133846283, Final Batch Loss: 0.15007178485393524\n",
      "Epoch 297, Loss: 0.34655752778053284, Final Batch Loss: 0.20898233354091644\n",
      "Epoch 298, Loss: 0.3415045291185379, Final Batch Loss: 0.20404021441936493\n",
      "Epoch 299, Loss: 0.2766016945242882, Final Batch Loss: 0.15690772235393524\n",
      "Epoch 300, Loss: 0.3015703707933426, Final Batch Loss: 0.14642411470413208\n",
      "Epoch 301, Loss: 0.26786674559116364, Final Batch Loss: 0.14065587520599365\n",
      "Epoch 302, Loss: 0.28673379868268967, Final Batch Loss: 0.12185844033956528\n",
      "Epoch 303, Loss: 0.34825196862220764, Final Batch Loss: 0.20320791006088257\n",
      "Epoch 304, Loss: 0.3337060660123825, Final Batch Loss: 0.19449634850025177\n",
      "Epoch 305, Loss: 0.3012300878763199, Final Batch Loss: 0.16392509639263153\n",
      "Epoch 306, Loss: 0.32147713005542755, Final Batch Loss: 0.16441476345062256\n",
      "Epoch 307, Loss: 0.3332265317440033, Final Batch Loss: 0.15124531090259552\n",
      "Epoch 308, Loss: 0.2618359550833702, Final Batch Loss: 0.11725229769945145\n",
      "Epoch 309, Loss: 0.2850703001022339, Final Batch Loss: 0.15825241804122925\n",
      "Epoch 310, Loss: 0.4024766683578491, Final Batch Loss: 0.1823616921901703\n",
      "Epoch 311, Loss: 0.2654707208275795, Final Batch Loss: 0.14573852717876434\n",
      "Epoch 312, Loss: 0.2984512448310852, Final Batch Loss: 0.1159151941537857\n",
      "Epoch 313, Loss: 0.2880803495645523, Final Batch Loss: 0.14802046120166779\n",
      "Epoch 314, Loss: 0.2661774158477783, Final Batch Loss: 0.1321210265159607\n",
      "Epoch 315, Loss: 0.2784558832645416, Final Batch Loss: 0.1355484277009964\n",
      "Epoch 316, Loss: 0.26089685410261154, Final Batch Loss: 0.10209416598081589\n",
      "Epoch 317, Loss: 0.2779708504676819, Final Batch Loss: 0.1815095692873001\n",
      "Epoch 318, Loss: 0.23155460506677628, Final Batch Loss: 0.12314610183238983\n",
      "Epoch 319, Loss: 0.2726903483271599, Final Batch Loss: 0.10479631274938583\n",
      "Epoch 320, Loss: 0.2690019905567169, Final Batch Loss: 0.11875700950622559\n",
      "Epoch 321, Loss: 0.29631368815898895, Final Batch Loss: 0.14529357850551605\n",
      "Epoch 322, Loss: 0.26556238532066345, Final Batch Loss: 0.14313699305057526\n",
      "Epoch 323, Loss: 0.3358290046453476, Final Batch Loss: 0.1642168015241623\n",
      "Epoch 324, Loss: 0.2646504193544388, Final Batch Loss: 0.10748462378978729\n",
      "Epoch 325, Loss: 0.30838773399591446, Final Batch Loss: 0.11780429631471634\n",
      "Epoch 326, Loss: 0.30331484973430634, Final Batch Loss: 0.12666065990924835\n",
      "Epoch 327, Loss: 0.2623424455523491, Final Batch Loss: 0.14654594659805298\n",
      "Epoch 328, Loss: 0.31707876920700073, Final Batch Loss: 0.14868305623531342\n",
      "Epoch 329, Loss: 0.27251967787742615, Final Batch Loss: 0.14980413019657135\n",
      "Epoch 330, Loss: 0.3159346878528595, Final Batch Loss: 0.16222701966762543\n",
      "Epoch 331, Loss: 0.34313541650772095, Final Batch Loss: 0.13277587294578552\n",
      "Epoch 332, Loss: 0.27698447555303574, Final Batch Loss: 0.124618299305439\n",
      "Epoch 333, Loss: 0.23419305682182312, Final Batch Loss: 0.13933387398719788\n",
      "Epoch 334, Loss: 0.28993943333625793, Final Batch Loss: 0.14377810060977936\n",
      "Epoch 335, Loss: 0.2654780298471451, Final Batch Loss: 0.12581272423267365\n",
      "Epoch 336, Loss: 0.2799350321292877, Final Batch Loss: 0.1573171317577362\n",
      "Epoch 337, Loss: 0.2826087027788162, Final Batch Loss: 0.1241479218006134\n",
      "Epoch 338, Loss: 0.26777803897857666, Final Batch Loss: 0.1387980878353119\n",
      "Epoch 339, Loss: 0.2702486515045166, Final Batch Loss: 0.16111691296100616\n",
      "Epoch 340, Loss: 0.2627246081829071, Final Batch Loss: 0.1290498822927475\n",
      "Epoch 341, Loss: 0.2751771807670593, Final Batch Loss: 0.0842931717634201\n",
      "Epoch 342, Loss: 0.374549999833107, Final Batch Loss: 0.19528888165950775\n",
      "Epoch 343, Loss: 0.28113432228565216, Final Batch Loss: 0.15311309695243835\n",
      "Epoch 344, Loss: 0.2611871734261513, Final Batch Loss: 0.1467815786600113\n",
      "Epoch 345, Loss: 0.2979780584573746, Final Batch Loss: 0.16706378757953644\n",
      "Epoch 346, Loss: 0.31463536620140076, Final Batch Loss: 0.1860305517911911\n",
      "Epoch 347, Loss: 0.34429584443569183, Final Batch Loss: 0.1875794380903244\n",
      "Epoch 348, Loss: 0.3687603920698166, Final Batch Loss: 0.21812765300273895\n",
      "Epoch 349, Loss: 0.30201391130685806, Final Batch Loss: 0.10239133983850479\n",
      "Epoch 350, Loss: 0.24914909899234772, Final Batch Loss: 0.08211439847946167\n",
      "Epoch 351, Loss: 0.24752376228570938, Final Batch Loss: 0.11121378093957901\n",
      "Epoch 352, Loss: 0.31175607442855835, Final Batch Loss: 0.14937430620193481\n",
      "Epoch 353, Loss: 0.252267524600029, Final Batch Loss: 0.0857560932636261\n",
      "Epoch 354, Loss: 0.27707769721746445, Final Batch Loss: 0.17136730253696442\n",
      "Epoch 355, Loss: 0.3102995902299881, Final Batch Loss: 0.1682589203119278\n",
      "Epoch 356, Loss: 0.2899269461631775, Final Batch Loss: 0.1386985331773758\n",
      "Epoch 357, Loss: 0.2815222293138504, Final Batch Loss: 0.1250676065683365\n",
      "Epoch 358, Loss: 0.27554427087306976, Final Batch Loss: 0.13420459628105164\n",
      "Epoch 359, Loss: 0.2603461220860481, Final Batch Loss: 0.12151836603879929\n",
      "Epoch 360, Loss: 0.23921623826026917, Final Batch Loss: 0.12133030593395233\n",
      "Epoch 361, Loss: 0.2626788690686226, Final Batch Loss: 0.11294344812631607\n",
      "Epoch 362, Loss: 0.35213205218315125, Final Batch Loss: 0.179040789604187\n",
      "Epoch 363, Loss: 0.2189267799258232, Final Batch Loss: 0.115999735891819\n",
      "Epoch 364, Loss: 0.2748875617980957, Final Batch Loss: 0.17289790511131287\n",
      "Epoch 365, Loss: 0.3112310916185379, Final Batch Loss: 0.1569119393825531\n",
      "Epoch 366, Loss: 0.2862482815980911, Final Batch Loss: 0.14607398211956024\n",
      "Epoch 367, Loss: 0.28036510199308395, Final Batch Loss: 0.1088443323969841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368, Loss: 0.36215628683567047, Final Batch Loss: 0.2080279141664505\n",
      "Epoch 369, Loss: 0.3249758780002594, Final Batch Loss: 0.19235074520111084\n",
      "Epoch 370, Loss: 0.2404775246977806, Final Batch Loss: 0.1342044621706009\n",
      "Epoch 371, Loss: 0.26114092767238617, Final Batch Loss: 0.12610676884651184\n",
      "Epoch 372, Loss: 0.31844931840896606, Final Batch Loss: 0.14973118901252747\n",
      "Epoch 373, Loss: 0.22128328680992126, Final Batch Loss: 0.11196095496416092\n",
      "Epoch 374, Loss: 0.24207116663455963, Final Batch Loss: 0.16834761202335358\n",
      "Epoch 375, Loss: 0.28213534504175186, Final Batch Loss: 0.11656389385461807\n",
      "Epoch 376, Loss: 0.21801511943340302, Final Batch Loss: 0.10462602972984314\n",
      "Epoch 377, Loss: 0.2657936289906502, Final Batch Loss: 0.10176167637109756\n",
      "Epoch 378, Loss: 0.2853250205516815, Final Batch Loss: 0.13664710521697998\n",
      "Epoch 379, Loss: 0.2279953882098198, Final Batch Loss: 0.1173020750284195\n",
      "Epoch 380, Loss: 0.21868230402469635, Final Batch Loss: 0.1028166115283966\n",
      "Epoch 381, Loss: 0.2565573677420616, Final Batch Loss: 0.1234784945845604\n",
      "Epoch 382, Loss: 0.30641013383865356, Final Batch Loss: 0.13816961646080017\n",
      "Epoch 383, Loss: 0.23802337050437927, Final Batch Loss: 0.1353069245815277\n",
      "Epoch 384, Loss: 0.2385500892996788, Final Batch Loss: 0.11092288047075272\n",
      "Epoch 385, Loss: 0.2797866612672806, Final Batch Loss: 0.15433913469314575\n",
      "Epoch 386, Loss: 0.21720970422029495, Final Batch Loss: 0.08282872289419174\n",
      "Epoch 387, Loss: 0.28707271814346313, Final Batch Loss: 0.14410986006259918\n",
      "Epoch 388, Loss: 0.2525542303919792, Final Batch Loss: 0.12901704013347626\n",
      "Epoch 389, Loss: 0.21257542818784714, Final Batch Loss: 0.08063653856515884\n",
      "Epoch 390, Loss: 0.22491956502199173, Final Batch Loss: 0.12115063518285751\n",
      "Epoch 391, Loss: 0.23931492120027542, Final Batch Loss: 0.13853657245635986\n",
      "Epoch 392, Loss: 0.2557640001177788, Final Batch Loss: 0.11696485430002213\n",
      "Epoch 393, Loss: 0.24321823567152023, Final Batch Loss: 0.1278020143508911\n",
      "Epoch 394, Loss: 0.20265020430088043, Final Batch Loss: 0.13197368383407593\n",
      "Epoch 395, Loss: 0.201565720140934, Final Batch Loss: 0.10037180781364441\n",
      "Epoch 396, Loss: 0.24119437485933304, Final Batch Loss: 0.11405577510595322\n",
      "Epoch 397, Loss: 0.24308957159519196, Final Batch Loss: 0.08409036695957184\n",
      "Epoch 398, Loss: 0.2682552635669708, Final Batch Loss: 0.14307168126106262\n",
      "Epoch 399, Loss: 0.26479610800743103, Final Batch Loss: 0.12891623377799988\n",
      "Epoch 400, Loss: 0.22811540216207504, Final Batch Loss: 0.1376512348651886\n",
      "Epoch 401, Loss: 0.2371883988380432, Final Batch Loss: 0.10663414001464844\n",
      "Epoch 402, Loss: 0.23522017151117325, Final Batch Loss: 0.1302000731229782\n",
      "Epoch 403, Loss: 0.2351459264755249, Final Batch Loss: 0.13615107536315918\n",
      "Epoch 404, Loss: 0.2798057869076729, Final Batch Loss: 0.185813307762146\n",
      "Epoch 405, Loss: 0.21754226088523865, Final Batch Loss: 0.12575416266918182\n",
      "Epoch 406, Loss: 0.24959561228752136, Final Batch Loss: 0.12242220342159271\n",
      "Epoch 407, Loss: 0.2537209391593933, Final Batch Loss: 0.14458897709846497\n",
      "Epoch 408, Loss: 0.22351286560297012, Final Batch Loss: 0.11871621012687683\n",
      "Epoch 409, Loss: 0.2478015273809433, Final Batch Loss: 0.13946138322353363\n",
      "Epoch 410, Loss: 0.24114418029785156, Final Batch Loss: 0.14431416988372803\n",
      "Epoch 411, Loss: 0.26821766048669815, Final Batch Loss: 0.16572223603725433\n",
      "Epoch 412, Loss: 0.22783564776182175, Final Batch Loss: 0.1319495588541031\n",
      "Epoch 413, Loss: 0.21908892691135406, Final Batch Loss: 0.14103713631629944\n",
      "Epoch 414, Loss: 0.2456505373120308, Final Batch Loss: 0.1178692951798439\n",
      "Epoch 415, Loss: 0.2583523616194725, Final Batch Loss: 0.14614559710025787\n",
      "Epoch 416, Loss: 0.21170677244663239, Final Batch Loss: 0.11888928711414337\n",
      "Epoch 417, Loss: 0.20645583420991898, Final Batch Loss: 0.10736490041017532\n",
      "Epoch 418, Loss: 0.24188768863677979, Final Batch Loss: 0.11825063824653625\n",
      "Epoch 419, Loss: 0.24996675550937653, Final Batch Loss: 0.1356727033853531\n",
      "Epoch 420, Loss: 0.25504785031080246, Final Batch Loss: 0.13074670732021332\n",
      "Epoch 421, Loss: 0.25656788796186447, Final Batch Loss: 0.08854097872972488\n",
      "Epoch 422, Loss: 0.2689644396305084, Final Batch Loss: 0.12607936561107635\n",
      "Epoch 423, Loss: 0.22681746631860733, Final Batch Loss: 0.10114846378564835\n",
      "Epoch 424, Loss: 0.21635059267282486, Final Batch Loss: 0.08548498898744583\n",
      "Epoch 425, Loss: 0.20674681663513184, Final Batch Loss: 0.08900973200798035\n",
      "Epoch 426, Loss: 0.1915602833032608, Final Batch Loss: 0.08803802728652954\n",
      "Epoch 427, Loss: 0.18045179545879364, Final Batch Loss: 0.07059317082166672\n",
      "Epoch 428, Loss: 0.3198878914117813, Final Batch Loss: 0.18598555028438568\n",
      "Epoch 429, Loss: 0.23602987825870514, Final Batch Loss: 0.08996060490608215\n",
      "Epoch 430, Loss: 0.2177034541964531, Final Batch Loss: 0.0962623581290245\n",
      "Epoch 431, Loss: 0.23147984594106674, Final Batch Loss: 0.12195733934640884\n",
      "Epoch 432, Loss: 0.22030892223119736, Final Batch Loss: 0.1016337051987648\n",
      "Epoch 433, Loss: 0.212356336414814, Final Batch Loss: 0.1375371366739273\n",
      "Epoch 434, Loss: 0.21912270039319992, Final Batch Loss: 0.09201640635728836\n",
      "Epoch 435, Loss: 0.24260825663805008, Final Batch Loss: 0.10034557431936264\n",
      "Epoch 436, Loss: 0.2682536393404007, Final Batch Loss: 0.17449980974197388\n",
      "Epoch 437, Loss: 0.20556630939245224, Final Batch Loss: 0.11591273546218872\n",
      "Epoch 438, Loss: 0.17985132336616516, Final Batch Loss: 0.09302810579538345\n",
      "Epoch 439, Loss: 0.19643854349851608, Final Batch Loss: 0.11423587799072266\n",
      "Epoch 440, Loss: 0.1834447830915451, Final Batch Loss: 0.08840231597423553\n",
      "Epoch 441, Loss: 0.23884126543998718, Final Batch Loss: 0.11609457433223724\n",
      "Epoch 442, Loss: 0.19335979968309402, Final Batch Loss: 0.05855678766965866\n",
      "Epoch 443, Loss: 0.23723047971725464, Final Batch Loss: 0.13290874660015106\n",
      "Epoch 444, Loss: 0.20240093022584915, Final Batch Loss: 0.09909412264823914\n",
      "Epoch 445, Loss: 0.18334928154945374, Final Batch Loss: 0.10673321038484573\n",
      "Epoch 446, Loss: 0.21180492639541626, Final Batch Loss: 0.11563169211149216\n",
      "Epoch 447, Loss: 0.26220206916332245, Final Batch Loss: 0.09738637506961823\n",
      "Epoch 448, Loss: 0.21530009806156158, Final Batch Loss: 0.13231982290744781\n",
      "Epoch 449, Loss: 0.23225558549165726, Final Batch Loss: 0.15177705883979797\n",
      "Epoch 450, Loss: 0.20146290957927704, Final Batch Loss: 0.09971507638692856\n",
      "Epoch 451, Loss: 0.17576848715543747, Final Batch Loss: 0.08367147296667099\n",
      "Epoch 452, Loss: 0.21822663396596909, Final Batch Loss: 0.07570298761129379\n",
      "Epoch 453, Loss: 0.20030485093593597, Final Batch Loss: 0.12034858018159866\n",
      "Epoch 454, Loss: 0.22254320234060287, Final Batch Loss: 0.1373298019170761\n",
      "Epoch 455, Loss: 0.21467678248882294, Final Batch Loss: 0.1209515631198883\n",
      "Epoch 456, Loss: 0.2063995748758316, Final Batch Loss: 0.10974340885877609\n",
      "Epoch 457, Loss: 0.2128099948167801, Final Batch Loss: 0.08388771116733551\n",
      "Epoch 458, Loss: 0.17159005999565125, Final Batch Loss: 0.08976505696773529\n",
      "Epoch 459, Loss: 0.21804478019475937, Final Batch Loss: 0.11162160336971283\n",
      "Epoch 460, Loss: 0.24049173295497894, Final Batch Loss: 0.12064819782972336\n",
      "Epoch 461, Loss: 0.22368048131465912, Final Batch Loss: 0.09413953125476837\n",
      "Epoch 462, Loss: 0.17016851156949997, Final Batch Loss: 0.08159428834915161\n",
      "Epoch 463, Loss: 0.20649049431085587, Final Batch Loss: 0.08578229695558548\n",
      "Epoch 464, Loss: 0.19979704171419144, Final Batch Loss: 0.10720092803239822\n",
      "Epoch 465, Loss: 0.19912805408239365, Final Batch Loss: 0.10529083758592606\n",
      "Epoch 466, Loss: 0.25662165880203247, Final Batch Loss: 0.116562619805336\n",
      "Epoch 467, Loss: 0.1835637241601944, Final Batch Loss: 0.08212647587060928\n",
      "Epoch 468, Loss: 0.1978679895401001, Final Batch Loss: 0.11900204420089722\n",
      "Epoch 469, Loss: 0.1776614412665367, Final Batch Loss: 0.09932658076286316\n",
      "Epoch 470, Loss: 0.183403380215168, Final Batch Loss: 0.09138805419206619\n",
      "Epoch 471, Loss: 0.18911518156528473, Final Batch Loss: 0.08094798773527145\n",
      "Epoch 472, Loss: 0.18875600397586823, Final Batch Loss: 0.11347002536058426\n",
      "Epoch 473, Loss: 0.20231852680444717, Final Batch Loss: 0.09838272631168365\n",
      "Epoch 474, Loss: 0.2564319521188736, Final Batch Loss: 0.10617540776729584\n",
      "Epoch 475, Loss: 0.26210546493530273, Final Batch Loss: 0.15984691679477692\n",
      "Epoch 476, Loss: 0.20578894764184952, Final Batch Loss: 0.09593009948730469\n",
      "Epoch 477, Loss: 0.2484622597694397, Final Batch Loss: 0.11513471603393555\n",
      "Epoch 478, Loss: 0.22322283685207367, Final Batch Loss: 0.12293130159378052\n",
      "Epoch 479, Loss: 0.22371739149093628, Final Batch Loss: 0.12830978631973267\n",
      "Epoch 480, Loss: 0.21435364335775375, Final Batch Loss: 0.1083187684416771\n",
      "Epoch 481, Loss: 0.2302432283759117, Final Batch Loss: 0.11539021134376526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 482, Loss: 0.2097921445965767, Final Batch Loss: 0.10215391218662262\n",
      "Epoch 483, Loss: 0.20769035816192627, Final Batch Loss: 0.10455331206321716\n",
      "Epoch 484, Loss: 0.16519523411989212, Final Batch Loss: 0.09101614356040955\n",
      "Epoch 485, Loss: 0.23069587349891663, Final Batch Loss: 0.12837155163288116\n",
      "Epoch 486, Loss: 0.18188375234603882, Final Batch Loss: 0.08608627319335938\n",
      "Epoch 487, Loss: 0.19632422924041748, Final Batch Loss: 0.10005683451890945\n",
      "Epoch 488, Loss: 0.29089125990867615, Final Batch Loss: 0.15368932485580444\n",
      "Epoch 489, Loss: 0.18978984653949738, Final Batch Loss: 0.08459505438804626\n",
      "Epoch 490, Loss: 0.17935581505298615, Final Batch Loss: 0.07321029901504517\n",
      "Epoch 491, Loss: 0.20031412690877914, Final Batch Loss: 0.11836648732423782\n",
      "Epoch 492, Loss: 0.18862804025411606, Final Batch Loss: 0.0787995234131813\n",
      "Epoch 493, Loss: 0.25033124536275864, Final Batch Loss: 0.16860820353031158\n",
      "Epoch 494, Loss: 0.18838780373334885, Final Batch Loss: 0.08081569522619247\n",
      "Epoch 495, Loss: 0.22258155047893524, Final Batch Loss: 0.12116377800703049\n",
      "Epoch 496, Loss: 0.17488384246826172, Final Batch Loss: 0.0884062796831131\n",
      "Epoch 497, Loss: 0.2061411738395691, Final Batch Loss: 0.0927158072590828\n",
      "Epoch 498, Loss: 0.24737532436847687, Final Batch Loss: 0.15998531877994537\n",
      "Epoch 499, Loss: 0.20004458725452423, Final Batch Loss: 0.07289557158946991\n",
      "Epoch 500, Loss: 0.15636353939771652, Final Batch Loss: 0.08091903477907181\n",
      "Epoch 501, Loss: 0.19473589956760406, Final Batch Loss: 0.08133193105459213\n",
      "Epoch 502, Loss: 0.2127782329916954, Final Batch Loss: 0.11189165711402893\n",
      "Epoch 503, Loss: 0.15566584467887878, Final Batch Loss: 0.06976811587810516\n",
      "Epoch 504, Loss: 0.18101875483989716, Final Batch Loss: 0.06954298913478851\n",
      "Epoch 505, Loss: 0.264933206140995, Final Batch Loss: 0.16195952892303467\n",
      "Epoch 506, Loss: 0.2063591331243515, Final Batch Loss: 0.08767463266849518\n",
      "Epoch 507, Loss: 0.16873794794082642, Final Batch Loss: 0.06425286084413528\n",
      "Epoch 508, Loss: 0.18963415175676346, Final Batch Loss: 0.0822136253118515\n",
      "Epoch 509, Loss: 0.2048104777932167, Final Batch Loss: 0.12345196306705475\n",
      "Epoch 510, Loss: 0.19189803302288055, Final Batch Loss: 0.10542033612728119\n",
      "Epoch 511, Loss: 0.2539590373635292, Final Batch Loss: 0.1244296059012413\n",
      "Epoch 512, Loss: 0.22811494022607803, Final Batch Loss: 0.1441139429807663\n",
      "Epoch 513, Loss: 0.17805975675582886, Final Batch Loss: 0.08826430886983871\n",
      "Epoch 514, Loss: 0.2720269188284874, Final Batch Loss: 0.11637748032808304\n",
      "Epoch 515, Loss: 0.21604619175195694, Final Batch Loss: 0.05659549683332443\n",
      "Epoch 516, Loss: 0.18999583274126053, Final Batch Loss: 0.10370422154664993\n",
      "Epoch 517, Loss: 0.2236049771308899, Final Batch Loss: 0.0966109186410904\n",
      "Epoch 518, Loss: 0.18845559656620026, Final Batch Loss: 0.08540766686201096\n",
      "Epoch 519, Loss: 0.2024170532822609, Final Batch Loss: 0.11141931265592575\n",
      "Epoch 520, Loss: 0.14734559506177902, Final Batch Loss: 0.08059301227331161\n",
      "Epoch 521, Loss: 0.19381989538669586, Final Batch Loss: 0.11485248059034348\n",
      "Epoch 522, Loss: 0.16562070697546005, Final Batch Loss: 0.06389597058296204\n",
      "Epoch 523, Loss: 0.22090119123458862, Final Batch Loss: 0.09367470443248749\n",
      "Epoch 524, Loss: 0.1708129420876503, Final Batch Loss: 0.06731760501861572\n",
      "Epoch 525, Loss: 0.192173071205616, Final Batch Loss: 0.12761326134204865\n",
      "Epoch 526, Loss: 0.1583181768655777, Final Batch Loss: 0.06523331254720688\n",
      "Epoch 527, Loss: 0.16735053062438965, Final Batch Loss: 0.0932219848036766\n",
      "Epoch 528, Loss: 0.21035126596689224, Final Batch Loss: 0.11308684200048447\n",
      "Epoch 529, Loss: 0.20806074887514114, Final Batch Loss: 0.10905014723539352\n",
      "Epoch 530, Loss: 0.18661706149578094, Final Batch Loss: 0.10697530955076218\n",
      "Epoch 531, Loss: 0.17842361330986023, Final Batch Loss: 0.07334929704666138\n",
      "Epoch 532, Loss: 0.1672053039073944, Final Batch Loss: 0.07602549344301224\n",
      "Epoch 533, Loss: 0.29115161299705505, Final Batch Loss: 0.1585710197687149\n",
      "Epoch 534, Loss: 0.19605734944343567, Final Batch Loss: 0.09700215607881546\n",
      "Epoch 535, Loss: 0.17369435727596283, Final Batch Loss: 0.09025580435991287\n",
      "Epoch 536, Loss: 0.18309787660837173, Final Batch Loss: 0.10494624823331833\n",
      "Epoch 537, Loss: 0.15288734436035156, Final Batch Loss: 0.08662782609462738\n",
      "Epoch 538, Loss: 0.17594905197620392, Final Batch Loss: 0.07164186239242554\n",
      "Epoch 539, Loss: 0.19576691091060638, Final Batch Loss: 0.07410339266061783\n",
      "Epoch 540, Loss: 0.23015768080949783, Final Batch Loss: 0.11648327857255936\n",
      "Epoch 541, Loss: 0.22597969323396683, Final Batch Loss: 0.11284355074167252\n",
      "Epoch 542, Loss: 0.24005410820245743, Final Batch Loss: 0.0799500122666359\n",
      "Epoch 543, Loss: 0.2128625512123108, Final Batch Loss: 0.1168920174241066\n",
      "Epoch 544, Loss: 0.17598693817853928, Final Batch Loss: 0.09869381040334702\n",
      "Epoch 545, Loss: 0.19683288782835007, Final Batch Loss: 0.08636508136987686\n",
      "Epoch 546, Loss: 0.1970938965678215, Final Batch Loss: 0.10274893045425415\n",
      "Epoch 547, Loss: 0.22173527628183365, Final Batch Loss: 0.07789381593465805\n",
      "Epoch 548, Loss: 0.173649363219738, Final Batch Loss: 0.10477066785097122\n",
      "Epoch 549, Loss: 0.20979303866624832, Final Batch Loss: 0.12498760223388672\n",
      "Epoch 550, Loss: 0.19189877808094025, Final Batch Loss: 0.11081334948539734\n",
      "Epoch 551, Loss: 0.16576237231492996, Final Batch Loss: 0.06762497872114182\n",
      "Epoch 552, Loss: 0.17065273225307465, Final Batch Loss: 0.10077840834856033\n",
      "Epoch 553, Loss: 0.22317096590995789, Final Batch Loss: 0.1457742303609848\n",
      "Epoch 554, Loss: 0.1640438660979271, Final Batch Loss: 0.09343895316123962\n",
      "Epoch 555, Loss: 0.1679621934890747, Final Batch Loss: 0.10414257645606995\n",
      "Epoch 556, Loss: 0.22027157247066498, Final Batch Loss: 0.13375690579414368\n",
      "Epoch 557, Loss: 0.2010279819369316, Final Batch Loss: 0.10450826585292816\n",
      "Epoch 558, Loss: 0.20716990530490875, Final Batch Loss: 0.0909150093793869\n",
      "Epoch 559, Loss: 0.1516755223274231, Final Batch Loss: 0.07964601367712021\n",
      "Epoch 560, Loss: 0.20484602451324463, Final Batch Loss: 0.11086910963058472\n",
      "Epoch 561, Loss: 0.16513199731707573, Final Batch Loss: 0.046322789043188095\n",
      "Epoch 562, Loss: 0.20397034287452698, Final Batch Loss: 0.13651816546916962\n",
      "Epoch 563, Loss: 0.18630577251315117, Final Batch Loss: 0.1245819479227066\n",
      "Epoch 564, Loss: 0.2068764865398407, Final Batch Loss: 0.11638301610946655\n",
      "Epoch 565, Loss: 0.2279076725244522, Final Batch Loss: 0.1102939173579216\n",
      "Epoch 566, Loss: 0.180820494890213, Final Batch Loss: 0.09938918799161911\n",
      "Epoch 567, Loss: 0.21330145001411438, Final Batch Loss: 0.08943706005811691\n",
      "Epoch 568, Loss: 0.13724268972873688, Final Batch Loss: 0.06478217244148254\n",
      "Epoch 569, Loss: 0.15344681590795517, Final Batch Loss: 0.0805300697684288\n",
      "Epoch 570, Loss: 0.20171833038330078, Final Batch Loss: 0.12601763010025024\n",
      "Epoch 571, Loss: 0.16018442809581757, Final Batch Loss: 0.06145205348730087\n",
      "Epoch 572, Loss: 0.2002926543354988, Final Batch Loss: 0.11830555647611618\n",
      "Epoch 573, Loss: 0.2074284851551056, Final Batch Loss: 0.09570649266242981\n",
      "Epoch 574, Loss: 0.16218216717243195, Final Batch Loss: 0.086125947535038\n",
      "Epoch 575, Loss: 0.20404940843582153, Final Batch Loss: 0.13860026001930237\n",
      "Epoch 576, Loss: 0.1480422206223011, Final Batch Loss: 0.0451696403324604\n",
      "Epoch 577, Loss: 0.2671470418572426, Final Batch Loss: 0.16684724390506744\n",
      "Epoch 578, Loss: 0.15016727894544601, Final Batch Loss: 0.0682622566819191\n",
      "Epoch 579, Loss: 0.17988796532154083, Final Batch Loss: 0.08957549184560776\n",
      "Epoch 580, Loss: 0.22146690636873245, Final Batch Loss: 0.11006451398134232\n",
      "Epoch 581, Loss: 0.17071081697940826, Final Batch Loss: 0.08048597723245621\n",
      "Epoch 582, Loss: 0.2008388489484787, Final Batch Loss: 0.13078254461288452\n",
      "Epoch 583, Loss: 0.2102099135518074, Final Batch Loss: 0.08762574195861816\n",
      "Epoch 584, Loss: 0.18125667423009872, Final Batch Loss: 0.08672785013914108\n",
      "Epoch 585, Loss: 0.1859203353524208, Final Batch Loss: 0.06631029397249222\n",
      "Epoch 586, Loss: 0.14314613491296768, Final Batch Loss: 0.07209364324808121\n",
      "Epoch 587, Loss: 0.18034318834543228, Final Batch Loss: 0.12750066816806793\n",
      "Epoch 588, Loss: 0.19122201204299927, Final Batch Loss: 0.10803457349538803\n",
      "Epoch 589, Loss: 0.15112566947937012, Final Batch Loss: 0.06511370837688446\n",
      "Epoch 590, Loss: 0.20771239697933197, Final Batch Loss: 0.1264762282371521\n",
      "Epoch 591, Loss: 0.18717999011278152, Final Batch Loss: 0.1056225448846817\n",
      "Epoch 592, Loss: 0.18994174897670746, Final Batch Loss: 0.09258013218641281\n",
      "Epoch 593, Loss: 0.1923838034272194, Final Batch Loss: 0.09731099009513855\n",
      "Epoch 594, Loss: 0.15391488373279572, Final Batch Loss: 0.08747532963752747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595, Loss: 0.22322409600019455, Final Batch Loss: 0.11286946386098862\n",
      "Epoch 596, Loss: 0.23758459836244583, Final Batch Loss: 0.13494278490543365\n",
      "Epoch 597, Loss: 0.17855551838874817, Final Batch Loss: 0.10152902454137802\n",
      "Epoch 598, Loss: 0.1718665137887001, Final Batch Loss: 0.07268117368221283\n",
      "Epoch 599, Loss: 0.1820598840713501, Final Batch Loss: 0.07408107817173004\n",
      "Epoch 600, Loss: 0.18654083460569382, Final Batch Loss: 0.11394423246383667\n",
      "Epoch 601, Loss: 0.2352740243077278, Final Batch Loss: 0.1324901580810547\n",
      "Epoch 602, Loss: 0.1904638186097145, Final Batch Loss: 0.08584272116422653\n",
      "Epoch 603, Loss: 0.15305162221193314, Final Batch Loss: 0.08339237421751022\n",
      "Epoch 604, Loss: 0.1440279446542263, Final Batch Loss: 0.05887765809893608\n",
      "Epoch 605, Loss: 0.1506466493010521, Final Batch Loss: 0.08582942932844162\n",
      "Epoch 606, Loss: 0.17298537492752075, Final Batch Loss: 0.07222874462604523\n",
      "Epoch 607, Loss: 0.21925168484449387, Final Batch Loss: 0.11390841752290726\n",
      "Epoch 608, Loss: 0.21449235081672668, Final Batch Loss: 0.09033497422933578\n",
      "Epoch 609, Loss: 0.2164522334933281, Final Batch Loss: 0.09858883917331696\n",
      "Epoch 610, Loss: 0.1876314878463745, Final Batch Loss: 0.10592543333768845\n",
      "Epoch 611, Loss: 0.15908144414424896, Final Batch Loss: 0.08527874201536179\n",
      "Epoch 612, Loss: 0.15981602668762207, Final Batch Loss: 0.07933846116065979\n",
      "Epoch 613, Loss: 0.17918553203344345, Final Batch Loss: 0.08868572115898132\n",
      "Epoch 614, Loss: 0.14624308049678802, Final Batch Loss: 0.06771444529294968\n",
      "Epoch 615, Loss: 0.23060546815395355, Final Batch Loss: 0.10354787111282349\n",
      "Epoch 616, Loss: 0.19421304762363434, Final Batch Loss: 0.10209708660840988\n",
      "Epoch 617, Loss: 0.15658770501613617, Final Batch Loss: 0.07807746529579163\n",
      "Epoch 618, Loss: 0.23706576228141785, Final Batch Loss: 0.13705696165561676\n",
      "Epoch 619, Loss: 0.1426270753145218, Final Batch Loss: 0.08079689741134644\n",
      "Epoch 620, Loss: 0.1994350701570511, Final Batch Loss: 0.08704876154661179\n",
      "Epoch 621, Loss: 0.16242124140262604, Final Batch Loss: 0.09797044098377228\n",
      "Epoch 622, Loss: 0.1727265603840351, Final Batch Loss: 0.043505508452653885\n",
      "Epoch 623, Loss: 0.2280614674091339, Final Batch Loss: 0.1264481246471405\n",
      "Epoch 624, Loss: 0.1772637516260147, Final Batch Loss: 0.08424293249845505\n",
      "Epoch 625, Loss: 0.1941644698381424, Final Batch Loss: 0.10673478990793228\n",
      "Epoch 626, Loss: 0.1991020143032074, Final Batch Loss: 0.09075330942869186\n",
      "Epoch 627, Loss: 0.2363090217113495, Final Batch Loss: 0.12314092367887497\n",
      "Epoch 628, Loss: 0.21793437004089355, Final Batch Loss: 0.11271525174379349\n",
      "Epoch 629, Loss: 0.17662500590085983, Final Batch Loss: 0.07034464180469513\n",
      "Epoch 630, Loss: 0.17766311019659042, Final Batch Loss: 0.08902665227651596\n",
      "Epoch 631, Loss: 0.20534180104732513, Final Batch Loss: 0.1030232310295105\n",
      "Epoch 632, Loss: 0.1784636154770851, Final Batch Loss: 0.1050758957862854\n",
      "Epoch 633, Loss: 0.2501463368535042, Final Batch Loss: 0.12139951437711716\n",
      "Epoch 634, Loss: 0.18274720758199692, Final Batch Loss: 0.07886745035648346\n",
      "Epoch 635, Loss: 0.18010792881250381, Final Batch Loss: 0.10480369627475739\n",
      "Epoch 636, Loss: 0.20551590621471405, Final Batch Loss: 0.11198735982179642\n",
      "Epoch 637, Loss: 0.29120883345603943, Final Batch Loss: 0.19372329115867615\n",
      "Epoch 638, Loss: 0.19972829520702362, Final Batch Loss: 0.11295203119516373\n",
      "Epoch 639, Loss: 0.14227715134620667, Final Batch Loss: 0.06577102094888687\n",
      "Epoch 640, Loss: 0.18652725219726562, Final Batch Loss: 0.11905403435230255\n",
      "Epoch 641, Loss: 0.18480738997459412, Final Batch Loss: 0.09751960635185242\n",
      "Epoch 642, Loss: 0.1725311502814293, Final Batch Loss: 0.06485521048307419\n",
      "Epoch 643, Loss: 0.19542402029037476, Final Batch Loss: 0.08265808969736099\n",
      "Epoch 644, Loss: 0.19297144562005997, Final Batch Loss: 0.10502753406763077\n",
      "Epoch 645, Loss: 0.13789018988609314, Final Batch Loss: 0.06153906136751175\n",
      "Epoch 646, Loss: 0.21571192890405655, Final Batch Loss: 0.12270347028970718\n",
      "Epoch 647, Loss: 0.25229284912347794, Final Batch Loss: 0.10683945566415787\n",
      "Epoch 648, Loss: 0.21557296812534332, Final Batch Loss: 0.062342479825019836\n",
      "Epoch 649, Loss: 0.11339443549513817, Final Batch Loss: 0.04726697877049446\n",
      "Epoch 650, Loss: 0.1865127608180046, Final Batch Loss: 0.08411243557929993\n",
      "Epoch 651, Loss: 0.19021127372980118, Final Batch Loss: 0.09299150854349136\n",
      "Epoch 652, Loss: 0.18715019524097443, Final Batch Loss: 0.07971149682998657\n",
      "Epoch 653, Loss: 0.17443668097257614, Final Batch Loss: 0.0906776487827301\n",
      "Epoch 654, Loss: 0.17466440051794052, Final Batch Loss: 0.10232234001159668\n",
      "Epoch 655, Loss: 0.21717041730880737, Final Batch Loss: 0.13563694059848785\n",
      "Epoch 656, Loss: 0.20157353579998016, Final Batch Loss: 0.11894579231739044\n",
      "Epoch 657, Loss: 0.15548404306173325, Final Batch Loss: 0.10845518857240677\n",
      "Epoch 658, Loss: 0.19266219809651375, Final Batch Loss: 0.13559187948703766\n",
      "Epoch 659, Loss: 0.19093114882707596, Final Batch Loss: 0.07378124445676804\n",
      "Epoch 660, Loss: 0.22886892408132553, Final Batch Loss: 0.10815101861953735\n",
      "Epoch 661, Loss: 0.14913517981767654, Final Batch Loss: 0.07915927469730377\n",
      "Epoch 662, Loss: 0.1512601003050804, Final Batch Loss: 0.0730985552072525\n",
      "Epoch 663, Loss: 0.14904557168483734, Final Batch Loss: 0.08037721365690231\n",
      "Epoch 664, Loss: 0.25186602771282196, Final Batch Loss: 0.1349172741174698\n",
      "Epoch 665, Loss: 0.16789128631353378, Final Batch Loss: 0.08394771814346313\n",
      "Epoch 666, Loss: 0.2006320059299469, Final Batch Loss: 0.07855929434299469\n",
      "Epoch 667, Loss: 0.15263261646032333, Final Batch Loss: 0.08301916718482971\n",
      "Epoch 668, Loss: 0.1969389244914055, Final Batch Loss: 0.08899310231208801\n",
      "Epoch 669, Loss: 0.2544091194868088, Final Batch Loss: 0.15085336565971375\n",
      "Epoch 670, Loss: 0.13919411599636078, Final Batch Loss: 0.08562178164720535\n",
      "Epoch 671, Loss: 0.14073582366108894, Final Batch Loss: 0.05631089583039284\n",
      "Epoch 672, Loss: 0.2587355449795723, Final Batch Loss: 0.17915961146354675\n",
      "Epoch 673, Loss: 0.17628826946020126, Final Batch Loss: 0.0693979412317276\n",
      "Epoch 674, Loss: 0.17073818296194077, Final Batch Loss: 0.11933520436286926\n",
      "Epoch 675, Loss: 0.1835511140525341, Final Batch Loss: 0.05244142934679985\n",
      "Epoch 676, Loss: 0.17950938642024994, Final Batch Loss: 0.1255020648241043\n",
      "Epoch 677, Loss: 0.2011728659272194, Final Batch Loss: 0.08342279493808746\n",
      "Epoch 678, Loss: 0.15263136476278305, Final Batch Loss: 0.07501707971096039\n",
      "Epoch 679, Loss: 0.1460994929075241, Final Batch Loss: 0.08027753978967667\n",
      "Epoch 680, Loss: 0.17229719460010529, Final Batch Loss: 0.07121633738279343\n",
      "Epoch 681, Loss: 0.14812008291482925, Final Batch Loss: 0.07177440077066422\n",
      "Epoch 682, Loss: 0.15751782432198524, Final Batch Loss: 0.10039931535720825\n",
      "Epoch 683, Loss: 0.15948936343193054, Final Batch Loss: 0.06397190690040588\n",
      "Epoch 684, Loss: 0.20000724494457245, Final Batch Loss: 0.10066112875938416\n",
      "Epoch 685, Loss: 0.21286645531654358, Final Batch Loss: 0.11561170220375061\n",
      "Epoch 686, Loss: 0.28003208339214325, Final Batch Loss: 0.14416192471981049\n",
      "Epoch 687, Loss: 0.14645473659038544, Final Batch Loss: 0.07107409089803696\n",
      "Epoch 688, Loss: 0.18141498416662216, Final Batch Loss: 0.10390062630176544\n",
      "Epoch 689, Loss: 0.17075234651565552, Final Batch Loss: 0.0886428952217102\n",
      "Epoch 690, Loss: 0.17693398892879486, Final Batch Loss: 0.07474400103092194\n",
      "Epoch 691, Loss: 0.16187923401594162, Final Batch Loss: 0.08599476516246796\n",
      "Epoch 692, Loss: 0.17931637912988663, Final Batch Loss: 0.0834483727812767\n",
      "Epoch 693, Loss: 0.16299723088741302, Final Batch Loss: 0.0659632757306099\n",
      "Epoch 694, Loss: 0.28272204101085663, Final Batch Loss: 0.17276188731193542\n",
      "Epoch 695, Loss: 0.16446799039840698, Final Batch Loss: 0.08970459550619125\n",
      "Epoch 696, Loss: 0.16364933550357819, Final Batch Loss: 0.07494398951530457\n",
      "Epoch 697, Loss: 0.20751181989908218, Final Batch Loss: 0.12582643330097198\n",
      "Epoch 698, Loss: 0.19159305095672607, Final Batch Loss: 0.10433997958898544\n",
      "Epoch 699, Loss: 0.15334946662187576, Final Batch Loss: 0.06383066624403\n",
      "Epoch 700, Loss: 0.1472257524728775, Final Batch Loss: 0.06315900385379791\n",
      "Epoch 701, Loss: 0.2217940166592598, Final Batch Loss: 0.1036679595708847\n",
      "Epoch 702, Loss: 0.16675705462694168, Final Batch Loss: 0.08768324553966522\n",
      "Epoch 703, Loss: 0.14490924775600433, Final Batch Loss: 0.06784995645284653\n",
      "Epoch 704, Loss: 0.15546472743153572, Final Batch Loss: 0.05629313364624977\n",
      "Epoch 705, Loss: 0.11984146386384964, Final Batch Loss: 0.07645149528980255\n",
      "Epoch 706, Loss: 0.17109544575214386, Final Batch Loss: 0.09300674498081207\n",
      "Epoch 707, Loss: 0.1549891233444214, Final Batch Loss: 0.08228551596403122\n",
      "Epoch 708, Loss: 0.17918245494365692, Final Batch Loss: 0.06410976499319077\n",
      "Epoch 709, Loss: 0.11544381827116013, Final Batch Loss: 0.07201740890741348\n",
      "Epoch 710, Loss: 0.14257045462727547, Final Batch Loss: 0.06161472573876381\n",
      "Epoch 711, Loss: 0.1564277708530426, Final Batch Loss: 0.07358180731534958\n",
      "Epoch 712, Loss: 0.1434907428920269, Final Batch Loss: 0.06129864975810051\n",
      "Epoch 713, Loss: 0.16284451633691788, Final Batch Loss: 0.06541607528924942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714, Loss: 0.11213862895965576, Final Batch Loss: 0.05460347980260849\n",
      "Epoch 715, Loss: 0.17124979943037033, Final Batch Loss: 0.08112949877977371\n",
      "Epoch 716, Loss: 0.21858303248882294, Final Batch Loss: 0.07660247385501862\n",
      "Epoch 717, Loss: 0.15510226041078568, Final Batch Loss: 0.08756604790687561\n",
      "Epoch 718, Loss: 0.14607049897313118, Final Batch Loss: 0.08466335386037827\n",
      "Epoch 719, Loss: 0.15828874707221985, Final Batch Loss: 0.06971775740385056\n",
      "Epoch 720, Loss: 0.14488229528069496, Final Batch Loss: 0.09212714433670044\n",
      "Epoch 721, Loss: 0.1523977816104889, Final Batch Loss: 0.06490952521562576\n",
      "Epoch 722, Loss: 0.18022827059030533, Final Batch Loss: 0.11658838391304016\n",
      "Epoch 723, Loss: 0.14836479723453522, Final Batch Loss: 0.07435217499732971\n",
      "Epoch 724, Loss: 0.149702750146389, Final Batch Loss: 0.08753808587789536\n",
      "Epoch 725, Loss: 0.1475142240524292, Final Batch Loss: 0.0764266848564148\n",
      "Epoch 726, Loss: 0.1827009990811348, Final Batch Loss: 0.12989391386508942\n",
      "Epoch 727, Loss: 0.1433303840458393, Final Batch Loss: 0.08449297398328781\n",
      "Epoch 728, Loss: 0.15027409046888351, Final Batch Loss: 0.08357900381088257\n",
      "Epoch 729, Loss: 0.22982246428728104, Final Batch Loss: 0.11447490006685257\n",
      "Epoch 730, Loss: 0.1366296336054802, Final Batch Loss: 0.052212879061698914\n",
      "Epoch 731, Loss: 0.17383840680122375, Final Batch Loss: 0.104603610932827\n",
      "Epoch 732, Loss: 0.1426156535744667, Final Batch Loss: 0.07889135926961899\n",
      "Epoch 733, Loss: 0.16002193838357925, Final Batch Loss: 0.09229782968759537\n",
      "Epoch 734, Loss: 0.17079957574605942, Final Batch Loss: 0.09062603861093521\n",
      "Epoch 735, Loss: 0.14514557272195816, Final Batch Loss: 0.08497972786426544\n",
      "Epoch 736, Loss: 0.13725515827536583, Final Batch Loss: 0.07982948422431946\n",
      "Epoch 737, Loss: 0.17627614736557007, Final Batch Loss: 0.08195799589157104\n",
      "Epoch 738, Loss: 0.18419556319713593, Final Batch Loss: 0.07753287255764008\n",
      "Epoch 739, Loss: 0.1790059357881546, Final Batch Loss: 0.11916624009609222\n",
      "Epoch 740, Loss: 0.15594862028956413, Final Batch Loss: 0.09471393376588821\n",
      "Epoch 741, Loss: 0.15661205351352692, Final Batch Loss: 0.09541986882686615\n",
      "Epoch 742, Loss: 0.1571597307920456, Final Batch Loss: 0.0709562823176384\n",
      "Epoch 743, Loss: 0.143526341766119, Final Batch Loss: 0.0535767488181591\n",
      "Epoch 744, Loss: 0.1719028204679489, Final Batch Loss: 0.06080465018749237\n",
      "Epoch 745, Loss: 0.16889696195721626, Final Batch Loss: 0.060317572206258774\n",
      "Epoch 746, Loss: 0.18596737831830978, Final Batch Loss: 0.07957304269075394\n",
      "Epoch 747, Loss: 0.16407475620508194, Final Batch Loss: 0.08149426430463791\n",
      "Epoch 748, Loss: 0.1806253083050251, Final Batch Loss: 0.1215258538722992\n",
      "Epoch 749, Loss: 0.25804857909679413, Final Batch Loss: 0.15875431895256042\n",
      "Epoch 750, Loss: 0.23492686450481415, Final Batch Loss: 0.11574894189834595\n",
      "Epoch 751, Loss: 0.1832696720957756, Final Batch Loss: 0.08302049338817596\n",
      "Epoch 752, Loss: 0.18232443183660507, Final Batch Loss: 0.1136898323893547\n",
      "Epoch 753, Loss: 0.14745264500379562, Final Batch Loss: 0.07903269678354263\n",
      "Epoch 754, Loss: 0.1975017450749874, Final Batch Loss: 0.13776005804538727\n",
      "Epoch 755, Loss: 0.15047629177570343, Final Batch Loss: 0.06549903750419617\n",
      "Epoch 756, Loss: 0.150322824716568, Final Batch Loss: 0.08953377604484558\n",
      "Epoch 757, Loss: 0.12715892493724823, Final Batch Loss: 0.04432697594165802\n",
      "Epoch 758, Loss: 0.14977560937404633, Final Batch Loss: 0.07568758726119995\n",
      "Epoch 759, Loss: 0.13001428544521332, Final Batch Loss: 0.06682552397251129\n",
      "Epoch 760, Loss: 0.17121653258800507, Final Batch Loss: 0.07873328775167465\n",
      "Epoch 761, Loss: 0.14511503279209137, Final Batch Loss: 0.08337459713220596\n",
      "Epoch 762, Loss: 0.18306195735931396, Final Batch Loss: 0.09309600293636322\n",
      "Epoch 763, Loss: 0.18097399175167084, Final Batch Loss: 0.10090546309947968\n",
      "Epoch 764, Loss: 0.15506254881620407, Final Batch Loss: 0.09220778197050095\n",
      "Epoch 765, Loss: 0.15375643968582153, Final Batch Loss: 0.0835946649312973\n",
      "Epoch 766, Loss: 0.16771718114614487, Final Batch Loss: 0.09516485780477524\n",
      "Epoch 767, Loss: 0.14344774186611176, Final Batch Loss: 0.0738377720117569\n",
      "Epoch 768, Loss: 0.1579796001315117, Final Batch Loss: 0.10581721365451813\n",
      "Epoch 769, Loss: 0.13352900370955467, Final Batch Loss: 0.07415502518415451\n",
      "Epoch 770, Loss: 0.1424117498099804, Final Batch Loss: 0.061817269772291183\n",
      "Epoch 771, Loss: 0.16333561390638351, Final Batch Loss: 0.0685867965221405\n",
      "Epoch 772, Loss: 0.16082713752985, Final Batch Loss: 0.06994567811489105\n",
      "Epoch 773, Loss: 0.16457706689834595, Final Batch Loss: 0.06608210504055023\n",
      "Epoch 774, Loss: 0.16934724897146225, Final Batch Loss: 0.10474792122840881\n",
      "Epoch 775, Loss: 0.13485677912831306, Final Batch Loss: 0.045275088399648666\n",
      "Epoch 776, Loss: 0.15469108149409294, Final Batch Loss: 0.09384436905384064\n",
      "Epoch 777, Loss: 0.19337733834981918, Final Batch Loss: 0.08374638110399246\n",
      "Epoch 778, Loss: 0.20422980934381485, Final Batch Loss: 0.0932486429810524\n",
      "Epoch 779, Loss: 0.13553638011217117, Final Batch Loss: 0.06662076711654663\n",
      "Epoch 780, Loss: 0.20970012247562408, Final Batch Loss: 0.08105936646461487\n",
      "Epoch 781, Loss: 0.2036132737994194, Final Batch Loss: 0.07822804898023605\n",
      "Epoch 782, Loss: 0.15237347036600113, Final Batch Loss: 0.06367029994726181\n",
      "Epoch 783, Loss: 0.21015820652246475, Final Batch Loss: 0.08435828238725662\n",
      "Epoch 784, Loss: 0.1080673597753048, Final Batch Loss: 0.057326123118400574\n",
      "Epoch 785, Loss: 0.15181610360741615, Final Batch Loss: 0.11930248141288757\n",
      "Epoch 786, Loss: 0.15030967444181442, Final Batch Loss: 0.04555065184831619\n",
      "Epoch 787, Loss: 0.1628081426024437, Final Batch Loss: 0.07913234829902649\n",
      "Epoch 788, Loss: 0.12486541271209717, Final Batch Loss: 0.06435250490903854\n",
      "Epoch 789, Loss: 0.18228699266910553, Final Batch Loss: 0.07794433832168579\n",
      "Epoch 790, Loss: 0.1564793810248375, Final Batch Loss: 0.0815078392624855\n",
      "Epoch 791, Loss: 0.12893015518784523, Final Batch Loss: 0.055017393082380295\n",
      "Epoch 792, Loss: 0.16321486234664917, Final Batch Loss: 0.09996548295021057\n",
      "Epoch 793, Loss: 0.2160680666565895, Final Batch Loss: 0.11505302041769028\n",
      "Epoch 794, Loss: 0.14941930025815964, Final Batch Loss: 0.0763668641448021\n",
      "Epoch 795, Loss: 0.16069303452968597, Final Batch Loss: 0.08616427332162857\n",
      "Epoch 796, Loss: 0.14141028374433517, Final Batch Loss: 0.08050430566072464\n",
      "Epoch 797, Loss: 0.1141391433775425, Final Batch Loss: 0.04878098890185356\n",
      "Epoch 798, Loss: 0.13658324629068375, Final Batch Loss: 0.08619602769613266\n",
      "Epoch 799, Loss: 0.1279759779572487, Final Batch Loss: 0.05774826556444168\n",
      "Epoch 800, Loss: 0.1351786032319069, Final Batch Loss: 0.06650964170694351\n",
      "Epoch 801, Loss: 0.16441470384597778, Final Batch Loss: 0.08225484192371368\n",
      "Epoch 802, Loss: 0.16461651027202606, Final Batch Loss: 0.09462004154920578\n",
      "Epoch 803, Loss: 0.1507076844573021, Final Batch Loss: 0.08659003674983978\n",
      "Epoch 804, Loss: 0.1787249594926834, Final Batch Loss: 0.10124410688877106\n",
      "Epoch 805, Loss: 0.1697019413113594, Final Batch Loss: 0.10216540098190308\n",
      "Epoch 806, Loss: 0.1497730016708374, Final Batch Loss: 0.07333756983280182\n",
      "Epoch 807, Loss: 0.17250338941812515, Final Batch Loss: 0.06564823538064957\n",
      "Epoch 808, Loss: 0.16736338660120964, Final Batch Loss: 0.10508891940116882\n",
      "Epoch 809, Loss: 0.14680000022053719, Final Batch Loss: 0.0973624661564827\n",
      "Epoch 810, Loss: 0.1473029889166355, Final Batch Loss: 0.08855362236499786\n",
      "Epoch 811, Loss: 0.12519867718219757, Final Batch Loss: 0.06713336706161499\n",
      "Epoch 812, Loss: 0.16104082018136978, Final Batch Loss: 0.08842196315526962\n",
      "Epoch 813, Loss: 0.13559381663799286, Final Batch Loss: 0.08141618967056274\n",
      "Epoch 814, Loss: 0.1485450640320778, Final Batch Loss: 0.07608363032341003\n",
      "Epoch 815, Loss: 0.19088568538427353, Final Batch Loss: 0.12109619379043579\n",
      "Epoch 816, Loss: 0.19143467396497726, Final Batch Loss: 0.0762035995721817\n",
      "Epoch 817, Loss: 0.2525327578186989, Final Batch Loss: 0.09085958451032639\n",
      "Epoch 818, Loss: 0.13776366412639618, Final Batch Loss: 0.07122394442558289\n",
      "Epoch 819, Loss: 0.1945638433098793, Final Batch Loss: 0.09558895230293274\n",
      "Epoch 820, Loss: 0.17717890441417694, Final Batch Loss: 0.07404006272554398\n",
      "Epoch 821, Loss: 0.1725413054227829, Final Batch Loss: 0.07650257647037506\n",
      "Epoch 822, Loss: 0.13188131898641586, Final Batch Loss: 0.04997730255126953\n",
      "Epoch 823, Loss: 0.19358154386281967, Final Batch Loss: 0.12011914700269699\n",
      "Epoch 824, Loss: 0.1736922264099121, Final Batch Loss: 0.07801260054111481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 825, Loss: 0.16261133551597595, Final Batch Loss: 0.08435998857021332\n",
      "Epoch 826, Loss: 0.17421207576990128, Final Batch Loss: 0.11638443917036057\n",
      "Epoch 827, Loss: 0.14604419469833374, Final Batch Loss: 0.07454390078783035\n",
      "Epoch 828, Loss: 0.14904636144638062, Final Batch Loss: 0.0656455010175705\n",
      "Epoch 829, Loss: 0.16804689913988113, Final Batch Loss: 0.07207010686397552\n",
      "Epoch 830, Loss: 0.12875349074602127, Final Batch Loss: 0.05975016951560974\n",
      "Epoch 831, Loss: 0.17276109009981155, Final Batch Loss: 0.08878712356090546\n",
      "Epoch 832, Loss: 0.15870946645736694, Final Batch Loss: 0.07472661137580872\n",
      "Epoch 833, Loss: 0.27244942262768745, Final Batch Loss: 0.21752090752124786\n",
      "Epoch 834, Loss: 0.1628449559211731, Final Batch Loss: 0.07577820122241974\n",
      "Epoch 835, Loss: 0.15302465856075287, Final Batch Loss: 0.09821423143148422\n",
      "Epoch 836, Loss: 0.15007638558745384, Final Batch Loss: 0.05612650886178017\n",
      "Epoch 837, Loss: 0.15653695911169052, Final Batch Loss: 0.06694922596216202\n",
      "Epoch 838, Loss: 0.11228740587830544, Final Batch Loss: 0.05660415440797806\n",
      "Epoch 839, Loss: 0.21072454750537872, Final Batch Loss: 0.06514190137386322\n",
      "Epoch 840, Loss: 0.14574183896183968, Final Batch Loss: 0.09261825680732727\n",
      "Epoch 841, Loss: 0.17837968468666077, Final Batch Loss: 0.11684862524271011\n",
      "Epoch 842, Loss: 0.1400599107146263, Final Batch Loss: 0.07053133845329285\n",
      "Epoch 843, Loss: 0.15375473350286484, Final Batch Loss: 0.08566708117723465\n",
      "Epoch 844, Loss: 0.1485636830329895, Final Batch Loss: 0.07749894261360168\n",
      "Epoch 845, Loss: 0.1284593604505062, Final Batch Loss: 0.0743236169219017\n",
      "Epoch 846, Loss: 0.17611686885356903, Final Batch Loss: 0.09864257276058197\n",
      "Epoch 847, Loss: 0.19167673587799072, Final Batch Loss: 0.11752449721097946\n",
      "Epoch 848, Loss: 0.16952607780694962, Final Batch Loss: 0.10699030011892319\n",
      "Epoch 849, Loss: 0.18960972130298615, Final Batch Loss: 0.07725096493959427\n",
      "Epoch 850, Loss: 0.19894332438707352, Final Batch Loss: 0.10814029723405838\n",
      "Epoch 851, Loss: 0.14590013027191162, Final Batch Loss: 0.04221988469362259\n",
      "Epoch 852, Loss: 0.2502790465950966, Final Batch Loss: 0.13561612367630005\n",
      "Epoch 853, Loss: 0.17362989485263824, Final Batch Loss: 0.08409169316291809\n",
      "Epoch 854, Loss: 0.1543716937303543, Final Batch Loss: 0.08496461063623428\n",
      "Epoch 855, Loss: 0.17417726665735245, Final Batch Loss: 0.11188460886478424\n",
      "Epoch 856, Loss: 0.1577957719564438, Final Batch Loss: 0.08258121460676193\n",
      "Epoch 857, Loss: 0.14683732390403748, Final Batch Loss: 0.07292930036783218\n",
      "Epoch 858, Loss: 0.17366911470890045, Final Batch Loss: 0.08550063520669937\n",
      "Epoch 859, Loss: 0.14735737442970276, Final Batch Loss: 0.07196540385484695\n",
      "Epoch 860, Loss: 0.164173923432827, Final Batch Loss: 0.09402003884315491\n",
      "Epoch 861, Loss: 0.11594943329691887, Final Batch Loss: 0.05583396926522255\n",
      "Epoch 862, Loss: 0.15210307389497757, Final Batch Loss: 0.0794641375541687\n",
      "Epoch 863, Loss: 0.13073407113552094, Final Batch Loss: 0.05611349642276764\n",
      "Epoch 864, Loss: 0.13141340389847755, Final Batch Loss: 0.08623591810464859\n",
      "Epoch 865, Loss: 0.16320015862584114, Final Batch Loss: 0.10138741135597229\n",
      "Epoch 866, Loss: 0.14203599095344543, Final Batch Loss: 0.0766952708363533\n",
      "Epoch 867, Loss: 0.1910482719540596, Final Batch Loss: 0.08661165088415146\n",
      "Epoch 868, Loss: 0.14663786441087723, Final Batch Loss: 0.07340256124734879\n",
      "Epoch 869, Loss: 0.1370287425816059, Final Batch Loss: 0.08351974934339523\n",
      "Epoch 870, Loss: 0.15048637986183167, Final Batch Loss: 0.07654448598623276\n",
      "Epoch 871, Loss: 0.12603553012013435, Final Batch Loss: 0.06106224283576012\n",
      "Epoch 872, Loss: 0.13593719899654388, Final Batch Loss: 0.07874101400375366\n",
      "Epoch 873, Loss: 0.16283392161130905, Final Batch Loss: 0.058476708829402924\n",
      "Epoch 874, Loss: 0.14112786948680878, Final Batch Loss: 0.06670571863651276\n",
      "Epoch 875, Loss: 0.10661587491631508, Final Batch Loss: 0.05115050449967384\n",
      "Epoch 876, Loss: 0.21672634780406952, Final Batch Loss: 0.14982101321220398\n",
      "Epoch 877, Loss: 0.1717529371380806, Final Batch Loss: 0.07359319180250168\n",
      "Epoch 878, Loss: 0.12706013023853302, Final Batch Loss: 0.06613665074110031\n",
      "Epoch 879, Loss: 0.11836117506027222, Final Batch Loss: 0.056015435606241226\n",
      "Epoch 880, Loss: 0.10994140431284904, Final Batch Loss: 0.059657786041498184\n",
      "Epoch 881, Loss: 0.10104067251086235, Final Batch Loss: 0.035147931426763535\n",
      "Epoch 882, Loss: 0.1393263153731823, Final Batch Loss: 0.07686333358287811\n",
      "Epoch 883, Loss: 0.11620768159627914, Final Batch Loss: 0.04260002821683884\n",
      "Epoch 884, Loss: 0.14567606896162033, Final Batch Loss: 0.09428250044584274\n",
      "Epoch 885, Loss: 0.15971370786428452, Final Batch Loss: 0.08882810175418854\n",
      "Epoch 886, Loss: 0.1483568698167801, Final Batch Loss: 0.08198275417089462\n",
      "Epoch 887, Loss: 0.15339671447873116, Final Batch Loss: 0.10269294679164886\n",
      "Epoch 888, Loss: 0.12644464895129204, Final Batch Loss: 0.055025000125169754\n",
      "Epoch 889, Loss: 0.134688101708889, Final Batch Loss: 0.0912550762295723\n",
      "Epoch 890, Loss: 0.15800613164901733, Final Batch Loss: 0.09081330895423889\n",
      "Epoch 891, Loss: 0.13775435835123062, Final Batch Loss: 0.06759613752365112\n",
      "Epoch 892, Loss: 0.16131363809108734, Final Batch Loss: 0.06400290131568909\n",
      "Epoch 893, Loss: 0.14214392006397247, Final Batch Loss: 0.05779755115509033\n",
      "Epoch 894, Loss: 0.1777377910912037, Final Batch Loss: 0.054758910089731216\n",
      "Epoch 895, Loss: 0.12374696880578995, Final Batch Loss: 0.04313051700592041\n",
      "Epoch 896, Loss: 0.14259101822972298, Final Batch Loss: 0.08671186864376068\n",
      "Epoch 897, Loss: 0.17883053794503212, Final Batch Loss: 0.11725667864084244\n",
      "Epoch 898, Loss: 0.13558578491210938, Final Batch Loss: 0.0868849828839302\n",
      "Epoch 899, Loss: 0.13002749532461166, Final Batch Loss: 0.07521586865186691\n",
      "Epoch 900, Loss: 0.1935456395149231, Final Batch Loss: 0.08546805381774902\n",
      "Epoch 901, Loss: 0.15105756744742393, Final Batch Loss: 0.060275498777627945\n",
      "Epoch 902, Loss: 0.13325535506010056, Final Batch Loss: 0.06471134722232819\n",
      "Epoch 903, Loss: 0.14903734996914864, Final Batch Loss: 0.054848555475473404\n",
      "Epoch 904, Loss: 0.14151613414287567, Final Batch Loss: 0.05930263549089432\n",
      "Epoch 905, Loss: 0.17979514971375465, Final Batch Loss: 0.1326713114976883\n",
      "Epoch 906, Loss: 0.2633553370833397, Final Batch Loss: 0.1853221207857132\n",
      "Epoch 907, Loss: 0.11241975799202919, Final Batch Loss: 0.06696797907352448\n",
      "Epoch 908, Loss: 0.16745426878333092, Final Batch Loss: 0.04782014712691307\n",
      "Epoch 909, Loss: 0.1743329092860222, Final Batch Loss: 0.09172692149877548\n",
      "Epoch 910, Loss: 0.14337558299303055, Final Batch Loss: 0.07307570427656174\n",
      "Epoch 911, Loss: 0.1300138719379902, Final Batch Loss: 0.05959315970540047\n",
      "Epoch 912, Loss: 0.19872207194566727, Final Batch Loss: 0.11266203224658966\n",
      "Epoch 913, Loss: 0.14087465405464172, Final Batch Loss: 0.1006777361035347\n",
      "Epoch 914, Loss: 0.12356144562363625, Final Batch Loss: 0.0696733221411705\n",
      "Epoch 915, Loss: 0.13667110726237297, Final Batch Loss: 0.05537114664912224\n",
      "Epoch 916, Loss: 0.13068228960037231, Final Batch Loss: 0.06722605228424072\n",
      "Epoch 917, Loss: 0.14218400418758392, Final Batch Loss: 0.07192631810903549\n",
      "Epoch 918, Loss: 0.15560328215360641, Final Batch Loss: 0.10284320265054703\n",
      "Epoch 919, Loss: 0.1229536160826683, Final Batch Loss: 0.06507609039545059\n",
      "Epoch 920, Loss: 0.16395029425621033, Final Batch Loss: 0.07229660451412201\n",
      "Epoch 921, Loss: 0.13818848133087158, Final Batch Loss: 0.06355902552604675\n",
      "Epoch 922, Loss: 0.15634328499436378, Final Batch Loss: 0.10535833239555359\n",
      "Epoch 923, Loss: 0.12188032642006874, Final Batch Loss: 0.061449017375707626\n",
      "Epoch 924, Loss: 0.16198310256004333, Final Batch Loss: 0.07472018897533417\n",
      "Epoch 925, Loss: 0.14003581926226616, Final Batch Loss: 0.06094982847571373\n",
      "Epoch 926, Loss: 0.14246519654989243, Final Batch Loss: 0.07966863363981247\n",
      "Epoch 927, Loss: 0.1291082538664341, Final Batch Loss: 0.053988266736269\n",
      "Epoch 928, Loss: 0.14453230425715446, Final Batch Loss: 0.053257156163454056\n",
      "Epoch 929, Loss: 0.15987416356801987, Final Batch Loss: 0.09472406655550003\n",
      "Epoch 930, Loss: 0.15808136016130447, Final Batch Loss: 0.09903133660554886\n",
      "Epoch 931, Loss: 0.15894009172916412, Final Batch Loss: 0.09319829195737839\n",
      "Epoch 932, Loss: 0.1292787417769432, Final Batch Loss: 0.05463438481092453\n",
      "Epoch 933, Loss: 0.1341012865304947, Final Batch Loss: 0.08204394578933716\n",
      "Epoch 934, Loss: 0.11713637784123421, Final Batch Loss: 0.06292852759361267\n",
      "Epoch 935, Loss: 0.12707360461354256, Final Batch Loss: 0.050148192793130875\n",
      "Epoch 936, Loss: 0.16314087808132172, Final Batch Loss: 0.05596090853214264\n",
      "Epoch 937, Loss: 0.1581227034330368, Final Batch Loss: 0.10535304993391037\n",
      "Epoch 938, Loss: 0.12817705422639847, Final Batch Loss: 0.07923221588134766\n",
      "Epoch 939, Loss: 0.1876610741019249, Final Batch Loss: 0.10010939836502075\n",
      "Epoch 940, Loss: 0.14766870439052582, Final Batch Loss: 0.0767529308795929\n",
      "Epoch 941, Loss: 0.14456678926944733, Final Batch Loss: 0.09480676800012589\n",
      "Epoch 942, Loss: 0.11072592809796333, Final Batch Loss: 0.07166367769241333\n",
      "Epoch 943, Loss: 0.12997151538729668, Final Batch Loss: 0.06989232450723648\n",
      "Epoch 944, Loss: 0.1610727161169052, Final Batch Loss: 0.07858816534280777\n",
      "Epoch 945, Loss: 0.11693547293543816, Final Batch Loss: 0.062075041234493256\n",
      "Epoch 946, Loss: 0.13002405688166618, Final Batch Loss: 0.06236499920487404\n",
      "Epoch 947, Loss: 0.12629782408475876, Final Batch Loss: 0.07385481148958206\n",
      "Epoch 948, Loss: 0.12329202517867088, Final Batch Loss: 0.048745546489953995\n",
      "Epoch 949, Loss: 0.1314457729458809, Final Batch Loss: 0.05611962825059891\n",
      "Epoch 950, Loss: 0.1341039128601551, Final Batch Loss: 0.07278069108724594\n",
      "Epoch 951, Loss: 0.11394484713673592, Final Batch Loss: 0.035599786788225174\n",
      "Epoch 952, Loss: 0.1382053978741169, Final Batch Loss: 0.06068485602736473\n",
      "Epoch 953, Loss: 0.10363184660673141, Final Batch Loss: 0.06543821841478348\n",
      "Epoch 954, Loss: 0.1255856417119503, Final Batch Loss: 0.05017664656043053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 955, Loss: 0.13473377376794815, Final Batch Loss: 0.06126222014427185\n",
      "Epoch 956, Loss: 0.17431336641311646, Final Batch Loss: 0.060117416083812714\n",
      "Epoch 957, Loss: 0.12487370520830154, Final Batch Loss: 0.06278444081544876\n",
      "Epoch 958, Loss: 0.15240398421883583, Final Batch Loss: 0.09448336809873581\n",
      "Epoch 959, Loss: 0.15476137399673462, Final Batch Loss: 0.08071446418762207\n",
      "Epoch 960, Loss: 0.14004899188876152, Final Batch Loss: 0.06240825727581978\n",
      "Epoch 961, Loss: 0.11600543931126595, Final Batch Loss: 0.055839233100414276\n",
      "Epoch 962, Loss: 0.2043215148150921, Final Batch Loss: 0.05742688104510307\n",
      "Epoch 963, Loss: 0.12040453776717186, Final Batch Loss: 0.05463533475995064\n",
      "Epoch 964, Loss: 0.2042982205748558, Final Batch Loss: 0.14609692990779877\n",
      "Epoch 965, Loss: 0.1273328699171543, Final Batch Loss: 0.05850541219115257\n",
      "Epoch 966, Loss: 0.12913020700216293, Final Batch Loss: 0.07032393664121628\n",
      "Epoch 967, Loss: 0.1418764367699623, Final Batch Loss: 0.06901994347572327\n",
      "Epoch 968, Loss: 0.15478383749723434, Final Batch Loss: 0.0775083675980568\n",
      "Epoch 969, Loss: 0.13747170194983482, Final Batch Loss: 0.09206606447696686\n",
      "Epoch 970, Loss: 0.11290246620774269, Final Batch Loss: 0.054340340197086334\n",
      "Epoch 971, Loss: 0.18442875146865845, Final Batch Loss: 0.08412613719701767\n",
      "Epoch 972, Loss: 0.12742524966597557, Final Batch Loss: 0.08196878433227539\n",
      "Epoch 973, Loss: 0.20775184035301208, Final Batch Loss: 0.08683807402849197\n",
      "Epoch 974, Loss: 0.12579956278204918, Final Batch Loss: 0.07709407061338425\n",
      "Epoch 975, Loss: 0.1981290504336357, Final Batch Loss: 0.12027560919523239\n",
      "Epoch 976, Loss: 0.14538073539733887, Final Batch Loss: 0.0719832181930542\n",
      "Epoch 977, Loss: 0.14445293322205544, Final Batch Loss: 0.09562268853187561\n",
      "Epoch 978, Loss: 0.1521270051598549, Final Batch Loss: 0.06785167753696442\n",
      "Epoch 979, Loss: 0.1966923549771309, Final Batch Loss: 0.09280452132225037\n",
      "Epoch 980, Loss: 0.15338869765400887, Final Batch Loss: 0.06085441634058952\n",
      "Epoch 981, Loss: 0.11041581630706787, Final Batch Loss: 0.045956507325172424\n",
      "Epoch 982, Loss: 0.15690066665410995, Final Batch Loss: 0.08780981600284576\n",
      "Epoch 983, Loss: 0.13570278882980347, Final Batch Loss: 0.08416977524757385\n",
      "Epoch 984, Loss: 0.1664441004395485, Final Batch Loss: 0.08362087607383728\n",
      "Epoch 985, Loss: 0.15249034762382507, Final Batch Loss: 0.06091511994600296\n",
      "Epoch 986, Loss: 0.13290126249194145, Final Batch Loss: 0.07724949717521667\n",
      "Epoch 987, Loss: 0.10722390189766884, Final Batch Loss: 0.040863748639822006\n",
      "Epoch 988, Loss: 0.18098317831754684, Final Batch Loss: 0.09084068238735199\n",
      "Epoch 989, Loss: 0.12228110060095787, Final Batch Loss: 0.05660078302025795\n",
      "Epoch 990, Loss: 0.12624341994524002, Final Batch Loss: 0.06538451462984085\n",
      "Epoch 991, Loss: 0.15295147150754929, Final Batch Loss: 0.05138269066810608\n",
      "Epoch 992, Loss: 0.11291471868753433, Final Batch Loss: 0.06566352397203445\n",
      "Epoch 993, Loss: 0.13082202151417732, Final Batch Loss: 0.04912735894322395\n",
      "Epoch 994, Loss: 0.1308947280049324, Final Batch Loss: 0.06440165638923645\n",
      "Epoch 995, Loss: 0.13531191274523735, Final Batch Loss: 0.055472638458013535\n",
      "Epoch 996, Loss: 0.11359032243490219, Final Batch Loss: 0.062085408717393875\n",
      "Epoch 997, Loss: 0.11890655010938644, Final Batch Loss: 0.06892083585262299\n",
      "Epoch 998, Loss: 0.1175554096698761, Final Batch Loss: 0.04635763168334961\n",
      "Epoch 999, Loss: 0.1343102753162384, Final Batch Loss: 0.053460463881492615\n",
      "Epoch 1000, Loss: 0.12450927123427391, Final Batch Loss: 0.05871794745326042\n",
      "Epoch 1001, Loss: 0.11155648157000542, Final Batch Loss: 0.06464482098817825\n",
      "Epoch 1002, Loss: 0.14312826842069626, Final Batch Loss: 0.09000059962272644\n",
      "Epoch 1003, Loss: 0.11661415174603462, Final Batch Loss: 0.05361669138073921\n",
      "Epoch 1004, Loss: 0.09990459680557251, Final Batch Loss: 0.054349448531866074\n",
      "Epoch 1005, Loss: 0.1407369077205658, Final Batch Loss: 0.0900656208395958\n",
      "Epoch 1006, Loss: 0.12454604357481003, Final Batch Loss: 0.046896666288375854\n",
      "Epoch 1007, Loss: 0.10084278509020805, Final Batch Loss: 0.05003431811928749\n",
      "Epoch 1008, Loss: 0.11495096236467361, Final Batch Loss: 0.06431609392166138\n",
      "Epoch 1009, Loss: 0.11148705333471298, Final Batch Loss: 0.051846250891685486\n",
      "Epoch 1010, Loss: 0.12796314805746078, Final Batch Loss: 0.05658569186925888\n",
      "Epoch 1011, Loss: 0.12334980443120003, Final Batch Loss: 0.07408477365970612\n",
      "Epoch 1012, Loss: 0.13417304307222366, Final Batch Loss: 0.08555523306131363\n",
      "Epoch 1013, Loss: 0.10726751014590263, Final Batch Loss: 0.05751212313771248\n",
      "Epoch 1014, Loss: 0.10060002654790878, Final Batch Loss: 0.04779184237122536\n",
      "Epoch 1015, Loss: 0.13419796526432037, Final Batch Loss: 0.08398008346557617\n",
      "Epoch 1016, Loss: 0.14280088990926743, Final Batch Loss: 0.058364175260066986\n",
      "Epoch 1017, Loss: 0.15439224615693092, Final Batch Loss: 0.04047000780701637\n",
      "Epoch 1018, Loss: 0.11492899060249329, Final Batch Loss: 0.06130071356892586\n",
      "Epoch 1019, Loss: 0.11913255974650383, Final Batch Loss: 0.05770391598343849\n",
      "Epoch 1020, Loss: 0.18132492527365685, Final Batch Loss: 0.12175672501325607\n",
      "Epoch 1021, Loss: 0.11865509673953056, Final Batch Loss: 0.04795306548476219\n",
      "Epoch 1022, Loss: 0.13088730350136757, Final Batch Loss: 0.06945300847291946\n",
      "Epoch 1023, Loss: 0.11834241449832916, Final Batch Loss: 0.06554525345563889\n",
      "Epoch 1024, Loss: 0.11812345683574677, Final Batch Loss: 0.0768565833568573\n",
      "Epoch 1025, Loss: 0.12123683094978333, Final Batch Loss: 0.04923095554113388\n",
      "Epoch 1026, Loss: 0.14376738667488098, Final Batch Loss: 0.0687241479754448\n",
      "Epoch 1027, Loss: 0.0963551439344883, Final Batch Loss: 0.059673432260751724\n",
      "Epoch 1028, Loss: 0.13929851725697517, Final Batch Loss: 0.051304545253515244\n",
      "Epoch 1029, Loss: 0.12416693940758705, Final Batch Loss: 0.05768906697630882\n",
      "Epoch 1030, Loss: 0.2531617730855942, Final Batch Loss: 0.20292896032333374\n",
      "Epoch 1031, Loss: 0.13289886713027954, Final Batch Loss: 0.08713692426681519\n",
      "Epoch 1032, Loss: 0.11669006943702698, Final Batch Loss: 0.06209169328212738\n",
      "Epoch 1033, Loss: 0.1101972796022892, Final Batch Loss: 0.05026960000395775\n",
      "Epoch 1034, Loss: 0.11545391753315926, Final Batch Loss: 0.056098684668540955\n",
      "Epoch 1035, Loss: 0.12070642039179802, Final Batch Loss: 0.055948663502931595\n",
      "Epoch 1036, Loss: 0.12657562643289566, Final Batch Loss: 0.05824948102235794\n",
      "Epoch 1037, Loss: 0.1244063451886177, Final Batch Loss: 0.0681978091597557\n",
      "Epoch 1038, Loss: 0.13686487823724747, Final Batch Loss: 0.06860470026731491\n",
      "Epoch 1039, Loss: 0.15083034336566925, Final Batch Loss: 0.08057823777198792\n",
      "Epoch 1040, Loss: 0.13872485980391502, Final Batch Loss: 0.08484426140785217\n",
      "Epoch 1041, Loss: 0.12666026502847672, Final Batch Loss: 0.05898390710353851\n",
      "Epoch 1042, Loss: 0.11933659389615059, Final Batch Loss: 0.03710271790623665\n",
      "Epoch 1043, Loss: 0.12807461991906166, Final Batch Loss: 0.05861872807145119\n",
      "Epoch 1044, Loss: 0.12506964057683945, Final Batch Loss: 0.0735975131392479\n",
      "Epoch 1045, Loss: 0.13216938450932503, Final Batch Loss: 0.07658465951681137\n",
      "Epoch 1046, Loss: 0.11620423197746277, Final Batch Loss: 0.06148115172982216\n",
      "Epoch 1047, Loss: 0.1337026245892048, Final Batch Loss: 0.0822090208530426\n",
      "Epoch 1048, Loss: 0.14482403174042702, Final Batch Loss: 0.05447862669825554\n",
      "Epoch 1049, Loss: 0.1512192115187645, Final Batch Loss: 0.07313697785139084\n",
      "Epoch 1050, Loss: 0.12966035306453705, Final Batch Loss: 0.07034014910459518\n",
      "Epoch 1051, Loss: 0.14573810994625092, Final Batch Loss: 0.06598228961229324\n",
      "Epoch 1052, Loss: 0.15532021969556808, Final Batch Loss: 0.07209276407957077\n",
      "Epoch 1053, Loss: 0.11160248145461082, Final Batch Loss: 0.061867136508226395\n",
      "Epoch 1054, Loss: 0.1518232449889183, Final Batch Loss: 0.06860601156949997\n",
      "Epoch 1055, Loss: 0.14838149398565292, Final Batch Loss: 0.05874456465244293\n",
      "Epoch 1056, Loss: 0.15739622712135315, Final Batch Loss: 0.0689825713634491\n",
      "Epoch 1057, Loss: 0.1738618090748787, Final Batch Loss: 0.05566719174385071\n",
      "Epoch 1058, Loss: 0.1519516557455063, Final Batch Loss: 0.08677242696285248\n",
      "Epoch 1059, Loss: 0.11607789620757103, Final Batch Loss: 0.06335653364658356\n",
      "Epoch 1060, Loss: 0.13615819066762924, Final Batch Loss: 0.062232211232185364\n",
      "Epoch 1061, Loss: 0.1686265841126442, Final Batch Loss: 0.0931941345334053\n",
      "Epoch 1062, Loss: 0.16031893715262413, Final Batch Loss: 0.054842229932546616\n",
      "Epoch 1063, Loss: 0.13170622289180756, Final Batch Loss: 0.07926374673843384\n",
      "Epoch 1064, Loss: 0.13624965958297253, Final Batch Loss: 0.030860556289553642\n",
      "Epoch 1065, Loss: 0.24928876012563705, Final Batch Loss: 0.1259819120168686\n",
      "Epoch 1066, Loss: 0.1094059869647026, Final Batch Loss: 0.038285210728645325\n",
      "Epoch 1067, Loss: 0.12361447513103485, Final Batch Loss: 0.049900248646736145\n",
      "Epoch 1068, Loss: 0.1377442628145218, Final Batch Loss: 0.08615157008171082\n",
      "Epoch 1069, Loss: 0.12962767854332924, Final Batch Loss: 0.04767842963337898\n",
      "Epoch 1070, Loss: 0.12172900885343552, Final Batch Loss: 0.04699777066707611\n",
      "Epoch 1071, Loss: 0.12381674721837044, Final Batch Loss: 0.06686437129974365\n",
      "Epoch 1072, Loss: 0.13276614993810654, Final Batch Loss: 0.06788042187690735\n",
      "Epoch 1073, Loss: 0.1259557493031025, Final Batch Loss: 0.06408873945474625\n",
      "Epoch 1074, Loss: 0.105423703789711, Final Batch Loss: 0.04371574521064758\n",
      "Epoch 1075, Loss: 0.13349415734410286, Final Batch Loss: 0.08180032670497894\n",
      "Epoch 1076, Loss: 0.10625622421503067, Final Batch Loss: 0.05946095287799835\n",
      "Epoch 1077, Loss: 0.13389576971530914, Final Batch Loss: 0.08004099130630493\n",
      "Epoch 1078, Loss: 0.1325737200677395, Final Batch Loss: 0.07308182120323181\n",
      "Epoch 1079, Loss: 0.10314249247312546, Final Batch Loss: 0.06889910995960236\n",
      "Epoch 1080, Loss: 0.14860443025827408, Final Batch Loss: 0.07996998727321625\n",
      "Epoch 1081, Loss: 0.1797589808702469, Final Batch Loss: 0.11052369326353073\n",
      "Epoch 1082, Loss: 0.14420679584145546, Final Batch Loss: 0.09237640351057053\n",
      "Epoch 1083, Loss: 0.14551108330488205, Final Batch Loss: 0.0980021059513092\n",
      "Epoch 1084, Loss: 0.1499060094356537, Final Batch Loss: 0.08859272301197052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1085, Loss: 0.10583129152655602, Final Batch Loss: 0.06203608959913254\n",
      "Epoch 1086, Loss: 0.1319557949900627, Final Batch Loss: 0.053787969052791595\n",
      "Epoch 1087, Loss: 0.11582981050014496, Final Batch Loss: 0.061909716576337814\n",
      "Epoch 1088, Loss: 0.12107664346694946, Final Batch Loss: 0.07483839243650436\n",
      "Epoch 1089, Loss: 0.1363949291408062, Final Batch Loss: 0.08034002035856247\n",
      "Epoch 1090, Loss: 0.13131816685199738, Final Batch Loss: 0.08076807856559753\n",
      "Epoch 1091, Loss: 0.12637752294540405, Final Batch Loss: 0.07682692259550095\n",
      "Epoch 1092, Loss: 0.08801675587892532, Final Batch Loss: 0.04439222067594528\n",
      "Epoch 1093, Loss: 0.14896022900938988, Final Batch Loss: 0.04445239529013634\n",
      "Epoch 1094, Loss: 0.12599357962608337, Final Batch Loss: 0.06951471418142319\n",
      "Epoch 1095, Loss: 0.10190651193261147, Final Batch Loss: 0.03611317649483681\n",
      "Epoch 1096, Loss: 0.12336723506450653, Final Batch Loss: 0.05039510875940323\n",
      "Epoch 1097, Loss: 0.12454306706786156, Final Batch Loss: 0.0623263418674469\n",
      "Epoch 1098, Loss: 0.19446949660778046, Final Batch Loss: 0.12467614561319351\n",
      "Epoch 1099, Loss: 0.13178115338087082, Final Batch Loss: 0.057953305542469025\n",
      "Epoch 1100, Loss: 0.12564875185489655, Final Batch Loss: 0.0573587641119957\n",
      "Epoch 1101, Loss: 0.1855350360274315, Final Batch Loss: 0.07258177548646927\n",
      "Epoch 1102, Loss: 0.1260399930179119, Final Batch Loss: 0.05928293988108635\n",
      "Epoch 1103, Loss: 0.13098597154021263, Final Batch Loss: 0.058371689170598984\n",
      "Epoch 1104, Loss: 0.10734845325350761, Final Batch Loss: 0.06110278144478798\n",
      "Epoch 1105, Loss: 0.11765651032328606, Final Batch Loss: 0.050573233515024185\n",
      "Epoch 1106, Loss: 0.11912891641259193, Final Batch Loss: 0.04807659611105919\n",
      "Epoch 1107, Loss: 0.1340671181678772, Final Batch Loss: 0.07255803793668747\n",
      "Epoch 1108, Loss: 0.16139378026127815, Final Batch Loss: 0.05978815630078316\n",
      "Epoch 1109, Loss: 0.08099750801920891, Final Batch Loss: 0.034241098910570145\n",
      "Epoch 1110, Loss: 0.15972225740551949, Final Batch Loss: 0.10532765835523605\n",
      "Epoch 1111, Loss: 0.13406342267990112, Final Batch Loss: 0.05864133685827255\n",
      "Epoch 1112, Loss: 0.12068696320056915, Final Batch Loss: 0.0659417062997818\n",
      "Epoch 1113, Loss: 0.09257305227220058, Final Batch Loss: 0.024036025628447533\n",
      "Epoch 1114, Loss: 0.12213676422834396, Final Batch Loss: 0.07292336225509644\n",
      "Epoch 1115, Loss: 0.12698937207460403, Final Batch Loss: 0.05033215880393982\n",
      "Epoch 1116, Loss: 0.10590412095189095, Final Batch Loss: 0.06179418787360191\n",
      "Epoch 1117, Loss: 0.14457790553569794, Final Batch Loss: 0.08055999875068665\n",
      "Epoch 1118, Loss: 0.10614683479070663, Final Batch Loss: 0.05851885676383972\n",
      "Epoch 1119, Loss: 0.11262622103095055, Final Batch Loss: 0.048843588680028915\n",
      "Epoch 1120, Loss: 0.13603464886546135, Final Batch Loss: 0.07488355040550232\n",
      "Epoch 1121, Loss: 0.09451515600085258, Final Batch Loss: 0.03401961550116539\n",
      "Epoch 1122, Loss: 0.146549042314291, Final Batch Loss: 0.08718234300613403\n",
      "Epoch 1123, Loss: 0.1058199591934681, Final Batch Loss: 0.037924204021692276\n",
      "Epoch 1124, Loss: 0.13751601055264473, Final Batch Loss: 0.08268532902002335\n",
      "Epoch 1125, Loss: 0.14152958244085312, Final Batch Loss: 0.08838213980197906\n",
      "Epoch 1126, Loss: 0.10791585594415665, Final Batch Loss: 0.04887012764811516\n",
      "Epoch 1127, Loss: 0.1424466222524643, Final Batch Loss: 0.08947917819023132\n",
      "Epoch 1128, Loss: 0.11631733551621437, Final Batch Loss: 0.05585027113556862\n",
      "Epoch 1129, Loss: 0.11544765159487724, Final Batch Loss: 0.06399956345558167\n",
      "Epoch 1130, Loss: 0.11674335971474648, Final Batch Loss: 0.04317447915673256\n",
      "Epoch 1131, Loss: 0.1433209627866745, Final Batch Loss: 0.07397697120904922\n",
      "Epoch 1132, Loss: 0.10623287782073021, Final Batch Loss: 0.04757380485534668\n",
      "Epoch 1133, Loss: 0.13484928756952286, Final Batch Loss: 0.0605575293302536\n",
      "Epoch 1134, Loss: 0.10678767785429955, Final Batch Loss: 0.05916726961731911\n",
      "Epoch 1135, Loss: 0.11601916700601578, Final Batch Loss: 0.05476546660065651\n",
      "Epoch 1136, Loss: 0.1090870089828968, Final Batch Loss: 0.05998174846172333\n",
      "Epoch 1137, Loss: 0.10809098929166794, Final Batch Loss: 0.04088631272315979\n",
      "Epoch 1138, Loss: 0.11259331554174423, Final Batch Loss: 0.06885976344347\n",
      "Epoch 1139, Loss: 0.10416268557310104, Final Batch Loss: 0.03763209283351898\n",
      "Epoch 1140, Loss: 0.09915907308459282, Final Batch Loss: 0.0567692331969738\n",
      "Epoch 1141, Loss: 0.12722448632121086, Final Batch Loss: 0.0858522281050682\n",
      "Epoch 1142, Loss: 0.11393237113952637, Final Batch Loss: 0.04828129708766937\n",
      "Epoch 1143, Loss: 0.12734263762831688, Final Batch Loss: 0.05557667836546898\n",
      "Epoch 1144, Loss: 0.12181773409247398, Final Batch Loss: 0.052986349910497665\n",
      "Epoch 1145, Loss: 0.1380135789513588, Final Batch Loss: 0.07364305108785629\n",
      "Epoch 1146, Loss: 0.1339709423482418, Final Batch Loss: 0.08692096918821335\n",
      "Epoch 1147, Loss: 0.11151589825749397, Final Batch Loss: 0.0352165512740612\n",
      "Epoch 1148, Loss: 0.11599478125572205, Final Batch Loss: 0.0660758838057518\n",
      "Epoch 1149, Loss: 0.09332553669810295, Final Batch Loss: 0.05766484513878822\n",
      "Epoch 1150, Loss: 0.1352158486843109, Final Batch Loss: 0.05111578106880188\n",
      "Epoch 1151, Loss: 0.12571416795253754, Final Batch Loss: 0.06116116791963577\n",
      "Epoch 1152, Loss: 0.15742595493793488, Final Batch Loss: 0.08227869868278503\n",
      "Epoch 1153, Loss: 0.1408509984612465, Final Batch Loss: 0.08747413754463196\n",
      "Epoch 1154, Loss: 0.08774589747190475, Final Batch Loss: 0.03952646628022194\n",
      "Epoch 1155, Loss: 0.1358674243092537, Final Batch Loss: 0.0682768002152443\n",
      "Epoch 1156, Loss: 0.12382173538208008, Final Batch Loss: 0.05752105265855789\n",
      "Epoch 1157, Loss: 0.12505396082997322, Final Batch Loss: 0.08506745100021362\n",
      "Epoch 1158, Loss: 0.1155795156955719, Final Batch Loss: 0.06041697785258293\n",
      "Epoch 1159, Loss: 0.13565197214484215, Final Batch Loss: 0.043213192373514175\n",
      "Epoch 1160, Loss: 0.08167107030749321, Final Batch Loss: 0.031364914029836655\n",
      "Epoch 1161, Loss: 0.11793871596455574, Final Batch Loss: 0.06708958745002747\n",
      "Epoch 1162, Loss: 0.12016892433166504, Final Batch Loss: 0.07055623829364777\n",
      "Epoch 1163, Loss: 0.09931337088346481, Final Batch Loss: 0.04511752724647522\n",
      "Epoch 1164, Loss: 0.09839054569602013, Final Batch Loss: 0.03616390377283096\n",
      "Epoch 1165, Loss: 0.12847937270998955, Final Batch Loss: 0.07073583453893661\n",
      "Epoch 1166, Loss: 0.0965978279709816, Final Batch Loss: 0.05652556195855141\n",
      "Epoch 1167, Loss: 0.10650980472564697, Final Batch Loss: 0.0508754700422287\n",
      "Epoch 1168, Loss: 0.08655992895364761, Final Batch Loss: 0.04325456544756889\n",
      "Epoch 1169, Loss: 0.10982349887490273, Final Batch Loss: 0.06838425248861313\n",
      "Epoch 1170, Loss: 0.12293143942952156, Final Batch Loss: 0.05067246034741402\n",
      "Epoch 1171, Loss: 0.09648767486214638, Final Batch Loss: 0.052852217108011246\n",
      "Epoch 1172, Loss: 0.1096530482172966, Final Batch Loss: 0.0545727014541626\n",
      "Epoch 1173, Loss: 0.08269787579774857, Final Batch Loss: 0.030241940170526505\n",
      "Epoch 1174, Loss: 0.10272716358304024, Final Batch Loss: 0.05590830743312836\n",
      "Epoch 1175, Loss: 0.11938711255788803, Final Batch Loss: 0.06081175431609154\n",
      "Epoch 1176, Loss: 0.13307646661996841, Final Batch Loss: 0.09105483442544937\n",
      "Epoch 1177, Loss: 0.11795235425233841, Final Batch Loss: 0.06543440371751785\n",
      "Epoch 1178, Loss: 0.12294743582606316, Final Batch Loss: 0.05730074271559715\n",
      "Epoch 1179, Loss: 0.09800558909773827, Final Batch Loss: 0.054489798843860626\n",
      "Epoch 1180, Loss: 0.1378960907459259, Final Batch Loss: 0.04361872375011444\n",
      "Epoch 1181, Loss: 0.12044436857104301, Final Batch Loss: 0.049143437296152115\n",
      "Epoch 1182, Loss: 0.10953470319509506, Final Batch Loss: 0.04788975417613983\n",
      "Epoch 1183, Loss: 0.13296473026275635, Final Batch Loss: 0.05477045476436615\n",
      "Epoch 1184, Loss: 0.14212865009903908, Final Batch Loss: 0.08421038836240768\n",
      "Epoch 1185, Loss: 0.12453402206301689, Final Batch Loss: 0.07741864770650864\n",
      "Epoch 1186, Loss: 0.14735177904367447, Final Batch Loss: 0.0704493522644043\n",
      "Epoch 1187, Loss: 0.15067313611507416, Final Batch Loss: 0.09295620024204254\n",
      "Epoch 1188, Loss: 0.11851493641734123, Final Batch Loss: 0.04374413564801216\n",
      "Epoch 1189, Loss: 0.11836856603622437, Final Batch Loss: 0.04443848133087158\n",
      "Epoch 1190, Loss: 0.13855572044849396, Final Batch Loss: 0.06977079808712006\n",
      "Epoch 1191, Loss: 0.13233894109725952, Final Batch Loss: 0.04329004883766174\n",
      "Epoch 1192, Loss: 0.155382439494133, Final Batch Loss: 0.0737556517124176\n",
      "Epoch 1193, Loss: 0.1866779625415802, Final Batch Loss: 0.10491000860929489\n",
      "Epoch 1194, Loss: 0.1440563090145588, Final Batch Loss: 0.09025759249925613\n",
      "Epoch 1195, Loss: 0.12767120450735092, Final Batch Loss: 0.07745202630758286\n",
      "Epoch 1196, Loss: 0.11401765793561935, Final Batch Loss: 0.0539797805249691\n",
      "Epoch 1197, Loss: 0.14334585890173912, Final Batch Loss: 0.08767200261354446\n",
      "Epoch 1198, Loss: 0.11556937918066978, Final Batch Loss: 0.07236216962337494\n",
      "Epoch 1199, Loss: 0.1257789582014084, Final Batch Loss: 0.07278990000486374\n",
      "Epoch 1200, Loss: 0.126205675303936, Final Batch Loss: 0.0659470409154892\n",
      "Epoch 1201, Loss: 0.163907952606678, Final Batch Loss: 0.064602330327034\n",
      "Epoch 1202, Loss: 0.12024198472499847, Final Batch Loss: 0.06470847874879837\n",
      "Epoch 1203, Loss: 0.13354815542697906, Final Batch Loss: 0.06475648283958435\n",
      "Epoch 1204, Loss: 0.09786513447761536, Final Batch Loss: 0.03758649900555611\n",
      "Epoch 1205, Loss: 0.16572636365890503, Final Batch Loss: 0.08980628103017807\n",
      "Epoch 1206, Loss: 0.1319296807050705, Final Batch Loss: 0.06703541427850723\n",
      "Epoch 1207, Loss: 0.1962457336485386, Final Batch Loss: 0.1502773016691208\n",
      "Epoch 1208, Loss: 0.13278179615736008, Final Batch Loss: 0.06319423764944077\n",
      "Epoch 1209, Loss: 0.09384478256106377, Final Batch Loss: 0.0591210275888443\n",
      "Epoch 1210, Loss: 0.17635945230722427, Final Batch Loss: 0.09728863835334778\n",
      "Epoch 1211, Loss: 0.14472206681966782, Final Batch Loss: 0.07720145583152771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1212, Loss: 0.13177871331572533, Final Batch Loss: 0.04947929456830025\n",
      "Epoch 1213, Loss: 0.12465645745396614, Final Batch Loss: 0.0705028846859932\n",
      "Epoch 1214, Loss: 0.1301460899412632, Final Batch Loss: 0.06850554049015045\n",
      "Epoch 1215, Loss: 0.12083529308438301, Final Batch Loss: 0.07704006135463715\n",
      "Epoch 1216, Loss: 0.18227874860167503, Final Batch Loss: 0.12115201354026794\n",
      "Epoch 1217, Loss: 0.11735019087791443, Final Batch Loss: 0.04006030410528183\n",
      "Epoch 1218, Loss: 0.11736665293574333, Final Batch Loss: 0.0440596304833889\n",
      "Epoch 1219, Loss: 0.13160390406847, Final Batch Loss: 0.06396953016519547\n",
      "Epoch 1220, Loss: 0.11366600915789604, Final Batch Loss: 0.06217356398701668\n",
      "Epoch 1221, Loss: 0.13637198135256767, Final Batch Loss: 0.05725223198533058\n",
      "Epoch 1222, Loss: 0.09903883934020996, Final Batch Loss: 0.05142928659915924\n",
      "Epoch 1223, Loss: 0.09517136961221695, Final Batch Loss: 0.05823018401861191\n",
      "Epoch 1224, Loss: 0.12953339144587517, Final Batch Loss: 0.059493858367204666\n",
      "Epoch 1225, Loss: 0.11922881007194519, Final Batch Loss: 0.07703152298927307\n",
      "Epoch 1226, Loss: 0.11936165019869804, Final Batch Loss: 0.05600360408425331\n",
      "Epoch 1227, Loss: 0.11312045156955719, Final Batch Loss: 0.0524541474878788\n",
      "Epoch 1228, Loss: 0.14772029966115952, Final Batch Loss: 0.06826917082071304\n",
      "Epoch 1229, Loss: 0.10317737609148026, Final Batch Loss: 0.05074196308851242\n",
      "Epoch 1230, Loss: 0.19495384395122528, Final Batch Loss: 0.07646478712558746\n",
      "Epoch 1231, Loss: 0.1584923416376114, Final Batch Loss: 0.07451982796192169\n",
      "Epoch 1232, Loss: 0.12144462391734123, Final Batch Loss: 0.07258845865726471\n",
      "Epoch 1233, Loss: 0.09467562660574913, Final Batch Loss: 0.0435580350458622\n",
      "Epoch 1234, Loss: 0.09989359602332115, Final Batch Loss: 0.05548236146569252\n",
      "Epoch 1235, Loss: 0.10701065883040428, Final Batch Loss: 0.04310217127203941\n",
      "Epoch 1236, Loss: 0.12077828869223595, Final Batch Loss: 0.04996934160590172\n",
      "Epoch 1237, Loss: 0.13008135557174683, Final Batch Loss: 0.05386469513177872\n",
      "Epoch 1238, Loss: 0.0942089594900608, Final Batch Loss: 0.05751105770468712\n",
      "Epoch 1239, Loss: 0.11562440544366837, Final Batch Loss: 0.04519127309322357\n",
      "Epoch 1240, Loss: 0.11020928993821144, Final Batch Loss: 0.042594071477651596\n",
      "Epoch 1241, Loss: 0.11922468990087509, Final Batch Loss: 0.06602099537849426\n",
      "Epoch 1242, Loss: 0.10889023542404175, Final Batch Loss: 0.04842117056250572\n",
      "Epoch 1243, Loss: 0.10588254034519196, Final Batch Loss: 0.04346964508295059\n",
      "Epoch 1244, Loss: 0.09177953377366066, Final Batch Loss: 0.03640498220920563\n",
      "Epoch 1245, Loss: 0.10655901581048965, Final Batch Loss: 0.07548867166042328\n",
      "Epoch 1246, Loss: 0.12189343944191933, Final Batch Loss: 0.03977387025952339\n",
      "Epoch 1247, Loss: 0.1118684634566307, Final Batch Loss: 0.066408671438694\n",
      "Epoch 1248, Loss: 0.11222147941589355, Final Batch Loss: 0.04853782057762146\n",
      "Epoch 1249, Loss: 0.11284170299768448, Final Batch Loss: 0.06874404847621918\n",
      "Epoch 1250, Loss: 0.13182533532381058, Final Batch Loss: 0.08298414945602417\n",
      "Epoch 1251, Loss: 0.10346424207091331, Final Batch Loss: 0.06424625962972641\n",
      "Epoch 1252, Loss: 0.09622569754719734, Final Batch Loss: 0.038964319974184036\n",
      "Epoch 1253, Loss: 0.1778191551566124, Final Batch Loss: 0.05638229101896286\n",
      "Epoch 1254, Loss: 0.12656287848949432, Final Batch Loss: 0.0665769875049591\n",
      "Epoch 1255, Loss: 0.08508706092834473, Final Batch Loss: 0.03142464533448219\n",
      "Epoch 1256, Loss: 0.11206100508570671, Final Batch Loss: 0.03847059980034828\n",
      "Epoch 1257, Loss: 0.11438896879553795, Final Batch Loss: 0.05341385304927826\n",
      "Epoch 1258, Loss: 0.08215659484267235, Final Batch Loss: 0.044614169746637344\n",
      "Epoch 1259, Loss: 0.1648051142692566, Final Batch Loss: 0.09103050827980042\n",
      "Epoch 1260, Loss: 0.10698160156607628, Final Batch Loss: 0.04909544438123703\n",
      "Epoch 1261, Loss: 0.12606675922870636, Final Batch Loss: 0.046801112592220306\n",
      "Epoch 1262, Loss: 0.10697250813245773, Final Batch Loss: 0.045129187405109406\n",
      "Epoch 1263, Loss: 0.11446802318096161, Final Batch Loss: 0.029323793947696686\n",
      "Epoch 1264, Loss: 0.08834601193666458, Final Batch Loss: 0.04174792766571045\n",
      "Epoch 1265, Loss: 0.10775324143469334, Final Batch Loss: 0.03117077238857746\n",
      "Epoch 1266, Loss: 0.09378752112388611, Final Batch Loss: 0.04802192002534866\n",
      "Epoch 1267, Loss: 0.08524073660373688, Final Batch Loss: 0.03501810505986214\n",
      "Epoch 1268, Loss: 0.13844868168234825, Final Batch Loss: 0.06074200198054314\n",
      "Epoch 1269, Loss: 0.0943753719329834, Final Batch Loss: 0.04167972505092621\n",
      "Epoch 1270, Loss: 0.15329071879386902, Final Batch Loss: 0.06673111021518707\n",
      "Epoch 1271, Loss: 0.10080606117844582, Final Batch Loss: 0.04409496858716011\n",
      "Epoch 1272, Loss: 0.09072897210717201, Final Batch Loss: 0.04093002527952194\n",
      "Epoch 1273, Loss: 0.09757101535797119, Final Batch Loss: 0.0544726625084877\n",
      "Epoch 1274, Loss: 0.09831671416759491, Final Batch Loss: 0.043169114738702774\n",
      "Epoch 1275, Loss: 0.11807749420404434, Final Batch Loss: 0.05384344607591629\n",
      "Epoch 1276, Loss: 0.09410234913229942, Final Batch Loss: 0.04199374094605446\n",
      "Epoch 1277, Loss: 0.1517624519765377, Final Batch Loss: 0.05724124237895012\n",
      "Epoch 1278, Loss: 0.12100142240524292, Final Batch Loss: 0.06249344348907471\n",
      "Epoch 1279, Loss: 0.13959891349077225, Final Batch Loss: 0.07166946679353714\n",
      "Epoch 1280, Loss: 0.11484116315841675, Final Batch Loss: 0.05658075213432312\n",
      "Epoch 1281, Loss: 0.18634647130966187, Final Batch Loss: 0.09339726716279984\n",
      "Epoch 1282, Loss: 0.1369420401751995, Final Batch Loss: 0.10415348410606384\n",
      "Epoch 1283, Loss: 0.11747340112924576, Final Batch Loss: 0.05673045292496681\n",
      "Epoch 1284, Loss: 0.10823559388518333, Final Batch Loss: 0.0526236928999424\n",
      "Epoch 1285, Loss: 0.16041696816682816, Final Batch Loss: 0.11050451546907425\n",
      "Epoch 1286, Loss: 0.10867329686880112, Final Batch Loss: 0.06214920058846474\n",
      "Epoch 1287, Loss: 0.1375858560204506, Final Batch Loss: 0.07654556632041931\n",
      "Epoch 1288, Loss: 0.11518648639321327, Final Batch Loss: 0.0604061596095562\n",
      "Epoch 1289, Loss: 0.11705182865262032, Final Batch Loss: 0.04341636225581169\n",
      "Epoch 1290, Loss: 0.11269811913371086, Final Batch Loss: 0.06518551707267761\n",
      "Epoch 1291, Loss: 0.1893165484070778, Final Batch Loss: 0.10363339632749557\n",
      "Epoch 1292, Loss: 0.13563146442174911, Final Batch Loss: 0.04879610240459442\n",
      "Epoch 1293, Loss: 0.1367068700492382, Final Batch Loss: 0.05405249819159508\n",
      "Epoch 1294, Loss: 0.12072115391492844, Final Batch Loss: 0.04030870646238327\n",
      "Epoch 1295, Loss: 0.1460874378681183, Final Batch Loss: 0.09337440878152847\n",
      "Epoch 1296, Loss: 0.13018586114048958, Final Batch Loss: 0.07575825601816177\n",
      "Epoch 1297, Loss: 0.10622572153806686, Final Batch Loss: 0.047573789954185486\n",
      "Epoch 1298, Loss: 0.12394822761416435, Final Batch Loss: 0.06778568029403687\n",
      "Epoch 1299, Loss: 0.11192768812179565, Final Batch Loss: 0.07533307373523712\n",
      "Epoch 1300, Loss: 0.10670481622219086, Final Batch Loss: 0.05494626984000206\n",
      "Epoch 1301, Loss: 0.13157714530825615, Final Batch Loss: 0.07776299864053726\n",
      "Epoch 1302, Loss: 0.08876952528953552, Final Batch Loss: 0.04732291027903557\n",
      "Epoch 1303, Loss: 0.11589748039841652, Final Batch Loss: 0.05072503164410591\n",
      "Epoch 1304, Loss: 0.12103644013404846, Final Batch Loss: 0.04925119876861572\n",
      "Epoch 1305, Loss: 0.13186471164226532, Final Batch Loss: 0.05625022202730179\n",
      "Epoch 1306, Loss: 0.08401878923177719, Final Batch Loss: 0.03903692215681076\n",
      "Epoch 1307, Loss: 0.17716866731643677, Final Batch Loss: 0.07243803888559341\n",
      "Epoch 1308, Loss: 0.113483976572752, Final Batch Loss: 0.034342747181653976\n",
      "Epoch 1309, Loss: 0.09516661614179611, Final Batch Loss: 0.05227886885404587\n",
      "Epoch 1310, Loss: 0.10063514485955238, Final Batch Loss: 0.05789695307612419\n",
      "Epoch 1311, Loss: 0.10095807909965515, Final Batch Loss: 0.05820014700293541\n",
      "Epoch 1312, Loss: 0.14868634939193726, Final Batch Loss: 0.07218576222658157\n",
      "Epoch 1313, Loss: 0.11391926556825638, Final Batch Loss: 0.07974359393119812\n",
      "Epoch 1314, Loss: 0.12119276821613312, Final Batch Loss: 0.04721779376268387\n",
      "Epoch 1315, Loss: 0.08452846296131611, Final Batch Loss: 0.05695529282093048\n",
      "Epoch 1316, Loss: 0.14292553067207336, Final Batch Loss: 0.06750877946615219\n",
      "Epoch 1317, Loss: 0.09426559135317802, Final Batch Loss: 0.04547803848981857\n",
      "Epoch 1318, Loss: 0.10937793552875519, Final Batch Loss: 0.041920050978660583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1319, Loss: 0.11051882058382034, Final Batch Loss: 0.0556640587747097\n",
      "Epoch 1320, Loss: 0.12386126443743706, Final Batch Loss: 0.057551804929971695\n",
      "Epoch 1321, Loss: 0.11458439007401466, Final Batch Loss: 0.042905617505311966\n",
      "Epoch 1322, Loss: 0.1090359278023243, Final Batch Loss: 0.052748363465070724\n",
      "Epoch 1323, Loss: 0.10165368393063545, Final Batch Loss: 0.02450479194521904\n",
      "Epoch 1324, Loss: 0.09771660715341568, Final Batch Loss: 0.04497880861163139\n",
      "Epoch 1325, Loss: 0.13213934749364853, Final Batch Loss: 0.06455385684967041\n",
      "Epoch 1326, Loss: 0.09146246314048767, Final Batch Loss: 0.05181128904223442\n",
      "Epoch 1327, Loss: 0.14074619486927986, Final Batch Loss: 0.08947524428367615\n",
      "Epoch 1328, Loss: 0.0892513245344162, Final Batch Loss: 0.0434764102101326\n",
      "Epoch 1329, Loss: 0.10026611015200615, Final Batch Loss: 0.061142709106206894\n",
      "Epoch 1330, Loss: 0.10325798764824867, Final Batch Loss: 0.04889507219195366\n",
      "Epoch 1331, Loss: 0.08117388561367989, Final Batch Loss: 0.043592669069767\n",
      "Epoch 1332, Loss: 0.11034790426492691, Final Batch Loss: 0.04937582463026047\n",
      "Epoch 1333, Loss: 0.12366441264748573, Final Batch Loss: 0.05053747072815895\n",
      "Epoch 1334, Loss: 0.10051694884896278, Final Batch Loss: 0.0634320005774498\n",
      "Epoch 1335, Loss: 0.10275314748287201, Final Batch Loss: 0.05547843128442764\n",
      "Epoch 1336, Loss: 0.12577782198786736, Final Batch Loss: 0.07331256568431854\n",
      "Epoch 1337, Loss: 0.10528946667909622, Final Batch Loss: 0.046122126281261444\n",
      "Epoch 1338, Loss: 0.10250896960496902, Final Batch Loss: 0.06594838201999664\n",
      "Epoch 1339, Loss: 0.09883090108633041, Final Batch Loss: 0.05755075439810753\n",
      "Epoch 1340, Loss: 0.14594214782118797, Final Batch Loss: 0.06010572239756584\n",
      "Epoch 1341, Loss: 0.08719178475439548, Final Batch Loss: 0.025613656267523766\n",
      "Epoch 1342, Loss: 0.1588454805314541, Final Batch Loss: 0.11611185222864151\n",
      "Epoch 1343, Loss: 0.11381164938211441, Final Batch Loss: 0.05228300020098686\n",
      "Epoch 1344, Loss: 0.10787107795476913, Final Batch Loss: 0.054815150797367096\n",
      "Epoch 1345, Loss: 0.11554624140262604, Final Batch Loss: 0.039958879351615906\n",
      "Epoch 1346, Loss: 0.15603558719158173, Final Batch Loss: 0.06892909109592438\n",
      "Epoch 1347, Loss: 0.1031421385705471, Final Batch Loss: 0.0483626052737236\n",
      "Epoch 1348, Loss: 0.10960235446691513, Final Batch Loss: 0.037626057863235474\n",
      "Epoch 1349, Loss: 0.10838695988059044, Final Batch Loss: 0.04039939120411873\n",
      "Epoch 1350, Loss: 0.09978665970265865, Final Batch Loss: 0.024298226460814476\n",
      "Epoch 1351, Loss: 0.11217154935002327, Final Batch Loss: 0.05456100404262543\n",
      "Epoch 1352, Loss: 0.11952372640371323, Final Batch Loss: 0.06844127178192139\n",
      "Epoch 1353, Loss: 0.10709036886692047, Final Batch Loss: 0.059145621955394745\n",
      "Epoch 1354, Loss: 0.12244591116905212, Final Batch Loss: 0.06274577230215073\n",
      "Epoch 1355, Loss: 0.13971829414367676, Final Batch Loss: 0.06744423508644104\n",
      "Epoch 1356, Loss: 0.08890684321522713, Final Batch Loss: 0.05022425204515457\n",
      "Epoch 1357, Loss: 0.10843022726476192, Final Batch Loss: 0.07772358506917953\n",
      "Epoch 1358, Loss: 0.12096280977129936, Final Batch Loss: 0.06587916612625122\n",
      "Epoch 1359, Loss: 0.10256233438849449, Final Batch Loss: 0.05184756964445114\n",
      "Epoch 1360, Loss: 0.11253497004508972, Final Batch Loss: 0.05877506360411644\n",
      "Epoch 1361, Loss: 0.11002492159605026, Final Batch Loss: 0.043426014482975006\n",
      "Epoch 1362, Loss: 0.09959123283624649, Final Batch Loss: 0.04428703337907791\n",
      "Epoch 1363, Loss: 0.09915737807750702, Final Batch Loss: 0.06640356034040451\n",
      "Epoch 1364, Loss: 0.11492634937167168, Final Batch Loss: 0.05995730683207512\n",
      "Epoch 1365, Loss: 0.0869157686829567, Final Batch Loss: 0.03627144545316696\n",
      "Epoch 1366, Loss: 0.12009112536907196, Final Batch Loss: 0.06437826156616211\n",
      "Epoch 1367, Loss: 0.13136234879493713, Final Batch Loss: 0.07224607467651367\n",
      "Epoch 1368, Loss: 0.11397475004196167, Final Batch Loss: 0.06934622675180435\n",
      "Epoch 1369, Loss: 0.10991904884576797, Final Batch Loss: 0.05898001417517662\n",
      "Epoch 1370, Loss: 0.10033313930034637, Final Batch Loss: 0.046113021671772\n",
      "Epoch 1371, Loss: 0.08258078992366791, Final Batch Loss: 0.0304889939725399\n",
      "Epoch 1372, Loss: 0.18941722065210342, Final Batch Loss: 0.07969958335161209\n",
      "Epoch 1373, Loss: 0.13916058093309402, Final Batch Loss: 0.0712459534406662\n",
      "Epoch 1374, Loss: 0.10284223407506943, Final Batch Loss: 0.05840883404016495\n",
      "Epoch 1375, Loss: 0.1008867621421814, Final Batch Loss: 0.059758491814136505\n",
      "Epoch 1376, Loss: 0.10535125061869621, Final Batch Loss: 0.05480670928955078\n",
      "Epoch 1377, Loss: 0.09668273851275444, Final Batch Loss: 0.04899391904473305\n",
      "Epoch 1378, Loss: 0.09452518448233604, Final Batch Loss: 0.05286407098174095\n",
      "Epoch 1379, Loss: 0.1438838243484497, Final Batch Loss: 0.05801063030958176\n",
      "Epoch 1380, Loss: 0.11649930477142334, Final Batch Loss: 0.06510049104690552\n",
      "Epoch 1381, Loss: 0.11846056953072548, Final Batch Loss: 0.07881931960582733\n",
      "Epoch 1382, Loss: 0.1410747990012169, Final Batch Loss: 0.07068143784999847\n",
      "Epoch 1383, Loss: 0.11122681945562363, Final Batch Loss: 0.04941083490848541\n",
      "Epoch 1384, Loss: 0.10584438219666481, Final Batch Loss: 0.06002526730298996\n",
      "Epoch 1385, Loss: 0.0998111143708229, Final Batch Loss: 0.04974362254142761\n",
      "Epoch 1386, Loss: 0.09621163457632065, Final Batch Loss: 0.042864300310611725\n",
      "Epoch 1387, Loss: 0.13930242508649826, Final Batch Loss: 0.08299749344587326\n",
      "Epoch 1388, Loss: 0.12803732976317406, Final Batch Loss: 0.09640242159366608\n",
      "Epoch 1389, Loss: 0.12175030261278152, Final Batch Loss: 0.05441402643918991\n",
      "Epoch 1390, Loss: 0.13262848556041718, Final Batch Loss: 0.07991322875022888\n",
      "Epoch 1391, Loss: 0.1377471350133419, Final Batch Loss: 0.03477353975176811\n",
      "Epoch 1392, Loss: 0.16504669934511185, Final Batch Loss: 0.07968884706497192\n",
      "Epoch 1393, Loss: 0.10766886547207832, Final Batch Loss: 0.03554008528590202\n",
      "Epoch 1394, Loss: 0.14141641929745674, Final Batch Loss: 0.09215174615383148\n",
      "Epoch 1395, Loss: 0.10173427313566208, Final Batch Loss: 0.03667990118265152\n",
      "Epoch 1396, Loss: 0.1553516909480095, Final Batch Loss: 0.08246931433677673\n",
      "Epoch 1397, Loss: 0.12422846257686615, Final Batch Loss: 0.05094306170940399\n",
      "Epoch 1398, Loss: 0.10651067644357681, Final Batch Loss: 0.06786967813968658\n",
      "Epoch 1399, Loss: 0.11590834707021713, Final Batch Loss: 0.05646984651684761\n",
      "Epoch 1400, Loss: 0.09788929671049118, Final Batch Loss: 0.055185843259096146\n",
      "Epoch 1401, Loss: 0.1149960421025753, Final Batch Loss: 0.04676707461476326\n",
      "Epoch 1402, Loss: 0.11277975142002106, Final Batch Loss: 0.06092872470617294\n",
      "Epoch 1403, Loss: 0.12990953400731087, Final Batch Loss: 0.05272066220641136\n",
      "Epoch 1404, Loss: 0.11669660359621048, Final Batch Loss: 0.05276811122894287\n",
      "Epoch 1405, Loss: 0.10295521467924118, Final Batch Loss: 0.04575143754482269\n",
      "Epoch 1406, Loss: 0.19389502704143524, Final Batch Loss: 0.07434412091970444\n",
      "Epoch 1407, Loss: 0.09048314765095711, Final Batch Loss: 0.04907911270856857\n",
      "Epoch 1408, Loss: 0.12053993344306946, Final Batch Loss: 0.07443784177303314\n",
      "Epoch 1409, Loss: 0.13836408779025078, Final Batch Loss: 0.0870409607887268\n",
      "Epoch 1410, Loss: 0.12408293411135674, Final Batch Loss: 0.05343134328722954\n",
      "Epoch 1411, Loss: 0.12900078669190407, Final Batch Loss: 0.04679611697793007\n",
      "Epoch 1412, Loss: 0.15088477358222008, Final Batch Loss: 0.04325775429606438\n",
      "Epoch 1413, Loss: 0.10871437937021255, Final Batch Loss: 0.06246958673000336\n",
      "Epoch 1414, Loss: 0.14877032488584518, Final Batch Loss: 0.08790337294340134\n",
      "Epoch 1415, Loss: 0.10994134098291397, Final Batch Loss: 0.06023559346795082\n",
      "Epoch 1416, Loss: 0.10475363954901695, Final Batch Loss: 0.05618397891521454\n",
      "Epoch 1417, Loss: 0.09098831936717033, Final Batch Loss: 0.05395657941699028\n",
      "Epoch 1418, Loss: 0.1147429347038269, Final Batch Loss: 0.049001388251781464\n",
      "Epoch 1419, Loss: 0.10500091314315796, Final Batch Loss: 0.0617356039583683\n",
      "Epoch 1420, Loss: 0.13021113350987434, Final Batch Loss: 0.03469676151871681\n",
      "Epoch 1421, Loss: 0.09863962605595589, Final Batch Loss: 0.04241286218166351\n",
      "Epoch 1422, Loss: 0.12121736258268356, Final Batch Loss: 0.06509286165237427\n",
      "Epoch 1423, Loss: 0.10146461427211761, Final Batch Loss: 0.04164239391684532\n",
      "Epoch 1424, Loss: 0.10393312573432922, Final Batch Loss: 0.045165855437517166\n",
      "Epoch 1425, Loss: 0.11050068587064743, Final Batch Loss: 0.040183812379837036\n",
      "Epoch 1426, Loss: 0.13774720579385757, Final Batch Loss: 0.06339407712221146\n",
      "Epoch 1427, Loss: 0.10534936189651489, Final Batch Loss: 0.05358321964740753\n",
      "Epoch 1428, Loss: 0.10617996379733086, Final Batch Loss: 0.05760335549712181\n",
      "Epoch 1429, Loss: 0.11504370346665382, Final Batch Loss: 0.052630264312028885\n",
      "Epoch 1430, Loss: 0.09912687540054321, Final Batch Loss: 0.04668690264225006\n",
      "Epoch 1431, Loss: 0.188299510627985, Final Batch Loss: 0.046839792281389236\n",
      "Epoch 1432, Loss: 0.13112873956561089, Final Batch Loss: 0.035842280834913254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1433, Loss: 0.11400772258639336, Final Batch Loss: 0.05513422563672066\n",
      "Epoch 1434, Loss: 0.1155979111790657, Final Batch Loss: 0.05012424290180206\n",
      "Epoch 1435, Loss: 0.10857231914997101, Final Batch Loss: 0.04846185818314552\n",
      "Epoch 1436, Loss: 0.15360093489289284, Final Batch Loss: 0.11737468838691711\n",
      "Epoch 1437, Loss: 0.08826975151896477, Final Batch Loss: 0.033505961298942566\n",
      "Epoch 1438, Loss: 0.13101018965244293, Final Batch Loss: 0.05245644599199295\n",
      "Epoch 1439, Loss: 0.12195337191224098, Final Batch Loss: 0.0734541043639183\n",
      "Epoch 1440, Loss: 0.12351800873875618, Final Batch Loss: 0.05449430271983147\n",
      "Epoch 1441, Loss: 0.12045120447874069, Final Batch Loss: 0.04768243432044983\n",
      "Epoch 1442, Loss: 0.10761231929063797, Final Batch Loss: 0.043663881719112396\n",
      "Epoch 1443, Loss: 0.11107468977570534, Final Batch Loss: 0.05409163609147072\n",
      "Epoch 1444, Loss: 0.13254722580313683, Final Batch Loss: 0.053008776158094406\n",
      "Epoch 1445, Loss: 0.09219324216246605, Final Batch Loss: 0.05045704171061516\n",
      "Epoch 1446, Loss: 0.15468331426382065, Final Batch Loss: 0.09696847945451736\n",
      "Epoch 1447, Loss: 0.11447165906429291, Final Batch Loss: 0.07643800973892212\n",
      "Epoch 1448, Loss: 0.19298384338617325, Final Batch Loss: 0.0925278291106224\n",
      "Epoch 1449, Loss: 0.12864325195550919, Final Batch Loss: 0.07959995418787003\n",
      "Epoch 1450, Loss: 0.13870330154895782, Final Batch Loss: 0.06897997111082077\n",
      "Epoch 1451, Loss: 0.12632495537400246, Final Batch Loss: 0.04735030606389046\n",
      "Epoch 1452, Loss: 0.13849884271621704, Final Batch Loss: 0.07533549517393112\n",
      "Epoch 1453, Loss: 0.09288293495774269, Final Batch Loss: 0.03762195631861687\n",
      "Epoch 1454, Loss: 0.1743330880999565, Final Batch Loss: 0.09108526259660721\n",
      "Epoch 1455, Loss: 0.19841376692056656, Final Batch Loss: 0.07750342786312103\n",
      "Epoch 1456, Loss: 0.20063965767621994, Final Batch Loss: 0.08643470704555511\n",
      "Epoch 1457, Loss: 0.13442417234182358, Final Batch Loss: 0.08819655328989029\n",
      "Epoch 1458, Loss: 0.12749745696783066, Final Batch Loss: 0.06384981423616409\n",
      "Epoch 1459, Loss: 0.23837359994649887, Final Batch Loss: 0.09774745255708694\n",
      "Epoch 1460, Loss: 0.15890677273273468, Final Batch Loss: 0.08089691400527954\n",
      "Epoch 1461, Loss: 0.2504502236843109, Final Batch Loss: 0.06580127775669098\n",
      "Epoch 1462, Loss: 0.1430899202823639, Final Batch Loss: 0.07306255400180817\n",
      "Epoch 1463, Loss: 0.13652336597442627, Final Batch Loss: 0.03422985225915909\n",
      "Epoch 1464, Loss: 0.1360544189810753, Final Batch Loss: 0.08850710839033127\n",
      "Epoch 1465, Loss: 0.17151552438735962, Final Batch Loss: 0.040273889899253845\n",
      "Epoch 1466, Loss: 0.11743410304188728, Final Batch Loss: 0.0755290538072586\n",
      "Epoch 1467, Loss: 0.13651758804917336, Final Batch Loss: 0.053516048938035965\n",
      "Epoch 1468, Loss: 0.13923494890332222, Final Batch Loss: 0.0807771161198616\n",
      "Epoch 1469, Loss: 0.09331804886460304, Final Batch Loss: 0.04325254634022713\n",
      "Epoch 1470, Loss: 0.14922289550304413, Final Batch Loss: 0.07896088063716888\n",
      "Epoch 1471, Loss: 0.12161019071936607, Final Batch Loss: 0.04112962260842323\n",
      "Epoch 1472, Loss: 0.11756544560194016, Final Batch Loss: 0.057428404688835144\n",
      "Epoch 1473, Loss: 0.08150776848196983, Final Batch Loss: 0.03223566338419914\n",
      "Epoch 1474, Loss: 0.09818460047245026, Final Batch Loss: 0.03894827514886856\n",
      "Epoch 1475, Loss: 0.12088385596871376, Final Batch Loss: 0.06683795154094696\n",
      "Epoch 1476, Loss: 0.11454750970005989, Final Batch Loss: 0.032807957381010056\n",
      "Epoch 1477, Loss: 0.2062016725540161, Final Batch Loss: 0.07870639860630035\n",
      "Epoch 1478, Loss: 0.08978240936994553, Final Batch Loss: 0.054384756833314896\n",
      "Epoch 1479, Loss: 0.14207244664430618, Final Batch Loss: 0.06847359240055084\n",
      "Epoch 1480, Loss: 0.10240908339619637, Final Batch Loss: 0.05826164782047272\n",
      "Epoch 1481, Loss: 0.12302505597472191, Final Batch Loss: 0.05593719705939293\n",
      "Epoch 1482, Loss: 0.10480596870183945, Final Batch Loss: 0.049700088798999786\n",
      "Epoch 1483, Loss: 0.12587984278798103, Final Batch Loss: 0.06957808136940002\n",
      "Epoch 1484, Loss: 0.12589552998542786, Final Batch Loss: 0.05623040348291397\n",
      "Epoch 1485, Loss: 0.07213610224425793, Final Batch Loss: 0.028491051867604256\n",
      "Epoch 1486, Loss: 0.23175246641039848, Final Batch Loss: 0.0555720292031765\n",
      "Epoch 1487, Loss: 0.11014412343502045, Final Batch Loss: 0.04620122164487839\n",
      "Epoch 1488, Loss: 0.12712863460183144, Final Batch Loss: 0.07159006595611572\n",
      "Epoch 1489, Loss: 0.10406672954559326, Final Batch Loss: 0.059988874942064285\n",
      "Epoch 1490, Loss: 0.11702616885304451, Final Batch Loss: 0.060832999646663666\n",
      "Epoch 1491, Loss: 0.10448259487748146, Final Batch Loss: 0.04004080221056938\n",
      "Epoch 1492, Loss: 0.14098741114139557, Final Batch Loss: 0.055619142949581146\n",
      "Epoch 1493, Loss: 0.11798765510320663, Final Batch Loss: 0.07175101339817047\n",
      "Epoch 1494, Loss: 0.1253698654472828, Final Batch Loss: 0.06866425275802612\n",
      "Epoch 1495, Loss: 0.12376060336828232, Final Batch Loss: 0.059680916368961334\n",
      "Epoch 1496, Loss: 0.11143666505813599, Final Batch Loss: 0.03608320653438568\n",
      "Epoch 1497, Loss: 0.09254755824804306, Final Batch Loss: 0.043658990412950516\n",
      "Epoch 1498, Loss: 0.107541024684906, Final Batch Loss: 0.04269493371248245\n",
      "Epoch 1499, Loss: 0.07372559607028961, Final Batch Loss: 0.04769463092088699\n",
      "Epoch 1500, Loss: 0.1083071418106556, Final Batch Loss: 0.04740530997514725\n",
      "Epoch 1501, Loss: 0.1065729558467865, Final Batch Loss: 0.06365533173084259\n",
      "Epoch 1502, Loss: 0.10919975861907005, Final Batch Loss: 0.06134822964668274\n",
      "Epoch 1503, Loss: 0.17187394201755524, Final Batch Loss: 0.03168797492980957\n",
      "Epoch 1504, Loss: 0.13352633640170097, Final Batch Loss: 0.09125260263681412\n",
      "Epoch 1505, Loss: 0.11365239322185516, Final Batch Loss: 0.050244636833667755\n",
      "Epoch 1506, Loss: 0.10485870391130447, Final Batch Loss: 0.03119424730539322\n",
      "Epoch 1507, Loss: 0.10216937586665154, Final Batch Loss: 0.06038932874798775\n",
      "Epoch 1508, Loss: 0.11315452679991722, Final Batch Loss: 0.0354962982237339\n",
      "Epoch 1509, Loss: 0.09999474883079529, Final Batch Loss: 0.05567279830574989\n",
      "Epoch 1510, Loss: 0.0991988517343998, Final Batch Loss: 0.05612191930413246\n",
      "Epoch 1511, Loss: 0.09022331610321999, Final Batch Loss: 0.045143552124500275\n",
      "Epoch 1512, Loss: 0.08844807744026184, Final Batch Loss: 0.03654945269227028\n",
      "Epoch 1513, Loss: 0.117687176913023, Final Batch Loss: 0.05067616328597069\n",
      "Epoch 1514, Loss: 0.1306094340980053, Final Batch Loss: 0.058664027601480484\n",
      "Epoch 1515, Loss: 0.0942029133439064, Final Batch Loss: 0.06274668127298355\n",
      "Epoch 1516, Loss: 0.09916352108120918, Final Batch Loss: 0.06913761049509048\n",
      "Epoch 1517, Loss: 0.11410671845078468, Final Batch Loss: 0.05320097506046295\n",
      "Epoch 1518, Loss: 0.10826883092522621, Final Batch Loss: 0.05236651003360748\n",
      "Epoch 1519, Loss: 0.10152248665690422, Final Batch Loss: 0.06502209603786469\n",
      "Epoch 1520, Loss: 0.09122337028384209, Final Batch Loss: 0.04585663974285126\n",
      "Epoch 1521, Loss: 0.1290203370153904, Final Batch Loss: 0.0833248496055603\n",
      "Epoch 1522, Loss: 0.09803477674722672, Final Batch Loss: 0.05760614573955536\n",
      "Epoch 1523, Loss: 0.08645215258002281, Final Batch Loss: 0.039453499019145966\n",
      "Epoch 1524, Loss: 0.0995105616748333, Final Batch Loss: 0.053670383989810944\n",
      "Epoch 1525, Loss: 0.11596644669771194, Final Batch Loss: 0.03949832171201706\n",
      "Epoch 1526, Loss: 0.10576022788882256, Final Batch Loss: 0.05063501372933388\n",
      "Epoch 1527, Loss: 0.11188847199082375, Final Batch Loss: 0.03304039314389229\n",
      "Epoch 1528, Loss: 0.09232886880636215, Final Batch Loss: 0.04986828193068504\n",
      "Epoch 1529, Loss: 0.09876874834299088, Final Batch Loss: 0.032116346061229706\n",
      "Epoch 1530, Loss: 0.12903137505054474, Final Batch Loss: 0.0797538161277771\n",
      "Epoch 1531, Loss: 0.12575671821832657, Final Batch Loss: 0.06634543836116791\n",
      "Epoch 1532, Loss: 0.1172979474067688, Final Batch Loss: 0.06535730510950089\n",
      "Epoch 1533, Loss: 0.14135218039155006, Final Batch Loss: 0.0967269092798233\n",
      "Epoch 1534, Loss: 0.12702561169862747, Final Batch Loss: 0.03878682106733322\n",
      "Epoch 1535, Loss: 0.14826775342226028, Final Batch Loss: 0.04957752674818039\n",
      "Epoch 1536, Loss: 0.09641694650053978, Final Batch Loss: 0.04277165234088898\n",
      "Epoch 1537, Loss: 0.13929037749767303, Final Batch Loss: 0.06572900712490082\n",
      "Epoch 1538, Loss: 0.11013882607221603, Final Batch Loss: 0.06830514967441559\n",
      "Epoch 1539, Loss: 0.14225411415100098, Final Batch Loss: 0.07030710577964783\n",
      "Epoch 1540, Loss: 0.11084096133708954, Final Batch Loss: 0.06604745984077454\n",
      "Epoch 1541, Loss: 0.11148560792207718, Final Batch Loss: 0.056174177676439285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1542, Loss: 0.09994553029537201, Final Batch Loss: 0.0311749130487442\n",
      "Epoch 1543, Loss: 0.10869345813989639, Final Batch Loss: 0.05238978937268257\n",
      "Epoch 1544, Loss: 0.11975471675395966, Final Batch Loss: 0.06818772107362747\n",
      "Epoch 1545, Loss: 0.08603050000965595, Final Batch Loss: 0.057771068066358566\n",
      "Epoch 1546, Loss: 0.1269889809191227, Final Batch Loss: 0.06865736842155457\n",
      "Epoch 1547, Loss: 0.10246207192540169, Final Batch Loss: 0.046500515192747116\n",
      "Epoch 1548, Loss: 0.1135106049478054, Final Batch Loss: 0.06274762749671936\n",
      "Epoch 1549, Loss: 0.09949745237827301, Final Batch Loss: 0.047412123531103134\n",
      "Epoch 1550, Loss: 0.09481922537088394, Final Batch Loss: 0.05612187087535858\n",
      "Epoch 1551, Loss: 0.12579309567809105, Final Batch Loss: 0.06021759286522865\n",
      "Epoch 1552, Loss: 0.17433347925543785, Final Batch Loss: 0.12168697267770767\n",
      "Epoch 1553, Loss: 0.09480679407715797, Final Batch Loss: 0.03909917175769806\n",
      "Epoch 1554, Loss: 0.089353758841753, Final Batch Loss: 0.042643025517463684\n",
      "Epoch 1555, Loss: 0.12141410633921623, Final Batch Loss: 0.05999305099248886\n",
      "Epoch 1556, Loss: 0.10746996477246284, Final Batch Loss: 0.06973622739315033\n",
      "Epoch 1557, Loss: 0.091391172260046, Final Batch Loss: 0.04541933909058571\n",
      "Epoch 1558, Loss: 0.12097779288887978, Final Batch Loss: 0.043081048876047134\n",
      "Epoch 1559, Loss: 0.12126315385103226, Final Batch Loss: 0.07546815276145935\n",
      "Epoch 1560, Loss: 0.12459731101989746, Final Batch Loss: 0.08933503180742264\n",
      "Epoch 1561, Loss: 0.11057448759675026, Final Batch Loss: 0.06029532849788666\n",
      "Epoch 1562, Loss: 0.18555211275815964, Final Batch Loss: 0.120443195104599\n",
      "Epoch 1563, Loss: 0.1275707110762596, Final Batch Loss: 0.06085348129272461\n",
      "Epoch 1564, Loss: 0.1294550895690918, Final Batch Loss: 0.059678807854652405\n",
      "Epoch 1565, Loss: 0.1166415736079216, Final Batch Loss: 0.04952302575111389\n",
      "Epoch 1566, Loss: 0.11682769656181335, Final Batch Loss: 0.05514096841216087\n",
      "Epoch 1567, Loss: 0.12998755276203156, Final Batch Loss: 0.07737584412097931\n",
      "Epoch 1568, Loss: 0.10984693840146065, Final Batch Loss: 0.04483963921666145\n",
      "Epoch 1569, Loss: 0.17230819538235664, Final Batch Loss: 0.05642148479819298\n",
      "Epoch 1570, Loss: 0.11511749029159546, Final Batch Loss: 0.04393009841442108\n",
      "Epoch 1571, Loss: 0.12290402129292488, Final Batch Loss: 0.060475703328847885\n",
      "Epoch 1572, Loss: 0.10407689586281776, Final Batch Loss: 0.05097772553563118\n",
      "Epoch 1573, Loss: 0.1462819203734398, Final Batch Loss: 0.07629557698965073\n",
      "Epoch 1574, Loss: 0.22636938840150833, Final Batch Loss: 0.13099442422389984\n",
      "Epoch 1575, Loss: 0.10773629322648048, Final Batch Loss: 0.035180892795324326\n",
      "Epoch 1576, Loss: 0.1500682458281517, Final Batch Loss: 0.06838139146566391\n",
      "Epoch 1577, Loss: 0.11552577838301659, Final Batch Loss: 0.07728324085474014\n",
      "Epoch 1578, Loss: 0.1213100291788578, Final Batch Loss: 0.07374723255634308\n",
      "Epoch 1579, Loss: 0.14550235122442245, Final Batch Loss: 0.07155809551477432\n",
      "Epoch 1580, Loss: 0.09348605200648308, Final Batch Loss: 0.05457952618598938\n",
      "Epoch 1581, Loss: 0.14653517678380013, Final Batch Loss: 0.09173574298620224\n",
      "Epoch 1582, Loss: 0.24493619799613953, Final Batch Loss: 0.16962210834026337\n",
      "Epoch 1583, Loss: 0.1246872991323471, Final Batch Loss: 0.07378929108381271\n",
      "Epoch 1584, Loss: 0.13976864516735077, Final Batch Loss: 0.05268791317939758\n",
      "Epoch 1585, Loss: 0.11605040356516838, Final Batch Loss: 0.07310665398836136\n",
      "Epoch 1586, Loss: 0.13555920869112015, Final Batch Loss: 0.07946690171957016\n",
      "Epoch 1587, Loss: 0.08629925176501274, Final Batch Loss: 0.041442397981882095\n",
      "Epoch 1588, Loss: 0.09782164916396141, Final Batch Loss: 0.06617585569620132\n",
      "Epoch 1589, Loss: 0.1089848130941391, Final Batch Loss: 0.07643189281225204\n",
      "Epoch 1590, Loss: 0.10507366806268692, Final Batch Loss: 0.04422891512513161\n",
      "Epoch 1591, Loss: 0.10080176219344139, Final Batch Loss: 0.049937549978494644\n",
      "Epoch 1592, Loss: 0.11487021297216415, Final Batch Loss: 0.05760130286216736\n",
      "Epoch 1593, Loss: 0.10900137946009636, Final Batch Loss: 0.059000276029109955\n",
      "Epoch 1594, Loss: 0.1053403951227665, Final Batch Loss: 0.04538316652178764\n",
      "Epoch 1595, Loss: 0.1078062653541565, Final Batch Loss: 0.05307947099208832\n",
      "Epoch 1596, Loss: 0.13311494141817093, Final Batch Loss: 0.06147453933954239\n",
      "Epoch 1597, Loss: 0.10413870960474014, Final Batch Loss: 0.05261214077472687\n",
      "Epoch 1598, Loss: 0.0749342329800129, Final Batch Loss: 0.033819619566202164\n",
      "Epoch 1599, Loss: 0.10200385376811028, Final Batch Loss: 0.031810011714696884\n",
      "Epoch 1600, Loss: 0.08864742517471313, Final Batch Loss: 0.05253058299422264\n",
      "Epoch 1601, Loss: 0.11403808742761612, Final Batch Loss: 0.0559740774333477\n",
      "Epoch 1602, Loss: 0.11308396235108376, Final Batch Loss: 0.05175759270787239\n",
      "Epoch 1603, Loss: 0.07942189648747444, Final Batch Loss: 0.03669518604874611\n",
      "Epoch 1604, Loss: 0.08662540465593338, Final Batch Loss: 0.04001840576529503\n",
      "Epoch 1605, Loss: 0.12839850038290024, Final Batch Loss: 0.0684933140873909\n",
      "Epoch 1606, Loss: 0.09205633029341698, Final Batch Loss: 0.03985147550702095\n",
      "Epoch 1607, Loss: 0.09471394494175911, Final Batch Loss: 0.05367816984653473\n",
      "Epoch 1608, Loss: 0.10175199806690216, Final Batch Loss: 0.05830613896250725\n",
      "Epoch 1609, Loss: 0.08165754936635494, Final Batch Loss: 0.05524662137031555\n",
      "Epoch 1610, Loss: 0.10675991699099541, Final Batch Loss: 0.04926761984825134\n",
      "Epoch 1611, Loss: 0.11759752407670021, Final Batch Loss: 0.06527294218540192\n",
      "Epoch 1612, Loss: 0.0819609947502613, Final Batch Loss: 0.03241758048534393\n",
      "Epoch 1613, Loss: 0.08661480247974396, Final Batch Loss: 0.055294036865234375\n",
      "Epoch 1614, Loss: 0.1024567298591137, Final Batch Loss: 0.040709156543016434\n",
      "Epoch 1615, Loss: 0.0802090223878622, Final Batch Loss: 0.05261026322841644\n",
      "Epoch 1616, Loss: 0.11236448958516121, Final Batch Loss: 0.057763323187828064\n",
      "Epoch 1617, Loss: 0.10585865750908852, Final Batch Loss: 0.07636132836341858\n",
      "Epoch 1618, Loss: 0.11766084656119347, Final Batch Loss: 0.05549759045243263\n",
      "Epoch 1619, Loss: 0.11440859362483025, Final Batch Loss: 0.055438332259655\n",
      "Epoch 1620, Loss: 0.17222070693969727, Final Batch Loss: 0.060677602887153625\n",
      "Epoch 1621, Loss: 0.08781816810369492, Final Batch Loss: 0.03766133263707161\n",
      "Epoch 1622, Loss: 0.10107120499014854, Final Batch Loss: 0.06485666334629059\n",
      "Epoch 1623, Loss: 0.10935340449213982, Final Batch Loss: 0.050722721964120865\n",
      "Epoch 1624, Loss: 0.09096060879528522, Final Batch Loss: 0.02818962372839451\n",
      "Epoch 1625, Loss: 0.0884715486317873, Final Batch Loss: 0.028406353667378426\n",
      "Epoch 1626, Loss: 0.0976102352142334, Final Batch Loss: 0.025828398764133453\n",
      "Epoch 1627, Loss: 0.07680263370275497, Final Batch Loss: 0.03670746088027954\n",
      "Epoch 1628, Loss: 0.10765921138226986, Final Batch Loss: 0.020941948518157005\n",
      "Epoch 1629, Loss: 0.09563423320651054, Final Batch Loss: 0.037006925791502\n",
      "Epoch 1630, Loss: 0.08221760764718056, Final Batch Loss: 0.033758047968149185\n",
      "Epoch 1631, Loss: 0.16414634510874748, Final Batch Loss: 0.10745120793581009\n",
      "Epoch 1632, Loss: 0.09699202701449394, Final Batch Loss: 0.04669153690338135\n",
      "Epoch 1633, Loss: 0.17595646902918816, Final Batch Loss: 0.13748836517333984\n",
      "Epoch 1634, Loss: 0.0914718396961689, Final Batch Loss: 0.04608910530805588\n",
      "Epoch 1635, Loss: 0.08649911358952522, Final Batch Loss: 0.05094223842024803\n",
      "Epoch 1636, Loss: 0.117143165320158, Final Batch Loss: 0.05590004846453667\n",
      "Epoch 1637, Loss: 0.12338345870375633, Final Batch Loss: 0.08385788649320602\n",
      "Epoch 1638, Loss: 0.08325637504458427, Final Batch Loss: 0.029608357697725296\n",
      "Epoch 1639, Loss: 0.0910404808819294, Final Batch Loss: 0.05290787294507027\n",
      "Epoch 1640, Loss: 0.10036258026957512, Final Batch Loss: 0.037245672196149826\n",
      "Epoch 1641, Loss: 0.09293948113918304, Final Batch Loss: 0.058517999947071075\n",
      "Epoch 1642, Loss: 0.10435733199119568, Final Batch Loss: 0.05717244744300842\n",
      "Epoch 1643, Loss: 0.09382018260657787, Final Batch Loss: 0.06711699813604355\n",
      "Epoch 1644, Loss: 0.1237071193754673, Final Batch Loss: 0.0952613353729248\n",
      "Epoch 1645, Loss: 0.09503387659788132, Final Batch Loss: 0.04702203348278999\n",
      "Epoch 1646, Loss: 0.09151914529502392, Final Batch Loss: 0.06464331597089767\n",
      "Epoch 1647, Loss: 0.0971250832080841, Final Batch Loss: 0.05752114579081535\n",
      "Epoch 1648, Loss: 0.09724380820989609, Final Batch Loss: 0.037832751870155334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1649, Loss: 0.11280184611678123, Final Batch Loss: 0.0531870536506176\n",
      "Epoch 1650, Loss: 0.090327687561512, Final Batch Loss: 0.03163110837340355\n",
      "Epoch 1651, Loss: 0.10688256844878197, Final Batch Loss: 0.0634719729423523\n",
      "Epoch 1652, Loss: 0.11551310494542122, Final Batch Loss: 0.07096178829669952\n",
      "Epoch 1653, Loss: 0.13007016107439995, Final Batch Loss: 0.06901434808969498\n",
      "Epoch 1654, Loss: 0.13727134466171265, Final Batch Loss: 0.06495799124240875\n",
      "Epoch 1655, Loss: 0.15930253267288208, Final Batch Loss: 0.07483387738466263\n",
      "Epoch 1656, Loss: 0.11470303684473038, Final Batch Loss: 0.04793928563594818\n",
      "Epoch 1657, Loss: 0.09903763234615326, Final Batch Loss: 0.045989517122507095\n",
      "Epoch 1658, Loss: 0.10828597843647003, Final Batch Loss: 0.07011473923921585\n",
      "Epoch 1659, Loss: 0.11815427616238594, Final Batch Loss: 0.057615216821432114\n",
      "Epoch 1660, Loss: 0.11270986497402191, Final Batch Loss: 0.05725495144724846\n",
      "Epoch 1661, Loss: 0.10420792177319527, Final Batch Loss: 0.042790696024894714\n",
      "Epoch 1662, Loss: 0.08984580263495445, Final Batch Loss: 0.042688895016908646\n",
      "Epoch 1663, Loss: 0.11590947210788727, Final Batch Loss: 0.0671171247959137\n",
      "Epoch 1664, Loss: 0.14516045525670052, Final Batch Loss: 0.06227036938071251\n",
      "Epoch 1665, Loss: 0.1409347876906395, Final Batch Loss: 0.09231449663639069\n",
      "Epoch 1666, Loss: 0.1175965927541256, Final Batch Loss: 0.053575512021780014\n",
      "Epoch 1667, Loss: 0.09678691625595093, Final Batch Loss: 0.04278671368956566\n",
      "Epoch 1668, Loss: 0.11994810402393341, Final Batch Loss: 0.04184984415769577\n",
      "Epoch 1669, Loss: 0.1251656897366047, Final Batch Loss: 0.06671001762151718\n",
      "Epoch 1670, Loss: 0.15320206433534622, Final Batch Loss: 0.07969110459089279\n",
      "Epoch 1671, Loss: 0.11273781582713127, Final Batch Loss: 0.047219786792993546\n",
      "Epoch 1672, Loss: 0.10153847560286522, Final Batch Loss: 0.050670817494392395\n",
      "Epoch 1673, Loss: 0.13506514206528664, Final Batch Loss: 0.08191858977079391\n",
      "Epoch 1674, Loss: 0.10432323813438416, Final Batch Loss: 0.054807450622320175\n",
      "Epoch 1675, Loss: 0.1252131126821041, Final Batch Loss: 0.05174829438328743\n",
      "Epoch 1676, Loss: 0.10395277291536331, Final Batch Loss: 0.04330836981534958\n",
      "Epoch 1677, Loss: 0.13466020673513412, Final Batch Loss: 0.05553731322288513\n",
      "Epoch 1678, Loss: 0.11061999201774597, Final Batch Loss: 0.05319537594914436\n",
      "Epoch 1679, Loss: 0.10420717298984528, Final Batch Loss: 0.04376677796244621\n",
      "Epoch 1680, Loss: 0.09241766482591629, Final Batch Loss: 0.04249952360987663\n",
      "Epoch 1681, Loss: 0.10613369569182396, Final Batch Loss: 0.05210518464446068\n",
      "Epoch 1682, Loss: 0.26922816783189774, Final Batch Loss: 0.18161611258983612\n",
      "Epoch 1683, Loss: 0.11657388880848885, Final Batch Loss: 0.0736963152885437\n",
      "Epoch 1684, Loss: 0.10710309818387032, Final Batch Loss: 0.0501931868493557\n",
      "Epoch 1685, Loss: 0.0918499007821083, Final Batch Loss: 0.045358020812273026\n",
      "Epoch 1686, Loss: 0.2208111733198166, Final Batch Loss: 0.046925634145736694\n",
      "Epoch 1687, Loss: 0.1104031354188919, Final Batch Loss: 0.07175753265619278\n",
      "Epoch 1688, Loss: 0.11808939650654793, Final Batch Loss: 0.06560579687356949\n",
      "Epoch 1689, Loss: 0.0994531475007534, Final Batch Loss: 0.05085592716932297\n",
      "Epoch 1690, Loss: 0.09185303375124931, Final Batch Loss: 0.05242665112018585\n",
      "Epoch 1691, Loss: 0.10187181830406189, Final Batch Loss: 0.062057048082351685\n",
      "Epoch 1692, Loss: 0.11497609317302704, Final Batch Loss: 0.06741127371788025\n",
      "Epoch 1693, Loss: 0.09560221806168556, Final Batch Loss: 0.03599613159894943\n",
      "Epoch 1694, Loss: 0.1359240710735321, Final Batch Loss: 0.09600375592708588\n",
      "Epoch 1695, Loss: 0.11734490469098091, Final Batch Loss: 0.06070081889629364\n",
      "Epoch 1696, Loss: 0.10414058342576027, Final Batch Loss: 0.06043926253914833\n",
      "Epoch 1697, Loss: 0.14753271266818047, Final Batch Loss: 0.05151520296931267\n",
      "Epoch 1698, Loss: 0.0850601140409708, Final Batch Loss: 0.029538417235016823\n",
      "Epoch 1699, Loss: 0.11271074786782265, Final Batch Loss: 0.05920135974884033\n",
      "Epoch 1700, Loss: 0.11420435085892677, Final Batch Loss: 0.0690067932009697\n",
      "Epoch 1701, Loss: 0.10814988613128662, Final Batch Loss: 0.040877506136894226\n",
      "Epoch 1702, Loss: 0.1118074283003807, Final Batch Loss: 0.051263030618429184\n",
      "Epoch 1703, Loss: 0.10971342027187347, Final Batch Loss: 0.06919799745082855\n",
      "Epoch 1704, Loss: 0.14730869233608246, Final Batch Loss: 0.07254020124673843\n",
      "Epoch 1705, Loss: 0.08719341643154621, Final Batch Loss: 0.02609691582620144\n",
      "Epoch 1706, Loss: 0.07001237943768501, Final Batch Loss: 0.01747911423444748\n",
      "Epoch 1707, Loss: 0.11489636823534966, Final Batch Loss: 0.06419624388217926\n",
      "Epoch 1708, Loss: 0.11903399601578712, Final Batch Loss: 0.08295688033103943\n",
      "Epoch 1709, Loss: 0.10334895923733711, Final Batch Loss: 0.06936392188072205\n",
      "Epoch 1710, Loss: 0.11101231351494789, Final Batch Loss: 0.04922730475664139\n",
      "Epoch 1711, Loss: 0.09135911613702774, Final Batch Loss: 0.04558664932847023\n",
      "Epoch 1712, Loss: 0.19139878451824188, Final Batch Loss: 0.1358892172574997\n",
      "Epoch 1713, Loss: 0.1213100366294384, Final Batch Loss: 0.06523896753787994\n",
      "Epoch 1714, Loss: 0.12235381081700325, Final Batch Loss: 0.08019871264696121\n",
      "Epoch 1715, Loss: 0.11411244049668312, Final Batch Loss: 0.06322949379682541\n",
      "Epoch 1716, Loss: 0.09004293382167816, Final Batch Loss: 0.04867355898022652\n",
      "Epoch 1717, Loss: 0.10064061358571053, Final Batch Loss: 0.04130277410149574\n",
      "Epoch 1718, Loss: 0.12165427953004837, Final Batch Loss: 0.024984464049339294\n",
      "Epoch 1719, Loss: 0.09533281996846199, Final Batch Loss: 0.05186149477958679\n",
      "Epoch 1720, Loss: 0.08808714523911476, Final Batch Loss: 0.03783868998289108\n",
      "Epoch 1721, Loss: 0.09522933140397072, Final Batch Loss: 0.04458115994930267\n",
      "Epoch 1722, Loss: 0.08145049773156643, Final Batch Loss: 0.02569957636296749\n",
      "Epoch 1723, Loss: 0.13444136828184128, Final Batch Loss: 0.07166729867458344\n",
      "Epoch 1724, Loss: 0.09374488890171051, Final Batch Loss: 0.035760097205638885\n",
      "Epoch 1725, Loss: 0.10296317934989929, Final Batch Loss: 0.045331258326768875\n",
      "Epoch 1726, Loss: 0.10667729005217552, Final Batch Loss: 0.042664315551519394\n",
      "Epoch 1727, Loss: 0.10977259650826454, Final Batch Loss: 0.05607660114765167\n",
      "Epoch 1728, Loss: 0.105555210262537, Final Batch Loss: 0.054198771715164185\n",
      "Epoch 1729, Loss: 0.08029646426439285, Final Batch Loss: 0.04123593121767044\n",
      "Epoch 1730, Loss: 0.10168936476111412, Final Batch Loss: 0.06620724499225616\n",
      "Epoch 1731, Loss: 0.07852794043719769, Final Batch Loss: 0.028246307745575905\n",
      "Epoch 1732, Loss: 0.1064811646938324, Final Batch Loss: 0.06573198735713959\n",
      "Epoch 1733, Loss: 0.0883682332932949, Final Batch Loss: 0.034813906997442245\n",
      "Epoch 1734, Loss: 0.0979839377105236, Final Batch Loss: 0.04595835506916046\n",
      "Epoch 1735, Loss: 0.12696979567408562, Final Batch Loss: 0.0891074538230896\n",
      "Epoch 1736, Loss: 0.09820511937141418, Final Batch Loss: 0.06677563488483429\n",
      "Epoch 1737, Loss: 0.0984933003783226, Final Batch Loss: 0.04954865947365761\n",
      "Epoch 1738, Loss: 0.07316901907324791, Final Batch Loss: 0.033344559371471405\n",
      "Epoch 1739, Loss: 0.09273875504732132, Final Batch Loss: 0.045708414167165756\n",
      "Epoch 1740, Loss: 0.10606838762760162, Final Batch Loss: 0.04827946797013283\n",
      "Epoch 1741, Loss: 0.12381930649280548, Final Batch Loss: 0.045545369386672974\n",
      "Epoch 1742, Loss: 0.12115418910980225, Final Batch Loss: 0.05913250893354416\n",
      "Epoch 1743, Loss: 0.14610350131988525, Final Batch Loss: 0.08418984711170197\n",
      "Epoch 1744, Loss: 0.13230736926198006, Final Batch Loss: 0.05139202997088432\n",
      "Epoch 1745, Loss: 0.14259221404790878, Final Batch Loss: 0.07496190071105957\n",
      "Epoch 1746, Loss: 0.11040397733449936, Final Batch Loss: 0.052575916051864624\n",
      "Epoch 1747, Loss: 0.12063166499137878, Final Batch Loss: 0.062198467552661896\n",
      "Epoch 1748, Loss: 0.09269298613071442, Final Batch Loss: 0.05285784229636192\n",
      "Epoch 1749, Loss: 0.11183054372668266, Final Batch Loss: 0.06598961353302002\n",
      "Epoch 1750, Loss: 0.1263270154595375, Final Batch Loss: 0.058943457901477814\n",
      "Epoch 1751, Loss: 0.1452358104288578, Final Batch Loss: 0.05400991812348366\n",
      "Epoch 1752, Loss: 0.10904976725578308, Final Batch Loss: 0.05265802890062332\n",
      "Epoch 1753, Loss: 0.10801827535033226, Final Batch Loss: 0.047555964440107346\n",
      "Epoch 1754, Loss: 0.09370097145438194, Final Batch Loss: 0.05399312824010849\n",
      "Epoch 1755, Loss: 0.12019078433513641, Final Batch Loss: 0.06643393635749817\n",
      "Epoch 1756, Loss: 0.17119282484054565, Final Batch Loss: 0.0691533237695694\n",
      "Epoch 1757, Loss: 0.12113712355494499, Final Batch Loss: 0.05467291548848152\n",
      "Epoch 1758, Loss: 0.11009649559855461, Final Batch Loss: 0.0685310885310173\n",
      "Epoch 1759, Loss: 0.10553866624832153, Final Batch Loss: 0.05191347748041153\n",
      "Epoch 1760, Loss: 0.09582185372710228, Final Batch Loss: 0.057973869144916534\n",
      "Epoch 1761, Loss: 0.14616210758686066, Final Batch Loss: 0.06906397640705109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1762, Loss: 0.11353372782468796, Final Batch Loss: 0.06884583085775375\n",
      "Epoch 1763, Loss: 0.10826173797249794, Final Batch Loss: 0.043570686131715775\n",
      "Epoch 1764, Loss: 0.08379607647657394, Final Batch Loss: 0.047020312398672104\n",
      "Epoch 1765, Loss: 0.10055293887853622, Final Batch Loss: 0.0508553683757782\n",
      "Epoch 1766, Loss: 0.11394952982664108, Final Batch Loss: 0.07096010446548462\n",
      "Epoch 1767, Loss: 0.12688051909208298, Final Batch Loss: 0.06381876766681671\n",
      "Epoch 1768, Loss: 0.10383894294500351, Final Batch Loss: 0.04666914418339729\n",
      "Epoch 1769, Loss: 0.08932417258620262, Final Batch Loss: 0.0311940535902977\n",
      "Epoch 1770, Loss: 0.09084612876176834, Final Batch Loss: 0.055271558463573456\n",
      "Epoch 1771, Loss: 0.10028280317783356, Final Batch Loss: 0.05116927623748779\n",
      "Epoch 1772, Loss: 0.1308097541332245, Final Batch Loss: 0.06275686621665955\n",
      "Epoch 1773, Loss: 0.10688063129782677, Final Batch Loss: 0.06428994238376617\n",
      "Epoch 1774, Loss: 0.10139282047748566, Final Batch Loss: 0.04038509353995323\n",
      "Epoch 1775, Loss: 0.09732988476753235, Final Batch Loss: 0.04954313859343529\n",
      "Epoch 1776, Loss: 0.11323757097125053, Final Batch Loss: 0.07686761021614075\n",
      "Epoch 1777, Loss: 0.10639066994190216, Final Batch Loss: 0.06729499995708466\n",
      "Epoch 1778, Loss: 0.10344471409916878, Final Batch Loss: 0.062480147927999496\n",
      "Epoch 1779, Loss: 0.09686635807156563, Final Batch Loss: 0.040964387357234955\n",
      "Epoch 1780, Loss: 0.1013745591044426, Final Batch Loss: 0.05043519288301468\n",
      "Epoch 1781, Loss: 0.10234248265624046, Final Batch Loss: 0.05571844056248665\n",
      "Epoch 1782, Loss: 0.16129421070218086, Final Batch Loss: 0.11458341032266617\n",
      "Epoch 1783, Loss: 0.12008821591734886, Final Batch Loss: 0.0668504610657692\n",
      "Epoch 1784, Loss: 0.1036742739379406, Final Batch Loss: 0.04991456866264343\n",
      "Epoch 1785, Loss: 0.09072302281856537, Final Batch Loss: 0.03221755847334862\n",
      "Epoch 1786, Loss: 0.09348863363265991, Final Batch Loss: 0.04238862544298172\n",
      "Epoch 1787, Loss: 0.1537676826119423, Final Batch Loss: 0.04895436018705368\n",
      "Epoch 1788, Loss: 0.09207318723201752, Final Batch Loss: 0.05500197783112526\n",
      "Epoch 1789, Loss: 0.08535577729344368, Final Batch Loss: 0.03993867337703705\n",
      "Epoch 1790, Loss: 0.0961308404803276, Final Batch Loss: 0.04107833281159401\n",
      "Epoch 1791, Loss: 0.14496316760778427, Final Batch Loss: 0.09041842073202133\n",
      "Epoch 1792, Loss: 0.11376255378127098, Final Batch Loss: 0.06995554268360138\n",
      "Epoch 1793, Loss: 0.0909687764942646, Final Batch Loss: 0.05059380456805229\n",
      "Epoch 1794, Loss: 0.09291177615523338, Final Batch Loss: 0.046344857662916183\n",
      "Epoch 1795, Loss: 0.08261645585298538, Final Batch Loss: 0.03473373129963875\n",
      "Epoch 1796, Loss: 0.09076056256890297, Final Batch Loss: 0.048818182200193405\n",
      "Epoch 1797, Loss: 0.1020243689417839, Final Batch Loss: 0.03916773200035095\n",
      "Epoch 1798, Loss: 0.11740386486053467, Final Batch Loss: 0.07235440611839294\n",
      "Epoch 1799, Loss: 0.10281222686171532, Final Batch Loss: 0.05544504523277283\n",
      "Epoch 1800, Loss: 0.09139203280210495, Final Batch Loss: 0.04419085383415222\n",
      "Epoch 1801, Loss: 0.23591454699635506, Final Batch Loss: 0.1874479204416275\n",
      "Epoch 1802, Loss: 0.14110931754112244, Final Batch Loss: 0.0619535893201828\n",
      "Epoch 1803, Loss: 0.09234392642974854, Final Batch Loss: 0.05574023351073265\n",
      "Epoch 1804, Loss: 0.13095294311642647, Final Batch Loss: 0.07804051786661148\n",
      "Epoch 1805, Loss: 0.11760009825229645, Final Batch Loss: 0.05491356551647186\n",
      "Epoch 1806, Loss: 0.080099081620574, Final Batch Loss: 0.026931671425700188\n",
      "Epoch 1807, Loss: 0.09410831704735756, Final Batch Loss: 0.05340820178389549\n",
      "Epoch 1808, Loss: 0.12071812897920609, Final Batch Loss: 0.06939281523227692\n",
      "Epoch 1809, Loss: 0.15538235753774643, Final Batch Loss: 0.10329266637563705\n",
      "Epoch 1810, Loss: 0.12614485248923302, Final Batch Loss: 0.05134495720267296\n",
      "Epoch 1811, Loss: 0.10808760300278664, Final Batch Loss: 0.053981274366378784\n",
      "Epoch 1812, Loss: 0.10270332545042038, Final Batch Loss: 0.03957372158765793\n",
      "Epoch 1813, Loss: 0.10700173303484917, Final Batch Loss: 0.06565648317337036\n",
      "Epoch 1814, Loss: 0.12109065800905228, Final Batch Loss: 0.07404053211212158\n",
      "Epoch 1815, Loss: 0.1049606166779995, Final Batch Loss: 0.042318183928728104\n",
      "Epoch 1816, Loss: 0.13518283888697624, Final Batch Loss: 0.053062256425619125\n",
      "Epoch 1817, Loss: 0.09819009900093079, Final Batch Loss: 0.056200213730335236\n",
      "Epoch 1818, Loss: 0.10390204563736916, Final Batch Loss: 0.06056998297572136\n",
      "Epoch 1819, Loss: 0.09437431022524834, Final Batch Loss: 0.04176650941371918\n",
      "Epoch 1820, Loss: 0.11598943546414375, Final Batch Loss: 0.05955447256565094\n",
      "Epoch 1821, Loss: 0.09978210180997849, Final Batch Loss: 0.055516306310892105\n",
      "Epoch 1822, Loss: 0.08951918035745621, Final Batch Loss: 0.040262073278427124\n",
      "Epoch 1823, Loss: 0.10735678300261497, Final Batch Loss: 0.03861043229699135\n",
      "Epoch 1824, Loss: 0.0871023628860712, Final Batch Loss: 0.02888067625463009\n",
      "Epoch 1825, Loss: 0.09239711612462997, Final Batch Loss: 0.04791185259819031\n",
      "Epoch 1826, Loss: 0.10097069293260574, Final Batch Loss: 0.053283993154764175\n",
      "Epoch 1827, Loss: 0.10631391033530235, Final Batch Loss: 0.04729781299829483\n",
      "Epoch 1828, Loss: 0.1255120486021042, Final Batch Loss: 0.05776943266391754\n",
      "Epoch 1829, Loss: 0.09763333573937416, Final Batch Loss: 0.05222666263580322\n",
      "Epoch 1830, Loss: 0.1397995389997959, Final Batch Loss: 0.05676623061299324\n",
      "Epoch 1831, Loss: 0.10994315892457962, Final Batch Loss: 0.0542810894548893\n",
      "Epoch 1832, Loss: 0.106672003865242, Final Batch Loss: 0.0744842067360878\n",
      "Epoch 1833, Loss: 0.10042239725589752, Final Batch Loss: 0.05806544050574303\n",
      "Epoch 1834, Loss: 0.08633171766996384, Final Batch Loss: 0.05324950069189072\n",
      "Epoch 1835, Loss: 0.11212381720542908, Final Batch Loss: 0.05481493100523949\n",
      "Epoch 1836, Loss: 0.10516060143709183, Final Batch Loss: 0.0510905459523201\n",
      "Epoch 1837, Loss: 0.1126939244568348, Final Batch Loss: 0.058464761823415756\n",
      "Epoch 1838, Loss: 0.09170184656977654, Final Batch Loss: 0.03113940730690956\n",
      "Epoch 1839, Loss: 0.12649082764983177, Final Batch Loss: 0.05265805497765541\n",
      "Epoch 1840, Loss: 0.1398710384964943, Final Batch Loss: 0.06456652283668518\n",
      "Epoch 1841, Loss: 0.09969653934240341, Final Batch Loss: 0.06579804420471191\n",
      "Epoch 1842, Loss: 0.09810754656791687, Final Batch Loss: 0.05462552607059479\n",
      "Epoch 1843, Loss: 0.10635858401656151, Final Batch Loss: 0.05671238899230957\n",
      "Epoch 1844, Loss: 0.11313756927847862, Final Batch Loss: 0.04715104028582573\n",
      "Epoch 1845, Loss: 0.13224387913942337, Final Batch Loss: 0.07787321507930756\n",
      "Epoch 1846, Loss: 0.09926754236221313, Final Batch Loss: 0.04041532799601555\n",
      "Epoch 1847, Loss: 0.09896363317966461, Final Batch Loss: 0.05313357710838318\n",
      "Epoch 1848, Loss: 0.09947677701711655, Final Batch Loss: 0.056670814752578735\n",
      "Epoch 1849, Loss: 0.12448423728346825, Final Batch Loss: 0.07037921994924545\n",
      "Epoch 1850, Loss: 0.10227830335497856, Final Batch Loss: 0.05123291164636612\n",
      "Epoch 1851, Loss: 0.08394258469343185, Final Batch Loss: 0.04429946839809418\n",
      "Epoch 1852, Loss: 0.08467523008584976, Final Batch Loss: 0.04614577442407608\n",
      "Epoch 1853, Loss: 0.16905511915683746, Final Batch Loss: 0.0720255970954895\n",
      "Epoch 1854, Loss: 0.13374412432312965, Final Batch Loss: 0.07579398155212402\n",
      "Epoch 1855, Loss: 0.11403168365359306, Final Batch Loss: 0.05172095447778702\n",
      "Epoch 1856, Loss: 0.13127943128347397, Final Batch Loss: 0.06355086714029312\n",
      "Epoch 1857, Loss: 0.1243344135582447, Final Batch Loss: 0.06877654790878296\n",
      "Epoch 1858, Loss: 0.10400230437517166, Final Batch Loss: 0.04991781711578369\n",
      "Epoch 1859, Loss: 0.11406787857413292, Final Batch Loss: 0.060116905719041824\n",
      "Epoch 1860, Loss: 0.10026736184954643, Final Batch Loss: 0.04490275681018829\n",
      "Epoch 1861, Loss: 0.11012468859553337, Final Batch Loss: 0.06971343606710434\n",
      "Epoch 1862, Loss: 0.10785777494311333, Final Batch Loss: 0.04654526337981224\n",
      "Epoch 1863, Loss: 0.13868233188986778, Final Batch Loss: 0.08300204575061798\n",
      "Epoch 1864, Loss: 0.13148749619722366, Final Batch Loss: 0.07753080129623413\n",
      "Epoch 1865, Loss: 0.10839803144335747, Final Batch Loss: 0.044487450271844864\n",
      "Epoch 1866, Loss: 0.11865943297743797, Final Batch Loss: 0.042923275381326675\n",
      "Epoch 1867, Loss: 0.09147876501083374, Final Batch Loss: 0.03377690538764\n",
      "Epoch 1868, Loss: 0.10428596660494804, Final Batch Loss: 0.04918632283806801\n",
      "Epoch 1869, Loss: 0.08700619265437126, Final Batch Loss: 0.04913906753063202\n",
      "Epoch 1870, Loss: 0.07110704109072685, Final Batch Loss: 0.03455944359302521\n",
      "Epoch 1871, Loss: 0.09723509848117828, Final Batch Loss: 0.04779064655303955\n",
      "Epoch 1872, Loss: 0.08936240896582603, Final Batch Loss: 0.04353731498122215\n",
      "Epoch 1873, Loss: 0.09213684499263763, Final Batch Loss: 0.041515689343214035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1874, Loss: 0.10521894693374634, Final Batch Loss: 0.05868712440133095\n",
      "Epoch 1875, Loss: 0.13207299262285233, Final Batch Loss: 0.07401872426271439\n",
      "Epoch 1876, Loss: 0.09067490696907043, Final Batch Loss: 0.04331677034497261\n",
      "Epoch 1877, Loss: 0.17395469546318054, Final Batch Loss: 0.0872638151049614\n",
      "Epoch 1878, Loss: 0.13853513076901436, Final Batch Loss: 0.11131200194358826\n",
      "Epoch 1879, Loss: 0.08266821131110191, Final Batch Loss: 0.03726356104016304\n",
      "Epoch 1880, Loss: 0.10320278257131577, Final Batch Loss: 0.054262641817331314\n",
      "Epoch 1881, Loss: 0.10924990847706795, Final Batch Loss: 0.0636613592505455\n",
      "Epoch 1882, Loss: 0.10060563683509827, Final Batch Loss: 0.029993541538715363\n",
      "Epoch 1883, Loss: 0.09470625594258308, Final Batch Loss: 0.03834860026836395\n",
      "Epoch 1884, Loss: 0.08605829067528248, Final Batch Loss: 0.026551058515906334\n",
      "Epoch 1885, Loss: 0.08765696361660957, Final Batch Loss: 0.0394025556743145\n",
      "Epoch 1886, Loss: 0.10276464745402336, Final Batch Loss: 0.05598914995789528\n",
      "Epoch 1887, Loss: 0.10483624041080475, Final Batch Loss: 0.06624159216880798\n",
      "Epoch 1888, Loss: 0.08242035284638405, Final Batch Loss: 0.056634679436683655\n",
      "Epoch 1889, Loss: 0.09469025395810604, Final Batch Loss: 0.03033909760415554\n",
      "Epoch 1890, Loss: 0.09434334561228752, Final Batch Loss: 0.0518665611743927\n",
      "Epoch 1891, Loss: 0.11819635704159737, Final Batch Loss: 0.0639520063996315\n",
      "Epoch 1892, Loss: 0.07082541659474373, Final Batch Loss: 0.03575291857123375\n",
      "Epoch 1893, Loss: 0.0981748066842556, Final Batch Loss: 0.052048224955797195\n",
      "Epoch 1894, Loss: 0.10896655172109604, Final Batch Loss: 0.033233754336833954\n",
      "Epoch 1895, Loss: 0.08375926688313484, Final Batch Loss: 0.04588845372200012\n",
      "Epoch 1896, Loss: 0.10306273959577084, Final Batch Loss: 0.027250049635767937\n",
      "Epoch 1897, Loss: 0.14980820566415787, Final Batch Loss: 0.07657518982887268\n",
      "Epoch 1898, Loss: 0.1132747046649456, Final Batch Loss: 0.04678955301642418\n",
      "Epoch 1899, Loss: 0.09999794885516167, Final Batch Loss: 0.0641675814986229\n",
      "Epoch 1900, Loss: 0.12300217896699905, Final Batch Loss: 0.049381472170352936\n",
      "Epoch 1901, Loss: 0.0831948071718216, Final Batch Loss: 0.03982369601726532\n",
      "Epoch 1902, Loss: 0.0958772525191307, Final Batch Loss: 0.051286909729242325\n",
      "Epoch 1903, Loss: 0.11962833255529404, Final Batch Loss: 0.05494391918182373\n",
      "Epoch 1904, Loss: 0.10474453493952751, Final Batch Loss: 0.049413345754146576\n",
      "Epoch 1905, Loss: 0.07766282744705677, Final Batch Loss: 0.047676753252744675\n",
      "Epoch 1906, Loss: 0.10346144437789917, Final Batch Loss: 0.046482495963573456\n",
      "Epoch 1907, Loss: 0.09537005797028542, Final Batch Loss: 0.0608348622918129\n",
      "Epoch 1908, Loss: 0.17013940960168839, Final Batch Loss: 0.07027267664670944\n",
      "Epoch 1909, Loss: 0.08842921257019043, Final Batch Loss: 0.02769589051604271\n",
      "Epoch 1910, Loss: 0.10510566830635071, Final Batch Loss: 0.057046934962272644\n",
      "Epoch 1911, Loss: 0.1701519563794136, Final Batch Loss: 0.06881159543991089\n",
      "Epoch 1912, Loss: 0.08628493547439575, Final Batch Loss: 0.054686289280653\n",
      "Epoch 1913, Loss: 0.10175453871488571, Final Batch Loss: 0.056238867342472076\n",
      "Epoch 1914, Loss: 0.13591571897268295, Final Batch Loss: 0.04655618220567703\n",
      "Epoch 1915, Loss: 0.13314896821975708, Final Batch Loss: 0.06995286047458649\n",
      "Epoch 1916, Loss: 0.10265572369098663, Final Batch Loss: 0.03955719619989395\n",
      "Epoch 1917, Loss: 0.09272143989801407, Final Batch Loss: 0.021622739732265472\n",
      "Epoch 1918, Loss: 0.11660442128777504, Final Batch Loss: 0.06128449738025665\n",
      "Epoch 1919, Loss: 0.10100897029042244, Final Batch Loss: 0.040332309901714325\n",
      "Epoch 1920, Loss: 0.1739157997071743, Final Batch Loss: 0.11992770433425903\n",
      "Epoch 1921, Loss: 0.13748407363891602, Final Batch Loss: 0.07036621123552322\n",
      "Epoch 1922, Loss: 0.10578812658786774, Final Batch Loss: 0.04111762344837189\n",
      "Epoch 1923, Loss: 0.128304623067379, Final Batch Loss: 0.06369078904390335\n",
      "Epoch 1924, Loss: 0.1227976642549038, Final Batch Loss: 0.07148375362157822\n",
      "Epoch 1925, Loss: 0.11554892361164093, Final Batch Loss: 0.06859754025936127\n",
      "Epoch 1926, Loss: 0.11634363234043121, Final Batch Loss: 0.0628705695271492\n",
      "Epoch 1927, Loss: 0.10808499157428741, Final Batch Loss: 0.06924930214881897\n",
      "Epoch 1928, Loss: 0.0953584536910057, Final Batch Loss: 0.040364187210798264\n",
      "Epoch 1929, Loss: 0.14272624626755714, Final Batch Loss: 0.08492601662874222\n",
      "Epoch 1930, Loss: 0.11098584905266762, Final Batch Loss: 0.06989166140556335\n",
      "Epoch 1931, Loss: 0.11416573077440262, Final Batch Loss: 0.07237423956394196\n",
      "Epoch 1932, Loss: 0.12247544154524803, Final Batch Loss: 0.05959852412343025\n",
      "Epoch 1933, Loss: 0.10457653179764748, Final Batch Loss: 0.052311137318611145\n",
      "Epoch 1934, Loss: 0.11249887943267822, Final Batch Loss: 0.035300880670547485\n",
      "Epoch 1935, Loss: 0.1183699332177639, Final Batch Loss: 0.05250344052910805\n",
      "Epoch 1936, Loss: 0.1192549392580986, Final Batch Loss: 0.07877448201179504\n",
      "Epoch 1937, Loss: 0.13696710765361786, Final Batch Loss: 0.06081399321556091\n",
      "Epoch 1938, Loss: 0.1710735708475113, Final Batch Loss: 0.08918766677379608\n",
      "Epoch 1939, Loss: 0.204590804874897, Final Batch Loss: 0.14612632989883423\n",
      "Epoch 1940, Loss: 0.11071831360459328, Final Batch Loss: 0.053447868674993515\n",
      "Epoch 1941, Loss: 0.11535745859146118, Final Batch Loss: 0.04672195762395859\n",
      "Epoch 1942, Loss: 0.14294597879052162, Final Batch Loss: 0.08044850081205368\n",
      "Epoch 1943, Loss: 0.11384481191635132, Final Batch Loss: 0.06135905161499977\n",
      "Epoch 1944, Loss: 0.15817910432815552, Final Batch Loss: 0.09398198872804642\n",
      "Epoch 1945, Loss: 0.10429614782333374, Final Batch Loss: 0.0650477483868599\n",
      "Epoch 1946, Loss: 0.15413636714220047, Final Batch Loss: 0.08190075308084488\n",
      "Epoch 1947, Loss: 0.11763870716094971, Final Batch Loss: 0.07215911149978638\n",
      "Epoch 1948, Loss: 0.10658202692866325, Final Batch Loss: 0.06662902235984802\n",
      "Epoch 1949, Loss: 0.09732834249734879, Final Batch Loss: 0.05114264041185379\n",
      "Epoch 1950, Loss: 0.09595373645424843, Final Batch Loss: 0.05083083361387253\n",
      "Epoch 1951, Loss: 0.1128915399312973, Final Batch Loss: 0.06887956708669662\n",
      "Epoch 1952, Loss: 0.0816282220184803, Final Batch Loss: 0.03927014395594597\n",
      "Epoch 1953, Loss: 0.09728122502565384, Final Batch Loss: 0.044457197189331055\n",
      "Epoch 1954, Loss: 0.09980660304427147, Final Batch Loss: 0.049768466502428055\n",
      "Epoch 1955, Loss: 0.09501354396343231, Final Batch Loss: 0.06254922598600388\n",
      "Epoch 1956, Loss: 0.09752344340085983, Final Batch Loss: 0.07521948218345642\n",
      "Epoch 1957, Loss: 0.09585240110754967, Final Batch Loss: 0.03241926059126854\n",
      "Epoch 1958, Loss: 0.12427601963281631, Final Batch Loss: 0.06805519014596939\n",
      "Epoch 1959, Loss: 0.10921211913228035, Final Batch Loss: 0.06192660331726074\n",
      "Epoch 1960, Loss: 0.15017109364271164, Final Batch Loss: 0.08425464481115341\n",
      "Epoch 1961, Loss: 0.17807772010564804, Final Batch Loss: 0.11425060033798218\n",
      "Epoch 1962, Loss: 0.1476164236664772, Final Batch Loss: 0.08243673294782639\n",
      "Epoch 1963, Loss: 0.10736911743879318, Final Batch Loss: 0.03362172842025757\n",
      "Epoch 1964, Loss: 0.08831369876861572, Final Batch Loss: 0.053262047469615936\n",
      "Epoch 1965, Loss: 0.17076019942760468, Final Batch Loss: 0.07275910675525665\n",
      "Epoch 1966, Loss: 0.08641616627573967, Final Batch Loss: 0.05065738782286644\n",
      "Epoch 1967, Loss: 0.13217247277498245, Final Batch Loss: 0.07596662640571594\n",
      "Epoch 1968, Loss: 0.12763941287994385, Final Batch Loss: 0.08048568665981293\n",
      "Epoch 1969, Loss: 0.10426997393369675, Final Batch Loss: 0.05802159383893013\n",
      "Epoch 1970, Loss: 0.10576479509472847, Final Batch Loss: 0.054364584386348724\n",
      "Epoch 1971, Loss: 0.10108532942831516, Final Batch Loss: 0.03106069751083851\n",
      "Epoch 1972, Loss: 0.11124220117926598, Final Batch Loss: 0.0659097358584404\n",
      "Epoch 1973, Loss: 0.11267389729619026, Final Batch Loss: 0.04615313187241554\n",
      "Epoch 1974, Loss: 0.08898580074310303, Final Batch Loss: 0.04023498669266701\n",
      "Epoch 1975, Loss: 0.0874653123319149, Final Batch Loss: 0.04604156315326691\n",
      "Epoch 1976, Loss: 0.09719286486506462, Final Batch Loss: 0.03676275908946991\n",
      "Epoch 1977, Loss: 0.11547135561704636, Final Batch Loss: 0.06186002120375633\n",
      "Epoch 1978, Loss: 0.10277757607400417, Final Batch Loss: 0.07318846136331558\n",
      "Epoch 1979, Loss: 0.09134746715426445, Final Batch Loss: 0.04849274456501007\n",
      "Epoch 1980, Loss: 0.09455084428191185, Final Batch Loss: 0.053521204739809036\n",
      "Epoch 1981, Loss: 0.09170438349246979, Final Batch Loss: 0.04683959484100342\n",
      "Epoch 1982, Loss: 0.10160484164953232, Final Batch Loss: 0.06690123677253723\n",
      "Epoch 1983, Loss: 0.09486187621951103, Final Batch Loss: 0.06209196895360947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1984, Loss: 0.08905550464987755, Final Batch Loss: 0.056176621466875076\n",
      "Epoch 1985, Loss: 0.08333385549485683, Final Batch Loss: 0.029481394216418266\n",
      "Epoch 1986, Loss: 0.1114530935883522, Final Batch Loss: 0.06276044994592667\n",
      "Epoch 1987, Loss: 0.10227863863110542, Final Batch Loss: 0.06129782646894455\n",
      "Epoch 1988, Loss: 0.08925344049930573, Final Batch Loss: 0.044263582676649094\n",
      "Epoch 1989, Loss: 0.08710891008377075, Final Batch Loss: 0.041240859776735306\n",
      "Epoch 1990, Loss: 0.09975260123610497, Final Batch Loss: 0.054588548839092255\n",
      "Epoch 1991, Loss: 0.08466785587370396, Final Batch Loss: 0.05774679407477379\n",
      "Epoch 1992, Loss: 0.09071048721671104, Final Batch Loss: 0.05361403524875641\n",
      "Epoch 1993, Loss: 0.10630835220217705, Final Batch Loss: 0.06797321885824203\n",
      "Epoch 1994, Loss: 0.11354183405637741, Final Batch Loss: 0.06298390030860901\n",
      "Epoch 1995, Loss: 0.09969596564769745, Final Batch Loss: 0.03302355855703354\n",
      "Epoch 1996, Loss: 0.0792585089802742, Final Batch Loss: 0.040701672434806824\n",
      "Epoch 1997, Loss: 0.08360124379396439, Final Batch Loss: 0.0395660437643528\n",
      "Epoch 1998, Loss: 0.10476172715425491, Final Batch Loss: 0.04113641381263733\n",
      "Epoch 1999, Loss: 0.08449975028634071, Final Batch Loss: 0.04000265523791313\n",
      "Epoch 2000, Loss: 0.07178636826574802, Final Batch Loss: 0.02871778793632984\n",
      "Epoch 2001, Loss: 0.11424362286925316, Final Batch Loss: 0.06380579620599747\n",
      "Epoch 2002, Loss: 0.0937352366745472, Final Batch Loss: 0.04942784830927849\n",
      "Epoch 2003, Loss: 0.09667510911822319, Final Batch Loss: 0.04890459030866623\n",
      "Epoch 2004, Loss: 0.09376894496381283, Final Batch Loss: 0.024218527600169182\n",
      "Epoch 2005, Loss: 0.0861186608672142, Final Batch Loss: 0.034343212842941284\n",
      "Epoch 2006, Loss: 0.1362629234790802, Final Batch Loss: 0.08276644349098206\n",
      "Epoch 2007, Loss: 0.09616444632411003, Final Batch Loss: 0.04954900965094566\n",
      "Epoch 2008, Loss: 0.07756848633289337, Final Batch Loss: 0.03759726881980896\n",
      "Epoch 2009, Loss: 0.07816430181264877, Final Batch Loss: 0.03363824635744095\n",
      "Epoch 2010, Loss: 0.11620009690523148, Final Batch Loss: 0.06122946739196777\n",
      "Epoch 2011, Loss: 0.10886496305465698, Final Batch Loss: 0.055803779512643814\n",
      "Epoch 2012, Loss: 0.11017413064837456, Final Batch Loss: 0.034875962883234024\n",
      "Epoch 2013, Loss: 0.07092247903347015, Final Batch Loss: 0.03721773624420166\n",
      "Epoch 2014, Loss: 0.12380985170602798, Final Batch Loss: 0.06259770691394806\n",
      "Epoch 2015, Loss: 0.09589321725070477, Final Batch Loss: 0.07029707729816437\n",
      "Epoch 2016, Loss: 0.07508257031440735, Final Batch Loss: 0.03820312023162842\n",
      "Epoch 2017, Loss: 0.09669725969433784, Final Batch Loss: 0.040966566652059555\n",
      "Epoch 2018, Loss: 0.17842819541692734, Final Batch Loss: 0.07101243734359741\n",
      "Epoch 2019, Loss: 0.11147930473089218, Final Batch Loss: 0.04285202920436859\n",
      "Epoch 2020, Loss: 0.08589820750057697, Final Batch Loss: 0.03120882622897625\n",
      "Epoch 2021, Loss: 0.12931182608008385, Final Batch Loss: 0.08123424649238586\n",
      "Epoch 2022, Loss: 0.09119011089205742, Final Batch Loss: 0.05716933310031891\n",
      "Epoch 2023, Loss: 0.1250571720302105, Final Batch Loss: 0.07710322737693787\n",
      "Epoch 2024, Loss: 0.10746535658836365, Final Batch Loss: 0.06099620833992958\n",
      "Epoch 2025, Loss: 0.10139090567827225, Final Batch Loss: 0.04030759260058403\n",
      "Epoch 2026, Loss: 0.09132076054811478, Final Batch Loss: 0.04482114315032959\n",
      "Epoch 2027, Loss: 0.08717573434114456, Final Batch Loss: 0.050069112330675125\n",
      "Epoch 2028, Loss: 0.1976456344127655, Final Batch Loss: 0.12857089936733246\n",
      "Epoch 2029, Loss: 0.09886619076132774, Final Batch Loss: 0.05718815699219704\n",
      "Epoch 2030, Loss: 0.09060803987085819, Final Batch Loss: 0.06857912242412567\n",
      "Epoch 2031, Loss: 0.09081759676337242, Final Batch Loss: 0.03672557324171066\n",
      "Epoch 2032, Loss: 0.08540887385606766, Final Batch Loss: 0.0368749164044857\n",
      "Epoch 2033, Loss: 0.10063643008470535, Final Batch Loss: 0.03493946045637131\n",
      "Epoch 2034, Loss: 0.09623936377465725, Final Batch Loss: 0.07123493403196335\n",
      "Epoch 2035, Loss: 0.13100171089172363, Final Batch Loss: 0.0727294385433197\n",
      "Epoch 2036, Loss: 0.09481041505932808, Final Batch Loss: 0.05619022250175476\n",
      "Epoch 2037, Loss: 0.08920103684067726, Final Batch Loss: 0.03517952561378479\n",
      "Epoch 2038, Loss: 0.10260023549199104, Final Batch Loss: 0.05198124423623085\n",
      "Epoch 2039, Loss: 0.10476993769407272, Final Batch Loss: 0.05510123446583748\n",
      "Epoch 2040, Loss: 0.11508483439683914, Final Batch Loss: 0.061991382390260696\n",
      "Epoch 2041, Loss: 0.08518463745713234, Final Batch Loss: 0.052415814250707626\n",
      "Epoch 2042, Loss: 0.12023675069212914, Final Batch Loss: 0.06115337088704109\n",
      "Epoch 2043, Loss: 0.10019421204924583, Final Batch Loss: 0.05067681521177292\n",
      "Epoch 2044, Loss: 0.1196766272187233, Final Batch Loss: 0.07890014350414276\n",
      "Epoch 2045, Loss: 0.10646263882517815, Final Batch Loss: 0.04352742061018944\n",
      "Epoch 2046, Loss: 0.08161712065339088, Final Batch Loss: 0.04676344245672226\n",
      "Epoch 2047, Loss: 0.11649588122963905, Final Batch Loss: 0.07330445200204849\n",
      "Epoch 2048, Loss: 0.08384334295988083, Final Batch Loss: 0.048368364572525024\n",
      "Epoch 2049, Loss: 0.07621745765209198, Final Batch Loss: 0.03686783090233803\n",
      "Epoch 2050, Loss: 0.10485519096255302, Final Batch Loss: 0.05064398795366287\n",
      "Epoch 2051, Loss: 0.10688993334770203, Final Batch Loss: 0.04518822953104973\n",
      "Epoch 2052, Loss: 0.08103593438863754, Final Batch Loss: 0.045144058763980865\n",
      "Epoch 2053, Loss: 0.08666162937879562, Final Batch Loss: 0.035302359610795975\n",
      "Epoch 2054, Loss: 0.10368239134550095, Final Batch Loss: 0.06483055651187897\n",
      "Epoch 2055, Loss: 0.09902400523424149, Final Batch Loss: 0.037693753838539124\n",
      "Epoch 2056, Loss: 0.09239302575588226, Final Batch Loss: 0.039296168833971024\n",
      "Epoch 2057, Loss: 0.10294733941555023, Final Batch Loss: 0.05877706781029701\n",
      "Epoch 2058, Loss: 0.07093178480863571, Final Batch Loss: 0.032884448766708374\n",
      "Epoch 2059, Loss: 0.11748713999986649, Final Batch Loss: 0.06746392697095871\n",
      "Epoch 2060, Loss: 0.09544902667403221, Final Batch Loss: 0.05217535048723221\n",
      "Epoch 2061, Loss: 0.12873101606965065, Final Batch Loss: 0.0398353673517704\n",
      "Epoch 2062, Loss: 0.09092103317379951, Final Batch Loss: 0.04855649918317795\n",
      "Epoch 2063, Loss: 0.0988643728196621, Final Batch Loss: 0.05463046208024025\n",
      "Epoch 2064, Loss: 0.1450868956744671, Final Batch Loss: 0.04626542702317238\n",
      "Epoch 2065, Loss: 0.10738180950284004, Final Batch Loss: 0.05446665734052658\n",
      "Epoch 2066, Loss: 0.08791577816009521, Final Batch Loss: 0.03688672557473183\n",
      "Epoch 2067, Loss: 0.08604883216321468, Final Batch Loss: 0.029928145930171013\n",
      "Epoch 2068, Loss: 0.11036879941821098, Final Batch Loss: 0.06260660290718079\n",
      "Epoch 2069, Loss: 0.10493556410074234, Final Batch Loss: 0.06267848610877991\n",
      "Epoch 2070, Loss: 0.10097377002239227, Final Batch Loss: 0.03825747221708298\n",
      "Epoch 2071, Loss: 0.08748972602188587, Final Batch Loss: 0.027669569477438927\n",
      "Epoch 2072, Loss: 0.1445215828716755, Final Batch Loss: 0.10621042549610138\n",
      "Epoch 2073, Loss: 0.091373510658741, Final Batch Loss: 0.04940178245306015\n",
      "Epoch 2074, Loss: 0.09213140606880188, Final Batch Loss: 0.04420771077275276\n",
      "Epoch 2075, Loss: 0.07525277696549892, Final Batch Loss: 0.0202465932816267\n",
      "Epoch 2076, Loss: 0.09470859169960022, Final Batch Loss: 0.05732142925262451\n",
      "Epoch 2077, Loss: 0.08947096019983292, Final Batch Loss: 0.05440876632928848\n",
      "Epoch 2078, Loss: 0.08009716868400574, Final Batch Loss: 0.030855365097522736\n",
      "Epoch 2079, Loss: 0.11924182996153831, Final Batch Loss: 0.07071899622678757\n",
      "Epoch 2080, Loss: 0.08705024421215057, Final Batch Loss: 0.04916038364171982\n",
      "Epoch 2081, Loss: 0.11386065930128098, Final Batch Loss: 0.0734756663441658\n",
      "Epoch 2082, Loss: 0.12133629992604256, Final Batch Loss: 0.059033747762441635\n",
      "Epoch 2083, Loss: 0.0857907198369503, Final Batch Loss: 0.04507861286401749\n",
      "Epoch 2084, Loss: 0.13539977371692657, Final Batch Loss: 0.06035630404949188\n",
      "Epoch 2085, Loss: 0.12047213315963745, Final Batch Loss: 0.07438140362501144\n",
      "Epoch 2086, Loss: 0.16051878780126572, Final Batch Loss: 0.07710140943527222\n",
      "Epoch 2087, Loss: 0.09545251354575157, Final Batch Loss: 0.04187559336423874\n",
      "Epoch 2088, Loss: 0.0874234139919281, Final Batch Loss: 0.034171607345342636\n",
      "Epoch 2089, Loss: 0.08984028920531273, Final Batch Loss: 0.0238693468272686\n",
      "Epoch 2090, Loss: 0.1072845347225666, Final Batch Loss: 0.04715244472026825\n",
      "Epoch 2091, Loss: 0.09079129993915558, Final Batch Loss: 0.031331565231084824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2092, Loss: 0.09325522556900978, Final Batch Loss: 0.04962309077382088\n",
      "Epoch 2093, Loss: 0.11638887599110603, Final Batch Loss: 0.044963423162698746\n",
      "Epoch 2094, Loss: 0.09627922624349594, Final Batch Loss: 0.04541559889912605\n",
      "Epoch 2095, Loss: 0.15494363382458687, Final Batch Loss: 0.09763320535421371\n",
      "Epoch 2096, Loss: 0.09135294333100319, Final Batch Loss: 0.05774984508752823\n",
      "Epoch 2097, Loss: 0.0917135588824749, Final Batch Loss: 0.05257536470890045\n",
      "Epoch 2098, Loss: 0.10429288819432259, Final Batch Loss: 0.04631068930029869\n",
      "Epoch 2099, Loss: 0.1240227036178112, Final Batch Loss: 0.06856018304824829\n",
      "Epoch 2100, Loss: 0.14253836125135422, Final Batch Loss: 0.06377283483743668\n",
      "Epoch 2101, Loss: 0.1015413235872984, Final Batch Loss: 0.07120892405509949\n",
      "Epoch 2102, Loss: 0.07930248230695724, Final Batch Loss: 0.032452937215566635\n",
      "Epoch 2103, Loss: 0.07897666096687317, Final Batch Loss: 0.03141005337238312\n",
      "Epoch 2104, Loss: 0.08601615577936172, Final Batch Loss: 0.05347616970539093\n",
      "Epoch 2105, Loss: 0.09153847396373749, Final Batch Loss: 0.041359979659318924\n",
      "Epoch 2106, Loss: 0.10710230097174644, Final Batch Loss: 0.043066319078207016\n",
      "Epoch 2107, Loss: 0.13059623166918755, Final Batch Loss: 0.09235180169343948\n",
      "Epoch 2108, Loss: 0.1209377646446228, Final Batch Loss: 0.052780263125896454\n",
      "Epoch 2109, Loss: 0.11080783605575562, Final Batch Loss: 0.055402129888534546\n",
      "Epoch 2110, Loss: 0.09307176247239113, Final Batch Loss: 0.05343329906463623\n",
      "Epoch 2111, Loss: 0.11991576105356216, Final Batch Loss: 0.04885890334844589\n",
      "Epoch 2112, Loss: 0.10694368928670883, Final Batch Loss: 0.037392757833004\n",
      "Epoch 2113, Loss: 0.09880271181464195, Final Batch Loss: 0.06257442384958267\n",
      "Epoch 2114, Loss: 0.14422524720430374, Final Batch Loss: 0.08024346083402634\n",
      "Epoch 2115, Loss: 0.13899171352386475, Final Batch Loss: 0.07176369428634644\n",
      "Epoch 2116, Loss: 0.08849652856588364, Final Batch Loss: 0.05290858820080757\n",
      "Epoch 2117, Loss: 0.09173233807086945, Final Batch Loss: 0.0570957250893116\n",
      "Epoch 2118, Loss: 0.08533351868391037, Final Batch Loss: 0.0446787066757679\n",
      "Epoch 2119, Loss: 0.0988658107817173, Final Batch Loss: 0.03377918526530266\n",
      "Epoch 2120, Loss: 0.08086465485394001, Final Batch Loss: 0.02275650016963482\n",
      "Epoch 2121, Loss: 0.1190459281206131, Final Batch Loss: 0.04495655745267868\n",
      "Epoch 2122, Loss: 0.09216492623090744, Final Batch Loss: 0.031993743032217026\n",
      "Epoch 2123, Loss: 0.0951017290353775, Final Batch Loss: 0.06074383482336998\n",
      "Epoch 2124, Loss: 0.1249699518084526, Final Batch Loss: 0.04857189208269119\n",
      "Epoch 2125, Loss: 0.11370770260691643, Final Batch Loss: 0.03527197614312172\n",
      "Epoch 2126, Loss: 0.0989612378180027, Final Batch Loss: 0.04768748953938484\n",
      "Epoch 2127, Loss: 0.1092088595032692, Final Batch Loss: 0.05009867995977402\n",
      "Epoch 2128, Loss: 0.10839962586760521, Final Batch Loss: 0.04831024631857872\n",
      "Epoch 2129, Loss: 0.08683730289340019, Final Batch Loss: 0.04587354138493538\n",
      "Epoch 2130, Loss: 0.13571113720536232, Final Batch Loss: 0.05165715143084526\n",
      "Epoch 2131, Loss: 0.09885596111416817, Final Batch Loss: 0.038810763508081436\n",
      "Epoch 2132, Loss: 0.10473683103919029, Final Batch Loss: 0.057649996131658554\n",
      "Epoch 2133, Loss: 0.09306057915091515, Final Batch Loss: 0.04382067173719406\n",
      "Epoch 2134, Loss: 0.06736917421221733, Final Batch Loss: 0.039619941264390945\n",
      "Epoch 2135, Loss: 0.10052110254764557, Final Batch Loss: 0.05724596977233887\n",
      "Epoch 2136, Loss: 0.08107096701860428, Final Batch Loss: 0.03804352879524231\n",
      "Epoch 2137, Loss: 0.08770755305886269, Final Batch Loss: 0.05132587254047394\n",
      "Epoch 2138, Loss: 0.0774880014359951, Final Batch Loss: 0.04620175063610077\n",
      "Epoch 2139, Loss: 0.10384157672524452, Final Batch Loss: 0.060519542545080185\n",
      "Epoch 2140, Loss: 0.08005366101861, Final Batch Loss: 0.0423424057662487\n",
      "Epoch 2141, Loss: 0.08639465644955635, Final Batch Loss: 0.04805142059922218\n",
      "Epoch 2142, Loss: 0.13675956800580025, Final Batch Loss: 0.04954260215163231\n",
      "Epoch 2143, Loss: 0.09208297543227673, Final Batch Loss: 0.06731238961219788\n",
      "Epoch 2144, Loss: 0.09173338860273361, Final Batch Loss: 0.053744152188301086\n",
      "Epoch 2145, Loss: 0.08879946172237396, Final Batch Loss: 0.04044332355260849\n",
      "Epoch 2146, Loss: 0.09844993986189365, Final Batch Loss: 0.06934815645217896\n",
      "Epoch 2147, Loss: 0.10003023967146873, Final Batch Loss: 0.04221750795841217\n",
      "Epoch 2148, Loss: 0.08646431006491184, Final Batch Loss: 0.05601527541875839\n",
      "Epoch 2149, Loss: 0.09660385176539421, Final Batch Loss: 0.04368235170841217\n",
      "Epoch 2150, Loss: 0.08856447041034698, Final Batch Loss: 0.041747357696294785\n",
      "Epoch 2151, Loss: 0.09429840743541718, Final Batch Loss: 0.04489538073539734\n",
      "Epoch 2152, Loss: 0.08315275236964226, Final Batch Loss: 0.04928364232182503\n",
      "Epoch 2153, Loss: 0.12582891061902046, Final Batch Loss: 0.06480156630277634\n",
      "Epoch 2154, Loss: 0.10014190897345543, Final Batch Loss: 0.04074916988611221\n",
      "Epoch 2155, Loss: 0.09958118200302124, Final Batch Loss: 0.0588354617357254\n",
      "Epoch 2156, Loss: 0.11991536617279053, Final Batch Loss: 0.06132280081510544\n",
      "Epoch 2157, Loss: 0.09516698122024536, Final Batch Loss: 0.05203724279999733\n",
      "Epoch 2158, Loss: 0.09535479545593262, Final Batch Loss: 0.04604272544384003\n",
      "Epoch 2159, Loss: 0.08228665590286255, Final Batch Loss: 0.033442359417676926\n",
      "Epoch 2160, Loss: 0.09592435136437416, Final Batch Loss: 0.0435628667473793\n",
      "Epoch 2161, Loss: 0.10629328712821007, Final Batch Loss: 0.038240011781454086\n",
      "Epoch 2162, Loss: 0.08668260276317596, Final Batch Loss: 0.046143729239702225\n",
      "Epoch 2163, Loss: 0.10206105932593346, Final Batch Loss: 0.05477441847324371\n",
      "Epoch 2164, Loss: 0.10242445394396782, Final Batch Loss: 0.06747663766145706\n",
      "Epoch 2165, Loss: 0.06574765034019947, Final Batch Loss: 0.0521957203745842\n",
      "Epoch 2166, Loss: 0.08792335167527199, Final Batch Loss: 0.03288247808814049\n",
      "Epoch 2167, Loss: 0.0965243000537157, Final Batch Loss: 0.06978939473628998\n",
      "Epoch 2168, Loss: 0.10075094923377037, Final Batch Loss: 0.058664340525865555\n",
      "Epoch 2169, Loss: 0.0842207595705986, Final Batch Loss: 0.0399625264108181\n",
      "Epoch 2170, Loss: 0.08307225629687309, Final Batch Loss: 0.05626439303159714\n",
      "Epoch 2171, Loss: 0.07707322388887405, Final Batch Loss: 0.04186570644378662\n",
      "Epoch 2172, Loss: 0.07361476495862007, Final Batch Loss: 0.028679706156253815\n",
      "Epoch 2173, Loss: 0.11222229711711407, Final Batch Loss: 0.028826987370848656\n",
      "Epoch 2174, Loss: 0.09426751732826233, Final Batch Loss: 0.07071935385465622\n",
      "Epoch 2175, Loss: 0.08385178446769714, Final Batch Loss: 0.04598384350538254\n",
      "Epoch 2176, Loss: 0.09826398640871048, Final Batch Loss: 0.04370732605457306\n",
      "Epoch 2177, Loss: 0.12554063275456429, Final Batch Loss: 0.045256052166223526\n",
      "Epoch 2178, Loss: 0.10923448950052261, Final Batch Loss: 0.06371575593948364\n",
      "Epoch 2179, Loss: 0.08742961287498474, Final Batch Loss: 0.04195462167263031\n",
      "Epoch 2180, Loss: 0.12998097389936447, Final Batch Loss: 0.050459012389183044\n",
      "Epoch 2181, Loss: 0.09057151898741722, Final Batch Loss: 0.03600471466779709\n",
      "Epoch 2182, Loss: 0.08695807307958603, Final Batch Loss: 0.04532332718372345\n",
      "Epoch 2183, Loss: 0.08742080628871918, Final Batch Loss: 0.05675068497657776\n",
      "Epoch 2184, Loss: 0.12293479219079018, Final Batch Loss: 0.06009997799992561\n",
      "Epoch 2185, Loss: 0.1124010756611824, Final Batch Loss: 0.062115199863910675\n",
      "Epoch 2186, Loss: 0.10283740237355232, Final Batch Loss: 0.05161956325173378\n",
      "Epoch 2187, Loss: 0.11571189016103745, Final Batch Loss: 0.0551072433590889\n",
      "Epoch 2188, Loss: 0.09147898852825165, Final Batch Loss: 0.044929079711437225\n",
      "Epoch 2189, Loss: 0.1410372443497181, Final Batch Loss: 0.08969554305076599\n",
      "Epoch 2190, Loss: 0.1255163736641407, Final Batch Loss: 0.06554660201072693\n",
      "Epoch 2191, Loss: 0.12639372423291206, Final Batch Loss: 0.07913564890623093\n",
      "Epoch 2192, Loss: 0.11091439053416252, Final Batch Loss: 0.06263509392738342\n",
      "Epoch 2193, Loss: 0.11765334755182266, Final Batch Loss: 0.08230078965425491\n",
      "Epoch 2194, Loss: 0.10987885296344757, Final Batch Loss: 0.027436181902885437\n",
      "Epoch 2195, Loss: 0.11443842574954033, Final Batch Loss: 0.052259597927331924\n",
      "Epoch 2196, Loss: 0.11832663416862488, Final Batch Loss: 0.07418790459632874\n",
      "Epoch 2197, Loss: 0.10684095323085785, Final Batch Loss: 0.06955067068338394\n",
      "Epoch 2198, Loss: 0.1712767593562603, Final Batch Loss: 0.13192994892597198\n",
      "Epoch 2199, Loss: 0.1337539702653885, Final Batch Loss: 0.0752207338809967\n",
      "Epoch 2200, Loss: 0.19377221912145615, Final Batch Loss: 0.12763512134552002\n",
      "Epoch 2201, Loss: 0.13778016343712807, Final Batch Loss: 0.048693109303712845\n",
      "Epoch 2202, Loss: 0.2510904148221016, Final Batch Loss: 0.17552156746387482\n",
      "Epoch 2203, Loss: 0.20391693711280823, Final Batch Loss: 0.07770881056785583\n",
      "Epoch 2204, Loss: 0.11496961489319801, Final Batch Loss: 0.05964798852801323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2205, Loss: 0.10583759099245071, Final Batch Loss: 0.05688265338540077\n",
      "Epoch 2206, Loss: 0.10877333208918571, Final Batch Loss: 0.0529942661523819\n",
      "Epoch 2207, Loss: 0.12878551706671715, Final Batch Loss: 0.07910680770874023\n",
      "Epoch 2208, Loss: 0.14614484086632729, Final Batch Loss: 0.09628067910671234\n",
      "Epoch 2209, Loss: 0.17237167805433273, Final Batch Loss: 0.07572263479232788\n",
      "Epoch 2210, Loss: 0.12911367788910866, Final Batch Loss: 0.06091735139489174\n",
      "Epoch 2211, Loss: 0.14598041027784348, Final Batch Loss: 0.07549390941858292\n",
      "Epoch 2212, Loss: 0.1640082448720932, Final Batch Loss: 0.08377131819725037\n",
      "Epoch 2213, Loss: 0.12640288472175598, Final Batch Loss: 0.060139112174510956\n",
      "Epoch 2214, Loss: 0.20708134025335312, Final Batch Loss: 0.11911440640687943\n",
      "Epoch 2215, Loss: 0.1841045767068863, Final Batch Loss: 0.046199485659599304\n",
      "Epoch 2216, Loss: 0.1489924117922783, Final Batch Loss: 0.05536937713623047\n",
      "Epoch 2217, Loss: 0.2078009694814682, Final Batch Loss: 0.14129561185836792\n",
      "Epoch 2218, Loss: 0.19533713161945343, Final Batch Loss: 0.09268833696842194\n",
      "Epoch 2219, Loss: 0.1615232601761818, Final Batch Loss: 0.07438930124044418\n",
      "Epoch 2220, Loss: 0.1735907718539238, Final Batch Loss: 0.06533721834421158\n",
      "Epoch 2221, Loss: 0.14583483710885048, Final Batch Loss: 0.0932418704032898\n",
      "Epoch 2222, Loss: 0.11860011518001556, Final Batch Loss: 0.05903676524758339\n",
      "Epoch 2223, Loss: 0.13135885447263718, Final Batch Loss: 0.07488267868757248\n",
      "Epoch 2224, Loss: 0.15300346910953522, Final Batch Loss: 0.08845765143632889\n",
      "Epoch 2225, Loss: 0.12867830321192741, Final Batch Loss: 0.0712469220161438\n",
      "Epoch 2226, Loss: 0.1148873008787632, Final Batch Loss: 0.055341560393571854\n",
      "Epoch 2227, Loss: 0.10894009098410606, Final Batch Loss: 0.0540865994989872\n",
      "Epoch 2228, Loss: 0.11531201004981995, Final Batch Loss: 0.05719362199306488\n",
      "Epoch 2229, Loss: 0.16853247582912445, Final Batch Loss: 0.07760165631771088\n",
      "Epoch 2230, Loss: 0.1031247191131115, Final Batch Loss: 0.07055410742759705\n",
      "Epoch 2231, Loss: 0.1906174048781395, Final Batch Loss: 0.1259380280971527\n",
      "Epoch 2232, Loss: 0.0898730680346489, Final Batch Loss: 0.04699142277240753\n",
      "Epoch 2233, Loss: 0.2873702794313431, Final Batch Loss: 0.10296942293643951\n",
      "Epoch 2234, Loss: 0.12459580600261688, Final Batch Loss: 0.05556240677833557\n",
      "Epoch 2235, Loss: 0.08464441075921059, Final Batch Loss: 0.03556017950177193\n",
      "Epoch 2236, Loss: 0.12340236827731133, Final Batch Loss: 0.05956171080470085\n",
      "Epoch 2237, Loss: 0.10847359895706177, Final Batch Loss: 0.054966479539871216\n",
      "Epoch 2238, Loss: 0.1326793022453785, Final Batch Loss: 0.05695487931370735\n",
      "Epoch 2239, Loss: 0.11184856295585632, Final Batch Loss: 0.039548031985759735\n",
      "Epoch 2240, Loss: 0.10625474154949188, Final Batch Loss: 0.04632219299674034\n",
      "Epoch 2241, Loss: 0.17437609285116196, Final Batch Loss: 0.10359802842140198\n",
      "Epoch 2242, Loss: 0.12085620686411858, Final Batch Loss: 0.05983608216047287\n",
      "Epoch 2243, Loss: 0.12293051555752754, Final Batch Loss: 0.062280841171741486\n",
      "Epoch 2244, Loss: 0.11853546649217606, Final Batch Loss: 0.06461795419454575\n",
      "Epoch 2245, Loss: 0.22202254831790924, Final Batch Loss: 0.11893350630998611\n",
      "Epoch 2246, Loss: 0.13557832315564156, Final Batch Loss: 0.08027312159538269\n",
      "Epoch 2247, Loss: 0.10970483720302582, Final Batch Loss: 0.054843802005052567\n",
      "Epoch 2248, Loss: 0.09870125725865364, Final Batch Loss: 0.05599358677864075\n",
      "Epoch 2249, Loss: 0.08606936410069466, Final Batch Loss: 0.037092190235853195\n",
      "Epoch 2250, Loss: 0.0972125381231308, Final Batch Loss: 0.04534735530614853\n",
      "Epoch 2251, Loss: 0.12076176330447197, Final Batch Loss: 0.07856187224388123\n",
      "Epoch 2252, Loss: 0.08488501608371735, Final Batch Loss: 0.03059673309326172\n",
      "Epoch 2253, Loss: 0.12568048387765884, Final Batch Loss: 0.05988004058599472\n",
      "Epoch 2254, Loss: 0.10699688643217087, Final Batch Loss: 0.04473596066236496\n",
      "Epoch 2255, Loss: 0.12652310729026794, Final Batch Loss: 0.057556986808776855\n",
      "Epoch 2256, Loss: 0.1013326644897461, Final Batch Loss: 0.05129028856754303\n",
      "Epoch 2257, Loss: 0.10041963681578636, Final Batch Loss: 0.05538129806518555\n",
      "Epoch 2258, Loss: 0.09917082265019417, Final Batch Loss: 0.04848528653383255\n",
      "Epoch 2259, Loss: 0.1258491650223732, Final Batch Loss: 0.06923730671405792\n",
      "Epoch 2260, Loss: 0.10691912658512592, Final Batch Loss: 0.07605170458555222\n",
      "Epoch 2261, Loss: 0.09970336779952049, Final Batch Loss: 0.05343906953930855\n",
      "Epoch 2262, Loss: 0.1083185002207756, Final Batch Loss: 0.05525340139865875\n",
      "Epoch 2263, Loss: 0.15949119627475739, Final Batch Loss: 0.05862591415643692\n",
      "Epoch 2264, Loss: 0.1131143793463707, Final Batch Loss: 0.05245155841112137\n",
      "Epoch 2265, Loss: 0.08734068274497986, Final Batch Loss: 0.04531964287161827\n",
      "Epoch 2266, Loss: 0.1193070150911808, Final Batch Loss: 0.05025375261902809\n",
      "Epoch 2267, Loss: 0.12247418984770775, Final Batch Loss: 0.048721905797719955\n",
      "Epoch 2268, Loss: 0.10836732760071754, Final Batch Loss: 0.033810246735811234\n",
      "Epoch 2269, Loss: 0.08064322546124458, Final Batch Loss: 0.04911675676703453\n",
      "Epoch 2270, Loss: 0.08942153304815292, Final Batch Loss: 0.056591253727674484\n",
      "Epoch 2271, Loss: 0.08769240230321884, Final Batch Loss: 0.03927690163254738\n",
      "Epoch 2272, Loss: 0.10173853114247322, Final Batch Loss: 0.042863842099905014\n",
      "Epoch 2273, Loss: 0.08414554223418236, Final Batch Loss: 0.041488196700811386\n",
      "Epoch 2274, Loss: 0.13179194554686546, Final Batch Loss: 0.049372997134923935\n",
      "Epoch 2275, Loss: 0.11101536825299263, Final Batch Loss: 0.07394498586654663\n",
      "Epoch 2276, Loss: 0.15822016447782516, Final Batch Loss: 0.0885433554649353\n",
      "Epoch 2277, Loss: 0.10304704308509827, Final Batch Loss: 0.047924771904945374\n",
      "Epoch 2278, Loss: 0.13495079800486565, Final Batch Loss: 0.08483833819627762\n",
      "Epoch 2279, Loss: 0.09795470163226128, Final Batch Loss: 0.06357787549495697\n",
      "Epoch 2280, Loss: 0.09346537292003632, Final Batch Loss: 0.04991692677140236\n",
      "Epoch 2281, Loss: 0.1381533183157444, Final Batch Loss: 0.08816833794116974\n",
      "Epoch 2282, Loss: 0.08336357027292252, Final Batch Loss: 0.04127906262874603\n",
      "Epoch 2283, Loss: 0.09464308619499207, Final Batch Loss: 0.05508384481072426\n",
      "Epoch 2284, Loss: 0.18017198145389557, Final Batch Loss: 0.13823775947093964\n",
      "Epoch 2285, Loss: 0.14940065890550613, Final Batch Loss: 0.08279305696487427\n",
      "Epoch 2286, Loss: 0.19069110602140427, Final Batch Loss: 0.09305571764707565\n",
      "Epoch 2287, Loss: 0.11737321689724922, Final Batch Loss: 0.0644756481051445\n",
      "Epoch 2288, Loss: 0.14879000559449196, Final Batch Loss: 0.05878889933228493\n",
      "Epoch 2289, Loss: 0.08289152011275291, Final Batch Loss: 0.032281678169965744\n",
      "Epoch 2290, Loss: 0.10319024696946144, Final Batch Loss: 0.045971598476171494\n",
      "Epoch 2291, Loss: 0.09580326825380325, Final Batch Loss: 0.06325244158506393\n",
      "Epoch 2292, Loss: 0.11437497287988663, Final Batch Loss: 0.07887796312570572\n",
      "Epoch 2293, Loss: 0.11738545075058937, Final Batch Loss: 0.04951745644211769\n",
      "Epoch 2294, Loss: 0.09857043996453285, Final Batch Loss: 0.06644805520772934\n",
      "Epoch 2295, Loss: 0.09121571108698845, Final Batch Loss: 0.03906863555312157\n",
      "Epoch 2296, Loss: 0.10112270712852478, Final Batch Loss: 0.048167333006858826\n",
      "Epoch 2297, Loss: 0.09577654115855694, Final Batch Loss: 0.026697730645537376\n",
      "Epoch 2298, Loss: 0.0777223389595747, Final Batch Loss: 0.02801150269806385\n",
      "Epoch 2299, Loss: 0.08652642741799355, Final Batch Loss: 0.05615105479955673\n",
      "Epoch 2300, Loss: 0.13670334219932556, Final Batch Loss: 0.058321669697761536\n",
      "Epoch 2301, Loss: 0.0850696973502636, Final Batch Loss: 0.036648962646722794\n",
      "Epoch 2302, Loss: 0.0991525873541832, Final Batch Loss: 0.07705630362033844\n",
      "Epoch 2303, Loss: 0.07895297929644585, Final Batch Loss: 0.03422900661826134\n",
      "Epoch 2304, Loss: 0.080586988478899, Final Batch Loss: 0.036318905651569366\n",
      "Epoch 2305, Loss: 0.10861849412322044, Final Batch Loss: 0.04148387536406517\n",
      "Epoch 2306, Loss: 0.11512186378240585, Final Batch Loss: 0.060152243822813034\n",
      "Epoch 2307, Loss: 0.09953916445374489, Final Batch Loss: 0.04817066341638565\n",
      "Epoch 2308, Loss: 0.0974297970533371, Final Batch Loss: 0.055854447185993195\n",
      "Epoch 2309, Loss: 0.07525397092103958, Final Batch Loss: 0.04104946181178093\n",
      "Epoch 2310, Loss: 0.0847455095499754, Final Batch Loss: 0.05407612770795822\n",
      "Epoch 2311, Loss: 0.07974406331777573, Final Batch Loss: 0.045394737273454666\n",
      "Epoch 2312, Loss: 0.09065638855099678, Final Batch Loss: 0.03628009930253029\n",
      "Epoch 2313, Loss: 0.09827827289700508, Final Batch Loss: 0.05249311774969101\n",
      "Epoch 2314, Loss: 0.10131930932402611, Final Batch Loss: 0.057662125676870346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2315, Loss: 0.09156575053930283, Final Batch Loss: 0.05063214153051376\n",
      "Epoch 2316, Loss: 0.09451209008693695, Final Batch Loss: 0.04702501371502876\n",
      "Epoch 2317, Loss: 0.1353609710931778, Final Batch Loss: 0.06270423531532288\n",
      "Epoch 2318, Loss: 0.10801708325743675, Final Batch Loss: 0.040026258677244186\n",
      "Epoch 2319, Loss: 0.1170947514474392, Final Batch Loss: 0.045736122876405716\n",
      "Epoch 2320, Loss: 0.08163190074265003, Final Batch Loss: 0.02578612230718136\n",
      "Epoch 2321, Loss: 0.08611376397311687, Final Batch Loss: 0.057058125734329224\n",
      "Epoch 2322, Loss: 0.0962103046476841, Final Batch Loss: 0.03750137984752655\n",
      "Epoch 2323, Loss: 0.09801984950900078, Final Batch Loss: 0.05185052752494812\n",
      "Epoch 2324, Loss: 0.0865944754332304, Final Batch Loss: 0.056942231953144073\n",
      "Epoch 2325, Loss: 0.10389941930770874, Final Batch Loss: 0.05694575235247612\n",
      "Epoch 2326, Loss: 0.08332023397088051, Final Batch Loss: 0.03618261590600014\n",
      "Epoch 2327, Loss: 0.06865568272769451, Final Batch Loss: 0.026797788217663765\n",
      "Epoch 2328, Loss: 0.10432224348187447, Final Batch Loss: 0.029413077980279922\n",
      "Epoch 2329, Loss: 0.09601575881242752, Final Batch Loss: 0.030540838837623596\n",
      "Epoch 2330, Loss: 0.08139881119132042, Final Batch Loss: 0.03557718172669411\n",
      "Epoch 2331, Loss: 0.07163838669657707, Final Batch Loss: 0.037636131048202515\n",
      "Epoch 2332, Loss: 0.07924257591366768, Final Batch Loss: 0.044282082468271255\n",
      "Epoch 2333, Loss: 0.0784134529531002, Final Batch Loss: 0.03923816606402397\n",
      "Epoch 2334, Loss: 0.10268522053956985, Final Batch Loss: 0.06618721038103104\n",
      "Epoch 2335, Loss: 0.08357599936425686, Final Batch Loss: 0.030911853536963463\n",
      "Epoch 2336, Loss: 0.0747995376586914, Final Batch Loss: 0.04300985857844353\n",
      "Epoch 2337, Loss: 0.09898108243942261, Final Batch Loss: 0.06206158921122551\n",
      "Epoch 2338, Loss: 0.11716864258050919, Final Batch Loss: 0.07401920855045319\n",
      "Epoch 2339, Loss: 0.10220808535814285, Final Batch Loss: 0.06135375425219536\n",
      "Epoch 2340, Loss: 0.0864037536084652, Final Batch Loss: 0.043866220861673355\n",
      "Epoch 2341, Loss: 0.09402896091341972, Final Batch Loss: 0.06027696654200554\n",
      "Epoch 2342, Loss: 0.07100032828748226, Final Batch Loss: 0.02499213255941868\n",
      "Epoch 2343, Loss: 0.08730313926935196, Final Batch Loss: 0.04346713051199913\n",
      "Epoch 2344, Loss: 0.11410810798406601, Final Batch Loss: 0.06756225228309631\n",
      "Epoch 2345, Loss: 0.08420095220208168, Final Batch Loss: 0.032614801079034805\n",
      "Epoch 2346, Loss: 0.08511362597346306, Final Batch Loss: 0.042172737419605255\n",
      "Epoch 2347, Loss: 0.08783132955431938, Final Batch Loss: 0.04818305745720863\n",
      "Epoch 2348, Loss: 0.09640763700008392, Final Batch Loss: 0.04196254536509514\n",
      "Epoch 2349, Loss: 0.08819056674838066, Final Batch Loss: 0.05554754287004471\n",
      "Epoch 2350, Loss: 0.1074135135859251, Final Batch Loss: 0.02671252004802227\n",
      "Epoch 2351, Loss: 0.08656872063875198, Final Batch Loss: 0.045963287353515625\n",
      "Epoch 2352, Loss: 0.08001117780804634, Final Batch Loss: 0.04153425619006157\n",
      "Epoch 2353, Loss: 0.09251187741756439, Final Batch Loss: 0.044552769511938095\n",
      "Epoch 2354, Loss: 0.09194031730294228, Final Batch Loss: 0.042255375534296036\n",
      "Epoch 2355, Loss: 0.10184119641780853, Final Batch Loss: 0.049266621470451355\n",
      "Epoch 2356, Loss: 0.07873165234923363, Final Batch Loss: 0.035754185169935226\n",
      "Epoch 2357, Loss: 0.08665311709046364, Final Batch Loss: 0.04514152556657791\n",
      "Epoch 2358, Loss: 0.09467609412968159, Final Batch Loss: 0.02750362642109394\n",
      "Epoch 2359, Loss: 0.09933396428823471, Final Batch Loss: 0.049390800297260284\n",
      "Epoch 2360, Loss: 0.10136957094073296, Final Batch Loss: 0.033172640949487686\n",
      "Epoch 2361, Loss: 0.097480159252882, Final Batch Loss: 0.05239463597536087\n",
      "Epoch 2362, Loss: 0.1618375927209854, Final Batch Loss: 0.04006814956665039\n",
      "Epoch 2363, Loss: 0.11328916251659393, Final Batch Loss: 0.0561581552028656\n",
      "Epoch 2364, Loss: 0.07743004336953163, Final Batch Loss: 0.03834860771894455\n",
      "Epoch 2365, Loss: 0.11671746149659157, Final Batch Loss: 0.06127307936549187\n",
      "Epoch 2366, Loss: 0.0915510430932045, Final Batch Loss: 0.053840238600969315\n",
      "Epoch 2367, Loss: 0.10722049698233604, Final Batch Loss: 0.07048112154006958\n",
      "Epoch 2368, Loss: 0.12690460681915283, Final Batch Loss: 0.06638935953378677\n",
      "Epoch 2369, Loss: 0.10106614977121353, Final Batch Loss: 0.04067168012261391\n",
      "Epoch 2370, Loss: 0.08910992741584778, Final Batch Loss: 0.05829966068267822\n",
      "Epoch 2371, Loss: 0.10784101486206055, Final Batch Loss: 0.04989037662744522\n",
      "Epoch 2372, Loss: 0.0867138747125864, Final Batch Loss: 0.030579684302210808\n",
      "Epoch 2373, Loss: 0.09794907085597515, Final Batch Loss: 0.07129059731960297\n",
      "Epoch 2374, Loss: 0.07765280827879906, Final Batch Loss: 0.040402285754680634\n",
      "Epoch 2375, Loss: 0.11336200311779976, Final Batch Loss: 0.07599572837352753\n",
      "Epoch 2376, Loss: 0.12822338566184044, Final Batch Loss: 0.06683320552110672\n",
      "Epoch 2377, Loss: 0.12387348711490631, Final Batch Loss: 0.05884638428688049\n",
      "Epoch 2378, Loss: 0.1238950677216053, Final Batch Loss: 0.0846157893538475\n",
      "Epoch 2379, Loss: 0.12181904725730419, Final Batch Loss: 0.031235849484801292\n",
      "Epoch 2380, Loss: 0.11544499918818474, Final Batch Loss: 0.055490702390670776\n",
      "Epoch 2381, Loss: 0.09686139598488808, Final Batch Loss: 0.04420667141675949\n",
      "Epoch 2382, Loss: 0.1016150489449501, Final Batch Loss: 0.059022482484579086\n",
      "Epoch 2383, Loss: 0.10595301911234856, Final Batch Loss: 0.0487523227930069\n",
      "Epoch 2384, Loss: 0.11349870264530182, Final Batch Loss: 0.06668473780155182\n",
      "Epoch 2385, Loss: 0.0823214091360569, Final Batch Loss: 0.04544205591082573\n",
      "Epoch 2386, Loss: 0.1136842593550682, Final Batch Loss: 0.05042888969182968\n",
      "Epoch 2387, Loss: 0.07371239736676216, Final Batch Loss: 0.03866180405020714\n",
      "Epoch 2388, Loss: 0.09769878722727299, Final Batch Loss: 0.07167252153158188\n",
      "Epoch 2389, Loss: 0.08833709359169006, Final Batch Loss: 0.038005802780389786\n",
      "Epoch 2390, Loss: 0.0724746510386467, Final Batch Loss: 0.016773488372564316\n",
      "Epoch 2391, Loss: 0.10520023852586746, Final Batch Loss: 0.04283469542860985\n",
      "Epoch 2392, Loss: 0.10886442847549915, Final Batch Loss: 0.024300800636410713\n",
      "Epoch 2393, Loss: 0.10073088109493256, Final Batch Loss: 0.057209424674510956\n",
      "Epoch 2394, Loss: 0.11006850376725197, Final Batch Loss: 0.0497460812330246\n",
      "Epoch 2395, Loss: 0.10431923344731331, Final Batch Loss: 0.04798610880970955\n",
      "Epoch 2396, Loss: 0.08223377913236618, Final Batch Loss: 0.037266165018081665\n",
      "Epoch 2397, Loss: 0.08195681124925613, Final Batch Loss: 0.044978003948926926\n",
      "Epoch 2398, Loss: 0.11510555818676949, Final Batch Loss: 0.04441944882273674\n",
      "Epoch 2399, Loss: 0.08850470557808876, Final Batch Loss: 0.047881968319416046\n",
      "Epoch 2400, Loss: 0.09593899920582771, Final Batch Loss: 0.040150053799152374\n",
      "Epoch 2401, Loss: 0.07945872470736504, Final Batch Loss: 0.05378038436174393\n",
      "Epoch 2402, Loss: 0.12829100340604782, Final Batch Loss: 0.040552616119384766\n",
      "Epoch 2403, Loss: 0.11724606156349182, Final Batch Loss: 0.06063750386238098\n",
      "Epoch 2404, Loss: 0.09051230177283287, Final Batch Loss: 0.041554637253284454\n",
      "Epoch 2405, Loss: 0.10696195811033249, Final Batch Loss: 0.06338464468717575\n",
      "Epoch 2406, Loss: 0.07880710996687412, Final Batch Loss: 0.04845843091607094\n",
      "Epoch 2407, Loss: 0.09289904125034809, Final Batch Loss: 0.06717143952846527\n",
      "Epoch 2408, Loss: 0.09068188071250916, Final Batch Loss: 0.05038470774888992\n",
      "Epoch 2409, Loss: 0.13791203871369362, Final Batch Loss: 0.054316017776727676\n",
      "Epoch 2410, Loss: 0.14736611768603325, Final Batch Loss: 0.04442572966217995\n",
      "Epoch 2411, Loss: 0.0802839882671833, Final Batch Loss: 0.04506407678127289\n",
      "Epoch 2412, Loss: 0.08759745210409164, Final Batch Loss: 0.0445605032145977\n",
      "Epoch 2413, Loss: 0.08808458223938942, Final Batch Loss: 0.032598111778497696\n",
      "Epoch 2414, Loss: 0.0835508219897747, Final Batch Loss: 0.04199250414967537\n",
      "Epoch 2415, Loss: 0.10398506745696068, Final Batch Loss: 0.05831370875239372\n",
      "Epoch 2416, Loss: 0.13120317459106445, Final Batch Loss: 0.05080416053533554\n",
      "Epoch 2417, Loss: 0.10629479214549065, Final Batch Loss: 0.0462852381169796\n",
      "Epoch 2418, Loss: 0.08217989839613438, Final Batch Loss: 0.029757490381598473\n",
      "Epoch 2419, Loss: 0.10855039581656456, Final Batch Loss: 0.07310811430215836\n",
      "Epoch 2420, Loss: 0.08691041171550751, Final Batch Loss: 0.03483825549483299\n",
      "Epoch 2421, Loss: 0.13696178048849106, Final Batch Loss: 0.07360531389713287\n",
      "Epoch 2422, Loss: 0.08507298305630684, Final Batch Loss: 0.05453350394964218\n",
      "Epoch 2423, Loss: 0.07881205901503563, Final Batch Loss: 0.03528163209557533\n",
      "Epoch 2424, Loss: 0.1157735176384449, Final Batch Loss: 0.048070017248392105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2425, Loss: 0.08581100404262543, Final Batch Loss: 0.0508221797645092\n",
      "Epoch 2426, Loss: 0.0916060023009777, Final Batch Loss: 0.05294042080640793\n",
      "Epoch 2427, Loss: 0.08301031962037086, Final Batch Loss: 0.06134398654103279\n",
      "Epoch 2428, Loss: 0.0917133130133152, Final Batch Loss: 0.05132821574807167\n",
      "Epoch 2429, Loss: 0.12331673130393028, Final Batch Loss: 0.04848921671509743\n",
      "Epoch 2430, Loss: 0.09584024176001549, Final Batch Loss: 0.053624823689460754\n",
      "Epoch 2431, Loss: 0.09097845479846, Final Batch Loss: 0.049689628183841705\n",
      "Epoch 2432, Loss: 0.11701617762446404, Final Batch Loss: 0.06186671182513237\n",
      "Epoch 2433, Loss: 0.10336919873952866, Final Batch Loss: 0.05147848650813103\n",
      "Epoch 2434, Loss: 0.09523650072515011, Final Batch Loss: 0.02961798571050167\n",
      "Epoch 2435, Loss: 0.09060992300510406, Final Batch Loss: 0.03941784426569939\n",
      "Epoch 2436, Loss: 0.11284579709172249, Final Batch Loss: 0.0654081478714943\n",
      "Epoch 2437, Loss: 0.11078464239835739, Final Batch Loss: 0.037320129573345184\n",
      "Epoch 2438, Loss: 0.11301915347576141, Final Batch Loss: 0.0413857027888298\n",
      "Epoch 2439, Loss: 0.09167488291859627, Final Batch Loss: 0.044069960713386536\n",
      "Epoch 2440, Loss: 0.08354764059185982, Final Batch Loss: 0.0341409333050251\n",
      "Epoch 2441, Loss: 0.11876118555665016, Final Batch Loss: 0.07209152728319168\n",
      "Epoch 2442, Loss: 0.11269422620534897, Final Batch Loss: 0.07065292447805405\n",
      "Epoch 2443, Loss: 0.10511482134461403, Final Batch Loss: 0.05012715607881546\n",
      "Epoch 2444, Loss: 0.09953844547271729, Final Batch Loss: 0.04930740222334862\n",
      "Epoch 2445, Loss: 0.08462009578943253, Final Batch Loss: 0.04629911482334137\n",
      "Epoch 2446, Loss: 0.08243702724575996, Final Batch Loss: 0.05227041244506836\n",
      "Epoch 2447, Loss: 0.09322823956608772, Final Batch Loss: 0.052730608731508255\n",
      "Epoch 2448, Loss: 0.0908593013882637, Final Batch Loss: 0.05700548365712166\n",
      "Epoch 2449, Loss: 0.11276092752814293, Final Batch Loss: 0.04789147898554802\n",
      "Epoch 2450, Loss: 0.08774269744753838, Final Batch Loss: 0.04537029191851616\n",
      "Epoch 2451, Loss: 0.0988565981388092, Final Batch Loss: 0.061102885752916336\n",
      "Epoch 2452, Loss: 0.10349580645561218, Final Batch Loss: 0.07051892578601837\n",
      "Epoch 2453, Loss: 0.11108430474996567, Final Batch Loss: 0.05776007100939751\n",
      "Epoch 2454, Loss: 0.0765097551047802, Final Batch Loss: 0.04443230479955673\n",
      "Epoch 2455, Loss: 0.08553365990519524, Final Batch Loss: 0.04552881047129631\n",
      "Epoch 2456, Loss: 0.11245592311024666, Final Batch Loss: 0.04503310099244118\n",
      "Epoch 2457, Loss: 0.08135613426566124, Final Batch Loss: 0.041201069951057434\n",
      "Epoch 2458, Loss: 0.09517543017864227, Final Batch Loss: 0.05659721791744232\n",
      "Epoch 2459, Loss: 0.07594200223684311, Final Batch Loss: 0.04496357962489128\n",
      "Epoch 2460, Loss: 0.06785563006997108, Final Batch Loss: 0.035825684666633606\n",
      "Epoch 2461, Loss: 0.08574679680168629, Final Batch Loss: 0.030444910749793053\n",
      "Epoch 2462, Loss: 0.10289338231086731, Final Batch Loss: 0.06222183257341385\n",
      "Epoch 2463, Loss: 0.06415123678743839, Final Batch Loss: 0.03842047229409218\n",
      "Epoch 2464, Loss: 0.07194488681852818, Final Batch Loss: 0.019511273130774498\n",
      "Epoch 2465, Loss: 0.08575321361422539, Final Batch Loss: 0.02684766799211502\n",
      "Epoch 2466, Loss: 0.10584374144673347, Final Batch Loss: 0.05398719385266304\n",
      "Epoch 2467, Loss: 0.0932200737297535, Final Batch Loss: 0.06724144518375397\n",
      "Epoch 2468, Loss: 0.10227539204061031, Final Batch Loss: 0.07107002288103104\n",
      "Epoch 2469, Loss: 0.08207885921001434, Final Batch Loss: 0.03630678728222847\n",
      "Epoch 2470, Loss: 0.09663015231490135, Final Batch Loss: 0.024344857782125473\n",
      "Epoch 2471, Loss: 0.11597426608204842, Final Batch Loss: 0.04313709959387779\n",
      "Epoch 2472, Loss: 0.07669837214052677, Final Batch Loss: 0.0455995574593544\n",
      "Epoch 2473, Loss: 0.09947756305336952, Final Batch Loss: 0.03253203257918358\n",
      "Epoch 2474, Loss: 0.08716317638754845, Final Batch Loss: 0.05395248532295227\n",
      "Epoch 2475, Loss: 0.07430723309516907, Final Batch Loss: 0.033099215477705\n",
      "Epoch 2476, Loss: 0.10072890669107437, Final Batch Loss: 0.036495715379714966\n",
      "Epoch 2477, Loss: 0.07041365280747414, Final Batch Loss: 0.03125438839197159\n",
      "Epoch 2478, Loss: 0.09444655478000641, Final Batch Loss: 0.031533412635326385\n",
      "Epoch 2479, Loss: 0.08596714958548546, Final Batch Loss: 0.04233117029070854\n",
      "Epoch 2480, Loss: 0.09880346059799194, Final Batch Loss: 0.032366082072257996\n",
      "Epoch 2481, Loss: 0.08275952190160751, Final Batch Loss: 0.0544184111058712\n",
      "Epoch 2482, Loss: 0.11377512291073799, Final Batch Loss: 0.060744039714336395\n",
      "Epoch 2483, Loss: 0.07672510109841824, Final Batch Loss: 0.026299534365534782\n",
      "Epoch 2484, Loss: 0.10186890512704849, Final Batch Loss: 0.05987987667322159\n",
      "Epoch 2485, Loss: 0.08523790538311005, Final Batch Loss: 0.0493367463350296\n",
      "Epoch 2486, Loss: 0.07253902778029442, Final Batch Loss: 0.03996835649013519\n",
      "Epoch 2487, Loss: 0.09056443348526955, Final Batch Loss: 0.036208394914865494\n",
      "Epoch 2488, Loss: 0.0802692286670208, Final Batch Loss: 0.04843851551413536\n",
      "Epoch 2489, Loss: 0.09610822051763535, Final Batch Loss: 0.05640288069844246\n",
      "Epoch 2490, Loss: 0.0799260102212429, Final Batch Loss: 0.03027096763253212\n",
      "Epoch 2491, Loss: 0.07641351968050003, Final Batch Loss: 0.0379948690533638\n",
      "Epoch 2492, Loss: 0.09339558705687523, Final Batch Loss: 0.03984250873327255\n",
      "Epoch 2493, Loss: 0.08280543237924576, Final Batch Loss: 0.039916783571243286\n",
      "Epoch 2494, Loss: 0.10319950059056282, Final Batch Loss: 0.040090661495923996\n",
      "Epoch 2495, Loss: 0.12721622735261917, Final Batch Loss: 0.04550418257713318\n",
      "Epoch 2496, Loss: 0.07951542735099792, Final Batch Loss: 0.04422392323613167\n",
      "Epoch 2497, Loss: 0.07476308941841125, Final Batch Loss: 0.03159402683377266\n",
      "Epoch 2498, Loss: 0.10704981908202171, Final Batch Loss: 0.06740183383226395\n",
      "Epoch 2499, Loss: 0.12312696501612663, Final Batch Loss: 0.05415330454707146\n",
      "Epoch 2500, Loss: 0.11404749378561974, Final Batch Loss: 0.05337042361497879\n",
      "Epoch 2501, Loss: 0.09136151149868965, Final Batch Loss: 0.03141086548566818\n",
      "Epoch 2502, Loss: 0.09335753694176674, Final Batch Loss: 0.05600297078490257\n",
      "Epoch 2503, Loss: 0.09493271261453629, Final Batch Loss: 0.035150136798620224\n",
      "Epoch 2504, Loss: 0.10410439968109131, Final Batch Loss: 0.07713323086500168\n",
      "Epoch 2505, Loss: 0.10702764242887497, Final Batch Loss: 0.04799932986497879\n",
      "Epoch 2506, Loss: 0.10296561568975449, Final Batch Loss: 0.04378238320350647\n",
      "Epoch 2507, Loss: 0.14329660311341286, Final Batch Loss: 0.09281326830387115\n",
      "Epoch 2508, Loss: 0.11754132434725761, Final Batch Loss: 0.07060455530881882\n",
      "Epoch 2509, Loss: 0.16195885092020035, Final Batch Loss: 0.08482403308153152\n",
      "Epoch 2510, Loss: 0.11321727558970451, Final Batch Loss: 0.03461286798119545\n",
      "Epoch 2511, Loss: 0.16234608367085457, Final Batch Loss: 0.11466901749372482\n",
      "Epoch 2512, Loss: 0.08653644844889641, Final Batch Loss: 0.04034378007054329\n",
      "Epoch 2513, Loss: 0.0871715322136879, Final Batch Loss: 0.05502214655280113\n",
      "Epoch 2514, Loss: 0.07895367592573166, Final Batch Loss: 0.037263207137584686\n",
      "Epoch 2515, Loss: 0.1367480270564556, Final Batch Loss: 0.09666071087121964\n",
      "Epoch 2516, Loss: 0.08001363463699818, Final Batch Loss: 0.02619088999927044\n",
      "Epoch 2517, Loss: 0.07768869027495384, Final Batch Loss: 0.03466848284006119\n",
      "Epoch 2518, Loss: 0.08585002273321152, Final Batch Loss: 0.04576325789093971\n",
      "Epoch 2519, Loss: 0.12434909492731094, Final Batch Loss: 0.05232185870409012\n",
      "Epoch 2520, Loss: 0.07684482634067535, Final Batch Loss: 0.041837841272354126\n",
      "Epoch 2521, Loss: 0.11748645454645157, Final Batch Loss: 0.06522897630929947\n",
      "Epoch 2522, Loss: 0.12092214077711105, Final Batch Loss: 0.06858295947313309\n",
      "Epoch 2523, Loss: 0.0943489596247673, Final Batch Loss: 0.046909525990486145\n",
      "Epoch 2524, Loss: 0.09913025423884392, Final Batch Loss: 0.0539095439016819\n",
      "Epoch 2525, Loss: 0.104977086186409, Final Batch Loss: 0.04778982698917389\n",
      "Epoch 2526, Loss: 0.14035923406481743, Final Batch Loss: 0.09823137521743774\n",
      "Epoch 2527, Loss: 0.0868946760892868, Final Batch Loss: 0.046573419123888016\n",
      "Epoch 2528, Loss: 0.1102396808564663, Final Batch Loss: 0.05710444226861\n",
      "Epoch 2529, Loss: 0.07758862897753716, Final Batch Loss: 0.03678209334611893\n",
      "Epoch 2530, Loss: 0.1136158462613821, Final Batch Loss: 0.08365001529455185\n",
      "Epoch 2531, Loss: 0.11891046538949013, Final Batch Loss: 0.06304321438074112\n",
      "Epoch 2532, Loss: 0.11898361891508102, Final Batch Loss: 0.07025209814310074\n",
      "Epoch 2533, Loss: 0.09415560215711594, Final Batch Loss: 0.059856195002794266\n",
      "Epoch 2534, Loss: 0.10632171481847763, Final Batch Loss: 0.03728530555963516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2535, Loss: 0.11872682347893715, Final Batch Loss: 0.06893330067396164\n",
      "Epoch 2536, Loss: 0.08758085034787655, Final Batch Loss: 0.030242087319493294\n",
      "Epoch 2537, Loss: 0.13526462763547897, Final Batch Loss: 0.09388032555580139\n",
      "Epoch 2538, Loss: 0.1152305118739605, Final Batch Loss: 0.0559447780251503\n",
      "Epoch 2539, Loss: 0.0957300141453743, Final Batch Loss: 0.05300057306885719\n",
      "Epoch 2540, Loss: 0.09506655856966972, Final Batch Loss: 0.0540173202753067\n",
      "Epoch 2541, Loss: 0.11256231740117073, Final Batch Loss: 0.07988882809877396\n",
      "Epoch 2542, Loss: 0.09062636643648148, Final Batch Loss: 0.024207904934883118\n",
      "Epoch 2543, Loss: 0.10739908739924431, Final Batch Loss: 0.044480327516794205\n",
      "Epoch 2544, Loss: 0.10530353710055351, Final Batch Loss: 0.05883755534887314\n",
      "Epoch 2545, Loss: 0.10274996235966682, Final Batch Loss: 0.07021992653608322\n",
      "Epoch 2546, Loss: 0.10869581438601017, Final Batch Loss: 0.08696593344211578\n",
      "Epoch 2547, Loss: 0.09955473616719246, Final Batch Loss: 0.04836251214146614\n",
      "Epoch 2548, Loss: 0.14076528325676918, Final Batch Loss: 0.055542703717947006\n",
      "Epoch 2549, Loss: 0.08091047033667564, Final Batch Loss: 0.04058946669101715\n",
      "Epoch 2550, Loss: 0.1572570838034153, Final Batch Loss: 0.03371679410338402\n",
      "Epoch 2551, Loss: 0.11615118384361267, Final Batch Loss: 0.05197419226169586\n",
      "Epoch 2552, Loss: 0.1397138275206089, Final Batch Loss: 0.09442835301160812\n",
      "Epoch 2553, Loss: 0.06593641638755798, Final Batch Loss: 0.029295548796653748\n",
      "Epoch 2554, Loss: 0.10981941595673561, Final Batch Loss: 0.052385035902261734\n",
      "Epoch 2555, Loss: 0.10299836099147797, Final Batch Loss: 0.05546053871512413\n",
      "Epoch 2556, Loss: 0.1631394922733307, Final Batch Loss: 0.05795803666114807\n",
      "Epoch 2557, Loss: 0.13192417472600937, Final Batch Loss: 0.07682108879089355\n",
      "Epoch 2558, Loss: 0.18931277096271515, Final Batch Loss: 0.07574579119682312\n",
      "Epoch 2559, Loss: 0.12933629006147385, Final Batch Loss: 0.04948458820581436\n",
      "Epoch 2560, Loss: 0.16498908400535583, Final Batch Loss: 0.11181358993053436\n",
      "Epoch 2561, Loss: 0.09966034069657326, Final Batch Loss: 0.04000556468963623\n",
      "Epoch 2562, Loss: 0.18206006288528442, Final Batch Loss: 0.1009545847773552\n",
      "Epoch 2563, Loss: 0.10813219845294952, Final Batch Loss: 0.041761308908462524\n",
      "Epoch 2564, Loss: 0.14901453256607056, Final Batch Loss: 0.06515246629714966\n",
      "Epoch 2565, Loss: 0.10661863535642624, Final Batch Loss: 0.0585942417383194\n",
      "Epoch 2566, Loss: 0.13972694799304008, Final Batch Loss: 0.0815594345331192\n",
      "Epoch 2567, Loss: 0.15518023818731308, Final Batch Loss: 0.059123627841472626\n",
      "Epoch 2568, Loss: 0.1542205512523651, Final Batch Loss: 0.08682864159345627\n",
      "Epoch 2569, Loss: 0.11098353937268257, Final Batch Loss: 0.04902965947985649\n",
      "Epoch 2570, Loss: 0.12290589511394501, Final Batch Loss: 0.0615525096654892\n",
      "Epoch 2571, Loss: 0.12039626762270927, Final Batch Loss: 0.0679081529378891\n",
      "Epoch 2572, Loss: 0.11251169443130493, Final Batch Loss: 0.06302130967378616\n",
      "Epoch 2573, Loss: 0.16846852749586105, Final Batch Loss: 0.07049640268087387\n",
      "Epoch 2574, Loss: 0.12784922122955322, Final Batch Loss: 0.044670961797237396\n",
      "Epoch 2575, Loss: 0.11133094877004623, Final Batch Loss: 0.059242673218250275\n",
      "Epoch 2576, Loss: 0.10381292924284935, Final Batch Loss: 0.03492893651127815\n",
      "Epoch 2577, Loss: 0.1302553154528141, Final Batch Loss: 0.09346183389425278\n",
      "Epoch 2578, Loss: 0.1280852071940899, Final Batch Loss: 0.036992643028497696\n",
      "Epoch 2579, Loss: 0.1061917394399643, Final Batch Loss: 0.05681981146335602\n",
      "Epoch 2580, Loss: 0.11105309799313545, Final Batch Loss: 0.058951448649168015\n",
      "Epoch 2581, Loss: 0.10180675238370895, Final Batch Loss: 0.048918869346380234\n",
      "Epoch 2582, Loss: 0.1245674192905426, Final Batch Loss: 0.05419047176837921\n",
      "Epoch 2583, Loss: 0.11742451041936874, Final Batch Loss: 0.04772353917360306\n",
      "Epoch 2584, Loss: 0.11510511487722397, Final Batch Loss: 0.08290629833936691\n",
      "Epoch 2585, Loss: 0.1127716414630413, Final Batch Loss: 0.04854974523186684\n",
      "Epoch 2586, Loss: 0.11725911870598793, Final Batch Loss: 0.040317799896001816\n",
      "Epoch 2587, Loss: 0.11316335201263428, Final Batch Loss: 0.04668430984020233\n",
      "Epoch 2588, Loss: 0.07978402450680733, Final Batch Loss: 0.027091432362794876\n",
      "Epoch 2589, Loss: 0.11519069969654083, Final Batch Loss: 0.06194642558693886\n",
      "Epoch 2590, Loss: 0.09259621053934097, Final Batch Loss: 0.031046874821186066\n",
      "Epoch 2591, Loss: 0.1145116351544857, Final Batch Loss: 0.04548456147313118\n",
      "Epoch 2592, Loss: 0.12666169553995132, Final Batch Loss: 0.03897120803594589\n",
      "Epoch 2593, Loss: 0.08594104647636414, Final Batch Loss: 0.032793883234262466\n",
      "Epoch 2594, Loss: 0.1450565792620182, Final Batch Loss: 0.04117606207728386\n",
      "Epoch 2595, Loss: 0.11326628923416138, Final Batch Loss: 0.037795014679431915\n",
      "Epoch 2596, Loss: 0.10453833267092705, Final Batch Loss: 0.05279432237148285\n",
      "Epoch 2597, Loss: 0.09102229028940201, Final Batch Loss: 0.03977885842323303\n",
      "Epoch 2598, Loss: 0.12025261297821999, Final Batch Loss: 0.07792413979768753\n",
      "Epoch 2599, Loss: 0.1741575375199318, Final Batch Loss: 0.11119674146175385\n",
      "Epoch 2600, Loss: 0.08904965966939926, Final Batch Loss: 0.038281992077827454\n",
      "Epoch 2601, Loss: 0.1319827251136303, Final Batch Loss: 0.07977018505334854\n",
      "Epoch 2602, Loss: 0.09115276113152504, Final Batch Loss: 0.04182706028223038\n",
      "Epoch 2603, Loss: 0.0803709626197815, Final Batch Loss: 0.033466413617134094\n",
      "Epoch 2604, Loss: 0.1061922200024128, Final Batch Loss: 0.06752866506576538\n",
      "Epoch 2605, Loss: 0.08546527102589607, Final Batch Loss: 0.04548047482967377\n",
      "Epoch 2606, Loss: 0.09173133969306946, Final Batch Loss: 0.05132154002785683\n",
      "Epoch 2607, Loss: 0.08690128102898598, Final Batch Loss: 0.05544888228178024\n",
      "Epoch 2608, Loss: 0.113477673381567, Final Batch Loss: 0.054513875395059586\n",
      "Epoch 2609, Loss: 0.09206066280603409, Final Batch Loss: 0.03935414180159569\n",
      "Epoch 2610, Loss: 0.13043781369924545, Final Batch Loss: 0.07471385598182678\n",
      "Epoch 2611, Loss: 0.09899497777223587, Final Batch Loss: 0.0406448096036911\n",
      "Epoch 2612, Loss: 0.08650253713130951, Final Batch Loss: 0.049912575632333755\n",
      "Epoch 2613, Loss: 0.0698360949754715, Final Batch Loss: 0.02860686555504799\n",
      "Epoch 2614, Loss: 0.11732558906078339, Final Batch Loss: 0.07228236645460129\n",
      "Epoch 2615, Loss: 0.10328318551182747, Final Batch Loss: 0.04620511084794998\n",
      "Epoch 2616, Loss: 0.12138572707772255, Final Batch Loss: 0.07250893861055374\n",
      "Epoch 2617, Loss: 0.11087197065353394, Final Batch Loss: 0.056231435388326645\n",
      "Epoch 2618, Loss: 0.09064843505620956, Final Batch Loss: 0.046062443405389786\n",
      "Epoch 2619, Loss: 0.08454208076000214, Final Batch Loss: 0.0471222959458828\n",
      "Epoch 2620, Loss: 0.09799839556217194, Final Batch Loss: 0.03537625074386597\n",
      "Epoch 2621, Loss: 0.09635176509618759, Final Batch Loss: 0.03487497568130493\n",
      "Epoch 2622, Loss: 0.08481097221374512, Final Batch Loss: 0.03556240350008011\n",
      "Epoch 2623, Loss: 0.12687834724783897, Final Batch Loss: 0.05602744594216347\n",
      "Epoch 2624, Loss: 0.07971111312508583, Final Batch Loss: 0.042836543172597885\n",
      "Epoch 2625, Loss: 0.12605202570557594, Final Batch Loss: 0.054880257695913315\n",
      "Epoch 2626, Loss: 0.08571377396583557, Final Batch Loss: 0.04716264829039574\n",
      "Epoch 2627, Loss: 0.09498804062604904, Final Batch Loss: 0.05363786220550537\n",
      "Epoch 2628, Loss: 0.10578504204750061, Final Batch Loss: 0.049318425357341766\n",
      "Epoch 2629, Loss: 0.07050864398479462, Final Batch Loss: 0.02764441817998886\n",
      "Epoch 2630, Loss: 0.07076391950249672, Final Batch Loss: 0.031416330486536026\n",
      "Epoch 2631, Loss: 0.07695268467068672, Final Batch Loss: 0.03602425381541252\n",
      "Epoch 2632, Loss: 0.08910584822297096, Final Batch Loss: 0.045359786599874496\n",
      "Epoch 2633, Loss: 0.08369250036776066, Final Batch Loss: 0.026075979694724083\n",
      "Epoch 2634, Loss: 0.09738790988922119, Final Batch Loss: 0.041208621114492416\n",
      "Epoch 2635, Loss: 0.0755770392715931, Final Batch Loss: 0.05078694224357605\n",
      "Epoch 2636, Loss: 0.07549307867884636, Final Batch Loss: 0.03344626724720001\n",
      "Epoch 2637, Loss: 0.07525087147951126, Final Batch Loss: 0.032034970819950104\n",
      "Epoch 2638, Loss: 0.08663662523031235, Final Batch Loss: 0.048293162137269974\n",
      "Epoch 2639, Loss: 0.08474507182836533, Final Batch Loss: 0.045543771237134933\n",
      "Epoch 2640, Loss: 0.0852593332529068, Final Batch Loss: 0.03750598430633545\n",
      "Epoch 2641, Loss: 0.08241494558751583, Final Batch Loss: 0.05399487912654877\n",
      "Epoch 2642, Loss: 0.08709533140063286, Final Batch Loss: 0.035715971142053604\n",
      "Epoch 2643, Loss: 0.07159677892923355, Final Batch Loss: 0.03291697800159454\n",
      "Epoch 2644, Loss: 0.09027375280857086, Final Batch Loss: 0.04808492213487625\n",
      "Epoch 2645, Loss: 0.08286470174789429, Final Batch Loss: 0.04450299218297005\n",
      "Epoch 2646, Loss: 0.10670749098062515, Final Batch Loss: 0.07004725188016891\n",
      "Epoch 2647, Loss: 0.10327766463160515, Final Batch Loss: 0.03243367001414299\n",
      "Epoch 2648, Loss: 0.09517471119761467, Final Batch Loss: 0.04987721890211105\n",
      "Epoch 2649, Loss: 0.10452486202120781, Final Batch Loss: 0.05979279801249504\n",
      "Epoch 2650, Loss: 0.06871551647782326, Final Batch Loss: 0.025372806936502457\n",
      "Epoch 2651, Loss: 0.08078968524932861, Final Batch Loss: 0.04522509500384331\n",
      "Epoch 2652, Loss: 0.10371604934334755, Final Batch Loss: 0.04070441052317619\n",
      "Epoch 2653, Loss: 0.13042708113789558, Final Batch Loss: 0.07683752477169037\n",
      "Epoch 2654, Loss: 0.10718042217195034, Final Batch Loss: 0.020709214732050896\n",
      "Epoch 2655, Loss: 0.08845960348844528, Final Batch Loss: 0.042279891669750214\n",
      "Epoch 2656, Loss: 0.08591669984161854, Final Batch Loss: 0.029075225815176964\n",
      "Epoch 2657, Loss: 0.083236463367939, Final Batch Loss: 0.04546341672539711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2658, Loss: 0.08159506879746914, Final Batch Loss: 0.030083609744906425\n",
      "Epoch 2659, Loss: 0.10778316110372543, Final Batch Loss: 0.06714857369661331\n",
      "Epoch 2660, Loss: 0.07794218882918358, Final Batch Loss: 0.03515860438346863\n",
      "Epoch 2661, Loss: 0.07586229220032692, Final Batch Loss: 0.04075321927666664\n",
      "Epoch 2662, Loss: 0.09062100946903229, Final Batch Loss: 0.04989323019981384\n",
      "Epoch 2663, Loss: 0.1350272111594677, Final Batch Loss: 0.08555231243371964\n",
      "Epoch 2664, Loss: 0.09441141039133072, Final Batch Loss: 0.04929172992706299\n",
      "Epoch 2665, Loss: 0.07941240817308426, Final Batch Loss: 0.03444695845246315\n",
      "Epoch 2666, Loss: 0.09466119483113289, Final Batch Loss: 0.0528871975839138\n",
      "Epoch 2667, Loss: 0.10039074718952179, Final Batch Loss: 0.04935072362422943\n",
      "Epoch 2668, Loss: 0.09099332243204117, Final Batch Loss: 0.025665707886219025\n",
      "Epoch 2669, Loss: 0.0928509347140789, Final Batch Loss: 0.042102500796318054\n",
      "Epoch 2670, Loss: 0.06426815502345562, Final Batch Loss: 0.038003139197826385\n",
      "Epoch 2671, Loss: 0.08294379338622093, Final Batch Loss: 0.034510865807533264\n",
      "Epoch 2672, Loss: 0.1789507158100605, Final Batch Loss: 0.1298932284116745\n",
      "Epoch 2673, Loss: 0.08703303337097168, Final Batch Loss: 0.04091145098209381\n",
      "Epoch 2674, Loss: 0.09855050221085548, Final Batch Loss: 0.05741975083947182\n",
      "Epoch 2675, Loss: 0.08032320439815521, Final Batch Loss: 0.05436334386467934\n",
      "Epoch 2676, Loss: 0.09513620287179947, Final Batch Loss: 0.04172441363334656\n",
      "Epoch 2677, Loss: 0.09149036556482315, Final Batch Loss: 0.054433293640613556\n",
      "Epoch 2678, Loss: 0.11301514506340027, Final Batch Loss: 0.0481509268283844\n",
      "Epoch 2679, Loss: 0.08135225623846054, Final Batch Loss: 0.03720336779952049\n",
      "Epoch 2680, Loss: 0.10191163048148155, Final Batch Loss: 0.05657245218753815\n",
      "Epoch 2681, Loss: 0.1111181452870369, Final Batch Loss: 0.04750476032495499\n",
      "Epoch 2682, Loss: 0.10355871543288231, Final Batch Loss: 0.05993940308690071\n",
      "Epoch 2683, Loss: 0.08023375645279884, Final Batch Loss: 0.03237319737672806\n",
      "Epoch 2684, Loss: 0.0724235512316227, Final Batch Loss: 0.035072896629571915\n",
      "Epoch 2685, Loss: 0.0842164047062397, Final Batch Loss: 0.039537861943244934\n",
      "Epoch 2686, Loss: 0.10131718404591084, Final Batch Loss: 0.028953077271580696\n",
      "Epoch 2687, Loss: 0.11719033122062683, Final Batch Loss: 0.05687107518315315\n",
      "Epoch 2688, Loss: 0.09698935970664024, Final Batch Loss: 0.03557593375444412\n",
      "Epoch 2689, Loss: 0.10450408235192299, Final Batch Loss: 0.05129920691251755\n",
      "Epoch 2690, Loss: 0.08376374840736389, Final Batch Loss: 0.04309411346912384\n",
      "Epoch 2691, Loss: 0.12852105870842934, Final Batch Loss: 0.058932457119226456\n",
      "Epoch 2692, Loss: 0.0914570800960064, Final Batch Loss: 0.057809241116046906\n",
      "Epoch 2693, Loss: 0.08938391134142876, Final Batch Loss: 0.022454265505075455\n",
      "Epoch 2694, Loss: 0.10318025574088097, Final Batch Loss: 0.05071904882788658\n",
      "Epoch 2695, Loss: 0.11094659939408302, Final Batch Loss: 0.03681419417262077\n",
      "Epoch 2696, Loss: 0.09379301965236664, Final Batch Loss: 0.05726484954357147\n",
      "Epoch 2697, Loss: 0.0907612144947052, Final Batch Loss: 0.05822975933551788\n",
      "Epoch 2698, Loss: 0.07436694577336311, Final Batch Loss: 0.041472725570201874\n",
      "Epoch 2699, Loss: 0.08403641730546951, Final Batch Loss: 0.037151020020246506\n",
      "Epoch 2700, Loss: 0.10288555547595024, Final Batch Loss: 0.06364579498767853\n",
      "Epoch 2701, Loss: 0.0931556187570095, Final Batch Loss: 0.03578793257474899\n",
      "Epoch 2702, Loss: 0.06962152943015099, Final Batch Loss: 0.04374534636735916\n",
      "Epoch 2703, Loss: 0.10165842995047569, Final Batch Loss: 0.06965940445661545\n",
      "Epoch 2704, Loss: 0.09859458170831203, Final Batch Loss: 0.02542104385793209\n",
      "Epoch 2705, Loss: 0.10924969241023064, Final Batch Loss: 0.05681900680065155\n",
      "Epoch 2706, Loss: 0.07134380750358105, Final Batch Loss: 0.03124728985130787\n",
      "Epoch 2707, Loss: 0.12064935639500618, Final Batch Loss: 0.0778178870677948\n",
      "Epoch 2708, Loss: 0.11184711568057537, Final Batch Loss: 0.0819794312119484\n",
      "Epoch 2709, Loss: 0.07922750897705555, Final Batch Loss: 0.04832169786095619\n",
      "Epoch 2710, Loss: 0.11147775128483772, Final Batch Loss: 0.05181145668029785\n",
      "Epoch 2711, Loss: 0.08394239284098148, Final Batch Loss: 0.022036952897906303\n",
      "Epoch 2712, Loss: 0.1297922097146511, Final Batch Loss: 0.052384767681360245\n",
      "Epoch 2713, Loss: 0.1162530705332756, Final Batch Loss: 0.03887851536273956\n",
      "Epoch 2714, Loss: 0.11415216512978077, Final Batch Loss: 0.08879457414150238\n",
      "Epoch 2715, Loss: 0.080286068841815, Final Batch Loss: 0.030833391472697258\n",
      "Epoch 2716, Loss: 0.12234130501747131, Final Batch Loss: 0.06606591492891312\n",
      "Epoch 2717, Loss: 0.1215551607310772, Final Batch Loss: 0.0720127522945404\n",
      "Epoch 2718, Loss: 0.10919307544827461, Final Batch Loss: 0.05488536134362221\n",
      "Epoch 2719, Loss: 0.07880003936588764, Final Batch Loss: 0.0604509636759758\n",
      "Epoch 2720, Loss: 0.09098472818732262, Final Batch Loss: 0.03266388550400734\n",
      "Epoch 2721, Loss: 0.13617589697241783, Final Batch Loss: 0.09733384847640991\n",
      "Epoch 2722, Loss: 0.09857451915740967, Final Batch Loss: 0.03967580199241638\n",
      "Epoch 2723, Loss: 0.08628291264176369, Final Batch Loss: 0.03008967638015747\n",
      "Epoch 2724, Loss: 0.09429144114255905, Final Batch Loss: 0.050224557518959045\n",
      "Epoch 2725, Loss: 0.10975877195596695, Final Batch Loss: 0.05449990928173065\n",
      "Epoch 2726, Loss: 0.10484332591295242, Final Batch Loss: 0.05785306915640831\n",
      "Epoch 2727, Loss: 0.09161433577537537, Final Batch Loss: 0.040977414697408676\n",
      "Epoch 2728, Loss: 0.09428434073925018, Final Batch Loss: 0.045778434723615646\n",
      "Epoch 2729, Loss: 0.13258875161409378, Final Batch Loss: 0.06480271369218826\n",
      "Epoch 2730, Loss: 0.08241576701402664, Final Batch Loss: 0.04033613204956055\n",
      "Epoch 2731, Loss: 0.10466476902365685, Final Batch Loss: 0.07045366615056992\n",
      "Epoch 2732, Loss: 0.08291986957192421, Final Batch Loss: 0.052495814859867096\n",
      "Epoch 2733, Loss: 0.0941319540143013, Final Batch Loss: 0.03703128546476364\n",
      "Epoch 2734, Loss: 0.11173912137746811, Final Batch Loss: 0.06293307989835739\n",
      "Epoch 2735, Loss: 0.08929628133773804, Final Batch Loss: 0.04593169316649437\n",
      "Epoch 2736, Loss: 0.11691337451338768, Final Batch Loss: 0.045725975185632706\n",
      "Epoch 2737, Loss: 0.08978323265910149, Final Batch Loss: 0.04913770407438278\n",
      "Epoch 2738, Loss: 0.10331804677844048, Final Batch Loss: 0.04545366391539574\n",
      "Epoch 2739, Loss: 0.10551315173506737, Final Batch Loss: 0.04738845303654671\n",
      "Epoch 2740, Loss: 0.08996722847223282, Final Batch Loss: 0.04217817261815071\n",
      "Epoch 2741, Loss: 0.11728548258543015, Final Batch Loss: 0.07228178530931473\n",
      "Epoch 2742, Loss: 0.10240060836076736, Final Batch Loss: 0.04445597901940346\n",
      "Epoch 2743, Loss: 0.09852668084204197, Final Batch Loss: 0.0704001784324646\n",
      "Epoch 2744, Loss: 0.07205255702137947, Final Batch Loss: 0.020747069269418716\n",
      "Epoch 2745, Loss: 0.12154285982251167, Final Batch Loss: 0.042685870081186295\n",
      "Epoch 2746, Loss: 0.14865423738956451, Final Batch Loss: 0.04217378795146942\n",
      "Epoch 2747, Loss: 0.1112692840397358, Final Batch Loss: 0.04279619827866554\n",
      "Epoch 2748, Loss: 0.12052219733595848, Final Batch Loss: 0.052774544805288315\n",
      "Epoch 2749, Loss: 0.08148516714572906, Final Batch Loss: 0.049448855221271515\n",
      "Epoch 2750, Loss: 0.11973730102181435, Final Batch Loss: 0.07962366193532944\n",
      "Epoch 2751, Loss: 0.11918379738926888, Final Batch Loss: 0.06793004274368286\n",
      "Epoch 2752, Loss: 0.082286287099123, Final Batch Loss: 0.04142216220498085\n",
      "Epoch 2753, Loss: 0.07346762716770172, Final Batch Loss: 0.03356977179646492\n",
      "Epoch 2754, Loss: 0.07295248843729496, Final Batch Loss: 0.02678743563592434\n",
      "Epoch 2755, Loss: 0.3420410454273224, Final Batch Loss: 0.2765010595321655\n",
      "Epoch 2756, Loss: 0.23946348577737808, Final Batch Loss: 0.03876771777868271\n",
      "Epoch 2757, Loss: 0.07783391699194908, Final Batch Loss: 0.038981564342975616\n",
      "Epoch 2758, Loss: 0.08961073867976665, Final Batch Loss: 0.029449747875332832\n",
      "Epoch 2759, Loss: 0.09350384399294853, Final Batch Loss: 0.04042908176779747\n",
      "Epoch 2760, Loss: 0.09575005993247032, Final Batch Loss: 0.0681789442896843\n",
      "Epoch 2761, Loss: 0.09455160424113274, Final Batch Loss: 0.05040642246603966\n",
      "Epoch 2762, Loss: 0.1055627092719078, Final Batch Loss: 0.06361033022403717\n",
      "Epoch 2763, Loss: 0.09968054667115211, Final Batch Loss: 0.03151962533593178\n",
      "Epoch 2764, Loss: 0.07760295085608959, Final Batch Loss: 0.026373280212283134\n",
      "Epoch 2765, Loss: 0.12401698902249336, Final Batch Loss: 0.09369974583387375\n",
      "Epoch 2766, Loss: 0.10224876552820206, Final Batch Loss: 0.041351038962602615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2767, Loss: 0.07932517677545547, Final Batch Loss: 0.04185803234577179\n",
      "Epoch 2768, Loss: 0.13855716958642006, Final Batch Loss: 0.05977518483996391\n",
      "Epoch 2769, Loss: 0.08059640973806381, Final Batch Loss: 0.04696614295244217\n",
      "Epoch 2770, Loss: 0.09888853132724762, Final Batch Loss: 0.060555629432201385\n",
      "Epoch 2771, Loss: 0.08331811428070068, Final Batch Loss: 0.044769950211048126\n",
      "Epoch 2772, Loss: 0.07865775004029274, Final Batch Loss: 0.039410751312971115\n",
      "Epoch 2773, Loss: 0.09752015769481659, Final Batch Loss: 0.06778699159622192\n",
      "Epoch 2774, Loss: 0.11129932478070259, Final Batch Loss: 0.044866036623716354\n",
      "Epoch 2775, Loss: 0.10443230159580708, Final Batch Loss: 0.027305366471409798\n",
      "Epoch 2776, Loss: 0.104612335562706, Final Batch Loss: 0.03795681893825531\n",
      "Epoch 2777, Loss: 0.1534644067287445, Final Batch Loss: 0.11021348088979721\n",
      "Epoch 2778, Loss: 0.11701452359557152, Final Batch Loss: 0.05570969730615616\n",
      "Epoch 2779, Loss: 0.09056593477725983, Final Batch Loss: 0.029262110590934753\n",
      "Epoch 2780, Loss: 0.0910794772207737, Final Batch Loss: 0.033655740320682526\n",
      "Epoch 2781, Loss: 0.11971838399767876, Final Batch Loss: 0.0310051329433918\n",
      "Epoch 2782, Loss: 0.07820938155055046, Final Batch Loss: 0.03828345239162445\n",
      "Epoch 2783, Loss: 0.10008152946829796, Final Batch Loss: 0.04492639750242233\n",
      "Epoch 2784, Loss: 0.09145288541913033, Final Batch Loss: 0.06037406623363495\n",
      "Epoch 2785, Loss: 0.1189553253352642, Final Batch Loss: 0.060138341039419174\n",
      "Epoch 2786, Loss: 0.08583515509963036, Final Batch Loss: 0.03280724212527275\n",
      "Epoch 2787, Loss: 0.11850672587752342, Final Batch Loss: 0.07693644613027573\n",
      "Epoch 2788, Loss: 0.08484164997935295, Final Batch Loss: 0.04288091883063316\n",
      "Epoch 2789, Loss: 0.11394723877310753, Final Batch Loss: 0.04949076846241951\n",
      "Epoch 2790, Loss: 0.08429135009646416, Final Batch Loss: 0.04361358657479286\n",
      "Epoch 2791, Loss: 0.12449192628264427, Final Batch Loss: 0.07423940300941467\n",
      "Epoch 2792, Loss: 0.10331331193447113, Final Batch Loss: 0.05529055371880531\n",
      "Epoch 2793, Loss: 0.07788800075650215, Final Batch Loss: 0.037939541041851044\n",
      "Epoch 2794, Loss: 0.07950184494256973, Final Batch Loss: 0.03752513229846954\n",
      "Epoch 2795, Loss: 0.08368578925728798, Final Batch Loss: 0.04210759699344635\n",
      "Epoch 2796, Loss: 0.08152168244123459, Final Batch Loss: 0.04532051831483841\n",
      "Epoch 2797, Loss: 0.11081883311271667, Final Batch Loss: 0.06019236519932747\n",
      "Epoch 2798, Loss: 0.14277027547359467, Final Batch Loss: 0.07336313277482986\n",
      "Epoch 2799, Loss: 0.12590396031737328, Final Batch Loss: 0.0710538774728775\n",
      "Epoch 2800, Loss: 0.09746555425226688, Final Batch Loss: 0.07158605009317398\n",
      "Epoch 2801, Loss: 0.08548877388238907, Final Batch Loss: 0.04348624497652054\n",
      "Epoch 2802, Loss: 0.09564191102981567, Final Batch Loss: 0.050875380635261536\n",
      "Epoch 2803, Loss: 0.12407596036791801, Final Batch Loss: 0.041415635496377945\n",
      "Epoch 2804, Loss: 0.08031535148620605, Final Batch Loss: 0.04127803444862366\n",
      "Epoch 2805, Loss: 0.1587495021522045, Final Batch Loss: 0.12420535832643509\n",
      "Epoch 2806, Loss: 0.09582700207829475, Final Batch Loss: 0.03876376152038574\n",
      "Epoch 2807, Loss: 0.07124785333871841, Final Batch Loss: 0.035619549453258514\n",
      "Epoch 2808, Loss: 0.09508981183171272, Final Batch Loss: 0.02967628464102745\n",
      "Epoch 2809, Loss: 0.08164431899785995, Final Batch Loss: 0.03379753232002258\n",
      "Epoch 2810, Loss: 0.09520914033055305, Final Batch Loss: 0.04591825231909752\n",
      "Epoch 2811, Loss: 0.09594321623444557, Final Batch Loss: 0.0448710136115551\n",
      "Epoch 2812, Loss: 0.07841138169169426, Final Batch Loss: 0.03450396656990051\n",
      "Epoch 2813, Loss: 0.09424340352416039, Final Batch Loss: 0.061392512172460556\n",
      "Epoch 2814, Loss: 0.09961838647723198, Final Batch Loss: 0.040013160556554794\n",
      "Epoch 2815, Loss: 0.09814155101776123, Final Batch Loss: 0.0645216777920723\n",
      "Epoch 2816, Loss: 0.14978356659412384, Final Batch Loss: 0.05568554252386093\n",
      "Epoch 2817, Loss: 0.07406098023056984, Final Batch Loss: 0.03595145046710968\n",
      "Epoch 2818, Loss: 0.09307662397623062, Final Batch Loss: 0.049899760633707047\n",
      "Epoch 2819, Loss: 0.07221201248466969, Final Batch Loss: 0.028445051982998848\n",
      "Epoch 2820, Loss: 0.10121011361479759, Final Batch Loss: 0.03976510465145111\n",
      "Epoch 2821, Loss: 0.08838136866688728, Final Batch Loss: 0.06246477738022804\n",
      "Epoch 2822, Loss: 0.07223742455244064, Final Batch Loss: 0.028159543871879578\n",
      "Epoch 2823, Loss: 0.09029793739318848, Final Batch Loss: 0.04772668331861496\n",
      "Epoch 2824, Loss: 0.11217119917273521, Final Batch Loss: 0.07889202237129211\n",
      "Epoch 2825, Loss: 0.09265613928437233, Final Batch Loss: 0.06037638336420059\n",
      "Epoch 2826, Loss: 0.09070141240954399, Final Batch Loss: 0.06122642010450363\n",
      "Epoch 2827, Loss: 0.11291900649666786, Final Batch Loss: 0.04991251602768898\n",
      "Epoch 2828, Loss: 0.0793184582144022, Final Batch Loss: 0.05170087888836861\n",
      "Epoch 2829, Loss: 0.0785382129251957, Final Batch Loss: 0.037202175706624985\n",
      "Epoch 2830, Loss: 0.10106348618865013, Final Batch Loss: 0.050444040447473526\n",
      "Epoch 2831, Loss: 0.06358405016362667, Final Batch Loss: 0.03344862908124924\n",
      "Epoch 2832, Loss: 0.09074145555496216, Final Batch Loss: 0.05776328593492508\n",
      "Epoch 2833, Loss: 0.09466804191470146, Final Batch Loss: 0.049026813358068466\n",
      "Epoch 2834, Loss: 0.13523689657449722, Final Batch Loss: 0.049439214169979095\n",
      "Epoch 2835, Loss: 0.16778448224067688, Final Batch Loss: 0.0767223909497261\n",
      "Epoch 2836, Loss: 0.08919239416718483, Final Batch Loss: 0.038830727338790894\n",
      "Epoch 2837, Loss: 0.08428965508937836, Final Batch Loss: 0.035389307886362076\n",
      "Epoch 2838, Loss: 0.09275271371006966, Final Batch Loss: 0.06320968270301819\n",
      "Epoch 2839, Loss: 0.07712602615356445, Final Batch Loss: 0.03508094325661659\n",
      "Epoch 2840, Loss: 0.09241435304284096, Final Batch Loss: 0.05577990412712097\n",
      "Epoch 2841, Loss: 0.09896310046315193, Final Batch Loss: 0.04640975594520569\n",
      "Epoch 2842, Loss: 0.09285565093159676, Final Batch Loss: 0.041424162685871124\n",
      "Epoch 2843, Loss: 0.09671138226985931, Final Batch Loss: 0.049431413412094116\n",
      "Epoch 2844, Loss: 0.14884107187390327, Final Batch Loss: 0.06033344194293022\n",
      "Epoch 2845, Loss: 0.17478757351636887, Final Batch Loss: 0.055528052151203156\n",
      "Epoch 2846, Loss: 0.10246656090021133, Final Batch Loss: 0.04232015460729599\n",
      "Epoch 2847, Loss: 0.103144820779562, Final Batch Loss: 0.05156398564577103\n",
      "Epoch 2848, Loss: 0.14760787039995193, Final Batch Loss: 0.1015065535902977\n",
      "Epoch 2849, Loss: 0.08331749588251114, Final Batch Loss: 0.04539354518055916\n",
      "Epoch 2850, Loss: 0.09021634235978127, Final Batch Loss: 0.02613287791609764\n",
      "Epoch 2851, Loss: 0.0916609913110733, Final Batch Loss: 0.04774116724729538\n",
      "Epoch 2852, Loss: 0.14001507312059402, Final Batch Loss: 0.04335108399391174\n",
      "Epoch 2853, Loss: 0.08322042971849442, Final Batch Loss: 0.03396238386631012\n",
      "Epoch 2854, Loss: 0.09345824271440506, Final Batch Loss: 0.03575994819402695\n",
      "Epoch 2855, Loss: 0.104160251095891, Final Batch Loss: 0.07865637540817261\n",
      "Epoch 2856, Loss: 0.0898982621729374, Final Batch Loss: 0.05570961534976959\n",
      "Epoch 2857, Loss: 0.1361069530248642, Final Batch Loss: 0.05724941939115524\n",
      "Epoch 2858, Loss: 0.09031027555465698, Final Batch Loss: 0.03603965789079666\n",
      "Epoch 2859, Loss: 0.0749235451221466, Final Batch Loss: 0.0454987958073616\n",
      "Epoch 2860, Loss: 0.0789667759090662, Final Batch Loss: 0.02694098837673664\n",
      "Epoch 2861, Loss: 0.10143959522247314, Final Batch Loss: 0.06074272096157074\n",
      "Epoch 2862, Loss: 0.08138507232069969, Final Batch Loss: 0.04277555271983147\n",
      "Epoch 2863, Loss: 0.15934742242097855, Final Batch Loss: 0.07278861850500107\n",
      "Epoch 2864, Loss: 0.11531179398298264, Final Batch Loss: 0.04013910889625549\n",
      "Epoch 2865, Loss: 0.11395135521888733, Final Batch Loss: 0.043366819620132446\n",
      "Epoch 2866, Loss: 0.08198191970586777, Final Batch Loss: 0.030802633613348007\n",
      "Epoch 2867, Loss: 0.09265758097171783, Final Batch Loss: 0.0533815398812294\n",
      "Epoch 2868, Loss: 0.12486681342124939, Final Batch Loss: 0.09048277139663696\n",
      "Epoch 2869, Loss: 0.08002747595310211, Final Batch Loss: 0.033144574612379074\n",
      "Epoch 2870, Loss: 0.08607189357280731, Final Batch Loss: 0.04976363107562065\n",
      "Epoch 2871, Loss: 0.11772442981600761, Final Batch Loss: 0.059981368482112885\n",
      "Epoch 2872, Loss: 0.07111011072993279, Final Batch Loss: 0.03781938925385475\n",
      "Epoch 2873, Loss: 0.07746826484799385, Final Batch Loss: 0.053814053535461426\n",
      "Epoch 2874, Loss: 0.09568548575043678, Final Batch Loss: 0.02992495521903038\n",
      "Epoch 2875, Loss: 0.08383526280522346, Final Batch Loss: 0.04752906784415245\n",
      "Epoch 2876, Loss: 0.10632045194506645, Final Batch Loss: 0.048979874700307846\n",
      "Epoch 2877, Loss: 0.07792780175805092, Final Batch Loss: 0.050567373633384705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2878, Loss: 0.07108308002352715, Final Batch Loss: 0.03899066895246506\n",
      "Epoch 2879, Loss: 0.10467666760087013, Final Batch Loss: 0.03163917735219002\n",
      "Epoch 2880, Loss: 0.06560169532895088, Final Batch Loss: 0.032918531447649\n",
      "Epoch 2881, Loss: 0.10322963818907738, Final Batch Loss: 0.040056344121694565\n",
      "Epoch 2882, Loss: 0.0800556167960167, Final Batch Loss: 0.03864484280347824\n",
      "Epoch 2883, Loss: 0.08813228085637093, Final Batch Loss: 0.04084782302379608\n",
      "Epoch 2884, Loss: 0.08608884364366531, Final Batch Loss: 0.0399148054420948\n",
      "Epoch 2885, Loss: 0.1020386703312397, Final Batch Loss: 0.06622126698493958\n",
      "Epoch 2886, Loss: 0.09772384911775589, Final Batch Loss: 0.05751168727874756\n",
      "Epoch 2887, Loss: 0.07603631541132927, Final Batch Loss: 0.04350227490067482\n",
      "Epoch 2888, Loss: 0.09470287710428238, Final Batch Loss: 0.06685935705900192\n",
      "Epoch 2889, Loss: 0.0795263946056366, Final Batch Loss: 0.02377810701727867\n",
      "Epoch 2890, Loss: 0.08393626101315022, Final Batch Loss: 0.030062684789299965\n",
      "Epoch 2891, Loss: 0.11495787464082241, Final Batch Loss: 0.0267960112541914\n",
      "Epoch 2892, Loss: 0.08834189176559448, Final Batch Loss: 0.05006958544254303\n",
      "Epoch 2893, Loss: 0.10473308712244034, Final Batch Loss: 0.037689000368118286\n",
      "Epoch 2894, Loss: 0.0765320211648941, Final Batch Loss: 0.03326837345957756\n",
      "Epoch 2895, Loss: 0.10586066916584969, Final Batch Loss: 0.055458493530750275\n",
      "Epoch 2896, Loss: 0.08455538377165794, Final Batch Loss: 0.03454606235027313\n",
      "Epoch 2897, Loss: 0.08356989175081253, Final Batch Loss: 0.03376268967986107\n",
      "Epoch 2898, Loss: 0.12479490041732788, Final Batch Loss: 0.07099802792072296\n",
      "Epoch 2899, Loss: 0.09188954532146454, Final Batch Loss: 0.0523114912211895\n",
      "Epoch 2900, Loss: 0.07250857166945934, Final Batch Loss: 0.041660189628601074\n",
      "Epoch 2901, Loss: 0.09316086024045944, Final Batch Loss: 0.046135708689689636\n",
      "Epoch 2902, Loss: 0.07054977491497993, Final Batch Loss: 0.02472022920846939\n",
      "Epoch 2903, Loss: 0.07846013829112053, Final Batch Loss: 0.0530904084444046\n",
      "Epoch 2904, Loss: 0.0733172781765461, Final Batch Loss: 0.036313459277153015\n",
      "Epoch 2905, Loss: 0.07682989910244942, Final Batch Loss: 0.03167757764458656\n",
      "Epoch 2906, Loss: 0.067324910312891, Final Batch Loss: 0.019260026514530182\n",
      "Epoch 2907, Loss: 0.10051338374614716, Final Batch Loss: 0.05548108369112015\n",
      "Epoch 2908, Loss: 0.09139250218868256, Final Batch Loss: 0.0460156612098217\n",
      "Epoch 2909, Loss: 0.08907146006822586, Final Batch Loss: 0.03236013278365135\n",
      "Epoch 2910, Loss: 0.08459549024701118, Final Batch Loss: 0.03307857736945152\n",
      "Epoch 2911, Loss: 0.10175275430083275, Final Batch Loss: 0.05298537760972977\n",
      "Epoch 2912, Loss: 0.10035877302289009, Final Batch Loss: 0.03679027035832405\n",
      "Epoch 2913, Loss: 0.07921270653605461, Final Batch Loss: 0.032172467559576035\n",
      "Epoch 2914, Loss: 0.07788775861263275, Final Batch Loss: 0.035432104021310806\n",
      "Epoch 2915, Loss: 0.08016850985586643, Final Batch Loss: 0.026035072281956673\n",
      "Epoch 2916, Loss: 0.08236198872327805, Final Batch Loss: 0.034934960305690765\n",
      "Epoch 2917, Loss: 0.06789925694465637, Final Batch Loss: 0.024645939469337463\n",
      "Epoch 2918, Loss: 0.08114324137568474, Final Batch Loss: 0.04096117615699768\n",
      "Epoch 2919, Loss: 0.0785632636398077, Final Batch Loss: 0.05060401186347008\n",
      "Epoch 2920, Loss: 0.06868980452418327, Final Batch Loss: 0.0316225104033947\n",
      "Epoch 2921, Loss: 0.07783873379230499, Final Batch Loss: 0.05234650894999504\n",
      "Epoch 2922, Loss: 0.07466313987970352, Final Batch Loss: 0.036744456738233566\n",
      "Epoch 2923, Loss: 0.07638176530599594, Final Batch Loss: 0.0337085984647274\n",
      "Epoch 2924, Loss: 0.07071376778185368, Final Batch Loss: 0.02819964475929737\n",
      "Epoch 2925, Loss: 0.0723154079169035, Final Batch Loss: 0.03122137300670147\n",
      "Epoch 2926, Loss: 0.06756941974163055, Final Batch Loss: 0.024372074753046036\n",
      "Epoch 2927, Loss: 0.07622983120381832, Final Batch Loss: 0.02097017504274845\n",
      "Epoch 2928, Loss: 0.0893397368490696, Final Batch Loss: 0.05083943158388138\n",
      "Epoch 2929, Loss: 0.08304869756102562, Final Batch Loss: 0.02728421613574028\n",
      "Epoch 2930, Loss: 0.0845550000667572, Final Batch Loss: 0.03913018852472305\n",
      "Epoch 2931, Loss: 0.08547065407037735, Final Batch Loss: 0.023325633257627487\n",
      "Epoch 2932, Loss: 0.07368716597557068, Final Batch Loss: 0.03743298351764679\n",
      "Epoch 2933, Loss: 0.06654213555157185, Final Batch Loss: 0.024511920288205147\n",
      "Epoch 2934, Loss: 0.07068005204200745, Final Batch Loss: 0.037462688982486725\n",
      "Epoch 2935, Loss: 0.0760215763002634, Final Batch Loss: 0.023462524637579918\n",
      "Epoch 2936, Loss: 0.07075180858373642, Final Batch Loss: 0.05599197372794151\n",
      "Epoch 2937, Loss: 0.10201047733426094, Final Batch Loss: 0.07764080166816711\n",
      "Epoch 2938, Loss: 0.08605735562741756, Final Batch Loss: 0.06601933389902115\n",
      "Epoch 2939, Loss: 0.08881595730781555, Final Batch Loss: 0.05145452544093132\n",
      "Epoch 2940, Loss: 0.08355411142110825, Final Batch Loss: 0.035330839455127716\n",
      "Epoch 2941, Loss: 0.09306756407022476, Final Batch Loss: 0.03609059751033783\n",
      "Epoch 2942, Loss: 0.08089561015367508, Final Batch Loss: 0.032695770263671875\n",
      "Epoch 2943, Loss: 0.07835354283452034, Final Batch Loss: 0.032509058713912964\n",
      "Epoch 2944, Loss: 0.10903381928801537, Final Batch Loss: 0.06355896592140198\n",
      "Epoch 2945, Loss: 0.09021812304854393, Final Batch Loss: 0.058224156498909\n",
      "Epoch 2946, Loss: 0.07355540618300438, Final Batch Loss: 0.039110008627176285\n",
      "Epoch 2947, Loss: 0.10484088957309723, Final Batch Loss: 0.05315634235739708\n",
      "Epoch 2948, Loss: 0.07664955966174603, Final Batch Loss: 0.02463231422007084\n",
      "Epoch 2949, Loss: 0.06694075465202332, Final Batch Loss: 0.035289887338876724\n",
      "Epoch 2950, Loss: 0.06651521101593971, Final Batch Loss: 0.02976299077272415\n",
      "Epoch 2951, Loss: 0.06795802339911461, Final Batch Loss: 0.03664359822869301\n",
      "Epoch 2952, Loss: 0.12229571491479874, Final Batch Loss: 0.03561942279338837\n",
      "Epoch 2953, Loss: 0.06157763674855232, Final Batch Loss: 0.026565823704004288\n",
      "Epoch 2954, Loss: 0.0919596441090107, Final Batch Loss: 0.05721809342503548\n",
      "Epoch 2955, Loss: 0.05601443536579609, Final Batch Loss: 0.024782584980130196\n",
      "Epoch 2956, Loss: 0.19513697922229767, Final Batch Loss: 0.07805286347866058\n",
      "Epoch 2957, Loss: 0.13485999032855034, Final Batch Loss: 0.09900656342506409\n",
      "Epoch 2958, Loss: 0.11002324894070625, Final Batch Loss: 0.0317978598177433\n",
      "Epoch 2959, Loss: 0.09764483943581581, Final Batch Loss: 0.0533638522028923\n",
      "Epoch 2960, Loss: 0.07398539409041405, Final Batch Loss: 0.04146624356508255\n",
      "Epoch 2961, Loss: 0.07807634025812149, Final Batch Loss: 0.040097229182720184\n",
      "Epoch 2962, Loss: 0.0942738950252533, Final Batch Loss: 0.03899269923567772\n",
      "Epoch 2963, Loss: 0.06438124738633633, Final Batch Loss: 0.034932155162096024\n",
      "Epoch 2964, Loss: 0.08901089057326317, Final Batch Loss: 0.05024706572294235\n",
      "Epoch 2965, Loss: 0.07415897399187088, Final Batch Loss: 0.042397357523441315\n",
      "Epoch 2966, Loss: 0.11693613603711128, Final Batch Loss: 0.04242973402142525\n",
      "Epoch 2967, Loss: 0.08744889870285988, Final Batch Loss: 0.04838675633072853\n",
      "Epoch 2968, Loss: 0.10121117159724236, Final Batch Loss: 0.05809646472334862\n",
      "Epoch 2969, Loss: 0.12144440598785877, Final Batch Loss: 0.02488877810537815\n",
      "Epoch 2970, Loss: 0.0929415374994278, Final Batch Loss: 0.05473463982343674\n",
      "Epoch 2971, Loss: 0.06344753876328468, Final Batch Loss: 0.03589564934372902\n",
      "Epoch 2972, Loss: 0.095733392983675, Final Batch Loss: 0.04577171802520752\n",
      "Epoch 2973, Loss: 0.10142097249627113, Final Batch Loss: 0.07187684625387192\n",
      "Epoch 2974, Loss: 0.08870314806699753, Final Batch Loss: 0.040921490639448166\n",
      "Epoch 2975, Loss: 0.06260670907795429, Final Batch Loss: 0.031624022871255875\n",
      "Epoch 2976, Loss: 0.0749956127256155, Final Batch Loss: 0.025101816281676292\n",
      "Epoch 2977, Loss: 0.09353001043200493, Final Batch Loss: 0.05043032392859459\n",
      "Epoch 2978, Loss: 0.09512939304113388, Final Batch Loss: 0.022646747529506683\n",
      "Epoch 2979, Loss: 0.08386151492595673, Final Batch Loss: 0.047576550394296646\n",
      "Epoch 2980, Loss: 0.07083921134471893, Final Batch Loss: 0.03652821108698845\n",
      "Epoch 2981, Loss: 0.08588361740112305, Final Batch Loss: 0.036462001502513885\n",
      "Epoch 2982, Loss: 0.07332048565149307, Final Batch Loss: 0.04185100272297859\n",
      "Epoch 2983, Loss: 0.078991973772645, Final Batch Loss: 0.050478607416152954\n",
      "Epoch 2984, Loss: 0.08396025747060776, Final Batch Loss: 0.0416075736284256\n",
      "Epoch 2985, Loss: 0.06572484597563744, Final Batch Loss: 0.03271310776472092\n",
      "Epoch 2986, Loss: 0.09615909680724144, Final Batch Loss: 0.05349324271082878\n",
      "Epoch 2987, Loss: 0.13328512758016586, Final Batch Loss: 0.09657265245914459\n",
      "Epoch 2988, Loss: 0.061491331085562706, Final Batch Loss: 0.025450902059674263\n",
      "Epoch 2989, Loss: 0.09884360432624817, Final Batch Loss: 0.05218172073364258\n",
      "Epoch 2990, Loss: 0.06540771946310997, Final Batch Loss: 0.030659928917884827\n",
      "Epoch 2991, Loss: 0.08269440941512585, Final Batch Loss: 0.059824567288160324\n",
      "Epoch 2992, Loss: 0.06565707921981812, Final Batch Loss: 0.035785410553216934\n",
      "Epoch 2993, Loss: 0.07296847552061081, Final Batch Loss: 0.03571988642215729\n",
      "Epoch 2994, Loss: 0.08888253569602966, Final Batch Loss: 0.04562501236796379\n",
      "Epoch 2995, Loss: 0.07539735734462738, Final Batch Loss: 0.0384596511721611\n",
      "Epoch 2996, Loss: 0.07411390542984009, Final Batch Loss: 0.033282700926065445\n",
      "Epoch 2997, Loss: 0.09215307608246803, Final Batch Loss: 0.05809645727276802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2998, Loss: 0.12066929787397385, Final Batch Loss: 0.033756323158741\n",
      "Epoch 2999, Loss: 0.10387151315808296, Final Batch Loss: 0.043153733015060425\n",
      "Epoch 3000, Loss: 0.07112683728337288, Final Batch Loss: 0.034524936228990555\n",
      "Epoch 3001, Loss: 0.12501190975308418, Final Batch Loss: 0.05304263159632683\n",
      "Epoch 3002, Loss: 0.12013770639896393, Final Batch Loss: 0.06384774297475815\n",
      "Epoch 3003, Loss: 0.0737382061779499, Final Batch Loss: 0.03347528353333473\n",
      "Epoch 3004, Loss: 0.0773641299456358, Final Batch Loss: 0.026159940287470818\n",
      "Epoch 3005, Loss: 0.11092662811279297, Final Batch Loss: 0.04886883124709129\n",
      "Epoch 3006, Loss: 0.07599840313196182, Final Batch Loss: 0.04441087692975998\n",
      "Epoch 3007, Loss: 0.07393212988972664, Final Batch Loss: 0.031617920845746994\n",
      "Epoch 3008, Loss: 0.08859348483383656, Final Batch Loss: 0.05860939621925354\n",
      "Epoch 3009, Loss: 0.12670521065592766, Final Batch Loss: 0.04034048691391945\n",
      "Epoch 3010, Loss: 0.07705435901880264, Final Batch Loss: 0.03770160675048828\n",
      "Epoch 3011, Loss: 0.12062717601656914, Final Batch Loss: 0.04642883315682411\n",
      "Epoch 3012, Loss: 0.07587593421339989, Final Batch Loss: 0.03743050619959831\n",
      "Epoch 3013, Loss: 0.0796867161989212, Final Batch Loss: 0.04293987154960632\n",
      "Epoch 3014, Loss: 0.07319187372922897, Final Batch Loss: 0.03491361811757088\n",
      "Epoch 3015, Loss: 0.09385016933083534, Final Batch Loss: 0.06103060767054558\n",
      "Epoch 3016, Loss: 0.1710517257452011, Final Batch Loss: 0.10687560588121414\n",
      "Epoch 3017, Loss: 0.07059775851666927, Final Batch Loss: 0.021941253915429115\n",
      "Epoch 3018, Loss: 0.11051179468631744, Final Batch Loss: 0.0480002835392952\n",
      "Epoch 3019, Loss: 0.07417178526520729, Final Batch Loss: 0.03784814849495888\n",
      "Epoch 3020, Loss: 0.10017909854650497, Final Batch Loss: 0.06955759227275848\n",
      "Epoch 3021, Loss: 0.09534210339188576, Final Batch Loss: 0.05920116603374481\n",
      "Epoch 3022, Loss: 0.09732671454548836, Final Batch Loss: 0.03319032862782478\n",
      "Epoch 3023, Loss: 0.10061240941286087, Final Batch Loss: 0.053152695298194885\n",
      "Epoch 3024, Loss: 0.11911085620522499, Final Batch Loss: 0.07391723245382309\n",
      "Epoch 3025, Loss: 0.09043339267373085, Final Batch Loss: 0.03488761931657791\n",
      "Epoch 3026, Loss: 0.09188492968678474, Final Batch Loss: 0.03759053722023964\n",
      "Epoch 3027, Loss: 0.07989618554711342, Final Batch Loss: 0.03490542992949486\n",
      "Epoch 3028, Loss: 0.11291912198066711, Final Batch Loss: 0.058840710669755936\n",
      "Epoch 3029, Loss: 0.10296924412250519, Final Batch Loss: 0.0680447369813919\n",
      "Epoch 3030, Loss: 0.14137756451964378, Final Batch Loss: 0.0434243343770504\n",
      "Epoch 3031, Loss: 0.221088707447052, Final Batch Loss: 0.11913269758224487\n",
      "Epoch 3032, Loss: 0.1727728322148323, Final Batch Loss: 0.08178015053272247\n",
      "Epoch 3033, Loss: 0.123386200517416, Final Batch Loss: 0.05222180113196373\n",
      "Epoch 3034, Loss: 0.12772154062986374, Final Batch Loss: 0.08022674173116684\n",
      "Epoch 3035, Loss: 0.11484763771295547, Final Batch Loss: 0.0744723305106163\n",
      "Epoch 3036, Loss: 0.0726875476539135, Final Batch Loss: 0.032610874623060226\n",
      "Epoch 3037, Loss: 0.11198049411177635, Final Batch Loss: 0.06543925404548645\n",
      "Epoch 3038, Loss: 0.1485390141606331, Final Batch Loss: 0.09283877909183502\n",
      "Epoch 3039, Loss: 0.13179701939225197, Final Batch Loss: 0.09108725935220718\n",
      "Epoch 3040, Loss: 0.09126286208629608, Final Batch Loss: 0.04922463372349739\n",
      "Epoch 3041, Loss: 0.09195385873317719, Final Batch Loss: 0.0494372695684433\n",
      "Epoch 3042, Loss: 0.07232767529785633, Final Batch Loss: 0.030680937692523003\n",
      "Epoch 3043, Loss: 0.11107965931296349, Final Batch Loss: 0.07265880703926086\n",
      "Epoch 3044, Loss: 0.09332120791077614, Final Batch Loss: 0.04840724542737007\n",
      "Epoch 3045, Loss: 0.0852990373969078, Final Batch Loss: 0.042673930525779724\n",
      "Epoch 3046, Loss: 0.09045008569955826, Final Batch Loss: 0.04679140821099281\n",
      "Epoch 3047, Loss: 0.08984988555312157, Final Batch Loss: 0.04975169152021408\n",
      "Epoch 3048, Loss: 0.08155662938952446, Final Batch Loss: 0.05310974642634392\n",
      "Epoch 3049, Loss: 0.10339929908514023, Final Batch Loss: 0.04112052917480469\n",
      "Epoch 3050, Loss: 0.09402615576982498, Final Batch Loss: 0.05211414769291878\n",
      "Epoch 3051, Loss: 0.08887224644422531, Final Batch Loss: 0.06242913380265236\n",
      "Epoch 3052, Loss: 0.10418776050209999, Final Batch Loss: 0.03502232953906059\n",
      "Epoch 3053, Loss: 0.10458656772971153, Final Batch Loss: 0.04039132222533226\n",
      "Epoch 3054, Loss: 0.08666788414120674, Final Batch Loss: 0.04727573320269585\n",
      "Epoch 3055, Loss: 0.11537496373057365, Final Batch Loss: 0.06506678462028503\n",
      "Epoch 3056, Loss: 0.09635772556066513, Final Batch Loss: 0.04295339062809944\n",
      "Epoch 3057, Loss: 0.10352621972560883, Final Batch Loss: 0.059932272881269455\n",
      "Epoch 3058, Loss: 0.09618674777448177, Final Batch Loss: 0.02624114416539669\n",
      "Epoch 3059, Loss: 0.1095036044716835, Final Batch Loss: 0.03601212054491043\n",
      "Epoch 3060, Loss: 0.1014745719730854, Final Batch Loss: 0.045375943183898926\n",
      "Epoch 3061, Loss: 0.09053115919232368, Final Batch Loss: 0.020731095224618912\n",
      "Epoch 3062, Loss: 0.08908523246645927, Final Batch Loss: 0.04438633844256401\n",
      "Epoch 3063, Loss: 0.07882930710911751, Final Batch Loss: 0.047956958413124084\n",
      "Epoch 3064, Loss: 0.08933921158313751, Final Batch Loss: 0.0452907532453537\n",
      "Epoch 3065, Loss: 0.08767851442098618, Final Batch Loss: 0.05053921043872833\n",
      "Epoch 3066, Loss: 0.10922453552484512, Final Batch Loss: 0.04643431305885315\n",
      "Epoch 3067, Loss: 0.09200078994035721, Final Batch Loss: 0.06841617822647095\n",
      "Epoch 3068, Loss: 0.1124415397644043, Final Batch Loss: 0.08017615228891373\n",
      "Epoch 3069, Loss: 0.13374624215066433, Final Batch Loss: 0.11053791642189026\n",
      "Epoch 3070, Loss: 0.13281776756048203, Final Batch Loss: 0.06471346318721771\n",
      "Epoch 3071, Loss: 0.08724947646260262, Final Batch Loss: 0.05865146592259407\n",
      "Epoch 3072, Loss: 0.09925048053264618, Final Batch Loss: 0.04633546620607376\n",
      "Epoch 3073, Loss: 0.1102529801428318, Final Batch Loss: 0.06003739684820175\n",
      "Epoch 3074, Loss: 0.09942195937037468, Final Batch Loss: 0.06391254812479019\n",
      "Epoch 3075, Loss: 0.09295009076595306, Final Batch Loss: 0.039765533059835434\n",
      "Epoch 3076, Loss: 0.09041636064648628, Final Batch Loss: 0.053959473967552185\n",
      "Epoch 3077, Loss: 0.10210264474153519, Final Batch Loss: 0.07421895861625671\n",
      "Epoch 3078, Loss: 0.08549569174647331, Final Batch Loss: 0.040321264415979385\n",
      "Epoch 3079, Loss: 0.1157866045832634, Final Batch Loss: 0.04753328859806061\n",
      "Epoch 3080, Loss: 0.11501269787549973, Final Batch Loss: 0.05054416507482529\n",
      "Epoch 3081, Loss: 0.11691373959183693, Final Batch Loss: 0.045357223600149155\n",
      "Epoch 3082, Loss: 0.07894651591777802, Final Batch Loss: 0.036322180181741714\n",
      "Epoch 3083, Loss: 0.08967209607362747, Final Batch Loss: 0.035260435193777084\n",
      "Epoch 3084, Loss: 0.0744249727576971, Final Batch Loss: 0.027324842289090157\n",
      "Epoch 3085, Loss: 0.1187436617910862, Final Batch Loss: 0.04243447259068489\n",
      "Epoch 3086, Loss: 0.10314737260341644, Final Batch Loss: 0.06788544356822968\n",
      "Epoch 3087, Loss: 0.08174172043800354, Final Batch Loss: 0.039042454212903976\n",
      "Epoch 3088, Loss: 0.11721020936965942, Final Batch Loss: 0.08593910932540894\n",
      "Epoch 3089, Loss: 0.10694864764809608, Final Batch Loss: 0.07127240300178528\n",
      "Epoch 3090, Loss: 0.07839558087289333, Final Batch Loss: 0.02900596149265766\n",
      "Epoch 3091, Loss: 0.1545632779598236, Final Batch Loss: 0.0401962548494339\n",
      "Epoch 3092, Loss: 0.06839198991656303, Final Batch Loss: 0.04068782925605774\n",
      "Epoch 3093, Loss: 0.08028186857700348, Final Batch Loss: 0.03611123934388161\n",
      "Epoch 3094, Loss: 0.10467156022787094, Final Batch Loss: 0.05915314331650734\n",
      "Epoch 3095, Loss: 0.0791848674416542, Final Batch Loss: 0.04167933017015457\n",
      "Epoch 3096, Loss: 0.09634016826748848, Final Batch Loss: 0.026454146951436996\n",
      "Epoch 3097, Loss: 0.06687494181096554, Final Batch Loss: 0.029135150834918022\n",
      "Epoch 3098, Loss: 0.07263483852148056, Final Batch Loss: 0.03988243639469147\n",
      "Epoch 3099, Loss: 0.08934653736650944, Final Batch Loss: 0.05851885303854942\n",
      "Epoch 3100, Loss: 0.07594913989305496, Final Batch Loss: 0.030296482145786285\n",
      "Epoch 3101, Loss: 0.0895959921181202, Final Batch Loss: 0.03815898299217224\n",
      "Epoch 3102, Loss: 0.10522114858031273, Final Batch Loss: 0.04716184362769127\n",
      "Epoch 3103, Loss: 0.09333376586437225, Final Batch Loss: 0.054413605481386185\n",
      "Epoch 3104, Loss: 0.10872772708535194, Final Batch Loss: 0.05287446826696396\n",
      "Epoch 3105, Loss: 0.13350895047187805, Final Batch Loss: 0.06449776142835617\n",
      "Epoch 3106, Loss: 0.09232629090547562, Final Batch Loss: 0.04693378508090973\n",
      "Epoch 3107, Loss: 0.10452822968363762, Final Batch Loss: 0.057988349348306656\n",
      "Epoch 3108, Loss: 0.0911017507314682, Final Batch Loss: 0.04928510636091232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3109, Loss: 0.1083914190530777, Final Batch Loss: 0.06247800961136818\n",
      "Epoch 3110, Loss: 0.10790452361106873, Final Batch Loss: 0.05368953198194504\n",
      "Epoch 3111, Loss: 0.11213847249746323, Final Batch Loss: 0.061942845582962036\n",
      "Epoch 3112, Loss: 0.1300856713205576, Final Batch Loss: 0.024319978430867195\n",
      "Epoch 3113, Loss: 0.10140666365623474, Final Batch Loss: 0.055015359073877335\n",
      "Epoch 3114, Loss: 0.20354609563946724, Final Batch Loss: 0.1512163132429123\n",
      "Epoch 3115, Loss: 0.09473102539777756, Final Batch Loss: 0.0597669743001461\n",
      "Epoch 3116, Loss: 0.11611029878258705, Final Batch Loss: 0.04602441564202309\n",
      "Epoch 3117, Loss: 0.082907535135746, Final Batch Loss: 0.05961218848824501\n",
      "Epoch 3118, Loss: 0.09051793068647385, Final Batch Loss: 0.032225288450717926\n",
      "Epoch 3119, Loss: 0.09108784049749374, Final Batch Loss: 0.05784285441040993\n",
      "Epoch 3120, Loss: 0.09344951063394547, Final Batch Loss: 0.04334414750337601\n",
      "Epoch 3121, Loss: 0.08594464138150215, Final Batch Loss: 0.032285477966070175\n",
      "Epoch 3122, Loss: 0.08810390532016754, Final Batch Loss: 0.047013215720653534\n",
      "Epoch 3123, Loss: 0.08351443707942963, Final Batch Loss: 0.0337376706302166\n",
      "Epoch 3124, Loss: 0.09501063823699951, Final Batch Loss: 0.05907319113612175\n",
      "Epoch 3125, Loss: 0.09615843370556831, Final Batch Loss: 0.037482891231775284\n",
      "Epoch 3126, Loss: 0.0771262738853693, Final Batch Loss: 0.030080458149313927\n",
      "Epoch 3127, Loss: 0.11828536167740822, Final Batch Loss: 0.035070691257715225\n",
      "Epoch 3128, Loss: 0.08126265183091164, Final Batch Loss: 0.05360671877861023\n",
      "Epoch 3129, Loss: 0.08079501241445541, Final Batch Loss: 0.0391753651201725\n",
      "Epoch 3130, Loss: 0.11754488945007324, Final Batch Loss: 0.06315118074417114\n",
      "Epoch 3131, Loss: 0.09150705486536026, Final Batch Loss: 0.026913709938526154\n",
      "Epoch 3132, Loss: 0.11813461594283581, Final Batch Loss: 0.023637285456061363\n",
      "Epoch 3133, Loss: 0.08157767355442047, Final Batch Loss: 0.04909152537584305\n",
      "Epoch 3134, Loss: 0.07855187729001045, Final Batch Loss: 0.04092726111412048\n",
      "Epoch 3135, Loss: 0.08968290686607361, Final Batch Loss: 0.03549128770828247\n",
      "Epoch 3136, Loss: 0.08131713327020407, Final Batch Loss: 0.014966933988034725\n",
      "Epoch 3137, Loss: 0.09397603198885918, Final Batch Loss: 0.03845064342021942\n",
      "Epoch 3138, Loss: 0.08425894007086754, Final Batch Loss: 0.0334281362593174\n",
      "Epoch 3139, Loss: 0.0942804180085659, Final Batch Loss: 0.039867617189884186\n",
      "Epoch 3140, Loss: 0.07278899103403091, Final Batch Loss: 0.04120803624391556\n",
      "Epoch 3141, Loss: 0.08562245219945908, Final Batch Loss: 0.0506727397441864\n",
      "Epoch 3142, Loss: 0.13506874814629555, Final Batch Loss: 0.03621464595198631\n",
      "Epoch 3143, Loss: 0.06035461649298668, Final Batch Loss: 0.024755310267210007\n",
      "Epoch 3144, Loss: 0.06829827558249235, Final Batch Loss: 0.013734339736402035\n",
      "Epoch 3145, Loss: 0.10136445239186287, Final Batch Loss: 0.05753658711910248\n",
      "Epoch 3146, Loss: 0.1464846394956112, Final Batch Loss: 0.0996214970946312\n",
      "Epoch 3147, Loss: 0.13228808343410492, Final Batch Loss: 0.03991418331861496\n",
      "Epoch 3148, Loss: 0.0823204256594181, Final Batch Loss: 0.05762651935219765\n",
      "Epoch 3149, Loss: 0.11641136184334755, Final Batch Loss: 0.06451740860939026\n",
      "Epoch 3150, Loss: 0.1170755885541439, Final Batch Loss: 0.04110425338149071\n",
      "Epoch 3151, Loss: 0.0848456434905529, Final Batch Loss: 0.04928450286388397\n",
      "Epoch 3152, Loss: 0.09267156943678856, Final Batch Loss: 0.028199922293424606\n",
      "Epoch 3153, Loss: 0.11922713369131088, Final Batch Loss: 0.070807084441185\n",
      "Epoch 3154, Loss: 0.10486753657460213, Final Batch Loss: 0.049796637147665024\n",
      "Epoch 3155, Loss: 0.13927076011896133, Final Batch Loss: 0.07480635493993759\n",
      "Epoch 3156, Loss: 0.10549014434218407, Final Batch Loss: 0.05963043496012688\n",
      "Epoch 3157, Loss: 0.13180110231041908, Final Batch Loss: 0.09791035205125809\n",
      "Epoch 3158, Loss: 0.11769146844744682, Final Batch Loss: 0.05183763429522514\n",
      "Epoch 3159, Loss: 0.10124476626515388, Final Batch Loss: 0.06555742025375366\n",
      "Epoch 3160, Loss: 0.07525258511304855, Final Batch Loss: 0.03603982925415039\n",
      "Epoch 3161, Loss: 0.10218673944473267, Final Batch Loss: 0.04606025293469429\n",
      "Epoch 3162, Loss: 0.1271429881453514, Final Batch Loss: 0.08663441240787506\n",
      "Epoch 3163, Loss: 0.0976414754986763, Final Batch Loss: 0.047215577214956284\n",
      "Epoch 3164, Loss: 0.06513526290655136, Final Batch Loss: 0.028936639428138733\n",
      "Epoch 3165, Loss: 0.1155608594417572, Final Batch Loss: 0.060241714119911194\n",
      "Epoch 3166, Loss: 0.1527981087565422, Final Batch Loss: 0.11996633559465408\n",
      "Epoch 3167, Loss: 0.07960337772965431, Final Batch Loss: 0.04113691672682762\n",
      "Epoch 3168, Loss: 0.11239664629101753, Final Batch Loss: 0.06578267365694046\n",
      "Epoch 3169, Loss: 0.07524800673127174, Final Batch Loss: 0.03288566693663597\n",
      "Epoch 3170, Loss: 0.072215985506773, Final Batch Loss: 0.03407863900065422\n",
      "Epoch 3171, Loss: 0.06838370487093925, Final Batch Loss: 0.04541124403476715\n",
      "Epoch 3172, Loss: 0.08565626665949821, Final Batch Loss: 0.04883778095245361\n",
      "Epoch 3173, Loss: 0.10452095419168472, Final Batch Loss: 0.03652215749025345\n",
      "Epoch 3174, Loss: 0.08957374468445778, Final Batch Loss: 0.05262547358870506\n",
      "Epoch 3175, Loss: 0.0741716381162405, Final Batch Loss: 0.027342190966010094\n",
      "Epoch 3176, Loss: 0.09664545953273773, Final Batch Loss: 0.054975029081106186\n",
      "Epoch 3177, Loss: 0.10851002857089043, Final Batch Loss: 0.040770407766103745\n",
      "Epoch 3178, Loss: 0.08035829663276672, Final Batch Loss: 0.04323083534836769\n",
      "Epoch 3179, Loss: 0.07755666971206665, Final Batch Loss: 0.03962353989481926\n",
      "Epoch 3180, Loss: 0.1034848652780056, Final Batch Loss: 0.0540250726044178\n",
      "Epoch 3181, Loss: 0.10063177160918713, Final Batch Loss: 0.0717657133936882\n",
      "Epoch 3182, Loss: 0.08578044176101685, Final Batch Loss: 0.0513884611427784\n",
      "Epoch 3183, Loss: 0.07665856368839741, Final Batch Loss: 0.025833530351519585\n",
      "Epoch 3184, Loss: 0.08378283679485321, Final Batch Loss: 0.03781447559595108\n",
      "Epoch 3185, Loss: 0.08274054527282715, Final Batch Loss: 0.04409734532237053\n",
      "Epoch 3186, Loss: 0.09627837687730789, Final Batch Loss: 0.05382532998919487\n",
      "Epoch 3187, Loss: 0.11899517476558685, Final Batch Loss: 0.040029242634773254\n",
      "Epoch 3188, Loss: 0.08650516159832478, Final Batch Loss: 0.023410411551594734\n",
      "Epoch 3189, Loss: 0.0677749291062355, Final Batch Loss: 0.034559693187475204\n",
      "Epoch 3190, Loss: 0.06693748198449612, Final Batch Loss: 0.02033703215420246\n",
      "Epoch 3191, Loss: 0.24595698341727257, Final Batch Loss: 0.21324224770069122\n",
      "Epoch 3192, Loss: 0.06505905836820602, Final Batch Loss: 0.03224163129925728\n",
      "Epoch 3193, Loss: 0.07686206325888634, Final Batch Loss: 0.041277699172496796\n",
      "Epoch 3194, Loss: 0.0830756425857544, Final Batch Loss: 0.042261816561222076\n",
      "Epoch 3195, Loss: 0.11070165038108826, Final Batch Loss: 0.06514660269021988\n",
      "Epoch 3196, Loss: 0.09515675529837608, Final Batch Loss: 0.05417068675160408\n",
      "Epoch 3197, Loss: 0.07103121466934681, Final Batch Loss: 0.03066600300371647\n",
      "Epoch 3198, Loss: 0.104727977886796, Final Batch Loss: 0.08158332109451294\n",
      "Epoch 3199, Loss: 0.0747830867767334, Final Batch Loss: 0.04726722463965416\n",
      "Epoch 3200, Loss: 0.09246527031064034, Final Batch Loss: 0.04292750731110573\n",
      "Epoch 3201, Loss: 0.0668578390032053, Final Batch Loss: 0.03700225427746773\n",
      "Epoch 3202, Loss: 0.0643398892134428, Final Batch Loss: 0.026386400684714317\n",
      "Epoch 3203, Loss: 0.11755887418985367, Final Batch Loss: 0.0708453506231308\n",
      "Epoch 3204, Loss: 0.13811495900154114, Final Batch Loss: 0.06714461743831635\n",
      "Epoch 3205, Loss: 0.07090274803340435, Final Batch Loss: 0.020768718793988228\n",
      "Epoch 3206, Loss: 0.09247352182865143, Final Batch Loss: 0.06588319689035416\n",
      "Epoch 3207, Loss: 0.08543622493743896, Final Batch Loss: 0.05591905117034912\n",
      "Epoch 3208, Loss: 0.10722922720015049, Final Batch Loss: 0.025272058323025703\n",
      "Epoch 3209, Loss: 0.07373614236712456, Final Batch Loss: 0.03455619141459465\n",
      "Epoch 3210, Loss: 0.09732386469841003, Final Batch Loss: 0.043196551501750946\n",
      "Epoch 3211, Loss: 0.11455004662275314, Final Batch Loss: 0.0741719901561737\n",
      "Epoch 3212, Loss: 0.09171833656728268, Final Batch Loss: 0.028172342106699944\n",
      "Epoch 3213, Loss: 0.07801607996225357, Final Batch Loss: 0.049851901829242706\n",
      "Epoch 3214, Loss: 0.08182377368211746, Final Batch Loss: 0.03460796922445297\n",
      "Epoch 3215, Loss: 0.06387137621641159, Final Batch Loss: 0.031083397567272186\n",
      "Epoch 3216, Loss: 0.08613841235637665, Final Batch Loss: 0.04356260970234871\n",
      "Epoch 3217, Loss: 0.10846730321645737, Final Batch Loss: 0.045300811529159546\n",
      "Epoch 3218, Loss: 0.07292910479009151, Final Batch Loss: 0.0421924851834774\n",
      "Epoch 3219, Loss: 0.081367127597332, Final Batch Loss: 0.034977030009031296\n",
      "Epoch 3220, Loss: 0.09541817381978035, Final Batch Loss: 0.06431467086076736\n",
      "Epoch 3221, Loss: 0.059799566864967346, Final Batch Loss: 0.03733184561133385\n",
      "Epoch 3222, Loss: 0.10206785798072815, Final Batch Loss: 0.07073058187961578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3223, Loss: 0.06800150871276855, Final Batch Loss: 0.03861289471387863\n",
      "Epoch 3224, Loss: 0.059610024094581604, Final Batch Loss: 0.027145162224769592\n",
      "Epoch 3225, Loss: 0.060245001688599586, Final Batch Loss: 0.02198622189462185\n",
      "Epoch 3226, Loss: 0.08108596876263618, Final Batch Loss: 0.05068200081586838\n",
      "Epoch 3227, Loss: 0.08252657949924469, Final Batch Loss: 0.05056999623775482\n",
      "Epoch 3228, Loss: 0.06729737669229507, Final Batch Loss: 0.032030992209911346\n",
      "Epoch 3229, Loss: 0.08137518167495728, Final Batch Loss: 0.039651207625865936\n",
      "Epoch 3230, Loss: 0.08187542110681534, Final Batch Loss: 0.013440363109111786\n",
      "Epoch 3231, Loss: 0.07646509259939194, Final Batch Loss: 0.03291000425815582\n",
      "Epoch 3232, Loss: 0.08624303340911865, Final Batch Loss: 0.03767652064561844\n",
      "Epoch 3233, Loss: 0.0732027105987072, Final Batch Loss: 0.04065718129277229\n",
      "Epoch 3234, Loss: 0.09132513403892517, Final Batch Loss: 0.033677130937576294\n",
      "Epoch 3235, Loss: 0.07623064145445824, Final Batch Loss: 0.0417352132499218\n",
      "Epoch 3236, Loss: 0.06691551953554153, Final Batch Loss: 0.03391405940055847\n",
      "Epoch 3237, Loss: 0.09266060218214989, Final Batch Loss: 0.039370469748973846\n",
      "Epoch 3238, Loss: 0.052048830315470695, Final Batch Loss: 0.01584194414317608\n",
      "Epoch 3239, Loss: 0.06368442438542843, Final Batch Loss: 0.03325413912534714\n",
      "Epoch 3240, Loss: 0.07468597963452339, Final Batch Loss: 0.03215869888663292\n",
      "Epoch 3241, Loss: 0.08109673857688904, Final Batch Loss: 0.040965836495161057\n",
      "Epoch 3242, Loss: 0.0713271014392376, Final Batch Loss: 0.03154194727540016\n",
      "Epoch 3243, Loss: 0.0696807187050581, Final Batch Loss: 0.02336825244128704\n",
      "Epoch 3244, Loss: 0.06605050899088383, Final Batch Loss: 0.024028899148106575\n",
      "Epoch 3245, Loss: 0.09954709559679031, Final Batch Loss: 0.049324147403240204\n",
      "Epoch 3246, Loss: 0.07858415320515633, Final Batch Loss: 0.03477614000439644\n",
      "Epoch 3247, Loss: 0.10288690030574799, Final Batch Loss: 0.034650497138500214\n",
      "Epoch 3248, Loss: 0.10189925506711006, Final Batch Loss: 0.05079307034611702\n",
      "Epoch 3249, Loss: 0.08413256332278252, Final Batch Loss: 0.05263718590140343\n",
      "Epoch 3250, Loss: 0.08878487534821033, Final Batch Loss: 0.029236113652586937\n",
      "Epoch 3251, Loss: 0.07452866807579994, Final Batch Loss: 0.03312423825263977\n",
      "Epoch 3252, Loss: 0.06667179241776466, Final Batch Loss: 0.03146422654390335\n",
      "Epoch 3253, Loss: 0.0791797824203968, Final Batch Loss: 0.03455062210559845\n",
      "Epoch 3254, Loss: 0.09536339715123177, Final Batch Loss: 0.03388236463069916\n",
      "Epoch 3255, Loss: 0.09323996305465698, Final Batch Loss: 0.05203951895236969\n",
      "Epoch 3256, Loss: 0.08645256981253624, Final Batch Loss: 0.03971853479743004\n",
      "Epoch 3257, Loss: 0.09019756689667702, Final Batch Loss: 0.04188545420765877\n",
      "Epoch 3258, Loss: 0.07854008302092552, Final Batch Loss: 0.035838596522808075\n",
      "Epoch 3259, Loss: 0.07127595879137516, Final Batch Loss: 0.025720907375216484\n",
      "Epoch 3260, Loss: 0.07291208952665329, Final Batch Loss: 0.037334177643060684\n",
      "Epoch 3261, Loss: 0.09476187825202942, Final Batch Loss: 0.055877551436424255\n",
      "Epoch 3262, Loss: 0.103736013174057, Final Batch Loss: 0.07569808512926102\n",
      "Epoch 3263, Loss: 0.062221039086580276, Final Batch Loss: 0.028414256870746613\n",
      "Epoch 3264, Loss: 0.07606163620948792, Final Batch Loss: 0.021764837205410004\n",
      "Epoch 3265, Loss: 0.07813972979784012, Final Batch Loss: 0.04304743558168411\n",
      "Epoch 3266, Loss: 0.05292479693889618, Final Batch Loss: 0.020955003798007965\n",
      "Epoch 3267, Loss: 0.07890155538916588, Final Batch Loss: 0.035076577216386795\n",
      "Epoch 3268, Loss: 0.09285122156143188, Final Batch Loss: 0.050219230353832245\n",
      "Epoch 3269, Loss: 0.07926206290721893, Final Batch Loss: 0.04179948940873146\n",
      "Epoch 3270, Loss: 0.08127981796860695, Final Batch Loss: 0.06329143047332764\n",
      "Epoch 3271, Loss: 0.06359494663774967, Final Batch Loss: 0.018089240416884422\n",
      "Epoch 3272, Loss: 0.09698276221752167, Final Batch Loss: 0.035018112510442734\n",
      "Epoch 3273, Loss: 0.07478694431483746, Final Batch Loss: 0.05145068094134331\n",
      "Epoch 3274, Loss: 0.0805080272257328, Final Batch Loss: 0.0204954594373703\n",
      "Epoch 3275, Loss: 0.10802463069558144, Final Batch Loss: 0.061083946377038956\n",
      "Epoch 3276, Loss: 0.09389922767877579, Final Batch Loss: 0.050414521247148514\n",
      "Epoch 3277, Loss: 0.07968192920088768, Final Batch Loss: 0.0335250198841095\n",
      "Epoch 3278, Loss: 0.1302494816482067, Final Batch Loss: 0.0711020901799202\n",
      "Epoch 3279, Loss: 0.1121884398162365, Final Batch Loss: 0.06377040594816208\n",
      "Epoch 3280, Loss: 0.08560997620224953, Final Batch Loss: 0.05003774166107178\n",
      "Epoch 3281, Loss: 0.07797008194029331, Final Batch Loss: 0.05332372710108757\n",
      "Epoch 3282, Loss: 0.08284550905227661, Final Batch Loss: 0.04167957603931427\n",
      "Epoch 3283, Loss: 0.11642395332455635, Final Batch Loss: 0.0728214904665947\n",
      "Epoch 3284, Loss: 0.06428097561001778, Final Batch Loss: 0.0345756858587265\n",
      "Epoch 3285, Loss: 0.07835081592202187, Final Batch Loss: 0.04235505312681198\n",
      "Epoch 3286, Loss: 0.07729366980493069, Final Batch Loss: 0.017711607739329338\n",
      "Epoch 3287, Loss: 0.07239512540400028, Final Batch Loss: 0.026085248216986656\n",
      "Epoch 3288, Loss: 0.07496356405317783, Final Batch Loss: 0.02564879320561886\n",
      "Epoch 3289, Loss: 0.09968617558479309, Final Batch Loss: 0.051669035106897354\n",
      "Epoch 3290, Loss: 0.0844856146723032, Final Batch Loss: 0.030234241858124733\n",
      "Epoch 3291, Loss: 0.10190802812576294, Final Batch Loss: 0.05103516951203346\n",
      "Epoch 3292, Loss: 0.07319471426308155, Final Batch Loss: 0.028816958889365196\n",
      "Epoch 3293, Loss: 0.11613060906529427, Final Batch Loss: 0.0463981069624424\n",
      "Epoch 3294, Loss: 0.07285166159272194, Final Batch Loss: 0.03978960961103439\n",
      "Epoch 3295, Loss: 0.09628335013985634, Final Batch Loss: 0.05103437975049019\n",
      "Epoch 3296, Loss: 0.061117397621273994, Final Batch Loss: 0.030621115118265152\n",
      "Epoch 3297, Loss: 0.07972003892064095, Final Batch Loss: 0.048294033855199814\n",
      "Epoch 3298, Loss: 0.0789827685803175, Final Batch Loss: 0.05066327750682831\n",
      "Epoch 3299, Loss: 0.06746948137879372, Final Batch Loss: 0.03835506737232208\n",
      "Epoch 3300, Loss: 0.07689999788999557, Final Batch Loss: 0.0432402566075325\n",
      "Epoch 3301, Loss: 0.08915408700704575, Final Batch Loss: 0.040079738944768906\n",
      "Epoch 3302, Loss: 0.07581803575158119, Final Batch Loss: 0.027051206678152084\n",
      "Epoch 3303, Loss: 0.0717256460338831, Final Batch Loss: 0.04861138015985489\n",
      "Epoch 3304, Loss: 0.06758972257375717, Final Batch Loss: 0.021576248109340668\n",
      "Epoch 3305, Loss: 0.1114046685397625, Final Batch Loss: 0.031960379332304\n",
      "Epoch 3306, Loss: 0.09081536903977394, Final Batch Loss: 0.052539777010679245\n",
      "Epoch 3307, Loss: 0.0672106221318245, Final Batch Loss: 0.03870737552642822\n",
      "Epoch 3308, Loss: 0.12936676666140556, Final Batch Loss: 0.061600442975759506\n",
      "Epoch 3309, Loss: 0.08694849163293839, Final Batch Loss: 0.04488523676991463\n",
      "Epoch 3310, Loss: 0.0712774321436882, Final Batch Loss: 0.028972424566745758\n",
      "Epoch 3311, Loss: 0.08754867687821388, Final Batch Loss: 0.0512971356511116\n",
      "Epoch 3312, Loss: 0.08213803172111511, Final Batch Loss: 0.04518333077430725\n",
      "Epoch 3313, Loss: 0.06640230491757393, Final Batch Loss: 0.034424345940351486\n",
      "Epoch 3314, Loss: 0.08945245295763016, Final Batch Loss: 0.053725942969322205\n",
      "Epoch 3315, Loss: 0.0657848659902811, Final Batch Loss: 0.038645222783088684\n",
      "Epoch 3316, Loss: 0.07414655387401581, Final Batch Loss: 0.035524480044841766\n",
      "Epoch 3317, Loss: 0.06350648403167725, Final Batch Loss: 0.03222412243485451\n",
      "Epoch 3318, Loss: 0.10538825020194054, Final Batch Loss: 0.06722022593021393\n",
      "Epoch 3319, Loss: 0.07483930140733719, Final Batch Loss: 0.03204081580042839\n",
      "Epoch 3320, Loss: 0.10154391080141068, Final Batch Loss: 0.037484318017959595\n",
      "Epoch 3321, Loss: 0.08370871096849442, Final Batch Loss: 0.033294886350631714\n",
      "Epoch 3322, Loss: 0.18203163146972656, Final Batch Loss: 0.08681297302246094\n",
      "Epoch 3323, Loss: 0.0659205075353384, Final Batch Loss: 0.0426207110285759\n",
      "Epoch 3324, Loss: 0.2460402399301529, Final Batch Loss: 0.18166427314281464\n",
      "Epoch 3325, Loss: 0.12225036695599556, Final Batch Loss: 0.05643746629357338\n",
      "Epoch 3326, Loss: 0.1643492579460144, Final Batch Loss: 0.08728565275669098\n",
      "Epoch 3327, Loss: 0.17862499505281448, Final Batch Loss: 0.10226038843393326\n",
      "Epoch 3328, Loss: 0.1581946350634098, Final Batch Loss: 0.05700809136033058\n",
      "Epoch 3329, Loss: 0.15539756789803505, Final Batch Loss: 0.10485445708036423\n",
      "Epoch 3330, Loss: 0.10973310843110085, Final Batch Loss: 0.05409977212548256\n",
      "Epoch 3331, Loss: 0.09595512226223946, Final Batch Loss: 0.038317885249853134\n",
      "Epoch 3332, Loss: 0.09496354684233665, Final Batch Loss: 0.05325291305780411\n",
      "Epoch 3333, Loss: 0.08757547661662102, Final Batch Loss: 0.0425981841981411\n",
      "Epoch 3334, Loss: 0.10285383649170399, Final Batch Loss: 0.029898108914494514\n",
      "Epoch 3335, Loss: 0.1331564523279667, Final Batch Loss: 0.07961960136890411\n",
      "Epoch 3336, Loss: 0.11096121370792389, Final Batch Loss: 0.04991398751735687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3337, Loss: 0.13494278118014336, Final Batch Loss: 0.08015765249729156\n",
      "Epoch 3338, Loss: 0.09820449911057949, Final Batch Loss: 0.03050672821700573\n",
      "Epoch 3339, Loss: 0.11012478545308113, Final Batch Loss: 0.024611350148916245\n",
      "Epoch 3340, Loss: 0.10436009243130684, Final Batch Loss: 0.05244921147823334\n",
      "Epoch 3341, Loss: 0.12659132108092308, Final Batch Loss: 0.06528693437576294\n",
      "Epoch 3342, Loss: 0.09506146609783173, Final Batch Loss: 0.041364021599292755\n",
      "Epoch 3343, Loss: 0.10389254800975323, Final Batch Loss: 0.01959487609565258\n",
      "Epoch 3344, Loss: 0.07088937610387802, Final Batch Loss: 0.03435707837343216\n",
      "Epoch 3345, Loss: 0.08182273805141449, Final Batch Loss: 0.045505862683057785\n",
      "Epoch 3346, Loss: 0.08436878025531769, Final Batch Loss: 0.048339419066905975\n",
      "Epoch 3347, Loss: 0.06940374709665775, Final Batch Loss: 0.041025929152965546\n",
      "Epoch 3348, Loss: 0.07696830108761787, Final Batch Loss: 0.0412527471780777\n",
      "Epoch 3349, Loss: 0.11928524821996689, Final Batch Loss: 0.04077302664518356\n",
      "Epoch 3350, Loss: 0.11552739515900612, Final Batch Loss: 0.05144358053803444\n",
      "Epoch 3351, Loss: 0.06820903159677982, Final Batch Loss: 0.04874003678560257\n",
      "Epoch 3352, Loss: 0.0917687714099884, Final Batch Loss: 0.051876001060009\n",
      "Epoch 3353, Loss: 0.11640788242220879, Final Batch Loss: 0.0736052468419075\n",
      "Epoch 3354, Loss: 0.09569663554430008, Final Batch Loss: 0.061792951077222824\n",
      "Epoch 3355, Loss: 0.11762591823935509, Final Batch Loss: 0.07679251581430435\n",
      "Epoch 3356, Loss: 0.07100996002554893, Final Batch Loss: 0.035110779106616974\n",
      "Epoch 3357, Loss: 0.09795159101486206, Final Batch Loss: 0.055428870022296906\n",
      "Epoch 3358, Loss: 0.11016906797885895, Final Batch Loss: 0.04302780330181122\n",
      "Epoch 3359, Loss: 0.10031453520059586, Final Batch Loss: 0.03582250326871872\n",
      "Epoch 3360, Loss: 0.11427142843604088, Final Batch Loss: 0.09049045294523239\n",
      "Epoch 3361, Loss: 0.13912110403180122, Final Batch Loss: 0.08980842679738998\n",
      "Epoch 3362, Loss: 0.09653742983937263, Final Batch Loss: 0.04470283165574074\n",
      "Epoch 3363, Loss: 0.3117132931947708, Final Batch Loss: 0.23345190286636353\n",
      "Epoch 3364, Loss: 0.10362411290407181, Final Batch Loss: 0.05059931054711342\n",
      "Epoch 3365, Loss: 0.11629968136548996, Final Batch Loss: 0.06525605916976929\n",
      "Epoch 3366, Loss: 0.1258922815322876, Final Batch Loss: 0.01760956645011902\n",
      "Epoch 3367, Loss: 0.12215937301516533, Final Batch Loss: 0.0669807493686676\n",
      "Epoch 3368, Loss: 0.0716291218996048, Final Batch Loss: 0.03236186131834984\n",
      "Epoch 3369, Loss: 0.07953188568353653, Final Batch Loss: 0.029531188309192657\n",
      "Epoch 3370, Loss: 0.08112136460840702, Final Batch Loss: 0.025947848334908485\n",
      "Epoch 3371, Loss: 0.07367754355072975, Final Batch Loss: 0.032921202480793\n",
      "Epoch 3372, Loss: 0.08811340481042862, Final Batch Loss: 0.04279443249106407\n",
      "Epoch 3373, Loss: 0.09350410476326942, Final Batch Loss: 0.04314997419714928\n",
      "Epoch 3374, Loss: 0.0847175121307373, Final Batch Loss: 0.04284823313355446\n",
      "Epoch 3375, Loss: 0.05614169500768185, Final Batch Loss: 0.030108988285064697\n",
      "Epoch 3376, Loss: 0.10663620010018349, Final Batch Loss: 0.05244751647114754\n",
      "Epoch 3377, Loss: 0.08825423195958138, Final Batch Loss: 0.05255523696541786\n",
      "Epoch 3378, Loss: 0.0873667411506176, Final Batch Loss: 0.04043594375252724\n",
      "Epoch 3379, Loss: 0.08185622841119766, Final Batch Loss: 0.03499505668878555\n",
      "Epoch 3380, Loss: 0.1789989061653614, Final Batch Loss: 0.11901947855949402\n",
      "Epoch 3381, Loss: 0.09215123951435089, Final Batch Loss: 0.055027611553668976\n",
      "Epoch 3382, Loss: 0.08026451617479324, Final Batch Loss: 0.030470911413431168\n",
      "Epoch 3383, Loss: 0.11547477543354034, Final Batch Loss: 0.06770090013742447\n",
      "Epoch 3384, Loss: 0.08356350660324097, Final Batch Loss: 0.05507299676537514\n",
      "Epoch 3385, Loss: 0.16907672584056854, Final Batch Loss: 0.09319623559713364\n",
      "Epoch 3386, Loss: 0.11074217781424522, Final Batch Loss: 0.06580330431461334\n",
      "Epoch 3387, Loss: 0.07274120673537254, Final Batch Loss: 0.036600131541490555\n",
      "Epoch 3388, Loss: 0.11080844327807426, Final Batch Loss: 0.06960586458444595\n",
      "Epoch 3389, Loss: 0.1064951978623867, Final Batch Loss: 0.07186347991228104\n",
      "Epoch 3390, Loss: 0.09244636632502079, Final Batch Loss: 0.06291848421096802\n",
      "Epoch 3391, Loss: 0.0722353532910347, Final Batch Loss: 0.040971074253320694\n",
      "Epoch 3392, Loss: 0.06413431093096733, Final Batch Loss: 0.029411502182483673\n",
      "Epoch 3393, Loss: 0.12869417667388916, Final Batch Loss: 0.06311115622520447\n",
      "Epoch 3394, Loss: 0.11080396175384521, Final Batch Loss: 0.07323485612869263\n",
      "Epoch 3395, Loss: 0.1107895877212286, Final Batch Loss: 0.02988898567855358\n",
      "Epoch 3396, Loss: 0.0894000269472599, Final Batch Loss: 0.05482902750372887\n",
      "Epoch 3397, Loss: 0.0929337590932846, Final Batch Loss: 0.03446744382381439\n",
      "Epoch 3398, Loss: 0.11424389481544495, Final Batch Loss: 0.0566534660756588\n",
      "Epoch 3399, Loss: 0.08961711823940277, Final Batch Loss: 0.051755357533693314\n",
      "Epoch 3400, Loss: 0.10508796200156212, Final Batch Loss: 0.048737864941358566\n",
      "Epoch 3401, Loss: 0.12173276767134666, Final Batch Loss: 0.07507871836423874\n",
      "Epoch 3402, Loss: 0.09182615205645561, Final Batch Loss: 0.0559430830180645\n",
      "Epoch 3403, Loss: 0.08909522742033005, Final Batch Loss: 0.049859728664159775\n",
      "Epoch 3404, Loss: 0.08596731722354889, Final Batch Loss: 0.04503900185227394\n",
      "Epoch 3405, Loss: 0.14858528226613998, Final Batch Loss: 0.029386863112449646\n",
      "Epoch 3406, Loss: 0.11715517938137054, Final Batch Loss: 0.029515467584133148\n",
      "Epoch 3407, Loss: 0.08511722087860107, Final Batch Loss: 0.039374884217977524\n",
      "Epoch 3408, Loss: 0.1169288158416748, Final Batch Loss: 0.04879099875688553\n",
      "Epoch 3409, Loss: 0.1297813169658184, Final Batch Loss: 0.09618767350912094\n",
      "Epoch 3410, Loss: 0.10936778783798218, Final Batch Loss: 0.04186191409826279\n",
      "Epoch 3411, Loss: 0.08124559372663498, Final Batch Loss: 0.044906023889780045\n",
      "Epoch 3412, Loss: 0.10767187550663948, Final Batch Loss: 0.05276032164692879\n",
      "Epoch 3413, Loss: 0.08685202151536942, Final Batch Loss: 0.03196248039603233\n",
      "Epoch 3414, Loss: 0.07648970931768417, Final Batch Loss: 0.0361962616443634\n",
      "Epoch 3415, Loss: 0.11745086684823036, Final Batch Loss: 0.05373978242278099\n",
      "Epoch 3416, Loss: 0.07480527088046074, Final Batch Loss: 0.03368634358048439\n",
      "Epoch 3417, Loss: 0.0799819566309452, Final Batch Loss: 0.035884395241737366\n",
      "Epoch 3418, Loss: 0.07409290969371796, Final Batch Loss: 0.04973164573311806\n",
      "Epoch 3419, Loss: 0.10372407734394073, Final Batch Loss: 0.05941999331116676\n",
      "Epoch 3420, Loss: 0.08558552525937557, Final Batch Loss: 0.05536960810422897\n",
      "Epoch 3421, Loss: 0.10335317067801952, Final Batch Loss: 0.0742034986615181\n",
      "Epoch 3422, Loss: 0.12154218554496765, Final Batch Loss: 0.08469258248806\n",
      "Epoch 3423, Loss: 0.07522333040833473, Final Batch Loss: 0.04056017845869064\n",
      "Epoch 3424, Loss: 0.07780177891254425, Final Batch Loss: 0.030291985720396042\n",
      "Epoch 3425, Loss: 0.0760083720088005, Final Batch Loss: 0.040005479007959366\n",
      "Epoch 3426, Loss: 0.08652443438768387, Final Batch Loss: 0.04851168766617775\n",
      "Epoch 3427, Loss: 0.08321498706936836, Final Batch Loss: 0.03974025696516037\n",
      "Epoch 3428, Loss: 0.08671664446592331, Final Batch Loss: 0.051331110298633575\n",
      "Epoch 3429, Loss: 0.08217399567365646, Final Batch Loss: 0.034828051924705505\n",
      "Epoch 3430, Loss: 0.09044187888503075, Final Batch Loss: 0.04627205803990364\n",
      "Epoch 3431, Loss: 0.08475301042199135, Final Batch Loss: 0.049038514494895935\n",
      "Epoch 3432, Loss: 0.09407882764935493, Final Batch Loss: 0.05328403413295746\n",
      "Epoch 3433, Loss: 0.08395217172801495, Final Batch Loss: 0.0528043657541275\n",
      "Epoch 3434, Loss: 0.10336145758628845, Final Batch Loss: 0.05611647665500641\n",
      "Epoch 3435, Loss: 0.08486569300293922, Final Batch Loss: 0.052152521908283234\n",
      "Epoch 3436, Loss: 0.08028564974665642, Final Batch Loss: 0.04596057906746864\n",
      "Epoch 3437, Loss: 0.08989987522363663, Final Batch Loss: 0.058150578290224075\n",
      "Epoch 3438, Loss: 0.1095207966864109, Final Batch Loss: 0.05557966232299805\n",
      "Epoch 3439, Loss: 0.0711384117603302, Final Batch Loss: 0.04084908962249756\n",
      "Epoch 3440, Loss: 0.10459281876683235, Final Batch Loss: 0.04017623886466026\n",
      "Epoch 3441, Loss: 0.08928883075714111, Final Batch Loss: 0.05718165263533592\n",
      "Epoch 3442, Loss: 0.10237676277756691, Final Batch Loss: 0.061632007360458374\n",
      "Epoch 3443, Loss: 0.188871867954731, Final Batch Loss: 0.07284411042928696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3444, Loss: 0.06178744323551655, Final Batch Loss: 0.023827174678444862\n",
      "Epoch 3445, Loss: 0.086425069719553, Final Batch Loss: 0.03657810389995575\n",
      "Epoch 3446, Loss: 0.09062530472874641, Final Batch Loss: 0.055937375873327255\n",
      "Epoch 3447, Loss: 0.08196450397372246, Final Batch Loss: 0.046098921447992325\n",
      "Epoch 3448, Loss: 0.12594671919941902, Final Batch Loss: 0.05280742421746254\n",
      "Epoch 3449, Loss: 0.08365675434470177, Final Batch Loss: 0.04511551931500435\n",
      "Epoch 3450, Loss: 0.09215760417282581, Final Batch Loss: 0.02730448730289936\n",
      "Epoch 3451, Loss: 0.06888354197144508, Final Batch Loss: 0.03293238580226898\n",
      "Epoch 3452, Loss: 0.0725831389427185, Final Batch Loss: 0.031291697174310684\n",
      "Epoch 3453, Loss: 0.07684990018606186, Final Batch Loss: 0.0339612178504467\n",
      "Epoch 3454, Loss: 0.09331326186656952, Final Batch Loss: 0.044576842337846756\n",
      "Epoch 3455, Loss: 0.07574455440044403, Final Batch Loss: 0.04126061871647835\n",
      "Epoch 3456, Loss: 0.08485511131584644, Final Batch Loss: 0.024004148319363594\n",
      "Epoch 3457, Loss: 0.07596486061811447, Final Batch Loss: 0.035460229963064194\n",
      "Epoch 3458, Loss: 0.08473607897758484, Final Batch Loss: 0.04895172640681267\n",
      "Epoch 3459, Loss: 0.0589017141610384, Final Batch Loss: 0.03141351416707039\n",
      "Epoch 3460, Loss: 0.08482393994927406, Final Batch Loss: 0.04796452820301056\n",
      "Epoch 3461, Loss: 0.07332278601825237, Final Batch Loss: 0.05616549029946327\n",
      "Epoch 3462, Loss: 0.08080034703016281, Final Batch Loss: 0.041983336210250854\n",
      "Epoch 3463, Loss: 0.0907089151442051, Final Batch Loss: 0.04851550981402397\n",
      "Epoch 3464, Loss: 0.0848902277648449, Final Batch Loss: 0.04031439498066902\n",
      "Epoch 3465, Loss: 0.0738799013197422, Final Batch Loss: 0.04144936427474022\n",
      "Epoch 3466, Loss: 0.08695066347718239, Final Batch Loss: 0.052587658166885376\n",
      "Epoch 3467, Loss: 0.1173679530620575, Final Batch Loss: 0.06894557923078537\n",
      "Epoch 3468, Loss: 0.0879794005304575, Final Batch Loss: 0.027438806369900703\n",
      "Epoch 3469, Loss: 0.07601001858711243, Final Batch Loss: 0.03196399658918381\n",
      "Epoch 3470, Loss: 0.09345719590783119, Final Batch Loss: 0.05379801243543625\n",
      "Epoch 3471, Loss: 0.07551177963614464, Final Batch Loss: 0.03902584686875343\n",
      "Epoch 3472, Loss: 0.084872305393219, Final Batch Loss: 0.04941628873348236\n",
      "Epoch 3473, Loss: 0.08611425757408142, Final Batch Loss: 0.020827636122703552\n",
      "Epoch 3474, Loss: 0.06988490372896194, Final Batch Loss: 0.04302994906902313\n",
      "Epoch 3475, Loss: 0.0699802115559578, Final Batch Loss: 0.035866186022758484\n",
      "Epoch 3476, Loss: 0.10858574509620667, Final Batch Loss: 0.0637018010020256\n",
      "Epoch 3477, Loss: 0.09271842241287231, Final Batch Loss: 0.03481072559952736\n",
      "Epoch 3478, Loss: 0.07659504562616348, Final Batch Loss: 0.040685683488845825\n",
      "Epoch 3479, Loss: 0.10290799662470818, Final Batch Loss: 0.059628963470458984\n",
      "Epoch 3480, Loss: 0.09839643351733685, Final Batch Loss: 0.0700761005282402\n",
      "Epoch 3481, Loss: 0.06936641968786716, Final Batch Loss: 0.029662756249308586\n",
      "Epoch 3482, Loss: 0.053275126963853836, Final Batch Loss: 0.022477181628346443\n",
      "Epoch 3483, Loss: 0.06900499947369099, Final Batch Loss: 0.04536588117480278\n",
      "Epoch 3484, Loss: 0.07476107962429523, Final Batch Loss: 0.031233707442879677\n",
      "Epoch 3485, Loss: 0.08156572096049786, Final Batch Loss: 0.052954476326704025\n",
      "Epoch 3486, Loss: 0.09585432708263397, Final Batch Loss: 0.03542563319206238\n",
      "Epoch 3487, Loss: 0.11634426563978195, Final Batch Loss: 0.05045855790376663\n",
      "Epoch 3488, Loss: 0.08399665355682373, Final Batch Loss: 0.030405953526496887\n",
      "Epoch 3489, Loss: 0.0755053199827671, Final Batch Loss: 0.039758019149303436\n",
      "Epoch 3490, Loss: 0.06858599558472633, Final Batch Loss: 0.03595524653792381\n",
      "Epoch 3491, Loss: 0.07765449583530426, Final Batch Loss: 0.029748443514108658\n",
      "Epoch 3492, Loss: 0.08242721855640411, Final Batch Loss: 0.04708750173449516\n",
      "Epoch 3493, Loss: 0.07188737392425537, Final Batch Loss: 0.04420172795653343\n",
      "Epoch 3494, Loss: 0.07030974328517914, Final Batch Loss: 0.03395324572920799\n",
      "Epoch 3495, Loss: 0.08019738644361496, Final Batch Loss: 0.04196098819375038\n",
      "Epoch 3496, Loss: 0.08565298467874527, Final Batch Loss: 0.041632357984781265\n",
      "Epoch 3497, Loss: 0.06681237928569317, Final Batch Loss: 0.039865799248218536\n",
      "Epoch 3498, Loss: 0.08389632031321526, Final Batch Loss: 0.037889715284109116\n",
      "Epoch 3499, Loss: 0.06802584417164326, Final Batch Loss: 0.023040881380438805\n",
      "Epoch 3500, Loss: 0.0853077694773674, Final Batch Loss: 0.0372285358607769\n",
      "Epoch 3501, Loss: 0.08730386570096016, Final Batch Loss: 0.04945671558380127\n",
      "Epoch 3502, Loss: 0.08696376159787178, Final Batch Loss: 0.04870869964361191\n",
      "Epoch 3503, Loss: 0.07857035100460052, Final Batch Loss: 0.04329727590084076\n",
      "Epoch 3504, Loss: 0.07162259705364704, Final Batch Loss: 0.04434513673186302\n",
      "Epoch 3505, Loss: 0.1050335131585598, Final Batch Loss: 0.06284848600625992\n",
      "Epoch 3506, Loss: 0.10152068175375462, Final Batch Loss: 0.0793764516711235\n",
      "Epoch 3507, Loss: 0.08549856022000313, Final Batch Loss: 0.043212536722421646\n",
      "Epoch 3508, Loss: 0.07712966576218605, Final Batch Loss: 0.03650369867682457\n",
      "Epoch 3509, Loss: 0.08027921989560127, Final Batch Loss: 0.04828029125928879\n",
      "Epoch 3510, Loss: 0.14302297681570053, Final Batch Loss: 0.03172723203897476\n",
      "Epoch 3511, Loss: 0.07232514396309853, Final Batch Loss: 0.047768957912921906\n",
      "Epoch 3512, Loss: 0.06410549953579903, Final Batch Loss: 0.032291002571582794\n",
      "Epoch 3513, Loss: 0.07601985521614552, Final Batch Loss: 0.0466914027929306\n",
      "Epoch 3514, Loss: 0.12114996090531349, Final Batch Loss: 0.07578404992818832\n",
      "Epoch 3515, Loss: 0.07851137593388557, Final Batch Loss: 0.04575198143720627\n",
      "Epoch 3516, Loss: 0.10594869032502174, Final Batch Loss: 0.037645649164915085\n",
      "Epoch 3517, Loss: 0.06555156596004963, Final Batch Loss: 0.024879204109311104\n",
      "Epoch 3518, Loss: 0.07640079036355019, Final Batch Loss: 0.035847846418619156\n",
      "Epoch 3519, Loss: 0.08808214403688908, Final Batch Loss: 0.06186629831790924\n",
      "Epoch 3520, Loss: 0.07599293626844883, Final Batch Loss: 0.05239925906062126\n",
      "Epoch 3521, Loss: 0.06584971398115158, Final Batch Loss: 0.035558778792619705\n",
      "Epoch 3522, Loss: 0.07434508949518204, Final Batch Loss: 0.031857337802648544\n",
      "Epoch 3523, Loss: 0.0709594301879406, Final Batch Loss: 0.03423161804676056\n",
      "Epoch 3524, Loss: 0.07413760758936405, Final Batch Loss: 0.049061279743909836\n",
      "Epoch 3525, Loss: 0.0818809736520052, Final Batch Loss: 0.05225374922156334\n",
      "Epoch 3526, Loss: 0.06400208920240402, Final Batch Loss: 0.04478389397263527\n",
      "Epoch 3527, Loss: 0.08554615452885628, Final Batch Loss: 0.04418693482875824\n",
      "Epoch 3528, Loss: 0.09049855917692184, Final Batch Loss: 0.042128611356019974\n",
      "Epoch 3529, Loss: 0.1060502864420414, Final Batch Loss: 0.0648682489991188\n",
      "Epoch 3530, Loss: 0.07699313014745712, Final Batch Loss: 0.02864917367696762\n",
      "Epoch 3531, Loss: 0.12251603603363037, Final Batch Loss: 0.06713054329156876\n",
      "Epoch 3532, Loss: 0.08835158497095108, Final Batch Loss: 0.050949305295944214\n",
      "Epoch 3533, Loss: 0.19771283119916916, Final Batch Loss: 0.1540326476097107\n",
      "Epoch 3534, Loss: 0.1445397026836872, Final Batch Loss: 0.05349389836192131\n",
      "Epoch 3535, Loss: 0.09821223840117455, Final Batch Loss: 0.0528288409113884\n",
      "Epoch 3536, Loss: 0.09017478674650192, Final Batch Loss: 0.049094706773757935\n",
      "Epoch 3537, Loss: 0.0961688943207264, Final Batch Loss: 0.06161009892821312\n",
      "Epoch 3538, Loss: 0.07210342399775982, Final Batch Loss: 0.04460396617650986\n",
      "Epoch 3539, Loss: 0.0810297504067421, Final Batch Loss: 0.03860738128423691\n",
      "Epoch 3540, Loss: 0.09819216281175613, Final Batch Loss: 0.05762645602226257\n",
      "Epoch 3541, Loss: 0.13503776863217354, Final Batch Loss: 0.09118804335594177\n",
      "Epoch 3542, Loss: 0.09155121445655823, Final Batch Loss: 0.04721982777118683\n",
      "Epoch 3543, Loss: 0.15127994865179062, Final Batch Loss: 0.03556086868047714\n",
      "Epoch 3544, Loss: 0.07185940444469452, Final Batch Loss: 0.03461067005991936\n",
      "Epoch 3545, Loss: 0.07858513668179512, Final Batch Loss: 0.029192756861448288\n",
      "Epoch 3546, Loss: 0.14101781882345676, Final Batch Loss: 0.028678225353360176\n",
      "Epoch 3547, Loss: 0.10400069504976273, Final Batch Loss: 0.04756104201078415\n",
      "Epoch 3548, Loss: 0.12053071707487106, Final Batch Loss: 0.04253612458705902\n",
      "Epoch 3549, Loss: 0.0769368689507246, Final Batch Loss: 0.02157965488731861\n",
      "Epoch 3550, Loss: 0.07534267753362656, Final Batch Loss: 0.03218197450041771\n",
      "Epoch 3551, Loss: 0.0994509682059288, Final Batch Loss: 0.046370729804039\n",
      "Epoch 3552, Loss: 0.08972253650426865, Final Batch Loss: 0.057874202728271484\n",
      "Epoch 3553, Loss: 0.082772396504879, Final Batch Loss: 0.03817205876111984\n",
      "Epoch 3554, Loss: 0.07792475447058678, Final Batch Loss: 0.02361118048429489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3555, Loss: 0.08407150208950043, Final Batch Loss: 0.046443503350019455\n",
      "Epoch 3556, Loss: 0.07389134913682938, Final Batch Loss: 0.04322922229766846\n",
      "Epoch 3557, Loss: 0.08292604982852936, Final Batch Loss: 0.05051562562584877\n",
      "Epoch 3558, Loss: 0.07180853001773357, Final Batch Loss: 0.02758028544485569\n",
      "Epoch 3559, Loss: 0.10620787739753723, Final Batch Loss: 0.021648332476615906\n",
      "Epoch 3560, Loss: 0.10542667657136917, Final Batch Loss: 0.06359916925430298\n",
      "Epoch 3561, Loss: 0.09284820035099983, Final Batch Loss: 0.04212581366300583\n",
      "Epoch 3562, Loss: 0.07617782056331635, Final Batch Loss: 0.05518324300646782\n",
      "Epoch 3563, Loss: 0.06959203258156776, Final Batch Loss: 0.034346532076597214\n",
      "Epoch 3564, Loss: 0.05901138857007027, Final Batch Loss: 0.03777461498975754\n",
      "Epoch 3565, Loss: 0.08202087879180908, Final Batch Loss: 0.04242679104208946\n",
      "Epoch 3566, Loss: 0.06548886001110077, Final Batch Loss: 0.027349762618541718\n",
      "Epoch 3567, Loss: 0.08138241618871689, Final Batch Loss: 0.04488101601600647\n",
      "Epoch 3568, Loss: 0.06894593685865402, Final Batch Loss: 0.03228231146931648\n",
      "Epoch 3569, Loss: 0.07253909111022949, Final Batch Loss: 0.038616105914115906\n",
      "Epoch 3570, Loss: 0.11334504559636116, Final Batch Loss: 0.035961393266916275\n",
      "Epoch 3571, Loss: 0.08913087472319603, Final Batch Loss: 0.03760230541229248\n",
      "Epoch 3572, Loss: 0.08701411634683609, Final Batch Loss: 0.05186197906732559\n",
      "Epoch 3573, Loss: 0.07111029699444771, Final Batch Loss: 0.03439098969101906\n",
      "Epoch 3574, Loss: 0.10450996086001396, Final Batch Loss: 0.06582282483577728\n",
      "Epoch 3575, Loss: 0.07502009905874729, Final Batch Loss: 0.04646729677915573\n",
      "Epoch 3576, Loss: 0.21411778777837753, Final Batch Loss: 0.11884983628988266\n",
      "Epoch 3577, Loss: 0.10367451608181, Final Batch Loss: 0.048622820526361465\n",
      "Epoch 3578, Loss: 0.15739084035158157, Final Batch Loss: 0.09933356940746307\n",
      "Epoch 3579, Loss: 0.15449479967355728, Final Batch Loss: 0.07395262271165848\n",
      "Epoch 3580, Loss: 0.14599203318357468, Final Batch Loss: 0.08686958998441696\n",
      "Epoch 3581, Loss: 0.12613609805703163, Final Batch Loss: 0.08052997291088104\n",
      "Epoch 3582, Loss: 0.09345391765236855, Final Batch Loss: 0.05250794067978859\n",
      "Epoch 3583, Loss: 0.10889554768800735, Final Batch Loss: 0.06410570442676544\n",
      "Epoch 3584, Loss: 0.1374608911573887, Final Batch Loss: 0.052975837141275406\n",
      "Epoch 3585, Loss: 0.13389215618371964, Final Batch Loss: 0.04353754222393036\n",
      "Epoch 3586, Loss: 0.09672180749475956, Final Batch Loss: 0.07574678957462311\n",
      "Epoch 3587, Loss: 0.10113074630498886, Final Batch Loss: 0.021717704832553864\n",
      "Epoch 3588, Loss: 0.15754498913884163, Final Batch Loss: 0.10268419981002808\n",
      "Epoch 3589, Loss: 0.10289939120411873, Final Batch Loss: 0.03740153834223747\n",
      "Epoch 3590, Loss: 0.12126506492495537, Final Batch Loss: 0.08987439423799515\n",
      "Epoch 3591, Loss: 0.09200089797377586, Final Batch Loss: 0.030718792229890823\n",
      "Epoch 3592, Loss: 0.10064484179019928, Final Batch Loss: 0.04024629667401314\n",
      "Epoch 3593, Loss: 0.15577123314142227, Final Batch Loss: 0.1114569827914238\n",
      "Epoch 3594, Loss: 0.1313585564494133, Final Batch Loss: 0.06746062636375427\n",
      "Epoch 3595, Loss: 0.11369885131716728, Final Batch Loss: 0.03785116598010063\n",
      "Epoch 3596, Loss: 0.10907153785228729, Final Batch Loss: 0.0632910206913948\n",
      "Epoch 3597, Loss: 0.15042917430400848, Final Batch Loss: 0.09473814815282822\n",
      "Epoch 3598, Loss: 0.0871630534529686, Final Batch Loss: 0.03904595598578453\n",
      "Epoch 3599, Loss: 0.13832617551088333, Final Batch Loss: 0.050847671926021576\n",
      "Epoch 3600, Loss: 0.09536556527018547, Final Batch Loss: 0.04517781361937523\n",
      "Epoch 3601, Loss: 0.11725503206253052, Final Batch Loss: 0.07000607997179031\n",
      "Epoch 3602, Loss: 0.14354754984378815, Final Batch Loss: 0.08902183175086975\n",
      "Epoch 3603, Loss: 0.08559459075331688, Final Batch Loss: 0.03827522322535515\n",
      "Epoch 3604, Loss: 0.08496416732668877, Final Batch Loss: 0.03319931775331497\n",
      "Epoch 3605, Loss: 0.1383727230131626, Final Batch Loss: 0.04856494441628456\n",
      "Epoch 3606, Loss: 0.09444869123399258, Final Batch Loss: 0.06476723402738571\n",
      "Epoch 3607, Loss: 0.10488446429371834, Final Batch Loss: 0.06919604539871216\n",
      "Epoch 3608, Loss: 0.07609333097934723, Final Batch Loss: 0.03494753688573837\n",
      "Epoch 3609, Loss: 0.06450449861586094, Final Batch Loss: 0.028204312548041344\n",
      "Epoch 3610, Loss: 0.0829375684261322, Final Batch Loss: 0.03164291009306908\n",
      "Epoch 3611, Loss: 0.08469176292419434, Final Batch Loss: 0.04844667762517929\n",
      "Epoch 3612, Loss: 0.10304414108395576, Final Batch Loss: 0.06769643723964691\n",
      "Epoch 3613, Loss: 0.06639503128826618, Final Batch Loss: 0.03704535588622093\n",
      "Epoch 3614, Loss: 0.09465837851166725, Final Batch Loss: 0.05402161180973053\n",
      "Epoch 3615, Loss: 0.11163148283958435, Final Batch Loss: 0.037216998636722565\n",
      "Epoch 3616, Loss: 0.07504728063941002, Final Batch Loss: 0.03717219457030296\n",
      "Epoch 3617, Loss: 0.08660488575696945, Final Batch Loss: 0.0536370575428009\n",
      "Epoch 3618, Loss: 0.0692364564165473, Final Batch Loss: 0.012359659187495708\n",
      "Epoch 3619, Loss: 0.07695096172392368, Final Batch Loss: 0.02272033877670765\n",
      "Epoch 3620, Loss: 0.073250912129879, Final Batch Loss: 0.0315982848405838\n",
      "Epoch 3621, Loss: 0.07703038305044174, Final Batch Loss: 0.03531024232506752\n",
      "Epoch 3622, Loss: 0.08568567037582397, Final Batch Loss: 0.05699853226542473\n",
      "Epoch 3623, Loss: 0.09893542900681496, Final Batch Loss: 0.03578789904713631\n",
      "Epoch 3624, Loss: 0.09001030400395393, Final Batch Loss: 0.05595364421606064\n",
      "Epoch 3625, Loss: 0.07308306731283665, Final Batch Loss: 0.02396341972053051\n",
      "Epoch 3626, Loss: 0.09714952856302261, Final Batch Loss: 0.041368268430233\n",
      "Epoch 3627, Loss: 0.08170164749026299, Final Batch Loss: 0.04326982423663139\n",
      "Epoch 3628, Loss: 0.08288805559277534, Final Batch Loss: 0.04716017097234726\n",
      "Epoch 3629, Loss: 0.10350371152162552, Final Batch Loss: 0.060360584408044815\n",
      "Epoch 3630, Loss: 0.06970572657883167, Final Batch Loss: 0.04008527100086212\n",
      "Epoch 3631, Loss: 0.06618620082736015, Final Batch Loss: 0.03388092666864395\n",
      "Epoch 3632, Loss: 0.08557843044400215, Final Batch Loss: 0.036857157945632935\n",
      "Epoch 3633, Loss: 0.08596967346966267, Final Batch Loss: 0.030471982434391975\n",
      "Epoch 3634, Loss: 0.08874475210905075, Final Batch Loss: 0.03274558484554291\n",
      "Epoch 3635, Loss: 0.08303277939558029, Final Batch Loss: 0.049986615777015686\n",
      "Epoch 3636, Loss: 0.09122893214225769, Final Batch Loss: 0.03942768648266792\n",
      "Epoch 3637, Loss: 0.13340337947010994, Final Batch Loss: 0.041642215102910995\n",
      "Epoch 3638, Loss: 0.08706094324588776, Final Batch Loss: 0.05362572520971298\n",
      "Epoch 3639, Loss: 0.08131389319896698, Final Batch Loss: 0.03712858632206917\n",
      "Epoch 3640, Loss: 0.08867712691426277, Final Batch Loss: 0.05352330952882767\n",
      "Epoch 3641, Loss: 0.16745948046445847, Final Batch Loss: 0.10878119617700577\n",
      "Epoch 3642, Loss: 0.1112079881131649, Final Batch Loss: 0.04339015856385231\n",
      "Epoch 3643, Loss: 0.08777552470564842, Final Batch Loss: 0.0279582217335701\n",
      "Epoch 3644, Loss: 0.08676161989569664, Final Batch Loss: 0.03837959095835686\n",
      "Epoch 3645, Loss: 0.08571313321590424, Final Batch Loss: 0.04992559179663658\n",
      "Epoch 3646, Loss: 0.08919471874833107, Final Batch Loss: 0.04030820354819298\n",
      "Epoch 3647, Loss: 0.0955992303788662, Final Batch Loss: 0.06447712332010269\n",
      "Epoch 3648, Loss: 0.13960756734013557, Final Batch Loss: 0.08666816353797913\n",
      "Epoch 3649, Loss: 0.0879821889102459, Final Batch Loss: 0.05161907523870468\n",
      "Epoch 3650, Loss: 0.07473649084568024, Final Batch Loss: 0.03914180397987366\n",
      "Epoch 3651, Loss: 0.09613467380404472, Final Batch Loss: 0.0406220480799675\n",
      "Epoch 3652, Loss: 0.07523979619145393, Final Batch Loss: 0.023098386824131012\n",
      "Epoch 3653, Loss: 0.08788932487368584, Final Batch Loss: 0.03528933227062225\n",
      "Epoch 3654, Loss: 0.1193360798060894, Final Batch Loss: 0.02982887253165245\n",
      "Epoch 3655, Loss: 0.11495856195688248, Final Batch Loss: 0.03311330825090408\n",
      "Epoch 3656, Loss: 0.08452710509300232, Final Batch Loss: 0.03846896439790726\n",
      "Epoch 3657, Loss: 0.10317838937044144, Final Batch Loss: 0.05044379457831383\n",
      "Epoch 3658, Loss: 0.07546467520296574, Final Batch Loss: 0.023793557658791542\n",
      "Epoch 3659, Loss: 0.09940043091773987, Final Batch Loss: 0.05899389460682869\n",
      "Epoch 3660, Loss: 0.07284588739275932, Final Batch Loss: 0.0348820723593235\n",
      "Epoch 3661, Loss: 0.09081204608082771, Final Batch Loss: 0.030987199395895004\n",
      "Epoch 3662, Loss: 0.08581436984241009, Final Batch Loss: 0.06333822011947632\n",
      "Epoch 3663, Loss: 0.10425388999283314, Final Batch Loss: 0.07811364531517029\n",
      "Epoch 3664, Loss: 0.05799376778304577, Final Batch Loss: 0.030114898458123207\n",
      "Epoch 3665, Loss: 0.10960302874445915, Final Batch Loss: 0.03434635326266289\n",
      "Epoch 3666, Loss: 0.06382541917264462, Final Batch Loss: 0.03900742530822754\n",
      "Epoch 3667, Loss: 0.08469717763364315, Final Batch Loss: 0.028294475749135017\n",
      "Epoch 3668, Loss: 0.0877431109547615, Final Batch Loss: 0.03868073970079422\n",
      "Epoch 3669, Loss: 0.07456813380122185, Final Batch Loss: 0.030588142573833466\n",
      "Epoch 3670, Loss: 0.07172861322760582, Final Batch Loss: 0.04292263463139534\n",
      "Epoch 3671, Loss: 0.10265849158167839, Final Batch Loss: 0.04869755730032921\n",
      "Epoch 3672, Loss: 0.1159520223736763, Final Batch Loss: 0.05452224239706993\n",
      "Epoch 3673, Loss: 0.0789741650223732, Final Batch Loss: 0.03828086331486702\n",
      "Epoch 3674, Loss: 0.07625776901841164, Final Batch Loss: 0.0365462489426136\n",
      "Epoch 3675, Loss: 0.08141954429447651, Final Batch Loss: 0.02277139388024807\n",
      "Epoch 3676, Loss: 0.08146405220031738, Final Batch Loss: 0.04268491640686989\n",
      "Epoch 3677, Loss: 0.09876399859786034, Final Batch Loss: 0.041443537920713425\n",
      "Epoch 3678, Loss: 0.1130727156996727, Final Batch Loss: 0.07319280505180359\n",
      "Epoch 3679, Loss: 0.06901980936527252, Final Batch Loss: 0.0424966886639595\n",
      "Epoch 3680, Loss: 0.13234513252973557, Final Batch Loss: 0.08749625086784363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3681, Loss: 0.06719687581062317, Final Batch Loss: 0.03602626174688339\n",
      "Epoch 3682, Loss: 0.060778357088565826, Final Batch Loss: 0.023213453590869904\n",
      "Epoch 3683, Loss: 0.13101636618375778, Final Batch Loss: 0.051925547420978546\n",
      "Epoch 3684, Loss: 0.0957607738673687, Final Batch Loss: 0.053872961550951004\n",
      "Epoch 3685, Loss: 0.09831675514578819, Final Batch Loss: 0.05313127115368843\n",
      "Epoch 3686, Loss: 0.08428969234228134, Final Batch Loss: 0.02742552012205124\n",
      "Epoch 3687, Loss: 0.09515612199902534, Final Batch Loss: 0.042392976582050323\n",
      "Epoch 3688, Loss: 0.06249156966805458, Final Batch Loss: 0.03522331640124321\n",
      "Epoch 3689, Loss: 0.07616673037409782, Final Batch Loss: 0.027130719274282455\n",
      "Epoch 3690, Loss: 0.1095481775701046, Final Batch Loss: 0.07091428339481354\n",
      "Epoch 3691, Loss: 0.06852749921381474, Final Batch Loss: 0.027693679556250572\n",
      "Epoch 3692, Loss: 0.07884673401713371, Final Batch Loss: 0.03661010041832924\n",
      "Epoch 3693, Loss: 0.06109360232949257, Final Batch Loss: 0.02946210652589798\n",
      "Epoch 3694, Loss: 0.0891521517187357, Final Batch Loss: 0.03069583885371685\n",
      "Epoch 3695, Loss: 0.07974395900964737, Final Batch Loss: 0.034069277346134186\n",
      "Epoch 3696, Loss: 0.06847433932125568, Final Batch Loss: 0.046466462314128876\n",
      "Epoch 3697, Loss: 0.07167024910449982, Final Batch Loss: 0.031457725912332535\n",
      "Epoch 3698, Loss: 0.07858904078602791, Final Batch Loss: 0.04395541176199913\n",
      "Epoch 3699, Loss: 0.11351596564054489, Final Batch Loss: 0.06043560802936554\n",
      "Epoch 3700, Loss: 0.08461185917258263, Final Batch Loss: 0.03995297849178314\n",
      "Epoch 3701, Loss: 0.06567626632750034, Final Batch Loss: 0.026894593611359596\n",
      "Epoch 3702, Loss: 0.08769160136580467, Final Batch Loss: 0.04265810549259186\n",
      "Epoch 3703, Loss: 0.08243635296821594, Final Batch Loss: 0.03471943363547325\n",
      "Epoch 3704, Loss: 0.08231423795223236, Final Batch Loss: 0.039007361978292465\n",
      "Epoch 3705, Loss: 0.08484393358230591, Final Batch Loss: 0.03844142705202103\n",
      "Epoch 3706, Loss: 0.07640516385436058, Final Batch Loss: 0.042900197207927704\n",
      "Epoch 3707, Loss: 0.07306808605790138, Final Batch Loss: 0.03695429861545563\n",
      "Epoch 3708, Loss: 0.07287449389696121, Final Batch Loss: 0.016667403280735016\n",
      "Epoch 3709, Loss: 0.08346300944685936, Final Batch Loss: 0.051016803830862045\n",
      "Epoch 3710, Loss: 0.07865656167268753, Final Batch Loss: 0.04034777358174324\n",
      "Epoch 3711, Loss: 0.09465660899877548, Final Batch Loss: 0.07105253636837006\n",
      "Epoch 3712, Loss: 0.07813932374119759, Final Batch Loss: 0.036543648689985275\n",
      "Epoch 3713, Loss: 0.08120210096240044, Final Batch Loss: 0.03661279007792473\n",
      "Epoch 3714, Loss: 0.08684178814291954, Final Batch Loss: 0.03588338568806648\n",
      "Epoch 3715, Loss: 0.07534543797373772, Final Batch Loss: 0.039573244750499725\n",
      "Epoch 3716, Loss: 0.08353404328227043, Final Batch Loss: 0.0446920171380043\n",
      "Epoch 3717, Loss: 0.09402696415781975, Final Batch Loss: 0.035273775458335876\n",
      "Epoch 3718, Loss: 0.05837773717939854, Final Batch Loss: 0.028521906584501266\n",
      "Epoch 3719, Loss: 0.0876786895096302, Final Batch Loss: 0.038788359612226486\n",
      "Epoch 3720, Loss: 0.08270345628261566, Final Batch Loss: 0.044271014630794525\n",
      "Epoch 3721, Loss: 0.07243635505437851, Final Batch Loss: 0.035899121314287186\n",
      "Epoch 3722, Loss: 0.07155694253742695, Final Batch Loss: 0.02506372146308422\n",
      "Epoch 3723, Loss: 0.07190816104412079, Final Batch Loss: 0.0337747298181057\n",
      "Epoch 3724, Loss: 0.07905747927725315, Final Batch Loss: 0.051333021372556686\n",
      "Epoch 3725, Loss: 0.0759342797100544, Final Batch Loss: 0.03685307130217552\n",
      "Epoch 3726, Loss: 0.07810216210782528, Final Batch Loss: 0.031208394095301628\n",
      "Epoch 3727, Loss: 0.07442926242947578, Final Batch Loss: 0.03854580596089363\n",
      "Epoch 3728, Loss: 0.07575052604079247, Final Batch Loss: 0.03419705480337143\n",
      "Epoch 3729, Loss: 0.0911768227815628, Final Batch Loss: 0.04961765557527542\n",
      "Epoch 3730, Loss: 0.06738099455833435, Final Batch Loss: 0.02962486818432808\n",
      "Epoch 3731, Loss: 0.10018201172351837, Final Batch Loss: 0.04416592791676521\n",
      "Epoch 3732, Loss: 0.0962727703154087, Final Batch Loss: 0.06525176018476486\n",
      "Epoch 3733, Loss: 0.09472698532044888, Final Batch Loss: 0.02759687416255474\n",
      "Epoch 3734, Loss: 0.08356398716568947, Final Batch Loss: 0.03276396915316582\n",
      "Epoch 3735, Loss: 0.05748732201755047, Final Batch Loss: 0.02703743241727352\n",
      "Epoch 3736, Loss: 0.06671829521656036, Final Batch Loss: 0.01656307280063629\n",
      "Epoch 3737, Loss: 0.06585764139890671, Final Batch Loss: 0.03173309192061424\n",
      "Epoch 3738, Loss: 0.1010948158800602, Final Batch Loss: 0.032431405037641525\n",
      "Epoch 3739, Loss: 0.09070459008216858, Final Batch Loss: 0.03288727253675461\n",
      "Epoch 3740, Loss: 0.07493138127028942, Final Batch Loss: 0.03001459501683712\n",
      "Epoch 3741, Loss: 0.07994385063648224, Final Batch Loss: 0.032725147902965546\n",
      "Epoch 3742, Loss: 0.05621187388896942, Final Batch Loss: 0.020585715770721436\n",
      "Epoch 3743, Loss: 0.08945726230740547, Final Batch Loss: 0.03965317830443382\n",
      "Epoch 3744, Loss: 0.0650208443403244, Final Batch Loss: 0.03280460461974144\n",
      "Epoch 3745, Loss: 0.07689497992396355, Final Batch Loss: 0.04583015665411949\n",
      "Epoch 3746, Loss: 0.08533719554543495, Final Batch Loss: 0.031235389411449432\n",
      "Epoch 3747, Loss: 0.1137254573404789, Final Batch Loss: 0.059200745075941086\n",
      "Epoch 3748, Loss: 0.10797884315252304, Final Batch Loss: 0.02730485051870346\n",
      "Epoch 3749, Loss: 0.07125348784029484, Final Batch Loss: 0.046064119786024094\n",
      "Epoch 3750, Loss: 0.0877070315182209, Final Batch Loss: 0.05296188220381737\n",
      "Epoch 3751, Loss: 0.09891757741570473, Final Batch Loss: 0.06325314193964005\n",
      "Epoch 3752, Loss: 0.08527645468711853, Final Batch Loss: 0.03342951461672783\n",
      "Epoch 3753, Loss: 0.06791340745985508, Final Batch Loss: 0.029644371941685677\n",
      "Epoch 3754, Loss: 0.08297991380095482, Final Batch Loss: 0.06188733130693436\n",
      "Epoch 3755, Loss: 0.14525551348924637, Final Batch Loss: 0.07347298413515091\n",
      "Epoch 3756, Loss: 0.08377623558044434, Final Batch Loss: 0.03940003737807274\n",
      "Epoch 3757, Loss: 0.11016038805246353, Final Batch Loss: 0.05428479611873627\n",
      "Epoch 3758, Loss: 0.11331033334136009, Final Batch Loss: 0.0642957091331482\n",
      "Epoch 3759, Loss: 0.15215451270341873, Final Batch Loss: 0.022473223507404327\n",
      "Epoch 3760, Loss: 0.08612983301281929, Final Batch Loss: 0.05428660660982132\n",
      "Epoch 3761, Loss: 0.18615443632006645, Final Batch Loss: 0.13802044093608856\n",
      "Epoch 3762, Loss: 0.08611029759049416, Final Batch Loss: 0.04080980271100998\n",
      "Epoch 3763, Loss: 0.14641736075282097, Final Batch Loss: 0.09293016046285629\n",
      "Epoch 3764, Loss: 0.16214902698993683, Final Batch Loss: 0.05690738558769226\n",
      "Epoch 3765, Loss: 0.1352372094988823, Final Batch Loss: 0.04098798334598541\n",
      "Epoch 3766, Loss: 0.10573962703347206, Final Batch Loss: 0.034625355154275894\n",
      "Epoch 3767, Loss: 0.08754904195666313, Final Batch Loss: 0.0356328971683979\n",
      "Epoch 3768, Loss: 0.10381155088543892, Final Batch Loss: 0.04539228230714798\n",
      "Epoch 3769, Loss: 0.11388684436678886, Final Batch Loss: 0.04204898700118065\n",
      "Epoch 3770, Loss: 0.08787836506962776, Final Batch Loss: 0.05044667050242424\n",
      "Epoch 3771, Loss: 0.0782635360956192, Final Batch Loss: 0.03424420580267906\n",
      "Epoch 3772, Loss: 0.09368862956762314, Final Batch Loss: 0.05143924057483673\n",
      "Epoch 3773, Loss: 0.11835432052612305, Final Batch Loss: 0.07549147307872772\n",
      "Epoch 3774, Loss: 0.09181011654436588, Final Batch Loss: 0.06790082901716232\n",
      "Epoch 3775, Loss: 0.08768945746123791, Final Batch Loss: 0.05690585821866989\n",
      "Epoch 3776, Loss: 0.09345192834734917, Final Batch Loss: 0.04487277939915657\n",
      "Epoch 3777, Loss: 0.07322172075510025, Final Batch Loss: 0.03453625738620758\n",
      "Epoch 3778, Loss: 0.11676272377371788, Final Batch Loss: 0.06009337306022644\n",
      "Epoch 3779, Loss: 0.09393873810768127, Final Batch Loss: 0.060795143246650696\n",
      "Epoch 3780, Loss: 0.1298518106341362, Final Batch Loss: 0.02596857398748398\n",
      "Epoch 3781, Loss: 0.0679139606654644, Final Batch Loss: 0.03377143293619156\n",
      "Epoch 3782, Loss: 0.10783985629677773, Final Batch Loss: 0.05651744082570076\n",
      "Epoch 3783, Loss: 0.10827887430787086, Final Batch Loss: 0.054171595722436905\n",
      "Epoch 3784, Loss: 0.07757345214486122, Final Batch Loss: 0.04133625701069832\n",
      "Epoch 3785, Loss: 0.08148101344704628, Final Batch Loss: 0.02987971156835556\n",
      "Epoch 3786, Loss: 0.11992041021585464, Final Batch Loss: 0.05059216916561127\n",
      "Epoch 3787, Loss: 0.0832037404179573, Final Batch Loss: 0.05035969242453575\n",
      "Epoch 3788, Loss: 0.08223149366676807, Final Batch Loss: 0.05305284634232521\n",
      "Epoch 3789, Loss: 0.08503014966845512, Final Batch Loss: 0.04635651782155037\n",
      "Epoch 3790, Loss: 0.07417679950594902, Final Batch Loss: 0.03798080235719681\n",
      "Epoch 3791, Loss: 0.11508548632264137, Final Batch Loss: 0.057125840336084366\n",
      "Epoch 3792, Loss: 0.09404092654585838, Final Batch Loss: 0.04679325595498085\n",
      "Epoch 3793, Loss: 0.08956946060061455, Final Batch Loss: 0.05248185992240906\n",
      "Epoch 3794, Loss: 0.07290175929665565, Final Batch Loss: 0.03495495021343231\n",
      "Epoch 3795, Loss: 0.10295812413096428, Final Batch Loss: 0.06361648440361023\n",
      "Epoch 3796, Loss: 0.1415231116116047, Final Batch Loss: 0.05223851278424263\n",
      "Epoch 3797, Loss: 0.07692082226276398, Final Batch Loss: 0.034807588905096054\n",
      "Epoch 3798, Loss: 0.09253789857029915, Final Batch Loss: 0.051245734095573425\n",
      "Epoch 3799, Loss: 0.14479286968708038, Final Batch Loss: 0.062069371342659\n",
      "Epoch 3800, Loss: 0.136968232691288, Final Batch Loss: 0.07494595646858215\n",
      "Epoch 3801, Loss: 0.09506624191999435, Final Batch Loss: 0.04880287125706673\n",
      "Epoch 3802, Loss: 0.09514167159795761, Final Batch Loss: 0.04006997123360634\n",
      "Epoch 3803, Loss: 0.08576648682355881, Final Batch Loss: 0.03808099403977394\n",
      "Epoch 3804, Loss: 0.07584375888109207, Final Batch Loss: 0.03158559650182724\n",
      "Epoch 3805, Loss: 0.10494715347886086, Final Batch Loss: 0.055181603878736496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3806, Loss: 0.07435452565550804, Final Batch Loss: 0.032354217022657394\n",
      "Epoch 3807, Loss: 0.09870901331305504, Final Batch Loss: 0.06173880398273468\n",
      "Epoch 3808, Loss: 0.07336878776550293, Final Batch Loss: 0.0319344699382782\n",
      "Epoch 3809, Loss: 0.10282407701015472, Final Batch Loss: 0.07441331446170807\n",
      "Epoch 3810, Loss: 0.06827590242028236, Final Batch Loss: 0.03402077779173851\n",
      "Epoch 3811, Loss: 0.0770054180175066, Final Batch Loss: 0.05173492059111595\n",
      "Epoch 3812, Loss: 0.11532362550497055, Final Batch Loss: 0.08146684616804123\n",
      "Epoch 3813, Loss: 0.060924630612134933, Final Batch Loss: 0.028018590062856674\n",
      "Epoch 3814, Loss: 0.07797500491142273, Final Batch Loss: 0.04045730456709862\n",
      "Epoch 3815, Loss: 0.10875319689512253, Final Batch Loss: 0.0673346221446991\n",
      "Epoch 3816, Loss: 0.06815632060170174, Final Batch Loss: 0.03459646925330162\n",
      "Epoch 3817, Loss: 0.07484076917171478, Final Batch Loss: 0.025657884776592255\n",
      "Epoch 3818, Loss: 0.08335883170366287, Final Batch Loss: 0.033232592046260834\n",
      "Epoch 3819, Loss: 0.08274335041642189, Final Batch Loss: 0.04963522404432297\n",
      "Epoch 3820, Loss: 0.08873043209314346, Final Batch Loss: 0.045963652431964874\n",
      "Epoch 3821, Loss: 0.0678566824644804, Final Batch Loss: 0.03923854976892471\n",
      "Epoch 3822, Loss: 0.06864022091031075, Final Batch Loss: 0.03556150943040848\n",
      "Epoch 3823, Loss: 0.08327677473425865, Final Batch Loss: 0.026188131421804428\n",
      "Epoch 3824, Loss: 0.06066443212330341, Final Batch Loss: 0.0247506070882082\n",
      "Epoch 3825, Loss: 0.08841247484087944, Final Batch Loss: 0.0471360869705677\n",
      "Epoch 3826, Loss: 0.06950714439153671, Final Batch Loss: 0.03932983800768852\n",
      "Epoch 3827, Loss: 0.12387704290449619, Final Batch Loss: 0.031048962846398354\n",
      "Epoch 3828, Loss: 0.09501004964113235, Final Batch Loss: 0.05410776659846306\n",
      "Epoch 3829, Loss: 0.0644006859511137, Final Batch Loss: 0.022706164047122\n",
      "Epoch 3830, Loss: 0.08788810670375824, Final Batch Loss: 0.05175895616412163\n",
      "Epoch 3831, Loss: 0.09623463824391365, Final Batch Loss: 0.03893302008509636\n",
      "Epoch 3832, Loss: 0.05669216439127922, Final Batch Loss: 0.023371312767267227\n",
      "Epoch 3833, Loss: 0.06493177637457848, Final Batch Loss: 0.03086388111114502\n",
      "Epoch 3834, Loss: 0.07636760547757149, Final Batch Loss: 0.031766925007104874\n",
      "Epoch 3835, Loss: 0.07705127075314522, Final Batch Loss: 0.0374731682240963\n",
      "Epoch 3836, Loss: 0.09771691262722015, Final Batch Loss: 0.05777496472001076\n",
      "Epoch 3837, Loss: 0.07375489920377731, Final Batch Loss: 0.03421831503510475\n",
      "Epoch 3838, Loss: 0.06550849229097366, Final Batch Loss: 0.03410894423723221\n",
      "Epoch 3839, Loss: 0.08367582783102989, Final Batch Loss: 0.04228736832737923\n",
      "Epoch 3840, Loss: 0.07084530964493752, Final Batch Loss: 0.0425788015127182\n",
      "Epoch 3841, Loss: 0.09141427651047707, Final Batch Loss: 0.044285017997026443\n",
      "Epoch 3842, Loss: 0.06405987404286861, Final Batch Loss: 0.037653300911188126\n",
      "Epoch 3843, Loss: 0.06986362487077713, Final Batch Loss: 0.02220001071691513\n",
      "Epoch 3844, Loss: 0.05781039223074913, Final Batch Loss: 0.03795136883854866\n",
      "Epoch 3845, Loss: 0.05914841406047344, Final Batch Loss: 0.035961706191301346\n",
      "Epoch 3846, Loss: 0.07777205854654312, Final Batch Loss: 0.03428589552640915\n",
      "Epoch 3847, Loss: 0.05615363456308842, Final Batch Loss: 0.026220904663205147\n",
      "Epoch 3848, Loss: 0.06859273836016655, Final Batch Loss: 0.03138178214430809\n",
      "Epoch 3849, Loss: 0.09359847754240036, Final Batch Loss: 0.03765304014086723\n",
      "Epoch 3850, Loss: 0.08884981647133827, Final Batch Loss: 0.04807053133845329\n",
      "Epoch 3851, Loss: 0.06334784626960754, Final Batch Loss: 0.04702650010585785\n",
      "Epoch 3852, Loss: 0.08160195872187614, Final Batch Loss: 0.037851426750421524\n",
      "Epoch 3853, Loss: 0.06454473733901978, Final Batch Loss: 0.03215663880109787\n",
      "Epoch 3854, Loss: 0.07027325592935085, Final Batch Loss: 0.03075876273214817\n",
      "Epoch 3855, Loss: 0.06745725870132446, Final Batch Loss: 0.04352773725986481\n",
      "Epoch 3856, Loss: 0.12882854789495468, Final Batch Loss: 0.041525550186634064\n",
      "Epoch 3857, Loss: 0.06765674240887165, Final Batch Loss: 0.027634570375084877\n",
      "Epoch 3858, Loss: 0.07800846733152866, Final Batch Loss: 0.048185426741838455\n",
      "Epoch 3859, Loss: 0.06920829601585865, Final Batch Loss: 0.026610510423779488\n",
      "Epoch 3860, Loss: 0.07714754715561867, Final Batch Loss: 0.04159393161535263\n",
      "Epoch 3861, Loss: 0.06541234441101551, Final Batch Loss: 0.036000628024339676\n",
      "Epoch 3862, Loss: 0.08801999688148499, Final Batch Loss: 0.05023186281323433\n",
      "Epoch 3863, Loss: 0.07902635261416435, Final Batch Loss: 0.0478644073009491\n",
      "Epoch 3864, Loss: 0.0794462226331234, Final Batch Loss: 0.04395224526524544\n",
      "Epoch 3865, Loss: 0.0687869880348444, Final Batch Loss: 0.04147353395819664\n",
      "Epoch 3866, Loss: 0.0655130185186863, Final Batch Loss: 0.023990396410226822\n",
      "Epoch 3867, Loss: 0.07411352172493935, Final Batch Loss: 0.03866278752684593\n",
      "Epoch 3868, Loss: 0.06620990484952927, Final Batch Loss: 0.027246583253145218\n",
      "Epoch 3869, Loss: 0.08149346709251404, Final Batch Loss: 0.03863285481929779\n",
      "Epoch 3870, Loss: 0.06844852305948734, Final Batch Loss: 0.020977387204766273\n",
      "Epoch 3871, Loss: 0.07520943135023117, Final Batch Loss: 0.03632739186286926\n",
      "Epoch 3872, Loss: 0.0716690868139267, Final Batch Loss: 0.027046874165534973\n",
      "Epoch 3873, Loss: 0.08307214267551899, Final Batch Loss: 0.029706282541155815\n",
      "Epoch 3874, Loss: 0.1020703949034214, Final Batch Loss: 0.06028565391898155\n",
      "Epoch 3875, Loss: 0.056314872577786446, Final Batch Loss: 0.024022219702601433\n",
      "Epoch 3876, Loss: 0.09322560764849186, Final Batch Loss: 0.06813328713178635\n",
      "Epoch 3877, Loss: 0.09997569769620895, Final Batch Loss: 0.06516853719949722\n",
      "Epoch 3878, Loss: 0.06365132890641689, Final Batch Loss: 0.02702094055712223\n",
      "Epoch 3879, Loss: 0.07383381947875023, Final Batch Loss: 0.024106338620185852\n",
      "Epoch 3880, Loss: 0.10418698564171791, Final Batch Loss: 0.06581880152225494\n",
      "Epoch 3881, Loss: 0.09701422601938248, Final Batch Loss: 0.03887692466378212\n",
      "Epoch 3882, Loss: 0.13067860156297684, Final Batch Loss: 0.08569887280464172\n",
      "Epoch 3883, Loss: 0.07499395683407784, Final Batch Loss: 0.03774847835302353\n",
      "Epoch 3884, Loss: 0.0841611884534359, Final Batch Loss: 0.045170050114393234\n",
      "Epoch 3885, Loss: 0.09213599562644958, Final Batch Loss: 0.045631106942892075\n",
      "Epoch 3886, Loss: 0.08812231197953224, Final Batch Loss: 0.05054609104990959\n",
      "Epoch 3887, Loss: 0.08724503964185715, Final Batch Loss: 0.05247017368674278\n",
      "Epoch 3888, Loss: 0.0690610520541668, Final Batch Loss: 0.03372960165143013\n",
      "Epoch 3889, Loss: 0.10750726982951164, Final Batch Loss: 0.048991505056619644\n",
      "Epoch 3890, Loss: 0.06721367314457893, Final Batch Loss: 0.028173979371786118\n",
      "Epoch 3891, Loss: 0.06310075893998146, Final Batch Loss: 0.02512340247631073\n",
      "Epoch 3892, Loss: 0.08231031894683838, Final Batch Loss: 0.05572788417339325\n",
      "Epoch 3893, Loss: 0.0854182131588459, Final Batch Loss: 0.04553988203406334\n",
      "Epoch 3894, Loss: 0.07275848276913166, Final Batch Loss: 0.02978738211095333\n",
      "Epoch 3895, Loss: 0.06873767264187336, Final Batch Loss: 0.028726251795887947\n",
      "Epoch 3896, Loss: 0.07705940306186676, Final Batch Loss: 0.03731684386730194\n",
      "Epoch 3897, Loss: 0.06768181920051575, Final Batch Loss: 0.033193618059158325\n",
      "Epoch 3898, Loss: 0.08432269841432571, Final Batch Loss: 0.03416983038187027\n",
      "Epoch 3899, Loss: 0.09319858439266682, Final Batch Loss: 0.06872573494911194\n",
      "Epoch 3900, Loss: 0.0894143208861351, Final Batch Loss: 0.03421005234122276\n",
      "Epoch 3901, Loss: 0.07993251457810402, Final Batch Loss: 0.05220382288098335\n",
      "Epoch 3902, Loss: 0.080135777592659, Final Batch Loss: 0.027988802641630173\n",
      "Epoch 3903, Loss: 0.09065698087215424, Final Batch Loss: 0.05368019640445709\n",
      "Epoch 3904, Loss: 0.07251640036702156, Final Batch Loss: 0.05186276137828827\n",
      "Epoch 3905, Loss: 0.12999824061989784, Final Batch Loss: 0.04414929822087288\n",
      "Epoch 3906, Loss: 0.06991639360785484, Final Batch Loss: 0.03660077974200249\n",
      "Epoch 3907, Loss: 0.07898057252168655, Final Batch Loss: 0.04608234390616417\n",
      "Epoch 3908, Loss: 0.060399942100048065, Final Batch Loss: 0.02350427210330963\n",
      "Epoch 3909, Loss: 0.08464585244655609, Final Batch Loss: 0.032880980521440506\n",
      "Epoch 3910, Loss: 0.0901307463645935, Final Batch Loss: 0.03104764223098755\n",
      "Epoch 3911, Loss: 0.09478482976555824, Final Batch Loss: 0.03289266303181648\n",
      "Epoch 3912, Loss: 0.06789018027484417, Final Batch Loss: 0.027418551966547966\n",
      "Epoch 3913, Loss: 0.0959315337240696, Final Batch Loss: 0.03406937047839165\n",
      "Epoch 3914, Loss: 0.0673487689346075, Final Batch Loss: 0.04276805743575096\n",
      "Epoch 3915, Loss: 0.059774987399578094, Final Batch Loss: 0.018072552978992462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3916, Loss: 0.06595421209931374, Final Batch Loss: 0.035641349852085114\n",
      "Epoch 3917, Loss: 0.07837187871336937, Final Batch Loss: 0.034330446273088455\n",
      "Epoch 3918, Loss: 0.059033505618572235, Final Batch Loss: 0.028435874730348587\n",
      "Epoch 3919, Loss: 0.07476158440113068, Final Batch Loss: 0.04178888350725174\n",
      "Epoch 3920, Loss: 0.06790000572800636, Final Batch Loss: 0.04222022742033005\n",
      "Epoch 3921, Loss: 0.07906543463468552, Final Batch Loss: 0.02831568941473961\n",
      "Epoch 3922, Loss: 0.055973680689930916, Final Batch Loss: 0.026766320690512657\n",
      "Epoch 3923, Loss: 0.08589095994830132, Final Batch Loss: 0.047523003071546555\n",
      "Epoch 3924, Loss: 0.05461881309747696, Final Batch Loss: 0.02001103386282921\n",
      "Epoch 3925, Loss: 0.08514165878295898, Final Batch Loss: 0.034279100596904755\n",
      "Epoch 3926, Loss: 0.07367897778749466, Final Batch Loss: 0.01884138211607933\n",
      "Epoch 3927, Loss: 0.07689102180302143, Final Batch Loss: 0.029458923265337944\n",
      "Epoch 3928, Loss: 0.05858745612204075, Final Batch Loss: 0.03070661798119545\n",
      "Epoch 3929, Loss: 0.08750930801033974, Final Batch Loss: 0.05037001892924309\n",
      "Epoch 3930, Loss: 0.07220703735947609, Final Batch Loss: 0.034795936197042465\n",
      "Epoch 3931, Loss: 0.07284562103450298, Final Batch Loss: 0.024763645604252815\n",
      "Epoch 3932, Loss: 0.11565579101443291, Final Batch Loss: 0.04687109217047691\n",
      "Epoch 3933, Loss: 0.06184312514960766, Final Batch Loss: 0.028307942673563957\n",
      "Epoch 3934, Loss: 0.09993839263916016, Final Batch Loss: 0.06926504522562027\n",
      "Epoch 3935, Loss: 0.050083767622709274, Final Batch Loss: 0.030409498140215874\n",
      "Epoch 3936, Loss: 0.06262722425162792, Final Batch Loss: 0.029959117993712425\n",
      "Epoch 3937, Loss: 0.06252528540790081, Final Batch Loss: 0.03360436111688614\n",
      "Epoch 3938, Loss: 0.05675150081515312, Final Batch Loss: 0.03597652539610863\n",
      "Epoch 3939, Loss: 0.05245407111942768, Final Batch Loss: 0.03272480517625809\n",
      "Epoch 3940, Loss: 0.06294028833508492, Final Batch Loss: 0.019673481583595276\n",
      "Epoch 3941, Loss: 0.05719876103103161, Final Batch Loss: 0.024078326299786568\n",
      "Epoch 3942, Loss: 0.04639400541782379, Final Batch Loss: 0.027264125645160675\n",
      "Epoch 3943, Loss: 0.05944149009883404, Final Batch Loss: 0.02015276439487934\n",
      "Epoch 3944, Loss: 0.07331545650959015, Final Batch Loss: 0.0401579886674881\n",
      "Epoch 3945, Loss: 0.07993269711732864, Final Batch Loss: 0.04804319888353348\n",
      "Epoch 3946, Loss: 0.1159625705331564, Final Batch Loss: 0.09752979129552841\n",
      "Epoch 3947, Loss: 0.07775251194834709, Final Batch Loss: 0.04476633667945862\n",
      "Epoch 3948, Loss: 0.11213308572769165, Final Batch Loss: 0.030828796327114105\n",
      "Epoch 3949, Loss: 0.05582829378545284, Final Batch Loss: 0.025459710508584976\n",
      "Epoch 3950, Loss: 0.08032677322626114, Final Batch Loss: 0.046342235058546066\n",
      "Epoch 3951, Loss: 0.07045388221740723, Final Batch Loss: 0.03819439187645912\n",
      "Epoch 3952, Loss: 0.0665101706981659, Final Batch Loss: 0.04316382855176926\n",
      "Epoch 3953, Loss: 0.050319431349635124, Final Batch Loss: 0.02276771329343319\n",
      "Epoch 3954, Loss: 0.09677644073963165, Final Batch Loss: 0.038550011813640594\n",
      "Epoch 3955, Loss: 0.06165671907365322, Final Batch Loss: 0.03887815773487091\n",
      "Epoch 3956, Loss: 0.0652406606823206, Final Batch Loss: 0.02004110999405384\n",
      "Epoch 3957, Loss: 0.07235100865364075, Final Batch Loss: 0.022052466869354248\n",
      "Epoch 3958, Loss: 0.0666850134730339, Final Batch Loss: 0.03449055179953575\n",
      "Epoch 3959, Loss: 0.08611686155200005, Final Batch Loss: 0.03949916362762451\n",
      "Epoch 3960, Loss: 0.09497349336743355, Final Batch Loss: 0.06619668751955032\n",
      "Epoch 3961, Loss: 0.06956554017961025, Final Batch Loss: 0.030429990962147713\n",
      "Epoch 3962, Loss: 0.10864235833287239, Final Batch Loss: 0.04733821377158165\n",
      "Epoch 3963, Loss: 0.1299939639866352, Final Batch Loss: 0.03379596397280693\n",
      "Epoch 3964, Loss: 0.07186350598931313, Final Batch Loss: 0.047679539769887924\n",
      "Epoch 3965, Loss: 0.09869688376784325, Final Batch Loss: 0.05662605166435242\n",
      "Epoch 3966, Loss: 0.0667393859475851, Final Batch Loss: 0.03995383530855179\n",
      "Epoch 3967, Loss: 0.07287133485078812, Final Batch Loss: 0.03747883439064026\n",
      "Epoch 3968, Loss: 0.061889393255114555, Final Batch Loss: 0.045450083911418915\n",
      "Epoch 3969, Loss: 0.10598007589578629, Final Batch Loss: 0.038961656391620636\n",
      "Epoch 3970, Loss: 0.10126296058297157, Final Batch Loss: 0.040999215096235275\n",
      "Epoch 3971, Loss: 0.07946358248591423, Final Batch Loss: 0.03962303698062897\n",
      "Epoch 3972, Loss: 0.11647714301943779, Final Batch Loss: 0.06483127921819687\n",
      "Epoch 3973, Loss: 0.14231597632169724, Final Batch Loss: 0.1033702865242958\n",
      "Epoch 3974, Loss: 0.1338289938867092, Final Batch Loss: 0.053470905870199203\n",
      "Epoch 3975, Loss: 0.13927744887769222, Final Batch Loss: 0.11135502904653549\n",
      "Epoch 3976, Loss: 0.2087986022233963, Final Batch Loss: 0.08475552499294281\n",
      "Epoch 3977, Loss: 0.20808634907007217, Final Batch Loss: 0.14473608136177063\n",
      "Epoch 3978, Loss: 0.09372088685631752, Final Batch Loss: 0.054933592677116394\n",
      "Epoch 3979, Loss: 0.0955638475716114, Final Batch Loss: 0.053557224571704865\n",
      "Epoch 3980, Loss: 0.1146879494190216, Final Batch Loss: 0.03946620970964432\n",
      "Epoch 3981, Loss: 0.1264692172408104, Final Batch Loss: 0.07736098021268845\n",
      "Epoch 3982, Loss: 0.09755098819732666, Final Batch Loss: 0.057028647512197495\n",
      "Epoch 3983, Loss: 0.1505376249551773, Final Batch Loss: 0.0850728452205658\n",
      "Epoch 3984, Loss: 0.08273129910230637, Final Batch Loss: 0.036563098430633545\n",
      "Epoch 3985, Loss: 0.07714184001088142, Final Batch Loss: 0.046677205711603165\n",
      "Epoch 3986, Loss: 0.08352842554450035, Final Batch Loss: 0.02928636223077774\n",
      "Epoch 3987, Loss: 0.1474164053797722, Final Batch Loss: 0.09285615384578705\n",
      "Epoch 3988, Loss: 0.08524532616138458, Final Batch Loss: 0.03820785507559776\n",
      "Epoch 3989, Loss: 0.060843247920274734, Final Batch Loss: 0.02033732831478119\n",
      "Epoch 3990, Loss: 0.14062638208270073, Final Batch Loss: 0.08170929551124573\n",
      "Epoch 3991, Loss: 0.10194259881973267, Final Batch Loss: 0.06883153319358826\n",
      "Epoch 3992, Loss: 0.08675182610750198, Final Batch Loss: 0.028859876096248627\n",
      "Epoch 3993, Loss: 0.08725324273109436, Final Batch Loss: 0.03378660976886749\n",
      "Epoch 3994, Loss: 0.10564787313342094, Final Batch Loss: 0.07810623198747635\n",
      "Epoch 3995, Loss: 0.09468019008636475, Final Batch Loss: 0.05103917047381401\n",
      "Epoch 3996, Loss: 0.06869878992438316, Final Batch Loss: 0.036846645176410675\n",
      "Epoch 3997, Loss: 0.1414823867380619, Final Batch Loss: 0.044303420931100845\n",
      "Epoch 3998, Loss: 0.16263779997825623, Final Batch Loss: 0.06957712769508362\n",
      "Epoch 3999, Loss: 0.10853242129087448, Final Batch Loss: 0.06129808351397514\n",
      "Epoch 4000, Loss: 0.10753799602389336, Final Batch Loss: 0.06241458281874657\n",
      "Epoch 4001, Loss: 0.07635119929909706, Final Batch Loss: 0.043349314481019974\n",
      "Epoch 4002, Loss: 0.08744167163968086, Final Batch Loss: 0.031716302037239075\n",
      "Epoch 4003, Loss: 0.10895125567913055, Final Batch Loss: 0.05307605490088463\n",
      "Epoch 4004, Loss: 0.10319831036031246, Final Batch Loss: 0.029881445690989494\n",
      "Epoch 4005, Loss: 0.09617846831679344, Final Batch Loss: 0.03350667282938957\n",
      "Epoch 4006, Loss: 0.1330096460878849, Final Batch Loss: 0.09269370883703232\n",
      "Epoch 4007, Loss: 0.11008750833570957, Final Batch Loss: 0.09073091298341751\n",
      "Epoch 4008, Loss: 0.12049375101923943, Final Batch Loss: 0.048722464591264725\n",
      "Epoch 4009, Loss: 0.08683352824300528, Final Batch Loss: 0.014866254292428493\n",
      "Epoch 4010, Loss: 0.07415243238210678, Final Batch Loss: 0.03275561332702637\n",
      "Epoch 4011, Loss: 0.15275055170059204, Final Batch Loss: 0.10908417403697968\n",
      "Epoch 4012, Loss: 0.11035649850964546, Final Batch Loss: 0.039426837116479874\n",
      "Epoch 4013, Loss: 0.08544660359621048, Final Batch Loss: 0.04109220206737518\n",
      "Epoch 4014, Loss: 0.10397226735949516, Final Batch Loss: 0.06882568448781967\n",
      "Epoch 4015, Loss: 0.08187820948660374, Final Batch Loss: 0.021861637011170387\n",
      "Epoch 4016, Loss: 0.089344572275877, Final Batch Loss: 0.058619577437639236\n",
      "Epoch 4017, Loss: 0.19139548763632774, Final Batch Loss: 0.0475861094892025\n",
      "Epoch 4018, Loss: 0.09136001020669937, Final Batch Loss: 0.03798432648181915\n",
      "Epoch 4019, Loss: 0.08264751173555851, Final Batch Loss: 0.029431240633130074\n",
      "Epoch 4020, Loss: 0.06661787070333958, Final Batch Loss: 0.050299160182476044\n",
      "Epoch 4021, Loss: 0.09424251317977905, Final Batch Loss: 0.05996410921216011\n",
      "Epoch 4022, Loss: 0.08827466890215874, Final Batch Loss: 0.03489135205745697\n",
      "Epoch 4023, Loss: 0.08942210301756859, Final Batch Loss: 0.04861943796277046\n",
      "Epoch 4024, Loss: 0.07556748948991299, Final Batch Loss: 0.023279963061213493\n",
      "Epoch 4025, Loss: 0.06412133388221264, Final Batch Loss: 0.038648705929517746\n",
      "Epoch 4026, Loss: 0.07502553798258305, Final Batch Loss: 0.03047308139503002\n",
      "Epoch 4027, Loss: 0.065779073163867, Final Batch Loss: 0.022021980956196785\n",
      "Epoch 4028, Loss: 0.0691365972161293, Final Batch Loss: 0.02510765567421913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4029, Loss: 0.08571777120232582, Final Batch Loss: 0.03687664866447449\n",
      "Epoch 4030, Loss: 0.11154982075095177, Final Batch Loss: 0.04840243235230446\n",
      "Epoch 4031, Loss: 0.07592169009149075, Final Batch Loss: 0.047091856598854065\n",
      "Epoch 4032, Loss: 0.12124333530664444, Final Batch Loss: 0.08225686103105545\n",
      "Epoch 4033, Loss: 0.11261886730790138, Final Batch Loss: 0.032665055245161057\n",
      "Epoch 4034, Loss: 0.08700217120349407, Final Batch Loss: 0.030006611719727516\n",
      "Epoch 4035, Loss: 0.08942464925348759, Final Batch Loss: 0.05983220785856247\n",
      "Epoch 4036, Loss: 0.0713136475533247, Final Batch Loss: 0.04046407714486122\n",
      "Epoch 4037, Loss: 0.0729265809059143, Final Batch Loss: 0.02986154705286026\n",
      "Epoch 4038, Loss: 0.059334563091397285, Final Batch Loss: 0.02127997763454914\n",
      "Epoch 4039, Loss: 0.08819897472858429, Final Batch Loss: 0.047576066106557846\n",
      "Epoch 4040, Loss: 0.07389336824417114, Final Batch Loss: 0.042286016047000885\n",
      "Epoch 4041, Loss: 0.09166591241955757, Final Batch Loss: 0.04372921213507652\n",
      "Epoch 4042, Loss: 0.059552768245339394, Final Batch Loss: 0.03315342962741852\n",
      "Epoch 4043, Loss: 0.06269959919154644, Final Batch Loss: 0.034578822553157806\n",
      "Epoch 4044, Loss: 0.07659433409571648, Final Batch Loss: 0.034335169941186905\n",
      "Epoch 4045, Loss: 0.09086043387651443, Final Batch Loss: 0.04208661615848541\n",
      "Epoch 4046, Loss: 0.058696163818240166, Final Batch Loss: 0.02496391348540783\n",
      "Epoch 4047, Loss: 0.07714980840682983, Final Batch Loss: 0.04440535604953766\n",
      "Epoch 4048, Loss: 0.1002718098461628, Final Batch Loss: 0.058623190969228745\n",
      "Epoch 4049, Loss: 0.08570501953363419, Final Batch Loss: 0.053799089044332504\n",
      "Epoch 4050, Loss: 0.07730504497885704, Final Batch Loss: 0.04035382717847824\n",
      "Epoch 4051, Loss: 0.14723453670740128, Final Batch Loss: 0.08362460881471634\n",
      "Epoch 4052, Loss: 0.09052755683660507, Final Batch Loss: 0.04851146414875984\n",
      "Epoch 4053, Loss: 0.06935815699398518, Final Batch Loss: 0.02890634723007679\n",
      "Epoch 4054, Loss: 0.08817372471094131, Final Batch Loss: 0.04413260519504547\n",
      "Epoch 4055, Loss: 0.06885362416505814, Final Batch Loss: 0.02880920097231865\n",
      "Epoch 4056, Loss: 0.07690304145216942, Final Batch Loss: 0.03574257716536522\n",
      "Epoch 4057, Loss: 0.08729637786746025, Final Batch Loss: 0.04267193377017975\n",
      "Epoch 4058, Loss: 0.09140835702419281, Final Batch Loss: 0.03907475993037224\n",
      "Epoch 4059, Loss: 0.11896886676549911, Final Batch Loss: 0.08694499731063843\n",
      "Epoch 4060, Loss: 0.07700176537036896, Final Batch Loss: 0.038742851465940475\n",
      "Epoch 4061, Loss: 0.10341143980622292, Final Batch Loss: 0.06105499714612961\n",
      "Epoch 4062, Loss: 0.10323657467961311, Final Batch Loss: 0.049736447632312775\n",
      "Epoch 4063, Loss: 0.08817334845662117, Final Batch Loss: 0.04337475076317787\n",
      "Epoch 4064, Loss: 0.062181539833545685, Final Batch Loss: 0.02601877599954605\n",
      "Epoch 4065, Loss: 0.08645341545343399, Final Batch Loss: 0.04729628935456276\n",
      "Epoch 4066, Loss: 0.1125565804541111, Final Batch Loss: 0.07360588014125824\n",
      "Epoch 4067, Loss: 0.07843289896845818, Final Batch Loss: 0.05387284606695175\n",
      "Epoch 4068, Loss: 0.0899935383349657, Final Batch Loss: 0.061004359275102615\n",
      "Epoch 4069, Loss: 0.11301176249980927, Final Batch Loss: 0.0489204078912735\n",
      "Epoch 4070, Loss: 0.0699254646897316, Final Batch Loss: 0.031545963138341904\n",
      "Epoch 4071, Loss: 0.08454864844679832, Final Batch Loss: 0.04041692614555359\n",
      "Epoch 4072, Loss: 0.07477598264813423, Final Batch Loss: 0.02813541516661644\n",
      "Epoch 4073, Loss: 0.06997946463525295, Final Batch Loss: 0.030966205522418022\n",
      "Epoch 4074, Loss: 0.0769927091896534, Final Batch Loss: 0.04293131083250046\n",
      "Epoch 4075, Loss: 0.08151853084564209, Final Batch Loss: 0.03677811846137047\n",
      "Epoch 4076, Loss: 0.078623928129673, Final Batch Loss: 0.032302066683769226\n",
      "Epoch 4077, Loss: 0.06064989045262337, Final Batch Loss: 0.04621381312608719\n",
      "Epoch 4078, Loss: 0.08989238739013672, Final Batch Loss: 0.05870545655488968\n",
      "Epoch 4079, Loss: 0.07911065220832825, Final Batch Loss: 0.04834115132689476\n",
      "Epoch 4080, Loss: 0.09218420088291168, Final Batch Loss: 0.0284588560461998\n",
      "Epoch 4081, Loss: 0.0659597497433424, Final Batch Loss: 0.04223927482962608\n",
      "Epoch 4082, Loss: 0.07701239362359047, Final Batch Loss: 0.03134381026029587\n",
      "Epoch 4083, Loss: 0.05919583514332771, Final Batch Loss: 0.033592939376831055\n",
      "Epoch 4084, Loss: 0.07420047000050545, Final Batch Loss: 0.04962530359625816\n",
      "Epoch 4085, Loss: 0.06740837544202805, Final Batch Loss: 0.027063965797424316\n",
      "Epoch 4086, Loss: 0.06274956278502941, Final Batch Loss: 0.0356886200606823\n",
      "Epoch 4087, Loss: 0.08487045392394066, Final Batch Loss: 0.05156391113996506\n",
      "Epoch 4088, Loss: 0.07773211598396301, Final Batch Loss: 0.057616379112005234\n",
      "Epoch 4089, Loss: 0.07486271113157272, Final Batch Loss: 0.03622981905937195\n",
      "Epoch 4090, Loss: 0.09263847768306732, Final Batch Loss: 0.023981913924217224\n",
      "Epoch 4091, Loss: 0.1389361470937729, Final Batch Loss: 0.02877037227153778\n",
      "Epoch 4092, Loss: 0.07764645293354988, Final Batch Loss: 0.0447254404425621\n",
      "Epoch 4093, Loss: 0.07398397475481033, Final Batch Loss: 0.03925987705588341\n",
      "Epoch 4094, Loss: 0.08102886192500591, Final Batch Loss: 0.051983993500471115\n",
      "Epoch 4095, Loss: 0.06661323271691799, Final Batch Loss: 0.03651930391788483\n",
      "Epoch 4096, Loss: 0.06001628004014492, Final Batch Loss: 0.038541074842214584\n",
      "Epoch 4097, Loss: 0.0690725576132536, Final Batch Loss: 0.029247725382447243\n",
      "Epoch 4098, Loss: 0.10435784608125687, Final Batch Loss: 0.05769239366054535\n",
      "Epoch 4099, Loss: 0.08496620506048203, Final Batch Loss: 0.037477441132068634\n",
      "Epoch 4100, Loss: 0.06371447630226612, Final Batch Loss: 0.02803896926343441\n",
      "Epoch 4101, Loss: 0.060484929010272026, Final Batch Loss: 0.03811166435480118\n",
      "Epoch 4102, Loss: 0.07414832152426243, Final Batch Loss: 0.028881872072815895\n",
      "Epoch 4103, Loss: 0.09758291020989418, Final Batch Loss: 0.04932061955332756\n",
      "Epoch 4104, Loss: 0.0747075267136097, Final Batch Loss: 0.03141198679804802\n",
      "Epoch 4105, Loss: 0.07953833788633347, Final Batch Loss: 0.05025434494018555\n",
      "Epoch 4106, Loss: 0.06473774835467339, Final Batch Loss: 0.03918158635497093\n",
      "Epoch 4107, Loss: 0.07696039602160454, Final Batch Loss: 0.03287883475422859\n",
      "Epoch 4108, Loss: 0.07732047513127327, Final Batch Loss: 0.05712060630321503\n",
      "Epoch 4109, Loss: 0.07135825417935848, Final Batch Loss: 0.04089217633008957\n",
      "Epoch 4110, Loss: 0.07589767314493656, Final Batch Loss: 0.027941541746258736\n",
      "Epoch 4111, Loss: 0.1367802880704403, Final Batch Loss: 0.08952070027589798\n",
      "Epoch 4112, Loss: 0.07818590849637985, Final Batch Loss: 0.04742034152150154\n",
      "Epoch 4113, Loss: 0.08771291375160217, Final Batch Loss: 0.0259082168340683\n",
      "Epoch 4114, Loss: 0.07423125579953194, Final Batch Loss: 0.032464317977428436\n",
      "Epoch 4115, Loss: 0.07904842123389244, Final Batch Loss: 0.05509312450885773\n",
      "Epoch 4116, Loss: 0.11516692116856575, Final Batch Loss: 0.04862048849463463\n",
      "Epoch 4117, Loss: 0.10684595629572868, Final Batch Loss: 0.05414927005767822\n",
      "Epoch 4118, Loss: 0.11542383953928947, Final Batch Loss: 0.07418624311685562\n",
      "Epoch 4119, Loss: 0.06619535945355892, Final Batch Loss: 0.029473474249243736\n",
      "Epoch 4120, Loss: 0.12884896248579025, Final Batch Loss: 0.03153345733880997\n",
      "Epoch 4121, Loss: 0.1756007894873619, Final Batch Loss: 0.11942828446626663\n",
      "Epoch 4122, Loss: 0.11779360100626945, Final Batch Loss: 0.07330464571714401\n",
      "Epoch 4123, Loss: 0.2111581489443779, Final Batch Loss: 0.10931402444839478\n",
      "Epoch 4124, Loss: 0.1358608603477478, Final Batch Loss: 0.04254399240016937\n",
      "Epoch 4125, Loss: 0.0964452400803566, Final Batch Loss: 0.03985900059342384\n",
      "Epoch 4126, Loss: 0.1498010940849781, Final Batch Loss: 0.08818347007036209\n",
      "Epoch 4127, Loss: 0.15404094010591507, Final Batch Loss: 0.049474067986011505\n",
      "Epoch 4128, Loss: 0.11102570220828056, Final Batch Loss: 0.03914107754826546\n",
      "Epoch 4129, Loss: 0.130583168938756, Final Batch Loss: 0.022049883380532265\n",
      "Epoch 4130, Loss: 0.16634655743837357, Final Batch Loss: 0.11995631456375122\n",
      "Epoch 4131, Loss: 0.12180468812584877, Final Batch Loss: 0.06176729500293732\n",
      "Epoch 4132, Loss: 0.09905169904232025, Final Batch Loss: 0.04859044402837753\n",
      "Epoch 4133, Loss: 0.08456744998693466, Final Batch Loss: 0.04822812229394913\n",
      "Epoch 4134, Loss: 0.16315633058547974, Final Batch Loss: 0.07777132838964462\n",
      "Epoch 4135, Loss: 0.10528858751058578, Final Batch Loss: 0.02936488389968872\n",
      "Epoch 4136, Loss: 0.09600426629185677, Final Batch Loss: 0.053124088793992996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4137, Loss: 0.21865078061819077, Final Batch Loss: 0.041839130222797394\n",
      "Epoch 4138, Loss: 0.08805988356471062, Final Batch Loss: 0.04503730311989784\n",
      "Epoch 4139, Loss: 0.10962279140949249, Final Batch Loss: 0.04955146834254265\n",
      "Epoch 4140, Loss: 0.081967793405056, Final Batch Loss: 0.03882080689072609\n",
      "Epoch 4141, Loss: 0.10270322486758232, Final Batch Loss: 0.05006631836295128\n",
      "Epoch 4142, Loss: 0.11576530523598194, Final Batch Loss: 0.030163662508130074\n",
      "Epoch 4143, Loss: 0.06128546968102455, Final Batch Loss: 0.020430423319339752\n",
      "Epoch 4144, Loss: 0.07261443510651588, Final Batch Loss: 0.03436293825507164\n",
      "Epoch 4145, Loss: 0.09373504295945168, Final Batch Loss: 0.051336027681827545\n",
      "Epoch 4146, Loss: 0.07235616073012352, Final Batch Loss: 0.03179772198200226\n",
      "Epoch 4147, Loss: 0.11934733018279076, Final Batch Loss: 0.05214493349194527\n",
      "Epoch 4148, Loss: 0.0861678384244442, Final Batch Loss: 0.042391467839479446\n",
      "Epoch 4149, Loss: 0.0754982978105545, Final Batch Loss: 0.03321286663413048\n",
      "Epoch 4150, Loss: 0.084722850471735, Final Batch Loss: 0.04151380434632301\n",
      "Epoch 4151, Loss: 0.06674897857010365, Final Batch Loss: 0.04612424597144127\n",
      "Epoch 4152, Loss: 0.09792555496096611, Final Batch Loss: 0.05478201061487198\n",
      "Epoch 4153, Loss: 0.0625660140067339, Final Batch Loss: 0.03906024247407913\n",
      "Epoch 4154, Loss: 0.07258807122707367, Final Batch Loss: 0.0386357307434082\n",
      "Epoch 4155, Loss: 0.10618268325924873, Final Batch Loss: 0.05556873977184296\n",
      "Epoch 4156, Loss: 0.0785360261797905, Final Batch Loss: 0.03771338239312172\n",
      "Epoch 4157, Loss: 0.08219063654541969, Final Batch Loss: 0.04018457606434822\n",
      "Epoch 4158, Loss: 0.08278684318065643, Final Batch Loss: 0.03825532644987106\n",
      "Epoch 4159, Loss: 0.1265830472111702, Final Batch Loss: 0.03315143287181854\n",
      "Epoch 4160, Loss: 0.11522846296429634, Final Batch Loss: 0.08235686272382736\n",
      "Epoch 4161, Loss: 0.11486826464533806, Final Batch Loss: 0.07495773583650589\n",
      "Epoch 4162, Loss: 0.14468670263886452, Final Batch Loss: 0.11679991334676743\n",
      "Epoch 4163, Loss: 0.09694764763116837, Final Batch Loss: 0.05771290510892868\n",
      "Epoch 4164, Loss: 0.11809205263853073, Final Batch Loss: 0.050213269889354706\n",
      "Epoch 4165, Loss: 0.09647022932767868, Final Batch Loss: 0.043741513043642044\n",
      "Epoch 4166, Loss: 0.0670114029198885, Final Batch Loss: 0.03962721303105354\n",
      "Epoch 4167, Loss: 0.07633442059159279, Final Batch Loss: 0.03653851896524429\n",
      "Epoch 4168, Loss: 0.11638501659035683, Final Batch Loss: 0.07317939400672913\n",
      "Epoch 4169, Loss: 0.1000420618802309, Final Batch Loss: 0.07513938099145889\n",
      "Epoch 4170, Loss: 0.07788034155964851, Final Batch Loss: 0.03997757285833359\n",
      "Epoch 4171, Loss: 0.10034136846661568, Final Batch Loss: 0.05679323524236679\n",
      "Epoch 4172, Loss: 0.2494761049747467, Final Batch Loss: 0.11099174618721008\n",
      "Epoch 4173, Loss: 0.09457663260400295, Final Batch Loss: 0.02771301381289959\n",
      "Epoch 4174, Loss: 0.08661410957574844, Final Batch Loss: 0.02469789609313011\n",
      "Epoch 4175, Loss: 0.0764471385627985, Final Batch Loss: 0.023983417078852654\n",
      "Epoch 4176, Loss: 0.07646321132779121, Final Batch Loss: 0.0460871197283268\n",
      "Epoch 4177, Loss: 0.07131221890449524, Final Batch Loss: 0.017766110599040985\n",
      "Epoch 4178, Loss: 0.08609008975327015, Final Batch Loss: 0.029397325590252876\n",
      "Epoch 4179, Loss: 0.07308758981525898, Final Batch Loss: 0.029951082542538643\n",
      "Epoch 4180, Loss: 0.15437927842140198, Final Batch Loss: 0.08957524597644806\n",
      "Epoch 4181, Loss: 0.07725861296057701, Final Batch Loss: 0.04467829689383507\n",
      "Epoch 4182, Loss: 0.0905386209487915, Final Batch Loss: 0.03962349519133568\n",
      "Epoch 4183, Loss: 0.06990605220198631, Final Batch Loss: 0.034842412918806076\n",
      "Epoch 4184, Loss: 0.0817849151790142, Final Batch Loss: 0.05543806031346321\n",
      "Epoch 4185, Loss: 0.06725130416452885, Final Batch Loss: 0.02608775906264782\n",
      "Epoch 4186, Loss: 0.06529554165899754, Final Batch Loss: 0.031044168397784233\n",
      "Epoch 4187, Loss: 0.07620914280414581, Final Batch Loss: 0.03439278528094292\n",
      "Epoch 4188, Loss: 0.06498940661549568, Final Batch Loss: 0.03153872862458229\n",
      "Epoch 4189, Loss: 0.07548482343554497, Final Batch Loss: 0.036598797887563705\n",
      "Epoch 4190, Loss: 0.11799808591604233, Final Batch Loss: 0.07838121056556702\n",
      "Epoch 4191, Loss: 0.0751480683684349, Final Batch Loss: 0.05007229745388031\n",
      "Epoch 4192, Loss: 0.07320383936166763, Final Batch Loss: 0.038694385439157486\n",
      "Epoch 4193, Loss: 0.07535285502672195, Final Batch Loss: 0.03555519878864288\n",
      "Epoch 4194, Loss: 0.06953245587646961, Final Batch Loss: 0.0393044538795948\n",
      "Epoch 4195, Loss: 0.08336520195007324, Final Batch Loss: 0.042579106986522675\n",
      "Epoch 4196, Loss: 0.06142340041697025, Final Batch Loss: 0.028714952990412712\n",
      "Epoch 4197, Loss: 0.05198599956929684, Final Batch Loss: 0.01913801021873951\n",
      "Epoch 4198, Loss: 0.09371650218963623, Final Batch Loss: 0.06717214733362198\n",
      "Epoch 4199, Loss: 0.10675157234072685, Final Batch Loss: 0.05346614494919777\n",
      "Epoch 4200, Loss: 0.09538041800260544, Final Batch Loss: 0.06396633386611938\n",
      "Epoch 4201, Loss: 0.061582233756780624, Final Batch Loss: 0.02487834542989731\n",
      "Epoch 4202, Loss: 0.0680856928229332, Final Batch Loss: 0.03330906480550766\n",
      "Epoch 4203, Loss: 0.06178147718310356, Final Batch Loss: 0.03924941644072533\n",
      "Epoch 4204, Loss: 0.06887836195528507, Final Batch Loss: 0.038653675466775894\n",
      "Epoch 4205, Loss: 0.07117871195077896, Final Batch Loss: 0.03609388694167137\n",
      "Epoch 4206, Loss: 0.0614179540425539, Final Batch Loss: 0.03291762247681618\n",
      "Epoch 4207, Loss: 0.06252674758434296, Final Batch Loss: 0.0377371609210968\n",
      "Epoch 4208, Loss: 0.07679995149374008, Final Batch Loss: 0.0335991345345974\n",
      "Epoch 4209, Loss: 0.13658008351922035, Final Batch Loss: 0.1151776984333992\n",
      "Epoch 4210, Loss: 0.08696550130844116, Final Batch Loss: 0.050134919583797455\n",
      "Epoch 4211, Loss: 0.08292515203356743, Final Batch Loss: 0.04891009256243706\n",
      "Epoch 4212, Loss: 0.08936264738440514, Final Batch Loss: 0.05555291846394539\n",
      "Epoch 4213, Loss: 0.10045607946813107, Final Batch Loss: 0.024357354268431664\n",
      "Epoch 4214, Loss: 0.06901280209422112, Final Batch Loss: 0.04387468844652176\n",
      "Epoch 4215, Loss: 0.06711332313716412, Final Batch Loss: 0.03962954506278038\n",
      "Epoch 4216, Loss: 0.05995793454349041, Final Batch Loss: 0.02497493289411068\n",
      "Epoch 4217, Loss: 0.07206789404153824, Final Batch Loss: 0.039857786148786545\n",
      "Epoch 4218, Loss: 0.06072574853897095, Final Batch Loss: 0.04007837921380997\n",
      "Epoch 4219, Loss: 0.0646533165127039, Final Batch Loss: 0.028528472408652306\n",
      "Epoch 4220, Loss: 0.09608768485486507, Final Batch Loss: 0.06851846724748611\n",
      "Epoch 4221, Loss: 0.08707382157444954, Final Batch Loss: 0.024567458778619766\n",
      "Epoch 4222, Loss: 0.1061290092766285, Final Batch Loss: 0.04521649703383446\n",
      "Epoch 4223, Loss: 0.06711879000067711, Final Batch Loss: 0.034815434366464615\n",
      "Epoch 4224, Loss: 0.10335629433393478, Final Batch Loss: 0.04792073369026184\n",
      "Epoch 4225, Loss: 0.0656516794115305, Final Batch Loss: 0.03115215338766575\n",
      "Epoch 4226, Loss: 0.06876932084560394, Final Batch Loss: 0.03715179115533829\n",
      "Epoch 4227, Loss: 0.07592209056019783, Final Batch Loss: 0.051819853484630585\n",
      "Epoch 4228, Loss: 0.1008056290447712, Final Batch Loss: 0.057861313223838806\n",
      "Epoch 4229, Loss: 0.06307649612426758, Final Batch Loss: 0.03558731824159622\n",
      "Epoch 4230, Loss: 0.06478726863861084, Final Batch Loss: 0.03239573910832405\n",
      "Epoch 4231, Loss: 0.06232352554798126, Final Batch Loss: 0.02702545002102852\n",
      "Epoch 4232, Loss: 0.06406700424849987, Final Batch Loss: 0.043889738619327545\n",
      "Epoch 4233, Loss: 0.07911229133605957, Final Batch Loss: 0.0315752848982811\n",
      "Epoch 4234, Loss: 0.1057015135884285, Final Batch Loss: 0.038086771965026855\n",
      "Epoch 4235, Loss: 0.0676583144813776, Final Batch Loss: 0.041156236082315445\n",
      "Epoch 4236, Loss: 0.07590083032846451, Final Batch Loss: 0.04810028895735741\n",
      "Epoch 4237, Loss: 0.07343069277703762, Final Batch Loss: 0.04543532431125641\n",
      "Epoch 4238, Loss: 0.06366881355643272, Final Batch Loss: 0.022923197597265244\n",
      "Epoch 4239, Loss: 0.06606327928602695, Final Batch Loss: 0.04560568556189537\n",
      "Epoch 4240, Loss: 0.07621827349066734, Final Batch Loss: 0.04491814598441124\n",
      "Epoch 4241, Loss: 0.06617625430226326, Final Batch Loss: 0.044684574007987976\n",
      "Epoch 4242, Loss: 0.07434888184070587, Final Batch Loss: 0.04515404999256134\n",
      "Epoch 4243, Loss: 0.0770655982196331, Final Batch Loss: 0.041922662407159805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4244, Loss: 0.08708955347537994, Final Batch Loss: 0.034386370331048965\n",
      "Epoch 4245, Loss: 0.07204843126237392, Final Batch Loss: 0.027064124122262\n",
      "Epoch 4246, Loss: 0.06870081834495068, Final Batch Loss: 0.012100337073206902\n",
      "Epoch 4247, Loss: 0.07846208103001118, Final Batch Loss: 0.030797598883509636\n",
      "Epoch 4248, Loss: 0.09257803857326508, Final Batch Loss: 0.04148873686790466\n",
      "Epoch 4249, Loss: 0.07764127478003502, Final Batch Loss: 0.04126589745283127\n",
      "Epoch 4250, Loss: 0.11564574390649796, Final Batch Loss: 0.07172852009534836\n",
      "Epoch 4251, Loss: 0.10808435268700123, Final Batch Loss: 0.08279086649417877\n",
      "Epoch 4252, Loss: 0.0829598642885685, Final Batch Loss: 0.04534595087170601\n",
      "Epoch 4253, Loss: 0.06940334476530552, Final Batch Loss: 0.04232064262032509\n",
      "Epoch 4254, Loss: 0.06774461269378662, Final Batch Loss: 0.03642933815717697\n",
      "Epoch 4255, Loss: 0.07045870646834373, Final Batch Loss: 0.0323888324201107\n",
      "Epoch 4256, Loss: 0.06913182139396667, Final Batch Loss: 0.024822898209095\n",
      "Epoch 4257, Loss: 0.06212862394750118, Final Batch Loss: 0.04623603820800781\n",
      "Epoch 4258, Loss: 0.06801813095808029, Final Batch Loss: 0.03388586640357971\n",
      "Epoch 4259, Loss: 0.1424884833395481, Final Batch Loss: 0.03144088760018349\n",
      "Epoch 4260, Loss: 0.06131426431238651, Final Batch Loss: 0.025826556608080864\n",
      "Epoch 4261, Loss: 0.06800775788724422, Final Batch Loss: 0.038789696991443634\n",
      "Epoch 4262, Loss: 0.07075944170355797, Final Batch Loss: 0.034967925399541855\n",
      "Epoch 4263, Loss: 0.06554101780056953, Final Batch Loss: 0.03325309976935387\n",
      "Epoch 4264, Loss: 0.09498843178153038, Final Batch Loss: 0.04572901502251625\n",
      "Epoch 4265, Loss: 0.10041991993784904, Final Batch Loss: 0.039905767887830734\n",
      "Epoch 4266, Loss: 0.08391335792839527, Final Batch Loss: 0.05615440011024475\n",
      "Epoch 4267, Loss: 0.07493968307971954, Final Batch Loss: 0.028428517282009125\n",
      "Epoch 4268, Loss: 0.0876241959631443, Final Batch Loss: 0.03524378314614296\n",
      "Epoch 4269, Loss: 0.06627882830798626, Final Batch Loss: 0.036792345345020294\n",
      "Epoch 4270, Loss: 0.09947190806269646, Final Batch Loss: 0.04184937849640846\n",
      "Epoch 4271, Loss: 0.1128983348608017, Final Batch Loss: 0.04405200481414795\n",
      "Epoch 4272, Loss: 0.10867105051875114, Final Batch Loss: 0.041712816804647446\n",
      "Epoch 4273, Loss: 0.10804629698395729, Final Batch Loss: 0.04111389443278313\n",
      "Epoch 4274, Loss: 0.07917572371661663, Final Batch Loss: 0.025008296594023705\n",
      "Epoch 4275, Loss: 0.05825044587254524, Final Batch Loss: 0.03248094022274017\n",
      "Epoch 4276, Loss: 0.0829743780195713, Final Batch Loss: 0.05653321370482445\n",
      "Epoch 4277, Loss: 0.10065904632210732, Final Batch Loss: 0.04958169907331467\n",
      "Epoch 4278, Loss: 0.09112054482102394, Final Batch Loss: 0.05057680979371071\n",
      "Epoch 4279, Loss: 0.07082478515803814, Final Batch Loss: 0.02177427150309086\n",
      "Epoch 4280, Loss: 0.06907162629067898, Final Batch Loss: 0.043464403599500656\n",
      "Epoch 4281, Loss: 0.12456031143665314, Final Batch Loss: 0.06490693241357803\n",
      "Epoch 4282, Loss: 0.07934996485710144, Final Batch Loss: 0.040543172508478165\n",
      "Epoch 4283, Loss: 0.06538182124495506, Final Batch Loss: 0.03379623964428902\n",
      "Epoch 4284, Loss: 0.08942162990570068, Final Batch Loss: 0.057195767760276794\n",
      "Epoch 4285, Loss: 0.07761136069893837, Final Batch Loss: 0.03839582949876785\n",
      "Epoch 4286, Loss: 0.07832517474889755, Final Batch Loss: 0.04075076803565025\n",
      "Epoch 4287, Loss: 0.08916252106428146, Final Batch Loss: 0.055802781134843826\n",
      "Epoch 4288, Loss: 0.06430945917963982, Final Batch Loss: 0.03237571567296982\n",
      "Epoch 4289, Loss: 0.07366717979311943, Final Batch Loss: 0.04237991198897362\n",
      "Epoch 4290, Loss: 0.06731950119137764, Final Batch Loss: 0.04462828114628792\n",
      "Epoch 4291, Loss: 0.07057721074670553, Final Batch Loss: 0.0557374507188797\n",
      "Epoch 4292, Loss: 0.10313326492905617, Final Batch Loss: 0.034963663667440414\n",
      "Epoch 4293, Loss: 0.08170656114816666, Final Batch Loss: 0.05022400990128517\n",
      "Epoch 4294, Loss: 0.0854124017059803, Final Batch Loss: 0.038698356598615646\n",
      "Epoch 4295, Loss: 0.09946011006832123, Final Batch Loss: 0.06477677822113037\n",
      "Epoch 4296, Loss: 0.0652643945068121, Final Batch Loss: 0.02831217460334301\n",
      "Epoch 4297, Loss: 0.09117679111659527, Final Batch Loss: 0.026119260117411613\n",
      "Epoch 4298, Loss: 0.1245044469833374, Final Batch Loss: 0.07089389860630035\n",
      "Epoch 4299, Loss: 0.06910341419279575, Final Batch Loss: 0.047027237713336945\n",
      "Epoch 4300, Loss: 0.1021529771387577, Final Batch Loss: 0.04267532378435135\n",
      "Epoch 4301, Loss: 0.09650533273816109, Final Batch Loss: 0.040143709629774094\n",
      "Epoch 4302, Loss: 0.07107654213905334, Final Batch Loss: 0.052773069590330124\n",
      "Epoch 4303, Loss: 0.09892125427722931, Final Batch Loss: 0.052591972053050995\n",
      "Epoch 4304, Loss: 0.1245134249329567, Final Batch Loss: 0.03595151752233505\n",
      "Epoch 4305, Loss: 0.09038005582988262, Final Batch Loss: 0.06234642490744591\n",
      "Epoch 4306, Loss: 0.07361292652785778, Final Batch Loss: 0.030234916135668755\n",
      "Epoch 4307, Loss: 0.08767962455749512, Final Batch Loss: 0.04751598834991455\n",
      "Epoch 4308, Loss: 0.09450310468673706, Final Batch Loss: 0.07218969613313675\n",
      "Epoch 4309, Loss: 0.11588632129132748, Final Batch Loss: 0.030062003061175346\n",
      "Epoch 4310, Loss: 0.08959479257464409, Final Batch Loss: 0.04696282744407654\n",
      "Epoch 4311, Loss: 0.07663558050990105, Final Batch Loss: 0.033922310918569565\n",
      "Epoch 4312, Loss: 0.07180370017886162, Final Batch Loss: 0.025139298290014267\n",
      "Epoch 4313, Loss: 0.10164130479097366, Final Batch Loss: 0.06509488075971603\n",
      "Epoch 4314, Loss: 0.07168612256646156, Final Batch Loss: 0.03333812206983566\n",
      "Epoch 4315, Loss: 0.0674223080277443, Final Batch Loss: 0.03407139703631401\n",
      "Epoch 4316, Loss: 0.0697958767414093, Final Batch Loss: 0.042930930852890015\n",
      "Epoch 4317, Loss: 0.09062248654663563, Final Batch Loss: 0.06151041015982628\n",
      "Epoch 4318, Loss: 0.06980562396347523, Final Batch Loss: 0.02737806923687458\n",
      "Epoch 4319, Loss: 0.062025535851716995, Final Batch Loss: 0.02787771075963974\n",
      "Epoch 4320, Loss: 0.08542000502347946, Final Batch Loss: 0.039358951151371\n",
      "Epoch 4321, Loss: 0.06942047271877527, Final Batch Loss: 0.012571287341415882\n",
      "Epoch 4322, Loss: 0.1841563880443573, Final Batch Loss: 0.03814098238945007\n",
      "Epoch 4323, Loss: 0.10312477871775627, Final Batch Loss: 0.041710373014211655\n",
      "Epoch 4324, Loss: 0.2515037041157484, Final Batch Loss: 0.02995438687503338\n",
      "Epoch 4325, Loss: 0.06665272824466228, Final Batch Loss: 0.02541879378259182\n",
      "Epoch 4326, Loss: 0.0687999278306961, Final Batch Loss: 0.03468412160873413\n",
      "Epoch 4327, Loss: 0.10687143355607986, Final Batch Loss: 0.06953556090593338\n",
      "Epoch 4328, Loss: 0.06827892363071442, Final Batch Loss: 0.03819366917014122\n",
      "Epoch 4329, Loss: 0.0577726773917675, Final Batch Loss: 0.038720887154340744\n",
      "Epoch 4330, Loss: 0.0851447656750679, Final Batch Loss: 0.027006909251213074\n",
      "Epoch 4331, Loss: 0.0657416507601738, Final Batch Loss: 0.031511109322309494\n",
      "Epoch 4332, Loss: 0.08184262365102768, Final Batch Loss: 0.04435141012072563\n",
      "Epoch 4333, Loss: 0.0761868916451931, Final Batch Loss: 0.04664365574717522\n",
      "Epoch 4334, Loss: 0.06579659879207611, Final Batch Loss: 0.028521135449409485\n",
      "Epoch 4335, Loss: 0.08644020557403564, Final Batch Loss: 0.03806300088763237\n",
      "Epoch 4336, Loss: 0.10329101979732513, Final Batch Loss: 0.06928287446498871\n",
      "Epoch 4337, Loss: 0.05263135954737663, Final Batch Loss: 0.03153735026717186\n",
      "Epoch 4338, Loss: 0.058683641254901886, Final Batch Loss: 0.032025158405303955\n",
      "Epoch 4339, Loss: 0.071615070104599, Final Batch Loss: 0.03784502297639847\n",
      "Epoch 4340, Loss: 0.06840284168720245, Final Batch Loss: 0.04345584660768509\n",
      "Epoch 4341, Loss: 0.07076173461973667, Final Batch Loss: 0.021909484639763832\n",
      "Epoch 4342, Loss: 0.11888878419995308, Final Batch Loss: 0.04689542576670647\n",
      "Epoch 4343, Loss: 0.09431304782629013, Final Batch Loss: 0.06951653212308884\n",
      "Epoch 4344, Loss: 0.08691196888685226, Final Batch Loss: 0.05910605564713478\n",
      "Epoch 4345, Loss: 0.07979424297809601, Final Batch Loss: 0.036377713084220886\n",
      "Epoch 4346, Loss: 0.06652896292507648, Final Batch Loss: 0.03921961784362793\n",
      "Epoch 4347, Loss: 0.09002580493688583, Final Batch Loss: 0.03287941962480545\n",
      "Epoch 4348, Loss: 0.06949338503181934, Final Batch Loss: 0.02572096325457096\n",
      "Epoch 4349, Loss: 0.057452162727713585, Final Batch Loss: 0.021782876923680305\n",
      "Epoch 4350, Loss: 0.0843808650970459, Final Batch Loss: 0.023934699594974518\n",
      "Epoch 4351, Loss: 0.07354361936450005, Final Batch Loss: 0.0322992168366909\n",
      "Epoch 4352, Loss: 0.06307138130068779, Final Batch Loss: 0.026893839240074158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4353, Loss: 0.059062933549284935, Final Batch Loss: 0.03622608631849289\n",
      "Epoch 4354, Loss: 0.06521979719400406, Final Batch Loss: 0.030393965542316437\n",
      "Epoch 4355, Loss: 0.05715937353670597, Final Batch Loss: 0.017211442813277245\n",
      "Epoch 4356, Loss: 0.058120143599808216, Final Batch Loss: 0.044422514736652374\n",
      "Epoch 4357, Loss: 0.10216033086180687, Final Batch Loss: 0.04184908792376518\n",
      "Epoch 4358, Loss: 0.08935268223285675, Final Batch Loss: 0.04460665211081505\n",
      "Epoch 4359, Loss: 0.0751224160194397, Final Batch Loss: 0.03596191853284836\n",
      "Epoch 4360, Loss: 0.07711054384708405, Final Batch Loss: 0.042209964245557785\n",
      "Epoch 4361, Loss: 0.06105495058000088, Final Batch Loss: 0.029774604365229607\n",
      "Epoch 4362, Loss: 0.14114508777856827, Final Batch Loss: 0.06465942412614822\n",
      "Epoch 4363, Loss: 0.0808478593826294, Final Batch Loss: 0.03826243057847023\n",
      "Epoch 4364, Loss: 0.09430300071835518, Final Batch Loss: 0.033593013882637024\n",
      "Epoch 4365, Loss: 0.06962352618575096, Final Batch Loss: 0.037670332938432693\n",
      "Epoch 4366, Loss: 0.06735707074403763, Final Batch Loss: 0.03405723720788956\n",
      "Epoch 4367, Loss: 0.06424056738615036, Final Batch Loss: 0.03566298261284828\n",
      "Epoch 4368, Loss: 0.10299330204725266, Final Batch Loss: 0.040555913001298904\n",
      "Epoch 4369, Loss: 0.05870668962597847, Final Batch Loss: 0.031870268285274506\n",
      "Epoch 4370, Loss: 0.05366086773574352, Final Batch Loss: 0.026512065902352333\n",
      "Epoch 4371, Loss: 0.09968402609229088, Final Batch Loss: 0.03797655552625656\n",
      "Epoch 4372, Loss: 0.056675394997000694, Final Batch Loss: 0.024120675399899483\n",
      "Epoch 4373, Loss: 0.07460576668381691, Final Batch Loss: 0.04440286383032799\n",
      "Epoch 4374, Loss: 0.05421045608818531, Final Batch Loss: 0.018151240423321724\n",
      "Epoch 4375, Loss: 0.06769778020679951, Final Batch Loss: 0.03801523894071579\n",
      "Epoch 4376, Loss: 0.06776838004589081, Final Batch Loss: 0.034105781465768814\n",
      "Epoch 4377, Loss: 0.07728609070181847, Final Batch Loss: 0.02813800424337387\n",
      "Epoch 4378, Loss: 0.08325383998453617, Final Batch Loss: 0.014014625921845436\n",
      "Epoch 4379, Loss: 0.08520906791090965, Final Batch Loss: 0.03575553372502327\n",
      "Epoch 4380, Loss: 0.0773918405175209, Final Batch Loss: 0.04480776563286781\n",
      "Epoch 4381, Loss: 0.09061462432146072, Final Batch Loss: 0.04417329654097557\n",
      "Epoch 4382, Loss: 0.1066812202334404, Final Batch Loss: 0.038027167320251465\n",
      "Epoch 4383, Loss: 0.06994332186877728, Final Batch Loss: 0.04405198618769646\n",
      "Epoch 4384, Loss: 0.13563865795731544, Final Batch Loss: 0.0764632299542427\n",
      "Epoch 4385, Loss: 0.09197208285331726, Final Batch Loss: 0.0509832538664341\n",
      "Epoch 4386, Loss: 0.18087586015462875, Final Batch Loss: 0.11088908463716507\n",
      "Epoch 4387, Loss: 0.15046994015574455, Final Batch Loss: 0.04522497579455376\n",
      "Epoch 4388, Loss: 0.16607745736837387, Final Batch Loss: 0.13139696419239044\n",
      "Epoch 4389, Loss: 0.11873158812522888, Final Batch Loss: 0.05238960683345795\n",
      "Epoch 4390, Loss: 0.18027722835540771, Final Batch Loss: 0.049396365880966187\n",
      "Epoch 4391, Loss: 0.11830080300569534, Final Batch Loss: 0.03845283389091492\n",
      "Epoch 4392, Loss: 0.14910372346639633, Final Batch Loss: 0.08700159937143326\n",
      "Epoch 4393, Loss: 0.10504573211073875, Final Batch Loss: 0.04228343442082405\n",
      "Epoch 4394, Loss: 0.10941416770219803, Final Batch Loss: 0.04064834117889404\n",
      "Epoch 4395, Loss: 0.1307278573513031, Final Batch Loss: 0.07921475917100906\n",
      "Epoch 4396, Loss: 0.11424873769283295, Final Batch Loss: 0.04465319961309433\n",
      "Epoch 4397, Loss: 0.07844829559326172, Final Batch Loss: 0.02615579590201378\n",
      "Epoch 4398, Loss: 0.10326431319117546, Final Batch Loss: 0.07308471947908401\n",
      "Epoch 4399, Loss: 0.07244029641151428, Final Batch Loss: 0.022639252245426178\n",
      "Epoch 4400, Loss: 0.06206092983484268, Final Batch Loss: 0.037291113287210464\n",
      "Epoch 4401, Loss: 0.09985385835170746, Final Batch Loss: 0.07264607399702072\n",
      "Epoch 4402, Loss: 0.05453881248831749, Final Batch Loss: 0.033332422375679016\n",
      "Epoch 4403, Loss: 0.08686485886573792, Final Batch Loss: 0.052577756345272064\n",
      "Epoch 4404, Loss: 0.06928856484591961, Final Batch Loss: 0.02851930446922779\n",
      "Epoch 4405, Loss: 0.06815801560878754, Final Batch Loss: 0.034856412559747696\n",
      "Epoch 4406, Loss: 0.06466464139521122, Final Batch Loss: 0.034378062933683395\n",
      "Epoch 4407, Loss: 0.0730099231004715, Final Batch Loss: 0.04369138181209564\n",
      "Epoch 4408, Loss: 0.07226543501019478, Final Batch Loss: 0.040237776935100555\n",
      "Epoch 4409, Loss: 0.08679604157805443, Final Batch Loss: 0.044210005551576614\n",
      "Epoch 4410, Loss: 0.09599927440285683, Final Batch Loss: 0.036355841904878616\n",
      "Epoch 4411, Loss: 0.12196840345859528, Final Batch Loss: 0.03472570329904556\n",
      "Epoch 4412, Loss: 0.08538518100976944, Final Batch Loss: 0.036214008927345276\n",
      "Epoch 4413, Loss: 0.0684406477957964, Final Batch Loss: 0.044584520161151886\n",
      "Epoch 4414, Loss: 0.08589610457420349, Final Batch Loss: 0.04914839193224907\n",
      "Epoch 4415, Loss: 0.07184229046106339, Final Batch Loss: 0.04438267648220062\n",
      "Epoch 4416, Loss: 0.07527332939207554, Final Batch Loss: 0.03059719316661358\n",
      "Epoch 4417, Loss: 0.09901788085699081, Final Batch Loss: 0.04933631420135498\n",
      "Epoch 4418, Loss: 0.08421865105628967, Final Batch Loss: 0.03775494173169136\n",
      "Epoch 4419, Loss: 0.09452132135629654, Final Batch Loss: 0.04595063254237175\n",
      "Epoch 4420, Loss: 0.07533834502100945, Final Batch Loss: 0.043692976236343384\n",
      "Epoch 4421, Loss: 0.09755948558449745, Final Batch Loss: 0.047499869018793106\n",
      "Epoch 4422, Loss: 0.09728994220495224, Final Batch Loss: 0.05769595131278038\n",
      "Epoch 4423, Loss: 0.04848312586545944, Final Batch Loss: 0.02283976413309574\n",
      "Epoch 4424, Loss: 0.09489496052265167, Final Batch Loss: 0.048703938722610474\n",
      "Epoch 4425, Loss: 0.1119880024343729, Final Batch Loss: 0.09113964438438416\n",
      "Epoch 4426, Loss: 0.08150681480765343, Final Batch Loss: 0.023973587900400162\n",
      "Epoch 4427, Loss: 0.08051178231835365, Final Batch Loss: 0.046140123158693314\n",
      "Epoch 4428, Loss: 0.10911133512854576, Final Batch Loss: 0.06309439986944199\n",
      "Epoch 4429, Loss: 0.08548544347286224, Final Batch Loss: 0.04121828451752663\n",
      "Epoch 4430, Loss: 0.10978488251566887, Final Batch Loss: 0.03166669234633446\n",
      "Epoch 4431, Loss: 0.09513889625668526, Final Batch Loss: 0.050984758883714676\n",
      "Epoch 4432, Loss: 0.1302492506802082, Final Batch Loss: 0.09198784828186035\n",
      "Epoch 4433, Loss: 0.08311018906533718, Final Batch Loss: 0.05633926764130592\n",
      "Epoch 4434, Loss: 0.09996332600712776, Final Batch Loss: 0.03163909539580345\n",
      "Epoch 4435, Loss: 0.08436072245240211, Final Batch Loss: 0.0502811037003994\n",
      "Epoch 4436, Loss: 0.07628091052174568, Final Batch Loss: 0.05292877182364464\n",
      "Epoch 4437, Loss: 0.11364564299583435, Final Batch Loss: 0.053321339190006256\n",
      "Epoch 4438, Loss: 0.11027337983250618, Final Batch Loss: 0.03363538905978203\n",
      "Epoch 4439, Loss: 0.09575844928622246, Final Batch Loss: 0.06560713797807693\n",
      "Epoch 4440, Loss: 0.08058673143386841, Final Batch Loss: 0.031272485852241516\n",
      "Epoch 4441, Loss: 0.08346403762698174, Final Batch Loss: 0.037077829241752625\n",
      "Epoch 4442, Loss: 0.09112859889864922, Final Batch Loss: 0.06774412095546722\n",
      "Epoch 4443, Loss: 0.2215504515916109, Final Batch Loss: 0.19135801494121552\n",
      "Epoch 4444, Loss: 0.07154667936265469, Final Batch Loss: 0.04100251942873001\n",
      "Epoch 4445, Loss: 0.06533097103238106, Final Batch Loss: 0.038895655423402786\n",
      "Epoch 4446, Loss: 0.10623370483517647, Final Batch Loss: 0.07256913930177689\n",
      "Epoch 4447, Loss: 0.08237019181251526, Final Batch Loss: 0.025296222418546677\n",
      "Epoch 4448, Loss: 0.06468713097274303, Final Batch Loss: 0.028587790206074715\n",
      "Epoch 4449, Loss: 0.098496213555336, Final Batch Loss: 0.05719058960676193\n",
      "Epoch 4450, Loss: 0.10532386228442192, Final Batch Loss: 0.06599216908216476\n",
      "Epoch 4451, Loss: 0.05850919336080551, Final Batch Loss: 0.029859542846679688\n",
      "Epoch 4452, Loss: 0.08119730092585087, Final Batch Loss: 0.051540035754442215\n",
      "Epoch 4453, Loss: 0.10607902705669403, Final Batch Loss: 0.07033216208219528\n",
      "Epoch 4454, Loss: 0.10106756910681725, Final Batch Loss: 0.04114003852009773\n",
      "Epoch 4455, Loss: 0.06422415003180504, Final Batch Loss: 0.034364454448223114\n",
      "Epoch 4456, Loss: 0.04939112067222595, Final Batch Loss: 0.01834329590201378\n",
      "Epoch 4457, Loss: 0.10255445912480354, Final Batch Loss: 0.051686521619558334\n",
      "Epoch 4458, Loss: 0.075809296220541, Final Batch Loss: 0.03179671987891197\n",
      "Epoch 4459, Loss: 0.08498672395944595, Final Batch Loss: 0.04794323816895485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4460, Loss: 0.10467258468270302, Final Batch Loss: 0.02937162294983864\n",
      "Epoch 4461, Loss: 0.0746440663933754, Final Batch Loss: 0.035183582454919815\n",
      "Epoch 4462, Loss: 0.06681203469634056, Final Batch Loss: 0.03558668494224548\n",
      "Epoch 4463, Loss: 0.07708801701664925, Final Batch Loss: 0.04091617837548256\n",
      "Epoch 4464, Loss: 0.07720591872930527, Final Batch Loss: 0.0432450957596302\n",
      "Epoch 4465, Loss: 0.06547709368169308, Final Batch Loss: 0.02191767655313015\n",
      "Epoch 4466, Loss: 0.07968831993639469, Final Batch Loss: 0.04872306063771248\n",
      "Epoch 4467, Loss: 0.07692991197109222, Final Batch Loss: 0.03889210522174835\n",
      "Epoch 4468, Loss: 0.05478089302778244, Final Batch Loss: 0.028213806450366974\n",
      "Epoch 4469, Loss: 0.057717470452189445, Final Batch Loss: 0.024647073820233345\n",
      "Epoch 4470, Loss: 0.0711723044514656, Final Batch Loss: 0.0452386774122715\n",
      "Epoch 4471, Loss: 0.0660681314766407, Final Batch Loss: 0.03371061012148857\n",
      "Epoch 4472, Loss: 0.06206567399203777, Final Batch Loss: 0.02522936649620533\n",
      "Epoch 4473, Loss: 0.07255401462316513, Final Batch Loss: 0.04278922453522682\n",
      "Epoch 4474, Loss: 0.07023783400654793, Final Batch Loss: 0.03061230480670929\n",
      "Epoch 4475, Loss: 0.0632398072630167, Final Batch Loss: 0.035125356167554855\n",
      "Epoch 4476, Loss: 0.09083889424800873, Final Batch Loss: 0.036201607435941696\n",
      "Epoch 4477, Loss: 0.07302587851881981, Final Batch Loss: 0.03232872113585472\n",
      "Epoch 4478, Loss: 0.12135862559080124, Final Batch Loss: 0.08784322440624237\n",
      "Epoch 4479, Loss: 0.08073113299906254, Final Batch Loss: 0.026335211470723152\n",
      "Epoch 4480, Loss: 0.08807826600968838, Final Batch Loss: 0.028361229225993156\n",
      "Epoch 4481, Loss: 0.10155882686376572, Final Batch Loss: 0.038283877074718475\n",
      "Epoch 4482, Loss: 0.07585807517170906, Final Batch Loss: 0.037546683102846146\n",
      "Epoch 4483, Loss: 0.06697357818484306, Final Batch Loss: 0.035115692764520645\n",
      "Epoch 4484, Loss: 0.06983321160078049, Final Batch Loss: 0.025547020137310028\n",
      "Epoch 4485, Loss: 0.07056421041488647, Final Batch Loss: 0.03692697733640671\n",
      "Epoch 4486, Loss: 0.06939914636313915, Final Batch Loss: 0.016508275642991066\n",
      "Epoch 4487, Loss: 0.06527694128453732, Final Batch Loss: 0.036671072244644165\n",
      "Epoch 4488, Loss: 0.06290040910243988, Final Batch Loss: 0.01992737129330635\n",
      "Epoch 4489, Loss: 0.06365517526865005, Final Batch Loss: 0.03155967965722084\n",
      "Epoch 4490, Loss: 0.10605079308152199, Final Batch Loss: 0.06688178330659866\n",
      "Epoch 4491, Loss: 0.05422993190586567, Final Batch Loss: 0.024981597438454628\n",
      "Epoch 4492, Loss: 0.06907553784549236, Final Batch Loss: 0.02801482193171978\n",
      "Epoch 4493, Loss: 0.07592656835913658, Final Batch Loss: 0.04541001096367836\n",
      "Epoch 4494, Loss: 0.06770176440477371, Final Batch Loss: 0.028028979897499084\n",
      "Epoch 4495, Loss: 0.08227456733584404, Final Batch Loss: 0.05436577647924423\n",
      "Epoch 4496, Loss: 0.06830517575144768, Final Batch Loss: 0.03679126128554344\n",
      "Epoch 4497, Loss: 0.05801643617451191, Final Batch Loss: 0.02239413373172283\n",
      "Epoch 4498, Loss: 0.06849243678152561, Final Batch Loss: 0.02886842004954815\n",
      "Epoch 4499, Loss: 0.076533243060112, Final Batch Loss: 0.03845859318971634\n",
      "Epoch 4500, Loss: 0.07900876179337502, Final Batch Loss: 0.04320496693253517\n",
      "Epoch 4501, Loss: 0.0631231963634491, Final Batch Loss: 0.02309875190258026\n",
      "Epoch 4502, Loss: 0.07299121841788292, Final Batch Loss: 0.03448447957634926\n",
      "Epoch 4503, Loss: 0.06803633458912373, Final Batch Loss: 0.03813021257519722\n",
      "Epoch 4504, Loss: 0.055960819125175476, Final Batch Loss: 0.03258439898490906\n",
      "Epoch 4505, Loss: 0.10731978714466095, Final Batch Loss: 0.06106282025575638\n",
      "Epoch 4506, Loss: 0.0652712844312191, Final Batch Loss: 0.03276440501213074\n",
      "Epoch 4507, Loss: 0.0525365024805069, Final Batch Loss: 0.018979527056217194\n",
      "Epoch 4508, Loss: 0.08241290412843227, Final Batch Loss: 0.05736065283417702\n",
      "Epoch 4509, Loss: 0.0530339777469635, Final Batch Loss: 0.029291164129972458\n",
      "Epoch 4510, Loss: 0.07722790166735649, Final Batch Loss: 0.029828514903783798\n",
      "Epoch 4511, Loss: 0.06121326610445976, Final Batch Loss: 0.03776276111602783\n",
      "Epoch 4512, Loss: 0.07143686525523663, Final Batch Loss: 0.041026417165994644\n",
      "Epoch 4513, Loss: 0.07702532224357128, Final Batch Loss: 0.047237053513526917\n",
      "Epoch 4514, Loss: 0.0599693488329649, Final Batch Loss: 0.03341028094291687\n",
      "Epoch 4515, Loss: 0.09201710671186447, Final Batch Loss: 0.04684080183506012\n",
      "Epoch 4516, Loss: 0.08559636771678925, Final Batch Loss: 0.04596235975623131\n",
      "Epoch 4517, Loss: 0.06121112033724785, Final Batch Loss: 0.031715333461761475\n",
      "Epoch 4518, Loss: 0.08704840391874313, Final Batch Loss: 0.04868624359369278\n",
      "Epoch 4519, Loss: 0.07157633453607559, Final Batch Loss: 0.01995979994535446\n",
      "Epoch 4520, Loss: 0.0763131845742464, Final Batch Loss: 0.05086136609315872\n",
      "Epoch 4521, Loss: 0.07971186935901642, Final Batch Loss: 0.031833477318286896\n",
      "Epoch 4522, Loss: 0.09841654822230339, Final Batch Loss: 0.0695127323269844\n",
      "Epoch 4523, Loss: 0.06220303848385811, Final Batch Loss: 0.0220775343477726\n",
      "Epoch 4524, Loss: 0.0541033111512661, Final Batch Loss: 0.023984862491488457\n",
      "Epoch 4525, Loss: 0.06329920329153538, Final Batch Loss: 0.039337676018476486\n",
      "Epoch 4526, Loss: 0.06696474552154541, Final Batch Loss: 0.023636095225811005\n",
      "Epoch 4527, Loss: 0.06635813042521477, Final Batch Loss: 0.02331548184156418\n",
      "Epoch 4528, Loss: 0.07034692354500294, Final Batch Loss: 0.023785309866070747\n",
      "Epoch 4529, Loss: 0.07642940431833267, Final Batch Loss: 0.05305616930127144\n",
      "Epoch 4530, Loss: 0.09941177815198898, Final Batch Loss: 0.05596346780657768\n",
      "Epoch 4531, Loss: 0.05684545263648033, Final Batch Loss: 0.017173554748296738\n",
      "Epoch 4532, Loss: 0.0745071992278099, Final Batch Loss: 0.05481719598174095\n",
      "Epoch 4533, Loss: 0.062202611938118935, Final Batch Loss: 0.043708279728889465\n",
      "Epoch 4534, Loss: 0.07413347996771336, Final Batch Loss: 0.024132395163178444\n",
      "Epoch 4535, Loss: 0.1093861348927021, Final Batch Loss: 0.0412660650908947\n",
      "Epoch 4536, Loss: 0.10413458198308945, Final Batch Loss: 0.06472577899694443\n",
      "Epoch 4537, Loss: 0.08385994285345078, Final Batch Loss: 0.05187005549669266\n",
      "Epoch 4538, Loss: 0.1211267039179802, Final Batch Loss: 0.08935796469449997\n",
      "Epoch 4539, Loss: 0.06750101782381535, Final Batch Loss: 0.020500721409916878\n",
      "Epoch 4540, Loss: 0.06941033527255058, Final Batch Loss: 0.03975606709718704\n",
      "Epoch 4541, Loss: 0.08433972671627998, Final Batch Loss: 0.04679322615265846\n",
      "Epoch 4542, Loss: 0.06592628173530102, Final Batch Loss: 0.026989726349711418\n",
      "Epoch 4543, Loss: 0.07428821176290512, Final Batch Loss: 0.05550813302397728\n",
      "Epoch 4544, Loss: 0.08398986980319023, Final Batch Loss: 0.034181948751211166\n",
      "Epoch 4545, Loss: 0.07281801104545593, Final Batch Loss: 0.029826276004314423\n",
      "Epoch 4546, Loss: 0.09561679512262344, Final Batch Loss: 0.05122947320342064\n",
      "Epoch 4547, Loss: 0.08494919911026955, Final Batch Loss: 0.04999978840351105\n",
      "Epoch 4548, Loss: 0.06278862059116364, Final Batch Loss: 0.0348900742828846\n",
      "Epoch 4549, Loss: 0.09997253678739071, Final Batch Loss: 0.028347043320536613\n",
      "Epoch 4550, Loss: 0.0653181541711092, Final Batch Loss: 0.01888059265911579\n",
      "Epoch 4551, Loss: 0.06594412215054035, Final Batch Loss: 0.028447212651371956\n",
      "Epoch 4552, Loss: 0.05477040261030197, Final Batch Loss: 0.03537888824939728\n",
      "Epoch 4553, Loss: 0.0637296587228775, Final Batch Loss: 0.04473111778497696\n",
      "Epoch 4554, Loss: 0.06432921439409256, Final Batch Loss: 0.038064196705818176\n",
      "Epoch 4555, Loss: 0.11779833771288395, Final Batch Loss: 0.02412971295416355\n",
      "Epoch 4556, Loss: 0.0757305957376957, Final Batch Loss: 0.029877200722694397\n",
      "Epoch 4557, Loss: 0.07705276645720005, Final Batch Loss: 0.04748043790459633\n",
      "Epoch 4558, Loss: 0.09631918929517269, Final Batch Loss: 0.07177163660526276\n",
      "Epoch 4559, Loss: 0.06805518828332424, Final Batch Loss: 0.048328787088394165\n",
      "Epoch 4560, Loss: 0.064652469009161, Final Batch Loss: 0.012571271508932114\n",
      "Epoch 4561, Loss: 0.1070295162498951, Final Batch Loss: 0.04685026779770851\n",
      "Epoch 4562, Loss: 0.07590287178754807, Final Batch Loss: 0.023286346346139908\n",
      "Epoch 4563, Loss: 0.09877372160553932, Final Batch Loss: 0.0363326221704483\n",
      "Epoch 4564, Loss: 0.0705548133701086, Final Batch Loss: 0.054452016949653625\n",
      "Epoch 4565, Loss: 0.0751330554485321, Final Batch Loss: 0.03619392588734627\n",
      "Epoch 4566, Loss: 0.061704251915216446, Final Batch Loss: 0.033130064606666565\n",
      "Epoch 4567, Loss: 0.0787653736770153, Final Batch Loss: 0.05000846087932587\n",
      "Epoch 4568, Loss: 0.10934993252158165, Final Batch Loss: 0.07501871138811111\n",
      "Epoch 4569, Loss: 0.0863693617284298, Final Batch Loss: 0.037008121609687805\n",
      "Epoch 4570, Loss: 0.10119311884045601, Final Batch Loss: 0.06639143079519272\n",
      "Epoch 4571, Loss: 0.059183958917856216, Final Batch Loss: 0.02498340979218483\n",
      "Epoch 4572, Loss: 0.07574505172669888, Final Batch Loss: 0.05481855943799019\n",
      "Epoch 4573, Loss: 0.06569571048021317, Final Batch Loss: 0.031546734273433685\n",
      "Epoch 4574, Loss: 0.0666610598564148, Final Batch Loss: 0.027444548904895782\n",
      "Epoch 4575, Loss: 0.06968829035758972, Final Batch Loss: 0.022785734385252\n",
      "Epoch 4576, Loss: 0.08494563400745392, Final Batch Loss: 0.02807149663567543\n",
      "Epoch 4577, Loss: 0.053284212946891785, Final Batch Loss: 0.035236015915870667\n",
      "Epoch 4578, Loss: 0.12666462734341621, Final Batch Loss: 0.09567275643348694\n",
      "Epoch 4579, Loss: 0.14185183122754097, Final Batch Loss: 0.05091984197497368\n",
      "Epoch 4580, Loss: 0.05364948324859142, Final Batch Loss: 0.027591966092586517\n",
      "Epoch 4581, Loss: 0.10402965545654297, Final Batch Loss: 0.033855587244033813\n",
      "Epoch 4582, Loss: 0.052776116877794266, Final Batch Loss: 0.021556489169597626\n",
      "Epoch 4583, Loss: 0.06397897005081177, Final Batch Loss: 0.03027188405394554\n",
      "Epoch 4584, Loss: 0.06839234568178654, Final Batch Loss: 0.022971661761403084\n",
      "Epoch 4585, Loss: 0.062239957973361015, Final Batch Loss: 0.027303515002131462\n",
      "Epoch 4586, Loss: 0.06252281740307808, Final Batch Loss: 0.044887762516736984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4587, Loss: 0.08057098090648651, Final Batch Loss: 0.057431720197200775\n",
      "Epoch 4588, Loss: 0.063355702906847, Final Batch Loss: 0.03389766439795494\n",
      "Epoch 4589, Loss: 0.07467414997518063, Final Batch Loss: 0.044649723917245865\n",
      "Epoch 4590, Loss: 0.06634247116744518, Final Batch Loss: 0.026190737262368202\n",
      "Epoch 4591, Loss: 0.06340846791863441, Final Batch Loss: 0.03619636595249176\n",
      "Epoch 4592, Loss: 0.060514120385050774, Final Batch Loss: 0.04182588309049606\n",
      "Epoch 4593, Loss: 0.05130358599126339, Final Batch Loss: 0.015640420839190483\n",
      "Epoch 4594, Loss: 0.06259522587060928, Final Batch Loss: 0.04186336696147919\n",
      "Epoch 4595, Loss: 0.06220117397606373, Final Batch Loss: 0.03333190083503723\n",
      "Epoch 4596, Loss: 0.06677559018135071, Final Batch Loss: 0.034724634140729904\n",
      "Epoch 4597, Loss: 0.08979848213493824, Final Batch Loss: 0.06358493864536285\n",
      "Epoch 4598, Loss: 0.06394020840525627, Final Batch Loss: 0.02953026443719864\n",
      "Epoch 4599, Loss: 0.14627145417034626, Final Batch Loss: 0.11615205556154251\n",
      "Epoch 4600, Loss: 0.05521824024617672, Final Batch Loss: 0.03429369628429413\n",
      "Epoch 4601, Loss: 0.061531951650977135, Final Batch Loss: 0.04485604912042618\n",
      "Epoch 4602, Loss: 0.09106504172086716, Final Batch Loss: 0.0540824830532074\n",
      "Epoch 4603, Loss: 0.08601248636841774, Final Batch Loss: 0.0375530906021595\n",
      "Epoch 4604, Loss: 0.06578117795288563, Final Batch Loss: 0.036011192947626114\n",
      "Epoch 4605, Loss: 0.062380192801356316, Final Batch Loss: 0.03339902684092522\n",
      "Epoch 4606, Loss: 0.06550229154527187, Final Batch Loss: 0.026649126783013344\n",
      "Epoch 4607, Loss: 0.10719741135835648, Final Batch Loss: 0.038173429667949677\n",
      "Epoch 4608, Loss: 0.05833721160888672, Final Batch Loss: 0.01278490200638771\n",
      "Epoch 4609, Loss: 0.07217234000563622, Final Batch Loss: 0.04836934432387352\n",
      "Epoch 4610, Loss: 0.06262966804206371, Final Batch Loss: 0.029188642278313637\n",
      "Epoch 4611, Loss: 0.08777092769742012, Final Batch Loss: 0.038093797862529755\n",
      "Epoch 4612, Loss: 0.07966484874486923, Final Batch Loss: 0.041461583226919174\n",
      "Epoch 4613, Loss: 0.06407151371240616, Final Batch Loss: 0.029751349240541458\n",
      "Epoch 4614, Loss: 0.0596500001847744, Final Batch Loss: 0.01679890975356102\n",
      "Epoch 4615, Loss: 0.06567084789276123, Final Batch Loss: 0.021518245339393616\n",
      "Epoch 4616, Loss: 0.06250686384737492, Final Batch Loss: 0.034599144011735916\n",
      "Epoch 4617, Loss: 0.05322611145675182, Final Batch Loss: 0.029946431517601013\n",
      "Epoch 4618, Loss: 0.0614676084369421, Final Batch Loss: 0.03531722351908684\n",
      "Epoch 4619, Loss: 0.07268246077001095, Final Batch Loss: 0.05052725970745087\n",
      "Epoch 4620, Loss: 0.05974595621228218, Final Batch Loss: 0.025737836956977844\n",
      "Epoch 4621, Loss: 0.08007139340043068, Final Batch Loss: 0.044040560722351074\n",
      "Epoch 4622, Loss: 0.052066072821617126, Final Batch Loss: 0.021476686000823975\n",
      "Epoch 4623, Loss: 0.05743860453367233, Final Batch Loss: 0.020014658570289612\n",
      "Epoch 4624, Loss: 0.05723786447197199, Final Batch Loss: 0.042126964777708054\n",
      "Epoch 4625, Loss: 0.046492839232087135, Final Batch Loss: 0.016432486474514008\n",
      "Epoch 4626, Loss: 0.06898083910346031, Final Batch Loss: 0.03577425703406334\n",
      "Epoch 4627, Loss: 0.06282572261989117, Final Batch Loss: 0.034103792160749435\n",
      "Epoch 4628, Loss: 0.05760049819946289, Final Batch Loss: 0.030981218442320824\n",
      "Epoch 4629, Loss: 0.07001526467502117, Final Batch Loss: 0.027016649022698402\n",
      "Epoch 4630, Loss: 0.060868438333272934, Final Batch Loss: 0.018376607447862625\n",
      "Epoch 4631, Loss: 0.0653018206357956, Final Batch Loss: 0.03710632771253586\n",
      "Epoch 4632, Loss: 0.08868760615587234, Final Batch Loss: 0.05209781229496002\n",
      "Epoch 4633, Loss: 0.10615887492895126, Final Batch Loss: 0.03416326642036438\n",
      "Epoch 4634, Loss: 0.05303144082427025, Final Batch Loss: 0.03364253789186478\n",
      "Epoch 4635, Loss: 0.0867483876645565, Final Batch Loss: 0.035586632788181305\n",
      "Epoch 4636, Loss: 0.06790655292570591, Final Batch Loss: 0.023149831220507622\n",
      "Epoch 4637, Loss: 0.07025239616632462, Final Batch Loss: 0.020981069654226303\n",
      "Epoch 4638, Loss: 0.11469381675124168, Final Batch Loss: 0.09137331694364548\n",
      "Epoch 4639, Loss: 0.08900153636932373, Final Batch Loss: 0.05737125873565674\n",
      "Epoch 4640, Loss: 0.06287375278770924, Final Batch Loss: 0.04203608259558678\n",
      "Epoch 4641, Loss: 0.07465020008385181, Final Batch Loss: 0.04724627360701561\n",
      "Epoch 4642, Loss: 0.10421750694513321, Final Batch Loss: 0.06884688884019852\n",
      "Epoch 4643, Loss: 0.13312608748674393, Final Batch Loss: 0.05054666846990585\n",
      "Epoch 4644, Loss: 0.07032205909490585, Final Batch Loss: 0.05164477601647377\n",
      "Epoch 4645, Loss: 0.08734939061105251, Final Batch Loss: 0.057753656059503555\n",
      "Epoch 4646, Loss: 0.07799885794520378, Final Batch Loss: 0.04504534974694252\n",
      "Epoch 4647, Loss: 0.08421347104012966, Final Batch Loss: 0.06624272465705872\n",
      "Epoch 4648, Loss: 0.06672145426273346, Final Batch Loss: 0.015459388494491577\n",
      "Epoch 4649, Loss: 0.05559160187840462, Final Batch Loss: 0.03426343947649002\n",
      "Epoch 4650, Loss: 0.04902110435068607, Final Batch Loss: 0.024203991517424583\n",
      "Epoch 4651, Loss: 0.07924266532063484, Final Batch Loss: 0.03553600609302521\n",
      "Epoch 4652, Loss: 0.0697394385933876, Final Batch Loss: 0.03724764287471771\n",
      "Epoch 4653, Loss: 0.07975872606039047, Final Batch Loss: 0.04102437198162079\n",
      "Epoch 4654, Loss: 0.07893309369683266, Final Batch Loss: 0.046271637082099915\n",
      "Epoch 4655, Loss: 0.06329132802784443, Final Batch Loss: 0.026967426761984825\n",
      "Epoch 4656, Loss: 0.043744464404881, Final Batch Loss: 0.01036760676652193\n",
      "Epoch 4657, Loss: 0.06525612436234951, Final Batch Loss: 0.03677346929907799\n",
      "Epoch 4658, Loss: 0.0780612975358963, Final Batch Loss: 0.05240388214588165\n",
      "Epoch 4659, Loss: 0.06050047464668751, Final Batch Loss: 0.03215267136693001\n",
      "Epoch 4660, Loss: 0.05955780670046806, Final Batch Loss: 0.02188057079911232\n",
      "Epoch 4661, Loss: 0.07390897907316685, Final Batch Loss: 0.04953370615839958\n",
      "Epoch 4662, Loss: 0.07740102522075176, Final Batch Loss: 0.028772810474038124\n",
      "Epoch 4663, Loss: 0.08035306446254253, Final Batch Loss: 0.04955720901489258\n",
      "Epoch 4664, Loss: 0.05804234929382801, Final Batch Loss: 0.01846928708255291\n",
      "Epoch 4665, Loss: 0.07414321973919868, Final Batch Loss: 0.04531003534793854\n",
      "Epoch 4666, Loss: 0.055461131036281586, Final Batch Loss: 0.030468903481960297\n",
      "Epoch 4667, Loss: 0.09104836545884609, Final Batch Loss: 0.06557925045490265\n",
      "Epoch 4668, Loss: 0.05486994795501232, Final Batch Loss: 0.016914816573262215\n",
      "Epoch 4669, Loss: 0.07445833273231983, Final Batch Loss: 0.04755832627415657\n",
      "Epoch 4670, Loss: 0.04812865238636732, Final Batch Loss: 0.03701907768845558\n",
      "Epoch 4671, Loss: 0.06345179863274097, Final Batch Loss: 0.022484662011265755\n",
      "Epoch 4672, Loss: 0.06611082516610622, Final Batch Loss: 0.02259599231183529\n",
      "Epoch 4673, Loss: 0.051516007632017136, Final Batch Loss: 0.028861628845334053\n",
      "Epoch 4674, Loss: 0.12358539924025536, Final Batch Loss: 0.03859991207718849\n",
      "Epoch 4675, Loss: 0.06968289986252785, Final Batch Loss: 0.03274933621287346\n",
      "Epoch 4676, Loss: 0.06821423210203648, Final Batch Loss: 0.026889899745583534\n",
      "Epoch 4677, Loss: 0.06451509520411491, Final Batch Loss: 0.03466842696070671\n",
      "Epoch 4678, Loss: 0.11249494180083275, Final Batch Loss: 0.08021219819784164\n",
      "Epoch 4679, Loss: 0.05677169933915138, Final Batch Loss: 0.02379404380917549\n",
      "Epoch 4680, Loss: 0.08228892087936401, Final Batch Loss: 0.04890697821974754\n",
      "Epoch 4681, Loss: 0.10436183959245682, Final Batch Loss: 0.07067649811506271\n",
      "Epoch 4682, Loss: 0.08954739943146706, Final Batch Loss: 0.04566578194499016\n",
      "Epoch 4683, Loss: 0.11929472535848618, Final Batch Loss: 0.05127745121717453\n",
      "Epoch 4684, Loss: 0.0933183804154396, Final Batch Loss: 0.04761672019958496\n",
      "Epoch 4685, Loss: 0.09626453369855881, Final Batch Loss: 0.05677369236946106\n",
      "Epoch 4686, Loss: 0.10858100652694702, Final Batch Loss: 0.05340038239955902\n",
      "Epoch 4687, Loss: 0.0886610858142376, Final Batch Loss: 0.03184724599123001\n",
      "Epoch 4688, Loss: 0.06299558654427528, Final Batch Loss: 0.028411325067281723\n",
      "Epoch 4689, Loss: 0.0973668061196804, Final Batch Loss: 0.040257010608911514\n",
      "Epoch 4690, Loss: 0.11672267317771912, Final Batch Loss: 0.051797881722450256\n",
      "Epoch 4691, Loss: 0.08618253469467163, Final Batch Loss: 0.044604234397411346\n",
      "Epoch 4692, Loss: 0.07803169079124928, Final Batch Loss: 0.05311325564980507\n",
      "Epoch 4693, Loss: 0.08608479797840118, Final Batch Loss: 0.04718969389796257\n",
      "Epoch 4694, Loss: 0.07763084396719933, Final Batch Loss: 0.0538393035531044\n",
      "Epoch 4695, Loss: 0.13413245975971222, Final Batch Loss: 0.09646288305521011\n",
      "Epoch 4696, Loss: 0.07670510932803154, Final Batch Loss: 0.0348874069750309\n",
      "Epoch 4697, Loss: 0.09908797405660152, Final Batch Loss: 0.07005253434181213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4698, Loss: 0.06265837326645851, Final Batch Loss: 0.03564123809337616\n",
      "Epoch 4699, Loss: 0.05432208813726902, Final Batch Loss: 0.022443054243922234\n",
      "Epoch 4700, Loss: 0.06645501963794231, Final Batch Loss: 0.027177168056368828\n",
      "Epoch 4701, Loss: 0.06365766003727913, Final Batch Loss: 0.03275275230407715\n",
      "Epoch 4702, Loss: 0.08137972094118595, Final Batch Loss: 0.053306691348552704\n",
      "Epoch 4703, Loss: 0.09753718972206116, Final Batch Loss: 0.0438767671585083\n",
      "Epoch 4704, Loss: 0.04597631096839905, Final Batch Loss: 0.023952415212988853\n",
      "Epoch 4705, Loss: 0.1274690367281437, Final Batch Loss: 0.10229888558387756\n",
      "Epoch 4706, Loss: 0.08742384053766727, Final Batch Loss: 0.06139308586716652\n",
      "Epoch 4707, Loss: 0.05244968272745609, Final Batch Loss: 0.02154061757028103\n",
      "Epoch 4708, Loss: 0.07396836765110493, Final Batch Loss: 0.052365824580192566\n",
      "Epoch 4709, Loss: 0.05498300865292549, Final Batch Loss: 0.022566333413124084\n",
      "Epoch 4710, Loss: 0.06722984462976456, Final Batch Loss: 0.02668815851211548\n",
      "Epoch 4711, Loss: 0.08782132714986801, Final Batch Loss: 0.017704345285892487\n",
      "Epoch 4712, Loss: 0.08053546026349068, Final Batch Loss: 0.04568995535373688\n",
      "Epoch 4713, Loss: 0.06179110333323479, Final Batch Loss: 0.03037375584244728\n",
      "Epoch 4714, Loss: 0.07542277965694666, Final Batch Loss: 0.01372011099010706\n",
      "Epoch 4715, Loss: 0.07895522192120552, Final Batch Loss: 0.03617651015520096\n",
      "Epoch 4716, Loss: 0.05616517551243305, Final Batch Loss: 0.03794847056269646\n",
      "Epoch 4717, Loss: 0.06259113363921642, Final Batch Loss: 0.036685921251773834\n",
      "Epoch 4718, Loss: 0.07173330150544643, Final Batch Loss: 0.020726678892970085\n",
      "Epoch 4719, Loss: 0.0655839815735817, Final Batch Loss: 0.03988635912537575\n",
      "Epoch 4720, Loss: 0.06556351855397224, Final Batch Loss: 0.041254911571741104\n",
      "Epoch 4721, Loss: 0.0728907659649849, Final Batch Loss: 0.04655664786696434\n",
      "Epoch 4722, Loss: 0.07528963685035706, Final Batch Loss: 0.03544183820486069\n",
      "Epoch 4723, Loss: 0.06552808359265327, Final Batch Loss: 0.05146817862987518\n",
      "Epoch 4724, Loss: 0.05443803779780865, Final Batch Loss: 0.03277667984366417\n",
      "Epoch 4725, Loss: 0.08573920652270317, Final Batch Loss: 0.047452911734580994\n",
      "Epoch 4726, Loss: 0.07788041606545448, Final Batch Loss: 0.04108140245079994\n",
      "Epoch 4727, Loss: 0.06988486275076866, Final Batch Loss: 0.034347303211688995\n",
      "Epoch 4728, Loss: 0.05842321738600731, Final Batch Loss: 0.029819650575518608\n",
      "Epoch 4729, Loss: 0.0457110907882452, Final Batch Loss: 0.027728375047445297\n",
      "Epoch 4730, Loss: 0.10114038363099098, Final Batch Loss: 0.06999356299638748\n",
      "Epoch 4731, Loss: 0.07664134725928307, Final Batch Loss: 0.030023321509361267\n",
      "Epoch 4732, Loss: 0.0743403285741806, Final Batch Loss: 0.019922390580177307\n",
      "Epoch 4733, Loss: 0.08697578310966492, Final Batch Loss: 0.05367943271994591\n",
      "Epoch 4734, Loss: 0.15523536689579487, Final Batch Loss: 0.03019811399281025\n",
      "Epoch 4735, Loss: 0.07850681617856026, Final Batch Loss: 0.04911753535270691\n",
      "Epoch 4736, Loss: 0.05886180233210325, Final Batch Loss: 0.015323675237596035\n",
      "Epoch 4737, Loss: 0.0644041895866394, Final Batch Loss: 0.03261187672615051\n",
      "Epoch 4738, Loss: 0.056551214307546616, Final Batch Loss: 0.016569722443819046\n",
      "Epoch 4739, Loss: 0.05235845595598221, Final Batch Loss: 0.027329573407769203\n",
      "Epoch 4740, Loss: 0.05579044297337532, Final Batch Loss: 0.029757769778370857\n",
      "Epoch 4741, Loss: 0.05881822109222412, Final Batch Loss: 0.031632449477910995\n",
      "Epoch 4742, Loss: 0.056905586272478104, Final Batch Loss: 0.03462918847799301\n",
      "Epoch 4743, Loss: 0.06401386298239231, Final Batch Loss: 0.027949990704655647\n",
      "Epoch 4744, Loss: 0.05223598703742027, Final Batch Loss: 0.026661371812224388\n",
      "Epoch 4745, Loss: 0.06906446255743504, Final Batch Loss: 0.046936869621276855\n",
      "Epoch 4746, Loss: 0.07165456376969814, Final Batch Loss: 0.05090462788939476\n",
      "Epoch 4747, Loss: 0.07036982290446758, Final Batch Loss: 0.024112297222018242\n",
      "Epoch 4748, Loss: 0.061135053634643555, Final Batch Loss: 0.041553471237421036\n",
      "Epoch 4749, Loss: 0.06028141640126705, Final Batch Loss: 0.03879806771874428\n",
      "Epoch 4750, Loss: 0.0751561876386404, Final Batch Loss: 0.06055135279893875\n",
      "Epoch 4751, Loss: 0.04854474402964115, Final Batch Loss: 0.015347732231020927\n",
      "Epoch 4752, Loss: 0.05857389234006405, Final Batch Loss: 0.04038447514176369\n",
      "Epoch 4753, Loss: 0.09129447396844625, Final Batch Loss: 0.07893865555524826\n",
      "Epoch 4754, Loss: 0.10008798539638519, Final Batch Loss: 0.04489155486226082\n",
      "Epoch 4755, Loss: 0.0719749927520752, Final Batch Loss: 0.049798574298620224\n",
      "Epoch 4756, Loss: 0.12392323091626167, Final Batch Loss: 0.08635196834802628\n",
      "Epoch 4757, Loss: 0.05461247079074383, Final Batch Loss: 0.02006254531443119\n",
      "Epoch 4758, Loss: 0.07148224115371704, Final Batch Loss: 0.037421297281980515\n",
      "Epoch 4759, Loss: 0.09277435392141342, Final Batch Loss: 0.05762248858809471\n",
      "Epoch 4760, Loss: 0.05856087803840637, Final Batch Loss: 0.03443976119160652\n",
      "Epoch 4761, Loss: 0.06367110274732113, Final Batch Loss: 0.019343925639986992\n",
      "Epoch 4762, Loss: 0.08771482482552528, Final Batch Loss: 0.032886892557144165\n",
      "Epoch 4763, Loss: 0.07372303679585457, Final Batch Loss: 0.032824307680130005\n",
      "Epoch 4764, Loss: 0.09122667834162712, Final Batch Loss: 0.046483371406793594\n",
      "Epoch 4765, Loss: 0.10473338514566422, Final Batch Loss: 0.06032997742295265\n",
      "Epoch 4766, Loss: 0.14606991782784462, Final Batch Loss: 0.0855904296040535\n",
      "Epoch 4767, Loss: 0.07169340923428535, Final Batch Loss: 0.03788167983293533\n",
      "Epoch 4768, Loss: 0.0875743217766285, Final Batch Loss: 0.05211634933948517\n",
      "Epoch 4769, Loss: 0.05997489392757416, Final Batch Loss: 0.02597154676914215\n",
      "Epoch 4770, Loss: 0.12538377195596695, Final Batch Loss: 0.041757211089134216\n",
      "Epoch 4771, Loss: 0.10312603041529655, Final Batch Loss: 0.03608207777142525\n",
      "Epoch 4772, Loss: 0.11060269549489021, Final Batch Loss: 0.055667679756879807\n",
      "Epoch 4773, Loss: 0.07909010723233223, Final Batch Loss: 0.048598166555166245\n",
      "Epoch 4774, Loss: 0.11104357801377773, Final Batch Loss: 0.08702459931373596\n",
      "Epoch 4775, Loss: 0.06866363808512688, Final Batch Loss: 0.02714652195572853\n",
      "Epoch 4776, Loss: 0.08061406388878822, Final Batch Loss: 0.03055652603507042\n",
      "Epoch 4777, Loss: 0.08926825225353241, Final Batch Loss: 0.04242913797497749\n",
      "Epoch 4778, Loss: 0.06979753077030182, Final Batch Loss: 0.02974756807088852\n",
      "Epoch 4779, Loss: 0.06598541792482138, Final Batch Loss: 0.012476007454097271\n",
      "Epoch 4780, Loss: 0.0884561687707901, Final Batch Loss: 0.04406435042619705\n",
      "Epoch 4781, Loss: 0.06857940554618835, Final Batch Loss: 0.04349067062139511\n",
      "Epoch 4782, Loss: 0.0772207248955965, Final Batch Loss: 0.02537713013589382\n",
      "Epoch 4783, Loss: 0.04997524805366993, Final Batch Loss: 0.022799789905548096\n",
      "Epoch 4784, Loss: 0.059702085331082344, Final Batch Loss: 0.03679829463362694\n",
      "Epoch 4785, Loss: 0.12617867812514305, Final Batch Loss: 0.08379986882209778\n",
      "Epoch 4786, Loss: 0.05155413597822189, Final Batch Loss: 0.02896803803741932\n",
      "Epoch 4787, Loss: 0.06879443675279617, Final Batch Loss: 0.05037267133593559\n",
      "Epoch 4788, Loss: 0.09101925417780876, Final Batch Loss: 0.035984523594379425\n",
      "Epoch 4789, Loss: 0.0922185517847538, Final Batch Loss: 0.04113011062145233\n",
      "Epoch 4790, Loss: 0.14098311215639114, Final Batch Loss: 0.06632119417190552\n",
      "Epoch 4791, Loss: 0.0994795560836792, Final Batch Loss: 0.05593676492571831\n",
      "Epoch 4792, Loss: 0.10193887911736965, Final Batch Loss: 0.02855600230395794\n",
      "Epoch 4793, Loss: 0.10028475522994995, Final Batch Loss: 0.03240065276622772\n",
      "Epoch 4794, Loss: 0.1303503904491663, Final Batch Loss: 0.10951843112707138\n",
      "Epoch 4795, Loss: 0.07654071040451527, Final Batch Loss: 0.02149592898786068\n",
      "Epoch 4796, Loss: 0.06626438908278942, Final Batch Loss: 0.03748869523406029\n",
      "Epoch 4797, Loss: 0.07936638966202736, Final Batch Loss: 0.0449831485748291\n",
      "Epoch 4798, Loss: 0.06203082390129566, Final Batch Loss: 0.027119042351841927\n",
      "Epoch 4799, Loss: 0.11874115839600563, Final Batch Loss: 0.05437479540705681\n",
      "Epoch 4800, Loss: 0.10301769152283669, Final Batch Loss: 0.04482027515769005\n",
      "Epoch 4801, Loss: 0.059219200164079666, Final Batch Loss: 0.03184925764799118\n",
      "Epoch 4802, Loss: 0.06109326332807541, Final Batch Loss: 0.045348796993494034\n",
      "Epoch 4803, Loss: 0.06770192086696625, Final Batch Loss: 0.0343475416302681\n",
      "Epoch 4804, Loss: 0.08598878607153893, Final Batch Loss: 0.032976116985082626\n",
      "Epoch 4805, Loss: 0.08647975698113441, Final Batch Loss: 0.042771823704242706\n",
      "Epoch 4806, Loss: 0.06985257938504219, Final Batch Loss: 0.03543315455317497\n",
      "Epoch 4807, Loss: 0.08347424492239952, Final Batch Loss: 0.04114752635359764\n",
      "Epoch 4808, Loss: 0.07565398514270782, Final Batch Loss: 0.041994597762823105\n",
      "Epoch 4809, Loss: 0.06647876463830471, Final Batch Loss: 0.03767719864845276\n",
      "Epoch 4810, Loss: 0.05575873702764511, Final Batch Loss: 0.03806796669960022\n",
      "Epoch 4811, Loss: 0.0657680556178093, Final Batch Loss: 0.03314569965004921\n",
      "Epoch 4812, Loss: 0.089189313352108, Final Batch Loss: 0.03735404834151268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4813, Loss: 0.05537322908639908, Final Batch Loss: 0.026966510340571404\n",
      "Epoch 4814, Loss: 0.04652475379407406, Final Batch Loss: 0.019364245235919952\n",
      "Epoch 4815, Loss: 0.07008452340960503, Final Batch Loss: 0.037425924092531204\n",
      "Epoch 4816, Loss: 0.06785956025123596, Final Batch Loss: 0.03741339221596718\n",
      "Epoch 4817, Loss: 0.07590310461819172, Final Batch Loss: 0.045497629791498184\n",
      "Epoch 4818, Loss: 0.06285184994339943, Final Batch Loss: 0.029162045568227768\n",
      "Epoch 4819, Loss: 0.04479066748172045, Final Batch Loss: 0.013616993092000484\n",
      "Epoch 4820, Loss: 0.07206208631396294, Final Batch Loss: 0.035107892006635666\n",
      "Epoch 4821, Loss: 0.08414017409086227, Final Batch Loss: 0.039118167012929916\n",
      "Epoch 4822, Loss: 0.09956568852066994, Final Batch Loss: 0.04266882687807083\n",
      "Epoch 4823, Loss: 0.11633170396089554, Final Batch Loss: 0.07621601223945618\n",
      "Epoch 4824, Loss: 0.07767194509506226, Final Batch Loss: 0.0321829728782177\n",
      "Epoch 4825, Loss: 0.07629971951246262, Final Batch Loss: 0.036988917738199234\n",
      "Epoch 4826, Loss: 0.10120585188269615, Final Batch Loss: 0.06080504134297371\n",
      "Epoch 4827, Loss: 0.0774734653532505, Final Batch Loss: 0.04552502557635307\n",
      "Epoch 4828, Loss: 0.10289847478270531, Final Batch Loss: 0.04240862652659416\n",
      "Epoch 4829, Loss: 0.09677737578749657, Final Batch Loss: 0.0380784347653389\n",
      "Epoch 4830, Loss: 0.08036556467413902, Final Batch Loss: 0.038000449538230896\n",
      "Epoch 4831, Loss: 0.05682951211929321, Final Batch Loss: 0.01914842426776886\n",
      "Epoch 4832, Loss: 0.07342668622732162, Final Batch Loss: 0.022314894944429398\n",
      "Epoch 4833, Loss: 0.05694436468183994, Final Batch Loss: 0.01796521432697773\n",
      "Epoch 4834, Loss: 0.07378124818205833, Final Batch Loss: 0.043383046984672546\n",
      "Epoch 4835, Loss: 0.09194372966885567, Final Batch Loss: 0.04676923155784607\n",
      "Epoch 4836, Loss: 0.07022017054259777, Final Batch Loss: 0.025654995813965797\n",
      "Epoch 4837, Loss: 0.062124766409397125, Final Batch Loss: 0.02895364910364151\n",
      "Epoch 4838, Loss: 0.07863636687397957, Final Batch Loss: 0.03004564344882965\n",
      "Epoch 4839, Loss: 0.06769758462905884, Final Batch Loss: 0.042598310858011246\n",
      "Epoch 4840, Loss: 0.07098613679409027, Final Batch Loss: 0.031657710671424866\n",
      "Epoch 4841, Loss: 0.06584420055150986, Final Batch Loss: 0.016881581395864487\n",
      "Epoch 4842, Loss: 0.04822022467851639, Final Batch Loss: 0.02108212374150753\n",
      "Epoch 4843, Loss: 0.0669682677835226, Final Batch Loss: 0.026021787896752357\n",
      "Epoch 4844, Loss: 0.06383482180535793, Final Batch Loss: 0.032871946692466736\n",
      "Epoch 4845, Loss: 0.06590690091252327, Final Batch Loss: 0.02500058338046074\n",
      "Epoch 4846, Loss: 0.07269969768822193, Final Batch Loss: 0.029366647824645042\n",
      "Epoch 4847, Loss: 0.06834560073912144, Final Batch Loss: 0.029331156983971596\n",
      "Epoch 4848, Loss: 0.046932692639529705, Final Batch Loss: 0.034994084388017654\n",
      "Epoch 4849, Loss: 0.05168766528367996, Final Batch Loss: 0.02759493887424469\n",
      "Epoch 4850, Loss: 0.05206698924303055, Final Batch Loss: 0.035844769328832626\n",
      "Epoch 4851, Loss: 0.07070300169289112, Final Batch Loss: 0.030127028003335\n",
      "Epoch 4852, Loss: 0.07724028825759888, Final Batch Loss: 0.028661243617534637\n",
      "Epoch 4853, Loss: 0.08150668442249298, Final Batch Loss: 0.02848517894744873\n",
      "Epoch 4854, Loss: 0.0677441917359829, Final Batch Loss: 0.03608481213450432\n",
      "Epoch 4855, Loss: 0.0492742657661438, Final Batch Loss: 0.03236175328493118\n",
      "Epoch 4856, Loss: 0.04990793764591217, Final Batch Loss: 0.032082851976156235\n",
      "Epoch 4857, Loss: 0.05426635779440403, Final Batch Loss: 0.03901763632893562\n",
      "Epoch 4858, Loss: 0.0631758812814951, Final Batch Loss: 0.04616481438279152\n",
      "Epoch 4859, Loss: 0.05528792925179005, Final Batch Loss: 0.03343605250120163\n",
      "Epoch 4860, Loss: 0.07591519132256508, Final Batch Loss: 0.034163039177656174\n",
      "Epoch 4861, Loss: 0.062417929992079735, Final Batch Loss: 0.03453975170850754\n",
      "Epoch 4862, Loss: 0.08314931392669678, Final Batch Loss: 0.03936026245355606\n",
      "Epoch 4863, Loss: 0.06502550467848778, Final Batch Loss: 0.03140733018517494\n",
      "Epoch 4864, Loss: 0.05138351023197174, Final Batch Loss: 0.021039530634880066\n",
      "Epoch 4865, Loss: 0.051118407398462296, Final Batch Loss: 0.02113933488726616\n",
      "Epoch 4866, Loss: 0.09245947003364563, Final Batch Loss: 0.055854469537734985\n",
      "Epoch 4867, Loss: 0.06699891574680805, Final Batch Loss: 0.03816763684153557\n",
      "Epoch 4868, Loss: 0.06837398745119572, Final Batch Loss: 0.04445904865860939\n",
      "Epoch 4869, Loss: 0.06260395422577858, Final Batch Loss: 0.025949105620384216\n",
      "Epoch 4870, Loss: 0.06470844149589539, Final Batch Loss: 0.0321371890604496\n",
      "Epoch 4871, Loss: 0.06680314242839813, Final Batch Loss: 0.03509204834699631\n",
      "Epoch 4872, Loss: 0.05560638755559921, Final Batch Loss: 0.021161220967769623\n",
      "Epoch 4873, Loss: 0.07610934972763062, Final Batch Loss: 0.03844130411744118\n",
      "Epoch 4874, Loss: 0.05102737061679363, Final Batch Loss: 0.026077140122652054\n",
      "Epoch 4875, Loss: 0.10194277577102184, Final Batch Loss: 0.07498110085725784\n",
      "Epoch 4876, Loss: 0.08777214586734772, Final Batch Loss: 0.044263843446969986\n",
      "Epoch 4877, Loss: 0.055350687354803085, Final Batch Loss: 0.037140052765607834\n",
      "Epoch 4878, Loss: 0.07621312513947487, Final Batch Loss: 0.03028017282485962\n",
      "Epoch 4879, Loss: 0.052555566653609276, Final Batch Loss: 0.026206135749816895\n",
      "Epoch 4880, Loss: 0.09613923355937004, Final Batch Loss: 0.04970923066139221\n",
      "Epoch 4881, Loss: 0.06246888265013695, Final Batch Loss: 0.03284618631005287\n",
      "Epoch 4882, Loss: 0.06787112914025784, Final Batch Loss: 0.016541587188839912\n",
      "Epoch 4883, Loss: 0.07322386279702187, Final Batch Loss: 0.03224513307213783\n",
      "Epoch 4884, Loss: 0.07010339200496674, Final Batch Loss: 0.03677314519882202\n",
      "Epoch 4885, Loss: 0.07119392976164818, Final Batch Loss: 0.036174897104501724\n",
      "Epoch 4886, Loss: 0.05020872876048088, Final Batch Loss: 0.014421675354242325\n",
      "Epoch 4887, Loss: 0.07078628242015839, Final Batch Loss: 0.026056692004203796\n",
      "Epoch 4888, Loss: 0.05981464311480522, Final Batch Loss: 0.021108660846948624\n",
      "Epoch 4889, Loss: 0.09391440451145172, Final Batch Loss: 0.02766258269548416\n",
      "Epoch 4890, Loss: 0.11855969205498695, Final Batch Loss: 0.08314773440361023\n",
      "Epoch 4891, Loss: 0.05891886353492737, Final Batch Loss: 0.030119694769382477\n",
      "Epoch 4892, Loss: 0.09107938408851624, Final Batch Loss: 0.02183564007282257\n",
      "Epoch 4893, Loss: 0.08643924444913864, Final Batch Loss: 0.035834673792123795\n",
      "Epoch 4894, Loss: 0.07707732170820236, Final Batch Loss: 0.05880872905254364\n",
      "Epoch 4895, Loss: 0.0710560604929924, Final Batch Loss: 0.033178944140672684\n",
      "Epoch 4896, Loss: 0.06486016698181629, Final Batch Loss: 0.04358290880918503\n",
      "Epoch 4897, Loss: 0.1017005704343319, Final Batch Loss: 0.06723736971616745\n",
      "Epoch 4898, Loss: 0.06994771771132946, Final Batch Loss: 0.01938038133084774\n",
      "Epoch 4899, Loss: 0.06457461230456829, Final Batch Loss: 0.028898978605866432\n",
      "Epoch 4900, Loss: 0.05373381823301315, Final Batch Loss: 0.025216683745384216\n",
      "Epoch 4901, Loss: 0.0871685016900301, Final Batch Loss: 0.06195220723748207\n",
      "Epoch 4902, Loss: 0.08320172876119614, Final Batch Loss: 0.05317235365509987\n",
      "Epoch 4903, Loss: 0.08079306408762932, Final Batch Loss: 0.017543625086545944\n",
      "Epoch 4904, Loss: 0.0926234982907772, Final Batch Loss: 0.04777171462774277\n",
      "Epoch 4905, Loss: 0.05821663700044155, Final Batch Loss: 0.03185051307082176\n",
      "Epoch 4906, Loss: 0.12548701092600822, Final Batch Loss: 0.06653333455324173\n",
      "Epoch 4907, Loss: 0.07983937859535217, Final Batch Loss: 0.03378210961818695\n",
      "Epoch 4908, Loss: 0.10818901471793652, Final Batch Loss: 0.07941623032093048\n",
      "Epoch 4909, Loss: 0.1156880110502243, Final Batch Loss: 0.06906764954328537\n",
      "Epoch 4910, Loss: 0.052415864542126656, Final Batch Loss: 0.03311226889491081\n",
      "Epoch 4911, Loss: 0.055701048113405704, Final Batch Loss: 0.04163963347673416\n",
      "Epoch 4912, Loss: 0.03845944721251726, Final Batch Loss: 0.026501789689064026\n",
      "Epoch 4913, Loss: 0.18058089911937714, Final Batch Loss: 0.10360816866159439\n",
      "Epoch 4914, Loss: 0.05414211191236973, Final Batch Loss: 0.018587494269013405\n",
      "Epoch 4915, Loss: 0.07497008144855499, Final Batch Loss: 0.03225181624293327\n",
      "Epoch 4916, Loss: 0.11966690048575401, Final Batch Loss: 0.046140048652887344\n",
      "Epoch 4917, Loss: 0.08980708941817284, Final Batch Loss: 0.06511067599058151\n",
      "Epoch 4918, Loss: 0.1641380861401558, Final Batch Loss: 0.1146610826253891\n",
      "Epoch 4919, Loss: 0.0742215272039175, Final Batch Loss: 0.02308591641485691\n",
      "Epoch 4920, Loss: 0.06588883697986603, Final Batch Loss: 0.03817461058497429\n",
      "Epoch 4921, Loss: 0.1040128767490387, Final Batch Loss: 0.03447258472442627\n",
      "Epoch 4922, Loss: 0.14126230031251907, Final Batch Loss: 0.09763237833976746\n",
      "Epoch 4923, Loss: 0.09786292165517807, Final Batch Loss: 0.06420759111642838\n",
      "Epoch 4924, Loss: 0.1364232636988163, Final Batch Loss: 0.05429088696837425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4925, Loss: 0.11745854467153549, Final Batch Loss: 0.05188785493373871\n",
      "Epoch 4926, Loss: 0.14731048792600632, Final Batch Loss: 0.08872197568416595\n",
      "Epoch 4927, Loss: 0.10037306323647499, Final Batch Loss: 0.05816425755620003\n",
      "Epoch 4928, Loss: 0.1382024735212326, Final Batch Loss: 0.09272068738937378\n",
      "Epoch 4929, Loss: 0.07944286987185478, Final Batch Loss: 0.030293099582195282\n",
      "Epoch 4930, Loss: 0.12317029386758804, Final Batch Loss: 0.07177787274122238\n",
      "Epoch 4931, Loss: 0.09175790846347809, Final Batch Loss: 0.043266601860523224\n",
      "Epoch 4932, Loss: 0.08096087723970413, Final Batch Loss: 0.0449591688811779\n",
      "Epoch 4933, Loss: 0.10294877737760544, Final Batch Loss: 0.04572112485766411\n",
      "Epoch 4934, Loss: 0.12872863560914993, Final Batch Loss: 0.04178113490343094\n",
      "Epoch 4935, Loss: 0.08136799558997154, Final Batch Loss: 0.033704619854688644\n",
      "Epoch 4936, Loss: 0.12796306423842907, Final Batch Loss: 0.10214005410671234\n",
      "Epoch 4937, Loss: 0.08512492664158344, Final Batch Loss: 0.05754866823554039\n",
      "Epoch 4938, Loss: 0.09637097269296646, Final Batch Loss: 0.03946078568696976\n",
      "Epoch 4939, Loss: 0.06248467601835728, Final Batch Loss: 0.032894257456064224\n",
      "Epoch 4940, Loss: 0.1338471919298172, Final Batch Loss: 0.055542632937431335\n",
      "Epoch 4941, Loss: 0.07225513644516468, Final Batch Loss: 0.042420126497745514\n",
      "Epoch 4942, Loss: 0.10207988508045673, Final Batch Loss: 0.022952290251851082\n",
      "Epoch 4943, Loss: 0.09269283339381218, Final Batch Loss: 0.0383557565510273\n",
      "Epoch 4944, Loss: 0.06589081138372421, Final Batch Loss: 0.035961832851171494\n",
      "Epoch 4945, Loss: 0.06771047785878181, Final Batch Loss: 0.04716865345835686\n",
      "Epoch 4946, Loss: 0.07089109346270561, Final Batch Loss: 0.0276329442858696\n",
      "Epoch 4947, Loss: 0.0799407884478569, Final Batch Loss: 0.04847165569663048\n",
      "Epoch 4948, Loss: 0.07190890051424503, Final Batch Loss: 0.050389617681503296\n",
      "Epoch 4949, Loss: 0.07365383021533489, Final Batch Loss: 0.0302871223539114\n",
      "Epoch 4950, Loss: 0.07692841812968254, Final Batch Loss: 0.039020005613565445\n",
      "Epoch 4951, Loss: 0.0968870297074318, Final Batch Loss: 0.05839170515537262\n",
      "Epoch 4952, Loss: 0.10688531026244164, Final Batch Loss: 0.05358695983886719\n",
      "Epoch 4953, Loss: 0.25862670689821243, Final Batch Loss: 0.07748036831617355\n",
      "Epoch 4954, Loss: 0.08231425285339355, Final Batch Loss: 0.03395551070570946\n",
      "Epoch 4955, Loss: 0.08353149890899658, Final Batch Loss: 0.03983313962817192\n",
      "Epoch 4956, Loss: 0.07563312537968159, Final Batch Loss: 0.04897849261760712\n",
      "Epoch 4957, Loss: 0.078152135014534, Final Batch Loss: 0.051825765520334244\n",
      "Epoch 4958, Loss: 0.07556304708123207, Final Batch Loss: 0.03309353068470955\n",
      "Epoch 4959, Loss: 0.09125266224145889, Final Batch Loss: 0.03861290588974953\n",
      "Epoch 4960, Loss: 0.1261206716299057, Final Batch Loss: 0.07504764199256897\n",
      "Epoch 4961, Loss: 0.09035034105181694, Final Batch Loss: 0.03577748313546181\n",
      "Epoch 4962, Loss: 0.0922306701540947, Final Batch Loss: 0.054283976554870605\n",
      "Epoch 4963, Loss: 0.08235414326190948, Final Batch Loss: 0.03694307059049606\n",
      "Epoch 4964, Loss: 0.05917688086628914, Final Batch Loss: 0.028067097067832947\n",
      "Epoch 4965, Loss: 0.12002405896782875, Final Batch Loss: 0.052478622645139694\n",
      "Epoch 4966, Loss: 0.07792177051305771, Final Batch Loss: 0.03912898898124695\n",
      "Epoch 4967, Loss: 0.06217781826853752, Final Batch Loss: 0.03685913607478142\n",
      "Epoch 4968, Loss: 0.10295175388455391, Final Batch Loss: 0.03470287844538689\n",
      "Epoch 4969, Loss: 0.0692928247153759, Final Batch Loss: 0.036393266171216965\n",
      "Epoch 4970, Loss: 0.07114739343523979, Final Batch Loss: 0.0341637060046196\n",
      "Epoch 4971, Loss: 0.1080113835632801, Final Batch Loss: 0.04873019829392433\n",
      "Epoch 4972, Loss: 0.068691685795784, Final Batch Loss: 0.020995818078517914\n",
      "Epoch 4973, Loss: 0.07912050932645798, Final Batch Loss: 0.04819600656628609\n",
      "Epoch 4974, Loss: 0.06942251324653625, Final Batch Loss: 0.049629464745521545\n",
      "Epoch 4975, Loss: 0.13263989984989166, Final Batch Loss: 0.07252505421638489\n",
      "Epoch 4976, Loss: 0.06905516237020493, Final Batch Loss: 0.03324703127145767\n",
      "Epoch 4977, Loss: 0.09621678292751312, Final Batch Loss: 0.030355796217918396\n",
      "Epoch 4978, Loss: 0.05586393363773823, Final Batch Loss: 0.037322402000427246\n",
      "Epoch 4979, Loss: 0.09065346978604794, Final Batch Loss: 0.06294292956590652\n",
      "Epoch 4980, Loss: 0.07039415463805199, Final Batch Loss: 0.02252592146396637\n",
      "Epoch 4981, Loss: 0.09760155156254768, Final Batch Loss: 0.036628156900405884\n",
      "Epoch 4982, Loss: 0.07794836163520813, Final Batch Loss: 0.04841794818639755\n",
      "Epoch 4983, Loss: 0.12612070888280869, Final Batch Loss: 0.0645204558968544\n",
      "Epoch 4984, Loss: 0.1097821556031704, Final Batch Loss: 0.04942189157009125\n",
      "Epoch 4985, Loss: 0.08724895119667053, Final Batch Loss: 0.01835082471370697\n",
      "Epoch 4986, Loss: 0.10501190647482872, Final Batch Loss: 0.06652418524026871\n",
      "Epoch 4987, Loss: 0.06476041115820408, Final Batch Loss: 0.03992682322859764\n",
      "Epoch 4988, Loss: 0.05703195184469223, Final Batch Loss: 0.01740756258368492\n",
      "Epoch 4989, Loss: 0.07179870642721653, Final Batch Loss: 0.022093983367085457\n",
      "Epoch 4990, Loss: 0.09017834439873695, Final Batch Loss: 0.047729287296533585\n",
      "Epoch 4991, Loss: 0.08893774077296257, Final Batch Loss: 0.05817711353302002\n",
      "Epoch 4992, Loss: 0.07665208354592323, Final Batch Loss: 0.05283915251493454\n",
      "Epoch 4993, Loss: 0.07227976061403751, Final Batch Loss: 0.020588764920830727\n",
      "Epoch 4994, Loss: 0.10839452967047691, Final Batch Loss: 0.06376142054796219\n",
      "Epoch 4995, Loss: 0.07902716100215912, Final Batch Loss: 0.0344066396355629\n",
      "Epoch 4996, Loss: 0.09683981165289879, Final Batch Loss: 0.04607119411230087\n",
      "Epoch 4997, Loss: 0.07749011553823948, Final Batch Loss: 0.05479172244668007\n",
      "Epoch 4998, Loss: 0.07167807221412659, Final Batch Loss: 0.03217456117272377\n",
      "Epoch 4999, Loss: 0.10847475379705429, Final Batch Loss: 0.06568680703639984\n",
      "Epoch 5000, Loss: 0.12202594056725502, Final Batch Loss: 0.06059692054986954\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  2  0]\n",
      " [ 0 31  0]\n",
      " [ 0  0 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.941     0.970        34\n",
      "           1      0.939     1.000     0.969        31\n",
      "           2      1.000     1.000     1.000        18\n",
      "\n",
      "    accuracy                          0.976        83\n",
      "   macro avg      0.980     0.980     0.979        83\n",
      "weighted avg      0.977     0.976     0.976        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../saved_models/UCI 3 User Classifier Group 5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
