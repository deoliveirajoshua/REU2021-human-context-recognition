{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 23) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 25) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 27) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A0 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_1 = gen(to_gen).detach().numpy()\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A1 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_2 = gen(to_gen).detach().numpy()\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A2 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_3 = gen(to_gen).detach().numpy()\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A0 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_4 = gen(to_gen).detach().numpy()\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A1 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_5 = gen(to_gen).detach().numpy()\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A2 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_6 = gen(to_gen).detach().numpy()\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A0 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_7 = gen(to_gen).detach().numpy()\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A1 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_8 = gen(to_gen).detach().numpy()\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A2 Excluded Group 5_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_9 = gen(to_gen).detach().numpy()\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_test = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_test = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [23, 25, 27]\n",
    "X_train, y_train = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.619829177856445, Final Batch Loss: 2.197638750076294\n",
      "Epoch 2, Loss: 6.648569345474243, Final Batch Loss: 2.2443320751190186\n",
      "Epoch 3, Loss: 6.587665796279907, Final Batch Loss: 2.1790571212768555\n",
      "Epoch 4, Loss: 6.585559129714966, Final Batch Loss: 2.187641143798828\n",
      "Epoch 5, Loss: 6.624388217926025, Final Batch Loss: 2.2437262535095215\n",
      "Epoch 6, Loss: 6.588857412338257, Final Batch Loss: 2.2103075981140137\n",
      "Epoch 7, Loss: 6.566272974014282, Final Batch Loss: 2.1961586475372314\n",
      "Epoch 8, Loss: 6.5700812339782715, Final Batch Loss: 2.2095401287078857\n",
      "Epoch 9, Loss: 6.5267133712768555, Final Batch Loss: 2.1743485927581787\n",
      "Epoch 10, Loss: 6.530782699584961, Final Batch Loss: 2.1883952617645264\n",
      "Epoch 11, Loss: 6.453585624694824, Final Batch Loss: 2.1248645782470703\n",
      "Epoch 12, Loss: 6.457779884338379, Final Batch Loss: 2.145509719848633\n",
      "Epoch 13, Loss: 6.444777965545654, Final Batch Loss: 2.1576883792877197\n",
      "Epoch 14, Loss: 6.367816925048828, Final Batch Loss: 2.0972516536712646\n",
      "Epoch 15, Loss: 6.369398832321167, Final Batch Loss: 2.1432433128356934\n",
      "Epoch 16, Loss: 6.262206315994263, Final Batch Loss: 2.050651788711548\n",
      "Epoch 17, Loss: 6.225767135620117, Final Batch Loss: 2.0538389682769775\n",
      "Epoch 18, Loss: 6.189725160598755, Final Batch Loss: 2.067756175994873\n",
      "Epoch 19, Loss: 6.097349405288696, Final Batch Loss: 2.0217673778533936\n",
      "Epoch 20, Loss: 6.0540196895599365, Final Batch Loss: 2.045808792114258\n",
      "Epoch 21, Loss: 5.881713509559631, Final Batch Loss: 1.9085184335708618\n",
      "Epoch 22, Loss: 5.891122341156006, Final Batch Loss: 1.9464904069900513\n",
      "Epoch 23, Loss: 5.754428505897522, Final Batch Loss: 1.8822768926620483\n",
      "Epoch 24, Loss: 5.678738236427307, Final Batch Loss: 1.8849668502807617\n",
      "Epoch 25, Loss: 5.5892722606658936, Final Batch Loss: 1.855576515197754\n",
      "Epoch 26, Loss: 5.565603613853455, Final Batch Loss: 1.854911208152771\n",
      "Epoch 27, Loss: 5.417763829231262, Final Batch Loss: 1.8038634061813354\n",
      "Epoch 28, Loss: 5.161655783653259, Final Batch Loss: 1.5814515352249146\n",
      "Epoch 29, Loss: 5.1803200244903564, Final Batch Loss: 1.6781902313232422\n",
      "Epoch 30, Loss: 5.222524285316467, Final Batch Loss: 1.7715402841567993\n",
      "Epoch 31, Loss: 5.010324954986572, Final Batch Loss: 1.6055916547775269\n",
      "Epoch 32, Loss: 5.14446759223938, Final Batch Loss: 1.821440577507019\n",
      "Epoch 33, Loss: 4.944793701171875, Final Batch Loss: 1.6579128503799438\n",
      "Epoch 34, Loss: 4.736905813217163, Final Batch Loss: 1.5094987154006958\n",
      "Epoch 35, Loss: 4.826544880867004, Final Batch Loss: 1.6547675132751465\n",
      "Epoch 36, Loss: 4.732208847999573, Final Batch Loss: 1.5733230113983154\n",
      "Epoch 37, Loss: 4.585052132606506, Final Batch Loss: 1.4300681352615356\n",
      "Epoch 38, Loss: 4.662245988845825, Final Batch Loss: 1.5303435325622559\n",
      "Epoch 39, Loss: 4.634540557861328, Final Batch Loss: 1.6097893714904785\n",
      "Epoch 40, Loss: 4.416741251945496, Final Batch Loss: 1.4502719640731812\n",
      "Epoch 41, Loss: 4.380737662315369, Final Batch Loss: 1.4336297512054443\n",
      "Epoch 42, Loss: 4.409411072731018, Final Batch Loss: 1.4793065786361694\n",
      "Epoch 43, Loss: 4.402199625968933, Final Batch Loss: 1.510042428970337\n",
      "Epoch 44, Loss: 4.226514220237732, Final Batch Loss: 1.3615869283676147\n",
      "Epoch 45, Loss: 4.204615592956543, Final Batch Loss: 1.4082248210906982\n",
      "Epoch 46, Loss: 4.138027548789978, Final Batch Loss: 1.389762043952942\n",
      "Epoch 47, Loss: 3.86279034614563, Final Batch Loss: 1.1544324159622192\n",
      "Epoch 48, Loss: 3.91989266872406, Final Batch Loss: 1.2258559465408325\n",
      "Epoch 49, Loss: 3.8490887880325317, Final Batch Loss: 1.2165454626083374\n",
      "Epoch 50, Loss: 3.9156335592269897, Final Batch Loss: 1.337061882019043\n",
      "Epoch 51, Loss: 3.8399298191070557, Final Batch Loss: 1.2306568622589111\n",
      "Epoch 52, Loss: 3.7350369691848755, Final Batch Loss: 1.1690095663070679\n",
      "Epoch 53, Loss: 3.876819610595703, Final Batch Loss: 1.4506891965866089\n",
      "Epoch 54, Loss: 3.614905595779419, Final Batch Loss: 1.100314736366272\n",
      "Epoch 55, Loss: 3.7306182384490967, Final Batch Loss: 1.259753704071045\n",
      "Epoch 56, Loss: 3.7663886547088623, Final Batch Loss: 1.4013457298278809\n",
      "Epoch 57, Loss: 3.5288575887680054, Final Batch Loss: 1.1883680820465088\n",
      "Epoch 58, Loss: 3.399421453475952, Final Batch Loss: 1.091452717781067\n",
      "Epoch 59, Loss: 3.395847201347351, Final Batch Loss: 1.1175435781478882\n",
      "Epoch 60, Loss: 3.291765570640564, Final Batch Loss: 1.0313748121261597\n",
      "Epoch 61, Loss: 3.2697229385375977, Final Batch Loss: 1.0683103799819946\n",
      "Epoch 62, Loss: 3.4021193981170654, Final Batch Loss: 1.1042369604110718\n",
      "Epoch 63, Loss: 3.1541796922683716, Final Batch Loss: 0.9997659921646118\n",
      "Epoch 64, Loss: 3.168409585952759, Final Batch Loss: 1.0238982439041138\n",
      "Epoch 65, Loss: 3.2789931297302246, Final Batch Loss: 1.1030898094177246\n",
      "Epoch 66, Loss: 3.0848487615585327, Final Batch Loss: 1.0030441284179688\n",
      "Epoch 67, Loss: 3.0809119939804077, Final Batch Loss: 1.0167092084884644\n",
      "Epoch 68, Loss: 3.130434215068817, Final Batch Loss: 1.0846489667892456\n",
      "Epoch 69, Loss: 2.9497501850128174, Final Batch Loss: 0.9752845168113708\n",
      "Epoch 70, Loss: 3.0546618700027466, Final Batch Loss: 1.0224058628082275\n",
      "Epoch 71, Loss: 2.900961399078369, Final Batch Loss: 0.9119877219200134\n",
      "Epoch 72, Loss: 3.013393819332123, Final Batch Loss: 1.0296125411987305\n",
      "Epoch 73, Loss: 3.0158631205558777, Final Batch Loss: 1.0130362510681152\n",
      "Epoch 74, Loss: 2.988543689250946, Final Batch Loss: 0.973011314868927\n",
      "Epoch 75, Loss: 2.810210108757019, Final Batch Loss: 0.8556467890739441\n",
      "Epoch 76, Loss: 2.940087914466858, Final Batch Loss: 1.0941730737686157\n",
      "Epoch 77, Loss: 2.7941901683807373, Final Batch Loss: 0.9435659646987915\n",
      "Epoch 78, Loss: 3.016923427581787, Final Batch Loss: 1.0280181169509888\n",
      "Epoch 79, Loss: 2.799581527709961, Final Batch Loss: 0.7641746997833252\n",
      "Epoch 80, Loss: 2.8513683080673218, Final Batch Loss: 0.8885128498077393\n",
      "Epoch 81, Loss: 2.9665858149528503, Final Batch Loss: 1.0691112279891968\n",
      "Epoch 82, Loss: 2.8343396186828613, Final Batch Loss: 0.9193875789642334\n",
      "Epoch 83, Loss: 2.930968403816223, Final Batch Loss: 0.9905861616134644\n",
      "Epoch 84, Loss: 2.842211425304413, Final Batch Loss: 0.9542969465255737\n",
      "Epoch 85, Loss: 2.7466441988945007, Final Batch Loss: 1.0015079975128174\n",
      "Epoch 86, Loss: 2.6826095581054688, Final Batch Loss: 0.8662809133529663\n",
      "Epoch 87, Loss: 2.6489917039871216, Final Batch Loss: 0.7812585830688477\n",
      "Epoch 88, Loss: 2.620137333869934, Final Batch Loss: 0.8408237099647522\n",
      "Epoch 89, Loss: 2.711186468601227, Final Batch Loss: 0.9404729008674622\n",
      "Epoch 90, Loss: 2.7046178579330444, Final Batch Loss: 0.9778772592544556\n",
      "Epoch 91, Loss: 2.787018835544586, Final Batch Loss: 0.9878443479537964\n",
      "Epoch 92, Loss: 2.641550302505493, Final Batch Loss: 0.8587737679481506\n",
      "Epoch 93, Loss: 2.620712101459503, Final Batch Loss: 0.8748540878295898\n",
      "Epoch 94, Loss: 2.479072690010071, Final Batch Loss: 0.7652778029441833\n",
      "Epoch 95, Loss: 2.655394971370697, Final Batch Loss: 0.895068347454071\n",
      "Epoch 96, Loss: 2.5318196415901184, Final Batch Loss: 0.8620158433914185\n",
      "Epoch 97, Loss: 2.4423032999038696, Final Batch Loss: 0.724461555480957\n",
      "Epoch 98, Loss: 2.76578152179718, Final Batch Loss: 0.9375572204589844\n",
      "Epoch 99, Loss: 2.7835075855255127, Final Batch Loss: 0.9812738299369812\n",
      "Epoch 100, Loss: 2.8115479350090027, Final Batch Loss: 0.9950905442237854\n",
      "Epoch 101, Loss: 2.5005223155021667, Final Batch Loss: 0.8467661738395691\n",
      "Epoch 102, Loss: 2.479975461959839, Final Batch Loss: 0.7955882549285889\n",
      "Epoch 103, Loss: 2.6278056502342224, Final Batch Loss: 0.8726862668991089\n",
      "Epoch 104, Loss: 2.588091790676117, Final Batch Loss: 0.9049540758132935\n",
      "Epoch 105, Loss: 2.449353039264679, Final Batch Loss: 0.7688767313957214\n",
      "Epoch 106, Loss: 2.656499683856964, Final Batch Loss: 0.8683919906616211\n",
      "Epoch 107, Loss: 2.8423126339912415, Final Batch Loss: 1.1382534503936768\n",
      "Epoch 108, Loss: 2.416585683822632, Final Batch Loss: 0.6997904181480408\n",
      "Epoch 109, Loss: 2.6337215304374695, Final Batch Loss: 0.9363127946853638\n",
      "Epoch 110, Loss: 2.431104302406311, Final Batch Loss: 0.7483612298965454\n",
      "Epoch 111, Loss: 2.4700910449028015, Final Batch Loss: 0.8112991452217102\n",
      "Epoch 112, Loss: 2.5003150701522827, Final Batch Loss: 0.8466392159461975\n",
      "Epoch 113, Loss: 2.5082871317863464, Final Batch Loss: 0.7961281538009644\n",
      "Epoch 114, Loss: 2.3168405294418335, Final Batch Loss: 0.7567383050918579\n",
      "Epoch 115, Loss: 2.4667709469795227, Final Batch Loss: 0.9080498218536377\n",
      "Epoch 116, Loss: 2.3845616579055786, Final Batch Loss: 0.7111287117004395\n",
      "Epoch 117, Loss: 2.3422086238861084, Final Batch Loss: 0.6720468401908875\n",
      "Epoch 118, Loss: 2.423885703086853, Final Batch Loss: 0.8272347450256348\n",
      "Epoch 119, Loss: 2.4860622882843018, Final Batch Loss: 0.8514747619628906\n",
      "Epoch 120, Loss: 2.4733974933624268, Final Batch Loss: 0.7759292721748352\n",
      "Epoch 121, Loss: 2.3945467472076416, Final Batch Loss: 0.766047477722168\n",
      "Epoch 122, Loss: 2.327383577823639, Final Batch Loss: 0.6678301095962524\n",
      "Epoch 123, Loss: 2.5299476981163025, Final Batch Loss: 0.982426643371582\n",
      "Epoch 124, Loss: 2.373940348625183, Final Batch Loss: 0.7901774644851685\n",
      "Epoch 125, Loss: 2.3001416325569153, Final Batch Loss: 0.666020929813385\n",
      "Epoch 126, Loss: 2.332195997238159, Final Batch Loss: 0.775665819644928\n",
      "Epoch 127, Loss: 2.579208493232727, Final Batch Loss: 1.0440363883972168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128, Loss: 2.463742434978485, Final Batch Loss: 0.8756040334701538\n",
      "Epoch 129, Loss: 2.3376263976097107, Final Batch Loss: 0.7665942311286926\n",
      "Epoch 130, Loss: 2.184199869632721, Final Batch Loss: 0.6583002209663391\n",
      "Epoch 131, Loss: 2.4381185173988342, Final Batch Loss: 0.9395009875297546\n",
      "Epoch 132, Loss: 2.3675923943519592, Final Batch Loss: 0.7617121934890747\n",
      "Epoch 133, Loss: 2.3294717669487, Final Batch Loss: 0.7748062610626221\n",
      "Epoch 134, Loss: 2.489205300807953, Final Batch Loss: 0.9380477666854858\n",
      "Epoch 135, Loss: 2.5087294578552246, Final Batch Loss: 0.9053663015365601\n",
      "Epoch 136, Loss: 2.2570835947990417, Final Batch Loss: 0.7095527052879333\n",
      "Epoch 137, Loss: 2.2963627576828003, Final Batch Loss: 0.7385416030883789\n",
      "Epoch 138, Loss: 2.221068859100342, Final Batch Loss: 0.6743912100791931\n",
      "Epoch 139, Loss: 2.5203492045402527, Final Batch Loss: 1.0324922800064087\n",
      "Epoch 140, Loss: 2.2309271097183228, Final Batch Loss: 0.6892232894897461\n",
      "Epoch 141, Loss: 2.241223931312561, Final Batch Loss: 0.6987471580505371\n",
      "Epoch 142, Loss: 2.2812381386756897, Final Batch Loss: 0.7150720953941345\n",
      "Epoch 143, Loss: 2.3155248761177063, Final Batch Loss: 0.8505228757858276\n",
      "Epoch 144, Loss: 2.271171510219574, Final Batch Loss: 0.730457067489624\n",
      "Epoch 145, Loss: 2.265439748764038, Final Batch Loss: 0.7872413396835327\n",
      "Epoch 146, Loss: 2.270330548286438, Final Batch Loss: 0.7426607608795166\n",
      "Epoch 147, Loss: 2.2407765984535217, Final Batch Loss: 0.7091638445854187\n",
      "Epoch 148, Loss: 2.307587206363678, Final Batch Loss: 0.7535392642021179\n",
      "Epoch 149, Loss: 2.2647531628608704, Final Batch Loss: 0.712184488773346\n",
      "Epoch 150, Loss: 2.2281911373138428, Final Batch Loss: 0.6899169683456421\n",
      "Epoch 151, Loss: 2.245484709739685, Final Batch Loss: 0.7862101197242737\n",
      "Epoch 152, Loss: 2.138154089450836, Final Batch Loss: 0.6900955438613892\n",
      "Epoch 153, Loss: 2.227247893810272, Final Batch Loss: 0.697406530380249\n",
      "Epoch 154, Loss: 2.1947473883628845, Final Batch Loss: 0.7166475653648376\n",
      "Epoch 155, Loss: 2.345058798789978, Final Batch Loss: 0.8213261365890503\n",
      "Epoch 156, Loss: 2.1014221906661987, Final Batch Loss: 0.5902613997459412\n",
      "Epoch 157, Loss: 2.1960301995277405, Final Batch Loss: 0.7822087407112122\n",
      "Epoch 158, Loss: 2.0763708353042603, Final Batch Loss: 0.5756769180297852\n",
      "Epoch 159, Loss: 2.400048315525055, Final Batch Loss: 0.9570480585098267\n",
      "Epoch 160, Loss: 2.2723260521888733, Final Batch Loss: 0.8178769946098328\n",
      "Epoch 161, Loss: 2.224104106426239, Final Batch Loss: 0.7256457209587097\n",
      "Epoch 162, Loss: 2.15377140045166, Final Batch Loss: 0.6359785795211792\n",
      "Epoch 163, Loss: 2.237213373184204, Final Batch Loss: 0.8373403549194336\n",
      "Epoch 164, Loss: 2.1648009419441223, Final Batch Loss: 0.6808435320854187\n",
      "Epoch 165, Loss: 2.1387943029403687, Final Batch Loss: 0.7352824211120605\n",
      "Epoch 166, Loss: 2.156630277633667, Final Batch Loss: 0.7094317674636841\n",
      "Epoch 167, Loss: 2.176677167415619, Final Batch Loss: 0.7932227253913879\n",
      "Epoch 168, Loss: 2.2003775238990784, Final Batch Loss: 0.7454143166542053\n",
      "Epoch 169, Loss: 2.1645795106887817, Final Batch Loss: 0.6936087608337402\n",
      "Epoch 170, Loss: 2.0584309697151184, Final Batch Loss: 0.5997981429100037\n",
      "Epoch 171, Loss: 2.136856436729431, Final Batch Loss: 0.7636772394180298\n",
      "Epoch 172, Loss: 2.164789080619812, Final Batch Loss: 0.7167747616767883\n",
      "Epoch 173, Loss: 2.2170673608779907, Final Batch Loss: 0.7941655516624451\n",
      "Epoch 174, Loss: 2.362398147583008, Final Batch Loss: 0.9096795916557312\n",
      "Epoch 175, Loss: 2.137649714946747, Final Batch Loss: 0.6886736750602722\n",
      "Epoch 176, Loss: 2.3193766474723816, Final Batch Loss: 0.7695637345314026\n",
      "Epoch 177, Loss: 2.2609257102012634, Final Batch Loss: 0.8412350416183472\n",
      "Epoch 178, Loss: 2.3060901165008545, Final Batch Loss: 0.7828339338302612\n",
      "Epoch 179, Loss: 2.231661796569824, Final Batch Loss: 0.7231356501579285\n",
      "Epoch 180, Loss: 2.1125676035881042, Final Batch Loss: 0.682508111000061\n",
      "Epoch 181, Loss: 2.305288076400757, Final Batch Loss: 0.8482691049575806\n",
      "Epoch 182, Loss: 2.267646312713623, Final Batch Loss: 0.7848650813102722\n",
      "Epoch 183, Loss: 1.9730563163757324, Final Batch Loss: 0.5971086621284485\n",
      "Epoch 184, Loss: 2.020920991897583, Final Batch Loss: 0.6146555542945862\n",
      "Epoch 185, Loss: 2.138694167137146, Final Batch Loss: 0.6537612676620483\n",
      "Epoch 186, Loss: 2.065428853034973, Final Batch Loss: 0.6045275926589966\n",
      "Epoch 187, Loss: 2.089288890361786, Final Batch Loss: 0.692351758480072\n",
      "Epoch 188, Loss: 2.0908327102661133, Final Batch Loss: 0.6785837411880493\n",
      "Epoch 189, Loss: 2.191631019115448, Final Batch Loss: 0.7528015971183777\n",
      "Epoch 190, Loss: 2.148225247859955, Final Batch Loss: 0.6893708109855652\n",
      "Epoch 191, Loss: 2.11723792552948, Final Batch Loss: 0.7433691024780273\n",
      "Epoch 192, Loss: 2.0847703218460083, Final Batch Loss: 0.8032602071762085\n",
      "Epoch 193, Loss: 2.2132838368415833, Final Batch Loss: 0.7643786668777466\n",
      "Epoch 194, Loss: 2.0933366417884827, Final Batch Loss: 0.6977816224098206\n",
      "Epoch 195, Loss: 2.0983834266662598, Final Batch Loss: 0.6256971955299377\n",
      "Epoch 196, Loss: 2.0219568014144897, Final Batch Loss: 0.5832769274711609\n",
      "Epoch 197, Loss: 2.0173102021217346, Final Batch Loss: 0.6091950535774231\n",
      "Epoch 198, Loss: 2.0959015488624573, Final Batch Loss: 0.6825383901596069\n",
      "Epoch 199, Loss: 2.135359466075897, Final Batch Loss: 0.6970664858818054\n",
      "Epoch 200, Loss: 2.123282253742218, Final Batch Loss: 0.7401418685913086\n",
      "Epoch 201, Loss: 2.049794375896454, Final Batch Loss: 0.6328215599060059\n",
      "Epoch 202, Loss: 2.059468924999237, Final Batch Loss: 0.7040982842445374\n",
      "Epoch 203, Loss: 1.9853096008300781, Final Batch Loss: 0.627321720123291\n",
      "Epoch 204, Loss: 2.0758426785469055, Final Batch Loss: 0.6567847728729248\n",
      "Epoch 205, Loss: 1.997994601726532, Final Batch Loss: 0.610633134841919\n",
      "Epoch 206, Loss: 2.1214101314544678, Final Batch Loss: 0.683861255645752\n",
      "Epoch 207, Loss: 2.179013669490814, Final Batch Loss: 0.7723132371902466\n",
      "Epoch 208, Loss: 2.1223140954971313, Final Batch Loss: 0.7380388379096985\n",
      "Epoch 209, Loss: 2.1098732352256775, Final Batch Loss: 0.7039812803268433\n",
      "Epoch 210, Loss: 2.0419045090675354, Final Batch Loss: 0.6479178667068481\n",
      "Epoch 211, Loss: 2.2057231664657593, Final Batch Loss: 0.8150167465209961\n",
      "Epoch 212, Loss: 1.9025532007217407, Final Batch Loss: 0.548341691493988\n",
      "Epoch 213, Loss: 1.931482970714569, Final Batch Loss: 0.5676463842391968\n",
      "Epoch 214, Loss: 1.9729022979736328, Final Batch Loss: 0.5915396809577942\n",
      "Epoch 215, Loss: 2.156430423259735, Final Batch Loss: 0.7727988362312317\n",
      "Epoch 216, Loss: 2.006213366985321, Final Batch Loss: 0.6555257439613342\n",
      "Epoch 217, Loss: 1.9357667565345764, Final Batch Loss: 0.5621428489685059\n",
      "Epoch 218, Loss: 1.8983275890350342, Final Batch Loss: 0.5493841767311096\n",
      "Epoch 219, Loss: 2.0097835659980774, Final Batch Loss: 0.6048268675804138\n",
      "Epoch 220, Loss: 1.9540170431137085, Final Batch Loss: 0.6136452555656433\n",
      "Epoch 221, Loss: 1.8173079192638397, Final Batch Loss: 0.4672872722148895\n",
      "Epoch 222, Loss: 2.1383546590805054, Final Batch Loss: 0.7754191160202026\n",
      "Epoch 223, Loss: 2.0833860635757446, Final Batch Loss: 0.7969611287117004\n",
      "Epoch 224, Loss: 2.101547598838806, Final Batch Loss: 0.7265703678131104\n",
      "Epoch 225, Loss: 2.0069640278816223, Final Batch Loss: 0.6434773206710815\n",
      "Epoch 226, Loss: 1.9193158149719238, Final Batch Loss: 0.5543155074119568\n",
      "Epoch 227, Loss: 2.007274329662323, Final Batch Loss: 0.6324766278266907\n",
      "Epoch 228, Loss: 2.038325846195221, Final Batch Loss: 0.6637852787971497\n",
      "Epoch 229, Loss: 1.9554880261421204, Final Batch Loss: 0.6217191219329834\n",
      "Epoch 230, Loss: 1.921481430530548, Final Batch Loss: 0.6400744915008545\n",
      "Epoch 231, Loss: 1.9239400029182434, Final Batch Loss: 0.6697605848312378\n",
      "Epoch 232, Loss: 2.088843286037445, Final Batch Loss: 0.727918267250061\n",
      "Epoch 233, Loss: 2.122462272644043, Final Batch Loss: 0.8437502980232239\n",
      "Epoch 234, Loss: 2.0364884734153748, Final Batch Loss: 0.7447629570960999\n",
      "Epoch 235, Loss: 1.9246872067451477, Final Batch Loss: 0.5745803713798523\n",
      "Epoch 236, Loss: 1.92054682970047, Final Batch Loss: 0.5128871202468872\n",
      "Epoch 237, Loss: 1.9973454475402832, Final Batch Loss: 0.6300830841064453\n",
      "Epoch 238, Loss: 1.961850106716156, Final Batch Loss: 0.61298006772995\n",
      "Epoch 239, Loss: 2.0680580735206604, Final Batch Loss: 0.7293410897254944\n",
      "Epoch 240, Loss: 2.068690001964569, Final Batch Loss: 0.7417956590652466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241, Loss: 1.975939393043518, Final Batch Loss: 0.6067597270011902\n",
      "Epoch 242, Loss: 2.0347346663475037, Final Batch Loss: 0.6805775165557861\n",
      "Epoch 243, Loss: 2.0059423446655273, Final Batch Loss: 0.6798161268234253\n",
      "Epoch 244, Loss: 2.0292674899101257, Final Batch Loss: 0.650604784488678\n",
      "Epoch 245, Loss: 2.026515543460846, Final Batch Loss: 0.6224198937416077\n",
      "Epoch 246, Loss: 2.0488044023513794, Final Batch Loss: 0.6539190411567688\n",
      "Epoch 247, Loss: 2.1211864948272705, Final Batch Loss: 0.7961387038230896\n",
      "Epoch 248, Loss: 1.9999868273735046, Final Batch Loss: 0.7639214396476746\n",
      "Epoch 249, Loss: 1.956190288066864, Final Batch Loss: 0.612602174282074\n",
      "Epoch 250, Loss: 1.9613187313079834, Final Batch Loss: 0.7039548754692078\n",
      "Epoch 251, Loss: 2.029394745826721, Final Batch Loss: 0.728575587272644\n",
      "Epoch 252, Loss: 2.0284444093704224, Final Batch Loss: 0.7442853450775146\n",
      "Epoch 253, Loss: 1.9892407059669495, Final Batch Loss: 0.6547983884811401\n",
      "Epoch 254, Loss: 1.9979564547538757, Final Batch Loss: 0.7008102536201477\n",
      "Epoch 255, Loss: 1.9018818736076355, Final Batch Loss: 0.6225983500480652\n",
      "Epoch 256, Loss: 1.8989555835723877, Final Batch Loss: 0.5521664619445801\n",
      "Epoch 257, Loss: 1.8547714352607727, Final Batch Loss: 0.5492187142372131\n",
      "Epoch 258, Loss: 2.0148914456367493, Final Batch Loss: 0.6725143790245056\n",
      "Epoch 259, Loss: 2.1048689484596252, Final Batch Loss: 0.7769140005111694\n",
      "Epoch 260, Loss: 1.9719005823135376, Final Batch Loss: 0.659858226776123\n",
      "Epoch 261, Loss: 1.983060896396637, Final Batch Loss: 0.6124297380447388\n",
      "Epoch 262, Loss: 1.948981523513794, Final Batch Loss: 0.5991443395614624\n",
      "Epoch 263, Loss: 1.9060037732124329, Final Batch Loss: 0.62496417760849\n",
      "Epoch 264, Loss: 1.8847021460533142, Final Batch Loss: 0.5747668147087097\n",
      "Epoch 265, Loss: 2.0826355814933777, Final Batch Loss: 0.8121086955070496\n",
      "Epoch 266, Loss: 1.925541639328003, Final Batch Loss: 0.6272431015968323\n",
      "Epoch 267, Loss: 1.8841608166694641, Final Batch Loss: 0.6085349321365356\n",
      "Epoch 268, Loss: 1.931514024734497, Final Batch Loss: 0.6353679299354553\n",
      "Epoch 269, Loss: 1.9509521126747131, Final Batch Loss: 0.7589833736419678\n",
      "Epoch 270, Loss: 1.8463448882102966, Final Batch Loss: 0.5535836219787598\n",
      "Epoch 271, Loss: 1.946308672428131, Final Batch Loss: 0.6866974830627441\n",
      "Epoch 272, Loss: 2.115582585334778, Final Batch Loss: 0.796501636505127\n",
      "Epoch 273, Loss: 1.9027835130691528, Final Batch Loss: 0.6294048428535461\n",
      "Epoch 274, Loss: 2.0102071166038513, Final Batch Loss: 0.6702314019203186\n",
      "Epoch 275, Loss: 1.838517189025879, Final Batch Loss: 0.5379490256309509\n",
      "Epoch 276, Loss: 2.0282744765281677, Final Batch Loss: 0.6463518142700195\n",
      "Epoch 277, Loss: 1.9295232892036438, Final Batch Loss: 0.6521305441856384\n",
      "Epoch 278, Loss: 1.9347265362739563, Final Batch Loss: 0.6506375074386597\n",
      "Epoch 279, Loss: 1.8413293957710266, Final Batch Loss: 0.5404202342033386\n",
      "Epoch 280, Loss: 2.0288875699043274, Final Batch Loss: 0.679438054561615\n",
      "Epoch 281, Loss: 1.9803906083106995, Final Batch Loss: 0.620585024356842\n",
      "Epoch 282, Loss: 1.7772371768951416, Final Batch Loss: 0.4539083242416382\n",
      "Epoch 283, Loss: 1.8719097971916199, Final Batch Loss: 0.6147867441177368\n",
      "Epoch 284, Loss: 1.9388402700424194, Final Batch Loss: 0.7093383073806763\n",
      "Epoch 285, Loss: 1.8370694518089294, Final Batch Loss: 0.6617863178253174\n",
      "Epoch 286, Loss: 1.9364436268806458, Final Batch Loss: 0.676842987537384\n",
      "Epoch 287, Loss: 1.838425099849701, Final Batch Loss: 0.5868056416511536\n",
      "Epoch 288, Loss: 1.7366012036800385, Final Batch Loss: 0.4668717682361603\n",
      "Epoch 289, Loss: 1.8080927729606628, Final Batch Loss: 0.5164626240730286\n",
      "Epoch 290, Loss: 1.827391505241394, Final Batch Loss: 0.5261377096176147\n",
      "Epoch 291, Loss: 2.0026081204414368, Final Batch Loss: 0.7664418816566467\n",
      "Epoch 292, Loss: 1.7901734411716461, Final Batch Loss: 0.4881153404712677\n",
      "Epoch 293, Loss: 1.7494198083877563, Final Batch Loss: 0.46597492694854736\n",
      "Epoch 294, Loss: 2.1206979751586914, Final Batch Loss: 0.8143633604049683\n",
      "Epoch 295, Loss: 1.7850285768508911, Final Batch Loss: 0.647118091583252\n",
      "Epoch 296, Loss: 1.855480134487152, Final Batch Loss: 0.5715368986129761\n",
      "Epoch 297, Loss: 1.7965173125267029, Final Batch Loss: 0.5883193612098694\n",
      "Epoch 298, Loss: 1.751203328371048, Final Batch Loss: 0.48570922017097473\n",
      "Epoch 299, Loss: 1.8431740403175354, Final Batch Loss: 0.626091480255127\n",
      "Epoch 300, Loss: 1.8439111113548279, Final Batch Loss: 0.5596967339515686\n",
      "Epoch 301, Loss: 2.1395269632339478, Final Batch Loss: 0.8591074347496033\n",
      "Epoch 302, Loss: 1.8188241124153137, Final Batch Loss: 0.5917103290557861\n",
      "Epoch 303, Loss: 1.84019136428833, Final Batch Loss: 0.588365375995636\n",
      "Epoch 304, Loss: 1.8715844750404358, Final Batch Loss: 0.647110104560852\n",
      "Epoch 305, Loss: 1.830293357372284, Final Batch Loss: 0.6411802172660828\n",
      "Epoch 306, Loss: 1.8872255086898804, Final Batch Loss: 0.6414526700973511\n",
      "Epoch 307, Loss: 1.7155821919441223, Final Batch Loss: 0.5133078694343567\n",
      "Epoch 308, Loss: 1.843381106853485, Final Batch Loss: 0.6024184823036194\n",
      "Epoch 309, Loss: 1.941959023475647, Final Batch Loss: 0.7443882822990417\n",
      "Epoch 310, Loss: 1.8226889967918396, Final Batch Loss: 0.6437204480171204\n",
      "Epoch 311, Loss: 1.8059197664260864, Final Batch Loss: 0.5788921117782593\n",
      "Epoch 312, Loss: 1.7596763372421265, Final Batch Loss: 0.5566667318344116\n",
      "Epoch 313, Loss: 1.9790440797805786, Final Batch Loss: 0.732355535030365\n",
      "Epoch 314, Loss: 1.69499933719635, Final Batch Loss: 0.4751790761947632\n",
      "Epoch 315, Loss: 1.8484646677970886, Final Batch Loss: 0.6420729756355286\n",
      "Epoch 316, Loss: 1.7237395644187927, Final Batch Loss: 0.5302016139030457\n",
      "Epoch 317, Loss: 1.8247867226600647, Final Batch Loss: 0.6051390171051025\n",
      "Epoch 318, Loss: 1.7375130653381348, Final Batch Loss: 0.48465466499328613\n",
      "Epoch 319, Loss: 1.774202585220337, Final Batch Loss: 0.5665906667709351\n",
      "Epoch 320, Loss: 1.8988443613052368, Final Batch Loss: 0.7360741496086121\n",
      "Epoch 321, Loss: 1.7914865612983704, Final Batch Loss: 0.6309321522712708\n",
      "Epoch 322, Loss: 1.6674602627754211, Final Batch Loss: 0.463925302028656\n",
      "Epoch 323, Loss: 1.8131955862045288, Final Batch Loss: 0.6182997822761536\n",
      "Epoch 324, Loss: 1.692798674106598, Final Batch Loss: 0.5055882930755615\n",
      "Epoch 325, Loss: 1.7984837889671326, Final Batch Loss: 0.5549005270004272\n",
      "Epoch 326, Loss: 1.6229168176651, Final Batch Loss: 0.49086833000183105\n",
      "Epoch 327, Loss: 1.7779576778411865, Final Batch Loss: 0.5841297507286072\n",
      "Epoch 328, Loss: 1.7302497625350952, Final Batch Loss: 0.5470208525657654\n",
      "Epoch 329, Loss: 1.7993178963661194, Final Batch Loss: 0.6717633605003357\n",
      "Epoch 330, Loss: 1.7022360563278198, Final Batch Loss: 0.5006380677223206\n",
      "Epoch 331, Loss: 1.6867729723453522, Final Batch Loss: 0.48612192273139954\n",
      "Epoch 332, Loss: 1.7875702381134033, Final Batch Loss: 0.6138531565666199\n",
      "Epoch 333, Loss: 1.7488742470741272, Final Batch Loss: 0.6030862927436829\n",
      "Epoch 334, Loss: 1.9015825986862183, Final Batch Loss: 0.686529278755188\n",
      "Epoch 335, Loss: 1.8036807775497437, Final Batch Loss: 0.6457874178886414\n",
      "Epoch 336, Loss: 1.7511244416236877, Final Batch Loss: 0.5301297307014465\n",
      "Epoch 337, Loss: 1.7452919483184814, Final Batch Loss: 0.5767357349395752\n",
      "Epoch 338, Loss: 1.793488085269928, Final Batch Loss: 0.6773094534873962\n",
      "Epoch 339, Loss: 1.6106045842170715, Final Batch Loss: 0.4704555869102478\n",
      "Epoch 340, Loss: 1.8020929098129272, Final Batch Loss: 0.5915331244468689\n",
      "Epoch 341, Loss: 1.8552832007408142, Final Batch Loss: 0.7282083630561829\n",
      "Epoch 342, Loss: 1.9031507968902588, Final Batch Loss: 0.6966146230697632\n",
      "Epoch 343, Loss: 1.7608891129493713, Final Batch Loss: 0.5843150615692139\n",
      "Epoch 344, Loss: 1.7004190683364868, Final Batch Loss: 0.5307559370994568\n",
      "Epoch 345, Loss: 1.7142969965934753, Final Batch Loss: 0.5853821039199829\n",
      "Epoch 346, Loss: 1.6825340390205383, Final Batch Loss: 0.5132028460502625\n",
      "Epoch 347, Loss: 1.8894500732421875, Final Batch Loss: 0.7610272169113159\n",
      "Epoch 348, Loss: 1.7815345525741577, Final Batch Loss: 0.6010435223579407\n",
      "Epoch 349, Loss: 1.7952294945716858, Final Batch Loss: 0.6678363084793091\n",
      "Epoch 350, Loss: 1.8078976273536682, Final Batch Loss: 0.652632474899292\n",
      "Epoch 351, Loss: 1.7600332498550415, Final Batch Loss: 0.6294689774513245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352, Loss: 1.8113875985145569, Final Batch Loss: 0.6981579661369324\n",
      "Epoch 353, Loss: 1.722436934709549, Final Batch Loss: 0.6365589499473572\n",
      "Epoch 354, Loss: 1.6417247951030731, Final Batch Loss: 0.46797290444374084\n",
      "Epoch 355, Loss: 1.6319147944450378, Final Batch Loss: 0.5420902967453003\n",
      "Epoch 356, Loss: 1.6197634935379028, Final Batch Loss: 0.5023446083068848\n",
      "Epoch 357, Loss: 1.6617023348808289, Final Batch Loss: 0.6057009696960449\n",
      "Epoch 358, Loss: 1.8543100357055664, Final Batch Loss: 0.6687543988227844\n",
      "Epoch 359, Loss: 1.724384844303131, Final Batch Loss: 0.6125060319900513\n",
      "Epoch 360, Loss: 1.8050233721733093, Final Batch Loss: 0.6299194693565369\n",
      "Epoch 361, Loss: 1.8523091077804565, Final Batch Loss: 0.7397468090057373\n",
      "Epoch 362, Loss: 1.7919533848762512, Final Batch Loss: 0.6553140878677368\n",
      "Epoch 363, Loss: 1.6314950585365295, Final Batch Loss: 0.5407915711402893\n",
      "Epoch 364, Loss: 1.8203009366989136, Final Batch Loss: 0.7171008586883545\n",
      "Epoch 365, Loss: 1.5799390077590942, Final Batch Loss: 0.5127207636833191\n",
      "Epoch 366, Loss: 1.5752207338809967, Final Batch Loss: 0.4000854790210724\n",
      "Epoch 367, Loss: 1.6468791365623474, Final Batch Loss: 0.6051428914070129\n",
      "Epoch 368, Loss: 1.680830478668213, Final Batch Loss: 0.561678409576416\n",
      "Epoch 369, Loss: 1.6151807308197021, Final Batch Loss: 0.5791962146759033\n",
      "Epoch 370, Loss: 1.5477022230625153, Final Batch Loss: 0.43731287121772766\n",
      "Epoch 371, Loss: 1.6115171313285828, Final Batch Loss: 0.5233759880065918\n",
      "Epoch 372, Loss: 1.7413084506988525, Final Batch Loss: 0.6063413023948669\n",
      "Epoch 373, Loss: 1.5294622480869293, Final Batch Loss: 0.39175376296043396\n",
      "Epoch 374, Loss: 1.7095407247543335, Final Batch Loss: 0.5771406292915344\n",
      "Epoch 375, Loss: 1.6226896047592163, Final Batch Loss: 0.5323075652122498\n",
      "Epoch 376, Loss: 1.6573948860168457, Final Batch Loss: 0.5445559620857239\n",
      "Epoch 377, Loss: 1.5252878367900848, Final Batch Loss: 0.41583964228630066\n",
      "Epoch 378, Loss: 1.7318041324615479, Final Batch Loss: 0.6523128151893616\n",
      "Epoch 379, Loss: 1.658011555671692, Final Batch Loss: 0.5064300894737244\n",
      "Epoch 380, Loss: 1.6935338079929352, Final Batch Loss: 0.6119072437286377\n",
      "Epoch 381, Loss: 1.612914651632309, Final Batch Loss: 0.4384024441242218\n",
      "Epoch 382, Loss: 1.53997141122818, Final Batch Loss: 0.4486987590789795\n",
      "Epoch 383, Loss: 1.6256626546382904, Final Batch Loss: 0.5421570539474487\n",
      "Epoch 384, Loss: 1.5358926355838776, Final Batch Loss: 0.415844589471817\n",
      "Epoch 385, Loss: 1.6001794040203094, Final Batch Loss: 0.45681872963905334\n",
      "Epoch 386, Loss: 1.656091570854187, Final Batch Loss: 0.493624746799469\n",
      "Epoch 387, Loss: 1.6641544103622437, Final Batch Loss: 0.5739089250564575\n",
      "Epoch 388, Loss: 1.5774149298667908, Final Batch Loss: 0.5163356065750122\n",
      "Epoch 389, Loss: 1.5655557513237, Final Batch Loss: 0.4790382385253906\n",
      "Epoch 390, Loss: 1.580839455127716, Final Batch Loss: 0.4991592764854431\n",
      "Epoch 391, Loss: 1.746401846408844, Final Batch Loss: 0.6376105546951294\n",
      "Epoch 392, Loss: 1.4415756165981293, Final Batch Loss: 0.42091140151023865\n",
      "Epoch 393, Loss: 1.6565180718898773, Final Batch Loss: 0.6270126700401306\n",
      "Epoch 394, Loss: 1.6730745434761047, Final Batch Loss: 0.5705187320709229\n",
      "Epoch 395, Loss: 1.6882115006446838, Final Batch Loss: 0.6595940589904785\n",
      "Epoch 396, Loss: 1.64643394947052, Final Batch Loss: 0.5284807085990906\n",
      "Epoch 397, Loss: 1.6874935030937195, Final Batch Loss: 0.5994839072227478\n",
      "Epoch 398, Loss: 1.4797702133655548, Final Batch Loss: 0.402260959148407\n",
      "Epoch 399, Loss: 1.7470834851264954, Final Batch Loss: 0.6491290926933289\n",
      "Epoch 400, Loss: 1.6331931054592133, Final Batch Loss: 0.4954163730144501\n",
      "Epoch 401, Loss: 1.5973429679870605, Final Batch Loss: 0.5423182845115662\n",
      "Epoch 402, Loss: 1.665770947933197, Final Batch Loss: 0.5655059218406677\n",
      "Epoch 403, Loss: 1.6268598437309265, Final Batch Loss: 0.5935640335083008\n",
      "Epoch 404, Loss: 1.5600769817829132, Final Batch Loss: 0.477401465177536\n",
      "Epoch 405, Loss: 1.56634521484375, Final Batch Loss: 0.5895174145698547\n",
      "Epoch 406, Loss: 1.5009983479976654, Final Batch Loss: 0.44000044465065\n",
      "Epoch 407, Loss: 1.7164399027824402, Final Batch Loss: 0.6558507680892944\n",
      "Epoch 408, Loss: 1.6110039353370667, Final Batch Loss: 0.5714420676231384\n",
      "Epoch 409, Loss: 1.6145610809326172, Final Batch Loss: 0.5337219834327698\n",
      "Epoch 410, Loss: 1.6293056011199951, Final Batch Loss: 0.5903397798538208\n",
      "Epoch 411, Loss: 1.4963652193546295, Final Batch Loss: 0.4829561412334442\n",
      "Epoch 412, Loss: 1.5639339089393616, Final Batch Loss: 0.504091203212738\n",
      "Epoch 413, Loss: 1.4877368211746216, Final Batch Loss: 0.46221035718917847\n",
      "Epoch 414, Loss: 1.5814737379550934, Final Batch Loss: 0.5601663589477539\n",
      "Epoch 415, Loss: 1.5704625248908997, Final Batch Loss: 0.48154258728027344\n",
      "Epoch 416, Loss: 1.4455456137657166, Final Batch Loss: 0.42241740226745605\n",
      "Epoch 417, Loss: 1.4878575801849365, Final Batch Loss: 0.466335266828537\n",
      "Epoch 418, Loss: 1.5525173246860504, Final Batch Loss: 0.4945358335971832\n",
      "Epoch 419, Loss: 1.601850688457489, Final Batch Loss: 0.5246860980987549\n",
      "Epoch 420, Loss: 1.5530290305614471, Final Batch Loss: 0.47753122448921204\n",
      "Epoch 421, Loss: 1.4268933832645416, Final Batch Loss: 0.39273926615715027\n",
      "Epoch 422, Loss: 1.415332168340683, Final Batch Loss: 0.39887553453445435\n",
      "Epoch 423, Loss: 1.5941060781478882, Final Batch Loss: 0.5799379348754883\n",
      "Epoch 424, Loss: 1.6302095651626587, Final Batch Loss: 0.5789649486541748\n",
      "Epoch 425, Loss: 1.4718547761440277, Final Batch Loss: 0.4984310269355774\n",
      "Epoch 426, Loss: 1.5152510106563568, Final Batch Loss: 0.4643217623233795\n",
      "Epoch 427, Loss: 1.5745259523391724, Final Batch Loss: 0.5573704242706299\n",
      "Epoch 428, Loss: 1.5098874270915985, Final Batch Loss: 0.5170916318893433\n",
      "Epoch 429, Loss: 1.4980736076831818, Final Batch Loss: 0.43426573276519775\n",
      "Epoch 430, Loss: 1.595212459564209, Final Batch Loss: 0.580702543258667\n",
      "Epoch 431, Loss: 1.736587941646576, Final Batch Loss: 0.641331672668457\n",
      "Epoch 432, Loss: 1.4414403140544891, Final Batch Loss: 0.4378604590892792\n",
      "Epoch 433, Loss: 1.405819445848465, Final Batch Loss: 0.4557662010192871\n",
      "Epoch 434, Loss: 1.3457969427108765, Final Batch Loss: 0.39484429359436035\n",
      "Epoch 435, Loss: 1.5271691977977753, Final Batch Loss: 0.4866512417793274\n",
      "Epoch 436, Loss: 1.3776764571666718, Final Batch Loss: 0.3756260871887207\n",
      "Epoch 437, Loss: 1.4653427302837372, Final Batch Loss: 0.43764686584472656\n",
      "Epoch 438, Loss: 1.4104209542274475, Final Batch Loss: 0.41544073820114136\n",
      "Epoch 439, Loss: 1.4167541563510895, Final Batch Loss: 0.4289582669734955\n",
      "Epoch 440, Loss: 1.5639083981513977, Final Batch Loss: 0.5379747152328491\n",
      "Epoch 441, Loss: 1.501708298921585, Final Batch Loss: 0.4721014201641083\n",
      "Epoch 442, Loss: 1.5728660821914673, Final Batch Loss: 0.5077807903289795\n",
      "Epoch 443, Loss: 1.4235287308692932, Final Batch Loss: 0.4535976052284241\n",
      "Epoch 444, Loss: 1.6729753017425537, Final Batch Loss: 0.530505359172821\n",
      "Epoch 445, Loss: 1.4529159665107727, Final Batch Loss: 0.45585253834724426\n",
      "Epoch 446, Loss: 1.5283024609088898, Final Batch Loss: 0.5610496997833252\n",
      "Epoch 447, Loss: 1.498197466135025, Final Batch Loss: 0.47375598549842834\n",
      "Epoch 448, Loss: 1.5929288268089294, Final Batch Loss: 0.5777729153633118\n",
      "Epoch 449, Loss: 1.5285158455371857, Final Batch Loss: 0.5361968278884888\n",
      "Epoch 450, Loss: 1.4445709586143494, Final Batch Loss: 0.4609382748603821\n",
      "Epoch 451, Loss: 1.499748408794403, Final Batch Loss: 0.5007026791572571\n",
      "Epoch 452, Loss: 1.4032777547836304, Final Batch Loss: 0.3933688998222351\n",
      "Epoch 453, Loss: 1.488406002521515, Final Batch Loss: 0.5656825304031372\n",
      "Epoch 454, Loss: 1.562349408864975, Final Batch Loss: 0.5618742108345032\n",
      "Epoch 455, Loss: 1.5061203837394714, Final Batch Loss: 0.4684494137763977\n",
      "Epoch 456, Loss: 1.5295033156871796, Final Batch Loss: 0.5115983486175537\n",
      "Epoch 457, Loss: 1.4172497689723969, Final Batch Loss: 0.40750351548194885\n",
      "Epoch 458, Loss: 1.401876449584961, Final Batch Loss: 0.41297152638435364\n",
      "Epoch 459, Loss: 1.3673837184906006, Final Batch Loss: 0.35563281178474426\n",
      "Epoch 460, Loss: 1.5261390209197998, Final Batch Loss: 0.5820673108100891\n",
      "Epoch 461, Loss: 1.4198143780231476, Final Batch Loss: 0.4108988046646118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462, Loss: 1.5068801641464233, Final Batch Loss: 0.5292260050773621\n",
      "Epoch 463, Loss: 1.3406701385974884, Final Batch Loss: 0.4069027900695801\n",
      "Epoch 464, Loss: 1.396308720111847, Final Batch Loss: 0.43953752517700195\n",
      "Epoch 465, Loss: 1.4139597117900848, Final Batch Loss: 0.5039380192756653\n",
      "Epoch 466, Loss: 1.5503033995628357, Final Batch Loss: 0.534761369228363\n",
      "Epoch 467, Loss: 1.4420036971569061, Final Batch Loss: 0.3927944600582123\n",
      "Epoch 468, Loss: 1.4506418704986572, Final Batch Loss: 0.4679446220397949\n",
      "Epoch 469, Loss: 1.4801006019115448, Final Batch Loss: 0.5001624226570129\n",
      "Epoch 470, Loss: 1.4079491198062897, Final Batch Loss: 0.40410134196281433\n",
      "Epoch 471, Loss: 1.4893344044685364, Final Batch Loss: 0.5207774639129639\n",
      "Epoch 472, Loss: 1.5837233364582062, Final Batch Loss: 0.603547215461731\n",
      "Epoch 473, Loss: 1.427085280418396, Final Batch Loss: 0.4784703850746155\n",
      "Epoch 474, Loss: 1.3491604328155518, Final Batch Loss: 0.33912497758865356\n",
      "Epoch 475, Loss: 1.3990404307842255, Final Batch Loss: 0.44274434447288513\n",
      "Epoch 476, Loss: 1.3197678625583649, Final Batch Loss: 0.3777712881565094\n",
      "Epoch 477, Loss: 1.3708249628543854, Final Batch Loss: 0.3391108214855194\n",
      "Epoch 478, Loss: 1.6333835422992706, Final Batch Loss: 0.5971693992614746\n",
      "Epoch 479, Loss: 1.3048787117004395, Final Batch Loss: 0.3373410701751709\n",
      "Epoch 480, Loss: 1.4626171588897705, Final Batch Loss: 0.46250075101852417\n",
      "Epoch 481, Loss: 1.5173028111457825, Final Batch Loss: 0.5569127202033997\n",
      "Epoch 482, Loss: 1.453081488609314, Final Batch Loss: 0.4873518645763397\n",
      "Epoch 483, Loss: 1.53967747092247, Final Batch Loss: 0.5646681785583496\n",
      "Epoch 484, Loss: 1.3385798633098602, Final Batch Loss: 0.3422633111476898\n",
      "Epoch 485, Loss: 1.4304715394973755, Final Batch Loss: 0.46335604786872864\n",
      "Epoch 486, Loss: 1.3019117712974548, Final Batch Loss: 0.3310273289680481\n",
      "Epoch 487, Loss: 1.4065174162387848, Final Batch Loss: 0.4385411739349365\n",
      "Epoch 488, Loss: 1.4178493320941925, Final Batch Loss: 0.4785190522670746\n",
      "Epoch 489, Loss: 1.4865064918994904, Final Batch Loss: 0.5789886116981506\n",
      "Epoch 490, Loss: 1.5365984439849854, Final Batch Loss: 0.5333620309829712\n",
      "Epoch 491, Loss: 1.433399260044098, Final Batch Loss: 0.5419979691505432\n",
      "Epoch 492, Loss: 1.4212620854377747, Final Batch Loss: 0.5343517065048218\n",
      "Epoch 493, Loss: 1.3524613976478577, Final Batch Loss: 0.42055171728134155\n",
      "Epoch 494, Loss: 1.492640733718872, Final Batch Loss: 0.5016032457351685\n",
      "Epoch 495, Loss: 1.3784831166267395, Final Batch Loss: 0.4496467709541321\n",
      "Epoch 496, Loss: 1.3851253390312195, Final Batch Loss: 0.39820992946624756\n",
      "Epoch 497, Loss: 1.3467358350753784, Final Batch Loss: 0.4214668571949005\n",
      "Epoch 498, Loss: 1.4591552317142487, Final Batch Loss: 0.4657217562198639\n",
      "Epoch 499, Loss: 1.3650620579719543, Final Batch Loss: 0.36199241876602173\n",
      "Epoch 500, Loss: 1.3418551981449127, Final Batch Loss: 0.40482085943222046\n",
      "Epoch 501, Loss: 1.4467675983905792, Final Batch Loss: 0.4325482249259949\n",
      "Epoch 502, Loss: 1.300821214914322, Final Batch Loss: 0.36048486828804016\n",
      "Epoch 503, Loss: 1.3572217524051666, Final Batch Loss: 0.406206339597702\n",
      "Epoch 504, Loss: 1.3778600692749023, Final Batch Loss: 0.41426604986190796\n",
      "Epoch 505, Loss: 1.557835727930069, Final Batch Loss: 0.5599516034126282\n",
      "Epoch 506, Loss: 1.3248715102672577, Final Batch Loss: 0.35321947932243347\n",
      "Epoch 507, Loss: 1.3492441773414612, Final Batch Loss: 0.3669593930244446\n",
      "Epoch 508, Loss: 1.3038610517978668, Final Batch Loss: 0.3996928036212921\n",
      "Epoch 509, Loss: 1.378409594297409, Final Batch Loss: 0.448657751083374\n",
      "Epoch 510, Loss: 1.5258344411849976, Final Batch Loss: 0.5143291354179382\n",
      "Epoch 511, Loss: 1.6418309211730957, Final Batch Loss: 0.6703931093215942\n",
      "Epoch 512, Loss: 1.3704310357570648, Final Batch Loss: 0.43385380506515503\n",
      "Epoch 513, Loss: 1.518088698387146, Final Batch Loss: 0.6176660060882568\n",
      "Epoch 514, Loss: 1.4181362092494965, Final Batch Loss: 0.4853043854236603\n",
      "Epoch 515, Loss: 1.409883290529251, Final Batch Loss: 0.4992876648902893\n",
      "Epoch 516, Loss: 1.4640074670314789, Final Batch Loss: 0.4477638304233551\n",
      "Epoch 517, Loss: 1.4070504903793335, Final Batch Loss: 0.44444540143013\n",
      "Epoch 518, Loss: 1.4987436830997467, Final Batch Loss: 0.5718403458595276\n",
      "Epoch 519, Loss: 1.4197305738925934, Final Batch Loss: 0.45782479643821716\n",
      "Epoch 520, Loss: 1.4604302644729614, Final Batch Loss: 0.5890289545059204\n",
      "Epoch 521, Loss: 1.4549922943115234, Final Batch Loss: 0.5380698442459106\n",
      "Epoch 522, Loss: 1.4681640267372131, Final Batch Loss: 0.5214586853981018\n",
      "Epoch 523, Loss: 1.334938794374466, Final Batch Loss: 0.4417388439178467\n",
      "Epoch 524, Loss: 1.2666598856449127, Final Batch Loss: 0.29500287771224976\n",
      "Epoch 525, Loss: 1.3277825713157654, Final Batch Loss: 0.4033995270729065\n",
      "Epoch 526, Loss: 1.2695374488830566, Final Batch Loss: 0.3321284055709839\n",
      "Epoch 527, Loss: 1.413993924856186, Final Batch Loss: 0.4892638027667999\n",
      "Epoch 528, Loss: 1.426087349653244, Final Batch Loss: 0.49005410075187683\n",
      "Epoch 529, Loss: 1.23698291182518, Final Batch Loss: 0.2694617807865143\n",
      "Epoch 530, Loss: 1.2501382529735565, Final Batch Loss: 0.3025890290737152\n",
      "Epoch 531, Loss: 1.382392019033432, Final Batch Loss: 0.5039874315261841\n",
      "Epoch 532, Loss: 1.281911551952362, Final Batch Loss: 0.3475777506828308\n",
      "Epoch 533, Loss: 1.4165646731853485, Final Batch Loss: 0.4786374866962433\n",
      "Epoch 534, Loss: 1.3797405660152435, Final Batch Loss: 0.4371097981929779\n",
      "Epoch 535, Loss: 1.4674567878246307, Final Batch Loss: 0.48893752694129944\n",
      "Epoch 536, Loss: 1.3840258121490479, Final Batch Loss: 0.5050827264785767\n",
      "Epoch 537, Loss: 1.559162199497223, Final Batch Loss: 0.5990998148918152\n",
      "Epoch 538, Loss: 1.2613964676856995, Final Batch Loss: 0.33365195989608765\n",
      "Epoch 539, Loss: 1.3057531714439392, Final Batch Loss: 0.4230860471725464\n",
      "Epoch 540, Loss: 1.3583906292915344, Final Batch Loss: 0.4919900596141815\n",
      "Epoch 541, Loss: 1.4349890053272247, Final Batch Loss: 0.5130862593650818\n",
      "Epoch 542, Loss: 1.4047830402851105, Final Batch Loss: 0.47982850670814514\n",
      "Epoch 543, Loss: 1.3363178670406342, Final Batch Loss: 0.3752795159816742\n",
      "Epoch 544, Loss: 1.2886368334293365, Final Batch Loss: 0.3896773159503937\n",
      "Epoch 545, Loss: 1.3231504559516907, Final Batch Loss: 0.4204745888710022\n",
      "Epoch 546, Loss: 1.4471218287944794, Final Batch Loss: 0.4806075990200043\n",
      "Epoch 547, Loss: 1.307013213634491, Final Batch Loss: 0.37566184997558594\n",
      "Epoch 548, Loss: 1.373631328344345, Final Batch Loss: 0.45293477177619934\n",
      "Epoch 549, Loss: 1.279584527015686, Final Batch Loss: 0.3537088930606842\n",
      "Epoch 550, Loss: 1.4590456783771515, Final Batch Loss: 0.5299835801124573\n",
      "Epoch 551, Loss: 1.2582399547100067, Final Batch Loss: 0.28055572509765625\n",
      "Epoch 552, Loss: 1.4697218537330627, Final Batch Loss: 0.5655131340026855\n",
      "Epoch 553, Loss: 1.373939871788025, Final Batch Loss: 0.4495096504688263\n",
      "Epoch 554, Loss: 1.3295496702194214, Final Batch Loss: 0.39677950739860535\n",
      "Epoch 555, Loss: 1.3329681754112244, Final Batch Loss: 0.504183292388916\n",
      "Epoch 556, Loss: 1.4025778472423553, Final Batch Loss: 0.5053777694702148\n",
      "Epoch 557, Loss: 1.3083437383174896, Final Batch Loss: 0.36385202407836914\n",
      "Epoch 558, Loss: 1.3904025852680206, Final Batch Loss: 0.4902517795562744\n",
      "Epoch 559, Loss: 1.3830670714378357, Final Batch Loss: 0.46848422288894653\n",
      "Epoch 560, Loss: 1.4432026147842407, Final Batch Loss: 0.5118785500526428\n",
      "Epoch 561, Loss: 1.3532594740390778, Final Batch Loss: 0.4515896141529083\n",
      "Epoch 562, Loss: 1.532317876815796, Final Batch Loss: 0.5855706930160522\n",
      "Epoch 563, Loss: 1.4243424534797668, Final Batch Loss: 0.48062729835510254\n",
      "Epoch 564, Loss: 1.3128246366977692, Final Batch Loss: 0.38548555970191956\n",
      "Epoch 565, Loss: 1.289111316204071, Final Batch Loss: 0.3654348850250244\n",
      "Epoch 566, Loss: 1.369305431842804, Final Batch Loss: 0.43610090017318726\n",
      "Epoch 567, Loss: 1.3747871816158295, Final Batch Loss: 0.4828745722770691\n",
      "Epoch 568, Loss: 1.2217560410499573, Final Batch Loss: 0.3264710307121277\n",
      "Epoch 569, Loss: 1.263456642627716, Final Batch Loss: 0.3499523103237152\n",
      "Epoch 570, Loss: 1.303755670785904, Final Batch Loss: 0.43953028321266174\n",
      "Epoch 571, Loss: 1.3248432576656342, Final Batch Loss: 0.4566485285758972\n",
      "Epoch 572, Loss: 1.3771544098854065, Final Batch Loss: 0.4524554908275604\n",
      "Epoch 573, Loss: 1.3328569829463959, Final Batch Loss: 0.431789368391037\n",
      "Epoch 574, Loss: 1.3316620886325836, Final Batch Loss: 0.4145300090312958\n",
      "Epoch 575, Loss: 1.2949865758419037, Final Batch Loss: 0.4073472321033478\n",
      "Epoch 576, Loss: 1.4237828850746155, Final Batch Loss: 0.5248492956161499\n",
      "Epoch 577, Loss: 1.417683333158493, Final Batch Loss: 0.5236355662345886\n",
      "Epoch 578, Loss: 1.3288872838020325, Final Batch Loss: 0.4081387519836426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 579, Loss: 1.336304485797882, Final Batch Loss: 0.42045748233795166\n",
      "Epoch 580, Loss: 1.5649436712265015, Final Batch Loss: 0.6586533188819885\n",
      "Epoch 581, Loss: 1.3602386116981506, Final Batch Loss: 0.4740404784679413\n",
      "Epoch 582, Loss: 1.4162884056568146, Final Batch Loss: 0.4732420742511749\n",
      "Epoch 583, Loss: 1.3383682668209076, Final Batch Loss: 0.43645352125167847\n",
      "Epoch 584, Loss: 1.1808565855026245, Final Batch Loss: 0.29185751080513\n",
      "Epoch 585, Loss: 1.4925489723682404, Final Batch Loss: 0.511898398399353\n",
      "Epoch 586, Loss: 1.376700609922409, Final Batch Loss: 0.4488873779773712\n",
      "Epoch 587, Loss: 1.2898103594779968, Final Batch Loss: 0.3447932004928589\n",
      "Epoch 588, Loss: 1.3017912805080414, Final Batch Loss: 0.4103189706802368\n",
      "Epoch 589, Loss: 1.5190983712673187, Final Batch Loss: 0.6357185244560242\n",
      "Epoch 590, Loss: 1.2415502071380615, Final Batch Loss: 0.3920905888080597\n",
      "Epoch 591, Loss: 1.3508054614067078, Final Batch Loss: 0.49650418758392334\n",
      "Epoch 592, Loss: 1.4045891165733337, Final Batch Loss: 0.4951877295970917\n",
      "Epoch 593, Loss: 1.2883776426315308, Final Batch Loss: 0.4392160177230835\n",
      "Epoch 594, Loss: 1.3485902547836304, Final Batch Loss: 0.4223676919937134\n",
      "Epoch 595, Loss: 1.205718219280243, Final Batch Loss: 0.33061209321022034\n",
      "Epoch 596, Loss: 1.4220871329307556, Final Batch Loss: 0.5135385394096375\n",
      "Epoch 597, Loss: 1.2973455786705017, Final Batch Loss: 0.3942360281944275\n",
      "Epoch 598, Loss: 1.3924800157546997, Final Batch Loss: 0.4782446622848511\n",
      "Epoch 599, Loss: 1.2879633009433746, Final Batch Loss: 0.38195785880088806\n",
      "Epoch 600, Loss: 1.2896096408367157, Final Batch Loss: 0.3962218165397644\n",
      "Epoch 601, Loss: 1.2558696269989014, Final Batch Loss: 0.37957799434661865\n",
      "Epoch 602, Loss: 1.4212707877159119, Final Batch Loss: 0.5142471790313721\n",
      "Epoch 603, Loss: 1.1998092234134674, Final Batch Loss: 0.33688899874687195\n",
      "Epoch 604, Loss: 1.416755497455597, Final Batch Loss: 0.5104743838310242\n",
      "Epoch 605, Loss: 1.284643977880478, Final Batch Loss: 0.4306703209877014\n",
      "Epoch 606, Loss: 1.3445039689540863, Final Batch Loss: 0.41204988956451416\n",
      "Epoch 607, Loss: 1.1602506339550018, Final Batch Loss: 0.27656090259552\n",
      "Epoch 608, Loss: 1.2748499512672424, Final Batch Loss: 0.4293913245201111\n",
      "Epoch 609, Loss: 1.2512775361537933, Final Batch Loss: 0.3293747305870056\n",
      "Epoch 610, Loss: 1.3909180462360382, Final Batch Loss: 0.4856570065021515\n",
      "Epoch 611, Loss: 1.3802697360515594, Final Batch Loss: 0.5125490427017212\n",
      "Epoch 612, Loss: 1.391693651676178, Final Batch Loss: 0.45396867394447327\n",
      "Epoch 613, Loss: 1.333814412355423, Final Batch Loss: 0.4707558751106262\n",
      "Epoch 614, Loss: 1.2723459601402283, Final Batch Loss: 0.4229094386100769\n",
      "Epoch 615, Loss: 1.4115962386131287, Final Batch Loss: 0.4906156361103058\n",
      "Epoch 616, Loss: 1.3177503049373627, Final Batch Loss: 0.38225075602531433\n",
      "Epoch 617, Loss: 1.4437038004398346, Final Batch Loss: 0.5307602882385254\n",
      "Epoch 618, Loss: 1.2344128787517548, Final Batch Loss: 0.34296396374702454\n",
      "Epoch 619, Loss: 1.4108621180057526, Final Batch Loss: 0.45024222135543823\n",
      "Epoch 620, Loss: 1.271919071674347, Final Batch Loss: 0.33263424038887024\n",
      "Epoch 621, Loss: 1.394468992948532, Final Batch Loss: 0.5330429673194885\n",
      "Epoch 622, Loss: 1.2328326404094696, Final Batch Loss: 0.3491343557834625\n",
      "Epoch 623, Loss: 1.3220781087875366, Final Batch Loss: 0.4313148856163025\n",
      "Epoch 624, Loss: 1.4278497695922852, Final Batch Loss: 0.5265858173370361\n",
      "Epoch 625, Loss: 1.4231968522071838, Final Batch Loss: 0.5281476974487305\n",
      "Epoch 626, Loss: 1.320708692073822, Final Batch Loss: 0.43785223364830017\n",
      "Epoch 627, Loss: 1.4936295747756958, Final Batch Loss: 0.5991531610488892\n",
      "Epoch 628, Loss: 1.219231128692627, Final Batch Loss: 0.3292198181152344\n",
      "Epoch 629, Loss: 1.3939732313156128, Final Batch Loss: 0.4931340515613556\n",
      "Epoch 630, Loss: 1.2433865666389465, Final Batch Loss: 0.3584912419319153\n",
      "Epoch 631, Loss: 1.197011113166809, Final Batch Loss: 0.3426756262779236\n",
      "Epoch 632, Loss: 1.3887868523597717, Final Batch Loss: 0.5080261826515198\n",
      "Epoch 633, Loss: 1.2850801348686218, Final Batch Loss: 0.3847813904285431\n",
      "Epoch 634, Loss: 1.3952006101608276, Final Batch Loss: 0.4874773323535919\n",
      "Epoch 635, Loss: 1.2589033544063568, Final Batch Loss: 0.4389391839504242\n",
      "Epoch 636, Loss: 1.2395759224891663, Final Batch Loss: 0.3960285484790802\n",
      "Epoch 637, Loss: 1.3212518990039825, Final Batch Loss: 0.4822463095188141\n",
      "Epoch 638, Loss: 1.2257626950740814, Final Batch Loss: 0.3771984279155731\n",
      "Epoch 639, Loss: 1.2983145415782928, Final Batch Loss: 0.3497909605503082\n",
      "Epoch 640, Loss: 1.3327939808368683, Final Batch Loss: 0.4895819425582886\n",
      "Epoch 641, Loss: 1.3209596574306488, Final Batch Loss: 0.45983508229255676\n",
      "Epoch 642, Loss: 1.1783519983291626, Final Batch Loss: 0.3541390597820282\n",
      "Epoch 643, Loss: 1.243070125579834, Final Batch Loss: 0.4042068123817444\n",
      "Epoch 644, Loss: 1.1367213428020477, Final Batch Loss: 0.2770303785800934\n",
      "Epoch 645, Loss: 1.2080077230930328, Final Batch Loss: 0.40239763259887695\n",
      "Epoch 646, Loss: 1.312466949224472, Final Batch Loss: 0.4555271565914154\n",
      "Epoch 647, Loss: 1.386581540107727, Final Batch Loss: 0.5273438096046448\n",
      "Epoch 648, Loss: 1.3673284649848938, Final Batch Loss: 0.5350243449211121\n",
      "Epoch 649, Loss: 1.3459024131298065, Final Batch Loss: 0.449178546667099\n",
      "Epoch 650, Loss: 1.316664695739746, Final Batch Loss: 0.45594164729118347\n",
      "Epoch 651, Loss: 1.134415626525879, Final Batch Loss: 0.27094754576683044\n",
      "Epoch 652, Loss: 1.5126334428787231, Final Batch Loss: 0.6113685965538025\n",
      "Epoch 653, Loss: 1.1569439768791199, Final Batch Loss: 0.3006083071231842\n",
      "Epoch 654, Loss: 1.1782108545303345, Final Batch Loss: 0.35696229338645935\n",
      "Epoch 655, Loss: 1.2384087443351746, Final Batch Loss: 0.3614407479763031\n",
      "Epoch 656, Loss: 1.3021254539489746, Final Batch Loss: 0.5050238966941833\n",
      "Epoch 657, Loss: 1.3099204003810883, Final Batch Loss: 0.44090014696121216\n",
      "Epoch 658, Loss: 1.1804856061935425, Final Batch Loss: 0.2659324109554291\n",
      "Epoch 659, Loss: 1.350746750831604, Final Batch Loss: 0.5497486591339111\n",
      "Epoch 660, Loss: 1.355950653553009, Final Batch Loss: 0.4792492091655731\n",
      "Epoch 661, Loss: 1.2973800003528595, Final Batch Loss: 0.4779536724090576\n",
      "Epoch 662, Loss: 1.2869046330451965, Final Batch Loss: 0.47539809346199036\n",
      "Epoch 663, Loss: 1.359027087688446, Final Batch Loss: 0.4916568994522095\n",
      "Epoch 664, Loss: 1.2359208464622498, Final Batch Loss: 0.401834100484848\n",
      "Epoch 665, Loss: 1.238584041595459, Final Batch Loss: 0.3448961079120636\n",
      "Epoch 666, Loss: 1.2584812641143799, Final Batch Loss: 0.3766990602016449\n",
      "Epoch 667, Loss: 1.3641797602176666, Final Batch Loss: 0.531274139881134\n",
      "Epoch 668, Loss: 1.2685361802577972, Final Batch Loss: 0.3858608603477478\n",
      "Epoch 669, Loss: 1.1874771118164062, Final Batch Loss: 0.32197967171669006\n",
      "Epoch 670, Loss: 1.3499782979488373, Final Batch Loss: 0.4519850015640259\n",
      "Epoch 671, Loss: 1.219744086265564, Final Batch Loss: 0.31586119532585144\n",
      "Epoch 672, Loss: 1.3018155694007874, Final Batch Loss: 0.4433489441871643\n",
      "Epoch 673, Loss: 1.4167725443840027, Final Batch Loss: 0.5960268974304199\n",
      "Epoch 674, Loss: 1.2472609281539917, Final Batch Loss: 0.39181071519851685\n",
      "Epoch 675, Loss: 1.2479847967624664, Final Batch Loss: 0.40777915716171265\n",
      "Epoch 676, Loss: 1.3888929784297943, Final Batch Loss: 0.5786810517311096\n",
      "Epoch 677, Loss: 1.3553915321826935, Final Batch Loss: 0.4686689078807831\n",
      "Epoch 678, Loss: 1.287770539522171, Final Batch Loss: 0.448977530002594\n",
      "Epoch 679, Loss: 1.3053174316883087, Final Batch Loss: 0.4449024200439453\n",
      "Epoch 680, Loss: 1.1349176168441772, Final Batch Loss: 0.3243081569671631\n",
      "Epoch 681, Loss: 1.2428690791130066, Final Batch Loss: 0.4238802194595337\n",
      "Epoch 682, Loss: 1.2681319117546082, Final Batch Loss: 0.38332268595695496\n",
      "Epoch 683, Loss: 1.2346518635749817, Final Batch Loss: 0.3919178247451782\n",
      "Epoch 684, Loss: 1.180290937423706, Final Batch Loss: 0.3398236632347107\n",
      "Epoch 685, Loss: 1.2498576045036316, Final Batch Loss: 0.3846631646156311\n",
      "Epoch 686, Loss: 1.1974587440490723, Final Batch Loss: 0.3097957670688629\n",
      "Epoch 687, Loss: 1.3621209859848022, Final Batch Loss: 0.4998064637184143\n",
      "Epoch 688, Loss: 1.3551103174686432, Final Batch Loss: 0.4928743541240692\n",
      "Epoch 689, Loss: 1.3396934270858765, Final Batch Loss: 0.5275548696517944\n",
      "Epoch 690, Loss: 1.2712068855762482, Final Batch Loss: 0.4468003511428833\n",
      "Epoch 691, Loss: 1.168590098619461, Final Batch Loss: 0.30758652091026306\n",
      "Epoch 692, Loss: 1.2911165654659271, Final Batch Loss: 0.4405684173107147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 693, Loss: 1.32013601064682, Final Batch Loss: 0.3834744989871979\n",
      "Epoch 694, Loss: 1.4373822510242462, Final Batch Loss: 0.5826935768127441\n",
      "Epoch 695, Loss: 1.2387716174125671, Final Batch Loss: 0.355552077293396\n",
      "Epoch 696, Loss: 1.3786358535289764, Final Batch Loss: 0.5597654581069946\n",
      "Epoch 697, Loss: 1.2468948066234589, Final Batch Loss: 0.37018609046936035\n",
      "Epoch 698, Loss: 1.2226195335388184, Final Batch Loss: 0.38003894686698914\n",
      "Epoch 699, Loss: 1.2170525193214417, Final Batch Loss: 0.42539143562316895\n",
      "Epoch 700, Loss: 1.198544293642044, Final Batch Loss: 0.32146069407463074\n",
      "Epoch 701, Loss: 1.23260697722435, Final Batch Loss: 0.39127829670906067\n",
      "Epoch 702, Loss: 1.2745936214923859, Final Batch Loss: 0.41591835021972656\n",
      "Epoch 703, Loss: 1.179671436548233, Final Batch Loss: 0.38238295912742615\n",
      "Epoch 704, Loss: 1.2927460074424744, Final Batch Loss: 0.471690833568573\n",
      "Epoch 705, Loss: 1.2483604848384857, Final Batch Loss: 0.3645186126232147\n",
      "Epoch 706, Loss: 1.3298391103744507, Final Batch Loss: 0.5344732403755188\n",
      "Epoch 707, Loss: 1.2612897157669067, Final Batch Loss: 0.40555110573768616\n",
      "Epoch 708, Loss: 1.2109960913658142, Final Batch Loss: 0.41838470101356506\n",
      "Epoch 709, Loss: 1.269802302122116, Final Batch Loss: 0.4357878565788269\n",
      "Epoch 710, Loss: 1.2450422942638397, Final Batch Loss: 0.39316466450691223\n",
      "Epoch 711, Loss: 1.3125909268856049, Final Batch Loss: 0.5198105573654175\n",
      "Epoch 712, Loss: 1.3159377574920654, Final Batch Loss: 0.5187342762947083\n",
      "Epoch 713, Loss: 1.1881343722343445, Final Batch Loss: 0.3292487859725952\n",
      "Epoch 714, Loss: 1.292292058467865, Final Batch Loss: 0.471918523311615\n",
      "Epoch 715, Loss: 1.2577088177204132, Final Batch Loss: 0.42060399055480957\n",
      "Epoch 716, Loss: 1.2877655029296875, Final Batch Loss: 0.46210405230522156\n",
      "Epoch 717, Loss: 1.2083124816417694, Final Batch Loss: 0.33033671975135803\n",
      "Epoch 718, Loss: 1.378728449344635, Final Batch Loss: 0.4398716986179352\n",
      "Epoch 719, Loss: 1.4332912266254425, Final Batch Loss: 0.6176299452781677\n",
      "Epoch 720, Loss: 1.1380201578140259, Final Batch Loss: 0.2860485315322876\n",
      "Epoch 721, Loss: 1.2128344178199768, Final Batch Loss: 0.40851277112960815\n",
      "Epoch 722, Loss: 1.2775581777095795, Final Batch Loss: 0.3607850670814514\n",
      "Epoch 723, Loss: 1.218826413154602, Final Batch Loss: 0.41686251759529114\n",
      "Epoch 724, Loss: 1.2374812066555023, Final Batch Loss: 0.37236925959587097\n",
      "Epoch 725, Loss: 1.1996406018733978, Final Batch Loss: 0.3428686559200287\n",
      "Epoch 726, Loss: 1.2144576013088226, Final Batch Loss: 0.41384759545326233\n",
      "Epoch 727, Loss: 1.1805256009101868, Final Batch Loss: 0.40244901180267334\n",
      "Epoch 728, Loss: 1.1727964282035828, Final Batch Loss: 0.41153836250305176\n",
      "Epoch 729, Loss: 1.301392287015915, Final Batch Loss: 0.4602566659450531\n",
      "Epoch 730, Loss: 1.1719470918178558, Final Batch Loss: 0.34035876393318176\n",
      "Epoch 731, Loss: 1.2689557075500488, Final Batch Loss: 0.44344431161880493\n",
      "Epoch 732, Loss: 1.2234687507152557, Final Batch Loss: 0.36996152997016907\n",
      "Epoch 733, Loss: 1.2913471460342407, Final Batch Loss: 0.4665478765964508\n",
      "Epoch 734, Loss: 1.1517680287361145, Final Batch Loss: 0.3574560582637787\n",
      "Epoch 735, Loss: 1.2770476639270782, Final Batch Loss: 0.3893524408340454\n",
      "Epoch 736, Loss: 1.307802826166153, Final Batch Loss: 0.4713470935821533\n",
      "Epoch 737, Loss: 1.1981771290302277, Final Batch Loss: 0.28003254532814026\n",
      "Epoch 738, Loss: 1.2668078243732452, Final Batch Loss: 0.3991239070892334\n",
      "Epoch 739, Loss: 1.3177279829978943, Final Batch Loss: 0.5017459988594055\n",
      "Epoch 740, Loss: 1.3224993646144867, Final Batch Loss: 0.5182119607925415\n",
      "Epoch 741, Loss: 1.4242606461048126, Final Batch Loss: 0.5632116198539734\n",
      "Epoch 742, Loss: 1.2206309139728546, Final Batch Loss: 0.444308340549469\n",
      "Epoch 743, Loss: 1.2248011827468872, Final Batch Loss: 0.4233572781085968\n",
      "Epoch 744, Loss: 1.2170177698135376, Final Batch Loss: 0.3560568392276764\n",
      "Epoch 745, Loss: 1.1414703130722046, Final Batch Loss: 0.33042725920677185\n",
      "Epoch 746, Loss: 1.106884241104126, Final Batch Loss: 0.2937634587287903\n",
      "Epoch 747, Loss: 1.2553549706935883, Final Batch Loss: 0.4274199306964874\n",
      "Epoch 748, Loss: 1.1324642598628998, Final Batch Loss: 0.32636958360671997\n",
      "Epoch 749, Loss: 1.1636066138744354, Final Batch Loss: 0.3806474506855011\n",
      "Epoch 750, Loss: 1.3225921094417572, Final Batch Loss: 0.5256449580192566\n",
      "Epoch 751, Loss: 1.164042353630066, Final Batch Loss: 0.3620089292526245\n",
      "Epoch 752, Loss: 1.3880144953727722, Final Batch Loss: 0.5361213088035583\n",
      "Epoch 753, Loss: 1.1342778503894806, Final Batch Loss: 0.34598708152770996\n",
      "Epoch 754, Loss: 1.1819294691085815, Final Batch Loss: 0.35352686047554016\n",
      "Epoch 755, Loss: 1.3537731766700745, Final Batch Loss: 0.5278646945953369\n",
      "Epoch 756, Loss: 1.2026521563529968, Final Batch Loss: 0.3987104892730713\n",
      "Epoch 757, Loss: 1.0822442173957825, Final Batch Loss: 0.28463214635849\n",
      "Epoch 758, Loss: 1.1974802017211914, Final Batch Loss: 0.37789151072502136\n",
      "Epoch 759, Loss: 1.1889860928058624, Final Batch Loss: 0.35550063848495483\n",
      "Epoch 760, Loss: 1.3181369304656982, Final Batch Loss: 0.5181320309638977\n",
      "Epoch 761, Loss: 1.1987998485565186, Final Batch Loss: 0.4014340341091156\n",
      "Epoch 762, Loss: 1.209364801645279, Final Batch Loss: 0.4123110771179199\n",
      "Epoch 763, Loss: 1.1836386322975159, Final Batch Loss: 0.34479063749313354\n",
      "Epoch 764, Loss: 1.1822720766067505, Final Batch Loss: 0.37994325160980225\n",
      "Epoch 765, Loss: 1.1699190437793732, Final Batch Loss: 0.37436673045158386\n",
      "Epoch 766, Loss: 1.313749372959137, Final Batch Loss: 0.40119412541389465\n",
      "Epoch 767, Loss: 1.218449741601944, Final Batch Loss: 0.4384914040565491\n",
      "Epoch 768, Loss: 1.150237113237381, Final Batch Loss: 0.3800122141838074\n",
      "Epoch 769, Loss: 1.072567194700241, Final Batch Loss: 0.3092157542705536\n",
      "Epoch 770, Loss: 1.2405398786067963, Final Batch Loss: 0.4603598713874817\n",
      "Epoch 771, Loss: 1.2524976134300232, Final Batch Loss: 0.4210781157016754\n",
      "Epoch 772, Loss: 1.204863280057907, Final Batch Loss: 0.4427645206451416\n",
      "Epoch 773, Loss: 1.1404150128364563, Final Batch Loss: 0.33444660902023315\n",
      "Epoch 774, Loss: 1.1757999658584595, Final Batch Loss: 0.38663193583488464\n",
      "Epoch 775, Loss: 1.2072827816009521, Final Batch Loss: 0.46342039108276367\n",
      "Epoch 776, Loss: 1.1654065549373627, Final Batch Loss: 0.34289395809173584\n",
      "Epoch 777, Loss: 1.377650797367096, Final Batch Loss: 0.6103445887565613\n",
      "Epoch 778, Loss: 1.15225088596344, Final Batch Loss: 0.33002611994743347\n",
      "Epoch 779, Loss: 1.2181525230407715, Final Batch Loss: 0.38221463561058044\n",
      "Epoch 780, Loss: 1.2710422277450562, Final Batch Loss: 0.5225695967674255\n",
      "Epoch 781, Loss: 1.2844130098819733, Final Batch Loss: 0.49424365162849426\n",
      "Epoch 782, Loss: 1.2414076626300812, Final Batch Loss: 0.42229536175727844\n",
      "Epoch 783, Loss: 1.2601850628852844, Final Batch Loss: 0.45181742310523987\n",
      "Epoch 784, Loss: 1.1766148209571838, Final Batch Loss: 0.3628975749015808\n",
      "Epoch 785, Loss: 1.1599282920360565, Final Batch Loss: 0.36715632677078247\n",
      "Epoch 786, Loss: 1.2087047696113586, Final Batch Loss: 0.42401766777038574\n",
      "Epoch 787, Loss: 1.283583253622055, Final Batch Loss: 0.3900957405567169\n",
      "Epoch 788, Loss: 1.3056394159793854, Final Batch Loss: 0.49815496802330017\n",
      "Epoch 789, Loss: 1.2524559497833252, Final Batch Loss: 0.4338063895702362\n",
      "Epoch 790, Loss: 1.156788855791092, Final Batch Loss: 0.33862224221229553\n",
      "Epoch 791, Loss: 1.231645405292511, Final Batch Loss: 0.4263467490673065\n",
      "Epoch 792, Loss: 1.2500405013561249, Final Batch Loss: 0.4534398317337036\n",
      "Epoch 793, Loss: 1.1835803985595703, Final Batch Loss: 0.3820737600326538\n",
      "Epoch 794, Loss: 1.1589223146438599, Final Batch Loss: 0.39027419686317444\n",
      "Epoch 795, Loss: 1.1650457680225372, Final Batch Loss: 0.36798402667045593\n",
      "Epoch 796, Loss: 1.223285734653473, Final Batch Loss: 0.4325941801071167\n",
      "Epoch 797, Loss: 1.184112936258316, Final Batch Loss: 0.35647115111351013\n",
      "Epoch 798, Loss: 1.2633735537528992, Final Batch Loss: 0.4461340308189392\n",
      "Epoch 799, Loss: 1.2671118378639221, Final Batch Loss: 0.47032874822616577\n",
      "Epoch 800, Loss: 1.1201841533184052, Final Batch Loss: 0.31187549233436584\n",
      "Epoch 801, Loss: 1.2703210711479187, Final Batch Loss: 0.4627140462398529\n",
      "Epoch 802, Loss: 1.1639336049556732, Final Batch Loss: 0.3441060185432434\n",
      "Epoch 803, Loss: 1.1322686076164246, Final Batch Loss: 0.29613378643989563\n",
      "Epoch 804, Loss: 1.1655951142311096, Final Batch Loss: 0.29413214325904846\n",
      "Epoch 805, Loss: 1.1516222059726715, Final Batch Loss: 0.33454275131225586\n",
      "Epoch 806, Loss: 1.17202627658844, Final Batch Loss: 0.4022599160671234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 807, Loss: 1.2122367322444916, Final Batch Loss: 0.4079825282096863\n",
      "Epoch 808, Loss: 1.149903029203415, Final Batch Loss: 0.36680957674980164\n",
      "Epoch 809, Loss: 1.0865289270877838, Final Batch Loss: 0.22201427817344666\n",
      "Epoch 810, Loss: 1.1162058115005493, Final Batch Loss: 0.3115398585796356\n",
      "Epoch 811, Loss: 1.3102939426898956, Final Batch Loss: 0.5499561429023743\n",
      "Epoch 812, Loss: 1.255557805299759, Final Batch Loss: 0.47918257117271423\n",
      "Epoch 813, Loss: 1.2333491742610931, Final Batch Loss: 0.48802652955055237\n",
      "Epoch 814, Loss: 1.175592452287674, Final Batch Loss: 0.42264145612716675\n",
      "Epoch 815, Loss: 1.1083691865205765, Final Batch Loss: 0.23159752786159515\n",
      "Epoch 816, Loss: 1.260285645723343, Final Batch Loss: 0.445413202047348\n",
      "Epoch 817, Loss: 1.1410743594169617, Final Batch Loss: 0.31533369421958923\n",
      "Epoch 818, Loss: 1.0328232049942017, Final Batch Loss: 0.27062517404556274\n",
      "Epoch 819, Loss: 1.2354448735713959, Final Batch Loss: 0.43513891100883484\n",
      "Epoch 820, Loss: 1.3128461837768555, Final Batch Loss: 0.5168216228485107\n",
      "Epoch 821, Loss: 1.1410320103168488, Final Batch Loss: 0.3040427267551422\n",
      "Epoch 822, Loss: 1.2430701851844788, Final Batch Loss: 0.4674922525882721\n",
      "Epoch 823, Loss: 1.20750492811203, Final Batch Loss: 0.3914303779602051\n",
      "Epoch 824, Loss: 1.1660377979278564, Final Batch Loss: 0.40534070134162903\n",
      "Epoch 825, Loss: 1.2104597687721252, Final Batch Loss: 0.4268406927585602\n",
      "Epoch 826, Loss: 1.10198974609375, Final Batch Loss: 0.3096432387828827\n",
      "Epoch 827, Loss: 1.1354276835918427, Final Batch Loss: 0.3630804717540741\n",
      "Epoch 828, Loss: 1.1790988147258759, Final Batch Loss: 0.3428884744644165\n",
      "Epoch 829, Loss: 1.1398724019527435, Final Batch Loss: 0.32625067234039307\n",
      "Epoch 830, Loss: 1.2519963383674622, Final Batch Loss: 0.4702250361442566\n",
      "Epoch 831, Loss: 1.186330109834671, Final Batch Loss: 0.37130218744277954\n",
      "Epoch 832, Loss: 1.1420885622501373, Final Batch Loss: 0.35388585925102234\n",
      "Epoch 833, Loss: 1.149183064699173, Final Batch Loss: 0.3053481876850128\n",
      "Epoch 834, Loss: 1.0193907171487808, Final Batch Loss: 0.247688427567482\n",
      "Epoch 835, Loss: 1.1484986245632172, Final Batch Loss: 0.346311092376709\n",
      "Epoch 836, Loss: 1.2396443486213684, Final Batch Loss: 0.49135807156562805\n",
      "Epoch 837, Loss: 1.1485322713851929, Final Batch Loss: 0.395682156085968\n",
      "Epoch 838, Loss: 1.1816775500774384, Final Batch Loss: 0.41348379850387573\n",
      "Epoch 839, Loss: 1.1042876541614532, Final Batch Loss: 0.36354151368141174\n",
      "Epoch 840, Loss: 1.290509194135666, Final Batch Loss: 0.5187029838562012\n",
      "Epoch 841, Loss: 1.1668942868709564, Final Batch Loss: 0.3271801471710205\n",
      "Epoch 842, Loss: 1.0562345683574677, Final Batch Loss: 0.3065659999847412\n",
      "Epoch 843, Loss: 1.1422656774520874, Final Batch Loss: 0.34081557393074036\n",
      "Epoch 844, Loss: 1.1910117268562317, Final Batch Loss: 0.36535000801086426\n",
      "Epoch 845, Loss: 1.2410141229629517, Final Batch Loss: 0.43910136818885803\n",
      "Epoch 846, Loss: 1.1493389010429382, Final Batch Loss: 0.3270409107208252\n",
      "Epoch 847, Loss: 1.113535612821579, Final Batch Loss: 0.34718918800354004\n",
      "Epoch 848, Loss: 1.3922613561153412, Final Batch Loss: 0.6080458760261536\n",
      "Epoch 849, Loss: 1.3321576714515686, Final Batch Loss: 0.520834743976593\n",
      "Epoch 850, Loss: 1.2094495296478271, Final Batch Loss: 0.4386439025402069\n",
      "Epoch 851, Loss: 1.2137924432754517, Final Batch Loss: 0.43626922369003296\n",
      "Epoch 852, Loss: 1.152425855398178, Final Batch Loss: 0.3976960778236389\n",
      "Epoch 853, Loss: 1.2887471914291382, Final Batch Loss: 0.4362230896949768\n",
      "Epoch 854, Loss: 1.1951158046722412, Final Batch Loss: 0.419098436832428\n",
      "Epoch 855, Loss: 1.180524617433548, Final Batch Loss: 0.37709835171699524\n",
      "Epoch 856, Loss: 1.180442452430725, Final Batch Loss: 0.3912988007068634\n",
      "Epoch 857, Loss: 1.222828894853592, Final Batch Loss: 0.5085152387619019\n",
      "Epoch 858, Loss: 1.261838674545288, Final Batch Loss: 0.4312492609024048\n",
      "Epoch 859, Loss: 1.1483704149723053, Final Batch Loss: 0.3830215036869049\n",
      "Epoch 860, Loss: 1.182380586862564, Final Batch Loss: 0.3268516957759857\n",
      "Epoch 861, Loss: 1.1015818119049072, Final Batch Loss: 0.31257393956184387\n",
      "Epoch 862, Loss: 1.1031792163848877, Final Batch Loss: 0.2987637519836426\n",
      "Epoch 863, Loss: 1.1643681824207306, Final Batch Loss: 0.3416423797607422\n",
      "Epoch 864, Loss: 1.2353662848472595, Final Batch Loss: 0.5027002692222595\n",
      "Epoch 865, Loss: 1.1367805898189545, Final Batch Loss: 0.31205815076828003\n",
      "Epoch 866, Loss: 1.1730095744132996, Final Batch Loss: 0.4321115016937256\n",
      "Epoch 867, Loss: 1.3121963739395142, Final Batch Loss: 0.5182469487190247\n",
      "Epoch 868, Loss: 1.2135901749134064, Final Batch Loss: 0.45501017570495605\n",
      "Epoch 869, Loss: 1.1222154796123505, Final Batch Loss: 0.33490484952926636\n",
      "Epoch 870, Loss: 1.1600520610809326, Final Batch Loss: 0.3859484791755676\n",
      "Epoch 871, Loss: 1.133863627910614, Final Batch Loss: 0.33948221802711487\n",
      "Epoch 872, Loss: 1.2402168810367584, Final Batch Loss: 0.4378174841403961\n",
      "Epoch 873, Loss: 1.0782427489757538, Final Batch Loss: 0.273529052734375\n",
      "Epoch 874, Loss: 1.3630591332912445, Final Batch Loss: 0.5640009045600891\n",
      "Epoch 875, Loss: 1.1572346687316895, Final Batch Loss: 0.33855870366096497\n",
      "Epoch 876, Loss: 1.1392862200737, Final Batch Loss: 0.38010329008102417\n",
      "Epoch 877, Loss: 1.1099135279655457, Final Batch Loss: 0.28283455967903137\n",
      "Epoch 878, Loss: 1.1364160180091858, Final Batch Loss: 0.2641991674900055\n",
      "Epoch 879, Loss: 1.2689380645751953, Final Batch Loss: 0.5186541080474854\n",
      "Epoch 880, Loss: 1.2960932850837708, Final Batch Loss: 0.5371975898742676\n",
      "Epoch 881, Loss: 1.1512046456336975, Final Batch Loss: 0.2910459041595459\n",
      "Epoch 882, Loss: 1.1795504987239838, Final Batch Loss: 0.37332606315612793\n",
      "Epoch 883, Loss: 1.2124739289283752, Final Batch Loss: 0.46731871366500854\n",
      "Epoch 884, Loss: 1.0630764663219452, Final Batch Loss: 0.3177560567855835\n",
      "Epoch 885, Loss: 1.2503764629364014, Final Batch Loss: 0.45190301537513733\n",
      "Epoch 886, Loss: 1.1734935939311981, Final Batch Loss: 0.37631556391716003\n",
      "Epoch 887, Loss: 1.1443942487239838, Final Batch Loss: 0.3468761742115021\n",
      "Epoch 888, Loss: 1.2983152568340302, Final Batch Loss: 0.5535468459129333\n",
      "Epoch 889, Loss: 1.179960697889328, Final Batch Loss: 0.4567684531211853\n",
      "Epoch 890, Loss: 1.086430847644806, Final Batch Loss: 0.263310968875885\n",
      "Epoch 891, Loss: 1.3487710058689117, Final Batch Loss: 0.5306149125099182\n",
      "Epoch 892, Loss: 1.2723096013069153, Final Batch Loss: 0.48383450508117676\n",
      "Epoch 893, Loss: 1.1999838054180145, Final Batch Loss: 0.3871166408061981\n",
      "Epoch 894, Loss: 1.1860560774803162, Final Batch Loss: 0.42953887581825256\n",
      "Epoch 895, Loss: 1.110871821641922, Final Batch Loss: 0.2925949990749359\n",
      "Epoch 896, Loss: 1.212759017944336, Final Batch Loss: 0.417947918176651\n",
      "Epoch 897, Loss: 1.2138305604457855, Final Batch Loss: 0.4801364541053772\n",
      "Epoch 898, Loss: 1.1395834684371948, Final Batch Loss: 0.39983710646629333\n",
      "Epoch 899, Loss: 1.1175499260425568, Final Batch Loss: 0.39462965726852417\n",
      "Epoch 900, Loss: 1.2616187930107117, Final Batch Loss: 0.3576555848121643\n",
      "Epoch 901, Loss: 1.088354229927063, Final Batch Loss: 0.3621683418750763\n",
      "Epoch 902, Loss: 1.1330333948135376, Final Batch Loss: 0.32720625400543213\n",
      "Epoch 903, Loss: 1.108965277671814, Final Batch Loss: 0.32031720876693726\n",
      "Epoch 904, Loss: 1.0840340554714203, Final Batch Loss: 0.31997331976890564\n",
      "Epoch 905, Loss: 1.1456791758537292, Final Batch Loss: 0.3477494716644287\n",
      "Epoch 906, Loss: 1.2663732469081879, Final Batch Loss: 0.5005655884742737\n",
      "Epoch 907, Loss: 1.1892551183700562, Final Batch Loss: 0.4004799723625183\n",
      "Epoch 908, Loss: 1.1107227206230164, Final Batch Loss: 0.32194003462791443\n",
      "Epoch 909, Loss: 1.1045860052108765, Final Batch Loss: 0.3426835834980011\n",
      "Epoch 910, Loss: 1.16043359041214, Final Batch Loss: 0.3824017643928528\n",
      "Epoch 911, Loss: 1.076152116060257, Final Batch Loss: 0.29360508918762207\n",
      "Epoch 912, Loss: 1.0981700718402863, Final Batch Loss: 0.3247542381286621\n",
      "Epoch 913, Loss: 1.0964741110801697, Final Batch Loss: 0.324146032333374\n",
      "Epoch 914, Loss: 1.2006983160972595, Final Batch Loss: 0.4760097861289978\n",
      "Epoch 915, Loss: 1.0568437278270721, Final Batch Loss: 0.27412521839141846\n",
      "Epoch 916, Loss: 1.1670311093330383, Final Batch Loss: 0.438213050365448\n",
      "Epoch 917, Loss: 1.273553192615509, Final Batch Loss: 0.5047551989555359\n",
      "Epoch 918, Loss: 1.1193090975284576, Final Batch Loss: 0.3444035053253174\n",
      "Epoch 919, Loss: 1.0667747855186462, Final Batch Loss: 0.3078584372997284\n",
      "Epoch 920, Loss: 1.1571430265903473, Final Batch Loss: 0.44150781631469727\n",
      "Epoch 921, Loss: 1.20981964468956, Final Batch Loss: 0.46630167961120605\n",
      "Epoch 922, Loss: 1.21985325217247, Final Batch Loss: 0.48586535453796387\n",
      "Epoch 923, Loss: 1.1446561813354492, Final Batch Loss: 0.3912754952907562\n",
      "Epoch 924, Loss: 1.1371372938156128, Final Batch Loss: 0.38033628463745117\n",
      "Epoch 925, Loss: 1.425110787153244, Final Batch Loss: 0.6461357474327087\n",
      "Epoch 926, Loss: 1.1559137105941772, Final Batch Loss: 0.390325665473938\n",
      "Epoch 927, Loss: 1.2861810326576233, Final Batch Loss: 0.52615886926651\n",
      "Epoch 928, Loss: 1.0334175825119019, Final Batch Loss: 0.27556777000427246\n",
      "Epoch 929, Loss: 1.146752506494522, Final Batch Loss: 0.41728463768959045\n",
      "Epoch 930, Loss: 1.1408870220184326, Final Batch Loss: 0.35136333107948303\n",
      "Epoch 931, Loss: 1.1297963559627533, Final Batch Loss: 0.3789879083633423\n",
      "Epoch 932, Loss: 1.051040455698967, Final Batch Loss: 0.22500653564929962\n",
      "Epoch 933, Loss: 1.1121907234191895, Final Batch Loss: 0.37336820363998413\n",
      "Epoch 934, Loss: 1.1413677334785461, Final Batch Loss: 0.4320231080055237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 935, Loss: 1.2144056856632233, Final Batch Loss: 0.43946149945259094\n",
      "Epoch 936, Loss: 1.1785212457180023, Final Batch Loss: 0.4802979528903961\n",
      "Epoch 937, Loss: 1.1723436117172241, Final Batch Loss: 0.4200983941555023\n",
      "Epoch 938, Loss: 1.2398640811443329, Final Batch Loss: 0.45187678933143616\n",
      "Epoch 939, Loss: 1.1935476064682007, Final Batch Loss: 0.4475451111793518\n",
      "Epoch 940, Loss: 1.1713268756866455, Final Batch Loss: 0.4114561676979065\n",
      "Epoch 941, Loss: 1.2667640149593353, Final Batch Loss: 0.4232606291770935\n",
      "Epoch 942, Loss: 1.090651273727417, Final Batch Loss: 0.30662959814071655\n",
      "Epoch 943, Loss: 1.204480528831482, Final Batch Loss: 0.4338996708393097\n",
      "Epoch 944, Loss: 1.0890664756298065, Final Batch Loss: 0.34182432293891907\n",
      "Epoch 945, Loss: 1.1739467978477478, Final Batch Loss: 0.4098173677921295\n",
      "Epoch 946, Loss: 1.230071872472763, Final Batch Loss: 0.48414674401283264\n",
      "Epoch 947, Loss: 1.0738274455070496, Final Batch Loss: 0.2840435802936554\n",
      "Epoch 948, Loss: 1.1555074453353882, Final Batch Loss: 0.3854252099990845\n",
      "Epoch 949, Loss: 1.1514657139778137, Final Batch Loss: 0.41362735629081726\n",
      "Epoch 950, Loss: 1.0438325703144073, Final Batch Loss: 0.2510448098182678\n",
      "Epoch 951, Loss: 1.1886786222457886, Final Batch Loss: 0.44077301025390625\n",
      "Epoch 952, Loss: 1.24966362118721, Final Batch Loss: 0.48568081855773926\n",
      "Epoch 953, Loss: 1.0219178050756454, Final Batch Loss: 0.23044650256633759\n",
      "Epoch 954, Loss: 1.2553209066390991, Final Batch Loss: 0.587498664855957\n",
      "Epoch 955, Loss: 0.9865936189889908, Final Batch Loss: 0.20426972210407257\n",
      "Epoch 956, Loss: 1.0535476803779602, Final Batch Loss: 0.3025343418121338\n",
      "Epoch 957, Loss: 1.1392240524291992, Final Batch Loss: 0.3799546957015991\n",
      "Epoch 958, Loss: 1.212841808795929, Final Batch Loss: 0.46409425139427185\n",
      "Epoch 959, Loss: 1.1282709538936615, Final Batch Loss: 0.3141535222530365\n",
      "Epoch 960, Loss: 1.0764449834823608, Final Batch Loss: 0.3434867858886719\n",
      "Epoch 961, Loss: 1.1867933571338654, Final Batch Loss: 0.4835827052593231\n",
      "Epoch 962, Loss: 1.1540726721286774, Final Batch Loss: 0.4141671061515808\n",
      "Epoch 963, Loss: 1.1321054100990295, Final Batch Loss: 0.34098827838897705\n",
      "Epoch 964, Loss: 1.1665943562984467, Final Batch Loss: 0.4121827781200409\n",
      "Epoch 965, Loss: 1.051855355501175, Final Batch Loss: 0.28076937794685364\n",
      "Epoch 966, Loss: 1.168797105550766, Final Batch Loss: 0.42011702060699463\n",
      "Epoch 967, Loss: 1.1977191865444183, Final Batch Loss: 0.46825990080833435\n",
      "Epoch 968, Loss: 1.029920607805252, Final Batch Loss: 0.2681589424610138\n",
      "Epoch 969, Loss: 1.1203352510929108, Final Batch Loss: 0.39456528425216675\n",
      "Epoch 970, Loss: 1.0060083866119385, Final Batch Loss: 0.27601391077041626\n",
      "Epoch 971, Loss: 1.1393694281578064, Final Batch Loss: 0.3783155679702759\n",
      "Epoch 972, Loss: 1.1785486042499542, Final Batch Loss: 0.46666383743286133\n",
      "Epoch 973, Loss: 1.129027932882309, Final Batch Loss: 0.2871040105819702\n",
      "Epoch 974, Loss: 1.2474094331264496, Final Batch Loss: 0.5027282238006592\n",
      "Epoch 975, Loss: 1.136093646287918, Final Batch Loss: 0.38665348291397095\n",
      "Epoch 976, Loss: 1.126068890094757, Final Batch Loss: 0.35761430859565735\n",
      "Epoch 977, Loss: 1.1492638289928436, Final Batch Loss: 0.3972901403903961\n",
      "Epoch 978, Loss: 1.1145327985286713, Final Batch Loss: 0.31149154901504517\n",
      "Epoch 979, Loss: 1.1546139419078827, Final Batch Loss: 0.39759156107902527\n",
      "Epoch 980, Loss: 1.0426214337348938, Final Batch Loss: 0.2708648145198822\n",
      "Epoch 981, Loss: 1.0225684642791748, Final Batch Loss: 0.25607314705848694\n",
      "Epoch 982, Loss: 1.1438256800174713, Final Batch Loss: 0.355567067861557\n",
      "Epoch 983, Loss: 1.207849770784378, Final Batch Loss: 0.4321998655796051\n",
      "Epoch 984, Loss: 1.1417667269706726, Final Batch Loss: 0.373934268951416\n",
      "Epoch 985, Loss: 1.147274523973465, Final Batch Loss: 0.3761610686779022\n",
      "Epoch 986, Loss: 1.226768434047699, Final Batch Loss: 0.46556195616722107\n",
      "Epoch 987, Loss: 1.1121922731399536, Final Batch Loss: 0.3748108148574829\n",
      "Epoch 988, Loss: 1.0543830692768097, Final Batch Loss: 0.3497978448867798\n",
      "Epoch 989, Loss: 1.1019188165664673, Final Batch Loss: 0.3214275538921356\n",
      "Epoch 990, Loss: 1.0544623583555222, Final Batch Loss: 0.24361281096935272\n",
      "Epoch 991, Loss: 1.1764815151691437, Final Batch Loss: 0.4769003987312317\n",
      "Epoch 992, Loss: 1.0476654469966888, Final Batch Loss: 0.30523109436035156\n",
      "Epoch 993, Loss: 1.1788620948791504, Final Batch Loss: 0.42659762501716614\n",
      "Epoch 994, Loss: 1.1249780058860779, Final Batch Loss: 0.3795420527458191\n",
      "Epoch 995, Loss: 1.118565559387207, Final Batch Loss: 0.3697343170642853\n",
      "Epoch 996, Loss: 1.253833681344986, Final Batch Loss: 0.5361833572387695\n",
      "Epoch 997, Loss: 1.0183677971363068, Final Batch Loss: 0.26948490738868713\n",
      "Epoch 998, Loss: 1.1400254666805267, Final Batch Loss: 0.37626174092292786\n",
      "Epoch 999, Loss: 1.1190962493419647, Final Batch Loss: 0.39800572395324707\n",
      "Epoch 1000, Loss: 1.3024575412273407, Final Batch Loss: 0.5634872913360596\n",
      "Epoch 1001, Loss: 1.2013622224330902, Final Batch Loss: 0.45296618342399597\n",
      "Epoch 1002, Loss: 1.2718155980110168, Final Batch Loss: 0.5297738313674927\n",
      "Epoch 1003, Loss: 1.1976403594017029, Final Batch Loss: 0.39457449316978455\n",
      "Epoch 1004, Loss: 1.0835552513599396, Final Batch Loss: 0.3710436224937439\n",
      "Epoch 1005, Loss: 1.065154790878296, Final Batch Loss: 0.2776566445827484\n",
      "Epoch 1006, Loss: 1.1558742821216583, Final Batch Loss: 0.36344096064567566\n",
      "Epoch 1007, Loss: 1.2429088354110718, Final Batch Loss: 0.46860384941101074\n",
      "Epoch 1008, Loss: 1.1055918037891388, Final Batch Loss: 0.31648021936416626\n",
      "Epoch 1009, Loss: 1.1296316385269165, Final Batch Loss: 0.3869515657424927\n",
      "Epoch 1010, Loss: 1.2089032232761383, Final Batch Loss: 0.4580434560775757\n",
      "Epoch 1011, Loss: 1.085381031036377, Final Batch Loss: 0.3794603645801544\n",
      "Epoch 1012, Loss: 1.203481286764145, Final Batch Loss: 0.45585760474205017\n",
      "Epoch 1013, Loss: 1.3643164932727814, Final Batch Loss: 0.6072053909301758\n",
      "Epoch 1014, Loss: 1.074522614479065, Final Batch Loss: 0.3207660913467407\n",
      "Epoch 1015, Loss: 1.221290111541748, Final Batch Loss: 0.46098488569259644\n",
      "Epoch 1016, Loss: 1.0992274582386017, Final Batch Loss: 0.30908551812171936\n",
      "Epoch 1017, Loss: 1.2214113473892212, Final Batch Loss: 0.45723557472229004\n",
      "Epoch 1018, Loss: 1.255470871925354, Final Batch Loss: 0.499647855758667\n",
      "Epoch 1019, Loss: 1.1125104427337646, Final Batch Loss: 0.32794326543807983\n",
      "Epoch 1020, Loss: 1.1030927896499634, Final Batch Loss: 0.3236652612686157\n",
      "Epoch 1021, Loss: 1.0536614656448364, Final Batch Loss: 0.3321848511695862\n",
      "Epoch 1022, Loss: 1.122326672077179, Final Batch Loss: 0.3567569851875305\n",
      "Epoch 1023, Loss: 1.105005830526352, Final Batch Loss: 0.32692551612854004\n",
      "Epoch 1024, Loss: 1.1544288098812103, Final Batch Loss: 0.42961862683296204\n",
      "Epoch 1025, Loss: 1.211842268705368, Final Batch Loss: 0.4277271032333374\n",
      "Epoch 1026, Loss: 1.2400244176387787, Final Batch Loss: 0.5241947770118713\n",
      "Epoch 1027, Loss: 1.0726054310798645, Final Batch Loss: 0.3299697935581207\n",
      "Epoch 1028, Loss: 1.1648620665073395, Final Batch Loss: 0.4474397301673889\n",
      "Epoch 1029, Loss: 1.1645840108394623, Final Batch Loss: 0.45999523997306824\n",
      "Epoch 1030, Loss: 1.3172224462032318, Final Batch Loss: 0.574630618095398\n",
      "Epoch 1031, Loss: 1.2244445383548737, Final Batch Loss: 0.5266225934028625\n",
      "Epoch 1032, Loss: 1.1932242810726166, Final Batch Loss: 0.43820881843566895\n",
      "Epoch 1033, Loss: 1.1152644753456116, Final Batch Loss: 0.35453033447265625\n",
      "Epoch 1034, Loss: 1.0594017505645752, Final Batch Loss: 0.28842177987098694\n",
      "Epoch 1035, Loss: 1.1413628160953522, Final Batch Loss: 0.4494050443172455\n",
      "Epoch 1036, Loss: 1.0770199596881866, Final Batch Loss: 0.33310598134994507\n",
      "Epoch 1037, Loss: 1.1175957024097443, Final Batch Loss: 0.3600883185863495\n",
      "Epoch 1038, Loss: 1.0867334604263306, Final Batch Loss: 0.345005065202713\n",
      "Epoch 1039, Loss: 1.1663801670074463, Final Batch Loss: 0.40673574805259705\n",
      "Epoch 1040, Loss: 1.005860909819603, Final Batch Loss: 0.20190618932247162\n",
      "Epoch 1041, Loss: 1.2051198780536652, Final Batch Loss: 0.4463941752910614\n",
      "Epoch 1042, Loss: 1.0841220617294312, Final Batch Loss: 0.369445264339447\n",
      "Epoch 1043, Loss: 1.2259694337844849, Final Batch Loss: 0.5174415707588196\n",
      "Epoch 1044, Loss: 1.135197639465332, Final Batch Loss: 0.3617987036705017\n",
      "Epoch 1045, Loss: 1.1266295909881592, Final Batch Loss: 0.43402573466300964\n",
      "Epoch 1046, Loss: 0.994482010602951, Final Batch Loss: 0.30238279700279236\n",
      "Epoch 1047, Loss: 1.1254838407039642, Final Batch Loss: 0.28580954670906067\n",
      "Epoch 1048, Loss: 1.0783761143684387, Final Batch Loss: 0.30076754093170166\n",
      "Epoch 1049, Loss: 1.152250811457634, Final Batch Loss: 0.40937766432762146\n",
      "Epoch 1050, Loss: 1.2086998224258423, Final Batch Loss: 0.4736914038658142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1051, Loss: 1.0041930377483368, Final Batch Loss: 0.25991132855415344\n",
      "Epoch 1052, Loss: 1.0505256354808807, Final Batch Loss: 0.315851628780365\n",
      "Epoch 1053, Loss: 1.0579722821712494, Final Batch Loss: 0.321939617395401\n",
      "Epoch 1054, Loss: 1.0141130983829498, Final Batch Loss: 0.2808387875556946\n",
      "Epoch 1055, Loss: 1.2440987527370453, Final Batch Loss: 0.49746695160865784\n",
      "Epoch 1056, Loss: 1.0813322961330414, Final Batch Loss: 0.35293665528297424\n",
      "Epoch 1057, Loss: 0.9715545773506165, Final Batch Loss: 0.22581839561462402\n",
      "Epoch 1058, Loss: 1.1042436063289642, Final Batch Loss: 0.3272510766983032\n",
      "Epoch 1059, Loss: 1.0222173035144806, Final Batch Loss: 0.2743895649909973\n",
      "Epoch 1060, Loss: 1.0858803987503052, Final Batch Loss: 0.35968682169914246\n",
      "Epoch 1061, Loss: 1.129756212234497, Final Batch Loss: 0.3310335576534271\n",
      "Epoch 1062, Loss: 1.0608887374401093, Final Batch Loss: 0.30446985363960266\n",
      "Epoch 1063, Loss: 1.0686039924621582, Final Batch Loss: 0.31545937061309814\n",
      "Epoch 1064, Loss: 0.9854442775249481, Final Batch Loss: 0.27349939942359924\n",
      "Epoch 1065, Loss: 1.1131506562232971, Final Batch Loss: 0.39805325865745544\n",
      "Epoch 1066, Loss: 1.086739718914032, Final Batch Loss: 0.331350713968277\n",
      "Epoch 1067, Loss: 1.0444281697273254, Final Batch Loss: 0.29426106810569763\n",
      "Epoch 1068, Loss: 1.022277981042862, Final Batch Loss: 0.25788792967796326\n",
      "Epoch 1069, Loss: 1.0411518216133118, Final Batch Loss: 0.2576466500759125\n",
      "Epoch 1070, Loss: 1.1535040736198425, Final Batch Loss: 0.42443394660949707\n",
      "Epoch 1071, Loss: 1.0466066896915436, Final Batch Loss: 0.31693780422210693\n",
      "Epoch 1072, Loss: 1.2601831555366516, Final Batch Loss: 0.4790197014808655\n",
      "Epoch 1073, Loss: 1.065306693315506, Final Batch Loss: 0.34496113657951355\n",
      "Epoch 1074, Loss: 1.1165294349193573, Final Batch Loss: 0.3978044092655182\n",
      "Epoch 1075, Loss: 1.021677404642105, Final Batch Loss: 0.33043864369392395\n",
      "Epoch 1076, Loss: 1.2104510068893433, Final Batch Loss: 0.4608347415924072\n",
      "Epoch 1077, Loss: 1.1203889548778534, Final Batch Loss: 0.3802017569541931\n",
      "Epoch 1078, Loss: 0.9679072797298431, Final Batch Loss: 0.21048149466514587\n",
      "Epoch 1079, Loss: 1.116203099489212, Final Batch Loss: 0.32533174753189087\n",
      "Epoch 1080, Loss: 1.0639055371284485, Final Batch Loss: 0.38880655169487\n",
      "Epoch 1081, Loss: 1.1871459186077118, Final Batch Loss: 0.42197874188423157\n",
      "Epoch 1082, Loss: 0.9916140139102936, Final Batch Loss: 0.2533356845378876\n",
      "Epoch 1083, Loss: 1.1286916434764862, Final Batch Loss: 0.41005265712738037\n",
      "Epoch 1084, Loss: 1.042507529258728, Final Batch Loss: 0.3122096359729767\n",
      "Epoch 1085, Loss: 1.0428675413131714, Final Batch Loss: 0.32021912932395935\n",
      "Epoch 1086, Loss: 1.1119470000267029, Final Batch Loss: 0.3792691230773926\n",
      "Epoch 1087, Loss: 1.1650331318378448, Final Batch Loss: 0.43792545795440674\n",
      "Epoch 1088, Loss: 1.1215945482254028, Final Batch Loss: 0.3027137517929077\n",
      "Epoch 1089, Loss: 1.1312996745109558, Final Batch Loss: 0.45364370942115784\n",
      "Epoch 1090, Loss: 1.044382005929947, Final Batch Loss: 0.3663851320743561\n",
      "Epoch 1091, Loss: 1.1930021345615387, Final Batch Loss: 0.35938915610313416\n",
      "Epoch 1092, Loss: 1.18379345536232, Final Batch Loss: 0.43776240944862366\n",
      "Epoch 1093, Loss: 1.0452392995357513, Final Batch Loss: 0.3071694076061249\n",
      "Epoch 1094, Loss: 1.1519116461277008, Final Batch Loss: 0.42817777395248413\n",
      "Epoch 1095, Loss: 1.103281944990158, Final Batch Loss: 0.3672471344470978\n",
      "Epoch 1096, Loss: 1.1863930523395538, Final Batch Loss: 0.4902547299861908\n",
      "Epoch 1097, Loss: 1.1242512464523315, Final Batch Loss: 0.3853941857814789\n",
      "Epoch 1098, Loss: 1.0488799512386322, Final Batch Loss: 0.28764820098876953\n",
      "Epoch 1099, Loss: 1.2796771228313446, Final Batch Loss: 0.5385086536407471\n",
      "Epoch 1100, Loss: 1.002706229686737, Final Batch Loss: 0.2672085762023926\n",
      "Epoch 1101, Loss: 1.107637345790863, Final Batch Loss: 0.3968828022480011\n",
      "Epoch 1102, Loss: 1.00880628824234, Final Batch Loss: 0.3146035075187683\n",
      "Epoch 1103, Loss: 1.1712957918643951, Final Batch Loss: 0.46277156472206116\n",
      "Epoch 1104, Loss: 1.2039412558078766, Final Batch Loss: 0.4991016089916229\n",
      "Epoch 1105, Loss: 0.9835593700408936, Final Batch Loss: 0.3027258813381195\n",
      "Epoch 1106, Loss: 0.8858306556940079, Final Batch Loss: 0.1520952731370926\n",
      "Epoch 1107, Loss: 1.150217443704605, Final Batch Loss: 0.3926994800567627\n",
      "Epoch 1108, Loss: 1.3074017465114594, Final Batch Loss: 0.5395258069038391\n",
      "Epoch 1109, Loss: 1.100676566362381, Final Batch Loss: 0.3386813700199127\n",
      "Epoch 1110, Loss: 1.1014215350151062, Final Batch Loss: 0.34763002395629883\n",
      "Epoch 1111, Loss: 1.0322133898735046, Final Batch Loss: 0.3446025848388672\n",
      "Epoch 1112, Loss: 1.0677496790885925, Final Batch Loss: 0.3532020151615143\n",
      "Epoch 1113, Loss: 1.161443144083023, Final Batch Loss: 0.46365898847579956\n",
      "Epoch 1114, Loss: 1.24922576546669, Final Batch Loss: 0.5385251045227051\n",
      "Epoch 1115, Loss: 1.015346646308899, Final Batch Loss: 0.350515753030777\n",
      "Epoch 1116, Loss: 1.311742752790451, Final Batch Loss: 0.5880378484725952\n",
      "Epoch 1117, Loss: 1.1313790380954742, Final Batch Loss: 0.42427533864974976\n",
      "Epoch 1118, Loss: 1.102602243423462, Final Batch Loss: 0.37187767028808594\n",
      "Epoch 1119, Loss: 1.0639610588550568, Final Batch Loss: 0.3097255527973175\n",
      "Epoch 1120, Loss: 1.0415424406528473, Final Batch Loss: 0.2893097996711731\n",
      "Epoch 1121, Loss: 1.031548410654068, Final Batch Loss: 0.3091316223144531\n",
      "Epoch 1122, Loss: 1.0279623568058014, Final Batch Loss: 0.2993122339248657\n",
      "Epoch 1123, Loss: 1.1660830974578857, Final Batch Loss: 0.37394678592681885\n",
      "Epoch 1124, Loss: 1.147425353527069, Final Batch Loss: 0.41993990540504456\n",
      "Epoch 1125, Loss: 1.0641734302043915, Final Batch Loss: 0.3013748526573181\n",
      "Epoch 1126, Loss: 1.0657983422279358, Final Batch Loss: 0.40578019618988037\n",
      "Epoch 1127, Loss: 1.1016845107078552, Final Batch Loss: 0.4040133059024811\n",
      "Epoch 1128, Loss: 1.1087131202220917, Final Batch Loss: 0.40820780396461487\n",
      "Epoch 1129, Loss: 1.0640012919902802, Final Batch Loss: 0.2797500491142273\n",
      "Epoch 1130, Loss: 1.077510416507721, Final Batch Loss: 0.4153486490249634\n",
      "Epoch 1131, Loss: 1.1242163479328156, Final Batch Loss: 0.41083115339279175\n",
      "Epoch 1132, Loss: 1.0600059032440186, Final Batch Loss: 0.3148883283138275\n",
      "Epoch 1133, Loss: 0.941750094294548, Final Batch Loss: 0.24202288687229156\n",
      "Epoch 1134, Loss: 1.078319787979126, Final Batch Loss: 0.28997454047203064\n",
      "Epoch 1135, Loss: 0.9572781026363373, Final Batch Loss: 0.2650960683822632\n",
      "Epoch 1136, Loss: 1.2154216170310974, Final Batch Loss: 0.4632805287837982\n",
      "Epoch 1137, Loss: 1.0552489161491394, Final Batch Loss: 0.3459782004356384\n",
      "Epoch 1138, Loss: 1.1335208117961884, Final Batch Loss: 0.4321504235267639\n",
      "Epoch 1139, Loss: 0.9836452156305313, Final Batch Loss: 0.24243749678134918\n",
      "Epoch 1140, Loss: 1.1881172955036163, Final Batch Loss: 0.5027155876159668\n",
      "Epoch 1141, Loss: 1.017255812883377, Final Batch Loss: 0.2930678725242615\n",
      "Epoch 1142, Loss: 1.049875557422638, Final Batch Loss: 0.37764573097229004\n",
      "Epoch 1143, Loss: 1.0059527158737183, Final Batch Loss: 0.25405097007751465\n",
      "Epoch 1144, Loss: 1.138563334941864, Final Batch Loss: 0.4001329243183136\n",
      "Epoch 1145, Loss: 1.184079110622406, Final Batch Loss: 0.4851643443107605\n",
      "Epoch 1146, Loss: 1.1166990399360657, Final Batch Loss: 0.38692304491996765\n",
      "Epoch 1147, Loss: 0.9981282353401184, Final Batch Loss: 0.32609328627586365\n",
      "Epoch 1148, Loss: 1.1335722208023071, Final Batch Loss: 0.4711952209472656\n",
      "Epoch 1149, Loss: 1.1089340448379517, Final Batch Loss: 0.3863057494163513\n",
      "Epoch 1150, Loss: 1.1311549544334412, Final Batch Loss: 0.4573901295661926\n",
      "Epoch 1151, Loss: 0.977168008685112, Final Batch Loss: 0.22705499827861786\n",
      "Epoch 1152, Loss: 0.9536504745483398, Final Batch Loss: 0.22972798347473145\n",
      "Epoch 1153, Loss: 1.0591039955615997, Final Batch Loss: 0.32446786761283875\n",
      "Epoch 1154, Loss: 1.0636748969554901, Final Batch Loss: 0.37693262100219727\n",
      "Epoch 1155, Loss: 1.1253046989440918, Final Batch Loss: 0.3972347378730774\n",
      "Epoch 1156, Loss: 1.059406965970993, Final Batch Loss: 0.2874681353569031\n",
      "Epoch 1157, Loss: 1.1200716197490692, Final Batch Loss: 0.3969508111476898\n",
      "Epoch 1158, Loss: 1.065436214208603, Final Batch Loss: 0.39199191331863403\n",
      "Epoch 1159, Loss: 1.0868732333183289, Final Batch Loss: 0.38301151990890503\n",
      "Epoch 1160, Loss: 0.9684288501739502, Final Batch Loss: 0.23345249891281128\n",
      "Epoch 1161, Loss: 1.171541452407837, Final Batch Loss: 0.4519610106945038\n",
      "Epoch 1162, Loss: 1.0587105751037598, Final Batch Loss: 0.2942800521850586\n",
      "Epoch 1163, Loss: 1.1986610889434814, Final Batch Loss: 0.4746609926223755\n",
      "Epoch 1164, Loss: 1.140703409910202, Final Batch Loss: 0.39259618520736694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1165, Loss: 0.9878647029399872, Final Batch Loss: 0.3338963985443115\n",
      "Epoch 1166, Loss: 1.0597603619098663, Final Batch Loss: 0.36482709646224976\n",
      "Epoch 1167, Loss: 1.1003900170326233, Final Batch Loss: 0.38781511783599854\n",
      "Epoch 1168, Loss: 1.1307547390460968, Final Batch Loss: 0.4019763767719269\n",
      "Epoch 1169, Loss: 0.9436830133199692, Final Batch Loss: 0.21021030843257904\n",
      "Epoch 1170, Loss: 1.0627919435501099, Final Batch Loss: 0.36461642384529114\n",
      "Epoch 1171, Loss: 1.0851281583309174, Final Batch Loss: 0.3919546902179718\n",
      "Epoch 1172, Loss: 1.0568081438541412, Final Batch Loss: 0.31787770986557007\n",
      "Epoch 1173, Loss: 1.033696860074997, Final Batch Loss: 0.32427993416786194\n",
      "Epoch 1174, Loss: 1.1192059516906738, Final Batch Loss: 0.3940187990665436\n",
      "Epoch 1175, Loss: 1.0260018408298492, Final Batch Loss: 0.3623500466346741\n",
      "Epoch 1176, Loss: 1.1368821859359741, Final Batch Loss: 0.42774099111557007\n",
      "Epoch 1177, Loss: 0.9617665410041809, Final Batch Loss: 0.29652631282806396\n",
      "Epoch 1178, Loss: 1.0522553324699402, Final Batch Loss: 0.30738648772239685\n",
      "Epoch 1179, Loss: 1.0559894144535065, Final Batch Loss: 0.3661201596260071\n",
      "Epoch 1180, Loss: 1.0526879131793976, Final Batch Loss: 0.3542483448982239\n",
      "Epoch 1181, Loss: 1.0232148468494415, Final Batch Loss: 0.3165132999420166\n",
      "Epoch 1182, Loss: 0.9243678152561188, Final Batch Loss: 0.2012878954410553\n",
      "Epoch 1183, Loss: 1.0234492719173431, Final Batch Loss: 0.31673336029052734\n",
      "Epoch 1184, Loss: 1.0148357450962067, Final Batch Loss: 0.2950632870197296\n",
      "Epoch 1185, Loss: 1.0524250864982605, Final Batch Loss: 0.38480934500694275\n",
      "Epoch 1186, Loss: 1.0325913429260254, Final Batch Loss: 0.3271353542804718\n",
      "Epoch 1187, Loss: 1.0747131705284119, Final Batch Loss: 0.3959387242794037\n",
      "Epoch 1188, Loss: 1.0182450115680695, Final Batch Loss: 0.32631802558898926\n",
      "Epoch 1189, Loss: 1.138197660446167, Final Batch Loss: 0.4299863576889038\n",
      "Epoch 1190, Loss: 1.0197249352931976, Final Batch Loss: 0.35883545875549316\n",
      "Epoch 1191, Loss: 1.0489741563796997, Final Batch Loss: 0.30216968059539795\n",
      "Epoch 1192, Loss: 0.9388429224491119, Final Batch Loss: 0.22345563769340515\n",
      "Epoch 1193, Loss: 1.1061775982379913, Final Batch Loss: 0.4323473274707794\n",
      "Epoch 1194, Loss: 0.8661359548568726, Final Batch Loss: 0.17451578378677368\n",
      "Epoch 1195, Loss: 1.1631711423397064, Final Batch Loss: 0.4579991400241852\n",
      "Epoch 1196, Loss: 1.033479928970337, Final Batch Loss: 0.32623305916786194\n",
      "Epoch 1197, Loss: 1.0940616726875305, Final Batch Loss: 0.38507261872291565\n",
      "Epoch 1198, Loss: 1.0636774897575378, Final Batch Loss: 0.34873661398887634\n",
      "Epoch 1199, Loss: 0.9682649970054626, Final Batch Loss: 0.2924005091190338\n",
      "Epoch 1200, Loss: 1.0025474429130554, Final Batch Loss: 0.3537190854549408\n",
      "Epoch 1201, Loss: 0.9670998752117157, Final Batch Loss: 0.28435853123664856\n",
      "Epoch 1202, Loss: 1.1015693545341492, Final Batch Loss: 0.43610626459121704\n",
      "Epoch 1203, Loss: 1.0259639620780945, Final Batch Loss: 0.35732463002204895\n",
      "Epoch 1204, Loss: 1.0888400375843048, Final Batch Loss: 0.3037601411342621\n",
      "Epoch 1205, Loss: 0.8971430361270905, Final Batch Loss: 0.20748355984687805\n",
      "Epoch 1206, Loss: 0.93400439620018, Final Batch Loss: 0.2524300217628479\n",
      "Epoch 1207, Loss: 1.095426768064499, Final Batch Loss: 0.39765334129333496\n",
      "Epoch 1208, Loss: 1.0597785413265228, Final Batch Loss: 0.39725711941719055\n",
      "Epoch 1209, Loss: 1.1567049324512482, Final Batch Loss: 0.43493425846099854\n",
      "Epoch 1210, Loss: 1.1299781799316406, Final Batch Loss: 0.41640907526016235\n",
      "Epoch 1211, Loss: 0.9225620478391647, Final Batch Loss: 0.2372739166021347\n",
      "Epoch 1212, Loss: 1.1401768922805786, Final Batch Loss: 0.401398628950119\n",
      "Epoch 1213, Loss: 1.0660087764263153, Final Batch Loss: 0.3466055691242218\n",
      "Epoch 1214, Loss: 1.080279678106308, Final Batch Loss: 0.38833844661712646\n",
      "Epoch 1215, Loss: 1.0449407398700714, Final Batch Loss: 0.3391799032688141\n",
      "Epoch 1216, Loss: 1.083192229270935, Final Batch Loss: 0.40727657079696655\n",
      "Epoch 1217, Loss: 1.0177950859069824, Final Batch Loss: 0.3630297780036926\n",
      "Epoch 1218, Loss: 1.0708091855049133, Final Batch Loss: 0.3758629262447357\n",
      "Epoch 1219, Loss: 0.9646191596984863, Final Batch Loss: 0.27323880791664124\n",
      "Epoch 1220, Loss: 0.9920835196971893, Final Batch Loss: 0.2935864329338074\n",
      "Epoch 1221, Loss: 1.1149404346942902, Final Batch Loss: 0.3606893718242645\n",
      "Epoch 1222, Loss: 1.11817267537117, Final Batch Loss: 0.41495436429977417\n",
      "Epoch 1223, Loss: 1.1626515984535217, Final Batch Loss: 0.45241037011146545\n",
      "Epoch 1224, Loss: 1.1129786372184753, Final Batch Loss: 0.3671702444553375\n",
      "Epoch 1225, Loss: 0.9532054364681244, Final Batch Loss: 0.26775062084198\n",
      "Epoch 1226, Loss: 1.133140116930008, Final Batch Loss: 0.4250926077365875\n",
      "Epoch 1227, Loss: 0.9034637212753296, Final Batch Loss: 0.2237299084663391\n",
      "Epoch 1228, Loss: 1.0973306894302368, Final Batch Loss: 0.42696747183799744\n",
      "Epoch 1229, Loss: 0.9179302603006363, Final Batch Loss: 0.21845866739749908\n",
      "Epoch 1230, Loss: 1.1740812063217163, Final Batch Loss: 0.5180459022521973\n",
      "Epoch 1231, Loss: 0.9546141475439072, Final Batch Loss: 0.23672811686992645\n",
      "Epoch 1232, Loss: 1.0774385929107666, Final Batch Loss: 0.4077102243900299\n",
      "Epoch 1233, Loss: 0.977293461561203, Final Batch Loss: 0.2845265865325928\n",
      "Epoch 1234, Loss: 1.1088187992572784, Final Batch Loss: 0.39601489901542664\n",
      "Epoch 1235, Loss: 0.9841296374797821, Final Batch Loss: 0.33737996220588684\n",
      "Epoch 1236, Loss: 1.0588818490505219, Final Batch Loss: 0.3575575649738312\n",
      "Epoch 1237, Loss: 0.9950911998748779, Final Batch Loss: 0.32195407152175903\n",
      "Epoch 1238, Loss: 1.114955484867096, Final Batch Loss: 0.4621284008026123\n",
      "Epoch 1239, Loss: 1.092130959033966, Final Batch Loss: 0.37634027004241943\n",
      "Epoch 1240, Loss: 0.8940100818872452, Final Batch Loss: 0.21253101527690887\n",
      "Epoch 1241, Loss: 1.0080889463424683, Final Batch Loss: 0.33891960978507996\n",
      "Epoch 1242, Loss: 1.026152104139328, Final Batch Loss: 0.3413313627243042\n",
      "Epoch 1243, Loss: 1.0853597521781921, Final Batch Loss: 0.410835862159729\n",
      "Epoch 1244, Loss: 0.9402441382408142, Final Batch Loss: 0.2694411873817444\n",
      "Epoch 1245, Loss: 0.9656534790992737, Final Batch Loss: 0.31716665625572205\n",
      "Epoch 1246, Loss: 1.2543592154979706, Final Batch Loss: 0.5455313920974731\n",
      "Epoch 1247, Loss: 1.134052574634552, Final Batch Loss: 0.4461519718170166\n",
      "Epoch 1248, Loss: 1.1884407997131348, Final Batch Loss: 0.43526002764701843\n",
      "Epoch 1249, Loss: 0.8898634314537048, Final Batch Loss: 0.2317480742931366\n",
      "Epoch 1250, Loss: 1.0144359469413757, Final Batch Loss: 0.30010926723480225\n",
      "Epoch 1251, Loss: 0.9870864450931549, Final Batch Loss: 0.30097731947898865\n",
      "Epoch 1252, Loss: 0.9473802298307419, Final Batch Loss: 0.24654991924762726\n",
      "Epoch 1253, Loss: 0.9371759593486786, Final Batch Loss: 0.2190968096256256\n",
      "Epoch 1254, Loss: 0.974399983882904, Final Batch Loss: 0.29147958755493164\n",
      "Epoch 1255, Loss: 0.9711454212665558, Final Batch Loss: 0.2892893850803375\n",
      "Epoch 1256, Loss: 1.0456636846065521, Final Batch Loss: 0.3401292562484741\n",
      "Epoch 1257, Loss: 1.1358855664730072, Final Batch Loss: 0.4555525481700897\n",
      "Epoch 1258, Loss: 0.988481879234314, Final Batch Loss: 0.32561081647872925\n",
      "Epoch 1259, Loss: 1.0315382182598114, Final Batch Loss: 0.35024523735046387\n",
      "Epoch 1260, Loss: 1.0601623952388763, Final Batch Loss: 0.43587514758110046\n",
      "Epoch 1261, Loss: 0.9221843481063843, Final Batch Loss: 0.19948986172676086\n",
      "Epoch 1262, Loss: 1.02484692633152, Final Batch Loss: 0.24919231235980988\n",
      "Epoch 1263, Loss: 1.093945175409317, Final Batch Loss: 0.37305402755737305\n",
      "Epoch 1264, Loss: 1.0942452251911163, Final Batch Loss: 0.3900125026702881\n",
      "Epoch 1265, Loss: 0.9797944128513336, Final Batch Loss: 0.3029843270778656\n",
      "Epoch 1266, Loss: 0.923381432890892, Final Batch Loss: 0.18552114069461823\n",
      "Epoch 1267, Loss: 0.9316783249378204, Final Batch Loss: 0.2579910457134247\n",
      "Epoch 1268, Loss: 1.0046628713607788, Final Batch Loss: 0.33000922203063965\n",
      "Epoch 1269, Loss: 1.0112248957157135, Final Batch Loss: 0.30658331513404846\n",
      "Epoch 1270, Loss: 0.9060512781143188, Final Batch Loss: 0.22631052136421204\n",
      "Epoch 1271, Loss: 1.0881679058074951, Final Batch Loss: 0.4102143347263336\n",
      "Epoch 1272, Loss: 0.9956059455871582, Final Batch Loss: 0.3423651158809662\n",
      "Epoch 1273, Loss: 0.9282745122909546, Final Batch Loss: 0.27574312686920166\n",
      "Epoch 1274, Loss: 0.9657402038574219, Final Batch Loss: 0.3254193067550659\n",
      "Epoch 1275, Loss: 0.9617302715778351, Final Batch Loss: 0.2974454462528229\n",
      "Epoch 1276, Loss: 1.159433662891388, Final Batch Loss: 0.4589034616947174\n",
      "Epoch 1277, Loss: 1.10627281665802, Final Batch Loss: 0.3778432607650757\n",
      "Epoch 1278, Loss: 1.0814478695392609, Final Batch Loss: 0.4050597846508026\n",
      "Epoch 1279, Loss: 0.9657391905784607, Final Batch Loss: 0.27734214067459106\n",
      "Epoch 1280, Loss: 1.1477462649345398, Final Batch Loss: 0.4564359188079834\n",
      "Epoch 1281, Loss: 0.9997652620077133, Final Batch Loss: 0.23289106786251068\n",
      "Epoch 1282, Loss: 1.1349195539951324, Final Batch Loss: 0.4417062997817993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1283, Loss: 1.0601167976856232, Final Batch Loss: 0.39643043279647827\n",
      "Epoch 1284, Loss: 0.9358494877815247, Final Batch Loss: 0.28449955582618713\n",
      "Epoch 1285, Loss: 1.04165118932724, Final Batch Loss: 0.38691866397857666\n",
      "Epoch 1286, Loss: 0.9694871008396149, Final Batch Loss: 0.2887240946292877\n",
      "Epoch 1287, Loss: 1.0323895812034607, Final Batch Loss: 0.316620796918869\n",
      "Epoch 1288, Loss: 1.1852714121341705, Final Batch Loss: 0.5123227834701538\n",
      "Epoch 1289, Loss: 1.0513583421707153, Final Batch Loss: 0.3473310172557831\n",
      "Epoch 1290, Loss: 1.1059152781963348, Final Batch Loss: 0.5001724362373352\n",
      "Epoch 1291, Loss: 0.9507034122943878, Final Batch Loss: 0.2797303795814514\n",
      "Epoch 1292, Loss: 1.1293001472949982, Final Batch Loss: 0.4432580769062042\n",
      "Epoch 1293, Loss: 1.032101333141327, Final Batch Loss: 0.3135850429534912\n",
      "Epoch 1294, Loss: 1.0528053641319275, Final Batch Loss: 0.3549133539199829\n",
      "Epoch 1295, Loss: 0.9504267275333405, Final Batch Loss: 0.3047363758087158\n",
      "Epoch 1296, Loss: 1.0153931379318237, Final Batch Loss: 0.3180184066295624\n",
      "Epoch 1297, Loss: 1.084174394607544, Final Batch Loss: 0.35392680764198303\n",
      "Epoch 1298, Loss: 1.0278267562389374, Final Batch Loss: 0.38372570276260376\n",
      "Epoch 1299, Loss: 1.113420695066452, Final Batch Loss: 0.40269649028778076\n",
      "Epoch 1300, Loss: 0.9211118519306183, Final Batch Loss: 0.28293007612228394\n",
      "Epoch 1301, Loss: 0.9302153885364532, Final Batch Loss: 0.31052669882774353\n",
      "Epoch 1302, Loss: 0.8877793252468109, Final Batch Loss: 0.2651914358139038\n",
      "Epoch 1303, Loss: 1.0457429587841034, Final Batch Loss: 0.3850660026073456\n",
      "Epoch 1304, Loss: 0.9914912283420563, Final Batch Loss: 0.31047868728637695\n",
      "Epoch 1305, Loss: 1.0715346038341522, Final Batch Loss: 0.4320107400417328\n",
      "Epoch 1306, Loss: 0.8664743453264236, Final Batch Loss: 0.20174024999141693\n",
      "Epoch 1307, Loss: 0.9260596483945847, Final Batch Loss: 0.2263064831495285\n",
      "Epoch 1308, Loss: 0.861474484205246, Final Batch Loss: 0.17332223057746887\n",
      "Epoch 1309, Loss: 0.9394663572311401, Final Batch Loss: 0.2589850425720215\n",
      "Epoch 1310, Loss: 0.9303570091724396, Final Batch Loss: 0.3082820475101471\n",
      "Epoch 1311, Loss: 0.9440255463123322, Final Batch Loss: 0.2880067527294159\n",
      "Epoch 1312, Loss: 0.9715200066566467, Final Batch Loss: 0.3446606993675232\n",
      "Epoch 1313, Loss: 0.8765586167573929, Final Batch Loss: 0.2219115048646927\n",
      "Epoch 1314, Loss: 0.9122506082057953, Final Batch Loss: 0.27921685576438904\n",
      "Epoch 1315, Loss: 0.9755478799343109, Final Batch Loss: 0.3025737404823303\n",
      "Epoch 1316, Loss: 0.9906765818595886, Final Batch Loss: 0.31869128346443176\n",
      "Epoch 1317, Loss: 0.9724438488483429, Final Batch Loss: 0.2877305746078491\n",
      "Epoch 1318, Loss: 1.0598115772008896, Final Batch Loss: 0.24225236475467682\n",
      "Epoch 1319, Loss: 0.9227210879325867, Final Batch Loss: 0.22503510117530823\n",
      "Epoch 1320, Loss: 0.8964125216007233, Final Batch Loss: 0.27689868211746216\n",
      "Epoch 1321, Loss: 1.1459333300590515, Final Batch Loss: 0.5053232908248901\n",
      "Epoch 1322, Loss: 0.999079555273056, Final Batch Loss: 0.32646262645721436\n",
      "Epoch 1323, Loss: 0.9537511765956879, Final Batch Loss: 0.2868973910808563\n",
      "Epoch 1324, Loss: 0.8778945356607437, Final Batch Loss: 0.22408650815486908\n",
      "Epoch 1325, Loss: 0.9699607491493225, Final Batch Loss: 0.2869081199169159\n",
      "Epoch 1326, Loss: 1.0258922278881073, Final Batch Loss: 0.38338473439216614\n",
      "Epoch 1327, Loss: 0.9146463871002197, Final Batch Loss: 0.2862972319126129\n",
      "Epoch 1328, Loss: 1.0295383632183075, Final Batch Loss: 0.30232658982276917\n",
      "Epoch 1329, Loss: 1.0808843076229095, Final Batch Loss: 0.4406091868877411\n",
      "Epoch 1330, Loss: 1.0842461585998535, Final Batch Loss: 0.3656604588031769\n",
      "Epoch 1331, Loss: 1.0253036320209503, Final Batch Loss: 0.3889119327068329\n",
      "Epoch 1332, Loss: 1.0255886912345886, Final Batch Loss: 0.38848015666007996\n",
      "Epoch 1333, Loss: 0.8554167002439499, Final Batch Loss: 0.22168146073818207\n",
      "Epoch 1334, Loss: 0.9486567080020905, Final Batch Loss: 0.24630290269851685\n",
      "Epoch 1335, Loss: 0.9233697056770325, Final Batch Loss: 0.2858833968639374\n",
      "Epoch 1336, Loss: 1.1631768345832825, Final Batch Loss: 0.4770076870918274\n",
      "Epoch 1337, Loss: 1.003057837486267, Final Batch Loss: 0.3069343864917755\n",
      "Epoch 1338, Loss: 1.008300244808197, Final Batch Loss: 0.3614630103111267\n",
      "Epoch 1339, Loss: 1.017486035823822, Final Batch Loss: 0.36290088295936584\n",
      "Epoch 1340, Loss: 1.0117314755916595, Final Batch Loss: 0.39707300066947937\n",
      "Epoch 1341, Loss: 0.9393251240253448, Final Batch Loss: 0.29361969232559204\n",
      "Epoch 1342, Loss: 1.0447727143764496, Final Batch Loss: 0.4006294012069702\n",
      "Epoch 1343, Loss: 0.8972036242485046, Final Batch Loss: 0.27534735202789307\n",
      "Epoch 1344, Loss: 0.9902184009552002, Final Batch Loss: 0.2825840711593628\n",
      "Epoch 1345, Loss: 0.9603943526744843, Final Batch Loss: 0.2551501989364624\n",
      "Epoch 1346, Loss: 1.034709870815277, Final Batch Loss: 0.40695178508758545\n",
      "Epoch 1347, Loss: 0.9695567190647125, Final Batch Loss: 0.37437671422958374\n",
      "Epoch 1348, Loss: 0.8860770165920258, Final Batch Loss: 0.237443745136261\n",
      "Epoch 1349, Loss: 0.8819755613803864, Final Batch Loss: 0.26140096783638\n",
      "Epoch 1350, Loss: 0.9693088829517365, Final Batch Loss: 0.31855371594429016\n",
      "Epoch 1351, Loss: 0.9240742921829224, Final Batch Loss: 0.2704513370990753\n",
      "Epoch 1352, Loss: 0.9032033234834671, Final Batch Loss: 0.21581698954105377\n",
      "Epoch 1353, Loss: 1.1214700639247894, Final Batch Loss: 0.4474865794181824\n",
      "Epoch 1354, Loss: 0.9474837779998779, Final Batch Loss: 0.29803934693336487\n",
      "Epoch 1355, Loss: 1.0883478820323944, Final Batch Loss: 0.41146230697631836\n",
      "Epoch 1356, Loss: 0.9834463000297546, Final Batch Loss: 0.2816798985004425\n",
      "Epoch 1357, Loss: 0.916591227054596, Final Batch Loss: 0.2947634756565094\n",
      "Epoch 1358, Loss: 0.9158913791179657, Final Batch Loss: 0.28679031133651733\n",
      "Epoch 1359, Loss: 0.9453716576099396, Final Batch Loss: 0.2732390761375427\n",
      "Epoch 1360, Loss: 0.8876843750476837, Final Batch Loss: 0.21010756492614746\n",
      "Epoch 1361, Loss: 0.9650758504867554, Final Batch Loss: 0.32718291878700256\n",
      "Epoch 1362, Loss: 1.0123654305934906, Final Batch Loss: 0.37928304076194763\n",
      "Epoch 1363, Loss: 1.0079137682914734, Final Batch Loss: 0.3897535800933838\n",
      "Epoch 1364, Loss: 0.9103581756353378, Final Batch Loss: 0.21906234323978424\n",
      "Epoch 1365, Loss: 0.9719187915325165, Final Batch Loss: 0.31608331203460693\n",
      "Epoch 1366, Loss: 0.9006975293159485, Final Batch Loss: 0.2760728597640991\n",
      "Epoch 1367, Loss: 1.0953090190887451, Final Batch Loss: 0.38154053688049316\n",
      "Epoch 1368, Loss: 0.8613452613353729, Final Batch Loss: 0.2033069133758545\n",
      "Epoch 1369, Loss: 1.0722917020320892, Final Batch Loss: 0.36279186606407166\n",
      "Epoch 1370, Loss: 0.9987954199314117, Final Batch Loss: 0.3626401424407959\n",
      "Epoch 1371, Loss: 0.9220277965068817, Final Batch Loss: 0.25047487020492554\n",
      "Epoch 1372, Loss: 0.9306674897670746, Final Batch Loss: 0.27146413922309875\n",
      "Epoch 1373, Loss: 1.1518016457557678, Final Batch Loss: 0.4726257622241974\n",
      "Epoch 1374, Loss: 1.1626823842525482, Final Batch Loss: 0.5341311693191528\n",
      "Epoch 1375, Loss: 0.9690006971359253, Final Batch Loss: 0.32034578919410706\n",
      "Epoch 1376, Loss: 0.9993462264537811, Final Batch Loss: 0.346546471118927\n",
      "Epoch 1377, Loss: 0.9708621203899384, Final Batch Loss: 0.30962660908699036\n",
      "Epoch 1378, Loss: 0.9786767661571503, Final Batch Loss: 0.3796570897102356\n",
      "Epoch 1379, Loss: 0.9942451119422913, Final Batch Loss: 0.3818660378456116\n",
      "Epoch 1380, Loss: 1.13721764087677, Final Batch Loss: 0.4861985146999359\n",
      "Epoch 1381, Loss: 0.9557168483734131, Final Batch Loss: 0.3290937840938568\n",
      "Epoch 1382, Loss: 0.9121650457382202, Final Batch Loss: 0.3036273121833801\n",
      "Epoch 1383, Loss: 0.9180413782596588, Final Batch Loss: 0.2829846441745758\n",
      "Epoch 1384, Loss: 1.008585810661316, Final Batch Loss: 0.34620338678359985\n",
      "Epoch 1385, Loss: 1.0410340130329132, Final Batch Loss: 0.42881885170936584\n",
      "Epoch 1386, Loss: 1.0274215638637543, Final Batch Loss: 0.3505480885505676\n",
      "Epoch 1387, Loss: 0.9884064495563507, Final Batch Loss: 0.3470516502857208\n",
      "Epoch 1388, Loss: 1.0335842370986938, Final Batch Loss: 0.4094166159629822\n",
      "Epoch 1389, Loss: 0.9613426923751831, Final Batch Loss: 0.34977293014526367\n",
      "Epoch 1390, Loss: 0.9243846535682678, Final Batch Loss: 0.2677133083343506\n",
      "Epoch 1391, Loss: 0.8403855115175247, Final Batch Loss: 0.20612819492816925\n",
      "Epoch 1392, Loss: 0.8327400088310242, Final Batch Loss: 0.1867324709892273\n",
      "Epoch 1393, Loss: 1.0263906717300415, Final Batch Loss: 0.36335065960884094\n",
      "Epoch 1394, Loss: 0.9934341311454773, Final Batch Loss: 0.3536073863506317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1395, Loss: 0.9067109972238541, Final Batch Loss: 0.20410917699337006\n",
      "Epoch 1396, Loss: 1.0648239850997925, Final Batch Loss: 0.4381382465362549\n",
      "Epoch 1397, Loss: 1.017362117767334, Final Batch Loss: 0.3592541217803955\n",
      "Epoch 1398, Loss: 0.8320303112268448, Final Batch Loss: 0.20099513232707977\n",
      "Epoch 1399, Loss: 1.0802457928657532, Final Batch Loss: 0.43996313214302063\n",
      "Epoch 1400, Loss: 1.0042479634284973, Final Batch Loss: 0.32221394777297974\n",
      "Epoch 1401, Loss: 0.8907184600830078, Final Batch Loss: 0.2876509130001068\n",
      "Epoch 1402, Loss: 0.9861136376857758, Final Batch Loss: 0.3401651680469513\n",
      "Epoch 1403, Loss: 0.914589136838913, Final Batch Loss: 0.26542147994041443\n",
      "Epoch 1404, Loss: 0.8435730338096619, Final Batch Loss: 0.23394617438316345\n",
      "Epoch 1405, Loss: 0.922384113073349, Final Batch Loss: 0.25157153606414795\n",
      "Epoch 1406, Loss: 0.9915272295475006, Final Batch Loss: 0.33254140615463257\n",
      "Epoch 1407, Loss: 0.8749279975891113, Final Batch Loss: 0.2122102677822113\n",
      "Epoch 1408, Loss: 0.8569146543741226, Final Batch Loss: 0.19849975407123566\n",
      "Epoch 1409, Loss: 0.8858419209718704, Final Batch Loss: 0.22371210157871246\n",
      "Epoch 1410, Loss: 0.9844225645065308, Final Batch Loss: 0.25150787830352783\n",
      "Epoch 1411, Loss: 1.0288199186325073, Final Batch Loss: 0.3790837526321411\n",
      "Epoch 1412, Loss: 0.8245824426412582, Final Batch Loss: 0.1498718112707138\n",
      "Epoch 1413, Loss: 0.9104006588459015, Final Batch Loss: 0.26912373304367065\n",
      "Epoch 1414, Loss: 0.9686471223831177, Final Batch Loss: 0.35874149203300476\n",
      "Epoch 1415, Loss: 1.068347454071045, Final Batch Loss: 0.4207291007041931\n",
      "Epoch 1416, Loss: 1.1105859875679016, Final Batch Loss: 0.39520514011383057\n",
      "Epoch 1417, Loss: 1.0264529585838318, Final Batch Loss: 0.38041967153549194\n",
      "Epoch 1418, Loss: 0.9106950759887695, Final Batch Loss: 0.2988458573818207\n",
      "Epoch 1419, Loss: 0.891532689332962, Final Batch Loss: 0.2722533941268921\n",
      "Epoch 1420, Loss: 0.9534037411212921, Final Batch Loss: 0.32435280084609985\n",
      "Epoch 1421, Loss: 1.0112940967082977, Final Batch Loss: 0.3461526036262512\n",
      "Epoch 1422, Loss: 0.8791392743587494, Final Batch Loss: 0.24991530179977417\n",
      "Epoch 1423, Loss: 0.9052921831607819, Final Batch Loss: 0.31013569235801697\n",
      "Epoch 1424, Loss: 0.9683596789836884, Final Batch Loss: 0.36860746145248413\n",
      "Epoch 1425, Loss: 0.8389414846897125, Final Batch Loss: 0.22354194521903992\n",
      "Epoch 1426, Loss: 0.8849210441112518, Final Batch Loss: 0.2740541398525238\n",
      "Epoch 1427, Loss: 0.8356882333755493, Final Batch Loss: 0.22574299573898315\n",
      "Epoch 1428, Loss: 0.9220645725727081, Final Batch Loss: 0.28087493777275085\n",
      "Epoch 1429, Loss: 0.9324010610580444, Final Batch Loss: 0.3430434763431549\n",
      "Epoch 1430, Loss: 1.042774260044098, Final Batch Loss: 0.44333168864250183\n",
      "Epoch 1431, Loss: 0.9687089622020721, Final Batch Loss: 0.3491761088371277\n",
      "Epoch 1432, Loss: 0.8338845372200012, Final Batch Loss: 0.22242692112922668\n",
      "Epoch 1433, Loss: 0.8977050185203552, Final Batch Loss: 0.27965131402015686\n",
      "Epoch 1434, Loss: 0.8033289015293121, Final Batch Loss: 0.24858251214027405\n",
      "Epoch 1435, Loss: 1.0154930651187897, Final Batch Loss: 0.41725295782089233\n",
      "Epoch 1436, Loss: 0.9540434181690216, Final Batch Loss: 0.3308987617492676\n",
      "Epoch 1437, Loss: 0.9706858098506927, Final Batch Loss: 0.32198581099510193\n",
      "Epoch 1438, Loss: 0.9169181287288666, Final Batch Loss: 0.254940927028656\n",
      "Epoch 1439, Loss: 0.9129635095596313, Final Batch Loss: 0.3052787184715271\n",
      "Epoch 1440, Loss: 0.9785739779472351, Final Batch Loss: 0.353487104177475\n",
      "Epoch 1441, Loss: 0.8840270638465881, Final Batch Loss: 0.2751842737197876\n",
      "Epoch 1442, Loss: 1.0390277802944183, Final Batch Loss: 0.3177321255207062\n",
      "Epoch 1443, Loss: 0.901849552989006, Final Batch Loss: 0.2429378479719162\n",
      "Epoch 1444, Loss: 0.9130748510360718, Final Batch Loss: 0.3090987205505371\n",
      "Epoch 1445, Loss: 0.842402845621109, Final Batch Loss: 0.20784395933151245\n",
      "Epoch 1446, Loss: 0.9646254777908325, Final Batch Loss: 0.35396313667297363\n",
      "Epoch 1447, Loss: 1.0183830261230469, Final Batch Loss: 0.4163098633289337\n",
      "Epoch 1448, Loss: 0.9151443541049957, Final Batch Loss: 0.30976101756095886\n",
      "Epoch 1449, Loss: 0.9189490675926208, Final Batch Loss: 0.3256968557834625\n",
      "Epoch 1450, Loss: 0.8853580057621002, Final Batch Loss: 0.22679603099822998\n",
      "Epoch 1451, Loss: 0.9402357637882233, Final Batch Loss: 0.27193617820739746\n",
      "Epoch 1452, Loss: 0.9657692611217499, Final Batch Loss: 0.3796030282974243\n",
      "Epoch 1453, Loss: 1.0946256816387177, Final Batch Loss: 0.4711649715900421\n",
      "Epoch 1454, Loss: 0.9439814984798431, Final Batch Loss: 0.35163506865501404\n",
      "Epoch 1455, Loss: 0.8003350794315338, Final Batch Loss: 0.18092918395996094\n",
      "Epoch 1456, Loss: 0.9599525928497314, Final Batch Loss: 0.2675504982471466\n",
      "Epoch 1457, Loss: 1.0073231160640717, Final Batch Loss: 0.3338454067707062\n",
      "Epoch 1458, Loss: 0.8775747716426849, Final Batch Loss: 0.20588010549545288\n",
      "Epoch 1459, Loss: 0.8802438825368881, Final Batch Loss: 0.23992474377155304\n",
      "Epoch 1460, Loss: 0.9411486983299255, Final Batch Loss: 0.27208787202835083\n",
      "Epoch 1461, Loss: 0.9332602024078369, Final Batch Loss: 0.28417983651161194\n",
      "Epoch 1462, Loss: 0.9100603461265564, Final Batch Loss: 0.35488343238830566\n",
      "Epoch 1463, Loss: 0.9655915796756744, Final Batch Loss: 0.3633292317390442\n",
      "Epoch 1464, Loss: 0.9444165229797363, Final Batch Loss: 0.3037228286266327\n",
      "Epoch 1465, Loss: 0.8720948398113251, Final Batch Loss: 0.24162226915359497\n",
      "Epoch 1466, Loss: 0.9323904812335968, Final Batch Loss: 0.3223518133163452\n",
      "Epoch 1467, Loss: 1.0232732892036438, Final Batch Loss: 0.34437623620033264\n",
      "Epoch 1468, Loss: 1.0883878767490387, Final Batch Loss: 0.4423660337924957\n",
      "Epoch 1469, Loss: 1.0883823931217194, Final Batch Loss: 0.4780786335468292\n",
      "Epoch 1470, Loss: 0.9614063799381256, Final Batch Loss: 0.3803652822971344\n",
      "Epoch 1471, Loss: 1.0564488470554352, Final Batch Loss: 0.4409000873565674\n",
      "Epoch 1472, Loss: 0.8682051599025726, Final Batch Loss: 0.20045393705368042\n",
      "Epoch 1473, Loss: 0.8587066233158112, Final Batch Loss: 0.22886866331100464\n",
      "Epoch 1474, Loss: 0.8551662713289261, Final Batch Loss: 0.22924719750881195\n",
      "Epoch 1475, Loss: 1.0318662524223328, Final Batch Loss: 0.43935278058052063\n",
      "Epoch 1476, Loss: 0.7736393064260483, Final Batch Loss: 0.15625758469104767\n",
      "Epoch 1477, Loss: 0.9886458516120911, Final Batch Loss: 0.360988587141037\n",
      "Epoch 1478, Loss: 0.9501829743385315, Final Batch Loss: 0.3444664180278778\n",
      "Epoch 1479, Loss: 0.9979232251644135, Final Batch Loss: 0.42450395226478577\n",
      "Epoch 1480, Loss: 0.9508747458457947, Final Batch Loss: 0.37359708547592163\n",
      "Epoch 1481, Loss: 0.8793061077594757, Final Batch Loss: 0.21057447791099548\n",
      "Epoch 1482, Loss: 0.9249963462352753, Final Batch Loss: 0.34764260053634644\n",
      "Epoch 1483, Loss: 0.8912477493286133, Final Batch Loss: 0.29051122069358826\n",
      "Epoch 1484, Loss: 0.9179372787475586, Final Batch Loss: 0.31909245252609253\n",
      "Epoch 1485, Loss: 0.8176665753126144, Final Batch Loss: 0.2341051548719406\n",
      "Epoch 1486, Loss: 0.8307546973228455, Final Batch Loss: 0.23206862807273865\n",
      "Epoch 1487, Loss: 0.8340487778186798, Final Batch Loss: 0.24873244762420654\n",
      "Epoch 1488, Loss: 1.0053561329841614, Final Batch Loss: 0.4188321530818939\n",
      "Epoch 1489, Loss: 0.8554258942604065, Final Batch Loss: 0.2943565547466278\n",
      "Epoch 1490, Loss: 0.8482714891433716, Final Batch Loss: 0.28274017572402954\n",
      "Epoch 1491, Loss: 0.9363884627819061, Final Batch Loss: 0.37548044323921204\n",
      "Epoch 1492, Loss: 0.9255385994911194, Final Batch Loss: 0.33962762355804443\n",
      "Epoch 1493, Loss: 0.8826370537281036, Final Batch Loss: 0.30397936701774597\n",
      "Epoch 1494, Loss: 0.8700702488422394, Final Batch Loss: 0.30394765734672546\n",
      "Epoch 1495, Loss: 0.8402210921049118, Final Batch Loss: 0.22714583575725555\n",
      "Epoch 1496, Loss: 0.9826153516769409, Final Batch Loss: 0.358010470867157\n",
      "Epoch 1497, Loss: 0.9957964718341827, Final Batch Loss: 0.34475961327552795\n",
      "Epoch 1498, Loss: 0.7716001868247986, Final Batch Loss: 0.1851109266281128\n",
      "Epoch 1499, Loss: 0.7886503487825394, Final Batch Loss: 0.22476519644260406\n",
      "Epoch 1500, Loss: 0.9469588398933411, Final Batch Loss: 0.35503867268562317\n",
      "Epoch 1501, Loss: 0.9512815177440643, Final Batch Loss: 0.31947869062423706\n",
      "Epoch 1502, Loss: 0.8119135648012161, Final Batch Loss: 0.1926431506872177\n",
      "Epoch 1503, Loss: 0.948555201292038, Final Batch Loss: 0.25336194038391113\n",
      "Epoch 1504, Loss: 0.8936235308647156, Final Batch Loss: 0.25508177280426025\n",
      "Epoch 1505, Loss: 0.9393492341041565, Final Batch Loss: 0.3545328676700592\n",
      "Epoch 1506, Loss: 0.8537018001079559, Final Batch Loss: 0.23925194144248962\n",
      "Epoch 1507, Loss: 0.7705997377634048, Final Batch Loss: 0.23247872292995453\n",
      "Epoch 1508, Loss: 0.8757084906101227, Final Batch Loss: 0.278400182723999\n",
      "Epoch 1509, Loss: 0.868830144405365, Final Batch Loss: 0.2622343599796295\n",
      "Epoch 1510, Loss: 0.7821917980909348, Final Batch Loss: 0.26889297366142273\n",
      "Epoch 1511, Loss: 0.8492203652858734, Final Batch Loss: 0.295123815536499\n",
      "Epoch 1512, Loss: 0.8384959101676941, Final Batch Loss: 0.31783798336982727\n",
      "Epoch 1513, Loss: 0.7835699915885925, Final Batch Loss: 0.24887612462043762\n",
      "Epoch 1514, Loss: 0.9600480794906616, Final Batch Loss: 0.36950433254241943\n",
      "Epoch 1515, Loss: 0.9250148832798004, Final Batch Loss: 0.3690352439880371\n",
      "Epoch 1516, Loss: 0.9277346730232239, Final Batch Loss: 0.3669162392616272\n",
      "Epoch 1517, Loss: 0.9152597188949585, Final Batch Loss: 0.36106693744659424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1518, Loss: 0.7514544129371643, Final Batch Loss: 0.15025067329406738\n",
      "Epoch 1519, Loss: 0.8019938766956329, Final Batch Loss: 0.25743094086647034\n",
      "Epoch 1520, Loss: 0.7925505340099335, Final Batch Loss: 0.24500387907028198\n",
      "Epoch 1521, Loss: 0.8201147019863129, Final Batch Loss: 0.28682297468185425\n",
      "Epoch 1522, Loss: 0.8868459761142731, Final Batch Loss: 0.30647802352905273\n",
      "Epoch 1523, Loss: 0.8226804137229919, Final Batch Loss: 0.2619021236896515\n",
      "Epoch 1524, Loss: 0.8679344654083252, Final Batch Loss: 0.329387903213501\n",
      "Epoch 1525, Loss: 0.8642193973064423, Final Batch Loss: 0.26927128434181213\n",
      "Epoch 1526, Loss: 0.737969234585762, Final Batch Loss: 0.22519409656524658\n",
      "Epoch 1527, Loss: 0.8888778686523438, Final Batch Loss: 0.28282564878463745\n",
      "Epoch 1528, Loss: 0.8561864793300629, Final Batch Loss: 0.33463597297668457\n",
      "Epoch 1529, Loss: 0.8489858657121658, Final Batch Loss: 0.15104977786540985\n",
      "Epoch 1530, Loss: 0.7845246493816376, Final Batch Loss: 0.23879173398017883\n",
      "Epoch 1531, Loss: 0.7913190722465515, Final Batch Loss: 0.19125589728355408\n",
      "Epoch 1532, Loss: 0.8204584717750549, Final Batch Loss: 0.26165902614593506\n",
      "Epoch 1533, Loss: 0.8052332401275635, Final Batch Loss: 0.1717081069946289\n",
      "Epoch 1534, Loss: 0.9246192872524261, Final Batch Loss: 0.42829814553260803\n",
      "Epoch 1535, Loss: 1.018082171678543, Final Batch Loss: 0.41397625207901\n",
      "Epoch 1536, Loss: 0.799762487411499, Final Batch Loss: 0.23628482222557068\n",
      "Epoch 1537, Loss: 0.763854131102562, Final Batch Loss: 0.2439151257276535\n",
      "Epoch 1538, Loss: 0.877222865819931, Final Batch Loss: 0.2803576588630676\n",
      "Epoch 1539, Loss: 0.8827224969863892, Final Batch Loss: 0.30486664175987244\n",
      "Epoch 1540, Loss: 0.7216152995824814, Final Batch Loss: 0.1404397040605545\n",
      "Epoch 1541, Loss: 0.7240324765443802, Final Batch Loss: 0.15858785808086395\n",
      "Epoch 1542, Loss: 0.9359330236911774, Final Batch Loss: 0.4050573706626892\n",
      "Epoch 1543, Loss: 0.893294170498848, Final Batch Loss: 0.3703615665435791\n",
      "Epoch 1544, Loss: 0.7270158231258392, Final Batch Loss: 0.18864655494689941\n",
      "Epoch 1545, Loss: 0.6674351841211319, Final Batch Loss: 0.12318746745586395\n",
      "Epoch 1546, Loss: 0.8647512495517731, Final Batch Loss: 0.27271920442581177\n",
      "Epoch 1547, Loss: 0.7688242495059967, Final Batch Loss: 0.21316125988960266\n",
      "Epoch 1548, Loss: 0.6945143267512321, Final Batch Loss: 0.12278630584478378\n",
      "Epoch 1549, Loss: 0.7745087295770645, Final Batch Loss: 0.22724010050296783\n",
      "Epoch 1550, Loss: 0.7932514101266861, Final Batch Loss: 0.24313253164291382\n",
      "Epoch 1551, Loss: 0.7407016158103943, Final Batch Loss: 0.20467442274093628\n",
      "Epoch 1552, Loss: 0.7955070585012436, Final Batch Loss: 0.26486751437187195\n",
      "Epoch 1553, Loss: 0.8814392983913422, Final Batch Loss: 0.3869500160217285\n",
      "Epoch 1554, Loss: 0.7691039890050888, Final Batch Loss: 0.20148499310016632\n",
      "Epoch 1555, Loss: 0.8359692245721817, Final Batch Loss: 0.2926550805568695\n",
      "Epoch 1556, Loss: 0.7324628084897995, Final Batch Loss: 0.23784294724464417\n",
      "Epoch 1557, Loss: 0.8653073608875275, Final Batch Loss: 0.39270004630088806\n",
      "Epoch 1558, Loss: 0.8375938534736633, Final Batch Loss: 0.3372863829135895\n",
      "Epoch 1559, Loss: 0.9185331165790558, Final Batch Loss: 0.38610100746154785\n",
      "Epoch 1560, Loss: 0.7929395437240601, Final Batch Loss: 0.2573825716972351\n",
      "Epoch 1561, Loss: 0.716675728559494, Final Batch Loss: 0.17530220746994019\n",
      "Epoch 1562, Loss: 0.8814586997032166, Final Batch Loss: 0.2740698754787445\n",
      "Epoch 1563, Loss: 0.79885134100914, Final Batch Loss: 0.2668381631374359\n",
      "Epoch 1564, Loss: 0.7415821552276611, Final Batch Loss: 0.16135919094085693\n",
      "Epoch 1565, Loss: 0.7573620080947876, Final Batch Loss: 0.25041651725769043\n",
      "Epoch 1566, Loss: 0.7915551662445068, Final Batch Loss: 0.21864375472068787\n",
      "Epoch 1567, Loss: 0.8296850025653839, Final Batch Loss: 0.3095313608646393\n",
      "Epoch 1568, Loss: 0.8402813673019409, Final Batch Loss: 0.33807966113090515\n",
      "Epoch 1569, Loss: 0.7752363532781601, Final Batch Loss: 0.19972999393939972\n",
      "Epoch 1570, Loss: 0.8698742985725403, Final Batch Loss: 0.2911108434200287\n",
      "Epoch 1571, Loss: 0.7156548500061035, Final Batch Loss: 0.15978610515594482\n",
      "Epoch 1572, Loss: 0.8098888099193573, Final Batch Loss: 0.23659349977970123\n",
      "Epoch 1573, Loss: 0.8779220581054688, Final Batch Loss: 0.29358887672424316\n",
      "Epoch 1574, Loss: 0.6934953331947327, Final Batch Loss: 0.20046751201152802\n",
      "Epoch 1575, Loss: 0.7834376096725464, Final Batch Loss: 0.264682412147522\n",
      "Epoch 1576, Loss: 0.838993638753891, Final Batch Loss: 0.2800789177417755\n",
      "Epoch 1577, Loss: 0.9918034374713898, Final Batch Loss: 0.48256221413612366\n",
      "Epoch 1578, Loss: 0.6806578040122986, Final Batch Loss: 0.17308960855007172\n",
      "Epoch 1579, Loss: 0.693250760436058, Final Batch Loss: 0.22028982639312744\n",
      "Epoch 1580, Loss: 0.7116985768079758, Final Batch Loss: 0.18964214622974396\n",
      "Epoch 1581, Loss: 0.873378187417984, Final Batch Loss: 0.3244859576225281\n",
      "Epoch 1582, Loss: 0.8141152262687683, Final Batch Loss: 0.2795972526073456\n",
      "Epoch 1583, Loss: 0.9703653752803802, Final Batch Loss: 0.37276220321655273\n",
      "Epoch 1584, Loss: 0.9534139037132263, Final Batch Loss: 0.38208121061325073\n",
      "Epoch 1585, Loss: 0.7627344131469727, Final Batch Loss: 0.26372572779655457\n",
      "Epoch 1586, Loss: 0.8029491454362869, Final Batch Loss: 0.2414230853319168\n",
      "Epoch 1587, Loss: 0.8502429127693176, Final Batch Loss: 0.2671176791191101\n",
      "Epoch 1588, Loss: 0.7339338809251785, Final Batch Loss: 0.26680558919906616\n",
      "Epoch 1589, Loss: 0.7691887617111206, Final Batch Loss: 0.27696889638900757\n",
      "Epoch 1590, Loss: 0.7455233484506607, Final Batch Loss: 0.27182719111442566\n",
      "Epoch 1591, Loss: 0.6583221852779388, Final Batch Loss: 0.18154393136501312\n",
      "Epoch 1592, Loss: 0.652040883898735, Final Batch Loss: 0.17433929443359375\n",
      "Epoch 1593, Loss: 0.7447143197059631, Final Batch Loss: 0.26381057500839233\n",
      "Epoch 1594, Loss: 0.8498310148715973, Final Batch Loss: 0.18962401151657104\n",
      "Epoch 1595, Loss: 0.7629178762435913, Final Batch Loss: 0.21360528469085693\n",
      "Epoch 1596, Loss: 0.8360275477170944, Final Batch Loss: 0.3129252791404724\n",
      "Epoch 1597, Loss: 0.6761107891798019, Final Batch Loss: 0.17398177087306976\n",
      "Epoch 1598, Loss: 0.6720652133226395, Final Batch Loss: 0.2201789766550064\n",
      "Epoch 1599, Loss: 0.7103404700756073, Final Batch Loss: 0.21894307434558868\n",
      "Epoch 1600, Loss: 0.7884727716445923, Final Batch Loss: 0.2579820156097412\n",
      "Epoch 1601, Loss: 0.8636204302310944, Final Batch Loss: 0.3526526093482971\n",
      "Epoch 1602, Loss: 0.7245319485664368, Final Batch Loss: 0.21140557527542114\n",
      "Epoch 1603, Loss: 0.7202019989490509, Final Batch Loss: 0.25923627614974976\n",
      "Epoch 1604, Loss: 0.6848619878292084, Final Batch Loss: 0.14495006203651428\n",
      "Epoch 1605, Loss: 0.6937086135149002, Final Batch Loss: 0.19483520090579987\n",
      "Epoch 1606, Loss: 0.7853263020515442, Final Batch Loss: 0.2931768596172333\n",
      "Epoch 1607, Loss: 0.7180639654397964, Final Batch Loss: 0.2807876765727997\n",
      "Epoch 1608, Loss: 0.6706697195768356, Final Batch Loss: 0.19299280643463135\n",
      "Epoch 1609, Loss: 0.7002145498991013, Final Batch Loss: 0.1903592199087143\n",
      "Epoch 1610, Loss: 0.7069747000932693, Final Batch Loss: 0.24427901208400726\n",
      "Epoch 1611, Loss: 0.7707276344299316, Final Batch Loss: 0.257945716381073\n",
      "Epoch 1612, Loss: 0.7730808109045029, Final Batch Loss: 0.2705279290676117\n",
      "Epoch 1613, Loss: 0.7585204392671585, Final Batch Loss: 0.2799440622329712\n",
      "Epoch 1614, Loss: 0.7195829004049301, Final Batch Loss: 0.1910492181777954\n",
      "Epoch 1615, Loss: 0.8566756546497345, Final Batch Loss: 0.3515351414680481\n",
      "Epoch 1616, Loss: 0.7111882418394089, Final Batch Loss: 0.24222412705421448\n",
      "Epoch 1617, Loss: 0.614233672618866, Final Batch Loss: 0.10538685321807861\n",
      "Epoch 1618, Loss: 0.7944838106632233, Final Batch Loss: 0.2863546907901764\n",
      "Epoch 1619, Loss: 0.7265460938215256, Final Batch Loss: 0.272938072681427\n",
      "Epoch 1620, Loss: 0.6771159172058105, Final Batch Loss: 0.20805887877941132\n",
      "Epoch 1621, Loss: 0.8085259795188904, Final Batch Loss: 0.21955233812332153\n",
      "Epoch 1622, Loss: 0.6105745658278465, Final Batch Loss: 0.11614947766065598\n",
      "Epoch 1623, Loss: 0.6879982203245163, Final Batch Loss: 0.15978200733661652\n",
      "Epoch 1624, Loss: 0.7033107578754425, Final Batch Loss: 0.23746344447135925\n",
      "Epoch 1625, Loss: 0.729655534029007, Final Batch Loss: 0.2324981838464737\n",
      "Epoch 1626, Loss: 0.7406080812215805, Final Batch Loss: 0.24813354015350342\n",
      "Epoch 1627, Loss: 0.8048451840877533, Final Batch Loss: 0.3303121328353882\n",
      "Epoch 1628, Loss: 0.672633945941925, Final Batch Loss: 0.2580089867115021\n",
      "Epoch 1629, Loss: 0.6769042313098907, Final Batch Loss: 0.19998078048229218\n",
      "Epoch 1630, Loss: 0.6789973974227905, Final Batch Loss: 0.20461565256118774\n",
      "Epoch 1631, Loss: 0.7757153511047363, Final Batch Loss: 0.227885901927948\n",
      "Epoch 1632, Loss: 0.7272644191980362, Final Batch Loss: 0.2466621845960617\n",
      "Epoch 1633, Loss: 0.7577336877584457, Final Batch Loss: 0.277669221162796\n",
      "Epoch 1634, Loss: 0.7048701643943787, Final Batch Loss: 0.2347639501094818\n",
      "Epoch 1635, Loss: 0.6698532253503799, Final Batch Loss: 0.22740624845027924\n",
      "Epoch 1636, Loss: 0.710064172744751, Final Batch Loss: 0.27453768253326416\n",
      "Epoch 1637, Loss: 0.6859467774629593, Final Batch Loss: 0.20037931203842163\n",
      "Epoch 1638, Loss: 0.7704205065965652, Final Batch Loss: 0.28050369024276733\n",
      "Epoch 1639, Loss: 0.9840061217546463, Final Batch Loss: 0.4474553167819977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1640, Loss: 0.6562048494815826, Final Batch Loss: 0.20422039926052094\n",
      "Epoch 1641, Loss: 0.76901775598526, Final Batch Loss: 0.261991411447525\n",
      "Epoch 1642, Loss: 0.7098180800676346, Final Batch Loss: 0.21455390751361847\n",
      "Epoch 1643, Loss: 0.7843719869852066, Final Batch Loss: 0.27169230580329895\n",
      "Epoch 1644, Loss: 0.7242927551269531, Final Batch Loss: 0.2271905243396759\n",
      "Epoch 1645, Loss: 0.8746046423912048, Final Batch Loss: 0.3507470488548279\n",
      "Epoch 1646, Loss: 0.7265426367521286, Final Batch Loss: 0.16571004688739777\n",
      "Epoch 1647, Loss: 0.7259509265422821, Final Batch Loss: 0.18781977891921997\n",
      "Epoch 1648, Loss: 0.7906856834888458, Final Batch Loss: 0.3082689046859741\n",
      "Epoch 1649, Loss: 0.6772970259189606, Final Batch Loss: 0.19183871150016785\n",
      "Epoch 1650, Loss: 0.905424028635025, Final Batch Loss: 0.37833744287490845\n",
      "Epoch 1651, Loss: 0.7456647902727127, Final Batch Loss: 0.29082512855529785\n",
      "Epoch 1652, Loss: 0.6890336126089096, Final Batch Loss: 0.24799227714538574\n",
      "Epoch 1653, Loss: 0.6966744959354401, Final Batch Loss: 0.28173619508743286\n",
      "Epoch 1654, Loss: 0.8603782206773758, Final Batch Loss: 0.3500474691390991\n",
      "Epoch 1655, Loss: 0.6522148698568344, Final Batch Loss: 0.14954964816570282\n",
      "Epoch 1656, Loss: 0.613429881632328, Final Batch Loss: 0.10864586383104324\n",
      "Epoch 1657, Loss: 0.7322323173284531, Final Batch Loss: 0.2723535895347595\n",
      "Epoch 1658, Loss: 0.65701524913311, Final Batch Loss: 0.13859696686267853\n",
      "Epoch 1659, Loss: 0.7699994891881943, Final Batch Loss: 0.3024579584598541\n",
      "Epoch 1660, Loss: 0.71514992415905, Final Batch Loss: 0.2059386521577835\n",
      "Epoch 1661, Loss: 0.7781906127929688, Final Batch Loss: 0.30974045395851135\n",
      "Epoch 1662, Loss: 0.5870959907770157, Final Batch Loss: 0.09882882237434387\n",
      "Epoch 1663, Loss: 0.6344354599714279, Final Batch Loss: 0.18563829362392426\n",
      "Epoch 1664, Loss: 0.6740519255399704, Final Batch Loss: 0.197795569896698\n",
      "Epoch 1665, Loss: 0.6488135159015656, Final Batch Loss: 0.1773516982793808\n",
      "Epoch 1666, Loss: 0.6364254206418991, Final Batch Loss: 0.225860133767128\n",
      "Epoch 1667, Loss: 0.7896292358636856, Final Batch Loss: 0.36877554655075073\n",
      "Epoch 1668, Loss: 0.7465212792158127, Final Batch Loss: 0.22471684217453003\n",
      "Epoch 1669, Loss: 0.684040293097496, Final Batch Loss: 0.17878617346286774\n",
      "Epoch 1670, Loss: 0.7404362708330154, Final Batch Loss: 0.2765450179576874\n",
      "Epoch 1671, Loss: 0.7586496323347092, Final Batch Loss: 0.30240750312805176\n",
      "Epoch 1672, Loss: 0.6618023216724396, Final Batch Loss: 0.24517321586608887\n",
      "Epoch 1673, Loss: 0.7613249123096466, Final Batch Loss: 0.28034213185310364\n",
      "Epoch 1674, Loss: 0.704600915312767, Final Batch Loss: 0.18283386528491974\n",
      "Epoch 1675, Loss: 0.7268715053796768, Final Batch Loss: 0.3242681920528412\n",
      "Epoch 1676, Loss: 0.5396417677402496, Final Batch Loss: 0.11356388032436371\n",
      "Epoch 1677, Loss: 0.8036341071128845, Final Batch Loss: 0.3370620906352997\n",
      "Epoch 1678, Loss: 0.6325906813144684, Final Batch Loss: 0.17248383164405823\n",
      "Epoch 1679, Loss: 0.67388616502285, Final Batch Loss: 0.22046153247356415\n",
      "Epoch 1680, Loss: 0.618945986032486, Final Batch Loss: 0.17899711430072784\n",
      "Epoch 1681, Loss: 0.5860393792390823, Final Batch Loss: 0.17003294825553894\n",
      "Epoch 1682, Loss: 0.6662226170301437, Final Batch Loss: 0.19319768249988556\n",
      "Epoch 1683, Loss: 0.6069856882095337, Final Batch Loss: 0.22948819398880005\n",
      "Epoch 1684, Loss: 0.7020650506019592, Final Batch Loss: 0.29344362020492554\n",
      "Epoch 1685, Loss: 0.7269418388605118, Final Batch Loss: 0.29853352904319763\n",
      "Epoch 1686, Loss: 0.7632398754358292, Final Batch Loss: 0.27393919229507446\n",
      "Epoch 1687, Loss: 0.8493149280548096, Final Batch Loss: 0.23546040058135986\n",
      "Epoch 1688, Loss: 0.6846775263547897, Final Batch Loss: 0.21447519958019257\n",
      "Epoch 1689, Loss: 0.6318226903676987, Final Batch Loss: 0.15520547330379486\n",
      "Epoch 1690, Loss: 0.6302313804626465, Final Batch Loss: 0.17284323275089264\n",
      "Epoch 1691, Loss: 0.5937488377094269, Final Batch Loss: 0.14493200182914734\n",
      "Epoch 1692, Loss: 0.6002360433340073, Final Batch Loss: 0.1471475511789322\n",
      "Epoch 1693, Loss: 0.6122588217258453, Final Batch Loss: 0.18649840354919434\n",
      "Epoch 1694, Loss: 0.618414580821991, Final Batch Loss: 0.19497042894363403\n",
      "Epoch 1695, Loss: 0.7531579881906509, Final Batch Loss: 0.31396546959877014\n",
      "Epoch 1696, Loss: 0.5810990035533905, Final Batch Loss: 0.15739105641841888\n",
      "Epoch 1697, Loss: 0.6619904041290283, Final Batch Loss: 0.21448220312595367\n",
      "Epoch 1698, Loss: 0.6474415510892868, Final Batch Loss: 0.1930718868970871\n",
      "Epoch 1699, Loss: 0.671658381819725, Final Batch Loss: 0.28839120268821716\n",
      "Epoch 1700, Loss: 0.7805095016956329, Final Batch Loss: 0.3482797145843506\n",
      "Epoch 1701, Loss: 0.6845506280660629, Final Batch Loss: 0.15174312889575958\n",
      "Epoch 1702, Loss: 0.6900649517774582, Final Batch Loss: 0.22578056156635284\n",
      "Epoch 1703, Loss: 0.6665464490652084, Final Batch Loss: 0.2529711127281189\n",
      "Epoch 1704, Loss: 0.6552799046039581, Final Batch Loss: 0.24567268788814545\n",
      "Epoch 1705, Loss: 0.6535279750823975, Final Batch Loss: 0.19449912011623383\n",
      "Epoch 1706, Loss: 0.5831556022167206, Final Batch Loss: 0.0918462872505188\n",
      "Epoch 1707, Loss: 0.7598569691181183, Final Batch Loss: 0.3189954459667206\n",
      "Epoch 1708, Loss: 0.7119425237178802, Final Batch Loss: 0.23811885714530945\n",
      "Epoch 1709, Loss: 0.620091587305069, Final Batch Loss: 0.21028442680835724\n",
      "Epoch 1710, Loss: 0.5831623673439026, Final Batch Loss: 0.18661968410015106\n",
      "Epoch 1711, Loss: 0.8426793813705444, Final Batch Loss: 0.4475173056125641\n",
      "Epoch 1712, Loss: 0.5917861759662628, Final Batch Loss: 0.15807004272937775\n",
      "Epoch 1713, Loss: 0.716291755437851, Final Batch Loss: 0.2973925471305847\n",
      "Epoch 1714, Loss: 0.572397992014885, Final Batch Loss: 0.13295474648475647\n",
      "Epoch 1715, Loss: 0.6347217410802841, Final Batch Loss: 0.21616330742835999\n",
      "Epoch 1716, Loss: 0.6097059398889542, Final Batch Loss: 0.1784614771604538\n",
      "Epoch 1717, Loss: 0.6830199360847473, Final Batch Loss: 0.25300610065460205\n",
      "Epoch 1718, Loss: 0.6718685477972031, Final Batch Loss: 0.24088701605796814\n",
      "Epoch 1719, Loss: 0.694098636507988, Final Batch Loss: 0.23671017587184906\n",
      "Epoch 1720, Loss: 0.6336065530776978, Final Batch Loss: 0.15422452986240387\n",
      "Epoch 1721, Loss: 0.8255061209201813, Final Batch Loss: 0.2811325490474701\n",
      "Epoch 1722, Loss: 0.6701629906892776, Final Batch Loss: 0.18194147944450378\n",
      "Epoch 1723, Loss: 0.6870043277740479, Final Batch Loss: 0.2633900046348572\n",
      "Epoch 1724, Loss: 0.6958809345960617, Final Batch Loss: 0.2005065679550171\n",
      "Epoch 1725, Loss: 0.5704018473625183, Final Batch Loss: 0.19590722024440765\n",
      "Epoch 1726, Loss: 0.5858920216560364, Final Batch Loss: 0.10754711925983429\n",
      "Epoch 1727, Loss: 0.6153605878353119, Final Batch Loss: 0.13654033839702606\n",
      "Epoch 1728, Loss: 0.6352621018886566, Final Batch Loss: 0.2701510488986969\n",
      "Epoch 1729, Loss: 0.6517741829156876, Final Batch Loss: 0.2711617648601532\n",
      "Epoch 1730, Loss: 0.6784714162349701, Final Batch Loss: 0.27437347173690796\n",
      "Epoch 1731, Loss: 0.7563046663999557, Final Batch Loss: 0.2829975187778473\n",
      "Epoch 1732, Loss: 0.5647342205047607, Final Batch Loss: 0.1101745218038559\n",
      "Epoch 1733, Loss: 0.6713153123855591, Final Batch Loss: 0.2345627248287201\n",
      "Epoch 1734, Loss: 0.5877010673284531, Final Batch Loss: 0.13484877347946167\n",
      "Epoch 1735, Loss: 0.6886765956878662, Final Batch Loss: 0.29151901602745056\n",
      "Epoch 1736, Loss: 0.6014119535684586, Final Batch Loss: 0.22266948223114014\n",
      "Epoch 1737, Loss: 0.6242586821317673, Final Batch Loss: 0.13221409916877747\n",
      "Epoch 1738, Loss: 0.6390891522169113, Final Batch Loss: 0.18392939865589142\n",
      "Epoch 1739, Loss: 0.6364970803260803, Final Batch Loss: 0.17467281222343445\n",
      "Epoch 1740, Loss: 0.8368477523326874, Final Batch Loss: 0.2715643048286438\n",
      "Epoch 1741, Loss: 0.5322546809911728, Final Batch Loss: 0.14862404763698578\n",
      "Epoch 1742, Loss: 0.5609691143035889, Final Batch Loss: 0.0939980149269104\n",
      "Epoch 1743, Loss: 0.7031477093696594, Final Batch Loss: 0.3242647647857666\n",
      "Epoch 1744, Loss: 0.6456599086523056, Final Batch Loss: 0.2214299440383911\n",
      "Epoch 1745, Loss: 0.6345193535089493, Final Batch Loss: 0.14155638217926025\n",
      "Epoch 1746, Loss: 0.6843859106302261, Final Batch Loss: 0.2716192901134491\n",
      "Epoch 1747, Loss: 0.49909909814596176, Final Batch Loss: 0.11305976659059525\n",
      "Epoch 1748, Loss: 0.5681168138980865, Final Batch Loss: 0.18506060540676117\n",
      "Epoch 1749, Loss: 0.6107368916273117, Final Batch Loss: 0.21896767616271973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1750, Loss: 0.7830056697130203, Final Batch Loss: 0.30993980169296265\n",
      "Epoch 1751, Loss: 0.7019809931516647, Final Batch Loss: 0.2859836518764496\n",
      "Epoch 1752, Loss: 0.8350355327129364, Final Batch Loss: 0.398555725812912\n",
      "Epoch 1753, Loss: 0.6300154030323029, Final Batch Loss: 0.2211693674325943\n",
      "Epoch 1754, Loss: 0.702947273850441, Final Batch Loss: 0.30465441942214966\n",
      "Epoch 1755, Loss: 0.6122661978006363, Final Batch Loss: 0.14130200445652008\n",
      "Epoch 1756, Loss: 0.7086788266897202, Final Batch Loss: 0.21603865921497345\n",
      "Epoch 1757, Loss: 0.6037947982549667, Final Batch Loss: 0.20852024853229523\n",
      "Epoch 1758, Loss: 0.6077934950590134, Final Batch Loss: 0.1413276195526123\n",
      "Epoch 1759, Loss: 0.6440007835626602, Final Batch Loss: 0.23544374108314514\n",
      "Epoch 1760, Loss: 0.7274632900953293, Final Batch Loss: 0.28202176094055176\n",
      "Epoch 1761, Loss: 0.6289271712303162, Final Batch Loss: 0.18918246030807495\n",
      "Epoch 1762, Loss: 0.5304027572274208, Final Batch Loss: 0.11245613545179367\n",
      "Epoch 1763, Loss: 0.5982593148946762, Final Batch Loss: 0.14441919326782227\n",
      "Epoch 1764, Loss: 0.6078035682439804, Final Batch Loss: 0.16486607491970062\n",
      "Epoch 1765, Loss: 0.7542550265789032, Final Batch Loss: 0.29053956270217896\n",
      "Epoch 1766, Loss: 0.6106355637311935, Final Batch Loss: 0.213145911693573\n",
      "Epoch 1767, Loss: 0.5343913286924362, Final Batch Loss: 0.14437724649906158\n",
      "Epoch 1768, Loss: 0.6674176454544067, Final Batch Loss: 0.25213053822517395\n",
      "Epoch 1769, Loss: 0.4765719398856163, Final Batch Loss: 0.0995170995593071\n",
      "Epoch 1770, Loss: 0.6613651812076569, Final Batch Loss: 0.24839961528778076\n",
      "Epoch 1771, Loss: 0.7612252831459045, Final Batch Loss: 0.35504648089408875\n",
      "Epoch 1772, Loss: 0.6282195150852203, Final Batch Loss: 0.20362478494644165\n",
      "Epoch 1773, Loss: 0.5907492786645889, Final Batch Loss: 0.1424400359392166\n",
      "Epoch 1774, Loss: 0.6641411334276199, Final Batch Loss: 0.19518741965293884\n",
      "Epoch 1775, Loss: 0.5997744649648666, Final Batch Loss: 0.22632917761802673\n",
      "Epoch 1776, Loss: 0.6218110769987106, Final Batch Loss: 0.23587234318256378\n",
      "Epoch 1777, Loss: 0.5863767713308334, Final Batch Loss: 0.15111905336380005\n",
      "Epoch 1778, Loss: 0.6753901839256287, Final Batch Loss: 0.218296617269516\n",
      "Epoch 1779, Loss: 0.7355470061302185, Final Batch Loss: 0.332452654838562\n",
      "Epoch 1780, Loss: 0.5593461096286774, Final Batch Loss: 0.10512882471084595\n",
      "Epoch 1781, Loss: 0.6543906778097153, Final Batch Loss: 0.19519811868667603\n",
      "Epoch 1782, Loss: 0.7161187678575516, Final Batch Loss: 0.302727073431015\n",
      "Epoch 1783, Loss: 0.6368004977703094, Final Batch Loss: 0.2527287006378174\n",
      "Epoch 1784, Loss: 0.6532415598630905, Final Batch Loss: 0.20460999011993408\n",
      "Epoch 1785, Loss: 0.4769558981060982, Final Batch Loss: 0.11064367741346359\n",
      "Epoch 1786, Loss: 0.5690110772848129, Final Batch Loss: 0.18405844271183014\n",
      "Epoch 1787, Loss: 0.5959948748350143, Final Batch Loss: 0.26218271255493164\n",
      "Epoch 1788, Loss: 0.8453617990016937, Final Batch Loss: 0.4634414613246918\n",
      "Epoch 1789, Loss: 0.6085041910409927, Final Batch Loss: 0.18374302983283997\n",
      "Epoch 1790, Loss: 0.6242630481719971, Final Batch Loss: 0.1792462021112442\n",
      "Epoch 1791, Loss: 0.6549640446901321, Final Batch Loss: 0.2005760371685028\n",
      "Epoch 1792, Loss: 0.6312658339738846, Final Batch Loss: 0.221466526389122\n",
      "Epoch 1793, Loss: 0.6647095084190369, Final Batch Loss: 0.16129063069820404\n",
      "Epoch 1794, Loss: 0.6367995291948318, Final Batch Loss: 0.18373394012451172\n",
      "Epoch 1795, Loss: 0.521519660949707, Final Batch Loss: 0.15726830065250397\n",
      "Epoch 1796, Loss: 0.5778138190507889, Final Batch Loss: 0.17286308109760284\n",
      "Epoch 1797, Loss: 0.5790678411722183, Final Batch Loss: 0.19009143114089966\n",
      "Epoch 1798, Loss: 0.5871099978685379, Final Batch Loss: 0.2001403123140335\n",
      "Epoch 1799, Loss: 0.5931286364793777, Final Batch Loss: 0.14069414138793945\n",
      "Epoch 1800, Loss: 0.4764755666255951, Final Batch Loss: 0.10977980494499207\n",
      "Epoch 1801, Loss: 0.8280406594276428, Final Batch Loss: 0.20789478719234467\n",
      "Epoch 1802, Loss: 0.5710370540618896, Final Batch Loss: 0.16090984642505646\n",
      "Epoch 1803, Loss: 0.5358041375875473, Final Batch Loss: 0.14381466805934906\n",
      "Epoch 1804, Loss: 0.6259114593267441, Final Batch Loss: 0.1986829936504364\n",
      "Epoch 1805, Loss: 0.7004968971014023, Final Batch Loss: 0.3213815987110138\n",
      "Epoch 1806, Loss: 0.6377798616886139, Final Batch Loss: 0.2171872854232788\n",
      "Epoch 1807, Loss: 0.5851961523294449, Final Batch Loss: 0.1715029925107956\n",
      "Epoch 1808, Loss: 0.5650991052389145, Final Batch Loss: 0.15926551818847656\n",
      "Epoch 1809, Loss: 0.6085768938064575, Final Batch Loss: 0.23217442631721497\n",
      "Epoch 1810, Loss: 0.5623992532491684, Final Batch Loss: 0.18086348474025726\n",
      "Epoch 1811, Loss: 0.5343606472015381, Final Batch Loss: 0.12840163707733154\n",
      "Epoch 1812, Loss: 0.6764135509729385, Final Batch Loss: 0.28520700335502625\n",
      "Epoch 1813, Loss: 0.6119927763938904, Final Batch Loss: 0.24274516105651855\n",
      "Epoch 1814, Loss: 0.5267170816659927, Final Batch Loss: 0.16299283504486084\n",
      "Epoch 1815, Loss: 0.587063655257225, Final Batch Loss: 0.17295199632644653\n",
      "Epoch 1816, Loss: 0.5901983380317688, Final Batch Loss: 0.1666443645954132\n",
      "Epoch 1817, Loss: 0.5094693899154663, Final Batch Loss: 0.08711427450180054\n",
      "Epoch 1818, Loss: 0.5707773417234421, Final Batch Loss: 0.23625130951404572\n",
      "Epoch 1819, Loss: 0.5862295478582382, Final Batch Loss: 0.14520850777626038\n",
      "Epoch 1820, Loss: 0.6049053966999054, Final Batch Loss: 0.18748748302459717\n",
      "Epoch 1821, Loss: 0.6114885807037354, Final Batch Loss: 0.2566211521625519\n",
      "Epoch 1822, Loss: 0.5790072530508041, Final Batch Loss: 0.1967298984527588\n",
      "Epoch 1823, Loss: 0.6543663144111633, Final Batch Loss: 0.2865191400051117\n",
      "Epoch 1824, Loss: 0.6087452173233032, Final Batch Loss: 0.21172289550304413\n",
      "Epoch 1825, Loss: 0.6997786164283752, Final Batch Loss: 0.3148863613605499\n",
      "Epoch 1826, Loss: 0.5395393818616867, Final Batch Loss: 0.16331356763839722\n",
      "Epoch 1827, Loss: 0.5397525802254677, Final Batch Loss: 0.10679889470338821\n",
      "Epoch 1828, Loss: 0.6125903278589249, Final Batch Loss: 0.18019798398017883\n",
      "Epoch 1829, Loss: 0.5896751135587692, Final Batch Loss: 0.1895214021205902\n",
      "Epoch 1830, Loss: 0.6057809740304947, Final Batch Loss: 0.18865132331848145\n",
      "Epoch 1831, Loss: 0.5659649968147278, Final Batch Loss: 0.19728098809719086\n",
      "Epoch 1832, Loss: 0.4686742350459099, Final Batch Loss: 0.12228069454431534\n",
      "Epoch 1833, Loss: 0.6930256634950638, Final Batch Loss: 0.2404976487159729\n",
      "Epoch 1834, Loss: 0.6179343611001968, Final Batch Loss: 0.22502799332141876\n",
      "Epoch 1835, Loss: 0.5338998436927795, Final Batch Loss: 0.142373189330101\n",
      "Epoch 1836, Loss: 0.5216524600982666, Final Batch Loss: 0.09931057691574097\n",
      "Epoch 1837, Loss: 0.6837660074234009, Final Batch Loss: 0.2922983467578888\n",
      "Epoch 1838, Loss: 0.5619044303894043, Final Batch Loss: 0.1399170160293579\n",
      "Epoch 1839, Loss: 0.4927169233560562, Final Batch Loss: 0.14306284487247467\n",
      "Epoch 1840, Loss: 0.5399982333183289, Final Batch Loss: 0.12872204184532166\n",
      "Epoch 1841, Loss: 0.5998504012823105, Final Batch Loss: 0.18426531553268433\n",
      "Epoch 1842, Loss: 0.5993009060621262, Final Batch Loss: 0.15496550500392914\n",
      "Epoch 1843, Loss: 0.49397657066583633, Final Batch Loss: 0.10443831235170364\n",
      "Epoch 1844, Loss: 0.4823165535926819, Final Batch Loss: 0.11902780830860138\n",
      "Epoch 1845, Loss: 0.5320930629968643, Final Batch Loss: 0.11412744224071503\n",
      "Epoch 1846, Loss: 0.649331733584404, Final Batch Loss: 0.21080946922302246\n",
      "Epoch 1847, Loss: 0.8079516440629959, Final Batch Loss: 0.34239551424980164\n",
      "Epoch 1848, Loss: 0.6612351685762405, Final Batch Loss: 0.3112553656101227\n",
      "Epoch 1849, Loss: 0.4958498626947403, Final Batch Loss: 0.12715986371040344\n",
      "Epoch 1850, Loss: 0.6788377314805984, Final Batch Loss: 0.280748188495636\n",
      "Epoch 1851, Loss: 0.46512608230113983, Final Batch Loss: 0.14842893183231354\n",
      "Epoch 1852, Loss: 0.6568121463060379, Final Batch Loss: 0.24948595464229584\n",
      "Epoch 1853, Loss: 0.6206865459680557, Final Batch Loss: 0.2576199173927307\n",
      "Epoch 1854, Loss: 0.5296537429094315, Final Batch Loss: 0.18094654381275177\n",
      "Epoch 1855, Loss: 0.5489936918020248, Final Batch Loss: 0.114094078540802\n",
      "Epoch 1856, Loss: 0.4892389327287674, Final Batch Loss: 0.14529100060462952\n",
      "Epoch 1857, Loss: 0.6899775862693787, Final Batch Loss: 0.3221868872642517\n",
      "Epoch 1858, Loss: 0.5359158962965012, Final Batch Loss: 0.16875050961971283\n",
      "Epoch 1859, Loss: 0.561919167637825, Final Batch Loss: 0.16334788501262665\n",
      "Epoch 1860, Loss: 0.590805858373642, Final Batch Loss: 0.20421525835990906\n",
      "Epoch 1861, Loss: 0.5809848010540009, Final Batch Loss: 0.1535768210887909\n",
      "Epoch 1862, Loss: 0.6549211591482162, Final Batch Loss: 0.27394938468933105\n",
      "Epoch 1863, Loss: 0.526096299290657, Final Batch Loss: 0.10606816411018372\n",
      "Epoch 1864, Loss: 0.5236837416887283, Final Batch Loss: 0.09432132542133331\n",
      "Epoch 1865, Loss: 0.6066798269748688, Final Batch Loss: 0.2127721607685089\n",
      "Epoch 1866, Loss: 0.5386863648891449, Final Batch Loss: 0.17178311944007874\n",
      "Epoch 1867, Loss: 0.6200997978448868, Final Batch Loss: 0.27294793725013733\n",
      "Epoch 1868, Loss: 0.5411517471075058, Final Batch Loss: 0.11478348076343536\n",
      "Epoch 1869, Loss: 0.6255473047494888, Final Batch Loss: 0.25581061840057373\n",
      "Epoch 1870, Loss: 0.581097811460495, Final Batch Loss: 0.1476115584373474\n",
      "Epoch 1871, Loss: 0.5867559164762497, Final Batch Loss: 0.18035323917865753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1872, Loss: 0.6367974579334259, Final Batch Loss: 0.20767849683761597\n",
      "Epoch 1873, Loss: 0.5096738561987877, Final Batch Loss: 0.10383772104978561\n",
      "Epoch 1874, Loss: 0.5774654000997543, Final Batch Loss: 0.19822362065315247\n",
      "Epoch 1875, Loss: 0.6647940129041672, Final Batch Loss: 0.2384800761938095\n",
      "Epoch 1876, Loss: 0.6924411058425903, Final Batch Loss: 0.26530593633651733\n",
      "Epoch 1877, Loss: 0.6082796156406403, Final Batch Loss: 0.12566550076007843\n",
      "Epoch 1878, Loss: 0.510098934173584, Final Batch Loss: 0.13396230340003967\n",
      "Epoch 1879, Loss: 0.6219261586666107, Final Batch Loss: 0.26762887835502625\n",
      "Epoch 1880, Loss: 0.7122610509395599, Final Batch Loss: 0.3999502956867218\n",
      "Epoch 1881, Loss: 0.5784424841403961, Final Batch Loss: 0.2465449720621109\n",
      "Epoch 1882, Loss: 0.5475047826766968, Final Batch Loss: 0.17478716373443604\n",
      "Epoch 1883, Loss: 0.6361254155635834, Final Batch Loss: 0.21224820613861084\n",
      "Epoch 1884, Loss: 0.7236862480640411, Final Batch Loss: 0.319695383310318\n",
      "Epoch 1885, Loss: 0.525303453207016, Final Batch Loss: 0.14945563673973083\n",
      "Epoch 1886, Loss: 0.6625152677297592, Final Batch Loss: 0.3346521556377411\n",
      "Epoch 1887, Loss: 0.5543957203626633, Final Batch Loss: 0.14059136807918549\n",
      "Epoch 1888, Loss: 0.652894988656044, Final Batch Loss: 0.21952775120735168\n",
      "Epoch 1889, Loss: 0.7138117551803589, Final Batch Loss: 0.2152666300535202\n",
      "Epoch 1890, Loss: 0.5843653380870819, Final Batch Loss: 0.17667052149772644\n",
      "Epoch 1891, Loss: 0.6498872488737106, Final Batch Loss: 0.25899797677993774\n",
      "Epoch 1892, Loss: 0.574389785528183, Final Batch Loss: 0.1713280975818634\n",
      "Epoch 1893, Loss: 0.6345568746328354, Final Batch Loss: 0.17738690972328186\n",
      "Epoch 1894, Loss: 0.724706158041954, Final Batch Loss: 0.35072576999664307\n",
      "Epoch 1895, Loss: 0.6367027312517166, Final Batch Loss: 0.21237634122371674\n",
      "Epoch 1896, Loss: 0.6506614685058594, Final Batch Loss: 0.24845337867736816\n",
      "Epoch 1897, Loss: 0.6171244978904724, Final Batch Loss: 0.21417933702468872\n",
      "Epoch 1898, Loss: 0.6108905076980591, Final Batch Loss: 0.16012321412563324\n",
      "Epoch 1899, Loss: 0.5856318175792694, Final Batch Loss: 0.24832114577293396\n",
      "Epoch 1900, Loss: 0.5274821221828461, Final Batch Loss: 0.160514235496521\n",
      "Epoch 1901, Loss: 0.6509042382240295, Final Batch Loss: 0.19207608699798584\n",
      "Epoch 1902, Loss: 0.5862793326377869, Final Batch Loss: 0.20595142245292664\n",
      "Epoch 1903, Loss: 0.6589641273021698, Final Batch Loss: 0.20933960378170013\n",
      "Epoch 1904, Loss: 0.8536855131387711, Final Batch Loss: 0.5214682817459106\n",
      "Epoch 1905, Loss: 0.47500357776880264, Final Batch Loss: 0.11043664067983627\n",
      "Epoch 1906, Loss: 0.6213150322437286, Final Batch Loss: 0.2314450442790985\n",
      "Epoch 1907, Loss: 0.5976213663816452, Final Batch Loss: 0.21817904710769653\n",
      "Epoch 1908, Loss: 0.5527745187282562, Final Batch Loss: 0.18370100855827332\n",
      "Epoch 1909, Loss: 0.5484440177679062, Final Batch Loss: 0.20993071794509888\n",
      "Epoch 1910, Loss: 0.6818151623010635, Final Batch Loss: 0.30775243043899536\n",
      "Epoch 1911, Loss: 0.5317185968160629, Final Batch Loss: 0.09098875522613525\n",
      "Epoch 1912, Loss: 0.5844512283802032, Final Batch Loss: 0.1666901707649231\n",
      "Epoch 1913, Loss: 0.5104146748781204, Final Batch Loss: 0.13288967311382294\n",
      "Epoch 1914, Loss: 0.5509272664785385, Final Batch Loss: 0.1724165827035904\n",
      "Epoch 1915, Loss: 0.5942530184984207, Final Batch Loss: 0.21953296661376953\n",
      "Epoch 1916, Loss: 0.5308965444564819, Final Batch Loss: 0.13898919522762299\n",
      "Epoch 1917, Loss: 0.5325160622596741, Final Batch Loss: 0.17218753695487976\n",
      "Epoch 1918, Loss: 0.5790068209171295, Final Batch Loss: 0.15816709399223328\n",
      "Epoch 1919, Loss: 0.51426612585783, Final Batch Loss: 0.10959275811910629\n",
      "Epoch 1920, Loss: 0.6402353942394257, Final Batch Loss: 0.2705066502094269\n",
      "Epoch 1921, Loss: 0.5048840045928955, Final Batch Loss: 0.11887869238853455\n",
      "Epoch 1922, Loss: 0.47430748492479324, Final Batch Loss: 0.10740124434232712\n",
      "Epoch 1923, Loss: 0.5383703410625458, Final Batch Loss: 0.18003462255001068\n",
      "Epoch 1924, Loss: 0.4896104708313942, Final Batch Loss: 0.09748055785894394\n",
      "Epoch 1925, Loss: 0.4863559901714325, Final Batch Loss: 0.1358993947505951\n",
      "Epoch 1926, Loss: 0.5807043462991714, Final Batch Loss: 0.2140563726425171\n",
      "Epoch 1927, Loss: 0.5134312883019447, Final Batch Loss: 0.10918325930833817\n",
      "Epoch 1928, Loss: 0.5008194297552109, Final Batch Loss: 0.06250129640102386\n",
      "Epoch 1929, Loss: 0.5218516886234283, Final Batch Loss: 0.18797773122787476\n",
      "Epoch 1930, Loss: 0.5679223984479904, Final Batch Loss: 0.15991337597370148\n",
      "Epoch 1931, Loss: 0.7327585369348526, Final Batch Loss: 0.38344547152519226\n",
      "Epoch 1932, Loss: 0.5686369091272354, Final Batch Loss: 0.18719877302646637\n",
      "Epoch 1933, Loss: 0.49697741866111755, Final Batch Loss: 0.11679469048976898\n",
      "Epoch 1934, Loss: 0.6781007647514343, Final Batch Loss: 0.31605973839759827\n",
      "Epoch 1935, Loss: 0.8221829682588577, Final Batch Loss: 0.3652069568634033\n",
      "Epoch 1936, Loss: 0.5470046699047089, Final Batch Loss: 0.17920581996440887\n",
      "Epoch 1937, Loss: 0.5280715078115463, Final Batch Loss: 0.1745840609073639\n",
      "Epoch 1938, Loss: 0.49044468998908997, Final Batch Loss: 0.07789352536201477\n",
      "Epoch 1939, Loss: 0.43711066991090775, Final Batch Loss: 0.1108187660574913\n",
      "Epoch 1940, Loss: 0.6207870543003082, Final Batch Loss: 0.1689590960741043\n",
      "Epoch 1941, Loss: 0.5503788143396378, Final Batch Loss: 0.20063604414463043\n",
      "Epoch 1942, Loss: 0.6219675987958908, Final Batch Loss: 0.24349099397659302\n",
      "Epoch 1943, Loss: 0.5537997782230377, Final Batch Loss: 0.17224010825157166\n",
      "Epoch 1944, Loss: 0.40639711171388626, Final Batch Loss: 0.08136957138776779\n",
      "Epoch 1945, Loss: 0.4571285992860794, Final Batch Loss: 0.15794970095157623\n",
      "Epoch 1946, Loss: 0.5910587906837463, Final Batch Loss: 0.23624469339847565\n",
      "Epoch 1947, Loss: 0.6170821189880371, Final Batch Loss: 0.2633543312549591\n",
      "Epoch 1948, Loss: 0.48570841550827026, Final Batch Loss: 0.15066230297088623\n",
      "Epoch 1949, Loss: 0.5024711638689041, Final Batch Loss: 0.13378407061100006\n",
      "Epoch 1950, Loss: 0.6036256700754166, Final Batch Loss: 0.23818719387054443\n",
      "Epoch 1951, Loss: 0.6932244151830673, Final Batch Loss: 0.3220953345298767\n",
      "Epoch 1952, Loss: 0.6147497892379761, Final Batch Loss: 0.2074536830186844\n",
      "Epoch 1953, Loss: 0.39301835745573044, Final Batch Loss: 0.11465441435575485\n",
      "Epoch 1954, Loss: 0.5678232461214066, Final Batch Loss: 0.1725589781999588\n",
      "Epoch 1955, Loss: 0.4573687016963959, Final Batch Loss: 0.14323148131370544\n",
      "Epoch 1956, Loss: 0.5738351494073868, Final Batch Loss: 0.19364237785339355\n",
      "Epoch 1957, Loss: 0.4737575203180313, Final Batch Loss: 0.08533585071563721\n",
      "Epoch 1958, Loss: 0.5433593988418579, Final Batch Loss: 0.21632663905620575\n",
      "Epoch 1959, Loss: 0.5200351998209953, Final Batch Loss: 0.06662765890359879\n",
      "Epoch 1960, Loss: 0.5560938864946365, Final Batch Loss: 0.14818285405635834\n",
      "Epoch 1961, Loss: 0.4267481490969658, Final Batch Loss: 0.07380606979131699\n",
      "Epoch 1962, Loss: 0.6500056982040405, Final Batch Loss: 0.2867354452610016\n",
      "Epoch 1963, Loss: 0.41856441646814346, Final Batch Loss: 0.0952276960015297\n",
      "Epoch 1964, Loss: 0.4999254122376442, Final Batch Loss: 0.11161855608224869\n",
      "Epoch 1965, Loss: 0.4013725519180298, Final Batch Loss: 0.1264817863702774\n",
      "Epoch 1966, Loss: 0.4574262350797653, Final Batch Loss: 0.13993175327777863\n",
      "Epoch 1967, Loss: 0.43670856952667236, Final Batch Loss: 0.10847832262516022\n",
      "Epoch 1968, Loss: 0.6227838695049286, Final Batch Loss: 0.21945951879024506\n",
      "Epoch 1969, Loss: 0.42728257179260254, Final Batch Loss: 0.09983806312084198\n",
      "Epoch 1970, Loss: 0.5981564819812775, Final Batch Loss: 0.20493435859680176\n",
      "Epoch 1971, Loss: 0.5155530944466591, Final Batch Loss: 0.12438300997018814\n",
      "Epoch 1972, Loss: 0.4857635945081711, Final Batch Loss: 0.07944750785827637\n",
      "Epoch 1973, Loss: 0.6838421523571014, Final Batch Loss: 0.24028250575065613\n",
      "Epoch 1974, Loss: 0.535928264260292, Final Batch Loss: 0.1877930760383606\n",
      "Epoch 1975, Loss: 0.7330876737833023, Final Batch Loss: 0.33484509587287903\n",
      "Epoch 1976, Loss: 0.5534211993217468, Final Batch Loss: 0.17005212604999542\n",
      "Epoch 1977, Loss: 0.6324110329151154, Final Batch Loss: 0.17013539373874664\n",
      "Epoch 1978, Loss: 0.6538350731134415, Final Batch Loss: 0.24728459119796753\n",
      "Epoch 1979, Loss: 0.6088868826627731, Final Batch Loss: 0.2616698741912842\n",
      "Epoch 1980, Loss: 0.6632153391838074, Final Batch Loss: 0.2464422732591629\n",
      "Epoch 1981, Loss: 0.705916091799736, Final Batch Loss: 0.32554543018341064\n",
      "Epoch 1982, Loss: 0.5687867850065231, Final Batch Loss: 0.21182933449745178\n",
      "Epoch 1983, Loss: 0.5364361554384232, Final Batch Loss: 0.1781913936138153\n",
      "Epoch 1984, Loss: 0.6519142389297485, Final Batch Loss: 0.19786185026168823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1985, Loss: 0.49174070358276367, Final Batch Loss: 0.13820818066596985\n",
      "Epoch 1986, Loss: 0.47835568338632584, Final Batch Loss: 0.10221981257200241\n",
      "Epoch 1987, Loss: 0.46638405323028564, Final Batch Loss: 0.1572795957326889\n",
      "Epoch 1988, Loss: 0.4715114235877991, Final Batch Loss: 0.12644928693771362\n",
      "Epoch 1989, Loss: 0.5656497329473495, Final Batch Loss: 0.14975041151046753\n",
      "Epoch 1990, Loss: 0.5743211209774017, Final Batch Loss: 0.2540634274482727\n",
      "Epoch 1991, Loss: 0.5112490057945251, Final Batch Loss: 0.17755261063575745\n",
      "Epoch 1992, Loss: 0.482405349612236, Final Batch Loss: 0.16327334940433502\n",
      "Epoch 1993, Loss: 0.4941322058439255, Final Batch Loss: 0.1669294387102127\n",
      "Epoch 1994, Loss: 0.5871356725692749, Final Batch Loss: 0.26848605275154114\n",
      "Epoch 1995, Loss: 0.4122430086135864, Final Batch Loss: 0.13635900616645813\n",
      "Epoch 1996, Loss: 0.5697303116321564, Final Batch Loss: 0.25180092453956604\n",
      "Epoch 1997, Loss: 0.5005271583795547, Final Batch Loss: 0.10506404936313629\n",
      "Epoch 1998, Loss: 0.6066811978816986, Final Batch Loss: 0.2445412129163742\n",
      "Epoch 1999, Loss: 0.5084334164857864, Final Batch Loss: 0.18501345813274384\n",
      "Epoch 2000, Loss: 0.4463295340538025, Final Batch Loss: 0.15962287783622742\n",
      "Epoch 2001, Loss: 0.6046843528747559, Final Batch Loss: 0.18866539001464844\n",
      "Epoch 2002, Loss: 0.5741944760084152, Final Batch Loss: 0.1821369081735611\n",
      "Epoch 2003, Loss: 0.4482831507921219, Final Batch Loss: 0.14645586907863617\n",
      "Epoch 2004, Loss: 0.4753190726041794, Final Batch Loss: 0.14771059155464172\n",
      "Epoch 2005, Loss: 0.42434050142765045, Final Batch Loss: 0.08304256200790405\n",
      "Epoch 2006, Loss: 0.5864030420780182, Final Batch Loss: 0.21394392848014832\n",
      "Epoch 2007, Loss: 0.7107985615730286, Final Batch Loss: 0.2922338843345642\n",
      "Epoch 2008, Loss: 0.5365288704633713, Final Batch Loss: 0.18332119286060333\n",
      "Epoch 2009, Loss: 0.41743409633636475, Final Batch Loss: 0.14248156547546387\n",
      "Epoch 2010, Loss: 0.5693733543157578, Final Batch Loss: 0.19167861342430115\n",
      "Epoch 2011, Loss: 0.5475154668092728, Final Batch Loss: 0.19174516201019287\n",
      "Epoch 2012, Loss: 0.5407587885856628, Final Batch Loss: 0.19167488813400269\n",
      "Epoch 2013, Loss: 0.6039340794086456, Final Batch Loss: 0.2032809555530548\n",
      "Epoch 2014, Loss: 0.5729834288358688, Final Batch Loss: 0.20056109130382538\n",
      "Epoch 2015, Loss: 0.5471648126840591, Final Batch Loss: 0.18390941619873047\n",
      "Epoch 2016, Loss: 0.5988060384988785, Final Batch Loss: 0.20506152510643005\n",
      "Epoch 2017, Loss: 0.4973336011171341, Final Batch Loss: 0.1308560073375702\n",
      "Epoch 2018, Loss: 0.486779123544693, Final Batch Loss: 0.1595187485218048\n",
      "Epoch 2019, Loss: 0.6753494441509247, Final Batch Loss: 0.2943150997161865\n",
      "Epoch 2020, Loss: 0.4926125183701515, Final Batch Loss: 0.09923479706048965\n",
      "Epoch 2021, Loss: 0.4950002133846283, Final Batch Loss: 0.12083771824836731\n",
      "Epoch 2022, Loss: 0.5087223649024963, Final Batch Loss: 0.12339633703231812\n",
      "Epoch 2023, Loss: 0.45645996183156967, Final Batch Loss: 0.10479768365621567\n",
      "Epoch 2024, Loss: 0.6345236897468567, Final Batch Loss: 0.2402074635028839\n",
      "Epoch 2025, Loss: 0.6836642920970917, Final Batch Loss: 0.25386565923690796\n",
      "Epoch 2026, Loss: 0.5284999161958694, Final Batch Loss: 0.1735980361700058\n",
      "Epoch 2027, Loss: 0.5723426342010498, Final Batch Loss: 0.23820029199123383\n",
      "Epoch 2028, Loss: 0.6978863254189491, Final Batch Loss: 0.3594871759414673\n",
      "Epoch 2029, Loss: 0.5114421397447586, Final Batch Loss: 0.12222224473953247\n",
      "Epoch 2030, Loss: 0.526949554681778, Final Batch Loss: 0.20105302333831787\n",
      "Epoch 2031, Loss: 0.6745617985725403, Final Batch Loss: 0.22523275017738342\n",
      "Epoch 2032, Loss: 0.5996240973472595, Final Batch Loss: 0.23277372121810913\n",
      "Epoch 2033, Loss: 0.5024795681238174, Final Batch Loss: 0.1738964170217514\n",
      "Epoch 2034, Loss: 0.470547690987587, Final Batch Loss: 0.122765451669693\n",
      "Epoch 2035, Loss: 0.5006589442491531, Final Batch Loss: 0.16608485579490662\n",
      "Epoch 2036, Loss: 0.5843467563390732, Final Batch Loss: 0.29061198234558105\n",
      "Epoch 2037, Loss: 0.47338974475860596, Final Batch Loss: 0.1282336562871933\n",
      "Epoch 2038, Loss: 0.38304461538791656, Final Batch Loss: 0.07570001482963562\n",
      "Epoch 2039, Loss: 0.5664292275905609, Final Batch Loss: 0.2345118224620819\n",
      "Epoch 2040, Loss: 0.5368703454732895, Final Batch Loss: 0.21633395552635193\n",
      "Epoch 2041, Loss: 0.5593275800347328, Final Batch Loss: 0.12177563458681107\n",
      "Epoch 2042, Loss: 0.5051229745149612, Final Batch Loss: 0.18906375765800476\n",
      "Epoch 2043, Loss: 0.6081470996141434, Final Batch Loss: 0.21959440410137177\n",
      "Epoch 2044, Loss: 0.44412534683942795, Final Batch Loss: 0.09793014079332352\n",
      "Epoch 2045, Loss: 0.41506731510162354, Final Batch Loss: 0.13900388777256012\n",
      "Epoch 2046, Loss: 0.7255607545375824, Final Batch Loss: 0.4311610162258148\n",
      "Epoch 2047, Loss: 0.5466579794883728, Final Batch Loss: 0.1836538463830948\n",
      "Epoch 2048, Loss: 0.4644145965576172, Final Batch Loss: 0.10235637426376343\n",
      "Epoch 2049, Loss: 0.7237297296524048, Final Batch Loss: 0.3172347843647003\n",
      "Epoch 2050, Loss: 0.49475836753845215, Final Batch Loss: 0.1719820648431778\n",
      "Epoch 2051, Loss: 0.5353267192840576, Final Batch Loss: 0.1853010058403015\n",
      "Epoch 2052, Loss: 0.600324884057045, Final Batch Loss: 0.15157923102378845\n",
      "Epoch 2053, Loss: 0.4873267635703087, Final Batch Loss: 0.10619088262319565\n",
      "Epoch 2054, Loss: 0.5583216547966003, Final Batch Loss: 0.17284922301769257\n",
      "Epoch 2055, Loss: 0.6019019186496735, Final Batch Loss: 0.2778841257095337\n",
      "Epoch 2056, Loss: 0.5008181929588318, Final Batch Loss: 0.178810715675354\n",
      "Epoch 2057, Loss: 0.48483067750930786, Final Batch Loss: 0.15324470400810242\n",
      "Epoch 2058, Loss: 0.4885171949863434, Final Batch Loss: 0.19308803975582123\n",
      "Epoch 2059, Loss: 0.4875952899456024, Final Batch Loss: 0.13222883641719818\n",
      "Epoch 2060, Loss: 0.5211090743541718, Final Batch Loss: 0.18768174946308136\n",
      "Epoch 2061, Loss: 0.5515316873788834, Final Batch Loss: 0.1918928027153015\n",
      "Epoch 2062, Loss: 0.5520254373550415, Final Batch Loss: 0.22371938824653625\n",
      "Epoch 2063, Loss: 0.4424370899796486, Final Batch Loss: 0.09398768097162247\n",
      "Epoch 2064, Loss: 0.49326345324516296, Final Batch Loss: 0.14603202044963837\n",
      "Epoch 2065, Loss: 0.43677230179309845, Final Batch Loss: 0.1290915459394455\n",
      "Epoch 2066, Loss: 0.553088828921318, Final Batch Loss: 0.16240179538726807\n",
      "Epoch 2067, Loss: 0.5714918971061707, Final Batch Loss: 0.1495399922132492\n",
      "Epoch 2068, Loss: 0.42910023033618927, Final Batch Loss: 0.14399923384189606\n",
      "Epoch 2069, Loss: 0.4272491782903671, Final Batch Loss: 0.12952031195163727\n",
      "Epoch 2070, Loss: 0.5412807762622833, Final Batch Loss: 0.15280787646770477\n",
      "Epoch 2071, Loss: 0.47891485691070557, Final Batch Loss: 0.1704702377319336\n",
      "Epoch 2072, Loss: 0.4241422712802887, Final Batch Loss: 0.12254790961742401\n",
      "Epoch 2073, Loss: 0.5289318114519119, Final Batch Loss: 0.23988939821720123\n",
      "Epoch 2074, Loss: 0.6011292636394501, Final Batch Loss: 0.23907272517681122\n",
      "Epoch 2075, Loss: 0.5271052867174149, Final Batch Loss: 0.18553130328655243\n",
      "Epoch 2076, Loss: 0.6032740920782089, Final Batch Loss: 0.26684045791625977\n",
      "Epoch 2077, Loss: 0.43848182260990143, Final Batch Loss: 0.1379787027835846\n",
      "Epoch 2078, Loss: 0.4977015256881714, Final Batch Loss: 0.1595935970544815\n",
      "Epoch 2079, Loss: 0.4870498478412628, Final Batch Loss: 0.08806389570236206\n",
      "Epoch 2080, Loss: 0.5123928785324097, Final Batch Loss: 0.15286624431610107\n",
      "Epoch 2081, Loss: 0.4843013286590576, Final Batch Loss: 0.15099383890628815\n",
      "Epoch 2082, Loss: 0.5358021706342697, Final Batch Loss: 0.1675623059272766\n",
      "Epoch 2083, Loss: 0.5058347880840302, Final Batch Loss: 0.1649913638830185\n",
      "Epoch 2084, Loss: 0.44292257726192474, Final Batch Loss: 0.13486045598983765\n",
      "Epoch 2085, Loss: 0.5638180077075958, Final Batch Loss: 0.17470920085906982\n",
      "Epoch 2086, Loss: 0.48849348723888397, Final Batch Loss: 0.1760667860507965\n",
      "Epoch 2087, Loss: 0.6228157430887222, Final Batch Loss: 0.147939532995224\n",
      "Epoch 2088, Loss: 0.5529553145170212, Final Batch Loss: 0.21063676476478577\n",
      "Epoch 2089, Loss: 0.5710993111133575, Final Batch Loss: 0.19909635186195374\n",
      "Epoch 2090, Loss: 0.5700539797544479, Final Batch Loss: 0.2449437379837036\n",
      "Epoch 2091, Loss: 0.5454195737838745, Final Batch Loss: 0.15330012142658234\n",
      "Epoch 2092, Loss: 0.3875385820865631, Final Batch Loss: 0.06468191742897034\n",
      "Epoch 2093, Loss: 0.5850183516740799, Final Batch Loss: 0.2399061769247055\n",
      "Epoch 2094, Loss: 0.5959167331457138, Final Batch Loss: 0.1656208634376526\n",
      "Epoch 2095, Loss: 0.6790242493152618, Final Batch Loss: 0.33069854974746704\n",
      "Epoch 2096, Loss: 0.50126913189888, Final Batch Loss: 0.170179545879364\n",
      "Epoch 2097, Loss: 0.40014688670635223, Final Batch Loss: 0.05980345606803894\n",
      "Epoch 2098, Loss: 0.5983185768127441, Final Batch Loss: 0.21479880809783936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2099, Loss: 0.5161178410053253, Final Batch Loss: 0.21827392280101776\n",
      "Epoch 2100, Loss: 0.5816162973642349, Final Batch Loss: 0.2433394193649292\n",
      "Epoch 2101, Loss: 0.5204064697027206, Final Batch Loss: 0.18530777096748352\n",
      "Epoch 2102, Loss: 0.5463062822818756, Final Batch Loss: 0.22908104956150055\n",
      "Epoch 2103, Loss: 0.6525324434041977, Final Batch Loss: 0.2244510054588318\n",
      "Epoch 2104, Loss: 0.49980857968330383, Final Batch Loss: 0.14552699029445648\n",
      "Epoch 2105, Loss: 0.6419019848108292, Final Batch Loss: 0.27027425169944763\n",
      "Epoch 2106, Loss: 0.47564999759197235, Final Batch Loss: 0.1590925008058548\n",
      "Epoch 2107, Loss: 0.4263036772608757, Final Batch Loss: 0.08925173431634903\n",
      "Epoch 2108, Loss: 0.5436798334121704, Final Batch Loss: 0.14315390586853027\n",
      "Epoch 2109, Loss: 0.5299185067415237, Final Batch Loss: 0.1915726512670517\n",
      "Epoch 2110, Loss: 0.4677809104323387, Final Batch Loss: 0.10573869198560715\n",
      "Epoch 2111, Loss: 0.5566408038139343, Final Batch Loss: 0.17466343939304352\n",
      "Epoch 2112, Loss: 0.556466594338417, Final Batch Loss: 0.211867094039917\n",
      "Epoch 2113, Loss: 0.46494221687316895, Final Batch Loss: 0.15419258177280426\n",
      "Epoch 2114, Loss: 0.4362243562936783, Final Batch Loss: 0.0775851309299469\n",
      "Epoch 2115, Loss: 0.5016975030303001, Final Batch Loss: 0.24664579331874847\n",
      "Epoch 2116, Loss: 0.570733830332756, Final Batch Loss: 0.22498008608818054\n",
      "Epoch 2117, Loss: 0.6178964227437973, Final Batch Loss: 0.2593310475349426\n",
      "Epoch 2118, Loss: 0.49100348353385925, Final Batch Loss: 0.1743861585855484\n",
      "Epoch 2119, Loss: 0.45080696046352386, Final Batch Loss: 0.13502201437950134\n",
      "Epoch 2120, Loss: 0.45971596986055374, Final Batch Loss: 0.11740017682313919\n",
      "Epoch 2121, Loss: 0.6408547461032867, Final Batch Loss: 0.24800406396389008\n",
      "Epoch 2122, Loss: 0.3894645869731903, Final Batch Loss: 0.06840763986110687\n",
      "Epoch 2123, Loss: 0.6492838859558105, Final Batch Loss: 0.30815553665161133\n",
      "Epoch 2124, Loss: 0.5137890428304672, Final Batch Loss: 0.17845384776592255\n",
      "Epoch 2125, Loss: 0.433641754090786, Final Batch Loss: 0.07812262326478958\n",
      "Epoch 2126, Loss: 0.469839483499527, Final Batch Loss: 0.16654376685619354\n",
      "Epoch 2127, Loss: 0.5744055062532425, Final Batch Loss: 0.18182621896266937\n",
      "Epoch 2128, Loss: 0.5445296913385391, Final Batch Loss: 0.1255992203950882\n",
      "Epoch 2129, Loss: 0.6157239675521851, Final Batch Loss: 0.27898240089416504\n",
      "Epoch 2130, Loss: 0.49797268211841583, Final Batch Loss: 0.1355484127998352\n",
      "Epoch 2131, Loss: 0.49322928488254547, Final Batch Loss: 0.14895790815353394\n",
      "Epoch 2132, Loss: 0.5081289038062096, Final Batch Loss: 0.06988144665956497\n",
      "Epoch 2133, Loss: 0.48370105773210526, Final Batch Loss: 0.08162561804056168\n",
      "Epoch 2134, Loss: 0.5489000231027603, Final Batch Loss: 0.1871100217103958\n",
      "Epoch 2135, Loss: 0.6289694458246231, Final Batch Loss: 0.31132543087005615\n",
      "Epoch 2136, Loss: 0.47005972266197205, Final Batch Loss: 0.1436309963464737\n",
      "Epoch 2137, Loss: 0.4523783251643181, Final Batch Loss: 0.08369382470846176\n",
      "Epoch 2138, Loss: 0.5375734567642212, Final Batch Loss: 0.1868906170129776\n",
      "Epoch 2139, Loss: 0.4599226266145706, Final Batch Loss: 0.128530815243721\n",
      "Epoch 2140, Loss: 0.5263788402080536, Final Batch Loss: 0.20150475203990936\n",
      "Epoch 2141, Loss: 0.6198053061962128, Final Batch Loss: 0.24104638397693634\n",
      "Epoch 2142, Loss: 0.4105760455131531, Final Batch Loss: 0.11136499047279358\n",
      "Epoch 2143, Loss: 0.5090922117233276, Final Batch Loss: 0.1465359926223755\n",
      "Epoch 2144, Loss: 0.410567507147789, Final Batch Loss: 0.09911292791366577\n",
      "Epoch 2145, Loss: 0.45214079320430756, Final Batch Loss: 0.12671540677547455\n",
      "Epoch 2146, Loss: 0.4501649737358093, Final Batch Loss: 0.14579962193965912\n",
      "Epoch 2147, Loss: 0.40516941249370575, Final Batch Loss: 0.06888897716999054\n",
      "Epoch 2148, Loss: 0.5010580569505692, Final Batch Loss: 0.21264363825321198\n",
      "Epoch 2149, Loss: 0.4381847083568573, Final Batch Loss: 0.11598452925682068\n",
      "Epoch 2150, Loss: 0.4794197827577591, Final Batch Loss: 0.12888427078723907\n",
      "Epoch 2151, Loss: 0.4184524938464165, Final Batch Loss: 0.07577519863843918\n",
      "Epoch 2152, Loss: 0.5768025666475296, Final Batch Loss: 0.16021212935447693\n",
      "Epoch 2153, Loss: 0.49656806886196136, Final Batch Loss: 0.21122288703918457\n",
      "Epoch 2154, Loss: 0.44040537625551224, Final Batch Loss: 0.10656077414751053\n",
      "Epoch 2155, Loss: 0.5883798748254776, Final Batch Loss: 0.24519005417823792\n",
      "Epoch 2156, Loss: 0.5751121342182159, Final Batch Loss: 0.21353618800640106\n",
      "Epoch 2157, Loss: 0.5738708451390266, Final Batch Loss: 0.20472602546215057\n",
      "Epoch 2158, Loss: 0.5045855566859245, Final Batch Loss: 0.17807637155056\n",
      "Epoch 2159, Loss: 0.5203917175531387, Final Batch Loss: 0.21448835730552673\n",
      "Epoch 2160, Loss: 0.4187258630990982, Final Batch Loss: 0.11128322780132294\n",
      "Epoch 2161, Loss: 0.4642731696367264, Final Batch Loss: 0.14328093826770782\n",
      "Epoch 2162, Loss: 0.5474802702665329, Final Batch Loss: 0.17916922271251678\n",
      "Epoch 2163, Loss: 0.5263119041919708, Final Batch Loss: 0.22520440816879272\n",
      "Epoch 2164, Loss: 0.5607902705669403, Final Batch Loss: 0.23004257678985596\n",
      "Epoch 2165, Loss: 0.5271271392703056, Final Batch Loss: 0.2525486350059509\n",
      "Epoch 2166, Loss: 0.3666098453104496, Final Batch Loss: 0.053800467401742935\n",
      "Epoch 2167, Loss: 0.5745765119791031, Final Batch Loss: 0.20754006505012512\n",
      "Epoch 2168, Loss: 0.6559590250253677, Final Batch Loss: 0.336370050907135\n",
      "Epoch 2169, Loss: 0.47941723465919495, Final Batch Loss: 0.13330614566802979\n",
      "Epoch 2170, Loss: 0.36573171615600586, Final Batch Loss: 0.09490244090557098\n",
      "Epoch 2171, Loss: 0.5733524411916733, Final Batch Loss: 0.24428094923496246\n",
      "Epoch 2172, Loss: 0.5252529978752136, Final Batch Loss: 0.11706575751304626\n",
      "Epoch 2173, Loss: 0.542917475104332, Final Batch Loss: 0.16735224425792694\n",
      "Epoch 2174, Loss: 0.48097965121269226, Final Batch Loss: 0.16180433332920074\n",
      "Epoch 2175, Loss: 0.5723040401935577, Final Batch Loss: 0.25019070506095886\n",
      "Epoch 2176, Loss: 0.49102143943309784, Final Batch Loss: 0.08059686422348022\n",
      "Epoch 2177, Loss: 0.5611112266778946, Final Batch Loss: 0.2245926409959793\n",
      "Epoch 2178, Loss: 0.5255213677883148, Final Batch Loss: 0.14801160991191864\n",
      "Epoch 2179, Loss: 0.3998207673430443, Final Batch Loss: 0.1307399868965149\n",
      "Epoch 2180, Loss: 0.5801876187324524, Final Batch Loss: 0.18283621966838837\n",
      "Epoch 2181, Loss: 0.5208431929349899, Final Batch Loss: 0.16725678741931915\n",
      "Epoch 2182, Loss: 0.647694855928421, Final Batch Loss: 0.30368635058403015\n",
      "Epoch 2183, Loss: 0.3837524801492691, Final Batch Loss: 0.08364515006542206\n",
      "Epoch 2184, Loss: 0.5257434695959091, Final Batch Loss: 0.18955546617507935\n",
      "Epoch 2185, Loss: 0.5238144397735596, Final Batch Loss: 0.16430014371871948\n",
      "Epoch 2186, Loss: 0.3960662856698036, Final Batch Loss: 0.08243394643068314\n",
      "Epoch 2187, Loss: 0.567933976650238, Final Batch Loss: 0.21178939938545227\n",
      "Epoch 2188, Loss: 0.46225929260253906, Final Batch Loss: 0.14344552159309387\n",
      "Epoch 2189, Loss: 0.5846561044454575, Final Batch Loss: 0.26938745379447937\n",
      "Epoch 2190, Loss: 0.41844649612903595, Final Batch Loss: 0.09207060933113098\n",
      "Epoch 2191, Loss: 0.5056236684322357, Final Batch Loss: 0.199426531791687\n",
      "Epoch 2192, Loss: 0.4294574558734894, Final Batch Loss: 0.1451241374015808\n",
      "Epoch 2193, Loss: 0.4548327699303627, Final Batch Loss: 0.1577843874692917\n",
      "Epoch 2194, Loss: 0.4562973976135254, Final Batch Loss: 0.1294223666191101\n",
      "Epoch 2195, Loss: 0.48859061300754547, Final Batch Loss: 0.13056565821170807\n",
      "Epoch 2196, Loss: 0.6082355380058289, Final Batch Loss: 0.1707170456647873\n",
      "Epoch 2197, Loss: 0.5127198994159698, Final Batch Loss: 0.12329146265983582\n",
      "Epoch 2198, Loss: 0.4541025683283806, Final Batch Loss: 0.08059220761060715\n",
      "Epoch 2199, Loss: 0.6105168014764786, Final Batch Loss: 0.2906762361526489\n",
      "Epoch 2200, Loss: 0.4090016707777977, Final Batch Loss: 0.10138153284788132\n",
      "Epoch 2201, Loss: 0.42227669805288315, Final Batch Loss: 0.10025619715452194\n",
      "Epoch 2202, Loss: 0.5782768428325653, Final Batch Loss: 0.2296210378408432\n",
      "Epoch 2203, Loss: 0.5629129260778427, Final Batch Loss: 0.2158205807209015\n",
      "Epoch 2204, Loss: 0.3955150544643402, Final Batch Loss: 0.10338407754898071\n",
      "Epoch 2205, Loss: 0.5431609749794006, Final Batch Loss: 0.17862346768379211\n",
      "Epoch 2206, Loss: 0.529202476143837, Final Batch Loss: 0.20549671351909637\n",
      "Epoch 2207, Loss: 0.5208559483289719, Final Batch Loss: 0.11644990742206573\n",
      "Epoch 2208, Loss: 0.5201826989650726, Final Batch Loss: 0.1336316019296646\n",
      "Epoch 2209, Loss: 0.5731601119041443, Final Batch Loss: 0.25491729378700256\n",
      "Epoch 2210, Loss: 0.5390701591968536, Final Batch Loss: 0.18930897116661072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2211, Loss: 0.6549733728170395, Final Batch Loss: 0.3918846547603607\n",
      "Epoch 2212, Loss: 0.4555790573358536, Final Batch Loss: 0.1579868197441101\n",
      "Epoch 2213, Loss: 0.45125991106033325, Final Batch Loss: 0.13627924025058746\n",
      "Epoch 2214, Loss: 0.4554644078016281, Final Batch Loss: 0.15176843106746674\n",
      "Epoch 2215, Loss: 0.6095756590366364, Final Batch Loss: 0.27281197905540466\n",
      "Epoch 2216, Loss: 0.5954091250896454, Final Batch Loss: 0.20826703310012817\n",
      "Epoch 2217, Loss: 0.755707286298275, Final Batch Loss: 0.44251179695129395\n",
      "Epoch 2218, Loss: 0.4919974207878113, Final Batch Loss: 0.18866200745105743\n",
      "Epoch 2219, Loss: 0.7409420311450958, Final Batch Loss: 0.32633715867996216\n",
      "Epoch 2220, Loss: 0.40956755727529526, Final Batch Loss: 0.0932733491063118\n",
      "Epoch 2221, Loss: 0.6901831328868866, Final Batch Loss: 0.30660805106163025\n",
      "Epoch 2222, Loss: 0.5212314873933792, Final Batch Loss: 0.21963591873645782\n",
      "Epoch 2223, Loss: 0.44055695831775665, Final Batch Loss: 0.14756856858730316\n",
      "Epoch 2224, Loss: 0.4670638144016266, Final Batch Loss: 0.17735545337200165\n",
      "Epoch 2225, Loss: 0.5631130039691925, Final Batch Loss: 0.20969097316265106\n",
      "Epoch 2226, Loss: 0.48419754207134247, Final Batch Loss: 0.14640167355537415\n",
      "Epoch 2227, Loss: 0.6897355765104294, Final Batch Loss: 0.31629854440689087\n",
      "Epoch 2228, Loss: 0.5413778871297836, Final Batch Loss: 0.2138334959745407\n",
      "Epoch 2229, Loss: 0.5073962509632111, Final Batch Loss: 0.1860358566045761\n",
      "Epoch 2230, Loss: 0.4237339347600937, Final Batch Loss: 0.11478017270565033\n",
      "Epoch 2231, Loss: 0.6668514013290405, Final Batch Loss: 0.305562824010849\n",
      "Epoch 2232, Loss: 0.4624645859003067, Final Batch Loss: 0.18135498464107513\n",
      "Epoch 2233, Loss: 0.5173105895519257, Final Batch Loss: 0.14499333500862122\n",
      "Epoch 2234, Loss: 0.48773710429668427, Final Batch Loss: 0.15002180635929108\n",
      "Epoch 2235, Loss: 0.5830740332603455, Final Batch Loss: 0.18761469423770905\n",
      "Epoch 2236, Loss: 0.520189106464386, Final Batch Loss: 0.24837245047092438\n",
      "Epoch 2237, Loss: 0.48060958832502365, Final Batch Loss: 0.0678272619843483\n",
      "Epoch 2238, Loss: 0.4094104990363121, Final Batch Loss: 0.09820912033319473\n",
      "Epoch 2239, Loss: 0.4647529423236847, Final Batch Loss: 0.10569655895233154\n",
      "Epoch 2240, Loss: 0.42228034138679504, Final Batch Loss: 0.13495640456676483\n",
      "Epoch 2241, Loss: 0.48454129695892334, Final Batch Loss: 0.11331003904342651\n",
      "Epoch 2242, Loss: 0.4705822616815567, Final Batch Loss: 0.11498269438743591\n",
      "Epoch 2243, Loss: 0.5973043590784073, Final Batch Loss: 0.23579509556293488\n",
      "Epoch 2244, Loss: 0.46883587539196014, Final Batch Loss: 0.17083363234996796\n",
      "Epoch 2245, Loss: 0.4028518721461296, Final Batch Loss: 0.09818585962057114\n",
      "Epoch 2246, Loss: 0.4828288108110428, Final Batch Loss: 0.1041189581155777\n",
      "Epoch 2247, Loss: 0.40872126817703247, Final Batch Loss: 0.12469746172428131\n",
      "Epoch 2248, Loss: 0.40583499521017075, Final Batch Loss: 0.10573063045740128\n",
      "Epoch 2249, Loss: 0.5164255946874619, Final Batch Loss: 0.24218125641345978\n",
      "Epoch 2250, Loss: 0.4376212954521179, Final Batch Loss: 0.1145835667848587\n",
      "Epoch 2251, Loss: 0.4223000407218933, Final Batch Loss: 0.07147493958473206\n",
      "Epoch 2252, Loss: 0.5519470274448395, Final Batch Loss: 0.16182784736156464\n",
      "Epoch 2253, Loss: 0.5300852358341217, Final Batch Loss: 0.19407528638839722\n",
      "Epoch 2254, Loss: 0.470818430185318, Final Batch Loss: 0.1645800769329071\n",
      "Epoch 2255, Loss: 0.49165016412734985, Final Batch Loss: 0.1566900759935379\n",
      "Epoch 2256, Loss: 0.39274194836616516, Final Batch Loss: 0.08398765325546265\n",
      "Epoch 2257, Loss: 0.5017914548516273, Final Batch Loss: 0.22926846146583557\n",
      "Epoch 2258, Loss: 0.4706713557243347, Final Batch Loss: 0.15393590927124023\n",
      "Epoch 2259, Loss: 0.4475026875734329, Final Batch Loss: 0.13336844742298126\n",
      "Epoch 2260, Loss: 0.36675455421209335, Final Batch Loss: 0.08944722265005112\n",
      "Epoch 2261, Loss: 0.5618555992841721, Final Batch Loss: 0.19633081555366516\n",
      "Epoch 2262, Loss: 0.4602081775665283, Final Batch Loss: 0.10906684398651123\n",
      "Epoch 2263, Loss: 0.4647448807954788, Final Batch Loss: 0.14818726480007172\n",
      "Epoch 2264, Loss: 0.502591647207737, Final Batch Loss: 0.23722615838050842\n",
      "Epoch 2265, Loss: 0.45428110659122467, Final Batch Loss: 0.17063845694065094\n",
      "Epoch 2266, Loss: 0.4009782150387764, Final Batch Loss: 0.08169142156839371\n",
      "Epoch 2267, Loss: 0.47111402451992035, Final Batch Loss: 0.11943142116069794\n",
      "Epoch 2268, Loss: 0.4871094226837158, Final Batch Loss: 0.20409035682678223\n",
      "Epoch 2269, Loss: 0.4700968787074089, Final Batch Loss: 0.16314281523227692\n",
      "Epoch 2270, Loss: 0.3910491392016411, Final Batch Loss: 0.09356329590082169\n",
      "Epoch 2271, Loss: 0.4039399027824402, Final Batch Loss: 0.07861396670341492\n",
      "Epoch 2272, Loss: 0.45874281972646713, Final Batch Loss: 0.11659612506628036\n",
      "Epoch 2273, Loss: 0.4268478825688362, Final Batch Loss: 0.07654371112585068\n",
      "Epoch 2274, Loss: 0.7393114417791367, Final Batch Loss: 0.36632880568504333\n",
      "Epoch 2275, Loss: 0.4975512772798538, Final Batch Loss: 0.19642066955566406\n",
      "Epoch 2276, Loss: 0.4695872813463211, Final Batch Loss: 0.14797943830490112\n",
      "Epoch 2277, Loss: 0.41256140917539597, Final Batch Loss: 0.13440658152103424\n",
      "Epoch 2278, Loss: 0.5177360028028488, Final Batch Loss: 0.16332745552062988\n",
      "Epoch 2279, Loss: 0.6986231952905655, Final Batch Loss: 0.3723888099193573\n",
      "Epoch 2280, Loss: 0.4620012491941452, Final Batch Loss: 0.11109177768230438\n",
      "Epoch 2281, Loss: 0.5060961246490479, Final Batch Loss: 0.18030603229999542\n",
      "Epoch 2282, Loss: 0.4595199152827263, Final Batch Loss: 0.11728932708501816\n",
      "Epoch 2283, Loss: 0.38798899203538895, Final Batch Loss: 0.09944585710763931\n",
      "Epoch 2284, Loss: 0.5102090984582901, Final Batch Loss: 0.15124468505382538\n",
      "Epoch 2285, Loss: 0.4124901294708252, Final Batch Loss: 0.07772909104824066\n",
      "Epoch 2286, Loss: 0.4556461423635483, Final Batch Loss: 0.18103353679180145\n",
      "Epoch 2287, Loss: 0.46287502348423004, Final Batch Loss: 0.21771639585494995\n",
      "Epoch 2288, Loss: 0.45875564217567444, Final Batch Loss: 0.1626790463924408\n",
      "Epoch 2289, Loss: 0.44029679894447327, Final Batch Loss: 0.11025956273078918\n",
      "Epoch 2290, Loss: 0.4225967675447464, Final Batch Loss: 0.05603235214948654\n",
      "Epoch 2291, Loss: 0.5283325463533401, Final Batch Loss: 0.21373426914215088\n",
      "Epoch 2292, Loss: 0.38766638934612274, Final Batch Loss: 0.1507628709077835\n",
      "Epoch 2293, Loss: 0.4610682725906372, Final Batch Loss: 0.11693155765533447\n",
      "Epoch 2294, Loss: 0.4856722205877304, Final Batch Loss: 0.18384253978729248\n",
      "Epoch 2295, Loss: 0.4855217784643173, Final Batch Loss: 0.19349399209022522\n",
      "Epoch 2296, Loss: 0.47229473292827606, Final Batch Loss: 0.2055925577878952\n",
      "Epoch 2297, Loss: 0.5405407696962357, Final Batch Loss: 0.21180738508701324\n",
      "Epoch 2298, Loss: 0.482272669672966, Final Batch Loss: 0.18712814152240753\n",
      "Epoch 2299, Loss: 0.36392559856176376, Final Batch Loss: 0.07964249700307846\n",
      "Epoch 2300, Loss: 0.42318397760391235, Final Batch Loss: 0.14952151477336884\n",
      "Epoch 2301, Loss: 0.4926530569791794, Final Batch Loss: 0.12753885984420776\n",
      "Epoch 2302, Loss: 0.47061821073293686, Final Batch Loss: 0.13798776268959045\n",
      "Epoch 2303, Loss: 0.4638182669878006, Final Batch Loss: 0.15655267238616943\n",
      "Epoch 2304, Loss: 0.6308313757181168, Final Batch Loss: 0.19202294945716858\n",
      "Epoch 2305, Loss: 0.4596242979168892, Final Batch Loss: 0.12369278818368912\n",
      "Epoch 2306, Loss: 0.46348895132541656, Final Batch Loss: 0.16609488427639008\n",
      "Epoch 2307, Loss: 0.49756552278995514, Final Batch Loss: 0.1970299482345581\n",
      "Epoch 2308, Loss: 0.45488370954990387, Final Batch Loss: 0.16465593874454498\n",
      "Epoch 2309, Loss: 0.500419095158577, Final Batch Loss: 0.18210718035697937\n",
      "Epoch 2310, Loss: 0.43357910960912704, Final Batch Loss: 0.12429117411375046\n",
      "Epoch 2311, Loss: 0.5233547985553741, Final Batch Loss: 0.2596227824687958\n",
      "Epoch 2312, Loss: 0.4425496608018875, Final Batch Loss: 0.13250324130058289\n",
      "Epoch 2313, Loss: 0.42691564559936523, Final Batch Loss: 0.10814650356769562\n",
      "Epoch 2314, Loss: 0.41565749794244766, Final Batch Loss: 0.10526316612958908\n",
      "Epoch 2315, Loss: 0.45829060673713684, Final Batch Loss: 0.1708870381116867\n",
      "Epoch 2316, Loss: 0.44724220037460327, Final Batch Loss: 0.1554652601480484\n",
      "Epoch 2317, Loss: 0.453955315053463, Final Batch Loss: 0.13249771296977997\n",
      "Epoch 2318, Loss: 0.7962890043854713, Final Batch Loss: 0.5092220306396484\n",
      "Epoch 2319, Loss: 0.5313427597284317, Final Batch Loss: 0.1355515867471695\n",
      "Epoch 2320, Loss: 0.4821188896894455, Final Batch Loss: 0.1325899213552475\n",
      "Epoch 2321, Loss: 0.4612448364496231, Final Batch Loss: 0.13596612215042114\n",
      "Epoch 2322, Loss: 0.5177081525325775, Final Batch Loss: 0.17748713493347168\n",
      "Epoch 2323, Loss: 0.39475778117775917, Final Batch Loss: 0.052759308367967606\n",
      "Epoch 2324, Loss: 0.45086581259965897, Final Batch Loss: 0.0801185593008995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2325, Loss: 0.4938662350177765, Final Batch Loss: 0.20572732388973236\n",
      "Epoch 2326, Loss: 0.5096866339445114, Final Batch Loss: 0.15468770265579224\n",
      "Epoch 2327, Loss: 0.4923558235168457, Final Batch Loss: 0.1728735715150833\n",
      "Epoch 2328, Loss: 0.45427270233631134, Final Batch Loss: 0.15988004207611084\n",
      "Epoch 2329, Loss: 0.39293598383665085, Final Batch Loss: 0.11107359081506729\n",
      "Epoch 2330, Loss: 0.45265285670757294, Final Batch Loss: 0.12659738957881927\n",
      "Epoch 2331, Loss: 0.4155648946762085, Final Batch Loss: 0.1329677402973175\n",
      "Epoch 2332, Loss: 0.4735541045665741, Final Batch Loss: 0.14188247919082642\n",
      "Epoch 2333, Loss: 0.49440933018922806, Final Batch Loss: 0.12010476738214493\n",
      "Epoch 2334, Loss: 0.4734557196497917, Final Batch Loss: 0.11274468153715134\n",
      "Epoch 2335, Loss: 0.4833315387368202, Final Batch Loss: 0.0734400525689125\n",
      "Epoch 2336, Loss: 0.4892798066139221, Final Batch Loss: 0.18534469604492188\n",
      "Epoch 2337, Loss: 0.4213665649294853, Final Batch Loss: 0.10926548391580582\n",
      "Epoch 2338, Loss: 0.42764638364315033, Final Batch Loss: 0.15677081048488617\n",
      "Epoch 2339, Loss: 0.47428739070892334, Final Batch Loss: 0.17178498208522797\n",
      "Epoch 2340, Loss: 0.48236867785453796, Final Batch Loss: 0.15225274860858917\n",
      "Epoch 2341, Loss: 0.38811466842889786, Final Batch Loss: 0.12169383466243744\n",
      "Epoch 2342, Loss: 0.48130957782268524, Final Batch Loss: 0.18055002391338348\n",
      "Epoch 2343, Loss: 0.39845137298107147, Final Batch Loss: 0.12199699878692627\n",
      "Epoch 2344, Loss: 0.45819246768951416, Final Batch Loss: 0.13566170632839203\n",
      "Epoch 2345, Loss: 0.49226924777030945, Final Batch Loss: 0.1964552402496338\n",
      "Epoch 2346, Loss: 0.4646834209561348, Final Batch Loss: 0.0961468294262886\n",
      "Epoch 2347, Loss: 0.3596802055835724, Final Batch Loss: 0.08825606107711792\n",
      "Epoch 2348, Loss: 0.4859673082828522, Final Batch Loss: 0.16371877491474152\n",
      "Epoch 2349, Loss: 0.4533941298723221, Final Batch Loss: 0.1481611132621765\n",
      "Epoch 2350, Loss: 0.38356616348028183, Final Batch Loss: 0.09021066874265671\n",
      "Epoch 2351, Loss: 0.42363395541906357, Final Batch Loss: 0.15514223277568817\n",
      "Epoch 2352, Loss: 0.4196124002337456, Final Batch Loss: 0.11216788738965988\n",
      "Epoch 2353, Loss: 0.5159625560045242, Final Batch Loss: 0.1596151888370514\n",
      "Epoch 2354, Loss: 0.436926044523716, Final Batch Loss: 0.08609075099229813\n",
      "Epoch 2355, Loss: 0.436619333922863, Final Batch Loss: 0.11888471990823746\n",
      "Epoch 2356, Loss: 0.3676711544394493, Final Batch Loss: 0.07167912274599075\n",
      "Epoch 2357, Loss: 0.49982626736164093, Final Batch Loss: 0.1446809619665146\n",
      "Epoch 2358, Loss: 0.3838730528950691, Final Batch Loss: 0.0831916555762291\n",
      "Epoch 2359, Loss: 0.5659711062908173, Final Batch Loss: 0.19151075184345245\n",
      "Epoch 2360, Loss: 0.38245318084955215, Final Batch Loss: 0.11737213283777237\n",
      "Epoch 2361, Loss: 0.5523967519402504, Final Batch Loss: 0.2680347263813019\n",
      "Epoch 2362, Loss: 0.5002792179584503, Final Batch Loss: 0.1292710304260254\n",
      "Epoch 2363, Loss: 0.525316521525383, Final Batch Loss: 0.1971677839756012\n",
      "Epoch 2364, Loss: 0.4491254687309265, Final Batch Loss: 0.1610189974308014\n",
      "Epoch 2365, Loss: 0.5002169609069824, Final Batch Loss: 0.1825803965330124\n",
      "Epoch 2366, Loss: 0.41268808394670486, Final Batch Loss: 0.13706724345684052\n",
      "Epoch 2367, Loss: 0.4615485519170761, Final Batch Loss: 0.11282479763031006\n",
      "Epoch 2368, Loss: 0.42522913217544556, Final Batch Loss: 0.139919251203537\n",
      "Epoch 2369, Loss: 0.532266654074192, Final Batch Loss: 0.06764145940542221\n",
      "Epoch 2370, Loss: 0.4081341475248337, Final Batch Loss: 0.08251248300075531\n",
      "Epoch 2371, Loss: 0.4168122932314873, Final Batch Loss: 0.11800847947597504\n",
      "Epoch 2372, Loss: 0.5009406581521034, Final Batch Loss: 0.12127663940191269\n",
      "Epoch 2373, Loss: 0.4158250391483307, Final Batch Loss: 0.10854347050189972\n",
      "Epoch 2374, Loss: 0.3915255665779114, Final Batch Loss: 0.09892190247774124\n",
      "Epoch 2375, Loss: 0.5389847308397293, Final Batch Loss: 0.2655012905597687\n",
      "Epoch 2376, Loss: 0.42264626920223236, Final Batch Loss: 0.08933748304843903\n",
      "Epoch 2377, Loss: 0.4083154574036598, Final Batch Loss: 0.12482693046331406\n",
      "Epoch 2378, Loss: 0.47608088701963425, Final Batch Loss: 0.21853813529014587\n",
      "Epoch 2379, Loss: 0.4750167727470398, Final Batch Loss: 0.16423657536506653\n",
      "Epoch 2380, Loss: 0.41540075838565826, Final Batch Loss: 0.10193240642547607\n",
      "Epoch 2381, Loss: 0.5002382695674896, Final Batch Loss: 0.21959464251995087\n",
      "Epoch 2382, Loss: 0.3495854362845421, Final Batch Loss: 0.088718481361866\n",
      "Epoch 2383, Loss: 0.5200286880135536, Final Batch Loss: 0.24098855257034302\n",
      "Epoch 2384, Loss: 0.36717135459184647, Final Batch Loss: 0.08269289880990982\n",
      "Epoch 2385, Loss: 0.4499887526035309, Final Batch Loss: 0.13839271664619446\n",
      "Epoch 2386, Loss: 0.4398837089538574, Final Batch Loss: 0.10976488888263702\n",
      "Epoch 2387, Loss: 0.3933805823326111, Final Batch Loss: 0.11733576655387878\n",
      "Epoch 2388, Loss: 0.44449543952941895, Final Batch Loss: 0.12775810062885284\n",
      "Epoch 2389, Loss: 0.45819056779146194, Final Batch Loss: 0.11432898789644241\n",
      "Epoch 2390, Loss: 0.5770489871501923, Final Batch Loss: 0.25774264335632324\n",
      "Epoch 2391, Loss: 0.5682581663131714, Final Batch Loss: 0.21362629532814026\n",
      "Epoch 2392, Loss: 0.4823343902826309, Final Batch Loss: 0.1441180557012558\n",
      "Epoch 2393, Loss: 0.46211928129196167, Final Batch Loss: 0.13307997584342957\n",
      "Epoch 2394, Loss: 0.5331628769636154, Final Batch Loss: 0.15240426361560822\n",
      "Epoch 2395, Loss: 0.5600295066833496, Final Batch Loss: 0.287477046251297\n",
      "Epoch 2396, Loss: 0.5736528784036636, Final Batch Loss: 0.2901269793510437\n",
      "Epoch 2397, Loss: 0.4704800397157669, Final Batch Loss: 0.1347929686307907\n",
      "Epoch 2398, Loss: 0.6056492775678635, Final Batch Loss: 0.2784016728401184\n",
      "Epoch 2399, Loss: 0.4424039274454117, Final Batch Loss: 0.09781086444854736\n",
      "Epoch 2400, Loss: 0.5919363796710968, Final Batch Loss: 0.26303738355636597\n",
      "Epoch 2401, Loss: 0.4856456369161606, Final Batch Loss: 0.18825016915798187\n",
      "Epoch 2402, Loss: 0.39375463128089905, Final Batch Loss: 0.12349863350391388\n",
      "Epoch 2403, Loss: 0.48438912630081177, Final Batch Loss: 0.23566347360610962\n",
      "Epoch 2404, Loss: 0.4526887536048889, Final Batch Loss: 0.1592150628566742\n",
      "Epoch 2405, Loss: 0.5211421176791191, Final Batch Loss: 0.1972118318080902\n",
      "Epoch 2406, Loss: 0.4453832060098648, Final Batch Loss: 0.13144396245479584\n",
      "Epoch 2407, Loss: 0.5467236340045929, Final Batch Loss: 0.2070840299129486\n",
      "Epoch 2408, Loss: 0.4287091940641403, Final Batch Loss: 0.12790143489837646\n",
      "Epoch 2409, Loss: 0.4888514131307602, Final Batch Loss: 0.14400067925453186\n",
      "Epoch 2410, Loss: 0.3558262884616852, Final Batch Loss: 0.046371400356292725\n",
      "Epoch 2411, Loss: 0.5536176711320877, Final Batch Loss: 0.2042846530675888\n",
      "Epoch 2412, Loss: 0.439990758895874, Final Batch Loss: 0.10625651478767395\n",
      "Epoch 2413, Loss: 0.5895424783229828, Final Batch Loss: 0.17429287731647491\n",
      "Epoch 2414, Loss: 0.49687230587005615, Final Batch Loss: 0.12901635468006134\n",
      "Epoch 2415, Loss: 0.40967823565006256, Final Batch Loss: 0.09646861255168915\n",
      "Epoch 2416, Loss: 0.5429528802633286, Final Batch Loss: 0.22045491635799408\n",
      "Epoch 2417, Loss: 0.4387856498360634, Final Batch Loss: 0.11696428805589676\n",
      "Epoch 2418, Loss: 0.4579707682132721, Final Batch Loss: 0.15895213186740875\n",
      "Epoch 2419, Loss: 0.4395964592695236, Final Batch Loss: 0.10743537545204163\n",
      "Epoch 2420, Loss: 0.4873201698064804, Final Batch Loss: 0.14352965354919434\n",
      "Epoch 2421, Loss: 0.363724447786808, Final Batch Loss: 0.07731647044420242\n",
      "Epoch 2422, Loss: 0.46652625501155853, Final Batch Loss: 0.14142127335071564\n",
      "Epoch 2423, Loss: 0.5311377048492432, Final Batch Loss: 0.17471814155578613\n",
      "Epoch 2424, Loss: 0.524598553776741, Final Batch Loss: 0.1858745813369751\n",
      "Epoch 2425, Loss: 0.5987716317176819, Final Batch Loss: 0.27936920523643494\n",
      "Epoch 2426, Loss: 0.5021662265062332, Final Batch Loss: 0.14622150361537933\n",
      "Epoch 2427, Loss: 0.399591788649559, Final Batch Loss: 0.09806089103221893\n",
      "Epoch 2428, Loss: 0.42704957723617554, Final Batch Loss: 0.12931135296821594\n",
      "Epoch 2429, Loss: 0.5966488569974899, Final Batch Loss: 0.27690136432647705\n",
      "Epoch 2430, Loss: 0.3193150907754898, Final Batch Loss: 0.0635257214307785\n",
      "Epoch 2431, Loss: 0.489344522356987, Final Batch Loss: 0.19518733024597168\n",
      "Epoch 2432, Loss: 0.5319065600633621, Final Batch Loss: 0.16786666214466095\n",
      "Epoch 2433, Loss: 0.37130335345864296, Final Batch Loss: 0.04433843865990639\n",
      "Epoch 2434, Loss: 0.45966608822345734, Final Batch Loss: 0.16077177226543427\n",
      "Epoch 2435, Loss: 0.4199723079800606, Final Batch Loss: 0.09995847195386887\n",
      "Epoch 2436, Loss: 0.42657608538866043, Final Batch Loss: 0.1236095204949379\n",
      "Epoch 2437, Loss: 0.4257977083325386, Final Batch Loss: 0.15106625854969025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2438, Loss: 0.4609062671661377, Final Batch Loss: 0.18381895124912262\n",
      "Epoch 2439, Loss: 0.4877002388238907, Final Batch Loss: 0.20262114703655243\n",
      "Epoch 2440, Loss: 0.8506750017404556, Final Batch Loss: 0.5689776539802551\n",
      "Epoch 2441, Loss: 0.4953639656305313, Final Batch Loss: 0.20876793563365936\n",
      "Epoch 2442, Loss: 0.5366385728120804, Final Batch Loss: 0.24845683574676514\n",
      "Epoch 2443, Loss: 0.5699691772460938, Final Batch Loss: 0.23956774175167084\n",
      "Epoch 2444, Loss: 0.4005950540304184, Final Batch Loss: 0.11088040471076965\n",
      "Epoch 2445, Loss: 0.5141143500804901, Final Batch Loss: 0.19466298818588257\n",
      "Epoch 2446, Loss: 0.4530339166522026, Final Batch Loss: 0.11061636358499527\n",
      "Epoch 2447, Loss: 0.48057207465171814, Final Batch Loss: 0.09801802039146423\n",
      "Epoch 2448, Loss: 0.4835344925522804, Final Batch Loss: 0.1059974953532219\n",
      "Epoch 2449, Loss: 0.453554704785347, Final Batch Loss: 0.10714639723300934\n",
      "Epoch 2450, Loss: 0.5026434659957886, Final Batch Loss: 0.21615590155124664\n",
      "Epoch 2451, Loss: 0.41913794726133347, Final Batch Loss: 0.12473716586828232\n",
      "Epoch 2452, Loss: 0.39904288947582245, Final Batch Loss: 0.12023089826107025\n",
      "Epoch 2453, Loss: 0.376383975148201, Final Batch Loss: 0.08662614226341248\n",
      "Epoch 2454, Loss: 0.42935752123594284, Final Batch Loss: 0.07283323258161545\n",
      "Epoch 2455, Loss: 0.468142032623291, Final Batch Loss: 0.1439281702041626\n",
      "Epoch 2456, Loss: 0.4900183081626892, Final Batch Loss: 0.1843900829553604\n",
      "Epoch 2457, Loss: 0.5468180626630783, Final Batch Loss: 0.24472108483314514\n",
      "Epoch 2458, Loss: 0.42111897468566895, Final Batch Loss: 0.09925609827041626\n",
      "Epoch 2459, Loss: 0.4926505833864212, Final Batch Loss: 0.11656172573566437\n",
      "Epoch 2460, Loss: 0.4414604902267456, Final Batch Loss: 0.14821386337280273\n",
      "Epoch 2461, Loss: 0.3846341744065285, Final Batch Loss: 0.12199213355779648\n",
      "Epoch 2462, Loss: 0.4978269040584564, Final Batch Loss: 0.19537466764450073\n",
      "Epoch 2463, Loss: 0.38959962874650955, Final Batch Loss: 0.07932362705469131\n",
      "Epoch 2464, Loss: 0.5401672422885895, Final Batch Loss: 0.20376524329185486\n",
      "Epoch 2465, Loss: 0.43859878927469254, Final Batch Loss: 0.0923420861363411\n",
      "Epoch 2466, Loss: 0.42277517914772034, Final Batch Loss: 0.10292591154575348\n",
      "Epoch 2467, Loss: 0.45464979112148285, Final Batch Loss: 0.20087388157844543\n",
      "Epoch 2468, Loss: 0.37757110595703125, Final Batch Loss: 0.10062874853610992\n",
      "Epoch 2469, Loss: 0.4254515990614891, Final Batch Loss: 0.17062370479106903\n",
      "Epoch 2470, Loss: 0.3775070458650589, Final Batch Loss: 0.07414397597312927\n",
      "Epoch 2471, Loss: 0.4370180517435074, Final Batch Loss: 0.09785601496696472\n",
      "Epoch 2472, Loss: 0.4747384935617447, Final Batch Loss: 0.19162863492965698\n",
      "Epoch 2473, Loss: 0.44787605106830597, Final Batch Loss: 0.1312071830034256\n",
      "Epoch 2474, Loss: 0.4376237541437149, Final Batch Loss: 0.1580839306116104\n",
      "Epoch 2475, Loss: 0.5601506382226944, Final Batch Loss: 0.21884265542030334\n",
      "Epoch 2476, Loss: 0.561990812420845, Final Batch Loss: 0.2903459966182709\n",
      "Epoch 2477, Loss: 0.40052369236946106, Final Batch Loss: 0.09974034130573273\n",
      "Epoch 2478, Loss: 0.47815505415201187, Final Batch Loss: 0.1044694259762764\n",
      "Epoch 2479, Loss: 0.5578935742378235, Final Batch Loss: 0.21561209857463837\n",
      "Epoch 2480, Loss: 0.4091791361570358, Final Batch Loss: 0.08354225754737854\n",
      "Epoch 2481, Loss: 0.6890217363834381, Final Batch Loss: 0.41079333424568176\n",
      "Epoch 2482, Loss: 0.37304313480854034, Final Batch Loss: 0.07891596853733063\n",
      "Epoch 2483, Loss: 0.6278331130743027, Final Batch Loss: 0.2949415445327759\n",
      "Epoch 2484, Loss: 0.4587775021791458, Final Batch Loss: 0.14157046377658844\n",
      "Epoch 2485, Loss: 0.4214756339788437, Final Batch Loss: 0.1210048645734787\n",
      "Epoch 2486, Loss: 0.3486699312925339, Final Batch Loss: 0.050641387701034546\n",
      "Epoch 2487, Loss: 0.47594258189201355, Final Batch Loss: 0.1548251360654831\n",
      "Epoch 2488, Loss: 0.5126481056213379, Final Batch Loss: 0.25192758440971375\n",
      "Epoch 2489, Loss: 0.4737066328525543, Final Batch Loss: 0.13634435832500458\n",
      "Epoch 2490, Loss: 0.4352201744914055, Final Batch Loss: 0.11264117807149887\n",
      "Epoch 2491, Loss: 0.3961024731397629, Final Batch Loss: 0.09963972866535187\n",
      "Epoch 2492, Loss: 0.40201741456985474, Final Batch Loss: 0.10256697237491608\n",
      "Epoch 2493, Loss: 0.3732263743877411, Final Batch Loss: 0.08510726690292358\n",
      "Epoch 2494, Loss: 0.44412216544151306, Final Batch Loss: 0.15531280636787415\n",
      "Epoch 2495, Loss: 0.530900627374649, Final Batch Loss: 0.23331806063652039\n",
      "Epoch 2496, Loss: 0.3647000268101692, Final Batch Loss: 0.10503845661878586\n",
      "Epoch 2497, Loss: 0.43995678424835205, Final Batch Loss: 0.13066208362579346\n",
      "Epoch 2498, Loss: 0.386014461517334, Final Batch Loss: 0.07653260231018066\n",
      "Epoch 2499, Loss: 0.5104401111602783, Final Batch Loss: 0.1363356113433838\n",
      "Epoch 2500, Loss: 0.450086772441864, Final Batch Loss: 0.13073305785655975\n",
      "Epoch 2501, Loss: 0.40286653488874435, Final Batch Loss: 0.10463451594114304\n",
      "Epoch 2502, Loss: 0.4516707956790924, Final Batch Loss: 0.12710385024547577\n",
      "Epoch 2503, Loss: 0.4986201673746109, Final Batch Loss: 0.19825568795204163\n",
      "Epoch 2504, Loss: 0.4511976093053818, Final Batch Loss: 0.19947127997875214\n",
      "Epoch 2505, Loss: 0.7021593153476715, Final Batch Loss: 0.35455232858657837\n",
      "Epoch 2506, Loss: 0.4206409156322479, Final Batch Loss: 0.10406006872653961\n",
      "Epoch 2507, Loss: 0.5053066611289978, Final Batch Loss: 0.14018292725086212\n",
      "Epoch 2508, Loss: 0.4684690237045288, Final Batch Loss: 0.14779061079025269\n",
      "Epoch 2509, Loss: 0.5064930319786072, Final Batch Loss: 0.1766815334558487\n",
      "Epoch 2510, Loss: 0.4273065775632858, Final Batch Loss: 0.14486366510391235\n",
      "Epoch 2511, Loss: 0.5039397552609444, Final Batch Loss: 0.20816175639629364\n",
      "Epoch 2512, Loss: 0.4400929883122444, Final Batch Loss: 0.08241920918226242\n",
      "Epoch 2513, Loss: 0.4179980307817459, Final Batch Loss: 0.09443461894989014\n",
      "Epoch 2514, Loss: 0.35775718092918396, Final Batch Loss: 0.12696848809719086\n",
      "Epoch 2515, Loss: 0.47434088587760925, Final Batch Loss: 0.16299641132354736\n",
      "Epoch 2516, Loss: 0.4041699916124344, Final Batch Loss: 0.1629924476146698\n",
      "Epoch 2517, Loss: 0.41778384894132614, Final Batch Loss: 0.10607897490262985\n",
      "Epoch 2518, Loss: 0.4902474731206894, Final Batch Loss: 0.23885434865951538\n",
      "Epoch 2519, Loss: 0.3579212948679924, Final Batch Loss: 0.07906067371368408\n",
      "Epoch 2520, Loss: 0.44591037929058075, Final Batch Loss: 0.14177779853343964\n",
      "Epoch 2521, Loss: 0.37667684257030487, Final Batch Loss: 0.09942702203989029\n",
      "Epoch 2522, Loss: 0.5846394151449203, Final Batch Loss: 0.24235805869102478\n",
      "Epoch 2523, Loss: 0.4136952757835388, Final Batch Loss: 0.13652828335762024\n",
      "Epoch 2524, Loss: 0.40344756841659546, Final Batch Loss: 0.11016301810741425\n",
      "Epoch 2525, Loss: 0.39682547748088837, Final Batch Loss: 0.11063282191753387\n",
      "Epoch 2526, Loss: 0.6338971480727196, Final Batch Loss: 0.37477022409439087\n",
      "Epoch 2527, Loss: 0.40785180777311325, Final Batch Loss: 0.11362484842538834\n",
      "Epoch 2528, Loss: 0.42201193422079086, Final Batch Loss: 0.08916785567998886\n",
      "Epoch 2529, Loss: 0.5730622857809067, Final Batch Loss: 0.20450390875339508\n",
      "Epoch 2530, Loss: 0.3984801471233368, Final Batch Loss: 0.13106639683246613\n",
      "Epoch 2531, Loss: 0.42929643392562866, Final Batch Loss: 0.1504967212677002\n",
      "Epoch 2532, Loss: 0.42826981097459793, Final Batch Loss: 0.15765082836151123\n",
      "Epoch 2533, Loss: 0.3955729827284813, Final Batch Loss: 0.1517389416694641\n",
      "Epoch 2534, Loss: 0.4130222052335739, Final Batch Loss: 0.12888172268867493\n",
      "Epoch 2535, Loss: 0.3899773880839348, Final Batch Loss: 0.06608455628156662\n",
      "Epoch 2536, Loss: 0.5353793650865555, Final Batch Loss: 0.23631587624549866\n",
      "Epoch 2537, Loss: 0.4644789397716522, Final Batch Loss: 0.16732189059257507\n",
      "Epoch 2538, Loss: 0.5247355252504349, Final Batch Loss: 0.1987074762582779\n",
      "Epoch 2539, Loss: 0.41908034682273865, Final Batch Loss: 0.07373449206352234\n",
      "Epoch 2540, Loss: 0.42489826679229736, Final Batch Loss: 0.1167898178100586\n",
      "Epoch 2541, Loss: 0.3706425055861473, Final Batch Loss: 0.08895803987979889\n",
      "Epoch 2542, Loss: 0.3682363033294678, Final Batch Loss: 0.09346465766429901\n",
      "Epoch 2543, Loss: 0.39766260236501694, Final Batch Loss: 0.08804553002119064\n",
      "Epoch 2544, Loss: 0.561511442065239, Final Batch Loss: 0.27839404344558716\n",
      "Epoch 2545, Loss: 0.4251638948917389, Final Batch Loss: 0.11376772820949554\n",
      "Epoch 2546, Loss: 0.32034095376729965, Final Batch Loss: 0.06451154500246048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2547, Loss: 0.4597407430410385, Final Batch Loss: 0.16252493858337402\n",
      "Epoch 2548, Loss: 0.44751688838005066, Final Batch Loss: 0.18441849946975708\n",
      "Epoch 2549, Loss: 0.4281018376350403, Final Batch Loss: 0.16069436073303223\n",
      "Epoch 2550, Loss: 0.6310714483261108, Final Batch Loss: 0.3741002678871155\n",
      "Epoch 2551, Loss: 0.4228513240814209, Final Batch Loss: 0.07769949734210968\n",
      "Epoch 2552, Loss: 0.4168155938386917, Final Batch Loss: 0.1279832124710083\n",
      "Epoch 2553, Loss: 0.3898528888821602, Final Batch Loss: 0.0938752219080925\n",
      "Epoch 2554, Loss: 0.41062095016241074, Final Batch Loss: 0.11462827771902084\n",
      "Epoch 2555, Loss: 0.37967873364686966, Final Batch Loss: 0.11112283915281296\n",
      "Epoch 2556, Loss: 0.46394775062799454, Final Batch Loss: 0.1379518061876297\n",
      "Epoch 2557, Loss: 0.5151195004582405, Final Batch Loss: 0.17454515397548676\n",
      "Epoch 2558, Loss: 0.43627213686704636, Final Batch Loss: 0.1835927963256836\n",
      "Epoch 2559, Loss: 0.4343222975730896, Final Batch Loss: 0.0767231285572052\n",
      "Epoch 2560, Loss: 0.5790697187185287, Final Batch Loss: 0.22351062297821045\n",
      "Epoch 2561, Loss: 0.4704194813966751, Final Batch Loss: 0.1472797840833664\n",
      "Epoch 2562, Loss: 0.39927370101213455, Final Batch Loss: 0.1367443948984146\n",
      "Epoch 2563, Loss: 0.4468192979693413, Final Batch Loss: 0.11111415177583694\n",
      "Epoch 2564, Loss: 0.4238733798265457, Final Batch Loss: 0.13988445699214935\n",
      "Epoch 2565, Loss: 0.3798428773880005, Final Batch Loss: 0.11297929286956787\n",
      "Epoch 2566, Loss: 0.4746747612953186, Final Batch Loss: 0.16720226407051086\n",
      "Epoch 2567, Loss: 0.45327383279800415, Final Batch Loss: 0.16052377223968506\n",
      "Epoch 2568, Loss: 0.3997954800724983, Final Batch Loss: 0.1050620749592781\n",
      "Epoch 2569, Loss: 0.37553736567497253, Final Batch Loss: 0.12465760856866837\n",
      "Epoch 2570, Loss: 0.4071865528821945, Final Batch Loss: 0.11431574076414108\n",
      "Epoch 2571, Loss: 0.4274675101041794, Final Batch Loss: 0.15618683397769928\n",
      "Epoch 2572, Loss: 0.4527138024568558, Final Batch Loss: 0.1665390133857727\n",
      "Epoch 2573, Loss: 0.395283080637455, Final Batch Loss: 0.10634250193834305\n",
      "Epoch 2574, Loss: 0.5413154661655426, Final Batch Loss: 0.2201433777809143\n",
      "Epoch 2575, Loss: 0.6180923581123352, Final Batch Loss: 0.3426664173603058\n",
      "Epoch 2576, Loss: 0.403516985476017, Final Batch Loss: 0.10362508147954941\n",
      "Epoch 2577, Loss: 0.4821958541870117, Final Batch Loss: 0.20160654187202454\n",
      "Epoch 2578, Loss: 0.4220094829797745, Final Batch Loss: 0.14089523255825043\n",
      "Epoch 2579, Loss: 0.4697142690420151, Final Batch Loss: 0.1966971606016159\n",
      "Epoch 2580, Loss: 0.4085606038570404, Final Batch Loss: 0.1434408575296402\n",
      "Epoch 2581, Loss: 0.33585505187511444, Final Batch Loss: 0.08408677577972412\n",
      "Epoch 2582, Loss: 0.4671386629343033, Final Batch Loss: 0.11234921216964722\n",
      "Epoch 2583, Loss: 0.5383836179971695, Final Batch Loss: 0.20137305557727814\n",
      "Epoch 2584, Loss: 0.45333458483219147, Final Batch Loss: 0.16430701315402985\n",
      "Epoch 2585, Loss: 0.36615102738142014, Final Batch Loss: 0.07865474373102188\n",
      "Epoch 2586, Loss: 0.3584453910589218, Final Batch Loss: 0.10618393123149872\n",
      "Epoch 2587, Loss: 0.5820685103535652, Final Batch Loss: 0.24708491563796997\n",
      "Epoch 2588, Loss: 0.3779933601617813, Final Batch Loss: 0.10956928879022598\n",
      "Epoch 2589, Loss: 0.3767051100730896, Final Batch Loss: 0.05580875277519226\n",
      "Epoch 2590, Loss: 0.5210659503936768, Final Batch Loss: 0.21705812215805054\n",
      "Epoch 2591, Loss: 0.635755717754364, Final Batch Loss: 0.29406359791755676\n",
      "Epoch 2592, Loss: 0.4016820937395096, Final Batch Loss: 0.04893043637275696\n",
      "Epoch 2593, Loss: 0.4748508334159851, Final Batch Loss: 0.16124653816223145\n",
      "Epoch 2594, Loss: 0.3440981097519398, Final Batch Loss: 0.039547327905893326\n",
      "Epoch 2595, Loss: 0.43539929389953613, Final Batch Loss: 0.13913226127624512\n",
      "Epoch 2596, Loss: 0.5596381574869156, Final Batch Loss: 0.2600992023944855\n",
      "Epoch 2597, Loss: 0.4783187508583069, Final Batch Loss: 0.10552829504013062\n",
      "Epoch 2598, Loss: 0.39033395051956177, Final Batch Loss: 0.07760113477706909\n",
      "Epoch 2599, Loss: 0.5534674227237701, Final Batch Loss: 0.21951033174991608\n",
      "Epoch 2600, Loss: 0.3356415331363678, Final Batch Loss: 0.06310789287090302\n",
      "Epoch 2601, Loss: 0.3877914398908615, Final Batch Loss: 0.11371879279613495\n",
      "Epoch 2602, Loss: 0.4839537739753723, Final Batch Loss: 0.2091902196407318\n",
      "Epoch 2603, Loss: 0.44172532856464386, Final Batch Loss: 0.13995784521102905\n",
      "Epoch 2604, Loss: 0.5034746825695038, Final Batch Loss: 0.24024370312690735\n",
      "Epoch 2605, Loss: 0.43211059272289276, Final Batch Loss: 0.12886549532413483\n",
      "Epoch 2606, Loss: 0.45004042237997055, Final Batch Loss: 0.11566618829965591\n",
      "Epoch 2607, Loss: 0.44124965369701385, Final Batch Loss: 0.17690452933311462\n",
      "Epoch 2608, Loss: 0.3661127910017967, Final Batch Loss: 0.09207461029291153\n",
      "Epoch 2609, Loss: 0.50608991086483, Final Batch Loss: 0.12252731621265411\n",
      "Epoch 2610, Loss: 0.4142296612262726, Final Batch Loss: 0.11981788277626038\n",
      "Epoch 2611, Loss: 0.42504921555519104, Final Batch Loss: 0.08629181981086731\n",
      "Epoch 2612, Loss: 0.5362277179956436, Final Batch Loss: 0.1744503229856491\n",
      "Epoch 2613, Loss: 0.4259217754006386, Final Batch Loss: 0.11890331655740738\n",
      "Epoch 2614, Loss: 0.41215136647224426, Final Batch Loss: 0.13056686520576477\n",
      "Epoch 2615, Loss: 0.4389605224132538, Final Batch Loss: 0.13827922940254211\n",
      "Epoch 2616, Loss: 0.3637673556804657, Final Batch Loss: 0.07806555926799774\n",
      "Epoch 2617, Loss: 0.4616023302078247, Final Batch Loss: 0.11943896859884262\n",
      "Epoch 2618, Loss: 0.33427713066339493, Final Batch Loss: 0.07917002588510513\n",
      "Epoch 2619, Loss: 0.4634465128183365, Final Batch Loss: 0.15508925914764404\n",
      "Epoch 2620, Loss: 0.38693858683109283, Final Batch Loss: 0.08475157618522644\n",
      "Epoch 2621, Loss: 0.5187607407569885, Final Batch Loss: 0.21308499574661255\n",
      "Epoch 2622, Loss: 0.4414539188146591, Final Batch Loss: 0.1381242573261261\n",
      "Epoch 2623, Loss: 0.4864956736564636, Final Batch Loss: 0.1044817715883255\n",
      "Epoch 2624, Loss: 0.4611218422651291, Final Batch Loss: 0.12109550833702087\n",
      "Epoch 2625, Loss: 0.436821848154068, Final Batch Loss: 0.14139780402183533\n",
      "Epoch 2626, Loss: 0.3362988941371441, Final Batch Loss: 0.05837954208254814\n",
      "Epoch 2627, Loss: 0.4955134391784668, Final Batch Loss: 0.1892460435628891\n",
      "Epoch 2628, Loss: 0.39205214381217957, Final Batch Loss: 0.13397139310836792\n",
      "Epoch 2629, Loss: 0.4886734187602997, Final Batch Loss: 0.1594102680683136\n",
      "Epoch 2630, Loss: 0.5620164573192596, Final Batch Loss: 0.23997056484222412\n",
      "Epoch 2631, Loss: 0.4395073130726814, Final Batch Loss: 0.10591145604848862\n",
      "Epoch 2632, Loss: 0.526103600859642, Final Batch Loss: 0.18385611474514008\n",
      "Epoch 2633, Loss: 0.40626469254493713, Final Batch Loss: 0.12502489984035492\n",
      "Epoch 2634, Loss: 0.4508221745491028, Final Batch Loss: 0.10726273059844971\n",
      "Epoch 2635, Loss: 0.593347892165184, Final Batch Loss: 0.22172050178050995\n",
      "Epoch 2636, Loss: 0.35981981456279755, Final Batch Loss: 0.09004206210374832\n",
      "Epoch 2637, Loss: 0.43407899886369705, Final Batch Loss: 0.08835042268037796\n",
      "Epoch 2638, Loss: 0.4403192922472954, Final Batch Loss: 0.1416740119457245\n",
      "Epoch 2639, Loss: 0.4624371752142906, Final Batch Loss: 0.12107165902853012\n",
      "Epoch 2640, Loss: 0.5000760704278946, Final Batch Loss: 0.15334552526474\n",
      "Epoch 2641, Loss: 0.4059111848473549, Final Batch Loss: 0.12625984847545624\n",
      "Epoch 2642, Loss: 0.3880104050040245, Final Batch Loss: 0.15623074769973755\n",
      "Epoch 2643, Loss: 0.4465550184249878, Final Batch Loss: 0.17969873547554016\n",
      "Epoch 2644, Loss: 0.34960731118917465, Final Batch Loss: 0.08052773028612137\n",
      "Epoch 2645, Loss: 0.4036543741822243, Final Batch Loss: 0.14450225234031677\n",
      "Epoch 2646, Loss: 0.3703818619251251, Final Batch Loss: 0.1380714625120163\n",
      "Epoch 2647, Loss: 0.3435221463441849, Final Batch Loss: 0.0744120180606842\n",
      "Epoch 2648, Loss: 0.4259127005934715, Final Batch Loss: 0.09294819086790085\n",
      "Epoch 2649, Loss: 0.35917994379997253, Final Batch Loss: 0.10059846937656403\n",
      "Epoch 2650, Loss: 0.3474671244621277, Final Batch Loss: 0.09326127916574478\n",
      "Epoch 2651, Loss: 0.43991251289844513, Final Batch Loss: 0.15432046353816986\n",
      "Epoch 2652, Loss: 0.47111619263887405, Final Batch Loss: 0.09083432704210281\n",
      "Epoch 2653, Loss: 0.45072805881500244, Final Batch Loss: 0.1669933795928955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2654, Loss: 0.41327909380197525, Final Batch Loss: 0.11096841096878052\n",
      "Epoch 2655, Loss: 0.42837005853652954, Final Batch Loss: 0.07902127504348755\n",
      "Epoch 2656, Loss: 0.37591810524463654, Final Batch Loss: 0.09348583221435547\n",
      "Epoch 2657, Loss: 0.5621110498905182, Final Batch Loss: 0.2720491886138916\n",
      "Epoch 2658, Loss: 0.4713190719485283, Final Batch Loss: 0.2316790074110031\n",
      "Epoch 2659, Loss: 0.4728876054286957, Final Batch Loss: 0.14369511604309082\n",
      "Epoch 2660, Loss: 0.5550961047410965, Final Batch Loss: 0.21591955423355103\n",
      "Epoch 2661, Loss: 0.3825976252555847, Final Batch Loss: 0.08976081013679504\n",
      "Epoch 2662, Loss: 0.37519627064466476, Final Batch Loss: 0.06894116848707199\n",
      "Epoch 2663, Loss: 0.4144672527909279, Final Batch Loss: 0.14042973518371582\n",
      "Epoch 2664, Loss: 0.38974854350090027, Final Batch Loss: 0.07670722901821136\n",
      "Epoch 2665, Loss: 0.3720967695116997, Final Batch Loss: 0.10627102106809616\n",
      "Epoch 2666, Loss: 0.3580688536167145, Final Batch Loss: 0.0960204005241394\n",
      "Epoch 2667, Loss: 0.3909375071525574, Final Batch Loss: 0.07116109877824783\n",
      "Epoch 2668, Loss: 0.38286519795656204, Final Batch Loss: 0.04449228197336197\n",
      "Epoch 2669, Loss: 0.41500166058540344, Final Batch Loss: 0.1409265547990799\n",
      "Epoch 2670, Loss: 0.5255430042743683, Final Batch Loss: 0.21205495297908783\n",
      "Epoch 2671, Loss: 0.4337804988026619, Final Batch Loss: 0.16373787820339203\n",
      "Epoch 2672, Loss: 0.42589762806892395, Final Batch Loss: 0.10974735021591187\n",
      "Epoch 2673, Loss: 0.406510166823864, Final Batch Loss: 0.10124457627534866\n",
      "Epoch 2674, Loss: 0.42775050550699234, Final Batch Loss: 0.11121473461389542\n",
      "Epoch 2675, Loss: 0.3680821657180786, Final Batch Loss: 0.13793791830539703\n",
      "Epoch 2676, Loss: 0.3349235877394676, Final Batch Loss: 0.08969675004482269\n",
      "Epoch 2677, Loss: 0.3772883787751198, Final Batch Loss: 0.08103538304567337\n",
      "Epoch 2678, Loss: 0.3957781344652176, Final Batch Loss: 0.09062404930591583\n",
      "Epoch 2679, Loss: 0.38076867908239365, Final Batch Loss: 0.11307763308286667\n",
      "Epoch 2680, Loss: 0.40153272449970245, Final Batch Loss: 0.12407884001731873\n",
      "Epoch 2681, Loss: 0.46842847764492035, Final Batch Loss: 0.20121842622756958\n",
      "Epoch 2682, Loss: 0.49111583828926086, Final Batch Loss: 0.206756591796875\n",
      "Epoch 2683, Loss: 0.41160838305950165, Final Batch Loss: 0.14011463522911072\n",
      "Epoch 2684, Loss: 0.4213593155145645, Final Batch Loss: 0.1245718002319336\n",
      "Epoch 2685, Loss: 0.3196581266820431, Final Batch Loss: 0.051486384123563766\n",
      "Epoch 2686, Loss: 0.33324868232011795, Final Batch Loss: 0.10378793627023697\n",
      "Epoch 2687, Loss: 0.30423878133296967, Final Batch Loss: 0.04811714589595795\n",
      "Epoch 2688, Loss: 0.31972838193178177, Final Batch Loss: 0.060119740664958954\n",
      "Epoch 2689, Loss: 0.40210939943790436, Final Batch Loss: 0.06724676489830017\n",
      "Epoch 2690, Loss: 0.5473460853099823, Final Batch Loss: 0.16381287574768066\n",
      "Epoch 2691, Loss: 0.4426166042685509, Final Batch Loss: 0.10079539567232132\n",
      "Epoch 2692, Loss: 0.4676213413476944, Final Batch Loss: 0.16658005118370056\n",
      "Epoch 2693, Loss: 0.3575151637196541, Final Batch Loss: 0.08039183169603348\n",
      "Epoch 2694, Loss: 0.3884088769555092, Final Batch Loss: 0.15880563855171204\n",
      "Epoch 2695, Loss: 0.38558878004550934, Final Batch Loss: 0.07612057030200958\n",
      "Epoch 2696, Loss: 0.5424427539110184, Final Batch Loss: 0.2248462438583374\n",
      "Epoch 2697, Loss: 0.48051996529102325, Final Batch Loss: 0.15642298758029938\n",
      "Epoch 2698, Loss: 0.3879532963037491, Final Batch Loss: 0.11278648674488068\n",
      "Epoch 2699, Loss: 0.6778396964073181, Final Batch Loss: 0.1604750007390976\n",
      "Epoch 2700, Loss: 0.4469396024942398, Final Batch Loss: 0.20230931043624878\n",
      "Epoch 2701, Loss: 0.5050419718027115, Final Batch Loss: 0.23171550035476685\n",
      "Epoch 2702, Loss: 0.5062601566314697, Final Batch Loss: 0.1986093521118164\n",
      "Epoch 2703, Loss: 0.3570469170808792, Final Batch Loss: 0.054371535778045654\n",
      "Epoch 2704, Loss: 0.37407802045345306, Final Batch Loss: 0.11241063475608826\n",
      "Epoch 2705, Loss: 0.4109770953655243, Final Batch Loss: 0.16705869138240814\n",
      "Epoch 2706, Loss: 0.43190740048885345, Final Batch Loss: 0.12732039391994476\n",
      "Epoch 2707, Loss: 0.505024254322052, Final Batch Loss: 0.17925052344799042\n",
      "Epoch 2708, Loss: 0.49205178022384644, Final Batch Loss: 0.17796868085861206\n",
      "Epoch 2709, Loss: 0.6756763607263565, Final Batch Loss: 0.3850895166397095\n",
      "Epoch 2710, Loss: 0.3941885232925415, Final Batch Loss: 0.08291402459144592\n",
      "Epoch 2711, Loss: 0.3505290746688843, Final Batch Loss: 0.07060255855321884\n",
      "Epoch 2712, Loss: 0.43721381574869156, Final Batch Loss: 0.09522009640932083\n",
      "Epoch 2713, Loss: 0.5048241466283798, Final Batch Loss: 0.20804114639759064\n",
      "Epoch 2714, Loss: 0.6559111177921295, Final Batch Loss: 0.4101112484931946\n",
      "Epoch 2715, Loss: 0.4266619309782982, Final Batch Loss: 0.11970024555921555\n",
      "Epoch 2716, Loss: 0.40162452310323715, Final Batch Loss: 0.13368777930736542\n",
      "Epoch 2717, Loss: 0.4595211297273636, Final Batch Loss: 0.2062794417142868\n",
      "Epoch 2718, Loss: 0.3577643409371376, Final Batch Loss: 0.083525650203228\n",
      "Epoch 2719, Loss: 0.4929850101470947, Final Batch Loss: 0.25269603729248047\n",
      "Epoch 2720, Loss: 0.5745750367641449, Final Batch Loss: 0.3250335156917572\n",
      "Epoch 2721, Loss: 0.4614991694688797, Final Batch Loss: 0.16016915440559387\n",
      "Epoch 2722, Loss: 0.42291082441806793, Final Batch Loss: 0.1338578760623932\n",
      "Epoch 2723, Loss: 0.4629286751151085, Final Batch Loss: 0.18907099962234497\n",
      "Epoch 2724, Loss: 0.3595191612839699, Final Batch Loss: 0.06056373566389084\n",
      "Epoch 2725, Loss: 0.5636536628007889, Final Batch Loss: 0.15182073414325714\n",
      "Epoch 2726, Loss: 0.3905676156282425, Final Batch Loss: 0.1331050544977188\n",
      "Epoch 2727, Loss: 0.39005032926797867, Final Batch Loss: 0.13226768374443054\n",
      "Epoch 2728, Loss: 0.45128224790096283, Final Batch Loss: 0.20196332037448883\n",
      "Epoch 2729, Loss: 0.4827081561088562, Final Batch Loss: 0.2243165224790573\n",
      "Epoch 2730, Loss: 0.31703510880470276, Final Batch Loss: 0.07106029242277145\n",
      "Epoch 2731, Loss: 0.37583231180906296, Final Batch Loss: 0.11883904039859772\n",
      "Epoch 2732, Loss: 0.34121013432741165, Final Batch Loss: 0.0899980291724205\n",
      "Epoch 2733, Loss: 0.5218088179826736, Final Batch Loss: 0.2577265799045563\n",
      "Epoch 2734, Loss: 0.48301833122968674, Final Batch Loss: 0.16385792195796967\n",
      "Epoch 2735, Loss: 0.37514252215623856, Final Batch Loss: 0.15578541159629822\n",
      "Epoch 2736, Loss: 0.5475669726729393, Final Batch Loss: 0.20971029996871948\n",
      "Epoch 2737, Loss: 0.430945061147213, Final Batch Loss: 0.1087469831109047\n",
      "Epoch 2738, Loss: 0.4428219348192215, Final Batch Loss: 0.1354718655347824\n",
      "Epoch 2739, Loss: 0.42053600400686264, Final Batch Loss: 0.13687129318714142\n",
      "Epoch 2740, Loss: 0.41054817289114, Final Batch Loss: 0.13306303322315216\n",
      "Epoch 2741, Loss: 0.546493411064148, Final Batch Loss: 0.16237692534923553\n",
      "Epoch 2742, Loss: 0.3357434682548046, Final Batch Loss: 0.05975419655442238\n",
      "Epoch 2743, Loss: 0.40337829291820526, Final Batch Loss: 0.11190798133611679\n",
      "Epoch 2744, Loss: 0.362553421407938, Final Batch Loss: 0.060224760323762894\n",
      "Epoch 2745, Loss: 0.43185393512248993, Final Batch Loss: 0.13575249910354614\n",
      "Epoch 2746, Loss: 0.5812041908502579, Final Batch Loss: 0.26991990208625793\n",
      "Epoch 2747, Loss: 0.4131300821900368, Final Batch Loss: 0.10593853145837784\n",
      "Epoch 2748, Loss: 0.5158248096704483, Final Batch Loss: 0.13053379952907562\n",
      "Epoch 2749, Loss: 0.4151586443185806, Final Batch Loss: 0.1494383066892624\n",
      "Epoch 2750, Loss: 0.39206670224666595, Final Batch Loss: 0.1408560574054718\n",
      "Epoch 2751, Loss: 0.46456898748874664, Final Batch Loss: 0.21118654310703278\n",
      "Epoch 2752, Loss: 0.35261379927396774, Final Batch Loss: 0.08067033439874649\n",
      "Epoch 2753, Loss: 0.47929228097200394, Final Batch Loss: 0.1787911206483841\n",
      "Epoch 2754, Loss: 0.46246079355478287, Final Batch Loss: 0.194004625082016\n",
      "Epoch 2755, Loss: 0.452648289501667, Final Batch Loss: 0.20853659510612488\n",
      "Epoch 2756, Loss: 0.5452235490083694, Final Batch Loss: 0.175810307264328\n",
      "Epoch 2757, Loss: 0.3694475218653679, Final Batch Loss: 0.12592679262161255\n",
      "Epoch 2758, Loss: 0.44107022881507874, Final Batch Loss: 0.07731416821479797\n",
      "Epoch 2759, Loss: 0.35603876411914825, Final Batch Loss: 0.10421119630336761\n",
      "Epoch 2760, Loss: 0.4788954481482506, Final Batch Loss: 0.21320252120494843\n",
      "Epoch 2761, Loss: 0.46463513374328613, Final Batch Loss: 0.1330116093158722\n",
      "Epoch 2762, Loss: 0.30564308911561966, Final Batch Loss: 0.05698961019515991\n",
      "Epoch 2763, Loss: 0.508431687951088, Final Batch Loss: 0.19447162747383118\n",
      "Epoch 2764, Loss: 0.38627248257398605, Final Batch Loss: 0.10210305452346802\n",
      "Epoch 2765, Loss: 0.41559139639139175, Final Batch Loss: 0.17480683326721191\n",
      "Epoch 2766, Loss: 0.3984333202242851, Final Batch Loss: 0.10864155739545822\n",
      "Epoch 2767, Loss: 0.3994683474302292, Final Batch Loss: 0.14492274820804596\n",
      "Epoch 2768, Loss: 0.4305693805217743, Final Batch Loss: 0.14994224905967712\n",
      "Epoch 2769, Loss: 0.3930901363492012, Final Batch Loss: 0.0829591155052185\n",
      "Epoch 2770, Loss: 0.3857918456196785, Final Batch Loss: 0.12117210775613785\n",
      "Epoch 2771, Loss: 0.48393720388412476, Final Batch Loss: 0.17956864833831787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2772, Loss: 0.3973052129149437, Final Batch Loss: 0.08807767182588577\n",
      "Epoch 2773, Loss: 0.52110455930233, Final Batch Loss: 0.15618114173412323\n",
      "Epoch 2774, Loss: 0.39333297312259674, Final Batch Loss: 0.17362800240516663\n",
      "Epoch 2775, Loss: 0.44727830588817596, Final Batch Loss: 0.1898568868637085\n",
      "Epoch 2776, Loss: 0.37274762988090515, Final Batch Loss: 0.10342346131801605\n",
      "Epoch 2777, Loss: 0.4281526058912277, Final Batch Loss: 0.08935962617397308\n",
      "Epoch 2778, Loss: 0.6588975042104721, Final Batch Loss: 0.2710576057434082\n",
      "Epoch 2779, Loss: 0.4227561056613922, Final Batch Loss: 0.09916327893733978\n",
      "Epoch 2780, Loss: 0.4450772926211357, Final Batch Loss: 0.14189279079437256\n",
      "Epoch 2781, Loss: 0.37905457615852356, Final Batch Loss: 0.11964721232652664\n",
      "Epoch 2782, Loss: 0.45098016411066055, Final Batch Loss: 0.11337435990571976\n",
      "Epoch 2783, Loss: 0.4157540053129196, Final Batch Loss: 0.07168446481227875\n",
      "Epoch 2784, Loss: 0.35941188782453537, Final Batch Loss: 0.07043462246656418\n",
      "Epoch 2785, Loss: 0.5207341313362122, Final Batch Loss: 0.16309621930122375\n",
      "Epoch 2786, Loss: 0.33213356137275696, Final Batch Loss: 0.05477461218833923\n",
      "Epoch 2787, Loss: 0.43142520636320114, Final Batch Loss: 0.13011209666728973\n",
      "Epoch 2788, Loss: 0.3573509305715561, Final Batch Loss: 0.1134151741862297\n",
      "Epoch 2789, Loss: 0.34856945276260376, Final Batch Loss: 0.05452924966812134\n",
      "Epoch 2790, Loss: 0.33475229889154434, Final Batch Loss: 0.06799618154764175\n",
      "Epoch 2791, Loss: 0.3969937488436699, Final Batch Loss: 0.06382108479738235\n",
      "Epoch 2792, Loss: 0.3208462968468666, Final Batch Loss: 0.10606712102890015\n",
      "Epoch 2793, Loss: 0.4113849103450775, Final Batch Loss: 0.17223381996154785\n",
      "Epoch 2794, Loss: 0.41561172157526016, Final Batch Loss: 0.14733630418777466\n",
      "Epoch 2795, Loss: 0.3590986579656601, Final Batch Loss: 0.1125016137957573\n",
      "Epoch 2796, Loss: 0.376391664147377, Final Batch Loss: 0.08649502694606781\n",
      "Epoch 2797, Loss: 0.41034695506095886, Final Batch Loss: 0.12693938612937927\n",
      "Epoch 2798, Loss: 0.33928677812218666, Final Batch Loss: 0.05896204337477684\n",
      "Epoch 2799, Loss: 0.3674953952431679, Final Batch Loss: 0.11247429251670837\n",
      "Epoch 2800, Loss: 0.6005794405937195, Final Batch Loss: 0.27127498388290405\n",
      "Epoch 2801, Loss: 0.406833253800869, Final Batch Loss: 0.12114455550909042\n",
      "Epoch 2802, Loss: 0.3245007395744324, Final Batch Loss: 0.07638014107942581\n",
      "Epoch 2803, Loss: 0.4860363155603409, Final Batch Loss: 0.18319085240364075\n",
      "Epoch 2804, Loss: 0.3923051282763481, Final Batch Loss: 0.11049225181341171\n",
      "Epoch 2805, Loss: 0.46247291564941406, Final Batch Loss: 0.2369004338979721\n",
      "Epoch 2806, Loss: 0.3627162575721741, Final Batch Loss: 0.09388673305511475\n",
      "Epoch 2807, Loss: 0.3891064524650574, Final Batch Loss: 0.06623473763465881\n",
      "Epoch 2808, Loss: 0.48774000257253647, Final Batch Loss: 0.20867952704429626\n",
      "Epoch 2809, Loss: 0.4789268597960472, Final Batch Loss: 0.25670138001441956\n",
      "Epoch 2810, Loss: 0.3932764530181885, Final Batch Loss: 0.13071274757385254\n",
      "Epoch 2811, Loss: 0.43788712471723557, Final Batch Loss: 0.1455056071281433\n",
      "Epoch 2812, Loss: 0.281015831977129, Final Batch Loss: 0.05838167294859886\n",
      "Epoch 2813, Loss: 0.36523088067770004, Final Batch Loss: 0.0693834200501442\n",
      "Epoch 2814, Loss: 0.5651805847883224, Final Batch Loss: 0.2523193955421448\n",
      "Epoch 2815, Loss: 0.4297241121530533, Final Batch Loss: 0.20863519608974457\n",
      "Epoch 2816, Loss: 0.5566148608922958, Final Batch Loss: 0.2002057582139969\n",
      "Epoch 2817, Loss: 0.4575572907924652, Final Batch Loss: 0.18348871171474457\n",
      "Epoch 2818, Loss: 0.34120841324329376, Final Batch Loss: 0.08459961414337158\n",
      "Epoch 2819, Loss: 0.34578879177570343, Final Batch Loss: 0.0948009118437767\n",
      "Epoch 2820, Loss: 0.39562660455703735, Final Batch Loss: 0.1515238732099533\n",
      "Epoch 2821, Loss: 0.4965130463242531, Final Batch Loss: 0.14341959357261658\n",
      "Epoch 2822, Loss: 0.5205743759870529, Final Batch Loss: 0.23705801367759705\n",
      "Epoch 2823, Loss: 0.5102546960115433, Final Batch Loss: 0.20426033437252045\n",
      "Epoch 2824, Loss: 0.405148446559906, Final Batch Loss: 0.12649428844451904\n",
      "Epoch 2825, Loss: 0.560151070356369, Final Batch Loss: 0.2138856053352356\n",
      "Epoch 2826, Loss: 0.5603570491075516, Final Batch Loss: 0.3287445902824402\n",
      "Epoch 2827, Loss: 0.5252875685691833, Final Batch Loss: 0.23449300229549408\n",
      "Epoch 2828, Loss: 0.3285086303949356, Final Batch Loss: 0.06321382522583008\n",
      "Epoch 2829, Loss: 0.44969621300697327, Final Batch Loss: 0.1852692812681198\n",
      "Epoch 2830, Loss: 0.37581174075603485, Final Batch Loss: 0.09887473285198212\n",
      "Epoch 2831, Loss: 0.4126627668738365, Final Batch Loss: 0.19220256805419922\n",
      "Epoch 2832, Loss: 0.3661988377571106, Final Batch Loss: 0.09135030210018158\n",
      "Epoch 2833, Loss: 0.40834739059209824, Final Batch Loss: 0.13370168209075928\n",
      "Epoch 2834, Loss: 0.4763059616088867, Final Batch Loss: 0.2091682255268097\n",
      "Epoch 2835, Loss: 0.4488687962293625, Final Batch Loss: 0.13321125507354736\n",
      "Epoch 2836, Loss: 0.5638679042458534, Final Batch Loss: 0.3112839460372925\n",
      "Epoch 2837, Loss: 0.4219289869070053, Final Batch Loss: 0.09943419694900513\n",
      "Epoch 2838, Loss: 0.4813265949487686, Final Batch Loss: 0.2733747959136963\n",
      "Epoch 2839, Loss: 0.4121130555868149, Final Batch Loss: 0.16339440643787384\n",
      "Epoch 2840, Loss: 0.41942913830280304, Final Batch Loss: 0.17978942394256592\n",
      "Epoch 2841, Loss: 0.3209206685423851, Final Batch Loss: 0.07558678835630417\n",
      "Epoch 2842, Loss: 0.42905694991350174, Final Batch Loss: 0.14981408417224884\n",
      "Epoch 2843, Loss: 0.43211086839437485, Final Batch Loss: 0.14679336547851562\n",
      "Epoch 2844, Loss: 0.3380335234105587, Final Batch Loss: 0.05975724384188652\n",
      "Epoch 2845, Loss: 0.42187562584877014, Final Batch Loss: 0.13035909831523895\n",
      "Epoch 2846, Loss: 0.4512649178504944, Final Batch Loss: 0.08241847157478333\n",
      "Epoch 2847, Loss: 0.44321610033512115, Final Batch Loss: 0.1491020917892456\n",
      "Epoch 2848, Loss: 0.49339546263217926, Final Batch Loss: 0.178789883852005\n",
      "Epoch 2849, Loss: 0.3974249064922333, Final Batch Loss: 0.15385718643665314\n",
      "Epoch 2850, Loss: 0.3814031183719635, Final Batch Loss: 0.1590701937675476\n",
      "Epoch 2851, Loss: 0.4487999379634857, Final Batch Loss: 0.167231947183609\n",
      "Epoch 2852, Loss: 0.4111076593399048, Final Batch Loss: 0.09333737194538116\n",
      "Epoch 2853, Loss: 0.4457438141107559, Final Batch Loss: 0.14157117903232574\n",
      "Epoch 2854, Loss: 0.46695081889629364, Final Batch Loss: 0.13346324861049652\n",
      "Epoch 2855, Loss: 0.4999084025621414, Final Batch Loss: 0.1467072069644928\n",
      "Epoch 2856, Loss: 0.373262457549572, Final Batch Loss: 0.09201813489198685\n",
      "Epoch 2857, Loss: 0.47300469875335693, Final Batch Loss: 0.12687388062477112\n",
      "Epoch 2858, Loss: 0.5331263095140457, Final Batch Loss: 0.19990770518779755\n",
      "Epoch 2859, Loss: 0.451128825545311, Final Batch Loss: 0.1498892605304718\n",
      "Epoch 2860, Loss: 0.4523726925253868, Final Batch Loss: 0.17909157276153564\n",
      "Epoch 2861, Loss: 0.4461234211921692, Final Batch Loss: 0.1377539485692978\n",
      "Epoch 2862, Loss: 0.6217003017663956, Final Batch Loss: 0.37330666184425354\n",
      "Epoch 2863, Loss: 0.37537000328302383, Final Batch Loss: 0.08468090742826462\n",
      "Epoch 2864, Loss: 0.3127502351999283, Final Batch Loss: 0.06652291864156723\n",
      "Epoch 2865, Loss: 0.4922266751527786, Final Batch Loss: 0.219716876745224\n",
      "Epoch 2866, Loss: 0.5662407577037811, Final Batch Loss: 0.2176344394683838\n",
      "Epoch 2867, Loss: 0.4991734027862549, Final Batch Loss: 0.23189949989318848\n",
      "Epoch 2868, Loss: 0.4136912077665329, Final Batch Loss: 0.15296682715415955\n",
      "Epoch 2869, Loss: 0.3833291158080101, Final Batch Loss: 0.13758037984371185\n",
      "Epoch 2870, Loss: 0.44883303344249725, Final Batch Loss: 0.15049226582050323\n",
      "Epoch 2871, Loss: 0.4034164920449257, Final Batch Loss: 0.1253495067358017\n",
      "Epoch 2872, Loss: 0.39958909898996353, Final Batch Loss: 0.0901046171784401\n",
      "Epoch 2873, Loss: 0.40199843794107437, Final Batch Loss: 0.09642987698316574\n",
      "Epoch 2874, Loss: 0.3734825402498245, Final Batch Loss: 0.1210705041885376\n",
      "Epoch 2875, Loss: 0.36592964828014374, Final Batch Loss: 0.054869599640369415\n",
      "Epoch 2876, Loss: 0.3423116207122803, Final Batch Loss: 0.1394173800945282\n",
      "Epoch 2877, Loss: 0.29644401371479034, Final Batch Loss: 0.04263855516910553\n",
      "Epoch 2878, Loss: 0.4409247785806656, Final Batch Loss: 0.15096713602542877\n",
      "Epoch 2879, Loss: 0.43845389783382416, Final Batch Loss: 0.14675556123256683\n",
      "Epoch 2880, Loss: 0.358411505818367, Final Batch Loss: 0.08543024957180023\n",
      "Epoch 2881, Loss: 0.4535987451672554, Final Batch Loss: 0.2057405263185501\n",
      "Epoch 2882, Loss: 0.3889443874359131, Final Batch Loss: 0.11095758527517319\n",
      "Epoch 2883, Loss: 0.42034274339675903, Final Batch Loss: 0.16085785627365112\n",
      "Epoch 2884, Loss: 0.407246395945549, Final Batch Loss: 0.11117090284824371\n",
      "Epoch 2885, Loss: 0.3756445422768593, Final Batch Loss: 0.14318877458572388\n",
      "Epoch 2886, Loss: 0.6400982588529587, Final Batch Loss: 0.2658153772354126\n",
      "Epoch 2887, Loss: 0.33684393763542175, Final Batch Loss: 0.10328593850135803\n",
      "Epoch 2888, Loss: 0.46087755262851715, Final Batch Loss: 0.1672002375125885\n",
      "Epoch 2889, Loss: 0.39249923825263977, Final Batch Loss: 0.11343428492546082\n",
      "Epoch 2890, Loss: 0.37964240461587906, Final Batch Loss: 0.11859102547168732\n",
      "Epoch 2891, Loss: 0.3355984538793564, Final Batch Loss: 0.0722048208117485\n",
      "Epoch 2892, Loss: 0.38741644471883774, Final Batch Loss: 0.11154190450906754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2893, Loss: 0.5176602900028229, Final Batch Loss: 0.21102850139141083\n",
      "Epoch 2894, Loss: 0.34605876356363297, Final Batch Loss: 0.093959279358387\n",
      "Epoch 2895, Loss: 0.35440250486135483, Final Batch Loss: 0.10922631621360779\n",
      "Epoch 2896, Loss: 0.4105893075466156, Final Batch Loss: 0.10592390596866608\n",
      "Epoch 2897, Loss: 0.3676092103123665, Final Batch Loss: 0.08972660452127457\n",
      "Epoch 2898, Loss: 0.4024197459220886, Final Batch Loss: 0.06967276334762573\n",
      "Epoch 2899, Loss: 0.514705702662468, Final Batch Loss: 0.22846779227256775\n",
      "Epoch 2900, Loss: 0.43670062720775604, Final Batch Loss: 0.11853013932704926\n",
      "Epoch 2901, Loss: 0.3717139661312103, Final Batch Loss: 0.13357214629650116\n",
      "Epoch 2902, Loss: 0.49784472584724426, Final Batch Loss: 0.18348748981952667\n",
      "Epoch 2903, Loss: 0.3954790532588959, Final Batch Loss: 0.11925369501113892\n",
      "Epoch 2904, Loss: 0.3411944508552551, Final Batch Loss: 0.08634933829307556\n",
      "Epoch 2905, Loss: 0.3810022473335266, Final Batch Loss: 0.15476655960083008\n",
      "Epoch 2906, Loss: 0.3730219826102257, Final Batch Loss: 0.09258400648832321\n",
      "Epoch 2907, Loss: 0.40272412449121475, Final Batch Loss: 0.16404734551906586\n",
      "Epoch 2908, Loss: 0.3882428780198097, Final Batch Loss: 0.12074059247970581\n",
      "Epoch 2909, Loss: 0.3287855014204979, Final Batch Loss: 0.08737440407276154\n",
      "Epoch 2910, Loss: 0.3016238659620285, Final Batch Loss: 0.0855073556303978\n",
      "Epoch 2911, Loss: 0.3526810109615326, Final Batch Loss: 0.11913934350013733\n",
      "Epoch 2912, Loss: 0.39785631746053696, Final Batch Loss: 0.12399699538946152\n",
      "Epoch 2913, Loss: 0.4997095316648483, Final Batch Loss: 0.19776050746440887\n",
      "Epoch 2914, Loss: 0.3676439970731735, Final Batch Loss: 0.08900348842144012\n",
      "Epoch 2915, Loss: 0.4468770995736122, Final Batch Loss: 0.16544252634048462\n",
      "Epoch 2916, Loss: 0.280168566852808, Final Batch Loss: 0.04455970600247383\n",
      "Epoch 2917, Loss: 0.38643353432416916, Final Batch Loss: 0.1011757180094719\n",
      "Epoch 2918, Loss: 0.37212713807821274, Final Batch Loss: 0.16240821778774261\n",
      "Epoch 2919, Loss: 0.37923727184534073, Final Batch Loss: 0.11403043568134308\n",
      "Epoch 2920, Loss: 0.300182580947876, Final Batch Loss: 0.08526696264743805\n",
      "Epoch 2921, Loss: 0.3408334031701088, Final Batch Loss: 0.12272012233734131\n",
      "Epoch 2922, Loss: 0.5396087616682053, Final Batch Loss: 0.209136962890625\n",
      "Epoch 2923, Loss: 0.5163145437836647, Final Batch Loss: 0.2680441737174988\n",
      "Epoch 2924, Loss: 0.47688545286655426, Final Batch Loss: 0.1294449120759964\n",
      "Epoch 2925, Loss: 0.3590084984898567, Final Batch Loss: 0.03587748855352402\n",
      "Epoch 2926, Loss: 0.4428606778383255, Final Batch Loss: 0.1019691675901413\n",
      "Epoch 2927, Loss: 0.3023657128214836, Final Batch Loss: 0.04678861051797867\n",
      "Epoch 2928, Loss: 0.3846823647618294, Final Batch Loss: 0.11200138181447983\n",
      "Epoch 2929, Loss: 0.35255979001522064, Final Batch Loss: 0.1070232018828392\n",
      "Epoch 2930, Loss: 0.36966414749622345, Final Batch Loss: 0.0822264701128006\n",
      "Epoch 2931, Loss: 0.42533912509679794, Final Batch Loss: 0.23545868694782257\n",
      "Epoch 2932, Loss: 0.4979354739189148, Final Batch Loss: 0.2393103688955307\n",
      "Epoch 2933, Loss: 0.4371853917837143, Final Batch Loss: 0.22358539700508118\n",
      "Epoch 2934, Loss: 0.34501006454229355, Final Batch Loss: 0.10267797857522964\n",
      "Epoch 2935, Loss: 0.38435085117816925, Final Batch Loss: 0.1229507103562355\n",
      "Epoch 2936, Loss: 0.2841634154319763, Final Batch Loss: 0.04914971441030502\n",
      "Epoch 2937, Loss: 0.417719267308712, Final Batch Loss: 0.11010763794183731\n",
      "Epoch 2938, Loss: 0.4951591342687607, Final Batch Loss: 0.21375323832035065\n",
      "Epoch 2939, Loss: 0.5252916514873505, Final Batch Loss: 0.2532913386821747\n",
      "Epoch 2940, Loss: 0.45560866594314575, Final Batch Loss: 0.1633571833372116\n",
      "Epoch 2941, Loss: 0.45998968183994293, Final Batch Loss: 0.16068950295448303\n",
      "Epoch 2942, Loss: 0.3716815151274204, Final Batch Loss: 0.06236991658806801\n",
      "Epoch 2943, Loss: 0.4840056896209717, Final Batch Loss: 0.1783764362335205\n",
      "Epoch 2944, Loss: 0.575934462249279, Final Batch Loss: 0.2911992073059082\n",
      "Epoch 2945, Loss: 0.45509934425354004, Final Batch Loss: 0.2086745649576187\n",
      "Epoch 2946, Loss: 0.36990495026111603, Final Batch Loss: 0.08767268061637878\n",
      "Epoch 2947, Loss: 0.29998981952667236, Final Batch Loss: 0.05955872684717178\n",
      "Epoch 2948, Loss: 0.5013704895973206, Final Batch Loss: 0.13319148123264313\n",
      "Epoch 2949, Loss: 0.4622117653489113, Final Batch Loss: 0.13230541348457336\n",
      "Epoch 2950, Loss: 0.34994416683912277, Final Batch Loss: 0.1140439361333847\n",
      "Epoch 2951, Loss: 0.5259493291378021, Final Batch Loss: 0.2131253331899643\n",
      "Epoch 2952, Loss: 0.38929153978824615, Final Batch Loss: 0.11975222826004028\n",
      "Epoch 2953, Loss: 0.36895009875297546, Final Batch Loss: 0.05794486403465271\n",
      "Epoch 2954, Loss: 0.36387622356414795, Final Batch Loss: 0.12620502710342407\n",
      "Epoch 2955, Loss: 0.4376623407006264, Final Batch Loss: 0.11568056792020798\n",
      "Epoch 2956, Loss: 0.4419538676738739, Final Batch Loss: 0.16122853755950928\n",
      "Epoch 2957, Loss: 0.36750777065753937, Final Batch Loss: 0.1250935196876526\n",
      "Epoch 2958, Loss: 0.38779719918966293, Final Batch Loss: 0.12194517999887466\n",
      "Epoch 2959, Loss: 0.3461719639599323, Final Batch Loss: 0.06098499521613121\n",
      "Epoch 2960, Loss: 0.4842064008116722, Final Batch Loss: 0.20066621899604797\n",
      "Epoch 2961, Loss: 0.4560108706355095, Final Batch Loss: 0.16131548583507538\n",
      "Epoch 2962, Loss: 0.39455780386924744, Final Batch Loss: 0.12212784588336945\n",
      "Epoch 2963, Loss: 0.35624364018440247, Final Batch Loss: 0.11999469995498657\n",
      "Epoch 2964, Loss: 0.3637738898396492, Final Batch Loss: 0.12359565496444702\n",
      "Epoch 2965, Loss: 0.4178018420934677, Final Batch Loss: 0.11130310595035553\n",
      "Epoch 2966, Loss: 0.4206244722008705, Final Batch Loss: 0.1819234937429428\n",
      "Epoch 2967, Loss: 0.33275145292282104, Final Batch Loss: 0.08683353662490845\n",
      "Epoch 2968, Loss: 0.2894333079457283, Final Batch Loss: 0.07163196802139282\n",
      "Epoch 2969, Loss: 0.39869120717048645, Final Batch Loss: 0.17702458798885345\n",
      "Epoch 2970, Loss: 0.36825669556856155, Final Batch Loss: 0.08585719764232635\n",
      "Epoch 2971, Loss: 0.35892560333013535, Final Batch Loss: 0.114027239382267\n",
      "Epoch 2972, Loss: 0.34086816012859344, Final Batch Loss: 0.07638469338417053\n",
      "Epoch 2973, Loss: 0.35743360221385956, Final Batch Loss: 0.10565607249736786\n",
      "Epoch 2974, Loss: 0.3678213283419609, Final Batch Loss: 0.08220774680376053\n",
      "Epoch 2975, Loss: 0.43320276588201523, Final Batch Loss: 0.1030130609869957\n",
      "Epoch 2976, Loss: 0.5300899147987366, Final Batch Loss: 0.20728333294391632\n",
      "Epoch 2977, Loss: 0.3982561230659485, Final Batch Loss: 0.10725212097167969\n",
      "Epoch 2978, Loss: 0.4338952898979187, Final Batch Loss: 0.1913408488035202\n",
      "Epoch 2979, Loss: 0.49471476674079895, Final Batch Loss: 0.21116676926612854\n",
      "Epoch 2980, Loss: 0.37693867087364197, Final Batch Loss: 0.1207222193479538\n",
      "Epoch 2981, Loss: 0.3800126910209656, Final Batch Loss: 0.1015358716249466\n",
      "Epoch 2982, Loss: 0.29904530197381973, Final Batch Loss: 0.07218363136053085\n",
      "Epoch 2983, Loss: 0.36769723892211914, Final Batch Loss: 0.11529216915369034\n",
      "Epoch 2984, Loss: 0.3114847019314766, Final Batch Loss: 0.06742404401302338\n",
      "Epoch 2985, Loss: 0.3657770976424217, Final Batch Loss: 0.12370099872350693\n",
      "Epoch 2986, Loss: 0.427001953125, Final Batch Loss: 0.1498328000307083\n",
      "Epoch 2987, Loss: 0.3593659773468971, Final Batch Loss: 0.06674961000680923\n",
      "Epoch 2988, Loss: 0.4109419956803322, Final Batch Loss: 0.14558625221252441\n",
      "Epoch 2989, Loss: 0.45505306124687195, Final Batch Loss: 0.19026583433151245\n",
      "Epoch 2990, Loss: 0.3790521025657654, Final Batch Loss: 0.09685420989990234\n",
      "Epoch 2991, Loss: 0.2938482388854027, Final Batch Loss: 0.07295208424329758\n",
      "Epoch 2992, Loss: 0.47900907695293427, Final Batch Loss: 0.17839382588863373\n",
      "Epoch 2993, Loss: 0.42431794852018356, Final Batch Loss: 0.11325530707836151\n",
      "Epoch 2994, Loss: 0.4200293868780136, Final Batch Loss: 0.16933594644069672\n",
      "Epoch 2995, Loss: 0.46408436447381973, Final Batch Loss: 0.1793494075536728\n",
      "Epoch 2996, Loss: 0.3614293858408928, Final Batch Loss: 0.11209709942340851\n",
      "Epoch 2997, Loss: 0.3085273392498493, Final Batch Loss: 0.05487355962395668\n",
      "Epoch 2998, Loss: 0.48360270261764526, Final Batch Loss: 0.14768826961517334\n",
      "Epoch 2999, Loss: 0.367354080080986, Final Batch Loss: 0.08251260221004486\n",
      "Epoch 3000, Loss: 0.47267238795757294, Final Batch Loss: 0.14997416734695435\n",
      "Epoch 3001, Loss: 0.4789067432284355, Final Batch Loss: 0.1965789794921875\n",
      "Epoch 3002, Loss: 0.4100029245018959, Final Batch Loss: 0.14558686316013336\n",
      "Epoch 3003, Loss: 0.4267205521464348, Final Batch Loss: 0.06014788895845413\n",
      "Epoch 3004, Loss: 0.35846205800771713, Final Batch Loss: 0.1584143340587616\n",
      "Epoch 3005, Loss: 0.3911755532026291, Final Batch Loss: 0.11919573694467545\n",
      "Epoch 3006, Loss: 0.5941526740789413, Final Batch Loss: 0.2739582657814026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3007, Loss: 0.3173997327685356, Final Batch Loss: 0.07179413735866547\n",
      "Epoch 3008, Loss: 0.3091486059129238, Final Batch Loss: 0.061164241284132004\n",
      "Epoch 3009, Loss: 0.5970513820648193, Final Batch Loss: 0.2839573919773102\n",
      "Epoch 3010, Loss: 0.42440635710954666, Final Batch Loss: 0.1204230859875679\n",
      "Epoch 3011, Loss: 0.3507937490940094, Final Batch Loss: 0.10995892435312271\n",
      "Epoch 3012, Loss: 0.6089459508657455, Final Batch Loss: 0.18823087215423584\n",
      "Epoch 3013, Loss: 0.4551984667778015, Final Batch Loss: 0.18223519623279572\n",
      "Epoch 3014, Loss: 0.38712378591299057, Final Batch Loss: 0.08119159191846848\n",
      "Epoch 3015, Loss: 0.522906556725502, Final Batch Loss: 0.1084146499633789\n",
      "Epoch 3016, Loss: 0.4379354491829872, Final Batch Loss: 0.15332746505737305\n",
      "Epoch 3017, Loss: 0.4409818947315216, Final Batch Loss: 0.1295449435710907\n",
      "Epoch 3018, Loss: 0.41766390949487686, Final Batch Loss: 0.11698798090219498\n",
      "Epoch 3019, Loss: 0.4285719618201256, Final Batch Loss: 0.2210807055234909\n",
      "Epoch 3020, Loss: 0.48547812551259995, Final Batch Loss: 0.14993703365325928\n",
      "Epoch 3021, Loss: 0.4326893836259842, Final Batch Loss: 0.14093166589736938\n",
      "Epoch 3022, Loss: 0.3457621932029724, Final Batch Loss: 0.13183404505252838\n",
      "Epoch 3023, Loss: 0.43601182103157043, Final Batch Loss: 0.1381053775548935\n",
      "Epoch 3024, Loss: 0.3369535058736801, Final Batch Loss: 0.06497420370578766\n",
      "Epoch 3025, Loss: 0.6116254925727844, Final Batch Loss: 0.21533240377902985\n",
      "Epoch 3026, Loss: 0.45350431650877, Final Batch Loss: 0.16259780526161194\n",
      "Epoch 3027, Loss: 0.3548704534769058, Final Batch Loss: 0.11842243373394012\n",
      "Epoch 3028, Loss: 0.37711387872695923, Final Batch Loss: 0.12881016731262207\n",
      "Epoch 3029, Loss: 0.5144812315702438, Final Batch Loss: 0.2057681828737259\n",
      "Epoch 3030, Loss: 0.4951719120144844, Final Batch Loss: 0.12338156253099442\n",
      "Epoch 3031, Loss: 0.514036662876606, Final Batch Loss: 0.21743066608905792\n",
      "Epoch 3032, Loss: 0.40012868493795395, Final Batch Loss: 0.11658401042222977\n",
      "Epoch 3033, Loss: 0.462187759578228, Final Batch Loss: 0.15643373131752014\n",
      "Epoch 3034, Loss: 0.3741837367415428, Final Batch Loss: 0.11140669882297516\n",
      "Epoch 3035, Loss: 0.4316064268350601, Final Batch Loss: 0.09654872119426727\n",
      "Epoch 3036, Loss: 0.3821806088089943, Final Batch Loss: 0.10661061108112335\n",
      "Epoch 3037, Loss: 0.5717395544052124, Final Batch Loss: 0.23763389885425568\n",
      "Epoch 3038, Loss: 0.5936135947704315, Final Batch Loss: 0.3263240456581116\n",
      "Epoch 3039, Loss: 0.49775660037994385, Final Batch Loss: 0.15138742327690125\n",
      "Epoch 3040, Loss: 0.37781766057014465, Final Batch Loss: 0.11598055064678192\n",
      "Epoch 3041, Loss: 0.5778940692543983, Final Batch Loss: 0.32040828466415405\n",
      "Epoch 3042, Loss: 0.4668639600276947, Final Batch Loss: 0.1794731616973877\n",
      "Epoch 3043, Loss: 0.3927137404680252, Final Batch Loss: 0.08851442486047745\n",
      "Epoch 3044, Loss: 0.5485176295042038, Final Batch Loss: 0.139700248837471\n",
      "Epoch 3045, Loss: 0.36316823214292526, Final Batch Loss: 0.08504209667444229\n",
      "Epoch 3046, Loss: 0.37777184695005417, Final Batch Loss: 0.12066860496997833\n",
      "Epoch 3047, Loss: 0.3621777296066284, Final Batch Loss: 0.07734163105487823\n",
      "Epoch 3048, Loss: 0.3856766074895859, Final Batch Loss: 0.1256289929151535\n",
      "Epoch 3049, Loss: 0.32793307304382324, Final Batch Loss: 0.0832444503903389\n",
      "Epoch 3050, Loss: 0.36945341527462006, Final Batch Loss: 0.10558374226093292\n",
      "Epoch 3051, Loss: 0.38363759964704514, Final Batch Loss: 0.08654765039682388\n",
      "Epoch 3052, Loss: 0.32565679401159286, Final Batch Loss: 0.07199541479349136\n",
      "Epoch 3053, Loss: 0.33519111946225166, Final Batch Loss: 0.05614006891846657\n",
      "Epoch 3054, Loss: 0.4477621167898178, Final Batch Loss: 0.0895293802022934\n",
      "Epoch 3055, Loss: 0.3165864832699299, Final Batch Loss: 0.045609209686517715\n",
      "Epoch 3056, Loss: 0.4785321354866028, Final Batch Loss: 0.19990220665931702\n",
      "Epoch 3057, Loss: 0.461995892226696, Final Batch Loss: 0.1274208426475525\n",
      "Epoch 3058, Loss: 0.31685571372509, Final Batch Loss: 0.06782489269971848\n",
      "Epoch 3059, Loss: 0.3926168158650398, Final Batch Loss: 0.11683361977338791\n",
      "Epoch 3060, Loss: 0.6021466851234436, Final Batch Loss: 0.3522893190383911\n",
      "Epoch 3061, Loss: 0.4451383426785469, Final Batch Loss: 0.2230769544839859\n",
      "Epoch 3062, Loss: 0.5733112022280693, Final Batch Loss: 0.3361856937408447\n",
      "Epoch 3063, Loss: 0.42196157574653625, Final Batch Loss: 0.1843499392271042\n",
      "Epoch 3064, Loss: 0.44781914353370667, Final Batch Loss: 0.18814226984977722\n",
      "Epoch 3065, Loss: 0.5386106818914413, Final Batch Loss: 0.22840175032615662\n",
      "Epoch 3066, Loss: 0.4731782376766205, Final Batch Loss: 0.21308176219463348\n",
      "Epoch 3067, Loss: 0.3393169492483139, Final Batch Loss: 0.08682005852460861\n",
      "Epoch 3068, Loss: 0.44253145158290863, Final Batch Loss: 0.13940227031707764\n",
      "Epoch 3069, Loss: 0.41651105880737305, Final Batch Loss: 0.1316479742527008\n",
      "Epoch 3070, Loss: 0.3733702749013901, Final Batch Loss: 0.06723061203956604\n",
      "Epoch 3071, Loss: 0.4524906277656555, Final Batch Loss: 0.15582498908042908\n",
      "Epoch 3072, Loss: 0.37201935797929764, Final Batch Loss: 0.1475728452205658\n",
      "Epoch 3073, Loss: 0.3130531534552574, Final Batch Loss: 0.05307430773973465\n",
      "Epoch 3074, Loss: 0.42686496675014496, Final Batch Loss: 0.2377047836780548\n",
      "Epoch 3075, Loss: 0.358974426984787, Final Batch Loss: 0.1061970666050911\n",
      "Epoch 3076, Loss: 0.3626185208559036, Final Batch Loss: 0.1269443929195404\n",
      "Epoch 3077, Loss: 0.3593286871910095, Final Batch Loss: 0.11209846287965775\n",
      "Epoch 3078, Loss: 0.3409135788679123, Final Batch Loss: 0.11136138439178467\n",
      "Epoch 3079, Loss: 0.3295474089682102, Final Batch Loss: 0.0529964454472065\n",
      "Epoch 3080, Loss: 0.4698551148176193, Final Batch Loss: 0.1460290104150772\n",
      "Epoch 3081, Loss: 0.461711123585701, Final Batch Loss: 0.2341305911540985\n",
      "Epoch 3082, Loss: 0.35384226590394974, Final Batch Loss: 0.10635438561439514\n",
      "Epoch 3083, Loss: 0.589339017868042, Final Batch Loss: 0.2810651361942291\n",
      "Epoch 3084, Loss: 0.31945402920246124, Final Batch Loss: 0.09355641156435013\n",
      "Epoch 3085, Loss: 0.3182826042175293, Final Batch Loss: 0.09984837472438812\n",
      "Epoch 3086, Loss: 0.3139442503452301, Final Batch Loss: 0.0932888314127922\n",
      "Epoch 3087, Loss: 0.4487827867269516, Final Batch Loss: 0.14718428254127502\n",
      "Epoch 3088, Loss: 0.4119567796587944, Final Batch Loss: 0.12083099037408829\n",
      "Epoch 3089, Loss: 0.3460545390844345, Final Batch Loss: 0.08042024821043015\n",
      "Epoch 3090, Loss: 0.43664146214723587, Final Batch Loss: 0.17412082850933075\n",
      "Epoch 3091, Loss: 0.3541509211063385, Final Batch Loss: 0.11865369975566864\n",
      "Epoch 3092, Loss: 0.4433588236570358, Final Batch Loss: 0.12702631950378418\n",
      "Epoch 3093, Loss: 0.3833666443824768, Final Batch Loss: 0.10052067041397095\n",
      "Epoch 3094, Loss: 0.4170428067445755, Final Batch Loss: 0.13814988732337952\n",
      "Epoch 3095, Loss: 0.45507118850946426, Final Batch Loss: 0.20988266170024872\n",
      "Epoch 3096, Loss: 0.4070574641227722, Final Batch Loss: 0.1820588856935501\n",
      "Epoch 3097, Loss: 0.3341732546687126, Final Batch Loss: 0.07759499549865723\n",
      "Epoch 3098, Loss: 0.31111396104097366, Final Batch Loss: 0.03841491788625717\n",
      "Epoch 3099, Loss: 0.4110162705183029, Final Batch Loss: 0.13149701058864594\n",
      "Epoch 3100, Loss: 0.4005231410264969, Final Batch Loss: 0.10514919459819794\n",
      "Epoch 3101, Loss: 0.3599350079894066, Final Batch Loss: 0.10260353982448578\n",
      "Epoch 3102, Loss: 0.5512239784002304, Final Batch Loss: 0.29001450538635254\n",
      "Epoch 3103, Loss: 0.27338651567697525, Final Batch Loss: 0.031157635152339935\n",
      "Epoch 3104, Loss: 0.3935241624712944, Final Batch Loss: 0.1342286318540573\n",
      "Epoch 3105, Loss: 0.32427430897951126, Final Batch Loss: 0.11778262257575989\n",
      "Epoch 3106, Loss: 0.44600939750671387, Final Batch Loss: 0.14374804496765137\n",
      "Epoch 3107, Loss: 0.3137922137975693, Final Batch Loss: 0.08629917353391647\n",
      "Epoch 3108, Loss: 0.3853253424167633, Final Batch Loss: 0.11807125806808472\n",
      "Epoch 3109, Loss: 0.3257122337818146, Final Batch Loss: 0.07627932727336884\n",
      "Epoch 3110, Loss: 0.452529713511467, Final Batch Loss: 0.1721789538860321\n",
      "Epoch 3111, Loss: 0.3247828558087349, Final Batch Loss: 0.10561405122280121\n",
      "Epoch 3112, Loss: 0.39419958740472794, Final Batch Loss: 0.13667728006839752\n",
      "Epoch 3113, Loss: 0.44639208912849426, Final Batch Loss: 0.18060001730918884\n",
      "Epoch 3114, Loss: 0.44088520109653473, Final Batch Loss: 0.22320517897605896\n",
      "Epoch 3115, Loss: 0.4065513014793396, Final Batch Loss: 0.1458512842655182\n",
      "Epoch 3116, Loss: 0.33359603583812714, Final Batch Loss: 0.08824387192726135\n",
      "Epoch 3117, Loss: 0.4434163048863411, Final Batch Loss: 0.1416148990392685\n",
      "Epoch 3118, Loss: 0.36348721385002136, Final Batch Loss: 0.027513578534126282\n",
      "Epoch 3119, Loss: 0.3598688170313835, Final Batch Loss: 0.06510194391012192\n",
      "Epoch 3120, Loss: 0.3922334164381027, Final Batch Loss: 0.13683496415615082\n",
      "Epoch 3121, Loss: 0.43710634857416153, Final Batch Loss: 0.16765549778938293\n",
      "Epoch 3122, Loss: 0.47550442814826965, Final Batch Loss: 0.16269096732139587\n",
      "Epoch 3123, Loss: 0.45445697009563446, Final Batch Loss: 0.18674004077911377\n",
      "Epoch 3124, Loss: 0.47936074435710907, Final Batch Loss: 0.09551796317100525\n",
      "Epoch 3125, Loss: 0.3970024809241295, Final Batch Loss: 0.10851307958364487\n",
      "Epoch 3126, Loss: 0.5944273918867111, Final Batch Loss: 0.3217937648296356\n",
      "Epoch 3127, Loss: 0.39203739166259766, Final Batch Loss: 0.09457327425479889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3128, Loss: 0.454169325530529, Final Batch Loss: 0.10958601534366608\n",
      "Epoch 3129, Loss: 0.34918881952762604, Final Batch Loss: 0.08202812820672989\n",
      "Epoch 3130, Loss: 0.47316113114356995, Final Batch Loss: 0.1877141296863556\n",
      "Epoch 3131, Loss: 0.4213520511984825, Final Batch Loss: 0.16288195550441742\n",
      "Epoch 3132, Loss: 0.35410168021917343, Final Batch Loss: 0.10589104145765305\n",
      "Epoch 3133, Loss: 0.41982918977737427, Final Batch Loss: 0.14598454535007477\n",
      "Epoch 3134, Loss: 0.4399792402982712, Final Batch Loss: 0.19893324375152588\n",
      "Epoch 3135, Loss: 0.36611930280923843, Final Batch Loss: 0.11944665759801865\n",
      "Epoch 3136, Loss: 0.340348020195961, Final Batch Loss: 0.0751512423157692\n",
      "Epoch 3137, Loss: 0.5628387928009033, Final Batch Loss: 0.2869553565979004\n",
      "Epoch 3138, Loss: 0.4672961086034775, Final Batch Loss: 0.23914697766304016\n",
      "Epoch 3139, Loss: 0.3727123737335205, Final Batch Loss: 0.12844230234622955\n",
      "Epoch 3140, Loss: 0.41208022087812424, Final Batch Loss: 0.17500802874565125\n",
      "Epoch 3141, Loss: 0.36939142644405365, Final Batch Loss: 0.11005470901727676\n",
      "Epoch 3142, Loss: 0.3625342473387718, Final Batch Loss: 0.12220721691846848\n",
      "Epoch 3143, Loss: 0.3635592386126518, Final Batch Loss: 0.11802313476800919\n",
      "Epoch 3144, Loss: 0.438458114862442, Final Batch Loss: 0.15020792186260223\n",
      "Epoch 3145, Loss: 0.39092160016298294, Final Batch Loss: 0.13427749276161194\n",
      "Epoch 3146, Loss: 0.3581847548484802, Final Batch Loss: 0.09380681812763214\n",
      "Epoch 3147, Loss: 0.38613361120224, Final Batch Loss: 0.13473127782344818\n",
      "Epoch 3148, Loss: 0.392679899930954, Final Batch Loss: 0.10651072859764099\n",
      "Epoch 3149, Loss: 0.3720514327287674, Final Batch Loss: 0.16269981861114502\n",
      "Epoch 3150, Loss: 0.544772669672966, Final Batch Loss: 0.3089759945869446\n",
      "Epoch 3151, Loss: 0.33548571169376373, Final Batch Loss: 0.06877415627241135\n",
      "Epoch 3152, Loss: 0.3547041416168213, Final Batch Loss: 0.13719263672828674\n",
      "Epoch 3153, Loss: 0.3578510731458664, Final Batch Loss: 0.13401485979557037\n",
      "Epoch 3154, Loss: 0.2784692235291004, Final Batch Loss: 0.05793655291199684\n",
      "Epoch 3155, Loss: 0.39070846140384674, Final Batch Loss: 0.13828828930854797\n",
      "Epoch 3156, Loss: 0.3965592235326767, Final Batch Loss: 0.12321774661540985\n",
      "Epoch 3157, Loss: 0.43783898651599884, Final Batch Loss: 0.1928897649049759\n",
      "Epoch 3158, Loss: 0.4438851326704025, Final Batch Loss: 0.15937551856040955\n",
      "Epoch 3159, Loss: 0.4218674749135971, Final Batch Loss: 0.15362903475761414\n",
      "Epoch 3160, Loss: 0.31444064900279045, Final Batch Loss: 0.03560468927025795\n",
      "Epoch 3161, Loss: 0.3263540118932724, Final Batch Loss: 0.06974701583385468\n",
      "Epoch 3162, Loss: 0.35806141048669815, Final Batch Loss: 0.11861079186201096\n",
      "Epoch 3163, Loss: 0.5291907712817192, Final Batch Loss: 0.1088695153594017\n",
      "Epoch 3164, Loss: 0.36052365228533745, Final Batch Loss: 0.05190384015440941\n",
      "Epoch 3165, Loss: 0.3466570973396301, Final Batch Loss: 0.11639747023582458\n",
      "Epoch 3166, Loss: 0.3151160106062889, Final Batch Loss: 0.09279696643352509\n",
      "Epoch 3167, Loss: 0.3718877136707306, Final Batch Loss: 0.11915701627731323\n",
      "Epoch 3168, Loss: 0.4615352973341942, Final Batch Loss: 0.19159060716629028\n",
      "Epoch 3169, Loss: 0.34587956219911575, Final Batch Loss: 0.07423172891139984\n",
      "Epoch 3170, Loss: 0.3835204169154167, Final Batch Loss: 0.10306800156831741\n",
      "Epoch 3171, Loss: 0.6699690818786621, Final Batch Loss: 0.32865405082702637\n",
      "Epoch 3172, Loss: 0.4200621694326401, Final Batch Loss: 0.18631616234779358\n",
      "Epoch 3173, Loss: 0.4729095697402954, Final Batch Loss: 0.2247762531042099\n",
      "Epoch 3174, Loss: 0.45559556037187576, Final Batch Loss: 0.14154323935508728\n",
      "Epoch 3175, Loss: 0.3875352665781975, Final Batch Loss: 0.11596237868070602\n",
      "Epoch 3176, Loss: 0.38684888184070587, Final Batch Loss: 0.07685817778110504\n",
      "Epoch 3177, Loss: 0.2927020192146301, Final Batch Loss: 0.06650857627391815\n",
      "Epoch 3178, Loss: 0.37146031856536865, Final Batch Loss: 0.14410096406936646\n",
      "Epoch 3179, Loss: 0.3464096039533615, Final Batch Loss: 0.09053818136453629\n",
      "Epoch 3180, Loss: 0.3173030987381935, Final Batch Loss: 0.0369429886341095\n",
      "Epoch 3181, Loss: 0.4115123599767685, Final Batch Loss: 0.10641153156757355\n",
      "Epoch 3182, Loss: 0.44926269352436066, Final Batch Loss: 0.17075024545192719\n",
      "Epoch 3183, Loss: 0.4333917573094368, Final Batch Loss: 0.14796839654445648\n",
      "Epoch 3184, Loss: 0.49996016919612885, Final Batch Loss: 0.19123339653015137\n",
      "Epoch 3185, Loss: 0.35107000172138214, Final Batch Loss: 0.1464027613401413\n",
      "Epoch 3186, Loss: 0.3880173936486244, Final Batch Loss: 0.1421390324831009\n",
      "Epoch 3187, Loss: 0.3271566927433014, Final Batch Loss: 0.10538648068904877\n",
      "Epoch 3188, Loss: 0.672330804169178, Final Batch Loss: 0.43107983469963074\n",
      "Epoch 3189, Loss: 0.4292727932333946, Final Batch Loss: 0.18953000009059906\n",
      "Epoch 3190, Loss: 0.534835010766983, Final Batch Loss: 0.24372783303260803\n",
      "Epoch 3191, Loss: 0.2946217693388462, Final Batch Loss: 0.04715678468346596\n",
      "Epoch 3192, Loss: 0.36666883528232574, Final Batch Loss: 0.07660225033760071\n",
      "Epoch 3193, Loss: 0.4187110662460327, Final Batch Loss: 0.16452348232269287\n",
      "Epoch 3194, Loss: 0.3706139177083969, Final Batch Loss: 0.1386936604976654\n",
      "Epoch 3195, Loss: 0.33610471338033676, Final Batch Loss: 0.09302116930484772\n",
      "Epoch 3196, Loss: 0.44942501187324524, Final Batch Loss: 0.15150751173496246\n",
      "Epoch 3197, Loss: 0.4587717205286026, Final Batch Loss: 0.17395949363708496\n",
      "Epoch 3198, Loss: 0.48140284419059753, Final Batch Loss: 0.13502073287963867\n",
      "Epoch 3199, Loss: 0.38250962644815445, Final Batch Loss: 0.1386200487613678\n",
      "Epoch 3200, Loss: 0.3568001091480255, Final Batch Loss: 0.07999715209007263\n",
      "Epoch 3201, Loss: 0.4466550946235657, Final Batch Loss: 0.1609567105770111\n",
      "Epoch 3202, Loss: 0.3790687993168831, Final Batch Loss: 0.1023397222161293\n",
      "Epoch 3203, Loss: 0.2818181738257408, Final Batch Loss: 0.03606802225112915\n",
      "Epoch 3204, Loss: 0.2908157929778099, Final Batch Loss: 0.04304278641939163\n",
      "Epoch 3205, Loss: 0.296521432697773, Final Batch Loss: 0.06673087924718857\n",
      "Epoch 3206, Loss: 0.41415147483348846, Final Batch Loss: 0.14330434799194336\n",
      "Epoch 3207, Loss: 0.395258404314518, Final Batch Loss: 0.09379713237285614\n",
      "Epoch 3208, Loss: 0.4224487915635109, Final Batch Loss: 0.10140863806009293\n",
      "Epoch 3209, Loss: 0.28710177540779114, Final Batch Loss: 0.04776176065206528\n",
      "Epoch 3210, Loss: 0.3398812264204025, Final Batch Loss: 0.10606878995895386\n",
      "Epoch 3211, Loss: 0.31025034934282303, Final Batch Loss: 0.06254878640174866\n",
      "Epoch 3212, Loss: 0.33921723812818527, Final Batch Loss: 0.0781034380197525\n",
      "Epoch 3213, Loss: 0.45919956266880035, Final Batch Loss: 0.08416908979415894\n",
      "Epoch 3214, Loss: 0.30188363790512085, Final Batch Loss: 0.05561751127243042\n",
      "Epoch 3215, Loss: 0.5003004297614098, Final Batch Loss: 0.15590065717697144\n",
      "Epoch 3216, Loss: 0.3168848305940628, Final Batch Loss: 0.06565127521753311\n",
      "Epoch 3217, Loss: 0.4031651169061661, Final Batch Loss: 0.16408424079418182\n",
      "Epoch 3218, Loss: 0.4812992215156555, Final Batch Loss: 0.18702390789985657\n",
      "Epoch 3219, Loss: 0.3419680967926979, Final Batch Loss: 0.07041125744581223\n",
      "Epoch 3220, Loss: 0.5244797468185425, Final Batch Loss: 0.22548295557498932\n",
      "Epoch 3221, Loss: 0.3198969438672066, Final Batch Loss: 0.07049094140529633\n",
      "Epoch 3222, Loss: 0.4589294195175171, Final Batch Loss: 0.15565769374370575\n",
      "Epoch 3223, Loss: 0.3493622839450836, Final Batch Loss: 0.1224558874964714\n",
      "Epoch 3224, Loss: 0.298421174287796, Final Batch Loss: 0.0881638452410698\n",
      "Epoch 3225, Loss: 0.36260633915662766, Final Batch Loss: 0.11246798187494278\n",
      "Epoch 3226, Loss: 0.343234583735466, Final Batch Loss: 0.14023765921592712\n",
      "Epoch 3227, Loss: 0.35019533336162567, Final Batch Loss: 0.10383233428001404\n",
      "Epoch 3228, Loss: 0.3595271036028862, Final Batch Loss: 0.09422610700130463\n",
      "Epoch 3229, Loss: 0.4105736017227173, Final Batch Loss: 0.1257316619157791\n",
      "Epoch 3230, Loss: 0.34250064939260483, Final Batch Loss: 0.09530069679021835\n",
      "Epoch 3231, Loss: 0.3740488216280937, Final Batch Loss: 0.0927041620016098\n",
      "Epoch 3232, Loss: 0.38095078617334366, Final Batch Loss: 0.1502581536769867\n",
      "Epoch 3233, Loss: 0.38730746507644653, Final Batch Loss: 0.1270648092031479\n",
      "Epoch 3234, Loss: 0.36843716353178024, Final Batch Loss: 0.12584088742733002\n",
      "Epoch 3235, Loss: 0.3248829320073128, Final Batch Loss: 0.06621284037828445\n",
      "Epoch 3236, Loss: 0.3054041936993599, Final Batch Loss: 0.05152757465839386\n",
      "Epoch 3237, Loss: 0.4962398111820221, Final Batch Loss: 0.2612009644508362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3238, Loss: 0.3724924921989441, Final Batch Loss: 0.10560783743858337\n",
      "Epoch 3239, Loss: 0.3667142316699028, Final Batch Loss: 0.08891715854406357\n",
      "Epoch 3240, Loss: 0.36036182194948196, Final Batch Loss: 0.13077889382839203\n",
      "Epoch 3241, Loss: 0.3990297019481659, Final Batch Loss: 0.13398820161819458\n",
      "Epoch 3242, Loss: 0.36056601256132126, Final Batch Loss: 0.08472651243209839\n",
      "Epoch 3243, Loss: 0.38128476589918137, Final Batch Loss: 0.12015195935964584\n",
      "Epoch 3244, Loss: 0.37409257143735886, Final Batch Loss: 0.08945702761411667\n",
      "Epoch 3245, Loss: 0.47195202112197876, Final Batch Loss: 0.2270260453224182\n",
      "Epoch 3246, Loss: 0.3811667412519455, Final Batch Loss: 0.12538599967956543\n",
      "Epoch 3247, Loss: 0.48337501287460327, Final Batch Loss: 0.18178823590278625\n",
      "Epoch 3248, Loss: 0.3570908457040787, Final Batch Loss: 0.05616883933544159\n",
      "Epoch 3249, Loss: 0.33542758226394653, Final Batch Loss: 0.08190788328647614\n",
      "Epoch 3250, Loss: 0.449397936463356, Final Batch Loss: 0.09906454384326935\n",
      "Epoch 3251, Loss: 0.4535380154848099, Final Batch Loss: 0.15570372343063354\n",
      "Epoch 3252, Loss: 0.3781094029545784, Final Batch Loss: 0.14467209577560425\n",
      "Epoch 3253, Loss: 0.3480760529637337, Final Batch Loss: 0.09039552509784698\n",
      "Epoch 3254, Loss: 0.42723651230335236, Final Batch Loss: 0.20704300701618195\n",
      "Epoch 3255, Loss: 0.35940972715616226, Final Batch Loss: 0.08981359004974365\n",
      "Epoch 3256, Loss: 0.4758695811033249, Final Batch Loss: 0.2079373449087143\n",
      "Epoch 3257, Loss: 0.35796924866735935, Final Batch Loss: 0.030398288741707802\n",
      "Epoch 3258, Loss: 0.3236399441957474, Final Batch Loss: 0.08801727741956711\n",
      "Epoch 3259, Loss: 0.4126007631421089, Final Batch Loss: 0.15208669006824493\n",
      "Epoch 3260, Loss: 0.33792946487665176, Final Batch Loss: 0.052396103739738464\n",
      "Epoch 3261, Loss: 0.41736656427383423, Final Batch Loss: 0.12614643573760986\n",
      "Epoch 3262, Loss: 0.302461676299572, Final Batch Loss: 0.07941525429487228\n",
      "Epoch 3263, Loss: 0.3950260952115059, Final Batch Loss: 0.12029752880334854\n",
      "Epoch 3264, Loss: 0.3917028680443764, Final Batch Loss: 0.15044528245925903\n",
      "Epoch 3265, Loss: 0.48753512650728226, Final Batch Loss: 0.27355819940567017\n",
      "Epoch 3266, Loss: 0.36119044572114944, Final Batch Loss: 0.08613162487745285\n",
      "Epoch 3267, Loss: 0.3226930573582649, Final Batch Loss: 0.052861057221889496\n",
      "Epoch 3268, Loss: 0.465145580470562, Final Batch Loss: 0.2142929583787918\n",
      "Epoch 3269, Loss: 0.35916558653116226, Final Batch Loss: 0.09960877150297165\n",
      "Epoch 3270, Loss: 0.382685624063015, Final Batch Loss: 0.13856448233127594\n",
      "Epoch 3271, Loss: 0.2944483533501625, Final Batch Loss: 0.06695209443569183\n",
      "Epoch 3272, Loss: 0.40894050896167755, Final Batch Loss: 0.11857105791568756\n",
      "Epoch 3273, Loss: 0.4826715588569641, Final Batch Loss: 0.23342007398605347\n",
      "Epoch 3274, Loss: 0.4650716334581375, Final Batch Loss: 0.2008286863565445\n",
      "Epoch 3275, Loss: 0.34371762722730637, Final Batch Loss: 0.07832952588796616\n",
      "Epoch 3276, Loss: 0.35317689925432205, Final Batch Loss: 0.10113629698753357\n",
      "Epoch 3277, Loss: 0.35333482176065445, Final Batch Loss: 0.07202888280153275\n",
      "Epoch 3278, Loss: 0.3900506719946861, Final Batch Loss: 0.09267845004796982\n",
      "Epoch 3279, Loss: 0.31552940607070923, Final Batch Loss: 0.06834365427494049\n",
      "Epoch 3280, Loss: 0.3147427588701248, Final Batch Loss: 0.08636835962533951\n",
      "Epoch 3281, Loss: 0.34655332565307617, Final Batch Loss: 0.07860493659973145\n",
      "Epoch 3282, Loss: 0.41582106798887253, Final Batch Loss: 0.13034161925315857\n",
      "Epoch 3283, Loss: 0.435808502137661, Final Batch Loss: 0.1815941333770752\n",
      "Epoch 3284, Loss: 0.5060330629348755, Final Batch Loss: 0.26912203431129456\n",
      "Epoch 3285, Loss: 0.38339997828006744, Final Batch Loss: 0.06665676832199097\n",
      "Epoch 3286, Loss: 0.3129236251115799, Final Batch Loss: 0.1094900444149971\n",
      "Epoch 3287, Loss: 0.4189252406358719, Final Batch Loss: 0.19711855053901672\n",
      "Epoch 3288, Loss: 0.3898417353630066, Final Batch Loss: 0.13154664635658264\n",
      "Epoch 3289, Loss: 0.45538634806871414, Final Batch Loss: 0.23001860082149506\n",
      "Epoch 3290, Loss: 0.3178414851427078, Final Batch Loss: 0.118825763463974\n",
      "Epoch 3291, Loss: 0.37800971418619156, Final Batch Loss: 0.13518290221691132\n",
      "Epoch 3292, Loss: 0.36169952154159546, Final Batch Loss: 0.10280068218708038\n",
      "Epoch 3293, Loss: 0.3435436636209488, Final Batch Loss: 0.1325148344039917\n",
      "Epoch 3294, Loss: 0.5130367204546928, Final Batch Loss: 0.2754787802696228\n",
      "Epoch 3295, Loss: 0.40931278467178345, Final Batch Loss: 0.1509813517332077\n",
      "Epoch 3296, Loss: 0.451211117208004, Final Batch Loss: 0.14840973913669586\n",
      "Epoch 3297, Loss: 0.3718026131391525, Final Batch Loss: 0.07899369299411774\n",
      "Epoch 3298, Loss: 0.28985927253961563, Final Batch Loss: 0.057812005281448364\n",
      "Epoch 3299, Loss: 0.42730002105236053, Final Batch Loss: 0.10888500511646271\n",
      "Epoch 3300, Loss: 0.5221879184246063, Final Batch Loss: 0.25661543011665344\n",
      "Epoch 3301, Loss: 0.37634560465812683, Final Batch Loss: 0.08708788454532623\n",
      "Epoch 3302, Loss: 0.4321666657924652, Final Batch Loss: 0.14985226094722748\n",
      "Epoch 3303, Loss: 0.41259464621543884, Final Batch Loss: 0.11099980771541595\n",
      "Epoch 3304, Loss: 0.3920794725418091, Final Batch Loss: 0.08939187228679657\n",
      "Epoch 3305, Loss: 0.34435224533081055, Final Batch Loss: 0.0639907568693161\n",
      "Epoch 3306, Loss: 0.3842693194746971, Final Batch Loss: 0.11921611428260803\n",
      "Epoch 3307, Loss: 0.29999445378780365, Final Batch Loss: 0.07388177514076233\n",
      "Epoch 3308, Loss: 0.3397154361009598, Final Batch Loss: 0.06730407476425171\n",
      "Epoch 3309, Loss: 0.5829905122518539, Final Batch Loss: 0.2700912356376648\n",
      "Epoch 3310, Loss: 0.38820386677980423, Final Batch Loss: 0.14525341987609863\n",
      "Epoch 3311, Loss: 0.4826088324189186, Final Batch Loss: 0.23786142468452454\n",
      "Epoch 3312, Loss: 0.46982482075691223, Final Batch Loss: 0.19593201577663422\n",
      "Epoch 3313, Loss: 0.430262953042984, Final Batch Loss: 0.17113086581230164\n",
      "Epoch 3314, Loss: 0.34773798286914825, Final Batch Loss: 0.08886625617742538\n",
      "Epoch 3315, Loss: 0.31568339467048645, Final Batch Loss: 0.09945204854011536\n",
      "Epoch 3316, Loss: 0.3564235121011734, Final Batch Loss: 0.17602317035198212\n",
      "Epoch 3317, Loss: 0.451816625893116, Final Batch Loss: 0.1001744344830513\n",
      "Epoch 3318, Loss: 0.31542622670531273, Final Batch Loss: 0.06134791299700737\n",
      "Epoch 3319, Loss: 0.44356169551610947, Final Batch Loss: 0.17765873670578003\n",
      "Epoch 3320, Loss: 0.3712465390563011, Final Batch Loss: 0.12258702516555786\n",
      "Epoch 3321, Loss: 0.3648976683616638, Final Batch Loss: 0.07855279743671417\n",
      "Epoch 3322, Loss: 0.41996199637651443, Final Batch Loss: 0.1706228107213974\n",
      "Epoch 3323, Loss: 0.37996450811624527, Final Batch Loss: 0.14917580783367157\n",
      "Epoch 3324, Loss: 0.3932981789112091, Final Batch Loss: 0.11704717576503754\n",
      "Epoch 3325, Loss: 0.3746219277381897, Final Batch Loss: 0.13802310824394226\n",
      "Epoch 3326, Loss: 0.3478981852531433, Final Batch Loss: 0.1049913763999939\n",
      "Epoch 3327, Loss: 0.458172082901001, Final Batch Loss: 0.17023654282093048\n",
      "Epoch 3328, Loss: 0.33475570380687714, Final Batch Loss: 0.0939544290304184\n",
      "Epoch 3329, Loss: 0.356169693171978, Final Batch Loss: 0.09255848824977875\n",
      "Epoch 3330, Loss: 0.37423061579465866, Final Batch Loss: 0.15033800899982452\n",
      "Epoch 3331, Loss: 0.42993248254060745, Final Batch Loss: 0.07784294337034225\n",
      "Epoch 3332, Loss: 0.42128853499889374, Final Batch Loss: 0.15180709958076477\n",
      "Epoch 3333, Loss: 0.41535747051239014, Final Batch Loss: 0.11561571061611176\n",
      "Epoch 3334, Loss: 0.5468808189034462, Final Batch Loss: 0.3396156132221222\n",
      "Epoch 3335, Loss: 0.32207004725933075, Final Batch Loss: 0.10287784785032272\n",
      "Epoch 3336, Loss: 0.3768782839179039, Final Batch Loss: 0.1450805813074112\n",
      "Epoch 3337, Loss: 0.30311618745326996, Final Batch Loss: 0.072959765791893\n",
      "Epoch 3338, Loss: 0.3437088280916214, Final Batch Loss: 0.03445965051651001\n",
      "Epoch 3339, Loss: 0.28119033575057983, Final Batch Loss: 0.05128335952758789\n",
      "Epoch 3340, Loss: 0.4778083264827728, Final Batch Loss: 0.30707693099975586\n",
      "Epoch 3341, Loss: 0.46236445009708405, Final Batch Loss: 0.20042064785957336\n",
      "Epoch 3342, Loss: 0.450391948223114, Final Batch Loss: 0.16903910040855408\n",
      "Epoch 3343, Loss: 0.40575237572193146, Final Batch Loss: 0.09433954954147339\n",
      "Epoch 3344, Loss: 0.3720170333981514, Final Batch Loss: 0.1027945876121521\n",
      "Epoch 3345, Loss: 0.36830098181962967, Final Batch Loss: 0.09351692348718643\n",
      "Epoch 3346, Loss: 0.32218945026397705, Final Batch Loss: 0.08844038099050522\n",
      "Epoch 3347, Loss: 0.36444928497076035, Final Batch Loss: 0.10705278068780899\n",
      "Epoch 3348, Loss: 0.36014676839113235, Final Batch Loss: 0.12425050884485245\n",
      "Epoch 3349, Loss: 0.43479684740304947, Final Batch Loss: 0.15154829621315002\n",
      "Epoch 3350, Loss: 0.4333227872848511, Final Batch Loss: 0.13984622061252594\n",
      "Epoch 3351, Loss: 0.35516121983528137, Final Batch Loss: 0.12934193015098572\n",
      "Epoch 3352, Loss: 0.31420787423849106, Final Batch Loss: 0.07913510501384735\n",
      "Epoch 3353, Loss: 0.4733869433403015, Final Batch Loss: 0.12661099433898926\n",
      "Epoch 3354, Loss: 0.4689675271511078, Final Batch Loss: 0.20779500901699066\n",
      "Epoch 3355, Loss: 0.3855244815349579, Final Batch Loss: 0.1069219708442688\n",
      "Epoch 3356, Loss: 0.4390479028224945, Final Batch Loss: 0.17968611419200897\n",
      "Epoch 3357, Loss: 0.3013320118188858, Final Batch Loss: 0.058502040803432465\n",
      "Epoch 3358, Loss: 0.33277470991015434, Final Batch Loss: 0.05796826258301735\n",
      "Epoch 3359, Loss: 0.4939551129937172, Final Batch Loss: 0.22469750046730042\n",
      "Epoch 3360, Loss: 0.3572341278195381, Final Batch Loss: 0.06951814144849777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3361, Loss: 0.3277999311685562, Final Batch Loss: 0.08471205830574036\n",
      "Epoch 3362, Loss: 0.3320997282862663, Final Batch Loss: 0.1459016352891922\n",
      "Epoch 3363, Loss: 0.44859471917152405, Final Batch Loss: 0.21334503591060638\n",
      "Epoch 3364, Loss: 0.3418233245611191, Final Batch Loss: 0.10617565363645554\n",
      "Epoch 3365, Loss: 0.41232284903526306, Final Batch Loss: 0.13943035900592804\n",
      "Epoch 3366, Loss: 0.4192368909716606, Final Batch Loss: 0.12481073290109634\n",
      "Epoch 3367, Loss: 0.3728938698768616, Final Batch Loss: 0.12084805965423584\n",
      "Epoch 3368, Loss: 0.38474665582180023, Final Batch Loss: 0.1620531529188156\n",
      "Epoch 3369, Loss: 0.2865681163966656, Final Batch Loss: 0.051532771438360214\n",
      "Epoch 3370, Loss: 0.365470290184021, Final Batch Loss: 0.14145244657993317\n",
      "Epoch 3371, Loss: 0.3969127759337425, Final Batch Loss: 0.15618710219860077\n",
      "Epoch 3372, Loss: 0.33835549652576447, Final Batch Loss: 0.07450179010629654\n",
      "Epoch 3373, Loss: 0.3512868955731392, Final Batch Loss: 0.09705350548028946\n",
      "Epoch 3374, Loss: 0.45470772683620453, Final Batch Loss: 0.19780458509922028\n",
      "Epoch 3375, Loss: 0.4344482645392418, Final Batch Loss: 0.18197296559810638\n",
      "Epoch 3376, Loss: 0.3324385955929756, Final Batch Loss: 0.14093002676963806\n",
      "Epoch 3377, Loss: 0.34665076434612274, Final Batch Loss: 0.06491959095001221\n",
      "Epoch 3378, Loss: 0.40145546197891235, Final Batch Loss: 0.14017745852470398\n",
      "Epoch 3379, Loss: 0.3955693393945694, Final Batch Loss: 0.12761615216732025\n",
      "Epoch 3380, Loss: 0.3760910928249359, Final Batch Loss: 0.12855343520641327\n",
      "Epoch 3381, Loss: 0.36977312713861465, Final Batch Loss: 0.11446814984083176\n",
      "Epoch 3382, Loss: 0.3976343795657158, Final Batch Loss: 0.14637663960456848\n",
      "Epoch 3383, Loss: 0.3641494885087013, Final Batch Loss: 0.10692619532346725\n",
      "Epoch 3384, Loss: 0.41573259234428406, Final Batch Loss: 0.11072619259357452\n",
      "Epoch 3385, Loss: 0.3592883571982384, Final Batch Loss: 0.11757012456655502\n",
      "Epoch 3386, Loss: 0.32077228277921677, Final Batch Loss: 0.08295372873544693\n",
      "Epoch 3387, Loss: 0.48495347797870636, Final Batch Loss: 0.21450500190258026\n",
      "Epoch 3388, Loss: 0.3707945644855499, Final Batch Loss: 0.18108995258808136\n",
      "Epoch 3389, Loss: 0.4078834727406502, Final Batch Loss: 0.14050863683223724\n",
      "Epoch 3390, Loss: 0.41216836124658585, Final Batch Loss: 0.1762388050556183\n",
      "Epoch 3391, Loss: 0.2999938353896141, Final Batch Loss: 0.09590387344360352\n",
      "Epoch 3392, Loss: 0.41922450065612793, Final Batch Loss: 0.11612533777952194\n",
      "Epoch 3393, Loss: 0.3851373493671417, Final Batch Loss: 0.14380128681659698\n",
      "Epoch 3394, Loss: 0.32162293791770935, Final Batch Loss: 0.09835433214902878\n",
      "Epoch 3395, Loss: 0.3137400224804878, Final Batch Loss: 0.04643864184617996\n",
      "Epoch 3396, Loss: 0.45062027871608734, Final Batch Loss: 0.19475851953029633\n",
      "Epoch 3397, Loss: 0.4350463002920151, Final Batch Loss: 0.13697724044322968\n",
      "Epoch 3398, Loss: 0.6899548768997192, Final Batch Loss: 0.44756779074668884\n",
      "Epoch 3399, Loss: 0.4690012186765671, Final Batch Loss: 0.16975365579128265\n",
      "Epoch 3400, Loss: 0.4193587154150009, Final Batch Loss: 0.12478774785995483\n",
      "Epoch 3401, Loss: 0.44859620928764343, Final Batch Loss: 0.13525418937206268\n",
      "Epoch 3402, Loss: 0.32674800604581833, Final Batch Loss: 0.047923631966114044\n",
      "Epoch 3403, Loss: 0.3383459448814392, Final Batch Loss: 0.1089639961719513\n",
      "Epoch 3404, Loss: 0.32840902730822563, Final Batch Loss: 0.056423891335725784\n",
      "Epoch 3405, Loss: 0.3310985565185547, Final Batch Loss: 0.1358182728290558\n",
      "Epoch 3406, Loss: 0.36474480479955673, Final Batch Loss: 0.09879527986049652\n",
      "Epoch 3407, Loss: 0.33047305792570114, Final Batch Loss: 0.12481410801410675\n",
      "Epoch 3408, Loss: 0.282718263566494, Final Batch Loss: 0.036608412861824036\n",
      "Epoch 3409, Loss: 0.41501057893037796, Final Batch Loss: 0.09739122539758682\n",
      "Epoch 3410, Loss: 0.3044741600751877, Final Batch Loss: 0.09873008728027344\n",
      "Epoch 3411, Loss: 0.3587745726108551, Final Batch Loss: 0.08111044764518738\n",
      "Epoch 3412, Loss: 0.4741418659687042, Final Batch Loss: 0.19248509407043457\n",
      "Epoch 3413, Loss: 0.3382304087281227, Final Batch Loss: 0.08871045708656311\n",
      "Epoch 3414, Loss: 0.3864544779062271, Final Batch Loss: 0.07813018560409546\n",
      "Epoch 3415, Loss: 0.4329104572534561, Final Batch Loss: 0.1477784663438797\n",
      "Epoch 3416, Loss: 0.31225329637527466, Final Batch Loss: 0.11079481989145279\n",
      "Epoch 3417, Loss: 0.36280710250139236, Final Batch Loss: 0.15611983835697174\n",
      "Epoch 3418, Loss: 0.41775523871183395, Final Batch Loss: 0.11136513203382492\n",
      "Epoch 3419, Loss: 0.32001492753624916, Final Batch Loss: 0.056110356003046036\n",
      "Epoch 3420, Loss: 0.4265977367758751, Final Batch Loss: 0.1739911437034607\n",
      "Epoch 3421, Loss: 0.41119031608104706, Final Batch Loss: 0.1833081692457199\n",
      "Epoch 3422, Loss: 0.351146936416626, Final Batch Loss: 0.10421860218048096\n",
      "Epoch 3423, Loss: 0.3113536387681961, Final Batch Loss: 0.08922939002513885\n",
      "Epoch 3424, Loss: 0.4660438299179077, Final Batch Loss: 0.15396612882614136\n",
      "Epoch 3425, Loss: 0.34169652312994003, Final Batch Loss: 0.08500954508781433\n",
      "Epoch 3426, Loss: 0.4189414605498314, Final Batch Loss: 0.18431630730628967\n",
      "Epoch 3427, Loss: 0.3585394024848938, Final Batch Loss: 0.09622739255428314\n",
      "Epoch 3428, Loss: 0.4119172841310501, Final Batch Loss: 0.18062357604503632\n",
      "Epoch 3429, Loss: 0.35097986459732056, Final Batch Loss: 0.11355561017990112\n",
      "Epoch 3430, Loss: 0.28122957423329353, Final Batch Loss: 0.0596071295440197\n",
      "Epoch 3431, Loss: 0.5228297412395477, Final Batch Loss: 0.23119032382965088\n",
      "Epoch 3432, Loss: 0.3704024478793144, Final Batch Loss: 0.12811139225959778\n",
      "Epoch 3433, Loss: 0.42219100147485733, Final Batch Loss: 0.10846369713544846\n",
      "Epoch 3434, Loss: 0.3785069212317467, Final Batch Loss: 0.053752802312374115\n",
      "Epoch 3435, Loss: 0.28473149985074997, Final Batch Loss: 0.06228165328502655\n",
      "Epoch 3436, Loss: 0.41789214313030243, Final Batch Loss: 0.1988951563835144\n",
      "Epoch 3437, Loss: 0.3537002429366112, Final Batch Loss: 0.10773283988237381\n",
      "Epoch 3438, Loss: 0.3446202129125595, Final Batch Loss: 0.0756414383649826\n",
      "Epoch 3439, Loss: 0.3879295215010643, Final Batch Loss: 0.18852290511131287\n",
      "Epoch 3440, Loss: 0.4117618501186371, Final Batch Loss: 0.13160815834999084\n",
      "Epoch 3441, Loss: 0.7726278752088547, Final Batch Loss: 0.4967092275619507\n",
      "Epoch 3442, Loss: 0.3377596512436867, Final Batch Loss: 0.09565750509500504\n",
      "Epoch 3443, Loss: 0.35638462752103806, Final Batch Loss: 0.08542723208665848\n",
      "Epoch 3444, Loss: 0.4175332188606262, Final Batch Loss: 0.13250046968460083\n",
      "Epoch 3445, Loss: 0.32291922718286514, Final Batch Loss: 0.11149845272302628\n",
      "Epoch 3446, Loss: 0.3593359664082527, Final Batch Loss: 0.07390586286783218\n",
      "Epoch 3447, Loss: 0.5909002721309662, Final Batch Loss: 0.320437490940094\n",
      "Epoch 3448, Loss: 0.46398089826107025, Final Batch Loss: 0.20187276601791382\n",
      "Epoch 3449, Loss: 0.44102752208709717, Final Batch Loss: 0.19937777519226074\n",
      "Epoch 3450, Loss: 0.33730191737413406, Final Batch Loss: 0.12656083703041077\n",
      "Epoch 3451, Loss: 0.33424514532089233, Final Batch Loss: 0.10073471814393997\n",
      "Epoch 3452, Loss: 0.44196318835020065, Final Batch Loss: 0.22571036219596863\n",
      "Epoch 3453, Loss: 0.4201299920678139, Final Batch Loss: 0.09758896380662918\n",
      "Epoch 3454, Loss: 0.3710050731897354, Final Batch Loss: 0.1275061070919037\n",
      "Epoch 3455, Loss: 0.37604162096977234, Final Batch Loss: 0.16456864774227142\n",
      "Epoch 3456, Loss: 0.35873397439718246, Final Batch Loss: 0.12262032926082611\n",
      "Epoch 3457, Loss: 0.33415669947862625, Final Batch Loss: 0.12530304491519928\n",
      "Epoch 3458, Loss: 0.3681268244981766, Final Batch Loss: 0.09389741718769073\n",
      "Epoch 3459, Loss: 0.3921091556549072, Final Batch Loss: 0.15346498787403107\n",
      "Epoch 3460, Loss: 0.34304505586624146, Final Batch Loss: 0.0753866508603096\n",
      "Epoch 3461, Loss: 0.26902421563863754, Final Batch Loss: 0.07698826491832733\n",
      "Epoch 3462, Loss: 0.4505389407277107, Final Batch Loss: 0.20006626844406128\n",
      "Epoch 3463, Loss: 0.39582281559705734, Final Batch Loss: 0.11030670255422592\n",
      "Epoch 3464, Loss: 0.563115730881691, Final Batch Loss: 0.32992443442344666\n",
      "Epoch 3465, Loss: 0.3753845915198326, Final Batch Loss: 0.07604176551103592\n",
      "Epoch 3466, Loss: 0.32059480994939804, Final Batch Loss: 0.10152661055326462\n",
      "Epoch 3467, Loss: 0.5381609201431274, Final Batch Loss: 0.3412185311317444\n",
      "Epoch 3468, Loss: 0.33990321308374405, Final Batch Loss: 0.06848351657390594\n",
      "Epoch 3469, Loss: 0.37287330627441406, Final Batch Loss: 0.12382924556732178\n",
      "Epoch 3470, Loss: 0.44775690138339996, Final Batch Loss: 0.146880105137825\n",
      "Epoch 3471, Loss: 0.36652152240276337, Final Batch Loss: 0.08428531885147095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3472, Loss: 0.3732303977012634, Final Batch Loss: 0.12615272402763367\n",
      "Epoch 3473, Loss: 0.37380651384592056, Final Batch Loss: 0.14940115809440613\n",
      "Epoch 3474, Loss: 0.29343580454587936, Final Batch Loss: 0.0682312473654747\n",
      "Epoch 3475, Loss: 0.3965274691581726, Final Batch Loss: 0.13295668363571167\n",
      "Epoch 3476, Loss: 0.35692793130874634, Final Batch Loss: 0.1194438710808754\n",
      "Epoch 3477, Loss: 0.27632470428943634, Final Batch Loss: 0.08074487000703812\n",
      "Epoch 3478, Loss: 0.3133755624294281, Final Batch Loss: 0.11554113775491714\n",
      "Epoch 3479, Loss: 0.37915077060461044, Final Batch Loss: 0.05669229477643967\n",
      "Epoch 3480, Loss: 0.4343492165207863, Final Batch Loss: 0.1694142073392868\n",
      "Epoch 3481, Loss: 0.34389790147542953, Final Batch Loss: 0.07898428291082382\n",
      "Epoch 3482, Loss: 0.26445838063955307, Final Batch Loss: 0.0618954673409462\n",
      "Epoch 3483, Loss: 0.40910984575748444, Final Batch Loss: 0.16441792249679565\n",
      "Epoch 3484, Loss: 0.294818751513958, Final Batch Loss: 0.08484300971031189\n",
      "Epoch 3485, Loss: 0.3997664526104927, Final Batch Loss: 0.15922874212265015\n",
      "Epoch 3486, Loss: 0.30440761893987656, Final Batch Loss: 0.0776759684085846\n",
      "Epoch 3487, Loss: 0.4857918322086334, Final Batch Loss: 0.22176775336265564\n",
      "Epoch 3488, Loss: 0.37271522730588913, Final Batch Loss: 0.09458796679973602\n",
      "Epoch 3489, Loss: 0.4064938873052597, Final Batch Loss: 0.11427364498376846\n",
      "Epoch 3490, Loss: 0.38692139834165573, Final Batch Loss: 0.1468726396560669\n",
      "Epoch 3491, Loss: 0.3207281529903412, Final Batch Loss: 0.07230538874864578\n",
      "Epoch 3492, Loss: 0.43779097497463226, Final Batch Loss: 0.21762733161449432\n",
      "Epoch 3493, Loss: 0.30069025233387947, Final Batch Loss: 0.058839526027441025\n",
      "Epoch 3494, Loss: 0.44136323034763336, Final Batch Loss: 0.18796423077583313\n",
      "Epoch 3495, Loss: 0.34767938405275345, Final Batch Loss: 0.10627022385597229\n",
      "Epoch 3496, Loss: 0.3968425542116165, Final Batch Loss: 0.10801513493061066\n",
      "Epoch 3497, Loss: 0.3911169469356537, Final Batch Loss: 0.0637739896774292\n",
      "Epoch 3498, Loss: 0.40308862179517746, Final Batch Loss: 0.15832993388175964\n",
      "Epoch 3499, Loss: 0.43607378005981445, Final Batch Loss: 0.14740446209907532\n",
      "Epoch 3500, Loss: 0.3916166350245476, Final Batch Loss: 0.11004260182380676\n",
      "Epoch 3501, Loss: 0.3747577965259552, Final Batch Loss: 0.16703377664089203\n",
      "Epoch 3502, Loss: 0.33282481878995895, Final Batch Loss: 0.06648009270429611\n",
      "Epoch 3503, Loss: 0.4271269589662552, Final Batch Loss: 0.1817176342010498\n",
      "Epoch 3504, Loss: 0.49622633308172226, Final Batch Loss: 0.25854501128196716\n",
      "Epoch 3505, Loss: 0.5085849538445473, Final Batch Loss: 0.18000169098377228\n",
      "Epoch 3506, Loss: 0.40411993116140366, Final Batch Loss: 0.14658363163471222\n",
      "Epoch 3507, Loss: 0.3785438984632492, Final Batch Loss: 0.08438307046890259\n",
      "Epoch 3508, Loss: 0.3529319614171982, Final Batch Loss: 0.14963801205158234\n",
      "Epoch 3509, Loss: 0.41247354820370674, Final Batch Loss: 0.057130325585603714\n",
      "Epoch 3510, Loss: 0.40064218640327454, Final Batch Loss: 0.13278080523014069\n",
      "Epoch 3511, Loss: 0.35892297327518463, Final Batch Loss: 0.12748728692531586\n",
      "Epoch 3512, Loss: 0.3122808113694191, Final Batch Loss: 0.0853036567568779\n",
      "Epoch 3513, Loss: 0.30255772918462753, Final Batch Loss: 0.07323813438415527\n",
      "Epoch 3514, Loss: 0.3009496256709099, Final Batch Loss: 0.0882408544421196\n",
      "Epoch 3515, Loss: 0.45396775752305984, Final Batch Loss: 0.19604767858982086\n",
      "Epoch 3516, Loss: 0.3302430659532547, Final Batch Loss: 0.11184685677289963\n",
      "Epoch 3517, Loss: 0.33282751590013504, Final Batch Loss: 0.09027791768312454\n",
      "Epoch 3518, Loss: 0.2958288937807083, Final Batch Loss: 0.0806715190410614\n",
      "Epoch 3519, Loss: 0.4304124116897583, Final Batch Loss: 0.14508754014968872\n",
      "Epoch 3520, Loss: 0.28957514464855194, Final Batch Loss: 0.04058709740638733\n",
      "Epoch 3521, Loss: 0.47512049973011017, Final Batch Loss: 0.22966010868549347\n",
      "Epoch 3522, Loss: 0.3606073409318924, Final Batch Loss: 0.12745359539985657\n",
      "Epoch 3523, Loss: 0.34665746986866, Final Batch Loss: 0.14744286239147186\n",
      "Epoch 3524, Loss: 0.33399128913879395, Final Batch Loss: 0.1079481765627861\n",
      "Epoch 3525, Loss: 0.41522490233182907, Final Batch Loss: 0.094905786216259\n",
      "Epoch 3526, Loss: 0.37121450901031494, Final Batch Loss: 0.07879123091697693\n",
      "Epoch 3527, Loss: 0.30465781688690186, Final Batch Loss: 0.06963896006345749\n",
      "Epoch 3528, Loss: 0.353651225566864, Final Batch Loss: 0.11916813254356384\n",
      "Epoch 3529, Loss: 0.34783656895160675, Final Batch Loss: 0.14259111881256104\n",
      "Epoch 3530, Loss: 0.3369273245334625, Final Batch Loss: 0.07783998548984528\n",
      "Epoch 3531, Loss: 0.40793799608945847, Final Batch Loss: 0.08893034607172012\n",
      "Epoch 3532, Loss: 0.3034563660621643, Final Batch Loss: 0.05459972470998764\n",
      "Epoch 3533, Loss: 0.4277108758687973, Final Batch Loss: 0.09288036823272705\n",
      "Epoch 3534, Loss: 0.34475576877593994, Final Batch Loss: 0.0713086724281311\n",
      "Epoch 3535, Loss: 0.37652891129255295, Final Batch Loss: 0.131486713886261\n",
      "Epoch 3536, Loss: 0.40968993306159973, Final Batch Loss: 0.16452497243881226\n",
      "Epoch 3537, Loss: 0.34412138909101486, Final Batch Loss: 0.12561112642288208\n",
      "Epoch 3538, Loss: 0.30101029574871063, Final Batch Loss: 0.09818214178085327\n",
      "Epoch 3539, Loss: 0.4639018699526787, Final Batch Loss: 0.1882646083831787\n",
      "Epoch 3540, Loss: 0.33517881482839584, Final Batch Loss: 0.11314098536968231\n",
      "Epoch 3541, Loss: 0.44529399275779724, Final Batch Loss: 0.18795736134052277\n",
      "Epoch 3542, Loss: 0.392814002931118, Final Batch Loss: 0.12100040912628174\n",
      "Epoch 3543, Loss: 0.3366226851940155, Final Batch Loss: 0.12050309777259827\n",
      "Epoch 3544, Loss: 0.27019916847348213, Final Batch Loss: 0.05207635834813118\n",
      "Epoch 3545, Loss: 0.31179144978523254, Final Batch Loss: 0.07120741903781891\n",
      "Epoch 3546, Loss: 0.2792907543480396, Final Batch Loss: 0.046467091888189316\n",
      "Epoch 3547, Loss: 0.4333727955818176, Final Batch Loss: 0.12632668018341064\n",
      "Epoch 3548, Loss: 0.29089073091745377, Final Batch Loss: 0.035703979432582855\n",
      "Epoch 3549, Loss: 0.3422214388847351, Final Batch Loss: 0.10006837546825409\n",
      "Epoch 3550, Loss: 0.4076087512075901, Final Batch Loss: 0.0434606559574604\n",
      "Epoch 3551, Loss: 0.39019786566495895, Final Batch Loss: 0.06581705063581467\n",
      "Epoch 3552, Loss: 0.3058435544371605, Final Batch Loss: 0.13379453122615814\n",
      "Epoch 3553, Loss: 0.38226158171892166, Final Batch Loss: 0.12968048453330994\n",
      "Epoch 3554, Loss: 0.40436068177223206, Final Batch Loss: 0.1854720562696457\n",
      "Epoch 3555, Loss: 0.3365728259086609, Final Batch Loss: 0.12452811747789383\n",
      "Epoch 3556, Loss: 0.3752264454960823, Final Batch Loss: 0.13659602403640747\n",
      "Epoch 3557, Loss: 0.3472069352865219, Final Batch Loss: 0.14387984573841095\n",
      "Epoch 3558, Loss: 0.322334848344326, Final Batch Loss: 0.11622432619333267\n",
      "Epoch 3559, Loss: 0.4132002964615822, Final Batch Loss: 0.0902947410941124\n",
      "Epoch 3560, Loss: 0.3367661386728287, Final Batch Loss: 0.10221594572067261\n",
      "Epoch 3561, Loss: 0.3032294884324074, Final Batch Loss: 0.08700201660394669\n",
      "Epoch 3562, Loss: 0.3369952216744423, Final Batch Loss: 0.046446435153484344\n",
      "Epoch 3563, Loss: 0.2785127982497215, Final Batch Loss: 0.07150661945343018\n",
      "Epoch 3564, Loss: 0.35889387875795364, Final Batch Loss: 0.061984606087207794\n",
      "Epoch 3565, Loss: 0.3089836612343788, Final Batch Loss: 0.07251471281051636\n",
      "Epoch 3566, Loss: 0.36388295888900757, Final Batch Loss: 0.1256331205368042\n",
      "Epoch 3567, Loss: 0.34858958423137665, Final Batch Loss: 0.1297900229692459\n",
      "Epoch 3568, Loss: 0.2585533857345581, Final Batch Loss: 0.08031132072210312\n",
      "Epoch 3569, Loss: 0.4376467764377594, Final Batch Loss: 0.1678924560546875\n",
      "Epoch 3570, Loss: 0.39787768572568893, Final Batch Loss: 0.1541336178779602\n",
      "Epoch 3571, Loss: 0.3308621942996979, Final Batch Loss: 0.08341748267412186\n",
      "Epoch 3572, Loss: 0.2553961053490639, Final Batch Loss: 0.051164835691452026\n",
      "Epoch 3573, Loss: 0.3902391046285629, Final Batch Loss: 0.16555698215961456\n",
      "Epoch 3574, Loss: 0.32214831560850143, Final Batch Loss: 0.10478579252958298\n",
      "Epoch 3575, Loss: 0.3758629932999611, Final Batch Loss: 0.10211550444364548\n",
      "Epoch 3576, Loss: 0.348693810403347, Final Batch Loss: 0.1393214762210846\n",
      "Epoch 3577, Loss: 0.3944903239607811, Final Batch Loss: 0.09382156282663345\n",
      "Epoch 3578, Loss: 0.4461083188652992, Final Batch Loss: 0.21840108931064606\n",
      "Epoch 3579, Loss: 0.28147516399621964, Final Batch Loss: 0.06455078721046448\n",
      "Epoch 3580, Loss: 0.332285039126873, Final Batch Loss: 0.08720365166664124\n",
      "Epoch 3581, Loss: 0.35635898262262344, Final Batch Loss: 0.11796604096889496\n",
      "Epoch 3582, Loss: 0.37185077369213104, Final Batch Loss: 0.14673316478729248\n",
      "Epoch 3583, Loss: 0.3200424425303936, Final Batch Loss: 0.05763090029358864\n",
      "Epoch 3584, Loss: 0.35295190662145615, Final Batch Loss: 0.07240459322929382\n",
      "Epoch 3585, Loss: 0.5286675840616226, Final Batch Loss: 0.2469702810049057\n",
      "Epoch 3586, Loss: 0.29425616189837456, Final Batch Loss: 0.04590953513979912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3587, Loss: 0.3810255005955696, Final Batch Loss: 0.19230148196220398\n",
      "Epoch 3588, Loss: 0.4952058270573616, Final Batch Loss: 0.2021845281124115\n",
      "Epoch 3589, Loss: 0.47683101892471313, Final Batch Loss: 0.20256781578063965\n",
      "Epoch 3590, Loss: 0.45828238129615784, Final Batch Loss: 0.17172451317310333\n",
      "Epoch 3591, Loss: 0.43423060327768326, Final Batch Loss: 0.18951416015625\n",
      "Epoch 3592, Loss: 0.33596330136060715, Final Batch Loss: 0.07009267807006836\n",
      "Epoch 3593, Loss: 0.38514846563339233, Final Batch Loss: 0.10654497891664505\n",
      "Epoch 3594, Loss: 0.4472864419221878, Final Batch Loss: 0.16523723304271698\n",
      "Epoch 3595, Loss: 0.3644617423415184, Final Batch Loss: 0.08566880971193314\n",
      "Epoch 3596, Loss: 0.4488654136657715, Final Batch Loss: 0.15849950909614563\n",
      "Epoch 3597, Loss: 0.294991921633482, Final Batch Loss: 0.04674801602959633\n",
      "Epoch 3598, Loss: 0.2664497084915638, Final Batch Loss: 0.05418943241238594\n",
      "Epoch 3599, Loss: 0.344720758497715, Final Batch Loss: 0.06648046523332596\n",
      "Epoch 3600, Loss: 0.3728889748454094, Final Batch Loss: 0.06577464938163757\n",
      "Epoch 3601, Loss: 0.41871586441993713, Final Batch Loss: 0.16273459792137146\n",
      "Epoch 3602, Loss: 0.33392050862312317, Final Batch Loss: 0.09597200900316238\n",
      "Epoch 3603, Loss: 0.32696668058633804, Final Batch Loss: 0.04387160390615463\n",
      "Epoch 3604, Loss: 0.36301613599061966, Final Batch Loss: 0.12087776511907578\n",
      "Epoch 3605, Loss: 0.3207313269376755, Final Batch Loss: 0.08570662140846252\n",
      "Epoch 3606, Loss: 0.30099452286958694, Final Batch Loss: 0.07940405607223511\n",
      "Epoch 3607, Loss: 0.3721238970756531, Final Batch Loss: 0.08556798100471497\n",
      "Epoch 3608, Loss: 0.4555356651544571, Final Batch Loss: 0.12860551476478577\n",
      "Epoch 3609, Loss: 0.4351579323410988, Final Batch Loss: 0.14192061126232147\n",
      "Epoch 3610, Loss: 0.3911881074309349, Final Batch Loss: 0.13622510433197021\n",
      "Epoch 3611, Loss: 0.45106425881385803, Final Batch Loss: 0.24007897078990936\n",
      "Epoch 3612, Loss: 0.30707110464572906, Final Batch Loss: 0.09407506883144379\n",
      "Epoch 3613, Loss: 0.3627229630947113, Final Batch Loss: 0.09116669744253159\n",
      "Epoch 3614, Loss: 0.4203988090157509, Final Batch Loss: 0.1974293440580368\n",
      "Epoch 3615, Loss: 0.5213392078876495, Final Batch Loss: 0.27573665976524353\n",
      "Epoch 3616, Loss: 0.4283764362335205, Final Batch Loss: 0.1776699721813202\n",
      "Epoch 3617, Loss: 0.3916705325245857, Final Batch Loss: 0.18932126462459564\n",
      "Epoch 3618, Loss: 0.39041586965322495, Final Batch Loss: 0.12087790668010712\n",
      "Epoch 3619, Loss: 0.36161164194345474, Final Batch Loss: 0.09105975925922394\n",
      "Epoch 3620, Loss: 0.42384108155965805, Final Batch Loss: 0.2030903697013855\n",
      "Epoch 3621, Loss: 0.38885290175676346, Final Batch Loss: 0.09939256310462952\n",
      "Epoch 3622, Loss: 0.37407398968935013, Final Batch Loss: 0.08538340777158737\n",
      "Epoch 3623, Loss: 0.43808697909116745, Final Batch Loss: 0.17846617102622986\n",
      "Epoch 3624, Loss: 0.476790651679039, Final Batch Loss: 0.23327338695526123\n",
      "Epoch 3625, Loss: 0.32787468284368515, Final Batch Loss: 0.03258046507835388\n",
      "Epoch 3626, Loss: 0.3180922642350197, Final Batch Loss: 0.13492777943611145\n",
      "Epoch 3627, Loss: 0.3018314316868782, Final Batch Loss: 0.11425713449716568\n",
      "Epoch 3628, Loss: 0.42900556325912476, Final Batch Loss: 0.23678065836429596\n",
      "Epoch 3629, Loss: 0.445356085896492, Final Batch Loss: 0.13282233476638794\n",
      "Epoch 3630, Loss: 0.35312697291374207, Final Batch Loss: 0.14410077035427094\n",
      "Epoch 3631, Loss: 0.3654481768608093, Final Batch Loss: 0.12734729051589966\n",
      "Epoch 3632, Loss: 0.4270080700516701, Final Batch Loss: 0.12228237837553024\n",
      "Epoch 3633, Loss: 0.5266644582152367, Final Batch Loss: 0.32898131012916565\n",
      "Epoch 3634, Loss: 0.3379298374056816, Final Batch Loss: 0.10602128505706787\n",
      "Epoch 3635, Loss: 0.274360753595829, Final Batch Loss: 0.042917922139167786\n",
      "Epoch 3636, Loss: 0.33941981941461563, Final Batch Loss: 0.051728807389736176\n",
      "Epoch 3637, Loss: 0.3910660967230797, Final Batch Loss: 0.11570511758327484\n",
      "Epoch 3638, Loss: 0.3361995965242386, Final Batch Loss: 0.11829609423875809\n",
      "Epoch 3639, Loss: 0.2858171984553337, Final Batch Loss: 0.06949763745069504\n",
      "Epoch 3640, Loss: 0.34883519262075424, Final Batch Loss: 0.08499497175216675\n",
      "Epoch 3641, Loss: 0.26321312598884106, Final Batch Loss: 0.026969490572810173\n",
      "Epoch 3642, Loss: 0.3030208945274353, Final Batch Loss: 0.0713539868593216\n",
      "Epoch 3643, Loss: 0.3215276747941971, Final Batch Loss: 0.07529474049806595\n",
      "Epoch 3644, Loss: 0.32523714005947113, Final Batch Loss: 0.10797519236803055\n",
      "Epoch 3645, Loss: 0.27702928334474564, Final Batch Loss: 0.06523984670639038\n",
      "Epoch 3646, Loss: 0.2797313742339611, Final Batch Loss: 0.05787844583392143\n",
      "Epoch 3647, Loss: 0.4015807434916496, Final Batch Loss: 0.10528574883937836\n",
      "Epoch 3648, Loss: 0.3717707246541977, Final Batch Loss: 0.13446514308452606\n",
      "Epoch 3649, Loss: 0.34235651791095734, Final Batch Loss: 0.1226208359003067\n",
      "Epoch 3650, Loss: 0.36196450144052505, Final Batch Loss: 0.07335119694471359\n",
      "Epoch 3651, Loss: 0.38394321501255035, Final Batch Loss: 0.070209801197052\n",
      "Epoch 3652, Loss: 0.3061802089214325, Final Batch Loss: 0.06178830564022064\n",
      "Epoch 3653, Loss: 0.3316270485520363, Final Batch Loss: 0.03361397236585617\n",
      "Epoch 3654, Loss: 0.36807356774806976, Final Batch Loss: 0.10448650270700455\n",
      "Epoch 3655, Loss: 0.4432884380221367, Final Batch Loss: 0.104749895632267\n",
      "Epoch 3656, Loss: 0.42955251038074493, Final Batch Loss: 0.10714304447174072\n",
      "Epoch 3657, Loss: 0.42421309649944305, Final Batch Loss: 0.17406544089317322\n",
      "Epoch 3658, Loss: 0.4027510955929756, Final Batch Loss: 0.11665105074644089\n",
      "Epoch 3659, Loss: 0.41329850256443024, Final Batch Loss: 0.15468652546405792\n",
      "Epoch 3660, Loss: 0.3759398087859154, Final Batch Loss: 0.11503587663173676\n",
      "Epoch 3661, Loss: 0.3842044249176979, Final Batch Loss: 0.1250339299440384\n",
      "Epoch 3662, Loss: 0.3027179539203644, Final Batch Loss: 0.07628199458122253\n",
      "Epoch 3663, Loss: 0.45164504647254944, Final Batch Loss: 0.1872793734073639\n",
      "Epoch 3664, Loss: 0.38557328283786774, Final Batch Loss: 0.07450923323631287\n",
      "Epoch 3665, Loss: 0.4216277077794075, Final Batch Loss: 0.1308518946170807\n",
      "Epoch 3666, Loss: 0.49729569256305695, Final Batch Loss: 0.17606399953365326\n",
      "Epoch 3667, Loss: 0.3842690587043762, Final Batch Loss: 0.13724832236766815\n",
      "Epoch 3668, Loss: 0.3861137852072716, Final Batch Loss: 0.15740594267845154\n",
      "Epoch 3669, Loss: 0.4384813532233238, Final Batch Loss: 0.2120363712310791\n",
      "Epoch 3670, Loss: 0.4908609986305237, Final Batch Loss: 0.1542859822511673\n",
      "Epoch 3671, Loss: 0.36596736311912537, Final Batch Loss: 0.10846289992332458\n",
      "Epoch 3672, Loss: 0.3318930044770241, Final Batch Loss: 0.10636161267757416\n",
      "Epoch 3673, Loss: 0.40246032178401947, Final Batch Loss: 0.17139925062656403\n",
      "Epoch 3674, Loss: 0.24878740310668945, Final Batch Loss: 0.04966408759355545\n",
      "Epoch 3675, Loss: 0.37047476321458817, Final Batch Loss: 0.13718637824058533\n",
      "Epoch 3676, Loss: 0.2930922992527485, Final Batch Loss: 0.042750563472509384\n",
      "Epoch 3677, Loss: 0.2942870333790779, Final Batch Loss: 0.06854137778282166\n",
      "Epoch 3678, Loss: 0.3065474256873131, Final Batch Loss: 0.07069072872400284\n",
      "Epoch 3679, Loss: 0.42767483741045, Final Batch Loss: 0.09613867849111557\n",
      "Epoch 3680, Loss: 0.3751670718193054, Final Batch Loss: 0.1033877432346344\n",
      "Epoch 3681, Loss: 0.313775472342968, Final Batch Loss: 0.08525986224412918\n",
      "Epoch 3682, Loss: 0.39260193705558777, Final Batch Loss: 0.15739335119724274\n",
      "Epoch 3683, Loss: 0.31538502871990204, Final Batch Loss: 0.08135384321212769\n",
      "Epoch 3684, Loss: 0.32656944543123245, Final Batch Loss: 0.12125333398580551\n",
      "Epoch 3685, Loss: 0.37457700818777084, Final Batch Loss: 0.09155359119176865\n",
      "Epoch 3686, Loss: 0.4264420419931412, Final Batch Loss: 0.0925992876291275\n",
      "Epoch 3687, Loss: 0.46542827785015106, Final Batch Loss: 0.18893960118293762\n",
      "Epoch 3688, Loss: 0.3061281591653824, Final Batch Loss: 0.10865551978349686\n",
      "Epoch 3689, Loss: 0.3006898909807205, Final Batch Loss: 0.03227655589580536\n",
      "Epoch 3690, Loss: 0.3644428253173828, Final Batch Loss: 0.12794606387615204\n",
      "Epoch 3691, Loss: 0.293976828455925, Final Batch Loss: 0.09875523298978806\n",
      "Epoch 3692, Loss: 0.36764130741357803, Final Batch Loss: 0.13148550689220428\n",
      "Epoch 3693, Loss: 0.3538903743028641, Final Batch Loss: 0.09072913974523544\n",
      "Epoch 3694, Loss: 0.4197552427649498, Final Batch Loss: 0.1812969297170639\n",
      "Epoch 3695, Loss: 0.3117065280675888, Final Batch Loss: 0.07683820277452469\n",
      "Epoch 3696, Loss: 0.3151063024997711, Final Batch Loss: 0.09814051538705826\n",
      "Epoch 3697, Loss: 0.30671894550323486, Final Batch Loss: 0.10394931584596634\n",
      "Epoch 3698, Loss: 0.3521167188882828, Final Batch Loss: 0.12786360085010529\n",
      "Epoch 3699, Loss: 0.42191696912050247, Final Batch Loss: 0.19092872738838196\n",
      "Epoch 3700, Loss: 0.37162937968969345, Final Batch Loss: 0.1165991872549057\n",
      "Epoch 3701, Loss: 0.35085906088352203, Final Batch Loss: 0.07210919260978699\n",
      "Epoch 3702, Loss: 0.32555073499679565, Final Batch Loss: 0.14025774598121643\n",
      "Epoch 3703, Loss: 0.2642778754234314, Final Batch Loss: 0.03747712820768356\n",
      "Epoch 3704, Loss: 0.4022913873195648, Final Batch Loss: 0.14522849023342133\n",
      "Epoch 3705, Loss: 0.3514088839292526, Final Batch Loss: 0.12183564156293869\n",
      "Epoch 3706, Loss: 0.3400619924068451, Final Batch Loss: 0.13835549354553223\n",
      "Epoch 3707, Loss: 0.5179200768470764, Final Batch Loss: 0.1888195276260376\n",
      "Epoch 3708, Loss: 0.36254002898931503, Final Batch Loss: 0.12716081738471985\n",
      "Epoch 3709, Loss: 0.39775533974170685, Final Batch Loss: 0.18748073279857635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3710, Loss: 0.33876196295022964, Final Batch Loss: 0.10555324703454971\n",
      "Epoch 3711, Loss: 0.3425235375761986, Final Batch Loss: 0.12034854292869568\n",
      "Epoch 3712, Loss: 0.4601128548383713, Final Batch Loss: 0.1211678609251976\n",
      "Epoch 3713, Loss: 0.28789082914590836, Final Batch Loss: 0.06067543476819992\n",
      "Epoch 3714, Loss: 0.3492046222090721, Final Batch Loss: 0.13949885964393616\n",
      "Epoch 3715, Loss: 0.46273934096097946, Final Batch Loss: 0.18374329805374146\n",
      "Epoch 3716, Loss: 0.3706756830215454, Final Batch Loss: 0.09073321521282196\n",
      "Epoch 3717, Loss: 0.2735045962035656, Final Batch Loss: 0.041341084986925125\n",
      "Epoch 3718, Loss: 0.41917379945516586, Final Batch Loss: 0.16669711470603943\n",
      "Epoch 3719, Loss: 0.4845881089568138, Final Batch Loss: 0.2392026036977768\n",
      "Epoch 3720, Loss: 0.38694947212934494, Final Batch Loss: 0.16017530858516693\n",
      "Epoch 3721, Loss: 0.45221658796072006, Final Batch Loss: 0.18473321199417114\n",
      "Epoch 3722, Loss: 0.4251026138663292, Final Batch Loss: 0.16048264503479004\n",
      "Epoch 3723, Loss: 0.342695027589798, Final Batch Loss: 0.07806184887886047\n",
      "Epoch 3724, Loss: 0.31765107810497284, Final Batch Loss: 0.10644295811653137\n",
      "Epoch 3725, Loss: 0.3389090746641159, Final Batch Loss: 0.06193719059228897\n",
      "Epoch 3726, Loss: 0.3034666031599045, Final Batch Loss: 0.06855002790689468\n",
      "Epoch 3727, Loss: 0.3320912942290306, Final Batch Loss: 0.09620054066181183\n",
      "Epoch 3728, Loss: 0.3326861187815666, Final Batch Loss: 0.11788230389356613\n",
      "Epoch 3729, Loss: 0.3687821850180626, Final Batch Loss: 0.10064417868852615\n",
      "Epoch 3730, Loss: 0.340023934841156, Final Batch Loss: 0.07807852327823639\n",
      "Epoch 3731, Loss: 0.36869970709085464, Final Batch Loss: 0.10795968770980835\n",
      "Epoch 3732, Loss: 0.4681096374988556, Final Batch Loss: 0.25778284668922424\n",
      "Epoch 3733, Loss: 0.41202180087566376, Final Batch Loss: 0.10827542096376419\n",
      "Epoch 3734, Loss: 0.3523520603775978, Final Batch Loss: 0.06675989180803299\n",
      "Epoch 3735, Loss: 0.3534306138753891, Final Batch Loss: 0.12749814987182617\n",
      "Epoch 3736, Loss: 0.2930173948407173, Final Batch Loss: 0.07153235375881195\n",
      "Epoch 3737, Loss: 0.31398505717515945, Final Batch Loss: 0.12245816737413406\n",
      "Epoch 3738, Loss: 0.3134477064013481, Final Batch Loss: 0.07096641510725021\n",
      "Epoch 3739, Loss: 0.33131900429725647, Final Batch Loss: 0.1384895145893097\n",
      "Epoch 3740, Loss: 0.4328615367412567, Final Batch Loss: 0.17025555670261383\n",
      "Epoch 3741, Loss: 0.28461238369345665, Final Batch Loss: 0.043754082173109055\n",
      "Epoch 3742, Loss: 0.3350571542978287, Final Batch Loss: 0.08554372936487198\n",
      "Epoch 3743, Loss: 0.34862587973475456, Final Batch Loss: 0.03317156806588173\n",
      "Epoch 3744, Loss: 0.407921627163887, Final Batch Loss: 0.1395503729581833\n",
      "Epoch 3745, Loss: 0.37309249490499496, Final Batch Loss: 0.1399364322423935\n",
      "Epoch 3746, Loss: 0.5329114720225334, Final Batch Loss: 0.2772831916809082\n",
      "Epoch 3747, Loss: 0.4329635947942734, Final Batch Loss: 0.18177825212478638\n",
      "Epoch 3748, Loss: 0.34479624778032303, Final Batch Loss: 0.09783990681171417\n",
      "Epoch 3749, Loss: 0.4437379762530327, Final Batch Loss: 0.21114636957645416\n",
      "Epoch 3750, Loss: 0.38289574533700943, Final Batch Loss: 0.1650010198354721\n",
      "Epoch 3751, Loss: 0.36255598813295364, Final Batch Loss: 0.096328966319561\n",
      "Epoch 3752, Loss: 0.3469370901584625, Final Batch Loss: 0.11068534851074219\n",
      "Epoch 3753, Loss: 0.38382165879011154, Final Batch Loss: 0.10053735971450806\n",
      "Epoch 3754, Loss: 0.3101847618818283, Final Batch Loss: 0.07714752107858658\n",
      "Epoch 3755, Loss: 0.2571575604379177, Final Batch Loss: 0.042459819465875626\n",
      "Epoch 3756, Loss: 0.4220288172364235, Final Batch Loss: 0.0929468497633934\n",
      "Epoch 3757, Loss: 0.31021440029144287, Final Batch Loss: 0.06731857359409332\n",
      "Epoch 3758, Loss: 0.3474721983075142, Final Batch Loss: 0.05195581167936325\n",
      "Epoch 3759, Loss: 0.3778451234102249, Final Batch Loss: 0.13842159509658813\n",
      "Epoch 3760, Loss: 0.3693679869174957, Final Batch Loss: 0.10042401403188705\n",
      "Epoch 3761, Loss: 0.39367738738656044, Final Batch Loss: 0.046667326241731644\n",
      "Epoch 3762, Loss: 0.3149842843413353, Final Batch Loss: 0.06785660237073898\n",
      "Epoch 3763, Loss: 0.2786831557750702, Final Batch Loss: 0.09219834208488464\n",
      "Epoch 3764, Loss: 0.34909406304359436, Final Batch Loss: 0.10218591243028641\n",
      "Epoch 3765, Loss: 0.31032710522413254, Final Batch Loss: 0.08973697572946548\n",
      "Epoch 3766, Loss: 0.2727464586496353, Final Batch Loss: 0.05291137099266052\n",
      "Epoch 3767, Loss: 0.3393348976969719, Final Batch Loss: 0.08882395923137665\n",
      "Epoch 3768, Loss: 0.43214235454797745, Final Batch Loss: 0.23747210204601288\n",
      "Epoch 3769, Loss: 0.34163402765989304, Final Batch Loss: 0.028168417513370514\n",
      "Epoch 3770, Loss: 0.4425124526023865, Final Batch Loss: 0.2102564424276352\n",
      "Epoch 3771, Loss: 0.4357379525899887, Final Batch Loss: 0.17866076529026031\n",
      "Epoch 3772, Loss: 0.3850904852151871, Final Batch Loss: 0.16540150344371796\n",
      "Epoch 3773, Loss: 0.37743499875068665, Final Batch Loss: 0.11695344746112823\n",
      "Epoch 3774, Loss: 0.3356928378343582, Final Batch Loss: 0.06014351546764374\n",
      "Epoch 3775, Loss: 0.2697064094245434, Final Batch Loss: 0.052549999207258224\n",
      "Epoch 3776, Loss: 0.4053633511066437, Final Batch Loss: 0.14822304248809814\n",
      "Epoch 3777, Loss: 0.31643271818757057, Final Batch Loss: 0.059673164039850235\n",
      "Epoch 3778, Loss: 0.38120365887880325, Final Batch Loss: 0.11916064471006393\n",
      "Epoch 3779, Loss: 0.33509450405836105, Final Batch Loss: 0.10652520507574081\n",
      "Epoch 3780, Loss: 0.321429006755352, Final Batch Loss: 0.016937918961048126\n",
      "Epoch 3781, Loss: 0.4032229408621788, Final Batch Loss: 0.1720811277627945\n",
      "Epoch 3782, Loss: 0.39911623299121857, Final Batch Loss: 0.11330800503492355\n",
      "Epoch 3783, Loss: 0.4724235236644745, Final Batch Loss: 0.22453951835632324\n",
      "Epoch 3784, Loss: 0.39187825471162796, Final Batch Loss: 0.19476725161075592\n",
      "Epoch 3785, Loss: 0.5540003329515457, Final Batch Loss: 0.2894611656665802\n",
      "Epoch 3786, Loss: 0.39406222850084305, Final Batch Loss: 0.14232763648033142\n",
      "Epoch 3787, Loss: 0.4336630702018738, Final Batch Loss: 0.15421023964881897\n",
      "Epoch 3788, Loss: 0.2924836128950119, Final Batch Loss: 0.07886099070310593\n",
      "Epoch 3789, Loss: 0.5163395032286644, Final Batch Loss: 0.2896621525287628\n",
      "Epoch 3790, Loss: 0.3058187887072563, Final Batch Loss: 0.07337318360805511\n",
      "Epoch 3791, Loss: 0.37116504460573196, Final Batch Loss: 0.10698095709085464\n",
      "Epoch 3792, Loss: 0.30266258865594864, Final Batch Loss: 0.07158256322145462\n",
      "Epoch 3793, Loss: 0.4289596155285835, Final Batch Loss: 0.1968778669834137\n",
      "Epoch 3794, Loss: 0.5101163014769554, Final Batch Loss: 0.272335410118103\n",
      "Epoch 3795, Loss: 0.4762793481349945, Final Batch Loss: 0.19559796154499054\n",
      "Epoch 3796, Loss: 0.40189870446920395, Final Batch Loss: 0.17948924005031586\n",
      "Epoch 3797, Loss: 0.32050373405218124, Final Batch Loss: 0.09455085545778275\n",
      "Epoch 3798, Loss: 0.5230611115694046, Final Batch Loss: 0.23743756115436554\n",
      "Epoch 3799, Loss: 0.3660450652241707, Final Batch Loss: 0.13364379107952118\n",
      "Epoch 3800, Loss: 0.351470522582531, Final Batch Loss: 0.10954087227582932\n",
      "Epoch 3801, Loss: 0.34005308896303177, Final Batch Loss: 0.05195016413927078\n",
      "Epoch 3802, Loss: 0.3965820223093033, Final Batch Loss: 0.12039642781019211\n",
      "Epoch 3803, Loss: 0.40087220072746277, Final Batch Loss: 0.1380395144224167\n",
      "Epoch 3804, Loss: 0.34400688856840134, Final Batch Loss: 0.06955508142709732\n",
      "Epoch 3805, Loss: 0.37915903329849243, Final Batch Loss: 0.08288684487342834\n",
      "Epoch 3806, Loss: 0.4824233502149582, Final Batch Loss: 0.1577824056148529\n",
      "Epoch 3807, Loss: 0.5112492740154266, Final Batch Loss: 0.2465042620897293\n",
      "Epoch 3808, Loss: 0.4380473494529724, Final Batch Loss: 0.13654173910617828\n",
      "Epoch 3809, Loss: 0.36300433427095413, Final Batch Loss: 0.08969733119010925\n",
      "Epoch 3810, Loss: 0.4933079928159714, Final Batch Loss: 0.21529102325439453\n",
      "Epoch 3811, Loss: 0.5019268691539764, Final Batch Loss: 0.16907232999801636\n",
      "Epoch 3812, Loss: 0.3291603848338127, Final Batch Loss: 0.08753623068332672\n",
      "Epoch 3813, Loss: 0.3906043842434883, Final Batch Loss: 0.10609576851129532\n",
      "Epoch 3814, Loss: 0.47498925030231476, Final Batch Loss: 0.19826169312000275\n",
      "Epoch 3815, Loss: 0.4344141036272049, Final Batch Loss: 0.14557497203350067\n",
      "Epoch 3816, Loss: 0.40797847509384155, Final Batch Loss: 0.13104753196239471\n",
      "Epoch 3817, Loss: 0.34607675671577454, Final Batch Loss: 0.09068187326192856\n",
      "Epoch 3818, Loss: 0.36344730108976364, Final Batch Loss: 0.1400715708732605\n",
      "Epoch 3819, Loss: 0.3518482372164726, Final Batch Loss: 0.06938531249761581\n",
      "Epoch 3820, Loss: 0.3738310858607292, Final Batch Loss: 0.10597193986177444\n",
      "Epoch 3821, Loss: 0.35536664724349976, Final Batch Loss: 0.07617603242397308\n",
      "Epoch 3822, Loss: 0.4652101397514343, Final Batch Loss: 0.22019445896148682\n",
      "Epoch 3823, Loss: 0.33195919543504715, Final Batch Loss: 0.10302948206663132\n",
      "Epoch 3824, Loss: 0.40493496507406235, Final Batch Loss: 0.11883481591939926\n",
      "Epoch 3825, Loss: 0.4745342433452606, Final Batch Loss: 0.15315988659858704\n",
      "Epoch 3826, Loss: 0.45621511340141296, Final Batch Loss: 0.23002460598945618\n",
      "Epoch 3827, Loss: 0.3307366743683815, Final Batch Loss: 0.09954820573329926\n",
      "Epoch 3828, Loss: 0.3256470039486885, Final Batch Loss: 0.09922996163368225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3829, Loss: 0.39701175689697266, Final Batch Loss: 0.12956424057483673\n",
      "Epoch 3830, Loss: 0.37035439163446426, Final Batch Loss: 0.13390310108661652\n",
      "Epoch 3831, Loss: 0.3246706649661064, Final Batch Loss: 0.09160569310188293\n",
      "Epoch 3832, Loss: 0.3235836550593376, Final Batch Loss: 0.06876406073570251\n",
      "Epoch 3833, Loss: 0.38450372964143753, Final Batch Loss: 0.09513016790151596\n",
      "Epoch 3834, Loss: 0.4059649929404259, Final Batch Loss: 0.09429817646741867\n",
      "Epoch 3835, Loss: 0.3727729991078377, Final Batch Loss: 0.12933817505836487\n",
      "Epoch 3836, Loss: 0.3428630977869034, Final Batch Loss: 0.06404730677604675\n",
      "Epoch 3837, Loss: 0.3552117794752121, Final Batch Loss: 0.08808793127536774\n",
      "Epoch 3838, Loss: 0.39427630603313446, Final Batch Loss: 0.17574985325336456\n",
      "Epoch 3839, Loss: 0.3812641128897667, Final Batch Loss: 0.17024677991867065\n",
      "Epoch 3840, Loss: 0.38086777925491333, Final Batch Loss: 0.10939497500658035\n",
      "Epoch 3841, Loss: 0.33229880034923553, Final Batch Loss: 0.11954605579376221\n",
      "Epoch 3842, Loss: 0.34869738668203354, Final Batch Loss: 0.07787822932004929\n",
      "Epoch 3843, Loss: 0.26580002158880234, Final Batch Loss: 0.07686556130647659\n",
      "Epoch 3844, Loss: 0.3357193022966385, Final Batch Loss: 0.10746737569570541\n",
      "Epoch 3845, Loss: 0.29522009938955307, Final Batch Loss: 0.07724538445472717\n",
      "Epoch 3846, Loss: 0.24870144203305244, Final Batch Loss: 0.05430493876338005\n",
      "Epoch 3847, Loss: 0.37132252007722855, Final Batch Loss: 0.1305343210697174\n",
      "Epoch 3848, Loss: 0.33044277131557465, Final Batch Loss: 0.06112639605998993\n",
      "Epoch 3849, Loss: 0.35497161746025085, Final Batch Loss: 0.1110045462846756\n",
      "Epoch 3850, Loss: 0.3677559420466423, Final Batch Loss: 0.0949346125125885\n",
      "Epoch 3851, Loss: 0.32887012511491776, Final Batch Loss: 0.1559303104877472\n",
      "Epoch 3852, Loss: 0.41397715359926224, Final Batch Loss: 0.18912625312805176\n",
      "Epoch 3853, Loss: 0.3195996880531311, Final Batch Loss: 0.08690084517002106\n",
      "Epoch 3854, Loss: 0.4000445753335953, Final Batch Loss: 0.16369056701660156\n",
      "Epoch 3855, Loss: 0.4280196726322174, Final Batch Loss: 0.12196104228496552\n",
      "Epoch 3856, Loss: 0.5026432275772095, Final Batch Loss: 0.295844703912735\n",
      "Epoch 3857, Loss: 0.40156856924295425, Final Batch Loss: 0.17624342441558838\n",
      "Epoch 3858, Loss: 0.32996468245983124, Final Batch Loss: 0.06239387392997742\n",
      "Epoch 3859, Loss: 0.4274964928627014, Final Batch Loss: 0.16641439497470856\n",
      "Epoch 3860, Loss: 0.34757451713085175, Final Batch Loss: 0.1503923237323761\n",
      "Epoch 3861, Loss: 0.26074326038360596, Final Batch Loss: 0.08194131404161453\n",
      "Epoch 3862, Loss: 0.27410688996315, Final Batch Loss: 0.0718810185790062\n",
      "Epoch 3863, Loss: 0.3648717813193798, Final Batch Loss: 0.05858158692717552\n",
      "Epoch 3864, Loss: 0.28859297186136246, Final Batch Loss: 0.10430055856704712\n",
      "Epoch 3865, Loss: 0.35170401632785797, Final Batch Loss: 0.09135227650403976\n",
      "Epoch 3866, Loss: 0.2810642197728157, Final Batch Loss: 0.060737088322639465\n",
      "Epoch 3867, Loss: 0.36504537612199783, Final Batch Loss: 0.1069416031241417\n",
      "Epoch 3868, Loss: 0.36952856183052063, Final Batch Loss: 0.15741047263145447\n",
      "Epoch 3869, Loss: 0.32242193073034286, Final Batch Loss: 0.0910869613289833\n",
      "Epoch 3870, Loss: 0.3711973503232002, Final Batch Loss: 0.1508418470621109\n",
      "Epoch 3871, Loss: 0.2942466735839844, Final Batch Loss: 0.08139736950397491\n",
      "Epoch 3872, Loss: 0.28570596873760223, Final Batch Loss: 0.07302464544773102\n",
      "Epoch 3873, Loss: 0.31540967524051666, Final Batch Loss: 0.07304368913173676\n",
      "Epoch 3874, Loss: 0.3671555668115616, Final Batch Loss: 0.06591755151748657\n",
      "Epoch 3875, Loss: 0.31793542951345444, Final Batch Loss: 0.09575176984071732\n",
      "Epoch 3876, Loss: 0.3470081388950348, Final Batch Loss: 0.11724790930747986\n",
      "Epoch 3877, Loss: 0.4145306125283241, Final Batch Loss: 0.1886928528547287\n",
      "Epoch 3878, Loss: 0.30530449748039246, Final Batch Loss: 0.09865087270736694\n",
      "Epoch 3879, Loss: 0.3086969517171383, Final Batch Loss: 0.04708017781376839\n",
      "Epoch 3880, Loss: 0.30955418944358826, Final Batch Loss: 0.10209652036428452\n",
      "Epoch 3881, Loss: 0.6352033615112305, Final Batch Loss: 0.4128738045692444\n",
      "Epoch 3882, Loss: 0.2936314269900322, Final Batch Loss: 0.07675182074308395\n",
      "Epoch 3883, Loss: 0.283920519053936, Final Batch Loss: 0.03940051794052124\n",
      "Epoch 3884, Loss: 0.32273930311203003, Final Batch Loss: 0.0756235346198082\n",
      "Epoch 3885, Loss: 0.2599412612617016, Final Batch Loss: 0.06230734661221504\n",
      "Epoch 3886, Loss: 0.4515448063611984, Final Batch Loss: 0.22639504075050354\n",
      "Epoch 3887, Loss: 0.3283032923936844, Final Batch Loss: 0.10058831423521042\n",
      "Epoch 3888, Loss: 0.28714393824338913, Final Batch Loss: 0.0684797391295433\n",
      "Epoch 3889, Loss: 0.2905854657292366, Final Batch Loss: 0.05750182271003723\n",
      "Epoch 3890, Loss: 0.3494415134191513, Final Batch Loss: 0.1507062166929245\n",
      "Epoch 3891, Loss: 0.30482493340969086, Final Batch Loss: 0.11845102906227112\n",
      "Epoch 3892, Loss: 0.335608072578907, Final Batch Loss: 0.11228017508983612\n",
      "Epoch 3893, Loss: 0.2899048626422882, Final Batch Loss: 0.07652970403432846\n",
      "Epoch 3894, Loss: 0.448161780834198, Final Batch Loss: 0.19258107244968414\n",
      "Epoch 3895, Loss: 0.2937658354640007, Final Batch Loss: 0.09169892966747284\n",
      "Epoch 3896, Loss: 0.40555471181869507, Final Batch Loss: 0.1372341811656952\n",
      "Epoch 3897, Loss: 0.41582218557596207, Final Batch Loss: 0.16098052263259888\n",
      "Epoch 3898, Loss: 0.3944099023938179, Final Batch Loss: 0.17299948632717133\n",
      "Epoch 3899, Loss: 0.27298684418201447, Final Batch Loss: 0.06458525359630585\n",
      "Epoch 3900, Loss: 0.3823857754468918, Final Batch Loss: 0.13154855370521545\n",
      "Epoch 3901, Loss: 0.2728921249508858, Final Batch Loss: 0.05226089060306549\n",
      "Epoch 3902, Loss: 0.3864732012152672, Final Batch Loss: 0.15014703571796417\n",
      "Epoch 3903, Loss: 0.3236594423651695, Final Batch Loss: 0.02307424694299698\n",
      "Epoch 3904, Loss: 0.4138311892747879, Final Batch Loss: 0.1655970811843872\n",
      "Epoch 3905, Loss: 0.2634613364934921, Final Batch Loss: 0.046382807195186615\n",
      "Epoch 3906, Loss: 0.39077772945165634, Final Batch Loss: 0.11141940206289291\n",
      "Epoch 3907, Loss: 0.38060762733221054, Final Batch Loss: 0.1258968710899353\n",
      "Epoch 3908, Loss: 0.4170771464705467, Final Batch Loss: 0.1045452356338501\n",
      "Epoch 3909, Loss: 0.31800857558846474, Final Batch Loss: 0.062062282115221024\n",
      "Epoch 3910, Loss: 0.3456020802259445, Final Batch Loss: 0.13362851738929749\n",
      "Epoch 3911, Loss: 0.3208802044391632, Final Batch Loss: 0.08358313888311386\n",
      "Epoch 3912, Loss: 0.38416483998298645, Final Batch Loss: 0.09983205795288086\n",
      "Epoch 3913, Loss: 0.3589870184659958, Final Batch Loss: 0.10705862939357758\n",
      "Epoch 3914, Loss: 0.3324708864092827, Final Batch Loss: 0.1136479303240776\n",
      "Epoch 3915, Loss: 0.3052329011261463, Final Batch Loss: 0.05330578610301018\n",
      "Epoch 3916, Loss: 0.25427134335041046, Final Batch Loss: 0.01865815371274948\n",
      "Epoch 3917, Loss: 0.32057805359363556, Final Batch Loss: 0.07683120667934418\n",
      "Epoch 3918, Loss: 0.2894788719713688, Final Batch Loss: 0.049808356910943985\n",
      "Epoch 3919, Loss: 0.36885126680135727, Final Batch Loss: 0.0998946875333786\n",
      "Epoch 3920, Loss: 0.26274101063609123, Final Batch Loss: 0.028422672301530838\n",
      "Epoch 3921, Loss: 0.26386404782533646, Final Batch Loss: 0.0657443106174469\n",
      "Epoch 3922, Loss: 0.39705584198236465, Final Batch Loss: 0.18978479504585266\n",
      "Epoch 3923, Loss: 0.35786645114421844, Final Batch Loss: 0.14578871428966522\n",
      "Epoch 3924, Loss: 0.38224537670612335, Final Batch Loss: 0.15578502416610718\n",
      "Epoch 3925, Loss: 0.3811889961361885, Final Batch Loss: 0.15500986576080322\n",
      "Epoch 3926, Loss: 0.4359406977891922, Final Batch Loss: 0.13073347508907318\n",
      "Epoch 3927, Loss: 0.3556305468082428, Final Batch Loss: 0.15828649699687958\n",
      "Epoch 3928, Loss: 0.373961865901947, Final Batch Loss: 0.15785491466522217\n",
      "Epoch 3929, Loss: 0.2964867688715458, Final Batch Loss: 0.04880228266119957\n",
      "Epoch 3930, Loss: 0.4352967143058777, Final Batch Loss: 0.20987319946289062\n",
      "Epoch 3931, Loss: 0.4047955498099327, Final Batch Loss: 0.15777096152305603\n",
      "Epoch 3932, Loss: 0.3976335749030113, Final Batch Loss: 0.13746123015880585\n",
      "Epoch 3933, Loss: 0.37052157521247864, Final Batch Loss: 0.059876397252082825\n",
      "Epoch 3934, Loss: 0.4186366945505142, Final Batch Loss: 0.15006642043590546\n",
      "Epoch 3935, Loss: 0.3765416443347931, Final Batch Loss: 0.14526110887527466\n",
      "Epoch 3936, Loss: 0.25194601714611053, Final Batch Loss: 0.038101330399513245\n",
      "Epoch 3937, Loss: 0.404376782476902, Final Batch Loss: 0.1648789793252945\n",
      "Epoch 3938, Loss: 0.5939915329217911, Final Batch Loss: 0.3805181086063385\n",
      "Epoch 3939, Loss: 0.3266522288322449, Final Batch Loss: 0.10544238984584808\n",
      "Epoch 3940, Loss: 0.396761454641819, Final Batch Loss: 0.11657866835594177\n",
      "Epoch 3941, Loss: 0.4474995657801628, Final Batch Loss: 0.22570310533046722\n",
      "Epoch 3942, Loss: 0.27210231497883797, Final Batch Loss: 0.04969489201903343\n",
      "Epoch 3943, Loss: 0.5355855524539948, Final Batch Loss: 0.2299337238073349\n",
      "Epoch 3944, Loss: 0.40568745136260986, Final Batch Loss: 0.14719738066196442\n",
      "Epoch 3945, Loss: 0.4067962318658829, Final Batch Loss: 0.15886883437633514\n",
      "Epoch 3946, Loss: 0.42362865060567856, Final Batch Loss: 0.18817482888698578\n",
      "Epoch 3947, Loss: 0.38733533024787903, Final Batch Loss: 0.11000189185142517\n",
      "Epoch 3948, Loss: 0.3490132987499237, Final Batch Loss: 0.0729961171746254\n",
      "Epoch 3949, Loss: 0.404301255941391, Final Batch Loss: 0.12028305232524872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3950, Loss: 0.6293066591024399, Final Batch Loss: 0.292459636926651\n",
      "Epoch 3951, Loss: 0.3180445358157158, Final Batch Loss: 0.09029586613178253\n",
      "Epoch 3952, Loss: 0.4259086474776268, Final Batch Loss: 0.17002317309379578\n",
      "Epoch 3953, Loss: 0.2681516706943512, Final Batch Loss: 0.059038929641246796\n",
      "Epoch 3954, Loss: 0.3928453251719475, Final Batch Loss: 0.10113503783941269\n",
      "Epoch 3955, Loss: 0.5069236904382706, Final Batch Loss: 0.1927545964717865\n",
      "Epoch 3956, Loss: 0.37866469472646713, Final Batch Loss: 0.07078947871923447\n",
      "Epoch 3957, Loss: 0.4219179078936577, Final Batch Loss: 0.09980747848749161\n",
      "Epoch 3958, Loss: 0.5250674337148666, Final Batch Loss: 0.32501357793807983\n",
      "Epoch 3959, Loss: 0.33738888055086136, Final Batch Loss: 0.08561871200799942\n",
      "Epoch 3960, Loss: 0.30817458033561707, Final Batch Loss: 0.0838436707854271\n",
      "Epoch 3961, Loss: 0.7008918523788452, Final Batch Loss: 0.4150274693965912\n",
      "Epoch 3962, Loss: 0.25023841857910156, Final Batch Loss: 0.033936791121959686\n",
      "Epoch 3963, Loss: 0.5065453946590424, Final Batch Loss: 0.2056151181459427\n",
      "Epoch 3964, Loss: 0.3273168355226517, Final Batch Loss: 0.07392880320549011\n",
      "Epoch 3965, Loss: 0.35039934515953064, Final Batch Loss: 0.11236730962991714\n",
      "Epoch 3966, Loss: 0.3153107836842537, Final Batch Loss: 0.08442361652851105\n",
      "Epoch 3967, Loss: 0.49171753227710724, Final Batch Loss: 0.22848829627037048\n",
      "Epoch 3968, Loss: 0.35195663571357727, Final Batch Loss: 0.07499907910823822\n",
      "Epoch 3969, Loss: 0.3812548443675041, Final Batch Loss: 0.1510332226753235\n",
      "Epoch 3970, Loss: 0.5672397464513779, Final Batch Loss: 0.24495136737823486\n",
      "Epoch 3971, Loss: 0.4652819186449051, Final Batch Loss: 0.25252190232276917\n",
      "Epoch 3972, Loss: 0.312450110912323, Final Batch Loss: 0.0939323753118515\n",
      "Epoch 3973, Loss: 0.3312251791357994, Final Batch Loss: 0.10288943350315094\n",
      "Epoch 3974, Loss: 0.4162295460700989, Final Batch Loss: 0.14002472162246704\n",
      "Epoch 3975, Loss: 0.28850007802248, Final Batch Loss: 0.07066988945007324\n",
      "Epoch 3976, Loss: 0.3366286978125572, Final Batch Loss: 0.10679439455270767\n",
      "Epoch 3977, Loss: 0.6067163571715355, Final Batch Loss: 0.36711183190345764\n",
      "Epoch 3978, Loss: 0.4062860831618309, Final Batch Loss: 0.1360994577407837\n",
      "Epoch 3979, Loss: 0.3063800185918808, Final Batch Loss: 0.09791450947523117\n",
      "Epoch 3980, Loss: 0.5164460837841034, Final Batch Loss: 0.26368844509124756\n",
      "Epoch 3981, Loss: 0.2789762392640114, Final Batch Loss: 0.07768917083740234\n",
      "Epoch 3982, Loss: 0.35315532982349396, Final Batch Loss: 0.10567940771579742\n",
      "Epoch 3983, Loss: 0.42986997961997986, Final Batch Loss: 0.13671790063381195\n",
      "Epoch 3984, Loss: 0.3482762798666954, Final Batch Loss: 0.08459661155939102\n",
      "Epoch 3985, Loss: 0.37825755774974823, Final Batch Loss: 0.11959372460842133\n",
      "Epoch 3986, Loss: 0.37008123844861984, Final Batch Loss: 0.13093750178813934\n",
      "Epoch 3987, Loss: 0.31360242515802383, Final Batch Loss: 0.07060815393924713\n",
      "Epoch 3988, Loss: 0.2787346839904785, Final Batch Loss: 0.07955941557884216\n",
      "Epoch 3989, Loss: 0.30576563999056816, Final Batch Loss: 0.06024755910038948\n",
      "Epoch 3990, Loss: 0.36968231946229935, Final Batch Loss: 0.11911723762750626\n",
      "Epoch 3991, Loss: 0.5439833998680115, Final Batch Loss: 0.26547300815582275\n",
      "Epoch 3992, Loss: 0.3957912400364876, Final Batch Loss: 0.15432660281658173\n",
      "Epoch 3993, Loss: 0.27748746424913406, Final Batch Loss: 0.05569104105234146\n",
      "Epoch 3994, Loss: 0.33445583283901215, Final Batch Loss: 0.12201472371816635\n",
      "Epoch 3995, Loss: 0.3308773636817932, Final Batch Loss: 0.08774951100349426\n",
      "Epoch 3996, Loss: 0.36651816219091415, Final Batch Loss: 0.1508248895406723\n",
      "Epoch 3997, Loss: 0.3961276337504387, Final Batch Loss: 0.08740545064210892\n",
      "Epoch 3998, Loss: 0.33886434882879257, Final Batch Loss: 0.10357365012168884\n",
      "Epoch 3999, Loss: 0.35734276473522186, Final Batch Loss: 0.11987264454364777\n",
      "Epoch 4000, Loss: 0.3526512160897255, Final Batch Loss: 0.09881166368722916\n",
      "Epoch 4001, Loss: 0.40748831629753113, Final Batch Loss: 0.19195109605789185\n",
      "Epoch 4002, Loss: 0.37522731721401215, Final Batch Loss: 0.17473791539669037\n",
      "Epoch 4003, Loss: 0.42425304651260376, Final Batch Loss: 0.18850788474082947\n",
      "Epoch 4004, Loss: 0.4096340164542198, Final Batch Loss: 0.17129361629486084\n",
      "Epoch 4005, Loss: 0.2581651583313942, Final Batch Loss: 0.07899018377065659\n",
      "Epoch 4006, Loss: 0.2862095385789871, Final Batch Loss: 0.0629987046122551\n",
      "Epoch 4007, Loss: 0.3970179632306099, Final Batch Loss: 0.19497163593769073\n",
      "Epoch 4008, Loss: 0.36105745285749435, Final Batch Loss: 0.10520157217979431\n",
      "Epoch 4009, Loss: 0.3913399204611778, Final Batch Loss: 0.16045275330543518\n",
      "Epoch 4010, Loss: 0.3130742684006691, Final Batch Loss: 0.07627687603235245\n",
      "Epoch 4011, Loss: 0.2838267832994461, Final Batch Loss: 0.10267136245965958\n",
      "Epoch 4012, Loss: 0.31315839290618896, Final Batch Loss: 0.12041076272726059\n",
      "Epoch 4013, Loss: 0.3835849389433861, Final Batch Loss: 0.1908617466688156\n",
      "Epoch 4014, Loss: 0.33435437828302383, Final Batch Loss: 0.03406410664319992\n",
      "Epoch 4015, Loss: 0.4274545833468437, Final Batch Loss: 0.19053155183792114\n",
      "Epoch 4016, Loss: 0.3038715347647667, Final Batch Loss: 0.08727334439754486\n",
      "Epoch 4017, Loss: 0.4160308986902237, Final Batch Loss: 0.11669708788394928\n",
      "Epoch 4018, Loss: 0.41990184783935547, Final Batch Loss: 0.14189833402633667\n",
      "Epoch 4019, Loss: 0.32147038727998734, Final Batch Loss: 0.10196325927972794\n",
      "Epoch 4020, Loss: 0.31942318379879, Final Batch Loss: 0.0649360865354538\n",
      "Epoch 4021, Loss: 0.3644276335835457, Final Batch Loss: 0.10919786989688873\n",
      "Epoch 4022, Loss: 0.3149254694581032, Final Batch Loss: 0.08671489357948303\n",
      "Epoch 4023, Loss: 0.27548298239707947, Final Batch Loss: 0.06362750381231308\n",
      "Epoch 4024, Loss: 0.3116084560751915, Final Batch Loss: 0.15117032825946808\n",
      "Epoch 4025, Loss: 0.2971590980887413, Final Batch Loss: 0.056273140013217926\n",
      "Epoch 4026, Loss: 0.37644238770008087, Final Batch Loss: 0.12496013194322586\n",
      "Epoch 4027, Loss: 0.34496406838297844, Final Batch Loss: 0.038120608776807785\n",
      "Epoch 4028, Loss: 0.3283819332718849, Final Batch Loss: 0.09750494360923767\n",
      "Epoch 4029, Loss: 0.34683703631162643, Final Batch Loss: 0.11568283289670944\n",
      "Epoch 4030, Loss: 0.43584106862545013, Final Batch Loss: 0.14883124828338623\n",
      "Epoch 4031, Loss: 0.4066051170229912, Final Batch Loss: 0.12656572461128235\n",
      "Epoch 4032, Loss: 0.3282212093472481, Final Batch Loss: 0.15682241320610046\n",
      "Epoch 4033, Loss: 0.2699686400592327, Final Batch Loss: 0.061428848654031754\n",
      "Epoch 4034, Loss: 0.2734357491135597, Final Batch Loss: 0.06518654525279999\n",
      "Epoch 4035, Loss: 0.4856392964720726, Final Batch Loss: 0.29894739389419556\n",
      "Epoch 4036, Loss: 0.3441689684987068, Final Batch Loss: 0.10409320890903473\n",
      "Epoch 4037, Loss: 0.3578796312212944, Final Batch Loss: 0.1714937388896942\n",
      "Epoch 4038, Loss: 0.27324843406677246, Final Batch Loss: 0.06539499014616013\n",
      "Epoch 4039, Loss: 0.3923461064696312, Final Batch Loss: 0.18103906512260437\n",
      "Epoch 4040, Loss: 0.3732133135199547, Final Batch Loss: 0.10711316019296646\n",
      "Epoch 4041, Loss: 0.3314853832125664, Final Batch Loss: 0.13428302109241486\n",
      "Epoch 4042, Loss: 0.33900265395641327, Final Batch Loss: 0.14198441803455353\n",
      "Epoch 4043, Loss: 0.3163052499294281, Final Batch Loss: 0.05567539483308792\n",
      "Epoch 4044, Loss: 0.40611008554697037, Final Batch Loss: 0.18438084423542023\n",
      "Epoch 4045, Loss: 0.4328765720129013, Final Batch Loss: 0.1829303503036499\n",
      "Epoch 4046, Loss: 0.45226018875837326, Final Batch Loss: 0.22278958559036255\n",
      "Epoch 4047, Loss: 0.27414829656481743, Final Batch Loss: 0.053779032081365585\n",
      "Epoch 4048, Loss: 0.2670172229409218, Final Batch Loss: 0.053254082798957825\n",
      "Epoch 4049, Loss: 0.4011295884847641, Final Batch Loss: 0.18850639462471008\n",
      "Epoch 4050, Loss: 0.36673127859830856, Final Batch Loss: 0.10973367840051651\n",
      "Epoch 4051, Loss: 0.3180762827396393, Final Batch Loss: 0.09171972423791885\n",
      "Epoch 4052, Loss: 0.330564022064209, Final Batch Loss: 0.1157340258359909\n",
      "Epoch 4053, Loss: 0.3013113960623741, Final Batch Loss: 0.0801129937171936\n",
      "Epoch 4054, Loss: 0.3209197074174881, Final Batch Loss: 0.06722166389226913\n",
      "Epoch 4055, Loss: 0.33951103687286377, Final Batch Loss: 0.12540142238140106\n",
      "Epoch 4056, Loss: 0.3200758993625641, Final Batch Loss: 0.07756976783275604\n",
      "Epoch 4057, Loss: 0.31364887207746506, Final Batch Loss: 0.13239702582359314\n",
      "Epoch 4058, Loss: 0.3069341108202934, Final Batch Loss: 0.09049755334854126\n",
      "Epoch 4059, Loss: 0.43830908834934235, Final Batch Loss: 0.27348852157592773\n",
      "Epoch 4060, Loss: 0.33945922553539276, Final Batch Loss: 0.12371081858873367\n",
      "Epoch 4061, Loss: 0.266678087413311, Final Batch Loss: 0.06130107492208481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4062, Loss: 0.3708689957857132, Final Batch Loss: 0.10447919368743896\n",
      "Epoch 4063, Loss: 0.3456334136426449, Final Batch Loss: 0.037702690809965134\n",
      "Epoch 4064, Loss: 0.442731037735939, Final Batch Loss: 0.1986066699028015\n",
      "Epoch 4065, Loss: 0.38280145078897476, Final Batch Loss: 0.10426098853349686\n",
      "Epoch 4066, Loss: 0.4526055157184601, Final Batch Loss: 0.2620723843574524\n",
      "Epoch 4067, Loss: 0.4244382455945015, Final Batch Loss: 0.24472613632678986\n",
      "Epoch 4068, Loss: 0.3923656940460205, Final Batch Loss: 0.11273244023323059\n",
      "Epoch 4069, Loss: 0.38341229408979416, Final Batch Loss: 0.14796346426010132\n",
      "Epoch 4070, Loss: 0.5202343016862869, Final Batch Loss: 0.276689738035202\n",
      "Epoch 4071, Loss: 0.30796289443969727, Final Batch Loss: 0.08955628424882889\n",
      "Epoch 4072, Loss: 0.35890672355890274, Final Batch Loss: 0.15464192628860474\n",
      "Epoch 4073, Loss: 0.3061201497912407, Final Batch Loss: 0.11027920991182327\n",
      "Epoch 4074, Loss: 0.3460985794663429, Final Batch Loss: 0.08785366266965866\n",
      "Epoch 4075, Loss: 0.31652478128671646, Final Batch Loss: 0.11503371596336365\n",
      "Epoch 4076, Loss: 0.4528811201453209, Final Batch Loss: 0.19026628136634827\n",
      "Epoch 4077, Loss: 0.42591361701488495, Final Batch Loss: 0.1437884271144867\n",
      "Epoch 4078, Loss: 0.4385107532143593, Final Batch Loss: 0.1729760617017746\n",
      "Epoch 4079, Loss: 0.29726047068834305, Final Batch Loss: 0.06538119167089462\n",
      "Epoch 4080, Loss: 0.3314417079091072, Final Batch Loss: 0.0756024494767189\n",
      "Epoch 4081, Loss: 0.35681167989969254, Final Batch Loss: 0.10182122886180878\n",
      "Epoch 4082, Loss: 0.321344755589962, Final Batch Loss: 0.08746036142110825\n",
      "Epoch 4083, Loss: 0.42306695878505707, Final Batch Loss: 0.2119484394788742\n",
      "Epoch 4084, Loss: 0.6295328885316849, Final Batch Loss: 0.3028276562690735\n",
      "Epoch 4085, Loss: 0.31409135088324547, Final Batch Loss: 0.038677092641592026\n",
      "Epoch 4086, Loss: 0.35553813725709915, Final Batch Loss: 0.131620854139328\n",
      "Epoch 4087, Loss: 0.31318681687116623, Final Batch Loss: 0.10901103913784027\n",
      "Epoch 4088, Loss: 0.32620997726917267, Final Batch Loss: 0.11172430962324142\n",
      "Epoch 4089, Loss: 0.31179603189229965, Final Batch Loss: 0.0803118422627449\n",
      "Epoch 4090, Loss: 0.5236934348940849, Final Batch Loss: 0.26015323400497437\n",
      "Epoch 4091, Loss: 0.3166392147541046, Final Batch Loss: 0.05901426076889038\n",
      "Epoch 4092, Loss: 0.3139664940536022, Final Batch Loss: 0.04897056147456169\n",
      "Epoch 4093, Loss: 0.28546782210469246, Final Batch Loss: 0.05577424541115761\n",
      "Epoch 4094, Loss: 0.41225336492061615, Final Batch Loss: 0.13320288062095642\n",
      "Epoch 4095, Loss: 0.4687919467687607, Final Batch Loss: 0.19849944114685059\n",
      "Epoch 4096, Loss: 0.34822189062833786, Final Batch Loss: 0.10830974578857422\n",
      "Epoch 4097, Loss: 0.3472446911036968, Final Batch Loss: 0.056701283901929855\n",
      "Epoch 4098, Loss: 0.2852833718061447, Final Batch Loss: 0.07362035661935806\n",
      "Epoch 4099, Loss: 0.26808101683855057, Final Batch Loss: 0.07958311587572098\n",
      "Epoch 4100, Loss: 0.2705431953072548, Final Batch Loss: 0.06517361104488373\n",
      "Epoch 4101, Loss: 0.32047072798013687, Final Batch Loss: 0.10564784705638885\n",
      "Epoch 4102, Loss: 0.30813123285770416, Final Batch Loss: 0.07263840734958649\n",
      "Epoch 4103, Loss: 0.30019891262054443, Final Batch Loss: 0.09776535630226135\n",
      "Epoch 4104, Loss: 0.3730193227529526, Final Batch Loss: 0.1127413660287857\n",
      "Epoch 4105, Loss: 0.3092194236814976, Final Batch Loss: 0.05835270509123802\n",
      "Epoch 4106, Loss: 0.32707326859235764, Final Batch Loss: 0.061354003846645355\n",
      "Epoch 4107, Loss: 0.25294283777475357, Final Batch Loss: 0.0331287682056427\n",
      "Epoch 4108, Loss: 0.2732554003596306, Final Batch Loss: 0.05142691731452942\n",
      "Epoch 4109, Loss: 0.5160236060619354, Final Batch Loss: 0.19304396212100983\n",
      "Epoch 4110, Loss: 0.3013371229171753, Final Batch Loss: 0.08179160207509995\n",
      "Epoch 4111, Loss: 0.254720501601696, Final Batch Loss: 0.039621636271476746\n",
      "Epoch 4112, Loss: 0.38696059584617615, Final Batch Loss: 0.15252746641635895\n",
      "Epoch 4113, Loss: 0.3324102312326431, Final Batch Loss: 0.09827551245689392\n",
      "Epoch 4114, Loss: 0.36226650327444077, Final Batch Loss: 0.15250477194786072\n",
      "Epoch 4115, Loss: 0.5068846642971039, Final Batch Loss: 0.15633662045001984\n",
      "Epoch 4116, Loss: 0.3191758021712303, Final Batch Loss: 0.07897210866212845\n",
      "Epoch 4117, Loss: 0.4039732366800308, Final Batch Loss: 0.10681673884391785\n",
      "Epoch 4118, Loss: 0.455339252948761, Final Batch Loss: 0.2673739492893219\n",
      "Epoch 4119, Loss: 0.24266892671585083, Final Batch Loss: 0.030022889375686646\n",
      "Epoch 4120, Loss: 0.3451490178704262, Final Batch Loss: 0.1332433968782425\n",
      "Epoch 4121, Loss: 0.28578297048807144, Final Batch Loss: 0.06603419035673141\n",
      "Epoch 4122, Loss: 0.3229770138859749, Final Batch Loss: 0.07130603492259979\n",
      "Epoch 4123, Loss: 0.33394449949264526, Final Batch Loss: 0.12091337144374847\n",
      "Epoch 4124, Loss: 0.34553423523902893, Final Batch Loss: 0.1209554672241211\n",
      "Epoch 4125, Loss: 0.27246759459376335, Final Batch Loss: 0.05126297101378441\n",
      "Epoch 4126, Loss: 0.25017235428094864, Final Batch Loss: 0.06209667772054672\n",
      "Epoch 4127, Loss: 0.3165351003408432, Final Batch Loss: 0.11593952029943466\n",
      "Epoch 4128, Loss: 0.2906113564968109, Final Batch Loss: 0.08503267914056778\n",
      "Epoch 4129, Loss: 0.4170152246952057, Final Batch Loss: 0.14911749958992004\n",
      "Epoch 4130, Loss: 0.3239761143922806, Final Batch Loss: 0.10394470393657684\n",
      "Epoch 4131, Loss: 0.3927155137062073, Final Batch Loss: 0.11816133558750153\n",
      "Epoch 4132, Loss: 0.44596293568611145, Final Batch Loss: 0.13550572097301483\n",
      "Epoch 4133, Loss: 0.3940076529979706, Final Batch Loss: 0.14818212389945984\n",
      "Epoch 4134, Loss: 0.27703477069735527, Final Batch Loss: 0.048728253692388535\n",
      "Epoch 4135, Loss: 0.4161812737584114, Final Batch Loss: 0.20315547287464142\n",
      "Epoch 4136, Loss: 0.4981128126382828, Final Batch Loss: 0.20645305514335632\n",
      "Epoch 4137, Loss: 0.3063005805015564, Final Batch Loss: 0.07768478989601135\n",
      "Epoch 4138, Loss: 0.27552981674671173, Final Batch Loss: 0.03176160901784897\n",
      "Epoch 4139, Loss: 0.3444204181432724, Final Batch Loss: 0.07609347254037857\n",
      "Epoch 4140, Loss: 0.3106280118227005, Final Batch Loss: 0.08341599255800247\n",
      "Epoch 4141, Loss: 0.2753807380795479, Final Batch Loss: 0.04180329293012619\n",
      "Epoch 4142, Loss: 0.27513882517814636, Final Batch Loss: 0.08178608119487762\n",
      "Epoch 4143, Loss: 0.3509763553738594, Final Batch Loss: 0.04166441410779953\n",
      "Epoch 4144, Loss: 0.3627502843737602, Final Batch Loss: 0.12345296144485474\n",
      "Epoch 4145, Loss: 0.250029344111681, Final Batch Loss: 0.039616797119379044\n",
      "Epoch 4146, Loss: 0.2982638478279114, Final Batch Loss: 0.10289410501718521\n",
      "Epoch 4147, Loss: 0.3031037896871567, Final Batch Loss: 0.09959638118743896\n",
      "Epoch 4148, Loss: 0.3727886229753494, Final Batch Loss: 0.14911732077598572\n",
      "Epoch 4149, Loss: 0.32458122074604034, Final Batch Loss: 0.09244313836097717\n",
      "Epoch 4150, Loss: 0.40790143609046936, Final Batch Loss: 0.12214813381433487\n",
      "Epoch 4151, Loss: 0.37305526435375214, Final Batch Loss: 0.12926387786865234\n",
      "Epoch 4152, Loss: 0.3172121122479439, Final Batch Loss: 0.08751770853996277\n",
      "Epoch 4153, Loss: 0.3180147334933281, Final Batch Loss: 0.15659037232398987\n",
      "Epoch 4154, Loss: 0.5176340192556381, Final Batch Loss: 0.09812316298484802\n",
      "Epoch 4155, Loss: 0.3311980813741684, Final Batch Loss: 0.12805667519569397\n",
      "Epoch 4156, Loss: 0.2891852855682373, Final Batch Loss: 0.0849941223859787\n",
      "Epoch 4157, Loss: 0.294134758412838, Final Batch Loss: 0.0707935020327568\n",
      "Epoch 4158, Loss: 0.4356132000684738, Final Batch Loss: 0.1613459289073944\n",
      "Epoch 4159, Loss: 0.3154953271150589, Final Batch Loss: 0.10984376817941666\n",
      "Epoch 4160, Loss: 0.5450269728899002, Final Batch Loss: 0.3563363254070282\n",
      "Epoch 4161, Loss: 0.3285380229353905, Final Batch Loss: 0.08874718099832535\n",
      "Epoch 4162, Loss: 0.2784815579652786, Final Batch Loss: 0.06521114706993103\n",
      "Epoch 4163, Loss: 0.30736416578292847, Final Batch Loss: 0.04807634651660919\n",
      "Epoch 4164, Loss: 0.34849177300930023, Final Batch Loss: 0.03729560971260071\n",
      "Epoch 4165, Loss: 0.35796723514795303, Final Batch Loss: 0.09684916585683823\n",
      "Epoch 4166, Loss: 0.4278459697961807, Final Batch Loss: 0.18124356865882874\n",
      "Epoch 4167, Loss: 0.34403227269649506, Final Batch Loss: 0.10356751084327698\n",
      "Epoch 4168, Loss: 0.3022849000990391, Final Batch Loss: 0.044575754553079605\n",
      "Epoch 4169, Loss: 0.3945181369781494, Final Batch Loss: 0.09753385931253433\n",
      "Epoch 4170, Loss: 0.4608381539583206, Final Batch Loss: 0.24961304664611816\n",
      "Epoch 4171, Loss: 0.29873715341091156, Final Batch Loss: 0.08143013715744019\n",
      "Epoch 4172, Loss: 0.35428090393543243, Final Batch Loss: 0.10734936594963074\n",
      "Epoch 4173, Loss: 0.3782913312315941, Final Batch Loss: 0.09354893118143082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4174, Loss: 0.3281483128666878, Final Batch Loss: 0.12732821702957153\n",
      "Epoch 4175, Loss: 0.3370954841375351, Final Batch Loss: 0.10153249651193619\n",
      "Epoch 4176, Loss: 0.33270323276519775, Final Batch Loss: 0.08094942569732666\n",
      "Epoch 4177, Loss: 0.2642992436885834, Final Batch Loss: 0.07626155763864517\n",
      "Epoch 4178, Loss: 0.30948711186647415, Final Batch Loss: 0.0847218781709671\n",
      "Epoch 4179, Loss: 0.35152581334114075, Final Batch Loss: 0.08965672552585602\n",
      "Epoch 4180, Loss: 0.34041890501976013, Final Batch Loss: 0.1609334647655487\n",
      "Epoch 4181, Loss: 0.2611866518855095, Final Batch Loss: 0.06644926965236664\n",
      "Epoch 4182, Loss: 0.305181086063385, Final Batch Loss: 0.11437851935625076\n",
      "Epoch 4183, Loss: 0.2897116467356682, Final Batch Loss: 0.06055382639169693\n",
      "Epoch 4184, Loss: 0.2971608266234398, Final Batch Loss: 0.08270242065191269\n",
      "Epoch 4185, Loss: 0.3201054409146309, Final Batch Loss: 0.10667546838521957\n",
      "Epoch 4186, Loss: 0.41683775186538696, Final Batch Loss: 0.21727626025676727\n",
      "Epoch 4187, Loss: 0.27372556179761887, Final Batch Loss: 0.08780384808778763\n",
      "Epoch 4188, Loss: 0.3164263740181923, Final Batch Loss: 0.06428638100624084\n",
      "Epoch 4189, Loss: 0.31809012591838837, Final Batch Loss: 0.07481943070888519\n",
      "Epoch 4190, Loss: 0.3598342165350914, Final Batch Loss: 0.04854149371385574\n",
      "Epoch 4191, Loss: 0.43527279794216156, Final Batch Loss: 0.1873406171798706\n",
      "Epoch 4192, Loss: 0.374939389526844, Final Batch Loss: 0.11534547805786133\n",
      "Epoch 4193, Loss: 0.31340570747852325, Final Batch Loss: 0.07799245417118073\n",
      "Epoch 4194, Loss: 0.3324563130736351, Final Batch Loss: 0.1132717877626419\n",
      "Epoch 4195, Loss: 0.4050500988960266, Final Batch Loss: 0.15238146483898163\n",
      "Epoch 4196, Loss: 0.35174082964658737, Final Batch Loss: 0.12173599749803543\n",
      "Epoch 4197, Loss: 0.3846382424235344, Final Batch Loss: 0.11847267299890518\n",
      "Epoch 4198, Loss: 0.3004939407110214, Final Batch Loss: 0.1226111426949501\n",
      "Epoch 4199, Loss: 0.28356971219182014, Final Batch Loss: 0.031890880316495895\n",
      "Epoch 4200, Loss: 0.48535623401403427, Final Batch Loss: 0.2300184965133667\n",
      "Epoch 4201, Loss: 0.371441014111042, Final Batch Loss: 0.12202685326337814\n",
      "Epoch 4202, Loss: 0.2928657829761505, Final Batch Loss: 0.09701534360647202\n",
      "Epoch 4203, Loss: 0.5639638379216194, Final Batch Loss: 0.3234564960002899\n",
      "Epoch 4204, Loss: 0.26854321360588074, Final Batch Loss: 0.07004823535680771\n",
      "Epoch 4205, Loss: 0.32599956542253494, Final Batch Loss: 0.07907078415155411\n",
      "Epoch 4206, Loss: 0.3068305477499962, Final Batch Loss: 0.07346256822347641\n",
      "Epoch 4207, Loss: 0.2737976685166359, Final Batch Loss: 0.096565380692482\n",
      "Epoch 4208, Loss: 0.34775203466415405, Final Batch Loss: 0.11957349628210068\n",
      "Epoch 4209, Loss: 0.44758564978837967, Final Batch Loss: 0.16931882500648499\n",
      "Epoch 4210, Loss: 0.3208865523338318, Final Batch Loss: 0.07972387969493866\n",
      "Epoch 4211, Loss: 0.3226023390889168, Final Batch Loss: 0.10055848211050034\n",
      "Epoch 4212, Loss: 0.33191293478012085, Final Batch Loss: 0.07457216084003448\n",
      "Epoch 4213, Loss: 0.28973039239645004, Final Batch Loss: 0.05349153280258179\n",
      "Epoch 4214, Loss: 0.33014576882123947, Final Batch Loss: 0.10219113528728485\n",
      "Epoch 4215, Loss: 0.3423191234469414, Final Batch Loss: 0.09370122849941254\n",
      "Epoch 4216, Loss: 0.34117553383111954, Final Batch Loss: 0.08957882970571518\n",
      "Epoch 4217, Loss: 0.4043409675359726, Final Batch Loss: 0.1750398874282837\n",
      "Epoch 4218, Loss: 0.3438930958509445, Final Batch Loss: 0.07452981173992157\n",
      "Epoch 4219, Loss: 0.37170611321926117, Final Batch Loss: 0.09039078652858734\n",
      "Epoch 4220, Loss: 0.3445203825831413, Final Batch Loss: 0.15570643544197083\n",
      "Epoch 4221, Loss: 0.36510349810123444, Final Batch Loss: 0.14765864610671997\n",
      "Epoch 4222, Loss: 0.41050005704164505, Final Batch Loss: 0.10847336798906326\n",
      "Epoch 4223, Loss: 0.34906331449747086, Final Batch Loss: 0.08077038079500198\n",
      "Epoch 4224, Loss: 0.372938372194767, Final Batch Loss: 0.11956959217786789\n",
      "Epoch 4225, Loss: 0.3350047320127487, Final Batch Loss: 0.09419918805360794\n",
      "Epoch 4226, Loss: 0.30368512868881226, Final Batch Loss: 0.1406957060098648\n",
      "Epoch 4227, Loss: 0.4793903976678848, Final Batch Loss: 0.25414249300956726\n",
      "Epoch 4228, Loss: 0.4733690321445465, Final Batch Loss: 0.25647568702697754\n",
      "Epoch 4229, Loss: 0.24733538553118706, Final Batch Loss: 0.03771242871880531\n",
      "Epoch 4230, Loss: 0.3481522649526596, Final Batch Loss: 0.1393776386976242\n",
      "Epoch 4231, Loss: 0.2701864689588547, Final Batch Loss: 0.04577917605638504\n",
      "Epoch 4232, Loss: 0.24125809594988823, Final Batch Loss: 0.04545900598168373\n",
      "Epoch 4233, Loss: 0.38606172055006027, Final Batch Loss: 0.159958153963089\n",
      "Epoch 4234, Loss: 0.31570635363459587, Final Batch Loss: 0.05445278808474541\n",
      "Epoch 4235, Loss: 0.376642107963562, Final Batch Loss: 0.16611826419830322\n",
      "Epoch 4236, Loss: 0.3944989815354347, Final Batch Loss: 0.1274825632572174\n",
      "Epoch 4237, Loss: 0.4080209955573082, Final Batch Loss: 0.17862820625305176\n",
      "Epoch 4238, Loss: 0.42715055495500565, Final Batch Loss: 0.20030389726161957\n",
      "Epoch 4239, Loss: 0.41231707483530045, Final Batch Loss: 0.19356803596019745\n",
      "Epoch 4240, Loss: 0.29106365144252777, Final Batch Loss: 0.05942174047231674\n",
      "Epoch 4241, Loss: 0.329643189907074, Final Batch Loss: 0.11043169349431992\n",
      "Epoch 4242, Loss: 0.30960770696401596, Final Batch Loss: 0.07766826450824738\n",
      "Epoch 4243, Loss: 0.3538210541009903, Final Batch Loss: 0.13634012639522552\n",
      "Epoch 4244, Loss: 0.3708890378475189, Final Batch Loss: 0.15711238980293274\n",
      "Epoch 4245, Loss: 0.3611776754260063, Final Batch Loss: 0.13182274997234344\n",
      "Epoch 4246, Loss: 0.3045247718691826, Final Batch Loss: 0.0660751610994339\n",
      "Epoch 4247, Loss: 0.3706977069377899, Final Batch Loss: 0.1107342392206192\n",
      "Epoch 4248, Loss: 0.40045517683029175, Final Batch Loss: 0.13348065316677094\n",
      "Epoch 4249, Loss: 0.2824154645204544, Final Batch Loss: 0.07569888979196548\n",
      "Epoch 4250, Loss: 0.5034609884023666, Final Batch Loss: 0.2716813385486603\n",
      "Epoch 4251, Loss: 0.2644040659070015, Final Batch Loss: 0.05458011478185654\n",
      "Epoch 4252, Loss: 0.3994428440928459, Final Batch Loss: 0.09481007605791092\n",
      "Epoch 4253, Loss: 0.34099793434143066, Final Batch Loss: 0.11308044195175171\n",
      "Epoch 4254, Loss: 0.34648650884628296, Final Batch Loss: 0.1499292105436325\n",
      "Epoch 4255, Loss: 0.40868229418992996, Final Batch Loss: 0.10680719465017319\n",
      "Epoch 4256, Loss: 0.4311303645372391, Final Batch Loss: 0.11921699345111847\n",
      "Epoch 4257, Loss: 0.369098924100399, Final Batch Loss: 0.11287961900234222\n",
      "Epoch 4258, Loss: 0.26040052250027657, Final Batch Loss: 0.032287921756505966\n",
      "Epoch 4259, Loss: 0.2745372951030731, Final Batch Loss: 0.07814085483551025\n",
      "Epoch 4260, Loss: 0.31325487792491913, Final Batch Loss: 0.0748419463634491\n",
      "Epoch 4261, Loss: 0.33131106197834015, Final Batch Loss: 0.07413007318973541\n",
      "Epoch 4262, Loss: 0.36479299515485764, Final Batch Loss: 0.16265714168548584\n",
      "Epoch 4263, Loss: 0.33033662289381027, Final Batch Loss: 0.11616849154233932\n",
      "Epoch 4264, Loss: 0.3181064873933792, Final Batch Loss: 0.07162073999643326\n",
      "Epoch 4265, Loss: 0.3449121192097664, Final Batch Loss: 0.09827753156423569\n",
      "Epoch 4266, Loss: 0.4324299618601799, Final Batch Loss: 0.20764851570129395\n",
      "Epoch 4267, Loss: 0.34805358201265335, Final Batch Loss: 0.06951015442609787\n",
      "Epoch 4268, Loss: 0.3738006353378296, Final Batch Loss: 0.13616883754730225\n",
      "Epoch 4269, Loss: 0.39774611592292786, Final Batch Loss: 0.14121229946613312\n",
      "Epoch 4270, Loss: 0.3748018145561218, Final Batch Loss: 0.16130894422531128\n",
      "Epoch 4271, Loss: 0.46512214839458466, Final Batch Loss: 0.10214698314666748\n",
      "Epoch 4272, Loss: 0.3283872827887535, Final Batch Loss: 0.0903734490275383\n",
      "Epoch 4273, Loss: 0.44652536511421204, Final Batch Loss: 0.21699771285057068\n",
      "Epoch 4274, Loss: 0.2629230245947838, Final Batch Loss: 0.06311795860528946\n",
      "Epoch 4275, Loss: 0.4292050376534462, Final Batch Loss: 0.1711498498916626\n",
      "Epoch 4276, Loss: 0.34628960490226746, Final Batch Loss: 0.12265193462371826\n",
      "Epoch 4277, Loss: 0.25507331639528275, Final Batch Loss: 0.06679806113243103\n",
      "Epoch 4278, Loss: 0.3234178274869919, Final Batch Loss: 0.09626897424459457\n",
      "Epoch 4279, Loss: 0.3421352282166481, Final Batch Loss: 0.10702981054782867\n",
      "Epoch 4280, Loss: 0.32088982313871384, Final Batch Loss: 0.12341166287660599\n",
      "Epoch 4281, Loss: 0.3687787726521492, Final Batch Loss: 0.07339002937078476\n",
      "Epoch 4282, Loss: 0.36213914304971695, Final Batch Loss: 0.08740375190973282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4283, Loss: 0.2802271768450737, Final Batch Loss: 0.07599353045225143\n",
      "Epoch 4284, Loss: 0.25416821986436844, Final Batch Loss: 0.06248869374394417\n",
      "Epoch 4285, Loss: 0.3271827846765518, Final Batch Loss: 0.12434510141611099\n",
      "Epoch 4286, Loss: 0.3027550205588341, Final Batch Loss: 0.08725263923406601\n",
      "Epoch 4287, Loss: 0.26800594478845596, Final Batch Loss: 0.039393939077854156\n",
      "Epoch 4288, Loss: 0.302647240459919, Final Batch Loss: 0.1158890351653099\n",
      "Epoch 4289, Loss: 0.3125789240002632, Final Batch Loss: 0.09961443394422531\n",
      "Epoch 4290, Loss: 0.33821727335453033, Final Batch Loss: 0.11994248628616333\n",
      "Epoch 4291, Loss: 0.28334177285432816, Final Batch Loss: 0.08160676062107086\n",
      "Epoch 4292, Loss: 0.3551412522792816, Final Batch Loss: 0.1532752811908722\n",
      "Epoch 4293, Loss: 0.33756549656391144, Final Batch Loss: 0.10917878895998001\n",
      "Epoch 4294, Loss: 0.2888610064983368, Final Batch Loss: 0.06509125232696533\n",
      "Epoch 4295, Loss: 0.3358549028635025, Final Batch Loss: 0.10788466036319733\n",
      "Epoch 4296, Loss: 0.3207215741276741, Final Batch Loss: 0.12095019966363907\n",
      "Epoch 4297, Loss: 0.34827034175395966, Final Batch Loss: 0.08344283699989319\n",
      "Epoch 4298, Loss: 0.30802071839571, Final Batch Loss: 0.06517959386110306\n",
      "Epoch 4299, Loss: 0.28812743350863457, Final Batch Loss: 0.0512542687356472\n",
      "Epoch 4300, Loss: 0.29542893171310425, Final Batch Loss: 0.07971339672803879\n",
      "Epoch 4301, Loss: 0.3838661089539528, Final Batch Loss: 0.14360883831977844\n",
      "Epoch 4302, Loss: 0.31012240052223206, Final Batch Loss: 0.08383727818727493\n",
      "Epoch 4303, Loss: 0.3291952759027481, Final Batch Loss: 0.1259172260761261\n",
      "Epoch 4304, Loss: 0.4451914578676224, Final Batch Loss: 0.24645696580410004\n",
      "Epoch 4305, Loss: 0.35920509696006775, Final Batch Loss: 0.09492068737745285\n",
      "Epoch 4306, Loss: 0.32936549186706543, Final Batch Loss: 0.0816030204296112\n",
      "Epoch 4307, Loss: 0.3084609732031822, Final Batch Loss: 0.10424239188432693\n",
      "Epoch 4308, Loss: 0.2808689773082733, Final Batch Loss: 0.07417231053113937\n",
      "Epoch 4309, Loss: 0.30499061942100525, Final Batch Loss: 0.08742819726467133\n",
      "Epoch 4310, Loss: 0.24915147572755814, Final Batch Loss: 0.06710534542798996\n",
      "Epoch 4311, Loss: 0.23161577433347702, Final Batch Loss: 0.03889019787311554\n",
      "Epoch 4312, Loss: 0.24120507389307022, Final Batch Loss: 0.043007515370845795\n",
      "Epoch 4313, Loss: 0.27260755002498627, Final Batch Loss: 0.05784578621387482\n",
      "Epoch 4314, Loss: 0.35109952837228775, Final Batch Loss: 0.06652142852544785\n",
      "Epoch 4315, Loss: 0.2863290272653103, Final Batch Loss: 0.058941710740327835\n",
      "Epoch 4316, Loss: 0.3509892001748085, Final Batch Loss: 0.14132262766361237\n",
      "Epoch 4317, Loss: 0.28728385269641876, Final Batch Loss: 0.0895097404718399\n",
      "Epoch 4318, Loss: 0.31971094012260437, Final Batch Loss: 0.08240784704685211\n",
      "Epoch 4319, Loss: 0.3111221417784691, Final Batch Loss: 0.07296242564916611\n",
      "Epoch 4320, Loss: 0.26842985302209854, Final Batch Loss: 0.06991653144359589\n",
      "Epoch 4321, Loss: 0.34476615488529205, Final Batch Loss: 0.10994987934827805\n",
      "Epoch 4322, Loss: 0.27515149116516113, Final Batch Loss: 0.08107054233551025\n",
      "Epoch 4323, Loss: 0.3393895775079727, Final Batch Loss: 0.12023533880710602\n",
      "Epoch 4324, Loss: 0.3691222444176674, Final Batch Loss: 0.09307222068309784\n",
      "Epoch 4325, Loss: 0.6242722421884537, Final Batch Loss: 0.24545323848724365\n",
      "Epoch 4326, Loss: 0.34307698905467987, Final Batch Loss: 0.11551018804311752\n",
      "Epoch 4327, Loss: 0.4581447094678879, Final Batch Loss: 0.08070991933345795\n",
      "Epoch 4328, Loss: 0.4861600399017334, Final Batch Loss: 0.2721252739429474\n",
      "Epoch 4329, Loss: 0.37963369488716125, Final Batch Loss: 0.125050887465477\n",
      "Epoch 4330, Loss: 0.3285124823451042, Final Batch Loss: 0.11685257405042648\n",
      "Epoch 4331, Loss: 0.3612736538052559, Final Batch Loss: 0.142197847366333\n",
      "Epoch 4332, Loss: 0.3409019932150841, Final Batch Loss: 0.1021152064204216\n",
      "Epoch 4333, Loss: 0.38550399988889694, Final Batch Loss: 0.18272866308689117\n",
      "Epoch 4334, Loss: 0.2897442951798439, Final Batch Loss: 0.053778357803821564\n",
      "Epoch 4335, Loss: 0.2573380172252655, Final Batch Loss: 0.05447117239236832\n",
      "Epoch 4336, Loss: 0.29750250466167927, Final Batch Loss: 0.016182737424969673\n",
      "Epoch 4337, Loss: 0.7558202296495438, Final Batch Loss: 0.45137542486190796\n",
      "Epoch 4338, Loss: 0.34330884367227554, Final Batch Loss: 0.09447786957025528\n",
      "Epoch 4339, Loss: 0.3216707333922386, Final Batch Loss: 0.08779650926589966\n",
      "Epoch 4340, Loss: 0.4095890298485756, Final Batch Loss: 0.1799984574317932\n",
      "Epoch 4341, Loss: 0.27983638644218445, Final Batch Loss: 0.052794888615608215\n",
      "Epoch 4342, Loss: 0.28615808114409447, Final Batch Loss: 0.027688708156347275\n",
      "Epoch 4343, Loss: 0.44813838601112366, Final Batch Loss: 0.2689455449581146\n",
      "Epoch 4344, Loss: 0.32152150571346283, Final Batch Loss: 0.10861126333475113\n",
      "Epoch 4345, Loss: 0.3207811638712883, Final Batch Loss: 0.052061572670936584\n",
      "Epoch 4346, Loss: 0.5709589272737503, Final Batch Loss: 0.30218517780303955\n",
      "Epoch 4347, Loss: 0.2817564979195595, Final Batch Loss: 0.07583577930927277\n",
      "Epoch 4348, Loss: 0.32370974123477936, Final Batch Loss: 0.04325904697179794\n",
      "Epoch 4349, Loss: 0.2846764624118805, Final Batch Loss: 0.07151873409748077\n",
      "Epoch 4350, Loss: 0.3218817189335823, Final Batch Loss: 0.0835651382803917\n",
      "Epoch 4351, Loss: 0.3487478494644165, Final Batch Loss: 0.08858830481767654\n",
      "Epoch 4352, Loss: 0.36763910204172134, Final Batch Loss: 0.14263711869716644\n",
      "Epoch 4353, Loss: 0.4098590090870857, Final Batch Loss: 0.16477401554584503\n",
      "Epoch 4354, Loss: 0.43382081389427185, Final Batch Loss: 0.16682782769203186\n",
      "Epoch 4355, Loss: 0.2730976343154907, Final Batch Loss: 0.07542882859706879\n",
      "Epoch 4356, Loss: 0.3498176112771034, Final Batch Loss: 0.08062029629945755\n",
      "Epoch 4357, Loss: 0.3668249323964119, Final Batch Loss: 0.08966157585382462\n",
      "Epoch 4358, Loss: 0.33804016560316086, Final Batch Loss: 0.08097129315137863\n",
      "Epoch 4359, Loss: 0.37091030180454254, Final Batch Loss: 0.1406199187040329\n",
      "Epoch 4360, Loss: 0.3283676579594612, Final Batch Loss: 0.12004014104604721\n",
      "Epoch 4361, Loss: 0.28112753480672836, Final Batch Loss: 0.06837677210569382\n",
      "Epoch 4362, Loss: 0.4069541469216347, Final Batch Loss: 0.08496645838022232\n",
      "Epoch 4363, Loss: 0.5948750823736191, Final Batch Loss: 0.35784631967544556\n",
      "Epoch 4364, Loss: 0.24862143397331238, Final Batch Loss: 0.07118558138608932\n",
      "Epoch 4365, Loss: 0.3315720707178116, Final Batch Loss: 0.13994835317134857\n",
      "Epoch 4366, Loss: 0.3682802803814411, Final Batch Loss: 0.06016959622502327\n",
      "Epoch 4367, Loss: 0.24419638141989708, Final Batch Loss: 0.033944543451070786\n",
      "Epoch 4368, Loss: 0.2586437799036503, Final Batch Loss: 0.04291852191090584\n",
      "Epoch 4369, Loss: 0.5681002512574196, Final Batch Loss: 0.31669536232948303\n",
      "Epoch 4370, Loss: 0.40615569055080414, Final Batch Loss: 0.13956263661384583\n",
      "Epoch 4371, Loss: 0.3679293245077133, Final Batch Loss: 0.12636719644069672\n",
      "Epoch 4372, Loss: 0.40529242530465126, Final Batch Loss: 0.03434200957417488\n",
      "Epoch 4373, Loss: 0.2652885913848877, Final Batch Loss: 0.08050592243671417\n",
      "Epoch 4374, Loss: 0.3637193739414215, Final Batch Loss: 0.07870899140834808\n",
      "Epoch 4375, Loss: 0.38408399373292923, Final Batch Loss: 0.15107950568199158\n",
      "Epoch 4376, Loss: 0.27594243735074997, Final Batch Loss: 0.05006451904773712\n",
      "Epoch 4377, Loss: 0.34141160547733307, Final Batch Loss: 0.0754445344209671\n",
      "Epoch 4378, Loss: 0.37382349371910095, Final Batch Loss: 0.19822941720485687\n",
      "Epoch 4379, Loss: 0.31921225786209106, Final Batch Loss: 0.09235747158527374\n",
      "Epoch 4380, Loss: 0.3351060748100281, Final Batch Loss: 0.08685719221830368\n",
      "Epoch 4381, Loss: 0.44614481925964355, Final Batch Loss: 0.21702226996421814\n",
      "Epoch 4382, Loss: 0.34500541538000107, Final Batch Loss: 0.14114978909492493\n",
      "Epoch 4383, Loss: 0.2876638025045395, Final Batch Loss: 0.08858339488506317\n",
      "Epoch 4384, Loss: 0.32924454659223557, Final Batch Loss: 0.04318143427371979\n",
      "Epoch 4385, Loss: 0.23909832164645195, Final Batch Loss: 0.047081682831048965\n",
      "Epoch 4386, Loss: 0.2731512486934662, Final Batch Loss: 0.04657978564500809\n",
      "Epoch 4387, Loss: 0.3012108951807022, Final Batch Loss: 0.09596114605665207\n",
      "Epoch 4388, Loss: 0.34563467651605606, Final Batch Loss: 0.0645611509680748\n",
      "Epoch 4389, Loss: 0.3444930911064148, Final Batch Loss: 0.06550221890211105\n",
      "Epoch 4390, Loss: 0.38069696351885796, Final Batch Loss: 0.11920343339443207\n",
      "Epoch 4391, Loss: 0.2837566062808037, Final Batch Loss: 0.08158484101295471\n",
      "Epoch 4392, Loss: 0.3332392945885658, Final Batch Loss: 0.14451003074645996\n",
      "Epoch 4393, Loss: 0.2816162705421448, Final Batch Loss: 0.07319293916225433\n",
      "Epoch 4394, Loss: 0.3029935359954834, Final Batch Loss: 0.1159583330154419\n",
      "Epoch 4395, Loss: 0.31962329149246216, Final Batch Loss: 0.07147346436977386\n",
      "Epoch 4396, Loss: 0.28273550048470497, Final Batch Loss: 0.03711969777941704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4397, Loss: 0.30489180982112885, Final Batch Loss: 0.09582510590553284\n",
      "Epoch 4398, Loss: 0.3129228875041008, Final Batch Loss: 0.08108285814523697\n",
      "Epoch 4399, Loss: 0.33537548780441284, Final Batch Loss: 0.09206259250640869\n",
      "Epoch 4400, Loss: 0.8798482939600945, Final Batch Loss: 0.6711209416389465\n",
      "Epoch 4401, Loss: 0.30746521055698395, Final Batch Loss: 0.10296693444252014\n",
      "Epoch 4402, Loss: 0.3078920915722847, Final Batch Loss: 0.15255892276763916\n",
      "Epoch 4403, Loss: 0.3227431923151016, Final Batch Loss: 0.15005283057689667\n",
      "Epoch 4404, Loss: 0.3125612884759903, Final Batch Loss: 0.07353640347719193\n",
      "Epoch 4405, Loss: 0.2694646269083023, Final Batch Loss: 0.0763152465224266\n",
      "Epoch 4406, Loss: 0.2906102240085602, Final Batch Loss: 0.05840405076742172\n",
      "Epoch 4407, Loss: 0.3558631092309952, Final Batch Loss: 0.08552611619234085\n",
      "Epoch 4408, Loss: 0.42401162534952164, Final Batch Loss: 0.18051746487617493\n",
      "Epoch 4409, Loss: 0.3415466547012329, Final Batch Loss: 0.11257855594158173\n",
      "Epoch 4410, Loss: 0.2332119233906269, Final Batch Loss: 0.03260096535086632\n",
      "Epoch 4411, Loss: 0.3514259085059166, Final Batch Loss: 0.07847022265195847\n",
      "Epoch 4412, Loss: 0.3274800218641758, Final Batch Loss: 0.06074276193976402\n",
      "Epoch 4413, Loss: 0.3260328397154808, Final Batch Loss: 0.07320211082696915\n",
      "Epoch 4414, Loss: 0.3446955978870392, Final Batch Loss: 0.12090196460485458\n",
      "Epoch 4415, Loss: 0.44664672762155533, Final Batch Loss: 0.21024136245250702\n",
      "Epoch 4416, Loss: 0.35903624445199966, Final Batch Loss: 0.17604053020477295\n",
      "Epoch 4417, Loss: 0.29888297617435455, Final Batch Loss: 0.11594517529010773\n",
      "Epoch 4418, Loss: 0.4079599156975746, Final Batch Loss: 0.1746029108762741\n",
      "Epoch 4419, Loss: 0.28058939427137375, Final Batch Loss: 0.10566914081573486\n",
      "Epoch 4420, Loss: 0.339475579559803, Final Batch Loss: 0.0972406268119812\n",
      "Epoch 4421, Loss: 0.2645769268274307, Final Batch Loss: 0.04998195916414261\n",
      "Epoch 4422, Loss: 0.3796349763870239, Final Batch Loss: 0.19886921346187592\n",
      "Epoch 4423, Loss: 0.2780901566147804, Final Batch Loss: 0.033096522092819214\n",
      "Epoch 4424, Loss: 0.2964100018143654, Final Batch Loss: 0.11241588741540909\n",
      "Epoch 4425, Loss: 0.3704633042216301, Final Batch Loss: 0.14748549461364746\n",
      "Epoch 4426, Loss: 0.29806554317474365, Final Batch Loss: 0.0780426636338234\n",
      "Epoch 4427, Loss: 0.29155999422073364, Final Batch Loss: 0.0660829171538353\n",
      "Epoch 4428, Loss: 0.3328201174736023, Final Batch Loss: 0.1101195216178894\n",
      "Epoch 4429, Loss: 0.2516910694539547, Final Batch Loss: 0.04487084224820137\n",
      "Epoch 4430, Loss: 0.3466084897518158, Final Batch Loss: 0.06240256130695343\n",
      "Epoch 4431, Loss: 0.27785978093743324, Final Batch Loss: 0.055636171251535416\n",
      "Epoch 4432, Loss: 0.3242676630616188, Final Batch Loss: 0.11915355920791626\n",
      "Epoch 4433, Loss: 0.29424379765987396, Final Batch Loss: 0.12031634151935577\n",
      "Epoch 4434, Loss: 0.2644360065460205, Final Batch Loss: 0.08927669376134872\n",
      "Epoch 4435, Loss: 0.2751891203224659, Final Batch Loss: 0.03768825903534889\n",
      "Epoch 4436, Loss: 0.35572513937950134, Final Batch Loss: 0.13482804596424103\n",
      "Epoch 4437, Loss: 0.3356553167104721, Final Batch Loss: 0.09674189239740372\n",
      "Epoch 4438, Loss: 0.2844204492866993, Final Batch Loss: 0.03176507726311684\n",
      "Epoch 4439, Loss: 0.2767203263938427, Final Batch Loss: 0.05626785382628441\n",
      "Epoch 4440, Loss: 0.4694862961769104, Final Batch Loss: 0.1853548288345337\n",
      "Epoch 4441, Loss: 0.386224951595068, Final Batch Loss: 0.21060314774513245\n",
      "Epoch 4442, Loss: 0.3865964934229851, Final Batch Loss: 0.13949625194072723\n",
      "Epoch 4443, Loss: 0.367216520011425, Final Batch Loss: 0.07298903912305832\n",
      "Epoch 4444, Loss: 0.2860611826181412, Final Batch Loss: 0.0913180410861969\n",
      "Epoch 4445, Loss: 0.312051922082901, Final Batch Loss: 0.09773454070091248\n",
      "Epoch 4446, Loss: 0.33874785155057907, Final Batch Loss: 0.11541339010000229\n",
      "Epoch 4447, Loss: 0.4489733576774597, Final Batch Loss: 0.17832238972187042\n",
      "Epoch 4448, Loss: 0.27392589300870895, Final Batch Loss: 0.09369897842407227\n",
      "Epoch 4449, Loss: 0.33081118762493134, Final Batch Loss: 0.14078965783119202\n",
      "Epoch 4450, Loss: 0.2791659403592348, Final Batch Loss: 0.02678767777979374\n",
      "Epoch 4451, Loss: 0.22768359258770943, Final Batch Loss: 0.05320112034678459\n",
      "Epoch 4452, Loss: 0.34345516562461853, Final Batch Loss: 0.06955511122941971\n",
      "Epoch 4453, Loss: 0.23125098086893559, Final Batch Loss: 0.024074235931038857\n",
      "Epoch 4454, Loss: 0.3377283811569214, Final Batch Loss: 0.11309491097927094\n",
      "Epoch 4455, Loss: 0.2926594763994217, Final Batch Loss: 0.10973205417394638\n",
      "Epoch 4456, Loss: 0.4553048238158226, Final Batch Loss: 0.2645350396633148\n",
      "Epoch 4457, Loss: 0.2719172537326813, Final Batch Loss: 0.089475117623806\n",
      "Epoch 4458, Loss: 0.31662706285715103, Final Batch Loss: 0.15210066735744476\n",
      "Epoch 4459, Loss: 0.34385617077350616, Final Batch Loss: 0.13666567206382751\n",
      "Epoch 4460, Loss: 0.49000634253025055, Final Batch Loss: 0.1938486546278\n",
      "Epoch 4461, Loss: 0.3245438113808632, Final Batch Loss: 0.11729883402585983\n",
      "Epoch 4462, Loss: 0.31974466890096664, Final Batch Loss: 0.07091213762760162\n",
      "Epoch 4463, Loss: 0.3343610167503357, Final Batch Loss: 0.0732387974858284\n",
      "Epoch 4464, Loss: 0.2991136610507965, Final Batch Loss: 0.0894387811422348\n",
      "Epoch 4465, Loss: 0.329605583101511, Final Batch Loss: 0.028522584587335587\n",
      "Epoch 4466, Loss: 0.33992890268564224, Final Batch Loss: 0.07037969678640366\n",
      "Epoch 4467, Loss: 0.3677920252084732, Final Batch Loss: 0.08624527603387833\n",
      "Epoch 4468, Loss: 0.3542601466178894, Final Batch Loss: 0.11682212352752686\n",
      "Epoch 4469, Loss: 0.41951796412467957, Final Batch Loss: 0.1784430742263794\n",
      "Epoch 4470, Loss: 0.41621149331331253, Final Batch Loss: 0.1296495646238327\n",
      "Epoch 4471, Loss: 0.29974766820669174, Final Batch Loss: 0.09426189213991165\n",
      "Epoch 4472, Loss: 0.37319207191467285, Final Batch Loss: 0.17216739058494568\n",
      "Epoch 4473, Loss: 0.4576738327741623, Final Batch Loss: 0.282539039850235\n",
      "Epoch 4474, Loss: 0.3323194608092308, Final Batch Loss: 0.07117210328578949\n",
      "Epoch 4475, Loss: 0.33579352498054504, Final Batch Loss: 0.080352783203125\n",
      "Epoch 4476, Loss: 0.44451211392879486, Final Batch Loss: 0.1888735443353653\n",
      "Epoch 4477, Loss: 0.32224109023809433, Final Batch Loss: 0.06845948845148087\n",
      "Epoch 4478, Loss: 0.3323632925748825, Final Batch Loss: 0.05370564013719559\n",
      "Epoch 4479, Loss: 0.4965464696288109, Final Batch Loss: 0.24449454247951508\n",
      "Epoch 4480, Loss: 0.40574032068252563, Final Batch Loss: 0.12998466193675995\n",
      "Epoch 4481, Loss: 0.43096277862787247, Final Batch Loss: 0.11240186542272568\n",
      "Epoch 4482, Loss: 0.30225592851638794, Final Batch Loss: 0.06981439143419266\n",
      "Epoch 4483, Loss: 0.3147216886281967, Final Batch Loss: 0.11564172804355621\n",
      "Epoch 4484, Loss: 0.28440921008586884, Final Batch Loss: 0.0682852640748024\n",
      "Epoch 4485, Loss: 0.2779463902115822, Final Batch Loss: 0.0695936307311058\n",
      "Epoch 4486, Loss: 0.45798787474632263, Final Batch Loss: 0.14930211007595062\n",
      "Epoch 4487, Loss: 0.28287550806999207, Final Batch Loss: 0.07485240697860718\n",
      "Epoch 4488, Loss: 0.3010924644768238, Final Batch Loss: 0.057202715426683426\n",
      "Epoch 4489, Loss: 0.28360728919506073, Final Batch Loss: 0.07113314419984818\n",
      "Epoch 4490, Loss: 0.305996410548687, Final Batch Loss: 0.08072149753570557\n",
      "Epoch 4491, Loss: 0.26240812987089157, Final Batch Loss: 0.1087101399898529\n",
      "Epoch 4492, Loss: 0.3043069541454315, Final Batch Loss: 0.10506303608417511\n",
      "Epoch 4493, Loss: 0.3779752030968666, Final Batch Loss: 0.15954531729221344\n",
      "Epoch 4494, Loss: 0.34683357924222946, Final Batch Loss: 0.18983261287212372\n",
      "Epoch 4495, Loss: 0.3144189640879631, Final Batch Loss: 0.13643093407154083\n",
      "Epoch 4496, Loss: 0.25871677696704865, Final Batch Loss: 0.04931240528821945\n",
      "Epoch 4497, Loss: 0.27674388140439987, Final Batch Loss: 0.05935123562812805\n",
      "Epoch 4498, Loss: 0.3968202695250511, Final Batch Loss: 0.13513854146003723\n",
      "Epoch 4499, Loss: 0.36736728996038437, Final Batch Loss: 0.16768690943717957\n",
      "Epoch 4500, Loss: 0.3699551150202751, Final Batch Loss: 0.16669616103172302\n",
      "Epoch 4501, Loss: 0.4366860017180443, Final Batch Loss: 0.21241453289985657\n",
      "Epoch 4502, Loss: 0.3848967179656029, Final Batch Loss: 0.20940272510051727\n",
      "Epoch 4503, Loss: 0.3059361204504967, Final Batch Loss: 0.09237301349639893\n",
      "Epoch 4504, Loss: 0.32669565826654434, Final Batch Loss: 0.06972692906856537\n",
      "Epoch 4505, Loss: 0.2665444426238537, Final Batch Loss: 0.05920001491904259\n",
      "Epoch 4506, Loss: 0.32865824550390244, Final Batch Loss: 0.08906975388526917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4507, Loss: 0.2647014334797859, Final Batch Loss: 0.08170058578252792\n",
      "Epoch 4508, Loss: 0.36285897344350815, Final Batch Loss: 0.05826465040445328\n",
      "Epoch 4509, Loss: 0.45149819552898407, Final Batch Loss: 0.20201624929904938\n",
      "Epoch 4510, Loss: 0.3332829847931862, Final Batch Loss: 0.1197148934006691\n",
      "Epoch 4511, Loss: 0.23209155350923538, Final Batch Loss: 0.03810727596282959\n",
      "Epoch 4512, Loss: 0.3558754324913025, Final Batch Loss: 0.14850911498069763\n",
      "Epoch 4513, Loss: 0.33621592819690704, Final Batch Loss: 0.1062137708067894\n",
      "Epoch 4514, Loss: 0.3299129605293274, Final Batch Loss: 0.16022346913814545\n",
      "Epoch 4515, Loss: 0.2708030864596367, Final Batch Loss: 0.0830310732126236\n",
      "Epoch 4516, Loss: 0.3159126862883568, Final Batch Loss: 0.0685291439294815\n",
      "Epoch 4517, Loss: 0.3249543011188507, Final Batch Loss: 0.13776159286499023\n",
      "Epoch 4518, Loss: 0.3088781572878361, Final Batch Loss: 0.06095435097813606\n",
      "Epoch 4519, Loss: 0.3647674694657326, Final Batch Loss: 0.1612696498632431\n",
      "Epoch 4520, Loss: 0.35895535349845886, Final Batch Loss: 0.15622834861278534\n",
      "Epoch 4521, Loss: 0.2955397740006447, Final Batch Loss: 0.07810593396425247\n",
      "Epoch 4522, Loss: 0.30703043192625046, Final Batch Loss: 0.10932869464159012\n",
      "Epoch 4523, Loss: 0.34383130073547363, Final Batch Loss: 0.07570908963680267\n",
      "Epoch 4524, Loss: 0.37142564356327057, Final Batch Loss: 0.1559027135372162\n",
      "Epoch 4525, Loss: 0.30850400775671005, Final Batch Loss: 0.11329008638858795\n",
      "Epoch 4526, Loss: 0.3740117773413658, Final Batch Loss: 0.17629897594451904\n",
      "Epoch 4527, Loss: 0.3230860233306885, Final Batch Loss: 0.10579220205545425\n",
      "Epoch 4528, Loss: 0.3651331216096878, Final Batch Loss: 0.10181587934494019\n",
      "Epoch 4529, Loss: 0.37715859711170197, Final Batch Loss: 0.1654249131679535\n",
      "Epoch 4530, Loss: 0.2546114921569824, Final Batch Loss: 0.047264114022254944\n",
      "Epoch 4531, Loss: 0.30896443128585815, Final Batch Loss: 0.11585910618305206\n",
      "Epoch 4532, Loss: 0.2898496985435486, Final Batch Loss: 0.0787629634141922\n",
      "Epoch 4533, Loss: 0.23496897146105766, Final Batch Loss: 0.05569049343466759\n",
      "Epoch 4534, Loss: 0.32464271038770676, Final Batch Loss: 0.10493320971727371\n",
      "Epoch 4535, Loss: 0.2677203044295311, Final Batch Loss: 0.07989875227212906\n",
      "Epoch 4536, Loss: 0.28187207132577896, Final Batch Loss: 0.07633473724126816\n",
      "Epoch 4537, Loss: 0.34444954991340637, Final Batch Loss: 0.14208228886127472\n",
      "Epoch 4538, Loss: 0.2774093225598335, Final Batch Loss: 0.08535083383321762\n",
      "Epoch 4539, Loss: 0.27968087792396545, Final Batch Loss: 0.056625328958034515\n",
      "Epoch 4540, Loss: 0.35135631263256073, Final Batch Loss: 0.1610255241394043\n",
      "Epoch 4541, Loss: 0.37768756970763206, Final Batch Loss: 0.06167170777916908\n",
      "Epoch 4542, Loss: 0.27728668600320816, Final Batch Loss: 0.06772641092538834\n",
      "Epoch 4543, Loss: 0.30394306778907776, Final Batch Loss: 0.08787046372890472\n",
      "Epoch 4544, Loss: 0.26086457818746567, Final Batch Loss: 0.07960522174835205\n",
      "Epoch 4545, Loss: 0.27886317670345306, Final Batch Loss: 0.05839168280363083\n",
      "Epoch 4546, Loss: 0.3799165189266205, Final Batch Loss: 0.12203121185302734\n",
      "Epoch 4547, Loss: 0.2578759044408798, Final Batch Loss: 0.07690192013978958\n",
      "Epoch 4548, Loss: 0.2924363240599632, Final Batch Loss: 0.059567034244537354\n",
      "Epoch 4549, Loss: 0.30331163853406906, Final Batch Loss: 0.06543359160423279\n",
      "Epoch 4550, Loss: 0.2915116474032402, Final Batch Loss: 0.11954924464225769\n",
      "Epoch 4551, Loss: 0.31631598621606827, Final Batch Loss: 0.10494674742221832\n",
      "Epoch 4552, Loss: 0.26805776730179787, Final Batch Loss: 0.04291830584406853\n",
      "Epoch 4553, Loss: 0.24951742216944695, Final Batch Loss: 0.05182596668601036\n",
      "Epoch 4554, Loss: 0.27983345836400986, Final Batch Loss: 0.09219380468130112\n",
      "Epoch 4555, Loss: 0.41096996515989304, Final Batch Loss: 0.21815870702266693\n",
      "Epoch 4556, Loss: 0.3117930442094803, Final Batch Loss: 0.048509083688259125\n",
      "Epoch 4557, Loss: 0.5210675448179245, Final Batch Loss: 0.2823788523674011\n",
      "Epoch 4558, Loss: 0.31298384070396423, Final Batch Loss: 0.09798723459243774\n",
      "Epoch 4559, Loss: 0.31443803012371063, Final Batch Loss: 0.11927702277898788\n",
      "Epoch 4560, Loss: 0.3190739154815674, Final Batch Loss: 0.11903100460767746\n",
      "Epoch 4561, Loss: 0.3205472156405449, Final Batch Loss: 0.11509963124990463\n",
      "Epoch 4562, Loss: 0.22753901407122612, Final Batch Loss: 0.033513832837343216\n",
      "Epoch 4563, Loss: 0.2725699320435524, Final Batch Loss: 0.09047621488571167\n",
      "Epoch 4564, Loss: 0.2766871377825737, Final Batch Loss: 0.03343169391155243\n",
      "Epoch 4565, Loss: 0.35629071295261383, Final Batch Loss: 0.18102003633975983\n",
      "Epoch 4566, Loss: 0.2741628736257553, Final Batch Loss: 0.08556883037090302\n",
      "Epoch 4567, Loss: 0.3605477288365364, Final Batch Loss: 0.13692237436771393\n",
      "Epoch 4568, Loss: 0.26928187906742096, Final Batch Loss: 0.09489542990922928\n",
      "Epoch 4569, Loss: 0.2225489467382431, Final Batch Loss: 0.037581026554107666\n",
      "Epoch 4570, Loss: 0.4299343377351761, Final Batch Loss: 0.2267407774925232\n",
      "Epoch 4571, Loss: 0.2563933655619621, Final Batch Loss: 0.08622980117797852\n",
      "Epoch 4572, Loss: 0.35664506256580353, Final Batch Loss: 0.16627271473407745\n",
      "Epoch 4573, Loss: 0.3601404130458832, Final Batch Loss: 0.08426904678344727\n",
      "Epoch 4574, Loss: 0.30469992756843567, Final Batch Loss: 0.11600742489099503\n",
      "Epoch 4575, Loss: 0.30317187309265137, Final Batch Loss: 0.11132364720106125\n",
      "Epoch 4576, Loss: 0.42927467077970505, Final Batch Loss: 0.18502241373062134\n",
      "Epoch 4577, Loss: 0.2659531868994236, Final Batch Loss: 0.11391738802194595\n",
      "Epoch 4578, Loss: 0.27475740388035774, Final Batch Loss: 0.04401721432805061\n",
      "Epoch 4579, Loss: 0.32326529175043106, Final Batch Loss: 0.07417371869087219\n",
      "Epoch 4580, Loss: 0.23489905893802643, Final Batch Loss: 0.06802456825971603\n",
      "Epoch 4581, Loss: 0.3229263201355934, Final Batch Loss: 0.11496199667453766\n",
      "Epoch 4582, Loss: 0.29853853210806847, Final Batch Loss: 0.03701971098780632\n",
      "Epoch 4583, Loss: 0.23691652342677116, Final Batch Loss: 0.036377910524606705\n",
      "Epoch 4584, Loss: 0.3319797068834305, Final Batch Loss: 0.13960488140583038\n",
      "Epoch 4585, Loss: 0.33447273075580597, Final Batch Loss: 0.1228698343038559\n",
      "Epoch 4586, Loss: 0.303276389837265, Final Batch Loss: 0.1079939529299736\n",
      "Epoch 4587, Loss: 0.3279201090335846, Final Batch Loss: 0.0909758135676384\n",
      "Epoch 4588, Loss: 0.31413088738918304, Final Batch Loss: 0.07798360288143158\n",
      "Epoch 4589, Loss: 0.41502270102500916, Final Batch Loss: 0.22593356668949127\n",
      "Epoch 4590, Loss: 0.2545465752482414, Final Batch Loss: 0.04939288645982742\n",
      "Epoch 4591, Loss: 0.2899092584848404, Final Batch Loss: 0.08344566822052002\n",
      "Epoch 4592, Loss: 0.2952541559934616, Final Batch Loss: 0.11543481051921844\n",
      "Epoch 4593, Loss: 0.2605673745274544, Final Batch Loss: 0.08803553134202957\n",
      "Epoch 4594, Loss: 0.32215022295713425, Final Batch Loss: 0.12417126446962357\n",
      "Epoch 4595, Loss: 0.29725851118564606, Final Batch Loss: 0.12454552203416824\n",
      "Epoch 4596, Loss: 0.3417252451181412, Final Batch Loss: 0.1322016716003418\n",
      "Epoch 4597, Loss: 0.318553376942873, Final Batch Loss: 0.04667672887444496\n",
      "Epoch 4598, Loss: 0.36318594962358475, Final Batch Loss: 0.14590537548065186\n",
      "Epoch 4599, Loss: 0.34048210084438324, Final Batch Loss: 0.07601697742938995\n",
      "Epoch 4600, Loss: 0.3093012571334839, Final Batch Loss: 0.11830703914165497\n",
      "Epoch 4601, Loss: 0.33327309787273407, Final Batch Loss: 0.13281238079071045\n",
      "Epoch 4602, Loss: 0.3787326291203499, Final Batch Loss: 0.12761016190052032\n",
      "Epoch 4603, Loss: 0.2870411053299904, Final Batch Loss: 0.03413639962673187\n",
      "Epoch 4604, Loss: 0.3408875986933708, Final Batch Loss: 0.06600470840930939\n",
      "Epoch 4605, Loss: 0.2599433735013008, Final Batch Loss: 0.025588475167751312\n",
      "Epoch 4606, Loss: 0.38377393037080765, Final Batch Loss: 0.12359634786844254\n",
      "Epoch 4607, Loss: 0.305286169052124, Final Batch Loss: 0.07895644754171371\n",
      "Epoch 4608, Loss: 0.27746807411313057, Final Batch Loss: 0.05749952420592308\n",
      "Epoch 4609, Loss: 0.26923467963933945, Final Batch Loss: 0.0687343180179596\n",
      "Epoch 4610, Loss: 0.36758581548929214, Final Batch Loss: 0.16411817073822021\n",
      "Epoch 4611, Loss: 0.24761556088924408, Final Batch Loss: 0.06609481573104858\n",
      "Epoch 4612, Loss: 0.3032050132751465, Final Batch Loss: 0.12709657847881317\n",
      "Epoch 4613, Loss: 0.29564929753541946, Final Batch Loss: 0.09733839333057404\n",
      "Epoch 4614, Loss: 0.3461754322052002, Final Batch Loss: 0.12285412847995758\n",
      "Epoch 4615, Loss: 0.3408844619989395, Final Batch Loss: 0.12050195783376694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4616, Loss: 0.48610279709100723, Final Batch Loss: 0.27485188841819763\n",
      "Epoch 4617, Loss: 0.35569683462381363, Final Batch Loss: 0.1263762265443802\n",
      "Epoch 4618, Loss: 0.2958154082298279, Final Batch Loss: 0.07248618453741074\n",
      "Epoch 4619, Loss: 0.3197617083787918, Final Batch Loss: 0.10963337123394012\n",
      "Epoch 4620, Loss: 0.34044988453388214, Final Batch Loss: 0.13751323521137238\n",
      "Epoch 4621, Loss: 0.30349200963974, Final Batch Loss: 0.08092719316482544\n",
      "Epoch 4622, Loss: 0.2952748015522957, Final Batch Loss: 0.049732863903045654\n",
      "Epoch 4623, Loss: 0.3689730539917946, Final Batch Loss: 0.13483236730098724\n",
      "Epoch 4624, Loss: 0.23615634813904762, Final Batch Loss: 0.06080830469727516\n",
      "Epoch 4625, Loss: 0.3898979052901268, Final Batch Loss: 0.21168117225170135\n",
      "Epoch 4626, Loss: 0.40315866470336914, Final Batch Loss: 0.22648313641548157\n",
      "Epoch 4627, Loss: 0.2692171409726143, Final Batch Loss: 0.07874655723571777\n",
      "Epoch 4628, Loss: 0.35683833807706833, Final Batch Loss: 0.13767053186893463\n",
      "Epoch 4629, Loss: 0.30866582691669464, Final Batch Loss: 0.12334458529949188\n",
      "Epoch 4630, Loss: 0.2678394094109535, Final Batch Loss: 0.052587948739528656\n",
      "Epoch 4631, Loss: 0.28078486397862434, Final Batch Loss: 0.058981847018003464\n",
      "Epoch 4632, Loss: 0.3271680250763893, Final Batch Loss: 0.1136360689997673\n",
      "Epoch 4633, Loss: 0.291313000023365, Final Batch Loss: 0.11491429060697556\n",
      "Epoch 4634, Loss: 0.3170604705810547, Final Batch Loss: 0.06237069517374039\n",
      "Epoch 4635, Loss: 0.46094413846731186, Final Batch Loss: 0.1067638024687767\n",
      "Epoch 4636, Loss: 0.29112864285707474, Final Batch Loss: 0.058977797627449036\n",
      "Epoch 4637, Loss: 0.2544682789593935, Final Batch Loss: 0.026327790692448616\n",
      "Epoch 4638, Loss: 0.29040516167879105, Final Batch Loss: 0.08448060601949692\n",
      "Epoch 4639, Loss: 0.34297408163547516, Final Batch Loss: 0.12699469923973083\n",
      "Epoch 4640, Loss: 0.2570471689105034, Final Batch Loss: 0.0770631730556488\n",
      "Epoch 4641, Loss: 0.33973949402570724, Final Batch Loss: 0.08798675239086151\n",
      "Epoch 4642, Loss: 0.2569849044084549, Final Batch Loss: 0.07684114575386047\n",
      "Epoch 4643, Loss: 0.32904036343097687, Final Batch Loss: 0.12682472169399261\n",
      "Epoch 4644, Loss: 0.39648718386888504, Final Batch Loss: 0.08534436672925949\n",
      "Epoch 4645, Loss: 0.47857607156038284, Final Batch Loss: 0.1531742811203003\n",
      "Epoch 4646, Loss: 0.2901192381978035, Final Batch Loss: 0.0462665855884552\n",
      "Epoch 4647, Loss: 0.30019523203372955, Final Batch Loss: 0.11612164974212646\n",
      "Epoch 4648, Loss: 0.319871686398983, Final Batch Loss: 0.11338604986667633\n",
      "Epoch 4649, Loss: 0.2393796667456627, Final Batch Loss: 0.044293589890003204\n",
      "Epoch 4650, Loss: 0.24668984860181808, Final Batch Loss: 0.06956188380718231\n",
      "Epoch 4651, Loss: 0.4128521531820297, Final Batch Loss: 0.20068004727363586\n",
      "Epoch 4652, Loss: 0.24839624390006065, Final Batch Loss: 0.03388635441660881\n",
      "Epoch 4653, Loss: 0.2632750105112791, Final Batch Loss: 0.023435955867171288\n",
      "Epoch 4654, Loss: 0.32231809198856354, Final Batch Loss: 0.08161333203315735\n",
      "Epoch 4655, Loss: 0.5428713858127594, Final Batch Loss: 0.3020331859588623\n",
      "Epoch 4656, Loss: 0.4097558706998825, Final Batch Loss: 0.2092844545841217\n",
      "Epoch 4657, Loss: 0.3740200996398926, Final Batch Loss: 0.16702333092689514\n",
      "Epoch 4658, Loss: 0.26043467968702316, Final Batch Loss: 0.07401856034994125\n",
      "Epoch 4659, Loss: 0.3112732544541359, Final Batch Loss: 0.10996370017528534\n",
      "Epoch 4660, Loss: 0.26081619411706924, Final Batch Loss: 0.05058177560567856\n",
      "Epoch 4661, Loss: 0.5403951331973076, Final Batch Loss: 0.3670256733894348\n",
      "Epoch 4662, Loss: 0.33423415571451187, Final Batch Loss: 0.13842491805553436\n",
      "Epoch 4663, Loss: 0.32763010263442993, Final Batch Loss: 0.07670913636684418\n",
      "Epoch 4664, Loss: 0.21307284012436867, Final Batch Loss: 0.034351665526628494\n",
      "Epoch 4665, Loss: 0.29892170429229736, Final Batch Loss: 0.07881222665309906\n",
      "Epoch 4666, Loss: 0.2817576602101326, Final Batch Loss: 0.07363059371709824\n",
      "Epoch 4667, Loss: 0.42729347199201584, Final Batch Loss: 0.20296943187713623\n",
      "Epoch 4668, Loss: 0.22357431054115295, Final Batch Loss: 0.052568480372428894\n",
      "Epoch 4669, Loss: 0.30574220418930054, Final Batch Loss: 0.0950716957449913\n",
      "Epoch 4670, Loss: 0.2584223486483097, Final Batch Loss: 0.037978921085596085\n",
      "Epoch 4671, Loss: 0.23526426032185555, Final Batch Loss: 0.0830962285399437\n",
      "Epoch 4672, Loss: 0.2315313033759594, Final Batch Loss: 0.03982754424214363\n",
      "Epoch 4673, Loss: 0.31432703882455826, Final Batch Loss: 0.09557337313890457\n",
      "Epoch 4674, Loss: 0.31780019775032997, Final Batch Loss: 0.05972475931048393\n",
      "Epoch 4675, Loss: 0.22804170101881027, Final Batch Loss: 0.06792658567428589\n",
      "Epoch 4676, Loss: 0.2850303649902344, Final Batch Loss: 0.10553836822509766\n",
      "Epoch 4677, Loss: 0.2830760031938553, Final Batch Loss: 0.08294925838708878\n",
      "Epoch 4678, Loss: 0.29866377264261246, Final Batch Loss: 0.08562895655632019\n",
      "Epoch 4679, Loss: 0.32315150648355484, Final Batch Loss: 0.1526312381029129\n",
      "Epoch 4680, Loss: 0.31097516417503357, Final Batch Loss: 0.11336308717727661\n",
      "Epoch 4681, Loss: 0.22672489657998085, Final Batch Loss: 0.05978213623166084\n",
      "Epoch 4682, Loss: 0.3190147951245308, Final Batch Loss: 0.10767938941717148\n",
      "Epoch 4683, Loss: 0.25446489080786705, Final Batch Loss: 0.05689455196261406\n",
      "Epoch 4684, Loss: 0.27186935022473335, Final Batch Loss: 0.05048314854502678\n",
      "Epoch 4685, Loss: 0.27975844219326973, Final Batch Loss: 0.05274892970919609\n",
      "Epoch 4686, Loss: 0.3536290228366852, Final Batch Loss: 0.1404033750295639\n",
      "Epoch 4687, Loss: 0.3476801887154579, Final Batch Loss: 0.18861669301986694\n",
      "Epoch 4688, Loss: 0.47161638736724854, Final Batch Loss: 0.24621309340000153\n",
      "Epoch 4689, Loss: 0.28882117569446564, Final Batch Loss: 0.08987396955490112\n",
      "Epoch 4690, Loss: 0.37508872151374817, Final Batch Loss: 0.093159019947052\n",
      "Epoch 4691, Loss: 0.30741726607084274, Final Batch Loss: 0.10800021141767502\n",
      "Epoch 4692, Loss: 0.2751309461891651, Final Batch Loss: 0.05113588646054268\n",
      "Epoch 4693, Loss: 0.28888484835624695, Final Batch Loss: 0.08982642740011215\n",
      "Epoch 4694, Loss: 0.277499757707119, Final Batch Loss: 0.08399293571710587\n",
      "Epoch 4695, Loss: 0.29636235162615776, Final Batch Loss: 0.04589204862713814\n",
      "Epoch 4696, Loss: 0.24056559056043625, Final Batch Loss: 0.05544447898864746\n",
      "Epoch 4697, Loss: 0.3763030916452408, Final Batch Loss: 0.14515915513038635\n",
      "Epoch 4698, Loss: 0.28781305998563766, Final Batch Loss: 0.07172207534313202\n",
      "Epoch 4699, Loss: 0.4797700345516205, Final Batch Loss: 0.21556255221366882\n",
      "Epoch 4700, Loss: 0.29259948432445526, Final Batch Loss: 0.11655574291944504\n",
      "Epoch 4701, Loss: 0.3542279899120331, Final Batch Loss: 0.1292649507522583\n",
      "Epoch 4702, Loss: 0.25692442804574966, Final Batch Loss: 0.08070980757474899\n",
      "Epoch 4703, Loss: 0.28525980561971664, Final Batch Loss: 0.07627879083156586\n",
      "Epoch 4704, Loss: 0.28238459676504135, Final Batch Loss: 0.08166876435279846\n",
      "Epoch 4705, Loss: 0.2655574791133404, Final Batch Loss: 0.05091763660311699\n",
      "Epoch 4706, Loss: 0.38391515612602234, Final Batch Loss: 0.14368189871311188\n",
      "Epoch 4707, Loss: 0.2508172132074833, Final Batch Loss: 0.041125278919935226\n",
      "Epoch 4708, Loss: 0.32833032310009, Final Batch Loss: 0.16071827709674835\n",
      "Epoch 4709, Loss: 0.2683575749397278, Final Batch Loss: 0.07913447171449661\n",
      "Epoch 4710, Loss: 0.21658163517713547, Final Batch Loss: 0.04689326137304306\n",
      "Epoch 4711, Loss: 0.2721957713365555, Final Batch Loss: 0.05394207686185837\n",
      "Epoch 4712, Loss: 0.3381172865629196, Final Batch Loss: 0.1390119343996048\n",
      "Epoch 4713, Loss: 0.32390106469392776, Final Batch Loss: 0.08946677297353745\n",
      "Epoch 4714, Loss: 0.3451189249753952, Final Batch Loss: 0.17576375603675842\n",
      "Epoch 4715, Loss: 0.36863692849874496, Final Batch Loss: 0.1096234992146492\n",
      "Epoch 4716, Loss: 0.2398376241326332, Final Batch Loss: 0.09054316580295563\n",
      "Epoch 4717, Loss: 0.2873856648802757, Final Batch Loss: 0.13368740677833557\n",
      "Epoch 4718, Loss: 0.3228323459625244, Final Batch Loss: 0.09358907490968704\n",
      "Epoch 4719, Loss: 0.21273632161319256, Final Batch Loss: 0.020914768800139427\n",
      "Epoch 4720, Loss: 0.3277835249900818, Final Batch Loss: 0.08095795661211014\n",
      "Epoch 4721, Loss: 0.35633328557014465, Final Batch Loss: 0.06785771995782852\n",
      "Epoch 4722, Loss: 0.4217195138335228, Final Batch Loss: 0.18427327275276184\n",
      "Epoch 4723, Loss: 0.25550494343042374, Final Batch Loss: 0.057742081582546234\n",
      "Epoch 4724, Loss: 0.25821733847260475, Final Batch Loss: 0.048156049102544785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4725, Loss: 0.37612713873386383, Final Batch Loss: 0.1414613425731659\n",
      "Epoch 4726, Loss: 0.2523181438446045, Final Batch Loss: 0.08298676460981369\n",
      "Epoch 4727, Loss: 0.27896780520677567, Final Batch Loss: 0.10288671404123306\n",
      "Epoch 4728, Loss: 0.430941641330719, Final Batch Loss: 0.10538917779922485\n",
      "Epoch 4729, Loss: 0.36336204409599304, Final Batch Loss: 0.1306615024805069\n",
      "Epoch 4730, Loss: 0.31934767961502075, Final Batch Loss: 0.07590359449386597\n",
      "Epoch 4731, Loss: 0.27514181286096573, Final Batch Loss: 0.08419052511453629\n",
      "Epoch 4732, Loss: 0.29424311220645905, Final Batch Loss: 0.1283578723669052\n",
      "Epoch 4733, Loss: 0.34988246113061905, Final Batch Loss: 0.15470010042190552\n",
      "Epoch 4734, Loss: 0.26409419253468513, Final Batch Loss: 0.04902709648013115\n",
      "Epoch 4735, Loss: 0.2666783891618252, Final Batch Loss: 0.12211160361766815\n",
      "Epoch 4736, Loss: 0.32799138873815536, Final Batch Loss: 0.09975694864988327\n",
      "Epoch 4737, Loss: 0.2280326411128044, Final Batch Loss: 0.051645562052726746\n",
      "Epoch 4738, Loss: 0.26444120705127716, Final Batch Loss: 0.0883834958076477\n",
      "Epoch 4739, Loss: 0.24417419731616974, Final Batch Loss: 0.0690164640545845\n",
      "Epoch 4740, Loss: 0.25716814398765564, Final Batch Loss: 0.10832676291465759\n",
      "Epoch 4741, Loss: 0.22254206240177155, Final Batch Loss: 0.0693332627415657\n",
      "Epoch 4742, Loss: 0.29556064307689667, Final Batch Loss: 0.0842919573187828\n",
      "Epoch 4743, Loss: 0.34308575093746185, Final Batch Loss: 0.12643513083457947\n",
      "Epoch 4744, Loss: 0.27999401837587357, Final Batch Loss: 0.11388412863016129\n",
      "Epoch 4745, Loss: 0.23921427130699158, Final Batch Loss: 0.07440758496522903\n",
      "Epoch 4746, Loss: 0.30584149807691574, Final Batch Loss: 0.08732906728982925\n",
      "Epoch 4747, Loss: 0.19467898085713387, Final Batch Loss: 0.03671897575259209\n",
      "Epoch 4748, Loss: 0.6560410112142563, Final Batch Loss: 0.4778418242931366\n",
      "Epoch 4749, Loss: 0.4018592983484268, Final Batch Loss: 0.13818641006946564\n",
      "Epoch 4750, Loss: 0.2887551337480545, Final Batch Loss: 0.09711995720863342\n",
      "Epoch 4751, Loss: 0.2980467826128006, Final Batch Loss: 0.12164103239774704\n",
      "Epoch 4752, Loss: 0.32772189378738403, Final Batch Loss: 0.0923018828034401\n",
      "Epoch 4753, Loss: 0.24188802763819695, Final Batch Loss: 0.048118706792593\n",
      "Epoch 4754, Loss: 0.27499817311763763, Final Batch Loss: 0.09017845243215561\n",
      "Epoch 4755, Loss: 0.28070444986224174, Final Batch Loss: 0.054677773267030716\n",
      "Epoch 4756, Loss: 0.2553766407072544, Final Batch Loss: 0.04974483326077461\n",
      "Epoch 4757, Loss: 0.3893057703971863, Final Batch Loss: 0.1920107752084732\n",
      "Epoch 4758, Loss: 0.2426421195268631, Final Batch Loss: 0.06320670992136002\n",
      "Epoch 4759, Loss: 0.2541113421320915, Final Batch Loss: 0.04463940113782883\n",
      "Epoch 4760, Loss: 0.3318137228488922, Final Batch Loss: 0.1257341057062149\n",
      "Epoch 4761, Loss: 0.3734769746661186, Final Batch Loss: 0.1383216381072998\n",
      "Epoch 4762, Loss: 0.2855469360947609, Final Batch Loss: 0.11164859682321548\n",
      "Epoch 4763, Loss: 0.2873302362859249, Final Batch Loss: 0.04983365908265114\n",
      "Epoch 4764, Loss: 0.3447231203317642, Final Batch Loss: 0.0730317234992981\n",
      "Epoch 4765, Loss: 0.26691117510199547, Final Batch Loss: 0.04624282196164131\n",
      "Epoch 4766, Loss: 0.3315439186990261, Final Batch Loss: 0.05154002085328102\n",
      "Epoch 4767, Loss: 0.25846096500754356, Final Batch Loss: 0.0491100437939167\n",
      "Epoch 4768, Loss: 0.2832943722605705, Final Batch Loss: 0.06662634760141373\n",
      "Epoch 4769, Loss: 0.34399440698325634, Final Batch Loss: 0.030692627653479576\n",
      "Epoch 4770, Loss: 0.2736954167485237, Final Batch Loss: 0.09397189319133759\n",
      "Epoch 4771, Loss: 0.2783970683813095, Final Batch Loss: 0.10602718591690063\n",
      "Epoch 4772, Loss: 0.27069055289030075, Final Batch Loss: 0.07851725071668625\n",
      "Epoch 4773, Loss: 0.32664917409420013, Final Batch Loss: 0.10566235333681107\n",
      "Epoch 4774, Loss: 0.268434077501297, Final Batch Loss: 0.10441744327545166\n",
      "Epoch 4775, Loss: 0.22613189462572336, Final Batch Loss: 0.009949908591806889\n",
      "Epoch 4776, Loss: 0.31077249348163605, Final Batch Loss: 0.12827827036380768\n",
      "Epoch 4777, Loss: 0.23820490762591362, Final Batch Loss: 0.05600864067673683\n",
      "Epoch 4778, Loss: 0.2977658361196518, Final Batch Loss: 0.10121800005435944\n",
      "Epoch 4779, Loss: 0.3652954772114754, Final Batch Loss: 0.17121689021587372\n",
      "Epoch 4780, Loss: 0.26054271310567856, Final Batch Loss: 0.08723558485507965\n",
      "Epoch 4781, Loss: 0.248393714427948, Final Batch Loss: 0.03631311655044556\n",
      "Epoch 4782, Loss: 0.3338865302503109, Final Batch Loss: 0.10784278810024261\n",
      "Epoch 4783, Loss: 0.22484955750405788, Final Batch Loss: 0.025846054777503014\n",
      "Epoch 4784, Loss: 0.2965231388807297, Final Batch Loss: 0.06686215847730637\n",
      "Epoch 4785, Loss: 0.2767070531845093, Final Batch Loss: 0.10298579931259155\n",
      "Epoch 4786, Loss: 0.2536287046968937, Final Batch Loss: 0.03442135825753212\n",
      "Epoch 4787, Loss: 0.2691700905561447, Final Batch Loss: 0.09347108006477356\n",
      "Epoch 4788, Loss: 0.2581959068775177, Final Batch Loss: 0.06255590170621872\n",
      "Epoch 4789, Loss: 0.2861549034714699, Final Batch Loss: 0.06372018903493881\n",
      "Epoch 4790, Loss: 0.19829195737838745, Final Batch Loss: 0.03859034180641174\n",
      "Epoch 4791, Loss: 0.3123222813010216, Final Batch Loss: 0.0840454027056694\n",
      "Epoch 4792, Loss: 0.3444711044430733, Final Batch Loss: 0.14355693757534027\n",
      "Epoch 4793, Loss: 0.25900569558143616, Final Batch Loss: 0.06263651698827744\n",
      "Epoch 4794, Loss: 0.3063337057828903, Final Batch Loss: 0.09706516563892365\n",
      "Epoch 4795, Loss: 0.3648456409573555, Final Batch Loss: 0.10061603039503098\n",
      "Epoch 4796, Loss: 0.2429668977856636, Final Batch Loss: 0.0702936053276062\n",
      "Epoch 4797, Loss: 0.27960005402565, Final Batch Loss: 0.10560271888971329\n",
      "Epoch 4798, Loss: 0.30113736167550087, Final Batch Loss: 0.058269549161195755\n",
      "Epoch 4799, Loss: 0.30048172175884247, Final Batch Loss: 0.11952950060367584\n",
      "Epoch 4800, Loss: 0.3004552945494652, Final Batch Loss: 0.10677383095026016\n",
      "Epoch 4801, Loss: 0.3834911286830902, Final Batch Loss: 0.22834351658821106\n",
      "Epoch 4802, Loss: 0.3700920417904854, Final Batch Loss: 0.10976427793502808\n",
      "Epoch 4803, Loss: 0.3295677527785301, Final Batch Loss: 0.1166943833231926\n",
      "Epoch 4804, Loss: 0.4367072582244873, Final Batch Loss: 0.17065289616584778\n",
      "Epoch 4805, Loss: 0.480379194021225, Final Batch Loss: 0.17954106628894806\n",
      "Epoch 4806, Loss: 0.3384997174143791, Final Batch Loss: 0.08499056100845337\n",
      "Epoch 4807, Loss: 0.4305243641138077, Final Batch Loss: 0.13300618529319763\n",
      "Epoch 4808, Loss: 0.2849840745329857, Final Batch Loss: 0.06724591553211212\n",
      "Epoch 4809, Loss: 0.3007969334721565, Final Batch Loss: 0.1384790986776352\n",
      "Epoch 4810, Loss: 0.3497069254517555, Final Batch Loss: 0.10880699753761292\n",
      "Epoch 4811, Loss: 0.3790443316102028, Final Batch Loss: 0.11384169012308121\n",
      "Epoch 4812, Loss: 0.47072944045066833, Final Batch Loss: 0.2726035416126251\n",
      "Epoch 4813, Loss: 0.2536841332912445, Final Batch Loss: 0.05794063210487366\n",
      "Epoch 4814, Loss: 0.24472332000732422, Final Batch Loss: 0.05089644342660904\n",
      "Epoch 4815, Loss: 0.22889164090156555, Final Batch Loss: 0.06608182191848755\n",
      "Epoch 4816, Loss: 0.27182791382074356, Final Batch Loss: 0.08375988155603409\n",
      "Epoch 4817, Loss: 0.2632048949599266, Final Batch Loss: 0.05122588574886322\n",
      "Epoch 4818, Loss: 0.26775435730814934, Final Batch Loss: 0.11994896084070206\n",
      "Epoch 4819, Loss: 0.2660160828381777, Final Batch Loss: 0.02868759073317051\n",
      "Epoch 4820, Loss: 0.2579369284212589, Final Batch Loss: 0.05689692124724388\n",
      "Epoch 4821, Loss: 0.5313474237918854, Final Batch Loss: 0.3655742406845093\n",
      "Epoch 4822, Loss: 0.2401244193315506, Final Batch Loss: 0.04783563315868378\n",
      "Epoch 4823, Loss: 0.32964932918548584, Final Batch Loss: 0.15835295617580414\n",
      "Epoch 4824, Loss: 0.29560788720846176, Final Batch Loss: 0.05412723124027252\n",
      "Epoch 4825, Loss: 0.2658158801496029, Final Batch Loss: 0.05193786695599556\n",
      "Epoch 4826, Loss: 0.2651820033788681, Final Batch Loss: 0.04949419945478439\n",
      "Epoch 4827, Loss: 0.5229991227388382, Final Batch Loss: 0.3419424593448639\n",
      "Epoch 4828, Loss: 0.23243062943220139, Final Batch Loss: 0.08150232583284378\n",
      "Epoch 4829, Loss: 0.30269433557987213, Final Batch Loss: 0.08399549126625061\n",
      "Epoch 4830, Loss: 0.28252531588077545, Final Batch Loss: 0.0888567641377449\n",
      "Epoch 4831, Loss: 0.28359121084213257, Final Batch Loss: 0.05020006746053696\n",
      "Epoch 4832, Loss: 0.30750665813684464, Final Batch Loss: 0.10990295559167862\n",
      "Epoch 4833, Loss: 0.2976790517568588, Final Batch Loss: 0.09765107184648514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4834, Loss: 0.28309402614831924, Final Batch Loss: 0.07431855797767639\n",
      "Epoch 4835, Loss: 0.39735599607229233, Final Batch Loss: 0.20732206106185913\n",
      "Epoch 4836, Loss: 0.24724913015961647, Final Batch Loss: 0.05724116787314415\n",
      "Epoch 4837, Loss: 0.261762086302042, Final Batch Loss: 0.04515950009226799\n",
      "Epoch 4838, Loss: 0.2784951776266098, Final Batch Loss: 0.09416014701128006\n",
      "Epoch 4839, Loss: 0.24793190509080887, Final Batch Loss: 0.07858175039291382\n",
      "Epoch 4840, Loss: 0.2517433650791645, Final Batch Loss: 0.04464341327548027\n",
      "Epoch 4841, Loss: 0.2564874403178692, Final Batch Loss: 0.051194656640291214\n",
      "Epoch 4842, Loss: 0.23625003546476364, Final Batch Loss: 0.059510692954063416\n",
      "Epoch 4843, Loss: 0.32010333240032196, Final Batch Loss: 0.14081905782222748\n",
      "Epoch 4844, Loss: 0.35649145394563675, Final Batch Loss: 0.11529888212680817\n",
      "Epoch 4845, Loss: 0.29508379846811295, Final Batch Loss: 0.08688142150640488\n",
      "Epoch 4846, Loss: 0.20732558146119118, Final Batch Loss: 0.052969928830862045\n",
      "Epoch 4847, Loss: 0.2593589723110199, Final Batch Loss: 0.09252019971609116\n",
      "Epoch 4848, Loss: 0.3967936635017395, Final Batch Loss: 0.19857211410999298\n",
      "Epoch 4849, Loss: 0.24423375725746155, Final Batch Loss: 0.05329184979200363\n",
      "Epoch 4850, Loss: 0.33523882180452347, Final Batch Loss: 0.14401017129421234\n",
      "Epoch 4851, Loss: 0.3680390492081642, Final Batch Loss: 0.1240859404206276\n",
      "Epoch 4852, Loss: 0.4300484135746956, Final Batch Loss: 0.27358725666999817\n",
      "Epoch 4853, Loss: 0.329338937997818, Final Batch Loss: 0.10968974977731705\n",
      "Epoch 4854, Loss: 0.2973746582865715, Final Batch Loss: 0.08481760323047638\n",
      "Epoch 4855, Loss: 0.34589288383722305, Final Batch Loss: 0.18101461231708527\n",
      "Epoch 4856, Loss: 0.2861856520175934, Final Batch Loss: 0.08989953994750977\n",
      "Epoch 4857, Loss: 0.2669600397348404, Final Batch Loss: 0.07634050399065018\n",
      "Epoch 4858, Loss: 0.3313552886247635, Final Batch Loss: 0.1288948953151703\n",
      "Epoch 4859, Loss: 0.2680078521370888, Final Batch Loss: 0.07858829200267792\n",
      "Epoch 4860, Loss: 0.30244898796081543, Final Batch Loss: 0.11900892853736877\n",
      "Epoch 4861, Loss: 0.3436524569988251, Final Batch Loss: 0.16783574223518372\n",
      "Epoch 4862, Loss: 0.43134941905736923, Final Batch Loss: 0.20550653338432312\n",
      "Epoch 4863, Loss: 0.24728253483772278, Final Batch Loss: 0.1082974225282669\n",
      "Epoch 4864, Loss: 0.38251446187496185, Final Batch Loss: 0.14547155797481537\n",
      "Epoch 4865, Loss: 0.4787447303533554, Final Batch Loss: 0.17597851157188416\n",
      "Epoch 4866, Loss: 0.41284407675266266, Final Batch Loss: 0.17036329209804535\n",
      "Epoch 4867, Loss: 0.3837016671895981, Final Batch Loss: 0.16726551949977875\n",
      "Epoch 4868, Loss: 0.31125735491514206, Final Batch Loss: 0.13572560250759125\n",
      "Epoch 4869, Loss: 0.2506019324064255, Final Batch Loss: 0.06889400631189346\n",
      "Epoch 4870, Loss: 0.2958367355167866, Final Batch Loss: 0.051563363522291183\n",
      "Epoch 4871, Loss: 0.40351755172014236, Final Batch Loss: 0.17757713794708252\n",
      "Epoch 4872, Loss: 0.3159148171544075, Final Batch Loss: 0.05204896628856659\n",
      "Epoch 4873, Loss: 0.40266772359609604, Final Batch Loss: 0.11686304956674576\n",
      "Epoch 4874, Loss: 0.3216661885380745, Final Batch Loss: 0.0948355570435524\n",
      "Epoch 4875, Loss: 0.26204608380794525, Final Batch Loss: 0.09395542740821838\n",
      "Epoch 4876, Loss: 0.25154148787260056, Final Batch Loss: 0.02463395893573761\n",
      "Epoch 4877, Loss: 0.27362366020679474, Final Batch Loss: 0.03838856518268585\n",
      "Epoch 4878, Loss: 0.3262650743126869, Final Batch Loss: 0.09432219713926315\n",
      "Epoch 4879, Loss: 0.22757074236869812, Final Batch Loss: 0.0694931373000145\n",
      "Epoch 4880, Loss: 0.33109229803085327, Final Batch Loss: 0.15112906694412231\n",
      "Epoch 4881, Loss: 0.26031819730997086, Final Batch Loss: 0.06313864886760712\n",
      "Epoch 4882, Loss: 0.3780906945466995, Final Batch Loss: 0.08914162218570709\n",
      "Epoch 4883, Loss: 0.32160861045122147, Final Batch Loss: 0.10636879503726959\n",
      "Epoch 4884, Loss: 0.28771042823791504, Final Batch Loss: 0.08912381529808044\n",
      "Epoch 4885, Loss: 0.24848533421754837, Final Batch Loss: 0.06658419966697693\n",
      "Epoch 4886, Loss: 0.5581685677170753, Final Batch Loss: 0.3310580253601074\n",
      "Epoch 4887, Loss: 0.41258715093135834, Final Batch Loss: 0.17975811660289764\n",
      "Epoch 4888, Loss: 0.3926781341433525, Final Batch Loss: 0.169903963804245\n",
      "Epoch 4889, Loss: 0.2715921774506569, Final Batch Loss: 0.0650348886847496\n",
      "Epoch 4890, Loss: 0.4131779298186302, Final Batch Loss: 0.17846272885799408\n",
      "Epoch 4891, Loss: 0.3026517853140831, Final Batch Loss: 0.04298575222492218\n",
      "Epoch 4892, Loss: 0.26753219962120056, Final Batch Loss: 0.06832181662321091\n",
      "Epoch 4893, Loss: 0.3751479461789131, Final Batch Loss: 0.17295347154140472\n",
      "Epoch 4894, Loss: 0.3318400979042053, Final Batch Loss: 0.11700507253408432\n",
      "Epoch 4895, Loss: 0.3068656772375107, Final Batch Loss: 0.06355303525924683\n",
      "Epoch 4896, Loss: 0.3150485157966614, Final Batch Loss: 0.10035257786512375\n",
      "Epoch 4897, Loss: 0.28153543174266815, Final Batch Loss: 0.10726546496152878\n",
      "Epoch 4898, Loss: 0.27653317153453827, Final Batch Loss: 0.07341707497835159\n",
      "Epoch 4899, Loss: 0.3165882006287575, Final Batch Loss: 0.11414115130901337\n",
      "Epoch 4900, Loss: 0.26581447571516037, Final Batch Loss: 0.10092300921678543\n",
      "Epoch 4901, Loss: 0.37261833250522614, Final Batch Loss: 0.1343558132648468\n",
      "Epoch 4902, Loss: 0.24046668782830238, Final Batch Loss: 0.04921625927090645\n",
      "Epoch 4903, Loss: 0.3749721273779869, Final Batch Loss: 0.18870484828948975\n",
      "Epoch 4904, Loss: 0.3986327573657036, Final Batch Loss: 0.20606549084186554\n",
      "Epoch 4905, Loss: 0.30540354922413826, Final Batch Loss: 0.062236178666353226\n",
      "Epoch 4906, Loss: 0.27205342799425125, Final Batch Loss: 0.08799871802330017\n",
      "Epoch 4907, Loss: 0.3434645086526871, Final Batch Loss: 0.15212994813919067\n",
      "Epoch 4908, Loss: 0.23142234236001968, Final Batch Loss: 0.04349809139966965\n",
      "Epoch 4909, Loss: 0.2576289251446724, Final Batch Loss: 0.09144745022058487\n",
      "Epoch 4910, Loss: 0.32702625542879105, Final Batch Loss: 0.11986760050058365\n",
      "Epoch 4911, Loss: 0.33901558816432953, Final Batch Loss: 0.09839624166488647\n",
      "Epoch 4912, Loss: 0.32508523762226105, Final Batch Loss: 0.08416134864091873\n",
      "Epoch 4913, Loss: 0.3363778367638588, Final Batch Loss: 0.126703679561615\n",
      "Epoch 4914, Loss: 0.3129005432128906, Final Batch Loss: 0.06492367386817932\n",
      "Epoch 4915, Loss: 0.23562751337885857, Final Batch Loss: 0.05759107694029808\n",
      "Epoch 4916, Loss: 0.32260001450777054, Final Batch Loss: 0.06323174387216568\n",
      "Epoch 4917, Loss: 0.255458801984787, Final Batch Loss: 0.09252434968948364\n",
      "Epoch 4918, Loss: 0.2438209168612957, Final Batch Loss: 0.0584680400788784\n",
      "Epoch 4919, Loss: 0.2686461918056011, Final Batch Loss: 0.05753718689084053\n",
      "Epoch 4920, Loss: 0.23511230945587158, Final Batch Loss: 0.06497271358966827\n",
      "Epoch 4921, Loss: 0.2816157564520836, Final Batch Loss: 0.08796471357345581\n",
      "Epoch 4922, Loss: 0.3785809651017189, Final Batch Loss: 0.17634247243404388\n",
      "Epoch 4923, Loss: 0.26127999275922775, Final Batch Loss: 0.0511075034737587\n",
      "Epoch 4924, Loss: 0.257114939391613, Final Batch Loss: 0.07661058753728867\n",
      "Epoch 4925, Loss: 0.299884170293808, Final Batch Loss: 0.11729206144809723\n",
      "Epoch 4926, Loss: 0.23465152457356453, Final Batch Loss: 0.031678106635808945\n",
      "Epoch 4927, Loss: 0.266269326210022, Final Batch Loss: 0.06195928156375885\n",
      "Epoch 4928, Loss: 0.32601019740104675, Final Batch Loss: 0.06114492565393448\n",
      "Epoch 4929, Loss: 0.3235846236348152, Final Batch Loss: 0.13618582487106323\n",
      "Epoch 4930, Loss: 0.2527593523263931, Final Batch Loss: 0.05455584079027176\n",
      "Epoch 4931, Loss: 0.2741019818931818, Final Batch Loss: 0.030940445140004158\n",
      "Epoch 4932, Loss: 0.4162721559405327, Final Batch Loss: 0.15928193926811218\n",
      "Epoch 4933, Loss: 0.2564706541597843, Final Batch Loss: 0.04471970722079277\n",
      "Epoch 4934, Loss: 0.19834067672491074, Final Batch Loss: 0.01690436154603958\n",
      "Epoch 4935, Loss: 0.2771396338939667, Final Batch Loss: 0.11621098965406418\n",
      "Epoch 4936, Loss: 0.3168014734983444, Final Batch Loss: 0.1196693703532219\n",
      "Epoch 4937, Loss: 0.3208424746990204, Final Batch Loss: 0.09223077446222305\n",
      "Epoch 4938, Loss: 0.30929625779390335, Final Batch Loss: 0.15648823976516724\n",
      "Epoch 4939, Loss: 0.21827882900834084, Final Batch Loss: 0.04572569206357002\n",
      "Epoch 4940, Loss: 0.41965726763010025, Final Batch Loss: 0.22543959319591522\n",
      "Epoch 4941, Loss: 0.2175150141119957, Final Batch Loss: 0.034788794815540314\n",
      "Epoch 4942, Loss: 0.3261432647705078, Final Batch Loss: 0.08326933532953262\n",
      "Epoch 4943, Loss: 0.31492210179567337, Final Batch Loss: 0.07402209937572479\n",
      "Epoch 4944, Loss: 0.2759504094719887, Final Batch Loss: 0.11422748863697052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4945, Loss: 0.2766335979104042, Final Batch Loss: 0.09977003931999207\n",
      "Epoch 4946, Loss: 0.3028443679213524, Final Batch Loss: 0.06658906489610672\n",
      "Epoch 4947, Loss: 0.30929144844412804, Final Batch Loss: 0.11770842224359512\n",
      "Epoch 4948, Loss: 0.2882499471306801, Final Batch Loss: 0.08860563486814499\n",
      "Epoch 4949, Loss: 0.3176240548491478, Final Batch Loss: 0.08476704359054565\n",
      "Epoch 4950, Loss: 0.3384006544947624, Final Batch Loss: 0.14760009944438934\n",
      "Epoch 4951, Loss: 0.2296758070588112, Final Batch Loss: 0.06050165742635727\n",
      "Epoch 4952, Loss: 0.34645141288638115, Final Batch Loss: 0.0494491271674633\n",
      "Epoch 4953, Loss: 0.22244971990585327, Final Batch Loss: 0.043375447392463684\n",
      "Epoch 4954, Loss: 0.39138974994421005, Final Batch Loss: 0.18590934574604034\n",
      "Epoch 4955, Loss: 0.27449624985456467, Final Batch Loss: 0.06262490898370743\n",
      "Epoch 4956, Loss: 0.3238535672426224, Final Batch Loss: 0.12807680666446686\n",
      "Epoch 4957, Loss: 0.33317334949970245, Final Batch Loss: 0.10482857376337051\n",
      "Epoch 4958, Loss: 0.26312580332159996, Final Batch Loss: 0.059245381504297256\n",
      "Epoch 4959, Loss: 0.3073980249464512, Final Batch Loss: 0.04641721770167351\n",
      "Epoch 4960, Loss: 0.21978048980236053, Final Batch Loss: 0.027910731732845306\n",
      "Epoch 4961, Loss: 0.5198291316628456, Final Batch Loss: 0.31796178221702576\n",
      "Epoch 4962, Loss: 0.3386421352624893, Final Batch Loss: 0.17908084392547607\n",
      "Epoch 4963, Loss: 0.3995016887784004, Final Batch Loss: 0.20263729989528656\n",
      "Epoch 4964, Loss: 0.2616240009665489, Final Batch Loss: 0.07240281999111176\n",
      "Epoch 4965, Loss: 0.26774924248456955, Final Batch Loss: 0.08833634108304977\n",
      "Epoch 4966, Loss: 0.3045661859214306, Final Batch Loss: 0.0582081638276577\n",
      "Epoch 4967, Loss: 0.2711278200149536, Final Batch Loss: 0.1032843217253685\n",
      "Epoch 4968, Loss: 0.32479415088891983, Final Batch Loss: 0.13530242443084717\n",
      "Epoch 4969, Loss: 0.2783469781279564, Final Batch Loss: 0.06831467151641846\n",
      "Epoch 4970, Loss: 0.27894720435142517, Final Batch Loss: 0.09010495245456696\n",
      "Epoch 4971, Loss: 0.2818339318037033, Final Batch Loss: 0.12147010117769241\n",
      "Epoch 4972, Loss: 0.29532571136951447, Final Batch Loss: 0.07239922136068344\n",
      "Epoch 4973, Loss: 0.2596636116504669, Final Batch Loss: 0.04738306999206543\n",
      "Epoch 4974, Loss: 0.30927399545907974, Final Batch Loss: 0.07945967465639114\n",
      "Epoch 4975, Loss: 0.40748733282089233, Final Batch Loss: 0.17334498465061188\n",
      "Epoch 4976, Loss: 0.2970519959926605, Final Batch Loss: 0.10965532064437866\n",
      "Epoch 4977, Loss: 0.26272786408662796, Final Batch Loss: 0.10043248534202576\n",
      "Epoch 4978, Loss: 0.3417557254433632, Final Batch Loss: 0.09982756525278091\n",
      "Epoch 4979, Loss: 0.39072152972221375, Final Batch Loss: 0.12466259300708771\n",
      "Epoch 4980, Loss: 0.405348539352417, Final Batch Loss: 0.13905403017997742\n",
      "Epoch 4981, Loss: 0.30114998668432236, Final Batch Loss: 0.06947139650583267\n",
      "Epoch 4982, Loss: 0.4142712876200676, Final Batch Loss: 0.17236271500587463\n",
      "Epoch 4983, Loss: 0.3374125584959984, Final Batch Loss: 0.12228448688983917\n",
      "Epoch 4984, Loss: 0.31715456396341324, Final Batch Loss: 0.11375398188829422\n",
      "Epoch 4985, Loss: 0.255910512059927, Final Batch Loss: 0.0457233302295208\n",
      "Epoch 4986, Loss: 0.2827715128660202, Final Batch Loss: 0.10582618415355682\n",
      "Epoch 4987, Loss: 0.3524531275033951, Final Batch Loss: 0.1102801114320755\n",
      "Epoch 4988, Loss: 0.3487590029835701, Final Batch Loss: 0.12539610266685486\n",
      "Epoch 4989, Loss: 0.3466890975832939, Final Batch Loss: 0.08258835971355438\n",
      "Epoch 4990, Loss: 0.2937907874584198, Final Batch Loss: 0.08657796680927277\n",
      "Epoch 4991, Loss: 0.2654247246682644, Final Batch Loss: 0.05039786919951439\n",
      "Epoch 4992, Loss: 0.2689209282398224, Final Batch Loss: 0.08773459494113922\n",
      "Epoch 4993, Loss: 0.3077642023563385, Final Batch Loss: 0.07668409496545792\n",
      "Epoch 4994, Loss: 0.35447855666279793, Final Batch Loss: 0.04290124401450157\n",
      "Epoch 4995, Loss: 0.2827383056282997, Final Batch Loss: 0.06602788716554642\n",
      "Epoch 4996, Loss: 0.3031657785177231, Final Batch Loss: 0.13508206605911255\n",
      "Epoch 4997, Loss: 0.25578492134809494, Final Batch Loss: 0.06568792462348938\n",
      "Epoch 4998, Loss: 0.30653734505176544, Final Batch Loss: 0.07477834075689316\n",
      "Epoch 4999, Loss: 0.32689446955919266, Final Batch Loss: 0.08455806225538254\n",
      "Epoch 5000, Loss: 0.24734514951705933, Final Batch Loss: 0.0551149845123291\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0  1  0  0  0  0  0  0]\n",
      " [34  1  0  0  0  0  0  0  0]\n",
      " [ 6  1 28  0  0  0  0  0  0]\n",
      " [ 0  0  0 35  0  0  0  0  0]\n",
      " [ 0  0  0  0 35  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 35]\n",
      " [ 0  0  0  1  0  0 34  0  0]\n",
      " [ 0  0  0  0  1  0  0 34  0]\n",
      " [ 0  0  4  0  0  7  0  0 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.45946   0.97143   0.62385        35\n",
      "         1.0    0.50000   0.02857   0.05405        35\n",
      "         2.0    0.84848   0.80000   0.82353        35\n",
      "         3.0    0.97222   1.00000   0.98592        35\n",
      "         4.0    0.97222   1.00000   0.98592        35\n",
      "         5.0    0.00000   0.00000   0.00000        35\n",
      "         6.0    1.00000   0.97143   0.98551        35\n",
      "         7.0    1.00000   0.97143   0.98551        35\n",
      "         8.0    0.40678   0.68571   0.51064        35\n",
      "\n",
      "    accuracy                        0.71429       315\n",
      "   macro avg    0.68435   0.71429   0.66166       315\n",
      "weighted avg    0.68435   0.71429   0.66166       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701266666666666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET RID OF 2, 5, 8 for JUST DYNAMIC\n",
    "(0.62385+0.05405+0.98592+0.98592+0.98551+0.98551)/6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
