{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_features = ['43 tGravityAcc-mean()-Z',\n",
    "#  '52 tGravityAcc-max()-Z',\n",
    "#  '54 tGravityAcc-min()-Y',\n",
    "#  '55 tGravityAcc-min()-Z',\n",
    "#  '56 tGravityAcc-sma()',\n",
    "#  '59 tGravityAcc-energy()-Z',\n",
    "#  '125 tBodyGyro-std()-Y',\n",
    "#  '128 tBodyGyro-mad()-Y',\n",
    "#  '138 tBodyGyro-energy()-Y',\n",
    "#  '425 fBodyGyro-mean()-Y',\n",
    "#  '441 fBodyGyro-energy()-Y',\n",
    "#  '475 fBodyGyro-bandsEnergy()-1,8',\n",
    "#  '483 fBodyGyro-bandsEnergy()-1,16',\n",
    "#  '559 angle(X,gravityMean)',\n",
    "#  '561 angle(Z,gravityMean)']\n",
    "\n",
    "# act_features = ['4 tBodyAcc-std()-X',\n",
    "#  '10 tBodyAcc-max()-X',\n",
    "#  '202 tBodyAccMag-std()',\n",
    "#  '215 tGravityAccMag-std()',\n",
    "#  '269 fBodyAcc-std()-X',\n",
    "#  '282 fBodyAcc-energy()-X',\n",
    "#  '303 fBodyAcc-bandsEnergy()-1,8',\n",
    "#  '311 fBodyAcc-bandsEnergy()-1,16',\n",
    "#  '315 fBodyAcc-bandsEnergy()-1,24',\n",
    "#  '504 fBodyAccMag-std()',\n",
    "#  '505 fBodyAccMag-mad()',\n",
    "#  '509 fBodyAccMag-energy()']\n",
    "\n",
    "# input_shape = len(act_features) + len(sub_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>275 fBodyAcc-max()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993756</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999372</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998158</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997404</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999277</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318185</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332146</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160368</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147421</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417612</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  275 fBodyAcc-max()-X  \\\n",
       "0                 -0.976623              -0.976353  ...             -0.993756   \n",
       "1                 -0.989046              -0.989038  ...             -0.999372   \n",
       "2                 -0.993552              -0.994122  ...             -0.998158   \n",
       "3                 -0.992407              -0.993142  ...             -0.997404   \n",
       "4                 -0.992378              -0.992542  ...             -0.999277   \n",
       "...                     ...                    ...  ...                   ...   \n",
       "7347               0.084878               0.065142  ...             -0.318185   \n",
       "7348               0.098249               0.091791  ...             -0.332146   \n",
       "7349               0.185902               0.170686  ...             -0.160368   \n",
       "7350               0.190360               0.178939  ...             -0.147421   \n",
       "7351               0.022216              -0.073681  ...             -0.417612   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  \n",
       "0                    -0.998285        1  \n",
       "1                    -0.999472        1  \n",
       "2                    -0.999807        1  \n",
       "3                    -0.999770        1  \n",
       "4                    -0.999873        1  \n",
       "...                        ...      ...  \n",
       "7347                 -0.584282       30  \n",
       "7348                 -0.632536       30  \n",
       "7349                 -0.641170       30  \n",
       "7350                 -0.663579       30  \n",
       "7351                 -0.698087       30  \n",
       "\n",
       "[7352 rows x 47 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[(X_train['Subject'] == 1) | (X_train['Subject'] == 3) | (X_train['Subject'] == 5)]\n",
    "X_train = X_train.iloc[:,:-1].values\n",
    "\n",
    "y_train = y_train[(y_train['Subject'] == 1) | (y_train['Subject'] == 3) | (y_train['Subject'] == 5)]\n",
    "y_train = y_train.values\n",
    "y_train = y_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 6000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.541415810585022, Final Batch Loss: 1.1472598314285278\n",
      "Epoch 2, Loss: 4.529441595077515, Final Batch Loss: 1.1453378200531006\n",
      "Epoch 3, Loss: 4.474188446998596, Final Batch Loss: 1.0903522968292236\n",
      "Epoch 4, Loss: 4.488919258117676, Final Batch Loss: 1.1367367506027222\n",
      "Epoch 5, Loss: 4.45430064201355, Final Batch Loss: 1.1084882020950317\n",
      "Epoch 6, Loss: 4.4289140701293945, Final Batch Loss: 1.1019666194915771\n",
      "Epoch 7, Loss: 4.4120999574661255, Final Batch Loss: 1.106124758720398\n",
      "Epoch 8, Loss: 4.39593780040741, Final Batch Loss: 1.0925155878067017\n",
      "Epoch 9, Loss: 4.366827845573425, Final Batch Loss: 1.0901483297348022\n",
      "Epoch 10, Loss: 4.374091386795044, Final Batch Loss: 1.089911937713623\n",
      "Epoch 11, Loss: 4.367084860801697, Final Batch Loss: 1.0793025493621826\n",
      "Epoch 12, Loss: 4.340619683265686, Final Batch Loss: 1.0729845762252808\n",
      "Epoch 13, Loss: 4.339486837387085, Final Batch Loss: 1.0754339694976807\n",
      "Epoch 14, Loss: 4.313631534576416, Final Batch Loss: 1.0798993110656738\n",
      "Epoch 15, Loss: 4.281504034996033, Final Batch Loss: 1.046101689338684\n",
      "Epoch 16, Loss: 4.289930939674377, Final Batch Loss: 1.0749040842056274\n",
      "Epoch 17, Loss: 4.253805875778198, Final Batch Loss: 1.05978524684906\n",
      "Epoch 18, Loss: 4.246626257896423, Final Batch Loss: 1.0642552375793457\n",
      "Epoch 19, Loss: 4.2181700468063354, Final Batch Loss: 1.0534443855285645\n",
      "Epoch 20, Loss: 4.197941780090332, Final Batch Loss: 1.075279712677002\n",
      "Epoch 21, Loss: 4.12871253490448, Final Batch Loss: 1.0273830890655518\n",
      "Epoch 22, Loss: 4.063006401062012, Final Batch Loss: 1.011527419090271\n",
      "Epoch 23, Loss: 4.024816453456879, Final Batch Loss: 0.9963024258613586\n",
      "Epoch 24, Loss: 3.9685811400413513, Final Batch Loss: 0.9996924996376038\n",
      "Epoch 25, Loss: 3.8156339526176453, Final Batch Loss: 0.9312624335289001\n",
      "Epoch 26, Loss: 3.7695499658584595, Final Batch Loss: 0.9198379516601562\n",
      "Epoch 27, Loss: 3.7124897837638855, Final Batch Loss: 0.9144753217697144\n",
      "Epoch 28, Loss: 3.610483169555664, Final Batch Loss: 0.8813906908035278\n",
      "Epoch 29, Loss: 3.511532485485077, Final Batch Loss: 0.8613104224205017\n",
      "Epoch 30, Loss: 3.482297420501709, Final Batch Loss: 0.8624418377876282\n",
      "Epoch 31, Loss: 3.4598288536071777, Final Batch Loss: 0.9018353223800659\n",
      "Epoch 32, Loss: 3.3929344415664673, Final Batch Loss: 0.880825400352478\n",
      "Epoch 33, Loss: 3.284240961074829, Final Batch Loss: 0.8227927088737488\n",
      "Epoch 34, Loss: 3.2625163793563843, Final Batch Loss: 0.804526686668396\n",
      "Epoch 35, Loss: 3.145717442035675, Final Batch Loss: 0.7447648644447327\n",
      "Epoch 36, Loss: 3.109859883785248, Final Batch Loss: 0.8124920129776001\n",
      "Epoch 37, Loss: 3.093441069126129, Final Batch Loss: 0.8079351782798767\n",
      "Epoch 38, Loss: 2.983565390110016, Final Batch Loss: 0.7066794633865356\n",
      "Epoch 39, Loss: 2.9253225326538086, Final Batch Loss: 0.772179901599884\n",
      "Epoch 40, Loss: 2.776553153991699, Final Batch Loss: 0.6091244220733643\n",
      "Epoch 41, Loss: 2.8323261737823486, Final Batch Loss: 0.7540180683135986\n",
      "Epoch 42, Loss: 2.720739781856537, Final Batch Loss: 0.690784752368927\n",
      "Epoch 43, Loss: 2.545785427093506, Final Batch Loss: 0.5490593910217285\n",
      "Epoch 44, Loss: 2.6914417147636414, Final Batch Loss: 0.7745482325553894\n",
      "Epoch 45, Loss: 2.4769874215126038, Final Batch Loss: 0.5479301810264587\n",
      "Epoch 46, Loss: 2.4471927285194397, Final Batch Loss: 0.5669647455215454\n",
      "Epoch 47, Loss: 2.477175712585449, Final Batch Loss: 0.622683584690094\n",
      "Epoch 48, Loss: 2.3448087573051453, Final Batch Loss: 0.5860359072685242\n",
      "Epoch 49, Loss: 2.427385449409485, Final Batch Loss: 0.6696997284889221\n",
      "Epoch 50, Loss: 2.307138681411743, Final Batch Loss: 0.5349167585372925\n",
      "Epoch 51, Loss: 2.3571298718452454, Final Batch Loss: 0.6186547875404358\n",
      "Epoch 52, Loss: 2.2709912061691284, Final Batch Loss: 0.5582266449928284\n",
      "Epoch 53, Loss: 2.1175827085971832, Final Batch Loss: 0.4826721251010895\n",
      "Epoch 54, Loss: 2.2279896438121796, Final Batch Loss: 0.6477663516998291\n",
      "Epoch 55, Loss: 2.2140883207321167, Final Batch Loss: 0.5697707533836365\n",
      "Epoch 56, Loss: 2.031171292066574, Final Batch Loss: 0.4315844476222992\n",
      "Epoch 57, Loss: 2.070410668849945, Final Batch Loss: 0.458455890417099\n",
      "Epoch 58, Loss: 2.1415508091449738, Final Batch Loss: 0.6011788249015808\n",
      "Epoch 59, Loss: 2.101421684026718, Final Batch Loss: 0.5840373039245605\n",
      "Epoch 60, Loss: 1.972423940896988, Final Batch Loss: 0.41645845770835876\n",
      "Epoch 61, Loss: 2.0641351640224457, Final Batch Loss: 0.5460036993026733\n",
      "Epoch 62, Loss: 1.9535247385501862, Final Batch Loss: 0.4694381654262543\n",
      "Epoch 63, Loss: 1.925855278968811, Final Batch Loss: 0.40720200538635254\n",
      "Epoch 64, Loss: 1.997900277376175, Final Batch Loss: 0.5311402678489685\n",
      "Epoch 65, Loss: 1.9600797891616821, Final Batch Loss: 0.47551533579826355\n",
      "Epoch 66, Loss: 1.8793442249298096, Final Batch Loss: 0.48849186301231384\n",
      "Epoch 67, Loss: 2.015013247728348, Final Batch Loss: 0.5125115513801575\n",
      "Epoch 68, Loss: 1.9109183549880981, Final Batch Loss: 0.35483068227767944\n",
      "Epoch 69, Loss: 1.9106764197349548, Final Batch Loss: 0.5486867427825928\n",
      "Epoch 70, Loss: 1.9058809876441956, Final Batch Loss: 0.5003791451454163\n",
      "Epoch 71, Loss: 1.8749566078186035, Final Batch Loss: 0.37341558933258057\n",
      "Epoch 72, Loss: 1.9099667072296143, Final Batch Loss: 0.4887683093547821\n",
      "Epoch 73, Loss: 1.8137696981430054, Final Batch Loss: 0.45124050974845886\n",
      "Epoch 74, Loss: 1.878814458847046, Final Batch Loss: 0.4754372835159302\n",
      "Epoch 75, Loss: 1.7846247553825378, Final Batch Loss: 0.3957560956478119\n",
      "Epoch 76, Loss: 1.7637414932250977, Final Batch Loss: 0.4295291304588318\n",
      "Epoch 77, Loss: 1.8612106144428253, Final Batch Loss: 0.44253283739089966\n",
      "Epoch 78, Loss: 1.8704897463321686, Final Batch Loss: 0.5210024118423462\n",
      "Epoch 79, Loss: 1.8618415892124176, Final Batch Loss: 0.4766818881034851\n",
      "Epoch 80, Loss: 1.782481700181961, Final Batch Loss: 0.46246206760406494\n",
      "Epoch 81, Loss: 1.888807475566864, Final Batch Loss: 0.5567744970321655\n",
      "Epoch 82, Loss: 1.7644964754581451, Final Batch Loss: 0.41786718368530273\n",
      "Epoch 83, Loss: 1.710306704044342, Final Batch Loss: 0.37291157245635986\n",
      "Epoch 84, Loss: 1.6825112104415894, Final Batch Loss: 0.3874087631702423\n",
      "Epoch 85, Loss: 1.8470312058925629, Final Batch Loss: 0.5856519341468811\n",
      "Epoch 86, Loss: 1.773283839225769, Final Batch Loss: 0.47439464926719666\n",
      "Epoch 87, Loss: 1.5847128331661224, Final Batch Loss: 0.34656423330307007\n",
      "Epoch 88, Loss: 1.7731595039367676, Final Batch Loss: 0.4532746970653534\n",
      "Epoch 89, Loss: 1.7031928896903992, Final Batch Loss: 0.5035783648490906\n",
      "Epoch 90, Loss: 1.6156498193740845, Final Batch Loss: 0.41213175654411316\n",
      "Epoch 91, Loss: 1.6888661086559296, Final Batch Loss: 0.4132576286792755\n",
      "Epoch 92, Loss: 1.738671749830246, Final Batch Loss: 0.45701682567596436\n",
      "Epoch 93, Loss: 1.6062668859958649, Final Batch Loss: 0.3845135271549225\n",
      "Epoch 94, Loss: 1.6459278464317322, Final Batch Loss: 0.4161514937877655\n",
      "Epoch 95, Loss: 1.6948612928390503, Final Batch Loss: 0.3670710027217865\n",
      "Epoch 96, Loss: 1.6368958353996277, Final Batch Loss: 0.40714696049690247\n",
      "Epoch 97, Loss: 1.6865049302577972, Final Batch Loss: 0.4171186089515686\n",
      "Epoch 98, Loss: 1.5338942110538483, Final Batch Loss: 0.30978044867515564\n",
      "Epoch 99, Loss: 1.6152247786521912, Final Batch Loss: 0.4421781897544861\n",
      "Epoch 100, Loss: 1.695319950580597, Final Batch Loss: 0.517300009727478\n",
      "Epoch 101, Loss: 1.6136367321014404, Final Batch Loss: 0.3652437627315521\n",
      "Epoch 102, Loss: 1.679233580827713, Final Batch Loss: 0.45500069856643677\n",
      "Epoch 103, Loss: 1.6015487909317017, Final Batch Loss: 0.3887728154659271\n",
      "Epoch 104, Loss: 1.6193974316120148, Final Batch Loss: 0.40863800048828125\n",
      "Epoch 105, Loss: 1.675770103931427, Final Batch Loss: 0.5139867067337036\n",
      "Epoch 106, Loss: 1.702507197856903, Final Batch Loss: 0.5092924237251282\n",
      "Epoch 107, Loss: 1.5727174878120422, Final Batch Loss: 0.3458734154701233\n",
      "Epoch 108, Loss: 1.6410740911960602, Final Batch Loss: 0.4594891667366028\n",
      "Epoch 109, Loss: 1.5659885704517365, Final Batch Loss: 0.33749595284461975\n",
      "Epoch 110, Loss: 1.5746058821678162, Final Batch Loss: 0.4311766028404236\n",
      "Epoch 111, Loss: 1.5528112649917603, Final Batch Loss: 0.3764667212963104\n",
      "Epoch 112, Loss: 1.5924978852272034, Final Batch Loss: 0.3807410001754761\n",
      "Epoch 113, Loss: 1.6132982671260834, Final Batch Loss: 0.4449233114719391\n",
      "Epoch 114, Loss: 1.5335545241832733, Final Batch Loss: 0.4011673033237457\n",
      "Epoch 115, Loss: 1.5891301929950714, Final Batch Loss: 0.35041317343711853\n",
      "Epoch 116, Loss: 1.6252778470516205, Final Batch Loss: 0.5122653245925903\n",
      "Epoch 117, Loss: 1.5560170710086823, Final Batch Loss: 0.4562545716762543\n",
      "Epoch 118, Loss: 1.5197056233882904, Final Batch Loss: 0.2951692044734955\n",
      "Epoch 119, Loss: 1.5513978600502014, Final Batch Loss: 0.40301668643951416\n",
      "Epoch 120, Loss: 1.5291244387626648, Final Batch Loss: 0.3864356577396393\n",
      "Epoch 121, Loss: 1.5622933208942413, Final Batch Loss: 0.3842717409133911\n",
      "Epoch 122, Loss: 1.4547724723815918, Final Batch Loss: 0.33813509345054626\n",
      "Epoch 123, Loss: 1.5488508939743042, Final Batch Loss: 0.4117688238620758\n",
      "Epoch 124, Loss: 1.5356290638446808, Final Batch Loss: 0.37067875266075134\n",
      "Epoch 125, Loss: 1.5565849840641022, Final Batch Loss: 0.47183096408843994\n",
      "Epoch 126, Loss: 1.520561307668686, Final Batch Loss: 0.37201038002967834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: 1.41526859998703, Final Batch Loss: 0.3053182363510132\n",
      "Epoch 128, Loss: 1.549314171075821, Final Batch Loss: 0.4225519597530365\n",
      "Epoch 129, Loss: 1.4812469482421875, Final Batch Loss: 0.3625519573688507\n",
      "Epoch 130, Loss: 1.4026625454425812, Final Batch Loss: 0.2601211965084076\n",
      "Epoch 131, Loss: 1.4763829708099365, Final Batch Loss: 0.3582393527030945\n",
      "Epoch 132, Loss: 1.5425027906894684, Final Batch Loss: 0.4359642565250397\n",
      "Epoch 133, Loss: 1.4171437323093414, Final Batch Loss: 0.33423012495040894\n",
      "Epoch 134, Loss: 1.485193818807602, Final Batch Loss: 0.3856531083583832\n",
      "Epoch 135, Loss: 1.4934208691120148, Final Batch Loss: 0.35247594118118286\n",
      "Epoch 136, Loss: 1.4828131198883057, Final Batch Loss: 0.3113672137260437\n",
      "Epoch 137, Loss: 1.5495238900184631, Final Batch Loss: 0.5048686861991882\n",
      "Epoch 138, Loss: 1.4951315820217133, Final Batch Loss: 0.4024078845977783\n",
      "Epoch 139, Loss: 1.3616831600666046, Final Batch Loss: 0.273358553647995\n",
      "Epoch 140, Loss: 1.4427745938301086, Final Batch Loss: 0.4084279537200928\n",
      "Epoch 141, Loss: 1.4313731491565704, Final Batch Loss: 0.3775545358657837\n",
      "Epoch 142, Loss: 1.3768316507339478, Final Batch Loss: 0.27096670866012573\n",
      "Epoch 143, Loss: 1.3509560227394104, Final Batch Loss: 0.2709540128707886\n",
      "Epoch 144, Loss: 1.4690890312194824, Final Batch Loss: 0.42130064964294434\n",
      "Epoch 145, Loss: 1.4555442929267883, Final Batch Loss: 0.40236788988113403\n",
      "Epoch 146, Loss: 1.4401509761810303, Final Batch Loss: 0.3767023980617523\n",
      "Epoch 147, Loss: 1.374396800994873, Final Batch Loss: 0.3621838688850403\n",
      "Epoch 148, Loss: 1.5105583369731903, Final Batch Loss: 0.5450240969657898\n",
      "Epoch 149, Loss: 1.4060302078723907, Final Batch Loss: 0.3860504627227783\n",
      "Epoch 150, Loss: 1.3583133518695831, Final Batch Loss: 0.27423277497291565\n",
      "Epoch 151, Loss: 1.2910243719816208, Final Batch Loss: 0.21208088099956512\n",
      "Epoch 152, Loss: 1.320323795080185, Final Batch Loss: 0.3023494780063629\n",
      "Epoch 153, Loss: 1.4542250037193298, Final Batch Loss: 0.45309269428253174\n",
      "Epoch 154, Loss: 1.393884152173996, Final Batch Loss: 0.390483558177948\n",
      "Epoch 155, Loss: 1.3679817914962769, Final Batch Loss: 0.2808172404766083\n",
      "Epoch 156, Loss: 1.4507531225681305, Final Batch Loss: 0.46465712785720825\n",
      "Epoch 157, Loss: 1.385505050420761, Final Batch Loss: 0.343967467546463\n",
      "Epoch 158, Loss: 1.4196139872074127, Final Batch Loss: 0.38100793957710266\n",
      "Epoch 159, Loss: 1.349621832370758, Final Batch Loss: 0.33047130703926086\n",
      "Epoch 160, Loss: 1.3221262991428375, Final Batch Loss: 0.2702160179615021\n",
      "Epoch 161, Loss: 1.355113834142685, Final Batch Loss: 0.33505138754844666\n",
      "Epoch 162, Loss: 1.237998753786087, Final Batch Loss: 0.289856880903244\n",
      "Epoch 163, Loss: 1.2567486464977264, Final Batch Loss: 0.2313786745071411\n",
      "Epoch 164, Loss: 1.3113276362419128, Final Batch Loss: 0.3033285140991211\n",
      "Epoch 165, Loss: 1.303836852312088, Final Batch Loss: 0.32904553413391113\n",
      "Epoch 166, Loss: 1.302677035331726, Final Batch Loss: 0.3310856521129608\n",
      "Epoch 167, Loss: 1.299345076084137, Final Batch Loss: 0.3288794159889221\n",
      "Epoch 168, Loss: 1.2511928975582123, Final Batch Loss: 0.275757372379303\n",
      "Epoch 169, Loss: 1.2932098507881165, Final Batch Loss: 0.34875521063804626\n",
      "Epoch 170, Loss: 1.3944292664527893, Final Batch Loss: 0.43967530131340027\n",
      "Epoch 171, Loss: 1.2912605106830597, Final Batch Loss: 0.3201885521411896\n",
      "Epoch 172, Loss: 1.242956280708313, Final Batch Loss: 0.2370814085006714\n",
      "Epoch 173, Loss: 1.2569826245307922, Final Batch Loss: 0.28876593708992004\n",
      "Epoch 174, Loss: 1.2190779745578766, Final Batch Loss: 0.2925085425376892\n",
      "Epoch 175, Loss: 1.2806478142738342, Final Batch Loss: 0.2845328450202942\n",
      "Epoch 176, Loss: 1.1983895003795624, Final Batch Loss: 0.2644893527030945\n",
      "Epoch 177, Loss: 1.199048399925232, Final Batch Loss: 0.2795671820640564\n",
      "Epoch 178, Loss: 1.2233460545539856, Final Batch Loss: 0.2646462917327881\n",
      "Epoch 179, Loss: 1.261020228266716, Final Batch Loss: 0.2347247153520584\n",
      "Epoch 180, Loss: 1.3837840557098389, Final Batch Loss: 0.42101743817329407\n",
      "Epoch 181, Loss: 1.2080735862255096, Final Batch Loss: 0.2547275125980377\n",
      "Epoch 182, Loss: 1.2735818028450012, Final Batch Loss: 0.32238906621932983\n",
      "Epoch 183, Loss: 1.2706125378608704, Final Batch Loss: 0.30764466524124146\n",
      "Epoch 184, Loss: 1.2356929183006287, Final Batch Loss: 0.3180687725543976\n",
      "Epoch 185, Loss: 1.232560396194458, Final Batch Loss: 0.27930039167404175\n",
      "Epoch 186, Loss: 1.2539169490337372, Final Batch Loss: 0.36134740710258484\n",
      "Epoch 187, Loss: 1.2310268878936768, Final Batch Loss: 0.3259059488773346\n",
      "Epoch 188, Loss: 1.2150506675243378, Final Batch Loss: 0.2986800968647003\n",
      "Epoch 189, Loss: 1.2510949969291687, Final Batch Loss: 0.3893009126186371\n",
      "Epoch 190, Loss: 1.1743526607751846, Final Batch Loss: 0.18747355043888092\n",
      "Epoch 191, Loss: 1.252286672592163, Final Batch Loss: 0.32825013995170593\n",
      "Epoch 192, Loss: 1.1740672290325165, Final Batch Loss: 0.27742207050323486\n",
      "Epoch 193, Loss: 1.2284934222698212, Final Batch Loss: 0.2958580255508423\n",
      "Epoch 194, Loss: 1.2168265581130981, Final Batch Loss: 0.31708934903144836\n",
      "Epoch 195, Loss: 1.2080479562282562, Final Batch Loss: 0.27136746048927307\n",
      "Epoch 196, Loss: 1.1787924766540527, Final Batch Loss: 0.310690313577652\n",
      "Epoch 197, Loss: 1.158901035785675, Final Batch Loss: 0.29222947359085083\n",
      "Epoch 198, Loss: 1.1691703796386719, Final Batch Loss: 0.34147706627845764\n",
      "Epoch 199, Loss: 1.2398723363876343, Final Batch Loss: 0.3377639651298523\n",
      "Epoch 200, Loss: 1.1390559673309326, Final Batch Loss: 0.23094549775123596\n",
      "Epoch 201, Loss: 1.1473964750766754, Final Batch Loss: 0.24538540840148926\n",
      "Epoch 202, Loss: 1.1424998044967651, Final Batch Loss: 0.3133052885532379\n",
      "Epoch 203, Loss: 1.1680137515068054, Final Batch Loss: 0.30781471729278564\n",
      "Epoch 204, Loss: 1.1597066521644592, Final Batch Loss: 0.3462376594543457\n",
      "Epoch 205, Loss: 1.1709412932395935, Final Batch Loss: 0.3135633170604706\n",
      "Epoch 206, Loss: 1.1862162053585052, Final Batch Loss: 0.3036230504512787\n",
      "Epoch 207, Loss: 1.2206970155239105, Final Batch Loss: 0.4210561513900757\n",
      "Epoch 208, Loss: 1.1963286399841309, Final Batch Loss: 0.35078907012939453\n",
      "Epoch 209, Loss: 1.2022579908370972, Final Batch Loss: 0.3651166260242462\n",
      "Epoch 210, Loss: 1.1771128177642822, Final Batch Loss: 0.3021615147590637\n",
      "Epoch 211, Loss: 1.1685625463724136, Final Batch Loss: 0.4015522599220276\n",
      "Epoch 212, Loss: 1.1515351235866547, Final Batch Loss: 0.28462982177734375\n",
      "Epoch 213, Loss: 1.2437421679496765, Final Batch Loss: 0.33952751755714417\n",
      "Epoch 214, Loss: 1.1319479793310165, Final Batch Loss: 0.2300737351179123\n",
      "Epoch 215, Loss: 1.118354171514511, Final Batch Loss: 0.224214106798172\n",
      "Epoch 216, Loss: 1.2030922770500183, Final Batch Loss: 0.327541321516037\n",
      "Epoch 217, Loss: 1.129518747329712, Final Batch Loss: 0.2566295862197876\n",
      "Epoch 218, Loss: 1.2211425304412842, Final Batch Loss: 0.3865358531475067\n",
      "Epoch 219, Loss: 1.1294488161802292, Final Batch Loss: 0.3045312166213989\n",
      "Epoch 220, Loss: 1.1312836855649948, Final Batch Loss: 0.2773003578186035\n",
      "Epoch 221, Loss: 1.202506572008133, Final Batch Loss: 0.37116074562072754\n",
      "Epoch 222, Loss: 1.060124233365059, Final Batch Loss: 0.22236748039722443\n",
      "Epoch 223, Loss: 1.1390844583511353, Final Batch Loss: 0.26911136507987976\n",
      "Epoch 224, Loss: 1.2308928221464157, Final Batch Loss: 0.3707391917705536\n",
      "Epoch 225, Loss: 1.0921847969293594, Final Batch Loss: 0.21980474889278412\n",
      "Epoch 226, Loss: 1.170005515217781, Final Batch Loss: 0.30647706985473633\n",
      "Epoch 227, Loss: 1.1717110872268677, Final Batch Loss: 0.3369775414466858\n",
      "Epoch 228, Loss: 1.2217282354831696, Final Batch Loss: 0.35192036628723145\n",
      "Epoch 229, Loss: 1.148880273103714, Final Batch Loss: 0.2754579782485962\n",
      "Epoch 230, Loss: 1.0972585082054138, Final Batch Loss: 0.21201661229133606\n",
      "Epoch 231, Loss: 1.1272089183330536, Final Batch Loss: 0.28655681014060974\n",
      "Epoch 232, Loss: 1.1396155655384064, Final Batch Loss: 0.2946203351020813\n",
      "Epoch 233, Loss: 1.0583688616752625, Final Batch Loss: 0.26403316855430603\n",
      "Epoch 234, Loss: 1.1344536244869232, Final Batch Loss: 0.3709357678890228\n",
      "Epoch 235, Loss: 1.1155465990304947, Final Batch Loss: 0.3353325128555298\n",
      "Epoch 236, Loss: 1.1266926974058151, Final Batch Loss: 0.2432072013616562\n",
      "Epoch 237, Loss: 1.1718294620513916, Final Batch Loss: 0.26696455478668213\n",
      "Epoch 238, Loss: 1.0774529874324799, Final Batch Loss: 0.26547107100486755\n",
      "Epoch 239, Loss: 1.0273619145154953, Final Batch Loss: 0.17993603646755219\n",
      "Epoch 240, Loss: 1.0705505907535553, Final Batch Loss: 0.2516094744205475\n",
      "Epoch 241, Loss: 1.0418810099363327, Final Batch Loss: 0.20262762904167175\n",
      "Epoch 242, Loss: 1.1568213105201721, Final Batch Loss: 0.32910463213920593\n",
      "Epoch 243, Loss: 1.120415061712265, Final Batch Loss: 0.3264552652835846\n",
      "Epoch 244, Loss: 1.0718218088150024, Final Batch Loss: 0.28034666180610657\n",
      "Epoch 245, Loss: 1.0411930531263351, Final Batch Loss: 0.3125966787338257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246, Loss: 1.026938557624817, Final Batch Loss: 0.23699188232421875\n",
      "Epoch 247, Loss: 1.076637789607048, Final Batch Loss: 0.28577151894569397\n",
      "Epoch 248, Loss: 1.163891762495041, Final Batch Loss: 0.31891176104545593\n",
      "Epoch 249, Loss: 1.1598665118217468, Final Batch Loss: 0.28209540247917175\n",
      "Epoch 250, Loss: 1.0981326252222061, Final Batch Loss: 0.3547370135784149\n",
      "Epoch 251, Loss: 1.0077074319124222, Final Batch Loss: 0.23815211653709412\n",
      "Epoch 252, Loss: 1.0844855308532715, Final Batch Loss: 0.2753636837005615\n",
      "Epoch 253, Loss: 1.0843344926834106, Final Batch Loss: 0.2892037630081177\n",
      "Epoch 254, Loss: 1.0795336067676544, Final Batch Loss: 0.2259281873703003\n",
      "Epoch 255, Loss: 1.0032567232847214, Final Batch Loss: 0.2501486539840698\n",
      "Epoch 256, Loss: 1.0351398438215256, Final Batch Loss: 0.19259421527385712\n",
      "Epoch 257, Loss: 1.0062847584486008, Final Batch Loss: 0.22555780410766602\n",
      "Epoch 258, Loss: 1.0250819623470306, Final Batch Loss: 0.30208709836006165\n",
      "Epoch 259, Loss: 1.0279211699962616, Final Batch Loss: 0.2573845088481903\n",
      "Epoch 260, Loss: 1.0359578281641006, Final Batch Loss: 0.2217741161584854\n",
      "Epoch 261, Loss: 0.999100461602211, Final Batch Loss: 0.23053906857967377\n",
      "Epoch 262, Loss: 0.9858794212341309, Final Batch Loss: 0.158563494682312\n",
      "Epoch 263, Loss: 0.9971440136432648, Final Batch Loss: 0.24916638433933258\n",
      "Epoch 264, Loss: 0.8904207944869995, Final Batch Loss: 0.1413687765598297\n",
      "Epoch 265, Loss: 1.072400376200676, Final Batch Loss: 0.3267195224761963\n",
      "Epoch 266, Loss: 1.063750147819519, Final Batch Loss: 0.23310863971710205\n",
      "Epoch 267, Loss: 0.9460445642471313, Final Batch Loss: 0.200871080160141\n",
      "Epoch 268, Loss: 0.9387743324041367, Final Batch Loss: 0.16408011317253113\n",
      "Epoch 269, Loss: 1.078932300209999, Final Batch Loss: 0.3009890913963318\n",
      "Epoch 270, Loss: 1.1485555171966553, Final Batch Loss: 0.2885754108428955\n",
      "Epoch 271, Loss: 1.0309611111879349, Final Batch Loss: 0.2444143146276474\n",
      "Epoch 272, Loss: 1.0911061018705368, Final Batch Loss: 0.32740798592567444\n",
      "Epoch 273, Loss: 1.0946966409683228, Final Batch Loss: 0.2633940875530243\n",
      "Epoch 274, Loss: 1.023854836821556, Final Batch Loss: 0.23515310883522034\n",
      "Epoch 275, Loss: 1.0738337934017181, Final Batch Loss: 0.3372654616832733\n",
      "Epoch 276, Loss: 1.0612186789512634, Final Batch Loss: 0.28009000420570374\n",
      "Epoch 277, Loss: 1.0326128602027893, Final Batch Loss: 0.2601686716079712\n",
      "Epoch 278, Loss: 1.1603648215532303, Final Batch Loss: 0.39787840843200684\n",
      "Epoch 279, Loss: 0.9433029890060425, Final Batch Loss: 0.15300270915031433\n",
      "Epoch 280, Loss: 0.952534556388855, Final Batch Loss: 0.23920390009880066\n",
      "Epoch 281, Loss: 1.011685460805893, Final Batch Loss: 0.25069546699523926\n",
      "Epoch 282, Loss: 1.1542308032512665, Final Batch Loss: 0.3716689348220825\n",
      "Epoch 283, Loss: 1.025344267487526, Final Batch Loss: 0.24950163066387177\n",
      "Epoch 284, Loss: 1.0440184473991394, Final Batch Loss: 0.2491847723722458\n",
      "Epoch 285, Loss: 1.0836565643548965, Final Batch Loss: 0.3258286118507385\n",
      "Epoch 286, Loss: 0.9772416949272156, Final Batch Loss: 0.2775132358074188\n",
      "Epoch 287, Loss: 1.1244112104177475, Final Batch Loss: 0.3327222168445587\n",
      "Epoch 288, Loss: 1.0551612973213196, Final Batch Loss: 0.2762761116027832\n",
      "Epoch 289, Loss: 0.9999522864818573, Final Batch Loss: 0.2680179178714752\n",
      "Epoch 290, Loss: 1.0451213121414185, Final Batch Loss: 0.30435308814048767\n",
      "Epoch 291, Loss: 1.0269072949886322, Final Batch Loss: 0.23331721127033234\n",
      "Epoch 292, Loss: 1.046852022409439, Final Batch Loss: 0.32035428285598755\n",
      "Epoch 293, Loss: 1.0471753478050232, Final Batch Loss: 0.27013668417930603\n",
      "Epoch 294, Loss: 0.9711252748966217, Final Batch Loss: 0.24230538308620453\n",
      "Epoch 295, Loss: 0.9927798062562943, Final Batch Loss: 0.2399299144744873\n",
      "Epoch 296, Loss: 0.9736524522304535, Final Batch Loss: 0.22253485023975372\n",
      "Epoch 297, Loss: 0.9388021230697632, Final Batch Loss: 0.18094110488891602\n",
      "Epoch 298, Loss: 0.9604370445013046, Final Batch Loss: 0.2971690595149994\n",
      "Epoch 299, Loss: 1.0221906900405884, Final Batch Loss: 0.29286810755729675\n",
      "Epoch 300, Loss: 1.0219558477401733, Final Batch Loss: 0.2681552767753601\n",
      "Epoch 301, Loss: 0.8808998465538025, Final Batch Loss: 0.2242252677679062\n",
      "Epoch 302, Loss: 0.907275453209877, Final Batch Loss: 0.22048673033714294\n",
      "Epoch 303, Loss: 0.9943388104438782, Final Batch Loss: 0.3112296462059021\n",
      "Epoch 304, Loss: 0.9448378086090088, Final Batch Loss: 0.27846139669418335\n",
      "Epoch 305, Loss: 1.0042942464351654, Final Batch Loss: 0.2803172469139099\n",
      "Epoch 306, Loss: 0.9280411750078201, Final Batch Loss: 0.23290221393108368\n",
      "Epoch 307, Loss: 0.9809183031320572, Final Batch Loss: 0.27056851983070374\n",
      "Epoch 308, Loss: 0.9368973970413208, Final Batch Loss: 0.2287856936454773\n",
      "Epoch 309, Loss: 0.9079069495201111, Final Batch Loss: 0.16868215799331665\n",
      "Epoch 310, Loss: 1.010241448879242, Final Batch Loss: 0.2657858431339264\n",
      "Epoch 311, Loss: 0.9149964451789856, Final Batch Loss: 0.2668221890926361\n",
      "Epoch 312, Loss: 0.9315670281648636, Final Batch Loss: 0.19305655360221863\n",
      "Epoch 313, Loss: 0.8958730846643448, Final Batch Loss: 0.2229328155517578\n",
      "Epoch 314, Loss: 0.9461300224065781, Final Batch Loss: 0.2460404485464096\n",
      "Epoch 315, Loss: 0.95114366710186, Final Batch Loss: 0.195978581905365\n",
      "Epoch 316, Loss: 0.9484108537435532, Final Batch Loss: 0.20426012575626373\n",
      "Epoch 317, Loss: 0.9997631311416626, Final Batch Loss: 0.2903509736061096\n",
      "Epoch 318, Loss: 0.9541315585374832, Final Batch Loss: 0.22000250220298767\n",
      "Epoch 319, Loss: 0.9086492955684662, Final Batch Loss: 0.234543114900589\n",
      "Epoch 320, Loss: 0.9359066337347031, Final Batch Loss: 0.24166293442249298\n",
      "Epoch 321, Loss: 0.8150168210268021, Final Batch Loss: 0.13266554474830627\n",
      "Epoch 322, Loss: 0.9493332356214523, Final Batch Loss: 0.28935569524765015\n",
      "Epoch 323, Loss: 0.8799122869968414, Final Batch Loss: 0.18148694932460785\n",
      "Epoch 324, Loss: 0.948170617222786, Final Batch Loss: 0.2371063381433487\n",
      "Epoch 325, Loss: 0.9175409227609634, Final Batch Loss: 0.2553867995738983\n",
      "Epoch 326, Loss: 0.8939659595489502, Final Batch Loss: 0.2171340137720108\n",
      "Epoch 327, Loss: 0.8651550561189651, Final Batch Loss: 0.20902426540851593\n",
      "Epoch 328, Loss: 0.9178913682699203, Final Batch Loss: 0.1796487271785736\n",
      "Epoch 329, Loss: 0.944541722536087, Final Batch Loss: 0.17377731204032898\n",
      "Epoch 330, Loss: 0.9481775313615799, Final Batch Loss: 0.18985258042812347\n",
      "Epoch 331, Loss: 0.931633323431015, Final Batch Loss: 0.21452847123146057\n",
      "Epoch 332, Loss: 0.9085988700389862, Final Batch Loss: 0.23251913487911224\n",
      "Epoch 333, Loss: 0.8247057199478149, Final Batch Loss: 0.13755933940410614\n",
      "Epoch 334, Loss: 0.9413182735443115, Final Batch Loss: 0.2394845336675644\n",
      "Epoch 335, Loss: 0.9050702005624771, Final Batch Loss: 0.22623084485530853\n",
      "Epoch 336, Loss: 0.8880115747451782, Final Batch Loss: 0.1854446679353714\n",
      "Epoch 337, Loss: 0.8399277180433273, Final Batch Loss: 0.15999555587768555\n",
      "Epoch 338, Loss: 0.8908307552337646, Final Batch Loss: 0.24375224113464355\n",
      "Epoch 339, Loss: 0.9691477119922638, Final Batch Loss: 0.27332112193107605\n",
      "Epoch 340, Loss: 0.8922935873270035, Final Batch Loss: 0.18662817776203156\n",
      "Epoch 341, Loss: 0.8155485540628433, Final Batch Loss: 0.18948312103748322\n",
      "Epoch 342, Loss: 0.8895547986030579, Final Batch Loss: 0.2570250630378723\n",
      "Epoch 343, Loss: 0.8496586382389069, Final Batch Loss: 0.21744298934936523\n",
      "Epoch 344, Loss: 0.8499477356672287, Final Batch Loss: 0.2068941295146942\n",
      "Epoch 345, Loss: 0.8291264176368713, Final Batch Loss: 0.2354484349489212\n",
      "Epoch 346, Loss: 0.9308624714612961, Final Batch Loss: 0.23591403663158417\n",
      "Epoch 347, Loss: 0.8683869540691376, Final Batch Loss: 0.23651637136936188\n",
      "Epoch 348, Loss: 0.8471697568893433, Final Batch Loss: 0.20381076633930206\n",
      "Epoch 349, Loss: 0.9203856438398361, Final Batch Loss: 0.20398414134979248\n",
      "Epoch 350, Loss: 0.8416599929332733, Final Batch Loss: 0.14740532636642456\n",
      "Epoch 351, Loss: 0.9565118700265884, Final Batch Loss: 0.2714075744152069\n",
      "Epoch 352, Loss: 0.9663247764110565, Final Batch Loss: 0.27149197459220886\n",
      "Epoch 353, Loss: 0.9825655370950699, Final Batch Loss: 0.352731853723526\n",
      "Epoch 354, Loss: 0.8445236384868622, Final Batch Loss: 0.22147035598754883\n",
      "Epoch 355, Loss: 0.8381154537200928, Final Batch Loss: 0.1952485889196396\n",
      "Epoch 356, Loss: 0.9320723861455917, Final Batch Loss: 0.2846180498600006\n",
      "Epoch 357, Loss: 0.8342239856719971, Final Batch Loss: 0.21550214290618896\n",
      "Epoch 358, Loss: 0.8373226523399353, Final Batch Loss: 0.20741768181324005\n",
      "Epoch 359, Loss: 0.8439199030399323, Final Batch Loss: 0.20069973170757294\n",
      "Epoch 360, Loss: 0.916358232498169, Final Batch Loss: 0.2354981154203415\n",
      "Epoch 361, Loss: 0.9183964729309082, Final Batch Loss: 0.26666778326034546\n",
      "Epoch 362, Loss: 0.8406512290239334, Final Batch Loss: 0.14075765013694763\n",
      "Epoch 363, Loss: 0.8467658013105392, Final Batch Loss: 0.2591734528541565\n",
      "Epoch 364, Loss: 0.8358499556779861, Final Batch Loss: 0.17010313272476196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365, Loss: 0.8521117866039276, Final Batch Loss: 0.2144850790500641\n",
      "Epoch 366, Loss: 0.8508502840995789, Final Batch Loss: 0.2341218888759613\n",
      "Epoch 367, Loss: 0.8306499123573303, Final Batch Loss: 0.25149238109588623\n",
      "Epoch 368, Loss: 0.8330760449171066, Final Batch Loss: 0.16389402747154236\n",
      "Epoch 369, Loss: 0.8975035101175308, Final Batch Loss: 0.2101123332977295\n",
      "Epoch 370, Loss: 0.8203452676534653, Final Batch Loss: 0.18085604906082153\n",
      "Epoch 371, Loss: 0.8204872608184814, Final Batch Loss: 0.1973779946565628\n",
      "Epoch 372, Loss: 0.8158446550369263, Final Batch Loss: 0.18310502171516418\n",
      "Epoch 373, Loss: 0.8944858312606812, Final Batch Loss: 0.20851732790470123\n",
      "Epoch 374, Loss: 0.8073942810297012, Final Batch Loss: 0.15082404017448425\n",
      "Epoch 375, Loss: 0.8059521019458771, Final Batch Loss: 0.17244410514831543\n",
      "Epoch 376, Loss: 0.8626739382743835, Final Batch Loss: 0.22453446686267853\n",
      "Epoch 377, Loss: 0.7810334414243698, Final Batch Loss: 0.12385888397693634\n",
      "Epoch 378, Loss: 0.8400710374116898, Final Batch Loss: 0.2264706790447235\n",
      "Epoch 379, Loss: 0.881914421916008, Final Batch Loss: 0.2566160261631012\n",
      "Epoch 380, Loss: 0.9005021303892136, Final Batch Loss: 0.25676411390304565\n",
      "Epoch 381, Loss: 0.8423746675252914, Final Batch Loss: 0.21640194952487946\n",
      "Epoch 382, Loss: 0.8069138824939728, Final Batch Loss: 0.1593780666589737\n",
      "Epoch 383, Loss: 0.8288053572177887, Final Batch Loss: 0.18208621442317963\n",
      "Epoch 384, Loss: 0.910015195608139, Final Batch Loss: 0.15330122411251068\n",
      "Epoch 385, Loss: 0.8651595264673233, Final Batch Loss: 0.19173651933670044\n",
      "Epoch 386, Loss: 0.7925327122211456, Final Batch Loss: 0.13801826536655426\n",
      "Epoch 387, Loss: 0.8506253063678741, Final Batch Loss: 0.23607805371284485\n",
      "Epoch 388, Loss: 0.7924774736166, Final Batch Loss: 0.2103312611579895\n",
      "Epoch 389, Loss: 0.7941156476736069, Final Batch Loss: 0.22123296558856964\n",
      "Epoch 390, Loss: 0.8179121166467667, Final Batch Loss: 0.17897656559944153\n",
      "Epoch 391, Loss: 0.7878044992685318, Final Batch Loss: 0.14196689426898956\n",
      "Epoch 392, Loss: 0.8826024234294891, Final Batch Loss: 0.22455056011676788\n",
      "Epoch 393, Loss: 0.7779785394668579, Final Batch Loss: 0.16932404041290283\n",
      "Epoch 394, Loss: 0.7557984292507172, Final Batch Loss: 0.13104599714279175\n",
      "Epoch 395, Loss: 0.8378061652183533, Final Batch Loss: 0.2234410047531128\n",
      "Epoch 396, Loss: 0.7396613955497742, Final Batch Loss: 0.14777372777462006\n",
      "Epoch 397, Loss: 0.8030050545930862, Final Batch Loss: 0.19151724874973297\n",
      "Epoch 398, Loss: 0.8835284113883972, Final Batch Loss: 0.2615527808666229\n",
      "Epoch 399, Loss: 0.7647410482168198, Final Batch Loss: 0.1741761565208435\n",
      "Epoch 400, Loss: 0.7560232579708099, Final Batch Loss: 0.1764269471168518\n",
      "Epoch 401, Loss: 0.7745948731899261, Final Batch Loss: 0.23260845243930817\n",
      "Epoch 402, Loss: 0.7173357680439949, Final Batch Loss: 0.10434580594301224\n",
      "Epoch 403, Loss: 0.7294603288173676, Final Batch Loss: 0.21727916598320007\n",
      "Epoch 404, Loss: 0.8496250063180923, Final Batch Loss: 0.2255159169435501\n",
      "Epoch 405, Loss: 0.7894332706928253, Final Batch Loss: 0.1959262639284134\n",
      "Epoch 406, Loss: 0.7216148376464844, Final Batch Loss: 0.25265881419181824\n",
      "Epoch 407, Loss: 0.7721289545297623, Final Batch Loss: 0.20416362583637238\n",
      "Epoch 408, Loss: 0.8538320511579514, Final Batch Loss: 0.3091336488723755\n",
      "Epoch 409, Loss: 0.7388243973255157, Final Batch Loss: 0.17388589680194855\n",
      "Epoch 410, Loss: 0.7318463176488876, Final Batch Loss: 0.17482216656208038\n",
      "Epoch 411, Loss: 0.7892803698778152, Final Batch Loss: 0.21675929427146912\n",
      "Epoch 412, Loss: 0.8961827605962753, Final Batch Loss: 0.2850652039051056\n",
      "Epoch 413, Loss: 0.7701603323221207, Final Batch Loss: 0.17568962275981903\n",
      "Epoch 414, Loss: 0.8558747172355652, Final Batch Loss: 0.19848161935806274\n",
      "Epoch 415, Loss: 0.8378005921840668, Final Batch Loss: 0.21546033024787903\n",
      "Epoch 416, Loss: 0.851011335849762, Final Batch Loss: 0.18920056521892548\n",
      "Epoch 417, Loss: 0.7417677044868469, Final Batch Loss: 0.1677243411540985\n",
      "Epoch 418, Loss: 0.7336345314979553, Final Batch Loss: 0.15991637110710144\n",
      "Epoch 419, Loss: 0.8413855135440826, Final Batch Loss: 0.25373369455337524\n",
      "Epoch 420, Loss: 0.7428484410047531, Final Batch Loss: 0.1376628875732422\n",
      "Epoch 421, Loss: 0.843689039349556, Final Batch Loss: 0.2466990053653717\n",
      "Epoch 422, Loss: 0.7928628921508789, Final Batch Loss: 0.1871374398469925\n",
      "Epoch 423, Loss: 0.7087823450565338, Final Batch Loss: 0.13798008859157562\n",
      "Epoch 424, Loss: 0.7797778099775314, Final Batch Loss: 0.256928026676178\n",
      "Epoch 425, Loss: 0.7697656154632568, Final Batch Loss: 0.1644292026758194\n",
      "Epoch 426, Loss: 0.9452748745679855, Final Batch Loss: 0.3587205111980438\n",
      "Epoch 427, Loss: 0.6883538663387299, Final Batch Loss: 0.13270510733127594\n",
      "Epoch 428, Loss: 0.8734065592288971, Final Batch Loss: 0.23649613559246063\n",
      "Epoch 429, Loss: 0.8157877326011658, Final Batch Loss: 0.19231978058815002\n",
      "Epoch 430, Loss: 0.8025031685829163, Final Batch Loss: 0.21040380001068115\n",
      "Epoch 431, Loss: 0.8953138887882233, Final Batch Loss: 0.25260448455810547\n",
      "Epoch 432, Loss: 0.7587055563926697, Final Batch Loss: 0.17979270219802856\n",
      "Epoch 433, Loss: 0.8690438568592072, Final Batch Loss: 0.2622881531715393\n",
      "Epoch 434, Loss: 0.828880175948143, Final Batch Loss: 0.25765547156333923\n",
      "Epoch 435, Loss: 0.8066525459289551, Final Batch Loss: 0.21333396434783936\n",
      "Epoch 436, Loss: 0.7335673719644547, Final Batch Loss: 0.18819622695446014\n",
      "Epoch 437, Loss: 0.7399159967899323, Final Batch Loss: 0.14373932778835297\n",
      "Epoch 438, Loss: 0.7771801203489304, Final Batch Loss: 0.16550356149673462\n",
      "Epoch 439, Loss: 0.696898564696312, Final Batch Loss: 0.12887035310268402\n",
      "Epoch 440, Loss: 0.7476017773151398, Final Batch Loss: 0.17969544231891632\n",
      "Epoch 441, Loss: 0.7970584332942963, Final Batch Loss: 0.21010656654834747\n",
      "Epoch 442, Loss: 0.7147773504257202, Final Batch Loss: 0.1344657689332962\n",
      "Epoch 443, Loss: 0.8771960437297821, Final Batch Loss: 0.29053398966789246\n",
      "Epoch 444, Loss: 0.753738284111023, Final Batch Loss: 0.1963849812746048\n",
      "Epoch 445, Loss: 0.7416260093450546, Final Batch Loss: 0.15940335392951965\n",
      "Epoch 446, Loss: 0.7916909754276276, Final Batch Loss: 0.12026199698448181\n",
      "Epoch 447, Loss: 0.7014692574739456, Final Batch Loss: 0.19670306146144867\n",
      "Epoch 448, Loss: 0.7665017396211624, Final Batch Loss: 0.27536067366600037\n",
      "Epoch 449, Loss: 0.7715321183204651, Final Batch Loss: 0.20422466099262238\n",
      "Epoch 450, Loss: 0.7012075036764145, Final Batch Loss: 0.14202351868152618\n",
      "Epoch 451, Loss: 0.8429050892591476, Final Batch Loss: 0.2966563105583191\n",
      "Epoch 452, Loss: 0.7815449237823486, Final Batch Loss: 0.16817109286785126\n",
      "Epoch 453, Loss: 0.7921478003263474, Final Batch Loss: 0.23787330090999603\n",
      "Epoch 454, Loss: 0.8189712017774582, Final Batch Loss: 0.23268131911754608\n",
      "Epoch 455, Loss: 0.7191195338964462, Final Batch Loss: 0.16151872277259827\n",
      "Epoch 456, Loss: 0.6771267056465149, Final Batch Loss: 0.14410695433616638\n",
      "Epoch 457, Loss: 0.6998886913061142, Final Batch Loss: 0.1595732867717743\n",
      "Epoch 458, Loss: 0.8095544874668121, Final Batch Loss: 0.25156068801879883\n",
      "Epoch 459, Loss: 0.7515206933021545, Final Batch Loss: 0.1421106606721878\n",
      "Epoch 460, Loss: 0.7333108633756638, Final Batch Loss: 0.14949356019496918\n",
      "Epoch 461, Loss: 0.7369006723165512, Final Batch Loss: 0.19474904239177704\n",
      "Epoch 462, Loss: 0.7180685251951218, Final Batch Loss: 0.19820034503936768\n",
      "Epoch 463, Loss: 0.789979487657547, Final Batch Loss: 0.223773792386055\n",
      "Epoch 464, Loss: 0.7957654893398285, Final Batch Loss: 0.19392581284046173\n",
      "Epoch 465, Loss: 0.7692453861236572, Final Batch Loss: 0.19896070659160614\n",
      "Epoch 466, Loss: 0.6658705770969391, Final Batch Loss: 0.16671808063983917\n",
      "Epoch 467, Loss: 0.7200055718421936, Final Batch Loss: 0.1611221432685852\n",
      "Epoch 468, Loss: 0.8069350123405457, Final Batch Loss: 0.25343412160873413\n",
      "Epoch 469, Loss: 0.7069148421287537, Final Batch Loss: 0.13991442322731018\n",
      "Epoch 470, Loss: 0.7588700652122498, Final Batch Loss: 0.14594215154647827\n",
      "Epoch 471, Loss: 0.6581284403800964, Final Batch Loss: 0.18820257484912872\n",
      "Epoch 472, Loss: 0.7774286419153214, Final Batch Loss: 0.17265285551548004\n",
      "Epoch 473, Loss: 0.8481758087873459, Final Batch Loss: 0.2255990207195282\n",
      "Epoch 474, Loss: 0.81162329018116, Final Batch Loss: 0.20579694211483002\n",
      "Epoch 475, Loss: 0.7364623993635178, Final Batch Loss: 0.15475593507289886\n",
      "Epoch 476, Loss: 0.7391533702611923, Final Batch Loss: 0.1372910588979721\n",
      "Epoch 477, Loss: 0.6741791442036629, Final Batch Loss: 0.07425437122583389\n",
      "Epoch 478, Loss: 0.7634887844324112, Final Batch Loss: 0.22249403595924377\n",
      "Epoch 479, Loss: 0.7917452901601791, Final Batch Loss: 0.2116333395242691\n",
      "Epoch 480, Loss: 0.6920436173677444, Final Batch Loss: 0.13537274301052094\n",
      "Epoch 481, Loss: 0.7360052764415741, Final Batch Loss: 0.1924157589673996\n",
      "Epoch 482, Loss: 0.7506337612867355, Final Batch Loss: 0.22078652679920197\n",
      "Epoch 483, Loss: 0.7182339280843735, Final Batch Loss: 0.16730816662311554\n",
      "Epoch 484, Loss: 0.6355826482176781, Final Batch Loss: 0.07429128140211105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485, Loss: 0.817156508564949, Final Batch Loss: 0.2055555135011673\n",
      "Epoch 486, Loss: 0.7208766788244247, Final Batch Loss: 0.1410355120897293\n",
      "Epoch 487, Loss: 0.7718211859464645, Final Batch Loss: 0.20904147624969482\n",
      "Epoch 488, Loss: 0.7367599904537201, Final Batch Loss: 0.16648942232131958\n",
      "Epoch 489, Loss: 0.7876281440258026, Final Batch Loss: 0.14673690497875214\n",
      "Epoch 490, Loss: 0.6811012774705887, Final Batch Loss: 0.11837969720363617\n",
      "Epoch 491, Loss: 0.773462563753128, Final Batch Loss: 0.17526811361312866\n",
      "Epoch 492, Loss: 0.8421406149864197, Final Batch Loss: 0.2004801332950592\n",
      "Epoch 493, Loss: 0.7132490575313568, Final Batch Loss: 0.14176388084888458\n",
      "Epoch 494, Loss: 0.7038370966911316, Final Batch Loss: 0.11491754651069641\n",
      "Epoch 495, Loss: 0.6906793266534805, Final Batch Loss: 0.1583898514509201\n",
      "Epoch 496, Loss: 0.7084178626537323, Final Batch Loss: 0.2027290314435959\n",
      "Epoch 497, Loss: 0.77812360227108, Final Batch Loss: 0.15115004777908325\n",
      "Epoch 498, Loss: 0.7543705403804779, Final Batch Loss: 0.2451949417591095\n",
      "Epoch 499, Loss: 0.671498566865921, Final Batch Loss: 0.145248681306839\n",
      "Epoch 500, Loss: 0.6710101813077927, Final Batch Loss: 0.14703989028930664\n",
      "Epoch 501, Loss: 0.7031691670417786, Final Batch Loss: 0.1739259511232376\n",
      "Epoch 502, Loss: 0.681946724653244, Final Batch Loss: 0.12271687388420105\n",
      "Epoch 503, Loss: 0.7646986693143845, Final Batch Loss: 0.15382982790470123\n",
      "Epoch 504, Loss: 0.7511380314826965, Final Batch Loss: 0.1962682008743286\n",
      "Epoch 505, Loss: 0.7162691354751587, Final Batch Loss: 0.15140193700790405\n",
      "Epoch 506, Loss: 0.8474015593528748, Final Batch Loss: 0.2721810042858124\n",
      "Epoch 507, Loss: 0.7733989357948303, Final Batch Loss: 0.2552947402000427\n",
      "Epoch 508, Loss: 0.6948821395635605, Final Batch Loss: 0.21176837384700775\n",
      "Epoch 509, Loss: 0.7222784906625748, Final Batch Loss: 0.14704561233520508\n",
      "Epoch 510, Loss: 0.6892365664243698, Final Batch Loss: 0.21655458211898804\n",
      "Epoch 511, Loss: 0.7611730843782425, Final Batch Loss: 0.22018633782863617\n",
      "Epoch 512, Loss: 0.6342499777674675, Final Batch Loss: 0.07418470829725266\n",
      "Epoch 513, Loss: 0.6789436340332031, Final Batch Loss: 0.16920292377471924\n",
      "Epoch 514, Loss: 0.8038235157728195, Final Batch Loss: 0.2579675316810608\n",
      "Epoch 515, Loss: 0.6318332105875015, Final Batch Loss: 0.15721085667610168\n",
      "Epoch 516, Loss: 0.6718854159116745, Final Batch Loss: 0.14337818324565887\n",
      "Epoch 517, Loss: 0.6915247440338135, Final Batch Loss: 0.14393073320388794\n",
      "Epoch 518, Loss: 0.6238821819424629, Final Batch Loss: 0.11817387491464615\n",
      "Epoch 519, Loss: 0.6254714578390121, Final Batch Loss: 0.15088853240013123\n",
      "Epoch 520, Loss: 0.7277389466762543, Final Batch Loss: 0.1784730851650238\n",
      "Epoch 521, Loss: 0.7982779443264008, Final Batch Loss: 0.22685851156711578\n",
      "Epoch 522, Loss: 0.7328067421913147, Final Batch Loss: 0.17849582433700562\n",
      "Epoch 523, Loss: 0.7853474169969559, Final Batch Loss: 0.22982555627822876\n",
      "Epoch 524, Loss: 0.8469861298799515, Final Batch Loss: 0.34582820534706116\n",
      "Epoch 525, Loss: 0.8199934661388397, Final Batch Loss: 0.2644277811050415\n",
      "Epoch 526, Loss: 0.7631517052650452, Final Batch Loss: 0.2265005260705948\n",
      "Epoch 527, Loss: 0.7217891067266464, Final Batch Loss: 0.2142273634672165\n",
      "Epoch 528, Loss: 0.8259138762950897, Final Batch Loss: 0.2871623933315277\n",
      "Epoch 529, Loss: 0.8568981736898422, Final Batch Loss: 0.24096499383449554\n",
      "Epoch 530, Loss: 0.7915983498096466, Final Batch Loss: 0.16542750597000122\n",
      "Epoch 531, Loss: 0.6482082679867744, Final Batch Loss: 0.11652498692274094\n",
      "Epoch 532, Loss: 0.6928659379482269, Final Batch Loss: 0.16768726706504822\n",
      "Epoch 533, Loss: 0.750329926609993, Final Batch Loss: 0.2156098484992981\n",
      "Epoch 534, Loss: 0.7226936370134354, Final Batch Loss: 0.16270652413368225\n",
      "Epoch 535, Loss: 0.7880508601665497, Final Batch Loss: 0.19029483199119568\n",
      "Epoch 536, Loss: 0.6814591139554977, Final Batch Loss: 0.15210506319999695\n",
      "Epoch 537, Loss: 0.7455321848392487, Final Batch Loss: 0.25762853026390076\n",
      "Epoch 538, Loss: 0.6488676592707634, Final Batch Loss: 0.12100888043642044\n",
      "Epoch 539, Loss: 0.6705846339464188, Final Batch Loss: 0.1363772749900818\n",
      "Epoch 540, Loss: 0.7168965339660645, Final Batch Loss: 0.22106778621673584\n",
      "Epoch 541, Loss: 0.6938915997743607, Final Batch Loss: 0.18194380402565002\n",
      "Epoch 542, Loss: 0.8418048173189163, Final Batch Loss: 0.3295615613460541\n",
      "Epoch 543, Loss: 0.6675678640604019, Final Batch Loss: 0.16098034381866455\n",
      "Epoch 544, Loss: 0.6887737363576889, Final Batch Loss: 0.1598070114850998\n",
      "Epoch 545, Loss: 0.7925043106079102, Final Batch Loss: 0.2848411798477173\n",
      "Epoch 546, Loss: 0.6186619251966476, Final Batch Loss: 0.1264098882675171\n",
      "Epoch 547, Loss: 0.6365496963262558, Final Batch Loss: 0.11766083538532257\n",
      "Epoch 548, Loss: 0.7225179672241211, Final Batch Loss: 0.23263590037822723\n",
      "Epoch 549, Loss: 0.7338283509016037, Final Batch Loss: 0.15500974655151367\n",
      "Epoch 550, Loss: 0.7242978513240814, Final Batch Loss: 0.18547551333904266\n",
      "Epoch 551, Loss: 0.667969599366188, Final Batch Loss: 0.15150302648544312\n",
      "Epoch 552, Loss: 0.7368860244750977, Final Batch Loss: 0.12793126702308655\n",
      "Epoch 553, Loss: 0.6805797517299652, Final Batch Loss: 0.16415837407112122\n",
      "Epoch 554, Loss: 0.5726044774055481, Final Batch Loss: 0.1330902874469757\n",
      "Epoch 555, Loss: 0.6704297065734863, Final Batch Loss: 0.20258095860481262\n",
      "Epoch 556, Loss: 0.7135166078805923, Final Batch Loss: 0.20204952359199524\n",
      "Epoch 557, Loss: 0.6538694202899933, Final Batch Loss: 0.1397579163312912\n",
      "Epoch 558, Loss: 0.7115432322025299, Final Batch Loss: 0.215829998254776\n",
      "Epoch 559, Loss: 0.6057068482041359, Final Batch Loss: 0.17288662493228912\n",
      "Epoch 560, Loss: 0.61491409689188, Final Batch Loss: 0.1557432860136032\n",
      "Epoch 561, Loss: 0.6986072510480881, Final Batch Loss: 0.18063487112522125\n",
      "Epoch 562, Loss: 0.6328791603446007, Final Batch Loss: 0.11409153789281845\n",
      "Epoch 563, Loss: 0.6927267611026764, Final Batch Loss: 0.18539175391197205\n",
      "Epoch 564, Loss: 0.6886081397533417, Final Batch Loss: 0.1912432461977005\n",
      "Epoch 565, Loss: 0.6948100328445435, Final Batch Loss: 0.1491868644952774\n",
      "Epoch 566, Loss: 0.7331637740135193, Final Batch Loss: 0.17373082041740417\n",
      "Epoch 567, Loss: 0.6392595320940018, Final Batch Loss: 0.14946289360523224\n",
      "Epoch 568, Loss: 0.7779372334480286, Final Batch Loss: 0.295745849609375\n",
      "Epoch 569, Loss: 0.6513917744159698, Final Batch Loss: 0.13706180453300476\n",
      "Epoch 570, Loss: 0.6708461791276932, Final Batch Loss: 0.18783800303936005\n",
      "Epoch 571, Loss: 0.6887784004211426, Final Batch Loss: 0.21622075140476227\n",
      "Epoch 572, Loss: 0.6851788759231567, Final Batch Loss: 0.16085930168628693\n",
      "Epoch 573, Loss: 0.6249369755387306, Final Batch Loss: 0.11157932132482529\n",
      "Epoch 574, Loss: 0.674436092376709, Final Batch Loss: 0.19105416536331177\n",
      "Epoch 575, Loss: 0.7269594222307205, Final Batch Loss: 0.160234272480011\n",
      "Epoch 576, Loss: 0.6423281580209732, Final Batch Loss: 0.13911651074886322\n",
      "Epoch 577, Loss: 0.7427099049091339, Final Batch Loss: 0.21622492372989655\n",
      "Epoch 578, Loss: 0.6323168128728867, Final Batch Loss: 0.19649644196033478\n",
      "Epoch 579, Loss: 0.7370041906833649, Final Batch Loss: 0.22179195284843445\n",
      "Epoch 580, Loss: 0.6528664231300354, Final Batch Loss: 0.16921983659267426\n",
      "Epoch 581, Loss: 0.7037756145000458, Final Batch Loss: 0.21537372469902039\n",
      "Epoch 582, Loss: 0.7249018847942352, Final Batch Loss: 0.20490099489688873\n",
      "Epoch 583, Loss: 0.6076875030994415, Final Batch Loss: 0.13129694759845734\n",
      "Epoch 584, Loss: 0.7051084190607071, Final Batch Loss: 0.1758422702550888\n",
      "Epoch 585, Loss: 0.6537722498178482, Final Batch Loss: 0.15523086488246918\n",
      "Epoch 586, Loss: 0.7689485549926758, Final Batch Loss: 0.28541630506515503\n",
      "Epoch 587, Loss: 0.7574756741523743, Final Batch Loss: 0.1946191042661667\n",
      "Epoch 588, Loss: 0.653052881360054, Final Batch Loss: 0.15686853229999542\n",
      "Epoch 589, Loss: 0.6330654621124268, Final Batch Loss: 0.13819025456905365\n",
      "Epoch 590, Loss: 0.6095622479915619, Final Batch Loss: 0.13738813996315002\n",
      "Epoch 591, Loss: 0.7028793692588806, Final Batch Loss: 0.1988181173801422\n",
      "Epoch 592, Loss: 0.7512118369340897, Final Batch Loss: 0.2761700451374054\n",
      "Epoch 593, Loss: 0.5537604242563248, Final Batch Loss: 0.08922658860683441\n",
      "Epoch 594, Loss: 0.6365193724632263, Final Batch Loss: 0.12067243456840515\n",
      "Epoch 595, Loss: 0.7695713639259338, Final Batch Loss: 0.1996665596961975\n",
      "Epoch 596, Loss: 0.6653328239917755, Final Batch Loss: 0.12633822858333588\n",
      "Epoch 597, Loss: 0.7577275782823563, Final Batch Loss: 0.28684812784194946\n",
      "Epoch 598, Loss: 0.5804140865802765, Final Batch Loss: 0.11190955340862274\n",
      "Epoch 599, Loss: 0.6498839259147644, Final Batch Loss: 0.16515767574310303\n",
      "Epoch 600, Loss: 0.6098009869456291, Final Batch Loss: 0.11938980966806412\n",
      "Epoch 601, Loss: 0.5823971405625343, Final Batch Loss: 0.1836104691028595\n",
      "Epoch 602, Loss: 0.682100921869278, Final Batch Loss: 0.14825420081615448\n",
      "Epoch 603, Loss: 0.6436938643455505, Final Batch Loss: 0.14620156586170197\n",
      "Epoch 604, Loss: 0.6840018182992935, Final Batch Loss: 0.1585395783185959\n",
      "Epoch 605, Loss: 0.6825040429830551, Final Batch Loss: 0.18222467601299286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 606, Loss: 0.7793358713388443, Final Batch Loss: 0.3136586546897888\n",
      "Epoch 607, Loss: 0.650889590382576, Final Batch Loss: 0.11952748894691467\n",
      "Epoch 608, Loss: 0.6341977119445801, Final Batch Loss: 0.15699389576911926\n",
      "Epoch 609, Loss: 0.737726628780365, Final Batch Loss: 0.21681635081768036\n",
      "Epoch 610, Loss: 0.5649976506829262, Final Batch Loss: 0.10796373337507248\n",
      "Epoch 611, Loss: 0.6880320012569427, Final Batch Loss: 0.16103114187717438\n",
      "Epoch 612, Loss: 0.7259486168622971, Final Batch Loss: 0.21291279792785645\n",
      "Epoch 613, Loss: 0.7425904124975204, Final Batch Loss: 0.2735501825809479\n",
      "Epoch 614, Loss: 0.7675172239542007, Final Batch Loss: 0.18206076323986053\n",
      "Epoch 615, Loss: 0.6215142458677292, Final Batch Loss: 0.14146862924098969\n",
      "Epoch 616, Loss: 0.6544069200754166, Final Batch Loss: 0.19402901828289032\n",
      "Epoch 617, Loss: 0.6597239822149277, Final Batch Loss: 0.09487918019294739\n",
      "Epoch 618, Loss: 0.7124811410903931, Final Batch Loss: 0.2062126249074936\n",
      "Epoch 619, Loss: 0.6656380891799927, Final Batch Loss: 0.21198830008506775\n",
      "Epoch 620, Loss: 0.6778985261917114, Final Batch Loss: 0.1773775964975357\n",
      "Epoch 621, Loss: 0.6186807006597519, Final Batch Loss: 0.18535803258419037\n",
      "Epoch 622, Loss: 0.5453654006123543, Final Batch Loss: 0.10437911003828049\n",
      "Epoch 623, Loss: 0.638078011572361, Final Batch Loss: 0.1174674853682518\n",
      "Epoch 624, Loss: 0.7227188646793365, Final Batch Loss: 0.23335669934749603\n",
      "Epoch 625, Loss: 0.6278285980224609, Final Batch Loss: 0.17676813900470734\n",
      "Epoch 626, Loss: 0.6832121908664703, Final Batch Loss: 0.15563437342643738\n",
      "Epoch 627, Loss: 0.5754817724227905, Final Batch Loss: 0.10981708765029907\n",
      "Epoch 628, Loss: 0.7106605470180511, Final Batch Loss: 0.1347770243883133\n",
      "Epoch 629, Loss: 0.6420069262385368, Final Batch Loss: 0.1655065417289734\n",
      "Epoch 630, Loss: 0.5599488019943237, Final Batch Loss: 0.0973820835351944\n",
      "Epoch 631, Loss: 0.6876058280467987, Final Batch Loss: 0.16403114795684814\n",
      "Epoch 632, Loss: 0.6554854363203049, Final Batch Loss: 0.14802414178848267\n",
      "Epoch 633, Loss: 0.579615980386734, Final Batch Loss: 0.12649662792682648\n",
      "Epoch 634, Loss: 0.5942508727312088, Final Batch Loss: 0.13937175273895264\n",
      "Epoch 635, Loss: 0.6871430724859238, Final Batch Loss: 0.14132387936115265\n",
      "Epoch 636, Loss: 0.6298806816339493, Final Batch Loss: 0.18302825093269348\n",
      "Epoch 637, Loss: 0.6161980256438255, Final Batch Loss: 0.103535957634449\n",
      "Epoch 638, Loss: 0.6938370615243912, Final Batch Loss: 0.20499227941036224\n",
      "Epoch 639, Loss: 0.6693814992904663, Final Batch Loss: 0.16483992338180542\n",
      "Epoch 640, Loss: 0.6430969834327698, Final Batch Loss: 0.13309982419013977\n",
      "Epoch 641, Loss: 0.5754047706723213, Final Batch Loss: 0.08833400160074234\n",
      "Epoch 642, Loss: 0.6421248018741608, Final Batch Loss: 0.15677590668201447\n",
      "Epoch 643, Loss: 0.6358178332448006, Final Batch Loss: 0.0958746150135994\n",
      "Epoch 644, Loss: 0.5737977474927902, Final Batch Loss: 0.14345404505729675\n",
      "Epoch 645, Loss: 0.6230739206075668, Final Batch Loss: 0.13480740785598755\n",
      "Epoch 646, Loss: 0.6000852882862091, Final Batch Loss: 0.13192777335643768\n",
      "Epoch 647, Loss: 0.5789902210235596, Final Batch Loss: 0.12217852473258972\n",
      "Epoch 648, Loss: 0.7917090356349945, Final Batch Loss: 0.29546600580215454\n",
      "Epoch 649, Loss: 0.6070829704403877, Final Batch Loss: 0.12038896232843399\n",
      "Epoch 650, Loss: 0.6497068256139755, Final Batch Loss: 0.15738490223884583\n",
      "Epoch 651, Loss: 0.7420019060373306, Final Batch Loss: 0.19651630520820618\n",
      "Epoch 652, Loss: 0.5966311991214752, Final Batch Loss: 0.09682853519916534\n",
      "Epoch 653, Loss: 0.6726090461015701, Final Batch Loss: 0.13819770514965057\n",
      "Epoch 654, Loss: 0.6095283180475235, Final Batch Loss: 0.14903363585472107\n",
      "Epoch 655, Loss: 0.6249250024557114, Final Batch Loss: 0.12041641771793365\n",
      "Epoch 656, Loss: 0.6033557951450348, Final Batch Loss: 0.10687138140201569\n",
      "Epoch 657, Loss: 0.652865931391716, Final Batch Loss: 0.16178295016288757\n",
      "Epoch 658, Loss: 0.6604667454957962, Final Batch Loss: 0.13561014831066132\n",
      "Epoch 659, Loss: 0.6504245772957802, Final Batch Loss: 0.13559919595718384\n",
      "Epoch 660, Loss: 0.7507162541151047, Final Batch Loss: 0.2592206299304962\n",
      "Epoch 661, Loss: 0.6643679887056351, Final Batch Loss: 0.12835213541984558\n",
      "Epoch 662, Loss: 0.6641610115766525, Final Batch Loss: 0.16920652985572815\n",
      "Epoch 663, Loss: 0.6524238139390945, Final Batch Loss: 0.14738458395004272\n",
      "Epoch 664, Loss: 0.6085956245660782, Final Batch Loss: 0.11221811175346375\n",
      "Epoch 665, Loss: 0.611144557595253, Final Batch Loss: 0.12156899273395538\n",
      "Epoch 666, Loss: 0.6217470392584801, Final Batch Loss: 0.09582836180925369\n",
      "Epoch 667, Loss: 0.6536078304052353, Final Batch Loss: 0.10369564592838287\n",
      "Epoch 668, Loss: 0.6085520014166832, Final Batch Loss: 0.18432848155498505\n",
      "Epoch 669, Loss: 0.5245456472039223, Final Batch Loss: 0.07992055267095566\n",
      "Epoch 670, Loss: 0.5690077915787697, Final Batch Loss: 0.11574114114046097\n",
      "Epoch 671, Loss: 0.577072262763977, Final Batch Loss: 0.1293335109949112\n",
      "Epoch 672, Loss: 0.6977219581604004, Final Batch Loss: 0.2142648994922638\n",
      "Epoch 673, Loss: 0.6438430994749069, Final Batch Loss: 0.18382778763771057\n",
      "Epoch 674, Loss: 0.583161011338234, Final Batch Loss: 0.1403713822364807\n",
      "Epoch 675, Loss: 0.6243692114949226, Final Batch Loss: 0.11699073761701584\n",
      "Epoch 676, Loss: 0.6655953228473663, Final Batch Loss: 0.18418574333190918\n",
      "Epoch 677, Loss: 0.6846596524119377, Final Batch Loss: 0.2216370552778244\n",
      "Epoch 678, Loss: 0.7744888812303543, Final Batch Loss: 0.2214740514755249\n",
      "Epoch 679, Loss: 0.5680596828460693, Final Batch Loss: 0.09046971797943115\n",
      "Epoch 680, Loss: 0.6479499489068985, Final Batch Loss: 0.10441151261329651\n",
      "Epoch 681, Loss: 0.6432731300592422, Final Batch Loss: 0.21564951539039612\n",
      "Epoch 682, Loss: 0.6160978674888611, Final Batch Loss: 0.15810759365558624\n",
      "Epoch 683, Loss: 0.5653655380010605, Final Batch Loss: 0.1572662740945816\n",
      "Epoch 684, Loss: 0.5789568722248077, Final Batch Loss: 0.12849725782871246\n",
      "Epoch 685, Loss: 0.6172414943575859, Final Batch Loss: 0.11808124929666519\n",
      "Epoch 686, Loss: 0.5993247330188751, Final Batch Loss: 0.10828191041946411\n",
      "Epoch 687, Loss: 0.6250700056552887, Final Batch Loss: 0.2095673382282257\n",
      "Epoch 688, Loss: 0.621190071105957, Final Batch Loss: 0.236399307847023\n",
      "Epoch 689, Loss: 0.6742659360170364, Final Batch Loss: 0.19132457673549652\n",
      "Epoch 690, Loss: 0.6302346959710121, Final Batch Loss: 0.10495775192975998\n",
      "Epoch 691, Loss: 0.6104577630758286, Final Batch Loss: 0.17067445814609528\n",
      "Epoch 692, Loss: 0.6992726773023605, Final Batch Loss: 0.17825382947921753\n",
      "Epoch 693, Loss: 0.6287629455327988, Final Batch Loss: 0.16803286969661713\n",
      "Epoch 694, Loss: 0.5942898616194725, Final Batch Loss: 0.07880640774965286\n",
      "Epoch 695, Loss: 0.551209457218647, Final Batch Loss: 0.13093321025371552\n",
      "Epoch 696, Loss: 0.6423826366662979, Final Batch Loss: 0.18810142576694489\n",
      "Epoch 697, Loss: 0.6136785000562668, Final Batch Loss: 0.16366928815841675\n",
      "Epoch 698, Loss: 0.5573371350765228, Final Batch Loss: 0.16882050037384033\n",
      "Epoch 699, Loss: 0.5628296285867691, Final Batch Loss: 0.15745452046394348\n",
      "Epoch 700, Loss: 0.6156924515962601, Final Batch Loss: 0.14034762978553772\n",
      "Epoch 701, Loss: 0.6664084196090698, Final Batch Loss: 0.15080980956554413\n",
      "Epoch 702, Loss: 0.6544587910175323, Final Batch Loss: 0.15636521577835083\n",
      "Epoch 703, Loss: 0.581102691590786, Final Batch Loss: 0.16156968474388123\n",
      "Epoch 704, Loss: 0.5863910466432571, Final Batch Loss: 0.13325393199920654\n",
      "Epoch 705, Loss: 0.6424600332975388, Final Batch Loss: 0.2093254178762436\n",
      "Epoch 706, Loss: 0.6890391260385513, Final Batch Loss: 0.2749110758304596\n",
      "Epoch 707, Loss: 0.6095879226922989, Final Batch Loss: 0.13997545838356018\n",
      "Epoch 708, Loss: 0.5439348667860031, Final Batch Loss: 0.13602781295776367\n",
      "Epoch 709, Loss: 0.5850715637207031, Final Batch Loss: 0.18219627439975739\n",
      "Epoch 710, Loss: 0.5560715273022652, Final Batch Loss: 0.10414493829011917\n",
      "Epoch 711, Loss: 0.48478785902261734, Final Batch Loss: 0.12667492032051086\n",
      "Epoch 712, Loss: 0.5613364279270172, Final Batch Loss: 0.11698341369628906\n",
      "Epoch 713, Loss: 0.6094082519412041, Final Batch Loss: 0.16598409414291382\n",
      "Epoch 714, Loss: 0.5130911469459534, Final Batch Loss: 0.1090695708990097\n",
      "Epoch 715, Loss: 0.6764956563711166, Final Batch Loss: 0.22910885512828827\n",
      "Epoch 716, Loss: 0.7344473004341125, Final Batch Loss: 0.2887728214263916\n",
      "Epoch 717, Loss: 0.5704667568206787, Final Batch Loss: 0.1584853231906891\n",
      "Epoch 718, Loss: 0.6047903001308441, Final Batch Loss: 0.1512007713317871\n",
      "Epoch 719, Loss: 0.48407014459371567, Final Batch Loss: 0.04545193910598755\n",
      "Epoch 720, Loss: 0.6548500284552574, Final Batch Loss: 0.18482404947280884\n",
      "Epoch 721, Loss: 0.6232116520404816, Final Batch Loss: 0.09810899198055267\n",
      "Epoch 722, Loss: 0.6353501006960869, Final Batch Loss: 0.1248965933918953\n",
      "Epoch 723, Loss: 0.6126167625188828, Final Batch Loss: 0.1277274638414383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 724, Loss: 0.579019583761692, Final Batch Loss: 0.08319463580846786\n",
      "Epoch 725, Loss: 0.5836187601089478, Final Batch Loss: 0.12161687016487122\n",
      "Epoch 726, Loss: 0.59437046200037, Final Batch Loss: 0.14699779450893402\n",
      "Epoch 727, Loss: 0.6309927925467491, Final Batch Loss: 0.11351270228624344\n",
      "Epoch 728, Loss: 0.6318398267030716, Final Batch Loss: 0.14078469574451447\n",
      "Epoch 729, Loss: 0.5678357928991318, Final Batch Loss: 0.07149890065193176\n",
      "Epoch 730, Loss: 0.5171669721603394, Final Batch Loss: 0.057745687663555145\n",
      "Epoch 731, Loss: 0.5547875836491585, Final Batch Loss: 0.13007473945617676\n",
      "Epoch 732, Loss: 0.5689029470086098, Final Batch Loss: 0.18575406074523926\n",
      "Epoch 733, Loss: 0.5239438563585281, Final Batch Loss: 0.12682101130485535\n",
      "Epoch 734, Loss: 0.5251524969935417, Final Batch Loss: 0.10861590504646301\n",
      "Epoch 735, Loss: 0.5659661740064621, Final Batch Loss: 0.12081264704465866\n",
      "Epoch 736, Loss: 0.5694796293973923, Final Batch Loss: 0.16077716648578644\n",
      "Epoch 737, Loss: 0.6228198558092117, Final Batch Loss: 0.14268611371517181\n",
      "Epoch 738, Loss: 0.5398512631654739, Final Batch Loss: 0.13128934800624847\n",
      "Epoch 739, Loss: 0.5610426366329193, Final Batch Loss: 0.06619930267333984\n",
      "Epoch 740, Loss: 0.553198978304863, Final Batch Loss: 0.12959367036819458\n",
      "Epoch 741, Loss: 0.6187562793493271, Final Batch Loss: 0.1948714405298233\n",
      "Epoch 742, Loss: 0.5900253728032112, Final Batch Loss: 0.13463948667049408\n",
      "Epoch 743, Loss: 0.536631315946579, Final Batch Loss: 0.15891745686531067\n",
      "Epoch 744, Loss: 0.6956801414489746, Final Batch Loss: 0.1723002791404724\n",
      "Epoch 745, Loss: 0.6664320528507233, Final Batch Loss: 0.18440459668636322\n",
      "Epoch 746, Loss: 0.5595998540520668, Final Batch Loss: 0.15356701612472534\n",
      "Epoch 747, Loss: 0.5608851164579391, Final Batch Loss: 0.15947453677654266\n",
      "Epoch 748, Loss: 0.6716545075178146, Final Batch Loss: 0.1618167757987976\n",
      "Epoch 749, Loss: 0.6847332715988159, Final Batch Loss: 0.23080624639987946\n",
      "Epoch 750, Loss: 0.6035626530647278, Final Batch Loss: 0.14940348267555237\n",
      "Epoch 751, Loss: 0.6407833248376846, Final Batch Loss: 0.16843734681606293\n",
      "Epoch 752, Loss: 0.6177390888333321, Final Batch Loss: 0.14886672794818878\n",
      "Epoch 753, Loss: 0.5978890508413315, Final Batch Loss: 0.13154739141464233\n",
      "Epoch 754, Loss: 0.5237756669521332, Final Batch Loss: 0.14165809750556946\n",
      "Epoch 755, Loss: 0.5932965129613876, Final Batch Loss: 0.11425064504146576\n",
      "Epoch 756, Loss: 0.6418224349617958, Final Batch Loss: 0.24655357003211975\n",
      "Epoch 757, Loss: 0.6152896881103516, Final Batch Loss: 0.2057814747095108\n",
      "Epoch 758, Loss: 0.5423017367720604, Final Batch Loss: 0.13573035597801208\n",
      "Epoch 759, Loss: 0.5359828472137451, Final Batch Loss: 0.12072965502738953\n",
      "Epoch 760, Loss: 0.569837361574173, Final Batch Loss: 0.1451254040002823\n",
      "Epoch 761, Loss: 0.6043896973133087, Final Batch Loss: 0.17319461703300476\n",
      "Epoch 762, Loss: 0.5841255113482475, Final Batch Loss: 0.19217368960380554\n",
      "Epoch 763, Loss: 0.5975037664175034, Final Batch Loss: 0.1356172412633896\n",
      "Epoch 764, Loss: 0.5991976037621498, Final Batch Loss: 0.1151084303855896\n",
      "Epoch 765, Loss: 0.5599226206541061, Final Batch Loss: 0.11440493166446686\n",
      "Epoch 766, Loss: 0.5990887954831123, Final Batch Loss: 0.10381481796503067\n",
      "Epoch 767, Loss: 0.5852525979280472, Final Batch Loss: 0.17014187574386597\n",
      "Epoch 768, Loss: 0.5376733541488647, Final Batch Loss: 0.16671548783779144\n",
      "Epoch 769, Loss: 0.6462685242295265, Final Batch Loss: 0.20203645527362823\n",
      "Epoch 770, Loss: 0.6068070381879807, Final Batch Loss: 0.1409560590982437\n",
      "Epoch 771, Loss: 0.5767141208052635, Final Batch Loss: 0.18237991631031036\n",
      "Epoch 772, Loss: 0.5643831938505173, Final Batch Loss: 0.12682420015335083\n",
      "Epoch 773, Loss: 0.5452600568532944, Final Batch Loss: 0.18027731776237488\n",
      "Epoch 774, Loss: 0.5667718276381493, Final Batch Loss: 0.2330779880285263\n",
      "Epoch 775, Loss: 0.5806553661823273, Final Batch Loss: 0.1303146630525589\n",
      "Epoch 776, Loss: 0.5143699496984482, Final Batch Loss: 0.07502387464046478\n",
      "Epoch 777, Loss: 0.5715618133544922, Final Batch Loss: 0.13604897260665894\n",
      "Epoch 778, Loss: 0.6127192825078964, Final Batch Loss: 0.18441469967365265\n",
      "Epoch 779, Loss: 0.5018455684185028, Final Batch Loss: 0.11497796326875687\n",
      "Epoch 780, Loss: 0.5220753476023674, Final Batch Loss: 0.07767219096422195\n",
      "Epoch 781, Loss: 0.5172169506549835, Final Batch Loss: 0.07958939671516418\n",
      "Epoch 782, Loss: 0.588629886507988, Final Batch Loss: 0.1382204294204712\n",
      "Epoch 783, Loss: 0.5850135385990143, Final Batch Loss: 0.1065550446510315\n",
      "Epoch 784, Loss: 0.5953436121344566, Final Batch Loss: 0.12597332894802094\n",
      "Epoch 785, Loss: 0.6854946911334991, Final Batch Loss: 0.2143932729959488\n",
      "Epoch 786, Loss: 0.5833722054958344, Final Batch Loss: 0.1488105058670044\n",
      "Epoch 787, Loss: 0.5049222856760025, Final Batch Loss: 0.07835647463798523\n",
      "Epoch 788, Loss: 0.5702112838625908, Final Batch Loss: 0.14784398674964905\n",
      "Epoch 789, Loss: 0.601057879626751, Final Batch Loss: 0.16799236834049225\n",
      "Epoch 790, Loss: 0.6946323812007904, Final Batch Loss: 0.20152200758457184\n",
      "Epoch 791, Loss: 0.5193623378872871, Final Batch Loss: 0.11102518439292908\n",
      "Epoch 792, Loss: 0.590134784579277, Final Batch Loss: 0.1972433626651764\n",
      "Epoch 793, Loss: 0.5858425498008728, Final Batch Loss: 0.13292376697063446\n",
      "Epoch 794, Loss: 0.5159009918570518, Final Batch Loss: 0.12984688580036163\n",
      "Epoch 795, Loss: 0.6076816916465759, Final Batch Loss: 0.12638472020626068\n",
      "Epoch 796, Loss: 0.6710617244243622, Final Batch Loss: 0.21813960373401642\n",
      "Epoch 797, Loss: 0.6044724434614182, Final Batch Loss: 0.10360851883888245\n",
      "Epoch 798, Loss: 0.5873131304979324, Final Batch Loss: 0.12369062751531601\n",
      "Epoch 799, Loss: 0.5738981813192368, Final Batch Loss: 0.12980692088603973\n",
      "Epoch 800, Loss: 0.5746559649705887, Final Batch Loss: 0.1472439020872116\n",
      "Epoch 801, Loss: 0.5573757588863373, Final Batch Loss: 0.09578061103820801\n",
      "Epoch 802, Loss: 0.575004830956459, Final Batch Loss: 0.10674824565649033\n",
      "Epoch 803, Loss: 0.5677946582436562, Final Batch Loss: 0.11679468303918839\n",
      "Epoch 804, Loss: 0.5388988628983498, Final Batch Loss: 0.13685281574726105\n",
      "Epoch 805, Loss: 0.48052307963371277, Final Batch Loss: 0.10047173500061035\n",
      "Epoch 806, Loss: 0.5655244737863541, Final Batch Loss: 0.12658187747001648\n",
      "Epoch 807, Loss: 0.6424686163663864, Final Batch Loss: 0.17190828919410706\n",
      "Epoch 808, Loss: 0.5565896779298782, Final Batch Loss: 0.13229285180568695\n",
      "Epoch 809, Loss: 0.5573079138994217, Final Batch Loss: 0.15029621124267578\n",
      "Epoch 810, Loss: 0.5793453603982925, Final Batch Loss: 0.11279913783073425\n",
      "Epoch 811, Loss: 0.4992241859436035, Final Batch Loss: 0.11049958318471909\n",
      "Epoch 812, Loss: 0.6164669394493103, Final Batch Loss: 0.1923256814479828\n",
      "Epoch 813, Loss: 0.5127934813499451, Final Batch Loss: 0.08442917466163635\n",
      "Epoch 814, Loss: 0.5754315555095673, Final Batch Loss: 0.1561587005853653\n",
      "Epoch 815, Loss: 0.5830728858709335, Final Batch Loss: 0.16485057771205902\n",
      "Epoch 816, Loss: 0.6423152089118958, Final Batch Loss: 0.26290175318717957\n",
      "Epoch 817, Loss: 0.5144586265087128, Final Batch Loss: 0.10372886061668396\n",
      "Epoch 818, Loss: 0.6469163820147514, Final Batch Loss: 0.19916823506355286\n",
      "Epoch 819, Loss: 0.44258152693510056, Final Batch Loss: 0.07697270810604095\n",
      "Epoch 820, Loss: 0.5131989866495132, Final Batch Loss: 0.11025895178318024\n",
      "Epoch 821, Loss: 0.5850169137120247, Final Batch Loss: 0.15963192284107208\n",
      "Epoch 822, Loss: 0.6364182233810425, Final Batch Loss: 0.1636434942483902\n",
      "Epoch 823, Loss: 0.5596099942922592, Final Batch Loss: 0.09334853291511536\n",
      "Epoch 824, Loss: 0.5532829314470291, Final Batch Loss: 0.1090940609574318\n",
      "Epoch 825, Loss: 0.5594897866249084, Final Batch Loss: 0.09081722795963287\n",
      "Epoch 826, Loss: 0.5474327877163887, Final Batch Loss: 0.14011698961257935\n",
      "Epoch 827, Loss: 0.6709391474723816, Final Batch Loss: 0.17642587423324585\n",
      "Epoch 828, Loss: 0.6031482145190239, Final Batch Loss: 0.1737516075372696\n",
      "Epoch 829, Loss: 0.5066197738051414, Final Batch Loss: 0.11164014786481857\n",
      "Epoch 830, Loss: 0.5368017852306366, Final Batch Loss: 0.15497969090938568\n",
      "Epoch 831, Loss: 0.6588409170508385, Final Batch Loss: 0.09122831374406815\n",
      "Epoch 832, Loss: 0.5312202051281929, Final Batch Loss: 0.15272778272628784\n",
      "Epoch 833, Loss: 0.6190468892455101, Final Batch Loss: 0.09310116618871689\n",
      "Epoch 834, Loss: 0.5715668126940727, Final Batch Loss: 0.09139242023229599\n",
      "Epoch 835, Loss: 0.46719367802143097, Final Batch Loss: 0.09177370369434357\n",
      "Epoch 836, Loss: 0.6102942526340485, Final Batch Loss: 0.14113996922969818\n",
      "Epoch 837, Loss: 0.6262202933430672, Final Batch Loss: 0.10932844132184982\n",
      "Epoch 838, Loss: 0.545106865465641, Final Batch Loss: 0.10312912613153458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 839, Loss: 0.5100116245448589, Final Batch Loss: 0.06109040603041649\n",
      "Epoch 840, Loss: 0.5434674099087715, Final Batch Loss: 0.1455186903476715\n",
      "Epoch 841, Loss: 0.6259599030017853, Final Batch Loss: 0.15917891263961792\n",
      "Epoch 842, Loss: 0.5358923971652985, Final Batch Loss: 0.11182931810617447\n",
      "Epoch 843, Loss: 0.6293057054281235, Final Batch Loss: 0.24902503192424774\n",
      "Epoch 844, Loss: 0.5430126860737801, Final Batch Loss: 0.15964564681053162\n",
      "Epoch 845, Loss: 0.5589391738176346, Final Batch Loss: 0.17317961156368256\n",
      "Epoch 846, Loss: 0.4880797043442726, Final Batch Loss: 0.14592766761779785\n",
      "Epoch 847, Loss: 0.6226038411259651, Final Batch Loss: 0.21566809713840485\n",
      "Epoch 848, Loss: 0.5839357525110245, Final Batch Loss: 0.2079716920852661\n",
      "Epoch 849, Loss: 0.6184699088335037, Final Batch Loss: 0.1940525621175766\n",
      "Epoch 850, Loss: 0.5140157416462898, Final Batch Loss: 0.1533498913049698\n",
      "Epoch 851, Loss: 0.540351539850235, Final Batch Loss: 0.10416579991579056\n",
      "Epoch 852, Loss: 0.585967555642128, Final Batch Loss: 0.14136575162410736\n",
      "Epoch 853, Loss: 0.5419986248016357, Final Batch Loss: 0.11555256694555283\n",
      "Epoch 854, Loss: 0.5055565387010574, Final Batch Loss: 0.08632943034172058\n",
      "Epoch 855, Loss: 0.5076141133904457, Final Batch Loss: 0.13091081380844116\n",
      "Epoch 856, Loss: 0.6543476432561874, Final Batch Loss: 0.20005473494529724\n",
      "Epoch 857, Loss: 0.46563277393579483, Final Batch Loss: 0.07208514958620071\n",
      "Epoch 858, Loss: 0.462375957518816, Final Batch Loss: 0.05779504403471947\n",
      "Epoch 859, Loss: 0.47516442090272903, Final Batch Loss: 0.11134713888168335\n",
      "Epoch 860, Loss: 0.5492942482233047, Final Batch Loss: 0.1769922524690628\n",
      "Epoch 861, Loss: 0.4558851942420006, Final Batch Loss: 0.07334606349468231\n",
      "Epoch 862, Loss: 0.5581353157758713, Final Batch Loss: 0.16967718303203583\n",
      "Epoch 863, Loss: 0.5351695120334625, Final Batch Loss: 0.1534784883260727\n",
      "Epoch 864, Loss: 0.563775971531868, Final Batch Loss: 0.2257859855890274\n",
      "Epoch 865, Loss: 0.46929678320884705, Final Batch Loss: 0.10730862617492676\n",
      "Epoch 866, Loss: 0.474006287753582, Final Batch Loss: 0.07643338292837143\n",
      "Epoch 867, Loss: 0.5678563863039017, Final Batch Loss: 0.1284017711877823\n",
      "Epoch 868, Loss: 0.5641743466258049, Final Batch Loss: 0.15342304110527039\n",
      "Epoch 869, Loss: 0.4924060329794884, Final Batch Loss: 0.14198200404644012\n",
      "Epoch 870, Loss: 0.45696694403886795, Final Batch Loss: 0.0701185017824173\n",
      "Epoch 871, Loss: 0.5032530725002289, Final Batch Loss: 0.12254944443702698\n",
      "Epoch 872, Loss: 0.6301532089710236, Final Batch Loss: 0.18476904928684235\n",
      "Epoch 873, Loss: 0.5674518272280693, Final Batch Loss: 0.10031632333993912\n",
      "Epoch 874, Loss: 0.47983987629413605, Final Batch Loss: 0.10076919198036194\n",
      "Epoch 875, Loss: 0.5639509037137032, Final Batch Loss: 0.17968088388442993\n",
      "Epoch 876, Loss: 0.5065220445394516, Final Batch Loss: 0.14534235000610352\n",
      "Epoch 877, Loss: 0.5726390331983566, Final Batch Loss: 0.14148011803627014\n",
      "Epoch 878, Loss: 0.615861251950264, Final Batch Loss: 0.22177480161190033\n",
      "Epoch 879, Loss: 0.5174039453268051, Final Batch Loss: 0.11204499751329422\n",
      "Epoch 880, Loss: 0.5861630812287331, Final Batch Loss: 0.09801199287176132\n",
      "Epoch 881, Loss: 0.5002837777137756, Final Batch Loss: 0.11286073178052902\n",
      "Epoch 882, Loss: 0.644058994948864, Final Batch Loss: 0.1692429780960083\n",
      "Epoch 883, Loss: 0.5216331034898758, Final Batch Loss: 0.10836946964263916\n",
      "Epoch 884, Loss: 0.6464259475469589, Final Batch Loss: 0.19059032201766968\n",
      "Epoch 885, Loss: 0.4593876972794533, Final Batch Loss: 0.1088319942355156\n",
      "Epoch 886, Loss: 0.4952695220708847, Final Batch Loss: 0.14016841351985931\n",
      "Epoch 887, Loss: 0.5323238372802734, Final Batch Loss: 0.14920999109745026\n",
      "Epoch 888, Loss: 0.5096760839223862, Final Batch Loss: 0.15849070250988007\n",
      "Epoch 889, Loss: 0.45856823772192, Final Batch Loss: 0.07393380254507065\n",
      "Epoch 890, Loss: 0.4713374674320221, Final Batch Loss: 0.09130263328552246\n",
      "Epoch 891, Loss: 0.5429632812738419, Final Batch Loss: 0.17720963060855865\n",
      "Epoch 892, Loss: 0.49678731709718704, Final Batch Loss: 0.1421358436346054\n",
      "Epoch 893, Loss: 0.5047510787844658, Final Batch Loss: 0.1639500856399536\n",
      "Epoch 894, Loss: 0.5516759529709816, Final Batch Loss: 0.19385983049869537\n",
      "Epoch 895, Loss: 0.5345591455698013, Final Batch Loss: 0.13253898918628693\n",
      "Epoch 896, Loss: 0.5411691889166832, Final Batch Loss: 0.19855612516403198\n",
      "Epoch 897, Loss: 0.4909735471010208, Final Batch Loss: 0.14237864315509796\n",
      "Epoch 898, Loss: 0.4405605420470238, Final Batch Loss: 0.07881313562393188\n",
      "Epoch 899, Loss: 0.42203837633132935, Final Batch Loss: 0.10391049087047577\n",
      "Epoch 900, Loss: 0.47314441204071045, Final Batch Loss: 0.12484109401702881\n",
      "Epoch 901, Loss: 0.4268930107355118, Final Batch Loss: 0.1019609272480011\n",
      "Epoch 902, Loss: 0.4414608031511307, Final Batch Loss: 0.07247621566057205\n",
      "Epoch 903, Loss: 0.5052140578627586, Final Batch Loss: 0.17265327274799347\n",
      "Epoch 904, Loss: 0.504130482673645, Final Batch Loss: 0.13315953314304352\n",
      "Epoch 905, Loss: 0.4496498219668865, Final Batch Loss: 0.043894391506910324\n",
      "Epoch 906, Loss: 0.4907296299934387, Final Batch Loss: 0.11851199716329575\n",
      "Epoch 907, Loss: 0.4677523076534271, Final Batch Loss: 0.09778568893671036\n",
      "Epoch 908, Loss: 0.6135699450969696, Final Batch Loss: 0.2077411264181137\n",
      "Epoch 909, Loss: 0.48903366178274155, Final Batch Loss: 0.16520752012729645\n",
      "Epoch 910, Loss: 0.46774253994226456, Final Batch Loss: 0.06649019569158554\n",
      "Epoch 911, Loss: 0.5170949101448059, Final Batch Loss: 0.10303197801113129\n",
      "Epoch 912, Loss: 0.6007348895072937, Final Batch Loss: 0.18505461513996124\n",
      "Epoch 913, Loss: 0.4277532249689102, Final Batch Loss: 0.061885394155979156\n",
      "Epoch 914, Loss: 0.450905978679657, Final Batch Loss: 0.11112228780984879\n",
      "Epoch 915, Loss: 0.5384089723229408, Final Batch Loss: 0.10004948824644089\n",
      "Epoch 916, Loss: 0.5055072754621506, Final Batch Loss: 0.15492205321788788\n",
      "Epoch 917, Loss: 0.48998191952705383, Final Batch Loss: 0.14629097282886505\n",
      "Epoch 918, Loss: 0.4564976990222931, Final Batch Loss: 0.12880489230155945\n",
      "Epoch 919, Loss: 0.5039295330643654, Final Batch Loss: 0.14869137108325958\n",
      "Epoch 920, Loss: 0.5103313401341438, Final Batch Loss: 0.1804833710193634\n",
      "Epoch 921, Loss: 0.514537625014782, Final Batch Loss: 0.09712409228086472\n",
      "Epoch 922, Loss: 0.5720237344503403, Final Batch Loss: 0.17716169357299805\n",
      "Epoch 923, Loss: 0.4686681404709816, Final Batch Loss: 0.10358463227748871\n",
      "Epoch 924, Loss: 0.5612679496407509, Final Batch Loss: 0.178874209523201\n",
      "Epoch 925, Loss: 0.5153261572122574, Final Batch Loss: 0.13813669979572296\n",
      "Epoch 926, Loss: 0.5830615013837814, Final Batch Loss: 0.16035063564777374\n",
      "Epoch 927, Loss: 0.6331066191196442, Final Batch Loss: 0.15029548108577728\n",
      "Epoch 928, Loss: 0.48231596499681473, Final Batch Loss: 0.11354169994592667\n",
      "Epoch 929, Loss: 0.5131138935685158, Final Batch Loss: 0.1550130695104599\n",
      "Epoch 930, Loss: 0.4311370700597763, Final Batch Loss: 0.07315010577440262\n",
      "Epoch 931, Loss: 0.41591788083314896, Final Batch Loss: 0.06842351704835892\n",
      "Epoch 932, Loss: 0.45349443703889847, Final Batch Loss: 0.08659381419420242\n",
      "Epoch 933, Loss: 0.4451168552041054, Final Batch Loss: 0.0704331323504448\n",
      "Epoch 934, Loss: 0.4520370811223984, Final Batch Loss: 0.0996774286031723\n",
      "Epoch 935, Loss: 0.5227719321846962, Final Batch Loss: 0.200760155916214\n",
      "Epoch 936, Loss: 0.43641578406095505, Final Batch Loss: 0.10341783612966537\n",
      "Epoch 937, Loss: 0.5231979787349701, Final Batch Loss: 0.13670669496059418\n",
      "Epoch 938, Loss: 0.438114732503891, Final Batch Loss: 0.08698466420173645\n",
      "Epoch 939, Loss: 0.5490131676197052, Final Batch Loss: 0.1456405520439148\n",
      "Epoch 940, Loss: 0.7747980654239655, Final Batch Loss: 0.33074885606765747\n",
      "Epoch 941, Loss: 0.47222230583429337, Final Batch Loss: 0.11285343766212463\n",
      "Epoch 942, Loss: 0.47801773995161057, Final Batch Loss: 0.13414587080478668\n",
      "Epoch 943, Loss: 0.5455607920885086, Final Batch Loss: 0.11934267729520798\n",
      "Epoch 944, Loss: 0.5616291388869286, Final Batch Loss: 0.14237448573112488\n",
      "Epoch 945, Loss: 0.49968695640563965, Final Batch Loss: 0.09912249445915222\n",
      "Epoch 946, Loss: 0.5186735466122627, Final Batch Loss: 0.12298184633255005\n",
      "Epoch 947, Loss: 0.5363426581025124, Final Batch Loss: 0.16695670783519745\n",
      "Epoch 948, Loss: 0.4765387773513794, Final Batch Loss: 0.06656212359666824\n",
      "Epoch 949, Loss: 0.5036799684166908, Final Batch Loss: 0.1253611147403717\n",
      "Epoch 950, Loss: 0.4715169370174408, Final Batch Loss: 0.08418318629264832\n",
      "Epoch 951, Loss: 0.3988374173641205, Final Batch Loss: 0.06426354497671127\n",
      "Epoch 952, Loss: 0.462459534406662, Final Batch Loss: 0.0990535169839859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 953, Loss: 0.5320549234747887, Final Batch Loss: 0.20507612824440002\n",
      "Epoch 954, Loss: 0.467096783220768, Final Batch Loss: 0.09341415017843246\n",
      "Epoch 955, Loss: 0.5815884843468666, Final Batch Loss: 0.1438932567834854\n",
      "Epoch 956, Loss: 0.5470622852444649, Final Batch Loss: 0.10014606267213821\n",
      "Epoch 957, Loss: 0.4872407019138336, Final Batch Loss: 0.07481309771537781\n",
      "Epoch 958, Loss: 0.5518329292535782, Final Batch Loss: 0.11367335915565491\n",
      "Epoch 959, Loss: 0.4872874319553375, Final Batch Loss: 0.1395566463470459\n",
      "Epoch 960, Loss: 0.48779288679361343, Final Batch Loss: 0.0932343453168869\n",
      "Epoch 961, Loss: 0.44435613602399826, Final Batch Loss: 0.0862078070640564\n",
      "Epoch 962, Loss: 0.5228632092475891, Final Batch Loss: 0.1597072333097458\n",
      "Epoch 963, Loss: 0.5062026977539062, Final Batch Loss: 0.1297287940979004\n",
      "Epoch 964, Loss: 0.4158931188285351, Final Batch Loss: 0.06071386858820915\n",
      "Epoch 965, Loss: 0.5324833691120148, Final Batch Loss: 0.16814042627811432\n",
      "Epoch 966, Loss: 0.5226241946220398, Final Batch Loss: 0.18724946677684784\n",
      "Epoch 967, Loss: 0.4509752541780472, Final Batch Loss: 0.06234388053417206\n",
      "Epoch 968, Loss: 0.5163786336779594, Final Batch Loss: 0.14026835560798645\n",
      "Epoch 969, Loss: 0.46821488440036774, Final Batch Loss: 0.12312892079353333\n",
      "Epoch 970, Loss: 0.5044803693890572, Final Batch Loss: 0.12387208640575409\n",
      "Epoch 971, Loss: 0.5615326017141342, Final Batch Loss: 0.1617843061685562\n",
      "Epoch 972, Loss: 0.5116516724228859, Final Batch Loss: 0.11090188473463058\n",
      "Epoch 973, Loss: 0.5109894275665283, Final Batch Loss: 0.15054966509342194\n",
      "Epoch 974, Loss: 0.44408779591321945, Final Batch Loss: 0.11935383081436157\n",
      "Epoch 975, Loss: 0.4307202324271202, Final Batch Loss: 0.12259823083877563\n",
      "Epoch 976, Loss: 0.43852296471595764, Final Batch Loss: 0.08782981336116791\n",
      "Epoch 977, Loss: 0.5202035456895828, Final Batch Loss: 0.1398564577102661\n",
      "Epoch 978, Loss: 0.5784932747483253, Final Batch Loss: 0.11910023540258408\n",
      "Epoch 979, Loss: 0.39003563672304153, Final Batch Loss: 0.07600075006484985\n",
      "Epoch 980, Loss: 0.5360786765813828, Final Batch Loss: 0.14546321332454681\n",
      "Epoch 981, Loss: 0.4783766493201256, Final Batch Loss: 0.09716557711362839\n",
      "Epoch 982, Loss: 0.43015914410352707, Final Batch Loss: 0.0843377560377121\n",
      "Epoch 983, Loss: 0.49359945207834244, Final Batch Loss: 0.10160703212022781\n",
      "Epoch 984, Loss: 0.5328496396541595, Final Batch Loss: 0.1027793288230896\n",
      "Epoch 985, Loss: 0.5562416464090347, Final Batch Loss: 0.15308822691440582\n",
      "Epoch 986, Loss: 0.4861730933189392, Final Batch Loss: 0.17336426675319672\n",
      "Epoch 987, Loss: 0.5736568868160248, Final Batch Loss: 0.19881238043308258\n",
      "Epoch 988, Loss: 0.40978986769914627, Final Batch Loss: 0.06707745790481567\n",
      "Epoch 989, Loss: 0.4559814855456352, Final Batch Loss: 0.06714978069067001\n",
      "Epoch 990, Loss: 0.7038411796092987, Final Batch Loss: 0.23363104462623596\n",
      "Epoch 991, Loss: 0.5332199409604073, Final Batch Loss: 0.18029449880123138\n",
      "Epoch 992, Loss: 0.4460359811782837, Final Batch Loss: 0.06726054847240448\n",
      "Epoch 993, Loss: 0.5573165267705917, Final Batch Loss: 0.09978269040584564\n",
      "Epoch 994, Loss: 0.49603722244501114, Final Batch Loss: 0.1224171444773674\n",
      "Epoch 995, Loss: 0.4810688719153404, Final Batch Loss: 0.11390253901481628\n",
      "Epoch 996, Loss: 0.5296674072742462, Final Batch Loss: 0.1083754375576973\n",
      "Epoch 997, Loss: 0.5373034328222275, Final Batch Loss: 0.13275155425071716\n",
      "Epoch 998, Loss: 0.41151028871536255, Final Batch Loss: 0.05906304717063904\n",
      "Epoch 999, Loss: 0.44920574873685837, Final Batch Loss: 0.10719286650419235\n",
      "Epoch 1000, Loss: 0.473902128636837, Final Batch Loss: 0.11763762682676315\n",
      "Epoch 1001, Loss: 0.6104576885700226, Final Batch Loss: 0.16627171635627747\n",
      "Epoch 1002, Loss: 0.4803478792309761, Final Batch Loss: 0.06942541152238846\n",
      "Epoch 1003, Loss: 0.5672804266214371, Final Batch Loss: 0.20984195172786713\n",
      "Epoch 1004, Loss: 0.5261141434311867, Final Batch Loss: 0.10696915537118912\n",
      "Epoch 1005, Loss: 0.5844321250915527, Final Batch Loss: 0.1385648399591446\n",
      "Epoch 1006, Loss: 0.48090318590402603, Final Batch Loss: 0.08140689134597778\n",
      "Epoch 1007, Loss: 0.40922922641038895, Final Batch Loss: 0.044980086386203766\n",
      "Epoch 1008, Loss: 0.492684468626976, Final Batch Loss: 0.17053277790546417\n",
      "Epoch 1009, Loss: 0.5042310655117035, Final Batch Loss: 0.08992068469524384\n",
      "Epoch 1010, Loss: 0.4653295427560806, Final Batch Loss: 0.1268828958272934\n",
      "Epoch 1011, Loss: 0.49134107679128647, Final Batch Loss: 0.09873905032873154\n",
      "Epoch 1012, Loss: 0.5414696112275124, Final Batch Loss: 0.1611836552619934\n",
      "Epoch 1013, Loss: 0.48673857748508453, Final Batch Loss: 0.14340868592262268\n",
      "Epoch 1014, Loss: 0.5396518930792809, Final Batch Loss: 0.20071807503700256\n",
      "Epoch 1015, Loss: 0.5285395458340645, Final Batch Loss: 0.1329742968082428\n",
      "Epoch 1016, Loss: 0.446915403008461, Final Batch Loss: 0.15598167479038239\n",
      "Epoch 1017, Loss: 0.5443326011300087, Final Batch Loss: 0.1270962506532669\n",
      "Epoch 1018, Loss: 0.4945712611079216, Final Batch Loss: 0.12468675523996353\n",
      "Epoch 1019, Loss: 0.49387045204639435, Final Batch Loss: 0.1317443698644638\n",
      "Epoch 1020, Loss: 0.5584368035197258, Final Batch Loss: 0.16900166869163513\n",
      "Epoch 1021, Loss: 0.538234293460846, Final Batch Loss: 0.13220682740211487\n",
      "Epoch 1022, Loss: 0.5663498118519783, Final Batch Loss: 0.19022983312606812\n",
      "Epoch 1023, Loss: 0.4979068860411644, Final Batch Loss: 0.14856354892253876\n",
      "Epoch 1024, Loss: 0.5573102682828903, Final Batch Loss: 0.16373313963413239\n",
      "Epoch 1025, Loss: 0.48888296633958817, Final Batch Loss: 0.07418511062860489\n",
      "Epoch 1026, Loss: 0.46608182042837143, Final Batch Loss: 0.06873280555009842\n",
      "Epoch 1027, Loss: 0.47052324563264847, Final Batch Loss: 0.15952306985855103\n",
      "Epoch 1028, Loss: 0.4765058681368828, Final Batch Loss: 0.11469308286905289\n",
      "Epoch 1029, Loss: 0.5860676988959312, Final Batch Loss: 0.1259041279554367\n",
      "Epoch 1030, Loss: 0.5820479989051819, Final Batch Loss: 0.17896094918251038\n",
      "Epoch 1031, Loss: 0.459704153239727, Final Batch Loss: 0.12120287865400314\n",
      "Epoch 1032, Loss: 0.5243795737624168, Final Batch Loss: 0.10482455790042877\n",
      "Epoch 1033, Loss: 0.4809351861476898, Final Batch Loss: 0.11759874224662781\n",
      "Epoch 1034, Loss: 0.46879784762859344, Final Batch Loss: 0.08007745444774628\n",
      "Epoch 1035, Loss: 0.4320540651679039, Final Batch Loss: 0.031105943024158478\n",
      "Epoch 1036, Loss: 0.4645487852394581, Final Batch Loss: 0.058242809027433395\n",
      "Epoch 1037, Loss: 0.4164447635412216, Final Batch Loss: 0.08052455633878708\n",
      "Epoch 1038, Loss: 0.4676354080438614, Final Batch Loss: 0.05536925047636032\n",
      "Epoch 1039, Loss: 0.5188744738698006, Final Batch Loss: 0.1078294888138771\n",
      "Epoch 1040, Loss: 0.44273168593645096, Final Batch Loss: 0.07562238723039627\n",
      "Epoch 1041, Loss: 0.4532402828335762, Final Batch Loss: 0.08531088382005692\n",
      "Epoch 1042, Loss: 0.41513749212026596, Final Batch Loss: 0.0866069346666336\n",
      "Epoch 1043, Loss: 0.46447063982486725, Final Batch Loss: 0.08974578231573105\n",
      "Epoch 1044, Loss: 0.5377289056777954, Final Batch Loss: 0.1594434380531311\n",
      "Epoch 1045, Loss: 0.47382327914237976, Final Batch Loss: 0.1472276896238327\n",
      "Epoch 1046, Loss: 0.5249632969498634, Final Batch Loss: 0.08940771967172623\n",
      "Epoch 1047, Loss: 0.460896797478199, Final Batch Loss: 0.12906749546527863\n",
      "Epoch 1048, Loss: 0.4741178900003433, Final Batch Loss: 0.11713138967752457\n",
      "Epoch 1049, Loss: 0.4486961290240288, Final Batch Loss: 0.08595045655965805\n",
      "Epoch 1050, Loss: 0.5027431324124336, Final Batch Loss: 0.1201443076133728\n",
      "Epoch 1051, Loss: 0.4277340918779373, Final Batch Loss: 0.10692685842514038\n",
      "Epoch 1052, Loss: 0.47413721680641174, Final Batch Loss: 0.11497030407190323\n",
      "Epoch 1053, Loss: 0.40496378764510155, Final Batch Loss: 0.06009162589907646\n",
      "Epoch 1054, Loss: 0.39587222039699554, Final Batch Loss: 0.11187068372964859\n",
      "Epoch 1055, Loss: 0.40564224123954773, Final Batch Loss: 0.050911761820316315\n",
      "Epoch 1056, Loss: 0.5553747564554214, Final Batch Loss: 0.20705515146255493\n",
      "Epoch 1057, Loss: 0.50185926258564, Final Batch Loss: 0.15026260912418365\n",
      "Epoch 1058, Loss: 0.5061845481395721, Final Batch Loss: 0.16376760601997375\n",
      "Epoch 1059, Loss: 0.4421205557882786, Final Batch Loss: 0.11079027503728867\n",
      "Epoch 1060, Loss: 0.464605376124382, Final Batch Loss: 0.15626227855682373\n",
      "Epoch 1061, Loss: 0.38772179186344147, Final Batch Loss: 0.09235593676567078\n",
      "Epoch 1062, Loss: 0.4440442696213722, Final Batch Loss: 0.09899745136499405\n",
      "Epoch 1063, Loss: 0.4448508508503437, Final Batch Loss: 0.061675604432821274\n",
      "Epoch 1064, Loss: 0.4196098819375038, Final Batch Loss: 0.10084494203329086\n",
      "Epoch 1065, Loss: 0.3559149093925953, Final Batch Loss: 0.060724955052137375\n",
      "Epoch 1066, Loss: 0.4720454141497612, Final Batch Loss: 0.07956408709287643\n",
      "Epoch 1067, Loss: 0.512269988656044, Final Batch Loss: 0.08953320980072021\n",
      "Epoch 1068, Loss: 0.48128098994493484, Final Batch Loss: 0.09172334522008896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1069, Loss: 0.4257805123925209, Final Batch Loss: 0.08483708649873734\n",
      "Epoch 1070, Loss: 0.3752516806125641, Final Batch Loss: 0.08652137964963913\n",
      "Epoch 1071, Loss: 0.5027762278914452, Final Batch Loss: 0.16031195223331451\n",
      "Epoch 1072, Loss: 0.47201141715049744, Final Batch Loss: 0.11355549842119217\n",
      "Epoch 1073, Loss: 0.49293383955955505, Final Batch Loss: 0.12920834124088287\n",
      "Epoch 1074, Loss: 0.45302917808294296, Final Batch Loss: 0.1339302510023117\n",
      "Epoch 1075, Loss: 0.439228892326355, Final Batch Loss: 0.07914948463439941\n",
      "Epoch 1076, Loss: 0.4079836532473564, Final Batch Loss: 0.1123666763305664\n",
      "Epoch 1077, Loss: 0.5310115292668343, Final Batch Loss: 0.1510729193687439\n",
      "Epoch 1078, Loss: 0.5023872330784798, Final Batch Loss: 0.12008102983236313\n",
      "Epoch 1079, Loss: 0.4982551112771034, Final Batch Loss: 0.13325096666812897\n",
      "Epoch 1080, Loss: 0.3737694099545479, Final Batch Loss: 0.06927447021007538\n",
      "Epoch 1081, Loss: 0.501884713768959, Final Batch Loss: 0.12792830169200897\n",
      "Epoch 1082, Loss: 0.5040096342563629, Final Batch Loss: 0.13223345577716827\n",
      "Epoch 1083, Loss: 0.4759093336760998, Final Batch Loss: 0.05871604010462761\n",
      "Epoch 1084, Loss: 0.5260186418890953, Final Batch Loss: 0.2213713824748993\n",
      "Epoch 1085, Loss: 0.5273120179772377, Final Batch Loss: 0.22629334032535553\n",
      "Epoch 1086, Loss: 0.509006530046463, Final Batch Loss: 0.1368936151266098\n",
      "Epoch 1087, Loss: 0.4396231397986412, Final Batch Loss: 0.15309903025627136\n",
      "Epoch 1088, Loss: 0.5876539498567581, Final Batch Loss: 0.12090997397899628\n",
      "Epoch 1089, Loss: 0.46776825934648514, Final Batch Loss: 0.09415796399116516\n",
      "Epoch 1090, Loss: 0.5391173735260963, Final Batch Loss: 0.1637669801712036\n",
      "Epoch 1091, Loss: 0.567537747323513, Final Batch Loss: 0.08156432956457138\n",
      "Epoch 1092, Loss: 0.5789739042520523, Final Batch Loss: 0.15139129757881165\n",
      "Epoch 1093, Loss: 0.6852796524763107, Final Batch Loss: 0.24574317038059235\n",
      "Epoch 1094, Loss: 0.5608053654432297, Final Batch Loss: 0.1200309470295906\n",
      "Epoch 1095, Loss: 0.44777312874794006, Final Batch Loss: 0.1372685730457306\n",
      "Epoch 1096, Loss: 0.42681653425097466, Final Batch Loss: 0.04815061762928963\n",
      "Epoch 1097, Loss: 0.4055200591683388, Final Batch Loss: 0.09495317935943604\n",
      "Epoch 1098, Loss: 0.4706770032644272, Final Batch Loss: 0.1130671426653862\n",
      "Epoch 1099, Loss: 0.4164436012506485, Final Batch Loss: 0.13417889177799225\n",
      "Epoch 1100, Loss: 0.4491156116127968, Final Batch Loss: 0.14500537514686584\n",
      "Epoch 1101, Loss: 0.39891317486763, Final Batch Loss: 0.0727342739701271\n",
      "Epoch 1102, Loss: 0.44339217990636826, Final Batch Loss: 0.17110861837863922\n",
      "Epoch 1103, Loss: 0.4180973097681999, Final Batch Loss: 0.08128570765256882\n",
      "Epoch 1104, Loss: 0.47863177210092545, Final Batch Loss: 0.13079507648944855\n",
      "Epoch 1105, Loss: 0.4636717438697815, Final Batch Loss: 0.14845822751522064\n",
      "Epoch 1106, Loss: 0.5400774702429771, Final Batch Loss: 0.13921865820884705\n",
      "Epoch 1107, Loss: 0.4687933772802353, Final Batch Loss: 0.14342202246189117\n",
      "Epoch 1108, Loss: 0.5212288051843643, Final Batch Loss: 0.15066784620285034\n",
      "Epoch 1109, Loss: 0.4250456839799881, Final Batch Loss: 0.06039343774318695\n",
      "Epoch 1110, Loss: 0.5241123139858246, Final Batch Loss: 0.14358362555503845\n",
      "Epoch 1111, Loss: 0.4367845579981804, Final Batch Loss: 0.10787101089954376\n",
      "Epoch 1112, Loss: 0.4189518764615059, Final Batch Loss: 0.0861741155385971\n",
      "Epoch 1113, Loss: 0.45808177441358566, Final Batch Loss: 0.07449185848236084\n",
      "Epoch 1114, Loss: 0.3960754945874214, Final Batch Loss: 0.06732471287250519\n",
      "Epoch 1115, Loss: 0.4934243783354759, Final Batch Loss: 0.17579782009124756\n",
      "Epoch 1116, Loss: 0.32595713809132576, Final Batch Loss: 0.03762708231806755\n",
      "Epoch 1117, Loss: 0.38012074679136276, Final Batch Loss: 0.10700713098049164\n",
      "Epoch 1118, Loss: 0.417935311794281, Final Batch Loss: 0.12678766250610352\n",
      "Epoch 1119, Loss: 0.4261457100510597, Final Batch Loss: 0.08874549716711044\n",
      "Epoch 1120, Loss: 0.433397613465786, Final Batch Loss: 0.08533402532339096\n",
      "Epoch 1121, Loss: 0.42372147738933563, Final Batch Loss: 0.09175906330347061\n",
      "Epoch 1122, Loss: 0.4251534193754196, Final Batch Loss: 0.07069941610097885\n",
      "Epoch 1123, Loss: 0.3755294010043144, Final Batch Loss: 0.10434351861476898\n",
      "Epoch 1124, Loss: 0.3408453091979027, Final Batch Loss: 0.08677469938993454\n",
      "Epoch 1125, Loss: 0.4122186228632927, Final Batch Loss: 0.08052287250757217\n",
      "Epoch 1126, Loss: 0.4064079821109772, Final Batch Loss: 0.13774631917476654\n",
      "Epoch 1127, Loss: 0.3992341086268425, Final Batch Loss: 0.09546521306037903\n",
      "Epoch 1128, Loss: 0.42685071378946304, Final Batch Loss: 0.11293825507164001\n",
      "Epoch 1129, Loss: 0.3779276832938194, Final Batch Loss: 0.10496463626623154\n",
      "Epoch 1130, Loss: 0.4205295071005821, Final Batch Loss: 0.16227367520332336\n",
      "Epoch 1131, Loss: 0.39771918207407, Final Batch Loss: 0.1045791506767273\n",
      "Epoch 1132, Loss: 0.33857889845967293, Final Batch Loss: 0.061408478766679764\n",
      "Epoch 1133, Loss: 0.4519543759524822, Final Batch Loss: 0.044193070381879807\n",
      "Epoch 1134, Loss: 0.45449934154748917, Final Batch Loss: 0.08301526308059692\n",
      "Epoch 1135, Loss: 0.42588377743959427, Final Batch Loss: 0.0963185578584671\n",
      "Epoch 1136, Loss: 0.4016892611980438, Final Batch Loss: 0.08384628593921661\n",
      "Epoch 1137, Loss: 0.45072973892092705, Final Batch Loss: 0.18821536004543304\n",
      "Epoch 1138, Loss: 0.40495988726615906, Final Batch Loss: 0.09187038242816925\n",
      "Epoch 1139, Loss: 0.49762412905693054, Final Batch Loss: 0.17334632575511932\n",
      "Epoch 1140, Loss: 0.4690311998128891, Final Batch Loss: 0.1329835057258606\n",
      "Epoch 1141, Loss: 0.3871866539120674, Final Batch Loss: 0.07446057349443436\n",
      "Epoch 1142, Loss: 0.40749605745077133, Final Batch Loss: 0.08180992305278778\n",
      "Epoch 1143, Loss: 0.4513119235634804, Final Batch Loss: 0.11021114140748978\n",
      "Epoch 1144, Loss: 0.5127269923686981, Final Batch Loss: 0.18996551632881165\n",
      "Epoch 1145, Loss: 0.5719766616821289, Final Batch Loss: 0.23813386261463165\n",
      "Epoch 1146, Loss: 0.3973722383379936, Final Batch Loss: 0.11258325725793839\n",
      "Epoch 1147, Loss: 0.43835682421922684, Final Batch Loss: 0.09851716458797455\n",
      "Epoch 1148, Loss: 0.4132101610302925, Final Batch Loss: 0.08083733916282654\n",
      "Epoch 1149, Loss: 0.43090683966875076, Final Batch Loss: 0.09351395815610886\n",
      "Epoch 1150, Loss: 0.4613058492541313, Final Batch Loss: 0.16896362602710724\n",
      "Epoch 1151, Loss: 0.4574740305542946, Final Batch Loss: 0.1171303391456604\n",
      "Epoch 1152, Loss: 0.5177203565835953, Final Batch Loss: 0.12047021836042404\n",
      "Epoch 1153, Loss: 0.4564671739935875, Final Batch Loss: 0.08758116513490677\n",
      "Epoch 1154, Loss: 0.38340821117162704, Final Batch Loss: 0.09014784544706345\n",
      "Epoch 1155, Loss: 0.3992874249815941, Final Batch Loss: 0.1316695213317871\n",
      "Epoch 1156, Loss: 0.5072711333632469, Final Batch Loss: 0.13216747343540192\n",
      "Epoch 1157, Loss: 0.41512807458639145, Final Batch Loss: 0.11870458722114563\n",
      "Epoch 1158, Loss: 0.6204727217555046, Final Batch Loss: 0.18881548941135406\n",
      "Epoch 1159, Loss: 0.49588022381067276, Final Batch Loss: 0.05244777351617813\n",
      "Epoch 1160, Loss: 0.4250396564602852, Final Batch Loss: 0.10367964953184128\n",
      "Epoch 1161, Loss: 0.4797075539827347, Final Batch Loss: 0.13563217222690582\n",
      "Epoch 1162, Loss: 0.42263974994421005, Final Batch Loss: 0.09721560031175613\n",
      "Epoch 1163, Loss: 0.47413206845521927, Final Batch Loss: 0.12017586827278137\n",
      "Epoch 1164, Loss: 0.4610694572329521, Final Batch Loss: 0.1078762337565422\n",
      "Epoch 1165, Loss: 0.39258700609207153, Final Batch Loss: 0.10663829743862152\n",
      "Epoch 1166, Loss: 0.3672502860426903, Final Batch Loss: 0.07807771116495132\n",
      "Epoch 1167, Loss: 0.4372001364827156, Final Batch Loss: 0.15345044434070587\n",
      "Epoch 1168, Loss: 0.5159393548965454, Final Batch Loss: 0.11060943454504013\n",
      "Epoch 1169, Loss: 0.40993257611989975, Final Batch Loss: 0.10029438883066177\n",
      "Epoch 1170, Loss: 0.45184585452079773, Final Batch Loss: 0.1156797930598259\n",
      "Epoch 1171, Loss: 0.4476834014058113, Final Batch Loss: 0.10782217979431152\n",
      "Epoch 1172, Loss: 0.44476600736379623, Final Batch Loss: 0.11556096374988556\n",
      "Epoch 1173, Loss: 0.4516148790717125, Final Batch Loss: 0.13072320818901062\n",
      "Epoch 1174, Loss: 0.4111357852816582, Final Batch Loss: 0.09162824600934982\n",
      "Epoch 1175, Loss: 0.39266007393598557, Final Batch Loss: 0.06364703178405762\n",
      "Epoch 1176, Loss: 0.49144209921360016, Final Batch Loss: 0.19807277619838715\n",
      "Epoch 1177, Loss: 0.48292309790849686, Final Batch Loss: 0.0917004942893982\n",
      "Epoch 1178, Loss: 0.4849681556224823, Final Batch Loss: 0.11510021984577179\n",
      "Epoch 1179, Loss: 0.4417799413204193, Final Batch Loss: 0.12529966235160828\n",
      "Epoch 1180, Loss: 0.4355026036500931, Final Batch Loss: 0.15649893879890442\n",
      "Epoch 1181, Loss: 0.44053006172180176, Final Batch Loss: 0.13682258129119873\n",
      "Epoch 1182, Loss: 0.386600598692894, Final Batch Loss: 0.06706240773200989\n",
      "Epoch 1183, Loss: 0.4082678332924843, Final Batch Loss: 0.08128312230110168\n",
      "Epoch 1184, Loss: 0.41541779041290283, Final Batch Loss: 0.13512475788593292\n",
      "Epoch 1185, Loss: 0.38649577647447586, Final Batch Loss: 0.09912585467100143\n",
      "Epoch 1186, Loss: 0.34887659549713135, Final Batch Loss: 0.07999271154403687\n",
      "Epoch 1187, Loss: 0.3589879125356674, Final Batch Loss: 0.04409744590520859\n",
      "Epoch 1188, Loss: 0.35455628484487534, Final Batch Loss: 0.07064062356948853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1189, Loss: 0.4067700654268265, Final Batch Loss: 0.1168108806014061\n",
      "Epoch 1190, Loss: 0.45017171651124954, Final Batch Loss: 0.09853558987379074\n",
      "Epoch 1191, Loss: 0.40006791427731514, Final Batch Loss: 0.06203102692961693\n",
      "Epoch 1192, Loss: 0.4224579334259033, Final Batch Loss: 0.09475468099117279\n",
      "Epoch 1193, Loss: 0.5026153549551964, Final Batch Loss: 0.1366019994020462\n",
      "Epoch 1194, Loss: 0.5277951583266258, Final Batch Loss: 0.15736989676952362\n",
      "Epoch 1195, Loss: 0.3857625722885132, Final Batch Loss: 0.10095725208520889\n",
      "Epoch 1196, Loss: 0.6021054461598396, Final Batch Loss: 0.22860918939113617\n",
      "Epoch 1197, Loss: 0.435541607439518, Final Batch Loss: 0.10419203341007233\n",
      "Epoch 1198, Loss: 0.4114806391298771, Final Batch Loss: 0.11267581582069397\n",
      "Epoch 1199, Loss: 0.47707897424697876, Final Batch Loss: 0.16551436483860016\n",
      "Epoch 1200, Loss: 0.345981203019619, Final Batch Loss: 0.09103845059871674\n",
      "Epoch 1201, Loss: 0.4273558184504509, Final Batch Loss: 0.09363563358783722\n",
      "Epoch 1202, Loss: 0.4951270893216133, Final Batch Loss: 0.19901694357395172\n",
      "Epoch 1203, Loss: 0.3467368744313717, Final Batch Loss: 0.047463905066251755\n",
      "Epoch 1204, Loss: 0.4567102640867233, Final Batch Loss: 0.11993134766817093\n",
      "Epoch 1205, Loss: 0.3722655177116394, Final Batch Loss: 0.04352954030036926\n",
      "Epoch 1206, Loss: 0.449628509581089, Final Batch Loss: 0.09756553918123245\n",
      "Epoch 1207, Loss: 0.3368038907647133, Final Batch Loss: 0.05818069726228714\n",
      "Epoch 1208, Loss: 0.3942543566226959, Final Batch Loss: 0.09111546725034714\n",
      "Epoch 1209, Loss: 0.42369863390922546, Final Batch Loss: 0.12859198451042175\n",
      "Epoch 1210, Loss: 0.35959164798259735, Final Batch Loss: 0.11949045211076736\n",
      "Epoch 1211, Loss: 0.4584481939673424, Final Batch Loss: 0.12169171124696732\n",
      "Epoch 1212, Loss: 0.33339981362223625, Final Batch Loss: 0.0575362928211689\n",
      "Epoch 1213, Loss: 0.3474042937159538, Final Batch Loss: 0.05080362409353256\n",
      "Epoch 1214, Loss: 0.44529061019420624, Final Batch Loss: 0.06634967029094696\n",
      "Epoch 1215, Loss: 0.41926201432943344, Final Batch Loss: 0.13609914481639862\n",
      "Epoch 1216, Loss: 0.37354567274451256, Final Batch Loss: 0.043001700192689896\n",
      "Epoch 1217, Loss: 0.515078492462635, Final Batch Loss: 0.2345736175775528\n",
      "Epoch 1218, Loss: 0.43641436845064163, Final Batch Loss: 0.0886053666472435\n",
      "Epoch 1219, Loss: 0.3754316195845604, Final Batch Loss: 0.09489935636520386\n",
      "Epoch 1220, Loss: 0.47934626787900925, Final Batch Loss: 0.10964436829090118\n",
      "Epoch 1221, Loss: 0.4386482313275337, Final Batch Loss: 0.12247318029403687\n",
      "Epoch 1222, Loss: 0.4006422758102417, Final Batch Loss: 0.08065254986286163\n",
      "Epoch 1223, Loss: 0.48506613075733185, Final Batch Loss: 0.1056680753827095\n",
      "Epoch 1224, Loss: 0.3681470975279808, Final Batch Loss: 0.06531001627445221\n",
      "Epoch 1225, Loss: 0.5555106028914452, Final Batch Loss: 0.23241935670375824\n",
      "Epoch 1226, Loss: 0.5148051008582115, Final Batch Loss: 0.13052013516426086\n",
      "Epoch 1227, Loss: 0.43802187591791153, Final Batch Loss: 0.12634339928627014\n",
      "Epoch 1228, Loss: 0.5563910752534866, Final Batch Loss: 0.13959558308124542\n",
      "Epoch 1229, Loss: 0.43382270634174347, Final Batch Loss: 0.10231992602348328\n",
      "Epoch 1230, Loss: 0.4474155232310295, Final Batch Loss: 0.07958859950304031\n",
      "Epoch 1231, Loss: 0.40046975016593933, Final Batch Loss: 0.1047482043504715\n",
      "Epoch 1232, Loss: 0.4866394028067589, Final Batch Loss: 0.20935429632663727\n",
      "Epoch 1233, Loss: 0.44280703365802765, Final Batch Loss: 0.12706409394741058\n",
      "Epoch 1234, Loss: 0.4542228877544403, Final Batch Loss: 0.16751159727573395\n",
      "Epoch 1235, Loss: 0.4453876167535782, Final Batch Loss: 0.12134911119937897\n",
      "Epoch 1236, Loss: 0.4851648733019829, Final Batch Loss: 0.08607732504606247\n",
      "Epoch 1237, Loss: 0.3871503472328186, Final Batch Loss: 0.09543168544769287\n",
      "Epoch 1238, Loss: 0.391107652336359, Final Batch Loss: 0.05341000482439995\n",
      "Epoch 1239, Loss: 0.4490877613425255, Final Batch Loss: 0.17240296304225922\n",
      "Epoch 1240, Loss: 0.44657373428344727, Final Batch Loss: 0.13095639646053314\n",
      "Epoch 1241, Loss: 0.38373062014579773, Final Batch Loss: 0.11277695745229721\n",
      "Epoch 1242, Loss: 0.4781505838036537, Final Batch Loss: 0.1437849998474121\n",
      "Epoch 1243, Loss: 0.42701081186532974, Final Batch Loss: 0.13891316950321198\n",
      "Epoch 1244, Loss: 0.3827672377228737, Final Batch Loss: 0.08072929084300995\n",
      "Epoch 1245, Loss: 0.41436246037483215, Final Batch Loss: 0.08738400042057037\n",
      "Epoch 1246, Loss: 0.36986005678772926, Final Batch Loss: 0.05163487419486046\n",
      "Epoch 1247, Loss: 0.4546828866004944, Final Batch Loss: 0.11857655644416809\n",
      "Epoch 1248, Loss: 0.4032892435789108, Final Batch Loss: 0.07262478023767471\n",
      "Epoch 1249, Loss: 0.4779840484261513, Final Batch Loss: 0.12730196118354797\n",
      "Epoch 1250, Loss: 0.4169272892177105, Final Batch Loss: 0.10097784548997879\n",
      "Epoch 1251, Loss: 0.49622759968042374, Final Batch Loss: 0.0999266728758812\n",
      "Epoch 1252, Loss: 0.4158039167523384, Final Batch Loss: 0.10679683089256287\n",
      "Epoch 1253, Loss: 0.38468628749251366, Final Batch Loss: 0.06187977269291878\n",
      "Epoch 1254, Loss: 0.3460259921848774, Final Batch Loss: 0.05240243300795555\n",
      "Epoch 1255, Loss: 0.4201667383313179, Final Batch Loss: 0.09145963191986084\n",
      "Epoch 1256, Loss: 0.45765676349401474, Final Batch Loss: 0.11008419841527939\n",
      "Epoch 1257, Loss: 0.43444904685020447, Final Batch Loss: 0.07645164430141449\n",
      "Epoch 1258, Loss: 0.34315987676382065, Final Batch Loss: 0.09805937111377716\n",
      "Epoch 1259, Loss: 0.4258875921368599, Final Batch Loss: 0.0678241029381752\n",
      "Epoch 1260, Loss: 0.3801646754145622, Final Batch Loss: 0.0659780278801918\n",
      "Epoch 1261, Loss: 0.38109224289655685, Final Batch Loss: 0.11406519263982773\n",
      "Epoch 1262, Loss: 0.4114449620246887, Final Batch Loss: 0.11120835691690445\n",
      "Epoch 1263, Loss: 0.38189734891057014, Final Batch Loss: 0.04591182991862297\n",
      "Epoch 1264, Loss: 0.3611629977822304, Final Batch Loss: 0.09669824689626694\n",
      "Epoch 1265, Loss: 0.45448464155197144, Final Batch Loss: 0.11013102531433105\n",
      "Epoch 1266, Loss: 0.38355712592601776, Final Batch Loss: 0.06313739717006683\n",
      "Epoch 1267, Loss: 0.369207501411438, Final Batch Loss: 0.08141782879829407\n",
      "Epoch 1268, Loss: 0.45658163726329803, Final Batch Loss: 0.13092409074306488\n",
      "Epoch 1269, Loss: 0.5027848333120346, Final Batch Loss: 0.14428316056728363\n",
      "Epoch 1270, Loss: 0.3840300627052784, Final Batch Loss: 0.0573740117251873\n",
      "Epoch 1271, Loss: 0.4451230950653553, Final Batch Loss: 0.10761701315641403\n",
      "Epoch 1272, Loss: 0.3448041081428528, Final Batch Loss: 0.0384083092212677\n",
      "Epoch 1273, Loss: 0.3914696127176285, Final Batch Loss: 0.08846074342727661\n",
      "Epoch 1274, Loss: 0.4649665728211403, Final Batch Loss: 0.09192629158496857\n",
      "Epoch 1275, Loss: 0.4008367285132408, Final Batch Loss: 0.07497896254062653\n",
      "Epoch 1276, Loss: 0.35797103866934776, Final Batch Loss: 0.07548392564058304\n",
      "Epoch 1277, Loss: 0.37280645966529846, Final Batch Loss: 0.0763673260807991\n",
      "Epoch 1278, Loss: 0.42155008018016815, Final Batch Loss: 0.129326730966568\n",
      "Epoch 1279, Loss: 0.3601994663476944, Final Batch Loss: 0.08337827026844025\n",
      "Epoch 1280, Loss: 0.47599318623542786, Final Batch Loss: 0.12050455063581467\n",
      "Epoch 1281, Loss: 0.3433179743587971, Final Batch Loss: 0.08517781645059586\n",
      "Epoch 1282, Loss: 0.38481996953487396, Final Batch Loss: 0.08909618109464645\n",
      "Epoch 1283, Loss: 0.3254551514983177, Final Batch Loss: 0.061541490256786346\n",
      "Epoch 1284, Loss: 0.42537040635943413, Final Batch Loss: 0.15030306577682495\n",
      "Epoch 1285, Loss: 0.4041810482740402, Final Batch Loss: 0.07683142274618149\n",
      "Epoch 1286, Loss: 0.309240210801363, Final Batch Loss: 0.05650416389107704\n",
      "Epoch 1287, Loss: 0.43519826978445053, Final Batch Loss: 0.1461116224527359\n",
      "Epoch 1288, Loss: 0.2987462356686592, Final Batch Loss: 0.07973679155111313\n",
      "Epoch 1289, Loss: 0.3830527365207672, Final Batch Loss: 0.10694914311170578\n",
      "Epoch 1290, Loss: 0.38020623475313187, Final Batch Loss: 0.07632569223642349\n",
      "Epoch 1291, Loss: 0.379221897572279, Final Batch Loss: 0.04671401157975197\n",
      "Epoch 1292, Loss: 0.35048121958971024, Final Batch Loss: 0.08512184023857117\n",
      "Epoch 1293, Loss: 0.500893160700798, Final Batch Loss: 0.16415034234523773\n",
      "Epoch 1294, Loss: 0.4596868008375168, Final Batch Loss: 0.16665016114711761\n",
      "Epoch 1295, Loss: 0.49106594547629356, Final Batch Loss: 0.1626528799533844\n",
      "Epoch 1296, Loss: 0.4634381905198097, Final Batch Loss: 0.16553065180778503\n",
      "Epoch 1297, Loss: 0.4064785987138748, Final Batch Loss: 0.10687974095344543\n",
      "Epoch 1298, Loss: 0.4151877239346504, Final Batch Loss: 0.08584488183259964\n",
      "Epoch 1299, Loss: 0.4780896157026291, Final Batch Loss: 0.13849987089633942\n",
      "Epoch 1300, Loss: 0.5809331238269806, Final Batch Loss: 0.261542946100235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1301, Loss: 0.46634211391210556, Final Batch Loss: 0.10454079508781433\n",
      "Epoch 1302, Loss: 0.44738566875457764, Final Batch Loss: 0.10484705120325089\n",
      "Epoch 1303, Loss: 0.43840473145246506, Final Batch Loss: 0.07386365532875061\n",
      "Epoch 1304, Loss: 0.41592277586460114, Final Batch Loss: 0.0370926633477211\n",
      "Epoch 1305, Loss: 0.4479351341724396, Final Batch Loss: 0.17242060601711273\n",
      "Epoch 1306, Loss: 0.4490453898906708, Final Batch Loss: 0.11733686178922653\n",
      "Epoch 1307, Loss: 0.47346578538417816, Final Batch Loss: 0.11876790970563889\n",
      "Epoch 1308, Loss: 0.425053708255291, Final Batch Loss: 0.08746637403964996\n",
      "Epoch 1309, Loss: 0.4762599840760231, Final Batch Loss: 0.17627812922000885\n",
      "Epoch 1310, Loss: 0.4110846072435379, Final Batch Loss: 0.10185608267784119\n",
      "Epoch 1311, Loss: 0.4204272925853729, Final Batch Loss: 0.08338576555252075\n",
      "Epoch 1312, Loss: 0.4415370225906372, Final Batch Loss: 0.0818697139620781\n",
      "Epoch 1313, Loss: 0.4283119849860668, Final Batch Loss: 0.050209108740091324\n",
      "Epoch 1314, Loss: 0.33499418944120407, Final Batch Loss: 0.09147270023822784\n",
      "Epoch 1315, Loss: 0.35154686123132706, Final Batch Loss: 0.11547353863716125\n",
      "Epoch 1316, Loss: 0.3585699759423733, Final Batch Loss: 0.09435690194368362\n",
      "Epoch 1317, Loss: 0.3807832710444927, Final Batch Loss: 0.10894599556922913\n",
      "Epoch 1318, Loss: 0.4409772604703903, Final Batch Loss: 0.17166388034820557\n",
      "Epoch 1319, Loss: 0.4029776379466057, Final Batch Loss: 0.12266037613153458\n",
      "Epoch 1320, Loss: 0.3157871216535568, Final Batch Loss: 0.07445941865444183\n",
      "Epoch 1321, Loss: 0.45356335490942, Final Batch Loss: 0.10338746756315231\n",
      "Epoch 1322, Loss: 0.4340762570500374, Final Batch Loss: 0.10714799165725708\n",
      "Epoch 1323, Loss: 0.35324010252952576, Final Batch Loss: 0.055879488587379456\n",
      "Epoch 1324, Loss: 0.36074303463101387, Final Batch Loss: 0.08246784657239914\n",
      "Epoch 1325, Loss: 0.49460695683956146, Final Batch Loss: 0.16542764008045197\n",
      "Epoch 1326, Loss: 0.3508402928709984, Final Batch Loss: 0.06846677511930466\n",
      "Epoch 1327, Loss: 0.3838002234697342, Final Batch Loss: 0.09823904931545258\n",
      "Epoch 1328, Loss: 0.34744857996702194, Final Batch Loss: 0.07901272177696228\n",
      "Epoch 1329, Loss: 0.32848379760980606, Final Batch Loss: 0.12137184292078018\n",
      "Epoch 1330, Loss: 0.3611925095319748, Final Batch Loss: 0.08820383995771408\n",
      "Epoch 1331, Loss: 0.4254670590162277, Final Batch Loss: 0.1484099179506302\n",
      "Epoch 1332, Loss: 0.3499048054218292, Final Batch Loss: 0.07983662188053131\n",
      "Epoch 1333, Loss: 0.413661427795887, Final Batch Loss: 0.08461184799671173\n",
      "Epoch 1334, Loss: 0.3340439349412918, Final Batch Loss: 0.0905391126871109\n",
      "Epoch 1335, Loss: 0.4102808013558388, Final Batch Loss: 0.07111470401287079\n",
      "Epoch 1336, Loss: 0.3816824331879616, Final Batch Loss: 0.09352339059114456\n",
      "Epoch 1337, Loss: 0.345121044665575, Final Batch Loss: 0.0479622520506382\n",
      "Epoch 1338, Loss: 0.4005798399448395, Final Batch Loss: 0.11381466686725616\n",
      "Epoch 1339, Loss: 0.4074680991470814, Final Batch Loss: 0.11877863854169846\n",
      "Epoch 1340, Loss: 0.5161619409918785, Final Batch Loss: 0.20502124726772308\n",
      "Epoch 1341, Loss: 0.3663175664842129, Final Batch Loss: 0.10673902183771133\n",
      "Epoch 1342, Loss: 0.5982470512390137, Final Batch Loss: 0.12730945646762848\n",
      "Epoch 1343, Loss: 0.5087353959679604, Final Batch Loss: 0.15976087749004364\n",
      "Epoch 1344, Loss: 0.5409021228551865, Final Batch Loss: 0.19556580483913422\n",
      "Epoch 1345, Loss: 0.38388898223638535, Final Batch Loss: 0.09542794525623322\n",
      "Epoch 1346, Loss: 0.47062213718891144, Final Batch Loss: 0.11460673063993454\n",
      "Epoch 1347, Loss: 0.5346327647566795, Final Batch Loss: 0.2050098329782486\n",
      "Epoch 1348, Loss: 0.3864988684654236, Final Batch Loss: 0.10158504545688629\n",
      "Epoch 1349, Loss: 0.4006586968898773, Final Batch Loss: 0.12753304839134216\n",
      "Epoch 1350, Loss: 0.34441573172807693, Final Batch Loss: 0.11217884719371796\n",
      "Epoch 1351, Loss: 0.47506799548864365, Final Batch Loss: 0.15760579705238342\n",
      "Epoch 1352, Loss: 0.3948381245136261, Final Batch Loss: 0.12867844104766846\n",
      "Epoch 1353, Loss: 0.46542368829250336, Final Batch Loss: 0.19165219366550446\n",
      "Epoch 1354, Loss: 0.4076821841299534, Final Batch Loss: 0.05348072573542595\n",
      "Epoch 1355, Loss: 0.4663655683398247, Final Batch Loss: 0.1864601969718933\n",
      "Epoch 1356, Loss: 0.3862348645925522, Final Batch Loss: 0.12280186265707016\n",
      "Epoch 1357, Loss: 0.40790145099163055, Final Batch Loss: 0.07936043292284012\n",
      "Epoch 1358, Loss: 0.43126870691776276, Final Batch Loss: 0.1643449068069458\n",
      "Epoch 1359, Loss: 0.43719499558210373, Final Batch Loss: 0.08519895374774933\n",
      "Epoch 1360, Loss: 0.4923427328467369, Final Batch Loss: 0.1166120171546936\n",
      "Epoch 1361, Loss: 0.33287838101387024, Final Batch Loss: 0.054100409150123596\n",
      "Epoch 1362, Loss: 0.4169005826115608, Final Batch Loss: 0.09979370981454849\n",
      "Epoch 1363, Loss: 0.38296474516391754, Final Batch Loss: 0.12203105539083481\n",
      "Epoch 1364, Loss: 0.3585633710026741, Final Batch Loss: 0.042934879660606384\n",
      "Epoch 1365, Loss: 0.38760825619101524, Final Batch Loss: 0.10438618063926697\n",
      "Epoch 1366, Loss: 0.40988052636384964, Final Batch Loss: 0.14722885191440582\n",
      "Epoch 1367, Loss: 0.4134228900074959, Final Batch Loss: 0.08009284734725952\n",
      "Epoch 1368, Loss: 0.4207593873143196, Final Batch Loss: 0.07673237472772598\n",
      "Epoch 1369, Loss: 0.3628178499639034, Final Batch Loss: 0.0618109293282032\n",
      "Epoch 1370, Loss: 0.3677073661237955, Final Batch Loss: 0.02795395441353321\n",
      "Epoch 1371, Loss: 0.3238598257303238, Final Batch Loss: 0.08642206341028214\n",
      "Epoch 1372, Loss: 0.33536297082901, Final Batch Loss: 0.10401959717273712\n",
      "Epoch 1373, Loss: 0.44793643057346344, Final Batch Loss: 0.10460905730724335\n",
      "Epoch 1374, Loss: 0.3466818854212761, Final Batch Loss: 0.04305309057235718\n",
      "Epoch 1375, Loss: 0.38975103199481964, Final Batch Loss: 0.09202691912651062\n",
      "Epoch 1376, Loss: 0.31276029720902443, Final Batch Loss: 0.09477346390485764\n",
      "Epoch 1377, Loss: 0.38290706276893616, Final Batch Loss: 0.09297362715005875\n",
      "Epoch 1378, Loss: 0.3697385936975479, Final Batch Loss: 0.11874997615814209\n",
      "Epoch 1379, Loss: 0.3607906922698021, Final Batch Loss: 0.06845049560070038\n",
      "Epoch 1380, Loss: 0.4149288982152939, Final Batch Loss: 0.08977822959423065\n",
      "Epoch 1381, Loss: 0.4126586280763149, Final Batch Loss: 0.14467230439186096\n",
      "Epoch 1382, Loss: 0.3462200053036213, Final Batch Loss: 0.05555405095219612\n",
      "Epoch 1383, Loss: 0.36718904227018356, Final Batch Loss: 0.09621342271566391\n",
      "Epoch 1384, Loss: 0.43188171088695526, Final Batch Loss: 0.09825000911951065\n",
      "Epoch 1385, Loss: 0.3778104931116104, Final Batch Loss: 0.10438396036624908\n",
      "Epoch 1386, Loss: 0.4166221022605896, Final Batch Loss: 0.08824987709522247\n",
      "Epoch 1387, Loss: 0.4096764400601387, Final Batch Loss: 0.1214015781879425\n",
      "Epoch 1388, Loss: 0.48498155176639557, Final Batch Loss: 0.12162790447473526\n",
      "Epoch 1389, Loss: 0.4010331407189369, Final Batch Loss: 0.08016309142112732\n",
      "Epoch 1390, Loss: 0.42300664633512497, Final Batch Loss: 0.14595262706279755\n",
      "Epoch 1391, Loss: 0.33209339529275894, Final Batch Loss: 0.06731175631284714\n",
      "Epoch 1392, Loss: 0.3935830816626549, Final Batch Loss: 0.08692817389965057\n",
      "Epoch 1393, Loss: 0.3574580103158951, Final Batch Loss: 0.0590776726603508\n",
      "Epoch 1394, Loss: 0.4136398658156395, Final Batch Loss: 0.12710492312908173\n",
      "Epoch 1395, Loss: 0.39893244206905365, Final Batch Loss: 0.08174818754196167\n",
      "Epoch 1396, Loss: 0.36274874210357666, Final Batch Loss: 0.09332969039678574\n",
      "Epoch 1397, Loss: 0.406478613615036, Final Batch Loss: 0.113921158015728\n",
      "Epoch 1398, Loss: 0.44270096719264984, Final Batch Loss: 0.13112589716911316\n",
      "Epoch 1399, Loss: 0.3522389568388462, Final Batch Loss: 0.04944242164492607\n",
      "Epoch 1400, Loss: 0.3779563829302788, Final Batch Loss: 0.11830823868513107\n",
      "Epoch 1401, Loss: 0.4525335878133774, Final Batch Loss: 0.08120562136173248\n",
      "Epoch 1402, Loss: 0.4469548612833023, Final Batch Loss: 0.17998534440994263\n",
      "Epoch 1403, Loss: 0.438787005841732, Final Batch Loss: 0.10968876630067825\n",
      "Epoch 1404, Loss: 0.4678098261356354, Final Batch Loss: 0.23193030059337616\n",
      "Epoch 1405, Loss: 0.4030980169773102, Final Batch Loss: 0.08588922768831253\n",
      "Epoch 1406, Loss: 0.3666309081017971, Final Batch Loss: 0.09018294513225555\n",
      "Epoch 1407, Loss: 0.37818704545497894, Final Batch Loss: 0.09136204421520233\n",
      "Epoch 1408, Loss: 0.3059975802898407, Final Batch Loss: 0.09177790582180023\n",
      "Epoch 1409, Loss: 0.48431937396526337, Final Batch Loss: 0.1255871206521988\n",
      "Epoch 1410, Loss: 0.46135127544403076, Final Batch Loss: 0.07744601368904114\n",
      "Epoch 1411, Loss: 0.3279203325510025, Final Batch Loss: 0.0269986093044281\n",
      "Epoch 1412, Loss: 0.34810785576701164, Final Batch Loss: 0.04358857497572899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1413, Loss: 0.33803997188806534, Final Batch Loss: 0.05301055312156677\n",
      "Epoch 1414, Loss: 0.5018009766936302, Final Batch Loss: 0.1337028294801712\n",
      "Epoch 1415, Loss: 0.3555639088153839, Final Batch Loss: 0.06645699590444565\n",
      "Epoch 1416, Loss: 0.4445248991250992, Final Batch Loss: 0.13232600688934326\n",
      "Epoch 1417, Loss: 0.4478360563516617, Final Batch Loss: 0.19387105107307434\n",
      "Epoch 1418, Loss: 0.4419395178556442, Final Batch Loss: 0.07872216403484344\n",
      "Epoch 1419, Loss: 0.3739909157156944, Final Batch Loss: 0.12873680889606476\n",
      "Epoch 1420, Loss: 0.49210435524582863, Final Batch Loss: 0.11474443972110748\n",
      "Epoch 1421, Loss: 0.33447353169322014, Final Batch Loss: 0.0465003065764904\n",
      "Epoch 1422, Loss: 0.37074557319283485, Final Batch Loss: 0.1450449377298355\n",
      "Epoch 1423, Loss: 0.49628956615924835, Final Batch Loss: 0.067416250705719\n",
      "Epoch 1424, Loss: 0.41425615549087524, Final Batch Loss: 0.14709362387657166\n",
      "Epoch 1425, Loss: 0.3945354074239731, Final Batch Loss: 0.05851566791534424\n",
      "Epoch 1426, Loss: 0.38280464708805084, Final Batch Loss: 0.09184414148330688\n",
      "Epoch 1427, Loss: 0.4378165453672409, Final Batch Loss: 0.13166728615760803\n",
      "Epoch 1428, Loss: 0.34755589067935944, Final Batch Loss: 0.07083488255739212\n",
      "Epoch 1429, Loss: 0.3956643119454384, Final Batch Loss: 0.1122915968298912\n",
      "Epoch 1430, Loss: 0.3059367910027504, Final Batch Loss: 0.0367618203163147\n",
      "Epoch 1431, Loss: 0.34435180574655533, Final Batch Loss: 0.08605523407459259\n",
      "Epoch 1432, Loss: 0.40269965678453445, Final Batch Loss: 0.12950827181339264\n",
      "Epoch 1433, Loss: 0.36271755397319794, Final Batch Loss: 0.07302448153495789\n",
      "Epoch 1434, Loss: 0.4095558822154999, Final Batch Loss: 0.07207947224378586\n",
      "Epoch 1435, Loss: 0.4044174551963806, Final Batch Loss: 0.12762443721294403\n",
      "Epoch 1436, Loss: 0.41148561984300613, Final Batch Loss: 0.14486725628376007\n",
      "Epoch 1437, Loss: 0.34460601955652237, Final Batch Loss: 0.06984148174524307\n",
      "Epoch 1438, Loss: 0.38076310604810715, Final Batch Loss: 0.13486348092556\n",
      "Epoch 1439, Loss: 0.3365978002548218, Final Batch Loss: 0.09238109737634659\n",
      "Epoch 1440, Loss: 0.3856624811887741, Final Batch Loss: 0.07760408520698547\n",
      "Epoch 1441, Loss: 0.4069974683225155, Final Batch Loss: 0.07394386827945709\n",
      "Epoch 1442, Loss: 0.4242948666214943, Final Batch Loss: 0.14168599247932434\n",
      "Epoch 1443, Loss: 0.37740373983979225, Final Batch Loss: 0.04245254024863243\n",
      "Epoch 1444, Loss: 0.4355282336473465, Final Batch Loss: 0.15013907849788666\n",
      "Epoch 1445, Loss: 0.3708038255572319, Final Batch Loss: 0.09332152456045151\n",
      "Epoch 1446, Loss: 0.31267786771059036, Final Batch Loss: 0.08213066309690475\n",
      "Epoch 1447, Loss: 0.3708898648619652, Final Batch Loss: 0.07386839389801025\n",
      "Epoch 1448, Loss: 0.3760845810174942, Final Batch Loss: 0.08827871084213257\n",
      "Epoch 1449, Loss: 0.37294115498661995, Final Batch Loss: 0.10683673620223999\n",
      "Epoch 1450, Loss: 0.35530567914247513, Final Batch Loss: 0.04503774642944336\n",
      "Epoch 1451, Loss: 0.3363794982433319, Final Batch Loss: 0.04366857558488846\n",
      "Epoch 1452, Loss: 0.3793119788169861, Final Batch Loss: 0.12087149918079376\n",
      "Epoch 1453, Loss: 0.43407294154167175, Final Batch Loss: 0.1271161139011383\n",
      "Epoch 1454, Loss: 0.33367910236120224, Final Batch Loss: 0.06567000597715378\n",
      "Epoch 1455, Loss: 0.4047894924879074, Final Batch Loss: 0.10675729066133499\n",
      "Epoch 1456, Loss: 0.32784443721175194, Final Batch Loss: 0.08175268024206161\n",
      "Epoch 1457, Loss: 0.4060356542468071, Final Batch Loss: 0.12546837329864502\n",
      "Epoch 1458, Loss: 0.43744009733200073, Final Batch Loss: 0.11326009035110474\n",
      "Epoch 1459, Loss: 0.44443073868751526, Final Batch Loss: 0.142117440700531\n",
      "Epoch 1460, Loss: 0.3860368989408016, Final Batch Loss: 0.0535709448158741\n",
      "Epoch 1461, Loss: 0.38947486132383347, Final Batch Loss: 0.10373363643884659\n",
      "Epoch 1462, Loss: 0.31847022101283073, Final Batch Loss: 0.04107994958758354\n",
      "Epoch 1463, Loss: 0.4632419943809509, Final Batch Loss: 0.13581739366054535\n",
      "Epoch 1464, Loss: 0.44076433777809143, Final Batch Loss: 0.10719490051269531\n",
      "Epoch 1465, Loss: 0.41453714668750763, Final Batch Loss: 0.09371420741081238\n",
      "Epoch 1466, Loss: 0.418095625936985, Final Batch Loss: 0.09298435598611832\n",
      "Epoch 1467, Loss: 0.4270613193511963, Final Batch Loss: 0.12624627351760864\n",
      "Epoch 1468, Loss: 0.36008844524621964, Final Batch Loss: 0.095354825258255\n",
      "Epoch 1469, Loss: 0.3517096936702728, Final Batch Loss: 0.07651364803314209\n",
      "Epoch 1470, Loss: 0.3414387106895447, Final Batch Loss: 0.05099308490753174\n",
      "Epoch 1471, Loss: 0.30629731342196465, Final Batch Loss: 0.061139147728681564\n",
      "Epoch 1472, Loss: 0.34943726658821106, Final Batch Loss: 0.1221158504486084\n",
      "Epoch 1473, Loss: 0.4836098700761795, Final Batch Loss: 0.15535645186901093\n",
      "Epoch 1474, Loss: 0.31856735050678253, Final Batch Loss: 0.04964292794466019\n",
      "Epoch 1475, Loss: 0.4316335916519165, Final Batch Loss: 0.09136194735765457\n",
      "Epoch 1476, Loss: 0.29149050265550613, Final Batch Loss: 0.09967634081840515\n",
      "Epoch 1477, Loss: 0.3316444382071495, Final Batch Loss: 0.05823349580168724\n",
      "Epoch 1478, Loss: 0.4240308701992035, Final Batch Loss: 0.10033922642469406\n",
      "Epoch 1479, Loss: 0.3464835025370121, Final Batch Loss: 0.0787728875875473\n",
      "Epoch 1480, Loss: 0.3962983340024948, Final Batch Loss: 0.10189352184534073\n",
      "Epoch 1481, Loss: 0.4070163071155548, Final Batch Loss: 0.09352094680070877\n",
      "Epoch 1482, Loss: 0.3772708624601364, Final Batch Loss: 0.1183045282959938\n",
      "Epoch 1483, Loss: 0.4007805734872818, Final Batch Loss: 0.09386217594146729\n",
      "Epoch 1484, Loss: 0.36309245228767395, Final Batch Loss: 0.07090955972671509\n",
      "Epoch 1485, Loss: 0.4341968521475792, Final Batch Loss: 0.1861242800951004\n",
      "Epoch 1486, Loss: 0.46897218376398087, Final Batch Loss: 0.09129524976015091\n",
      "Epoch 1487, Loss: 0.4864828735589981, Final Batch Loss: 0.097489133477211\n",
      "Epoch 1488, Loss: 0.41007445752620697, Final Batch Loss: 0.1073141098022461\n",
      "Epoch 1489, Loss: 0.4201979488134384, Final Batch Loss: 0.06831815093755722\n",
      "Epoch 1490, Loss: 0.41750359162688255, Final Batch Loss: 0.05909733846783638\n",
      "Epoch 1491, Loss: 0.35691146552562714, Final Batch Loss: 0.06860548257827759\n",
      "Epoch 1492, Loss: 0.37295422330498695, Final Batch Loss: 0.10781912505626678\n",
      "Epoch 1493, Loss: 0.39645859971642494, Final Batch Loss: 0.060817915946245193\n",
      "Epoch 1494, Loss: 0.40628769248723984, Final Batch Loss: 0.08616090565919876\n",
      "Epoch 1495, Loss: 0.4754984453320503, Final Batch Loss: 0.12936004996299744\n",
      "Epoch 1496, Loss: 0.3239431008696556, Final Batch Loss: 0.08812154084444046\n",
      "Epoch 1497, Loss: 0.37099794298410416, Final Batch Loss: 0.11842121183872223\n",
      "Epoch 1498, Loss: 0.30964743345975876, Final Batch Loss: 0.05383255332708359\n",
      "Epoch 1499, Loss: 0.40939728170633316, Final Batch Loss: 0.07907050848007202\n",
      "Epoch 1500, Loss: 0.2808196395635605, Final Batch Loss: 0.04470415785908699\n",
      "Epoch 1501, Loss: 0.3491014465689659, Final Batch Loss: 0.08828490227460861\n",
      "Epoch 1502, Loss: 0.35980311036109924, Final Batch Loss: 0.06498690694570541\n",
      "Epoch 1503, Loss: 0.3426496237516403, Final Batch Loss: 0.08336346596479416\n",
      "Epoch 1504, Loss: 0.341329850256443, Final Batch Loss: 0.06259936839342117\n",
      "Epoch 1505, Loss: 0.3577697202563286, Final Batch Loss: 0.06445474177598953\n",
      "Epoch 1506, Loss: 0.42804136127233505, Final Batch Loss: 0.1446317583322525\n",
      "Epoch 1507, Loss: 0.3795870505273342, Final Batch Loss: 0.034914132207632065\n",
      "Epoch 1508, Loss: 0.4332413300871849, Final Batch Loss: 0.13523618876934052\n",
      "Epoch 1509, Loss: 0.39292190968990326, Final Batch Loss: 0.09323128312826157\n",
      "Epoch 1510, Loss: 0.3344736360013485, Final Batch Loss: 0.05594458058476448\n",
      "Epoch 1511, Loss: 0.4305211827158928, Final Batch Loss: 0.0922127217054367\n",
      "Epoch 1512, Loss: 0.3266945257782936, Final Batch Loss: 0.10585540533065796\n",
      "Epoch 1513, Loss: 0.3854482099413872, Final Batch Loss: 0.10005232691764832\n",
      "Epoch 1514, Loss: 0.31010739132761955, Final Batch Loss: 0.05528350546956062\n",
      "Epoch 1515, Loss: 0.32825884222984314, Final Batch Loss: 0.07195888459682465\n",
      "Epoch 1516, Loss: 0.36783481389284134, Final Batch Loss: 0.06339561194181442\n",
      "Epoch 1517, Loss: 0.3358634486794472, Final Batch Loss: 0.0837441086769104\n",
      "Epoch 1518, Loss: 0.43327324092388153, Final Batch Loss: 0.13160580396652222\n",
      "Epoch 1519, Loss: 0.42047280073165894, Final Batch Loss: 0.11583852022886276\n",
      "Epoch 1520, Loss: 0.3306492865085602, Final Batch Loss: 0.05819261074066162\n",
      "Epoch 1521, Loss: 0.4044870510697365, Final Batch Loss: 0.13191848993301392\n",
      "Epoch 1522, Loss: 0.4332035332918167, Final Batch Loss: 0.16183985769748688\n",
      "Epoch 1523, Loss: 0.4031953364610672, Final Batch Loss: 0.11188071966171265\n",
      "Epoch 1524, Loss: 0.3514614962041378, Final Batch Loss: 0.06145769730210304\n",
      "Epoch 1525, Loss: 0.38692206144332886, Final Batch Loss: 0.08808375895023346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1526, Loss: 0.44901619106531143, Final Batch Loss: 0.0989912897348404\n",
      "Epoch 1527, Loss: 0.44678911566734314, Final Batch Loss: 0.19942495226860046\n",
      "Epoch 1528, Loss: 0.34350624680519104, Final Batch Loss: 0.08322411775588989\n",
      "Epoch 1529, Loss: 0.3884795531630516, Final Batch Loss: 0.09036765247583389\n",
      "Epoch 1530, Loss: 0.45895230025053024, Final Batch Loss: 0.06816249340772629\n",
      "Epoch 1531, Loss: 0.3278815895318985, Final Batch Loss: 0.0776500403881073\n",
      "Epoch 1532, Loss: 0.3083369508385658, Final Batch Loss: 0.042957499623298645\n",
      "Epoch 1533, Loss: 0.36497409641742706, Final Batch Loss: 0.07921552658081055\n",
      "Epoch 1534, Loss: 0.2723562531173229, Final Batch Loss: 0.04644545912742615\n",
      "Epoch 1535, Loss: 0.34884168952703476, Final Batch Loss: 0.12402858585119247\n",
      "Epoch 1536, Loss: 0.36535850167274475, Final Batch Loss: 0.11775101721286774\n",
      "Epoch 1537, Loss: 0.43928421288728714, Final Batch Loss: 0.19212862849235535\n",
      "Epoch 1538, Loss: 0.32837918028235435, Final Batch Loss: 0.05167592689394951\n",
      "Epoch 1539, Loss: 0.35723643749952316, Final Batch Loss: 0.1124006137251854\n",
      "Epoch 1540, Loss: 0.3314811587333679, Final Batch Loss: 0.06541761755943298\n",
      "Epoch 1541, Loss: 0.3848787099123001, Final Batch Loss: 0.1374429613351822\n",
      "Epoch 1542, Loss: 0.33242327719926834, Final Batch Loss: 0.10449803620576859\n",
      "Epoch 1543, Loss: 0.308557353913784, Final Batch Loss: 0.06147482991218567\n",
      "Epoch 1544, Loss: 0.3854738026857376, Final Batch Loss: 0.07788487523794174\n",
      "Epoch 1545, Loss: 0.314658310264349, Final Batch Loss: 0.05736770108342171\n",
      "Epoch 1546, Loss: 0.380474753677845, Final Batch Loss: 0.08738598227500916\n",
      "Epoch 1547, Loss: 0.3959726095199585, Final Batch Loss: 0.09870962798595428\n",
      "Epoch 1548, Loss: 0.30567745119333267, Final Batch Loss: 0.07691913098096848\n",
      "Epoch 1549, Loss: 0.3356000781059265, Final Batch Loss: 0.06693334132432938\n",
      "Epoch 1550, Loss: 0.3583562672138214, Final Batch Loss: 0.08538132160902023\n",
      "Epoch 1551, Loss: 0.3654732033610344, Final Batch Loss: 0.08469130098819733\n",
      "Epoch 1552, Loss: 0.38184013962745667, Final Batch Loss: 0.045218415558338165\n",
      "Epoch 1553, Loss: 0.2845582403242588, Final Batch Loss: 0.0532822348177433\n",
      "Epoch 1554, Loss: 0.35605987906455994, Final Batch Loss: 0.09638169407844543\n",
      "Epoch 1555, Loss: 0.37844589352607727, Final Batch Loss: 0.10256967693567276\n",
      "Epoch 1556, Loss: 0.3578673005104065, Final Batch Loss: 0.12029655277729034\n",
      "Epoch 1557, Loss: 0.2998974807560444, Final Batch Loss: 0.06899841129779816\n",
      "Epoch 1558, Loss: 0.31062063202261925, Final Batch Loss: 0.05607623979449272\n",
      "Epoch 1559, Loss: 0.42390797287225723, Final Batch Loss: 0.16270840167999268\n",
      "Epoch 1560, Loss: 0.4409804195165634, Final Batch Loss: 0.10134761035442352\n",
      "Epoch 1561, Loss: 0.3034266009926796, Final Batch Loss: 0.055717915296554565\n",
      "Epoch 1562, Loss: 0.47380510717630386, Final Batch Loss: 0.1000528335571289\n",
      "Epoch 1563, Loss: 0.3699311837553978, Final Batch Loss: 0.09883978217840195\n",
      "Epoch 1564, Loss: 0.2505752369761467, Final Batch Loss: 0.039035119116306305\n",
      "Epoch 1565, Loss: 0.3600565642118454, Final Batch Loss: 0.06172845512628555\n",
      "Epoch 1566, Loss: 0.3668493330478668, Final Batch Loss: 0.08280125260353088\n",
      "Epoch 1567, Loss: 0.447109367698431, Final Batch Loss: 0.24321287870407104\n",
      "Epoch 1568, Loss: 0.3812096230685711, Final Batch Loss: 0.12038333714008331\n",
      "Epoch 1569, Loss: 0.2878587655723095, Final Batch Loss: 0.06136194244027138\n",
      "Epoch 1570, Loss: 0.4129675477743149, Final Batch Loss: 0.11905425786972046\n",
      "Epoch 1571, Loss: 0.40992728620767593, Final Batch Loss: 0.08226768672466278\n",
      "Epoch 1572, Loss: 0.4301019050180912, Final Batch Loss: 0.14781296253204346\n",
      "Epoch 1573, Loss: 0.43281757086515427, Final Batch Loss: 0.14057180285453796\n",
      "Epoch 1574, Loss: 0.37228985130786896, Final Batch Loss: 0.10555116087198257\n",
      "Epoch 1575, Loss: 0.5014996975660324, Final Batch Loss: 0.0766315907239914\n",
      "Epoch 1576, Loss: 0.36167657375335693, Final Batch Loss: 0.09852629154920578\n",
      "Epoch 1577, Loss: 0.354933463037014, Final Batch Loss: 0.09793951362371445\n",
      "Epoch 1578, Loss: 0.2752090133726597, Final Batch Loss: 0.03212273120880127\n",
      "Epoch 1579, Loss: 0.39438546448946, Final Batch Loss: 0.14211489260196686\n",
      "Epoch 1580, Loss: 0.3362347073853016, Final Batch Loss: 0.08527573198080063\n",
      "Epoch 1581, Loss: 0.30131029337644577, Final Batch Loss: 0.07243547588586807\n",
      "Epoch 1582, Loss: 0.46761591732501984, Final Batch Loss: 0.11100776493549347\n",
      "Epoch 1583, Loss: 0.27977034635841846, Final Batch Loss: 0.02023574896156788\n",
      "Epoch 1584, Loss: 0.40076278895139694, Final Batch Loss: 0.06960012018680573\n",
      "Epoch 1585, Loss: 0.3548812046647072, Final Batch Loss: 0.07401154935359955\n",
      "Epoch 1586, Loss: 0.48215633630752563, Final Batch Loss: 0.16520915925502777\n",
      "Epoch 1587, Loss: 0.3208925276994705, Final Batch Loss: 0.03318800777196884\n",
      "Epoch 1588, Loss: 0.4500650390982628, Final Batch Loss: 0.07697837799787521\n",
      "Epoch 1589, Loss: 0.3993857763707638, Final Batch Loss: 0.1284847855567932\n",
      "Epoch 1590, Loss: 0.34184108674526215, Final Batch Loss: 0.0751776248216629\n",
      "Epoch 1591, Loss: 0.28677142783999443, Final Batch Loss: 0.05972421541810036\n",
      "Epoch 1592, Loss: 0.5123365670442581, Final Batch Loss: 0.17090018093585968\n",
      "Epoch 1593, Loss: 0.35533307492733, Final Batch Loss: 0.07074016332626343\n",
      "Epoch 1594, Loss: 0.3935233876109123, Final Batch Loss: 0.06579945981502533\n",
      "Epoch 1595, Loss: 0.3761509656906128, Final Batch Loss: 0.0834229588508606\n",
      "Epoch 1596, Loss: 0.4505978524684906, Final Batch Loss: 0.15400125086307526\n",
      "Epoch 1597, Loss: 0.4511406011879444, Final Batch Loss: 0.18911755084991455\n",
      "Epoch 1598, Loss: 0.42936767637729645, Final Batch Loss: 0.11646569520235062\n",
      "Epoch 1599, Loss: 0.37956349179148674, Final Batch Loss: 0.060090068727731705\n",
      "Epoch 1600, Loss: 0.34176620841026306, Final Batch Loss: 0.08340591937303543\n",
      "Epoch 1601, Loss: 0.32295023649930954, Final Batch Loss: 0.07317006587982178\n",
      "Epoch 1602, Loss: 0.2349686473608017, Final Batch Loss: 0.027522828429937363\n",
      "Epoch 1603, Loss: 0.31057702377438545, Final Batch Loss: 0.044078875333070755\n",
      "Epoch 1604, Loss: 0.34536752104759216, Final Batch Loss: 0.07391437143087387\n",
      "Epoch 1605, Loss: 0.3021012544631958, Final Batch Loss: 0.1030438095331192\n",
      "Epoch 1606, Loss: 0.25361058861017227, Final Batch Loss: 0.048979710787534714\n",
      "Epoch 1607, Loss: 0.32612134143710136, Final Batch Loss: 0.10188355296850204\n",
      "Epoch 1608, Loss: 0.2851182892918587, Final Batch Loss: 0.09192926436662674\n",
      "Epoch 1609, Loss: 0.324617862701416, Final Batch Loss: 0.0918176993727684\n",
      "Epoch 1610, Loss: 0.25122304633259773, Final Batch Loss: 0.03603477403521538\n",
      "Epoch 1611, Loss: 0.3672094792127609, Final Batch Loss: 0.0945521742105484\n",
      "Epoch 1612, Loss: 0.28992246463894844, Final Batch Loss: 0.032244157046079636\n",
      "Epoch 1613, Loss: 0.426084466278553, Final Batch Loss: 0.1612236648797989\n",
      "Epoch 1614, Loss: 0.3010089322924614, Final Batch Loss: 0.10393673926591873\n",
      "Epoch 1615, Loss: 0.39208944141864777, Final Batch Loss: 0.1525706797838211\n",
      "Epoch 1616, Loss: 0.33908262848854065, Final Batch Loss: 0.05973845720291138\n",
      "Epoch 1617, Loss: 0.4039941839873791, Final Batch Loss: 0.18563085794448853\n",
      "Epoch 1618, Loss: 0.2551647797226906, Final Batch Loss: 0.031528107821941376\n",
      "Epoch 1619, Loss: 0.33788109570741653, Final Batch Loss: 0.08049764484167099\n",
      "Epoch 1620, Loss: 0.3391353636980057, Final Batch Loss: 0.1178758442401886\n",
      "Epoch 1621, Loss: 0.42165157198905945, Final Batch Loss: 0.1570589691400528\n",
      "Epoch 1622, Loss: 0.40145861357450485, Final Batch Loss: 0.0759710967540741\n",
      "Epoch 1623, Loss: 0.2839657627046108, Final Batch Loss: 0.041213471442461014\n",
      "Epoch 1624, Loss: 0.2947546914219856, Final Batch Loss: 0.09326868504285812\n",
      "Epoch 1625, Loss: 0.33674344420433044, Final Batch Loss: 0.09924329817295074\n",
      "Epoch 1626, Loss: 0.3670240491628647, Final Batch Loss: 0.0949045866727829\n",
      "Epoch 1627, Loss: 0.4442622810602188, Final Batch Loss: 0.12551197409629822\n",
      "Epoch 1628, Loss: 0.38445892557501793, Final Batch Loss: 0.17241667211055756\n",
      "Epoch 1629, Loss: 0.5580139309167862, Final Batch Loss: 0.1843683421611786\n",
      "Epoch 1630, Loss: 0.3592410013079643, Final Batch Loss: 0.11088043451309204\n",
      "Epoch 1631, Loss: 0.34526534378528595, Final Batch Loss: 0.05834886431694031\n",
      "Epoch 1632, Loss: 0.3496102802455425, Final Batch Loss: 0.052286986261606216\n",
      "Epoch 1633, Loss: 0.3963273838162422, Final Batch Loss: 0.10775616765022278\n",
      "Epoch 1634, Loss: 0.3245070092380047, Final Batch Loss: 0.050439272075891495\n",
      "Epoch 1635, Loss: 0.3438461199402809, Final Batch Loss: 0.08809476345777512\n",
      "Epoch 1636, Loss: 0.3102387525141239, Final Batch Loss: 0.07125754654407501\n",
      "Epoch 1637, Loss: 0.30608511343598366, Final Batch Loss: 0.051989030092954636\n",
      "Epoch 1638, Loss: 0.26131996139883995, Final Batch Loss: 0.050202179700136185\n",
      "Epoch 1639, Loss: 0.28887834027409554, Final Batch Loss: 0.0701867863535881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1640, Loss: 0.29798073694109917, Final Batch Loss: 0.039609793573617935\n",
      "Epoch 1641, Loss: 0.3830718323588371, Final Batch Loss: 0.06258870661258698\n",
      "Epoch 1642, Loss: 0.2689323388040066, Final Batch Loss: 0.0331566296517849\n",
      "Epoch 1643, Loss: 0.3356315493583679, Final Batch Loss: 0.0694466382265091\n",
      "Epoch 1644, Loss: 0.26679252833127975, Final Batch Loss: 0.05741576850414276\n",
      "Epoch 1645, Loss: 0.2636874094605446, Final Batch Loss: 0.05569721758365631\n",
      "Epoch 1646, Loss: 0.3378465324640274, Final Batch Loss: 0.07484180480241776\n",
      "Epoch 1647, Loss: 0.3335324563086033, Final Batch Loss: 0.08088557422161102\n",
      "Epoch 1648, Loss: 0.40118129551410675, Final Batch Loss: 0.16233450174331665\n",
      "Epoch 1649, Loss: 0.3271850273013115, Final Batch Loss: 0.09048252552747726\n",
      "Epoch 1650, Loss: 0.3366530407220125, Final Batch Loss: 0.02835717238485813\n",
      "Epoch 1651, Loss: 0.29514825716614723, Final Batch Loss: 0.07878434658050537\n",
      "Epoch 1652, Loss: 0.34765639528632164, Final Batch Loss: 0.05807947739958763\n",
      "Epoch 1653, Loss: 0.29268716275691986, Final Batch Loss: 0.08074868470430374\n",
      "Epoch 1654, Loss: 0.3620208539068699, Final Batch Loss: 0.13066384196281433\n",
      "Epoch 1655, Loss: 0.39299993216991425, Final Batch Loss: 0.08905293047428131\n",
      "Epoch 1656, Loss: 0.30511701107025146, Final Batch Loss: 0.06086520850658417\n",
      "Epoch 1657, Loss: 0.34491103142499924, Final Batch Loss: 0.06819263845682144\n",
      "Epoch 1658, Loss: 0.38758768141269684, Final Batch Loss: 0.12428995221853256\n",
      "Epoch 1659, Loss: 0.2674659062176943, Final Batch Loss: 0.026180816814303398\n",
      "Epoch 1660, Loss: 0.3331909030675888, Final Batch Loss: 0.09041714668273926\n",
      "Epoch 1661, Loss: 0.26204201951622963, Final Batch Loss: 0.04734848439693451\n",
      "Epoch 1662, Loss: 0.24492131173610687, Final Batch Loss: 0.04714036360383034\n",
      "Epoch 1663, Loss: 0.3228783681988716, Final Batch Loss: 0.024547964334487915\n",
      "Epoch 1664, Loss: 0.34056737646460533, Final Batch Loss: 0.09182904660701752\n",
      "Epoch 1665, Loss: 0.31608084216713905, Final Batch Loss: 0.05200030282139778\n",
      "Epoch 1666, Loss: 0.2895233742892742, Final Batch Loss: 0.04698305204510689\n",
      "Epoch 1667, Loss: 0.3838520348072052, Final Batch Loss: 0.13915733993053436\n",
      "Epoch 1668, Loss: 0.3109496831893921, Final Batch Loss: 0.039145976305007935\n",
      "Epoch 1669, Loss: 0.2681938074529171, Final Batch Loss: 0.07853344827890396\n",
      "Epoch 1670, Loss: 0.3178331181406975, Final Batch Loss: 0.0794929787516594\n",
      "Epoch 1671, Loss: 0.3467610999941826, Final Batch Loss: 0.11260247230529785\n",
      "Epoch 1672, Loss: 0.34087181091308594, Final Batch Loss: 0.05398575961589813\n",
      "Epoch 1673, Loss: 0.3157673478126526, Final Batch Loss: 0.07851023972034454\n",
      "Epoch 1674, Loss: 0.3328604996204376, Final Batch Loss: 0.025448806583881378\n",
      "Epoch 1675, Loss: 0.47106798738241196, Final Batch Loss: 0.20796939730644226\n",
      "Epoch 1676, Loss: 0.46180960536003113, Final Batch Loss: 0.14461539685726166\n",
      "Epoch 1677, Loss: 0.5767369270324707, Final Batch Loss: 0.15199331939220428\n",
      "Epoch 1678, Loss: 0.47019147127866745, Final Batch Loss: 0.06026092916727066\n",
      "Epoch 1679, Loss: 0.40028003603219986, Final Batch Loss: 0.10686108469963074\n",
      "Epoch 1680, Loss: 0.41079651564359665, Final Batch Loss: 0.0912923514842987\n",
      "Epoch 1681, Loss: 0.38827476650476456, Final Batch Loss: 0.09881225228309631\n",
      "Epoch 1682, Loss: 0.3434082642197609, Final Batch Loss: 0.06489478051662445\n",
      "Epoch 1683, Loss: 0.3438040316104889, Final Batch Loss: 0.09634262323379517\n",
      "Epoch 1684, Loss: 0.41028333455324173, Final Batch Loss: 0.1108626276254654\n",
      "Epoch 1685, Loss: 0.3454709053039551, Final Batch Loss: 0.059432610869407654\n",
      "Epoch 1686, Loss: 0.4141889810562134, Final Batch Loss: 0.1051233634352684\n",
      "Epoch 1687, Loss: 0.33368436992168427, Final Batch Loss: 0.06774505972862244\n",
      "Epoch 1688, Loss: 0.37245237827301025, Final Batch Loss: 0.04987509548664093\n",
      "Epoch 1689, Loss: 0.3192882090806961, Final Batch Loss: 0.06810130178928375\n",
      "Epoch 1690, Loss: 0.3128621485084295, Final Batch Loss: 0.0236309003084898\n",
      "Epoch 1691, Loss: 0.2869993820786476, Final Batch Loss: 0.07528100907802582\n",
      "Epoch 1692, Loss: 0.3934590220451355, Final Batch Loss: 0.07878933846950531\n",
      "Epoch 1693, Loss: 0.44149869680404663, Final Batch Loss: 0.1938733160495758\n",
      "Epoch 1694, Loss: 0.2538256458938122, Final Batch Loss: 0.046140577644109726\n",
      "Epoch 1695, Loss: 0.3509705364704132, Final Batch Loss: 0.12341975420713425\n",
      "Epoch 1696, Loss: 0.3085130453109741, Final Batch Loss: 0.0861901119351387\n",
      "Epoch 1697, Loss: 0.5746818482875824, Final Batch Loss: 0.32432040572166443\n",
      "Epoch 1698, Loss: 0.3226463980972767, Final Batch Loss: 0.03858703747391701\n",
      "Epoch 1699, Loss: 0.4057892635464668, Final Batch Loss: 0.10691659897565842\n",
      "Epoch 1700, Loss: 0.32495690137147903, Final Batch Loss: 0.09096557646989822\n",
      "Epoch 1701, Loss: 0.33037056401371956, Final Batch Loss: 0.0470154695212841\n",
      "Epoch 1702, Loss: 0.4193909615278244, Final Batch Loss: 0.16321147978305817\n",
      "Epoch 1703, Loss: 0.30130014941096306, Final Batch Loss: 0.040271658450365067\n",
      "Epoch 1704, Loss: 0.4907345473766327, Final Batch Loss: 0.1467190980911255\n",
      "Epoch 1705, Loss: 0.31656553596258163, Final Batch Loss: 0.07222965359687805\n",
      "Epoch 1706, Loss: 0.361796073615551, Final Batch Loss: 0.08189737051725388\n",
      "Epoch 1707, Loss: 0.3089294619858265, Final Batch Loss: 0.05408062785863876\n",
      "Epoch 1708, Loss: 0.4027612432837486, Final Batch Loss: 0.07824361324310303\n",
      "Epoch 1709, Loss: 0.3291814923286438, Final Batch Loss: 0.11175601929426193\n",
      "Epoch 1710, Loss: 0.35364922881126404, Final Batch Loss: 0.08909153938293457\n",
      "Epoch 1711, Loss: 0.2947215959429741, Final Batch Loss: 0.06660547107458115\n",
      "Epoch 1712, Loss: 0.31901149451732635, Final Batch Loss: 0.10757993161678314\n",
      "Epoch 1713, Loss: 0.2937065325677395, Final Batch Loss: 0.03773942217230797\n",
      "Epoch 1714, Loss: 0.3296815790235996, Final Batch Loss: 0.05248814448714256\n",
      "Epoch 1715, Loss: 0.39883314818143845, Final Batch Loss: 0.14594876766204834\n",
      "Epoch 1716, Loss: 0.28257159516215324, Final Batch Loss: 0.05726747587323189\n",
      "Epoch 1717, Loss: 0.3420015797019005, Final Batch Loss: 0.04397042095661163\n",
      "Epoch 1718, Loss: 0.3381725586950779, Final Batch Loss: 0.06763742864131927\n",
      "Epoch 1719, Loss: 0.3460850492119789, Final Batch Loss: 0.10686583071947098\n",
      "Epoch 1720, Loss: 0.2891274932771921, Final Batch Loss: 0.0905371680855751\n",
      "Epoch 1721, Loss: 0.3018325939774513, Final Batch Loss: 0.07700856029987335\n",
      "Epoch 1722, Loss: 0.3189774490892887, Final Batch Loss: 0.09203115105628967\n",
      "Epoch 1723, Loss: 0.26474183797836304, Final Batch Loss: 0.07583865523338318\n",
      "Epoch 1724, Loss: 0.26758047938346863, Final Batch Loss: 0.04320962727069855\n",
      "Epoch 1725, Loss: 0.46213121712207794, Final Batch Loss: 0.11967269331216812\n",
      "Epoch 1726, Loss: 0.3508215621113777, Final Batch Loss: 0.12656816840171814\n",
      "Epoch 1727, Loss: 0.2890741154551506, Final Batch Loss: 0.08501659333705902\n",
      "Epoch 1728, Loss: 0.2774052917957306, Final Batch Loss: 0.03741646930575371\n",
      "Epoch 1729, Loss: 0.3353569619357586, Final Batch Loss: 0.09588560461997986\n",
      "Epoch 1730, Loss: 0.3656325116753578, Final Batch Loss: 0.07281818240880966\n",
      "Epoch 1731, Loss: 0.3124789260327816, Final Batch Loss: 0.025459017604589462\n",
      "Epoch 1732, Loss: 0.3175678141415119, Final Batch Loss: 0.041804272681474686\n",
      "Epoch 1733, Loss: 0.2607773318886757, Final Batch Loss: 0.06665986031293869\n",
      "Epoch 1734, Loss: 0.35261235386133194, Final Batch Loss: 0.1640210747718811\n",
      "Epoch 1735, Loss: 0.42103176563978195, Final Batch Loss: 0.09338775277137756\n",
      "Epoch 1736, Loss: 0.34237275272607803, Final Batch Loss: 0.07171850651502609\n",
      "Epoch 1737, Loss: 0.3268960155546665, Final Batch Loss: 0.09644348174333572\n",
      "Epoch 1738, Loss: 0.30414607003331184, Final Batch Loss: 0.036039162427186966\n",
      "Epoch 1739, Loss: 0.26758868619799614, Final Batch Loss: 0.03654317185282707\n",
      "Epoch 1740, Loss: 0.234609704464674, Final Batch Loss: 0.03159556910395622\n",
      "Epoch 1741, Loss: 0.3469834476709366, Final Batch Loss: 0.06630338728427887\n",
      "Epoch 1742, Loss: 0.3854699954390526, Final Batch Loss: 0.10886397957801819\n",
      "Epoch 1743, Loss: 0.37148283421993256, Final Batch Loss: 0.12068063020706177\n",
      "Epoch 1744, Loss: 0.3589639589190483, Final Batch Loss: 0.06692087650299072\n",
      "Epoch 1745, Loss: 0.3081623539328575, Final Batch Loss: 0.036598414182662964\n",
      "Epoch 1746, Loss: 0.3306731656193733, Final Batch Loss: 0.06330744922161102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1747, Loss: 0.3719625920057297, Final Batch Loss: 0.13083887100219727\n",
      "Epoch 1748, Loss: 0.3842562884092331, Final Batch Loss: 0.09064484387636185\n",
      "Epoch 1749, Loss: 0.4825514107942581, Final Batch Loss: 0.2793351411819458\n",
      "Epoch 1750, Loss: 0.40956319868564606, Final Batch Loss: 0.11375269293785095\n",
      "Epoch 1751, Loss: 0.42544248700141907, Final Batch Loss: 0.15442879498004913\n",
      "Epoch 1752, Loss: 0.276371318846941, Final Batch Loss: 0.058889634907245636\n",
      "Epoch 1753, Loss: 0.42262428998947144, Final Batch Loss: 0.10923975706100464\n",
      "Epoch 1754, Loss: 0.28917646408081055, Final Batch Loss: 0.0623214989900589\n",
      "Epoch 1755, Loss: 0.5375474989414215, Final Batch Loss: 0.2746073305606842\n",
      "Epoch 1756, Loss: 0.41844164580106735, Final Batch Loss: 0.12394796311855316\n",
      "Epoch 1757, Loss: 0.36983001232147217, Final Batch Loss: 0.08752705156803131\n",
      "Epoch 1758, Loss: 0.2932814136147499, Final Batch Loss: 0.050132159143686295\n",
      "Epoch 1759, Loss: 0.4361758641898632, Final Batch Loss: 0.12209834903478622\n",
      "Epoch 1760, Loss: 0.37295714020729065, Final Batch Loss: 0.07635325193405151\n",
      "Epoch 1761, Loss: 0.37252045050263405, Final Batch Loss: 0.16329297423362732\n",
      "Epoch 1762, Loss: 0.3222648501396179, Final Batch Loss: 0.07221091538667679\n",
      "Epoch 1763, Loss: 0.44376207143068314, Final Batch Loss: 0.07313190400600433\n",
      "Epoch 1764, Loss: 0.4102461710572243, Final Batch Loss: 0.07720591127872467\n",
      "Epoch 1765, Loss: 0.34846657514572144, Final Batch Loss: 0.09261935204267502\n",
      "Epoch 1766, Loss: 0.31998568028211594, Final Batch Loss: 0.05711373686790466\n",
      "Epoch 1767, Loss: 0.3351083882153034, Final Batch Loss: 0.11163974553346634\n",
      "Epoch 1768, Loss: 0.34162765741348267, Final Batch Loss: 0.09154459834098816\n",
      "Epoch 1769, Loss: 0.3330768793821335, Final Batch Loss: 0.10121291875839233\n",
      "Epoch 1770, Loss: 0.40787623077630997, Final Batch Loss: 0.14869804680347443\n",
      "Epoch 1771, Loss: 0.31010372936725616, Final Batch Loss: 0.10255558043718338\n",
      "Epoch 1772, Loss: 0.33625905215740204, Final Batch Loss: 0.09560182690620422\n",
      "Epoch 1773, Loss: 0.46503330767154694, Final Batch Loss: 0.21297159790992737\n",
      "Epoch 1774, Loss: 0.2799365296959877, Final Batch Loss: 0.06279819458723068\n",
      "Epoch 1775, Loss: 0.2899106666445732, Final Batch Loss: 0.11642709374427795\n",
      "Epoch 1776, Loss: 0.25491292029619217, Final Batch Loss: 0.055464014410972595\n",
      "Epoch 1777, Loss: 0.2835574336349964, Final Batch Loss: 0.04272827133536339\n",
      "Epoch 1778, Loss: 0.3010743707418442, Final Batch Loss: 0.10025212168693542\n",
      "Epoch 1779, Loss: 0.4196922108530998, Final Batch Loss: 0.15270262956619263\n",
      "Epoch 1780, Loss: 0.2798765264451504, Final Batch Loss: 0.06382685899734497\n",
      "Epoch 1781, Loss: 0.3272937200963497, Final Batch Loss: 0.08990845084190369\n",
      "Epoch 1782, Loss: 0.2937754988670349, Final Batch Loss: 0.06195230782032013\n",
      "Epoch 1783, Loss: 0.2554255537688732, Final Batch Loss: 0.03989823907613754\n",
      "Epoch 1784, Loss: 0.42129845172166824, Final Batch Loss: 0.1268673986196518\n",
      "Epoch 1785, Loss: 0.3286276198923588, Final Batch Loss: 0.052212800830602646\n",
      "Epoch 1786, Loss: 0.4656565859913826, Final Batch Loss: 0.0771762877702713\n",
      "Epoch 1787, Loss: 0.5385067388415337, Final Batch Loss: 0.21322429180145264\n",
      "Epoch 1788, Loss: 0.33982663974165916, Final Batch Loss: 0.055005334317684174\n",
      "Epoch 1789, Loss: 0.3023189604282379, Final Batch Loss: 0.06639738380908966\n",
      "Epoch 1790, Loss: 0.40257684141397476, Final Batch Loss: 0.1584746241569519\n",
      "Epoch 1791, Loss: 0.2739972174167633, Final Batch Loss: 0.047963038086891174\n",
      "Epoch 1792, Loss: 0.3873251974582672, Final Batch Loss: 0.17774103581905365\n",
      "Epoch 1793, Loss: 0.3726694956421852, Final Batch Loss: 0.15384788811206818\n",
      "Epoch 1794, Loss: 0.36189255863428116, Final Batch Loss: 0.057802293449640274\n",
      "Epoch 1795, Loss: 0.3527063578367233, Final Batch Loss: 0.07268799841403961\n",
      "Epoch 1796, Loss: 0.4165161922574043, Final Batch Loss: 0.12241682410240173\n",
      "Epoch 1797, Loss: 0.3026563189923763, Final Batch Loss: 0.07995989918708801\n",
      "Epoch 1798, Loss: 0.40420058369636536, Final Batch Loss: 0.14039848744869232\n",
      "Epoch 1799, Loss: 0.4167576730251312, Final Batch Loss: 0.13051393628120422\n",
      "Epoch 1800, Loss: 0.34833457320928574, Final Batch Loss: 0.07140088826417923\n",
      "Epoch 1801, Loss: 0.3912184536457062, Final Batch Loss: 0.12971089780330658\n",
      "Epoch 1802, Loss: 0.4699965566396713, Final Batch Loss: 0.17397256195545197\n",
      "Epoch 1803, Loss: 0.3614320904016495, Final Batch Loss: 0.05691102147102356\n",
      "Epoch 1804, Loss: 0.47297972440719604, Final Batch Loss: 0.19101421535015106\n",
      "Epoch 1805, Loss: 0.4112224578857422, Final Batch Loss: 0.11930994689464569\n",
      "Epoch 1806, Loss: 0.4723893478512764, Final Batch Loss: 0.14531226456165314\n",
      "Epoch 1807, Loss: 0.3653002753853798, Final Batch Loss: 0.09938403218984604\n",
      "Epoch 1808, Loss: 0.29219551011919975, Final Batch Loss: 0.06041566655039787\n",
      "Epoch 1809, Loss: 0.3455778881907463, Final Batch Loss: 0.03766553848981857\n",
      "Epoch 1810, Loss: 0.4023415893316269, Final Batch Loss: 0.11505099385976791\n",
      "Epoch 1811, Loss: 0.2922450341284275, Final Batch Loss: 0.05090159550309181\n",
      "Epoch 1812, Loss: 0.3102446533739567, Final Batch Loss: 0.05992342531681061\n",
      "Epoch 1813, Loss: 0.363583043217659, Final Batch Loss: 0.03957006335258484\n",
      "Epoch 1814, Loss: 0.32904401794075966, Final Batch Loss: 0.043867986649274826\n",
      "Epoch 1815, Loss: 0.27766168862581253, Final Batch Loss: 0.0812680721282959\n",
      "Epoch 1816, Loss: 0.41083090007305145, Final Batch Loss: 0.09099084138870239\n",
      "Epoch 1817, Loss: 0.31734250485897064, Final Batch Loss: 0.03463810682296753\n",
      "Epoch 1818, Loss: 0.3041977733373642, Final Batch Loss: 0.08550876379013062\n",
      "Epoch 1819, Loss: 0.3644287697970867, Final Batch Loss: 0.12792442739009857\n",
      "Epoch 1820, Loss: 0.36137987673282623, Final Batch Loss: 0.07682336866855621\n",
      "Epoch 1821, Loss: 0.26421815156936646, Final Batch Loss: 0.11427503824234009\n",
      "Epoch 1822, Loss: 0.3764670863747597, Final Batch Loss: 0.1872442066669464\n",
      "Epoch 1823, Loss: 0.38919325917959213, Final Batch Loss: 0.10088781267404556\n",
      "Epoch 1824, Loss: 0.28784002363681793, Final Batch Loss: 0.04207899421453476\n",
      "Epoch 1825, Loss: 0.2586281970143318, Final Batch Loss: 0.06867590546607971\n",
      "Epoch 1826, Loss: 0.364606574177742, Final Batch Loss: 0.09661473333835602\n",
      "Epoch 1827, Loss: 0.3153708688914776, Final Batch Loss: 0.05392107740044594\n",
      "Epoch 1828, Loss: 0.3896878622472286, Final Batch Loss: 0.10067792981863022\n",
      "Epoch 1829, Loss: 0.31125742942094803, Final Batch Loss: 0.07547206431627274\n",
      "Epoch 1830, Loss: 0.42442646622657776, Final Batch Loss: 0.11302027106285095\n",
      "Epoch 1831, Loss: 0.3573443628847599, Final Batch Loss: 0.14375995099544525\n",
      "Epoch 1832, Loss: 0.3293370008468628, Final Batch Loss: 0.09539449959993362\n",
      "Epoch 1833, Loss: 0.34759389609098434, Final Batch Loss: 0.055159784853458405\n",
      "Epoch 1834, Loss: 0.3370119910687208, Final Batch Loss: 0.026710106059908867\n",
      "Epoch 1835, Loss: 0.2809738591313362, Final Batch Loss: 0.0697363093495369\n",
      "Epoch 1836, Loss: 0.3919778987765312, Final Batch Loss: 0.07240115851163864\n",
      "Epoch 1837, Loss: 0.2659798786044121, Final Batch Loss: 0.09004020690917969\n",
      "Epoch 1838, Loss: 0.34689806774258614, Final Batch Loss: 0.04504048824310303\n",
      "Epoch 1839, Loss: 0.3171679303050041, Final Batch Loss: 0.07628388702869415\n",
      "Epoch 1840, Loss: 0.29233353585004807, Final Batch Loss: 0.0564836822450161\n",
      "Epoch 1841, Loss: 0.26092346012592316, Final Batch Loss: 0.05977369844913483\n",
      "Epoch 1842, Loss: 0.3407399207353592, Final Batch Loss: 0.10324051976203918\n",
      "Epoch 1843, Loss: 0.43174413591623306, Final Batch Loss: 0.0842883363366127\n",
      "Epoch 1844, Loss: 0.333106592297554, Final Batch Loss: 0.09768468886613846\n",
      "Epoch 1845, Loss: 0.3649302199482918, Final Batch Loss: 0.07539073377847672\n",
      "Epoch 1846, Loss: 0.30162010341882706, Final Batch Loss: 0.05302770808339119\n",
      "Epoch 1847, Loss: 0.33254051953554153, Final Batch Loss: 0.06488011032342911\n",
      "Epoch 1848, Loss: 0.4148116633296013, Final Batch Loss: 0.1902296245098114\n",
      "Epoch 1849, Loss: 0.44297046959400177, Final Batch Loss: 0.18773837387561798\n",
      "Epoch 1850, Loss: 0.40969308465719223, Final Batch Loss: 0.1658945232629776\n",
      "Epoch 1851, Loss: 0.2937285974621773, Final Batch Loss: 0.036761850118637085\n",
      "Epoch 1852, Loss: 0.29112803749740124, Final Batch Loss: 0.10121128708124161\n",
      "Epoch 1853, Loss: 0.2990615591406822, Final Batch Loss: 0.03488545119762421\n",
      "Epoch 1854, Loss: 0.2993672899901867, Final Batch Loss: 0.07124123722314835\n",
      "Epoch 1855, Loss: 0.4169134348630905, Final Batch Loss: 0.17397131025791168\n",
      "Epoch 1856, Loss: 0.38184691593050957, Final Batch Loss: 0.15792971849441528\n",
      "Epoch 1857, Loss: 0.2688542976975441, Final Batch Loss: 0.07782746851444244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1858, Loss: 0.3207477480173111, Final Batch Loss: 0.07726355642080307\n",
      "Epoch 1859, Loss: 0.3190947212278843, Final Batch Loss: 0.048290546983480453\n",
      "Epoch 1860, Loss: 0.3679482638835907, Final Batch Loss: 0.15133073925971985\n",
      "Epoch 1861, Loss: 0.2706451751291752, Final Batch Loss: 0.07325488328933716\n",
      "Epoch 1862, Loss: 0.27572665736079216, Final Batch Loss: 0.030106578022241592\n",
      "Epoch 1863, Loss: 0.33257992565631866, Final Batch Loss: 0.0830853208899498\n",
      "Epoch 1864, Loss: 0.352426253259182, Final Batch Loss: 0.06146413832902908\n",
      "Epoch 1865, Loss: 0.36850639805197716, Final Batch Loss: 0.05973910912871361\n",
      "Epoch 1866, Loss: 0.36862315237522125, Final Batch Loss: 0.09663238376379013\n",
      "Epoch 1867, Loss: 0.28367847949266434, Final Batch Loss: 0.06517850607633591\n",
      "Epoch 1868, Loss: 0.28652745857834816, Final Batch Loss: 0.029195185750722885\n",
      "Epoch 1869, Loss: 0.39298421144485474, Final Batch Loss: 0.12908068299293518\n",
      "Epoch 1870, Loss: 0.2788293808698654, Final Batch Loss: 0.07176356762647629\n",
      "Epoch 1871, Loss: 0.23087989538908005, Final Batch Loss: 0.04842863976955414\n",
      "Epoch 1872, Loss: 0.33733150362968445, Final Batch Loss: 0.08699138462543488\n",
      "Epoch 1873, Loss: 0.32293038815259933, Final Batch Loss: 0.05967816710472107\n",
      "Epoch 1874, Loss: 0.3444584831595421, Final Batch Loss: 0.1053193062543869\n",
      "Epoch 1875, Loss: 0.31322961300611496, Final Batch Loss: 0.11846630275249481\n",
      "Epoch 1876, Loss: 0.3924321308732033, Final Batch Loss: 0.08717107027769089\n",
      "Epoch 1877, Loss: 0.2986617796123028, Final Batch Loss: 0.05566644296050072\n",
      "Epoch 1878, Loss: 0.32401303201913834, Final Batch Loss: 0.06578779220581055\n",
      "Epoch 1879, Loss: 0.29807979241013527, Final Batch Loss: 0.02395576611161232\n",
      "Epoch 1880, Loss: 0.37753088027238846, Final Batch Loss: 0.13180609047412872\n",
      "Epoch 1881, Loss: 0.2713775113224983, Final Batch Loss: 0.06603622436523438\n",
      "Epoch 1882, Loss: 0.299014575779438, Final Batch Loss: 0.06573580950498581\n",
      "Epoch 1883, Loss: 0.2608434185385704, Final Batch Loss: 0.0480135977268219\n",
      "Epoch 1884, Loss: 0.36676179245114326, Final Batch Loss: 0.14422808587551117\n",
      "Epoch 1885, Loss: 0.3051716163754463, Final Batch Loss: 0.05639439821243286\n",
      "Epoch 1886, Loss: 0.3586159870028496, Final Batch Loss: 0.12973299622535706\n",
      "Epoch 1887, Loss: 0.26696984097361565, Final Batch Loss: 0.07504507154226303\n",
      "Epoch 1888, Loss: 0.404728002846241, Final Batch Loss: 0.04718049615621567\n",
      "Epoch 1889, Loss: 0.28481800481677055, Final Batch Loss: 0.038789745420217514\n",
      "Epoch 1890, Loss: 0.34578246623277664, Final Batch Loss: 0.05714545398950577\n",
      "Epoch 1891, Loss: 0.39936694130301476, Final Batch Loss: 0.16298207640647888\n",
      "Epoch 1892, Loss: 0.30197500064969063, Final Batch Loss: 0.11822851747274399\n",
      "Epoch 1893, Loss: 0.3241082429885864, Final Batch Loss: 0.0411224439740181\n",
      "Epoch 1894, Loss: 0.33103956282138824, Final Batch Loss: 0.06704043596982956\n",
      "Epoch 1895, Loss: 0.2791126109659672, Final Batch Loss: 0.0433780737221241\n",
      "Epoch 1896, Loss: 0.45266593247652054, Final Batch Loss: 0.1871638298034668\n",
      "Epoch 1897, Loss: 0.34134169667959213, Final Batch Loss: 0.06475594639778137\n",
      "Epoch 1898, Loss: 0.2934594005346298, Final Batch Loss: 0.08447061479091644\n",
      "Epoch 1899, Loss: 0.328802602365613, Final Batch Loss: 0.023504206910729408\n",
      "Epoch 1900, Loss: 0.3096993528306484, Final Batch Loss: 0.050564903765916824\n",
      "Epoch 1901, Loss: 0.29135865718126297, Final Batch Loss: 0.05399446189403534\n",
      "Epoch 1902, Loss: 0.19718698039650917, Final Batch Loss: 0.031403131783008575\n",
      "Epoch 1903, Loss: 0.29871805012226105, Final Batch Loss: 0.09217251092195511\n",
      "Epoch 1904, Loss: 0.2454401794821024, Final Batch Loss: 0.029807457700371742\n",
      "Epoch 1905, Loss: 0.2932327426970005, Final Batch Loss: 0.08348827809095383\n",
      "Epoch 1906, Loss: 0.32910582795739174, Final Batch Loss: 0.08730493485927582\n",
      "Epoch 1907, Loss: 0.30477022379636765, Final Batch Loss: 0.03917183727025986\n",
      "Epoch 1908, Loss: 0.36013808846473694, Final Batch Loss: 0.10504034161567688\n",
      "Epoch 1909, Loss: 0.31069450825452805, Final Batch Loss: 0.05519075691699982\n",
      "Epoch 1910, Loss: 0.3977794945240021, Final Batch Loss: 0.18061350286006927\n",
      "Epoch 1911, Loss: 0.2773030735552311, Final Batch Loss: 0.04827427119016647\n",
      "Epoch 1912, Loss: 0.41610876470804214, Final Batch Loss: 0.12106441706418991\n",
      "Epoch 1913, Loss: 0.38645779341459274, Final Batch Loss: 0.19658099114894867\n",
      "Epoch 1914, Loss: 0.31983712315559387, Final Batch Loss: 0.09052050113677979\n",
      "Epoch 1915, Loss: 0.4383094757795334, Final Batch Loss: 0.2009640634059906\n",
      "Epoch 1916, Loss: 0.47187699377536774, Final Batch Loss: 0.1313534379005432\n",
      "Epoch 1917, Loss: 0.3870275020599365, Final Batch Loss: 0.07979235053062439\n",
      "Epoch 1918, Loss: 0.3808670677244663, Final Batch Loss: 0.14261113107204437\n",
      "Epoch 1919, Loss: 0.39016328006982803, Final Batch Loss: 0.04553859680891037\n",
      "Epoch 1920, Loss: 0.43985918909311295, Final Batch Loss: 0.10457638651132584\n",
      "Epoch 1921, Loss: 0.37550313770771027, Final Batch Loss: 0.07560586929321289\n",
      "Epoch 1922, Loss: 0.3653493672609329, Final Batch Loss: 0.07680812478065491\n",
      "Epoch 1923, Loss: 0.4032570496201515, Final Batch Loss: 0.09026114642620087\n",
      "Epoch 1924, Loss: 0.3584175184369087, Final Batch Loss: 0.15297071635723114\n",
      "Epoch 1925, Loss: 0.30744775757193565, Final Batch Loss: 0.05076584592461586\n",
      "Epoch 1926, Loss: 0.3490782119333744, Final Batch Loss: 0.12003816664218903\n",
      "Epoch 1927, Loss: 0.3357516750693321, Final Batch Loss: 0.12242822349071503\n",
      "Epoch 1928, Loss: 0.3086988925933838, Final Batch Loss: 0.03550156205892563\n",
      "Epoch 1929, Loss: 0.32922498509287834, Final Batch Loss: 0.08521877974271774\n",
      "Epoch 1930, Loss: 0.22671563178300858, Final Batch Loss: 0.04078690707683563\n",
      "Epoch 1931, Loss: 0.38552815094590187, Final Batch Loss: 0.15599872171878815\n",
      "Epoch 1932, Loss: 0.22168530710041523, Final Batch Loss: 0.025363130494952202\n",
      "Epoch 1933, Loss: 0.2924206964671612, Final Batch Loss: 0.07375136762857437\n",
      "Epoch 1934, Loss: 0.311431847512722, Final Batch Loss: 0.0628589540719986\n",
      "Epoch 1935, Loss: 0.3293837308883667, Final Batch Loss: 0.055280786007642746\n",
      "Epoch 1936, Loss: 0.2753880061209202, Final Batch Loss: 0.03606504574418068\n",
      "Epoch 1937, Loss: 0.41496700048446655, Final Batch Loss: 0.08760500699281693\n",
      "Epoch 1938, Loss: 0.28155628219246864, Final Batch Loss: 0.059851039201021194\n",
      "Epoch 1939, Loss: 0.4144251123070717, Final Batch Loss: 0.06286168098449707\n",
      "Epoch 1940, Loss: 0.3777971565723419, Final Batch Loss: 0.06858309358358383\n",
      "Epoch 1941, Loss: 0.38337454199790955, Final Batch Loss: 0.04228552430868149\n",
      "Epoch 1942, Loss: 0.3633321300148964, Final Batch Loss: 0.035719916224479675\n",
      "Epoch 1943, Loss: 0.31125476583838463, Final Batch Loss: 0.08951333165168762\n",
      "Epoch 1944, Loss: 0.34528374671936035, Final Batch Loss: 0.0569932758808136\n",
      "Epoch 1945, Loss: 0.3480337858200073, Final Batch Loss: 0.0944804698228836\n",
      "Epoch 1946, Loss: 0.2829923667013645, Final Batch Loss: 0.04100627079606056\n",
      "Epoch 1947, Loss: 0.3512861728668213, Final Batch Loss: 0.09321279078722\n",
      "Epoch 1948, Loss: 0.39615776389837265, Final Batch Loss: 0.13778431713581085\n",
      "Epoch 1949, Loss: 0.38388189673423767, Final Batch Loss: 0.1490396410226822\n",
      "Epoch 1950, Loss: 0.3824402391910553, Final Batch Loss: 0.12662534415721893\n",
      "Epoch 1951, Loss: 0.3827431946992874, Final Batch Loss: 0.1313530057668686\n",
      "Epoch 1952, Loss: 0.41683197766542435, Final Batch Loss: 0.1139703243970871\n",
      "Epoch 1953, Loss: 0.36531247571110725, Final Batch Loss: 0.11648623645305634\n",
      "Epoch 1954, Loss: 0.32811494544148445, Final Batch Loss: 0.11408666521310806\n",
      "Epoch 1955, Loss: 0.36596789211034775, Final Batch Loss: 0.06946787983179092\n",
      "Epoch 1956, Loss: 0.2765617296099663, Final Batch Loss: 0.06820976734161377\n",
      "Epoch 1957, Loss: 0.3745453730225563, Final Batch Loss: 0.09898246824741364\n",
      "Epoch 1958, Loss: 0.2956313490867615, Final Batch Loss: 0.09896963834762573\n",
      "Epoch 1959, Loss: 0.3188147060573101, Final Batch Loss: 0.0685076117515564\n",
      "Epoch 1960, Loss: 0.3247525990009308, Final Batch Loss: 0.1121533140540123\n",
      "Epoch 1961, Loss: 0.4161466546356678, Final Batch Loss: 0.17594127357006073\n",
      "Epoch 1962, Loss: 0.3546077720820904, Final Batch Loss: 0.08935026824474335\n",
      "Epoch 1963, Loss: 0.3982665203511715, Final Batch Loss: 0.05036270245909691\n",
      "Epoch 1964, Loss: 0.47374267131090164, Final Batch Loss: 0.11681339889764786\n",
      "Epoch 1965, Loss: 0.39757105708122253, Final Batch Loss: 0.07202887535095215\n",
      "Epoch 1966, Loss: 0.45242150500416756, Final Batch Loss: 0.1245143786072731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1967, Loss: 0.4633207470178604, Final Batch Loss: 0.10382933169603348\n",
      "Epoch 1968, Loss: 0.31733010709285736, Final Batch Loss: 0.06314197182655334\n",
      "Epoch 1969, Loss: 0.3032957352697849, Final Batch Loss: 0.027749691158533096\n",
      "Epoch 1970, Loss: 0.3810781314969063, Final Batch Loss: 0.10502961277961731\n",
      "Epoch 1971, Loss: 0.3952949829399586, Final Batch Loss: 0.14124347269535065\n",
      "Epoch 1972, Loss: 0.3436112627387047, Final Batch Loss: 0.09350576251745224\n",
      "Epoch 1973, Loss: 0.39208151027560234, Final Batch Loss: 0.10729825496673584\n",
      "Epoch 1974, Loss: 0.35012979060411453, Final Batch Loss: 0.10790257155895233\n",
      "Epoch 1975, Loss: 0.3460441939532757, Final Batch Loss: 0.07474040985107422\n",
      "Epoch 1976, Loss: 0.3272881954908371, Final Batch Loss: 0.09661172330379486\n",
      "Epoch 1977, Loss: 0.2774644009768963, Final Batch Loss: 0.06169254705309868\n",
      "Epoch 1978, Loss: 0.3517645001411438, Final Batch Loss: 0.052047841250896454\n",
      "Epoch 1979, Loss: 0.22005688026547432, Final Batch Loss: 0.028334081172943115\n",
      "Epoch 1980, Loss: 0.30305641517043114, Final Batch Loss: 0.0837542712688446\n",
      "Epoch 1981, Loss: 0.28865980729460716, Final Batch Loss: 0.06360847502946854\n",
      "Epoch 1982, Loss: 0.42867639660835266, Final Batch Loss: 0.16311775147914886\n",
      "Epoch 1983, Loss: 0.2409953698515892, Final Batch Loss: 0.059908777475357056\n",
      "Epoch 1984, Loss: 0.4729558378458023, Final Batch Loss: 0.20472392439842224\n",
      "Epoch 1985, Loss: 0.2547951675951481, Final Batch Loss: 0.04711045324802399\n",
      "Epoch 1986, Loss: 0.347574595361948, Final Batch Loss: 0.07829734683036804\n",
      "Epoch 1987, Loss: 0.3641456961631775, Final Batch Loss: 0.0869445875287056\n",
      "Epoch 1988, Loss: 0.35556071251630783, Final Batch Loss: 0.11944855749607086\n",
      "Epoch 1989, Loss: 0.31859272718429565, Final Batch Loss: 0.08614624291658401\n",
      "Epoch 1990, Loss: 0.30215708166360855, Final Batch Loss: 0.1022881492972374\n",
      "Epoch 1991, Loss: 0.2664870545268059, Final Batch Loss: 0.0591956228017807\n",
      "Epoch 1992, Loss: 0.28821268305182457, Final Batch Loss: 0.03022729977965355\n",
      "Epoch 1993, Loss: 0.2618320398032665, Final Batch Loss: 0.059307534247636795\n",
      "Epoch 1994, Loss: 0.2760162018239498, Final Batch Loss: 0.08604472875595093\n",
      "Epoch 1995, Loss: 0.254470519721508, Final Batch Loss: 0.06655799597501755\n",
      "Epoch 1996, Loss: 0.27395564317703247, Final Batch Loss: 0.05986522138118744\n",
      "Epoch 1997, Loss: 0.333381287753582, Final Batch Loss: 0.12119712680578232\n",
      "Epoch 1998, Loss: 0.27515759319067, Final Batch Loss: 0.06946573406457901\n",
      "Epoch 1999, Loss: 0.2661391422152519, Final Batch Loss: 0.049831751734018326\n",
      "Epoch 2000, Loss: 0.34518150612711906, Final Batch Loss: 0.10891880095005035\n",
      "Epoch 2001, Loss: 0.2758914865553379, Final Batch Loss: 0.03875909373164177\n",
      "Epoch 2002, Loss: 0.28911880403757095, Final Batch Loss: 0.07797349989414215\n",
      "Epoch 2003, Loss: 0.2782275825738907, Final Batch Loss: 0.04702428728342056\n",
      "Epoch 2004, Loss: 0.22216152399778366, Final Batch Loss: 0.043713562190532684\n",
      "Epoch 2005, Loss: 0.32824255153536797, Final Batch Loss: 0.05773777514696121\n",
      "Epoch 2006, Loss: 0.2712862268090248, Final Batch Loss: 0.06111135706305504\n",
      "Epoch 2007, Loss: 0.28739751502871513, Final Batch Loss: 0.069037064909935\n",
      "Epoch 2008, Loss: 0.3379223868250847, Final Batch Loss: 0.0651366263628006\n",
      "Epoch 2009, Loss: 0.3058008626103401, Final Batch Loss: 0.06789597868919373\n",
      "Epoch 2010, Loss: 0.2636902704834938, Final Batch Loss: 0.06167782098054886\n",
      "Epoch 2011, Loss: 0.26581160724163055, Final Batch Loss: 0.05839307978749275\n",
      "Epoch 2012, Loss: 0.2829035632312298, Final Batch Loss: 0.07562550157308578\n",
      "Epoch 2013, Loss: 0.28995705023407936, Final Batch Loss: 0.09109201282262802\n",
      "Epoch 2014, Loss: 0.26514603942632675, Final Batch Loss: 0.039475977420806885\n",
      "Epoch 2015, Loss: 0.35223904997110367, Final Batch Loss: 0.14029274880886078\n",
      "Epoch 2016, Loss: 0.3309275098145008, Final Batch Loss: 0.16782520711421967\n",
      "Epoch 2017, Loss: 0.24063418060541153, Final Batch Loss: 0.0464220829308033\n",
      "Epoch 2018, Loss: 0.26758452877402306, Final Batch Loss: 0.0743924155831337\n",
      "Epoch 2019, Loss: 0.21921764686703682, Final Batch Loss: 0.03798401355743408\n",
      "Epoch 2020, Loss: 0.28619879856705666, Final Batch Loss: 0.08971520513296127\n",
      "Epoch 2021, Loss: 0.3388301059603691, Final Batch Loss: 0.10558166354894638\n",
      "Epoch 2022, Loss: 0.3032010346651077, Final Batch Loss: 0.0770949199795723\n",
      "Epoch 2023, Loss: 0.3179891109466553, Final Batch Loss: 0.07040838152170181\n",
      "Epoch 2024, Loss: 0.2923382855951786, Final Batch Loss: 0.03516899421811104\n",
      "Epoch 2025, Loss: 0.3357887975871563, Final Batch Loss: 0.14330953359603882\n",
      "Epoch 2026, Loss: 0.2884862422943115, Final Batch Loss: 0.0615510568022728\n",
      "Epoch 2027, Loss: 0.3728819116950035, Final Batch Loss: 0.1403859704732895\n",
      "Epoch 2028, Loss: 0.35801396518945694, Final Batch Loss: 0.08062522858381271\n",
      "Epoch 2029, Loss: 0.27901285886764526, Final Batch Loss: 0.06361077725887299\n",
      "Epoch 2030, Loss: 0.4113461896777153, Final Batch Loss: 0.15355193614959717\n",
      "Epoch 2031, Loss: 0.2602642998099327, Final Batch Loss: 0.07137502729892731\n",
      "Epoch 2032, Loss: 0.3432348072528839, Final Batch Loss: 0.1423639953136444\n",
      "Epoch 2033, Loss: 0.26292648538947105, Final Batch Loss: 0.047133807092905045\n",
      "Epoch 2034, Loss: 0.30823197215795517, Final Batch Loss: 0.0740973949432373\n",
      "Epoch 2035, Loss: 0.40283578261733055, Final Batch Loss: 0.1589958667755127\n",
      "Epoch 2036, Loss: 0.22936083376407623, Final Batch Loss: 0.05848667770624161\n",
      "Epoch 2037, Loss: 0.33875468000769615, Final Batch Loss: 0.051432665437459946\n",
      "Epoch 2038, Loss: 0.3627920299768448, Final Batch Loss: 0.12289158254861832\n",
      "Epoch 2039, Loss: 0.29883433133363724, Final Batch Loss: 0.06271132081747055\n",
      "Epoch 2040, Loss: 0.411550834774971, Final Batch Loss: 0.06627505272626877\n",
      "Epoch 2041, Loss: 0.36047541722655296, Final Batch Loss: 0.14741818606853485\n",
      "Epoch 2042, Loss: 0.2397526055574417, Final Batch Loss: 0.05512024462223053\n",
      "Epoch 2043, Loss: 0.34500959143042564, Final Batch Loss: 0.060254909098148346\n",
      "Epoch 2044, Loss: 0.5085626021027565, Final Batch Loss: 0.20105156302452087\n",
      "Epoch 2045, Loss: 0.3387994319200516, Final Batch Loss: 0.08182224631309509\n",
      "Epoch 2046, Loss: 0.3839709162712097, Final Batch Loss: 0.11839345842599869\n",
      "Epoch 2047, Loss: 0.28098607063293457, Final Batch Loss: 0.07085045427083969\n",
      "Epoch 2048, Loss: 0.3347315862774849, Final Batch Loss: 0.07774253934621811\n",
      "Epoch 2049, Loss: 0.2966112159192562, Final Batch Loss: 0.06499849259853363\n",
      "Epoch 2050, Loss: 0.24721036478877068, Final Batch Loss: 0.05977163836359978\n",
      "Epoch 2051, Loss: 0.32878851890563965, Final Batch Loss: 0.08683650195598602\n",
      "Epoch 2052, Loss: 0.22874239832162857, Final Batch Loss: 0.04487068951129913\n",
      "Epoch 2053, Loss: 0.28422364965081215, Final Batch Loss: 0.0756286159157753\n",
      "Epoch 2054, Loss: 0.38917895779013634, Final Batch Loss: 0.18193301558494568\n",
      "Epoch 2055, Loss: 0.3000592850148678, Final Batch Loss: 0.10142820328474045\n",
      "Epoch 2056, Loss: 0.3347771279513836, Final Batch Loss: 0.1539236307144165\n",
      "Epoch 2057, Loss: 0.25048548355698586, Final Batch Loss: 0.06179732084274292\n",
      "Epoch 2058, Loss: 0.35732539370656013, Final Batch Loss: 0.062340449541807175\n",
      "Epoch 2059, Loss: 0.37179798632860184, Final Batch Loss: 0.05841971933841705\n",
      "Epoch 2060, Loss: 0.36755943670868874, Final Batch Loss: 0.14500673115253448\n",
      "Epoch 2061, Loss: 0.28695981204509735, Final Batch Loss: 0.12450891733169556\n",
      "Epoch 2062, Loss: 0.3197730854153633, Final Batch Loss: 0.0657125860452652\n",
      "Epoch 2063, Loss: 0.25204914435744286, Final Batch Loss: 0.04768170788884163\n",
      "Epoch 2064, Loss: 0.28511183708906174, Final Batch Loss: 0.05717416852712631\n",
      "Epoch 2065, Loss: 0.3255276158452034, Final Batch Loss: 0.06356681883335114\n",
      "Epoch 2066, Loss: 0.233404990285635, Final Batch Loss: 0.06674546748399734\n",
      "Epoch 2067, Loss: 0.3212137594819069, Final Batch Loss: 0.0870300680398941\n",
      "Epoch 2068, Loss: 0.29732174053788185, Final Batch Loss: 0.05531446263194084\n",
      "Epoch 2069, Loss: 0.3081836886703968, Final Batch Loss: 0.10556209832429886\n",
      "Epoch 2070, Loss: 0.29947618022561073, Final Batch Loss: 0.09467406570911407\n",
      "Epoch 2071, Loss: 0.25505629926919937, Final Batch Loss: 0.055445633828639984\n",
      "Epoch 2072, Loss: 0.28362491354346275, Final Batch Loss: 0.06108848378062248\n",
      "Epoch 2073, Loss: 0.2090265415608883, Final Batch Loss: 0.02647242695093155\n",
      "Epoch 2074, Loss: 0.25315240770578384, Final Batch Loss: 0.049782589077949524\n",
      "Epoch 2075, Loss: 0.2973698228597641, Final Batch Loss: 0.07857366651296616\n",
      "Epoch 2076, Loss: 0.28114543855190277, Final Batch Loss: 0.05255885049700737\n",
      "Epoch 2077, Loss: 0.25459976494312286, Final Batch Loss: 0.031632278114557266\n",
      "Epoch 2078, Loss: 0.33092017471790314, Final Batch Loss: 0.1221662238240242\n",
      "Epoch 2079, Loss: 0.28175440430641174, Final Batch Loss: 0.06375467777252197\n",
      "Epoch 2080, Loss: 0.4288790673017502, Final Batch Loss: 0.17577210068702698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2081, Loss: 0.29714053496718407, Final Batch Loss: 0.023612629622220993\n",
      "Epoch 2082, Loss: 0.3764183148741722, Final Batch Loss: 0.11086401343345642\n",
      "Epoch 2083, Loss: 0.27284736558794975, Final Batch Loss: 0.04628153517842293\n",
      "Epoch 2084, Loss: 0.33128612488508224, Final Batch Loss: 0.045120883733034134\n",
      "Epoch 2085, Loss: 0.33982592821121216, Final Batch Loss: 0.061482444405555725\n",
      "Epoch 2086, Loss: 0.2647178955376148, Final Batch Loss: 0.047198351472616196\n",
      "Epoch 2087, Loss: 0.34547024965286255, Final Batch Loss: 0.13141337037086487\n",
      "Epoch 2088, Loss: 0.253506550565362, Final Batch Loss: 0.028442995622754097\n",
      "Epoch 2089, Loss: 0.4141943193972111, Final Batch Loss: 0.13363631069660187\n",
      "Epoch 2090, Loss: 0.290885753929615, Final Batch Loss: 0.043498676270246506\n",
      "Epoch 2091, Loss: 0.3433554656803608, Final Batch Loss: 0.1852702647447586\n",
      "Epoch 2092, Loss: 0.3336375840008259, Final Batch Loss: 0.04180016741156578\n",
      "Epoch 2093, Loss: 0.40172285586595535, Final Batch Loss: 0.06179413944482803\n",
      "Epoch 2094, Loss: 0.3297814205288887, Final Batch Loss: 0.144334614276886\n",
      "Epoch 2095, Loss: 0.2696022801101208, Final Batch Loss: 0.09092237800359726\n",
      "Epoch 2096, Loss: 0.25688791275024414, Final Batch Loss: 0.02193666622042656\n",
      "Epoch 2097, Loss: 0.30450625345110893, Final Batch Loss: 0.07920672744512558\n",
      "Epoch 2098, Loss: 0.1922910176217556, Final Batch Loss: 0.02471330389380455\n",
      "Epoch 2099, Loss: 0.26274249330163, Final Batch Loss: 0.07847443968057632\n",
      "Epoch 2100, Loss: 0.34088923782110214, Final Batch Loss: 0.056907907128334045\n",
      "Epoch 2101, Loss: 0.2542637325823307, Final Batch Loss: 0.051050860434770584\n",
      "Epoch 2102, Loss: 0.25312450900673866, Final Batch Loss: 0.06911435723304749\n",
      "Epoch 2103, Loss: 0.25769203901290894, Final Batch Loss: 0.019279874861240387\n",
      "Epoch 2104, Loss: 0.2522513326257467, Final Batch Loss: 0.030831003561615944\n",
      "Epoch 2105, Loss: 0.32531217113137245, Final Batch Loss: 0.12883567810058594\n",
      "Epoch 2106, Loss: 0.32962603121995926, Final Batch Loss: 0.10777228325605392\n",
      "Epoch 2107, Loss: 0.3082990124821663, Final Batch Loss: 0.09116385877132416\n",
      "Epoch 2108, Loss: 0.2923322878777981, Final Batch Loss: 0.10945278406143188\n",
      "Epoch 2109, Loss: 0.28408567421138287, Final Batch Loss: 0.023127568885684013\n",
      "Epoch 2110, Loss: 0.28967701829969883, Final Batch Loss: 0.02587747760117054\n",
      "Epoch 2111, Loss: 0.27703652903437614, Final Batch Loss: 0.10278831422328949\n",
      "Epoch 2112, Loss: 0.22962896898388863, Final Batch Loss: 0.057957395911216736\n",
      "Epoch 2113, Loss: 0.2325817085802555, Final Batch Loss: 0.03788772225379944\n",
      "Epoch 2114, Loss: 0.23601519875228405, Final Batch Loss: 0.023639535531401634\n",
      "Epoch 2115, Loss: 0.26009322330355644, Final Batch Loss: 0.06047627329826355\n",
      "Epoch 2116, Loss: 0.2697461135685444, Final Batch Loss: 0.07121904194355011\n",
      "Epoch 2117, Loss: 0.32314107939600945, Final Batch Loss: 0.12349402159452438\n",
      "Epoch 2118, Loss: 0.24563681334257126, Final Batch Loss: 0.09362100064754486\n",
      "Epoch 2119, Loss: 0.30618277564644814, Final Batch Loss: 0.046347178518772125\n",
      "Epoch 2120, Loss: 0.25810961052775383, Final Batch Loss: 0.08514142036437988\n",
      "Epoch 2121, Loss: 0.22913441061973572, Final Batch Loss: 0.04606378450989723\n",
      "Epoch 2122, Loss: 0.30070946738123894, Final Batch Loss: 0.08047118037939072\n",
      "Epoch 2123, Loss: 0.27198803424835205, Final Batch Loss: 0.027608446776866913\n",
      "Epoch 2124, Loss: 0.248011845164001, Final Batch Loss: 0.013377456925809383\n",
      "Epoch 2125, Loss: 0.2895675376057625, Final Batch Loss: 0.05448802560567856\n",
      "Epoch 2126, Loss: 0.3225187659263611, Final Batch Loss: 0.06628946214914322\n",
      "Epoch 2127, Loss: 0.28258437290787697, Final Batch Loss: 0.07829327881336212\n",
      "Epoch 2128, Loss: 0.28756191954016685, Final Batch Loss: 0.034431878477334976\n",
      "Epoch 2129, Loss: 0.30901041626930237, Final Batch Loss: 0.11603984236717224\n",
      "Epoch 2130, Loss: 0.3495700806379318, Final Batch Loss: 0.09623631089925766\n",
      "Epoch 2131, Loss: 0.40683553367853165, Final Batch Loss: 0.1452237367630005\n",
      "Epoch 2132, Loss: 0.31951241940259933, Final Batch Loss: 0.05686943233013153\n",
      "Epoch 2133, Loss: 0.3613060228526592, Final Batch Loss: 0.08381650596857071\n",
      "Epoch 2134, Loss: 0.34233318269252777, Final Batch Loss: 0.06830941140651703\n",
      "Epoch 2135, Loss: 0.37359028309583664, Final Batch Loss: 0.07348816096782684\n",
      "Epoch 2136, Loss: 0.31727297976613045, Final Batch Loss: 0.06583930552005768\n",
      "Epoch 2137, Loss: 0.3697858639061451, Final Batch Loss: 0.062147680670022964\n",
      "Epoch 2138, Loss: 0.32058659568428993, Final Batch Loss: 0.08383826166391373\n",
      "Epoch 2139, Loss: 0.27483558282256126, Final Batch Loss: 0.05679960921406746\n",
      "Epoch 2140, Loss: 0.2765277326107025, Final Batch Loss: 0.06519721448421478\n",
      "Epoch 2141, Loss: 0.35097721964120865, Final Batch Loss: 0.05626343563199043\n",
      "Epoch 2142, Loss: 0.28847960010170937, Final Batch Loss: 0.0846342220902443\n",
      "Epoch 2143, Loss: 0.32029081135988235, Final Batch Loss: 0.09334266930818558\n",
      "Epoch 2144, Loss: 0.2641339525580406, Final Batch Loss: 0.06293149292469025\n",
      "Epoch 2145, Loss: 0.3496068939566612, Final Batch Loss: 0.06700985133647919\n",
      "Epoch 2146, Loss: 0.32120292633771896, Final Batch Loss: 0.11633075773715973\n",
      "Epoch 2147, Loss: 0.2756351940333843, Final Batch Loss: 0.06222383677959442\n",
      "Epoch 2148, Loss: 0.3518914729356766, Final Batch Loss: 0.08581100404262543\n",
      "Epoch 2149, Loss: 0.25323447212576866, Final Batch Loss: 0.034309063106775284\n",
      "Epoch 2150, Loss: 0.2872113287448883, Final Batch Loss: 0.050132714211940765\n",
      "Epoch 2151, Loss: 0.30136554315686226, Final Batch Loss: 0.1021914854645729\n",
      "Epoch 2152, Loss: 0.32571958750486374, Final Batch Loss: 0.14089395105838776\n",
      "Epoch 2153, Loss: 0.28875434026122093, Final Batch Loss: 0.08213634043931961\n",
      "Epoch 2154, Loss: 0.23019255138933659, Final Batch Loss: 0.045630212873220444\n",
      "Epoch 2155, Loss: 0.23537656664848328, Final Batch Loss: 0.05117349699139595\n",
      "Epoch 2156, Loss: 0.23758962005376816, Final Batch Loss: 0.05982700735330582\n",
      "Epoch 2157, Loss: 0.3057699017226696, Final Batch Loss: 0.11436104029417038\n",
      "Epoch 2158, Loss: 0.3065147064626217, Final Batch Loss: 0.05670266970992088\n",
      "Epoch 2159, Loss: 0.19646672531962395, Final Batch Loss: 0.025133609771728516\n",
      "Epoch 2160, Loss: 0.2960443776100874, Final Batch Loss: 0.06798646599054337\n",
      "Epoch 2161, Loss: 0.27311554178595543, Final Batch Loss: 0.032336603850126266\n",
      "Epoch 2162, Loss: 0.2971443310379982, Final Batch Loss: 0.08516798913478851\n",
      "Epoch 2163, Loss: 0.30241287499666214, Final Batch Loss: 0.12363573163747787\n",
      "Epoch 2164, Loss: 0.32987315207719803, Final Batch Loss: 0.16069698333740234\n",
      "Epoch 2165, Loss: 0.36907991021871567, Final Batch Loss: 0.11239415407180786\n",
      "Epoch 2166, Loss: 0.4010433927178383, Final Batch Loss: 0.04191214591264725\n",
      "Epoch 2167, Loss: 0.2951239552348852, Final Batch Loss: 0.030141109600663185\n",
      "Epoch 2168, Loss: 0.3457176089286804, Final Batch Loss: 0.06500926613807678\n",
      "Epoch 2169, Loss: 0.2969455160200596, Final Batch Loss: 0.10650864988565445\n",
      "Epoch 2170, Loss: 0.30993717163801193, Final Batch Loss: 0.05313123017549515\n",
      "Epoch 2171, Loss: 0.3714639022946358, Final Batch Loss: 0.10971209406852722\n",
      "Epoch 2172, Loss: 0.30213290452957153, Final Batch Loss: 0.12212253361940384\n",
      "Epoch 2173, Loss: 0.2994150035083294, Final Batch Loss: 0.07385984063148499\n",
      "Epoch 2174, Loss: 0.3059326894581318, Final Batch Loss: 0.054265882819890976\n",
      "Epoch 2175, Loss: 0.29749810695648193, Final Batch Loss: 0.08027896285057068\n",
      "Epoch 2176, Loss: 0.27174685522913933, Final Batch Loss: 0.06771675497293472\n",
      "Epoch 2177, Loss: 0.2848454602062702, Final Batch Loss: 0.05223075672984123\n",
      "Epoch 2178, Loss: 0.37791529297828674, Final Batch Loss: 0.043945059180259705\n",
      "Epoch 2179, Loss: 0.2502879034727812, Final Batch Loss: 0.021059149876236916\n",
      "Epoch 2180, Loss: 0.28338151425123215, Final Batch Loss: 0.08841867744922638\n",
      "Epoch 2181, Loss: 0.259759783744812, Final Batch Loss: 0.07993407547473907\n",
      "Epoch 2182, Loss: 0.2714485004544258, Final Batch Loss: 0.0379917211830616\n",
      "Epoch 2183, Loss: 0.3097159266471863, Final Batch Loss: 0.08214228600263596\n",
      "Epoch 2184, Loss: 0.23737471923232079, Final Batch Loss: 0.048387106508016586\n",
      "Epoch 2185, Loss: 0.26315924152731895, Final Batch Loss: 0.057988762855529785\n",
      "Epoch 2186, Loss: 0.27428197115659714, Final Batch Loss: 0.04307243973016739\n",
      "Epoch 2187, Loss: 0.2726270370185375, Final Batch Loss: 0.04879370704293251\n",
      "Epoch 2188, Loss: 0.3713477663695812, Final Batch Loss: 0.11502879858016968\n",
      "Epoch 2189, Loss: 0.34114301577210426, Final Batch Loss: 0.13775025308132172\n",
      "Epoch 2190, Loss: 0.3005027435719967, Final Batch Loss: 0.06415625661611557\n",
      "Epoch 2191, Loss: 0.27345165982842445, Final Batch Loss: 0.07436960190534592\n",
      "Epoch 2192, Loss: 0.3688449449837208, Final Batch Loss: 0.14851787686347961\n",
      "Epoch 2193, Loss: 0.22349249944090843, Final Batch Loss: 0.06738065183162689\n",
      "Epoch 2194, Loss: 0.28429101034998894, Final Batch Loss: 0.07098179310560226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2195, Loss: 0.24751312658190727, Final Batch Loss: 0.04439350962638855\n",
      "Epoch 2196, Loss: 0.3651842176914215, Final Batch Loss: 0.07527294009923935\n",
      "Epoch 2197, Loss: 0.32736721634864807, Final Batch Loss: 0.08941390365362167\n",
      "Epoch 2198, Loss: 0.2980741746723652, Final Batch Loss: 0.09156747162342072\n",
      "Epoch 2199, Loss: 0.34493307396769524, Final Batch Loss: 0.09534632414579391\n",
      "Epoch 2200, Loss: 0.28131019324064255, Final Batch Loss: 0.08551862090826035\n",
      "Epoch 2201, Loss: 0.2754398435354233, Final Batch Loss: 0.06679096817970276\n",
      "Epoch 2202, Loss: 0.27223585918545723, Final Batch Loss: 0.07469215244054794\n",
      "Epoch 2203, Loss: 0.25939735770225525, Final Batch Loss: 0.07467874139547348\n",
      "Epoch 2204, Loss: 0.24460333213210106, Final Batch Loss: 0.041026897728443146\n",
      "Epoch 2205, Loss: 0.2925722301006317, Final Batch Loss: 0.05983951687812805\n",
      "Epoch 2206, Loss: 0.31662360578775406, Final Batch Loss: 0.09738756716251373\n",
      "Epoch 2207, Loss: 0.289597287774086, Final Batch Loss: 0.12939408421516418\n",
      "Epoch 2208, Loss: 0.24481182545423508, Final Batch Loss: 0.03443888574838638\n",
      "Epoch 2209, Loss: 0.33942558988928795, Final Batch Loss: 0.15958049893379211\n",
      "Epoch 2210, Loss: 0.20751183852553368, Final Batch Loss: 0.03261251375079155\n",
      "Epoch 2211, Loss: 0.38409698382019997, Final Batch Loss: 0.10671491920948029\n",
      "Epoch 2212, Loss: 0.24921219795942307, Final Batch Loss: 0.03511884808540344\n",
      "Epoch 2213, Loss: 0.2662717178463936, Final Batch Loss: 0.0543835386633873\n",
      "Epoch 2214, Loss: 0.3499552756547928, Final Batch Loss: 0.0640077292919159\n",
      "Epoch 2215, Loss: 0.4164627864956856, Final Batch Loss: 0.08788895606994629\n",
      "Epoch 2216, Loss: 0.2345176748931408, Final Batch Loss: 0.025069385766983032\n",
      "Epoch 2217, Loss: 0.2828189581632614, Final Batch Loss: 0.06236831843852997\n",
      "Epoch 2218, Loss: 0.29498550295829773, Final Batch Loss: 0.10148805379867554\n",
      "Epoch 2219, Loss: 0.2604973800480366, Final Batch Loss: 0.0519096702337265\n",
      "Epoch 2220, Loss: 0.26268175430595875, Final Batch Loss: 0.021266194060444832\n",
      "Epoch 2221, Loss: 0.23394222557544708, Final Batch Loss: 0.054380349814891815\n",
      "Epoch 2222, Loss: 0.3250749707221985, Final Batch Loss: 0.09783884137868881\n",
      "Epoch 2223, Loss: 0.31502022594213486, Final Batch Loss: 0.09417753666639328\n",
      "Epoch 2224, Loss: 0.24568257108330727, Final Batch Loss: 0.04111926630139351\n",
      "Epoch 2225, Loss: 0.3409360386431217, Final Batch Loss: 0.13980795443058014\n",
      "Epoch 2226, Loss: 0.29952919483184814, Final Batch Loss: 0.1158265620470047\n",
      "Epoch 2227, Loss: 0.3226706348359585, Final Batch Loss: 0.046185530722141266\n",
      "Epoch 2228, Loss: 0.3539149835705757, Final Batch Loss: 0.11778117716312408\n",
      "Epoch 2229, Loss: 0.5446460992097855, Final Batch Loss: 0.19747743010520935\n",
      "Epoch 2230, Loss: 0.3550388850271702, Final Batch Loss: 0.156618133187294\n",
      "Epoch 2231, Loss: 0.44517623633146286, Final Batch Loss: 0.04016498476266861\n",
      "Epoch 2232, Loss: 0.3589926064014435, Final Batch Loss: 0.055460333824157715\n",
      "Epoch 2233, Loss: 0.3543708324432373, Final Batch Loss: 0.0828375443816185\n",
      "Epoch 2234, Loss: 0.35769620165228844, Final Batch Loss: 0.12917333841323853\n",
      "Epoch 2235, Loss: 0.21455731615424156, Final Batch Loss: 0.030281968414783478\n",
      "Epoch 2236, Loss: 0.3932283818721771, Final Batch Loss: 0.1408398300409317\n",
      "Epoch 2237, Loss: 0.41809647530317307, Final Batch Loss: 0.16919150948524475\n",
      "Epoch 2238, Loss: 0.46962597221136093, Final Batch Loss: 0.13829973340034485\n",
      "Epoch 2239, Loss: 0.34986789524555206, Final Batch Loss: 0.09642188251018524\n",
      "Epoch 2240, Loss: 0.24230952374637127, Final Batch Loss: 0.045436371117830276\n",
      "Epoch 2241, Loss: 0.2735205851495266, Final Batch Loss: 0.07798616588115692\n",
      "Epoch 2242, Loss: 0.32121849060058594, Final Batch Loss: 0.06236355006694794\n",
      "Epoch 2243, Loss: 0.30560316517949104, Final Batch Loss: 0.11130056530237198\n",
      "Epoch 2244, Loss: 0.32627302780747414, Final Batch Loss: 0.04305936023592949\n",
      "Epoch 2245, Loss: 0.2858119383454323, Final Batch Loss: 0.06883548200130463\n",
      "Epoch 2246, Loss: 0.2602956034243107, Final Batch Loss: 0.053791675716638565\n",
      "Epoch 2247, Loss: 0.32308533042669296, Final Batch Loss: 0.07009845972061157\n",
      "Epoch 2248, Loss: 0.34289998933672905, Final Batch Loss: 0.055228907614946365\n",
      "Epoch 2249, Loss: 0.3031904697418213, Final Batch Loss: 0.09691409021615982\n",
      "Epoch 2250, Loss: 0.30781930312514305, Final Batch Loss: 0.048535775393247604\n",
      "Epoch 2251, Loss: 0.19463903829455376, Final Batch Loss: 0.04737449064850807\n",
      "Epoch 2252, Loss: 0.3409884311258793, Final Batch Loss: 0.11958567798137665\n",
      "Epoch 2253, Loss: 0.24690672382712364, Final Batch Loss: 0.03107687085866928\n",
      "Epoch 2254, Loss: 0.22745658084750175, Final Batch Loss: 0.10632256418466568\n",
      "Epoch 2255, Loss: 0.23803412169218063, Final Batch Loss: 0.06765096634626389\n",
      "Epoch 2256, Loss: 0.24692125618457794, Final Batch Loss: 0.07493862509727478\n",
      "Epoch 2257, Loss: 0.3017819933593273, Final Batch Loss: 0.05628173425793648\n",
      "Epoch 2258, Loss: 0.2310590259730816, Final Batch Loss: 0.03414897993206978\n",
      "Epoch 2259, Loss: 0.2579471357166767, Final Batch Loss: 0.08238714933395386\n",
      "Epoch 2260, Loss: 0.234243705868721, Final Batch Loss: 0.055343180894851685\n",
      "Epoch 2261, Loss: 0.2699153311550617, Final Batch Loss: 0.02682563289999962\n",
      "Epoch 2262, Loss: 0.28854991495609283, Final Batch Loss: 0.05056874826550484\n",
      "Epoch 2263, Loss: 0.3165431618690491, Final Batch Loss: 0.12451447546482086\n",
      "Epoch 2264, Loss: 0.39818326011300087, Final Batch Loss: 0.05611616000533104\n",
      "Epoch 2265, Loss: 0.27527356147766113, Final Batch Loss: 0.09528928250074387\n",
      "Epoch 2266, Loss: 0.3856087103486061, Final Batch Loss: 0.1578913778066635\n",
      "Epoch 2267, Loss: 0.28745434060692787, Final Batch Loss: 0.050355155020952225\n",
      "Epoch 2268, Loss: 0.3581785224378109, Final Batch Loss: 0.06553386151790619\n",
      "Epoch 2269, Loss: 0.24003313295543194, Final Batch Loss: 0.030321506783366203\n",
      "Epoch 2270, Loss: 0.3119717091321945, Final Batch Loss: 0.07060378044843674\n",
      "Epoch 2271, Loss: 0.2697171363979578, Final Batch Loss: 0.07815767824649811\n",
      "Epoch 2272, Loss: 0.3578855022788048, Final Batch Loss: 0.042091481387615204\n",
      "Epoch 2273, Loss: 0.2731168046593666, Final Batch Loss: 0.08674492686986923\n",
      "Epoch 2274, Loss: 0.274038702249527, Final Batch Loss: 0.09670783579349518\n",
      "Epoch 2275, Loss: 0.39910992980003357, Final Batch Loss: 0.21423371136188507\n",
      "Epoch 2276, Loss: 0.25627074390649796, Final Batch Loss: 0.07233267277479172\n",
      "Epoch 2277, Loss: 0.2899639904499054, Final Batch Loss: 0.08606074005365372\n",
      "Epoch 2278, Loss: 0.3103138394653797, Final Batch Loss: 0.05562354624271393\n",
      "Epoch 2279, Loss: 0.32402584329247475, Final Batch Loss: 0.046634722501039505\n",
      "Epoch 2280, Loss: 0.3546250984072685, Final Batch Loss: 0.1264398992061615\n",
      "Epoch 2281, Loss: 0.32261674106121063, Final Batch Loss: 0.07580168545246124\n",
      "Epoch 2282, Loss: 0.3474137857556343, Final Batch Loss: 0.11915020644664764\n",
      "Epoch 2283, Loss: 0.2909832261502743, Final Batch Loss: 0.04991782829165459\n",
      "Epoch 2284, Loss: 0.37490179017186165, Final Batch Loss: 0.19320055842399597\n",
      "Epoch 2285, Loss: 0.32613571360707283, Final Batch Loss: 0.07488811761140823\n",
      "Epoch 2286, Loss: 0.29863647371530533, Final Batch Loss: 0.0649450495839119\n",
      "Epoch 2287, Loss: 0.3388274721801281, Final Batch Loss: 0.03837056830525398\n",
      "Epoch 2288, Loss: 0.2665868401527405, Final Batch Loss: 0.079056516289711\n",
      "Epoch 2289, Loss: 0.2595585808157921, Final Batch Loss: 0.05400015786290169\n",
      "Epoch 2290, Loss: 0.35317807644605637, Final Batch Loss: 0.10022928565740585\n",
      "Epoch 2291, Loss: 0.2317097783088684, Final Batch Loss: 0.05430656298995018\n",
      "Epoch 2292, Loss: 0.25428510643541813, Final Batch Loss: 0.060650937259197235\n",
      "Epoch 2293, Loss: 0.2543930523097515, Final Batch Loss: 0.06415800750255585\n",
      "Epoch 2294, Loss: 0.2985719367861748, Final Batch Loss: 0.11438040435314178\n",
      "Epoch 2295, Loss: 0.37591445446014404, Final Batch Loss: 0.16439050436019897\n",
      "Epoch 2296, Loss: 0.29249151051044464, Final Batch Loss: 0.05913626402616501\n",
      "Epoch 2297, Loss: 0.30462274700403214, Final Batch Loss: 0.08480546623468399\n",
      "Epoch 2298, Loss: 0.3322623260319233, Final Batch Loss: 0.15015731751918793\n",
      "Epoch 2299, Loss: 0.2855375036597252, Final Batch Loss: 0.04106678441166878\n",
      "Epoch 2300, Loss: 0.25980761274695396, Final Batch Loss: 0.046911563724279404\n",
      "Epoch 2301, Loss: 0.2960614077746868, Final Batch Loss: 0.04570705443620682\n",
      "Epoch 2302, Loss: 0.302602119743824, Final Batch Loss: 0.04936935007572174\n",
      "Epoch 2303, Loss: 0.2428349293768406, Final Batch Loss: 0.03173811733722687\n",
      "Epoch 2304, Loss: 0.3091576211154461, Final Batch Loss: 0.04366631433367729\n",
      "Epoch 2305, Loss: 0.2498628832399845, Final Batch Loss: 0.06689358502626419\n",
      "Epoch 2306, Loss: 0.23069101944565773, Final Batch Loss: 0.02415207400918007\n",
      "Epoch 2307, Loss: 0.26075852662324905, Final Batch Loss: 0.06400605291128159\n",
      "Epoch 2308, Loss: 0.23937290534377098, Final Batch Loss: 0.05254540592432022\n",
      "Epoch 2309, Loss: 0.210225198417902, Final Batch Loss: 0.061327990144491196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2310, Loss: 0.25105390325188637, Final Batch Loss: 0.03353999927639961\n",
      "Epoch 2311, Loss: 0.23641153052449226, Final Batch Loss: 0.04169655218720436\n",
      "Epoch 2312, Loss: 0.27973297983407974, Final Batch Loss: 0.11306682974100113\n",
      "Epoch 2313, Loss: 0.27087508141994476, Final Batch Loss: 0.034577466547489166\n",
      "Epoch 2314, Loss: 0.3499126136302948, Final Batch Loss: 0.12830349802970886\n",
      "Epoch 2315, Loss: 0.23330261185765266, Final Batch Loss: 0.037914399057626724\n",
      "Epoch 2316, Loss: 0.2569616436958313, Final Batch Loss: 0.05861780047416687\n",
      "Epoch 2317, Loss: 0.22180350497364998, Final Batch Loss: 0.024321570992469788\n",
      "Epoch 2318, Loss: 0.2999020591378212, Final Batch Loss: 0.06931903213262558\n",
      "Epoch 2319, Loss: 0.26229849457740784, Final Batch Loss: 0.047163523733615875\n",
      "Epoch 2320, Loss: 0.25711206160485744, Final Batch Loss: 0.027643365785479546\n",
      "Epoch 2321, Loss: 0.2944225184619427, Final Batch Loss: 0.12461589276790619\n",
      "Epoch 2322, Loss: 0.3004801981151104, Final Batch Loss: 0.04359118267893791\n",
      "Epoch 2323, Loss: 0.368729829788208, Final Batch Loss: 0.07593482732772827\n",
      "Epoch 2324, Loss: 0.3398207016289234, Final Batch Loss: 0.11077459901571274\n",
      "Epoch 2325, Loss: 0.30416180193424225, Final Batch Loss: 0.08253546804189682\n",
      "Epoch 2326, Loss: 0.26391080766916275, Final Batch Loss: 0.07426884770393372\n",
      "Epoch 2327, Loss: 0.369499072432518, Final Batch Loss: 0.1245683953166008\n",
      "Epoch 2328, Loss: 0.3809858337044716, Final Batch Loss: 0.08834201842546463\n",
      "Epoch 2329, Loss: 0.354884322732687, Final Batch Loss: 0.12497299164533615\n",
      "Epoch 2330, Loss: 0.33494947850704193, Final Batch Loss: 0.08855538815259933\n",
      "Epoch 2331, Loss: 0.24946511536836624, Final Batch Loss: 0.04892468824982643\n",
      "Epoch 2332, Loss: 0.3303654044866562, Final Batch Loss: 0.04879181832075119\n",
      "Epoch 2333, Loss: 0.547025240957737, Final Batch Loss: 0.22337576746940613\n",
      "Epoch 2334, Loss: 0.4516102075576782, Final Batch Loss: 0.10426557809114456\n",
      "Epoch 2335, Loss: 0.35917143523693085, Final Batch Loss: 0.10682351142168045\n",
      "Epoch 2336, Loss: 0.3521180935204029, Final Batch Loss: 0.05511755123734474\n",
      "Epoch 2337, Loss: 0.23254120349884033, Final Batch Loss: 0.025662098079919815\n",
      "Epoch 2338, Loss: 0.3904745429754257, Final Batch Loss: 0.056975096464157104\n",
      "Epoch 2339, Loss: 0.3708652891218662, Final Batch Loss: 0.0606747604906559\n",
      "Epoch 2340, Loss: 0.4187695011496544, Final Batch Loss: 0.12454978376626968\n",
      "Epoch 2341, Loss: 0.3908487930893898, Final Batch Loss: 0.07205884903669357\n",
      "Epoch 2342, Loss: 0.3342055156826973, Final Batch Loss: 0.08355690538883209\n",
      "Epoch 2343, Loss: 0.304820004850626, Final Batch Loss: 0.07475170493125916\n",
      "Epoch 2344, Loss: 0.5326395556330681, Final Batch Loss: 0.1587882936000824\n",
      "Epoch 2345, Loss: 0.36637259274721146, Final Batch Loss: 0.06771445274353027\n",
      "Epoch 2346, Loss: 0.4133000820875168, Final Batch Loss: 0.16625091433525085\n",
      "Epoch 2347, Loss: 0.2675993852317333, Final Batch Loss: 0.054146066308021545\n",
      "Epoch 2348, Loss: 0.285392414778471, Final Batch Loss: 0.045578669756650925\n",
      "Epoch 2349, Loss: 0.37074587494134903, Final Batch Loss: 0.08214199542999268\n",
      "Epoch 2350, Loss: 0.4102395251393318, Final Batch Loss: 0.11247127503156662\n",
      "Epoch 2351, Loss: 0.30834101513028145, Final Batch Loss: 0.08355895429849625\n",
      "Epoch 2352, Loss: 0.4354151338338852, Final Batch Loss: 0.11214178055524826\n",
      "Epoch 2353, Loss: 0.3288193792104721, Final Batch Loss: 0.05831560119986534\n",
      "Epoch 2354, Loss: 0.3572278507053852, Final Batch Loss: 0.12680330872535706\n",
      "Epoch 2355, Loss: 0.25193943083286285, Final Batch Loss: 0.05096658319234848\n",
      "Epoch 2356, Loss: 0.28663361072540283, Final Batch Loss: 0.047940708696842194\n",
      "Epoch 2357, Loss: 0.37509380280971527, Final Batch Loss: 0.07157571613788605\n",
      "Epoch 2358, Loss: 0.29386531189084053, Final Batch Loss: 0.057864367961883545\n",
      "Epoch 2359, Loss: 0.3146626651287079, Final Batch Loss: 0.06601247936487198\n",
      "Epoch 2360, Loss: 0.28957903757691383, Final Batch Loss: 0.07055656611919403\n",
      "Epoch 2361, Loss: 0.37602880597114563, Final Batch Loss: 0.1438692808151245\n",
      "Epoch 2362, Loss: 0.26323722302913666, Final Batch Loss: 0.061549700796604156\n",
      "Epoch 2363, Loss: 0.34033653885126114, Final Batch Loss: 0.09509087353944778\n",
      "Epoch 2364, Loss: 0.2547733336687088, Final Batch Loss: 0.04303537681698799\n",
      "Epoch 2365, Loss: 0.3659389242529869, Final Batch Loss: 0.16535881161689758\n",
      "Epoch 2366, Loss: 0.3165012188255787, Final Batch Loss: 0.12344110757112503\n",
      "Epoch 2367, Loss: 0.2802398279309273, Final Batch Loss: 0.07243222743272781\n",
      "Epoch 2368, Loss: 0.19551941938698292, Final Batch Loss: 0.05317772924900055\n",
      "Epoch 2369, Loss: 0.33674121648073196, Final Batch Loss: 0.07722597569227219\n",
      "Epoch 2370, Loss: 0.2988893426954746, Final Batch Loss: 0.06228797882795334\n",
      "Epoch 2371, Loss: 0.2829749323427677, Final Batch Loss: 0.0744110569357872\n",
      "Epoch 2372, Loss: 0.20616806112229824, Final Batch Loss: 0.02729167602956295\n",
      "Epoch 2373, Loss: 0.32389848306775093, Final Batch Loss: 0.08246786147356033\n",
      "Epoch 2374, Loss: 0.2190224677324295, Final Batch Loss: 0.045110493898391724\n",
      "Epoch 2375, Loss: 0.2715114466845989, Final Batch Loss: 0.023828372359275818\n",
      "Epoch 2376, Loss: 0.40005263686180115, Final Batch Loss: 0.15421868860721588\n",
      "Epoch 2377, Loss: 0.364948146045208, Final Batch Loss: 0.15756800770759583\n",
      "Epoch 2378, Loss: 0.24092791974544525, Final Batch Loss: 0.0802605152130127\n",
      "Epoch 2379, Loss: 0.46275465935468674, Final Batch Loss: 0.16500116884708405\n",
      "Epoch 2380, Loss: 0.2921854518353939, Final Batch Loss: 0.054629284888505936\n",
      "Epoch 2381, Loss: 0.5243968963623047, Final Batch Loss: 0.13837556540966034\n",
      "Epoch 2382, Loss: 0.29815755039453506, Final Batch Loss: 0.10916129499673843\n",
      "Epoch 2383, Loss: 0.4408745691180229, Final Batch Loss: 0.07757798582315445\n",
      "Epoch 2384, Loss: 0.32267145067453384, Final Batch Loss: 0.06266408413648605\n",
      "Epoch 2385, Loss: 0.37483545392751694, Final Batch Loss: 0.09689047932624817\n",
      "Epoch 2386, Loss: 0.3401390239596367, Final Batch Loss: 0.08072105795145035\n",
      "Epoch 2387, Loss: 0.36702265590429306, Final Batch Loss: 0.08205553889274597\n",
      "Epoch 2388, Loss: 0.2860265299677849, Final Batch Loss: 0.0681847557425499\n",
      "Epoch 2389, Loss: 0.2514561526477337, Final Batch Loss: 0.032182831317186356\n",
      "Epoch 2390, Loss: 0.21134496666491032, Final Batch Loss: 0.02284218929708004\n",
      "Epoch 2391, Loss: 0.26680803671479225, Final Batch Loss: 0.06633860617876053\n",
      "Epoch 2392, Loss: 0.3270322158932686, Final Batch Loss: 0.10055724531412125\n",
      "Epoch 2393, Loss: 0.21110955625772476, Final Batch Loss: 0.03955080360174179\n",
      "Epoch 2394, Loss: 0.24959948658943176, Final Batch Loss: 0.031292520463466644\n",
      "Epoch 2395, Loss: 0.2637513466179371, Final Batch Loss: 0.042439356446266174\n",
      "Epoch 2396, Loss: 0.2906288281083107, Final Batch Loss: 0.06118590384721756\n",
      "Epoch 2397, Loss: 0.3537809383124113, Final Batch Loss: 0.18004390597343445\n",
      "Epoch 2398, Loss: 0.30090704932808876, Final Batch Loss: 0.11370962113142014\n",
      "Epoch 2399, Loss: 0.23497705906629562, Final Batch Loss: 0.056124646216630936\n",
      "Epoch 2400, Loss: 0.3726965859532356, Final Batch Loss: 0.15026825666427612\n",
      "Epoch 2401, Loss: 0.23410745337605476, Final Batch Loss: 0.04393139109015465\n",
      "Epoch 2402, Loss: 0.2498033232986927, Final Batch Loss: 0.06213263049721718\n",
      "Epoch 2403, Loss: 0.18779706209897995, Final Batch Loss: 0.031077250838279724\n",
      "Epoch 2404, Loss: 0.26539918035268784, Final Batch Loss: 0.04556265473365784\n",
      "Epoch 2405, Loss: 0.32695553451776505, Final Batch Loss: 0.08432336896657944\n",
      "Epoch 2406, Loss: 0.3131934553384781, Final Batch Loss: 0.10089944303035736\n",
      "Epoch 2407, Loss: 0.31837597861886024, Final Batch Loss: 0.03635735809803009\n",
      "Epoch 2408, Loss: 0.2350926734507084, Final Batch Loss: 0.07351755350828171\n",
      "Epoch 2409, Loss: 0.2462559211999178, Final Batch Loss: 0.02135639078915119\n",
      "Epoch 2410, Loss: 0.2419964335858822, Final Batch Loss: 0.04320476949214935\n",
      "Epoch 2411, Loss: 0.28130388259887695, Final Batch Loss: 0.08675330877304077\n",
      "Epoch 2412, Loss: 0.327000193297863, Final Batch Loss: 0.08728372305631638\n",
      "Epoch 2413, Loss: 0.2744211256504059, Final Batch Loss: 0.06932835280895233\n",
      "Epoch 2414, Loss: 0.29554762318730354, Final Batch Loss: 0.03546423837542534\n",
      "Epoch 2415, Loss: 0.3476132098585367, Final Batch Loss: 0.03049534000456333\n",
      "Epoch 2416, Loss: 0.24606700986623764, Final Batch Loss: 0.056284788995981216\n",
      "Epoch 2417, Loss: 0.28305595740675926, Final Batch Loss: 0.06707438081502914\n",
      "Epoch 2418, Loss: 0.24767024628818035, Final Batch Loss: 0.02652815915644169\n",
      "Epoch 2419, Loss: 0.253975223749876, Final Batch Loss: 0.0691504180431366\n",
      "Epoch 2420, Loss: 0.28524600341916084, Final Batch Loss: 0.0930691733956337\n",
      "Epoch 2421, Loss: 0.22172866389155388, Final Batch Loss: 0.045222196727991104\n",
      "Epoch 2422, Loss: 0.29021143168210983, Final Batch Loss: 0.02297629415988922\n",
      "Epoch 2423, Loss: 0.24625998735427856, Final Batch Loss: 0.08044324815273285\n",
      "Epoch 2424, Loss: 0.24924077466130257, Final Batch Loss: 0.0809411033987999\n",
      "Epoch 2425, Loss: 0.2899552546441555, Final Batch Loss: 0.0376499705016613\n",
      "Epoch 2426, Loss: 0.2022046037018299, Final Batch Loss: 0.03973112627863884\n",
      "Epoch 2427, Loss: 0.2761359252035618, Final Batch Loss: 0.021712739020586014\n",
      "Epoch 2428, Loss: 0.27374816685914993, Final Batch Loss: 0.08298864960670471\n",
      "Epoch 2429, Loss: 0.2798347435891628, Final Batch Loss: 0.046232517808675766\n",
      "Epoch 2430, Loss: 0.20455686934292316, Final Batch Loss: 0.028102515265345573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2431, Loss: 0.2325669564306736, Final Batch Loss: 0.017996445298194885\n",
      "Epoch 2432, Loss: 0.20628616213798523, Final Batch Loss: 0.036120180040597916\n",
      "Epoch 2433, Loss: 0.28291894122958183, Final Batch Loss: 0.11787447333335876\n",
      "Epoch 2434, Loss: 0.2764308750629425, Final Batch Loss: 0.047663602977991104\n",
      "Epoch 2435, Loss: 0.2870708256959915, Final Batch Loss: 0.0895327553153038\n",
      "Epoch 2436, Loss: 0.2652764096856117, Final Batch Loss: 0.040206074714660645\n",
      "Epoch 2437, Loss: 0.28323500603437424, Final Batch Loss: 0.08646602183580399\n",
      "Epoch 2438, Loss: 0.2799246311187744, Final Batch Loss: 0.06422953307628632\n",
      "Epoch 2439, Loss: 0.29972337931394577, Final Batch Loss: 0.11276742070913315\n",
      "Epoch 2440, Loss: 0.242396280169487, Final Batch Loss: 0.05281352996826172\n",
      "Epoch 2441, Loss: 0.3102096766233444, Final Batch Loss: 0.057473257184028625\n",
      "Epoch 2442, Loss: 0.36404648423194885, Final Batch Loss: 0.08532989025115967\n",
      "Epoch 2443, Loss: 0.2684783414006233, Final Batch Loss: 0.1076880544424057\n",
      "Epoch 2444, Loss: 0.34552786499261856, Final Batch Loss: 0.17102275788784027\n",
      "Epoch 2445, Loss: 0.34569600969552994, Final Batch Loss: 0.13077472150325775\n",
      "Epoch 2446, Loss: 0.3147088512778282, Final Batch Loss: 0.0726434662938118\n",
      "Epoch 2447, Loss: 0.2675294317305088, Final Batch Loss: 0.05138386785984039\n",
      "Epoch 2448, Loss: 0.339475117623806, Final Batch Loss: 0.1709059476852417\n",
      "Epoch 2449, Loss: 0.23176676407456398, Final Batch Loss: 0.04561564326286316\n",
      "Epoch 2450, Loss: 0.30149495229125023, Final Batch Loss: 0.112388014793396\n",
      "Epoch 2451, Loss: 0.29617946222424507, Final Batch Loss: 0.0733129233121872\n",
      "Epoch 2452, Loss: 0.3417294919490814, Final Batch Loss: 0.12567557394504547\n",
      "Epoch 2453, Loss: 0.20059184357523918, Final Batch Loss: 0.01797589287161827\n",
      "Epoch 2454, Loss: 0.28510593995451927, Final Batch Loss: 0.05040667578577995\n",
      "Epoch 2455, Loss: 0.33736508153378963, Final Batch Loss: 0.0171933826059103\n",
      "Epoch 2456, Loss: 0.207717377692461, Final Batch Loss: 0.039869531989097595\n",
      "Epoch 2457, Loss: 0.2770760916173458, Final Batch Loss: 0.046557605266571045\n",
      "Epoch 2458, Loss: 0.2997990846633911, Final Batch Loss: 0.11387331038713455\n",
      "Epoch 2459, Loss: 0.29015052318573, Final Batch Loss: 0.06034735217690468\n",
      "Epoch 2460, Loss: 0.3992510065436363, Final Batch Loss: 0.11237987875938416\n",
      "Epoch 2461, Loss: 0.3433977775275707, Final Batch Loss: 0.053672920912504196\n",
      "Epoch 2462, Loss: 0.2998094856739044, Final Batch Loss: 0.09130779653787613\n",
      "Epoch 2463, Loss: 0.31831222772598267, Final Batch Loss: 0.08343414962291718\n",
      "Epoch 2464, Loss: 0.26217394322156906, Final Batch Loss: 0.05901956930756569\n",
      "Epoch 2465, Loss: 0.28017307072877884, Final Batch Loss: 0.03638370335102081\n",
      "Epoch 2466, Loss: 0.34655117243528366, Final Batch Loss: 0.04962455481290817\n",
      "Epoch 2467, Loss: 0.23157425969839096, Final Batch Loss: 0.06371624022722244\n",
      "Epoch 2468, Loss: 0.2898388020694256, Final Batch Loss: 0.1298530548810959\n",
      "Epoch 2469, Loss: 0.265063788741827, Final Batch Loss: 0.044451724737882614\n",
      "Epoch 2470, Loss: 0.33680102601647377, Final Batch Loss: 0.1472296565771103\n",
      "Epoch 2471, Loss: 0.2543697375804186, Final Batch Loss: 0.03030821867287159\n",
      "Epoch 2472, Loss: 0.340214965865016, Final Batch Loss: 0.023555321618914604\n",
      "Epoch 2473, Loss: 0.3013682924211025, Final Batch Loss: 0.027407392859458923\n",
      "Epoch 2474, Loss: 0.26800502836704254, Final Batch Loss: 0.10571561753749847\n",
      "Epoch 2475, Loss: 0.22877676039934158, Final Batch Loss: 0.07131759077310562\n",
      "Epoch 2476, Loss: 0.2454625628888607, Final Batch Loss: 0.08228987455368042\n",
      "Epoch 2477, Loss: 0.25013723969459534, Final Batch Loss: 0.050358787178993225\n",
      "Epoch 2478, Loss: 0.289046335965395, Final Batch Loss: 0.10983920842409134\n",
      "Epoch 2479, Loss: 0.30856688879430294, Final Batch Loss: 0.029613284394145012\n",
      "Epoch 2480, Loss: 0.31024058908224106, Final Batch Loss: 0.10757601261138916\n",
      "Epoch 2481, Loss: 0.27679601684212685, Final Batch Loss: 0.10276228189468384\n",
      "Epoch 2482, Loss: 0.4551626816391945, Final Batch Loss: 0.08353199809789658\n",
      "Epoch 2483, Loss: 0.22353070974349976, Final Batch Loss: 0.04847110062837601\n",
      "Epoch 2484, Loss: 0.2892838232219219, Final Batch Loss: 0.10336912423372269\n",
      "Epoch 2485, Loss: 0.27406639233231544, Final Batch Loss: 0.05698603391647339\n",
      "Epoch 2486, Loss: 0.3330885171890259, Final Batch Loss: 0.1108638346195221\n",
      "Epoch 2487, Loss: 0.23693588748574257, Final Batch Loss: 0.06160237267613411\n",
      "Epoch 2488, Loss: 0.3392617665231228, Final Batch Loss: 0.10296931862831116\n",
      "Epoch 2489, Loss: 0.3373227119445801, Final Batch Loss: 0.08110951632261276\n",
      "Epoch 2490, Loss: 0.2788897007703781, Final Batch Loss: 0.062436074018478394\n",
      "Epoch 2491, Loss: 0.36331913620233536, Final Batch Loss: 0.11803750693798065\n",
      "Epoch 2492, Loss: 0.47602738440036774, Final Batch Loss: 0.12597954273223877\n",
      "Epoch 2493, Loss: 0.28978846967220306, Final Batch Loss: 0.054538361728191376\n",
      "Epoch 2494, Loss: 0.2993040047585964, Final Batch Loss: 0.08471516519784927\n",
      "Epoch 2495, Loss: 0.2655653730034828, Final Batch Loss: 0.06651485711336136\n",
      "Epoch 2496, Loss: 0.3504428379237652, Final Batch Loss: 0.11099529266357422\n",
      "Epoch 2497, Loss: 0.2587187811732292, Final Batch Loss: 0.10593470185995102\n",
      "Epoch 2498, Loss: 0.28576644510030746, Final Batch Loss: 0.04386883229017258\n",
      "Epoch 2499, Loss: 0.2992362976074219, Final Batch Loss: 0.10558636486530304\n",
      "Epoch 2500, Loss: 0.2706526145339012, Final Batch Loss: 0.09278363734483719\n",
      "Epoch 2501, Loss: 0.26282674446702003, Final Batch Loss: 0.1086970642209053\n",
      "Epoch 2502, Loss: 0.28009944036602974, Final Batch Loss: 0.07689734548330307\n",
      "Epoch 2503, Loss: 0.35636959224939346, Final Batch Loss: 0.12618374824523926\n",
      "Epoch 2504, Loss: 0.3080364763736725, Final Batch Loss: 0.07605019956827164\n",
      "Epoch 2505, Loss: 0.2710420452058315, Final Batch Loss: 0.08388987183570862\n",
      "Epoch 2506, Loss: 0.25112712755799294, Final Batch Loss: 0.07340042293071747\n",
      "Epoch 2507, Loss: 0.23254916071891785, Final Batch Loss: 0.04089146479964256\n",
      "Epoch 2508, Loss: 0.33647267520427704, Final Batch Loss: 0.08015900105237961\n",
      "Epoch 2509, Loss: 0.25485342368483543, Final Batch Loss: 0.08196844160556793\n",
      "Epoch 2510, Loss: 0.2329169511795044, Final Batch Loss: 0.0374131053686142\n",
      "Epoch 2511, Loss: 0.24784518778324127, Final Batch Loss: 0.0405462421476841\n",
      "Epoch 2512, Loss: 0.24667932465672493, Final Batch Loss: 0.02967187389731407\n",
      "Epoch 2513, Loss: 0.2754546105861664, Final Batch Loss: 0.07108049839735031\n",
      "Epoch 2514, Loss: 0.2746370695531368, Final Batch Loss: 0.0646895319223404\n",
      "Epoch 2515, Loss: 0.21240575797855854, Final Batch Loss: 0.042964641004800797\n",
      "Epoch 2516, Loss: 0.28639455884695053, Final Batch Loss: 0.09402592480182648\n",
      "Epoch 2517, Loss: 0.23639925569295883, Final Batch Loss: 0.01778104156255722\n",
      "Epoch 2518, Loss: 0.2552792578935623, Final Batch Loss: 0.0957411676645279\n",
      "Epoch 2519, Loss: 0.29522333294153214, Final Batch Loss: 0.10014725476503372\n",
      "Epoch 2520, Loss: 0.3085520267486572, Final Batch Loss: 0.03924299404025078\n",
      "Epoch 2521, Loss: 0.24229562282562256, Final Batch Loss: 0.03245444595813751\n",
      "Epoch 2522, Loss: 0.33990955352783203, Final Batch Loss: 0.09663684666156769\n",
      "Epoch 2523, Loss: 0.4134930819272995, Final Batch Loss: 0.12413293868303299\n",
      "Epoch 2524, Loss: 0.2772947959601879, Final Batch Loss: 0.08210104703903198\n",
      "Epoch 2525, Loss: 0.2558679170906544, Final Batch Loss: 0.08155025541782379\n",
      "Epoch 2526, Loss: 0.3196369968354702, Final Batch Loss: 0.04881877079606056\n",
      "Epoch 2527, Loss: 0.20597875118255615, Final Batch Loss: 0.055943477898836136\n",
      "Epoch 2528, Loss: 0.2964095249772072, Final Batch Loss: 0.12054330110549927\n",
      "Epoch 2529, Loss: 0.29399149492383003, Final Batch Loss: 0.11883438378572464\n",
      "Epoch 2530, Loss: 0.25325601547956467, Final Batch Loss: 0.034579940140247345\n",
      "Epoch 2531, Loss: 0.29240580275654793, Final Batch Loss: 0.05874438211321831\n",
      "Epoch 2532, Loss: 0.2644767686724663, Final Batch Loss: 0.048857443034648895\n",
      "Epoch 2533, Loss: 0.29889264330267906, Final Batch Loss: 0.03456248715519905\n",
      "Epoch 2534, Loss: 0.3822908513247967, Final Batch Loss: 0.1621178686618805\n",
      "Epoch 2535, Loss: 0.30892302468419075, Final Batch Loss: 0.04665759950876236\n",
      "Epoch 2536, Loss: 0.3435260206460953, Final Batch Loss: 0.18987588584423065\n",
      "Epoch 2537, Loss: 0.4009825475513935, Final Batch Loss: 0.17886130511760712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2538, Loss: 0.30773160234093666, Final Batch Loss: 0.052417151629924774\n",
      "Epoch 2539, Loss: 0.29766531847417355, Final Batch Loss: 0.024858547374606133\n",
      "Epoch 2540, Loss: 0.3235539384186268, Final Batch Loss: 0.046342235058546066\n",
      "Epoch 2541, Loss: 0.36725234240293503, Final Batch Loss: 0.15922610461711884\n",
      "Epoch 2542, Loss: 0.2404419668018818, Final Batch Loss: 0.03972037881612778\n",
      "Epoch 2543, Loss: 0.3661445900797844, Final Batch Loss: 0.14142797887325287\n",
      "Epoch 2544, Loss: 0.31606828421354294, Final Batch Loss: 0.05293538421392441\n",
      "Epoch 2545, Loss: 0.3113231398165226, Final Batch Loss: 0.025813374668359756\n",
      "Epoch 2546, Loss: 0.26408467441797256, Final Batch Loss: 0.08872783929109573\n",
      "Epoch 2547, Loss: 0.3101172372698784, Final Batch Loss: 0.13810452818870544\n",
      "Epoch 2548, Loss: 0.2063208408653736, Final Batch Loss: 0.052227120846509933\n",
      "Epoch 2549, Loss: 0.3821171447634697, Final Batch Loss: 0.1250022053718567\n",
      "Epoch 2550, Loss: 0.31095194071531296, Final Batch Loss: 0.06836464256048203\n",
      "Epoch 2551, Loss: 0.33333706483244896, Final Batch Loss: 0.12802955508232117\n",
      "Epoch 2552, Loss: 0.2656555213034153, Final Batch Loss: 0.0754082053899765\n",
      "Epoch 2553, Loss: 0.31967905908823013, Final Batch Loss: 0.07111472636461258\n",
      "Epoch 2554, Loss: 0.27343643456697464, Final Batch Loss: 0.040828414261341095\n",
      "Epoch 2555, Loss: 0.24256744608283043, Final Batch Loss: 0.04223934933543205\n",
      "Epoch 2556, Loss: 0.24736059829592705, Final Batch Loss: 0.03054143860936165\n",
      "Epoch 2557, Loss: 0.24941643700003624, Final Batch Loss: 0.10261102020740509\n",
      "Epoch 2558, Loss: 0.21882073394954205, Final Batch Loss: 0.025106003507971764\n",
      "Epoch 2559, Loss: 0.25320449098944664, Final Batch Loss: 0.08452533930540085\n",
      "Epoch 2560, Loss: 0.32083766907453537, Final Batch Loss: 0.08255352079868317\n",
      "Epoch 2561, Loss: 0.2495930679142475, Final Batch Loss: 0.07857771962881088\n",
      "Epoch 2562, Loss: 0.2474848460406065, Final Batch Loss: 0.02041015960276127\n",
      "Epoch 2563, Loss: 0.28580254688858986, Final Batch Loss: 0.11797519028186798\n",
      "Epoch 2564, Loss: 0.2684789001941681, Final Batch Loss: 0.0910375714302063\n",
      "Epoch 2565, Loss: 0.3026800751686096, Final Batch Loss: 0.0637112483382225\n",
      "Epoch 2566, Loss: 0.21921906992793083, Final Batch Loss: 0.05482228100299835\n",
      "Epoch 2567, Loss: 0.2720990777015686, Final Batch Loss: 0.08987867832183838\n",
      "Epoch 2568, Loss: 0.27761532738804817, Final Batch Loss: 0.05404454097151756\n",
      "Epoch 2569, Loss: 0.2545361742377281, Final Batch Loss: 0.04816433787345886\n",
      "Epoch 2570, Loss: 0.356622114777565, Final Batch Loss: 0.07425708323717117\n",
      "Epoch 2571, Loss: 0.22026477754116058, Final Batch Loss: 0.05289725586771965\n",
      "Epoch 2572, Loss: 0.20392031781375408, Final Batch Loss: 0.02293860726058483\n",
      "Epoch 2573, Loss: 0.22960219345986843, Final Batch Loss: 0.06550012528896332\n",
      "Epoch 2574, Loss: 0.2529859133064747, Final Batch Loss: 0.05189308151602745\n",
      "Epoch 2575, Loss: 0.25522997230291367, Final Batch Loss: 0.028203003108501434\n",
      "Epoch 2576, Loss: 0.25082216784358025, Final Batch Loss: 0.03720412030816078\n",
      "Epoch 2577, Loss: 0.19884057249873877, Final Batch Loss: 0.01394737046211958\n",
      "Epoch 2578, Loss: 0.327461339533329, Final Batch Loss: 0.09836900234222412\n",
      "Epoch 2579, Loss: 0.32448484748601913, Final Batch Loss: 0.08607584238052368\n",
      "Epoch 2580, Loss: 0.28977537527680397, Final Batch Loss: 0.052384153008461\n",
      "Epoch 2581, Loss: 0.3058915250003338, Final Batch Loss: 0.11102944612503052\n",
      "Epoch 2582, Loss: 0.29867152497172356, Final Batch Loss: 0.12416809052228928\n",
      "Epoch 2583, Loss: 0.3652179054915905, Final Batch Loss: 0.06866913288831711\n",
      "Epoch 2584, Loss: 0.2607533223927021, Final Batch Loss: 0.06213473901152611\n",
      "Epoch 2585, Loss: 0.28264907747507095, Final Batch Loss: 0.048718295991420746\n",
      "Epoch 2586, Loss: 0.3379488103091717, Final Batch Loss: 0.15177908539772034\n",
      "Epoch 2587, Loss: 0.20791388489305973, Final Batch Loss: 0.02822955884039402\n",
      "Epoch 2588, Loss: 0.30969947949051857, Final Batch Loss: 0.05850477144122124\n",
      "Epoch 2589, Loss: 0.34241510555148125, Final Batch Loss: 0.10668540745973587\n",
      "Epoch 2590, Loss: 0.4135774299502373, Final Batch Loss: 0.09421844035387039\n",
      "Epoch 2591, Loss: 0.3267161026597023, Final Batch Loss: 0.09592992812395096\n",
      "Epoch 2592, Loss: 0.3174032233655453, Final Batch Loss: 0.037933945655822754\n",
      "Epoch 2593, Loss: 0.24468456581234932, Final Batch Loss: 0.03738084062933922\n",
      "Epoch 2594, Loss: 0.22630725987255573, Final Batch Loss: 0.028758777305483818\n",
      "Epoch 2595, Loss: 0.24606264010071754, Final Batch Loss: 0.0575781986117363\n",
      "Epoch 2596, Loss: 0.27305813878774643, Final Batch Loss: 0.1225927397608757\n",
      "Epoch 2597, Loss: 0.26904652640223503, Final Batch Loss: 0.04187558591365814\n",
      "Epoch 2598, Loss: 0.26492659002542496, Final Batch Loss: 0.036290720105171204\n",
      "Epoch 2599, Loss: 0.26253657415509224, Final Batch Loss: 0.10036948323249817\n",
      "Epoch 2600, Loss: 0.234026737511158, Final Batch Loss: 0.08859752863645554\n",
      "Epoch 2601, Loss: 0.25743914768099785, Final Batch Loss: 0.07086683809757233\n",
      "Epoch 2602, Loss: 0.24500375986099243, Final Batch Loss: 0.03595441207289696\n",
      "Epoch 2603, Loss: 0.24449168890714645, Final Batch Loss: 0.04114009439945221\n",
      "Epoch 2604, Loss: 0.20394793152809143, Final Batch Loss: 0.03353648632764816\n",
      "Epoch 2605, Loss: 0.225960748270154, Final Batch Loss: 0.011433949694037437\n",
      "Epoch 2606, Loss: 0.250826645642519, Final Batch Loss: 0.026676874607801437\n",
      "Epoch 2607, Loss: 0.20250468514859676, Final Batch Loss: 0.03653092309832573\n",
      "Epoch 2608, Loss: 0.29327983409166336, Final Batch Loss: 0.060427334159612656\n",
      "Epoch 2609, Loss: 0.24160780757665634, Final Batch Loss: 0.0369337797164917\n",
      "Epoch 2610, Loss: 0.1920990515500307, Final Batch Loss: 0.021100295707583427\n",
      "Epoch 2611, Loss: 0.2822646554559469, Final Batch Loss: 0.13342957198619843\n",
      "Epoch 2612, Loss: 0.23696394078433514, Final Batch Loss: 0.02789982594549656\n",
      "Epoch 2613, Loss: 0.24049722403287888, Final Batch Loss: 0.02982281520962715\n",
      "Epoch 2614, Loss: 0.2017674781382084, Final Batch Loss: 0.045369431376457214\n",
      "Epoch 2615, Loss: 0.2024196982383728, Final Batch Loss: 0.06097283214330673\n",
      "Epoch 2616, Loss: 0.31269461289048195, Final Batch Loss: 0.10462887585163116\n",
      "Epoch 2617, Loss: 0.20904575660824776, Final Batch Loss: 0.0641658678650856\n",
      "Epoch 2618, Loss: 0.347749512642622, Final Batch Loss: 0.07703187316656113\n",
      "Epoch 2619, Loss: 0.25856367126107216, Final Batch Loss: 0.04068445786833763\n",
      "Epoch 2620, Loss: 0.2695653401315212, Final Batch Loss: 0.0992085263133049\n",
      "Epoch 2621, Loss: 0.2616560310125351, Final Batch Loss: 0.10834408551454544\n",
      "Epoch 2622, Loss: 0.22787554562091827, Final Batch Loss: 0.029015690088272095\n",
      "Epoch 2623, Loss: 0.3326495662331581, Final Batch Loss: 0.11751864850521088\n",
      "Epoch 2624, Loss: 0.2291586510837078, Final Batch Loss: 0.01769624650478363\n",
      "Epoch 2625, Loss: 0.3315005488693714, Final Batch Loss: 0.09060635417699814\n",
      "Epoch 2626, Loss: 0.35883041098713875, Final Batch Loss: 0.16641823947429657\n",
      "Epoch 2627, Loss: 0.20537706092000008, Final Batch Loss: 0.049443721771240234\n",
      "Epoch 2628, Loss: 0.28041432052850723, Final Batch Loss: 0.0687451958656311\n",
      "Epoch 2629, Loss: 0.1950959376990795, Final Batch Loss: 0.04389103874564171\n",
      "Epoch 2630, Loss: 0.261457834392786, Final Batch Loss: 0.03852735832333565\n",
      "Epoch 2631, Loss: 0.2552388869225979, Final Batch Loss: 0.0734376609325409\n",
      "Epoch 2632, Loss: 0.24195311218500137, Final Batch Loss: 0.05464167147874832\n",
      "Epoch 2633, Loss: 0.26243384554982185, Final Batch Loss: 0.09954313188791275\n",
      "Epoch 2634, Loss: 0.27948566153645515, Final Batch Loss: 0.043562695384025574\n",
      "Epoch 2635, Loss: 0.2583947218954563, Final Batch Loss: 0.0653267353773117\n",
      "Epoch 2636, Loss: 0.34330907091498375, Final Batch Loss: 0.09370594471693039\n",
      "Epoch 2637, Loss: 0.32943956926465034, Final Batch Loss: 0.08336048573255539\n",
      "Epoch 2638, Loss: 0.2988632060587406, Final Batch Loss: 0.10857757925987244\n",
      "Epoch 2639, Loss: 0.30835510790348053, Final Batch Loss: 0.062261443585157394\n",
      "Epoch 2640, Loss: 0.2516019996255636, Final Batch Loss: 0.021423762664198875\n",
      "Epoch 2641, Loss: 0.33197564631700516, Final Batch Loss: 0.12580890953540802\n",
      "Epoch 2642, Loss: 0.3121405839920044, Final Batch Loss: 0.06723572313785553\n",
      "Epoch 2643, Loss: 0.4726236052811146, Final Batch Loss: 0.05243363603949547\n",
      "Epoch 2644, Loss: 0.26326774433255196, Final Batch Loss: 0.059631988406181335\n",
      "Epoch 2645, Loss: 0.36360374838113785, Final Batch Loss: 0.09797892719507217\n",
      "Epoch 2646, Loss: 0.42472440749406815, Final Batch Loss: 0.12433549016714096\n",
      "Epoch 2647, Loss: 0.2977845259010792, Final Batch Loss: 0.03879910334944725\n",
      "Epoch 2648, Loss: 0.3695494569838047, Final Batch Loss: 0.11954503506422043\n",
      "Epoch 2649, Loss: 0.30899900011718273, Final Batch Loss: 0.01824798993766308\n",
      "Epoch 2650, Loss: 0.3392249159514904, Final Batch Loss: 0.10570814460515976\n",
      "Epoch 2651, Loss: 0.22295550256967545, Final Batch Loss: 0.025765907019376755\n",
      "Epoch 2652, Loss: 0.24770685657858849, Final Batch Loss: 0.06941269338130951\n",
      "Epoch 2653, Loss: 0.280428871512413, Final Batch Loss: 0.04301667585968971\n",
      "Epoch 2654, Loss: 0.2841941863298416, Final Batch Loss: 0.08006090670824051\n",
      "Epoch 2655, Loss: 0.24025138095021248, Final Batch Loss: 0.09021034836769104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2656, Loss: 0.26824162155389786, Final Batch Loss: 0.039863795042037964\n",
      "Epoch 2657, Loss: 0.3460831046104431, Final Batch Loss: 0.11021693795919418\n",
      "Epoch 2658, Loss: 0.2897929511964321, Final Batch Loss: 0.08093253523111343\n",
      "Epoch 2659, Loss: 0.3131896108388901, Final Batch Loss: 0.06444799154996872\n",
      "Epoch 2660, Loss: 0.27467288821935654, Final Batch Loss: 0.07730083912611008\n",
      "Epoch 2661, Loss: 0.2959476076066494, Final Batch Loss: 0.031010989099740982\n",
      "Epoch 2662, Loss: 0.25725941359996796, Final Batch Loss: 0.0766206905245781\n",
      "Epoch 2663, Loss: 0.2914476953446865, Final Batch Loss: 0.06304873526096344\n",
      "Epoch 2664, Loss: 0.2583506666123867, Final Batch Loss: 0.08662733435630798\n",
      "Epoch 2665, Loss: 0.29194654896855354, Final Batch Loss: 0.09048361331224442\n",
      "Epoch 2666, Loss: 0.20717020519077778, Final Batch Loss: 0.023830244317650795\n",
      "Epoch 2667, Loss: 0.34718702360987663, Final Batch Loss: 0.14574982225894928\n",
      "Epoch 2668, Loss: 0.22836945205926895, Final Batch Loss: 0.06969818472862244\n",
      "Epoch 2669, Loss: 0.3315269388258457, Final Batch Loss: 0.09305855631828308\n",
      "Epoch 2670, Loss: 0.2946636490523815, Final Batch Loss: 0.090210922062397\n",
      "Epoch 2671, Loss: 0.423916969448328, Final Batch Loss: 0.1701103001832962\n",
      "Epoch 2672, Loss: 0.29438431933522224, Final Batch Loss: 0.10002166777849197\n",
      "Epoch 2673, Loss: 0.30766381695866585, Final Batch Loss: 0.06502244621515274\n",
      "Epoch 2674, Loss: 0.23386595398187637, Final Batch Loss: 0.059238191694021225\n",
      "Epoch 2675, Loss: 0.18600709084421396, Final Batch Loss: 0.009482522495090961\n",
      "Epoch 2676, Loss: 0.1812632605433464, Final Batch Loss: 0.03305766358971596\n",
      "Epoch 2677, Loss: 0.2961837835609913, Final Batch Loss: 0.12290734797716141\n",
      "Epoch 2678, Loss: 0.3022576905786991, Final Batch Loss: 0.09784022718667984\n",
      "Epoch 2679, Loss: 0.3341301716864109, Final Batch Loss: 0.08739947527647018\n",
      "Epoch 2680, Loss: 0.29925045743584633, Final Batch Loss: 0.13215911388397217\n",
      "Epoch 2681, Loss: 0.2460637278854847, Final Batch Loss: 0.043180037289857864\n",
      "Epoch 2682, Loss: 0.3702358640730381, Final Batch Loss: 0.16536690294742584\n",
      "Epoch 2683, Loss: 0.2907656952738762, Final Batch Loss: 0.06690452992916107\n",
      "Epoch 2684, Loss: 0.2526202127337456, Final Batch Loss: 0.07386884093284607\n",
      "Epoch 2685, Loss: 0.26636141538619995, Final Batch Loss: 0.06485535204410553\n",
      "Epoch 2686, Loss: 0.2666422538459301, Final Batch Loss: 0.052044276148080826\n",
      "Epoch 2687, Loss: 0.33480288460850716, Final Batch Loss: 0.12991350889205933\n",
      "Epoch 2688, Loss: 0.28135916590690613, Final Batch Loss: 0.07639452815055847\n",
      "Epoch 2689, Loss: 0.3542121797800064, Final Batch Loss: 0.07694690674543381\n",
      "Epoch 2690, Loss: 0.38887932896614075, Final Batch Loss: 0.1314188539981842\n",
      "Epoch 2691, Loss: 0.3786521889269352, Final Batch Loss: 0.1792898029088974\n",
      "Epoch 2692, Loss: 0.28342974185943604, Final Batch Loss: 0.04247964173555374\n",
      "Epoch 2693, Loss: 0.371396541595459, Final Batch Loss: 0.12217804789543152\n",
      "Epoch 2694, Loss: 0.3262915648519993, Final Batch Loss: 0.1397627592086792\n",
      "Epoch 2695, Loss: 0.1844075284898281, Final Batch Loss: 0.03439111262559891\n",
      "Epoch 2696, Loss: 0.25627733767032623, Final Batch Loss: 0.04935629293322563\n",
      "Epoch 2697, Loss: 0.2518348768353462, Final Batch Loss: 0.0677388533949852\n",
      "Epoch 2698, Loss: 0.18285421654582024, Final Batch Loss: 0.021633900701999664\n",
      "Epoch 2699, Loss: 0.27637989073991776, Final Batch Loss: 0.06012255698442459\n",
      "Epoch 2700, Loss: 0.2172803170979023, Final Batch Loss: 0.03975654020905495\n",
      "Epoch 2701, Loss: 0.2333653438836336, Final Batch Loss: 0.02012009359896183\n",
      "Epoch 2702, Loss: 0.25815750658512115, Final Batch Loss: 0.07394526898860931\n",
      "Epoch 2703, Loss: 0.18781113997101784, Final Batch Loss: 0.04588582366704941\n",
      "Epoch 2704, Loss: 0.18209581822156906, Final Batch Loss: 0.033737991005182266\n",
      "Epoch 2705, Loss: 0.26640643179416656, Final Batch Loss: 0.04464232921600342\n",
      "Epoch 2706, Loss: 0.29675403982400894, Final Batch Loss: 0.04099549353122711\n",
      "Epoch 2707, Loss: 0.2563491612672806, Final Batch Loss: 0.09771542251110077\n",
      "Epoch 2708, Loss: 0.3339542858302593, Final Batch Loss: 0.09082601219415665\n",
      "Epoch 2709, Loss: 0.2782445661723614, Final Batch Loss: 0.08629214018583298\n",
      "Epoch 2710, Loss: 0.2983652092516422, Final Batch Loss: 0.09187524765729904\n",
      "Epoch 2711, Loss: 0.26021262630820274, Final Batch Loss: 0.03720281273126602\n",
      "Epoch 2712, Loss: 0.22762996703386307, Final Batch Loss: 0.03161148726940155\n",
      "Epoch 2713, Loss: 0.21727682650089264, Final Batch Loss: 0.06135278195142746\n",
      "Epoch 2714, Loss: 0.2908718213438988, Final Batch Loss: 0.051040343940258026\n",
      "Epoch 2715, Loss: 0.22937844321131706, Final Batch Loss: 0.08529079705476761\n",
      "Epoch 2716, Loss: 0.21459678187966347, Final Batch Loss: 0.06550028175115585\n",
      "Epoch 2717, Loss: 0.2932421453297138, Final Batch Loss: 0.1054828017950058\n",
      "Epoch 2718, Loss: 0.31787343323230743, Final Batch Loss: 0.13662703335285187\n",
      "Epoch 2719, Loss: 0.2843719944357872, Final Batch Loss: 0.05914132297039032\n",
      "Epoch 2720, Loss: 0.2130080945789814, Final Batch Loss: 0.04662320390343666\n",
      "Epoch 2721, Loss: 0.21534374356269836, Final Batch Loss: 0.06920845061540604\n",
      "Epoch 2722, Loss: 0.2164313979446888, Final Batch Loss: 0.047885458916425705\n",
      "Epoch 2723, Loss: 0.3946427907794714, Final Batch Loss: 0.20611369609832764\n",
      "Epoch 2724, Loss: 0.24906925484538078, Final Batch Loss: 0.05965687334537506\n",
      "Epoch 2725, Loss: 0.25238138996064663, Final Batch Loss: 0.017861386761069298\n",
      "Epoch 2726, Loss: 0.37822670489549637, Final Batch Loss: 0.07579666376113892\n",
      "Epoch 2727, Loss: 0.4893031995743513, Final Batch Loss: 0.28966307640075684\n",
      "Epoch 2728, Loss: 0.2389741688966751, Final Batch Loss: 0.025703057646751404\n",
      "Epoch 2729, Loss: 0.3052145764231682, Final Batch Loss: 0.09286166727542877\n",
      "Epoch 2730, Loss: 0.18809932470321655, Final Batch Loss: 0.06340522319078445\n",
      "Epoch 2731, Loss: 0.2649253197014332, Final Batch Loss: 0.09792907536029816\n",
      "Epoch 2732, Loss: 0.24299192056059837, Final Batch Loss: 0.03607431799173355\n",
      "Epoch 2733, Loss: 0.22101616859436035, Final Batch Loss: 0.0660686194896698\n",
      "Epoch 2734, Loss: 0.22569280862808228, Final Batch Loss: 0.04333801195025444\n",
      "Epoch 2735, Loss: 0.20617032051086426, Final Batch Loss: 0.057976119220256805\n",
      "Epoch 2736, Loss: 0.22302073054015636, Final Batch Loss: 0.0923430398106575\n",
      "Epoch 2737, Loss: 0.2023145742714405, Final Batch Loss: 0.031008228659629822\n",
      "Epoch 2738, Loss: 0.20691931061446667, Final Batch Loss: 0.030504008755087852\n",
      "Epoch 2739, Loss: 0.3945031613111496, Final Batch Loss: 0.15901541709899902\n",
      "Epoch 2740, Loss: 0.3493956234306097, Final Batch Loss: 0.21944670379161835\n",
      "Epoch 2741, Loss: 0.19249237049371004, Final Batch Loss: 0.013681997545063496\n",
      "Epoch 2742, Loss: 0.2248576320707798, Final Batch Loss: 0.05952168628573418\n",
      "Epoch 2743, Loss: 0.3234780579805374, Final Batch Loss: 0.08485747128725052\n",
      "Epoch 2744, Loss: 0.344594269990921, Final Batch Loss: 0.10814355313777924\n",
      "Epoch 2745, Loss: 0.29122597724199295, Final Batch Loss: 0.07673762738704681\n",
      "Epoch 2746, Loss: 0.2895072437822819, Final Batch Loss: 0.052585769444704056\n",
      "Epoch 2747, Loss: 0.3079054579138756, Final Batch Loss: 0.07936016470193863\n",
      "Epoch 2748, Loss: 0.25281740725040436, Final Batch Loss: 0.03583044186234474\n",
      "Epoch 2749, Loss: 0.29816336557269096, Final Batch Loss: 0.0812743678689003\n",
      "Epoch 2750, Loss: 0.26310209929943085, Final Batch Loss: 0.048292987048625946\n",
      "Epoch 2751, Loss: 0.26656270027160645, Final Batch Loss: 0.04315561056137085\n",
      "Epoch 2752, Loss: 0.2625303976237774, Final Batch Loss: 0.04367426410317421\n",
      "Epoch 2753, Loss: 0.25155271403491497, Final Batch Loss: 0.02771158330142498\n",
      "Epoch 2754, Loss: 0.23352186381816864, Final Batch Loss: 0.03345431387424469\n",
      "Epoch 2755, Loss: 0.2620437741279602, Final Batch Loss: 0.05842939391732216\n",
      "Epoch 2756, Loss: 0.21505974605679512, Final Batch Loss: 0.06486738473176956\n",
      "Epoch 2757, Loss: 0.242680873721838, Final Batch Loss: 0.06422477960586548\n",
      "Epoch 2758, Loss: 0.2733299769461155, Final Batch Loss: 0.10484734922647476\n",
      "Epoch 2759, Loss: 0.36599068343639374, Final Batch Loss: 0.13357782363891602\n",
      "Epoch 2760, Loss: 0.21593945287168026, Final Batch Loss: 0.030538571998476982\n",
      "Epoch 2761, Loss: 0.20160026475787163, Final Batch Loss: 0.024573449045419693\n",
      "Epoch 2762, Loss: 0.24028674885630608, Final Batch Loss: 0.05059405043721199\n",
      "Epoch 2763, Loss: 0.20985571667551994, Final Batch Loss: 0.044803835451602936\n",
      "Epoch 2764, Loss: 0.17659116350114346, Final Batch Loss: 0.024997273460030556\n",
      "Epoch 2765, Loss: 0.26968447118997574, Final Batch Loss: 0.07677394151687622\n",
      "Epoch 2766, Loss: 0.23942973092198372, Final Batch Loss: 0.047865841537714005\n",
      "Epoch 2767, Loss: 0.23676445707678795, Final Batch Loss: 0.046250682324171066\n",
      "Epoch 2768, Loss: 0.26491355523467064, Final Batch Loss: 0.1264692097902298\n",
      "Epoch 2769, Loss: 0.29586677998304367, Final Batch Loss: 0.09213457256555557\n",
      "Epoch 2770, Loss: 0.22944042086601257, Final Batch Loss: 0.1097969189286232\n",
      "Epoch 2771, Loss: 0.23907126113772392, Final Batch Loss: 0.08054927736520767\n",
      "Epoch 2772, Loss: 0.1673087254166603, Final Batch Loss: 0.025436703115701675\n",
      "Epoch 2773, Loss: 0.20290524885058403, Final Batch Loss: 0.04750041291117668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2774, Loss: 0.1751757189631462, Final Batch Loss: 0.03914296627044678\n",
      "Epoch 2775, Loss: 0.27872690185904503, Final Batch Loss: 0.046565715223550797\n",
      "Epoch 2776, Loss: 0.14383381884545088, Final Batch Loss: 0.015403210185468197\n",
      "Epoch 2777, Loss: 0.25450254417955875, Final Batch Loss: 0.03052581287920475\n",
      "Epoch 2778, Loss: 0.23098750039935112, Final Batch Loss: 0.048035431653261185\n",
      "Epoch 2779, Loss: 0.29100583493709564, Final Batch Loss: 0.06778223067522049\n",
      "Epoch 2780, Loss: 0.25155552104115486, Final Batch Loss: 0.09985395520925522\n",
      "Epoch 2781, Loss: 0.22885950654745102, Final Batch Loss: 0.04492481052875519\n",
      "Epoch 2782, Loss: 0.2787422128021717, Final Batch Loss: 0.042050547897815704\n",
      "Epoch 2783, Loss: 0.28230324015021324, Final Batch Loss: 0.050768740475177765\n",
      "Epoch 2784, Loss: 0.1903133587911725, Final Batch Loss: 0.012297355569899082\n",
      "Epoch 2785, Loss: 0.2520168796181679, Final Batch Loss: 0.0486689992249012\n",
      "Epoch 2786, Loss: 0.20684528723359108, Final Batch Loss: 0.02997894585132599\n",
      "Epoch 2787, Loss: 0.2574029676616192, Final Batch Loss: 0.06175380200147629\n",
      "Epoch 2788, Loss: 0.19391151145100594, Final Batch Loss: 0.061472535133361816\n",
      "Epoch 2789, Loss: 0.24415021762251854, Final Batch Loss: 0.097129225730896\n",
      "Epoch 2790, Loss: 0.25263987481594086, Final Batch Loss: 0.07337675988674164\n",
      "Epoch 2791, Loss: 0.2326443549245596, Final Batch Loss: 0.01869252882897854\n",
      "Epoch 2792, Loss: 0.40529483556747437, Final Batch Loss: 0.11827827244997025\n",
      "Epoch 2793, Loss: 0.37407539412379265, Final Batch Loss: 0.13064420223236084\n",
      "Epoch 2794, Loss: 0.3905606307089329, Final Batch Loss: 0.14193476736545563\n",
      "Epoch 2795, Loss: 0.394478440284729, Final Batch Loss: 0.13451945781707764\n",
      "Epoch 2796, Loss: 0.3563544750213623, Final Batch Loss: 0.10201650857925415\n",
      "Epoch 2797, Loss: 0.26355738192796707, Final Batch Loss: 0.05260089784860611\n",
      "Epoch 2798, Loss: 0.23913422226905823, Final Batch Loss: 0.038001712411642075\n",
      "Epoch 2799, Loss: 0.3442213647067547, Final Batch Loss: 0.1268952190876007\n",
      "Epoch 2800, Loss: 0.31653979048132896, Final Batch Loss: 0.09887650609016418\n",
      "Epoch 2801, Loss: 0.2702367976307869, Final Batch Loss: 0.07681409269571304\n",
      "Epoch 2802, Loss: 0.26048918440937996, Final Batch Loss: 0.041550200432538986\n",
      "Epoch 2803, Loss: 0.2370944395661354, Final Batch Loss: 0.04597046598792076\n",
      "Epoch 2804, Loss: 0.31326533667743206, Final Batch Loss: 0.02944180928170681\n",
      "Epoch 2805, Loss: 0.35364367067813873, Final Batch Loss: 0.09875738620758057\n",
      "Epoch 2806, Loss: 0.38631589338183403, Final Batch Loss: 0.21019643545150757\n",
      "Epoch 2807, Loss: 0.30468216724693775, Final Batch Loss: 0.10162732750177383\n",
      "Epoch 2808, Loss: 0.30127911269664764, Final Batch Loss: 0.05553185194730759\n",
      "Epoch 2809, Loss: 0.3223101571202278, Final Batch Loss: 0.12168695777654648\n",
      "Epoch 2810, Loss: 0.20983076095581055, Final Batch Loss: 0.039894621819257736\n",
      "Epoch 2811, Loss: 0.22228823229670525, Final Batch Loss: 0.0390738770365715\n",
      "Epoch 2812, Loss: 0.2930721864104271, Final Batch Loss: 0.0645594447851181\n",
      "Epoch 2813, Loss: 0.2897435314953327, Final Batch Loss: 0.03689161688089371\n",
      "Epoch 2814, Loss: 0.3523121848702431, Final Batch Loss: 0.13639986515045166\n",
      "Epoch 2815, Loss: 0.23468976095318794, Final Batch Loss: 0.04848066717386246\n",
      "Epoch 2816, Loss: 0.3369603082537651, Final Batch Loss: 0.16170036792755127\n",
      "Epoch 2817, Loss: 0.2956019453704357, Final Batch Loss: 0.09791015088558197\n",
      "Epoch 2818, Loss: 0.3037320226430893, Final Batch Loss: 0.08810777962207794\n",
      "Epoch 2819, Loss: 0.3415542021393776, Final Batch Loss: 0.11439042538404465\n",
      "Epoch 2820, Loss: 0.26401085779070854, Final Batch Loss: 0.03724215179681778\n",
      "Epoch 2821, Loss: 0.3295607678592205, Final Batch Loss: 0.1058865636587143\n",
      "Epoch 2822, Loss: 0.2605595402419567, Final Batch Loss: 0.04780526086688042\n",
      "Epoch 2823, Loss: 0.23609691858291626, Final Batch Loss: 0.04111213609576225\n",
      "Epoch 2824, Loss: 0.2575877532362938, Final Batch Loss: 0.08323592692613602\n",
      "Epoch 2825, Loss: 0.2464926317334175, Final Batch Loss: 0.08148260414600372\n",
      "Epoch 2826, Loss: 0.3093433119356632, Final Batch Loss: 0.07502012699842453\n",
      "Epoch 2827, Loss: 0.2513488158583641, Final Batch Loss: 0.05171818286180496\n",
      "Epoch 2828, Loss: 0.3024481125175953, Final Batch Loss: 0.10610461980104446\n",
      "Epoch 2829, Loss: 0.24887528270483017, Final Batch Loss: 0.044320762157440186\n",
      "Epoch 2830, Loss: 0.26142841950058937, Final Batch Loss: 0.1046941950917244\n",
      "Epoch 2831, Loss: 0.3870711736381054, Final Batch Loss: 0.12501005828380585\n",
      "Epoch 2832, Loss: 0.31261395290493965, Final Batch Loss: 0.08692659437656403\n",
      "Epoch 2833, Loss: 0.28722184151411057, Final Batch Loss: 0.08292582631111145\n",
      "Epoch 2834, Loss: 0.23173066973686218, Final Batch Loss: 0.023267842829227448\n",
      "Epoch 2835, Loss: 0.28755348920822144, Final Batch Loss: 0.05814749747514725\n",
      "Epoch 2836, Loss: 0.20211455412209034, Final Batch Loss: 0.022394156083464622\n",
      "Epoch 2837, Loss: 0.3193007558584213, Final Batch Loss: 0.04409899562597275\n",
      "Epoch 2838, Loss: 0.29086572118103504, Final Batch Loss: 0.0290811900049448\n",
      "Epoch 2839, Loss: 0.23689306527376175, Final Batch Loss: 0.04300256446003914\n",
      "Epoch 2840, Loss: 0.3263852968811989, Final Batch Loss: 0.11219405382871628\n",
      "Epoch 2841, Loss: 0.29382333159446716, Final Batch Loss: 0.058374084532260895\n",
      "Epoch 2842, Loss: 0.26755572482943535, Final Batch Loss: 0.05909089371562004\n",
      "Epoch 2843, Loss: 0.32966842874884605, Final Batch Loss: 0.07995354384183884\n",
      "Epoch 2844, Loss: 0.30311020463705063, Final Batch Loss: 0.04716665670275688\n",
      "Epoch 2845, Loss: 0.3172708824276924, Final Batch Loss: 0.16373136639595032\n",
      "Epoch 2846, Loss: 0.36802929639816284, Final Batch Loss: 0.16652584075927734\n",
      "Epoch 2847, Loss: 0.34628352150321007, Final Batch Loss: 0.12803228199481964\n",
      "Epoch 2848, Loss: 0.31324391812086105, Final Batch Loss: 0.09468499571084976\n",
      "Epoch 2849, Loss: 0.3774276450276375, Final Batch Loss: 0.09495622664690018\n",
      "Epoch 2850, Loss: 0.3893173336982727, Final Batch Loss: 0.16519059240818024\n",
      "Epoch 2851, Loss: 0.40622784197330475, Final Batch Loss: 0.027784109115600586\n",
      "Epoch 2852, Loss: 0.33001720160245895, Final Batch Loss: 0.16363412141799927\n",
      "Epoch 2853, Loss: 0.3588225841522217, Final Batch Loss: 0.09681247919797897\n",
      "Epoch 2854, Loss: 0.21807161532342434, Final Batch Loss: 0.022046031430363655\n",
      "Epoch 2855, Loss: 0.2995014637708664, Final Batch Loss: 0.044475629925727844\n",
      "Epoch 2856, Loss: 0.22290519252419472, Final Batch Loss: 0.08369847387075424\n",
      "Epoch 2857, Loss: 0.31061500683426857, Final Batch Loss: 0.0937754437327385\n",
      "Epoch 2858, Loss: 0.2699902132153511, Final Batch Loss: 0.07973818480968475\n",
      "Epoch 2859, Loss: 0.26701727136969566, Final Batch Loss: 0.0595184825360775\n",
      "Epoch 2860, Loss: 0.24518367275595665, Final Batch Loss: 0.04224701598286629\n",
      "Epoch 2861, Loss: 0.251580573618412, Final Batch Loss: 0.06446358561515808\n",
      "Epoch 2862, Loss: 0.2611018903553486, Final Batch Loss: 0.07349276542663574\n",
      "Epoch 2863, Loss: 0.23004435002803802, Final Batch Loss: 0.05147359520196915\n",
      "Epoch 2864, Loss: 0.23326610028743744, Final Batch Loss: 0.02108805626630783\n",
      "Epoch 2865, Loss: 0.30523937195539474, Final Batch Loss: 0.11649328470230103\n",
      "Epoch 2866, Loss: 0.2890259064733982, Final Batch Loss: 0.11726614087820053\n",
      "Epoch 2867, Loss: 0.1787090003490448, Final Batch Loss: 0.014249850064516068\n",
      "Epoch 2868, Loss: 0.3329670950770378, Final Batch Loss: 0.10408834367990494\n",
      "Epoch 2869, Loss: 0.26188888773322105, Final Batch Loss: 0.07469803839921951\n",
      "Epoch 2870, Loss: 0.3004249297082424, Final Batch Loss: 0.0767265185713768\n",
      "Epoch 2871, Loss: 0.24890467524528503, Final Batch Loss: 0.07009821385145187\n",
      "Epoch 2872, Loss: 0.22089705243706703, Final Batch Loss: 0.08944270014762878\n",
      "Epoch 2873, Loss: 0.30352718383073807, Final Batch Loss: 0.07282254099845886\n",
      "Epoch 2874, Loss: 0.25452210009098053, Final Batch Loss: 0.0649961307644844\n",
      "Epoch 2875, Loss: 0.2288004867732525, Final Batch Loss: 0.06657097488641739\n",
      "Epoch 2876, Loss: 0.25676798820495605, Final Batch Loss: 0.0590396486222744\n",
      "Epoch 2877, Loss: 0.2687564305961132, Final Batch Loss: 0.13326624035835266\n",
      "Epoch 2878, Loss: 0.2893048897385597, Final Batch Loss: 0.12333470582962036\n",
      "Epoch 2879, Loss: 0.4124663770198822, Final Batch Loss: 0.15445919334888458\n",
      "Epoch 2880, Loss: 0.3634899780154228, Final Batch Loss: 0.14346371591091156\n",
      "Epoch 2881, Loss: 0.39229584485292435, Final Batch Loss: 0.1522776186466217\n",
      "Epoch 2882, Loss: 0.32088207826018333, Final Batch Loss: 0.08878326416015625\n",
      "Epoch 2883, Loss: 0.2912442535161972, Final Batch Loss: 0.09224100410938263\n",
      "Epoch 2884, Loss: 0.3476444184780121, Final Batch Loss: 0.09234064817428589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2885, Loss: 0.33694976568222046, Final Batch Loss: 0.06661225110292435\n",
      "Epoch 2886, Loss: 0.23928776383399963, Final Batch Loss: 0.05094055086374283\n",
      "Epoch 2887, Loss: 0.1679791957139969, Final Batch Loss: 0.016898129135370255\n",
      "Epoch 2888, Loss: 0.16418521106243134, Final Batch Loss: 0.021827485412359238\n",
      "Epoch 2889, Loss: 0.19180142506957054, Final Batch Loss: 0.046206165105104446\n",
      "Epoch 2890, Loss: 0.1325147319585085, Final Batch Loss: 0.021266963332891464\n",
      "Epoch 2891, Loss: 0.18911042343825102, Final Batch Loss: 0.014837871305644512\n",
      "Epoch 2892, Loss: 0.2347390055656433, Final Batch Loss: 0.02996954321861267\n",
      "Epoch 2893, Loss: 0.19117623753845692, Final Batch Loss: 0.03461666777729988\n",
      "Epoch 2894, Loss: 0.243392170406878, Final Batch Loss: 0.014900085516273975\n",
      "Epoch 2895, Loss: 0.25443243980407715, Final Batch Loss: 0.05990014225244522\n",
      "Epoch 2896, Loss: 0.22471720352768898, Final Batch Loss: 0.042892567813396454\n",
      "Epoch 2897, Loss: 0.21486395224928856, Final Batch Loss: 0.043758463114500046\n",
      "Epoch 2898, Loss: 0.30164987221360207, Final Batch Loss: 0.08897458761930466\n",
      "Epoch 2899, Loss: 0.1980794370174408, Final Batch Loss: 0.02392636239528656\n",
      "Epoch 2900, Loss: 0.33241941779851913, Final Batch Loss: 0.13860554993152618\n",
      "Epoch 2901, Loss: 0.22111692279577255, Final Batch Loss: 0.03703003004193306\n",
      "Epoch 2902, Loss: 0.22187481075525284, Final Batch Loss: 0.023217327892780304\n",
      "Epoch 2903, Loss: 0.25793272629380226, Final Batch Loss: 0.07963291555643082\n",
      "Epoch 2904, Loss: 0.19904181361198425, Final Batch Loss: 0.04031359404325485\n",
      "Epoch 2905, Loss: 0.23144320398569107, Final Batch Loss: 0.052954353392124176\n",
      "Epoch 2906, Loss: 0.20350439101457596, Final Batch Loss: 0.045795172452926636\n",
      "Epoch 2907, Loss: 0.24657178670167923, Final Batch Loss: 0.028948679566383362\n",
      "Epoch 2908, Loss: 0.23523383028805256, Final Batch Loss: 0.06156827509403229\n",
      "Epoch 2909, Loss: 0.2088298313319683, Final Batch Loss: 0.047086842358112335\n",
      "Epoch 2910, Loss: 0.2545192018151283, Final Batch Loss: 0.09015515446662903\n",
      "Epoch 2911, Loss: 0.25028993003070354, Final Batch Loss: 0.08277657628059387\n",
      "Epoch 2912, Loss: 0.25821353495121, Final Batch Loss: 0.05957052856683731\n",
      "Epoch 2913, Loss: 0.23242370784282684, Final Batch Loss: 0.032680410891771317\n",
      "Epoch 2914, Loss: 0.2569655179977417, Final Batch Loss: 0.058124836534261703\n",
      "Epoch 2915, Loss: 0.18522002920508385, Final Batch Loss: 0.027835596352815628\n",
      "Epoch 2916, Loss: 0.24671870097517967, Final Batch Loss: 0.07858967036008835\n",
      "Epoch 2917, Loss: 0.17611884325742722, Final Batch Loss: 0.014593474566936493\n",
      "Epoch 2918, Loss: 0.23331816121935844, Final Batch Loss: 0.07602626085281372\n",
      "Epoch 2919, Loss: 0.27639881893992424, Final Batch Loss: 0.06135663762688637\n",
      "Epoch 2920, Loss: 0.18530340865254402, Final Batch Loss: 0.03989434987306595\n",
      "Epoch 2921, Loss: 0.18954784236848354, Final Batch Loss: 0.021495142951607704\n",
      "Epoch 2922, Loss: 0.23331578075885773, Final Batch Loss: 0.05572982877492905\n",
      "Epoch 2923, Loss: 0.19256608188152313, Final Batch Loss: 0.03763434290885925\n",
      "Epoch 2924, Loss: 0.2700841436162591, Final Batch Loss: 0.00824833381921053\n",
      "Epoch 2925, Loss: 0.43505560606718063, Final Batch Loss: 0.2005307823419571\n",
      "Epoch 2926, Loss: 0.25139063224196434, Final Batch Loss: 0.05265634134411812\n",
      "Epoch 2927, Loss: 0.33510710299015045, Final Batch Loss: 0.09915949404239655\n",
      "Epoch 2928, Loss: 0.3006879538297653, Final Batch Loss: 0.03075544908642769\n",
      "Epoch 2929, Loss: 0.3338662460446358, Final Batch Loss: 0.08155649155378342\n",
      "Epoch 2930, Loss: 0.29600031673908234, Final Batch Loss: 0.0488680861890316\n",
      "Epoch 2931, Loss: 0.29393257200717926, Final Batch Loss: 0.0697326585650444\n",
      "Epoch 2932, Loss: 0.21623008139431477, Final Batch Loss: 0.058348678052425385\n",
      "Epoch 2933, Loss: 0.4438939802348614, Final Batch Loss: 0.22169576585292816\n",
      "Epoch 2934, Loss: 0.341123316437006, Final Batch Loss: 0.15131379663944244\n",
      "Epoch 2935, Loss: 0.32961922138929367, Final Batch Loss: 0.0475652813911438\n",
      "Epoch 2936, Loss: 0.25374471116811037, Final Batch Loss: 0.015190397389233112\n",
      "Epoch 2937, Loss: 0.3588961064815521, Final Batch Loss: 0.05772879719734192\n",
      "Epoch 2938, Loss: 0.2546355128288269, Final Batch Loss: 0.08931092917919159\n",
      "Epoch 2939, Loss: 0.30839599668979645, Final Batch Loss: 0.10909043997526169\n",
      "Epoch 2940, Loss: 0.23403892293572426, Final Batch Loss: 0.05938274785876274\n",
      "Epoch 2941, Loss: 0.3033743388950825, Final Batch Loss: 0.05486909672617912\n",
      "Epoch 2942, Loss: 0.4061591774225235, Final Batch Loss: 0.1365722417831421\n",
      "Epoch 2943, Loss: 0.2455858625471592, Final Batch Loss: 0.03326720371842384\n",
      "Epoch 2944, Loss: 0.26593521796166897, Final Batch Loss: 0.017870059236884117\n",
      "Epoch 2945, Loss: 0.35087893158197403, Final Batch Loss: 0.07701724022626877\n",
      "Epoch 2946, Loss: 0.21057048812508583, Final Batch Loss: 0.03147631883621216\n",
      "Epoch 2947, Loss: 0.28231146186590195, Final Batch Loss: 0.0927693173289299\n",
      "Epoch 2948, Loss: 0.2536807283759117, Final Batch Loss: 0.03534853458404541\n",
      "Epoch 2949, Loss: 0.26207956299185753, Final Batch Loss: 0.0521010085940361\n",
      "Epoch 2950, Loss: 0.26199110597372055, Final Batch Loss: 0.06388730555772781\n",
      "Epoch 2951, Loss: 0.2550718244165182, Final Batch Loss: 0.029285868629813194\n",
      "Epoch 2952, Loss: 0.22218702547252178, Final Batch Loss: 0.029239660128951073\n",
      "Epoch 2953, Loss: 0.22589382156729698, Final Batch Loss: 0.062290020287036896\n",
      "Epoch 2954, Loss: 0.25861072540283203, Final Batch Loss: 0.056364841759204865\n",
      "Epoch 2955, Loss: 0.23667657747864723, Final Batch Loss: 0.06308380514383316\n",
      "Epoch 2956, Loss: 0.20364176854491234, Final Batch Loss: 0.031705353409051895\n",
      "Epoch 2957, Loss: 0.2823180705308914, Final Batch Loss: 0.11984299123287201\n",
      "Epoch 2958, Loss: 0.2721049040555954, Final Batch Loss: 0.050583288073539734\n",
      "Epoch 2959, Loss: 0.25783051550388336, Final Batch Loss: 0.07276226580142975\n",
      "Epoch 2960, Loss: 0.1814349815249443, Final Batch Loss: 0.017934434115886688\n",
      "Epoch 2961, Loss: 0.21394623443484306, Final Batch Loss: 0.04626160487532616\n",
      "Epoch 2962, Loss: 0.26356862112879753, Final Batch Loss: 0.05615564435720444\n",
      "Epoch 2963, Loss: 0.2155174668878317, Final Batch Loss: 0.016975725069642067\n",
      "Epoch 2964, Loss: 0.22422035411000252, Final Batch Loss: 0.03566877916455269\n",
      "Epoch 2965, Loss: 0.3034595623612404, Final Batch Loss: 0.07381574809551239\n",
      "Epoch 2966, Loss: 0.2126888632774353, Final Batch Loss: 0.07985377311706543\n",
      "Epoch 2967, Loss: 0.2666365234181285, Final Batch Loss: 0.0140211908146739\n",
      "Epoch 2968, Loss: 0.197067741304636, Final Batch Loss: 0.059163469821214676\n",
      "Epoch 2969, Loss: 0.17640405520796776, Final Batch Loss: 0.044602688401937485\n",
      "Epoch 2970, Loss: 0.22320912033319473, Final Batch Loss: 0.07976134866476059\n",
      "Epoch 2971, Loss: 0.40001460164785385, Final Batch Loss: 0.1938077062368393\n",
      "Epoch 2972, Loss: 0.2086963802576065, Final Batch Loss: 0.02214851975440979\n",
      "Epoch 2973, Loss: 0.20513945445418358, Final Batch Loss: 0.04376429319381714\n",
      "Epoch 2974, Loss: 0.214967155829072, Final Batch Loss: 0.028446024283766747\n",
      "Epoch 2975, Loss: 0.1838834583759308, Final Batch Loss: 0.050524089485406876\n",
      "Epoch 2976, Loss: 0.2414015531539917, Final Batch Loss: 0.026013262569904327\n",
      "Epoch 2977, Loss: 0.21327706426382065, Final Batch Loss: 0.03633120656013489\n",
      "Epoch 2978, Loss: 0.22182605229318142, Final Batch Loss: 0.04952967166900635\n",
      "Epoch 2979, Loss: 0.20733811985701323, Final Batch Loss: 0.015226672403514385\n",
      "Epoch 2980, Loss: 0.21903518214821815, Final Batch Loss: 0.0085684135556221\n",
      "Epoch 2981, Loss: 0.21265053004026413, Final Batch Loss: 0.08316411823034286\n",
      "Epoch 2982, Loss: 0.33070947974920273, Final Batch Loss: 0.10269324481487274\n",
      "Epoch 2983, Loss: 0.32626694068312645, Final Batch Loss: 0.09986501187086105\n",
      "Epoch 2984, Loss: 0.27812783047556877, Final Batch Loss: 0.03153328225016594\n",
      "Epoch 2985, Loss: 0.36614301800727844, Final Batch Loss: 0.07332952320575714\n",
      "Epoch 2986, Loss: 0.26153771206736565, Final Batch Loss: 0.07654339075088501\n",
      "Epoch 2987, Loss: 0.2933406550437212, Final Batch Loss: 0.020265815779566765\n",
      "Epoch 2988, Loss: 0.26447613537311554, Final Batch Loss: 0.0559203140437603\n",
      "Epoch 2989, Loss: 0.2996528670191765, Final Batch Loss: 0.09053225815296173\n",
      "Epoch 2990, Loss: 0.35781141743063927, Final Batch Loss: 0.12849462032318115\n",
      "Epoch 2991, Loss: 0.3507835082709789, Final Batch Loss: 0.1380709409713745\n",
      "Epoch 2992, Loss: 0.3966795280575752, Final Batch Loss: 0.12141748517751694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2993, Loss: 0.228891771286726, Final Batch Loss: 0.04160534590482712\n",
      "Epoch 2994, Loss: 0.28231170400977135, Final Batch Loss: 0.08622588962316513\n",
      "Epoch 2995, Loss: 0.3302830196917057, Final Batch Loss: 0.13651280105113983\n",
      "Epoch 2996, Loss: 0.43571436777710915, Final Batch Loss: 0.0497591458261013\n",
      "Epoch 2997, Loss: 0.343175545334816, Final Batch Loss: 0.0634835734963417\n",
      "Epoch 2998, Loss: 0.3003614693880081, Final Batch Loss: 0.0348658561706543\n",
      "Epoch 2999, Loss: 0.3224777430295944, Final Batch Loss: 0.10999486595392227\n",
      "Epoch 3000, Loss: 0.39169275015592575, Final Batch Loss: 0.05531446263194084\n",
      "Epoch 3001, Loss: 0.3728373982012272, Final Batch Loss: 0.09416236728429794\n",
      "Epoch 3002, Loss: 0.18913543596863747, Final Batch Loss: 0.017823707312345505\n",
      "Epoch 3003, Loss: 0.48322245851159096, Final Batch Loss: 0.09024515002965927\n",
      "Epoch 3004, Loss: 0.3792063966393471, Final Batch Loss: 0.1280423402786255\n",
      "Epoch 3005, Loss: 0.22989915683865547, Final Batch Loss: 0.06058826670050621\n",
      "Epoch 3006, Loss: 0.20933933556079865, Final Batch Loss: 0.0532483272254467\n",
      "Epoch 3007, Loss: 0.32723191007971764, Final Batch Loss: 0.09501046687364578\n",
      "Epoch 3008, Loss: 0.25736241787672043, Final Batch Loss: 0.05908076465129852\n",
      "Epoch 3009, Loss: 0.2585369013249874, Final Batch Loss: 0.04000792279839516\n",
      "Epoch 3010, Loss: 0.27895063906908035, Final Batch Loss: 0.05465169996023178\n",
      "Epoch 3011, Loss: 0.30250463634729385, Final Batch Loss: 0.0441351942718029\n",
      "Epoch 3012, Loss: 0.3015633746981621, Final Batch Loss: 0.06845361739397049\n",
      "Epoch 3013, Loss: 0.28212304413318634, Final Batch Loss: 0.09936052560806274\n",
      "Epoch 3014, Loss: 0.2830842286348343, Final Batch Loss: 0.11845389753580093\n",
      "Epoch 3015, Loss: 0.23051626980304718, Final Batch Loss: 0.021416600793600082\n",
      "Epoch 3016, Loss: 0.20281052216887474, Final Batch Loss: 0.033022426068782806\n",
      "Epoch 3017, Loss: 0.23947451636195183, Final Batch Loss: 0.08047062903642654\n",
      "Epoch 3018, Loss: 0.2567976452410221, Final Batch Loss: 0.07397162169218063\n",
      "Epoch 3019, Loss: 0.3030751831829548, Final Batch Loss: 0.07172214984893799\n",
      "Epoch 3020, Loss: 0.25045887753367424, Final Batch Loss: 0.042889147996902466\n",
      "Epoch 3021, Loss: 0.29873427748680115, Final Batch Loss: 0.03441368043422699\n",
      "Epoch 3022, Loss: 0.23509598895907402, Final Batch Loss: 0.0853818878531456\n",
      "Epoch 3023, Loss: 0.18372545391321182, Final Batch Loss: 0.03629067912697792\n",
      "Epoch 3024, Loss: 0.2810349855571985, Final Batch Loss: 0.14929980039596558\n",
      "Epoch 3025, Loss: 0.2858680561184883, Final Batch Loss: 0.12618951499462128\n",
      "Epoch 3026, Loss: 0.2355038784444332, Final Batch Loss: 0.05421244353055954\n",
      "Epoch 3027, Loss: 0.2696104384958744, Final Batch Loss: 0.10689274221658707\n",
      "Epoch 3028, Loss: 0.2422817498445511, Final Batch Loss: 0.04152399301528931\n",
      "Epoch 3029, Loss: 0.2780394069850445, Final Batch Loss: 0.08001194894313812\n",
      "Epoch 3030, Loss: 0.2637989018112421, Final Batch Loss: 0.030885109677910805\n",
      "Epoch 3031, Loss: 0.27789929509162903, Final Batch Loss: 0.0441003143787384\n",
      "Epoch 3032, Loss: 0.2776441238820553, Final Batch Loss: 0.06965837627649307\n",
      "Epoch 3033, Loss: 0.2746715806424618, Final Batch Loss: 0.06395360082387924\n",
      "Epoch 3034, Loss: 0.21527637913823128, Final Batch Loss: 0.062967948615551\n",
      "Epoch 3035, Loss: 0.21553025022149086, Final Batch Loss: 0.0289117731153965\n",
      "Epoch 3036, Loss: 0.2655405253171921, Final Batch Loss: 0.07254106551408768\n",
      "Epoch 3037, Loss: 0.24657917395234108, Final Batch Loss: 0.06483522057533264\n",
      "Epoch 3038, Loss: 0.23482906073331833, Final Batch Loss: 0.06459466367959976\n",
      "Epoch 3039, Loss: 0.18969141878187656, Final Batch Loss: 0.027112754061818123\n",
      "Epoch 3040, Loss: 0.2834160178899765, Final Batch Loss: 0.10340336710214615\n",
      "Epoch 3041, Loss: 0.197842076420784, Final Batch Loss: 0.02927616611123085\n",
      "Epoch 3042, Loss: 0.1679515652358532, Final Batch Loss: 0.050558995455503464\n",
      "Epoch 3043, Loss: 0.3139231987297535, Final Batch Loss: 0.05991386994719505\n",
      "Epoch 3044, Loss: 0.332729272544384, Final Batch Loss: 0.07934149354696274\n",
      "Epoch 3045, Loss: 0.2524953745305538, Final Batch Loss: 0.07910973578691483\n",
      "Epoch 3046, Loss: 0.29877112060785294, Final Batch Loss: 0.031061537563800812\n",
      "Epoch 3047, Loss: 0.3996667340397835, Final Batch Loss: 0.21345411241054535\n",
      "Epoch 3048, Loss: 0.3926965147256851, Final Batch Loss: 0.09956332296133041\n",
      "Epoch 3049, Loss: 0.2762100100517273, Final Batch Loss: 0.06820564717054367\n",
      "Epoch 3050, Loss: 0.2987695224583149, Final Batch Loss: 0.09037229418754578\n",
      "Epoch 3051, Loss: 0.4024771489202976, Final Batch Loss: 0.1488412767648697\n",
      "Epoch 3052, Loss: 0.2758234404027462, Final Batch Loss: 0.03520674258470535\n",
      "Epoch 3053, Loss: 0.23569529131054878, Final Batch Loss: 0.02426021173596382\n",
      "Epoch 3054, Loss: 0.19233446568250656, Final Batch Loss: 0.05246907100081444\n",
      "Epoch 3055, Loss: 0.18800406157970428, Final Batch Loss: 0.03497510775923729\n",
      "Epoch 3056, Loss: 0.2807697467505932, Final Batch Loss: 0.11530455946922302\n",
      "Epoch 3057, Loss: 0.2276436910033226, Final Batch Loss: 0.01872369647026062\n",
      "Epoch 3058, Loss: 0.21895543858408928, Final Batch Loss: 0.05218806117773056\n",
      "Epoch 3059, Loss: 0.2805435098707676, Final Batch Loss: 0.07094171643257141\n",
      "Epoch 3060, Loss: 0.23383630439639091, Final Batch Loss: 0.08906364440917969\n",
      "Epoch 3061, Loss: 0.31582265719771385, Final Batch Loss: 0.11393129825592041\n",
      "Epoch 3062, Loss: 0.2793513536453247, Final Batch Loss: 0.0649874210357666\n",
      "Epoch 3063, Loss: 0.23770543932914734, Final Batch Loss: 0.05311639979481697\n",
      "Epoch 3064, Loss: 0.21344252303242683, Final Batch Loss: 0.07114280760288239\n",
      "Epoch 3065, Loss: 0.1851980835199356, Final Batch Loss: 0.015777193009853363\n",
      "Epoch 3066, Loss: 0.2791868560016155, Final Batch Loss: 0.09864435344934464\n",
      "Epoch 3067, Loss: 0.17047359142452478, Final Batch Loss: 0.01136078778654337\n",
      "Epoch 3068, Loss: 0.32381849363446236, Final Batch Loss: 0.1462402194738388\n",
      "Epoch 3069, Loss: 0.2844364456832409, Final Batch Loss: 0.12760215997695923\n",
      "Epoch 3070, Loss: 0.1986053129658103, Final Batch Loss: 0.015566005371510983\n",
      "Epoch 3071, Loss: 0.23098313063383102, Final Batch Loss: 0.07458379864692688\n",
      "Epoch 3072, Loss: 0.2574016787111759, Final Batch Loss: 0.08383148163557053\n",
      "Epoch 3073, Loss: 0.2118518278002739, Final Batch Loss: 0.06426633149385452\n",
      "Epoch 3074, Loss: 0.2878929525613785, Final Batch Loss: 0.10819247364997864\n",
      "Epoch 3075, Loss: 0.3461284264922142, Final Batch Loss: 0.0986354723572731\n",
      "Epoch 3076, Loss: 0.24766933917999268, Final Batch Loss: 0.021927475929260254\n",
      "Epoch 3077, Loss: 0.400502972304821, Final Batch Loss: 0.1688729226589203\n",
      "Epoch 3078, Loss: 0.369168508797884, Final Batch Loss: 0.1265988051891327\n",
      "Epoch 3079, Loss: 0.30750297009944916, Final Batch Loss: 0.08136722445487976\n",
      "Epoch 3080, Loss: 0.26144250482320786, Final Batch Loss: 0.05575905367732048\n",
      "Epoch 3081, Loss: 0.26680154725909233, Final Batch Loss: 0.046316344290971756\n",
      "Epoch 3082, Loss: 0.17061490565538406, Final Batch Loss: 0.01903947815299034\n",
      "Epoch 3083, Loss: 0.2433999665081501, Final Batch Loss: 0.033153221011161804\n",
      "Epoch 3084, Loss: 0.18685301765799522, Final Batch Loss: 0.042071036994457245\n",
      "Epoch 3085, Loss: 0.2588387206196785, Final Batch Loss: 0.04438085854053497\n",
      "Epoch 3086, Loss: 0.19000648148357868, Final Batch Loss: 0.03895187005400658\n",
      "Epoch 3087, Loss: 0.20538225769996643, Final Batch Loss: 0.06825551390647888\n",
      "Epoch 3088, Loss: 0.2308138832449913, Final Batch Loss: 0.0434211902320385\n",
      "Epoch 3089, Loss: 0.26109572499990463, Final Batch Loss: 0.05308028683066368\n",
      "Epoch 3090, Loss: 0.24937807396054268, Final Batch Loss: 0.07584851235151291\n",
      "Epoch 3091, Loss: 0.18458697572350502, Final Batch Loss: 0.03600621595978737\n",
      "Epoch 3092, Loss: 0.3493187874555588, Final Batch Loss: 0.1402568221092224\n",
      "Epoch 3093, Loss: 0.1942860148847103, Final Batch Loss: 0.04069583863019943\n",
      "Epoch 3094, Loss: 0.22759350948035717, Final Batch Loss: 0.024364786222577095\n",
      "Epoch 3095, Loss: 0.21281851455569267, Final Batch Loss: 0.0359308235347271\n",
      "Epoch 3096, Loss: 0.19145895168185234, Final Batch Loss: 0.05279606580734253\n",
      "Epoch 3097, Loss: 0.3783114328980446, Final Batch Loss: 0.0973263829946518\n",
      "Epoch 3098, Loss: 0.19516880810260773, Final Batch Loss: 0.021437976509332657\n",
      "Epoch 3099, Loss: 0.24570393934845924, Final Batch Loss: 0.06293607503175735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3100, Loss: 0.24903207644820213, Final Batch Loss: 0.07054218649864197\n",
      "Epoch 3101, Loss: 0.295366358011961, Final Batch Loss: 0.11188793927431107\n",
      "Epoch 3102, Loss: 0.2145838886499405, Final Batch Loss: 0.02164984866976738\n",
      "Epoch 3103, Loss: 0.1587724033743143, Final Batch Loss: 0.020503858104348183\n",
      "Epoch 3104, Loss: 0.19785203784704208, Final Batch Loss: 0.021442484110593796\n",
      "Epoch 3105, Loss: 0.1973792463541031, Final Batch Loss: 0.06488973647356033\n",
      "Epoch 3106, Loss: 0.4072338603436947, Final Batch Loss: 0.20181125402450562\n",
      "Epoch 3107, Loss: 0.2360960729420185, Final Batch Loss: 0.046930067241191864\n",
      "Epoch 3108, Loss: 0.2379964329302311, Final Batch Loss: 0.06644083559513092\n",
      "Epoch 3109, Loss: 0.26150599494576454, Final Batch Loss: 0.11448046565055847\n",
      "Epoch 3110, Loss: 0.18588145077228546, Final Batch Loss: 0.050719261169433594\n",
      "Epoch 3111, Loss: 0.2467060536146164, Final Batch Loss: 0.053115807473659515\n",
      "Epoch 3112, Loss: 0.282710000872612, Final Batch Loss: 0.06250070035457611\n",
      "Epoch 3113, Loss: 0.3068402037024498, Final Batch Loss: 0.041175201535224915\n",
      "Epoch 3114, Loss: 0.2466629557311535, Final Batch Loss: 0.07456063479185104\n",
      "Epoch 3115, Loss: 0.3216460384428501, Final Batch Loss: 0.06543157249689102\n",
      "Epoch 3116, Loss: 0.27473077923059464, Final Batch Loss: 0.05086846277117729\n",
      "Epoch 3117, Loss: 0.23982449993491173, Final Batch Loss: 0.047803569585084915\n",
      "Epoch 3118, Loss: 0.28099510446190834, Final Batch Loss: 0.03623892739415169\n",
      "Epoch 3119, Loss: 0.3238379918038845, Final Batch Loss: 0.08353785425424576\n",
      "Epoch 3120, Loss: 0.2849428318440914, Final Batch Loss: 0.06748982518911362\n",
      "Epoch 3121, Loss: 0.1906975470483303, Final Batch Loss: 0.020941048860549927\n",
      "Epoch 3122, Loss: 0.29899464547634125, Final Batch Loss: 0.056549716740846634\n",
      "Epoch 3123, Loss: 0.27872588858008385, Final Batch Loss: 0.07950953394174576\n",
      "Epoch 3124, Loss: 0.188005356118083, Final Batch Loss: 0.04125484451651573\n",
      "Epoch 3125, Loss: 0.2040609046816826, Final Batch Loss: 0.021457921713590622\n",
      "Epoch 3126, Loss: 0.3589887470006943, Final Batch Loss: 0.09451338648796082\n",
      "Epoch 3127, Loss: 0.16635212115943432, Final Batch Loss: 0.03568299859762192\n",
      "Epoch 3128, Loss: 0.24074498377740383, Final Batch Loss: 0.02243087626993656\n",
      "Epoch 3129, Loss: 0.18270713090896606, Final Batch Loss: 0.034151189029216766\n",
      "Epoch 3130, Loss: 0.23946285992860794, Final Batch Loss: 0.04586096480488777\n",
      "Epoch 3131, Loss: 0.2040819488465786, Final Batch Loss: 0.06227055564522743\n",
      "Epoch 3132, Loss: 0.2239028662443161, Final Batch Loss: 0.032590601593256\n",
      "Epoch 3133, Loss: 0.18036797270178795, Final Batch Loss: 0.037524640560150146\n",
      "Epoch 3134, Loss: 0.20101000182330608, Final Batch Loss: 0.02183784358203411\n",
      "Epoch 3135, Loss: 0.28385092690587044, Final Batch Loss: 0.01899215206503868\n",
      "Epoch 3136, Loss: 0.28363868966698647, Final Batch Loss: 0.047385670244693756\n",
      "Epoch 3137, Loss: 0.301389392465353, Final Batch Loss: 0.10589120537042618\n",
      "Epoch 3138, Loss: 0.29064882546663284, Final Batch Loss: 0.10692988336086273\n",
      "Epoch 3139, Loss: 0.17378802970051765, Final Batch Loss: 0.01784859597682953\n",
      "Epoch 3140, Loss: 0.24654627591371536, Final Batch Loss: 0.05535658076405525\n",
      "Epoch 3141, Loss: 0.25256871059536934, Final Batch Loss: 0.06940436363220215\n",
      "Epoch 3142, Loss: 0.15270211920142174, Final Batch Loss: 0.05517695099115372\n",
      "Epoch 3143, Loss: 0.2093416079878807, Final Batch Loss: 0.03344646468758583\n",
      "Epoch 3144, Loss: 0.2494777850806713, Final Batch Loss: 0.11210189014673233\n",
      "Epoch 3145, Loss: 0.20907344669103622, Final Batch Loss: 0.039879363030195236\n",
      "Epoch 3146, Loss: 0.22562422044575214, Final Batch Loss: 0.025372931733727455\n",
      "Epoch 3147, Loss: 0.34486206248402596, Final Batch Loss: 0.1209021583199501\n",
      "Epoch 3148, Loss: 0.18551643006503582, Final Batch Loss: 0.05155627429485321\n",
      "Epoch 3149, Loss: 0.3419312499463558, Final Batch Loss: 0.13714973628520966\n",
      "Epoch 3150, Loss: 0.18043972924351692, Final Batch Loss: 0.042203374207019806\n",
      "Epoch 3151, Loss: 0.31999244540929794, Final Batch Loss: 0.12154926359653473\n",
      "Epoch 3152, Loss: 0.2261168472468853, Final Batch Loss: 0.027863960713148117\n",
      "Epoch 3153, Loss: 0.2880200482904911, Final Batch Loss: 0.05764836072921753\n",
      "Epoch 3154, Loss: 0.23496852815151215, Final Batch Loss: 0.07552634924650192\n",
      "Epoch 3155, Loss: 0.2863683607429266, Final Batch Loss: 0.16846704483032227\n",
      "Epoch 3156, Loss: 0.21603718400001526, Final Batch Loss: 0.042117390781641006\n",
      "Epoch 3157, Loss: 0.28588552214205265, Final Batch Loss: 0.07665691524744034\n",
      "Epoch 3158, Loss: 0.24545183219015598, Final Batch Loss: 0.05671660974621773\n",
      "Epoch 3159, Loss: 0.2991728447377682, Final Batch Loss: 0.07472989708185196\n",
      "Epoch 3160, Loss: 0.27050141245126724, Final Batch Loss: 0.11294129490852356\n",
      "Epoch 3161, Loss: 0.29295483231544495, Final Batch Loss: 0.05368509888648987\n",
      "Epoch 3162, Loss: 0.238004419952631, Final Batch Loss: 0.03930980712175369\n",
      "Epoch 3163, Loss: 0.28373849391937256, Final Batch Loss: 0.04828202724456787\n",
      "Epoch 3164, Loss: 0.2980513349175453, Final Batch Loss: 0.051486581563949585\n",
      "Epoch 3165, Loss: 0.2405003271996975, Final Batch Loss: 0.07165422290563583\n",
      "Epoch 3166, Loss: 0.2300975415855646, Final Batch Loss: 0.07642463594675064\n",
      "Epoch 3167, Loss: 0.24786963872611523, Final Batch Loss: 0.0605265349149704\n",
      "Epoch 3168, Loss: 0.20460054650902748, Final Batch Loss: 0.05322635546326637\n",
      "Epoch 3169, Loss: 0.2483844980597496, Final Batch Loss: 0.08381018042564392\n",
      "Epoch 3170, Loss: 0.18276342377066612, Final Batch Loss: 0.024131357669830322\n",
      "Epoch 3171, Loss: 0.277442567050457, Final Batch Loss: 0.06334609538316727\n",
      "Epoch 3172, Loss: 0.2717542052268982, Final Batch Loss: 0.08706708252429962\n",
      "Epoch 3173, Loss: 0.1774084735661745, Final Batch Loss: 0.024674009531736374\n",
      "Epoch 3174, Loss: 0.21306178346276283, Final Batch Loss: 0.07686769217252731\n",
      "Epoch 3175, Loss: 0.2714173160493374, Final Batch Loss: 0.1054835170507431\n",
      "Epoch 3176, Loss: 0.23286598548293114, Final Batch Loss: 0.05851851403713226\n",
      "Epoch 3177, Loss: 0.238468948751688, Final Batch Loss: 0.06601385772228241\n",
      "Epoch 3178, Loss: 0.26263517886400223, Final Batch Loss: 0.054413843899965286\n",
      "Epoch 3179, Loss: 0.2579270303249359, Final Batch Loss: 0.03493814542889595\n",
      "Epoch 3180, Loss: 0.2734714671969414, Final Batch Loss: 0.08570042252540588\n",
      "Epoch 3181, Loss: 0.2981967218220234, Final Batch Loss: 0.06571260094642639\n",
      "Epoch 3182, Loss: 0.23795240744948387, Final Batch Loss: 0.014666419476270676\n",
      "Epoch 3183, Loss: 0.21423961035907269, Final Batch Loss: 0.07377341389656067\n",
      "Epoch 3184, Loss: 0.20152224972844124, Final Batch Loss: 0.058009788393974304\n",
      "Epoch 3185, Loss: 0.14630900882184505, Final Batch Loss: 0.020094605162739754\n",
      "Epoch 3186, Loss: 0.233377855271101, Final Batch Loss: 0.05628364533185959\n",
      "Epoch 3187, Loss: 0.21583432331681252, Final Batch Loss: 0.04341863468289375\n",
      "Epoch 3188, Loss: 0.2550944648683071, Final Batch Loss: 0.05316014960408211\n",
      "Epoch 3189, Loss: 0.27871890366077423, Final Batch Loss: 0.11055874079465866\n",
      "Epoch 3190, Loss: 0.20601364225149155, Final Batch Loss: 0.048406198620796204\n",
      "Epoch 3191, Loss: 0.22826766595244408, Final Batch Loss: 0.043626364320516586\n",
      "Epoch 3192, Loss: 0.22424918413162231, Final Batch Loss: 0.03255564719438553\n",
      "Epoch 3193, Loss: 0.23674190416932106, Final Batch Loss: 0.09013763815164566\n",
      "Epoch 3194, Loss: 0.22994002886116505, Final Batch Loss: 0.059511080384254456\n",
      "Epoch 3195, Loss: 0.2098483219742775, Final Batch Loss: 0.07965082675218582\n",
      "Epoch 3196, Loss: 0.24669455736875534, Final Batch Loss: 0.06604621559381485\n",
      "Epoch 3197, Loss: 0.2635364904999733, Final Batch Loss: 0.1079489216208458\n",
      "Epoch 3198, Loss: 0.2514571323990822, Final Batch Loss: 0.04350799694657326\n",
      "Epoch 3199, Loss: 0.20875779911875725, Final Batch Loss: 0.038635727018117905\n",
      "Epoch 3200, Loss: 0.2955002747476101, Final Batch Loss: 0.02030126005411148\n",
      "Epoch 3201, Loss: 0.2039392627775669, Final Batch Loss: 0.05110424757003784\n",
      "Epoch 3202, Loss: 0.3616894707083702, Final Batch Loss: 0.0814608559012413\n",
      "Epoch 3203, Loss: 0.26433779671788216, Final Batch Loss: 0.05934365838766098\n",
      "Epoch 3204, Loss: 0.16578943189233541, Final Batch Loss: 0.008259917609393597\n",
      "Epoch 3205, Loss: 0.2141244038939476, Final Batch Loss: 0.04786065220832825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3206, Loss: 0.2636476829648018, Final Batch Loss: 0.08326833695173264\n",
      "Epoch 3207, Loss: 0.275967538356781, Final Batch Loss: 0.10989991575479507\n",
      "Epoch 3208, Loss: 0.20080945640802383, Final Batch Loss: 0.08539499342441559\n",
      "Epoch 3209, Loss: 0.24432243034243584, Final Batch Loss: 0.04967726022005081\n",
      "Epoch 3210, Loss: 0.24151025898754597, Final Batch Loss: 0.057756900787353516\n",
      "Epoch 3211, Loss: 0.2440197616815567, Final Batch Loss: 0.03525374457240105\n",
      "Epoch 3212, Loss: 0.32891442626714706, Final Batch Loss: 0.05525851994752884\n",
      "Epoch 3213, Loss: 0.3374977149069309, Final Batch Loss: 0.04192351922392845\n",
      "Epoch 3214, Loss: 0.29149657115340233, Final Batch Loss: 0.07763452082872391\n",
      "Epoch 3215, Loss: 0.21257886104285717, Final Batch Loss: 0.02235148288309574\n",
      "Epoch 3216, Loss: 0.2860022597014904, Final Batch Loss: 0.14989854395389557\n",
      "Epoch 3217, Loss: 0.2357147503644228, Final Batch Loss: 0.06575692445039749\n",
      "Epoch 3218, Loss: 0.2973350062966347, Final Batch Loss: 0.08948736637830734\n",
      "Epoch 3219, Loss: 0.4031692110002041, Final Batch Loss: 0.12413256615400314\n",
      "Epoch 3220, Loss: 0.33036448433995247, Final Batch Loss: 0.14010010659694672\n",
      "Epoch 3221, Loss: 0.26775911822915077, Final Batch Loss: 0.05318789929151535\n",
      "Epoch 3222, Loss: 0.22597350738942623, Final Batch Loss: 0.11539941281080246\n",
      "Epoch 3223, Loss: 0.21923262253403664, Final Batch Loss: 0.04320384934544563\n",
      "Epoch 3224, Loss: 0.26412925496697426, Final Batch Loss: 0.07602860033512115\n",
      "Epoch 3225, Loss: 0.32772575318813324, Final Batch Loss: 0.08627942949533463\n",
      "Epoch 3226, Loss: 0.20617720857262611, Final Batch Loss: 0.035852354019880295\n",
      "Epoch 3227, Loss: 0.2242606319487095, Final Batch Loss: 0.06773338466882706\n",
      "Epoch 3228, Loss: 0.28102096915245056, Final Batch Loss: 0.07757218182086945\n",
      "Epoch 3229, Loss: 0.3563089445233345, Final Batch Loss: 0.13475331664085388\n",
      "Epoch 3230, Loss: 0.22410330548882484, Final Batch Loss: 0.04328829050064087\n",
      "Epoch 3231, Loss: 0.2128804437816143, Final Batch Loss: 0.05473366007208824\n",
      "Epoch 3232, Loss: 0.21592425554990768, Final Batch Loss: 0.017917703837156296\n",
      "Epoch 3233, Loss: 0.18724429979920387, Final Batch Loss: 0.02092748135328293\n",
      "Epoch 3234, Loss: 0.3115212991833687, Final Batch Loss: 0.10111059248447418\n",
      "Epoch 3235, Loss: 0.262382160872221, Final Batch Loss: 0.04661119356751442\n",
      "Epoch 3236, Loss: 0.2259926274418831, Final Batch Loss: 0.027139708399772644\n",
      "Epoch 3237, Loss: 0.22522803582251072, Final Batch Loss: 0.02034039981663227\n",
      "Epoch 3238, Loss: 0.18960518017411232, Final Batch Loss: 0.023866169154644012\n",
      "Epoch 3239, Loss: 0.19616109877824783, Final Batch Loss: 0.0412675216794014\n",
      "Epoch 3240, Loss: 0.24933113902807236, Final Batch Loss: 0.0476883128285408\n",
      "Epoch 3241, Loss: 0.3000561073422432, Final Batch Loss: 0.12909650802612305\n",
      "Epoch 3242, Loss: 0.1885908991098404, Final Batch Loss: 0.04382806643843651\n",
      "Epoch 3243, Loss: 0.28892289847135544, Final Batch Loss: 0.08236557990312576\n",
      "Epoch 3244, Loss: 0.30388185381889343, Final Batch Loss: 0.08546020090579987\n",
      "Epoch 3245, Loss: 0.28254484385252, Final Batch Loss: 0.0758863240480423\n",
      "Epoch 3246, Loss: 0.20934733375906944, Final Batch Loss: 0.02905183658003807\n",
      "Epoch 3247, Loss: 0.2535436451435089, Final Batch Loss: 0.03940248489379883\n",
      "Epoch 3248, Loss: 0.2076357901096344, Final Batch Loss: 0.04279383644461632\n",
      "Epoch 3249, Loss: 0.2247179076075554, Final Batch Loss: 0.06817552447319031\n",
      "Epoch 3250, Loss: 0.17677728459239006, Final Batch Loss: 0.031425923109054565\n",
      "Epoch 3251, Loss: 0.2123722806572914, Final Batch Loss: 0.04596484452486038\n",
      "Epoch 3252, Loss: 0.22942857816815376, Final Batch Loss: 0.033296190202236176\n",
      "Epoch 3253, Loss: 0.2525125462561846, Final Batch Loss: 0.06044294685125351\n",
      "Epoch 3254, Loss: 0.26650478318333626, Final Batch Loss: 0.10560110211372375\n",
      "Epoch 3255, Loss: 0.5122099667787552, Final Batch Loss: 0.27455776929855347\n",
      "Epoch 3256, Loss: 0.3168638125061989, Final Batch Loss: 0.03535502031445503\n",
      "Epoch 3257, Loss: 0.34011516720056534, Final Batch Loss: 0.09579236060380936\n",
      "Epoch 3258, Loss: 0.4210928939282894, Final Batch Loss: 0.12988430261611938\n",
      "Epoch 3259, Loss: 0.257454477250576, Final Batch Loss: 0.085907943546772\n",
      "Epoch 3260, Loss: 0.26685886457562447, Final Batch Loss: 0.03337683901190758\n",
      "Epoch 3261, Loss: 0.3273051343858242, Final Batch Loss: 0.050269823521375656\n",
      "Epoch 3262, Loss: 0.36768631637096405, Final Batch Loss: 0.11124469339847565\n",
      "Epoch 3263, Loss: 0.268915805965662, Final Batch Loss: 0.042419642210006714\n",
      "Epoch 3264, Loss: 0.26658549904823303, Final Batch Loss: 0.044987212866544724\n",
      "Epoch 3265, Loss: 0.3976617567241192, Final Batch Loss: 0.12691070139408112\n",
      "Epoch 3266, Loss: 0.38909125328063965, Final Batch Loss: 0.09989474713802338\n",
      "Epoch 3267, Loss: 0.3846701718866825, Final Batch Loss: 0.053463276475667953\n",
      "Epoch 3268, Loss: 0.37962959706783295, Final Batch Loss: 0.13149118423461914\n",
      "Epoch 3269, Loss: 0.47124823182821274, Final Batch Loss: 0.20734210312366486\n",
      "Epoch 3270, Loss: 0.3866598457098007, Final Batch Loss: 0.04843740165233612\n",
      "Epoch 3271, Loss: 0.3984493389725685, Final Batch Loss: 0.14368288218975067\n",
      "Epoch 3272, Loss: 0.3702987805008888, Final Batch Loss: 0.09858512878417969\n",
      "Epoch 3273, Loss: 0.37668679282069206, Final Batch Loss: 0.061373863369226456\n",
      "Epoch 3274, Loss: 0.2944844290614128, Final Batch Loss: 0.08046141266822815\n",
      "Epoch 3275, Loss: 0.30135801807045937, Final Batch Loss: 0.09951163828372955\n",
      "Epoch 3276, Loss: 0.27765675634145737, Final Batch Loss: 0.054923005402088165\n",
      "Epoch 3277, Loss: 0.2512288838624954, Final Batch Loss: 0.058547262102365494\n",
      "Epoch 3278, Loss: 0.25222873874008656, Final Batch Loss: 0.030854055657982826\n",
      "Epoch 3279, Loss: 0.25560032948851585, Final Batch Loss: 0.05588848516345024\n",
      "Epoch 3280, Loss: 0.317757535725832, Final Batch Loss: 0.11703338474035263\n",
      "Epoch 3281, Loss: 0.25033850595355034, Final Batch Loss: 0.024513278156518936\n",
      "Epoch 3282, Loss: 0.3623781353235245, Final Batch Loss: 0.17809173464775085\n",
      "Epoch 3283, Loss: 0.25671274960041046, Final Batch Loss: 0.031900156289339066\n",
      "Epoch 3284, Loss: 0.28294095769524574, Final Batch Loss: 0.025061193853616714\n",
      "Epoch 3285, Loss: 0.20535873249173164, Final Batch Loss: 0.05961257964372635\n",
      "Epoch 3286, Loss: 0.33701827749609947, Final Batch Loss: 0.1494673490524292\n",
      "Epoch 3287, Loss: 0.31423621252179146, Final Batch Loss: 0.06434816867113113\n",
      "Epoch 3288, Loss: 0.20899822190403938, Final Batch Loss: 0.022189192473888397\n",
      "Epoch 3289, Loss: 0.19730150885879993, Final Batch Loss: 0.07642723619937897\n",
      "Epoch 3290, Loss: 0.20301801338791847, Final Batch Loss: 0.05798240751028061\n",
      "Epoch 3291, Loss: 0.21873312816023827, Final Batch Loss: 0.04578002914786339\n",
      "Epoch 3292, Loss: 0.2921944446861744, Final Batch Loss: 0.07758915424346924\n",
      "Epoch 3293, Loss: 0.22149907425045967, Final Batch Loss: 0.05212999880313873\n",
      "Epoch 3294, Loss: 0.2431502565741539, Final Batch Loss: 0.056812480092048645\n",
      "Epoch 3295, Loss: 0.2923900857567787, Final Batch Loss: 0.08956339955329895\n",
      "Epoch 3296, Loss: 0.3044317811727524, Final Batch Loss: 0.11599752306938171\n",
      "Epoch 3297, Loss: 0.29417603835463524, Final Batch Loss: 0.022128794342279434\n",
      "Epoch 3298, Loss: 0.3490752801299095, Final Batch Loss: 0.118050217628479\n",
      "Epoch 3299, Loss: 0.24894479289650917, Final Batch Loss: 0.04248689115047455\n",
      "Epoch 3300, Loss: 0.21916133910417557, Final Batch Loss: 0.026011478155851364\n",
      "Epoch 3301, Loss: 0.23480574414134026, Final Batch Loss: 0.07951685041189194\n",
      "Epoch 3302, Loss: 0.1940016783773899, Final Batch Loss: 0.0586143434047699\n",
      "Epoch 3303, Loss: 0.266384094953537, Final Batch Loss: 0.024857912212610245\n",
      "Epoch 3304, Loss: 0.20578595250844955, Final Batch Loss: 0.032023072242736816\n",
      "Epoch 3305, Loss: 0.1447652466595173, Final Batch Loss: 0.008470090106129646\n",
      "Epoch 3306, Loss: 0.24186187982559204, Final Batch Loss: 0.06961950659751892\n",
      "Epoch 3307, Loss: 0.16870197653770447, Final Batch Loss: 0.028655625879764557\n",
      "Epoch 3308, Loss: 0.23891715332865715, Final Batch Loss: 0.053667452186346054\n",
      "Epoch 3309, Loss: 0.2790587618947029, Final Batch Loss: 0.1122027263045311\n",
      "Epoch 3310, Loss: 0.33961527049541473, Final Batch Loss: 0.16366514563560486\n",
      "Epoch 3311, Loss: 0.27751966938376427, Final Batch Loss: 0.08508534729480743\n",
      "Epoch 3312, Loss: 0.25582822039723396, Final Batch Loss: 0.06285873055458069\n",
      "Epoch 3313, Loss: 0.3431781977415085, Final Batch Loss: 0.11434147506952286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3314, Loss: 0.18027406930923462, Final Batch Loss: 0.04118196666240692\n",
      "Epoch 3315, Loss: 0.47549041733145714, Final Batch Loss: 0.28503236174583435\n",
      "Epoch 3316, Loss: 0.23623273149132729, Final Batch Loss: 0.07274407893419266\n",
      "Epoch 3317, Loss: 0.3068225011229515, Final Batch Loss: 0.10585599392652512\n",
      "Epoch 3318, Loss: 0.2988923192024231, Final Batch Loss: 0.09593460708856583\n",
      "Epoch 3319, Loss: 0.3071560822427273, Final Batch Loss: 0.08363557606935501\n",
      "Epoch 3320, Loss: 0.27638474106788635, Final Batch Loss: 0.07019446790218353\n",
      "Epoch 3321, Loss: 0.2676957808434963, Final Batch Loss: 0.05139506608247757\n",
      "Epoch 3322, Loss: 0.29917772859334946, Final Batch Loss: 0.07775449007749557\n",
      "Epoch 3323, Loss: 0.2617424689233303, Final Batch Loss: 0.08933723717927933\n",
      "Epoch 3324, Loss: 0.24954398721456528, Final Batch Loss: 0.09494224935770035\n",
      "Epoch 3325, Loss: 0.2682730220258236, Final Batch Loss: 0.07791177183389664\n",
      "Epoch 3326, Loss: 0.29203080385923386, Final Batch Loss: 0.11173882335424423\n",
      "Epoch 3327, Loss: 0.23412029445171356, Final Batch Loss: 0.05270934849977493\n",
      "Epoch 3328, Loss: 0.185674786567688, Final Batch Loss: 0.05090102553367615\n",
      "Epoch 3329, Loss: 0.22857764177024364, Final Batch Loss: 0.02757529728114605\n",
      "Epoch 3330, Loss: 0.2048342488706112, Final Batch Loss: 0.041826263070106506\n",
      "Epoch 3331, Loss: 0.24755658768117428, Final Batch Loss: 0.09983416646718979\n",
      "Epoch 3332, Loss: 0.16149531491100788, Final Batch Loss: 0.026783112436532974\n",
      "Epoch 3333, Loss: 0.25038622319698334, Final Batch Loss: 0.043887920677661896\n",
      "Epoch 3334, Loss: 0.20434884168207645, Final Batch Loss: 0.029652336612343788\n",
      "Epoch 3335, Loss: 0.3050083629786968, Final Batch Loss: 0.13467416167259216\n",
      "Epoch 3336, Loss: 0.2291402779519558, Final Batch Loss: 0.09009580314159393\n",
      "Epoch 3337, Loss: 0.24471507221460342, Final Batch Loss: 0.03315570950508118\n",
      "Epoch 3338, Loss: 0.21916049346327782, Final Batch Loss: 0.037691880017519\n",
      "Epoch 3339, Loss: 0.17012961581349373, Final Batch Loss: 0.015995420515537262\n",
      "Epoch 3340, Loss: 0.2166428416967392, Final Batch Loss: 0.07516515254974365\n",
      "Epoch 3341, Loss: 0.2579554095864296, Final Batch Loss: 0.07603757828474045\n",
      "Epoch 3342, Loss: 0.2107238955795765, Final Batch Loss: 0.07384565472602844\n",
      "Epoch 3343, Loss: 0.2862525675445795, Final Batch Loss: 0.11166449636220932\n",
      "Epoch 3344, Loss: 0.195037841796875, Final Batch Loss: 0.0403248593211174\n",
      "Epoch 3345, Loss: 0.21195726469159126, Final Batch Loss: 0.056410957127809525\n",
      "Epoch 3346, Loss: 0.2735614962875843, Final Batch Loss: 0.08795441687107086\n",
      "Epoch 3347, Loss: 0.18512996286153793, Final Batch Loss: 0.034535832703113556\n",
      "Epoch 3348, Loss: 0.24750128015875816, Final Batch Loss: 0.07832932472229004\n",
      "Epoch 3349, Loss: 0.27136241272091866, Final Batch Loss: 0.030034251511096954\n",
      "Epoch 3350, Loss: 0.2684074193239212, Final Batch Loss: 0.10508593171834946\n",
      "Epoch 3351, Loss: 0.23245516046881676, Final Batch Loss: 0.033265940845012665\n",
      "Epoch 3352, Loss: 0.2754683718085289, Final Batch Loss: 0.0680549368262291\n",
      "Epoch 3353, Loss: 0.2321973480284214, Final Batch Loss: 0.03967291861772537\n",
      "Epoch 3354, Loss: 0.24477040022611618, Final Batch Loss: 0.02027040347456932\n",
      "Epoch 3355, Loss: 0.18503595888614655, Final Batch Loss: 0.04137200489640236\n",
      "Epoch 3356, Loss: 0.25669119879603386, Final Batch Loss: 0.1093241423368454\n",
      "Epoch 3357, Loss: 0.22034282237291336, Final Batch Loss: 0.056405894458293915\n",
      "Epoch 3358, Loss: 0.3710353225469589, Final Batch Loss: 0.15556976199150085\n",
      "Epoch 3359, Loss: 0.32540079951286316, Final Batch Loss: 0.08974623680114746\n",
      "Epoch 3360, Loss: 0.22197049856185913, Final Batch Loss: 0.08436783403158188\n",
      "Epoch 3361, Loss: 0.24798418022692204, Final Batch Loss: 0.1181962862610817\n",
      "Epoch 3362, Loss: 0.22748863324522972, Final Batch Loss: 0.07035655528306961\n",
      "Epoch 3363, Loss: 0.2521645836532116, Final Batch Loss: 0.05571557208895683\n",
      "Epoch 3364, Loss: 0.3085890859365463, Final Batch Loss: 0.09031625092029572\n",
      "Epoch 3365, Loss: 0.20758657157421112, Final Batch Loss: 0.08389396220445633\n",
      "Epoch 3366, Loss: 0.20903171598911285, Final Batch Loss: 0.06463026255369186\n",
      "Epoch 3367, Loss: 0.24776895716786385, Final Batch Loss: 0.08441190421581268\n",
      "Epoch 3368, Loss: 0.4413105174899101, Final Batch Loss: 0.20480088889598846\n",
      "Epoch 3369, Loss: 0.2906810827553272, Final Batch Loss: 0.09400507807731628\n",
      "Epoch 3370, Loss: 0.3274526298046112, Final Batch Loss: 0.18526382744312286\n",
      "Epoch 3371, Loss: 0.31813567876815796, Final Batch Loss: 0.1182485818862915\n",
      "Epoch 3372, Loss: 0.2734452746808529, Final Batch Loss: 0.06099116802215576\n",
      "Epoch 3373, Loss: 0.34186095744371414, Final Batch Loss: 0.09803416579961777\n",
      "Epoch 3374, Loss: 0.27739841118454933, Final Batch Loss: 0.047274861484766006\n",
      "Epoch 3375, Loss: 0.37080674804747105, Final Batch Loss: 0.17760322988033295\n",
      "Epoch 3376, Loss: 0.23451244737952948, Final Batch Loss: 0.012987426482141018\n",
      "Epoch 3377, Loss: 0.26990610733628273, Final Batch Loss: 0.08703045547008514\n",
      "Epoch 3378, Loss: 0.33282504975795746, Final Batch Loss: 0.1146884560585022\n",
      "Epoch 3379, Loss: 0.21061961352825165, Final Batch Loss: 0.0786600187420845\n",
      "Epoch 3380, Loss: 0.23382426798343658, Final Batch Loss: 0.04725629836320877\n",
      "Epoch 3381, Loss: 0.20484430342912674, Final Batch Loss: 0.03178684413433075\n",
      "Epoch 3382, Loss: 0.25373704731464386, Final Batch Loss: 0.062264516949653625\n",
      "Epoch 3383, Loss: 0.297833364456892, Final Batch Loss: 0.06947024166584015\n",
      "Epoch 3384, Loss: 0.2205798663198948, Final Batch Loss: 0.02042258158326149\n",
      "Epoch 3385, Loss: 0.27772701159119606, Final Batch Loss: 0.09139344841241837\n",
      "Epoch 3386, Loss: 0.2398248091340065, Final Batch Loss: 0.05158468335866928\n",
      "Epoch 3387, Loss: 0.25068455189466476, Final Batch Loss: 0.07772425562143326\n",
      "Epoch 3388, Loss: 0.27372185327112675, Final Batch Loss: 0.02159360982477665\n",
      "Epoch 3389, Loss: 0.31805960088968277, Final Batch Loss: 0.10394567996263504\n",
      "Epoch 3390, Loss: 0.31900227814912796, Final Batch Loss: 0.104068323969841\n",
      "Epoch 3391, Loss: 0.19348250329494476, Final Batch Loss: 0.08868828415870667\n",
      "Epoch 3392, Loss: 0.3271067440509796, Final Batch Loss: 0.07020827382802963\n",
      "Epoch 3393, Loss: 0.3946321904659271, Final Batch Loss: 0.1353149563074112\n",
      "Epoch 3394, Loss: 0.2974347025156021, Final Batch Loss: 0.12948551774024963\n",
      "Epoch 3395, Loss: 0.16823499649763107, Final Batch Loss: 0.022365454584360123\n",
      "Epoch 3396, Loss: 0.20738286897540092, Final Batch Loss: 0.023506250232458115\n",
      "Epoch 3397, Loss: 0.1975347436964512, Final Batch Loss: 0.07561684399843216\n",
      "Epoch 3398, Loss: 0.28234022483229637, Final Batch Loss: 0.09539615362882614\n",
      "Epoch 3399, Loss: 0.25289387069642544, Final Batch Loss: 0.07007154822349548\n",
      "Epoch 3400, Loss: 0.22223253175616264, Final Batch Loss: 0.11157769709825516\n",
      "Epoch 3401, Loss: 0.27365903928875923, Final Batch Loss: 0.09135300666093826\n",
      "Epoch 3402, Loss: 0.2475595362484455, Final Batch Loss: 0.062338367104530334\n",
      "Epoch 3403, Loss: 0.26269858703017235, Final Batch Loss: 0.050698231905698776\n",
      "Epoch 3404, Loss: 0.215229706838727, Final Batch Loss: 0.053350355476140976\n",
      "Epoch 3405, Loss: 0.21222220733761787, Final Batch Loss: 0.0305863656103611\n",
      "Epoch 3406, Loss: 0.19950449839234352, Final Batch Loss: 0.03449074551463127\n",
      "Epoch 3407, Loss: 0.16056741308420897, Final Batch Loss: 0.014209209941327572\n",
      "Epoch 3408, Loss: 0.26682887226343155, Final Batch Loss: 0.1004725992679596\n",
      "Epoch 3409, Loss: 0.23640386387705803, Final Batch Loss: 0.09006048738956451\n",
      "Epoch 3410, Loss: 0.2099253237247467, Final Batch Loss: 0.05764468014240265\n",
      "Epoch 3411, Loss: 0.21421584114432335, Final Batch Loss: 0.020489558577537537\n",
      "Epoch 3412, Loss: 0.24330002442002296, Final Batch Loss: 0.05680474266409874\n",
      "Epoch 3413, Loss: 0.27280116453766823, Final Batch Loss: 0.03823639824986458\n",
      "Epoch 3414, Loss: 0.2609988022595644, Final Batch Loss: 0.10284652560949326\n",
      "Epoch 3415, Loss: 0.22941072657704353, Final Batch Loss: 0.023266050964593887\n",
      "Epoch 3416, Loss: 0.26098454743623734, Final Batch Loss: 0.09786085784435272\n",
      "Epoch 3417, Loss: 0.27736405469477177, Final Batch Loss: 0.027375241741538048\n",
      "Epoch 3418, Loss: 0.20124214887619019, Final Batch Loss: 0.05772953853011131\n",
      "Epoch 3419, Loss: 0.18275662884116173, Final Batch Loss: 0.024993285536766052\n",
      "Epoch 3420, Loss: 0.23686803132295609, Final Batch Loss: 0.04842445254325867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3421, Loss: 0.22497010976076126, Final Batch Loss: 0.03870460018515587\n",
      "Epoch 3422, Loss: 0.21148712746798992, Final Batch Loss: 0.017923953011631966\n",
      "Epoch 3423, Loss: 0.27184949070215225, Final Batch Loss: 0.09124447405338287\n",
      "Epoch 3424, Loss: 0.25889716669917107, Final Batch Loss: 0.08238406479358673\n",
      "Epoch 3425, Loss: 0.17176281847059727, Final Batch Loss: 0.022887153550982475\n",
      "Epoch 3426, Loss: 0.22227256931364536, Final Batch Loss: 0.05049827694892883\n",
      "Epoch 3427, Loss: 0.1616481263190508, Final Batch Loss: 0.01844205893576145\n",
      "Epoch 3428, Loss: 0.3030616566538811, Final Batch Loss: 0.164465993642807\n",
      "Epoch 3429, Loss: 0.19007724709808826, Final Batch Loss: 0.013690432533621788\n",
      "Epoch 3430, Loss: 0.2769363224506378, Final Batch Loss: 0.04560767114162445\n",
      "Epoch 3431, Loss: 0.167889516800642, Final Batch Loss: 0.03382716700434685\n",
      "Epoch 3432, Loss: 0.19897149503231049, Final Batch Loss: 0.053100332617759705\n",
      "Epoch 3433, Loss: 0.17016890086233616, Final Batch Loss: 0.01798512227833271\n",
      "Epoch 3434, Loss: 0.13611379824578762, Final Batch Loss: 0.02922877110540867\n",
      "Epoch 3435, Loss: 0.19307386130094528, Final Batch Loss: 0.017109498381614685\n",
      "Epoch 3436, Loss: 0.2926349937915802, Final Batch Loss: 0.049738042056560516\n",
      "Epoch 3437, Loss: 0.24899254366755486, Final Batch Loss: 0.03197220340371132\n",
      "Epoch 3438, Loss: 0.21972626075148582, Final Batch Loss: 0.08103657513856888\n",
      "Epoch 3439, Loss: 0.21183663606643677, Final Batch Loss: 0.06714439392089844\n",
      "Epoch 3440, Loss: 0.3228767141699791, Final Batch Loss: 0.06215064227581024\n",
      "Epoch 3441, Loss: 0.3270883671939373, Final Batch Loss: 0.0933874174952507\n",
      "Epoch 3442, Loss: 0.33204226568341255, Final Batch Loss: 0.031303662806749344\n",
      "Epoch 3443, Loss: 0.30456846579909325, Final Batch Loss: 0.09595046937465668\n",
      "Epoch 3444, Loss: 0.2819863297045231, Final Batch Loss: 0.061082933098077774\n",
      "Epoch 3445, Loss: 0.2754787504673004, Final Batch Loss: 0.10370798408985138\n",
      "Epoch 3446, Loss: 0.24892989173531532, Final Batch Loss: 0.07015682756900787\n",
      "Epoch 3447, Loss: 0.18102610856294632, Final Batch Loss: 0.043112095445394516\n",
      "Epoch 3448, Loss: 0.307005126029253, Final Batch Loss: 0.13702504336833954\n",
      "Epoch 3449, Loss: 0.20854805037379265, Final Batch Loss: 0.04719080775976181\n",
      "Epoch 3450, Loss: 0.27238187193870544, Final Batch Loss: 0.03547051548957825\n",
      "Epoch 3451, Loss: 0.2481148410588503, Final Batch Loss: 0.02110333926975727\n",
      "Epoch 3452, Loss: 0.22782615572214127, Final Batch Loss: 0.05174177139997482\n",
      "Epoch 3453, Loss: 0.19322151690721512, Final Batch Loss: 0.027205228805541992\n",
      "Epoch 3454, Loss: 0.2140168398618698, Final Batch Loss: 0.04140264540910721\n",
      "Epoch 3455, Loss: 0.22395916655659676, Final Batch Loss: 0.0819651186466217\n",
      "Epoch 3456, Loss: 0.2877421583980322, Final Batch Loss: 0.13219822943210602\n",
      "Epoch 3457, Loss: 0.17139269039034843, Final Batch Loss: 0.04404040798544884\n",
      "Epoch 3458, Loss: 0.21149490773677826, Final Batch Loss: 0.03923027589917183\n",
      "Epoch 3459, Loss: 0.2374611236155033, Final Batch Loss: 0.09409786015748978\n",
      "Epoch 3460, Loss: 0.1659205574542284, Final Batch Loss: 0.051752932369709015\n",
      "Epoch 3461, Loss: 0.16949089989066124, Final Batch Loss: 0.016734175384044647\n",
      "Epoch 3462, Loss: 0.30740638077259064, Final Batch Loss: 0.11836265027523041\n",
      "Epoch 3463, Loss: 0.20450024120509624, Final Batch Loss: 0.03096156381070614\n",
      "Epoch 3464, Loss: 0.27469199895858765, Final Batch Loss: 0.05311383306980133\n",
      "Epoch 3465, Loss: 0.26436465978622437, Final Batch Loss: 0.06026001274585724\n",
      "Epoch 3466, Loss: 0.19070805236697197, Final Batch Loss: 0.03335662558674812\n",
      "Epoch 3467, Loss: 0.2732416205108166, Final Batch Loss: 0.08786500990390778\n",
      "Epoch 3468, Loss: 0.22250228375196457, Final Batch Loss: 0.06206619739532471\n",
      "Epoch 3469, Loss: 0.2291383147239685, Final Batch Loss: 0.07360409200191498\n",
      "Epoch 3470, Loss: 0.18435484915971756, Final Batch Loss: 0.04024915397167206\n",
      "Epoch 3471, Loss: 0.21920395083725452, Final Batch Loss: 0.02290465496480465\n",
      "Epoch 3472, Loss: 0.2685951143503189, Final Batch Loss: 0.12898540496826172\n",
      "Epoch 3473, Loss: 0.3089769557118416, Final Batch Loss: 0.07586217671632767\n",
      "Epoch 3474, Loss: 0.44682783633470535, Final Batch Loss: 0.09091487526893616\n",
      "Epoch 3475, Loss: 0.45350804179906845, Final Batch Loss: 0.17751720547676086\n",
      "Epoch 3476, Loss: 0.2628669776022434, Final Batch Loss: 0.11142820119857788\n",
      "Epoch 3477, Loss: 0.2960987463593483, Final Batch Loss: 0.0546870194375515\n",
      "Epoch 3478, Loss: 0.2686498835682869, Final Batch Loss: 0.04239219054579735\n",
      "Epoch 3479, Loss: 0.30293481796979904, Final Batch Loss: 0.0336373969912529\n",
      "Epoch 3480, Loss: 0.2726125940680504, Final Batch Loss: 0.08182729780673981\n",
      "Epoch 3481, Loss: 0.20716869831085205, Final Batch Loss: 0.05526701360940933\n",
      "Epoch 3482, Loss: 0.22954069077968597, Final Batch Loss: 0.03702288120985031\n",
      "Epoch 3483, Loss: 0.3178237397223711, Final Batch Loss: 0.12079552561044693\n",
      "Epoch 3484, Loss: 0.2791597247123718, Final Batch Loss: 0.142708882689476\n",
      "Epoch 3485, Loss: 0.3392241969704628, Final Batch Loss: 0.1088772639632225\n",
      "Epoch 3486, Loss: 0.49428053200244904, Final Batch Loss: 0.1775471419095993\n",
      "Epoch 3487, Loss: 0.3519519679248333, Final Batch Loss: 0.13440875709056854\n",
      "Epoch 3488, Loss: 0.3204494081437588, Final Batch Loss: 0.046703096479177475\n",
      "Epoch 3489, Loss: 0.21462174132466316, Final Batch Loss: 0.040968503803014755\n",
      "Epoch 3490, Loss: 0.23106150329113007, Final Batch Loss: 0.045061178505420685\n",
      "Epoch 3491, Loss: 0.3219247870147228, Final Batch Loss: 0.04092874750494957\n",
      "Epoch 3492, Loss: 0.284234706312418, Final Batch Loss: 0.08313079923391342\n",
      "Epoch 3493, Loss: 0.28683968260884285, Final Batch Loss: 0.1118120551109314\n",
      "Epoch 3494, Loss: 0.33378447219729424, Final Batch Loss: 0.12718087434768677\n",
      "Epoch 3495, Loss: 0.2622018791735172, Final Batch Loss: 0.08803266286849976\n",
      "Epoch 3496, Loss: 0.36064066737890244, Final Batch Loss: 0.04322679340839386\n",
      "Epoch 3497, Loss: 0.33173732832074165, Final Batch Loss: 0.12034112215042114\n",
      "Epoch 3498, Loss: 0.1897641383111477, Final Batch Loss: 0.014593999832868576\n",
      "Epoch 3499, Loss: 0.236604206264019, Final Batch Loss: 0.09216025471687317\n",
      "Epoch 3500, Loss: 0.3009687103331089, Final Batch Loss: 0.09146932512521744\n",
      "Epoch 3501, Loss: 0.21879056096076965, Final Batch Loss: 0.07762758433818817\n",
      "Epoch 3502, Loss: 0.2601592391729355, Final Batch Loss: 0.057195574045181274\n",
      "Epoch 3503, Loss: 0.2369128055870533, Final Batch Loss: 0.023052606731653214\n",
      "Epoch 3504, Loss: 0.3216070085763931, Final Batch Loss: 0.11271366477012634\n",
      "Epoch 3505, Loss: 0.22997045330703259, Final Batch Loss: 0.01767018251121044\n",
      "Epoch 3506, Loss: 0.1910840217024088, Final Batch Loss: 0.02334952913224697\n",
      "Epoch 3507, Loss: 0.29715191200375557, Final Batch Loss: 0.0842539444565773\n",
      "Epoch 3508, Loss: 0.21891894191503525, Final Batch Loss: 0.04636470973491669\n",
      "Epoch 3509, Loss: 0.23324688896536827, Final Batch Loss: 0.06942763179540634\n",
      "Epoch 3510, Loss: 0.3552924804389477, Final Batch Loss: 0.13367369771003723\n",
      "Epoch 3511, Loss: 0.27063263207674026, Final Batch Loss: 0.08201055973768234\n",
      "Epoch 3512, Loss: 0.27086929231882095, Final Batch Loss: 0.045946843922138214\n",
      "Epoch 3513, Loss: 0.3285600356757641, Final Batch Loss: 0.02262600138783455\n",
      "Epoch 3514, Loss: 0.19944646209478378, Final Batch Loss: 0.07403641939163208\n",
      "Epoch 3515, Loss: 0.22157831117510796, Final Batch Loss: 0.036920007318258286\n",
      "Epoch 3516, Loss: 0.21353007480502129, Final Batch Loss: 0.051572274416685104\n",
      "Epoch 3517, Loss: 0.2557472251355648, Final Batch Loss: 0.1368982344865799\n",
      "Epoch 3518, Loss: 0.20222503505647182, Final Batch Loss: 0.020804906263947487\n",
      "Epoch 3519, Loss: 0.29425064101815224, Final Batch Loss: 0.12597262859344482\n",
      "Epoch 3520, Loss: 0.2242344617843628, Final Batch Loss: 0.08869274705648422\n",
      "Epoch 3521, Loss: 0.3092721663415432, Final Batch Loss: 0.032846152782440186\n",
      "Epoch 3522, Loss: 0.22325812093913555, Final Batch Loss: 0.01037103496491909\n",
      "Epoch 3523, Loss: 0.32856572046875954, Final Batch Loss: 0.13077355921268463\n",
      "Epoch 3524, Loss: 0.2550036460161209, Final Batch Loss: 0.10507385432720184\n",
      "Epoch 3525, Loss: 0.20366962254047394, Final Batch Loss: 0.08124666661024094\n",
      "Epoch 3526, Loss: 0.3161197640001774, Final Batch Loss: 0.0895504355430603\n",
      "Epoch 3527, Loss: 0.2553163394331932, Final Batch Loss: 0.04082305729389191\n",
      "Epoch 3528, Loss: 0.24107619002461433, Final Batch Loss: 0.01895049959421158\n",
      "Epoch 3529, Loss: 0.26929162070155144, Final Batch Loss: 0.06494702398777008\n",
      "Epoch 3530, Loss: 0.2193550057709217, Final Batch Loss: 0.08358019590377808\n",
      "Epoch 3531, Loss: 0.2032206617295742, Final Batch Loss: 0.033770330250263214\n",
      "Epoch 3532, Loss: 0.19393078237771988, Final Batch Loss: 0.038976188749074936\n",
      "Epoch 3533, Loss: 0.30610913783311844, Final Batch Loss: 0.04928293824195862\n",
      "Epoch 3534, Loss: 0.21720227971673012, Final Batch Loss: 0.040687743574380875\n",
      "Epoch 3535, Loss: 0.2617410346865654, Final Batch Loss: 0.09516680240631104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3536, Loss: 0.16950819082558155, Final Batch Loss: 0.013144878670573235\n",
      "Epoch 3537, Loss: 0.2532353959977627, Final Batch Loss: 0.058831121772527695\n",
      "Epoch 3538, Loss: 0.24909278377890587, Final Batch Loss: 0.07572101056575775\n",
      "Epoch 3539, Loss: 0.22842558845877647, Final Batch Loss: 0.032795026898384094\n",
      "Epoch 3540, Loss: 0.19511571899056435, Final Batch Loss: 0.038870807737112045\n",
      "Epoch 3541, Loss: 0.3003556840121746, Final Batch Loss: 0.04534484073519707\n",
      "Epoch 3542, Loss: 0.26288555189967155, Final Batch Loss: 0.08465209603309631\n",
      "Epoch 3543, Loss: 0.1631820872426033, Final Batch Loss: 0.0327177532017231\n",
      "Epoch 3544, Loss: 0.18212741799652576, Final Batch Loss: 0.05249940603971481\n",
      "Epoch 3545, Loss: 0.21118957363069057, Final Batch Loss: 0.07694847136735916\n",
      "Epoch 3546, Loss: 0.28232870250940323, Final Batch Loss: 0.01984959840774536\n",
      "Epoch 3547, Loss: 0.2341371290385723, Final Batch Loss: 0.05051829293370247\n",
      "Epoch 3548, Loss: 0.1898434143513441, Final Batch Loss: 0.07155805826187134\n",
      "Epoch 3549, Loss: 0.24093488231301308, Final Batch Loss: 0.016379646956920624\n",
      "Epoch 3550, Loss: 0.1503252498805523, Final Batch Loss: 0.020017525181174278\n",
      "Epoch 3551, Loss: 0.2501542791724205, Final Batch Loss: 0.06448627263307571\n",
      "Epoch 3552, Loss: 0.28172698616981506, Final Batch Loss: 0.11750221252441406\n",
      "Epoch 3553, Loss: 0.18661119230091572, Final Batch Loss: 0.07068648189306259\n",
      "Epoch 3554, Loss: 0.26176275312900543, Final Batch Loss: 0.09784378856420517\n",
      "Epoch 3555, Loss: 0.2362692914903164, Final Batch Loss: 0.036793652921915054\n",
      "Epoch 3556, Loss: 0.28245692141354084, Final Batch Loss: 0.021956713870167732\n",
      "Epoch 3557, Loss: 0.15549328178167343, Final Batch Loss: 0.029862860217690468\n",
      "Epoch 3558, Loss: 0.2519948799163103, Final Batch Loss: 0.0665924921631813\n",
      "Epoch 3559, Loss: 0.3162270337343216, Final Batch Loss: 0.09686122834682465\n",
      "Epoch 3560, Loss: 0.3357851915061474, Final Batch Loss: 0.15723150968551636\n",
      "Epoch 3561, Loss: 0.28044678643345833, Final Batch Loss: 0.09367424994707108\n",
      "Epoch 3562, Loss: 0.2121461182832718, Final Batch Loss: 0.03643418848514557\n",
      "Epoch 3563, Loss: 0.27282608672976494, Final Batch Loss: 0.04620719701051712\n",
      "Epoch 3564, Loss: 0.31216203421354294, Final Batch Loss: 0.10571873933076859\n",
      "Epoch 3565, Loss: 0.30103248357772827, Final Batch Loss: 0.08152720332145691\n",
      "Epoch 3566, Loss: 0.2415270172059536, Final Batch Loss: 0.05172934755682945\n",
      "Epoch 3567, Loss: 0.2918423004448414, Final Batch Loss: 0.10325861722230911\n",
      "Epoch 3568, Loss: 0.26142049953341484, Final Batch Loss: 0.09987158328294754\n",
      "Epoch 3569, Loss: 0.2181592360138893, Final Batch Loss: 0.03298940137028694\n",
      "Epoch 3570, Loss: 0.21416220255196095, Final Batch Loss: 0.014826664701104164\n",
      "Epoch 3571, Loss: 0.2199559286236763, Final Batch Loss: 0.03999941796064377\n",
      "Epoch 3572, Loss: 0.27929285913705826, Final Batch Loss: 0.10276526212692261\n",
      "Epoch 3573, Loss: 0.2392834573984146, Final Batch Loss: 0.06074189767241478\n",
      "Epoch 3574, Loss: 0.22571877017617226, Final Batch Loss: 0.059898894280195236\n",
      "Epoch 3575, Loss: 0.26280515268445015, Final Batch Loss: 0.0384039469063282\n",
      "Epoch 3576, Loss: 0.26288139820098877, Final Batch Loss: 0.08425182104110718\n",
      "Epoch 3577, Loss: 0.23545370995998383, Final Batch Loss: 0.1214679703116417\n",
      "Epoch 3578, Loss: 0.19876237213611603, Final Batch Loss: 0.05595075711607933\n",
      "Epoch 3579, Loss: 0.2512601241469383, Final Batch Loss: 0.11025954782962799\n",
      "Epoch 3580, Loss: 0.303300891071558, Final Batch Loss: 0.06756112724542618\n",
      "Epoch 3581, Loss: 0.1839265450835228, Final Batch Loss: 0.0357399508357048\n",
      "Epoch 3582, Loss: 0.30258360132575035, Final Batch Loss: 0.08201786875724792\n",
      "Epoch 3583, Loss: 0.25080805644392967, Final Batch Loss: 0.06782841682434082\n",
      "Epoch 3584, Loss: 0.20931505039334297, Final Batch Loss: 0.025273136794567108\n",
      "Epoch 3585, Loss: 0.22858001664280891, Final Batch Loss: 0.048398490995168686\n",
      "Epoch 3586, Loss: 0.21225288324058056, Final Batch Loss: 0.04342000186443329\n",
      "Epoch 3587, Loss: 0.20454447716474533, Final Batch Loss: 0.054048407822847366\n",
      "Epoch 3588, Loss: 0.25115473568439484, Final Batch Loss: 0.05522185191512108\n",
      "Epoch 3589, Loss: 0.18747059628367424, Final Batch Loss: 0.03642916679382324\n",
      "Epoch 3590, Loss: 0.1965783853083849, Final Batch Loss: 0.01589733548462391\n",
      "Epoch 3591, Loss: 0.2529391199350357, Final Batch Loss: 0.054369937628507614\n",
      "Epoch 3592, Loss: 0.15934991836547852, Final Batch Loss: 0.04706462845206261\n",
      "Epoch 3593, Loss: 0.19936475902795792, Final Batch Loss: 0.04114823043346405\n",
      "Epoch 3594, Loss: 0.2229122556746006, Final Batch Loss: 0.04407341405749321\n",
      "Epoch 3595, Loss: 0.22395193949341774, Final Batch Loss: 0.07168354094028473\n",
      "Epoch 3596, Loss: 0.2334720492362976, Final Batch Loss: 0.08594771474599838\n",
      "Epoch 3597, Loss: 0.19501002877950668, Final Batch Loss: 0.06933082640171051\n",
      "Epoch 3598, Loss: 0.22373934090137482, Final Batch Loss: 0.07641784101724625\n",
      "Epoch 3599, Loss: 0.24792896583676338, Final Batch Loss: 0.08753569424152374\n",
      "Epoch 3600, Loss: 0.410075344145298, Final Batch Loss: 0.15519529581069946\n",
      "Epoch 3601, Loss: 0.21401214972138405, Final Batch Loss: 0.06610574573278427\n",
      "Epoch 3602, Loss: 0.31320861726999283, Final Batch Loss: 0.09135930985212326\n",
      "Epoch 3603, Loss: 0.22236531227827072, Final Batch Loss: 0.017681248486042023\n",
      "Epoch 3604, Loss: 0.23935586959123611, Final Batch Loss: 0.05534472316503525\n",
      "Epoch 3605, Loss: 0.23150297440588474, Final Batch Loss: 0.02095872536301613\n",
      "Epoch 3606, Loss: 0.1890173852443695, Final Batch Loss: 0.03624623268842697\n",
      "Epoch 3607, Loss: 0.249343890696764, Final Batch Loss: 0.060242701321840286\n",
      "Epoch 3608, Loss: 0.19267336651682854, Final Batch Loss: 0.03856033831834793\n",
      "Epoch 3609, Loss: 0.1579007115215063, Final Batch Loss: 0.05840883031487465\n",
      "Epoch 3610, Loss: 0.23782916367053986, Final Batch Loss: 0.09929202497005463\n",
      "Epoch 3611, Loss: 0.21015194430947304, Final Batch Loss: 0.06072528660297394\n",
      "Epoch 3612, Loss: 0.22382983937859535, Final Batch Loss: 0.060686998069286346\n",
      "Epoch 3613, Loss: 0.13518737815320492, Final Batch Loss: 0.033219825476408005\n",
      "Epoch 3614, Loss: 0.2573455795645714, Final Batch Loss: 0.08739523589611053\n",
      "Epoch 3615, Loss: 0.21234633028507233, Final Batch Loss: 0.01643645390868187\n",
      "Epoch 3616, Loss: 0.1686720410361886, Final Batch Loss: 0.012023632414638996\n",
      "Epoch 3617, Loss: 0.3380184471607208, Final Batch Loss: 0.12281151860952377\n",
      "Epoch 3618, Loss: 0.16665444150567055, Final Batch Loss: 0.04112724959850311\n",
      "Epoch 3619, Loss: 0.14864153694361448, Final Batch Loss: 0.011213053949177265\n",
      "Epoch 3620, Loss: 0.23004179075360298, Final Batch Loss: 0.07066403329372406\n",
      "Epoch 3621, Loss: 0.19943486899137497, Final Batch Loss: 0.04277130961418152\n",
      "Epoch 3622, Loss: 0.21149160340428352, Final Batch Loss: 0.03301456943154335\n",
      "Epoch 3623, Loss: 0.2847350500524044, Final Batch Loss: 0.08244727551937103\n",
      "Epoch 3624, Loss: 0.3440031632781029, Final Batch Loss: 0.11140933632850647\n",
      "Epoch 3625, Loss: 0.21124159917235374, Final Batch Loss: 0.056581854820251465\n",
      "Epoch 3626, Loss: 0.4265073090791702, Final Batch Loss: 0.13534918427467346\n",
      "Epoch 3627, Loss: 0.2721877805888653, Final Batch Loss: 0.03711365535855293\n",
      "Epoch 3628, Loss: 0.3494906462728977, Final Batch Loss: 0.09346470236778259\n",
      "Epoch 3629, Loss: 0.26747702434659004, Final Batch Loss: 0.09858022630214691\n",
      "Epoch 3630, Loss: 0.4110666662454605, Final Batch Loss: 0.13533224165439606\n",
      "Epoch 3631, Loss: 0.2134551666676998, Final Batch Loss: 0.020889028906822205\n",
      "Epoch 3632, Loss: 0.24076846335083246, Final Batch Loss: 0.007876061834394932\n",
      "Epoch 3633, Loss: 0.2521907724440098, Final Batch Loss: 0.0780210793018341\n",
      "Epoch 3634, Loss: 0.19542460516095161, Final Batch Loss: 0.03630391135811806\n",
      "Epoch 3635, Loss: 0.26422831416130066, Final Batch Loss: 0.053907688707113266\n",
      "Epoch 3636, Loss: 0.2565889023244381, Final Batch Loss: 0.032255228608846664\n",
      "Epoch 3637, Loss: 0.3648520205169916, Final Batch Loss: 0.2305038422346115\n",
      "Epoch 3638, Loss: 0.24688728898763657, Final Batch Loss: 0.08073970675468445\n",
      "Epoch 3639, Loss: 0.20120366662740707, Final Batch Loss: 0.04119276627898216\n",
      "Epoch 3640, Loss: 0.24810345098376274, Final Batch Loss: 0.04548264667391777\n",
      "Epoch 3641, Loss: 0.16381219867616892, Final Batch Loss: 0.011578322388231754\n",
      "Epoch 3642, Loss: 0.2977089025080204, Final Batch Loss: 0.132063627243042\n",
      "Epoch 3643, Loss: 0.22040900215506554, Final Batch Loss: 0.06210144981741905\n",
      "Epoch 3644, Loss: 0.17825031839311123, Final Batch Loss: 0.023912036791443825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3645, Loss: 0.2846480831503868, Final Batch Loss: 0.10045020282268524\n",
      "Epoch 3646, Loss: 0.24488109350204468, Final Batch Loss: 0.03918822854757309\n",
      "Epoch 3647, Loss: 0.23915527015924454, Final Batch Loss: 0.08748140186071396\n",
      "Epoch 3648, Loss: 0.27974352799355984, Final Batch Loss: 0.14499451220035553\n",
      "Epoch 3649, Loss: 0.2638493664562702, Final Batch Loss: 0.033819779753685\n",
      "Epoch 3650, Loss: 0.2796355113387108, Final Batch Loss: 0.04621428623795509\n",
      "Epoch 3651, Loss: 0.22094007581472397, Final Batch Loss: 0.028735175728797913\n",
      "Epoch 3652, Loss: 0.21788671612739563, Final Batch Loss: 0.038181837648153305\n",
      "Epoch 3653, Loss: 0.3483536019921303, Final Batch Loss: 0.14348208904266357\n",
      "Epoch 3654, Loss: 0.25107308849692345, Final Batch Loss: 0.05516727641224861\n",
      "Epoch 3655, Loss: 0.2115337885916233, Final Batch Loss: 0.049865465611219406\n",
      "Epoch 3656, Loss: 0.1453802604228258, Final Batch Loss: 0.05453448370099068\n",
      "Epoch 3657, Loss: 0.2720201648771763, Final Batch Loss: 0.07469072937965393\n",
      "Epoch 3658, Loss: 0.17384681105613708, Final Batch Loss: 0.04567573964595795\n",
      "Epoch 3659, Loss: 0.26201590709388256, Final Batch Loss: 0.026791797950863838\n",
      "Epoch 3660, Loss: 0.17396705597639084, Final Batch Loss: 0.04064454510807991\n",
      "Epoch 3661, Loss: 0.18643227964639664, Final Batch Loss: 0.04846975952386856\n",
      "Epoch 3662, Loss: 0.20324945449829102, Final Batch Loss: 0.0764075443148613\n",
      "Epoch 3663, Loss: 0.14889276959002018, Final Batch Loss: 0.017513195052742958\n",
      "Epoch 3664, Loss: 0.2006380930542946, Final Batch Loss: 0.04281310364603996\n",
      "Epoch 3665, Loss: 0.28613052517175674, Final Batch Loss: 0.07996618002653122\n",
      "Epoch 3666, Loss: 0.2803221605718136, Final Batch Loss: 0.07797734439373016\n",
      "Epoch 3667, Loss: 0.27123045176267624, Final Batch Loss: 0.07566875964403152\n",
      "Epoch 3668, Loss: 0.1735632698982954, Final Batch Loss: 0.05814326927065849\n",
      "Epoch 3669, Loss: 0.21832296065986156, Final Batch Loss: 0.08785124123096466\n",
      "Epoch 3670, Loss: 0.26795943826436996, Final Batch Loss: 0.1009593978524208\n",
      "Epoch 3671, Loss: 0.2507611233741045, Final Batch Loss: 0.016996318474411964\n",
      "Epoch 3672, Loss: 0.29702064767479897, Final Batch Loss: 0.11436745524406433\n",
      "Epoch 3673, Loss: 0.25926749408245087, Final Batch Loss: 0.0767437294125557\n",
      "Epoch 3674, Loss: 0.30900174006819725, Final Batch Loss: 0.10223344713449478\n",
      "Epoch 3675, Loss: 0.1708870818838477, Final Batch Loss: 0.012926029972732067\n",
      "Epoch 3676, Loss: 0.24185458198189735, Final Batch Loss: 0.05905396491289139\n",
      "Epoch 3677, Loss: 0.1895989179611206, Final Batch Loss: 0.03423933684825897\n",
      "Epoch 3678, Loss: 0.2599264830350876, Final Batch Loss: 0.09836096316576004\n",
      "Epoch 3679, Loss: 0.23224037140607834, Final Batch Loss: 0.05990615114569664\n",
      "Epoch 3680, Loss: 0.22002688981592655, Final Batch Loss: 0.02165827713906765\n",
      "Epoch 3681, Loss: 0.3140173442661762, Final Batch Loss: 0.047880660742521286\n",
      "Epoch 3682, Loss: 0.20796135999262333, Final Batch Loss: 0.02935045398771763\n",
      "Epoch 3683, Loss: 0.3081056699156761, Final Batch Loss: 0.07894560694694519\n",
      "Epoch 3684, Loss: 0.19195651449263096, Final Batch Loss: 0.053299903869628906\n",
      "Epoch 3685, Loss: 0.21331307664513588, Final Batch Loss: 0.051874130964279175\n",
      "Epoch 3686, Loss: 0.20717730931937695, Final Batch Loss: 0.018770771101117134\n",
      "Epoch 3687, Loss: 0.20125596597790718, Final Batch Loss: 0.03551957756280899\n",
      "Epoch 3688, Loss: 0.18300257623195648, Final Batch Loss: 0.019035663455724716\n",
      "Epoch 3689, Loss: 0.27050473541021347, Final Batch Loss: 0.07390712201595306\n",
      "Epoch 3690, Loss: 0.22423003613948822, Final Batch Loss: 0.04489868879318237\n",
      "Epoch 3691, Loss: 0.3484271802008152, Final Batch Loss: 0.03548014909029007\n",
      "Epoch 3692, Loss: 0.263987485319376, Final Batch Loss: 0.08320964872837067\n",
      "Epoch 3693, Loss: 0.3100917153060436, Final Batch Loss: 0.148411825299263\n",
      "Epoch 3694, Loss: 0.24781831912696362, Final Batch Loss: 0.02247467450797558\n",
      "Epoch 3695, Loss: 0.22874385118484497, Final Batch Loss: 0.04027194902300835\n",
      "Epoch 3696, Loss: 0.16857995092868805, Final Batch Loss: 0.020097769796848297\n",
      "Epoch 3697, Loss: 0.2180745918303728, Final Batch Loss: 0.019731448963284492\n",
      "Epoch 3698, Loss: 0.14111922308802605, Final Batch Loss: 0.033368900418281555\n",
      "Epoch 3699, Loss: 0.2511339895427227, Final Batch Loss: 0.06757871806621552\n",
      "Epoch 3700, Loss: 0.2270340397953987, Final Batch Loss: 0.023694761097431183\n",
      "Epoch 3701, Loss: 0.2368571236729622, Final Batch Loss: 0.0934765562415123\n",
      "Epoch 3702, Loss: 0.15601663291454315, Final Batch Loss: 0.03137722238898277\n",
      "Epoch 3703, Loss: 0.1525266245007515, Final Batch Loss: 0.012285538017749786\n",
      "Epoch 3704, Loss: 0.2501092702150345, Final Batch Loss: 0.08953800797462463\n",
      "Epoch 3705, Loss: 0.33616507053375244, Final Batch Loss: 0.1933073252439499\n",
      "Epoch 3706, Loss: 0.204638060182333, Final Batch Loss: 0.09384933859109879\n",
      "Epoch 3707, Loss: 0.2602407969534397, Final Batch Loss: 0.04937223717570305\n",
      "Epoch 3708, Loss: 0.40003278478980064, Final Batch Loss: 0.1970064342021942\n",
      "Epoch 3709, Loss: 0.17036147974431515, Final Batch Loss: 0.014881080016493797\n",
      "Epoch 3710, Loss: 0.23804635182023048, Final Batch Loss: 0.06359822303056717\n",
      "Epoch 3711, Loss: 0.23199425637722015, Final Batch Loss: 0.06052442267537117\n",
      "Epoch 3712, Loss: 0.22295943647623062, Final Batch Loss: 0.04214605316519737\n",
      "Epoch 3713, Loss: 0.3392031341791153, Final Batch Loss: 0.13550414144992828\n",
      "Epoch 3714, Loss: 0.18480995669960976, Final Batch Loss: 0.06711242347955704\n",
      "Epoch 3715, Loss: 0.2663496248424053, Final Batch Loss: 0.09021534770727158\n",
      "Epoch 3716, Loss: 0.23541336320340633, Final Batch Loss: 0.09123709797859192\n",
      "Epoch 3717, Loss: 0.24194209650158882, Final Batch Loss: 0.08112233877182007\n",
      "Epoch 3718, Loss: 0.2693640813231468, Final Batch Loss: 0.025614343583583832\n",
      "Epoch 3719, Loss: 0.24853960238397121, Final Batch Loss: 0.052670929580926895\n",
      "Epoch 3720, Loss: 0.3107456751167774, Final Batch Loss: 0.045493658632040024\n",
      "Epoch 3721, Loss: 0.276744969189167, Final Batch Loss: 0.09509694576263428\n",
      "Epoch 3722, Loss: 0.32226284220814705, Final Batch Loss: 0.09282496571540833\n",
      "Epoch 3723, Loss: 0.26162097975611687, Final Batch Loss: 0.08327168971300125\n",
      "Epoch 3724, Loss: 0.25948214530944824, Final Batch Loss: 0.046783026307821274\n",
      "Epoch 3725, Loss: 0.21385622769594193, Final Batch Loss: 0.03684789314866066\n",
      "Epoch 3726, Loss: 0.21222597360610962, Final Batch Loss: 0.0625416561961174\n",
      "Epoch 3727, Loss: 0.2484230250120163, Final Batch Loss: 0.04711969196796417\n",
      "Epoch 3728, Loss: 0.15025886334478855, Final Batch Loss: 0.025444205850362778\n",
      "Epoch 3729, Loss: 0.24512610584497452, Final Batch Loss: 0.05984814092516899\n",
      "Epoch 3730, Loss: 0.2213468700647354, Final Batch Loss: 0.01660899817943573\n",
      "Epoch 3731, Loss: 0.2751947455108166, Final Batch Loss: 0.1163199171423912\n",
      "Epoch 3732, Loss: 0.2558754272758961, Final Batch Loss: 0.09447021037340164\n",
      "Epoch 3733, Loss: 0.18187874928116798, Final Batch Loss: 0.03213430568575859\n",
      "Epoch 3734, Loss: 0.2924133874475956, Final Batch Loss: 0.07696564495563507\n",
      "Epoch 3735, Loss: 0.13639875501394272, Final Batch Loss: 0.034092023968696594\n",
      "Epoch 3736, Loss: 0.1595325404778123, Final Batch Loss: 0.03003680892288685\n",
      "Epoch 3737, Loss: 0.22080966271460056, Final Batch Loss: 0.06504884362220764\n",
      "Epoch 3738, Loss: 0.1864958554506302, Final Batch Loss: 0.04160524904727936\n",
      "Epoch 3739, Loss: 0.19648498110473156, Final Batch Loss: 0.06076687574386597\n",
      "Epoch 3740, Loss: 0.1702872235327959, Final Batch Loss: 0.04353691637516022\n",
      "Epoch 3741, Loss: 0.1695596594363451, Final Batch Loss: 0.02756921574473381\n",
      "Epoch 3742, Loss: 0.24768031015992165, Final Batch Loss: 0.04914458841085434\n",
      "Epoch 3743, Loss: 0.21455655619502068, Final Batch Loss: 0.0689052939414978\n",
      "Epoch 3744, Loss: 0.2807016335427761, Final Batch Loss: 0.09640742093324661\n",
      "Epoch 3745, Loss: 0.22411350533366203, Final Batch Loss: 0.036175552755594254\n",
      "Epoch 3746, Loss: 0.30495535582304, Final Batch Loss: 0.12310153990983963\n",
      "Epoch 3747, Loss: 0.2583945393562317, Final Batch Loss: 0.06411334872245789\n",
      "Epoch 3748, Loss: 0.2697054520249367, Final Batch Loss: 0.06659111380577087\n",
      "Epoch 3749, Loss: 0.2545721624046564, Final Batch Loss: 0.030493000522255898\n",
      "Epoch 3750, Loss: 0.31958502158522606, Final Batch Loss: 0.06519576162099838\n",
      "Epoch 3751, Loss: 0.20740630477666855, Final Batch Loss: 0.03804453834891319\n",
      "Epoch 3752, Loss: 0.16793286986649036, Final Batch Loss: 0.036869168281555176\n",
      "Epoch 3753, Loss: 0.16293332912027836, Final Batch Loss: 0.022178230807185173\n",
      "Epoch 3754, Loss: 0.23314486630260944, Final Batch Loss: 0.0818089097738266\n",
      "Epoch 3755, Loss: 0.33237479254603386, Final Batch Loss: 0.14484602212905884\n",
      "Epoch 3756, Loss: 0.26073938235640526, Final Batch Loss: 0.08652230352163315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3757, Loss: 0.15698915347456932, Final Batch Loss: 0.02390485815703869\n",
      "Epoch 3758, Loss: 0.18467270582914352, Final Batch Loss: 0.04051515832543373\n",
      "Epoch 3759, Loss: 0.19851566851139069, Final Batch Loss: 0.04259258508682251\n",
      "Epoch 3760, Loss: 0.225511372089386, Final Batch Loss: 0.05387575551867485\n",
      "Epoch 3761, Loss: 0.1936833057552576, Final Batch Loss: 0.03651734068989754\n",
      "Epoch 3762, Loss: 0.2242862954735756, Final Batch Loss: 0.023919709026813507\n",
      "Epoch 3763, Loss: 0.35197703912854195, Final Batch Loss: 0.11760834604501724\n",
      "Epoch 3764, Loss: 0.2760109715163708, Final Batch Loss: 0.05085849389433861\n",
      "Epoch 3765, Loss: 0.2880652956664562, Final Batch Loss: 0.07036305218935013\n",
      "Epoch 3766, Loss: 0.1825700020417571, Final Batch Loss: 0.013926888816058636\n",
      "Epoch 3767, Loss: 0.2559117376804352, Final Batch Loss: 0.07805289328098297\n",
      "Epoch 3768, Loss: 0.2715963199734688, Final Batch Loss: 0.12616975605487823\n",
      "Epoch 3769, Loss: 0.2773054149001837, Final Batch Loss: 0.10755615681409836\n",
      "Epoch 3770, Loss: 0.2106310985982418, Final Batch Loss: 0.062185414135456085\n",
      "Epoch 3771, Loss: 0.30493762716650963, Final Batch Loss: 0.059275638312101364\n",
      "Epoch 3772, Loss: 0.22485080733895302, Final Batch Loss: 0.056187089532613754\n",
      "Epoch 3773, Loss: 0.2112508313730359, Final Batch Loss: 0.01079829502850771\n",
      "Epoch 3774, Loss: 0.20665919035673141, Final Batch Loss: 0.06351413577795029\n",
      "Epoch 3775, Loss: 0.23989255353808403, Final Batch Loss: 0.05950254946947098\n",
      "Epoch 3776, Loss: 0.23444954119622707, Final Batch Loss: 0.10294370353221893\n",
      "Epoch 3777, Loss: 0.31514081731438637, Final Batch Loss: 0.10307388752698898\n",
      "Epoch 3778, Loss: 0.24306626990437508, Final Batch Loss: 0.023948658257722855\n",
      "Epoch 3779, Loss: 0.3038860149681568, Final Batch Loss: 0.08724813908338547\n",
      "Epoch 3780, Loss: 0.29094430431723595, Final Batch Loss: 0.07248236984014511\n",
      "Epoch 3781, Loss: 0.2616726644337177, Final Batch Loss: 0.04034668952226639\n",
      "Epoch 3782, Loss: 0.19618027098476887, Final Batch Loss: 0.02288065105676651\n",
      "Epoch 3783, Loss: 0.3396386653184891, Final Batch Loss: 0.07192674279212952\n",
      "Epoch 3784, Loss: 0.21949658915400505, Final Batch Loss: 0.02150515466928482\n",
      "Epoch 3785, Loss: 0.22547795251011848, Final Batch Loss: 0.07400960475206375\n",
      "Epoch 3786, Loss: 0.18798983469605446, Final Batch Loss: 0.045068006962537766\n",
      "Epoch 3787, Loss: 0.20989876426756382, Final Batch Loss: 0.0871412456035614\n",
      "Epoch 3788, Loss: 0.16781944502145052, Final Batch Loss: 0.015584041364490986\n",
      "Epoch 3789, Loss: 0.25301724672317505, Final Batch Loss: 0.012288738042116165\n",
      "Epoch 3790, Loss: 0.2490543294698, Final Batch Loss: 0.07727380096912384\n",
      "Epoch 3791, Loss: 0.20175335370004177, Final Batch Loss: 0.03911101073026657\n",
      "Epoch 3792, Loss: 0.18358851596713066, Final Batch Loss: 0.03232159465551376\n",
      "Epoch 3793, Loss: 0.17006360553205013, Final Batch Loss: 0.05530359596014023\n",
      "Epoch 3794, Loss: 0.17422299459576607, Final Batch Loss: 0.0654936134815216\n",
      "Epoch 3795, Loss: 0.16038836166262627, Final Batch Loss: 0.03499345853924751\n",
      "Epoch 3796, Loss: 0.12585936579853296, Final Batch Loss: 0.015006662346422672\n",
      "Epoch 3797, Loss: 0.2527346722781658, Final Batch Loss: 0.08119047433137894\n",
      "Epoch 3798, Loss: 0.24706177413463593, Final Batch Loss: 0.06492398679256439\n",
      "Epoch 3799, Loss: 0.24789440259337425, Final Batch Loss: 0.039763450622558594\n",
      "Epoch 3800, Loss: 0.23042047768831253, Final Batch Loss: 0.10088208317756653\n",
      "Epoch 3801, Loss: 0.1333390399813652, Final Batch Loss: 0.02419067919254303\n",
      "Epoch 3802, Loss: 0.18560288846492767, Final Batch Loss: 0.026339497417211533\n",
      "Epoch 3803, Loss: 0.27959489822387695, Final Batch Loss: 0.12209637463092804\n",
      "Epoch 3804, Loss: 0.19562644138932228, Final Batch Loss: 0.046124108135700226\n",
      "Epoch 3805, Loss: 0.3508932925760746, Final Batch Loss: 0.18929441273212433\n",
      "Epoch 3806, Loss: 0.18086450174450874, Final Batch Loss: 0.03417651727795601\n",
      "Epoch 3807, Loss: 0.3203805238008499, Final Batch Loss: 0.1109115406870842\n",
      "Epoch 3808, Loss: 0.3229844979941845, Final Batch Loss: 0.11901440471410751\n",
      "Epoch 3809, Loss: 0.23497701063752174, Final Batch Loss: 0.04871353134512901\n",
      "Epoch 3810, Loss: 0.21985851041972637, Final Batch Loss: 0.04732057452201843\n",
      "Epoch 3811, Loss: 0.2179775945842266, Final Batch Loss: 0.03372316434979439\n",
      "Epoch 3812, Loss: 0.28551432490348816, Final Batch Loss: 0.0725465938448906\n",
      "Epoch 3813, Loss: 0.1854376420378685, Final Batch Loss: 0.0636104866862297\n",
      "Epoch 3814, Loss: 0.1663658283650875, Final Batch Loss: 0.02919386327266693\n",
      "Epoch 3815, Loss: 0.2337546870112419, Final Batch Loss: 0.07668381929397583\n",
      "Epoch 3816, Loss: 0.19671538285911083, Final Batch Loss: 0.025319835171103477\n",
      "Epoch 3817, Loss: 0.1724427528679371, Final Batch Loss: 0.03769933804869652\n",
      "Epoch 3818, Loss: 0.17261356860399246, Final Batch Loss: 0.027269698679447174\n",
      "Epoch 3819, Loss: 0.21438445150852203, Final Batch Loss: 0.03332868590950966\n",
      "Epoch 3820, Loss: 0.2028084732592106, Final Batch Loss: 0.07390240579843521\n",
      "Epoch 3821, Loss: 0.4020134061574936, Final Batch Loss: 0.15720747411251068\n",
      "Epoch 3822, Loss: 0.2611238807439804, Final Batch Loss: 0.05940398946404457\n",
      "Epoch 3823, Loss: 0.2691016234457493, Final Batch Loss: 0.03969186544418335\n",
      "Epoch 3824, Loss: 0.19946662336587906, Final Batch Loss: 0.06582143902778625\n",
      "Epoch 3825, Loss: 0.21925878338515759, Final Batch Loss: 0.022257735952734947\n",
      "Epoch 3826, Loss: 0.24494966119527817, Final Batch Loss: 0.08802515268325806\n",
      "Epoch 3827, Loss: 0.26379337534308434, Final Batch Loss: 0.09675058722496033\n",
      "Epoch 3828, Loss: 0.3299448564648628, Final Batch Loss: 0.1159568652510643\n",
      "Epoch 3829, Loss: 0.25828414410352707, Final Batch Loss: 0.08554825186729431\n",
      "Epoch 3830, Loss: 0.1938950326293707, Final Batch Loss: 0.013869395479559898\n",
      "Epoch 3831, Loss: 0.23117006197571754, Final Batch Loss: 0.03663516417145729\n",
      "Epoch 3832, Loss: 0.1893368922173977, Final Batch Loss: 0.04024253785610199\n",
      "Epoch 3833, Loss: 0.22548629716038704, Final Batch Loss: 0.03747629374265671\n",
      "Epoch 3834, Loss: 0.21527350321412086, Final Batch Loss: 0.042061369866132736\n",
      "Epoch 3835, Loss: 0.2121281586587429, Final Batch Loss: 0.09239830821752548\n",
      "Epoch 3836, Loss: 0.26153507828712463, Final Batch Loss: 0.08027730882167816\n",
      "Epoch 3837, Loss: 0.206844724714756, Final Batch Loss: 0.03784998878836632\n",
      "Epoch 3838, Loss: 0.14409752376377583, Final Batch Loss: 0.032284535467624664\n",
      "Epoch 3839, Loss: 0.3096989393234253, Final Batch Loss: 0.17221775650978088\n",
      "Epoch 3840, Loss: 0.16799310222268105, Final Batch Loss: 0.03731805458664894\n",
      "Epoch 3841, Loss: 0.20316316047683358, Final Batch Loss: 0.006350858602672815\n",
      "Epoch 3842, Loss: 0.2372956033796072, Final Batch Loss: 0.08767591416835785\n",
      "Epoch 3843, Loss: 0.21662251092493534, Final Batch Loss: 0.02419624663889408\n",
      "Epoch 3844, Loss: 0.20882739126682281, Final Batch Loss: 0.015855897217988968\n",
      "Epoch 3845, Loss: 0.1987344827502966, Final Batch Loss: 0.07448045909404755\n",
      "Epoch 3846, Loss: 0.14354119263589382, Final Batch Loss: 0.009655306115746498\n",
      "Epoch 3847, Loss: 0.1932821311056614, Final Batch Loss: 0.043435174971818924\n",
      "Epoch 3848, Loss: 0.20980404689908028, Final Batch Loss: 0.03658531978726387\n",
      "Epoch 3849, Loss: 0.22920210659503937, Final Batch Loss: 0.04180137440562248\n",
      "Epoch 3850, Loss: 0.3737136162817478, Final Batch Loss: 0.09623996913433075\n",
      "Epoch 3851, Loss: 0.23384660482406616, Final Batch Loss: 0.09879529476165771\n",
      "Epoch 3852, Loss: 0.2540693059563637, Final Batch Loss: 0.039118219166994095\n",
      "Epoch 3853, Loss: 0.2793801501393318, Final Batch Loss: 0.03886706382036209\n",
      "Epoch 3854, Loss: 0.2329709269106388, Final Batch Loss: 0.06003938987851143\n",
      "Epoch 3855, Loss: 0.26417405903339386, Final Batch Loss: 0.0938858911395073\n",
      "Epoch 3856, Loss: 0.2313743606209755, Final Batch Loss: 0.13469156622886658\n",
      "Epoch 3857, Loss: 0.2436973750591278, Final Batch Loss: 0.05741216987371445\n",
      "Epoch 3858, Loss: 0.24686932563781738, Final Batch Loss: 0.05171498283743858\n",
      "Epoch 3859, Loss: 0.20137051865458488, Final Batch Loss: 0.05114254727959633\n",
      "Epoch 3860, Loss: 0.16484291572123766, Final Batch Loss: 0.01240535918623209\n",
      "Epoch 3861, Loss: 0.26969016902148724, Final Batch Loss: 0.02784540317952633\n",
      "Epoch 3862, Loss: 0.17761757597327232, Final Batch Loss: 0.06472786515951157\n",
      "Epoch 3863, Loss: 0.22418375499546528, Final Batch Loss: 0.0444345623254776\n",
      "Epoch 3864, Loss: 0.2344796024262905, Final Batch Loss: 0.06950140744447708\n",
      "Epoch 3865, Loss: 0.27773696184158325, Final Batch Loss: 0.11068571358919144\n",
      "Epoch 3866, Loss: 0.20866799727082253, Final Batch Loss: 0.04072516784071922\n",
      "Epoch 3867, Loss: 0.3208918161690235, Final Batch Loss: 0.04695716127753258\n",
      "Epoch 3868, Loss: 0.13154233805835247, Final Batch Loss: 0.03312184661626816\n",
      "Epoch 3869, Loss: 0.1900162547826767, Final Batch Loss: 0.03643333539366722\n",
      "Epoch 3870, Loss: 0.33135249093174934, Final Batch Loss: 0.18155965209007263\n",
      "Epoch 3871, Loss: 0.1492215171456337, Final Batch Loss: 0.029066357761621475\n",
      "Epoch 3872, Loss: 0.21151749603450298, Final Batch Loss: 0.018555110320448875\n",
      "Epoch 3873, Loss: 0.13847528584301472, Final Batch Loss: 0.01403004489839077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3874, Loss: 0.26770738884806633, Final Batch Loss: 0.03748707100749016\n",
      "Epoch 3875, Loss: 0.1816819254308939, Final Batch Loss: 0.010583290830254555\n",
      "Epoch 3876, Loss: 0.3858480006456375, Final Batch Loss: 0.23873655498027802\n",
      "Epoch 3877, Loss: 0.20840835198760033, Final Batch Loss: 0.02277929149568081\n",
      "Epoch 3878, Loss: 0.22612854838371277, Final Batch Loss: 0.07409936934709549\n",
      "Epoch 3879, Loss: 0.19868667051196098, Final Batch Loss: 0.0851299911737442\n",
      "Epoch 3880, Loss: 0.2322665974497795, Final Batch Loss: 0.02204028144478798\n",
      "Epoch 3881, Loss: 0.27688410971313715, Final Batch Loss: 0.01380509976297617\n",
      "Epoch 3882, Loss: 0.2441603448241949, Final Batch Loss: 0.08035628497600555\n",
      "Epoch 3883, Loss: 0.1688385121524334, Final Batch Loss: 0.03685150295495987\n",
      "Epoch 3884, Loss: 0.20540735870599747, Final Batch Loss: 0.03781213238835335\n",
      "Epoch 3885, Loss: 0.16086602956056595, Final Batch Loss: 0.024166662245988846\n",
      "Epoch 3886, Loss: 0.1693285033106804, Final Batch Loss: 0.03966028615832329\n",
      "Epoch 3887, Loss: 0.17491253092885017, Final Batch Loss: 0.02964859828352928\n",
      "Epoch 3888, Loss: 0.2710934206843376, Final Batch Loss: 0.10359946638345718\n",
      "Epoch 3889, Loss: 0.20643082074820995, Final Batch Loss: 0.022839127108454704\n",
      "Epoch 3890, Loss: 0.24826846085488796, Final Batch Loss: 0.11254838109016418\n",
      "Epoch 3891, Loss: 0.23638109862804413, Final Batch Loss: 0.051584478467702866\n",
      "Epoch 3892, Loss: 0.22742951288819313, Final Batch Loss: 0.06635797023773193\n",
      "Epoch 3893, Loss: 0.2189795821905136, Final Batch Loss: 0.08187789469957352\n",
      "Epoch 3894, Loss: 0.1967548280954361, Final Batch Loss: 0.0460677295923233\n",
      "Epoch 3895, Loss: 0.19456199277192354, Final Batch Loss: 0.013074991293251514\n",
      "Epoch 3896, Loss: 0.19565263204276562, Final Batch Loss: 0.0566006600856781\n",
      "Epoch 3897, Loss: 0.1899631954729557, Final Batch Loss: 0.024806641042232513\n",
      "Epoch 3898, Loss: 0.1794179305434227, Final Batch Loss: 0.02198081463575363\n",
      "Epoch 3899, Loss: 0.20858604088425636, Final Batch Loss: 0.07341642677783966\n",
      "Epoch 3900, Loss: 0.1943499967455864, Final Batch Loss: 0.08903631567955017\n",
      "Epoch 3901, Loss: 0.28380340337753296, Final Batch Loss: 0.09872078150510788\n",
      "Epoch 3902, Loss: 0.29365550726652145, Final Batch Loss: 0.06471988558769226\n",
      "Epoch 3903, Loss: 0.26421522721648216, Final Batch Loss: 0.09746275097131729\n",
      "Epoch 3904, Loss: 0.2895776182413101, Final Batch Loss: 0.08850181847810745\n",
      "Epoch 3905, Loss: 0.2804553732275963, Final Batch Loss: 0.10114136338233948\n",
      "Epoch 3906, Loss: 0.3035830073058605, Final Batch Loss: 0.06793961673974991\n",
      "Epoch 3907, Loss: 0.2702040523290634, Final Batch Loss: 0.0689961239695549\n",
      "Epoch 3908, Loss: 0.2518262080848217, Final Batch Loss: 0.0937967523932457\n",
      "Epoch 3909, Loss: 0.20375441759824753, Final Batch Loss: 0.027366388589143753\n",
      "Epoch 3910, Loss: 0.32161134481430054, Final Batch Loss: 0.0552535280585289\n",
      "Epoch 3911, Loss: 0.33750126883387566, Final Batch Loss: 0.10897126793861389\n",
      "Epoch 3912, Loss: 0.3176809325814247, Final Batch Loss: 0.028327643871307373\n",
      "Epoch 3913, Loss: 0.16001339815557003, Final Batch Loss: 0.016042226925492287\n",
      "Epoch 3914, Loss: 0.2845293693244457, Final Batch Loss: 0.09729790687561035\n",
      "Epoch 3915, Loss: 0.28306445106863976, Final Batch Loss: 0.055504459887742996\n",
      "Epoch 3916, Loss: 0.19439801201224327, Final Batch Loss: 0.0331362709403038\n",
      "Epoch 3917, Loss: 0.28447218611836433, Final Batch Loss: 0.07212124019861221\n",
      "Epoch 3918, Loss: 0.23587925732135773, Final Batch Loss: 0.07614976912736893\n",
      "Epoch 3919, Loss: 0.21398425847291946, Final Batch Loss: 0.03697742521762848\n",
      "Epoch 3920, Loss: 0.20251361653208733, Final Batch Loss: 0.035121768712997437\n",
      "Epoch 3921, Loss: 0.1856196764856577, Final Batch Loss: 0.019289398565888405\n",
      "Epoch 3922, Loss: 0.22159165143966675, Final Batch Loss: 0.08319217711687088\n",
      "Epoch 3923, Loss: 0.21816936880350113, Final Batch Loss: 0.06971793621778488\n",
      "Epoch 3924, Loss: 0.14577927999198437, Final Batch Loss: 0.020631490275263786\n",
      "Epoch 3925, Loss: 0.21757423132658005, Final Batch Loss: 0.013695791363716125\n",
      "Epoch 3926, Loss: 0.2838760130107403, Final Batch Loss: 0.09945598989725113\n",
      "Epoch 3927, Loss: 0.31229112297296524, Final Batch Loss: 0.06947000324726105\n",
      "Epoch 3928, Loss: 0.16631406918168068, Final Batch Loss: 0.03154149651527405\n",
      "Epoch 3929, Loss: 0.1996324546635151, Final Batch Loss: 0.029969248920679092\n",
      "Epoch 3930, Loss: 0.28763094171881676, Final Batch Loss: 0.10928114503622055\n",
      "Epoch 3931, Loss: 0.18997418694198132, Final Batch Loss: 0.053603071719408035\n",
      "Epoch 3932, Loss: 0.22431737929582596, Final Batch Loss: 0.050362084060907364\n",
      "Epoch 3933, Loss: 0.2773437649011612, Final Batch Loss: 0.03331419825553894\n",
      "Epoch 3934, Loss: 0.22494100779294968, Final Batch Loss: 0.030775927007198334\n",
      "Epoch 3935, Loss: 0.20062339305877686, Final Batch Loss: 0.054174307733774185\n",
      "Epoch 3936, Loss: 0.1674262396991253, Final Batch Loss: 0.025580521672964096\n",
      "Epoch 3937, Loss: 0.16439013183116913, Final Batch Loss: 0.021825164556503296\n",
      "Epoch 3938, Loss: 0.2045772783458233, Final Batch Loss: 0.022378455847501755\n",
      "Epoch 3939, Loss: 0.19971011392772198, Final Batch Loss: 0.040141090750694275\n",
      "Epoch 3940, Loss: 0.25391799956560135, Final Batch Loss: 0.07766758650541306\n",
      "Epoch 3941, Loss: 0.2309933304786682, Final Batch Loss: 0.04657537862658501\n",
      "Epoch 3942, Loss: 0.2649048175662756, Final Batch Loss: 0.020682645961642265\n",
      "Epoch 3943, Loss: 0.17950178496539593, Final Batch Loss: 0.01921813003718853\n",
      "Epoch 3944, Loss: 0.23104673065245152, Final Batch Loss: 0.039510805159807205\n",
      "Epoch 3945, Loss: 0.18235358223319054, Final Batch Loss: 0.04539622738957405\n",
      "Epoch 3946, Loss: 0.2433284856379032, Final Batch Loss: 0.07663390785455704\n",
      "Epoch 3947, Loss: 0.2744719460606575, Final Batch Loss: 0.08226833492517471\n",
      "Epoch 3948, Loss: 0.2942056581377983, Final Batch Loss: 0.09559717029333115\n",
      "Epoch 3949, Loss: 0.21664851903915405, Final Batch Loss: 0.01961418241262436\n",
      "Epoch 3950, Loss: 0.2782344948500395, Final Batch Loss: 0.028593024238944054\n",
      "Epoch 3951, Loss: 0.19238805398344994, Final Batch Loss: 0.04049764201045036\n",
      "Epoch 3952, Loss: 0.21593431383371353, Final Batch Loss: 0.028654254972934723\n",
      "Epoch 3953, Loss: 0.23727038502693176, Final Batch Loss: 0.06351180374622345\n",
      "Epoch 3954, Loss: 0.189173124730587, Final Batch Loss: 0.02821190282702446\n",
      "Epoch 3955, Loss: 0.26577144861221313, Final Batch Loss: 0.04745284095406532\n",
      "Epoch 3956, Loss: 0.1651591844856739, Final Batch Loss: 0.022824332118034363\n",
      "Epoch 3957, Loss: 0.1677090171724558, Final Batch Loss: 0.03576609864830971\n",
      "Epoch 3958, Loss: 0.18747016228735447, Final Batch Loss: 0.03632586821913719\n",
      "Epoch 3959, Loss: 0.25346381962299347, Final Batch Loss: 0.07883735001087189\n",
      "Epoch 3960, Loss: 0.24549441784620285, Final Batch Loss: 0.08845502138137817\n",
      "Epoch 3961, Loss: 0.14668521843850613, Final Batch Loss: 0.019017988815903664\n",
      "Epoch 3962, Loss: 0.31264440529048443, Final Batch Loss: 0.1340368390083313\n",
      "Epoch 3963, Loss: 0.17345713824033737, Final Batch Loss: 0.02426227368414402\n",
      "Epoch 3964, Loss: 0.25372980162501335, Final Batch Loss: 0.053738079965114594\n",
      "Epoch 3965, Loss: 0.17172984965145588, Final Batch Loss: 0.01740393228828907\n",
      "Epoch 3966, Loss: 0.2754148729145527, Final Batch Loss: 0.04028183966875076\n",
      "Epoch 3967, Loss: 0.2235819585621357, Final Batch Loss: 0.03874504566192627\n",
      "Epoch 3968, Loss: 0.162015737965703, Final Batch Loss: 0.01366693526506424\n",
      "Epoch 3969, Loss: 0.30067626386880875, Final Batch Loss: 0.042035285383462906\n",
      "Epoch 3970, Loss: 0.2805217541754246, Final Batch Loss: 0.08267417550086975\n",
      "Epoch 3971, Loss: 0.16309897974133492, Final Batch Loss: 0.04800400510430336\n",
      "Epoch 3972, Loss: 0.3295287564396858, Final Batch Loss: 0.12258052825927734\n",
      "Epoch 3973, Loss: 0.21067785285413265, Final Batch Loss: 0.09276401251554489\n",
      "Epoch 3974, Loss: 0.24955422431230545, Final Batch Loss: 0.055413298308849335\n",
      "Epoch 3975, Loss: 0.1869872733950615, Final Batch Loss: 0.02168808877468109\n",
      "Epoch 3976, Loss: 0.21515241265296936, Final Batch Loss: 0.08322367072105408\n",
      "Epoch 3977, Loss: 0.16582798585295677, Final Batch Loss: 0.012055318802595139\n",
      "Epoch 3978, Loss: 0.19521979056298733, Final Batch Loss: 0.030814776197075844\n",
      "Epoch 3979, Loss: 0.22698041796684265, Final Batch Loss: 0.017717812210321426\n",
      "Epoch 3980, Loss: 0.2209345754235983, Final Batch Loss: 0.029271068051457405\n",
      "Epoch 3981, Loss: 0.1603365894407034, Final Batch Loss: 0.05830242857336998\n",
      "Epoch 3982, Loss: 0.2679188400506973, Final Batch Loss: 0.08730294555425644\n",
      "Epoch 3983, Loss: 0.2670251838862896, Final Batch Loss: 0.03750939294695854\n",
      "Epoch 3984, Loss: 0.24544193595647812, Final Batch Loss: 0.09071750193834305\n",
      "Epoch 3985, Loss: 0.20424357987940311, Final Batch Loss: 0.02168726548552513\n",
      "Epoch 3986, Loss: 0.26737267058342695, Final Batch Loss: 0.011452040635049343\n",
      "Epoch 3987, Loss: 0.17682042717933655, Final Batch Loss: 0.032543037086725235\n",
      "Epoch 3988, Loss: 0.2608248386532068, Final Batch Loss: 0.10650943219661713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3989, Loss: 0.24345949105918407, Final Batch Loss: 0.11354755610227585\n",
      "Epoch 3990, Loss: 0.1888238489627838, Final Batch Loss: 0.022268932312726974\n",
      "Epoch 3991, Loss: 0.2787537947297096, Final Batch Loss: 0.03664169833064079\n",
      "Epoch 3992, Loss: 0.2572852410376072, Final Batch Loss: 0.06451606005430222\n",
      "Epoch 3993, Loss: 0.23175745271146297, Final Batch Loss: 0.020044678822159767\n",
      "Epoch 3994, Loss: 0.30192745476961136, Final Batch Loss: 0.07696893811225891\n",
      "Epoch 3995, Loss: 0.25249611400067806, Final Batch Loss: 0.03637823835015297\n",
      "Epoch 3996, Loss: 0.2223256751894951, Final Batch Loss: 0.06665010005235672\n",
      "Epoch 3997, Loss: 0.18068919889628887, Final Batch Loss: 0.019366135820746422\n",
      "Epoch 3998, Loss: 0.25363384187221527, Final Batch Loss: 0.06483758985996246\n",
      "Epoch 3999, Loss: 0.27280258014798164, Final Batch Loss: 0.04607381299138069\n",
      "Epoch 4000, Loss: 0.25016700103878975, Final Batch Loss: 0.08078981935977936\n",
      "Epoch 4001, Loss: 0.3337073400616646, Final Batch Loss: 0.1330394595861435\n",
      "Epoch 4002, Loss: 0.2790484894067049, Final Batch Loss: 0.08141352236270905\n",
      "Epoch 4003, Loss: 0.22960948944091797, Final Batch Loss: 0.04394106939435005\n",
      "Epoch 4004, Loss: 0.23968633636832237, Final Batch Loss: 0.053477220237255096\n",
      "Epoch 4005, Loss: 0.2411991823464632, Final Batch Loss: 0.02629639022052288\n",
      "Epoch 4006, Loss: 0.15741297416388988, Final Batch Loss: 0.0531865619122982\n",
      "Epoch 4007, Loss: 0.23903531953692436, Final Batch Loss: 0.057214025408029556\n",
      "Epoch 4008, Loss: 0.2714984379708767, Final Batch Loss: 0.06905786693096161\n",
      "Epoch 4009, Loss: 0.36473962292075157, Final Batch Loss: 0.17304584383964539\n",
      "Epoch 4010, Loss: 0.21830715611577034, Final Batch Loss: 0.08668120205402374\n",
      "Epoch 4011, Loss: 0.191337201744318, Final Batch Loss: 0.031799525022506714\n",
      "Epoch 4012, Loss: 0.20161275751888752, Final Batch Loss: 0.05559480935335159\n",
      "Epoch 4013, Loss: 0.2667572796344757, Final Batch Loss: 0.022445078939199448\n",
      "Epoch 4014, Loss: 0.19453378207981586, Final Batch Loss: 0.04216459020972252\n",
      "Epoch 4015, Loss: 0.19467408955097198, Final Batch Loss: 0.038403235375881195\n",
      "Epoch 4016, Loss: 0.2454097904264927, Final Batch Loss: 0.0803615003824234\n",
      "Epoch 4017, Loss: 0.19646744802594185, Final Batch Loss: 0.04741508513689041\n",
      "Epoch 4018, Loss: 0.2159024104475975, Final Batch Loss: 0.035647664219141006\n",
      "Epoch 4019, Loss: 0.23216751962900162, Final Batch Loss: 0.03757614642381668\n",
      "Epoch 4020, Loss: 0.25106525607407093, Final Batch Loss: 0.09628702700138092\n",
      "Epoch 4021, Loss: 0.23314739763736725, Final Batch Loss: 0.097480408847332\n",
      "Epoch 4022, Loss: 0.21730949357151985, Final Batch Loss: 0.08142392337322235\n",
      "Epoch 4023, Loss: 0.18680238537490368, Final Batch Loss: 0.021741939708590508\n",
      "Epoch 4024, Loss: 0.31079358607530594, Final Batch Loss: 0.06421075016260147\n",
      "Epoch 4025, Loss: 0.22235727682709694, Final Batch Loss: 0.0367954783141613\n",
      "Epoch 4026, Loss: 0.15802092477679253, Final Batch Loss: 0.03132551908493042\n",
      "Epoch 4027, Loss: 0.28513156436383724, Final Batch Loss: 0.026646962389349937\n",
      "Epoch 4028, Loss: 0.23979470506310463, Final Batch Loss: 0.06164196506142616\n",
      "Epoch 4029, Loss: 0.24379845708608627, Final Batch Loss: 0.053535472601652145\n",
      "Epoch 4030, Loss: 0.2612243853509426, Final Batch Loss: 0.06282979249954224\n",
      "Epoch 4031, Loss: 0.2111354060471058, Final Batch Loss: 0.0542309507727623\n",
      "Epoch 4032, Loss: 0.16279260255396366, Final Batch Loss: 0.027342038229107857\n",
      "Epoch 4033, Loss: 0.17188797518610954, Final Batch Loss: 0.05684608593583107\n",
      "Epoch 4034, Loss: 0.13071496225893497, Final Batch Loss: 0.03838953375816345\n",
      "Epoch 4035, Loss: 0.2588190697133541, Final Batch Loss: 0.051611024886369705\n",
      "Epoch 4036, Loss: 0.23514194041490555, Final Batch Loss: 0.05480062961578369\n",
      "Epoch 4037, Loss: 0.2224681545048952, Final Batch Loss: 0.05122584477066994\n",
      "Epoch 4038, Loss: 0.17866175435483456, Final Batch Loss: 0.024093350395560265\n",
      "Epoch 4039, Loss: 0.256436113268137, Final Batch Loss: 0.06814171373844147\n",
      "Epoch 4040, Loss: 0.2618752382695675, Final Batch Loss: 0.016882192343473434\n",
      "Epoch 4041, Loss: 0.18355254642665386, Final Batch Loss: 0.02040615864098072\n",
      "Epoch 4042, Loss: 0.1927447458729148, Final Batch Loss: 0.011800429783761501\n",
      "Epoch 4043, Loss: 0.18007156997919083, Final Batch Loss: 0.05484506115317345\n",
      "Epoch 4044, Loss: 0.24485239014029503, Final Batch Loss: 0.07339481264352798\n",
      "Epoch 4045, Loss: 0.28159579262137413, Final Batch Loss: 0.09728556126356125\n",
      "Epoch 4046, Loss: 0.21274005807936192, Final Batch Loss: 0.07568525522947311\n",
      "Epoch 4047, Loss: 0.17711308598518372, Final Batch Loss: 0.02738163247704506\n",
      "Epoch 4048, Loss: 0.29763732850551605, Final Batch Loss: 0.0853278636932373\n",
      "Epoch 4049, Loss: 0.1552793886512518, Final Batch Loss: 0.022837143391370773\n",
      "Epoch 4050, Loss: 0.24845785275101662, Final Batch Loss: 0.10376296937465668\n",
      "Epoch 4051, Loss: 0.22693536430597305, Final Batch Loss: 0.025783095508813858\n",
      "Epoch 4052, Loss: 0.18867744132876396, Final Batch Loss: 0.04537692293524742\n",
      "Epoch 4053, Loss: 0.13625723123550415, Final Batch Loss: 0.017939649522304535\n",
      "Epoch 4054, Loss: 0.33336394280195236, Final Batch Loss: 0.09378340095281601\n",
      "Epoch 4055, Loss: 0.3197070583701134, Final Batch Loss: 0.11336594074964523\n",
      "Epoch 4056, Loss: 0.201926214620471, Final Batch Loss: 0.030043968930840492\n",
      "Epoch 4057, Loss: 0.333430502563715, Final Batch Loss: 0.08867114782333374\n",
      "Epoch 4058, Loss: 0.28616344183683395, Final Batch Loss: 0.09999842941761017\n",
      "Epoch 4059, Loss: 0.20233528316020966, Final Batch Loss: 0.035516150295734406\n",
      "Epoch 4060, Loss: 0.290926530957222, Final Batch Loss: 0.07420963793992996\n",
      "Epoch 4061, Loss: 0.22872972302138805, Final Batch Loss: 0.05227014049887657\n",
      "Epoch 4062, Loss: 0.275896567851305, Final Batch Loss: 0.03885452076792717\n",
      "Epoch 4063, Loss: 0.20679254829883575, Final Batch Loss: 0.05953023582696915\n",
      "Epoch 4064, Loss: 0.3057473860681057, Final Batch Loss: 0.058347318321466446\n",
      "Epoch 4065, Loss: 0.293951153755188, Final Batch Loss: 0.12522409856319427\n",
      "Epoch 4066, Loss: 0.30102608539164066, Final Batch Loss: 0.10449963063001633\n",
      "Epoch 4067, Loss: 0.3013453856110573, Final Batch Loss: 0.08524370938539505\n",
      "Epoch 4068, Loss: 0.271566029638052, Final Batch Loss: 0.04410180822014809\n",
      "Epoch 4069, Loss: 0.1912303976714611, Final Batch Loss: 0.022394798696041107\n",
      "Epoch 4070, Loss: 0.20305142179131508, Final Batch Loss: 0.04112803563475609\n",
      "Epoch 4071, Loss: 0.20702985115349293, Final Batch Loss: 0.018575219437479973\n",
      "Epoch 4072, Loss: 0.1477487813681364, Final Batch Loss: 0.038528405129909515\n",
      "Epoch 4073, Loss: 0.2260796632617712, Final Batch Loss: 0.016673216596245766\n",
      "Epoch 4074, Loss: 0.1946649979799986, Final Batch Loss: 0.026313582435250282\n",
      "Epoch 4075, Loss: 0.1625782661139965, Final Batch Loss: 0.03662360459566116\n",
      "Epoch 4076, Loss: 0.2791270799934864, Final Batch Loss: 0.09034968912601471\n",
      "Epoch 4077, Loss: 0.22650228440761566, Final Batch Loss: 0.03512994945049286\n",
      "Epoch 4078, Loss: 0.1808340810239315, Final Batch Loss: 0.046474892646074295\n",
      "Epoch 4079, Loss: 0.25043439120054245, Final Batch Loss: 0.036880478262901306\n",
      "Epoch 4080, Loss: 0.2336658574640751, Final Batch Loss: 0.06602359563112259\n",
      "Epoch 4081, Loss: 0.18792136758565903, Final Batch Loss: 0.0445835180580616\n",
      "Epoch 4082, Loss: 0.1879318170249462, Final Batch Loss: 0.0478387251496315\n",
      "Epoch 4083, Loss: 0.2718598283827305, Final Batch Loss: 0.04442306235432625\n",
      "Epoch 4084, Loss: 0.2598501369357109, Final Batch Loss: 0.038123998790979385\n",
      "Epoch 4085, Loss: 0.22537347860634327, Final Batch Loss: 0.06755559891462326\n",
      "Epoch 4086, Loss: 0.19678442180156708, Final Batch Loss: 0.036007821559906006\n",
      "Epoch 4087, Loss: 0.30421503633260727, Final Batch Loss: 0.13338114321231842\n",
      "Epoch 4088, Loss: 0.22042149864137173, Final Batch Loss: 0.039482906460762024\n",
      "Epoch 4089, Loss: 0.25787805393338203, Final Batch Loss: 0.05432102084159851\n",
      "Epoch 4090, Loss: 0.17585414275527, Final Batch Loss: 0.012793263420462608\n",
      "Epoch 4091, Loss: 0.18455611169338226, Final Batch Loss: 0.045000188052654266\n",
      "Epoch 4092, Loss: 0.13991254894062877, Final Batch Loss: 0.002942956518381834\n",
      "Epoch 4093, Loss: 0.1684071160852909, Final Batch Loss: 0.04166794195771217\n",
      "Epoch 4094, Loss: 0.18359196558594704, Final Batch Loss: 0.06781774759292603\n",
      "Epoch 4095, Loss: 0.1788485422730446, Final Batch Loss: 0.026252636685967445\n",
      "Epoch 4096, Loss: 0.23309830762445927, Final Batch Loss: 0.07129194587469101\n",
      "Epoch 4097, Loss: 0.12077316828072071, Final Batch Loss: 0.02867933176457882\n",
      "Epoch 4098, Loss: 0.32759709656238556, Final Batch Loss: 0.08493411540985107\n",
      "Epoch 4099, Loss: 0.30919139459729195, Final Batch Loss: 0.04279298707842827\n",
      "Epoch 4100, Loss: 0.2455007378011942, Final Batch Loss: 0.05769281089305878\n",
      "Epoch 4101, Loss: 0.21012869849801064, Final Batch Loss: 0.033098939806222916\n",
      "Epoch 4102, Loss: 0.26107364147901535, Final Batch Loss: 0.040017884224653244\n",
      "Epoch 4103, Loss: 0.1913687102496624, Final Batch Loss: 0.04438653960824013\n",
      "Epoch 4104, Loss: 0.18910108506679535, Final Batch Loss: 0.028606902807950974\n",
      "Epoch 4105, Loss: 0.1920832134783268, Final Batch Loss: 0.05710887536406517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4106, Loss: 0.2770504355430603, Final Batch Loss: 0.12430062890052795\n",
      "Epoch 4107, Loss: 0.21177107095718384, Final Batch Loss: 0.02457522228360176\n",
      "Epoch 4108, Loss: 0.3031410425901413, Final Batch Loss: 0.07703471928834915\n",
      "Epoch 4109, Loss: 0.24411420337855816, Final Batch Loss: 0.09206687659025192\n",
      "Epoch 4110, Loss: 0.2548675462603569, Final Batch Loss: 0.01935570314526558\n",
      "Epoch 4111, Loss: 0.26887842267751694, Final Batch Loss: 0.09703927487134933\n",
      "Epoch 4112, Loss: 0.34396199882030487, Final Batch Loss: 0.06136868894100189\n",
      "Epoch 4113, Loss: 0.36671939492225647, Final Batch Loss: 0.08916410803794861\n",
      "Epoch 4114, Loss: 0.23248504102230072, Final Batch Loss: 0.049914564937353134\n",
      "Epoch 4115, Loss: 0.21774659678339958, Final Batch Loss: 0.04197315871715546\n",
      "Epoch 4116, Loss: 0.32639604061841965, Final Batch Loss: 0.13704724609851837\n",
      "Epoch 4117, Loss: 0.27138475701212883, Final Batch Loss: 0.07128111273050308\n",
      "Epoch 4118, Loss: 0.19656402431428432, Final Batch Loss: 0.026439903303980827\n",
      "Epoch 4119, Loss: 0.24955252930521965, Final Batch Loss: 0.06066824868321419\n",
      "Epoch 4120, Loss: 0.24479269981384277, Final Batch Loss: 0.04537137970328331\n",
      "Epoch 4121, Loss: 0.1875324659049511, Final Batch Loss: 0.026722446084022522\n",
      "Epoch 4122, Loss: 0.29101792722940445, Final Batch Loss: 0.10012471675872803\n",
      "Epoch 4123, Loss: 0.21523061767220497, Final Batch Loss: 0.050044745206832886\n",
      "Epoch 4124, Loss: 0.21414067596197128, Final Batch Loss: 0.08306997269392014\n",
      "Epoch 4125, Loss: 0.20856092311441898, Final Batch Loss: 0.028929227963089943\n",
      "Epoch 4126, Loss: 0.24056978151202202, Final Batch Loss: 0.06372518092393875\n",
      "Epoch 4127, Loss: 0.2859666980803013, Final Batch Loss: 0.04446600377559662\n",
      "Epoch 4128, Loss: 0.19074975326657295, Final Batch Loss: 0.07343722134828568\n",
      "Epoch 4129, Loss: 0.16173343919217587, Final Batch Loss: 0.01993432454764843\n",
      "Epoch 4130, Loss: 0.2089709062129259, Final Batch Loss: 0.08119179308414459\n",
      "Epoch 4131, Loss: 0.1956432592123747, Final Batch Loss: 0.02374553494155407\n",
      "Epoch 4132, Loss: 0.22533509880304337, Final Batch Loss: 0.04375224933028221\n",
      "Epoch 4133, Loss: 0.2214499581605196, Final Batch Loss: 0.05537177249789238\n",
      "Epoch 4134, Loss: 0.330212339758873, Final Batch Loss: 0.20250432193279266\n",
      "Epoch 4135, Loss: 0.18913442082703114, Final Batch Loss: 0.036135997623205185\n",
      "Epoch 4136, Loss: 0.29617478139698505, Final Batch Loss: 0.15582309663295746\n",
      "Epoch 4137, Loss: 0.2148597165942192, Final Batch Loss: 0.05339265242218971\n",
      "Epoch 4138, Loss: 0.2192104198038578, Final Batch Loss: 0.041887734085321426\n",
      "Epoch 4139, Loss: 0.20315588265657425, Final Batch Loss: 0.035378292202949524\n",
      "Epoch 4140, Loss: 0.2303260825574398, Final Batch Loss: 0.03936611860990524\n",
      "Epoch 4141, Loss: 0.22605203464627266, Final Batch Loss: 0.07464165985584259\n",
      "Epoch 4142, Loss: 0.2308916114270687, Final Batch Loss: 0.07980241626501083\n",
      "Epoch 4143, Loss: 0.2698498088866472, Final Batch Loss: 0.09818129241466522\n",
      "Epoch 4144, Loss: 0.24304162338376045, Final Batch Loss: 0.047255389392375946\n",
      "Epoch 4145, Loss: 0.2685966547578573, Final Batch Loss: 0.1518154740333557\n",
      "Epoch 4146, Loss: 0.3822921887040138, Final Batch Loss: 0.09115520119667053\n",
      "Epoch 4147, Loss: 0.3001057244837284, Final Batch Loss: 0.06248698756098747\n",
      "Epoch 4148, Loss: 0.2979331575334072, Final Batch Loss: 0.096683070063591\n",
      "Epoch 4149, Loss: 0.27813419699668884, Final Batch Loss: 0.06316350400447845\n",
      "Epoch 4150, Loss: 0.37508293241262436, Final Batch Loss: 0.1960235834121704\n",
      "Epoch 4151, Loss: 0.2594451941549778, Final Batch Loss: 0.05943123996257782\n",
      "Epoch 4152, Loss: 0.292484313249588, Final Batch Loss: 0.07944649457931519\n",
      "Epoch 4153, Loss: 0.3941810801625252, Final Batch Loss: 0.12165994197130203\n",
      "Epoch 4154, Loss: 0.24770725518465042, Final Batch Loss: 0.05244123190641403\n",
      "Epoch 4155, Loss: 0.23648427054286003, Final Batch Loss: 0.04342753440141678\n",
      "Epoch 4156, Loss: 0.3065682128071785, Final Batch Loss: 0.04720599949359894\n",
      "Epoch 4157, Loss: 0.25158777832984924, Final Batch Loss: 0.060933563858270645\n",
      "Epoch 4158, Loss: 0.20363458804786205, Final Batch Loss: 0.02396574430167675\n",
      "Epoch 4159, Loss: 0.20278886891901493, Final Batch Loss: 0.028009755536913872\n",
      "Epoch 4160, Loss: 0.28912781551480293, Final Batch Loss: 0.038709886372089386\n",
      "Epoch 4161, Loss: 0.1788655575364828, Final Batch Loss: 0.01901998557150364\n",
      "Epoch 4162, Loss: 0.2058006376028061, Final Batch Loss: 0.034990448504686356\n",
      "Epoch 4163, Loss: 0.20392022095620632, Final Batch Loss: 0.015724970027804375\n",
      "Epoch 4164, Loss: 0.2089136466383934, Final Batch Loss: 0.0321250855922699\n",
      "Epoch 4165, Loss: 0.2674499601125717, Final Batch Loss: 0.1266627013683319\n",
      "Epoch 4166, Loss: 0.1274919379502535, Final Batch Loss: 0.016067640855908394\n",
      "Epoch 4167, Loss: 0.20473077334463596, Final Batch Loss: 0.030785178765654564\n",
      "Epoch 4168, Loss: 0.16093461215496063, Final Batch Loss: 0.04170862212777138\n",
      "Epoch 4169, Loss: 0.26722920686006546, Final Batch Loss: 0.04329084977507591\n",
      "Epoch 4170, Loss: 0.16446930170059204, Final Batch Loss: 0.03125520423054695\n",
      "Epoch 4171, Loss: 0.15713375434279442, Final Batch Loss: 0.03471041098237038\n",
      "Epoch 4172, Loss: 0.29118306189775467, Final Batch Loss: 0.07714801281690598\n",
      "Epoch 4173, Loss: 0.1893288940191269, Final Batch Loss: 0.05272746831178665\n",
      "Epoch 4174, Loss: 0.20065562799572945, Final Batch Loss: 0.05128727853298187\n",
      "Epoch 4175, Loss: 0.24880259484052658, Final Batch Loss: 0.13504093885421753\n",
      "Epoch 4176, Loss: 0.20455007255077362, Final Batch Loss: 0.03627825155854225\n",
      "Epoch 4177, Loss: 0.1588974166661501, Final Batch Loss: 0.027034757658839226\n",
      "Epoch 4178, Loss: 0.20961529202759266, Final Batch Loss: 0.057245198637247086\n",
      "Epoch 4179, Loss: 0.19433333538472652, Final Batch Loss: 0.018161414191126823\n",
      "Epoch 4180, Loss: 0.22686699219048023, Final Batch Loss: 0.08868114650249481\n",
      "Epoch 4181, Loss: 0.21543998084962368, Final Batch Loss: 0.06723976880311966\n",
      "Epoch 4182, Loss: 0.20066707395017147, Final Batch Loss: 0.01780666969716549\n",
      "Epoch 4183, Loss: 0.21281473338603973, Final Batch Loss: 0.04461224377155304\n",
      "Epoch 4184, Loss: 0.27963380329310894, Final Batch Loss: 0.12079079449176788\n",
      "Epoch 4185, Loss: 0.2056888323277235, Final Batch Loss: 0.025088617578148842\n",
      "Epoch 4186, Loss: 0.15418509393930435, Final Batch Loss: 0.039959125220775604\n",
      "Epoch 4187, Loss: 0.12992569338530302, Final Batch Loss: 0.011793256737291813\n",
      "Epoch 4188, Loss: 0.16420971788465977, Final Batch Loss: 0.01828678511083126\n",
      "Epoch 4189, Loss: 0.24954572319984436, Final Batch Loss: 0.03572792187333107\n",
      "Epoch 4190, Loss: 0.17799591086804867, Final Batch Loss: 0.02046819217503071\n",
      "Epoch 4191, Loss: 0.18420232273638248, Final Batch Loss: 0.025484303012490273\n",
      "Epoch 4192, Loss: 0.2569737397134304, Final Batch Loss: 0.09110230952501297\n",
      "Epoch 4193, Loss: 0.18278139177709818, Final Batch Loss: 0.009730887599289417\n",
      "Epoch 4194, Loss: 0.22119402885437012, Final Batch Loss: 0.034657493233680725\n",
      "Epoch 4195, Loss: 0.2091258754953742, Final Batch Loss: 0.014367057941854\n",
      "Epoch 4196, Loss: 0.1847077403217554, Final Batch Loss: 0.052777163684368134\n",
      "Epoch 4197, Loss: 0.21406419202685356, Final Batch Loss: 0.05439968779683113\n",
      "Epoch 4198, Loss: 0.19176985695958138, Final Batch Loss: 0.03681689128279686\n",
      "Epoch 4199, Loss: 0.21189596876502037, Final Batch Loss: 0.04967368766665459\n",
      "Epoch 4200, Loss: 0.18083255738019943, Final Batch Loss: 0.03377784788608551\n",
      "Epoch 4201, Loss: 0.16988416761159897, Final Batch Loss: 0.023986009880900383\n",
      "Epoch 4202, Loss: 0.2701597735285759, Final Batch Loss: 0.09060346335172653\n",
      "Epoch 4203, Loss: 0.2779245600104332, Final Batch Loss: 0.11354683339595795\n",
      "Epoch 4204, Loss: 0.14429759047925472, Final Batch Loss: 0.02069113589823246\n",
      "Epoch 4205, Loss: 0.189616397023201, Final Batch Loss: 0.05993346869945526\n",
      "Epoch 4206, Loss: 0.32570070400834084, Final Batch Loss: 0.13778823614120483\n",
      "Epoch 4207, Loss: 0.24922310933470726, Final Batch Loss: 0.026543714106082916\n",
      "Epoch 4208, Loss: 0.2384883677586913, Final Batch Loss: 0.013588867150247097\n",
      "Epoch 4209, Loss: 0.20671160519123077, Final Batch Loss: 0.020651035010814667\n",
      "Epoch 4210, Loss: 0.26423119148239493, Final Batch Loss: 0.00671375310048461\n",
      "Epoch 4211, Loss: 0.25268611684441566, Final Batch Loss: 0.0525122806429863\n",
      "Epoch 4212, Loss: 0.28026843443512917, Final Batch Loss: 0.12836004793643951\n",
      "Epoch 4213, Loss: 0.15811733901500702, Final Batch Loss: 0.048924896866083145\n",
      "Epoch 4214, Loss: 0.23421110212802887, Final Batch Loss: 0.03965704143047333\n",
      "Epoch 4215, Loss: 0.23935170471668243, Final Batch Loss: 0.08442112803459167\n",
      "Epoch 4216, Loss: 0.21734200045466423, Final Batch Loss: 0.07439594715833664\n",
      "Epoch 4217, Loss: 0.17396180517971516, Final Batch Loss: 0.02094450779259205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4218, Loss: 0.17203727550804615, Final Batch Loss: 0.028241703286767006\n",
      "Epoch 4219, Loss: 0.25448907911777496, Final Batch Loss: 0.042465146631002426\n",
      "Epoch 4220, Loss: 0.23798657208681107, Final Batch Loss: 0.11025980859994888\n",
      "Epoch 4221, Loss: 0.20017899572849274, Final Batch Loss: 0.03159775212407112\n",
      "Epoch 4222, Loss: 0.2827298976480961, Final Batch Loss: 0.08911699801683426\n",
      "Epoch 4223, Loss: 0.23596466332674026, Final Batch Loss: 0.04682883992791176\n",
      "Epoch 4224, Loss: 0.22407781705260277, Final Batch Loss: 0.03365131840109825\n",
      "Epoch 4225, Loss: 0.22110481932759285, Final Batch Loss: 0.08250609785318375\n",
      "Epoch 4226, Loss: 0.23560567758977413, Final Batch Loss: 0.06893814355134964\n",
      "Epoch 4227, Loss: 0.18796180188655853, Final Batch Loss: 0.032307688146829605\n",
      "Epoch 4228, Loss: 0.34466830641031265, Final Batch Loss: 0.07589803636074066\n",
      "Epoch 4229, Loss: 0.28200628235936165, Final Batch Loss: 0.07948921620845795\n",
      "Epoch 4230, Loss: 0.2765739820897579, Final Batch Loss: 0.04657517373561859\n",
      "Epoch 4231, Loss: 0.25872065499424934, Final Batch Loss: 0.04275099188089371\n",
      "Epoch 4232, Loss: 0.29628485068678856, Final Batch Loss: 0.04335222765803337\n",
      "Epoch 4233, Loss: 0.19092384725809097, Final Batch Loss: 0.059985291212797165\n",
      "Epoch 4234, Loss: 0.2116500549018383, Final Batch Loss: 0.036383919417858124\n",
      "Epoch 4235, Loss: 0.21113508194684982, Final Batch Loss: 0.04767533019185066\n",
      "Epoch 4236, Loss: 0.27858182042837143, Final Batch Loss: 0.14572656154632568\n",
      "Epoch 4237, Loss: 0.21473118849098682, Final Batch Loss: 0.024854162707924843\n",
      "Epoch 4238, Loss: 0.26253286749124527, Final Batch Loss: 0.07268974930047989\n",
      "Epoch 4239, Loss: 0.270242964848876, Final Batch Loss: 0.02417444996535778\n",
      "Epoch 4240, Loss: 0.20356006734073162, Final Batch Loss: 0.021034425124526024\n",
      "Epoch 4241, Loss: 0.3187272250652313, Final Batch Loss: 0.0720759779214859\n",
      "Epoch 4242, Loss: 0.2758074440062046, Final Batch Loss: 0.10730823129415512\n",
      "Epoch 4243, Loss: 0.24734880588948727, Final Batch Loss: 0.05095535144209862\n",
      "Epoch 4244, Loss: 0.31646086648106575, Final Batch Loss: 0.03028910979628563\n",
      "Epoch 4245, Loss: 0.2710006646811962, Final Batch Loss: 0.03984365984797478\n",
      "Epoch 4246, Loss: 0.3085145317018032, Final Batch Loss: 0.09705687314271927\n",
      "Epoch 4247, Loss: 0.18055259436368942, Final Batch Loss: 0.08027804642915726\n",
      "Epoch 4248, Loss: 0.221579197794199, Final Batch Loss: 0.04346858337521553\n",
      "Epoch 4249, Loss: 0.305239450186491, Final Batch Loss: 0.08871182054281235\n",
      "Epoch 4250, Loss: 0.18093942664563656, Final Batch Loss: 0.028546711429953575\n",
      "Epoch 4251, Loss: 0.2760085090994835, Final Batch Loss: 0.0803321972489357\n",
      "Epoch 4252, Loss: 0.298921313136816, Final Batch Loss: 0.1418573409318924\n",
      "Epoch 4253, Loss: 0.19138341210782528, Final Batch Loss: 0.034013502299785614\n",
      "Epoch 4254, Loss: 0.22083060070872307, Final Batch Loss: 0.05520651489496231\n",
      "Epoch 4255, Loss: 0.2821495123207569, Final Batch Loss: 0.05674144625663757\n",
      "Epoch 4256, Loss: 0.16076863184571266, Final Batch Loss: 0.04405441880226135\n",
      "Epoch 4257, Loss: 0.2144760973751545, Final Batch Loss: 0.015453528612852097\n",
      "Epoch 4258, Loss: 0.24131680466234684, Final Batch Loss: 0.01959333010017872\n",
      "Epoch 4259, Loss: 0.18524424731731415, Final Batch Loss: 0.027075212448835373\n",
      "Epoch 4260, Loss: 0.30533815920352936, Final Batch Loss: 0.13175351917743683\n",
      "Epoch 4261, Loss: 0.18235184997320175, Final Batch Loss: 0.046771563589572906\n",
      "Epoch 4262, Loss: 0.18565139919519424, Final Batch Loss: 0.03484008461236954\n",
      "Epoch 4263, Loss: 0.21968049928545952, Final Batch Loss: 0.05709940940141678\n",
      "Epoch 4264, Loss: 0.18913700617849827, Final Batch Loss: 0.05011538788676262\n",
      "Epoch 4265, Loss: 0.21538637205958366, Final Batch Loss: 0.050141144543886185\n",
      "Epoch 4266, Loss: 0.15626781433820724, Final Batch Loss: 0.0500517301261425\n",
      "Epoch 4267, Loss: 0.2300289273262024, Final Batch Loss: 0.07095159590244293\n",
      "Epoch 4268, Loss: 0.22795619070529938, Final Batch Loss: 0.07112708687782288\n",
      "Epoch 4269, Loss: 0.1355397328734398, Final Batch Loss: 0.018545104190707207\n",
      "Epoch 4270, Loss: 0.21246488578617573, Final Batch Loss: 0.024266036227345467\n",
      "Epoch 4271, Loss: 0.25440365821123123, Final Batch Loss: 0.07478144764900208\n",
      "Epoch 4272, Loss: 0.12809917703270912, Final Batch Loss: 0.027148500084877014\n",
      "Epoch 4273, Loss: 0.3061790242791176, Final Batch Loss: 0.0974356085062027\n",
      "Epoch 4274, Loss: 0.13280240446329117, Final Batch Loss: 0.03939215838909149\n",
      "Epoch 4275, Loss: 0.2205423004925251, Final Batch Loss: 0.03933623805642128\n",
      "Epoch 4276, Loss: 0.23845864459872246, Final Batch Loss: 0.0810076966881752\n",
      "Epoch 4277, Loss: 0.13918425515294075, Final Batch Loss: 0.025552596896886826\n",
      "Epoch 4278, Loss: 0.1961592584848404, Final Batch Loss: 0.021400514990091324\n",
      "Epoch 4279, Loss: 0.20357021689414978, Final Batch Loss: 0.03297295793890953\n",
      "Epoch 4280, Loss: 0.20959658175706863, Final Batch Loss: 0.03414586931467056\n",
      "Epoch 4281, Loss: 0.20758402347564697, Final Batch Loss: 0.07059875130653381\n",
      "Epoch 4282, Loss: 0.24464507400989532, Final Batch Loss: 0.04120120778679848\n",
      "Epoch 4283, Loss: 0.27396366372704506, Final Batch Loss: 0.08869660645723343\n",
      "Epoch 4284, Loss: 0.2520178258419037, Final Batch Loss: 0.07032103836536407\n",
      "Epoch 4285, Loss: 0.08687245938926935, Final Batch Loss: 0.007959998212754726\n",
      "Epoch 4286, Loss: 0.23365134745836258, Final Batch Loss: 0.04079778492450714\n",
      "Epoch 4287, Loss: 0.1894245883449912, Final Batch Loss: 0.013846657238900661\n",
      "Epoch 4288, Loss: 0.2279812917113304, Final Batch Loss: 0.03040517121553421\n",
      "Epoch 4289, Loss: 0.26352692767977715, Final Batch Loss: 0.09952516108751297\n",
      "Epoch 4290, Loss: 0.15736189112067223, Final Batch Loss: 0.031148742884397507\n",
      "Epoch 4291, Loss: 0.2357129529118538, Final Batch Loss: 0.019918080419301987\n",
      "Epoch 4292, Loss: 0.1740310601890087, Final Batch Loss: 0.03281761333346367\n",
      "Epoch 4293, Loss: 0.21100233495235443, Final Batch Loss: 0.06366868317127228\n",
      "Epoch 4294, Loss: 0.21492296177893877, Final Batch Loss: 0.062054287642240524\n",
      "Epoch 4295, Loss: 0.15778043679893017, Final Batch Loss: 0.028177564963698387\n",
      "Epoch 4296, Loss: 0.18857634998857975, Final Batch Loss: 0.049124058336019516\n",
      "Epoch 4297, Loss: 0.16796079091727734, Final Batch Loss: 0.023076148703694344\n",
      "Epoch 4298, Loss: 0.23374971002340317, Final Batch Loss: 0.05905386060476303\n",
      "Epoch 4299, Loss: 0.23400169610977173, Final Batch Loss: 0.05126439779996872\n",
      "Epoch 4300, Loss: 0.23144389316439629, Final Batch Loss: 0.07769936323165894\n",
      "Epoch 4301, Loss: 0.17219579312950373, Final Batch Loss: 0.012529228813946247\n",
      "Epoch 4302, Loss: 0.22946370020508766, Final Batch Loss: 0.02446538582444191\n",
      "Epoch 4303, Loss: 0.2072191797196865, Final Batch Loss: 0.06441092491149902\n",
      "Epoch 4304, Loss: 0.23522564955055714, Final Batch Loss: 0.08543440699577332\n",
      "Epoch 4305, Loss: 0.21261382848024368, Final Batch Loss: 0.05605992674827576\n",
      "Epoch 4306, Loss: 0.2539493031799793, Final Batch Loss: 0.0861758291721344\n",
      "Epoch 4307, Loss: 0.2598913237452507, Final Batch Loss: 0.028246738016605377\n",
      "Epoch 4308, Loss: 0.24251031130552292, Final Batch Loss: 0.08981438726186752\n",
      "Epoch 4309, Loss: 0.26707809045910835, Final Batch Loss: 0.12030772119760513\n",
      "Epoch 4310, Loss: 0.20331250317394733, Final Batch Loss: 0.018618984147906303\n",
      "Epoch 4311, Loss: 0.31977372989058495, Final Batch Loss: 0.036747392266988754\n",
      "Epoch 4312, Loss: 0.1619824543595314, Final Batch Loss: 0.036585483700037\n",
      "Epoch 4313, Loss: 0.3060736358165741, Final Batch Loss: 0.04634595662355423\n",
      "Epoch 4314, Loss: 0.2049538567662239, Final Batch Loss: 0.03389323130249977\n",
      "Epoch 4315, Loss: 0.1809113323688507, Final Batch Loss: 0.054896578192710876\n",
      "Epoch 4316, Loss: 0.12724026665091515, Final Batch Loss: 0.019926084205508232\n",
      "Epoch 4317, Loss: 0.21116285398602486, Final Batch Loss: 0.07751811295747757\n",
      "Epoch 4318, Loss: 0.16717627830803394, Final Batch Loss: 0.06604259461164474\n",
      "Epoch 4319, Loss: 0.1954747699201107, Final Batch Loss: 0.021803531795740128\n",
      "Epoch 4320, Loss: 0.24885590374469757, Final Batch Loss: 0.05999937653541565\n",
      "Epoch 4321, Loss: 0.24226138647645712, Final Batch Loss: 0.014301932416856289\n",
      "Epoch 4322, Loss: 0.18758505582809448, Final Batch Loss: 0.04277101159095764\n",
      "Epoch 4323, Loss: 0.3413628488779068, Final Batch Loss: 0.18552063405513763\n",
      "Epoch 4324, Loss: 0.28765489906072617, Final Batch Loss: 0.0912364199757576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4325, Loss: 0.28248724341392517, Final Batch Loss: 0.07951691746711731\n",
      "Epoch 4326, Loss: 0.22945847548544407, Final Batch Loss: 0.029776187613606453\n",
      "Epoch 4327, Loss: 0.2595316171646118, Final Batch Loss: 0.031450092792510986\n",
      "Epoch 4328, Loss: 0.22521167248487473, Final Batch Loss: 0.05243613198399544\n",
      "Epoch 4329, Loss: 0.26939305290579796, Final Batch Loss: 0.08589804917573929\n",
      "Epoch 4330, Loss: 0.48719317466020584, Final Batch Loss: 0.24072140455245972\n",
      "Epoch 4331, Loss: 0.22894424200057983, Final Batch Loss: 0.0514625646173954\n",
      "Epoch 4332, Loss: 0.3598861023783684, Final Batch Loss: 0.03618251532316208\n",
      "Epoch 4333, Loss: 0.3764728382229805, Final Batch Loss: 0.08293897658586502\n",
      "Epoch 4334, Loss: 0.26757315173745155, Final Batch Loss: 0.07205230742692947\n",
      "Epoch 4335, Loss: 0.1999258827418089, Final Batch Loss: 0.022324929013848305\n",
      "Epoch 4336, Loss: 0.2991625778377056, Final Batch Loss: 0.11916890740394592\n",
      "Epoch 4337, Loss: 0.29048585146665573, Final Batch Loss: 0.04899714142084122\n",
      "Epoch 4338, Loss: 0.26706868782639503, Final Batch Loss: 0.08247008919715881\n",
      "Epoch 4339, Loss: 0.2814771234989166, Final Batch Loss: 0.08686444163322449\n",
      "Epoch 4340, Loss: 0.3208417370915413, Final Batch Loss: 0.15482783317565918\n",
      "Epoch 4341, Loss: 0.2733942121267319, Final Batch Loss: 0.11547042429447174\n",
      "Epoch 4342, Loss: 0.27934246417135, Final Batch Loss: 0.015333685092628002\n",
      "Epoch 4343, Loss: 0.4332282990217209, Final Batch Loss: 0.1717660129070282\n",
      "Epoch 4344, Loss: 0.2507544010877609, Final Batch Loss: 0.047206759452819824\n",
      "Epoch 4345, Loss: 0.28199273720383644, Final Batch Loss: 0.09344936162233353\n",
      "Epoch 4346, Loss: 0.27780545875430107, Final Batch Loss: 0.11876150965690613\n",
      "Epoch 4347, Loss: 0.263317808508873, Final Batch Loss: 0.06389737874269485\n",
      "Epoch 4348, Loss: 0.2701220028102398, Final Batch Loss: 0.09140141308307648\n",
      "Epoch 4349, Loss: 0.2172667421400547, Final Batch Loss: 0.053502533584833145\n",
      "Epoch 4350, Loss: 0.1929531916975975, Final Batch Loss: 0.04957515746355057\n",
      "Epoch 4351, Loss: 0.21346812136471272, Final Batch Loss: 0.026155991479754448\n",
      "Epoch 4352, Loss: 0.17360112629830837, Final Batch Loss: 0.026280546560883522\n",
      "Epoch 4353, Loss: 0.2287076022475958, Final Batch Loss: 0.017681283876299858\n",
      "Epoch 4354, Loss: 0.1732014026492834, Final Batch Loss: 0.07012402266263962\n",
      "Epoch 4355, Loss: 0.14003000408411026, Final Batch Loss: 0.02596297115087509\n",
      "Epoch 4356, Loss: 0.13698195479810238, Final Batch Loss: 0.023010555654764175\n",
      "Epoch 4357, Loss: 0.18378628976643085, Final Batch Loss: 0.05687887966632843\n",
      "Epoch 4358, Loss: 0.16951914317905903, Final Batch Loss: 0.028066733852028847\n",
      "Epoch 4359, Loss: 0.15719096548855305, Final Batch Loss: 0.026522906497120857\n",
      "Epoch 4360, Loss: 0.20799950882792473, Final Batch Loss: 0.03917790576815605\n",
      "Epoch 4361, Loss: 0.3163984250277281, Final Batch Loss: 0.19565163552761078\n",
      "Epoch 4362, Loss: 0.20671170577406883, Final Batch Loss: 0.05940455570816994\n",
      "Epoch 4363, Loss: 0.2560391463339329, Final Batch Loss: 0.027405526489019394\n",
      "Epoch 4364, Loss: 0.18094480969011784, Final Batch Loss: 0.052316419780254364\n",
      "Epoch 4365, Loss: 0.26766553334891796, Final Batch Loss: 0.1262541264295578\n",
      "Epoch 4366, Loss: 0.2036693785339594, Final Batch Loss: 0.028408681973814964\n",
      "Epoch 4367, Loss: 0.28703104704618454, Final Batch Loss: 0.08243369311094284\n",
      "Epoch 4368, Loss: 0.20486169680953026, Final Batch Loss: 0.04639820381999016\n",
      "Epoch 4369, Loss: 0.12397897988557816, Final Batch Loss: 0.02092774584889412\n",
      "Epoch 4370, Loss: 0.13511708937585354, Final Batch Loss: 0.02634807676076889\n",
      "Epoch 4371, Loss: 0.3263464756309986, Final Batch Loss: 0.204747274518013\n",
      "Epoch 4372, Loss: 0.21988454088568687, Final Batch Loss: 0.06676322966814041\n",
      "Epoch 4373, Loss: 0.2565088961273432, Final Batch Loss: 0.10528065264225006\n",
      "Epoch 4374, Loss: 0.22987448796629906, Final Batch Loss: 0.04562068730592728\n",
      "Epoch 4375, Loss: 0.28509439900517464, Final Batch Loss: 0.08857247978448868\n",
      "Epoch 4376, Loss: 0.20046882703900337, Final Batch Loss: 0.014218088239431381\n",
      "Epoch 4377, Loss: 0.2486610896885395, Final Batch Loss: 0.060764819383621216\n",
      "Epoch 4378, Loss: 0.24208467826247215, Final Batch Loss: 0.06531541049480438\n",
      "Epoch 4379, Loss: 0.1782795898616314, Final Batch Loss: 0.03440402075648308\n",
      "Epoch 4380, Loss: 0.2984183579683304, Final Batch Loss: 0.07649460434913635\n",
      "Epoch 4381, Loss: 0.32043755427002907, Final Batch Loss: 0.11974836885929108\n",
      "Epoch 4382, Loss: 0.3140036091208458, Final Batch Loss: 0.031620435416698456\n",
      "Epoch 4383, Loss: 0.2570771686732769, Final Batch Loss: 0.07251971960067749\n",
      "Epoch 4384, Loss: 0.31474341824650764, Final Batch Loss: 0.1280273050069809\n",
      "Epoch 4385, Loss: 0.25323917903006077, Final Batch Loss: 0.07921722531318665\n",
      "Epoch 4386, Loss: 0.22893846966326237, Final Batch Loss: 0.05986473709344864\n",
      "Epoch 4387, Loss: 0.28288452699780464, Final Batch Loss: 0.08593866229057312\n",
      "Epoch 4388, Loss: 0.2618486452847719, Final Batch Loss: 0.1140466183423996\n",
      "Epoch 4389, Loss: 0.199919655919075, Final Batch Loss: 0.026695597916841507\n",
      "Epoch 4390, Loss: 0.1837446354329586, Final Batch Loss: 0.036275673657655716\n",
      "Epoch 4391, Loss: 0.309032142162323, Final Batch Loss: 0.12111666053533554\n",
      "Epoch 4392, Loss: 0.17928567342460155, Final Batch Loss: 0.02931847982108593\n",
      "Epoch 4393, Loss: 0.24370282515883446, Final Batch Loss: 0.05351250618696213\n",
      "Epoch 4394, Loss: 0.2578428275883198, Final Batch Loss: 0.060907233506441116\n",
      "Epoch 4395, Loss: 0.22207876853644848, Final Batch Loss: 0.021163133904337883\n",
      "Epoch 4396, Loss: 0.13310516439378262, Final Batch Loss: 0.015898311510682106\n",
      "Epoch 4397, Loss: 0.2208859734237194, Final Batch Loss: 0.03882880136370659\n",
      "Epoch 4398, Loss: 0.15102671459317207, Final Batch Loss: 0.033511485904455185\n",
      "Epoch 4399, Loss: 0.14889715798199177, Final Batch Loss: 0.008500011637806892\n",
      "Epoch 4400, Loss: 0.16200324520468712, Final Batch Loss: 0.01669830083847046\n",
      "Epoch 4401, Loss: 0.1949337851256132, Final Batch Loss: 0.030945198610424995\n",
      "Epoch 4402, Loss: 0.232254009693861, Final Batch Loss: 0.031734649091959\n",
      "Epoch 4403, Loss: 0.1784859262406826, Final Batch Loss: 0.03570825979113579\n",
      "Epoch 4404, Loss: 0.2167445346713066, Final Batch Loss: 0.043731689453125\n",
      "Epoch 4405, Loss: 0.1435619406402111, Final Batch Loss: 0.023639794439077377\n",
      "Epoch 4406, Loss: 0.17208191193640232, Final Batch Loss: 0.025972673669457436\n",
      "Epoch 4407, Loss: 0.17268197983503342, Final Batch Loss: 0.017048578709363937\n",
      "Epoch 4408, Loss: 0.2526300884783268, Final Batch Loss: 0.03764237090945244\n",
      "Epoch 4409, Loss: 0.22653748281300068, Final Batch Loss: 0.05178029090166092\n",
      "Epoch 4410, Loss: 0.2908039838075638, Final Batch Loss: 0.10810581594705582\n",
      "Epoch 4411, Loss: 0.2020598240196705, Final Batch Loss: 0.03755679354071617\n",
      "Epoch 4412, Loss: 0.2216120846569538, Final Batch Loss: 0.07218266278505325\n",
      "Epoch 4413, Loss: 0.3047215286642313, Final Batch Loss: 0.13175661861896515\n",
      "Epoch 4414, Loss: 0.2804325222969055, Final Batch Loss: 0.07374099642038345\n",
      "Epoch 4415, Loss: 0.20450382679700851, Final Batch Loss: 0.04827785864472389\n",
      "Epoch 4416, Loss: 0.19204162620007992, Final Batch Loss: 0.03691216558218002\n",
      "Epoch 4417, Loss: 0.21562554314732552, Final Batch Loss: 0.06404119729995728\n",
      "Epoch 4418, Loss: 0.30249233543872833, Final Batch Loss: 0.0680808573961258\n",
      "Epoch 4419, Loss: 0.30715273320674896, Final Batch Loss: 0.10863204300403595\n",
      "Epoch 4420, Loss: 0.22262035310268402, Final Batch Loss: 0.05584366247057915\n",
      "Epoch 4421, Loss: 0.3861534968018532, Final Batch Loss: 0.1264818012714386\n",
      "Epoch 4422, Loss: 0.2373553179204464, Final Batch Loss: 0.10533268749713898\n",
      "Epoch 4423, Loss: 0.23419589176774025, Final Batch Loss: 0.00945623591542244\n",
      "Epoch 4424, Loss: 0.20430566370487213, Final Batch Loss: 0.06091468781232834\n",
      "Epoch 4425, Loss: 0.2463359460234642, Final Batch Loss: 0.060174115002155304\n",
      "Epoch 4426, Loss: 0.18751557916402817, Final Batch Loss: 0.02013632282614708\n",
      "Epoch 4427, Loss: 0.18569130077958107, Final Batch Loss: 0.039239831268787384\n",
      "Epoch 4428, Loss: 0.1761283427476883, Final Batch Loss: 0.03737225383520126\n",
      "Epoch 4429, Loss: 0.28710588440299034, Final Batch Loss: 0.12854817509651184\n",
      "Epoch 4430, Loss: 0.14148839376866817, Final Batch Loss: 0.02598876692354679\n",
      "Epoch 4431, Loss: 0.19962727092206478, Final Batch Loss: 0.09202493727207184\n",
      "Epoch 4432, Loss: 0.15890781581401825, Final Batch Loss: 0.05159805715084076\n",
      "Epoch 4433, Loss: 0.1926986500620842, Final Batch Loss: 0.06110978499054909\n",
      "Epoch 4434, Loss: 0.16689484007656574, Final Batch Loss: 0.014004642143845558\n",
      "Epoch 4435, Loss: 0.206867303699255, Final Batch Loss: 0.0682489350438118\n",
      "Epoch 4436, Loss: 0.1615295046940446, Final Batch Loss: 0.005875782109797001\n",
      "Epoch 4437, Loss: 0.2295851018279791, Final Batch Loss: 0.09689343720674515\n",
      "Epoch 4438, Loss: 0.16597943380475044, Final Batch Loss: 0.03031233698129654\n",
      "Epoch 4439, Loss: 0.2614906430244446, Final Batch Loss: 0.12845008075237274\n",
      "Epoch 4440, Loss: 0.11364560853689909, Final Batch Loss: 0.008994321338832378\n",
      "Epoch 4441, Loss: 0.18354610539972782, Final Batch Loss: 0.0674189031124115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4442, Loss: 0.20526955649256706, Final Batch Loss: 0.050324853509664536\n",
      "Epoch 4443, Loss: 0.1937437243759632, Final Batch Loss: 0.06628844141960144\n",
      "Epoch 4444, Loss: 0.16690430417656898, Final Batch Loss: 0.011718329042196274\n",
      "Epoch 4445, Loss: 0.226004958152771, Final Batch Loss: 0.056014616042375565\n",
      "Epoch 4446, Loss: 0.19246339797973633, Final Batch Loss: 0.0689954087138176\n",
      "Epoch 4447, Loss: 0.27818823233246803, Final Batch Loss: 0.12066003680229187\n",
      "Epoch 4448, Loss: 0.24645650759339333, Final Batch Loss: 0.07321307808160782\n",
      "Epoch 4449, Loss: 0.17003287933766842, Final Batch Loss: 0.0238781925290823\n",
      "Epoch 4450, Loss: 0.2774172015488148, Final Batch Loss: 0.03963480889797211\n",
      "Epoch 4451, Loss: 0.3320261240005493, Final Batch Loss: 0.19506660103797913\n",
      "Epoch 4452, Loss: 0.22592148557305336, Final Batch Loss: 0.06549551337957382\n",
      "Epoch 4453, Loss: 0.30753545090556145, Final Batch Loss: 0.11819569021463394\n",
      "Epoch 4454, Loss: 0.21554433181881905, Final Batch Loss: 0.02266484871506691\n",
      "Epoch 4455, Loss: 0.2030043937265873, Final Batch Loss: 0.03832594305276871\n",
      "Epoch 4456, Loss: 0.2364651784300804, Final Batch Loss: 0.0897560641169548\n",
      "Epoch 4457, Loss: 0.2354386430233717, Final Batch Loss: 0.08392366766929626\n",
      "Epoch 4458, Loss: 0.2371604312211275, Final Batch Loss: 0.10880538076162338\n",
      "Epoch 4459, Loss: 0.17105362564325333, Final Batch Loss: 0.01791602373123169\n",
      "Epoch 4460, Loss: 0.18120130337774754, Final Batch Loss: 0.06572207808494568\n",
      "Epoch 4461, Loss: 0.15797682479023933, Final Batch Loss: 0.0321924090385437\n",
      "Epoch 4462, Loss: 0.19495324417948723, Final Batch Loss: 0.0436125211417675\n",
      "Epoch 4463, Loss: 0.20690346881747246, Final Batch Loss: 0.037419773638248444\n",
      "Epoch 4464, Loss: 0.2888575866818428, Final Batch Loss: 0.0804024338722229\n",
      "Epoch 4465, Loss: 0.34727856889367104, Final Batch Loss: 0.2489529252052307\n",
      "Epoch 4466, Loss: 0.20709078386425972, Final Batch Loss: 0.023632396012544632\n",
      "Epoch 4467, Loss: 0.26269075088202953, Final Batch Loss: 0.09468001127243042\n",
      "Epoch 4468, Loss: 0.2160715088248253, Final Batch Loss: 0.07174676656723022\n",
      "Epoch 4469, Loss: 0.36089375987648964, Final Batch Loss: 0.15102243423461914\n",
      "Epoch 4470, Loss: 0.20866794139146805, Final Batch Loss: 0.04725731909275055\n",
      "Epoch 4471, Loss: 0.19954678043723106, Final Batch Loss: 0.08364152163267136\n",
      "Epoch 4472, Loss: 0.2005733698606491, Final Batch Loss: 0.052925560623407364\n",
      "Epoch 4473, Loss: 0.2735927179455757, Final Batch Loss: 0.036992669105529785\n",
      "Epoch 4474, Loss: 0.21755759045481682, Final Batch Loss: 0.06689552962779999\n",
      "Epoch 4475, Loss: 0.23078695312142372, Final Batch Loss: 0.036713603883981705\n",
      "Epoch 4476, Loss: 0.31285781040787697, Final Batch Loss: 0.11495030671358109\n",
      "Epoch 4477, Loss: 0.25227682664990425, Final Batch Loss: 0.05449133738875389\n",
      "Epoch 4478, Loss: 0.21691018342971802, Final Batch Loss: 0.03777267411351204\n",
      "Epoch 4479, Loss: 0.2205715849995613, Final Batch Loss: 0.05038944259285927\n",
      "Epoch 4480, Loss: 0.2481447421014309, Final Batch Loss: 0.06128835305571556\n",
      "Epoch 4481, Loss: 0.2045241855084896, Final Batch Loss: 0.036703359335660934\n",
      "Epoch 4482, Loss: 0.2926248200237751, Final Batch Loss: 0.08946802467107773\n",
      "Epoch 4483, Loss: 0.15838902071118355, Final Batch Loss: 0.03605908527970314\n",
      "Epoch 4484, Loss: 0.18212394416332245, Final Batch Loss: 0.04270892217755318\n",
      "Epoch 4485, Loss: 0.16452131606638432, Final Batch Loss: 0.029319260269403458\n",
      "Epoch 4486, Loss: 0.2967175990343094, Final Batch Loss: 0.19038495421409607\n",
      "Epoch 4487, Loss: 0.17139089107513428, Final Batch Loss: 0.032571855932474136\n",
      "Epoch 4488, Loss: 0.23470613360404968, Final Batch Loss: 0.0494517907500267\n",
      "Epoch 4489, Loss: 0.23510712571442127, Final Batch Loss: 0.024053165689110756\n",
      "Epoch 4490, Loss: 0.18203048594295979, Final Batch Loss: 0.038224056363105774\n",
      "Epoch 4491, Loss: 0.2519661281257868, Final Batch Loss: 0.03030361421406269\n",
      "Epoch 4492, Loss: 0.18007918260991573, Final Batch Loss: 0.029963357374072075\n",
      "Epoch 4493, Loss: 0.16196702979505062, Final Batch Loss: 0.03111875243484974\n",
      "Epoch 4494, Loss: 0.14331644214689732, Final Batch Loss: 0.051788780838251114\n",
      "Epoch 4495, Loss: 0.26208999566733837, Final Batch Loss: 0.14221003651618958\n",
      "Epoch 4496, Loss: 0.1467419471591711, Final Batch Loss: 0.023546231910586357\n",
      "Epoch 4497, Loss: 0.22944927960634232, Final Batch Loss: 0.06518709659576416\n",
      "Epoch 4498, Loss: 0.21630208939313889, Final Batch Loss: 0.05382249504327774\n",
      "Epoch 4499, Loss: 0.23962770402431488, Final Batch Loss: 0.041628994047641754\n",
      "Epoch 4500, Loss: 0.22115738317370415, Final Batch Loss: 0.10652894526720047\n",
      "Epoch 4501, Loss: 0.2129586897790432, Final Batch Loss: 0.016815125942230225\n",
      "Epoch 4502, Loss: 0.2836897298693657, Final Batch Loss: 0.0920654758810997\n",
      "Epoch 4503, Loss: 0.16595210321247578, Final Batch Loss: 0.0491701178252697\n",
      "Epoch 4504, Loss: 0.23407430201768875, Final Batch Loss: 0.07176589220762253\n",
      "Epoch 4505, Loss: 0.17157061770558357, Final Batch Loss: 0.021288923919200897\n",
      "Epoch 4506, Loss: 0.18508026003837585, Final Batch Loss: 0.054511651396751404\n",
      "Epoch 4507, Loss: 0.20007231272757053, Final Batch Loss: 0.06829643994569778\n",
      "Epoch 4508, Loss: 0.2902817577123642, Final Batch Loss: 0.13395251333713531\n",
      "Epoch 4509, Loss: 0.24355898424983025, Final Batch Loss: 0.016540568321943283\n",
      "Epoch 4510, Loss: 0.2608449310064316, Final Batch Loss: 0.09470558911561966\n",
      "Epoch 4511, Loss: 0.20132273994386196, Final Batch Loss: 0.018192531540989876\n",
      "Epoch 4512, Loss: 0.1692778319120407, Final Batch Loss: 0.029462717473506927\n",
      "Epoch 4513, Loss: 0.2159859724342823, Final Batch Loss: 0.06297845393419266\n",
      "Epoch 4514, Loss: 0.1919906847178936, Final Batch Loss: 0.034579332917928696\n",
      "Epoch 4515, Loss: 0.2102073524147272, Final Batch Loss: 0.08261042088270187\n",
      "Epoch 4516, Loss: 0.3299626372754574, Final Batch Loss: 0.09098687767982483\n",
      "Epoch 4517, Loss: 0.20409305207431316, Final Batch Loss: 0.022971348837018013\n",
      "Epoch 4518, Loss: 0.15392720326781273, Final Batch Loss: 0.015484694391489029\n",
      "Epoch 4519, Loss: 0.22928335517644882, Final Batch Loss: 0.0531579963862896\n",
      "Epoch 4520, Loss: 0.28235457465052605, Final Batch Loss: 0.06247783452272415\n",
      "Epoch 4521, Loss: 0.32775409519672394, Final Batch Loss: 0.16286084055900574\n",
      "Epoch 4522, Loss: 0.20366164296865463, Final Batch Loss: 0.04210285097360611\n",
      "Epoch 4523, Loss: 0.3247312977910042, Final Batch Loss: 0.07182207703590393\n",
      "Epoch 4524, Loss: 0.22028694860637188, Final Batch Loss: 0.029492603614926338\n",
      "Epoch 4525, Loss: 0.1694495566189289, Final Batch Loss: 0.007328413426876068\n",
      "Epoch 4526, Loss: 0.20257125608623028, Final Batch Loss: 0.05300358310341835\n",
      "Epoch 4527, Loss: 0.18002484738826752, Final Batch Loss: 0.02930392324924469\n",
      "Epoch 4528, Loss: 0.23344996571540833, Final Batch Loss: 0.03723464533686638\n",
      "Epoch 4529, Loss: 0.17183572985231876, Final Batch Loss: 0.023402122780680656\n",
      "Epoch 4530, Loss: 0.1810340266674757, Final Batch Loss: 0.04962508752942085\n",
      "Epoch 4531, Loss: 0.1372819021344185, Final Batch Loss: 0.016648931428790092\n",
      "Epoch 4532, Loss: 0.14205761812627316, Final Batch Loss: 0.020509274676442146\n",
      "Epoch 4533, Loss: 0.17741172015666962, Final Batch Loss: 0.04046779125928879\n",
      "Epoch 4534, Loss: 0.20010825712233782, Final Batch Loss: 0.013397679664194584\n",
      "Epoch 4535, Loss: 0.18285799026489258, Final Batch Loss: 0.06802349537611008\n",
      "Epoch 4536, Loss: 0.1804616693407297, Final Batch Loss: 0.025635411962866783\n",
      "Epoch 4537, Loss: 0.21668144688010216, Final Batch Loss: 0.04666924104094505\n",
      "Epoch 4538, Loss: 0.20807470381259918, Final Batch Loss: 0.03831899166107178\n",
      "Epoch 4539, Loss: 0.1982378363609314, Final Batch Loss: 0.025193914771080017\n",
      "Epoch 4540, Loss: 0.2477899082005024, Final Batch Loss: 0.04611781984567642\n",
      "Epoch 4541, Loss: 0.15451341308653355, Final Batch Loss: 0.050969675183296204\n",
      "Epoch 4542, Loss: 0.1428738348186016, Final Batch Loss: 0.035570334643125534\n",
      "Epoch 4543, Loss: 0.2049017921090126, Final Batch Loss: 0.055302027612924576\n",
      "Epoch 4544, Loss: 0.13381507992744446, Final Batch Loss: 0.03122095577418804\n",
      "Epoch 4545, Loss: 0.16005579940974712, Final Batch Loss: 0.012564146891236305\n",
      "Epoch 4546, Loss: 0.15475616790354252, Final Batch Loss: 0.02195400930941105\n",
      "Epoch 4547, Loss: 0.13933949917554855, Final Batch Loss: 0.0292216744273901\n",
      "Epoch 4548, Loss: 0.26119084656238556, Final Batch Loss: 0.050479527562856674\n",
      "Epoch 4549, Loss: 0.173351620323956, Final Batch Loss: 0.012044047005474567\n",
      "Epoch 4550, Loss: 0.1308675715699792, Final Batch Loss: 0.013140642084181309\n",
      "Epoch 4551, Loss: 0.21106751263141632, Final Batch Loss: 0.04226138815283775\n",
      "Epoch 4552, Loss: 0.21427608653903008, Final Batch Loss: 0.09489503502845764\n",
      "Epoch 4553, Loss: 0.1314542144536972, Final Batch Loss: 0.029393458738923073\n",
      "Epoch 4554, Loss: 0.19455119222402573, Final Batch Loss: 0.055364955216646194\n",
      "Epoch 4555, Loss: 0.25312577933073044, Final Batch Loss: 0.07536770403385162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4556, Loss: 0.32667751237750053, Final Batch Loss: 0.18124982714653015\n",
      "Epoch 4557, Loss: 0.27595749124884605, Final Batch Loss: 0.11945516616106033\n",
      "Epoch 4558, Loss: 0.20486801862716675, Final Batch Loss: 0.03901410102844238\n",
      "Epoch 4559, Loss: 0.2768558897078037, Final Batch Loss: 0.04328067600727081\n",
      "Epoch 4560, Loss: 0.20188860967755318, Final Batch Loss: 0.04481561481952667\n",
      "Epoch 4561, Loss: 0.333823524415493, Final Batch Loss: 0.13640034198760986\n",
      "Epoch 4562, Loss: 0.23319094441831112, Final Batch Loss: 0.023675160482525826\n",
      "Epoch 4563, Loss: 0.217091653496027, Final Batch Loss: 0.05996821075677872\n",
      "Epoch 4564, Loss: 0.21120404824614525, Final Batch Loss: 0.04725152254104614\n",
      "Epoch 4565, Loss: 0.16987857036292553, Final Batch Loss: 0.0512629970908165\n",
      "Epoch 4566, Loss: 0.23062757030129433, Final Batch Loss: 0.06966515630483627\n",
      "Epoch 4567, Loss: 0.18777994066476822, Final Batch Loss: 0.0489342138171196\n",
      "Epoch 4568, Loss: 0.23738353699445724, Final Batch Loss: 0.0734986662864685\n",
      "Epoch 4569, Loss: 0.2563564330339432, Final Batch Loss: 0.07639768719673157\n",
      "Epoch 4570, Loss: 0.28490489162504673, Final Batch Loss: 0.0268035177141428\n",
      "Epoch 4571, Loss: 0.2946855500340462, Final Batch Loss: 0.09531073272228241\n",
      "Epoch 4572, Loss: 0.23212150111794472, Final Batch Loss: 0.02306569367647171\n",
      "Epoch 4573, Loss: 0.19987214729189873, Final Batch Loss: 0.06970919668674469\n",
      "Epoch 4574, Loss: 0.20710203424096107, Final Batch Loss: 0.09308792650699615\n",
      "Epoch 4575, Loss: 0.20591653510928154, Final Batch Loss: 0.08685631304979324\n",
      "Epoch 4576, Loss: 0.19659768044948578, Final Batch Loss: 0.05238839611411095\n",
      "Epoch 4577, Loss: 0.1837619673460722, Final Batch Loss: 0.06243792548775673\n",
      "Epoch 4578, Loss: 0.17992013692855835, Final Batch Loss: 0.05366911739110947\n",
      "Epoch 4579, Loss: 0.22046435996890068, Final Batch Loss: 0.0609351247549057\n",
      "Epoch 4580, Loss: 0.230356615036726, Final Batch Loss: 0.050868988037109375\n",
      "Epoch 4581, Loss: 0.29378631338477135, Final Batch Loss: 0.12309644371271133\n",
      "Epoch 4582, Loss: 0.29639679566025734, Final Batch Loss: 0.039994385093450546\n",
      "Epoch 4583, Loss: 0.20229385048151016, Final Batch Loss: 0.01668892800807953\n",
      "Epoch 4584, Loss: 0.25221364572644234, Final Batch Loss: 0.07818640023469925\n",
      "Epoch 4585, Loss: 0.3089543469250202, Final Batch Loss: 0.0578603632748127\n",
      "Epoch 4586, Loss: 0.2723081558942795, Final Batch Loss: 0.10581783950328827\n",
      "Epoch 4587, Loss: 0.3131969291716814, Final Batch Loss: 0.14989179372787476\n",
      "Epoch 4588, Loss: 0.284169415012002, Final Batch Loss: 0.15463025867938995\n",
      "Epoch 4589, Loss: 0.2595997788012028, Final Batch Loss: 0.05291444808244705\n",
      "Epoch 4590, Loss: 0.22960734739899635, Final Batch Loss: 0.035534609109163284\n",
      "Epoch 4591, Loss: 0.2428816445171833, Final Batch Loss: 0.019710171967744827\n",
      "Epoch 4592, Loss: 0.38722359016537666, Final Batch Loss: 0.06807120889425278\n",
      "Epoch 4593, Loss: 0.18248250149190426, Final Batch Loss: 0.06062724068760872\n",
      "Epoch 4594, Loss: 0.2869129776954651, Final Batch Loss: 0.05030868202447891\n",
      "Epoch 4595, Loss: 0.27549320086836815, Final Batch Loss: 0.04255414381623268\n",
      "Epoch 4596, Loss: 0.21039610728621483, Final Batch Loss: 0.05572464317083359\n",
      "Epoch 4597, Loss: 0.14584757760167122, Final Batch Loss: 0.028714818879961967\n",
      "Epoch 4598, Loss: 0.18676640838384628, Final Batch Loss: 0.05297888070344925\n",
      "Epoch 4599, Loss: 0.22103074565529823, Final Batch Loss: 0.04837494716048241\n",
      "Epoch 4600, Loss: 0.12803399562835693, Final Batch Loss: 0.012388957664370537\n",
      "Epoch 4601, Loss: 0.22358579002320766, Final Batch Loss: 0.013038119301199913\n",
      "Epoch 4602, Loss: 0.14846500381827354, Final Batch Loss: 0.02214808203279972\n",
      "Epoch 4603, Loss: 0.21831130981445312, Final Batch Loss: 0.024322818964719772\n",
      "Epoch 4604, Loss: 0.24638896062970161, Final Batch Loss: 0.055157940834760666\n",
      "Epoch 4605, Loss: 0.1984873954206705, Final Batch Loss: 0.01434103213250637\n",
      "Epoch 4606, Loss: 0.13730421848595142, Final Batch Loss: 0.011675899848341942\n",
      "Epoch 4607, Loss: 0.16853928938508034, Final Batch Loss: 0.019357506185770035\n",
      "Epoch 4608, Loss: 0.13597524911165237, Final Batch Loss: 0.02797735668718815\n",
      "Epoch 4609, Loss: 0.18254522047936916, Final Batch Loss: 0.0897873267531395\n",
      "Epoch 4610, Loss: 0.23599303141236305, Final Batch Loss: 0.04030332341790199\n",
      "Epoch 4611, Loss: 0.22963236272335052, Final Batch Loss: 0.02626003324985504\n",
      "Epoch 4612, Loss: 0.2265057023614645, Final Batch Loss: 0.08033818751573563\n",
      "Epoch 4613, Loss: 0.2266296837478876, Final Batch Loss: 0.07275334745645523\n",
      "Epoch 4614, Loss: 0.17106164060533047, Final Batch Loss: 0.056226398795843124\n",
      "Epoch 4615, Loss: 0.19050811138004065, Final Batch Loss: 0.013682260178029537\n",
      "Epoch 4616, Loss: 0.21014204621315002, Final Batch Loss: 0.03368331864476204\n",
      "Epoch 4617, Loss: 0.2909092865884304, Final Batch Loss: 0.0796775370836258\n",
      "Epoch 4618, Loss: 0.18063312955200672, Final Batch Loss: 0.048006314784288406\n",
      "Epoch 4619, Loss: 0.18302498571574688, Final Batch Loss: 0.05546673759818077\n",
      "Epoch 4620, Loss: 0.19376585632562637, Final Batch Loss: 0.03449084237217903\n",
      "Epoch 4621, Loss: 0.16579494252800941, Final Batch Loss: 0.047041818499565125\n",
      "Epoch 4622, Loss: 0.1664842590689659, Final Batch Loss: 0.06665436178445816\n",
      "Epoch 4623, Loss: 0.2097092866897583, Final Batch Loss: 0.05502045899629593\n",
      "Epoch 4624, Loss: 0.1698634922504425, Final Batch Loss: 0.06126127764582634\n",
      "Epoch 4625, Loss: 0.15432540327310562, Final Batch Loss: 0.029968945309519768\n",
      "Epoch 4626, Loss: 0.20170861296355724, Final Batch Loss: 0.009124381467700005\n",
      "Epoch 4627, Loss: 0.25567223876714706, Final Batch Loss: 0.024626143276691437\n",
      "Epoch 4628, Loss: 0.17807067930698395, Final Batch Loss: 0.054519400000572205\n",
      "Epoch 4629, Loss: 0.3567589111626148, Final Batch Loss: 0.10609111934900284\n",
      "Epoch 4630, Loss: 0.2633630447089672, Final Batch Loss: 0.08386189490556717\n",
      "Epoch 4631, Loss: 0.4505961015820503, Final Batch Loss: 0.16007795929908752\n",
      "Epoch 4632, Loss: 0.2658601552248001, Final Batch Loss: 0.09725789725780487\n",
      "Epoch 4633, Loss: 0.2974136807024479, Final Batch Loss: 0.07324773818254471\n",
      "Epoch 4634, Loss: 0.26809942722320557, Final Batch Loss: 0.01595514640212059\n",
      "Epoch 4635, Loss: 0.16029419004917145, Final Batch Loss: 0.0321139357984066\n",
      "Epoch 4636, Loss: 0.24402576684951782, Final Batch Loss: 0.0382373109459877\n",
      "Epoch 4637, Loss: 0.20626788586378098, Final Batch Loss: 0.02507563680410385\n",
      "Epoch 4638, Loss: 0.2162145134061575, Final Batch Loss: 0.02033207006752491\n",
      "Epoch 4639, Loss: 0.29578427597880363, Final Batch Loss: 0.11075779795646667\n",
      "Epoch 4640, Loss: 0.21307016722857952, Final Batch Loss: 0.028705934062600136\n",
      "Epoch 4641, Loss: 0.30255208164453506, Final Batch Loss: 0.11011365801095963\n",
      "Epoch 4642, Loss: 0.19291052967309952, Final Batch Loss: 0.06939137727022171\n",
      "Epoch 4643, Loss: 0.2841777093708515, Final Batch Loss: 0.08445300161838531\n",
      "Epoch 4644, Loss: 0.21176090463995934, Final Batch Loss: 0.04115515947341919\n",
      "Epoch 4645, Loss: 0.21305475383996964, Final Batch Loss: 0.032829441130161285\n",
      "Epoch 4646, Loss: 0.18715794384479523, Final Batch Loss: 0.04538404569029808\n",
      "Epoch 4647, Loss: 0.19779210165143013, Final Batch Loss: 0.06633090227842331\n",
      "Epoch 4648, Loss: 0.19997086748480797, Final Batch Loss: 0.03809387981891632\n",
      "Epoch 4649, Loss: 0.22165526449680328, Final Batch Loss: 0.025615371763706207\n",
      "Epoch 4650, Loss: 0.2359006591141224, Final Batch Loss: 0.06432098895311356\n",
      "Epoch 4651, Loss: 0.21135706081986427, Final Batch Loss: 0.04338623955845833\n",
      "Epoch 4652, Loss: 0.39815230667591095, Final Batch Loss: 0.13280121982097626\n",
      "Epoch 4653, Loss: 0.2282469253987074, Final Batch Loss: 0.09016221761703491\n",
      "Epoch 4654, Loss: 0.2555866129696369, Final Batch Loss: 0.05404629558324814\n",
      "Epoch 4655, Loss: 0.28951652348041534, Final Batch Loss: 0.0690145418047905\n",
      "Epoch 4656, Loss: 0.284128587692976, Final Batch Loss: 0.10469183325767517\n",
      "Epoch 4657, Loss: 0.2659640908241272, Final Batch Loss: 0.06568136066198349\n",
      "Epoch 4658, Loss: 0.22439336404204369, Final Batch Loss: 0.047620438039302826\n",
      "Epoch 4659, Loss: 0.2438075803220272, Final Batch Loss: 0.02884500101208687\n",
      "Epoch 4660, Loss: 0.20572463795542717, Final Batch Loss: 0.03677342087030411\n",
      "Epoch 4661, Loss: 0.20695358887314796, Final Batch Loss: 0.057207170873880386\n",
      "Epoch 4662, Loss: 0.22015164233744144, Final Batch Loss: 0.06330816447734833\n",
      "Epoch 4663, Loss: 0.2789365276694298, Final Batch Loss: 0.1171732246875763\n",
      "Epoch 4664, Loss: 0.24592192471027374, Final Batch Loss: 0.0656447783112526\n",
      "Epoch 4665, Loss: 0.18708746135234833, Final Batch Loss: 0.039113231003284454\n",
      "Epoch 4666, Loss: 0.19620153680443764, Final Batch Loss: 0.05106474831700325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4667, Loss: 0.18302678875625134, Final Batch Loss: 0.069306880235672\n",
      "Epoch 4668, Loss: 0.23748177289962769, Final Batch Loss: 0.0507817305624485\n",
      "Epoch 4669, Loss: 0.25814731046557426, Final Batch Loss: 0.08309131115674973\n",
      "Epoch 4670, Loss: 0.1642615646123886, Final Batch Loss: 0.039267007261514664\n",
      "Epoch 4671, Loss: 0.20122944191098213, Final Batch Loss: 0.05551576241850853\n",
      "Epoch 4672, Loss: 0.17602654546499252, Final Batch Loss: 0.040526729077100754\n",
      "Epoch 4673, Loss: 0.194038238376379, Final Batch Loss: 0.051228705793619156\n",
      "Epoch 4674, Loss: 0.15291492082178593, Final Batch Loss: 0.02715385891497135\n",
      "Epoch 4675, Loss: 0.12017071805894375, Final Batch Loss: 0.017217664048075676\n",
      "Epoch 4676, Loss: 0.16289905924350023, Final Batch Loss: 0.025547271594405174\n",
      "Epoch 4677, Loss: 0.19753902032971382, Final Batch Loss: 0.0437932088971138\n",
      "Epoch 4678, Loss: 0.12828907556831837, Final Batch Loss: 0.02682717889547348\n",
      "Epoch 4679, Loss: 0.1765955202281475, Final Batch Loss: 0.025808071717619896\n",
      "Epoch 4680, Loss: 0.1903767604380846, Final Batch Loss: 0.01930021494626999\n",
      "Epoch 4681, Loss: 0.213757935911417, Final Batch Loss: 0.038811471313238144\n",
      "Epoch 4682, Loss: 0.2249218113720417, Final Batch Loss: 0.07397237420082092\n",
      "Epoch 4683, Loss: 0.22149057686328888, Final Batch Loss: 0.07498341798782349\n",
      "Epoch 4684, Loss: 0.29449010267853737, Final Batch Loss: 0.187593013048172\n",
      "Epoch 4685, Loss: 0.31100498512387276, Final Batch Loss: 0.1128460094332695\n",
      "Epoch 4686, Loss: 0.28731128945946693, Final Batch Loss: 0.08602768927812576\n",
      "Epoch 4687, Loss: 0.24253026023507118, Final Batch Loss: 0.030636969953775406\n",
      "Epoch 4688, Loss: 0.2477193921804428, Final Batch Loss: 0.034571047872304916\n",
      "Epoch 4689, Loss: 0.22061464935541153, Final Batch Loss: 0.05323564633727074\n",
      "Epoch 4690, Loss: 0.205037709325552, Final Batch Loss: 0.03421064838767052\n",
      "Epoch 4691, Loss: 0.23879441246390343, Final Batch Loss: 0.02472592517733574\n",
      "Epoch 4692, Loss: 0.2230685129761696, Final Batch Loss: 0.06500408053398132\n",
      "Epoch 4693, Loss: 0.2245575599372387, Final Batch Loss: 0.08091193437576294\n",
      "Epoch 4694, Loss: 0.21249065548181534, Final Batch Loss: 0.07570036500692368\n",
      "Epoch 4695, Loss: 0.22032743319869041, Final Batch Loss: 0.05439038947224617\n",
      "Epoch 4696, Loss: 0.17045283876359463, Final Batch Loss: 0.024405570700764656\n",
      "Epoch 4697, Loss: 0.22976473346352577, Final Batch Loss: 0.03581804782152176\n",
      "Epoch 4698, Loss: 0.1809430606663227, Final Batch Loss: 0.04553959518671036\n",
      "Epoch 4699, Loss: 0.3281739205121994, Final Batch Loss: 0.12552902102470398\n",
      "Epoch 4700, Loss: 0.2826397828757763, Final Batch Loss: 0.07330723851919174\n",
      "Epoch 4701, Loss: 0.26308656856417656, Final Batch Loss: 0.07316837459802628\n",
      "Epoch 4702, Loss: 0.16293049231171608, Final Batch Loss: 0.018996508792042732\n",
      "Epoch 4703, Loss: 0.20116239227354527, Final Batch Loss: 0.025885550305247307\n",
      "Epoch 4704, Loss: 0.1656659124419093, Final Batch Loss: 0.010848834179341793\n",
      "Epoch 4705, Loss: 0.3063938431441784, Final Batch Loss: 0.11157288402318954\n",
      "Epoch 4706, Loss: 0.25763173401355743, Final Batch Loss: 0.025918863713741302\n",
      "Epoch 4707, Loss: 0.2229127287864685, Final Batch Loss: 0.03721321001648903\n",
      "Epoch 4708, Loss: 0.2727734297513962, Final Batch Loss: 0.06368957459926605\n",
      "Epoch 4709, Loss: 0.21623941138386726, Final Batch Loss: 0.05618670582771301\n",
      "Epoch 4710, Loss: 0.24317023903131485, Final Batch Loss: 0.08079582452774048\n",
      "Epoch 4711, Loss: 0.24357954785227776, Final Batch Loss: 0.07231923937797546\n",
      "Epoch 4712, Loss: 0.31118613481521606, Final Batch Loss: 0.08187757432460785\n",
      "Epoch 4713, Loss: 0.18639444187283516, Final Batch Loss: 0.01644996553659439\n",
      "Epoch 4714, Loss: 0.20043732598423958, Final Batch Loss: 0.018988320603966713\n",
      "Epoch 4715, Loss: 0.2579747810959816, Final Batch Loss: 0.05819185450673103\n",
      "Epoch 4716, Loss: 0.2081802934408188, Final Batch Loss: 0.049889516085386276\n",
      "Epoch 4717, Loss: 0.18958070874214172, Final Batch Loss: 0.03816291317343712\n",
      "Epoch 4718, Loss: 0.22175972908735275, Final Batch Loss: 0.07912512868642807\n",
      "Epoch 4719, Loss: 0.17084138840436935, Final Batch Loss: 0.027042008936405182\n",
      "Epoch 4720, Loss: 0.1670063715428114, Final Batch Loss: 0.028842350468039513\n",
      "Epoch 4721, Loss: 0.185013173148036, Final Batch Loss: 0.027851657941937447\n",
      "Epoch 4722, Loss: 0.20602858066558838, Final Batch Loss: 0.04384681582450867\n",
      "Epoch 4723, Loss: 0.19782160595059395, Final Batch Loss: 0.04298008233308792\n",
      "Epoch 4724, Loss: 0.16365681402385235, Final Batch Loss: 0.06339125335216522\n",
      "Epoch 4725, Loss: 0.2004793956875801, Final Batch Loss: 0.06328210979700089\n",
      "Epoch 4726, Loss: 0.22198607586324215, Final Batch Loss: 0.06436258554458618\n",
      "Epoch 4727, Loss: 0.1780093889683485, Final Batch Loss: 0.023973265662789345\n",
      "Epoch 4728, Loss: 0.2268229373730719, Final Batch Loss: 0.0055150859989225864\n",
      "Epoch 4729, Loss: 0.2724912669509649, Final Batch Loss: 0.11298524588346481\n",
      "Epoch 4730, Loss: 0.31241506338119507, Final Batch Loss: 0.09521268308162689\n",
      "Epoch 4731, Loss: 0.27466486766934395, Final Batch Loss: 0.0774729922413826\n",
      "Epoch 4732, Loss: 0.21895439364016056, Final Batch Loss: 0.09512612968683243\n",
      "Epoch 4733, Loss: 0.2632913179695606, Final Batch Loss: 0.0566653273999691\n",
      "Epoch 4734, Loss: 0.21247300505638123, Final Batch Loss: 0.04668445512652397\n",
      "Epoch 4735, Loss: 0.23737041279673576, Final Batch Loss: 0.09935301542282104\n",
      "Epoch 4736, Loss: 0.2214707676321268, Final Batch Loss: 0.0281242486089468\n",
      "Epoch 4737, Loss: 0.2705833148211241, Final Batch Loss: 0.04729458689689636\n",
      "Epoch 4738, Loss: 0.25128968618810177, Final Batch Loss: 0.029525691643357277\n",
      "Epoch 4739, Loss: 0.20185651071369648, Final Batch Loss: 0.09825717657804489\n",
      "Epoch 4740, Loss: 0.19689299911260605, Final Batch Loss: 0.062973253428936\n",
      "Epoch 4741, Loss: 0.23147699609398842, Final Batch Loss: 0.05417660251259804\n",
      "Epoch 4742, Loss: 0.2514682002365589, Final Batch Loss: 0.06373349577188492\n",
      "Epoch 4743, Loss: 0.2236480414867401, Final Batch Loss: 0.03118683025240898\n",
      "Epoch 4744, Loss: 0.1836220771074295, Final Batch Loss: 0.053736262023448944\n",
      "Epoch 4745, Loss: 0.25493788719177246, Final Batch Loss: 0.04802463948726654\n",
      "Epoch 4746, Loss: 0.23351161740720272, Final Batch Loss: 0.026727082207798958\n",
      "Epoch 4747, Loss: 0.23919330351054668, Final Batch Loss: 0.023376567289233208\n",
      "Epoch 4748, Loss: 0.16392948664724827, Final Batch Loss: 0.025574257597327232\n",
      "Epoch 4749, Loss: 0.1853011380881071, Final Batch Loss: 0.03432060778141022\n",
      "Epoch 4750, Loss: 0.19394928216934204, Final Batch Loss: 0.024888742715120316\n",
      "Epoch 4751, Loss: 0.17619514092803001, Final Batch Loss: 0.017250053584575653\n",
      "Epoch 4752, Loss: 0.16618643794208765, Final Batch Loss: 0.013535094447433949\n",
      "Epoch 4753, Loss: 0.2285442091524601, Final Batch Loss: 0.050027817487716675\n",
      "Epoch 4754, Loss: 0.24910971149802208, Final Batch Loss: 0.06292504817247391\n",
      "Epoch 4755, Loss: 0.1747488770633936, Final Batch Loss: 0.053545691072940826\n",
      "Epoch 4756, Loss: 0.19353321380913258, Final Batch Loss: 0.019117141142487526\n",
      "Epoch 4757, Loss: 0.1847547683864832, Final Batch Loss: 0.06229139119386673\n",
      "Epoch 4758, Loss: 0.18891974817961454, Final Batch Loss: 0.009670875035226345\n",
      "Epoch 4759, Loss: 0.29606070928275585, Final Batch Loss: 0.011046094819903374\n",
      "Epoch 4760, Loss: 0.2030128724873066, Final Batch Loss: 0.03201276808977127\n",
      "Epoch 4761, Loss: 0.2288549654185772, Final Batch Loss: 0.06546548008918762\n",
      "Epoch 4762, Loss: 0.22582686692476273, Final Batch Loss: 0.05316535755991936\n",
      "Epoch 4763, Loss: 0.24637492559850216, Final Batch Loss: 0.03874242305755615\n",
      "Epoch 4764, Loss: 0.25818759948015213, Final Batch Loss: 0.07592751830816269\n",
      "Epoch 4765, Loss: 0.25028612464666367, Final Batch Loss: 0.07608941197395325\n",
      "Epoch 4766, Loss: 0.16916796239092946, Final Batch Loss: 0.006394535768777132\n",
      "Epoch 4767, Loss: 0.2538672722876072, Final Batch Loss: 0.10291189700365067\n",
      "Epoch 4768, Loss: 0.2508890703320503, Final Batch Loss: 0.055398643016815186\n",
      "Epoch 4769, Loss: 0.31755244731903076, Final Batch Loss: 0.09876060485839844\n",
      "Epoch 4770, Loss: 0.36082706041634083, Final Batch Loss: 0.08174540847539902\n",
      "Epoch 4771, Loss: 0.26902472227811813, Final Batch Loss: 0.05368032678961754\n",
      "Epoch 4772, Loss: 0.1675770841538906, Final Batch Loss: 0.029216445982456207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4773, Loss: 0.2207968570291996, Final Batch Loss: 0.08801305294036865\n",
      "Epoch 4774, Loss: 0.36317945271730423, Final Batch Loss: 0.13764312863349915\n",
      "Epoch 4775, Loss: 0.23556510172784328, Final Batch Loss: 0.053712088614702225\n",
      "Epoch 4776, Loss: 0.25647616293281317, Final Batch Loss: 0.01451414730399847\n",
      "Epoch 4777, Loss: 0.3740028850734234, Final Batch Loss: 0.15430378913879395\n",
      "Epoch 4778, Loss: 0.2793576866388321, Final Batch Loss: 0.06579384207725525\n",
      "Epoch 4779, Loss: 0.2804831825196743, Final Batch Loss: 0.09685635566711426\n",
      "Epoch 4780, Loss: 0.2095327489078045, Final Batch Loss: 0.0521278940141201\n",
      "Epoch 4781, Loss: 0.24790649861097336, Final Batch Loss: 0.05193931981921196\n",
      "Epoch 4782, Loss: 0.1623377464711666, Final Batch Loss: 0.042559996247291565\n",
      "Epoch 4783, Loss: 0.21174162440001965, Final Batch Loss: 0.026323137804865837\n",
      "Epoch 4784, Loss: 0.23907964676618576, Final Batch Loss: 0.03898163512349129\n",
      "Epoch 4785, Loss: 0.2956672254949808, Final Batch Loss: 0.15210336446762085\n",
      "Epoch 4786, Loss: 0.24782777205109596, Final Batch Loss: 0.04377417638897896\n",
      "Epoch 4787, Loss: 0.253357470035553, Final Batch Loss: 0.08818923681974411\n",
      "Epoch 4788, Loss: 0.23011800274252892, Final Batch Loss: 0.036595724523067474\n",
      "Epoch 4789, Loss: 0.29680048674345016, Final Batch Loss: 0.08668886125087738\n",
      "Epoch 4790, Loss: 0.23199494183063507, Final Batch Loss: 0.04054327309131622\n",
      "Epoch 4791, Loss: 0.3007409870624542, Final Batch Loss: 0.11977437883615494\n",
      "Epoch 4792, Loss: 0.16699609905481339, Final Batch Loss: 0.0134211964905262\n",
      "Epoch 4793, Loss: 0.24946821853518486, Final Batch Loss: 0.054214347153902054\n",
      "Epoch 4794, Loss: 0.18168263137340546, Final Batch Loss: 0.03436616435647011\n",
      "Epoch 4795, Loss: 0.14031852688640356, Final Batch Loss: 0.015535335056483746\n",
      "Epoch 4796, Loss: 0.16609402559697628, Final Batch Loss: 0.04062958061695099\n",
      "Epoch 4797, Loss: 0.24544387683272362, Final Batch Loss: 0.07193002104759216\n",
      "Epoch 4798, Loss: 0.206400565803051, Final Batch Loss: 0.04472041875123978\n",
      "Epoch 4799, Loss: 0.234843323007226, Final Batch Loss: 0.021875323727726936\n",
      "Epoch 4800, Loss: 0.32964687049388885, Final Batch Loss: 0.1471475511789322\n",
      "Epoch 4801, Loss: 0.19502877444028854, Final Batch Loss: 0.04107644408941269\n",
      "Epoch 4802, Loss: 0.31050457805395126, Final Batch Loss: 0.0994901955127716\n",
      "Epoch 4803, Loss: 0.1780518228188157, Final Batch Loss: 0.014751684851944447\n",
      "Epoch 4804, Loss: 0.24239523522555828, Final Batch Loss: 0.04118455573916435\n",
      "Epoch 4805, Loss: 0.21685410663485527, Final Batch Loss: 0.03545738756656647\n",
      "Epoch 4806, Loss: 0.1534238215535879, Final Batch Loss: 0.013013215735554695\n",
      "Epoch 4807, Loss: 0.16689715720713139, Final Batch Loss: 0.007492126896977425\n",
      "Epoch 4808, Loss: 0.32475996762514114, Final Batch Loss: 0.10058649629354477\n",
      "Epoch 4809, Loss: 0.2200215794146061, Final Batch Loss: 0.05918451398611069\n",
      "Epoch 4810, Loss: 0.2003585360944271, Final Batch Loss: 0.06803960353136063\n",
      "Epoch 4811, Loss: 0.2247116807848215, Final Batch Loss: 0.017216643318533897\n",
      "Epoch 4812, Loss: 0.35945095866918564, Final Batch Loss: 0.11079772561788559\n",
      "Epoch 4813, Loss: 0.20343101024627686, Final Batch Loss: 0.043312493711709976\n",
      "Epoch 4814, Loss: 0.3392036184668541, Final Batch Loss: 0.09211261570453644\n",
      "Epoch 4815, Loss: 0.15390474535524845, Final Batch Loss: 0.04355451092123985\n",
      "Epoch 4816, Loss: 0.3230801597237587, Final Batch Loss: 0.06811437010765076\n",
      "Epoch 4817, Loss: 0.26090047508478165, Final Batch Loss: 0.07456512749195099\n",
      "Epoch 4818, Loss: 0.3796377256512642, Final Batch Loss: 0.16864529252052307\n",
      "Epoch 4819, Loss: 0.2575166206806898, Final Batch Loss: 0.09916465729475021\n",
      "Epoch 4820, Loss: 0.2909088507294655, Final Batch Loss: 0.09352696686983109\n",
      "Epoch 4821, Loss: 0.2488627154380083, Final Batch Loss: 0.061603184789419174\n",
      "Epoch 4822, Loss: 0.2410401925444603, Final Batch Loss: 0.03862980008125305\n",
      "Epoch 4823, Loss: 0.21645521000027657, Final Batch Loss: 0.07048545032739639\n",
      "Epoch 4824, Loss: 0.16748264990746975, Final Batch Loss: 0.019497955217957497\n",
      "Epoch 4825, Loss: 0.22791152447462082, Final Batch Loss: 0.07796575874090195\n",
      "Epoch 4826, Loss: 0.20389527454972267, Final Batch Loss: 0.061682555824518204\n",
      "Epoch 4827, Loss: 0.24538038671016693, Final Batch Loss: 0.10452607274055481\n",
      "Epoch 4828, Loss: 0.20557588525116444, Final Batch Loss: 0.023848799988627434\n",
      "Epoch 4829, Loss: 0.3193493727594614, Final Batch Loss: 0.1294458955526352\n",
      "Epoch 4830, Loss: 0.20205000415444374, Final Batch Loss: 0.07233511656522751\n",
      "Epoch 4831, Loss: 0.24589853733778, Final Batch Loss: 0.10775302350521088\n",
      "Epoch 4832, Loss: 0.25958308205008507, Final Batch Loss: 0.1062428206205368\n",
      "Epoch 4833, Loss: 0.26320943236351013, Final Batch Loss: 0.028181925415992737\n",
      "Epoch 4834, Loss: 0.2101655751466751, Final Batch Loss: 0.06315059214830399\n",
      "Epoch 4835, Loss: 0.23868111334741116, Final Batch Loss: 0.0511862151324749\n",
      "Epoch 4836, Loss: 0.20086750760674477, Final Batch Loss: 0.05797714740037918\n",
      "Epoch 4837, Loss: 0.1879253750666976, Final Batch Loss: 0.012196111492812634\n",
      "Epoch 4838, Loss: 0.18666736409068108, Final Batch Loss: 0.07181006669998169\n",
      "Epoch 4839, Loss: 0.18408781848847866, Final Batch Loss: 0.05659007653594017\n",
      "Epoch 4840, Loss: 0.14881616597995162, Final Batch Loss: 0.007731014396995306\n",
      "Epoch 4841, Loss: 0.1229760441929102, Final Batch Loss: 0.019838280975818634\n",
      "Epoch 4842, Loss: 0.20792285352945328, Final Batch Loss: 0.0700860247015953\n",
      "Epoch 4843, Loss: 0.16192489489912987, Final Batch Loss: 0.05899212881922722\n",
      "Epoch 4844, Loss: 0.23982786014676094, Final Batch Loss: 0.024370942264795303\n",
      "Epoch 4845, Loss: 0.20861939154565334, Final Batch Loss: 0.07168649882078171\n",
      "Epoch 4846, Loss: 0.24562445655465126, Final Batch Loss: 0.09162446856498718\n",
      "Epoch 4847, Loss: 0.15371943823993206, Final Batch Loss: 0.0469512939453125\n",
      "Epoch 4848, Loss: 0.16314956545829773, Final Batch Loss: 0.044061869382858276\n",
      "Epoch 4849, Loss: 0.14735645148903131, Final Batch Loss: 0.014829804189503193\n",
      "Epoch 4850, Loss: 0.10430985502898693, Final Batch Loss: 0.03720623999834061\n",
      "Epoch 4851, Loss: 0.1703500021249056, Final Batch Loss: 0.028801651671528816\n",
      "Epoch 4852, Loss: 0.1908816657960415, Final Batch Loss: 0.052308496087789536\n",
      "Epoch 4853, Loss: 0.2005823478102684, Final Batch Loss: 0.0502171628177166\n",
      "Epoch 4854, Loss: 0.2164006233215332, Final Batch Loss: 0.0403260663151741\n",
      "Epoch 4855, Loss: 0.17358989361673594, Final Batch Loss: 0.01382590550929308\n",
      "Epoch 4856, Loss: 0.2510147988796234, Final Batch Loss: 0.0494614839553833\n",
      "Epoch 4857, Loss: 0.18512960150837898, Final Batch Loss: 0.031078364700078964\n",
      "Epoch 4858, Loss: 0.2351379543542862, Final Batch Loss: 0.06905382126569748\n",
      "Epoch 4859, Loss: 0.16867808997631073, Final Batch Loss: 0.04153594747185707\n",
      "Epoch 4860, Loss: 0.1942693404853344, Final Batch Loss: 0.06756893545389175\n",
      "Epoch 4861, Loss: 0.12326279003173113, Final Batch Loss: 0.03660201653838158\n",
      "Epoch 4862, Loss: 0.2025512531399727, Final Batch Loss: 0.09312774240970612\n",
      "Epoch 4863, Loss: 0.2424701601266861, Final Batch Loss: 0.08239813894033432\n",
      "Epoch 4864, Loss: 0.17340264096856117, Final Batch Loss: 0.04247673973441124\n",
      "Epoch 4865, Loss: 0.21296118944883347, Final Batch Loss: 0.03340500593185425\n",
      "Epoch 4866, Loss: 0.24379779025912285, Final Batch Loss: 0.0468100979924202\n",
      "Epoch 4867, Loss: 0.26551419496536255, Final Batch Loss: 0.1258719265460968\n",
      "Epoch 4868, Loss: 0.26822374761104584, Final Batch Loss: 0.10401244461536407\n",
      "Epoch 4869, Loss: 0.2375398837029934, Final Batch Loss: 0.04881219193339348\n",
      "Epoch 4870, Loss: 0.26325126737356186, Final Batch Loss: 0.08820068091154099\n",
      "Epoch 4871, Loss: 0.2191102560609579, Final Batch Loss: 0.06910236924886703\n",
      "Epoch 4872, Loss: 0.2518317885696888, Final Batch Loss: 0.05642599239945412\n",
      "Epoch 4873, Loss: 0.18369011208415031, Final Batch Loss: 0.03589094430208206\n",
      "Epoch 4874, Loss: 0.1832382958382368, Final Batch Loss: 0.018048370257019997\n",
      "Epoch 4875, Loss: 0.17879504151642323, Final Batch Loss: 0.028407694771885872\n",
      "Epoch 4876, Loss: 0.1746653113514185, Final Batch Loss: 0.02916669100522995\n",
      "Epoch 4877, Loss: 0.263791736215353, Final Batch Loss: 0.05442415922880173\n",
      "Epoch 4878, Loss: 0.15137296728789806, Final Batch Loss: 0.027023373171687126\n",
      "Epoch 4879, Loss: 0.19506529718637466, Final Batch Loss: 0.06737495958805084\n",
      "Epoch 4880, Loss: 0.12483545765280724, Final Batch Loss: 0.027969352900981903\n",
      "Epoch 4881, Loss: 0.09850914590060711, Final Batch Loss: 0.022526251152157784\n",
      "Epoch 4882, Loss: 0.14132635295391083, Final Batch Loss: 0.03175679221749306\n",
      "Epoch 4883, Loss: 0.2298760563135147, Final Batch Loss: 0.03505626693367958\n",
      "Epoch 4884, Loss: 0.2057010941207409, Final Batch Loss: 0.06337175518274307\n",
      "Epoch 4885, Loss: 0.17796513810753822, Final Batch Loss: 0.05122048035264015\n",
      "Epoch 4886, Loss: 0.24209022894501686, Final Batch Loss: 0.022579368203878403\n",
      "Epoch 4887, Loss: 0.29129406437277794, Final Batch Loss: 0.09467032551765442\n",
      "Epoch 4888, Loss: 0.3138747289776802, Final Batch Loss: 0.1265120804309845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4889, Loss: 0.282918356359005, Final Batch Loss: 0.07383448630571365\n",
      "Epoch 4890, Loss: 0.22802551835775375, Final Batch Loss: 0.055974919348955154\n",
      "Epoch 4891, Loss: 0.22546948865056038, Final Batch Loss: 0.0802459791302681\n",
      "Epoch 4892, Loss: 0.2853939011693001, Final Batch Loss: 0.07723066210746765\n",
      "Epoch 4893, Loss: 0.25692469626665115, Final Batch Loss: 0.08604522049427032\n",
      "Epoch 4894, Loss: 0.22548296302556992, Final Batch Loss: 0.03228488191962242\n",
      "Epoch 4895, Loss: 0.22862008027732372, Final Batch Loss: 0.08308862894773483\n",
      "Epoch 4896, Loss: 0.18693657591938972, Final Batch Loss: 0.0696827694773674\n",
      "Epoch 4897, Loss: 0.16770699247717857, Final Batch Loss: 0.009665338322520256\n",
      "Epoch 4898, Loss: 0.19691535457968712, Final Batch Loss: 0.04835250973701477\n",
      "Epoch 4899, Loss: 0.1897036600857973, Final Batch Loss: 0.03012220375239849\n",
      "Epoch 4900, Loss: 0.23040373623371124, Final Batch Loss: 0.07057143747806549\n",
      "Epoch 4901, Loss: 0.17443430423736572, Final Batch Loss: 0.024742912501096725\n",
      "Epoch 4902, Loss: 0.22523153573274612, Final Batch Loss: 0.050436459481716156\n",
      "Epoch 4903, Loss: 0.21604859456419945, Final Batch Loss: 0.034171462059020996\n",
      "Epoch 4904, Loss: 0.2406169306486845, Final Batch Loss: 0.026596928015351295\n",
      "Epoch 4905, Loss: 0.22694171220064163, Final Batch Loss: 0.053654756397008896\n",
      "Epoch 4906, Loss: 0.1467534638941288, Final Batch Loss: 0.03173351660370827\n",
      "Epoch 4907, Loss: 0.22408970072865486, Final Batch Loss: 0.054044708609580994\n",
      "Epoch 4908, Loss: 0.22978878766298294, Final Batch Loss: 0.04910392686724663\n",
      "Epoch 4909, Loss: 0.14495205506682396, Final Batch Loss: 0.018933391198515892\n",
      "Epoch 4910, Loss: 0.14962142333388329, Final Batch Loss: 0.03857673704624176\n",
      "Epoch 4911, Loss: 0.18917610496282578, Final Batch Loss: 0.05447158217430115\n",
      "Epoch 4912, Loss: 0.2083684243261814, Final Batch Loss: 0.07789750397205353\n",
      "Epoch 4913, Loss: 0.2290892507880926, Final Batch Loss: 0.028575962409377098\n",
      "Epoch 4914, Loss: 0.1802915409207344, Final Batch Loss: 0.04197332262992859\n",
      "Epoch 4915, Loss: 0.16245325654745102, Final Batch Loss: 0.04118123650550842\n",
      "Epoch 4916, Loss: 0.19849942065775394, Final Batch Loss: 0.068880096077919\n",
      "Epoch 4917, Loss: 0.17046438716351986, Final Batch Loss: 0.01818074844777584\n",
      "Epoch 4918, Loss: 0.20071490202099085, Final Batch Loss: 0.01503738109022379\n",
      "Epoch 4919, Loss: 0.18073485046625137, Final Batch Loss: 0.022968530654907227\n",
      "Epoch 4920, Loss: 0.2341040074825287, Final Batch Loss: 0.06837104260921478\n",
      "Epoch 4921, Loss: 0.19976015388965607, Final Batch Loss: 0.054877277463674545\n",
      "Epoch 4922, Loss: 0.24885749444365501, Final Batch Loss: 0.05360258370637894\n",
      "Epoch 4923, Loss: 0.240588691085577, Final Batch Loss: 0.05976297706365585\n",
      "Epoch 4924, Loss: 0.18428661860525608, Final Batch Loss: 0.05862273648381233\n",
      "Epoch 4925, Loss: 0.2302758190780878, Final Batch Loss: 0.1069997102022171\n",
      "Epoch 4926, Loss: 0.21226825006306171, Final Batch Loss: 0.0566515289247036\n",
      "Epoch 4927, Loss: 0.14799763821065426, Final Batch Loss: 0.01077129878103733\n",
      "Epoch 4928, Loss: 0.11818766593933105, Final Batch Loss: 0.022622644901275635\n",
      "Epoch 4929, Loss: 0.2473604492843151, Final Batch Loss: 0.04696642607450485\n",
      "Epoch 4930, Loss: 0.17557043954730034, Final Batch Loss: 0.043367788195610046\n",
      "Epoch 4931, Loss: 0.13944873213768005, Final Batch Loss: 0.018659036606550217\n",
      "Epoch 4932, Loss: 0.1760992631316185, Final Batch Loss: 0.03849347308278084\n",
      "Epoch 4933, Loss: 0.14296683855354786, Final Batch Loss: 0.024625267833471298\n",
      "Epoch 4934, Loss: 0.16373789869248867, Final Batch Loss: 0.0346720889210701\n",
      "Epoch 4935, Loss: 0.23638655245304108, Final Batch Loss: 0.018509909510612488\n",
      "Epoch 4936, Loss: 0.1799665093421936, Final Batch Loss: 0.0679711177945137\n",
      "Epoch 4937, Loss: 0.3491689171642065, Final Batch Loss: 0.04526712745428085\n",
      "Epoch 4938, Loss: 0.1609918735921383, Final Batch Loss: 0.03542083874344826\n",
      "Epoch 4939, Loss: 0.22102264314889908, Final Batch Loss: 0.07409686595201492\n",
      "Epoch 4940, Loss: 0.17075566202402115, Final Batch Loss: 0.058647576719522476\n",
      "Epoch 4941, Loss: 0.25856512412428856, Final Batch Loss: 0.040844131261110306\n",
      "Epoch 4942, Loss: 0.23513538762927055, Final Batch Loss: 0.03307033330202103\n",
      "Epoch 4943, Loss: 0.18391082622110844, Final Batch Loss: 0.009522350504994392\n",
      "Epoch 4944, Loss: 0.150657438673079, Final Batch Loss: 0.012836501933634281\n",
      "Epoch 4945, Loss: 0.4100263640284538, Final Batch Loss: 0.14035190641880035\n",
      "Epoch 4946, Loss: 0.22172313556075096, Final Batch Loss: 0.06559037417173386\n",
      "Epoch 4947, Loss: 0.21606414020061493, Final Batch Loss: 0.055826250463724136\n",
      "Epoch 4948, Loss: 0.2235022634267807, Final Batch Loss: 0.05835236608982086\n",
      "Epoch 4949, Loss: 0.14348765648901463, Final Batch Loss: 0.030440019443631172\n",
      "Epoch 4950, Loss: 0.25241533666849136, Final Batch Loss: 0.09778758138418198\n",
      "Epoch 4951, Loss: 0.2035844437777996, Final Batch Loss: 0.023069187998771667\n",
      "Epoch 4952, Loss: 0.24790409952402115, Final Batch Loss: 0.02903991937637329\n",
      "Epoch 4953, Loss: 0.22747302055358887, Final Batch Loss: 0.027996256947517395\n",
      "Epoch 4954, Loss: 0.21949449554085732, Final Batch Loss: 0.035053737461566925\n",
      "Epoch 4955, Loss: 0.19824447110295296, Final Batch Loss: 0.04378007352352142\n",
      "Epoch 4956, Loss: 0.252670306712389, Final Batch Loss: 0.0628088042140007\n",
      "Epoch 4957, Loss: 0.3432832434773445, Final Batch Loss: 0.1181822344660759\n",
      "Epoch 4958, Loss: 0.25971846282482147, Final Batch Loss: 0.10214720666408539\n",
      "Epoch 4959, Loss: 0.2026586588472128, Final Batch Loss: 0.0181692186743021\n",
      "Epoch 4960, Loss: 0.23316944018006325, Final Batch Loss: 0.10229304432868958\n",
      "Epoch 4961, Loss: 0.23164127394557, Final Batch Loss: 0.09455925226211548\n",
      "Epoch 4962, Loss: 0.283586286008358, Final Batch Loss: 0.030115559697151184\n",
      "Epoch 4963, Loss: 0.2296930644661188, Final Batch Loss: 0.04759221524000168\n",
      "Epoch 4964, Loss: 0.23351888731122017, Final Batch Loss: 0.05434025079011917\n",
      "Epoch 4965, Loss: 0.4199551120400429, Final Batch Loss: 0.2331427037715912\n",
      "Epoch 4966, Loss: 0.36870957911014557, Final Batch Loss: 0.1456712931394577\n",
      "Epoch 4967, Loss: 0.20790927670896053, Final Batch Loss: 0.04006168246269226\n",
      "Epoch 4968, Loss: 0.26882266625761986, Final Batch Loss: 0.11803701519966125\n",
      "Epoch 4969, Loss: 0.2362183891236782, Final Batch Loss: 0.0653122141957283\n",
      "Epoch 4970, Loss: 0.20131433010101318, Final Batch Loss: 0.02640450745820999\n",
      "Epoch 4971, Loss: 0.20559658855199814, Final Batch Loss: 0.04510960355401039\n",
      "Epoch 4972, Loss: 0.22922196425497532, Final Batch Loss: 0.11949887871742249\n",
      "Epoch 4973, Loss: 0.1653045415878296, Final Batch Loss: 0.02118104323744774\n",
      "Epoch 4974, Loss: 0.16863400675356388, Final Batch Loss: 0.015978408977389336\n",
      "Epoch 4975, Loss: 0.25032564625144005, Final Batch Loss: 0.049781061708927155\n",
      "Epoch 4976, Loss: 0.20379475876688957, Final Batch Loss: 0.023062169551849365\n",
      "Epoch 4977, Loss: 0.28658716194331646, Final Batch Loss: 0.09352792799472809\n",
      "Epoch 4978, Loss: 0.24280955642461777, Final Batch Loss: 0.08152741193771362\n",
      "Epoch 4979, Loss: 0.21194113790988922, Final Batch Loss: 0.03916073590517044\n",
      "Epoch 4980, Loss: 0.2215528003871441, Final Batch Loss: 0.06227370724081993\n",
      "Epoch 4981, Loss: 0.20228366553783417, Final Batch Loss: 0.03299538791179657\n",
      "Epoch 4982, Loss: 0.3129163086414337, Final Batch Loss: 0.08126726001501083\n",
      "Epoch 4983, Loss: 0.21343787014484406, Final Batch Loss: 0.058533381670713425\n",
      "Epoch 4984, Loss: 0.2357744313776493, Final Batch Loss: 0.04990750178694725\n",
      "Epoch 4985, Loss: 0.27890627831220627, Final Batch Loss: 0.06371480971574783\n",
      "Epoch 4986, Loss: 0.3533363863825798, Final Batch Loss: 0.17795269191265106\n",
      "Epoch 4987, Loss: 0.3924247920513153, Final Batch Loss: 0.07748095691204071\n",
      "Epoch 4988, Loss: 0.2331407368183136, Final Batch Loss: 0.08295858651399612\n",
      "Epoch 4989, Loss: 0.25240572169423103, Final Batch Loss: 0.032745737582445145\n",
      "Epoch 4990, Loss: 0.2336946241557598, Final Batch Loss: 0.055534519255161285\n",
      "Epoch 4991, Loss: 0.24143996089696884, Final Batch Loss: 0.05566691979765892\n",
      "Epoch 4992, Loss: 0.2029341645538807, Final Batch Loss: 0.05338696762919426\n",
      "Epoch 4993, Loss: 0.24822277203202248, Final Batch Loss: 0.06329213827848434\n",
      "Epoch 4994, Loss: 0.22311578691005707, Final Batch Loss: 0.0350850485265255\n",
      "Epoch 4995, Loss: 0.354224756360054, Final Batch Loss: 0.20627382397651672\n",
      "Epoch 4996, Loss: 0.19326039962470531, Final Batch Loss: 0.015750428661704063\n",
      "Epoch 4997, Loss: 0.1690730769187212, Final Batch Loss: 0.02310994453728199\n",
      "Epoch 4998, Loss: 0.26605192944407463, Final Batch Loss: 0.04425464943051338\n",
      "Epoch 4999, Loss: 0.19745893776416779, Final Batch Loss: 0.05752336233854294\n",
      "Epoch 5000, Loss: 0.13211087603121996, Final Batch Loss: 0.013600767590105534\n",
      "Epoch 5001, Loss: 0.19117743149399757, Final Batch Loss: 0.03814885765314102\n",
      "Epoch 5002, Loss: 0.18172869831323624, Final Batch Loss: 0.07843638211488724\n",
      "Epoch 5003, Loss: 0.2260069027543068, Final Batch Loss: 0.0747939944267273\n",
      "Epoch 5004, Loss: 0.2558124642819166, Final Batch Loss: 0.029013408347964287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5005, Loss: 0.2477523349225521, Final Batch Loss: 0.04115644469857216\n",
      "Epoch 5006, Loss: 0.15925361961126328, Final Batch Loss: 0.0423695333302021\n",
      "Epoch 5007, Loss: 0.22844751924276352, Final Batch Loss: 0.03804026171565056\n",
      "Epoch 5008, Loss: 0.2898798305541277, Final Batch Loss: 0.05518866702914238\n",
      "Epoch 5009, Loss: 0.23388909175992012, Final Batch Loss: 0.039292871952056885\n",
      "Epoch 5010, Loss: 0.2299989890307188, Final Batch Loss: 0.10979680716991425\n",
      "Epoch 5011, Loss: 0.19553674012422562, Final Batch Loss: 0.05749120935797691\n",
      "Epoch 5012, Loss: 0.17891361936926842, Final Batch Loss: 0.058189988136291504\n",
      "Epoch 5013, Loss: 0.14002617076039314, Final Batch Loss: 0.031916696578264236\n",
      "Epoch 5014, Loss: 0.29242871701717377, Final Batch Loss: 0.09880519658327103\n",
      "Epoch 5015, Loss: 0.167243380099535, Final Batch Loss: 0.009898548945784569\n",
      "Epoch 5016, Loss: 0.1439163126051426, Final Batch Loss: 0.027996886521577835\n",
      "Epoch 5017, Loss: 0.22764058038592339, Final Batch Loss: 0.04902792349457741\n",
      "Epoch 5018, Loss: 0.26229033432900906, Final Batch Loss: 0.11762651801109314\n",
      "Epoch 5019, Loss: 0.28067532554268837, Final Batch Loss: 0.05072055384516716\n",
      "Epoch 5020, Loss: 0.21974271908402443, Final Batch Loss: 0.043261658400297165\n",
      "Epoch 5021, Loss: 0.30337337404489517, Final Batch Loss: 0.08587924391031265\n",
      "Epoch 5022, Loss: 0.22850878350436687, Final Batch Loss: 0.06881946325302124\n",
      "Epoch 5023, Loss: 0.3018634058535099, Final Batch Loss: 0.18502911925315857\n",
      "Epoch 5024, Loss: 0.19210374541580677, Final Batch Loss: 0.021132687106728554\n",
      "Epoch 5025, Loss: 0.20050550624728203, Final Batch Loss: 0.049735769629478455\n",
      "Epoch 5026, Loss: 0.2473507635295391, Final Batch Loss: 0.04853047430515289\n",
      "Epoch 5027, Loss: 0.17647993192076683, Final Batch Loss: 0.03526364266872406\n",
      "Epoch 5028, Loss: 0.217333709821105, Final Batch Loss: 0.01489195041358471\n",
      "Epoch 5029, Loss: 0.21603060886263847, Final Batch Loss: 0.040451861917972565\n",
      "Epoch 5030, Loss: 0.16578966937959194, Final Batch Loss: 0.04401177540421486\n",
      "Epoch 5031, Loss: 0.2117523904889822, Final Batch Loss: 0.022595049813389778\n",
      "Epoch 5032, Loss: 0.24134084023535252, Final Batch Loss: 0.06293102353811264\n",
      "Epoch 5033, Loss: 0.21086331084370613, Final Batch Loss: 0.07780879735946655\n",
      "Epoch 5034, Loss: 0.2339884340763092, Final Batch Loss: 0.06826478987932205\n",
      "Epoch 5035, Loss: 0.20277680829167366, Final Batch Loss: 0.032302845269441605\n",
      "Epoch 5036, Loss: 0.19839534349739552, Final Batch Loss: 0.034868936985731125\n",
      "Epoch 5037, Loss: 0.15926503017544746, Final Batch Loss: 0.0494411401450634\n",
      "Epoch 5038, Loss: 0.21418233960866928, Final Batch Loss: 0.08476796746253967\n",
      "Epoch 5039, Loss: 0.1537415124475956, Final Batch Loss: 0.04309942200779915\n",
      "Epoch 5040, Loss: 0.13854994252324104, Final Batch Loss: 0.05091418698430061\n",
      "Epoch 5041, Loss: 0.17682534269988537, Final Batch Loss: 0.04367358610033989\n",
      "Epoch 5042, Loss: 0.3377641402184963, Final Batch Loss: 0.09340641647577286\n",
      "Epoch 5043, Loss: 0.212266955524683, Final Batch Loss: 0.04709915071725845\n",
      "Epoch 5044, Loss: 0.1885332092642784, Final Batch Loss: 0.04571480304002762\n",
      "Epoch 5045, Loss: 0.22850123047828674, Final Batch Loss: 0.04691559448838234\n",
      "Epoch 5046, Loss: 0.18349154852330685, Final Batch Loss: 0.020535556599497795\n",
      "Epoch 5047, Loss: 0.25615840405225754, Final Batch Loss: 0.09645773470401764\n",
      "Epoch 5048, Loss: 0.1874638795852661, Final Batch Loss: 0.033334508538246155\n",
      "Epoch 5049, Loss: 0.24535615742206573, Final Batch Loss: 0.052338216453790665\n",
      "Epoch 5050, Loss: 0.24757497012615204, Final Batch Loss: 0.09692508727312088\n",
      "Epoch 5051, Loss: 0.21297669038176537, Final Batch Loss: 0.024717047810554504\n",
      "Epoch 5052, Loss: 0.16473101638257504, Final Batch Loss: 0.03795931115746498\n",
      "Epoch 5053, Loss: 0.19456675741821527, Final Batch Loss: 0.014463108964264393\n",
      "Epoch 5054, Loss: 0.1548781804740429, Final Batch Loss: 0.0411008782684803\n",
      "Epoch 5055, Loss: 0.21856582164764404, Final Batch Loss: 0.034010808914899826\n",
      "Epoch 5056, Loss: 0.16296039894223213, Final Batch Loss: 0.03257547691464424\n",
      "Epoch 5057, Loss: 0.1457832669839263, Final Batch Loss: 0.07148320972919464\n",
      "Epoch 5058, Loss: 0.1964310184121132, Final Batch Loss: 0.07648645341396332\n",
      "Epoch 5059, Loss: 0.2309846542775631, Final Batch Loss: 0.0745811015367508\n",
      "Epoch 5060, Loss: 0.2365970741957426, Final Batch Loss: 0.0956069752573967\n",
      "Epoch 5061, Loss: 0.23720696941018105, Final Batch Loss: 0.07408082485198975\n",
      "Epoch 5062, Loss: 0.16874318569898605, Final Batch Loss: 0.04701872542500496\n",
      "Epoch 5063, Loss: 0.2254846952855587, Final Batch Loss: 0.033177588135004044\n",
      "Epoch 5064, Loss: 0.2142365165054798, Final Batch Loss: 0.08012289553880692\n",
      "Epoch 5065, Loss: 0.2448381558060646, Final Batch Loss: 0.05516820773482323\n",
      "Epoch 5066, Loss: 0.3248607888817787, Final Batch Loss: 0.13299866020679474\n",
      "Epoch 5067, Loss: 0.23691347241401672, Final Batch Loss: 0.05222496762871742\n",
      "Epoch 5068, Loss: 0.15773380175232887, Final Batch Loss: 0.018605627119541168\n",
      "Epoch 5069, Loss: 0.18869265168905258, Final Batch Loss: 0.0427548848092556\n",
      "Epoch 5070, Loss: 0.3269120939075947, Final Batch Loss: 0.14055794477462769\n",
      "Epoch 5071, Loss: 0.18664555065333843, Final Batch Loss: 0.024373603984713554\n",
      "Epoch 5072, Loss: 0.18744036741554737, Final Batch Loss: 0.009077752009034157\n",
      "Epoch 5073, Loss: 0.26573848724365234, Final Batch Loss: 0.053271617740392685\n",
      "Epoch 5074, Loss: 0.33420826494693756, Final Batch Loss: 0.1260201334953308\n",
      "Epoch 5075, Loss: 0.3016379177570343, Final Batch Loss: 0.1443571299314499\n",
      "Epoch 5076, Loss: 0.25719644874334335, Final Batch Loss: 0.059698686003685\n",
      "Epoch 5077, Loss: 0.26332251727581024, Final Batch Loss: 0.02401886135339737\n",
      "Epoch 5078, Loss: 0.2318182960152626, Final Batch Loss: 0.0436679944396019\n",
      "Epoch 5079, Loss: 0.23380760475993156, Final Batch Loss: 0.07727409899234772\n",
      "Epoch 5080, Loss: 0.2500985376536846, Final Batch Loss: 0.06464838981628418\n",
      "Epoch 5081, Loss: 0.28633712977170944, Final Batch Loss: 0.05352422222495079\n",
      "Epoch 5082, Loss: 0.11188806965947151, Final Batch Loss: 0.023456597700715065\n",
      "Epoch 5083, Loss: 0.19730684906244278, Final Batch Loss: 0.06221625953912735\n",
      "Epoch 5084, Loss: 0.2140011042356491, Final Batch Loss: 0.08070623874664307\n",
      "Epoch 5085, Loss: 0.15982405841350555, Final Batch Loss: 0.0513145849108696\n",
      "Epoch 5086, Loss: 0.20954041928052902, Final Batch Loss: 0.05477916821837425\n",
      "Epoch 5087, Loss: 0.16529334615916014, Final Batch Loss: 0.01278288196772337\n",
      "Epoch 5088, Loss: 0.19303175061941147, Final Batch Loss: 0.047040849924087524\n",
      "Epoch 5089, Loss: 0.2431671880185604, Final Batch Loss: 0.09732906520366669\n",
      "Epoch 5090, Loss: 0.20727894827723503, Final Batch Loss: 0.050105929374694824\n",
      "Epoch 5091, Loss: 0.23283946886658669, Final Batch Loss: 0.0665343701839447\n",
      "Epoch 5092, Loss: 0.24207332730293274, Final Batch Loss: 0.031842853873968124\n",
      "Epoch 5093, Loss: 0.2793502099812031, Final Batch Loss: 0.061971958726644516\n",
      "Epoch 5094, Loss: 0.30785898491740227, Final Batch Loss: 0.09811220318078995\n",
      "Epoch 5095, Loss: 0.22286896780133247, Final Batch Loss: 0.0815860703587532\n",
      "Epoch 5096, Loss: 0.3148534744977951, Final Batch Loss: 0.0787663385272026\n",
      "Epoch 5097, Loss: 0.19550317525863647, Final Batch Loss: 0.03840586543083191\n",
      "Epoch 5098, Loss: 0.20490997843444347, Final Batch Loss: 0.022562788799405098\n",
      "Epoch 5099, Loss: 0.27861836180090904, Final Batch Loss: 0.061546966433525085\n",
      "Epoch 5100, Loss: 0.25217775255441666, Final Batch Loss: 0.046846386045217514\n",
      "Epoch 5101, Loss: 0.1986198853701353, Final Batch Loss: 0.03222603350877762\n",
      "Epoch 5102, Loss: 0.28396208956837654, Final Batch Loss: 0.052386846393346786\n",
      "Epoch 5103, Loss: 0.2532144524157047, Final Batch Loss: 0.0953335091471672\n",
      "Epoch 5104, Loss: 0.22462018951773643, Final Batch Loss: 0.07890652865171432\n",
      "Epoch 5105, Loss: 0.2412159163504839, Final Batch Loss: 0.10360655188560486\n",
      "Epoch 5106, Loss: 0.3071439526975155, Final Batch Loss: 0.06295927613973618\n",
      "Epoch 5107, Loss: 0.2779042087495327, Final Batch Loss: 0.0812954232096672\n",
      "Epoch 5108, Loss: 0.26860618218779564, Final Batch Loss: 0.0918796956539154\n",
      "Epoch 5109, Loss: 0.349467433989048, Final Batch Loss: 0.07467399537563324\n",
      "Epoch 5110, Loss: 0.24756696075201035, Final Batch Loss: 0.07024823874235153\n",
      "Epoch 5111, Loss: 0.3245916962623596, Final Batch Loss: 0.07239969819784164\n",
      "Epoch 5112, Loss: 0.19038619101047516, Final Batch Loss: 0.026507146656513214\n",
      "Epoch 5113, Loss: 0.2585441283881664, Final Batch Loss: 0.059611208736896515\n",
      "Epoch 5114, Loss: 0.21135160513222218, Final Batch Loss: 0.03368458151817322\n",
      "Epoch 5115, Loss: 0.2263077050447464, Final Batch Loss: 0.036425139755010605\n",
      "Epoch 5116, Loss: 0.28415394201874733, Final Batch Loss: 0.07895391434431076\n",
      "Epoch 5117, Loss: 0.26560845971107483, Final Batch Loss: 0.05456112325191498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5118, Loss: 0.1783575303852558, Final Batch Loss: 0.041335854679346085\n",
      "Epoch 5119, Loss: 0.16422443836927414, Final Batch Loss: 0.05303702503442764\n",
      "Epoch 5120, Loss: 0.24502107128500938, Final Batch Loss: 0.05116872489452362\n",
      "Epoch 5121, Loss: 0.1839573010802269, Final Batch Loss: 0.04136120527982712\n",
      "Epoch 5122, Loss: 0.29189224168658257, Final Batch Loss: 0.06254927814006805\n",
      "Epoch 5123, Loss: 0.19157710671424866, Final Batch Loss: 0.0432608425617218\n",
      "Epoch 5124, Loss: 0.15053152106702328, Final Batch Loss: 0.035712577402591705\n",
      "Epoch 5125, Loss: 0.21648633480072021, Final Batch Loss: 0.05276995524764061\n",
      "Epoch 5126, Loss: 0.2656578831374645, Final Batch Loss: 0.07166547328233719\n",
      "Epoch 5127, Loss: 0.2652715593576431, Final Batch Loss: 0.08562021702528\n",
      "Epoch 5128, Loss: 0.204579409211874, Final Batch Loss: 0.02737252786755562\n",
      "Epoch 5129, Loss: 0.33155687153339386, Final Batch Loss: 0.09051159769296646\n",
      "Epoch 5130, Loss: 0.26637476682662964, Final Batch Loss: 0.06103190407156944\n",
      "Epoch 5131, Loss: 0.22568392753601074, Final Batch Loss: 0.0676686018705368\n",
      "Epoch 5132, Loss: 0.22263645566999912, Final Batch Loss: 0.0641358271241188\n",
      "Epoch 5133, Loss: 0.20679155737161636, Final Batch Loss: 0.06205539032816887\n",
      "Epoch 5134, Loss: 0.2634875178337097, Final Batch Loss: 0.09415703266859055\n",
      "Epoch 5135, Loss: 0.17872221861034632, Final Batch Loss: 0.01340507809072733\n",
      "Epoch 5136, Loss: 0.15145592764019966, Final Batch Loss: 0.0584084652364254\n",
      "Epoch 5137, Loss: 0.22289501316845417, Final Batch Loss: 0.028448445722460747\n",
      "Epoch 5138, Loss: 0.18263499066233635, Final Batch Loss: 0.035207442939281464\n",
      "Epoch 5139, Loss: 0.21844731457531452, Final Batch Loss: 0.05868954211473465\n",
      "Epoch 5140, Loss: 0.30471326783299446, Final Batch Loss: 0.11840172857046127\n",
      "Epoch 5141, Loss: 0.207444716244936, Final Batch Loss: 0.05007322132587433\n",
      "Epoch 5142, Loss: 0.16396784596145153, Final Batch Loss: 0.03063870035111904\n",
      "Epoch 5143, Loss: 0.2816109452396631, Final Batch Loss: 0.10433611273765564\n",
      "Epoch 5144, Loss: 0.30826836824417114, Final Batch Loss: 0.07735265791416168\n",
      "Epoch 5145, Loss: 0.23908591642975807, Final Batch Loss: 0.03243354335427284\n",
      "Epoch 5146, Loss: 0.22618304938077927, Final Batch Loss: 0.020088236778974533\n",
      "Epoch 5147, Loss: 0.19068768247961998, Final Batch Loss: 0.04599221050739288\n",
      "Epoch 5148, Loss: 0.17305438965559006, Final Batch Loss: 0.04922981560230255\n",
      "Epoch 5149, Loss: 0.16085176542401314, Final Batch Loss: 0.06452787667512894\n",
      "Epoch 5150, Loss: 0.17463749647140503, Final Batch Loss: 0.010936085134744644\n",
      "Epoch 5151, Loss: 0.195553382858634, Final Batch Loss: 0.04438391327857971\n",
      "Epoch 5152, Loss: 0.1499414900317788, Final Batch Loss: 0.014566921629011631\n",
      "Epoch 5153, Loss: 0.2605234272778034, Final Batch Loss: 0.06711728870868683\n",
      "Epoch 5154, Loss: 0.295229971408844, Final Batch Loss: 0.13784481585025787\n",
      "Epoch 5155, Loss: 0.19099028781056404, Final Batch Loss: 0.01920943334698677\n",
      "Epoch 5156, Loss: 0.20861956104636192, Final Batch Loss: 0.02225145697593689\n",
      "Epoch 5157, Loss: 0.19078005384653807, Final Batch Loss: 0.01360123511403799\n",
      "Epoch 5158, Loss: 0.14246184844523668, Final Batch Loss: 0.013231958262622356\n",
      "Epoch 5159, Loss: 0.19456948712468147, Final Batch Loss: 0.023346424102783203\n",
      "Epoch 5160, Loss: 0.19524599984288216, Final Batch Loss: 0.05441133677959442\n",
      "Epoch 5161, Loss: 0.21224279701709747, Final Batch Loss: 0.0849890485405922\n",
      "Epoch 5162, Loss: 0.2075807899236679, Final Batch Loss: 0.023916278034448624\n",
      "Epoch 5163, Loss: 0.23388101905584335, Final Batch Loss: 0.06551878154277802\n",
      "Epoch 5164, Loss: 0.15550624299794436, Final Batch Loss: 0.00905474554747343\n",
      "Epoch 5165, Loss: 0.4351711980998516, Final Batch Loss: 0.1942305564880371\n",
      "Epoch 5166, Loss: 0.23549464717507362, Final Batch Loss: 0.07630589604377747\n",
      "Epoch 5167, Loss: 0.15985065698623657, Final Batch Loss: 0.014697100967168808\n",
      "Epoch 5168, Loss: 0.28809748589992523, Final Batch Loss: 0.08601595461368561\n",
      "Epoch 5169, Loss: 0.20878202840685844, Final Batch Loss: 0.053475599735975266\n",
      "Epoch 5170, Loss: 0.2685615196824074, Final Batch Loss: 0.10849445313215256\n",
      "Epoch 5171, Loss: 0.14857016876339912, Final Batch Loss: 0.031138157472014427\n",
      "Epoch 5172, Loss: 0.2247365526854992, Final Batch Loss: 0.06175261735916138\n",
      "Epoch 5173, Loss: 0.24939429014921188, Final Batch Loss: 0.09094370156526566\n",
      "Epoch 5174, Loss: 0.18706156313419342, Final Batch Loss: 0.02389267459511757\n",
      "Epoch 5175, Loss: 0.26327529177069664, Final Batch Loss: 0.0727054700255394\n",
      "Epoch 5176, Loss: 0.21878809668123722, Final Batch Loss: 0.02974199689924717\n",
      "Epoch 5177, Loss: 0.23773089051246643, Final Batch Loss: 0.041282910853624344\n",
      "Epoch 5178, Loss: 0.2391200102865696, Final Batch Loss: 0.06657660752534866\n",
      "Epoch 5179, Loss: 0.19115052605047822, Final Batch Loss: 0.005517559591680765\n",
      "Epoch 5180, Loss: 0.2618195302784443, Final Batch Loss: 0.08678184449672699\n",
      "Epoch 5181, Loss: 0.21049540862441063, Final Batch Loss: 0.06412323564291\n",
      "Epoch 5182, Loss: 0.42816729098558426, Final Batch Loss: 0.16414709389209747\n",
      "Epoch 5183, Loss: 0.25693779066205025, Final Batch Loss: 0.10300347208976746\n",
      "Epoch 5184, Loss: 0.20569398254156113, Final Batch Loss: 0.04850710928440094\n",
      "Epoch 5185, Loss: 0.31550171226263046, Final Batch Loss: 0.03959241509437561\n",
      "Epoch 5186, Loss: 0.22332944069057703, Final Batch Loss: 0.010090199299156666\n",
      "Epoch 5187, Loss: 0.21631800383329391, Final Batch Loss: 0.05274096131324768\n",
      "Epoch 5188, Loss: 0.2005468886345625, Final Batch Loss: 0.04957761988043785\n",
      "Epoch 5189, Loss: 0.16669991612434387, Final Batch Loss: 0.02605695277452469\n",
      "Epoch 5190, Loss: 0.15416868031024933, Final Batch Loss: 0.02419554814696312\n",
      "Epoch 5191, Loss: 0.21043143048882484, Final Batch Loss: 0.03917960077524185\n",
      "Epoch 5192, Loss: 0.18742401245981455, Final Batch Loss: 0.013057560659945011\n",
      "Epoch 5193, Loss: 0.3497476577758789, Final Batch Loss: 0.05474729463458061\n",
      "Epoch 5194, Loss: 0.2266475036740303, Final Batch Loss: 0.045642443001270294\n",
      "Epoch 5195, Loss: 0.1761682815849781, Final Batch Loss: 0.04691840708255768\n",
      "Epoch 5196, Loss: 0.15603461069986224, Final Batch Loss: 0.006679426413029432\n",
      "Epoch 5197, Loss: 0.2218530159443617, Final Batch Loss: 0.030245205387473106\n",
      "Epoch 5198, Loss: 0.2353412713855505, Final Batch Loss: 0.03752655163407326\n",
      "Epoch 5199, Loss: 0.15551334246993065, Final Batch Loss: 0.0631951242685318\n",
      "Epoch 5200, Loss: 0.1846868395805359, Final Batch Loss: 0.03792308643460274\n",
      "Epoch 5201, Loss: 0.22347545623779297, Final Batch Loss: 0.02518310397863388\n",
      "Epoch 5202, Loss: 0.21502557024359703, Final Batch Loss: 0.054674699902534485\n",
      "Epoch 5203, Loss: 0.2099006623029709, Final Batch Loss: 0.021958526223897934\n",
      "Epoch 5204, Loss: 0.12295783869922161, Final Batch Loss: 0.04125197231769562\n",
      "Epoch 5205, Loss: 0.31033259630203247, Final Batch Loss: 0.1139933168888092\n",
      "Epoch 5206, Loss: 0.1853676475584507, Final Batch Loss: 0.07833671569824219\n",
      "Epoch 5207, Loss: 0.22796383500099182, Final Batch Loss: 0.04345782473683357\n",
      "Epoch 5208, Loss: 0.2648075371980667, Final Batch Loss: 0.08461090922355652\n",
      "Epoch 5209, Loss: 0.1906587854027748, Final Batch Loss: 0.06884468346834183\n",
      "Epoch 5210, Loss: 0.12960640527307987, Final Batch Loss: 0.03308780491352081\n",
      "Epoch 5211, Loss: 0.1256694532930851, Final Batch Loss: 0.017987702041864395\n",
      "Epoch 5212, Loss: 0.19012268260121346, Final Batch Loss: 0.07367806136608124\n",
      "Epoch 5213, Loss: 0.141621308401227, Final Batch Loss: 0.04151749238371849\n",
      "Epoch 5214, Loss: 0.3112947214394808, Final Batch Loss: 0.09685248136520386\n",
      "Epoch 5215, Loss: 0.1403997540473938, Final Batch Loss: 0.05997953191399574\n",
      "Epoch 5216, Loss: 0.3085087276995182, Final Batch Loss: 0.047655556350946426\n",
      "Epoch 5217, Loss: 0.23296676576137543, Final Batch Loss: 0.05902671441435814\n",
      "Epoch 5218, Loss: 0.1963440217077732, Final Batch Loss: 0.03128733113408089\n",
      "Epoch 5219, Loss: 0.30437053740024567, Final Batch Loss: 0.06823340803384781\n",
      "Epoch 5220, Loss: 0.40871845558285713, Final Batch Loss: 0.1323336660861969\n",
      "Epoch 5221, Loss: 0.1988334059715271, Final Batch Loss: 0.016029685735702515\n",
      "Epoch 5222, Loss: 0.20255016442388296, Final Batch Loss: 0.015227398835122585\n",
      "Epoch 5223, Loss: 0.25003739818930626, Final Batch Loss: 0.07118429243564606\n",
      "Epoch 5224, Loss: 0.16041303053498268, Final Batch Loss: 0.01267869770526886\n",
      "Epoch 5225, Loss: 0.2188033815473318, Final Batch Loss: 0.04853665828704834\n",
      "Epoch 5226, Loss: 0.2377476654946804, Final Batch Loss: 0.09434333443641663\n",
      "Epoch 5227, Loss: 0.2035411186516285, Final Batch Loss: 0.07383224368095398\n",
      "Epoch 5228, Loss: 0.21007542498409748, Final Batch Loss: 0.07726854085922241\n",
      "Epoch 5229, Loss: 0.173336336389184, Final Batch Loss: 0.022398822009563446\n",
      "Epoch 5230, Loss: 0.16566353105008602, Final Batch Loss: 0.012457149103283882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5231, Loss: 0.20433053374290466, Final Batch Loss: 0.06139754131436348\n",
      "Epoch 5232, Loss: 0.15487812831997871, Final Batch Loss: 0.03299613669514656\n",
      "Epoch 5233, Loss: 0.22105643153190613, Final Batch Loss: 0.06422998011112213\n",
      "Epoch 5234, Loss: 0.32490890845656395, Final Batch Loss: 0.11084157228469849\n",
      "Epoch 5235, Loss: 0.20381151512265205, Final Batch Loss: 0.03467395901679993\n",
      "Epoch 5236, Loss: 0.2157655730843544, Final Batch Loss: 0.08978471904993057\n",
      "Epoch 5237, Loss: 0.1762470155954361, Final Batch Loss: 0.03024245798587799\n",
      "Epoch 5238, Loss: 0.2189681176096201, Final Batch Loss: 0.009293192997574806\n",
      "Epoch 5239, Loss: 0.14318006485700607, Final Batch Loss: 0.034335821866989136\n",
      "Epoch 5240, Loss: 0.244162168353796, Final Batch Loss: 0.06256555020809174\n",
      "Epoch 5241, Loss: 0.1866629607975483, Final Batch Loss: 0.014877863228321075\n",
      "Epoch 5242, Loss: 0.24964479729533195, Final Batch Loss: 0.05513709411025047\n",
      "Epoch 5243, Loss: 0.19789197854697704, Final Batch Loss: 0.018775610253214836\n",
      "Epoch 5244, Loss: 0.16870081052184105, Final Batch Loss: 0.031086649745702744\n",
      "Epoch 5245, Loss: 0.27417211793363094, Final Batch Loss: 0.1316291093826294\n",
      "Epoch 5246, Loss: 0.21723809093236923, Final Batch Loss: 0.04486754164099693\n",
      "Epoch 5247, Loss: 0.24005963653326035, Final Batch Loss: 0.04795162007212639\n",
      "Epoch 5248, Loss: 0.21802596747875214, Final Batch Loss: 0.026957273483276367\n",
      "Epoch 5249, Loss: 0.2920460067689419, Final Batch Loss: 0.09785612672567368\n",
      "Epoch 5250, Loss: 0.20833041332662106, Final Batch Loss: 0.024759696796536446\n",
      "Epoch 5251, Loss: 0.2457369640469551, Final Batch Loss: 0.07309728860855103\n",
      "Epoch 5252, Loss: 0.17420871555805206, Final Batch Loss: 0.06457657366991043\n",
      "Epoch 5253, Loss: 0.11804033815860748, Final Batch Loss: 0.01656411588191986\n",
      "Epoch 5254, Loss: 0.13626894354820251, Final Batch Loss: 0.032327886670827866\n",
      "Epoch 5255, Loss: 0.14442353509366512, Final Batch Loss: 0.03904053941369057\n",
      "Epoch 5256, Loss: 0.17169858887791634, Final Batch Loss: 0.03741450607776642\n",
      "Epoch 5257, Loss: 0.25710029155015945, Final Batch Loss: 0.14172938466072083\n",
      "Epoch 5258, Loss: 0.2762736789882183, Final Batch Loss: 0.10177529603242874\n",
      "Epoch 5259, Loss: 0.2839405722916126, Final Batch Loss: 0.06989657878875732\n",
      "Epoch 5260, Loss: 0.18564186850562692, Final Batch Loss: 0.004774103406816721\n",
      "Epoch 5261, Loss: 0.36099200416356325, Final Batch Loss: 0.015129481442272663\n",
      "Epoch 5262, Loss: 0.23769639059901237, Final Batch Loss: 0.05107977241277695\n",
      "Epoch 5263, Loss: 0.33640795573592186, Final Batch Loss: 0.15158139169216156\n",
      "Epoch 5264, Loss: 0.19993088021874428, Final Batch Loss: 0.04362262040376663\n",
      "Epoch 5265, Loss: 0.3568159304559231, Final Batch Loss: 0.11758602410554886\n",
      "Epoch 5266, Loss: 0.2706960253417492, Final Batch Loss: 0.09339820593595505\n",
      "Epoch 5267, Loss: 0.2294789757579565, Final Batch Loss: 0.041532136499881744\n",
      "Epoch 5268, Loss: 0.25716523081064224, Final Batch Loss: 0.06676918268203735\n",
      "Epoch 5269, Loss: 0.2340477854013443, Final Batch Loss: 0.0601673349738121\n",
      "Epoch 5270, Loss: 0.2268748739734292, Final Batch Loss: 0.011327994056046009\n",
      "Epoch 5271, Loss: 0.17424932308495045, Final Batch Loss: 0.02380809746682644\n",
      "Epoch 5272, Loss: 0.2129870094358921, Final Batch Loss: 0.05603336915373802\n",
      "Epoch 5273, Loss: 0.2517583779990673, Final Batch Loss: 0.05426329746842384\n",
      "Epoch 5274, Loss: 0.27474845573306084, Final Batch Loss: 0.06178742274641991\n",
      "Epoch 5275, Loss: 0.1779633667320013, Final Batch Loss: 0.02422712929546833\n",
      "Epoch 5276, Loss: 0.30171341076493263, Final Batch Loss: 0.11465350538492203\n",
      "Epoch 5277, Loss: 0.22390847466886044, Final Batch Loss: 0.022635741159319878\n",
      "Epoch 5278, Loss: 0.22936637327075005, Final Batch Loss: 0.03258047625422478\n",
      "Epoch 5279, Loss: 0.16205938905477524, Final Batch Loss: 0.02904214709997177\n",
      "Epoch 5280, Loss: 0.22336898744106293, Final Batch Loss: 0.06075318902730942\n",
      "Epoch 5281, Loss: 0.2983303591609001, Final Batch Loss: 0.05739428848028183\n",
      "Epoch 5282, Loss: 0.317752406001091, Final Batch Loss: 0.046502090990543365\n",
      "Epoch 5283, Loss: 0.37902140244841576, Final Batch Loss: 0.18766166269779205\n",
      "Epoch 5284, Loss: 0.2899363376200199, Final Batch Loss: 0.0815737247467041\n",
      "Epoch 5285, Loss: 0.22973410040140152, Final Batch Loss: 0.03727931156754494\n",
      "Epoch 5286, Loss: 0.2831846512854099, Final Batch Loss: 0.05421178787946701\n",
      "Epoch 5287, Loss: 0.21364108100533485, Final Batch Loss: 0.06216147541999817\n",
      "Epoch 5288, Loss: 0.2562496215105057, Final Batch Loss: 0.04706951975822449\n",
      "Epoch 5289, Loss: 0.2522451691329479, Final Batch Loss: 0.058734968304634094\n",
      "Epoch 5290, Loss: 0.1950678601861, Final Batch Loss: 0.04291210696101189\n",
      "Epoch 5291, Loss: 0.1854876819998026, Final Batch Loss: 0.017956392839550972\n",
      "Epoch 5292, Loss: 0.23088568821549416, Final Batch Loss: 0.02250061184167862\n",
      "Epoch 5293, Loss: 0.21932477876544, Final Batch Loss: 0.09090588241815567\n",
      "Epoch 5294, Loss: 0.23528348468244076, Final Batch Loss: 0.0777120590209961\n",
      "Epoch 5295, Loss: 0.3037821352481842, Final Batch Loss: 0.06275127083063126\n",
      "Epoch 5296, Loss: 0.17936690337955952, Final Batch Loss: 0.042860034853219986\n",
      "Epoch 5297, Loss: 0.26792558282613754, Final Batch Loss: 0.11435357481241226\n",
      "Epoch 5298, Loss: 0.14388025365769863, Final Batch Loss: 0.01457187905907631\n",
      "Epoch 5299, Loss: 0.2405063733458519, Final Batch Loss: 0.08413664251565933\n",
      "Epoch 5300, Loss: 0.21328646875917912, Final Batch Loss: 0.061636243015527725\n",
      "Epoch 5301, Loss: 0.24821874126791954, Final Batch Loss: 0.10011544823646545\n",
      "Epoch 5302, Loss: 0.23363034427165985, Final Batch Loss: 0.08444464951753616\n",
      "Epoch 5303, Loss: 0.22825855389237404, Final Batch Loss: 0.0661986693739891\n",
      "Epoch 5304, Loss: 0.1877129264175892, Final Batch Loss: 0.014870937913656235\n",
      "Epoch 5305, Loss: 0.4637732692062855, Final Batch Loss: 0.26468417048454285\n",
      "Epoch 5306, Loss: 0.15714542753994465, Final Batch Loss: 0.03280099853873253\n",
      "Epoch 5307, Loss: 0.2233375757932663, Final Batch Loss: 0.057585664093494415\n",
      "Epoch 5308, Loss: 0.19434625282883644, Final Batch Loss: 0.06816957145929337\n",
      "Epoch 5309, Loss: 0.21763775870203972, Final Batch Loss: 0.04074377939105034\n",
      "Epoch 5310, Loss: 0.20125672221183777, Final Batch Loss: 0.07220014929771423\n",
      "Epoch 5311, Loss: 0.2109360545873642, Final Batch Loss: 0.08458728343248367\n",
      "Epoch 5312, Loss: 0.18960281275212765, Final Batch Loss: 0.03765673190355301\n",
      "Epoch 5313, Loss: 0.1969447359442711, Final Batch Loss: 0.10792533308267593\n",
      "Epoch 5314, Loss: 0.22087102010846138, Final Batch Loss: 0.06668392568826675\n",
      "Epoch 5315, Loss: 0.16799579933285713, Final Batch Loss: 0.04137653857469559\n",
      "Epoch 5316, Loss: 0.21150505915284157, Final Batch Loss: 0.04792662337422371\n",
      "Epoch 5317, Loss: 0.24077338352799416, Final Batch Loss: 0.038234684616327286\n",
      "Epoch 5318, Loss: 0.31728533655405045, Final Batch Loss: 0.08436410129070282\n",
      "Epoch 5319, Loss: 0.2687837090343237, Final Batch Loss: 0.021513862535357475\n",
      "Epoch 5320, Loss: 0.2727702334523201, Final Batch Loss: 0.018972691148519516\n",
      "Epoch 5321, Loss: 0.21963290125131607, Final Batch Loss: 0.02998265251517296\n",
      "Epoch 5322, Loss: 0.23950029537081718, Final Batch Loss: 0.04587370902299881\n",
      "Epoch 5323, Loss: 0.22954021021723747, Final Batch Loss: 0.07595479488372803\n",
      "Epoch 5324, Loss: 0.1851563397794962, Final Batch Loss: 0.02542181871831417\n",
      "Epoch 5325, Loss: 0.20769894123077393, Final Batch Loss: 0.04492796212434769\n",
      "Epoch 5326, Loss: 0.17994173988699913, Final Batch Loss: 0.03601997718214989\n",
      "Epoch 5327, Loss: 0.21534442901611328, Final Batch Loss: 0.07804442197084427\n",
      "Epoch 5328, Loss: 0.1959216259419918, Final Batch Loss: 0.05157675966620445\n",
      "Epoch 5329, Loss: 0.20771756395697594, Final Batch Loss: 0.04225015640258789\n",
      "Epoch 5330, Loss: 0.17388064041733742, Final Batch Loss: 0.026275858283042908\n",
      "Epoch 5331, Loss: 0.22287916764616966, Final Batch Loss: 0.050062183290719986\n",
      "Epoch 5332, Loss: 0.1459427773952484, Final Batch Loss: 0.044415757060050964\n",
      "Epoch 5333, Loss: 0.18971059657633305, Final Batch Loss: 0.024031778797507286\n",
      "Epoch 5334, Loss: 0.23261688090860844, Final Batch Loss: 0.09675126522779465\n",
      "Epoch 5335, Loss: 0.14452585205435753, Final Batch Loss: 0.02438349463045597\n",
      "Epoch 5336, Loss: 0.29040852189064026, Final Batch Loss: 0.09751526266336441\n",
      "Epoch 5337, Loss: 0.20252758264541626, Final Batch Loss: 0.025464780628681183\n",
      "Epoch 5338, Loss: 0.2138625606894493, Final Batch Loss: 0.08058720082044601\n",
      "Epoch 5339, Loss: 0.2819562666118145, Final Batch Loss: 0.11550971120595932\n",
      "Epoch 5340, Loss: 0.19803703390061855, Final Batch Loss: 0.07702664285898209\n",
      "Epoch 5341, Loss: 0.28138571605086327, Final Batch Loss: 0.10917671024799347\n",
      "Epoch 5342, Loss: 0.2512577623128891, Final Batch Loss: 0.04309731721878052\n",
      "Epoch 5343, Loss: 0.18930544704198837, Final Batch Loss: 0.017826447263360023\n",
      "Epoch 5344, Loss: 0.35025832802057266, Final Batch Loss: 0.14956215023994446\n",
      "Epoch 5345, Loss: 0.28476378694176674, Final Batch Loss: 0.04170200601220131\n",
      "Epoch 5346, Loss: 0.31216856837272644, Final Batch Loss: 0.13494902849197388\n",
      "Epoch 5347, Loss: 0.23531613498926163, Final Batch Loss: 0.04356728494167328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5348, Loss: 0.22724519483745098, Final Batch Loss: 0.02859140746295452\n",
      "Epoch 5349, Loss: 0.28812649101018906, Final Batch Loss: 0.08609700202941895\n",
      "Epoch 5350, Loss: 0.165925451554358, Final Batch Loss: 0.011995592154562473\n",
      "Epoch 5351, Loss: 0.18903821147978306, Final Batch Loss: 0.048116546124219894\n",
      "Epoch 5352, Loss: 0.23055854439735413, Final Batch Loss: 0.08146700263023376\n",
      "Epoch 5353, Loss: 0.29414742067456245, Final Batch Loss: 0.12617553770542145\n",
      "Epoch 5354, Loss: 0.20747527107596397, Final Batch Loss: 0.051607001572847366\n",
      "Epoch 5355, Loss: 0.22039748914539814, Final Batch Loss: 0.11206915229558945\n",
      "Epoch 5356, Loss: 0.16281474754214287, Final Batch Loss: 0.041216738522052765\n",
      "Epoch 5357, Loss: 0.22078948095440865, Final Batch Loss: 0.04724397882819176\n",
      "Epoch 5358, Loss: 0.23412762954831123, Final Batch Loss: 0.05192030221223831\n",
      "Epoch 5359, Loss: 0.25446208380162716, Final Batch Loss: 0.06808885931968689\n",
      "Epoch 5360, Loss: 0.28071456030011177, Final Batch Loss: 0.13984493911266327\n",
      "Epoch 5361, Loss: 0.20244517177343369, Final Batch Loss: 0.04755108058452606\n",
      "Epoch 5362, Loss: 0.2551692631095648, Final Batch Loss: 0.1233542189002037\n",
      "Epoch 5363, Loss: 0.15429313853383064, Final Batch Loss: 0.0347549133002758\n",
      "Epoch 5364, Loss: 0.20401395484805107, Final Batch Loss: 0.044379785656929016\n",
      "Epoch 5365, Loss: 0.21008814498782158, Final Batch Loss: 0.043629880994558334\n",
      "Epoch 5366, Loss: 0.1741181630641222, Final Batch Loss: 0.05494934692978859\n",
      "Epoch 5367, Loss: 0.17017317190766335, Final Batch Loss: 0.020556755363941193\n",
      "Epoch 5368, Loss: 0.16969599667936563, Final Batch Loss: 0.014687404967844486\n",
      "Epoch 5369, Loss: 0.22208420187234879, Final Batch Loss: 0.07082308083772659\n",
      "Epoch 5370, Loss: 0.2412574328482151, Final Batch Loss: 0.10266394168138504\n",
      "Epoch 5371, Loss: 0.15904277376830578, Final Batch Loss: 0.0074403975158929825\n",
      "Epoch 5372, Loss: 0.254335381090641, Final Batch Loss: 0.02635134384036064\n",
      "Epoch 5373, Loss: 0.24746174737811089, Final Batch Loss: 0.04239093139767647\n",
      "Epoch 5374, Loss: 0.16719773039221764, Final Batch Loss: 0.015017908066511154\n",
      "Epoch 5375, Loss: 0.21068986132740974, Final Batch Loss: 0.08658143877983093\n",
      "Epoch 5376, Loss: 0.15667466260492802, Final Batch Loss: 0.017868848517537117\n",
      "Epoch 5377, Loss: 0.21396943926811218, Final Batch Loss: 0.030285675078630447\n",
      "Epoch 5378, Loss: 0.2822628580033779, Final Batch Loss: 0.07617755234241486\n",
      "Epoch 5379, Loss: 0.1891235988587141, Final Batch Loss: 0.016647638753056526\n",
      "Epoch 5380, Loss: 0.24835001304745674, Final Batch Loss: 0.13370782136917114\n",
      "Epoch 5381, Loss: 0.19515602476894855, Final Batch Loss: 0.10382623970508575\n",
      "Epoch 5382, Loss: 0.26809690333902836, Final Batch Loss: 0.16057854890823364\n",
      "Epoch 5383, Loss: 0.13324999436736107, Final Batch Loss: 0.023145979270339012\n",
      "Epoch 5384, Loss: 0.2730027474462986, Final Batch Loss: 0.027911953628063202\n",
      "Epoch 5385, Loss: 0.2682477980852127, Final Batch Loss: 0.08372164517641068\n",
      "Epoch 5386, Loss: 0.2317873165011406, Final Batch Loss: 0.08060401678085327\n",
      "Epoch 5387, Loss: 0.31656618788838387, Final Batch Loss: 0.14811484515666962\n",
      "Epoch 5388, Loss: 0.24002602696418762, Final Batch Loss: 0.02918895334005356\n",
      "Epoch 5389, Loss: 0.1840808019042015, Final Batch Loss: 0.019406065344810486\n",
      "Epoch 5390, Loss: 0.2441749144345522, Final Batch Loss: 0.13862593472003937\n",
      "Epoch 5391, Loss: 0.17808327078819275, Final Batch Loss: 0.040803734213113785\n",
      "Epoch 5392, Loss: 0.27161491848528385, Final Batch Loss: 0.08859238028526306\n",
      "Epoch 5393, Loss: 0.18974419683218002, Final Batch Loss: 0.042822957038879395\n",
      "Epoch 5394, Loss: 0.17425835318863392, Final Batch Loss: 0.06604567915201187\n",
      "Epoch 5395, Loss: 0.16798732057213783, Final Batch Loss: 0.04867880791425705\n",
      "Epoch 5396, Loss: 0.22476490959525108, Final Batch Loss: 0.059523340314626694\n",
      "Epoch 5397, Loss: 0.22559413313865662, Final Batch Loss: 0.07045572251081467\n",
      "Epoch 5398, Loss: 0.2380417138338089, Final Batch Loss: 0.1000509038567543\n",
      "Epoch 5399, Loss: 0.2090713456273079, Final Batch Loss: 0.05538499727845192\n",
      "Epoch 5400, Loss: 0.24897053837776184, Final Batch Loss: 0.08151990175247192\n",
      "Epoch 5401, Loss: 0.3609399199485779, Final Batch Loss: 0.14612896740436554\n",
      "Epoch 5402, Loss: 0.3583654537796974, Final Batch Loss: 0.09419895708560944\n",
      "Epoch 5403, Loss: 0.3338180221617222, Final Batch Loss: 0.06202409043908119\n",
      "Epoch 5404, Loss: 0.3334420993924141, Final Batch Loss: 0.07469258457422256\n",
      "Epoch 5405, Loss: 0.25884341076016426, Final Batch Loss: 0.07648631930351257\n",
      "Epoch 5406, Loss: 0.3001242056488991, Final Batch Loss: 0.032565888017416\n",
      "Epoch 5407, Loss: 0.21966547332704067, Final Batch Loss: 0.0221213698387146\n",
      "Epoch 5408, Loss: 0.18511933088302612, Final Batch Loss: 0.034513991326093674\n",
      "Epoch 5409, Loss: 0.20245723612606525, Final Batch Loss: 0.07963051646947861\n",
      "Epoch 5410, Loss: 0.2121303603053093, Final Batch Loss: 0.07783889025449753\n",
      "Epoch 5411, Loss: 0.1413382408209145, Final Batch Loss: 0.007152827922254801\n",
      "Epoch 5412, Loss: 0.2103065513074398, Final Batch Loss: 0.05583236366510391\n",
      "Epoch 5413, Loss: 0.26627957820892334, Final Batch Loss: 0.09939559549093246\n",
      "Epoch 5414, Loss: 0.2348567470908165, Final Batch Loss: 0.07065925747156143\n",
      "Epoch 5415, Loss: 0.25648724287748337, Final Batch Loss: 0.034052520990371704\n",
      "Epoch 5416, Loss: 0.2290901392698288, Final Batch Loss: 0.052279625087976456\n",
      "Epoch 5417, Loss: 0.16406389325857162, Final Batch Loss: 0.03641460835933685\n",
      "Epoch 5418, Loss: 0.1865161582827568, Final Batch Loss: 0.057795628905296326\n",
      "Epoch 5419, Loss: 0.217889953404665, Final Batch Loss: 0.06319398432970047\n",
      "Epoch 5420, Loss: 0.22482527792453766, Final Batch Loss: 0.05463014915585518\n",
      "Epoch 5421, Loss: 0.23517197743058205, Final Batch Loss: 0.025316458195447922\n",
      "Epoch 5422, Loss: 0.2572300583124161, Final Batch Loss: 0.07246050238609314\n",
      "Epoch 5423, Loss: 0.26144149526953697, Final Batch Loss: 0.10149961709976196\n",
      "Epoch 5424, Loss: 0.18987446650862694, Final Batch Loss: 0.038261301815509796\n",
      "Epoch 5425, Loss: 0.2647249661386013, Final Batch Loss: 0.013249021023511887\n",
      "Epoch 5426, Loss: 0.20250792801380157, Final Batch Loss: 0.03323560580611229\n",
      "Epoch 5427, Loss: 0.2539930399507284, Final Batch Loss: 0.11055878549814224\n",
      "Epoch 5428, Loss: 0.16370290145277977, Final Batch Loss: 0.03699341416358948\n",
      "Epoch 5429, Loss: 0.18849458545446396, Final Batch Loss: 0.024273350834846497\n",
      "Epoch 5430, Loss: 0.20485234260559082, Final Batch Loss: 0.033842768520116806\n",
      "Epoch 5431, Loss: 0.19197504222393036, Final Batch Loss: 0.07004418969154358\n",
      "Epoch 5432, Loss: 0.19252572394907475, Final Batch Loss: 0.029110753908753395\n",
      "Epoch 5433, Loss: 0.15817598067224026, Final Batch Loss: 0.048800427466630936\n",
      "Epoch 5434, Loss: 0.16257286816835403, Final Batch Loss: 0.04562544450163841\n",
      "Epoch 5435, Loss: 0.263881079852581, Final Batch Loss: 0.14576104283332825\n",
      "Epoch 5436, Loss: 0.21316758170723915, Final Batch Loss: 0.03636958450078964\n",
      "Epoch 5437, Loss: 0.14540904387831688, Final Batch Loss: 0.032939836382865906\n",
      "Epoch 5438, Loss: 0.15599647723138332, Final Batch Loss: 0.03099033795297146\n",
      "Epoch 5439, Loss: 0.21846724674105644, Final Batch Loss: 0.08125296235084534\n",
      "Epoch 5440, Loss: 0.18728072009980679, Final Batch Loss: 0.03931993618607521\n",
      "Epoch 5441, Loss: 0.17632562294602394, Final Batch Loss: 0.02008182182908058\n",
      "Epoch 5442, Loss: 0.12907241471111774, Final Batch Loss: 0.003614138811826706\n",
      "Epoch 5443, Loss: 0.25317367166280746, Final Batch Loss: 0.08319155126810074\n",
      "Epoch 5444, Loss: 0.16709082201123238, Final Batch Loss: 0.028413210064172745\n",
      "Epoch 5445, Loss: 0.09788691066205502, Final Batch Loss: 0.01635882258415222\n",
      "Epoch 5446, Loss: 0.23539229109883308, Final Batch Loss: 0.08649206161499023\n",
      "Epoch 5447, Loss: 0.1647294946014881, Final Batch Loss: 0.032657768577337265\n",
      "Epoch 5448, Loss: 0.1976864840835333, Final Batch Loss: 0.07166190445423126\n",
      "Epoch 5449, Loss: 0.26634998619556427, Final Batch Loss: 0.10534191876649857\n",
      "Epoch 5450, Loss: 0.24418127536773682, Final Batch Loss: 0.05558352544903755\n",
      "Epoch 5451, Loss: 0.16830898448824883, Final Batch Loss: 0.0401848703622818\n",
      "Epoch 5452, Loss: 0.24780500307679176, Final Batch Loss: 0.10612080246210098\n",
      "Epoch 5453, Loss: 0.2634139358997345, Final Batch Loss: 0.08655159175395966\n",
      "Epoch 5454, Loss: 0.20298141054809093, Final Batch Loss: 0.021375978365540504\n",
      "Epoch 5455, Loss: 0.17895595729351044, Final Batch Loss: 0.05998486280441284\n",
      "Epoch 5456, Loss: 0.1957405023276806, Final Batch Loss: 0.0841178297996521\n",
      "Epoch 5457, Loss: 0.2620067223906517, Final Batch Loss: 0.09948567301034927\n",
      "Epoch 5458, Loss: 0.18511570245027542, Final Batch Loss: 0.059020671993494034\n",
      "Epoch 5459, Loss: 0.23273205384612083, Final Batch Loss: 0.037687748670578\n",
      "Epoch 5460, Loss: 0.1836578343063593, Final Batch Loss: 0.04374700412154198\n",
      "Epoch 5461, Loss: 0.14447916019707918, Final Batch Loss: 0.011846915818750858\n",
      "Epoch 5462, Loss: 0.2604167275130749, Final Batch Loss: 0.07491713017225266\n",
      "Epoch 5463, Loss: 0.16598589438945055, Final Batch Loss: 0.011529483832418919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5464, Loss: 0.23463325947523117, Final Batch Loss: 0.043818991631269455\n",
      "Epoch 5465, Loss: 0.16121982224285603, Final Batch Loss: 0.022872421890497208\n",
      "Epoch 5466, Loss: 0.16176737844944, Final Batch Loss: 0.028585055842995644\n",
      "Epoch 5467, Loss: 0.25120851024985313, Final Batch Loss: 0.07961469888687134\n",
      "Epoch 5468, Loss: 0.18176846951246262, Final Batch Loss: 0.019001279026269913\n",
      "Epoch 5469, Loss: 0.20749759674072266, Final Batch Loss: 0.04386976733803749\n",
      "Epoch 5470, Loss: 0.24547003209590912, Final Batch Loss: 0.060237277299165726\n",
      "Epoch 5471, Loss: 0.24028539471328259, Final Batch Loss: 0.06021440401673317\n",
      "Epoch 5472, Loss: 0.22027422860264778, Final Batch Loss: 0.08828737586736679\n",
      "Epoch 5473, Loss: 0.20429330505430698, Final Batch Loss: 0.015753908082842827\n",
      "Epoch 5474, Loss: 0.14149109460413456, Final Batch Loss: 0.010704727843403816\n",
      "Epoch 5475, Loss: 0.27443885803222656, Final Batch Loss: 0.14170032739639282\n",
      "Epoch 5476, Loss: 0.20399052277207375, Final Batch Loss: 0.07760018855333328\n",
      "Epoch 5477, Loss: 0.19611212983727455, Final Batch Loss: 0.039532460272312164\n",
      "Epoch 5478, Loss: 0.17860951274633408, Final Batch Loss: 0.04405391961336136\n",
      "Epoch 5479, Loss: 0.17090507596731186, Final Batch Loss: 0.04441651329398155\n",
      "Epoch 5480, Loss: 0.14875180646777153, Final Batch Loss: 0.02253037691116333\n",
      "Epoch 5481, Loss: 0.12243849877268076, Final Batch Loss: 0.025512926280498505\n",
      "Epoch 5482, Loss: 0.1869263257831335, Final Batch Loss: 0.03662911802530289\n",
      "Epoch 5483, Loss: 0.1386761236935854, Final Batch Loss: 0.029940439388155937\n",
      "Epoch 5484, Loss: 0.20418377965688705, Final Batch Loss: 0.009733933955430984\n",
      "Epoch 5485, Loss: 0.20274680107831955, Final Batch Loss: 0.0815357193350792\n",
      "Epoch 5486, Loss: 0.1870957724750042, Final Batch Loss: 0.055179063230752945\n",
      "Epoch 5487, Loss: 0.13088655285537243, Final Batch Loss: 0.022543443366885185\n",
      "Epoch 5488, Loss: 0.13004006631672382, Final Batch Loss: 0.011125655844807625\n",
      "Epoch 5489, Loss: 0.24043302796781063, Final Batch Loss: 0.09340719878673553\n",
      "Epoch 5490, Loss: 0.18346069380640984, Final Batch Loss: 0.018408171832561493\n",
      "Epoch 5491, Loss: 0.13471277989447117, Final Batch Loss: 0.045908939093351364\n",
      "Epoch 5492, Loss: 0.13723374344408512, Final Batch Loss: 0.03644487261772156\n",
      "Epoch 5493, Loss: 0.14264158718287945, Final Batch Loss: 0.010891620069742203\n",
      "Epoch 5494, Loss: 0.18590117432177067, Final Batch Loss: 0.016850411891937256\n",
      "Epoch 5495, Loss: 0.1434645801782608, Final Batch Loss: 0.01897033303976059\n",
      "Epoch 5496, Loss: 0.193381542339921, Final Batch Loss: 0.09593654423952103\n",
      "Epoch 5497, Loss: 0.11860684305429459, Final Batch Loss: 0.007787963375449181\n",
      "Epoch 5498, Loss: 0.20609952881932259, Final Batch Loss: 0.07717989385128021\n",
      "Epoch 5499, Loss: 0.24611153081059456, Final Batch Loss: 0.0759449452161789\n",
      "Epoch 5500, Loss: 0.17392106354236603, Final Batch Loss: 0.04089367389678955\n",
      "Epoch 5501, Loss: 0.21977530047297478, Final Batch Loss: 0.02456369251012802\n",
      "Epoch 5502, Loss: 0.2686628960072994, Final Batch Loss: 0.0665113776922226\n",
      "Epoch 5503, Loss: 0.41527105309069157, Final Batch Loss: 0.20634981989860535\n",
      "Epoch 5504, Loss: 0.2518179342150688, Final Batch Loss: 0.048448868095874786\n",
      "Epoch 5505, Loss: 0.2732238359749317, Final Batch Loss: 0.07602942734956741\n",
      "Epoch 5506, Loss: 0.19221539422869682, Final Batch Loss: 0.06353998929262161\n",
      "Epoch 5507, Loss: 0.18664322420954704, Final Batch Loss: 0.05743329972028732\n",
      "Epoch 5508, Loss: 0.19930091872811317, Final Batch Loss: 0.04726240038871765\n",
      "Epoch 5509, Loss: 0.1992870457470417, Final Batch Loss: 0.07259203493595123\n",
      "Epoch 5510, Loss: 0.1955561228096485, Final Batch Loss: 0.08664761483669281\n",
      "Epoch 5511, Loss: 0.33809976279735565, Final Batch Loss: 0.19553638994693756\n",
      "Epoch 5512, Loss: 0.27174705266952515, Final Batch Loss: 0.053144365549087524\n",
      "Epoch 5513, Loss: 0.25748719461262226, Final Batch Loss: 0.1106325164437294\n",
      "Epoch 5514, Loss: 0.3404318243265152, Final Batch Loss: 0.09096836298704147\n",
      "Epoch 5515, Loss: 0.2935263030230999, Final Batch Loss: 0.01714935153722763\n",
      "Epoch 5516, Loss: 0.21291391737759113, Final Batch Loss: 0.0193023718893528\n",
      "Epoch 5517, Loss: 0.2594590373337269, Final Batch Loss: 0.06544768810272217\n",
      "Epoch 5518, Loss: 0.22597805224359035, Final Batch Loss: 0.030966391786932945\n",
      "Epoch 5519, Loss: 0.24502893164753914, Final Batch Loss: 0.07612691819667816\n",
      "Epoch 5520, Loss: 0.1969004161655903, Final Batch Loss: 0.03935270011425018\n",
      "Epoch 5521, Loss: 0.17467490211129189, Final Batch Loss: 0.02705458551645279\n",
      "Epoch 5522, Loss: 0.23715296387672424, Final Batch Loss: 0.03318887576460838\n",
      "Epoch 5523, Loss: 0.3812774084508419, Final Batch Loss: 0.13230237364768982\n",
      "Epoch 5524, Loss: 0.1903490349650383, Final Batch Loss: 0.022586511448025703\n",
      "Epoch 5525, Loss: 0.2892048880457878, Final Batch Loss: 0.08106149733066559\n",
      "Epoch 5526, Loss: 0.23550999909639359, Final Batch Loss: 0.12572741508483887\n",
      "Epoch 5527, Loss: 0.19590822234749794, Final Batch Loss: 0.0374920628964901\n",
      "Epoch 5528, Loss: 0.177801800891757, Final Batch Loss: 0.02359737642109394\n",
      "Epoch 5529, Loss: 0.2386334352195263, Final Batch Loss: 0.036268092691898346\n",
      "Epoch 5530, Loss: 0.16465491522103548, Final Batch Loss: 0.03354022279381752\n",
      "Epoch 5531, Loss: 0.1421732623130083, Final Batch Loss: 0.02159995213150978\n",
      "Epoch 5532, Loss: 0.16599931009113789, Final Batch Loss: 0.0478951632976532\n",
      "Epoch 5533, Loss: 0.146248247474432, Final Batch Loss: 0.03608930855989456\n",
      "Epoch 5534, Loss: 0.17912296950817108, Final Batch Loss: 0.04649193212389946\n",
      "Epoch 5535, Loss: 0.2303803525865078, Final Batch Loss: 0.0837298110127449\n",
      "Epoch 5536, Loss: 0.2554093338549137, Final Batch Loss: 0.0856260433793068\n",
      "Epoch 5537, Loss: 0.21368703804910183, Final Batch Loss: 0.045490022748708725\n",
      "Epoch 5538, Loss: 0.1838800348341465, Final Batch Loss: 0.04662556201219559\n",
      "Epoch 5539, Loss: 0.15601552836596966, Final Batch Loss: 0.018111972138285637\n",
      "Epoch 5540, Loss: 0.15853801183402538, Final Batch Loss: 0.03822217509150505\n",
      "Epoch 5541, Loss: 0.12920579686760902, Final Batch Loss: 0.02681068331003189\n",
      "Epoch 5542, Loss: 0.24290582165122032, Final Batch Loss: 0.04845018684864044\n",
      "Epoch 5543, Loss: 0.16573582217097282, Final Batch Loss: 0.022705795243382454\n",
      "Epoch 5544, Loss: 0.2019774317741394, Final Batch Loss: 0.05100719630718231\n",
      "Epoch 5545, Loss: 0.26016878336668015, Final Batch Loss: 0.10117597132921219\n",
      "Epoch 5546, Loss: 0.14113467000424862, Final Batch Loss: 0.05101018399000168\n",
      "Epoch 5547, Loss: 0.22016246430575848, Final Batch Loss: 0.04249519482254982\n",
      "Epoch 5548, Loss: 0.151726633310318, Final Batch Loss: 0.05231505259871483\n",
      "Epoch 5549, Loss: 0.16604172578081489, Final Batch Loss: 0.0067773363552987576\n",
      "Epoch 5550, Loss: 0.19835309870541096, Final Batch Loss: 0.03000115044414997\n",
      "Epoch 5551, Loss: 0.18512350879609585, Final Batch Loss: 0.04883705452084541\n",
      "Epoch 5552, Loss: 0.227059967815876, Final Batch Loss: 0.028655681759119034\n",
      "Epoch 5553, Loss: 0.23912061378359795, Final Batch Loss: 0.11514601111412048\n",
      "Epoch 5554, Loss: 0.19150564819574356, Final Batch Loss: 0.07716874033212662\n",
      "Epoch 5555, Loss: 0.17599274963140488, Final Batch Loss: 0.024055488407611847\n",
      "Epoch 5556, Loss: 0.3131481744349003, Final Batch Loss: 0.06865202635526657\n",
      "Epoch 5557, Loss: 0.198793338611722, Final Batch Loss: 0.05004531145095825\n",
      "Epoch 5558, Loss: 0.18496015295386314, Final Batch Loss: 0.04585668444633484\n",
      "Epoch 5559, Loss: 0.28583187237381935, Final Batch Loss: 0.13954323530197144\n",
      "Epoch 5560, Loss: 0.18242886289954185, Final Batch Loss: 0.04725418612360954\n",
      "Epoch 5561, Loss: 0.22972275502979755, Final Batch Loss: 0.05192071944475174\n",
      "Epoch 5562, Loss: 0.18318035826086998, Final Batch Loss: 0.03555486723780632\n",
      "Epoch 5563, Loss: 0.28033670596778393, Final Batch Loss: 0.013731414452195168\n",
      "Epoch 5564, Loss: 0.20130019262433052, Final Batch Loss: 0.03937678411602974\n",
      "Epoch 5565, Loss: 0.21301944367587566, Final Batch Loss: 0.04459178447723389\n",
      "Epoch 5566, Loss: 0.19596199318766594, Final Batch Loss: 0.0319884829223156\n",
      "Epoch 5567, Loss: 0.19990700855851173, Final Batch Loss: 0.05820092186331749\n",
      "Epoch 5568, Loss: 0.1532692378386855, Final Batch Loss: 0.014981509186327457\n",
      "Epoch 5569, Loss: 0.21065611951053143, Final Batch Loss: 0.0890885442495346\n",
      "Epoch 5570, Loss: 0.19970921613276005, Final Batch Loss: 0.07765819877386093\n",
      "Epoch 5571, Loss: 0.20050645619630814, Final Batch Loss: 0.06073421984910965\n",
      "Epoch 5572, Loss: 0.14008764922618866, Final Batch Loss: 0.02623818814754486\n",
      "Epoch 5573, Loss: 0.17909158766269684, Final Batch Loss: 0.027851317077875137\n",
      "Epoch 5574, Loss: 0.2219119407236576, Final Batch Loss: 0.1053084284067154\n",
      "Epoch 5575, Loss: 0.18934845551848412, Final Batch Loss: 0.02085777372121811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5576, Loss: 0.135127454996109, Final Batch Loss: 0.020425470545887947\n",
      "Epoch 5577, Loss: 0.2011545542627573, Final Batch Loss: 0.03305598720908165\n",
      "Epoch 5578, Loss: 0.18741981126368046, Final Batch Loss: 0.02447081170976162\n",
      "Epoch 5579, Loss: 0.22167128883302212, Final Batch Loss: 0.0983002632856369\n",
      "Epoch 5580, Loss: 0.165477829053998, Final Batch Loss: 0.02525254152715206\n",
      "Epoch 5581, Loss: 0.0974983493797481, Final Batch Loss: 0.007283331360667944\n",
      "Epoch 5582, Loss: 0.08495672699064016, Final Batch Loss: 0.017496097832918167\n",
      "Epoch 5583, Loss: 0.24350832775235176, Final Batch Loss: 0.11799997836351395\n",
      "Epoch 5584, Loss: 0.15967200137674809, Final Batch Loss: 0.03633945435285568\n",
      "Epoch 5585, Loss: 0.20911740604788065, Final Batch Loss: 0.01557211671024561\n",
      "Epoch 5586, Loss: 0.1501067839562893, Final Batch Loss: 0.03676991909742355\n",
      "Epoch 5587, Loss: 0.19637829065322876, Final Batch Loss: 0.033879391849040985\n",
      "Epoch 5588, Loss: 0.15399694629013538, Final Batch Loss: 0.014083398506045341\n",
      "Epoch 5589, Loss: 0.2347712479531765, Final Batch Loss: 0.07799354195594788\n",
      "Epoch 5590, Loss: 0.1304288785904646, Final Batch Loss: 0.02145778387784958\n",
      "Epoch 5591, Loss: 0.1691711973398924, Final Batch Loss: 0.021186424419283867\n",
      "Epoch 5592, Loss: 0.17690917663276196, Final Batch Loss: 0.008824482560157776\n",
      "Epoch 5593, Loss: 0.17866459395736456, Final Batch Loss: 0.015609989874064922\n",
      "Epoch 5594, Loss: 0.18033789470791817, Final Batch Loss: 0.03315278887748718\n",
      "Epoch 5595, Loss: 0.17555666901171207, Final Batch Loss: 0.0401809997856617\n",
      "Epoch 5596, Loss: 0.13948292657732964, Final Batch Loss: 0.02237904816865921\n",
      "Epoch 5597, Loss: 0.1722562275826931, Final Batch Loss: 0.03819180652499199\n",
      "Epoch 5598, Loss: 0.23725133016705513, Final Batch Loss: 0.08832965791225433\n",
      "Epoch 5599, Loss: 0.17434265464544296, Final Batch Loss: 0.021001042798161507\n",
      "Epoch 5600, Loss: 0.19114549830555916, Final Batch Loss: 0.041178181767463684\n",
      "Epoch 5601, Loss: 0.10578637383878231, Final Batch Loss: 0.010893397033214569\n",
      "Epoch 5602, Loss: 0.16019177809357643, Final Batch Loss: 0.010975743643939495\n",
      "Epoch 5603, Loss: 0.2005400937050581, Final Batch Loss: 0.01863294281065464\n",
      "Epoch 5604, Loss: 0.2941339686512947, Final Batch Loss: 0.038524724543094635\n",
      "Epoch 5605, Loss: 0.17350885830819607, Final Batch Loss: 0.02485242486000061\n",
      "Epoch 5606, Loss: 0.1813564971089363, Final Batch Loss: 0.029378671199083328\n",
      "Epoch 5607, Loss: 0.1300976201891899, Final Batch Loss: 0.024014510214328766\n",
      "Epoch 5608, Loss: 0.2615912100300193, Final Batch Loss: 0.1441773772239685\n",
      "Epoch 5609, Loss: 0.21056279260665178, Final Batch Loss: 0.008513093926012516\n",
      "Epoch 5610, Loss: 0.3564034253358841, Final Batch Loss: 0.11987382918596268\n",
      "Epoch 5611, Loss: 0.3466317132115364, Final Batch Loss: 0.07800613343715668\n",
      "Epoch 5612, Loss: 0.2818533927202225, Final Batch Loss: 0.0361790657043457\n",
      "Epoch 5613, Loss: 0.21147695183753967, Final Batch Loss: 0.050327084958553314\n",
      "Epoch 5614, Loss: 0.25718147680163383, Final Batch Loss: 0.0648401603102684\n",
      "Epoch 5615, Loss: 0.2625737898051739, Final Batch Loss: 0.10010731965303421\n",
      "Epoch 5616, Loss: 0.26481613516807556, Final Batch Loss: 0.09484560042619705\n",
      "Epoch 5617, Loss: 0.267839640378952, Final Batch Loss: 0.026682138442993164\n",
      "Epoch 5618, Loss: 0.2007949110120535, Final Batch Loss: 0.02820589952170849\n",
      "Epoch 5619, Loss: 0.3034481927752495, Final Batch Loss: 0.06422876566648483\n",
      "Epoch 5620, Loss: 0.19655139558017254, Final Batch Loss: 0.028868915513157845\n",
      "Epoch 5621, Loss: 0.20149294286966324, Final Batch Loss: 0.06768530607223511\n",
      "Epoch 5622, Loss: 0.20173080265522003, Final Batch Loss: 0.018402941524982452\n",
      "Epoch 5623, Loss: 0.21829351224005222, Final Batch Loss: 0.07439371943473816\n",
      "Epoch 5624, Loss: 0.1323548685759306, Final Batch Loss: 0.020086679607629776\n",
      "Epoch 5625, Loss: 0.1248192097991705, Final Batch Loss: 0.023230841383337975\n",
      "Epoch 5626, Loss: 0.17939608916640282, Final Batch Loss: 0.04801257699728012\n",
      "Epoch 5627, Loss: 0.14691853150725365, Final Batch Loss: 0.0326651893556118\n",
      "Epoch 5628, Loss: 0.24618451297283173, Final Batch Loss: 0.10787215828895569\n",
      "Epoch 5629, Loss: 0.18360665254294872, Final Batch Loss: 0.034965530037879944\n",
      "Epoch 5630, Loss: 0.24070452898740768, Final Batch Loss: 0.09769444912672043\n",
      "Epoch 5631, Loss: 0.21623064950108528, Final Batch Loss: 0.03249407559633255\n",
      "Epoch 5632, Loss: 0.1951183071359992, Final Batch Loss: 0.012579373084008694\n",
      "Epoch 5633, Loss: 0.1960792075842619, Final Batch Loss: 0.026849063113331795\n",
      "Epoch 5634, Loss: 0.24908819422125816, Final Batch Loss: 0.042538609355688095\n",
      "Epoch 5635, Loss: 0.19563031941652298, Final Batch Loss: 0.03470899909734726\n",
      "Epoch 5636, Loss: 0.16282580606639385, Final Batch Loss: 0.06683425605297089\n",
      "Epoch 5637, Loss: 0.15770580433309078, Final Batch Loss: 0.01583319716155529\n",
      "Epoch 5638, Loss: 0.20793776400387287, Final Batch Loss: 0.058208681643009186\n",
      "Epoch 5639, Loss: 0.2108684666454792, Final Batch Loss: 0.043739430606365204\n",
      "Epoch 5640, Loss: 0.2736232653260231, Final Batch Loss: 0.10932997614145279\n",
      "Epoch 5641, Loss: 0.2548700198531151, Final Batch Loss: 0.033461857587099075\n",
      "Epoch 5642, Loss: 0.24743355065584183, Final Batch Loss: 0.038771696388721466\n",
      "Epoch 5643, Loss: 0.3778754211962223, Final Batch Loss: 0.14227551221847534\n",
      "Epoch 5644, Loss: 0.2111462727189064, Final Batch Loss: 0.07063421607017517\n",
      "Epoch 5645, Loss: 0.19690441340208054, Final Batch Loss: 0.0315956175327301\n",
      "Epoch 5646, Loss: 0.16747006960213184, Final Batch Loss: 0.007986338809132576\n",
      "Epoch 5647, Loss: 0.15261768084019423, Final Batch Loss: 0.0152658736333251\n",
      "Epoch 5648, Loss: 0.3087381012737751, Final Batch Loss: 0.0972917452454567\n",
      "Epoch 5649, Loss: 0.14600814320147038, Final Batch Loss: 0.032785411924123764\n",
      "Epoch 5650, Loss: 0.16827860474586487, Final Batch Loss: 0.016268640756607056\n",
      "Epoch 5651, Loss: 0.17267206870019436, Final Batch Loss: 0.02743084542453289\n",
      "Epoch 5652, Loss: 0.2518024295568466, Final Batch Loss: 0.06160422042012215\n",
      "Epoch 5653, Loss: 0.23664810881018639, Final Batch Loss: 0.05728587508201599\n",
      "Epoch 5654, Loss: 0.17028214037418365, Final Batch Loss: 0.03070022538304329\n",
      "Epoch 5655, Loss: 0.1761869303882122, Final Batch Loss: 0.031028136610984802\n",
      "Epoch 5656, Loss: 0.14773896476253867, Final Batch Loss: 0.006348454859107733\n",
      "Epoch 5657, Loss: 0.14649361371994019, Final Batch Loss: 0.022780191153287888\n",
      "Epoch 5658, Loss: 0.14973277039825916, Final Batch Loss: 0.021545695140957832\n",
      "Epoch 5659, Loss: 0.2225441336631775, Final Batch Loss: 0.0568879134953022\n",
      "Epoch 5660, Loss: 0.1423205640166998, Final Batch Loss: 0.03126709163188934\n",
      "Epoch 5661, Loss: 0.20068107172846794, Final Batch Loss: 0.07826874405145645\n",
      "Epoch 5662, Loss: 0.20194550417363644, Final Batch Loss: 0.08200151473283768\n",
      "Epoch 5663, Loss: 0.14836095459759235, Final Batch Loss: 0.022836364805698395\n",
      "Epoch 5664, Loss: 0.26493328623473644, Final Batch Loss: 0.13534751534461975\n",
      "Epoch 5665, Loss: 0.2524361275136471, Final Batch Loss: 0.03423844650387764\n",
      "Epoch 5666, Loss: 0.16113374382257462, Final Batch Loss: 0.01726584881544113\n",
      "Epoch 5667, Loss: 0.18043798580765724, Final Batch Loss: 0.04300103709101677\n",
      "Epoch 5668, Loss: 0.1412986610084772, Final Batch Loss: 0.034906525164842606\n",
      "Epoch 5669, Loss: 0.2500847317278385, Final Batch Loss: 0.07472890615463257\n",
      "Epoch 5670, Loss: 0.11163624189794064, Final Batch Loss: 0.015747377648949623\n",
      "Epoch 5671, Loss: 0.2093999795615673, Final Batch Loss: 0.05507388338446617\n",
      "Epoch 5672, Loss: 0.18510144390165806, Final Batch Loss: 0.03083828277885914\n",
      "Epoch 5673, Loss: 0.20508779026567936, Final Batch Loss: 0.017305443063378334\n",
      "Epoch 5674, Loss: 0.16323749162256718, Final Batch Loss: 0.030229149386286736\n",
      "Epoch 5675, Loss: 0.17765486985445023, Final Batch Loss: 0.017353449016809464\n",
      "Epoch 5676, Loss: 0.2054634764790535, Final Batch Loss: 0.04019907861948013\n",
      "Epoch 5677, Loss: 0.14582446590065956, Final Batch Loss: 0.02569974958896637\n",
      "Epoch 5678, Loss: 0.2181227020919323, Final Batch Loss: 0.03747996315360069\n",
      "Epoch 5679, Loss: 0.14972460269927979, Final Batch Loss: 0.025133609771728516\n",
      "Epoch 5680, Loss: 0.16284769214689732, Final Batch Loss: 0.021927403286099434\n",
      "Epoch 5681, Loss: 0.2326582409441471, Final Batch Loss: 0.07704430818557739\n",
      "Epoch 5682, Loss: 0.27492183446884155, Final Batch Loss: 0.14071986079216003\n",
      "Epoch 5683, Loss: 0.1922207474708557, Final Batch Loss: 0.022270388901233673\n",
      "Epoch 5684, Loss: 0.2386874109506607, Final Batch Loss: 0.0812370628118515\n",
      "Epoch 5685, Loss: 0.29234931245446205, Final Batch Loss: 0.11444396525621414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5686, Loss: 0.12069492414593697, Final Batch Loss: 0.014601973816752434\n",
      "Epoch 5687, Loss: 0.1670805588364601, Final Batch Loss: 0.011666929349303246\n",
      "Epoch 5688, Loss: 0.35122474655508995, Final Batch Loss: 0.14882834255695343\n",
      "Epoch 5689, Loss: 0.17283951304852962, Final Batch Loss: 0.02748151309788227\n",
      "Epoch 5690, Loss: 0.209949541836977, Final Batch Loss: 0.023440483957529068\n",
      "Epoch 5691, Loss: 0.20871398597955704, Final Batch Loss: 0.030506474897265434\n",
      "Epoch 5692, Loss: 0.17135434597730637, Final Batch Loss: 0.03878966346383095\n",
      "Epoch 5693, Loss: 0.17619162052869797, Final Batch Loss: 0.04846421256661415\n",
      "Epoch 5694, Loss: 0.21005123388022184, Final Batch Loss: 0.01091527845710516\n",
      "Epoch 5695, Loss: 0.20146027952432632, Final Batch Loss: 0.0842362567782402\n",
      "Epoch 5696, Loss: 0.1590726375579834, Final Batch Loss: 0.031166819855570793\n",
      "Epoch 5697, Loss: 0.23597786575555801, Final Batch Loss: 0.05998056381940842\n",
      "Epoch 5698, Loss: 0.3028542138636112, Final Batch Loss: 0.05110543593764305\n",
      "Epoch 5699, Loss: 0.1835899632424116, Final Batch Loss: 0.00931178592145443\n",
      "Epoch 5700, Loss: 0.20183097198605537, Final Batch Loss: 0.04606875404715538\n",
      "Epoch 5701, Loss: 0.24577059596776962, Final Batch Loss: 0.06494750827550888\n",
      "Epoch 5702, Loss: 0.13351254910230637, Final Batch Loss: 0.027843713760375977\n",
      "Epoch 5703, Loss: 0.1939431820064783, Final Batch Loss: 0.04308164864778519\n",
      "Epoch 5704, Loss: 0.16134099615737796, Final Batch Loss: 0.004288311582058668\n",
      "Epoch 5705, Loss: 0.18057277332991362, Final Batch Loss: 0.013603496365249157\n",
      "Epoch 5706, Loss: 0.288348488509655, Final Batch Loss: 0.06331566721200943\n",
      "Epoch 5707, Loss: 0.15730983205139637, Final Batch Loss: 0.03152138367295265\n",
      "Epoch 5708, Loss: 0.21819708868861198, Final Batch Loss: 0.04649278149008751\n",
      "Epoch 5709, Loss: 0.19862709380686283, Final Batch Loss: 0.03066563792526722\n",
      "Epoch 5710, Loss: 0.20661825686693192, Final Batch Loss: 0.05887680873274803\n",
      "Epoch 5711, Loss: 0.2426898367702961, Final Batch Loss: 0.04593845084309578\n",
      "Epoch 5712, Loss: 0.20077927224338055, Final Batch Loss: 0.02471172995865345\n",
      "Epoch 5713, Loss: 0.13540791533887386, Final Batch Loss: 0.04238472878932953\n",
      "Epoch 5714, Loss: 0.12486068904399872, Final Batch Loss: 0.004157885909080505\n",
      "Epoch 5715, Loss: 0.10809397138655186, Final Batch Loss: 0.006507348269224167\n",
      "Epoch 5716, Loss: 0.17370960302650928, Final Batch Loss: 0.029216205701231956\n",
      "Epoch 5717, Loss: 0.16764616407454014, Final Batch Loss: 0.07226180285215378\n",
      "Epoch 5718, Loss: 0.2778612934052944, Final Batch Loss: 0.05866669490933418\n",
      "Epoch 5719, Loss: 0.1944290678948164, Final Batch Loss: 0.02904059924185276\n",
      "Epoch 5720, Loss: 0.18266644142568111, Final Batch Loss: 0.060329414904117584\n",
      "Epoch 5721, Loss: 0.17698871344327927, Final Batch Loss: 0.024347806349396706\n",
      "Epoch 5722, Loss: 0.24134660512208939, Final Batch Loss: 0.050126783549785614\n",
      "Epoch 5723, Loss: 0.24944590963423252, Final Batch Loss: 0.09218280762434006\n",
      "Epoch 5724, Loss: 0.16314401477575302, Final Batch Loss: 0.027168169617652893\n",
      "Epoch 5725, Loss: 0.2685142531991005, Final Batch Loss: 0.12070305645465851\n",
      "Epoch 5726, Loss: 0.1696309121325612, Final Batch Loss: 0.014672634191811085\n",
      "Epoch 5727, Loss: 0.13388752192258835, Final Batch Loss: 0.02150932513177395\n",
      "Epoch 5728, Loss: 0.23903026059269905, Final Batch Loss: 0.07066009938716888\n",
      "Epoch 5729, Loss: 0.1995987333357334, Final Batch Loss: 0.05860256403684616\n",
      "Epoch 5730, Loss: 0.14370892196893692, Final Batch Loss: 0.07029251754283905\n",
      "Epoch 5731, Loss: 0.20593007281422615, Final Batch Loss: 0.09853068739175797\n",
      "Epoch 5732, Loss: 0.15598457865417004, Final Batch Loss: 0.023999782279133797\n",
      "Epoch 5733, Loss: 0.22376087307929993, Final Batch Loss: 0.03740566596388817\n",
      "Epoch 5734, Loss: 0.2421913854777813, Final Batch Loss: 0.037390146404504776\n",
      "Epoch 5735, Loss: 0.18617135286331177, Final Batch Loss: 0.03691083565354347\n",
      "Epoch 5736, Loss: 0.2501039579510689, Final Batch Loss: 0.0549151673913002\n",
      "Epoch 5737, Loss: 0.1550507014617324, Final Batch Loss: 0.0075585683807730675\n",
      "Epoch 5738, Loss: 0.24227295815944672, Final Batch Loss: 0.09013248980045319\n",
      "Epoch 5739, Loss: 0.22562655061483383, Final Batch Loss: 0.09382297098636627\n",
      "Epoch 5740, Loss: 0.23412343487143517, Final Batch Loss: 0.038937706500291824\n",
      "Epoch 5741, Loss: 0.12069347035139799, Final Batch Loss: 0.01208109688013792\n",
      "Epoch 5742, Loss: 0.15823801048099995, Final Batch Loss: 0.03077142871916294\n",
      "Epoch 5743, Loss: 0.17907781153917313, Final Batch Loss: 0.04223348945379257\n",
      "Epoch 5744, Loss: 0.27806318551301956, Final Batch Loss: 0.07410775870084763\n",
      "Epoch 5745, Loss: 0.2230020258575678, Final Batch Loss: 0.08377007395029068\n",
      "Epoch 5746, Loss: 0.16936971619725227, Final Batch Loss: 0.024338124319911003\n",
      "Epoch 5747, Loss: 0.29034893959760666, Final Batch Loss: 0.08377576619386673\n",
      "Epoch 5748, Loss: 0.2637063506990671, Final Batch Loss: 0.06113316863775253\n",
      "Epoch 5749, Loss: 0.23980308696627617, Final Batch Loss: 0.07948919385671616\n",
      "Epoch 5750, Loss: 0.2508823461830616, Final Batch Loss: 0.03834731504321098\n",
      "Epoch 5751, Loss: 0.30651629716157913, Final Batch Loss: 0.07136324048042297\n",
      "Epoch 5752, Loss: 0.2241535447537899, Final Batch Loss: 0.0799221396446228\n",
      "Epoch 5753, Loss: 0.2139398343861103, Final Batch Loss: 0.035091809928417206\n",
      "Epoch 5754, Loss: 0.143126817420125, Final Batch Loss: 0.015835894271731377\n",
      "Epoch 5755, Loss: 0.261811813339591, Final Batch Loss: 0.02530672214925289\n",
      "Epoch 5756, Loss: 0.24911976233124733, Final Batch Loss: 0.05564761534333229\n",
      "Epoch 5757, Loss: 0.226444561034441, Final Batch Loss: 0.04132657125592232\n",
      "Epoch 5758, Loss: 0.19057563599199057, Final Batch Loss: 0.010954487137496471\n",
      "Epoch 5759, Loss: 0.1878525484353304, Final Batch Loss: 0.08026161044836044\n",
      "Epoch 5760, Loss: 0.21929091215133667, Final Batch Loss: 0.07465937733650208\n",
      "Epoch 5761, Loss: 0.30087900161743164, Final Batch Loss: 0.0880100429058075\n",
      "Epoch 5762, Loss: 0.18222185969352722, Final Batch Loss: 0.01670045033097267\n",
      "Epoch 5763, Loss: 0.2771868593990803, Final Batch Loss: 0.05276003107428551\n",
      "Epoch 5764, Loss: 0.16134263668209314, Final Batch Loss: 0.014028632082045078\n",
      "Epoch 5765, Loss: 0.23595579154789448, Final Batch Loss: 0.10092960298061371\n",
      "Epoch 5766, Loss: 0.10633386112749577, Final Batch Loss: 0.031902942806482315\n",
      "Epoch 5767, Loss: 0.1860942244529724, Final Batch Loss: 0.03638315200805664\n",
      "Epoch 5768, Loss: 0.12115106545388699, Final Batch Loss: 0.02128184773027897\n",
      "Epoch 5769, Loss: 0.17175701260566711, Final Batch Loss: 0.015563104301691055\n",
      "Epoch 5770, Loss: 0.16904503107070923, Final Batch Loss: 0.05137372761964798\n",
      "Epoch 5771, Loss: 0.18152919970452785, Final Batch Loss: 0.035061974078416824\n",
      "Epoch 5772, Loss: 0.23294071853160858, Final Batch Loss: 0.0697614774107933\n",
      "Epoch 5773, Loss: 0.16601556725800037, Final Batch Loss: 0.012095553800463676\n",
      "Epoch 5774, Loss: 0.2744530327618122, Final Batch Loss: 0.10412174463272095\n",
      "Epoch 5775, Loss: 0.17484523728489876, Final Batch Loss: 0.040183745324611664\n",
      "Epoch 5776, Loss: 0.18063804879784584, Final Batch Loss: 0.04032790660858154\n",
      "Epoch 5777, Loss: 0.23844910971820354, Final Batch Loss: 0.13489888608455658\n",
      "Epoch 5778, Loss: 0.2340552620589733, Final Batch Loss: 0.0853787362575531\n",
      "Epoch 5779, Loss: 0.21339626051485538, Final Batch Loss: 0.04881710931658745\n",
      "Epoch 5780, Loss: 0.12308926694095135, Final Batch Loss: 0.023819467052817345\n",
      "Epoch 5781, Loss: 0.15930386632680893, Final Batch Loss: 0.01734868437051773\n",
      "Epoch 5782, Loss: 0.1478051170706749, Final Batch Loss: 0.06377080082893372\n",
      "Epoch 5783, Loss: 0.21613388136029243, Final Batch Loss: 0.05707935243844986\n",
      "Epoch 5784, Loss: 0.18034659326076508, Final Batch Loss: 0.06924084573984146\n",
      "Epoch 5785, Loss: 0.19285164028406143, Final Batch Loss: 0.032015107572078705\n",
      "Epoch 5786, Loss: 0.20205648243427277, Final Batch Loss: 0.04539814963936806\n",
      "Epoch 5787, Loss: 0.20366211608052254, Final Batch Loss: 0.04239577427506447\n",
      "Epoch 5788, Loss: 0.1450627613812685, Final Batch Loss: 0.02129645273089409\n",
      "Epoch 5789, Loss: 0.32843269407749176, Final Batch Loss: 0.0892840251326561\n",
      "Epoch 5790, Loss: 0.2702852003276348, Final Batch Loss: 0.07481686025857925\n",
      "Epoch 5791, Loss: 0.27295804023742676, Final Batch Loss: 0.0708804801106453\n",
      "Epoch 5792, Loss: 0.2189023718237877, Final Batch Loss: 0.0239071287214756\n",
      "Epoch 5793, Loss: 0.2053852118551731, Final Batch Loss: 0.04803166911005974\n",
      "Epoch 5794, Loss: 0.18075212836265564, Final Batch Loss: 0.03523794189095497\n",
      "Epoch 5795, Loss: 0.20859618578106165, Final Batch Loss: 0.011968986131250858\n",
      "Epoch 5796, Loss: 0.11888636462390423, Final Batch Loss: 0.02415502443909645\n",
      "Epoch 5797, Loss: 0.19443302787840366, Final Batch Loss: 0.07540541142225266\n",
      "Epoch 5798, Loss: 0.20099974423646927, Final Batch Loss: 0.034767162054777145\n",
      "Epoch 5799, Loss: 0.15739967487752438, Final Batch Loss: 0.08184294402599335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5800, Loss: 0.17361157294362783, Final Batch Loss: 0.013685797341167927\n",
      "Epoch 5801, Loss: 0.13745164684951305, Final Batch Loss: 0.025640422478318214\n",
      "Epoch 5802, Loss: 0.16290271282196045, Final Batch Loss: 0.027814071625471115\n",
      "Epoch 5803, Loss: 0.20342903211712837, Final Batch Loss: 0.033628661185503006\n",
      "Epoch 5804, Loss: 0.18842872977256775, Final Batch Loss: 0.03652399033308029\n",
      "Epoch 5805, Loss: 0.15612060576677322, Final Batch Loss: 0.036980677396059036\n",
      "Epoch 5806, Loss: 0.20637176930904388, Final Batch Loss: 0.05928882583975792\n",
      "Epoch 5807, Loss: 0.17564366105943918, Final Batch Loss: 0.015191220678389072\n",
      "Epoch 5808, Loss: 0.11708669364452362, Final Batch Loss: 0.01986921951174736\n",
      "Epoch 5809, Loss: 0.1316557638347149, Final Batch Loss: 0.022190233692526817\n",
      "Epoch 5810, Loss: 0.39964500442147255, Final Batch Loss: 0.13158877193927765\n",
      "Epoch 5811, Loss: 0.15737557038664818, Final Batch Loss: 0.01510433480143547\n",
      "Epoch 5812, Loss: 0.16594167053699493, Final Batch Loss: 0.025929495692253113\n",
      "Epoch 5813, Loss: 0.18377736397087574, Final Batch Loss: 0.0528927706182003\n",
      "Epoch 5814, Loss: 0.15643011033535004, Final Batch Loss: 0.016414957121014595\n",
      "Epoch 5815, Loss: 0.2418885976076126, Final Batch Loss: 0.029387615621089935\n",
      "Epoch 5816, Loss: 0.17623869329690933, Final Batch Loss: 0.01958576962351799\n",
      "Epoch 5817, Loss: 0.13177127577364445, Final Batch Loss: 0.015989024192094803\n",
      "Epoch 5818, Loss: 0.15270637907087803, Final Batch Loss: 0.029717104509472847\n",
      "Epoch 5819, Loss: 0.18658889830112457, Final Batch Loss: 0.021718986332416534\n",
      "Epoch 5820, Loss: 0.1475459821522236, Final Batch Loss: 0.03474806621670723\n",
      "Epoch 5821, Loss: 0.15416164137423038, Final Batch Loss: 0.056449681520462036\n",
      "Epoch 5822, Loss: 0.15319184213876724, Final Batch Loss: 0.04663209244608879\n",
      "Epoch 5823, Loss: 0.1883629858493805, Final Batch Loss: 0.06072781980037689\n",
      "Epoch 5824, Loss: 0.18516656011343002, Final Batch Loss: 0.05509215220808983\n",
      "Epoch 5825, Loss: 0.2335221990942955, Final Batch Loss: 0.1018889844417572\n",
      "Epoch 5826, Loss: 0.19616583734750748, Final Batch Loss: 0.028377719223499298\n",
      "Epoch 5827, Loss: 0.1520307008177042, Final Batch Loss: 0.038445815443992615\n",
      "Epoch 5828, Loss: 0.2607543207705021, Final Batch Loss: 0.030885402113199234\n",
      "Epoch 5829, Loss: 0.19778556376695633, Final Batch Loss: 0.10932158678770065\n",
      "Epoch 5830, Loss: 0.18738484010100365, Final Batch Loss: 0.0791165754199028\n",
      "Epoch 5831, Loss: 0.31082617305219173, Final Batch Loss: 0.02656724862754345\n",
      "Epoch 5832, Loss: 0.1387030454352498, Final Batch Loss: 0.012361484579741955\n",
      "Epoch 5833, Loss: 0.25541079975664616, Final Batch Loss: 0.09074358642101288\n",
      "Epoch 5834, Loss: 0.09112463798373938, Final Batch Loss: 0.023297928273677826\n",
      "Epoch 5835, Loss: 0.17268117889761925, Final Batch Loss: 0.06692963093519211\n",
      "Epoch 5836, Loss: 0.22350508347153664, Final Batch Loss: 0.04678812623023987\n",
      "Epoch 5837, Loss: 0.16377405263483524, Final Batch Loss: 0.046068355441093445\n",
      "Epoch 5838, Loss: 0.11775673273950815, Final Batch Loss: 0.009702921845018864\n",
      "Epoch 5839, Loss: 0.28744979575276375, Final Batch Loss: 0.15509328246116638\n",
      "Epoch 5840, Loss: 0.21384233236312866, Final Batch Loss: 0.04531342536211014\n",
      "Epoch 5841, Loss: 0.26126504316926, Final Batch Loss: 0.07813118398189545\n",
      "Epoch 5842, Loss: 0.41265199333429337, Final Batch Loss: 0.14185447990894318\n",
      "Epoch 5843, Loss: 0.17353153601288795, Final Batch Loss: 0.033437084406614304\n",
      "Epoch 5844, Loss: 0.18346733786165714, Final Batch Loss: 0.08136056363582611\n",
      "Epoch 5845, Loss: 0.18670859187841415, Final Batch Loss: 0.03335896506905556\n",
      "Epoch 5846, Loss: 0.3426101729273796, Final Batch Loss: 0.08392080664634705\n",
      "Epoch 5847, Loss: 0.2368028685450554, Final Batch Loss: 0.0494050607085228\n",
      "Epoch 5848, Loss: 0.24114279076457024, Final Batch Loss: 0.07734847813844681\n",
      "Epoch 5849, Loss: 0.28453387320041656, Final Batch Loss: 0.11265338957309723\n",
      "Epoch 5850, Loss: 0.34355199337005615, Final Batch Loss: 0.11077374964952469\n",
      "Epoch 5851, Loss: 0.31087223812937737, Final Batch Loss: 0.03274990990757942\n",
      "Epoch 5852, Loss: 0.24181892722845078, Final Batch Loss: 0.0357157327234745\n",
      "Epoch 5853, Loss: 0.295591052621603, Final Batch Loss: 0.08881869912147522\n",
      "Epoch 5854, Loss: 0.34322934597730637, Final Batch Loss: 0.054201640188694\n",
      "Epoch 5855, Loss: 0.29668546840548515, Final Batch Loss: 0.04888879507780075\n",
      "Epoch 5856, Loss: 0.23105108365416527, Final Batch Loss: 0.03645836561918259\n",
      "Epoch 5857, Loss: 0.21961697190999985, Final Batch Loss: 0.025132998824119568\n",
      "Epoch 5858, Loss: 0.18523539043962955, Final Batch Loss: 0.06712287664413452\n",
      "Epoch 5859, Loss: 0.19729336351156235, Final Batch Loss: 0.027496691793203354\n",
      "Epoch 5860, Loss: 0.20523998141288757, Final Batch Loss: 0.024710025638341904\n",
      "Epoch 5861, Loss: 0.15042069368064404, Final Batch Loss: 0.023165946826338768\n",
      "Epoch 5862, Loss: 0.2038173507899046, Final Batch Loss: 0.030426951125264168\n",
      "Epoch 5863, Loss: 0.30879396572709084, Final Batch Loss: 0.13749434053897858\n",
      "Epoch 5864, Loss: 0.11873475275933743, Final Batch Loss: 0.03357624262571335\n",
      "Epoch 5865, Loss: 0.1529718181118369, Final Batch Loss: 0.013575385324656963\n",
      "Epoch 5866, Loss: 0.1991083361208439, Final Batch Loss: 0.07950114458799362\n",
      "Epoch 5867, Loss: 0.15622231550514698, Final Batch Loss: 0.012660132721066475\n",
      "Epoch 5868, Loss: 0.3164630588144064, Final Batch Loss: 0.026155946776270866\n",
      "Epoch 5869, Loss: 0.15960059128701687, Final Batch Loss: 0.010020293295383453\n",
      "Epoch 5870, Loss: 0.16706373449414968, Final Batch Loss: 0.01160930935293436\n",
      "Epoch 5871, Loss: 0.24285707250237465, Final Batch Loss: 0.09037308394908905\n",
      "Epoch 5872, Loss: 0.2774667926132679, Final Batch Loss: 0.08508626371622086\n",
      "Epoch 5873, Loss: 0.17614519968628883, Final Batch Loss: 0.01612972468137741\n",
      "Epoch 5874, Loss: 0.241504468023777, Final Batch Loss: 0.08224829286336899\n",
      "Epoch 5875, Loss: 0.24110481515526772, Final Batch Loss: 0.05081953480839729\n",
      "Epoch 5876, Loss: 0.21075500547885895, Final Batch Loss: 0.07073074579238892\n",
      "Epoch 5877, Loss: 0.2521108277142048, Final Batch Loss: 0.10483358055353165\n",
      "Epoch 5878, Loss: 0.31509209610521793, Final Batch Loss: 0.18088248372077942\n",
      "Epoch 5879, Loss: 0.30866503715515137, Final Batch Loss: 0.13170452415943146\n",
      "Epoch 5880, Loss: 0.17271188274025917, Final Batch Loss: 0.021685656160116196\n",
      "Epoch 5881, Loss: 0.19076466001570225, Final Batch Loss: 0.0576668418943882\n",
      "Epoch 5882, Loss: 0.2420824095606804, Final Batch Loss: 0.08662775903940201\n",
      "Epoch 5883, Loss: 0.3002197816967964, Final Batch Loss: 0.06478184461593628\n",
      "Epoch 5884, Loss: 0.31422341242432594, Final Batch Loss: 0.07825908809900284\n",
      "Epoch 5885, Loss: 0.18336340691894293, Final Batch Loss: 0.015213253907859325\n",
      "Epoch 5886, Loss: 0.2871122919023037, Final Batch Loss: 0.044838108122348785\n",
      "Epoch 5887, Loss: 0.2505570314824581, Final Batch Loss: 0.06432832777500153\n",
      "Epoch 5888, Loss: 0.21493502520024776, Final Batch Loss: 0.031020374968647957\n",
      "Epoch 5889, Loss: 0.22057306580245495, Final Batch Loss: 0.10194224119186401\n",
      "Epoch 5890, Loss: 0.23788435570895672, Final Batch Loss: 0.04843935742974281\n",
      "Epoch 5891, Loss: 0.16936068143695593, Final Batch Loss: 0.06756801158189774\n",
      "Epoch 5892, Loss: 0.2167260032147169, Final Batch Loss: 0.09442824125289917\n",
      "Epoch 5893, Loss: 0.2053546980023384, Final Batch Loss: 0.054085873067379\n",
      "Epoch 5894, Loss: 0.2285647764801979, Final Batch Loss: 0.04326453059911728\n",
      "Epoch 5895, Loss: 0.1997854895889759, Final Batch Loss: 0.025269661098718643\n",
      "Epoch 5896, Loss: 0.2784907855093479, Final Batch Loss: 0.09625069051980972\n",
      "Epoch 5897, Loss: 0.26538558676838875, Final Batch Loss: 0.053899891674518585\n",
      "Epoch 5898, Loss: 0.15003094263374805, Final Batch Loss: 0.028799954801797867\n",
      "Epoch 5899, Loss: 0.18301887065172195, Final Batch Loss: 0.05725063756108284\n",
      "Epoch 5900, Loss: 0.22021955624222755, Final Batch Loss: 0.09526120871305466\n",
      "Epoch 5901, Loss: 0.18537662364542484, Final Batch Loss: 0.049491997808218\n",
      "Epoch 5902, Loss: 0.1941114403307438, Final Batch Loss: 0.06018534675240517\n",
      "Epoch 5903, Loss: 0.1910634282976389, Final Batch Loss: 0.023711467161774635\n",
      "Epoch 5904, Loss: 0.25718263164162636, Final Batch Loss: 0.11101832985877991\n",
      "Epoch 5905, Loss: 0.20760996639728546, Final Batch Loss: 0.034346386790275574\n",
      "Epoch 5906, Loss: 0.24129635095596313, Final Batch Loss: 0.03645053505897522\n",
      "Epoch 5907, Loss: 0.2720077633857727, Final Batch Loss: 0.1271098405122757\n",
      "Epoch 5908, Loss: 0.19083065912127495, Final Batch Loss: 0.058727435767650604\n",
      "Epoch 5909, Loss: 0.24840298295021057, Final Batch Loss: 0.034430209547281265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5910, Loss: 0.18431279063224792, Final Batch Loss: 0.030321799218654633\n",
      "Epoch 5911, Loss: 0.15181222930550575, Final Batch Loss: 0.03192596882581711\n",
      "Epoch 5912, Loss: 0.15518302842974663, Final Batch Loss: 0.04111022874712944\n",
      "Epoch 5913, Loss: 0.1851642206311226, Final Batch Loss: 0.06146009638905525\n",
      "Epoch 5914, Loss: 0.1932877255603671, Final Batch Loss: 0.009058143012225628\n",
      "Epoch 5915, Loss: 0.2168816141784191, Final Batch Loss: 0.04608163237571716\n",
      "Epoch 5916, Loss: 0.21511850133538246, Final Batch Loss: 0.08368702977895737\n",
      "Epoch 5917, Loss: 0.13270817324519157, Final Batch Loss: 0.030231934040784836\n",
      "Epoch 5918, Loss: 0.2876902334392071, Final Batch Loss: 0.11376888304948807\n",
      "Epoch 5919, Loss: 0.17097492888569832, Final Batch Loss: 0.05450289696455002\n",
      "Epoch 5920, Loss: 0.16073672473430634, Final Batch Loss: 0.022203076630830765\n",
      "Epoch 5921, Loss: 0.14362961426377296, Final Batch Loss: 0.05243454873561859\n",
      "Epoch 5922, Loss: 0.1281035579741001, Final Batch Loss: 0.01624235138297081\n",
      "Epoch 5923, Loss: 0.18804680928587914, Final Batch Loss: 0.02012474089860916\n",
      "Epoch 5924, Loss: 0.18095011450350285, Final Batch Loss: 0.025956599041819572\n",
      "Epoch 5925, Loss: 0.1775815486907959, Final Batch Loss: 0.011983964592218399\n",
      "Epoch 5926, Loss: 0.12184395454823971, Final Batch Loss: 0.017964854836463928\n",
      "Epoch 5927, Loss: 0.15390141494572163, Final Batch Loss: 0.04363009333610535\n",
      "Epoch 5928, Loss: 0.17505975952371955, Final Batch Loss: 0.007273474242538214\n",
      "Epoch 5929, Loss: 0.1469304095953703, Final Batch Loss: 0.03315037488937378\n",
      "Epoch 5930, Loss: 0.2300627063959837, Final Batch Loss: 0.018069380894303322\n",
      "Epoch 5931, Loss: 0.13637082278728485, Final Batch Loss: 0.043189745396375656\n",
      "Epoch 5932, Loss: 0.15036169812083244, Final Batch Loss: 0.0258808396756649\n",
      "Epoch 5933, Loss: 0.18609444610774517, Final Batch Loss: 0.038422368466854095\n",
      "Epoch 5934, Loss: 0.13846264965832233, Final Batch Loss: 0.02033925987780094\n",
      "Epoch 5935, Loss: 0.22657199576497078, Final Batch Loss: 0.07629109174013138\n",
      "Epoch 5936, Loss: 0.24406718090176582, Final Batch Loss: 0.07307173311710358\n",
      "Epoch 5937, Loss: 0.17552652023732662, Final Batch Loss: 0.03703667223453522\n",
      "Epoch 5938, Loss: 0.17695715092122555, Final Batch Loss: 0.041623666882514954\n",
      "Epoch 5939, Loss: 0.18815132044255733, Final Batch Loss: 0.05205441638827324\n",
      "Epoch 5940, Loss: 0.21958866156637669, Final Batch Loss: 0.051429010927677155\n",
      "Epoch 5941, Loss: 0.11060841009020805, Final Batch Loss: 0.03496498987078667\n",
      "Epoch 5942, Loss: 0.1470629433169961, Final Batch Loss: 0.06707748770713806\n",
      "Epoch 5943, Loss: 0.14791623130440712, Final Batch Loss: 0.04125306010246277\n",
      "Epoch 5944, Loss: 0.33158668875694275, Final Batch Loss: 0.12875887751579285\n",
      "Epoch 5945, Loss: 0.16739808395504951, Final Batch Loss: 0.025942519307136536\n",
      "Epoch 5946, Loss: 0.1371092014014721, Final Batch Loss: 0.03152003511786461\n",
      "Epoch 5947, Loss: 0.17676212079823017, Final Batch Loss: 0.04835706204175949\n",
      "Epoch 5948, Loss: 0.1801004521548748, Final Batch Loss: 0.03812853619456291\n",
      "Epoch 5949, Loss: 0.21563540026545525, Final Batch Loss: 0.07222042232751846\n",
      "Epoch 5950, Loss: 0.19572177901864052, Final Batch Loss: 0.032121408730745316\n",
      "Epoch 5951, Loss: 0.25905280373990536, Final Batch Loss: 0.09049565345048904\n",
      "Epoch 5952, Loss: 0.15596970077604055, Final Batch Loss: 0.015146923251450062\n",
      "Epoch 5953, Loss: 0.2696552537381649, Final Batch Loss: 0.06352133303880692\n",
      "Epoch 5954, Loss: 0.19181173294782639, Final Batch Loss: 0.028706498444080353\n",
      "Epoch 5955, Loss: 0.16022904869168997, Final Batch Loss: 0.046786654740571976\n",
      "Epoch 5956, Loss: 0.1908966638147831, Final Batch Loss: 0.08434715867042542\n",
      "Epoch 5957, Loss: 0.2687919922173023, Final Batch Loss: 0.07792215049266815\n",
      "Epoch 5958, Loss: 0.20348816737532616, Final Batch Loss: 0.08117319643497467\n",
      "Epoch 5959, Loss: 0.22895196825265884, Final Batch Loss: 0.03722473606467247\n",
      "Epoch 5960, Loss: 0.15628317929804325, Final Batch Loss: 0.04863810911774635\n",
      "Epoch 5961, Loss: 0.17165344767272472, Final Batch Loss: 0.0701114758849144\n",
      "Epoch 5962, Loss: 0.12125263456255198, Final Batch Loss: 0.014236333779990673\n",
      "Epoch 5963, Loss: 0.2904692180454731, Final Batch Loss: 0.08634001016616821\n",
      "Epoch 5964, Loss: 0.1630198247730732, Final Batch Loss: 0.02037852630019188\n",
      "Epoch 5965, Loss: 0.29520824924111366, Final Batch Loss: 0.08657854795455933\n",
      "Epoch 5966, Loss: 0.20932601392269135, Final Batch Loss: 0.045875005424022675\n",
      "Epoch 5967, Loss: 0.2977119144052267, Final Batch Loss: 0.02094212733209133\n",
      "Epoch 5968, Loss: 0.1916390359401703, Final Batch Loss: 0.0666099414229393\n",
      "Epoch 5969, Loss: 0.17746591474860907, Final Batch Loss: 0.009903228841722012\n",
      "Epoch 5970, Loss: 0.2288137748837471, Final Batch Loss: 0.08826464414596558\n",
      "Epoch 5971, Loss: 0.12190289050340652, Final Batch Loss: 0.02837476134300232\n",
      "Epoch 5972, Loss: 0.1439473070204258, Final Batch Loss: 0.03216523304581642\n",
      "Epoch 5973, Loss: 0.24264554679393768, Final Batch Loss: 0.06524544209241867\n",
      "Epoch 5974, Loss: 0.18708748929202557, Final Batch Loss: 0.011767007410526276\n",
      "Epoch 5975, Loss: 0.23279687389731407, Final Batch Loss: 0.0929146409034729\n",
      "Epoch 5976, Loss: 0.34759359061717987, Final Batch Loss: 0.1692362278699875\n",
      "Epoch 5977, Loss: 0.15844475105404854, Final Batch Loss: 0.015777859836816788\n",
      "Epoch 5978, Loss: 0.21925085969269276, Final Batch Loss: 0.022967323660850525\n",
      "Epoch 5979, Loss: 0.21475095860660076, Final Batch Loss: 0.02436174638569355\n",
      "Epoch 5980, Loss: 0.16442691907286644, Final Batch Loss: 0.05078504979610443\n",
      "Epoch 5981, Loss: 0.13924719113856554, Final Batch Loss: 0.056675128638744354\n",
      "Epoch 5982, Loss: 0.15936217829585075, Final Batch Loss: 0.018363680690526962\n",
      "Epoch 5983, Loss: 0.12021966651082039, Final Batch Loss: 0.03368854522705078\n",
      "Epoch 5984, Loss: 0.12577313277870417, Final Batch Loss: 0.012342954985797405\n",
      "Epoch 5985, Loss: 0.13374818116426468, Final Batch Loss: 0.06070481240749359\n",
      "Epoch 5986, Loss: 0.14771785773336887, Final Batch Loss: 0.061198122799396515\n",
      "Epoch 5987, Loss: 0.2026633732020855, Final Batch Loss: 0.04135522246360779\n",
      "Epoch 5988, Loss: 0.22765722125768661, Final Batch Loss: 0.08207477629184723\n",
      "Epoch 5989, Loss: 0.23993799090385437, Final Batch Loss: 0.07867050170898438\n",
      "Epoch 5990, Loss: 0.14901619590818882, Final Batch Loss: 0.018827660009264946\n",
      "Epoch 5991, Loss: 0.17036895640194416, Final Batch Loss: 0.060847628861665726\n",
      "Epoch 5992, Loss: 0.22421608492732048, Final Batch Loss: 0.038201428949832916\n",
      "Epoch 5993, Loss: 0.17133835423737764, Final Batch Loss: 0.010255158878862858\n",
      "Epoch 5994, Loss: 0.14337740279734135, Final Batch Loss: 0.0144368726760149\n",
      "Epoch 5995, Loss: 0.1304360069334507, Final Batch Loss: 0.008601894602179527\n",
      "Epoch 5996, Loss: 0.24570368975400925, Final Batch Loss: 0.0629899725317955\n",
      "Epoch 5997, Loss: 0.23351287469267845, Final Batch Loss: 0.04160387068986893\n",
      "Epoch 5998, Loss: 0.1809440404176712, Final Batch Loss: 0.06646740436553955\n",
      "Epoch 5999, Loss: 0.1624531615525484, Final Batch Loss: 0.06676758080720901\n",
      "Epoch 6000, Loss: 0.15889537800103426, Final Batch Loss: 0.04195232316851616\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  2  0]\n",
      " [ 1 54  0]\n",
      " [ 0  0 52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.952     0.964        42\n",
      "           1      0.964     0.982     0.973        55\n",
      "           2      1.000     1.000     1.000        52\n",
      "\n",
      "    accuracy                          0.980       149\n",
      "   macro avg      0.980     0.978     0.979       149\n",
      "weighted avg      0.980     0.980     0.980       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../saved_models/UCI 3 User Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkimr\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:36:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.976     0.988        42\n",
      "           1      0.982     1.000     0.991        55\n",
      "           2      1.000     1.000     1.000        52\n",
      "\n",
      "    accuracy                          0.993       149\n",
      "   macro avg      0.994     0.992     0.993       149\n",
      "weighted avg      0.993     0.993     0.993       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
