{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                 -0.976623              -0.976353  ...   \n",
       "1                 -0.989046              -0.989038  ...   \n",
       "2                 -0.993552              -0.994122  ...   \n",
       "3                 -0.992407              -0.993142  ...   \n",
       "4                 -0.992378              -0.992542  ...   \n",
       "...                     ...                    ...  ...   \n",
       "7347               0.084878               0.065142  ...   \n",
       "7348               0.098249               0.091791  ...   \n",
       "7349               0.185902               0.170686  ...   \n",
       "7350               0.190360               0.178939  ...   \n",
       "7351               0.022216              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                    -0.998285        1         5  \n",
       "1                    -0.999472        1         5  \n",
       "2                    -0.999807        1         5  \n",
       "3                    -0.999770        1         5  \n",
       "4                    -0.999873        1         5  \n",
       "...                        ...      ...       ...  \n",
       "7347                 -0.584282       30         2  \n",
       "7348                 -0.632536       30         2  \n",
       "7349                 -0.641170       30         2  \n",
       "7350                 -0.663579       30         2  \n",
       "7351                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 48 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2561851739883423, Final Batch Loss: 1.1315664052963257\n",
      "Epoch 2, Loss: 2.250634789466858, Final Batch Loss: 1.1281012296676636\n",
      "Epoch 3, Loss: 2.24462354183197, Final Batch Loss: 1.122954249382019\n",
      "Epoch 4, Loss: 2.2436797618865967, Final Batch Loss: 1.1250792741775513\n",
      "Epoch 5, Loss: 2.2361903190612793, Final Batch Loss: 1.1176565885543823\n",
      "Epoch 6, Loss: 2.2359023094177246, Final Batch Loss: 1.117112398147583\n",
      "Epoch 7, Loss: 2.230678915977478, Final Batch Loss: 1.1096144914627075\n",
      "Epoch 8, Loss: 2.229153275489807, Final Batch Loss: 1.1143981218338013\n",
      "Epoch 9, Loss: 2.2271103858947754, Final Batch Loss: 1.1177709102630615\n",
      "Epoch 10, Loss: 2.2187159061431885, Final Batch Loss: 1.1038583517074585\n",
      "Epoch 11, Loss: 2.2184274196624756, Final Batch Loss: 1.1028419733047485\n",
      "Epoch 12, Loss: 2.2169301509857178, Final Batch Loss: 1.106490969657898\n",
      "Epoch 13, Loss: 2.2157028913497925, Final Batch Loss: 1.108971118927002\n",
      "Epoch 14, Loss: 2.212509036064148, Final Batch Loss: 1.1096324920654297\n",
      "Epoch 15, Loss: 2.2075119018554688, Final Batch Loss: 1.1081939935684204\n",
      "Epoch 16, Loss: 2.2047780752182007, Final Batch Loss: 1.104714274406433\n",
      "Epoch 17, Loss: 2.2008368968963623, Final Batch Loss: 1.1056267023086548\n",
      "Epoch 18, Loss: 2.1952173709869385, Final Batch Loss: 1.0955994129180908\n",
      "Epoch 19, Loss: 2.186653256416321, Final Batch Loss: 1.0824189186096191\n",
      "Epoch 20, Loss: 2.1889108419418335, Final Batch Loss: 1.09318208694458\n",
      "Epoch 21, Loss: 2.1807661056518555, Final Batch Loss: 1.0861936807632446\n",
      "Epoch 22, Loss: 2.1750839948654175, Final Batch Loss: 1.0870050191879272\n",
      "Epoch 23, Loss: 2.158980369567871, Final Batch Loss: 1.0739821195602417\n",
      "Epoch 24, Loss: 2.1539809703826904, Final Batch Loss: 1.0730750560760498\n",
      "Epoch 25, Loss: 2.144044041633606, Final Batch Loss: 1.0638329982757568\n",
      "Epoch 26, Loss: 2.1343061923980713, Final Batch Loss: 1.0652519464492798\n",
      "Epoch 27, Loss: 2.118276000022888, Final Batch Loss: 1.056599497795105\n",
      "Epoch 28, Loss: 2.1054837703704834, Final Batch Loss: 1.0528677701950073\n",
      "Epoch 29, Loss: 2.100741147994995, Final Batch Loss: 1.051344394683838\n",
      "Epoch 30, Loss: 2.0713584423065186, Final Batch Loss: 1.0320719480514526\n",
      "Epoch 31, Loss: 2.055939793586731, Final Batch Loss: 1.0280711650848389\n",
      "Epoch 32, Loss: 2.0404211282730103, Final Batch Loss: 1.0189464092254639\n",
      "Epoch 33, Loss: 2.029049515724182, Final Batch Loss: 1.0111172199249268\n",
      "Epoch 34, Loss: 1.9789727330207825, Final Batch Loss: 0.981414794921875\n",
      "Epoch 35, Loss: 1.9718930125236511, Final Batch Loss: 0.9929473996162415\n",
      "Epoch 36, Loss: 1.9278658032417297, Final Batch Loss: 0.9695354700088501\n",
      "Epoch 37, Loss: 1.8817724585533142, Final Batch Loss: 0.9308284521102905\n",
      "Epoch 38, Loss: 1.8505173325538635, Final Batch Loss: 0.9050569534301758\n",
      "Epoch 39, Loss: 1.8484282493591309, Final Batch Loss: 0.9105491042137146\n",
      "Epoch 40, Loss: 1.785910964012146, Final Batch Loss: 0.8724806308746338\n",
      "Epoch 41, Loss: 1.7671688199043274, Final Batch Loss: 0.8488082885742188\n",
      "Epoch 42, Loss: 1.6841436624526978, Final Batch Loss: 0.816580057144165\n",
      "Epoch 43, Loss: 1.6894973516464233, Final Batch Loss: 0.8557379245758057\n",
      "Epoch 44, Loss: 1.6310377717018127, Final Batch Loss: 0.8482852578163147\n",
      "Epoch 45, Loss: 1.5641412734985352, Final Batch Loss: 0.7630767226219177\n",
      "Epoch 46, Loss: 1.5295414924621582, Final Batch Loss: 0.7328048944473267\n",
      "Epoch 47, Loss: 1.5802047848701477, Final Batch Loss: 0.7806981205940247\n",
      "Epoch 48, Loss: 1.5109349489212036, Final Batch Loss: 0.7607152462005615\n",
      "Epoch 49, Loss: 1.43120276927948, Final Batch Loss: 0.7606812119483948\n",
      "Epoch 50, Loss: 1.4363255500793457, Final Batch Loss: 0.7296576499938965\n",
      "Epoch 51, Loss: 1.3936858177185059, Final Batch Loss: 0.6713854074478149\n",
      "Epoch 52, Loss: 1.3664804100990295, Final Batch Loss: 0.676259458065033\n",
      "Epoch 53, Loss: 1.3573930263519287, Final Batch Loss: 0.6414870619773865\n",
      "Epoch 54, Loss: 1.3249504566192627, Final Batch Loss: 0.6170933842658997\n",
      "Epoch 55, Loss: 1.3461397886276245, Final Batch Loss: 0.6722182631492615\n",
      "Epoch 56, Loss: 1.3211338520050049, Final Batch Loss: 0.6978088021278381\n",
      "Epoch 57, Loss: 1.2866849899291992, Final Batch Loss: 0.5997415781021118\n",
      "Epoch 58, Loss: 1.211650550365448, Final Batch Loss: 0.5685606598854065\n",
      "Epoch 59, Loss: 1.2926299571990967, Final Batch Loss: 0.7007730007171631\n",
      "Epoch 60, Loss: 1.2012177109718323, Final Batch Loss: 0.5969376564025879\n",
      "Epoch 61, Loss: 1.251434326171875, Final Batch Loss: 0.6673756241798401\n",
      "Epoch 62, Loss: 1.2041887640953064, Final Batch Loss: 0.5978952050209045\n",
      "Epoch 63, Loss: 1.207775354385376, Final Batch Loss: 0.6186587810516357\n",
      "Epoch 64, Loss: 1.2072426080703735, Final Batch Loss: 0.5736616849899292\n",
      "Epoch 65, Loss: 1.1763268113136292, Final Batch Loss: 0.5232524871826172\n",
      "Epoch 66, Loss: 1.1522796750068665, Final Batch Loss: 0.5897597670555115\n",
      "Epoch 67, Loss: 1.145469307899475, Final Batch Loss: 0.6071045398712158\n",
      "Epoch 68, Loss: 1.1346464157104492, Final Batch Loss: 0.5459545254707336\n",
      "Epoch 69, Loss: 1.1138120889663696, Final Batch Loss: 0.5388618111610413\n",
      "Epoch 70, Loss: 1.0777313113212585, Final Batch Loss: 0.539600670337677\n",
      "Epoch 71, Loss: 1.0687021315097809, Final Batch Loss: 0.49400803446769714\n",
      "Epoch 72, Loss: 1.124768614768982, Final Batch Loss: 0.6016479134559631\n",
      "Epoch 73, Loss: 1.0689839720726013, Final Batch Loss: 0.5266345143318176\n",
      "Epoch 74, Loss: 1.0422359108924866, Final Batch Loss: 0.5235839486122131\n",
      "Epoch 75, Loss: 1.0776576399803162, Final Batch Loss: 0.5527549982070923\n",
      "Epoch 76, Loss: 1.0108362436294556, Final Batch Loss: 0.5051247477531433\n",
      "Epoch 77, Loss: 1.035755306482315, Final Batch Loss: 0.48220089077949524\n",
      "Epoch 78, Loss: 1.0488105714321136, Final Batch Loss: 0.4903773367404938\n",
      "Epoch 79, Loss: 1.042729914188385, Final Batch Loss: 0.5652071237564087\n",
      "Epoch 80, Loss: 1.020227074623108, Final Batch Loss: 0.5018786787986755\n",
      "Epoch 81, Loss: 0.9455130696296692, Final Batch Loss: 0.47578102350234985\n",
      "Epoch 82, Loss: 0.9939490556716919, Final Batch Loss: 0.46985018253326416\n",
      "Epoch 83, Loss: 0.9803656041622162, Final Batch Loss: 0.5144388675689697\n",
      "Epoch 84, Loss: 0.99271559715271, Final Batch Loss: 0.47159266471862793\n",
      "Epoch 85, Loss: 0.967903196811676, Final Batch Loss: 0.5256332159042358\n",
      "Epoch 86, Loss: 0.9293585121631622, Final Batch Loss: 0.44120219349861145\n",
      "Epoch 87, Loss: 0.9329769313335419, Final Batch Loss: 0.4663684368133545\n",
      "Epoch 88, Loss: 0.9226385056972504, Final Batch Loss: 0.4507887065410614\n",
      "Epoch 89, Loss: 0.9609251320362091, Final Batch Loss: 0.5240111947059631\n",
      "Epoch 90, Loss: 0.9105006158351898, Final Batch Loss: 0.4817444980144501\n",
      "Epoch 91, Loss: 0.8821538984775543, Final Batch Loss: 0.404279500246048\n",
      "Epoch 92, Loss: 0.9122559130191803, Final Batch Loss: 0.430724561214447\n",
      "Epoch 93, Loss: 0.9132010042667389, Final Batch Loss: 0.4064527451992035\n",
      "Epoch 94, Loss: 0.895525187253952, Final Batch Loss: 0.4915826916694641\n",
      "Epoch 95, Loss: 0.8528590500354767, Final Batch Loss: 0.4357791244983673\n",
      "Epoch 96, Loss: 0.8701395094394684, Final Batch Loss: 0.4269571900367737\n",
      "Epoch 97, Loss: 0.8433880507946014, Final Batch Loss: 0.38436171412467957\n",
      "Epoch 98, Loss: 0.8118247985839844, Final Batch Loss: 0.4098465144634247\n",
      "Epoch 99, Loss: 0.8310047388076782, Final Batch Loss: 0.42069119215011597\n",
      "Epoch 100, Loss: 0.8890286684036255, Final Batch Loss: 0.49198734760284424\n",
      "Epoch 101, Loss: 0.7834764420986176, Final Batch Loss: 0.340739369392395\n",
      "Epoch 102, Loss: 0.8390448987483978, Final Batch Loss: 0.4291589856147766\n",
      "Epoch 103, Loss: 0.7634178996086121, Final Batch Loss: 0.35433751344680786\n",
      "Epoch 104, Loss: 0.7522217929363251, Final Batch Loss: 0.37353023886680603\n",
      "Epoch 105, Loss: 0.8397682309150696, Final Batch Loss: 0.4627948999404907\n",
      "Epoch 106, Loss: 0.8039442598819733, Final Batch Loss: 0.4243928790092468\n",
      "Epoch 107, Loss: 0.8202199637889862, Final Batch Loss: 0.4221976101398468\n",
      "Epoch 108, Loss: 0.7527172267436981, Final Batch Loss: 0.38282451033592224\n",
      "Epoch 109, Loss: 0.7897106111049652, Final Batch Loss: 0.45349767804145813\n",
      "Epoch 110, Loss: 0.7478369176387787, Final Batch Loss: 0.38490423560142517\n",
      "Epoch 111, Loss: 0.7575654089450836, Final Batch Loss: 0.4023554027080536\n",
      "Epoch 112, Loss: 0.7282633781433105, Final Batch Loss: 0.33642154932022095\n",
      "Epoch 113, Loss: 0.7055363059043884, Final Batch Loss: 0.3645082414150238\n",
      "Epoch 114, Loss: 0.7031747400760651, Final Batch Loss: 0.40448197722435\n",
      "Epoch 115, Loss: 0.6807917654514313, Final Batch Loss: 0.2910386323928833\n",
      "Epoch 116, Loss: 0.7215392291545868, Final Batch Loss: 0.35654449462890625\n",
      "Epoch 117, Loss: 0.7439083158969879, Final Batch Loss: 0.3391539454460144\n",
      "Epoch 118, Loss: 0.7845856249332428, Final Batch Loss: 0.4437679946422577\n",
      "Epoch 119, Loss: 0.6910708546638489, Final Batch Loss: 0.32799777388572693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 0.7351240515708923, Final Batch Loss: 0.3997364044189453\n",
      "Epoch 121, Loss: 0.7000839114189148, Final Batch Loss: 0.3566740155220032\n",
      "Epoch 122, Loss: 0.7996819615364075, Final Batch Loss: 0.4666072428226471\n",
      "Epoch 123, Loss: 0.6707537472248077, Final Batch Loss: 0.34138649702072144\n",
      "Epoch 124, Loss: 0.6499307155609131, Final Batch Loss: 0.3267380893230438\n",
      "Epoch 125, Loss: 0.6515974402427673, Final Batch Loss: 0.30976197123527527\n",
      "Epoch 126, Loss: 0.6838276386260986, Final Batch Loss: 0.33560600876808167\n",
      "Epoch 127, Loss: 0.7162894010543823, Final Batch Loss: 0.3535943627357483\n",
      "Epoch 128, Loss: 0.6940189599990845, Final Batch Loss: 0.33244559168815613\n",
      "Epoch 129, Loss: 0.6674171388149261, Final Batch Loss: 0.3701196610927582\n",
      "Epoch 130, Loss: 0.645494282245636, Final Batch Loss: 0.30947360396385193\n",
      "Epoch 131, Loss: 0.6788782775402069, Final Batch Loss: 0.36584582924842834\n",
      "Epoch 132, Loss: 0.6601908206939697, Final Batch Loss: 0.35880202054977417\n",
      "Epoch 133, Loss: 0.6995886862277985, Final Batch Loss: 0.358432412147522\n",
      "Epoch 134, Loss: 0.6692832410335541, Final Batch Loss: 0.346000075340271\n",
      "Epoch 135, Loss: 0.6867000460624695, Final Batch Loss: 0.3431389629840851\n",
      "Epoch 136, Loss: 0.6154429912567139, Final Batch Loss: 0.34923896193504333\n",
      "Epoch 137, Loss: 0.5768948793411255, Final Batch Loss: 0.24054798483848572\n",
      "Epoch 138, Loss: 0.6136956512928009, Final Batch Loss: 0.3077430725097656\n",
      "Epoch 139, Loss: 0.5615153014659882, Final Batch Loss: 0.2801480293273926\n",
      "Epoch 140, Loss: 0.6071102321147919, Final Batch Loss: 0.31832924485206604\n",
      "Epoch 141, Loss: 0.6373214721679688, Final Batch Loss: 0.3385363519191742\n",
      "Epoch 142, Loss: 0.5582405626773834, Final Batch Loss: 0.26258715987205505\n",
      "Epoch 143, Loss: 0.6085532903671265, Final Batch Loss: 0.3036683201789856\n",
      "Epoch 144, Loss: 0.6032659411430359, Final Batch Loss: 0.2846508324146271\n",
      "Epoch 145, Loss: 0.5796414315700531, Final Batch Loss: 0.2414400577545166\n",
      "Epoch 146, Loss: 0.6008633971214294, Final Batch Loss: 0.3466618061065674\n",
      "Epoch 147, Loss: 0.5872270166873932, Final Batch Loss: 0.31177186965942383\n",
      "Epoch 148, Loss: 0.6105722188949585, Final Batch Loss: 0.3131629526615143\n",
      "Epoch 149, Loss: 0.5913447141647339, Final Batch Loss: 0.29332235455513\n",
      "Epoch 150, Loss: 0.6096203923225403, Final Batch Loss: 0.33478090167045593\n",
      "Epoch 151, Loss: 0.6261745691299438, Final Batch Loss: 0.324612557888031\n",
      "Epoch 152, Loss: 0.533322811126709, Final Batch Loss: 0.2468157410621643\n",
      "Epoch 153, Loss: 0.566094160079956, Final Batch Loss: 0.2905670404434204\n",
      "Epoch 154, Loss: 0.5354204475879669, Final Batch Loss: 0.25687140226364136\n",
      "Epoch 155, Loss: 0.5660272836685181, Final Batch Loss: 0.2773556113243103\n",
      "Epoch 156, Loss: 0.5377807319164276, Final Batch Loss: 0.21786350011825562\n",
      "Epoch 157, Loss: 0.6050533652305603, Final Batch Loss: 0.31163665652275085\n",
      "Epoch 158, Loss: 0.5629905164241791, Final Batch Loss: 0.276368111371994\n",
      "Epoch 159, Loss: 0.5199055820703506, Final Batch Loss: 0.24984927475452423\n",
      "Epoch 160, Loss: 0.5661044716835022, Final Batch Loss: 0.3000493347644806\n",
      "Epoch 161, Loss: 0.573094516992569, Final Batch Loss: 0.3012370467185974\n",
      "Epoch 162, Loss: 0.6078886389732361, Final Batch Loss: 0.27042126655578613\n",
      "Epoch 163, Loss: 0.5601083934307098, Final Batch Loss: 0.3040899932384491\n",
      "Epoch 164, Loss: 0.5316198766231537, Final Batch Loss: 0.23731231689453125\n",
      "Epoch 165, Loss: 0.553743302822113, Final Batch Loss: 0.2552943825721741\n",
      "Epoch 166, Loss: 0.5395351350307465, Final Batch Loss: 0.24078595638275146\n",
      "Epoch 167, Loss: 0.4983048290014267, Final Batch Loss: 0.23155911266803741\n",
      "Epoch 168, Loss: 0.49811404943466187, Final Batch Loss: 0.21656489372253418\n",
      "Epoch 169, Loss: 0.5271148681640625, Final Batch Loss: 0.2732835114002228\n",
      "Epoch 170, Loss: 0.56178018450737, Final Batch Loss: 0.31249380111694336\n",
      "Epoch 171, Loss: 0.5640315413475037, Final Batch Loss: 0.2691744863986969\n",
      "Epoch 172, Loss: 0.5574043989181519, Final Batch Loss: 0.2917078733444214\n",
      "Epoch 173, Loss: 0.5329859554767609, Final Batch Loss: 0.2551892399787903\n",
      "Epoch 174, Loss: 0.5034007430076599, Final Batch Loss: 0.250272661447525\n",
      "Epoch 175, Loss: 0.5370394736528397, Final Batch Loss: 0.2997400760650635\n",
      "Epoch 176, Loss: 0.5274661779403687, Final Batch Loss: 0.23966971039772034\n",
      "Epoch 177, Loss: 0.5102867633104324, Final Batch Loss: 0.27812090516090393\n",
      "Epoch 178, Loss: 0.5331031829118729, Final Batch Loss: 0.29082822799682617\n",
      "Epoch 179, Loss: 0.5298333764076233, Final Batch Loss: 0.29915115237236023\n",
      "Epoch 180, Loss: 0.5530907213687897, Final Batch Loss: 0.28317317366600037\n",
      "Epoch 181, Loss: 0.5080739110708237, Final Batch Loss: 0.24579469859600067\n",
      "Epoch 182, Loss: 0.5088445395231247, Final Batch Loss: 0.2687222361564636\n",
      "Epoch 183, Loss: 0.525042250752449, Final Batch Loss: 0.305582731962204\n",
      "Epoch 184, Loss: 0.4498223811388016, Final Batch Loss: 0.23834677040576935\n",
      "Epoch 185, Loss: 0.49915075302124023, Final Batch Loss: 0.22474920749664307\n",
      "Epoch 186, Loss: 0.5117918848991394, Final Batch Loss: 0.24474266171455383\n",
      "Epoch 187, Loss: 0.4593774229288101, Final Batch Loss: 0.22616046667099\n",
      "Epoch 188, Loss: 0.4597429484128952, Final Batch Loss: 0.2227432280778885\n",
      "Epoch 189, Loss: 0.47920002043247223, Final Batch Loss: 0.2068110853433609\n",
      "Epoch 190, Loss: 0.48130664229393005, Final Batch Loss: 0.20416101813316345\n",
      "Epoch 191, Loss: 0.43943923711776733, Final Batch Loss: 0.17424970865249634\n",
      "Epoch 192, Loss: 0.4682234823703766, Final Batch Loss: 0.23859690129756927\n",
      "Epoch 193, Loss: 0.4170420914888382, Final Batch Loss: 0.19101974368095398\n",
      "Epoch 194, Loss: 0.4733211398124695, Final Batch Loss: 0.2145937979221344\n",
      "Epoch 195, Loss: 0.4965905100107193, Final Batch Loss: 0.2604465186595917\n",
      "Epoch 196, Loss: 0.4626995176076889, Final Batch Loss: 0.22068864107131958\n",
      "Epoch 197, Loss: 0.4802263230085373, Final Batch Loss: 0.2659596800804138\n",
      "Epoch 198, Loss: 0.46067680418491364, Final Batch Loss: 0.226941779255867\n",
      "Epoch 199, Loss: 0.4776126444339752, Final Batch Loss: 0.23891080915927887\n",
      "Epoch 200, Loss: 0.46858397126197815, Final Batch Loss: 0.2663641571998596\n",
      "Epoch 201, Loss: 0.43359050154685974, Final Batch Loss: 0.23564687371253967\n",
      "Epoch 202, Loss: 0.4883747845888138, Final Batch Loss: 0.2705153822898865\n",
      "Epoch 203, Loss: 0.4798465967178345, Final Batch Loss: 0.24746160209178925\n",
      "Epoch 204, Loss: 0.49781104922294617, Final Batch Loss: 0.280857652425766\n",
      "Epoch 205, Loss: 0.4898337870836258, Final Batch Loss: 0.2613622844219208\n",
      "Epoch 206, Loss: 0.46337950229644775, Final Batch Loss: 0.222402423620224\n",
      "Epoch 207, Loss: 0.4416993707418442, Final Batch Loss: 0.2423926740884781\n",
      "Epoch 208, Loss: 0.4532533884048462, Final Batch Loss: 0.21787086129188538\n",
      "Epoch 209, Loss: 0.46862202882766724, Final Batch Loss: 0.22198790311813354\n",
      "Epoch 210, Loss: 0.446722149848938, Final Batch Loss: 0.21202509105205536\n",
      "Epoch 211, Loss: 0.49785952270030975, Final Batch Loss: 0.2602088451385498\n",
      "Epoch 212, Loss: 0.41704584658145905, Final Batch Loss: 0.18933236598968506\n",
      "Epoch 213, Loss: 0.4195498526096344, Final Batch Loss: 0.19850870966911316\n",
      "Epoch 214, Loss: 0.45852652192115784, Final Batch Loss: 0.22839827835559845\n",
      "Epoch 215, Loss: 0.4681348204612732, Final Batch Loss: 0.2559159994125366\n",
      "Epoch 216, Loss: 0.48853543400764465, Final Batch Loss: 0.26854363083839417\n",
      "Epoch 217, Loss: 0.45308876037597656, Final Batch Loss: 0.25392377376556396\n",
      "Epoch 218, Loss: 0.46631480753421783, Final Batch Loss: 0.25848355889320374\n",
      "Epoch 219, Loss: 0.46847932040691376, Final Batch Loss: 0.20491553843021393\n",
      "Epoch 220, Loss: 0.41440384089946747, Final Batch Loss: 0.18905629217624664\n",
      "Epoch 221, Loss: 0.4557068794965744, Final Batch Loss: 0.23910291492938995\n",
      "Epoch 222, Loss: 0.43803246319293976, Final Batch Loss: 0.20382167398929596\n",
      "Epoch 223, Loss: 0.43027159571647644, Final Batch Loss: 0.21887937188148499\n",
      "Epoch 224, Loss: 0.5068739950656891, Final Batch Loss: 0.2838558852672577\n",
      "Epoch 225, Loss: 0.43358515202999115, Final Batch Loss: 0.22770272195339203\n",
      "Epoch 226, Loss: 0.5003149807453156, Final Batch Loss: 0.285297691822052\n",
      "Epoch 227, Loss: 0.3979790657758713, Final Batch Loss: 0.17194031178951263\n",
      "Epoch 228, Loss: 0.43831661343574524, Final Batch Loss: 0.22396054863929749\n",
      "Epoch 229, Loss: 0.43423987925052643, Final Batch Loss: 0.22661063075065613\n",
      "Epoch 230, Loss: 0.46047787368297577, Final Batch Loss: 0.25607767701148987\n",
      "Epoch 231, Loss: 0.44759659469127655, Final Batch Loss: 0.23457108438014984\n",
      "Epoch 232, Loss: 0.3969832956790924, Final Batch Loss: 0.16728676855564117\n",
      "Epoch 233, Loss: 0.46676503121852875, Final Batch Loss: 0.26148557662963867\n",
      "Epoch 234, Loss: 0.42369240522384644, Final Batch Loss: 0.20635487139225006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235, Loss: 0.4052198827266693, Final Batch Loss: 0.17176900804042816\n",
      "Epoch 236, Loss: 0.43391500413417816, Final Batch Loss: 0.23978878557682037\n",
      "Epoch 237, Loss: 0.4316342920064926, Final Batch Loss: 0.21888595819473267\n",
      "Epoch 238, Loss: 0.4220286160707474, Final Batch Loss: 0.21852603554725647\n",
      "Epoch 239, Loss: 0.42752671241760254, Final Batch Loss: 0.1995512992143631\n",
      "Epoch 240, Loss: 0.4142807573080063, Final Batch Loss: 0.234005868434906\n",
      "Epoch 241, Loss: 0.38969600200653076, Final Batch Loss: 0.15723726153373718\n",
      "Epoch 242, Loss: 0.4098879396915436, Final Batch Loss: 0.2136598378419876\n",
      "Epoch 243, Loss: 0.4227823168039322, Final Batch Loss: 0.2077873796224594\n",
      "Epoch 244, Loss: 0.43095147609710693, Final Batch Loss: 0.19363749027252197\n",
      "Epoch 245, Loss: 0.4399360418319702, Final Batch Loss: 0.23653081059455872\n",
      "Epoch 246, Loss: 0.3953663259744644, Final Batch Loss: 0.19994734227657318\n",
      "Epoch 247, Loss: 0.40117375552654266, Final Batch Loss: 0.18128472566604614\n",
      "Epoch 248, Loss: 0.3913278877735138, Final Batch Loss: 0.18626956641674042\n",
      "Epoch 249, Loss: 0.3505152016878128, Final Batch Loss: 0.13605180382728577\n",
      "Epoch 250, Loss: 0.3925108015537262, Final Batch Loss: 0.1894465833902359\n",
      "Epoch 251, Loss: 0.41978301107883453, Final Batch Loss: 0.20713505148887634\n",
      "Epoch 252, Loss: 0.38322773575782776, Final Batch Loss: 0.1512048989534378\n",
      "Epoch 253, Loss: 0.37008534371852875, Final Batch Loss: 0.1849977672100067\n",
      "Epoch 254, Loss: 0.4010334312915802, Final Batch Loss: 0.18658843636512756\n",
      "Epoch 255, Loss: 0.4551668167114258, Final Batch Loss: 0.25286757946014404\n",
      "Epoch 256, Loss: 0.43188586831092834, Final Batch Loss: 0.22462400794029236\n",
      "Epoch 257, Loss: 0.4161140024662018, Final Batch Loss: 0.1824781745672226\n",
      "Epoch 258, Loss: 0.41447772085666656, Final Batch Loss: 0.16442854702472687\n",
      "Epoch 259, Loss: 0.4254416972398758, Final Batch Loss: 0.18478022515773773\n",
      "Epoch 260, Loss: 0.3916417211294174, Final Batch Loss: 0.21276991069316864\n",
      "Epoch 261, Loss: 0.3916216790676117, Final Batch Loss: 0.21581541001796722\n",
      "Epoch 262, Loss: 0.4040680527687073, Final Batch Loss: 0.19194911420345306\n",
      "Epoch 263, Loss: 0.4003850966691971, Final Batch Loss: 0.19078972935676575\n",
      "Epoch 264, Loss: 0.38291749358177185, Final Batch Loss: 0.1868215650320053\n",
      "Epoch 265, Loss: 0.39413005113601685, Final Batch Loss: 0.17324298620224\n",
      "Epoch 266, Loss: 0.3699515610933304, Final Batch Loss: 0.16495095193386078\n",
      "Epoch 267, Loss: 0.4006648063659668, Final Batch Loss: 0.19452843070030212\n",
      "Epoch 268, Loss: 0.3568231910467148, Final Batch Loss: 0.1865883767604828\n",
      "Epoch 269, Loss: 0.3930891305208206, Final Batch Loss: 0.23287899792194366\n",
      "Epoch 270, Loss: 0.43340227007865906, Final Batch Loss: 0.2447720170021057\n",
      "Epoch 271, Loss: 0.37658099830150604, Final Batch Loss: 0.19023822247982025\n",
      "Epoch 272, Loss: 0.3660695403814316, Final Batch Loss: 0.2000669240951538\n",
      "Epoch 273, Loss: 0.3634730875492096, Final Batch Loss: 0.19802282750606537\n",
      "Epoch 274, Loss: 0.41114601492881775, Final Batch Loss: 0.20316511392593384\n",
      "Epoch 275, Loss: 0.36835455894470215, Final Batch Loss: 0.1696546971797943\n",
      "Epoch 276, Loss: 0.40008747577667236, Final Batch Loss: 0.22323064506053925\n",
      "Epoch 277, Loss: 0.3840716928243637, Final Batch Loss: 0.18859277665615082\n",
      "Epoch 278, Loss: 0.3729914575815201, Final Batch Loss: 0.2025468349456787\n",
      "Epoch 279, Loss: 0.3608027398586273, Final Batch Loss: 0.18334302306175232\n",
      "Epoch 280, Loss: 0.39205022156238556, Final Batch Loss: 0.19380560517311096\n",
      "Epoch 281, Loss: 0.33821289241313934, Final Batch Loss: 0.15961070358753204\n",
      "Epoch 282, Loss: 0.3695499002933502, Final Batch Loss: 0.19356146454811096\n",
      "Epoch 283, Loss: 0.3721087872982025, Final Batch Loss: 0.16780145466327667\n",
      "Epoch 284, Loss: 0.3472340852022171, Final Batch Loss: 0.1436976045370102\n",
      "Epoch 285, Loss: 0.39974014461040497, Final Batch Loss: 0.18919740617275238\n",
      "Epoch 286, Loss: 0.3783279210329056, Final Batch Loss: 0.18029001355171204\n",
      "Epoch 287, Loss: 0.37882333993911743, Final Batch Loss: 0.18838347494602203\n",
      "Epoch 288, Loss: 0.38020390272140503, Final Batch Loss: 0.20579832792282104\n",
      "Epoch 289, Loss: 0.36102452874183655, Final Batch Loss: 0.12663915753364563\n",
      "Epoch 290, Loss: 0.37504370510578156, Final Batch Loss: 0.18749617040157318\n",
      "Epoch 291, Loss: 0.36755993962287903, Final Batch Loss: 0.2064991146326065\n",
      "Epoch 292, Loss: 0.35047009587287903, Final Batch Loss: 0.16251002252101898\n",
      "Epoch 293, Loss: 0.35745444893836975, Final Batch Loss: 0.17284083366394043\n",
      "Epoch 294, Loss: 0.34244346618652344, Final Batch Loss: 0.15174728631973267\n",
      "Epoch 295, Loss: 0.37631067633628845, Final Batch Loss: 0.18466153740882874\n",
      "Epoch 296, Loss: 0.39314600825309753, Final Batch Loss: 0.21695087850093842\n",
      "Epoch 297, Loss: 0.364892840385437, Final Batch Loss: 0.2054150551557541\n",
      "Epoch 298, Loss: 0.34025222063064575, Final Batch Loss: 0.17745472490787506\n",
      "Epoch 299, Loss: 0.3427091985940933, Final Batch Loss: 0.15720094740390778\n",
      "Epoch 300, Loss: 0.3603130131959915, Final Batch Loss: 0.18603749573230743\n",
      "Epoch 301, Loss: 0.3085731267929077, Final Batch Loss: 0.1705293208360672\n",
      "Epoch 302, Loss: 0.3712601512670517, Final Batch Loss: 0.2096291184425354\n",
      "Epoch 303, Loss: 0.3391827940940857, Final Batch Loss: 0.17694053053855896\n",
      "Epoch 304, Loss: 0.33646824955940247, Final Batch Loss: 0.1458287090063095\n",
      "Epoch 305, Loss: 0.364036500453949, Final Batch Loss: 0.14965838193893433\n",
      "Epoch 306, Loss: 0.3304544687271118, Final Batch Loss: 0.14507585763931274\n",
      "Epoch 307, Loss: 0.4124690443277359, Final Batch Loss: 0.24819865822792053\n",
      "Epoch 308, Loss: 0.34009161591529846, Final Batch Loss: 0.14353109896183014\n",
      "Epoch 309, Loss: 0.3669774979352951, Final Batch Loss: 0.20148803293704987\n",
      "Epoch 310, Loss: 0.3450666218996048, Final Batch Loss: 0.1316361278295517\n",
      "Epoch 311, Loss: 0.3400822877883911, Final Batch Loss: 0.18500719964504242\n",
      "Epoch 312, Loss: 0.3195765018463135, Final Batch Loss: 0.1775141805410385\n",
      "Epoch 313, Loss: 0.32877059280872345, Final Batch Loss: 0.157765194773674\n",
      "Epoch 314, Loss: 0.34410350024700165, Final Batch Loss: 0.16019107401371002\n",
      "Epoch 315, Loss: 0.3523373007774353, Final Batch Loss: 0.17365311086177826\n",
      "Epoch 316, Loss: 0.3328503370285034, Final Batch Loss: 0.17113962769508362\n",
      "Epoch 317, Loss: 0.38360974192619324, Final Batch Loss: 0.21288195252418518\n",
      "Epoch 318, Loss: 0.36528921127319336, Final Batch Loss: 0.20206433534622192\n",
      "Epoch 319, Loss: 0.32096637785434723, Final Batch Loss: 0.1436806321144104\n",
      "Epoch 320, Loss: 0.3322626054286957, Final Batch Loss: 0.18937905132770538\n",
      "Epoch 321, Loss: 0.32379747927188873, Final Batch Loss: 0.1614210605621338\n",
      "Epoch 322, Loss: 0.3301068842411041, Final Batch Loss: 0.15950541198253632\n",
      "Epoch 323, Loss: 0.3253956288099289, Final Batch Loss: 0.1382378339767456\n",
      "Epoch 324, Loss: 0.36187881231307983, Final Batch Loss: 0.20395778119564056\n",
      "Epoch 325, Loss: 0.31220556795597076, Final Batch Loss: 0.17491944134235382\n",
      "Epoch 326, Loss: 0.35215169191360474, Final Batch Loss: 0.14177489280700684\n",
      "Epoch 327, Loss: 0.3176729381084442, Final Batch Loss: 0.16889584064483643\n",
      "Epoch 328, Loss: 0.30648693442344666, Final Batch Loss: 0.13162654638290405\n",
      "Epoch 329, Loss: 0.3294968158006668, Final Batch Loss: 0.17457309365272522\n",
      "Epoch 330, Loss: 0.3415846526622772, Final Batch Loss: 0.16005194187164307\n",
      "Epoch 331, Loss: 0.3569864332675934, Final Batch Loss: 0.16739991307258606\n",
      "Epoch 332, Loss: 0.356616348028183, Final Batch Loss: 0.16158057749271393\n",
      "Epoch 333, Loss: 0.410918727517128, Final Batch Loss: 0.21379508078098297\n",
      "Epoch 334, Loss: 0.3237565904855728, Final Batch Loss: 0.14727318286895752\n",
      "Epoch 335, Loss: 0.34642843902111053, Final Batch Loss: 0.18050308525562286\n",
      "Epoch 336, Loss: 0.3384896069765091, Final Batch Loss: 0.2002432644367218\n",
      "Epoch 337, Loss: 0.31963829696178436, Final Batch Loss: 0.1784035712480545\n",
      "Epoch 338, Loss: 0.3339226245880127, Final Batch Loss: 0.16207094490528107\n",
      "Epoch 339, Loss: 0.3458409607410431, Final Batch Loss: 0.1567012369632721\n",
      "Epoch 340, Loss: 0.386614128947258, Final Batch Loss: 0.22952519357204437\n",
      "Epoch 341, Loss: 0.3773684948682785, Final Batch Loss: 0.1764073520898819\n",
      "Epoch 342, Loss: 0.29252755641937256, Final Batch Loss: 0.13877779245376587\n",
      "Epoch 343, Loss: 0.3312252461910248, Final Batch Loss: 0.15870456397533417\n",
      "Epoch 344, Loss: 0.35233066976070404, Final Batch Loss: 0.2064676135778427\n",
      "Epoch 345, Loss: 0.3167658597230911, Final Batch Loss: 0.14865726232528687\n",
      "Epoch 346, Loss: 0.3279928117990494, Final Batch Loss: 0.16611936688423157\n",
      "Epoch 347, Loss: 0.3330587297677994, Final Batch Loss: 0.17954328656196594\n",
      "Epoch 348, Loss: 0.3194890320301056, Final Batch Loss: 0.14544931054115295\n",
      "Epoch 349, Loss: 0.307789571583271, Final Batch Loss: 0.11931446939706802\n",
      "Epoch 350, Loss: 0.36664965748786926, Final Batch Loss: 0.21274416148662567\n",
      "Epoch 351, Loss: 0.3419535458087921, Final Batch Loss: 0.1888488084077835\n",
      "Epoch 352, Loss: 0.32608649134635925, Final Batch Loss: 0.16863608360290527\n",
      "Epoch 353, Loss: 0.37157702445983887, Final Batch Loss: 0.2235897034406662\n",
      "Epoch 354, Loss: 0.31066446006298065, Final Batch Loss: 0.15565048158168793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355, Loss: 0.30357682704925537, Final Batch Loss: 0.16713933646678925\n",
      "Epoch 356, Loss: 0.314951092004776, Final Batch Loss: 0.16348332166671753\n",
      "Epoch 357, Loss: 0.33481544256210327, Final Batch Loss: 0.14971734583377838\n",
      "Epoch 358, Loss: 0.2731170654296875, Final Batch Loss: 0.13527734577655792\n",
      "Epoch 359, Loss: 0.38680826127529144, Final Batch Loss: 0.22523216903209686\n",
      "Epoch 360, Loss: 0.3171018362045288, Final Batch Loss: 0.16123396158218384\n",
      "Epoch 361, Loss: 0.2938493341207504, Final Batch Loss: 0.13370515406131744\n",
      "Epoch 362, Loss: 0.2887512743473053, Final Batch Loss: 0.13262218236923218\n",
      "Epoch 363, Loss: 0.2963967025279999, Final Batch Loss: 0.13466015458106995\n",
      "Epoch 364, Loss: 0.30450212955474854, Final Batch Loss: 0.15269790589809418\n",
      "Epoch 365, Loss: 0.2838943600654602, Final Batch Loss: 0.13460659980773926\n",
      "Epoch 366, Loss: 0.30519451200962067, Final Batch Loss: 0.133981391787529\n",
      "Epoch 367, Loss: 0.31704941391944885, Final Batch Loss: 0.1733507663011551\n",
      "Epoch 368, Loss: 0.29986438155174255, Final Batch Loss: 0.1320597231388092\n",
      "Epoch 369, Loss: 0.3304635137319565, Final Batch Loss: 0.17670582234859467\n",
      "Epoch 370, Loss: 0.2853889763355255, Final Batch Loss: 0.15357345342636108\n",
      "Epoch 371, Loss: 0.30129916965961456, Final Batch Loss: 0.17607678472995758\n",
      "Epoch 372, Loss: 0.31704311072826385, Final Batch Loss: 0.16737131774425507\n",
      "Epoch 373, Loss: 0.3294641971588135, Final Batch Loss: 0.193797305226326\n",
      "Epoch 374, Loss: 0.2984692305326462, Final Batch Loss: 0.1432509571313858\n",
      "Epoch 375, Loss: 0.2989845871925354, Final Batch Loss: 0.16450956463813782\n",
      "Epoch 376, Loss: 0.27167437970638275, Final Batch Loss: 0.1260107010602951\n",
      "Epoch 377, Loss: 0.2933424413204193, Final Batch Loss: 0.13342547416687012\n",
      "Epoch 378, Loss: 0.2798497676849365, Final Batch Loss: 0.14024634659290314\n",
      "Epoch 379, Loss: 0.30299991369247437, Final Batch Loss: 0.13902276754379272\n",
      "Epoch 380, Loss: 0.3202894777059555, Final Batch Loss: 0.16280466318130493\n",
      "Epoch 381, Loss: 0.2881143242120743, Final Batch Loss: 0.12515288591384888\n",
      "Epoch 382, Loss: 0.36715467274188995, Final Batch Loss: 0.22230669856071472\n",
      "Epoch 383, Loss: 0.29782599210739136, Final Batch Loss: 0.1360119730234146\n",
      "Epoch 384, Loss: 0.33010367304086685, Final Batch Loss: 0.12022288888692856\n",
      "Epoch 385, Loss: 0.2689082697033882, Final Batch Loss: 0.15271848440170288\n",
      "Epoch 386, Loss: 0.31097176671028137, Final Batch Loss: 0.1618230640888214\n",
      "Epoch 387, Loss: 0.26146409660577774, Final Batch Loss: 0.13978561758995056\n",
      "Epoch 388, Loss: 0.3139783591032028, Final Batch Loss: 0.15845134854316711\n",
      "Epoch 389, Loss: 0.28575094789266586, Final Batch Loss: 0.16725289821624756\n",
      "Epoch 390, Loss: 0.2891259416937828, Final Batch Loss: 0.11006171256303787\n",
      "Epoch 391, Loss: 0.24601935595273972, Final Batch Loss: 0.11530261486768723\n",
      "Epoch 392, Loss: 0.2598123997449875, Final Batch Loss: 0.13548454642295837\n",
      "Epoch 393, Loss: 0.2753371149301529, Final Batch Loss: 0.133279949426651\n",
      "Epoch 394, Loss: 0.26403485238552094, Final Batch Loss: 0.12730738520622253\n",
      "Epoch 395, Loss: 0.2722856253385544, Final Batch Loss: 0.14556583762168884\n",
      "Epoch 396, Loss: 0.29301100969314575, Final Batch Loss: 0.16143961250782013\n",
      "Epoch 397, Loss: 0.2930354177951813, Final Batch Loss: 0.15691755712032318\n",
      "Epoch 398, Loss: 0.2747979164123535, Final Batch Loss: 0.14409838616847992\n",
      "Epoch 399, Loss: 0.28708963096141815, Final Batch Loss: 0.1752319484949112\n",
      "Epoch 400, Loss: 0.29273588955402374, Final Batch Loss: 0.12662798166275024\n",
      "Epoch 401, Loss: 0.24997787922620773, Final Batch Loss: 0.11758298426866531\n",
      "Epoch 402, Loss: 0.30532480776309967, Final Batch Loss: 0.15637171268463135\n",
      "Epoch 403, Loss: 0.29096831381320953, Final Batch Loss: 0.16254960000514984\n",
      "Epoch 404, Loss: 0.29556555300951004, Final Batch Loss: 0.18388144671916962\n",
      "Epoch 405, Loss: 0.28253223747015, Final Batch Loss: 0.1196211650967598\n",
      "Epoch 406, Loss: 0.2817733734846115, Final Batch Loss: 0.14629417657852173\n",
      "Epoch 407, Loss: 0.2940063178539276, Final Batch Loss: 0.13292302191257477\n",
      "Epoch 408, Loss: 0.29522378742694855, Final Batch Loss: 0.1405118703842163\n",
      "Epoch 409, Loss: 0.2932600975036621, Final Batch Loss: 0.12481957674026489\n",
      "Epoch 410, Loss: 0.3290414661169052, Final Batch Loss: 0.1650884598493576\n",
      "Epoch 411, Loss: 0.2963326647877693, Final Batch Loss: 0.18178437650203705\n",
      "Epoch 412, Loss: 0.27106088399887085, Final Batch Loss: 0.13745346665382385\n",
      "Epoch 413, Loss: 0.31123295426368713, Final Batch Loss: 0.1728925257921219\n",
      "Epoch 414, Loss: 0.2592974528670311, Final Batch Loss: 0.1419002115726471\n",
      "Epoch 415, Loss: 0.3422593027353287, Final Batch Loss: 0.15910185873508453\n",
      "Epoch 416, Loss: 0.3118598461151123, Final Batch Loss: 0.18286727368831635\n",
      "Epoch 417, Loss: 0.25073859095573425, Final Batch Loss: 0.13646221160888672\n",
      "Epoch 418, Loss: 0.31342004239559174, Final Batch Loss: 0.18161547183990479\n",
      "Epoch 419, Loss: 0.23121809214353561, Final Batch Loss: 0.11786045879125595\n",
      "Epoch 420, Loss: 0.26592186093330383, Final Batch Loss: 0.13908734917640686\n",
      "Epoch 421, Loss: 0.26702989637851715, Final Batch Loss: 0.13521331548690796\n",
      "Epoch 422, Loss: 0.23688898980617523, Final Batch Loss: 0.11938036233186722\n",
      "Epoch 423, Loss: 0.29652925580739975, Final Batch Loss: 0.17610540986061096\n",
      "Epoch 424, Loss: 0.27017824351787567, Final Batch Loss: 0.1566743701696396\n",
      "Epoch 425, Loss: 0.2520991861820221, Final Batch Loss: 0.0916636735200882\n",
      "Epoch 426, Loss: 0.2273336872458458, Final Batch Loss: 0.10823506116867065\n",
      "Epoch 427, Loss: 0.2883933261036873, Final Batch Loss: 0.1652783900499344\n",
      "Epoch 428, Loss: 0.2720310240983963, Final Batch Loss: 0.14571644365787506\n",
      "Epoch 429, Loss: 0.28140194714069366, Final Batch Loss: 0.10666973888874054\n",
      "Epoch 430, Loss: 0.26209818571805954, Final Batch Loss: 0.10358551889657974\n",
      "Epoch 431, Loss: 0.2954963445663452, Final Batch Loss: 0.17404939234256744\n",
      "Epoch 432, Loss: 0.28921009600162506, Final Batch Loss: 0.1624959111213684\n",
      "Epoch 433, Loss: 0.23095368593931198, Final Batch Loss: 0.12284060567617416\n",
      "Epoch 434, Loss: 0.2536405771970749, Final Batch Loss: 0.12393295764923096\n",
      "Epoch 435, Loss: 0.2694166600704193, Final Batch Loss: 0.128621906042099\n",
      "Epoch 436, Loss: 0.2728685289621353, Final Batch Loss: 0.14210346341133118\n",
      "Epoch 437, Loss: 0.2414843663573265, Final Batch Loss: 0.12436458468437195\n",
      "Epoch 438, Loss: 0.2793962359428406, Final Batch Loss: 0.15427103638648987\n",
      "Epoch 439, Loss: 0.2819388061761856, Final Batch Loss: 0.1506836712360382\n",
      "Epoch 440, Loss: 0.3024648278951645, Final Batch Loss: 0.15908169746398926\n",
      "Epoch 441, Loss: 0.2792349010705948, Final Batch Loss: 0.15059314668178558\n",
      "Epoch 442, Loss: 0.36642707884311676, Final Batch Loss: 0.23857338726520538\n",
      "Epoch 443, Loss: 0.24858012795448303, Final Batch Loss: 0.11711661517620087\n",
      "Epoch 444, Loss: 0.25351621210575104, Final Batch Loss: 0.14038170874118805\n",
      "Epoch 445, Loss: 0.2940094470977783, Final Batch Loss: 0.1453568935394287\n",
      "Epoch 446, Loss: 0.264480784535408, Final Batch Loss: 0.12664152681827545\n",
      "Epoch 447, Loss: 0.299719899892807, Final Batch Loss: 0.15553796291351318\n",
      "Epoch 448, Loss: 0.25101614743471146, Final Batch Loss: 0.15262575447559357\n",
      "Epoch 449, Loss: 0.24253150820732117, Final Batch Loss: 0.1292308121919632\n",
      "Epoch 450, Loss: 0.2549974322319031, Final Batch Loss: 0.13321539759635925\n",
      "Epoch 451, Loss: 0.2643171027302742, Final Batch Loss: 0.12237454205751419\n",
      "Epoch 452, Loss: 0.24303452670574188, Final Batch Loss: 0.1307893991470337\n",
      "Epoch 453, Loss: 0.2861228436231613, Final Batch Loss: 0.13065578043460846\n",
      "Epoch 454, Loss: 0.23144769668579102, Final Batch Loss: 0.10829044133424759\n",
      "Epoch 455, Loss: 0.23395689576864243, Final Batch Loss: 0.13064491748809814\n",
      "Epoch 456, Loss: 0.23512493073940277, Final Batch Loss: 0.12553344666957855\n",
      "Epoch 457, Loss: 0.22772853076457977, Final Batch Loss: 0.10194045305252075\n",
      "Epoch 458, Loss: 0.23668046295642853, Final Batch Loss: 0.11550220102071762\n",
      "Epoch 459, Loss: 0.22963783144950867, Final Batch Loss: 0.09407231211662292\n",
      "Epoch 460, Loss: 0.23391303420066833, Final Batch Loss: 0.09609425067901611\n",
      "Epoch 461, Loss: 0.2543456628918648, Final Batch Loss: 0.13165050745010376\n",
      "Epoch 462, Loss: 0.23954055458307266, Final Batch Loss: 0.1323351413011551\n",
      "Epoch 463, Loss: 0.23898782581090927, Final Batch Loss: 0.12272662669420242\n",
      "Epoch 464, Loss: 0.23173116892576218, Final Batch Loss: 0.10564406961202621\n",
      "Epoch 465, Loss: 0.23864319920539856, Final Batch Loss: 0.13176430761814117\n",
      "Epoch 466, Loss: 0.26043860614299774, Final Batch Loss: 0.13144950568675995\n",
      "Epoch 467, Loss: 0.2656438648700714, Final Batch Loss: 0.13546182215213776\n",
      "Epoch 468, Loss: 0.2814006805419922, Final Batch Loss: 0.1181928813457489\n",
      "Epoch 469, Loss: 0.22320681810379028, Final Batch Loss: 0.11995410174131393\n",
      "Epoch 470, Loss: 0.2718221992254257, Final Batch Loss: 0.14776921272277832\n",
      "Epoch 471, Loss: 0.25734640657901764, Final Batch Loss: 0.07275886833667755\n",
      "Epoch 472, Loss: 0.2579292505979538, Final Batch Loss: 0.0913936048746109\n",
      "Epoch 473, Loss: 0.23447027802467346, Final Batch Loss: 0.12119708210229874\n",
      "Epoch 474, Loss: 0.2879272997379303, Final Batch Loss: 0.16627737879753113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475, Loss: 0.24781100451946259, Final Batch Loss: 0.13838490843772888\n",
      "Epoch 476, Loss: 0.272852823138237, Final Batch Loss: 0.13911913335323334\n",
      "Epoch 477, Loss: 0.2967003136873245, Final Batch Loss: 0.17714045941829681\n",
      "Epoch 478, Loss: 0.2657996937632561, Final Batch Loss: 0.11030536144971848\n",
      "Epoch 479, Loss: 0.2630617618560791, Final Batch Loss: 0.1456230729818344\n",
      "Epoch 480, Loss: 0.23420243710279465, Final Batch Loss: 0.11744038015604019\n",
      "Epoch 481, Loss: 0.26312319934368134, Final Batch Loss: 0.11208924651145935\n",
      "Epoch 482, Loss: 0.24010176211595535, Final Batch Loss: 0.09309021383523941\n",
      "Epoch 483, Loss: 0.2215960994362831, Final Batch Loss: 0.10932207852602005\n",
      "Epoch 484, Loss: 0.23068936169147491, Final Batch Loss: 0.1074671521782875\n",
      "Epoch 485, Loss: 0.2366073876619339, Final Batch Loss: 0.14307639002799988\n",
      "Epoch 486, Loss: 0.29621851444244385, Final Batch Loss: 0.17394119501113892\n",
      "Epoch 487, Loss: 0.21244976669549942, Final Batch Loss: 0.088860422372818\n",
      "Epoch 488, Loss: 0.22278668731451035, Final Batch Loss: 0.11453823745250702\n",
      "Epoch 489, Loss: 0.2722495123744011, Final Batch Loss: 0.10836976021528244\n",
      "Epoch 490, Loss: 0.2829173728823662, Final Batch Loss: 0.11611085385084152\n",
      "Epoch 491, Loss: 0.22825059294700623, Final Batch Loss: 0.1307656466960907\n",
      "Epoch 492, Loss: 0.23775669187307358, Final Batch Loss: 0.1434130072593689\n",
      "Epoch 493, Loss: 0.22848621010780334, Final Batch Loss: 0.08743251860141754\n",
      "Epoch 494, Loss: 0.2594872862100601, Final Batch Loss: 0.1463504433631897\n",
      "Epoch 495, Loss: 0.2055894210934639, Final Batch Loss: 0.09984658658504486\n",
      "Epoch 496, Loss: 0.23721899092197418, Final Batch Loss: 0.11545833200216293\n",
      "Epoch 497, Loss: 0.24653355032205582, Final Batch Loss: 0.12273189425468445\n",
      "Epoch 498, Loss: 0.24287670850753784, Final Batch Loss: 0.13552308082580566\n",
      "Epoch 499, Loss: 0.2618507891893387, Final Batch Loss: 0.11125868558883667\n",
      "Epoch 500, Loss: 0.20438729226589203, Final Batch Loss: 0.10931679606437683\n",
      "Epoch 501, Loss: 0.22368835657835007, Final Batch Loss: 0.12642216682434082\n",
      "Epoch 502, Loss: 0.2474486157298088, Final Batch Loss: 0.13490623235702515\n",
      "Epoch 503, Loss: 0.2422303780913353, Final Batch Loss: 0.1218276247382164\n",
      "Epoch 504, Loss: 0.23130583763122559, Final Batch Loss: 0.13189861178398132\n",
      "Epoch 505, Loss: 0.26601144671440125, Final Batch Loss: 0.13685494661331177\n",
      "Epoch 506, Loss: 0.3011731877923012, Final Batch Loss: 0.20004308223724365\n",
      "Epoch 507, Loss: 0.2307463437318802, Final Batch Loss: 0.11216823756694794\n",
      "Epoch 508, Loss: 0.23376218229532242, Final Batch Loss: 0.1400647759437561\n",
      "Epoch 509, Loss: 0.24603436887264252, Final Batch Loss: 0.132199227809906\n",
      "Epoch 510, Loss: 0.2035958543419838, Final Batch Loss: 0.09145145118236542\n",
      "Epoch 511, Loss: 0.2097606062889099, Final Batch Loss: 0.09164681285619736\n",
      "Epoch 512, Loss: 0.2273610681295395, Final Batch Loss: 0.07319086790084839\n",
      "Epoch 513, Loss: 0.24647464603185654, Final Batch Loss: 0.10313520580530167\n",
      "Epoch 514, Loss: 0.22676756232976913, Final Batch Loss: 0.10640042275190353\n",
      "Epoch 515, Loss: 0.2350531369447708, Final Batch Loss: 0.11062384396791458\n",
      "Epoch 516, Loss: 0.20073029398918152, Final Batch Loss: 0.09646935760974884\n",
      "Epoch 517, Loss: 0.24360840022563934, Final Batch Loss: 0.09573802351951599\n",
      "Epoch 518, Loss: 0.26019173860549927, Final Batch Loss: 0.13040493428707123\n",
      "Epoch 519, Loss: 0.24513879418373108, Final Batch Loss: 0.13111010193824768\n",
      "Epoch 520, Loss: 0.26542481780052185, Final Batch Loss: 0.1487375795841217\n",
      "Epoch 521, Loss: 0.19760924577713013, Final Batch Loss: 0.08025030046701431\n",
      "Epoch 522, Loss: 0.20466239005327225, Final Batch Loss: 0.10837111622095108\n",
      "Epoch 523, Loss: 0.22670969367027283, Final Batch Loss: 0.12683892250061035\n",
      "Epoch 524, Loss: 0.19635803252458572, Final Batch Loss: 0.11099272966384888\n",
      "Epoch 525, Loss: 0.24192152172327042, Final Batch Loss: 0.13584426045417786\n",
      "Epoch 526, Loss: 0.1879231408238411, Final Batch Loss: 0.0700383260846138\n",
      "Epoch 527, Loss: 0.22651278972625732, Final Batch Loss: 0.11729638278484344\n",
      "Epoch 528, Loss: 0.20932704210281372, Final Batch Loss: 0.13000239431858063\n",
      "Epoch 529, Loss: 0.21653805673122406, Final Batch Loss: 0.06133356690406799\n",
      "Epoch 530, Loss: 0.17153240740299225, Final Batch Loss: 0.08246488124132156\n",
      "Epoch 531, Loss: 0.19801992177963257, Final Batch Loss: 0.1088024377822876\n",
      "Epoch 532, Loss: 0.18270181864500046, Final Batch Loss: 0.0739365890622139\n",
      "Epoch 533, Loss: 0.22709905356168747, Final Batch Loss: 0.09309778362512589\n",
      "Epoch 534, Loss: 0.20138341188430786, Final Batch Loss: 0.10913000255823135\n",
      "Epoch 535, Loss: 0.2264823541045189, Final Batch Loss: 0.12279465049505234\n",
      "Epoch 536, Loss: 0.23969688266515732, Final Batch Loss: 0.11399006098508835\n",
      "Epoch 537, Loss: 0.20266427099704742, Final Batch Loss: 0.06333769857883453\n",
      "Epoch 538, Loss: 0.18737613409757614, Final Batch Loss: 0.10306694358587265\n",
      "Epoch 539, Loss: 0.18225491046905518, Final Batch Loss: 0.05483722686767578\n",
      "Epoch 540, Loss: 0.23416489362716675, Final Batch Loss: 0.11868588626384735\n",
      "Epoch 541, Loss: 0.2741375118494034, Final Batch Loss: 0.14660929143428802\n",
      "Epoch 542, Loss: 0.25478678941726685, Final Batch Loss: 0.13592492043972015\n",
      "Epoch 543, Loss: 0.28552672266960144, Final Batch Loss: 0.14511042833328247\n",
      "Epoch 544, Loss: 0.278815358877182, Final Batch Loss: 0.11900828778743744\n",
      "Epoch 545, Loss: 0.2285991534590721, Final Batch Loss: 0.09612176567316055\n",
      "Epoch 546, Loss: 0.3267708197236061, Final Batch Loss: 0.21676160395145416\n",
      "Epoch 547, Loss: 0.24877911061048508, Final Batch Loss: 0.10721001774072647\n",
      "Epoch 548, Loss: 0.2420855090022087, Final Batch Loss: 0.10995178669691086\n",
      "Epoch 549, Loss: 0.25003430247306824, Final Batch Loss: 0.11326101422309875\n",
      "Epoch 550, Loss: 0.2141578570008278, Final Batch Loss: 0.12414448708295822\n",
      "Epoch 551, Loss: 0.22591190040111542, Final Batch Loss: 0.12051662057638168\n",
      "Epoch 552, Loss: 0.24689188599586487, Final Batch Loss: 0.13381539285182953\n",
      "Epoch 553, Loss: 0.2624398320913315, Final Batch Loss: 0.16917115449905396\n",
      "Epoch 554, Loss: 0.2163447141647339, Final Batch Loss: 0.09310667961835861\n",
      "Epoch 555, Loss: 0.2219255566596985, Final Batch Loss: 0.13507157564163208\n",
      "Epoch 556, Loss: 0.25188974291086197, Final Batch Loss: 0.10039427131414413\n",
      "Epoch 557, Loss: 0.20188328623771667, Final Batch Loss: 0.10891366750001907\n",
      "Epoch 558, Loss: 0.23704838007688522, Final Batch Loss: 0.10889223963022232\n",
      "Epoch 559, Loss: 0.22260941565036774, Final Batch Loss: 0.11081686615943909\n",
      "Epoch 560, Loss: 0.18510475009679794, Final Batch Loss: 0.06309828907251358\n",
      "Epoch 561, Loss: 0.24192454665899277, Final Batch Loss: 0.1407686024904251\n",
      "Epoch 562, Loss: 0.19256853312253952, Final Batch Loss: 0.10293442755937576\n",
      "Epoch 563, Loss: 0.20578940212726593, Final Batch Loss: 0.10132055729627609\n",
      "Epoch 564, Loss: 0.26309577375650406, Final Batch Loss: 0.08566547185182571\n",
      "Epoch 565, Loss: 0.26927921175956726, Final Batch Loss: 0.14212748408317566\n",
      "Epoch 566, Loss: 0.27346033602952957, Final Batch Loss: 0.15503720939159393\n",
      "Epoch 567, Loss: 0.2184838354587555, Final Batch Loss: 0.09511863440275192\n",
      "Epoch 568, Loss: 0.25910238176584244, Final Batch Loss: 0.15644590556621552\n",
      "Epoch 569, Loss: 0.2512640506029129, Final Batch Loss: 0.15040072798728943\n",
      "Epoch 570, Loss: 0.2807169705629349, Final Batch Loss: 0.17335844039916992\n",
      "Epoch 571, Loss: 0.22988299280405045, Final Batch Loss: 0.11535882949829102\n",
      "Epoch 572, Loss: 0.26755961030721664, Final Batch Loss: 0.1549895703792572\n",
      "Epoch 573, Loss: 0.1993398368358612, Final Batch Loss: 0.12189611047506332\n",
      "Epoch 574, Loss: 0.16369248181581497, Final Batch Loss: 0.07339614629745483\n",
      "Epoch 575, Loss: 0.2068886235356331, Final Batch Loss: 0.0835682675242424\n",
      "Epoch 576, Loss: 0.2084352895617485, Final Batch Loss: 0.08412644267082214\n",
      "Epoch 577, Loss: 0.2112754061818123, Final Batch Loss: 0.11912941187620163\n",
      "Epoch 578, Loss: 0.23171015828847885, Final Batch Loss: 0.14488288760185242\n",
      "Epoch 579, Loss: 0.20602040737867355, Final Batch Loss: 0.08752302080392838\n",
      "Epoch 580, Loss: 0.17843938618898392, Final Batch Loss: 0.09314501285552979\n",
      "Epoch 581, Loss: 0.20042734593153, Final Batch Loss: 0.12220755219459534\n",
      "Epoch 582, Loss: 0.20742610096931458, Final Batch Loss: 0.09510371088981628\n",
      "Epoch 583, Loss: 0.2153278961777687, Final Batch Loss: 0.07957731932401657\n",
      "Epoch 584, Loss: 0.20859511941671371, Final Batch Loss: 0.10608666390180588\n",
      "Epoch 585, Loss: 0.22699473053216934, Final Batch Loss: 0.0949297621846199\n",
      "Epoch 586, Loss: 0.2267235592007637, Final Batch Loss: 0.13239412009716034\n",
      "Epoch 587, Loss: 0.23474092781543732, Final Batch Loss: 0.13522733747959137\n",
      "Epoch 588, Loss: 0.25635263323783875, Final Batch Loss: 0.13962459564208984\n",
      "Epoch 589, Loss: 0.2226131409406662, Final Batch Loss: 0.08952414989471436\n",
      "Epoch 590, Loss: 0.19662808626890182, Final Batch Loss: 0.09757039695978165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591, Loss: 0.22898586839437485, Final Batch Loss: 0.09833545237779617\n",
      "Epoch 592, Loss: 0.2118704617023468, Final Batch Loss: 0.1163901537656784\n",
      "Epoch 593, Loss: 0.23287706077098846, Final Batch Loss: 0.1499568223953247\n",
      "Epoch 594, Loss: 0.19043397903442383, Final Batch Loss: 0.07313258945941925\n",
      "Epoch 595, Loss: 0.27246254682540894, Final Batch Loss: 0.16000008583068848\n",
      "Epoch 596, Loss: 0.27078869193792343, Final Batch Loss: 0.10632941871881485\n",
      "Epoch 597, Loss: 0.2077374905347824, Final Batch Loss: 0.09535810351371765\n",
      "Epoch 598, Loss: 0.21908176690340042, Final Batch Loss: 0.11893121898174286\n",
      "Epoch 599, Loss: 0.19289518892765045, Final Batch Loss: 0.09231436252593994\n",
      "Epoch 600, Loss: 0.22501470893621445, Final Batch Loss: 0.1353813260793686\n",
      "Epoch 601, Loss: 0.17885036021471024, Final Batch Loss: 0.09837070852518082\n",
      "Epoch 602, Loss: 0.1866225153207779, Final Batch Loss: 0.10656601935625076\n",
      "Epoch 603, Loss: 0.19451475143432617, Final Batch Loss: 0.09599539637565613\n",
      "Epoch 604, Loss: 0.23549973219633102, Final Batch Loss: 0.14520137012004852\n",
      "Epoch 605, Loss: 0.17763090878725052, Final Batch Loss: 0.08478648215532303\n",
      "Epoch 606, Loss: 0.1970682516694069, Final Batch Loss: 0.08954282850027084\n",
      "Epoch 607, Loss: 0.19762645661830902, Final Batch Loss: 0.10652446001768112\n",
      "Epoch 608, Loss: 0.23227215558290482, Final Batch Loss: 0.113129161298275\n",
      "Epoch 609, Loss: 0.20843349397182465, Final Batch Loss: 0.08564545214176178\n",
      "Epoch 610, Loss: 0.24257871508598328, Final Batch Loss: 0.1521146148443222\n",
      "Epoch 611, Loss: 0.17762301862239838, Final Batch Loss: 0.06690307706594467\n",
      "Epoch 612, Loss: 0.2450491264462471, Final Batch Loss: 0.11156020313501358\n",
      "Epoch 613, Loss: 0.18398582935333252, Final Batch Loss: 0.09388471394777298\n",
      "Epoch 614, Loss: 0.20676317811012268, Final Batch Loss: 0.0930032804608345\n",
      "Epoch 615, Loss: 0.2238372266292572, Final Batch Loss: 0.1258183866739273\n",
      "Epoch 616, Loss: 0.20394552499055862, Final Batch Loss: 0.0973898395895958\n",
      "Epoch 617, Loss: 0.21582606434822083, Final Batch Loss: 0.10406098514795303\n",
      "Epoch 618, Loss: 0.21709129214286804, Final Batch Loss: 0.10288551449775696\n",
      "Epoch 619, Loss: 0.2018292471766472, Final Batch Loss: 0.10163673013448715\n",
      "Epoch 620, Loss: 0.24546775966882706, Final Batch Loss: 0.14606279134750366\n",
      "Epoch 621, Loss: 0.17903421819210052, Final Batch Loss: 0.06339141726493835\n",
      "Epoch 622, Loss: 0.18917864561080933, Final Batch Loss: 0.07680899649858475\n",
      "Epoch 623, Loss: 0.22277361154556274, Final Batch Loss: 0.13333769142627716\n",
      "Epoch 624, Loss: 0.21092437207698822, Final Batch Loss: 0.10144272446632385\n",
      "Epoch 625, Loss: 0.19047851860523224, Final Batch Loss: 0.10552778840065002\n",
      "Epoch 626, Loss: 0.2124980464577675, Final Batch Loss: 0.13566181063652039\n",
      "Epoch 627, Loss: 0.23084217309951782, Final Batch Loss: 0.127299964427948\n",
      "Epoch 628, Loss: 0.18712089210748672, Final Batch Loss: 0.07929246872663498\n",
      "Epoch 629, Loss: 0.2309739887714386, Final Batch Loss: 0.08936937153339386\n",
      "Epoch 630, Loss: 0.19335108250379562, Final Batch Loss: 0.07856324315071106\n",
      "Epoch 631, Loss: 0.23562517017126083, Final Batch Loss: 0.11413490772247314\n",
      "Epoch 632, Loss: 0.1517440676689148, Final Batch Loss: 0.08359712362289429\n",
      "Epoch 633, Loss: 0.21648387610912323, Final Batch Loss: 0.0909881442785263\n",
      "Epoch 634, Loss: 0.19377416372299194, Final Batch Loss: 0.08592444658279419\n",
      "Epoch 635, Loss: 0.18208632618188858, Final Batch Loss: 0.08939790725708008\n",
      "Epoch 636, Loss: 0.16693975776433945, Final Batch Loss: 0.09759680181741714\n",
      "Epoch 637, Loss: 0.17768502235412598, Final Batch Loss: 0.09795987606048584\n",
      "Epoch 638, Loss: 0.22116173058748245, Final Batch Loss: 0.1503259688615799\n",
      "Epoch 639, Loss: 0.24062073975801468, Final Batch Loss: 0.15188542008399963\n",
      "Epoch 640, Loss: 0.17510885745286942, Final Batch Loss: 0.08152127265930176\n",
      "Epoch 641, Loss: 0.1734544187784195, Final Batch Loss: 0.0747835636138916\n",
      "Epoch 642, Loss: 0.17777542769908905, Final Batch Loss: 0.09358077496290207\n",
      "Epoch 643, Loss: 0.15771973878145218, Final Batch Loss: 0.06260262429714203\n",
      "Epoch 644, Loss: 0.19514208287000656, Final Batch Loss: 0.0686006024479866\n",
      "Epoch 645, Loss: 0.2394888699054718, Final Batch Loss: 0.164601132273674\n",
      "Epoch 646, Loss: 0.17969849705696106, Final Batch Loss: 0.08202308416366577\n",
      "Epoch 647, Loss: 0.15532997995615005, Final Batch Loss: 0.09281005710363388\n",
      "Epoch 648, Loss: 0.19783058762550354, Final Batch Loss: 0.11673453450202942\n",
      "Epoch 649, Loss: 0.22167444229125977, Final Batch Loss: 0.1269150823354721\n",
      "Epoch 650, Loss: 0.1664694994688034, Final Batch Loss: 0.0889679342508316\n",
      "Epoch 651, Loss: 0.17248842865228653, Final Batch Loss: 0.10085005313158035\n",
      "Epoch 652, Loss: 0.1983591839671135, Final Batch Loss: 0.1113739013671875\n",
      "Epoch 653, Loss: 0.2919309139251709, Final Batch Loss: 0.1518208235502243\n",
      "Epoch 654, Loss: 0.18512580543756485, Final Batch Loss: 0.1018587127327919\n",
      "Epoch 655, Loss: 0.17171769589185715, Final Batch Loss: 0.10050621628761292\n",
      "Epoch 656, Loss: 0.18016677349805832, Final Batch Loss: 0.09028105437755585\n",
      "Epoch 657, Loss: 0.1812117025256157, Final Batch Loss: 0.05823875963687897\n",
      "Epoch 658, Loss: 0.18550977855920792, Final Batch Loss: 0.1238819807767868\n",
      "Epoch 659, Loss: 0.17989784851670265, Final Batch Loss: 0.06109822168946266\n",
      "Epoch 660, Loss: 0.20555467158555984, Final Batch Loss: 0.0891866534948349\n",
      "Epoch 661, Loss: 0.20853446424007416, Final Batch Loss: 0.1090979054570198\n",
      "Epoch 662, Loss: 0.16352181136608124, Final Batch Loss: 0.09930013865232468\n",
      "Epoch 663, Loss: 0.24045314639806747, Final Batch Loss: 0.09852080792188644\n",
      "Epoch 664, Loss: 0.22971289604902267, Final Batch Loss: 0.1190568134188652\n",
      "Epoch 665, Loss: 0.1868366003036499, Final Batch Loss: 0.09451336413621902\n",
      "Epoch 666, Loss: 0.2217574194073677, Final Batch Loss: 0.09082155674695969\n",
      "Epoch 667, Loss: 0.19949132949113846, Final Batch Loss: 0.0899132713675499\n",
      "Epoch 668, Loss: 0.23461409658193588, Final Batch Loss: 0.10433103889226913\n",
      "Epoch 669, Loss: 0.21262045204639435, Final Batch Loss: 0.08440712094306946\n",
      "Epoch 670, Loss: 0.15524505078792572, Final Batch Loss: 0.08394961804151535\n",
      "Epoch 671, Loss: 0.18417400121688843, Final Batch Loss: 0.08653752505779266\n",
      "Epoch 672, Loss: 0.19570611417293549, Final Batch Loss: 0.10735934227705002\n",
      "Epoch 673, Loss: 0.18324023485183716, Final Batch Loss: 0.06769956648349762\n",
      "Epoch 674, Loss: 0.1960320919752121, Final Batch Loss: 0.07656941562891006\n",
      "Epoch 675, Loss: 0.17885108664631844, Final Batch Loss: 0.05977579578757286\n",
      "Epoch 676, Loss: 0.1885089948773384, Final Batch Loss: 0.10855470597743988\n",
      "Epoch 677, Loss: 0.1842227503657341, Final Batch Loss: 0.07546591013669968\n",
      "Epoch 678, Loss: 0.17305628955364227, Final Batch Loss: 0.09487850964069366\n",
      "Epoch 679, Loss: 0.1627599373459816, Final Batch Loss: 0.07168007642030716\n",
      "Epoch 680, Loss: 0.21802517026662827, Final Batch Loss: 0.09806320071220398\n",
      "Epoch 681, Loss: 0.22788465023040771, Final Batch Loss: 0.1423288881778717\n",
      "Epoch 682, Loss: 0.2030494436621666, Final Batch Loss: 0.1173393651843071\n",
      "Epoch 683, Loss: 0.18086928129196167, Final Batch Loss: 0.10153849422931671\n",
      "Epoch 684, Loss: 0.18305373936891556, Final Batch Loss: 0.09796274453401566\n",
      "Epoch 685, Loss: 0.18004564195871353, Final Batch Loss: 0.08986502140760422\n",
      "Epoch 686, Loss: 0.20900383591651917, Final Batch Loss: 0.11353868991136551\n",
      "Epoch 687, Loss: 0.23765410482883453, Final Batch Loss: 0.12464296072721481\n",
      "Epoch 688, Loss: 0.24106958508491516, Final Batch Loss: 0.12171132117509842\n",
      "Epoch 689, Loss: 0.1767049953341484, Final Batch Loss: 0.06451614946126938\n",
      "Epoch 690, Loss: 0.18688692897558212, Final Batch Loss: 0.10343639552593231\n",
      "Epoch 691, Loss: 0.19371745735406876, Final Batch Loss: 0.06245843321084976\n",
      "Epoch 692, Loss: 0.19710926711559296, Final Batch Loss: 0.08606670051813126\n",
      "Epoch 693, Loss: 0.19139926880598068, Final Batch Loss: 0.11140762269496918\n",
      "Epoch 694, Loss: 0.16702532768249512, Final Batch Loss: 0.07943544536828995\n",
      "Epoch 695, Loss: 0.1590845137834549, Final Batch Loss: 0.07654739171266556\n",
      "Epoch 696, Loss: 0.2583267316222191, Final Batch Loss: 0.1138671264052391\n",
      "Epoch 697, Loss: 0.1655925065279007, Final Batch Loss: 0.06883979588747025\n",
      "Epoch 698, Loss: 0.1903388425707817, Final Batch Loss: 0.08211082220077515\n",
      "Epoch 699, Loss: 0.18585468083620071, Final Batch Loss: 0.09291484206914902\n",
      "Epoch 700, Loss: 0.1954176351428032, Final Batch Loss: 0.10290519893169403\n",
      "Epoch 701, Loss: 0.18805056810379028, Final Batch Loss: 0.10619118064641953\n",
      "Epoch 702, Loss: 0.17978481203317642, Final Batch Loss: 0.08397244662046432\n",
      "Epoch 703, Loss: 0.23039411008358002, Final Batch Loss: 0.133383646607399\n",
      "Epoch 704, Loss: 0.22910183668136597, Final Batch Loss: 0.14613521099090576\n",
      "Epoch 705, Loss: 0.21432450413703918, Final Batch Loss: 0.13629946112632751\n",
      "Epoch 706, Loss: 0.2077789306640625, Final Batch Loss: 0.12190312892198563\n",
      "Epoch 707, Loss: 0.14482978731393814, Final Batch Loss: 0.0820883959531784\n",
      "Epoch 708, Loss: 0.2185930535197258, Final Batch Loss: 0.12464381009340286\n",
      "Epoch 709, Loss: 0.1902216151356697, Final Batch Loss: 0.09102877974510193\n",
      "Epoch 710, Loss: 0.2183949127793312, Final Batch Loss: 0.08186463266611099\n",
      "Epoch 711, Loss: 0.18063001334667206, Final Batch Loss: 0.10002176463603973\n",
      "Epoch 712, Loss: 0.21169207990169525, Final Batch Loss: 0.11244027316570282\n",
      "Epoch 713, Loss: 0.22640349715948105, Final Batch Loss: 0.1475922018289566\n",
      "Epoch 714, Loss: 0.19283241778612137, Final Batch Loss: 0.07609568536281586\n",
      "Epoch 715, Loss: 0.21551376581192017, Final Batch Loss: 0.10620664805173874\n",
      "Epoch 716, Loss: 0.199265718460083, Final Batch Loss: 0.09650366008281708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 717, Loss: 0.1730889528989792, Final Batch Loss: 0.0908680185675621\n",
      "Epoch 718, Loss: 0.17179662734270096, Final Batch Loss: 0.06700265407562256\n",
      "Epoch 719, Loss: 0.16585494577884674, Final Batch Loss: 0.07799149304628372\n",
      "Epoch 720, Loss: 0.14388705790042877, Final Batch Loss: 0.07482484728097916\n",
      "Epoch 721, Loss: 0.15009882301092148, Final Batch Loss: 0.051254093647003174\n",
      "Epoch 722, Loss: 0.26866573095321655, Final Batch Loss: 0.1594972163438797\n",
      "Epoch 723, Loss: 0.15988053381443024, Final Batch Loss: 0.06936369836330414\n",
      "Epoch 724, Loss: 0.17425328493118286, Final Batch Loss: 0.08435685187578201\n",
      "Epoch 725, Loss: 0.16164756938815117, Final Batch Loss: 0.0619138665497303\n",
      "Epoch 726, Loss: 0.20070472359657288, Final Batch Loss: 0.13279736042022705\n",
      "Epoch 727, Loss: 0.19210808724164963, Final Batch Loss: 0.1060243770480156\n",
      "Epoch 728, Loss: 0.1798592135310173, Final Batch Loss: 0.0806160569190979\n",
      "Epoch 729, Loss: 0.18632956594228745, Final Batch Loss: 0.09483247250318527\n",
      "Epoch 730, Loss: 0.16213083267211914, Final Batch Loss: 0.09687329083681107\n",
      "Epoch 731, Loss: 0.19750795513391495, Final Batch Loss: 0.08723712712526321\n",
      "Epoch 732, Loss: 0.20807212591171265, Final Batch Loss: 0.09935599565505981\n",
      "Epoch 733, Loss: 0.15218722075223923, Final Batch Loss: 0.06928949803113937\n",
      "Epoch 734, Loss: 0.1813790202140808, Final Batch Loss: 0.10092417895793915\n",
      "Epoch 735, Loss: 0.21091726422309875, Final Batch Loss: 0.07481969892978668\n",
      "Epoch 736, Loss: 0.22052475064992905, Final Batch Loss: 0.07457289844751358\n",
      "Epoch 737, Loss: 0.23287151753902435, Final Batch Loss: 0.10257014632225037\n",
      "Epoch 738, Loss: 0.18022947758436203, Final Batch Loss: 0.09042907506227493\n",
      "Epoch 739, Loss: 0.20459553599357605, Final Batch Loss: 0.11322527378797531\n",
      "Epoch 740, Loss: 0.17190412431955338, Final Batch Loss: 0.10374829918146133\n",
      "Epoch 741, Loss: 0.22182970494031906, Final Batch Loss: 0.12376277148723602\n",
      "Epoch 742, Loss: 0.1654934138059616, Final Batch Loss: 0.09571872651576996\n",
      "Epoch 743, Loss: 0.17079389840364456, Final Batch Loss: 0.0730704739689827\n",
      "Epoch 744, Loss: 0.14056488126516342, Final Batch Loss: 0.0679197832942009\n",
      "Epoch 745, Loss: 0.17386319488286972, Final Batch Loss: 0.0880693569779396\n",
      "Epoch 746, Loss: 0.18235617876052856, Final Batch Loss: 0.11303281038999557\n",
      "Epoch 747, Loss: 0.1584557220339775, Final Batch Loss: 0.08015651255846024\n",
      "Epoch 748, Loss: 0.22961940243840218, Final Batch Loss: 0.05895233526825905\n",
      "Epoch 749, Loss: 0.21179655194282532, Final Batch Loss: 0.12619705498218536\n",
      "Epoch 750, Loss: 0.2279331535100937, Final Batch Loss: 0.1168610230088234\n",
      "Epoch 751, Loss: 0.16731836646795273, Final Batch Loss: 0.09985699504613876\n",
      "Epoch 752, Loss: 0.17132146656513214, Final Batch Loss: 0.08209390938282013\n",
      "Epoch 753, Loss: 0.16049006581306458, Final Batch Loss: 0.06575217843055725\n",
      "Epoch 754, Loss: 0.23052959144115448, Final Batch Loss: 0.10500746965408325\n",
      "Epoch 755, Loss: 0.16383527591824532, Final Batch Loss: 0.10761980712413788\n",
      "Epoch 756, Loss: 0.16292207315564156, Final Batch Loss: 0.10187137871980667\n",
      "Epoch 757, Loss: 0.1800927296280861, Final Batch Loss: 0.11015067994594574\n",
      "Epoch 758, Loss: 0.1797911301255226, Final Batch Loss: 0.06373541057109833\n",
      "Epoch 759, Loss: 0.14259213209152222, Final Batch Loss: 0.07338187098503113\n",
      "Epoch 760, Loss: 0.182639017701149, Final Batch Loss: 0.1046377569437027\n",
      "Epoch 761, Loss: 0.1908348947763443, Final Batch Loss: 0.10623575001955032\n",
      "Epoch 762, Loss: 0.23907041549682617, Final Batch Loss: 0.17702212929725647\n",
      "Epoch 763, Loss: 0.17812951654195786, Final Batch Loss: 0.06813085079193115\n",
      "Epoch 764, Loss: 0.16227081418037415, Final Batch Loss: 0.0769295021891594\n",
      "Epoch 765, Loss: 0.18941932171583176, Final Batch Loss: 0.09724492579698563\n",
      "Epoch 766, Loss: 0.19504888355731964, Final Batch Loss: 0.0771290585398674\n",
      "Epoch 767, Loss: 0.19950339198112488, Final Batch Loss: 0.08229750394821167\n",
      "Epoch 768, Loss: 0.20301537960767746, Final Batch Loss: 0.09879998117685318\n",
      "Epoch 769, Loss: 0.22896668314933777, Final Batch Loss: 0.16216236352920532\n",
      "Epoch 770, Loss: 0.17015375941991806, Final Batch Loss: 0.13098448514938354\n",
      "Epoch 771, Loss: 0.17889301478862762, Final Batch Loss: 0.08264616131782532\n",
      "Epoch 772, Loss: 0.19325008988380432, Final Batch Loss: 0.07357563823461533\n",
      "Epoch 773, Loss: 0.18734902143478394, Final Batch Loss: 0.114569753408432\n",
      "Epoch 774, Loss: 0.18646547198295593, Final Batch Loss: 0.07827086746692657\n",
      "Epoch 775, Loss: 0.23081225901842117, Final Batch Loss: 0.10009325295686722\n",
      "Epoch 776, Loss: 0.16982989758253098, Final Batch Loss: 0.09476833790540695\n",
      "Epoch 777, Loss: 0.1610102578997612, Final Batch Loss: 0.0663449615240097\n",
      "Epoch 778, Loss: 0.19528505206108093, Final Batch Loss: 0.10738293081521988\n",
      "Epoch 779, Loss: 0.17953011393547058, Final Batch Loss: 0.11702816188335419\n",
      "Epoch 780, Loss: 0.166324220597744, Final Batch Loss: 0.081706702709198\n",
      "Epoch 781, Loss: 0.19146403670310974, Final Batch Loss: 0.08639673888683319\n",
      "Epoch 782, Loss: 0.16392605006694794, Final Batch Loss: 0.08588621765375137\n",
      "Epoch 783, Loss: 0.23923788964748383, Final Batch Loss: 0.1259995996952057\n",
      "Epoch 784, Loss: 0.1951553113758564, Final Batch Loss: 0.13522307574748993\n",
      "Epoch 785, Loss: 0.17187311872839928, Final Batch Loss: 0.05676402524113655\n",
      "Epoch 786, Loss: 0.16635998338460922, Final Batch Loss: 0.07152090221643448\n",
      "Epoch 787, Loss: 0.15037091821432114, Final Batch Loss: 0.07220877707004547\n",
      "Epoch 788, Loss: 0.18622234463691711, Final Batch Loss: 0.09077335149049759\n",
      "Epoch 789, Loss: 0.19170979410409927, Final Batch Loss: 0.08464647084474564\n",
      "Epoch 790, Loss: 0.16206422448158264, Final Batch Loss: 0.0797172263264656\n",
      "Epoch 791, Loss: 0.17722784727811813, Final Batch Loss: 0.09105890244245529\n",
      "Epoch 792, Loss: 0.1411535106599331, Final Batch Loss: 0.06076599285006523\n",
      "Epoch 793, Loss: 0.1934274435043335, Final Batch Loss: 0.10130580514669418\n",
      "Epoch 794, Loss: 0.1535668596625328, Final Batch Loss: 0.06577989459037781\n",
      "Epoch 795, Loss: 0.14408249408006668, Final Batch Loss: 0.0681714117527008\n",
      "Epoch 796, Loss: 0.20480957627296448, Final Batch Loss: 0.11819696426391602\n",
      "Epoch 797, Loss: 0.18485940247774124, Final Batch Loss: 0.09506729245185852\n",
      "Epoch 798, Loss: 0.20078739523887634, Final Batch Loss: 0.1385844498872757\n",
      "Epoch 799, Loss: 0.14149919152259827, Final Batch Loss: 0.08137660473585129\n",
      "Epoch 800, Loss: 0.17441880702972412, Final Batch Loss: 0.07844138890504837\n",
      "Epoch 801, Loss: 0.1631341278553009, Final Batch Loss: 0.07808443903923035\n",
      "Epoch 802, Loss: 0.13821884989738464, Final Batch Loss: 0.06320540606975555\n",
      "Epoch 803, Loss: 0.15471167862415314, Final Batch Loss: 0.07148297131061554\n",
      "Epoch 804, Loss: 0.1817413903772831, Final Batch Loss: 0.11928880959749222\n",
      "Epoch 805, Loss: 0.11718248948454857, Final Batch Loss: 0.0637633353471756\n",
      "Epoch 806, Loss: 0.1646851971745491, Final Batch Loss: 0.0675160139799118\n",
      "Epoch 807, Loss: 0.2037399560213089, Final Batch Loss: 0.10522089153528214\n",
      "Epoch 808, Loss: 0.14747855067253113, Final Batch Loss: 0.060071103274822235\n",
      "Epoch 809, Loss: 0.19136172533035278, Final Batch Loss: 0.05518345534801483\n",
      "Epoch 810, Loss: 0.21614566445350647, Final Batch Loss: 0.08436921238899231\n",
      "Epoch 811, Loss: 0.19118131697177887, Final Batch Loss: 0.11837411671876907\n",
      "Epoch 812, Loss: 0.18120933324098587, Final Batch Loss: 0.11870865523815155\n",
      "Epoch 813, Loss: 0.20372260361909866, Final Batch Loss: 0.10297522693872452\n",
      "Epoch 814, Loss: 0.19839409738779068, Final Batch Loss: 0.12173623591661453\n",
      "Epoch 815, Loss: 0.1542450115084648, Final Batch Loss: 0.08079545199871063\n",
      "Epoch 816, Loss: 0.14547568187117577, Final Batch Loss: 0.043520066887140274\n",
      "Epoch 817, Loss: 0.17317581176757812, Final Batch Loss: 0.10100580751895905\n",
      "Epoch 818, Loss: 0.1710786148905754, Final Batch Loss: 0.07087983936071396\n",
      "Epoch 819, Loss: 0.1588016375899315, Final Batch Loss: 0.09564769268035889\n",
      "Epoch 820, Loss: 0.17488765716552734, Final Batch Loss: 0.08270246535539627\n",
      "Epoch 821, Loss: 0.13775009661912918, Final Batch Loss: 0.07912587374448776\n",
      "Epoch 822, Loss: 0.2015259936451912, Final Batch Loss: 0.11415994167327881\n",
      "Epoch 823, Loss: 0.18044224381446838, Final Batch Loss: 0.10244093090295792\n",
      "Epoch 824, Loss: 0.17599473148584366, Final Batch Loss: 0.10110758990049362\n",
      "Epoch 825, Loss: 0.17198416590690613, Final Batch Loss: 0.10282959043979645\n",
      "Epoch 826, Loss: 0.15480640530586243, Final Batch Loss: 0.06622575223445892\n",
      "Epoch 827, Loss: 0.16672546416521072, Final Batch Loss: 0.08231066167354584\n",
      "Epoch 828, Loss: 0.12730630487203598, Final Batch Loss: 0.05363740026950836\n",
      "Epoch 829, Loss: 0.17921072617173195, Final Batch Loss: 0.04331933334469795\n",
      "Epoch 830, Loss: 0.21763993054628372, Final Batch Loss: 0.1445198953151703\n",
      "Epoch 831, Loss: 0.15987687557935715, Final Batch Loss: 0.08289525657892227\n",
      "Epoch 832, Loss: 0.20888327807188034, Final Batch Loss: 0.11530718952417374\n",
      "Epoch 833, Loss: 0.13490887731313705, Final Batch Loss: 0.07016871124505997\n",
      "Epoch 834, Loss: 0.1729220226407051, Final Batch Loss: 0.11646527051925659\n",
      "Epoch 835, Loss: 0.13723436370491982, Final Batch Loss: 0.06096779927611351\n",
      "Epoch 836, Loss: 0.17933835089206696, Final Batch Loss: 0.09131043404340744\n",
      "Epoch 837, Loss: 0.1744445413351059, Final Batch Loss: 0.09714746475219727\n",
      "Epoch 838, Loss: 0.13912487775087357, Final Batch Loss: 0.07387127727270126\n",
      "Epoch 839, Loss: 0.20149589329957962, Final Batch Loss: 0.07655593752861023\n",
      "Epoch 840, Loss: 0.168050866574049, Final Batch Loss: 0.11528241634368896\n",
      "Epoch 841, Loss: 0.1676998734474182, Final Batch Loss: 0.08329068124294281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 842, Loss: 0.18118177354335785, Final Batch Loss: 0.09624052047729492\n",
      "Epoch 843, Loss: 0.17836880683898926, Final Batch Loss: 0.08447693288326263\n",
      "Epoch 844, Loss: 0.15733253583312035, Final Batch Loss: 0.049213435500860214\n",
      "Epoch 845, Loss: 0.1255107820034027, Final Batch Loss: 0.07172783464193344\n",
      "Epoch 846, Loss: 0.185890793800354, Final Batch Loss: 0.09305740892887115\n",
      "Epoch 847, Loss: 0.15411686152219772, Final Batch Loss: 0.08945592492818832\n",
      "Epoch 848, Loss: 0.20455637574195862, Final Batch Loss: 0.10982048511505127\n",
      "Epoch 849, Loss: 0.15074191242456436, Final Batch Loss: 0.07626647502183914\n",
      "Epoch 850, Loss: 0.15191151946783066, Final Batch Loss: 0.057010240852832794\n",
      "Epoch 851, Loss: 0.19664904475212097, Final Batch Loss: 0.06283992528915405\n",
      "Epoch 852, Loss: 0.257224440574646, Final Batch Loss: 0.1346398890018463\n",
      "Epoch 853, Loss: 0.1579015851020813, Final Batch Loss: 0.09142401069402695\n",
      "Epoch 854, Loss: 0.15388192981481552, Final Batch Loss: 0.09106750041246414\n",
      "Epoch 855, Loss: 0.1695280373096466, Final Batch Loss: 0.07933294773101807\n",
      "Epoch 856, Loss: 0.17706558108329773, Final Batch Loss: 0.10168266296386719\n",
      "Epoch 857, Loss: 0.1812710165977478, Final Batch Loss: 0.10820028185844421\n",
      "Epoch 858, Loss: 0.1483301967382431, Final Batch Loss: 0.05617102235555649\n",
      "Epoch 859, Loss: 0.15118848532438278, Final Batch Loss: 0.06686027348041534\n",
      "Epoch 860, Loss: 0.1948559656739235, Final Batch Loss: 0.1262243241071701\n",
      "Epoch 861, Loss: 0.13662558421492577, Final Batch Loss: 0.05715126916766167\n",
      "Epoch 862, Loss: 0.15120379626750946, Final Batch Loss: 0.08588241785764694\n",
      "Epoch 863, Loss: 0.19725661724805832, Final Batch Loss: 0.11599214375019073\n",
      "Epoch 864, Loss: 0.17581114172935486, Final Batch Loss: 0.07998041063547134\n",
      "Epoch 865, Loss: 0.20426520705223083, Final Batch Loss: 0.12521152198314667\n",
      "Epoch 866, Loss: 0.17975735664367676, Final Batch Loss: 0.10337112098932266\n",
      "Epoch 867, Loss: 0.15056317299604416, Final Batch Loss: 0.07195024192333221\n",
      "Epoch 868, Loss: 0.1335846297442913, Final Batch Loss: 0.07219690829515457\n",
      "Epoch 869, Loss: 0.13572805002331734, Final Batch Loss: 0.07982329279184341\n",
      "Epoch 870, Loss: 0.11591263860464096, Final Batch Loss: 0.06167012080550194\n",
      "Epoch 871, Loss: 0.159993976354599, Final Batch Loss: 0.08086814731359482\n",
      "Epoch 872, Loss: 0.12514175474643707, Final Batch Loss: 0.06220880150794983\n",
      "Epoch 873, Loss: 0.12765974551439285, Final Batch Loss: 0.06318148225545883\n",
      "Epoch 874, Loss: 0.1948254406452179, Final Batch Loss: 0.1244298666715622\n",
      "Epoch 875, Loss: 0.15148432925343513, Final Batch Loss: 0.05899038538336754\n",
      "Epoch 876, Loss: 0.14520518481731415, Final Batch Loss: 0.0703488290309906\n",
      "Epoch 877, Loss: 0.1254103034734726, Final Batch Loss: 0.060993194580078125\n",
      "Epoch 878, Loss: 0.12368691340088844, Final Batch Loss: 0.03702756389975548\n",
      "Epoch 879, Loss: 0.15301628783345222, Final Batch Loss: 0.04278973862528801\n",
      "Epoch 880, Loss: 0.15992888063192368, Final Batch Loss: 0.08733119815587997\n",
      "Epoch 881, Loss: 0.1890915483236313, Final Batch Loss: 0.08622777462005615\n",
      "Epoch 882, Loss: 0.13803688436746597, Final Batch Loss: 0.06757524609565735\n",
      "Epoch 883, Loss: 0.11215358227491379, Final Batch Loss: 0.03564515709877014\n",
      "Epoch 884, Loss: 0.18008800595998764, Final Batch Loss: 0.10345861315727234\n",
      "Epoch 885, Loss: 0.14737744629383087, Final Batch Loss: 0.04239016771316528\n",
      "Epoch 886, Loss: 0.18143942207098007, Final Batch Loss: 0.1197805106639862\n",
      "Epoch 887, Loss: 0.13279075920581818, Final Batch Loss: 0.06970081478357315\n",
      "Epoch 888, Loss: 0.1710275337100029, Final Batch Loss: 0.11295847594738007\n",
      "Epoch 889, Loss: 0.2056252434849739, Final Batch Loss: 0.13617467880249023\n",
      "Epoch 890, Loss: 0.14018284156918526, Final Batch Loss: 0.045887332409620285\n",
      "Epoch 891, Loss: 0.17654003202915192, Final Batch Loss: 0.09966539591550827\n",
      "Epoch 892, Loss: 0.1309039369225502, Final Batch Loss: 0.06631958484649658\n",
      "Epoch 893, Loss: 0.16638164967298508, Final Batch Loss: 0.0790204182267189\n",
      "Epoch 894, Loss: 0.14242272078990936, Final Batch Loss: 0.05475887656211853\n",
      "Epoch 895, Loss: 0.216996431350708, Final Batch Loss: 0.1035301610827446\n",
      "Epoch 896, Loss: 0.18325795233249664, Final Batch Loss: 0.10971909016370773\n",
      "Epoch 897, Loss: 0.14019513875246048, Final Batch Loss: 0.06286730617284775\n",
      "Epoch 898, Loss: 0.18153265118598938, Final Batch Loss: 0.12277260422706604\n",
      "Epoch 899, Loss: 0.21215899288654327, Final Batch Loss: 0.06949925422668457\n",
      "Epoch 900, Loss: 0.13770461827516556, Final Batch Loss: 0.0845981240272522\n",
      "Epoch 901, Loss: 0.14214129745960236, Final Batch Loss: 0.08339577168226242\n",
      "Epoch 902, Loss: 0.12472142279148102, Final Batch Loss: 0.06893015652894974\n",
      "Epoch 903, Loss: 0.17632976919412613, Final Batch Loss: 0.09622183442115784\n",
      "Epoch 904, Loss: 0.15716364979743958, Final Batch Loss: 0.06659495830535889\n",
      "Epoch 905, Loss: 0.1381065845489502, Final Batch Loss: 0.06764757633209229\n",
      "Epoch 906, Loss: 0.17426273971796036, Final Batch Loss: 0.10142989456653595\n",
      "Epoch 907, Loss: 0.17369318008422852, Final Batch Loss: 0.08106544613838196\n",
      "Epoch 908, Loss: 0.18483176827430725, Final Batch Loss: 0.09227010607719421\n",
      "Epoch 909, Loss: 0.16850879043340683, Final Batch Loss: 0.09622187912464142\n",
      "Epoch 910, Loss: 0.15273936092853546, Final Batch Loss: 0.07104405760765076\n",
      "Epoch 911, Loss: 0.15181934088468552, Final Batch Loss: 0.07249560207128525\n",
      "Epoch 912, Loss: 0.1753915697336197, Final Batch Loss: 0.10729201138019562\n",
      "Epoch 913, Loss: 0.14414579421281815, Final Batch Loss: 0.0769307017326355\n",
      "Epoch 914, Loss: 0.14620064198970795, Final Batch Loss: 0.05780903249979019\n",
      "Epoch 915, Loss: 0.1350303515791893, Final Batch Loss: 0.06542892754077911\n",
      "Epoch 916, Loss: 0.09533030167222023, Final Batch Loss: 0.039755839854478836\n",
      "Epoch 917, Loss: 0.19149012863636017, Final Batch Loss: 0.11333002895116806\n",
      "Epoch 918, Loss: 0.17569798976182938, Final Batch Loss: 0.11050642281770706\n",
      "Epoch 919, Loss: 0.18010395020246506, Final Batch Loss: 0.08224359154701233\n",
      "Epoch 920, Loss: 0.11260528489947319, Final Batch Loss: 0.05777113884687424\n",
      "Epoch 921, Loss: 0.15437056869268417, Final Batch Loss: 0.08151070773601532\n",
      "Epoch 922, Loss: 0.13857464492321014, Final Batch Loss: 0.07063846290111542\n",
      "Epoch 923, Loss: 0.16811082512140274, Final Batch Loss: 0.06689941883087158\n",
      "Epoch 924, Loss: 0.1541505679488182, Final Batch Loss: 0.10908233374357224\n",
      "Epoch 925, Loss: 0.16662514954805374, Final Batch Loss: 0.08480055630207062\n",
      "Epoch 926, Loss: 0.1528211459517479, Final Batch Loss: 0.07813479006290436\n",
      "Epoch 927, Loss: 0.11140338703989983, Final Batch Loss: 0.04851129278540611\n",
      "Epoch 928, Loss: 0.14902474731206894, Final Batch Loss: 0.06816262751817703\n",
      "Epoch 929, Loss: 0.14549162238836288, Final Batch Loss: 0.0629853904247284\n",
      "Epoch 930, Loss: 0.15536793693900108, Final Batch Loss: 0.049128104001283646\n",
      "Epoch 931, Loss: 0.14412222430109978, Final Batch Loss: 0.10582290589809418\n",
      "Epoch 932, Loss: 0.10013992711901665, Final Batch Loss: 0.05687780678272247\n",
      "Epoch 933, Loss: 0.1293010711669922, Final Batch Loss: 0.06898897886276245\n",
      "Epoch 934, Loss: 0.16353564709424973, Final Batch Loss: 0.06907903403043747\n",
      "Epoch 935, Loss: 0.16246923804283142, Final Batch Loss: 0.1012190654873848\n",
      "Epoch 936, Loss: 0.1936178058385849, Final Batch Loss: 0.08350951224565506\n",
      "Epoch 937, Loss: 0.1281171590089798, Final Batch Loss: 0.07835421711206436\n",
      "Epoch 938, Loss: 0.152483731508255, Final Batch Loss: 0.08848243206739426\n",
      "Epoch 939, Loss: 0.17746864259243011, Final Batch Loss: 0.12871570885181427\n",
      "Epoch 940, Loss: 0.1654873490333557, Final Batch Loss: 0.0972297266125679\n",
      "Epoch 941, Loss: 0.13752983883023262, Final Batch Loss: 0.0858829990029335\n",
      "Epoch 942, Loss: 0.16544366627931595, Final Batch Loss: 0.07187020778656006\n",
      "Epoch 943, Loss: 0.11187824234366417, Final Batch Loss: 0.05207238718867302\n",
      "Epoch 944, Loss: 0.19277628511190414, Final Batch Loss: 0.11754278838634491\n",
      "Epoch 945, Loss: 0.1510329730808735, Final Batch Loss: 0.09645815938711166\n",
      "Epoch 946, Loss: 0.16629279404878616, Final Batch Loss: 0.10077822208404541\n",
      "Epoch 947, Loss: 0.13837480545043945, Final Batch Loss: 0.08574450016021729\n",
      "Epoch 948, Loss: 0.15832612663507462, Final Batch Loss: 0.06478980928659439\n",
      "Epoch 949, Loss: 0.1390039548277855, Final Batch Loss: 0.04922968149185181\n",
      "Epoch 950, Loss: 0.2148011401295662, Final Batch Loss: 0.11580056697130203\n",
      "Epoch 951, Loss: 0.14152543246746063, Final Batch Loss: 0.06377287954092026\n",
      "Epoch 952, Loss: 0.13218766450881958, Final Batch Loss: 0.04539977014064789\n",
      "Epoch 953, Loss: 0.13928061351180077, Final Batch Loss: 0.05973498150706291\n",
      "Epoch 954, Loss: 0.1350513994693756, Final Batch Loss: 0.07117006182670593\n",
      "Epoch 955, Loss: 0.15461189672350883, Final Batch Loss: 0.05782225355505943\n",
      "Epoch 956, Loss: 0.1506200172007084, Final Batch Loss: 0.05754384770989418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 957, Loss: 0.1356610469520092, Final Batch Loss: 0.053854282945394516\n",
      "Epoch 958, Loss: 0.17502155527472496, Final Batch Loss: 0.11979034543037415\n",
      "Epoch 959, Loss: 0.15380095690488815, Final Batch Loss: 0.07604342699050903\n",
      "Epoch 960, Loss: 0.14685948193073273, Final Batch Loss: 0.05533209443092346\n",
      "Epoch 961, Loss: 0.1621386595070362, Final Batch Loss: 0.10626702755689621\n",
      "Epoch 962, Loss: 0.18495047837495804, Final Batch Loss: 0.10702541470527649\n",
      "Epoch 963, Loss: 0.13566658645868301, Final Batch Loss: 0.040606558322906494\n",
      "Epoch 964, Loss: 0.13908828422427177, Final Batch Loss: 0.049127247184515\n",
      "Epoch 965, Loss: 0.12454251572489738, Final Batch Loss: 0.06772105395793915\n",
      "Epoch 966, Loss: 0.15317267552018166, Final Batch Loss: 0.04724200442433357\n",
      "Epoch 967, Loss: 0.173354834318161, Final Batch Loss: 0.09263044595718384\n",
      "Epoch 968, Loss: 0.17949657887220383, Final Batch Loss: 0.11431880295276642\n",
      "Epoch 969, Loss: 0.13184478878974915, Final Batch Loss: 0.0630345419049263\n",
      "Epoch 970, Loss: 0.13212349638342857, Final Batch Loss: 0.05861201509833336\n",
      "Epoch 971, Loss: 0.1370900757610798, Final Batch Loss: 0.054616156965494156\n",
      "Epoch 972, Loss: 0.11610116809606552, Final Batch Loss: 0.05797388777136803\n",
      "Epoch 973, Loss: 0.11804676428437233, Final Batch Loss: 0.050319623202085495\n",
      "Epoch 974, Loss: 0.17143963277339935, Final Batch Loss: 0.12194672226905823\n",
      "Epoch 975, Loss: 0.1757839173078537, Final Batch Loss: 0.10480774194002151\n",
      "Epoch 976, Loss: 0.11352416127920151, Final Batch Loss: 0.0555487684905529\n",
      "Epoch 977, Loss: 0.18751265853643417, Final Batch Loss: 0.07651989907026291\n",
      "Epoch 978, Loss: 0.1548236906528473, Final Batch Loss: 0.05554548650979996\n",
      "Epoch 979, Loss: 0.11540883406996727, Final Batch Loss: 0.05853842571377754\n",
      "Epoch 980, Loss: 0.1438991315662861, Final Batch Loss: 0.05286731943488121\n",
      "Epoch 981, Loss: 0.1590505801141262, Final Batch Loss: 0.09733327478170395\n",
      "Epoch 982, Loss: 0.16137157380580902, Final Batch Loss: 0.09381251037120819\n",
      "Epoch 983, Loss: 0.17854105681180954, Final Batch Loss: 0.12555886805057526\n",
      "Epoch 984, Loss: 0.16816475987434387, Final Batch Loss: 0.09969548135995865\n",
      "Epoch 985, Loss: 0.12350182235240936, Final Batch Loss: 0.06489279866218567\n",
      "Epoch 986, Loss: 0.15932416543364525, Final Batch Loss: 0.05913257971405983\n",
      "Epoch 987, Loss: 0.14451995491981506, Final Batch Loss: 0.06968767940998077\n",
      "Epoch 988, Loss: 0.2054930031299591, Final Batch Loss: 0.13400496542453766\n",
      "Epoch 989, Loss: 0.1616377830505371, Final Batch Loss: 0.06622640043497086\n",
      "Epoch 990, Loss: 0.14599645882844925, Final Batch Loss: 0.09056224673986435\n",
      "Epoch 991, Loss: 0.18668369203805923, Final Batch Loss: 0.09577587246894836\n",
      "Epoch 992, Loss: 0.2164115458726883, Final Batch Loss: 0.12690868973731995\n",
      "Epoch 993, Loss: 0.17207343131303787, Final Batch Loss: 0.08789581805467606\n",
      "Epoch 994, Loss: 0.16853127256035805, Final Batch Loss: 0.11430235952138901\n",
      "Epoch 995, Loss: 0.16371244937181473, Final Batch Loss: 0.10537846386432648\n",
      "Epoch 996, Loss: 0.16282784193754196, Final Batch Loss: 0.08627689629793167\n",
      "Epoch 997, Loss: 0.12953809276223183, Final Batch Loss: 0.07241301983594894\n",
      "Epoch 998, Loss: 0.16361503303050995, Final Batch Loss: 0.09449745714664459\n",
      "Epoch 999, Loss: 0.17623721063137054, Final Batch Loss: 0.07487170398235321\n",
      "Epoch 1000, Loss: 0.13936715200543404, Final Batch Loss: 0.04258861020207405\n",
      "Epoch 1001, Loss: 0.13480207696557045, Final Batch Loss: 0.07405409961938858\n",
      "Epoch 1002, Loss: 0.19291707128286362, Final Batch Loss: 0.09765361249446869\n",
      "Epoch 1003, Loss: 0.15167170763015747, Final Batch Loss: 0.09231597930192947\n",
      "Epoch 1004, Loss: 0.18961402028799057, Final Batch Loss: 0.08000253885984421\n",
      "Epoch 1005, Loss: 0.10656865686178207, Final Batch Loss: 0.04080983251333237\n",
      "Epoch 1006, Loss: 0.1154407225549221, Final Batch Loss: 0.05500618368387222\n",
      "Epoch 1007, Loss: 0.12711472436785698, Final Batch Loss: 0.0916215181350708\n",
      "Epoch 1008, Loss: 0.13228779658675194, Final Batch Loss: 0.04762854799628258\n",
      "Epoch 1009, Loss: 0.14614993333816528, Final Batch Loss: 0.07620279490947723\n",
      "Epoch 1010, Loss: 0.15249542891979218, Final Batch Loss: 0.07992352545261383\n",
      "Epoch 1011, Loss: 0.13541438430547714, Final Batch Loss: 0.06839228421449661\n",
      "Epoch 1012, Loss: 0.11620595678687096, Final Batch Loss: 0.05629332736134529\n",
      "Epoch 1013, Loss: 0.14185559004545212, Final Batch Loss: 0.06720328330993652\n",
      "Epoch 1014, Loss: 0.11922343820333481, Final Batch Loss: 0.0782432109117508\n",
      "Epoch 1015, Loss: 0.14079929143190384, Final Batch Loss: 0.05904741585254669\n",
      "Epoch 1016, Loss: 0.1409575343132019, Final Batch Loss: 0.06556880474090576\n",
      "Epoch 1017, Loss: 0.10245171189308167, Final Batch Loss: 0.06061545014381409\n",
      "Epoch 1018, Loss: 0.14297295361757278, Final Batch Loss: 0.07380487024784088\n",
      "Epoch 1019, Loss: 0.13048551976680756, Final Batch Loss: 0.06450073421001434\n",
      "Epoch 1020, Loss: 0.11415622010827065, Final Batch Loss: 0.05242788419127464\n",
      "Epoch 1021, Loss: 0.11432844400405884, Final Batch Loss: 0.05732392892241478\n",
      "Epoch 1022, Loss: 0.20203795284032822, Final Batch Loss: 0.10970313847064972\n",
      "Epoch 1023, Loss: 0.190659299492836, Final Batch Loss: 0.10062751173973083\n",
      "Epoch 1024, Loss: 0.209166057407856, Final Batch Loss: 0.12922391295433044\n",
      "Epoch 1025, Loss: 0.14340104907751083, Final Batch Loss: 0.0825694128870964\n",
      "Epoch 1026, Loss: 0.1298179216682911, Final Batch Loss: 0.053747985512018204\n",
      "Epoch 1027, Loss: 0.14407171308994293, Final Batch Loss: 0.05954243987798691\n",
      "Epoch 1028, Loss: 0.12932390347123146, Final Batch Loss: 0.07026991248130798\n",
      "Epoch 1029, Loss: 0.13406547158956528, Final Batch Loss: 0.03702753782272339\n",
      "Epoch 1030, Loss: 0.11511165648698807, Final Batch Loss: 0.07011806964874268\n",
      "Epoch 1031, Loss: 0.1076071560382843, Final Batch Loss: 0.04580492898821831\n",
      "Epoch 1032, Loss: 0.13521819189190865, Final Batch Loss: 0.0563875250518322\n",
      "Epoch 1033, Loss: 0.16222494095563889, Final Batch Loss: 0.05465122312307358\n",
      "Epoch 1034, Loss: 0.1595878154039383, Final Batch Loss: 0.07649608701467514\n",
      "Epoch 1035, Loss: 0.12561539560556412, Final Batch Loss: 0.04744558036327362\n",
      "Epoch 1036, Loss: 0.14280712604522705, Final Batch Loss: 0.07498682290315628\n",
      "Epoch 1037, Loss: 0.15003377944231033, Final Batch Loss: 0.052327051758766174\n",
      "Epoch 1038, Loss: 0.12960022687911987, Final Batch Loss: 0.041064754128456116\n",
      "Epoch 1039, Loss: 0.19517459720373154, Final Batch Loss: 0.10934191197156906\n",
      "Epoch 1040, Loss: 0.1846635490655899, Final Batch Loss: 0.12030787020921707\n",
      "Epoch 1041, Loss: 0.1502595692873001, Final Batch Loss: 0.08421220630407333\n",
      "Epoch 1042, Loss: 0.12164502218365669, Final Batch Loss: 0.0473334826529026\n",
      "Epoch 1043, Loss: 0.17066912725567818, Final Batch Loss: 0.06092378869652748\n",
      "Epoch 1044, Loss: 0.113119687885046, Final Batch Loss: 0.06000012159347534\n",
      "Epoch 1045, Loss: 0.1220908910036087, Final Batch Loss: 0.06538186222314835\n",
      "Epoch 1046, Loss: 0.17477045580744743, Final Batch Loss: 0.05406202748417854\n",
      "Epoch 1047, Loss: 0.15752212703227997, Final Batch Loss: 0.11132682859897614\n",
      "Epoch 1048, Loss: 0.12156040593981743, Final Batch Loss: 0.04165074601769447\n",
      "Epoch 1049, Loss: 0.14253665506839752, Final Batch Loss: 0.07230754941701889\n",
      "Epoch 1050, Loss: 0.10838012024760246, Final Batch Loss: 0.04835677519440651\n",
      "Epoch 1051, Loss: 0.12513678148388863, Final Batch Loss: 0.08228114992380142\n",
      "Epoch 1052, Loss: 0.10233445093035698, Final Batch Loss: 0.06295892596244812\n",
      "Epoch 1053, Loss: 0.15011704713106155, Final Batch Loss: 0.08705323189496994\n",
      "Epoch 1054, Loss: 0.09902895241975784, Final Batch Loss: 0.042779698967933655\n",
      "Epoch 1055, Loss: 0.11806235462427139, Final Batch Loss: 0.0821516364812851\n",
      "Epoch 1056, Loss: 0.11739888414740562, Final Batch Loss: 0.05891375243663788\n",
      "Epoch 1057, Loss: 0.11609100922942162, Final Batch Loss: 0.0638221725821495\n",
      "Epoch 1058, Loss: 0.1880221962928772, Final Batch Loss: 0.1190437376499176\n",
      "Epoch 1059, Loss: 0.18234483152627945, Final Batch Loss: 0.10570576041936874\n",
      "Epoch 1060, Loss: 0.1494450680911541, Final Batch Loss: 0.08824419975280762\n",
      "Epoch 1061, Loss: 0.14045000448822975, Final Batch Loss: 0.07876414060592651\n",
      "Epoch 1062, Loss: 0.11715772747993469, Final Batch Loss: 0.050309956073760986\n",
      "Epoch 1063, Loss: 0.14842677488923073, Final Batch Loss: 0.08940095454454422\n",
      "Epoch 1064, Loss: 0.12684116885066032, Final Batch Loss: 0.04167421534657478\n",
      "Epoch 1065, Loss: 0.11966178193688393, Final Batch Loss: 0.059923287481069565\n",
      "Epoch 1066, Loss: 0.10656264424324036, Final Batch Loss: 0.05069759860634804\n",
      "Epoch 1067, Loss: 0.13253305107355118, Final Batch Loss: 0.07563477009534836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1068, Loss: 0.17493196949362755, Final Batch Loss: 0.12273325771093369\n",
      "Epoch 1069, Loss: 0.1500585712492466, Final Batch Loss: 0.09132997691631317\n",
      "Epoch 1070, Loss: 0.1258544735610485, Final Batch Loss: 0.05667292699217796\n",
      "Epoch 1071, Loss: 0.06949659064412117, Final Batch Loss: 0.022184845060110092\n",
      "Epoch 1072, Loss: 0.11986460536718369, Final Batch Loss: 0.07268614321947098\n",
      "Epoch 1073, Loss: 0.15497085824608803, Final Batch Loss: 0.10305964946746826\n",
      "Epoch 1074, Loss: 0.1262887865304947, Final Batch Loss: 0.041639916598796844\n",
      "Epoch 1075, Loss: 0.12044161930680275, Final Batch Loss: 0.06925947964191437\n",
      "Epoch 1076, Loss: 0.10304045677185059, Final Batch Loss: 0.04863496869802475\n",
      "Epoch 1077, Loss: 0.12639669328927994, Final Batch Loss: 0.06999699771404266\n",
      "Epoch 1078, Loss: 0.11676008626818657, Final Batch Loss: 0.043731313198804855\n",
      "Epoch 1079, Loss: 0.12523134425282478, Final Batch Loss: 0.0428280346095562\n",
      "Epoch 1080, Loss: 0.08731842041015625, Final Batch Loss: 0.05300041288137436\n",
      "Epoch 1081, Loss: 0.13447833433747292, Final Batch Loss: 0.08401753753423691\n",
      "Epoch 1082, Loss: 0.16536828130483627, Final Batch Loss: 0.06725659221410751\n",
      "Epoch 1083, Loss: 0.13363705202937126, Final Batch Loss: 0.05290905758738518\n",
      "Epoch 1084, Loss: 0.14118634909391403, Final Batch Loss: 0.06352373957633972\n",
      "Epoch 1085, Loss: 0.12817858532071114, Final Batch Loss: 0.06801720708608627\n",
      "Epoch 1086, Loss: 0.09807856194674969, Final Batch Loss: 0.025915151461958885\n",
      "Epoch 1087, Loss: 0.12052388861775398, Final Batch Loss: 0.05111122503876686\n",
      "Epoch 1088, Loss: 0.11465143784880638, Final Batch Loss: 0.04436621442437172\n",
      "Epoch 1089, Loss: 0.10976415500044823, Final Batch Loss: 0.041177015751600266\n",
      "Epoch 1090, Loss: 0.1545059010386467, Final Batch Loss: 0.07139372080564499\n",
      "Epoch 1091, Loss: 0.09524450078606606, Final Batch Loss: 0.048754654824733734\n",
      "Epoch 1092, Loss: 0.14366205036640167, Final Batch Loss: 0.07304727286100388\n",
      "Epoch 1093, Loss: 0.10889387130737305, Final Batch Loss: 0.04525125026702881\n",
      "Epoch 1094, Loss: 0.1199641264975071, Final Batch Loss: 0.06578069180250168\n",
      "Epoch 1095, Loss: 0.15525364875793457, Final Batch Loss: 0.08106260746717453\n",
      "Epoch 1096, Loss: 0.12563280388712883, Final Batch Loss: 0.08077195286750793\n",
      "Epoch 1097, Loss: 0.12331239134073257, Final Batch Loss: 0.08294486254453659\n",
      "Epoch 1098, Loss: 0.13368181325495243, Final Batch Loss: 0.02537759579718113\n",
      "Epoch 1099, Loss: 0.12677478790283203, Final Batch Loss: 0.06327857077121735\n",
      "Epoch 1100, Loss: 0.09842076897621155, Final Batch Loss: 0.04137817397713661\n",
      "Epoch 1101, Loss: 0.12703870981931686, Final Batch Loss: 0.07124043256044388\n",
      "Epoch 1102, Loss: 0.13695292547345161, Final Batch Loss: 0.027495231479406357\n",
      "Epoch 1103, Loss: 0.15209084376692772, Final Batch Loss: 0.09559567272663116\n",
      "Epoch 1104, Loss: 0.11118131503462791, Final Batch Loss: 0.04418620094656944\n",
      "Epoch 1105, Loss: 0.13169483840465546, Final Batch Loss: 0.059775225818157196\n",
      "Epoch 1106, Loss: 0.14466359838843346, Final Batch Loss: 0.08423060178756714\n",
      "Epoch 1107, Loss: 0.1334151327610016, Final Batch Loss: 0.07032332569360733\n",
      "Epoch 1108, Loss: 0.1814170815050602, Final Batch Loss: 0.13362328708171844\n",
      "Epoch 1109, Loss: 0.15326053276658058, Final Batch Loss: 0.06186893209815025\n",
      "Epoch 1110, Loss: 0.1752195507287979, Final Batch Loss: 0.07790534198284149\n",
      "Epoch 1111, Loss: 0.23963186144828796, Final Batch Loss: 0.13695068657398224\n",
      "Epoch 1112, Loss: 0.14855726063251495, Final Batch Loss: 0.08096639811992645\n",
      "Epoch 1113, Loss: 0.18251453340053558, Final Batch Loss: 0.07114028930664062\n",
      "Epoch 1114, Loss: 0.2089214101433754, Final Batch Loss: 0.09475968778133392\n",
      "Epoch 1115, Loss: 0.1680933404713869, Final Batch Loss: 0.022315414622426033\n",
      "Epoch 1116, Loss: 0.17233403772115707, Final Batch Loss: 0.08446411043405533\n",
      "Epoch 1117, Loss: 0.13863983377814293, Final Batch Loss: 0.06112932786345482\n",
      "Epoch 1118, Loss: 0.1821579784154892, Final Batch Loss: 0.11484895646572113\n",
      "Epoch 1119, Loss: 0.10814378783106804, Final Batch Loss: 0.060835011303424835\n",
      "Epoch 1120, Loss: 0.1423856019973755, Final Batch Loss: 0.08541790395975113\n",
      "Epoch 1121, Loss: 0.2302406206727028, Final Batch Loss: 0.11057313531637192\n",
      "Epoch 1122, Loss: 0.11095306277275085, Final Batch Loss: 0.03489947319030762\n",
      "Epoch 1123, Loss: 0.10443373396992683, Final Batch Loss: 0.05363810434937477\n",
      "Epoch 1124, Loss: 0.14415061473846436, Final Batch Loss: 0.09881684184074402\n",
      "Epoch 1125, Loss: 0.12590373307466507, Final Batch Loss: 0.07803966104984283\n",
      "Epoch 1126, Loss: 0.13648947328329086, Final Batch Loss: 0.06368570029735565\n",
      "Epoch 1127, Loss: 0.17065644264221191, Final Batch Loss: 0.07104146480560303\n",
      "Epoch 1128, Loss: 0.19804701581597328, Final Batch Loss: 0.1356731653213501\n",
      "Epoch 1129, Loss: 0.1089681014418602, Final Batch Loss: 0.03602224588394165\n",
      "Epoch 1130, Loss: 0.16974632069468498, Final Batch Loss: 0.05474850907921791\n",
      "Epoch 1131, Loss: 0.11504335701465607, Final Batch Loss: 0.07097333669662476\n",
      "Epoch 1132, Loss: 0.10997037962079048, Final Batch Loss: 0.05208607017993927\n",
      "Epoch 1133, Loss: 0.08929477259516716, Final Batch Loss: 0.038383327424526215\n",
      "Epoch 1134, Loss: 0.16104447841644287, Final Batch Loss: 0.08438780158758163\n",
      "Epoch 1135, Loss: 0.08453856408596039, Final Batch Loss: 0.04079384356737137\n",
      "Epoch 1136, Loss: 0.1345311924815178, Final Batch Loss: 0.08473285287618637\n",
      "Epoch 1137, Loss: 0.1388542503118515, Final Batch Loss: 0.08375336229801178\n",
      "Epoch 1138, Loss: 0.188576802611351, Final Batch Loss: 0.09848165512084961\n",
      "Epoch 1139, Loss: 0.13137656822800636, Final Batch Loss: 0.047281403094530106\n",
      "Epoch 1140, Loss: 0.159279502928257, Final Batch Loss: 0.0882742628455162\n",
      "Epoch 1141, Loss: 0.16163519769906998, Final Batch Loss: 0.05747891962528229\n",
      "Epoch 1142, Loss: 0.12869522720575333, Final Batch Loss: 0.0905698761343956\n",
      "Epoch 1143, Loss: 0.14408080279827118, Final Batch Loss: 0.06759803742170334\n",
      "Epoch 1144, Loss: 0.15969306603074074, Final Batch Loss: 0.061416979879140854\n",
      "Epoch 1145, Loss: 0.10884622111916542, Final Batch Loss: 0.06632478535175323\n",
      "Epoch 1146, Loss: 0.11397005617618561, Final Batch Loss: 0.05133403092622757\n",
      "Epoch 1147, Loss: 0.09258697554469109, Final Batch Loss: 0.05831370875239372\n",
      "Epoch 1148, Loss: 0.10107376798987389, Final Batch Loss: 0.05173809453845024\n",
      "Epoch 1149, Loss: 0.14804424345493317, Final Batch Loss: 0.08525446802377701\n",
      "Epoch 1150, Loss: 0.18077082186937332, Final Batch Loss: 0.12628203630447388\n",
      "Epoch 1151, Loss: 0.10491914674639702, Final Batch Loss: 0.037503983825445175\n",
      "Epoch 1152, Loss: 0.14519904553890228, Final Batch Loss: 0.07182403653860092\n",
      "Epoch 1153, Loss: 0.13145561516284943, Final Batch Loss: 0.04015778750181198\n",
      "Epoch 1154, Loss: 0.11614682897925377, Final Batch Loss: 0.049546148627996445\n",
      "Epoch 1155, Loss: 0.10829990357160568, Final Batch Loss: 0.0432216078042984\n",
      "Epoch 1156, Loss: 0.10413047298789024, Final Batch Loss: 0.03239068016409874\n",
      "Epoch 1157, Loss: 0.09762409329414368, Final Batch Loss: 0.046894773840904236\n",
      "Epoch 1158, Loss: 0.1204378679394722, Final Batch Loss: 0.042583614587783813\n",
      "Epoch 1159, Loss: 0.11995643377304077, Final Batch Loss: 0.0485219806432724\n",
      "Epoch 1160, Loss: 0.09818325936794281, Final Batch Loss: 0.03659161180257797\n",
      "Epoch 1161, Loss: 0.0974784754216671, Final Batch Loss: 0.06562653183937073\n",
      "Epoch 1162, Loss: 0.10214987397193909, Final Batch Loss: 0.05224943533539772\n",
      "Epoch 1163, Loss: 0.1367507427930832, Final Batch Loss: 0.06395037472248077\n",
      "Epoch 1164, Loss: 0.10749630630016327, Final Batch Loss: 0.05093032121658325\n",
      "Epoch 1165, Loss: 0.10058718919754028, Final Batch Loss: 0.05104393884539604\n",
      "Epoch 1166, Loss: 0.15225659683346748, Final Batch Loss: 0.05088471248745918\n",
      "Epoch 1167, Loss: 0.09683674946427345, Final Batch Loss: 0.054718878120183945\n",
      "Epoch 1168, Loss: 0.16807199269533157, Final Batch Loss: 0.1217942163348198\n",
      "Epoch 1169, Loss: 0.18719207495450974, Final Batch Loss: 0.09614822268486023\n",
      "Epoch 1170, Loss: 0.2003919780254364, Final Batch Loss: 0.1183706521987915\n",
      "Epoch 1171, Loss: 0.11980438604950905, Final Batch Loss: 0.04297652468085289\n",
      "Epoch 1172, Loss: 0.12017878144979477, Final Batch Loss: 0.054809533059597015\n",
      "Epoch 1173, Loss: 0.16371428966522217, Final Batch Loss: 0.08909179270267487\n",
      "Epoch 1174, Loss: 0.15462793037295341, Final Batch Loss: 0.10150031000375748\n",
      "Epoch 1175, Loss: 0.13368875533342361, Final Batch Loss: 0.07427488267421722\n",
      "Epoch 1176, Loss: 0.10718938708305359, Final Batch Loss: 0.048967596143484116\n",
      "Epoch 1177, Loss: 0.13750452175736427, Final Batch Loss: 0.0569089911878109\n",
      "Epoch 1178, Loss: 0.16849471628665924, Final Batch Loss: 0.07762927561998367\n",
      "Epoch 1179, Loss: 0.14256899803876877, Final Batch Loss: 0.0948137566447258\n",
      "Epoch 1180, Loss: 0.14282400533556938, Final Batch Loss: 0.0813363790512085\n",
      "Epoch 1181, Loss: 0.12712304666638374, Final Batch Loss: 0.05944499745965004\n",
      "Epoch 1182, Loss: 0.14274243265390396, Final Batch Loss: 0.037462830543518066\n",
      "Epoch 1183, Loss: 0.14613474160432816, Final Batch Loss: 0.07729104906320572\n",
      "Epoch 1184, Loss: 0.12651852890849113, Final Batch Loss: 0.06193951144814491\n",
      "Epoch 1185, Loss: 0.12836074829101562, Final Batch Loss: 0.08199581503868103\n",
      "Epoch 1186, Loss: 0.0943729504942894, Final Batch Loss: 0.03607786074280739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1187, Loss: 0.13315149024128914, Final Batch Loss: 0.05558285489678383\n",
      "Epoch 1188, Loss: 0.11372755467891693, Final Batch Loss: 0.06383650749921799\n",
      "Epoch 1189, Loss: 0.08838718011975288, Final Batch Loss: 0.03575380519032478\n",
      "Epoch 1190, Loss: 0.17502613365650177, Final Batch Loss: 0.07199561595916748\n",
      "Epoch 1191, Loss: 0.15753434225916862, Final Batch Loss: 0.047417763620615005\n",
      "Epoch 1192, Loss: 0.13458794355392456, Final Batch Loss: 0.06937382370233536\n",
      "Epoch 1193, Loss: 0.08602381683886051, Final Batch Loss: 0.062222808599472046\n",
      "Epoch 1194, Loss: 0.12313662096858025, Final Batch Loss: 0.08500653505325317\n",
      "Epoch 1195, Loss: 0.1454676240682602, Final Batch Loss: 0.05296334624290466\n",
      "Epoch 1196, Loss: 0.17654138430953026, Final Batch Loss: 0.11466705054044724\n",
      "Epoch 1197, Loss: 0.11464748531579971, Final Batch Loss: 0.054755810648202896\n",
      "Epoch 1198, Loss: 0.13893061503767967, Final Batch Loss: 0.09999367594718933\n",
      "Epoch 1199, Loss: 0.1441865637898445, Final Batch Loss: 0.07707469165325165\n",
      "Epoch 1200, Loss: 0.1718774139881134, Final Batch Loss: 0.07649221271276474\n",
      "Epoch 1201, Loss: 0.11883635818958282, Final Batch Loss: 0.06918440759181976\n",
      "Epoch 1202, Loss: 0.09718681126832962, Final Batch Loss: 0.051435552537441254\n",
      "Epoch 1203, Loss: 0.1261187307536602, Final Batch Loss: 0.08749744296073914\n",
      "Epoch 1204, Loss: 0.11529107764363289, Final Batch Loss: 0.027198653668165207\n",
      "Epoch 1205, Loss: 0.12710420042276382, Final Batch Loss: 0.03631959855556488\n",
      "Epoch 1206, Loss: 0.1353767141699791, Final Batch Loss: 0.10402079671621323\n",
      "Epoch 1207, Loss: 0.11088090389966965, Final Batch Loss: 0.06961912661790848\n",
      "Epoch 1208, Loss: 0.11318078264594078, Final Batch Loss: 0.06656400859355927\n",
      "Epoch 1209, Loss: 0.1360486075282097, Final Batch Loss: 0.06303046643733978\n",
      "Epoch 1210, Loss: 0.1399223953485489, Final Batch Loss: 0.07194449007511139\n",
      "Epoch 1211, Loss: 0.13535229861736298, Final Batch Loss: 0.05598713457584381\n",
      "Epoch 1212, Loss: 0.1103675551712513, Final Batch Loss: 0.061933521181344986\n",
      "Epoch 1213, Loss: 0.13180873543024063, Final Batch Loss: 0.07273092120885849\n",
      "Epoch 1214, Loss: 0.12689848989248276, Final Batch Loss: 0.0395786315202713\n",
      "Epoch 1215, Loss: 0.11721444129943848, Final Batch Loss: 0.07320542633533478\n",
      "Epoch 1216, Loss: 0.1116953082382679, Final Batch Loss: 0.05497382953763008\n",
      "Epoch 1217, Loss: 0.1580435447394848, Final Batch Loss: 0.05577526614069939\n",
      "Epoch 1218, Loss: 0.1736610308289528, Final Batch Loss: 0.11036748439073563\n",
      "Epoch 1219, Loss: 0.12010441720485687, Final Batch Loss: 0.0807623565196991\n",
      "Epoch 1220, Loss: 0.11550421267747879, Final Batch Loss: 0.05446843057870865\n",
      "Epoch 1221, Loss: 0.0878232829272747, Final Batch Loss: 0.04986729845404625\n",
      "Epoch 1222, Loss: 0.13638105615973473, Final Batch Loss: 0.08306590467691422\n",
      "Epoch 1223, Loss: 0.166997991502285, Final Batch Loss: 0.09474463760852814\n",
      "Epoch 1224, Loss: 0.16057860106229782, Final Batch Loss: 0.08464476466178894\n",
      "Epoch 1225, Loss: 0.11520533263683319, Final Batch Loss: 0.06265374273061752\n",
      "Epoch 1226, Loss: 0.12559818103909492, Final Batch Loss: 0.057547520846128464\n",
      "Epoch 1227, Loss: 0.10982019826769829, Final Batch Loss: 0.032542917877435684\n",
      "Epoch 1228, Loss: 0.1513608992099762, Final Batch Loss: 0.07030048221349716\n",
      "Epoch 1229, Loss: 0.11089441180229187, Final Batch Loss: 0.06364648789167404\n",
      "Epoch 1230, Loss: 0.1661698892712593, Final Batch Loss: 0.1114901602268219\n",
      "Epoch 1231, Loss: 0.10073705390095711, Final Batch Loss: 0.04318787530064583\n",
      "Epoch 1232, Loss: 0.12311225011944771, Final Batch Loss: 0.08131695538759232\n",
      "Epoch 1233, Loss: 0.08960042893886566, Final Batch Loss: 0.03845279663801193\n",
      "Epoch 1234, Loss: 0.12181692570447922, Final Batch Loss: 0.07696372270584106\n",
      "Epoch 1235, Loss: 0.10627557337284088, Final Batch Loss: 0.04585213586688042\n",
      "Epoch 1236, Loss: 0.19906213507056236, Final Batch Loss: 0.05552045628428459\n",
      "Epoch 1237, Loss: 0.11604267358779907, Final Batch Loss: 0.06682801991701126\n",
      "Epoch 1238, Loss: 0.07174347713589668, Final Batch Loss: 0.0346776582300663\n",
      "Epoch 1239, Loss: 0.1353880576789379, Final Batch Loss: 0.07742460072040558\n",
      "Epoch 1240, Loss: 0.08616136759519577, Final Batch Loss: 0.039504315704107285\n",
      "Epoch 1241, Loss: 0.0960468277335167, Final Batch Loss: 0.0386635847389698\n",
      "Epoch 1242, Loss: 0.12811099365353584, Final Batch Loss: 0.0732879713177681\n",
      "Epoch 1243, Loss: 0.1822425052523613, Final Batch Loss: 0.10977949947118759\n",
      "Epoch 1244, Loss: 0.1648865081369877, Final Batch Loss: 0.11724904179573059\n",
      "Epoch 1245, Loss: 0.11439718306064606, Final Batch Loss: 0.08166401088237762\n",
      "Epoch 1246, Loss: 0.11577359586954117, Final Batch Loss: 0.05753627046942711\n",
      "Epoch 1247, Loss: 0.1829073391854763, Final Batch Loss: 0.13119058310985565\n",
      "Epoch 1248, Loss: 0.12789582088589668, Final Batch Loss: 0.05120855197310448\n",
      "Epoch 1249, Loss: 0.12457773461937904, Final Batch Loss: 0.047195468097925186\n",
      "Epoch 1250, Loss: 0.11168313026428223, Final Batch Loss: 0.030644752085208893\n",
      "Epoch 1251, Loss: 0.15370914340019226, Final Batch Loss: 0.09419973194599152\n",
      "Epoch 1252, Loss: 0.11484116315841675, Final Batch Loss: 0.05118045210838318\n",
      "Epoch 1253, Loss: 0.11332955583930016, Final Batch Loss: 0.06540432572364807\n",
      "Epoch 1254, Loss: 0.18223073333501816, Final Batch Loss: 0.12113212794065475\n",
      "Epoch 1255, Loss: 0.12216011434793472, Final Batch Loss: 0.03528348356485367\n",
      "Epoch 1256, Loss: 0.10719109512865543, Final Batch Loss: 0.08961807936429977\n",
      "Epoch 1257, Loss: 0.08955574780702591, Final Batch Loss: 0.04199426248669624\n",
      "Epoch 1258, Loss: 0.13585948944091797, Final Batch Loss: 0.06916682422161102\n",
      "Epoch 1259, Loss: 0.1297605037689209, Final Batch Loss: 0.06733379513025284\n",
      "Epoch 1260, Loss: 0.12500983104109764, Final Batch Loss: 0.057221170514822006\n",
      "Epoch 1261, Loss: 0.1419418305158615, Final Batch Loss: 0.07687582075595856\n",
      "Epoch 1262, Loss: 0.1335713155567646, Final Batch Loss: 0.07547928392887115\n",
      "Epoch 1263, Loss: 0.1194864846765995, Final Batch Loss: 0.042914409190416336\n",
      "Epoch 1264, Loss: 0.14837238937616348, Final Batch Loss: 0.06833449751138687\n",
      "Epoch 1265, Loss: 0.08383457735180855, Final Batch Loss: 0.03243574872612953\n",
      "Epoch 1266, Loss: 0.1109139546751976, Final Batch Loss: 0.05880945175886154\n",
      "Epoch 1267, Loss: 0.09779636561870575, Final Batch Loss: 0.040638893842697144\n",
      "Epoch 1268, Loss: 0.11380638182163239, Final Batch Loss: 0.05400771275162697\n",
      "Epoch 1269, Loss: 0.12106414139270782, Final Batch Loss: 0.05816008150577545\n",
      "Epoch 1270, Loss: 0.11304266005754471, Final Batch Loss: 0.07198774814605713\n",
      "Epoch 1271, Loss: 0.11419803276658058, Final Batch Loss: 0.058210160583257675\n",
      "Epoch 1272, Loss: 0.11549631506204605, Final Batch Loss: 0.057003460824489594\n",
      "Epoch 1273, Loss: 0.11183305457234383, Final Batch Loss: 0.043082233518362045\n",
      "Epoch 1274, Loss: 0.09607182070612907, Final Batch Loss: 0.03396839275956154\n",
      "Epoch 1275, Loss: 0.10771295055747032, Final Batch Loss: 0.0671805664896965\n",
      "Epoch 1276, Loss: 0.11702723056077957, Final Batch Loss: 0.05946382135152817\n",
      "Epoch 1277, Loss: 0.10212942212820053, Final Batch Loss: 0.04012742266058922\n",
      "Epoch 1278, Loss: 0.07738855108618736, Final Batch Loss: 0.04052746668457985\n",
      "Epoch 1279, Loss: 0.16146327555179596, Final Batch Loss: 0.11053966730833054\n",
      "Epoch 1280, Loss: 0.1287137307226658, Final Batch Loss: 0.07839605957269669\n",
      "Epoch 1281, Loss: 0.12745202332735062, Final Batch Loss: 0.06289171427488327\n",
      "Epoch 1282, Loss: 0.1339414417743683, Final Batch Loss: 0.046863965690135956\n",
      "Epoch 1283, Loss: 0.13561447709798813, Final Batch Loss: 0.07619351893663406\n",
      "Epoch 1284, Loss: 0.14775512740015984, Final Batch Loss: 0.0867442861199379\n",
      "Epoch 1285, Loss: 0.09165476262569427, Final Batch Loss: 0.04693221673369408\n",
      "Epoch 1286, Loss: 0.1460728943347931, Final Batch Loss: 0.07603282481431961\n",
      "Epoch 1287, Loss: 0.11067226901650429, Final Batch Loss: 0.051824819296598434\n",
      "Epoch 1288, Loss: 0.1489780992269516, Final Batch Loss: 0.06745854020118713\n",
      "Epoch 1289, Loss: 0.14193418249487877, Final Batch Loss: 0.055719781666994095\n",
      "Epoch 1290, Loss: 0.13944551721215248, Final Batch Loss: 0.08157594501972198\n",
      "Epoch 1291, Loss: 0.16522172838449478, Final Batch Loss: 0.06949619948863983\n",
      "Epoch 1292, Loss: 0.11233548820018768, Final Batch Loss: 0.04300262778997421\n",
      "Epoch 1293, Loss: 0.1262674704194069, Final Batch Loss: 0.05954252928495407\n",
      "Epoch 1294, Loss: 0.11650120094418526, Final Batch Loss: 0.05667588859796524\n",
      "Epoch 1295, Loss: 0.08066016808152199, Final Batch Loss: 0.027631036937236786\n",
      "Epoch 1296, Loss: 0.09563960507512093, Final Batch Loss: 0.031508494168519974\n",
      "Epoch 1297, Loss: 0.11447036638855934, Final Batch Loss: 0.04811233654618263\n",
      "Epoch 1298, Loss: 0.15788177400827408, Final Batch Loss: 0.11406977474689484\n",
      "Epoch 1299, Loss: 0.12656927481293678, Final Batch Loss: 0.04879436269402504\n",
      "Epoch 1300, Loss: 0.1476818397641182, Final Batch Loss: 0.06437190622091293\n",
      "Epoch 1301, Loss: 0.09725569933652878, Final Batch Loss: 0.03474868834018707\n",
      "Epoch 1302, Loss: 0.11852485686540604, Final Batch Loss: 0.0597507618367672\n",
      "Epoch 1303, Loss: 0.1472368836402893, Final Batch Loss: 0.07674068957567215\n",
      "Epoch 1304, Loss: 0.15043817460536957, Final Batch Loss: 0.1048363670706749\n",
      "Epoch 1305, Loss: 0.17655962705612183, Final Batch Loss: 0.10818445682525635\n",
      "Epoch 1306, Loss: 0.17670093476772308, Final Batch Loss: 0.08683745563030243\n",
      "Epoch 1307, Loss: 0.16703927516937256, Final Batch Loss: 0.09933555871248245\n",
      "Epoch 1308, Loss: 0.1461258977651596, Final Batch Loss: 0.0676410123705864\n",
      "Epoch 1309, Loss: 0.11938747391104698, Final Batch Loss: 0.055291030555963516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1310, Loss: 0.14795339107513428, Final Batch Loss: 0.06882462650537491\n",
      "Epoch 1311, Loss: 0.12698087468743324, Final Batch Loss: 0.05124856159090996\n",
      "Epoch 1312, Loss: 0.06373587436974049, Final Batch Loss: 0.017437780275940895\n",
      "Epoch 1313, Loss: 0.11914778128266335, Final Batch Loss: 0.057086192071437836\n",
      "Epoch 1314, Loss: 0.1167190931737423, Final Batch Loss: 0.06557031720876694\n",
      "Epoch 1315, Loss: 0.11576542258262634, Final Batch Loss: 0.07119600474834442\n",
      "Epoch 1316, Loss: 0.09713781997561455, Final Batch Loss: 0.05776319280266762\n",
      "Epoch 1317, Loss: 0.11257054284214973, Final Batch Loss: 0.030674856156110764\n",
      "Epoch 1318, Loss: 0.12652622535824776, Final Batch Loss: 0.07493836432695389\n",
      "Epoch 1319, Loss: 0.08618670329451561, Final Batch Loss: 0.05005772039294243\n",
      "Epoch 1320, Loss: 0.08577558398246765, Final Batch Loss: 0.04620623588562012\n",
      "Epoch 1321, Loss: 0.10969485342502594, Final Batch Loss: 0.0698699802160263\n",
      "Epoch 1322, Loss: 0.11838166415691376, Final Batch Loss: 0.05651340261101723\n",
      "Epoch 1323, Loss: 0.08952191099524498, Final Batch Loss: 0.04664678871631622\n",
      "Epoch 1324, Loss: 0.13189401850104332, Final Batch Loss: 0.04730666056275368\n",
      "Epoch 1325, Loss: 0.09152887761592865, Final Batch Loss: 0.05983416363596916\n",
      "Epoch 1326, Loss: 0.07235313951969147, Final Batch Loss: 0.03714591637253761\n",
      "Epoch 1327, Loss: 0.10867016762495041, Final Batch Loss: 0.05943073704838753\n",
      "Epoch 1328, Loss: 0.07070764899253845, Final Batch Loss: 0.04412637650966644\n",
      "Epoch 1329, Loss: 0.10697922110557556, Final Batch Loss: 0.04914972931146622\n",
      "Epoch 1330, Loss: 0.13572680950164795, Final Batch Loss: 0.06588611751794815\n",
      "Epoch 1331, Loss: 0.09454939514398575, Final Batch Loss: 0.03663412481546402\n",
      "Epoch 1332, Loss: 0.17179103940725327, Final Batch Loss: 0.08297498524188995\n",
      "Epoch 1333, Loss: 0.0944114550948143, Final Batch Loss: 0.024487994611263275\n",
      "Epoch 1334, Loss: 0.0845939852297306, Final Batch Loss: 0.035725731402635574\n",
      "Epoch 1335, Loss: 0.08773220703005791, Final Batch Loss: 0.038668442517519\n",
      "Epoch 1336, Loss: 0.1083701066672802, Final Batch Loss: 0.03668693080544472\n",
      "Epoch 1337, Loss: 0.13047655671834946, Final Batch Loss: 0.09081541746854782\n",
      "Epoch 1338, Loss: 0.09141669049859047, Final Batch Loss: 0.03611544147133827\n",
      "Epoch 1339, Loss: 0.08329484984278679, Final Batch Loss: 0.03653325140476227\n",
      "Epoch 1340, Loss: 0.08270144835114479, Final Batch Loss: 0.030685313045978546\n",
      "Epoch 1341, Loss: 0.13265149667859077, Final Batch Loss: 0.07165125012397766\n",
      "Epoch 1342, Loss: 0.10579735785722733, Final Batch Loss: 0.051874443888664246\n",
      "Epoch 1343, Loss: 0.09803195670247078, Final Batch Loss: 0.054704442620277405\n",
      "Epoch 1344, Loss: 0.09241592884063721, Final Batch Loss: 0.05554133653640747\n",
      "Epoch 1345, Loss: 0.09121381118893623, Final Batch Loss: 0.04379365220665932\n",
      "Epoch 1346, Loss: 0.08334333449602127, Final Batch Loss: 0.03898407518863678\n",
      "Epoch 1347, Loss: 0.06436585541814566, Final Batch Loss: 0.014186359010636806\n",
      "Epoch 1348, Loss: 0.14845523610711098, Final Batch Loss: 0.10761608183383942\n",
      "Epoch 1349, Loss: 0.15454568341374397, Final Batch Loss: 0.11245770752429962\n",
      "Epoch 1350, Loss: 0.18334371596574783, Final Batch Loss: 0.09657223522663116\n",
      "Epoch 1351, Loss: 0.13168330863118172, Final Batch Loss: 0.04840877279639244\n",
      "Epoch 1352, Loss: 0.10885310918092728, Final Batch Loss: 0.07391996681690216\n",
      "Epoch 1353, Loss: 0.10261881723999977, Final Batch Loss: 0.03649798408150673\n",
      "Epoch 1354, Loss: 0.11851687356829643, Final Batch Loss: 0.03484081104397774\n",
      "Epoch 1355, Loss: 0.08155909553170204, Final Batch Loss: 0.044832270592451096\n",
      "Epoch 1356, Loss: 0.10082760453224182, Final Batch Loss: 0.051204122602939606\n",
      "Epoch 1357, Loss: 0.13657133653759956, Final Batch Loss: 0.10127324610948563\n",
      "Epoch 1358, Loss: 0.12393968552350998, Final Batch Loss: 0.07778489589691162\n",
      "Epoch 1359, Loss: 0.12266914173960686, Final Batch Loss: 0.04716002568602562\n",
      "Epoch 1360, Loss: 0.09737244620919228, Final Batch Loss: 0.051507316529750824\n",
      "Epoch 1361, Loss: 0.08794322982430458, Final Batch Loss: 0.052070025354623795\n",
      "Epoch 1362, Loss: 0.11586293578147888, Final Batch Loss: 0.07097912579774857\n",
      "Epoch 1363, Loss: 0.1290038414299488, Final Batch Loss: 0.08748259395360947\n",
      "Epoch 1364, Loss: 0.06973591074347496, Final Batch Loss: 0.02754880115389824\n",
      "Epoch 1365, Loss: 0.12410376965999603, Final Batch Loss: 0.06582619994878769\n",
      "Epoch 1366, Loss: 0.11291929706931114, Final Batch Loss: 0.039713140577077866\n",
      "Epoch 1367, Loss: 0.12154521048069, Final Batch Loss: 0.0715615525841713\n",
      "Epoch 1368, Loss: 0.1024116687476635, Final Batch Loss: 0.026742134243249893\n",
      "Epoch 1369, Loss: 0.11841334402561188, Final Batch Loss: 0.04833472520112991\n",
      "Epoch 1370, Loss: 0.11294952407479286, Final Batch Loss: 0.06164827570319176\n",
      "Epoch 1371, Loss: 0.10531080514192581, Final Batch Loss: 0.06311166286468506\n",
      "Epoch 1372, Loss: 0.08338507264852524, Final Batch Loss: 0.03237917274236679\n",
      "Epoch 1373, Loss: 0.0799847673624754, Final Batch Loss: 0.02825692854821682\n",
      "Epoch 1374, Loss: 0.15583328530192375, Final Batch Loss: 0.04988016560673714\n",
      "Epoch 1375, Loss: 0.08071814477443695, Final Batch Loss: 0.040194470435380936\n",
      "Epoch 1376, Loss: 0.12138447165489197, Final Batch Loss: 0.05633113533258438\n",
      "Epoch 1377, Loss: 0.20871936529874802, Final Batch Loss: 0.134074404835701\n",
      "Epoch 1378, Loss: 0.11445637047290802, Final Batch Loss: 0.0546870231628418\n",
      "Epoch 1379, Loss: 0.1111903227865696, Final Batch Loss: 0.06910567730665207\n",
      "Epoch 1380, Loss: 0.08463194593787193, Final Batch Loss: 0.042409420013427734\n",
      "Epoch 1381, Loss: 0.15173903852701187, Final Batch Loss: 0.07296198606491089\n",
      "Epoch 1382, Loss: 0.08756938576698303, Final Batch Loss: 0.03435341641306877\n",
      "Epoch 1383, Loss: 0.08757055550813675, Final Batch Loss: 0.03483203798532486\n",
      "Epoch 1384, Loss: 0.11956755816936493, Final Batch Loss: 0.0765337124466896\n",
      "Epoch 1385, Loss: 0.08485934510827065, Final Batch Loss: 0.040974341332912445\n",
      "Epoch 1386, Loss: 0.09795348346233368, Final Batch Loss: 0.04241681843996048\n",
      "Epoch 1387, Loss: 0.08052483201026917, Final Batch Loss: 0.0345371775329113\n",
      "Epoch 1388, Loss: 0.09835768304765224, Final Batch Loss: 0.07718724757432938\n",
      "Epoch 1389, Loss: 0.06778300926089287, Final Batch Loss: 0.04353310167789459\n",
      "Epoch 1390, Loss: 0.12529198825359344, Final Batch Loss: 0.08361659198999405\n",
      "Epoch 1391, Loss: 0.12066473439335823, Final Batch Loss: 0.06646452099084854\n",
      "Epoch 1392, Loss: 0.09425517171621323, Final Batch Loss: 0.04726652801036835\n",
      "Epoch 1393, Loss: 0.1317058689892292, Final Batch Loss: 0.07236527651548386\n",
      "Epoch 1394, Loss: 0.09029115363955498, Final Batch Loss: 0.038165051490068436\n",
      "Epoch 1395, Loss: 0.11953118070960045, Final Batch Loss: 0.05066775903105736\n",
      "Epoch 1396, Loss: 0.10792547091841698, Final Batch Loss: 0.06046957150101662\n",
      "Epoch 1397, Loss: 0.10863291099667549, Final Batch Loss: 0.07386042177677155\n",
      "Epoch 1398, Loss: 0.09074168652296066, Final Batch Loss: 0.04563233256340027\n",
      "Epoch 1399, Loss: 0.0907515212893486, Final Batch Loss: 0.03247692808508873\n",
      "Epoch 1400, Loss: 0.08197787962853909, Final Batch Loss: 0.06415324658155441\n",
      "Epoch 1401, Loss: 0.12147577479481697, Final Batch Loss: 0.04522797837853432\n",
      "Epoch 1402, Loss: 0.09342116862535477, Final Batch Loss: 0.04103930667042732\n",
      "Epoch 1403, Loss: 0.207950621843338, Final Batch Loss: 0.0844479575753212\n",
      "Epoch 1404, Loss: 0.16844233870506287, Final Batch Loss: 0.09125420451164246\n",
      "Epoch 1405, Loss: 0.07247080840170383, Final Batch Loss: 0.04741713032126427\n",
      "Epoch 1406, Loss: 0.13490601256489754, Final Batch Loss: 0.08203624933958054\n",
      "Epoch 1407, Loss: 0.1096041090786457, Final Batch Loss: 0.0417451448738575\n",
      "Epoch 1408, Loss: 0.06609554216265678, Final Batch Loss: 0.030742067843675613\n",
      "Epoch 1409, Loss: 0.1613152176141739, Final Batch Loss: 0.07049000263214111\n",
      "Epoch 1410, Loss: 0.12207765877246857, Final Batch Loss: 0.03842644393444061\n",
      "Epoch 1411, Loss: 0.09530051052570343, Final Batch Loss: 0.0634194165468216\n",
      "Epoch 1412, Loss: 0.11216475069522858, Final Batch Loss: 0.04913056641817093\n",
      "Epoch 1413, Loss: 0.17156250029802322, Final Batch Loss: 0.09186066687107086\n",
      "Epoch 1414, Loss: 0.10565559938549995, Final Batch Loss: 0.0447833314538002\n",
      "Epoch 1415, Loss: 0.10148328356444836, Final Batch Loss: 0.026446064934134483\n",
      "Epoch 1416, Loss: 0.09852654114365578, Final Batch Loss: 0.03497408702969551\n",
      "Epoch 1417, Loss: 0.11334648728370667, Final Batch Loss: 0.04593251645565033\n",
      "Epoch 1418, Loss: 0.13960355520248413, Final Batch Loss: 0.0839068666100502\n",
      "Epoch 1419, Loss: 0.18663736432790756, Final Batch Loss: 0.09322186559438705\n",
      "Epoch 1420, Loss: 0.1351093165576458, Final Batch Loss: 0.0439256988465786\n",
      "Epoch 1421, Loss: 0.11269541457295418, Final Batch Loss: 0.04743369296193123\n",
      "Epoch 1422, Loss: 0.12523984909057617, Final Batch Loss: 0.04936011880636215\n",
      "Epoch 1423, Loss: 0.08264288492500782, Final Batch Loss: 0.026392007246613503\n",
      "Epoch 1424, Loss: 0.14050381258130074, Final Batch Loss: 0.04056180641055107\n",
      "Epoch 1425, Loss: 0.14438985660672188, Final Batch Loss: 0.05551128461956978\n",
      "Epoch 1426, Loss: 0.16141953319311142, Final Batch Loss: 0.11810582131147385\n",
      "Epoch 1427, Loss: 0.17543940991163254, Final Batch Loss: 0.10575414448976517\n",
      "Epoch 1428, Loss: 0.09697722643613815, Final Batch Loss: 0.03193657845258713\n",
      "Epoch 1429, Loss: 0.18249279260635376, Final Batch Loss: 0.10737521946430206\n",
      "Epoch 1430, Loss: 0.11145175248384476, Final Batch Loss: 0.08567693084478378\n",
      "Epoch 1431, Loss: 0.11236753687262535, Final Batch Loss: 0.05633610486984253\n",
      "Epoch 1432, Loss: 0.13552477955818176, Final Batch Loss: 0.08962247520685196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1433, Loss: 0.07572793588042259, Final Batch Loss: 0.043229278177022934\n",
      "Epoch 1434, Loss: 0.1721617579460144, Final Batch Loss: 0.0681316927075386\n",
      "Epoch 1435, Loss: 0.12742778286337852, Final Batch Loss: 0.07588808983564377\n",
      "Epoch 1436, Loss: 0.17598573118448257, Final Batch Loss: 0.11415693163871765\n",
      "Epoch 1437, Loss: 0.12461415305733681, Final Batch Loss: 0.06561426818370819\n",
      "Epoch 1438, Loss: 0.12381495907902718, Final Batch Loss: 0.05682359263300896\n",
      "Epoch 1439, Loss: 0.13487014174461365, Final Batch Loss: 0.05032612383365631\n",
      "Epoch 1440, Loss: 0.14298296719789505, Final Batch Loss: 0.06658032536506653\n",
      "Epoch 1441, Loss: 0.12144016847014427, Final Batch Loss: 0.07666302472352982\n",
      "Epoch 1442, Loss: 0.09058896452188492, Final Batch Loss: 0.03636293485760689\n",
      "Epoch 1443, Loss: 0.08684233017265797, Final Batch Loss: 0.056432005017995834\n",
      "Epoch 1444, Loss: 0.09910513088107109, Final Batch Loss: 0.03862253576517105\n",
      "Epoch 1445, Loss: 0.10887705162167549, Final Batch Loss: 0.057437434792518616\n",
      "Epoch 1446, Loss: 0.15267352014780045, Final Batch Loss: 0.05184418708086014\n",
      "Epoch 1447, Loss: 0.12062456458806992, Final Batch Loss: 0.06454049795866013\n",
      "Epoch 1448, Loss: 0.1124657541513443, Final Batch Loss: 0.06911215931177139\n",
      "Epoch 1449, Loss: 0.12200083956122398, Final Batch Loss: 0.07673344016075134\n",
      "Epoch 1450, Loss: 0.08608134090900421, Final Batch Loss: 0.044670481234788895\n",
      "Epoch 1451, Loss: 0.1401285044848919, Final Batch Loss: 0.09142516553401947\n",
      "Epoch 1452, Loss: 0.08123248815536499, Final Batch Loss: 0.039443738758563995\n",
      "Epoch 1453, Loss: 0.07949334755539894, Final Batch Loss: 0.041554782539606094\n",
      "Epoch 1454, Loss: 0.09081298112869263, Final Batch Loss: 0.039358142763376236\n",
      "Epoch 1455, Loss: 0.17716271802783012, Final Batch Loss: 0.116176538169384\n",
      "Epoch 1456, Loss: 0.07969988137483597, Final Batch Loss: 0.03536341339349747\n",
      "Epoch 1457, Loss: 0.12002221122384071, Final Batch Loss: 0.05611355975270271\n",
      "Epoch 1458, Loss: 0.09674196317791939, Final Batch Loss: 0.04189741238951683\n",
      "Epoch 1459, Loss: 0.10474327951669693, Final Batch Loss: 0.031838029623031616\n",
      "Epoch 1460, Loss: 0.07589579187333584, Final Batch Loss: 0.05238372087478638\n",
      "Epoch 1461, Loss: 0.07834886014461517, Final Batch Loss: 0.03040289878845215\n",
      "Epoch 1462, Loss: 0.0758168064057827, Final Batch Loss: 0.02145879715681076\n",
      "Epoch 1463, Loss: 0.12794169038534164, Final Batch Loss: 0.0831708163022995\n",
      "Epoch 1464, Loss: 0.11010328307747841, Final Batch Loss: 0.052116598933935165\n",
      "Epoch 1465, Loss: 0.11502497270703316, Final Batch Loss: 0.07181528210639954\n",
      "Epoch 1466, Loss: 0.09780270233750343, Final Batch Loss: 0.03504450246691704\n",
      "Epoch 1467, Loss: 0.09089972078800201, Final Batch Loss: 0.03639998659491539\n",
      "Epoch 1468, Loss: 0.05840529501438141, Final Batch Loss: 0.020566504448652267\n",
      "Epoch 1469, Loss: 0.1006789281964302, Final Batch Loss: 0.0566755011677742\n",
      "Epoch 1470, Loss: 0.11609112843871117, Final Batch Loss: 0.07026785612106323\n",
      "Epoch 1471, Loss: 0.08437778428196907, Final Batch Loss: 0.03614732623100281\n",
      "Epoch 1472, Loss: 0.08675174415111542, Final Batch Loss: 0.050329163670539856\n",
      "Epoch 1473, Loss: 0.1305323839187622, Final Batch Loss: 0.07359585165977478\n",
      "Epoch 1474, Loss: 0.10296338051557541, Final Batch Loss: 0.052420467138290405\n",
      "Epoch 1475, Loss: 0.14407113194465637, Final Batch Loss: 0.07679011672735214\n",
      "Epoch 1476, Loss: 0.10323524847626686, Final Batch Loss: 0.05538412556052208\n",
      "Epoch 1477, Loss: 0.10917975381016731, Final Batch Loss: 0.08041167259216309\n",
      "Epoch 1478, Loss: 0.12166792899370193, Final Batch Loss: 0.04769723862409592\n",
      "Epoch 1479, Loss: 0.08676986768841743, Final Batch Loss: 0.03834819421172142\n",
      "Epoch 1480, Loss: 0.11855751648545265, Final Batch Loss: 0.07801724225282669\n",
      "Epoch 1481, Loss: 0.10649394243955612, Final Batch Loss: 0.04738542065024376\n",
      "Epoch 1482, Loss: 0.09511842206120491, Final Batch Loss: 0.03399118408560753\n",
      "Epoch 1483, Loss: 0.12094159424304962, Final Batch Loss: 0.07526058703660965\n",
      "Epoch 1484, Loss: 0.08716065436601639, Final Batch Loss: 0.02768142893910408\n",
      "Epoch 1485, Loss: 0.06654033809900284, Final Batch Loss: 0.034755706787109375\n",
      "Epoch 1486, Loss: 0.12673752754926682, Final Batch Loss: 0.1054234504699707\n",
      "Epoch 1487, Loss: 0.09975241497159004, Final Batch Loss: 0.04753974825143814\n",
      "Epoch 1488, Loss: 0.092911871150136, Final Batch Loss: 0.02807445265352726\n",
      "Epoch 1489, Loss: 0.1354922465980053, Final Batch Loss: 0.05548785999417305\n",
      "Epoch 1490, Loss: 0.10692040249705315, Final Batch Loss: 0.03652045503258705\n",
      "Epoch 1491, Loss: 0.11142018064856529, Final Batch Loss: 0.06067666411399841\n",
      "Epoch 1492, Loss: 0.14316794276237488, Final Batch Loss: 0.09462672472000122\n",
      "Epoch 1493, Loss: 0.12641552835702896, Final Batch Loss: 0.036657802760601044\n",
      "Epoch 1494, Loss: 0.11042692512273788, Final Batch Loss: 0.05566974729299545\n",
      "Epoch 1495, Loss: 0.12074832618236542, Final Batch Loss: 0.040961287915706635\n",
      "Epoch 1496, Loss: 0.10494239628314972, Final Batch Loss: 0.045272812247276306\n",
      "Epoch 1497, Loss: 0.10925660654902458, Final Batch Loss: 0.06560518592596054\n",
      "Epoch 1498, Loss: 0.15391231328248978, Final Batch Loss: 0.07061150670051575\n",
      "Epoch 1499, Loss: 0.0887891873717308, Final Batch Loss: 0.03718682378530502\n",
      "Epoch 1500, Loss: 0.08793879672884941, Final Batch Loss: 0.04154375195503235\n",
      "Epoch 1501, Loss: 0.08753368258476257, Final Batch Loss: 0.043847959488630295\n",
      "Epoch 1502, Loss: 0.08383709006011486, Final Batch Loss: 0.026154903694987297\n",
      "Epoch 1503, Loss: 0.05256190337240696, Final Batch Loss: 0.028102509677410126\n",
      "Epoch 1504, Loss: 0.09876075014472008, Final Batch Loss: 0.05133112147450447\n",
      "Epoch 1505, Loss: 0.06969467364251614, Final Batch Loss: 0.0462765209376812\n",
      "Epoch 1506, Loss: 0.0561523474752903, Final Batch Loss: 0.016739308834075928\n",
      "Epoch 1507, Loss: 0.12122974544763565, Final Batch Loss: 0.05835409462451935\n",
      "Epoch 1508, Loss: 0.092406515032053, Final Batch Loss: 0.055429548025131226\n",
      "Epoch 1509, Loss: 0.07574417814612389, Final Batch Loss: 0.028801329433918\n",
      "Epoch 1510, Loss: 0.11192930862307549, Final Batch Loss: 0.053924642503261566\n",
      "Epoch 1511, Loss: 0.0868831668049097, Final Batch Loss: 0.06283483654260635\n",
      "Epoch 1512, Loss: 0.07070255279541016, Final Batch Loss: 0.03664438799023628\n",
      "Epoch 1513, Loss: 0.11410439945757389, Final Batch Loss: 0.022389771416783333\n",
      "Epoch 1514, Loss: 0.06993576884269714, Final Batch Loss: 0.020168859511613846\n",
      "Epoch 1515, Loss: 0.12191802263259888, Final Batch Loss: 0.04957638680934906\n",
      "Epoch 1516, Loss: 0.13667670637369156, Final Batch Loss: 0.04631491005420685\n",
      "Epoch 1517, Loss: 0.09144885838031769, Final Batch Loss: 0.022313937544822693\n",
      "Epoch 1518, Loss: 0.09546169266104698, Final Batch Loss: 0.031630586832761765\n",
      "Epoch 1519, Loss: 0.15669863298535347, Final Batch Loss: 0.11280082911252975\n",
      "Epoch 1520, Loss: 0.08446352370083332, Final Batch Loss: 0.02551250346004963\n",
      "Epoch 1521, Loss: 0.09236658737063408, Final Batch Loss: 0.04735937342047691\n",
      "Epoch 1522, Loss: 0.10358625277876854, Final Batch Loss: 0.039139408618211746\n",
      "Epoch 1523, Loss: 0.09529844112694263, Final Batch Loss: 0.03021376021206379\n",
      "Epoch 1524, Loss: 0.06557445228099823, Final Batch Loss: 0.019818536937236786\n",
      "Epoch 1525, Loss: 0.130656398832798, Final Batch Loss: 0.0986146405339241\n",
      "Epoch 1526, Loss: 0.08829275518655777, Final Batch Loss: 0.03130963817238808\n",
      "Epoch 1527, Loss: 0.08876793086528778, Final Batch Loss: 0.039062440395355225\n",
      "Epoch 1528, Loss: 0.08558453433215618, Final Batch Loss: 0.028694091364741325\n",
      "Epoch 1529, Loss: 0.08973125740885735, Final Batch Loss: 0.03522000089287758\n",
      "Epoch 1530, Loss: 0.09608041122555733, Final Batch Loss: 0.05178844556212425\n",
      "Epoch 1531, Loss: 0.11676573753356934, Final Batch Loss: 0.05656546726822853\n",
      "Epoch 1532, Loss: 0.09881090000271797, Final Batch Loss: 0.037429872900247574\n",
      "Epoch 1533, Loss: 0.1273125559091568, Final Batch Loss: 0.0792362317442894\n",
      "Epoch 1534, Loss: 0.08084721490740776, Final Batch Loss: 0.04416002705693245\n",
      "Epoch 1535, Loss: 0.11122220382094383, Final Batch Loss: 0.04790038242936134\n",
      "Epoch 1536, Loss: 0.0939888134598732, Final Batch Loss: 0.05364241823554039\n",
      "Epoch 1537, Loss: 0.08870386146008968, Final Batch Loss: 0.017936235293745995\n",
      "Epoch 1538, Loss: 0.11418984085321426, Final Batch Loss: 0.056437715888023376\n",
      "Epoch 1539, Loss: 0.06963567063212395, Final Batch Loss: 0.03675417602062225\n",
      "Epoch 1540, Loss: 0.07742440328001976, Final Batch Loss: 0.04273208975791931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1541, Loss: 0.055399006232619286, Final Batch Loss: 0.027044404298067093\n",
      "Epoch 1542, Loss: 0.12389450892806053, Final Batch Loss: 0.09059787541627884\n",
      "Epoch 1543, Loss: 0.08331247419118881, Final Batch Loss: 0.04795508086681366\n",
      "Epoch 1544, Loss: 0.08084998652338982, Final Batch Loss: 0.04420076310634613\n",
      "Epoch 1545, Loss: 0.12305714748799801, Final Batch Loss: 0.09700217097997665\n",
      "Epoch 1546, Loss: 0.08929643407464027, Final Batch Loss: 0.040586136281490326\n",
      "Epoch 1547, Loss: 0.1174816507846117, Final Batch Loss: 0.026217805221676826\n",
      "Epoch 1548, Loss: 0.10459888353943825, Final Batch Loss: 0.024952750653028488\n",
      "Epoch 1549, Loss: 0.08739821426570415, Final Batch Loss: 0.06779207289218903\n",
      "Epoch 1550, Loss: 0.06771508231759071, Final Batch Loss: 0.027458395808935165\n",
      "Epoch 1551, Loss: 0.11920552141964436, Final Batch Loss: 0.09250600636005402\n",
      "Epoch 1552, Loss: 0.06930587813258171, Final Batch Loss: 0.03412510082125664\n",
      "Epoch 1553, Loss: 0.12264937534928322, Final Batch Loss: 0.07898756861686707\n",
      "Epoch 1554, Loss: 0.12831446528434753, Final Batch Loss: 0.07697131484746933\n",
      "Epoch 1555, Loss: 0.10175137594342232, Final Batch Loss: 0.03704093024134636\n",
      "Epoch 1556, Loss: 0.11911682412028313, Final Batch Loss: 0.07433117926120758\n",
      "Epoch 1557, Loss: 0.08314373530447483, Final Batch Loss: 0.02774851582944393\n",
      "Epoch 1558, Loss: 0.09362323209643364, Final Batch Loss: 0.04839027673006058\n",
      "Epoch 1559, Loss: 0.09433762729167938, Final Batch Loss: 0.055960532277822495\n",
      "Epoch 1560, Loss: 0.0723641887307167, Final Batch Loss: 0.0366869680583477\n",
      "Epoch 1561, Loss: 0.06125416420400143, Final Batch Loss: 0.030772317200899124\n",
      "Epoch 1562, Loss: 0.08375498279929161, Final Batch Loss: 0.03462317958474159\n",
      "Epoch 1563, Loss: 0.07110061123967171, Final Batch Loss: 0.01734960824251175\n",
      "Epoch 1564, Loss: 0.10543637722730637, Final Batch Loss: 0.050802744925022125\n",
      "Epoch 1565, Loss: 0.15573731064796448, Final Batch Loss: 0.06351105868816376\n",
      "Epoch 1566, Loss: 0.09781612455844879, Final Batch Loss: 0.04193640127778053\n",
      "Epoch 1567, Loss: 0.10851307213306427, Final Batch Loss: 0.06089155748486519\n",
      "Epoch 1568, Loss: 0.13750597089529037, Final Batch Loss: 0.07398773729801178\n",
      "Epoch 1569, Loss: 0.11821765825152397, Final Batch Loss: 0.046583179384469986\n",
      "Epoch 1570, Loss: 0.15142161399126053, Final Batch Loss: 0.10412256419658661\n",
      "Epoch 1571, Loss: 0.08367361687123775, Final Batch Loss: 0.03074960596859455\n",
      "Epoch 1572, Loss: 0.07010704465210438, Final Batch Loss: 0.02993532083928585\n",
      "Epoch 1573, Loss: 0.052788130939006805, Final Batch Loss: 0.02498798258602619\n",
      "Epoch 1574, Loss: 0.06250107102096081, Final Batch Loss: 0.028389835730195045\n",
      "Epoch 1575, Loss: 0.12143867090344429, Final Batch Loss: 0.053694795817136765\n",
      "Epoch 1576, Loss: 0.07376683689653873, Final Batch Loss: 0.04620927572250366\n",
      "Epoch 1577, Loss: 0.09969279915094376, Final Batch Loss: 0.0575423464179039\n",
      "Epoch 1578, Loss: 0.07787255570292473, Final Batch Loss: 0.031254079192876816\n",
      "Epoch 1579, Loss: 0.12014216557145119, Final Batch Loss: 0.047274116426706314\n",
      "Epoch 1580, Loss: 0.08000976219773293, Final Batch Loss: 0.032086730003356934\n",
      "Epoch 1581, Loss: 0.0775315873324871, Final Batch Loss: 0.040810443460941315\n",
      "Epoch 1582, Loss: 0.07093434035778046, Final Batch Loss: 0.03014359623193741\n",
      "Epoch 1583, Loss: 0.05205210857093334, Final Batch Loss: 0.021823206916451454\n",
      "Epoch 1584, Loss: 0.08284188061952591, Final Batch Loss: 0.03412659093737602\n",
      "Epoch 1585, Loss: 0.06424545869231224, Final Batch Loss: 0.02943332865834236\n",
      "Epoch 1586, Loss: 0.12578178942203522, Final Batch Loss: 0.06709589809179306\n",
      "Epoch 1587, Loss: 0.11186334304511547, Final Batch Loss: 0.026336034759879112\n",
      "Epoch 1588, Loss: 0.0676032081246376, Final Batch Loss: 0.0427854061126709\n",
      "Epoch 1589, Loss: 0.1255740448832512, Final Batch Loss: 0.08234690129756927\n",
      "Epoch 1590, Loss: 0.06895388662815094, Final Batch Loss: 0.030240241438150406\n",
      "Epoch 1591, Loss: 0.08450908586382866, Final Batch Loss: 0.05711609870195389\n",
      "Epoch 1592, Loss: 0.13479395397007465, Final Batch Loss: 0.10513734072446823\n",
      "Epoch 1593, Loss: 0.06800056621432304, Final Batch Loss: 0.0348929725587368\n",
      "Epoch 1594, Loss: 0.12372023984789848, Final Batch Loss: 0.08151919394731522\n",
      "Epoch 1595, Loss: 0.10814202204346657, Final Batch Loss: 0.039785247296094894\n",
      "Epoch 1596, Loss: 0.1641370803117752, Final Batch Loss: 0.08437912166118622\n",
      "Epoch 1597, Loss: 0.12480316683650017, Final Batch Loss: 0.0643058568239212\n",
      "Epoch 1598, Loss: 0.07637462951242924, Final Batch Loss: 0.01762390322983265\n",
      "Epoch 1599, Loss: 0.12417357042431831, Final Batch Loss: 0.04562634602189064\n",
      "Epoch 1600, Loss: 0.12529580295085907, Final Batch Loss: 0.09520404040813446\n",
      "Epoch 1601, Loss: 0.06797898933291435, Final Batch Loss: 0.044879451394081116\n",
      "Epoch 1602, Loss: 0.12863034009933472, Final Batch Loss: 0.05772296339273453\n",
      "Epoch 1603, Loss: 0.16547319293022156, Final Batch Loss: 0.09803437441587448\n",
      "Epoch 1604, Loss: 0.106938935816288, Final Batch Loss: 0.05894159898161888\n",
      "Epoch 1605, Loss: 0.09769247099757195, Final Batch Loss: 0.047192540019750595\n",
      "Epoch 1606, Loss: 0.0812512468546629, Final Batch Loss: 0.05180436000227928\n",
      "Epoch 1607, Loss: 0.07994359359145164, Final Batch Loss: 0.02075803279876709\n",
      "Epoch 1608, Loss: 0.1301545947790146, Final Batch Loss: 0.0833282470703125\n",
      "Epoch 1609, Loss: 0.16453468054533005, Final Batch Loss: 0.0911455899477005\n",
      "Epoch 1610, Loss: 0.10994438454508781, Final Batch Loss: 0.06854354590177536\n",
      "Epoch 1611, Loss: 0.0789378359913826, Final Batch Loss: 0.0359189547598362\n",
      "Epoch 1612, Loss: 0.06431840918958187, Final Batch Loss: 0.039325863122940063\n",
      "Epoch 1613, Loss: 0.1036381758749485, Final Batch Loss: 0.06210033968091011\n",
      "Epoch 1614, Loss: 0.08401448279619217, Final Batch Loss: 0.06113505735993385\n",
      "Epoch 1615, Loss: 0.08874064311385155, Final Batch Loss: 0.040431417524814606\n",
      "Epoch 1616, Loss: 0.11929595097899437, Final Batch Loss: 0.05993540212512016\n",
      "Epoch 1617, Loss: 0.08728737011551857, Final Batch Loss: 0.0400908887386322\n",
      "Epoch 1618, Loss: 0.10458363592624664, Final Batch Loss: 0.06920193880796432\n",
      "Epoch 1619, Loss: 0.1172415055334568, Final Batch Loss: 0.08100256323814392\n",
      "Epoch 1620, Loss: 0.09613140486180782, Final Batch Loss: 0.03056189976632595\n",
      "Epoch 1621, Loss: 0.07112244516611099, Final Batch Loss: 0.030098773539066315\n",
      "Epoch 1622, Loss: 0.10611540265381336, Final Batch Loss: 0.013759465888142586\n",
      "Epoch 1623, Loss: 0.06669431924819946, Final Batch Loss: 0.032372649759054184\n",
      "Epoch 1624, Loss: 0.05786273814737797, Final Batch Loss: 0.02910243347287178\n",
      "Epoch 1625, Loss: 0.09251392260193825, Final Batch Loss: 0.05507022887468338\n",
      "Epoch 1626, Loss: 0.08685211092233658, Final Batch Loss: 0.04665449634194374\n",
      "Epoch 1627, Loss: 0.0744562465697527, Final Batch Loss: 0.046705540269613266\n",
      "Epoch 1628, Loss: 0.12671607360243797, Final Batch Loss: 0.04050905629992485\n",
      "Epoch 1629, Loss: 0.1043116133660078, Final Batch Loss: 0.029369575902819633\n",
      "Epoch 1630, Loss: 0.06110551208257675, Final Batch Loss: 0.039228711277246475\n",
      "Epoch 1631, Loss: 0.0534578412771225, Final Batch Loss: 0.03698311746120453\n",
      "Epoch 1632, Loss: 0.11079409159719944, Final Batch Loss: 0.02288966067135334\n",
      "Epoch 1633, Loss: 0.07688008062541485, Final Batch Loss: 0.017545251175761223\n",
      "Epoch 1634, Loss: 0.12672284990549088, Final Batch Loss: 0.06919671595096588\n",
      "Epoch 1635, Loss: 0.06368096731603146, Final Batch Loss: 0.042757321149110794\n",
      "Epoch 1636, Loss: 0.14059395343065262, Final Batch Loss: 0.06986996531486511\n",
      "Epoch 1637, Loss: 0.08549886010587215, Final Batch Loss: 0.02308175526559353\n",
      "Epoch 1638, Loss: 0.10212643817067146, Final Batch Loss: 0.05850124731659889\n",
      "Epoch 1639, Loss: 0.09476259723305702, Final Batch Loss: 0.01704435423016548\n",
      "Epoch 1640, Loss: 0.07086537592113018, Final Batch Loss: 0.022999512031674385\n",
      "Epoch 1641, Loss: 0.11900553479790688, Final Batch Loss: 0.05503039434552193\n",
      "Epoch 1642, Loss: 0.14367249980568886, Final Batch Loss: 0.10173545032739639\n",
      "Epoch 1643, Loss: 0.10894273966550827, Final Batch Loss: 0.03556077182292938\n",
      "Epoch 1644, Loss: 0.08675997890532017, Final Batch Loss: 0.05817616730928421\n",
      "Epoch 1645, Loss: 0.12566114589571953, Final Batch Loss: 0.0641556829214096\n",
      "Epoch 1646, Loss: 0.09022166207432747, Final Batch Loss: 0.04895404726266861\n",
      "Epoch 1647, Loss: 0.12844408303499222, Final Batch Loss: 0.07639101147651672\n",
      "Epoch 1648, Loss: 0.08632510900497437, Final Batch Loss: 0.03821257874369621\n",
      "Epoch 1649, Loss: 0.1269126608967781, Final Batch Loss: 0.07666731625795364\n",
      "Epoch 1650, Loss: 0.08092690818011761, Final Batch Loss: 0.02855764515697956\n",
      "Epoch 1651, Loss: 0.11500920355319977, Final Batch Loss: 0.04091744124889374\n",
      "Epoch 1652, Loss: 0.10163478180766106, Final Batch Loss: 0.05180734395980835\n",
      "Epoch 1653, Loss: 0.11366523616015911, Final Batch Loss: 0.08788415789604187\n",
      "Epoch 1654, Loss: 0.08638593927025795, Final Batch Loss: 0.05381971225142479\n",
      "Epoch 1655, Loss: 0.1102948859333992, Final Batch Loss: 0.02832123637199402\n",
      "Epoch 1656, Loss: 0.06292141228914261, Final Batch Loss: 0.008286766707897186\n",
      "Epoch 1657, Loss: 0.12066471949219704, Final Batch Loss: 0.040228817611932755\n",
      "Epoch 1658, Loss: 0.06375310197472572, Final Batch Loss: 0.03176995366811752\n",
      "Epoch 1659, Loss: 0.05931738018989563, Final Batch Loss: 0.03063295967876911\n",
      "Epoch 1660, Loss: 0.08638695627450943, Final Batch Loss: 0.055417709052562714\n",
      "Epoch 1661, Loss: 0.07296499982476234, Final Batch Loss: 0.026755020022392273\n",
      "Epoch 1662, Loss: 0.11181771755218506, Final Batch Loss: 0.03136228024959564\n",
      "Epoch 1663, Loss: 0.08053924888372421, Final Batch Loss: 0.0340321883559227\n",
      "Epoch 1664, Loss: 0.053822942078113556, Final Batch Loss: 0.02250649407505989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1665, Loss: 0.09236365929245949, Final Batch Loss: 0.05244641378521919\n",
      "Epoch 1666, Loss: 0.09336712211370468, Final Batch Loss: 0.041679367423057556\n",
      "Epoch 1667, Loss: 0.13475317135453224, Final Batch Loss: 0.09231982380151749\n",
      "Epoch 1668, Loss: 0.11768120527267456, Final Batch Loss: 0.06363382190465927\n",
      "Epoch 1669, Loss: 0.08929616585373878, Final Batch Loss: 0.06180734559893608\n",
      "Epoch 1670, Loss: 0.08514787256717682, Final Batch Loss: 0.05782986804842949\n",
      "Epoch 1671, Loss: 0.12979601323604584, Final Batch Loss: 0.07508368790149689\n",
      "Epoch 1672, Loss: 0.09374717622995377, Final Batch Loss: 0.02904481440782547\n",
      "Epoch 1673, Loss: 0.11416468396782875, Final Batch Loss: 0.034093502908945084\n",
      "Epoch 1674, Loss: 0.13566026836633682, Final Batch Loss: 0.08911426365375519\n",
      "Epoch 1675, Loss: 0.06400298699736595, Final Batch Loss: 0.03839915990829468\n",
      "Epoch 1676, Loss: 0.07318525575101376, Final Batch Loss: 0.02682356722652912\n",
      "Epoch 1677, Loss: 0.1111842691898346, Final Batch Loss: 0.05343875288963318\n",
      "Epoch 1678, Loss: 0.0766952820122242, Final Batch Loss: 0.02458196133375168\n",
      "Epoch 1679, Loss: 0.06033235974609852, Final Batch Loss: 0.027694715186953545\n",
      "Epoch 1680, Loss: 0.08440723456442356, Final Batch Loss: 0.02001439966261387\n",
      "Epoch 1681, Loss: 0.1168375089764595, Final Batch Loss: 0.04514150321483612\n",
      "Epoch 1682, Loss: 0.09226037934422493, Final Batch Loss: 0.042017098516225815\n",
      "Epoch 1683, Loss: 0.09399381279945374, Final Batch Loss: 0.04863828793168068\n",
      "Epoch 1684, Loss: 0.08751897886395454, Final Batch Loss: 0.036315228790044785\n",
      "Epoch 1685, Loss: 0.06842658668756485, Final Batch Loss: 0.03207571804523468\n",
      "Epoch 1686, Loss: 0.09400341659784317, Final Batch Loss: 0.062354084104299545\n",
      "Epoch 1687, Loss: 0.09552175737917423, Final Batch Loss: 0.021400580182671547\n",
      "Epoch 1688, Loss: 0.06075392849743366, Final Batch Loss: 0.027400681748986244\n",
      "Epoch 1689, Loss: 0.09447940066456795, Final Batch Loss: 0.03928438574075699\n",
      "Epoch 1690, Loss: 0.12354865670204163, Final Batch Loss: 0.08117737621068954\n",
      "Epoch 1691, Loss: 0.11138486862182617, Final Batch Loss: 0.06528020650148392\n",
      "Epoch 1692, Loss: 0.1275610327720642, Final Batch Loss: 0.09597980231046677\n",
      "Epoch 1693, Loss: 0.09032895416021347, Final Batch Loss: 0.04882098734378815\n",
      "Epoch 1694, Loss: 0.11510049179196358, Final Batch Loss: 0.03633483126759529\n",
      "Epoch 1695, Loss: 0.0763942338526249, Final Batch Loss: 0.03359202295541763\n",
      "Epoch 1696, Loss: 0.09673898667097092, Final Batch Loss: 0.03704356774687767\n",
      "Epoch 1697, Loss: 0.08734406903386116, Final Batch Loss: 0.019448693841695786\n",
      "Epoch 1698, Loss: 0.06330181285738945, Final Batch Loss: 0.02673361822962761\n",
      "Epoch 1699, Loss: 0.07974998652935028, Final Batch Loss: 0.03473720699548721\n",
      "Epoch 1700, Loss: 0.11795277148485184, Final Batch Loss: 0.06460054218769073\n",
      "Epoch 1701, Loss: 0.06639812234789133, Final Batch Loss: 0.051390353590250015\n",
      "Epoch 1702, Loss: 0.14189595729112625, Final Batch Loss: 0.08329898864030838\n",
      "Epoch 1703, Loss: 0.051223549991846085, Final Batch Loss: 0.017214007675647736\n",
      "Epoch 1704, Loss: 0.07030146941542625, Final Batch Loss: 0.029636163264513016\n",
      "Epoch 1705, Loss: 0.08168766647577286, Final Batch Loss: 0.03606672212481499\n",
      "Epoch 1706, Loss: 0.10491975396871567, Final Batch Loss: 0.05420038849115372\n",
      "Epoch 1707, Loss: 0.08802282065153122, Final Batch Loss: 0.04434635862708092\n",
      "Epoch 1708, Loss: 0.11545009166002274, Final Batch Loss: 0.04178684949874878\n",
      "Epoch 1709, Loss: 0.08073500916361809, Final Batch Loss: 0.031796280294656754\n",
      "Epoch 1710, Loss: 0.05388804338872433, Final Batch Loss: 0.018920471891760826\n",
      "Epoch 1711, Loss: 0.07389538735151291, Final Batch Loss: 0.026572823524475098\n",
      "Epoch 1712, Loss: 0.10621725022792816, Final Batch Loss: 0.05028127133846283\n",
      "Epoch 1713, Loss: 0.10278620198369026, Final Batch Loss: 0.049230292439460754\n",
      "Epoch 1714, Loss: 0.08737217076122761, Final Batch Loss: 0.06683486700057983\n",
      "Epoch 1715, Loss: 0.15594296157360077, Final Batch Loss: 0.1081247627735138\n",
      "Epoch 1716, Loss: 0.06298716925084591, Final Batch Loss: 0.01577148027718067\n",
      "Epoch 1717, Loss: 0.10020971298217773, Final Batch Loss: 0.04074999317526817\n",
      "Epoch 1718, Loss: 0.08621517941355705, Final Batch Loss: 0.06691936403512955\n",
      "Epoch 1719, Loss: 0.06413760222494602, Final Batch Loss: 0.025721987709403038\n",
      "Epoch 1720, Loss: 0.0851096622645855, Final Batch Loss: 0.04147905483841896\n",
      "Epoch 1721, Loss: 0.06860813498497009, Final Batch Loss: 0.024036429822444916\n",
      "Epoch 1722, Loss: 0.11483243480324745, Final Batch Loss: 0.07401204854249954\n",
      "Epoch 1723, Loss: 0.05832215957343578, Final Batch Loss: 0.02313985489308834\n",
      "Epoch 1724, Loss: 0.1000695563852787, Final Batch Loss: 0.06587892025709152\n",
      "Epoch 1725, Loss: 0.06763545796275139, Final Batch Loss: 0.025919795036315918\n",
      "Epoch 1726, Loss: 0.09110818803310394, Final Batch Loss: 0.05633386969566345\n",
      "Epoch 1727, Loss: 0.08372548269107938, Final Batch Loss: 0.005226545501500368\n",
      "Epoch 1728, Loss: 0.15449657663702965, Final Batch Loss: 0.09220053255558014\n",
      "Epoch 1729, Loss: 0.1641763225197792, Final Batch Loss: 0.09834298491477966\n",
      "Epoch 1730, Loss: 0.062446270138025284, Final Batch Loss: 0.03189196437597275\n",
      "Epoch 1731, Loss: 0.058328437618911266, Final Batch Loss: 0.015198647044599056\n",
      "Epoch 1732, Loss: 0.11072505638003349, Final Batch Loss: 0.07339081168174744\n",
      "Epoch 1733, Loss: 0.07014994323253632, Final Batch Loss: 0.03311625495553017\n",
      "Epoch 1734, Loss: 0.10184246674180031, Final Batch Loss: 0.059371594339609146\n",
      "Epoch 1735, Loss: 0.09979893639683723, Final Batch Loss: 0.05442540720105171\n",
      "Epoch 1736, Loss: 0.09877313673496246, Final Batch Loss: 0.07326357811689377\n",
      "Epoch 1737, Loss: 0.07055804319679737, Final Batch Loss: 0.021761750802397728\n",
      "Epoch 1738, Loss: 0.06213443912565708, Final Batch Loss: 0.042560409754514694\n",
      "Epoch 1739, Loss: 0.12827513366937637, Final Batch Loss: 0.06454677879810333\n",
      "Epoch 1740, Loss: 0.11437373980879784, Final Batch Loss: 0.0628003478050232\n",
      "Epoch 1741, Loss: 0.07829560153186321, Final Batch Loss: 0.014211935922503471\n",
      "Epoch 1742, Loss: 0.1444094330072403, Final Batch Loss: 0.07857325673103333\n",
      "Epoch 1743, Loss: 0.12603415176272392, Final Batch Loss: 0.10268238186836243\n",
      "Epoch 1744, Loss: 0.06362065300345421, Final Batch Loss: 0.02689357101917267\n",
      "Epoch 1745, Loss: 0.1284872330725193, Final Batch Loss: 0.07840017229318619\n",
      "Epoch 1746, Loss: 0.11367269232869148, Final Batch Loss: 0.0664883404970169\n",
      "Epoch 1747, Loss: 0.11387334764003754, Final Batch Loss: 0.07192032784223557\n",
      "Epoch 1748, Loss: 0.07974014058709145, Final Batch Loss: 0.030537310987710953\n",
      "Epoch 1749, Loss: 0.12694177776575089, Final Batch Loss: 0.08242529630661011\n",
      "Epoch 1750, Loss: 0.13909617438912392, Final Batch Loss: 0.09540750831365585\n",
      "Epoch 1751, Loss: 0.08428361266851425, Final Batch Loss: 0.048990558832883835\n",
      "Epoch 1752, Loss: 0.09681665152311325, Final Batch Loss: 0.03270867466926575\n",
      "Epoch 1753, Loss: 0.13301663100719452, Final Batch Loss: 0.045430950820446014\n",
      "Epoch 1754, Loss: 0.09308254346251488, Final Batch Loss: 0.06333786249160767\n",
      "Epoch 1755, Loss: 0.1592024639248848, Final Batch Loss: 0.07993882894515991\n",
      "Epoch 1756, Loss: 0.08795983716845512, Final Batch Loss: 0.04225771501660347\n",
      "Epoch 1757, Loss: 0.10646822303533554, Final Batch Loss: 0.040934786200523376\n",
      "Epoch 1758, Loss: 0.1261112131178379, Final Batch Loss: 0.048052866011857986\n",
      "Epoch 1759, Loss: 0.15180694311857224, Final Batch Loss: 0.06588845700025558\n",
      "Epoch 1760, Loss: 0.12630820646882057, Final Batch Loss: 0.07862818241119385\n",
      "Epoch 1761, Loss: 0.09412578120827675, Final Batch Loss: 0.037651967257261276\n",
      "Epoch 1762, Loss: 0.10540443658828735, Final Batch Loss: 0.05125586315989494\n",
      "Epoch 1763, Loss: 0.09680624678730965, Final Batch Loss: 0.04264029115438461\n",
      "Epoch 1764, Loss: 0.060900427401065826, Final Batch Loss: 0.034509945660829544\n",
      "Epoch 1765, Loss: 0.054861120879650116, Final Batch Loss: 0.03179822862148285\n",
      "Epoch 1766, Loss: 0.13574843853712082, Final Batch Loss: 0.05889595299959183\n",
      "Epoch 1767, Loss: 0.09076398983597755, Final Batch Loss: 0.05470508337020874\n",
      "Epoch 1768, Loss: 0.15248209983110428, Final Batch Loss: 0.07617740333080292\n",
      "Epoch 1769, Loss: 0.1018776036798954, Final Batch Loss: 0.04066682234406471\n",
      "Epoch 1770, Loss: 0.10094288736581802, Final Batch Loss: 0.04688994958996773\n",
      "Epoch 1771, Loss: 0.09357141330838203, Final Batch Loss: 0.060817256569862366\n",
      "Epoch 1772, Loss: 0.08163659274578094, Final Batch Loss: 0.041548825800418854\n",
      "Epoch 1773, Loss: 0.10171777009963989, Final Batch Loss: 0.05255092680454254\n",
      "Epoch 1774, Loss: 0.11868846043944359, Final Batch Loss: 0.05759575590491295\n",
      "Epoch 1775, Loss: 0.05760944448411465, Final Batch Loss: 0.027918076142668724\n",
      "Epoch 1776, Loss: 0.09513845480978489, Final Batch Loss: 0.06893504410982132\n",
      "Epoch 1777, Loss: 0.08577539771795273, Final Batch Loss: 0.04995855689048767\n",
      "Epoch 1778, Loss: 0.058397551998496056, Final Batch Loss: 0.01635902188718319\n",
      "Epoch 1779, Loss: 0.07814333587884903, Final Batch Loss: 0.026341471821069717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1780, Loss: 0.08492616191506386, Final Batch Loss: 0.06357937306165695\n",
      "Epoch 1781, Loss: 0.15270250663161278, Final Batch Loss: 0.1238529160618782\n",
      "Epoch 1782, Loss: 0.093253955245018, Final Batch Loss: 0.027996152639389038\n",
      "Epoch 1783, Loss: 0.10949572548270226, Final Batch Loss: 0.07984915375709534\n",
      "Epoch 1784, Loss: 0.06586594879627228, Final Batch Loss: 0.03323381766676903\n",
      "Epoch 1785, Loss: 0.0832330733537674, Final Batch Loss: 0.0514114610850811\n",
      "Epoch 1786, Loss: 0.1173166036605835, Final Batch Loss: 0.03174935281276703\n",
      "Epoch 1787, Loss: 0.14342854171991348, Final Batch Loss: 0.10604549944400787\n",
      "Epoch 1788, Loss: 0.12065844237804413, Final Batch Loss: 0.055688463151454926\n",
      "Epoch 1789, Loss: 0.07453501410782337, Final Batch Loss: 0.02001522295176983\n",
      "Epoch 1790, Loss: 0.07655770890414715, Final Batch Loss: 0.049020394682884216\n",
      "Epoch 1791, Loss: 0.0651472769677639, Final Batch Loss: 0.043961361050605774\n",
      "Epoch 1792, Loss: 0.08153524436056614, Final Batch Loss: 0.05326296016573906\n",
      "Epoch 1793, Loss: 0.08409425243735313, Final Batch Loss: 0.050437189638614655\n",
      "Epoch 1794, Loss: 0.09506451711058617, Final Batch Loss: 0.053006261587142944\n",
      "Epoch 1795, Loss: 0.058890579268336296, Final Batch Loss: 0.025732921436429024\n",
      "Epoch 1796, Loss: 0.08436356298625469, Final Batch Loss: 0.01710696332156658\n",
      "Epoch 1797, Loss: 0.07103902846574783, Final Batch Loss: 0.028532590717077255\n",
      "Epoch 1798, Loss: 0.061744483187794685, Final Batch Loss: 0.031184803694486618\n",
      "Epoch 1799, Loss: 0.07595826871693134, Final Batch Loss: 0.028040947392582893\n",
      "Epoch 1800, Loss: 0.07691040262579918, Final Batch Loss: 0.04095989465713501\n",
      "Epoch 1801, Loss: 0.0685131847858429, Final Batch Loss: 0.03273354470729828\n",
      "Epoch 1802, Loss: 0.07882566750049591, Final Batch Loss: 0.03844363987445831\n",
      "Epoch 1803, Loss: 0.07477866671979427, Final Batch Loss: 0.051257334649562836\n",
      "Epoch 1804, Loss: 0.1065523810684681, Final Batch Loss: 0.034790169447660446\n",
      "Epoch 1805, Loss: 0.08482684567570686, Final Batch Loss: 0.04343219846487045\n",
      "Epoch 1806, Loss: 0.06458782032132149, Final Batch Loss: 0.026556771248579025\n",
      "Epoch 1807, Loss: 0.07998301088809967, Final Batch Loss: 0.029464788734912872\n",
      "Epoch 1808, Loss: 0.06309065967798233, Final Batch Loss: 0.01655764877796173\n",
      "Epoch 1809, Loss: 0.10865924134850502, Final Batch Loss: 0.043510060757398605\n",
      "Epoch 1810, Loss: 0.12453622370958328, Final Batch Loss: 0.04180165380239487\n",
      "Epoch 1811, Loss: 0.06537024490535259, Final Batch Loss: 0.026680225506424904\n",
      "Epoch 1812, Loss: 0.07138707302510738, Final Batch Loss: 0.02173042856156826\n",
      "Epoch 1813, Loss: 0.08497358113527298, Final Batch Loss: 0.04152355343103409\n",
      "Epoch 1814, Loss: 0.10739610716700554, Final Batch Loss: 0.04591194540262222\n",
      "Epoch 1815, Loss: 0.09354832768440247, Final Batch Loss: 0.05943036451935768\n",
      "Epoch 1816, Loss: 0.11245616152882576, Final Batch Loss: 0.06426221132278442\n",
      "Epoch 1817, Loss: 0.10452743992209435, Final Batch Loss: 0.059577010571956635\n",
      "Epoch 1818, Loss: 0.07784120738506317, Final Batch Loss: 0.037956446409225464\n",
      "Epoch 1819, Loss: 0.08714711852371693, Final Batch Loss: 0.059911150485277176\n",
      "Epoch 1820, Loss: 0.10711900144815445, Final Batch Loss: 0.06651882827281952\n",
      "Epoch 1821, Loss: 0.06850448995828629, Final Batch Loss: 0.02576548606157303\n",
      "Epoch 1822, Loss: 0.09558902308344841, Final Batch Loss: 0.05579165369272232\n",
      "Epoch 1823, Loss: 0.05365185625851154, Final Batch Loss: 0.0180636178702116\n",
      "Epoch 1824, Loss: 0.07001660764217377, Final Batch Loss: 0.03156289830803871\n",
      "Epoch 1825, Loss: 0.03264767397195101, Final Batch Loss: 0.013311740942299366\n",
      "Epoch 1826, Loss: 0.06788412481546402, Final Batch Loss: 0.03983583673834801\n",
      "Epoch 1827, Loss: 0.06510063074529171, Final Batch Loss: 0.04425090551376343\n",
      "Epoch 1828, Loss: 0.079225555062294, Final Batch Loss: 0.042970359325408936\n",
      "Epoch 1829, Loss: 0.09393138065934181, Final Batch Loss: 0.05995659530162811\n",
      "Epoch 1830, Loss: 0.06849901378154755, Final Batch Loss: 0.01772148534655571\n",
      "Epoch 1831, Loss: 0.08163241017609835, Final Batch Loss: 0.06735505908727646\n",
      "Epoch 1832, Loss: 0.045367949642241, Final Batch Loss: 0.014314009808003902\n",
      "Epoch 1833, Loss: 0.15495164692401886, Final Batch Loss: 0.08675643801689148\n",
      "Epoch 1834, Loss: 0.0891728475689888, Final Batch Loss: 0.0387289933860302\n",
      "Epoch 1835, Loss: 0.12937966361641884, Final Batch Loss: 0.045397091656923294\n",
      "Epoch 1836, Loss: 0.11075425520539284, Final Batch Loss: 0.0848768949508667\n",
      "Epoch 1837, Loss: 0.07680456340312958, Final Batch Loss: 0.056928619742393494\n",
      "Epoch 1838, Loss: 0.07294665649533272, Final Batch Loss: 0.02699940651655197\n",
      "Epoch 1839, Loss: 0.12274730578064919, Final Batch Loss: 0.05243616923689842\n",
      "Epoch 1840, Loss: 0.12396018579602242, Final Batch Loss: 0.037228476256132126\n",
      "Epoch 1841, Loss: 0.12386199086904526, Final Batch Loss: 0.0683153048157692\n",
      "Epoch 1842, Loss: 0.0753935594111681, Final Batch Loss: 0.029463911429047585\n",
      "Epoch 1843, Loss: 0.056573035195469856, Final Batch Loss: 0.015690015628933907\n",
      "Epoch 1844, Loss: 0.09545081108808517, Final Batch Loss: 0.037082113325595856\n",
      "Epoch 1845, Loss: 0.09156116098165512, Final Batch Loss: 0.059309300035238266\n",
      "Epoch 1846, Loss: 0.12630248814821243, Final Batch Loss: 0.06016896665096283\n",
      "Epoch 1847, Loss: 0.16872034966945648, Final Batch Loss: 0.08199518918991089\n",
      "Epoch 1848, Loss: 0.08024099469184875, Final Batch Loss: 0.024358823895454407\n",
      "Epoch 1849, Loss: 0.08265407755970955, Final Batch Loss: 0.04281990975141525\n",
      "Epoch 1850, Loss: 0.09352413192391396, Final Batch Loss: 0.03264576196670532\n",
      "Epoch 1851, Loss: 0.09596962481737137, Final Batch Loss: 0.05587022379040718\n",
      "Epoch 1852, Loss: 0.14303384721279144, Final Batch Loss: 0.07575610280036926\n",
      "Epoch 1853, Loss: 0.050794173032045364, Final Batch Loss: 0.033419039100408554\n",
      "Epoch 1854, Loss: 0.09925731271505356, Final Batch Loss: 0.032841168344020844\n",
      "Epoch 1855, Loss: 0.06947612017393112, Final Batch Loss: 0.016502168029546738\n",
      "Epoch 1856, Loss: 0.11607451364398003, Final Batch Loss: 0.04792897030711174\n",
      "Epoch 1857, Loss: 0.09828244894742966, Final Batch Loss: 0.04686010628938675\n",
      "Epoch 1858, Loss: 0.11926697380840778, Final Batch Loss: 0.09974607080221176\n",
      "Epoch 1859, Loss: 0.06545853428542614, Final Batch Loss: 0.0406758189201355\n",
      "Epoch 1860, Loss: 0.1055173110216856, Final Batch Loss: 0.07596975564956665\n",
      "Epoch 1861, Loss: 0.09761654958128929, Final Batch Loss: 0.04027828574180603\n",
      "Epoch 1862, Loss: 0.1073378212749958, Final Batch Loss: 0.033882517367601395\n",
      "Epoch 1863, Loss: 0.08938132971525192, Final Batch Loss: 0.04398506507277489\n",
      "Epoch 1864, Loss: 0.09101101756095886, Final Batch Loss: 0.055683400481939316\n",
      "Epoch 1865, Loss: 0.07789508625864983, Final Batch Loss: 0.04262830689549446\n",
      "Epoch 1866, Loss: 0.09090844541788101, Final Batch Loss: 0.031003408133983612\n",
      "Epoch 1867, Loss: 0.06929795071482658, Final Batch Loss: 0.04523767903447151\n",
      "Epoch 1868, Loss: 0.06066947616636753, Final Batch Loss: 0.035617195069789886\n",
      "Epoch 1869, Loss: 0.13684605062007904, Final Batch Loss: 0.06192081421613693\n",
      "Epoch 1870, Loss: 0.09015435539186001, Final Batch Loss: 0.020204422995448112\n",
      "Epoch 1871, Loss: 0.11718454584479332, Final Batch Loss: 0.05127275362610817\n",
      "Epoch 1872, Loss: 0.0786995217204094, Final Batch Loss: 0.03672903776168823\n",
      "Epoch 1873, Loss: 0.09494664520025253, Final Batch Loss: 0.03928116336464882\n",
      "Epoch 1874, Loss: 0.07729708962142467, Final Batch Loss: 0.054989710450172424\n",
      "Epoch 1875, Loss: 0.06405560672283173, Final Batch Loss: 0.028111357241868973\n",
      "Epoch 1876, Loss: 0.09153129905462265, Final Batch Loss: 0.036609794944524765\n",
      "Epoch 1877, Loss: 0.059765465557575226, Final Batch Loss: 0.033727407455444336\n",
      "Epoch 1878, Loss: 0.10334578901529312, Final Batch Loss: 0.05271963030099869\n",
      "Epoch 1879, Loss: 0.15410689264535904, Final Batch Loss: 0.08715538680553436\n",
      "Epoch 1880, Loss: 0.13730710372328758, Final Batch Loss: 0.08432523906230927\n",
      "Epoch 1881, Loss: 0.0861217025667429, Final Batch Loss: 0.0637911856174469\n",
      "Epoch 1882, Loss: 0.10577569529414177, Final Batch Loss: 0.0650402382016182\n",
      "Epoch 1883, Loss: 0.09038589522242546, Final Batch Loss: 0.06235675513744354\n",
      "Epoch 1884, Loss: 0.08859379589557648, Final Batch Loss: 0.056815944612026215\n",
      "Epoch 1885, Loss: 0.09220368042588234, Final Batch Loss: 0.03642597049474716\n",
      "Epoch 1886, Loss: 0.102602019906044, Final Batch Loss: 0.055985357612371445\n",
      "Epoch 1887, Loss: 0.0872374102473259, Final Batch Loss: 0.03697848692536354\n",
      "Epoch 1888, Loss: 0.09589773416519165, Final Batch Loss: 0.06343921273946762\n",
      "Epoch 1889, Loss: 0.1226385235786438, Final Batch Loss: 0.07755246013402939\n",
      "Epoch 1890, Loss: 0.10396576672792435, Final Batch Loss: 0.04532546177506447\n",
      "Epoch 1891, Loss: 0.09104279056191444, Final Batch Loss: 0.03639461100101471\n",
      "Epoch 1892, Loss: 0.07701323926448822, Final Batch Loss: 0.041979074478149414\n",
      "Epoch 1893, Loss: 0.09654698520898819, Final Batch Loss: 0.04047157242894173\n",
      "Epoch 1894, Loss: 0.08534231036901474, Final Batch Loss: 0.05865015089511871\n",
      "Epoch 1895, Loss: 0.09375287033617496, Final Batch Loss: 0.02287699468433857\n",
      "Epoch 1896, Loss: 0.07425162568688393, Final Batch Loss: 0.05278187617659569\n",
      "Epoch 1897, Loss: 0.1285274624824524, Final Batch Loss: 0.08877202868461609\n",
      "Epoch 1898, Loss: 0.12228256091475487, Final Batch Loss: 0.09113982319831848\n",
      "Epoch 1899, Loss: 0.09796792082488537, Final Batch Loss: 0.02962561510503292\n",
      "Epoch 1900, Loss: 0.0908685252070427, Final Batch Loss: 0.04339010640978813\n",
      "Epoch 1901, Loss: 0.050803777761757374, Final Batch Loss: 0.039508406072854996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1902, Loss: 0.08931602165102959, Final Batch Loss: 0.06943319737911224\n",
      "Epoch 1903, Loss: 0.07847181335091591, Final Batch Loss: 0.04097019135951996\n",
      "Epoch 1904, Loss: 0.07980536855757236, Final Batch Loss: 0.029756994917988777\n",
      "Epoch 1905, Loss: 0.16058260574936867, Final Batch Loss: 0.04992878809571266\n",
      "Epoch 1906, Loss: 0.056486595422029495, Final Batch Loss: 0.01583690196275711\n",
      "Epoch 1907, Loss: 0.07509779185056686, Final Batch Loss: 0.012989938259124756\n",
      "Epoch 1908, Loss: 0.06473041325807571, Final Batch Loss: 0.04411690682172775\n",
      "Epoch 1909, Loss: 0.06932879984378815, Final Batch Loss: 0.029618244618177414\n",
      "Epoch 1910, Loss: 0.08479434996843338, Final Batch Loss: 0.03698040544986725\n",
      "Epoch 1911, Loss: 0.05489280819892883, Final Batch Loss: 0.03303517773747444\n",
      "Epoch 1912, Loss: 0.1610981486737728, Final Batch Loss: 0.09936375916004181\n",
      "Epoch 1913, Loss: 0.10230804234743118, Final Batch Loss: 0.08312833309173584\n",
      "Epoch 1914, Loss: 0.06446986459195614, Final Batch Loss: 0.0279779564589262\n",
      "Epoch 1915, Loss: 0.12113730981945992, Final Batch Loss: 0.10291031748056412\n",
      "Epoch 1916, Loss: 0.0910690538585186, Final Batch Loss: 0.054873429238796234\n",
      "Epoch 1917, Loss: 0.08381714671850204, Final Batch Loss: 0.04248303547501564\n",
      "Epoch 1918, Loss: 0.07489643432199955, Final Batch Loss: 0.04483330622315407\n",
      "Epoch 1919, Loss: 0.07256176322698593, Final Batch Loss: 0.021950311958789825\n",
      "Epoch 1920, Loss: 0.1313355565071106, Final Batch Loss: 0.045937441289424896\n",
      "Epoch 1921, Loss: 0.07681374251842499, Final Batch Loss: 0.03141207620501518\n",
      "Epoch 1922, Loss: 0.08874055370688438, Final Batch Loss: 0.04688603803515434\n",
      "Epoch 1923, Loss: 0.06753196194767952, Final Batch Loss: 0.02497563511133194\n",
      "Epoch 1924, Loss: 0.08040739223361015, Final Batch Loss: 0.05489291623234749\n",
      "Epoch 1925, Loss: 0.040515560656785965, Final Batch Loss: 0.0156620554625988\n",
      "Epoch 1926, Loss: 0.11647619307041168, Final Batch Loss: 0.033449187874794006\n",
      "Epoch 1927, Loss: 0.06657079793512821, Final Batch Loss: 0.02167612873017788\n",
      "Epoch 1928, Loss: 0.1170370765030384, Final Batch Loss: 0.05917239561676979\n",
      "Epoch 1929, Loss: 0.060077544301748276, Final Batch Loss: 0.027149636298418045\n",
      "Epoch 1930, Loss: 0.07168236654251814, Final Batch Loss: 0.01412415411323309\n",
      "Epoch 1931, Loss: 0.05852953903377056, Final Batch Loss: 0.03370044007897377\n",
      "Epoch 1932, Loss: 0.05577431060373783, Final Batch Loss: 0.035079438239336014\n",
      "Epoch 1933, Loss: 0.05493255890905857, Final Batch Loss: 0.02791452221572399\n",
      "Epoch 1934, Loss: 0.09157831221818924, Final Batch Loss: 0.04720331355929375\n",
      "Epoch 1935, Loss: 0.050535308197140694, Final Batch Loss: 0.0336432084441185\n",
      "Epoch 1936, Loss: 0.0822572335600853, Final Batch Loss: 0.0649043545126915\n",
      "Epoch 1937, Loss: 0.12928849458694458, Final Batch Loss: 0.06990624964237213\n",
      "Epoch 1938, Loss: 0.08354227244853973, Final Batch Loss: 0.03169767186045647\n",
      "Epoch 1939, Loss: 0.0802631564438343, Final Batch Loss: 0.032937586307525635\n",
      "Epoch 1940, Loss: 0.09079163521528244, Final Batch Loss: 0.025150440633296967\n",
      "Epoch 1941, Loss: 0.08729393407702446, Final Batch Loss: 0.0387190505862236\n",
      "Epoch 1942, Loss: 0.0877307690680027, Final Batch Loss: 0.06153041869401932\n",
      "Epoch 1943, Loss: 0.13910112529993057, Final Batch Loss: 0.0634661316871643\n",
      "Epoch 1944, Loss: 0.07211577333509922, Final Batch Loss: 0.020455868914723396\n",
      "Epoch 1945, Loss: 0.11190532520413399, Final Batch Loss: 0.059648897498846054\n",
      "Epoch 1946, Loss: 0.09815110266208649, Final Batch Loss: 0.034222833812236786\n",
      "Epoch 1947, Loss: 0.0829253327101469, Final Batch Loss: 0.06165759637951851\n",
      "Epoch 1948, Loss: 0.05681074596941471, Final Batch Loss: 0.033055778592824936\n",
      "Epoch 1949, Loss: 0.07425467111170292, Final Batch Loss: 0.02013884298503399\n",
      "Epoch 1950, Loss: 0.07971945032477379, Final Batch Loss: 0.03988815099000931\n",
      "Epoch 1951, Loss: 0.07162323780357838, Final Batch Loss: 0.030920954421162605\n",
      "Epoch 1952, Loss: 0.1551874279975891, Final Batch Loss: 0.09179318696260452\n",
      "Epoch 1953, Loss: 0.08476621937006712, Final Batch Loss: 0.015421557240188122\n",
      "Epoch 1954, Loss: 0.15145711600780487, Final Batch Loss: 0.051459088921546936\n",
      "Epoch 1955, Loss: 0.12388467416167259, Final Batch Loss: 0.06614946573972702\n",
      "Epoch 1956, Loss: 0.11792363598942757, Final Batch Loss: 0.046301137655973434\n",
      "Epoch 1957, Loss: 0.15120916441082954, Final Batch Loss: 0.09324830025434494\n",
      "Epoch 1958, Loss: 0.07953174784779549, Final Batch Loss: 0.026579011231660843\n",
      "Epoch 1959, Loss: 0.0800276417285204, Final Batch Loss: 0.05052519962191582\n",
      "Epoch 1960, Loss: 0.05929380841553211, Final Batch Loss: 0.03131041303277016\n",
      "Epoch 1961, Loss: 0.10178384184837341, Final Batch Loss: 0.04368910938501358\n",
      "Epoch 1962, Loss: 0.07984818145632744, Final Batch Loss: 0.0397290475666523\n",
      "Epoch 1963, Loss: 0.0906449407339096, Final Batch Loss: 0.042291413992643356\n",
      "Epoch 1964, Loss: 0.054564425721764565, Final Batch Loss: 0.0358123779296875\n",
      "Epoch 1965, Loss: 0.11650172993540764, Final Batch Loss: 0.09691844880580902\n",
      "Epoch 1966, Loss: 0.0789962001144886, Final Batch Loss: 0.03313084691762924\n",
      "Epoch 1967, Loss: 0.10464733466506004, Final Batch Loss: 0.06128523126244545\n",
      "Epoch 1968, Loss: 0.1034717708826065, Final Batch Loss: 0.05387074500322342\n",
      "Epoch 1969, Loss: 0.045921800658106804, Final Batch Loss: 0.01780967228114605\n",
      "Epoch 1970, Loss: 0.09029646217823029, Final Batch Loss: 0.043520115315914154\n",
      "Epoch 1971, Loss: 0.10224565863609314, Final Batch Loss: 0.05414499342441559\n",
      "Epoch 1972, Loss: 0.047175660729408264, Final Batch Loss: 0.03085256740450859\n",
      "Epoch 1973, Loss: 0.09008430503308773, Final Batch Loss: 0.07361404597759247\n",
      "Epoch 1974, Loss: 0.09490461088716984, Final Batch Loss: 0.028039472177624702\n",
      "Epoch 1975, Loss: 0.08699531108140945, Final Batch Loss: 0.030079610645771027\n",
      "Epoch 1976, Loss: 0.06802054308354855, Final Batch Loss: 0.028719769790768623\n",
      "Epoch 1977, Loss: 0.0832110196352005, Final Batch Loss: 0.03445994108915329\n",
      "Epoch 1978, Loss: 0.07840722240507603, Final Batch Loss: 0.05667587369680405\n",
      "Epoch 1979, Loss: 0.07453406974673271, Final Batch Loss: 0.04516388848423958\n",
      "Epoch 1980, Loss: 0.0751545075327158, Final Batch Loss: 0.030676836147904396\n",
      "Epoch 1981, Loss: 0.09086136892437935, Final Batch Loss: 0.04878697171807289\n",
      "Epoch 1982, Loss: 0.039704713970422745, Final Batch Loss: 0.018741996958851814\n",
      "Epoch 1983, Loss: 0.07694016024470329, Final Batch Loss: 0.043738964945077896\n",
      "Epoch 1984, Loss: 0.07626509480178356, Final Batch Loss: 0.018438657745718956\n",
      "Epoch 1985, Loss: 0.12121836468577385, Final Batch Loss: 0.06562602519989014\n",
      "Epoch 1986, Loss: 0.03324354346841574, Final Batch Loss: 0.012421018444001675\n",
      "Epoch 1987, Loss: 0.10638584196567535, Final Batch Loss: 0.05829517915844917\n",
      "Epoch 1988, Loss: 0.13010172545909882, Final Batch Loss: 0.09036058932542801\n",
      "Epoch 1989, Loss: 0.06933984160423279, Final Batch Loss: 0.03598589822649956\n",
      "Epoch 1990, Loss: 0.06697994843125343, Final Batch Loss: 0.028480272740125656\n",
      "Epoch 1991, Loss: 0.10077124089002609, Final Batch Loss: 0.0430019274353981\n",
      "Epoch 1992, Loss: 0.09170357137918472, Final Batch Loss: 0.035932332277297974\n",
      "Epoch 1993, Loss: 0.11741116642951965, Final Batch Loss: 0.0560261495411396\n",
      "Epoch 1994, Loss: 0.10627777315676212, Final Batch Loss: 0.08534610271453857\n",
      "Epoch 1995, Loss: 0.0931045264005661, Final Batch Loss: 0.051919370889663696\n",
      "Epoch 1996, Loss: 0.11726497113704681, Final Batch Loss: 0.06531663984060287\n",
      "Epoch 1997, Loss: 0.08952778950333595, Final Batch Loss: 0.03205054625868797\n",
      "Epoch 1998, Loss: 0.13608045876026154, Final Batch Loss: 0.06876466423273087\n",
      "Epoch 1999, Loss: 0.06072104349732399, Final Batch Loss: 0.044506803154945374\n",
      "Epoch 2000, Loss: 0.07196320220828056, Final Batch Loss: 0.05513380467891693\n",
      "Epoch 2001, Loss: 0.05726036801934242, Final Batch Loss: 0.028781481087207794\n",
      "Epoch 2002, Loss: 0.07579565793275833, Final Batch Loss: 0.052361611276865005\n",
      "Epoch 2003, Loss: 0.05695191211998463, Final Batch Loss: 0.027580369263887405\n",
      "Epoch 2004, Loss: 0.07738942839205265, Final Batch Loss: 0.04812070354819298\n",
      "Epoch 2005, Loss: 0.05337459687143564, Final Batch Loss: 0.03898712992668152\n",
      "Epoch 2006, Loss: 0.07611849904060364, Final Batch Loss: 0.03677653893828392\n",
      "Epoch 2007, Loss: 0.08933866024017334, Final Batch Loss: 0.03917786106467247\n",
      "Epoch 2008, Loss: 0.06507786549627781, Final Batch Loss: 0.04231603816151619\n",
      "Epoch 2009, Loss: 0.08426320552825928, Final Batch Loss: 0.057240795344114304\n",
      "Epoch 2010, Loss: 0.056448500603437424, Final Batch Loss: 0.02746920846402645\n",
      "Epoch 2011, Loss: 0.06774884089827538, Final Batch Loss: 0.03298875689506531\n",
      "Epoch 2012, Loss: 0.10024696215987206, Final Batch Loss: 0.06106957793235779\n",
      "Epoch 2013, Loss: 0.061896612867712975, Final Batch Loss: 0.018248485401272774\n",
      "Epoch 2014, Loss: 0.07938496768474579, Final Batch Loss: 0.04432613030076027\n",
      "Epoch 2015, Loss: 0.10411451756954193, Final Batch Loss: 0.04646413400769234\n",
      "Epoch 2016, Loss: 0.0766687598079443, Final Batch Loss: 0.02488729916512966\n",
      "Epoch 2017, Loss: 0.06550023332238197, Final Batch Loss: 0.03647056967020035\n",
      "Epoch 2018, Loss: 0.08884445205330849, Final Batch Loss: 0.05040738359093666\n",
      "Epoch 2019, Loss: 0.10972631722688675, Final Batch Loss: 0.06201755627989769\n",
      "Epoch 2020, Loss: 0.0746443085372448, Final Batch Loss: 0.036342788487672806\n",
      "Epoch 2021, Loss: 0.0871044471859932, Final Batch Loss: 0.04335444048047066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2022, Loss: 0.053658803924918175, Final Batch Loss: 0.01459924690425396\n",
      "Epoch 2023, Loss: 0.1546206772327423, Final Batch Loss: 0.08349188417196274\n",
      "Epoch 2024, Loss: 0.07521054148674011, Final Batch Loss: 0.039064921438694\n",
      "Epoch 2025, Loss: 0.06314121279865503, Final Batch Loss: 0.015608769841492176\n",
      "Epoch 2026, Loss: 0.06667710095643997, Final Batch Loss: 0.04095856100320816\n",
      "Epoch 2027, Loss: 0.1330241672694683, Final Batch Loss: 0.11009452491998672\n",
      "Epoch 2028, Loss: 0.10442222654819489, Final Batch Loss: 0.042009901255369186\n",
      "Epoch 2029, Loss: 0.04932152479887009, Final Batch Loss: 0.02736295945942402\n",
      "Epoch 2030, Loss: 0.09273982420563698, Final Batch Loss: 0.06031280755996704\n",
      "Epoch 2031, Loss: 0.13281967863440514, Final Batch Loss: 0.08883735537528992\n",
      "Epoch 2032, Loss: 0.0948333628475666, Final Batch Loss: 0.040001556277275085\n",
      "Epoch 2033, Loss: 0.07054861262440681, Final Batch Loss: 0.01776273176074028\n",
      "Epoch 2034, Loss: 0.09120471030473709, Final Batch Loss: 0.059612080454826355\n",
      "Epoch 2035, Loss: 0.0891924761235714, Final Batch Loss: 0.056573882699012756\n",
      "Epoch 2036, Loss: 0.09699143655598164, Final Batch Loss: 0.0778517797589302\n",
      "Epoch 2037, Loss: 0.10197160020470619, Final Batch Loss: 0.06024118885397911\n",
      "Epoch 2038, Loss: 0.04629836976528168, Final Batch Loss: 0.0181273203343153\n",
      "Epoch 2039, Loss: 0.10562273114919662, Final Batch Loss: 0.04560987651348114\n",
      "Epoch 2040, Loss: 0.103666752576828, Final Batch Loss: 0.06842470914125443\n",
      "Epoch 2041, Loss: 0.07417263463139534, Final Batch Loss: 0.018134165555238724\n",
      "Epoch 2042, Loss: 0.10433294251561165, Final Batch Loss: 0.04585311934351921\n",
      "Epoch 2043, Loss: 0.1405293121933937, Final Batch Loss: 0.0906977504491806\n",
      "Epoch 2044, Loss: 0.08350931853055954, Final Batch Loss: 0.049873679876327515\n",
      "Epoch 2045, Loss: 0.09613778628408909, Final Batch Loss: 0.02666543610394001\n",
      "Epoch 2046, Loss: 0.09971980005502701, Final Batch Loss: 0.045494697988033295\n",
      "Epoch 2047, Loss: 0.09597898460924625, Final Batch Loss: 0.030819745734333992\n",
      "Epoch 2048, Loss: 0.06631707586348057, Final Batch Loss: 0.022520212456583977\n",
      "Epoch 2049, Loss: 0.1485469564795494, Final Batch Loss: 0.039087288081645966\n",
      "Epoch 2050, Loss: 0.07990783080458641, Final Batch Loss: 0.0321430079638958\n",
      "Epoch 2051, Loss: 0.09551848843693733, Final Batch Loss: 0.06742016971111298\n",
      "Epoch 2052, Loss: 0.13317483477294445, Final Batch Loss: 0.024605391547083855\n",
      "Epoch 2053, Loss: 0.08542003575712442, Final Batch Loss: 0.01493136677891016\n",
      "Epoch 2054, Loss: 0.08114105276763439, Final Batch Loss: 0.02466348372399807\n",
      "Epoch 2055, Loss: 0.09349029324948788, Final Batch Loss: 0.06271438300609589\n",
      "Epoch 2056, Loss: 0.055547695606946945, Final Batch Loss: 0.032334573566913605\n",
      "Epoch 2057, Loss: 0.09861811809241772, Final Batch Loss: 0.07822741568088531\n",
      "Epoch 2058, Loss: 0.06577886082231998, Final Batch Loss: 0.018687700852751732\n",
      "Epoch 2059, Loss: 0.07500576227903366, Final Batch Loss: 0.035263415426015854\n",
      "Epoch 2060, Loss: 0.0635746531188488, Final Batch Loss: 0.02870156243443489\n",
      "Epoch 2061, Loss: 0.0646753404289484, Final Batch Loss: 0.03732673078775406\n",
      "Epoch 2062, Loss: 0.09645316563546658, Final Batch Loss: 0.030359847471117973\n",
      "Epoch 2063, Loss: 0.06838314887136221, Final Batch Loss: 0.011088049970567226\n",
      "Epoch 2064, Loss: 0.16005591303110123, Final Batch Loss: 0.09309455007314682\n",
      "Epoch 2065, Loss: 0.08523862808942795, Final Batch Loss: 0.05616184324026108\n",
      "Epoch 2066, Loss: 0.08478367328643799, Final Batch Loss: 0.05182399973273277\n",
      "Epoch 2067, Loss: 0.0891229659318924, Final Batch Loss: 0.047051768749952316\n",
      "Epoch 2068, Loss: 0.06958994269371033, Final Batch Loss: 0.026010625064373016\n",
      "Epoch 2069, Loss: 0.08706272765994072, Final Batch Loss: 0.05561309680342674\n",
      "Epoch 2070, Loss: 0.0702665988355875, Final Batch Loss: 0.02528580091893673\n",
      "Epoch 2071, Loss: 0.06264475546777248, Final Batch Loss: 0.018809406086802483\n",
      "Epoch 2072, Loss: 0.060546696186065674, Final Batch Loss: 0.028269711881875992\n",
      "Epoch 2073, Loss: 0.10807203128933907, Final Batch Loss: 0.05299881473183632\n",
      "Epoch 2074, Loss: 0.08916907757520676, Final Batch Loss: 0.02561768889427185\n",
      "Epoch 2075, Loss: 0.11236860230565071, Final Batch Loss: 0.06833641231060028\n",
      "Epoch 2076, Loss: 0.06361355260014534, Final Batch Loss: 0.012289401143789291\n",
      "Epoch 2077, Loss: 0.04562571831047535, Final Batch Loss: 0.027251143008470535\n",
      "Epoch 2078, Loss: 0.08510550856590271, Final Batch Loss: 0.03710806369781494\n",
      "Epoch 2079, Loss: 0.08661306649446487, Final Batch Loss: 0.06293132901191711\n",
      "Epoch 2080, Loss: 0.09266186878085136, Final Batch Loss: 0.03401622921228409\n",
      "Epoch 2081, Loss: 0.09003798104822636, Final Batch Loss: 0.026792610064148903\n",
      "Epoch 2082, Loss: 0.09955509565770626, Final Batch Loss: 0.07067788392305374\n",
      "Epoch 2083, Loss: 0.10775581002235413, Final Batch Loss: 0.053718529641628265\n",
      "Epoch 2084, Loss: 0.05125146359205246, Final Batch Loss: 0.02745376154780388\n",
      "Epoch 2085, Loss: 0.0964014120399952, Final Batch Loss: 0.038526058197021484\n",
      "Epoch 2086, Loss: 0.0818014033138752, Final Batch Loss: 0.05747515708208084\n",
      "Epoch 2087, Loss: 0.07493741437792778, Final Batch Loss: 0.024868454784154892\n",
      "Epoch 2088, Loss: 0.09019255265593529, Final Batch Loss: 0.054022274911403656\n",
      "Epoch 2089, Loss: 0.05771530792117119, Final Batch Loss: 0.038651492446660995\n",
      "Epoch 2090, Loss: 0.07536994107067585, Final Batch Loss: 0.015541324391961098\n",
      "Epoch 2091, Loss: 0.12431345134973526, Final Batch Loss: 0.08565724641084671\n",
      "Epoch 2092, Loss: 0.052854273468256, Final Batch Loss: 0.018082480877637863\n",
      "Epoch 2093, Loss: 0.03799353167414665, Final Batch Loss: 0.017944393679499626\n",
      "Epoch 2094, Loss: 0.1287500597536564, Final Batch Loss: 0.05813426151871681\n",
      "Epoch 2095, Loss: 0.07094141282141209, Final Batch Loss: 0.028718670830130577\n",
      "Epoch 2096, Loss: 0.06212796363979578, Final Batch Loss: 0.014621776528656483\n",
      "Epoch 2097, Loss: 0.07739096507430077, Final Batch Loss: 0.03746340796351433\n",
      "Epoch 2098, Loss: 0.09831552952528, Final Batch Loss: 0.07023771107196808\n",
      "Epoch 2099, Loss: 0.0787910558283329, Final Batch Loss: 0.0424109622836113\n",
      "Epoch 2100, Loss: 0.05927231349050999, Final Batch Loss: 0.024052226915955544\n",
      "Epoch 2101, Loss: 0.07528096809983253, Final Batch Loss: 0.04390409216284752\n",
      "Epoch 2102, Loss: 0.02928122878074646, Final Batch Loss: 0.013947783969342709\n",
      "Epoch 2103, Loss: 0.08134179562330246, Final Batch Loss: 0.03775286301970482\n",
      "Epoch 2104, Loss: 0.09051578305661678, Final Batch Loss: 0.02499234490096569\n",
      "Epoch 2105, Loss: 0.07079886645078659, Final Batch Loss: 0.04488636553287506\n",
      "Epoch 2106, Loss: 0.10432088747620583, Final Batch Loss: 0.0656123161315918\n",
      "Epoch 2107, Loss: 0.0371642354875803, Final Batch Loss: 0.012865543365478516\n",
      "Epoch 2108, Loss: 0.07097140699625015, Final Batch Loss: 0.0406816340982914\n",
      "Epoch 2109, Loss: 0.09753159061074257, Final Batch Loss: 0.04937092214822769\n",
      "Epoch 2110, Loss: 0.12822947278618813, Final Batch Loss: 0.09624005109071732\n",
      "Epoch 2111, Loss: 0.10318409278988838, Final Batch Loss: 0.05302854999899864\n",
      "Epoch 2112, Loss: 0.12303006649017334, Final Batch Loss: 0.06402572244405746\n",
      "Epoch 2113, Loss: 0.10352175310254097, Final Batch Loss: 0.061724841594696045\n",
      "Epoch 2114, Loss: 0.07051368802785873, Final Batch Loss: 0.03432977944612503\n",
      "Epoch 2115, Loss: 0.10844670608639717, Final Batch Loss: 0.07701287418603897\n",
      "Epoch 2116, Loss: 0.12801247462630272, Final Batch Loss: 0.05654248967766762\n",
      "Epoch 2117, Loss: 0.05205322615802288, Final Batch Loss: 0.04138244688510895\n",
      "Epoch 2118, Loss: 0.08874949812889099, Final Batch Loss: 0.03816641867160797\n",
      "Epoch 2119, Loss: 0.13409075140953064, Final Batch Loss: 0.04947143793106079\n",
      "Epoch 2120, Loss: 0.10665949061512947, Final Batch Loss: 0.0388900525867939\n",
      "Epoch 2121, Loss: 0.08541349321603775, Final Batch Loss: 0.049780622124671936\n",
      "Epoch 2122, Loss: 0.11188962683081627, Final Batch Loss: 0.04584171995520592\n",
      "Epoch 2123, Loss: 0.07031690701842308, Final Batch Loss: 0.04418991506099701\n",
      "Epoch 2124, Loss: 0.08939015865325928, Final Batch Loss: 0.045969124883413315\n",
      "Epoch 2125, Loss: 0.06315567344427109, Final Batch Loss: 0.029279112815856934\n",
      "Epoch 2126, Loss: 0.07998539507389069, Final Batch Loss: 0.022823911160230637\n",
      "Epoch 2127, Loss: 0.03655758406966925, Final Batch Loss: 0.02390277571976185\n",
      "Epoch 2128, Loss: 0.05304048769176006, Final Batch Loss: 0.039508312940597534\n",
      "Epoch 2129, Loss: 0.1357138454914093, Final Batch Loss: 0.06657195836305618\n",
      "Epoch 2130, Loss: 0.06981800217181444, Final Batch Loss: 0.012683317996561527\n",
      "Epoch 2131, Loss: 0.10040231980383396, Final Batch Loss: 0.02493399567902088\n",
      "Epoch 2132, Loss: 0.11231090873479843, Final Batch Loss: 0.07382313162088394\n",
      "Epoch 2133, Loss: 0.06873565167188644, Final Batch Loss: 0.030741527676582336\n",
      "Epoch 2134, Loss: 0.03817029483616352, Final Batch Loss: 0.019789569079875946\n",
      "Epoch 2135, Loss: 0.12445785105228424, Final Batch Loss: 0.05213727056980133\n",
      "Epoch 2136, Loss: 0.10562817007303238, Final Batch Loss: 0.06168096512556076\n",
      "Epoch 2137, Loss: 0.06755460798740387, Final Batch Loss: 0.03151647001504898\n",
      "Epoch 2138, Loss: 0.10822875425219536, Final Batch Loss: 0.04980549216270447\n",
      "Epoch 2139, Loss: 0.09215853735804558, Final Batch Loss: 0.04173438251018524\n",
      "Epoch 2140, Loss: 0.10931208357214928, Final Batch Loss: 0.04304834082722664\n",
      "Epoch 2141, Loss: 0.13961035758256912, Final Batch Loss: 0.07950855046510696\n",
      "Epoch 2142, Loss: 0.06130327843129635, Final Batch Loss: 0.03312612697482109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2143, Loss: 0.12746510282158852, Final Batch Loss: 0.09628515690565109\n",
      "Epoch 2144, Loss: 0.05992146208882332, Final Batch Loss: 0.013733934611082077\n",
      "Epoch 2145, Loss: 0.08696306124329567, Final Batch Loss: 0.06006278097629547\n",
      "Epoch 2146, Loss: 0.07179034315049648, Final Batch Loss: 0.027044573798775673\n",
      "Epoch 2147, Loss: 0.09533179551362991, Final Batch Loss: 0.029806606471538544\n",
      "Epoch 2148, Loss: 0.10187486186623573, Final Batch Loss: 0.043281398713588715\n",
      "Epoch 2149, Loss: 0.07575587555766106, Final Batch Loss: 0.03856102004647255\n",
      "Epoch 2150, Loss: 0.08693278767168522, Final Batch Loss: 0.06762436032295227\n",
      "Epoch 2151, Loss: 0.08957809396088123, Final Batch Loss: 0.06632513552904129\n",
      "Epoch 2152, Loss: 0.04999542608857155, Final Batch Loss: 0.01884530484676361\n",
      "Epoch 2153, Loss: 0.06877049058675766, Final Batch Loss: 0.016183797270059586\n",
      "Epoch 2154, Loss: 0.064600576646626, Final Batch Loss: 0.013977658934891224\n",
      "Epoch 2155, Loss: 0.05138945672661066, Final Batch Loss: 0.012710058130323887\n",
      "Epoch 2156, Loss: 0.052074626088142395, Final Batch Loss: 0.019056402146816254\n",
      "Epoch 2157, Loss: 0.06874740496277809, Final Batch Loss: 0.034989189356565475\n",
      "Epoch 2158, Loss: 0.1055959165096283, Final Batch Loss: 0.051798805594444275\n",
      "Epoch 2159, Loss: 0.04947885498404503, Final Batch Loss: 0.02791718766093254\n",
      "Epoch 2160, Loss: 0.06368330493569374, Final Batch Loss: 0.027157433331012726\n",
      "Epoch 2161, Loss: 0.056150530464947224, Final Batch Loss: 0.04436596855521202\n",
      "Epoch 2162, Loss: 0.07072741910815239, Final Batch Loss: 0.04179082810878754\n",
      "Epoch 2163, Loss: 0.05084572732448578, Final Batch Loss: 0.024760378524661064\n",
      "Epoch 2164, Loss: 0.083576999604702, Final Batch Loss: 0.04018493741750717\n",
      "Epoch 2165, Loss: 0.05150766670703888, Final Batch Loss: 0.021467827260494232\n",
      "Epoch 2166, Loss: 0.06606321223080158, Final Batch Loss: 0.028908168897032738\n",
      "Epoch 2167, Loss: 0.10116607695817947, Final Batch Loss: 0.0623583123087883\n",
      "Epoch 2168, Loss: 0.08145845867693424, Final Batch Loss: 0.06308702379465103\n",
      "Epoch 2169, Loss: 0.07370324805378914, Final Batch Loss: 0.03181334584951401\n",
      "Epoch 2170, Loss: 0.06295996904373169, Final Batch Loss: 0.029817048460245132\n",
      "Epoch 2171, Loss: 0.07423080876469612, Final Batch Loss: 0.04225628823041916\n",
      "Epoch 2172, Loss: 0.08015730138868093, Final Batch Loss: 0.015004015527665615\n",
      "Epoch 2173, Loss: 0.08862470090389252, Final Batch Loss: 0.027094144374132156\n",
      "Epoch 2174, Loss: 0.06548295728862286, Final Batch Loss: 0.04521215334534645\n",
      "Epoch 2175, Loss: 0.06876976788043976, Final Batch Loss: 0.04343366250395775\n",
      "Epoch 2176, Loss: 0.08906574174761772, Final Batch Loss: 0.03989562764763832\n",
      "Epoch 2177, Loss: 0.10448883473873138, Final Batch Loss: 0.0728859081864357\n",
      "Epoch 2178, Loss: 0.04796904884278774, Final Batch Loss: 0.00934077613055706\n",
      "Epoch 2179, Loss: 0.06416951678693295, Final Batch Loss: 0.0368349514901638\n",
      "Epoch 2180, Loss: 0.06909028626978397, Final Batch Loss: 0.02051887847483158\n",
      "Epoch 2181, Loss: 0.06487389653921127, Final Batch Loss: 0.017131533473730087\n",
      "Epoch 2182, Loss: 0.05051952041685581, Final Batch Loss: 0.033442266285419464\n",
      "Epoch 2183, Loss: 0.031751141883432865, Final Batch Loss: 0.021297739818692207\n",
      "Epoch 2184, Loss: 0.05969374254345894, Final Batch Loss: 0.033800043165683746\n",
      "Epoch 2185, Loss: 0.08040428534150124, Final Batch Loss: 0.0510561428964138\n",
      "Epoch 2186, Loss: 0.08148618787527084, Final Batch Loss: 0.04878614842891693\n",
      "Epoch 2187, Loss: 0.04255556873977184, Final Batch Loss: 0.027243733406066895\n",
      "Epoch 2188, Loss: 0.07604697719216347, Final Batch Loss: 0.04343244805932045\n",
      "Epoch 2189, Loss: 0.07817194424569607, Final Batch Loss: 0.063716821372509\n",
      "Epoch 2190, Loss: 0.053181352093815804, Final Batch Loss: 0.021937955170869827\n",
      "Epoch 2191, Loss: 0.05640900880098343, Final Batch Loss: 0.03007558546960354\n",
      "Epoch 2192, Loss: 0.05189463682472706, Final Batch Loss: 0.01738837920129299\n",
      "Epoch 2193, Loss: 0.03263523243367672, Final Batch Loss: 0.012743482366204262\n",
      "Epoch 2194, Loss: 0.10153613612055779, Final Batch Loss: 0.060803063213825226\n",
      "Epoch 2195, Loss: 0.13737081736326218, Final Batch Loss: 0.07352976500988007\n",
      "Epoch 2196, Loss: 0.07116146385669708, Final Batch Loss: 0.02625884860754013\n",
      "Epoch 2197, Loss: 0.07324709556996822, Final Batch Loss: 0.02661275677382946\n",
      "Epoch 2198, Loss: 0.07029530126601458, Final Batch Loss: 0.01495517510920763\n",
      "Epoch 2199, Loss: 0.07907675206661224, Final Batch Loss: 0.03392210975289345\n",
      "Epoch 2200, Loss: 0.05528370663523674, Final Batch Loss: 0.020030703395605087\n",
      "Epoch 2201, Loss: 0.09575251303613186, Final Batch Loss: 0.022084636613726616\n",
      "Epoch 2202, Loss: 0.056227609515190125, Final Batch Loss: 0.02850605733692646\n",
      "Epoch 2203, Loss: 0.05564500764012337, Final Batch Loss: 0.02094210311770439\n",
      "Epoch 2204, Loss: 0.05832687392830849, Final Batch Loss: 0.03599165752530098\n",
      "Epoch 2205, Loss: 0.0703912302851677, Final Batch Loss: 0.03156702592968941\n",
      "Epoch 2206, Loss: 0.06907903775572777, Final Batch Loss: 0.03276485204696655\n",
      "Epoch 2207, Loss: 0.09545009955763817, Final Batch Loss: 0.035825058817863464\n",
      "Epoch 2208, Loss: 0.06223645433783531, Final Batch Loss: 0.0357196144759655\n",
      "Epoch 2209, Loss: 0.030939755029976368, Final Batch Loss: 0.013298333622515202\n",
      "Epoch 2210, Loss: 0.08770853653550148, Final Batch Loss: 0.04265392944216728\n",
      "Epoch 2211, Loss: 0.06265627779066563, Final Batch Loss: 0.03205404430627823\n",
      "Epoch 2212, Loss: 0.03612720128148794, Final Batch Loss: 0.023227788507938385\n",
      "Epoch 2213, Loss: 0.049914758652448654, Final Batch Loss: 0.032857511192560196\n",
      "Epoch 2214, Loss: 0.07753377966582775, Final Batch Loss: 0.048824965953826904\n",
      "Epoch 2215, Loss: 0.056661175563931465, Final Batch Loss: 0.03031509928405285\n",
      "Epoch 2216, Loss: 0.05010652914643288, Final Batch Loss: 0.019508207216858864\n",
      "Epoch 2217, Loss: 0.0999475046992302, Final Batch Loss: 0.04072700813412666\n",
      "Epoch 2218, Loss: 0.0385807016864419, Final Batch Loss: 0.013586821965873241\n",
      "Epoch 2219, Loss: 0.045103272423148155, Final Batch Loss: 0.01945441961288452\n",
      "Epoch 2220, Loss: 0.10903710871934891, Final Batch Loss: 0.06074318289756775\n",
      "Epoch 2221, Loss: 0.06820761784911156, Final Batch Loss: 0.02056356519460678\n",
      "Epoch 2222, Loss: 0.10451290756464005, Final Batch Loss: 0.04056021571159363\n",
      "Epoch 2223, Loss: 0.08072427846491337, Final Batch Loss: 0.023249005898833275\n",
      "Epoch 2224, Loss: 0.12101005017757416, Final Batch Loss: 0.0560721680521965\n",
      "Epoch 2225, Loss: 0.12291444092988968, Final Batch Loss: 0.06394548714160919\n",
      "Epoch 2226, Loss: 0.07130173966288567, Final Batch Loss: 0.03868620842695236\n",
      "Epoch 2227, Loss: 0.057938480749726295, Final Batch Loss: 0.02449202351272106\n",
      "Epoch 2228, Loss: 0.10452578961849213, Final Batch Loss: 0.06411801278591156\n",
      "Epoch 2229, Loss: 0.07103562168776989, Final Batch Loss: 0.05335276201367378\n",
      "Epoch 2230, Loss: 0.06714772060513496, Final Batch Loss: 0.02616260200738907\n",
      "Epoch 2231, Loss: 0.05796176195144653, Final Batch Loss: 0.02243947610259056\n",
      "Epoch 2232, Loss: 0.10132699273526669, Final Batch Loss: 0.023299334570765495\n",
      "Epoch 2233, Loss: 0.09680168330669403, Final Batch Loss: 0.062202997505664825\n",
      "Epoch 2234, Loss: 0.11672495678067207, Final Batch Loss: 0.05304731801152229\n",
      "Epoch 2235, Loss: 0.06431100331246853, Final Batch Loss: 0.01596871204674244\n",
      "Epoch 2236, Loss: 0.10532871261239052, Final Batch Loss: 0.05430346354842186\n",
      "Epoch 2237, Loss: 0.02967723459005356, Final Batch Loss: 0.019604546949267387\n",
      "Epoch 2238, Loss: 0.14746057242155075, Final Batch Loss: 0.07010025531053543\n",
      "Epoch 2239, Loss: 0.06403112411499023, Final Batch Loss: 0.040189217776060104\n",
      "Epoch 2240, Loss: 0.06529481243342161, Final Batch Loss: 0.050049591809511185\n",
      "Epoch 2241, Loss: 0.07691898941993713, Final Batch Loss: 0.05495699122548103\n",
      "Epoch 2242, Loss: 0.0759945958852768, Final Batch Loss: 0.046486254781484604\n",
      "Epoch 2243, Loss: 0.07358293049037457, Final Batch Loss: 0.02381707914173603\n",
      "Epoch 2244, Loss: 0.12245558202266693, Final Batch Loss: 0.054876066744327545\n",
      "Epoch 2245, Loss: 0.08883953094482422, Final Batch Loss: 0.015824854373931885\n",
      "Epoch 2246, Loss: 0.06879406422376633, Final Batch Loss: 0.0319533497095108\n",
      "Epoch 2247, Loss: 0.0748511552810669, Final Batch Loss: 0.03031662106513977\n",
      "Epoch 2248, Loss: 0.08203592896461487, Final Batch Loss: 0.03820737451314926\n",
      "Epoch 2249, Loss: 0.0858701728284359, Final Batch Loss: 0.05003044754266739\n",
      "Epoch 2250, Loss: 0.15665802359580994, Final Batch Loss: 0.09304030239582062\n",
      "Epoch 2251, Loss: 0.15638486295938492, Final Batch Loss: 0.10605466365814209\n",
      "Epoch 2252, Loss: 0.10762402042746544, Final Batch Loss: 0.035101961344480515\n",
      "Epoch 2253, Loss: 0.047823915258049965, Final Batch Loss: 0.023512210696935654\n",
      "Epoch 2254, Loss: 0.1602395549416542, Final Batch Loss: 0.09849479794502258\n",
      "Epoch 2255, Loss: 0.08619211614131927, Final Batch Loss: 0.046650830656290054\n",
      "Epoch 2256, Loss: 0.06959903240203857, Final Batch Loss: 0.02783110737800598\n",
      "Epoch 2257, Loss: 0.10888282582163811, Final Batch Loss: 0.06995028257369995\n",
      "Epoch 2258, Loss: 0.07201672717928886, Final Batch Loss: 0.02981553226709366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2259, Loss: 0.043957823887467384, Final Batch Loss: 0.027180761098861694\n",
      "Epoch 2260, Loss: 0.11641308665275574, Final Batch Loss: 0.07478011399507523\n",
      "Epoch 2261, Loss: 0.08021536841988564, Final Batch Loss: 0.03258657827973366\n",
      "Epoch 2262, Loss: 0.06095271185040474, Final Batch Loss: 0.02988891303539276\n",
      "Epoch 2263, Loss: 0.07914786413311958, Final Batch Loss: 0.046425703912973404\n",
      "Epoch 2264, Loss: 0.11444230750203133, Final Batch Loss: 0.04100150242447853\n",
      "Epoch 2265, Loss: 0.10167941823601723, Final Batch Loss: 0.0677817240357399\n",
      "Epoch 2266, Loss: 0.06507448572665453, Final Batch Loss: 0.054864682257175446\n",
      "Epoch 2267, Loss: 0.07496530562639236, Final Batch Loss: 0.038807038217782974\n",
      "Epoch 2268, Loss: 0.047065747901797295, Final Batch Loss: 0.015118187293410301\n",
      "Epoch 2269, Loss: 0.10929687134921551, Final Batch Loss: 0.08234701305627823\n",
      "Epoch 2270, Loss: 0.05133326165378094, Final Batch Loss: 0.029529623687267303\n",
      "Epoch 2271, Loss: 0.03949936665594578, Final Batch Loss: 0.021504443138837814\n",
      "Epoch 2272, Loss: 0.07556017395108938, Final Batch Loss: 0.015299386344850063\n",
      "Epoch 2273, Loss: 0.07847650721669197, Final Batch Loss: 0.04320386052131653\n",
      "Epoch 2274, Loss: 0.07666705176234245, Final Batch Loss: 0.04017587751150131\n",
      "Epoch 2275, Loss: 0.0911860503256321, Final Batch Loss: 0.036281537264585495\n",
      "Epoch 2276, Loss: 0.04746449179947376, Final Batch Loss: 0.017389506101608276\n",
      "Epoch 2277, Loss: 0.06118121184408665, Final Batch Loss: 0.02672065980732441\n",
      "Epoch 2278, Loss: 0.07324006129056215, Final Batch Loss: 0.014860839582979679\n",
      "Epoch 2279, Loss: 0.08257918059825897, Final Batch Loss: 0.05116250738501549\n",
      "Epoch 2280, Loss: 0.08554500713944435, Final Batch Loss: 0.04624616354703903\n",
      "Epoch 2281, Loss: 0.08583837747573853, Final Batch Loss: 0.031234566122293472\n",
      "Epoch 2282, Loss: 0.08567424863576889, Final Batch Loss: 0.037130918353796005\n",
      "Epoch 2283, Loss: 0.052952470257878304, Final Batch Loss: 0.02760922722518444\n",
      "Epoch 2284, Loss: 0.087825458496809, Final Batch Loss: 0.035605836659669876\n",
      "Epoch 2285, Loss: 0.06733008846640587, Final Batch Loss: 0.03465409204363823\n",
      "Epoch 2286, Loss: 0.12358271330595016, Final Batch Loss: 0.08595314621925354\n",
      "Epoch 2287, Loss: 0.1393166184425354, Final Batch Loss: 0.02569267898797989\n",
      "Epoch 2288, Loss: 0.08684538304805756, Final Batch Loss: 0.03565358743071556\n",
      "Epoch 2289, Loss: 0.058532826602458954, Final Batch Loss: 0.019813332706689835\n",
      "Epoch 2290, Loss: 0.08615998551249504, Final Batch Loss: 0.03611322492361069\n",
      "Epoch 2291, Loss: 0.07502397149801254, Final Batch Loss: 0.06061645969748497\n",
      "Epoch 2292, Loss: 0.08856216073036194, Final Batch Loss: 0.05164032801985741\n",
      "Epoch 2293, Loss: 0.056527236476540565, Final Batch Loss: 0.028281427919864655\n",
      "Epoch 2294, Loss: 0.14189540594816208, Final Batch Loss: 0.06447651982307434\n",
      "Epoch 2295, Loss: 0.08670718222856522, Final Batch Loss: 0.054419685155153275\n",
      "Epoch 2296, Loss: 0.10952683165669441, Final Batch Loss: 0.04067240282893181\n",
      "Epoch 2297, Loss: 0.06915524788200855, Final Batch Loss: 0.029903171584010124\n",
      "Epoch 2298, Loss: 0.10856457054615021, Final Batch Loss: 0.07311809062957764\n",
      "Epoch 2299, Loss: 0.07611599192023277, Final Batch Loss: 0.02178378775715828\n",
      "Epoch 2300, Loss: 0.07193895429372787, Final Batch Loss: 0.0542871356010437\n",
      "Epoch 2301, Loss: 0.08342666178941727, Final Batch Loss: 0.05246633291244507\n",
      "Epoch 2302, Loss: 0.11143334582448006, Final Batch Loss: 0.07373666763305664\n",
      "Epoch 2303, Loss: 0.0973742213100195, Final Batch Loss: 0.06988339871168137\n",
      "Epoch 2304, Loss: 0.041862365789711475, Final Batch Loss: 0.012220467440783978\n",
      "Epoch 2305, Loss: 0.06698711402714252, Final Batch Loss: 0.020151382312178612\n",
      "Epoch 2306, Loss: 0.07758113369345665, Final Batch Loss: 0.0329841673374176\n",
      "Epoch 2307, Loss: 0.08289637789130211, Final Batch Loss: 0.04137639328837395\n",
      "Epoch 2308, Loss: 0.053505998104810715, Final Batch Loss: 0.01729676127433777\n",
      "Epoch 2309, Loss: 0.07037387415766716, Final Batch Loss: 0.035811591893434525\n",
      "Epoch 2310, Loss: 0.0984058752655983, Final Batch Loss: 0.058859098702669144\n",
      "Epoch 2311, Loss: 0.09402202814817429, Final Batch Loss: 0.05388732999563217\n",
      "Epoch 2312, Loss: 0.06562735140323639, Final Batch Loss: 0.0367082841694355\n",
      "Epoch 2313, Loss: 0.09057813137769699, Final Batch Loss: 0.06933364272117615\n",
      "Epoch 2314, Loss: 0.07003456726670265, Final Batch Loss: 0.03391551971435547\n",
      "Epoch 2315, Loss: 0.05932023748755455, Final Batch Loss: 0.023591376841068268\n",
      "Epoch 2316, Loss: 0.051769960671663284, Final Batch Loss: 0.023253541439771652\n",
      "Epoch 2317, Loss: 0.0790863074362278, Final Batch Loss: 0.04540960118174553\n",
      "Epoch 2318, Loss: 0.07857899367809296, Final Batch Loss: 0.05906769260764122\n",
      "Epoch 2319, Loss: 0.08728016912937164, Final Batch Loss: 0.06118431314826012\n",
      "Epoch 2320, Loss: 0.12267031148076057, Final Batch Loss: 0.06584262102842331\n",
      "Epoch 2321, Loss: 0.09594126604497433, Final Batch Loss: 0.06549272686243057\n",
      "Epoch 2322, Loss: 0.07959333620965481, Final Batch Loss: 0.02459176816046238\n",
      "Epoch 2323, Loss: 0.04419948719441891, Final Batch Loss: 0.020242685452103615\n",
      "Epoch 2324, Loss: 0.059072475880384445, Final Batch Loss: 0.026771750301122665\n",
      "Epoch 2325, Loss: 0.08542586676776409, Final Batch Loss: 0.056636638939380646\n",
      "Epoch 2326, Loss: 0.06600289791822433, Final Batch Loss: 0.017781801521778107\n",
      "Epoch 2327, Loss: 0.12421930581331253, Final Batch Loss: 0.08250943571329117\n",
      "Epoch 2328, Loss: 0.06781109608709812, Final Batch Loss: 0.02040877379477024\n",
      "Epoch 2329, Loss: 0.0661903340369463, Final Batch Loss: 0.03773249685764313\n",
      "Epoch 2330, Loss: 0.054293785244226456, Final Batch Loss: 0.033830538392066956\n",
      "Epoch 2331, Loss: 0.0435489434748888, Final Batch Loss: 0.027588212862610817\n",
      "Epoch 2332, Loss: 0.06368988193571568, Final Batch Loss: 0.0232535507529974\n",
      "Epoch 2333, Loss: 0.06007703207433224, Final Batch Loss: 0.026732100173830986\n",
      "Epoch 2334, Loss: 0.04500221461057663, Final Batch Loss: 0.01752280443906784\n",
      "Epoch 2335, Loss: 0.05272566806524992, Final Batch Loss: 0.041868843138217926\n",
      "Epoch 2336, Loss: 0.047236790880560875, Final Batch Loss: 0.03202785551548004\n",
      "Epoch 2337, Loss: 0.035036404617130756, Final Batch Loss: 0.023063166067004204\n",
      "Epoch 2338, Loss: 0.06192920543253422, Final Batch Loss: 0.03672155365347862\n",
      "Epoch 2339, Loss: 0.08581604063510895, Final Batch Loss: 0.047742344439029694\n",
      "Epoch 2340, Loss: 0.07429801672697067, Final Batch Loss: 0.03654346242547035\n",
      "Epoch 2341, Loss: 0.1318073347210884, Final Batch Loss: 0.032513104379177094\n",
      "Epoch 2342, Loss: 0.04876989126205444, Final Batch Loss: 0.020604895427823067\n",
      "Epoch 2343, Loss: 0.07304461114108562, Final Batch Loss: 0.031158236786723137\n",
      "Epoch 2344, Loss: 0.0877868477255106, Final Batch Loss: 0.031165065243840218\n",
      "Epoch 2345, Loss: 0.08380940183997154, Final Batch Loss: 0.032939448952674866\n",
      "Epoch 2346, Loss: 0.08917718566954136, Final Batch Loss: 0.06117752566933632\n",
      "Epoch 2347, Loss: 0.07734232023358345, Final Batch Loss: 0.03423559293150902\n",
      "Epoch 2348, Loss: 0.05739462003111839, Final Batch Loss: 0.0249188132584095\n",
      "Epoch 2349, Loss: 0.05314385611563921, Final Batch Loss: 0.015054346062242985\n",
      "Epoch 2350, Loss: 0.12255494482815266, Final Batch Loss: 0.028483333066105843\n",
      "Epoch 2351, Loss: 0.03826825972646475, Final Batch Loss: 0.013245726935565472\n",
      "Epoch 2352, Loss: 0.10301514342427254, Final Batch Loss: 0.05876268818974495\n",
      "Epoch 2353, Loss: 0.06565569248050451, Final Batch Loss: 0.011223292909562588\n",
      "Epoch 2354, Loss: 0.070154232904315, Final Batch Loss: 0.04514783248305321\n",
      "Epoch 2355, Loss: 0.06954218726605177, Final Batch Loss: 0.014328244142234325\n",
      "Epoch 2356, Loss: 0.046119590289890766, Final Batch Loss: 0.012122892774641514\n",
      "Epoch 2357, Loss: 0.06349239684641361, Final Batch Loss: 0.027080832049250603\n",
      "Epoch 2358, Loss: 0.08075067773461342, Final Batch Loss: 0.0469980463385582\n",
      "Epoch 2359, Loss: 0.06079147942364216, Final Batch Loss: 0.032563433051109314\n",
      "Epoch 2360, Loss: 0.06993110291659832, Final Batch Loss: 0.04162362962961197\n",
      "Epoch 2361, Loss: 0.049800435081124306, Final Batch Loss: 0.019123448058962822\n",
      "Epoch 2362, Loss: 0.048418477177619934, Final Batch Loss: 0.018537182360887527\n",
      "Epoch 2363, Loss: 0.03435192909091711, Final Batch Loss: 0.009291213937103748\n",
      "Epoch 2364, Loss: 0.05254935100674629, Final Batch Loss: 0.02840934507548809\n",
      "Epoch 2365, Loss: 0.03998603671789169, Final Batch Loss: 0.02557704970240593\n",
      "Epoch 2366, Loss: 0.05029494408518076, Final Batch Loss: 0.014931918121874332\n",
      "Epoch 2367, Loss: 0.06970741041004658, Final Batch Loss: 0.05901797488331795\n",
      "Epoch 2368, Loss: 0.03622444812208414, Final Batch Loss: 0.023067384958267212\n",
      "Epoch 2369, Loss: 0.0439878199249506, Final Batch Loss: 0.018162859603762627\n",
      "Epoch 2370, Loss: 0.08768044784665108, Final Batch Loss: 0.03174054995179176\n",
      "Epoch 2371, Loss: 0.08752524852752686, Final Batch Loss: 0.04885585233569145\n",
      "Epoch 2372, Loss: 0.08949073404073715, Final Batch Loss: 0.05494995042681694\n",
      "Epoch 2373, Loss: 0.10551381576806307, Final Batch Loss: 0.004917849786579609\n",
      "Epoch 2374, Loss: 0.11169658601284027, Final Batch Loss: 0.08294514566659927\n",
      "Epoch 2375, Loss: 0.07884916290640831, Final Batch Loss: 0.025475043803453445\n",
      "Epoch 2376, Loss: 0.09168952144682407, Final Batch Loss: 0.01863240636885166\n",
      "Epoch 2377, Loss: 0.048006923869252205, Final Batch Loss: 0.02997557446360588\n",
      "Epoch 2378, Loss: 0.10184992477297783, Final Batch Loss: 0.049154773354530334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2379, Loss: 0.06402379274368286, Final Batch Loss: 0.04065718874335289\n",
      "Epoch 2380, Loss: 0.0918229129165411, Final Batch Loss: 0.06399399042129517\n",
      "Epoch 2381, Loss: 0.054925587028265, Final Batch Loss: 0.025852849707007408\n",
      "Epoch 2382, Loss: 0.05469980277121067, Final Batch Loss: 0.030056634917855263\n",
      "Epoch 2383, Loss: 0.09936561435461044, Final Batch Loss: 0.06293139606714249\n",
      "Epoch 2384, Loss: 0.0600504856556654, Final Batch Loss: 0.017166225239634514\n",
      "Epoch 2385, Loss: 0.06633072346448898, Final Batch Loss: 0.05051805078983307\n",
      "Epoch 2386, Loss: 0.058730125427246094, Final Batch Loss: 0.03468989208340645\n",
      "Epoch 2387, Loss: 0.045537302270531654, Final Batch Loss: 0.025157194584608078\n",
      "Epoch 2388, Loss: 0.08579754550009966, Final Batch Loss: 0.07594044506549835\n",
      "Epoch 2389, Loss: 0.07961869612336159, Final Batch Loss: 0.04777951166033745\n",
      "Epoch 2390, Loss: 0.046220969408750534, Final Batch Loss: 0.01707940548658371\n",
      "Epoch 2391, Loss: 0.1073000580072403, Final Batch Loss: 0.026134200394153595\n",
      "Epoch 2392, Loss: 0.041485766880214214, Final Batch Loss: 0.028199583292007446\n",
      "Epoch 2393, Loss: 0.09861923567950726, Final Batch Loss: 0.06993506848812103\n",
      "Epoch 2394, Loss: 0.03948019631206989, Final Batch Loss: 0.013440819457173347\n",
      "Epoch 2395, Loss: 0.07931103557348251, Final Batch Loss: 0.05198405683040619\n",
      "Epoch 2396, Loss: 0.09112300351262093, Final Batch Loss: 0.060236044228076935\n",
      "Epoch 2397, Loss: 0.08862848207354546, Final Batch Loss: 0.048687368631362915\n",
      "Epoch 2398, Loss: 0.06490038521587849, Final Batch Loss: 0.024890730157494545\n",
      "Epoch 2399, Loss: 0.08310721069574356, Final Batch Loss: 0.050606027245521545\n",
      "Epoch 2400, Loss: 0.1001841276884079, Final Batch Loss: 0.046281564980745316\n",
      "Epoch 2401, Loss: 0.04364674724638462, Final Batch Loss: 0.01682407781481743\n",
      "Epoch 2402, Loss: 0.05314992181956768, Final Batch Loss: 0.029416289180517197\n",
      "Epoch 2403, Loss: 0.05416365526616573, Final Batch Loss: 0.028566233813762665\n",
      "Epoch 2404, Loss: 0.04994131624698639, Final Batch Loss: 0.01718795672059059\n",
      "Epoch 2405, Loss: 0.0447034053504467, Final Batch Loss: 0.02476929873228073\n",
      "Epoch 2406, Loss: 0.05744601599872112, Final Batch Loss: 0.023436663672327995\n",
      "Epoch 2407, Loss: 0.05142191797494888, Final Batch Loss: 0.03022165410220623\n",
      "Epoch 2408, Loss: 0.03714658319950104, Final Batch Loss: 0.021292321383953094\n",
      "Epoch 2409, Loss: 0.05468008480966091, Final Batch Loss: 0.03824661672115326\n",
      "Epoch 2410, Loss: 0.04522588290274143, Final Batch Loss: 0.02443060837686062\n",
      "Epoch 2411, Loss: 0.05059709865599871, Final Batch Loss: 0.012818695046007633\n",
      "Epoch 2412, Loss: 0.04012119583785534, Final Batch Loss: 0.028925469145178795\n",
      "Epoch 2413, Loss: 0.1250162571668625, Final Batch Loss: 0.02333686500787735\n",
      "Epoch 2414, Loss: 0.041813457384705544, Final Batch Loss: 0.021628491580486298\n",
      "Epoch 2415, Loss: 0.07161217741668224, Final Batch Loss: 0.029143737629055977\n",
      "Epoch 2416, Loss: 0.05370248295366764, Final Batch Loss: 0.02938167192041874\n",
      "Epoch 2417, Loss: 0.06820954196155071, Final Batch Loss: 0.04812467843294144\n",
      "Epoch 2418, Loss: 0.09941678494215012, Final Batch Loss: 0.046460580080747604\n",
      "Epoch 2419, Loss: 0.0553400032222271, Final Batch Loss: 0.029255717992782593\n",
      "Epoch 2420, Loss: 0.05950433760881424, Final Batch Loss: 0.016296595335006714\n",
      "Epoch 2421, Loss: 0.09098345413804054, Final Batch Loss: 0.04977086931467056\n",
      "Epoch 2422, Loss: 0.0570123391225934, Final Batch Loss: 0.008800202049314976\n",
      "Epoch 2423, Loss: 0.053411949425935745, Final Batch Loss: 0.02554941177368164\n",
      "Epoch 2424, Loss: 0.040545081719756126, Final Batch Loss: 0.013217851519584656\n",
      "Epoch 2425, Loss: 0.03953360393643379, Final Batch Loss: 0.023159097880125046\n",
      "Epoch 2426, Loss: 0.05525974556803703, Final Batch Loss: 0.013196337968111038\n",
      "Epoch 2427, Loss: 0.050628168508410454, Final Batch Loss: 0.02111002430319786\n",
      "Epoch 2428, Loss: 0.06769641675055027, Final Batch Loss: 0.0302788857370615\n",
      "Epoch 2429, Loss: 0.07153443992137909, Final Batch Loss: 0.02771463245153427\n",
      "Epoch 2430, Loss: 0.0697406753897667, Final Batch Loss: 0.033663779497146606\n",
      "Epoch 2431, Loss: 0.02591172605752945, Final Batch Loss: 0.006604574620723724\n",
      "Epoch 2432, Loss: 0.04735609423369169, Final Batch Loss: 0.011139514856040478\n",
      "Epoch 2433, Loss: 0.10903574526309967, Final Batch Loss: 0.07489608228206635\n",
      "Epoch 2434, Loss: 0.08294857665896416, Final Batch Loss: 0.021652478724718094\n",
      "Epoch 2435, Loss: 0.060998765751719475, Final Batch Loss: 0.01966329850256443\n",
      "Epoch 2436, Loss: 0.04044052213430405, Final Batch Loss: 0.018188007175922394\n",
      "Epoch 2437, Loss: 0.029540548101067543, Final Batch Loss: 0.014688014052808285\n",
      "Epoch 2438, Loss: 0.06715649552643299, Final Batch Loss: 0.03759361803531647\n",
      "Epoch 2439, Loss: 0.046648887917399406, Final Batch Loss: 0.027927661314606667\n",
      "Epoch 2440, Loss: 0.12330127134919167, Final Batch Loss: 0.04370192065834999\n",
      "Epoch 2441, Loss: 0.05124006234109402, Final Batch Loss: 0.031816091388463974\n",
      "Epoch 2442, Loss: 0.05962744913995266, Final Batch Loss: 0.021502329036593437\n",
      "Epoch 2443, Loss: 0.03675086423754692, Final Batch Loss: 0.018489142879843712\n",
      "Epoch 2444, Loss: 0.06050208583474159, Final Batch Loss: 0.022755544632673264\n",
      "Epoch 2445, Loss: 0.03949825093150139, Final Batch Loss: 0.01590251922607422\n",
      "Epoch 2446, Loss: 0.06436123140156269, Final Batch Loss: 0.028529537841677666\n",
      "Epoch 2447, Loss: 0.09322347864508629, Final Batch Loss: 0.04763084277510643\n",
      "Epoch 2448, Loss: 0.08061624318361282, Final Batch Loss: 0.03636413812637329\n",
      "Epoch 2449, Loss: 0.04943604674190283, Final Batch Loss: 0.03565420210361481\n",
      "Epoch 2450, Loss: 0.09097344428300858, Final Batch Loss: 0.06288024038076401\n",
      "Epoch 2451, Loss: 0.06375714763998985, Final Batch Loss: 0.013917099684476852\n",
      "Epoch 2452, Loss: 0.05593792907893658, Final Batch Loss: 0.0286591574549675\n",
      "Epoch 2453, Loss: 0.09765420481562614, Final Batch Loss: 0.05009358003735542\n",
      "Epoch 2454, Loss: 0.030174347572028637, Final Batch Loss: 0.016938846558332443\n",
      "Epoch 2455, Loss: 0.044877585023641586, Final Batch Loss: 0.028562381863594055\n",
      "Epoch 2456, Loss: 0.05448310635983944, Final Batch Loss: 0.016489287838339806\n",
      "Epoch 2457, Loss: 0.06279217824339867, Final Batch Loss: 0.03456155210733414\n",
      "Epoch 2458, Loss: 0.042598916217684746, Final Batch Loss: 0.01333560049533844\n",
      "Epoch 2459, Loss: 0.059657298028469086, Final Batch Loss: 0.04373270645737648\n",
      "Epoch 2460, Loss: 0.06732378713786602, Final Batch Loss: 0.02974766306579113\n",
      "Epoch 2461, Loss: 0.04844069294631481, Final Batch Loss: 0.02188444882631302\n",
      "Epoch 2462, Loss: 0.06330410204827785, Final Batch Loss: 0.034054405987262726\n",
      "Epoch 2463, Loss: 0.06732767634093761, Final Batch Loss: 0.045856185257434845\n",
      "Epoch 2464, Loss: 0.08376756496727467, Final Batch Loss: 0.05471275746822357\n",
      "Epoch 2465, Loss: 0.023768597282469273, Final Batch Loss: 0.011257890611886978\n",
      "Epoch 2466, Loss: 0.10800431668758392, Final Batch Loss: 0.048038922250270844\n",
      "Epoch 2467, Loss: 0.05765896290540695, Final Batch Loss: 0.03191623464226723\n",
      "Epoch 2468, Loss: 0.04282543156296015, Final Batch Loss: 0.015059939585626125\n",
      "Epoch 2469, Loss: 0.07795201614499092, Final Batch Loss: 0.04162682965397835\n",
      "Epoch 2470, Loss: 0.06241726689040661, Final Batch Loss: 0.009284639731049538\n",
      "Epoch 2471, Loss: 0.07261692360043526, Final Batch Loss: 0.016811709851026535\n",
      "Epoch 2472, Loss: 0.04251926299184561, Final Batch Loss: 0.029754679650068283\n",
      "Epoch 2473, Loss: 0.06882937625050545, Final Batch Loss: 0.0343647263944149\n",
      "Epoch 2474, Loss: 0.0506760198622942, Final Batch Loss: 0.03266940265893936\n",
      "Epoch 2475, Loss: 0.0678914301097393, Final Batch Loss: 0.03842533007264137\n",
      "Epoch 2476, Loss: 0.06970256194472313, Final Batch Loss: 0.013579711318016052\n",
      "Epoch 2477, Loss: 0.05013542249798775, Final Batch Loss: 0.02354017272591591\n",
      "Epoch 2478, Loss: 0.1819721907377243, Final Batch Loss: 0.12231538444757462\n",
      "Epoch 2479, Loss: 0.03841767366975546, Final Batch Loss: 0.00996317993849516\n",
      "Epoch 2480, Loss: 0.07754047214984894, Final Batch Loss: 0.055002350360155106\n",
      "Epoch 2481, Loss: 0.04431516770273447, Final Batch Loss: 0.008673840202391148\n",
      "Epoch 2482, Loss: 0.15376868471503258, Final Batch Loss: 0.10886748135089874\n",
      "Epoch 2483, Loss: 0.11206461116671562, Final Batch Loss: 0.06389696896076202\n",
      "Epoch 2484, Loss: 0.07432099059224129, Final Batch Loss: 0.039563246071338654\n",
      "Epoch 2485, Loss: 0.0831923820078373, Final Batch Loss: 0.04310251772403717\n",
      "Epoch 2486, Loss: 0.11111507564783096, Final Batch Loss: 0.039943285286426544\n",
      "Epoch 2487, Loss: 0.09310638345777988, Final Batch Loss: 0.02949311025440693\n",
      "Epoch 2488, Loss: 0.12343326583504677, Final Batch Loss: 0.039171744138002396\n",
      "Epoch 2489, Loss: 0.046432022005319595, Final Batch Loss: 0.017859315499663353\n",
      "Epoch 2490, Loss: 0.07087530568242073, Final Batch Loss: 0.038086745887994766\n",
      "Epoch 2491, Loss: 0.07332595810294151, Final Batch Loss: 0.02810698002576828\n",
      "Epoch 2492, Loss: 0.06273858807981014, Final Batch Loss: 0.02257719077169895\n",
      "Epoch 2493, Loss: 0.07400625385344028, Final Batch Loss: 0.04611685499548912\n",
      "Epoch 2494, Loss: 0.12473388388752937, Final Batch Loss: 0.06837114691734314\n",
      "Epoch 2495, Loss: 0.07410763949155807, Final Batch Loss: 0.028218738734722137\n",
      "Epoch 2496, Loss: 0.050555165857076645, Final Batch Loss: 0.02481020800769329\n",
      "Epoch 2497, Loss: 0.04994890745729208, Final Batch Loss: 0.01219907309859991\n",
      "Epoch 2498, Loss: 0.08189526945352554, Final Batch Loss: 0.03289907053112984\n",
      "Epoch 2499, Loss: 0.06547104939818382, Final Batch Loss: 0.02317175269126892\n",
      "Epoch 2500, Loss: 0.06787582859396935, Final Batch Loss: 0.03255414962768555\n",
      "Epoch 2501, Loss: 0.09384488873183727, Final Batch Loss: 0.06563746929168701\n",
      "Epoch 2502, Loss: 0.04519927967339754, Final Batch Loss: 0.030196188017725945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2503, Loss: 0.06356969475746155, Final Batch Loss: 0.02074739709496498\n",
      "Epoch 2504, Loss: 0.026113220490515232, Final Batch Loss: 0.013670527376234531\n",
      "Epoch 2505, Loss: 0.030540585052222013, Final Batch Loss: 0.022810781374573708\n",
      "Epoch 2506, Loss: 0.049030797090381384, Final Batch Loss: 0.0077707706950604916\n",
      "Epoch 2507, Loss: 0.05998854059726, Final Batch Loss: 0.009375532157719135\n",
      "Epoch 2508, Loss: 0.06751174479722977, Final Batch Loss: 0.046610038727521896\n",
      "Epoch 2509, Loss: 0.09250698797404766, Final Batch Loss: 0.028870901092886925\n",
      "Epoch 2510, Loss: 0.11424538493156433, Final Batch Loss: 0.07448665052652359\n",
      "Epoch 2511, Loss: 0.04798435978591442, Final Batch Loss: 0.013210443779826164\n",
      "Epoch 2512, Loss: 0.08251466602087021, Final Batch Loss: 0.046558912843465805\n",
      "Epoch 2513, Loss: 0.07871423661708832, Final Batch Loss: 0.040656138211488724\n",
      "Epoch 2514, Loss: 0.0956016294658184, Final Batch Loss: 0.0424727238714695\n",
      "Epoch 2515, Loss: 0.06854682043194771, Final Batch Loss: 0.02973927929997444\n",
      "Epoch 2516, Loss: 0.08743888139724731, Final Batch Loss: 0.05249540135264397\n",
      "Epoch 2517, Loss: 0.10216852650046349, Final Batch Loss: 0.06430119276046753\n",
      "Epoch 2518, Loss: 0.05725374072790146, Final Batch Loss: 0.02852015756070614\n",
      "Epoch 2519, Loss: 0.1171993650496006, Final Batch Loss: 0.07418063282966614\n",
      "Epoch 2520, Loss: 0.1188211627304554, Final Batch Loss: 0.07122990489006042\n",
      "Epoch 2521, Loss: 0.052273768931627274, Final Batch Loss: 0.008668292313814163\n",
      "Epoch 2522, Loss: 0.054656656458973885, Final Batch Loss: 0.017849409952759743\n",
      "Epoch 2523, Loss: 0.04788934253156185, Final Batch Loss: 0.0349808894097805\n",
      "Epoch 2524, Loss: 0.08055021986365318, Final Batch Loss: 0.0584324412047863\n",
      "Epoch 2525, Loss: 0.10005787387490273, Final Batch Loss: 0.0428655743598938\n",
      "Epoch 2526, Loss: 0.03665590472519398, Final Batch Loss: 0.01392439380288124\n",
      "Epoch 2527, Loss: 0.05146531015634537, Final Batch Loss: 0.016326405107975006\n",
      "Epoch 2528, Loss: 0.12323818728327751, Final Batch Loss: 0.08186735957860947\n",
      "Epoch 2529, Loss: 0.06266545504331589, Final Batch Loss: 0.021248366683721542\n",
      "Epoch 2530, Loss: 0.06255540065467358, Final Batch Loss: 0.021724360063672066\n",
      "Epoch 2531, Loss: 0.0533683467656374, Final Batch Loss: 0.023281913250684738\n",
      "Epoch 2532, Loss: 0.06109206937253475, Final Batch Loss: 0.044698432087898254\n",
      "Epoch 2533, Loss: 0.11099192500114441, Final Batch Loss: 0.07019444555044174\n",
      "Epoch 2534, Loss: 0.043871378526091576, Final Batch Loss: 0.023380273953080177\n",
      "Epoch 2535, Loss: 0.05248608998954296, Final Batch Loss: 0.026372646912932396\n",
      "Epoch 2536, Loss: 0.129758533090353, Final Batch Loss: 0.10034684091806412\n",
      "Epoch 2537, Loss: 0.057564444839954376, Final Batch Loss: 0.017252914607524872\n",
      "Epoch 2538, Loss: 0.08004836179316044, Final Batch Loss: 0.05030852556228638\n",
      "Epoch 2539, Loss: 0.0944131352007389, Final Batch Loss: 0.0523686520755291\n",
      "Epoch 2540, Loss: 0.15566803328692913, Final Batch Loss: 0.1353255808353424\n",
      "Epoch 2541, Loss: 0.044433631002902985, Final Batch Loss: 0.018170880153775215\n",
      "Epoch 2542, Loss: 0.03788098320364952, Final Batch Loss: 0.02333950437605381\n",
      "Epoch 2543, Loss: 0.07898617535829544, Final Batch Loss: 0.03160744532942772\n",
      "Epoch 2544, Loss: 0.04169454891234636, Final Batch Loss: 0.0321640819311142\n",
      "Epoch 2545, Loss: 0.04868850018829107, Final Batch Loss: 0.03541532903909683\n",
      "Epoch 2546, Loss: 0.0974174402654171, Final Batch Loss: 0.03542887791991234\n",
      "Epoch 2547, Loss: 0.06900000106543303, Final Batch Loss: 0.05664834380149841\n",
      "Epoch 2548, Loss: 0.1807631552219391, Final Batch Loss: 0.13400626182556152\n",
      "Epoch 2549, Loss: 0.09245394170284271, Final Batch Loss: 0.06547489017248154\n",
      "Epoch 2550, Loss: 0.0758329126983881, Final Batch Loss: 0.021355127915740013\n",
      "Epoch 2551, Loss: 0.07852644845843315, Final Batch Loss: 0.03332502767443657\n",
      "Epoch 2552, Loss: 0.07743322663009167, Final Batch Loss: 0.029923176392912865\n",
      "Epoch 2553, Loss: 0.08973772451281548, Final Batch Loss: 0.026816021651029587\n",
      "Epoch 2554, Loss: 0.09835650585591793, Final Batch Loss: 0.023081405088305473\n",
      "Epoch 2555, Loss: 0.04045416135340929, Final Batch Loss: 0.025706149637699127\n",
      "Epoch 2556, Loss: 0.05318445060402155, Final Batch Loss: 0.012894188053905964\n",
      "Epoch 2557, Loss: 0.030133637599647045, Final Batch Loss: 0.013422350399196148\n",
      "Epoch 2558, Loss: 0.07056946493685246, Final Batch Loss: 0.02320757322013378\n",
      "Epoch 2559, Loss: 0.0955321341753006, Final Batch Loss: 0.02581300586462021\n",
      "Epoch 2560, Loss: 0.06432167999446392, Final Batch Loss: 0.038066454231739044\n",
      "Epoch 2561, Loss: 0.07704605162143707, Final Batch Loss: 0.052687156945466995\n",
      "Epoch 2562, Loss: 0.05402290262281895, Final Batch Loss: 0.0215656366199255\n",
      "Epoch 2563, Loss: 0.044419881887733936, Final Batch Loss: 0.014832145534455776\n",
      "Epoch 2564, Loss: 0.050091927871108055, Final Batch Loss: 0.016579141840338707\n",
      "Epoch 2565, Loss: 0.0656304880976677, Final Batch Loss: 0.045636605471372604\n",
      "Epoch 2566, Loss: 0.025662008207291365, Final Batch Loss: 0.007415273692458868\n",
      "Epoch 2567, Loss: 0.07481989823281765, Final Batch Loss: 0.046356357634067535\n",
      "Epoch 2568, Loss: 0.04716124013066292, Final Batch Loss: 0.0065877437591552734\n",
      "Epoch 2569, Loss: 0.08361009508371353, Final Batch Loss: 0.029500383883714676\n",
      "Epoch 2570, Loss: 0.058087827637791634, Final Batch Loss: 0.02677159197628498\n",
      "Epoch 2571, Loss: 0.1303553283214569, Final Batch Loss: 0.09715186804533005\n",
      "Epoch 2572, Loss: 0.0749125275760889, Final Batch Loss: 0.027112076058983803\n",
      "Epoch 2573, Loss: 0.05844183545559645, Final Batch Loss: 0.014945208095014095\n",
      "Epoch 2574, Loss: 0.04642727039754391, Final Batch Loss: 0.02099723368883133\n",
      "Epoch 2575, Loss: 0.07826144248247147, Final Batch Loss: 0.043007247149944305\n",
      "Epoch 2576, Loss: 0.03767749108374119, Final Batch Loss: 0.01707678660750389\n",
      "Epoch 2577, Loss: 0.049155218526721, Final Batch Loss: 0.015818843618035316\n",
      "Epoch 2578, Loss: 0.0465209037065506, Final Batch Loss: 0.019124753773212433\n",
      "Epoch 2579, Loss: 0.13385416194796562, Final Batch Loss: 0.10685466974973679\n",
      "Epoch 2580, Loss: 0.09027683921158314, Final Batch Loss: 0.061111800372600555\n",
      "Epoch 2581, Loss: 0.08903871476650238, Final Batch Loss: 0.026816170662641525\n",
      "Epoch 2582, Loss: 0.08259090594947338, Final Batch Loss: 0.06324998289346695\n",
      "Epoch 2583, Loss: 0.058831293135881424, Final Batch Loss: 0.025528304278850555\n",
      "Epoch 2584, Loss: 0.06332288682460785, Final Batch Loss: 0.031923625618219376\n",
      "Epoch 2585, Loss: 0.09203331544995308, Final Batch Loss: 0.05780980736017227\n",
      "Epoch 2586, Loss: 0.045575838536024094, Final Batch Loss: 0.012103069573640823\n",
      "Epoch 2587, Loss: 0.049072034657001495, Final Batch Loss: 0.018550574779510498\n",
      "Epoch 2588, Loss: 0.06705900095403194, Final Batch Loss: 0.020718006417155266\n",
      "Epoch 2589, Loss: 0.03580291569232941, Final Batch Loss: 0.012497598305344582\n",
      "Epoch 2590, Loss: 0.0516024362295866, Final Batch Loss: 0.00855291448533535\n",
      "Epoch 2591, Loss: 0.06264154985547066, Final Batch Loss: 0.024357415735721588\n",
      "Epoch 2592, Loss: 0.04689602740108967, Final Batch Loss: 0.017056448385119438\n",
      "Epoch 2593, Loss: 0.049420105293393135, Final Batch Loss: 0.02726343832910061\n",
      "Epoch 2594, Loss: 0.07572311162948608, Final Batch Loss: 0.029181580990552902\n",
      "Epoch 2595, Loss: 0.12176202982664108, Final Batch Loss: 0.03273440897464752\n",
      "Epoch 2596, Loss: 0.06511920224875212, Final Batch Loss: 0.05573452264070511\n",
      "Epoch 2597, Loss: 0.10113395377993584, Final Batch Loss: 0.07323163002729416\n",
      "Epoch 2598, Loss: 0.049443576484918594, Final Batch Loss: 0.020939495414495468\n",
      "Epoch 2599, Loss: 0.042749207466840744, Final Batch Loss: 0.023603513836860657\n",
      "Epoch 2600, Loss: 0.09591951034963131, Final Batch Loss: 0.026709092780947685\n",
      "Epoch 2601, Loss: 0.09711092710494995, Final Batch Loss: 0.0508732795715332\n",
      "Epoch 2602, Loss: 0.06961356848478317, Final Batch Loss: 0.023069296032190323\n",
      "Epoch 2603, Loss: 0.037463863380253315, Final Batch Loss: 0.010525994934141636\n",
      "Epoch 2604, Loss: 0.06854715198278427, Final Batch Loss: 0.03540205582976341\n",
      "Epoch 2605, Loss: 0.06892848014831543, Final Batch Loss: 0.029629185795783997\n",
      "Epoch 2606, Loss: 0.10422245040535927, Final Batch Loss: 0.058930329978466034\n",
      "Epoch 2607, Loss: 0.08041390404105186, Final Batch Loss: 0.04055934026837349\n",
      "Epoch 2608, Loss: 0.13576726242899895, Final Batch Loss: 0.05975273624062538\n",
      "Epoch 2609, Loss: 0.05166007298976183, Final Batch Loss: 0.0421428419649601\n",
      "Epoch 2610, Loss: 0.1218979824334383, Final Batch Loss: 0.09984986484050751\n",
      "Epoch 2611, Loss: 0.07537370920181274, Final Batch Loss: 0.039962369948625565\n",
      "Epoch 2612, Loss: 0.07750228978693485, Final Batch Loss: 0.02784871496260166\n",
      "Epoch 2613, Loss: 0.09363358840346336, Final Batch Loss: 0.030593570321798325\n",
      "Epoch 2614, Loss: 0.10334459319710732, Final Batch Loss: 0.08271538466215134\n",
      "Epoch 2615, Loss: 0.059882091358304024, Final Batch Loss: 0.03049110248684883\n",
      "Epoch 2616, Loss: 0.08639346621930599, Final Batch Loss: 0.05952378362417221\n",
      "Epoch 2617, Loss: 0.06326427683234215, Final Batch Loss: 0.010454107075929642\n",
      "Epoch 2618, Loss: 0.09349741414189339, Final Batch Loss: 0.05069701001048088\n",
      "Epoch 2619, Loss: 0.12057848274707794, Final Batch Loss: 0.08637039363384247\n",
      "Epoch 2620, Loss: 0.1288472954183817, Final Batch Loss: 0.026729004457592964\n",
      "Epoch 2621, Loss: 0.08467470668256283, Final Batch Loss: 0.05730064585804939\n",
      "Epoch 2622, Loss: 0.09853036887943745, Final Batch Loss: 0.02381746657192707\n",
      "Epoch 2623, Loss: 0.08126316964626312, Final Batch Loss: 0.041150618344545364\n",
      "Epoch 2624, Loss: 0.045175475999712944, Final Batch Loss: 0.010128429159522057\n",
      "Epoch 2625, Loss: 0.06341509334743023, Final Batch Loss: 0.03021976910531521\n",
      "Epoch 2626, Loss: 0.051490418612957, Final Batch Loss: 0.025055818259716034\n",
      "Epoch 2627, Loss: 0.034799451008439064, Final Batch Loss: 0.018390337005257607\n",
      "Epoch 2628, Loss: 0.06178984418511391, Final Batch Loss: 0.026220668107271194\n",
      "Epoch 2629, Loss: 0.056561561301350594, Final Batch Loss: 0.018904363736510277\n",
      "Epoch 2630, Loss: 0.08621339127421379, Final Batch Loss: 0.037105053663253784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2631, Loss: 0.09323529899120331, Final Batch Loss: 0.03639933094382286\n",
      "Epoch 2632, Loss: 0.06560691446065903, Final Batch Loss: 0.0192934088408947\n",
      "Epoch 2633, Loss: 0.06906619109213352, Final Batch Loss: 0.019468950107693672\n",
      "Epoch 2634, Loss: 0.058206900022923946, Final Batch Loss: 0.04467860609292984\n",
      "Epoch 2635, Loss: 0.07457789033651352, Final Batch Loss: 0.028352957218885422\n",
      "Epoch 2636, Loss: 0.07806261256337166, Final Batch Loss: 0.04122187942266464\n",
      "Epoch 2637, Loss: 0.06661534495651722, Final Batch Loss: 0.02273804135620594\n",
      "Epoch 2638, Loss: 0.1125418022274971, Final Batch Loss: 0.060446932911872864\n",
      "Epoch 2639, Loss: 0.04685288667678833, Final Batch Loss: 0.02252023294568062\n",
      "Epoch 2640, Loss: 0.05932205356657505, Final Batch Loss: 0.042846571654081345\n",
      "Epoch 2641, Loss: 0.030128448270261288, Final Batch Loss: 0.012049135752022266\n",
      "Epoch 2642, Loss: 0.05365982837975025, Final Batch Loss: 0.016458196565508842\n",
      "Epoch 2643, Loss: 0.04723525233566761, Final Batch Loss: 0.016552790999412537\n",
      "Epoch 2644, Loss: 0.048111459240317345, Final Batch Loss: 0.01593918912112713\n",
      "Epoch 2645, Loss: 0.048787953332066536, Final Batch Loss: 0.025708504021167755\n",
      "Epoch 2646, Loss: 0.08624732866883278, Final Batch Loss: 0.034261565655469894\n",
      "Epoch 2647, Loss: 0.047026706859469414, Final Batch Loss: 0.023636430501937866\n",
      "Epoch 2648, Loss: 0.06941497698426247, Final Batch Loss: 0.029679082334041595\n",
      "Epoch 2649, Loss: 0.049845133908092976, Final Batch Loss: 0.009574593044817448\n",
      "Epoch 2650, Loss: 0.04414580948650837, Final Batch Loss: 0.022946881130337715\n",
      "Epoch 2651, Loss: 0.06472751498222351, Final Batch Loss: 0.03279487043619156\n",
      "Epoch 2652, Loss: 0.07970820553600788, Final Batch Loss: 0.06509378552436829\n",
      "Epoch 2653, Loss: 0.044061836786568165, Final Batch Loss: 0.011224337853491306\n",
      "Epoch 2654, Loss: 0.05622213240712881, Final Batch Loss: 0.013759593479335308\n",
      "Epoch 2655, Loss: 0.02457958646118641, Final Batch Loss: 0.009037293493747711\n",
      "Epoch 2656, Loss: 0.034255060367286205, Final Batch Loss: 0.021496685221791267\n",
      "Epoch 2657, Loss: 0.11066753789782524, Final Batch Loss: 0.04906678572297096\n",
      "Epoch 2658, Loss: 0.04356855619698763, Final Batch Loss: 0.010652550496160984\n",
      "Epoch 2659, Loss: 0.045100703835487366, Final Batch Loss: 0.018395567312836647\n",
      "Epoch 2660, Loss: 0.07649230025708675, Final Batch Loss: 0.04753323644399643\n",
      "Epoch 2661, Loss: 0.04900311306118965, Final Batch Loss: 0.024632884189486504\n",
      "Epoch 2662, Loss: 0.06040623597800732, Final Batch Loss: 0.039680734276771545\n",
      "Epoch 2663, Loss: 0.06852032989263535, Final Batch Loss: 0.022945120930671692\n",
      "Epoch 2664, Loss: 0.07011465355753899, Final Batch Loss: 0.02862124890089035\n",
      "Epoch 2665, Loss: 0.029282450675964355, Final Batch Loss: 0.011888405308127403\n",
      "Epoch 2666, Loss: 0.05115480814129114, Final Batch Loss: 0.011323903687298298\n",
      "Epoch 2667, Loss: 0.06930684484541416, Final Batch Loss: 0.041151728481054306\n",
      "Epoch 2668, Loss: 0.03817298077046871, Final Batch Loss: 0.0188955906778574\n",
      "Epoch 2669, Loss: 0.07462519034743309, Final Batch Loss: 0.043158527463674545\n",
      "Epoch 2670, Loss: 0.03702163323760033, Final Batch Loss: 0.017521940171718597\n",
      "Epoch 2671, Loss: 0.04898286983370781, Final Batch Loss: 0.03551419824361801\n",
      "Epoch 2672, Loss: 0.13941757380962372, Final Batch Loss: 0.07757502794265747\n",
      "Epoch 2673, Loss: 0.094839321449399, Final Batch Loss: 0.07848207652568817\n",
      "Epoch 2674, Loss: 0.07466975040733814, Final Batch Loss: 0.04695899412035942\n",
      "Epoch 2675, Loss: 0.05323478765785694, Final Batch Loss: 0.021861208602786064\n",
      "Epoch 2676, Loss: 0.09069769084453583, Final Batch Loss: 0.05776020139455795\n",
      "Epoch 2677, Loss: 0.0679062083363533, Final Batch Loss: 0.03986494243144989\n",
      "Epoch 2678, Loss: 0.09651765413582325, Final Batch Loss: 0.07312972098588943\n",
      "Epoch 2679, Loss: 0.12323851883411407, Final Batch Loss: 0.07344706356525421\n",
      "Epoch 2680, Loss: 0.10017573088407516, Final Batch Loss: 0.062171149998903275\n",
      "Epoch 2681, Loss: 0.09420866146683693, Final Batch Loss: 0.04700835421681404\n",
      "Epoch 2682, Loss: 0.09103826247155666, Final Batch Loss: 0.02102980576455593\n",
      "Epoch 2683, Loss: 0.0779488068073988, Final Batch Loss: 0.05334769934415817\n",
      "Epoch 2684, Loss: 0.1126305740326643, Final Batch Loss: 0.026275811716914177\n",
      "Epoch 2685, Loss: 0.0679371077567339, Final Batch Loss: 0.02534599043428898\n",
      "Epoch 2686, Loss: 0.06082348711788654, Final Batch Loss: 0.02144649811089039\n",
      "Epoch 2687, Loss: 0.14513806998729706, Final Batch Loss: 0.06645917147397995\n",
      "Epoch 2688, Loss: 0.041310036554932594, Final Batch Loss: 0.017796851694583893\n",
      "Epoch 2689, Loss: 0.06469016894698143, Final Batch Loss: 0.029010284692049026\n",
      "Epoch 2690, Loss: 0.04468842502683401, Final Batch Loss: 0.015608577989041805\n",
      "Epoch 2691, Loss: 0.07154453545808792, Final Batch Loss: 0.0249369814991951\n",
      "Epoch 2692, Loss: 0.07296611182391644, Final Batch Loss: 0.042119886726140976\n",
      "Epoch 2693, Loss: 0.06863345764577389, Final Batch Loss: 0.01579957641661167\n",
      "Epoch 2694, Loss: 0.07435266673564911, Final Batch Loss: 0.05805362015962601\n",
      "Epoch 2695, Loss: 0.03409609757363796, Final Batch Loss: 0.012780075892806053\n",
      "Epoch 2696, Loss: 0.11251884698867798, Final Batch Loss: 0.077327199280262\n",
      "Epoch 2697, Loss: 0.04365837760269642, Final Batch Loss: 0.024840643629431725\n",
      "Epoch 2698, Loss: 0.05899590626358986, Final Batch Loss: 0.02649988979101181\n",
      "Epoch 2699, Loss: 0.048191532492637634, Final Batch Loss: 0.024382436648011208\n",
      "Epoch 2700, Loss: 0.054255472496151924, Final Batch Loss: 0.024177148938179016\n",
      "Epoch 2701, Loss: 0.13922934979200363, Final Batch Loss: 0.06285727024078369\n",
      "Epoch 2702, Loss: 0.06471182964742184, Final Batch Loss: 0.0395466573536396\n",
      "Epoch 2703, Loss: 0.08126058429479599, Final Batch Loss: 0.04651684686541557\n",
      "Epoch 2704, Loss: 0.09625471755862236, Final Batch Loss: 0.06679659336805344\n",
      "Epoch 2705, Loss: 0.044457859359681606, Final Batch Loss: 0.030289392918348312\n",
      "Epoch 2706, Loss: 0.0258242329582572, Final Batch Loss: 0.01544759888201952\n",
      "Epoch 2707, Loss: 0.037638699635863304, Final Batch Loss: 0.016166843473911285\n",
      "Epoch 2708, Loss: 0.056143222376704216, Final Batch Loss: 0.026435010135173798\n",
      "Epoch 2709, Loss: 0.03592784143984318, Final Batch Loss: 0.01917642541229725\n",
      "Epoch 2710, Loss: 0.035491748712956905, Final Batch Loss: 0.02542261965572834\n",
      "Epoch 2711, Loss: 0.025956634432077408, Final Batch Loss: 0.011342435143887997\n",
      "Epoch 2712, Loss: 0.07875465601682663, Final Batch Loss: 0.05232984572649002\n",
      "Epoch 2713, Loss: 0.08819354511797428, Final Batch Loss: 0.06676197052001953\n",
      "Epoch 2714, Loss: 0.03510821145027876, Final Batch Loss: 0.01073422934859991\n",
      "Epoch 2715, Loss: 0.05154290236532688, Final Batch Loss: 0.025344401597976685\n",
      "Epoch 2716, Loss: 0.09494194015860558, Final Batch Loss: 0.044381093233823776\n",
      "Epoch 2717, Loss: 0.05163808353245258, Final Batch Loss: 0.025323403999209404\n",
      "Epoch 2718, Loss: 0.06361286155879498, Final Batch Loss: 0.02175990678369999\n",
      "Epoch 2719, Loss: 0.021683070808649063, Final Batch Loss: 0.011063622310757637\n",
      "Epoch 2720, Loss: 0.06804438307881355, Final Batch Loss: 0.05158909410238266\n",
      "Epoch 2721, Loss: 0.05677325464785099, Final Batch Loss: 0.0389416478574276\n",
      "Epoch 2722, Loss: 0.061780021991580725, Final Batch Loss: 0.006838878151029348\n",
      "Epoch 2723, Loss: 0.036081502214074135, Final Batch Loss: 0.020951254293322563\n",
      "Epoch 2724, Loss: 0.08297177217900753, Final Batch Loss: 0.022622132673859596\n",
      "Epoch 2725, Loss: 0.04582933522760868, Final Batch Loss: 0.018811244517564774\n",
      "Epoch 2726, Loss: 0.07366514950990677, Final Batch Loss: 0.03694005310535431\n",
      "Epoch 2727, Loss: 0.03478963579982519, Final Batch Loss: 0.011531352065503597\n",
      "Epoch 2728, Loss: 0.056613439694046974, Final Batch Loss: 0.018095100298523903\n",
      "Epoch 2729, Loss: 0.04440854676067829, Final Batch Loss: 0.029461145401000977\n",
      "Epoch 2730, Loss: 0.056084657087922096, Final Batch Loss: 0.009418567642569542\n",
      "Epoch 2731, Loss: 0.035206593573093414, Final Batch Loss: 0.019629109650850296\n",
      "Epoch 2732, Loss: 0.03294601570814848, Final Batch Loss: 0.021940428763628006\n",
      "Epoch 2733, Loss: 0.06515520997345448, Final Batch Loss: 0.03610478341579437\n",
      "Epoch 2734, Loss: 0.03531955275684595, Final Batch Loss: 0.013926726765930653\n",
      "Epoch 2735, Loss: 0.027973611373454332, Final Batch Loss: 0.004397429991513491\n",
      "Epoch 2736, Loss: 0.0296537890098989, Final Batch Loss: 0.006239527370780706\n",
      "Epoch 2737, Loss: 0.05288819782435894, Final Batch Loss: 0.03599952161312103\n",
      "Epoch 2738, Loss: 0.04850693792104721, Final Batch Loss: 0.031117431819438934\n",
      "Epoch 2739, Loss: 0.04350157082080841, Final Batch Loss: 0.03313400223851204\n",
      "Epoch 2740, Loss: 0.06506236642599106, Final Batch Loss: 0.027212433516979218\n",
      "Epoch 2741, Loss: 0.0596325509250164, Final Batch Loss: 0.01291600614786148\n",
      "Epoch 2742, Loss: 0.01828460581600666, Final Batch Loss: 0.01166854240000248\n",
      "Epoch 2743, Loss: 0.07042082771658897, Final Batch Loss: 0.044205520302057266\n",
      "Epoch 2744, Loss: 0.028377747163176537, Final Batch Loss: 0.020345615223050117\n",
      "Epoch 2745, Loss: 0.026409932412207127, Final Batch Loss: 0.012920502573251724\n",
      "Epoch 2746, Loss: 0.060095472261309624, Final Batch Loss: 0.03455030173063278\n",
      "Epoch 2747, Loss: 0.13887698203325272, Final Batch Loss: 0.10549089312553406\n",
      "Epoch 2748, Loss: 0.06262334436178207, Final Batch Loss: 0.030229903757572174\n",
      "Epoch 2749, Loss: 0.03895220626145601, Final Batch Loss: 0.013368011452257633\n",
      "Epoch 2750, Loss: 0.044124020263552666, Final Batch Loss: 0.02564355917274952\n",
      "Epoch 2751, Loss: 0.11269480735063553, Final Batch Loss: 0.07844516634941101\n",
      "Epoch 2752, Loss: 0.04757069610059261, Final Batch Loss: 0.020674802362918854\n",
      "Epoch 2753, Loss: 0.06601820141077042, Final Batch Loss: 0.017316259443759918\n",
      "Epoch 2754, Loss: 0.09814260341227055, Final Batch Loss: 0.02802267111837864\n",
      "Epoch 2755, Loss: 0.07227575965225697, Final Batch Loss: 0.05161559209227562\n",
      "Epoch 2756, Loss: 0.0633273608982563, Final Batch Loss: 0.04141503944993019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2757, Loss: 0.04472629353404045, Final Batch Loss: 0.015579154714941978\n",
      "Epoch 2758, Loss: 0.049821872264146805, Final Batch Loss: 0.030747096985578537\n",
      "Epoch 2759, Loss: 0.047167932614684105, Final Batch Loss: 0.0165423434227705\n",
      "Epoch 2760, Loss: 0.053988391533493996, Final Batch Loss: 0.020779600366950035\n",
      "Epoch 2761, Loss: 0.05496014002710581, Final Batch Loss: 0.006514527834951878\n",
      "Epoch 2762, Loss: 0.04915274400264025, Final Batch Loss: 0.039213985204696655\n",
      "Epoch 2763, Loss: 0.05185854248702526, Final Batch Loss: 0.032922860234975815\n",
      "Epoch 2764, Loss: 0.027756004594266415, Final Batch Loss: 0.008395464159548283\n",
      "Epoch 2765, Loss: 0.04421860910952091, Final Batch Loss: 0.02038194239139557\n",
      "Epoch 2766, Loss: 0.05232657492160797, Final Batch Loss: 0.03296351805329323\n",
      "Epoch 2767, Loss: 0.06557173654437065, Final Batch Loss: 0.032547544687986374\n",
      "Epoch 2768, Loss: 0.0642714574933052, Final Batch Loss: 0.011924024671316147\n",
      "Epoch 2769, Loss: 0.03237596061080694, Final Batch Loss: 0.007436684332787991\n",
      "Epoch 2770, Loss: 0.03840775415301323, Final Batch Loss: 0.021228522062301636\n",
      "Epoch 2771, Loss: 0.04631111118942499, Final Batch Loss: 0.015564623288810253\n",
      "Epoch 2772, Loss: 0.04887611232697964, Final Batch Loss: 0.019616739824414253\n",
      "Epoch 2773, Loss: 0.027465655468404293, Final Batch Loss: 0.014235115610063076\n",
      "Epoch 2774, Loss: 0.029100478626787663, Final Batch Loss: 0.014918716624379158\n",
      "Epoch 2775, Loss: 0.05789753794670105, Final Batch Loss: 0.02122027799487114\n",
      "Epoch 2776, Loss: 0.041023909114301205, Final Batch Loss: 0.013034217990934849\n",
      "Epoch 2777, Loss: 0.07162945717573166, Final Batch Loss: 0.017192285507917404\n",
      "Epoch 2778, Loss: 0.05170772224664688, Final Batch Loss: 0.029059894382953644\n",
      "Epoch 2779, Loss: 0.0293197026476264, Final Batch Loss: 0.010104726068675518\n",
      "Epoch 2780, Loss: 0.08003831468522549, Final Batch Loss: 0.02969830296933651\n",
      "Epoch 2781, Loss: 0.04529125057160854, Final Batch Loss: 0.011275393888354301\n",
      "Epoch 2782, Loss: 0.10240478068590164, Final Batch Loss: 0.07745490968227386\n",
      "Epoch 2783, Loss: 0.09314363496378064, Final Batch Loss: 0.005597651470452547\n",
      "Epoch 2784, Loss: 0.06994155421853065, Final Batch Loss: 0.048268795013427734\n",
      "Epoch 2785, Loss: 0.09668666869401932, Final Batch Loss: 0.033044181764125824\n",
      "Epoch 2786, Loss: 0.06358412280678749, Final Batch Loss: 0.015365339815616608\n",
      "Epoch 2787, Loss: 0.10805996134877205, Final Batch Loss: 0.06993651390075684\n",
      "Epoch 2788, Loss: 0.060475246980786324, Final Batch Loss: 0.021922891959547997\n",
      "Epoch 2789, Loss: 0.09348579868674278, Final Batch Loss: 0.026205051690340042\n",
      "Epoch 2790, Loss: 0.07736332342028618, Final Batch Loss: 0.03103906288743019\n",
      "Epoch 2791, Loss: 0.034567441791296005, Final Batch Loss: 0.018170608207583427\n",
      "Epoch 2792, Loss: 0.06184281641617417, Final Batch Loss: 0.007507531438022852\n",
      "Epoch 2793, Loss: 0.07431918755173683, Final Batch Loss: 0.03808699920773506\n",
      "Epoch 2794, Loss: 0.023220278322696686, Final Batch Loss: 0.011586753651499748\n",
      "Epoch 2795, Loss: 0.06613871641457081, Final Batch Loss: 0.04048880189657211\n",
      "Epoch 2796, Loss: 0.0901296604424715, Final Batch Loss: 0.06983126699924469\n",
      "Epoch 2797, Loss: 0.06511354632675648, Final Batch Loss: 0.03845249116420746\n",
      "Epoch 2798, Loss: 0.07509088702499866, Final Batch Loss: 0.025603028014302254\n",
      "Epoch 2799, Loss: 0.05791831016540527, Final Batch Loss: 0.03007768839597702\n",
      "Epoch 2800, Loss: 0.06214350648224354, Final Batch Loss: 0.04058486223220825\n",
      "Epoch 2801, Loss: 0.07744895294308662, Final Batch Loss: 0.04565795883536339\n",
      "Epoch 2802, Loss: 0.06049677822738886, Final Batch Loss: 0.012443394400179386\n",
      "Epoch 2803, Loss: 0.07845817320048809, Final Batch Loss: 0.05590688809752464\n",
      "Epoch 2804, Loss: 0.08393309265375137, Final Batch Loss: 0.015871137380599976\n",
      "Epoch 2805, Loss: 0.09735687263309956, Final Batch Loss: 0.07000783085823059\n",
      "Epoch 2806, Loss: 0.029654564801603556, Final Batch Loss: 0.02510800212621689\n",
      "Epoch 2807, Loss: 0.04190292954444885, Final Batch Loss: 0.012644132599234581\n",
      "Epoch 2808, Loss: 0.08432623371481895, Final Batch Loss: 0.03537111356854439\n",
      "Epoch 2809, Loss: 0.09759276360273361, Final Batch Loss: 0.04586251452565193\n",
      "Epoch 2810, Loss: 0.11445022374391556, Final Batch Loss: 0.03133905678987503\n",
      "Epoch 2811, Loss: 0.1035807840526104, Final Batch Loss: 0.04121950641274452\n",
      "Epoch 2812, Loss: 0.03608161397278309, Final Batch Loss: 0.01418745145201683\n",
      "Epoch 2813, Loss: 0.08630838245153427, Final Batch Loss: 0.0463094636797905\n",
      "Epoch 2814, Loss: 0.07890752702951431, Final Batch Loss: 0.043153587728738785\n",
      "Epoch 2815, Loss: 0.05098079890012741, Final Batch Loss: 0.016244884580373764\n",
      "Epoch 2816, Loss: 0.1414049193263054, Final Batch Loss: 0.05997435003519058\n",
      "Epoch 2817, Loss: 0.14371077716350555, Final Batch Loss: 0.055016107857227325\n",
      "Epoch 2818, Loss: 0.05483174882829189, Final Batch Loss: 0.031807512044906616\n",
      "Epoch 2819, Loss: 0.11872181296348572, Final Batch Loss: 0.06619187444448471\n",
      "Epoch 2820, Loss: 0.09163486957550049, Final Batch Loss: 0.048415251076221466\n",
      "Epoch 2821, Loss: 0.14659259282052517, Final Batch Loss: 0.027346281334757805\n",
      "Epoch 2822, Loss: 0.0949525535106659, Final Batch Loss: 0.034682806581258774\n",
      "Epoch 2823, Loss: 0.059580614790320396, Final Batch Loss: 0.03692071512341499\n",
      "Epoch 2824, Loss: 0.0890020988881588, Final Batch Loss: 0.05385414883494377\n",
      "Epoch 2825, Loss: 0.06368114985525608, Final Batch Loss: 0.032700277864933014\n",
      "Epoch 2826, Loss: 0.02685605827718973, Final Batch Loss: 0.00788430217653513\n",
      "Epoch 2827, Loss: 0.095840934664011, Final Batch Loss: 0.021415013819932938\n",
      "Epoch 2828, Loss: 0.0350795965641737, Final Batch Loss: 0.018330680206418037\n",
      "Epoch 2829, Loss: 0.03513691388070583, Final Batch Loss: 0.008226439356803894\n",
      "Epoch 2830, Loss: 0.05046192556619644, Final Batch Loss: 0.027564678341150284\n",
      "Epoch 2831, Loss: 0.06940235197544098, Final Batch Loss: 0.056570373475551605\n",
      "Epoch 2832, Loss: 0.05288073793053627, Final Batch Loss: 0.025547465309500694\n",
      "Epoch 2833, Loss: 0.07488945871591568, Final Batch Loss: 0.02114526927471161\n",
      "Epoch 2834, Loss: 0.08353513106703758, Final Batch Loss: 0.04682605713605881\n",
      "Epoch 2835, Loss: 0.09050319343805313, Final Batch Loss: 0.051207736134529114\n",
      "Epoch 2836, Loss: 0.06918240617960691, Final Batch Loss: 0.05475328117609024\n",
      "Epoch 2837, Loss: 0.04678523167967796, Final Batch Loss: 0.03281418979167938\n",
      "Epoch 2838, Loss: 0.09286561235785484, Final Batch Loss: 0.05176892876625061\n",
      "Epoch 2839, Loss: 0.05454869009554386, Final Batch Loss: 0.020966822281479836\n",
      "Epoch 2840, Loss: 0.0490513127297163, Final Batch Loss: 0.02591497078537941\n",
      "Epoch 2841, Loss: 0.07672114670276642, Final Batch Loss: 0.023940246552228928\n",
      "Epoch 2842, Loss: 0.10154112428426743, Final Batch Loss: 0.04578527808189392\n",
      "Epoch 2843, Loss: 0.049230821430683136, Final Batch Loss: 0.020996956154704094\n",
      "Epoch 2844, Loss: 0.06355918571352959, Final Batch Loss: 0.028812725096940994\n",
      "Epoch 2845, Loss: 0.017061464488506317, Final Batch Loss: 0.008343859575688839\n",
      "Epoch 2846, Loss: 0.05797589011490345, Final Batch Loss: 0.019025949761271477\n",
      "Epoch 2847, Loss: 0.03251040540635586, Final Batch Loss: 0.019135791808366776\n",
      "Epoch 2848, Loss: 0.10025589540600777, Final Batch Loss: 0.044884975999593735\n",
      "Epoch 2849, Loss: 0.060556480661034584, Final Batch Loss: 0.03807293623685837\n",
      "Epoch 2850, Loss: 0.0831023994833231, Final Batch Loss: 0.025839583948254585\n",
      "Epoch 2851, Loss: 0.08094687946140766, Final Batch Loss: 0.05431414023041725\n",
      "Epoch 2852, Loss: 0.054343920201063156, Final Batch Loss: 0.03807028383016586\n",
      "Epoch 2853, Loss: 0.039799537509679794, Final Batch Loss: 0.017247317358851433\n",
      "Epoch 2854, Loss: 0.0545014962553978, Final Batch Loss: 0.030223460868000984\n",
      "Epoch 2855, Loss: 0.04530978761613369, Final Batch Loss: 0.03360225260257721\n",
      "Epoch 2856, Loss: 0.04739931970834732, Final Batch Loss: 0.023974599316716194\n",
      "Epoch 2857, Loss: 0.052465686574578285, Final Batch Loss: 0.02524549700319767\n",
      "Epoch 2858, Loss: 0.08082457259297371, Final Batch Loss: 0.041617561131715775\n",
      "Epoch 2859, Loss: 0.042102668434381485, Final Batch Loss: 0.010596249252557755\n",
      "Epoch 2860, Loss: 0.026075140573084354, Final Batch Loss: 0.006651547737419605\n",
      "Epoch 2861, Loss: 0.06700316444039345, Final Batch Loss: 0.02249128744006157\n",
      "Epoch 2862, Loss: 0.035766477696597576, Final Batch Loss: 0.02187390811741352\n",
      "Epoch 2863, Loss: 0.13429481722414494, Final Batch Loss: 0.10439234972000122\n",
      "Epoch 2864, Loss: 0.04738486558198929, Final Batch Loss: 0.016535360366106033\n",
      "Epoch 2865, Loss: 0.061413517221808434, Final Batch Loss: 0.038658592849969864\n",
      "Epoch 2866, Loss: 0.09526250883936882, Final Batch Loss: 0.027381163090467453\n",
      "Epoch 2867, Loss: 0.04949868656694889, Final Batch Loss: 0.032521072775125504\n",
      "Epoch 2868, Loss: 0.021512539125978947, Final Batch Loss: 0.009218795225024223\n",
      "Epoch 2869, Loss: 0.05831711366772652, Final Batch Loss: 0.03134031221270561\n",
      "Epoch 2870, Loss: 0.07849796488881111, Final Batch Loss: 0.03555189073085785\n",
      "Epoch 2871, Loss: 0.027298751287162304, Final Batch Loss: 0.013026691041886806\n",
      "Epoch 2872, Loss: 0.059295615181326866, Final Batch Loss: 0.029401754960417747\n",
      "Epoch 2873, Loss: 0.057664391584694386, Final Batch Loss: 0.04372413083910942\n",
      "Epoch 2874, Loss: 0.04884364269673824, Final Batch Loss: 0.019954631105065346\n",
      "Epoch 2875, Loss: 0.042498933151364326, Final Batch Loss: 0.01605425402522087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2876, Loss: 0.02409395296126604, Final Batch Loss: 0.013128609396517277\n",
      "Epoch 2877, Loss: 0.052754905074834824, Final Batch Loss: 0.017753731459379196\n",
      "Epoch 2878, Loss: 0.10815388336777687, Final Batch Loss: 0.05871569737792015\n",
      "Epoch 2879, Loss: 0.07123809307813644, Final Batch Loss: 0.03232431784272194\n",
      "Epoch 2880, Loss: 0.07044274732470512, Final Batch Loss: 0.061217863112688065\n",
      "Epoch 2881, Loss: 0.04644162626937032, Final Batch Loss: 0.005100302863866091\n",
      "Epoch 2882, Loss: 0.058558566495776176, Final Batch Loss: 0.04209287837147713\n",
      "Epoch 2883, Loss: 0.08836439251899719, Final Batch Loss: 0.044640589505434036\n",
      "Epoch 2884, Loss: 0.09188982471823692, Final Batch Loss: 0.05927874147891998\n",
      "Epoch 2885, Loss: 0.037417104467749596, Final Batch Loss: 0.02526434138417244\n",
      "Epoch 2886, Loss: 0.08402236178517342, Final Batch Loss: 0.04669903591275215\n",
      "Epoch 2887, Loss: 0.056035688146948814, Final Batch Loss: 0.01800001971423626\n",
      "Epoch 2888, Loss: 0.04303261358290911, Final Batch Loss: 0.031274717301130295\n",
      "Epoch 2889, Loss: 0.07519461587071419, Final Batch Loss: 0.033372506499290466\n",
      "Epoch 2890, Loss: 0.04037546366453171, Final Batch Loss: 0.011333435773849487\n",
      "Epoch 2891, Loss: 0.1361873261630535, Final Batch Loss: 0.05341679975390434\n",
      "Epoch 2892, Loss: 0.07226721569895744, Final Batch Loss: 0.03340059891343117\n",
      "Epoch 2893, Loss: 0.03566095232963562, Final Batch Loss: 0.01730225421488285\n",
      "Epoch 2894, Loss: 0.06672679632902145, Final Batch Loss: 0.032877322286367416\n",
      "Epoch 2895, Loss: 0.04866486229002476, Final Batch Loss: 0.02661868929862976\n",
      "Epoch 2896, Loss: 0.08301880583167076, Final Batch Loss: 0.04433717951178551\n",
      "Epoch 2897, Loss: 0.04277714714407921, Final Batch Loss: 0.015774069353938103\n",
      "Epoch 2898, Loss: 0.0893847867846489, Final Batch Loss: 0.030117232352495193\n",
      "Epoch 2899, Loss: 0.056732709519565105, Final Batch Loss: 0.015173631720244884\n",
      "Epoch 2900, Loss: 0.033126892521977425, Final Batch Loss: 0.016942910850048065\n",
      "Epoch 2901, Loss: 0.10825024545192719, Final Batch Loss: 0.06876783818006516\n",
      "Epoch 2902, Loss: 0.05315633397549391, Final Batch Loss: 0.012021570466458797\n",
      "Epoch 2903, Loss: 0.0628963802009821, Final Batch Loss: 0.03525841236114502\n",
      "Epoch 2904, Loss: 0.08298608660697937, Final Batch Loss: 0.04464145749807358\n",
      "Epoch 2905, Loss: 0.10661119595170021, Final Batch Loss: 0.06142457202076912\n",
      "Epoch 2906, Loss: 0.0693427287042141, Final Batch Loss: 0.032004933804273605\n",
      "Epoch 2907, Loss: 0.04682333581149578, Final Batch Loss: 0.02562127821147442\n",
      "Epoch 2908, Loss: 0.09801007062196732, Final Batch Loss: 0.06549185514450073\n",
      "Epoch 2909, Loss: 0.11129882000386715, Final Batch Loss: 0.08071061968803406\n",
      "Epoch 2910, Loss: 0.06269500777125359, Final Batch Loss: 0.02434314414858818\n",
      "Epoch 2911, Loss: 0.10450964793562889, Final Batch Loss: 0.07033193111419678\n",
      "Epoch 2912, Loss: 0.09161709994077682, Final Batch Loss: 0.03778970614075661\n",
      "Epoch 2913, Loss: 0.03646267578005791, Final Batch Loss: 0.01418093778192997\n",
      "Epoch 2914, Loss: 0.08290866017341614, Final Batch Loss: 0.05331987515091896\n",
      "Epoch 2915, Loss: 0.06549378484487534, Final Batch Loss: 0.020492825657129288\n",
      "Epoch 2916, Loss: 0.14239691570401192, Final Batch Loss: 0.08255349099636078\n",
      "Epoch 2917, Loss: 0.09032348543405533, Final Batch Loss: 0.04855116456747055\n",
      "Epoch 2918, Loss: 0.11864149197936058, Final Batch Loss: 0.062339942902326584\n",
      "Epoch 2919, Loss: 0.1106429472565651, Final Batch Loss: 0.05663776397705078\n",
      "Epoch 2920, Loss: 0.12306773662567139, Final Batch Loss: 0.06923908740282059\n",
      "Epoch 2921, Loss: 0.08444333169609308, Final Batch Loss: 0.010881626047194004\n",
      "Epoch 2922, Loss: 0.0605276208370924, Final Batch Loss: 0.03530982881784439\n",
      "Epoch 2923, Loss: 0.0714639825746417, Final Batch Loss: 0.01464052777737379\n",
      "Epoch 2924, Loss: 0.06131387688219547, Final Batch Loss: 0.0400969423353672\n",
      "Epoch 2925, Loss: 0.0789430495351553, Final Batch Loss: 0.05103909596800804\n",
      "Epoch 2926, Loss: 0.09754223003983498, Final Batch Loss: 0.04170898348093033\n",
      "Epoch 2927, Loss: 0.08767804130911827, Final Batch Loss: 0.05484222248196602\n",
      "Epoch 2928, Loss: 0.08481063693761826, Final Batch Loss: 0.01641218364238739\n",
      "Epoch 2929, Loss: 0.06019190140068531, Final Batch Loss: 0.01639258675277233\n",
      "Epoch 2930, Loss: 0.04154855851083994, Final Batch Loss: 0.028426801785826683\n",
      "Epoch 2931, Loss: 0.07484731078147888, Final Batch Loss: 0.036030516028404236\n",
      "Epoch 2932, Loss: 0.07301420904695988, Final Batch Loss: 0.04269661381840706\n",
      "Epoch 2933, Loss: 0.10308698564767838, Final Batch Loss: 0.05383140593767166\n",
      "Epoch 2934, Loss: 0.04034356214106083, Final Batch Loss: 0.028432006016373634\n",
      "Epoch 2935, Loss: 0.07993638515472412, Final Batch Loss: 0.04725074768066406\n",
      "Epoch 2936, Loss: 0.13509681075811386, Final Batch Loss: 0.08062493056058884\n",
      "Epoch 2937, Loss: 0.044517429545521736, Final Batch Loss: 0.020165450870990753\n",
      "Epoch 2938, Loss: 0.056740352883934975, Final Batch Loss: 0.03960698843002319\n",
      "Epoch 2939, Loss: 0.09433096647262573, Final Batch Loss: 0.03926863893866539\n",
      "Epoch 2940, Loss: 0.029320072382688522, Final Batch Loss: 0.01989109255373478\n",
      "Epoch 2941, Loss: 0.07634937204420567, Final Batch Loss: 0.026927737519145012\n",
      "Epoch 2942, Loss: 0.06268490478396416, Final Batch Loss: 0.033216189593076706\n",
      "Epoch 2943, Loss: 0.07218960486352444, Final Batch Loss: 0.01549392007291317\n",
      "Epoch 2944, Loss: 0.05567788518965244, Final Batch Loss: 0.03697766363620758\n",
      "Epoch 2945, Loss: 0.07113786041736603, Final Batch Loss: 0.03234994783997536\n",
      "Epoch 2946, Loss: 0.06163802556693554, Final Batch Loss: 0.022454900667071342\n",
      "Epoch 2947, Loss: 0.04904510919004679, Final Batch Loss: 0.014611967839300632\n",
      "Epoch 2948, Loss: 0.07783854380249977, Final Batch Loss: 0.05924747884273529\n",
      "Epoch 2949, Loss: 0.09886106103658676, Final Batch Loss: 0.030878208577632904\n",
      "Epoch 2950, Loss: 0.04176781326532364, Final Batch Loss: 0.022431034594774246\n",
      "Epoch 2951, Loss: 0.03254779800772667, Final Batch Loss: 0.013595104217529297\n",
      "Epoch 2952, Loss: 0.08851492777466774, Final Batch Loss: 0.056780267506837845\n",
      "Epoch 2953, Loss: 0.024967128410935402, Final Batch Loss: 0.009941988624632359\n",
      "Epoch 2954, Loss: 0.11509601026773453, Final Batch Loss: 0.06413586437702179\n",
      "Epoch 2955, Loss: 0.029735656455159187, Final Batch Loss: 0.00916365534067154\n",
      "Epoch 2956, Loss: 0.038507684133946896, Final Batch Loss: 0.013896049000322819\n",
      "Epoch 2957, Loss: 0.0822106096893549, Final Batch Loss: 0.06242106109857559\n",
      "Epoch 2958, Loss: 0.031700863502919674, Final Batch Loss: 0.01682227849960327\n",
      "Epoch 2959, Loss: 0.045884592458605766, Final Batch Loss: 0.029189838096499443\n",
      "Epoch 2960, Loss: 0.028396006673574448, Final Batch Loss: 0.009487288072705269\n",
      "Epoch 2961, Loss: 0.043060013093054295, Final Batch Loss: 0.028731318190693855\n",
      "Epoch 2962, Loss: 0.02927092183381319, Final Batch Loss: 0.009809483774006367\n",
      "Epoch 2963, Loss: 0.04426197335124016, Final Batch Loss: 0.021362552419304848\n",
      "Epoch 2964, Loss: 0.04687513783574104, Final Batch Loss: 0.01725851185619831\n",
      "Epoch 2965, Loss: 0.05274298042058945, Final Batch Loss: 0.015591371804475784\n",
      "Epoch 2966, Loss: 0.07763876393437386, Final Batch Loss: 0.04311857372522354\n",
      "Epoch 2967, Loss: 0.043195038102567196, Final Batch Loss: 0.014258100651204586\n",
      "Epoch 2968, Loss: 0.02518853358924389, Final Batch Loss: 0.011350282467901707\n",
      "Epoch 2969, Loss: 0.07446389459073544, Final Batch Loss: 0.04404958337545395\n",
      "Epoch 2970, Loss: 0.05377877876162529, Final Batch Loss: 0.023600749671459198\n",
      "Epoch 2971, Loss: 0.041247306391596794, Final Batch Loss: 0.020719509571790695\n",
      "Epoch 2972, Loss: 0.0604590829461813, Final Batch Loss: 0.015887366607785225\n",
      "Epoch 2973, Loss: 0.019462285563349724, Final Batch Loss: 0.007930039428174496\n",
      "Epoch 2974, Loss: 0.07710353285074234, Final Batch Loss: 0.056576624512672424\n",
      "Epoch 2975, Loss: 0.025086652487516403, Final Batch Loss: 0.01954396441578865\n",
      "Epoch 2976, Loss: 0.024592370726168156, Final Batch Loss: 0.0048788124695420265\n",
      "Epoch 2977, Loss: 0.05244632810354233, Final Batch Loss: 0.04148842394351959\n",
      "Epoch 2978, Loss: 0.04527675360441208, Final Batch Loss: 0.00952840968966484\n",
      "Epoch 2979, Loss: 0.034461699426174164, Final Batch Loss: 0.017786750569939613\n",
      "Epoch 2980, Loss: 0.05393587052822113, Final Batch Loss: 0.039949532598257065\n",
      "Epoch 2981, Loss: 0.0718392999842763, Final Batch Loss: 0.009846885688602924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2982, Loss: 0.067826421931386, Final Batch Loss: 0.027570081874728203\n",
      "Epoch 2983, Loss: 0.055368319153785706, Final Batch Loss: 0.03589313104748726\n",
      "Epoch 2984, Loss: 0.041039491072297096, Final Batch Loss: 0.020090479403734207\n",
      "Epoch 2985, Loss: 0.04697325639426708, Final Batch Loss: 0.01827358826994896\n",
      "Epoch 2986, Loss: 0.029971441254019737, Final Batch Loss: 0.0078309066593647\n",
      "Epoch 2987, Loss: 0.045422641560435295, Final Batch Loss: 0.022090740501880646\n",
      "Epoch 2988, Loss: 0.06755595654249191, Final Batch Loss: 0.033299997448921204\n",
      "Epoch 2989, Loss: 0.08406565338373184, Final Batch Loss: 0.04548994079232216\n",
      "Epoch 2990, Loss: 0.04649237636476755, Final Batch Loss: 0.037120748311281204\n",
      "Epoch 2991, Loss: 0.0730627290904522, Final Batch Loss: 0.024739354848861694\n",
      "Epoch 2992, Loss: 0.05040753725916147, Final Batch Loss: 0.006103680469095707\n",
      "Epoch 2993, Loss: 0.06397981941699982, Final Batch Loss: 0.029482722282409668\n",
      "Epoch 2994, Loss: 0.07862208411097527, Final Batch Loss: 0.03410373628139496\n",
      "Epoch 2995, Loss: 0.060953741893172264, Final Batch Loss: 0.012870142236351967\n",
      "Epoch 2996, Loss: 0.0455250283703208, Final Batch Loss: 0.013953222893178463\n",
      "Epoch 2997, Loss: 0.06640395149588585, Final Batch Loss: 0.015034995973110199\n",
      "Epoch 2998, Loss: 0.044558243826031685, Final Batch Loss: 0.010513996705412865\n",
      "Epoch 2999, Loss: 0.049703425727784634, Final Batch Loss: 0.010993189178407192\n",
      "Epoch 3000, Loss: 0.04359876550734043, Final Batch Loss: 0.009061167016625404\n",
      "Epoch 3001, Loss: 0.0767279602587223, Final Batch Loss: 0.012016106396913528\n",
      "Epoch 3002, Loss: 0.032361216843128204, Final Batch Loss: 0.01465708576142788\n",
      "Epoch 3003, Loss: 0.04323790408670902, Final Batch Loss: 0.008485598489642143\n",
      "Epoch 3004, Loss: 0.06454963982105255, Final Batch Loss: 0.039212796837091446\n",
      "Epoch 3005, Loss: 0.05924655869603157, Final Batch Loss: 0.03979936242103577\n",
      "Epoch 3006, Loss: 0.06920290365815163, Final Batch Loss: 0.033030375838279724\n",
      "Epoch 3007, Loss: 0.041557060554623604, Final Batch Loss: 0.024902399629354477\n",
      "Epoch 3008, Loss: 0.07043292932212353, Final Batch Loss: 0.042327526956796646\n",
      "Epoch 3009, Loss: 0.05300111882388592, Final Batch Loss: 0.022144798189401627\n",
      "Epoch 3010, Loss: 0.04433252662420273, Final Batch Loss: 0.026366448029875755\n",
      "Epoch 3011, Loss: 0.07655098661780357, Final Batch Loss: 0.03203793615102768\n",
      "Epoch 3012, Loss: 0.06280398927628994, Final Batch Loss: 0.04026301950216293\n",
      "Epoch 3013, Loss: 0.09046939387917519, Final Batch Loss: 0.029534470289945602\n",
      "Epoch 3014, Loss: 0.031210248358547688, Final Batch Loss: 0.015948189422488213\n",
      "Epoch 3015, Loss: 0.08145378902554512, Final Batch Loss: 0.03597518056631088\n",
      "Epoch 3016, Loss: 0.07669183611869812, Final Batch Loss: 0.05347740277647972\n",
      "Epoch 3017, Loss: 0.055119629949331284, Final Batch Loss: 0.02776014246046543\n",
      "Epoch 3018, Loss: 0.07162918150424957, Final Batch Loss: 0.03401150926947594\n",
      "Epoch 3019, Loss: 0.10791125148534775, Final Batch Loss: 0.05644688010215759\n",
      "Epoch 3020, Loss: 0.05177099630236626, Final Batch Loss: 0.03401483967900276\n",
      "Epoch 3021, Loss: 0.09763141348958015, Final Batch Loss: 0.04997216910123825\n",
      "Epoch 3022, Loss: 0.055214560590684414, Final Batch Loss: 0.014802149496972561\n",
      "Epoch 3023, Loss: 0.057923631742596626, Final Batch Loss: 0.009600209072232246\n",
      "Epoch 3024, Loss: 0.07089327462017536, Final Batch Loss: 0.02787277288734913\n",
      "Epoch 3025, Loss: 0.13889428973197937, Final Batch Loss: 0.07006368041038513\n",
      "Epoch 3026, Loss: 0.10739200189709663, Final Batch Loss: 0.06622131168842316\n",
      "Epoch 3027, Loss: 0.05886872671544552, Final Batch Loss: 0.024989092722535133\n",
      "Epoch 3028, Loss: 0.055439380928874016, Final Batch Loss: 0.02858749032020569\n",
      "Epoch 3029, Loss: 0.0994887463748455, Final Batch Loss: 0.047528889030218124\n",
      "Epoch 3030, Loss: 0.10385045409202576, Final Batch Loss: 0.080388642847538\n",
      "Epoch 3031, Loss: 0.06881225667893887, Final Batch Loss: 0.02546904794871807\n",
      "Epoch 3032, Loss: 0.04198483191430569, Final Batch Loss: 0.011188410222530365\n",
      "Epoch 3033, Loss: 0.08585134986788034, Final Batch Loss: 0.012765795923769474\n",
      "Epoch 3034, Loss: 0.07365907728672028, Final Batch Loss: 0.0266154445707798\n",
      "Epoch 3035, Loss: 0.0924919992685318, Final Batch Loss: 0.0662049949169159\n",
      "Epoch 3036, Loss: 0.052124256268143654, Final Batch Loss: 0.012284236028790474\n",
      "Epoch 3037, Loss: 0.0520306583493948, Final Batch Loss: 0.037591204047203064\n",
      "Epoch 3038, Loss: 0.03558340109884739, Final Batch Loss: 0.020199568942189217\n",
      "Epoch 3039, Loss: 0.07457567937672138, Final Batch Loss: 0.02637152560055256\n",
      "Epoch 3040, Loss: 0.07007605582475662, Final Batch Loss: 0.017485465854406357\n",
      "Epoch 3041, Loss: 0.03799894358962774, Final Batch Loss: 0.01391113642603159\n",
      "Epoch 3042, Loss: 0.03837159648537636, Final Batch Loss: 0.026696834713220596\n",
      "Epoch 3043, Loss: 0.05924118682742119, Final Batch Loss: 0.037192702293395996\n",
      "Epoch 3044, Loss: 0.04005614295601845, Final Batch Loss: 0.017816215753555298\n",
      "Epoch 3045, Loss: 0.032331409864127636, Final Batch Loss: 0.012076170183718204\n",
      "Epoch 3046, Loss: 0.12330715730786324, Final Batch Loss: 0.07067128270864487\n",
      "Epoch 3047, Loss: 0.03849313501268625, Final Batch Loss: 0.012762731872498989\n",
      "Epoch 3048, Loss: 0.11428106017410755, Final Batch Loss: 0.09502741694450378\n",
      "Epoch 3049, Loss: 0.06138112582266331, Final Batch Loss: 0.0317470021545887\n",
      "Epoch 3050, Loss: 0.09903693944215775, Final Batch Loss: 0.05982932448387146\n",
      "Epoch 3051, Loss: 0.06448039785027504, Final Batch Loss: 0.03255128487944603\n",
      "Epoch 3052, Loss: 0.08709998428821564, Final Batch Loss: 0.01611051708459854\n",
      "Epoch 3053, Loss: 0.19328515976667404, Final Batch Loss: 0.0706271231174469\n",
      "Epoch 3054, Loss: 0.141458410769701, Final Batch Loss: 0.05831303820014\n",
      "Epoch 3055, Loss: 0.13237323611974716, Final Batch Loss: 0.06303569674491882\n",
      "Epoch 3056, Loss: 0.12536204233765602, Final Batch Loss: 0.03848537430167198\n",
      "Epoch 3057, Loss: 0.03609689325094223, Final Batch Loss: 0.015758095309138298\n",
      "Epoch 3058, Loss: 0.038946990855038166, Final Batch Loss: 0.0246138758957386\n",
      "Epoch 3059, Loss: 0.061524759978055954, Final Batch Loss: 0.03695561736822128\n",
      "Epoch 3060, Loss: 0.0465429350733757, Final Batch Loss: 0.010188717395067215\n",
      "Epoch 3061, Loss: 0.0855574794113636, Final Batch Loss: 0.04637342318892479\n",
      "Epoch 3062, Loss: 0.035472478717565536, Final Batch Loss: 0.021272826939821243\n",
      "Epoch 3063, Loss: 0.06230812892317772, Final Batch Loss: 0.045304153114557266\n",
      "Epoch 3064, Loss: 0.04324602987617254, Final Batch Loss: 0.012972547672688961\n",
      "Epoch 3065, Loss: 0.05442281626164913, Final Batch Loss: 0.02694668062031269\n",
      "Epoch 3066, Loss: 0.07003411278128624, Final Batch Loss: 0.03674425557255745\n",
      "Epoch 3067, Loss: 0.07564980909228325, Final Batch Loss: 0.03326975926756859\n",
      "Epoch 3068, Loss: 0.0819842778146267, Final Batch Loss: 0.0627937838435173\n",
      "Epoch 3069, Loss: 0.044749293476343155, Final Batch Loss: 0.025787891820073128\n",
      "Epoch 3070, Loss: 0.06382807157933712, Final Batch Loss: 0.017401481047272682\n",
      "Epoch 3071, Loss: 0.08320123516023159, Final Batch Loss: 0.06361604481935501\n",
      "Epoch 3072, Loss: 0.06614908948540688, Final Batch Loss: 0.049110379070043564\n",
      "Epoch 3073, Loss: 0.08130358904600143, Final Batch Loss: 0.03368022292852402\n",
      "Epoch 3074, Loss: 0.13226963207125664, Final Batch Loss: 0.10447453707456589\n",
      "Epoch 3075, Loss: 0.10634531080722809, Final Batch Loss: 0.07017207890748978\n",
      "Epoch 3076, Loss: 0.11119886487722397, Final Batch Loss: 0.056243497878313065\n",
      "Epoch 3077, Loss: 0.12523231282830238, Final Batch Loss: 0.05122086778283119\n",
      "Epoch 3078, Loss: 0.11695059761404991, Final Batch Loss: 0.06243569403886795\n",
      "Epoch 3079, Loss: 0.1251668930053711, Final Batch Loss: 0.03717656433582306\n",
      "Epoch 3080, Loss: 0.09174146875739098, Final Batch Loss: 0.05926719307899475\n",
      "Epoch 3081, Loss: 0.06954817846417427, Final Batch Loss: 0.04333585128188133\n",
      "Epoch 3082, Loss: 0.051997365429997444, Final Batch Loss: 0.023912763223052025\n",
      "Epoch 3083, Loss: 0.08645235002040863, Final Batch Loss: 0.03944465517997742\n",
      "Epoch 3084, Loss: 0.09584787115454674, Final Batch Loss: 0.06304694712162018\n",
      "Epoch 3085, Loss: 0.08109450712800026, Final Batch Loss: 0.029799524694681168\n",
      "Epoch 3086, Loss: 0.084513820707798, Final Batch Loss: 0.050072792917490005\n",
      "Epoch 3087, Loss: 0.08591814339160919, Final Batch Loss: 0.041090648621320724\n",
      "Epoch 3088, Loss: 0.04978465475142002, Final Batch Loss: 0.010306289419531822\n",
      "Epoch 3089, Loss: 0.060403309762477875, Final Batch Loss: 0.020957767963409424\n",
      "Epoch 3090, Loss: 0.11470069736242294, Final Batch Loss: 0.05902114138007164\n",
      "Epoch 3091, Loss: 0.061765797436237335, Final Batch Loss: 0.03884541615843773\n",
      "Epoch 3092, Loss: 0.07852858677506447, Final Batch Loss: 0.01932011917233467\n",
      "Epoch 3093, Loss: 0.05254119075834751, Final Batch Loss: 0.03361120820045471\n",
      "Epoch 3094, Loss: 0.09319014847278595, Final Batch Loss: 0.06100134551525116\n",
      "Epoch 3095, Loss: 0.09581414982676506, Final Batch Loss: 0.05657405033707619\n",
      "Epoch 3096, Loss: 0.04855700023472309, Final Batch Loss: 0.02036130614578724\n",
      "Epoch 3097, Loss: 0.10667162388563156, Final Batch Loss: 0.07000856846570969\n",
      "Epoch 3098, Loss: 0.03887950163334608, Final Batch Loss: 0.009988515637814999\n",
      "Epoch 3099, Loss: 0.16439244151115417, Final Batch Loss: 0.12834009528160095\n",
      "Epoch 3100, Loss: 0.10955769568681717, Final Batch Loss: 0.061829227954149246\n",
      "Epoch 3101, Loss: 0.05032142996788025, Final Batch Loss: 0.019654136151075363\n",
      "Epoch 3102, Loss: 0.06468183360993862, Final Batch Loss: 0.01847979985177517\n",
      "Epoch 3103, Loss: 0.15007421374320984, Final Batch Loss: 0.06986358016729355\n",
      "Epoch 3104, Loss: 0.047387007623910904, Final Batch Loss: 0.009922858327627182\n",
      "Epoch 3105, Loss: 0.06659236922860146, Final Batch Loss: 0.0427849143743515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3106, Loss: 0.09156295284628868, Final Batch Loss: 0.03167055547237396\n",
      "Epoch 3107, Loss: 0.06369818933308125, Final Batch Loss: 0.018732724711298943\n",
      "Epoch 3108, Loss: 0.057242248207330704, Final Batch Loss: 0.014470323920249939\n",
      "Epoch 3109, Loss: 0.061582667753100395, Final Batch Loss: 0.020040711387991905\n",
      "Epoch 3110, Loss: 0.09237141907215118, Final Batch Loss: 0.03055494651198387\n",
      "Epoch 3111, Loss: 0.0431519765406847, Final Batch Loss: 0.02475769817829132\n",
      "Epoch 3112, Loss: 0.08157394453883171, Final Batch Loss: 0.05242205783724785\n",
      "Epoch 3113, Loss: 0.06279933825135231, Final Batch Loss: 0.04566855728626251\n",
      "Epoch 3114, Loss: 0.07393491640686989, Final Batch Loss: 0.035108741372823715\n",
      "Epoch 3115, Loss: 0.08068517968058586, Final Batch Loss: 0.019371096044778824\n",
      "Epoch 3116, Loss: 0.08328482322394848, Final Batch Loss: 0.021658414974808693\n",
      "Epoch 3117, Loss: 0.13258341327309608, Final Batch Loss: 0.07898522913455963\n",
      "Epoch 3118, Loss: 0.11230760253965855, Final Batch Loss: 0.026937047019600868\n",
      "Epoch 3119, Loss: 0.07690086960792542, Final Batch Loss: 0.01623750850558281\n",
      "Epoch 3120, Loss: 0.08890101313591003, Final Batch Loss: 0.04137043654918671\n",
      "Epoch 3121, Loss: 0.09317770414054394, Final Batch Loss: 0.026192909106612206\n",
      "Epoch 3122, Loss: 0.0803225040435791, Final Batch Loss: 0.05059550702571869\n",
      "Epoch 3123, Loss: 0.05850301496684551, Final Batch Loss: 0.028266113251447678\n",
      "Epoch 3124, Loss: 0.05816973280161619, Final Batch Loss: 0.008974074386060238\n",
      "Epoch 3125, Loss: 0.05357307009398937, Final Batch Loss: 0.020519236102700233\n",
      "Epoch 3126, Loss: 0.06021888181567192, Final Batch Loss: 0.01982094720005989\n",
      "Epoch 3127, Loss: 0.039217278361320496, Final Batch Loss: 0.017374683171510696\n",
      "Epoch 3128, Loss: 0.06287246197462082, Final Batch Loss: 0.04111392796039581\n",
      "Epoch 3129, Loss: 0.04114930145442486, Final Batch Loss: 0.015991734340786934\n",
      "Epoch 3130, Loss: 0.08349723555147648, Final Batch Loss: 0.06729291379451752\n",
      "Epoch 3131, Loss: 0.043285978958010674, Final Batch Loss: 0.02709454484283924\n",
      "Epoch 3132, Loss: 0.08786710910499096, Final Batch Loss: 0.07259386777877808\n",
      "Epoch 3133, Loss: 0.07640857808291912, Final Batch Loss: 0.05239780992269516\n",
      "Epoch 3134, Loss: 0.08682611584663391, Final Batch Loss: 0.05564499273896217\n",
      "Epoch 3135, Loss: 0.07650130800902843, Final Batch Loss: 0.049171753227710724\n",
      "Epoch 3136, Loss: 0.07820872217416763, Final Batch Loss: 0.049327101558446884\n",
      "Epoch 3137, Loss: 0.04368007183074951, Final Batch Loss: 0.024674635380506516\n",
      "Epoch 3138, Loss: 0.07039083167910576, Final Batch Loss: 0.036199670284986496\n",
      "Epoch 3139, Loss: 0.05524105206131935, Final Batch Loss: 0.04134688898921013\n",
      "Epoch 3140, Loss: 0.07635442912578583, Final Batch Loss: 0.03397262468934059\n",
      "Epoch 3141, Loss: 0.07506183907389641, Final Batch Loss: 0.03927520662546158\n",
      "Epoch 3142, Loss: 0.03826226852834225, Final Batch Loss: 0.019287673756480217\n",
      "Epoch 3143, Loss: 0.1266465075314045, Final Batch Loss: 0.05905033275485039\n",
      "Epoch 3144, Loss: 0.03041886631399393, Final Batch Loss: 0.0078074736520648\n",
      "Epoch 3145, Loss: 0.04146426264196634, Final Batch Loss: 0.009504110552370548\n",
      "Epoch 3146, Loss: 0.07819576561450958, Final Batch Loss: 0.04137216508388519\n",
      "Epoch 3147, Loss: 0.054439776577055454, Final Batch Loss: 0.039430808275938034\n",
      "Epoch 3148, Loss: 0.03513349033892155, Final Batch Loss: 0.025849604979157448\n",
      "Epoch 3149, Loss: 0.04642242752015591, Final Batch Loss: 0.015914946794509888\n",
      "Epoch 3150, Loss: 0.10047737136483192, Final Batch Loss: 0.07626160234212875\n",
      "Epoch 3151, Loss: 0.0686256755143404, Final Batch Loss: 0.05255407467484474\n",
      "Epoch 3152, Loss: 0.04470244329422712, Final Batch Loss: 0.010611000470817089\n",
      "Epoch 3153, Loss: 0.07010441832244396, Final Batch Loss: 0.02847151644527912\n",
      "Epoch 3154, Loss: 0.061567364260554314, Final Batch Loss: 0.029214775189757347\n",
      "Epoch 3155, Loss: 0.05314837861806154, Final Batch Loss: 0.012969409115612507\n",
      "Epoch 3156, Loss: 0.05219932459294796, Final Batch Loss: 0.019122188910841942\n",
      "Epoch 3157, Loss: 0.05552434641867876, Final Batch Loss: 0.04454204812645912\n",
      "Epoch 3158, Loss: 0.06462730094790459, Final Batch Loss: 0.030580788850784302\n",
      "Epoch 3159, Loss: 0.038713094778358936, Final Batch Loss: 0.011621997691690922\n",
      "Epoch 3160, Loss: 0.06261942442506552, Final Batch Loss: 0.04844396561384201\n",
      "Epoch 3161, Loss: 0.035265419632196426, Final Batch Loss: 0.010862203314900398\n",
      "Epoch 3162, Loss: 0.07113638892769814, Final Batch Loss: 0.051361069083213806\n",
      "Epoch 3163, Loss: 0.06243995577096939, Final Batch Loss: 0.029111795127391815\n",
      "Epoch 3164, Loss: 0.06672904454171658, Final Batch Loss: 0.041069988161325455\n",
      "Epoch 3165, Loss: 0.07288594171404839, Final Batch Loss: 0.038305602967739105\n",
      "Epoch 3166, Loss: 0.036312755197286606, Final Batch Loss: 0.02311314269900322\n",
      "Epoch 3167, Loss: 0.034424472600221634, Final Batch Loss: 0.013133218511939049\n",
      "Epoch 3168, Loss: 0.07044872269034386, Final Batch Loss: 0.03547285869717598\n",
      "Epoch 3169, Loss: 0.06790458969771862, Final Batch Loss: 0.030473368242383003\n",
      "Epoch 3170, Loss: 0.04232235439121723, Final Batch Loss: 0.023419152945280075\n",
      "Epoch 3171, Loss: 0.05112592317163944, Final Batch Loss: 0.037389907985925674\n",
      "Epoch 3172, Loss: 0.033865392208099365, Final Batch Loss: 0.010845087468624115\n",
      "Epoch 3173, Loss: 0.04705100134015083, Final Batch Loss: 0.027094414457678795\n",
      "Epoch 3174, Loss: 0.05332206375896931, Final Batch Loss: 0.02210972271859646\n",
      "Epoch 3175, Loss: 0.0758038368076086, Final Batch Loss: 0.051233917474746704\n",
      "Epoch 3176, Loss: 0.06489468738436699, Final Batch Loss: 0.04591638967394829\n",
      "Epoch 3177, Loss: 0.029575315304100513, Final Batch Loss: 0.013307514600455761\n",
      "Epoch 3178, Loss: 0.0837320052087307, Final Batch Loss: 0.04899754002690315\n",
      "Epoch 3179, Loss: 0.08358584716916084, Final Batch Loss: 0.022917043417692184\n",
      "Epoch 3180, Loss: 0.018539410084486008, Final Batch Loss: 0.00822458602488041\n",
      "Epoch 3181, Loss: 0.041920166462659836, Final Batch Loss: 0.025842273607850075\n",
      "Epoch 3182, Loss: 0.04086424224078655, Final Batch Loss: 0.010900033637881279\n",
      "Epoch 3183, Loss: 0.03785447869449854, Final Batch Loss: 0.028350891545414925\n",
      "Epoch 3184, Loss: 0.051911285147070885, Final Batch Loss: 0.03578568249940872\n",
      "Epoch 3185, Loss: 0.050399746745824814, Final Batch Loss: 0.042047660797834396\n",
      "Epoch 3186, Loss: 0.0495122242718935, Final Batch Loss: 0.017017362639307976\n",
      "Epoch 3187, Loss: 0.12081199325621128, Final Batch Loss: 0.10505883395671844\n",
      "Epoch 3188, Loss: 0.03611212968826294, Final Batch Loss: 0.01911274529993534\n",
      "Epoch 3189, Loss: 0.04498992767184973, Final Batch Loss: 0.008958487771451473\n",
      "Epoch 3190, Loss: 0.06583448126912117, Final Batch Loss: 0.02160530537366867\n",
      "Epoch 3191, Loss: 0.052105991169810295, Final Batch Loss: 0.032562505453825\n",
      "Epoch 3192, Loss: 0.01964942878112197, Final Batch Loss: 0.015057103708386421\n",
      "Epoch 3193, Loss: 0.058906939812004566, Final Batch Loss: 0.04980722814798355\n",
      "Epoch 3194, Loss: 0.05053954757750034, Final Batch Loss: 0.03157375752925873\n",
      "Epoch 3195, Loss: 0.039159540086984634, Final Batch Loss: 0.02542220801115036\n",
      "Epoch 3196, Loss: 0.02842104062438011, Final Batch Loss: 0.014616805128753185\n",
      "Epoch 3197, Loss: 0.024336591362953186, Final Batch Loss: 0.00863281637430191\n",
      "Epoch 3198, Loss: 0.07535516656935215, Final Batch Loss: 0.047314248979091644\n",
      "Epoch 3199, Loss: 0.07925891503691673, Final Batch Loss: 0.026286449283361435\n",
      "Epoch 3200, Loss: 0.0534429419785738, Final Batch Loss: 0.018580297008156776\n",
      "Epoch 3201, Loss: 0.0687938816845417, Final Batch Loss: 0.04325536638498306\n",
      "Epoch 3202, Loss: 0.049790896475315094, Final Batch Loss: 0.03832244500517845\n",
      "Epoch 3203, Loss: 0.05222591757774353, Final Batch Loss: 0.015569839626550674\n",
      "Epoch 3204, Loss: 0.05456717126071453, Final Batch Loss: 0.026189463213086128\n",
      "Epoch 3205, Loss: 0.04077637195587158, Final Batch Loss: 0.021812545135617256\n",
      "Epoch 3206, Loss: 0.05556280352175236, Final Batch Loss: 0.03434290736913681\n",
      "Epoch 3207, Loss: 0.07873132452368736, Final Batch Loss: 0.022514861077070236\n",
      "Epoch 3208, Loss: 0.04292759578675032, Final Batch Loss: 0.030435755848884583\n",
      "Epoch 3209, Loss: 0.050446925684809685, Final Batch Loss: 0.026464099064469337\n",
      "Epoch 3210, Loss: 0.03339705802500248, Final Batch Loss: 0.0221731998026371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3211, Loss: 0.059105902910232544, Final Batch Loss: 0.04004767909646034\n",
      "Epoch 3212, Loss: 0.03904865402728319, Final Batch Loss: 0.026283008977770805\n",
      "Epoch 3213, Loss: 0.0775309968739748, Final Batch Loss: 0.04822671040892601\n",
      "Epoch 3214, Loss: 0.03523951768875122, Final Batch Loss: 0.011931667104363441\n",
      "Epoch 3215, Loss: 0.03681394271552563, Final Batch Loss: 0.013858318328857422\n",
      "Epoch 3216, Loss: 0.046583887189626694, Final Batch Loss: 0.0293768011033535\n",
      "Epoch 3217, Loss: 0.06355103105306625, Final Batch Loss: 0.0414373017847538\n",
      "Epoch 3218, Loss: 0.05464933440089226, Final Batch Loss: 0.020787030458450317\n",
      "Epoch 3219, Loss: 0.06662547029554844, Final Batch Loss: 0.04763389006257057\n",
      "Epoch 3220, Loss: 0.05743647925555706, Final Batch Loss: 0.022937795147299767\n",
      "Epoch 3221, Loss: 0.041636332869529724, Final Batch Loss: 0.021802889183163643\n",
      "Epoch 3222, Loss: 0.05233219638466835, Final Batch Loss: 0.01451178640127182\n",
      "Epoch 3223, Loss: 0.03485789429396391, Final Batch Loss: 0.012523035518825054\n",
      "Epoch 3224, Loss: 0.02182573825120926, Final Batch Loss: 0.00832552369683981\n",
      "Epoch 3225, Loss: 0.02628849959000945, Final Batch Loss: 0.006854071747511625\n",
      "Epoch 3226, Loss: 0.01446380140259862, Final Batch Loss: 0.0066089932806789875\n",
      "Epoch 3227, Loss: 0.035346413031220436, Final Batch Loss: 0.019410202279686928\n",
      "Epoch 3228, Loss: 0.03929779212921858, Final Batch Loss: 0.02724253572523594\n",
      "Epoch 3229, Loss: 0.06110135279595852, Final Batch Loss: 0.023689361289143562\n",
      "Epoch 3230, Loss: 0.06208637170493603, Final Batch Loss: 0.017894519492983818\n",
      "Epoch 3231, Loss: 0.08565822429955006, Final Batch Loss: 0.057827096432447433\n",
      "Epoch 3232, Loss: 0.030941727571189404, Final Batch Loss: 0.012623726390302181\n",
      "Epoch 3233, Loss: 0.023128901608288288, Final Batch Loss: 0.010863371193408966\n",
      "Epoch 3234, Loss: 0.03377327974885702, Final Batch Loss: 0.014198512770235538\n",
      "Epoch 3235, Loss: 0.11090729758143425, Final Batch Loss: 0.06493287533521652\n",
      "Epoch 3236, Loss: 0.02992411982268095, Final Batch Loss: 0.008407480083405972\n",
      "Epoch 3237, Loss: 0.08697402104735374, Final Batch Loss: 0.03323214873671532\n",
      "Epoch 3238, Loss: 0.056294551119208336, Final Batch Loss: 0.020149512216448784\n",
      "Epoch 3239, Loss: 0.05506474711000919, Final Batch Loss: 0.03447779640555382\n",
      "Epoch 3240, Loss: 0.050064871087670326, Final Batch Loss: 0.025970730930566788\n",
      "Epoch 3241, Loss: 0.0267040291801095, Final Batch Loss: 0.012788926251232624\n",
      "Epoch 3242, Loss: 0.04699249006807804, Final Batch Loss: 0.03739034757018089\n",
      "Epoch 3243, Loss: 0.013307827524840832, Final Batch Loss: 0.006570789031684399\n",
      "Epoch 3244, Loss: 0.07057887688279152, Final Batch Loss: 0.027370691299438477\n",
      "Epoch 3245, Loss: 0.0366382822394371, Final Batch Loss: 0.004973597824573517\n",
      "Epoch 3246, Loss: 0.05534159857779741, Final Batch Loss: 0.013229520060122013\n",
      "Epoch 3247, Loss: 0.04291940340772271, Final Batch Loss: 0.004577810410410166\n",
      "Epoch 3248, Loss: 0.06557622831314802, Final Batch Loss: 0.014968371950089931\n",
      "Epoch 3249, Loss: 0.13133727759122849, Final Batch Loss: 0.09624044597148895\n",
      "Epoch 3250, Loss: 0.0761711597442627, Final Batch Loss: 0.03460998088121414\n",
      "Epoch 3251, Loss: 0.03673167433589697, Final Batch Loss: 0.025282468646764755\n",
      "Epoch 3252, Loss: 0.05344174616038799, Final Batch Loss: 0.02283049374818802\n",
      "Epoch 3253, Loss: 0.02270058309659362, Final Batch Loss: 0.007702197413891554\n",
      "Epoch 3254, Loss: 0.058563013561069965, Final Batch Loss: 0.010646791197359562\n",
      "Epoch 3255, Loss: 0.05203728377819061, Final Batch Loss: 0.03718431293964386\n",
      "Epoch 3256, Loss: 0.10455526411533356, Final Batch Loss: 0.08747551590204239\n",
      "Epoch 3257, Loss: 0.04861323721706867, Final Batch Loss: 0.01837669126689434\n",
      "Epoch 3258, Loss: 0.06231318786740303, Final Batch Loss: 0.027610190212726593\n",
      "Epoch 3259, Loss: 0.03966164868324995, Final Batch Loss: 0.008157460950314999\n",
      "Epoch 3260, Loss: 0.016839402727782726, Final Batch Loss: 0.007706805132329464\n",
      "Epoch 3261, Loss: 0.054567232728004456, Final Batch Loss: 0.00793469324707985\n",
      "Epoch 3262, Loss: 0.04404734447598457, Final Batch Loss: 0.010618310421705246\n",
      "Epoch 3263, Loss: 0.09895416349172592, Final Batch Loss: 0.07566303014755249\n",
      "Epoch 3264, Loss: 0.04565514624118805, Final Batch Loss: 0.011657275259494781\n",
      "Epoch 3265, Loss: 0.06672132760286331, Final Batch Loss: 0.037609539926052094\n",
      "Epoch 3266, Loss: 0.06778537575155497, Final Batch Loss: 0.015369580127298832\n",
      "Epoch 3267, Loss: 0.037946008145809174, Final Batch Loss: 0.017360873520374298\n",
      "Epoch 3268, Loss: 0.08264283742755651, Final Batch Loss: 0.0709104835987091\n",
      "Epoch 3269, Loss: 0.02628911565989256, Final Batch Loss: 0.00869439821690321\n",
      "Epoch 3270, Loss: 0.04939940385520458, Final Batch Loss: 0.04075227677822113\n",
      "Epoch 3271, Loss: 0.11579706892371178, Final Batch Loss: 0.07237062603235245\n",
      "Epoch 3272, Loss: 0.0943012498319149, Final Batch Loss: 0.03698241710662842\n",
      "Epoch 3273, Loss: 0.046466726809740067, Final Batch Loss: 0.0174314696341753\n",
      "Epoch 3274, Loss: 0.03171172644942999, Final Batch Loss: 0.011310667730867863\n",
      "Epoch 3275, Loss: 0.04356572311371565, Final Batch Loss: 0.010076112113893032\n",
      "Epoch 3276, Loss: 0.08676844276487827, Final Batch Loss: 0.02287633903324604\n",
      "Epoch 3277, Loss: 0.055125702172517776, Final Batch Loss: 0.03071206994354725\n",
      "Epoch 3278, Loss: 0.033591791056096554, Final Batch Loss: 0.007863075472414494\n",
      "Epoch 3279, Loss: 0.07547128945589066, Final Batch Loss: 0.038037918508052826\n",
      "Epoch 3280, Loss: 0.020051279105246067, Final Batch Loss: 0.008459290489554405\n",
      "Epoch 3281, Loss: 0.04139084555208683, Final Batch Loss: 0.023777466267347336\n",
      "Epoch 3282, Loss: 0.09491410478949547, Final Batch Loss: 0.08481355756521225\n",
      "Epoch 3283, Loss: 0.03915250767022371, Final Batch Loss: 0.015010136179625988\n",
      "Epoch 3284, Loss: 0.06011861562728882, Final Batch Loss: 0.03779389709234238\n",
      "Epoch 3285, Loss: 0.03653675317764282, Final Batch Loss: 0.017835533246397972\n",
      "Epoch 3286, Loss: 0.03438742458820343, Final Batch Loss: 0.019374003633856773\n",
      "Epoch 3287, Loss: 0.06933380756527185, Final Batch Loss: 0.05657489225268364\n",
      "Epoch 3288, Loss: 0.11194927617907524, Final Batch Loss: 0.06651437282562256\n",
      "Epoch 3289, Loss: 0.051101768389344215, Final Batch Loss: 0.024684643372893333\n",
      "Epoch 3290, Loss: 0.0698103066533804, Final Batch Loss: 0.04105227068066597\n",
      "Epoch 3291, Loss: 0.07781326957046986, Final Batch Loss: 0.02089160494506359\n",
      "Epoch 3292, Loss: 0.04563853144645691, Final Batch Loss: 0.026611527428030968\n",
      "Epoch 3293, Loss: 0.08742362447082996, Final Batch Loss: 0.02815425582230091\n",
      "Epoch 3294, Loss: 0.08121485635638237, Final Batch Loss: 0.04480971768498421\n",
      "Epoch 3295, Loss: 0.05415294133126736, Final Batch Loss: 0.01812526024878025\n",
      "Epoch 3296, Loss: 0.11427513882517815, Final Batch Loss: 0.08172600716352463\n",
      "Epoch 3297, Loss: 0.07908593863248825, Final Batch Loss: 0.045697636902332306\n",
      "Epoch 3298, Loss: 0.08390148356556892, Final Batch Loss: 0.03254124894738197\n",
      "Epoch 3299, Loss: 0.08762340433895588, Final Batch Loss: 0.028510501608252525\n",
      "Epoch 3300, Loss: 0.08814122155308723, Final Batch Loss: 0.0646945908665657\n",
      "Epoch 3301, Loss: 0.0473607461899519, Final Batch Loss: 0.015772441402077675\n",
      "Epoch 3302, Loss: 0.056437764316797256, Final Batch Loss: 0.019728392362594604\n",
      "Epoch 3303, Loss: 0.09907511994242668, Final Batch Loss: 0.04030513018369675\n",
      "Epoch 3304, Loss: 0.05512083135545254, Final Batch Loss: 0.03766918182373047\n",
      "Epoch 3305, Loss: 0.031967559363693, Final Batch Loss: 0.005382900591939688\n",
      "Epoch 3306, Loss: 0.06475397944450378, Final Batch Loss: 0.025949131697416306\n",
      "Epoch 3307, Loss: 0.0789528489112854, Final Batch Loss: 0.030138198286294937\n",
      "Epoch 3308, Loss: 0.08896827138960361, Final Batch Loss: 0.026935243979096413\n",
      "Epoch 3309, Loss: 0.04064441844820976, Final Batch Loss: 0.017330411821603775\n",
      "Epoch 3310, Loss: 0.07420349679887295, Final Batch Loss: 0.04573431238532066\n",
      "Epoch 3311, Loss: 0.041888948529958725, Final Batch Loss: 0.01149311289191246\n",
      "Epoch 3312, Loss: 0.04591079428792, Final Batch Loss: 0.0268265288323164\n",
      "Epoch 3313, Loss: 0.060196276754140854, Final Batch Loss: 0.02264835312962532\n",
      "Epoch 3314, Loss: 0.05719035118818283, Final Batch Loss: 0.03264793008565903\n",
      "Epoch 3315, Loss: 0.03607231192290783, Final Batch Loss: 0.01777065545320511\n",
      "Epoch 3316, Loss: 0.09690266102552414, Final Batch Loss: 0.06486759334802628\n",
      "Epoch 3317, Loss: 0.05974033288657665, Final Batch Loss: 0.050832394510507584\n",
      "Epoch 3318, Loss: 0.047432372346520424, Final Batch Loss: 0.027530426159501076\n",
      "Epoch 3319, Loss: 0.0770455002784729, Final Batch Loss: 0.03564564138650894\n",
      "Epoch 3320, Loss: 0.03487187344580889, Final Batch Loss: 0.026659073308110237\n",
      "Epoch 3321, Loss: 0.06136025860905647, Final Batch Loss: 0.02763637900352478\n",
      "Epoch 3322, Loss: 0.04569849371910095, Final Batch Loss: 0.02012266032397747\n",
      "Epoch 3323, Loss: 0.07906776666641235, Final Batch Loss: 0.03541291505098343\n",
      "Epoch 3324, Loss: 0.05021458491683006, Final Batch Loss: 0.026300905272364616\n",
      "Epoch 3325, Loss: 0.051920171827077866, Final Batch Loss: 0.016914252191781998\n",
      "Epoch 3326, Loss: 0.15243787690997124, Final Batch Loss: 0.10112971067428589\n",
      "Epoch 3327, Loss: 0.05141575448215008, Final Batch Loss: 0.015432732179760933\n",
      "Epoch 3328, Loss: 0.06574074365198612, Final Batch Loss: 0.019672201946377754\n",
      "Epoch 3329, Loss: 0.051297396421432495, Final Batch Loss: 0.014739666134119034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3330, Loss: 0.0899042934179306, Final Batch Loss: 0.053538378328084946\n",
      "Epoch 3331, Loss: 0.11225656047463417, Final Batch Loss: 0.07760105282068253\n",
      "Epoch 3332, Loss: 0.07437746971845627, Final Batch Loss: 0.037966348230838776\n",
      "Epoch 3333, Loss: 0.07852647826075554, Final Batch Loss: 0.01967884972691536\n",
      "Epoch 3334, Loss: 0.05870047677308321, Final Batch Loss: 0.04332641139626503\n",
      "Epoch 3335, Loss: 0.03506372403353453, Final Batch Loss: 0.014532850123941898\n",
      "Epoch 3336, Loss: 0.12916111759841442, Final Batch Loss: 0.10126443207263947\n",
      "Epoch 3337, Loss: 0.057115075178444386, Final Batch Loss: 0.0047340551391243935\n",
      "Epoch 3338, Loss: 0.0373891880735755, Final Batch Loss: 0.014168721623718739\n",
      "Epoch 3339, Loss: 0.0574884507805109, Final Batch Loss: 0.030643707141280174\n",
      "Epoch 3340, Loss: 0.09698944166302681, Final Batch Loss: 0.06429951637983322\n",
      "Epoch 3341, Loss: 0.04935141187161207, Final Batch Loss: 0.009173321537673473\n",
      "Epoch 3342, Loss: 0.13265053927898407, Final Batch Loss: 0.06619323790073395\n",
      "Epoch 3343, Loss: 0.08913883753120899, Final Batch Loss: 0.06730446964502335\n",
      "Epoch 3344, Loss: 0.06792549788951874, Final Batch Loss: 0.03433702886104584\n",
      "Epoch 3345, Loss: 0.13051837682724, Final Batch Loss: 0.060645684599876404\n",
      "Epoch 3346, Loss: 0.034818087704479694, Final Batch Loss: 0.009269309230148792\n",
      "Epoch 3347, Loss: 0.03221219405531883, Final Batch Loss: 0.016224700957536697\n",
      "Epoch 3348, Loss: 0.061114486306905746, Final Batch Loss: 0.03184127435088158\n",
      "Epoch 3349, Loss: 0.08307794248685241, Final Batch Loss: 0.004712333437055349\n",
      "Epoch 3350, Loss: 0.05433802865445614, Final Batch Loss: 0.028574422001838684\n",
      "Epoch 3351, Loss: 0.057187811471521854, Final Batch Loss: 0.014660772867500782\n",
      "Epoch 3352, Loss: 0.07262397557497025, Final Batch Loss: 0.04029585421085358\n",
      "Epoch 3353, Loss: 0.04435332864522934, Final Batch Loss: 0.02528291568160057\n",
      "Epoch 3354, Loss: 0.07890408486127853, Final Batch Loss: 0.049115825444459915\n",
      "Epoch 3355, Loss: 0.06475445069372654, Final Batch Loss: 0.022082647308707237\n",
      "Epoch 3356, Loss: 0.04600663762539625, Final Batch Loss: 0.010044286958873272\n",
      "Epoch 3357, Loss: 0.030519980937242508, Final Batch Loss: 0.01649634912610054\n",
      "Epoch 3358, Loss: 0.031314547173678875, Final Batch Loss: 0.018522139638662338\n",
      "Epoch 3359, Loss: 0.04156077839434147, Final Batch Loss: 0.013630498200654984\n",
      "Epoch 3360, Loss: 0.05448492616415024, Final Batch Loss: 0.03838468715548515\n",
      "Epoch 3361, Loss: 0.06376598589122295, Final Batch Loss: 0.024899551644921303\n",
      "Epoch 3362, Loss: 0.05791805684566498, Final Batch Loss: 0.018824107944965363\n",
      "Epoch 3363, Loss: 0.0779648870229721, Final Batch Loss: 0.052537381649017334\n",
      "Epoch 3364, Loss: 0.08143261633813381, Final Batch Loss: 0.05890117585659027\n",
      "Epoch 3365, Loss: 0.05589567869901657, Final Batch Loss: 0.04049088433384895\n",
      "Epoch 3366, Loss: 0.08720134664326906, Final Batch Loss: 0.0755087286233902\n",
      "Epoch 3367, Loss: 0.03998390305787325, Final Batch Loss: 0.010972036980092525\n",
      "Epoch 3368, Loss: 0.03438113536685705, Final Batch Loss: 0.00933064054697752\n",
      "Epoch 3369, Loss: 0.03732532449066639, Final Batch Loss: 0.016708046197891235\n",
      "Epoch 3370, Loss: 0.10430992394685745, Final Batch Loss: 0.07266201823949814\n",
      "Epoch 3371, Loss: 0.07224328629672527, Final Batch Loss: 0.04659133777022362\n",
      "Epoch 3372, Loss: 0.05223420821130276, Final Batch Loss: 0.035155028104782104\n",
      "Epoch 3373, Loss: 0.04455982614308596, Final Batch Loss: 0.012299307622015476\n",
      "Epoch 3374, Loss: 0.03237421344965696, Final Batch Loss: 0.014664019457995892\n",
      "Epoch 3375, Loss: 0.08903066255152225, Final Batch Loss: 0.061791207641363144\n",
      "Epoch 3376, Loss: 0.028070062398910522, Final Batch Loss: 0.017278628423810005\n",
      "Epoch 3377, Loss: 0.033036426641047, Final Batch Loss: 0.012195915915071964\n",
      "Epoch 3378, Loss: 0.06380104273557663, Final Batch Loss: 0.04145648330450058\n",
      "Epoch 3379, Loss: 0.03554079681634903, Final Batch Loss: 0.019750798121094704\n",
      "Epoch 3380, Loss: 0.040608338080346584, Final Batch Loss: 0.027118977159261703\n",
      "Epoch 3381, Loss: 0.037141287699341774, Final Batch Loss: 0.022071311250329018\n",
      "Epoch 3382, Loss: 0.03851385693997145, Final Batch Loss: 0.023688049986958504\n",
      "Epoch 3383, Loss: 0.0441704373806715, Final Batch Loss: 0.02569729834794998\n",
      "Epoch 3384, Loss: 0.07866277731955051, Final Batch Loss: 0.030944501981139183\n",
      "Epoch 3385, Loss: 0.14189374074339867, Final Batch Loss: 0.09379717707633972\n",
      "Epoch 3386, Loss: 0.04166391305625439, Final Batch Loss: 0.028615711256861687\n",
      "Epoch 3387, Loss: 0.06590991839766502, Final Batch Loss: 0.03509881719946861\n",
      "Epoch 3388, Loss: 0.03033373598009348, Final Batch Loss: 0.008237737230956554\n",
      "Epoch 3389, Loss: 0.10389045812189579, Final Batch Loss: 0.07454203814268112\n",
      "Epoch 3390, Loss: 0.028131027705967426, Final Batch Loss: 0.013323338702321053\n",
      "Epoch 3391, Loss: 0.06969844736158848, Final Batch Loss: 0.027550196275115013\n",
      "Epoch 3392, Loss: 0.02497483603656292, Final Batch Loss: 0.015241142362356186\n",
      "Epoch 3393, Loss: 0.1005777083337307, Final Batch Loss: 0.06658945232629776\n",
      "Epoch 3394, Loss: 0.0459424052387476, Final Batch Loss: 0.015730680897831917\n",
      "Epoch 3395, Loss: 0.024044611491262913, Final Batch Loss: 0.011367373168468475\n",
      "Epoch 3396, Loss: 0.07723824959248304, Final Batch Loss: 0.06295911222696304\n",
      "Epoch 3397, Loss: 0.034850673750042915, Final Batch Loss: 0.008564600721001625\n",
      "Epoch 3398, Loss: 0.0788678415119648, Final Batch Loss: 0.03665926679968834\n",
      "Epoch 3399, Loss: 0.06497770547866821, Final Batch Loss: 0.03581448644399643\n",
      "Epoch 3400, Loss: 0.07972878217697144, Final Batch Loss: 0.024425558745861053\n",
      "Epoch 3401, Loss: 0.04535314440727234, Final Batch Loss: 0.00791209563612938\n",
      "Epoch 3402, Loss: 0.06816210597753525, Final Batch Loss: 0.046054769307374954\n",
      "Epoch 3403, Loss: 0.018875398207455873, Final Batch Loss: 0.007199079263955355\n",
      "Epoch 3404, Loss: 0.08092702925205231, Final Batch Loss: 0.03348648175597191\n",
      "Epoch 3405, Loss: 0.07489818707108498, Final Batch Loss: 0.0492745041847229\n",
      "Epoch 3406, Loss: 0.0497119314968586, Final Batch Loss: 0.028787361457943916\n",
      "Epoch 3407, Loss: 0.09719034656882286, Final Batch Loss: 0.07828951627016068\n",
      "Epoch 3408, Loss: 0.053928881883621216, Final Batch Loss: 0.02228103205561638\n",
      "Epoch 3409, Loss: 0.03397676255553961, Final Batch Loss: 0.023708859458565712\n",
      "Epoch 3410, Loss: 0.051652269437909126, Final Batch Loss: 0.022077953442931175\n",
      "Epoch 3411, Loss: 0.06803930550813675, Final Batch Loss: 0.03130811080336571\n",
      "Epoch 3412, Loss: 0.06915416941046715, Final Batch Loss: 0.04908420890569687\n",
      "Epoch 3413, Loss: 0.07129129394888878, Final Batch Loss: 0.04995744675397873\n",
      "Epoch 3414, Loss: 0.05802885815501213, Final Batch Loss: 0.02366366982460022\n",
      "Epoch 3415, Loss: 0.0637186523526907, Final Batch Loss: 0.04832343012094498\n",
      "Epoch 3416, Loss: 0.04929173458367586, Final Batch Loss: 0.036821067333221436\n",
      "Epoch 3417, Loss: 0.08841920644044876, Final Batch Loss: 0.05611971765756607\n",
      "Epoch 3418, Loss: 0.03532488364726305, Final Batch Loss: 0.022699354216456413\n",
      "Epoch 3419, Loss: 0.0565418591722846, Final Batch Loss: 0.01278326939791441\n",
      "Epoch 3420, Loss: 0.10827720910310745, Final Batch Loss: 0.06580624729394913\n",
      "Epoch 3421, Loss: 0.11534835398197174, Final Batch Loss: 0.03664809465408325\n",
      "Epoch 3422, Loss: 0.05361772421747446, Final Batch Loss: 0.013241422362625599\n",
      "Epoch 3423, Loss: 0.12151864916086197, Final Batch Loss: 0.03750685602426529\n",
      "Epoch 3424, Loss: 0.10045298933982849, Final Batch Loss: 0.05138319730758667\n",
      "Epoch 3425, Loss: 0.04795746132731438, Final Batch Loss: 0.02131745219230652\n",
      "Epoch 3426, Loss: 0.033719453029334545, Final Batch Loss: 0.02440555766224861\n",
      "Epoch 3427, Loss: 0.03438910748809576, Final Batch Loss: 0.022803127765655518\n",
      "Epoch 3428, Loss: 0.07047603465616703, Final Batch Loss: 0.0293776523321867\n",
      "Epoch 3429, Loss: 0.03430742584168911, Final Batch Loss: 0.016622859984636307\n",
      "Epoch 3430, Loss: 0.029079481959342957, Final Batch Loss: 0.015017358586192131\n",
      "Epoch 3431, Loss: 0.022223547101020813, Final Batch Loss: 0.014079753309488297\n",
      "Epoch 3432, Loss: 0.0735020712018013, Final Batch Loss: 0.023773878812789917\n",
      "Epoch 3433, Loss: 0.04394667502492666, Final Batch Loss: 0.031874880194664\n",
      "Epoch 3434, Loss: 0.07802332378923893, Final Batch Loss: 0.023811398074030876\n",
      "Epoch 3435, Loss: 0.0938404742628336, Final Batch Loss: 0.0659945085644722\n",
      "Epoch 3436, Loss: 0.04559515230357647, Final Batch Loss: 0.01418122835457325\n",
      "Epoch 3437, Loss: 0.05280519276857376, Final Batch Loss: 0.01821175590157509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3438, Loss: 0.06769683677703142, Final Batch Loss: 0.056990768760442734\n",
      "Epoch 3439, Loss: 0.03230773564428091, Final Batch Loss: 0.00938574131578207\n",
      "Epoch 3440, Loss: 0.04577534180134535, Final Batch Loss: 0.03779534250497818\n",
      "Epoch 3441, Loss: 0.03570481017231941, Final Batch Loss: 0.023537935689091682\n",
      "Epoch 3442, Loss: 0.026904109865427017, Final Batch Loss: 0.016117699444293976\n",
      "Epoch 3443, Loss: 0.047073936089873314, Final Batch Loss: 0.023845229297876358\n",
      "Epoch 3444, Loss: 0.03756856080144644, Final Batch Loss: 0.0263535063713789\n",
      "Epoch 3445, Loss: 0.022048749029636383, Final Batch Loss: 0.010939477011561394\n",
      "Epoch 3446, Loss: 0.021732349880039692, Final Batch Loss: 0.008394553326070309\n",
      "Epoch 3447, Loss: 0.02526600006967783, Final Batch Loss: 0.005856757052242756\n",
      "Epoch 3448, Loss: 0.017249067779630423, Final Batch Loss: 0.006897494662553072\n",
      "Epoch 3449, Loss: 0.04757962189614773, Final Batch Loss: 0.0264306478202343\n",
      "Epoch 3450, Loss: 0.055686842650175095, Final Batch Loss: 0.01670479029417038\n",
      "Epoch 3451, Loss: 0.0740273967385292, Final Batch Loss: 0.06319274753332138\n",
      "Epoch 3452, Loss: 0.07705628871917725, Final Batch Loss: 0.057314760982990265\n",
      "Epoch 3453, Loss: 0.03917333297431469, Final Batch Loss: 0.0211953055113554\n",
      "Epoch 3454, Loss: 0.0753505527973175, Final Batch Loss: 0.03110438957810402\n",
      "Epoch 3455, Loss: 0.045653801411390305, Final Batch Loss: 0.010801251977682114\n",
      "Epoch 3456, Loss: 0.024309265427291393, Final Batch Loss: 0.015538837760686874\n",
      "Epoch 3457, Loss: 0.024908374063670635, Final Batch Loss: 0.01648443564772606\n",
      "Epoch 3458, Loss: 0.04340241476893425, Final Batch Loss: 0.035587988793849945\n",
      "Epoch 3459, Loss: 0.07148975133895874, Final Batch Loss: 0.03015010803937912\n",
      "Epoch 3460, Loss: 0.07052919082343578, Final Batch Loss: 0.02615254931151867\n",
      "Epoch 3461, Loss: 0.039907784666866064, Final Batch Loss: 0.03461148962378502\n",
      "Epoch 3462, Loss: 0.0756728257983923, Final Batch Loss: 0.028068436309695244\n",
      "Epoch 3463, Loss: 0.028099396266043186, Final Batch Loss: 0.010492169298231602\n",
      "Epoch 3464, Loss: 0.07838420197367668, Final Batch Loss: 0.027313265949487686\n",
      "Epoch 3465, Loss: 0.05755613371729851, Final Batch Loss: 0.03547852486371994\n",
      "Epoch 3466, Loss: 0.09567581303417683, Final Batch Loss: 0.07186925411224365\n",
      "Epoch 3467, Loss: 0.06858527101576328, Final Batch Loss: 0.050856977701187134\n",
      "Epoch 3468, Loss: 0.021744045661762357, Final Batch Loss: 0.003820638405159116\n",
      "Epoch 3469, Loss: 0.034893031232059, Final Batch Loss: 0.011854232288897038\n",
      "Epoch 3470, Loss: 0.016012736596167088, Final Batch Loss: 0.009407643228769302\n",
      "Epoch 3471, Loss: 0.09502642974257469, Final Batch Loss: 0.04051797464489937\n",
      "Epoch 3472, Loss: 0.04128928016871214, Final Batch Loss: 0.00976248737424612\n",
      "Epoch 3473, Loss: 0.06816074997186661, Final Batch Loss: 0.03287861496210098\n",
      "Epoch 3474, Loss: 0.07314959913492203, Final Batch Loss: 0.03739691898226738\n",
      "Epoch 3475, Loss: 0.052874622866511345, Final Batch Loss: 0.04188913106918335\n",
      "Epoch 3476, Loss: 0.06677062623202801, Final Batch Loss: 0.014509031549096107\n",
      "Epoch 3477, Loss: 0.04287789948284626, Final Batch Loss: 0.014174966141581535\n",
      "Epoch 3478, Loss: 0.06130162253975868, Final Batch Loss: 0.016044162213802338\n",
      "Epoch 3479, Loss: 0.029629923403263092, Final Batch Loss: 0.011776622384786606\n",
      "Epoch 3480, Loss: 0.039505455642938614, Final Batch Loss: 0.014848330989480019\n",
      "Epoch 3481, Loss: 0.05766056478023529, Final Batch Loss: 0.01806139200925827\n",
      "Epoch 3482, Loss: 0.048059494234621525, Final Batch Loss: 0.00984483677893877\n",
      "Epoch 3483, Loss: 0.07470130547881126, Final Batch Loss: 0.030802030116319656\n",
      "Epoch 3484, Loss: 0.07147178053855896, Final Batch Loss: 0.05192248150706291\n",
      "Epoch 3485, Loss: 0.03722446132451296, Final Batch Loss: 0.02945820614695549\n",
      "Epoch 3486, Loss: 0.03558250144124031, Final Batch Loss: 0.00973362848162651\n",
      "Epoch 3487, Loss: 0.03818838112056255, Final Batch Loss: 0.01796521432697773\n",
      "Epoch 3488, Loss: 0.02855829242616892, Final Batch Loss: 0.018361851572990417\n",
      "Epoch 3489, Loss: 0.05535614490509033, Final Batch Loss: 0.024176979437470436\n",
      "Epoch 3490, Loss: 0.058907534927129745, Final Batch Loss: 0.02384170889854431\n",
      "Epoch 3491, Loss: 0.07152264751493931, Final Batch Loss: 0.020490063354372978\n",
      "Epoch 3492, Loss: 0.07871577329933643, Final Batch Loss: 0.029071172699332237\n",
      "Epoch 3493, Loss: 0.025603904388844967, Final Batch Loss: 0.013695270754396915\n",
      "Epoch 3494, Loss: 0.05299796722829342, Final Batch Loss: 0.043689146637916565\n",
      "Epoch 3495, Loss: 0.030022121034562588, Final Batch Loss: 0.016725314781069756\n",
      "Epoch 3496, Loss: 0.04128270875662565, Final Batch Loss: 0.013458742760121822\n",
      "Epoch 3497, Loss: 0.040114883333444595, Final Batch Loss: 0.01630098558962345\n",
      "Epoch 3498, Loss: 0.036619107238948345, Final Batch Loss: 0.012471127323806286\n",
      "Epoch 3499, Loss: 0.08850296912714839, Final Batch Loss: 0.007043916266411543\n",
      "Epoch 3500, Loss: 0.06677206046879292, Final Batch Loss: 0.03054390661418438\n",
      "Epoch 3501, Loss: 0.03135912865400314, Final Batch Loss: 0.01806609518826008\n",
      "Epoch 3502, Loss: 0.07678112387657166, Final Batch Loss: 0.055400244891643524\n",
      "Epoch 3503, Loss: 0.03881227783858776, Final Batch Loss: 0.019608447328209877\n",
      "Epoch 3504, Loss: 0.07110112346708775, Final Batch Loss: 0.024042712524533272\n",
      "Epoch 3505, Loss: 0.10990321636199951, Final Batch Loss: 0.060320351272821426\n",
      "Epoch 3506, Loss: 0.056229942478239536, Final Batch Loss: 0.040698710829019547\n",
      "Epoch 3507, Loss: 0.09155435021966696, Final Batch Loss: 0.008507802151143551\n",
      "Epoch 3508, Loss: 0.05369131453335285, Final Batch Loss: 0.03185558319091797\n",
      "Epoch 3509, Loss: 0.08101865649223328, Final Batch Loss: 0.04281189665198326\n",
      "Epoch 3510, Loss: 0.033284829929471016, Final Batch Loss: 0.016019996255636215\n",
      "Epoch 3511, Loss: 0.02794697042554617, Final Batch Loss: 0.008066493086516857\n",
      "Epoch 3512, Loss: 0.015157251618802547, Final Batch Loss: 0.007993045262992382\n",
      "Epoch 3513, Loss: 0.03032602183520794, Final Batch Loss: 0.01453838124871254\n",
      "Epoch 3514, Loss: 0.03473766800016165, Final Batch Loss: 0.01086712721735239\n",
      "Epoch 3515, Loss: 0.032101462595164776, Final Batch Loss: 0.008023160509765148\n",
      "Epoch 3516, Loss: 0.05749955400824547, Final Batch Loss: 0.03968013823032379\n",
      "Epoch 3517, Loss: 0.03223480936139822, Final Batch Loss: 0.006611219607293606\n",
      "Epoch 3518, Loss: 0.0900566503405571, Final Batch Loss: 0.05690367892384529\n",
      "Epoch 3519, Loss: 0.041086453944444656, Final Batch Loss: 0.02521049790084362\n",
      "Epoch 3520, Loss: 0.06298798881471157, Final Batch Loss: 0.048945941030979156\n",
      "Epoch 3521, Loss: 0.052300198934972286, Final Batch Loss: 0.00907482486218214\n",
      "Epoch 3522, Loss: 0.05503966473042965, Final Batch Loss: 0.010296544060111046\n",
      "Epoch 3523, Loss: 0.08129709213972092, Final Batch Loss: 0.012698493897914886\n",
      "Epoch 3524, Loss: 0.052502048201859, Final Batch Loss: 0.009134405292570591\n",
      "Epoch 3525, Loss: 0.029582671355456114, Final Batch Loss: 0.02261151187121868\n",
      "Epoch 3526, Loss: 0.048139216378331184, Final Batch Loss: 0.016924360767006874\n",
      "Epoch 3527, Loss: 0.07996192574501038, Final Batch Loss: 0.05036170035600662\n",
      "Epoch 3528, Loss: 0.05228752875700593, Final Batch Loss: 0.006896125618368387\n",
      "Epoch 3529, Loss: 0.05834699235856533, Final Batch Loss: 0.028584860265254974\n",
      "Epoch 3530, Loss: 0.07452868483960629, Final Batch Loss: 0.016018537804484367\n",
      "Epoch 3531, Loss: 0.06655634194612503, Final Batch Loss: 0.034569233655929565\n",
      "Epoch 3532, Loss: 0.08708315342664719, Final Batch Loss: 0.04467374086380005\n",
      "Epoch 3533, Loss: 0.05920258164405823, Final Batch Loss: 0.028852742165327072\n",
      "Epoch 3534, Loss: 0.080175356939435, Final Batch Loss: 0.06160411238670349\n",
      "Epoch 3535, Loss: 0.08977248147130013, Final Batch Loss: 0.055944330990314484\n",
      "Epoch 3536, Loss: 0.03675180487334728, Final Batch Loss: 0.01718343049287796\n",
      "Epoch 3537, Loss: 0.08329563029110432, Final Batch Loss: 0.07127628475427628\n",
      "Epoch 3538, Loss: 0.060323743149638176, Final Batch Loss: 0.030552251264452934\n",
      "Epoch 3539, Loss: 0.061288630589842796, Final Batch Loss: 0.045901067554950714\n",
      "Epoch 3540, Loss: 0.04084814805537462, Final Batch Loss: 0.027427855879068375\n",
      "Epoch 3541, Loss: 0.09726454317569733, Final Batch Loss: 0.0458378866314888\n",
      "Epoch 3542, Loss: 0.09203878417611122, Final Batch Loss: 0.04636450484395027\n",
      "Epoch 3543, Loss: 0.08574865385890007, Final Batch Loss: 0.06215782091021538\n",
      "Epoch 3544, Loss: 0.06967531144618988, Final Batch Loss: 0.03528214991092682\n",
      "Epoch 3545, Loss: 0.07522567734122276, Final Batch Loss: 0.037891797721385956\n",
      "Epoch 3546, Loss: 0.03971659950911999, Final Batch Loss: 0.0336519330739975\n",
      "Epoch 3547, Loss: 0.030876334756612778, Final Batch Loss: 0.0103653185069561\n",
      "Epoch 3548, Loss: 0.0638297088444233, Final Batch Loss: 0.04047653451561928\n",
      "Epoch 3549, Loss: 0.05636447295546532, Final Batch Loss: 0.025024816393852234\n",
      "Epoch 3550, Loss: 0.06492120027542114, Final Batch Loss: 0.035212110728025436\n",
      "Epoch 3551, Loss: 0.04692000802606344, Final Batch Loss: 0.013404658995568752\n",
      "Epoch 3552, Loss: 0.054346345365047455, Final Batch Loss: 0.022527743130922318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3553, Loss: 0.05649575963616371, Final Batch Loss: 0.022492557764053345\n",
      "Epoch 3554, Loss: 0.05057332292199135, Final Batch Loss: 0.02594935894012451\n",
      "Epoch 3555, Loss: 0.09929077699780464, Final Batch Loss: 0.06446444988250732\n",
      "Epoch 3556, Loss: 0.0969497375190258, Final Batch Loss: 0.05684325844049454\n",
      "Epoch 3557, Loss: 0.0563488993793726, Final Batch Loss: 0.01868314854800701\n",
      "Epoch 3558, Loss: 0.039189521223306656, Final Batch Loss: 0.00838533230125904\n",
      "Epoch 3559, Loss: 0.09117485582828522, Final Batch Loss: 0.04114169254899025\n",
      "Epoch 3560, Loss: 0.07123496755957603, Final Batch Loss: 0.03727341443300247\n",
      "Epoch 3561, Loss: 0.06695293262600899, Final Batch Loss: 0.03352971002459526\n",
      "Epoch 3562, Loss: 0.06498501636087894, Final Batch Loss: 0.027922270819544792\n",
      "Epoch 3563, Loss: 0.059202905744314194, Final Batch Loss: 0.04135411977767944\n",
      "Epoch 3564, Loss: 0.06434006430208683, Final Batch Loss: 0.02619878016412258\n",
      "Epoch 3565, Loss: 0.08177968859672546, Final Batch Loss: 0.04425841197371483\n",
      "Epoch 3566, Loss: 0.04391244985163212, Final Batch Loss: 0.020512938499450684\n",
      "Epoch 3567, Loss: 0.08116335235536098, Final Batch Loss: 0.02160107158124447\n",
      "Epoch 3568, Loss: 0.03679463732987642, Final Batch Loss: 0.022835714742541313\n",
      "Epoch 3569, Loss: 0.04503266140818596, Final Batch Loss: 0.019504135474562645\n",
      "Epoch 3570, Loss: 0.043565280735492706, Final Batch Loss: 0.022934623062610626\n",
      "Epoch 3571, Loss: 0.026727765798568726, Final Batch Loss: 0.010958954691886902\n",
      "Epoch 3572, Loss: 0.04670041985809803, Final Batch Loss: 0.01665361598134041\n",
      "Epoch 3573, Loss: 0.0653972178697586, Final Batch Loss: 0.02227288857102394\n",
      "Epoch 3574, Loss: 0.019696317613124847, Final Batch Loss: 0.010639733634889126\n",
      "Epoch 3575, Loss: 0.04572819825261831, Final Batch Loss: 0.03305719047784805\n",
      "Epoch 3576, Loss: 0.07609177380800247, Final Batch Loss: 0.04667190462350845\n",
      "Epoch 3577, Loss: 0.07066762261092663, Final Batch Loss: 0.02375647984445095\n",
      "Epoch 3578, Loss: 0.034560215659439564, Final Batch Loss: 0.02315455488860607\n",
      "Epoch 3579, Loss: 0.050390711054205894, Final Batch Loss: 0.03974262997508049\n",
      "Epoch 3580, Loss: 0.043262552469968796, Final Batch Loss: 0.026402166113257408\n",
      "Epoch 3581, Loss: 0.04743373766541481, Final Batch Loss: 0.029373854398727417\n",
      "Epoch 3582, Loss: 0.04947434738278389, Final Batch Loss: 0.031937144696712494\n",
      "Epoch 3583, Loss: 0.07003616634756327, Final Batch Loss: 0.012474269606173038\n",
      "Epoch 3584, Loss: 0.050061071291565895, Final Batch Loss: 0.021301260218024254\n",
      "Epoch 3585, Loss: 0.06940173357725143, Final Batch Loss: 0.04151517152786255\n",
      "Epoch 3586, Loss: 0.0778532363474369, Final Batch Loss: 0.024865444749593735\n",
      "Epoch 3587, Loss: 0.05376823805272579, Final Batch Loss: 0.033396605402231216\n",
      "Epoch 3588, Loss: 0.025649938266724348, Final Batch Loss: 0.007539037149399519\n",
      "Epoch 3589, Loss: 0.03203973639756441, Final Batch Loss: 0.010669001378118992\n",
      "Epoch 3590, Loss: 0.02866336051374674, Final Batch Loss: 0.009434220381081104\n",
      "Epoch 3591, Loss: 0.021897926926612854, Final Batch Loss: 0.012204107828438282\n",
      "Epoch 3592, Loss: 0.04388406500220299, Final Batch Loss: 0.016257215291261673\n",
      "Epoch 3593, Loss: 0.0459019485861063, Final Batch Loss: 0.02343956008553505\n",
      "Epoch 3594, Loss: 0.07029325142502785, Final Batch Loss: 0.03362547233700752\n",
      "Epoch 3595, Loss: 0.0524190217256546, Final Batch Loss: 0.021872201934456825\n",
      "Epoch 3596, Loss: 0.033872916363179684, Final Batch Loss: 0.014324543066322803\n",
      "Epoch 3597, Loss: 0.06164559721946716, Final Batch Loss: 0.044398292899131775\n",
      "Epoch 3598, Loss: 0.0699190255254507, Final Batch Loss: 0.04246746748685837\n",
      "Epoch 3599, Loss: 0.043813131749629974, Final Batch Loss: 0.02213355526328087\n",
      "Epoch 3600, Loss: 0.03450646251440048, Final Batch Loss: 0.009465483948588371\n",
      "Epoch 3601, Loss: 0.0194985787384212, Final Batch Loss: 0.006330993492156267\n",
      "Epoch 3602, Loss: 0.035652050748467445, Final Batch Loss: 0.017409976571798325\n",
      "Epoch 3603, Loss: 0.050098761916160583, Final Batch Loss: 0.04149894788861275\n",
      "Epoch 3604, Loss: 0.05446661403402686, Final Batch Loss: 0.007261210586875677\n",
      "Epoch 3605, Loss: 0.014007367892190814, Final Batch Loss: 0.0027591378893703222\n",
      "Epoch 3606, Loss: 0.02638576366007328, Final Batch Loss: 0.01628619059920311\n",
      "Epoch 3607, Loss: 0.02228586934506893, Final Batch Loss: 0.0058419425040483475\n",
      "Epoch 3608, Loss: 0.020737020298838615, Final Batch Loss: 0.006917083635926247\n",
      "Epoch 3609, Loss: 0.014146185014396906, Final Batch Loss: 0.004448301624506712\n",
      "Epoch 3610, Loss: 0.0478052357211709, Final Batch Loss: 0.010681242682039738\n",
      "Epoch 3611, Loss: 0.017491818871349096, Final Batch Loss: 0.009911373257637024\n",
      "Epoch 3612, Loss: 0.03905332228168845, Final Batch Loss: 0.0324753113090992\n",
      "Epoch 3613, Loss: 0.05895905941724777, Final Batch Loss: 0.024147596210241318\n",
      "Epoch 3614, Loss: 0.03476785751990974, Final Batch Loss: 0.0032742421608418226\n",
      "Epoch 3615, Loss: 0.02444720221683383, Final Batch Loss: 0.018497837707400322\n",
      "Epoch 3616, Loss: 0.07679873704910278, Final Batch Loss: 0.036372508853673935\n",
      "Epoch 3617, Loss: 0.0491452869027853, Final Batch Loss: 0.02964414469897747\n",
      "Epoch 3618, Loss: 0.045599997974932194, Final Batch Loss: 0.008097746409475803\n",
      "Epoch 3619, Loss: 0.05898968316614628, Final Batch Loss: 0.02346252091228962\n",
      "Epoch 3620, Loss: 0.03042585961520672, Final Batch Loss: 0.016195088624954224\n",
      "Epoch 3621, Loss: 0.059129019267857075, Final Batch Loss: 0.00845488253980875\n",
      "Epoch 3622, Loss: 0.05088484287261963, Final Batch Loss: 0.026273909956216812\n",
      "Epoch 3623, Loss: 0.0758059099316597, Final Batch Loss: 0.03355827182531357\n",
      "Epoch 3624, Loss: 0.04344778694212437, Final Batch Loss: 0.011551430448889732\n",
      "Epoch 3625, Loss: 0.04076929297298193, Final Batch Loss: 0.007405425421893597\n",
      "Epoch 3626, Loss: 0.06757053174078465, Final Batch Loss: 0.05242437496781349\n",
      "Epoch 3627, Loss: 0.08112052083015442, Final Batch Loss: 0.05736854299902916\n",
      "Epoch 3628, Loss: 0.061301516368985176, Final Batch Loss: 0.03492158651351929\n",
      "Epoch 3629, Loss: 0.10389932617545128, Final Batch Loss: 0.05095097795128822\n",
      "Epoch 3630, Loss: 0.05205771792680025, Final Batch Loss: 0.009141446091234684\n",
      "Epoch 3631, Loss: 0.06989690009504557, Final Batch Loss: 0.010049638338387012\n",
      "Epoch 3632, Loss: 0.12621410191059113, Final Batch Loss: 0.07961404323577881\n",
      "Epoch 3633, Loss: 0.06482209172099829, Final Batch Loss: 0.050083935260772705\n",
      "Epoch 3634, Loss: 0.08219544216990471, Final Batch Loss: 0.04905518889427185\n",
      "Epoch 3635, Loss: 0.07802272774279118, Final Batch Loss: 0.026128293946385384\n",
      "Epoch 3636, Loss: 0.05184751935303211, Final Batch Loss: 0.03166964650154114\n",
      "Epoch 3637, Loss: 0.039721873588860035, Final Batch Loss: 0.024097658693790436\n",
      "Epoch 3638, Loss: 0.0831184908747673, Final Batch Loss: 0.04854864999651909\n",
      "Epoch 3639, Loss: 0.07236737199127674, Final Batch Loss: 0.0446477048099041\n",
      "Epoch 3640, Loss: 0.09638999402523041, Final Batch Loss: 0.07294117659330368\n",
      "Epoch 3641, Loss: 0.030992137268185616, Final Batch Loss: 0.016601858660578728\n",
      "Epoch 3642, Loss: 0.09271233156323433, Final Batch Loss: 0.04143787920475006\n",
      "Epoch 3643, Loss: 0.10636461339890957, Final Batch Loss: 0.07646985352039337\n",
      "Epoch 3644, Loss: 0.05897751450538635, Final Batch Loss: 0.02902737259864807\n",
      "Epoch 3645, Loss: 0.05240284837782383, Final Batch Loss: 0.01851784996688366\n",
      "Epoch 3646, Loss: 0.08411530777812004, Final Batch Loss: 0.04902597516775131\n",
      "Epoch 3647, Loss: 0.061680810526013374, Final Batch Loss: 0.037003178149461746\n",
      "Epoch 3648, Loss: 0.08076606504619122, Final Batch Loss: 0.024615252390503883\n",
      "Epoch 3649, Loss: 0.048282820731401443, Final Batch Loss: 0.026821773499250412\n",
      "Epoch 3650, Loss: 0.07291114330291748, Final Batch Loss: 0.041326168924570084\n",
      "Epoch 3651, Loss: 0.046979278326034546, Final Batch Loss: 0.00798998773097992\n",
      "Epoch 3652, Loss: 0.08961310982704163, Final Batch Loss: 0.06422746181488037\n",
      "Epoch 3653, Loss: 0.06106032244861126, Final Batch Loss: 0.03528536111116409\n",
      "Epoch 3654, Loss: 0.04516400024294853, Final Batch Loss: 0.01884051039814949\n",
      "Epoch 3655, Loss: 0.04444565065205097, Final Batch Loss: 0.03678738325834274\n",
      "Epoch 3656, Loss: 0.025699551217257977, Final Batch Loss: 0.01472417451441288\n",
      "Epoch 3657, Loss: 0.07138346321880817, Final Batch Loss: 0.04635806754231453\n",
      "Epoch 3658, Loss: 0.053029787726700306, Final Batch Loss: 0.011570067144930363\n",
      "Epoch 3659, Loss: 0.041673384606838226, Final Batch Loss: 0.02962593361735344\n",
      "Epoch 3660, Loss: 0.0403304579667747, Final Batch Loss: 0.006109073292464018\n",
      "Epoch 3661, Loss: 0.01880650594830513, Final Batch Loss: 0.011813804507255554\n",
      "Epoch 3662, Loss: 0.06165502592921257, Final Batch Loss: 0.049157336354255676\n",
      "Epoch 3663, Loss: 0.024773048236966133, Final Batch Loss: 0.010340537875890732\n",
      "Epoch 3664, Loss: 0.05515489727258682, Final Batch Loss: 0.0296444334089756\n",
      "Epoch 3665, Loss: 0.05886472947895527, Final Batch Loss: 0.043285418301820755\n",
      "Epoch 3666, Loss: 0.060583608224987984, Final Batch Loss: 0.02513875998556614\n",
      "Epoch 3667, Loss: 0.06779952300712466, Final Batch Loss: 0.06141907721757889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3668, Loss: 0.04865339584648609, Final Batch Loss: 0.03426947817206383\n",
      "Epoch 3669, Loss: 0.044156781397759914, Final Batch Loss: 0.00808653887361288\n",
      "Epoch 3670, Loss: 0.0337242903187871, Final Batch Loss: 0.020052513107657433\n",
      "Epoch 3671, Loss: 0.039068469777703285, Final Batch Loss: 0.023424677550792694\n",
      "Epoch 3672, Loss: 0.03782873507589102, Final Batch Loss: 0.030124930664896965\n",
      "Epoch 3673, Loss: 0.05658181197941303, Final Batch Loss: 0.02781238593161106\n",
      "Epoch 3674, Loss: 0.04103774856775999, Final Batch Loss: 0.01211570855230093\n",
      "Epoch 3675, Loss: 0.06575316190719604, Final Batch Loss: 0.02993994578719139\n",
      "Epoch 3676, Loss: 0.028300253208726645, Final Batch Loss: 0.004908740054816008\n",
      "Epoch 3677, Loss: 0.046113165095448494, Final Batch Loss: 0.02839389443397522\n",
      "Epoch 3678, Loss: 0.047286816872656345, Final Batch Loss: 0.03295900672674179\n",
      "Epoch 3679, Loss: 0.041473872028291225, Final Batch Loss: 0.02802066132426262\n",
      "Epoch 3680, Loss: 0.04720226489007473, Final Batch Loss: 0.02940388396382332\n",
      "Epoch 3681, Loss: 0.04149332828819752, Final Batch Loss: 0.012769291177392006\n",
      "Epoch 3682, Loss: 0.023331177420914173, Final Batch Loss: 0.011274036020040512\n",
      "Epoch 3683, Loss: 0.06311559863388538, Final Batch Loss: 0.046406134963035583\n",
      "Epoch 3684, Loss: 0.026459621265530586, Final Batch Loss: 0.013439184054732323\n",
      "Epoch 3685, Loss: 0.06312283128499985, Final Batch Loss: 0.041973527520895004\n",
      "Epoch 3686, Loss: 0.06119798123836517, Final Batch Loss: 0.01720890775322914\n",
      "Epoch 3687, Loss: 0.03125636652112007, Final Batch Loss: 0.01581464521586895\n",
      "Epoch 3688, Loss: 0.07884358987212181, Final Batch Loss: 0.04298117384314537\n",
      "Epoch 3689, Loss: 0.07661749050021172, Final Batch Loss: 0.04303012788295746\n",
      "Epoch 3690, Loss: 0.031238767318427563, Final Batch Loss: 0.012115214951336384\n",
      "Epoch 3691, Loss: 0.026070130988955498, Final Batch Loss: 0.015582061372697353\n",
      "Epoch 3692, Loss: 0.10940502397716045, Final Batch Loss: 0.0835120752453804\n",
      "Epoch 3693, Loss: 0.03023387072607875, Final Batch Loss: 0.0040075215511024\n",
      "Epoch 3694, Loss: 0.03900156356394291, Final Batch Loss: 0.016060976311564445\n",
      "Epoch 3695, Loss: 0.10027769953012466, Final Batch Loss: 0.03574226051568985\n",
      "Epoch 3696, Loss: 0.05612511280924082, Final Batch Loss: 0.009856236167252064\n",
      "Epoch 3697, Loss: 0.05593949183821678, Final Batch Loss: 0.03137555718421936\n",
      "Epoch 3698, Loss: 0.056396326050162315, Final Batch Loss: 0.009448224678635597\n",
      "Epoch 3699, Loss: 0.045820342376828194, Final Batch Loss: 0.013827631250023842\n",
      "Epoch 3700, Loss: 0.0426244530826807, Final Batch Loss: 0.021705370396375656\n",
      "Epoch 3701, Loss: 0.04643589910119772, Final Batch Loss: 0.01550151314586401\n",
      "Epoch 3702, Loss: 0.04398604482412338, Final Batch Loss: 0.027051040902733803\n",
      "Epoch 3703, Loss: 0.03282323945313692, Final Batch Loss: 0.010082601569592953\n",
      "Epoch 3704, Loss: 0.07383747771382332, Final Batch Loss: 0.03701627254486084\n",
      "Epoch 3705, Loss: 0.05422306340187788, Final Batch Loss: 0.04199861362576485\n",
      "Epoch 3706, Loss: 0.09140501916408539, Final Batch Loss: 0.04401581361889839\n",
      "Epoch 3707, Loss: 0.026015490759164095, Final Batch Loss: 0.007346306461840868\n",
      "Epoch 3708, Loss: 0.03644810616970062, Final Batch Loss: 0.012732153758406639\n",
      "Epoch 3709, Loss: 0.0719694159924984, Final Batch Loss: 0.0577496699988842\n",
      "Epoch 3710, Loss: 0.06854430586099625, Final Batch Loss: 0.03400564566254616\n",
      "Epoch 3711, Loss: 0.04811715520918369, Final Batch Loss: 0.011288126930594444\n",
      "Epoch 3712, Loss: 0.026868551969528198, Final Batch Loss: 0.017596008256077766\n",
      "Epoch 3713, Loss: 0.0791709702461958, Final Batch Loss: 0.008837005123496056\n",
      "Epoch 3714, Loss: 0.07322573661804199, Final Batch Loss: 0.04029255732893944\n",
      "Epoch 3715, Loss: 0.06882084906101227, Final Batch Loss: 0.02837774157524109\n",
      "Epoch 3716, Loss: 0.04444838408380747, Final Batch Loss: 0.006861989386379719\n",
      "Epoch 3717, Loss: 0.05189167335629463, Final Batch Loss: 0.0286264531314373\n",
      "Epoch 3718, Loss: 0.0447340440005064, Final Batch Loss: 0.01763380877673626\n",
      "Epoch 3719, Loss: 0.07278417609632015, Final Batch Loss: 0.030999353155493736\n",
      "Epoch 3720, Loss: 0.05906176287680864, Final Batch Loss: 0.04573224484920502\n",
      "Epoch 3721, Loss: 0.0893191546201706, Final Batch Loss: 0.03941239416599274\n",
      "Epoch 3722, Loss: 0.07397649064660072, Final Batch Loss: 0.04853878542780876\n",
      "Epoch 3723, Loss: 0.02405427396297455, Final Batch Loss: 0.00602332316339016\n",
      "Epoch 3724, Loss: 0.07271109335124493, Final Batch Loss: 0.042001381516456604\n",
      "Epoch 3725, Loss: 0.07982496172189713, Final Batch Loss: 0.031685616821050644\n",
      "Epoch 3726, Loss: 0.07466338574886322, Final Batch Loss: 0.03159470483660698\n",
      "Epoch 3727, Loss: 0.054641881957650185, Final Batch Loss: 0.025446012616157532\n",
      "Epoch 3728, Loss: 0.05862465687096119, Final Batch Loss: 0.020953482016921043\n",
      "Epoch 3729, Loss: 0.07575650699436665, Final Batch Loss: 0.031213345006108284\n",
      "Epoch 3730, Loss: 0.0941716656088829, Final Batch Loss: 0.038258813321590424\n",
      "Epoch 3731, Loss: 0.04728209041059017, Final Batch Loss: 0.023516222834587097\n",
      "Epoch 3732, Loss: 0.04735508747398853, Final Batch Loss: 0.028233017772436142\n",
      "Epoch 3733, Loss: 0.05242716148495674, Final Batch Loss: 0.008759520947933197\n",
      "Epoch 3734, Loss: 0.07541157864034176, Final Batch Loss: 0.020758798345923424\n",
      "Epoch 3735, Loss: 0.05104903131723404, Final Batch Loss: 0.03784804046154022\n",
      "Epoch 3736, Loss: 0.11261245794594288, Final Batch Loss: 0.02907833643257618\n",
      "Epoch 3737, Loss: 0.04505891166627407, Final Batch Loss: 0.0211313609033823\n",
      "Epoch 3738, Loss: 0.1247972883284092, Final Batch Loss: 0.06833970546722412\n",
      "Epoch 3739, Loss: 0.11235010996460915, Final Batch Loss: 0.05058697611093521\n",
      "Epoch 3740, Loss: 0.04685802385210991, Final Batch Loss: 0.01992163620889187\n",
      "Epoch 3741, Loss: 0.09241963177919388, Final Batch Loss: 0.0378447063267231\n",
      "Epoch 3742, Loss: 0.08061501756310463, Final Batch Loss: 0.018759246915578842\n",
      "Epoch 3743, Loss: 0.091362114995718, Final Batch Loss: 0.021205049008131027\n",
      "Epoch 3744, Loss: 0.070432523265481, Final Batch Loss: 0.013196894899010658\n",
      "Epoch 3745, Loss: 0.06065304018557072, Final Batch Loss: 0.04457291215658188\n",
      "Epoch 3746, Loss: 0.04228639416396618, Final Batch Loss: 0.011739367619156837\n",
      "Epoch 3747, Loss: 0.09612881578505039, Final Batch Loss: 0.06593954563140869\n",
      "Epoch 3748, Loss: 0.048592041246593, Final Batch Loss: 0.01443431805819273\n",
      "Epoch 3749, Loss: 0.08968979772180319, Final Batch Loss: 0.007126226089894772\n",
      "Epoch 3750, Loss: 0.059846000745892525, Final Batch Loss: 0.037340179085731506\n",
      "Epoch 3751, Loss: 0.051307572051882744, Final Batch Loss: 0.03776370733976364\n",
      "Epoch 3752, Loss: 0.06572860851883888, Final Batch Loss: 0.038258373737335205\n",
      "Epoch 3753, Loss: 0.05591805279254913, Final Batch Loss: 0.024318262934684753\n",
      "Epoch 3754, Loss: 0.07160061970353127, Final Batch Loss: 0.03919036686420441\n",
      "Epoch 3755, Loss: 0.11466430500149727, Final Batch Loss: 0.07066621631383896\n",
      "Epoch 3756, Loss: 0.06575382128357887, Final Batch Loss: 0.019708462059497833\n",
      "Epoch 3757, Loss: 0.053276631981134415, Final Batch Loss: 0.03946589678525925\n",
      "Epoch 3758, Loss: 0.0912604108452797, Final Batch Loss: 0.044369470328092575\n",
      "Epoch 3759, Loss: 0.053593783639371395, Final Batch Loss: 0.012523616664111614\n",
      "Epoch 3760, Loss: 0.08444172143936157, Final Batch Loss: 0.0431932769715786\n",
      "Epoch 3761, Loss: 0.062197739258408546, Final Batch Loss: 0.02747337706387043\n",
      "Epoch 3762, Loss: 0.06724966503679752, Final Batch Loss: 0.05430031940340996\n",
      "Epoch 3763, Loss: 0.035713077522814274, Final Batch Loss: 0.02210652083158493\n",
      "Epoch 3764, Loss: 0.026369474828243256, Final Batch Loss: 0.009605512022972107\n",
      "Epoch 3765, Loss: 0.03937498480081558, Final Batch Loss: 0.023051539435982704\n",
      "Epoch 3766, Loss: 0.06204764358699322, Final Batch Loss: 0.03639838099479675\n",
      "Epoch 3767, Loss: 0.03113541007041931, Final Batch Loss: 0.012700259685516357\n",
      "Epoch 3768, Loss: 0.03312636725604534, Final Batch Loss: 0.01716531813144684\n",
      "Epoch 3769, Loss: 0.03721135947853327, Final Batch Loss: 0.00836088601499796\n",
      "Epoch 3770, Loss: 0.04354720935225487, Final Batch Loss: 0.01711776852607727\n",
      "Epoch 3771, Loss: 0.03919637529179454, Final Batch Loss: 0.007138985674828291\n",
      "Epoch 3772, Loss: 0.029007833218201995, Final Batch Loss: 0.0037093234714120626\n",
      "Epoch 3773, Loss: 0.09684493206441402, Final Batch Loss: 0.026960356160998344\n",
      "Epoch 3774, Loss: 0.06120351608842611, Final Batch Loss: 0.012528902851045132\n",
      "Epoch 3775, Loss: 0.05505221709609032, Final Batch Loss: 0.013087105005979538\n",
      "Epoch 3776, Loss: 0.04880501329898834, Final Batch Loss: 0.03543520346283913\n",
      "Epoch 3777, Loss: 0.0569487139582634, Final Batch Loss: 0.040893420577049255\n",
      "Epoch 3778, Loss: 0.031426281202584505, Final Batch Loss: 0.024288110435009003\n",
      "Epoch 3779, Loss: 0.02585720457136631, Final Batch Loss: 0.005500204861164093\n",
      "Epoch 3780, Loss: 0.05639755353331566, Final Batch Loss: 0.018576372414827347\n",
      "Epoch 3781, Loss: 0.0573792178183794, Final Batch Loss: 0.03504469618201256\n",
      "Epoch 3782, Loss: 0.09614359121769667, Final Batch Loss: 0.08576290309429169\n",
      "Epoch 3783, Loss: 0.06104636751115322, Final Batch Loss: 0.024933530017733574\n",
      "Epoch 3784, Loss: 0.06958610564470291, Final Batch Loss: 0.05159769952297211\n",
      "Epoch 3785, Loss: 0.0637652650475502, Final Batch Loss: 0.038510750979185104\n",
      "Epoch 3786, Loss: 0.04940980393439531, Final Batch Loss: 0.034741610288619995\n",
      "Epoch 3787, Loss: 0.06980830244719982, Final Batch Loss: 0.02558819390833378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3788, Loss: 0.07666109874844551, Final Batch Loss: 0.03474460542201996\n",
      "Epoch 3789, Loss: 0.08904135227203369, Final Batch Loss: 0.02759663388133049\n",
      "Epoch 3790, Loss: 0.03719381149858236, Final Batch Loss: 0.01193187665194273\n",
      "Epoch 3791, Loss: 0.07974814716726542, Final Batch Loss: 0.06998935341835022\n",
      "Epoch 3792, Loss: 0.07725720666348934, Final Batch Loss: 0.053589023649692535\n",
      "Epoch 3793, Loss: 0.038499098271131516, Final Batch Loss: 0.008025838062167168\n",
      "Epoch 3794, Loss: 0.058988023549318314, Final Batch Loss: 0.038323067128658295\n",
      "Epoch 3795, Loss: 0.09301522048190236, Final Batch Loss: 0.08626812696456909\n",
      "Epoch 3796, Loss: 0.05057989992201328, Final Batch Loss: 0.032443445175886154\n",
      "Epoch 3797, Loss: 0.06257036700844765, Final Batch Loss: 0.03394925221800804\n",
      "Epoch 3798, Loss: 0.05914594233036041, Final Batch Loss: 0.02069804072380066\n",
      "Epoch 3799, Loss: 0.042917972430586815, Final Batch Loss: 0.02409750409424305\n",
      "Epoch 3800, Loss: 0.04037477821111679, Final Batch Loss: 0.021131161600351334\n",
      "Epoch 3801, Loss: 0.044823139905929565, Final Batch Loss: 0.013584259897470474\n",
      "Epoch 3802, Loss: 0.08061763271689415, Final Batch Loss: 0.04907672479748726\n",
      "Epoch 3803, Loss: 0.04148830100893974, Final Batch Loss: 0.026067931205034256\n",
      "Epoch 3804, Loss: 0.09287462010979652, Final Batch Loss: 0.06480581313371658\n",
      "Epoch 3805, Loss: 0.06950154062360525, Final Batch Loss: 0.014197207055985928\n",
      "Epoch 3806, Loss: 0.055647362023591995, Final Batch Loss: 0.042188312858343124\n",
      "Epoch 3807, Loss: 0.047568827867507935, Final Batch Loss: 0.014205802232027054\n",
      "Epoch 3808, Loss: 0.06209029071033001, Final Batch Loss: 0.04216920956969261\n",
      "Epoch 3809, Loss: 0.05269450508058071, Final Batch Loss: 0.02736988104879856\n",
      "Epoch 3810, Loss: 0.09658869355916977, Final Batch Loss: 0.04929937422275543\n",
      "Epoch 3811, Loss: 0.1050969660282135, Final Batch Loss: 0.040028199553489685\n",
      "Epoch 3812, Loss: 0.13523535802960396, Final Batch Loss: 0.08837635815143585\n",
      "Epoch 3813, Loss: 0.1174423024058342, Final Batch Loss: 0.0436771884560585\n",
      "Epoch 3814, Loss: 0.1369430087506771, Final Batch Loss: 0.09347151964902878\n",
      "Epoch 3815, Loss: 0.07185745611786842, Final Batch Loss: 0.03257668390870094\n",
      "Epoch 3816, Loss: 0.04515514709055424, Final Batch Loss: 0.022826669737696648\n",
      "Epoch 3817, Loss: 0.07901365868747234, Final Batch Loss: 0.05483722686767578\n",
      "Epoch 3818, Loss: 0.06660270877182484, Final Batch Loss: 0.050209373235702515\n",
      "Epoch 3819, Loss: 0.07950527407228947, Final Batch Loss: 0.05723636597394943\n",
      "Epoch 3820, Loss: 0.10343651846051216, Final Batch Loss: 0.03962839022278786\n",
      "Epoch 3821, Loss: 0.07592634670436382, Final Batch Loss: 0.046810582280159\n",
      "Epoch 3822, Loss: 0.0352634796872735, Final Batch Loss: 0.022127030417323112\n",
      "Epoch 3823, Loss: 0.09401749074459076, Final Batch Loss: 0.05515706166625023\n",
      "Epoch 3824, Loss: 0.12711613066494465, Final Batch Loss: 0.1001465693116188\n",
      "Epoch 3825, Loss: 0.05611867178231478, Final Batch Loss: 0.011902795173227787\n",
      "Epoch 3826, Loss: 0.047181129455566406, Final Batch Loss: 0.021729590371251106\n",
      "Epoch 3827, Loss: 0.0760275088250637, Final Batch Loss: 0.01107676699757576\n",
      "Epoch 3828, Loss: 0.07999264448881149, Final Batch Loss: 0.034138768911361694\n",
      "Epoch 3829, Loss: 0.026642389595508575, Final Batch Loss: 0.015755008906126022\n",
      "Epoch 3830, Loss: 0.02700365986675024, Final Batch Loss: 0.012200933881103992\n",
      "Epoch 3831, Loss: 0.06140161119401455, Final Batch Loss: 0.023205237463116646\n",
      "Epoch 3832, Loss: 0.0675171297043562, Final Batch Loss: 0.025382233783602715\n",
      "Epoch 3833, Loss: 0.02632784377783537, Final Batch Loss: 0.010679868049919605\n",
      "Epoch 3834, Loss: 0.10999556258320808, Final Batch Loss: 0.07064873725175858\n",
      "Epoch 3835, Loss: 0.03776122257113457, Final Batch Loss: 0.01651877537369728\n",
      "Epoch 3836, Loss: 0.030764759052544832, Final Batch Loss: 0.023609034717082977\n",
      "Epoch 3837, Loss: 0.0570451021194458, Final Batch Loss: 0.014496542513370514\n",
      "Epoch 3838, Loss: 0.04780951887369156, Final Batch Loss: 0.009988635778427124\n",
      "Epoch 3839, Loss: 0.056882524862885475, Final Batch Loss: 0.026345906779170036\n",
      "Epoch 3840, Loss: 0.08635899983346462, Final Batch Loss: 0.025436019524931908\n",
      "Epoch 3841, Loss: 0.031465854961425066, Final Batch Loss: 0.004962513688951731\n",
      "Epoch 3842, Loss: 0.05501320771872997, Final Batch Loss: 0.0390583798289299\n",
      "Epoch 3843, Loss: 0.04535060003399849, Final Batch Loss: 0.02630920149385929\n",
      "Epoch 3844, Loss: 0.03363373130559921, Final Batch Loss: 0.00935562327504158\n",
      "Epoch 3845, Loss: 0.037952459417283535, Final Batch Loss: 0.014189143665134907\n",
      "Epoch 3846, Loss: 0.06509454175829887, Final Batch Loss: 0.04153401777148247\n",
      "Epoch 3847, Loss: 0.032608918845653534, Final Batch Loss: 0.0159597285091877\n",
      "Epoch 3848, Loss: 0.04506143555045128, Final Batch Loss: 0.01681278832256794\n",
      "Epoch 3849, Loss: 0.03316422179341316, Final Batch Loss: 0.014455875381827354\n",
      "Epoch 3850, Loss: 0.05962676182389259, Final Batch Loss: 0.008226174861192703\n",
      "Epoch 3851, Loss: 0.09131419286131859, Final Batch Loss: 0.03856683149933815\n",
      "Epoch 3852, Loss: 0.028925227001309395, Final Batch Loss: 0.021056586876511574\n",
      "Epoch 3853, Loss: 0.03744363598525524, Final Batch Loss: 0.01565518043935299\n",
      "Epoch 3854, Loss: 0.09568609297275543, Final Batch Loss: 0.07281695306301117\n",
      "Epoch 3855, Loss: 0.025132213719189167, Final Batch Loss: 0.016297895461320877\n",
      "Epoch 3856, Loss: 0.031705379486083984, Final Batch Loss: 0.00847901962697506\n",
      "Epoch 3857, Loss: 0.02981984056532383, Final Batch Loss: 0.02162560261785984\n",
      "Epoch 3858, Loss: 0.03699197340756655, Final Batch Loss: 0.028645487502217293\n",
      "Epoch 3859, Loss: 0.01819567382335663, Final Batch Loss: 0.008367864415049553\n",
      "Epoch 3860, Loss: 0.05005580559372902, Final Batch Loss: 0.028969531878829002\n",
      "Epoch 3861, Loss: 0.03359835408627987, Final Batch Loss: 0.013085387647151947\n",
      "Epoch 3862, Loss: 0.0267476849257946, Final Batch Loss: 0.015246882103383541\n",
      "Epoch 3863, Loss: 0.042328374460339546, Final Batch Loss: 0.015975074842572212\n",
      "Epoch 3864, Loss: 0.025154453702270985, Final Batch Loss: 0.014424321241676807\n",
      "Epoch 3865, Loss: 0.041631968691945076, Final Batch Loss: 0.028333447873592377\n",
      "Epoch 3866, Loss: 0.02750421781092882, Final Batch Loss: 0.008366995491087437\n",
      "Epoch 3867, Loss: 0.10172782838344574, Final Batch Loss: 0.08772012591362\n",
      "Epoch 3868, Loss: 0.03460203483700752, Final Batch Loss: 0.001994982361793518\n",
      "Epoch 3869, Loss: 0.030151288956403732, Final Batch Loss: 0.024629229679703712\n",
      "Epoch 3870, Loss: 0.026222649961709976, Final Batch Loss: 0.012158519588410854\n",
      "Epoch 3871, Loss: 0.060465652495622635, Final Batch Loss: 0.02455582097172737\n",
      "Epoch 3872, Loss: 0.039336733520030975, Final Batch Loss: 0.024416355416178703\n",
      "Epoch 3873, Loss: 0.025722076185047626, Final Batch Loss: 0.01117006316781044\n",
      "Epoch 3874, Loss: 0.03649728698655963, Final Batch Loss: 0.007504600565880537\n",
      "Epoch 3875, Loss: 0.03281513508409262, Final Batch Loss: 0.019487110897898674\n",
      "Epoch 3876, Loss: 0.0495808981359005, Final Batch Loss: 0.021626658737659454\n",
      "Epoch 3877, Loss: 0.04180806875228882, Final Batch Loss: 0.031899962574243546\n",
      "Epoch 3878, Loss: 0.06049058958888054, Final Batch Loss: 0.02565225586295128\n",
      "Epoch 3879, Loss: 0.03349030576646328, Final Batch Loss: 0.014231136068701744\n",
      "Epoch 3880, Loss: 0.05258709378540516, Final Batch Loss: 0.016951406374573708\n",
      "Epoch 3881, Loss: 0.03611627593636513, Final Batch Loss: 0.01670532301068306\n",
      "Epoch 3882, Loss: 0.024070042185485363, Final Batch Loss: 0.013295883312821388\n",
      "Epoch 3883, Loss: 0.030519952531903982, Final Batch Loss: 0.003994414117187262\n",
      "Epoch 3884, Loss: 0.09177615493535995, Final Batch Loss: 0.03437897935509682\n",
      "Epoch 3885, Loss: 0.04251339752227068, Final Batch Loss: 0.013475176878273487\n",
      "Epoch 3886, Loss: 0.030855328775942326, Final Batch Loss: 0.01886569708585739\n",
      "Epoch 3887, Loss: 0.048944566398859024, Final Batch Loss: 0.024451637640595436\n",
      "Epoch 3888, Loss: 0.0229700468480587, Final Batch Loss: 0.0049393028020858765\n",
      "Epoch 3889, Loss: 0.037259441800415516, Final Batch Loss: 0.0052019329741597176\n",
      "Epoch 3890, Loss: 0.0475100614130497, Final Batch Loss: 0.011289924383163452\n",
      "Epoch 3891, Loss: 0.022806469351053238, Final Batch Loss: 0.012902106158435345\n",
      "Epoch 3892, Loss: 0.07222333177924156, Final Batch Loss: 0.0419602133333683\n",
      "Epoch 3893, Loss: 0.023552820086479187, Final Batch Loss: 0.01779167167842388\n",
      "Epoch 3894, Loss: 0.022811992093920708, Final Batch Loss: 0.013753843493759632\n",
      "Epoch 3895, Loss: 0.018689695978537202, Final Batch Loss: 0.003903655568137765\n",
      "Epoch 3896, Loss: 0.043011605739593506, Final Batch Loss: 0.025121815502643585\n",
      "Epoch 3897, Loss: 0.03143392316997051, Final Batch Loss: 0.023960862308740616\n",
      "Epoch 3898, Loss: 0.06967463903129101, Final Batch Loss: 0.04166347533464432\n",
      "Epoch 3899, Loss: 0.08705894090235233, Final Batch Loss: 0.026991618797183037\n",
      "Epoch 3900, Loss: 0.01663254853338003, Final Batch Loss: 0.010515619069337845\n",
      "Epoch 3901, Loss: 0.04507335741072893, Final Batch Loss: 0.02948365919291973\n",
      "Epoch 3902, Loss: 0.027152717113494873, Final Batch Loss: 0.008010748773813248\n",
      "Epoch 3903, Loss: 0.02972790878266096, Final Batch Loss: 0.018641643226146698\n",
      "Epoch 3904, Loss: 0.05316607002168894, Final Batch Loss: 0.007367280311882496\n",
      "Epoch 3905, Loss: 0.02581646852195263, Final Batch Loss: 0.009649917483329773\n",
      "Epoch 3906, Loss: 0.04309840314090252, Final Batch Loss: 0.009952379390597343\n",
      "Epoch 3907, Loss: 0.04300377331674099, Final Batch Loss: 0.026246462017297745\n",
      "Epoch 3908, Loss: 0.04675222188234329, Final Batch Loss: 0.025501394644379616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3909, Loss: 0.08083082176744938, Final Batch Loss: 0.0650145635008812\n",
      "Epoch 3910, Loss: 0.08094232715666294, Final Batch Loss: 0.051683150231838226\n",
      "Epoch 3911, Loss: 0.03361999988555908, Final Batch Loss: 0.009036149829626083\n",
      "Epoch 3912, Loss: 0.043457504361867905, Final Batch Loss: 0.023020561784505844\n",
      "Epoch 3913, Loss: 0.0526614710688591, Final Batch Loss: 0.018934112042188644\n",
      "Epoch 3914, Loss: 0.02721257694065571, Final Batch Loss: 0.005798362195491791\n",
      "Epoch 3915, Loss: 0.03089440707117319, Final Batch Loss: 0.008393346332013607\n",
      "Epoch 3916, Loss: 0.05328848585486412, Final Batch Loss: 0.019725516438484192\n",
      "Epoch 3917, Loss: 0.02797430194914341, Final Batch Loss: 0.012485268525779247\n",
      "Epoch 3918, Loss: 0.03501617256551981, Final Batch Loss: 0.011760127730667591\n",
      "Epoch 3919, Loss: 0.04353868588805199, Final Batch Loss: 0.026840630918741226\n",
      "Epoch 3920, Loss: 0.023351880721747875, Final Batch Loss: 0.010759364813566208\n",
      "Epoch 3921, Loss: 0.018650393467396498, Final Batch Loss: 0.013723580166697502\n",
      "Epoch 3922, Loss: 0.0825501848012209, Final Batch Loss: 0.02224406786262989\n",
      "Epoch 3923, Loss: 0.036708022467792034, Final Batch Loss: 0.009594603441655636\n",
      "Epoch 3924, Loss: 0.04882759042084217, Final Batch Loss: 0.03621479868888855\n",
      "Epoch 3925, Loss: 0.037251899018883705, Final Batch Loss: 0.013462552800774574\n",
      "Epoch 3926, Loss: 0.045590635389089584, Final Batch Loss: 0.023217028006911278\n",
      "Epoch 3927, Loss: 0.03158199042081833, Final Batch Loss: 0.026191318407654762\n",
      "Epoch 3928, Loss: 0.05802401900291443, Final Batch Loss: 0.02826887182891369\n",
      "Epoch 3929, Loss: 0.03286227583885193, Final Batch Loss: 0.02293163351714611\n",
      "Epoch 3930, Loss: 0.03568736929446459, Final Batch Loss: 0.009436666034162045\n",
      "Epoch 3931, Loss: 0.01993985939770937, Final Batch Loss: 0.010444809682667255\n",
      "Epoch 3932, Loss: 0.032391857355833054, Final Batch Loss: 0.01938495598733425\n",
      "Epoch 3933, Loss: 0.02554246550425887, Final Batch Loss: 0.018594685941934586\n",
      "Epoch 3934, Loss: 0.0648998012766242, Final Batch Loss: 0.05799209326505661\n",
      "Epoch 3935, Loss: 0.04893825575709343, Final Batch Loss: 0.026758629828691483\n",
      "Epoch 3936, Loss: 0.04245248343795538, Final Batch Loss: 0.03335011750459671\n",
      "Epoch 3937, Loss: 0.03394066262990236, Final Batch Loss: 0.028925519436597824\n",
      "Epoch 3938, Loss: 0.032991708256304264, Final Batch Loss: 0.009743650443851948\n",
      "Epoch 3939, Loss: 0.01646867161616683, Final Batch Loss: 0.006327613722532988\n",
      "Epoch 3940, Loss: 0.05061541870236397, Final Batch Loss: 0.03274301439523697\n",
      "Epoch 3941, Loss: 0.049217935651540756, Final Batch Loss: 0.024837682023644447\n",
      "Epoch 3942, Loss: 0.033036875538527966, Final Batch Loss: 0.00546638946980238\n",
      "Epoch 3943, Loss: 0.012587455101311207, Final Batch Loss: 0.0023475708439946175\n",
      "Epoch 3944, Loss: 0.031408144161105156, Final Batch Loss: 0.01957736536860466\n",
      "Epoch 3945, Loss: 0.0558848325163126, Final Batch Loss: 0.018208174034953117\n",
      "Epoch 3946, Loss: 0.08815573528409004, Final Batch Loss: 0.07004985213279724\n",
      "Epoch 3947, Loss: 0.041867706924676895, Final Batch Loss: 0.019431455060839653\n",
      "Epoch 3948, Loss: 0.03730645962059498, Final Batch Loss: 0.020005658268928528\n",
      "Epoch 3949, Loss: 0.06493928655982018, Final Batch Loss: 0.042525455355644226\n",
      "Epoch 3950, Loss: 0.06688840687274933, Final Batch Loss: 0.04074788838624954\n",
      "Epoch 3951, Loss: 0.03030952624976635, Final Batch Loss: 0.02097492851316929\n",
      "Epoch 3952, Loss: 0.09738685563206673, Final Batch Loss: 0.037279047071933746\n",
      "Epoch 3953, Loss: 0.03272172063589096, Final Batch Loss: 0.02203378453850746\n",
      "Epoch 3954, Loss: 0.0646412568166852, Final Batch Loss: 0.053225766867399216\n",
      "Epoch 3955, Loss: 0.11970993503928185, Final Batch Loss: 0.048461105674505234\n",
      "Epoch 3956, Loss: 0.03697222098708153, Final Batch Loss: 0.026695240288972855\n",
      "Epoch 3957, Loss: 0.10014045611023903, Final Batch Loss: 0.05072707310318947\n",
      "Epoch 3958, Loss: 0.030444185249507427, Final Batch Loss: 0.01246351283043623\n",
      "Epoch 3959, Loss: 0.0833938717842102, Final Batch Loss: 0.06943453848361969\n",
      "Epoch 3960, Loss: 0.09993171133100986, Final Batch Loss: 0.07592382282018661\n",
      "Epoch 3961, Loss: 0.0847522672265768, Final Batch Loss: 0.05637306347489357\n",
      "Epoch 3962, Loss: 0.05141119100153446, Final Batch Loss: 0.027729788795113564\n",
      "Epoch 3963, Loss: 0.05996919143944979, Final Batch Loss: 0.013359320349991322\n",
      "Epoch 3964, Loss: 0.07846999168395996, Final Batch Loss: 0.06079526245594025\n",
      "Epoch 3965, Loss: 0.056783152744174004, Final Batch Loss: 0.032084871083498\n",
      "Epoch 3966, Loss: 0.02772501017898321, Final Batch Loss: 0.01751287840306759\n",
      "Epoch 3967, Loss: 0.09679916873574257, Final Batch Loss: 0.05396170914173126\n",
      "Epoch 3968, Loss: 0.07659466564655304, Final Batch Loss: 0.039496153593063354\n",
      "Epoch 3969, Loss: 0.04584893770515919, Final Batch Loss: 0.018587836995720863\n",
      "Epoch 3970, Loss: 0.018872497137635946, Final Batch Loss: 0.005935582797974348\n",
      "Epoch 3971, Loss: 0.03183993976563215, Final Batch Loss: 0.022589460015296936\n",
      "Epoch 3972, Loss: 0.022425408009439707, Final Batch Loss: 0.007170234341174364\n",
      "Epoch 3973, Loss: 0.031962193083018064, Final Batch Loss: 0.026941901072859764\n",
      "Epoch 3974, Loss: 0.03040529228746891, Final Batch Loss: 0.014783084392547607\n",
      "Epoch 3975, Loss: 0.024439011700451374, Final Batch Loss: 0.00990018155425787\n",
      "Epoch 3976, Loss: 0.035071542486548424, Final Batch Loss: 0.011422988027334213\n",
      "Epoch 3977, Loss: 0.03366007376462221, Final Batch Loss: 0.020373182371258736\n",
      "Epoch 3978, Loss: 0.03971293941140175, Final Batch Loss: 0.03308994323015213\n",
      "Epoch 3979, Loss: 0.025679088197648525, Final Batch Loss: 0.008042330853641033\n",
      "Epoch 3980, Loss: 0.04266929533332586, Final Batch Loss: 0.013889207504689693\n",
      "Epoch 3981, Loss: 0.028339780867099762, Final Batch Loss: 0.013882381841540337\n",
      "Epoch 3982, Loss: 0.04226873442530632, Final Batch Loss: 0.024536650627851486\n",
      "Epoch 3983, Loss: 0.016054597916081548, Final Batch Loss: 0.0032970693428069353\n",
      "Epoch 3984, Loss: 0.08848172426223755, Final Batch Loss: 0.05801891162991524\n",
      "Epoch 3985, Loss: 0.01701419334858656, Final Batch Loss: 0.007533772848546505\n",
      "Epoch 3986, Loss: 0.024470395408570766, Final Batch Loss: 0.00642030593007803\n",
      "Epoch 3987, Loss: 0.024897871538996696, Final Batch Loss: 0.01168539933860302\n",
      "Epoch 3988, Loss: 0.03339406754821539, Final Batch Loss: 0.013459664769470692\n",
      "Epoch 3989, Loss: 0.02124449284747243, Final Batch Loss: 0.015386391431093216\n",
      "Epoch 3990, Loss: 0.017311491537839174, Final Batch Loss: 0.009504574351012707\n",
      "Epoch 3991, Loss: 0.032370299100875854, Final Batch Loss: 0.012244114652276039\n",
      "Epoch 3992, Loss: 0.06757337972521782, Final Batch Loss: 0.04727613553404808\n",
      "Epoch 3993, Loss: 0.033202096819877625, Final Batch Loss: 0.014392273500561714\n",
      "Epoch 3994, Loss: 0.0568965058773756, Final Batch Loss: 0.009744470939040184\n",
      "Epoch 3995, Loss: 0.045349661726504564, Final Batch Loss: 0.005353190470486879\n",
      "Epoch 3996, Loss: 0.0401720954105258, Final Batch Loss: 0.00380632933229208\n",
      "Epoch 3997, Loss: 0.0498138852417469, Final Batch Loss: 0.017749935388565063\n",
      "Epoch 3998, Loss: 0.05517829395830631, Final Batch Loss: 0.03745778277516365\n",
      "Epoch 3999, Loss: 0.0514702326618135, Final Batch Loss: 0.004784343298524618\n",
      "Epoch 4000, Loss: 0.035931698977947235, Final Batch Loss: 0.009217720478773117\n",
      "Epoch 4001, Loss: 0.01782377902418375, Final Batch Loss: 0.008125091902911663\n",
      "Epoch 4002, Loss: 0.018396988045424223, Final Batch Loss: 0.005313486326485872\n",
      "Epoch 4003, Loss: 0.07284486759454012, Final Batch Loss: 0.05870945006608963\n",
      "Epoch 4004, Loss: 0.03812352940440178, Final Batch Loss: 0.026245487853884697\n",
      "Epoch 4005, Loss: 0.036163618322461843, Final Batch Loss: 0.006975383963435888\n",
      "Epoch 4006, Loss: 0.05787937715649605, Final Batch Loss: 0.02962898649275303\n",
      "Epoch 4007, Loss: 0.040258944034576416, Final Batch Loss: 0.031998928636312485\n",
      "Epoch 4008, Loss: 0.0377792427316308, Final Batch Loss: 0.026213757693767548\n",
      "Epoch 4009, Loss: 0.08706275001168251, Final Batch Loss: 0.05910199135541916\n",
      "Epoch 4010, Loss: 0.13595988787710667, Final Batch Loss: 0.02803926356136799\n",
      "Epoch 4011, Loss: 0.049436165019869804, Final Batch Loss: 0.0076649803668260574\n",
      "Epoch 4012, Loss: 0.13321925699710846, Final Batch Loss: 0.06528244912624359\n",
      "Epoch 4013, Loss: 0.1347809173166752, Final Batch Loss: 0.07710433006286621\n",
      "Epoch 4014, Loss: 0.060746435075998306, Final Batch Loss: 0.036301854997873306\n",
      "Epoch 4015, Loss: 0.0471050851047039, Final Batch Loss: 0.02386704459786415\n",
      "Epoch 4016, Loss: 0.062165144830942154, Final Batch Loss: 0.01731923222541809\n",
      "Epoch 4017, Loss: 0.16037569753825665, Final Batch Loss: 0.1363699734210968\n",
      "Epoch 4018, Loss: 0.06848699040710926, Final Batch Loss: 0.0504460334777832\n",
      "Epoch 4019, Loss: 0.038968078792095184, Final Batch Loss: 0.017651010304689407\n",
      "Epoch 4020, Loss: 0.04500342067331076, Final Batch Loss: 0.010237137787044048\n",
      "Epoch 4021, Loss: 0.04248344013467431, Final Batch Loss: 0.03640551492571831\n",
      "Epoch 4022, Loss: 0.06713716872036457, Final Batch Loss: 0.015342840924859047\n",
      "Epoch 4023, Loss: 0.022561391815543175, Final Batch Loss: 0.013073032721877098\n",
      "Epoch 4024, Loss: 0.02372267609462142, Final Batch Loss: 0.005946512799710035\n",
      "Epoch 4025, Loss: 0.09050515294075012, Final Batch Loss: 0.07372824847698212\n",
      "Epoch 4026, Loss: 0.07884100079536438, Final Batch Loss: 0.05734061822295189\n",
      "Epoch 4027, Loss: 0.057749656960368156, Final Batch Loss: 0.030503103509545326\n",
      "Epoch 4028, Loss: 0.03155357576906681, Final Batch Loss: 0.01935875602066517\n",
      "Epoch 4029, Loss: 0.06001226231455803, Final Batch Loss: 0.027881614863872528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4030, Loss: 0.07005873508751392, Final Batch Loss: 0.04791218787431717\n",
      "Epoch 4031, Loss: 0.04033611621707678, Final Batch Loss: 0.02608444169163704\n",
      "Epoch 4032, Loss: 0.05864584632217884, Final Batch Loss: 0.03180912882089615\n",
      "Epoch 4033, Loss: 0.04060241533443332, Final Batch Loss: 0.0076604667119681835\n",
      "Epoch 4034, Loss: 0.04456809628754854, Final Batch Loss: 0.011823602952063084\n",
      "Epoch 4035, Loss: 0.04698692820966244, Final Batch Loss: 0.020697247236967087\n",
      "Epoch 4036, Loss: 0.045128912664949894, Final Batch Loss: 0.037208378314971924\n",
      "Epoch 4037, Loss: 0.043144235387444496, Final Batch Loss: 0.012488294392824173\n",
      "Epoch 4038, Loss: 0.06020740047097206, Final Batch Loss: 0.034141477197408676\n",
      "Epoch 4039, Loss: 0.09545447677373886, Final Batch Loss: 0.05368730053305626\n",
      "Epoch 4040, Loss: 0.06903389096260071, Final Batch Loss: 0.03740094602108002\n",
      "Epoch 4041, Loss: 0.0934213176369667, Final Batch Loss: 0.03946481645107269\n",
      "Epoch 4042, Loss: 0.07001818716526031, Final Batch Loss: 0.03916756063699722\n",
      "Epoch 4043, Loss: 0.09573706053197384, Final Batch Loss: 0.0701821893453598\n",
      "Epoch 4044, Loss: 0.10092875175178051, Final Batch Loss: 0.08022990077733994\n",
      "Epoch 4045, Loss: 0.061772191897034645, Final Batch Loss: 0.024888230487704277\n",
      "Epoch 4046, Loss: 0.1261015459895134, Final Batch Loss: 0.10581818222999573\n",
      "Epoch 4047, Loss: 0.07719092816114426, Final Batch Loss: 0.030778080224990845\n",
      "Epoch 4048, Loss: 0.055783363059163094, Final Batch Loss: 0.01108122430741787\n",
      "Epoch 4049, Loss: 0.05118595436215401, Final Batch Loss: 0.02219449356198311\n",
      "Epoch 4050, Loss: 0.05642390996217728, Final Batch Loss: 0.03695112466812134\n",
      "Epoch 4051, Loss: 0.043530311435461044, Final Batch Loss: 0.02250065468251705\n",
      "Epoch 4052, Loss: 0.04168232902884483, Final Batch Loss: 0.01968981698155403\n",
      "Epoch 4053, Loss: 0.04544421751052141, Final Batch Loss: 0.03313446417450905\n",
      "Epoch 4054, Loss: 0.0389739666134119, Final Batch Loss: 0.029030045494437218\n",
      "Epoch 4055, Loss: 0.05677915643900633, Final Batch Loss: 0.041959427297115326\n",
      "Epoch 4056, Loss: 0.053846003487706184, Final Batch Loss: 0.017246747389435768\n",
      "Epoch 4057, Loss: 0.03554586321115494, Final Batch Loss: 0.016340123489499092\n",
      "Epoch 4058, Loss: 0.04169206041842699, Final Batch Loss: 0.01260893139988184\n",
      "Epoch 4059, Loss: 0.0668532382696867, Final Batch Loss: 0.031126761808991432\n",
      "Epoch 4060, Loss: 0.13113706558942795, Final Batch Loss: 0.0520457848906517\n",
      "Epoch 4061, Loss: 0.08377963677048683, Final Batch Loss: 0.047107595950365067\n",
      "Epoch 4062, Loss: 0.05201899632811546, Final Batch Loss: 0.03283749520778656\n",
      "Epoch 4063, Loss: 0.06167158670723438, Final Batch Loss: 0.03432682901620865\n",
      "Epoch 4064, Loss: 0.10041244700551033, Final Batch Loss: 0.0315583236515522\n",
      "Epoch 4065, Loss: 0.08683793805539608, Final Batch Loss: 0.02534308470785618\n",
      "Epoch 4066, Loss: 0.04160796292126179, Final Batch Loss: 0.025872167199850082\n",
      "Epoch 4067, Loss: 0.037447500973939896, Final Batch Loss: 0.008622361347079277\n",
      "Epoch 4068, Loss: 0.07305125519633293, Final Batch Loss: 0.03491028770804405\n",
      "Epoch 4069, Loss: 0.04353914223611355, Final Batch Loss: 0.029296137392520905\n",
      "Epoch 4070, Loss: 0.05304034799337387, Final Batch Loss: 0.020226050168275833\n",
      "Epoch 4071, Loss: 0.0654723271727562, Final Batch Loss: 0.02059965953230858\n",
      "Epoch 4072, Loss: 0.07048394158482552, Final Batch Loss: 0.03885849192738533\n",
      "Epoch 4073, Loss: 0.030659690499305725, Final Batch Loss: 0.02026199921965599\n",
      "Epoch 4074, Loss: 0.04710621386766434, Final Batch Loss: 0.030323250219225883\n",
      "Epoch 4075, Loss: 0.05401868559420109, Final Batch Loss: 0.026697220280766487\n",
      "Epoch 4076, Loss: 0.10038919746875763, Final Batch Loss: 0.06822244822978973\n",
      "Epoch 4077, Loss: 0.0706122387200594, Final Batch Loss: 0.054757438600063324\n",
      "Epoch 4078, Loss: 0.0436775553971529, Final Batch Loss: 0.027790851891040802\n",
      "Epoch 4079, Loss: 0.04968671314418316, Final Batch Loss: 0.0338812917470932\n",
      "Epoch 4080, Loss: 0.07188078761100769, Final Batch Loss: 0.031480543315410614\n",
      "Epoch 4081, Loss: 0.056941237300634384, Final Batch Loss: 0.016269434243440628\n",
      "Epoch 4082, Loss: 0.10305616352707148, Final Batch Loss: 0.010373682714998722\n",
      "Epoch 4083, Loss: 0.04587380960583687, Final Batch Loss: 0.014336511492729187\n",
      "Epoch 4084, Loss: 0.030303506180644035, Final Batch Loss: 0.019754786044359207\n",
      "Epoch 4085, Loss: 0.03438034560531378, Final Batch Loss: 0.013160827569663525\n",
      "Epoch 4086, Loss: 0.08525066077709198, Final Batch Loss: 0.057452086359262466\n",
      "Epoch 4087, Loss: 0.03888689028099179, Final Batch Loss: 0.007004499901086092\n",
      "Epoch 4088, Loss: 0.056559568271040916, Final Batch Loss: 0.046904537826776505\n",
      "Epoch 4089, Loss: 0.049301423132419586, Final Batch Loss: 0.032411202788352966\n",
      "Epoch 4090, Loss: 0.03522502863779664, Final Batch Loss: 0.006234352011233568\n",
      "Epoch 4091, Loss: 0.048626089468598366, Final Batch Loss: 0.029781466349959373\n",
      "Epoch 4092, Loss: 0.03126149531453848, Final Batch Loss: 0.020395569503307343\n",
      "Epoch 4093, Loss: 0.055493407882750034, Final Batch Loss: 0.008841299451887608\n",
      "Epoch 4094, Loss: 0.048482853919267654, Final Batch Loss: 0.011478912085294724\n",
      "Epoch 4095, Loss: 0.02896901499480009, Final Batch Loss: 0.009358526207506657\n",
      "Epoch 4096, Loss: 0.05158114805817604, Final Batch Loss: 0.04053620249032974\n",
      "Epoch 4097, Loss: 0.04257940500974655, Final Batch Loss: 0.018028808757662773\n",
      "Epoch 4098, Loss: 0.05717589519917965, Final Batch Loss: 0.019941722974181175\n",
      "Epoch 4099, Loss: 0.02187595935538411, Final Batch Loss: 0.006983876693993807\n",
      "Epoch 4100, Loss: 0.023895205929875374, Final Batch Loss: 0.018721265718340874\n",
      "Epoch 4101, Loss: 0.031136779114603996, Final Batch Loss: 0.015150515362620354\n",
      "Epoch 4102, Loss: 0.014191587455570698, Final Batch Loss: 0.008744949474930763\n",
      "Epoch 4103, Loss: 0.03349500894546509, Final Batch Loss: 0.021026073023676872\n",
      "Epoch 4104, Loss: 0.029592303559184074, Final Batch Loss: 0.021652717143297195\n",
      "Epoch 4105, Loss: 0.03397142980247736, Final Batch Loss: 0.009896215982735157\n",
      "Epoch 4106, Loss: 0.03359539434313774, Final Batch Loss: 0.020631175488233566\n",
      "Epoch 4107, Loss: 0.0832115150988102, Final Batch Loss: 0.047384679317474365\n",
      "Epoch 4108, Loss: 0.042600324377417564, Final Batch Loss: 0.02536340057849884\n",
      "Epoch 4109, Loss: 0.095448674634099, Final Batch Loss: 0.06696560978889465\n",
      "Epoch 4110, Loss: 0.059808832593262196, Final Batch Loss: 0.013344989158213139\n",
      "Epoch 4111, Loss: 0.06381117552518845, Final Batch Loss: 0.04137523099780083\n",
      "Epoch 4112, Loss: 0.013737688306719065, Final Batch Loss: 0.00684214336797595\n",
      "Epoch 4113, Loss: 0.0382960494607687, Final Batch Loss: 0.026374537497758865\n",
      "Epoch 4114, Loss: 0.06327488739043474, Final Batch Loss: 0.01092201191931963\n",
      "Epoch 4115, Loss: 0.03762648068368435, Final Batch Loss: 0.022830992937088013\n",
      "Epoch 4116, Loss: 0.05029118712991476, Final Batch Loss: 0.012739314697682858\n",
      "Epoch 4117, Loss: 0.05265085119754076, Final Batch Loss: 0.04473258927464485\n",
      "Epoch 4118, Loss: 0.027792532928287983, Final Batch Loss: 0.01311140414327383\n",
      "Epoch 4119, Loss: 0.05227546766400337, Final Batch Loss: 0.03577960282564163\n",
      "Epoch 4120, Loss: 0.05530880391597748, Final Batch Loss: 0.044605422765016556\n",
      "Epoch 4121, Loss: 0.02449155133217573, Final Batch Loss: 0.013108523562550545\n",
      "Epoch 4122, Loss: 0.0770039763301611, Final Batch Loss: 0.0176201444119215\n",
      "Epoch 4123, Loss: 0.03650256711989641, Final Batch Loss: 0.02322392724454403\n",
      "Epoch 4124, Loss: 0.026598212774842978, Final Batch Loss: 0.007117451634258032\n",
      "Epoch 4125, Loss: 0.04198251664638519, Final Batch Loss: 0.013725057244300842\n",
      "Epoch 4126, Loss: 0.031731567811220884, Final Batch Loss: 0.02720501832664013\n",
      "Epoch 4127, Loss: 0.04029190260916948, Final Batch Loss: 0.007526741363108158\n",
      "Epoch 4128, Loss: 0.06299477815628052, Final Batch Loss: 0.03403058275580406\n",
      "Epoch 4129, Loss: 0.032048510387539864, Final Batch Loss: 0.013670321553945541\n",
      "Epoch 4130, Loss: 0.09614788368344307, Final Batch Loss: 0.07162698358297348\n",
      "Epoch 4131, Loss: 0.038150456734001637, Final Batch Loss: 0.02399180456995964\n",
      "Epoch 4132, Loss: 0.05070575699210167, Final Batch Loss: 0.027631409466266632\n",
      "Epoch 4133, Loss: 0.06594692915678024, Final Batch Loss: 0.018667306751012802\n",
      "Epoch 4134, Loss: 0.05919626168906689, Final Batch Loss: 0.01986243762075901\n",
      "Epoch 4135, Loss: 0.04883293714374304, Final Batch Loss: 0.014350811950862408\n",
      "Epoch 4136, Loss: 0.10345854610204697, Final Batch Loss: 0.05373215302824974\n",
      "Epoch 4137, Loss: 0.021516372449696064, Final Batch Loss: 0.006612956523895264\n",
      "Epoch 4138, Loss: 0.06359907053411007, Final Batch Loss: 0.028404338285326958\n",
      "Epoch 4139, Loss: 0.08435924258083105, Final Batch Loss: 0.014446734450757504\n",
      "Epoch 4140, Loss: 0.03444250673055649, Final Batch Loss: 0.009764054790139198\n",
      "Epoch 4141, Loss: 0.04552174732089043, Final Batch Loss: 0.027189839631319046\n",
      "Epoch 4142, Loss: 0.0468787532299757, Final Batch Loss: 0.00991634838283062\n",
      "Epoch 4143, Loss: 0.031196672469377518, Final Batch Loss: 0.017567023634910583\n",
      "Epoch 4144, Loss: 0.03669699374586344, Final Batch Loss: 0.009417801164090633\n",
      "Epoch 4145, Loss: 0.06145694851875305, Final Batch Loss: 0.05061684548854828\n",
      "Epoch 4146, Loss: 0.06848875246942043, Final Batch Loss: 0.0581643246114254\n",
      "Epoch 4147, Loss: 0.05372076667845249, Final Batch Loss: 0.03830702230334282\n",
      "Epoch 4148, Loss: 0.04256677720695734, Final Batch Loss: 0.034850139170885086\n",
      "Epoch 4149, Loss: 0.0687260702252388, Final Batch Loss: 0.003997616469860077\n",
      "Epoch 4150, Loss: 0.0505802147090435, Final Batch Loss: 0.03443611040711403\n",
      "Epoch 4151, Loss: 0.05324872210621834, Final Batch Loss: 0.03347789868712425\n",
      "Epoch 4152, Loss: 0.1466331984847784, Final Batch Loss: 0.11730498820543289\n",
      "Epoch 4153, Loss: 0.10433736443519592, Final Batch Loss: 0.06688123196363449\n",
      "Epoch 4154, Loss: 0.05133741721510887, Final Batch Loss: 0.035684894770383835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4155, Loss: 0.05479179695248604, Final Batch Loss: 0.021520573645830154\n",
      "Epoch 4156, Loss: 0.058203866705298424, Final Batch Loss: 0.03480904549360275\n",
      "Epoch 4157, Loss: 0.04015343822538853, Final Batch Loss: 0.018221402540802956\n",
      "Epoch 4158, Loss: 0.08282747492194176, Final Batch Loss: 0.039318084716796875\n",
      "Epoch 4159, Loss: 0.05865267664194107, Final Batch Loss: 0.02626275271177292\n",
      "Epoch 4160, Loss: 0.06470662914216518, Final Batch Loss: 0.04860078543424606\n",
      "Epoch 4161, Loss: 0.0396064892411232, Final Batch Loss: 0.030949421226978302\n",
      "Epoch 4162, Loss: 0.044213020242750645, Final Batch Loss: 0.03330191597342491\n",
      "Epoch 4163, Loss: 0.05444131977856159, Final Batch Loss: 0.042904701083898544\n",
      "Epoch 4164, Loss: 0.07184046879410744, Final Batch Loss: 0.03380506485700607\n",
      "Epoch 4165, Loss: 0.04116968624293804, Final Batch Loss: 0.019913069903850555\n",
      "Epoch 4166, Loss: 0.04192784056067467, Final Batch Loss: 0.02304350584745407\n",
      "Epoch 4167, Loss: 0.029661363922059536, Final Batch Loss: 0.011331203393638134\n",
      "Epoch 4168, Loss: 0.04159172438085079, Final Batch Loss: 0.019799429923295975\n",
      "Epoch 4169, Loss: 0.0462226215749979, Final Batch Loss: 0.02240358479321003\n",
      "Epoch 4170, Loss: 0.030481343157589436, Final Batch Loss: 0.0089985067024827\n",
      "Epoch 4171, Loss: 0.07653986010700464, Final Batch Loss: 0.011983857490122318\n",
      "Epoch 4172, Loss: 0.06303397379815578, Final Batch Loss: 0.041189853101968765\n",
      "Epoch 4173, Loss: 0.05834302864968777, Final Batch Loss: 0.029368605464696884\n",
      "Epoch 4174, Loss: 0.05702153127640486, Final Batch Loss: 0.050196804106235504\n",
      "Epoch 4175, Loss: 0.059018347412347794, Final Batch Loss: 0.02666141465306282\n",
      "Epoch 4176, Loss: 0.025642763823270798, Final Batch Loss: 0.011585363186895847\n",
      "Epoch 4177, Loss: 0.03957914747297764, Final Batch Loss: 0.022284358739852905\n",
      "Epoch 4178, Loss: 0.10356253944337368, Final Batch Loss: 0.09165292978286743\n",
      "Epoch 4179, Loss: 0.08184864744544029, Final Batch Loss: 0.03645623102784157\n",
      "Epoch 4180, Loss: 0.04527515172958374, Final Batch Loss: 0.018125535920262337\n",
      "Epoch 4181, Loss: 0.05334074608981609, Final Batch Loss: 0.03225220367312431\n",
      "Epoch 4182, Loss: 0.036490113474428654, Final Batch Loss: 0.010345079936087132\n",
      "Epoch 4183, Loss: 0.040781717747449875, Final Batch Loss: 0.02556810900568962\n",
      "Epoch 4184, Loss: 0.03529031574726105, Final Batch Loss: 0.018970133736729622\n",
      "Epoch 4185, Loss: 0.029596807435154915, Final Batch Loss: 0.016001299023628235\n",
      "Epoch 4186, Loss: 0.04484819620847702, Final Batch Loss: 0.014294251799583435\n",
      "Epoch 4187, Loss: 0.0352157074958086, Final Batch Loss: 0.015347948297858238\n",
      "Epoch 4188, Loss: 0.03769852966070175, Final Batch Loss: 0.01618592068552971\n",
      "Epoch 4189, Loss: 0.02868952788412571, Final Batch Loss: 0.015478194691240788\n",
      "Epoch 4190, Loss: 0.027112258598208427, Final Batch Loss: 0.01536816731095314\n",
      "Epoch 4191, Loss: 0.0930485688149929, Final Batch Loss: 0.05245741456747055\n",
      "Epoch 4192, Loss: 0.04612751863896847, Final Batch Loss: 0.02509879507124424\n",
      "Epoch 4193, Loss: 0.03651512507349253, Final Batch Loss: 0.022186806425452232\n",
      "Epoch 4194, Loss: 0.031098715029656887, Final Batch Loss: 0.017767880111932755\n",
      "Epoch 4195, Loss: 0.056581538170576096, Final Batch Loss: 0.018140893429517746\n",
      "Epoch 4196, Loss: 0.026705381460487843, Final Batch Loss: 0.016301700845360756\n",
      "Epoch 4197, Loss: 0.03166747000068426, Final Batch Loss: 0.016352428123354912\n",
      "Epoch 4198, Loss: 0.02979131229221821, Final Batch Loss: 0.01701943390071392\n",
      "Epoch 4199, Loss: 0.020279685966670513, Final Batch Loss: 0.00949806161224842\n",
      "Epoch 4200, Loss: 0.03820463456213474, Final Batch Loss: 0.030424928292632103\n",
      "Epoch 4201, Loss: 0.060116895474493504, Final Batch Loss: 0.04978257045149803\n",
      "Epoch 4202, Loss: 0.02644717413932085, Final Batch Loss: 0.019160142168402672\n",
      "Epoch 4203, Loss: 0.021350808441638947, Final Batch Loss: 0.01559453271329403\n",
      "Epoch 4204, Loss: 0.034944857470691204, Final Batch Loss: 0.01172820944339037\n",
      "Epoch 4205, Loss: 0.02633632905781269, Final Batch Loss: 0.011790767312049866\n",
      "Epoch 4206, Loss: 0.06790992990136147, Final Batch Loss: 0.01348039135336876\n",
      "Epoch 4207, Loss: 0.036360643804073334, Final Batch Loss: 0.018107805401086807\n",
      "Epoch 4208, Loss: 0.031779997050762177, Final Batch Loss: 0.015995148569345474\n",
      "Epoch 4209, Loss: 0.03385266847908497, Final Batch Loss: 0.011314664036035538\n",
      "Epoch 4210, Loss: 0.05452495347708464, Final Batch Loss: 0.038937315344810486\n",
      "Epoch 4211, Loss: 0.050437845289707184, Final Batch Loss: 0.026220254600048065\n",
      "Epoch 4212, Loss: 0.034714686684310436, Final Batch Loss: 0.010689419694244862\n",
      "Epoch 4213, Loss: 0.03803114593029022, Final Batch Loss: 0.018247412517666817\n",
      "Epoch 4214, Loss: 0.052027628757059574, Final Batch Loss: 0.004850086756050587\n",
      "Epoch 4215, Loss: 0.03961728326976299, Final Batch Loss: 0.021757273003458977\n",
      "Epoch 4216, Loss: 0.013778331223875284, Final Batch Loss: 0.003909432794898748\n",
      "Epoch 4217, Loss: 0.04134887922555208, Final Batch Loss: 0.014772349037230015\n",
      "Epoch 4218, Loss: 0.05639204382896423, Final Batch Loss: 0.01909637451171875\n",
      "Epoch 4219, Loss: 0.051143964752554893, Final Batch Loss: 0.02712138369679451\n",
      "Epoch 4220, Loss: 0.03152985963970423, Final Batch Loss: 0.020230116322636604\n",
      "Epoch 4221, Loss: 0.044494688510894775, Final Batch Loss: 0.010242700576782227\n",
      "Epoch 4222, Loss: 0.09743891842663288, Final Batch Loss: 0.06788869947195053\n",
      "Epoch 4223, Loss: 0.0364246042445302, Final Batch Loss: 0.009979848749935627\n",
      "Epoch 4224, Loss: 0.06254277378320694, Final Batch Loss: 0.045924559235572815\n",
      "Epoch 4225, Loss: 0.02592899650335312, Final Batch Loss: 0.005594097077846527\n",
      "Epoch 4226, Loss: 0.042639339342713356, Final Batch Loss: 0.012587428092956543\n",
      "Epoch 4227, Loss: 0.028950940817594528, Final Batch Loss: 0.0165586955845356\n",
      "Epoch 4228, Loss: 0.04340021125972271, Final Batch Loss: 0.019133033230900764\n",
      "Epoch 4229, Loss: 0.029306272976100445, Final Batch Loss: 0.0066066524013876915\n",
      "Epoch 4230, Loss: 0.05086689628660679, Final Batch Loss: 0.01899649389088154\n",
      "Epoch 4231, Loss: 0.029255532659590244, Final Batch Loss: 0.019644824787974358\n",
      "Epoch 4232, Loss: 0.035075267776846886, Final Batch Loss: 0.01754312962293625\n",
      "Epoch 4233, Loss: 0.049830030649900436, Final Batch Loss: 0.036126621067523956\n",
      "Epoch 4234, Loss: 0.018541008234024048, Final Batch Loss: 0.008394502103328705\n",
      "Epoch 4235, Loss: 0.0353036280721426, Final Batch Loss: 0.01970764249563217\n",
      "Epoch 4236, Loss: 0.04935399442911148, Final Batch Loss: 0.03261047601699829\n",
      "Epoch 4237, Loss: 0.14550761505961418, Final Batch Loss: 0.09158534556627274\n",
      "Epoch 4238, Loss: 0.08188429474830627, Final Batch Loss: 0.033182594925165176\n",
      "Epoch 4239, Loss: 0.13988374546170235, Final Batch Loss: 0.08099719882011414\n",
      "Epoch 4240, Loss: 0.031941309571266174, Final Batch Loss: 0.01089792512357235\n",
      "Epoch 4241, Loss: 0.041573416441679, Final Batch Loss: 0.019759058952331543\n",
      "Epoch 4242, Loss: 0.07385697960853577, Final Batch Loss: 0.047613583505153656\n",
      "Epoch 4243, Loss: 0.05112415738403797, Final Batch Loss: 0.022363636642694473\n",
      "Epoch 4244, Loss: 0.10962363332509995, Final Batch Loss: 0.03345830738544464\n",
      "Epoch 4245, Loss: 0.032453591004014015, Final Batch Loss: 0.01920522376894951\n",
      "Epoch 4246, Loss: 0.05781347118318081, Final Batch Loss: 0.030380262061953545\n",
      "Epoch 4247, Loss: 0.06177768111228943, Final Batch Loss: 0.02199944108724594\n",
      "Epoch 4248, Loss: 0.057896563783288, Final Batch Loss: 0.031855978071689606\n",
      "Epoch 4249, Loss: 0.035360322799533606, Final Batch Loss: 0.006954565178602934\n",
      "Epoch 4250, Loss: 0.07339426875114441, Final Batch Loss: 0.026926938444375992\n",
      "Epoch 4251, Loss: 0.07039562612771988, Final Batch Loss: 0.03077051043510437\n",
      "Epoch 4252, Loss: 0.02901473455131054, Final Batch Loss: 0.015509720891714096\n",
      "Epoch 4253, Loss: 0.017130275256931782, Final Batch Loss: 0.005240556783974171\n",
      "Epoch 4254, Loss: 0.033573905006051064, Final Batch Loss: 0.016337867826223373\n",
      "Epoch 4255, Loss: 0.04714495316147804, Final Batch Loss: 0.02643389254808426\n",
      "Epoch 4256, Loss: 0.06145365536212921, Final Batch Loss: 0.037042293697595596\n",
      "Epoch 4257, Loss: 0.04429513216018677, Final Batch Loss: 0.021432332694530487\n",
      "Epoch 4258, Loss: 0.06877463683485985, Final Batch Loss: 0.04726335033774376\n",
      "Epoch 4259, Loss: 0.06842085532844067, Final Batch Loss: 0.04027226194739342\n",
      "Epoch 4260, Loss: 0.039534494280815125, Final Batch Loss: 0.010251585394144058\n",
      "Epoch 4261, Loss: 0.029412930831313133, Final Batch Loss: 0.019241122528910637\n",
      "Epoch 4262, Loss: 0.029131684452295303, Final Batch Loss: 0.02096380665898323\n",
      "Epoch 4263, Loss: 0.06198358256369829, Final Batch Loss: 0.05271400883793831\n",
      "Epoch 4264, Loss: 0.0553298220038414, Final Batch Loss: 0.044168032705783844\n",
      "Epoch 4265, Loss: 0.07381057366728783, Final Batch Loss: 0.048751458525657654\n",
      "Epoch 4266, Loss: 0.06378745660185814, Final Batch Loss: 0.03620878607034683\n",
      "Epoch 4267, Loss: 0.047975970432162285, Final Batch Loss: 0.013291439041495323\n",
      "Epoch 4268, Loss: 0.016090912278741598, Final Batch Loss: 0.00913385208696127\n",
      "Epoch 4269, Loss: 0.049041492864489555, Final Batch Loss: 0.022164635360240936\n",
      "Epoch 4270, Loss: 0.027313907630741596, Final Batch Loss: 0.01745552569627762\n",
      "Epoch 4271, Loss: 0.07835215516388416, Final Batch Loss: 0.056262195110321045\n",
      "Epoch 4272, Loss: 0.0341768953949213, Final Batch Loss: 0.010596266016364098\n",
      "Epoch 4273, Loss: 0.015106239821761847, Final Batch Loss: 0.009487458504736423\n",
      "Epoch 4274, Loss: 0.05336534045636654, Final Batch Loss: 0.03818400949239731\n",
      "Epoch 4275, Loss: 0.042171910405159, Final Batch Loss: 0.02574405074119568\n",
      "Epoch 4276, Loss: 0.06692808493971825, Final Batch Loss: 0.04507404938340187\n",
      "Epoch 4277, Loss: 0.04557865019887686, Final Batch Loss: 0.012775558046996593\n",
      "Epoch 4278, Loss: 0.019461617805063725, Final Batch Loss: 0.004794580861926079\n",
      "Epoch 4279, Loss: 0.03283556131646037, Final Batch Loss: 0.02635776810348034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4280, Loss: 0.05022990331053734, Final Batch Loss: 0.018783003091812134\n",
      "Epoch 4281, Loss: 0.013528466690331697, Final Batch Loss: 0.007837370969355106\n",
      "Epoch 4282, Loss: 0.03934512287378311, Final Batch Loss: 0.02103203907608986\n",
      "Epoch 4283, Loss: 0.025643538683652878, Final Batch Loss: 0.015251122415065765\n",
      "Epoch 4284, Loss: 0.051164766773581505, Final Batch Loss: 0.034100983291864395\n",
      "Epoch 4285, Loss: 0.02634421829134226, Final Batch Loss: 0.017648017033934593\n",
      "Epoch 4286, Loss: 0.019717125687748194, Final Batch Loss: 0.006346970330923796\n",
      "Epoch 4287, Loss: 0.057080186903476715, Final Batch Loss: 0.01938071846961975\n",
      "Epoch 4288, Loss: 0.08060770481824875, Final Batch Loss: 0.04318464547395706\n",
      "Epoch 4289, Loss: 0.03888511098921299, Final Batch Loss: 0.021395055577158928\n",
      "Epoch 4290, Loss: 0.03555198013782501, Final Batch Loss: 0.016489742323756218\n",
      "Epoch 4291, Loss: 0.025585792493075132, Final Batch Loss: 0.019739730283617973\n",
      "Epoch 4292, Loss: 0.04945193976163864, Final Batch Loss: 0.019715383648872375\n",
      "Epoch 4293, Loss: 0.02418487798422575, Final Batch Loss: 0.008782878518104553\n",
      "Epoch 4294, Loss: 0.07869885489344597, Final Batch Loss: 0.057541221380233765\n",
      "Epoch 4295, Loss: 0.03937792684882879, Final Batch Loss: 0.012736831791698933\n",
      "Epoch 4296, Loss: 0.035533768124878407, Final Batch Loss: 0.0076040951535105705\n",
      "Epoch 4297, Loss: 0.021257687360048294, Final Batch Loss: 0.004679851233959198\n",
      "Epoch 4298, Loss: 0.06702635250985622, Final Batch Loss: 0.038766536861658096\n",
      "Epoch 4299, Loss: 0.03866834007203579, Final Batch Loss: 0.021046875044703484\n",
      "Epoch 4300, Loss: 0.048726314678788185, Final Batch Loss: 0.028132189065217972\n",
      "Epoch 4301, Loss: 0.04248129762709141, Final Batch Loss: 0.0298696830868721\n",
      "Epoch 4302, Loss: 0.012894662097096443, Final Batch Loss: 0.00817920546978712\n",
      "Epoch 4303, Loss: 0.046164088882505894, Final Batch Loss: 0.011250966228544712\n",
      "Epoch 4304, Loss: 0.041114317253232, Final Batch Loss: 0.023828694596886635\n",
      "Epoch 4305, Loss: 0.019618660677224398, Final Batch Loss: 0.012796374037861824\n",
      "Epoch 4306, Loss: 0.020084256306290627, Final Batch Loss: 0.01199424173682928\n",
      "Epoch 4307, Loss: 0.017293440643697977, Final Batch Loss: 0.00617849500849843\n",
      "Epoch 4308, Loss: 0.012598262634128332, Final Batch Loss: 0.006975474767386913\n",
      "Epoch 4309, Loss: 0.02230716682970524, Final Batch Loss: 0.01253871712833643\n",
      "Epoch 4310, Loss: 0.045624107122421265, Final Batch Loss: 0.013002417981624603\n",
      "Epoch 4311, Loss: 0.04274040833115578, Final Batch Loss: 0.0364583358168602\n",
      "Epoch 4312, Loss: 0.07001139782369137, Final Batch Loss: 0.008060818538069725\n",
      "Epoch 4313, Loss: 0.05645551159977913, Final Batch Loss: 0.031186886131763458\n",
      "Epoch 4314, Loss: 0.04446978587657213, Final Batch Loss: 0.008314014412462711\n",
      "Epoch 4315, Loss: 0.07828052155673504, Final Batch Loss: 0.05083417892456055\n",
      "Epoch 4316, Loss: 0.07817881647497416, Final Batch Loss: 0.06621601432561874\n",
      "Epoch 4317, Loss: 0.06017020344734192, Final Batch Loss: 0.03906220942735672\n",
      "Epoch 4318, Loss: 0.03052991908043623, Final Batch Loss: 0.016840631142258644\n",
      "Epoch 4319, Loss: 0.05275542847812176, Final Batch Loss: 0.017751002684235573\n",
      "Epoch 4320, Loss: 0.02486256556585431, Final Batch Loss: 0.006513640750199556\n",
      "Epoch 4321, Loss: 0.01900891773402691, Final Batch Loss: 0.010523984208703041\n",
      "Epoch 4322, Loss: 0.03838982246816158, Final Batch Loss: 0.01077829860150814\n",
      "Epoch 4323, Loss: 0.08516510389745235, Final Batch Loss: 0.018284855410456657\n",
      "Epoch 4324, Loss: 0.06359499879181385, Final Batch Loss: 0.02984357438981533\n",
      "Epoch 4325, Loss: 0.12550552934408188, Final Batch Loss: 0.07723278552293777\n",
      "Epoch 4326, Loss: 0.03820029413327575, Final Batch Loss: 0.030638940632343292\n",
      "Epoch 4327, Loss: 0.08932502940297127, Final Batch Loss: 0.0350814014673233\n",
      "Epoch 4328, Loss: 0.05277166608721018, Final Batch Loss: 0.04021233320236206\n",
      "Epoch 4329, Loss: 0.05518448352813721, Final Batch Loss: 0.014712456613779068\n",
      "Epoch 4330, Loss: 0.0503634512424469, Final Batch Loss: 0.01828698441386223\n",
      "Epoch 4331, Loss: 0.03229236789047718, Final Batch Loss: 0.01572093367576599\n",
      "Epoch 4332, Loss: 0.07594744674861431, Final Batch Loss: 0.0581631064414978\n",
      "Epoch 4333, Loss: 0.03143392689526081, Final Batch Loss: 0.01046626828610897\n",
      "Epoch 4334, Loss: 0.06518256291747093, Final Batch Loss: 0.025376081466674805\n",
      "Epoch 4335, Loss: 0.0616522878408432, Final Batch Loss: 0.03838226571679115\n",
      "Epoch 4336, Loss: 0.037463036365807056, Final Batch Loss: 0.022281145676970482\n",
      "Epoch 4337, Loss: 0.039864580146968365, Final Batch Loss: 0.013129633851349354\n",
      "Epoch 4338, Loss: 0.0777321495115757, Final Batch Loss: 0.05085548013448715\n",
      "Epoch 4339, Loss: 0.06612534634768963, Final Batch Loss: 0.021098269149661064\n",
      "Epoch 4340, Loss: 0.025654340162873268, Final Batch Loss: 0.009270764887332916\n",
      "Epoch 4341, Loss: 0.05370215326547623, Final Batch Loss: 0.03674132749438286\n",
      "Epoch 4342, Loss: 0.03581539914011955, Final Batch Loss: 0.007361609488725662\n",
      "Epoch 4343, Loss: 0.08543659932911396, Final Batch Loss: 0.05570835992693901\n",
      "Epoch 4344, Loss: 0.08243058808147907, Final Batch Loss: 0.05392026901245117\n",
      "Epoch 4345, Loss: 0.04373915120959282, Final Batch Loss: 0.022302137687802315\n",
      "Epoch 4346, Loss: 0.05555906146764755, Final Batch Loss: 0.021863393485546112\n",
      "Epoch 4347, Loss: 0.042364874854683876, Final Batch Loss: 0.034219563007354736\n",
      "Epoch 4348, Loss: 0.02083877194672823, Final Batch Loss: 0.010027239099144936\n",
      "Epoch 4349, Loss: 0.03207908011972904, Final Batch Loss: 0.015049494802951813\n",
      "Epoch 4350, Loss: 0.053405269514769316, Final Batch Loss: 0.007666226010769606\n",
      "Epoch 4351, Loss: 0.03091719839721918, Final Batch Loss: 0.015376218594610691\n",
      "Epoch 4352, Loss: 0.035589614883065224, Final Batch Loss: 0.01058056578040123\n",
      "Epoch 4353, Loss: 0.08192107453942299, Final Batch Loss: 0.06699688732624054\n",
      "Epoch 4354, Loss: 0.023058604449033737, Final Batch Loss: 0.013513685204088688\n",
      "Epoch 4355, Loss: 0.07013727631419897, Final Batch Loss: 0.055222660303115845\n",
      "Epoch 4356, Loss: 0.10359885916113853, Final Batch Loss: 0.06803028285503387\n",
      "Epoch 4357, Loss: 0.0501044187694788, Final Batch Loss: 0.023868417367339134\n",
      "Epoch 4358, Loss: 0.031969355419278145, Final Batch Loss: 0.007411230355501175\n",
      "Epoch 4359, Loss: 0.06292901933193207, Final Batch Loss: 0.0319858156144619\n",
      "Epoch 4360, Loss: 0.07171486131846905, Final Batch Loss: 0.013316920027136803\n",
      "Epoch 4361, Loss: 0.01863518264144659, Final Batch Loss: 0.009118404239416122\n",
      "Epoch 4362, Loss: 0.016564117278903723, Final Batch Loss: 0.010037834756076336\n",
      "Epoch 4363, Loss: 0.023367916233837605, Final Batch Loss: 0.01501184981316328\n",
      "Epoch 4364, Loss: 0.03229421749711037, Final Batch Loss: 0.010419681668281555\n",
      "Epoch 4365, Loss: 0.11187779530882835, Final Batch Loss: 0.03154506906867027\n",
      "Epoch 4366, Loss: 0.025919930543750525, Final Batch Loss: 0.01951633207499981\n",
      "Epoch 4367, Loss: 0.017719199415296316, Final Batch Loss: 0.0049663870595395565\n",
      "Epoch 4368, Loss: 0.07384096458554268, Final Batch Loss: 0.03387940302491188\n",
      "Epoch 4369, Loss: 0.023405710235238075, Final Batch Loss: 0.007990301586687565\n",
      "Epoch 4370, Loss: 0.041490502655506134, Final Batch Loss: 0.020211614668369293\n",
      "Epoch 4371, Loss: 0.03823193907737732, Final Batch Loss: 0.015978442505002022\n",
      "Epoch 4372, Loss: 0.13028528541326523, Final Batch Loss: 0.08527840673923492\n",
      "Epoch 4373, Loss: 0.03259247774258256, Final Batch Loss: 0.006662779022008181\n",
      "Epoch 4374, Loss: 0.09995508566498756, Final Batch Loss: 0.07837425172328949\n",
      "Epoch 4375, Loss: 0.05809658393263817, Final Batch Loss: 0.016713324934244156\n",
      "Epoch 4376, Loss: 0.020244386978447437, Final Batch Loss: 0.011358730494976044\n",
      "Epoch 4377, Loss: 0.03023242112249136, Final Batch Loss: 0.006619815714657307\n",
      "Epoch 4378, Loss: 0.04637482762336731, Final Batch Loss: 0.035259488970041275\n",
      "Epoch 4379, Loss: 0.03310469537973404, Final Batch Loss: 0.013314073905348778\n",
      "Epoch 4380, Loss: 0.036512505263090134, Final Batch Loss: 0.016475046053528786\n",
      "Epoch 4381, Loss: 0.04944245703518391, Final Batch Loss: 0.022026563063263893\n",
      "Epoch 4382, Loss: 0.030930425971746445, Final Batch Loss: 0.014561153948307037\n",
      "Epoch 4383, Loss: 0.07166546955704689, Final Batch Loss: 0.035174429416656494\n",
      "Epoch 4384, Loss: 0.03712192177772522, Final Batch Loss: 0.00884145125746727\n",
      "Epoch 4385, Loss: 0.06655548140406609, Final Batch Loss: 0.020578041672706604\n",
      "Epoch 4386, Loss: 0.04746522381901741, Final Batch Loss: 0.008498385548591614\n",
      "Epoch 4387, Loss: 0.06968681886792183, Final Batch Loss: 0.018122516572475433\n",
      "Epoch 4388, Loss: 0.05342430993914604, Final Batch Loss: 0.019546110183000565\n",
      "Epoch 4389, Loss: 0.0735345408320427, Final Batch Loss: 0.0444493368268013\n",
      "Epoch 4390, Loss: 0.03220824431627989, Final Batch Loss: 0.02436905726790428\n",
      "Epoch 4391, Loss: 0.05740627646446228, Final Batch Loss: 0.03861376270651817\n",
      "Epoch 4392, Loss: 0.05356882233172655, Final Batch Loss: 0.044245198369026184\n",
      "Epoch 4393, Loss: 0.043885184451937675, Final Batch Loss: 0.03588263317942619\n",
      "Epoch 4394, Loss: 0.04495893605053425, Final Batch Loss: 0.02943003922700882\n",
      "Epoch 4395, Loss: 0.05072830989956856, Final Batch Loss: 0.043072640895843506\n",
      "Epoch 4396, Loss: 0.02250752178952098, Final Batch Loss: 0.015989739447832108\n",
      "Epoch 4397, Loss: 0.09547285363078117, Final Batch Loss: 0.06769272685050964\n",
      "Epoch 4398, Loss: 0.024868623353540897, Final Batch Loss: 0.007570345886051655\n",
      "Epoch 4399, Loss: 0.048115044832229614, Final Batch Loss: 0.02758217602968216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4400, Loss: 0.04809821397066116, Final Batch Loss: 0.015748154371976852\n",
      "Epoch 4401, Loss: 0.03459502477198839, Final Batch Loss: 0.027097901329398155\n",
      "Epoch 4402, Loss: 0.028053753543645144, Final Batch Loss: 0.003097016829997301\n",
      "Epoch 4403, Loss: 0.032133346889168024, Final Batch Loss: 0.025723226368427277\n",
      "Epoch 4404, Loss: 0.02692589908838272, Final Batch Loss: 0.013052879832684994\n",
      "Epoch 4405, Loss: 0.03553565498441458, Final Batch Loss: 0.0106692248955369\n",
      "Epoch 4406, Loss: 0.06925670802593231, Final Batch Loss: 0.032706066966056824\n",
      "Epoch 4407, Loss: 0.026680351234972477, Final Batch Loss: 0.009966247715055943\n",
      "Epoch 4408, Loss: 0.06587873958051205, Final Batch Loss: 0.024375444278120995\n",
      "Epoch 4409, Loss: 0.04857040196657181, Final Batch Loss: 0.00919831171631813\n",
      "Epoch 4410, Loss: 0.021519876550883055, Final Batch Loss: 0.014484154991805553\n",
      "Epoch 4411, Loss: 0.04807084798812866, Final Batch Loss: 0.0262793879956007\n",
      "Epoch 4412, Loss: 0.04426898481324315, Final Batch Loss: 0.00607299106195569\n",
      "Epoch 4413, Loss: 0.04829399287700653, Final Batch Loss: 0.028122618794441223\n",
      "Epoch 4414, Loss: 0.02226028311997652, Final Batch Loss: 0.007220543920993805\n",
      "Epoch 4415, Loss: 0.04931309632956982, Final Batch Loss: 0.03225816786289215\n",
      "Epoch 4416, Loss: 0.04670453071594238, Final Batch Loss: 0.011213820427656174\n",
      "Epoch 4417, Loss: 0.06744785234332085, Final Batch Loss: 0.034912265837192535\n",
      "Epoch 4418, Loss: 0.07743076980113983, Final Batch Loss: 0.03377952799201012\n",
      "Epoch 4419, Loss: 0.017557297367602587, Final Batch Loss: 0.005730059463530779\n",
      "Epoch 4420, Loss: 0.04843908827751875, Final Batch Loss: 0.03861244022846222\n",
      "Epoch 4421, Loss: 0.06370211206376553, Final Batch Loss: 0.014736631885170937\n",
      "Epoch 4422, Loss: 0.0709209949709475, Final Batch Loss: 0.007497368846088648\n",
      "Epoch 4423, Loss: 0.030874744057655334, Final Batch Loss: 0.010553186759352684\n",
      "Epoch 4424, Loss: 0.04781544208526611, Final Batch Loss: 0.017825383692979813\n",
      "Epoch 4425, Loss: 0.06630310788750648, Final Batch Loss: 0.01723352074623108\n",
      "Epoch 4426, Loss: 0.07719540596008301, Final Batch Loss: 0.025991883128881454\n",
      "Epoch 4427, Loss: 0.012372908182442188, Final Batch Loss: 0.00855587050318718\n",
      "Epoch 4428, Loss: 0.05589492991566658, Final Batch Loss: 0.05126354843378067\n",
      "Epoch 4429, Loss: 0.020353997591882944, Final Batch Loss: 0.013702549040317535\n",
      "Epoch 4430, Loss: 0.03203568980097771, Final Batch Loss: 0.01700533740222454\n",
      "Epoch 4431, Loss: 0.02438725996762514, Final Batch Loss: 0.008503223769366741\n",
      "Epoch 4432, Loss: 0.023549659177660942, Final Batch Loss: 0.00852106511592865\n",
      "Epoch 4433, Loss: 0.0671708881855011, Final Batch Loss: 0.012255705893039703\n",
      "Epoch 4434, Loss: 0.019117665477097034, Final Batch Loss: 0.00986203271895647\n",
      "Epoch 4435, Loss: 0.036849568132311106, Final Batch Loss: 0.030185217037796974\n",
      "Epoch 4436, Loss: 0.048703357577323914, Final Batch Loss: 0.03610427305102348\n",
      "Epoch 4437, Loss: 0.03163930494338274, Final Batch Loss: 0.011197303421795368\n",
      "Epoch 4438, Loss: 0.01573198731057346, Final Batch Loss: 0.012300829403102398\n",
      "Epoch 4439, Loss: 0.010425286833196878, Final Batch Loss: 0.006390477996319532\n",
      "Epoch 4440, Loss: 0.021862849593162537, Final Batch Loss: 0.010270830243825912\n",
      "Epoch 4441, Loss: 0.04513181280344725, Final Batch Loss: 0.031797561794519424\n",
      "Epoch 4442, Loss: 0.03325274167582393, Final Batch Loss: 0.026917297393083572\n",
      "Epoch 4443, Loss: 0.04164921375922859, Final Batch Loss: 0.002794707892462611\n",
      "Epoch 4444, Loss: 0.010769835906103253, Final Batch Loss: 0.002725037978962064\n",
      "Epoch 4445, Loss: 0.024967246921733022, Final Batch Loss: 0.002738501178100705\n",
      "Epoch 4446, Loss: 0.02622800972312689, Final Batch Loss: 0.019374942407011986\n",
      "Epoch 4447, Loss: 0.022803841158747673, Final Batch Loss: 0.009428724646568298\n",
      "Epoch 4448, Loss: 0.04958277940750122, Final Batch Loss: 0.014948222786188126\n",
      "Epoch 4449, Loss: 0.05427385028451681, Final Batch Loss: 0.041531357914209366\n",
      "Epoch 4450, Loss: 0.10112562775611877, Final Batch Loss: 0.021652698516845703\n",
      "Epoch 4451, Loss: 0.03235269617289305, Final Batch Loss: 0.00903404038399458\n",
      "Epoch 4452, Loss: 0.03691461868584156, Final Batch Loss: 0.005848998203873634\n",
      "Epoch 4453, Loss: 0.03524634335190058, Final Batch Loss: 0.022607440128922462\n",
      "Epoch 4454, Loss: 0.029738097684457898, Final Batch Loss: 0.0030199785251170397\n",
      "Epoch 4455, Loss: 0.030149330385029316, Final Batch Loss: 0.01999330148100853\n",
      "Epoch 4456, Loss: 0.08390229754149914, Final Batch Loss: 0.017984366044402122\n",
      "Epoch 4457, Loss: 0.08556242566555738, Final Batch Loss: 0.07055111974477768\n",
      "Epoch 4458, Loss: 0.14126700721681118, Final Batch Loss: 0.1114763468503952\n",
      "Epoch 4459, Loss: 0.02860849630087614, Final Batch Loss: 0.016306420788168907\n",
      "Epoch 4460, Loss: 0.05444946885108948, Final Batch Loss: 0.02695026807487011\n",
      "Epoch 4461, Loss: 0.04183240234851837, Final Batch Loss: 0.029101435095071793\n",
      "Epoch 4462, Loss: 0.03150857239961624, Final Batch Loss: 0.006253479048609734\n",
      "Epoch 4463, Loss: 0.046565378084778786, Final Batch Loss: 0.03221966326236725\n",
      "Epoch 4464, Loss: 0.03700842522084713, Final Batch Loss: 0.019233299419283867\n",
      "Epoch 4465, Loss: 0.044585594441741705, Final Batch Loss: 0.03818892315030098\n",
      "Epoch 4466, Loss: 0.06037810631096363, Final Batch Loss: 0.025373434647917747\n",
      "Epoch 4467, Loss: 0.12924169469624758, Final Batch Loss: 0.12119404971599579\n",
      "Epoch 4468, Loss: 0.029584920033812523, Final Batch Loss: 0.015389891341328621\n",
      "Epoch 4469, Loss: 0.028557442128658295, Final Batch Loss: 0.01605033129453659\n",
      "Epoch 4470, Loss: 0.0523987440392375, Final Batch Loss: 0.013184870593249798\n",
      "Epoch 4471, Loss: 0.02181227970868349, Final Batch Loss: 0.003918814472854137\n",
      "Epoch 4472, Loss: 0.028333282098174095, Final Batch Loss: 0.004396047443151474\n",
      "Epoch 4473, Loss: 0.06949589401483536, Final Batch Loss: 0.028084374964237213\n",
      "Epoch 4474, Loss: 0.11122231557965279, Final Batch Loss: 0.05270162224769592\n",
      "Epoch 4475, Loss: 0.05629110708832741, Final Batch Loss: 0.017668120563030243\n",
      "Epoch 4476, Loss: 0.04473773576319218, Final Batch Loss: 0.016327204182744026\n",
      "Epoch 4477, Loss: 0.04583614878356457, Final Batch Loss: 0.021315107122063637\n",
      "Epoch 4478, Loss: 0.08687841892242432, Final Batch Loss: 0.07099011540412903\n",
      "Epoch 4479, Loss: 0.039210423827171326, Final Batch Loss: 0.02075727842748165\n",
      "Epoch 4480, Loss: 0.02806002157740295, Final Batch Loss: 0.0024154323618859053\n",
      "Epoch 4481, Loss: 0.07140739820897579, Final Batch Loss: 0.02959439717233181\n",
      "Epoch 4482, Loss: 0.03364854771643877, Final Batch Loss: 0.01411671843379736\n",
      "Epoch 4483, Loss: 0.04982879478484392, Final Batch Loss: 0.03725652024149895\n",
      "Epoch 4484, Loss: 0.034263149835169315, Final Batch Loss: 0.012758235447108746\n",
      "Epoch 4485, Loss: 0.10004468262195587, Final Batch Loss: 0.06543371826410294\n",
      "Epoch 4486, Loss: 0.06183438375592232, Final Batch Loss: 0.03725379705429077\n",
      "Epoch 4487, Loss: 0.04219003766775131, Final Batch Loss: 0.01924016699194908\n",
      "Epoch 4488, Loss: 0.04747018963098526, Final Batch Loss: 0.030669206753373146\n",
      "Epoch 4489, Loss: 0.04742169287055731, Final Batch Loss: 0.010710570029914379\n",
      "Epoch 4490, Loss: 0.045147787779569626, Final Batch Loss: 0.009563472121953964\n",
      "Epoch 4491, Loss: 0.04358829464763403, Final Batch Loss: 0.0317792072892189\n",
      "Epoch 4492, Loss: 0.16470980644226074, Final Batch Loss: 0.09589000046253204\n",
      "Epoch 4493, Loss: 0.0742532629519701, Final Batch Loss: 0.005232321098446846\n",
      "Epoch 4494, Loss: 0.09620656073093414, Final Batch Loss: 0.05146714299917221\n",
      "Epoch 4495, Loss: 0.0801389841362834, Final Batch Loss: 0.009968171827495098\n",
      "Epoch 4496, Loss: 0.06915234588086605, Final Batch Loss: 0.0525599867105484\n",
      "Epoch 4497, Loss: 0.08088474720716476, Final Batch Loss: 0.037103332579135895\n",
      "Epoch 4498, Loss: 0.06492845341563225, Final Batch Loss: 0.0314921997487545\n",
      "Epoch 4499, Loss: 0.050738636404275894, Final Batch Loss: 0.013785243034362793\n",
      "Epoch 4500, Loss: 0.05118013545870781, Final Batch Loss: 0.018811870366334915\n",
      "Epoch 4501, Loss: 0.05450672283768654, Final Batch Loss: 0.005448274314403534\n",
      "Epoch 4502, Loss: 0.05336654372513294, Final Batch Loss: 0.022262878715991974\n",
      "Epoch 4503, Loss: 0.05082403030246496, Final Batch Loss: 0.03670503571629524\n",
      "Epoch 4504, Loss: 0.12507343292236328, Final Batch Loss: 0.08871650695800781\n",
      "Epoch 4505, Loss: 0.05504757910966873, Final Batch Loss: 0.03414345532655716\n",
      "Epoch 4506, Loss: 0.05354827456176281, Final Batch Loss: 0.036372896283864975\n",
      "Epoch 4507, Loss: 0.036435398273169994, Final Batch Loss: 0.009689592756330967\n",
      "Epoch 4508, Loss: 0.03487045597285032, Final Batch Loss: 0.019342781975865364\n",
      "Epoch 4509, Loss: 0.033005163073539734, Final Batch Loss: 0.02013974077999592\n",
      "Epoch 4510, Loss: 0.040991212241351604, Final Batch Loss: 0.007512292824685574\n",
      "Epoch 4511, Loss: 0.035087754018604755, Final Batch Loss: 0.009571150876581669\n",
      "Epoch 4512, Loss: 0.05304946377873421, Final Batch Loss: 0.04609297960996628\n",
      "Epoch 4513, Loss: 0.03861147537827492, Final Batch Loss: 0.012790355831384659\n",
      "Epoch 4514, Loss: 0.06498425966128707, Final Batch Loss: 0.05876629799604416\n",
      "Epoch 4515, Loss: 0.038420604541897774, Final Batch Loss: 0.029987819492816925\n",
      "Epoch 4516, Loss: 0.06289365794509649, Final Batch Loss: 0.01394645031541586\n",
      "Epoch 4517, Loss: 0.034303643740713596, Final Batch Loss: 0.015545773319900036\n",
      "Epoch 4518, Loss: 0.04637746885418892, Final Batch Loss: 0.02994651347398758\n",
      "Epoch 4519, Loss: 0.03996826335787773, Final Batch Loss: 0.02663225121796131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4520, Loss: 0.04873305093497038, Final Batch Loss: 0.010570148937404156\n",
      "Epoch 4521, Loss: 0.015135520603507757, Final Batch Loss: 0.006144603248685598\n",
      "Epoch 4522, Loss: 0.06149775953963399, Final Batch Loss: 0.05385907366871834\n",
      "Epoch 4523, Loss: 0.013011096976697445, Final Batch Loss: 0.006536617409437895\n",
      "Epoch 4524, Loss: 0.03542946744710207, Final Batch Loss: 0.014100835658609867\n",
      "Epoch 4525, Loss: 0.027009612880647182, Final Batch Loss: 0.007910761050879955\n",
      "Epoch 4526, Loss: 0.04450718406587839, Final Batch Loss: 0.034832797944545746\n",
      "Epoch 4527, Loss: 0.037928370758891106, Final Batch Loss: 0.011277776211500168\n",
      "Epoch 4528, Loss: 0.04331305995583534, Final Batch Loss: 0.01934557780623436\n",
      "Epoch 4529, Loss: 0.06536359339952469, Final Batch Loss: 0.018036305904388428\n",
      "Epoch 4530, Loss: 0.028925570659339428, Final Batch Loss: 0.005348448641598225\n",
      "Epoch 4531, Loss: 0.03314994275569916, Final Batch Loss: 0.010407017543911934\n",
      "Epoch 4532, Loss: 0.03914918005466461, Final Batch Loss: 0.030975444242358208\n",
      "Epoch 4533, Loss: 0.02646616566926241, Final Batch Loss: 0.015447746962308884\n",
      "Epoch 4534, Loss: 0.026342052966356277, Final Batch Loss: 0.01103015523403883\n",
      "Epoch 4535, Loss: 0.02072683861479163, Final Batch Loss: 0.014174887910485268\n",
      "Epoch 4536, Loss: 0.06088162027299404, Final Batch Loss: 0.025378050282597542\n",
      "Epoch 4537, Loss: 0.03832720383070409, Final Batch Loss: 0.0026836597826331854\n",
      "Epoch 4538, Loss: 0.078748419880867, Final Batch Loss: 0.033670056611299515\n",
      "Epoch 4539, Loss: 0.08922886848449707, Final Batch Loss: 0.05552117899060249\n",
      "Epoch 4540, Loss: 0.08169299550354481, Final Batch Loss: 0.0069197360426187515\n",
      "Epoch 4541, Loss: 0.0628842692822218, Final Batch Loss: 0.038245439529418945\n",
      "Epoch 4542, Loss: 0.05973666533827782, Final Batch Loss: 0.036440394818782806\n",
      "Epoch 4543, Loss: 0.06244370527565479, Final Batch Loss: 0.0357443243265152\n",
      "Epoch 4544, Loss: 0.062152303755283356, Final Batch Loss: 0.019908178597688675\n",
      "Epoch 4545, Loss: 0.018067984376102686, Final Batch Loss: 0.011744524352252483\n",
      "Epoch 4546, Loss: 0.032034563831985, Final Batch Loss: 0.010687201283872128\n",
      "Epoch 4547, Loss: 0.0262513249181211, Final Batch Loss: 0.018889347091317177\n",
      "Epoch 4548, Loss: 0.08779517561197281, Final Batch Loss: 0.04160088300704956\n",
      "Epoch 4549, Loss: 0.037270814180374146, Final Batch Loss: 0.01072894036769867\n",
      "Epoch 4550, Loss: 0.03130941092967987, Final Batch Loss: 0.015681372955441475\n",
      "Epoch 4551, Loss: 0.0616250466555357, Final Batch Loss: 0.04390162229537964\n",
      "Epoch 4552, Loss: 0.05989198200404644, Final Batch Loss: 0.03431664779782295\n",
      "Epoch 4553, Loss: 0.04514661617577076, Final Batch Loss: 0.03239918500185013\n",
      "Epoch 4554, Loss: 0.02188529260456562, Final Batch Loss: 0.010796078480780125\n",
      "Epoch 4555, Loss: 0.06509850546717644, Final Batch Loss: 0.033852480351924896\n",
      "Epoch 4556, Loss: 0.019192634150385857, Final Batch Loss: 0.010898959822952747\n",
      "Epoch 4557, Loss: 0.03658206854015589, Final Batch Loss: 0.012862413190305233\n",
      "Epoch 4558, Loss: 0.023086794652044773, Final Batch Loss: 0.011402846314013004\n",
      "Epoch 4559, Loss: 0.05966554209589958, Final Batch Loss: 0.03532901033759117\n",
      "Epoch 4560, Loss: 0.05657561495900154, Final Batch Loss: 0.025503216311335564\n",
      "Epoch 4561, Loss: 0.042711867950856686, Final Batch Loss: 0.032496511936187744\n",
      "Epoch 4562, Loss: 0.07432299479842186, Final Batch Loss: 0.011935338377952576\n",
      "Epoch 4563, Loss: 0.023324540816247463, Final Batch Loss: 0.011178186163306236\n",
      "Epoch 4564, Loss: 0.0477385139092803, Final Batch Loss: 0.035731878131628036\n",
      "Epoch 4565, Loss: 0.026470120064914227, Final Batch Loss: 0.017270779237151146\n",
      "Epoch 4566, Loss: 0.045843335799872875, Final Batch Loss: 0.013461067341268063\n",
      "Epoch 4567, Loss: 0.03041632566601038, Final Batch Loss: 0.0113226855173707\n",
      "Epoch 4568, Loss: 0.02362725418061018, Final Batch Loss: 0.008357224054634571\n",
      "Epoch 4569, Loss: 0.0326465992256999, Final Batch Loss: 0.023242546245455742\n",
      "Epoch 4570, Loss: 0.0443439707159996, Final Batch Loss: 0.010112043470144272\n",
      "Epoch 4571, Loss: 0.027834797278046608, Final Batch Loss: 0.00592833012342453\n",
      "Epoch 4572, Loss: 0.02523222006857395, Final Batch Loss: 0.016644619405269623\n",
      "Epoch 4573, Loss: 0.03912659455090761, Final Batch Loss: 0.013946629129350185\n",
      "Epoch 4574, Loss: 0.07285729888826609, Final Batch Loss: 0.013689347542822361\n",
      "Epoch 4575, Loss: 0.01846983376890421, Final Batch Loss: 0.009614161215722561\n",
      "Epoch 4576, Loss: 0.048531819600611925, Final Batch Loss: 0.007429585326462984\n",
      "Epoch 4577, Loss: 0.02243678830564022, Final Batch Loss: 0.011776736937463284\n",
      "Epoch 4578, Loss: 0.02185620693489909, Final Batch Loss: 0.00701720779761672\n",
      "Epoch 4579, Loss: 0.03479443117976189, Final Batch Loss: 0.024947721511125565\n",
      "Epoch 4580, Loss: 0.029760624282062054, Final Batch Loss: 0.023409374058246613\n",
      "Epoch 4581, Loss: 0.04166393168270588, Final Batch Loss: 0.01986180804669857\n",
      "Epoch 4582, Loss: 0.03813675604760647, Final Batch Loss: 0.00839928351342678\n",
      "Epoch 4583, Loss: 0.029565880075097084, Final Batch Loss: 0.013394264504313469\n",
      "Epoch 4584, Loss: 0.039163160137832165, Final Batch Loss: 0.0133128697052598\n",
      "Epoch 4585, Loss: 0.058900827541947365, Final Batch Loss: 0.03788427636027336\n",
      "Epoch 4586, Loss: 0.04885951988399029, Final Batch Loss: 0.02228509448468685\n",
      "Epoch 4587, Loss: 0.05552576296031475, Final Batch Loss: 0.036089759320020676\n",
      "Epoch 4588, Loss: 0.048531582579016685, Final Batch Loss: 0.03965558111667633\n",
      "Epoch 4589, Loss: 0.06489465292543173, Final Batch Loss: 0.006862684153020382\n",
      "Epoch 4590, Loss: 0.02947538811713457, Final Batch Loss: 0.017125925049185753\n",
      "Epoch 4591, Loss: 0.031305357813835144, Final Batch Loss: 0.010009659454226494\n",
      "Epoch 4592, Loss: 0.03087755013257265, Final Batch Loss: 0.017417985945940018\n",
      "Epoch 4593, Loss: 0.05511186830699444, Final Batch Loss: 0.02096729166805744\n",
      "Epoch 4594, Loss: 0.019579609856009483, Final Batch Loss: 0.007389670237898827\n",
      "Epoch 4595, Loss: 0.02259145863354206, Final Batch Loss: 0.014835354872047901\n",
      "Epoch 4596, Loss: 0.052470605820417404, Final Batch Loss: 0.027572955936193466\n",
      "Epoch 4597, Loss: 0.035750774666666985, Final Batch Loss: 0.008120108395814896\n",
      "Epoch 4598, Loss: 0.024600590812042356, Final Batch Loss: 0.0029779814649373293\n",
      "Epoch 4599, Loss: 0.04117604810744524, Final Batch Loss: 0.03027411364018917\n",
      "Epoch 4600, Loss: 0.022430471843108535, Final Batch Loss: 0.0022793521638959646\n",
      "Epoch 4601, Loss: 0.0394001342356205, Final Batch Loss: 0.02437964640557766\n",
      "Epoch 4602, Loss: 0.060543591156601906, Final Batch Loss: 0.04862039536237717\n",
      "Epoch 4603, Loss: 0.017504052259027958, Final Batch Loss: 0.00484816636890173\n",
      "Epoch 4604, Loss: 0.05662487354129553, Final Batch Loss: 0.014687160961329937\n",
      "Epoch 4605, Loss: 0.044794248417019844, Final Batch Loss: 0.02097374014556408\n",
      "Epoch 4606, Loss: 0.057164931669831276, Final Batch Loss: 0.052768852561712265\n",
      "Epoch 4607, Loss: 0.04463784024119377, Final Batch Loss: 0.019983993843197823\n",
      "Epoch 4608, Loss: 0.03792629484087229, Final Batch Loss: 0.02736840210855007\n",
      "Epoch 4609, Loss: 0.04645025171339512, Final Batch Loss: 0.01611059531569481\n",
      "Epoch 4610, Loss: 0.013767418451607227, Final Batch Loss: 0.004854870028793812\n",
      "Epoch 4611, Loss: 0.03168803360313177, Final Batch Loss: 0.013654292561113834\n",
      "Epoch 4612, Loss: 0.030107678845524788, Final Batch Loss: 0.01644798368215561\n",
      "Epoch 4613, Loss: 0.07475689426064491, Final Batch Loss: 0.05484180524945259\n",
      "Epoch 4614, Loss: 0.03858747333288193, Final Batch Loss: 0.02554699406027794\n",
      "Epoch 4615, Loss: 0.022372331470251083, Final Batch Loss: 0.005195470526814461\n",
      "Epoch 4616, Loss: 0.03217258397489786, Final Batch Loss: 0.011490878649055958\n",
      "Epoch 4617, Loss: 0.021091789938509464, Final Batch Loss: 0.010448304936289787\n",
      "Epoch 4618, Loss: 0.04565334506332874, Final Batch Loss: 0.027648555114865303\n",
      "Epoch 4619, Loss: 0.02537513617426157, Final Batch Loss: 0.016855766996741295\n",
      "Epoch 4620, Loss: 0.02568443026393652, Final Batch Loss: 0.012622717767953873\n",
      "Epoch 4621, Loss: 0.0812577586621046, Final Batch Loss: 0.05542635917663574\n",
      "Epoch 4622, Loss: 0.02336260676383972, Final Batch Loss: 0.017846327275037766\n",
      "Epoch 4623, Loss: 0.04754894087091088, Final Batch Loss: 0.007323610130697489\n",
      "Epoch 4624, Loss: 0.02288222499191761, Final Batch Loss: 0.01089823991060257\n",
      "Epoch 4625, Loss: 0.03395582642406225, Final Batch Loss: 0.010906706564128399\n",
      "Epoch 4626, Loss: 0.04842022340744734, Final Batch Loss: 0.03794635087251663\n",
      "Epoch 4627, Loss: 0.07699423655867577, Final Batch Loss: 0.04528915509581566\n",
      "Epoch 4628, Loss: 0.043733854312449694, Final Batch Loss: 0.006106518674641848\n",
      "Epoch 4629, Loss: 0.014638251857832074, Final Batch Loss: 0.0037337245885282755\n",
      "Epoch 4630, Loss: 0.022694189101457596, Final Batch Loss: 0.00654941238462925\n",
      "Epoch 4631, Loss: 0.055514898151159286, Final Batch Loss: 0.036563120782375336\n",
      "Epoch 4632, Loss: 0.05454840883612633, Final Batch Loss: 0.029900608584284782\n",
      "Epoch 4633, Loss: 0.03747959062457085, Final Batch Loss: 0.015660841017961502\n",
      "Epoch 4634, Loss: 0.0444394052028656, Final Batch Loss: 0.01153576746582985\n",
      "Epoch 4635, Loss: 0.07523406576365232, Final Batch Loss: 0.014461099170148373\n",
      "Epoch 4636, Loss: 0.04297127295285463, Final Batch Loss: 0.033796511590480804\n",
      "Epoch 4637, Loss: 0.0636554379016161, Final Batch Loss: 0.02580844797194004\n",
      "Epoch 4638, Loss: 0.02673220355063677, Final Batch Loss: 0.005938013084232807\n",
      "Epoch 4639, Loss: 0.025905806571245193, Final Batch Loss: 0.012035525403916836\n",
      "Epoch 4640, Loss: 0.0559772583656013, Final Batch Loss: 0.004898927640169859\n",
      "Epoch 4641, Loss: 0.05471101775765419, Final Batch Loss: 0.02501784637570381\n",
      "Epoch 4642, Loss: 0.03773584309965372, Final Batch Loss: 0.005398844368755817\n",
      "Epoch 4643, Loss: 0.057957776822149754, Final Batch Loss: 0.04839929938316345\n",
      "Epoch 4644, Loss: 0.05638810805976391, Final Batch Loss: 0.02093815617263317\n",
      "Epoch 4645, Loss: 0.0407879538834095, Final Batch Loss: 0.018697883933782578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4646, Loss: 0.06111707538366318, Final Batch Loss: 0.040909528732299805\n",
      "Epoch 4647, Loss: 0.04145905561745167, Final Batch Loss: 0.030384233221411705\n",
      "Epoch 4648, Loss: 0.03445218876004219, Final Batch Loss: 0.016758570447564125\n",
      "Epoch 4649, Loss: 0.03979500010609627, Final Batch Loss: 0.019968528300523758\n",
      "Epoch 4650, Loss: 0.11102604866027832, Final Batch Loss: 0.08524411916732788\n",
      "Epoch 4651, Loss: 0.049902401864528656, Final Batch Loss: 0.007384292781352997\n",
      "Epoch 4652, Loss: 0.021206604316830635, Final Batch Loss: 0.00845564715564251\n",
      "Epoch 4653, Loss: 0.05240497924387455, Final Batch Loss: 0.01579306460916996\n",
      "Epoch 4654, Loss: 0.038599914871156216, Final Batch Loss: 0.011691768653690815\n",
      "Epoch 4655, Loss: 0.04804748669266701, Final Batch Loss: 0.02665521577000618\n",
      "Epoch 4656, Loss: 0.1241803765296936, Final Batch Loss: 0.0675557404756546\n",
      "Epoch 4657, Loss: 0.04138907045125961, Final Batch Loss: 0.020137788727879524\n",
      "Epoch 4658, Loss: 0.029844233766198158, Final Batch Loss: 0.015559551306068897\n",
      "Epoch 4659, Loss: 0.07411131449043751, Final Batch Loss: 0.029721463099122047\n",
      "Epoch 4660, Loss: 0.015662563499063253, Final Batch Loss: 0.007231384050101042\n",
      "Epoch 4661, Loss: 0.037503475323319435, Final Batch Loss: 0.019827986136078835\n",
      "Epoch 4662, Loss: 0.07168148271739483, Final Batch Loss: 0.04670032113790512\n",
      "Epoch 4663, Loss: 0.07607019692659378, Final Batch Loss: 0.01719975471496582\n",
      "Epoch 4664, Loss: 0.043996384367346764, Final Batch Loss: 0.020770011469721794\n",
      "Epoch 4665, Loss: 0.019954407587647438, Final Batch Loss: 0.011078215204179287\n",
      "Epoch 4666, Loss: 0.0186643716879189, Final Batch Loss: 0.00478039076551795\n",
      "Epoch 4667, Loss: 0.030882567167282104, Final Batch Loss: 0.018198780715465546\n",
      "Epoch 4668, Loss: 0.0599252674728632, Final Batch Loss: 0.026782190427184105\n",
      "Epoch 4669, Loss: 0.025216668844223022, Final Batch Loss: 0.014972871169447899\n",
      "Epoch 4670, Loss: 0.06390976347029209, Final Batch Loss: 0.013042943552136421\n",
      "Epoch 4671, Loss: 0.05789969675242901, Final Batch Loss: 0.04213513433933258\n",
      "Epoch 4672, Loss: 0.08708998002111912, Final Batch Loss: 0.025418927893042564\n",
      "Epoch 4673, Loss: 0.04877554811537266, Final Batch Loss: 0.01940210722386837\n",
      "Epoch 4674, Loss: 0.024476471357047558, Final Batch Loss: 0.009927644394338131\n",
      "Epoch 4675, Loss: 0.032119072042405605, Final Batch Loss: 0.017619894817471504\n",
      "Epoch 4676, Loss: 0.0496814027428627, Final Batch Loss: 0.010724201798439026\n",
      "Epoch 4677, Loss: 0.06126156076788902, Final Batch Loss: 0.02738865092396736\n",
      "Epoch 4678, Loss: 0.013837330508977175, Final Batch Loss: 0.004403260070830584\n",
      "Epoch 4679, Loss: 0.025444259867072105, Final Batch Loss: 0.009830140508711338\n",
      "Epoch 4680, Loss: 0.0328468382358551, Final Batch Loss: 0.008142828941345215\n",
      "Epoch 4681, Loss: 0.0514997998252511, Final Batch Loss: 0.04326547682285309\n",
      "Epoch 4682, Loss: 0.03797171451151371, Final Batch Loss: 0.025888554751873016\n",
      "Epoch 4683, Loss: 0.05063287355005741, Final Batch Loss: 0.02453938126564026\n",
      "Epoch 4684, Loss: 0.1285397745668888, Final Batch Loss: 0.08680585026741028\n",
      "Epoch 4685, Loss: 0.11118683591485023, Final Batch Loss: 0.04862281307578087\n",
      "Epoch 4686, Loss: 0.06540453340858221, Final Batch Loss: 0.00802427064627409\n",
      "Epoch 4687, Loss: 0.041520338505506516, Final Batch Loss: 0.017572207376360893\n",
      "Epoch 4688, Loss: 0.07197941839694977, Final Batch Loss: 0.040267158299684525\n",
      "Epoch 4689, Loss: 0.029562763404101133, Final Batch Loss: 0.005488777067512274\n",
      "Epoch 4690, Loss: 0.05144390091300011, Final Batch Loss: 0.01495242491364479\n",
      "Epoch 4691, Loss: 0.049615973606705666, Final Batch Loss: 0.03441685810685158\n",
      "Epoch 4692, Loss: 0.07046395726501942, Final Batch Loss: 0.02200344018638134\n",
      "Epoch 4693, Loss: 0.041842710226774216, Final Batch Loss: 0.02907204069197178\n",
      "Epoch 4694, Loss: 0.048334287479519844, Final Batch Loss: 0.014254877343773842\n",
      "Epoch 4695, Loss: 0.027211780659854412, Final Batch Loss: 0.00946952123194933\n",
      "Epoch 4696, Loss: 0.061312997713685036, Final Batch Loss: 0.03351253271102905\n",
      "Epoch 4697, Loss: 0.07417319342494011, Final Batch Loss: 0.02777613326907158\n",
      "Epoch 4698, Loss: 0.05119507759809494, Final Batch Loss: 0.020549271255731583\n",
      "Epoch 4699, Loss: 0.025444519706070423, Final Batch Loss: 0.008176296018064022\n",
      "Epoch 4700, Loss: 0.051837204955518246, Final Batch Loss: 0.015428691171109676\n",
      "Epoch 4701, Loss: 0.05444265529513359, Final Batch Loss: 0.013127997517585754\n",
      "Epoch 4702, Loss: 0.07677176780998707, Final Batch Loss: 0.009496191516518593\n",
      "Epoch 4703, Loss: 0.02167971059679985, Final Batch Loss: 0.012072191573679447\n",
      "Epoch 4704, Loss: 0.11559184268116951, Final Batch Loss: 0.10375102609395981\n",
      "Epoch 4705, Loss: 0.038742853328585625, Final Batch Loss: 0.030119888484477997\n",
      "Epoch 4706, Loss: 0.016300273593515158, Final Batch Loss: 0.008993777446448803\n",
      "Epoch 4707, Loss: 0.09367441385984421, Final Batch Loss: 0.06035823002457619\n",
      "Epoch 4708, Loss: 0.025681662373244762, Final Batch Loss: 0.020522959530353546\n",
      "Epoch 4709, Loss: 0.0335667347535491, Final Batch Loss: 0.022848643362522125\n",
      "Epoch 4710, Loss: 0.056065079756081104, Final Batch Loss: 0.014976178295910358\n",
      "Epoch 4711, Loss: 0.119436364620924, Final Batch Loss: 0.07406564801931381\n",
      "Epoch 4712, Loss: 0.029948212206363678, Final Batch Loss: 0.020718997344374657\n",
      "Epoch 4713, Loss: 0.04251948231831193, Final Batch Loss: 0.00401249760761857\n",
      "Epoch 4714, Loss: 0.022230906412005424, Final Batch Loss: 0.007167695090174675\n",
      "Epoch 4715, Loss: 0.07289801724255085, Final Batch Loss: 0.06181950867176056\n",
      "Epoch 4716, Loss: 0.05184586998075247, Final Batch Loss: 0.039312876760959625\n",
      "Epoch 4717, Loss: 0.03279386553913355, Final Batch Loss: 0.008763634599745274\n",
      "Epoch 4718, Loss: 0.033488355576992035, Final Batch Loss: 0.02362840063869953\n",
      "Epoch 4719, Loss: 0.04674193821847439, Final Batch Loss: 0.03162069246172905\n",
      "Epoch 4720, Loss: 0.03703652648255229, Final Batch Loss: 0.0073667182587087154\n",
      "Epoch 4721, Loss: 0.043880645185709, Final Batch Loss: 0.02578519470989704\n",
      "Epoch 4722, Loss: 0.07474789209663868, Final Batch Loss: 0.0671674832701683\n",
      "Epoch 4723, Loss: 0.013215310173109174, Final Batch Loss: 0.002685403684154153\n",
      "Epoch 4724, Loss: 0.041665978729724884, Final Batch Loss: 0.0308240819722414\n",
      "Epoch 4725, Loss: 0.025995705276727676, Final Batch Loss: 0.008542517200112343\n",
      "Epoch 4726, Loss: 0.01797455921769142, Final Batch Loss: 0.004681063815951347\n",
      "Epoch 4727, Loss: 0.03700755909085274, Final Batch Loss: 0.020505523309111595\n",
      "Epoch 4728, Loss: 0.02558046206831932, Final Batch Loss: 0.012265441007912159\n",
      "Epoch 4729, Loss: 0.036716071888804436, Final Batch Loss: 0.021755119785666466\n",
      "Epoch 4730, Loss: 0.061979190446436405, Final Batch Loss: 0.012048077769577503\n",
      "Epoch 4731, Loss: 0.037663289811462164, Final Batch Loss: 0.004367852117866278\n",
      "Epoch 4732, Loss: 0.06099653150886297, Final Batch Loss: 0.009411036036908627\n",
      "Epoch 4733, Loss: 0.06246975064277649, Final Batch Loss: 0.03729862719774246\n",
      "Epoch 4734, Loss: 0.03331989515572786, Final Batch Loss: 0.019565744325518608\n",
      "Epoch 4735, Loss: 0.03106575831770897, Final Batch Loss: 0.010465467348694801\n",
      "Epoch 4736, Loss: 0.05736878886818886, Final Batch Loss: 0.05046778917312622\n",
      "Epoch 4737, Loss: 0.041337231174111366, Final Batch Loss: 0.0268386323004961\n",
      "Epoch 4738, Loss: 0.0590510219335556, Final Batch Loss: 0.020661160349845886\n",
      "Epoch 4739, Loss: 0.11259695887565613, Final Batch Loss: 0.05178198590874672\n",
      "Epoch 4740, Loss: 0.06568048987537622, Final Batch Loss: 0.010548868216574192\n",
      "Epoch 4741, Loss: 0.022993599995970726, Final Batch Loss: 0.013389558531343937\n",
      "Epoch 4742, Loss: 0.05333248618990183, Final Batch Loss: 0.04535339027643204\n",
      "Epoch 4743, Loss: 0.06175655499100685, Final Batch Loss: 0.020845673978328705\n",
      "Epoch 4744, Loss: 0.04629181697964668, Final Batch Loss: 0.008690565824508667\n",
      "Epoch 4745, Loss: 0.05320145841687918, Final Batch Loss: 0.015026706270873547\n",
      "Epoch 4746, Loss: 0.04948251321911812, Final Batch Loss: 0.021091962233185768\n",
      "Epoch 4747, Loss: 0.10693898424506187, Final Batch Loss: 0.04054764285683632\n",
      "Epoch 4748, Loss: 0.06296775490045547, Final Batch Loss: 0.013276025652885437\n",
      "Epoch 4749, Loss: 0.03598682675510645, Final Batch Loss: 0.015603610314428806\n",
      "Epoch 4750, Loss: 0.07808406464755535, Final Batch Loss: 0.061822082847356796\n",
      "Epoch 4751, Loss: 0.05837537348270416, Final Batch Loss: 0.03738711029291153\n",
      "Epoch 4752, Loss: 0.034412222914397717, Final Batch Loss: 0.011956059373915195\n",
      "Epoch 4753, Loss: 0.013786508236080408, Final Batch Loss: 0.006710189860314131\n",
      "Epoch 4754, Loss: 0.04707412142306566, Final Batch Loss: 0.006917853839695454\n",
      "Epoch 4755, Loss: 0.01375993387773633, Final Batch Loss: 0.009809425100684166\n",
      "Epoch 4756, Loss: 0.02980667632073164, Final Batch Loss: 0.01493647787719965\n",
      "Epoch 4757, Loss: 0.07292085140943527, Final Batch Loss: 0.010848473757505417\n",
      "Epoch 4758, Loss: 0.04093390144407749, Final Batch Loss: 0.01538405753672123\n",
      "Epoch 4759, Loss: 0.033807095140218735, Final Batch Loss: 0.016928821802139282\n",
      "Epoch 4760, Loss: 0.015058637131005526, Final Batch Loss: 0.013589702546596527\n",
      "Epoch 4761, Loss: 0.0645924061536789, Final Batch Loss: 0.04593241587281227\n",
      "Epoch 4762, Loss: 0.03829420730471611, Final Batch Loss: 0.022773871198296547\n",
      "Epoch 4763, Loss: 0.07104974798858166, Final Batch Loss: 0.006936786696314812\n",
      "Epoch 4764, Loss: 0.03850211948156357, Final Batch Loss: 0.017559967935085297\n",
      "Epoch 4765, Loss: 0.022776728495955467, Final Batch Loss: 0.012159764766693115\n",
      "Epoch 4766, Loss: 0.05440093018114567, Final Batch Loss: 0.03218397498130798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4767, Loss: 0.06415492296218872, Final Batch Loss: 0.05451267957687378\n",
      "Epoch 4768, Loss: 0.051523846574127674, Final Batch Loss: 0.03702307492494583\n",
      "Epoch 4769, Loss: 0.04727324657142162, Final Batch Loss: 0.016653111204504967\n",
      "Epoch 4770, Loss: 0.029908861964941025, Final Batch Loss: 0.008560771122574806\n",
      "Epoch 4771, Loss: 0.04463765490800142, Final Batch Loss: 0.03081297129392624\n",
      "Epoch 4772, Loss: 0.050276764668524265, Final Batch Loss: 0.03870803117752075\n",
      "Epoch 4773, Loss: 0.10484659671783447, Final Batch Loss: 0.0792328342795372\n",
      "Epoch 4774, Loss: 0.032939108088612556, Final Batch Loss: 0.012035571038722992\n",
      "Epoch 4775, Loss: 0.052488356828689575, Final Batch Loss: 0.02339373342692852\n",
      "Epoch 4776, Loss: 0.021523209055885673, Final Batch Loss: 0.0024451257195323706\n",
      "Epoch 4777, Loss: 0.06776026636362076, Final Batch Loss: 0.01707817241549492\n",
      "Epoch 4778, Loss: 0.03852699510753155, Final Batch Loss: 0.022057553753256798\n",
      "Epoch 4779, Loss: 0.06228388473391533, Final Batch Loss: 0.0168163925409317\n",
      "Epoch 4780, Loss: 0.029813767410814762, Final Batch Loss: 0.021921949461102486\n",
      "Epoch 4781, Loss: 0.11249906569719315, Final Batch Loss: 0.09047138690948486\n",
      "Epoch 4782, Loss: 0.07509490102529526, Final Batch Loss: 0.01855386793613434\n",
      "Epoch 4783, Loss: 0.027907405979931355, Final Batch Loss: 0.0100243566557765\n",
      "Epoch 4784, Loss: 0.05849727988243103, Final Batch Loss: 0.005675174295902252\n",
      "Epoch 4785, Loss: 0.04675938840955496, Final Batch Loss: 0.011649767868220806\n",
      "Epoch 4786, Loss: 0.03423753101378679, Final Batch Loss: 0.02017524279654026\n",
      "Epoch 4787, Loss: 0.0636768527328968, Final Batch Loss: 0.025634311139583588\n",
      "Epoch 4788, Loss: 0.03026896622031927, Final Batch Loss: 0.005056089721620083\n",
      "Epoch 4789, Loss: 0.03844677656888962, Final Batch Loss: 0.01352774165570736\n",
      "Epoch 4790, Loss: 0.06229649297893047, Final Batch Loss: 0.03781409189105034\n",
      "Epoch 4791, Loss: 0.05399504257366061, Final Batch Loss: 0.04890630394220352\n",
      "Epoch 4792, Loss: 0.06490685045719147, Final Batch Loss: 0.03937499597668648\n",
      "Epoch 4793, Loss: 0.019252542406320572, Final Batch Loss: 0.008127858862280846\n",
      "Epoch 4794, Loss: 0.029395204037427902, Final Batch Loss: 0.014977255836129189\n",
      "Epoch 4795, Loss: 0.06724798679351807, Final Batch Loss: 0.044380538165569305\n",
      "Epoch 4796, Loss: 0.07232408970594406, Final Batch Loss: 0.03512688726186752\n",
      "Epoch 4797, Loss: 0.016355752479285002, Final Batch Loss: 0.005842068698257208\n",
      "Epoch 4798, Loss: 0.012994103599339724, Final Batch Loss: 0.00586532149463892\n",
      "Epoch 4799, Loss: 0.02716772072017193, Final Batch Loss: 0.0195869579911232\n",
      "Epoch 4800, Loss: 0.07504948228597641, Final Batch Loss: 0.035890646278858185\n",
      "Epoch 4801, Loss: 0.02238784311339259, Final Batch Loss: 0.007388552185148001\n",
      "Epoch 4802, Loss: 0.030456729233264923, Final Batch Loss: 0.010076498612761497\n",
      "Epoch 4803, Loss: 0.05122728459537029, Final Batch Loss: 0.013772910460829735\n",
      "Epoch 4804, Loss: 0.016378102358430624, Final Batch Loss: 0.009953510016202927\n",
      "Epoch 4805, Loss: 0.04366570524871349, Final Batch Loss: 0.02566111646592617\n",
      "Epoch 4806, Loss: 0.034319995902478695, Final Batch Loss: 0.024794362485408783\n",
      "Epoch 4807, Loss: 0.055867902003228664, Final Batch Loss: 0.04593769833445549\n",
      "Epoch 4808, Loss: 0.06048479117453098, Final Batch Loss: 0.021753249689936638\n",
      "Epoch 4809, Loss: 0.017122055403888226, Final Batch Loss: 0.008664587512612343\n",
      "Epoch 4810, Loss: 0.028111175633966923, Final Batch Loss: 0.01698760688304901\n",
      "Epoch 4811, Loss: 0.050991376861929893, Final Batch Loss: 0.034691523760557175\n",
      "Epoch 4812, Loss: 0.09056984633207321, Final Batch Loss: 0.049056556075811386\n",
      "Epoch 4813, Loss: 0.03191274916753173, Final Batch Loss: 0.004076838027685881\n",
      "Epoch 4814, Loss: 0.061549799516797066, Final Batch Loss: 0.03202420845627785\n",
      "Epoch 4815, Loss: 0.05179846193641424, Final Batch Loss: 0.009672551415860653\n",
      "Epoch 4816, Loss: 0.09188167750835419, Final Batch Loss: 0.05964597314596176\n",
      "Epoch 4817, Loss: 0.03619297407567501, Final Batch Loss: 0.012589974328875542\n",
      "Epoch 4818, Loss: 0.11760084889829159, Final Batch Loss: 0.08772209286689758\n",
      "Epoch 4819, Loss: 0.08111175149679184, Final Batch Loss: 0.03965078294277191\n",
      "Epoch 4820, Loss: 0.15653377026319504, Final Batch Loss: 0.10415676981210709\n",
      "Epoch 4821, Loss: 0.11309879273176193, Final Batch Loss: 0.028880976140499115\n",
      "Epoch 4822, Loss: 0.08883903920650482, Final Batch Loss: 0.052866142243146896\n",
      "Epoch 4823, Loss: 0.0633673220872879, Final Batch Loss: 0.054483164101839066\n",
      "Epoch 4824, Loss: 0.11302817985415459, Final Batch Loss: 0.059825316071510315\n",
      "Epoch 4825, Loss: 0.06579453311860561, Final Batch Loss: 0.024541961029171944\n",
      "Epoch 4826, Loss: 0.05322437919676304, Final Batch Loss: 0.02664857730269432\n",
      "Epoch 4827, Loss: 0.034480100497603416, Final Batch Loss: 0.018164927139878273\n",
      "Epoch 4828, Loss: 0.025531063321977854, Final Batch Loss: 0.004578756634145975\n",
      "Epoch 4829, Loss: 0.13843036442995071, Final Batch Loss: 0.07084353268146515\n",
      "Epoch 4830, Loss: 0.06339875608682632, Final Batch Loss: 0.01876876875758171\n",
      "Epoch 4831, Loss: 0.03131870646029711, Final Batch Loss: 0.010951501317322254\n",
      "Epoch 4832, Loss: 0.06897668167948723, Final Batch Loss: 0.02806086093187332\n",
      "Epoch 4833, Loss: 0.029987409710884094, Final Batch Loss: 0.013096673414111137\n",
      "Epoch 4834, Loss: 0.027798322029411793, Final Batch Loss: 0.014433330856263638\n",
      "Epoch 4835, Loss: 0.019911133218556643, Final Batch Loss: 0.006292578298598528\n",
      "Epoch 4836, Loss: 0.0170969320461154, Final Batch Loss: 0.011262474581599236\n",
      "Epoch 4837, Loss: 0.02959098294377327, Final Batch Loss: 0.018171770498156548\n",
      "Epoch 4838, Loss: 0.05322055146098137, Final Batch Loss: 0.03738043084740639\n",
      "Epoch 4839, Loss: 0.03994307480752468, Final Batch Loss: 0.01840280182659626\n",
      "Epoch 4840, Loss: 0.04652315750718117, Final Batch Loss: 0.026304325088858604\n",
      "Epoch 4841, Loss: 0.05931694619357586, Final Batch Loss: 0.016377748921513557\n",
      "Epoch 4842, Loss: 0.0454107029363513, Final Batch Loss: 0.013036507181823254\n",
      "Epoch 4843, Loss: 0.015562382992357016, Final Batch Loss: 0.006622608285397291\n",
      "Epoch 4844, Loss: 0.0698520839214325, Final Batch Loss: 0.03470977023243904\n",
      "Epoch 4845, Loss: 0.04329833388328552, Final Batch Loss: 0.01877603679895401\n",
      "Epoch 4846, Loss: 0.04663441143929958, Final Batch Loss: 0.026856347918510437\n",
      "Epoch 4847, Loss: 0.09553970023989677, Final Batch Loss: 0.07631052285432816\n",
      "Epoch 4848, Loss: 0.028174149803817272, Final Batch Loss: 0.01937319152057171\n",
      "Epoch 4849, Loss: 0.06709212251007557, Final Batch Loss: 0.030885083600878716\n",
      "Epoch 4850, Loss: 0.0365483658388257, Final Batch Loss: 0.008292379789054394\n",
      "Epoch 4851, Loss: 0.0773489959537983, Final Batch Loss: 0.031763385981321335\n",
      "Epoch 4852, Loss: 0.0633103009313345, Final Batch Loss: 0.007087761536240578\n",
      "Epoch 4853, Loss: 0.08895079791545868, Final Batch Loss: 0.0524652898311615\n",
      "Epoch 4854, Loss: 0.03861475829035044, Final Batch Loss: 0.01118843536823988\n",
      "Epoch 4855, Loss: 0.06785235367715359, Final Batch Loss: 0.01033509336411953\n",
      "Epoch 4856, Loss: 0.10423491895198822, Final Batch Loss: 0.05959399417042732\n",
      "Epoch 4857, Loss: 0.0272884713485837, Final Batch Loss: 0.01233434583991766\n",
      "Epoch 4858, Loss: 0.01315570343285799, Final Batch Loss: 0.006533702369779348\n",
      "Epoch 4859, Loss: 0.03586329845711589, Final Batch Loss: 0.00626729940995574\n",
      "Epoch 4860, Loss: 0.07501771673560143, Final Batch Loss: 0.024924732744693756\n",
      "Epoch 4861, Loss: 0.06667238846421242, Final Batch Loss: 0.029505331069231033\n",
      "Epoch 4862, Loss: 0.04258286580443382, Final Batch Loss: 0.017987024039030075\n",
      "Epoch 4863, Loss: 0.07560976408421993, Final Batch Loss: 0.060323990881443024\n",
      "Epoch 4864, Loss: 0.06277194246649742, Final Batch Loss: 0.04782862588763237\n",
      "Epoch 4865, Loss: 0.06031733192503452, Final Batch Loss: 0.03097476437687874\n",
      "Epoch 4866, Loss: 0.0634871874935925, Final Batch Loss: 0.00616878317669034\n",
      "Epoch 4867, Loss: 0.036885266192257404, Final Batch Loss: 0.026587622240185738\n",
      "Epoch 4868, Loss: 0.1093019787222147, Final Batch Loss: 0.09421157836914062\n",
      "Epoch 4869, Loss: 0.03680463694036007, Final Batch Loss: 0.01638779044151306\n",
      "Epoch 4870, Loss: 0.06786087341606617, Final Batch Loss: 0.025095155462622643\n",
      "Epoch 4871, Loss: 0.09111862629652023, Final Batch Loss: 0.04014170542359352\n",
      "Epoch 4872, Loss: 0.05378645099699497, Final Batch Loss: 0.029762886464595795\n",
      "Epoch 4873, Loss: 0.062095757573843, Final Batch Loss: 0.018596161156892776\n",
      "Epoch 4874, Loss: 0.034446186386048794, Final Batch Loss: 0.0076699526980519295\n",
      "Epoch 4875, Loss: 0.020915153436362743, Final Batch Loss: 0.006158999167382717\n",
      "Epoch 4876, Loss: 0.06755468901246786, Final Batch Loss: 0.015559951774775982\n",
      "Epoch 4877, Loss: 0.10066432505846024, Final Batch Loss: 0.05625510215759277\n",
      "Epoch 4878, Loss: 0.05330933630466461, Final Batch Loss: 0.03573445975780487\n",
      "Epoch 4879, Loss: 0.03086819313466549, Final Batch Loss: 0.01839422807097435\n",
      "Epoch 4880, Loss: 0.038457589223980904, Final Batch Loss: 0.014522574841976166\n",
      "Epoch 4881, Loss: 0.04368886724114418, Final Batch Loss: 0.018084289506077766\n",
      "Epoch 4882, Loss: 0.05401296354830265, Final Batch Loss: 0.011956838890910149\n",
      "Epoch 4883, Loss: 0.03630916588008404, Final Batch Loss: 0.013027986511588097\n",
      "Epoch 4884, Loss: 0.014912466052919626, Final Batch Loss: 0.012009375728666782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4885, Loss: 0.04005832690745592, Final Batch Loss: 0.006642703898251057\n",
      "Epoch 4886, Loss: 0.07646936923265457, Final Batch Loss: 0.01897936314344406\n",
      "Epoch 4887, Loss: 0.059758927673101425, Final Batch Loss: 0.018648583441972733\n",
      "Epoch 4888, Loss: 0.04164288192987442, Final Batch Loss: 0.022047223523259163\n",
      "Epoch 4889, Loss: 0.03822125028818846, Final Batch Loss: 0.02373487502336502\n",
      "Epoch 4890, Loss: 0.03717825748026371, Final Batch Loss: 0.011373385787010193\n",
      "Epoch 4891, Loss: 0.03251049295067787, Final Batch Loss: 0.010227803140878677\n",
      "Epoch 4892, Loss: 0.08500232174992561, Final Batch Loss: 0.0561419352889061\n",
      "Epoch 4893, Loss: 0.06826655566692352, Final Batch Loss: 0.03160594403743744\n",
      "Epoch 4894, Loss: 0.05029465025290847, Final Batch Loss: 0.004149915184825659\n",
      "Epoch 4895, Loss: 0.04787779785692692, Final Batch Loss: 0.028320007026195526\n",
      "Epoch 4896, Loss: 0.05079388990998268, Final Batch Loss: 0.017034165561199188\n",
      "Epoch 4897, Loss: 0.028461449313908815, Final Batch Loss: 0.005922472570091486\n",
      "Epoch 4898, Loss: 0.06118286959826946, Final Batch Loss: 0.05057353898882866\n",
      "Epoch 4899, Loss: 0.039620895870029926, Final Batch Loss: 0.024787625297904015\n",
      "Epoch 4900, Loss: 0.027247708290815353, Final Batch Loss: 0.011129206046462059\n",
      "Epoch 4901, Loss: 0.07783304480835795, Final Batch Loss: 0.07046536356210709\n",
      "Epoch 4902, Loss: 0.07231802493333817, Final Batch Loss: 0.05039137601852417\n",
      "Epoch 4903, Loss: 0.03338072448968887, Final Batch Loss: 0.011149296537041664\n",
      "Epoch 4904, Loss: 0.021972530521452427, Final Batch Loss: 0.00686993170529604\n",
      "Epoch 4905, Loss: 0.045964556746184826, Final Batch Loss: 0.012585739605128765\n",
      "Epoch 4906, Loss: 0.03616408910602331, Final Batch Loss: 0.014040668494999409\n",
      "Epoch 4907, Loss: 0.04227980971336365, Final Batch Loss: 0.028665702790021896\n",
      "Epoch 4908, Loss: 0.07481366442516446, Final Batch Loss: 0.07001496106386185\n",
      "Epoch 4909, Loss: 0.06951674446463585, Final Batch Loss: 0.015621151775121689\n",
      "Epoch 4910, Loss: 0.062147920951247215, Final Batch Loss: 0.0408756323158741\n",
      "Epoch 4911, Loss: 0.049838053062558174, Final Batch Loss: 0.018115820363163948\n",
      "Epoch 4912, Loss: 0.054770950227975845, Final Batch Loss: 0.03500453382730484\n",
      "Epoch 4913, Loss: 0.05898270569741726, Final Batch Loss: 0.039870236068964005\n",
      "Epoch 4914, Loss: 0.04372582398355007, Final Batch Loss: 0.02214123122394085\n",
      "Epoch 4915, Loss: 0.03426090069115162, Final Batch Loss: 0.010338947176933289\n",
      "Epoch 4916, Loss: 0.1477022599428892, Final Batch Loss: 0.01795363239943981\n",
      "Epoch 4917, Loss: 0.07496693357825279, Final Batch Loss: 0.0346270389854908\n",
      "Epoch 4918, Loss: 0.1091497540473938, Final Batch Loss: 0.02883324772119522\n",
      "Epoch 4919, Loss: 0.049652102403342724, Final Batch Loss: 0.014452959410846233\n",
      "Epoch 4920, Loss: 0.08238985389471054, Final Batch Loss: 0.02983410656452179\n",
      "Epoch 4921, Loss: 0.042447375133633614, Final Batch Loss: 0.013307949528098106\n",
      "Epoch 4922, Loss: 0.043258827179670334, Final Batch Loss: 0.03323892131447792\n",
      "Epoch 4923, Loss: 0.03631962835788727, Final Batch Loss: 0.0162238497287035\n",
      "Epoch 4924, Loss: 0.046676645055413246, Final Batch Loss: 0.019272955134510994\n",
      "Epoch 4925, Loss: 0.016579624731093645, Final Batch Loss: 0.011179640889167786\n",
      "Epoch 4926, Loss: 0.0436844676733017, Final Batch Loss: 0.018320929259061813\n",
      "Epoch 4927, Loss: 0.017350294161587954, Final Batch Loss: 0.002210028935223818\n",
      "Epoch 4928, Loss: 0.02070570271462202, Final Batch Loss: 0.008655006065964699\n",
      "Epoch 4929, Loss: 0.05027589946985245, Final Batch Loss: 0.036456961184740067\n",
      "Epoch 4930, Loss: 0.04324473161250353, Final Batch Loss: 0.00912997592240572\n",
      "Epoch 4931, Loss: 0.06779499724507332, Final Batch Loss: 0.04352376610040665\n",
      "Epoch 4932, Loss: 0.0450093112885952, Final Batch Loss: 0.014652522280812263\n",
      "Epoch 4933, Loss: 0.023721875622868538, Final Batch Loss: 0.006550060585141182\n",
      "Epoch 4934, Loss: 0.017243345268070698, Final Batch Loss: 0.009304077364504337\n",
      "Epoch 4935, Loss: 0.03065352002158761, Final Batch Loss: 0.005520474631339312\n",
      "Epoch 4936, Loss: 0.051565226167440414, Final Batch Loss: 0.025096802040934563\n",
      "Epoch 4937, Loss: 0.050607338547706604, Final Batch Loss: 0.0337810255587101\n",
      "Epoch 4938, Loss: 0.04698018915951252, Final Batch Loss: 0.026711206883192062\n",
      "Epoch 4939, Loss: 0.08770826458930969, Final Batch Loss: 0.029279213398694992\n",
      "Epoch 4940, Loss: 0.049004772678017616, Final Batch Loss: 0.0107110645622015\n",
      "Epoch 4941, Loss: 0.019336462020874023, Final Batch Loss: 0.004887307994067669\n",
      "Epoch 4942, Loss: 0.069741141051054, Final Batch Loss: 0.023534435778856277\n",
      "Epoch 4943, Loss: 0.0441329013556242, Final Batch Loss: 0.021602893248200417\n",
      "Epoch 4944, Loss: 0.02965314732864499, Final Batch Loss: 0.0035992790944874287\n",
      "Epoch 4945, Loss: 0.05730842426419258, Final Batch Loss: 0.02411692589521408\n",
      "Epoch 4946, Loss: 0.038893863558769226, Final Batch Loss: 0.03021681308746338\n",
      "Epoch 4947, Loss: 0.023792050313204527, Final Batch Loss: 0.0067924088798463345\n",
      "Epoch 4948, Loss: 0.06943603069521487, Final Batch Loss: 0.0029540082905441523\n",
      "Epoch 4949, Loss: 0.023390567861497402, Final Batch Loss: 0.008143317885696888\n",
      "Epoch 4950, Loss: 0.023219040594995022, Final Batch Loss: 0.014275655150413513\n",
      "Epoch 4951, Loss: 0.029501641169190407, Final Batch Loss: 0.016924183815717697\n",
      "Epoch 4952, Loss: 0.040395038202404976, Final Batch Loss: 0.02120971865952015\n",
      "Epoch 4953, Loss: 0.04505706578493118, Final Batch Loss: 0.03383326157927513\n",
      "Epoch 4954, Loss: 0.08184889703989029, Final Batch Loss: 0.034654174000024796\n",
      "Epoch 4955, Loss: 0.08199531119316816, Final Batch Loss: 0.011959712021052837\n",
      "Epoch 4956, Loss: 0.037939803674817085, Final Batch Loss: 0.019539186730980873\n",
      "Epoch 4957, Loss: 0.020697022788226604, Final Batch Loss: 0.011832178570330143\n",
      "Epoch 4958, Loss: 0.030303104780614376, Final Batch Loss: 0.008443550206720829\n",
      "Epoch 4959, Loss: 0.030910552479326725, Final Batch Loss: 0.022734425961971283\n",
      "Epoch 4960, Loss: 0.039410509169101715, Final Batch Loss: 0.022328205406665802\n",
      "Epoch 4961, Loss: 0.03401612490415573, Final Batch Loss: 0.01283094473183155\n",
      "Epoch 4962, Loss: 0.028211087454110384, Final Batch Loss: 0.006055376026779413\n",
      "Epoch 4963, Loss: 0.04050303064286709, Final Batch Loss: 0.017681170254945755\n",
      "Epoch 4964, Loss: 0.035405115224421024, Final Batch Loss: 0.02693493478000164\n",
      "Epoch 4965, Loss: 0.0255131796002388, Final Batch Loss: 0.013184367679059505\n",
      "Epoch 4966, Loss: 0.025103186257183552, Final Batch Loss: 0.01634119637310505\n",
      "Epoch 4967, Loss: 0.07611771859228611, Final Batch Loss: 0.056642770767211914\n",
      "Epoch 4968, Loss: 0.029280569404363632, Final Batch Loss: 0.024626474827528\n",
      "Epoch 4969, Loss: 0.03648861078545451, Final Batch Loss: 0.029849689453840256\n",
      "Epoch 4970, Loss: 0.038053352385759354, Final Batch Loss: 0.016546355560421944\n",
      "Epoch 4971, Loss: 0.030222634319216013, Final Batch Loss: 0.025609781965613365\n",
      "Epoch 4972, Loss: 0.08368604630231857, Final Batch Loss: 0.03144184127449989\n",
      "Epoch 4973, Loss: 0.10587980598211288, Final Batch Loss: 0.05433758348226547\n",
      "Epoch 4974, Loss: 0.03765121381729841, Final Batch Loss: 0.028268400579690933\n",
      "Epoch 4975, Loss: 0.03737229947000742, Final Batch Loss: 0.03220311552286148\n",
      "Epoch 4976, Loss: 0.03118794597685337, Final Batch Loss: 0.017852915450930595\n",
      "Epoch 4977, Loss: 0.016107480507344007, Final Batch Loss: 0.005036287475377321\n",
      "Epoch 4978, Loss: 0.03708531986922026, Final Batch Loss: 0.003616384230554104\n",
      "Epoch 4979, Loss: 0.04726734198629856, Final Batch Loss: 0.025989435613155365\n",
      "Epoch 4980, Loss: 0.033348861150443554, Final Batch Loss: 0.010798276402056217\n",
      "Epoch 4981, Loss: 0.049797745421528816, Final Batch Loss: 0.02018631249666214\n",
      "Epoch 4982, Loss: 0.07803202793002129, Final Batch Loss: 0.03170786798000336\n",
      "Epoch 4983, Loss: 0.0757705345749855, Final Batch Loss: 0.035136088728904724\n",
      "Epoch 4984, Loss: 0.025255640037357807, Final Batch Loss: 0.016573725268244743\n",
      "Epoch 4985, Loss: 0.044559746980667114, Final Batch Loss: 0.01271674782037735\n",
      "Epoch 4986, Loss: 0.0365743562579155, Final Batch Loss: 0.02073029614984989\n",
      "Epoch 4987, Loss: 0.03806218132376671, Final Batch Loss: 0.018538085743784904\n",
      "Epoch 4988, Loss: 0.02881060354411602, Final Batch Loss: 0.0044517070055007935\n",
      "Epoch 4989, Loss: 0.02344318851828575, Final Batch Loss: 0.011087793856859207\n",
      "Epoch 4990, Loss: 0.015585535205900669, Final Batch Loss: 0.00716942735016346\n",
      "Epoch 4991, Loss: 0.02646703226491809, Final Batch Loss: 0.019217373803257942\n",
      "Epoch 4992, Loss: 0.02417421038262546, Final Batch Loss: 0.02052464336156845\n",
      "Epoch 4993, Loss: 0.054434558376669884, Final Batch Loss: 0.050156306475400925\n",
      "Epoch 4994, Loss: 0.057138810865581036, Final Batch Loss: 0.05286148935556412\n",
      "Epoch 4995, Loss: 0.03355699125677347, Final Batch Loss: 0.024356789886951447\n",
      "Epoch 4996, Loss: 0.07021061144769192, Final Batch Loss: 0.04112086072564125\n",
      "Epoch 4997, Loss: 0.01989668235182762, Final Batch Loss: 0.0065205711871385574\n",
      "Epoch 4998, Loss: 0.021970571018755436, Final Batch Loss: 0.002128046937286854\n",
      "Epoch 4999, Loss: 0.01853750552982092, Final Batch Loss: 0.00894432608038187\n",
      "Epoch 5000, Loss: 0.010056587867438793, Final Batch Loss: 0.004187007900327444\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29  0  0]\n",
      " [ 0 25  0]\n",
      " [ 0  0 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        29\n",
      "           1      1.000     1.000     1.000        25\n",
      "           2      1.000     1.000     1.000        21\n",
      "\n",
      "    accuracy                          1.000        75\n",
      "   macro avg      1.000     1.000     1.000        75\n",
      "weighted avg      1.000     1.000     1.000        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../saved_models/UCI 3 User Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
