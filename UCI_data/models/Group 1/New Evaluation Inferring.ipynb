{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_1 = gen(to_gen).detach().numpy()\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_2 = gen(to_gen).detach().numpy()\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_3 = gen(to_gen).detach().numpy()\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_4 = gen(to_gen).detach().numpy()\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_5 = gen(to_gen).detach().numpy()\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_6 = gen(to_gen).detach().numpy()\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_7 = gen(to_gen).detach().numpy()\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_8 = gen(to_gen).detach().numpy()\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_9 = gen(to_gen).detach().numpy()\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_train = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_train = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "X_test, y_test = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "_, X_test, _, y_test = train_test_split(X_test, y_test, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.413949251174927, Final Batch Loss: 2.2008166313171387\n",
      "Epoch 2, Loss: 4.431711673736572, Final Batch Loss: 2.2269327640533447\n",
      "Epoch 3, Loss: 4.416304588317871, Final Batch Loss: 2.2068591117858887\n",
      "Epoch 4, Loss: 4.40888524055481, Final Batch Loss: 2.199678659439087\n",
      "Epoch 5, Loss: 4.407526969909668, Final Batch Loss: 2.198256492614746\n",
      "Epoch 6, Loss: 4.407911539077759, Final Batch Loss: 2.199852228164673\n",
      "Epoch 7, Loss: 4.419575214385986, Final Batch Loss: 2.2201623916625977\n",
      "Epoch 8, Loss: 4.387150287628174, Final Batch Loss: 2.1789753437042236\n",
      "Epoch 9, Loss: 4.408385276794434, Final Batch Loss: 2.211353302001953\n",
      "Epoch 10, Loss: 4.398369789123535, Final Batch Loss: 2.2031071186065674\n",
      "Epoch 11, Loss: 4.397565841674805, Final Batch Loss: 2.200876235961914\n",
      "Epoch 12, Loss: 4.394002676010132, Final Batch Loss: 2.2080044746398926\n",
      "Epoch 13, Loss: 4.383869171142578, Final Batch Loss: 2.2001445293426514\n",
      "Epoch 14, Loss: 4.354028701782227, Final Batch Loss: 2.1690430641174316\n",
      "Epoch 15, Loss: 4.368342399597168, Final Batch Loss: 2.188810110092163\n",
      "Epoch 16, Loss: 4.34142279624939, Final Batch Loss: 2.1714420318603516\n",
      "Epoch 17, Loss: 4.319534540176392, Final Batch Loss: 2.1495776176452637\n",
      "Epoch 18, Loss: 4.321863889694214, Final Batch Loss: 2.1642708778381348\n",
      "Epoch 19, Loss: 4.292271375656128, Final Batch Loss: 2.133216381072998\n",
      "Epoch 20, Loss: 4.25963568687439, Final Batch Loss: 2.1194543838500977\n",
      "Epoch 21, Loss: 4.235452890396118, Final Batch Loss: 2.1117618083953857\n",
      "Epoch 22, Loss: 4.213597536087036, Final Batch Loss: 2.099996566772461\n",
      "Epoch 23, Loss: 4.196000099182129, Final Batch Loss: 2.095613718032837\n",
      "Epoch 24, Loss: 4.156662940979004, Final Batch Loss: 2.0775744915008545\n",
      "Epoch 25, Loss: 4.116366386413574, Final Batch Loss: 2.0523297786712646\n",
      "Epoch 26, Loss: 4.071539402008057, Final Batch Loss: 2.018061399459839\n",
      "Epoch 27, Loss: 4.008667826652527, Final Batch Loss: 1.9892534017562866\n",
      "Epoch 28, Loss: 4.023836851119995, Final Batch Loss: 2.005610466003418\n",
      "Epoch 29, Loss: 3.917122006416321, Final Batch Loss: 1.9140621423721313\n",
      "Epoch 30, Loss: 3.9245173931121826, Final Batch Loss: 1.9581607580184937\n",
      "Epoch 31, Loss: 3.848630428314209, Final Batch Loss: 1.870110273361206\n",
      "Epoch 32, Loss: 3.8594751358032227, Final Batch Loss: 1.9217735528945923\n",
      "Epoch 33, Loss: 3.809028387069702, Final Batch Loss: 1.8893420696258545\n",
      "Epoch 34, Loss: 3.8120347261428833, Final Batch Loss: 1.9028048515319824\n",
      "Epoch 35, Loss: 3.814408779144287, Final Batch Loss: 1.9333252906799316\n",
      "Epoch 36, Loss: 3.757967710494995, Final Batch Loss: 1.8756120204925537\n",
      "Epoch 37, Loss: 3.8182886838912964, Final Batch Loss: 1.9636895656585693\n",
      "Epoch 38, Loss: 3.7253220081329346, Final Batch Loss: 1.8645092248916626\n",
      "Epoch 39, Loss: 3.692749857902527, Final Batch Loss: 1.8348532915115356\n",
      "Epoch 40, Loss: 3.694085955619812, Final Batch Loss: 1.8565856218338013\n",
      "Epoch 41, Loss: 3.7586913108825684, Final Batch Loss: 1.9237817525863647\n",
      "Epoch 42, Loss: 3.6627743244171143, Final Batch Loss: 1.8358900547027588\n",
      "Epoch 43, Loss: 3.6971911191940308, Final Batch Loss: 1.882214069366455\n",
      "Epoch 44, Loss: 3.6138644218444824, Final Batch Loss: 1.7973575592041016\n",
      "Epoch 45, Loss: 3.5404430627822876, Final Batch Loss: 1.7021398544311523\n",
      "Epoch 46, Loss: 3.6170586347579956, Final Batch Loss: 1.7773531675338745\n",
      "Epoch 47, Loss: 3.6024609804153442, Final Batch Loss: 1.8170583248138428\n",
      "Epoch 48, Loss: 3.6195034980773926, Final Batch Loss: 1.8709056377410889\n",
      "Epoch 49, Loss: 3.5014432668685913, Final Batch Loss: 1.704704999923706\n",
      "Epoch 50, Loss: 3.5850629806518555, Final Batch Loss: 1.7954732179641724\n",
      "Epoch 51, Loss: 3.541896343231201, Final Batch Loss: 1.753443717956543\n",
      "Epoch 52, Loss: 3.5272213220596313, Final Batch Loss: 1.7625850439071655\n",
      "Epoch 53, Loss: 3.5420544147491455, Final Batch Loss: 1.8151850700378418\n",
      "Epoch 54, Loss: 3.4294469356536865, Final Batch Loss: 1.656899094581604\n",
      "Epoch 55, Loss: 3.4081796407699585, Final Batch Loss: 1.609449028968811\n",
      "Epoch 56, Loss: 3.5362765789031982, Final Batch Loss: 1.8305386304855347\n",
      "Epoch 57, Loss: 3.5120177268981934, Final Batch Loss: 1.77113676071167\n",
      "Epoch 58, Loss: 3.3771246671676636, Final Batch Loss: 1.6496044397354126\n",
      "Epoch 59, Loss: 3.5054032802581787, Final Batch Loss: 1.8064100742340088\n",
      "Epoch 60, Loss: 3.4619193077087402, Final Batch Loss: 1.754055380821228\n",
      "Epoch 61, Loss: 3.403657913208008, Final Batch Loss: 1.7009910345077515\n",
      "Epoch 62, Loss: 3.3922799825668335, Final Batch Loss: 1.65068519115448\n",
      "Epoch 63, Loss: 3.3845601081848145, Final Batch Loss: 1.6972543001174927\n",
      "Epoch 64, Loss: 3.378775119781494, Final Batch Loss: 1.7134860754013062\n",
      "Epoch 65, Loss: 3.4036556482315063, Final Batch Loss: 1.7282958030700684\n",
      "Epoch 66, Loss: 3.361986756324768, Final Batch Loss: 1.7434571981430054\n",
      "Epoch 67, Loss: 3.3126838207244873, Final Batch Loss: 1.668195128440857\n",
      "Epoch 68, Loss: 3.2412869930267334, Final Batch Loss: 1.5751702785491943\n",
      "Epoch 69, Loss: 3.2775917053222656, Final Batch Loss: 1.6187633275985718\n",
      "Epoch 70, Loss: 3.12551212310791, Final Batch Loss: 1.4402275085449219\n",
      "Epoch 71, Loss: 3.3530519008636475, Final Batch Loss: 1.7468311786651611\n",
      "Epoch 72, Loss: 3.2543346881866455, Final Batch Loss: 1.6570466756820679\n",
      "Epoch 73, Loss: 3.2094359397888184, Final Batch Loss: 1.666111946105957\n",
      "Epoch 74, Loss: 3.2283079624176025, Final Batch Loss: 1.6373989582061768\n",
      "Epoch 75, Loss: 3.051352024078369, Final Batch Loss: 1.4839046001434326\n",
      "Epoch 76, Loss: 3.2346636056900024, Final Batch Loss: 1.632094383239746\n",
      "Epoch 77, Loss: 3.13417649269104, Final Batch Loss: 1.5994764566421509\n",
      "Epoch 78, Loss: 3.113170027732849, Final Batch Loss: 1.5992622375488281\n",
      "Epoch 79, Loss: 3.094854235649109, Final Batch Loss: 1.5335578918457031\n",
      "Epoch 80, Loss: 3.09380304813385, Final Batch Loss: 1.5344513654708862\n",
      "Epoch 81, Loss: 3.002239465713501, Final Batch Loss: 1.5037357807159424\n",
      "Epoch 82, Loss: 2.981972336769104, Final Batch Loss: 1.4910881519317627\n",
      "Epoch 83, Loss: 3.080649256706238, Final Batch Loss: 1.5788867473602295\n",
      "Epoch 84, Loss: 2.9595649242401123, Final Batch Loss: 1.4446566104888916\n",
      "Epoch 85, Loss: 2.9464184045791626, Final Batch Loss: 1.4549555778503418\n",
      "Epoch 86, Loss: 3.0986043214797974, Final Batch Loss: 1.5984679460525513\n",
      "Epoch 87, Loss: 3.0028955936431885, Final Batch Loss: 1.5586529970169067\n",
      "Epoch 88, Loss: 2.9941056966781616, Final Batch Loss: 1.4788399934768677\n",
      "Epoch 89, Loss: 2.9388352632522583, Final Batch Loss: 1.5004976987838745\n",
      "Epoch 90, Loss: 2.8812003135681152, Final Batch Loss: 1.4365054368972778\n",
      "Epoch 91, Loss: 2.9023475646972656, Final Batch Loss: 1.4884799718856812\n",
      "Epoch 92, Loss: 2.876008629798889, Final Batch Loss: 1.489460825920105\n",
      "Epoch 93, Loss: 2.820895552635193, Final Batch Loss: 1.3796583414077759\n",
      "Epoch 94, Loss: 2.707651734352112, Final Batch Loss: 1.3320854902267456\n",
      "Epoch 95, Loss: 2.777178168296814, Final Batch Loss: 1.4188586473464966\n",
      "Epoch 96, Loss: 2.7399805784225464, Final Batch Loss: 1.3357517719268799\n",
      "Epoch 97, Loss: 2.8416292667388916, Final Batch Loss: 1.463953971862793\n",
      "Epoch 98, Loss: 2.8133283853530884, Final Batch Loss: 1.4369449615478516\n",
      "Epoch 99, Loss: 2.7432961463928223, Final Batch Loss: 1.3951090574264526\n",
      "Epoch 100, Loss: 2.572309374809265, Final Batch Loss: 1.273785948753357\n",
      "Epoch 101, Loss: 2.7562451362609863, Final Batch Loss: 1.3534047603607178\n",
      "Epoch 102, Loss: 2.624251961708069, Final Batch Loss: 1.304017424583435\n",
      "Epoch 103, Loss: 2.594646453857422, Final Batch Loss: 1.2759790420532227\n",
      "Epoch 104, Loss: 2.5423914194107056, Final Batch Loss: 1.276843786239624\n",
      "Epoch 105, Loss: 2.469182014465332, Final Batch Loss: 1.2227084636688232\n",
      "Epoch 106, Loss: 2.571101427078247, Final Batch Loss: 1.3098382949829102\n",
      "Epoch 107, Loss: 2.5783663988113403, Final Batch Loss: 1.2947074174880981\n",
      "Epoch 108, Loss: 2.4378323554992676, Final Batch Loss: 1.1880033016204834\n",
      "Epoch 109, Loss: 2.546698570251465, Final Batch Loss: 1.3048999309539795\n",
      "Epoch 110, Loss: 2.58623206615448, Final Batch Loss: 1.299115538597107\n",
      "Epoch 111, Loss: 2.4247970581054688, Final Batch Loss: 1.2140034437179565\n",
      "Epoch 112, Loss: 2.380631923675537, Final Batch Loss: 1.1554826498031616\n",
      "Epoch 113, Loss: 2.424140691757202, Final Batch Loss: 1.2038507461547852\n",
      "Epoch 114, Loss: 2.3909902572631836, Final Batch Loss: 1.234305739402771\n",
      "Epoch 115, Loss: 2.376277446746826, Final Batch Loss: 1.2027859687805176\n",
      "Epoch 116, Loss: 2.4142996072769165, Final Batch Loss: 1.243098258972168\n",
      "Epoch 117, Loss: 2.3860796689987183, Final Batch Loss: 1.208499789237976\n",
      "Epoch 118, Loss: 2.2568355798721313, Final Batch Loss: 1.077746868133545\n",
      "Epoch 119, Loss: 2.419979214668274, Final Batch Loss: 1.2280490398406982\n",
      "Epoch 120, Loss: 2.258971929550171, Final Batch Loss: 1.1181979179382324\n",
      "Epoch 121, Loss: 2.3361202478408813, Final Batch Loss: 1.169074296951294\n",
      "Epoch 122, Loss: 2.3171510696411133, Final Batch Loss: 1.2154724597930908\n",
      "Epoch 123, Loss: 2.2203023433685303, Final Batch Loss: 1.0654045343399048\n",
      "Epoch 124, Loss: 2.291377305984497, Final Batch Loss: 1.1540449857711792\n",
      "Epoch 125, Loss: 2.183170795440674, Final Batch Loss: 1.0523443222045898\n",
      "Epoch 126, Loss: 2.1708016395568848, Final Batch Loss: 1.024774193763733\n",
      "Epoch 127, Loss: 2.0600531101226807, Final Batch Loss: 0.9535852670669556\n",
      "Epoch 128, Loss: 2.269300937652588, Final Batch Loss: 1.1480239629745483\n",
      "Epoch 129, Loss: 2.208334803581238, Final Batch Loss: 1.0910377502441406\n",
      "Epoch 130, Loss: 2.2007367610931396, Final Batch Loss: 1.06678307056427\n",
      "Epoch 131, Loss: 2.1774604320526123, Final Batch Loss: 1.1015735864639282\n",
      "Epoch 132, Loss: 2.2174999713897705, Final Batch Loss: 1.0841219425201416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133, Loss: 2.148605227470398, Final Batch Loss: 1.0656206607818604\n",
      "Epoch 134, Loss: 2.2219574451446533, Final Batch Loss: 1.1627492904663086\n",
      "Epoch 135, Loss: 2.064661741256714, Final Batch Loss: 1.0076119899749756\n",
      "Epoch 136, Loss: 2.1647006273269653, Final Batch Loss: 1.1457512378692627\n",
      "Epoch 137, Loss: 2.120636463165283, Final Batch Loss: 1.0430405139923096\n",
      "Epoch 138, Loss: 2.1041252613067627, Final Batch Loss: 1.0282894372940063\n",
      "Epoch 139, Loss: 2.1791491508483887, Final Batch Loss: 1.1174991130828857\n",
      "Epoch 140, Loss: 2.077961564064026, Final Batch Loss: 1.0573025941848755\n",
      "Epoch 141, Loss: 2.0254147052764893, Final Batch Loss: 1.0610508918762207\n",
      "Epoch 142, Loss: 1.9701151251792908, Final Batch Loss: 0.9610900282859802\n",
      "Epoch 143, Loss: 1.970308005809784, Final Batch Loss: 1.044114112854004\n",
      "Epoch 144, Loss: 2.176750063896179, Final Batch Loss: 1.1239242553710938\n",
      "Epoch 145, Loss: 1.9815624356269836, Final Batch Loss: 0.937921941280365\n",
      "Epoch 146, Loss: 1.9613284468650818, Final Batch Loss: 0.918204128742218\n",
      "Epoch 147, Loss: 1.975681483745575, Final Batch Loss: 1.0453084707260132\n",
      "Epoch 148, Loss: 2.1268404722213745, Final Batch Loss: 1.0831685066223145\n",
      "Epoch 149, Loss: 1.948413908481598, Final Batch Loss: 0.910516083240509\n",
      "Epoch 150, Loss: 1.7677100896835327, Final Batch Loss: 0.836518406867981\n",
      "Epoch 151, Loss: 2.045236825942993, Final Batch Loss: 1.0438742637634277\n",
      "Epoch 152, Loss: 1.9325914978981018, Final Batch Loss: 0.9760881662368774\n",
      "Epoch 153, Loss: 1.8157798647880554, Final Batch Loss: 0.8864689469337463\n",
      "Epoch 154, Loss: 2.002916097640991, Final Batch Loss: 1.007519006729126\n",
      "Epoch 155, Loss: 1.8859825134277344, Final Batch Loss: 0.9274851083755493\n",
      "Epoch 156, Loss: 1.86369127035141, Final Batch Loss: 0.826716959476471\n",
      "Epoch 157, Loss: 1.997078537940979, Final Batch Loss: 1.0302159786224365\n",
      "Epoch 158, Loss: 1.8108367919921875, Final Batch Loss: 0.8730964064598083\n",
      "Epoch 159, Loss: 1.8922110199928284, Final Batch Loss: 0.9255282282829285\n",
      "Epoch 160, Loss: 1.896129071712494, Final Batch Loss: 1.000609040260315\n",
      "Epoch 161, Loss: 1.9143884181976318, Final Batch Loss: 0.9606696963310242\n",
      "Epoch 162, Loss: 1.8066381812095642, Final Batch Loss: 0.9055772423744202\n",
      "Epoch 163, Loss: 1.910652756690979, Final Batch Loss: 0.9911353588104248\n",
      "Epoch 164, Loss: 1.880618155002594, Final Batch Loss: 0.914914608001709\n",
      "Epoch 165, Loss: 1.8684142231941223, Final Batch Loss: 0.9222509860992432\n",
      "Epoch 166, Loss: 1.94009667634964, Final Batch Loss: 1.025085687637329\n",
      "Epoch 167, Loss: 1.801810622215271, Final Batch Loss: 0.9455891847610474\n",
      "Epoch 168, Loss: 2.014141261577606, Final Batch Loss: 1.030838966369629\n",
      "Epoch 169, Loss: 2.0217853784561157, Final Batch Loss: 1.1496535539627075\n",
      "Epoch 170, Loss: 1.7266092896461487, Final Batch Loss: 0.8550360798835754\n",
      "Epoch 171, Loss: 1.8409915566444397, Final Batch Loss: 0.8823497295379639\n",
      "Epoch 172, Loss: 1.7072933316230774, Final Batch Loss: 0.8145212531089783\n",
      "Epoch 173, Loss: 1.6957366466522217, Final Batch Loss: 0.7701877951622009\n",
      "Epoch 174, Loss: 1.7740086317062378, Final Batch Loss: 0.8858605623245239\n",
      "Epoch 175, Loss: 1.6113389134407043, Final Batch Loss: 0.7422472238540649\n",
      "Epoch 176, Loss: 1.7983283400535583, Final Batch Loss: 0.8688042759895325\n",
      "Epoch 177, Loss: 1.9046313166618347, Final Batch Loss: 1.0149571895599365\n",
      "Epoch 178, Loss: 1.8424756526947021, Final Batch Loss: 0.9415770173072815\n",
      "Epoch 179, Loss: 1.7643486857414246, Final Batch Loss: 0.8490066528320312\n",
      "Epoch 180, Loss: 1.7757859826087952, Final Batch Loss: 0.9363208413124084\n",
      "Epoch 181, Loss: 1.7717910408973694, Final Batch Loss: 0.8841941952705383\n",
      "Epoch 182, Loss: 1.6829653978347778, Final Batch Loss: 0.8301821947097778\n",
      "Epoch 183, Loss: 1.6575852632522583, Final Batch Loss: 0.8220164775848389\n",
      "Epoch 184, Loss: 1.6480923295021057, Final Batch Loss: 0.7928174138069153\n",
      "Epoch 185, Loss: 1.8618550896644592, Final Batch Loss: 1.036167025566101\n",
      "Epoch 186, Loss: 1.6476569771766663, Final Batch Loss: 0.8394088745117188\n",
      "Epoch 187, Loss: 1.7677816152572632, Final Batch Loss: 0.8977749347686768\n",
      "Epoch 188, Loss: 1.6167988777160645, Final Batch Loss: 0.8102172613143921\n",
      "Epoch 189, Loss: 1.7794985175132751, Final Batch Loss: 0.9059661626815796\n",
      "Epoch 190, Loss: 1.7810859084129333, Final Batch Loss: 0.8850119113922119\n",
      "Epoch 191, Loss: 1.6751448512077332, Final Batch Loss: 0.8123980164527893\n",
      "Epoch 192, Loss: 1.6274053454399109, Final Batch Loss: 0.8036556243896484\n",
      "Epoch 193, Loss: 1.4874610304832458, Final Batch Loss: 0.6315522193908691\n",
      "Epoch 194, Loss: 1.5092602372169495, Final Batch Loss: 0.7284966707229614\n",
      "Epoch 195, Loss: 1.9564979076385498, Final Batch Loss: 1.0690292119979858\n",
      "Epoch 196, Loss: 1.6783913373947144, Final Batch Loss: 0.8649246096611023\n",
      "Epoch 197, Loss: 1.7802852988243103, Final Batch Loss: 0.9237178564071655\n",
      "Epoch 198, Loss: 1.7554715275764465, Final Batch Loss: 0.9251265525817871\n",
      "Epoch 199, Loss: 1.6082820892333984, Final Batch Loss: 0.821404755115509\n",
      "Epoch 200, Loss: 1.6732945442199707, Final Batch Loss: 0.8329180479049683\n",
      "Epoch 201, Loss: 1.556043267250061, Final Batch Loss: 0.7276296615600586\n",
      "Epoch 202, Loss: 1.5470942258834839, Final Batch Loss: 0.7531087398529053\n",
      "Epoch 203, Loss: 1.5943951606750488, Final Batch Loss: 0.7845717072486877\n",
      "Epoch 204, Loss: 1.5561480522155762, Final Batch Loss: 0.7382305860519409\n",
      "Epoch 205, Loss: 1.6423380970954895, Final Batch Loss: 0.8022758960723877\n",
      "Epoch 206, Loss: 1.5795337557792664, Final Batch Loss: 0.790217936038971\n",
      "Epoch 207, Loss: 1.7465991377830505, Final Batch Loss: 0.8861969113349915\n",
      "Epoch 208, Loss: 1.7570211291313171, Final Batch Loss: 0.9315685629844666\n",
      "Epoch 209, Loss: 1.5399046540260315, Final Batch Loss: 0.7286224365234375\n",
      "Epoch 210, Loss: 1.594684362411499, Final Batch Loss: 0.8535795211791992\n",
      "Epoch 211, Loss: 1.5098812580108643, Final Batch Loss: 0.7906395792961121\n",
      "Epoch 212, Loss: 1.53899484872818, Final Batch Loss: 0.7269846200942993\n",
      "Epoch 213, Loss: 1.7144202589988708, Final Batch Loss: 0.9321998357772827\n",
      "Epoch 214, Loss: 1.5603245496749878, Final Batch Loss: 0.7978214025497437\n",
      "Epoch 215, Loss: 1.5855278968811035, Final Batch Loss: 0.7901033163070679\n",
      "Epoch 216, Loss: 1.5069808959960938, Final Batch Loss: 0.739879846572876\n",
      "Epoch 217, Loss: 1.564689576625824, Final Batch Loss: 0.8173387050628662\n",
      "Epoch 218, Loss: 1.4344080686569214, Final Batch Loss: 0.6847900152206421\n",
      "Epoch 219, Loss: 1.5809057354927063, Final Batch Loss: 0.8229995369911194\n",
      "Epoch 220, Loss: 1.4987145066261292, Final Batch Loss: 0.6972034573554993\n",
      "Epoch 221, Loss: 1.6447354555130005, Final Batch Loss: 0.8705878853797913\n",
      "Epoch 222, Loss: 1.4884766936302185, Final Batch Loss: 0.7574509978294373\n",
      "Epoch 223, Loss: 1.4576767086982727, Final Batch Loss: 0.7280848026275635\n",
      "Epoch 224, Loss: 1.6110695004463196, Final Batch Loss: 0.8339178562164307\n",
      "Epoch 225, Loss: 1.52154141664505, Final Batch Loss: 0.7946545481681824\n",
      "Epoch 226, Loss: 1.6163814067840576, Final Batch Loss: 0.868065357208252\n",
      "Epoch 227, Loss: 1.4279109239578247, Final Batch Loss: 0.7351750731468201\n",
      "Epoch 228, Loss: 1.4942724704742432, Final Batch Loss: 0.7324138283729553\n",
      "Epoch 229, Loss: 1.5303475260734558, Final Batch Loss: 0.8344322443008423\n",
      "Epoch 230, Loss: 1.37969309091568, Final Batch Loss: 0.6557184457778931\n",
      "Epoch 231, Loss: 1.5615955591201782, Final Batch Loss: 0.7872525453567505\n",
      "Epoch 232, Loss: 1.3685946464538574, Final Batch Loss: 0.6430694460868835\n",
      "Epoch 233, Loss: 1.378244400024414, Final Batch Loss: 0.6949288845062256\n",
      "Epoch 234, Loss: 1.4604910016059875, Final Batch Loss: 0.7233095765113831\n",
      "Epoch 235, Loss: 1.4129427671432495, Final Batch Loss: 0.7539710402488708\n",
      "Epoch 236, Loss: 1.3225654363632202, Final Batch Loss: 0.643442690372467\n",
      "Epoch 237, Loss: 1.4558197259902954, Final Batch Loss: 0.7242093682289124\n",
      "Epoch 238, Loss: 1.399911642074585, Final Batch Loss: 0.6989761590957642\n",
      "Epoch 239, Loss: 1.5410329699516296, Final Batch Loss: 0.8011409640312195\n",
      "Epoch 240, Loss: 1.5045021772384644, Final Batch Loss: 0.7572938203811646\n",
      "Epoch 241, Loss: 1.3359935879707336, Final Batch Loss: 0.6609971523284912\n",
      "Epoch 242, Loss: 1.4276461005210876, Final Batch Loss: 0.7865090370178223\n",
      "Epoch 243, Loss: 1.4291395545005798, Final Batch Loss: 0.7150735855102539\n",
      "Epoch 244, Loss: 1.3822849988937378, Final Batch Loss: 0.6460586190223694\n",
      "Epoch 245, Loss: 1.4661521315574646, Final Batch Loss: 0.7464355230331421\n",
      "Epoch 246, Loss: 1.2887467741966248, Final Batch Loss: 0.601642370223999\n",
      "Epoch 247, Loss: 1.3466956615447998, Final Batch Loss: 0.707757830619812\n",
      "Epoch 248, Loss: 1.4155850410461426, Final Batch Loss: 0.7788032293319702\n",
      "Epoch 249, Loss: 1.3535709977149963, Final Batch Loss: 0.6913744807243347\n",
      "Epoch 250, Loss: 1.4609739184379578, Final Batch Loss: 0.7815787196159363\n",
      "Epoch 251, Loss: 1.3771468997001648, Final Batch Loss: 0.6589140295982361\n",
      "Epoch 252, Loss: 1.282828688621521, Final Batch Loss: 0.6154178380966187\n",
      "Epoch 253, Loss: 1.4026827812194824, Final Batch Loss: 0.7465057373046875\n",
      "Epoch 254, Loss: 1.1882505416870117, Final Batch Loss: 0.5259594917297363\n",
      "Epoch 255, Loss: 1.432359516620636, Final Batch Loss: 0.7671541571617126\n",
      "Epoch 256, Loss: 1.224748134613037, Final Batch Loss: 0.5791490077972412\n",
      "Epoch 257, Loss: 1.3643375635147095, Final Batch Loss: 0.7009062767028809\n",
      "Epoch 258, Loss: 1.3339011669158936, Final Batch Loss: 0.6353701949119568\n",
      "Epoch 259, Loss: 1.2995129227638245, Final Batch Loss: 0.6433277130126953\n",
      "Epoch 260, Loss: 1.2141087651252747, Final Batch Loss: 0.5865445137023926\n",
      "Epoch 261, Loss: 1.3271413445472717, Final Batch Loss: 0.6954523921012878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262, Loss: 1.1759455800056458, Final Batch Loss: 0.5237343907356262\n",
      "Epoch 263, Loss: 1.2374668717384338, Final Batch Loss: 0.5326216816902161\n",
      "Epoch 264, Loss: 1.318650245666504, Final Batch Loss: 0.7443643808364868\n",
      "Epoch 265, Loss: 1.2007532119750977, Final Batch Loss: 0.6236628293991089\n",
      "Epoch 266, Loss: 1.210332989692688, Final Batch Loss: 0.5408207178115845\n",
      "Epoch 267, Loss: 1.3074212670326233, Final Batch Loss: 0.6441075205802917\n",
      "Epoch 268, Loss: 1.3648419976234436, Final Batch Loss: 0.7036536335945129\n",
      "Epoch 269, Loss: 1.3689398765563965, Final Batch Loss: 0.6968533992767334\n",
      "Epoch 270, Loss: 1.2337549328804016, Final Batch Loss: 0.6071485280990601\n",
      "Epoch 271, Loss: 1.2706822156906128, Final Batch Loss: 0.6235003471374512\n",
      "Epoch 272, Loss: 1.2381436228752136, Final Batch Loss: 0.5838121771812439\n",
      "Epoch 273, Loss: 1.2226536273956299, Final Batch Loss: 0.5680387616157532\n",
      "Epoch 274, Loss: 1.2038246393203735, Final Batch Loss: 0.5902066826820374\n",
      "Epoch 275, Loss: 1.178099513053894, Final Batch Loss: 0.6031844615936279\n",
      "Epoch 276, Loss: 1.1755539178848267, Final Batch Loss: 0.5649229884147644\n",
      "Epoch 277, Loss: 1.2972129583358765, Final Batch Loss: 0.7155353426933289\n",
      "Epoch 278, Loss: 1.2125158905982971, Final Batch Loss: 0.651742696762085\n",
      "Epoch 279, Loss: 1.2217029333114624, Final Batch Loss: 0.5859665870666504\n",
      "Epoch 280, Loss: 1.1218637228012085, Final Batch Loss: 0.5525188446044922\n",
      "Epoch 281, Loss: 1.0863332748413086, Final Batch Loss: 0.5479391813278198\n",
      "Epoch 282, Loss: 1.0783135890960693, Final Batch Loss: 0.5169858932495117\n",
      "Epoch 283, Loss: 1.1717689037322998, Final Batch Loss: 0.5831654667854309\n",
      "Epoch 284, Loss: 1.0923209190368652, Final Batch Loss: 0.5655087828636169\n",
      "Epoch 285, Loss: 1.1424624919891357, Final Batch Loss: 0.5800718069076538\n",
      "Epoch 286, Loss: 1.0863888263702393, Final Batch Loss: 0.47525936365127563\n",
      "Epoch 287, Loss: 1.134101688861847, Final Batch Loss: 0.5452094674110413\n",
      "Epoch 288, Loss: 1.0221291184425354, Final Batch Loss: 0.4570539593696594\n",
      "Epoch 289, Loss: 1.080666184425354, Final Batch Loss: 0.5372357964515686\n",
      "Epoch 290, Loss: 1.0904959440231323, Final Batch Loss: 0.5537025332450867\n",
      "Epoch 291, Loss: 1.1292808651924133, Final Batch Loss: 0.5700547695159912\n",
      "Epoch 292, Loss: 1.045758992433548, Final Batch Loss: 0.5754892826080322\n",
      "Epoch 293, Loss: 1.0571784377098083, Final Batch Loss: 0.5060567259788513\n",
      "Epoch 294, Loss: 1.0363993644714355, Final Batch Loss: 0.5249965190887451\n",
      "Epoch 295, Loss: 0.9755199253559113, Final Batch Loss: 0.451334148645401\n",
      "Epoch 296, Loss: 1.131453514099121, Final Batch Loss: 0.5937652587890625\n",
      "Epoch 297, Loss: 1.0798552632331848, Final Batch Loss: 0.5087288022041321\n",
      "Epoch 298, Loss: 0.9439131915569305, Final Batch Loss: 0.47430604696273804\n",
      "Epoch 299, Loss: 1.0115019083023071, Final Batch Loss: 0.503108561038971\n",
      "Epoch 300, Loss: 0.9426426887512207, Final Batch Loss: 0.46735647320747375\n",
      "Epoch 301, Loss: 0.8830772638320923, Final Batch Loss: 0.4038810133934021\n",
      "Epoch 302, Loss: 0.9236727356910706, Final Batch Loss: 0.3994801640510559\n",
      "Epoch 303, Loss: 0.8604553043842316, Final Batch Loss: 0.4079025685787201\n",
      "Epoch 304, Loss: 0.9614285528659821, Final Batch Loss: 0.47048357129096985\n",
      "Epoch 305, Loss: 0.8945438265800476, Final Batch Loss: 0.46874991059303284\n",
      "Epoch 306, Loss: 0.8941107094287872, Final Batch Loss: 0.4582044780254364\n",
      "Epoch 307, Loss: 1.0394696295261383, Final Batch Loss: 0.5747610330581665\n",
      "Epoch 308, Loss: 0.9200197756290436, Final Batch Loss: 0.45327842235565186\n",
      "Epoch 309, Loss: 0.9083641767501831, Final Batch Loss: 0.44151318073272705\n",
      "Epoch 310, Loss: 0.9547196924686432, Final Batch Loss: 0.504590630531311\n",
      "Epoch 311, Loss: 0.8260253667831421, Final Batch Loss: 0.3618658781051636\n",
      "Epoch 312, Loss: 0.809395432472229, Final Batch Loss: 0.4037679135799408\n",
      "Epoch 313, Loss: 0.9496982097625732, Final Batch Loss: 0.529748797416687\n",
      "Epoch 314, Loss: 0.8476948142051697, Final Batch Loss: 0.44939762353897095\n",
      "Epoch 315, Loss: 0.8155404627323151, Final Batch Loss: 0.37125900387763977\n",
      "Epoch 316, Loss: 0.8687391579151154, Final Batch Loss: 0.4550851881504059\n",
      "Epoch 317, Loss: 0.7902867794036865, Final Batch Loss: 0.34783506393432617\n",
      "Epoch 318, Loss: 0.7449440360069275, Final Batch Loss: 0.35445716977119446\n",
      "Epoch 319, Loss: 0.8808515071868896, Final Batch Loss: 0.4335287809371948\n",
      "Epoch 320, Loss: 0.8378314077854156, Final Batch Loss: 0.3936421275138855\n",
      "Epoch 321, Loss: 0.7536312937736511, Final Batch Loss: 0.3492836654186249\n",
      "Epoch 322, Loss: 0.9160373210906982, Final Batch Loss: 0.3982716202735901\n",
      "Epoch 323, Loss: 0.7279704511165619, Final Batch Loss: 0.3683704137802124\n",
      "Epoch 324, Loss: 0.7532918155193329, Final Batch Loss: 0.3777884840965271\n",
      "Epoch 325, Loss: 0.7720829546451569, Final Batch Loss: 0.3159216046333313\n",
      "Epoch 326, Loss: 0.9260585308074951, Final Batch Loss: 0.49455970525741577\n",
      "Epoch 327, Loss: 0.7030037343502045, Final Batch Loss: 0.312337726354599\n",
      "Epoch 328, Loss: 0.8092060089111328, Final Batch Loss: 0.3796602487564087\n",
      "Epoch 329, Loss: 0.9190509021282196, Final Batch Loss: 0.496784508228302\n",
      "Epoch 330, Loss: 0.7316953539848328, Final Batch Loss: 0.3854239284992218\n",
      "Epoch 331, Loss: 0.8276348412036896, Final Batch Loss: 0.4603852927684784\n",
      "Epoch 332, Loss: 0.8360609710216522, Final Batch Loss: 0.40880411863327026\n",
      "Epoch 333, Loss: 0.927757203578949, Final Batch Loss: 0.5487093925476074\n",
      "Epoch 334, Loss: 0.7832651436328888, Final Batch Loss: 0.40349525213241577\n",
      "Epoch 335, Loss: 0.7935671806335449, Final Batch Loss: 0.36981427669525146\n",
      "Epoch 336, Loss: 0.7362224459648132, Final Batch Loss: 0.33110252022743225\n",
      "Epoch 337, Loss: 0.7417519092559814, Final Batch Loss: 0.33150514960289\n",
      "Epoch 338, Loss: 0.9888174831867218, Final Batch Loss: 0.5602594017982483\n",
      "Epoch 339, Loss: 0.8047801554203033, Final Batch Loss: 0.40502649545669556\n",
      "Epoch 340, Loss: 0.6511644721031189, Final Batch Loss: 0.29706814885139465\n",
      "Epoch 341, Loss: 0.7180751860141754, Final Batch Loss: 0.3479946255683899\n",
      "Epoch 342, Loss: 0.7457666099071503, Final Batch Loss: 0.31515470147132874\n",
      "Epoch 343, Loss: 0.7889903485774994, Final Batch Loss: 0.4406362473964691\n",
      "Epoch 344, Loss: 0.7365833222866058, Final Batch Loss: 0.365566611289978\n",
      "Epoch 345, Loss: 0.6888565123081207, Final Batch Loss: 0.3485518991947174\n",
      "Epoch 346, Loss: 0.6780249774456024, Final Batch Loss: 0.41199466586112976\n",
      "Epoch 347, Loss: 0.7348628640174866, Final Batch Loss: 0.3759743571281433\n",
      "Epoch 348, Loss: 0.6401924788951874, Final Batch Loss: 0.305808961391449\n",
      "Epoch 349, Loss: 0.7685509324073792, Final Batch Loss: 0.4012596309185028\n",
      "Epoch 350, Loss: 0.639013797044754, Final Batch Loss: 0.33595722913742065\n",
      "Epoch 351, Loss: 0.8121838271617889, Final Batch Loss: 0.4475565552711487\n",
      "Epoch 352, Loss: 0.6460497975349426, Final Batch Loss: 0.34164905548095703\n",
      "Epoch 353, Loss: 0.7294798493385315, Final Batch Loss: 0.3337879180908203\n",
      "Epoch 354, Loss: 0.79131880402565, Final Batch Loss: 0.42744043469429016\n",
      "Epoch 355, Loss: 0.6215368211269379, Final Batch Loss: 0.2835429310798645\n",
      "Epoch 356, Loss: 0.7072039246559143, Final Batch Loss: 0.43948057293891907\n",
      "Epoch 357, Loss: 0.7154217660427094, Final Batch Loss: 0.3438814580440521\n",
      "Epoch 358, Loss: 0.6052211225032806, Final Batch Loss: 0.2994322180747986\n",
      "Epoch 359, Loss: 0.6360709071159363, Final Batch Loss: 0.3075437545776367\n",
      "Epoch 360, Loss: 0.7660576105117798, Final Batch Loss: 0.46283459663391113\n",
      "Epoch 361, Loss: 0.6696639358997345, Final Batch Loss: 0.30951979756355286\n",
      "Epoch 362, Loss: 0.5731154084205627, Final Batch Loss: 0.28322604298591614\n",
      "Epoch 363, Loss: 0.781592071056366, Final Batch Loss: 0.4139428436756134\n",
      "Epoch 364, Loss: 0.5778335630893707, Final Batch Loss: 0.26723024249076843\n",
      "Epoch 365, Loss: 0.5543867945671082, Final Batch Loss: 0.24442583322525024\n",
      "Epoch 366, Loss: 0.6237240433692932, Final Batch Loss: 0.24677228927612305\n",
      "Epoch 367, Loss: 0.6676729023456573, Final Batch Loss: 0.30746588110923767\n",
      "Epoch 368, Loss: 0.5976094901561737, Final Batch Loss: 0.2892346978187561\n",
      "Epoch 369, Loss: 0.5925005972385406, Final Batch Loss: 0.2596116364002228\n",
      "Epoch 370, Loss: 0.5609050095081329, Final Batch Loss: 0.27562811970710754\n",
      "Epoch 371, Loss: 0.660430371761322, Final Batch Loss: 0.3698168694972992\n",
      "Epoch 372, Loss: 0.6704072952270508, Final Batch Loss: 0.37110400199890137\n",
      "Epoch 373, Loss: 0.6703247427940369, Final Batch Loss: 0.3960745930671692\n",
      "Epoch 374, Loss: 0.667609453201294, Final Batch Loss: 0.32920971512794495\n",
      "Epoch 375, Loss: 0.6379411518573761, Final Batch Loss: 0.3811055123806\n",
      "Epoch 376, Loss: 0.3771773725748062, Final Batch Loss: 0.1499718427658081\n",
      "Epoch 377, Loss: 0.5980203747749329, Final Batch Loss: 0.3446040749549866\n",
      "Epoch 378, Loss: 0.657891571521759, Final Batch Loss: 0.3426620066165924\n",
      "Epoch 379, Loss: 0.5189618915319443, Final Batch Loss: 0.2141697257757187\n",
      "Epoch 380, Loss: 0.5995409488677979, Final Batch Loss: 0.29504460096359253\n",
      "Epoch 381, Loss: 0.5534563958644867, Final Batch Loss: 0.29825976490974426\n",
      "Epoch 382, Loss: 0.4891998767852783, Final Batch Loss: 0.22696387767791748\n",
      "Epoch 383, Loss: 0.6145371198654175, Final Batch Loss: 0.30570903420448303\n",
      "Epoch 384, Loss: 0.5384650826454163, Final Batch Loss: 0.2795477509498596\n",
      "Epoch 385, Loss: 0.5877596735954285, Final Batch Loss: 0.2771188020706177\n",
      "Epoch 386, Loss: 0.542270764708519, Final Batch Loss: 0.2367028445005417\n",
      "Epoch 387, Loss: 0.3884054124355316, Final Batch Loss: 0.1236886978149414\n",
      "Epoch 388, Loss: 0.642215222120285, Final Batch Loss: 0.36304396390914917\n",
      "Epoch 389, Loss: 0.5685400068759918, Final Batch Loss: 0.25887539982795715\n",
      "Epoch 390, Loss: 0.6400963068008423, Final Batch Loss: 0.33621513843536377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391, Loss: 0.5839110314846039, Final Batch Loss: 0.2530598044395447\n",
      "Epoch 392, Loss: 0.5263825356960297, Final Batch Loss: 0.23349860310554504\n",
      "Epoch 393, Loss: 0.597059965133667, Final Batch Loss: 0.333942711353302\n",
      "Epoch 394, Loss: 0.4945167452096939, Final Batch Loss: 0.2485407292842865\n",
      "Epoch 395, Loss: 0.4382820278406143, Final Batch Loss: 0.20833449065685272\n",
      "Epoch 396, Loss: 0.48337221145629883, Final Batch Loss: 0.2543613314628601\n",
      "Epoch 397, Loss: 0.49173930287361145, Final Batch Loss: 0.19750818610191345\n",
      "Epoch 398, Loss: 0.5439663231372833, Final Batch Loss: 0.28462886810302734\n",
      "Epoch 399, Loss: 0.49111609160900116, Final Batch Loss: 0.2610728442668915\n",
      "Epoch 400, Loss: 0.5370165705680847, Final Batch Loss: 0.2609894275665283\n",
      "Epoch 401, Loss: 0.5133425295352936, Final Batch Loss: 0.22650936245918274\n",
      "Epoch 402, Loss: 0.45670609176158905, Final Batch Loss: 0.18724869191646576\n",
      "Epoch 403, Loss: 0.5441243797540665, Final Batch Loss: 0.31270259618759155\n",
      "Epoch 404, Loss: 0.6136711239814758, Final Batch Loss: 0.36630937457084656\n",
      "Epoch 405, Loss: 0.5882962346076965, Final Batch Loss: 0.30081531405448914\n",
      "Epoch 406, Loss: 0.6474516987800598, Final Batch Loss: 0.39368805289268494\n",
      "Epoch 407, Loss: 0.5194330364465714, Final Batch Loss: 0.23492349684238434\n",
      "Epoch 408, Loss: 0.5109867453575134, Final Batch Loss: 0.23526418209075928\n",
      "Epoch 409, Loss: 0.5740263760089874, Final Batch Loss: 0.2706240713596344\n",
      "Epoch 410, Loss: 0.5679726004600525, Final Batch Loss: 0.3128232955932617\n",
      "Epoch 411, Loss: 0.5956118702888489, Final Batch Loss: 0.3610990047454834\n",
      "Epoch 412, Loss: 0.5541660934686661, Final Batch Loss: 0.3407253324985504\n",
      "Epoch 413, Loss: 0.4441990703344345, Final Batch Loss: 0.19579166173934937\n",
      "Epoch 414, Loss: 0.5633717477321625, Final Batch Loss: 0.3222272992134094\n",
      "Epoch 415, Loss: 0.4756228178739548, Final Batch Loss: 0.22644193470478058\n",
      "Epoch 416, Loss: 0.5812180638313293, Final Batch Loss: 0.3154020309448242\n",
      "Epoch 417, Loss: 0.4848037362098694, Final Batch Loss: 0.2566423714160919\n",
      "Epoch 418, Loss: 0.42559920251369476, Final Batch Loss: 0.20445089042186737\n",
      "Epoch 419, Loss: 0.5250398516654968, Final Batch Loss: 0.2652125060558319\n",
      "Epoch 420, Loss: 0.4752195179462433, Final Batch Loss: 0.246124267578125\n",
      "Epoch 421, Loss: 0.5577516555786133, Final Batch Loss: 0.26554927229881287\n",
      "Epoch 422, Loss: 0.4508763402700424, Final Batch Loss: 0.18884174525737762\n",
      "Epoch 423, Loss: 0.5105605870485306, Final Batch Loss: 0.24816583096981049\n",
      "Epoch 424, Loss: 0.47946012020111084, Final Batch Loss: 0.19817855954170227\n",
      "Epoch 425, Loss: 0.42434336245059967, Final Batch Loss: 0.2064480036497116\n",
      "Epoch 426, Loss: 0.48572736978530884, Final Batch Loss: 0.2566514313220978\n",
      "Epoch 427, Loss: 0.4701745957136154, Final Batch Loss: 0.3118531405925751\n",
      "Epoch 428, Loss: 0.5547376573085785, Final Batch Loss: 0.2799859344959259\n",
      "Epoch 429, Loss: 0.44341060519218445, Final Batch Loss: 0.24334745109081268\n",
      "Epoch 430, Loss: 0.3665238097310066, Final Batch Loss: 0.11993133276700974\n",
      "Epoch 431, Loss: 0.528362050652504, Final Batch Loss: 0.24004589021205902\n",
      "Epoch 432, Loss: 0.5880882740020752, Final Batch Loss: 0.2849201560020447\n",
      "Epoch 433, Loss: 0.40091684460639954, Final Batch Loss: 0.22340400516986847\n",
      "Epoch 434, Loss: 0.5087767243385315, Final Batch Loss: 0.3302881121635437\n",
      "Epoch 435, Loss: 0.5242902487516403, Final Batch Loss: 0.275050550699234\n",
      "Epoch 436, Loss: 0.48329657316207886, Final Batch Loss: 0.20466196537017822\n",
      "Epoch 437, Loss: 0.5566038936376572, Final Batch Loss: 0.3172638714313507\n",
      "Epoch 438, Loss: 0.4674692451953888, Final Batch Loss: 0.22195349633693695\n",
      "Epoch 439, Loss: 0.42173314094543457, Final Batch Loss: 0.1542639434337616\n",
      "Epoch 440, Loss: 0.5218945145606995, Final Batch Loss: 0.29431578516960144\n",
      "Epoch 441, Loss: 0.5174994170665741, Final Batch Loss: 0.22939106822013855\n",
      "Epoch 442, Loss: 0.6195367872714996, Final Batch Loss: 0.262832373380661\n",
      "Epoch 443, Loss: 0.4647857993841171, Final Batch Loss: 0.22974637150764465\n",
      "Epoch 444, Loss: 0.4133679270744324, Final Batch Loss: 0.18941204249858856\n",
      "Epoch 445, Loss: 0.4508676081895828, Final Batch Loss: 0.26139912009239197\n",
      "Epoch 446, Loss: 0.4721556156873703, Final Batch Loss: 0.2468649297952652\n",
      "Epoch 447, Loss: 0.4195048213005066, Final Batch Loss: 0.14101052284240723\n",
      "Epoch 448, Loss: 0.43321025371551514, Final Batch Loss: 0.2123422473669052\n",
      "Epoch 449, Loss: 0.4467237740755081, Final Batch Loss: 0.24152985215187073\n",
      "Epoch 450, Loss: 0.480879470705986, Final Batch Loss: 0.22610057890415192\n",
      "Epoch 451, Loss: 0.5543659627437592, Final Batch Loss: 0.28287410736083984\n",
      "Epoch 452, Loss: 0.49753282964229584, Final Batch Loss: 0.23395399749279022\n",
      "Epoch 453, Loss: 0.44349531829357147, Final Batch Loss: 0.21841788291931152\n",
      "Epoch 454, Loss: 0.3387402445077896, Final Batch Loss: 0.13667252659797668\n",
      "Epoch 455, Loss: 0.3622921109199524, Final Batch Loss: 0.16757367551326752\n",
      "Epoch 456, Loss: 0.4263724386692047, Final Batch Loss: 0.21362976729869843\n",
      "Epoch 457, Loss: 0.3300085663795471, Final Batch Loss: 0.17288582026958466\n",
      "Epoch 458, Loss: 0.2978954315185547, Final Batch Loss: 0.10380779206752777\n",
      "Epoch 459, Loss: 0.46513764560222626, Final Batch Loss: 0.26936376094818115\n",
      "Epoch 460, Loss: 0.46676307916641235, Final Batch Loss: 0.22779051959514618\n",
      "Epoch 461, Loss: 0.46753592789173126, Final Batch Loss: 0.20536722242832184\n",
      "Epoch 462, Loss: 0.3258190304040909, Final Batch Loss: 0.1480078250169754\n",
      "Epoch 463, Loss: 0.41330352425575256, Final Batch Loss: 0.1877567172050476\n",
      "Epoch 464, Loss: 0.35827672481536865, Final Batch Loss: 0.20595450699329376\n",
      "Epoch 465, Loss: 0.3587299883365631, Final Batch Loss: 0.14450272917747498\n",
      "Epoch 466, Loss: 0.3735159635543823, Final Batch Loss: 0.19329413771629333\n",
      "Epoch 467, Loss: 0.457063227891922, Final Batch Loss: 0.24303965270519257\n",
      "Epoch 468, Loss: 0.4543590098619461, Final Batch Loss: 0.26635944843292236\n",
      "Epoch 469, Loss: 0.4002864956855774, Final Batch Loss: 0.22679896652698517\n",
      "Epoch 470, Loss: 0.3288620188832283, Final Batch Loss: 0.12432227283716202\n",
      "Epoch 471, Loss: 0.37649181485176086, Final Batch Loss: 0.20111404359340668\n",
      "Epoch 472, Loss: 0.3134320229291916, Final Batch Loss: 0.13612355291843414\n",
      "Epoch 473, Loss: 0.38348977267742157, Final Batch Loss: 0.18170800805091858\n",
      "Epoch 474, Loss: 0.4058600068092346, Final Batch Loss: 0.1817265748977661\n",
      "Epoch 475, Loss: 0.38182662427425385, Final Batch Loss: 0.15925897657871246\n",
      "Epoch 476, Loss: 0.42452920973300934, Final Batch Loss: 0.2162809520959854\n",
      "Epoch 477, Loss: 0.4319867789745331, Final Batch Loss: 0.21010524034500122\n",
      "Epoch 478, Loss: 0.39977388083934784, Final Batch Loss: 0.2096899300813675\n",
      "Epoch 479, Loss: 0.5046144276857376, Final Batch Loss: 0.2782096862792969\n",
      "Epoch 480, Loss: 0.34421955049037933, Final Batch Loss: 0.14612948894500732\n",
      "Epoch 481, Loss: 0.5059482753276825, Final Batch Loss: 0.2695990800857544\n",
      "Epoch 482, Loss: 0.44675682485103607, Final Batch Loss: 0.23976071178913116\n",
      "Epoch 483, Loss: 0.3497942239046097, Final Batch Loss: 0.17871373891830444\n",
      "Epoch 484, Loss: 0.3972596079111099, Final Batch Loss: 0.15497136116027832\n",
      "Epoch 485, Loss: 0.31535039842128754, Final Batch Loss: 0.13360553979873657\n",
      "Epoch 486, Loss: 0.3129641264677048, Final Batch Loss: 0.15960751473903656\n",
      "Epoch 487, Loss: 0.27836261689662933, Final Batch Loss: 0.11983661353588104\n",
      "Epoch 488, Loss: 0.42895087599754333, Final Batch Loss: 0.18063752353191376\n",
      "Epoch 489, Loss: 0.4066731631755829, Final Batch Loss: 0.2054363191127777\n",
      "Epoch 490, Loss: 0.29885171353816986, Final Batch Loss: 0.13526184856891632\n",
      "Epoch 491, Loss: 0.2756549119949341, Final Batch Loss: 0.11857253313064575\n",
      "Epoch 492, Loss: 0.37247617542743683, Final Batch Loss: 0.16959871351718903\n",
      "Epoch 493, Loss: 0.32481910288333893, Final Batch Loss: 0.12900695204734802\n",
      "Epoch 494, Loss: 0.2851371169090271, Final Batch Loss: 0.11096310615539551\n",
      "Epoch 495, Loss: 0.3214891403913498, Final Batch Loss: 0.1543273627758026\n",
      "Epoch 496, Loss: 0.41553279757499695, Final Batch Loss: 0.1855209320783615\n",
      "Epoch 497, Loss: 0.41293397545814514, Final Batch Loss: 0.18491345643997192\n",
      "Epoch 498, Loss: 0.4071563482284546, Final Batch Loss: 0.20507366955280304\n",
      "Epoch 499, Loss: 0.3534405678510666, Final Batch Loss: 0.1765855997800827\n",
      "Epoch 500, Loss: 0.4947199672460556, Final Batch Loss: 0.29008129239082336\n",
      "Epoch 501, Loss: 0.3388938903808594, Final Batch Loss: 0.14510121941566467\n",
      "Epoch 502, Loss: 0.48586517572402954, Final Batch Loss: 0.24700769782066345\n",
      "Epoch 503, Loss: 0.31676799058914185, Final Batch Loss: 0.16196218132972717\n",
      "Epoch 504, Loss: 0.298673540353775, Final Batch Loss: 0.13354697823524475\n",
      "Epoch 505, Loss: 0.26533976197242737, Final Batch Loss: 0.10019607841968536\n",
      "Epoch 506, Loss: 0.3968852311372757, Final Batch Loss: 0.21095970273017883\n",
      "Epoch 507, Loss: 0.32428981363773346, Final Batch Loss: 0.1380208283662796\n",
      "Epoch 508, Loss: 0.3225487917661667, Final Batch Loss: 0.17184969782829285\n",
      "Epoch 509, Loss: 0.30044619739055634, Final Batch Loss: 0.13911665976047516\n",
      "Epoch 510, Loss: 0.31163859367370605, Final Batch Loss: 0.11385926604270935\n",
      "Epoch 511, Loss: 0.2791232317686081, Final Batch Loss: 0.12824693322181702\n",
      "Epoch 512, Loss: 0.460142120718956, Final Batch Loss: 0.27670618891716003\n",
      "Epoch 513, Loss: 0.49512939155101776, Final Batch Loss: 0.29135677218437195\n",
      "Epoch 514, Loss: 0.33362118899822235, Final Batch Loss: 0.1677774041891098\n",
      "Epoch 515, Loss: 0.44713637232780457, Final Batch Loss: 0.20032751560211182\n",
      "Epoch 516, Loss: 0.3356696218252182, Final Batch Loss: 0.1859435886144638\n",
      "Epoch 517, Loss: 0.3658173680305481, Final Batch Loss: 0.2111082226037979\n",
      "Epoch 518, Loss: 0.36351674795150757, Final Batch Loss: 0.23424090445041656\n",
      "Epoch 519, Loss: 0.38345666229724884, Final Batch Loss: 0.2085813581943512\n",
      "Epoch 520, Loss: 0.45909756422042847, Final Batch Loss: 0.2705700397491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 521, Loss: 0.313468798995018, Final Batch Loss: 0.14162860810756683\n",
      "Epoch 522, Loss: 0.3610745221376419, Final Batch Loss: 0.18508115410804749\n",
      "Epoch 523, Loss: 0.3074585646390915, Final Batch Loss: 0.1536046266555786\n",
      "Epoch 524, Loss: 0.3905794620513916, Final Batch Loss: 0.2013835906982422\n",
      "Epoch 525, Loss: 0.24903105944395065, Final Batch Loss: 0.10178273171186447\n",
      "Epoch 526, Loss: 0.22354263812303543, Final Batch Loss: 0.10038977861404419\n",
      "Epoch 527, Loss: 0.32604360580444336, Final Batch Loss: 0.14108778536319733\n",
      "Epoch 528, Loss: 0.28754329681396484, Final Batch Loss: 0.11028532683849335\n",
      "Epoch 529, Loss: 0.44519568979740143, Final Batch Loss: 0.3073287606239319\n",
      "Epoch 530, Loss: 0.42404040694236755, Final Batch Loss: 0.21934862434864044\n",
      "Epoch 531, Loss: 0.27606096118688583, Final Batch Loss: 0.11494158953428268\n",
      "Epoch 532, Loss: 0.3825228214263916, Final Batch Loss: 0.2205248773097992\n",
      "Epoch 533, Loss: 0.41063156723976135, Final Batch Loss: 0.2447146624326706\n",
      "Epoch 534, Loss: 0.21982206404209137, Final Batch Loss: 0.09402403235435486\n",
      "Epoch 535, Loss: 0.28348392993211746, Final Batch Loss: 0.16787712275981903\n",
      "Epoch 536, Loss: 0.24628955125808716, Final Batch Loss: 0.101321280002594\n",
      "Epoch 537, Loss: 0.33872920274734497, Final Batch Loss: 0.17277966439723969\n",
      "Epoch 538, Loss: 0.2864217311143875, Final Batch Loss: 0.1129923015832901\n",
      "Epoch 539, Loss: 0.2831549793481827, Final Batch Loss: 0.14909148216247559\n",
      "Epoch 540, Loss: 0.3886944651603699, Final Batch Loss: 0.25786155462265015\n",
      "Epoch 541, Loss: 0.23685061186552048, Final Batch Loss: 0.08491379767656326\n",
      "Epoch 542, Loss: 0.2993512600660324, Final Batch Loss: 0.15434686839580536\n",
      "Epoch 543, Loss: 0.37258733808994293, Final Batch Loss: 0.17631405591964722\n",
      "Epoch 544, Loss: 0.36176422238349915, Final Batch Loss: 0.18759185075759888\n",
      "Epoch 545, Loss: 0.34652552008628845, Final Batch Loss: 0.1693817526102066\n",
      "Epoch 546, Loss: 0.3019435256719589, Final Batch Loss: 0.15683716535568237\n",
      "Epoch 547, Loss: 0.3735024333000183, Final Batch Loss: 0.18124674260616302\n",
      "Epoch 548, Loss: 0.23675795644521713, Final Batch Loss: 0.059420280158519745\n",
      "Epoch 549, Loss: 0.2752862200140953, Final Batch Loss: 0.08938615769147873\n",
      "Epoch 550, Loss: 0.25422535836696625, Final Batch Loss: 0.1361054629087448\n",
      "Epoch 551, Loss: 0.33564499020576477, Final Batch Loss: 0.17989104986190796\n",
      "Epoch 552, Loss: 0.32831375300884247, Final Batch Loss: 0.14385968446731567\n",
      "Epoch 553, Loss: 0.3493383377790451, Final Batch Loss: 0.19983477890491486\n",
      "Epoch 554, Loss: 0.3817294239997864, Final Batch Loss: 0.18722088634967804\n",
      "Epoch 555, Loss: 0.38818998634815216, Final Batch Loss: 0.21461816132068634\n",
      "Epoch 556, Loss: 0.3021388351917267, Final Batch Loss: 0.1629391461610794\n",
      "Epoch 557, Loss: 0.24518193304538727, Final Batch Loss: 0.11520642042160034\n",
      "Epoch 558, Loss: 0.31884171068668365, Final Batch Loss: 0.15773817896842957\n",
      "Epoch 559, Loss: 0.3163990005850792, Final Batch Loss: 0.1192290261387825\n",
      "Epoch 560, Loss: 0.4041290432214737, Final Batch Loss: 0.19265154004096985\n",
      "Epoch 561, Loss: 0.2516011744737625, Final Batch Loss: 0.13041245937347412\n",
      "Epoch 562, Loss: 0.3064820170402527, Final Batch Loss: 0.17463259398937225\n",
      "Epoch 563, Loss: 0.29165883362293243, Final Batch Loss: 0.15322548151016235\n",
      "Epoch 564, Loss: 0.2405954897403717, Final Batch Loss: 0.09686970710754395\n",
      "Epoch 565, Loss: 0.3154420256614685, Final Batch Loss: 0.15282750129699707\n",
      "Epoch 566, Loss: 0.4105011820793152, Final Batch Loss: 0.228924959897995\n",
      "Epoch 567, Loss: 0.3785061240196228, Final Batch Loss: 0.2409394085407257\n",
      "Epoch 568, Loss: 0.2974236160516739, Final Batch Loss: 0.126132071018219\n",
      "Epoch 569, Loss: 0.2696525454521179, Final Batch Loss: 0.11573536694049835\n",
      "Epoch 570, Loss: 0.25161533802747726, Final Batch Loss: 0.12295866757631302\n",
      "Epoch 571, Loss: 0.22880063205957413, Final Batch Loss: 0.11164261400699615\n",
      "Epoch 572, Loss: 0.2276373729109764, Final Batch Loss: 0.09550314396619797\n",
      "Epoch 573, Loss: 0.26892662048339844, Final Batch Loss: 0.11655528843402863\n",
      "Epoch 574, Loss: 0.3233967572450638, Final Batch Loss: 0.1288323700428009\n",
      "Epoch 575, Loss: 0.42170973122119904, Final Batch Loss: 0.1972273737192154\n",
      "Epoch 576, Loss: 0.3722977042198181, Final Batch Loss: 0.23418915271759033\n",
      "Epoch 577, Loss: 0.35735271871089935, Final Batch Loss: 0.21636024117469788\n",
      "Epoch 578, Loss: 0.2376837357878685, Final Batch Loss: 0.10854389518499374\n",
      "Epoch 579, Loss: 0.18469907715916634, Final Batch Loss: 0.050833333283662796\n",
      "Epoch 580, Loss: 0.27798615396022797, Final Batch Loss: 0.11477184295654297\n",
      "Epoch 581, Loss: 0.3252454251050949, Final Batch Loss: 0.18944904208183289\n",
      "Epoch 582, Loss: 0.31718094646930695, Final Batch Loss: 0.16996707022190094\n",
      "Epoch 583, Loss: 0.31632982194423676, Final Batch Loss: 0.1305171698331833\n",
      "Epoch 584, Loss: 0.25036225467920303, Final Batch Loss: 0.0818059965968132\n",
      "Epoch 585, Loss: 0.39099688827991486, Final Batch Loss: 0.21553286910057068\n",
      "Epoch 586, Loss: 0.24151934683322906, Final Batch Loss: 0.11409592628479004\n",
      "Epoch 587, Loss: 0.28750084340572357, Final Batch Loss: 0.15794864296913147\n",
      "Epoch 588, Loss: 0.27934835851192474, Final Batch Loss: 0.1255253404378891\n",
      "Epoch 589, Loss: 0.2909735143184662, Final Batch Loss: 0.16606169939041138\n",
      "Epoch 590, Loss: 0.24568859487771988, Final Batch Loss: 0.0915178582072258\n",
      "Epoch 591, Loss: 0.2975655496120453, Final Batch Loss: 0.16309864819049835\n",
      "Epoch 592, Loss: 0.3743036836385727, Final Batch Loss: 0.22977185249328613\n",
      "Epoch 593, Loss: 0.324211448431015, Final Batch Loss: 0.16824446618556976\n",
      "Epoch 594, Loss: 0.3502519130706787, Final Batch Loss: 0.1600494384765625\n",
      "Epoch 595, Loss: 0.28338514268398285, Final Batch Loss: 0.1279229074716568\n",
      "Epoch 596, Loss: 0.295321561396122, Final Batch Loss: 0.12189475446939468\n",
      "Epoch 597, Loss: 0.25957922637462616, Final Batch Loss: 0.058908432722091675\n",
      "Epoch 598, Loss: 0.28076374530792236, Final Batch Loss: 0.10863921046257019\n",
      "Epoch 599, Loss: 0.27096208930015564, Final Batch Loss: 0.09845905005931854\n",
      "Epoch 600, Loss: 0.23141776025295258, Final Batch Loss: 0.08323748409748077\n",
      "Epoch 601, Loss: 0.27690761536359787, Final Batch Loss: 0.09657921642065048\n",
      "Epoch 602, Loss: 0.29330316185951233, Final Batch Loss: 0.17577533423900604\n",
      "Epoch 603, Loss: 0.302851177752018, Final Batch Loss: 0.18146608769893646\n",
      "Epoch 604, Loss: 0.37290576100349426, Final Batch Loss: 0.2131960391998291\n",
      "Epoch 605, Loss: 0.2028696984052658, Final Batch Loss: 0.07050937414169312\n",
      "Epoch 606, Loss: 0.2365005612373352, Final Batch Loss: 0.0922907143831253\n",
      "Epoch 607, Loss: 0.2590316832065582, Final Batch Loss: 0.1253693848848343\n",
      "Epoch 608, Loss: 0.2506033182144165, Final Batch Loss: 0.08693718910217285\n",
      "Epoch 609, Loss: 0.2696303576231003, Final Batch Loss: 0.1588996797800064\n",
      "Epoch 610, Loss: 0.24871912598609924, Final Batch Loss: 0.11382296681404114\n",
      "Epoch 611, Loss: 0.29934311658143997, Final Batch Loss: 0.17605265974998474\n",
      "Epoch 612, Loss: 0.39728449285030365, Final Batch Loss: 0.23081034421920776\n",
      "Epoch 613, Loss: 0.2782876715064049, Final Batch Loss: 0.09909350425004959\n",
      "Epoch 614, Loss: 0.2466355264186859, Final Batch Loss: 0.09867554903030396\n",
      "Epoch 615, Loss: 0.31544229388237, Final Batch Loss: 0.17914368212223053\n",
      "Epoch 616, Loss: 0.3114861100912094, Final Batch Loss: 0.1877354085445404\n",
      "Epoch 617, Loss: 0.21834391355514526, Final Batch Loss: 0.07616838812828064\n",
      "Epoch 618, Loss: 0.22154692560434341, Final Batch Loss: 0.09136874228715897\n",
      "Epoch 619, Loss: 0.28059303760528564, Final Batch Loss: 0.11608770489692688\n",
      "Epoch 620, Loss: 0.19538405537605286, Final Batch Loss: 0.0798105001449585\n",
      "Epoch 621, Loss: 0.27031177282333374, Final Batch Loss: 0.13779672980308533\n",
      "Epoch 622, Loss: 0.3233325630426407, Final Batch Loss: 0.1744392216205597\n",
      "Epoch 623, Loss: 0.3401332050561905, Final Batch Loss: 0.18852797150611877\n",
      "Epoch 624, Loss: 0.3251779079437256, Final Batch Loss: 0.16765442490577698\n",
      "Epoch 625, Loss: 0.2848231643438339, Final Batch Loss: 0.14328446984291077\n",
      "Epoch 626, Loss: 0.3343706503510475, Final Batch Loss: 0.22007325291633606\n",
      "Epoch 627, Loss: 0.18796460330486298, Final Batch Loss: 0.06960223615169525\n",
      "Epoch 628, Loss: 0.3259813189506531, Final Batch Loss: 0.14687837660312653\n",
      "Epoch 629, Loss: 0.3806741386651993, Final Batch Loss: 0.1754244714975357\n",
      "Epoch 630, Loss: 0.29509224742650986, Final Batch Loss: 0.12316075712442398\n",
      "Epoch 631, Loss: 0.393316313624382, Final Batch Loss: 0.18773502111434937\n",
      "Epoch 632, Loss: 0.34500089287757874, Final Batch Loss: 0.20832045376300812\n",
      "Epoch 633, Loss: 0.35099466890096664, Final Batch Loss: 0.23671270906925201\n",
      "Epoch 634, Loss: 0.26932793855667114, Final Batch Loss: 0.14118067920207977\n",
      "Epoch 635, Loss: 0.17683014273643494, Final Batch Loss: 0.07315272092819214\n",
      "Epoch 636, Loss: 0.26612234115600586, Final Batch Loss: 0.14604172110557556\n",
      "Epoch 637, Loss: 0.20537316799163818, Final Batch Loss: 0.06521564722061157\n",
      "Epoch 638, Loss: 0.2136334702372551, Final Batch Loss: 0.10148517787456512\n",
      "Epoch 639, Loss: 0.33475489169359207, Final Batch Loss: 0.21421031653881073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 640, Loss: 0.20859381556510925, Final Batch Loss: 0.10338141024112701\n",
      "Epoch 641, Loss: 0.2676571160554886, Final Batch Loss: 0.1403389573097229\n",
      "Epoch 642, Loss: 0.27594366669654846, Final Batch Loss: 0.1087736040353775\n",
      "Epoch 643, Loss: 0.2414136826992035, Final Batch Loss: 0.09178832173347473\n",
      "Epoch 644, Loss: 0.3079337328672409, Final Batch Loss: 0.15620538592338562\n",
      "Epoch 645, Loss: 0.35570093989372253, Final Batch Loss: 0.17960073053836823\n",
      "Epoch 646, Loss: 0.25811659544706345, Final Batch Loss: 0.1227821633219719\n",
      "Epoch 647, Loss: 0.2038239985704422, Final Batch Loss: 0.1040237620472908\n",
      "Epoch 648, Loss: 0.2467198148369789, Final Batch Loss: 0.08727993816137314\n",
      "Epoch 649, Loss: 0.2661527991294861, Final Batch Loss: 0.15256986021995544\n",
      "Epoch 650, Loss: 0.2561757490038872, Final Batch Loss: 0.08403297513723373\n",
      "Epoch 651, Loss: 0.29489126801490784, Final Batch Loss: 0.1310819387435913\n",
      "Epoch 652, Loss: 0.4444889426231384, Final Batch Loss: 0.31499606370925903\n",
      "Epoch 653, Loss: 0.21856501698493958, Final Batch Loss: 0.11625252664089203\n",
      "Epoch 654, Loss: 0.30098551511764526, Final Batch Loss: 0.15390034019947052\n",
      "Epoch 655, Loss: 0.26081253588199615, Final Batch Loss: 0.1437257081270218\n",
      "Epoch 656, Loss: 0.22883488982915878, Final Batch Loss: 0.1379852592945099\n",
      "Epoch 657, Loss: 0.3704990893602371, Final Batch Loss: 0.1936928629875183\n",
      "Epoch 658, Loss: 0.37899501621723175, Final Batch Loss: 0.2082168459892273\n",
      "Epoch 659, Loss: 0.2530232146382332, Final Batch Loss: 0.0883040651679039\n",
      "Epoch 660, Loss: 0.18195659667253494, Final Batch Loss: 0.06900756061077118\n",
      "Epoch 661, Loss: 0.23246575891971588, Final Batch Loss: 0.08958779275417328\n",
      "Epoch 662, Loss: 0.2807459682226181, Final Batch Loss: 0.13680410385131836\n",
      "Epoch 663, Loss: 0.289359912276268, Final Batch Loss: 0.15443533658981323\n",
      "Epoch 664, Loss: 0.2486802339553833, Final Batch Loss: 0.07897007465362549\n",
      "Epoch 665, Loss: 0.3245978057384491, Final Batch Loss: 0.14776748418807983\n",
      "Epoch 666, Loss: 0.21489263325929642, Final Batch Loss: 0.11159062385559082\n",
      "Epoch 667, Loss: 0.19694280624389648, Final Batch Loss: 0.09341151267290115\n",
      "Epoch 668, Loss: 0.2588346302509308, Final Batch Loss: 0.13931681215763092\n",
      "Epoch 669, Loss: 0.20940541476011276, Final Batch Loss: 0.08881833404302597\n",
      "Epoch 670, Loss: 0.2655053287744522, Final Batch Loss: 0.14914779365062714\n",
      "Epoch 671, Loss: 0.16607468202710152, Final Batch Loss: 0.05527682974934578\n",
      "Epoch 672, Loss: 0.19943448901176453, Final Batch Loss: 0.12552958726882935\n",
      "Epoch 673, Loss: 0.16266798228025436, Final Batch Loss: 0.058913685381412506\n",
      "Epoch 674, Loss: 0.26915356516838074, Final Batch Loss: 0.13988801836967468\n",
      "Epoch 675, Loss: 0.17295295000076294, Final Batch Loss: 0.0689687430858612\n",
      "Epoch 676, Loss: 0.3749000132083893, Final Batch Loss: 0.24343112111091614\n",
      "Epoch 677, Loss: 0.17768021672964096, Final Batch Loss: 0.08124076575040817\n",
      "Epoch 678, Loss: 0.22020862251520157, Final Batch Loss: 0.12706083059310913\n",
      "Epoch 679, Loss: 0.2397710680961609, Final Batch Loss: 0.11485537886619568\n",
      "Epoch 680, Loss: 0.3508456349372864, Final Batch Loss: 0.20081095397472382\n",
      "Epoch 681, Loss: 0.19237060844898224, Final Batch Loss: 0.06990884989500046\n",
      "Epoch 682, Loss: 0.22856535762548447, Final Batch Loss: 0.09220235794782639\n",
      "Epoch 683, Loss: 0.2363203465938568, Final Batch Loss: 0.14632683992385864\n",
      "Epoch 684, Loss: 0.20585310459136963, Final Batch Loss: 0.06829126179218292\n",
      "Epoch 685, Loss: 0.2226366400718689, Final Batch Loss: 0.1036597490310669\n",
      "Epoch 686, Loss: 0.25997789204120636, Final Batch Loss: 0.14335259795188904\n",
      "Epoch 687, Loss: 0.27151191234588623, Final Batch Loss: 0.12323400378227234\n",
      "Epoch 688, Loss: 0.25843141973018646, Final Batch Loss: 0.13335435092449188\n",
      "Epoch 689, Loss: 0.3655702620744705, Final Batch Loss: 0.2327052652835846\n",
      "Epoch 690, Loss: 0.32184015959501266, Final Batch Loss: 0.20429609715938568\n",
      "Epoch 691, Loss: 0.16734402626752853, Final Batch Loss: 0.07947105914354324\n",
      "Epoch 692, Loss: 0.17466123402118683, Final Batch Loss: 0.0755251944065094\n",
      "Epoch 693, Loss: 0.24664773792028427, Final Batch Loss: 0.15616580843925476\n",
      "Epoch 694, Loss: 0.22294669598340988, Final Batch Loss: 0.11680882424116135\n",
      "Epoch 695, Loss: 0.21973755210638046, Final Batch Loss: 0.09661965817213058\n",
      "Epoch 696, Loss: 0.3438701331615448, Final Batch Loss: 0.13889293372631073\n",
      "Epoch 697, Loss: 0.19311323016881943, Final Batch Loss: 0.06673253327608109\n",
      "Epoch 698, Loss: 0.245230533182621, Final Batch Loss: 0.08986140042543411\n",
      "Epoch 699, Loss: 0.1965658888220787, Final Batch Loss: 0.10756421089172363\n",
      "Epoch 700, Loss: 0.2827688902616501, Final Batch Loss: 0.11657539010047913\n",
      "Epoch 701, Loss: 0.2531098574399948, Final Batch Loss: 0.134837806224823\n",
      "Epoch 702, Loss: 0.2097029760479927, Final Batch Loss: 0.09014403074979782\n",
      "Epoch 703, Loss: 0.24365362524986267, Final Batch Loss: 0.13862626254558563\n",
      "Epoch 704, Loss: 0.30517200380563736, Final Batch Loss: 0.18776270747184753\n",
      "Epoch 705, Loss: 0.19429555535316467, Final Batch Loss: 0.07324758172035217\n",
      "Epoch 706, Loss: 0.2108154296875, Final Batch Loss: 0.07892484962940216\n",
      "Epoch 707, Loss: 0.20219162851572037, Final Batch Loss: 0.09292997419834137\n",
      "Epoch 708, Loss: 0.24590810388326645, Final Batch Loss: 0.11229177564382553\n",
      "Epoch 709, Loss: 0.20901019871234894, Final Batch Loss: 0.10287804901599884\n",
      "Epoch 710, Loss: 0.3085160031914711, Final Batch Loss: 0.22098100185394287\n",
      "Epoch 711, Loss: 0.15815524756908417, Final Batch Loss: 0.04822251945734024\n",
      "Epoch 712, Loss: 0.16271086782217026, Final Batch Loss: 0.06965029239654541\n",
      "Epoch 713, Loss: 0.19092213362455368, Final Batch Loss: 0.06998270004987717\n",
      "Epoch 714, Loss: 0.20846080034971237, Final Batch Loss: 0.059937380254268646\n",
      "Epoch 715, Loss: 0.2811676263809204, Final Batch Loss: 0.1308470070362091\n",
      "Epoch 716, Loss: 0.2577548995614052, Final Batch Loss: 0.13936147093772888\n",
      "Epoch 717, Loss: 0.21020524203777313, Final Batch Loss: 0.07395762205123901\n",
      "Epoch 718, Loss: 0.27877284586429596, Final Batch Loss: 0.1504083275794983\n",
      "Epoch 719, Loss: 0.19725927710533142, Final Batch Loss: 0.06974150240421295\n",
      "Epoch 720, Loss: 0.2741445451974869, Final Batch Loss: 0.16525861620903015\n",
      "Epoch 721, Loss: 0.2997794598340988, Final Batch Loss: 0.14851976931095123\n",
      "Epoch 722, Loss: 0.3024284392595291, Final Batch Loss: 0.20806589722633362\n",
      "Epoch 723, Loss: 0.2283250167965889, Final Batch Loss: 0.12320364266633987\n",
      "Epoch 724, Loss: 0.20898982882499695, Final Batch Loss: 0.08770906925201416\n",
      "Epoch 725, Loss: 0.1687362901866436, Final Batch Loss: 0.054845403879880905\n",
      "Epoch 726, Loss: 0.2360062077641487, Final Batch Loss: 0.10820323973894119\n",
      "Epoch 727, Loss: 0.24626946449279785, Final Batch Loss: 0.1131819635629654\n",
      "Epoch 728, Loss: 0.21355152875185013, Final Batch Loss: 0.07527682930231094\n",
      "Epoch 729, Loss: 0.19649457186460495, Final Batch Loss: 0.07269629091024399\n",
      "Epoch 730, Loss: 0.2870003581047058, Final Batch Loss: 0.14884832501411438\n",
      "Epoch 731, Loss: 0.26338524371385574, Final Batch Loss: 0.13920223712921143\n",
      "Epoch 732, Loss: 0.1814395636320114, Final Batch Loss: 0.0650576576590538\n",
      "Epoch 733, Loss: 0.2392505183815956, Final Batch Loss: 0.07042161375284195\n",
      "Epoch 734, Loss: 0.13523363322019577, Final Batch Loss: 0.06761755049228668\n",
      "Epoch 735, Loss: 0.23445994406938553, Final Batch Loss: 0.09492594748735428\n",
      "Epoch 736, Loss: 0.23922806233167648, Final Batch Loss: 0.12647469341754913\n",
      "Epoch 737, Loss: 0.15362928062677383, Final Batch Loss: 0.06559444218873978\n",
      "Epoch 738, Loss: 0.28467969596385956, Final Batch Loss: 0.1473865509033203\n",
      "Epoch 739, Loss: 0.24894366413354874, Final Batch Loss: 0.12979568541049957\n",
      "Epoch 740, Loss: 0.1633804440498352, Final Batch Loss: 0.04974258691072464\n",
      "Epoch 741, Loss: 0.19378486275672913, Final Batch Loss: 0.085980124771595\n",
      "Epoch 742, Loss: 0.27867450565099716, Final Batch Loss: 0.0888417586684227\n",
      "Epoch 743, Loss: 0.2948634773492813, Final Batch Loss: 0.18356087803840637\n",
      "Epoch 744, Loss: 0.21513216942548752, Final Batch Loss: 0.05711094290018082\n",
      "Epoch 745, Loss: 0.22463437914848328, Final Batch Loss: 0.07522551715373993\n",
      "Epoch 746, Loss: 0.24356818199157715, Final Batch Loss: 0.14066116511821747\n",
      "Epoch 747, Loss: 0.1433832049369812, Final Batch Loss: 0.06462538987398148\n",
      "Epoch 748, Loss: 0.22161205112934113, Final Batch Loss: 0.09859858453273773\n",
      "Epoch 749, Loss: 0.27733711153268814, Final Batch Loss: 0.173909530043602\n",
      "Epoch 750, Loss: 0.19764182716608047, Final Batch Loss: 0.10509009659290314\n",
      "Epoch 751, Loss: 0.2557920888066292, Final Batch Loss: 0.1495589166879654\n",
      "Epoch 752, Loss: 0.20040879398584366, Final Batch Loss: 0.11013975739479065\n",
      "Epoch 753, Loss: 0.19992810487747192, Final Batch Loss: 0.0921950489282608\n",
      "Epoch 754, Loss: 0.1607341654598713, Final Batch Loss: 0.05805325135588646\n",
      "Epoch 755, Loss: 0.19031164050102234, Final Batch Loss: 0.0835663303732872\n",
      "Epoch 756, Loss: 0.2777097523212433, Final Batch Loss: 0.15738773345947266\n",
      "Epoch 757, Loss: 0.23674417287111282, Final Batch Loss: 0.08559077233076096\n",
      "Epoch 758, Loss: 0.22461820393800735, Final Batch Loss: 0.08314516395330429\n",
      "Epoch 759, Loss: 0.1947544813156128, Final Batch Loss: 0.09898066520690918\n",
      "Epoch 760, Loss: 0.2739821672439575, Final Batch Loss: 0.2010028064250946\n",
      "Epoch 761, Loss: 0.30938267707824707, Final Batch Loss: 0.21207919716835022\n",
      "Epoch 762, Loss: 0.1771431639790535, Final Batch Loss: 0.09483068436384201\n",
      "Epoch 763, Loss: 0.23893901705741882, Final Batch Loss: 0.10317939519882202\n",
      "Epoch 764, Loss: 0.20061885938048363, Final Batch Loss: 0.05184270069003105\n",
      "Epoch 765, Loss: 0.21845649182796478, Final Batch Loss: 0.1091553196310997\n",
      "Epoch 766, Loss: 0.1878311149775982, Final Batch Loss: 0.056867193430662155\n",
      "Epoch 767, Loss: 0.279667004942894, Final Batch Loss: 0.1577853262424469\n",
      "Epoch 768, Loss: 0.192982017993927, Final Batch Loss: 0.06159600615501404\n",
      "Epoch 769, Loss: 0.15829865634441376, Final Batch Loss: 0.06755860149860382\n",
      "Epoch 770, Loss: 0.2545628845691681, Final Batch Loss: 0.17056778073310852\n",
      "Epoch 771, Loss: 0.19030095636844635, Final Batch Loss: 0.09126535058021545\n",
      "Epoch 772, Loss: 0.3193424716591835, Final Batch Loss: 0.20956920087337494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 773, Loss: 0.17570561170578003, Final Batch Loss: 0.09990726411342621\n",
      "Epoch 774, Loss: 0.177428737282753, Final Batch Loss: 0.06791278719902039\n",
      "Epoch 775, Loss: 0.16123676300048828, Final Batch Loss: 0.05426134169101715\n",
      "Epoch 776, Loss: 0.1549474596977234, Final Batch Loss: 0.044301822781562805\n",
      "Epoch 777, Loss: 0.1927216276526451, Final Batch Loss: 0.07145590335130692\n",
      "Epoch 778, Loss: 0.22186267375946045, Final Batch Loss: 0.06869138777256012\n",
      "Epoch 779, Loss: 0.1688704937696457, Final Batch Loss: 0.07691092789173126\n",
      "Epoch 780, Loss: 0.22923000901937485, Final Batch Loss: 0.12602563202381134\n",
      "Epoch 781, Loss: 0.2832295000553131, Final Batch Loss: 0.15037555992603302\n",
      "Epoch 782, Loss: 0.2623576521873474, Final Batch Loss: 0.15331609547138214\n",
      "Epoch 783, Loss: 0.33169180154800415, Final Batch Loss: 0.18020190298557281\n",
      "Epoch 784, Loss: 0.1504429578781128, Final Batch Loss: 0.06342784315347672\n",
      "Epoch 785, Loss: 0.24156683683395386, Final Batch Loss: 0.11619612574577332\n",
      "Epoch 786, Loss: 0.16720987111330032, Final Batch Loss: 0.08024130016565323\n",
      "Epoch 787, Loss: 0.18633953481912613, Final Batch Loss: 0.06782890111207962\n",
      "Epoch 788, Loss: 0.17798887938261032, Final Batch Loss: 0.09441586583852768\n",
      "Epoch 789, Loss: 0.2706974670290947, Final Batch Loss: 0.17737066745758057\n",
      "Epoch 790, Loss: 0.2531971335411072, Final Batch Loss: 0.16093628108501434\n",
      "Epoch 791, Loss: 0.16153639554977417, Final Batch Loss: 0.08756481856107712\n",
      "Epoch 792, Loss: 0.1864474192261696, Final Batch Loss: 0.06404738128185272\n",
      "Epoch 793, Loss: 0.20504015684127808, Final Batch Loss: 0.10452531278133392\n",
      "Epoch 794, Loss: 0.1741286665201187, Final Batch Loss: 0.07877026498317719\n",
      "Epoch 795, Loss: 0.30974214524030685, Final Batch Loss: 0.24176768958568573\n",
      "Epoch 796, Loss: 0.17907451838254929, Final Batch Loss: 0.08339812606573105\n",
      "Epoch 797, Loss: 0.2979387938976288, Final Batch Loss: 0.16575348377227783\n",
      "Epoch 798, Loss: 0.19592714309692383, Final Batch Loss: 0.09447821229696274\n",
      "Epoch 799, Loss: 0.228369802236557, Final Batch Loss: 0.1246376782655716\n",
      "Epoch 800, Loss: 0.13260399922728539, Final Batch Loss: 0.0445660762488842\n",
      "Epoch 801, Loss: 0.23742453753948212, Final Batch Loss: 0.0904223769903183\n",
      "Epoch 802, Loss: 0.20327165722846985, Final Batch Loss: 0.09180627763271332\n",
      "Epoch 803, Loss: 0.1753753274679184, Final Batch Loss: 0.08438992500305176\n",
      "Epoch 804, Loss: 0.15857264772057533, Final Batch Loss: 0.04639849439263344\n",
      "Epoch 805, Loss: 0.22466419637203217, Final Batch Loss: 0.11846654117107391\n",
      "Epoch 806, Loss: 0.19317081570625305, Final Batch Loss: 0.10889171063899994\n",
      "Epoch 807, Loss: 0.20332198590040207, Final Batch Loss: 0.08258859813213348\n",
      "Epoch 808, Loss: 0.16623372584581375, Final Batch Loss: 0.06488719582557678\n",
      "Epoch 809, Loss: 0.19486362487077713, Final Batch Loss: 0.1136775016784668\n",
      "Epoch 810, Loss: 0.2656114250421524, Final Batch Loss: 0.1460040658712387\n",
      "Epoch 811, Loss: 0.18197575956583023, Final Batch Loss: 0.08371710777282715\n",
      "Epoch 812, Loss: 0.19380253553390503, Final Batch Loss: 0.11957619339227676\n",
      "Epoch 813, Loss: 0.1457594707608223, Final Batch Loss: 0.05121905356645584\n",
      "Epoch 814, Loss: 0.1513827219605446, Final Batch Loss: 0.08711869269609451\n",
      "Epoch 815, Loss: 0.18671289831399918, Final Batch Loss: 0.08645347505807877\n",
      "Epoch 816, Loss: 0.1777307540178299, Final Batch Loss: 0.08174484968185425\n",
      "Epoch 817, Loss: 0.15903542190790176, Final Batch Loss: 0.06993741542100906\n",
      "Epoch 818, Loss: 0.16390951722860336, Final Batch Loss: 0.057289279997348785\n",
      "Epoch 819, Loss: 0.12716364488005638, Final Batch Loss: 0.056624699383974075\n",
      "Epoch 820, Loss: 0.11988826841115952, Final Batch Loss: 0.04044056683778763\n",
      "Epoch 821, Loss: 0.14850756898522377, Final Batch Loss: 0.05679132416844368\n",
      "Epoch 822, Loss: 0.19369813799858093, Final Batch Loss: 0.0937003344297409\n",
      "Epoch 823, Loss: 0.21337199956178665, Final Batch Loss: 0.13385069370269775\n",
      "Epoch 824, Loss: 0.1225782223045826, Final Batch Loss: 0.048416104167699814\n",
      "Epoch 825, Loss: 0.21733995527029037, Final Batch Loss: 0.1474532037973404\n",
      "Epoch 826, Loss: 0.24197711050510406, Final Batch Loss: 0.12432315200567245\n",
      "Epoch 827, Loss: 0.22317899018526077, Final Batch Loss: 0.14293615520000458\n",
      "Epoch 828, Loss: 0.18246711045503616, Final Batch Loss: 0.11192033439874649\n",
      "Epoch 829, Loss: 0.23950422555208206, Final Batch Loss: 0.14042158424854279\n",
      "Epoch 830, Loss: 0.2110566347837448, Final Batch Loss: 0.13852035999298096\n",
      "Epoch 831, Loss: 0.1438332051038742, Final Batch Loss: 0.06353854387998581\n",
      "Epoch 832, Loss: 0.14596884697675705, Final Batch Loss: 0.05549490451812744\n",
      "Epoch 833, Loss: 0.14959640055894852, Final Batch Loss: 0.07532309740781784\n",
      "Epoch 834, Loss: 0.19518927484750748, Final Batch Loss: 0.10526318848133087\n",
      "Epoch 835, Loss: 0.2854152321815491, Final Batch Loss: 0.15426425635814667\n",
      "Epoch 836, Loss: 0.30491962283849716, Final Batch Loss: 0.23870886862277985\n",
      "Epoch 837, Loss: 0.21467335522174835, Final Batch Loss: 0.1336701512336731\n",
      "Epoch 838, Loss: 0.2772115468978882, Final Batch Loss: 0.14229167997837067\n",
      "Epoch 839, Loss: 0.17651541717350483, Final Batch Loss: 0.021022336557507515\n",
      "Epoch 840, Loss: 0.1607387475669384, Final Batch Loss: 0.05780348554253578\n",
      "Epoch 841, Loss: 0.18955596536397934, Final Batch Loss: 0.10776026546955109\n",
      "Epoch 842, Loss: 0.17072319984436035, Final Batch Loss: 0.04116334021091461\n",
      "Epoch 843, Loss: 0.09401792287826538, Final Batch Loss: 0.03296874836087227\n",
      "Epoch 844, Loss: 0.14418576657772064, Final Batch Loss: 0.050355926156044006\n",
      "Epoch 845, Loss: 0.275784008204937, Final Batch Loss: 0.1765042394399643\n",
      "Epoch 846, Loss: 0.3070875480771065, Final Batch Loss: 0.20938944816589355\n",
      "Epoch 847, Loss: 0.20075702667236328, Final Batch Loss: 0.13638271391391754\n",
      "Epoch 848, Loss: 0.1721882000565529, Final Batch Loss: 0.0652703270316124\n",
      "Epoch 849, Loss: 0.13535455614328384, Final Batch Loss: 0.06526707112789154\n",
      "Epoch 850, Loss: 0.17197692021727562, Final Batch Loss: 0.11090889573097229\n",
      "Epoch 851, Loss: 0.13805002719163895, Final Batch Loss: 0.07224306464195251\n",
      "Epoch 852, Loss: 0.1302652843296528, Final Batch Loss: 0.0347968153655529\n",
      "Epoch 853, Loss: 0.14965622872114182, Final Batch Loss: 0.06005196273326874\n",
      "Epoch 854, Loss: 0.18959008157253265, Final Batch Loss: 0.08431627601385117\n",
      "Epoch 855, Loss: 0.20843176543712616, Final Batch Loss: 0.12532955408096313\n",
      "Epoch 856, Loss: 0.21146968007087708, Final Batch Loss: 0.14744848012924194\n",
      "Epoch 857, Loss: 0.14350994303822517, Final Batch Loss: 0.04763391986489296\n",
      "Epoch 858, Loss: 0.15135494619607925, Final Batch Loss: 0.07711499184370041\n",
      "Epoch 859, Loss: 0.15428093820810318, Final Batch Loss: 0.07225184142589569\n",
      "Epoch 860, Loss: 0.22851145267486572, Final Batch Loss: 0.13978049159049988\n",
      "Epoch 861, Loss: 0.20605583488941193, Final Batch Loss: 0.06540969014167786\n",
      "Epoch 862, Loss: 0.22559887170791626, Final Batch Loss: 0.11878965049982071\n",
      "Epoch 863, Loss: 0.20057819038629532, Final Batch Loss: 0.11262480914592743\n",
      "Epoch 864, Loss: 0.16576532274484634, Final Batch Loss: 0.09411539882421494\n",
      "Epoch 865, Loss: 0.15290024876594543, Final Batch Loss: 0.06384366750717163\n",
      "Epoch 866, Loss: 0.19032353162765503, Final Batch Loss: 0.0752386525273323\n",
      "Epoch 867, Loss: 0.15717387944459915, Final Batch Loss: 0.039270445704460144\n",
      "Epoch 868, Loss: 0.1554139368236065, Final Batch Loss: 0.10488869249820709\n",
      "Epoch 869, Loss: 0.18456636369228363, Final Batch Loss: 0.11138055473566055\n",
      "Epoch 870, Loss: 0.20444706082344055, Final Batch Loss: 0.08226072043180466\n",
      "Epoch 871, Loss: 0.25810756534338, Final Batch Loss: 0.19144518673419952\n",
      "Epoch 872, Loss: 0.09601902961730957, Final Batch Loss: 0.027427546679973602\n",
      "Epoch 873, Loss: 0.23560215532779694, Final Batch Loss: 0.10415942966938019\n",
      "Epoch 874, Loss: 0.25444022566080093, Final Batch Loss: 0.16191937029361725\n",
      "Epoch 875, Loss: 0.17328854650259018, Final Batch Loss: 0.09682607650756836\n",
      "Epoch 876, Loss: 0.10875901579856873, Final Batch Loss: 0.04949042201042175\n",
      "Epoch 877, Loss: 0.13831383734941483, Final Batch Loss: 0.07306379824876785\n",
      "Epoch 878, Loss: 0.1755150929093361, Final Batch Loss: 0.08266474306583405\n",
      "Epoch 879, Loss: 0.26972224563360214, Final Batch Loss: 0.1679188460111618\n",
      "Epoch 880, Loss: 0.21615272760391235, Final Batch Loss: 0.07483652234077454\n",
      "Epoch 881, Loss: 0.14440738037228584, Final Batch Loss: 0.05037229135632515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 882, Loss: 0.2062511146068573, Final Batch Loss: 0.10240110754966736\n",
      "Epoch 883, Loss: 0.1545076221227646, Final Batch Loss: 0.07091309130191803\n",
      "Epoch 884, Loss: 0.1470370590686798, Final Batch Loss: 0.08512519299983978\n",
      "Epoch 885, Loss: 0.1322932317852974, Final Batch Loss: 0.056009672582149506\n",
      "Epoch 886, Loss: 0.30822376161813736, Final Batch Loss: 0.1932825893163681\n",
      "Epoch 887, Loss: 0.16366641223430634, Final Batch Loss: 0.08255826681852341\n",
      "Epoch 888, Loss: 0.17664088308811188, Final Batch Loss: 0.10624120384454727\n",
      "Epoch 889, Loss: 0.1721876934170723, Final Batch Loss: 0.09750968217849731\n",
      "Epoch 890, Loss: 0.17070680856704712, Final Batch Loss: 0.07806763052940369\n",
      "Epoch 891, Loss: 0.16570544987916946, Final Batch Loss: 0.10952113568782806\n",
      "Epoch 892, Loss: 0.17627087980508804, Final Batch Loss: 0.10637596249580383\n",
      "Epoch 893, Loss: 0.09962610900402069, Final Batch Loss: 0.04187415912747383\n",
      "Epoch 894, Loss: 0.12261999025940895, Final Batch Loss: 0.04579422250390053\n",
      "Epoch 895, Loss: 0.1982673704624176, Final Batch Loss: 0.10823486000299454\n",
      "Epoch 896, Loss: 0.1952316164970398, Final Batch Loss: 0.10607557743787766\n",
      "Epoch 897, Loss: 0.11989500001072884, Final Batch Loss: 0.04642263427376747\n",
      "Epoch 898, Loss: 0.14571644738316536, Final Batch Loss: 0.06239113584160805\n",
      "Epoch 899, Loss: 0.21420396119356155, Final Batch Loss: 0.09561547636985779\n",
      "Epoch 900, Loss: 0.14219820871949196, Final Batch Loss: 0.06149517372250557\n",
      "Epoch 901, Loss: 0.2489393875002861, Final Batch Loss: 0.15169912576675415\n",
      "Epoch 902, Loss: 0.15028711408376694, Final Batch Loss: 0.06137135624885559\n",
      "Epoch 903, Loss: 0.2105412781238556, Final Batch Loss: 0.05875280499458313\n",
      "Epoch 904, Loss: 0.1349359005689621, Final Batch Loss: 0.048608534038066864\n",
      "Epoch 905, Loss: 0.12673981860280037, Final Batch Loss: 0.04890831187367439\n",
      "Epoch 906, Loss: 0.13790494576096535, Final Batch Loss: 0.039883244782686234\n",
      "Epoch 907, Loss: 0.1301416978240013, Final Batch Loss: 0.04675648361444473\n",
      "Epoch 908, Loss: 0.15647851675748825, Final Batch Loss: 0.0690358504652977\n",
      "Epoch 909, Loss: 0.2527032271027565, Final Batch Loss: 0.1753225028514862\n",
      "Epoch 910, Loss: 0.12610578536987305, Final Batch Loss: 0.03555019944906235\n",
      "Epoch 911, Loss: 0.28178319334983826, Final Batch Loss: 0.17402507364749908\n",
      "Epoch 912, Loss: 0.19504695013165474, Final Batch Loss: 0.05364126339554787\n",
      "Epoch 913, Loss: 0.2753365933895111, Final Batch Loss: 0.17839589715003967\n",
      "Epoch 914, Loss: 0.1340374331921339, Final Batch Loss: 0.029906323179602623\n",
      "Epoch 915, Loss: 0.16047923266887665, Final Batch Loss: 0.0757947713136673\n",
      "Epoch 916, Loss: 0.1748456284403801, Final Batch Loss: 0.08309394866228104\n",
      "Epoch 917, Loss: 0.17807116359472275, Final Batch Loss: 0.09360767900943756\n",
      "Epoch 918, Loss: 0.15686427429318428, Final Batch Loss: 0.051359739154577255\n",
      "Epoch 919, Loss: 0.1406852938234806, Final Batch Loss: 0.09338395297527313\n",
      "Epoch 920, Loss: 0.1693039834499359, Final Batch Loss: 0.048532478511333466\n",
      "Epoch 921, Loss: 0.10428274795413017, Final Batch Loss: 0.04548829793930054\n",
      "Epoch 922, Loss: 0.2955147922039032, Final Batch Loss: 0.15703773498535156\n",
      "Epoch 923, Loss: 0.10009025409817696, Final Batch Loss: 0.05539216846227646\n",
      "Epoch 924, Loss: 0.22100239247083664, Final Batch Loss: 0.12715649604797363\n",
      "Epoch 925, Loss: 0.14602046459913254, Final Batch Loss: 0.05189534276723862\n",
      "Epoch 926, Loss: 0.12507593259215355, Final Batch Loss: 0.046014223247766495\n",
      "Epoch 927, Loss: 0.1511971428990364, Final Batch Loss: 0.058096930384635925\n",
      "Epoch 928, Loss: 0.3250338286161423, Final Batch Loss: 0.22508621215820312\n",
      "Epoch 929, Loss: 0.14552213996648788, Final Batch Loss: 0.0618039071559906\n",
      "Epoch 930, Loss: 0.13701793551445007, Final Batch Loss: 0.07189245522022247\n",
      "Epoch 931, Loss: 0.12148815765976906, Final Batch Loss: 0.03061254695057869\n",
      "Epoch 932, Loss: 0.10293004103004932, Final Batch Loss: 0.02504378743469715\n",
      "Epoch 933, Loss: 0.1548592448234558, Final Batch Loss: 0.0779518187046051\n",
      "Epoch 934, Loss: 0.17497392743825912, Final Batch Loss: 0.09707604348659515\n",
      "Epoch 935, Loss: 0.22091467678546906, Final Batch Loss: 0.12441571801900864\n",
      "Epoch 936, Loss: 0.15276360884308815, Final Batch Loss: 0.09683223068714142\n",
      "Epoch 937, Loss: 0.13921544328331947, Final Batch Loss: 0.05641692504286766\n",
      "Epoch 938, Loss: 0.1801724210381508, Final Batch Loss: 0.06616542488336563\n",
      "Epoch 939, Loss: 0.17519710212945938, Final Batch Loss: 0.11612585186958313\n",
      "Epoch 940, Loss: 0.14512281119823456, Final Batch Loss: 0.08067362755537033\n",
      "Epoch 941, Loss: 0.1173620093613863, Final Batch Loss: 0.023588674142956734\n",
      "Epoch 942, Loss: 0.12244774401187897, Final Batch Loss: 0.020236246287822723\n",
      "Epoch 943, Loss: 0.14276858419179916, Final Batch Loss: 0.07667946070432663\n",
      "Epoch 944, Loss: 0.22733556479215622, Final Batch Loss: 0.1572677493095398\n",
      "Epoch 945, Loss: 0.14755848050117493, Final Batch Loss: 0.0780109167098999\n",
      "Epoch 946, Loss: 0.16626764088869095, Final Batch Loss: 0.08923448622226715\n",
      "Epoch 947, Loss: 0.18026921898126602, Final Batch Loss: 0.06627999246120453\n",
      "Epoch 948, Loss: 0.1983252391219139, Final Batch Loss: 0.09218120574951172\n",
      "Epoch 949, Loss: 0.14992573112249374, Final Batch Loss: 0.08483533561229706\n",
      "Epoch 950, Loss: 0.24797427654266357, Final Batch Loss: 0.1543947011232376\n",
      "Epoch 951, Loss: 0.1512930616736412, Final Batch Loss: 0.05134253203868866\n",
      "Epoch 952, Loss: 0.15786544233560562, Final Batch Loss: 0.06116113066673279\n",
      "Epoch 953, Loss: 0.26907119899988174, Final Batch Loss: 0.15077339112758636\n",
      "Epoch 954, Loss: 0.20918013900518417, Final Batch Loss: 0.1056189313530922\n",
      "Epoch 955, Loss: 0.11513958126306534, Final Batch Loss: 0.057141657918691635\n",
      "Epoch 956, Loss: 0.12262864038348198, Final Batch Loss: 0.06402802467346191\n",
      "Epoch 957, Loss: 0.18415910750627518, Final Batch Loss: 0.10375714302062988\n",
      "Epoch 958, Loss: 0.1680227816104889, Final Batch Loss: 0.07987753301858902\n",
      "Epoch 959, Loss: 0.18539204075932503, Final Batch Loss: 0.03609441593289375\n",
      "Epoch 960, Loss: 0.15545152127742767, Final Batch Loss: 0.08030857890844345\n",
      "Epoch 961, Loss: 0.1201411746442318, Final Batch Loss: 0.037771452218294144\n",
      "Epoch 962, Loss: 0.1673397719860077, Final Batch Loss: 0.083599753677845\n",
      "Epoch 963, Loss: 0.22711946070194244, Final Batch Loss: 0.104505255818367\n",
      "Epoch 964, Loss: 0.16893179714679718, Final Batch Loss: 0.1035023182630539\n",
      "Epoch 965, Loss: 0.1805081143975258, Final Batch Loss: 0.09347157180309296\n",
      "Epoch 966, Loss: 0.17530514299869537, Final Batch Loss: 0.06658871471881866\n",
      "Epoch 967, Loss: 0.09839073941111565, Final Batch Loss: 0.04221891611814499\n",
      "Epoch 968, Loss: 0.07896050810813904, Final Batch Loss: 0.03780989721417427\n",
      "Epoch 969, Loss: 0.129265908151865, Final Batch Loss: 0.07683586329221725\n",
      "Epoch 970, Loss: 0.12094957754015923, Final Batch Loss: 0.03641282394528389\n",
      "Epoch 971, Loss: 0.13573754206299782, Final Batch Loss: 0.08115320652723312\n",
      "Epoch 972, Loss: 0.10756922140717506, Final Batch Loss: 0.04120911285281181\n",
      "Epoch 973, Loss: 0.13201092183589935, Final Batch Loss: 0.04772428423166275\n",
      "Epoch 974, Loss: 0.19254257529973984, Final Batch Loss: 0.0782991275191307\n",
      "Epoch 975, Loss: 0.1524595096707344, Final Batch Loss: 0.08135814219713211\n",
      "Epoch 976, Loss: 0.280256949365139, Final Batch Loss: 0.20002728700637817\n",
      "Epoch 977, Loss: 0.18123681098222733, Final Batch Loss: 0.09840766340494156\n",
      "Epoch 978, Loss: 0.1994534283876419, Final Batch Loss: 0.060922056436538696\n",
      "Epoch 979, Loss: 0.20774786919355392, Final Batch Loss: 0.12291045486927032\n",
      "Epoch 980, Loss: 0.20263244956731796, Final Batch Loss: 0.13033427298069\n",
      "Epoch 981, Loss: 0.37631209939718246, Final Batch Loss: 0.30664941668510437\n",
      "Epoch 982, Loss: 0.2007138952612877, Final Batch Loss: 0.1335669457912445\n",
      "Epoch 983, Loss: 0.16223270818591118, Final Batch Loss: 0.06129278615117073\n",
      "Epoch 984, Loss: 0.11797574162483215, Final Batch Loss: 0.0711778774857521\n",
      "Epoch 985, Loss: 0.2039358913898468, Final Batch Loss: 0.09927991032600403\n",
      "Epoch 986, Loss: 0.10122351720929146, Final Batch Loss: 0.05273305997252464\n",
      "Epoch 987, Loss: 0.09363003075122833, Final Batch Loss: 0.0426497720181942\n",
      "Epoch 988, Loss: 0.22219853848218918, Final Batch Loss: 0.11355577409267426\n",
      "Epoch 989, Loss: 0.14772452041506767, Final Batch Loss: 0.09002738445997238\n",
      "Epoch 990, Loss: 0.16134516149759293, Final Batch Loss: 0.05755561590194702\n",
      "Epoch 991, Loss: 0.14433232322335243, Final Batch Loss: 0.06202242895960808\n",
      "Epoch 992, Loss: 0.08937482722103596, Final Batch Loss: 0.02681806869804859\n",
      "Epoch 993, Loss: 0.2305397093296051, Final Batch Loss: 0.09956970810890198\n",
      "Epoch 994, Loss: 0.13174304738640785, Final Batch Loss: 0.08359245955944061\n",
      "Epoch 995, Loss: 0.1504124142229557, Final Batch Loss: 0.05615132674574852\n",
      "Epoch 996, Loss: 0.12501896172761917, Final Batch Loss: 0.09084288030862808\n",
      "Epoch 997, Loss: 0.1382271647453308, Final Batch Loss: 0.057222552597522736\n",
      "Epoch 998, Loss: 0.14154405146837234, Final Batch Loss: 0.07831650972366333\n",
      "Epoch 999, Loss: 0.1314646303653717, Final Batch Loss: 0.05940389633178711\n",
      "Epoch 1000, Loss: 0.19670488685369492, Final Batch Loss: 0.11959699541330338\n",
      "Epoch 1001, Loss: 0.21485291421413422, Final Batch Loss: 0.08028802275657654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1002, Loss: 0.10465088486671448, Final Batch Loss: 0.04926241561770439\n",
      "Epoch 1003, Loss: 0.13616535440087318, Final Batch Loss: 0.0514027439057827\n",
      "Epoch 1004, Loss: 0.13666803389787674, Final Batch Loss: 0.022007465362548828\n",
      "Epoch 1005, Loss: 0.12347995489835739, Final Batch Loss: 0.059324197471141815\n",
      "Epoch 1006, Loss: 0.24135415256023407, Final Batch Loss: 0.15260079503059387\n",
      "Epoch 1007, Loss: 0.1442856676876545, Final Batch Loss: 0.04321196302771568\n",
      "Epoch 1008, Loss: 0.2164890021085739, Final Batch Loss: 0.13789057731628418\n",
      "Epoch 1009, Loss: 0.13629288598895073, Final Batch Loss: 0.040536653250455856\n",
      "Epoch 1010, Loss: 0.16243235021829605, Final Batch Loss: 0.09453330188989639\n",
      "Epoch 1011, Loss: 0.19671640545129776, Final Batch Loss: 0.10519567877054214\n",
      "Epoch 1012, Loss: 0.14929887652397156, Final Batch Loss: 0.07869302481412888\n",
      "Epoch 1013, Loss: 0.1323057785630226, Final Batch Loss: 0.03910687565803528\n",
      "Epoch 1014, Loss: 0.11316338554024696, Final Batch Loss: 0.030228715389966965\n",
      "Epoch 1015, Loss: 0.21142807602882385, Final Batch Loss: 0.14312945306301117\n",
      "Epoch 1016, Loss: 0.09551512449979782, Final Batch Loss: 0.050185441970825195\n",
      "Epoch 1017, Loss: 0.15128640085458755, Final Batch Loss: 0.09023183584213257\n",
      "Epoch 1018, Loss: 0.11577220633625984, Final Batch Loss: 0.053861018270254135\n",
      "Epoch 1019, Loss: 0.14123506844043732, Final Batch Loss: 0.06415994465351105\n",
      "Epoch 1020, Loss: 0.14506105333566666, Final Batch Loss: 0.07716202735900879\n",
      "Epoch 1021, Loss: 0.14712602272629738, Final Batch Loss: 0.057579923421144485\n",
      "Epoch 1022, Loss: 0.1580425128340721, Final Batch Loss: 0.07723638415336609\n",
      "Epoch 1023, Loss: 0.13715249300003052, Final Batch Loss: 0.05544070154428482\n",
      "Epoch 1024, Loss: 0.19536179304122925, Final Batch Loss: 0.07259956747293472\n",
      "Epoch 1025, Loss: 0.22086834907531738, Final Batch Loss: 0.1294679194688797\n",
      "Epoch 1026, Loss: 0.22338150441646576, Final Batch Loss: 0.13237160444259644\n",
      "Epoch 1027, Loss: 0.2720552310347557, Final Batch Loss: 0.1873333603143692\n",
      "Epoch 1028, Loss: 0.2437015324831009, Final Batch Loss: 0.1086784303188324\n",
      "Epoch 1029, Loss: 0.18181224167346954, Final Batch Loss: 0.1066986620426178\n",
      "Epoch 1030, Loss: 0.11994863674044609, Final Batch Loss: 0.04595993086695671\n",
      "Epoch 1031, Loss: 0.24851784110069275, Final Batch Loss: 0.14306935667991638\n",
      "Epoch 1032, Loss: 0.04787870030850172, Final Batch Loss: 0.014509442262351513\n",
      "Epoch 1033, Loss: 0.21988797932863235, Final Batch Loss: 0.13562318682670593\n",
      "Epoch 1034, Loss: 0.16451822966337204, Final Batch Loss: 0.08332695066928864\n",
      "Epoch 1035, Loss: 0.16658678650856018, Final Batch Loss: 0.07466937601566315\n",
      "Epoch 1036, Loss: 0.11924820020794868, Final Batch Loss: 0.06765656173229218\n",
      "Epoch 1037, Loss: 0.09677073359489441, Final Batch Loss: 0.03149016946554184\n",
      "Epoch 1038, Loss: 0.12720200791954994, Final Batch Loss: 0.03997049108147621\n",
      "Epoch 1039, Loss: 0.19063035398721695, Final Batch Loss: 0.092033751308918\n",
      "Epoch 1040, Loss: 0.09902176260948181, Final Batch Loss: 0.032336845993995667\n",
      "Epoch 1041, Loss: 0.08980906382203102, Final Batch Loss: 0.03318188712000847\n",
      "Epoch 1042, Loss: 0.10208522900938988, Final Batch Loss: 0.03865426406264305\n",
      "Epoch 1043, Loss: 0.17830055207014084, Final Batch Loss: 0.060670822858810425\n",
      "Epoch 1044, Loss: 0.09141172841191292, Final Batch Loss: 0.043054454028606415\n",
      "Epoch 1045, Loss: 0.17542710155248642, Final Batch Loss: 0.08240219950675964\n",
      "Epoch 1046, Loss: 0.1766602098941803, Final Batch Loss: 0.07851898670196533\n",
      "Epoch 1047, Loss: 0.145461518317461, Final Batch Loss: 0.043555308133363724\n",
      "Epoch 1048, Loss: 0.1348307654261589, Final Batch Loss: 0.0429084450006485\n",
      "Epoch 1049, Loss: 0.18229398131370544, Final Batch Loss: 0.09345367550849915\n",
      "Epoch 1050, Loss: 0.1920158639550209, Final Batch Loss: 0.11064470559358597\n",
      "Epoch 1051, Loss: 0.15840024128556252, Final Batch Loss: 0.10005757957696915\n",
      "Epoch 1052, Loss: 0.14663642272353172, Final Batch Loss: 0.08757179975509644\n",
      "Epoch 1053, Loss: 0.09286309406161308, Final Batch Loss: 0.04615873470902443\n",
      "Epoch 1054, Loss: 0.1679818406701088, Final Batch Loss: 0.09971974790096283\n",
      "Epoch 1055, Loss: 0.12523381412029266, Final Batch Loss: 0.08248359709978104\n",
      "Epoch 1056, Loss: 0.1139342449605465, Final Batch Loss: 0.04661430045962334\n",
      "Epoch 1057, Loss: 0.22636806219816208, Final Batch Loss: 0.07911861687898636\n",
      "Epoch 1058, Loss: 0.11460445821285248, Final Batch Loss: 0.03715748339891434\n",
      "Epoch 1059, Loss: 0.16799665242433548, Final Batch Loss: 0.10456419736146927\n",
      "Epoch 1060, Loss: 0.17166787013411522, Final Batch Loss: 0.12958183884620667\n",
      "Epoch 1061, Loss: 0.25112029910087585, Final Batch Loss: 0.1563095897436142\n",
      "Epoch 1062, Loss: 0.10868274793028831, Final Batch Loss: 0.026356901973485947\n",
      "Epoch 1063, Loss: 0.15717535838484764, Final Batch Loss: 0.0600881390273571\n",
      "Epoch 1064, Loss: 0.1450657993555069, Final Batch Loss: 0.044046856462955475\n",
      "Epoch 1065, Loss: 0.08586602658033371, Final Batch Loss: 0.03858364373445511\n",
      "Epoch 1066, Loss: 0.09950786083936691, Final Batch Loss: 0.04146566614508629\n",
      "Epoch 1067, Loss: 0.10086524486541748, Final Batch Loss: 0.029842793941497803\n",
      "Epoch 1068, Loss: 0.14631395787000656, Final Batch Loss: 0.07178628444671631\n",
      "Epoch 1069, Loss: 0.2162274718284607, Final Batch Loss: 0.14646016061306\n",
      "Epoch 1070, Loss: 0.2177443578839302, Final Batch Loss: 0.11232608556747437\n",
      "Epoch 1071, Loss: 0.13505885750055313, Final Batch Loss: 0.04016558825969696\n",
      "Epoch 1072, Loss: 0.16695775091648102, Final Batch Loss: 0.050299882888793945\n",
      "Epoch 1073, Loss: 0.0824924185872078, Final Batch Loss: 0.029070012271404266\n",
      "Epoch 1074, Loss: 0.12712006643414497, Final Batch Loss: 0.041371699422597885\n",
      "Epoch 1075, Loss: 0.14032237976789474, Final Batch Loss: 0.06306317448616028\n",
      "Epoch 1076, Loss: 0.13660643249750137, Final Batch Loss: 0.0756213441491127\n",
      "Epoch 1077, Loss: 0.1330309808254242, Final Batch Loss: 0.0590021088719368\n",
      "Epoch 1078, Loss: 0.19756781309843063, Final Batch Loss: 0.13325126469135284\n",
      "Epoch 1079, Loss: 0.08701157011091709, Final Batch Loss: 0.026554888114333153\n",
      "Epoch 1080, Loss: 0.24514349922537804, Final Batch Loss: 0.1891760230064392\n",
      "Epoch 1081, Loss: 0.1472860425710678, Final Batch Loss: 0.06879185140132904\n",
      "Epoch 1082, Loss: 0.17755821347236633, Final Batch Loss: 0.11071870476007462\n",
      "Epoch 1083, Loss: 0.1700824797153473, Final Batch Loss: 0.10104634612798691\n",
      "Epoch 1084, Loss: 0.18411125987768173, Final Batch Loss: 0.10207103192806244\n",
      "Epoch 1085, Loss: 0.16812869161367416, Final Batch Loss: 0.10346843302249908\n",
      "Epoch 1086, Loss: 0.2072344794869423, Final Batch Loss: 0.09505701065063477\n",
      "Epoch 1087, Loss: 0.13051867857575417, Final Batch Loss: 0.03318638727068901\n",
      "Epoch 1088, Loss: 0.15315383672714233, Final Batch Loss: 0.08728577941656113\n",
      "Epoch 1089, Loss: 0.24773867800831795, Final Batch Loss: 0.18544527888298035\n",
      "Epoch 1090, Loss: 0.1988450586795807, Final Batch Loss: 0.1267918348312378\n",
      "Epoch 1091, Loss: 0.12818995490670204, Final Batch Loss: 0.0746573954820633\n",
      "Epoch 1092, Loss: 0.10849537327885628, Final Batch Loss: 0.02488059177994728\n",
      "Epoch 1093, Loss: 0.31156328320503235, Final Batch Loss: 0.22066263854503632\n",
      "Epoch 1094, Loss: 0.22384195774793625, Final Batch Loss: 0.14203056693077087\n",
      "Epoch 1095, Loss: 0.18606506288051605, Final Batch Loss: 0.09357281774282455\n",
      "Epoch 1096, Loss: 0.11352834105491638, Final Batch Loss: 0.04240301251411438\n",
      "Epoch 1097, Loss: 0.10643661022186279, Final Batch Loss: 0.03376081585884094\n",
      "Epoch 1098, Loss: 0.164790790528059, Final Batch Loss: 0.11621171236038208\n",
      "Epoch 1099, Loss: 0.19925975799560547, Final Batch Loss: 0.12563937902450562\n",
      "Epoch 1100, Loss: 0.10810221545398235, Final Batch Loss: 0.028794964775443077\n",
      "Epoch 1101, Loss: 0.09607866033911705, Final Batch Loss: 0.05393121391534805\n",
      "Epoch 1102, Loss: 0.15245738625526428, Final Batch Loss: 0.08535146713256836\n",
      "Epoch 1103, Loss: 0.1345805786550045, Final Batch Loss: 0.04254812374711037\n",
      "Epoch 1104, Loss: 0.11297319829463959, Final Batch Loss: 0.06226775050163269\n",
      "Epoch 1105, Loss: 0.11046798154711723, Final Batch Loss: 0.052918870002031326\n",
      "Epoch 1106, Loss: 0.1982136219739914, Final Batch Loss: 0.11247587203979492\n",
      "Epoch 1107, Loss: 0.13784199208021164, Final Batch Loss: 0.05345805734395981\n",
      "Epoch 1108, Loss: 0.13065748289227486, Final Batch Loss: 0.06865345686674118\n",
      "Epoch 1109, Loss: 0.15409664809703827, Final Batch Loss: 0.0777934193611145\n",
      "Epoch 1110, Loss: 0.17938482016324997, Final Batch Loss: 0.09310793876647949\n",
      "Epoch 1111, Loss: 0.24510595202445984, Final Batch Loss: 0.16858530044555664\n",
      "Epoch 1112, Loss: 0.11082667484879494, Final Batch Loss: 0.042728032916784286\n",
      "Epoch 1113, Loss: 0.08839906007051468, Final Batch Loss: 0.03830166161060333\n",
      "Epoch 1114, Loss: 0.14060800522565842, Final Batch Loss: 0.07474150508642197\n",
      "Epoch 1115, Loss: 0.12796691805124283, Final Batch Loss: 0.04605584591627121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1116, Loss: 0.16022568941116333, Final Batch Loss: 0.08331561088562012\n",
      "Epoch 1117, Loss: 0.229901522397995, Final Batch Loss: 0.15135793387889862\n",
      "Epoch 1118, Loss: 0.19772010296583176, Final Batch Loss: 0.10670221596956253\n",
      "Epoch 1119, Loss: 0.12712528184056282, Final Batch Loss: 0.07430243492126465\n",
      "Epoch 1120, Loss: 0.14083464443683624, Final Batch Loss: 0.07487719506025314\n",
      "Epoch 1121, Loss: 0.21314337104558945, Final Batch Loss: 0.14423148334026337\n",
      "Epoch 1122, Loss: 0.16731761395931244, Final Batch Loss: 0.10591785609722137\n",
      "Epoch 1123, Loss: 0.16063664108514786, Final Batch Loss: 0.07676396518945694\n",
      "Epoch 1124, Loss: 0.16015543788671494, Final Batch Loss: 0.06778031587600708\n",
      "Epoch 1125, Loss: 0.11779031157493591, Final Batch Loss: 0.04538694769144058\n",
      "Epoch 1126, Loss: 0.10483760759234428, Final Batch Loss: 0.03565603122115135\n",
      "Epoch 1127, Loss: 0.14165493845939636, Final Batch Loss: 0.06299752742052078\n",
      "Epoch 1128, Loss: 0.0979829840362072, Final Batch Loss: 0.053217049688100815\n",
      "Epoch 1129, Loss: 0.16194915771484375, Final Batch Loss: 0.10097120702266693\n",
      "Epoch 1130, Loss: 0.15651990473270416, Final Batch Loss: 0.11338311433792114\n",
      "Epoch 1131, Loss: 0.29302117973566055, Final Batch Loss: 0.22670629620552063\n",
      "Epoch 1132, Loss: 0.34852592647075653, Final Batch Loss: 0.2591015696525574\n",
      "Epoch 1133, Loss: 0.10031243786215782, Final Batch Loss: 0.03499821200966835\n",
      "Epoch 1134, Loss: 0.12687591463327408, Final Batch Loss: 0.05578991770744324\n",
      "Epoch 1135, Loss: 0.1827375441789627, Final Batch Loss: 0.11379456520080566\n",
      "Epoch 1136, Loss: 0.12913678586483002, Final Batch Loss: 0.05046458542346954\n",
      "Epoch 1137, Loss: 0.100981580093503, Final Batch Loss: 0.021805087104439735\n",
      "Epoch 1138, Loss: 0.131979800760746, Final Batch Loss: 0.05312547832727432\n",
      "Epoch 1139, Loss: 0.2831731587648392, Final Batch Loss: 0.19489145278930664\n",
      "Epoch 1140, Loss: 0.1084575243294239, Final Batch Loss: 0.03805888071656227\n",
      "Epoch 1141, Loss: 0.09254002943634987, Final Batch Loss: 0.04590177163481712\n",
      "Epoch 1142, Loss: 0.17480650916695595, Final Batch Loss: 0.04648855701088905\n",
      "Epoch 1143, Loss: 0.09421001002192497, Final Batch Loss: 0.0294005386531353\n",
      "Epoch 1144, Loss: 0.09912467002868652, Final Batch Loss: 0.03526921570301056\n",
      "Epoch 1145, Loss: 0.1526821069419384, Final Batch Loss: 0.0313592366874218\n",
      "Epoch 1146, Loss: 0.15311450883746147, Final Batch Loss: 0.10124600678682327\n",
      "Epoch 1147, Loss: 0.08840126544237137, Final Batch Loss: 0.022208303213119507\n",
      "Epoch 1148, Loss: 0.08920367434620857, Final Batch Loss: 0.04316040500998497\n",
      "Epoch 1149, Loss: 0.150739386677742, Final Batch Loss: 0.055338792502880096\n",
      "Epoch 1150, Loss: 0.10522733256220818, Final Batch Loss: 0.035388145595788956\n",
      "Epoch 1151, Loss: 0.14533748850226402, Final Batch Loss: 0.08336898684501648\n",
      "Epoch 1152, Loss: 0.18925368785858154, Final Batch Loss: 0.09024396538734436\n",
      "Epoch 1153, Loss: 0.11271632462739944, Final Batch Loss: 0.04123938828706741\n",
      "Epoch 1154, Loss: 0.10245629772543907, Final Batch Loss: 0.02501695230603218\n",
      "Epoch 1155, Loss: 0.16326647251844406, Final Batch Loss: 0.09788476675748825\n",
      "Epoch 1156, Loss: 0.1364578902721405, Final Batch Loss: 0.07903258502483368\n",
      "Epoch 1157, Loss: 0.08695361576974392, Final Batch Loss: 0.02376898191869259\n",
      "Epoch 1158, Loss: 0.1337823085486889, Final Batch Loss: 0.061488401144742966\n",
      "Epoch 1159, Loss: 0.1437431424856186, Final Batch Loss: 0.03892417252063751\n",
      "Epoch 1160, Loss: 0.10050415247678757, Final Batch Loss: 0.03669114410877228\n",
      "Epoch 1161, Loss: 0.08247395232319832, Final Batch Loss: 0.02729462832212448\n",
      "Epoch 1162, Loss: 0.08873477578163147, Final Batch Loss: 0.05483662337064743\n",
      "Epoch 1163, Loss: 0.12219151481986046, Final Batch Loss: 0.050743479281663895\n",
      "Epoch 1164, Loss: 0.19691859558224678, Final Batch Loss: 0.14881113171577454\n",
      "Epoch 1165, Loss: 0.2160644456744194, Final Batch Loss: 0.1205165758728981\n",
      "Epoch 1166, Loss: 0.16574446111917496, Final Batch Loss: 0.07093531638383865\n",
      "Epoch 1167, Loss: 0.20051704347133636, Final Batch Loss: 0.09029948711395264\n",
      "Epoch 1168, Loss: 0.12027366831898689, Final Batch Loss: 0.030193466693162918\n",
      "Epoch 1169, Loss: 0.17024662345647812, Final Batch Loss: 0.07292519509792328\n",
      "Epoch 1170, Loss: 0.16046220809221268, Final Batch Loss: 0.09761318564414978\n",
      "Epoch 1171, Loss: 0.17839112505316734, Final Batch Loss: 0.11892785131931305\n",
      "Epoch 1172, Loss: 0.13217344135046005, Final Batch Loss: 0.06134112924337387\n",
      "Epoch 1173, Loss: 0.10620901733636856, Final Batch Loss: 0.03938353806734085\n",
      "Epoch 1174, Loss: 0.10830901190638542, Final Batch Loss: 0.0405791811645031\n",
      "Epoch 1175, Loss: 0.1412760466337204, Final Batch Loss: 0.07467307895421982\n",
      "Epoch 1176, Loss: 0.08875810354948044, Final Batch Loss: 0.030900530517101288\n",
      "Epoch 1177, Loss: 0.12555304542183876, Final Batch Loss: 0.026592735201120377\n",
      "Epoch 1178, Loss: 0.2168687991797924, Final Batch Loss: 0.1566345989704132\n",
      "Epoch 1179, Loss: 0.13471218943595886, Final Batch Loss: 0.08666030317544937\n",
      "Epoch 1180, Loss: 0.10082388669252396, Final Batch Loss: 0.045691996812820435\n",
      "Epoch 1181, Loss: 0.10377565398812294, Final Batch Loss: 0.035852063447237015\n",
      "Epoch 1182, Loss: 0.1810174286365509, Final Batch Loss: 0.1104590892791748\n",
      "Epoch 1183, Loss: 0.14883311465382576, Final Batch Loss: 0.04677748307585716\n",
      "Epoch 1184, Loss: 0.1674993932247162, Final Batch Loss: 0.10072185099124908\n",
      "Epoch 1185, Loss: 0.1021396741271019, Final Batch Loss: 0.017821982502937317\n",
      "Epoch 1186, Loss: 0.12993507459759712, Final Batch Loss: 0.08706841617822647\n",
      "Epoch 1187, Loss: 0.16634876653552055, Final Batch Loss: 0.04627129063010216\n",
      "Epoch 1188, Loss: 0.10246670618653297, Final Batch Loss: 0.044772956520318985\n",
      "Epoch 1189, Loss: 0.14005591347813606, Final Batch Loss: 0.0786285400390625\n",
      "Epoch 1190, Loss: 0.12137517891824245, Final Batch Loss: 0.026878012344241142\n",
      "Epoch 1191, Loss: 0.08891081809997559, Final Batch Loss: 0.042917054146528244\n",
      "Epoch 1192, Loss: 0.06537892855703831, Final Batch Loss: 0.01707630045711994\n",
      "Epoch 1193, Loss: 0.13830631226301193, Final Batch Loss: 0.0691068023443222\n",
      "Epoch 1194, Loss: 0.1387876346707344, Final Batch Loss: 0.06468389928340912\n",
      "Epoch 1195, Loss: 0.097864530980587, Final Batch Loss: 0.05729902163147926\n",
      "Epoch 1196, Loss: 0.12821153923869133, Final Batch Loss: 0.032269950956106186\n",
      "Epoch 1197, Loss: 0.09851037710905075, Final Batch Loss: 0.05082035809755325\n",
      "Epoch 1198, Loss: 0.19038639217615128, Final Batch Loss: 0.11500111222267151\n",
      "Epoch 1199, Loss: 0.16854511946439743, Final Batch Loss: 0.08585189282894135\n",
      "Epoch 1200, Loss: 0.15003203600645065, Final Batch Loss: 0.07480190694332123\n",
      "Epoch 1201, Loss: 0.13758030533790588, Final Batch Loss: 0.07294183224439621\n",
      "Epoch 1202, Loss: 0.11310641467571259, Final Batch Loss: 0.03811880946159363\n",
      "Epoch 1203, Loss: 0.13221966847777367, Final Batch Loss: 0.0869055688381195\n",
      "Epoch 1204, Loss: 0.26973506063222885, Final Batch Loss: 0.19042064249515533\n",
      "Epoch 1205, Loss: 0.14035823941230774, Final Batch Loss: 0.0773409977555275\n",
      "Epoch 1206, Loss: 0.15329640358686447, Final Batch Loss: 0.09799317270517349\n",
      "Epoch 1207, Loss: 0.19795383512973785, Final Batch Loss: 0.11916254460811615\n",
      "Epoch 1208, Loss: 0.1573786959052086, Final Batch Loss: 0.07095563411712646\n",
      "Epoch 1209, Loss: 0.17425405234098434, Final Batch Loss: 0.07459726929664612\n",
      "Epoch 1210, Loss: 0.18382113426923752, Final Batch Loss: 0.07211321592330933\n",
      "Epoch 1211, Loss: 0.17041438072919846, Final Batch Loss: 0.06497728079557419\n",
      "Epoch 1212, Loss: 0.12554071843624115, Final Batch Loss: 0.05931457132101059\n",
      "Epoch 1213, Loss: 0.0699998326599598, Final Batch Loss: 0.01675134152173996\n",
      "Epoch 1214, Loss: 0.09643346071243286, Final Batch Loss: 0.04715370014309883\n",
      "Epoch 1215, Loss: 0.12189007923007011, Final Batch Loss: 0.0751257985830307\n",
      "Epoch 1216, Loss: 0.10457427240908146, Final Batch Loss: 0.0268468726426363\n",
      "Epoch 1217, Loss: 0.15487457066774368, Final Batch Loss: 0.07558172196149826\n",
      "Epoch 1218, Loss: 0.11684863269329071, Final Batch Loss: 0.06048211827874184\n",
      "Epoch 1219, Loss: 0.18861858919262886, Final Batch Loss: 0.12948444485664368\n",
      "Epoch 1220, Loss: 0.10993039608001709, Final Batch Loss: 0.018612802028656006\n",
      "Epoch 1221, Loss: 0.1441493257880211, Final Batch Loss: 0.06880778819322586\n",
      "Epoch 1222, Loss: 0.1080368272960186, Final Batch Loss: 0.06252312660217285\n",
      "Epoch 1223, Loss: 0.16164354979991913, Final Batch Loss: 0.09407534450292587\n",
      "Epoch 1224, Loss: 0.20621896535158157, Final Batch Loss: 0.12486092746257782\n",
      "Epoch 1225, Loss: 0.14677908644080162, Final Batch Loss: 0.050939079374074936\n",
      "Epoch 1226, Loss: 0.17120066285133362, Final Batch Loss: 0.10381101071834564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1227, Loss: 0.18942491337656975, Final Batch Loss: 0.14300771057605743\n",
      "Epoch 1228, Loss: 0.09753756783902645, Final Batch Loss: 0.02833486907184124\n",
      "Epoch 1229, Loss: 0.12473003566265106, Final Batch Loss: 0.04089205712080002\n",
      "Epoch 1230, Loss: 0.09993590600788593, Final Batch Loss: 0.02275211550295353\n",
      "Epoch 1231, Loss: 0.09381992369890213, Final Batch Loss: 0.03859121352434158\n",
      "Epoch 1232, Loss: 0.13927823677659035, Final Batch Loss: 0.07788447290658951\n",
      "Epoch 1233, Loss: 0.22801091149449348, Final Batch Loss: 0.1728442758321762\n",
      "Epoch 1234, Loss: 0.10900060459971428, Final Batch Loss: 0.07047057151794434\n",
      "Epoch 1235, Loss: 0.17849521338939667, Final Batch Loss: 0.07946471869945526\n",
      "Epoch 1236, Loss: 0.0948520228266716, Final Batch Loss: 0.03180377185344696\n",
      "Epoch 1237, Loss: 0.15450117737054825, Final Batch Loss: 0.07073580473661423\n",
      "Epoch 1238, Loss: 0.13749991357326508, Final Batch Loss: 0.06982270628213882\n",
      "Epoch 1239, Loss: 0.12762032821774483, Final Batch Loss: 0.07508523017168045\n",
      "Epoch 1240, Loss: 0.10521064698696136, Final Batch Loss: 0.04905726760625839\n",
      "Epoch 1241, Loss: 0.11744274944067001, Final Batch Loss: 0.05804602429270744\n",
      "Epoch 1242, Loss: 0.15493834018707275, Final Batch Loss: 0.08846430480480194\n",
      "Epoch 1243, Loss: 0.13115253299474716, Final Batch Loss: 0.07026725262403488\n",
      "Epoch 1244, Loss: 0.07935765013098717, Final Batch Loss: 0.036155957728624344\n",
      "Epoch 1245, Loss: 0.11771348491311073, Final Batch Loss: 0.045298729091882706\n",
      "Epoch 1246, Loss: 0.17298974841833115, Final Batch Loss: 0.108843594789505\n",
      "Epoch 1247, Loss: 0.14391768723726273, Final Batch Loss: 0.09070330113172531\n",
      "Epoch 1248, Loss: 0.13170424848794937, Final Batch Loss: 0.04779842495918274\n",
      "Epoch 1249, Loss: 0.18022358417510986, Final Batch Loss: 0.09105072170495987\n",
      "Epoch 1250, Loss: 0.12404435314238071, Final Batch Loss: 0.02609063871204853\n",
      "Epoch 1251, Loss: 0.10663824900984764, Final Batch Loss: 0.04445968195796013\n",
      "Epoch 1252, Loss: 0.18139738589525223, Final Batch Loss: 0.11923487484455109\n",
      "Epoch 1253, Loss: 0.09946294873952866, Final Batch Loss: 0.032263725996017456\n",
      "Epoch 1254, Loss: 0.10455045849084854, Final Batch Loss: 0.06941523402929306\n",
      "Epoch 1255, Loss: 0.09118795394897461, Final Batch Loss: 0.04903251305222511\n",
      "Epoch 1256, Loss: 0.12574205920100212, Final Batch Loss: 0.05778271332383156\n",
      "Epoch 1257, Loss: 0.1389397196471691, Final Batch Loss: 0.057654786854982376\n",
      "Epoch 1258, Loss: 0.1264648549258709, Final Batch Loss: 0.052736084908246994\n",
      "Epoch 1259, Loss: 0.08776064962148666, Final Batch Loss: 0.036252640187740326\n",
      "Epoch 1260, Loss: 0.0975976400077343, Final Batch Loss: 0.056288979947566986\n",
      "Epoch 1261, Loss: 0.15362638235092163, Final Batch Loss: 0.08120888471603394\n",
      "Epoch 1262, Loss: 0.07861343212425709, Final Batch Loss: 0.02284061349928379\n",
      "Epoch 1263, Loss: 0.17737478017807007, Final Batch Loss: 0.12475849688053131\n",
      "Epoch 1264, Loss: 0.13823004812002182, Final Batch Loss: 0.0718538910150528\n",
      "Epoch 1265, Loss: 0.0671228519640863, Final Batch Loss: 0.007366751786321402\n",
      "Epoch 1266, Loss: 0.3476554751396179, Final Batch Loss: 0.2668165862560272\n",
      "Epoch 1267, Loss: 0.08965669013559818, Final Batch Loss: 0.06436801701784134\n",
      "Epoch 1268, Loss: 0.05394202284514904, Final Batch Loss: 0.025750653818249702\n",
      "Epoch 1269, Loss: 0.0943331141024828, Final Batch Loss: 0.023290472105145454\n",
      "Epoch 1270, Loss: 0.0843917727470398, Final Batch Loss: 0.014927178621292114\n",
      "Epoch 1271, Loss: 0.14556246995925903, Final Batch Loss: 0.06746459007263184\n",
      "Epoch 1272, Loss: 0.11901119723916054, Final Batch Loss: 0.04251774773001671\n",
      "Epoch 1273, Loss: 0.19337911903858185, Final Batch Loss: 0.12628264725208282\n",
      "Epoch 1274, Loss: 0.1394975483417511, Final Batch Loss: 0.04256651550531387\n",
      "Epoch 1275, Loss: 0.14378032088279724, Final Batch Loss: 0.06971199065446854\n",
      "Epoch 1276, Loss: 0.1516810953617096, Final Batch Loss: 0.08333911001682281\n",
      "Epoch 1277, Loss: 0.07162616029381752, Final Batch Loss: 0.022544022649526596\n",
      "Epoch 1278, Loss: 0.13025984168052673, Final Batch Loss: 0.0719708651304245\n",
      "Epoch 1279, Loss: 0.10270994156599045, Final Batch Loss: 0.054635901004076004\n",
      "Epoch 1280, Loss: 0.1312563605606556, Final Batch Loss: 0.07276129722595215\n",
      "Epoch 1281, Loss: 0.15830279886722565, Final Batch Loss: 0.027400851249694824\n",
      "Epoch 1282, Loss: 0.11458790674805641, Final Batch Loss: 0.053475528955459595\n",
      "Epoch 1283, Loss: 0.1598239690065384, Final Batch Loss: 0.07192057371139526\n",
      "Epoch 1284, Loss: 0.13751914724707603, Final Batch Loss: 0.087581567466259\n",
      "Epoch 1285, Loss: 0.1598154678940773, Final Batch Loss: 0.07263615727424622\n",
      "Epoch 1286, Loss: 0.10179362818598747, Final Batch Loss: 0.041548121720552444\n",
      "Epoch 1287, Loss: 0.08332093618810177, Final Batch Loss: 0.028780193999409676\n",
      "Epoch 1288, Loss: 0.10387761890888214, Final Batch Loss: 0.05370485782623291\n",
      "Epoch 1289, Loss: 0.09419063106179237, Final Batch Loss: 0.02839212492108345\n",
      "Epoch 1290, Loss: 0.07931703701615334, Final Batch Loss: 0.0473816841840744\n",
      "Epoch 1291, Loss: 0.08719933778047562, Final Batch Loss: 0.039455167949199677\n",
      "Epoch 1292, Loss: 0.11531677842140198, Final Batch Loss: 0.057387787848711014\n",
      "Epoch 1293, Loss: 0.14049945026636124, Final Batch Loss: 0.05764588713645935\n",
      "Epoch 1294, Loss: 0.11225586757063866, Final Batch Loss: 0.04192685708403587\n",
      "Epoch 1295, Loss: 0.09737731143832207, Final Batch Loss: 0.05046036094427109\n",
      "Epoch 1296, Loss: 0.219131488353014, Final Batch Loss: 0.16990268230438232\n",
      "Epoch 1297, Loss: 0.08857756480574608, Final Batch Loss: 0.04349138215184212\n",
      "Epoch 1298, Loss: 0.1199556291103363, Final Batch Loss: 0.04322192817926407\n",
      "Epoch 1299, Loss: 0.16368437558412552, Final Batch Loss: 0.12091471999883652\n",
      "Epoch 1300, Loss: 0.11798664182424545, Final Batch Loss: 0.07134375721216202\n",
      "Epoch 1301, Loss: 0.15097368508577347, Final Batch Loss: 0.08229383826255798\n",
      "Epoch 1302, Loss: 0.21775739639997482, Final Batch Loss: 0.11714780330657959\n",
      "Epoch 1303, Loss: 0.15919847786426544, Final Batch Loss: 0.08478820323944092\n",
      "Epoch 1304, Loss: 0.11479423940181732, Final Batch Loss: 0.02250000834465027\n",
      "Epoch 1305, Loss: 0.1014135554432869, Final Batch Loss: 0.04247173294425011\n",
      "Epoch 1306, Loss: 0.20043861120939255, Final Batch Loss: 0.11107257753610611\n",
      "Epoch 1307, Loss: 0.2030365765094757, Final Batch Loss: 0.09814469516277313\n",
      "Epoch 1308, Loss: 0.07620731554925442, Final Batch Loss: 0.024847375229001045\n",
      "Epoch 1309, Loss: 0.11329075321555138, Final Batch Loss: 0.04131745919585228\n",
      "Epoch 1310, Loss: 0.09116871654987335, Final Batch Loss: 0.048217806965112686\n",
      "Epoch 1311, Loss: 0.2294989600777626, Final Batch Loss: 0.16840782761573792\n",
      "Epoch 1312, Loss: 0.12127496302127838, Final Batch Loss: 0.04703262448310852\n",
      "Epoch 1313, Loss: 0.13568173721432686, Final Batch Loss: 0.05417640134692192\n",
      "Epoch 1314, Loss: 0.10000836104154587, Final Batch Loss: 0.04106144234538078\n",
      "Epoch 1315, Loss: 0.15575817972421646, Final Batch Loss: 0.08717405050992966\n",
      "Epoch 1316, Loss: 0.12981900572776794, Final Batch Loss: 0.06216231733560562\n",
      "Epoch 1317, Loss: 0.07325874641537666, Final Batch Loss: 0.02845030650496483\n",
      "Epoch 1318, Loss: 0.1251755878329277, Final Batch Loss: 0.06401603668928146\n",
      "Epoch 1319, Loss: 0.13080613687634468, Final Batch Loss: 0.07495608925819397\n",
      "Epoch 1320, Loss: 0.0790297333151102, Final Batch Loss: 0.023748507723212242\n",
      "Epoch 1321, Loss: 0.11040864884853363, Final Batch Loss: 0.03197031468153\n",
      "Epoch 1322, Loss: 0.0880049504339695, Final Batch Loss: 0.05220896378159523\n",
      "Epoch 1323, Loss: 0.14766985550522804, Final Batch Loss: 0.047240424901247025\n",
      "Epoch 1324, Loss: 0.07833363860845566, Final Batch Loss: 0.03751206770539284\n",
      "Epoch 1325, Loss: 0.0703250989317894, Final Batch Loss: 0.023836441338062286\n",
      "Epoch 1326, Loss: 0.11145011335611343, Final Batch Loss: 0.031629256904125214\n",
      "Epoch 1327, Loss: 0.07966244220733643, Final Batch Loss: 0.0430222749710083\n",
      "Epoch 1328, Loss: 0.11109490692615509, Final Batch Loss: 0.07381456345319748\n",
      "Epoch 1329, Loss: 0.0727397259324789, Final Batch Loss: 0.019625579938292503\n",
      "Epoch 1330, Loss: 0.20296062529087067, Final Batch Loss: 0.06744019687175751\n",
      "Epoch 1331, Loss: 0.11082666739821434, Final Batch Loss: 0.07265224307775497\n",
      "Epoch 1332, Loss: 0.10934979096055031, Final Batch Loss: 0.060917288064956665\n",
      "Epoch 1333, Loss: 0.11159678176045418, Final Batch Loss: 0.0531131774187088\n",
      "Epoch 1334, Loss: 0.1726701408624649, Final Batch Loss: 0.10161734372377396\n",
      "Epoch 1335, Loss: 0.05085131525993347, Final Batch Loss: 0.020898694172501564\n",
      "Epoch 1336, Loss: 0.08806086145341396, Final Batch Loss: 0.022945692762732506\n",
      "Epoch 1337, Loss: 0.14264851436018944, Final Batch Loss: 0.040602389723062515\n",
      "Epoch 1338, Loss: 0.08741527050733566, Final Batch Loss: 0.051004666835069656\n",
      "Epoch 1339, Loss: 0.11183637753129005, Final Batch Loss: 0.02191970869898796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1340, Loss: 0.1108817532658577, Final Batch Loss: 0.0662003681063652\n",
      "Epoch 1341, Loss: 0.03122553601861, Final Batch Loss: 0.010877944529056549\n",
      "Epoch 1342, Loss: 0.13228482007980347, Final Batch Loss: 0.04482688754796982\n",
      "Epoch 1343, Loss: 0.0826728492975235, Final Batch Loss: 0.04494035243988037\n",
      "Epoch 1344, Loss: 0.11900540441274643, Final Batch Loss: 0.04372353106737137\n",
      "Epoch 1345, Loss: 0.1156026441603899, Final Batch Loss: 0.01710142381489277\n",
      "Epoch 1346, Loss: 0.12382781878113747, Final Batch Loss: 0.06698420643806458\n",
      "Epoch 1347, Loss: 0.08490565232932568, Final Batch Loss: 0.01300068013370037\n",
      "Epoch 1348, Loss: 0.09630370512604713, Final Batch Loss: 0.043054353445768356\n",
      "Epoch 1349, Loss: 0.08567503467202187, Final Batch Loss: 0.033431921154260635\n",
      "Epoch 1350, Loss: 0.10801446437835693, Final Batch Loss: 0.05315866693854332\n",
      "Epoch 1351, Loss: 0.13546442985534668, Final Batch Loss: 0.031906574964523315\n",
      "Epoch 1352, Loss: 0.14818769320845604, Final Batch Loss: 0.09953873604536057\n",
      "Epoch 1353, Loss: 0.05812794342637062, Final Batch Loss: 0.018522925674915314\n",
      "Epoch 1354, Loss: 0.10243447124958038, Final Batch Loss: 0.02728942781686783\n",
      "Epoch 1355, Loss: 0.15101651847362518, Final Batch Loss: 0.08267083019018173\n",
      "Epoch 1356, Loss: 0.11844461038708687, Final Batch Loss: 0.0682545080780983\n",
      "Epoch 1357, Loss: 0.06297260895371437, Final Batch Loss: 0.02408897504210472\n",
      "Epoch 1358, Loss: 0.06931269355118275, Final Batch Loss: 0.029540127143263817\n",
      "Epoch 1359, Loss: 0.15281985327601433, Final Batch Loss: 0.054199833422899246\n",
      "Epoch 1360, Loss: 0.11999313160777092, Final Batch Loss: 0.06421604752540588\n",
      "Epoch 1361, Loss: 0.07484655641019344, Final Batch Loss: 0.02024831436574459\n",
      "Epoch 1362, Loss: 0.14449334517121315, Final Batch Loss: 0.056659597903490067\n",
      "Epoch 1363, Loss: 0.08100415021181107, Final Batch Loss: 0.04968395456671715\n",
      "Epoch 1364, Loss: 0.11317726224660873, Final Batch Loss: 0.0590677373111248\n",
      "Epoch 1365, Loss: 0.11988642811775208, Final Batch Loss: 0.09228231757879257\n",
      "Epoch 1366, Loss: 0.10089931637048721, Final Batch Loss: 0.038693226873874664\n",
      "Epoch 1367, Loss: 0.1188214123249054, Final Batch Loss: 0.06770011782646179\n",
      "Epoch 1368, Loss: 0.07949674036353827, Final Batch Loss: 0.0139989098533988\n",
      "Epoch 1369, Loss: 0.127692561596632, Final Batch Loss: 0.06840237230062485\n",
      "Epoch 1370, Loss: 0.11842811480164528, Final Batch Loss: 0.02909998968243599\n",
      "Epoch 1371, Loss: 0.08211395144462585, Final Batch Loss: 0.04459303617477417\n",
      "Epoch 1372, Loss: 0.12412969022989273, Final Batch Loss: 0.0640287920832634\n",
      "Epoch 1373, Loss: 0.1684383638203144, Final Batch Loss: 0.11579397320747375\n",
      "Epoch 1374, Loss: 0.0947619341313839, Final Batch Loss: 0.034940045326948166\n",
      "Epoch 1375, Loss: 0.12955355644226074, Final Batch Loss: 0.039083510637283325\n",
      "Epoch 1376, Loss: 0.09079420566558838, Final Batch Loss: 0.04017600044608116\n",
      "Epoch 1377, Loss: 0.10778123140335083, Final Batch Loss: 0.05029469355940819\n",
      "Epoch 1378, Loss: 0.23768340051174164, Final Batch Loss: 0.18526522815227509\n",
      "Epoch 1379, Loss: 0.11154593154788017, Final Batch Loss: 0.027842503041028976\n",
      "Epoch 1380, Loss: 0.13852152228355408, Final Batch Loss: 0.08240069448947906\n",
      "Epoch 1381, Loss: 0.06009243614971638, Final Batch Loss: 0.018253805115818977\n",
      "Epoch 1382, Loss: 0.10476833581924438, Final Batch Loss: 0.048994459211826324\n",
      "Epoch 1383, Loss: 0.11803285032510757, Final Batch Loss: 0.07595854997634888\n",
      "Epoch 1384, Loss: 0.10462460294365883, Final Batch Loss: 0.04224742203950882\n",
      "Epoch 1385, Loss: 0.118917316198349, Final Batch Loss: 0.03194892406463623\n",
      "Epoch 1386, Loss: 0.15170272439718246, Final Batch Loss: 0.09806466847658157\n",
      "Epoch 1387, Loss: 0.1589868776500225, Final Batch Loss: 0.03319885954260826\n",
      "Epoch 1388, Loss: 0.13250820711255074, Final Batch Loss: 0.08919918537139893\n",
      "Epoch 1389, Loss: 0.07710655219852924, Final Batch Loss: 0.02357194386422634\n",
      "Epoch 1390, Loss: 0.07532872632145882, Final Batch Loss: 0.021487869322299957\n",
      "Epoch 1391, Loss: 0.12914203852415085, Final Batch Loss: 0.07513926923274994\n",
      "Epoch 1392, Loss: 0.08874797262251377, Final Batch Loss: 0.025437461212277412\n",
      "Epoch 1393, Loss: 0.08235281705856323, Final Batch Loss: 0.03704267367720604\n",
      "Epoch 1394, Loss: 0.1705108843743801, Final Batch Loss: 0.10939154028892517\n",
      "Epoch 1395, Loss: 0.1014263890683651, Final Batch Loss: 0.03611617162823677\n",
      "Epoch 1396, Loss: 0.12231695279479027, Final Batch Loss: 0.04301795735955238\n",
      "Epoch 1397, Loss: 0.1256262306123972, Final Batch Loss: 0.031012462452054024\n",
      "Epoch 1398, Loss: 0.14608686417341232, Final Batch Loss: 0.06437613070011139\n",
      "Epoch 1399, Loss: 0.10110320523381233, Final Batch Loss: 0.06634630262851715\n",
      "Epoch 1400, Loss: 0.09910301491618156, Final Batch Loss: 0.026686053723096848\n",
      "Epoch 1401, Loss: 0.09482121840119362, Final Batch Loss: 0.04852135479450226\n",
      "Epoch 1402, Loss: 0.2916879430413246, Final Batch Loss: 0.19368015229701996\n",
      "Epoch 1403, Loss: 0.18814799934625626, Final Batch Loss: 0.1464974284172058\n",
      "Epoch 1404, Loss: 0.188060462474823, Final Batch Loss: 0.11966875195503235\n",
      "Epoch 1405, Loss: 0.08546417579054832, Final Batch Loss: 0.03728725388646126\n",
      "Epoch 1406, Loss: 0.119684599339962, Final Batch Loss: 0.06149512156844139\n",
      "Epoch 1407, Loss: 0.08063433691859245, Final Batch Loss: 0.038207173347473145\n",
      "Epoch 1408, Loss: 0.2800895385444164, Final Batch Loss: 0.23284998536109924\n",
      "Epoch 1409, Loss: 0.05029773619025946, Final Batch Loss: 0.01093906071037054\n",
      "Epoch 1410, Loss: 0.09201249852776527, Final Batch Loss: 0.04393373057246208\n",
      "Epoch 1411, Loss: 0.1369650773704052, Final Batch Loss: 0.0772387757897377\n",
      "Epoch 1412, Loss: 0.1131814494729042, Final Batch Loss: 0.08059462159872055\n",
      "Epoch 1413, Loss: 0.12328055873513222, Final Batch Loss: 0.08018340915441513\n",
      "Epoch 1414, Loss: 0.10591619834303856, Final Batch Loss: 0.029075901955366135\n",
      "Epoch 1415, Loss: 0.0907001905143261, Final Batch Loss: 0.04037739336490631\n",
      "Epoch 1416, Loss: 0.06494854018092155, Final Batch Loss: 0.025965139269828796\n",
      "Epoch 1417, Loss: 0.1254904456436634, Final Batch Loss: 0.058766674250364304\n",
      "Epoch 1418, Loss: 0.08787009492516518, Final Batch Loss: 0.0271901935338974\n",
      "Epoch 1419, Loss: 0.13434119522571564, Final Batch Loss: 0.09061750769615173\n",
      "Epoch 1420, Loss: 0.1909368932247162, Final Batch Loss: 0.08266887813806534\n",
      "Epoch 1421, Loss: 0.11580013856291771, Final Batch Loss: 0.03141573444008827\n",
      "Epoch 1422, Loss: 0.08035143092274666, Final Batch Loss: 0.03784237056970596\n",
      "Epoch 1423, Loss: 0.18108019977808, Final Batch Loss: 0.11751876771450043\n",
      "Epoch 1424, Loss: 0.09216391295194626, Final Batch Loss: 0.03470756486058235\n",
      "Epoch 1425, Loss: 0.10776140913367271, Final Batch Loss: 0.0688285231590271\n",
      "Epoch 1426, Loss: 0.10885646939277649, Final Batch Loss: 0.06311395019292831\n",
      "Epoch 1427, Loss: 0.12223584577441216, Final Batch Loss: 0.07898620516061783\n",
      "Epoch 1428, Loss: 0.1802643984556198, Final Batch Loss: 0.1007617935538292\n",
      "Epoch 1429, Loss: 0.15390435233712196, Final Batch Loss: 0.0944308340549469\n",
      "Epoch 1430, Loss: 0.06779037788510323, Final Batch Loss: 0.04134882986545563\n",
      "Epoch 1431, Loss: 0.13649968057870865, Final Batch Loss: 0.07750009000301361\n",
      "Epoch 1432, Loss: 0.08674637414515018, Final Batch Loss: 0.014686444774270058\n",
      "Epoch 1433, Loss: 0.09472406283020973, Final Batch Loss: 0.051914166659116745\n",
      "Epoch 1434, Loss: 0.13324153423309326, Final Batch Loss: 0.0911659523844719\n",
      "Epoch 1435, Loss: 0.09263639338314533, Final Batch Loss: 0.018606992438435555\n",
      "Epoch 1436, Loss: 0.11653092503547668, Final Batch Loss: 0.06772273033857346\n",
      "Epoch 1437, Loss: 0.14239835366606712, Final Batch Loss: 0.05776628479361534\n",
      "Epoch 1438, Loss: 0.18744366616010666, Final Batch Loss: 0.12112436443567276\n",
      "Epoch 1439, Loss: 0.11187281087040901, Final Batch Loss: 0.05418859422206879\n",
      "Epoch 1440, Loss: 0.10787845030426979, Final Batch Loss: 0.04693164303898811\n",
      "Epoch 1441, Loss: 0.10436980426311493, Final Batch Loss: 0.04409440979361534\n",
      "Epoch 1442, Loss: 0.08431371673941612, Final Batch Loss: 0.048870548605918884\n",
      "Epoch 1443, Loss: 0.12651128321886063, Final Batch Loss: 0.0650351345539093\n",
      "Epoch 1444, Loss: 0.1456567645072937, Final Batch Loss: 0.09636959433555603\n",
      "Epoch 1445, Loss: 0.07541239261627197, Final Batch Loss: 0.041420672088861465\n",
      "Epoch 1446, Loss: 0.08013503812253475, Final Batch Loss: 0.017648162320256233\n",
      "Epoch 1447, Loss: 0.1283194199204445, Final Batch Loss: 0.09925621002912521\n",
      "Epoch 1448, Loss: 0.06619049794971943, Final Batch Loss: 0.030936861410737038\n",
      "Epoch 1449, Loss: 0.08267267793416977, Final Batch Loss: 0.04780154675245285\n",
      "Epoch 1450, Loss: 0.0987781323492527, Final Batch Loss: 0.051450520753860474\n",
      "Epoch 1451, Loss: 0.24659806489944458, Final Batch Loss: 0.1911451816558838\n",
      "Epoch 1452, Loss: 0.12083236128091812, Final Batch Loss: 0.06464345753192902\n",
      "Epoch 1453, Loss: 0.09384625032544136, Final Batch Loss: 0.03828999772667885\n",
      "Epoch 1454, Loss: 0.13033412024378777, Final Batch Loss: 0.055304501205682755\n",
      "Epoch 1455, Loss: 0.14715322852134705, Final Batch Loss: 0.09242136031389236\n",
      "Epoch 1456, Loss: 0.12832346558570862, Final Batch Loss: 0.053347885608673096\n",
      "Epoch 1457, Loss: 0.17084701359272003, Final Batch Loss: 0.10488848388195038\n",
      "Epoch 1458, Loss: 0.1853727325797081, Final Batch Loss: 0.09613924473524094\n",
      "Epoch 1459, Loss: 0.06781299039721489, Final Batch Loss: 0.028650466352701187\n",
      "Epoch 1460, Loss: 0.12297040969133377, Final Batch Loss: 0.03376191854476929\n",
      "Epoch 1461, Loss: 0.07908379659056664, Final Batch Loss: 0.04396938532590866\n",
      "Epoch 1462, Loss: 0.18529891222715378, Final Batch Loss: 0.0718180388212204\n",
      "Epoch 1463, Loss: 0.06787310540676117, Final Batch Loss: 0.033560384064912796\n",
      "Epoch 1464, Loss: 0.17835792154073715, Final Batch Loss: 0.09911026805639267\n",
      "Epoch 1465, Loss: 0.09850320219993591, Final Batch Loss: 0.03704901412129402\n",
      "Epoch 1466, Loss: 0.15722324326634407, Final Batch Loss: 0.09959127008914948\n",
      "Epoch 1467, Loss: 0.06443226151168346, Final Batch Loss: 0.02005317620933056\n",
      "Epoch 1468, Loss: 0.08272584527730942, Final Batch Loss: 0.034099776297807693\n",
      "Epoch 1469, Loss: 0.11838973686099052, Final Batch Loss: 0.09053821861743927\n",
      "Epoch 1470, Loss: 0.07949856109917164, Final Batch Loss: 0.050692033022642136\n",
      "Epoch 1471, Loss: 0.08480098843574524, Final Batch Loss: 0.016189076006412506\n",
      "Epoch 1472, Loss: 0.09128682687878609, Final Batch Loss: 0.05891452729701996\n",
      "Epoch 1473, Loss: 0.15516526252031326, Final Batch Loss: 0.06640304625034332\n",
      "Epoch 1474, Loss: 0.13517815992236137, Final Batch Loss: 0.05994207784533501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1475, Loss: 0.08504177629947662, Final Batch Loss: 0.0516328439116478\n",
      "Epoch 1476, Loss: 0.076327133923769, Final Batch Loss: 0.03446813300251961\n",
      "Epoch 1477, Loss: 0.09007680416107178, Final Batch Loss: 0.03306679427623749\n",
      "Epoch 1478, Loss: 0.061874374747276306, Final Batch Loss: 0.03325143828988075\n",
      "Epoch 1479, Loss: 0.06663702614605427, Final Batch Loss: 0.03101813606917858\n",
      "Epoch 1480, Loss: 0.12132232263684273, Final Batch Loss: 0.0752505287528038\n",
      "Epoch 1481, Loss: 0.17193806916475296, Final Batch Loss: 0.12138394266366959\n",
      "Epoch 1482, Loss: 0.13610303401947021, Final Batch Loss: 0.07454032450914383\n",
      "Epoch 1483, Loss: 0.09545287489891052, Final Batch Loss: 0.06112492084503174\n",
      "Epoch 1484, Loss: 0.09980326890945435, Final Batch Loss: 0.05892418324947357\n",
      "Epoch 1485, Loss: 0.13224267959594727, Final Batch Loss: 0.0560683012008667\n",
      "Epoch 1486, Loss: 0.10261890292167664, Final Batch Loss: 0.06195639818906784\n",
      "Epoch 1487, Loss: 0.08704319596290588, Final Batch Loss: 0.039712537080049515\n",
      "Epoch 1488, Loss: 0.1301761530339718, Final Batch Loss: 0.1013251394033432\n",
      "Epoch 1489, Loss: 0.09662843495607376, Final Batch Loss: 0.0342353954911232\n",
      "Epoch 1490, Loss: 0.1634926274418831, Final Batch Loss: 0.06830573081970215\n",
      "Epoch 1491, Loss: 0.10218365862965584, Final Batch Loss: 0.05987836420536041\n",
      "Epoch 1492, Loss: 0.08335767779499292, Final Batch Loss: 0.014922522939741611\n",
      "Epoch 1493, Loss: 0.14224567636847496, Final Batch Loss: 0.10054108500480652\n",
      "Epoch 1494, Loss: 0.11831242591142654, Final Batch Loss: 0.07016333937644958\n",
      "Epoch 1495, Loss: 0.08762216195464134, Final Batch Loss: 0.04745818302035332\n",
      "Epoch 1496, Loss: 0.07606927677989006, Final Batch Loss: 0.024639613926410675\n",
      "Epoch 1497, Loss: 0.1504906453192234, Final Batch Loss: 0.10675974190235138\n",
      "Epoch 1498, Loss: 0.1843586601316929, Final Batch Loss: 0.1328394114971161\n",
      "Epoch 1499, Loss: 0.15212862938642502, Final Batch Loss: 0.08396974951028824\n",
      "Epoch 1500, Loss: 0.0777442567050457, Final Batch Loss: 0.040016304701566696\n",
      "Epoch 1501, Loss: 0.09584548510611057, Final Batch Loss: 0.028423795476555824\n",
      "Epoch 1502, Loss: 0.07130332663655281, Final Batch Loss: 0.030551448464393616\n",
      "Epoch 1503, Loss: 0.11004631221294403, Final Batch Loss: 0.05061211436986923\n",
      "Epoch 1504, Loss: 0.06689806282520294, Final Batch Loss: 0.04428829625248909\n",
      "Epoch 1505, Loss: 0.1018444299697876, Final Batch Loss: 0.06729929894208908\n",
      "Epoch 1506, Loss: 0.08569537103176117, Final Batch Loss: 0.03598220646381378\n",
      "Epoch 1507, Loss: 0.06055726669728756, Final Batch Loss: 0.01762775518000126\n",
      "Epoch 1508, Loss: 0.13607049733400345, Final Batch Loss: 0.03984047472476959\n",
      "Epoch 1509, Loss: 0.15329799428582191, Final Batch Loss: 0.10058105736970901\n",
      "Epoch 1510, Loss: 0.10087047144770622, Final Batch Loss: 0.06567277759313583\n",
      "Epoch 1511, Loss: 0.11950650066137314, Final Batch Loss: 0.06667628884315491\n",
      "Epoch 1512, Loss: 0.12354867160320282, Final Batch Loss: 0.08718731254339218\n",
      "Epoch 1513, Loss: 0.12586568668484688, Final Batch Loss: 0.06682280451059341\n",
      "Epoch 1514, Loss: 0.09322964586317539, Final Batch Loss: 0.02783881314098835\n",
      "Epoch 1515, Loss: 0.23187939077615738, Final Batch Loss: 0.19332768023014069\n",
      "Epoch 1516, Loss: 0.10574748553335667, Final Batch Loss: 0.029262395575642586\n",
      "Epoch 1517, Loss: 0.06456211023032665, Final Batch Loss: 0.023976532742381096\n",
      "Epoch 1518, Loss: 0.10408778488636017, Final Batch Loss: 0.034594617784023285\n",
      "Epoch 1519, Loss: 0.06530535593628883, Final Batch Loss: 0.023670431226491928\n",
      "Epoch 1520, Loss: 0.11160705611109734, Final Batch Loss: 0.06430172920227051\n",
      "Epoch 1521, Loss: 0.16194839030504227, Final Batch Loss: 0.10009927302598953\n",
      "Epoch 1522, Loss: 0.09332358464598656, Final Batch Loss: 0.04420057684183121\n",
      "Epoch 1523, Loss: 0.15013014152646065, Final Batch Loss: 0.09925124049186707\n",
      "Epoch 1524, Loss: 0.09188051894307137, Final Batch Loss: 0.050083864480257034\n",
      "Epoch 1525, Loss: 0.11257946491241455, Final Batch Loss: 0.034939251840114594\n",
      "Epoch 1526, Loss: 0.10228346660733223, Final Batch Loss: 0.06369123607873917\n",
      "Epoch 1527, Loss: 0.05281347408890724, Final Batch Loss: 0.0307631753385067\n",
      "Epoch 1528, Loss: 0.1674475520849228, Final Batch Loss: 0.08099275082349777\n",
      "Epoch 1529, Loss: 0.10570695251226425, Final Batch Loss: 0.06451070308685303\n",
      "Epoch 1530, Loss: 0.1231004111468792, Final Batch Loss: 0.08350927382707596\n",
      "Epoch 1531, Loss: 0.1338558793067932, Final Batch Loss: 0.09004674106836319\n",
      "Epoch 1532, Loss: 0.09898728877305984, Final Batch Loss: 0.03810310363769531\n",
      "Epoch 1533, Loss: 0.08141912519931793, Final Batch Loss: 0.022899776697158813\n",
      "Epoch 1534, Loss: 0.07954346761107445, Final Batch Loss: 0.031456444412469864\n",
      "Epoch 1535, Loss: 0.13630877062678337, Final Batch Loss: 0.08885639160871506\n",
      "Epoch 1536, Loss: 0.06438683904707432, Final Batch Loss: 0.027707362547516823\n",
      "Epoch 1537, Loss: 0.1447746306657791, Final Batch Loss: 0.035194575786590576\n",
      "Epoch 1538, Loss: 0.10465032607316971, Final Batch Loss: 0.06531744450330734\n",
      "Epoch 1539, Loss: 0.11506576836109161, Final Batch Loss: 0.06904536485671997\n",
      "Epoch 1540, Loss: 0.05375324375927448, Final Batch Loss: 0.01979350857436657\n",
      "Epoch 1541, Loss: 0.07981684245169163, Final Batch Loss: 0.05286693200469017\n",
      "Epoch 1542, Loss: 0.07256731111556292, Final Batch Loss: 0.007617202587425709\n",
      "Epoch 1543, Loss: 0.10003278031945229, Final Batch Loss: 0.03430904820561409\n",
      "Epoch 1544, Loss: 0.15590856969356537, Final Batch Loss: 0.05183352530002594\n",
      "Epoch 1545, Loss: 0.188602514564991, Final Batch Loss: 0.11865869164466858\n",
      "Epoch 1546, Loss: 0.12847209721803665, Final Batch Loss: 0.08513028919696808\n",
      "Epoch 1547, Loss: 0.13322626054286957, Final Batch Loss: 0.08843736350536346\n",
      "Epoch 1548, Loss: 0.09205383248627186, Final Batch Loss: 0.019288750365376472\n",
      "Epoch 1549, Loss: 0.0909983403980732, Final Batch Loss: 0.0506596565246582\n",
      "Epoch 1550, Loss: 0.07515854761004448, Final Batch Loss: 0.0234718956053257\n",
      "Epoch 1551, Loss: 0.09118227660655975, Final Batch Loss: 0.053262341767549515\n",
      "Epoch 1552, Loss: 0.11032025143504143, Final Batch Loss: 0.04892902076244354\n",
      "Epoch 1553, Loss: 0.09342282265424728, Final Batch Loss: 0.037217769771814346\n",
      "Epoch 1554, Loss: 0.11548599228262901, Final Batch Loss: 0.07909722626209259\n",
      "Epoch 1555, Loss: 0.18102483451366425, Final Batch Loss: 0.11096777766942978\n",
      "Epoch 1556, Loss: 0.12312910705804825, Final Batch Loss: 0.038541339337825775\n",
      "Epoch 1557, Loss: 0.0906657800078392, Final Batch Loss: 0.0345427431166172\n",
      "Epoch 1558, Loss: 0.10200216248631477, Final Batch Loss: 0.060447972267866135\n",
      "Epoch 1559, Loss: 0.1001666933298111, Final Batch Loss: 0.03416869789361954\n",
      "Epoch 1560, Loss: 0.11618959158658981, Final Batch Loss: 0.05382092669606209\n",
      "Epoch 1561, Loss: 0.16986646130681038, Final Batch Loss: 0.12447549402713776\n",
      "Epoch 1562, Loss: 0.07549717091023922, Final Batch Loss: 0.05040542036294937\n",
      "Epoch 1563, Loss: 0.11367192491889, Final Batch Loss: 0.06686163693666458\n",
      "Epoch 1564, Loss: 0.11945126205682755, Final Batch Loss: 0.05836350843310356\n",
      "Epoch 1565, Loss: 0.10530933365225792, Final Batch Loss: 0.06068797409534454\n",
      "Epoch 1566, Loss: 0.071317657828331, Final Batch Loss: 0.043618422001600266\n",
      "Epoch 1567, Loss: 0.12394241243600845, Final Batch Loss: 0.06809239834547043\n",
      "Epoch 1568, Loss: 0.059525808319449425, Final Batch Loss: 0.021609677001833916\n",
      "Epoch 1569, Loss: 0.16585680842399597, Final Batch Loss: 0.09971281886100769\n",
      "Epoch 1570, Loss: 0.09829843789339066, Final Batch Loss: 0.03683770075440407\n",
      "Epoch 1571, Loss: 0.11841997131705284, Final Batch Loss: 0.0492977611720562\n",
      "Epoch 1572, Loss: 0.12229704484343529, Final Batch Loss: 0.07553442567586899\n",
      "Epoch 1573, Loss: 0.09537102840840816, Final Batch Loss: 0.06628985702991486\n",
      "Epoch 1574, Loss: 0.1627974957227707, Final Batch Loss: 0.10850060731172562\n",
      "Epoch 1575, Loss: 0.0636514388024807, Final Batch Loss: 0.013481725007295609\n",
      "Epoch 1576, Loss: 0.10244819521903992, Final Batch Loss: 0.06559566408395767\n",
      "Epoch 1577, Loss: 0.10316288471221924, Final Batch Loss: 0.055538419634103775\n",
      "Epoch 1578, Loss: 0.08700536005198956, Final Batch Loss: 0.057274580001831055\n",
      "Epoch 1579, Loss: 0.07164619117975235, Final Batch Loss: 0.025830727070569992\n",
      "Epoch 1580, Loss: 0.08773768320679665, Final Batch Loss: 0.02297983691096306\n",
      "Epoch 1581, Loss: 0.14889304339885712, Final Batch Loss: 0.07204602658748627\n",
      "Epoch 1582, Loss: 0.06490668747574091, Final Batch Loss: 0.011056014336645603\n",
      "Epoch 1583, Loss: 0.06473503541201353, Final Batch Loss: 0.013186871074140072\n",
      "Epoch 1584, Loss: 0.11638246849179268, Final Batch Loss: 0.057685766369104385\n",
      "Epoch 1585, Loss: 0.18092523515224457, Final Batch Loss: 0.12106680870056152\n",
      "Epoch 1586, Loss: 0.10797835513949394, Final Batch Loss: 0.03878133371472359\n",
      "Epoch 1587, Loss: 0.15596722066402435, Final Batch Loss: 0.06549065560102463\n",
      "Epoch 1588, Loss: 0.07563995942473412, Final Batch Loss: 0.02151976153254509\n",
      "Epoch 1589, Loss: 0.1354973427951336, Final Batch Loss: 0.05352208390831947\n",
      "Epoch 1590, Loss: 0.09945214912295341, Final Batch Loss: 0.04621576890349388\n",
      "Epoch 1591, Loss: 0.14724567532539368, Final Batch Loss: 0.09876297414302826\n",
      "Epoch 1592, Loss: 0.14708497375249863, Final Batch Loss: 0.07591965049505234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1593, Loss: 0.0859791599214077, Final Batch Loss: 0.037938009947538376\n",
      "Epoch 1594, Loss: 0.10175646282732487, Final Batch Loss: 0.07390264421701431\n",
      "Epoch 1595, Loss: 0.058067455887794495, Final Batch Loss: 0.02732245810329914\n",
      "Epoch 1596, Loss: 0.062011370435357094, Final Batch Loss: 0.021639546379446983\n",
      "Epoch 1597, Loss: 0.10926065221428871, Final Batch Loss: 0.06514014303684235\n",
      "Epoch 1598, Loss: 0.11113390326499939, Final Batch Loss: 0.09237509965896606\n",
      "Epoch 1599, Loss: 0.16245682537555695, Final Batch Loss: 0.08684682846069336\n",
      "Epoch 1600, Loss: 0.09988782554864883, Final Batch Loss: 0.05969683453440666\n",
      "Epoch 1601, Loss: 0.17989562451839447, Final Batch Loss: 0.07639187574386597\n",
      "Epoch 1602, Loss: 0.11757678166031837, Final Batch Loss: 0.050490494817495346\n",
      "Epoch 1603, Loss: 0.1288081593811512, Final Batch Loss: 0.09025545418262482\n",
      "Epoch 1604, Loss: 0.12192147225141525, Final Batch Loss: 0.051831282675266266\n",
      "Epoch 1605, Loss: 0.1344980001449585, Final Batch Loss: 0.07003439217805862\n",
      "Epoch 1606, Loss: 0.05930977128446102, Final Batch Loss: 0.023742539808154106\n",
      "Epoch 1607, Loss: 0.057974983006715775, Final Batch Loss: 0.02676207572221756\n",
      "Epoch 1608, Loss: 0.09273556992411613, Final Batch Loss: 0.03264755383133888\n",
      "Epoch 1609, Loss: 0.10802758857607841, Final Batch Loss: 0.06100648641586304\n",
      "Epoch 1610, Loss: 0.13382979854941368, Final Batch Loss: 0.06248300150036812\n",
      "Epoch 1611, Loss: 0.09018253907561302, Final Batch Loss: 0.05012984573841095\n",
      "Epoch 1612, Loss: 0.1253824159502983, Final Batch Loss: 0.07807472348213196\n",
      "Epoch 1613, Loss: 0.12314470112323761, Final Batch Loss: 0.0675133690237999\n",
      "Epoch 1614, Loss: 0.04471152275800705, Final Batch Loss: 0.019988493993878365\n",
      "Epoch 1615, Loss: 0.06395098194479942, Final Batch Loss: 0.02370152249932289\n",
      "Epoch 1616, Loss: 0.07066520303487778, Final Batch Loss: 0.03195098042488098\n",
      "Epoch 1617, Loss: 0.09091795608401299, Final Batch Loss: 0.04032202064990997\n",
      "Epoch 1618, Loss: 0.1194353923201561, Final Batch Loss: 0.08260003477334976\n",
      "Epoch 1619, Loss: 0.045679667964577675, Final Batch Loss: 0.015048349276185036\n",
      "Epoch 1620, Loss: 0.07016557455062866, Final Batch Loss: 0.011999744921922684\n",
      "Epoch 1621, Loss: 0.19809915497899055, Final Batch Loss: 0.053984690457582474\n",
      "Epoch 1622, Loss: 0.1749766208231449, Final Batch Loss: 0.11641383171081543\n",
      "Epoch 1623, Loss: 0.06451316364109516, Final Batch Loss: 0.02066110260784626\n",
      "Epoch 1624, Loss: 0.05141155607998371, Final Batch Loss: 0.022491319105029106\n",
      "Epoch 1625, Loss: 0.09941146895289421, Final Batch Loss: 0.04572899639606476\n",
      "Epoch 1626, Loss: 0.08833585679531097, Final Batch Loss: 0.02110418677330017\n",
      "Epoch 1627, Loss: 0.1259758397936821, Final Batch Loss: 0.04373864084482193\n",
      "Epoch 1628, Loss: 0.08723554015159607, Final Batch Loss: 0.02671903371810913\n",
      "Epoch 1629, Loss: 0.055282123386859894, Final Batch Loss: 0.03773188218474388\n",
      "Epoch 1630, Loss: 0.07528611272573471, Final Batch Loss: 0.04400203377008438\n",
      "Epoch 1631, Loss: 0.048770323395729065, Final Batch Loss: 0.024380695074796677\n",
      "Epoch 1632, Loss: 0.07092269882559776, Final Batch Loss: 0.03341735899448395\n",
      "Epoch 1633, Loss: 0.08262385800480843, Final Batch Loss: 0.03372786566615105\n",
      "Epoch 1634, Loss: 0.09996592812240124, Final Batch Loss: 0.07605213671922684\n",
      "Epoch 1635, Loss: 0.05816289130598307, Final Batch Loss: 0.015540321357548237\n",
      "Epoch 1636, Loss: 0.147659033536911, Final Batch Loss: 0.0879664495587349\n",
      "Epoch 1637, Loss: 0.07663133554160595, Final Batch Loss: 0.0462489053606987\n",
      "Epoch 1638, Loss: 0.07366372272372246, Final Batch Loss: 0.03808426856994629\n",
      "Epoch 1639, Loss: 0.2463759034872055, Final Batch Loss: 0.14013482630252838\n",
      "Epoch 1640, Loss: 0.06159514468163252, Final Batch Loss: 0.011683429591357708\n",
      "Epoch 1641, Loss: 0.12776746228337288, Final Batch Loss: 0.09149722009897232\n",
      "Epoch 1642, Loss: 0.13516413420438766, Final Batch Loss: 0.07728160917758942\n",
      "Epoch 1643, Loss: 0.12584853917360306, Final Batch Loss: 0.09193520992994308\n",
      "Epoch 1644, Loss: 0.15441061556339264, Final Batch Loss: 0.08931887894868851\n",
      "Epoch 1645, Loss: 0.0491841621696949, Final Batch Loss: 0.02079685963690281\n",
      "Epoch 1646, Loss: 0.11232467368245125, Final Batch Loss: 0.061013974249362946\n",
      "Epoch 1647, Loss: 0.09801862016320229, Final Batch Loss: 0.04811498522758484\n",
      "Epoch 1648, Loss: 0.04896489344537258, Final Batch Loss: 0.019525794312357903\n",
      "Epoch 1649, Loss: 0.05595100671052933, Final Batch Loss: 0.025221621617674828\n",
      "Epoch 1650, Loss: 0.0982634536921978, Final Batch Loss: 0.055788803845644\n",
      "Epoch 1651, Loss: 0.06920111924409866, Final Batch Loss: 0.028606228530406952\n",
      "Epoch 1652, Loss: 0.0792446918785572, Final Batch Loss: 0.03312497213482857\n",
      "Epoch 1653, Loss: 0.05424167215824127, Final Batch Loss: 0.030660899356007576\n",
      "Epoch 1654, Loss: 0.08389805257320404, Final Batch Loss: 0.020643197000026703\n",
      "Epoch 1655, Loss: 0.054529499262571335, Final Batch Loss: 0.014122072607278824\n",
      "Epoch 1656, Loss: 0.22285771928727627, Final Batch Loss: 0.19687137007713318\n",
      "Epoch 1657, Loss: 0.19821327179670334, Final Batch Loss: 0.12683556973934174\n",
      "Epoch 1658, Loss: 0.16495491564273834, Final Batch Loss: 0.13348445296287537\n",
      "Epoch 1659, Loss: 0.0873364694416523, Final Batch Loss: 0.05056106671690941\n",
      "Epoch 1660, Loss: 0.08450229093432426, Final Batch Loss: 0.05123016610741615\n",
      "Epoch 1661, Loss: 0.06141956150531769, Final Batch Loss: 0.01603037118911743\n",
      "Epoch 1662, Loss: 0.13314718753099442, Final Batch Loss: 0.04236246645450592\n",
      "Epoch 1663, Loss: 0.05095076188445091, Final Batch Loss: 0.014215800911188126\n",
      "Epoch 1664, Loss: 0.10715407691895962, Final Batch Loss: 0.022838054224848747\n",
      "Epoch 1665, Loss: 0.11349787935614586, Final Batch Loss: 0.058523669838905334\n",
      "Epoch 1666, Loss: 0.07832824438810349, Final Batch Loss: 0.025978542864322662\n",
      "Epoch 1667, Loss: 0.09876922890543938, Final Batch Loss: 0.056666918098926544\n",
      "Epoch 1668, Loss: 0.07433384098112583, Final Batch Loss: 0.02518490143120289\n",
      "Epoch 1669, Loss: 0.21227547153830528, Final Batch Loss: 0.16162672638893127\n",
      "Epoch 1670, Loss: 0.043077126145362854, Final Batch Loss: 0.011954642832279205\n",
      "Epoch 1671, Loss: 0.1145840585231781, Final Batch Loss: 0.07407298684120178\n",
      "Epoch 1672, Loss: 0.059293728321790695, Final Batch Loss: 0.023187410086393356\n",
      "Epoch 1673, Loss: 0.09292159602046013, Final Batch Loss: 0.03514736890792847\n",
      "Epoch 1674, Loss: 0.1134958453476429, Final Batch Loss: 0.06775598973035812\n",
      "Epoch 1675, Loss: 0.08763018622994423, Final Batch Loss: 0.04684269428253174\n",
      "Epoch 1676, Loss: 0.10634548403322697, Final Batch Loss: 0.02287726290524006\n",
      "Epoch 1677, Loss: 0.060710713267326355, Final Batch Loss: 0.029061831533908844\n",
      "Epoch 1678, Loss: 0.06323913857340813, Final Batch Loss: 0.011995598673820496\n",
      "Epoch 1679, Loss: 0.09784780442714691, Final Batch Loss: 0.05101636424660683\n",
      "Epoch 1680, Loss: 0.19847089797258377, Final Batch Loss: 0.14237309992313385\n",
      "Epoch 1681, Loss: 0.12465972825884819, Final Batch Loss: 0.05280263349413872\n",
      "Epoch 1682, Loss: 0.09470020979642868, Final Batch Loss: 0.028035305440425873\n",
      "Epoch 1683, Loss: 0.08283836767077446, Final Batch Loss: 0.04445352405309677\n",
      "Epoch 1684, Loss: 0.05550096184015274, Final Batch Loss: 0.016492672264575958\n",
      "Epoch 1685, Loss: 0.0525397676974535, Final Batch Loss: 0.02736588381230831\n",
      "Epoch 1686, Loss: 0.09003593027591705, Final Batch Loss: 0.059043481945991516\n",
      "Epoch 1687, Loss: 0.09162051975727081, Final Batch Loss: 0.03888893872499466\n",
      "Epoch 1688, Loss: 0.10893041267991066, Final Batch Loss: 0.04726839438080788\n",
      "Epoch 1689, Loss: 0.10046470537781715, Final Batch Loss: 0.06402729451656342\n",
      "Epoch 1690, Loss: 0.05096372775733471, Final Batch Loss: 0.01916400156915188\n",
      "Epoch 1691, Loss: 0.07543940283358097, Final Batch Loss: 0.0208637285977602\n",
      "Epoch 1692, Loss: 0.07626066543161869, Final Batch Loss: 0.024680258706212044\n",
      "Epoch 1693, Loss: 0.1423080489039421, Final Batch Loss: 0.10109639167785645\n",
      "Epoch 1694, Loss: 0.10431307926774025, Final Batch Loss: 0.035382818430662155\n",
      "Epoch 1695, Loss: 0.08565367758274078, Final Batch Loss: 0.03291192278265953\n",
      "Epoch 1696, Loss: 0.08322804048657417, Final Batch Loss: 0.046867068856954575\n",
      "Epoch 1697, Loss: 0.06609743274748325, Final Batch Loss: 0.02803543396294117\n",
      "Epoch 1698, Loss: 0.0393024580553174, Final Batch Loss: 0.00982175674289465\n",
      "Epoch 1699, Loss: 0.09272832050919533, Final Batch Loss: 0.05735984444618225\n",
      "Epoch 1700, Loss: 0.13519204780459404, Final Batch Loss: 0.10215076059103012\n",
      "Epoch 1701, Loss: 0.0954432561993599, Final Batch Loss: 0.0702250525355339\n",
      "Epoch 1702, Loss: 0.11634398251771927, Final Batch Loss: 0.047742292284965515\n",
      "Epoch 1703, Loss: 0.0980171486735344, Final Batch Loss: 0.04613816738128662\n",
      "Epoch 1704, Loss: 0.07218867167830467, Final Batch Loss: 0.04209507629275322\n",
      "Epoch 1705, Loss: 0.07077683508396149, Final Batch Loss: 0.041420742869377136\n",
      "Epoch 1706, Loss: 0.09491218999028206, Final Batch Loss: 0.04618324339389801\n",
      "Epoch 1707, Loss: 0.14525224268436432, Final Batch Loss: 0.10796493291854858\n",
      "Epoch 1708, Loss: 0.13742811977863312, Final Batch Loss: 0.0708213597536087\n",
      "Epoch 1709, Loss: 0.08178363740444183, Final Batch Loss: 0.03658430278301239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1710, Loss: 0.0900456216186285, Final Batch Loss: 0.05961417034268379\n",
      "Epoch 1711, Loss: 0.0945948176085949, Final Batch Loss: 0.03691789507865906\n",
      "Epoch 1712, Loss: 0.09867728129029274, Final Batch Loss: 0.033109862357378006\n",
      "Epoch 1713, Loss: 0.10945839807391167, Final Batch Loss: 0.03414381667971611\n",
      "Epoch 1714, Loss: 0.12049756571650505, Final Batch Loss: 0.09600923210382462\n",
      "Epoch 1715, Loss: 0.07824452221393585, Final Batch Loss: 0.035039760172367096\n",
      "Epoch 1716, Loss: 0.06864059716463089, Final Batch Loss: 0.030818182975053787\n",
      "Epoch 1717, Loss: 0.06058571673929691, Final Batch Loss: 0.01045927219092846\n",
      "Epoch 1718, Loss: 0.08067572303116322, Final Batch Loss: 0.02411486767232418\n",
      "Epoch 1719, Loss: 0.0430422592908144, Final Batch Loss: 0.010559586808085442\n",
      "Epoch 1720, Loss: 0.08146922290325165, Final Batch Loss: 0.023075014352798462\n",
      "Epoch 1721, Loss: 0.1635138988494873, Final Batch Loss: 0.08553668111562729\n",
      "Epoch 1722, Loss: 0.042296914383769035, Final Batch Loss: 0.018133044242858887\n",
      "Epoch 1723, Loss: 0.06887185759842396, Final Batch Loss: 0.02364390902221203\n",
      "Epoch 1724, Loss: 0.10277806222438812, Final Batch Loss: 0.06422897428274155\n",
      "Epoch 1725, Loss: 0.10619143769145012, Final Batch Loss: 0.08789261430501938\n",
      "Epoch 1726, Loss: 0.10146505665034056, Final Batch Loss: 0.009884322993457317\n",
      "Epoch 1727, Loss: 0.09263267181813717, Final Batch Loss: 0.030050596222281456\n",
      "Epoch 1728, Loss: 0.04792241333052516, Final Batch Loss: 0.005927752237766981\n",
      "Epoch 1729, Loss: 0.15858307480812073, Final Batch Loss: 0.10579897463321686\n",
      "Epoch 1730, Loss: 0.14982377737760544, Final Batch Loss: 0.07677409052848816\n",
      "Epoch 1731, Loss: 0.05447231885045767, Final Batch Loss: 0.011668204329907894\n",
      "Epoch 1732, Loss: 0.10261445119976997, Final Batch Loss: 0.0459248386323452\n",
      "Epoch 1733, Loss: 0.0961666889488697, Final Batch Loss: 0.06066308915615082\n",
      "Epoch 1734, Loss: 0.052044449374079704, Final Batch Loss: 0.028943052515387535\n",
      "Epoch 1735, Loss: 0.08359326794743538, Final Batch Loss: 0.012704063206911087\n",
      "Epoch 1736, Loss: 0.09617895074188709, Final Batch Loss: 0.02344577945768833\n",
      "Epoch 1737, Loss: 0.14884322881698608, Final Batch Loss: 0.06442096829414368\n",
      "Epoch 1738, Loss: 0.05187029577791691, Final Batch Loss: 0.025519240647554398\n",
      "Epoch 1739, Loss: 0.11286674439907074, Final Batch Loss: 0.07091689109802246\n",
      "Epoch 1740, Loss: 0.07888790220022202, Final Batch Loss: 0.033491864800453186\n",
      "Epoch 1741, Loss: 0.08306377567350864, Final Batch Loss: 0.02679944597184658\n",
      "Epoch 1742, Loss: 0.10191516950726509, Final Batch Loss: 0.061742305755615234\n",
      "Epoch 1743, Loss: 0.07844255492091179, Final Batch Loss: 0.04592994973063469\n",
      "Epoch 1744, Loss: 0.08606557920575142, Final Batch Loss: 0.03699309751391411\n",
      "Epoch 1745, Loss: 0.0770952645689249, Final Batch Loss: 0.019478311762213707\n",
      "Epoch 1746, Loss: 0.06462739035487175, Final Batch Loss: 0.021976832300424576\n",
      "Epoch 1747, Loss: 0.08090425096452236, Final Batch Loss: 0.019008098170161247\n",
      "Epoch 1748, Loss: 0.03986309841275215, Final Batch Loss: 0.00862804427742958\n",
      "Epoch 1749, Loss: 0.060724448412656784, Final Batch Loss: 0.02533675730228424\n",
      "Epoch 1750, Loss: 0.07029068656265736, Final Batch Loss: 0.022393768653273582\n",
      "Epoch 1751, Loss: 0.06752565875649452, Final Batch Loss: 0.032300181686878204\n",
      "Epoch 1752, Loss: 0.062241170555353165, Final Batch Loss: 0.02072620764374733\n",
      "Epoch 1753, Loss: 0.08762656711041927, Final Batch Loss: 0.025937924161553383\n",
      "Epoch 1754, Loss: 0.0898328348994255, Final Batch Loss: 0.04248833283782005\n",
      "Epoch 1755, Loss: 0.057225706055760384, Final Batch Loss: 0.033354341983795166\n",
      "Epoch 1756, Loss: 0.07140698097646236, Final Batch Loss: 0.02405613474547863\n",
      "Epoch 1757, Loss: 0.08884011581540108, Final Batch Loss: 0.045596878975629807\n",
      "Epoch 1758, Loss: 0.05358599312603474, Final Batch Loss: 0.014318453148007393\n",
      "Epoch 1759, Loss: 0.07651955634355545, Final Batch Loss: 0.04762309044599533\n",
      "Epoch 1760, Loss: 0.07670242711901665, Final Batch Loss: 0.05515390262007713\n",
      "Epoch 1761, Loss: 0.20400532335042953, Final Batch Loss: 0.14706586301326752\n",
      "Epoch 1762, Loss: 0.2203371562063694, Final Batch Loss: 0.17629916965961456\n",
      "Epoch 1763, Loss: 0.05092946533113718, Final Batch Loss: 0.011585437692701817\n",
      "Epoch 1764, Loss: 0.08979305624961853, Final Batch Loss: 0.029982220381498337\n",
      "Epoch 1765, Loss: 0.07773714140057564, Final Batch Loss: 0.042063646018505096\n",
      "Epoch 1766, Loss: 0.15868887677788734, Final Batch Loss: 0.1124296560883522\n",
      "Epoch 1767, Loss: 0.13777083531022072, Final Batch Loss: 0.09550456702709198\n",
      "Epoch 1768, Loss: 0.09442353248596191, Final Batch Loss: 0.030591905117034912\n",
      "Epoch 1769, Loss: 0.06761121936142445, Final Batch Loss: 0.02501676417887211\n",
      "Epoch 1770, Loss: 0.08598457649350166, Final Batch Loss: 0.041074834764003754\n",
      "Epoch 1771, Loss: 0.13463641703128815, Final Batch Loss: 0.03467903286218643\n",
      "Epoch 1772, Loss: 0.0834704078733921, Final Batch Loss: 0.04085616394877434\n",
      "Epoch 1773, Loss: 0.027180457022041082, Final Batch Loss: 0.006996212061494589\n",
      "Epoch 1774, Loss: 0.08034980297088623, Final Batch Loss: 0.03781624138355255\n",
      "Epoch 1775, Loss: 0.12398586049675941, Final Batch Loss: 0.08184753358364105\n",
      "Epoch 1776, Loss: 0.14334584027528763, Final Batch Loss: 0.07976708561182022\n",
      "Epoch 1777, Loss: 0.06564116664230824, Final Batch Loss: 0.025378579273819923\n",
      "Epoch 1778, Loss: 0.051422735676169395, Final Batch Loss: 0.02990770898759365\n",
      "Epoch 1779, Loss: 0.09395322576165199, Final Batch Loss: 0.05229515582323074\n",
      "Epoch 1780, Loss: 0.04341321066021919, Final Batch Loss: 0.025697194039821625\n",
      "Epoch 1781, Loss: 0.15346273221075535, Final Batch Loss: 0.12466038763523102\n",
      "Epoch 1782, Loss: 0.11119777336716652, Final Batch Loss: 0.05228060483932495\n",
      "Epoch 1783, Loss: 0.06215717829763889, Final Batch Loss: 0.01330089382827282\n",
      "Epoch 1784, Loss: 0.15258730202913284, Final Batch Loss: 0.08948081731796265\n",
      "Epoch 1785, Loss: 0.10553955659270287, Final Batch Loss: 0.06206690892577171\n",
      "Epoch 1786, Loss: 0.1356159783899784, Final Batch Loss: 0.07642649114131927\n",
      "Epoch 1787, Loss: 0.059171659871935844, Final Batch Loss: 0.029321640729904175\n",
      "Epoch 1788, Loss: 0.0470116026699543, Final Batch Loss: 0.015909623354673386\n",
      "Epoch 1789, Loss: 0.07190313562750816, Final Batch Loss: 0.04682348296046257\n",
      "Epoch 1790, Loss: 0.0853961929678917, Final Batch Loss: 0.02522239089012146\n",
      "Epoch 1791, Loss: 0.05217509809881449, Final Batch Loss: 0.007886399514973164\n",
      "Epoch 1792, Loss: 0.09915942512452602, Final Batch Loss: 0.02226233296096325\n",
      "Epoch 1793, Loss: 0.08105922304093838, Final Batch Loss: 0.023470284417271614\n",
      "Epoch 1794, Loss: 0.1286640577018261, Final Batch Loss: 0.07418517768383026\n",
      "Epoch 1795, Loss: 0.07105552032589912, Final Batch Loss: 0.027348186820745468\n",
      "Epoch 1796, Loss: 0.10843895003199577, Final Batch Loss: 0.04823824390769005\n",
      "Epoch 1797, Loss: 0.11411536484956741, Final Batch Loss: 0.03945595771074295\n",
      "Epoch 1798, Loss: 0.05696318671107292, Final Batch Loss: 0.03441334515810013\n",
      "Epoch 1799, Loss: 0.10446716845035553, Final Batch Loss: 0.023263201117515564\n",
      "Epoch 1800, Loss: 0.0423181988298893, Final Batch Loss: 0.021148841828107834\n",
      "Epoch 1801, Loss: 0.132038664072752, Final Batch Loss: 0.089288130402565\n",
      "Epoch 1802, Loss: 0.07159047573804855, Final Batch Loss: 0.036692067980766296\n",
      "Epoch 1803, Loss: 0.10101978480815887, Final Batch Loss: 0.0500703789293766\n",
      "Epoch 1804, Loss: 0.15842370316386223, Final Batch Loss: 0.10355734080076218\n",
      "Epoch 1805, Loss: 0.10112931951880455, Final Batch Loss: 0.0496433787047863\n",
      "Epoch 1806, Loss: 0.10309001989662647, Final Batch Loss: 0.07666852325201035\n",
      "Epoch 1807, Loss: 0.05408250633627176, Final Batch Loss: 0.01275740284472704\n",
      "Epoch 1808, Loss: 0.17703144252300262, Final Batch Loss: 0.08161357790231705\n",
      "Epoch 1809, Loss: 0.06140836328268051, Final Batch Loss: 0.042954105883836746\n",
      "Epoch 1810, Loss: 0.050200268626213074, Final Batch Loss: 0.012280609458684921\n",
      "Epoch 1811, Loss: 0.09589236043393612, Final Batch Loss: 0.025389818474650383\n",
      "Epoch 1812, Loss: 0.11654405295848846, Final Batch Loss: 0.07473909109830856\n",
      "Epoch 1813, Loss: 0.06470732763409615, Final Batch Loss: 0.0374617800116539\n",
      "Epoch 1814, Loss: 0.06404503993690014, Final Batch Loss: 0.022707173600792885\n",
      "Epoch 1815, Loss: 0.06985028088092804, Final Batch Loss: 0.03206414356827736\n",
      "Epoch 1816, Loss: 0.12490088865160942, Final Batch Loss: 0.06333444267511368\n",
      "Epoch 1817, Loss: 0.09468497335910797, Final Batch Loss: 0.03463687747716904\n",
      "Epoch 1818, Loss: 0.10591859742999077, Final Batch Loss: 0.08075602352619171\n",
      "Epoch 1819, Loss: 0.09954807162284851, Final Batch Loss: 0.0455012246966362\n",
      "Epoch 1820, Loss: 0.08280464634299278, Final Batch Loss: 0.052828099578619\n",
      "Epoch 1821, Loss: 0.06856393441557884, Final Batch Loss: 0.03711217641830444\n",
      "Epoch 1822, Loss: 0.04554307460784912, Final Batch Loss: 0.019816197454929352\n",
      "Epoch 1823, Loss: 0.13204321265220642, Final Batch Loss: 0.044022515416145325\n",
      "Epoch 1824, Loss: 0.0416118036955595, Final Batch Loss: 0.025155754759907722\n",
      "Epoch 1825, Loss: 0.11303982883691788, Final Batch Loss: 0.07481571286916733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1826, Loss: 0.05025172978639603, Final Batch Loss: 0.02217179909348488\n",
      "Epoch 1827, Loss: 0.10029976442456245, Final Batch Loss: 0.04988354444503784\n",
      "Epoch 1828, Loss: 0.0912269614636898, Final Batch Loss: 0.05976276844739914\n",
      "Epoch 1829, Loss: 0.09824764914810658, Final Batch Loss: 0.02411247231066227\n",
      "Epoch 1830, Loss: 0.036505104042589664, Final Batch Loss: 0.014627351425588131\n",
      "Epoch 1831, Loss: 0.08602968603372574, Final Batch Loss: 0.04556667432188988\n",
      "Epoch 1832, Loss: 0.07212288677692413, Final Batch Loss: 0.03423153609037399\n",
      "Epoch 1833, Loss: 0.08542852848768234, Final Batch Loss: 0.0596102736890316\n",
      "Epoch 1834, Loss: 0.06566966697573662, Final Batch Loss: 0.014793302863836288\n",
      "Epoch 1835, Loss: 0.08136776462197304, Final Batch Loss: 0.0346212312579155\n",
      "Epoch 1836, Loss: 0.07629469595849514, Final Batch Loss: 0.01825159229338169\n",
      "Epoch 1837, Loss: 0.12214426323771477, Final Batch Loss: 0.047032441943883896\n",
      "Epoch 1838, Loss: 0.07975404895842075, Final Batch Loss: 0.02345055155456066\n",
      "Epoch 1839, Loss: 0.11373905092477798, Final Batch Loss: 0.0615294948220253\n",
      "Epoch 1840, Loss: 0.13529600575566292, Final Batch Loss: 0.09491720050573349\n",
      "Epoch 1841, Loss: 0.08298611268401146, Final Batch Loss: 0.0390327014029026\n",
      "Epoch 1842, Loss: 0.08773914538323879, Final Batch Loss: 0.019994085654616356\n",
      "Epoch 1843, Loss: 0.0360116520896554, Final Batch Loss: 0.012087526731193066\n",
      "Epoch 1844, Loss: 0.05521713010966778, Final Batch Loss: 0.035330649465322495\n",
      "Epoch 1845, Loss: 0.13083375990390778, Final Batch Loss: 0.09002973884344101\n",
      "Epoch 1846, Loss: 0.14376386627554893, Final Batch Loss: 0.09422080963850021\n",
      "Epoch 1847, Loss: 0.06724673137068748, Final Batch Loss: 0.0193791463971138\n",
      "Epoch 1848, Loss: 0.06792985461652279, Final Batch Loss: 0.03101361356675625\n",
      "Epoch 1849, Loss: 0.08601517975330353, Final Batch Loss: 0.05176686868071556\n",
      "Epoch 1850, Loss: 0.11193131655454636, Final Batch Loss: 0.07190829515457153\n",
      "Epoch 1851, Loss: 0.09192785620689392, Final Batch Loss: 0.019651047885417938\n",
      "Epoch 1852, Loss: 0.06651427783071995, Final Batch Loss: 0.023027801886200905\n",
      "Epoch 1853, Loss: 0.11829198896884918, Final Batch Loss: 0.08126258850097656\n",
      "Epoch 1854, Loss: 0.05265462398529053, Final Batch Loss: 0.034863971173763275\n",
      "Epoch 1855, Loss: 0.049913935363292694, Final Batch Loss: 0.01989574357867241\n",
      "Epoch 1856, Loss: 0.059646745678037405, Final Batch Loss: 0.004721285309642553\n",
      "Epoch 1857, Loss: 0.04254111275076866, Final Batch Loss: 0.014004010707139969\n",
      "Epoch 1858, Loss: 0.049669500440359116, Final Batch Loss: 0.01854107156395912\n",
      "Epoch 1859, Loss: 0.06609062477946281, Final Batch Loss: 0.03087092563509941\n",
      "Epoch 1860, Loss: 0.14969860389828682, Final Batch Loss: 0.09169698506593704\n",
      "Epoch 1861, Loss: 0.06619771756231785, Final Batch Loss: 0.02808411233127117\n",
      "Epoch 1862, Loss: 0.05282043293118477, Final Batch Loss: 0.0232536718249321\n",
      "Epoch 1863, Loss: 0.12134291231632233, Final Batch Loss: 0.09690128266811371\n",
      "Epoch 1864, Loss: 0.07424329593777657, Final Batch Loss: 0.019637078046798706\n",
      "Epoch 1865, Loss: 0.10796234011650085, Final Batch Loss: 0.05897543206810951\n",
      "Epoch 1866, Loss: 0.054850210435688496, Final Batch Loss: 0.012140520848333836\n",
      "Epoch 1867, Loss: 0.0660132858902216, Final Batch Loss: 0.026199152693152428\n",
      "Epoch 1868, Loss: 0.06293096020817757, Final Batch Loss: 0.024569954723119736\n",
      "Epoch 1869, Loss: 0.16455502063035965, Final Batch Loss: 0.09817731380462646\n",
      "Epoch 1870, Loss: 0.14636909030377865, Final Batch Loss: 0.1277037113904953\n",
      "Epoch 1871, Loss: 0.049385905265808105, Final Batch Loss: 0.023693254217505455\n",
      "Epoch 1872, Loss: 0.03912374936044216, Final Batch Loss: 0.017170969396829605\n",
      "Epoch 1873, Loss: 0.09282864816486835, Final Batch Loss: 0.012884298339486122\n",
      "Epoch 1874, Loss: 0.07500893622636795, Final Batch Loss: 0.055905699729919434\n",
      "Epoch 1875, Loss: 0.07350842654705048, Final Batch Loss: 0.032052747905254364\n",
      "Epoch 1876, Loss: 0.0416770800948143, Final Batch Loss: 0.025613903999328613\n",
      "Epoch 1877, Loss: 0.15197587758302689, Final Batch Loss: 0.11106803268194199\n",
      "Epoch 1878, Loss: 0.10769598372280598, Final Batch Loss: 0.07785750925540924\n",
      "Epoch 1879, Loss: 0.06893444806337357, Final Batch Loss: 0.028396930545568466\n",
      "Epoch 1880, Loss: 0.03380238451063633, Final Batch Loss: 0.011368798092007637\n",
      "Epoch 1881, Loss: 0.0369021175429225, Final Batch Loss: 0.011220588348805904\n",
      "Epoch 1882, Loss: 0.09015069901943207, Final Batch Loss: 0.058464791625738144\n",
      "Epoch 1883, Loss: 0.09475378692150116, Final Batch Loss: 0.04794805124402046\n",
      "Epoch 1884, Loss: 0.0969894528388977, Final Batch Loss: 0.048943355679512024\n",
      "Epoch 1885, Loss: 0.08674639835953712, Final Batch Loss: 0.03302350640296936\n",
      "Epoch 1886, Loss: 0.1289413906633854, Final Batch Loss: 0.05051982030272484\n",
      "Epoch 1887, Loss: 0.08483447041362524, Final Batch Loss: 0.0094961142167449\n",
      "Epoch 1888, Loss: 0.054799577221274376, Final Batch Loss: 0.03083747625350952\n",
      "Epoch 1889, Loss: 0.05585248302668333, Final Batch Loss: 0.015610176138579845\n",
      "Epoch 1890, Loss: 0.11731516942381859, Final Batch Loss: 0.03348461911082268\n",
      "Epoch 1891, Loss: 0.10775129869580269, Final Batch Loss: 0.05487441271543503\n",
      "Epoch 1892, Loss: 0.053133318200707436, Final Batch Loss: 0.03123142570257187\n",
      "Epoch 1893, Loss: 0.09164091944694519, Final Batch Loss: 0.058046068996191025\n",
      "Epoch 1894, Loss: 0.09224650636315346, Final Batch Loss: 0.019418220967054367\n",
      "Epoch 1895, Loss: 0.10682345367968082, Final Batch Loss: 0.02005770243704319\n",
      "Epoch 1896, Loss: 0.057594314217567444, Final Batch Loss: 0.023475006222724915\n",
      "Epoch 1897, Loss: 0.09408793598413467, Final Batch Loss: 0.02866216003894806\n",
      "Epoch 1898, Loss: 0.037167894653975964, Final Batch Loss: 0.010167012922465801\n",
      "Epoch 1899, Loss: 0.09535476565361023, Final Batch Loss: 0.059328362345695496\n",
      "Epoch 1900, Loss: 0.0392786618322134, Final Batch Loss: 0.01715129241347313\n",
      "Epoch 1901, Loss: 0.07078198902308941, Final Batch Loss: 0.02106228657066822\n",
      "Epoch 1902, Loss: 0.0504832835868001, Final Batch Loss: 0.004861858673393726\n",
      "Epoch 1903, Loss: 0.08379765599966049, Final Batch Loss: 0.051269758492708206\n",
      "Epoch 1904, Loss: 0.11744387075304985, Final Batch Loss: 0.04509526863694191\n",
      "Epoch 1905, Loss: 0.05229186546057463, Final Batch Loss: 0.014660513959825039\n",
      "Epoch 1906, Loss: 0.07907966524362564, Final Batch Loss: 0.041490331292152405\n",
      "Epoch 1907, Loss: 0.07194393500685692, Final Batch Loss: 0.05041315406560898\n",
      "Epoch 1908, Loss: 0.04012139141559601, Final Batch Loss: 0.02202046476304531\n",
      "Epoch 1909, Loss: 0.06107993423938751, Final Batch Loss: 0.031205782666802406\n",
      "Epoch 1910, Loss: 0.12477095425128937, Final Batch Loss: 0.0840785801410675\n",
      "Epoch 1911, Loss: 0.07664369978010654, Final Batch Loss: 0.025430919602513313\n",
      "Epoch 1912, Loss: 0.06089925207197666, Final Batch Loss: 0.022502673789858818\n",
      "Epoch 1913, Loss: 0.055667564272880554, Final Batch Loss: 0.02229936793446541\n",
      "Epoch 1914, Loss: 0.07877369225025177, Final Batch Loss: 0.04687562212347984\n",
      "Epoch 1915, Loss: 0.08154643513262272, Final Batch Loss: 0.0566028356552124\n",
      "Epoch 1916, Loss: 0.07595975138247013, Final Batch Loss: 0.013564525172114372\n",
      "Epoch 1917, Loss: 0.0875858012586832, Final Batch Loss: 0.028076963499188423\n",
      "Epoch 1918, Loss: 0.08363888412714005, Final Batch Loss: 0.039994191378355026\n",
      "Epoch 1919, Loss: 0.24996692687273026, Final Batch Loss: 0.1980002522468567\n",
      "Epoch 1920, Loss: 0.06967543996870518, Final Batch Loss: 0.04015693813562393\n",
      "Epoch 1921, Loss: 0.10121069476008415, Final Batch Loss: 0.04870816692709923\n",
      "Epoch 1922, Loss: 0.09730532113462687, Final Batch Loss: 0.003998563624918461\n",
      "Epoch 1923, Loss: 0.04295693710446358, Final Batch Loss: 0.015724048018455505\n",
      "Epoch 1924, Loss: 0.0805584117770195, Final Batch Loss: 0.026375330984592438\n",
      "Epoch 1925, Loss: 0.03509917017072439, Final Batch Loss: 0.005994441919028759\n",
      "Epoch 1926, Loss: 0.08485753834247589, Final Batch Loss: 0.0249139703810215\n",
      "Epoch 1927, Loss: 0.08200684748589993, Final Batch Loss: 0.02344377525150776\n",
      "Epoch 1928, Loss: 0.12672047317028046, Final Batch Loss: 0.0566910058259964\n",
      "Epoch 1929, Loss: 0.09649563208222389, Final Batch Loss: 0.020324435085058212\n",
      "Epoch 1930, Loss: 0.0860772468149662, Final Batch Loss: 0.012762155383825302\n",
      "Epoch 1931, Loss: 0.08556413650512695, Final Batch Loss: 0.04880887642502785\n",
      "Epoch 1932, Loss: 0.15883706882596016, Final Batch Loss: 0.1161814033985138\n",
      "Epoch 1933, Loss: 0.04140690341591835, Final Batch Loss: 0.016240620985627174\n",
      "Epoch 1934, Loss: 0.06659515388309956, Final Batch Loss: 0.018623685464262962\n",
      "Epoch 1935, Loss: 0.08139160135760903, Final Batch Loss: 0.00690580764785409\n",
      "Epoch 1936, Loss: 0.0913645327091217, Final Batch Loss: 0.05323793739080429\n",
      "Epoch 1937, Loss: 0.11642809212207794, Final Batch Loss: 0.08153937011957169\n",
      "Epoch 1938, Loss: 0.04690312221646309, Final Batch Loss: 0.022325055673718452\n",
      "Epoch 1939, Loss: 0.0477581974118948, Final Batch Loss: 0.020371025428175926\n",
      "Epoch 1940, Loss: 0.053090473636984825, Final Batch Loss: 0.01894422061741352\n",
      "Epoch 1941, Loss: 0.09746583923697472, Final Batch Loss: 0.056197989732027054\n",
      "Epoch 1942, Loss: 0.06789744645357132, Final Batch Loss: 0.04842735454440117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1943, Loss: 0.06428069435060024, Final Batch Loss: 0.023699132725596428\n",
      "Epoch 1944, Loss: 0.137071518227458, Final Batch Loss: 0.10880033671855927\n",
      "Epoch 1945, Loss: 0.05304937623441219, Final Batch Loss: 0.03124101646244526\n",
      "Epoch 1946, Loss: 0.07792316749691963, Final Batch Loss: 0.01886240392923355\n",
      "Epoch 1947, Loss: 0.10385611280798912, Final Batch Loss: 0.08214452117681503\n",
      "Epoch 1948, Loss: 0.05709540657699108, Final Batch Loss: 0.030679790303111076\n",
      "Epoch 1949, Loss: 0.10101262480020523, Final Batch Loss: 0.0542897954583168\n",
      "Epoch 1950, Loss: 0.04330448713153601, Final Batch Loss: 0.012092244811356068\n",
      "Epoch 1951, Loss: 0.10901312902569771, Final Batch Loss: 0.045324746519327164\n",
      "Epoch 1952, Loss: 0.07261195778846741, Final Batch Loss: 0.03232154622673988\n",
      "Epoch 1953, Loss: 0.0830163024365902, Final Batch Loss: 0.053339868783950806\n",
      "Epoch 1954, Loss: 0.03549398388713598, Final Batch Loss: 0.02010522410273552\n",
      "Epoch 1955, Loss: 0.07166561484336853, Final Batch Loss: 0.030255645513534546\n",
      "Epoch 1956, Loss: 0.041269326116889715, Final Batch Loss: 0.006651637610048056\n",
      "Epoch 1957, Loss: 0.09732709731906652, Final Batch Loss: 0.008920415304601192\n",
      "Epoch 1958, Loss: 0.09979468956589699, Final Batch Loss: 0.03864463418722153\n",
      "Epoch 1959, Loss: 0.0541271697729826, Final Batch Loss: 0.016666660085320473\n",
      "Epoch 1960, Loss: 0.04573497734963894, Final Batch Loss: 0.028980540111660957\n",
      "Epoch 1961, Loss: 0.07142945006489754, Final Batch Loss: 0.045249488204717636\n",
      "Epoch 1962, Loss: 0.04598208796232939, Final Batch Loss: 0.010026647709310055\n",
      "Epoch 1963, Loss: 0.11373735778033733, Final Batch Loss: 0.02458873949944973\n",
      "Epoch 1964, Loss: 0.05991970933973789, Final Batch Loss: 0.030156677588820457\n",
      "Epoch 1965, Loss: 0.11630826443433762, Final Batch Loss: 0.05069121718406677\n",
      "Epoch 1966, Loss: 0.06283326633274555, Final Batch Loss: 0.040895089507102966\n",
      "Epoch 1967, Loss: 0.03187005966901779, Final Batch Loss: 0.01875290274620056\n",
      "Epoch 1968, Loss: 0.08306510373950005, Final Batch Loss: 0.02331896498799324\n",
      "Epoch 1969, Loss: 0.06584629788994789, Final Batch Loss: 0.009970970451831818\n",
      "Epoch 1970, Loss: 0.0682852528989315, Final Batch Loss: 0.028074488043785095\n",
      "Epoch 1971, Loss: 0.05045082978904247, Final Batch Loss: 0.021922679618000984\n",
      "Epoch 1972, Loss: 0.041901057586073875, Final Batch Loss: 0.02060350961983204\n",
      "Epoch 1973, Loss: 0.06028551794588566, Final Batch Loss: 0.014592690393328667\n",
      "Epoch 1974, Loss: 0.07802978157997131, Final Batch Loss: 0.038664501160383224\n",
      "Epoch 1975, Loss: 0.05131741426885128, Final Batch Loss: 0.0288632120937109\n",
      "Epoch 1976, Loss: 0.04637812962755561, Final Batch Loss: 0.007042029406875372\n",
      "Epoch 1977, Loss: 0.06561547331511974, Final Batch Loss: 0.043113622814416885\n",
      "Epoch 1978, Loss: 0.10399924591183662, Final Batch Loss: 0.06947704404592514\n",
      "Epoch 1979, Loss: 0.11307643353939056, Final Batch Loss: 0.073656365275383\n",
      "Epoch 1980, Loss: 0.042102182283997536, Final Batch Loss: 0.011712228879332542\n",
      "Epoch 1981, Loss: 0.039723461493849754, Final Batch Loss: 0.020519211888313293\n",
      "Epoch 1982, Loss: 0.09962484985589981, Final Batch Loss: 0.06577665358781815\n",
      "Epoch 1983, Loss: 0.0916617289185524, Final Batch Loss: 0.04835709184408188\n",
      "Epoch 1984, Loss: 0.13240177929401398, Final Batch Loss: 0.04899679869413376\n",
      "Epoch 1985, Loss: 0.1706313118338585, Final Batch Loss: 0.07941924780607224\n",
      "Epoch 1986, Loss: 0.07992787659168243, Final Batch Loss: 0.03818807005882263\n",
      "Epoch 1987, Loss: 0.0452705230563879, Final Batch Loss: 0.029670024290680885\n",
      "Epoch 1988, Loss: 0.053240254521369934, Final Batch Loss: 0.032144591212272644\n",
      "Epoch 1989, Loss: 0.06863428466022015, Final Batch Loss: 0.038830988109111786\n",
      "Epoch 1990, Loss: 0.15951814129948616, Final Batch Loss: 0.1049661636352539\n",
      "Epoch 1991, Loss: 0.05847509391605854, Final Batch Loss: 0.027201639488339424\n",
      "Epoch 1992, Loss: 0.03721435647457838, Final Batch Loss: 0.010412846691906452\n",
      "Epoch 1993, Loss: 0.0450265109539032, Final Batch Loss: 0.01089656725525856\n",
      "Epoch 1994, Loss: 0.06449490040540695, Final Batch Loss: 0.019357748329639435\n",
      "Epoch 1995, Loss: 0.09402467124164104, Final Batch Loss: 0.07373775541782379\n",
      "Epoch 1996, Loss: 0.0743912123143673, Final Batch Loss: 0.02860352024435997\n",
      "Epoch 1997, Loss: 0.09941147826611996, Final Batch Loss: 0.07519102841615677\n",
      "Epoch 1998, Loss: 0.044884782284498215, Final Batch Loss: 0.025246987119317055\n",
      "Epoch 1999, Loss: 0.1324017383158207, Final Batch Loss: 0.11053164303302765\n",
      "Epoch 2000, Loss: 0.05694541148841381, Final Batch Loss: 0.028194742277264595\n",
      "Epoch 2001, Loss: 0.14485373347997665, Final Batch Loss: 0.11180684715509415\n",
      "Epoch 2002, Loss: 0.13811512012034655, Final Batch Loss: 0.00990017969161272\n",
      "Epoch 2003, Loss: 0.15712372586131096, Final Batch Loss: 0.10248782485723495\n",
      "Epoch 2004, Loss: 0.17720690742135048, Final Batch Loss: 0.13936659693717957\n",
      "Epoch 2005, Loss: 0.08880522847175598, Final Batch Loss: 0.04316346347332001\n",
      "Epoch 2006, Loss: 0.0685238316655159, Final Batch Loss: 0.015875983983278275\n",
      "Epoch 2007, Loss: 0.058948054909706116, Final Batch Loss: 0.014518998563289642\n",
      "Epoch 2008, Loss: 0.05302052944898605, Final Batch Loss: 0.018531091511249542\n",
      "Epoch 2009, Loss: 0.07180265337228775, Final Batch Loss: 0.044220566749572754\n",
      "Epoch 2010, Loss: 0.13263068720698357, Final Batch Loss: 0.09318596124649048\n",
      "Epoch 2011, Loss: 0.11410673707723618, Final Batch Loss: 0.051810167729854584\n",
      "Epoch 2012, Loss: 0.12034580297768116, Final Batch Loss: 0.10407422482967377\n",
      "Epoch 2013, Loss: 0.06262090429663658, Final Batch Loss: 0.027864836156368256\n",
      "Epoch 2014, Loss: 0.07159487437456846, Final Batch Loss: 0.05693919211626053\n",
      "Epoch 2015, Loss: 0.10268933698534966, Final Batch Loss: 0.06372474879026413\n",
      "Epoch 2016, Loss: 0.08214681409299374, Final Batch Loss: 0.05834827944636345\n",
      "Epoch 2017, Loss: 0.035035280510783195, Final Batch Loss: 0.01776309870183468\n",
      "Epoch 2018, Loss: 0.07417308911681175, Final Batch Loss: 0.04136020317673683\n",
      "Epoch 2019, Loss: 0.07842754200100899, Final Batch Loss: 0.045971304178237915\n",
      "Epoch 2020, Loss: 0.06750287860631943, Final Batch Loss: 0.022684603929519653\n",
      "Epoch 2021, Loss: 0.10147330164909363, Final Batch Loss: 0.07879558950662613\n",
      "Epoch 2022, Loss: 0.12109232321381569, Final Batch Loss: 0.08140332996845245\n",
      "Epoch 2023, Loss: 0.05751156434416771, Final Batch Loss: 0.03253868222236633\n",
      "Epoch 2024, Loss: 0.1387241892516613, Final Batch Loss: 0.1131427213549614\n",
      "Epoch 2025, Loss: 0.06628364883363247, Final Batch Loss: 0.012640422210097313\n",
      "Epoch 2026, Loss: 0.05996622331440449, Final Batch Loss: 0.03406565263867378\n",
      "Epoch 2027, Loss: 0.046645376831293106, Final Batch Loss: 0.012469720095396042\n",
      "Epoch 2028, Loss: 0.08235249668359756, Final Batch Loss: 0.05360180512070656\n",
      "Epoch 2029, Loss: 0.09903698414564133, Final Batch Loss: 0.06497158110141754\n",
      "Epoch 2030, Loss: 0.0612990353256464, Final Batch Loss: 0.03959006443619728\n",
      "Epoch 2031, Loss: 0.06263516005128622, Final Batch Loss: 0.008690829388797283\n",
      "Epoch 2032, Loss: 0.10539959743618965, Final Batch Loss: 0.081365667283535\n",
      "Epoch 2033, Loss: 0.08570216968655586, Final Batch Loss: 0.05145355686545372\n",
      "Epoch 2034, Loss: 0.06463026814162731, Final Batch Loss: 0.03480140119791031\n",
      "Epoch 2035, Loss: 0.056276313960552216, Final Batch Loss: 0.0138714499771595\n",
      "Epoch 2036, Loss: 0.09846413508057594, Final Batch Loss: 0.027330901473760605\n",
      "Epoch 2037, Loss: 0.05730140395462513, Final Batch Loss: 0.02348228730261326\n",
      "Epoch 2038, Loss: 0.04548138566315174, Final Batch Loss: 0.019341396167874336\n",
      "Epoch 2039, Loss: 0.11876792088150978, Final Batch Loss: 0.052631061524152756\n",
      "Epoch 2040, Loss: 0.15960169956088066, Final Batch Loss: 0.12104835361242294\n",
      "Epoch 2041, Loss: 0.062344254925847054, Final Batch Loss: 0.026709144935011864\n",
      "Epoch 2042, Loss: 0.06454741954803467, Final Batch Loss: 0.03148361295461655\n",
      "Epoch 2043, Loss: 0.06186377629637718, Final Batch Loss: 0.025025226175785065\n",
      "Epoch 2044, Loss: 0.067805465310812, Final Batch Loss: 0.03875529393553734\n",
      "Epoch 2045, Loss: 0.115887600928545, Final Batch Loss: 0.0643596276640892\n",
      "Epoch 2046, Loss: 0.10593806207180023, Final Batch Loss: 0.05122433230280876\n",
      "Epoch 2047, Loss: 0.06597485207021236, Final Batch Loss: 0.0277132336050272\n",
      "Epoch 2048, Loss: 0.06639096885919571, Final Batch Loss: 0.04466624930500984\n",
      "Epoch 2049, Loss: 0.06790788285434246, Final Batch Loss: 0.020015740767121315\n",
      "Epoch 2050, Loss: 0.03836617432534695, Final Batch Loss: 0.006847912445664406\n",
      "Epoch 2051, Loss: 0.21754971891641617, Final Batch Loss: 0.15414714813232422\n",
      "Epoch 2052, Loss: 0.08623301796615124, Final Batch Loss: 0.014020262286067009\n",
      "Epoch 2053, Loss: 0.05181089974939823, Final Batch Loss: 0.03167644143104553\n",
      "Epoch 2054, Loss: 0.072510477155447, Final Batch Loss: 0.035051461309194565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2055, Loss: 0.09481724724173546, Final Batch Loss: 0.061592116951942444\n",
      "Epoch 2056, Loss: 0.07192600518465042, Final Batch Loss: 0.032443705946207047\n",
      "Epoch 2057, Loss: 0.03875435050576925, Final Batch Loss: 0.012547125108540058\n",
      "Epoch 2058, Loss: 0.04313913360238075, Final Batch Loss: 0.0183983463793993\n",
      "Epoch 2059, Loss: 0.10573548451066017, Final Batch Loss: 0.057754505425691605\n",
      "Epoch 2060, Loss: 0.01995676849037409, Final Batch Loss: 0.005224118009209633\n",
      "Epoch 2061, Loss: 0.0719323381781578, Final Batch Loss: 0.04792797192931175\n",
      "Epoch 2062, Loss: 0.11677806079387665, Final Batch Loss: 0.08331165462732315\n",
      "Epoch 2063, Loss: 0.05639209225773811, Final Batch Loss: 0.019750334322452545\n",
      "Epoch 2064, Loss: 0.06971758417785168, Final Batch Loss: 0.042420580983161926\n",
      "Epoch 2065, Loss: 0.07842740416526794, Final Batch Loss: 0.056771956384181976\n",
      "Epoch 2066, Loss: 0.03490623366087675, Final Batch Loss: 0.009003330953419209\n",
      "Epoch 2067, Loss: 0.11138736829161644, Final Batch Loss: 0.03635849431157112\n",
      "Epoch 2068, Loss: 0.06660319864749908, Final Batch Loss: 0.03299124538898468\n",
      "Epoch 2069, Loss: 0.06836340390145779, Final Batch Loss: 0.03896600380539894\n",
      "Epoch 2070, Loss: 0.129825621843338, Final Batch Loss: 0.09629583358764648\n",
      "Epoch 2071, Loss: 0.12600189074873924, Final Batch Loss: 0.07204369455575943\n",
      "Epoch 2072, Loss: 0.03868746478110552, Final Batch Loss: 0.014906574971973896\n",
      "Epoch 2073, Loss: 0.06187596917152405, Final Batch Loss: 0.016086295247077942\n",
      "Epoch 2074, Loss: 0.058574666269123554, Final Batch Loss: 0.008378763683140278\n",
      "Epoch 2075, Loss: 0.1349828988313675, Final Batch Loss: 0.06385048478841782\n",
      "Epoch 2076, Loss: 0.03584467992186546, Final Batch Loss: 0.018002115190029144\n",
      "Epoch 2077, Loss: 0.07762543298304081, Final Batch Loss: 0.013111719861626625\n",
      "Epoch 2078, Loss: 0.06272585596889257, Final Batch Loss: 0.014759446494281292\n",
      "Epoch 2079, Loss: 0.0543306777253747, Final Batch Loss: 0.010157858021557331\n",
      "Epoch 2080, Loss: 0.062117740511894226, Final Batch Loss: 0.015996500849723816\n",
      "Epoch 2081, Loss: 0.027212217450141907, Final Batch Loss: 0.010510940104722977\n",
      "Epoch 2082, Loss: 0.03972639515995979, Final Batch Loss: 0.020293548703193665\n",
      "Epoch 2083, Loss: 0.09183792024850845, Final Batch Loss: 0.03523608669638634\n",
      "Epoch 2084, Loss: 0.07237248308956623, Final Batch Loss: 0.012346366420388222\n",
      "Epoch 2085, Loss: 0.04524912312626839, Final Batch Loss: 0.02856588549911976\n",
      "Epoch 2086, Loss: 0.07826459594070911, Final Batch Loss: 0.05993970111012459\n",
      "Epoch 2087, Loss: 0.07127823121845722, Final Batch Loss: 0.04611954465508461\n",
      "Epoch 2088, Loss: 0.07092983089387417, Final Batch Loss: 0.029604142531752586\n",
      "Epoch 2089, Loss: 0.053541846573352814, Final Batch Loss: 0.03116033598780632\n",
      "Epoch 2090, Loss: 0.0886908769607544, Final Batch Loss: 0.0373249314725399\n",
      "Epoch 2091, Loss: 0.1071824673563242, Final Batch Loss: 0.023333759978413582\n",
      "Epoch 2092, Loss: 0.04907553270459175, Final Batch Loss: 0.024815591052174568\n",
      "Epoch 2093, Loss: 0.12517422065138817, Final Batch Loss: 0.05089284107089043\n",
      "Epoch 2094, Loss: 0.12414964661002159, Final Batch Loss: 0.07594374567270279\n",
      "Epoch 2095, Loss: 0.023886699229478836, Final Batch Loss: 0.008534185588359833\n",
      "Epoch 2096, Loss: 0.046952564269304276, Final Batch Loss: 0.022281713783740997\n",
      "Epoch 2097, Loss: 0.06402236595749855, Final Batch Loss: 0.016365252435207367\n",
      "Epoch 2098, Loss: 0.11729156225919724, Final Batch Loss: 0.0662965402007103\n",
      "Epoch 2099, Loss: 0.040272729471325874, Final Batch Loss: 0.02671300806105137\n",
      "Epoch 2100, Loss: 0.03720809519290924, Final Batch Loss: 0.009939651936292648\n",
      "Epoch 2101, Loss: 0.0641583576798439, Final Batch Loss: 0.025118589401245117\n",
      "Epoch 2102, Loss: 0.0494892206043005, Final Batch Loss: 0.02587173692882061\n",
      "Epoch 2103, Loss: 0.06462866812944412, Final Batch Loss: 0.034664954990148544\n",
      "Epoch 2104, Loss: 0.06282135657966137, Final Batch Loss: 0.03491082787513733\n",
      "Epoch 2105, Loss: 0.05245761573314667, Final Batch Loss: 0.017709743231534958\n",
      "Epoch 2106, Loss: 0.05738406255841255, Final Batch Loss: 0.016427241265773773\n",
      "Epoch 2107, Loss: 0.05891428515315056, Final Batch Loss: 0.031781528145074844\n",
      "Epoch 2108, Loss: 0.056914711371064186, Final Batch Loss: 0.01774565316736698\n",
      "Epoch 2109, Loss: 0.06348087079823017, Final Batch Loss: 0.015369543805718422\n",
      "Epoch 2110, Loss: 0.04153372719883919, Final Batch Loss: 0.010828051716089249\n",
      "Epoch 2111, Loss: 0.08670608699321747, Final Batch Loss: 0.06250878423452377\n",
      "Epoch 2112, Loss: 0.03889011591672897, Final Batch Loss: 0.015390628948807716\n",
      "Epoch 2113, Loss: 0.11062812805175781, Final Batch Loss: 0.08787564933300018\n",
      "Epoch 2114, Loss: 0.058590007945895195, Final Batch Loss: 0.019983330741524696\n",
      "Epoch 2115, Loss: 0.11122336890548468, Final Batch Loss: 0.09678652137517929\n",
      "Epoch 2116, Loss: 0.03500761929899454, Final Batch Loss: 0.01437609363347292\n",
      "Epoch 2117, Loss: 0.06973380967974663, Final Batch Loss: 0.019719164818525314\n",
      "Epoch 2118, Loss: 0.028116654604673386, Final Batch Loss: 0.017414821311831474\n",
      "Epoch 2119, Loss: 0.12046556174755096, Final Batch Loss: 0.07799140363931656\n",
      "Epoch 2120, Loss: 0.13723058439791203, Final Batch Loss: 0.11580255627632141\n",
      "Epoch 2121, Loss: 0.07090692967176437, Final Batch Loss: 0.018626920878887177\n",
      "Epoch 2122, Loss: 0.05033826641738415, Final Batch Loss: 0.026237130165100098\n",
      "Epoch 2123, Loss: 0.0904582291841507, Final Batch Loss: 0.06340359896421432\n",
      "Epoch 2124, Loss: 0.08353163115680218, Final Batch Loss: 0.024980543181300163\n",
      "Epoch 2125, Loss: 0.03731763921678066, Final Batch Loss: 0.009948279708623886\n",
      "Epoch 2126, Loss: 0.12292251735925674, Final Batch Loss: 0.05360942333936691\n",
      "Epoch 2127, Loss: 0.10170593857765198, Final Batch Loss: 0.08423128724098206\n",
      "Epoch 2128, Loss: 0.06452778168022633, Final Batch Loss: 0.03074595518410206\n",
      "Epoch 2129, Loss: 0.07119805924594402, Final Batch Loss: 0.045165516436100006\n",
      "Epoch 2130, Loss: 0.07306727953255177, Final Batch Loss: 0.04359661415219307\n",
      "Epoch 2131, Loss: 0.040140291675925255, Final Batch Loss: 0.01845446415245533\n",
      "Epoch 2132, Loss: 0.04501899424940348, Final Batch Loss: 0.006537632085382938\n",
      "Epoch 2133, Loss: 0.0539862047880888, Final Batch Loss: 0.0286260973662138\n",
      "Epoch 2134, Loss: 0.053190870210528374, Final Batch Loss: 0.024316217750310898\n",
      "Epoch 2135, Loss: 0.034806519746780396, Final Batch Loss: 0.018270401284098625\n",
      "Epoch 2136, Loss: 0.16762964241206646, Final Batch Loss: 0.14158183336257935\n",
      "Epoch 2137, Loss: 0.06030695140361786, Final Batch Loss: 0.024843234568834305\n",
      "Epoch 2138, Loss: 0.10339899361133575, Final Batch Loss: 0.030349470674991608\n",
      "Epoch 2139, Loss: 0.14636823534965515, Final Batch Loss: 0.06909677386283875\n",
      "Epoch 2140, Loss: 0.10906855389475822, Final Batch Loss: 0.07711880654096603\n",
      "Epoch 2141, Loss: 0.1317935660481453, Final Batch Loss: 0.09305372089147568\n",
      "Epoch 2142, Loss: 0.09101109579205513, Final Batch Loss: 0.020268525928258896\n",
      "Epoch 2143, Loss: 0.12235501781105995, Final Batch Loss: 0.08461181074380875\n",
      "Epoch 2144, Loss: 0.1382792890071869, Final Batch Loss: 0.04509933292865753\n",
      "Epoch 2145, Loss: 0.08552784472703934, Final Batch Loss: 0.04253992438316345\n",
      "Epoch 2146, Loss: 0.07460000179708004, Final Batch Loss: 0.02315387688577175\n",
      "Epoch 2147, Loss: 0.09108629077672958, Final Batch Loss: 0.035589877516031265\n",
      "Epoch 2148, Loss: 0.05426622834056616, Final Batch Loss: 0.009972254745662212\n",
      "Epoch 2149, Loss: 0.08287840336561203, Final Batch Loss: 0.02136518433690071\n",
      "Epoch 2150, Loss: 0.0847384762018919, Final Batch Loss: 0.0662572830915451\n",
      "Epoch 2151, Loss: 0.03555359411984682, Final Batch Loss: 0.010038171894848347\n",
      "Epoch 2152, Loss: 0.06158504169434309, Final Batch Loss: 0.006685382686555386\n",
      "Epoch 2153, Loss: 0.05760645493865013, Final Batch Loss: 0.03445976600050926\n",
      "Epoch 2154, Loss: 0.07099911756813526, Final Batch Loss: 0.04563302919268608\n",
      "Epoch 2155, Loss: 0.06848261132836342, Final Batch Loss: 0.01767563447356224\n",
      "Epoch 2156, Loss: 0.08949533104896545, Final Batch Loss: 0.04494946449995041\n",
      "Epoch 2157, Loss: 0.058149353601038456, Final Batch Loss: 0.01136429701000452\n",
      "Epoch 2158, Loss: 0.0781831443309784, Final Batch Loss: 0.02219674363732338\n",
      "Epoch 2159, Loss: 0.043066900223493576, Final Batch Loss: 0.021190958097577095\n",
      "Epoch 2160, Loss: 0.09066042304039001, Final Batch Loss: 0.04274161532521248\n",
      "Epoch 2161, Loss: 0.09316711127758026, Final Batch Loss: 0.05272674560546875\n",
      "Epoch 2162, Loss: 0.0507762897759676, Final Batch Loss: 0.021102191880345345\n",
      "Epoch 2163, Loss: 0.203934321179986, Final Batch Loss: 0.17517293989658356\n",
      "Epoch 2164, Loss: 0.05773010477423668, Final Batch Loss: 0.03715958073735237\n",
      "Epoch 2165, Loss: 0.03330682497471571, Final Batch Loss: 0.011067603714764118\n",
      "Epoch 2166, Loss: 0.03483925433829427, Final Batch Loss: 0.006586740259081125\n",
      "Epoch 2167, Loss: 0.07401709258556366, Final Batch Loss: 0.02603035792708397\n",
      "Epoch 2168, Loss: 0.10507754236459732, Final Batch Loss: 0.07293770462274551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2169, Loss: 0.07630249671638012, Final Batch Loss: 0.027463501319289207\n",
      "Epoch 2170, Loss: 0.04935251921415329, Final Batch Loss: 0.03252759575843811\n",
      "Epoch 2171, Loss: 0.07887814566493034, Final Batch Loss: 0.052521247416734695\n",
      "Epoch 2172, Loss: 0.053246958181262016, Final Batch Loss: 0.018071187660098076\n",
      "Epoch 2173, Loss: 0.030575635842978954, Final Batch Loss: 0.008774642832577229\n",
      "Epoch 2174, Loss: 0.12971260026097298, Final Batch Loss: 0.10494580119848251\n",
      "Epoch 2175, Loss: 0.0543474992737174, Final Batch Loss: 0.014697941951453686\n",
      "Epoch 2176, Loss: 0.04284060327336192, Final Batch Loss: 0.035046763718128204\n",
      "Epoch 2177, Loss: 0.09051530435681343, Final Batch Loss: 0.04071306809782982\n",
      "Epoch 2178, Loss: 0.018645726144313812, Final Batch Loss: 0.005808033049106598\n",
      "Epoch 2179, Loss: 0.0535777248442173, Final Batch Loss: 0.03044954687356949\n",
      "Epoch 2180, Loss: 0.03678739909082651, Final Batch Loss: 0.007286165840923786\n",
      "Epoch 2181, Loss: 0.07972629927098751, Final Batch Loss: 0.02420748956501484\n",
      "Epoch 2182, Loss: 0.0888812318444252, Final Batch Loss: 0.03896873816847801\n",
      "Epoch 2183, Loss: 0.07548222132027149, Final Batch Loss: 0.03075815923511982\n",
      "Epoch 2184, Loss: 0.05720154941082001, Final Batch Loss: 0.02646087482571602\n",
      "Epoch 2185, Loss: 0.08214177004992962, Final Batch Loss: 0.02221747674047947\n",
      "Epoch 2186, Loss: 0.043417125940322876, Final Batch Loss: 0.02146768383681774\n",
      "Epoch 2187, Loss: 0.05608069896697998, Final Batch Loss: 0.01985996961593628\n",
      "Epoch 2188, Loss: 0.10478482395410538, Final Batch Loss: 0.041727736592292786\n",
      "Epoch 2189, Loss: 0.09935670718550682, Final Batch Loss: 0.055330030620098114\n",
      "Epoch 2190, Loss: 0.05224109813570976, Final Batch Loss: 0.022701911628246307\n",
      "Epoch 2191, Loss: 0.0657816119492054, Final Batch Loss: 0.025143492966890335\n",
      "Epoch 2192, Loss: 0.10134543851017952, Final Batch Loss: 0.00809350237250328\n",
      "Epoch 2193, Loss: 0.05158671457320452, Final Batch Loss: 0.015066531486809254\n",
      "Epoch 2194, Loss: 0.1250253152102232, Final Batch Loss: 0.02071571908891201\n",
      "Epoch 2195, Loss: 0.07062111049890518, Final Batch Loss: 0.04644408077001572\n",
      "Epoch 2196, Loss: 0.06332903355360031, Final Batch Loss: 0.03018597885966301\n",
      "Epoch 2197, Loss: 0.10400095954537392, Final Batch Loss: 0.03947721794247627\n",
      "Epoch 2198, Loss: 0.028832961805164814, Final Batch Loss: 0.008744952268898487\n",
      "Epoch 2199, Loss: 0.05758875608444214, Final Batch Loss: 0.015142414718866348\n",
      "Epoch 2200, Loss: 0.06282558850944042, Final Batch Loss: 0.04094507172703743\n",
      "Epoch 2201, Loss: 0.18565173260867596, Final Batch Loss: 0.15610243380069733\n",
      "Epoch 2202, Loss: 0.1359589919447899, Final Batch Loss: 0.06655064970254898\n",
      "Epoch 2203, Loss: 0.10929803177714348, Final Batch Loss: 0.04668617621064186\n",
      "Epoch 2204, Loss: 0.04166034609079361, Final Batch Loss: 0.00819602981209755\n",
      "Epoch 2205, Loss: 0.025047489907592535, Final Batch Loss: 0.004615824203938246\n",
      "Epoch 2206, Loss: 0.0474169235676527, Final Batch Loss: 0.02618503011763096\n",
      "Epoch 2207, Loss: 0.07107187062501907, Final Batch Loss: 0.042142294347286224\n",
      "Epoch 2208, Loss: 0.13184000179171562, Final Batch Loss: 0.051178913563489914\n",
      "Epoch 2209, Loss: 0.04066376620903611, Final Batch Loss: 0.00771985063329339\n",
      "Epoch 2210, Loss: 0.05164891108870506, Final Batch Loss: 0.028371168300509453\n",
      "Epoch 2211, Loss: 0.026745300623588264, Final Batch Loss: 0.0018713230965659022\n",
      "Epoch 2212, Loss: 0.08653190732002258, Final Batch Loss: 0.05709715187549591\n",
      "Epoch 2213, Loss: 0.05778580531477928, Final Batch Loss: 0.014096774160861969\n",
      "Epoch 2214, Loss: 0.07816872652620077, Final Batch Loss: 0.014793469570577145\n",
      "Epoch 2215, Loss: 0.1328970305621624, Final Batch Loss: 0.08171485364437103\n",
      "Epoch 2216, Loss: 0.0520486943423748, Final Batch Loss: 0.019349567592144012\n",
      "Epoch 2217, Loss: 0.05943874455988407, Final Batch Loss: 0.026220181956887245\n",
      "Epoch 2218, Loss: 0.04701591096818447, Final Batch Loss: 0.01789783500134945\n",
      "Epoch 2219, Loss: 0.03674513567239046, Final Batch Loss: 0.01183567475527525\n",
      "Epoch 2220, Loss: 0.06320298835635185, Final Batch Loss: 0.021282661706209183\n",
      "Epoch 2221, Loss: 0.23012622445821762, Final Batch Loss: 0.18201550841331482\n",
      "Epoch 2222, Loss: 0.08545952290296555, Final Batch Loss: 0.051479633897542953\n",
      "Epoch 2223, Loss: 0.06711312010884285, Final Batch Loss: 0.04003674536943436\n",
      "Epoch 2224, Loss: 0.054173738695681095, Final Batch Loss: 0.008924406953155994\n",
      "Epoch 2225, Loss: 0.08688980340957642, Final Batch Loss: 0.02593047171831131\n",
      "Epoch 2226, Loss: 0.055591817013919353, Final Batch Loss: 0.00929288100451231\n",
      "Epoch 2227, Loss: 0.1069640964269638, Final Batch Loss: 0.07500939816236496\n",
      "Epoch 2228, Loss: 0.1036793403327465, Final Batch Loss: 0.06357841938734055\n",
      "Epoch 2229, Loss: 0.05172710679471493, Final Batch Loss: 0.018640385940670967\n",
      "Epoch 2230, Loss: 0.04749896191060543, Final Batch Loss: 0.024403883144259453\n",
      "Epoch 2231, Loss: 0.06194080784916878, Final Batch Loss: 0.021994423121213913\n",
      "Epoch 2232, Loss: 0.05031720222905278, Final Batch Loss: 0.005755514372140169\n",
      "Epoch 2233, Loss: 0.044377841986715794, Final Batch Loss: 0.013203141279518604\n",
      "Epoch 2234, Loss: 0.025277025066316128, Final Batch Loss: 0.008183022029697895\n",
      "Epoch 2235, Loss: 0.06383747979998589, Final Batch Loss: 0.012616526335477829\n",
      "Epoch 2236, Loss: 0.034518623258918524, Final Batch Loss: 0.007769807707518339\n",
      "Epoch 2237, Loss: 0.053221010603010654, Final Batch Loss: 0.012807928957045078\n",
      "Epoch 2238, Loss: 0.08224990963935852, Final Batch Loss: 0.03146527335047722\n",
      "Epoch 2239, Loss: 0.07479611784219742, Final Batch Loss: 0.048145487904548645\n",
      "Epoch 2240, Loss: 0.0581185445189476, Final Batch Loss: 0.025416571646928787\n",
      "Epoch 2241, Loss: 0.09688786789774895, Final Batch Loss: 0.06127152964472771\n",
      "Epoch 2242, Loss: 0.06780515238642693, Final Batch Loss: 0.03379731625318527\n",
      "Epoch 2243, Loss: 0.06584855541586876, Final Batch Loss: 0.02152348682284355\n",
      "Epoch 2244, Loss: 0.06197895482182503, Final Batch Loss: 0.024708639830350876\n",
      "Epoch 2245, Loss: 0.06291989795863628, Final Batch Loss: 0.024872755631804466\n",
      "Epoch 2246, Loss: 0.10934541374444962, Final Batch Loss: 0.04357881098985672\n",
      "Epoch 2247, Loss: 0.02870350144803524, Final Batch Loss: 0.01036985032260418\n",
      "Epoch 2248, Loss: 0.04686158522963524, Final Batch Loss: 0.021880589425563812\n",
      "Epoch 2249, Loss: 0.05760232172906399, Final Batch Loss: 0.032496340572834015\n",
      "Epoch 2250, Loss: 0.04688181076198816, Final Batch Loss: 0.010600336827337742\n",
      "Epoch 2251, Loss: 0.05598844960331917, Final Batch Loss: 0.02832510508596897\n",
      "Epoch 2252, Loss: 0.03284003771841526, Final Batch Loss: 0.012388480827212334\n",
      "Epoch 2253, Loss: 0.0713534727692604, Final Batch Loss: 0.0396513007581234\n",
      "Epoch 2254, Loss: 0.028856458142399788, Final Batch Loss: 0.00835997425019741\n",
      "Epoch 2255, Loss: 0.03651115391403437, Final Batch Loss: 0.01167526375502348\n",
      "Epoch 2256, Loss: 0.06462468486279249, Final Batch Loss: 0.011986619792878628\n",
      "Epoch 2257, Loss: 0.02510977443307638, Final Batch Loss: 0.01184278167784214\n",
      "Epoch 2258, Loss: 0.048013877123594284, Final Batch Loss: 0.02643178403377533\n",
      "Epoch 2259, Loss: 0.034262511413544416, Final Batch Loss: 0.007770152296870947\n",
      "Epoch 2260, Loss: 0.05847899429500103, Final Batch Loss: 0.03086685761809349\n",
      "Epoch 2261, Loss: 0.08264306746423244, Final Batch Loss: 0.05579933151602745\n",
      "Epoch 2262, Loss: 0.044764752965420485, Final Batch Loss: 0.006892460864037275\n",
      "Epoch 2263, Loss: 0.05405485723167658, Final Batch Loss: 0.008625076152384281\n",
      "Epoch 2264, Loss: 0.079510398209095, Final Batch Loss: 0.03857004642486572\n",
      "Epoch 2265, Loss: 0.08846237882971764, Final Batch Loss: 0.04118834808468819\n",
      "Epoch 2266, Loss: 0.05469880625605583, Final Batch Loss: 0.02310226485133171\n",
      "Epoch 2267, Loss: 0.03229633625596762, Final Batch Loss: 0.013279653154313564\n",
      "Epoch 2268, Loss: 0.07563690468668938, Final Batch Loss: 0.05015232786536217\n",
      "Epoch 2269, Loss: 0.12065765634179115, Final Batch Loss: 0.019339222460985184\n",
      "Epoch 2270, Loss: 0.09666338190436363, Final Batch Loss: 0.05361774191260338\n",
      "Epoch 2271, Loss: 0.0438653314486146, Final Batch Loss: 0.028462952002882957\n",
      "Epoch 2272, Loss: 0.0924203097820282, Final Batch Loss: 0.0345359668135643\n",
      "Epoch 2273, Loss: 0.04048918932676315, Final Batch Loss: 0.022092381492257118\n",
      "Epoch 2274, Loss: 0.0751825887709856, Final Batch Loss: 0.02080153115093708\n",
      "Epoch 2275, Loss: 0.06804706901311874, Final Batch Loss: 0.029610004276037216\n",
      "Epoch 2276, Loss: 0.05839797295629978, Final Batch Loss: 0.04664243012666702\n",
      "Epoch 2277, Loss: 0.09249411523342133, Final Batch Loss: 0.06580667197704315\n",
      "Epoch 2278, Loss: 0.17590581253170967, Final Batch Loss: 0.11815017461776733\n",
      "Epoch 2279, Loss: 0.05956763029098511, Final Batch Loss: 0.03414172679185867\n",
      "Epoch 2280, Loss: 0.10208208672702312, Final Batch Loss: 0.03031059168279171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2281, Loss: 0.11173029243946075, Final Batch Loss: 0.06732330471277237\n",
      "Epoch 2282, Loss: 0.11508232727646828, Final Batch Loss: 0.03399739786982536\n",
      "Epoch 2283, Loss: 0.06346268393099308, Final Batch Loss: 0.030206872150301933\n",
      "Epoch 2284, Loss: 0.087149728089571, Final Batch Loss: 0.04977493733167648\n",
      "Epoch 2285, Loss: 0.06752812769263983, Final Batch Loss: 0.011100872419774532\n",
      "Epoch 2286, Loss: 0.08350769802927971, Final Batch Loss: 0.03852922469377518\n",
      "Epoch 2287, Loss: 0.06610419787466526, Final Batch Loss: 0.027829138562083244\n",
      "Epoch 2288, Loss: 0.03591006062924862, Final Batch Loss: 0.006883528083562851\n",
      "Epoch 2289, Loss: 0.04224413074553013, Final Batch Loss: 0.02223762311041355\n",
      "Epoch 2290, Loss: 0.06939488463103771, Final Batch Loss: 0.05319471284747124\n",
      "Epoch 2291, Loss: 0.08422265201807022, Final Batch Loss: 0.039987970143556595\n",
      "Epoch 2292, Loss: 0.0920681320130825, Final Batch Loss: 0.05918661877512932\n",
      "Epoch 2293, Loss: 0.052989840507507324, Final Batch Loss: 0.0103449746966362\n",
      "Epoch 2294, Loss: 0.13145744428038597, Final Batch Loss: 0.09589596837759018\n",
      "Epoch 2295, Loss: 0.023832286708056927, Final Batch Loss: 0.011009584181010723\n",
      "Epoch 2296, Loss: 0.04207539185881615, Final Batch Loss: 0.020698312669992447\n",
      "Epoch 2297, Loss: 0.037228191271424294, Final Batch Loss: 0.015715466812253\n",
      "Epoch 2298, Loss: 0.06900747306644917, Final Batch Loss: 0.0480685755610466\n",
      "Epoch 2299, Loss: 0.16463901475071907, Final Batch Loss: 0.1318408101797104\n",
      "Epoch 2300, Loss: 0.08049964718520641, Final Batch Loss: 0.02934429608285427\n",
      "Epoch 2301, Loss: 0.038354676216840744, Final Batch Loss: 0.024502163752913475\n",
      "Epoch 2302, Loss: 0.11854323372244835, Final Batch Loss: 0.06534425169229507\n",
      "Epoch 2303, Loss: 0.05275022238492966, Final Batch Loss: 0.020576152950525284\n",
      "Epoch 2304, Loss: 0.07581506296992302, Final Batch Loss: 0.0483977310359478\n",
      "Epoch 2305, Loss: 0.12441405281424522, Final Batch Loss: 0.0966937318444252\n",
      "Epoch 2306, Loss: 0.04030003026127815, Final Batch Loss: 0.01920580305159092\n",
      "Epoch 2307, Loss: 0.06728114932775497, Final Batch Loss: 0.022434663027524948\n",
      "Epoch 2308, Loss: 0.05212421715259552, Final Batch Loss: 0.028866099193692207\n",
      "Epoch 2309, Loss: 0.037940649315714836, Final Batch Loss: 0.01992744207382202\n",
      "Epoch 2310, Loss: 0.04487905837595463, Final Batch Loss: 0.031952086836099625\n",
      "Epoch 2311, Loss: 0.04615524457767606, Final Batch Loss: 0.005333684850484133\n",
      "Epoch 2312, Loss: 0.06276445463299751, Final Batch Loss: 0.019008100032806396\n",
      "Epoch 2313, Loss: 0.06596256606280804, Final Batch Loss: 0.008941290900111198\n",
      "Epoch 2314, Loss: 0.03337641153484583, Final Batch Loss: 0.020185621455311775\n",
      "Epoch 2315, Loss: 0.03988209366798401, Final Batch Loss: 0.015315111726522446\n",
      "Epoch 2316, Loss: 0.06603900156915188, Final Batch Loss: 0.03651927411556244\n",
      "Epoch 2317, Loss: 0.1789858639240265, Final Batch Loss: 0.0961252748966217\n",
      "Epoch 2318, Loss: 0.033854350447654724, Final Batch Loss: 0.014570072293281555\n",
      "Epoch 2319, Loss: 0.029783478938043118, Final Batch Loss: 0.009311969392001629\n",
      "Epoch 2320, Loss: 0.04938675370067358, Final Batch Loss: 0.011634222231805325\n",
      "Epoch 2321, Loss: 0.041943822987377644, Final Batch Loss: 0.007799864746630192\n",
      "Epoch 2322, Loss: 0.09039423242211342, Final Batch Loss: 0.007239583879709244\n",
      "Epoch 2323, Loss: 0.07157744141295552, Final Batch Loss: 0.004944842774420977\n",
      "Epoch 2324, Loss: 0.08112489432096481, Final Batch Loss: 0.039942096918821335\n",
      "Epoch 2325, Loss: 0.06696351431310177, Final Batch Loss: 0.011391842737793922\n",
      "Epoch 2326, Loss: 0.08184117823839188, Final Batch Loss: 0.06247352436184883\n",
      "Epoch 2327, Loss: 0.09195104613900185, Final Batch Loss: 0.05503414571285248\n",
      "Epoch 2328, Loss: 0.047537547536194324, Final Batch Loss: 0.009907652623951435\n",
      "Epoch 2329, Loss: 0.04380941949784756, Final Batch Loss: 0.025937886908650398\n",
      "Epoch 2330, Loss: 0.04889404680579901, Final Batch Loss: 0.036205366253852844\n",
      "Epoch 2331, Loss: 0.08385047316551208, Final Batch Loss: 0.04535198211669922\n",
      "Epoch 2332, Loss: 0.019777140580117702, Final Batch Loss: 0.004475739784538746\n",
      "Epoch 2333, Loss: 0.18474641349166632, Final Batch Loss: 0.1723312884569168\n",
      "Epoch 2334, Loss: 0.04003146477043629, Final Batch Loss: 0.01625961810350418\n",
      "Epoch 2335, Loss: 0.046217761002480984, Final Batch Loss: 0.009720857255160809\n",
      "Epoch 2336, Loss: 0.08452118374407291, Final Batch Loss: 0.020867208018898964\n",
      "Epoch 2337, Loss: 0.12744413688778877, Final Batch Loss: 0.06635838001966476\n",
      "Epoch 2338, Loss: 0.11812181025743484, Final Batch Loss: 0.07572980970144272\n",
      "Epoch 2339, Loss: 0.08720780909061432, Final Batch Loss: 0.029651127755641937\n",
      "Epoch 2340, Loss: 0.04991203546524048, Final Batch Loss: 0.02600986137986183\n",
      "Epoch 2341, Loss: 0.08177732303738594, Final Batch Loss: 0.042131971567869186\n",
      "Epoch 2342, Loss: 0.029334385879337788, Final Batch Loss: 0.00722638051956892\n",
      "Epoch 2343, Loss: 0.17842604964971542, Final Batch Loss: 0.11049670726060867\n",
      "Epoch 2344, Loss: 0.0645647281780839, Final Batch Loss: 0.010099031962454319\n",
      "Epoch 2345, Loss: 0.08987874910235405, Final Batch Loss: 0.04793617129325867\n",
      "Epoch 2346, Loss: 0.040099404752254486, Final Batch Loss: 0.0124944057315588\n",
      "Epoch 2347, Loss: 0.15786726400256157, Final Batch Loss: 0.13587626814842224\n",
      "Epoch 2348, Loss: 0.0747034028172493, Final Batch Loss: 0.056596722453832626\n",
      "Epoch 2349, Loss: 0.05935879051685333, Final Batch Loss: 0.038973528891801834\n",
      "Epoch 2350, Loss: 0.04619189444929361, Final Batch Loss: 0.010502812452614307\n",
      "Epoch 2351, Loss: 0.11930593103170395, Final Batch Loss: 0.05674625188112259\n",
      "Epoch 2352, Loss: 0.04491912014782429, Final Batch Loss: 0.023647639900445938\n",
      "Epoch 2353, Loss: 0.038871755823493004, Final Batch Loss: 0.011387109756469727\n",
      "Epoch 2354, Loss: 0.05324648134410381, Final Batch Loss: 0.03357834741473198\n",
      "Epoch 2355, Loss: 0.04180699493736029, Final Batch Loss: 0.01517514232546091\n",
      "Epoch 2356, Loss: 0.05329900421202183, Final Batch Loss: 0.03458942845463753\n",
      "Epoch 2357, Loss: 0.0629500076174736, Final Batch Loss: 0.041329525411129\n",
      "Epoch 2358, Loss: 0.04307217337191105, Final Batch Loss: 0.018678581342101097\n",
      "Epoch 2359, Loss: 0.06441493052989244, Final Batch Loss: 0.0498725101351738\n",
      "Epoch 2360, Loss: 0.09533464908599854, Final Batch Loss: 0.07113282382488251\n",
      "Epoch 2361, Loss: 0.06594710424542427, Final Batch Loss: 0.02500981092453003\n",
      "Epoch 2362, Loss: 0.03857436589896679, Final Batch Loss: 0.017437616363167763\n",
      "Epoch 2363, Loss: 0.06787420064210892, Final Batch Loss: 0.022924166172742844\n",
      "Epoch 2364, Loss: 0.06443913467228413, Final Batch Loss: 0.040355026721954346\n",
      "Epoch 2365, Loss: 0.06641208287328482, Final Batch Loss: 0.008907332085072994\n",
      "Epoch 2366, Loss: 0.07533115521073341, Final Batch Loss: 0.04052214324474335\n",
      "Epoch 2367, Loss: 0.044666873291134834, Final Batch Loss: 0.032498348504304886\n",
      "Epoch 2368, Loss: 0.06234796345233917, Final Batch Loss: 0.01689894124865532\n",
      "Epoch 2369, Loss: 0.031209902837872505, Final Batch Loss: 0.00847679190337658\n",
      "Epoch 2370, Loss: 0.06866484507918358, Final Batch Loss: 0.031546443700790405\n",
      "Epoch 2371, Loss: 0.07194251008331776, Final Batch Loss: 0.011902065947651863\n",
      "Epoch 2372, Loss: 0.03131352365016937, Final Batch Loss: 0.008471496403217316\n",
      "Epoch 2373, Loss: 0.03714664280414581, Final Batch Loss: 0.01730271242558956\n",
      "Epoch 2374, Loss: 0.05742153525352478, Final Batch Loss: 0.03513103350996971\n",
      "Epoch 2375, Loss: 0.07291598059237003, Final Batch Loss: 0.05668436363339424\n",
      "Epoch 2376, Loss: 0.09565391018986702, Final Batch Loss: 0.06861310452222824\n",
      "Epoch 2377, Loss: 0.040870241820812225, Final Batch Loss: 0.023359032347798347\n",
      "Epoch 2378, Loss: 0.1802763007581234, Final Batch Loss: 0.14076834917068481\n",
      "Epoch 2379, Loss: 0.06598279066383839, Final Batch Loss: 0.019123850390315056\n",
      "Epoch 2380, Loss: 0.04685187712311745, Final Batch Loss: 0.0317297987639904\n",
      "Epoch 2381, Loss: 0.12696107849478722, Final Batch Loss: 0.10703656077384949\n",
      "Epoch 2382, Loss: 0.03942974470555782, Final Batch Loss: 0.019010255113244057\n",
      "Epoch 2383, Loss: 0.04390682838857174, Final Batch Loss: 0.020520858466625214\n",
      "Epoch 2384, Loss: 0.03650468774139881, Final Batch Loss: 0.01974930800497532\n",
      "Epoch 2385, Loss: 0.11528989486396313, Final Batch Loss: 0.09736835956573486\n",
      "Epoch 2386, Loss: 0.07647457718849182, Final Batch Loss: 0.04209958016872406\n",
      "Epoch 2387, Loss: 0.057807509787380695, Final Batch Loss: 0.04368587210774422\n",
      "Epoch 2388, Loss: 0.04614204401150346, Final Batch Loss: 0.0022646174766123295\n",
      "Epoch 2389, Loss: 0.05150160193443298, Final Batch Loss: 0.021903198212385178\n",
      "Epoch 2390, Loss: 0.10181199386715889, Final Batch Loss: 0.06606996059417725\n",
      "Epoch 2391, Loss: 0.07175154611468315, Final Batch Loss: 0.017360977828502655\n",
      "Epoch 2392, Loss: 0.0973413847386837, Final Batch Loss: 0.042383044958114624\n",
      "Epoch 2393, Loss: 0.0655404832214117, Final Batch Loss: 0.025784621015191078\n",
      "Epoch 2394, Loss: 0.08874749206006527, Final Batch Loss: 0.027780810371041298\n",
      "Epoch 2395, Loss: 0.045819833409041166, Final Batch Loss: 0.004024881403893232\n",
      "Epoch 2396, Loss: 0.10641555674374104, Final Batch Loss: 0.07719936221837997\n",
      "Epoch 2397, Loss: 0.09758396819233894, Final Batch Loss: 0.06058713048696518\n",
      "Epoch 2398, Loss: 0.04049733933061361, Final Batch Loss: 0.014448118396103382\n",
      "Epoch 2399, Loss: 0.06771615520119667, Final Batch Loss: 0.029861755669116974\n",
      "Epoch 2400, Loss: 0.13429732248187065, Final Batch Loss: 0.08673334866762161\n",
      "Epoch 2401, Loss: 0.047265597619116306, Final Batch Loss: 0.014968623407185078\n",
      "Epoch 2402, Loss: 0.06670044176280499, Final Batch Loss: 0.03563220426440239\n",
      "Epoch 2403, Loss: 0.06146183470264077, Final Batch Loss: 0.0033903331495821476\n",
      "Epoch 2404, Loss: 0.08099479973316193, Final Batch Loss: 0.05392909795045853\n",
      "Epoch 2405, Loss: 0.047803131863474846, Final Batch Loss: 0.01753850094974041\n",
      "Epoch 2406, Loss: 0.10723784193396568, Final Batch Loss: 0.08967816084623337\n",
      "Epoch 2407, Loss: 0.03753563016653061, Final Batch Loss: 0.018738729879260063\n",
      "Epoch 2408, Loss: 0.040945335291326046, Final Batch Loss: 0.014305814169347286\n",
      "Epoch 2409, Loss: 0.05250499863177538, Final Batch Loss: 0.01175853330641985\n",
      "Epoch 2410, Loss: 0.04343109857290983, Final Batch Loss: 0.03238551318645477\n",
      "Epoch 2411, Loss: 0.04294818453490734, Final Batch Loss: 0.022123055532574654\n",
      "Epoch 2412, Loss: 0.04196271114051342, Final Batch Loss: 0.025974394753575325\n",
      "Epoch 2413, Loss: 0.046056234277784824, Final Batch Loss: 0.01561014074832201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2414, Loss: 0.05065158801153302, Final Batch Loss: 0.006753851193934679\n",
      "Epoch 2415, Loss: 0.05360040068626404, Final Batch Loss: 0.030231811106204987\n",
      "Epoch 2416, Loss: 0.0700566042214632, Final Batch Loss: 0.019133659079670906\n",
      "Epoch 2417, Loss: 0.07836721651256084, Final Batch Loss: 0.06273727118968964\n",
      "Epoch 2418, Loss: 0.09417350590229034, Final Batch Loss: 0.03341903164982796\n",
      "Epoch 2419, Loss: 0.0747966505587101, Final Batch Loss: 0.02738253027200699\n",
      "Epoch 2420, Loss: 0.055510333739221096, Final Batch Loss: 0.012401609681546688\n",
      "Epoch 2421, Loss: 0.05277242884039879, Final Batch Loss: 0.017183005809783936\n",
      "Epoch 2422, Loss: 0.042604854330420494, Final Batch Loss: 0.022679850459098816\n",
      "Epoch 2423, Loss: 0.039851811714470387, Final Batch Loss: 0.008409337140619755\n",
      "Epoch 2424, Loss: 0.053058658726513386, Final Batch Loss: 0.015563682653009892\n",
      "Epoch 2425, Loss: 0.06493438966572285, Final Batch Loss: 0.047650180757045746\n",
      "Epoch 2426, Loss: 0.03734823875129223, Final Batch Loss: 0.01677907072007656\n",
      "Epoch 2427, Loss: 0.06390229146927595, Final Batch Loss: 0.05278785154223442\n",
      "Epoch 2428, Loss: 0.052419571205973625, Final Batch Loss: 0.018192993476986885\n",
      "Epoch 2429, Loss: 0.020177486818283796, Final Batch Loss: 0.007797510828822851\n",
      "Epoch 2430, Loss: 0.029888085089623928, Final Batch Loss: 0.009155296720564365\n",
      "Epoch 2431, Loss: 0.08316096290946007, Final Batch Loss: 0.05179494246840477\n",
      "Epoch 2432, Loss: 0.05587722919881344, Final Batch Loss: 0.007761964574456215\n",
      "Epoch 2433, Loss: 0.051474254578351974, Final Batch Loss: 0.032287728041410446\n",
      "Epoch 2434, Loss: 0.037906729616224766, Final Batch Loss: 0.026099054142832756\n",
      "Epoch 2435, Loss: 0.0333273746073246, Final Batch Loss: 0.011113924905657768\n",
      "Epoch 2436, Loss: 0.07335158437490463, Final Batch Loss: 0.039920955896377563\n",
      "Epoch 2437, Loss: 0.07070869114249945, Final Batch Loss: 0.05740866810083389\n",
      "Epoch 2438, Loss: 0.13106674514710903, Final Batch Loss: 0.10417943447828293\n",
      "Epoch 2439, Loss: 0.05715968273580074, Final Batch Loss: 0.0312048327177763\n",
      "Epoch 2440, Loss: 0.07343480549752712, Final Batch Loss: 0.0437568835914135\n",
      "Epoch 2441, Loss: 0.16326729208230972, Final Batch Loss: 0.09384334832429886\n",
      "Epoch 2442, Loss: 0.0983892660588026, Final Batch Loss: 0.0742005854845047\n",
      "Epoch 2443, Loss: 0.07852020487189293, Final Batch Loss: 0.060542020946741104\n",
      "Epoch 2444, Loss: 0.03287416696548462, Final Batch Loss: 0.016575565561652184\n",
      "Epoch 2445, Loss: 0.055672308430075645, Final Batch Loss: 0.031729403883218765\n",
      "Epoch 2446, Loss: 0.11057233810424805, Final Batch Loss: 0.055971939116716385\n",
      "Epoch 2447, Loss: 0.04531572945415974, Final Batch Loss: 0.01578935794532299\n",
      "Epoch 2448, Loss: 0.05185568332672119, Final Batch Loss: 0.022812601178884506\n",
      "Epoch 2449, Loss: 0.06862343847751617, Final Batch Loss: 0.011860638856887817\n",
      "Epoch 2450, Loss: 0.11704874597489834, Final Batch Loss: 0.02592417411506176\n",
      "Epoch 2451, Loss: 0.07836044207215309, Final Batch Loss: 0.043518539518117905\n",
      "Epoch 2452, Loss: 0.07958544231951237, Final Batch Loss: 0.05833916366100311\n",
      "Epoch 2453, Loss: 0.09104260429739952, Final Batch Loss: 0.06000126153230667\n",
      "Epoch 2454, Loss: 0.07225954160094261, Final Batch Loss: 0.034103911370038986\n",
      "Epoch 2455, Loss: 0.050778940320014954, Final Batch Loss: 0.03061513602733612\n",
      "Epoch 2456, Loss: 0.05017965938895941, Final Batch Loss: 0.013630400411784649\n",
      "Epoch 2457, Loss: 0.038187301717698574, Final Batch Loss: 0.011545649729669094\n",
      "Epoch 2458, Loss: 0.08076237700879574, Final Batch Loss: 0.05695519968867302\n",
      "Epoch 2459, Loss: 0.03755308501422405, Final Batch Loss: 0.013426942750811577\n",
      "Epoch 2460, Loss: 0.0459890142083168, Final Batch Loss: 0.015662647783756256\n",
      "Epoch 2461, Loss: 0.1804216429591179, Final Batch Loss: 0.14379659295082092\n",
      "Epoch 2462, Loss: 0.22218696027994156, Final Batch Loss: 0.15615159273147583\n",
      "Epoch 2463, Loss: 0.12729047238826752, Final Batch Loss: 0.06853098422288895\n",
      "Epoch 2464, Loss: 0.07222479954361916, Final Batch Loss: 0.029849033802747726\n",
      "Epoch 2465, Loss: 0.27357647847384214, Final Batch Loss: 0.25853627920150757\n",
      "Epoch 2466, Loss: 0.05460263881832361, Final Batch Loss: 0.013774395920336246\n",
      "Epoch 2467, Loss: 0.06796391680836678, Final Batch Loss: 0.04737558960914612\n",
      "Epoch 2468, Loss: 0.11791633069515228, Final Batch Loss: 0.07550600171089172\n",
      "Epoch 2469, Loss: 0.07941599562764168, Final Batch Loss: 0.03148695081472397\n",
      "Epoch 2470, Loss: 0.027881763875484467, Final Batch Loss: 0.010830147191882133\n",
      "Epoch 2471, Loss: 0.03585237171500921, Final Batch Loss: 0.015138327144086361\n",
      "Epoch 2472, Loss: 0.04287826828658581, Final Batch Loss: 0.016836319118738174\n",
      "Epoch 2473, Loss: 0.07248653843998909, Final Batch Loss: 0.044865917414426804\n",
      "Epoch 2474, Loss: 0.03419854585081339, Final Batch Loss: 0.011648139916360378\n",
      "Epoch 2475, Loss: 0.0343662672676146, Final Batch Loss: 0.0074359760619699955\n",
      "Epoch 2476, Loss: 0.06729923561215401, Final Batch Loss: 0.02898251637816429\n",
      "Epoch 2477, Loss: 0.17980007827281952, Final Batch Loss: 0.09880193322896957\n",
      "Epoch 2478, Loss: 0.08687762543559074, Final Batch Loss: 0.051087379455566406\n",
      "Epoch 2479, Loss: 0.11038553714752197, Final Batch Loss: 0.0860181376338005\n",
      "Epoch 2480, Loss: 0.08568109385669231, Final Batch Loss: 0.023499296978116035\n",
      "Epoch 2481, Loss: 0.07251851633191109, Final Batch Loss: 0.056041087955236435\n",
      "Epoch 2482, Loss: 0.04373246058821678, Final Batch Loss: 0.016242297366261482\n",
      "Epoch 2483, Loss: 0.05970166623592377, Final Batch Loss: 0.03897519037127495\n",
      "Epoch 2484, Loss: 0.09028203785419464, Final Batch Loss: 0.06198667362332344\n",
      "Epoch 2485, Loss: 0.042164331302046776, Final Batch Loss: 0.008664501830935478\n",
      "Epoch 2486, Loss: 0.06195790134370327, Final Batch Loss: 0.03838007524609566\n",
      "Epoch 2487, Loss: 0.020148470997810364, Final Batch Loss: 0.008610432036221027\n",
      "Epoch 2488, Loss: 0.11444069445133209, Final Batch Loss: 0.07366205006837845\n",
      "Epoch 2489, Loss: 0.07010084390640259, Final Batch Loss: 0.035684335976839066\n",
      "Epoch 2490, Loss: 0.049390023574233055, Final Batch Loss: 0.0220891535282135\n",
      "Epoch 2491, Loss: 0.042777037248015404, Final Batch Loss: 0.018540849909186363\n",
      "Epoch 2492, Loss: 0.06067649833858013, Final Batch Loss: 0.024923665449023247\n",
      "Epoch 2493, Loss: 0.1250624619424343, Final Batch Loss: 0.0930553749203682\n",
      "Epoch 2494, Loss: 0.07566779293119907, Final Batch Loss: 0.029691917821764946\n",
      "Epoch 2495, Loss: 0.07116800360381603, Final Batch Loss: 0.030516771599650383\n",
      "Epoch 2496, Loss: 0.08360018860548735, Final Batch Loss: 0.07269465923309326\n",
      "Epoch 2497, Loss: 0.027822191826999187, Final Batch Loss: 0.008415733464062214\n",
      "Epoch 2498, Loss: 0.040490017272531986, Final Batch Loss: 0.010154162533581257\n",
      "Epoch 2499, Loss: 0.042938655242323875, Final Batch Loss: 0.014366166666150093\n",
      "Epoch 2500, Loss: 0.05096221715211868, Final Batch Loss: 0.02757537178695202\n",
      "Epoch 2501, Loss: 0.023076378740370274, Final Batch Loss: 0.010377905331552029\n",
      "Epoch 2502, Loss: 0.10145482420921326, Final Batch Loss: 0.06155301630496979\n",
      "Epoch 2503, Loss: 0.03678170591592789, Final Batch Loss: 0.007556864991784096\n",
      "Epoch 2504, Loss: 0.034641653299331665, Final Batch Loss: 0.009249351918697357\n",
      "Epoch 2505, Loss: 0.055120063945651054, Final Batch Loss: 0.03280412778258324\n",
      "Epoch 2506, Loss: 0.058556804433465004, Final Batch Loss: 0.02149888686835766\n",
      "Epoch 2507, Loss: 0.03201369754970074, Final Batch Loss: 0.010287899523973465\n",
      "Epoch 2508, Loss: 0.05679861456155777, Final Batch Loss: 0.03591177240014076\n",
      "Epoch 2509, Loss: 0.02220326755195856, Final Batch Loss: 0.008206983096897602\n",
      "Epoch 2510, Loss: 0.09432809054851532, Final Batch Loss: 0.07320818305015564\n",
      "Epoch 2511, Loss: 0.06209265813231468, Final Batch Loss: 0.046284306794404984\n",
      "Epoch 2512, Loss: 0.065973199903965, Final Batch Loss: 0.028635919094085693\n",
      "Epoch 2513, Loss: 0.058614205569028854, Final Batch Loss: 0.024702128022909164\n",
      "Epoch 2514, Loss: 0.02617473527789116, Final Batch Loss: 0.005857991054654121\n",
      "Epoch 2515, Loss: 0.10309183597564697, Final Batch Loss: 0.05838516354560852\n",
      "Epoch 2516, Loss: 0.05455809459090233, Final Batch Loss: 0.0231088325381279\n",
      "Epoch 2517, Loss: 0.02699021901935339, Final Batch Loss: 0.007172723300755024\n",
      "Epoch 2518, Loss: 0.04800817649811506, Final Batch Loss: 0.03486011177301407\n",
      "Epoch 2519, Loss: 0.03633674420416355, Final Batch Loss: 0.008562479168176651\n",
      "Epoch 2520, Loss: 0.043085427954792976, Final Batch Loss: 0.02203693799674511\n",
      "Epoch 2521, Loss: 0.11230878531932831, Final Batch Loss: 0.02149738371372223\n",
      "Epoch 2522, Loss: 0.043861448764801025, Final Batch Loss: 0.028214380145072937\n",
      "Epoch 2523, Loss: 0.04031236842274666, Final Batch Loss: 0.02230861224234104\n",
      "Epoch 2524, Loss: 0.04859167896211147, Final Batch Loss: 0.02488943561911583\n",
      "Epoch 2525, Loss: 0.025305289775133133, Final Batch Loss: 0.01052689179778099\n",
      "Epoch 2526, Loss: 0.025741579942405224, Final Batch Loss: 0.008841522969305515\n",
      "Epoch 2527, Loss: 0.09912185743451118, Final Batch Loss: 0.03070373460650444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2528, Loss: 0.04387800209224224, Final Batch Loss: 0.016615131869912148\n",
      "Epoch 2529, Loss: 0.030135590117424726, Final Batch Loss: 0.001677734311670065\n",
      "Epoch 2530, Loss: 0.03528998140245676, Final Batch Loss: 0.010540175251662731\n",
      "Epoch 2531, Loss: 0.07910802960395813, Final Batch Loss: 0.04289203882217407\n",
      "Epoch 2532, Loss: 0.03963606804609299, Final Batch Loss: 0.01614755392074585\n",
      "Epoch 2533, Loss: 0.06010979600250721, Final Batch Loss: 0.034863103181123734\n",
      "Epoch 2534, Loss: 0.053383221849799156, Final Batch Loss: 0.02792823500931263\n",
      "Epoch 2535, Loss: 0.07001280598342419, Final Batch Loss: 0.008077861741185188\n",
      "Epoch 2536, Loss: 0.05843056179583073, Final Batch Loss: 0.03674381226301193\n",
      "Epoch 2537, Loss: 0.10404473543167114, Final Batch Loss: 0.058054108172655106\n",
      "Epoch 2538, Loss: 0.027432517614215612, Final Batch Loss: 0.0063080801628530025\n",
      "Epoch 2539, Loss: 0.13725381717085838, Final Batch Loss: 0.12204340100288391\n",
      "Epoch 2540, Loss: 0.03870796598494053, Final Batch Loss: 0.008951490744948387\n",
      "Epoch 2541, Loss: 0.03564809076488018, Final Batch Loss: 0.0132345762103796\n",
      "Epoch 2542, Loss: 0.05773204006254673, Final Batch Loss: 0.044465143233537674\n",
      "Epoch 2543, Loss: 0.02613601554185152, Final Batch Loss: 0.01032936293631792\n",
      "Epoch 2544, Loss: 0.05469725281000137, Final Batch Loss: 0.026246292516589165\n",
      "Epoch 2545, Loss: 0.10583819821476936, Final Batch Loss: 0.06829327344894409\n",
      "Epoch 2546, Loss: 0.06112364120781422, Final Batch Loss: 0.033940065652132034\n",
      "Epoch 2547, Loss: 0.04511597193777561, Final Batch Loss: 0.019280632957816124\n",
      "Epoch 2548, Loss: 0.05331277661025524, Final Batch Loss: 0.03415394574403763\n",
      "Epoch 2549, Loss: 0.07931413315236568, Final Batch Loss: 0.054338764399290085\n",
      "Epoch 2550, Loss: 0.05908682569861412, Final Batch Loss: 0.03393838182091713\n",
      "Epoch 2551, Loss: 0.05684512108564377, Final Batch Loss: 0.03929224982857704\n",
      "Epoch 2552, Loss: 0.09214012883603573, Final Batch Loss: 0.06211099028587341\n",
      "Epoch 2553, Loss: 0.03356172237545252, Final Batch Loss: 0.008672486059367657\n",
      "Epoch 2554, Loss: 0.04335714038461447, Final Batch Loss: 0.013196649961173534\n",
      "Epoch 2555, Loss: 0.08865402638912201, Final Batch Loss: 0.06648039072751999\n",
      "Epoch 2556, Loss: 0.05818318109959364, Final Batch Loss: 0.010185285471379757\n",
      "Epoch 2557, Loss: 0.03691312111914158, Final Batch Loss: 0.016508162021636963\n",
      "Epoch 2558, Loss: 0.024130597710609436, Final Batch Loss: 0.0054408591240644455\n",
      "Epoch 2559, Loss: 0.04543231427669525, Final Batch Loss: 0.004753135144710541\n",
      "Epoch 2560, Loss: 0.0826814416795969, Final Batch Loss: 0.05936380475759506\n",
      "Epoch 2561, Loss: 0.06598053686320782, Final Batch Loss: 0.021564511582255363\n",
      "Epoch 2562, Loss: 0.06516853347420692, Final Batch Loss: 0.032718200236558914\n",
      "Epoch 2563, Loss: 0.08220777846872807, Final Batch Loss: 0.0052107516676187515\n",
      "Epoch 2564, Loss: 0.09320162236690521, Final Batch Loss: 0.03984455764293671\n",
      "Epoch 2565, Loss: 0.03923038113862276, Final Batch Loss: 0.011434030719101429\n",
      "Epoch 2566, Loss: 0.02427701884880662, Final Batch Loss: 0.005219195503741503\n",
      "Epoch 2567, Loss: 0.07559292763471603, Final Batch Loss: 0.011708036065101624\n",
      "Epoch 2568, Loss: 0.05635133944451809, Final Batch Loss: 0.029994698241353035\n",
      "Epoch 2569, Loss: 0.03924625925719738, Final Batch Loss: 0.013641389086842537\n",
      "Epoch 2570, Loss: 0.04860316216945648, Final Batch Loss: 0.020401936024427414\n",
      "Epoch 2571, Loss: 0.0513212401419878, Final Batch Loss: 0.007014235481619835\n",
      "Epoch 2572, Loss: 0.03218970447778702, Final Batch Loss: 0.008139872923493385\n",
      "Epoch 2573, Loss: 0.03078683651983738, Final Batch Loss: 0.007500778883695602\n",
      "Epoch 2574, Loss: 0.027379503706470132, Final Batch Loss: 0.0031850601080805063\n",
      "Epoch 2575, Loss: 0.04478234611451626, Final Batch Loss: 0.018337901681661606\n",
      "Epoch 2576, Loss: 0.037436443381011486, Final Batch Loss: 0.007874487899243832\n",
      "Epoch 2577, Loss: 0.03519904147833586, Final Batch Loss: 0.008192797191441059\n",
      "Epoch 2578, Loss: 0.06202791631221771, Final Batch Loss: 0.03105378895998001\n",
      "Epoch 2579, Loss: 0.04115547239780426, Final Batch Loss: 0.01491142064332962\n",
      "Epoch 2580, Loss: 0.07429620251059532, Final Batch Loss: 0.01617564633488655\n",
      "Epoch 2581, Loss: 0.2171400785446167, Final Batch Loss: 0.17610089480876923\n",
      "Epoch 2582, Loss: 0.11254405975341797, Final Batch Loss: 0.08124691992998123\n",
      "Epoch 2583, Loss: 0.029871201142668724, Final Batch Loss: 0.008806798607110977\n",
      "Epoch 2584, Loss: 0.05335008539259434, Final Batch Loss: 0.008692590519785881\n",
      "Epoch 2585, Loss: 0.04256525635719299, Final Batch Loss: 0.018873978406190872\n",
      "Epoch 2586, Loss: 0.04363776743412018, Final Batch Loss: 0.02220178209245205\n",
      "Epoch 2587, Loss: 0.04142747074365616, Final Batch Loss: 0.018491506576538086\n",
      "Epoch 2588, Loss: 0.04404604807496071, Final Batch Loss: 0.023368896916508675\n",
      "Epoch 2589, Loss: 0.04970633238554001, Final Batch Loss: 0.012260101735591888\n",
      "Epoch 2590, Loss: 0.028870333917438984, Final Batch Loss: 0.011122633703052998\n",
      "Epoch 2591, Loss: 0.08839627727866173, Final Batch Loss: 0.0451025664806366\n",
      "Epoch 2592, Loss: 0.09519679099321365, Final Batch Loss: 0.014571458101272583\n",
      "Epoch 2593, Loss: 0.048555254470556974, Final Batch Loss: 0.0052018011920154095\n",
      "Epoch 2594, Loss: 0.06802807375788689, Final Batch Loss: 0.019565194845199585\n",
      "Epoch 2595, Loss: 0.060463981702923775, Final Batch Loss: 0.021924180909991264\n",
      "Epoch 2596, Loss: 0.06464923918247223, Final Batch Loss: 0.030911728739738464\n",
      "Epoch 2597, Loss: 0.031038145534694195, Final Batch Loss: 0.019624054431915283\n",
      "Epoch 2598, Loss: 0.08917666971683502, Final Batch Loss: 0.03183315321803093\n",
      "Epoch 2599, Loss: 0.05433298461139202, Final Batch Loss: 0.020174318924546242\n",
      "Epoch 2600, Loss: 0.05634123831987381, Final Batch Loss: 0.030441006645560265\n",
      "Epoch 2601, Loss: 0.08495604619383812, Final Batch Loss: 0.06400804966688156\n",
      "Epoch 2602, Loss: 0.0497242072597146, Final Batch Loss: 0.01463219616562128\n",
      "Epoch 2603, Loss: 0.044430097565054893, Final Batch Loss: 0.024941403418779373\n",
      "Epoch 2604, Loss: 0.06833415292203426, Final Batch Loss: 0.04217638820409775\n",
      "Epoch 2605, Loss: 0.0424233116209507, Final Batch Loss: 0.023089177906513214\n",
      "Epoch 2606, Loss: 0.05405020713806152, Final Batch Loss: 0.021610386669635773\n",
      "Epoch 2607, Loss: 0.06519009359180927, Final Batch Loss: 0.03604955971240997\n",
      "Epoch 2608, Loss: 0.08135258033871651, Final Batch Loss: 0.032337263226509094\n",
      "Epoch 2609, Loss: 0.05809898488223553, Final Batch Loss: 0.018642233684659004\n",
      "Epoch 2610, Loss: 0.07638957165181637, Final Batch Loss: 0.05565651133656502\n",
      "Epoch 2611, Loss: 0.0643815491348505, Final Batch Loss: 0.0415511317551136\n",
      "Epoch 2612, Loss: 0.050451960414648056, Final Batch Loss: 0.032520841807127\n",
      "Epoch 2613, Loss: 0.05792931467294693, Final Batch Loss: 0.018108349293470383\n",
      "Epoch 2614, Loss: 0.056661395356059074, Final Batch Loss: 0.02039414457976818\n",
      "Epoch 2615, Loss: 0.0701555423438549, Final Batch Loss: 0.0324515663087368\n",
      "Epoch 2616, Loss: 0.04509134404361248, Final Batch Loss: 0.012600591406226158\n",
      "Epoch 2617, Loss: 0.03772745653986931, Final Batch Loss: 0.016211913898587227\n",
      "Epoch 2618, Loss: 0.08420618064701557, Final Batch Loss: 0.025642873719334602\n",
      "Epoch 2619, Loss: 0.05183650553226471, Final Batch Loss: 0.026003582403063774\n",
      "Epoch 2620, Loss: 0.034222167916595936, Final Batch Loss: 0.011623715050518513\n",
      "Epoch 2621, Loss: 0.04035920836031437, Final Batch Loss: 0.01769350655376911\n",
      "Epoch 2622, Loss: 0.06002633087337017, Final Batch Loss: 0.040393732488155365\n",
      "Epoch 2623, Loss: 0.01836410630494356, Final Batch Loss: 0.0037259450182318687\n",
      "Epoch 2624, Loss: 0.12818549945950508, Final Batch Loss: 0.07098802924156189\n",
      "Epoch 2625, Loss: 0.03276980482041836, Final Batch Loss: 0.0075233932584524155\n",
      "Epoch 2626, Loss: 0.06174792721867561, Final Batch Loss: 0.023478426039218903\n",
      "Epoch 2627, Loss: 0.04615388251841068, Final Batch Loss: 0.015524160116910934\n",
      "Epoch 2628, Loss: 0.0408853841945529, Final Batch Loss: 0.012762834317982197\n",
      "Epoch 2629, Loss: 0.031229001469910145, Final Batch Loss: 0.01368079986423254\n",
      "Epoch 2630, Loss: 0.06659466307610273, Final Batch Loss: 0.05423429608345032\n",
      "Epoch 2631, Loss: 0.10357526689767838, Final Batch Loss: 0.06655793637037277\n",
      "Epoch 2632, Loss: 0.04203302972018719, Final Batch Loss: 0.030379217118024826\n",
      "Epoch 2633, Loss: 0.03159434162080288, Final Batch Loss: 0.01181294210255146\n",
      "Epoch 2634, Loss: 0.04440788924694061, Final Batch Loss: 0.020847661420702934\n",
      "Epoch 2635, Loss: 0.06087581813335419, Final Batch Loss: 0.016421735286712646\n",
      "Epoch 2636, Loss: 0.0705945324152708, Final Batch Loss: 0.022153107449412346\n",
      "Epoch 2637, Loss: 0.026909750886261463, Final Batch Loss: 0.008331107906997204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2638, Loss: 0.039290945045650005, Final Batch Loss: 0.008433903567492962\n",
      "Epoch 2639, Loss: 0.05467196414247155, Final Batch Loss: 0.005694253835827112\n",
      "Epoch 2640, Loss: 0.044374026358127594, Final Batch Loss: 0.009731888771057129\n",
      "Epoch 2641, Loss: 0.0643972847610712, Final Batch Loss: 0.027964042499661446\n",
      "Epoch 2642, Loss: 0.035735842771828175, Final Batch Loss: 0.023458128795027733\n",
      "Epoch 2643, Loss: 0.06753521412611008, Final Batch Loss: 0.04655160754919052\n",
      "Epoch 2644, Loss: 0.04077192582190037, Final Batch Loss: 0.010745000094175339\n",
      "Epoch 2645, Loss: 0.06889447756111622, Final Batch Loss: 0.020872609689831734\n",
      "Epoch 2646, Loss: 0.021818792447447777, Final Batch Loss: 0.009169955737888813\n",
      "Epoch 2647, Loss: 0.050411103293299675, Final Batch Loss: 0.008903870359063148\n",
      "Epoch 2648, Loss: 0.23504526168107986, Final Batch Loss: 0.19158032536506653\n",
      "Epoch 2649, Loss: 0.1148068867623806, Final Batch Loss: 0.049411457031965256\n",
      "Epoch 2650, Loss: 0.019181276205927134, Final Batch Loss: 0.00424096779897809\n",
      "Epoch 2651, Loss: 0.04712793603539467, Final Batch Loss: 0.02272510901093483\n",
      "Epoch 2652, Loss: 0.03487254586070776, Final Batch Loss: 0.006970786489546299\n",
      "Epoch 2653, Loss: 0.06148912571370602, Final Batch Loss: 0.04140852391719818\n",
      "Epoch 2654, Loss: 0.08447759971022606, Final Batch Loss: 0.020789962261915207\n",
      "Epoch 2655, Loss: 0.026739399880170822, Final Batch Loss: 0.010092779994010925\n",
      "Epoch 2656, Loss: 0.06115524657070637, Final Batch Loss: 0.031932394951581955\n",
      "Epoch 2657, Loss: 0.06604867987334728, Final Batch Loss: 0.01883849687874317\n",
      "Epoch 2658, Loss: 0.10723553597927094, Final Batch Loss: 0.017271339893341064\n",
      "Epoch 2659, Loss: 0.07956070452928543, Final Batch Loss: 0.049418509006500244\n",
      "Epoch 2660, Loss: 0.07732198759913445, Final Batch Loss: 0.047554533928632736\n",
      "Epoch 2661, Loss: 0.05149947665631771, Final Batch Loss: 0.017819037660956383\n",
      "Epoch 2662, Loss: 0.09319654852151871, Final Batch Loss: 0.04057910293340683\n",
      "Epoch 2663, Loss: 0.019375931471586227, Final Batch Loss: 0.010201314464211464\n",
      "Epoch 2664, Loss: 0.044716402888298035, Final Batch Loss: 0.019131716340780258\n",
      "Epoch 2665, Loss: 0.03622326161712408, Final Batch Loss: 0.006776151247322559\n",
      "Epoch 2666, Loss: 0.04408060014247894, Final Batch Loss: 0.018464962020516396\n",
      "Epoch 2667, Loss: 0.044561078771948814, Final Batch Loss: 0.023490166291594505\n",
      "Epoch 2668, Loss: 0.09324324876070023, Final Batch Loss: 0.06694705784320831\n",
      "Epoch 2669, Loss: 0.12039920501410961, Final Batch Loss: 0.10300617665052414\n",
      "Epoch 2670, Loss: 0.08730434998869896, Final Batch Loss: 0.04622436314821243\n",
      "Epoch 2671, Loss: 0.03211112320423126, Final Batch Loss: 0.01578177884221077\n",
      "Epoch 2672, Loss: 0.10379296541213989, Final Batch Loss: 0.08470257371664047\n",
      "Epoch 2673, Loss: 0.06804849952459335, Final Batch Loss: 0.0415550172328949\n",
      "Epoch 2674, Loss: 0.09322491101920605, Final Batch Loss: 0.08366359770298004\n",
      "Epoch 2675, Loss: 0.05422841012477875, Final Batch Loss: 0.021703455597162247\n",
      "Epoch 2676, Loss: 0.12571127340197563, Final Batch Loss: 0.10400745272636414\n",
      "Epoch 2677, Loss: 0.0599290756508708, Final Batch Loss: 0.046781960874795914\n",
      "Epoch 2678, Loss: 0.07211842574179173, Final Batch Loss: 0.05565638467669487\n",
      "Epoch 2679, Loss: 0.07039503939449787, Final Batch Loss: 0.0209506768733263\n",
      "Epoch 2680, Loss: 0.05028587020933628, Final Batch Loss: 0.02422480657696724\n",
      "Epoch 2681, Loss: 0.09895487129688263, Final Batch Loss: 0.054732948541641235\n",
      "Epoch 2682, Loss: 0.14080240577459335, Final Batch Loss: 0.08278435468673706\n",
      "Epoch 2683, Loss: 0.03519336972385645, Final Batch Loss: 0.014243192039430141\n",
      "Epoch 2684, Loss: 0.05950072966516018, Final Batch Loss: 0.0352918766438961\n",
      "Epoch 2685, Loss: 0.044919852167367935, Final Batch Loss: 0.01615719124674797\n",
      "Epoch 2686, Loss: 0.08048271387815475, Final Batch Loss: 0.0388217568397522\n",
      "Epoch 2687, Loss: 0.053665757179260254, Final Batch Loss: 0.030221927911043167\n",
      "Epoch 2688, Loss: 0.04351565055549145, Final Batch Loss: 0.01697903499007225\n",
      "Epoch 2689, Loss: 0.06700737215578556, Final Batch Loss: 0.02129753865301609\n",
      "Epoch 2690, Loss: 0.09228554368019104, Final Batch Loss: 0.06151283532381058\n",
      "Epoch 2691, Loss: 0.07099056895822287, Final Batch Loss: 0.055991996079683304\n",
      "Epoch 2692, Loss: 0.10245763510465622, Final Batch Loss: 0.05665212124586105\n",
      "Epoch 2693, Loss: 0.04346512816846371, Final Batch Loss: 0.013060957193374634\n",
      "Epoch 2694, Loss: 0.02881074883043766, Final Batch Loss: 0.020647656172513962\n",
      "Epoch 2695, Loss: 0.0388491190969944, Final Batch Loss: 0.015215305611491203\n",
      "Epoch 2696, Loss: 0.054287402890622616, Final Batch Loss: 0.010670810006558895\n",
      "Epoch 2697, Loss: 0.031060132198035717, Final Batch Loss: 0.006713715381920338\n",
      "Epoch 2698, Loss: 0.07413093186914921, Final Batch Loss: 0.017218513414263725\n",
      "Epoch 2699, Loss: 0.031664027366787195, Final Batch Loss: 0.007576745469123125\n",
      "Epoch 2700, Loss: 0.03708963468670845, Final Batch Loss: 0.01059270091354847\n",
      "Epoch 2701, Loss: 0.03207389172166586, Final Batch Loss: 0.01324810553342104\n",
      "Epoch 2702, Loss: 0.0237688347697258, Final Batch Loss: 0.009972971864044666\n",
      "Epoch 2703, Loss: 0.04151102155447006, Final Batch Loss: 0.013265576213598251\n",
      "Epoch 2704, Loss: 0.03519053943455219, Final Batch Loss: 0.01285526528954506\n",
      "Epoch 2705, Loss: 0.0823429562151432, Final Batch Loss: 0.05675334110856056\n",
      "Epoch 2706, Loss: 0.13488281145691872, Final Batch Loss: 0.10245408862829208\n",
      "Epoch 2707, Loss: 0.08726435899734497, Final Batch Loss: 0.06408275663852692\n",
      "Epoch 2708, Loss: 0.05211907625198364, Final Batch Loss: 0.04005421698093414\n",
      "Epoch 2709, Loss: 0.02351447194814682, Final Batch Loss: 0.011458015069365501\n",
      "Epoch 2710, Loss: 0.04990868456661701, Final Batch Loss: 0.03404625132679939\n",
      "Epoch 2711, Loss: 0.038586956448853016, Final Batch Loss: 0.023730657994747162\n",
      "Epoch 2712, Loss: 0.0415178406983614, Final Batch Loss: 0.007176415994763374\n",
      "Epoch 2713, Loss: 0.03359268046915531, Final Batch Loss: 0.01633472740650177\n",
      "Epoch 2714, Loss: 0.01995550049468875, Final Batch Loss: 0.006252279970794916\n",
      "Epoch 2715, Loss: 0.03915855381637812, Final Batch Loss: 0.008265954442322254\n",
      "Epoch 2716, Loss: 0.022458831779658794, Final Batch Loss: 0.006529604084789753\n",
      "Epoch 2717, Loss: 0.05948524363338947, Final Batch Loss: 0.02974369190633297\n",
      "Epoch 2718, Loss: 0.040523788426071405, Final Batch Loss: 0.005617415998131037\n",
      "Epoch 2719, Loss: 0.03585298266261816, Final Batch Loss: 0.011101565323770046\n",
      "Epoch 2720, Loss: 0.02168944338336587, Final Batch Loss: 0.014821105636656284\n",
      "Epoch 2721, Loss: 0.038098543882369995, Final Batch Loss: 0.016211915761232376\n",
      "Epoch 2722, Loss: 0.04449954070150852, Final Batch Loss: 0.02840367890894413\n",
      "Epoch 2723, Loss: 0.08958729729056358, Final Batch Loss: 0.03458017855882645\n",
      "Epoch 2724, Loss: 0.04465991444885731, Final Batch Loss: 0.02603849023580551\n",
      "Epoch 2725, Loss: 0.03545860480517149, Final Batch Loss: 0.007563111372292042\n",
      "Epoch 2726, Loss: 0.08000720664858818, Final Batch Loss: 0.030080802738666534\n",
      "Epoch 2727, Loss: 0.02493867091834545, Final Batch Loss: 0.007009366527199745\n",
      "Epoch 2728, Loss: 0.018908968660980463, Final Batch Loss: 0.002571370918303728\n",
      "Epoch 2729, Loss: 0.034987992607057095, Final Batch Loss: 0.009343153797090054\n",
      "Epoch 2730, Loss: 0.05634172447025776, Final Batch Loss: 0.019510170444846153\n",
      "Epoch 2731, Loss: 0.05554089043289423, Final Batch Loss: 0.04045338183641434\n",
      "Epoch 2732, Loss: 0.026652727276086807, Final Batch Loss: 0.006195547059178352\n",
      "Epoch 2733, Loss: 0.024779762141406536, Final Batch Loss: 0.007979531772434711\n",
      "Epoch 2734, Loss: 0.06842167302966118, Final Batch Loss: 0.015050698071718216\n",
      "Epoch 2735, Loss: 0.029150943038985133, Final Batch Loss: 0.0013778700958937407\n",
      "Epoch 2736, Loss: 0.04520754609256983, Final Batch Loss: 0.03280623257160187\n",
      "Epoch 2737, Loss: 0.11043769866228104, Final Batch Loss: 0.07388634979724884\n",
      "Epoch 2738, Loss: 0.047431787475943565, Final Batch Loss: 0.019071681424975395\n",
      "Epoch 2739, Loss: 0.07024871185421944, Final Batch Loss: 0.059850454330444336\n",
      "Epoch 2740, Loss: 0.040121045894920826, Final Batch Loss: 0.01295475009828806\n",
      "Epoch 2741, Loss: 0.0254707969725132, Final Batch Loss: 0.01829090528190136\n",
      "Epoch 2742, Loss: 0.03884864877909422, Final Batch Loss: 0.01024800818413496\n",
      "Epoch 2743, Loss: 0.046174049377441406, Final Batch Loss: 0.021494360640645027\n",
      "Epoch 2744, Loss: 0.01573715778067708, Final Batch Loss: 0.008001926355063915\n",
      "Epoch 2745, Loss: 0.05269312672317028, Final Batch Loss: 0.028663018718361855\n",
      "Epoch 2746, Loss: 0.039793919771909714, Final Batch Loss: 0.013972725719213486\n",
      "Epoch 2747, Loss: 0.03568987920880318, Final Batch Loss: 0.0031449533998966217\n",
      "Epoch 2748, Loss: 0.0764237716794014, Final Batch Loss: 0.0357477180659771\n",
      "Epoch 2749, Loss: 0.04677360691130161, Final Batch Loss: 0.022789442911744118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2750, Loss: 0.08625495247542858, Final Batch Loss: 0.06916066259145737\n",
      "Epoch 2751, Loss: 0.014441253384575248, Final Batch Loss: 0.002379361307248473\n",
      "Epoch 2752, Loss: 0.08457441674545407, Final Batch Loss: 0.004640259314328432\n",
      "Epoch 2753, Loss: 0.06340577825903893, Final Batch Loss: 0.034999046474695206\n",
      "Epoch 2754, Loss: 0.037534965202212334, Final Batch Loss: 0.021735643967986107\n",
      "Epoch 2755, Loss: 0.03664717264473438, Final Batch Loss: 0.00870007835328579\n",
      "Epoch 2756, Loss: 0.02154942136257887, Final Batch Loss: 0.013067045249044895\n",
      "Epoch 2757, Loss: 0.043666888028383255, Final Batch Loss: 0.017181115224957466\n",
      "Epoch 2758, Loss: 0.043200934305787086, Final Batch Loss: 0.02137978933751583\n",
      "Epoch 2759, Loss: 0.09442249312996864, Final Batch Loss: 0.05291338637471199\n",
      "Epoch 2760, Loss: 0.06285255961120129, Final Batch Loss: 0.04474222660064697\n",
      "Epoch 2761, Loss: 0.05397047847509384, Final Batch Loss: 0.03810301795601845\n",
      "Epoch 2762, Loss: 0.030777242965996265, Final Batch Loss: 0.011700593866407871\n",
      "Epoch 2763, Loss: 0.03821941278874874, Final Batch Loss: 0.021070681512355804\n",
      "Epoch 2764, Loss: 0.05375511944293976, Final Batch Loss: 0.020794596523046494\n",
      "Epoch 2765, Loss: 0.059143923223018646, Final Batch Loss: 0.01696586236357689\n",
      "Epoch 2766, Loss: 0.09090598672628403, Final Batch Loss: 0.014054112136363983\n",
      "Epoch 2767, Loss: 0.08885863330215216, Final Batch Loss: 0.006044371984899044\n",
      "Epoch 2768, Loss: 0.025932569056749344, Final Batch Loss: 0.011286083608865738\n",
      "Epoch 2769, Loss: 0.0462124515324831, Final Batch Loss: 0.029742099344730377\n",
      "Epoch 2770, Loss: 0.050616408698260784, Final Batch Loss: 0.03863687813282013\n",
      "Epoch 2771, Loss: 0.056889123283326626, Final Batch Loss: 0.013563464395701885\n",
      "Epoch 2772, Loss: 0.04970488324761391, Final Batch Loss: 0.013130832463502884\n",
      "Epoch 2773, Loss: 0.05865918844938278, Final Batch Loss: 0.030456282198429108\n",
      "Epoch 2774, Loss: 0.02794601023197174, Final Batch Loss: 0.0109199658036232\n",
      "Epoch 2775, Loss: 0.07808076776564121, Final Batch Loss: 0.030311303213238716\n",
      "Epoch 2776, Loss: 0.035827611573040485, Final Batch Loss: 0.021017810329794884\n",
      "Epoch 2777, Loss: 0.02599987853318453, Final Batch Loss: 0.011843734420835972\n",
      "Epoch 2778, Loss: 0.06421690620481968, Final Batch Loss: 0.020228100940585136\n",
      "Epoch 2779, Loss: 0.04910318274050951, Final Batch Loss: 0.014360425062477589\n",
      "Epoch 2780, Loss: 0.09510031156241894, Final Batch Loss: 0.0660460814833641\n",
      "Epoch 2781, Loss: 0.21083780005574226, Final Batch Loss: 0.1804281324148178\n",
      "Epoch 2782, Loss: 0.017237807624042034, Final Batch Loss: 0.008583800867199898\n",
      "Epoch 2783, Loss: 0.038675121031701565, Final Batch Loss: 0.012076123617589474\n",
      "Epoch 2784, Loss: 0.041233853437006474, Final Batch Loss: 0.013528346084058285\n",
      "Epoch 2785, Loss: 0.05831295810639858, Final Batch Loss: 0.041410062462091446\n",
      "Epoch 2786, Loss: 0.028815648518502712, Final Batch Loss: 0.00533419381827116\n",
      "Epoch 2787, Loss: 0.036450562067329884, Final Batch Loss: 0.004463815130293369\n",
      "Epoch 2788, Loss: 0.10324038937687874, Final Batch Loss: 0.07155799120664597\n",
      "Epoch 2789, Loss: 0.06837508082389832, Final Batch Loss: 0.019446860998868942\n",
      "Epoch 2790, Loss: 0.04872805532068014, Final Batch Loss: 0.013779296539723873\n",
      "Epoch 2791, Loss: 0.0242239311337471, Final Batch Loss: 0.012091493234038353\n",
      "Epoch 2792, Loss: 0.05402485653758049, Final Batch Loss: 0.021723348647356033\n",
      "Epoch 2793, Loss: 0.055776823312044144, Final Batch Loss: 0.028199734166264534\n",
      "Epoch 2794, Loss: 0.06271929293870926, Final Batch Loss: 0.02349051833152771\n",
      "Epoch 2795, Loss: 0.03596467478200793, Final Batch Loss: 0.006590263452380896\n",
      "Epoch 2796, Loss: 0.04687308147549629, Final Batch Loss: 0.03828214854001999\n",
      "Epoch 2797, Loss: 0.03280079085379839, Final Batch Loss: 0.009937782771885395\n",
      "Epoch 2798, Loss: 0.0884312316775322, Final Batch Loss: 0.038906529545784\n",
      "Epoch 2799, Loss: 0.10197506658732891, Final Batch Loss: 0.08468639105558395\n",
      "Epoch 2800, Loss: 0.054764701053500175, Final Batch Loss: 0.01309664361178875\n",
      "Epoch 2801, Loss: 0.048634614795446396, Final Batch Loss: 0.019688377156853676\n",
      "Epoch 2802, Loss: 0.03599428944289684, Final Batch Loss: 0.021563418209552765\n",
      "Epoch 2803, Loss: 0.035082449205219746, Final Batch Loss: 0.025580408051609993\n",
      "Epoch 2804, Loss: 0.04647667892277241, Final Batch Loss: 0.02676461450755596\n",
      "Epoch 2805, Loss: 0.038631356321275234, Final Batch Loss: 0.028961138799786568\n",
      "Epoch 2806, Loss: 0.07348418422043324, Final Batch Loss: 0.05932969972491264\n",
      "Epoch 2807, Loss: 0.043131875805556774, Final Batch Loss: 0.011273658834397793\n",
      "Epoch 2808, Loss: 0.04538624361157417, Final Batch Loss: 0.018507150933146477\n",
      "Epoch 2809, Loss: 0.02547261491417885, Final Batch Loss: 0.00890311412513256\n",
      "Epoch 2810, Loss: 0.03923533204942942, Final Batch Loss: 0.01433741394430399\n",
      "Epoch 2811, Loss: 0.03018935862928629, Final Batch Loss: 0.008873042650520802\n",
      "Epoch 2812, Loss: 0.0889190062880516, Final Batch Loss: 0.04911085218191147\n",
      "Epoch 2813, Loss: 0.04023994691669941, Final Batch Loss: 0.017330288887023926\n",
      "Epoch 2814, Loss: 0.02733672270551324, Final Batch Loss: 0.005630119238048792\n",
      "Epoch 2815, Loss: 0.041621072217822075, Final Batch Loss: 0.01277942955493927\n",
      "Epoch 2816, Loss: 0.04456233140081167, Final Batch Loss: 0.011557972989976406\n",
      "Epoch 2817, Loss: 0.03799998667091131, Final Batch Loss: 0.015185222961008549\n",
      "Epoch 2818, Loss: 0.09893769398331642, Final Batch Loss: 0.061106402426958084\n",
      "Epoch 2819, Loss: 0.03597326576709747, Final Batch Loss: 0.01914559304714203\n",
      "Epoch 2820, Loss: 0.07664323225617409, Final Batch Loss: 0.014227345585823059\n",
      "Epoch 2821, Loss: 0.07824311219155788, Final Batch Loss: 0.04837793484330177\n",
      "Epoch 2822, Loss: 0.054997073486447334, Final Batch Loss: 0.031834136694669724\n",
      "Epoch 2823, Loss: 0.033473371528089046, Final Batch Loss: 0.020490171387791634\n",
      "Epoch 2824, Loss: 0.0455377334728837, Final Batch Loss: 0.010101956315338612\n",
      "Epoch 2825, Loss: 0.0504702627658844, Final Batch Loss: 0.020602978765964508\n",
      "Epoch 2826, Loss: 0.039906153455376625, Final Batch Loss: 0.0065805334597826\n",
      "Epoch 2827, Loss: 0.03688162472099066, Final Batch Loss: 0.009246365167200565\n",
      "Epoch 2828, Loss: 0.03664248436689377, Final Batch Loss: 0.015819881111383438\n",
      "Epoch 2829, Loss: 0.042824032716453075, Final Batch Loss: 0.015267211012542248\n",
      "Epoch 2830, Loss: 0.054734671488404274, Final Batch Loss: 0.03740399703383446\n",
      "Epoch 2831, Loss: 0.01761955628171563, Final Batch Loss: 0.005762575659900904\n",
      "Epoch 2832, Loss: 0.030843633227050304, Final Batch Loss: 0.016044283285737038\n",
      "Epoch 2833, Loss: 0.02078224578872323, Final Batch Loss: 0.007288074586540461\n",
      "Epoch 2834, Loss: 0.035460819490253925, Final Batch Loss: 0.007811904884874821\n",
      "Epoch 2835, Loss: 0.03749846015125513, Final Batch Loss: 0.008778763003647327\n",
      "Epoch 2836, Loss: 0.18572423048317432, Final Batch Loss: 0.15792858600616455\n",
      "Epoch 2837, Loss: 0.06716202944517136, Final Batch Loss: 0.04464263841509819\n",
      "Epoch 2838, Loss: 0.06400453299283981, Final Batch Loss: 0.02293045073747635\n",
      "Epoch 2839, Loss: 0.05002864310517907, Final Batch Loss: 0.007570064160972834\n",
      "Epoch 2840, Loss: 0.0698392279446125, Final Batch Loss: 0.02501392364501953\n",
      "Epoch 2841, Loss: 0.02446433308068663, Final Batch Loss: 0.0017065104329958558\n",
      "Epoch 2842, Loss: 0.03460272587835789, Final Batch Loss: 0.01270010881125927\n",
      "Epoch 2843, Loss: 0.03948599752038717, Final Batch Loss: 0.02660437487065792\n",
      "Epoch 2844, Loss: 0.05424228357151151, Final Batch Loss: 0.005738515872508287\n",
      "Epoch 2845, Loss: 0.016655273968353868, Final Batch Loss: 0.003135320032015443\n",
      "Epoch 2846, Loss: 0.03656326700001955, Final Batch Loss: 0.003995371051132679\n",
      "Epoch 2847, Loss: 0.022974598687142134, Final Batch Loss: 0.005582868587225676\n",
      "Epoch 2848, Loss: 0.03774503245949745, Final Batch Loss: 0.01759124919772148\n",
      "Epoch 2849, Loss: 0.12115783803164959, Final Batch Loss: 0.10216809809207916\n",
      "Epoch 2850, Loss: 0.14416717365384102, Final Batch Loss: 0.13109628856182098\n",
      "Epoch 2851, Loss: 0.029756939504295588, Final Batch Loss: 0.005815079901367426\n",
      "Epoch 2852, Loss: 0.04892808198928833, Final Batch Loss: 0.027003243565559387\n",
      "Epoch 2853, Loss: 0.03651395067572594, Final Batch Loss: 0.011820467188954353\n",
      "Epoch 2854, Loss: 0.03707927465438843, Final Batch Loss: 0.022961260750889778\n",
      "Epoch 2855, Loss: 0.030510939192026854, Final Batch Loss: 0.004336632322520018\n",
      "Epoch 2856, Loss: 0.08881031908094883, Final Batch Loss: 0.0307746734470129\n",
      "Epoch 2857, Loss: 0.06857994571328163, Final Batch Loss: 0.035240888595581055\n",
      "Epoch 2858, Loss: 0.017308217007666826, Final Batch Loss: 0.007406196091324091\n",
      "Epoch 2859, Loss: 0.06857522390782833, Final Batch Loss: 0.047758519649505615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2860, Loss: 0.04160406719893217, Final Batch Loss: 0.028248609974980354\n",
      "Epoch 2861, Loss: 0.043054716661572456, Final Batch Loss: 0.01946706883609295\n",
      "Epoch 2862, Loss: 0.029503798112273216, Final Batch Loss: 0.011669803410768509\n",
      "Epoch 2863, Loss: 0.04293630365282297, Final Batch Loss: 0.012520390562713146\n",
      "Epoch 2864, Loss: 0.15521028265357018, Final Batch Loss: 0.12845559418201447\n",
      "Epoch 2865, Loss: 0.07582939602434635, Final Batch Loss: 0.0458589531481266\n",
      "Epoch 2866, Loss: 0.040520571172237396, Final Batch Loss: 0.018403837457299232\n",
      "Epoch 2867, Loss: 0.03983000107109547, Final Batch Loss: 0.02012069709599018\n",
      "Epoch 2868, Loss: 0.019531741738319397, Final Batch Loss: 0.007855783216655254\n",
      "Epoch 2869, Loss: 0.05280754156410694, Final Batch Loss: 0.012797323986887932\n",
      "Epoch 2870, Loss: 0.0437896428629756, Final Batch Loss: 0.032089799642562866\n",
      "Epoch 2871, Loss: 0.029493138194084167, Final Batch Loss: 0.006237437948584557\n",
      "Epoch 2872, Loss: 0.05378059484064579, Final Batch Loss: 0.02639034017920494\n",
      "Epoch 2873, Loss: 0.062325493432581425, Final Batch Loss: 0.005523555912077427\n",
      "Epoch 2874, Loss: 0.026267148554325104, Final Batch Loss: 0.006373705342411995\n",
      "Epoch 2875, Loss: 0.07126031629741192, Final Batch Loss: 0.053689226508140564\n",
      "Epoch 2876, Loss: 0.04065867932513356, Final Batch Loss: 0.0059349616058170795\n",
      "Epoch 2877, Loss: 0.02895870478823781, Final Batch Loss: 0.02152610756456852\n",
      "Epoch 2878, Loss: 0.04571142513304949, Final Batch Loss: 0.03076557256281376\n",
      "Epoch 2879, Loss: 0.09653646126389503, Final Batch Loss: 0.07790125161409378\n",
      "Epoch 2880, Loss: 0.029127446934580803, Final Batch Loss: 0.013425342738628387\n",
      "Epoch 2881, Loss: 0.03490063734352589, Final Batch Loss: 0.019722802564501762\n",
      "Epoch 2882, Loss: 0.03131726663559675, Final Batch Loss: 0.004946415312588215\n",
      "Epoch 2883, Loss: 0.06171081028878689, Final Batch Loss: 0.0408015213906765\n",
      "Epoch 2884, Loss: 0.08258361648768187, Final Batch Loss: 0.01384230237454176\n",
      "Epoch 2885, Loss: 0.05562303215265274, Final Batch Loss: 0.01743045449256897\n",
      "Epoch 2886, Loss: 0.12096774205565453, Final Batch Loss: 0.06296566128730774\n",
      "Epoch 2887, Loss: 0.04584277607500553, Final Batch Loss: 0.012436041608452797\n",
      "Epoch 2888, Loss: 0.023398718796670437, Final Batch Loss: 0.01521911658346653\n",
      "Epoch 2889, Loss: 0.05162994936108589, Final Batch Loss: 0.029847867786884308\n",
      "Epoch 2890, Loss: 0.03959508240222931, Final Batch Loss: 0.015903914347290993\n",
      "Epoch 2891, Loss: 0.03211183566600084, Final Batch Loss: 0.012202179990708828\n",
      "Epoch 2892, Loss: 0.02216298133134842, Final Batch Loss: 0.010467859916388988\n",
      "Epoch 2893, Loss: 0.028319493867456913, Final Batch Loss: 0.014202441088855267\n",
      "Epoch 2894, Loss: 0.020471968222409487, Final Batch Loss: 0.007343105506151915\n",
      "Epoch 2895, Loss: 0.03965824842453003, Final Batch Loss: 0.02818019688129425\n",
      "Epoch 2896, Loss: 0.060604521073400974, Final Batch Loss: 0.013513305224478245\n",
      "Epoch 2897, Loss: 0.06597246788442135, Final Batch Loss: 0.027592206373810768\n",
      "Epoch 2898, Loss: 0.05416007153689861, Final Batch Loss: 0.03198843076825142\n",
      "Epoch 2899, Loss: 0.0430870046839118, Final Batch Loss: 0.03281303495168686\n",
      "Epoch 2900, Loss: 0.11964894831180573, Final Batch Loss: 0.0829416811466217\n",
      "Epoch 2901, Loss: 0.011800899636000395, Final Batch Loss: 0.005372717976570129\n",
      "Epoch 2902, Loss: 0.04430551826953888, Final Batch Loss: 0.014118501916527748\n",
      "Epoch 2903, Loss: 0.02541557583026588, Final Batch Loss: 0.0038028403650969267\n",
      "Epoch 2904, Loss: 0.038682032376527786, Final Batch Loss: 0.019596222788095474\n",
      "Epoch 2905, Loss: 0.02404968487098813, Final Batch Loss: 0.007277571130543947\n",
      "Epoch 2906, Loss: 0.067525296472013, Final Batch Loss: 0.055108483880758286\n",
      "Epoch 2907, Loss: 0.10214246436953545, Final Batch Loss: 0.038477156311273575\n",
      "Epoch 2908, Loss: 0.06163633614778519, Final Batch Loss: 0.0388559065759182\n",
      "Epoch 2909, Loss: 0.03601696062833071, Final Batch Loss: 0.02380194142460823\n",
      "Epoch 2910, Loss: 0.037494292482733727, Final Batch Loss: 0.017410723492503166\n",
      "Epoch 2911, Loss: 0.03904833272099495, Final Batch Loss: 0.005238182842731476\n",
      "Epoch 2912, Loss: 0.0636725202202797, Final Batch Loss: 0.023214243352413177\n",
      "Epoch 2913, Loss: 0.030029698740690947, Final Batch Loss: 0.0024771871976554394\n",
      "Epoch 2914, Loss: 0.04638876859098673, Final Batch Loss: 0.005611729808151722\n",
      "Epoch 2915, Loss: 0.08737380988895893, Final Batch Loss: 0.01748390682041645\n",
      "Epoch 2916, Loss: 0.095000802539289, Final Batch Loss: 0.08261211216449738\n",
      "Epoch 2917, Loss: 0.06767554581165314, Final Batch Loss: 0.02431749925017357\n",
      "Epoch 2918, Loss: 0.03219417296350002, Final Batch Loss: 0.0053188614547252655\n",
      "Epoch 2919, Loss: 0.09405946359038353, Final Batch Loss: 0.05584961920976639\n",
      "Epoch 2920, Loss: 0.10051367431879044, Final Batch Loss: 0.056945111602544785\n",
      "Epoch 2921, Loss: 0.08607842493802309, Final Batch Loss: 0.01424208004027605\n",
      "Epoch 2922, Loss: 0.033948661759495735, Final Batch Loss: 0.01506502740085125\n",
      "Epoch 2923, Loss: 0.037334127351641655, Final Batch Loss: 0.01876027323305607\n",
      "Epoch 2924, Loss: 0.04650265350937843, Final Batch Loss: 0.022743860259652138\n",
      "Epoch 2925, Loss: 0.039321635849773884, Final Batch Loss: 0.02918863110244274\n",
      "Epoch 2926, Loss: 0.04843434691429138, Final Batch Loss: 0.028506848961114883\n",
      "Epoch 2927, Loss: 0.08302379213273525, Final Batch Loss: 0.060091160237789154\n",
      "Epoch 2928, Loss: 0.039528392255306244, Final Batch Loss: 0.028558870777487755\n",
      "Epoch 2929, Loss: 0.01556222839280963, Final Batch Loss: 0.005125540774315596\n",
      "Epoch 2930, Loss: 0.025731507688760757, Final Batch Loss: 0.009848708286881447\n",
      "Epoch 2931, Loss: 0.0437488192692399, Final Batch Loss: 0.01058726105839014\n",
      "Epoch 2932, Loss: 0.11768605187535286, Final Batch Loss: 0.07730095833539963\n",
      "Epoch 2933, Loss: 0.03192219976335764, Final Batch Loss: 0.018711959943175316\n",
      "Epoch 2934, Loss: 0.058552294969558716, Final Batch Loss: 0.019853215664625168\n",
      "Epoch 2935, Loss: 0.04756387136876583, Final Batch Loss: 0.018075039610266685\n",
      "Epoch 2936, Loss: 0.03644021414220333, Final Batch Loss: 0.010222598910331726\n",
      "Epoch 2937, Loss: 0.057111283764243126, Final Batch Loss: 0.006629848852753639\n",
      "Epoch 2938, Loss: 0.043043702840805054, Final Batch Loss: 0.024808647111058235\n",
      "Epoch 2939, Loss: 0.057053509168326855, Final Batch Loss: 0.014043032191693783\n",
      "Epoch 2940, Loss: 0.032895585522055626, Final Batch Loss: 0.012504331767559052\n",
      "Epoch 2941, Loss: 0.027812642976641655, Final Batch Loss: 0.020880108699202538\n",
      "Epoch 2942, Loss: 0.061252882704138756, Final Batch Loss: 0.024666281417012215\n",
      "Epoch 2943, Loss: 0.028215988539159298, Final Batch Loss: 0.007861067540943623\n",
      "Epoch 2944, Loss: 0.059511832892894745, Final Batch Loss: 0.03473210707306862\n",
      "Epoch 2945, Loss: 0.04585371259599924, Final Batch Loss: 0.03368646278977394\n",
      "Epoch 2946, Loss: 0.03167627193033695, Final Batch Loss: 0.017310209572315216\n",
      "Epoch 2947, Loss: 0.029562880285084248, Final Batch Loss: 0.016039660200476646\n",
      "Epoch 2948, Loss: 0.0830460675060749, Final Batch Loss: 0.05023873224854469\n",
      "Epoch 2949, Loss: 0.10479966923594475, Final Batch Loss: 0.06949301064014435\n",
      "Epoch 2950, Loss: 0.04002863820642233, Final Batch Loss: 0.013055616058409214\n",
      "Epoch 2951, Loss: 0.05160495452582836, Final Batch Loss: 0.028050782158970833\n",
      "Epoch 2952, Loss: 0.06429404206573963, Final Batch Loss: 0.05858706682920456\n",
      "Epoch 2953, Loss: 0.020115112885832787, Final Batch Loss: 0.010130934417247772\n",
      "Epoch 2954, Loss: 0.03334811073727906, Final Batch Loss: 0.002637495519593358\n",
      "Epoch 2955, Loss: 0.06763429380953312, Final Batch Loss: 0.0189445149153471\n",
      "Epoch 2956, Loss: 0.06722250021994114, Final Batch Loss: 0.03932814300060272\n",
      "Epoch 2957, Loss: 0.01890244334936142, Final Batch Loss: 0.010209174826741219\n",
      "Epoch 2958, Loss: 0.039190623909235, Final Batch Loss: 0.009704701602458954\n",
      "Epoch 2959, Loss: 0.05210704356431961, Final Batch Loss: 0.03453394025564194\n",
      "Epoch 2960, Loss: 0.06259617581963539, Final Batch Loss: 0.03254735842347145\n",
      "Epoch 2961, Loss: 0.033381180837750435, Final Batch Loss: 0.018289489671587944\n",
      "Epoch 2962, Loss: 0.10542133077979088, Final Batch Loss: 0.0708157867193222\n",
      "Epoch 2963, Loss: 0.08290176279842854, Final Batch Loss: 0.06309940665960312\n",
      "Epoch 2964, Loss: 0.06229529716074467, Final Batch Loss: 0.05723990499973297\n",
      "Epoch 2965, Loss: 0.047086575999855995, Final Batch Loss: 0.016067110002040863\n",
      "Epoch 2966, Loss: 0.04850737098604441, Final Batch Loss: 0.012223572470247746\n",
      "Epoch 2967, Loss: 0.09699517861008644, Final Batch Loss: 0.055210575461387634\n",
      "Epoch 2968, Loss: 0.03359932545572519, Final Batch Loss: 0.012651600874960423\n",
      "Epoch 2969, Loss: 0.041922034695744514, Final Batch Loss: 0.018364280462265015\n",
      "Epoch 2970, Loss: 0.093472670763731, Final Batch Loss: 0.05044509470462799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2971, Loss: 0.036088306456804276, Final Batch Loss: 0.020304715260863304\n",
      "Epoch 2972, Loss: 0.03101212438195944, Final Batch Loss: 0.01177325937896967\n",
      "Epoch 2973, Loss: 0.03829262685030699, Final Batch Loss: 0.012982957996428013\n",
      "Epoch 2974, Loss: 0.03855843283236027, Final Batch Loss: 0.022136524319648743\n",
      "Epoch 2975, Loss: 0.02094241208396852, Final Batch Loss: 0.0017811579164117575\n",
      "Epoch 2976, Loss: 0.06178183760493994, Final Batch Loss: 0.0058464547619223595\n",
      "Epoch 2977, Loss: 0.027051697485148907, Final Batch Loss: 0.010101157240569592\n",
      "Epoch 2978, Loss: 0.06018386222422123, Final Batch Loss: 0.03138844296336174\n",
      "Epoch 2979, Loss: 0.02006144542247057, Final Batch Loss: 0.00661133136600256\n",
      "Epoch 2980, Loss: 0.07061026245355606, Final Batch Loss: 0.03921441361308098\n",
      "Epoch 2981, Loss: 0.0529603473842144, Final Batch Loss: 0.021845554932951927\n",
      "Epoch 2982, Loss: 0.06408364931121469, Final Batch Loss: 0.007398607674986124\n",
      "Epoch 2983, Loss: 0.051050134003162384, Final Batch Loss: 0.03587159514427185\n",
      "Epoch 2984, Loss: 0.022334645502269268, Final Batch Loss: 0.009045221842825413\n",
      "Epoch 2985, Loss: 0.02901422418653965, Final Batch Loss: 0.011011185124516487\n",
      "Epoch 2986, Loss: 0.03992278128862381, Final Batch Loss: 0.022389156743884087\n",
      "Epoch 2987, Loss: 0.05881736986339092, Final Batch Loss: 0.023247091099619865\n",
      "Epoch 2988, Loss: 0.08719847723841667, Final Batch Loss: 0.03191174566745758\n",
      "Epoch 2989, Loss: 0.09531951695680618, Final Batch Loss: 0.05841004475951195\n",
      "Epoch 2990, Loss: 0.02909549418836832, Final Batch Loss: 0.00889271218329668\n",
      "Epoch 2991, Loss: 0.05897847190499306, Final Batch Loss: 0.03961322084069252\n",
      "Epoch 2992, Loss: 0.020493214949965477, Final Batch Loss: 0.007879027165472507\n",
      "Epoch 2993, Loss: 0.033044698648154736, Final Batch Loss: 0.004840335808694363\n",
      "Epoch 2994, Loss: 0.03334792796522379, Final Batch Loss: 0.013199402950704098\n",
      "Epoch 2995, Loss: 0.03412405401468277, Final Batch Loss: 0.0192680973559618\n",
      "Epoch 2996, Loss: 0.05884070694446564, Final Batch Loss: 0.024997305124998093\n",
      "Epoch 2997, Loss: 0.016833516769111156, Final Batch Loss: 0.0070068202912807465\n",
      "Epoch 2998, Loss: 0.03727731015533209, Final Batch Loss: 0.01259543839842081\n",
      "Epoch 2999, Loss: 0.03332697949372232, Final Batch Loss: 0.0025556457694619894\n",
      "Epoch 3000, Loss: 0.03162303939461708, Final Batch Loss: 0.0036265719681978226\n",
      "Epoch 3001, Loss: 0.09341852553188801, Final Batch Loss: 0.08170703053474426\n",
      "Epoch 3002, Loss: 0.02724641840904951, Final Batch Loss: 0.009409422986209393\n",
      "Epoch 3003, Loss: 0.01975130196660757, Final Batch Loss: 0.01038817036896944\n",
      "Epoch 3004, Loss: 0.038077320670709014, Final Batch Loss: 0.003899350529536605\n",
      "Epoch 3005, Loss: 0.07030832394957542, Final Batch Loss: 0.05165492743253708\n",
      "Epoch 3006, Loss: 0.032467903569340706, Final Batch Loss: 0.016424313187599182\n",
      "Epoch 3007, Loss: 0.04705756902694702, Final Batch Loss: 0.02429172769188881\n",
      "Epoch 3008, Loss: 0.035616230219602585, Final Batch Loss: 0.0189419724047184\n",
      "Epoch 3009, Loss: 0.02844868041574955, Final Batch Loss: 0.0059064775705337524\n",
      "Epoch 3010, Loss: 0.084041528403759, Final Batch Loss: 0.06316373497247696\n",
      "Epoch 3011, Loss: 0.05010100454092026, Final Batch Loss: 0.030326347798109055\n",
      "Epoch 3012, Loss: 0.026634156703948975, Final Batch Loss: 0.008236270397901535\n",
      "Epoch 3013, Loss: 0.029243365861475468, Final Batch Loss: 0.013020395301282406\n",
      "Epoch 3014, Loss: 0.026476639322936535, Final Batch Loss: 0.01347165834158659\n",
      "Epoch 3015, Loss: 0.07506088539958, Final Batch Loss: 0.034435153007507324\n",
      "Epoch 3016, Loss: 0.03983015567064285, Final Batch Loss: 0.01840328611433506\n",
      "Epoch 3017, Loss: 0.016516666510142386, Final Batch Loss: 0.0015226021641865373\n",
      "Epoch 3018, Loss: 0.04858573153614998, Final Batch Loss: 0.02042200043797493\n",
      "Epoch 3019, Loss: 0.04609163664281368, Final Batch Loss: 0.018940318375825882\n",
      "Epoch 3020, Loss: 0.058430952951312065, Final Batch Loss: 0.008879756554961205\n",
      "Epoch 3021, Loss: 0.07614914420992136, Final Batch Loss: 0.010522768832743168\n",
      "Epoch 3022, Loss: 0.03685494791716337, Final Batch Loss: 0.009482336230576038\n",
      "Epoch 3023, Loss: 0.03317542187869549, Final Batch Loss: 0.019044967368245125\n",
      "Epoch 3024, Loss: 0.07467587571591139, Final Batch Loss: 0.00824193935841322\n",
      "Epoch 3025, Loss: 0.04027971439063549, Final Batch Loss: 0.00860007293522358\n",
      "Epoch 3026, Loss: 0.02326786145567894, Final Batch Loss: 0.01434837095439434\n",
      "Epoch 3027, Loss: 0.016672676661983132, Final Batch Loss: 0.0017633691895753145\n",
      "Epoch 3028, Loss: 0.023720702156424522, Final Batch Loss: 0.009683688171207905\n",
      "Epoch 3029, Loss: 0.034730644430965185, Final Batch Loss: 0.0038646492175757885\n",
      "Epoch 3030, Loss: 0.14158443734049797, Final Batch Loss: 0.12078902125358582\n",
      "Epoch 3031, Loss: 0.019119723234325647, Final Batch Loss: 0.00606621103361249\n",
      "Epoch 3032, Loss: 0.22372185625135899, Final Batch Loss: 0.20264877378940582\n",
      "Epoch 3033, Loss: 0.07603043504059315, Final Batch Loss: 0.05298691242933273\n",
      "Epoch 3034, Loss: 0.035043755546212196, Final Batch Loss: 0.0074091143906116486\n",
      "Epoch 3035, Loss: 0.03629268426448107, Final Batch Loss: 0.004210461862385273\n",
      "Epoch 3036, Loss: 0.023109718691557646, Final Batch Loss: 0.015712913125753403\n",
      "Epoch 3037, Loss: 0.050918715074658394, Final Batch Loss: 0.022258460521697998\n",
      "Epoch 3038, Loss: 0.07687634043395519, Final Batch Loss: 0.04701978340744972\n",
      "Epoch 3039, Loss: 0.027359481900930405, Final Batch Loss: 0.015510568395256996\n",
      "Epoch 3040, Loss: 0.0699367057532072, Final Batch Loss: 0.05142377316951752\n",
      "Epoch 3041, Loss: 0.019914772361516953, Final Batch Loss: 0.00420214980840683\n",
      "Epoch 3042, Loss: 0.042496658861637115, Final Batch Loss: 0.023416191339492798\n",
      "Epoch 3043, Loss: 0.029010347090661526, Final Batch Loss: 0.01716034859418869\n",
      "Epoch 3044, Loss: 0.06442335993051529, Final Batch Loss: 0.02027805522084236\n",
      "Epoch 3045, Loss: 0.044337511993944645, Final Batch Loss: 0.006227915175259113\n",
      "Epoch 3046, Loss: 0.07364196702837944, Final Batch Loss: 0.04562215134501457\n",
      "Epoch 3047, Loss: 0.03747646091505885, Final Batch Loss: 0.0056032235734164715\n",
      "Epoch 3048, Loss: 0.025725520681589842, Final Batch Loss: 0.006451002787798643\n",
      "Epoch 3049, Loss: 0.02773220930248499, Final Batch Loss: 0.005182546563446522\n",
      "Epoch 3050, Loss: 0.01797028910368681, Final Batch Loss: 0.005921863950788975\n",
      "Epoch 3051, Loss: 0.02078237885143608, Final Batch Loss: 0.0018562463810667396\n",
      "Epoch 3052, Loss: 0.034620569087564945, Final Batch Loss: 0.010126537643373013\n",
      "Epoch 3053, Loss: 0.11839062534272671, Final Batch Loss: 0.10150951147079468\n",
      "Epoch 3054, Loss: 0.0402704831212759, Final Batch Loss: 0.013664394617080688\n",
      "Epoch 3055, Loss: 0.02679147943854332, Final Batch Loss: 0.01274013426154852\n",
      "Epoch 3056, Loss: 0.07679172046482563, Final Batch Loss: 0.05326464772224426\n",
      "Epoch 3057, Loss: 0.06850097887217999, Final Batch Loss: 0.018657060340046883\n",
      "Epoch 3058, Loss: 0.020780527032911777, Final Batch Loss: 0.010393369011580944\n",
      "Epoch 3059, Loss: 0.032527729868888855, Final Batch Loss: 0.017714686691761017\n",
      "Epoch 3060, Loss: 0.02323023695498705, Final Batch Loss: 0.0067689074203372\n",
      "Epoch 3061, Loss: 0.02081279829144478, Final Batch Loss: 0.008416617289185524\n",
      "Epoch 3062, Loss: 0.12369637284427881, Final Batch Loss: 0.10979018360376358\n",
      "Epoch 3063, Loss: 0.09783052653074265, Final Batch Loss: 0.06493443250656128\n",
      "Epoch 3064, Loss: 0.06289193406701088, Final Batch Loss: 0.010197393596172333\n",
      "Epoch 3065, Loss: 0.06901360675692558, Final Batch Loss: 0.04816478490829468\n",
      "Epoch 3066, Loss: 0.10592183843255043, Final Batch Loss: 0.0905519500374794\n",
      "Epoch 3067, Loss: 0.0428460082039237, Final Batch Loss: 0.012886588461697102\n",
      "Epoch 3068, Loss: 0.0511672692373395, Final Batch Loss: 0.03902824595570564\n",
      "Epoch 3069, Loss: 0.10782432183623314, Final Batch Loss: 0.0803743302822113\n",
      "Epoch 3070, Loss: 0.09540423657745123, Final Batch Loss: 0.0830635130405426\n",
      "Epoch 3071, Loss: 0.03203143738210201, Final Batch Loss: 0.01825025863945484\n",
      "Epoch 3072, Loss: 0.1664799526333809, Final Batch Loss: 0.14526499807834625\n",
      "Epoch 3073, Loss: 0.02859331201761961, Final Batch Loss: 0.0035471124574542046\n",
      "Epoch 3074, Loss: 0.04913871921598911, Final Batch Loss: 0.022791322320699692\n",
      "Epoch 3075, Loss: 0.048870456404984, Final Batch Loss: 0.03842657431960106\n",
      "Epoch 3076, Loss: 0.02513440977782011, Final Batch Loss: 0.007279121316969395\n",
      "Epoch 3077, Loss: 0.04578339168801904, Final Batch Loss: 0.038268398493528366\n",
      "Epoch 3078, Loss: 0.08618273586034775, Final Batch Loss: 0.05016785115003586\n",
      "Epoch 3079, Loss: 0.043313320726156235, Final Batch Loss: 0.004154440015554428\n",
      "Epoch 3080, Loss: 0.011854195268824697, Final Batch Loss: 0.0029758319724351168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3081, Loss: 0.05490166135132313, Final Batch Loss: 0.032804735004901886\n",
      "Epoch 3082, Loss: 0.0352941257879138, Final Batch Loss: 0.009909247048199177\n",
      "Epoch 3083, Loss: 0.06526282802224159, Final Batch Loss: 0.04498511925339699\n",
      "Epoch 3084, Loss: 0.04451656248420477, Final Batch Loss: 0.033103570342063904\n",
      "Epoch 3085, Loss: 0.08780166879296303, Final Batch Loss: 0.07166913896799088\n",
      "Epoch 3086, Loss: 0.08099185675382614, Final Batch Loss: 0.06894317269325256\n",
      "Epoch 3087, Loss: 0.056788588874042034, Final Batch Loss: 0.042502351105213165\n",
      "Epoch 3088, Loss: 0.09016822837293148, Final Batch Loss: 0.07401380687952042\n",
      "Epoch 3089, Loss: 0.04636036604642868, Final Batch Loss: 0.028303563594818115\n",
      "Epoch 3090, Loss: 0.06334299221634865, Final Batch Loss: 0.03315621614456177\n",
      "Epoch 3091, Loss: 0.0669516995549202, Final Batch Loss: 0.05256460979580879\n",
      "Epoch 3092, Loss: 0.03769204393029213, Final Batch Loss: 0.01126842200756073\n",
      "Epoch 3093, Loss: 0.03427380230277777, Final Batch Loss: 0.0071541620418429375\n",
      "Epoch 3094, Loss: 0.026120193302631378, Final Batch Loss: 0.011937282048165798\n",
      "Epoch 3095, Loss: 0.04868457838892937, Final Batch Loss: 0.027925193309783936\n",
      "Epoch 3096, Loss: 0.08721918426454067, Final Batch Loss: 0.05669500306248665\n",
      "Epoch 3097, Loss: 0.085444126278162, Final Batch Loss: 0.06826458126306534\n",
      "Epoch 3098, Loss: 0.04122414067387581, Final Batch Loss: 0.02504010684788227\n",
      "Epoch 3099, Loss: 0.018461979925632477, Final Batch Loss: 0.01044235285371542\n",
      "Epoch 3100, Loss: 0.032195711974054575, Final Batch Loss: 0.004603453446179628\n",
      "Epoch 3101, Loss: 0.027948904782533646, Final Batch Loss: 0.009563736617565155\n",
      "Epoch 3102, Loss: 0.015293408185243607, Final Batch Loss: 0.004279496148228645\n",
      "Epoch 3103, Loss: 0.022910705767571926, Final Batch Loss: 0.007862922735512257\n",
      "Epoch 3104, Loss: 0.03456338681280613, Final Batch Loss: 0.008202513679862022\n",
      "Epoch 3105, Loss: 0.020038479939103127, Final Batch Loss: 0.012110384181141853\n",
      "Epoch 3106, Loss: 0.02399844489991665, Final Batch Loss: 0.010653523728251457\n",
      "Epoch 3107, Loss: 0.023533098865300417, Final Batch Loss: 0.004074836615473032\n",
      "Epoch 3108, Loss: 0.022117230109870434, Final Batch Loss: 0.01106230728328228\n",
      "Epoch 3109, Loss: 0.06558142602443695, Final Batch Loss: 0.04018581658601761\n",
      "Epoch 3110, Loss: 0.06119859032332897, Final Batch Loss: 0.028556695207953453\n",
      "Epoch 3111, Loss: 0.09768261015415192, Final Batch Loss: 0.01462554931640625\n",
      "Epoch 3112, Loss: 0.10402327962219715, Final Batch Loss: 0.07731404155492783\n",
      "Epoch 3113, Loss: 0.06838250905275345, Final Batch Loss: 0.01412704586982727\n",
      "Epoch 3114, Loss: 0.052488019689917564, Final Batch Loss: 0.027424922212958336\n",
      "Epoch 3115, Loss: 0.04943309724330902, Final Batch Loss: 0.03819830343127251\n",
      "Epoch 3116, Loss: 0.035424312110990286, Final Batch Loss: 0.004335926380008459\n",
      "Epoch 3117, Loss: 0.02649299893528223, Final Batch Loss: 0.008568587712943554\n",
      "Epoch 3118, Loss: 0.0554269440472126, Final Batch Loss: 0.028265133500099182\n",
      "Epoch 3119, Loss: 0.041562121361494064, Final Batch Loss: 0.011924652382731438\n",
      "Epoch 3120, Loss: 0.013343007303774357, Final Batch Loss: 0.0071317474357783794\n",
      "Epoch 3121, Loss: 0.04603789793327451, Final Batch Loss: 0.00729454355314374\n",
      "Epoch 3122, Loss: 0.03492134250700474, Final Batch Loss: 0.022742435336112976\n",
      "Epoch 3123, Loss: 0.06173685006797314, Final Batch Loss: 0.03895833343267441\n",
      "Epoch 3124, Loss: 0.10485554859042168, Final Batch Loss: 0.0888470858335495\n",
      "Epoch 3125, Loss: 0.037036540918052197, Final Batch Loss: 0.027629336342215538\n",
      "Epoch 3126, Loss: 0.04022279381752014, Final Batch Loss: 0.010713666677474976\n",
      "Epoch 3127, Loss: 0.026761971414089203, Final Batch Loss: 0.011386255733668804\n",
      "Epoch 3128, Loss: 0.06596316583454609, Final Batch Loss: 0.008200382813811302\n",
      "Epoch 3129, Loss: 0.10278692841529846, Final Batch Loss: 0.058202750980854034\n",
      "Epoch 3130, Loss: 0.06342915119603276, Final Batch Loss: 0.05585545673966408\n",
      "Epoch 3131, Loss: 0.052296651527285576, Final Batch Loss: 0.019127963110804558\n",
      "Epoch 3132, Loss: 0.060616638511419296, Final Batch Loss: 0.04503779485821724\n",
      "Epoch 3133, Loss: 0.05127444677054882, Final Batch Loss: 0.010323228314518929\n",
      "Epoch 3134, Loss: 0.05157617665827274, Final Batch Loss: 0.025084199383854866\n",
      "Epoch 3135, Loss: 0.05179824121296406, Final Batch Loss: 0.03478100523352623\n",
      "Epoch 3136, Loss: 0.03126757964491844, Final Batch Loss: 0.011940939351916313\n",
      "Epoch 3137, Loss: 0.032143023796379566, Final Batch Loss: 0.01951054483652115\n",
      "Epoch 3138, Loss: 0.032633242197334766, Final Batch Loss: 0.012691686861217022\n",
      "Epoch 3139, Loss: 0.052997078746557236, Final Batch Loss: 0.025429420173168182\n",
      "Epoch 3140, Loss: 0.03559081628918648, Final Batch Loss: 0.018963027745485306\n",
      "Epoch 3141, Loss: 0.03195548476651311, Final Batch Loss: 0.0056906105019152164\n",
      "Epoch 3142, Loss: 0.042259001173079014, Final Batch Loss: 0.009606209583580494\n",
      "Epoch 3143, Loss: 0.05293356999754906, Final Batch Loss: 0.02705664373934269\n",
      "Epoch 3144, Loss: 0.03595262486487627, Final Batch Loss: 0.010173979215323925\n",
      "Epoch 3145, Loss: 0.04982558824121952, Final Batch Loss: 0.020438728854060173\n",
      "Epoch 3146, Loss: 0.013502507470548153, Final Batch Loss: 0.004255123436450958\n",
      "Epoch 3147, Loss: 0.03923055715858936, Final Batch Loss: 0.015520105138421059\n",
      "Epoch 3148, Loss: 0.025662187486886978, Final Batch Loss: 0.015083902515470982\n",
      "Epoch 3149, Loss: 0.040437038987874985, Final Batch Loss: 0.020960334688425064\n",
      "Epoch 3150, Loss: 0.049702245742082596, Final Batch Loss: 0.027867592871189117\n",
      "Epoch 3151, Loss: 0.018748546484857798, Final Batch Loss: 0.00777210621163249\n",
      "Epoch 3152, Loss: 0.01820043195039034, Final Batch Loss: 0.009559324942529202\n",
      "Epoch 3153, Loss: 0.023362584877759218, Final Batch Loss: 0.006621705833822489\n",
      "Epoch 3154, Loss: 0.021179082337766886, Final Batch Loss: 0.003539408091455698\n",
      "Epoch 3155, Loss: 0.02877480536699295, Final Batch Loss: 0.01443889457732439\n",
      "Epoch 3156, Loss: 0.021505117416381836, Final Batch Loss: 0.017694322392344475\n",
      "Epoch 3157, Loss: 0.04657560610212386, Final Batch Loss: 0.002563931280747056\n",
      "Epoch 3158, Loss: 0.028614363633096218, Final Batch Loss: 0.008228727616369724\n",
      "Epoch 3159, Loss: 0.12933978997170925, Final Batch Loss: 0.11672978848218918\n",
      "Epoch 3160, Loss: 0.05365824792534113, Final Batch Loss: 0.01339759025722742\n",
      "Epoch 3161, Loss: 0.04242444410920143, Final Batch Loss: 0.030639437958598137\n",
      "Epoch 3162, Loss: 0.015257770661264658, Final Batch Loss: 0.005200389306992292\n",
      "Epoch 3163, Loss: 0.025555018335580826, Final Batch Loss: 0.005617467686533928\n",
      "Epoch 3164, Loss: 0.09972134791314602, Final Batch Loss: 0.030226727947592735\n",
      "Epoch 3165, Loss: 0.03595481347292662, Final Batch Loss: 0.013707819394767284\n",
      "Epoch 3166, Loss: 0.040970523841679096, Final Batch Loss: 0.029077373445034027\n",
      "Epoch 3167, Loss: 0.023724994622170925, Final Batch Loss: 0.005966736935079098\n",
      "Epoch 3168, Loss: 0.06586131453514099, Final Batch Loss: 0.010730963200330734\n",
      "Epoch 3169, Loss: 0.031020442955195904, Final Batch Loss: 0.019148776307702065\n",
      "Epoch 3170, Loss: 0.028601084370166063, Final Batch Loss: 0.005412357393652201\n",
      "Epoch 3171, Loss: 0.08028096985071898, Final Batch Loss: 0.009617949835956097\n",
      "Epoch 3172, Loss: 0.02428797958418727, Final Batch Loss: 0.004748887848109007\n",
      "Epoch 3173, Loss: 0.04293704219162464, Final Batch Loss: 0.012270275503396988\n",
      "Epoch 3174, Loss: 0.02491853293031454, Final Batch Loss: 0.010997200384736061\n",
      "Epoch 3175, Loss: 0.03356814105063677, Final Batch Loss: 0.0081216124817729\n",
      "Epoch 3176, Loss: 0.05248368717730045, Final Batch Loss: 0.026813475415110588\n",
      "Epoch 3177, Loss: 0.027678423561155796, Final Batch Loss: 0.01787503808736801\n",
      "Epoch 3178, Loss: 0.02528375666588545, Final Batch Loss: 0.015357756987214088\n",
      "Epoch 3179, Loss: 0.023415856063365936, Final Batch Loss: 0.01588256284594536\n",
      "Epoch 3180, Loss: 0.020502811297774315, Final Batch Loss: 0.006093915551900864\n",
      "Epoch 3181, Loss: 0.026468664407730103, Final Batch Loss: 0.019639072939753532\n",
      "Epoch 3182, Loss: 0.037313437089324, Final Batch Loss: 0.021229911595582962\n",
      "Epoch 3183, Loss: 0.0710005764849484, Final Batch Loss: 0.0040066917426884174\n",
      "Epoch 3184, Loss: 0.045039172284305096, Final Batch Loss: 0.012709236703813076\n",
      "Epoch 3185, Loss: 0.04991711117327213, Final Batch Loss: 0.025717318058013916\n",
      "Epoch 3186, Loss: 0.05313827097415924, Final Batch Loss: 0.03102133795619011\n",
      "Epoch 3187, Loss: 0.049120242707431316, Final Batch Loss: 0.011725104413926601\n",
      "Epoch 3188, Loss: 0.029007823206484318, Final Batch Loss: 0.018120864406228065\n",
      "Epoch 3189, Loss: 0.027340803761035204, Final Batch Loss: 0.00628850469365716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3190, Loss: 0.03763972595334053, Final Batch Loss: 0.028606606647372246\n",
      "Epoch 3191, Loss: 0.0606099721044302, Final Batch Loss: 0.014221137389540672\n",
      "Epoch 3192, Loss: 0.03392744343727827, Final Batch Loss: 0.022354545071721077\n",
      "Epoch 3193, Loss: 0.03054287936538458, Final Batch Loss: 0.012010644190013409\n",
      "Epoch 3194, Loss: 0.049359331373125315, Final Batch Loss: 0.005522964987903833\n",
      "Epoch 3195, Loss: 0.10051316022872925, Final Batch Loss: 0.07064235955476761\n",
      "Epoch 3196, Loss: 0.04615691024810076, Final Batch Loss: 0.03190340846776962\n",
      "Epoch 3197, Loss: 0.02766949823126197, Final Batch Loss: 0.006973741110414267\n",
      "Epoch 3198, Loss: 0.02627254230901599, Final Batch Loss: 0.0028795558027923107\n",
      "Epoch 3199, Loss: 0.021444321842864156, Final Batch Loss: 0.0031914145220071077\n",
      "Epoch 3200, Loss: 0.07366047240793705, Final Batch Loss: 0.0166239682585001\n",
      "Epoch 3201, Loss: 0.036785936914384365, Final Batch Loss: 0.015174041502177715\n",
      "Epoch 3202, Loss: 0.013219232205301523, Final Batch Loss: 0.006101161707192659\n",
      "Epoch 3203, Loss: 0.02113138511776924, Final Batch Loss: 0.004522835835814476\n",
      "Epoch 3204, Loss: 0.03850825410336256, Final Batch Loss: 0.014304622076451778\n",
      "Epoch 3205, Loss: 0.030123640783131123, Final Batch Loss: 0.01051251869648695\n",
      "Epoch 3206, Loss: 0.013100170996040106, Final Batch Loss: 0.005063875112682581\n",
      "Epoch 3207, Loss: 0.04305472411215305, Final Batch Loss: 0.012035654857754707\n",
      "Epoch 3208, Loss: 0.02281145751476288, Final Batch Loss: 0.01582375168800354\n",
      "Epoch 3209, Loss: 0.018648781813681126, Final Batch Loss: 0.004277248866856098\n",
      "Epoch 3210, Loss: 0.03222946170717478, Final Batch Loss: 0.003519953228533268\n",
      "Epoch 3211, Loss: 0.03496270999312401, Final Batch Loss: 0.024023115634918213\n",
      "Epoch 3212, Loss: 0.10586687922477722, Final Batch Loss: 0.021543562412261963\n",
      "Epoch 3213, Loss: 0.037989589385688305, Final Batch Loss: 0.004867355339229107\n",
      "Epoch 3214, Loss: 0.0834138710051775, Final Batch Loss: 0.06312672048807144\n",
      "Epoch 3215, Loss: 0.05532433744519949, Final Batch Loss: 0.04992958530783653\n",
      "Epoch 3216, Loss: 0.04389621037989855, Final Batch Loss: 0.01256499532610178\n",
      "Epoch 3217, Loss: 0.032205358147621155, Final Batch Loss: 0.019663799554109573\n",
      "Epoch 3218, Loss: 0.02041582716628909, Final Batch Loss: 0.004610891919583082\n",
      "Epoch 3219, Loss: 0.054711099714040756, Final Batch Loss: 0.020322825759649277\n",
      "Epoch 3220, Loss: 0.02045484073460102, Final Batch Loss: 0.007886648178100586\n",
      "Epoch 3221, Loss: 0.010221711359918118, Final Batch Loss: 0.003972188103944063\n",
      "Epoch 3222, Loss: 0.06531326100230217, Final Batch Loss: 0.052442099899053574\n",
      "Epoch 3223, Loss: 0.016787965781986713, Final Batch Loss: 0.008564494550228119\n",
      "Epoch 3224, Loss: 0.07831627316772938, Final Batch Loss: 0.029352368786931038\n",
      "Epoch 3225, Loss: 0.022782268468290567, Final Batch Loss: 0.007728771772235632\n",
      "Epoch 3226, Loss: 0.013953003566712141, Final Batch Loss: 0.006118892226368189\n",
      "Epoch 3227, Loss: 0.13353750109672546, Final Batch Loss: 0.07454856485128403\n",
      "Epoch 3228, Loss: 0.01983516849577427, Final Batch Loss: 0.005083343014121056\n",
      "Epoch 3229, Loss: 0.026773905381560326, Final Batch Loss: 0.009040743112564087\n",
      "Epoch 3230, Loss: 0.017910556867718697, Final Batch Loss: 0.009210527874529362\n",
      "Epoch 3231, Loss: 0.09896975290030241, Final Batch Loss: 0.08360414952039719\n",
      "Epoch 3232, Loss: 0.03503304719924927, Final Batch Loss: 0.006033994257450104\n",
      "Epoch 3233, Loss: 0.053489331156015396, Final Batch Loss: 0.0252826064825058\n",
      "Epoch 3234, Loss: 0.03085658699274063, Final Batch Loss: 0.020622551441192627\n",
      "Epoch 3235, Loss: 0.0271321595646441, Final Batch Loss: 0.006058237049728632\n",
      "Epoch 3236, Loss: 0.03637393470853567, Final Batch Loss: 0.006101931445300579\n",
      "Epoch 3237, Loss: 0.050831595435738564, Final Batch Loss: 0.032706763595342636\n",
      "Epoch 3238, Loss: 0.0291977496817708, Final Batch Loss: 0.014103108085691929\n",
      "Epoch 3239, Loss: 0.02935777883976698, Final Batch Loss: 0.017767950892448425\n",
      "Epoch 3240, Loss: 0.03388943336904049, Final Batch Loss: 0.004955988377332687\n",
      "Epoch 3241, Loss: 0.03869999758899212, Final Batch Loss: 0.019170012325048447\n",
      "Epoch 3242, Loss: 0.034268084447830915, Final Batch Loss: 0.026558127254247665\n",
      "Epoch 3243, Loss: 0.0251177535392344, Final Batch Loss: 0.006842537317425013\n",
      "Epoch 3244, Loss: 0.029741399455815554, Final Batch Loss: 0.0060860333032906055\n",
      "Epoch 3245, Loss: 0.024064273573458195, Final Batch Loss: 0.010016338899731636\n",
      "Epoch 3246, Loss: 0.05370437167584896, Final Batch Loss: 0.029821205884218216\n",
      "Epoch 3247, Loss: 0.02830408327281475, Final Batch Loss: 0.017669042572379112\n",
      "Epoch 3248, Loss: 0.07994028553366661, Final Batch Loss: 0.044806379824876785\n",
      "Epoch 3249, Loss: 0.02332546841353178, Final Batch Loss: 0.009265156462788582\n",
      "Epoch 3250, Loss: 0.03910568356513977, Final Batch Loss: 0.020792793482542038\n",
      "Epoch 3251, Loss: 0.07066127471625805, Final Batch Loss: 0.05027584359049797\n",
      "Epoch 3252, Loss: 0.02142007602378726, Final Batch Loss: 0.014282534830272198\n",
      "Epoch 3253, Loss: 0.025650271214544773, Final Batch Loss: 0.01771734096109867\n",
      "Epoch 3254, Loss: 0.12437557801604271, Final Batch Loss: 0.06825520098209381\n",
      "Epoch 3255, Loss: 0.048835279420018196, Final Batch Loss: 0.028580520302057266\n",
      "Epoch 3256, Loss: 0.02136276289820671, Final Batch Loss: 0.009938272647559643\n",
      "Epoch 3257, Loss: 0.030391265638172626, Final Batch Loss: 0.015275430865585804\n",
      "Epoch 3258, Loss: 0.020231831818819046, Final Batch Loss: 0.009907480329275131\n",
      "Epoch 3259, Loss: 0.06700700521469116, Final Batch Loss: 0.03426814079284668\n",
      "Epoch 3260, Loss: 0.04402669984847307, Final Batch Loss: 0.012670782394707203\n",
      "Epoch 3261, Loss: 0.05813237465918064, Final Batch Loss: 0.01999318040907383\n",
      "Epoch 3262, Loss: 0.037359971553087234, Final Batch Loss: 0.0238207820802927\n",
      "Epoch 3263, Loss: 0.01814670953899622, Final Batch Loss: 0.006797597743570805\n",
      "Epoch 3264, Loss: 0.01727188006043434, Final Batch Loss: 0.005962330847978592\n",
      "Epoch 3265, Loss: 0.026128221303224564, Final Batch Loss: 0.005448102951049805\n",
      "Epoch 3266, Loss: 0.03290562145411968, Final Batch Loss: 0.006335755810141563\n",
      "Epoch 3267, Loss: 0.056326692923903465, Final Batch Loss: 0.04905157536268234\n",
      "Epoch 3268, Loss: 0.03732629958540201, Final Batch Loss: 0.008553524501621723\n",
      "Epoch 3269, Loss: 0.03486207709647715, Final Batch Loss: 0.003483297536149621\n",
      "Epoch 3270, Loss: 0.02456633932888508, Final Batch Loss: 0.004526035860180855\n",
      "Epoch 3271, Loss: 0.041523272171616554, Final Batch Loss: 0.01675819419324398\n",
      "Epoch 3272, Loss: 0.03298326442018151, Final Batch Loss: 0.02623746544122696\n",
      "Epoch 3273, Loss: 0.01680689468048513, Final Batch Loss: 0.003653497202321887\n",
      "Epoch 3274, Loss: 0.06285255029797554, Final Batch Loss: 0.042587850242853165\n",
      "Epoch 3275, Loss: 0.018679069355130196, Final Batch Loss: 0.007352045737206936\n",
      "Epoch 3276, Loss: 0.04073079861700535, Final Batch Loss: 0.023043936118483543\n",
      "Epoch 3277, Loss: 0.02884138748049736, Final Batch Loss: 0.005930032581090927\n",
      "Epoch 3278, Loss: 0.03428405895829201, Final Batch Loss: 0.01601659692823887\n",
      "Epoch 3279, Loss: 0.032404157333076, Final Batch Loss: 0.006668129004538059\n",
      "Epoch 3280, Loss: 0.03101025754585862, Final Batch Loss: 0.0064835925586521626\n",
      "Epoch 3281, Loss: 0.08352094842121005, Final Batch Loss: 0.005274518858641386\n",
      "Epoch 3282, Loss: 0.02710098121315241, Final Batch Loss: 0.01721995137631893\n",
      "Epoch 3283, Loss: 0.03743117768317461, Final Batch Loss: 0.010497043840587139\n",
      "Epoch 3284, Loss: 0.03333058301359415, Final Batch Loss: 0.00794393103569746\n",
      "Epoch 3285, Loss: 0.03991374955512583, Final Batch Loss: 0.0024558885488659143\n",
      "Epoch 3286, Loss: 0.0701168142259121, Final Batch Loss: 0.02866995707154274\n",
      "Epoch 3287, Loss: 0.01257550553418696, Final Batch Loss: 0.0017262476030737162\n",
      "Epoch 3288, Loss: 0.18010966759175062, Final Batch Loss: 0.16793768107891083\n",
      "Epoch 3289, Loss: 0.02905146637931466, Final Batch Loss: 0.007002749014645815\n",
      "Epoch 3290, Loss: 0.01960689015686512, Final Batch Loss: 0.005057408474385738\n",
      "Epoch 3291, Loss: 0.08454069681465626, Final Batch Loss: 0.06852709501981735\n",
      "Epoch 3292, Loss: 0.02294733840972185, Final Batch Loss: 0.006307964213192463\n",
      "Epoch 3293, Loss: 0.06003055162727833, Final Batch Loss: 0.029689185321331024\n",
      "Epoch 3294, Loss: 0.02051836543250829, Final Batch Loss: 0.0015236801700666547\n",
      "Epoch 3295, Loss: 0.07236611843109131, Final Batch Loss: 0.02742663025856018\n",
      "Epoch 3296, Loss: 0.036099473014473915, Final Batch Loss: 0.022310661152005196\n",
      "Epoch 3297, Loss: 0.021422812715172768, Final Batch Loss: 0.011079814285039902\n",
      "Epoch 3298, Loss: 0.017424683086574078, Final Batch Loss: 0.008887759409844875\n",
      "Epoch 3299, Loss: 0.019239195622503757, Final Batch Loss: 0.004021237604320049\n",
      "Epoch 3300, Loss: 0.014481781981885433, Final Batch Loss: 0.0064080217853188515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3301, Loss: 0.03579002618789673, Final Batch Loss: 0.01739743910729885\n",
      "Epoch 3302, Loss: 0.027225649682804942, Final Batch Loss: 0.0016476612072438002\n",
      "Epoch 3303, Loss: 0.01657929178327322, Final Batch Loss: 0.0030014915391802788\n",
      "Epoch 3304, Loss: 0.0332190222106874, Final Batch Loss: 0.0029823961667716503\n",
      "Epoch 3305, Loss: 0.027272077277302742, Final Batch Loss: 0.011694693937897682\n",
      "Epoch 3306, Loss: 0.05111994408071041, Final Batch Loss: 0.019388554617762566\n",
      "Epoch 3307, Loss: 0.0921572633087635, Final Batch Loss: 0.05660286545753479\n",
      "Epoch 3308, Loss: 0.028925188817083836, Final Batch Loss: 0.00890654232352972\n",
      "Epoch 3309, Loss: 0.11541783344000578, Final Batch Loss: 0.10111219435930252\n",
      "Epoch 3310, Loss: 0.019969849847257137, Final Batch Loss: 0.005603511817753315\n",
      "Epoch 3311, Loss: 0.09281369671225548, Final Batch Loss: 0.05943500995635986\n",
      "Epoch 3312, Loss: 0.03295592009089887, Final Batch Loss: 0.0036611666437238455\n",
      "Epoch 3313, Loss: 0.06430239975452423, Final Batch Loss: 0.017342545092105865\n",
      "Epoch 3314, Loss: 0.01732116239145398, Final Batch Loss: 0.0030247061513364315\n",
      "Epoch 3315, Loss: 0.04891353286802769, Final Batch Loss: 0.022818705067038536\n",
      "Epoch 3316, Loss: 0.0741659663617611, Final Batch Loss: 0.03356495872139931\n",
      "Epoch 3317, Loss: 0.030813736841082573, Final Batch Loss: 0.006308309733867645\n",
      "Epoch 3318, Loss: 0.016100347507745028, Final Batch Loss: 0.003989655990153551\n",
      "Epoch 3319, Loss: 0.01699401717633009, Final Batch Loss: 0.005886010825634003\n",
      "Epoch 3320, Loss: 0.029463188722729683, Final Batch Loss: 0.012759098783135414\n",
      "Epoch 3321, Loss: 0.034018940292298794, Final Batch Loss: 0.006940540857613087\n",
      "Epoch 3322, Loss: 0.03074816521257162, Final Batch Loss: 0.009548558853566647\n",
      "Epoch 3323, Loss: 0.016789653338491917, Final Batch Loss: 0.0076303184032440186\n",
      "Epoch 3324, Loss: 0.035067141987383366, Final Batch Loss: 0.010059614665806293\n",
      "Epoch 3325, Loss: 0.030214525293558836, Final Batch Loss: 0.002531766425818205\n",
      "Epoch 3326, Loss: 0.023304732516407967, Final Batch Loss: 0.00864675547927618\n",
      "Epoch 3327, Loss: 0.051192919723689556, Final Batch Loss: 0.037862081080675125\n",
      "Epoch 3328, Loss: 0.024913989007472992, Final Batch Loss: 0.010161228477954865\n",
      "Epoch 3329, Loss: 0.030576087534427643, Final Batch Loss: 0.011965721845626831\n",
      "Epoch 3330, Loss: 0.02622680552303791, Final Batch Loss: 0.017652837559580803\n",
      "Epoch 3331, Loss: 0.038541967049241066, Final Batch Loss: 0.02069878950715065\n",
      "Epoch 3332, Loss: 0.016297357622534037, Final Batch Loss: 0.006183280143886805\n",
      "Epoch 3333, Loss: 0.05344620905816555, Final Batch Loss: 0.02902943640947342\n",
      "Epoch 3334, Loss: 0.056035229936242104, Final Batch Loss: 0.03487112373113632\n",
      "Epoch 3335, Loss: 0.2000031303614378, Final Batch Loss: 0.17973555624485016\n",
      "Epoch 3336, Loss: 0.06575593538582325, Final Batch Loss: 0.04394090175628662\n",
      "Epoch 3337, Loss: 0.05210999492555857, Final Batch Loss: 0.04631160572171211\n",
      "Epoch 3338, Loss: 0.03410473559051752, Final Batch Loss: 0.014859744347631931\n",
      "Epoch 3339, Loss: 0.018190851900726557, Final Batch Loss: 0.007353190798312426\n",
      "Epoch 3340, Loss: 0.03218706604093313, Final Batch Loss: 0.008428257890045643\n",
      "Epoch 3341, Loss: 0.01912506390362978, Final Batch Loss: 0.010335181839764118\n",
      "Epoch 3342, Loss: 0.042263662442564964, Final Batch Loss: 0.012146729975938797\n",
      "Epoch 3343, Loss: 0.047560819424688816, Final Batch Loss: 0.033991020172834396\n",
      "Epoch 3344, Loss: 0.047551000490784645, Final Batch Loss: 0.013677844777703285\n",
      "Epoch 3345, Loss: 0.03995620319619775, Final Batch Loss: 0.032185811549425125\n",
      "Epoch 3346, Loss: 0.038897257298231125, Final Batch Loss: 0.017299583181738853\n",
      "Epoch 3347, Loss: 0.031817423179745674, Final Batch Loss: 0.00838611088693142\n",
      "Epoch 3348, Loss: 0.045321984216570854, Final Batch Loss: 0.005338842049241066\n",
      "Epoch 3349, Loss: 0.08301487285643816, Final Batch Loss: 0.07362041622400284\n",
      "Epoch 3350, Loss: 0.028907814295962453, Final Batch Loss: 0.0033813316840678453\n",
      "Epoch 3351, Loss: 0.04488568939268589, Final Batch Loss: 0.02859119139611721\n",
      "Epoch 3352, Loss: 0.03268295992165804, Final Batch Loss: 0.017254427075386047\n",
      "Epoch 3353, Loss: 0.024540401063859463, Final Batch Loss: 0.012589006684720516\n",
      "Epoch 3354, Loss: 0.03387986961752176, Final Batch Loss: 0.014514853246510029\n",
      "Epoch 3355, Loss: 0.0783867109566927, Final Batch Loss: 0.060412026941776276\n",
      "Epoch 3356, Loss: 0.027354884427040815, Final Batch Loss: 0.007141598965972662\n",
      "Epoch 3357, Loss: 0.026857154443860054, Final Batch Loss: 0.015357762575149536\n",
      "Epoch 3358, Loss: 0.07020602561533451, Final Batch Loss: 0.029265524819493294\n",
      "Epoch 3359, Loss: 0.03608014527708292, Final Batch Loss: 0.02175109274685383\n",
      "Epoch 3360, Loss: 0.036740025505423546, Final Batch Loss: 0.026909343898296356\n",
      "Epoch 3361, Loss: 0.0258853891864419, Final Batch Loss: 0.017072735354304314\n",
      "Epoch 3362, Loss: 0.03192027471959591, Final Batch Loss: 0.02013983204960823\n",
      "Epoch 3363, Loss: 0.040480000898242, Final Batch Loss: 0.021889105439186096\n",
      "Epoch 3364, Loss: 0.061258092522621155, Final Batch Loss: 0.03410045802593231\n",
      "Epoch 3365, Loss: 0.044773018918931484, Final Batch Loss: 0.011902979575097561\n",
      "Epoch 3366, Loss: 0.08519159257411957, Final Batch Loss: 0.07095002382993698\n",
      "Epoch 3367, Loss: 0.021025695838034153, Final Batch Loss: 0.010830183513462543\n",
      "Epoch 3368, Loss: 0.05547565780580044, Final Batch Loss: 0.02331341616809368\n",
      "Epoch 3369, Loss: 0.08497720025479794, Final Batch Loss: 0.058867212384939194\n",
      "Epoch 3370, Loss: 0.032189172226935625, Final Batch Loss: 0.006080134306102991\n",
      "Epoch 3371, Loss: 0.011261196807026863, Final Batch Loss: 0.0038632815703749657\n",
      "Epoch 3372, Loss: 0.06842414196580648, Final Batch Loss: 0.05713639408349991\n",
      "Epoch 3373, Loss: 0.03199745574966073, Final Batch Loss: 0.006977757904678583\n",
      "Epoch 3374, Loss: 0.03960579354315996, Final Batch Loss: 0.014312279410660267\n",
      "Epoch 3375, Loss: 0.04067494859918952, Final Batch Loss: 0.007672873791307211\n",
      "Epoch 3376, Loss: 0.033217876218259335, Final Batch Loss: 0.015204665251076221\n",
      "Epoch 3377, Loss: 0.02660613087937236, Final Batch Loss: 0.021112356334924698\n",
      "Epoch 3378, Loss: 0.06335256714373827, Final Batch Loss: 0.04994826391339302\n",
      "Epoch 3379, Loss: 0.02171475626528263, Final Batch Loss: 0.010623577050864697\n",
      "Epoch 3380, Loss: 0.058922652155160904, Final Batch Loss: 0.010721400380134583\n",
      "Epoch 3381, Loss: 0.021731087006628513, Final Batch Loss: 0.011312466114759445\n",
      "Epoch 3382, Loss: 0.02342890202999115, Final Batch Loss: 0.005168965086340904\n",
      "Epoch 3383, Loss: 0.05036150850355625, Final Batch Loss: 0.027143795043230057\n",
      "Epoch 3384, Loss: 0.02511382568627596, Final Batch Loss: 0.010387592948973179\n",
      "Epoch 3385, Loss: 0.14015072770416737, Final Batch Loss: 0.11589722335338593\n",
      "Epoch 3386, Loss: 0.024383188225328922, Final Batch Loss: 0.013490099459886551\n",
      "Epoch 3387, Loss: 0.07347147446125746, Final Batch Loss: 0.05819433182477951\n",
      "Epoch 3388, Loss: 0.03013346018269658, Final Batch Loss: 0.006859913934022188\n",
      "Epoch 3389, Loss: 0.03126414492726326, Final Batch Loss: 0.011899761855602264\n",
      "Epoch 3390, Loss: 0.021447278559207916, Final Batch Loss: 0.007185602560639381\n",
      "Epoch 3391, Loss: 0.021180998533964157, Final Batch Loss: 0.01176245417445898\n",
      "Epoch 3392, Loss: 0.049908327870070934, Final Batch Loss: 0.011350705288350582\n",
      "Epoch 3393, Loss: 0.03054490126669407, Final Batch Loss: 0.018263300880789757\n",
      "Epoch 3394, Loss: 0.040478235110640526, Final Batch Loss: 0.017796233296394348\n",
      "Epoch 3395, Loss: 0.02350594662129879, Final Batch Loss: 0.007209045812487602\n",
      "Epoch 3396, Loss: 0.021589661948382854, Final Batch Loss: 0.004238198511302471\n",
      "Epoch 3397, Loss: 0.024258698569610715, Final Batch Loss: 0.0033540755975991488\n",
      "Epoch 3398, Loss: 0.10021656658500433, Final Batch Loss: 0.09114596992731094\n",
      "Epoch 3399, Loss: 0.03956868499517441, Final Batch Loss: 0.015065513551235199\n",
      "Epoch 3400, Loss: 0.08317024260759354, Final Batch Loss: 0.0643572136759758\n",
      "Epoch 3401, Loss: 0.02646377170458436, Final Batch Loss: 0.006122362334281206\n",
      "Epoch 3402, Loss: 0.027076272293925285, Final Batch Loss: 0.01625880040228367\n",
      "Epoch 3403, Loss: 0.015304852975532413, Final Batch Loss: 0.0033424084540456533\n",
      "Epoch 3404, Loss: 0.02645460981875658, Final Batch Loss: 0.01591513492166996\n",
      "Epoch 3405, Loss: 0.05521307699382305, Final Batch Loss: 0.02521183155477047\n",
      "Epoch 3406, Loss: 0.04304720275104046, Final Batch Loss: 0.02748795971274376\n",
      "Epoch 3407, Loss: 0.0456521101295948, Final Batch Loss: 0.018976904451847076\n",
      "Epoch 3408, Loss: 0.053416505455970764, Final Batch Loss: 0.024758916348218918\n",
      "Epoch 3409, Loss: 0.07214410975575447, Final Batch Loss: 0.03597110137343407\n",
      "Epoch 3410, Loss: 0.019557963823899627, Final Batch Loss: 0.003388575976714492\n",
      "Epoch 3411, Loss: 0.06529159098863602, Final Batch Loss: 0.054688356816768646\n",
      "Epoch 3412, Loss: 0.01591743528842926, Final Batch Loss: 0.009957089088857174\n",
      "Epoch 3413, Loss: 0.04418725520372391, Final Batch Loss: 0.025402972474694252\n",
      "Epoch 3414, Loss: 0.028246290981769562, Final Batch Loss: 0.0020291246473789215\n",
      "Epoch 3415, Loss: 0.02827226184308529, Final Batch Loss: 0.019153352826833725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3416, Loss: 0.022726131603121758, Final Batch Loss: 0.0042016878724098206\n",
      "Epoch 3417, Loss: 0.018050645478069782, Final Batch Loss: 0.005753093399107456\n",
      "Epoch 3418, Loss: 0.02759353071451187, Final Batch Loss: 0.00931539200246334\n",
      "Epoch 3419, Loss: 0.08048400282859802, Final Batch Loss: 0.059259284287691116\n",
      "Epoch 3420, Loss: 0.03985445760190487, Final Batch Loss: 0.020950719714164734\n",
      "Epoch 3421, Loss: 0.04006597772240639, Final Batch Loss: 0.01775679737329483\n",
      "Epoch 3422, Loss: 0.02863115631043911, Final Batch Loss: 0.011162886396050453\n",
      "Epoch 3423, Loss: 0.02103341557085514, Final Batch Loss: 0.010759271681308746\n",
      "Epoch 3424, Loss: 0.040169321931898594, Final Batch Loss: 0.012580146081745625\n",
      "Epoch 3425, Loss: 0.04292571451514959, Final Batch Loss: 0.029261380434036255\n",
      "Epoch 3426, Loss: 0.030537543818354607, Final Batch Loss: 0.020026640966534615\n",
      "Epoch 3427, Loss: 0.028817445039749146, Final Batch Loss: 0.015448123216629028\n",
      "Epoch 3428, Loss: 0.06485994346439838, Final Batch Loss: 0.039518166333436966\n",
      "Epoch 3429, Loss: 0.0589924231171608, Final Batch Loss: 0.034432001411914825\n",
      "Epoch 3430, Loss: 0.02150537446141243, Final Batch Loss: 0.003132261335849762\n",
      "Epoch 3431, Loss: 0.03439530357718468, Final Batch Loss: 0.010269388556480408\n",
      "Epoch 3432, Loss: 0.04084258573129773, Final Batch Loss: 0.0063515012152493\n",
      "Epoch 3433, Loss: 0.02784102875739336, Final Batch Loss: 0.01289170142263174\n",
      "Epoch 3434, Loss: 0.03472956642508507, Final Batch Loss: 0.025864338502287865\n",
      "Epoch 3435, Loss: 0.020912124775350094, Final Batch Loss: 0.008717162534594536\n",
      "Epoch 3436, Loss: 0.02880706638097763, Final Batch Loss: 0.015011508949100971\n",
      "Epoch 3437, Loss: 0.019047704990953207, Final Batch Loss: 0.007259234320372343\n",
      "Epoch 3438, Loss: 0.03477277420461178, Final Batch Loss: 0.01437089778482914\n",
      "Epoch 3439, Loss: 0.031823684461414814, Final Batch Loss: 0.010253136046230793\n",
      "Epoch 3440, Loss: 0.03412783890962601, Final Batch Loss: 0.010373702272772789\n",
      "Epoch 3441, Loss: 0.0275387205183506, Final Batch Loss: 0.010011294856667519\n",
      "Epoch 3442, Loss: 0.012810708954930305, Final Batch Loss: 0.007410129997879267\n",
      "Epoch 3443, Loss: 0.023798230569809675, Final Batch Loss: 0.007694825064390898\n",
      "Epoch 3444, Loss: 0.04497554711997509, Final Batch Loss: 0.02389560639858246\n",
      "Epoch 3445, Loss: 0.026611908338963985, Final Batch Loss: 0.0037186676636338234\n",
      "Epoch 3446, Loss: 0.022281952667981386, Final Batch Loss: 0.005918410141021013\n",
      "Epoch 3447, Loss: 0.05675383284687996, Final Batch Loss: 0.033682968467473984\n",
      "Epoch 3448, Loss: 0.02166472002863884, Final Batch Loss: 0.010942710563540459\n",
      "Epoch 3449, Loss: 0.10414424911141396, Final Batch Loss: 0.0267949141561985\n",
      "Epoch 3450, Loss: 0.015202177222818136, Final Batch Loss: 0.004443343263119459\n",
      "Epoch 3451, Loss: 0.0629733856767416, Final Batch Loss: 0.025138454511761665\n",
      "Epoch 3452, Loss: 0.04796028696000576, Final Batch Loss: 0.0180683396756649\n",
      "Epoch 3453, Loss: 0.029594887048006058, Final Batch Loss: 0.011249125003814697\n",
      "Epoch 3454, Loss: 0.034995716996490955, Final Batch Loss: 0.02533106878399849\n",
      "Epoch 3455, Loss: 0.012370644602924585, Final Batch Loss: 0.002455561887472868\n",
      "Epoch 3456, Loss: 0.17025688011199236, Final Batch Loss: 0.15472549200057983\n",
      "Epoch 3457, Loss: 0.03887302987277508, Final Batch Loss: 0.022343449294567108\n",
      "Epoch 3458, Loss: 0.046880942303687334, Final Batch Loss: 0.004847277421504259\n",
      "Epoch 3459, Loss: 0.041017670184373856, Final Batch Loss: 0.02207978069782257\n",
      "Epoch 3460, Loss: 0.021374729927629232, Final Batch Loss: 0.004603005480021238\n",
      "Epoch 3461, Loss: 0.021247798576951027, Final Batch Loss: 0.006078191101551056\n",
      "Epoch 3462, Loss: 0.03109885659068823, Final Batch Loss: 0.019197355955839157\n",
      "Epoch 3463, Loss: 0.10330678289756179, Final Batch Loss: 0.09571734815835953\n",
      "Epoch 3464, Loss: 0.04143716813996434, Final Batch Loss: 0.0054768663831055164\n",
      "Epoch 3465, Loss: 0.032949197106063366, Final Batch Loss: 0.008898354135453701\n",
      "Epoch 3466, Loss: 0.029121502302587032, Final Batch Loss: 0.010984928347170353\n",
      "Epoch 3467, Loss: 0.04248486110009253, Final Batch Loss: 0.0037106575910001993\n",
      "Epoch 3468, Loss: 0.0867763664573431, Final Batch Loss: 0.018249234184622765\n",
      "Epoch 3469, Loss: 0.03424302954226732, Final Batch Loss: 0.025264598429203033\n",
      "Epoch 3470, Loss: 0.11061860201880336, Final Batch Loss: 0.005832407157868147\n",
      "Epoch 3471, Loss: 0.0202953084371984, Final Batch Loss: 0.013700149953365326\n",
      "Epoch 3472, Loss: 0.04736344609409571, Final Batch Loss: 0.032205816358327866\n",
      "Epoch 3473, Loss: 0.03809534991160035, Final Batch Loss: 0.004029946867376566\n",
      "Epoch 3474, Loss: 0.021613824646919966, Final Batch Loss: 0.007517839316278696\n",
      "Epoch 3475, Loss: 0.020089812576770782, Final Batch Loss: 0.007041384465992451\n",
      "Epoch 3476, Loss: 0.027081764303147793, Final Batch Loss: 0.01722903735935688\n",
      "Epoch 3477, Loss: 0.01181314466521144, Final Batch Loss: 0.0054184612818062305\n",
      "Epoch 3478, Loss: 0.035027521662414074, Final Batch Loss: 0.014635496772825718\n",
      "Epoch 3479, Loss: 0.05215339129790664, Final Batch Loss: 0.0029094605706632137\n",
      "Epoch 3480, Loss: 0.027543415315449238, Final Batch Loss: 0.013483932241797447\n",
      "Epoch 3481, Loss: 0.023443990387022495, Final Batch Loss: 0.013282419182360172\n",
      "Epoch 3482, Loss: 0.027843737043440342, Final Batch Loss: 0.013914741575717926\n",
      "Epoch 3483, Loss: 0.07347041368484497, Final Batch Loss: 0.03288192301988602\n",
      "Epoch 3484, Loss: 0.12052096612751484, Final Batch Loss: 0.09673412144184113\n",
      "Epoch 3485, Loss: 0.02945506665855646, Final Batch Loss: 0.02036980539560318\n",
      "Epoch 3486, Loss: 0.009932055487297475, Final Batch Loss: 0.0010626617586240172\n",
      "Epoch 3487, Loss: 0.031172946095466614, Final Batch Loss: 0.02302490547299385\n",
      "Epoch 3488, Loss: 0.055723387748003006, Final Batch Loss: 0.03356136754155159\n",
      "Epoch 3489, Loss: 0.039753266144543886, Final Batch Loss: 0.007094856817275286\n",
      "Epoch 3490, Loss: 0.2535190861672163, Final Batch Loss: 0.23187658190727234\n",
      "Epoch 3491, Loss: 0.022961268899962306, Final Batch Loss: 0.0036233465652912855\n",
      "Epoch 3492, Loss: 0.03809573873877525, Final Batch Loss: 0.0204275231808424\n",
      "Epoch 3493, Loss: 0.035218629986047745, Final Batch Loss: 0.02575908787548542\n",
      "Epoch 3494, Loss: 0.01907079527154565, Final Batch Loss: 0.005020396318286657\n",
      "Epoch 3495, Loss: 0.053523795679211617, Final Batch Loss: 0.019342711195349693\n",
      "Epoch 3496, Loss: 0.02770372573286295, Final Batch Loss: 0.01251668855547905\n",
      "Epoch 3497, Loss: 0.06474095769226551, Final Batch Loss: 0.03405681625008583\n",
      "Epoch 3498, Loss: 0.026581112295389175, Final Batch Loss: 0.009783348068594933\n",
      "Epoch 3499, Loss: 0.04421491175889969, Final Batch Loss: 0.029534708708524704\n",
      "Epoch 3500, Loss: 0.09376632235944271, Final Batch Loss: 0.06275934725999832\n",
      "Epoch 3501, Loss: 0.03608119348064065, Final Batch Loss: 0.005593244452029467\n",
      "Epoch 3502, Loss: 0.011183797614648938, Final Batch Loss: 0.0027742881793528795\n",
      "Epoch 3503, Loss: 0.03399486793205142, Final Batch Loss: 0.0029215863905847073\n",
      "Epoch 3504, Loss: 0.038343618623912334, Final Batch Loss: 0.006138707511126995\n",
      "Epoch 3505, Loss: 0.0558459497988224, Final Batch Loss: 0.03951843082904816\n",
      "Epoch 3506, Loss: 0.06439875438809395, Final Batch Loss: 0.028731249272823334\n",
      "Epoch 3507, Loss: 0.059060584753751755, Final Batch Loss: 0.017802894115447998\n",
      "Epoch 3508, Loss: 0.01715146703645587, Final Batch Loss: 0.010025870986282825\n",
      "Epoch 3509, Loss: 0.044812943786382675, Final Batch Loss: 0.025106584653258324\n",
      "Epoch 3510, Loss: 0.03179731499403715, Final Batch Loss: 0.021758364513516426\n",
      "Epoch 3511, Loss: 0.05408389214426279, Final Batch Loss: 0.03975871205329895\n",
      "Epoch 3512, Loss: 0.043876009061932564, Final Batch Loss: 0.023285845294594765\n",
      "Epoch 3513, Loss: 0.034462593495845795, Final Batch Loss: 0.01682022027671337\n",
      "Epoch 3514, Loss: 0.022213758900761604, Final Batch Loss: 0.009610192850232124\n",
      "Epoch 3515, Loss: 0.04168110340833664, Final Batch Loss: 0.02571798488497734\n",
      "Epoch 3516, Loss: 0.0479358471930027, Final Batch Loss: 0.02585200034081936\n",
      "Epoch 3517, Loss: 0.05808618105947971, Final Batch Loss: 0.04436701908707619\n",
      "Epoch 3518, Loss: 0.056369271129369736, Final Batch Loss: 0.038622938096523285\n",
      "Epoch 3519, Loss: 0.06953960843384266, Final Batch Loss: 0.017647476866841316\n",
      "Epoch 3520, Loss: 0.06789632141590118, Final Batch Loss: 0.03681831434369087\n",
      "Epoch 3521, Loss: 0.13343265280127525, Final Batch Loss: 0.09901031106710434\n",
      "Epoch 3522, Loss: 0.12705068290233612, Final Batch Loss: 0.10211676359176636\n",
      "Epoch 3523, Loss: 0.027496536262333393, Final Batch Loss: 0.006993080489337444\n",
      "Epoch 3524, Loss: 0.05313993990421295, Final Batch Loss: 0.028218932449817657\n",
      "Epoch 3525, Loss: 0.05597351957112551, Final Batch Loss: 0.013564047403633595\n",
      "Epoch 3526, Loss: 0.019942309707403183, Final Batch Loss: 0.006002451293170452\n",
      "Epoch 3527, Loss: 0.025919525884091854, Final Batch Loss: 0.01360542606562376\n",
      "Epoch 3528, Loss: 0.019953586161136627, Final Batch Loss: 0.009344087913632393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3529, Loss: 0.10976970940828323, Final Batch Loss: 0.09177979081869125\n",
      "Epoch 3530, Loss: 0.03691313415765762, Final Batch Loss: 0.008465617895126343\n",
      "Epoch 3531, Loss: 0.04649882949888706, Final Batch Loss: 0.006104001775383949\n",
      "Epoch 3532, Loss: 0.041036173701286316, Final Batch Loss: 0.02920435182750225\n",
      "Epoch 3533, Loss: 0.0666105430573225, Final Batch Loss: 0.039689283818006516\n",
      "Epoch 3534, Loss: 0.06119230575859547, Final Batch Loss: 0.04054880514740944\n",
      "Epoch 3535, Loss: 0.02186022838577628, Final Batch Loss: 0.003503610845655203\n",
      "Epoch 3536, Loss: 0.020561971701681614, Final Batch Loss: 0.008953061886131763\n",
      "Epoch 3537, Loss: 0.048878771252930164, Final Batch Loss: 0.03503430262207985\n",
      "Epoch 3538, Loss: 0.07414571940898895, Final Batch Loss: 0.03627855330705643\n",
      "Epoch 3539, Loss: 0.05272655561566353, Final Batch Loss: 0.012767385691404343\n",
      "Epoch 3540, Loss: 0.03906325902789831, Final Batch Loss: 0.02778744138777256\n",
      "Epoch 3541, Loss: 0.06627929210662842, Final Batch Loss: 0.039519377052783966\n",
      "Epoch 3542, Loss: 0.013699461473152041, Final Batch Loss: 0.0037701360415667295\n",
      "Epoch 3543, Loss: 0.025084232911467552, Final Batch Loss: 0.005408048629760742\n",
      "Epoch 3544, Loss: 0.025765265803784132, Final Batch Loss: 0.006318956147879362\n",
      "Epoch 3545, Loss: 0.022769526578485966, Final Batch Loss: 0.00808155070990324\n",
      "Epoch 3546, Loss: 0.03153893072158098, Final Batch Loss: 0.01833081804215908\n",
      "Epoch 3547, Loss: 0.02776001743040979, Final Batch Loss: 0.0038656198885291815\n",
      "Epoch 3548, Loss: 0.03909073118120432, Final Batch Loss: 0.026801399886608124\n",
      "Epoch 3549, Loss: 0.048683315981179476, Final Batch Loss: 0.006607778836041689\n",
      "Epoch 3550, Loss: 0.034983827732503414, Final Batch Loss: 0.008307128213346004\n",
      "Epoch 3551, Loss: 0.07294647954404354, Final Batch Loss: 0.06471081078052521\n",
      "Epoch 3552, Loss: 0.030923567712306976, Final Batch Loss: 0.015391030348837376\n",
      "Epoch 3553, Loss: 0.014035983942449093, Final Batch Loss: 0.007733627688139677\n",
      "Epoch 3554, Loss: 0.012176368851214647, Final Batch Loss: 0.004656415432691574\n",
      "Epoch 3555, Loss: 0.03135634399950504, Final Batch Loss: 0.012942411005496979\n",
      "Epoch 3556, Loss: 0.014315834268927574, Final Batch Loss: 0.0068471794947981834\n",
      "Epoch 3557, Loss: 0.02629178110510111, Final Batch Loss: 0.010945549234747887\n",
      "Epoch 3558, Loss: 0.015235038939863443, Final Batch Loss: 0.0033672009594738483\n",
      "Epoch 3559, Loss: 0.09240459091961384, Final Batch Loss: 0.06490235775709152\n",
      "Epoch 3560, Loss: 0.07438692450523376, Final Batch Loss: 0.04765167459845543\n",
      "Epoch 3561, Loss: 0.019654802978038788, Final Batch Loss: 0.005154835991561413\n",
      "Epoch 3562, Loss: 0.017745214514434338, Final Batch Loss: 0.009860171005129814\n",
      "Epoch 3563, Loss: 0.03176622558385134, Final Batch Loss: 0.022709151729941368\n",
      "Epoch 3564, Loss: 0.03976085549220443, Final Batch Loss: 0.0325431190431118\n",
      "Epoch 3565, Loss: 0.03741435892879963, Final Batch Loss: 0.018538378179073334\n",
      "Epoch 3566, Loss: 0.02408624440431595, Final Batch Loss: 0.009935788810253143\n",
      "Epoch 3567, Loss: 0.01584182260558009, Final Batch Loss: 0.004703630227595568\n",
      "Epoch 3568, Loss: 0.038214909844100475, Final Batch Loss: 0.012722414918243885\n",
      "Epoch 3569, Loss: 0.0143356011249125, Final Batch Loss: 0.004256879445165396\n",
      "Epoch 3570, Loss: 0.013361990451812744, Final Batch Loss: 0.005108614452183247\n",
      "Epoch 3571, Loss: 0.024362663738429546, Final Batch Loss: 0.011822921223938465\n",
      "Epoch 3572, Loss: 0.021747226361185312, Final Batch Loss: 0.0076237390749156475\n",
      "Epoch 3573, Loss: 0.015996667090803385, Final Batch Loss: 0.005298891570419073\n",
      "Epoch 3574, Loss: 0.05577016808092594, Final Batch Loss: 0.042783889919519424\n",
      "Epoch 3575, Loss: 0.041242675855755806, Final Batch Loss: 0.022673916071653366\n",
      "Epoch 3576, Loss: 0.02487320639193058, Final Batch Loss: 0.001465078443288803\n",
      "Epoch 3577, Loss: 0.055344619788229465, Final Batch Loss: 0.015082084573805332\n",
      "Epoch 3578, Loss: 0.022807713132351637, Final Batch Loss: 0.0048083956353366375\n",
      "Epoch 3579, Loss: 0.021501845796592534, Final Batch Loss: 0.0017850539879873395\n",
      "Epoch 3580, Loss: 0.08379152044653893, Final Batch Loss: 0.07483438402414322\n",
      "Epoch 3581, Loss: 0.02469646418467164, Final Batch Loss: 0.003992213401943445\n",
      "Epoch 3582, Loss: 0.019380029290914536, Final Batch Loss: 0.008543179370462894\n",
      "Epoch 3583, Loss: 0.012653620913624763, Final Batch Loss: 0.0025061024352908134\n",
      "Epoch 3584, Loss: 0.04558439739048481, Final Batch Loss: 0.016082100570201874\n",
      "Epoch 3585, Loss: 0.1015808917582035, Final Batch Loss: 0.04701315239071846\n",
      "Epoch 3586, Loss: 0.026735635474324226, Final Batch Loss: 0.017612121999263763\n",
      "Epoch 3587, Loss: 0.040517874993383884, Final Batch Loss: 0.009014246053993702\n",
      "Epoch 3588, Loss: 0.05373596027493477, Final Batch Loss: 0.03759939596056938\n",
      "Epoch 3589, Loss: 0.017766268458217382, Final Batch Loss: 0.00772049417719245\n",
      "Epoch 3590, Loss: 0.01606367202475667, Final Batch Loss: 0.005119237583130598\n",
      "Epoch 3591, Loss: 0.06109957117587328, Final Batch Loss: 0.051703907549381256\n",
      "Epoch 3592, Loss: 0.027400156017392874, Final Batch Loss: 0.0044702342711389065\n",
      "Epoch 3593, Loss: 0.016158616170287132, Final Batch Loss: 0.0035271840170025826\n",
      "Epoch 3594, Loss: 0.02729267254471779, Final Batch Loss: 0.013731598854064941\n",
      "Epoch 3595, Loss: 0.018087186850607395, Final Batch Loss: 0.0028082337230443954\n",
      "Epoch 3596, Loss: 0.016574527136981487, Final Batch Loss: 0.004620076157152653\n",
      "Epoch 3597, Loss: 0.042971812188625336, Final Batch Loss: 0.013383323326706886\n",
      "Epoch 3598, Loss: 0.09404607489705086, Final Batch Loss: 0.05634468421339989\n",
      "Epoch 3599, Loss: 0.013168948702514172, Final Batch Loss: 0.002924911677837372\n",
      "Epoch 3600, Loss: 0.05983293242752552, Final Batch Loss: 0.03919494152069092\n",
      "Epoch 3601, Loss: 0.04530142620205879, Final Batch Loss: 0.009451892226934433\n",
      "Epoch 3602, Loss: 0.01738910935819149, Final Batch Loss: 0.007056623697280884\n",
      "Epoch 3603, Loss: 0.04665744863450527, Final Batch Loss: 0.018283942714333534\n",
      "Epoch 3604, Loss: 0.032422750256955624, Final Batch Loss: 0.01884414069354534\n",
      "Epoch 3605, Loss: 0.026113199535757303, Final Batch Loss: 0.007566813845187426\n",
      "Epoch 3606, Loss: 0.050584517419338226, Final Batch Loss: 0.0217306986451149\n",
      "Epoch 3607, Loss: 0.02545863389968872, Final Batch Loss: 0.009059099480509758\n",
      "Epoch 3608, Loss: 0.016728883143514395, Final Batch Loss: 0.005992913153022528\n",
      "Epoch 3609, Loss: 0.01776381954550743, Final Batch Loss: 0.0047279344871640205\n",
      "Epoch 3610, Loss: 0.030506412498652935, Final Batch Loss: 0.009985248558223248\n",
      "Epoch 3611, Loss: 0.05904258042573929, Final Batch Loss: 0.02295009046792984\n",
      "Epoch 3612, Loss: 0.014820209005847573, Final Batch Loss: 0.00323052448220551\n",
      "Epoch 3613, Loss: 0.00810672645457089, Final Batch Loss: 0.002953289309516549\n",
      "Epoch 3614, Loss: 0.017882675863802433, Final Batch Loss: 0.005852069705724716\n",
      "Epoch 3615, Loss: 0.05584001913666725, Final Batch Loss: 0.04207108914852142\n",
      "Epoch 3616, Loss: 0.01515283901244402, Final Batch Loss: 0.006308120675384998\n",
      "Epoch 3617, Loss: 0.03525752015411854, Final Batch Loss: 0.019647475332021713\n",
      "Epoch 3618, Loss: 0.022318757139146328, Final Batch Loss: 0.00917106308043003\n",
      "Epoch 3619, Loss: 0.039582107216119766, Final Batch Loss: 0.01813393458724022\n",
      "Epoch 3620, Loss: 0.022241181693971157, Final Batch Loss: 0.009349963627755642\n",
      "Epoch 3621, Loss: 0.03724000044167042, Final Batch Loss: 0.02008930779993534\n",
      "Epoch 3622, Loss: 0.04139438655693084, Final Batch Loss: 0.001412060228176415\n",
      "Epoch 3623, Loss: 0.051867094822227955, Final Batch Loss: 0.03950642794370651\n",
      "Epoch 3624, Loss: 0.018710751086473465, Final Batch Loss: 0.0037185242399573326\n",
      "Epoch 3625, Loss: 0.025268007069826126, Final Batch Loss: 0.006936915218830109\n",
      "Epoch 3626, Loss: 0.046379074454307556, Final Batch Loss: 0.030088694766163826\n",
      "Epoch 3627, Loss: 0.047314345836639404, Final Batch Loss: 0.02293330244719982\n",
      "Epoch 3628, Loss: 0.02246002061292529, Final Batch Loss: 0.0034345868043601513\n",
      "Epoch 3629, Loss: 0.026397481560707092, Final Batch Loss: 0.018123934045433998\n",
      "Epoch 3630, Loss: 0.03403229173272848, Final Batch Loss: 0.020189881324768066\n",
      "Epoch 3631, Loss: 0.021135983057320118, Final Batch Loss: 0.010635505430400372\n",
      "Epoch 3632, Loss: 0.010556328808888793, Final Batch Loss: 0.002835308900102973\n",
      "Epoch 3633, Loss: 0.27738812007009983, Final Batch Loss: 0.2541652321815491\n",
      "Epoch 3634, Loss: 0.0160873387940228, Final Batch Loss: 0.0036989529617130756\n",
      "Epoch 3635, Loss: 0.07555332779884338, Final Batch Loss: 0.06682208925485611\n",
      "Epoch 3636, Loss: 0.02720680832862854, Final Batch Loss: 0.008380312472581863\n",
      "Epoch 3637, Loss: 0.019781576469540596, Final Batch Loss: 0.0049458155408501625\n",
      "Epoch 3638, Loss: 0.052511872723698616, Final Batch Loss: 0.018343059346079826\n",
      "Epoch 3639, Loss: 0.03731092344969511, Final Batch Loss: 0.0062742577865719795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3640, Loss: 0.04101669229567051, Final Batch Loss: 0.029994919896125793\n",
      "Epoch 3641, Loss: 0.07235515676438808, Final Batch Loss: 0.018281059339642525\n",
      "Epoch 3642, Loss: 0.005980609334073961, Final Batch Loss: 0.001582191209308803\n",
      "Epoch 3643, Loss: 0.042975371703505516, Final Batch Loss: 0.010025965049862862\n",
      "Epoch 3644, Loss: 0.022683524526655674, Final Batch Loss: 0.011089096777141094\n",
      "Epoch 3645, Loss: 0.030544734559953213, Final Batch Loss: 0.015098683536052704\n",
      "Epoch 3646, Loss: 0.010358714032918215, Final Batch Loss: 0.002579750493168831\n",
      "Epoch 3647, Loss: 0.03653127746656537, Final Batch Loss: 0.005928183440119028\n",
      "Epoch 3648, Loss: 0.018164228182286024, Final Batch Loss: 0.0024087647907435894\n",
      "Epoch 3649, Loss: 0.030360199511051178, Final Batch Loss: 0.020977718755602837\n",
      "Epoch 3650, Loss: 0.02564163552597165, Final Batch Loss: 0.005268505308777094\n",
      "Epoch 3651, Loss: 0.013184867333620787, Final Batch Loss: 0.007996004074811935\n",
      "Epoch 3652, Loss: 0.12460541818290949, Final Batch Loss: 0.11021173000335693\n",
      "Epoch 3653, Loss: 0.03077190276235342, Final Batch Loss: 0.013469583354890347\n",
      "Epoch 3654, Loss: 0.023681949824094772, Final Batch Loss: 0.013086599297821522\n",
      "Epoch 3655, Loss: 0.019016058184206486, Final Batch Loss: 0.004829884506762028\n",
      "Epoch 3656, Loss: 0.0198781187646091, Final Batch Loss: 0.007707636337727308\n",
      "Epoch 3657, Loss: 0.058833468705415726, Final Batch Loss: 0.01565162092447281\n",
      "Epoch 3658, Loss: 0.027284056413918734, Final Batch Loss: 0.006306071300059557\n",
      "Epoch 3659, Loss: 0.02629705984145403, Final Batch Loss: 0.005392848514020443\n",
      "Epoch 3660, Loss: 0.022580251097679138, Final Batch Loss: 0.013621191494166851\n",
      "Epoch 3661, Loss: 0.024348608683794737, Final Batch Loss: 0.017478760331869125\n",
      "Epoch 3662, Loss: 0.05707831401377916, Final Batch Loss: 0.048650894314050674\n",
      "Epoch 3663, Loss: 0.04054155759513378, Final Batch Loss: 0.01888199709355831\n",
      "Epoch 3664, Loss: 0.020401854068040848, Final Batch Loss: 0.009025146253407001\n",
      "Epoch 3665, Loss: 0.021159048657864332, Final Batch Loss: 0.0054846215061843395\n",
      "Epoch 3666, Loss: 0.04155448079109192, Final Batch Loss: 0.03151962533593178\n",
      "Epoch 3667, Loss: 0.020272729452699423, Final Batch Loss: 0.012506580911576748\n",
      "Epoch 3668, Loss: 0.05263475142419338, Final Batch Loss: 0.033066485077142715\n",
      "Epoch 3669, Loss: 0.11745607480406761, Final Batch Loss: 0.09574839472770691\n",
      "Epoch 3670, Loss: 0.02918981690891087, Final Batch Loss: 0.0033536863047629595\n",
      "Epoch 3671, Loss: 0.016858376562595367, Final Batch Loss: 0.011017335578799248\n",
      "Epoch 3672, Loss: 0.028407348319888115, Final Batch Loss: 0.012399477884173393\n",
      "Epoch 3673, Loss: 0.026613088324666023, Final Batch Loss: 0.010554743930697441\n",
      "Epoch 3674, Loss: 0.06376021914184093, Final Batch Loss: 0.025954371318221092\n",
      "Epoch 3675, Loss: 0.0350489616394043, Final Batch Loss: 0.010483114048838615\n",
      "Epoch 3676, Loss: 0.08174112997949123, Final Batch Loss: 0.030240239575505257\n",
      "Epoch 3677, Loss: 0.1202504001557827, Final Batch Loss: 0.06308072805404663\n",
      "Epoch 3678, Loss: 0.06730031128972769, Final Batch Loss: 0.058130137622356415\n",
      "Epoch 3679, Loss: 0.019005384761840105, Final Batch Loss: 0.006967736873775721\n",
      "Epoch 3680, Loss: 0.03092066291719675, Final Batch Loss: 0.007425862364470959\n",
      "Epoch 3681, Loss: 0.02619091421365738, Final Batch Loss: 0.008708298206329346\n",
      "Epoch 3682, Loss: 0.024133347906172276, Final Batch Loss: 0.010617106221616268\n",
      "Epoch 3683, Loss: 0.021066060289740562, Final Batch Loss: 0.0085985716432333\n",
      "Epoch 3684, Loss: 0.026336458045989275, Final Batch Loss: 0.005027808714658022\n",
      "Epoch 3685, Loss: 0.028335826471447945, Final Batch Loss: 0.012323372066020966\n",
      "Epoch 3686, Loss: 0.0374741405248642, Final Batch Loss: 0.019956424832344055\n",
      "Epoch 3687, Loss: 0.03022440057247877, Final Batch Loss: 0.01801431179046631\n",
      "Epoch 3688, Loss: 0.03280602674931288, Final Batch Loss: 0.017593447118997574\n",
      "Epoch 3689, Loss: 0.0131703345105052, Final Batch Loss: 0.0018861722201108932\n",
      "Epoch 3690, Loss: 0.04420118220150471, Final Batch Loss: 0.018289359286427498\n",
      "Epoch 3691, Loss: 0.03534896578639746, Final Batch Loss: 0.021716369315981865\n",
      "Epoch 3692, Loss: 0.01305573363788426, Final Batch Loss: 0.003051015781238675\n",
      "Epoch 3693, Loss: 0.04297707788646221, Final Batch Loss: 0.007764270529150963\n",
      "Epoch 3694, Loss: 0.013181257992982864, Final Batch Loss: 0.007374004926532507\n",
      "Epoch 3695, Loss: 0.01593168533872813, Final Batch Loss: 0.0018655358580872416\n",
      "Epoch 3696, Loss: 0.022921117022633553, Final Batch Loss: 0.013838599435985088\n",
      "Epoch 3697, Loss: 0.06026108376681805, Final Batch Loss: 0.03599384054541588\n",
      "Epoch 3698, Loss: 0.050290146842598915, Final Batch Loss: 0.0224958173930645\n",
      "Epoch 3699, Loss: 0.0367564931511879, Final Batch Loss: 0.0031529590487480164\n",
      "Epoch 3700, Loss: 0.027113645803183317, Final Batch Loss: 0.0021892613731324673\n",
      "Epoch 3701, Loss: 0.021962297149002552, Final Batch Loss: 0.010903439484536648\n",
      "Epoch 3702, Loss: 0.052585408091545105, Final Batch Loss: 0.04312800243496895\n",
      "Epoch 3703, Loss: 0.06979610119014978, Final Batch Loss: 0.055552106350660324\n",
      "Epoch 3704, Loss: 0.03370759543031454, Final Batch Loss: 0.018269985914230347\n",
      "Epoch 3705, Loss: 0.03482264652848244, Final Batch Loss: 0.02099878340959549\n",
      "Epoch 3706, Loss: 0.06421210989356041, Final Batch Loss: 0.048335961997509\n",
      "Epoch 3707, Loss: 0.03866117959842086, Final Batch Loss: 0.0040677874349057674\n",
      "Epoch 3708, Loss: 0.037128375843167305, Final Batch Loss: 0.011988524347543716\n",
      "Epoch 3709, Loss: 0.03415563330054283, Final Batch Loss: 0.014728425070643425\n",
      "Epoch 3710, Loss: 0.02754787914454937, Final Batch Loss: 0.015204621478915215\n",
      "Epoch 3711, Loss: 0.05697517283260822, Final Batch Loss: 0.02270791493356228\n",
      "Epoch 3712, Loss: 0.015680283773690462, Final Batch Loss: 0.003108460921794176\n",
      "Epoch 3713, Loss: 0.04128200560808182, Final Batch Loss: 0.03229565918445587\n",
      "Epoch 3714, Loss: 0.06024397537112236, Final Batch Loss: 0.04415431618690491\n",
      "Epoch 3715, Loss: 0.04484599828720093, Final Batch Loss: 0.016903022304177284\n",
      "Epoch 3716, Loss: 0.013433792628347874, Final Batch Loss: 0.0015609972178936005\n",
      "Epoch 3717, Loss: 0.028142825234681368, Final Batch Loss: 0.007695887703448534\n",
      "Epoch 3718, Loss: 0.03270757291465998, Final Batch Loss: 0.015197106637060642\n",
      "Epoch 3719, Loss: 0.016797127202153206, Final Batch Loss: 0.0056887175887823105\n",
      "Epoch 3720, Loss: 0.03324073739349842, Final Batch Loss: 0.013867028057575226\n",
      "Epoch 3721, Loss: 0.017128375824540854, Final Batch Loss: 0.007779677864164114\n",
      "Epoch 3722, Loss: 0.04084893874824047, Final Batch Loss: 0.022921737283468246\n",
      "Epoch 3723, Loss: 0.038068173453211784, Final Batch Loss: 0.02375180646777153\n",
      "Epoch 3724, Loss: 0.02297294931486249, Final Batch Loss: 0.006815907079726458\n",
      "Epoch 3725, Loss: 0.02629129495471716, Final Batch Loss: 0.004106004722416401\n",
      "Epoch 3726, Loss: 0.031474229879677296, Final Batch Loss: 0.022629283368587494\n",
      "Epoch 3727, Loss: 0.026665747864171863, Final Batch Loss: 0.00372673268429935\n",
      "Epoch 3728, Loss: 0.0241301734931767, Final Batch Loss: 0.0074301366694271564\n",
      "Epoch 3729, Loss: 0.104927534237504, Final Batch Loss: 0.07390350103378296\n",
      "Epoch 3730, Loss: 0.039270506240427494, Final Batch Loss: 0.012758159078657627\n",
      "Epoch 3731, Loss: 0.08146077208220959, Final Batch Loss: 0.053643107414245605\n",
      "Epoch 3732, Loss: 0.016693489160388708, Final Batch Loss: 0.007338262628763914\n",
      "Epoch 3733, Loss: 0.03220533765852451, Final Batch Loss: 0.022759534418582916\n",
      "Epoch 3734, Loss: 0.022804733831435442, Final Batch Loss: 0.01633523963391781\n",
      "Epoch 3735, Loss: 0.05457131005823612, Final Batch Loss: 0.02214960567653179\n",
      "Epoch 3736, Loss: 0.02300079306587577, Final Batch Loss: 0.00436796760186553\n",
      "Epoch 3737, Loss: 0.014089173637330532, Final Batch Loss: 0.007510779425501823\n",
      "Epoch 3738, Loss: 0.03838179958984256, Final Batch Loss: 0.004517089109867811\n",
      "Epoch 3739, Loss: 0.027613655664026737, Final Batch Loss: 0.011617477051913738\n",
      "Epoch 3740, Loss: 0.012678174767643213, Final Batch Loss: 0.005839848890900612\n",
      "Epoch 3741, Loss: 0.01930497121065855, Final Batch Loss: 0.00847657397389412\n",
      "Epoch 3742, Loss: 0.021924286615103483, Final Batch Loss: 0.004116434138268232\n",
      "Epoch 3743, Loss: 0.016551663167774677, Final Batch Loss: 0.009254137985408306\n",
      "Epoch 3744, Loss: 0.03994285617955029, Final Batch Loss: 0.002207062439993024\n",
      "Epoch 3745, Loss: 0.02851403970271349, Final Batch Loss: 0.006024251691997051\n",
      "Epoch 3746, Loss: 0.05189745873212814, Final Batch Loss: 0.021884867921471596\n",
      "Epoch 3747, Loss: 0.008125689695589244, Final Batch Loss: 0.0005961888236925006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3748, Loss: 0.006449863780289888, Final Batch Loss: 0.0024921558797359467\n",
      "Epoch 3749, Loss: 0.037112738005816936, Final Batch Loss: 0.027994565665721893\n",
      "Epoch 3750, Loss: 0.024294987320899963, Final Batch Loss: 0.007940340787172318\n",
      "Epoch 3751, Loss: 0.01891338126733899, Final Batch Loss: 0.004326151218265295\n",
      "Epoch 3752, Loss: 0.013983739074319601, Final Batch Loss: 0.005070523824542761\n",
      "Epoch 3753, Loss: 0.025280762230977416, Final Batch Loss: 0.0028235723730176687\n",
      "Epoch 3754, Loss: 0.018894642125815153, Final Batch Loss: 0.0033142645843327045\n",
      "Epoch 3755, Loss: 0.03184448182582855, Final Batch Loss: 0.010502230376005173\n",
      "Epoch 3756, Loss: 0.029550510924309492, Final Batch Loss: 0.007542902138084173\n",
      "Epoch 3757, Loss: 0.06711108982563019, Final Batch Loss: 0.051366325467824936\n",
      "Epoch 3758, Loss: 0.13922250270843506, Final Batch Loss: 0.12989862263202667\n",
      "Epoch 3759, Loss: 0.054903266951441765, Final Batch Loss: 0.038571711629629135\n",
      "Epoch 3760, Loss: 0.010588805074803531, Final Batch Loss: 0.0011873877374455333\n",
      "Epoch 3761, Loss: 0.0207098089158535, Final Batch Loss: 0.004212178289890289\n",
      "Epoch 3762, Loss: 0.028070854721590877, Final Batch Loss: 0.0036129679065197706\n",
      "Epoch 3763, Loss: 0.04268443211913109, Final Batch Loss: 0.025868970900774002\n",
      "Epoch 3764, Loss: 0.014074594248086214, Final Batch Loss: 0.004011799115687609\n",
      "Epoch 3765, Loss: 0.02539450954645872, Final Batch Loss: 0.009054482914507389\n",
      "Epoch 3766, Loss: 0.04767928645014763, Final Batch Loss: 0.02217080257833004\n",
      "Epoch 3767, Loss: 0.026397040113806725, Final Batch Loss: 0.019462883472442627\n",
      "Epoch 3768, Loss: 0.036347195506095886, Final Batch Loss: 0.007961565628647804\n",
      "Epoch 3769, Loss: 0.028361873235553503, Final Batch Loss: 0.007224987726658583\n",
      "Epoch 3770, Loss: 0.024187830975279212, Final Batch Loss: 0.0037648396100848913\n",
      "Epoch 3771, Loss: 0.041369506157934666, Final Batch Loss: 0.015174373053014278\n",
      "Epoch 3772, Loss: 0.026369462721049786, Final Batch Loss: 0.011582605540752411\n",
      "Epoch 3773, Loss: 0.031080035725608468, Final Batch Loss: 0.0018888788763433695\n",
      "Epoch 3774, Loss: 0.034358710050582886, Final Batch Loss: 0.022946272045373917\n",
      "Epoch 3775, Loss: 0.028823532164096832, Final Batch Loss: 0.017202498391270638\n",
      "Epoch 3776, Loss: 0.041016463190317154, Final Batch Loss: 0.027196910232305527\n",
      "Epoch 3777, Loss: 0.017237078864127398, Final Batch Loss: 0.00469233701005578\n",
      "Epoch 3778, Loss: 0.036542941350489855, Final Batch Loss: 0.028798257932066917\n",
      "Epoch 3779, Loss: 0.03409427963197231, Final Batch Loss: 0.009961707517504692\n",
      "Epoch 3780, Loss: 0.019675610587000847, Final Batch Loss: 0.006191891618072987\n",
      "Epoch 3781, Loss: 0.02408388233743608, Final Batch Loss: 0.0017671601381152868\n",
      "Epoch 3782, Loss: 0.030703095719218254, Final Batch Loss: 0.02049833908677101\n",
      "Epoch 3783, Loss: 0.14559616893529892, Final Batch Loss: 0.11199621856212616\n",
      "Epoch 3784, Loss: 0.024594855727627873, Final Batch Loss: 0.0029406503308564425\n",
      "Epoch 3785, Loss: 0.017157908994704485, Final Batch Loss: 0.01255908515304327\n",
      "Epoch 3786, Loss: 0.06552654877305031, Final Batch Loss: 0.028735801577568054\n",
      "Epoch 3787, Loss: 0.08206867054104805, Final Batch Loss: 0.027102310210466385\n",
      "Epoch 3788, Loss: 0.03632226772606373, Final Batch Loss: 0.0189414843916893\n",
      "Epoch 3789, Loss: 0.02422697376459837, Final Batch Loss: 0.014975963160395622\n",
      "Epoch 3790, Loss: 0.021638600155711174, Final Batch Loss: 0.004608266055583954\n",
      "Epoch 3791, Loss: 0.027735882438719273, Final Batch Loss: 0.020428184419870377\n",
      "Epoch 3792, Loss: 0.05057491734623909, Final Batch Loss: 0.008833039551973343\n",
      "Epoch 3793, Loss: 0.03660629130899906, Final Batch Loss: 0.019416555762290955\n",
      "Epoch 3794, Loss: 0.053130168467760086, Final Batch Loss: 0.03661341592669487\n",
      "Epoch 3795, Loss: 0.018462576437741518, Final Batch Loss: 0.005299484822899103\n",
      "Epoch 3796, Loss: 0.02045306982472539, Final Batch Loss: 0.007229770068079233\n",
      "Epoch 3797, Loss: 0.018547570798546076, Final Batch Loss: 0.0025287852622568607\n",
      "Epoch 3798, Loss: 0.02854259219020605, Final Batch Loss: 0.01312323659658432\n",
      "Epoch 3799, Loss: 0.016230549663305283, Final Batch Loss: 0.002162541262805462\n",
      "Epoch 3800, Loss: 0.04787139780819416, Final Batch Loss: 0.02204461768269539\n",
      "Epoch 3801, Loss: 0.015848403796553612, Final Batch Loss: 0.009871067479252815\n",
      "Epoch 3802, Loss: 0.04325054585933685, Final Batch Loss: 0.014136416837573051\n",
      "Epoch 3803, Loss: 0.014792964328080416, Final Batch Loss: 0.004905794281512499\n",
      "Epoch 3804, Loss: 0.0519261434674263, Final Batch Loss: 0.0246183592826128\n",
      "Epoch 3805, Loss: 0.03570583276450634, Final Batch Loss: 0.019077720120549202\n",
      "Epoch 3806, Loss: 0.06654047593474388, Final Batch Loss: 0.026565536856651306\n",
      "Epoch 3807, Loss: 0.013656370574608445, Final Batch Loss: 0.002435357542708516\n",
      "Epoch 3808, Loss: 0.01021546171978116, Final Batch Loss: 0.0031751422211527824\n",
      "Epoch 3809, Loss: 0.040909131057560444, Final Batch Loss: 0.02797851338982582\n",
      "Epoch 3810, Loss: 0.03739704005420208, Final Batch Loss: 0.009936792775988579\n",
      "Epoch 3811, Loss: 0.09217678848654032, Final Batch Loss: 0.07719594985246658\n",
      "Epoch 3812, Loss: 0.04975147172808647, Final Batch Loss: 0.027668515220284462\n",
      "Epoch 3813, Loss: 0.03463610541075468, Final Batch Loss: 0.014710147865116596\n",
      "Epoch 3814, Loss: 0.03094875067472458, Final Batch Loss: 0.010434119030833244\n",
      "Epoch 3815, Loss: 0.02583266980946064, Final Batch Loss: 0.015680944547057152\n",
      "Epoch 3816, Loss: 0.026280123740434647, Final Batch Loss: 0.005951175466179848\n",
      "Epoch 3817, Loss: 0.01572414580732584, Final Batch Loss: 0.0050568049773573875\n",
      "Epoch 3818, Loss: 0.02489118045195937, Final Batch Loss: 0.002444272395223379\n",
      "Epoch 3819, Loss: 0.019219072768464684, Final Batch Loss: 0.003875782946124673\n",
      "Epoch 3820, Loss: 0.05316019989550114, Final Batch Loss: 0.036831870675086975\n",
      "Epoch 3821, Loss: 0.025829344987869263, Final Batch Loss: 0.01049721334129572\n",
      "Epoch 3822, Loss: 0.032788654789328575, Final Batch Loss: 0.01908842846751213\n",
      "Epoch 3823, Loss: 0.010584667790681124, Final Batch Loss: 0.005997050553560257\n",
      "Epoch 3824, Loss: 0.010760982055217028, Final Batch Loss: 0.0035012932494282722\n",
      "Epoch 3825, Loss: 0.018499381840229034, Final Batch Loss: 0.0027313660830259323\n",
      "Epoch 3826, Loss: 0.017132438253611326, Final Batch Loss: 0.002894038800150156\n",
      "Epoch 3827, Loss: 0.036728314589709044, Final Batch Loss: 0.0070642451755702496\n",
      "Epoch 3828, Loss: 0.024939345195889473, Final Batch Loss: 0.015630442649126053\n",
      "Epoch 3829, Loss: 0.007937600137665868, Final Batch Loss: 0.0027139110025018454\n",
      "Epoch 3830, Loss: 0.017900336533784866, Final Batch Loss: 0.00964473094791174\n",
      "Epoch 3831, Loss: 0.01000632019713521, Final Batch Loss: 0.004235242959111929\n",
      "Epoch 3832, Loss: 0.03975784219801426, Final Batch Loss: 0.021353648975491524\n",
      "Epoch 3833, Loss: 0.02013932727277279, Final Batch Loss: 0.008835459128022194\n",
      "Epoch 3834, Loss: 0.17575598508119583, Final Batch Loss: 0.101618193089962\n",
      "Epoch 3835, Loss: 0.033968023024499416, Final Batch Loss: 0.004080667160451412\n",
      "Epoch 3836, Loss: 0.015346057713031769, Final Batch Loss: 0.0060258666053414345\n",
      "Epoch 3837, Loss: 0.06858314014971256, Final Batch Loss: 0.03819391131401062\n",
      "Epoch 3838, Loss: 0.013152193045243621, Final Batch Loss: 0.002328303409740329\n",
      "Epoch 3839, Loss: 0.019156078808009624, Final Batch Loss: 0.007604612037539482\n",
      "Epoch 3840, Loss: 0.03703226521611214, Final Batch Loss: 0.015345055609941483\n",
      "Epoch 3841, Loss: 0.08485720213502645, Final Batch Loss: 0.01498930063098669\n",
      "Epoch 3842, Loss: 0.03319142572581768, Final Batch Loss: 0.015830500051379204\n",
      "Epoch 3843, Loss: 0.011005876120179892, Final Batch Loss: 0.0065936679020524025\n",
      "Epoch 3844, Loss: 0.07148171775043011, Final Batch Loss: 0.05604350194334984\n",
      "Epoch 3845, Loss: 0.018099497072398663, Final Batch Loss: 0.011077020317316055\n",
      "Epoch 3846, Loss: 0.023181180469691753, Final Batch Loss: 0.017075851559638977\n",
      "Epoch 3847, Loss: 0.02719980012625456, Final Batch Loss: 0.007642393000423908\n",
      "Epoch 3848, Loss: 0.023791885934770107, Final Batch Loss: 0.010595887899398804\n",
      "Epoch 3849, Loss: 0.034343204577453434, Final Batch Loss: 0.0017860970692709088\n",
      "Epoch 3850, Loss: 0.024059605319052935, Final Batch Loss: 0.007403327617794275\n",
      "Epoch 3851, Loss: 0.006184782483614981, Final Batch Loss: 0.0014327858807519078\n",
      "Epoch 3852, Loss: 0.015716382302343845, Final Batch Loss: 0.004563692957162857\n",
      "Epoch 3853, Loss: 0.10748723475262523, Final Batch Loss: 0.10171261429786682\n",
      "Epoch 3854, Loss: 0.04336758702993393, Final Batch Loss: 0.022462548688054085\n",
      "Epoch 3855, Loss: 0.018014176283031702, Final Batch Loss: 0.012392965145409107\n",
      "Epoch 3856, Loss: 0.07335619628429413, Final Batch Loss: 0.04207468032836914\n",
      "Epoch 3857, Loss: 0.061256070621311665, Final Batch Loss: 0.007596631534397602\n",
      "Epoch 3858, Loss: 0.051149306586012244, Final Batch Loss: 0.003836551448330283\n",
      "Epoch 3859, Loss: 0.03391595557332039, Final Batch Loss: 0.015585670247673988\n",
      "Epoch 3860, Loss: 0.016792569076642394, Final Batch Loss: 0.0015046324115246534\n",
      "Epoch 3861, Loss: 0.05056333728134632, Final Batch Loss: 0.04312644898891449\n",
      "Epoch 3862, Loss: 0.037370385602116585, Final Batch Loss: 0.009941570460796356\n",
      "Epoch 3863, Loss: 0.010674059391021729, Final Batch Loss: 0.004735364578664303\n",
      "Epoch 3864, Loss: 0.08316130936145782, Final Batch Loss: 0.06117401644587517\n",
      "Epoch 3865, Loss: 0.06763013824820518, Final Batch Loss: 0.04523053392767906\n",
      "Epoch 3866, Loss: 0.02238941192626953, Final Batch Loss: 0.013450655154883862\n",
      "Epoch 3867, Loss: 0.030439311638474464, Final Batch Loss: 0.0168535728007555\n",
      "Epoch 3868, Loss: 0.08847278822213411, Final Batch Loss: 0.004527662880718708\n",
      "Epoch 3869, Loss: 0.01613584766164422, Final Batch Loss: 0.009528419934213161\n",
      "Epoch 3870, Loss: 0.018371418584138155, Final Batch Loss: 0.010952552780508995\n",
      "Epoch 3871, Loss: 0.020454491022974253, Final Batch Loss: 0.005364913959056139\n",
      "Epoch 3872, Loss: 0.012874000705778599, Final Batch Loss: 0.008768419735133648\n",
      "Epoch 3873, Loss: 0.06506789196282625, Final Batch Loss: 0.05623568221926689\n",
      "Epoch 3874, Loss: 0.021185592282563448, Final Batch Loss: 0.006358301732689142\n",
      "Epoch 3875, Loss: 0.04610099829733372, Final Batch Loss: 0.019234051927924156\n",
      "Epoch 3876, Loss: 0.038976953364908695, Final Batch Loss: 0.027511540800333023\n",
      "Epoch 3877, Loss: 0.018749321810901165, Final Batch Loss: 0.008709833957254887\n",
      "Epoch 3878, Loss: 0.06092474330216646, Final Batch Loss: 0.00682948250323534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3879, Loss: 0.027213597670197487, Final Batch Loss: 0.017847619950771332\n",
      "Epoch 3880, Loss: 0.03310801833868027, Final Batch Loss: 0.009294705465435982\n",
      "Epoch 3881, Loss: 0.035014173947274685, Final Batch Loss: 0.01247543003410101\n",
      "Epoch 3882, Loss: 0.061188685707747936, Final Batch Loss: 0.013571667484939098\n",
      "Epoch 3883, Loss: 0.01300665270537138, Final Batch Loss: 0.004432646557688713\n",
      "Epoch 3884, Loss: 0.05484498105943203, Final Batch Loss: 0.04120377451181412\n",
      "Epoch 3885, Loss: 0.013334141112864017, Final Batch Loss: 0.007394461426883936\n",
      "Epoch 3886, Loss: 0.026675314642488956, Final Batch Loss: 0.00969568733125925\n",
      "Epoch 3887, Loss: 0.014477581717073917, Final Batch Loss: 0.006255875341594219\n",
      "Epoch 3888, Loss: 0.030244584195315838, Final Batch Loss: 0.010314865969121456\n",
      "Epoch 3889, Loss: 0.01339843450114131, Final Batch Loss: 0.004643555264919996\n",
      "Epoch 3890, Loss: 0.019976514857262373, Final Batch Loss: 0.00739543279632926\n",
      "Epoch 3891, Loss: 0.008145598461851478, Final Batch Loss: 0.002922149607911706\n",
      "Epoch 3892, Loss: 0.03364258911460638, Final Batch Loss: 0.003339220769703388\n",
      "Epoch 3893, Loss: 0.0413366612046957, Final Batch Loss: 0.02036534994840622\n",
      "Epoch 3894, Loss: 0.026955386623740196, Final Batch Loss: 0.010825974866747856\n",
      "Epoch 3895, Loss: 0.027763379737734795, Final Batch Loss: 0.015794742852449417\n",
      "Epoch 3896, Loss: 0.01937427558004856, Final Batch Loss: 0.012867974117398262\n",
      "Epoch 3897, Loss: 0.020091097801923752, Final Batch Loss: 0.0015726648271083832\n",
      "Epoch 3898, Loss: 0.021852958016097546, Final Batch Loss: 0.0031238244846463203\n",
      "Epoch 3899, Loss: 0.028206744696944952, Final Batch Loss: 0.0053594051860272884\n",
      "Epoch 3900, Loss: 0.025929813273251057, Final Batch Loss: 0.00621387641876936\n",
      "Epoch 3901, Loss: 0.03852429334074259, Final Batch Loss: 0.030241066589951515\n",
      "Epoch 3902, Loss: 0.04466881463304162, Final Batch Loss: 0.007128580007702112\n",
      "Epoch 3903, Loss: 0.03211166989058256, Final Batch Loss: 0.009591509588062763\n",
      "Epoch 3904, Loss: 0.01841597445309162, Final Batch Loss: 0.010431930422782898\n",
      "Epoch 3905, Loss: 0.031928474083542824, Final Batch Loss: 0.015895701944828033\n",
      "Epoch 3906, Loss: 0.019602781161665916, Final Batch Loss: 0.010653132572770119\n",
      "Epoch 3907, Loss: 0.02275304961949587, Final Batch Loss: 0.009174447506666183\n",
      "Epoch 3908, Loss: 0.040548815624788404, Final Batch Loss: 0.0028901316691190004\n",
      "Epoch 3909, Loss: 0.03509536664932966, Final Batch Loss: 0.01105636078864336\n",
      "Epoch 3910, Loss: 0.025188908679410815, Final Batch Loss: 0.002706055296584964\n",
      "Epoch 3911, Loss: 0.021529533434659243, Final Batch Loss: 0.0026809857226908207\n",
      "Epoch 3912, Loss: 0.012513067107647657, Final Batch Loss: 0.0059259384870529175\n",
      "Epoch 3913, Loss: 0.054653274826705456, Final Batch Loss: 0.013355947099626064\n",
      "Epoch 3914, Loss: 0.0821906398050487, Final Batch Loss: 0.07513868808746338\n",
      "Epoch 3915, Loss: 0.016859788447618484, Final Batch Loss: 0.007174268364906311\n",
      "Epoch 3916, Loss: 0.015822067856788635, Final Batch Loss: 0.012850257568061352\n",
      "Epoch 3917, Loss: 0.032230157405138016, Final Batch Loss: 0.02630515955388546\n",
      "Epoch 3918, Loss: 0.03662527725100517, Final Batch Loss: 0.026590630412101746\n",
      "Epoch 3919, Loss: 0.023230882361531258, Final Batch Loss: 0.012389631010591984\n",
      "Epoch 3920, Loss: 0.011343958554789424, Final Batch Loss: 0.00175340729765594\n",
      "Epoch 3921, Loss: 0.026538411155343056, Final Batch Loss: 0.005274051800370216\n",
      "Epoch 3922, Loss: 0.03510176111012697, Final Batch Loss: 0.006826062686741352\n",
      "Epoch 3923, Loss: 0.056916228495538235, Final Batch Loss: 0.04521835595369339\n",
      "Epoch 3924, Loss: 0.03798341564834118, Final Batch Loss: 0.01747886836528778\n",
      "Epoch 3925, Loss: 0.02788390964269638, Final Batch Loss: 0.01553348172456026\n",
      "Epoch 3926, Loss: 0.195212472230196, Final Batch Loss: 0.16446952521800995\n",
      "Epoch 3927, Loss: 0.030128291342407465, Final Batch Loss: 0.022597825154662132\n",
      "Epoch 3928, Loss: 0.027287269942462444, Final Batch Loss: 0.010347080416977406\n",
      "Epoch 3929, Loss: 0.01834496995434165, Final Batch Loss: 0.004903510678559542\n",
      "Epoch 3930, Loss: 0.1298550684005022, Final Batch Loss: 0.11673858761787415\n",
      "Epoch 3931, Loss: 0.03311463026329875, Final Batch Loss: 0.0061769853346049786\n",
      "Epoch 3932, Loss: 0.026563770603388548, Final Batch Loss: 0.0064171976409852505\n",
      "Epoch 3933, Loss: 0.013146277982741594, Final Batch Loss: 0.0064833080396056175\n",
      "Epoch 3934, Loss: 0.05044415593147278, Final Batch Loss: 0.022740334272384644\n",
      "Epoch 3935, Loss: 0.04225362744182348, Final Batch Loss: 0.03476967662572861\n",
      "Epoch 3936, Loss: 0.04483515955507755, Final Batch Loss: 0.013122456148266792\n",
      "Epoch 3937, Loss: 0.01818251796066761, Final Batch Loss: 0.0017597973346710205\n",
      "Epoch 3938, Loss: 0.02617567591369152, Final Batch Loss: 0.007299585267901421\n",
      "Epoch 3939, Loss: 0.05338184628635645, Final Batch Loss: 0.03860050439834595\n",
      "Epoch 3940, Loss: 0.0207821992225945, Final Batch Loss: 0.015198646113276482\n",
      "Epoch 3941, Loss: 0.03762924764305353, Final Batch Loss: 0.030725611373782158\n",
      "Epoch 3942, Loss: 0.01649649115279317, Final Batch Loss: 0.004120963159948587\n",
      "Epoch 3943, Loss: 0.05403955839574337, Final Batch Loss: 0.03630095720291138\n",
      "Epoch 3944, Loss: 0.08806446567177773, Final Batch Loss: 0.012323614209890366\n",
      "Epoch 3945, Loss: 0.020831787958741188, Final Batch Loss: 0.003988804295659065\n",
      "Epoch 3946, Loss: 0.03256141021847725, Final Batch Loss: 0.011874057352542877\n",
      "Epoch 3947, Loss: 0.011534351157024503, Final Batch Loss: 0.0012666017282754183\n",
      "Epoch 3948, Loss: 0.03499456308782101, Final Batch Loss: 0.010445110499858856\n",
      "Epoch 3949, Loss: 0.017608190653845668, Final Batch Loss: 0.002398906974121928\n",
      "Epoch 3950, Loss: 0.025259567890316248, Final Batch Loss: 0.004269918892532587\n",
      "Epoch 3951, Loss: 0.043143291026353836, Final Batch Loss: 0.0275051798671484\n",
      "Epoch 3952, Loss: 0.030086232349276543, Final Batch Loss: 0.006587162613868713\n",
      "Epoch 3953, Loss: 0.0239875391125679, Final Batch Loss: 0.006159801036119461\n",
      "Epoch 3954, Loss: 0.029119026847183704, Final Batch Loss: 0.017179999500513077\n",
      "Epoch 3955, Loss: 0.049601840786635876, Final Batch Loss: 0.0388505645096302\n",
      "Epoch 3956, Loss: 0.029081592336297035, Final Batch Loss: 0.01643083058297634\n",
      "Epoch 3957, Loss: 0.015452975407242775, Final Batch Loss: 0.006334438920021057\n",
      "Epoch 3958, Loss: 0.08888089749962091, Final Batch Loss: 0.07384367287158966\n",
      "Epoch 3959, Loss: 0.0311877247877419, Final Batch Loss: 0.023461012169718742\n",
      "Epoch 3960, Loss: 0.025290051475167274, Final Batch Loss: 0.013263221830129623\n",
      "Epoch 3961, Loss: 0.017349645495414734, Final Batch Loss: 0.007399151101708412\n",
      "Epoch 3962, Loss: 0.06407488975673914, Final Batch Loss: 0.012505077756941319\n",
      "Epoch 3963, Loss: 0.026736514642834663, Final Batch Loss: 0.008992934599518776\n",
      "Epoch 3964, Loss: 0.034608667716383934, Final Batch Loss: 0.01601482182741165\n",
      "Epoch 3965, Loss: 0.011068035149946809, Final Batch Loss: 0.003414819249883294\n",
      "Epoch 3966, Loss: 0.0139225865714252, Final Batch Loss: 0.008405226282775402\n",
      "Epoch 3967, Loss: 0.011995578650385141, Final Batch Loss: 0.0025968938134610653\n",
      "Epoch 3968, Loss: 0.032056644558906555, Final Batch Loss: 0.016319094225764275\n",
      "Epoch 3969, Loss: 0.022181138396263123, Final Batch Loss: 0.01213739812374115\n",
      "Epoch 3970, Loss: 0.0612337701022625, Final Batch Loss: 0.021115999668836594\n",
      "Epoch 3971, Loss: 0.01452436437830329, Final Batch Loss: 0.004815352614969015\n",
      "Epoch 3972, Loss: 0.02271040342748165, Final Batch Loss: 0.005695845931768417\n",
      "Epoch 3973, Loss: 0.01883367821574211, Final Batch Loss: 0.009222043678164482\n",
      "Epoch 3974, Loss: 0.014985548332333565, Final Batch Loss: 0.004722089506685734\n",
      "Epoch 3975, Loss: 0.014860463794320822, Final Batch Loss: 0.006174346897751093\n",
      "Epoch 3976, Loss: 0.015521134017035365, Final Batch Loss: 0.00204372382722795\n",
      "Epoch 3977, Loss: 0.011092283297330141, Final Batch Loss: 0.005043264478445053\n",
      "Epoch 3978, Loss: 0.014176420168951154, Final Batch Loss: 0.002707517007365823\n",
      "Epoch 3979, Loss: 0.02309628576040268, Final Batch Loss: 0.002812124788761139\n",
      "Epoch 3980, Loss: 0.03462275676429272, Final Batch Loss: 0.027433302253484726\n",
      "Epoch 3981, Loss: 0.018795992247760296, Final Batch Loss: 0.006112563423812389\n",
      "Epoch 3982, Loss: 0.021340713719837368, Final Batch Loss: 0.0014907036675140262\n",
      "Epoch 3983, Loss: 0.043187934905290604, Final Batch Loss: 0.008123256266117096\n",
      "Epoch 3984, Loss: 0.06446556560695171, Final Batch Loss: 0.021469244733452797\n",
      "Epoch 3985, Loss: 0.0161660045851022, Final Batch Loss: 0.0036715876776725054\n",
      "Epoch 3986, Loss: 0.03216430451720953, Final Batch Loss: 0.008959189988672733\n",
      "Epoch 3987, Loss: 0.0461013475432992, Final Batch Loss: 0.037151917815208435\n",
      "Epoch 3988, Loss: 0.018663913011550903, Final Batch Loss: 0.005020340904593468\n",
      "Epoch 3989, Loss: 0.04073882009834051, Final Batch Loss: 0.02748725563287735\n",
      "Epoch 3990, Loss: 0.017252556164748967, Final Batch Loss: 0.0014145347522571683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3991, Loss: 0.01481290627270937, Final Batch Loss: 0.0016082236543297768\n",
      "Epoch 3992, Loss: 0.014326749369502068, Final Batch Loss: 0.003149673342704773\n",
      "Epoch 3993, Loss: 0.04260116629302502, Final Batch Loss: 0.02606741152703762\n",
      "Epoch 3994, Loss: 0.030690127052366734, Final Batch Loss: 0.017939627170562744\n",
      "Epoch 3995, Loss: 0.02834659325890243, Final Batch Loss: 0.0020017053466290236\n",
      "Epoch 3996, Loss: 0.01573326939251274, Final Batch Loss: 0.0015114446869120002\n",
      "Epoch 3997, Loss: 0.01089072646573186, Final Batch Loss: 0.005254155956208706\n",
      "Epoch 3998, Loss: 0.014955532737076283, Final Batch Loss: 0.0032161036506295204\n",
      "Epoch 3999, Loss: 0.012162649538367987, Final Batch Loss: 0.007099498063325882\n",
      "Epoch 4000, Loss: 0.02232482749968767, Final Batch Loss: 0.009789618663489819\n",
      "Epoch 4001, Loss: 0.021232369355857372, Final Batch Loss: 0.011765655130147934\n",
      "Epoch 4002, Loss: 0.04742814972996712, Final Batch Loss: 0.03316720947623253\n",
      "Epoch 4003, Loss: 0.010388093767687678, Final Batch Loss: 0.0033902942668646574\n",
      "Epoch 4004, Loss: 0.014219551347196102, Final Batch Loss: 0.00439410749822855\n",
      "Epoch 4005, Loss: 0.02327105263248086, Final Batch Loss: 0.0066259135492146015\n",
      "Epoch 4006, Loss: 0.018088664393872023, Final Batch Loss: 0.007495142985135317\n",
      "Epoch 4007, Loss: 0.031001180410385132, Final Batch Loss: 0.010192489251494408\n",
      "Epoch 4008, Loss: 0.04304460063576698, Final Batch Loss: 0.008816391229629517\n",
      "Epoch 4009, Loss: 0.03789665922522545, Final Batch Loss: 0.019228478893637657\n",
      "Epoch 4010, Loss: 0.06820815382525325, Final Batch Loss: 0.06222802773118019\n",
      "Epoch 4011, Loss: 0.0585938673466444, Final Batch Loss: 0.049946993589401245\n",
      "Epoch 4012, Loss: 0.008558309869840741, Final Batch Loss: 0.0022422324400395155\n",
      "Epoch 4013, Loss: 0.03983914386481047, Final Batch Loss: 0.026074163615703583\n",
      "Epoch 4014, Loss: 0.06445341370999813, Final Batch Loss: 0.043428581207990646\n",
      "Epoch 4015, Loss: 0.017659201752394438, Final Batch Loss: 0.002230572048574686\n",
      "Epoch 4016, Loss: 0.03377452492713928, Final Batch Loss: 0.018197907134890556\n",
      "Epoch 4017, Loss: 0.00955341407097876, Final Batch Loss: 0.0029210264328867197\n",
      "Epoch 4018, Loss: 0.037664370611310005, Final Batch Loss: 0.02584432251751423\n",
      "Epoch 4019, Loss: 0.04030773416161537, Final Batch Loss: 0.021497249603271484\n",
      "Epoch 4020, Loss: 0.055719733238220215, Final Batch Loss: 0.031247086822986603\n",
      "Epoch 4021, Loss: 0.019342687912285328, Final Batch Loss: 0.00904216431081295\n",
      "Epoch 4022, Loss: 0.024273437447845936, Final Batch Loss: 0.0023796716704964638\n",
      "Epoch 4023, Loss: 0.04315977357327938, Final Batch Loss: 0.026030950248241425\n",
      "Epoch 4024, Loss: 0.015313054667785764, Final Batch Loss: 0.0035126369912177324\n",
      "Epoch 4025, Loss: 0.011619432363659143, Final Batch Loss: 0.003850786481052637\n",
      "Epoch 4026, Loss: 0.025901968590915203, Final Batch Loss: 0.01908859796822071\n",
      "Epoch 4027, Loss: 0.023473041132092476, Final Batch Loss: 0.01210890430957079\n",
      "Epoch 4028, Loss: 0.04763804096728563, Final Batch Loss: 0.007615533657371998\n",
      "Epoch 4029, Loss: 0.021230642683804035, Final Batch Loss: 0.013532853685319424\n",
      "Epoch 4030, Loss: 0.04235642496496439, Final Batch Loss: 0.032972387969493866\n",
      "Epoch 4031, Loss: 0.017286918126046658, Final Batch Loss: 0.009844658896327019\n",
      "Epoch 4032, Loss: 0.018968532793223858, Final Batch Loss: 0.004286601208150387\n",
      "Epoch 4033, Loss: 0.02465724665671587, Final Batch Loss: 0.010948657058179379\n",
      "Epoch 4034, Loss: 0.02604093588888645, Final Batch Loss: 0.00916271097958088\n",
      "Epoch 4035, Loss: 0.030012238770723343, Final Batch Loss: 0.01003061793744564\n",
      "Epoch 4036, Loss: 0.024579332675784826, Final Batch Loss: 0.018716195598244667\n",
      "Epoch 4037, Loss: 0.011191432713530958, Final Batch Loss: 0.0018425587331876159\n",
      "Epoch 4038, Loss: 0.012854228960350156, Final Batch Loss: 0.003192154923453927\n",
      "Epoch 4039, Loss: 0.02459513023495674, Final Batch Loss: 0.0039531998336315155\n",
      "Epoch 4040, Loss: 0.013585754204541445, Final Batch Loss: 0.006215157452970743\n",
      "Epoch 4041, Loss: 0.047535291872918606, Final Batch Loss: 0.0397062711417675\n",
      "Epoch 4042, Loss: 0.08031364530324936, Final Batch Loss: 0.03534402325749397\n",
      "Epoch 4043, Loss: 0.03594216424971819, Final Batch Loss: 0.02100995182991028\n",
      "Epoch 4044, Loss: 0.013277948484756052, Final Batch Loss: 0.0018622573697939515\n",
      "Epoch 4045, Loss: 0.01680253865197301, Final Batch Loss: 0.01124652475118637\n",
      "Epoch 4046, Loss: 0.038805004209280014, Final Batch Loss: 0.020309921354055405\n",
      "Epoch 4047, Loss: 0.026877030497416854, Final Batch Loss: 0.023044932633638382\n",
      "Epoch 4048, Loss: 0.014135733479633927, Final Batch Loss: 0.0026103819254785776\n",
      "Epoch 4049, Loss: 0.015720587456598878, Final Batch Loss: 0.0035105759743601084\n",
      "Epoch 4050, Loss: 0.0294018283020705, Final Batch Loss: 0.0020263760816305876\n",
      "Epoch 4051, Loss: 0.03134354576468468, Final Batch Loss: 0.011761896312236786\n",
      "Epoch 4052, Loss: 0.050804320722818375, Final Batch Loss: 0.022382374852895737\n",
      "Epoch 4053, Loss: 0.013962578028440475, Final Batch Loss: 0.00835130549967289\n",
      "Epoch 4054, Loss: 0.014894216321408749, Final Batch Loss: 0.00902953464537859\n",
      "Epoch 4055, Loss: 0.02607414871454239, Final Batch Loss: 0.009732572361826897\n",
      "Epoch 4056, Loss: 0.015852367039769888, Final Batch Loss: 0.010203707031905651\n",
      "Epoch 4057, Loss: 0.018613258376717567, Final Batch Loss: 0.0025329217314720154\n",
      "Epoch 4058, Loss: 0.015789824537932873, Final Batch Loss: 0.008218308910727501\n",
      "Epoch 4059, Loss: 0.0435742293484509, Final Batch Loss: 0.03893321380019188\n",
      "Epoch 4060, Loss: 0.013935928232967854, Final Batch Loss: 0.008224043995141983\n",
      "Epoch 4061, Loss: 0.02974922675639391, Final Batch Loss: 0.009221802465617657\n",
      "Epoch 4062, Loss: 0.10192524455487728, Final Batch Loss: 0.09165111929178238\n",
      "Epoch 4063, Loss: 0.03601934015750885, Final Batch Loss: 0.011909868568181992\n",
      "Epoch 4064, Loss: 0.01528623339254409, Final Batch Loss: 0.0015594401629641652\n",
      "Epoch 4065, Loss: 0.02520235860720277, Final Batch Loss: 0.005560501012951136\n",
      "Epoch 4066, Loss: 0.03295720648020506, Final Batch Loss: 0.028252705931663513\n",
      "Epoch 4067, Loss: 0.09478344768285751, Final Batch Loss: 0.05725766718387604\n",
      "Epoch 4068, Loss: 0.041134278289973736, Final Batch Loss: 0.008061706088483334\n",
      "Epoch 4069, Loss: 0.012502307072281837, Final Batch Loss: 0.005493733566254377\n",
      "Epoch 4070, Loss: 0.008829777594655752, Final Batch Loss: 0.004171030130237341\n",
      "Epoch 4071, Loss: 0.11423166748136282, Final Batch Loss: 0.10266133397817612\n",
      "Epoch 4072, Loss: 0.017607763409614563, Final Batch Loss: 0.006156912073493004\n",
      "Epoch 4073, Loss: 0.008370001800358295, Final Batch Loss: 0.002203261014074087\n",
      "Epoch 4074, Loss: 0.015255180653184652, Final Batch Loss: 0.007729735691100359\n",
      "Epoch 4075, Loss: 0.03521688375622034, Final Batch Loss: 0.004680127836763859\n",
      "Epoch 4076, Loss: 0.03620028682053089, Final Batch Loss: 0.009813640266656876\n",
      "Epoch 4077, Loss: 0.06837744824588299, Final Batch Loss: 0.022389540448784828\n",
      "Epoch 4078, Loss: 0.013635685667395592, Final Batch Loss: 0.0042991312220692635\n",
      "Epoch 4079, Loss: 0.050584218464791775, Final Batch Loss: 0.03862797096371651\n",
      "Epoch 4080, Loss: 0.017281516920775175, Final Batch Loss: 0.003729111049324274\n",
      "Epoch 4081, Loss: 0.026869025081396103, Final Batch Loss: 0.012033474631607533\n",
      "Epoch 4082, Loss: 0.025175321847200394, Final Batch Loss: 0.00487883947789669\n",
      "Epoch 4083, Loss: 0.0373424127465114, Final Batch Loss: 0.0014709181850776076\n",
      "Epoch 4084, Loss: 0.03912010882049799, Final Batch Loss: 0.012239363975822926\n",
      "Epoch 4085, Loss: 0.04276263061910868, Final Batch Loss: 0.004757271148264408\n",
      "Epoch 4086, Loss: 0.021430291701108217, Final Batch Loss: 0.0027585686184465885\n",
      "Epoch 4087, Loss: 0.012183044105768204, Final Batch Loss: 0.0065004522912204266\n",
      "Epoch 4088, Loss: 0.050904362462460995, Final Batch Loss: 0.036344822496175766\n",
      "Epoch 4089, Loss: 0.009954852983355522, Final Batch Loss: 0.0036745299585163593\n",
      "Epoch 4090, Loss: 0.01648106286302209, Final Batch Loss: 0.008749458007514477\n",
      "Epoch 4091, Loss: 0.02707604318857193, Final Batch Loss: 0.01798761449754238\n",
      "Epoch 4092, Loss: 0.012863325886428356, Final Batch Loss: 0.007356632500886917\n",
      "Epoch 4093, Loss: 0.06316249631345272, Final Batch Loss: 0.019421497359871864\n",
      "Epoch 4094, Loss: 0.022099994122982025, Final Batch Loss: 0.008846760727465153\n",
      "Epoch 4095, Loss: 0.012994349468499422, Final Batch Loss: 0.0046171280555427074\n",
      "Epoch 4096, Loss: 0.019260795321315527, Final Batch Loss: 0.01245332881808281\n",
      "Epoch 4097, Loss: 0.08355705440044403, Final Batch Loss: 0.039077453315258026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4098, Loss: 0.042506867088377476, Final Batch Loss: 0.010552381165325642\n",
      "Epoch 4099, Loss: 0.011144385440275073, Final Batch Loss: 0.003898870898410678\n",
      "Epoch 4100, Loss: 0.015186471864581108, Final Batch Loss: 0.0036109481006860733\n",
      "Epoch 4101, Loss: 0.019818004220724106, Final Batch Loss: 0.006769981235265732\n",
      "Epoch 4102, Loss: 0.07027405966073275, Final Batch Loss: 0.009756305254995823\n",
      "Epoch 4103, Loss: 0.029993997886776924, Final Batch Loss: 0.016523702070116997\n",
      "Epoch 4104, Loss: 0.02207967988215387, Final Batch Loss: 0.003105232259258628\n",
      "Epoch 4105, Loss: 0.050564393401145935, Final Batch Loss: 0.03984029218554497\n",
      "Epoch 4106, Loss: 0.009652563137933612, Final Batch Loss: 0.0019749037455767393\n",
      "Epoch 4107, Loss: 0.02689806930720806, Final Batch Loss: 0.004912296310067177\n",
      "Epoch 4108, Loss: 0.011257278267294168, Final Batch Loss: 0.003718036226928234\n",
      "Epoch 4109, Loss: 0.01906414213590324, Final Batch Loss: 0.00137937325052917\n",
      "Epoch 4110, Loss: 0.012027631979435682, Final Batch Loss: 0.006459547206759453\n",
      "Epoch 4111, Loss: 0.043187396600842476, Final Batch Loss: 0.017016708850860596\n",
      "Epoch 4112, Loss: 0.07049005478620529, Final Batch Loss: 0.007719635963439941\n",
      "Epoch 4113, Loss: 0.03881876450031996, Final Batch Loss: 0.03131377324461937\n",
      "Epoch 4114, Loss: 0.04251674562692642, Final Batch Loss: 0.02364145964384079\n",
      "Epoch 4115, Loss: 0.03508879942819476, Final Batch Loss: 0.0050314138643443584\n",
      "Epoch 4116, Loss: 0.04444790072739124, Final Batch Loss: 0.027885878458619118\n",
      "Epoch 4117, Loss: 0.007248047273606062, Final Batch Loss: 0.0034925476647913456\n",
      "Epoch 4118, Loss: 0.030907859560102224, Final Batch Loss: 0.002057794015854597\n",
      "Epoch 4119, Loss: 0.037798802368342876, Final Batch Loss: 0.01309563685208559\n",
      "Epoch 4120, Loss: 0.021977487485855818, Final Batch Loss: 0.017340099439024925\n",
      "Epoch 4121, Loss: 0.047780757769942284, Final Batch Loss: 0.02552337944507599\n",
      "Epoch 4122, Loss: 0.015551477437838912, Final Batch Loss: 0.0028382146265357733\n",
      "Epoch 4123, Loss: 0.051888322457671165, Final Batch Loss: 0.04493696987628937\n",
      "Epoch 4124, Loss: 0.0837205620482564, Final Batch Loss: 0.06955219060182571\n",
      "Epoch 4125, Loss: 0.014590574894100428, Final Batch Loss: 0.005987279582768679\n",
      "Epoch 4126, Loss: 0.009476305916905403, Final Batch Loss: 0.0037010894156992435\n",
      "Epoch 4127, Loss: 0.013085999060422182, Final Batch Loss: 0.004613947588950396\n",
      "Epoch 4128, Loss: 0.0402276162058115, Final Batch Loss: 0.022524239495396614\n",
      "Epoch 4129, Loss: 0.024292558431625366, Final Batch Loss: 0.014398313127458096\n",
      "Epoch 4130, Loss: 0.01252999622374773, Final Batch Loss: 0.005569166038185358\n",
      "Epoch 4131, Loss: 0.05246657319366932, Final Batch Loss: 0.017328565940260887\n",
      "Epoch 4132, Loss: 0.0138161676004529, Final Batch Loss: 0.0029160715639591217\n",
      "Epoch 4133, Loss: 0.023867871146649122, Final Batch Loss: 0.005185141693800688\n",
      "Epoch 4134, Loss: 0.04405011050403118, Final Batch Loss: 0.02897244691848755\n",
      "Epoch 4135, Loss: 0.027311714366078377, Final Batch Loss: 0.01240617036819458\n",
      "Epoch 4136, Loss: 0.0555384149774909, Final Batch Loss: 0.006334683857858181\n",
      "Epoch 4137, Loss: 0.03068894986063242, Final Batch Loss: 0.022753693163394928\n",
      "Epoch 4138, Loss: 0.028670229483395815, Final Batch Loss: 0.005967386532574892\n",
      "Epoch 4139, Loss: 0.03869859129190445, Final Batch Loss: 0.021918784826993942\n",
      "Epoch 4140, Loss: 0.01961751375347376, Final Batch Loss: 0.010739842429757118\n",
      "Epoch 4141, Loss: 0.016169120790436864, Final Batch Loss: 0.003277688520029187\n",
      "Epoch 4142, Loss: 0.016627545235678554, Final Batch Loss: 0.0025959794875234365\n",
      "Epoch 4143, Loss: 0.011703082825988531, Final Batch Loss: 0.005286298226565123\n",
      "Epoch 4144, Loss: 0.02457836619578302, Final Batch Loss: 0.002641817322000861\n",
      "Epoch 4145, Loss: 0.011175862047821283, Final Batch Loss: 0.0038397167809307575\n",
      "Epoch 4146, Loss: 0.022866252809762955, Final Batch Loss: 0.002874048426747322\n",
      "Epoch 4147, Loss: 0.014947013929486275, Final Batch Loss: 0.00980225671082735\n",
      "Epoch 4148, Loss: 0.011734768049791455, Final Batch Loss: 0.00234398921020329\n",
      "Epoch 4149, Loss: 0.009224411565810442, Final Batch Loss: 0.0021234932355582714\n",
      "Epoch 4150, Loss: 0.032760932110249996, Final Batch Loss: 0.007064583711326122\n",
      "Epoch 4151, Loss: 0.0395079767331481, Final Batch Loss: 0.005068696103990078\n",
      "Epoch 4152, Loss: 0.052481016144156456, Final Batch Loss: 0.033462006598711014\n",
      "Epoch 4153, Loss: 0.009148194454610348, Final Batch Loss: 0.004543110262602568\n",
      "Epoch 4154, Loss: 0.013495465274900198, Final Batch Loss: 0.00718431081622839\n",
      "Epoch 4155, Loss: 0.027193363290280104, Final Batch Loss: 0.02097790129482746\n",
      "Epoch 4156, Loss: 0.059927405789494514, Final Batch Loss: 0.048616982996463776\n",
      "Epoch 4157, Loss: 0.022681869100779295, Final Batch Loss: 0.006217300426214933\n",
      "Epoch 4158, Loss: 0.027386841364204884, Final Batch Loss: 0.017966685816645622\n",
      "Epoch 4159, Loss: 0.020881902892142534, Final Batch Loss: 0.014628931879997253\n",
      "Epoch 4160, Loss: 0.030803574016317725, Final Batch Loss: 0.003064576303586364\n",
      "Epoch 4161, Loss: 0.01089414395391941, Final Batch Loss: 0.00614117830991745\n",
      "Epoch 4162, Loss: 0.02733646333217621, Final Batch Loss: 0.002554507926106453\n",
      "Epoch 4163, Loss: 0.012276808731257915, Final Batch Loss: 0.006444012746214867\n",
      "Epoch 4164, Loss: 0.02476427610963583, Final Batch Loss: 0.010731509886682034\n",
      "Epoch 4165, Loss: 0.06109698070213199, Final Batch Loss: 0.05873090773820877\n",
      "Epoch 4166, Loss: 0.009356532478705049, Final Batch Loss: 0.0029709881637245417\n",
      "Epoch 4167, Loss: 0.028515968471765518, Final Batch Loss: 0.0034581609070301056\n",
      "Epoch 4168, Loss: 0.02640014886856079, Final Batch Loss: 0.010668594390153885\n",
      "Epoch 4169, Loss: 0.02274783980101347, Final Batch Loss: 0.017727743834257126\n",
      "Epoch 4170, Loss: 0.09654933866113424, Final Batch Loss: 0.08484943211078644\n",
      "Epoch 4171, Loss: 0.010766371618956327, Final Batch Loss: 0.0030293739400804043\n",
      "Epoch 4172, Loss: 0.010306954849511385, Final Batch Loss: 0.003956950269639492\n",
      "Epoch 4173, Loss: 0.02150063496083021, Final Batch Loss: 0.010524233803153038\n",
      "Epoch 4174, Loss: 0.07819893071427941, Final Batch Loss: 0.07152358442544937\n",
      "Epoch 4175, Loss: 0.021740746218711138, Final Batch Loss: 0.00330642843618989\n",
      "Epoch 4176, Loss: 0.023110724985599518, Final Batch Loss: 0.007883944548666477\n",
      "Epoch 4177, Loss: 0.02665921114385128, Final Batch Loss: 0.004891129210591316\n",
      "Epoch 4178, Loss: 0.010068517178297043, Final Batch Loss: 0.005488214083015919\n",
      "Epoch 4179, Loss: 0.0674765557050705, Final Batch Loss: 0.008799605071544647\n",
      "Epoch 4180, Loss: 0.015785349532961845, Final Batch Loss: 0.003917902708053589\n",
      "Epoch 4181, Loss: 0.016863258555531502, Final Batch Loss: 0.007867718115448952\n",
      "Epoch 4182, Loss: 0.04660455696284771, Final Batch Loss: 0.03070688433945179\n",
      "Epoch 4183, Loss: 0.022103657014667988, Final Batch Loss: 0.013131451793015003\n",
      "Epoch 4184, Loss: 0.043951038271188736, Final Batch Loss: 0.011822398751974106\n",
      "Epoch 4185, Loss: 0.05276501737535, Final Batch Loss: 0.008611219003796577\n",
      "Epoch 4186, Loss: 0.05817832238972187, Final Batch Loss: 0.04587496072053909\n",
      "Epoch 4187, Loss: 0.031838386319577694, Final Batch Loss: 0.025582564994692802\n",
      "Epoch 4188, Loss: 0.03739171475172043, Final Batch Loss: 0.022961625829339027\n",
      "Epoch 4189, Loss: 0.020487419329583645, Final Batch Loss: 0.01137846801429987\n",
      "Epoch 4190, Loss: 0.027396446093916893, Final Batch Loss: 0.020503122359514236\n",
      "Epoch 4191, Loss: 0.029874606523662806, Final Batch Loss: 0.023829011246562004\n",
      "Epoch 4192, Loss: 0.016326180659234524, Final Batch Loss: 0.0062721893191337585\n",
      "Epoch 4193, Loss: 0.017444893717765808, Final Batch Loss: 0.012604150921106339\n",
      "Epoch 4194, Loss: 0.01938807498663664, Final Batch Loss: 0.011554194614291191\n",
      "Epoch 4195, Loss: 0.025959758087992668, Final Batch Loss: 0.014041101559996605\n",
      "Epoch 4196, Loss: 0.016566818580031395, Final Batch Loss: 0.004363631829619408\n",
      "Epoch 4197, Loss: 0.023257650900632143, Final Batch Loss: 0.007686393801122904\n",
      "Epoch 4198, Loss: 0.017902527004480362, Final Batch Loss: 0.0043891435489058495\n",
      "Epoch 4199, Loss: 0.04758020397275686, Final Batch Loss: 0.01242180448025465\n",
      "Epoch 4200, Loss: 0.09207882359623909, Final Batch Loss: 0.049296099692583084\n",
      "Epoch 4201, Loss: 0.06965869292616844, Final Batch Loss: 0.03771740198135376\n",
      "Epoch 4202, Loss: 0.10706151276826859, Final Batch Loss: 0.09798789024353027\n",
      "Epoch 4203, Loss: 0.017893717624247074, Final Batch Loss: 0.010417056269943714\n",
      "Epoch 4204, Loss: 0.0046882020542398095, Final Batch Loss: 0.0017378312768414617\n",
      "Epoch 4205, Loss: 0.01149739045649767, Final Batch Loss: 0.004954094532877207\n",
      "Epoch 4206, Loss: 0.033172456081956625, Final Batch Loss: 0.025722933933138847\n",
      "Epoch 4207, Loss: 0.031480918638408184, Final Batch Loss: 0.017595024779438972\n",
      "Epoch 4208, Loss: 0.0379900261759758, Final Batch Loss: 0.02960500493645668\n",
      "Epoch 4209, Loss: 0.05202291440218687, Final Batch Loss: 0.010230175219476223\n",
      "Epoch 4210, Loss: 0.01514447433874011, Final Batch Loss: 0.00483689783141017\n",
      "Epoch 4211, Loss: 0.02226648386567831, Final Batch Loss: 0.00884515792131424\n",
      "Epoch 4212, Loss: 0.012975346064195037, Final Batch Loss: 0.0026230632793158293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4213, Loss: 0.01964364293962717, Final Batch Loss: 0.0114234434440732\n",
      "Epoch 4214, Loss: 0.02556557720527053, Final Batch Loss: 0.02012299746274948\n",
      "Epoch 4215, Loss: 0.029155080672353506, Final Batch Loss: 0.022717373445630074\n",
      "Epoch 4216, Loss: 0.022940464317798615, Final Batch Loss: 0.0031609758734703064\n",
      "Epoch 4217, Loss: 0.03065122477710247, Final Batch Loss: 0.012366920709609985\n",
      "Epoch 4218, Loss: 0.0401857728138566, Final Batch Loss: 0.02936299331486225\n",
      "Epoch 4219, Loss: 0.09047029167413712, Final Batch Loss: 0.051107343286275864\n",
      "Epoch 4220, Loss: 0.017791564110666513, Final Batch Loss: 0.007212153170257807\n",
      "Epoch 4221, Loss: 0.02572251297533512, Final Batch Loss: 0.016903361305594444\n",
      "Epoch 4222, Loss: 0.015277386642992496, Final Batch Loss: 0.006246692501008511\n",
      "Epoch 4223, Loss: 0.04074074886739254, Final Batch Loss: 0.024355614557862282\n",
      "Epoch 4224, Loss: 0.016140193212777376, Final Batch Loss: 0.0078062196262180805\n",
      "Epoch 4225, Loss: 0.014427451649680734, Final Batch Loss: 0.003819897072389722\n",
      "Epoch 4226, Loss: 0.013744167983531952, Final Batch Loss: 0.006567677017301321\n",
      "Epoch 4227, Loss: 0.009306522086262703, Final Batch Loss: 0.0025559901259839535\n",
      "Epoch 4228, Loss: 0.01753048785030842, Final Batch Loss: 0.007444639690220356\n",
      "Epoch 4229, Loss: 0.023097415454685688, Final Batch Loss: 0.007185286842286587\n",
      "Epoch 4230, Loss: 0.025729225017130375, Final Batch Loss: 0.014714344404637814\n",
      "Epoch 4231, Loss: 0.010469399858266115, Final Batch Loss: 0.005235021933913231\n",
      "Epoch 4232, Loss: 0.016210895031690598, Final Batch Loss: 0.008931557647883892\n",
      "Epoch 4233, Loss: 0.019496073480695486, Final Batch Loss: 0.012555616907775402\n",
      "Epoch 4234, Loss: 0.03115091286599636, Final Batch Loss: 0.021600810810923576\n",
      "Epoch 4235, Loss: 0.007645179866813123, Final Batch Loss: 0.0013754315441474319\n",
      "Epoch 4236, Loss: 0.019056715071201324, Final Batch Loss: 0.006321694701910019\n",
      "Epoch 4237, Loss: 0.019806227646768093, Final Batch Loss: 0.011982078664004803\n",
      "Epoch 4238, Loss: 0.03725743480026722, Final Batch Loss: 0.024976225569844246\n",
      "Epoch 4239, Loss: 0.006090191891416907, Final Batch Loss: 0.002183399861678481\n",
      "Epoch 4240, Loss: 0.023928622715175152, Final Batch Loss: 0.018213307484984398\n",
      "Epoch 4241, Loss: 0.01802688674069941, Final Batch Loss: 0.0016311875078827143\n",
      "Epoch 4242, Loss: 0.034518332220613956, Final Batch Loss: 0.028827868402004242\n",
      "Epoch 4243, Loss: 0.02088268380612135, Final Batch Loss: 0.005537793040275574\n",
      "Epoch 4244, Loss: 0.02473030937835574, Final Batch Loss: 0.003314028959721327\n",
      "Epoch 4245, Loss: 0.06643624138087034, Final Batch Loss: 0.004678207449615002\n",
      "Epoch 4246, Loss: 0.012025001458823681, Final Batch Loss: 0.003934183157980442\n",
      "Epoch 4247, Loss: 0.011859448626637459, Final Batch Loss: 0.007271021604537964\n",
      "Epoch 4248, Loss: 0.019621609477326274, Final Batch Loss: 0.0017834517639130354\n",
      "Epoch 4249, Loss: 0.04206653218716383, Final Batch Loss: 0.013992690481245518\n",
      "Epoch 4250, Loss: 0.01411793241277337, Final Batch Loss: 0.004338763188570738\n",
      "Epoch 4251, Loss: 0.030380464158952236, Final Batch Loss: 0.015720533207058907\n",
      "Epoch 4252, Loss: 0.0269902553409338, Final Batch Loss: 0.0039138272404670715\n",
      "Epoch 4253, Loss: 0.02664501592516899, Final Batch Loss: 0.013704925775527954\n",
      "Epoch 4254, Loss: 0.02236095117405057, Final Batch Loss: 0.0055713471956551075\n",
      "Epoch 4255, Loss: 0.02628512680530548, Final Batch Loss: 0.008082989603281021\n",
      "Epoch 4256, Loss: 0.029099419713020325, Final Batch Loss: 0.011588741093873978\n",
      "Epoch 4257, Loss: 0.018508388428017497, Final Batch Loss: 0.0031161222141236067\n",
      "Epoch 4258, Loss: 0.021893009776249528, Final Batch Loss: 0.0019808488432317972\n",
      "Epoch 4259, Loss: 0.01468380494043231, Final Batch Loss: 0.005626324098557234\n",
      "Epoch 4260, Loss: 0.016216730931773782, Final Batch Loss: 0.003313084365800023\n",
      "Epoch 4261, Loss: 0.017788738012313843, Final Batch Loss: 0.008019489236176014\n",
      "Epoch 4262, Loss: 0.020213135285302997, Final Batch Loss: 0.0035478274803608656\n",
      "Epoch 4263, Loss: 0.016606957651674747, Final Batch Loss: 0.0072861323133111\n",
      "Epoch 4264, Loss: 0.021260786801576614, Final Batch Loss: 0.004801921546459198\n",
      "Epoch 4265, Loss: 0.0757483709603548, Final Batch Loss: 0.062291402369737625\n",
      "Epoch 4266, Loss: 0.018449580064043403, Final Batch Loss: 0.002377133583649993\n",
      "Epoch 4267, Loss: 0.020900436211377382, Final Batch Loss: 0.007747787516564131\n",
      "Epoch 4268, Loss: 0.01880451338365674, Final Batch Loss: 0.00643271254375577\n",
      "Epoch 4269, Loss: 0.025769774336367846, Final Batch Loss: 0.0069190082140266895\n",
      "Epoch 4270, Loss: 0.00819790968671441, Final Batch Loss: 0.002834280487149954\n",
      "Epoch 4271, Loss: 0.027133232913911343, Final Batch Loss: 0.008383388631045818\n",
      "Epoch 4272, Loss: 0.01607235101982951, Final Batch Loss: 0.004649502690881491\n",
      "Epoch 4273, Loss: 0.02099208626896143, Final Batch Loss: 0.01159427035599947\n",
      "Epoch 4274, Loss: 0.027409246657043695, Final Batch Loss: 0.004640257451683283\n",
      "Epoch 4275, Loss: 0.01928991382010281, Final Batch Loss: 0.0021893002558499575\n",
      "Epoch 4276, Loss: 0.011490042321383953, Final Batch Loss: 0.004136034287512302\n",
      "Epoch 4277, Loss: 0.03920024260878563, Final Batch Loss: 0.03043743222951889\n",
      "Epoch 4278, Loss: 0.012622416485100985, Final Batch Loss: 0.00850104633718729\n",
      "Epoch 4279, Loss: 0.019369224086403847, Final Batch Loss: 0.004592320881783962\n",
      "Epoch 4280, Loss: 0.03379622474312782, Final Batch Loss: 0.013471178710460663\n",
      "Epoch 4281, Loss: 0.05743309436365962, Final Batch Loss: 0.051360130310058594\n",
      "Epoch 4282, Loss: 0.07745898328721523, Final Batch Loss: 0.06696714460849762\n",
      "Epoch 4283, Loss: 0.039432070683687925, Final Batch Loss: 0.03307710960507393\n",
      "Epoch 4284, Loss: 0.0862848674878478, Final Batch Loss: 0.07649113237857819\n",
      "Epoch 4285, Loss: 0.01744982972741127, Final Batch Loss: 0.009208153933286667\n",
      "Epoch 4286, Loss: 0.03444630466401577, Final Batch Loss: 0.015604402869939804\n",
      "Epoch 4287, Loss: 0.02782553667202592, Final Batch Loss: 0.003924981225281954\n",
      "Epoch 4288, Loss: 0.020463960245251656, Final Batch Loss: 0.007971201092004776\n",
      "Epoch 4289, Loss: 0.019615528406575322, Final Batch Loss: 0.0013099361676722765\n",
      "Epoch 4290, Loss: 0.009735169354826212, Final Batch Loss: 0.0026657558046281338\n",
      "Epoch 4291, Loss: 0.010705720167607069, Final Batch Loss: 0.005027528386563063\n",
      "Epoch 4292, Loss: 0.017682068864814937, Final Batch Loss: 0.0010769133223220706\n",
      "Epoch 4293, Loss: 0.012245317921042442, Final Batch Loss: 0.00323287770152092\n",
      "Epoch 4294, Loss: 0.016981061082333326, Final Batch Loss: 0.0058717005886137486\n",
      "Epoch 4295, Loss: 0.07068447582423687, Final Batch Loss: 0.04271681606769562\n",
      "Epoch 4296, Loss: 0.057534550316631794, Final Batch Loss: 0.008212403394281864\n",
      "Epoch 4297, Loss: 0.018139392603188753, Final Batch Loss: 0.014627277851104736\n",
      "Epoch 4298, Loss: 0.009337611496448517, Final Batch Loss: 0.0017530354671180248\n",
      "Epoch 4299, Loss: 0.02010449767112732, Final Batch Loss: 0.014063271693885326\n",
      "Epoch 4300, Loss: 0.012496677692979574, Final Batch Loss: 0.003351678606122732\n",
      "Epoch 4301, Loss: 0.01505065057426691, Final Batch Loss: 0.005838070996105671\n",
      "Epoch 4302, Loss: 0.016198076773434877, Final Batch Loss: 0.006480984855443239\n",
      "Epoch 4303, Loss: 0.012615593615919352, Final Batch Loss: 0.007067977916449308\n",
      "Epoch 4304, Loss: 0.014809869695454836, Final Batch Loss: 0.007137154694646597\n",
      "Epoch 4305, Loss: 0.034932266804389656, Final Batch Loss: 0.0019453986315056682\n",
      "Epoch 4306, Loss: 0.008429765235632658, Final Batch Loss: 0.0026219887658953667\n",
      "Epoch 4307, Loss: 0.015063650091178715, Final Batch Loss: 0.0016265102894976735\n",
      "Epoch 4308, Loss: 0.02183660678565502, Final Batch Loss: 0.009572964161634445\n",
      "Epoch 4309, Loss: 0.015331107657402754, Final Batch Loss: 0.007690034806728363\n",
      "Epoch 4310, Loss: 0.022464328445494175, Final Batch Loss: 0.005970901809632778\n",
      "Epoch 4311, Loss: 0.02118005184456706, Final Batch Loss: 0.013497868552803993\n",
      "Epoch 4312, Loss: 0.0523790093138814, Final Batch Loss: 0.04532220587134361\n",
      "Epoch 4313, Loss: 0.013239603023976088, Final Batch Loss: 0.005878171883523464\n",
      "Epoch 4314, Loss: 0.007708366960287094, Final Batch Loss: 0.0014487374573946\n",
      "Epoch 4315, Loss: 0.0912025822326541, Final Batch Loss: 0.07891696691513062\n",
      "Epoch 4316, Loss: 0.029322738060727715, Final Batch Loss: 0.0010371210519224405\n",
      "Epoch 4317, Loss: 0.008499599294736981, Final Batch Loss: 0.004813870415091515\n",
      "Epoch 4318, Loss: 0.03656111750751734, Final Batch Loss: 0.027552084997296333\n",
      "Epoch 4319, Loss: 0.01354318973608315, Final Batch Loss: 0.0038099719677120447\n",
      "Epoch 4320, Loss: 0.016199516598135233, Final Batch Loss: 0.006887799594551325\n",
      "Epoch 4321, Loss: 0.018861311487853527, Final Batch Loss: 0.011907774023711681\n",
      "Epoch 4322, Loss: 0.07416302477940917, Final Batch Loss: 0.005471458192914724\n",
      "Epoch 4323, Loss: 0.016223786864429712, Final Batch Loss: 0.0056230840273201466\n",
      "Epoch 4324, Loss: 0.021436974871903658, Final Batch Loss: 0.006885850336402655\n",
      "Epoch 4325, Loss: 0.027925913222134113, Final Batch Loss: 0.01994270272552967\n",
      "Epoch 4326, Loss: 0.00837250193580985, Final Batch Loss: 0.0030985670164227486\n",
      "Epoch 4327, Loss: 0.06381364865228534, Final Batch Loss: 0.006903729867190123\n",
      "Epoch 4328, Loss: 0.04258230701088905, Final Batch Loss: 0.037061937153339386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4329, Loss: 0.011092270724475384, Final Batch Loss: 0.005090076941996813\n",
      "Epoch 4330, Loss: 0.0132085089571774, Final Batch Loss: 0.007092958781868219\n",
      "Epoch 4331, Loss: 0.06692549958825111, Final Batch Loss: 0.033818550407886505\n",
      "Epoch 4332, Loss: 0.020857967901974916, Final Batch Loss: 0.016188109293580055\n",
      "Epoch 4333, Loss: 0.026514661498367786, Final Batch Loss: 0.005155508406460285\n",
      "Epoch 4334, Loss: 0.011076217982918024, Final Batch Loss: 0.0058699497021734715\n",
      "Epoch 4335, Loss: 0.00895994424354285, Final Batch Loss: 0.00175675458740443\n",
      "Epoch 4336, Loss: 0.018809655215591192, Final Batch Loss: 0.011718139983713627\n",
      "Epoch 4337, Loss: 0.012810604413971305, Final Batch Loss: 0.002500884933397174\n",
      "Epoch 4338, Loss: 0.047025545965880156, Final Batch Loss: 0.040078602731227875\n",
      "Epoch 4339, Loss: 0.02650278527289629, Final Batch Loss: 0.005021139048039913\n",
      "Epoch 4340, Loss: 0.18531224830076098, Final Batch Loss: 0.18131612241268158\n",
      "Epoch 4341, Loss: 0.021571065299212933, Final Batch Loss: 0.010503736324608326\n",
      "Epoch 4342, Loss: 0.011437670793384314, Final Batch Loss: 0.004166013095527887\n",
      "Epoch 4343, Loss: 0.02421552548184991, Final Batch Loss: 0.01801908202469349\n",
      "Epoch 4344, Loss: 0.07722115842625499, Final Batch Loss: 0.07165748625993729\n",
      "Epoch 4345, Loss: 0.010034518083557487, Final Batch Loss: 0.007466606795787811\n",
      "Epoch 4346, Loss: 0.06316747888922691, Final Batch Loss: 0.02077614888548851\n",
      "Epoch 4347, Loss: 0.061476655304431915, Final Batch Loss: 0.042650967836380005\n",
      "Epoch 4348, Loss: 0.02860345132648945, Final Batch Loss: 0.02092135325074196\n",
      "Epoch 4349, Loss: 0.013331496738828719, Final Batch Loss: 0.001263482146896422\n",
      "Epoch 4350, Loss: 0.019405187107622623, Final Batch Loss: 0.007902419194579124\n",
      "Epoch 4351, Loss: 0.025569147896021605, Final Batch Loss: 0.01804284192621708\n",
      "Epoch 4352, Loss: 0.029767323285341263, Final Batch Loss: 0.02053871937096119\n",
      "Epoch 4353, Loss: 0.013849240262061357, Final Batch Loss: 0.0062057217583060265\n",
      "Epoch 4354, Loss: 0.01050497218966484, Final Batch Loss: 0.005488603841513395\n",
      "Epoch 4355, Loss: 0.06913948804140091, Final Batch Loss: 0.060282379388809204\n",
      "Epoch 4356, Loss: 0.023986928164958954, Final Batch Loss: 0.013456818647682667\n",
      "Epoch 4357, Loss: 0.08746798615902662, Final Batch Loss: 0.07434433698654175\n",
      "Epoch 4358, Loss: 0.054136146791279316, Final Batch Loss: 0.008885686285793781\n",
      "Epoch 4359, Loss: 0.05178933497518301, Final Batch Loss: 0.04359996318817139\n",
      "Epoch 4360, Loss: 0.014829063322395086, Final Batch Loss: 0.0036917426623404026\n",
      "Epoch 4361, Loss: 0.015514940023422241, Final Batch Loss: 0.006439197808504105\n",
      "Epoch 4362, Loss: 0.049491449259221554, Final Batch Loss: 0.012706971727311611\n",
      "Epoch 4363, Loss: 0.052538445219397545, Final Batch Loss: 0.0313105545938015\n",
      "Epoch 4364, Loss: 0.02314548846334219, Final Batch Loss: 0.010373750701546669\n",
      "Epoch 4365, Loss: 0.04224951006472111, Final Batch Loss: 0.027838807553052902\n",
      "Epoch 4366, Loss: 0.06519734906032681, Final Batch Loss: 0.06052505970001221\n",
      "Epoch 4367, Loss: 0.01989768212661147, Final Batch Loss: 0.007733065169304609\n",
      "Epoch 4368, Loss: 0.04215842392295599, Final Batch Loss: 0.02774466946721077\n",
      "Epoch 4369, Loss: 0.019947451539337635, Final Batch Loss: 0.01575457490980625\n",
      "Epoch 4370, Loss: 0.021443745121359825, Final Batch Loss: 0.012719819322228432\n",
      "Epoch 4371, Loss: 0.014950292184948921, Final Batch Loss: 0.005378685891628265\n",
      "Epoch 4372, Loss: 0.04398888465948403, Final Batch Loss: 0.0029701937455683947\n",
      "Epoch 4373, Loss: 0.02002617111429572, Final Batch Loss: 0.003082695882767439\n",
      "Epoch 4374, Loss: 0.08124368824064732, Final Batch Loss: 0.017122553661465645\n",
      "Epoch 4375, Loss: 0.011676645372062922, Final Batch Loss: 0.0053064399398863316\n",
      "Epoch 4376, Loss: 0.03440426755696535, Final Batch Loss: 0.02308698371052742\n",
      "Epoch 4377, Loss: 0.010768075473606586, Final Batch Loss: 0.006939298007637262\n",
      "Epoch 4378, Loss: 0.04277413338422775, Final Batch Loss: 0.018149062991142273\n",
      "Epoch 4379, Loss: 0.015157748479396105, Final Batch Loss: 0.00414406368508935\n",
      "Epoch 4380, Loss: 0.015939649660140276, Final Batch Loss: 0.0062494478188455105\n",
      "Epoch 4381, Loss: 0.017151921638287604, Final Batch Loss: 0.000977718853391707\n",
      "Epoch 4382, Loss: 0.060702054761350155, Final Batch Loss: 0.05355285108089447\n",
      "Epoch 4383, Loss: 0.059303389862179756, Final Batch Loss: 0.04347998648881912\n",
      "Epoch 4384, Loss: 0.04415816953405738, Final Batch Loss: 0.03972093015909195\n",
      "Epoch 4385, Loss: 0.0820333231240511, Final Batch Loss: 0.050873205065727234\n",
      "Epoch 4386, Loss: 0.02183574065566063, Final Batch Loss: 0.011211342178285122\n",
      "Epoch 4387, Loss: 0.010832591913640499, Final Batch Loss: 0.004596009850502014\n",
      "Epoch 4388, Loss: 0.05568634904921055, Final Batch Loss: 0.051097091287374496\n",
      "Epoch 4389, Loss: 0.024912403896450996, Final Batch Loss: 0.009680486284196377\n",
      "Epoch 4390, Loss: 0.015373196452856064, Final Batch Loss: 0.0070748282596468925\n",
      "Epoch 4391, Loss: 0.008893368300050497, Final Batch Loss: 0.0043411352671682835\n",
      "Epoch 4392, Loss: 0.02336474508047104, Final Batch Loss: 0.0048384591937065125\n",
      "Epoch 4393, Loss: 0.011321719735860825, Final Batch Loss: 0.00501939607784152\n",
      "Epoch 4394, Loss: 0.052654013969004154, Final Batch Loss: 0.008260575123131275\n",
      "Epoch 4395, Loss: 0.01678518275730312, Final Batch Loss: 0.012953710742294788\n",
      "Epoch 4396, Loss: 0.046346952905878425, Final Batch Loss: 0.0037878507282584906\n",
      "Epoch 4397, Loss: 0.046997690573334694, Final Batch Loss: 0.02279486320912838\n",
      "Epoch 4398, Loss: 0.025101969949901104, Final Batch Loss: 0.007132369093596935\n",
      "Epoch 4399, Loss: 0.07375448476523161, Final Batch Loss: 0.06405811011791229\n",
      "Epoch 4400, Loss: 0.012571946252137423, Final Batch Loss: 0.0066916560754179955\n",
      "Epoch 4401, Loss: 0.021087149158120155, Final Batch Loss: 0.00869043916463852\n",
      "Epoch 4402, Loss: 0.016941752983257174, Final Batch Loss: 0.0017137869726866484\n",
      "Epoch 4403, Loss: 0.043342409655451775, Final Batch Loss: 0.008021140471100807\n",
      "Epoch 4404, Loss: 0.014250914100557566, Final Batch Loss: 0.0020783967338502407\n",
      "Epoch 4405, Loss: 0.01441968057770282, Final Batch Loss: 0.0019483325304463506\n",
      "Epoch 4406, Loss: 0.014739072881639004, Final Batch Loss: 0.004749014973640442\n",
      "Epoch 4407, Loss: 0.014852696098387241, Final Batch Loss: 0.007572398521006107\n",
      "Epoch 4408, Loss: 0.04704170720651746, Final Batch Loss: 0.0033825677819550037\n",
      "Epoch 4409, Loss: 0.012030426878482103, Final Batch Loss: 0.006507841404527426\n",
      "Epoch 4410, Loss: 0.019379680044949055, Final Batch Loss: 0.006978335790336132\n",
      "Epoch 4411, Loss: 0.021868488285690546, Final Batch Loss: 0.005227189976722002\n",
      "Epoch 4412, Loss: 0.028264797758311033, Final Batch Loss: 0.02376341074705124\n",
      "Epoch 4413, Loss: 0.013204114977270365, Final Batch Loss: 0.002129725646227598\n",
      "Epoch 4414, Loss: 0.020063084550201893, Final Batch Loss: 0.012347509153187275\n",
      "Epoch 4415, Loss: 0.010721846017986536, Final Batch Loss: 0.0019882055930793285\n",
      "Epoch 4416, Loss: 0.011782908346503973, Final Batch Loss: 0.0050787548534572124\n",
      "Epoch 4417, Loss: 0.022717854008078575, Final Batch Loss: 0.0085989348590374\n",
      "Epoch 4418, Loss: 0.012122938642278314, Final Batch Loss: 0.0024148665834218264\n",
      "Epoch 4419, Loss: 0.021680056117475033, Final Batch Loss: 0.015139550901949406\n",
      "Epoch 4420, Loss: 0.02953246235847473, Final Batch Loss: 0.022733530029654503\n",
      "Epoch 4421, Loss: 0.043849455658346415, Final Batch Loss: 0.03831768408417702\n",
      "Epoch 4422, Loss: 0.017249511554837227, Final Batch Loss: 0.011628367006778717\n",
      "Epoch 4423, Loss: 0.014724913984537125, Final Batch Loss: 0.0064257606863975525\n",
      "Epoch 4424, Loss: 0.022048812359571457, Final Batch Loss: 0.010583193972706795\n",
      "Epoch 4425, Loss: 0.016574605368077755, Final Batch Loss: 0.0052480194717645645\n",
      "Epoch 4426, Loss: 0.0077439941233024, Final Batch Loss: 0.001546103390865028\n",
      "Epoch 4427, Loss: 0.009117391193285584, Final Batch Loss: 0.0019436210859566927\n",
      "Epoch 4428, Loss: 0.009460452594794333, Final Batch Loss: 0.001606773235835135\n",
      "Epoch 4429, Loss: 0.011906381463631988, Final Batch Loss: 0.003734392812475562\n",
      "Epoch 4430, Loss: 0.024583672638982534, Final Batch Loss: 0.0034627034328877926\n",
      "Epoch 4431, Loss: 0.011004060041159391, Final Batch Loss: 0.0013583316467702389\n",
      "Epoch 4432, Loss: 0.013087344355881214, Final Batch Loss: 0.006434854585677385\n",
      "Epoch 4433, Loss: 0.025480104610323906, Final Batch Loss: 0.008583156391978264\n",
      "Epoch 4434, Loss: 0.015621769707649946, Final Batch Loss: 0.004320104140788317\n",
      "Epoch 4435, Loss: 0.017291832249611616, Final Batch Loss: 0.006324262823909521\n",
      "Epoch 4436, Loss: 0.04197546932846308, Final Batch Loss: 0.028294291347265244\n",
      "Epoch 4437, Loss: 0.034200018271803856, Final Batch Loss: 0.01669475808739662\n",
      "Epoch 4438, Loss: 0.08904466591775417, Final Batch Loss: 0.08063509315252304\n",
      "Epoch 4439, Loss: 0.03759829606860876, Final Batch Loss: 0.031090186908841133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4440, Loss: 0.015995396301150322, Final Batch Loss: 0.008751487359404564\n",
      "Epoch 4441, Loss: 0.008970635710284114, Final Batch Loss: 0.002385397208854556\n",
      "Epoch 4442, Loss: 0.0312784796115011, Final Batch Loss: 0.002329412614926696\n",
      "Epoch 4443, Loss: 0.014665356371551752, Final Batch Loss: 0.004816846456378698\n",
      "Epoch 4444, Loss: 0.018713680561631918, Final Batch Loss: 0.01133857574313879\n",
      "Epoch 4445, Loss: 0.01822551153600216, Final Batch Loss: 0.007228285074234009\n",
      "Epoch 4446, Loss: 0.00989909190684557, Final Batch Loss: 0.0035260245203971863\n",
      "Epoch 4447, Loss: 0.013161727460101247, Final Batch Loss: 0.0026576111558824778\n",
      "Epoch 4448, Loss: 0.00905604287981987, Final Batch Loss: 0.005189787596464157\n",
      "Epoch 4449, Loss: 0.046328090131282806, Final Batch Loss: 0.03421146050095558\n",
      "Epoch 4450, Loss: 0.014547926839441061, Final Batch Loss: 0.007305283099412918\n",
      "Epoch 4451, Loss: 0.047048079781234264, Final Batch Loss: 0.03243772312998772\n",
      "Epoch 4452, Loss: 0.03488900698721409, Final Batch Loss: 0.01715030148625374\n",
      "Epoch 4453, Loss: 0.24433248629793525, Final Batch Loss: 0.2395292967557907\n",
      "Epoch 4454, Loss: 0.02448386698961258, Final Batch Loss: 0.004990007728338242\n",
      "Epoch 4455, Loss: 0.04293119441717863, Final Batch Loss: 0.039940547198057175\n",
      "Epoch 4456, Loss: 0.02720305696129799, Final Batch Loss: 0.018224839121103287\n",
      "Epoch 4457, Loss: 0.0065756854601204395, Final Batch Loss: 0.0013683936558663845\n",
      "Epoch 4458, Loss: 0.014311282895505428, Final Batch Loss: 0.005814298987388611\n",
      "Epoch 4459, Loss: 0.026428639888763428, Final Batch Loss: 0.008516790345311165\n",
      "Epoch 4460, Loss: 0.021162858232855797, Final Batch Loss: 0.008797986432909966\n",
      "Epoch 4461, Loss: 0.011743772309273481, Final Batch Loss: 0.0008743214420974255\n",
      "Epoch 4462, Loss: 0.024372668005526066, Final Batch Loss: 0.01728963293135166\n",
      "Epoch 4463, Loss: 0.011863237945362926, Final Batch Loss: 0.001603081589564681\n",
      "Epoch 4464, Loss: 0.01877022860571742, Final Batch Loss: 0.013659439980983734\n",
      "Epoch 4465, Loss: 0.010025759227573872, Final Batch Loss: 0.003908686805516481\n",
      "Epoch 4466, Loss: 0.008520142175257206, Final Batch Loss: 0.0034976210445165634\n",
      "Epoch 4467, Loss: 0.011389405932277441, Final Batch Loss: 0.003926506265997887\n",
      "Epoch 4468, Loss: 0.029260044917464256, Final Batch Loss: 0.01977577619254589\n",
      "Epoch 4469, Loss: 0.02173340506851673, Final Batch Loss: 0.005263203755021095\n",
      "Epoch 4470, Loss: 0.043864836916327477, Final Batch Loss: 0.023895859718322754\n",
      "Epoch 4471, Loss: 0.03744730819016695, Final Batch Loss: 0.015372044406831264\n",
      "Epoch 4472, Loss: 0.0240052817389369, Final Batch Loss: 0.01143239252269268\n",
      "Epoch 4473, Loss: 0.033362770453095436, Final Batch Loss: 0.024305310100317\n",
      "Epoch 4474, Loss: 0.020907878875732422, Final Batch Loss: 0.0062531111761927605\n",
      "Epoch 4475, Loss: 0.018466474022716284, Final Batch Loss: 0.005458258558064699\n",
      "Epoch 4476, Loss: 0.04657756071537733, Final Batch Loss: 0.032813407480716705\n",
      "Epoch 4477, Loss: 0.018901148810982704, Final Batch Loss: 0.01030866801738739\n",
      "Epoch 4478, Loss: 0.03870376106351614, Final Batch Loss: 0.02965003252029419\n",
      "Epoch 4479, Loss: 0.010970600415021181, Final Batch Loss: 0.005148835014551878\n",
      "Epoch 4480, Loss: 0.01850318117067218, Final Batch Loss: 0.011203166097402573\n",
      "Epoch 4481, Loss: 0.01569323753938079, Final Batch Loss: 0.009178525768220425\n",
      "Epoch 4482, Loss: 0.023500163108110428, Final Batch Loss: 0.01079879142343998\n",
      "Epoch 4483, Loss: 0.023264551535248756, Final Batch Loss: 0.012858868576586246\n",
      "Epoch 4484, Loss: 0.017283849767409265, Final Batch Loss: 0.001742259948514402\n",
      "Epoch 4485, Loss: 0.03207311872392893, Final Batch Loss: 0.014527463354170322\n",
      "Epoch 4486, Loss: 0.013936156872659922, Final Batch Loss: 0.007302585057914257\n",
      "Epoch 4487, Loss: 0.08392065390944481, Final Batch Loss: 0.07383027672767639\n",
      "Epoch 4488, Loss: 0.015332742128521204, Final Batch Loss: 0.004167463164776564\n",
      "Epoch 4489, Loss: 0.021125314757227898, Final Batch Loss: 0.004647567868232727\n",
      "Epoch 4490, Loss: 0.018970093689858913, Final Batch Loss: 0.0036614900454878807\n",
      "Epoch 4491, Loss: 0.02487519010901451, Final Batch Loss: 0.006824685260653496\n",
      "Epoch 4492, Loss: 0.039321780670434237, Final Batch Loss: 0.03440001979470253\n",
      "Epoch 4493, Loss: 0.018273361958563328, Final Batch Loss: 0.009844395332038403\n",
      "Epoch 4494, Loss: 0.014891328988596797, Final Batch Loss: 0.0010169378947466612\n",
      "Epoch 4495, Loss: 0.03803235339000821, Final Batch Loss: 0.004819121677428484\n",
      "Epoch 4496, Loss: 0.02140663913451135, Final Batch Loss: 0.0032322199549525976\n",
      "Epoch 4497, Loss: 0.0168296592310071, Final Batch Loss: 0.008004911243915558\n",
      "Epoch 4498, Loss: 0.021351049654185772, Final Batch Loss: 0.00378990825265646\n",
      "Epoch 4499, Loss: 0.01130812766496092, Final Batch Loss: 0.0007368909427896142\n",
      "Epoch 4500, Loss: 0.029250938445329666, Final Batch Loss: 0.022226544097065926\n",
      "Epoch 4501, Loss: 0.01625462854281068, Final Batch Loss: 0.0063538518734276295\n",
      "Epoch 4502, Loss: 0.014851037878543139, Final Batch Loss: 0.0042283483780920506\n",
      "Epoch 4503, Loss: 0.0485595790669322, Final Batch Loss: 0.03989625722169876\n",
      "Epoch 4504, Loss: 0.040465641766786575, Final Batch Loss: 0.016304295510053635\n",
      "Epoch 4505, Loss: 0.028803641442209482, Final Batch Loss: 0.00316837290301919\n",
      "Epoch 4506, Loss: 0.13970315270125866, Final Batch Loss: 0.11741732060909271\n",
      "Epoch 4507, Loss: 0.007847824832424521, Final Batch Loss: 0.004740898497402668\n",
      "Epoch 4508, Loss: 0.038266037590801716, Final Batch Loss: 0.027567589655518532\n",
      "Epoch 4509, Loss: 0.019963860977441072, Final Batch Loss: 0.006173666100949049\n",
      "Epoch 4510, Loss: 0.01519268425181508, Final Batch Loss: 0.013418104499578476\n",
      "Epoch 4511, Loss: 0.008857650682330132, Final Batch Loss: 0.001860255841165781\n",
      "Epoch 4512, Loss: 0.016374220605939627, Final Batch Loss: 0.007147183176130056\n",
      "Epoch 4513, Loss: 0.15208790451288223, Final Batch Loss: 0.13333117961883545\n",
      "Epoch 4514, Loss: 0.016631043516099453, Final Batch Loss: 0.007817885838449001\n",
      "Epoch 4515, Loss: 0.016047248616814613, Final Batch Loss: 0.01150781475007534\n",
      "Epoch 4516, Loss: 0.016949153970927, Final Batch Loss: 0.005097587127238512\n",
      "Epoch 4517, Loss: 0.006752772489562631, Final Batch Loss: 0.0033236397430300713\n",
      "Epoch 4518, Loss: 0.037061973474919796, Final Batch Loss: 0.027533115819096565\n",
      "Epoch 4519, Loss: 0.013874237891286612, Final Batch Loss: 0.0060114008374512196\n",
      "Epoch 4520, Loss: 0.021779588423669338, Final Batch Loss: 0.005051611922681332\n",
      "Epoch 4521, Loss: 0.015500264707952738, Final Batch Loss: 0.0045984419994056225\n",
      "Epoch 4522, Loss: 0.01131476298905909, Final Batch Loss: 0.0014174256939440966\n",
      "Epoch 4523, Loss: 0.014478814788162708, Final Batch Loss: 0.010494747199118137\n",
      "Epoch 4524, Loss: 0.00685643614269793, Final Batch Loss: 0.002502372255548835\n",
      "Epoch 4525, Loss: 0.00818488746881485, Final Batch Loss: 0.001752612181007862\n",
      "Epoch 4526, Loss: 0.017054250463843346, Final Batch Loss: 0.00908768642693758\n",
      "Epoch 4527, Loss: 0.01982097513973713, Final Batch Loss: 0.008409479632973671\n",
      "Epoch 4528, Loss: 0.013035831274464726, Final Batch Loss: 0.0023673374671489\n",
      "Epoch 4529, Loss: 0.009804787812754512, Final Batch Loss: 0.0027958916034549475\n",
      "Epoch 4530, Loss: 0.007522399537265301, Final Batch Loss: 0.0033489768393337727\n",
      "Epoch 4531, Loss: 0.0187431531958282, Final Batch Loss: 0.007348913233727217\n",
      "Epoch 4532, Loss: 0.03797336854040623, Final Batch Loss: 0.02614523284137249\n",
      "Epoch 4533, Loss: 0.013551801908761263, Final Batch Loss: 0.004735737573355436\n",
      "Epoch 4534, Loss: 0.06853222846984863, Final Batch Loss: 0.051283303648233414\n",
      "Epoch 4535, Loss: 0.007531186332926154, Final Batch Loss: 0.0018217351753264666\n",
      "Epoch 4536, Loss: 0.011972672771662474, Final Batch Loss: 0.0036147073842585087\n",
      "Epoch 4537, Loss: 0.02197991870343685, Final Batch Loss: 0.0056842658668756485\n",
      "Epoch 4538, Loss: 0.02074836567044258, Final Batch Loss: 0.014116653241217136\n",
      "Epoch 4539, Loss: 0.012405742367263883, Final Batch Loss: 0.0008675925782881677\n",
      "Epoch 4540, Loss: 0.03133806120604277, Final Batch Loss: 0.021648598834872246\n",
      "Epoch 4541, Loss: 0.01053481874987483, Final Batch Loss: 0.006137029267847538\n",
      "Epoch 4542, Loss: 0.022702696034684777, Final Batch Loss: 0.001400947803631425\n",
      "Epoch 4543, Loss: 0.016898575704544783, Final Batch Loss: 0.01130917388945818\n",
      "Epoch 4544, Loss: 0.06676697731018066, Final Batch Loss: 0.00733480229973793\n",
      "Epoch 4545, Loss: 0.060095339082181454, Final Batch Loss: 0.0453345887362957\n",
      "Epoch 4546, Loss: 0.01216369098983705, Final Batch Loss: 0.0016191022004932165\n",
      "Epoch 4547, Loss: 0.009251307230442762, Final Batch Loss: 0.004215991590172052\n",
      "Epoch 4548, Loss: 0.017236868850886822, Final Batch Loss: 0.005206812173128128\n",
      "Epoch 4549, Loss: 0.016865896061062813, Final Batch Loss: 0.007999961264431477\n",
      "Epoch 4550, Loss: 0.005839743418619037, Final Batch Loss: 0.0013174510095268488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4551, Loss: 0.05547057744115591, Final Batch Loss: 0.04284088313579559\n",
      "Epoch 4552, Loss: 0.013406615238636732, Final Batch Loss: 0.008154496550559998\n",
      "Epoch 4553, Loss: 0.023034241050481796, Final Batch Loss: 0.010441129095852375\n",
      "Epoch 4554, Loss: 0.02098484057933092, Final Batch Loss: 0.005651518702507019\n",
      "Epoch 4555, Loss: 0.03725062683224678, Final Batch Loss: 0.028102343901991844\n",
      "Epoch 4556, Loss: 0.02292789425700903, Final Batch Loss: 0.011816931888461113\n",
      "Epoch 4557, Loss: 0.026384311728179455, Final Batch Loss: 0.02095942199230194\n",
      "Epoch 4558, Loss: 0.014427911955863237, Final Batch Loss: 0.005429826211184263\n",
      "Epoch 4559, Loss: 0.033824807964265347, Final Batch Loss: 0.025425953790545464\n",
      "Epoch 4560, Loss: 0.01225450192578137, Final Batch Loss: 0.0036748137790709734\n",
      "Epoch 4561, Loss: 0.02402372471988201, Final Batch Loss: 0.008810858242213726\n",
      "Epoch 4562, Loss: 0.005960999988019466, Final Batch Loss: 0.002226489130407572\n",
      "Epoch 4563, Loss: 0.009903803002089262, Final Batch Loss: 0.0030745752155780792\n",
      "Epoch 4564, Loss: 0.045759146101772785, Final Batch Loss: 0.035420842468738556\n",
      "Epoch 4565, Loss: 0.01987102208659053, Final Batch Loss: 0.01361885666847229\n",
      "Epoch 4566, Loss: 0.016251508379355073, Final Batch Loss: 0.002283375011757016\n",
      "Epoch 4567, Loss: 0.05230465345084667, Final Batch Loss: 0.04736538976430893\n",
      "Epoch 4568, Loss: 0.019740422954782844, Final Batch Loss: 0.0011229196097701788\n",
      "Epoch 4569, Loss: 0.021647882647812366, Final Batch Loss: 0.012900491245090961\n",
      "Epoch 4570, Loss: 0.03480666223913431, Final Batch Loss: 0.019656071439385414\n",
      "Epoch 4571, Loss: 0.03813389129936695, Final Batch Loss: 0.027396636083722115\n",
      "Epoch 4572, Loss: 0.02088727429509163, Final Batch Loss: 0.01296848151832819\n",
      "Epoch 4573, Loss: 0.03620929829776287, Final Batch Loss: 0.024583088234066963\n",
      "Epoch 4574, Loss: 0.03174126171506941, Final Batch Loss: 0.0032274744007736444\n",
      "Epoch 4575, Loss: 0.03485047351568937, Final Batch Loss: 0.02529023587703705\n",
      "Epoch 4576, Loss: 0.017502627801150084, Final Batch Loss: 0.00460811099037528\n",
      "Epoch 4577, Loss: 0.041084904223680496, Final Batch Loss: 0.024213505908846855\n",
      "Epoch 4578, Loss: 0.004878507344983518, Final Batch Loss: 0.0017683269688859582\n",
      "Epoch 4579, Loss: 0.01992400363087654, Final Batch Loss: 0.003182336688041687\n",
      "Epoch 4580, Loss: 0.051011388190090656, Final Batch Loss: 0.041729092597961426\n",
      "Epoch 4581, Loss: 0.036765540950000286, Final Batch Loss: 0.011478065513074398\n",
      "Epoch 4582, Loss: 0.06753835722338408, Final Batch Loss: 0.0018379677785560489\n",
      "Epoch 4583, Loss: 0.013600456528365612, Final Batch Loss: 0.006341246422380209\n",
      "Epoch 4584, Loss: 0.014503258746117353, Final Batch Loss: 0.003749650437384844\n",
      "Epoch 4585, Loss: 0.010223730467259884, Final Batch Loss: 0.005684360861778259\n",
      "Epoch 4586, Loss: 0.035003000404685736, Final Batch Loss: 0.00409619091078639\n",
      "Epoch 4587, Loss: 0.026700991205871105, Final Batch Loss: 0.001965445466339588\n",
      "Epoch 4588, Loss: 0.04779761657118797, Final Batch Loss: 0.019048303365707397\n",
      "Epoch 4589, Loss: 0.009373326553031802, Final Batch Loss: 0.0009058897849172354\n",
      "Epoch 4590, Loss: 0.02869171742349863, Final Batch Loss: 0.008985172025859356\n",
      "Epoch 4591, Loss: 0.011557015590369701, Final Batch Loss: 0.005460780579596758\n",
      "Epoch 4592, Loss: 0.03404898662120104, Final Batch Loss: 0.025986090302467346\n",
      "Epoch 4593, Loss: 0.004552895901724696, Final Batch Loss: 0.001698987791314721\n",
      "Epoch 4594, Loss: 0.03435941133648157, Final Batch Loss: 0.0010040858760476112\n",
      "Epoch 4595, Loss: 0.045827267691493034, Final Batch Loss: 0.006929168477654457\n",
      "Epoch 4596, Loss: 0.014826113590970635, Final Batch Loss: 0.002956358017399907\n",
      "Epoch 4597, Loss: 0.024069001898169518, Final Batch Loss: 0.01696499064564705\n",
      "Epoch 4598, Loss: 0.026640274561941624, Final Batch Loss: 0.014149672351777554\n",
      "Epoch 4599, Loss: 0.021654180251061916, Final Batch Loss: 0.011742908507585526\n",
      "Epoch 4600, Loss: 0.02753542992286384, Final Batch Loss: 0.0022031243424862623\n",
      "Epoch 4601, Loss: 0.015196402091532946, Final Batch Loss: 0.00916564092040062\n",
      "Epoch 4602, Loss: 0.007405934855341911, Final Batch Loss: 0.0022909375838935375\n",
      "Epoch 4603, Loss: 0.012287214398384094, Final Batch Loss: 0.004165259189903736\n",
      "Epoch 4604, Loss: 0.01255756989121437, Final Batch Loss: 0.0025943992659449577\n",
      "Epoch 4605, Loss: 0.03244007425382733, Final Batch Loss: 0.025865759700536728\n",
      "Epoch 4606, Loss: 0.02664591697975993, Final Batch Loss: 0.00521751819178462\n",
      "Epoch 4607, Loss: 0.05278071295469999, Final Batch Loss: 0.009974158369004726\n",
      "Epoch 4608, Loss: 0.020333279855549335, Final Batch Loss: 0.009943848475813866\n",
      "Epoch 4609, Loss: 0.02944145817309618, Final Batch Loss: 0.012739929370582104\n",
      "Epoch 4610, Loss: 0.0251135784201324, Final Batch Loss: 0.020856797695159912\n",
      "Epoch 4611, Loss: 0.018959527369588614, Final Batch Loss: 0.012527459301054478\n",
      "Epoch 4612, Loss: 0.014720139093697071, Final Batch Loss: 0.008419147692620754\n",
      "Epoch 4613, Loss: 0.008400187827646732, Final Batch Loss: 0.002398403827100992\n",
      "Epoch 4614, Loss: 0.007470420794561505, Final Batch Loss: 0.002126536099240184\n",
      "Epoch 4615, Loss: 0.024681492242962122, Final Batch Loss: 0.002936192322522402\n",
      "Epoch 4616, Loss: 0.019824528135359287, Final Batch Loss: 0.006878983229398727\n",
      "Epoch 4617, Loss: 0.028658557683229446, Final Batch Loss: 0.009789597243070602\n",
      "Epoch 4618, Loss: 0.027990720118395984, Final Batch Loss: 0.0016499896300956607\n",
      "Epoch 4619, Loss: 0.045792230404913425, Final Batch Loss: 0.011905771680176258\n",
      "Epoch 4620, Loss: 0.05436417879536748, Final Batch Loss: 0.001519464422017336\n",
      "Epoch 4621, Loss: 0.01676257560029626, Final Batch Loss: 0.0048646689392626286\n",
      "Epoch 4622, Loss: 0.022660624235868454, Final Batch Loss: 0.009139645844697952\n",
      "Epoch 4623, Loss: 0.015999531839042902, Final Batch Loss: 0.006169268395751715\n",
      "Epoch 4624, Loss: 0.01661895541474223, Final Batch Loss: 0.005327476654201746\n",
      "Epoch 4625, Loss: 0.022810449823737144, Final Batch Loss: 0.008244359865784645\n",
      "Epoch 4626, Loss: 0.014444173779338598, Final Batch Loss: 0.004223930183798075\n",
      "Epoch 4627, Loss: 0.007934206980280578, Final Batch Loss: 0.0009419954149052501\n",
      "Epoch 4628, Loss: 0.0378850307315588, Final Batch Loss: 0.019736284390091896\n",
      "Epoch 4629, Loss: 0.04140173550695181, Final Batch Loss: 0.029711242765188217\n",
      "Epoch 4630, Loss: 0.010812753578647971, Final Batch Loss: 0.0035790700931102037\n",
      "Epoch 4631, Loss: 0.02332042995840311, Final Batch Loss: 0.013242283836007118\n",
      "Epoch 4632, Loss: 0.025844191201031208, Final Batch Loss: 0.020767170935869217\n",
      "Epoch 4633, Loss: 0.015462581068277359, Final Batch Loss: 0.0031771576032042503\n",
      "Epoch 4634, Loss: 0.05611604265868664, Final Batch Loss: 0.04507923126220703\n",
      "Epoch 4635, Loss: 0.0230295374058187, Final Batch Loss: 0.01644105091691017\n",
      "Epoch 4636, Loss: 0.02391887828707695, Final Batch Loss: 0.0121297063305974\n",
      "Epoch 4637, Loss: 0.011348328087478876, Final Batch Loss: 0.0030646496452391148\n",
      "Epoch 4638, Loss: 0.015129129868000746, Final Batch Loss: 0.0044397334568202496\n",
      "Epoch 4639, Loss: 0.024258033372461796, Final Batch Loss: 0.008916356600821018\n",
      "Epoch 4640, Loss: 0.009270460810512304, Final Batch Loss: 0.0044738175347447395\n",
      "Epoch 4641, Loss: 0.017534406622871757, Final Batch Loss: 0.0031918955501168966\n",
      "Epoch 4642, Loss: 0.03882989287376404, Final Batch Loss: 0.018163416534662247\n",
      "Epoch 4643, Loss: 0.02208444569259882, Final Batch Loss: 0.006053893826901913\n",
      "Epoch 4644, Loss: 0.015486038057133555, Final Batch Loss: 0.002401448553428054\n",
      "Epoch 4645, Loss: 0.027797258459031582, Final Batch Loss: 0.021858394145965576\n",
      "Epoch 4646, Loss: 0.01051374152302742, Final Batch Loss: 0.0021977759897708893\n",
      "Epoch 4647, Loss: 0.04459710046648979, Final Batch Loss: 0.02493351697921753\n",
      "Epoch 4648, Loss: 0.009896045550704002, Final Batch Loss: 0.00347639387473464\n",
      "Epoch 4649, Loss: 0.028681660536676645, Final Batch Loss: 0.023047873750329018\n",
      "Epoch 4650, Loss: 0.033839824609458447, Final Batch Loss: 0.029021885246038437\n",
      "Epoch 4651, Loss: 0.012850491795688868, Final Batch Loss: 0.010313984006643295\n",
      "Epoch 4652, Loss: 0.020830110181123018, Final Batch Loss: 0.0057480656541883945\n",
      "Epoch 4653, Loss: 0.056704290211200714, Final Batch Loss: 0.036993034183979034\n",
      "Epoch 4654, Loss: 0.011709219077602029, Final Batch Loss: 0.002977825002744794\n",
      "Epoch 4655, Loss: 0.045473264530301094, Final Batch Loss: 0.005356041714549065\n",
      "Epoch 4656, Loss: 0.022657765075564384, Final Batch Loss: 0.0071536824107170105\n",
      "Epoch 4657, Loss: 0.01889840792864561, Final Batch Loss: 0.004399466328322887\n",
      "Epoch 4658, Loss: 0.016517626587301493, Final Batch Loss: 0.01020808145403862\n",
      "Epoch 4659, Loss: 0.009011540096253157, Final Batch Loss: 0.004818914458155632\n",
      "Epoch 4660, Loss: 0.014483437407761812, Final Batch Loss: 0.009792596101760864\n",
      "Epoch 4661, Loss: 0.05419092811644077, Final Batch Loss: 0.008512122556567192\n",
      "Epoch 4662, Loss: 0.057972109876573086, Final Batch Loss: 0.05130346119403839\n",
      "Epoch 4663, Loss: 0.017963016405701637, Final Batch Loss: 0.010124382562935352\n",
      "Epoch 4664, Loss: 0.01013195002451539, Final Batch Loss: 0.005809424445033073\n",
      "Epoch 4665, Loss: 0.020156229846179485, Final Batch Loss: 0.011922853998839855\n",
      "Epoch 4666, Loss: 0.01678217714652419, Final Batch Loss: 0.012147924862802029\n",
      "Epoch 4667, Loss: 0.007982830400578678, Final Batch Loss: 0.0017613278469070792\n",
      "Epoch 4668, Loss: 0.010744421277195215, Final Batch Loss: 0.0031880801543593407\n",
      "Epoch 4669, Loss: 0.03231773478910327, Final Batch Loss: 0.025888418778777122\n",
      "Epoch 4670, Loss: 0.013710183557122946, Final Batch Loss: 0.00847349502146244\n",
      "Epoch 4671, Loss: 0.022977006854489446, Final Batch Loss: 0.002409330802038312\n",
      "Epoch 4672, Loss: 0.013519021682441235, Final Batch Loss: 0.00655644154176116\n",
      "Epoch 4673, Loss: 0.014125863322988153, Final Batch Loss: 0.0023877157364040613\n",
      "Epoch 4674, Loss: 0.01839768816716969, Final Batch Loss: 0.0020526957232505083\n",
      "Epoch 4675, Loss: 0.02940281294286251, Final Batch Loss: 0.018084388226270676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4676, Loss: 0.02170246746391058, Final Batch Loss: 0.012469543144106865\n",
      "Epoch 4677, Loss: 0.024748736061155796, Final Batch Loss: 0.0030968161299824715\n",
      "Epoch 4678, Loss: 0.0343352637719363, Final Batch Loss: 0.0022960014175623655\n",
      "Epoch 4679, Loss: 0.02745119296014309, Final Batch Loss: 0.005754653364419937\n",
      "Epoch 4680, Loss: 0.05284066451713443, Final Batch Loss: 0.04764990136027336\n",
      "Epoch 4681, Loss: 0.04447172209620476, Final Batch Loss: 0.01693810150027275\n",
      "Epoch 4682, Loss: 0.011436871718615294, Final Batch Loss: 0.006268599536269903\n",
      "Epoch 4683, Loss: 0.014367723953910172, Final Batch Loss: 0.001671253819949925\n",
      "Epoch 4684, Loss: 0.022215277887880802, Final Batch Loss: 0.012517857365310192\n",
      "Epoch 4685, Loss: 0.028956088237464428, Final Batch Loss: 0.016192171722650528\n",
      "Epoch 4686, Loss: 0.007734199520200491, Final Batch Loss: 0.004439669661223888\n",
      "Epoch 4687, Loss: 0.018504141829907894, Final Batch Loss: 0.0058763958513736725\n",
      "Epoch 4688, Loss: 0.024804630782455206, Final Batch Loss: 0.0055094207637012005\n",
      "Epoch 4689, Loss: 0.013147338526323438, Final Batch Loss: 0.009575768373906612\n",
      "Epoch 4690, Loss: 0.010465491097420454, Final Batch Loss: 0.006445939186960459\n",
      "Epoch 4691, Loss: 0.011308301705867052, Final Batch Loss: 0.00288620637729764\n",
      "Epoch 4692, Loss: 0.043179791420698166, Final Batch Loss: 0.009558569639921188\n",
      "Epoch 4693, Loss: 0.023721315898001194, Final Batch Loss: 0.013613873161375523\n",
      "Epoch 4694, Loss: 0.015985596226528287, Final Batch Loss: 0.002186666475608945\n",
      "Epoch 4695, Loss: 0.007617460563778877, Final Batch Loss: 0.003261967096477747\n",
      "Epoch 4696, Loss: 0.06409084796905518, Final Batch Loss: 0.04281871020793915\n",
      "Epoch 4697, Loss: 0.042893966659903526, Final Batch Loss: 0.009367892518639565\n",
      "Epoch 4698, Loss: 0.019423909951001406, Final Batch Loss: 0.006531611550599337\n",
      "Epoch 4699, Loss: 0.024175917729735374, Final Batch Loss: 0.009460202418267727\n",
      "Epoch 4700, Loss: 0.011522987391799688, Final Batch Loss: 0.0062563433311879635\n",
      "Epoch 4701, Loss: 0.03464734088629484, Final Batch Loss: 0.009327067993581295\n",
      "Epoch 4702, Loss: 0.03857278754003346, Final Batch Loss: 0.0037194148171693087\n",
      "Epoch 4703, Loss: 0.019992963410913944, Final Batch Loss: 0.010612917132675648\n",
      "Epoch 4704, Loss: 0.007888828287832439, Final Batch Loss: 0.001261750585399568\n",
      "Epoch 4705, Loss: 0.012858594302088022, Final Batch Loss: 0.006098112557083368\n",
      "Epoch 4706, Loss: 0.058662289287894964, Final Batch Loss: 0.05523517355322838\n",
      "Epoch 4707, Loss: 0.006267470889724791, Final Batch Loss: 0.0019253703067079186\n",
      "Epoch 4708, Loss: 0.01600684178993106, Final Batch Loss: 0.004672914277762175\n",
      "Epoch 4709, Loss: 0.024224916007369757, Final Batch Loss: 0.007614468690007925\n",
      "Epoch 4710, Loss: 0.09078294457867742, Final Batch Loss: 0.08623093366622925\n",
      "Epoch 4711, Loss: 0.014272072818130255, Final Batch Loss: 0.005719469394534826\n",
      "Epoch 4712, Loss: 0.013393606524914503, Final Batch Loss: 0.001959738787263632\n",
      "Epoch 4713, Loss: 0.01328565739095211, Final Batch Loss: 0.004133990034461021\n",
      "Epoch 4714, Loss: 0.026863406412303448, Final Batch Loss: 0.017013108357787132\n",
      "Epoch 4715, Loss: 0.023620942374691367, Final Batch Loss: 0.00325490883551538\n",
      "Epoch 4716, Loss: 0.011631658067926764, Final Batch Loss: 0.0019801973830908537\n",
      "Epoch 4717, Loss: 0.006373892421834171, Final Batch Loss: 0.0010415049036964774\n",
      "Epoch 4718, Loss: 0.04347163811326027, Final Batch Loss: 0.032482556998729706\n",
      "Epoch 4719, Loss: 0.019021012354642153, Final Batch Loss: 0.014536346308887005\n",
      "Epoch 4720, Loss: 0.019696648232638836, Final Batch Loss: 0.004822974093258381\n",
      "Epoch 4721, Loss: 0.013180139474570751, Final Batch Loss: 0.004938301630318165\n",
      "Epoch 4722, Loss: 0.014538164716213942, Final Batch Loss: 0.0035909167490899563\n",
      "Epoch 4723, Loss: 0.02347744069993496, Final Batch Loss: 0.016516702249646187\n",
      "Epoch 4724, Loss: 0.008826707256957889, Final Batch Loss: 0.003810328198596835\n",
      "Epoch 4725, Loss: 0.02418652828782797, Final Batch Loss: 0.011425483040511608\n",
      "Epoch 4726, Loss: 0.027725336141884327, Final Batch Loss: 0.003826661966741085\n",
      "Epoch 4727, Loss: 0.01154530979692936, Final Batch Loss: 0.004817040637135506\n",
      "Epoch 4728, Loss: 0.01255651842802763, Final Batch Loss: 0.005518405698239803\n",
      "Epoch 4729, Loss: 0.028635184280574322, Final Batch Loss: 0.014590981416404247\n",
      "Epoch 4730, Loss: 0.012123615015298128, Final Batch Loss: 0.007528111804276705\n",
      "Epoch 4731, Loss: 0.028356053866446018, Final Batch Loss: 0.020956071093678474\n",
      "Epoch 4732, Loss: 0.019977892749011517, Final Batch Loss: 0.005100026726722717\n",
      "Epoch 4733, Loss: 0.017555271741002798, Final Batch Loss: 0.00771810719743371\n",
      "Epoch 4734, Loss: 0.013054560869932175, Final Batch Loss: 0.004501824267208576\n",
      "Epoch 4735, Loss: 0.02247238764539361, Final Batch Loss: 0.005668459925800562\n",
      "Epoch 4736, Loss: 0.016351616475731134, Final Batch Loss: 0.010048994794487953\n",
      "Epoch 4737, Loss: 0.015493975952267647, Final Batch Loss: 0.010248971171677113\n",
      "Epoch 4738, Loss: 0.00967214535921812, Final Batch Loss: 0.005590913817286491\n",
      "Epoch 4739, Loss: 0.01697011385113001, Final Batch Loss: 0.00762857124209404\n",
      "Epoch 4740, Loss: 0.21438110899180174, Final Batch Loss: 0.210989847779274\n",
      "Epoch 4741, Loss: 0.03666517976671457, Final Batch Loss: 0.008363769389688969\n",
      "Epoch 4742, Loss: 0.014613242354243994, Final Batch Loss: 0.005066366400569677\n",
      "Epoch 4743, Loss: 0.04181003198027611, Final Batch Loss: 0.01783398725092411\n",
      "Epoch 4744, Loss: 0.02858020830899477, Final Batch Loss: 0.012150737456977367\n",
      "Epoch 4745, Loss: 0.01878889137879014, Final Batch Loss: 0.005528538022190332\n",
      "Epoch 4746, Loss: 0.023727786727249622, Final Batch Loss: 0.011642878875136375\n",
      "Epoch 4747, Loss: 0.012596319895237684, Final Batch Loss: 0.004142481368035078\n",
      "Epoch 4748, Loss: 0.022479940205812454, Final Batch Loss: 0.0055272746831178665\n",
      "Epoch 4749, Loss: 0.01994827395537868, Final Batch Loss: 0.0008515921072103083\n",
      "Epoch 4750, Loss: 0.007273231167346239, Final Batch Loss: 0.0023867907002568245\n",
      "Epoch 4751, Loss: 0.012241506250575185, Final Batch Loss: 0.0031798614654690027\n",
      "Epoch 4752, Loss: 0.009133340558037162, Final Batch Loss: 0.003175443736836314\n",
      "Epoch 4753, Loss: 0.024300767108798027, Final Batch Loss: 0.009583928622305393\n",
      "Epoch 4754, Loss: 0.01910681719891727, Final Batch Loss: 0.002900735242292285\n",
      "Epoch 4755, Loss: 0.02503139805048704, Final Batch Loss: 0.007025974802672863\n",
      "Epoch 4756, Loss: 0.07673599757254124, Final Batch Loss: 0.0573275089263916\n",
      "Epoch 4757, Loss: 0.01946419198065996, Final Batch Loss: 0.01070262398570776\n",
      "Epoch 4758, Loss: 0.008794528432190418, Final Batch Loss: 0.0023912163451313972\n",
      "Epoch 4759, Loss: 0.02499461080878973, Final Batch Loss: 0.002177317626774311\n",
      "Epoch 4760, Loss: 0.015653519425541162, Final Batch Loss: 0.007119899149984121\n",
      "Epoch 4761, Loss: 0.013469460885971785, Final Batch Loss: 0.0020823650993406773\n",
      "Epoch 4762, Loss: 0.009982496965676546, Final Batch Loss: 0.0017080861143767834\n",
      "Epoch 4763, Loss: 0.016442916356027126, Final Batch Loss: 0.012231910601258278\n",
      "Epoch 4764, Loss: 0.018587332218885422, Final Batch Loss: 0.009243655949831009\n",
      "Epoch 4765, Loss: 0.008735201321542263, Final Batch Loss: 0.004863289184868336\n",
      "Epoch 4766, Loss: 0.013570931740105152, Final Batch Loss: 0.005450209602713585\n",
      "Epoch 4767, Loss: 0.008814382366836071, Final Batch Loss: 0.0017775045707821846\n",
      "Epoch 4768, Loss: 0.023730622604489326, Final Batch Loss: 0.006504630669951439\n",
      "Epoch 4769, Loss: 0.045340174343436956, Final Batch Loss: 0.005594776477664709\n",
      "Epoch 4770, Loss: 0.028521394822746515, Final Batch Loss: 0.003913574386388063\n",
      "Epoch 4771, Loss: 0.016004251781851053, Final Batch Loss: 0.007658796850591898\n",
      "Epoch 4772, Loss: 0.028250408358871937, Final Batch Loss: 0.016827963292598724\n",
      "Epoch 4773, Loss: 0.011888500768691301, Final Batch Loss: 0.007145428564399481\n",
      "Epoch 4774, Loss: 0.008860793896019459, Final Batch Loss: 0.003748535178601742\n",
      "Epoch 4775, Loss: 0.011262095998972654, Final Batch Loss: 0.005847467575222254\n",
      "Epoch 4776, Loss: 0.011755850864574313, Final Batch Loss: 0.0029200443532317877\n",
      "Epoch 4777, Loss: 0.006932183285243809, Final Batch Loss: 0.001948245451785624\n",
      "Epoch 4778, Loss: 0.0478347297757864, Final Batch Loss: 0.035626377910375595\n",
      "Epoch 4779, Loss: 0.006836161715909839, Final Batch Loss: 0.001577639253810048\n",
      "Epoch 4780, Loss: 0.010442843427881598, Final Batch Loss: 0.0028864273335784674\n",
      "Epoch 4781, Loss: 0.021718484349548817, Final Batch Loss: 0.008084344677627087\n",
      "Epoch 4782, Loss: 0.03975108196027577, Final Batch Loss: 0.0037430443335324526\n",
      "Epoch 4783, Loss: 0.033262368058785796, Final Batch Loss: 0.029880989342927933\n",
      "Epoch 4784, Loss: 0.011755219893530011, Final Batch Loss: 0.007872700691223145\n",
      "Epoch 4785, Loss: 0.049702840857207775, Final Batch Loss: 0.03847045451402664\n",
      "Epoch 4786, Loss: 0.017401553690433502, Final Batch Loss: 0.004620956256985664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4787, Loss: 0.007622437318786979, Final Batch Loss: 0.004032039549201727\n",
      "Epoch 4788, Loss: 0.015633638482540846, Final Batch Loss: 0.0065560718066990376\n",
      "Epoch 4789, Loss: 0.0298927859403193, Final Batch Loss: 0.02450314350426197\n",
      "Epoch 4790, Loss: 0.010138883721083403, Final Batch Loss: 0.00394267775118351\n",
      "Epoch 4791, Loss: 0.019383170874789357, Final Batch Loss: 0.0021095287520438433\n",
      "Epoch 4792, Loss: 0.03018774464726448, Final Batch Loss: 0.021113183349370956\n",
      "Epoch 4793, Loss: 0.020962849259376526, Final Batch Loss: 0.011864950880408287\n",
      "Epoch 4794, Loss: 0.034352818969637156, Final Batch Loss: 0.005605789367109537\n",
      "Epoch 4795, Loss: 0.009084594901651144, Final Batch Loss: 0.004857964348047972\n",
      "Epoch 4796, Loss: 0.011196876410394907, Final Batch Loss: 0.00242116441950202\n",
      "Epoch 4797, Loss: 0.07223477913066745, Final Batch Loss: 0.06576424092054367\n",
      "Epoch 4798, Loss: 0.02601418551057577, Final Batch Loss: 0.01942754164338112\n",
      "Epoch 4799, Loss: 0.012088938150554895, Final Batch Loss: 0.007105374708771706\n",
      "Epoch 4800, Loss: 0.02326018735766411, Final Batch Loss: 0.007954194210469723\n",
      "Epoch 4801, Loss: 0.1149027505889535, Final Batch Loss: 0.0999603122472763\n",
      "Epoch 4802, Loss: 0.023569051176309586, Final Batch Loss: 0.009510415606200695\n",
      "Epoch 4803, Loss: 0.011019804747775197, Final Batch Loss: 0.002103587845340371\n",
      "Epoch 4804, Loss: 0.006352157914079726, Final Batch Loss: 0.0013218895765021443\n",
      "Epoch 4805, Loss: 0.04745703563094139, Final Batch Loss: 0.004703555256128311\n",
      "Epoch 4806, Loss: 0.009682900039479136, Final Batch Loss: 0.0022510283160954714\n",
      "Epoch 4807, Loss: 0.0688543263822794, Final Batch Loss: 0.05715586990118027\n",
      "Epoch 4808, Loss: 0.03494052775204182, Final Batch Loss: 0.031652987003326416\n",
      "Epoch 4809, Loss: 0.016599228838458657, Final Batch Loss: 0.002939086640253663\n",
      "Epoch 4810, Loss: 0.014403349254280329, Final Batch Loss: 0.006683367770165205\n",
      "Epoch 4811, Loss: 0.015185311436653137, Final Batch Loss: 0.010540223680436611\n",
      "Epoch 4812, Loss: 0.012433427502401173, Final Batch Loss: 0.0017842781962826848\n",
      "Epoch 4813, Loss: 0.07388242892920971, Final Batch Loss: 0.05239065736532211\n",
      "Epoch 4814, Loss: 0.0264586778357625, Final Batch Loss: 0.010201369412243366\n",
      "Epoch 4815, Loss: 0.018060251139104366, Final Batch Loss: 0.009466161951422691\n",
      "Epoch 4816, Loss: 0.029101213440299034, Final Batch Loss: 0.01754964515566826\n",
      "Epoch 4817, Loss: 0.013111930573359132, Final Batch Loss: 0.002471980871632695\n",
      "Epoch 4818, Loss: 0.005966591881588101, Final Batch Loss: 0.0014869633596390486\n",
      "Epoch 4819, Loss: 0.010550095699727535, Final Batch Loss: 0.003962495364248753\n",
      "Epoch 4820, Loss: 0.011730057187378407, Final Batch Loss: 0.003270828165113926\n",
      "Epoch 4821, Loss: 0.009012121008709073, Final Batch Loss: 0.0022350025828927755\n",
      "Epoch 4822, Loss: 0.026413919869810343, Final Batch Loss: 0.020062027499079704\n",
      "Epoch 4823, Loss: 0.019023679196834564, Final Batch Loss: 0.0022662747651338577\n",
      "Epoch 4824, Loss: 0.007601289544254541, Final Batch Loss: 0.0031991619616746902\n",
      "Epoch 4825, Loss: 0.008245532866567373, Final Batch Loss: 0.003926075994968414\n",
      "Epoch 4826, Loss: 0.01032667001709342, Final Batch Loss: 0.004321407526731491\n",
      "Epoch 4827, Loss: 0.08329331874847412, Final Batch Loss: 0.0724361464381218\n",
      "Epoch 4828, Loss: 0.02210146700963378, Final Batch Loss: 0.0038613690994679928\n",
      "Epoch 4829, Loss: 0.12305804342031479, Final Batch Loss: 0.046999379992485046\n",
      "Epoch 4830, Loss: 0.023144480772316456, Final Batch Loss: 0.00790386088192463\n",
      "Epoch 4831, Loss: 0.019820494577288628, Final Batch Loss: 0.011773540638387203\n",
      "Epoch 4832, Loss: 0.013074098154902458, Final Batch Loss: 0.002959262579679489\n",
      "Epoch 4833, Loss: 0.009117529494687915, Final Batch Loss: 0.005471284035593271\n",
      "Epoch 4834, Loss: 0.011989978142082691, Final Batch Loss: 0.004038706421852112\n",
      "Epoch 4835, Loss: 0.01780092716217041, Final Batch Loss: 0.013159209862351418\n",
      "Epoch 4836, Loss: 0.1309586949646473, Final Batch Loss: 0.1080162525177002\n",
      "Epoch 4837, Loss: 0.015285904053598642, Final Batch Loss: 0.00498240115121007\n",
      "Epoch 4838, Loss: 0.010333789512515068, Final Batch Loss: 0.006660844665020704\n",
      "Epoch 4839, Loss: 0.0960669806227088, Final Batch Loss: 0.0029140179976820946\n",
      "Epoch 4840, Loss: 0.015565912239253521, Final Batch Loss: 0.00839839968830347\n",
      "Epoch 4841, Loss: 0.0072925027925521135, Final Batch Loss: 0.00228957855142653\n",
      "Epoch 4842, Loss: 0.015352059155702591, Final Batch Loss: 0.007893524132668972\n",
      "Epoch 4843, Loss: 0.022616436704993248, Final Batch Loss: 0.014459953643381596\n",
      "Epoch 4844, Loss: 0.008999315090477467, Final Batch Loss: 0.0037149088457226753\n",
      "Epoch 4845, Loss: 0.014996376819908619, Final Batch Loss: 0.005215467885136604\n",
      "Epoch 4846, Loss: 0.03884748951531947, Final Batch Loss: 0.003316177288070321\n",
      "Epoch 4847, Loss: 0.03338669240474701, Final Batch Loss: 0.014149488881230354\n",
      "Epoch 4848, Loss: 0.01624920265749097, Final Batch Loss: 0.0048270816914737225\n",
      "Epoch 4849, Loss: 0.008253872278146446, Final Batch Loss: 0.006642645224928856\n",
      "Epoch 4850, Loss: 0.02077324641868472, Final Batch Loss: 0.01510640699416399\n",
      "Epoch 4851, Loss: 0.03770116064697504, Final Batch Loss: 0.010136635042726994\n",
      "Epoch 4852, Loss: 0.017847876995801926, Final Batch Loss: 0.004596978425979614\n",
      "Epoch 4853, Loss: 0.051534704864025116, Final Batch Loss: 0.033287644386291504\n",
      "Epoch 4854, Loss: 0.0768822468817234, Final Batch Loss: 0.0361984558403492\n",
      "Epoch 4855, Loss: 0.010016205254942179, Final Batch Loss: 0.0044764201156795025\n",
      "Epoch 4856, Loss: 0.02429025713354349, Final Batch Loss: 0.006696292199194431\n",
      "Epoch 4857, Loss: 0.013048962922766805, Final Batch Loss: 0.003832052694633603\n",
      "Epoch 4858, Loss: 0.01311567984521389, Final Batch Loss: 0.0051588620990514755\n",
      "Epoch 4859, Loss: 0.008995113894343376, Final Batch Loss: 0.005281074438244104\n",
      "Epoch 4860, Loss: 0.02131179114803672, Final Batch Loss: 0.013516352511942387\n",
      "Epoch 4861, Loss: 0.010104851797223091, Final Batch Loss: 0.0020344629883766174\n",
      "Epoch 4862, Loss: 0.007721489411778748, Final Batch Loss: 0.0013941848883405328\n",
      "Epoch 4863, Loss: 0.03163008112460375, Final Batch Loss: 0.01865481398999691\n",
      "Epoch 4864, Loss: 0.030575371347367764, Final Batch Loss: 0.010884235613048077\n",
      "Epoch 4865, Loss: 0.03682153392583132, Final Batch Loss: 0.032451096922159195\n",
      "Epoch 4866, Loss: 0.04609960597008467, Final Batch Loss: 0.0035438640043139458\n",
      "Epoch 4867, Loss: 0.008558915695175529, Final Batch Loss: 0.002039931947365403\n",
      "Epoch 4868, Loss: 0.026264515705406666, Final Batch Loss: 0.01927591674029827\n",
      "Epoch 4869, Loss: 0.014685736503452063, Final Batch Loss: 0.006625782232731581\n",
      "Epoch 4870, Loss: 0.00870078383013606, Final Batch Loss: 0.004400588572025299\n",
      "Epoch 4871, Loss: 0.05846778862178326, Final Batch Loss: 0.04219996929168701\n",
      "Epoch 4872, Loss: 0.024067587219178677, Final Batch Loss: 0.008733048103749752\n",
      "Epoch 4873, Loss: 0.010984949301928282, Final Batch Loss: 0.0043176934123039246\n",
      "Epoch 4874, Loss: 0.02077525481581688, Final Batch Loss: 0.004900945350527763\n",
      "Epoch 4875, Loss: 0.06144383363425732, Final Batch Loss: 0.058276958763599396\n",
      "Epoch 4876, Loss: 0.019140345859341323, Final Batch Loss: 0.0012683476088568568\n",
      "Epoch 4877, Loss: 0.010563990101218224, Final Batch Loss: 0.0043722293339669704\n",
      "Epoch 4878, Loss: 0.02601023530587554, Final Batch Loss: 0.021212948486208916\n",
      "Epoch 4879, Loss: 0.018347835633903742, Final Batch Loss: 0.0024642408825457096\n",
      "Epoch 4880, Loss: 0.026509365998208523, Final Batch Loss: 0.014696906320750713\n",
      "Epoch 4881, Loss: 0.02361825807020068, Final Batch Loss: 0.006361910607665777\n",
      "Epoch 4882, Loss: 0.016329305712133646, Final Batch Loss: 0.005296282935887575\n",
      "Epoch 4883, Loss: 0.01594935031607747, Final Batch Loss: 0.004616887774318457\n",
      "Epoch 4884, Loss: 0.022390310652554035, Final Batch Loss: 0.0022962605580687523\n",
      "Epoch 4885, Loss: 0.041343191638588905, Final Batch Loss: 0.032283782958984375\n",
      "Epoch 4886, Loss: 0.01007959246635437, Final Batch Loss: 0.0028328169137239456\n",
      "Epoch 4887, Loss: 0.011128957383334637, Final Batch Loss: 0.005514324177056551\n",
      "Epoch 4888, Loss: 0.014461440034210682, Final Batch Loss: 0.004663537256419659\n",
      "Epoch 4889, Loss: 0.030614641495049, Final Batch Loss: 0.005078726448118687\n",
      "Epoch 4890, Loss: 0.02963116392493248, Final Batch Loss: 0.01492681447416544\n",
      "Epoch 4891, Loss: 0.017899210331961513, Final Batch Loss: 0.0031609281431883574\n",
      "Epoch 4892, Loss: 0.035853857174515724, Final Batch Loss: 0.008953148499131203\n",
      "Epoch 4893, Loss: 0.030909311026334763, Final Batch Loss: 0.0057020168751478195\n",
      "Epoch 4894, Loss: 0.008691946510225534, Final Batch Loss: 0.004195523913949728\n",
      "Epoch 4895, Loss: 0.024616789072752, Final Batch Loss: 0.00707944855093956\n",
      "Epoch 4896, Loss: 0.012252314016222954, Final Batch Loss: 0.007023169193416834\n",
      "Epoch 4897, Loss: 0.02034035511314869, Final Batch Loss: 0.009306292980909348\n",
      "Epoch 4898, Loss: 0.010607638396322727, Final Batch Loss: 0.004784049466252327\n",
      "Epoch 4899, Loss: 0.017820353619754314, Final Batch Loss: 0.006927422247827053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4900, Loss: 0.011063859332352877, Final Batch Loss: 0.006123675033450127\n",
      "Epoch 4901, Loss: 0.03161316458135843, Final Batch Loss: 0.010086451657116413\n",
      "Epoch 4902, Loss: 0.033180566504597664, Final Batch Loss: 0.017816094681620598\n",
      "Epoch 4903, Loss: 0.007738381274975836, Final Batch Loss: 0.0013218960957601666\n",
      "Epoch 4904, Loss: 0.024531796108931303, Final Batch Loss: 0.0022759917192161083\n",
      "Epoch 4905, Loss: 0.018468664959073067, Final Batch Loss: 0.003971543163061142\n",
      "Epoch 4906, Loss: 0.025271897204220295, Final Batch Loss: 0.009757359512150288\n",
      "Epoch 4907, Loss: 0.013554560951888561, Final Batch Loss: 0.0021198326721787453\n",
      "Epoch 4908, Loss: 0.023295882623642683, Final Batch Loss: 0.016993563622236252\n",
      "Epoch 4909, Loss: 0.01353152608498931, Final Batch Loss: 0.009072506800293922\n",
      "Epoch 4910, Loss: 0.014622297137975693, Final Batch Loss: 0.0030295420438051224\n",
      "Epoch 4911, Loss: 0.006942389765754342, Final Batch Loss: 0.0018031119834631681\n",
      "Epoch 4912, Loss: 0.010925285052508116, Final Batch Loss: 0.005770587362349033\n",
      "Epoch 4913, Loss: 0.010229120962321758, Final Batch Loss: 0.0041277362033724785\n",
      "Epoch 4914, Loss: 0.0075216086115688086, Final Batch Loss: 0.0017651484813541174\n",
      "Epoch 4915, Loss: 0.013000064063817263, Final Batch Loss: 0.007838248275220394\n",
      "Epoch 4916, Loss: 0.006428093649446964, Final Batch Loss: 0.0033107108902186155\n",
      "Epoch 4917, Loss: 0.0296173058450222, Final Batch Loss: 0.007949938997626305\n",
      "Epoch 4918, Loss: 0.024244940374046564, Final Batch Loss: 0.02041202038526535\n",
      "Epoch 4919, Loss: 0.03181486576795578, Final Batch Loss: 0.023786069825291634\n",
      "Epoch 4920, Loss: 0.019226128235459328, Final Batch Loss: 0.006032437086105347\n",
      "Epoch 4921, Loss: 0.02878421009518206, Final Batch Loss: 0.003313217544928193\n",
      "Epoch 4922, Loss: 0.008375475881621242, Final Batch Loss: 0.0023129230830818415\n",
      "Epoch 4923, Loss: 0.03020505513995886, Final Batch Loss: 0.012296861968934536\n",
      "Epoch 4924, Loss: 0.012818094808608294, Final Batch Loss: 0.008496720343828201\n",
      "Epoch 4925, Loss: 0.010138925863429904, Final Batch Loss: 0.002242346992716193\n",
      "Epoch 4926, Loss: 0.007555710151791573, Final Batch Loss: 0.0034744408912956715\n",
      "Epoch 4927, Loss: 0.016658217180520296, Final Batch Loss: 0.007234649267047644\n",
      "Epoch 4928, Loss: 0.03112678322941065, Final Batch Loss: 0.022631824016571045\n",
      "Epoch 4929, Loss: 0.08450758596882224, Final Batch Loss: 0.07730147987604141\n",
      "Epoch 4930, Loss: 0.025403395760804415, Final Batch Loss: 0.01813897304236889\n",
      "Epoch 4931, Loss: 0.059992766473442316, Final Batch Loss: 0.006978473160415888\n",
      "Epoch 4932, Loss: 0.019976113922894, Final Batch Loss: 0.011102765798568726\n",
      "Epoch 4933, Loss: 0.01698473235592246, Final Batch Loss: 0.004564605187624693\n",
      "Epoch 4934, Loss: 0.04570393776521087, Final Batch Loss: 0.039809685200452805\n",
      "Epoch 4935, Loss: 0.021124676801264286, Final Batch Loss: 0.01528744027018547\n",
      "Epoch 4936, Loss: 0.015216192696243525, Final Batch Loss: 0.00826253741979599\n",
      "Epoch 4937, Loss: 0.033087399788200855, Final Batch Loss: 0.007402702234685421\n",
      "Epoch 4938, Loss: 0.014221610967069864, Final Batch Loss: 0.002924556378275156\n",
      "Epoch 4939, Loss: 0.012303620111197233, Final Batch Loss: 0.008004821836948395\n",
      "Epoch 4940, Loss: 0.04022304201498628, Final Batch Loss: 0.0031381756998598576\n",
      "Epoch 4941, Loss: 0.015776231652125716, Final Batch Loss: 0.003150528995320201\n",
      "Epoch 4942, Loss: 0.013215857092291117, Final Batch Loss: 0.0077767325565218925\n",
      "Epoch 4943, Loss: 0.046210197266191244, Final Batch Loss: 0.005569977220147848\n",
      "Epoch 4944, Loss: 0.024782421067357063, Final Batch Loss: 0.0032807495445013046\n",
      "Epoch 4945, Loss: 0.013953544897958636, Final Batch Loss: 0.010582423768937588\n",
      "Epoch 4946, Loss: 0.010849597631022334, Final Batch Loss: 0.003068688092753291\n",
      "Epoch 4947, Loss: 0.029217519069788978, Final Batch Loss: 0.0003958737652283162\n",
      "Epoch 4948, Loss: 0.020551281981170177, Final Batch Loss: 0.01195246446877718\n",
      "Epoch 4949, Loss: 0.015618217410519719, Final Batch Loss: 0.0013734011445194483\n",
      "Epoch 4950, Loss: 0.024669221602380276, Final Batch Loss: 0.019481506198644638\n",
      "Epoch 4951, Loss: 0.08234352059662342, Final Batch Loss: 0.07849983870983124\n",
      "Epoch 4952, Loss: 0.020832881331443787, Final Batch Loss: 0.008957303129136562\n",
      "Epoch 4953, Loss: 0.021938339807093143, Final Batch Loss: 0.004263301379978657\n",
      "Epoch 4954, Loss: 0.007630826905369759, Final Batch Loss: 0.002314613666385412\n",
      "Epoch 4955, Loss: 0.007622084580361843, Final Batch Loss: 0.0020593549124896526\n",
      "Epoch 4956, Loss: 0.010335665661841631, Final Batch Loss: 0.00560159794986248\n",
      "Epoch 4957, Loss: 0.03650610032491386, Final Batch Loss: 0.0022996466141194105\n",
      "Epoch 4958, Loss: 0.019359396304935217, Final Batch Loss: 0.012383222579956055\n",
      "Epoch 4959, Loss: 0.01986077637411654, Final Batch Loss: 0.0023275355342775583\n",
      "Epoch 4960, Loss: 0.06241306662559509, Final Batch Loss: 0.031415488570928574\n",
      "Epoch 4961, Loss: 0.012493799207732081, Final Batch Loss: 0.0038039206992834806\n",
      "Epoch 4962, Loss: 0.010609328979626298, Final Batch Loss: 0.0025965787936002016\n",
      "Epoch 4963, Loss: 0.013745443429797888, Final Batch Loss: 0.005123847629874945\n",
      "Epoch 4964, Loss: 0.012692733202129602, Final Batch Loss: 0.008047333918511868\n",
      "Epoch 4965, Loss: 0.010992305353283882, Final Batch Loss: 0.00652272067964077\n",
      "Epoch 4966, Loss: 0.01566487643867731, Final Batch Loss: 0.006588168442249298\n",
      "Epoch 4967, Loss: 0.007702229544520378, Final Batch Loss: 0.003995909821242094\n",
      "Epoch 4968, Loss: 0.013565014116466045, Final Batch Loss: 0.006942883599549532\n",
      "Epoch 4969, Loss: 0.05216844938695431, Final Batch Loss: 0.013631129637360573\n",
      "Epoch 4970, Loss: 0.017052627401426435, Final Batch Loss: 0.002497136825695634\n",
      "Epoch 4971, Loss: 0.008250547398347408, Final Batch Loss: 0.00043144152732566\n",
      "Epoch 4972, Loss: 0.04310756130144, Final Batch Loss: 0.004185560625046492\n",
      "Epoch 4973, Loss: 0.01548730256035924, Final Batch Loss: 0.00892669428139925\n",
      "Epoch 4974, Loss: 0.02426516730338335, Final Batch Loss: 0.010976103134453297\n",
      "Epoch 4975, Loss: 0.020361304515972733, Final Batch Loss: 0.016505224630236626\n",
      "Epoch 4976, Loss: 0.018817314528860152, Final Batch Loss: 0.0009315606439486146\n",
      "Epoch 4977, Loss: 0.006575404433533549, Final Batch Loss: 0.0028526312671601772\n",
      "Epoch 4978, Loss: 0.017887394060380757, Final Batch Loss: 0.0014807089464738965\n",
      "Epoch 4979, Loss: 0.033055747393518686, Final Batch Loss: 0.028334110975265503\n",
      "Epoch 4980, Loss: 0.02499058935791254, Final Batch Loss: 0.018163738772273064\n",
      "Epoch 4981, Loss: 0.046501340344548225, Final Batch Loss: 0.025337520986795425\n",
      "Epoch 4982, Loss: 0.02437737723812461, Final Batch Loss: 0.005277314689010382\n",
      "Epoch 4983, Loss: 0.028098090086132288, Final Batch Loss: 0.023938164114952087\n",
      "Epoch 4984, Loss: 0.019452382810413837, Final Batch Loss: 0.008368775248527527\n",
      "Epoch 4985, Loss: 0.019664158578962088, Final Batch Loss: 0.01265470590442419\n",
      "Epoch 4986, Loss: 0.007098827278241515, Final Batch Loss: 0.0012876300606876612\n",
      "Epoch 4987, Loss: 0.010140841361135244, Final Batch Loss: 0.004704829305410385\n",
      "Epoch 4988, Loss: 0.018297779373824596, Final Batch Loss: 0.00936257652938366\n",
      "Epoch 4989, Loss: 0.033436816185712814, Final Batch Loss: 0.0037963800132274628\n",
      "Epoch 4990, Loss: 0.0294285099953413, Final Batch Loss: 0.022389965131878853\n",
      "Epoch 4991, Loss: 0.007083993055857718, Final Batch Loss: 0.001243133214302361\n",
      "Epoch 4992, Loss: 0.014709584880620241, Final Batch Loss: 0.0022571790032088757\n",
      "Epoch 4993, Loss: 0.01288458751514554, Final Batch Loss: 0.005916440859436989\n",
      "Epoch 4994, Loss: 0.014325325842946768, Final Batch Loss: 0.00489310035482049\n",
      "Epoch 4995, Loss: 0.016870310995727777, Final Batch Loss: 0.010114734061062336\n",
      "Epoch 4996, Loss: 0.031424483051523566, Final Batch Loss: 0.029238145798444748\n",
      "Epoch 4997, Loss: 0.01469047018326819, Final Batch Loss: 0.0023503548000007868\n",
      "Epoch 4998, Loss: 0.02057465771213174, Final Batch Loss: 0.013162963092327118\n",
      "Epoch 4999, Loss: 0.009290568996220827, Final Batch Loss: 0.007028444204479456\n",
      "Epoch 5000, Loss: 0.0343108419328928, Final Batch Loss: 0.028219709172844887\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  1  3  0  0  0  0  0]\n",
      " [ 0  6  0  0  3  0  0  2  0]\n",
      " [ 0  0  5  0  0  3  0  0  1]\n",
      " [ 0  0  0  9  0  0  0  0  0]\n",
      " [ 1  2  0  0  6  0  0  1  0]\n",
      " [ 0  0  2  0  0  5  0  0  4]\n",
      " [ 0  0  0  0  0  0 12  3  0]\n",
      " [ 0  1  0  0  0  0  0  8  0]\n",
      " [ 0  0  5  0  0  2  0  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.923     0.750     0.828        16\n",
      "           1      0.667     0.545     0.600        11\n",
      "           2      0.385     0.556     0.455         9\n",
      "           3      0.750     1.000     0.857         9\n",
      "           4      0.667     0.600     0.632        10\n",
      "           5      0.500     0.455     0.476        11\n",
      "           6      1.000     0.800     0.889        15\n",
      "           7      0.571     0.889     0.696         9\n",
      "           8      0.375     0.300     0.333        10\n",
      "\n",
      "    accuracy                          0.660       100\n",
      "   macro avg      0.649     0.655     0.641       100\n",
      "weighted avg      0.684     0.660     0.661       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16+11+9+10+15+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12+6+9+6+12+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7571428571428571"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
