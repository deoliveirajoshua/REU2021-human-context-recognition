{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A0 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_1 = gen(to_gen).detach().numpy()\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A1 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_2 = gen(to_gen).detach().numpy()\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A2 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_3 = gen(to_gen).detach().numpy()\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A0 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_4 = gen(to_gen).detach().numpy()\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A1 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_5 = gen(to_gen).detach().numpy()\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A2 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_6 = gen(to_gen).detach().numpy()\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A0 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_7 = gen(to_gen).detach().numpy()\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A1 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_8 = gen(to_gen).detach().numpy()\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A2 Excluded Group 1_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_9 = gen(to_gen).detach().numpy()\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_test = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_test = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "X_train, y_train = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.436426877975464, Final Batch Loss: 2.2291717529296875\n",
      "Epoch 2, Loss: 4.4310455322265625, Final Batch Loss: 2.230130195617676\n",
      "Epoch 3, Loss: 4.4253387451171875, Final Batch Loss: 2.2210352420806885\n",
      "Epoch 4, Loss: 4.424116134643555, Final Batch Loss: 2.2234346866607666\n",
      "Epoch 5, Loss: 4.4197587966918945, Final Batch Loss: 2.2062106132507324\n",
      "Epoch 6, Loss: 4.418846607208252, Final Batch Loss: 2.2107017040252686\n",
      "Epoch 7, Loss: 4.4090735912323, Final Batch Loss: 2.2020959854125977\n",
      "Epoch 8, Loss: 4.403632402420044, Final Batch Loss: 2.1967828273773193\n",
      "Epoch 9, Loss: 4.403399705886841, Final Batch Loss: 2.1970789432525635\n",
      "Epoch 10, Loss: 4.403429985046387, Final Batch Loss: 2.209475040435791\n",
      "Epoch 11, Loss: 4.398860931396484, Final Batch Loss: 2.2025015354156494\n",
      "Epoch 12, Loss: 4.395235538482666, Final Batch Loss: 2.1900064945220947\n",
      "Epoch 13, Loss: 4.38377046585083, Final Batch Loss: 2.1911420822143555\n",
      "Epoch 14, Loss: 4.38370418548584, Final Batch Loss: 2.1901049613952637\n",
      "Epoch 15, Loss: 4.38016414642334, Final Batch Loss: 2.2034687995910645\n",
      "Epoch 16, Loss: 4.370011568069458, Final Batch Loss: 2.1885595321655273\n",
      "Epoch 17, Loss: 4.374059438705444, Final Batch Loss: 2.1882693767547607\n",
      "Epoch 18, Loss: 4.363377332687378, Final Batch Loss: 2.190803289413452\n",
      "Epoch 19, Loss: 4.360401630401611, Final Batch Loss: 2.1897621154785156\n",
      "Epoch 20, Loss: 4.3574042320251465, Final Batch Loss: 2.1770424842834473\n",
      "Epoch 21, Loss: 4.349050283432007, Final Batch Loss: 2.1907236576080322\n",
      "Epoch 22, Loss: 4.335981130599976, Final Batch Loss: 2.162790060043335\n",
      "Epoch 23, Loss: 4.331324577331543, Final Batch Loss: 2.1756372451782227\n",
      "Epoch 24, Loss: 4.3279008865356445, Final Batch Loss: 2.1604676246643066\n",
      "Epoch 25, Loss: 4.3162877559661865, Final Batch Loss: 2.1405200958251953\n",
      "Epoch 26, Loss: 4.296682596206665, Final Batch Loss: 2.137908458709717\n",
      "Epoch 27, Loss: 4.288594961166382, Final Batch Loss: 2.1454527378082275\n",
      "Epoch 28, Loss: 4.278081655502319, Final Batch Loss: 2.141517400741577\n",
      "Epoch 29, Loss: 4.259321928024292, Final Batch Loss: 2.112259864807129\n",
      "Epoch 30, Loss: 4.232728958129883, Final Batch Loss: 2.1103973388671875\n",
      "Epoch 31, Loss: 4.206886529922485, Final Batch Loss: 2.1131882667541504\n",
      "Epoch 32, Loss: 4.1730663776397705, Final Batch Loss: 2.0772030353546143\n",
      "Epoch 33, Loss: 4.146182060241699, Final Batch Loss: 2.0736539363861084\n",
      "Epoch 34, Loss: 4.108155012130737, Final Batch Loss: 2.042126417160034\n",
      "Epoch 35, Loss: 4.077970027923584, Final Batch Loss: 2.042733669281006\n",
      "Epoch 36, Loss: 4.013147830963135, Final Batch Loss: 2.0056421756744385\n",
      "Epoch 37, Loss: 3.9667876958847046, Final Batch Loss: 1.983862042427063\n",
      "Epoch 38, Loss: 3.911352276802063, Final Batch Loss: 1.9406343698501587\n",
      "Epoch 39, Loss: 3.876500368118286, Final Batch Loss: 1.9264862537384033\n",
      "Epoch 40, Loss: 3.7804369926452637, Final Batch Loss: 1.8740870952606201\n",
      "Epoch 41, Loss: 3.73859179019928, Final Batch Loss: 1.8708220720291138\n",
      "Epoch 42, Loss: 3.6321940422058105, Final Batch Loss: 1.8452633619308472\n",
      "Epoch 43, Loss: 3.5948678255081177, Final Batch Loss: 1.7707908153533936\n",
      "Epoch 44, Loss: 3.4917536973953247, Final Batch Loss: 1.7259193658828735\n",
      "Epoch 45, Loss: 3.431286573410034, Final Batch Loss: 1.6574273109436035\n",
      "Epoch 46, Loss: 3.348055124282837, Final Batch Loss: 1.6436036825180054\n",
      "Epoch 47, Loss: 3.3208314180374146, Final Batch Loss: 1.6367892026901245\n",
      "Epoch 48, Loss: 3.2104673385620117, Final Batch Loss: 1.6108379364013672\n",
      "Epoch 49, Loss: 3.1487066745758057, Final Batch Loss: 1.508973240852356\n",
      "Epoch 50, Loss: 3.08550226688385, Final Batch Loss: 1.5244351625442505\n",
      "Epoch 51, Loss: 3.017472982406616, Final Batch Loss: 1.480076789855957\n",
      "Epoch 52, Loss: 2.888338565826416, Final Batch Loss: 1.4933737516403198\n",
      "Epoch 53, Loss: 2.922142505645752, Final Batch Loss: 1.4883211851119995\n",
      "Epoch 54, Loss: 2.792062520980835, Final Batch Loss: 1.3828232288360596\n",
      "Epoch 55, Loss: 2.73520565032959, Final Batch Loss: 1.357896327972412\n",
      "Epoch 56, Loss: 2.7856106758117676, Final Batch Loss: 1.3789780139923096\n",
      "Epoch 57, Loss: 2.664480686187744, Final Batch Loss: 1.3188632726669312\n",
      "Epoch 58, Loss: 2.6429741382598877, Final Batch Loss: 1.3196995258331299\n",
      "Epoch 59, Loss: 2.6195173263549805, Final Batch Loss: 1.2975108623504639\n",
      "Epoch 60, Loss: 2.559582471847534, Final Batch Loss: 1.2748628854751587\n",
      "Epoch 61, Loss: 2.5882374048233032, Final Batch Loss: 1.2901842594146729\n",
      "Epoch 62, Loss: 2.4814541339874268, Final Batch Loss: 1.2477390766143799\n",
      "Epoch 63, Loss: 2.4746869802474976, Final Batch Loss: 1.2414016723632812\n",
      "Epoch 64, Loss: 2.4174033403396606, Final Batch Loss: 1.1959285736083984\n",
      "Epoch 65, Loss: 2.4118106365203857, Final Batch Loss: 1.1950030326843262\n",
      "Epoch 66, Loss: 2.3842488527297974, Final Batch Loss: 1.1241883039474487\n",
      "Epoch 67, Loss: 2.3222585916519165, Final Batch Loss: 1.2273198366165161\n",
      "Epoch 68, Loss: 2.317778468132019, Final Batch Loss: 1.1204032897949219\n",
      "Epoch 69, Loss: 2.3094156980514526, Final Batch Loss: 1.1431760787963867\n",
      "Epoch 70, Loss: 2.2352662086486816, Final Batch Loss: 1.1028658151626587\n",
      "Epoch 71, Loss: 2.2391929626464844, Final Batch Loss: 1.0940505266189575\n",
      "Epoch 72, Loss: 2.1914703845977783, Final Batch Loss: 1.0860117673873901\n",
      "Epoch 73, Loss: 2.2092673778533936, Final Batch Loss: 1.1320267915725708\n",
      "Epoch 74, Loss: 2.1804271936416626, Final Batch Loss: 1.096755027770996\n",
      "Epoch 75, Loss: 2.159670114517212, Final Batch Loss: 1.0465196371078491\n",
      "Epoch 76, Loss: 2.1151838302612305, Final Batch Loss: 1.0307698249816895\n",
      "Epoch 77, Loss: 2.141087532043457, Final Batch Loss: 1.0520970821380615\n",
      "Epoch 78, Loss: 2.15276837348938, Final Batch Loss: 1.124878168106079\n",
      "Epoch 79, Loss: 2.0327150225639343, Final Batch Loss: 0.9555746912956238\n",
      "Epoch 80, Loss: 2.028140664100647, Final Batch Loss: 1.054169774055481\n",
      "Epoch 81, Loss: 2.0438008308410645, Final Batch Loss: 1.0476105213165283\n",
      "Epoch 82, Loss: 2.021169424057007, Final Batch Loss: 1.063734769821167\n",
      "Epoch 83, Loss: 1.9625308513641357, Final Batch Loss: 1.0218398571014404\n",
      "Epoch 84, Loss: 1.9324058294296265, Final Batch Loss: 1.0138533115386963\n",
      "Epoch 85, Loss: 1.9266359210014343, Final Batch Loss: 0.9914109110832214\n",
      "Epoch 86, Loss: 1.9178364872932434, Final Batch Loss: 0.9665847420692444\n",
      "Epoch 87, Loss: 1.9383121132850647, Final Batch Loss: 0.9836923480033875\n",
      "Epoch 88, Loss: 1.8796989917755127, Final Batch Loss: 0.9167051911354065\n",
      "Epoch 89, Loss: 1.9141275882720947, Final Batch Loss: 0.9780617952346802\n",
      "Epoch 90, Loss: 1.8702774047851562, Final Batch Loss: 0.9447814226150513\n",
      "Epoch 91, Loss: 1.89411199092865, Final Batch Loss: 1.0144972801208496\n",
      "Epoch 92, Loss: 1.8694666028022766, Final Batch Loss: 0.9374904632568359\n",
      "Epoch 93, Loss: 1.843791127204895, Final Batch Loss: 0.9247871041297913\n",
      "Epoch 94, Loss: 1.7932208180427551, Final Batch Loss: 0.8815895318984985\n",
      "Epoch 95, Loss: 1.7427566647529602, Final Batch Loss: 0.8906827569007874\n",
      "Epoch 96, Loss: 1.783644437789917, Final Batch Loss: 0.9163958430290222\n",
      "Epoch 97, Loss: 1.7339244484901428, Final Batch Loss: 0.8568923473358154\n",
      "Epoch 98, Loss: 1.7427574396133423, Final Batch Loss: 0.9193911552429199\n",
      "Epoch 99, Loss: 1.737001359462738, Final Batch Loss: 0.9342368245124817\n",
      "Epoch 100, Loss: 1.7187232971191406, Final Batch Loss: 0.8629500269889832\n",
      "Epoch 101, Loss: 1.7414368987083435, Final Batch Loss: 0.8341707587242126\n",
      "Epoch 102, Loss: 1.7452529668807983, Final Batch Loss: 0.8741205930709839\n",
      "Epoch 103, Loss: 1.7761808037757874, Final Batch Loss: 0.8874005675315857\n",
      "Epoch 104, Loss: 1.6944626569747925, Final Batch Loss: 0.8071269392967224\n",
      "Epoch 105, Loss: 1.6873743534088135, Final Batch Loss: 0.8812881708145142\n",
      "Epoch 106, Loss: 1.6555218696594238, Final Batch Loss: 0.7803112864494324\n",
      "Epoch 107, Loss: 1.7384279370307922, Final Batch Loss: 0.8426677584648132\n",
      "Epoch 108, Loss: 1.6348451972007751, Final Batch Loss: 0.8347599506378174\n",
      "Epoch 109, Loss: 1.656510591506958, Final Batch Loss: 0.8411598801612854\n",
      "Epoch 110, Loss: 1.6938867568969727, Final Batch Loss: 0.8401026725769043\n",
      "Epoch 111, Loss: 1.654119074344635, Final Batch Loss: 0.8340686559677124\n",
      "Epoch 112, Loss: 1.682561218738556, Final Batch Loss: 0.8781654238700867\n",
      "Epoch 113, Loss: 1.604092001914978, Final Batch Loss: 0.7719602584838867\n",
      "Epoch 114, Loss: 1.65758615732193, Final Batch Loss: 0.8344086408615112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: 1.6839829683303833, Final Batch Loss: 0.8215690851211548\n",
      "Epoch 116, Loss: 1.6216468811035156, Final Batch Loss: 0.8429588079452515\n",
      "Epoch 117, Loss: 1.5856303572654724, Final Batch Loss: 0.7052487134933472\n",
      "Epoch 118, Loss: 1.706580936908722, Final Batch Loss: 0.8682622909545898\n",
      "Epoch 119, Loss: 1.6082261800765991, Final Batch Loss: 0.8483725190162659\n",
      "Epoch 120, Loss: 1.5817093253135681, Final Batch Loss: 0.8023898601531982\n",
      "Epoch 121, Loss: 1.5856998562812805, Final Batch Loss: 0.7947825789451599\n",
      "Epoch 122, Loss: 1.589961290359497, Final Batch Loss: 0.8263174295425415\n",
      "Epoch 123, Loss: 1.5764853358268738, Final Batch Loss: 0.7780173420906067\n",
      "Epoch 124, Loss: 1.530397653579712, Final Batch Loss: 0.7950828075408936\n",
      "Epoch 125, Loss: 1.6088129878044128, Final Batch Loss: 0.7982548475265503\n",
      "Epoch 126, Loss: 1.6498457789421082, Final Batch Loss: 0.8949368000030518\n",
      "Epoch 127, Loss: 1.5200533270835876, Final Batch Loss: 0.7794538736343384\n",
      "Epoch 128, Loss: 1.6373355388641357, Final Batch Loss: 0.8033487796783447\n",
      "Epoch 129, Loss: 1.5422923564910889, Final Batch Loss: 0.791608452796936\n",
      "Epoch 130, Loss: 1.5595493912696838, Final Batch Loss: 0.7814261317253113\n",
      "Epoch 131, Loss: 1.5405672192573547, Final Batch Loss: 0.7965498566627502\n",
      "Epoch 132, Loss: 1.615786850452423, Final Batch Loss: 0.7926740646362305\n",
      "Epoch 133, Loss: 1.4924808740615845, Final Batch Loss: 0.7582493424415588\n",
      "Epoch 134, Loss: 1.5719838738441467, Final Batch Loss: 0.7546389698982239\n",
      "Epoch 135, Loss: 1.5862858891487122, Final Batch Loss: 0.8053774237632751\n",
      "Epoch 136, Loss: 1.3955033421516418, Final Batch Loss: 0.7475061416625977\n",
      "Epoch 137, Loss: 1.506399691104889, Final Batch Loss: 0.7353310585021973\n",
      "Epoch 138, Loss: 1.5856451392173767, Final Batch Loss: 0.7914019823074341\n",
      "Epoch 139, Loss: 1.5559328198432922, Final Batch Loss: 0.8250683546066284\n",
      "Epoch 140, Loss: 1.5243300199508667, Final Batch Loss: 0.7011146545410156\n",
      "Epoch 141, Loss: 1.578148901462555, Final Batch Loss: 0.7785161733627319\n",
      "Epoch 142, Loss: 1.5412115454673767, Final Batch Loss: 0.7665757536888123\n",
      "Epoch 143, Loss: 1.5610888600349426, Final Batch Loss: 0.8599356412887573\n",
      "Epoch 144, Loss: 1.5140272974967957, Final Batch Loss: 0.7824629545211792\n",
      "Epoch 145, Loss: 1.5437418222427368, Final Batch Loss: 0.7669241428375244\n",
      "Epoch 146, Loss: 1.4769471883773804, Final Batch Loss: 0.7297559976577759\n",
      "Epoch 147, Loss: 1.429835021495819, Final Batch Loss: 0.7061816453933716\n",
      "Epoch 148, Loss: 1.4276392459869385, Final Batch Loss: 0.6738832592964172\n",
      "Epoch 149, Loss: 1.50405615568161, Final Batch Loss: 0.73199862241745\n",
      "Epoch 150, Loss: 1.4619517922401428, Final Batch Loss: 0.7837279438972473\n",
      "Epoch 151, Loss: 1.5104742646217346, Final Batch Loss: 0.7709093689918518\n",
      "Epoch 152, Loss: 1.5697356462478638, Final Batch Loss: 0.7663465142250061\n",
      "Epoch 153, Loss: 1.5112857222557068, Final Batch Loss: 0.7355395555496216\n",
      "Epoch 154, Loss: 1.4875003099441528, Final Batch Loss: 0.7328953742980957\n",
      "Epoch 155, Loss: 1.4434635639190674, Final Batch Loss: 0.7431511282920837\n",
      "Epoch 156, Loss: 1.3786688446998596, Final Batch Loss: 0.6532865166664124\n",
      "Epoch 157, Loss: 1.4926105737686157, Final Batch Loss: 0.7791449427604675\n",
      "Epoch 158, Loss: 1.5061451196670532, Final Batch Loss: 0.7900751829147339\n",
      "Epoch 159, Loss: 1.457728087902069, Final Batch Loss: 0.7107927203178406\n",
      "Epoch 160, Loss: 1.397472083568573, Final Batch Loss: 0.7102372646331787\n",
      "Epoch 161, Loss: 1.361695647239685, Final Batch Loss: 0.7005490660667419\n",
      "Epoch 162, Loss: 1.516053020954132, Final Batch Loss: 0.7202348709106445\n",
      "Epoch 163, Loss: 1.3771452903747559, Final Batch Loss: 0.6998798251152039\n",
      "Epoch 164, Loss: 1.3556570410728455, Final Batch Loss: 0.6466348171234131\n",
      "Epoch 165, Loss: 1.4209095239639282, Final Batch Loss: 0.749436616897583\n",
      "Epoch 166, Loss: 1.4877601861953735, Final Batch Loss: 0.7828382253646851\n",
      "Epoch 167, Loss: 1.4574125409126282, Final Batch Loss: 0.7071472406387329\n",
      "Epoch 168, Loss: 1.4071508049964905, Final Batch Loss: 0.6945185661315918\n",
      "Epoch 169, Loss: 1.4010823965072632, Final Batch Loss: 0.7053952217102051\n",
      "Epoch 170, Loss: 1.3855637907981873, Final Batch Loss: 0.6639971137046814\n",
      "Epoch 171, Loss: 1.388949453830719, Final Batch Loss: 0.6590672135353088\n",
      "Epoch 172, Loss: 1.4164605736732483, Final Batch Loss: 0.7436241507530212\n",
      "Epoch 173, Loss: 1.4104548692703247, Final Batch Loss: 0.6700477600097656\n",
      "Epoch 174, Loss: 1.4816372990608215, Final Batch Loss: 0.7732175588607788\n",
      "Epoch 175, Loss: 1.338812232017517, Final Batch Loss: 0.6990132331848145\n",
      "Epoch 176, Loss: 1.4197640419006348, Final Batch Loss: 0.7379716634750366\n",
      "Epoch 177, Loss: 1.412034273147583, Final Batch Loss: 0.7505393028259277\n",
      "Epoch 178, Loss: 1.337872564792633, Final Batch Loss: 0.6537845134735107\n",
      "Epoch 179, Loss: 1.3477928042411804, Final Batch Loss: 0.6868696808815002\n",
      "Epoch 180, Loss: 1.4099039435386658, Final Batch Loss: 0.7115038633346558\n",
      "Epoch 181, Loss: 1.3654632568359375, Final Batch Loss: 0.6823852062225342\n",
      "Epoch 182, Loss: 1.3771913647651672, Final Batch Loss: 0.6798556447029114\n",
      "Epoch 183, Loss: 1.4225794672966003, Final Batch Loss: 0.6995187997817993\n",
      "Epoch 184, Loss: 1.4113627672195435, Final Batch Loss: 0.6900439262390137\n",
      "Epoch 185, Loss: 1.3563588857650757, Final Batch Loss: 0.6658689379692078\n",
      "Epoch 186, Loss: 1.353236436843872, Final Batch Loss: 0.6977110505104065\n",
      "Epoch 187, Loss: 1.3190473914146423, Final Batch Loss: 0.6156755685806274\n",
      "Epoch 188, Loss: 1.3351373076438904, Final Batch Loss: 0.667573094367981\n",
      "Epoch 189, Loss: 1.3316455483436584, Final Batch Loss: 0.6430822610855103\n",
      "Epoch 190, Loss: 1.3341240882873535, Final Batch Loss: 0.6958258748054504\n",
      "Epoch 191, Loss: 1.3641465306282043, Final Batch Loss: 0.6874558329582214\n",
      "Epoch 192, Loss: 1.328861951828003, Final Batch Loss: 0.7280605435371399\n",
      "Epoch 193, Loss: 1.3337104320526123, Final Batch Loss: 0.6536849737167358\n",
      "Epoch 194, Loss: 1.3204401135444641, Final Batch Loss: 0.6546820402145386\n",
      "Epoch 195, Loss: 1.3513569235801697, Final Batch Loss: 0.6694638133049011\n",
      "Epoch 196, Loss: 1.376473605632782, Final Batch Loss: 0.7391846776008606\n",
      "Epoch 197, Loss: 1.34891277551651, Final Batch Loss: 0.6416366696357727\n",
      "Epoch 198, Loss: 1.3134812712669373, Final Batch Loss: 0.6348074078559875\n",
      "Epoch 199, Loss: 1.3343589901924133, Final Batch Loss: 0.7264056205749512\n",
      "Epoch 200, Loss: 1.3533024787902832, Final Batch Loss: 0.6668245792388916\n",
      "Epoch 201, Loss: 1.3926194906234741, Final Batch Loss: 0.6843563318252563\n",
      "Epoch 202, Loss: 1.256909728050232, Final Batch Loss: 0.5945380330085754\n",
      "Epoch 203, Loss: 1.3375226259231567, Final Batch Loss: 0.6583099961280823\n",
      "Epoch 204, Loss: 1.340591311454773, Final Batch Loss: 0.6617059707641602\n",
      "Epoch 205, Loss: 1.3250404596328735, Final Batch Loss: 0.6506715416908264\n",
      "Epoch 206, Loss: 1.2857324481010437, Final Batch Loss: 0.6726080775260925\n",
      "Epoch 207, Loss: 1.3624222874641418, Final Batch Loss: 0.6513977646827698\n",
      "Epoch 208, Loss: 1.3049283623695374, Final Batch Loss: 0.6375444531440735\n",
      "Epoch 209, Loss: 1.3169353604316711, Final Batch Loss: 0.6304560303688049\n",
      "Epoch 210, Loss: 1.3278460502624512, Final Batch Loss: 0.6430611610412598\n",
      "Epoch 211, Loss: 1.3780304193496704, Final Batch Loss: 0.6761361956596375\n",
      "Epoch 212, Loss: 1.2853423357009888, Final Batch Loss: 0.6877323389053345\n",
      "Epoch 213, Loss: 1.2422280311584473, Final Batch Loss: 0.5849015116691589\n",
      "Epoch 214, Loss: 1.285972774028778, Final Batch Loss: 0.6138827204704285\n",
      "Epoch 215, Loss: 1.2892463207244873, Final Batch Loss: 0.6141269207000732\n",
      "Epoch 216, Loss: 1.2918058633804321, Final Batch Loss: 0.5963513255119324\n",
      "Epoch 217, Loss: 1.3140300512313843, Final Batch Loss: 0.6379250884056091\n",
      "Epoch 218, Loss: 1.3342524766921997, Final Batch Loss: 0.6719611883163452\n",
      "Epoch 219, Loss: 1.3279387950897217, Final Batch Loss: 0.6988409757614136\n",
      "Epoch 220, Loss: 1.3043416142463684, Final Batch Loss: 0.6774829030036926\n",
      "Epoch 221, Loss: 1.2520774006843567, Final Batch Loss: 0.6193665862083435\n",
      "Epoch 222, Loss: 1.2613391280174255, Final Batch Loss: 0.6209198236465454\n",
      "Epoch 223, Loss: 1.329232633113861, Final Batch Loss: 0.6764503717422485\n",
      "Epoch 224, Loss: 1.2821842432022095, Final Batch Loss: 0.5786914229393005\n",
      "Epoch 225, Loss: 1.3023748993873596, Final Batch Loss: 0.6402031779289246\n",
      "Epoch 226, Loss: 1.2811322212219238, Final Batch Loss: 0.6594603657722473\n",
      "Epoch 227, Loss: 1.3067476749420166, Final Batch Loss: 0.6765424013137817\n",
      "Epoch 228, Loss: 1.2691068053245544, Final Batch Loss: 0.5653239488601685\n",
      "Epoch 229, Loss: 1.250840425491333, Final Batch Loss: 0.6309703588485718\n",
      "Epoch 230, Loss: 1.2797669172286987, Final Batch Loss: 0.6578836441040039\n",
      "Epoch 231, Loss: 1.2777462005615234, Final Batch Loss: 0.6321960091590881\n",
      "Epoch 232, Loss: 1.2402926087379456, Final Batch Loss: 0.6332086324691772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233, Loss: 1.2172107696533203, Final Batch Loss: 0.6110031008720398\n",
      "Epoch 234, Loss: 1.298873245716095, Final Batch Loss: 0.6825545430183411\n",
      "Epoch 235, Loss: 1.2533661723136902, Final Batch Loss: 0.6417868733406067\n",
      "Epoch 236, Loss: 1.2942447066307068, Final Batch Loss: 0.6749334931373596\n",
      "Epoch 237, Loss: 1.2214624881744385, Final Batch Loss: 0.6052015423774719\n",
      "Epoch 238, Loss: 1.225910484790802, Final Batch Loss: 0.6014999151229858\n",
      "Epoch 239, Loss: 1.267816960811615, Final Batch Loss: 0.6609687805175781\n",
      "Epoch 240, Loss: 1.2330355644226074, Final Batch Loss: 0.6585784554481506\n",
      "Epoch 241, Loss: 1.2217450141906738, Final Batch Loss: 0.6165177822113037\n",
      "Epoch 242, Loss: 1.241684913635254, Final Batch Loss: 0.5933823585510254\n",
      "Epoch 243, Loss: 1.2872781157493591, Final Batch Loss: 0.648409366607666\n",
      "Epoch 244, Loss: 1.1859389543533325, Final Batch Loss: 0.6204643249511719\n",
      "Epoch 245, Loss: 1.2794899344444275, Final Batch Loss: 0.6715895533561707\n",
      "Epoch 246, Loss: 1.2906911373138428, Final Batch Loss: 0.6145022511482239\n",
      "Epoch 247, Loss: 1.2965222597122192, Final Batch Loss: 0.5662103891372681\n",
      "Epoch 248, Loss: 1.3140528798103333, Final Batch Loss: 0.7125465273857117\n",
      "Epoch 249, Loss: 1.2229298949241638, Final Batch Loss: 0.6295217871665955\n",
      "Epoch 250, Loss: 1.1752353310585022, Final Batch Loss: 0.583099901676178\n",
      "Epoch 251, Loss: 1.2008206248283386, Final Batch Loss: 0.5884462594985962\n",
      "Epoch 252, Loss: 1.3332365155220032, Final Batch Loss: 0.6599829196929932\n",
      "Epoch 253, Loss: 1.183345079421997, Final Batch Loss: 0.6048557162284851\n",
      "Epoch 254, Loss: 1.274168074131012, Final Batch Loss: 0.6433982849121094\n",
      "Epoch 255, Loss: 1.1969263553619385, Final Batch Loss: 0.5964464545249939\n",
      "Epoch 256, Loss: 1.1709275245666504, Final Batch Loss: 0.6189175844192505\n",
      "Epoch 257, Loss: 1.2163459062576294, Final Batch Loss: 0.6404555439949036\n",
      "Epoch 258, Loss: 1.2568984627723694, Final Batch Loss: 0.6045388579368591\n",
      "Epoch 259, Loss: 1.200814425945282, Final Batch Loss: 0.5940462350845337\n",
      "Epoch 260, Loss: 1.1926702857017517, Final Batch Loss: 0.5601846575737\n",
      "Epoch 261, Loss: 1.1766100525856018, Final Batch Loss: 0.5568607449531555\n",
      "Epoch 262, Loss: 1.2354260683059692, Final Batch Loss: 0.6552485227584839\n",
      "Epoch 263, Loss: 1.221505880355835, Final Batch Loss: 0.6208308339118958\n",
      "Epoch 264, Loss: 1.1746079325675964, Final Batch Loss: 0.5889847278594971\n",
      "Epoch 265, Loss: 1.2111786007881165, Final Batch Loss: 0.5960605144500732\n",
      "Epoch 266, Loss: 1.2276256084442139, Final Batch Loss: 0.6190004944801331\n",
      "Epoch 267, Loss: 1.2515803575515747, Final Batch Loss: 0.5876515507698059\n",
      "Epoch 268, Loss: 1.2565150260925293, Final Batch Loss: 0.6503117084503174\n",
      "Epoch 269, Loss: 1.2441768050193787, Final Batch Loss: 0.6411600708961487\n",
      "Epoch 270, Loss: 1.2503427267074585, Final Batch Loss: 0.5758022665977478\n",
      "Epoch 271, Loss: 1.1400432586669922, Final Batch Loss: 0.5528164505958557\n",
      "Epoch 272, Loss: 1.1777451634407043, Final Batch Loss: 0.6643019914627075\n",
      "Epoch 273, Loss: 1.20594322681427, Final Batch Loss: 0.5707277059555054\n",
      "Epoch 274, Loss: 1.2539476156234741, Final Batch Loss: 0.6318866610527039\n",
      "Epoch 275, Loss: 1.1472319960594177, Final Batch Loss: 0.601610004901886\n",
      "Epoch 276, Loss: 1.1759559512138367, Final Batch Loss: 0.5696406364440918\n",
      "Epoch 277, Loss: 1.2427053451538086, Final Batch Loss: 0.5913054943084717\n",
      "Epoch 278, Loss: 1.1995489001274109, Final Batch Loss: 0.6030049920082092\n",
      "Epoch 279, Loss: 1.2121511697769165, Final Batch Loss: 0.5871566534042358\n",
      "Epoch 280, Loss: 1.2465345859527588, Final Batch Loss: 0.6126187443733215\n",
      "Epoch 281, Loss: 1.2008862495422363, Final Batch Loss: 0.6121986508369446\n",
      "Epoch 282, Loss: 1.2314062118530273, Final Batch Loss: 0.6067730188369751\n",
      "Epoch 283, Loss: 1.194914698600769, Final Batch Loss: 0.5898386240005493\n",
      "Epoch 284, Loss: 1.1901793479919434, Final Batch Loss: 0.532762885093689\n",
      "Epoch 285, Loss: 1.1997731924057007, Final Batch Loss: 0.6231380105018616\n",
      "Epoch 286, Loss: 1.1961682438850403, Final Batch Loss: 0.5951049327850342\n",
      "Epoch 287, Loss: 1.0925301313400269, Final Batch Loss: 0.5463111996650696\n",
      "Epoch 288, Loss: 1.159924566745758, Final Batch Loss: 0.5241757035255432\n",
      "Epoch 289, Loss: 1.1819691061973572, Final Batch Loss: 0.6110398173332214\n",
      "Epoch 290, Loss: 1.1700530052185059, Final Batch Loss: 0.5715157389640808\n",
      "Epoch 291, Loss: 1.1382856965065002, Final Batch Loss: 0.5612587928771973\n",
      "Epoch 292, Loss: 1.1872074604034424, Final Batch Loss: 0.605309784412384\n",
      "Epoch 293, Loss: 1.1061140298843384, Final Batch Loss: 0.5693344473838806\n",
      "Epoch 294, Loss: 1.1510456204414368, Final Batch Loss: 0.5474967360496521\n",
      "Epoch 295, Loss: 1.0963046550750732, Final Batch Loss: 0.5594049096107483\n",
      "Epoch 296, Loss: 1.2120787501335144, Final Batch Loss: 0.5817726254463196\n",
      "Epoch 297, Loss: 1.2000628113746643, Final Batch Loss: 0.54301518201828\n",
      "Epoch 298, Loss: 1.2350251078605652, Final Batch Loss: 0.6028071641921997\n",
      "Epoch 299, Loss: 1.1841843724250793, Final Batch Loss: 0.5351243615150452\n",
      "Epoch 300, Loss: 1.1473023891448975, Final Batch Loss: 0.5980802178382874\n",
      "Epoch 301, Loss: 1.158753752708435, Final Batch Loss: 0.5718720555305481\n",
      "Epoch 302, Loss: 1.1316233277320862, Final Batch Loss: 0.5923368334770203\n",
      "Epoch 303, Loss: 1.1459147334098816, Final Batch Loss: 0.5992923974990845\n",
      "Epoch 304, Loss: 1.1579053401947021, Final Batch Loss: 0.576035737991333\n",
      "Epoch 305, Loss: 1.1340554356575012, Final Batch Loss: 0.5642239451408386\n",
      "Epoch 306, Loss: 1.1253448128700256, Final Batch Loss: 0.5834840536117554\n",
      "Epoch 307, Loss: 1.0885174870491028, Final Batch Loss: 0.5101273655891418\n",
      "Epoch 308, Loss: 1.1240262985229492, Final Batch Loss: 0.538800835609436\n",
      "Epoch 309, Loss: 1.1742032766342163, Final Batch Loss: 0.5104044079780579\n",
      "Epoch 310, Loss: 1.109961450099945, Final Batch Loss: 0.5753465890884399\n",
      "Epoch 311, Loss: 1.0895988941192627, Final Batch Loss: 0.5875391960144043\n",
      "Epoch 312, Loss: 1.127948522567749, Final Batch Loss: 0.6036685109138489\n",
      "Epoch 313, Loss: 1.1567803025245667, Final Batch Loss: 0.569686770439148\n",
      "Epoch 314, Loss: 1.1010069847106934, Final Batch Loss: 0.5355120301246643\n",
      "Epoch 315, Loss: 1.0765600800514221, Final Batch Loss: 0.5094279050827026\n",
      "Epoch 316, Loss: 1.1574274897575378, Final Batch Loss: 0.5477584004402161\n",
      "Epoch 317, Loss: 1.0827783942222595, Final Batch Loss: 0.48928308486938477\n",
      "Epoch 318, Loss: 1.0825119018554688, Final Batch Loss: 0.501569390296936\n",
      "Epoch 319, Loss: 1.135434627532959, Final Batch Loss: 0.5857077240943909\n",
      "Epoch 320, Loss: 1.1712533831596375, Final Batch Loss: 0.5902150273323059\n",
      "Epoch 321, Loss: 1.1032652854919434, Final Batch Loss: 0.5729112029075623\n",
      "Epoch 322, Loss: 1.144083321094513, Final Batch Loss: 0.5737525224685669\n",
      "Epoch 323, Loss: 1.0702518224716187, Final Batch Loss: 0.5074315071105957\n",
      "Epoch 324, Loss: 1.0264889001846313, Final Batch Loss: 0.5087145566940308\n",
      "Epoch 325, Loss: 1.0893301367759705, Final Batch Loss: 0.5479325652122498\n",
      "Epoch 326, Loss: 1.1483151316642761, Final Batch Loss: 0.518762469291687\n",
      "Epoch 327, Loss: 1.1078495383262634, Final Batch Loss: 0.592179000377655\n",
      "Epoch 328, Loss: 1.091235637664795, Final Batch Loss: 0.5490054488182068\n",
      "Epoch 329, Loss: 1.0586585998535156, Final Batch Loss: 0.5228351950645447\n",
      "Epoch 330, Loss: 1.0778886675834656, Final Batch Loss: 0.5607417225837708\n",
      "Epoch 331, Loss: 1.1020566821098328, Final Batch Loss: 0.5786495208740234\n",
      "Epoch 332, Loss: 1.0509189367294312, Final Batch Loss: 0.5239903926849365\n",
      "Epoch 333, Loss: 1.1209352612495422, Final Batch Loss: 0.5253374576568604\n",
      "Epoch 334, Loss: 1.1144042611122131, Final Batch Loss: 0.6110724210739136\n",
      "Epoch 335, Loss: 1.072498857975006, Final Batch Loss: 0.5664074420928955\n",
      "Epoch 336, Loss: 1.098769724369049, Final Batch Loss: 0.5351022481918335\n",
      "Epoch 337, Loss: 1.0952923893928528, Final Batch Loss: 0.5374169945716858\n",
      "Epoch 338, Loss: 1.031452864408493, Final Batch Loss: 0.5415465831756592\n",
      "Epoch 339, Loss: 1.1326918601989746, Final Batch Loss: 0.6046983003616333\n",
      "Epoch 340, Loss: 1.1469744443893433, Final Batch Loss: 0.5576968193054199\n",
      "Epoch 341, Loss: 1.0517005324363708, Final Batch Loss: 0.5466737151145935\n",
      "Epoch 342, Loss: 1.0586613416671753, Final Batch Loss: 0.5065817832946777\n",
      "Epoch 343, Loss: 1.0833515524864197, Final Batch Loss: 0.5176233649253845\n",
      "Epoch 344, Loss: 1.068698525428772, Final Batch Loss: 0.5275859236717224\n",
      "Epoch 345, Loss: 1.0582553148269653, Final Batch Loss: 0.5837090015411377\n",
      "Epoch 346, Loss: 1.044791042804718, Final Batch Loss: 0.5076929330825806\n",
      "Epoch 347, Loss: 1.0883868932724, Final Batch Loss: 0.5470716953277588\n",
      "Epoch 348, Loss: 1.0847856998443604, Final Batch Loss: 0.5921952724456787\n",
      "Epoch 349, Loss: 1.0536485314369202, Final Batch Loss: 0.5193156003952026\n",
      "Epoch 350, Loss: 1.0552250146865845, Final Batch Loss: 0.5468894839286804\n",
      "Epoch 351, Loss: 1.0385693609714508, Final Batch Loss: 0.5599921345710754\n",
      "Epoch 352, Loss: 1.0747002363204956, Final Batch Loss: 0.5548043847084045\n",
      "Epoch 353, Loss: 1.0474979877471924, Final Batch Loss: 0.5130574107170105\n",
      "Epoch 354, Loss: 1.0477714538574219, Final Batch Loss: 0.5173578262329102\n",
      "Epoch 355, Loss: 1.0437040328979492, Final Batch Loss: 0.5448583364486694\n",
      "Epoch 356, Loss: 1.0147883296012878, Final Batch Loss: 0.5194672346115112\n",
      "Epoch 357, Loss: 1.027972936630249, Final Batch Loss: 0.513388454914093\n",
      "Epoch 358, Loss: 0.9490446746349335, Final Batch Loss: 0.4521903097629547\n",
      "Epoch 359, Loss: 0.9998058080673218, Final Batch Loss: 0.44604361057281494\n",
      "Epoch 360, Loss: 0.9643204808235168, Final Batch Loss: 0.508865475654602\n",
      "Epoch 361, Loss: 1.0174496471881866, Final Batch Loss: 0.5274863839149475\n",
      "Epoch 362, Loss: 0.9144359529018402, Final Batch Loss: 0.4226877987384796\n",
      "Epoch 363, Loss: 0.9926051497459412, Final Batch Loss: 0.532216489315033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364, Loss: 1.0133461952209473, Final Batch Loss: 0.5086928606033325\n",
      "Epoch 365, Loss: 0.9728292226791382, Final Batch Loss: 0.4645129442214966\n",
      "Epoch 366, Loss: 0.9169464111328125, Final Batch Loss: 0.4166707396507263\n",
      "Epoch 367, Loss: 0.9173822999000549, Final Batch Loss: 0.4693338871002197\n",
      "Epoch 368, Loss: 0.9402782618999481, Final Batch Loss: 0.5169180631637573\n",
      "Epoch 369, Loss: 0.9567766785621643, Final Batch Loss: 0.46405017375946045\n",
      "Epoch 370, Loss: 0.9765393137931824, Final Batch Loss: 0.5213350653648376\n",
      "Epoch 371, Loss: 1.0019967555999756, Final Batch Loss: 0.5072609782218933\n",
      "Epoch 372, Loss: 0.9726221561431885, Final Batch Loss: 0.4770532250404358\n",
      "Epoch 373, Loss: 0.9182192087173462, Final Batch Loss: 0.4771227538585663\n",
      "Epoch 374, Loss: 0.9195526540279388, Final Batch Loss: 0.4458175599575043\n",
      "Epoch 375, Loss: 0.9450108110904694, Final Batch Loss: 0.5204097628593445\n",
      "Epoch 376, Loss: 1.008950412273407, Final Batch Loss: 0.5268309712409973\n",
      "Epoch 377, Loss: 0.960371196269989, Final Batch Loss: 0.4484521150588989\n",
      "Epoch 378, Loss: 0.934523731470108, Final Batch Loss: 0.5152544975280762\n",
      "Epoch 379, Loss: 0.9436127543449402, Final Batch Loss: 0.4391621947288513\n",
      "Epoch 380, Loss: 0.8909951746463776, Final Batch Loss: 0.44110769033432007\n",
      "Epoch 381, Loss: 0.9065271317958832, Final Batch Loss: 0.4663672149181366\n",
      "Epoch 382, Loss: 0.9215897619724274, Final Batch Loss: 0.45899632573127747\n",
      "Epoch 383, Loss: 0.8556081056594849, Final Batch Loss: 0.46974268555641174\n",
      "Epoch 384, Loss: 0.9417814314365387, Final Batch Loss: 0.505997359752655\n",
      "Epoch 385, Loss: 0.9639152884483337, Final Batch Loss: 0.48843520879745483\n",
      "Epoch 386, Loss: 0.8763640820980072, Final Batch Loss: 0.38758742809295654\n",
      "Epoch 387, Loss: 0.8824646770954132, Final Batch Loss: 0.4364504814147949\n",
      "Epoch 388, Loss: 0.8383972942829132, Final Batch Loss: 0.4227026700973511\n",
      "Epoch 389, Loss: 0.9412743151187897, Final Batch Loss: 0.44033291935920715\n",
      "Epoch 390, Loss: 0.9225710332393646, Final Batch Loss: 0.49200108647346497\n",
      "Epoch 391, Loss: 0.9201313853263855, Final Batch Loss: 0.5536770820617676\n",
      "Epoch 392, Loss: 0.8533937633037567, Final Batch Loss: 0.3902432918548584\n",
      "Epoch 393, Loss: 0.8202458620071411, Final Batch Loss: 0.3858848512172699\n",
      "Epoch 394, Loss: 0.8945327997207642, Final Batch Loss: 0.4333311915397644\n",
      "Epoch 395, Loss: 0.828323483467102, Final Batch Loss: 0.4002097547054291\n",
      "Epoch 396, Loss: 0.8821491301059723, Final Batch Loss: 0.43444108963012695\n",
      "Epoch 397, Loss: 0.8145628273487091, Final Batch Loss: 0.42521464824676514\n",
      "Epoch 398, Loss: 0.9102943539619446, Final Batch Loss: 0.4945116639137268\n",
      "Epoch 399, Loss: 0.8525591790676117, Final Batch Loss: 0.3934791386127472\n",
      "Epoch 400, Loss: 0.8997144401073456, Final Batch Loss: 0.44601836800575256\n",
      "Epoch 401, Loss: 0.8615981936454773, Final Batch Loss: 0.4032798409461975\n",
      "Epoch 402, Loss: 0.8125237226486206, Final Batch Loss: 0.3852599859237671\n",
      "Epoch 403, Loss: 0.8611692190170288, Final Batch Loss: 0.42888087034225464\n",
      "Epoch 404, Loss: 0.9045343101024628, Final Batch Loss: 0.4267267882823944\n",
      "Epoch 405, Loss: 0.775557667016983, Final Batch Loss: 0.37177884578704834\n",
      "Epoch 406, Loss: 0.9022438228130341, Final Batch Loss: 0.4710337221622467\n",
      "Epoch 407, Loss: 0.8623032867908478, Final Batch Loss: 0.4359555244445801\n",
      "Epoch 408, Loss: 0.8762426972389221, Final Batch Loss: 0.4164738953113556\n",
      "Epoch 409, Loss: 0.8303450644016266, Final Batch Loss: 0.4083799123764038\n",
      "Epoch 410, Loss: 0.7888137996196747, Final Batch Loss: 0.38640981912612915\n",
      "Epoch 411, Loss: 0.8184161186218262, Final Batch Loss: 0.44431841373443604\n",
      "Epoch 412, Loss: 0.9029335379600525, Final Batch Loss: 0.43724340200424194\n",
      "Epoch 413, Loss: 0.8753638565540314, Final Batch Loss: 0.4140250086784363\n",
      "Epoch 414, Loss: 0.8340954184532166, Final Batch Loss: 0.4428735077381134\n",
      "Epoch 415, Loss: 0.7999190390110016, Final Batch Loss: 0.4211500287055969\n",
      "Epoch 416, Loss: 0.8652822375297546, Final Batch Loss: 0.45701003074645996\n",
      "Epoch 417, Loss: 0.8693454563617706, Final Batch Loss: 0.4452453553676605\n",
      "Epoch 418, Loss: 0.8228582739830017, Final Batch Loss: 0.3895515501499176\n",
      "Epoch 419, Loss: 0.7789767980575562, Final Batch Loss: 0.41936472058296204\n",
      "Epoch 420, Loss: 0.8389530777931213, Final Batch Loss: 0.4734874963760376\n",
      "Epoch 421, Loss: 0.7888155579566956, Final Batch Loss: 0.39728179574012756\n",
      "Epoch 422, Loss: 0.8429665565490723, Final Batch Loss: 0.4302949607372284\n",
      "Epoch 423, Loss: 0.8851995766162872, Final Batch Loss: 0.3976443409919739\n",
      "Epoch 424, Loss: 0.8041922748088837, Final Batch Loss: 0.35735294222831726\n",
      "Epoch 425, Loss: 0.8183041214942932, Final Batch Loss: 0.4348982870578766\n",
      "Epoch 426, Loss: 0.842416524887085, Final Batch Loss: 0.44255298376083374\n",
      "Epoch 427, Loss: 0.8338437676429749, Final Batch Loss: 0.43826988339424133\n",
      "Epoch 428, Loss: 0.8654390573501587, Final Batch Loss: 0.40198126435279846\n",
      "Epoch 429, Loss: 0.8531863987445831, Final Batch Loss: 0.44372373819351196\n",
      "Epoch 430, Loss: 0.863737016916275, Final Batch Loss: 0.44303637742996216\n",
      "Epoch 431, Loss: 0.8412533402442932, Final Batch Loss: 0.4131688177585602\n",
      "Epoch 432, Loss: 0.887651115655899, Final Batch Loss: 0.4462427496910095\n",
      "Epoch 433, Loss: 0.7907467782497406, Final Batch Loss: 0.35720157623291016\n",
      "Epoch 434, Loss: 0.7761373519897461, Final Batch Loss: 0.3832904100418091\n",
      "Epoch 435, Loss: 0.8064985871315002, Final Batch Loss: 0.3791486620903015\n",
      "Epoch 436, Loss: 0.83912193775177, Final Batch Loss: 0.45518243312835693\n",
      "Epoch 437, Loss: 0.8046244084835052, Final Batch Loss: 0.442650705575943\n",
      "Epoch 438, Loss: 0.7954194843769073, Final Batch Loss: 0.39564090967178345\n",
      "Epoch 439, Loss: 0.8774715065956116, Final Batch Loss: 0.40855100750923157\n",
      "Epoch 440, Loss: 0.7973456978797913, Final Batch Loss: 0.3138122260570526\n",
      "Epoch 441, Loss: 0.7580139338970184, Final Batch Loss: 0.41609394550323486\n",
      "Epoch 442, Loss: 0.7708345651626587, Final Batch Loss: 0.3939143717288971\n",
      "Epoch 443, Loss: 0.7564417719841003, Final Batch Loss: 0.3938463628292084\n",
      "Epoch 444, Loss: 0.8627290427684784, Final Batch Loss: 0.4424551725387573\n",
      "Epoch 445, Loss: 0.744279682636261, Final Batch Loss: 0.368405282497406\n",
      "Epoch 446, Loss: 0.7603484392166138, Final Batch Loss: 0.37147006392478943\n",
      "Epoch 447, Loss: 0.7391452193260193, Final Batch Loss: 0.37812140583992004\n",
      "Epoch 448, Loss: 0.7875367999076843, Final Batch Loss: 0.43796679377555847\n",
      "Epoch 449, Loss: 0.803156167268753, Final Batch Loss: 0.42664238810539246\n",
      "Epoch 450, Loss: 0.7803730964660645, Final Batch Loss: 0.37302619218826294\n",
      "Epoch 451, Loss: 0.7594470083713531, Final Batch Loss: 0.37063366174697876\n",
      "Epoch 452, Loss: 0.8218352198600769, Final Batch Loss: 0.4026602804660797\n",
      "Epoch 453, Loss: 0.7177033424377441, Final Batch Loss: 0.35413193702697754\n",
      "Epoch 454, Loss: 0.7213418483734131, Final Batch Loss: 0.34694144129753113\n",
      "Epoch 455, Loss: 0.7992416620254517, Final Batch Loss: 0.41583606600761414\n",
      "Epoch 456, Loss: 0.7752842605113983, Final Batch Loss: 0.416638046503067\n",
      "Epoch 457, Loss: 0.8033786714076996, Final Batch Loss: 0.41054943203926086\n",
      "Epoch 458, Loss: 0.7120547592639923, Final Batch Loss: 0.38093772530555725\n",
      "Epoch 459, Loss: 0.7755201756954193, Final Batch Loss: 0.3909165561199188\n",
      "Epoch 460, Loss: 0.7154315710067749, Final Batch Loss: 0.37720778584480286\n",
      "Epoch 461, Loss: 0.7485007047653198, Final Batch Loss: 0.3848014771938324\n",
      "Epoch 462, Loss: 0.7905189096927643, Final Batch Loss: 0.3952884078025818\n",
      "Epoch 463, Loss: 0.7703874111175537, Final Batch Loss: 0.41569358110427856\n",
      "Epoch 464, Loss: 0.7230239808559418, Final Batch Loss: 0.3841620981693268\n",
      "Epoch 465, Loss: 0.7332033812999725, Final Batch Loss: 0.36557573080062866\n",
      "Epoch 466, Loss: 0.7401068806648254, Final Batch Loss: 0.3675704300403595\n",
      "Epoch 467, Loss: 0.7260785698890686, Final Batch Loss: 0.33799099922180176\n",
      "Epoch 468, Loss: 0.7558931410312653, Final Batch Loss: 0.4275030493736267\n",
      "Epoch 469, Loss: 0.7767170369625092, Final Batch Loss: 0.3876558244228363\n",
      "Epoch 470, Loss: 0.770961195230484, Final Batch Loss: 0.42738211154937744\n",
      "Epoch 471, Loss: 0.7349395751953125, Final Batch Loss: 0.3546636402606964\n",
      "Epoch 472, Loss: 0.7219246923923492, Final Batch Loss: 0.3506167232990265\n",
      "Epoch 473, Loss: 0.6305119097232819, Final Batch Loss: 0.3476560711860657\n",
      "Epoch 474, Loss: 0.7145209312438965, Final Batch Loss: 0.3446741998195648\n",
      "Epoch 475, Loss: 0.7272977232933044, Final Batch Loss: 0.34479784965515137\n",
      "Epoch 476, Loss: 0.7135402262210846, Final Batch Loss: 0.3508789837360382\n",
      "Epoch 477, Loss: 0.7371859550476074, Final Batch Loss: 0.3814430832862854\n",
      "Epoch 478, Loss: 0.7978956699371338, Final Batch Loss: 0.402681827545166\n",
      "Epoch 479, Loss: 0.7431489527225494, Final Batch Loss: 0.3457150459289551\n",
      "Epoch 480, Loss: 0.7226390540599823, Final Batch Loss: 0.3732614517211914\n",
      "Epoch 481, Loss: 0.7377117872238159, Final Batch Loss: 0.3539932370185852\n",
      "Epoch 482, Loss: 0.7618311643600464, Final Batch Loss: 0.412891685962677\n",
      "Epoch 483, Loss: 0.7282306849956512, Final Batch Loss: 0.3347412049770355\n",
      "Epoch 484, Loss: 0.6766728162765503, Final Batch Loss: 0.3590599000453949\n",
      "Epoch 485, Loss: 0.689521849155426, Final Batch Loss: 0.34188926219940186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486, Loss: 0.7262003719806671, Final Batch Loss: 0.3460754156112671\n",
      "Epoch 487, Loss: 0.6891772449016571, Final Batch Loss: 0.33585211634635925\n",
      "Epoch 488, Loss: 0.6726702749729156, Final Batch Loss: 0.3321797847747803\n",
      "Epoch 489, Loss: 0.7678123414516449, Final Batch Loss: 0.37074488401412964\n",
      "Epoch 490, Loss: 0.7393369078636169, Final Batch Loss: 0.3806057274341583\n",
      "Epoch 491, Loss: 0.7408308684825897, Final Batch Loss: 0.36471062898635864\n",
      "Epoch 492, Loss: 0.7045098841190338, Final Batch Loss: 0.300109326839447\n",
      "Epoch 493, Loss: 0.7320595681667328, Final Batch Loss: 0.3482154905796051\n",
      "Epoch 494, Loss: 0.7008711993694305, Final Batch Loss: 0.35975417494773865\n",
      "Epoch 495, Loss: 0.7314071953296661, Final Batch Loss: 0.32364925742149353\n",
      "Epoch 496, Loss: 0.6775670647621155, Final Batch Loss: 0.3360159397125244\n",
      "Epoch 497, Loss: 0.7218075692653656, Final Batch Loss: 0.34494733810424805\n",
      "Epoch 498, Loss: 0.7550051510334015, Final Batch Loss: 0.40119025111198425\n",
      "Epoch 499, Loss: 0.7038206160068512, Final Batch Loss: 0.37610816955566406\n",
      "Epoch 500, Loss: 0.710204541683197, Final Batch Loss: 0.39626145362854004\n",
      "Epoch 501, Loss: 0.730486661195755, Final Batch Loss: 0.3226325511932373\n",
      "Epoch 502, Loss: 0.7127818763256073, Final Batch Loss: 0.3496681749820709\n",
      "Epoch 503, Loss: 0.688285768032074, Final Batch Loss: 0.36500731110572815\n",
      "Epoch 504, Loss: 0.652572363615036, Final Batch Loss: 0.29594218730926514\n",
      "Epoch 505, Loss: 0.677995353937149, Final Batch Loss: 0.3270825743675232\n",
      "Epoch 506, Loss: 0.7114690542221069, Final Batch Loss: 0.34301096200942993\n",
      "Epoch 507, Loss: 0.6680821776390076, Final Batch Loss: 0.3043113350868225\n",
      "Epoch 508, Loss: 0.7016890645027161, Final Batch Loss: 0.3775203824043274\n",
      "Epoch 509, Loss: 0.6751398146152496, Final Batch Loss: 0.39490270614624023\n",
      "Epoch 510, Loss: 0.7013205885887146, Final Batch Loss: 0.359125018119812\n",
      "Epoch 511, Loss: 0.6822378039360046, Final Batch Loss: 0.37545111775398254\n",
      "Epoch 512, Loss: 0.7426230311393738, Final Batch Loss: 0.33198657631874084\n",
      "Epoch 513, Loss: 0.6589950025081635, Final Batch Loss: 0.3109644055366516\n",
      "Epoch 514, Loss: 0.7105198502540588, Final Batch Loss: 0.32271549105644226\n",
      "Epoch 515, Loss: 0.6991482675075531, Final Batch Loss: 0.2992473840713501\n",
      "Epoch 516, Loss: 0.6970470249652863, Final Batch Loss: 0.3542681634426117\n",
      "Epoch 517, Loss: 0.6696441173553467, Final Batch Loss: 0.3012855648994446\n",
      "Epoch 518, Loss: 0.6758832633495331, Final Batch Loss: 0.3231334984302521\n",
      "Epoch 519, Loss: 0.6399278938770294, Final Batch Loss: 0.3347213566303253\n",
      "Epoch 520, Loss: 0.6284778416156769, Final Batch Loss: 0.35113197565078735\n",
      "Epoch 521, Loss: 0.6607233881950378, Final Batch Loss: 0.3360503017902374\n",
      "Epoch 522, Loss: 0.6602969467639923, Final Batch Loss: 0.3674474060535431\n",
      "Epoch 523, Loss: 0.6352492272853851, Final Batch Loss: 0.2823026478290558\n",
      "Epoch 524, Loss: 0.7199101150035858, Final Batch Loss: 0.34357115626335144\n",
      "Epoch 525, Loss: 0.6691595613956451, Final Batch Loss: 0.33989065885543823\n",
      "Epoch 526, Loss: 0.671147495508194, Final Batch Loss: 0.31261444091796875\n",
      "Epoch 527, Loss: 0.7059203088283539, Final Batch Loss: 0.3468063771724701\n",
      "Epoch 528, Loss: 0.6046588718891144, Final Batch Loss: 0.30296921730041504\n",
      "Epoch 529, Loss: 0.7500313818454742, Final Batch Loss: 0.39574649930000305\n",
      "Epoch 530, Loss: 0.6957070529460907, Final Batch Loss: 0.34594473242759705\n",
      "Epoch 531, Loss: 0.6600302755832672, Final Batch Loss: 0.3123106062412262\n",
      "Epoch 532, Loss: 0.6600276231765747, Final Batch Loss: 0.36893555521965027\n",
      "Epoch 533, Loss: 0.6851401627063751, Final Batch Loss: 0.34940433502197266\n",
      "Epoch 534, Loss: 0.6858862638473511, Final Batch Loss: 0.3386254608631134\n",
      "Epoch 535, Loss: 0.6399812400341034, Final Batch Loss: 0.324208527803421\n",
      "Epoch 536, Loss: 0.6444502174854279, Final Batch Loss: 0.31815409660339355\n",
      "Epoch 537, Loss: 0.6555963754653931, Final Batch Loss: 0.299046128988266\n",
      "Epoch 538, Loss: 0.7024207711219788, Final Batch Loss: 0.330534964799881\n",
      "Epoch 539, Loss: 0.6452428102493286, Final Batch Loss: 0.33509543538093567\n",
      "Epoch 540, Loss: 0.6996063590049744, Final Batch Loss: 0.31521859765052795\n",
      "Epoch 541, Loss: 0.7155685424804688, Final Batch Loss: 0.3222655951976776\n",
      "Epoch 542, Loss: 0.6753391325473785, Final Batch Loss: 0.26382899284362793\n",
      "Epoch 543, Loss: 0.6048116087913513, Final Batch Loss: 0.244103342294693\n",
      "Epoch 544, Loss: 0.6429993510246277, Final Batch Loss: 0.29397451877593994\n",
      "Epoch 545, Loss: 0.6603927314281464, Final Batch Loss: 0.34301063418388367\n",
      "Epoch 546, Loss: 0.6206100285053253, Final Batch Loss: 0.31706199049949646\n",
      "Epoch 547, Loss: 0.6290864944458008, Final Batch Loss: 0.2850988209247589\n",
      "Epoch 548, Loss: 0.6922349035739899, Final Batch Loss: 0.3354871869087219\n",
      "Epoch 549, Loss: 0.6455878019332886, Final Batch Loss: 0.31086811423301697\n",
      "Epoch 550, Loss: 0.610528290271759, Final Batch Loss: 0.303088903427124\n",
      "Epoch 551, Loss: 0.5811451077461243, Final Batch Loss: 0.2754393517971039\n",
      "Epoch 552, Loss: 0.6526433527469635, Final Batch Loss: 0.34782901406288147\n",
      "Epoch 553, Loss: 0.6129759550094604, Final Batch Loss: 0.27390238642692566\n",
      "Epoch 554, Loss: 0.6902251839637756, Final Batch Loss: 0.34020349383354187\n",
      "Epoch 555, Loss: 0.6726045906543732, Final Batch Loss: 0.31679868698120117\n",
      "Epoch 556, Loss: 0.6958856880664825, Final Batch Loss: 0.33923691511154175\n",
      "Epoch 557, Loss: 0.697575181722641, Final Batch Loss: 0.3854229152202606\n",
      "Epoch 558, Loss: 0.6670548021793365, Final Batch Loss: 0.3779807686805725\n",
      "Epoch 559, Loss: 0.6658237874507904, Final Batch Loss: 0.26539346575737\n",
      "Epoch 560, Loss: 0.6520418226718903, Final Batch Loss: 0.28874021768569946\n",
      "Epoch 561, Loss: 0.6499993205070496, Final Batch Loss: 0.3475547432899475\n",
      "Epoch 562, Loss: 0.604181557893753, Final Batch Loss: 0.31571105122566223\n",
      "Epoch 563, Loss: 0.619105726480484, Final Batch Loss: 0.34974199533462524\n",
      "Epoch 564, Loss: 0.6684096455574036, Final Batch Loss: 0.3150591254234314\n",
      "Epoch 565, Loss: 0.66407710313797, Final Batch Loss: 0.3036952316761017\n",
      "Epoch 566, Loss: 0.6227661967277527, Final Batch Loss: 0.33469730615615845\n",
      "Epoch 567, Loss: 0.6124457120895386, Final Batch Loss: 0.30390775203704834\n",
      "Epoch 568, Loss: 0.6594818532466888, Final Batch Loss: 0.31482842564582825\n",
      "Epoch 569, Loss: 0.6345365643501282, Final Batch Loss: 0.38448816537857056\n",
      "Epoch 570, Loss: 0.6012081503868103, Final Batch Loss: 0.2963774800300598\n",
      "Epoch 571, Loss: 0.650630384683609, Final Batch Loss: 0.306796669960022\n",
      "Epoch 572, Loss: 0.6028697192668915, Final Batch Loss: 0.3070831894874573\n",
      "Epoch 573, Loss: 0.5779596567153931, Final Batch Loss: 0.31176358461380005\n",
      "Epoch 574, Loss: 0.5713667869567871, Final Batch Loss: 0.2761649489402771\n",
      "Epoch 575, Loss: 0.6001268625259399, Final Batch Loss: 0.30039581656455994\n",
      "Epoch 576, Loss: 0.5689069926738739, Final Batch Loss: 0.2938011884689331\n",
      "Epoch 577, Loss: 0.6424683332443237, Final Batch Loss: 0.3427398204803467\n",
      "Epoch 578, Loss: 0.6189071238040924, Final Batch Loss: 0.35730722546577454\n",
      "Epoch 579, Loss: 0.6122508645057678, Final Batch Loss: 0.30640172958374023\n",
      "Epoch 580, Loss: 0.6142920553684235, Final Batch Loss: 0.3135201930999756\n",
      "Epoch 581, Loss: 0.5708256512880325, Final Batch Loss: 0.2314785271883011\n",
      "Epoch 582, Loss: 0.5904656946659088, Final Batch Loss: 0.32249459624290466\n",
      "Epoch 583, Loss: 0.5568915605545044, Final Batch Loss: 0.30237534642219543\n",
      "Epoch 584, Loss: 0.5757473707199097, Final Batch Loss: 0.29468899965286255\n",
      "Epoch 585, Loss: 0.5353911519050598, Final Batch Loss: 0.25419124960899353\n",
      "Epoch 586, Loss: 0.642339289188385, Final Batch Loss: 0.2828453481197357\n",
      "Epoch 587, Loss: 0.592692568898201, Final Batch Loss: 0.3430407643318176\n",
      "Epoch 588, Loss: 0.6598541736602783, Final Batch Loss: 0.3076992332935333\n",
      "Epoch 589, Loss: 0.6285237371921539, Final Batch Loss: 0.30380183458328247\n",
      "Epoch 590, Loss: 0.5900510549545288, Final Batch Loss: 0.2704247832298279\n",
      "Epoch 591, Loss: 0.5851736068725586, Final Batch Loss: 0.2902604341506958\n",
      "Epoch 592, Loss: 0.6293458640575409, Final Batch Loss: 0.27541711926460266\n",
      "Epoch 593, Loss: 0.5958500802516937, Final Batch Loss: 0.2717168927192688\n",
      "Epoch 594, Loss: 0.643144816160202, Final Batch Loss: 0.32219550013542175\n",
      "Epoch 595, Loss: 0.6392175257205963, Final Batch Loss: 0.31375762820243835\n",
      "Epoch 596, Loss: 0.6014933586120605, Final Batch Loss: 0.3450961410999298\n",
      "Epoch 597, Loss: 0.6406784355640411, Final Batch Loss: 0.3167118728160858\n",
      "Epoch 598, Loss: 0.5846903920173645, Final Batch Loss: 0.31040677428245544\n",
      "Epoch 599, Loss: 0.6038752794265747, Final Batch Loss: 0.3184231221675873\n",
      "Epoch 600, Loss: 0.6320672333240509, Final Batch Loss: 0.3197897970676422\n",
      "Epoch 601, Loss: 0.6470032930374146, Final Batch Loss: 0.369617760181427\n",
      "Epoch 602, Loss: 0.6010360419750214, Final Batch Loss: 0.2766115665435791\n",
      "Epoch 603, Loss: 0.5639722347259521, Final Batch Loss: 0.2832581698894501\n",
      "Epoch 604, Loss: 0.6695099472999573, Final Batch Loss: 0.34663739800453186\n",
      "Epoch 605, Loss: 0.5913482010364532, Final Batch Loss: 0.292767196893692\n",
      "Epoch 606, Loss: 0.5735597312450409, Final Batch Loss: 0.27670997381210327\n",
      "Epoch 607, Loss: 0.6157042533159256, Final Batch Loss: 0.24635352194309235\n",
      "Epoch 608, Loss: 0.5855700373649597, Final Batch Loss: 0.2888678312301636\n",
      "Epoch 609, Loss: 0.5899108350276947, Final Batch Loss: 0.3069626986980438\n",
      "Epoch 610, Loss: 0.6296629011631012, Final Batch Loss: 0.34069517254829407\n",
      "Epoch 611, Loss: 0.6104706227779388, Final Batch Loss: 0.31711411476135254\n",
      "Epoch 612, Loss: 0.5972067415714264, Final Batch Loss: 0.3286903500556946\n",
      "Epoch 613, Loss: 0.6079424023628235, Final Batch Loss: 0.29298415780067444\n",
      "Epoch 614, Loss: 0.6028535664081573, Final Batch Loss: 0.26574981212615967\n",
      "Epoch 615, Loss: 0.564549058675766, Final Batch Loss: 0.24931514263153076\n",
      "Epoch 616, Loss: 0.5985419750213623, Final Batch Loss: 0.3121756911277771\n",
      "Epoch 617, Loss: 0.5300532579421997, Final Batch Loss: 0.2743373215198517\n",
      "Epoch 618, Loss: 0.5585946440696716, Final Batch Loss: 0.2825830578804016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 619, Loss: 0.6125830411911011, Final Batch Loss: 0.33700791001319885\n",
      "Epoch 620, Loss: 0.5956385731697083, Final Batch Loss: 0.28533098101615906\n",
      "Epoch 621, Loss: 0.6168025135993958, Final Batch Loss: 0.31493696570396423\n",
      "Epoch 622, Loss: 0.5864737629890442, Final Batch Loss: 0.3024385869503021\n",
      "Epoch 623, Loss: 0.5861012041568756, Final Batch Loss: 0.26631245017051697\n",
      "Epoch 624, Loss: 0.603333592414856, Final Batch Loss: 0.2922765016555786\n",
      "Epoch 625, Loss: 0.5594184398651123, Final Batch Loss: 0.28277602791786194\n",
      "Epoch 626, Loss: 0.6150080561637878, Final Batch Loss: 0.29516294598579407\n",
      "Epoch 627, Loss: 0.5375066995620728, Final Batch Loss: 0.2836238145828247\n",
      "Epoch 628, Loss: 0.6138612926006317, Final Batch Loss: 0.30333781242370605\n",
      "Epoch 629, Loss: 0.5947224497795105, Final Batch Loss: 0.2935940623283386\n",
      "Epoch 630, Loss: 0.53525111079216, Final Batch Loss: 0.2788897752761841\n",
      "Epoch 631, Loss: 0.5545533895492554, Final Batch Loss: 0.30396121740341187\n",
      "Epoch 632, Loss: 0.5953446924686432, Final Batch Loss: 0.2818780541419983\n",
      "Epoch 633, Loss: 0.5833990126848221, Final Batch Loss: 0.24514715373516083\n",
      "Epoch 634, Loss: 0.6110759675502777, Final Batch Loss: 0.33030176162719727\n",
      "Epoch 635, Loss: 0.591706782579422, Final Batch Loss: 0.2923907935619354\n",
      "Epoch 636, Loss: 0.5822581052780151, Final Batch Loss: 0.25712811946868896\n",
      "Epoch 637, Loss: 0.5397856831550598, Final Batch Loss: 0.2936941087245941\n",
      "Epoch 638, Loss: 0.6143937706947327, Final Batch Loss: 0.311073899269104\n",
      "Epoch 639, Loss: 0.551565945148468, Final Batch Loss: 0.28497114777565\n",
      "Epoch 640, Loss: 0.6010658144950867, Final Batch Loss: 0.3411039113998413\n",
      "Epoch 641, Loss: 0.615838348865509, Final Batch Loss: 0.3105998933315277\n",
      "Epoch 642, Loss: 0.5876916944980621, Final Batch Loss: 0.3073313236236572\n",
      "Epoch 643, Loss: 0.5450069904327393, Final Batch Loss: 0.27726954221725464\n",
      "Epoch 644, Loss: 0.5880285501480103, Final Batch Loss: 0.28516969084739685\n",
      "Epoch 645, Loss: 0.5415088683366776, Final Batch Loss: 0.22528167068958282\n",
      "Epoch 646, Loss: 0.5784039795398712, Final Batch Loss: 0.31315702199935913\n",
      "Epoch 647, Loss: 0.5603029429912567, Final Batch Loss: 0.2904762029647827\n",
      "Epoch 648, Loss: 0.5654561370611191, Final Batch Loss: 0.24919207394123077\n",
      "Epoch 649, Loss: 0.596500813961029, Final Batch Loss: 0.28113362193107605\n",
      "Epoch 650, Loss: 0.5497768819332123, Final Batch Loss: 0.2473876178264618\n",
      "Epoch 651, Loss: 0.5437786132097244, Final Batch Loss: 0.2321651130914688\n",
      "Epoch 652, Loss: 0.5548389852046967, Final Batch Loss: 0.2685694098472595\n",
      "Epoch 653, Loss: 0.6085234880447388, Final Batch Loss: 0.2870327830314636\n",
      "Epoch 654, Loss: 0.6433724164962769, Final Batch Loss: 0.32371774315834045\n",
      "Epoch 655, Loss: 0.5598542392253876, Final Batch Loss: 0.3042599558830261\n",
      "Epoch 656, Loss: 0.5216621905565262, Final Batch Loss: 0.27505022287368774\n",
      "Epoch 657, Loss: 0.5618453472852707, Final Batch Loss: 0.2469368427991867\n",
      "Epoch 658, Loss: 0.570352092385292, Final Batch Loss: 0.24348406493663788\n",
      "Epoch 659, Loss: 0.6011003851890564, Final Batch Loss: 0.300816148519516\n",
      "Epoch 660, Loss: 0.5281805694103241, Final Batch Loss: 0.2414950430393219\n",
      "Epoch 661, Loss: 0.6121001243591309, Final Batch Loss: 0.3320908844470978\n",
      "Epoch 662, Loss: 0.5739464163780212, Final Batch Loss: 0.29406100511550903\n",
      "Epoch 663, Loss: 0.5734078884124756, Final Batch Loss: 0.2657146155834198\n",
      "Epoch 664, Loss: 0.6109068393707275, Final Batch Loss: 0.3035343289375305\n",
      "Epoch 665, Loss: 0.6262141168117523, Final Batch Loss: 0.2971954047679901\n",
      "Epoch 666, Loss: 0.5533233284950256, Final Batch Loss: 0.2882611155509949\n",
      "Epoch 667, Loss: 0.5741812884807587, Final Batch Loss: 0.30080661177635193\n",
      "Epoch 668, Loss: 0.5557726919651031, Final Batch Loss: 0.2909723222255707\n",
      "Epoch 669, Loss: 0.5650758743286133, Final Batch Loss: 0.26298272609710693\n",
      "Epoch 670, Loss: 0.6112642139196396, Final Batch Loss: 0.23969845473766327\n",
      "Epoch 671, Loss: 0.5654993951320648, Final Batch Loss: 0.27672192454338074\n",
      "Epoch 672, Loss: 0.5941269993782043, Final Batch Loss: 0.2854616045951843\n",
      "Epoch 673, Loss: 0.598425567150116, Final Batch Loss: 0.3036969304084778\n",
      "Epoch 674, Loss: 0.5266431123018265, Final Batch Loss: 0.24034930765628815\n",
      "Epoch 675, Loss: 0.5390594005584717, Final Batch Loss: 0.2644111216068268\n",
      "Epoch 676, Loss: 0.5717938840389252, Final Batch Loss: 0.29322555661201477\n",
      "Epoch 677, Loss: 0.6005330681800842, Final Batch Loss: 0.259095162153244\n",
      "Epoch 678, Loss: 0.6182425022125244, Final Batch Loss: 0.39404767751693726\n",
      "Epoch 679, Loss: 0.6282958090305328, Final Batch Loss: 0.30583691596984863\n",
      "Epoch 680, Loss: 0.5236051976680756, Final Batch Loss: 0.25836318731307983\n",
      "Epoch 681, Loss: 0.5649388283491135, Final Batch Loss: 0.3171100616455078\n",
      "Epoch 682, Loss: 0.6252656280994415, Final Batch Loss: 0.31025591492652893\n",
      "Epoch 683, Loss: 0.6018912494182587, Final Batch Loss: 0.28435635566711426\n",
      "Epoch 684, Loss: 0.5839916467666626, Final Batch Loss: 0.32030659914016724\n",
      "Epoch 685, Loss: 0.5094982087612152, Final Batch Loss: 0.23210018873214722\n",
      "Epoch 686, Loss: 0.5147958546876907, Final Batch Loss: 0.23333527147769928\n",
      "Epoch 687, Loss: 0.6122576892375946, Final Batch Loss: 0.3107118308544159\n",
      "Epoch 688, Loss: 0.5692195892333984, Final Batch Loss: 0.2595813572406769\n",
      "Epoch 689, Loss: 0.593770295381546, Final Batch Loss: 0.3228531777858734\n",
      "Epoch 690, Loss: 0.5590621680021286, Final Batch Loss: 0.23930545151233673\n",
      "Epoch 691, Loss: 0.5615291893482208, Final Batch Loss: 0.2620543837547302\n",
      "Epoch 692, Loss: 0.5645826160907745, Final Batch Loss: 0.3044109344482422\n",
      "Epoch 693, Loss: 0.622447669506073, Final Batch Loss: 0.33544155955314636\n",
      "Epoch 694, Loss: 0.5338150560855865, Final Batch Loss: 0.26673272252082825\n",
      "Epoch 695, Loss: 0.5911935865879059, Final Batch Loss: 0.2669525444507599\n",
      "Epoch 696, Loss: 0.5321909487247467, Final Batch Loss: 0.2421424388885498\n",
      "Epoch 697, Loss: 0.5053723156452179, Final Batch Loss: 0.2457525134086609\n",
      "Epoch 698, Loss: 0.6260475814342499, Final Batch Loss: 0.3050900101661682\n",
      "Epoch 699, Loss: 0.5370994508266449, Final Batch Loss: 0.2703581154346466\n",
      "Epoch 700, Loss: 0.5464383065700531, Final Batch Loss: 0.26121968030929565\n",
      "Epoch 701, Loss: 0.5360243022441864, Final Batch Loss: 0.2742283344268799\n",
      "Epoch 702, Loss: 0.6166794002056122, Final Batch Loss: 0.28436699509620667\n",
      "Epoch 703, Loss: 0.5496174395084381, Final Batch Loss: 0.2602834105491638\n",
      "Epoch 704, Loss: 0.6147580146789551, Final Batch Loss: 0.32751867175102234\n",
      "Epoch 705, Loss: 0.5567368865013123, Final Batch Loss: 0.27650001645088196\n",
      "Epoch 706, Loss: 0.5751469135284424, Final Batch Loss: 0.3001505732536316\n",
      "Epoch 707, Loss: 0.6076395213603973, Final Batch Loss: 0.32187291979789734\n",
      "Epoch 708, Loss: 0.570936918258667, Final Batch Loss: 0.33752548694610596\n",
      "Epoch 709, Loss: 0.5536841154098511, Final Batch Loss: 0.3059685230255127\n",
      "Epoch 710, Loss: 0.5264857262372971, Final Batch Loss: 0.30099472403526306\n",
      "Epoch 711, Loss: 0.5461812019348145, Final Batch Loss: 0.26919659972190857\n",
      "Epoch 712, Loss: 0.5480373501777649, Final Batch Loss: 0.29190221428871155\n",
      "Epoch 713, Loss: 0.5219421982765198, Final Batch Loss: 0.23737916350364685\n",
      "Epoch 714, Loss: 0.5180695205926895, Final Batch Loss: 0.2250765711069107\n",
      "Epoch 715, Loss: 0.5723626017570496, Final Batch Loss: 0.2929975092411041\n",
      "Epoch 716, Loss: 0.5858522355556488, Final Batch Loss: 0.2948930561542511\n",
      "Epoch 717, Loss: 0.5284640491008759, Final Batch Loss: 0.24668818712234497\n",
      "Epoch 718, Loss: 0.5277940928936005, Final Batch Loss: 0.24757781624794006\n",
      "Epoch 719, Loss: 0.5386279970407486, Final Batch Loss: 0.2358831912279129\n",
      "Epoch 720, Loss: 0.517062783241272, Final Batch Loss: 0.25970080494880676\n",
      "Epoch 721, Loss: 0.5545018315315247, Final Batch Loss: 0.28829264640808105\n",
      "Epoch 722, Loss: 0.5066190212965012, Final Batch Loss: 0.22887305915355682\n",
      "Epoch 723, Loss: 0.5224135667085648, Final Batch Loss: 0.24484457075595856\n",
      "Epoch 724, Loss: 0.5072062462568283, Final Batch Loss: 0.30108433961868286\n",
      "Epoch 725, Loss: 0.5724250078201294, Final Batch Loss: 0.29444876313209534\n",
      "Epoch 726, Loss: 0.5233450531959534, Final Batch Loss: 0.2720394432544708\n",
      "Epoch 727, Loss: 0.5762584805488586, Final Batch Loss: 0.27982568740844727\n",
      "Epoch 728, Loss: 0.5796122401952744, Final Batch Loss: 0.24804897606372833\n",
      "Epoch 729, Loss: 0.5652486830949783, Final Batch Loss: 0.24817831814289093\n",
      "Epoch 730, Loss: 0.5512699782848358, Final Batch Loss: 0.27757754921913147\n",
      "Epoch 731, Loss: 0.5931895673274994, Final Batch Loss: 0.27033817768096924\n",
      "Epoch 732, Loss: 0.5391499400138855, Final Batch Loss: 0.21409404277801514\n",
      "Epoch 733, Loss: 0.5287854224443436, Final Batch Loss: 0.23991747200489044\n",
      "Epoch 734, Loss: 0.6002059280872345, Final Batch Loss: 0.27598387002944946\n",
      "Epoch 735, Loss: 0.49875082075595856, Final Batch Loss: 0.25963157415390015\n",
      "Epoch 736, Loss: 0.5462738871574402, Final Batch Loss: 0.26789307594299316\n",
      "Epoch 737, Loss: 0.5612812638282776, Final Batch Loss: 0.2759770154953003\n",
      "Epoch 738, Loss: 0.5113723278045654, Final Batch Loss: 0.27930548787117004\n",
      "Epoch 739, Loss: 0.5736575126647949, Final Batch Loss: 0.24827957153320312\n",
      "Epoch 740, Loss: 0.5315013080835342, Final Batch Loss: 0.2095695286989212\n",
      "Epoch 741, Loss: 0.5244331657886505, Final Batch Loss: 0.26167866587638855\n",
      "Epoch 742, Loss: 0.5437833368778229, Final Batch Loss: 0.2656552493572235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 743, Loss: 0.5377707779407501, Final Batch Loss: 0.2593080997467041\n",
      "Epoch 744, Loss: 0.5618968307971954, Final Batch Loss: 0.28008833527565\n",
      "Epoch 745, Loss: 0.542630061507225, Final Batch Loss: 0.24236710369586945\n",
      "Epoch 746, Loss: 0.5773952007293701, Final Batch Loss: 0.2966780364513397\n",
      "Epoch 747, Loss: 0.5245439261198044, Final Batch Loss: 0.31048282980918884\n",
      "Epoch 748, Loss: 0.493501678109169, Final Batch Loss: 0.2368541806936264\n",
      "Epoch 749, Loss: 0.5622580349445343, Final Batch Loss: 0.29308611154556274\n",
      "Epoch 750, Loss: 0.5151001662015915, Final Batch Loss: 0.2383430153131485\n",
      "Epoch 751, Loss: 0.5704143941402435, Final Batch Loss: 0.2936861217021942\n",
      "Epoch 752, Loss: 0.531240314245224, Final Batch Loss: 0.26121005415916443\n",
      "Epoch 753, Loss: 0.5095780938863754, Final Batch Loss: 0.24757008254528046\n",
      "Epoch 754, Loss: 0.5132156163454056, Final Batch Loss: 0.2298973947763443\n",
      "Epoch 755, Loss: 0.4948841631412506, Final Batch Loss: 0.24706773459911346\n",
      "Epoch 756, Loss: 0.5306456536054611, Final Batch Loss: 0.24988822638988495\n",
      "Epoch 757, Loss: 0.5128395855426788, Final Batch Loss: 0.2540319561958313\n",
      "Epoch 758, Loss: 0.5548092871904373, Final Batch Loss: 0.2419268935918808\n",
      "Epoch 759, Loss: 0.5401711016893387, Final Batch Loss: 0.2278522104024887\n",
      "Epoch 760, Loss: 0.5470621585845947, Final Batch Loss: 0.2663411796092987\n",
      "Epoch 761, Loss: 0.4988587647676468, Final Batch Loss: 0.234282448887825\n",
      "Epoch 762, Loss: 0.6182873845100403, Final Batch Loss: 0.3305325508117676\n",
      "Epoch 763, Loss: 0.6231762170791626, Final Batch Loss: 0.35514843463897705\n",
      "Epoch 764, Loss: 0.530071347951889, Final Batch Loss: 0.2572254240512848\n",
      "Epoch 765, Loss: 0.5451418161392212, Final Batch Loss: 0.3247997462749481\n",
      "Epoch 766, Loss: 0.5822417736053467, Final Batch Loss: 0.26690149307250977\n",
      "Epoch 767, Loss: 0.5579301118850708, Final Batch Loss: 0.24705025553703308\n",
      "Epoch 768, Loss: 0.5013703107833862, Final Batch Loss: 0.2773582339286804\n",
      "Epoch 769, Loss: 0.5400729477405548, Final Batch Loss: 0.2539580762386322\n",
      "Epoch 770, Loss: 0.5419604182243347, Final Batch Loss: 0.2699427008628845\n",
      "Epoch 771, Loss: 0.5501292049884796, Final Batch Loss: 0.26252907514572144\n",
      "Epoch 772, Loss: 0.5418472588062286, Final Batch Loss: 0.26667243242263794\n",
      "Epoch 773, Loss: 0.5385658144950867, Final Batch Loss: 0.28433534502983093\n",
      "Epoch 774, Loss: 0.4960487484931946, Final Batch Loss: 0.21294191479682922\n",
      "Epoch 775, Loss: 0.5533865839242935, Final Batch Loss: 0.3124149441719055\n",
      "Epoch 776, Loss: 0.5507429540157318, Final Batch Loss: 0.2588787078857422\n",
      "Epoch 777, Loss: 0.5267277956008911, Final Batch Loss: 0.25719764828681946\n",
      "Epoch 778, Loss: 0.5411114245653152, Final Batch Loss: 0.29505759477615356\n",
      "Epoch 779, Loss: 0.5180889964103699, Final Batch Loss: 0.2875715494155884\n",
      "Epoch 780, Loss: 0.5402452051639557, Final Batch Loss: 0.26943421363830566\n",
      "Epoch 781, Loss: 0.5378521084785461, Final Batch Loss: 0.27636849880218506\n",
      "Epoch 782, Loss: 0.5403935760259628, Final Batch Loss: 0.24485529959201813\n",
      "Epoch 783, Loss: 0.5232887864112854, Final Batch Loss: 0.24000892043113708\n",
      "Epoch 784, Loss: 0.4718397855758667, Final Batch Loss: 0.22186899185180664\n",
      "Epoch 785, Loss: 0.5060064792633057, Final Batch Loss: 0.25009191036224365\n",
      "Epoch 786, Loss: 0.5070982426404953, Final Batch Loss: 0.2491428405046463\n",
      "Epoch 787, Loss: 0.5460479557514191, Final Batch Loss: 0.28768858313560486\n",
      "Epoch 788, Loss: 0.5064858794212341, Final Batch Loss: 0.2371695339679718\n",
      "Epoch 789, Loss: 0.5520142614841461, Final Batch Loss: 0.2850537896156311\n",
      "Epoch 790, Loss: 0.5915657877922058, Final Batch Loss: 0.30236735939979553\n",
      "Epoch 791, Loss: 0.5853471457958221, Final Batch Loss: 0.30263516306877136\n",
      "Epoch 792, Loss: 0.554475873708725, Final Batch Loss: 0.2647424340248108\n",
      "Epoch 793, Loss: 0.5163199603557587, Final Batch Loss: 0.23470962047576904\n",
      "Epoch 794, Loss: 0.4956747889518738, Final Batch Loss: 0.22900065779685974\n",
      "Epoch 795, Loss: 0.4841649830341339, Final Batch Loss: 0.2181013524532318\n",
      "Epoch 796, Loss: 0.5139153748750687, Final Batch Loss: 0.2480515092611313\n",
      "Epoch 797, Loss: 0.536835104227066, Final Batch Loss: 0.31552982330322266\n",
      "Epoch 798, Loss: 0.5221085399389267, Final Batch Loss: 0.2439238578081131\n",
      "Epoch 799, Loss: 0.5468446016311646, Final Batch Loss: 0.2898017168045044\n",
      "Epoch 800, Loss: 0.5645530819892883, Final Batch Loss: 0.2988869845867157\n",
      "Epoch 801, Loss: 0.4916606992483139, Final Batch Loss: 0.28183695673942566\n",
      "Epoch 802, Loss: 0.5113148391246796, Final Batch Loss: 0.25445061922073364\n",
      "Epoch 803, Loss: 0.49912941455841064, Final Batch Loss: 0.25251615047454834\n",
      "Epoch 804, Loss: 0.5255003869533539, Final Batch Loss: 0.24357986450195312\n",
      "Epoch 805, Loss: 0.5126309394836426, Final Batch Loss: 0.24189770221710205\n",
      "Epoch 806, Loss: 0.5372674465179443, Final Batch Loss: 0.2952728867530823\n",
      "Epoch 807, Loss: 0.49828100204467773, Final Batch Loss: 0.2506025433540344\n",
      "Epoch 808, Loss: 0.5097164809703827, Final Batch Loss: 0.24722561240196228\n",
      "Epoch 809, Loss: 0.5359302163124084, Final Batch Loss: 0.26653650403022766\n",
      "Epoch 810, Loss: 0.5372551381587982, Final Batch Loss: 0.2768971920013428\n",
      "Epoch 811, Loss: 0.5007712841033936, Final Batch Loss: 0.2418791949748993\n",
      "Epoch 812, Loss: 0.5465478897094727, Final Batch Loss: 0.28989702463150024\n",
      "Epoch 813, Loss: 0.511713057756424, Final Batch Loss: 0.27025261521339417\n",
      "Epoch 814, Loss: 0.51981982588768, Final Batch Loss: 0.30111339688301086\n",
      "Epoch 815, Loss: 0.49047476053237915, Final Batch Loss: 0.25172218680381775\n",
      "Epoch 816, Loss: 0.5320518165826797, Final Batch Loss: 0.2902759611606598\n",
      "Epoch 817, Loss: 0.5124137848615646, Final Batch Loss: 0.23298151791095734\n",
      "Epoch 818, Loss: 0.47250306606292725, Final Batch Loss: 0.24817341566085815\n",
      "Epoch 819, Loss: 0.5040861219167709, Final Batch Loss: 0.2559135854244232\n",
      "Epoch 820, Loss: 0.5090567618608475, Final Batch Loss: 0.24658729135990143\n",
      "Epoch 821, Loss: 0.5650109350681305, Final Batch Loss: 0.30122414231300354\n",
      "Epoch 822, Loss: 0.5290607362985611, Final Batch Loss: 0.2831292748451233\n",
      "Epoch 823, Loss: 0.537074401974678, Final Batch Loss: 0.24269746243953705\n",
      "Epoch 824, Loss: 0.4798712581396103, Final Batch Loss: 0.23465390503406525\n",
      "Epoch 825, Loss: 0.5022837072610855, Final Batch Loss: 0.25321072340011597\n",
      "Epoch 826, Loss: 0.5667674541473389, Final Batch Loss: 0.2633521854877472\n",
      "Epoch 827, Loss: 0.5110795050859451, Final Batch Loss: 0.24605299532413483\n",
      "Epoch 828, Loss: 0.4950047731399536, Final Batch Loss: 0.23858597874641418\n",
      "Epoch 829, Loss: 0.4978440850973129, Final Batch Loss: 0.2590959966182709\n",
      "Epoch 830, Loss: 0.43993987143039703, Final Batch Loss: 0.1832703799009323\n",
      "Epoch 831, Loss: 0.5225156992673874, Final Batch Loss: 0.28142160177230835\n",
      "Epoch 832, Loss: 0.48722440004348755, Final Batch Loss: 0.22604915499687195\n",
      "Epoch 833, Loss: 0.5328090041875839, Final Batch Loss: 0.24143566191196442\n",
      "Epoch 834, Loss: 0.5190752893686295, Final Batch Loss: 0.21042583882808685\n",
      "Epoch 835, Loss: 0.5507509112358093, Final Batch Loss: 0.256572425365448\n",
      "Epoch 836, Loss: 0.4793903976678848, Final Batch Loss: 0.25166019797325134\n",
      "Epoch 837, Loss: 0.5045625120401382, Final Batch Loss: 0.29099196195602417\n",
      "Epoch 838, Loss: 0.5401607900857925, Final Batch Loss: 0.29082241654396057\n",
      "Epoch 839, Loss: 0.4828714281320572, Final Batch Loss: 0.26510894298553467\n",
      "Epoch 840, Loss: 0.49194851517677307, Final Batch Loss: 0.2735191881656647\n",
      "Epoch 841, Loss: 0.5274283289909363, Final Batch Loss: 0.2511979043483734\n",
      "Epoch 842, Loss: 0.5731138437986374, Final Batch Loss: 0.3256473243236542\n",
      "Epoch 843, Loss: 0.5164308845996857, Final Batch Loss: 0.2529926002025604\n",
      "Epoch 844, Loss: 0.5257239043712616, Final Batch Loss: 0.2475939393043518\n",
      "Epoch 845, Loss: 0.5334699153900146, Final Batch Loss: 0.2873050272464752\n",
      "Epoch 846, Loss: 0.48769819736480713, Final Batch Loss: 0.26110801100730896\n",
      "Epoch 847, Loss: 0.5824083387851715, Final Batch Loss: 0.3127521574497223\n",
      "Epoch 848, Loss: 0.5132166296243668, Final Batch Loss: 0.2404807060956955\n",
      "Epoch 849, Loss: 0.47537584602832794, Final Batch Loss: 0.2492513656616211\n",
      "Epoch 850, Loss: 0.5561514496803284, Final Batch Loss: 0.3220527768135071\n",
      "Epoch 851, Loss: 0.5044789761304855, Final Batch Loss: 0.27546361088752747\n",
      "Epoch 852, Loss: 0.5371229946613312, Final Batch Loss: 0.27805888652801514\n",
      "Epoch 853, Loss: 0.4931584596633911, Final Batch Loss: 0.2581063210964203\n",
      "Epoch 854, Loss: 0.5465399622917175, Final Batch Loss: 0.21400213241577148\n",
      "Epoch 855, Loss: 0.4949653148651123, Final Batch Loss: 0.25138792395591736\n",
      "Epoch 856, Loss: 0.4755038022994995, Final Batch Loss: 0.23671568930149078\n",
      "Epoch 857, Loss: 0.5333104878664017, Final Batch Loss: 0.23456482589244843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 858, Loss: 0.5167044997215271, Final Batch Loss: 0.2550466060638428\n",
      "Epoch 859, Loss: 0.5425248742103577, Final Batch Loss: 0.22562363743782043\n",
      "Epoch 860, Loss: 0.4674598127603531, Final Batch Loss: 0.2173692137002945\n",
      "Epoch 861, Loss: 0.4964914917945862, Final Batch Loss: 0.20831334590911865\n",
      "Epoch 862, Loss: 0.5934013724327087, Final Batch Loss: 0.321930855512619\n",
      "Epoch 863, Loss: 0.45399874448776245, Final Batch Loss: 0.180374413728714\n",
      "Epoch 864, Loss: 0.4949209690093994, Final Batch Loss: 0.2458765059709549\n",
      "Epoch 865, Loss: 0.46334581077098846, Final Batch Loss: 0.23385575413703918\n",
      "Epoch 866, Loss: 0.5419861376285553, Final Batch Loss: 0.28395166993141174\n",
      "Epoch 867, Loss: 0.4791632741689682, Final Batch Loss: 0.2654060423374176\n",
      "Epoch 868, Loss: 0.5322445780038834, Final Batch Loss: 0.2878330647945404\n",
      "Epoch 869, Loss: 0.4569166451692581, Final Batch Loss: 0.239376038312912\n",
      "Epoch 870, Loss: 0.5572150200605392, Final Batch Loss: 0.23937036097049713\n",
      "Epoch 871, Loss: 0.4875919073820114, Final Batch Loss: 0.23264943063259125\n",
      "Epoch 872, Loss: 0.47012969851493835, Final Batch Loss: 0.22896063327789307\n",
      "Epoch 873, Loss: 0.5060381889343262, Final Batch Loss: 0.22978487610816956\n",
      "Epoch 874, Loss: 0.5031826794147491, Final Batch Loss: 0.24334630370140076\n",
      "Epoch 875, Loss: 0.5232635885477066, Final Batch Loss: 0.24922184646129608\n",
      "Epoch 876, Loss: 0.4833761006593704, Final Batch Loss: 0.21101893484592438\n",
      "Epoch 877, Loss: 0.4940919131040573, Final Batch Loss: 0.24687965214252472\n",
      "Epoch 878, Loss: 0.5165191292762756, Final Batch Loss: 0.24017333984375\n",
      "Epoch 879, Loss: 0.4872899055480957, Final Batch Loss: 0.24543572962284088\n",
      "Epoch 880, Loss: 0.49036015570163727, Final Batch Loss: 0.23143859207630157\n",
      "Epoch 881, Loss: 0.5339109599590302, Final Batch Loss: 0.24541381001472473\n",
      "Epoch 882, Loss: 0.4542253315448761, Final Batch Loss: 0.20519837737083435\n",
      "Epoch 883, Loss: 0.5096302032470703, Final Batch Loss: 0.2168656289577484\n",
      "Epoch 884, Loss: 0.4489928334951401, Final Batch Loss: 0.25258371233940125\n",
      "Epoch 885, Loss: 0.5302314162254333, Final Batch Loss: 0.27365627884864807\n",
      "Epoch 886, Loss: 0.4951286017894745, Final Batch Loss: 0.2511489689350128\n",
      "Epoch 887, Loss: 0.5779727399349213, Final Batch Loss: 0.3238660395145416\n",
      "Epoch 888, Loss: 0.47697360813617706, Final Batch Loss: 0.23788568377494812\n",
      "Epoch 889, Loss: 0.49531765282154083, Final Batch Loss: 0.2253369837999344\n",
      "Epoch 890, Loss: 0.48771806061267853, Final Batch Loss: 0.24744412302970886\n",
      "Epoch 891, Loss: 0.4749352037906647, Final Batch Loss: 0.2385416030883789\n",
      "Epoch 892, Loss: 0.5047397613525391, Final Batch Loss: 0.2466965913772583\n",
      "Epoch 893, Loss: 0.475375771522522, Final Batch Loss: 0.24448414146900177\n",
      "Epoch 894, Loss: 0.4700806587934494, Final Batch Loss: 0.2477576583623886\n",
      "Epoch 895, Loss: 0.4728051871061325, Final Batch Loss: 0.2279096394777298\n",
      "Epoch 896, Loss: 0.46078304946422577, Final Batch Loss: 0.23242168128490448\n",
      "Epoch 897, Loss: 0.5024832040071487, Final Batch Loss: 0.26082658767700195\n",
      "Epoch 898, Loss: 0.5085895955562592, Final Batch Loss: 0.21921521425247192\n",
      "Epoch 899, Loss: 0.46623288094997406, Final Batch Loss: 0.21017517149448395\n",
      "Epoch 900, Loss: 0.48027503490448, Final Batch Loss: 0.20926758646965027\n",
      "Epoch 901, Loss: 0.48131148517131805, Final Batch Loss: 0.23454459011554718\n",
      "Epoch 902, Loss: 0.4843909442424774, Final Batch Loss: 0.21719104051589966\n",
      "Epoch 903, Loss: 0.5191673934459686, Final Batch Loss: 0.2668312191963196\n",
      "Epoch 904, Loss: 0.47952334582805634, Final Batch Loss: 0.2106800228357315\n",
      "Epoch 905, Loss: 0.5202153772115707, Final Batch Loss: 0.24476714432239532\n",
      "Epoch 906, Loss: 0.4929967522621155, Final Batch Loss: 0.23823297023773193\n",
      "Epoch 907, Loss: 0.514459490776062, Final Batch Loss: 0.2420939803123474\n",
      "Epoch 908, Loss: 0.48291847109794617, Final Batch Loss: 0.22579216957092285\n",
      "Epoch 909, Loss: 0.47491684556007385, Final Batch Loss: 0.23973920941352844\n",
      "Epoch 910, Loss: 0.479278564453125, Final Batch Loss: 0.2412051558494568\n",
      "Epoch 911, Loss: 0.5110948383808136, Final Batch Loss: 0.2479192018508911\n",
      "Epoch 912, Loss: 0.5115491598844528, Final Batch Loss: 0.2471398562192917\n",
      "Epoch 913, Loss: 0.4633125960826874, Final Batch Loss: 0.2719678580760956\n",
      "Epoch 914, Loss: 0.5000119507312775, Final Batch Loss: 0.22981253266334534\n",
      "Epoch 915, Loss: 0.4764249920845032, Final Batch Loss: 0.2465752214193344\n",
      "Epoch 916, Loss: 0.5267860889434814, Final Batch Loss: 0.20950815081596375\n",
      "Epoch 917, Loss: 0.4725082516670227, Final Batch Loss: 0.21107858419418335\n",
      "Epoch 918, Loss: 0.46828025579452515, Final Batch Loss: 0.21238180994987488\n",
      "Epoch 919, Loss: 0.47168754041194916, Final Batch Loss: 0.22970369458198547\n",
      "Epoch 920, Loss: 0.5090334266424179, Final Batch Loss: 0.23291771113872528\n",
      "Epoch 921, Loss: 0.46278049051761627, Final Batch Loss: 0.247771218419075\n",
      "Epoch 922, Loss: 0.4967665821313858, Final Batch Loss: 0.23007015883922577\n",
      "Epoch 923, Loss: 0.5315766036510468, Final Batch Loss: 0.2675904333591461\n",
      "Epoch 924, Loss: 0.4915240854024887, Final Batch Loss: 0.25866928696632385\n",
      "Epoch 925, Loss: 0.4594368040561676, Final Batch Loss: 0.23249180614948273\n",
      "Epoch 926, Loss: 0.5016769766807556, Final Batch Loss: 0.2570810616016388\n",
      "Epoch 927, Loss: 0.5021923333406448, Final Batch Loss: 0.2959175407886505\n",
      "Epoch 928, Loss: 0.49726317822933197, Final Batch Loss: 0.2370990663766861\n",
      "Epoch 929, Loss: 0.48708540201187134, Final Batch Loss: 0.238983616232872\n",
      "Epoch 930, Loss: 0.4972366839647293, Final Batch Loss: 0.24870887398719788\n",
      "Epoch 931, Loss: 0.5429031997919083, Final Batch Loss: 0.2342318445444107\n",
      "Epoch 932, Loss: 0.48795437812805176, Final Batch Loss: 0.2242620587348938\n",
      "Epoch 933, Loss: 0.4964098632335663, Final Batch Loss: 0.263971209526062\n",
      "Epoch 934, Loss: 0.4808988571166992, Final Batch Loss: 0.22169342637062073\n",
      "Epoch 935, Loss: 0.468354195356369, Final Batch Loss: 0.1987684667110443\n",
      "Epoch 936, Loss: 0.5669814944267273, Final Batch Loss: 0.25749796628952026\n",
      "Epoch 937, Loss: 0.49988144636154175, Final Batch Loss: 0.2699044644832611\n",
      "Epoch 938, Loss: 0.46202220022678375, Final Batch Loss: 0.22089989483356476\n",
      "Epoch 939, Loss: 0.5162324756383896, Final Batch Loss: 0.2945789396762848\n",
      "Epoch 940, Loss: 0.5165062844753265, Final Batch Loss: 0.2375170886516571\n",
      "Epoch 941, Loss: 0.4807455986738205, Final Batch Loss: 0.22007514536380768\n",
      "Epoch 942, Loss: 0.5436031520366669, Final Batch Loss: 0.25774678587913513\n",
      "Epoch 943, Loss: 0.48572851717472076, Final Batch Loss: 0.22062824666500092\n",
      "Epoch 944, Loss: 0.5231524407863617, Final Batch Loss: 0.2572019398212433\n",
      "Epoch 945, Loss: 0.5011294186115265, Final Batch Loss: 0.27408894896507263\n",
      "Epoch 946, Loss: 0.5039162337779999, Final Batch Loss: 0.276033490896225\n",
      "Epoch 947, Loss: 0.5213915705680847, Final Batch Loss: 0.2771185636520386\n",
      "Epoch 948, Loss: 0.4723222851753235, Final Batch Loss: 0.2497584968805313\n",
      "Epoch 949, Loss: 0.508116289973259, Final Batch Loss: 0.2682708501815796\n",
      "Epoch 950, Loss: 0.47346752882003784, Final Batch Loss: 0.2327623814344406\n",
      "Epoch 951, Loss: 0.4887147694826126, Final Batch Loss: 0.2166534811258316\n",
      "Epoch 952, Loss: 0.5114264190196991, Final Batch Loss: 0.2698996961116791\n",
      "Epoch 953, Loss: 0.46803706884384155, Final Batch Loss: 0.22593840956687927\n",
      "Epoch 954, Loss: 0.5012290775775909, Final Batch Loss: 0.23613852262496948\n",
      "Epoch 955, Loss: 0.46544642746448517, Final Batch Loss: 0.24524648487567902\n",
      "Epoch 956, Loss: 0.5672447681427002, Final Batch Loss: 0.33143019676208496\n",
      "Epoch 957, Loss: 0.44703568518161774, Final Batch Loss: 0.21802084147930145\n",
      "Epoch 958, Loss: 0.5060209631919861, Final Batch Loss: 0.2510744631290436\n",
      "Epoch 959, Loss: 0.45885904133319855, Final Batch Loss: 0.25717592239379883\n",
      "Epoch 960, Loss: 0.5023442506790161, Final Batch Loss: 0.27316683530807495\n",
      "Epoch 961, Loss: 0.48214326798915863, Final Batch Loss: 0.21297292411327362\n",
      "Epoch 962, Loss: 0.49492599070072174, Final Batch Loss: 0.28483790159225464\n",
      "Epoch 963, Loss: 0.4869256168603897, Final Batch Loss: 0.21428675949573517\n",
      "Epoch 964, Loss: 0.547268807888031, Final Batch Loss: 0.3103083670139313\n",
      "Epoch 965, Loss: 0.46952709555625916, Final Batch Loss: 0.24344895780086517\n",
      "Epoch 966, Loss: 0.5186725109815598, Final Batch Loss: 0.22621460258960724\n",
      "Epoch 967, Loss: 0.4507710337638855, Final Batch Loss: 0.2555842101573944\n",
      "Epoch 968, Loss: 0.4566838890314102, Final Batch Loss: 0.20146100223064423\n",
      "Epoch 969, Loss: 0.4759836941957474, Final Batch Loss: 0.27129927277565\n",
      "Epoch 970, Loss: 0.4994266480207443, Final Batch Loss: 0.2504545748233795\n",
      "Epoch 971, Loss: 0.49671967327594757, Final Batch Loss: 0.2741977870464325\n",
      "Epoch 972, Loss: 0.4667365998029709, Final Batch Loss: 0.2250674068927765\n",
      "Epoch 973, Loss: 0.5147525370121002, Final Batch Loss: 0.25094661116600037\n",
      "Epoch 974, Loss: 0.48306043446063995, Final Batch Loss: 0.2169017642736435\n",
      "Epoch 975, Loss: 0.48757416009902954, Final Batch Loss: 0.2513022720813751\n",
      "Epoch 976, Loss: 0.5463140308856964, Final Batch Loss: 0.3404932916164398\n",
      "Epoch 977, Loss: 0.48247021436691284, Final Batch Loss: 0.2497507482767105\n",
      "Epoch 978, Loss: 0.5065719187259674, Final Batch Loss: 0.23906293511390686\n",
      "Epoch 979, Loss: 0.4840843826532364, Final Batch Loss: 0.278803288936615\n",
      "Epoch 980, Loss: 0.49897849559783936, Final Batch Loss: 0.28560248017311096\n",
      "Epoch 981, Loss: 0.47615790367126465, Final Batch Loss: 0.21677130460739136\n",
      "Epoch 982, Loss: 0.4696260839700699, Final Batch Loss: 0.2031271904706955\n",
      "Epoch 983, Loss: 0.4830556809902191, Final Batch Loss: 0.21733298897743225\n",
      "Epoch 984, Loss: 0.4866199493408203, Final Batch Loss: 0.26838502287864685\n",
      "Epoch 985, Loss: 0.483759805560112, Final Batch Loss: 0.219054713845253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986, Loss: 0.4387296438217163, Final Batch Loss: 0.21578332781791687\n",
      "Epoch 987, Loss: 0.4554750770330429, Final Batch Loss: 0.21940135955810547\n",
      "Epoch 988, Loss: 0.46799227595329285, Final Batch Loss: 0.2401169240474701\n",
      "Epoch 989, Loss: 0.5076595097780228, Final Batch Loss: 0.29046210646629333\n",
      "Epoch 990, Loss: 0.46870332956314087, Final Batch Loss: 0.2354733794927597\n",
      "Epoch 991, Loss: 0.5354191064834595, Final Batch Loss: 0.2743333876132965\n",
      "Epoch 992, Loss: 0.44824692606925964, Final Batch Loss: 0.2281275987625122\n",
      "Epoch 993, Loss: 0.49535414576530457, Final Batch Loss: 0.23028680682182312\n",
      "Epoch 994, Loss: 0.4441376328468323, Final Batch Loss: 0.24040141701698303\n",
      "Epoch 995, Loss: 0.43630507588386536, Final Batch Loss: 0.22369183599948883\n",
      "Epoch 996, Loss: 0.4760608524084091, Final Batch Loss: 0.24727387726306915\n",
      "Epoch 997, Loss: 0.500202476978302, Final Batch Loss: 0.22849106788635254\n",
      "Epoch 998, Loss: 0.5630232989788055, Final Batch Loss: 0.29117992520332336\n",
      "Epoch 999, Loss: 0.5369066894054413, Final Batch Loss: 0.24782001972198486\n",
      "Epoch 1000, Loss: 0.4527134597301483, Final Batch Loss: 0.2356216311454773\n",
      "Epoch 1001, Loss: 0.48269572854042053, Final Batch Loss: 0.20186808705329895\n",
      "Epoch 1002, Loss: 0.48989301919937134, Final Batch Loss: 0.24521800875663757\n",
      "Epoch 1003, Loss: 0.4740282893180847, Final Batch Loss: 0.22849194705486298\n",
      "Epoch 1004, Loss: 0.5047587603330612, Final Batch Loss: 0.29383623600006104\n",
      "Epoch 1005, Loss: 0.48808297514915466, Final Batch Loss: 0.2401382029056549\n",
      "Epoch 1006, Loss: 0.514705091714859, Final Batch Loss: 0.302044153213501\n",
      "Epoch 1007, Loss: 0.5071452260017395, Final Batch Loss: 0.2564375102519989\n",
      "Epoch 1008, Loss: 0.4977053254842758, Final Batch Loss: 0.25081348419189453\n",
      "Epoch 1009, Loss: 0.4807876646518707, Final Batch Loss: 0.22456330060958862\n",
      "Epoch 1010, Loss: 0.487620010972023, Final Batch Loss: 0.2607786953449249\n",
      "Epoch 1011, Loss: 0.46867069602012634, Final Batch Loss: 0.2453726828098297\n",
      "Epoch 1012, Loss: 0.4997503310441971, Final Batch Loss: 0.23479916155338287\n",
      "Epoch 1013, Loss: 0.4273102879524231, Final Batch Loss: 0.2337733507156372\n",
      "Epoch 1014, Loss: 0.46639716625213623, Final Batch Loss: 0.22921445965766907\n",
      "Epoch 1015, Loss: 0.5148726403713226, Final Batch Loss: 0.3102625608444214\n",
      "Epoch 1016, Loss: 0.526983916759491, Final Batch Loss: 0.2681356966495514\n",
      "Epoch 1017, Loss: 0.5295133888721466, Final Batch Loss: 0.2552641034126282\n",
      "Epoch 1018, Loss: 0.42017118632793427, Final Batch Loss: 0.19942986965179443\n",
      "Epoch 1019, Loss: 0.4507555365562439, Final Batch Loss: 0.20358507335186005\n",
      "Epoch 1020, Loss: 0.5075681805610657, Final Batch Loss: 0.26662978529930115\n",
      "Epoch 1021, Loss: 0.42232079803943634, Final Batch Loss: 0.24676010012626648\n",
      "Epoch 1022, Loss: 0.4742172211408615, Final Batch Loss: 0.24294181168079376\n",
      "Epoch 1023, Loss: 0.4700571149587631, Final Batch Loss: 0.21238242089748383\n",
      "Epoch 1024, Loss: 0.478700190782547, Final Batch Loss: 0.22050803899765015\n",
      "Epoch 1025, Loss: 0.4279244840145111, Final Batch Loss: 0.22056759893894196\n",
      "Epoch 1026, Loss: 0.4901067167520523, Final Batch Loss: 0.29206204414367676\n",
      "Epoch 1027, Loss: 0.4520064741373062, Final Batch Loss: 0.23555617034435272\n",
      "Epoch 1028, Loss: 0.4679665267467499, Final Batch Loss: 0.2378651201725006\n",
      "Epoch 1029, Loss: 0.45047613978385925, Final Batch Loss: 0.23109813034534454\n",
      "Epoch 1030, Loss: 0.48363320529460907, Final Batch Loss: 0.22116003930568695\n",
      "Epoch 1031, Loss: 0.49049121141433716, Final Batch Loss: 0.22387230396270752\n",
      "Epoch 1032, Loss: 0.4851037710905075, Final Batch Loss: 0.23520012199878693\n",
      "Epoch 1033, Loss: 0.499519944190979, Final Batch Loss: 0.20645630359649658\n",
      "Epoch 1034, Loss: 0.4852467328310013, Final Batch Loss: 0.26723548769950867\n",
      "Epoch 1035, Loss: 0.4632943719625473, Final Batch Loss: 0.2561710774898529\n",
      "Epoch 1036, Loss: 0.4648457467556, Final Batch Loss: 0.21895365417003632\n",
      "Epoch 1037, Loss: 0.4827076494693756, Final Batch Loss: 0.21989578008651733\n",
      "Epoch 1038, Loss: 0.44489359855651855, Final Batch Loss: 0.23902906477451324\n",
      "Epoch 1039, Loss: 0.5021162182092667, Final Batch Loss: 0.27269792556762695\n",
      "Epoch 1040, Loss: 0.48437516391277313, Final Batch Loss: 0.2587592303752899\n",
      "Epoch 1041, Loss: 0.4408997744321823, Final Batch Loss: 0.23618851602077484\n",
      "Epoch 1042, Loss: 0.45406240224838257, Final Batch Loss: 0.25010478496551514\n",
      "Epoch 1043, Loss: 0.4436325281858444, Final Batch Loss: 0.1916111558675766\n",
      "Epoch 1044, Loss: 0.46877841651439667, Final Batch Loss: 0.25141245126724243\n",
      "Epoch 1045, Loss: 0.4934517592191696, Final Batch Loss: 0.24538706243038177\n",
      "Epoch 1046, Loss: 0.4931200295686722, Final Batch Loss: 0.2665857970714569\n",
      "Epoch 1047, Loss: 0.4146214574575424, Final Batch Loss: 0.19456493854522705\n",
      "Epoch 1048, Loss: 0.42762117087841034, Final Batch Loss: 0.22191394865512848\n",
      "Epoch 1049, Loss: 0.45645375549793243, Final Batch Loss: 0.20007382333278656\n",
      "Epoch 1050, Loss: 0.43821336328983307, Final Batch Loss: 0.24357978999614716\n",
      "Epoch 1051, Loss: 0.49227048456668854, Final Batch Loss: 0.22836808860301971\n",
      "Epoch 1052, Loss: 0.44536569714546204, Final Batch Loss: 0.22288884222507477\n",
      "Epoch 1053, Loss: 0.501975804567337, Final Batch Loss: 0.24190878868103027\n",
      "Epoch 1054, Loss: 0.44971761107444763, Final Batch Loss: 0.22493566572666168\n",
      "Epoch 1055, Loss: 0.46232540905475616, Final Batch Loss: 0.25101473927497864\n",
      "Epoch 1056, Loss: 0.4777297079563141, Final Batch Loss: 0.23243577778339386\n",
      "Epoch 1057, Loss: 0.4662850499153137, Final Batch Loss: 0.23323579132556915\n",
      "Epoch 1058, Loss: 0.4643837809562683, Final Batch Loss: 0.24366140365600586\n",
      "Epoch 1059, Loss: 0.45462483167648315, Final Batch Loss: 0.2258632332086563\n",
      "Epoch 1060, Loss: 0.49542833864688873, Final Batch Loss: 0.27522239089012146\n",
      "Epoch 1061, Loss: 0.4930286407470703, Final Batch Loss: 0.2870710790157318\n",
      "Epoch 1062, Loss: 0.5024105161428452, Final Batch Loss: 0.23047401010990143\n",
      "Epoch 1063, Loss: 0.4691270887851715, Final Batch Loss: 0.2194969207048416\n",
      "Epoch 1064, Loss: 0.4497968852519989, Final Batch Loss: 0.20737580955028534\n",
      "Epoch 1065, Loss: 0.460404634475708, Final Batch Loss: 0.2410431206226349\n",
      "Epoch 1066, Loss: 0.5075821280479431, Final Batch Loss: 0.2611083686351776\n",
      "Epoch 1067, Loss: 0.4586111605167389, Final Batch Loss: 0.24468658864498138\n",
      "Epoch 1068, Loss: 0.4679608792066574, Final Batch Loss: 0.2742857038974762\n",
      "Epoch 1069, Loss: 0.4629637897014618, Final Batch Loss: 0.2136332094669342\n",
      "Epoch 1070, Loss: 0.44493719935417175, Final Batch Loss: 0.24627000093460083\n",
      "Epoch 1071, Loss: 0.5024278312921524, Final Batch Loss: 0.23505748808383942\n",
      "Epoch 1072, Loss: 0.4456108510494232, Final Batch Loss: 0.22334156930446625\n",
      "Epoch 1073, Loss: 0.43171098828315735, Final Batch Loss: 0.22187113761901855\n",
      "Epoch 1074, Loss: 0.5018051862716675, Final Batch Loss: 0.26589515805244446\n",
      "Epoch 1075, Loss: 0.4547269642353058, Final Batch Loss: 0.21427889168262482\n",
      "Epoch 1076, Loss: 0.46817848086357117, Final Batch Loss: 0.2634185552597046\n",
      "Epoch 1077, Loss: 0.4424413740634918, Final Batch Loss: 0.21425816416740417\n",
      "Epoch 1078, Loss: 0.4333667755126953, Final Batch Loss: 0.22939525544643402\n",
      "Epoch 1079, Loss: 0.43328288197517395, Final Batch Loss: 0.22754742205142975\n",
      "Epoch 1080, Loss: 0.44488656520843506, Final Batch Loss: 0.20664635300636292\n",
      "Epoch 1081, Loss: 0.4210794270038605, Final Batch Loss: 0.21944990754127502\n",
      "Epoch 1082, Loss: 0.46014204621315, Final Batch Loss: 0.2059263288974762\n",
      "Epoch 1083, Loss: 0.5054127722978592, Final Batch Loss: 0.213014617562294\n",
      "Epoch 1084, Loss: 0.43947236239910126, Final Batch Loss: 0.2303415834903717\n",
      "Epoch 1085, Loss: 0.4342828392982483, Final Batch Loss: 0.23377428948879242\n",
      "Epoch 1086, Loss: 0.49558791518211365, Final Batch Loss: 0.22566193342208862\n",
      "Epoch 1087, Loss: 0.4604261815547943, Final Batch Loss: 0.22533990442752838\n",
      "Epoch 1088, Loss: 0.4572553038597107, Final Batch Loss: 0.23356233537197113\n",
      "Epoch 1089, Loss: 0.40824173390865326, Final Batch Loss: 0.20096294581890106\n",
      "Epoch 1090, Loss: 0.436543270945549, Final Batch Loss: 0.2316591590642929\n",
      "Epoch 1091, Loss: 0.43909792602062225, Final Batch Loss: 0.22612684965133667\n",
      "Epoch 1092, Loss: 0.47548162937164307, Final Batch Loss: 0.26385602355003357\n",
      "Epoch 1093, Loss: 0.473614439368248, Final Batch Loss: 0.2256857305765152\n",
      "Epoch 1094, Loss: 0.43192556500434875, Final Batch Loss: 0.21741878986358643\n",
      "Epoch 1095, Loss: 0.43589726090431213, Final Batch Loss: 0.24878104031085968\n",
      "Epoch 1096, Loss: 0.49644336104393005, Final Batch Loss: 0.18722528219223022\n",
      "Epoch 1097, Loss: 0.5149930417537689, Final Batch Loss: 0.28330469131469727\n",
      "Epoch 1098, Loss: 0.438362792134285, Final Batch Loss: 0.1757751852273941\n",
      "Epoch 1099, Loss: 0.4366607218980789, Final Batch Loss: 0.22837303578853607\n",
      "Epoch 1100, Loss: 0.46294958889484406, Final Batch Loss: 0.2539127767086029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1101, Loss: 0.4662153124809265, Final Batch Loss: 0.22258901596069336\n",
      "Epoch 1102, Loss: 0.5135054141283035, Final Batch Loss: 0.27595651149749756\n",
      "Epoch 1103, Loss: 0.42651228606700897, Final Batch Loss: 0.20372922718524933\n",
      "Epoch 1104, Loss: 0.43091540038585663, Final Batch Loss: 0.21997088193893433\n",
      "Epoch 1105, Loss: 0.39793138206005096, Final Batch Loss: 0.19536210596561432\n",
      "Epoch 1106, Loss: 0.4818122684955597, Final Batch Loss: 0.2303071916103363\n",
      "Epoch 1107, Loss: 0.4429764747619629, Final Batch Loss: 0.24289511144161224\n",
      "Epoch 1108, Loss: 0.46498166024684906, Final Batch Loss: 0.23056910932064056\n",
      "Epoch 1109, Loss: 0.4923849105834961, Final Batch Loss: 0.2745908498764038\n",
      "Epoch 1110, Loss: 0.4476421922445297, Final Batch Loss: 0.2292845994234085\n",
      "Epoch 1111, Loss: 0.4901701956987381, Final Batch Loss: 0.21423228085041046\n",
      "Epoch 1112, Loss: 0.4551326334476471, Final Batch Loss: 0.2059219479560852\n",
      "Epoch 1113, Loss: 0.46744492650032043, Final Batch Loss: 0.22772416472434998\n",
      "Epoch 1114, Loss: 0.4804805517196655, Final Batch Loss: 0.24578417837619781\n",
      "Epoch 1115, Loss: 0.4351176470518112, Final Batch Loss: 0.2294756919145584\n",
      "Epoch 1116, Loss: 0.43370339274406433, Final Batch Loss: 0.23936045169830322\n",
      "Epoch 1117, Loss: 0.44525226950645447, Final Batch Loss: 0.24222753942012787\n",
      "Epoch 1118, Loss: 0.4795220345258713, Final Batch Loss: 0.24704746901988983\n",
      "Epoch 1119, Loss: 0.44524085521698, Final Batch Loss: 0.2154008150100708\n",
      "Epoch 1120, Loss: 0.5165876001119614, Final Batch Loss: 0.2932804524898529\n",
      "Epoch 1121, Loss: 0.4253023862838745, Final Batch Loss: 0.19101831316947937\n",
      "Epoch 1122, Loss: 0.44857004284858704, Final Batch Loss: 0.25420984625816345\n",
      "Epoch 1123, Loss: 0.41805794835090637, Final Batch Loss: 0.23002372682094574\n",
      "Epoch 1124, Loss: 0.4551074653863907, Final Batch Loss: 0.21304801106452942\n",
      "Epoch 1125, Loss: 0.4498883932828903, Final Batch Loss: 0.23420436680316925\n",
      "Epoch 1126, Loss: 0.45729871094226837, Final Batch Loss: 0.23630988597869873\n",
      "Epoch 1127, Loss: 0.44346483051776886, Final Batch Loss: 0.24341711401939392\n",
      "Epoch 1128, Loss: 0.4285277724266052, Final Batch Loss: 0.19834135472774506\n",
      "Epoch 1129, Loss: 0.4266349971294403, Final Batch Loss: 0.2237875759601593\n",
      "Epoch 1130, Loss: 0.4429602473974228, Final Batch Loss: 0.2118869572877884\n",
      "Epoch 1131, Loss: 0.3980737179517746, Final Batch Loss: 0.18188339471817017\n",
      "Epoch 1132, Loss: 0.463837206363678, Final Batch Loss: 0.23699694871902466\n",
      "Epoch 1133, Loss: 0.4285508543252945, Final Batch Loss: 0.2364054024219513\n",
      "Epoch 1134, Loss: 0.4190759211778641, Final Batch Loss: 0.1720251739025116\n",
      "Epoch 1135, Loss: 0.4770496189594269, Final Batch Loss: 0.2578572630882263\n",
      "Epoch 1136, Loss: 0.4607758969068527, Final Batch Loss: 0.2516370415687561\n",
      "Epoch 1137, Loss: 0.5381501913070679, Final Batch Loss: 0.3014611303806305\n",
      "Epoch 1138, Loss: 0.48240523040294647, Final Batch Loss: 0.21800191700458527\n",
      "Epoch 1139, Loss: 0.47940075397491455, Final Batch Loss: 0.251579225063324\n",
      "Epoch 1140, Loss: 0.5161190330982208, Final Batch Loss: 0.26041337847709656\n",
      "Epoch 1141, Loss: 0.5219508707523346, Final Batch Loss: 0.2583606541156769\n",
      "Epoch 1142, Loss: 0.4783041328191757, Final Batch Loss: 0.24623402953147888\n",
      "Epoch 1143, Loss: 0.42172808945178986, Final Batch Loss: 0.20314040780067444\n",
      "Epoch 1144, Loss: 0.46043761074543, Final Batch Loss: 0.23045234382152557\n",
      "Epoch 1145, Loss: 0.44314226508140564, Final Batch Loss: 0.19888775050640106\n",
      "Epoch 1146, Loss: 0.4656088948249817, Final Batch Loss: 0.24094225466251373\n",
      "Epoch 1147, Loss: 0.4162556231021881, Final Batch Loss: 0.23916064202785492\n",
      "Epoch 1148, Loss: 0.44283467531204224, Final Batch Loss: 0.23069316148757935\n",
      "Epoch 1149, Loss: 0.4251104146242142, Final Batch Loss: 0.20592913031578064\n",
      "Epoch 1150, Loss: 0.4447239637374878, Final Batch Loss: 0.21377937495708466\n",
      "Epoch 1151, Loss: 0.4489915817975998, Final Batch Loss: 0.21527977287769318\n",
      "Epoch 1152, Loss: 0.4324941784143448, Final Batch Loss: 0.26550471782684326\n",
      "Epoch 1153, Loss: 0.4136917442083359, Final Batch Loss: 0.165445014834404\n",
      "Epoch 1154, Loss: 0.4477749764919281, Final Batch Loss: 0.24010248482227325\n",
      "Epoch 1155, Loss: 0.4810104966163635, Final Batch Loss: 0.25265318155288696\n",
      "Epoch 1156, Loss: 0.46381179988384247, Final Batch Loss: 0.22726808488368988\n",
      "Epoch 1157, Loss: 0.4770687520503998, Final Batch Loss: 0.23904173076152802\n",
      "Epoch 1158, Loss: 0.4541308134794235, Final Batch Loss: 0.22664828598499298\n",
      "Epoch 1159, Loss: 0.442123606801033, Final Batch Loss: 0.2313247174024582\n",
      "Epoch 1160, Loss: 0.45088453590869904, Final Batch Loss: 0.2606160342693329\n",
      "Epoch 1161, Loss: 0.4634912312030792, Final Batch Loss: 0.2370966225862503\n",
      "Epoch 1162, Loss: 0.432317778468132, Final Batch Loss: 0.18387196958065033\n",
      "Epoch 1163, Loss: 0.4588000029325485, Final Batch Loss: 0.22867347300052643\n",
      "Epoch 1164, Loss: 0.43964648246765137, Final Batch Loss: 0.22195066511631012\n",
      "Epoch 1165, Loss: 0.4573112428188324, Final Batch Loss: 0.2578529119491577\n",
      "Epoch 1166, Loss: 0.4528985917568207, Final Batch Loss: 0.2212292104959488\n",
      "Epoch 1167, Loss: 0.44987934827804565, Final Batch Loss: 0.19753777980804443\n",
      "Epoch 1168, Loss: 0.4096633791923523, Final Batch Loss: 0.1996842622756958\n",
      "Epoch 1169, Loss: 0.4794512242078781, Final Batch Loss: 0.2998673617839813\n",
      "Epoch 1170, Loss: 0.4920618236064911, Final Batch Loss: 0.22196295857429504\n",
      "Epoch 1171, Loss: 0.4544346481561661, Final Batch Loss: 0.243458092212677\n",
      "Epoch 1172, Loss: 0.4355951398611069, Final Batch Loss: 0.2187911719083786\n",
      "Epoch 1173, Loss: 0.4659084379673004, Final Batch Loss: 0.22942602634429932\n",
      "Epoch 1174, Loss: 0.4612787514925003, Final Batch Loss: 0.20507468283176422\n",
      "Epoch 1175, Loss: 0.5092385709285736, Final Batch Loss: 0.24544957280158997\n",
      "Epoch 1176, Loss: 0.4838162958621979, Final Batch Loss: 0.22519400715827942\n",
      "Epoch 1177, Loss: 0.4496750235557556, Final Batch Loss: 0.22209933400154114\n",
      "Epoch 1178, Loss: 0.4504925459623337, Final Batch Loss: 0.2502785921096802\n",
      "Epoch 1179, Loss: 0.49895626306533813, Final Batch Loss: 0.25554659962654114\n",
      "Epoch 1180, Loss: 0.4586169123649597, Final Batch Loss: 0.25004643201828003\n",
      "Epoch 1181, Loss: 0.4834202080965042, Final Batch Loss: 0.2751908004283905\n",
      "Epoch 1182, Loss: 0.48747920989990234, Final Batch Loss: 0.2513327896595001\n",
      "Epoch 1183, Loss: 0.4838373512029648, Final Batch Loss: 0.2607278525829315\n",
      "Epoch 1184, Loss: 0.4471254348754883, Final Batch Loss: 0.21628522872924805\n",
      "Epoch 1185, Loss: 0.4302798807621002, Final Batch Loss: 0.2410300225019455\n",
      "Epoch 1186, Loss: 0.44858603179454803, Final Batch Loss: 0.23070943355560303\n",
      "Epoch 1187, Loss: 0.4345763623714447, Final Batch Loss: 0.21163855493068695\n",
      "Epoch 1188, Loss: 0.512963593006134, Final Batch Loss: 0.2221926748752594\n",
      "Epoch 1189, Loss: 0.4181176722049713, Final Batch Loss: 0.23824968934059143\n",
      "Epoch 1190, Loss: 0.47790810465812683, Final Batch Loss: 0.2322523295879364\n",
      "Epoch 1191, Loss: 0.49093422293663025, Final Batch Loss: 0.27458009123802185\n",
      "Epoch 1192, Loss: 0.4267376661300659, Final Batch Loss: 0.22545331716537476\n",
      "Epoch 1193, Loss: 0.41898807883262634, Final Batch Loss: 0.2054939866065979\n",
      "Epoch 1194, Loss: 0.5032793432474136, Final Batch Loss: 0.25661808252334595\n",
      "Epoch 1195, Loss: 0.45837967097759247, Final Batch Loss: 0.23897312581539154\n",
      "Epoch 1196, Loss: 0.45788808166980743, Final Batch Loss: 0.20702128112316132\n",
      "Epoch 1197, Loss: 0.42693862318992615, Final Batch Loss: 0.21485543251037598\n",
      "Epoch 1198, Loss: 0.4362400621175766, Final Batch Loss: 0.2255503386259079\n",
      "Epoch 1199, Loss: 0.4715646207332611, Final Batch Loss: 0.26720577478408813\n",
      "Epoch 1200, Loss: 0.45077237486839294, Final Batch Loss: 0.22689619660377502\n",
      "Epoch 1201, Loss: 0.4385827034711838, Final Batch Loss: 0.23043562471866608\n",
      "Epoch 1202, Loss: 0.45301705598831177, Final Batch Loss: 0.23077398538589478\n",
      "Epoch 1203, Loss: 0.44290269911289215, Final Batch Loss: 0.20378798246383667\n",
      "Epoch 1204, Loss: 0.4475286304950714, Final Batch Loss: 0.18953663110733032\n",
      "Epoch 1205, Loss: 0.46447116136550903, Final Batch Loss: 0.2008281946182251\n",
      "Epoch 1206, Loss: 0.42598477005958557, Final Batch Loss: 0.21546827256679535\n",
      "Epoch 1207, Loss: 0.43668052554130554, Final Batch Loss: 0.21690958738327026\n",
      "Epoch 1208, Loss: 0.4884173423051834, Final Batch Loss: 0.23151995241641998\n",
      "Epoch 1209, Loss: 0.46839144825935364, Final Batch Loss: 0.213159441947937\n",
      "Epoch 1210, Loss: 0.4066411405801773, Final Batch Loss: 0.1886330246925354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1211, Loss: 0.411773145198822, Final Batch Loss: 0.1781023144721985\n",
      "Epoch 1212, Loss: 0.42740562558174133, Final Batch Loss: 0.18722568452358246\n",
      "Epoch 1213, Loss: 0.5010546743869781, Final Batch Loss: 0.23991551995277405\n",
      "Epoch 1214, Loss: 0.4693761020898819, Final Batch Loss: 0.2179858237504959\n",
      "Epoch 1215, Loss: 0.43502989411354065, Final Batch Loss: 0.20683972537517548\n",
      "Epoch 1216, Loss: 0.4569554179906845, Final Batch Loss: 0.2235645353794098\n",
      "Epoch 1217, Loss: 0.4611867219209671, Final Batch Loss: 0.20326144993305206\n",
      "Epoch 1218, Loss: 0.44329358637332916, Final Batch Loss: 0.22668349742889404\n",
      "Epoch 1219, Loss: 0.44122833013534546, Final Batch Loss: 0.2287272959947586\n",
      "Epoch 1220, Loss: 0.42670126259326935, Final Batch Loss: 0.17584244906902313\n",
      "Epoch 1221, Loss: 0.492119625210762, Final Batch Loss: 0.26828527450561523\n",
      "Epoch 1222, Loss: 0.4334867298603058, Final Batch Loss: 0.2157905250787735\n",
      "Epoch 1223, Loss: 0.40283870697021484, Final Batch Loss: 0.18537366390228271\n",
      "Epoch 1224, Loss: 0.4049409478902817, Final Batch Loss: 0.18237638473510742\n",
      "Epoch 1225, Loss: 0.4665013551712036, Final Batch Loss: 0.21014311909675598\n",
      "Epoch 1226, Loss: 0.45259493589401245, Final Batch Loss: 0.24286626279354095\n",
      "Epoch 1227, Loss: 0.4561609923839569, Final Batch Loss: 0.23757420480251312\n",
      "Epoch 1228, Loss: 0.46474799513816833, Final Batch Loss: 0.2594059407711029\n",
      "Epoch 1229, Loss: 0.45681367814540863, Final Batch Loss: 0.19223718345165253\n",
      "Epoch 1230, Loss: 0.4376075714826584, Final Batch Loss: 0.23824918270111084\n",
      "Epoch 1231, Loss: 0.41620610654354095, Final Batch Loss: 0.21674059331417084\n",
      "Epoch 1232, Loss: 0.4196396470069885, Final Batch Loss: 0.2196657806634903\n",
      "Epoch 1233, Loss: 0.4956296533346176, Final Batch Loss: 0.2464589774608612\n",
      "Epoch 1234, Loss: 0.3992255926132202, Final Batch Loss: 0.19061966240406036\n",
      "Epoch 1235, Loss: 0.45982135832309723, Final Batch Loss: 0.2377517968416214\n",
      "Epoch 1236, Loss: 0.3930180221796036, Final Batch Loss: 0.15472139418125153\n",
      "Epoch 1237, Loss: 0.46728038787841797, Final Batch Loss: 0.2179557979106903\n",
      "Epoch 1238, Loss: 0.4482717663049698, Final Batch Loss: 0.21416202187538147\n",
      "Epoch 1239, Loss: 0.4244239032268524, Final Batch Loss: 0.1941043883562088\n",
      "Epoch 1240, Loss: 0.4204290807247162, Final Batch Loss: 0.17522381246089935\n",
      "Epoch 1241, Loss: 0.4375363886356354, Final Batch Loss: 0.2515656650066376\n",
      "Epoch 1242, Loss: 0.3932512551546097, Final Batch Loss: 0.18496115505695343\n",
      "Epoch 1243, Loss: 0.451729416847229, Final Batch Loss: 0.18490514159202576\n",
      "Epoch 1244, Loss: 0.41737496852874756, Final Batch Loss: 0.20036302506923676\n",
      "Epoch 1245, Loss: 0.49489738047122955, Final Batch Loss: 0.2265334278345108\n",
      "Epoch 1246, Loss: 0.4416857361793518, Final Batch Loss: 0.2583909034729004\n",
      "Epoch 1247, Loss: 0.4644814133644104, Final Batch Loss: 0.277681827545166\n",
      "Epoch 1248, Loss: 0.46072424948215485, Final Batch Loss: 0.22006775438785553\n",
      "Epoch 1249, Loss: 0.45434822142124176, Final Batch Loss: 0.23171278834342957\n",
      "Epoch 1250, Loss: 0.44322891533374786, Final Batch Loss: 0.21651306748390198\n",
      "Epoch 1251, Loss: 0.4487055242061615, Final Batch Loss: 0.2431562840938568\n",
      "Epoch 1252, Loss: 0.4385019838809967, Final Batch Loss: 0.1835412085056305\n",
      "Epoch 1253, Loss: 0.42168526351451874, Final Batch Loss: 0.21803273260593414\n",
      "Epoch 1254, Loss: 0.4374445825815201, Final Batch Loss: 0.23986734449863434\n",
      "Epoch 1255, Loss: 0.42929892241954803, Final Batch Loss: 0.23099470138549805\n",
      "Epoch 1256, Loss: 0.40641307830810547, Final Batch Loss: 0.1895168274641037\n",
      "Epoch 1257, Loss: 0.5006089210510254, Final Batch Loss: 0.22585660219192505\n",
      "Epoch 1258, Loss: 0.47578898072242737, Final Batch Loss: 0.2537204921245575\n",
      "Epoch 1259, Loss: 0.4261125475168228, Final Batch Loss: 0.23152653872966766\n",
      "Epoch 1260, Loss: 0.4244864732027054, Final Batch Loss: 0.17381079494953156\n",
      "Epoch 1261, Loss: 0.4659753739833832, Final Batch Loss: 0.2052573561668396\n",
      "Epoch 1262, Loss: 0.41796404123306274, Final Batch Loss: 0.1868726909160614\n",
      "Epoch 1263, Loss: 0.41561470925807953, Final Batch Loss: 0.20361781120300293\n",
      "Epoch 1264, Loss: 0.4073636829853058, Final Batch Loss: 0.2042587548494339\n",
      "Epoch 1265, Loss: 0.4161809980869293, Final Batch Loss: 0.18670828640460968\n",
      "Epoch 1266, Loss: 0.44547101855278015, Final Batch Loss: 0.19856230914592743\n",
      "Epoch 1267, Loss: 0.4314056634902954, Final Batch Loss: 0.23366785049438477\n",
      "Epoch 1268, Loss: 0.48016685247421265, Final Batch Loss: 0.28004127740859985\n",
      "Epoch 1269, Loss: 0.45338134467601776, Final Batch Loss: 0.21792291104793549\n",
      "Epoch 1270, Loss: 0.4544094651937485, Final Batch Loss: 0.22436721622943878\n",
      "Epoch 1271, Loss: 0.4245276153087616, Final Batch Loss: 0.20302216708660126\n",
      "Epoch 1272, Loss: 0.4411972016096115, Final Batch Loss: 0.23084063827991486\n",
      "Epoch 1273, Loss: 0.3836047500371933, Final Batch Loss: 0.18536585569381714\n",
      "Epoch 1274, Loss: 0.45882153511047363, Final Batch Loss: 0.2118569165468216\n",
      "Epoch 1275, Loss: 0.43767137825489044, Final Batch Loss: 0.2261190265417099\n",
      "Epoch 1276, Loss: 0.46162398159503937, Final Batch Loss: 0.22714461386203766\n",
      "Epoch 1277, Loss: 0.4500415474176407, Final Batch Loss: 0.2555367052555084\n",
      "Epoch 1278, Loss: 0.4193500876426697, Final Batch Loss: 0.21015392243862152\n",
      "Epoch 1279, Loss: 0.4017927795648575, Final Batch Loss: 0.2445165365934372\n",
      "Epoch 1280, Loss: 0.4143660217523575, Final Batch Loss: 0.19365283846855164\n",
      "Epoch 1281, Loss: 0.39673489332199097, Final Batch Loss: 0.2073887288570404\n",
      "Epoch 1282, Loss: 0.41292740404605865, Final Batch Loss: 0.21066676080226898\n",
      "Epoch 1283, Loss: 0.4423922002315521, Final Batch Loss: 0.2100706547498703\n",
      "Epoch 1284, Loss: 0.4553736597299576, Final Batch Loss: 0.2274279147386551\n",
      "Epoch 1285, Loss: 0.48696093261241913, Final Batch Loss: 0.24973037838935852\n",
      "Epoch 1286, Loss: 0.39961594343185425, Final Batch Loss: 0.1930229365825653\n",
      "Epoch 1287, Loss: 0.42051224410533905, Final Batch Loss: 0.24798010289669037\n",
      "Epoch 1288, Loss: 0.4611235558986664, Final Batch Loss: 0.1726090908050537\n",
      "Epoch 1289, Loss: 0.463592991232872, Final Batch Loss: 0.2350786328315735\n",
      "Epoch 1290, Loss: 0.4142143279314041, Final Batch Loss: 0.20383445918560028\n",
      "Epoch 1291, Loss: 0.4113047569990158, Final Batch Loss: 0.22251355648040771\n",
      "Epoch 1292, Loss: 0.41781993210315704, Final Batch Loss: 0.20738306641578674\n",
      "Epoch 1293, Loss: 0.41011933982372284, Final Batch Loss: 0.21922965347766876\n",
      "Epoch 1294, Loss: 0.46914294362068176, Final Batch Loss: 0.21935579180717468\n",
      "Epoch 1295, Loss: 0.367490291595459, Final Batch Loss: 0.16980254650115967\n",
      "Epoch 1296, Loss: 0.4715810865163803, Final Batch Loss: 0.18977363407611847\n",
      "Epoch 1297, Loss: 0.44236327707767487, Final Batch Loss: 0.1964820921421051\n",
      "Epoch 1298, Loss: 0.387536957859993, Final Batch Loss: 0.19827604293823242\n",
      "Epoch 1299, Loss: 0.44884464144706726, Final Batch Loss: 0.2599354088306427\n",
      "Epoch 1300, Loss: 0.430056631565094, Final Batch Loss: 0.2293398380279541\n",
      "Epoch 1301, Loss: 0.45746977627277374, Final Batch Loss: 0.21870969235897064\n",
      "Epoch 1302, Loss: 0.4908791780471802, Final Batch Loss: 0.25274619460105896\n",
      "Epoch 1303, Loss: 0.45209717750549316, Final Batch Loss: 0.2639884054660797\n",
      "Epoch 1304, Loss: 0.4007774591445923, Final Batch Loss: 0.2042342871427536\n",
      "Epoch 1305, Loss: 0.4035644382238388, Final Batch Loss: 0.20859120786190033\n",
      "Epoch 1306, Loss: 0.4219867140054703, Final Batch Loss: 0.21427656710147858\n",
      "Epoch 1307, Loss: 0.40486377477645874, Final Batch Loss: 0.200735405087471\n",
      "Epoch 1308, Loss: 0.4634860008955002, Final Batch Loss: 0.20434851944446564\n",
      "Epoch 1309, Loss: 0.43180060386657715, Final Batch Loss: 0.20955099165439606\n",
      "Epoch 1310, Loss: 0.40918971598148346, Final Batch Loss: 0.20013174414634705\n",
      "Epoch 1311, Loss: 0.4289900064468384, Final Batch Loss: 0.2465561032295227\n",
      "Epoch 1312, Loss: 0.4398292005062103, Final Batch Loss: 0.23762769997119904\n",
      "Epoch 1313, Loss: 0.4175123870372772, Final Batch Loss: 0.19801931083202362\n",
      "Epoch 1314, Loss: 0.4113982915878296, Final Batch Loss: 0.188533753156662\n",
      "Epoch 1315, Loss: 0.4054824262857437, Final Batch Loss: 0.22339069843292236\n",
      "Epoch 1316, Loss: 0.4130915552377701, Final Batch Loss: 0.21550558507442474\n",
      "Epoch 1317, Loss: 0.4143018573522568, Final Batch Loss: 0.2102736532688141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1318, Loss: 0.4021601527929306, Final Batch Loss: 0.19849024713039398\n",
      "Epoch 1319, Loss: 0.48268023133277893, Final Batch Loss: 0.19186052680015564\n",
      "Epoch 1320, Loss: 0.5013450533151627, Final Batch Loss: 0.20237420499324799\n",
      "Epoch 1321, Loss: 0.40926434099674225, Final Batch Loss: 0.2307685911655426\n",
      "Epoch 1322, Loss: 0.4099562019109726, Final Batch Loss: 0.21213895082473755\n",
      "Epoch 1323, Loss: 0.4416647106409073, Final Batch Loss: 0.19483937323093414\n",
      "Epoch 1324, Loss: 0.47860659658908844, Final Batch Loss: 0.22385884821414948\n",
      "Epoch 1325, Loss: 0.46076954901218414, Final Batch Loss: 0.24985361099243164\n",
      "Epoch 1326, Loss: 0.380356028676033, Final Batch Loss: 0.20764508843421936\n",
      "Epoch 1327, Loss: 0.4386848211288452, Final Batch Loss: 0.23984521627426147\n",
      "Epoch 1328, Loss: 0.4773295968770981, Final Batch Loss: 0.2548089027404785\n",
      "Epoch 1329, Loss: 0.4161055088043213, Final Batch Loss: 0.18869298696517944\n",
      "Epoch 1330, Loss: 0.4186280518770218, Final Batch Loss: 0.22576089203357697\n",
      "Epoch 1331, Loss: 0.3956235200166702, Final Batch Loss: 0.18133048713207245\n",
      "Epoch 1332, Loss: 0.41896113753318787, Final Batch Loss: 0.22907622158527374\n",
      "Epoch 1333, Loss: 0.4019979238510132, Final Batch Loss: 0.2471928596496582\n",
      "Epoch 1334, Loss: 0.4265441298484802, Final Batch Loss: 0.22494687139987946\n",
      "Epoch 1335, Loss: 0.4274873286485672, Final Batch Loss: 0.2025541365146637\n",
      "Epoch 1336, Loss: 0.47418250143527985, Final Batch Loss: 0.2497681975364685\n",
      "Epoch 1337, Loss: 0.4618198275566101, Final Batch Loss: 0.22587524354457855\n",
      "Epoch 1338, Loss: 0.42605529725551605, Final Batch Loss: 0.21870405972003937\n",
      "Epoch 1339, Loss: 0.4061577022075653, Final Batch Loss: 0.1961304098367691\n",
      "Epoch 1340, Loss: 0.4438003599643707, Final Batch Loss: 0.2514827251434326\n",
      "Epoch 1341, Loss: 0.44246989488601685, Final Batch Loss: 0.2335136979818344\n",
      "Epoch 1342, Loss: 0.43287140130996704, Final Batch Loss: 0.21767403185367584\n",
      "Epoch 1343, Loss: 0.4424431622028351, Final Batch Loss: 0.18354064226150513\n",
      "Epoch 1344, Loss: 0.40099839866161346, Final Batch Loss: 0.1864847093820572\n",
      "Epoch 1345, Loss: 0.40394824743270874, Final Batch Loss: 0.2220308631658554\n",
      "Epoch 1346, Loss: 0.39264118671417236, Final Batch Loss: 0.21386973559856415\n",
      "Epoch 1347, Loss: 0.4082101732492447, Final Batch Loss: 0.19887159764766693\n",
      "Epoch 1348, Loss: 0.40245521068573, Final Batch Loss: 0.1973666399717331\n",
      "Epoch 1349, Loss: 0.3823901265859604, Final Batch Loss: 0.19736388325691223\n",
      "Epoch 1350, Loss: 0.40595658123493195, Final Batch Loss: 0.1978389024734497\n",
      "Epoch 1351, Loss: 0.44610248506069183, Final Batch Loss: 0.22364665567874908\n",
      "Epoch 1352, Loss: 0.40838631987571716, Final Batch Loss: 0.20044076442718506\n",
      "Epoch 1353, Loss: 0.4250470995903015, Final Batch Loss: 0.24724753201007843\n",
      "Epoch 1354, Loss: 0.3952541798353195, Final Batch Loss: 0.21452684700489044\n",
      "Epoch 1355, Loss: 0.3896234780550003, Final Batch Loss: 0.18513920903205872\n",
      "Epoch 1356, Loss: 0.3728581368923187, Final Batch Loss: 0.1960763931274414\n",
      "Epoch 1357, Loss: 0.42454440891742706, Final Batch Loss: 0.1931021809577942\n",
      "Epoch 1358, Loss: 0.41357262432575226, Final Batch Loss: 0.1937619000673294\n",
      "Epoch 1359, Loss: 0.43146173655986786, Final Batch Loss: 0.22960440814495087\n",
      "Epoch 1360, Loss: 0.46131153404712677, Final Batch Loss: 0.2657545506954193\n",
      "Epoch 1361, Loss: 0.4547007083892822, Final Batch Loss: 0.23769046366214752\n",
      "Epoch 1362, Loss: 0.3981686532497406, Final Batch Loss: 0.21449416875839233\n",
      "Epoch 1363, Loss: 0.46496860682964325, Final Batch Loss: 0.24582433700561523\n",
      "Epoch 1364, Loss: 0.45322364568710327, Final Batch Loss: 0.2791193127632141\n",
      "Epoch 1365, Loss: 0.4199765622615814, Final Batch Loss: 0.20212365686893463\n",
      "Epoch 1366, Loss: 0.4177420139312744, Final Batch Loss: 0.20339173078536987\n",
      "Epoch 1367, Loss: 0.4894656091928482, Final Batch Loss: 0.24943386018276215\n",
      "Epoch 1368, Loss: 0.4016198515892029, Final Batch Loss: 0.2202734649181366\n",
      "Epoch 1369, Loss: 0.46462541818618774, Final Batch Loss: 0.2306922972202301\n",
      "Epoch 1370, Loss: 0.4369514286518097, Final Batch Loss: 0.23488181829452515\n",
      "Epoch 1371, Loss: 0.423260822892189, Final Batch Loss: 0.23482097685337067\n",
      "Epoch 1372, Loss: 0.3922109454870224, Final Batch Loss: 0.20657169818878174\n",
      "Epoch 1373, Loss: 0.43725091218948364, Final Batch Loss: 0.214730903506279\n",
      "Epoch 1374, Loss: 0.3936210423707962, Final Batch Loss: 0.1975463181734085\n",
      "Epoch 1375, Loss: 0.40495307743549347, Final Batch Loss: 0.22645394504070282\n",
      "Epoch 1376, Loss: 0.4422670155763626, Final Batch Loss: 0.20421867072582245\n",
      "Epoch 1377, Loss: 0.44838014245033264, Final Batch Loss: 0.221368208527565\n",
      "Epoch 1378, Loss: 0.44109438359737396, Final Batch Loss: 0.2321799248456955\n",
      "Epoch 1379, Loss: 0.43281106650829315, Final Batch Loss: 0.21633727848529816\n",
      "Epoch 1380, Loss: 0.4397584944963455, Final Batch Loss: 0.1775609701871872\n",
      "Epoch 1381, Loss: 0.43097203969955444, Final Batch Loss: 0.18972855806350708\n",
      "Epoch 1382, Loss: 0.4506629705429077, Final Batch Loss: 0.22405411303043365\n",
      "Epoch 1383, Loss: 0.43506257236003876, Final Batch Loss: 0.2112569659948349\n",
      "Epoch 1384, Loss: 0.40649354457855225, Final Batch Loss: 0.1819344013929367\n",
      "Epoch 1385, Loss: 0.40855905413627625, Final Batch Loss: 0.21748723089694977\n",
      "Epoch 1386, Loss: 0.47311045229434967, Final Batch Loss: 0.23017513751983643\n",
      "Epoch 1387, Loss: 0.42890051007270813, Final Batch Loss: 0.21061888337135315\n",
      "Epoch 1388, Loss: 0.38111065328121185, Final Batch Loss: 0.22886580228805542\n",
      "Epoch 1389, Loss: 0.4238671362400055, Final Batch Loss: 0.21971824765205383\n",
      "Epoch 1390, Loss: 0.39076513051986694, Final Batch Loss: 0.184828981757164\n",
      "Epoch 1391, Loss: 0.3840778172016144, Final Batch Loss: 0.20264200866222382\n",
      "Epoch 1392, Loss: 0.4345242530107498, Final Batch Loss: 0.2673317790031433\n",
      "Epoch 1393, Loss: 0.40775397419929504, Final Batch Loss: 0.21350722014904022\n",
      "Epoch 1394, Loss: 0.41766148805618286, Final Batch Loss: 0.23319284617900848\n",
      "Epoch 1395, Loss: 0.4197322130203247, Final Batch Loss: 0.18614964187145233\n",
      "Epoch 1396, Loss: 0.4566630572080612, Final Batch Loss: 0.23196616768836975\n",
      "Epoch 1397, Loss: 0.3834592401981354, Final Batch Loss: 0.19952747225761414\n",
      "Epoch 1398, Loss: 0.41947266459465027, Final Batch Loss: 0.2011621594429016\n",
      "Epoch 1399, Loss: 0.37292206287384033, Final Batch Loss: 0.18554967641830444\n",
      "Epoch 1400, Loss: 0.43660834431648254, Final Batch Loss: 0.24063189327716827\n",
      "Epoch 1401, Loss: 0.4197908639907837, Final Batch Loss: 0.2181888073682785\n",
      "Epoch 1402, Loss: 0.5110350251197815, Final Batch Loss: 0.2736901044845581\n",
      "Epoch 1403, Loss: 0.414289265871048, Final Batch Loss: 0.22241736948490143\n",
      "Epoch 1404, Loss: 0.4769741892814636, Final Batch Loss: 0.25860071182250977\n",
      "Epoch 1405, Loss: 0.4506185054779053, Final Batch Loss: 0.23831118643283844\n",
      "Epoch 1406, Loss: 0.46083931624889374, Final Batch Loss: 0.2486126571893692\n",
      "Epoch 1407, Loss: 0.4198775142431259, Final Batch Loss: 0.1836351901292801\n",
      "Epoch 1408, Loss: 0.4503852427005768, Final Batch Loss: 0.2312774509191513\n",
      "Epoch 1409, Loss: 0.4737769663333893, Final Batch Loss: 0.2516278624534607\n",
      "Epoch 1410, Loss: 0.40214303135871887, Final Batch Loss: 0.22321990132331848\n",
      "Epoch 1411, Loss: 0.3863046318292618, Final Batch Loss: 0.197478786110878\n",
      "Epoch 1412, Loss: 0.45984071493148804, Final Batch Loss: 0.27113115787506104\n",
      "Epoch 1413, Loss: 0.4130488783121109, Final Batch Loss: 0.22074337303638458\n",
      "Epoch 1414, Loss: 0.4335115998983383, Final Batch Loss: 0.24221672117710114\n",
      "Epoch 1415, Loss: 0.39878323674201965, Final Batch Loss: 0.20628845691680908\n",
      "Epoch 1416, Loss: 0.4213978499174118, Final Batch Loss: 0.21999745070934296\n",
      "Epoch 1417, Loss: 0.39132489264011383, Final Batch Loss: 0.18271410465240479\n",
      "Epoch 1418, Loss: 0.42719727754592896, Final Batch Loss: 0.19159720838069916\n",
      "Epoch 1419, Loss: 0.3833702802658081, Final Batch Loss: 0.19173972308635712\n",
      "Epoch 1420, Loss: 0.4390695095062256, Final Batch Loss: 0.2097974270582199\n",
      "Epoch 1421, Loss: 0.39751994609832764, Final Batch Loss: 0.17917566001415253\n",
      "Epoch 1422, Loss: 0.41361089050769806, Final Batch Loss: 0.19770333170890808\n",
      "Epoch 1423, Loss: 0.41457492113113403, Final Batch Loss: 0.2116069197654724\n",
      "Epoch 1424, Loss: 0.46256665885448456, Final Batch Loss: 0.2623024582862854\n",
      "Epoch 1425, Loss: 0.3960234224796295, Final Batch Loss: 0.199459508061409\n",
      "Epoch 1426, Loss: 0.44906994700431824, Final Batch Loss: 0.21191222965717316\n",
      "Epoch 1427, Loss: 0.43805915117263794, Final Batch Loss: 0.2289157658815384\n",
      "Epoch 1428, Loss: 0.41174571216106415, Final Batch Loss: 0.18546181917190552\n",
      "Epoch 1429, Loss: 0.394545242190361, Final Batch Loss: 0.1863078624010086\n",
      "Epoch 1430, Loss: 0.39953432977199554, Final Batch Loss: 0.2216784805059433\n",
      "Epoch 1431, Loss: 0.45774000883102417, Final Batch Loss: 0.2234528362751007\n",
      "Epoch 1432, Loss: 0.41079531610012054, Final Batch Loss: 0.19950251281261444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1433, Loss: 0.38694222271442413, Final Batch Loss: 0.17875836789608002\n",
      "Epoch 1434, Loss: 0.3788500726222992, Final Batch Loss: 0.19220660626888275\n",
      "Epoch 1435, Loss: 0.4462132453918457, Final Batch Loss: 0.21881364285945892\n",
      "Epoch 1436, Loss: 0.4061543643474579, Final Batch Loss: 0.22522075474262238\n",
      "Epoch 1437, Loss: 0.3644693195819855, Final Batch Loss: 0.17084291577339172\n",
      "Epoch 1438, Loss: 0.4195302277803421, Final Batch Loss: 0.2300553023815155\n",
      "Epoch 1439, Loss: 0.42565135657787323, Final Batch Loss: 0.24737662076950073\n",
      "Epoch 1440, Loss: 0.37611716985702515, Final Batch Loss: 0.18337613344192505\n",
      "Epoch 1441, Loss: 0.3981035500764847, Final Batch Loss: 0.1883067637681961\n",
      "Epoch 1442, Loss: 0.4179557114839554, Final Batch Loss: 0.21681052446365356\n",
      "Epoch 1443, Loss: 0.43217773735523224, Final Batch Loss: 0.22548562288284302\n",
      "Epoch 1444, Loss: 0.4498194605112076, Final Batch Loss: 0.19973565638065338\n",
      "Epoch 1445, Loss: 0.43065279722213745, Final Batch Loss: 0.18766073882579803\n",
      "Epoch 1446, Loss: 0.4775044471025467, Final Batch Loss: 0.26961061358451843\n",
      "Epoch 1447, Loss: 0.41828814148902893, Final Batch Loss: 0.23545509576797485\n",
      "Epoch 1448, Loss: 0.4366327375173569, Final Batch Loss: 0.18218015134334564\n",
      "Epoch 1449, Loss: 0.4356376975774765, Final Batch Loss: 0.19400064647197723\n",
      "Epoch 1450, Loss: 0.40737035870552063, Final Batch Loss: 0.22268179059028625\n",
      "Epoch 1451, Loss: 0.4340815842151642, Final Batch Loss: 0.21593689918518066\n",
      "Epoch 1452, Loss: 0.46973761916160583, Final Batch Loss: 0.2575562596321106\n",
      "Epoch 1453, Loss: 0.3924434632062912, Final Batch Loss: 0.19392579793930054\n",
      "Epoch 1454, Loss: 0.42066965997219086, Final Batch Loss: 0.19338050484657288\n",
      "Epoch 1455, Loss: 0.4347211867570877, Final Batch Loss: 0.2029004693031311\n",
      "Epoch 1456, Loss: 0.4686054289340973, Final Batch Loss: 0.18158990144729614\n",
      "Epoch 1457, Loss: 0.4252699911594391, Final Batch Loss: 0.19983933866024017\n",
      "Epoch 1458, Loss: 0.3969799727201462, Final Batch Loss: 0.1744552105665207\n",
      "Epoch 1459, Loss: 0.45433273911476135, Final Batch Loss: 0.22949577867984772\n",
      "Epoch 1460, Loss: 0.4097503423690796, Final Batch Loss: 0.18880382180213928\n",
      "Epoch 1461, Loss: 0.3890618085861206, Final Batch Loss: 0.16770417988300323\n",
      "Epoch 1462, Loss: 0.42154134809970856, Final Batch Loss: 0.21555167436599731\n",
      "Epoch 1463, Loss: 0.45902882516384125, Final Batch Loss: 0.24114416539669037\n",
      "Epoch 1464, Loss: 0.4168923795223236, Final Batch Loss: 0.189167320728302\n",
      "Epoch 1465, Loss: 0.38392433524131775, Final Batch Loss: 0.18159283697605133\n",
      "Epoch 1466, Loss: 0.4022133946418762, Final Batch Loss: 0.25147974491119385\n",
      "Epoch 1467, Loss: 0.38722604513168335, Final Batch Loss: 0.19431020319461823\n",
      "Epoch 1468, Loss: 0.40224824845790863, Final Batch Loss: 0.18518321216106415\n",
      "Epoch 1469, Loss: 0.40411844849586487, Final Batch Loss: 0.22463911771774292\n",
      "Epoch 1470, Loss: 0.39360660314559937, Final Batch Loss: 0.21170169115066528\n",
      "Epoch 1471, Loss: 0.42955581843852997, Final Batch Loss: 0.1887212097644806\n",
      "Epoch 1472, Loss: 0.4135535955429077, Final Batch Loss: 0.1930488646030426\n",
      "Epoch 1473, Loss: 0.41422221064567566, Final Batch Loss: 0.19989325106143951\n",
      "Epoch 1474, Loss: 0.3756259083747864, Final Batch Loss: 0.21605515480041504\n",
      "Epoch 1475, Loss: 0.40281160175800323, Final Batch Loss: 0.2170713096857071\n",
      "Epoch 1476, Loss: 0.4082232564687729, Final Batch Loss: 0.2149302363395691\n",
      "Epoch 1477, Loss: 0.4042426496744156, Final Batch Loss: 0.1925102025270462\n",
      "Epoch 1478, Loss: 0.4467434734106064, Final Batch Loss: 0.22743310034275055\n",
      "Epoch 1479, Loss: 0.49133485555648804, Final Batch Loss: 0.2811617851257324\n",
      "Epoch 1480, Loss: 0.44628122448921204, Final Batch Loss: 0.20879562199115753\n",
      "Epoch 1481, Loss: 0.4067050814628601, Final Batch Loss: 0.19753868877887726\n",
      "Epoch 1482, Loss: 0.4747481793165207, Final Batch Loss: 0.2558925449848175\n",
      "Epoch 1483, Loss: 0.4375366270542145, Final Batch Loss: 0.20974110066890717\n",
      "Epoch 1484, Loss: 0.4195224493741989, Final Batch Loss: 0.16934292018413544\n",
      "Epoch 1485, Loss: 0.39781346917152405, Final Batch Loss: 0.18488231301307678\n",
      "Epoch 1486, Loss: 0.40148112177848816, Final Batch Loss: 0.21565446257591248\n",
      "Epoch 1487, Loss: 0.4114501178264618, Final Batch Loss: 0.2255275696516037\n",
      "Epoch 1488, Loss: 0.44668181240558624, Final Batch Loss: 0.2397153377532959\n",
      "Epoch 1489, Loss: 0.4058476686477661, Final Batch Loss: 0.15777096152305603\n",
      "Epoch 1490, Loss: 0.40755853056907654, Final Batch Loss: 0.19427070021629333\n",
      "Epoch 1491, Loss: 0.37837882339954376, Final Batch Loss: 0.20604106783866882\n",
      "Epoch 1492, Loss: 0.39866597950458527, Final Batch Loss: 0.2161160558462143\n",
      "Epoch 1493, Loss: 0.4515154957771301, Final Batch Loss: 0.2662108540534973\n",
      "Epoch 1494, Loss: 0.40602585673332214, Final Batch Loss: 0.21368533372879028\n",
      "Epoch 1495, Loss: 0.43202030658721924, Final Batch Loss: 0.21673870086669922\n",
      "Epoch 1496, Loss: 0.40086260437965393, Final Batch Loss: 0.24431252479553223\n",
      "Epoch 1497, Loss: 0.37885497510433197, Final Batch Loss: 0.18825261294841766\n",
      "Epoch 1498, Loss: 0.46082884073257446, Final Batch Loss: 0.24841272830963135\n",
      "Epoch 1499, Loss: 0.3733656406402588, Final Batch Loss: 0.20638884603977203\n",
      "Epoch 1500, Loss: 0.3881925642490387, Final Batch Loss: 0.2169501930475235\n",
      "Epoch 1501, Loss: 0.4531344026327133, Final Batch Loss: 0.19382797181606293\n",
      "Epoch 1502, Loss: 0.3963410556316376, Final Batch Loss: 0.1946958303451538\n",
      "Epoch 1503, Loss: 0.393304705619812, Final Batch Loss: 0.20068444311618805\n",
      "Epoch 1504, Loss: 0.3821800947189331, Final Batch Loss: 0.21911057829856873\n",
      "Epoch 1505, Loss: 0.3821380138397217, Final Batch Loss: 0.17637410759925842\n",
      "Epoch 1506, Loss: 0.5185585469007492, Final Batch Loss: 0.2747665345668793\n",
      "Epoch 1507, Loss: 0.4109131991863251, Final Batch Loss: 0.20397578179836273\n",
      "Epoch 1508, Loss: 0.3753897547721863, Final Batch Loss: 0.18694113194942474\n",
      "Epoch 1509, Loss: 0.4251289665699005, Final Batch Loss: 0.19405554234981537\n",
      "Epoch 1510, Loss: 0.4041326940059662, Final Batch Loss: 0.21207502484321594\n",
      "Epoch 1511, Loss: 0.4247729033231735, Final Batch Loss: 0.18729497492313385\n",
      "Epoch 1512, Loss: 0.41649429500102997, Final Batch Loss: 0.17951072752475739\n",
      "Epoch 1513, Loss: 0.38703759014606476, Final Batch Loss: 0.1955568790435791\n",
      "Epoch 1514, Loss: 0.44137249886989594, Final Batch Loss: 0.21046745777130127\n",
      "Epoch 1515, Loss: 0.3769996017217636, Final Batch Loss: 0.20213459432125092\n",
      "Epoch 1516, Loss: 0.4123147279024124, Final Batch Loss: 0.2125861644744873\n",
      "Epoch 1517, Loss: 0.39272867143154144, Final Batch Loss: 0.18496201932430267\n",
      "Epoch 1518, Loss: 0.39770959317684174, Final Batch Loss: 0.20685040950775146\n",
      "Epoch 1519, Loss: 0.39157402515411377, Final Batch Loss: 0.20145851373672485\n",
      "Epoch 1520, Loss: 0.3822288364171982, Final Batch Loss: 0.20380103588104248\n",
      "Epoch 1521, Loss: 0.4108840078115463, Final Batch Loss: 0.1853512078523636\n",
      "Epoch 1522, Loss: 0.40864197909832, Final Batch Loss: 0.1943664401769638\n",
      "Epoch 1523, Loss: 0.42913396656513214, Final Batch Loss: 0.2486669272184372\n",
      "Epoch 1524, Loss: 0.4294361174106598, Final Batch Loss: 0.22816050052642822\n",
      "Epoch 1525, Loss: 0.44467078149318695, Final Batch Loss: 0.20173189043998718\n",
      "Epoch 1526, Loss: 0.415512815117836, Final Batch Loss: 0.17444811761379242\n",
      "Epoch 1527, Loss: 0.4019670635461807, Final Batch Loss: 0.2124704271554947\n",
      "Epoch 1528, Loss: 0.42959046363830566, Final Batch Loss: 0.2564695477485657\n",
      "Epoch 1529, Loss: 0.4337841719388962, Final Batch Loss: 0.2256413996219635\n",
      "Epoch 1530, Loss: 0.3652341067790985, Final Batch Loss: 0.18690574169158936\n",
      "Epoch 1531, Loss: 0.3892974257469177, Final Batch Loss: 0.19040942192077637\n",
      "Epoch 1532, Loss: 0.41164374351501465, Final Batch Loss: 0.202728271484375\n",
      "Epoch 1533, Loss: 0.4180975556373596, Final Batch Loss: 0.2313741147518158\n",
      "Epoch 1534, Loss: 0.4116126000881195, Final Batch Loss: 0.24838639795780182\n",
      "Epoch 1535, Loss: 0.4032696336507797, Final Batch Loss: 0.18658944964408875\n",
      "Epoch 1536, Loss: 0.37100863456726074, Final Batch Loss: 0.17691557109355927\n",
      "Epoch 1537, Loss: 0.3957277685403824, Final Batch Loss: 0.18066789209842682\n",
      "Epoch 1538, Loss: 0.40546512603759766, Final Batch Loss: 0.21134309470653534\n",
      "Epoch 1539, Loss: 0.4209171533584595, Final Batch Loss: 0.20626240968704224\n",
      "Epoch 1540, Loss: 0.40054257214069366, Final Batch Loss: 0.16564202308654785\n",
      "Epoch 1541, Loss: 0.35412345826625824, Final Batch Loss: 0.18035592138767242\n",
      "Epoch 1542, Loss: 0.3846340626478195, Final Batch Loss: 0.19338920712471008\n",
      "Epoch 1543, Loss: 0.37463077902793884, Final Batch Loss: 0.21146653592586517\n",
      "Epoch 1544, Loss: 0.43641847372055054, Final Batch Loss: 0.245566725730896\n",
      "Epoch 1545, Loss: 0.46090084314346313, Final Batch Loss: 0.23000280559062958\n",
      "Epoch 1546, Loss: 0.4738786965608597, Final Batch Loss: 0.21785695850849152\n",
      "Epoch 1547, Loss: 0.4133501797914505, Final Batch Loss: 0.21409600973129272\n",
      "Epoch 1548, Loss: 0.5076554715633392, Final Batch Loss: 0.23673030734062195\n",
      "Epoch 1549, Loss: 0.4489544779062271, Final Batch Loss: 0.20770595967769623\n",
      "Epoch 1550, Loss: 0.41219572722911835, Final Batch Loss: 0.23932665586471558\n",
      "Epoch 1551, Loss: 0.43716752529144287, Final Batch Loss: 0.24829065799713135\n",
      "Epoch 1552, Loss: 0.45462140440940857, Final Batch Loss: 0.24274727702140808\n",
      "Epoch 1553, Loss: 0.41553691029548645, Final Batch Loss: 0.1988717019557953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1554, Loss: 0.3805057853460312, Final Batch Loss: 0.17954348027706146\n",
      "Epoch 1555, Loss: 0.38655172288417816, Final Batch Loss: 0.17429570853710175\n",
      "Epoch 1556, Loss: 0.43704549968242645, Final Batch Loss: 0.2351401001214981\n",
      "Epoch 1557, Loss: 0.3836655169725418, Final Batch Loss: 0.20636995136737823\n",
      "Epoch 1558, Loss: 0.3947560787200928, Final Batch Loss: 0.21174116432666779\n",
      "Epoch 1559, Loss: 0.37604033946990967, Final Batch Loss: 0.1835242509841919\n",
      "Epoch 1560, Loss: 0.4016368240118027, Final Batch Loss: 0.23371857404708862\n",
      "Epoch 1561, Loss: 0.44656212627887726, Final Batch Loss: 0.2236328423023224\n",
      "Epoch 1562, Loss: 0.4102553427219391, Final Batch Loss: 0.23374821245670319\n",
      "Epoch 1563, Loss: 0.3835490494966507, Final Batch Loss: 0.23395214974880219\n",
      "Epoch 1564, Loss: 0.43702466785907745, Final Batch Loss: 0.2177123874425888\n",
      "Epoch 1565, Loss: 0.365557998418808, Final Batch Loss: 0.1970207542181015\n",
      "Epoch 1566, Loss: 0.44151751697063446, Final Batch Loss: 0.19510284066200256\n",
      "Epoch 1567, Loss: 0.3844708800315857, Final Batch Loss: 0.18165245652198792\n",
      "Epoch 1568, Loss: 0.3719557672739029, Final Batch Loss: 0.1981702446937561\n",
      "Epoch 1569, Loss: 0.379987508058548, Final Batch Loss: 0.2023351639509201\n",
      "Epoch 1570, Loss: 0.4202718734741211, Final Batch Loss: 0.22374148666858673\n",
      "Epoch 1571, Loss: 0.38390323519706726, Final Batch Loss: 0.19289299845695496\n",
      "Epoch 1572, Loss: 0.39262866973876953, Final Batch Loss: 0.18613438308238983\n",
      "Epoch 1573, Loss: 0.4030887931585312, Final Batch Loss: 0.20095296204090118\n",
      "Epoch 1574, Loss: 0.3560614287853241, Final Batch Loss: 0.19849488139152527\n",
      "Epoch 1575, Loss: 0.40564893186092377, Final Batch Loss: 0.20488350093364716\n",
      "Epoch 1576, Loss: 0.38558848202228546, Final Batch Loss: 0.1868801862001419\n",
      "Epoch 1577, Loss: 0.3848435878753662, Final Batch Loss: 0.17191173136234283\n",
      "Epoch 1578, Loss: 0.42810966074466705, Final Batch Loss: 0.24899521470069885\n",
      "Epoch 1579, Loss: 0.4195227324962616, Final Batch Loss: 0.1917933225631714\n",
      "Epoch 1580, Loss: 0.4054882675409317, Final Batch Loss: 0.1998874545097351\n",
      "Epoch 1581, Loss: 0.4331362396478653, Final Batch Loss: 0.20001070201396942\n",
      "Epoch 1582, Loss: 0.4021627902984619, Final Batch Loss: 0.2322319597005844\n",
      "Epoch 1583, Loss: 0.3838474005460739, Final Batch Loss: 0.17227798700332642\n",
      "Epoch 1584, Loss: 0.3970460593700409, Final Batch Loss: 0.2342890053987503\n",
      "Epoch 1585, Loss: 0.37315158545970917, Final Batch Loss: 0.2004115730524063\n",
      "Epoch 1586, Loss: 0.4556083381175995, Final Batch Loss: 0.20702442526817322\n",
      "Epoch 1587, Loss: 0.3991754949092865, Final Batch Loss: 0.2272806465625763\n",
      "Epoch 1588, Loss: 0.39050908386707306, Final Batch Loss: 0.1848774403333664\n",
      "Epoch 1589, Loss: 0.38633544743061066, Final Batch Loss: 0.20870454609394073\n",
      "Epoch 1590, Loss: 0.46079888939857483, Final Batch Loss: 0.22023780643939972\n",
      "Epoch 1591, Loss: 0.3789602071046829, Final Batch Loss: 0.2196960598230362\n",
      "Epoch 1592, Loss: 0.3937077522277832, Final Batch Loss: 0.19799745082855225\n",
      "Epoch 1593, Loss: 0.40458349883556366, Final Batch Loss: 0.16860215365886688\n",
      "Epoch 1594, Loss: 0.38405632972717285, Final Batch Loss: 0.18514218926429749\n",
      "Epoch 1595, Loss: 0.37405021488666534, Final Batch Loss: 0.21440225839614868\n",
      "Epoch 1596, Loss: 0.38145868480205536, Final Batch Loss: 0.17381541430950165\n",
      "Epoch 1597, Loss: 0.436314195394516, Final Batch Loss: 0.21733757853507996\n",
      "Epoch 1598, Loss: 0.3998816907405853, Final Batch Loss: 0.173497274518013\n",
      "Epoch 1599, Loss: 0.3769897520542145, Final Batch Loss: 0.1762542873620987\n",
      "Epoch 1600, Loss: 0.35507048666477203, Final Batch Loss: 0.18822120130062103\n",
      "Epoch 1601, Loss: 0.38761700689792633, Final Batch Loss: 0.21029433608055115\n",
      "Epoch 1602, Loss: 0.39684565365314484, Final Batch Loss: 0.19221067428588867\n",
      "Epoch 1603, Loss: 0.4532002806663513, Final Batch Loss: 0.25583359599113464\n",
      "Epoch 1604, Loss: 0.40545813739299774, Final Batch Loss: 0.1920991837978363\n",
      "Epoch 1605, Loss: 0.36320410668849945, Final Batch Loss: 0.17197534441947937\n",
      "Epoch 1606, Loss: 0.39123663306236267, Final Batch Loss: 0.2077317237854004\n",
      "Epoch 1607, Loss: 0.4264736771583557, Final Batch Loss: 0.21516552567481995\n",
      "Epoch 1608, Loss: 0.44241489470005035, Final Batch Loss: 0.2470952272415161\n",
      "Epoch 1609, Loss: 0.4010540097951889, Final Batch Loss: 0.19915509223937988\n",
      "Epoch 1610, Loss: 0.3725317716598511, Final Batch Loss: 0.17218442261219025\n",
      "Epoch 1611, Loss: 0.42601850628852844, Final Batch Loss: 0.2186865657567978\n",
      "Epoch 1612, Loss: 0.370410293340683, Final Batch Loss: 0.18775062263011932\n",
      "Epoch 1613, Loss: 0.36072684824466705, Final Batch Loss: 0.18339233100414276\n",
      "Epoch 1614, Loss: 0.4103599786758423, Final Batch Loss: 0.2181766927242279\n",
      "Epoch 1615, Loss: 0.3807394802570343, Final Batch Loss: 0.22636504471302032\n",
      "Epoch 1616, Loss: 0.40327994525432587, Final Batch Loss: 0.1878313273191452\n",
      "Epoch 1617, Loss: 0.419209286570549, Final Batch Loss: 0.2242407649755478\n",
      "Epoch 1618, Loss: 0.40596903860569, Final Batch Loss: 0.17289204895496368\n",
      "Epoch 1619, Loss: 0.34468479454517365, Final Batch Loss: 0.15924988687038422\n",
      "Epoch 1620, Loss: 0.42542099952697754, Final Batch Loss: 0.23145955801010132\n",
      "Epoch 1621, Loss: 0.38684214651584625, Final Batch Loss: 0.1916884034872055\n",
      "Epoch 1622, Loss: 0.41825903952121735, Final Batch Loss: 0.2139545977115631\n",
      "Epoch 1623, Loss: 0.4305662214756012, Final Batch Loss: 0.21303293108940125\n",
      "Epoch 1624, Loss: 0.3628142476081848, Final Batch Loss: 0.18941225111484528\n",
      "Epoch 1625, Loss: 0.44733256101608276, Final Batch Loss: 0.25026705861091614\n",
      "Epoch 1626, Loss: 0.40222495794296265, Final Batch Loss: 0.22181682288646698\n",
      "Epoch 1627, Loss: 0.4512978047132492, Final Batch Loss: 0.1775580197572708\n",
      "Epoch 1628, Loss: 0.44511060416698456, Final Batch Loss: 0.22448298335075378\n",
      "Epoch 1629, Loss: 0.38132986426353455, Final Batch Loss: 0.18661105632781982\n",
      "Epoch 1630, Loss: 0.3607759177684784, Final Batch Loss: 0.18378695845603943\n",
      "Epoch 1631, Loss: 0.3948161154985428, Final Batch Loss: 0.23341885209083557\n",
      "Epoch 1632, Loss: 0.4070563465356827, Final Batch Loss: 0.19151975214481354\n",
      "Epoch 1633, Loss: 0.38042424619197845, Final Batch Loss: 0.1878638118505478\n",
      "Epoch 1634, Loss: 0.4085264354944229, Final Batch Loss: 0.2036038637161255\n",
      "Epoch 1635, Loss: 0.388814315199852, Final Batch Loss: 0.1945938616991043\n",
      "Epoch 1636, Loss: 0.4190139174461365, Final Batch Loss: 0.18997524678707123\n",
      "Epoch 1637, Loss: 0.41969601809978485, Final Batch Loss: 0.20317533612251282\n",
      "Epoch 1638, Loss: 0.3564874082803726, Final Batch Loss: 0.1788260042667389\n",
      "Epoch 1639, Loss: 0.38682110607624054, Final Batch Loss: 0.16641399264335632\n",
      "Epoch 1640, Loss: 0.36001116037368774, Final Batch Loss: 0.2064739316701889\n",
      "Epoch 1641, Loss: 0.4055587202310562, Final Batch Loss: 0.1966993361711502\n",
      "Epoch 1642, Loss: 0.3810625225305557, Final Batch Loss: 0.20573337376117706\n",
      "Epoch 1643, Loss: 0.3696468472480774, Final Batch Loss: 0.1720762848854065\n",
      "Epoch 1644, Loss: 0.38020454347133636, Final Batch Loss: 0.20485416054725647\n",
      "Epoch 1645, Loss: 0.4158634692430496, Final Batch Loss: 0.22366957366466522\n",
      "Epoch 1646, Loss: 0.415686696767807, Final Batch Loss: 0.1890619844198227\n",
      "Epoch 1647, Loss: 0.4303485155105591, Final Batch Loss: 0.1720345914363861\n",
      "Epoch 1648, Loss: 0.3623330742120743, Final Batch Loss: 0.1821468323469162\n",
      "Epoch 1649, Loss: 0.38579656183719635, Final Batch Loss: 0.16928324103355408\n",
      "Epoch 1650, Loss: 0.37197986245155334, Final Batch Loss: 0.20470409095287323\n",
      "Epoch 1651, Loss: 0.37306883931159973, Final Batch Loss: 0.17236004769802094\n",
      "Epoch 1652, Loss: 0.3810232877731323, Final Batch Loss: 0.19419480860233307\n",
      "Epoch 1653, Loss: 0.42607833445072174, Final Batch Loss: 0.2245124727487564\n",
      "Epoch 1654, Loss: 0.4177730679512024, Final Batch Loss: 0.22678913176059723\n",
      "Epoch 1655, Loss: 0.38960985839366913, Final Batch Loss: 0.20728541910648346\n",
      "Epoch 1656, Loss: 0.35422053933143616, Final Batch Loss: 0.19326746463775635\n",
      "Epoch 1657, Loss: 0.413136824965477, Final Batch Loss: 0.1990698277950287\n",
      "Epoch 1658, Loss: 0.40413253009319305, Final Batch Loss: 0.20712819695472717\n",
      "Epoch 1659, Loss: 0.34956182539463043, Final Batch Loss: 0.16718964278697968\n",
      "Epoch 1660, Loss: 0.3990006148815155, Final Batch Loss: 0.16543729603290558\n",
      "Epoch 1661, Loss: 0.408540278673172, Final Batch Loss: 0.2170652598142624\n",
      "Epoch 1662, Loss: 0.40197667479515076, Final Batch Loss: 0.20520000159740448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1663, Loss: 0.38997167348861694, Final Batch Loss: 0.20790168642997742\n",
      "Epoch 1664, Loss: 0.40997469425201416, Final Batch Loss: 0.22512094676494598\n",
      "Epoch 1665, Loss: 0.3521731197834015, Final Batch Loss: 0.17669777572155\n",
      "Epoch 1666, Loss: 0.39702680706977844, Final Batch Loss: 0.2093839794397354\n",
      "Epoch 1667, Loss: 0.38338570296764374, Final Batch Loss: 0.15294349193572998\n",
      "Epoch 1668, Loss: 0.40562985837459564, Final Batch Loss: 0.19961848855018616\n",
      "Epoch 1669, Loss: 0.4155438393354416, Final Batch Loss: 0.1989302784204483\n",
      "Epoch 1670, Loss: 0.4049563854932785, Final Batch Loss: 0.22111541032791138\n",
      "Epoch 1671, Loss: 0.39918744564056396, Final Batch Loss: 0.2096414417028427\n",
      "Epoch 1672, Loss: 0.3772140443325043, Final Batch Loss: 0.18468521535396576\n",
      "Epoch 1673, Loss: 0.3528134524822235, Final Batch Loss: 0.18441247940063477\n",
      "Epoch 1674, Loss: 0.39321647584438324, Final Batch Loss: 0.19717834889888763\n",
      "Epoch 1675, Loss: 0.34518949687480927, Final Batch Loss: 0.16356857120990753\n",
      "Epoch 1676, Loss: 0.3744069039821625, Final Batch Loss: 0.19481799006462097\n",
      "Epoch 1677, Loss: 0.3884531706571579, Final Batch Loss: 0.18761268258094788\n",
      "Epoch 1678, Loss: 0.39998963475227356, Final Batch Loss: 0.21497385203838348\n",
      "Epoch 1679, Loss: 0.3688735067844391, Final Batch Loss: 0.1851194053888321\n",
      "Epoch 1680, Loss: 0.37628792226314545, Final Batch Loss: 0.21916495263576508\n",
      "Epoch 1681, Loss: 0.4247443675994873, Final Batch Loss: 0.20578861236572266\n",
      "Epoch 1682, Loss: 0.44007593393325806, Final Batch Loss: 0.22894510626792908\n",
      "Epoch 1683, Loss: 0.41272056102752686, Final Batch Loss: 0.22809065878391266\n",
      "Epoch 1684, Loss: 0.4730943292379379, Final Batch Loss: 0.2895630896091461\n",
      "Epoch 1685, Loss: 0.4324735403060913, Final Batch Loss: 0.23009838163852692\n",
      "Epoch 1686, Loss: 0.3567291647195816, Final Batch Loss: 0.1892559975385666\n",
      "Epoch 1687, Loss: 0.384266197681427, Final Batch Loss: 0.17881308495998383\n",
      "Epoch 1688, Loss: 0.4087218791246414, Final Batch Loss: 0.20977668464183807\n",
      "Epoch 1689, Loss: 0.38561879098415375, Final Batch Loss: 0.16371437907218933\n",
      "Epoch 1690, Loss: 0.40643273293972015, Final Batch Loss: 0.23425792157649994\n",
      "Epoch 1691, Loss: 0.4179115742444992, Final Batch Loss: 0.2125258892774582\n",
      "Epoch 1692, Loss: 0.38132280111312866, Final Batch Loss: 0.2153010070323944\n",
      "Epoch 1693, Loss: 0.35549020767211914, Final Batch Loss: 0.1833757609128952\n",
      "Epoch 1694, Loss: 0.4210675060749054, Final Batch Loss: 0.20708860456943512\n",
      "Epoch 1695, Loss: 0.3932970315217972, Final Batch Loss: 0.2110874205827713\n",
      "Epoch 1696, Loss: 0.3403974026441574, Final Batch Loss: 0.13049569725990295\n",
      "Epoch 1697, Loss: 0.4194684624671936, Final Batch Loss: 0.1959252506494522\n",
      "Epoch 1698, Loss: 0.3501892387866974, Final Batch Loss: 0.1724134236574173\n",
      "Epoch 1699, Loss: 0.38863006234169006, Final Batch Loss: 0.16612643003463745\n",
      "Epoch 1700, Loss: 0.3880741447210312, Final Batch Loss: 0.21380019187927246\n",
      "Epoch 1701, Loss: 0.41286128759384155, Final Batch Loss: 0.16870558261871338\n",
      "Epoch 1702, Loss: 0.3990749567747116, Final Batch Loss: 0.21865756809711456\n",
      "Epoch 1703, Loss: 0.3919716477394104, Final Batch Loss: 0.19956783950328827\n",
      "Epoch 1704, Loss: 0.3830660730600357, Final Batch Loss: 0.1799294799566269\n",
      "Epoch 1705, Loss: 0.3785501569509506, Final Batch Loss: 0.20299313962459564\n",
      "Epoch 1706, Loss: 0.3966768980026245, Final Batch Loss: 0.2011336088180542\n",
      "Epoch 1707, Loss: 0.3710532933473587, Final Batch Loss: 0.1731482893228531\n",
      "Epoch 1708, Loss: 0.449828639626503, Final Batch Loss: 0.231133833527565\n",
      "Epoch 1709, Loss: 0.3792339265346527, Final Batch Loss: 0.19909033179283142\n",
      "Epoch 1710, Loss: 0.3947911411523819, Final Batch Loss: 0.1735529601573944\n",
      "Epoch 1711, Loss: 0.36518244445323944, Final Batch Loss: 0.18196699023246765\n",
      "Epoch 1712, Loss: 0.391713485121727, Final Batch Loss: 0.22035035490989685\n",
      "Epoch 1713, Loss: 0.47117744386196136, Final Batch Loss: 0.2574981153011322\n",
      "Epoch 1714, Loss: 0.4922860413789749, Final Batch Loss: 0.23063607513904572\n",
      "Epoch 1715, Loss: 0.35082897543907166, Final Batch Loss: 0.16291514039039612\n",
      "Epoch 1716, Loss: 0.41591542959213257, Final Batch Loss: 0.22617420554161072\n",
      "Epoch 1717, Loss: 0.4264213591814041, Final Batch Loss: 0.20498037338256836\n",
      "Epoch 1718, Loss: 0.3949914574623108, Final Batch Loss: 0.23879079520702362\n",
      "Epoch 1719, Loss: 0.38741275668144226, Final Batch Loss: 0.15974728763103485\n",
      "Epoch 1720, Loss: 0.42661210894584656, Final Batch Loss: 0.20097847282886505\n",
      "Epoch 1721, Loss: 0.4026612937450409, Final Batch Loss: 0.21971319615840912\n",
      "Epoch 1722, Loss: 0.4161132127046585, Final Batch Loss: 0.19858889281749725\n",
      "Epoch 1723, Loss: 0.36540794372558594, Final Batch Loss: 0.19449582695960999\n",
      "Epoch 1724, Loss: 0.4574791193008423, Final Batch Loss: 0.22440141439437866\n",
      "Epoch 1725, Loss: 0.4113328456878662, Final Batch Loss: 0.22782090306282043\n",
      "Epoch 1726, Loss: 0.4361172169446945, Final Batch Loss: 0.18422038853168488\n",
      "Epoch 1727, Loss: 0.42071206867694855, Final Batch Loss: 0.1903889924287796\n",
      "Epoch 1728, Loss: 0.365549772977829, Final Batch Loss: 0.16626323759555817\n",
      "Epoch 1729, Loss: 0.4048774689435959, Final Batch Loss: 0.17917129397392273\n",
      "Epoch 1730, Loss: 0.40692177414894104, Final Batch Loss: 0.21292270720005035\n",
      "Epoch 1731, Loss: 0.3881600648164749, Final Batch Loss: 0.203242689371109\n",
      "Epoch 1732, Loss: 0.41923728585243225, Final Batch Loss: 0.20974411070346832\n",
      "Epoch 1733, Loss: 0.36629618704319, Final Batch Loss: 0.2089996486902237\n",
      "Epoch 1734, Loss: 0.3835221379995346, Final Batch Loss: 0.17746774852275848\n",
      "Epoch 1735, Loss: 0.44397127628326416, Final Batch Loss: 0.21909861266613007\n",
      "Epoch 1736, Loss: 0.40802328288555145, Final Batch Loss: 0.21474607288837433\n",
      "Epoch 1737, Loss: 0.3914247155189514, Final Batch Loss: 0.19887816905975342\n",
      "Epoch 1738, Loss: 0.39271171391010284, Final Batch Loss: 0.19940567016601562\n",
      "Epoch 1739, Loss: 0.367207333445549, Final Batch Loss: 0.1625753790140152\n",
      "Epoch 1740, Loss: 0.38085320591926575, Final Batch Loss: 0.19519905745983124\n",
      "Epoch 1741, Loss: 0.37772153317928314, Final Batch Loss: 0.20604102313518524\n",
      "Epoch 1742, Loss: 0.4316559135913849, Final Batch Loss: 0.2227237969636917\n",
      "Epoch 1743, Loss: 0.3862542062997818, Final Batch Loss: 0.1946459263563156\n",
      "Epoch 1744, Loss: 0.3827318698167801, Final Batch Loss: 0.1885136067867279\n",
      "Epoch 1745, Loss: 0.3704501986503601, Final Batch Loss: 0.18999004364013672\n",
      "Epoch 1746, Loss: 0.37277427315711975, Final Batch Loss: 0.18847328424453735\n",
      "Epoch 1747, Loss: 0.3805800825357437, Final Batch Loss: 0.19312667846679688\n",
      "Epoch 1748, Loss: 0.3643083870410919, Final Batch Loss: 0.16782839596271515\n",
      "Epoch 1749, Loss: 0.4027058184146881, Final Batch Loss: 0.2493317425251007\n",
      "Epoch 1750, Loss: 0.3899264931678772, Final Batch Loss: 0.1731751263141632\n",
      "Epoch 1751, Loss: 0.4161052405834198, Final Batch Loss: 0.18684257566928864\n",
      "Epoch 1752, Loss: 0.3813009560108185, Final Batch Loss: 0.1697344034910202\n",
      "Epoch 1753, Loss: 0.37302547693252563, Final Batch Loss: 0.1846306473016739\n",
      "Epoch 1754, Loss: 0.3938569724559784, Final Batch Loss: 0.22437657415866852\n",
      "Epoch 1755, Loss: 0.36708061397075653, Final Batch Loss: 0.1738181859254837\n",
      "Epoch 1756, Loss: 0.40300826728343964, Final Batch Loss: 0.1970183402299881\n",
      "Epoch 1757, Loss: 0.3746388256549835, Final Batch Loss: 0.23078224062919617\n",
      "Epoch 1758, Loss: 0.3968237489461899, Final Batch Loss: 0.20483720302581787\n",
      "Epoch 1759, Loss: 0.4272073805332184, Final Batch Loss: 0.21672002971172333\n",
      "Epoch 1760, Loss: 0.32946132123470306, Final Batch Loss: 0.15544074773788452\n",
      "Epoch 1761, Loss: 0.42208023369312286, Final Batch Loss: 0.23911938071250916\n",
      "Epoch 1762, Loss: 0.4274575412273407, Final Batch Loss: 0.21328206360340118\n",
      "Epoch 1763, Loss: 0.41035863757133484, Final Batch Loss: 0.19927549362182617\n",
      "Epoch 1764, Loss: 0.4033748209476471, Final Batch Loss: 0.1873629093170166\n",
      "Epoch 1765, Loss: 0.36011046171188354, Final Batch Loss: 0.20767512917518616\n",
      "Epoch 1766, Loss: 0.36426496505737305, Final Batch Loss: 0.197567880153656\n",
      "Epoch 1767, Loss: 0.39068280160427094, Final Batch Loss: 0.17956781387329102\n",
      "Epoch 1768, Loss: 0.37777210772037506, Final Batch Loss: 0.2446674406528473\n",
      "Epoch 1769, Loss: 0.36517056822776794, Final Batch Loss: 0.2120777666568756\n",
      "Epoch 1770, Loss: 0.3785300999879837, Final Batch Loss: 0.1957559734582901\n",
      "Epoch 1771, Loss: 0.31948836147785187, Final Batch Loss: 0.15048004686832428\n",
      "Epoch 1772, Loss: 0.40365077555179596, Final Batch Loss: 0.2345651090145111\n",
      "Epoch 1773, Loss: 0.4279753863811493, Final Batch Loss: 0.21089749038219452\n",
      "Epoch 1774, Loss: 0.39458683133125305, Final Batch Loss: 0.18150290846824646\n",
      "Epoch 1775, Loss: 0.3990945369005203, Final Batch Loss: 0.26135972142219543\n",
      "Epoch 1776, Loss: 0.3796134740114212, Final Batch Loss: 0.1901385337114334\n",
      "Epoch 1777, Loss: 0.409870907664299, Final Batch Loss: 0.18907730281352997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1778, Loss: 0.3541502058506012, Final Batch Loss: 0.1568423956632614\n",
      "Epoch 1779, Loss: 0.3902605473995209, Final Batch Loss: 0.2026287317276001\n",
      "Epoch 1780, Loss: 0.34942907094955444, Final Batch Loss: 0.16051283478736877\n",
      "Epoch 1781, Loss: 0.4779384434223175, Final Batch Loss: 0.23794730007648468\n",
      "Epoch 1782, Loss: 0.37620650231838226, Final Batch Loss: 0.18594709038734436\n",
      "Epoch 1783, Loss: 0.37077103555202484, Final Batch Loss: 0.1790120005607605\n",
      "Epoch 1784, Loss: 0.3856000006198883, Final Batch Loss: 0.1943664699792862\n",
      "Epoch 1785, Loss: 0.40271350741386414, Final Batch Loss: 0.18448930978775024\n",
      "Epoch 1786, Loss: 0.3818015158176422, Final Batch Loss: 0.20675085484981537\n",
      "Epoch 1787, Loss: 0.3487461656332016, Final Batch Loss: 0.17847110331058502\n",
      "Epoch 1788, Loss: 0.35506150126457214, Final Batch Loss: 0.18167106807231903\n",
      "Epoch 1789, Loss: 0.33808720111846924, Final Batch Loss: 0.15599900484085083\n",
      "Epoch 1790, Loss: 0.3676459491252899, Final Batch Loss: 0.19266027212142944\n",
      "Epoch 1791, Loss: 0.3796592950820923, Final Batch Loss: 0.1932864785194397\n",
      "Epoch 1792, Loss: 0.38098224997520447, Final Batch Loss: 0.20214399695396423\n",
      "Epoch 1793, Loss: 0.3633918911218643, Final Batch Loss: 0.1664966493844986\n",
      "Epoch 1794, Loss: 0.41550061106681824, Final Batch Loss: 0.19008402526378632\n",
      "Epoch 1795, Loss: 0.35844944417476654, Final Batch Loss: 0.16579605638980865\n",
      "Epoch 1796, Loss: 0.3796004056930542, Final Batch Loss: 0.18773338198661804\n",
      "Epoch 1797, Loss: 0.3658568263053894, Final Batch Loss: 0.16652067005634308\n",
      "Epoch 1798, Loss: 0.3850392997264862, Final Batch Loss: 0.20097951591014862\n",
      "Epoch 1799, Loss: 0.3849211782217026, Final Batch Loss: 0.2080979347229004\n",
      "Epoch 1800, Loss: 0.38267481327056885, Final Batch Loss: 0.17224112153053284\n",
      "Epoch 1801, Loss: 0.42477641999721527, Final Batch Loss: 0.19480818510055542\n",
      "Epoch 1802, Loss: 0.3732488602399826, Final Batch Loss: 0.15411806106567383\n",
      "Epoch 1803, Loss: 0.37243475019931793, Final Batch Loss: 0.20587271451950073\n",
      "Epoch 1804, Loss: 0.38647228479385376, Final Batch Loss: 0.2246314138174057\n",
      "Epoch 1805, Loss: 0.39566241204738617, Final Batch Loss: 0.171262726187706\n",
      "Epoch 1806, Loss: 0.42108097672462463, Final Batch Loss: 0.19023674726486206\n",
      "Epoch 1807, Loss: 0.4119790196418762, Final Batch Loss: 0.1807098090648651\n",
      "Epoch 1808, Loss: 0.3676757365465164, Final Batch Loss: 0.17786136269569397\n",
      "Epoch 1809, Loss: 0.3905665725469589, Final Batch Loss: 0.19754813611507416\n",
      "Epoch 1810, Loss: 0.41028936207294464, Final Batch Loss: 0.16405081748962402\n",
      "Epoch 1811, Loss: 0.4000731259584427, Final Batch Loss: 0.2335774302482605\n",
      "Epoch 1812, Loss: 0.37519852817058563, Final Batch Loss: 0.1457003355026245\n",
      "Epoch 1813, Loss: 0.373437762260437, Final Batch Loss: 0.17558376491069794\n",
      "Epoch 1814, Loss: 0.36237089335918427, Final Batch Loss: 0.18381065130233765\n",
      "Epoch 1815, Loss: 0.4130183458328247, Final Batch Loss: 0.19582483172416687\n",
      "Epoch 1816, Loss: 0.3376506268978119, Final Batch Loss: 0.1665063202381134\n",
      "Epoch 1817, Loss: 0.38682085275650024, Final Batch Loss: 0.20136454701423645\n",
      "Epoch 1818, Loss: 0.3948657363653183, Final Batch Loss: 0.19387675821781158\n",
      "Epoch 1819, Loss: 0.3871033042669296, Final Batch Loss: 0.1623530238866806\n",
      "Epoch 1820, Loss: 0.3569587171077728, Final Batch Loss: 0.21257713437080383\n",
      "Epoch 1821, Loss: 0.42381513118743896, Final Batch Loss: 0.2241216003894806\n",
      "Epoch 1822, Loss: 0.3566853702068329, Final Batch Loss: 0.17173826694488525\n",
      "Epoch 1823, Loss: 0.36113692820072174, Final Batch Loss: 0.17916221916675568\n",
      "Epoch 1824, Loss: 0.39876869320869446, Final Batch Loss: 0.23386110365390778\n",
      "Epoch 1825, Loss: 0.3820973187685013, Final Batch Loss: 0.18036094307899475\n",
      "Epoch 1826, Loss: 0.4403716027736664, Final Batch Loss: 0.2318107783794403\n",
      "Epoch 1827, Loss: 0.3541505038738251, Final Batch Loss: 0.1663263440132141\n",
      "Epoch 1828, Loss: 0.3998849242925644, Final Batch Loss: 0.16842488944530487\n",
      "Epoch 1829, Loss: 0.3834359496831894, Final Batch Loss: 0.2032049596309662\n",
      "Epoch 1830, Loss: 0.3189885914325714, Final Batch Loss: 0.16351968050003052\n",
      "Epoch 1831, Loss: 0.3755446821451187, Final Batch Loss: 0.1816140115261078\n",
      "Epoch 1832, Loss: 0.4456908851861954, Final Batch Loss: 0.20060302317142487\n",
      "Epoch 1833, Loss: 0.41760551929473877, Final Batch Loss: 0.21222715079784393\n",
      "Epoch 1834, Loss: 0.4205867648124695, Final Batch Loss: 0.21287235617637634\n",
      "Epoch 1835, Loss: 0.3692094534635544, Final Batch Loss: 0.18128132820129395\n",
      "Epoch 1836, Loss: 0.36878059804439545, Final Batch Loss: 0.15545618534088135\n",
      "Epoch 1837, Loss: 0.40095892548561096, Final Batch Loss: 0.19262869656085968\n",
      "Epoch 1838, Loss: 0.38728244602680206, Final Batch Loss: 0.20877566933631897\n",
      "Epoch 1839, Loss: 0.37011682987213135, Final Batch Loss: 0.1866239756345749\n",
      "Epoch 1840, Loss: 0.3517857789993286, Final Batch Loss: 0.1714935153722763\n",
      "Epoch 1841, Loss: 0.3756731301546097, Final Batch Loss: 0.19837872684001923\n",
      "Epoch 1842, Loss: 0.3675314486026764, Final Batch Loss: 0.18359380960464478\n",
      "Epoch 1843, Loss: 0.3568633794784546, Final Batch Loss: 0.17997996509075165\n",
      "Epoch 1844, Loss: 0.41730350255966187, Final Batch Loss: 0.17405426502227783\n",
      "Epoch 1845, Loss: 0.37081797420978546, Final Batch Loss: 0.2159138023853302\n",
      "Epoch 1846, Loss: 0.3677227050065994, Final Batch Loss: 0.1774628460407257\n",
      "Epoch 1847, Loss: 0.3442566990852356, Final Batch Loss: 0.20438091456890106\n",
      "Epoch 1848, Loss: 0.38963428139686584, Final Batch Loss: 0.23019154369831085\n",
      "Epoch 1849, Loss: 0.3717794120311737, Final Batch Loss: 0.17456695437431335\n",
      "Epoch 1850, Loss: 0.3729289472103119, Final Batch Loss: 0.16729652881622314\n",
      "Epoch 1851, Loss: 0.39217428863048553, Final Batch Loss: 0.2213597446680069\n",
      "Epoch 1852, Loss: 0.41970784962177277, Final Batch Loss: 0.17656460404396057\n",
      "Epoch 1853, Loss: 0.3557005077600479, Final Batch Loss: 0.16328412294387817\n",
      "Epoch 1854, Loss: 0.368148997426033, Final Batch Loss: 0.17330235242843628\n",
      "Epoch 1855, Loss: 0.34773074090480804, Final Batch Loss: 0.18144667148590088\n",
      "Epoch 1856, Loss: 0.35438358783721924, Final Batch Loss: 0.17799362540245056\n",
      "Epoch 1857, Loss: 0.344274178147316, Final Batch Loss: 0.17984093725681305\n",
      "Epoch 1858, Loss: 0.3427831679582596, Final Batch Loss: 0.18966063857078552\n",
      "Epoch 1859, Loss: 0.38777635991573334, Final Batch Loss: 0.17216980457305908\n",
      "Epoch 1860, Loss: 0.42753052711486816, Final Batch Loss: 0.23114311695098877\n",
      "Epoch 1861, Loss: 0.3491257131099701, Final Batch Loss: 0.18008314073085785\n",
      "Epoch 1862, Loss: 0.39729802310466766, Final Batch Loss: 0.17673514783382416\n",
      "Epoch 1863, Loss: 0.3583544194698334, Final Batch Loss: 0.1850590854883194\n",
      "Epoch 1864, Loss: 0.4340895563364029, Final Batch Loss: 0.22684675455093384\n",
      "Epoch 1865, Loss: 0.39480993151664734, Final Batch Loss: 0.1719849407672882\n",
      "Epoch 1866, Loss: 0.34654758870601654, Final Batch Loss: 0.15988747775554657\n",
      "Epoch 1867, Loss: 0.3973165303468704, Final Batch Loss: 0.1994055211544037\n",
      "Epoch 1868, Loss: 0.3545449376106262, Final Batch Loss: 0.15837466716766357\n",
      "Epoch 1869, Loss: 0.3413096219301224, Final Batch Loss: 0.14226870238780975\n",
      "Epoch 1870, Loss: 0.3764900863170624, Final Batch Loss: 0.1794651299715042\n",
      "Epoch 1871, Loss: 0.4343104213476181, Final Batch Loss: 0.2651088237762451\n",
      "Epoch 1872, Loss: 0.3968481719493866, Final Batch Loss: 0.1807207465171814\n",
      "Epoch 1873, Loss: 0.36774270236492157, Final Batch Loss: 0.1733902394771576\n",
      "Epoch 1874, Loss: 0.3978265970945358, Final Batch Loss: 0.15051357448101044\n",
      "Epoch 1875, Loss: 0.39095546305179596, Final Batch Loss: 0.1723390817642212\n",
      "Epoch 1876, Loss: 0.35442425310611725, Final Batch Loss: 0.21217456459999084\n",
      "Epoch 1877, Loss: 0.32423415780067444, Final Batch Loss: 0.18970108032226562\n",
      "Epoch 1878, Loss: 0.36288584768772125, Final Batch Loss: 0.1993367224931717\n",
      "Epoch 1879, Loss: 0.35483528673648834, Final Batch Loss: 0.18630628287792206\n",
      "Epoch 1880, Loss: 0.3660135716199875, Final Batch Loss: 0.1896866112947464\n",
      "Epoch 1881, Loss: 0.3326476812362671, Final Batch Loss: 0.15163177251815796\n",
      "Epoch 1882, Loss: 0.38762445747852325, Final Batch Loss: 0.19192320108413696\n",
      "Epoch 1883, Loss: 0.35344237089157104, Final Batch Loss: 0.19652564823627472\n",
      "Epoch 1884, Loss: 0.3538319766521454, Final Batch Loss: 0.16469912230968475\n",
      "Epoch 1885, Loss: 0.35399770736694336, Final Batch Loss: 0.22244618833065033\n",
      "Epoch 1886, Loss: 0.41945625841617584, Final Batch Loss: 0.2508881092071533\n",
      "Epoch 1887, Loss: 0.37380899488925934, Final Batch Loss: 0.1854461133480072\n",
      "Epoch 1888, Loss: 0.3653602749109268, Final Batch Loss: 0.18358832597732544\n",
      "Epoch 1889, Loss: 0.4025309085845947, Final Batch Loss: 0.20866242051124573\n",
      "Epoch 1890, Loss: 0.37420986592769623, Final Batch Loss: 0.18070481717586517\n",
      "Epoch 1891, Loss: 0.3919838070869446, Final Batch Loss: 0.16244946420192719\n",
      "Epoch 1892, Loss: 0.38632988929748535, Final Batch Loss: 0.18618711829185486\n",
      "Epoch 1893, Loss: 0.3952978700399399, Final Batch Loss: 0.18879380822181702\n",
      "Epoch 1894, Loss: 0.3899170756340027, Final Batch Loss: 0.20630089938640594\n",
      "Epoch 1895, Loss: 0.373413622379303, Final Batch Loss: 0.16880415380001068\n",
      "Epoch 1896, Loss: 0.38055019080638885, Final Batch Loss: 0.19345910847187042\n",
      "Epoch 1897, Loss: 0.4028875231742859, Final Batch Loss: 0.21487969160079956\n",
      "Epoch 1898, Loss: 0.37223948538303375, Final Batch Loss: 0.22382253408432007\n",
      "Epoch 1899, Loss: 0.3587287664413452, Final Batch Loss: 0.18173998594284058\n",
      "Epoch 1900, Loss: 0.3817065805196762, Final Batch Loss: 0.18506141006946564\n",
      "Epoch 1901, Loss: 0.36487850546836853, Final Batch Loss: 0.17125166952610016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1902, Loss: 0.3995547592639923, Final Batch Loss: 0.17445436120033264\n",
      "Epoch 1903, Loss: 0.4602823257446289, Final Batch Loss: 0.1924445927143097\n",
      "Epoch 1904, Loss: 0.3759099543094635, Final Batch Loss: 0.21163184940814972\n",
      "Epoch 1905, Loss: 0.344563364982605, Final Batch Loss: 0.16165921092033386\n",
      "Epoch 1906, Loss: 0.3830341398715973, Final Batch Loss: 0.18098148703575134\n",
      "Epoch 1907, Loss: 0.35511085391044617, Final Batch Loss: 0.1414978951215744\n",
      "Epoch 1908, Loss: 0.37623023986816406, Final Batch Loss: 0.19460192322731018\n",
      "Epoch 1909, Loss: 0.36835838854312897, Final Batch Loss: 0.1927911341190338\n",
      "Epoch 1910, Loss: 0.3832547813653946, Final Batch Loss: 0.20306865870952606\n",
      "Epoch 1911, Loss: 0.3233676701784134, Final Batch Loss: 0.14329758286476135\n",
      "Epoch 1912, Loss: 0.31323301792144775, Final Batch Loss: 0.15346401929855347\n",
      "Epoch 1913, Loss: 0.41858240962028503, Final Batch Loss: 0.19232112169265747\n",
      "Epoch 1914, Loss: 0.3960207551717758, Final Batch Loss: 0.1947331428527832\n",
      "Epoch 1915, Loss: 0.3771509379148483, Final Batch Loss: 0.19905880093574524\n",
      "Epoch 1916, Loss: 0.39499326050281525, Final Batch Loss: 0.20257879793643951\n",
      "Epoch 1917, Loss: 0.4303961247205734, Final Batch Loss: 0.16719748079776764\n",
      "Epoch 1918, Loss: 0.4072501063346863, Final Batch Loss: 0.19766053557395935\n",
      "Epoch 1919, Loss: 0.4209546744823456, Final Batch Loss: 0.2567289471626282\n",
      "Epoch 1920, Loss: 0.3838934302330017, Final Batch Loss: 0.18657808005809784\n",
      "Epoch 1921, Loss: 0.342479407787323, Final Batch Loss: 0.17742110788822174\n",
      "Epoch 1922, Loss: 0.37901148200035095, Final Batch Loss: 0.20043052732944489\n",
      "Epoch 1923, Loss: 0.3906458169221878, Final Batch Loss: 0.16458004713058472\n",
      "Epoch 1924, Loss: 0.3886411488056183, Final Batch Loss: 0.19480015337467194\n",
      "Epoch 1925, Loss: 0.35570235550403595, Final Batch Loss: 0.1525803804397583\n",
      "Epoch 1926, Loss: 0.3562973886728287, Final Batch Loss: 0.14731156826019287\n",
      "Epoch 1927, Loss: 0.37404438853263855, Final Batch Loss: 0.18788188695907593\n",
      "Epoch 1928, Loss: 0.3613574951887131, Final Batch Loss: 0.15508615970611572\n",
      "Epoch 1929, Loss: 0.3900827616453171, Final Batch Loss: 0.20453870296478271\n",
      "Epoch 1930, Loss: 0.4296231120824814, Final Batch Loss: 0.247401624917984\n",
      "Epoch 1931, Loss: 0.41626983880996704, Final Batch Loss: 0.19993560016155243\n",
      "Epoch 1932, Loss: 0.33714376389980316, Final Batch Loss: 0.19466394186019897\n",
      "Epoch 1933, Loss: 0.36429761350154877, Final Batch Loss: 0.15993985533714294\n",
      "Epoch 1934, Loss: 0.3690175712108612, Final Batch Loss: 0.18799225986003876\n",
      "Epoch 1935, Loss: 0.40116676688194275, Final Batch Loss: 0.2221137136220932\n",
      "Epoch 1936, Loss: 0.34619511663913727, Final Batch Loss: 0.1366402804851532\n",
      "Epoch 1937, Loss: 0.3671240508556366, Final Batch Loss: 0.187693789601326\n",
      "Epoch 1938, Loss: 0.4247983694076538, Final Batch Loss: 0.22549223899841309\n",
      "Epoch 1939, Loss: 0.3759627342224121, Final Batch Loss: 0.19438399374485016\n",
      "Epoch 1940, Loss: 0.3705996721982956, Final Batch Loss: 0.18405568599700928\n",
      "Epoch 1941, Loss: 0.369113028049469, Final Batch Loss: 0.16676771640777588\n",
      "Epoch 1942, Loss: 0.3213459998369217, Final Batch Loss: 0.16977529227733612\n",
      "Epoch 1943, Loss: 0.35510969161987305, Final Batch Loss: 0.1957426369190216\n",
      "Epoch 1944, Loss: 0.3514886796474457, Final Batch Loss: 0.1701391339302063\n",
      "Epoch 1945, Loss: 0.3750667870044708, Final Batch Loss: 0.17505766451358795\n",
      "Epoch 1946, Loss: 0.38150274753570557, Final Batch Loss: 0.18135644495487213\n",
      "Epoch 1947, Loss: 0.3611244559288025, Final Batch Loss: 0.21081574261188507\n",
      "Epoch 1948, Loss: 0.4669980853796005, Final Batch Loss: 0.30789291858673096\n",
      "Epoch 1949, Loss: 0.34776873886585236, Final Batch Loss: 0.1935967206954956\n",
      "Epoch 1950, Loss: 0.32742011547088623, Final Batch Loss: 0.1915835291147232\n",
      "Epoch 1951, Loss: 0.326959490776062, Final Batch Loss: 0.15647155046463013\n",
      "Epoch 1952, Loss: 0.3643408566713333, Final Batch Loss: 0.151707723736763\n",
      "Epoch 1953, Loss: 0.3919631987810135, Final Batch Loss: 0.2174738645553589\n",
      "Epoch 1954, Loss: 0.38867001235485077, Final Batch Loss: 0.2112676501274109\n",
      "Epoch 1955, Loss: 0.32563653588294983, Final Batch Loss: 0.1715947538614273\n",
      "Epoch 1956, Loss: 0.34904977679252625, Final Batch Loss: 0.16355979442596436\n",
      "Epoch 1957, Loss: 0.3279472589492798, Final Batch Loss: 0.1657659113407135\n",
      "Epoch 1958, Loss: 0.3511163890361786, Final Batch Loss: 0.18349653482437134\n",
      "Epoch 1959, Loss: 0.3809530436992645, Final Batch Loss: 0.22428596019744873\n",
      "Epoch 1960, Loss: 0.3787368834018707, Final Batch Loss: 0.22118878364562988\n",
      "Epoch 1961, Loss: 0.33649976551532745, Final Batch Loss: 0.1541496068239212\n",
      "Epoch 1962, Loss: 0.38402703404426575, Final Batch Loss: 0.1866801530122757\n",
      "Epoch 1963, Loss: 0.3860951364040375, Final Batch Loss: 0.16603630781173706\n",
      "Epoch 1964, Loss: 0.3724425435066223, Final Batch Loss: 0.2104751169681549\n",
      "Epoch 1965, Loss: 0.3472532331943512, Final Batch Loss: 0.1531025469303131\n",
      "Epoch 1966, Loss: 0.3768480569124222, Final Batch Loss: 0.19403551518917084\n",
      "Epoch 1967, Loss: 0.35145077109336853, Final Batch Loss: 0.17120423913002014\n",
      "Epoch 1968, Loss: 0.3629225194454193, Final Batch Loss: 0.20405679941177368\n",
      "Epoch 1969, Loss: 0.37348443269729614, Final Batch Loss: 0.17285123467445374\n",
      "Epoch 1970, Loss: 0.3518787622451782, Final Batch Loss: 0.18744640052318573\n",
      "Epoch 1971, Loss: 0.3529934138059616, Final Batch Loss: 0.17591044306755066\n",
      "Epoch 1972, Loss: 0.3382730782032013, Final Batch Loss: 0.1792861372232437\n",
      "Epoch 1973, Loss: 0.39392805099487305, Final Batch Loss: 0.2277051955461502\n",
      "Epoch 1974, Loss: 0.360854834318161, Final Batch Loss: 0.19217988848686218\n",
      "Epoch 1975, Loss: 0.3782747983932495, Final Batch Loss: 0.1950419396162033\n",
      "Epoch 1976, Loss: 0.3240395784378052, Final Batch Loss: 0.17736686766147614\n",
      "Epoch 1977, Loss: 0.3876909911632538, Final Batch Loss: 0.19413621723651886\n",
      "Epoch 1978, Loss: 0.4032045304775238, Final Batch Loss: 0.23180356621742249\n",
      "Epoch 1979, Loss: 0.32849839329719543, Final Batch Loss: 0.1489405632019043\n",
      "Epoch 1980, Loss: 0.35889601707458496, Final Batch Loss: 0.17985381186008453\n",
      "Epoch 1981, Loss: 0.36126095056533813, Final Batch Loss: 0.16232793033123016\n",
      "Epoch 1982, Loss: 0.37007570266723633, Final Batch Loss: 0.13227945566177368\n",
      "Epoch 1983, Loss: 0.3957350254058838, Final Batch Loss: 0.21494723856449127\n",
      "Epoch 1984, Loss: 0.3569933921098709, Final Batch Loss: 0.19182425737380981\n",
      "Epoch 1985, Loss: 0.33817166090011597, Final Batch Loss: 0.189522385597229\n",
      "Epoch 1986, Loss: 0.32968537509441376, Final Batch Loss: 0.17784075438976288\n",
      "Epoch 1987, Loss: 0.33996324241161346, Final Batch Loss: 0.15662160515785217\n",
      "Epoch 1988, Loss: 0.346373051404953, Final Batch Loss: 0.19570644199848175\n",
      "Epoch 1989, Loss: 0.35963962972164154, Final Batch Loss: 0.18253199756145477\n",
      "Epoch 1990, Loss: 0.37840698659420013, Final Batch Loss: 0.17800170183181763\n",
      "Epoch 1991, Loss: 0.3548930138349533, Final Batch Loss: 0.18240812420845032\n",
      "Epoch 1992, Loss: 0.37229539453983307, Final Batch Loss: 0.22146153450012207\n",
      "Epoch 1993, Loss: 0.357561856508255, Final Batch Loss: 0.20307283103466034\n",
      "Epoch 1994, Loss: 0.3705582022666931, Final Batch Loss: 0.2075883150100708\n",
      "Epoch 1995, Loss: 0.4227293133735657, Final Batch Loss: 0.20319418609142303\n",
      "Epoch 1996, Loss: 0.38320592045783997, Final Batch Loss: 0.2215573489665985\n",
      "Epoch 1997, Loss: 0.3591117709875107, Final Batch Loss: 0.16944622993469238\n",
      "Epoch 1998, Loss: 0.3362281769514084, Final Batch Loss: 0.1748603880405426\n",
      "Epoch 1999, Loss: 0.42896275222301483, Final Batch Loss: 0.24185864627361298\n",
      "Epoch 2000, Loss: 0.40273530781269073, Final Batch Loss: 0.16844752430915833\n",
      "Epoch 2001, Loss: 0.38008740544319153, Final Batch Loss: 0.19283312559127808\n",
      "Epoch 2002, Loss: 0.3796849548816681, Final Batch Loss: 0.19152361154556274\n",
      "Epoch 2003, Loss: 0.35815979540348053, Final Batch Loss: 0.19705282151699066\n",
      "Epoch 2004, Loss: 0.34062713384628296, Final Batch Loss: 0.17101247608661652\n",
      "Epoch 2005, Loss: 0.36507628858089447, Final Batch Loss: 0.21627873182296753\n",
      "Epoch 2006, Loss: 0.3507234901189804, Final Batch Loss: 0.14935149252414703\n",
      "Epoch 2007, Loss: 0.3440936207771301, Final Batch Loss: 0.17490443587303162\n",
      "Epoch 2008, Loss: 0.3578566908836365, Final Batch Loss: 0.1663384735584259\n",
      "Epoch 2009, Loss: 0.41057589650154114, Final Batch Loss: 0.26209378242492676\n",
      "Epoch 2010, Loss: 0.39689287543296814, Final Batch Loss: 0.23557762801647186\n",
      "Epoch 2011, Loss: 0.33865076303482056, Final Batch Loss: 0.17066152393817902\n",
      "Epoch 2012, Loss: 0.3439045399427414, Final Batch Loss: 0.18827441334724426\n",
      "Epoch 2013, Loss: 0.36825744807720184, Final Batch Loss: 0.17213378846645355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2014, Loss: 0.4348597675561905, Final Batch Loss: 0.19489245116710663\n",
      "Epoch 2015, Loss: 0.37788791954517365, Final Batch Loss: 0.20342649519443512\n",
      "Epoch 2016, Loss: 0.35268159210681915, Final Batch Loss: 0.21483254432678223\n",
      "Epoch 2017, Loss: 0.3489952087402344, Final Batch Loss: 0.20192858576774597\n",
      "Epoch 2018, Loss: 0.3527197986841202, Final Batch Loss: 0.13756442070007324\n",
      "Epoch 2019, Loss: 0.3656805753707886, Final Batch Loss: 0.15654294192790985\n",
      "Epoch 2020, Loss: 0.40757231414318085, Final Batch Loss: 0.19162589311599731\n",
      "Epoch 2021, Loss: 0.35333554446697235, Final Batch Loss: 0.17276622354984283\n",
      "Epoch 2022, Loss: 0.3737320005893707, Final Batch Loss: 0.19229137897491455\n",
      "Epoch 2023, Loss: 0.3437119871377945, Final Batch Loss: 0.15265679359436035\n",
      "Epoch 2024, Loss: 0.3797507882118225, Final Batch Loss: 0.18916594982147217\n",
      "Epoch 2025, Loss: 0.3126458376646042, Final Batch Loss: 0.14712485671043396\n",
      "Epoch 2026, Loss: 0.32483892142772675, Final Batch Loss: 0.15926671028137207\n",
      "Epoch 2027, Loss: 0.3783435672521591, Final Batch Loss: 0.1987161636352539\n",
      "Epoch 2028, Loss: 0.3479364514350891, Final Batch Loss: 0.20033839344978333\n",
      "Epoch 2029, Loss: 0.3529084026813507, Final Batch Loss: 0.18246760964393616\n",
      "Epoch 2030, Loss: 0.34536878764629364, Final Batch Loss: 0.15677638351917267\n",
      "Epoch 2031, Loss: 0.37311260402202606, Final Batch Loss: 0.19079044461250305\n",
      "Epoch 2032, Loss: 0.3901873826980591, Final Batch Loss: 0.16902637481689453\n",
      "Epoch 2033, Loss: 0.3884012997150421, Final Batch Loss: 0.17755183577537537\n",
      "Epoch 2034, Loss: 0.37531115114688873, Final Batch Loss: 0.16218680143356323\n",
      "Epoch 2035, Loss: 0.3860514909029007, Final Batch Loss: 0.2030886560678482\n",
      "Epoch 2036, Loss: 0.4092579185962677, Final Batch Loss: 0.20754794776439667\n",
      "Epoch 2037, Loss: 0.435580313205719, Final Batch Loss: 0.21064941585063934\n",
      "Epoch 2038, Loss: 0.35333333909511566, Final Batch Loss: 0.17945435643196106\n",
      "Epoch 2039, Loss: 0.3510398268699646, Final Batch Loss: 0.16498705744743347\n",
      "Epoch 2040, Loss: 0.345977783203125, Final Batch Loss: 0.1542123705148697\n",
      "Epoch 2041, Loss: 0.33295756578445435, Final Batch Loss: 0.1744261085987091\n",
      "Epoch 2042, Loss: 0.35374169051647186, Final Batch Loss: 0.17761334776878357\n",
      "Epoch 2043, Loss: 0.3281124085187912, Final Batch Loss: 0.17519764602184296\n",
      "Epoch 2044, Loss: 0.34075914323329926, Final Batch Loss: 0.16417883336544037\n",
      "Epoch 2045, Loss: 0.38195885717868805, Final Batch Loss: 0.20927594602108002\n",
      "Epoch 2046, Loss: 0.3388045132160187, Final Batch Loss: 0.14510008692741394\n",
      "Epoch 2047, Loss: 0.3453999012708664, Final Batch Loss: 0.1588650494813919\n",
      "Epoch 2048, Loss: 0.3299223482608795, Final Batch Loss: 0.1783364862203598\n",
      "Epoch 2049, Loss: 0.3477949798107147, Final Batch Loss: 0.18111073970794678\n",
      "Epoch 2050, Loss: 0.39639949798583984, Final Batch Loss: 0.15204815566539764\n",
      "Epoch 2051, Loss: 0.378975048661232, Final Batch Loss: 0.18090936541557312\n",
      "Epoch 2052, Loss: 0.366012766957283, Final Batch Loss: 0.20579074323177338\n",
      "Epoch 2053, Loss: 0.33527444303035736, Final Batch Loss: 0.1499999463558197\n",
      "Epoch 2054, Loss: 0.410224124789238, Final Batch Loss: 0.20064999163150787\n",
      "Epoch 2055, Loss: 0.3636641204357147, Final Batch Loss: 0.14744499325752258\n",
      "Epoch 2056, Loss: 0.3639834225177765, Final Batch Loss: 0.19342871010303497\n",
      "Epoch 2057, Loss: 0.3232101947069168, Final Batch Loss: 0.16636142134666443\n",
      "Epoch 2058, Loss: 0.32148024439811707, Final Batch Loss: 0.14957240223884583\n",
      "Epoch 2059, Loss: 0.3873572498559952, Final Batch Loss: 0.19615866243839264\n",
      "Epoch 2060, Loss: 0.3331141173839569, Final Batch Loss: 0.13640263676643372\n",
      "Epoch 2061, Loss: 0.40944239497184753, Final Batch Loss: 0.19000299274921417\n",
      "Epoch 2062, Loss: 0.3971627652645111, Final Batch Loss: 0.18705596029758453\n",
      "Epoch 2063, Loss: 0.3472703546285629, Final Batch Loss: 0.15509934723377228\n",
      "Epoch 2064, Loss: 0.3782952129840851, Final Batch Loss: 0.17396052181720734\n",
      "Epoch 2065, Loss: 0.3714646100997925, Final Batch Loss: 0.16882675886154175\n",
      "Epoch 2066, Loss: 0.35569359362125397, Final Batch Loss: 0.1840793937444687\n",
      "Epoch 2067, Loss: 0.3434775322675705, Final Batch Loss: 0.1717987358570099\n",
      "Epoch 2068, Loss: 0.33558741211891174, Final Batch Loss: 0.19512268900871277\n",
      "Epoch 2069, Loss: 0.3907843232154846, Final Batch Loss: 0.1755315214395523\n",
      "Epoch 2070, Loss: 0.3436352163553238, Final Batch Loss: 0.17917148768901825\n",
      "Epoch 2071, Loss: 0.3564099222421646, Final Batch Loss: 0.15095865726470947\n",
      "Epoch 2072, Loss: 0.35266827046871185, Final Batch Loss: 0.16414617002010345\n",
      "Epoch 2073, Loss: 0.3886175602674484, Final Batch Loss: 0.16752153635025024\n",
      "Epoch 2074, Loss: 0.35691045224666595, Final Batch Loss: 0.173281729221344\n",
      "Epoch 2075, Loss: 0.3533356040716171, Final Batch Loss: 0.18651625514030457\n",
      "Epoch 2076, Loss: 0.3767029643058777, Final Batch Loss: 0.1670723408460617\n",
      "Epoch 2077, Loss: 0.41807280480861664, Final Batch Loss: 0.206265389919281\n",
      "Epoch 2078, Loss: 0.3895256668329239, Final Batch Loss: 0.1896505206823349\n",
      "Epoch 2079, Loss: 0.36126114428043365, Final Batch Loss: 0.18781153857707977\n",
      "Epoch 2080, Loss: 0.3289370536804199, Final Batch Loss: 0.15390737354755402\n",
      "Epoch 2081, Loss: 0.38225027918815613, Final Batch Loss: 0.16144083440303802\n",
      "Epoch 2082, Loss: 0.402437299489975, Final Batch Loss: 0.20477604866027832\n",
      "Epoch 2083, Loss: 0.37455078959465027, Final Batch Loss: 0.19298656284809113\n",
      "Epoch 2084, Loss: 0.33838219940662384, Final Batch Loss: 0.16941070556640625\n",
      "Epoch 2085, Loss: 0.34171733260154724, Final Batch Loss: 0.14522181451320648\n",
      "Epoch 2086, Loss: 0.30872730910778046, Final Batch Loss: 0.14445194602012634\n",
      "Epoch 2087, Loss: 0.40014202892780304, Final Batch Loss: 0.15917529165744781\n",
      "Epoch 2088, Loss: 0.35808679461479187, Final Batch Loss: 0.16604560613632202\n",
      "Epoch 2089, Loss: 0.3591340333223343, Final Batch Loss: 0.15047061443328857\n",
      "Epoch 2090, Loss: 0.3303816318511963, Final Batch Loss: 0.16931380331516266\n",
      "Epoch 2091, Loss: 0.34611108899116516, Final Batch Loss: 0.18470732867717743\n",
      "Epoch 2092, Loss: 0.3680513948202133, Final Batch Loss: 0.17555461823940277\n",
      "Epoch 2093, Loss: 0.35922881960868835, Final Batch Loss: 0.1888965219259262\n",
      "Epoch 2094, Loss: 0.3753894865512848, Final Batch Loss: 0.17713886499404907\n",
      "Epoch 2095, Loss: 0.40941523015499115, Final Batch Loss: 0.1827811300754547\n",
      "Epoch 2096, Loss: 0.34331265091896057, Final Batch Loss: 0.18253132700920105\n",
      "Epoch 2097, Loss: 0.408903107047081, Final Batch Loss: 0.20934845507144928\n",
      "Epoch 2098, Loss: 0.3765043020248413, Final Batch Loss: 0.20449885725975037\n",
      "Epoch 2099, Loss: 0.34531180560588837, Final Batch Loss: 0.15945254266262054\n",
      "Epoch 2100, Loss: 0.3588276207447052, Final Batch Loss: 0.1832457333803177\n",
      "Epoch 2101, Loss: 0.3583092838525772, Final Batch Loss: 0.20886467397212982\n",
      "Epoch 2102, Loss: 0.37976278364658356, Final Batch Loss: 0.2281767725944519\n",
      "Epoch 2103, Loss: 0.41039636731147766, Final Batch Loss: 0.18324971199035645\n",
      "Epoch 2104, Loss: 0.3609045594930649, Final Batch Loss: 0.18747596442699432\n",
      "Epoch 2105, Loss: 0.33477209508419037, Final Batch Loss: 0.1705896109342575\n",
      "Epoch 2106, Loss: 0.3574768900871277, Final Batch Loss: 0.15223604440689087\n",
      "Epoch 2107, Loss: 0.3310464173555374, Final Batch Loss: 0.16547057032585144\n",
      "Epoch 2108, Loss: 0.3305361866950989, Final Batch Loss: 0.16664189100265503\n",
      "Epoch 2109, Loss: 0.32856258749961853, Final Batch Loss: 0.14837010204792023\n",
      "Epoch 2110, Loss: 0.396522656083107, Final Batch Loss: 0.19657732546329498\n",
      "Epoch 2111, Loss: 0.3665720671415329, Final Batch Loss: 0.196121945977211\n",
      "Epoch 2112, Loss: 0.37574295699596405, Final Batch Loss: 0.16951590776443481\n",
      "Epoch 2113, Loss: 0.3389585018157959, Final Batch Loss: 0.1579696238040924\n",
      "Epoch 2114, Loss: 0.3585953265428543, Final Batch Loss: 0.20715361833572388\n",
      "Epoch 2115, Loss: 0.34365028142929077, Final Batch Loss: 0.15891675651073456\n",
      "Epoch 2116, Loss: 0.3523491472005844, Final Batch Loss: 0.16716226935386658\n",
      "Epoch 2117, Loss: 0.376206710934639, Final Batch Loss: 0.16432620584964752\n",
      "Epoch 2118, Loss: 0.374281644821167, Final Batch Loss: 0.18843331933021545\n",
      "Epoch 2119, Loss: 0.33699271082878113, Final Batch Loss: 0.15183337032794952\n",
      "Epoch 2120, Loss: 0.3861590176820755, Final Batch Loss: 0.14696305990219116\n",
      "Epoch 2121, Loss: 0.4037572294473648, Final Batch Loss: 0.2107895463705063\n",
      "Epoch 2122, Loss: 0.36620987951755524, Final Batch Loss: 0.1859898865222931\n",
      "Epoch 2123, Loss: 0.39155803620815277, Final Batch Loss: 0.17941001057624817\n",
      "Epoch 2124, Loss: 0.3612661361694336, Final Batch Loss: 0.16303366422653198\n",
      "Epoch 2125, Loss: 0.3544962853193283, Final Batch Loss: 0.1771787405014038\n",
      "Epoch 2126, Loss: 0.35036857426166534, Final Batch Loss: 0.16638347506523132\n",
      "Epoch 2127, Loss: 0.3276478350162506, Final Batch Loss: 0.16007275879383087\n",
      "Epoch 2128, Loss: 0.3459990471601486, Final Batch Loss: 0.1865127831697464\n",
      "Epoch 2129, Loss: 0.41546475887298584, Final Batch Loss: 0.18599089980125427\n",
      "Epoch 2130, Loss: 0.3089195042848587, Final Batch Loss: 0.15075939893722534\n",
      "Epoch 2131, Loss: 0.3528551161289215, Final Batch Loss: 0.15847280621528625\n",
      "Epoch 2132, Loss: 0.3652765452861786, Final Batch Loss: 0.1914675384759903\n",
      "Epoch 2133, Loss: 0.3431929051876068, Final Batch Loss: 0.1792505383491516\n",
      "Epoch 2134, Loss: 0.3494270592927933, Final Batch Loss: 0.17880593240261078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2135, Loss: 0.36709851026535034, Final Batch Loss: 0.17780090868473053\n",
      "Epoch 2136, Loss: 0.3333622068166733, Final Batch Loss: 0.17102164030075073\n",
      "Epoch 2137, Loss: 0.3923339694738388, Final Batch Loss: 0.1694827377796173\n",
      "Epoch 2138, Loss: 0.34031452238559723, Final Batch Loss: 0.16424345970153809\n",
      "Epoch 2139, Loss: 0.38069814443588257, Final Batch Loss: 0.19729982316493988\n",
      "Epoch 2140, Loss: 0.32076942920684814, Final Batch Loss: 0.14505361020565033\n",
      "Epoch 2141, Loss: 0.33321601152420044, Final Batch Loss: 0.140965074300766\n",
      "Epoch 2142, Loss: 0.29855576157569885, Final Batch Loss: 0.15102770924568176\n",
      "Epoch 2143, Loss: 0.35173650085926056, Final Batch Loss: 0.1747668832540512\n",
      "Epoch 2144, Loss: 0.3631788492202759, Final Batch Loss: 0.19345751404762268\n",
      "Epoch 2145, Loss: 0.36265677213668823, Final Batch Loss: 0.17791172862052917\n",
      "Epoch 2146, Loss: 0.36622224748134613, Final Batch Loss: 0.18071532249450684\n",
      "Epoch 2147, Loss: 0.40554966032505035, Final Batch Loss: 0.23336324095726013\n",
      "Epoch 2148, Loss: 0.40047144889831543, Final Batch Loss: 0.18053120374679565\n",
      "Epoch 2149, Loss: 0.38519930839538574, Final Batch Loss: 0.16948983073234558\n",
      "Epoch 2150, Loss: 0.37397076189517975, Final Batch Loss: 0.1662449985742569\n",
      "Epoch 2151, Loss: 0.36981964111328125, Final Batch Loss: 0.15327250957489014\n",
      "Epoch 2152, Loss: 0.3810320496559143, Final Batch Loss: 0.20065642893314362\n",
      "Epoch 2153, Loss: 0.36025771498680115, Final Batch Loss: 0.16172456741333008\n",
      "Epoch 2154, Loss: 0.364017978310585, Final Batch Loss: 0.21335090696811676\n",
      "Epoch 2155, Loss: 0.34384752810001373, Final Batch Loss: 0.15521562099456787\n",
      "Epoch 2156, Loss: 0.40376636385917664, Final Batch Loss: 0.23848330974578857\n",
      "Epoch 2157, Loss: 0.361354798078537, Final Batch Loss: 0.18368972837924957\n",
      "Epoch 2158, Loss: 0.40055736899375916, Final Batch Loss: 0.18763820827007294\n",
      "Epoch 2159, Loss: 0.3449130952358246, Final Batch Loss: 0.1740804761648178\n",
      "Epoch 2160, Loss: 0.385013610124588, Final Batch Loss: 0.1346392035484314\n",
      "Epoch 2161, Loss: 0.32391658425331116, Final Batch Loss: 0.14955194294452667\n",
      "Epoch 2162, Loss: 0.3697006404399872, Final Batch Loss: 0.16231368482112885\n",
      "Epoch 2163, Loss: 0.38571617007255554, Final Batch Loss: 0.19088543951511383\n",
      "Epoch 2164, Loss: 0.34326571226119995, Final Batch Loss: 0.18779364228248596\n",
      "Epoch 2165, Loss: 0.3552928864955902, Final Batch Loss: 0.1720161736011505\n",
      "Epoch 2166, Loss: 0.32721900939941406, Final Batch Loss: 0.18182319402694702\n",
      "Epoch 2167, Loss: 0.3683377653360367, Final Batch Loss: 0.19782383739948273\n",
      "Epoch 2168, Loss: 0.31467363238334656, Final Batch Loss: 0.19281084835529327\n",
      "Epoch 2169, Loss: 0.3944658041000366, Final Batch Loss: 0.16992132365703583\n",
      "Epoch 2170, Loss: 0.36677321791648865, Final Batch Loss: 0.17008087038993835\n",
      "Epoch 2171, Loss: 0.34058429300785065, Final Batch Loss: 0.19580203294754028\n",
      "Epoch 2172, Loss: 0.32983914017677307, Final Batch Loss: 0.1668548882007599\n",
      "Epoch 2173, Loss: 0.29234495759010315, Final Batch Loss: 0.1307506412267685\n",
      "Epoch 2174, Loss: 0.34798429906368256, Final Batch Loss: 0.1720350831747055\n",
      "Epoch 2175, Loss: 0.3524197190999985, Final Batch Loss: 0.19086237251758575\n",
      "Epoch 2176, Loss: 0.36081573367118835, Final Batch Loss: 0.19430001080036163\n",
      "Epoch 2177, Loss: 0.3682023882865906, Final Batch Loss: 0.1848989874124527\n",
      "Epoch 2178, Loss: 0.33738256990909576, Final Batch Loss: 0.1911453902721405\n",
      "Epoch 2179, Loss: 0.37452124059200287, Final Batch Loss: 0.15796756744384766\n",
      "Epoch 2180, Loss: 0.3290518969297409, Final Batch Loss: 0.17626209557056427\n",
      "Epoch 2181, Loss: 0.32743367552757263, Final Batch Loss: 0.172251895070076\n",
      "Epoch 2182, Loss: 0.3837849199771881, Final Batch Loss: 0.19160886108875275\n",
      "Epoch 2183, Loss: 0.3501427620649338, Final Batch Loss: 0.17911650240421295\n",
      "Epoch 2184, Loss: 0.3616652339696884, Final Batch Loss: 0.20773836970329285\n",
      "Epoch 2185, Loss: 0.3324398100376129, Final Batch Loss: 0.17414477467536926\n",
      "Epoch 2186, Loss: 0.3432636857032776, Final Batch Loss: 0.18800950050354004\n",
      "Epoch 2187, Loss: 0.3622460216283798, Final Batch Loss: 0.18278935551643372\n",
      "Epoch 2188, Loss: 0.32911112904548645, Final Batch Loss: 0.21553896367549896\n",
      "Epoch 2189, Loss: 0.3343851566314697, Final Batch Loss: 0.18396638333797455\n",
      "Epoch 2190, Loss: 0.3688801974058151, Final Batch Loss: 0.19974444806575775\n",
      "Epoch 2191, Loss: 0.31789688766002655, Final Batch Loss: 0.14723758399486542\n",
      "Epoch 2192, Loss: 0.3573559671640396, Final Batch Loss: 0.16321390867233276\n",
      "Epoch 2193, Loss: 0.35265742242336273, Final Batch Loss: 0.19376935064792633\n",
      "Epoch 2194, Loss: 0.34360527992248535, Final Batch Loss: 0.1455221027135849\n",
      "Epoch 2195, Loss: 0.3293691575527191, Final Batch Loss: 0.15825514495372772\n",
      "Epoch 2196, Loss: 0.3448280841112137, Final Batch Loss: 0.16289643943309784\n",
      "Epoch 2197, Loss: 0.37698890268802643, Final Batch Loss: 0.15914484858512878\n",
      "Epoch 2198, Loss: 0.3611215204000473, Final Batch Loss: 0.18104682862758636\n",
      "Epoch 2199, Loss: 0.3874340057373047, Final Batch Loss: 0.18162551522254944\n",
      "Epoch 2200, Loss: 0.393855020403862, Final Batch Loss: 0.16757342219352722\n",
      "Epoch 2201, Loss: 0.3661959171295166, Final Batch Loss: 0.19681799411773682\n",
      "Epoch 2202, Loss: 0.35559244453907013, Final Batch Loss: 0.18806232511997223\n",
      "Epoch 2203, Loss: 0.34493276476860046, Final Batch Loss: 0.16809359192848206\n",
      "Epoch 2204, Loss: 0.40035246312618256, Final Batch Loss: 0.17552310228347778\n",
      "Epoch 2205, Loss: 0.3086186349391937, Final Batch Loss: 0.14880815148353577\n",
      "Epoch 2206, Loss: 0.3379630744457245, Final Batch Loss: 0.15504854917526245\n",
      "Epoch 2207, Loss: 0.37598416209220886, Final Batch Loss: 0.18842223286628723\n",
      "Epoch 2208, Loss: 0.38544540107250214, Final Batch Loss: 0.18993011116981506\n",
      "Epoch 2209, Loss: 0.3153610974550247, Final Batch Loss: 0.14216086268424988\n",
      "Epoch 2210, Loss: 0.35967594385147095, Final Batch Loss: 0.1784004271030426\n",
      "Epoch 2211, Loss: 0.32881349325180054, Final Batch Loss: 0.1692744493484497\n",
      "Epoch 2212, Loss: 0.33733169734477997, Final Batch Loss: 0.17651201784610748\n",
      "Epoch 2213, Loss: 0.3079100251197815, Final Batch Loss: 0.13286332786083221\n",
      "Epoch 2214, Loss: 0.352047324180603, Final Batch Loss: 0.18574392795562744\n",
      "Epoch 2215, Loss: 0.3930974453687668, Final Batch Loss: 0.19586533308029175\n",
      "Epoch 2216, Loss: 0.35704414546489716, Final Batch Loss: 0.17841117084026337\n",
      "Epoch 2217, Loss: 0.37064220011234283, Final Batch Loss: 0.1836540400981903\n",
      "Epoch 2218, Loss: 0.33760683238506317, Final Batch Loss: 0.1729438602924347\n",
      "Epoch 2219, Loss: 0.3225194662809372, Final Batch Loss: 0.16948452591896057\n",
      "Epoch 2220, Loss: 0.3565075844526291, Final Batch Loss: 0.19740168750286102\n",
      "Epoch 2221, Loss: 0.37709765136241913, Final Batch Loss: 0.18215137720108032\n",
      "Epoch 2222, Loss: 0.332605704665184, Final Batch Loss: 0.12966997921466827\n",
      "Epoch 2223, Loss: 0.3528098911046982, Final Batch Loss: 0.19486883282661438\n",
      "Epoch 2224, Loss: 0.34172144532203674, Final Batch Loss: 0.13936229050159454\n",
      "Epoch 2225, Loss: 0.3570691645145416, Final Batch Loss: 0.16041883826255798\n",
      "Epoch 2226, Loss: 0.38298939168453217, Final Batch Loss: 0.18212534487247467\n",
      "Epoch 2227, Loss: 0.33554835617542267, Final Batch Loss: 0.15923768281936646\n",
      "Epoch 2228, Loss: 0.3360707312822342, Final Batch Loss: 0.14799974858760834\n",
      "Epoch 2229, Loss: 0.34786202013492584, Final Batch Loss: 0.16162239015102386\n",
      "Epoch 2230, Loss: 0.33621180057525635, Final Batch Loss: 0.15745534002780914\n",
      "Epoch 2231, Loss: 0.37451171875, Final Batch Loss: 0.20132295787334442\n",
      "Epoch 2232, Loss: 0.39733971655368805, Final Batch Loss: 0.15944993495941162\n",
      "Epoch 2233, Loss: 0.34236136078834534, Final Batch Loss: 0.18853701651096344\n",
      "Epoch 2234, Loss: 0.3520963340997696, Final Batch Loss: 0.1828838288784027\n",
      "Epoch 2235, Loss: 0.34011103212833405, Final Batch Loss: 0.17773878574371338\n",
      "Epoch 2236, Loss: 0.35760296881198883, Final Batch Loss: 0.18661874532699585\n",
      "Epoch 2237, Loss: 0.35224542021751404, Final Batch Loss: 0.17034567892551422\n",
      "Epoch 2238, Loss: 0.3502289205789566, Final Batch Loss: 0.13210506737232208\n",
      "Epoch 2239, Loss: 0.3985709249973297, Final Batch Loss: 0.19530875980854034\n",
      "Epoch 2240, Loss: 0.3227769136428833, Final Batch Loss: 0.16492994129657745\n",
      "Epoch 2241, Loss: 0.37252767384052277, Final Batch Loss: 0.17383189499378204\n",
      "Epoch 2242, Loss: 0.3903290778398514, Final Batch Loss: 0.2120034545660019\n",
      "Epoch 2243, Loss: 0.3386728763580322, Final Batch Loss: 0.15875332057476044\n",
      "Epoch 2244, Loss: 0.3503418415784836, Final Batch Loss: 0.22162754833698273\n",
      "Epoch 2245, Loss: 0.32517826557159424, Final Batch Loss: 0.15453214943408966\n",
      "Epoch 2246, Loss: 0.3529311418533325, Final Batch Loss: 0.20229162275791168\n",
      "Epoch 2247, Loss: 0.3569575548171997, Final Batch Loss: 0.20203395187854767\n",
      "Epoch 2248, Loss: 0.35269100964069366, Final Batch Loss: 0.18366053700447083\n",
      "Epoch 2249, Loss: 0.35312992334365845, Final Batch Loss: 0.17064236104488373\n",
      "Epoch 2250, Loss: 0.358899787068367, Final Batch Loss: 0.17624694108963013\n",
      "Epoch 2251, Loss: 0.3412388861179352, Final Batch Loss: 0.13747915625572205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2252, Loss: 0.3232986629009247, Final Batch Loss: 0.15614652633666992\n",
      "Epoch 2253, Loss: 0.319175660610199, Final Batch Loss: 0.15750807523727417\n",
      "Epoch 2254, Loss: 0.3335191011428833, Final Batch Loss: 0.17386439442634583\n",
      "Epoch 2255, Loss: 0.3373425304889679, Final Batch Loss: 0.1729801595211029\n",
      "Epoch 2256, Loss: 0.3912542164325714, Final Batch Loss: 0.1988471895456314\n",
      "Epoch 2257, Loss: 0.3418321758508682, Final Batch Loss: 0.18404217064380646\n",
      "Epoch 2258, Loss: 0.3273097276687622, Final Batch Loss: 0.14830270409584045\n",
      "Epoch 2259, Loss: 0.3058701306581497, Final Batch Loss: 0.16946358978748322\n",
      "Epoch 2260, Loss: 0.3192906230688095, Final Batch Loss: 0.17064201831817627\n",
      "Epoch 2261, Loss: 0.36057549715042114, Final Batch Loss: 0.17128726840019226\n",
      "Epoch 2262, Loss: 0.389249786734581, Final Batch Loss: 0.17523695528507233\n",
      "Epoch 2263, Loss: 0.3446449041366577, Final Batch Loss: 0.18784622848033905\n",
      "Epoch 2264, Loss: 0.35837920010089874, Final Batch Loss: 0.1734316498041153\n",
      "Epoch 2265, Loss: 0.34117700159549713, Final Batch Loss: 0.17687328159809113\n",
      "Epoch 2266, Loss: 0.32358303666114807, Final Batch Loss: 0.1453370451927185\n",
      "Epoch 2267, Loss: 0.4037817269563675, Final Batch Loss: 0.18593020737171173\n",
      "Epoch 2268, Loss: 0.3714561015367508, Final Batch Loss: 0.1783566027879715\n",
      "Epoch 2269, Loss: 0.38686417043209076, Final Batch Loss: 0.22134117782115936\n",
      "Epoch 2270, Loss: 0.2934354543685913, Final Batch Loss: 0.14156390726566315\n",
      "Epoch 2271, Loss: 0.3445388972759247, Final Batch Loss: 0.16768932342529297\n",
      "Epoch 2272, Loss: 0.33450621366500854, Final Batch Loss: 0.16221459209918976\n",
      "Epoch 2273, Loss: 0.4310135990381241, Final Batch Loss: 0.16972433030605316\n",
      "Epoch 2274, Loss: 0.4037020206451416, Final Batch Loss: 0.24589857459068298\n",
      "Epoch 2275, Loss: 0.34614116698503494, Final Batch Loss: 0.11973274499177933\n",
      "Epoch 2276, Loss: 0.4077669680118561, Final Batch Loss: 0.14969295263290405\n",
      "Epoch 2277, Loss: 0.33629539608955383, Final Batch Loss: 0.17635509371757507\n",
      "Epoch 2278, Loss: 0.38357841968536377, Final Batch Loss: 0.16230325400829315\n",
      "Epoch 2279, Loss: 0.367220476269722, Final Batch Loss: 0.1740448921918869\n",
      "Epoch 2280, Loss: 0.339795783162117, Final Batch Loss: 0.16284604370594025\n",
      "Epoch 2281, Loss: 0.3094491511583328, Final Batch Loss: 0.17262916266918182\n",
      "Epoch 2282, Loss: 0.3633881062269211, Final Batch Loss: 0.18788063526153564\n",
      "Epoch 2283, Loss: 0.3240801990032196, Final Batch Loss: 0.19284801185131073\n",
      "Epoch 2284, Loss: 0.3537992238998413, Final Batch Loss: 0.20168831944465637\n",
      "Epoch 2285, Loss: 0.3263673931360245, Final Batch Loss: 0.13589197397232056\n",
      "Epoch 2286, Loss: 0.4232347011566162, Final Batch Loss: 0.2446618527173996\n",
      "Epoch 2287, Loss: 0.3437860906124115, Final Batch Loss: 0.15158511698246002\n",
      "Epoch 2288, Loss: 0.3461403399705887, Final Batch Loss: 0.19069740176200867\n",
      "Epoch 2289, Loss: 0.42378945648670197, Final Batch Loss: 0.23279622197151184\n",
      "Epoch 2290, Loss: 0.3675719350576401, Final Batch Loss: 0.21742214262485504\n",
      "Epoch 2291, Loss: 0.35357581079006195, Final Batch Loss: 0.15027225017547607\n",
      "Epoch 2292, Loss: 0.3731868118047714, Final Batch Loss: 0.1964426338672638\n",
      "Epoch 2293, Loss: 0.29927653074264526, Final Batch Loss: 0.15180926024913788\n",
      "Epoch 2294, Loss: 0.376279816031456, Final Batch Loss: 0.198245108127594\n",
      "Epoch 2295, Loss: 0.3704962581396103, Final Batch Loss: 0.17426328361034393\n",
      "Epoch 2296, Loss: 0.335582971572876, Final Batch Loss: 0.18153749406337738\n",
      "Epoch 2297, Loss: 0.3028329461812973, Final Batch Loss: 0.13800105452537537\n",
      "Epoch 2298, Loss: 0.359225332736969, Final Batch Loss: 0.1809198409318924\n",
      "Epoch 2299, Loss: 0.3368108719587326, Final Batch Loss: 0.16847828030586243\n",
      "Epoch 2300, Loss: 0.3853166252374649, Final Batch Loss: 0.1888284534215927\n",
      "Epoch 2301, Loss: 0.318316787481308, Final Batch Loss: 0.17022617161273956\n",
      "Epoch 2302, Loss: 0.32127752900123596, Final Batch Loss: 0.14742761850357056\n",
      "Epoch 2303, Loss: 0.3262157142162323, Final Batch Loss: 0.1949557065963745\n",
      "Epoch 2304, Loss: 0.33820389211177826, Final Batch Loss: 0.18773971498012543\n",
      "Epoch 2305, Loss: 0.33140650391578674, Final Batch Loss: 0.16714558005332947\n",
      "Epoch 2306, Loss: 0.3759762793779373, Final Batch Loss: 0.22145669162273407\n",
      "Epoch 2307, Loss: 0.38987216353416443, Final Batch Loss: 0.2080710381269455\n",
      "Epoch 2308, Loss: 0.39987169206142426, Final Batch Loss: 0.16247962415218353\n",
      "Epoch 2309, Loss: 0.31574268639087677, Final Batch Loss: 0.1887895166873932\n",
      "Epoch 2310, Loss: 0.32359422743320465, Final Batch Loss: 0.178360715508461\n",
      "Epoch 2311, Loss: 0.3579227328300476, Final Batch Loss: 0.16132427752017975\n",
      "Epoch 2312, Loss: 0.3099168837070465, Final Batch Loss: 0.14669832587242126\n",
      "Epoch 2313, Loss: 0.33631159365177155, Final Batch Loss: 0.18117408454418182\n",
      "Epoch 2314, Loss: 0.33626240491867065, Final Batch Loss: 0.16552749276161194\n",
      "Epoch 2315, Loss: 0.3469657748937607, Final Batch Loss: 0.14407606422901154\n",
      "Epoch 2316, Loss: 0.35089853405952454, Final Batch Loss: 0.1732521802186966\n",
      "Epoch 2317, Loss: 0.34173156321048737, Final Batch Loss: 0.16827024519443512\n",
      "Epoch 2318, Loss: 0.3700111359357834, Final Batch Loss: 0.21013085544109344\n",
      "Epoch 2319, Loss: 0.32644812762737274, Final Batch Loss: 0.1609524041414261\n",
      "Epoch 2320, Loss: 0.33946189284324646, Final Batch Loss: 0.16475141048431396\n",
      "Epoch 2321, Loss: 0.4198852479457855, Final Batch Loss: 0.21243080496788025\n",
      "Epoch 2322, Loss: 0.3302094340324402, Final Batch Loss: 0.1356963962316513\n",
      "Epoch 2323, Loss: 0.39407651126384735, Final Batch Loss: 0.17855331301689148\n",
      "Epoch 2324, Loss: 0.32810521125793457, Final Batch Loss: 0.16716939210891724\n",
      "Epoch 2325, Loss: 0.3547426462173462, Final Batch Loss: 0.16819775104522705\n",
      "Epoch 2326, Loss: 0.37186098098754883, Final Batch Loss: 0.16541963815689087\n",
      "Epoch 2327, Loss: 0.42188481986522675, Final Batch Loss: 0.23001113533973694\n",
      "Epoch 2328, Loss: 0.3267764002084732, Final Batch Loss: 0.15555347502231598\n",
      "Epoch 2329, Loss: 0.3385765105485916, Final Batch Loss: 0.18161311745643616\n",
      "Epoch 2330, Loss: 0.315417155623436, Final Batch Loss: 0.14957930147647858\n",
      "Epoch 2331, Loss: 0.32064536213874817, Final Batch Loss: 0.16049523651599884\n",
      "Epoch 2332, Loss: 0.3870849609375, Final Batch Loss: 0.19112712144851685\n",
      "Epoch 2333, Loss: 0.3626481145620346, Final Batch Loss: 0.17404475808143616\n",
      "Epoch 2334, Loss: 0.36317119002342224, Final Batch Loss: 0.16737402975559235\n",
      "Epoch 2335, Loss: 0.3634978383779526, Final Batch Loss: 0.17561030387878418\n",
      "Epoch 2336, Loss: 0.3120676279067993, Final Batch Loss: 0.15571783483028412\n",
      "Epoch 2337, Loss: 0.33900371193885803, Final Batch Loss: 0.1466231644153595\n",
      "Epoch 2338, Loss: 0.3333260864019394, Final Batch Loss: 0.18222802877426147\n",
      "Epoch 2339, Loss: 0.37360623478889465, Final Batch Loss: 0.19656698405742645\n",
      "Epoch 2340, Loss: 0.3573566675186157, Final Batch Loss: 0.17698359489440918\n",
      "Epoch 2341, Loss: 0.3640821576118469, Final Batch Loss: 0.21789857745170593\n",
      "Epoch 2342, Loss: 0.3596496880054474, Final Batch Loss: 0.2103109210729599\n",
      "Epoch 2343, Loss: 0.3512320816516876, Final Batch Loss: 0.1910708248615265\n",
      "Epoch 2344, Loss: 0.3534974306821823, Final Batch Loss: 0.19490094482898712\n",
      "Epoch 2345, Loss: 0.3776773065328598, Final Batch Loss: 0.21993669867515564\n",
      "Epoch 2346, Loss: 0.38208959996700287, Final Batch Loss: 0.1971849948167801\n",
      "Epoch 2347, Loss: 0.3288256824016571, Final Batch Loss: 0.12656331062316895\n",
      "Epoch 2348, Loss: 0.32291851937770844, Final Batch Loss: 0.16034536063671112\n",
      "Epoch 2349, Loss: 0.3220170587301254, Final Batch Loss: 0.14401090145111084\n",
      "Epoch 2350, Loss: 0.3860081732273102, Final Batch Loss: 0.19726914167404175\n",
      "Epoch 2351, Loss: 0.35028985142707825, Final Batch Loss: 0.18486511707305908\n",
      "Epoch 2352, Loss: 0.34354354441165924, Final Batch Loss: 0.17685727775096893\n",
      "Epoch 2353, Loss: 0.3327648192644119, Final Batch Loss: 0.15872125327587128\n",
      "Epoch 2354, Loss: 0.3219098299741745, Final Batch Loss: 0.1496392786502838\n",
      "Epoch 2355, Loss: 0.3820579797029495, Final Batch Loss: 0.21870437264442444\n",
      "Epoch 2356, Loss: 0.3046967536211014, Final Batch Loss: 0.13832035660743713\n",
      "Epoch 2357, Loss: 0.3554302603006363, Final Batch Loss: 0.2038603127002716\n",
      "Epoch 2358, Loss: 0.3373376280069351, Final Batch Loss: 0.17451630532741547\n",
      "Epoch 2359, Loss: 0.3131854385137558, Final Batch Loss: 0.14801329374313354\n",
      "Epoch 2360, Loss: 0.4134581536054611, Final Batch Loss: 0.21394479274749756\n",
      "Epoch 2361, Loss: 0.3771861344575882, Final Batch Loss: 0.25458231568336487\n",
      "Epoch 2362, Loss: 0.33872292935848236, Final Batch Loss: 0.17659497261047363\n",
      "Epoch 2363, Loss: 0.36761339008808136, Final Batch Loss: 0.184973806142807\n",
      "Epoch 2364, Loss: 0.360631987452507, Final Batch Loss: 0.15309956669807434\n",
      "Epoch 2365, Loss: 0.35805743932724, Final Batch Loss: 0.2082599401473999\n",
      "Epoch 2366, Loss: 0.3667936474084854, Final Batch Loss: 0.17621217668056488\n",
      "Epoch 2367, Loss: 0.3341406136751175, Final Batch Loss: 0.16322700679302216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2368, Loss: 0.4028596132993698, Final Batch Loss: 0.1816004067659378\n",
      "Epoch 2369, Loss: 0.3431978225708008, Final Batch Loss: 0.18971143662929535\n",
      "Epoch 2370, Loss: 0.31579597294330597, Final Batch Loss: 0.1552090048789978\n",
      "Epoch 2371, Loss: 0.3489721417427063, Final Batch Loss: 0.20033393800258636\n",
      "Epoch 2372, Loss: 0.3104066848754883, Final Batch Loss: 0.12969505786895752\n",
      "Epoch 2373, Loss: 0.3310583829879761, Final Batch Loss: 0.1453147828578949\n",
      "Epoch 2374, Loss: 0.31437037885189056, Final Batch Loss: 0.15644600987434387\n",
      "Epoch 2375, Loss: 0.42812855541706085, Final Batch Loss: 0.2738671600818634\n",
      "Epoch 2376, Loss: 0.3255939185619354, Final Batch Loss: 0.16880208253860474\n",
      "Epoch 2377, Loss: 0.3595493286848068, Final Batch Loss: 0.1608668863773346\n",
      "Epoch 2378, Loss: 0.3660759776830673, Final Batch Loss: 0.1959189474582672\n",
      "Epoch 2379, Loss: 0.3383437842130661, Final Batch Loss: 0.16408611834049225\n",
      "Epoch 2380, Loss: 0.3967122286558151, Final Batch Loss: 0.20580941438674927\n",
      "Epoch 2381, Loss: 0.3429356813430786, Final Batch Loss: 0.15023455023765564\n",
      "Epoch 2382, Loss: 0.37203747034072876, Final Batch Loss: 0.1748528927564621\n",
      "Epoch 2383, Loss: 0.3344659060239792, Final Batch Loss: 0.17497749626636505\n",
      "Epoch 2384, Loss: 0.3264007866382599, Final Batch Loss: 0.18790249526500702\n",
      "Epoch 2385, Loss: 0.3369997441768646, Final Batch Loss: 0.15262800455093384\n",
      "Epoch 2386, Loss: 0.3094094693660736, Final Batch Loss: 0.13671696186065674\n",
      "Epoch 2387, Loss: 0.2991396188735962, Final Batch Loss: 0.13899549841880798\n",
      "Epoch 2388, Loss: 0.3458390384912491, Final Batch Loss: 0.14285415410995483\n",
      "Epoch 2389, Loss: 0.3693762868642807, Final Batch Loss: 0.19005973637104034\n",
      "Epoch 2390, Loss: 0.3036874532699585, Final Batch Loss: 0.12824511528015137\n",
      "Epoch 2391, Loss: 0.35027429461479187, Final Batch Loss: 0.15422850847244263\n",
      "Epoch 2392, Loss: 0.2979820817708969, Final Batch Loss: 0.14161719381809235\n",
      "Epoch 2393, Loss: 0.3616228252649307, Final Batch Loss: 0.1898794025182724\n",
      "Epoch 2394, Loss: 0.3687703460454941, Final Batch Loss: 0.1669239103794098\n",
      "Epoch 2395, Loss: 0.32364773750305176, Final Batch Loss: 0.15250347554683685\n",
      "Epoch 2396, Loss: 0.35962988436222076, Final Batch Loss: 0.19170117378234863\n",
      "Epoch 2397, Loss: 0.3741973042488098, Final Batch Loss: 0.17949806153774261\n",
      "Epoch 2398, Loss: 0.3606181740760803, Final Batch Loss: 0.18807050585746765\n",
      "Epoch 2399, Loss: 0.3388853222131729, Final Batch Loss: 0.17421041429042816\n",
      "Epoch 2400, Loss: 0.3724489063024521, Final Batch Loss: 0.17763125896453857\n",
      "Epoch 2401, Loss: 0.3150564581155777, Final Batch Loss: 0.16044402122497559\n",
      "Epoch 2402, Loss: 0.31586170196533203, Final Batch Loss: 0.19816088676452637\n",
      "Epoch 2403, Loss: 0.3124934434890747, Final Batch Loss: 0.12717097997665405\n",
      "Epoch 2404, Loss: 0.362196609377861, Final Batch Loss: 0.1636447310447693\n",
      "Epoch 2405, Loss: 0.3395835757255554, Final Batch Loss: 0.15409038960933685\n",
      "Epoch 2406, Loss: 0.34843192994594574, Final Batch Loss: 0.1847873032093048\n",
      "Epoch 2407, Loss: 0.3420979231595993, Final Batch Loss: 0.15760579705238342\n",
      "Epoch 2408, Loss: 0.355238676071167, Final Batch Loss: 0.1648043394088745\n",
      "Epoch 2409, Loss: 0.32556724548339844, Final Batch Loss: 0.18434782326221466\n",
      "Epoch 2410, Loss: 0.3589599281549454, Final Batch Loss: 0.16858023405075073\n",
      "Epoch 2411, Loss: 0.38678982853889465, Final Batch Loss: 0.22507770359516144\n",
      "Epoch 2412, Loss: 0.3247235417366028, Final Batch Loss: 0.15618807077407837\n",
      "Epoch 2413, Loss: 0.3994782119989395, Final Batch Loss: 0.21633200347423553\n",
      "Epoch 2414, Loss: 0.31724077463150024, Final Batch Loss: 0.1750345677137375\n",
      "Epoch 2415, Loss: 0.3720044940710068, Final Batch Loss: 0.15660345554351807\n",
      "Epoch 2416, Loss: 0.36196379363536835, Final Batch Loss: 0.18205885589122772\n",
      "Epoch 2417, Loss: 0.34392350912094116, Final Batch Loss: 0.164338618516922\n",
      "Epoch 2418, Loss: 0.3483974486589432, Final Batch Loss: 0.13775524497032166\n",
      "Epoch 2419, Loss: 0.32598310708999634, Final Batch Loss: 0.1778353899717331\n",
      "Epoch 2420, Loss: 0.298986554145813, Final Batch Loss: 0.13397543132305145\n",
      "Epoch 2421, Loss: 0.32099252939224243, Final Batch Loss: 0.1361202448606491\n",
      "Epoch 2422, Loss: 0.3245609551668167, Final Batch Loss: 0.17109781503677368\n",
      "Epoch 2423, Loss: 0.311593621969223, Final Batch Loss: 0.16485771536827087\n",
      "Epoch 2424, Loss: 0.3170950412750244, Final Batch Loss: 0.17149025201797485\n",
      "Epoch 2425, Loss: 0.3584129810333252, Final Batch Loss: 0.18509259819984436\n",
      "Epoch 2426, Loss: 0.33223697543144226, Final Batch Loss: 0.1636931449174881\n",
      "Epoch 2427, Loss: 0.3075640946626663, Final Batch Loss: 0.14539562165737152\n",
      "Epoch 2428, Loss: 0.329934224486351, Final Batch Loss: 0.17176946997642517\n",
      "Epoch 2429, Loss: 0.39350494742393494, Final Batch Loss: 0.20094604790210724\n",
      "Epoch 2430, Loss: 0.3280830830335617, Final Batch Loss: 0.17409490048885345\n",
      "Epoch 2431, Loss: 0.352859690785408, Final Batch Loss: 0.16681571304798126\n",
      "Epoch 2432, Loss: 0.3301318287849426, Final Batch Loss: 0.14921773970127106\n",
      "Epoch 2433, Loss: 0.31098510324954987, Final Batch Loss: 0.1350765973329544\n",
      "Epoch 2434, Loss: 0.3439245671033859, Final Batch Loss: 0.1793714463710785\n",
      "Epoch 2435, Loss: 0.3494647592306137, Final Batch Loss: 0.15337303280830383\n",
      "Epoch 2436, Loss: 0.3215232342481613, Final Batch Loss: 0.13908036053180695\n",
      "Epoch 2437, Loss: 0.33496952056884766, Final Batch Loss: 0.1345381885766983\n",
      "Epoch 2438, Loss: 0.35292820632457733, Final Batch Loss: 0.15853819251060486\n",
      "Epoch 2439, Loss: 0.33774855732917786, Final Batch Loss: 0.1675097793340683\n",
      "Epoch 2440, Loss: 0.34313586354255676, Final Batch Loss: 0.15364788472652435\n",
      "Epoch 2441, Loss: 0.33375947177410126, Final Batch Loss: 0.1969004124403\n",
      "Epoch 2442, Loss: 0.41409067809581757, Final Batch Loss: 0.23222878575325012\n",
      "Epoch 2443, Loss: 0.35924383997917175, Final Batch Loss: 0.1719721108675003\n",
      "Epoch 2444, Loss: 0.3277751952409744, Final Batch Loss: 0.1626456081867218\n",
      "Epoch 2445, Loss: 0.3589288592338562, Final Batch Loss: 0.1789933145046234\n",
      "Epoch 2446, Loss: 0.40718671679496765, Final Batch Loss: 0.19952769577503204\n",
      "Epoch 2447, Loss: 0.34239575266838074, Final Batch Loss: 0.16695314645767212\n",
      "Epoch 2448, Loss: 0.34832902252674103, Final Batch Loss: 0.1939639151096344\n",
      "Epoch 2449, Loss: 0.38680852949619293, Final Batch Loss: 0.1991882175207138\n",
      "Epoch 2450, Loss: 0.34482742846012115, Final Batch Loss: 0.18377646803855896\n",
      "Epoch 2451, Loss: 0.34569522738456726, Final Batch Loss: 0.18643610179424286\n",
      "Epoch 2452, Loss: 0.32940998673439026, Final Batch Loss: 0.15332700312137604\n",
      "Epoch 2453, Loss: 0.3454435467720032, Final Batch Loss: 0.15755991637706757\n",
      "Epoch 2454, Loss: 0.37825776636600494, Final Batch Loss: 0.19694949686527252\n",
      "Epoch 2455, Loss: 0.33455584943294525, Final Batch Loss: 0.16735483705997467\n",
      "Epoch 2456, Loss: 0.33069756627082825, Final Batch Loss: 0.12901143729686737\n",
      "Epoch 2457, Loss: 0.3245517164468765, Final Batch Loss: 0.14759349822998047\n",
      "Epoch 2458, Loss: 0.33618633449077606, Final Batch Loss: 0.172514870762825\n",
      "Epoch 2459, Loss: 0.33768002688884735, Final Batch Loss: 0.1571812927722931\n",
      "Epoch 2460, Loss: 0.3418690413236618, Final Batch Loss: 0.19773033261299133\n",
      "Epoch 2461, Loss: 0.3613889515399933, Final Batch Loss: 0.2214857041835785\n",
      "Epoch 2462, Loss: 0.33841855823993683, Final Batch Loss: 0.16867683827877045\n",
      "Epoch 2463, Loss: 0.2911880165338516, Final Batch Loss: 0.14670658111572266\n",
      "Epoch 2464, Loss: 0.34402161836624146, Final Batch Loss: 0.15826551616191864\n",
      "Epoch 2465, Loss: 0.3406631350517273, Final Batch Loss: 0.15485639870166779\n",
      "Epoch 2466, Loss: 0.3903464525938034, Final Batch Loss: 0.218441903591156\n",
      "Epoch 2467, Loss: 0.35088610649108887, Final Batch Loss: 0.17795570194721222\n",
      "Epoch 2468, Loss: 0.31963109970092773, Final Batch Loss: 0.1829952895641327\n",
      "Epoch 2469, Loss: 0.35137221217155457, Final Batch Loss: 0.16794031858444214\n",
      "Epoch 2470, Loss: 0.3377750664949417, Final Batch Loss: 0.14463432133197784\n",
      "Epoch 2471, Loss: 0.30807264149188995, Final Batch Loss: 0.18587946891784668\n",
      "Epoch 2472, Loss: 0.33411988615989685, Final Batch Loss: 0.15859811007976532\n",
      "Epoch 2473, Loss: 0.31946004927158356, Final Batch Loss: 0.1793215274810791\n",
      "Epoch 2474, Loss: 0.3012911379337311, Final Batch Loss: 0.14466813206672668\n",
      "Epoch 2475, Loss: 0.37206806242465973, Final Batch Loss: 0.1777799129486084\n",
      "Epoch 2476, Loss: 0.3363747000694275, Final Batch Loss: 0.17225433886051178\n",
      "Epoch 2477, Loss: 0.3499710261821747, Final Batch Loss: 0.16969247162342072\n",
      "Epoch 2478, Loss: 0.4010916203260422, Final Batch Loss: 0.23531615734100342\n",
      "Epoch 2479, Loss: 0.3862856924533844, Final Batch Loss: 0.14576055109500885\n",
      "Epoch 2480, Loss: 0.3593195229768753, Final Batch Loss: 0.18032021820545197\n",
      "Epoch 2481, Loss: 0.33501091599464417, Final Batch Loss: 0.16733475029468536\n",
      "Epoch 2482, Loss: 0.33563025295734406, Final Batch Loss: 0.150276780128479\n",
      "Epoch 2483, Loss: 0.3446188122034073, Final Batch Loss: 0.17761532962322235\n",
      "Epoch 2484, Loss: 0.3714257478713989, Final Batch Loss: 0.19029361009597778\n",
      "Epoch 2485, Loss: 0.3726959079504013, Final Batch Loss: 0.18057268857955933\n",
      "Epoch 2486, Loss: 0.32380570471286774, Final Batch Loss: 0.14662262797355652\n",
      "Epoch 2487, Loss: 0.31604208052158356, Final Batch Loss: 0.16538658738136292\n",
      "Epoch 2488, Loss: 0.3591592311859131, Final Batch Loss: 0.16654770076274872\n",
      "Epoch 2489, Loss: 0.33171433210372925, Final Batch Loss: 0.16313961148262024\n",
      "Epoch 2490, Loss: 0.3468443751335144, Final Batch Loss: 0.20111437141895294\n",
      "Epoch 2491, Loss: 0.3362954258918762, Final Batch Loss: 0.18398509919643402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2492, Loss: 0.34255072474479675, Final Batch Loss: 0.19298890233039856\n",
      "Epoch 2493, Loss: 0.3336252123117447, Final Batch Loss: 0.16442885994911194\n",
      "Epoch 2494, Loss: 0.326744019985199, Final Batch Loss: 0.19432491064071655\n",
      "Epoch 2495, Loss: 0.3707020729780197, Final Batch Loss: 0.2001185566186905\n",
      "Epoch 2496, Loss: 0.33245308697223663, Final Batch Loss: 0.19142992794513702\n",
      "Epoch 2497, Loss: 0.34161871671676636, Final Batch Loss: 0.14715315401554108\n",
      "Epoch 2498, Loss: 0.31141039729118347, Final Batch Loss: 0.12862128019332886\n",
      "Epoch 2499, Loss: 0.317985475063324, Final Batch Loss: 0.15918853878974915\n",
      "Epoch 2500, Loss: 0.3788612484931946, Final Batch Loss: 0.22009967267513275\n",
      "Epoch 2501, Loss: 0.3268773704767227, Final Batch Loss: 0.1443297266960144\n",
      "Epoch 2502, Loss: 0.3386394679546356, Final Batch Loss: 0.16040560603141785\n",
      "Epoch 2503, Loss: 0.3374703824520111, Final Batch Loss: 0.17012152075767517\n",
      "Epoch 2504, Loss: 0.3490373492240906, Final Batch Loss: 0.16325105726718903\n",
      "Epoch 2505, Loss: 0.31404460966587067, Final Batch Loss: 0.15449686348438263\n",
      "Epoch 2506, Loss: 0.33893895149230957, Final Batch Loss: 0.161008819937706\n",
      "Epoch 2507, Loss: 0.3518410921096802, Final Batch Loss: 0.1643117219209671\n",
      "Epoch 2508, Loss: 0.35230451822280884, Final Batch Loss: 0.15422816574573517\n",
      "Epoch 2509, Loss: 0.3507259786128998, Final Batch Loss: 0.159620463848114\n",
      "Epoch 2510, Loss: 0.31287530064582825, Final Batch Loss: 0.13366498053073883\n",
      "Epoch 2511, Loss: 0.29058414697647095, Final Batch Loss: 0.13096094131469727\n",
      "Epoch 2512, Loss: 0.3623780608177185, Final Batch Loss: 0.19424889981746674\n",
      "Epoch 2513, Loss: 0.34228673577308655, Final Batch Loss: 0.15041552484035492\n",
      "Epoch 2514, Loss: 0.308719664812088, Final Batch Loss: 0.13937672972679138\n",
      "Epoch 2515, Loss: 0.33484673500061035, Final Batch Loss: 0.15254950523376465\n",
      "Epoch 2516, Loss: 0.35139745473861694, Final Batch Loss: 0.15363328158855438\n",
      "Epoch 2517, Loss: 0.3111779987812042, Final Batch Loss: 0.13042041659355164\n",
      "Epoch 2518, Loss: 0.29576871544122696, Final Batch Loss: 0.11922449618577957\n",
      "Epoch 2519, Loss: 0.36021459102630615, Final Batch Loss: 0.22346869111061096\n",
      "Epoch 2520, Loss: 0.3614426553249359, Final Batch Loss: 0.20267058908939362\n",
      "Epoch 2521, Loss: 0.33409935235977173, Final Batch Loss: 0.16663165390491486\n",
      "Epoch 2522, Loss: 0.33376002311706543, Final Batch Loss: 0.16721463203430176\n",
      "Epoch 2523, Loss: 0.3511057198047638, Final Batch Loss: 0.18456220626831055\n",
      "Epoch 2524, Loss: 0.32759690284729004, Final Batch Loss: 0.1899038404226303\n",
      "Epoch 2525, Loss: 0.30584338307380676, Final Batch Loss: 0.15503564476966858\n",
      "Epoch 2526, Loss: 0.3764946460723877, Final Batch Loss: 0.21286417543888092\n",
      "Epoch 2527, Loss: 0.3266431391239166, Final Batch Loss: 0.17088018357753754\n",
      "Epoch 2528, Loss: 0.3140310198068619, Final Batch Loss: 0.14826005697250366\n",
      "Epoch 2529, Loss: 0.3487543761730194, Final Batch Loss: 0.2018771916627884\n",
      "Epoch 2530, Loss: 0.30968375504016876, Final Batch Loss: 0.1415981650352478\n",
      "Epoch 2531, Loss: 0.3104782998561859, Final Batch Loss: 0.1661996841430664\n",
      "Epoch 2532, Loss: 0.3353377431631088, Final Batch Loss: 0.16726046800613403\n",
      "Epoch 2533, Loss: 0.35184894502162933, Final Batch Loss: 0.15541769564151764\n",
      "Epoch 2534, Loss: 0.42045626044273376, Final Batch Loss: 0.17983102798461914\n",
      "Epoch 2535, Loss: 0.3437194377183914, Final Batch Loss: 0.18322531878948212\n",
      "Epoch 2536, Loss: 0.306837797164917, Final Batch Loss: 0.1364331990480423\n",
      "Epoch 2537, Loss: 0.3514526039361954, Final Batch Loss: 0.1708301156759262\n",
      "Epoch 2538, Loss: 0.3229827880859375, Final Batch Loss: 0.14887477457523346\n",
      "Epoch 2539, Loss: 0.33684244751930237, Final Batch Loss: 0.12794825434684753\n",
      "Epoch 2540, Loss: 0.3469076454639435, Final Batch Loss: 0.19182559847831726\n",
      "Epoch 2541, Loss: 0.33146245777606964, Final Batch Loss: 0.19478708505630493\n",
      "Epoch 2542, Loss: 0.30992715060710907, Final Batch Loss: 0.13346733152866364\n",
      "Epoch 2543, Loss: 0.31646697223186493, Final Batch Loss: 0.15410137176513672\n",
      "Epoch 2544, Loss: 0.314345583319664, Final Batch Loss: 0.16272005438804626\n",
      "Epoch 2545, Loss: 0.33932070434093475, Final Batch Loss: 0.1832398921251297\n",
      "Epoch 2546, Loss: 0.30800245702266693, Final Batch Loss: 0.16057628393173218\n",
      "Epoch 2547, Loss: 0.33344124257564545, Final Batch Loss: 0.18896940350532532\n",
      "Epoch 2548, Loss: 0.33296991884708405, Final Batch Loss: 0.1529984176158905\n",
      "Epoch 2549, Loss: 0.3704206347465515, Final Batch Loss: 0.18362118303775787\n",
      "Epoch 2550, Loss: 0.29583054780960083, Final Batch Loss: 0.14087964594364166\n",
      "Epoch 2551, Loss: 0.2858467549085617, Final Batch Loss: 0.14044401049613953\n",
      "Epoch 2552, Loss: 0.35469813644886017, Final Batch Loss: 0.2038552165031433\n",
      "Epoch 2553, Loss: 0.42791374027729034, Final Batch Loss: 0.24112878739833832\n",
      "Epoch 2554, Loss: 0.3200678825378418, Final Batch Loss: 0.1543467938899994\n",
      "Epoch 2555, Loss: 0.3604235500097275, Final Batch Loss: 0.17500299215316772\n",
      "Epoch 2556, Loss: 0.3492880165576935, Final Batch Loss: 0.18858659267425537\n",
      "Epoch 2557, Loss: 0.30789926648139954, Final Batch Loss: 0.14385627210140228\n",
      "Epoch 2558, Loss: 0.32126298546791077, Final Batch Loss: 0.1721840500831604\n",
      "Epoch 2559, Loss: 0.3583158850669861, Final Batch Loss: 0.1654745191335678\n",
      "Epoch 2560, Loss: 0.31782835721969604, Final Batch Loss: 0.16341574490070343\n",
      "Epoch 2561, Loss: 0.3413911461830139, Final Batch Loss: 0.15650127828121185\n",
      "Epoch 2562, Loss: 0.3506866842508316, Final Batch Loss: 0.15538184344768524\n",
      "Epoch 2563, Loss: 0.3293090760707855, Final Batch Loss: 0.16931213438510895\n",
      "Epoch 2564, Loss: 0.3633006066083908, Final Batch Loss: 0.20232605934143066\n",
      "Epoch 2565, Loss: 0.3460313081741333, Final Batch Loss: 0.1814408153295517\n",
      "Epoch 2566, Loss: 0.31832171976566315, Final Batch Loss: 0.1565665453672409\n",
      "Epoch 2567, Loss: 0.3177341818809509, Final Batch Loss: 0.15809191763401031\n",
      "Epoch 2568, Loss: 0.36273665726184845, Final Batch Loss: 0.1729995310306549\n",
      "Epoch 2569, Loss: 0.33221422135829926, Final Batch Loss: 0.16157062351703644\n",
      "Epoch 2570, Loss: 0.3766724616289139, Final Batch Loss: 0.1852392703294754\n",
      "Epoch 2571, Loss: 0.3265543282032013, Final Batch Loss: 0.1910681128501892\n",
      "Epoch 2572, Loss: 0.325593039393425, Final Batch Loss: 0.19172826409339905\n",
      "Epoch 2573, Loss: 0.3433602899312973, Final Batch Loss: 0.17680802941322327\n",
      "Epoch 2574, Loss: 0.3369170129299164, Final Batch Loss: 0.17256666719913483\n",
      "Epoch 2575, Loss: 0.3484712243080139, Final Batch Loss: 0.19751384854316711\n",
      "Epoch 2576, Loss: 0.35335393249988556, Final Batch Loss: 0.170381098985672\n",
      "Epoch 2577, Loss: 0.320295050740242, Final Batch Loss: 0.16628047823905945\n",
      "Epoch 2578, Loss: 0.35521960258483887, Final Batch Loss: 0.14472800493240356\n",
      "Epoch 2579, Loss: 0.3627054840326309, Final Batch Loss: 0.2179626226425171\n",
      "Epoch 2580, Loss: 0.3946271240711212, Final Batch Loss: 0.18561075627803802\n",
      "Epoch 2581, Loss: 0.32157057523727417, Final Batch Loss: 0.14187604188919067\n",
      "Epoch 2582, Loss: 0.4045390486717224, Final Batch Loss: 0.1779206395149231\n",
      "Epoch 2583, Loss: 0.38164471089839935, Final Batch Loss: 0.18940284848213196\n",
      "Epoch 2584, Loss: 0.30891604721546173, Final Batch Loss: 0.1651313751935959\n",
      "Epoch 2585, Loss: 0.3043147176504135, Final Batch Loss: 0.15356263518333435\n",
      "Epoch 2586, Loss: 0.3238610476255417, Final Batch Loss: 0.131145179271698\n",
      "Epoch 2587, Loss: 0.3656119257211685, Final Batch Loss: 0.18348267674446106\n",
      "Epoch 2588, Loss: 0.3459269404411316, Final Batch Loss: 0.14129367470741272\n",
      "Epoch 2589, Loss: 0.3054891973733902, Final Batch Loss: 0.1690191626548767\n",
      "Epoch 2590, Loss: 0.3064647316932678, Final Batch Loss: 0.15571941435337067\n",
      "Epoch 2591, Loss: 0.3569248467683792, Final Batch Loss: 0.1968194842338562\n",
      "Epoch 2592, Loss: 0.35100221633911133, Final Batch Loss: 0.15686626732349396\n",
      "Epoch 2593, Loss: 0.3162246495485306, Final Batch Loss: 0.14148610830307007\n",
      "Epoch 2594, Loss: 0.31868596374988556, Final Batch Loss: 0.15757495164871216\n",
      "Epoch 2595, Loss: 0.29658475518226624, Final Batch Loss: 0.15241257846355438\n",
      "Epoch 2596, Loss: 0.3856751471757889, Final Batch Loss: 0.22004453837871552\n",
      "Epoch 2597, Loss: 0.3852875679731369, Final Batch Loss: 0.19694814085960388\n",
      "Epoch 2598, Loss: 0.3484915941953659, Final Batch Loss: 0.17858220636844635\n",
      "Epoch 2599, Loss: 0.3482910394668579, Final Batch Loss: 0.1895586997270584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2600, Loss: 0.33920155465602875, Final Batch Loss: 0.1637817770242691\n",
      "Epoch 2601, Loss: 0.38428185880184174, Final Batch Loss: 0.2052079439163208\n",
      "Epoch 2602, Loss: 0.3178318589925766, Final Batch Loss: 0.17950081825256348\n",
      "Epoch 2603, Loss: 0.3644919842481613, Final Batch Loss: 0.21976172924041748\n",
      "Epoch 2604, Loss: 0.3210633546113968, Final Batch Loss: 0.14811113476753235\n",
      "Epoch 2605, Loss: 0.38553689420223236, Final Batch Loss: 0.18007764220237732\n",
      "Epoch 2606, Loss: 0.3061593174934387, Final Batch Loss: 0.15473031997680664\n",
      "Epoch 2607, Loss: 0.32289470732212067, Final Batch Loss: 0.17166896164417267\n",
      "Epoch 2608, Loss: 0.35381922125816345, Final Batch Loss: 0.18191488087177277\n",
      "Epoch 2609, Loss: 0.3718947470188141, Final Batch Loss: 0.16131703555583954\n",
      "Epoch 2610, Loss: 0.38401976227760315, Final Batch Loss: 0.23137077689170837\n",
      "Epoch 2611, Loss: 0.3930823653936386, Final Batch Loss: 0.15841177105903625\n",
      "Epoch 2612, Loss: 0.34166383743286133, Final Batch Loss: 0.16921493411064148\n",
      "Epoch 2613, Loss: 0.39273157715797424, Final Batch Loss: 0.19959470629692078\n",
      "Epoch 2614, Loss: 0.34770137071609497, Final Batch Loss: 0.1632634997367859\n",
      "Epoch 2615, Loss: 0.3235088437795639, Final Batch Loss: 0.17965838313102722\n",
      "Epoch 2616, Loss: 0.34873001277446747, Final Batch Loss: 0.17950519919395447\n",
      "Epoch 2617, Loss: 0.342071995139122, Final Batch Loss: 0.1872379630804062\n",
      "Epoch 2618, Loss: 0.39358745515346527, Final Batch Loss: 0.2131107747554779\n",
      "Epoch 2619, Loss: 0.36288033425807953, Final Batch Loss: 0.16844145953655243\n",
      "Epoch 2620, Loss: 0.3040844053030014, Final Batch Loss: 0.12951862812042236\n",
      "Epoch 2621, Loss: 0.342989444732666, Final Batch Loss: 0.17468629777431488\n",
      "Epoch 2622, Loss: 0.310477152466774, Final Batch Loss: 0.14927269518375397\n",
      "Epoch 2623, Loss: 0.30150604248046875, Final Batch Loss: 0.14973798394203186\n",
      "Epoch 2624, Loss: 0.3433508425951004, Final Batch Loss: 0.15248236060142517\n",
      "Epoch 2625, Loss: 0.33587056398391724, Final Batch Loss: 0.15727587044239044\n",
      "Epoch 2626, Loss: 0.35375842452049255, Final Batch Loss: 0.19410154223442078\n",
      "Epoch 2627, Loss: 0.35492849349975586, Final Batch Loss: 0.198412224650383\n",
      "Epoch 2628, Loss: 0.34311528503894806, Final Batch Loss: 0.19326262176036835\n",
      "Epoch 2629, Loss: 0.32005149126052856, Final Batch Loss: 0.16208939254283905\n",
      "Epoch 2630, Loss: 0.2882384955883026, Final Batch Loss: 0.15095271170139313\n",
      "Epoch 2631, Loss: 0.34732869267463684, Final Batch Loss: 0.17025262117385864\n",
      "Epoch 2632, Loss: 0.2996388077735901, Final Batch Loss: 0.14391551911830902\n",
      "Epoch 2633, Loss: 0.33488012850284576, Final Batch Loss: 0.18077680468559265\n",
      "Epoch 2634, Loss: 0.3313871771097183, Final Batch Loss: 0.163045272231102\n",
      "Epoch 2635, Loss: 0.36227938532829285, Final Batch Loss: 0.16174712777137756\n",
      "Epoch 2636, Loss: 0.31907518208026886, Final Batch Loss: 0.13797062635421753\n",
      "Epoch 2637, Loss: 0.40016819536685944, Final Batch Loss: 0.1980479508638382\n",
      "Epoch 2638, Loss: 0.32527583837509155, Final Batch Loss: 0.16100046038627625\n",
      "Epoch 2639, Loss: 0.3178224116563797, Final Batch Loss: 0.14539200067520142\n",
      "Epoch 2640, Loss: 0.34566351771354675, Final Batch Loss: 0.19025099277496338\n",
      "Epoch 2641, Loss: 0.3183024078607559, Final Batch Loss: 0.16160881519317627\n",
      "Epoch 2642, Loss: 0.3003056198358536, Final Batch Loss: 0.1592758595943451\n",
      "Epoch 2643, Loss: 0.3281451016664505, Final Batch Loss: 0.14239491522312164\n",
      "Epoch 2644, Loss: 0.33823496103286743, Final Batch Loss: 0.15421642363071442\n",
      "Epoch 2645, Loss: 0.326686330139637, Final Batch Loss: 0.2024787813425064\n",
      "Epoch 2646, Loss: 0.37570425868034363, Final Batch Loss: 0.14400775730609894\n",
      "Epoch 2647, Loss: 0.32840876281261444, Final Batch Loss: 0.17089085280895233\n",
      "Epoch 2648, Loss: 0.32119446992874146, Final Batch Loss: 0.17666800320148468\n",
      "Epoch 2649, Loss: 0.311589851975441, Final Batch Loss: 0.171211838722229\n",
      "Epoch 2650, Loss: 0.3026648759841919, Final Batch Loss: 0.14017416536808014\n",
      "Epoch 2651, Loss: 0.3005307763814926, Final Batch Loss: 0.1648212969303131\n",
      "Epoch 2652, Loss: 0.3514057546854019, Final Batch Loss: 0.1776520311832428\n",
      "Epoch 2653, Loss: 0.34753791987895966, Final Batch Loss: 0.1704069972038269\n",
      "Epoch 2654, Loss: 0.3208792209625244, Final Batch Loss: 0.14031486213207245\n",
      "Epoch 2655, Loss: 0.32403111457824707, Final Batch Loss: 0.17570511996746063\n",
      "Epoch 2656, Loss: 0.3401343524456024, Final Batch Loss: 0.16201911866664886\n",
      "Epoch 2657, Loss: 0.38215337693691254, Final Batch Loss: 0.2528769373893738\n",
      "Epoch 2658, Loss: 0.35315728187561035, Final Batch Loss: 0.15902648866176605\n",
      "Epoch 2659, Loss: 0.3467860370874405, Final Batch Loss: 0.1609712839126587\n",
      "Epoch 2660, Loss: 0.3186611980199814, Final Batch Loss: 0.15453431010246277\n",
      "Epoch 2661, Loss: 0.29270343482494354, Final Batch Loss: 0.1372344046831131\n",
      "Epoch 2662, Loss: 0.3336288034915924, Final Batch Loss: 0.17005608975887299\n",
      "Epoch 2663, Loss: 0.3401869237422943, Final Batch Loss: 0.16203445196151733\n",
      "Epoch 2664, Loss: 0.2988864779472351, Final Batch Loss: 0.14325791597366333\n",
      "Epoch 2665, Loss: 0.3021031767129898, Final Batch Loss: 0.1403713971376419\n",
      "Epoch 2666, Loss: 0.3366333842277527, Final Batch Loss: 0.17720915377140045\n",
      "Epoch 2667, Loss: 0.3324192315340042, Final Batch Loss: 0.15734727680683136\n",
      "Epoch 2668, Loss: 0.3657841980457306, Final Batch Loss: 0.19290600717067719\n",
      "Epoch 2669, Loss: 0.38511718809604645, Final Batch Loss: 0.16617386043071747\n",
      "Epoch 2670, Loss: 0.33771032094955444, Final Batch Loss: 0.19329103827476501\n",
      "Epoch 2671, Loss: 0.37025418877601624, Final Batch Loss: 0.19092808663845062\n",
      "Epoch 2672, Loss: 0.33130137622356415, Final Batch Loss: 0.1880495399236679\n",
      "Epoch 2673, Loss: 0.3319922089576721, Final Batch Loss: 0.13690407574176788\n",
      "Epoch 2674, Loss: 0.3216402977705002, Final Batch Loss: 0.15170805156230927\n",
      "Epoch 2675, Loss: 0.3326224535703659, Final Batch Loss: 0.18536534905433655\n",
      "Epoch 2676, Loss: 0.36614999175071716, Final Batch Loss: 0.18345880508422852\n",
      "Epoch 2677, Loss: 0.3141099214553833, Final Batch Loss: 0.1885082870721817\n",
      "Epoch 2678, Loss: 0.345883846282959, Final Batch Loss: 0.18897905945777893\n",
      "Epoch 2679, Loss: 0.3535502552986145, Final Batch Loss: 0.17185068130493164\n",
      "Epoch 2680, Loss: 0.3099192827939987, Final Batch Loss: 0.14320802688598633\n",
      "Epoch 2681, Loss: 0.3273586928844452, Final Batch Loss: 0.16069111227989197\n",
      "Epoch 2682, Loss: 0.3550964593887329, Final Batch Loss: 0.15906651318073273\n",
      "Epoch 2683, Loss: 0.28571324050426483, Final Batch Loss: 0.15052787959575653\n",
      "Epoch 2684, Loss: 0.3082096129655838, Final Batch Loss: 0.16603577136993408\n",
      "Epoch 2685, Loss: 0.37434153258800507, Final Batch Loss: 0.17095856368541718\n",
      "Epoch 2686, Loss: 0.33326150476932526, Final Batch Loss: 0.16497206687927246\n",
      "Epoch 2687, Loss: 0.3244091272354126, Final Batch Loss: 0.1487339287996292\n",
      "Epoch 2688, Loss: 0.31625036895275116, Final Batch Loss: 0.1688404381275177\n",
      "Epoch 2689, Loss: 0.3479918986558914, Final Batch Loss: 0.1962277889251709\n",
      "Epoch 2690, Loss: 0.35088784992694855, Final Batch Loss: 0.16753363609313965\n",
      "Epoch 2691, Loss: 0.3425169438123703, Final Batch Loss: 0.17826110124588013\n",
      "Epoch 2692, Loss: 0.36141906678676605, Final Batch Loss: 0.15498463809490204\n",
      "Epoch 2693, Loss: 0.3521362841129303, Final Batch Loss: 0.16845203936100006\n",
      "Epoch 2694, Loss: 0.34053100645542145, Final Batch Loss: 0.20729795098304749\n",
      "Epoch 2695, Loss: 0.321761816740036, Final Batch Loss: 0.16536466777324677\n",
      "Epoch 2696, Loss: 0.30302950739860535, Final Batch Loss: 0.15773755311965942\n",
      "Epoch 2697, Loss: 0.44733908772468567, Final Batch Loss: 0.30268004536628723\n",
      "Epoch 2698, Loss: 0.3575079143047333, Final Batch Loss: 0.1697119176387787\n",
      "Epoch 2699, Loss: 0.31561318039894104, Final Batch Loss: 0.18887588381767273\n",
      "Epoch 2700, Loss: 0.27995137870311737, Final Batch Loss: 0.15410757064819336\n",
      "Epoch 2701, Loss: 0.322940930724144, Final Batch Loss: 0.1599641889333725\n",
      "Epoch 2702, Loss: 0.3436652719974518, Final Batch Loss: 0.1728239357471466\n",
      "Epoch 2703, Loss: 0.327774778008461, Final Batch Loss: 0.19329482316970825\n",
      "Epoch 2704, Loss: 0.33992645144462585, Final Batch Loss: 0.12561890482902527\n",
      "Epoch 2705, Loss: 0.3350052237510681, Final Batch Loss: 0.15433305501937866\n",
      "Epoch 2706, Loss: 0.3315824866294861, Final Batch Loss: 0.1439419388771057\n",
      "Epoch 2707, Loss: 0.324722021818161, Final Batch Loss: 0.14625971019268036\n",
      "Epoch 2708, Loss: 0.33543938398361206, Final Batch Loss: 0.1769058108329773\n",
      "Epoch 2709, Loss: 0.3074968308210373, Final Batch Loss: 0.1290619671344757\n",
      "Epoch 2710, Loss: 0.2956773042678833, Final Batch Loss: 0.14285607635974884\n",
      "Epoch 2711, Loss: 0.33286577463150024, Final Batch Loss: 0.17164546251296997\n",
      "Epoch 2712, Loss: 0.2925049513578415, Final Batch Loss: 0.13868601620197296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2713, Loss: 0.30859607458114624, Final Batch Loss: 0.14713558554649353\n",
      "Epoch 2714, Loss: 0.33147746324539185, Final Batch Loss: 0.20699869096279144\n",
      "Epoch 2715, Loss: 0.2885596230626106, Final Batch Loss: 0.1738944798707962\n",
      "Epoch 2716, Loss: 0.30262230336666107, Final Batch Loss: 0.1424800306558609\n",
      "Epoch 2717, Loss: 0.29922159761190414, Final Batch Loss: 0.18751706182956696\n",
      "Epoch 2718, Loss: 0.31196165084838867, Final Batch Loss: 0.1566935032606125\n",
      "Epoch 2719, Loss: 0.366093173623085, Final Batch Loss: 0.16534540057182312\n",
      "Epoch 2720, Loss: 0.33791621029376984, Final Batch Loss: 0.17798636853694916\n",
      "Epoch 2721, Loss: 0.3185178190469742, Final Batch Loss: 0.16378439962863922\n",
      "Epoch 2722, Loss: 0.28598733246326447, Final Batch Loss: 0.12807078659534454\n",
      "Epoch 2723, Loss: 0.35424213111400604, Final Batch Loss: 0.165432408452034\n",
      "Epoch 2724, Loss: 0.3488864302635193, Final Batch Loss: 0.1952151507139206\n",
      "Epoch 2725, Loss: 0.31340159475803375, Final Batch Loss: 0.14837145805358887\n",
      "Epoch 2726, Loss: 0.41467106342315674, Final Batch Loss: 0.19576483964920044\n",
      "Epoch 2727, Loss: 0.3245292603969574, Final Batch Loss: 0.1681293398141861\n",
      "Epoch 2728, Loss: 0.37824612855911255, Final Batch Loss: 0.17295706272125244\n",
      "Epoch 2729, Loss: 0.3411432355642319, Final Batch Loss: 0.1856129914522171\n",
      "Epoch 2730, Loss: 0.3498953878879547, Final Batch Loss: 0.18173100054264069\n",
      "Epoch 2731, Loss: 0.36798617243766785, Final Batch Loss: 0.167509987950325\n",
      "Epoch 2732, Loss: 0.33314813673496246, Final Batch Loss: 0.17174191772937775\n",
      "Epoch 2733, Loss: 0.44527146220207214, Final Batch Loss: 0.302462637424469\n",
      "Epoch 2734, Loss: 0.3862770199775696, Final Batch Loss: 0.20751461386680603\n",
      "Epoch 2735, Loss: 0.3437933921813965, Final Batch Loss: 0.1422615647315979\n",
      "Epoch 2736, Loss: 0.4332723319530487, Final Batch Loss: 0.25177639722824097\n",
      "Epoch 2737, Loss: 0.34168271720409393, Final Batch Loss: 0.15821656584739685\n",
      "Epoch 2738, Loss: 0.32469208538532257, Final Batch Loss: 0.15277321636676788\n",
      "Epoch 2739, Loss: 0.38556745648384094, Final Batch Loss: 0.22088244557380676\n",
      "Epoch 2740, Loss: 0.3567599654197693, Final Batch Loss: 0.18411597609519958\n",
      "Epoch 2741, Loss: 0.29678745567798615, Final Batch Loss: 0.1668722927570343\n",
      "Epoch 2742, Loss: 0.3292154520750046, Final Batch Loss: 0.1673259139060974\n",
      "Epoch 2743, Loss: 0.3868524879217148, Final Batch Loss: 0.1764485239982605\n",
      "Epoch 2744, Loss: 0.3130720257759094, Final Batch Loss: 0.15213905274868011\n",
      "Epoch 2745, Loss: 0.34519703686237335, Final Batch Loss: 0.18047218024730682\n",
      "Epoch 2746, Loss: 0.32274574041366577, Final Batch Loss: 0.15653124451637268\n",
      "Epoch 2747, Loss: 0.31654682755470276, Final Batch Loss: 0.17975345253944397\n",
      "Epoch 2748, Loss: 0.32831500470638275, Final Batch Loss: 0.14816778898239136\n",
      "Epoch 2749, Loss: 0.3694487363100052, Final Batch Loss: 0.18614082038402557\n",
      "Epoch 2750, Loss: 0.3508721739053726, Final Batch Loss: 0.17023581266403198\n",
      "Epoch 2751, Loss: 0.3232780396938324, Final Batch Loss: 0.17388087511062622\n",
      "Epoch 2752, Loss: 0.3274364769458771, Final Batch Loss: 0.1550656110048294\n",
      "Epoch 2753, Loss: 0.31023652851581573, Final Batch Loss: 0.13093869388103485\n",
      "Epoch 2754, Loss: 0.3126416504383087, Final Batch Loss: 0.14127415418624878\n",
      "Epoch 2755, Loss: 0.3518899530172348, Final Batch Loss: 0.20409724116325378\n",
      "Epoch 2756, Loss: 0.32217757403850555, Final Batch Loss: 0.16170576214790344\n",
      "Epoch 2757, Loss: 0.302658349275589, Final Batch Loss: 0.1555420160293579\n",
      "Epoch 2758, Loss: 0.3543889969587326, Final Batch Loss: 0.17696909606456757\n",
      "Epoch 2759, Loss: 0.31353652477264404, Final Batch Loss: 0.16494549810886383\n",
      "Epoch 2760, Loss: 0.3022383004426956, Final Batch Loss: 0.1563527137041092\n",
      "Epoch 2761, Loss: 0.41044269502162933, Final Batch Loss: 0.21323776245117188\n",
      "Epoch 2762, Loss: 0.32080531120300293, Final Batch Loss: 0.14025118947029114\n",
      "Epoch 2763, Loss: 0.328713521361351, Final Batch Loss: 0.18645988404750824\n",
      "Epoch 2764, Loss: 0.28160756081342697, Final Batch Loss: 0.11525992304086685\n",
      "Epoch 2765, Loss: 0.31588760018348694, Final Batch Loss: 0.1625412553548813\n",
      "Epoch 2766, Loss: 0.3041670173406601, Final Batch Loss: 0.1723528653383255\n",
      "Epoch 2767, Loss: 0.3182234466075897, Final Batch Loss: 0.1497795134782791\n",
      "Epoch 2768, Loss: 0.2796839475631714, Final Batch Loss: 0.14284782111644745\n",
      "Epoch 2769, Loss: 0.32808831334114075, Final Batch Loss: 0.16031044721603394\n",
      "Epoch 2770, Loss: 0.33450545370578766, Final Batch Loss: 0.19006875157356262\n",
      "Epoch 2771, Loss: 0.34542545676231384, Final Batch Loss: 0.2204129546880722\n",
      "Epoch 2772, Loss: 0.3342839330434799, Final Batch Loss: 0.18069088459014893\n",
      "Epoch 2773, Loss: 0.31181401014328003, Final Batch Loss: 0.15595519542694092\n",
      "Epoch 2774, Loss: 0.33471447229385376, Final Batch Loss: 0.1653359979391098\n",
      "Epoch 2775, Loss: 0.30216142535209656, Final Batch Loss: 0.13480252027511597\n",
      "Epoch 2776, Loss: 0.38420939445495605, Final Batch Loss: 0.2430894672870636\n",
      "Epoch 2777, Loss: 0.31984739005565643, Final Batch Loss: 0.14641889929771423\n",
      "Epoch 2778, Loss: 0.3257075399160385, Final Batch Loss: 0.15282119810581207\n",
      "Epoch 2779, Loss: 0.3482896387577057, Final Batch Loss: 0.19707243144512177\n",
      "Epoch 2780, Loss: 0.33420513570308685, Final Batch Loss: 0.186698317527771\n",
      "Epoch 2781, Loss: 0.3733984977006912, Final Batch Loss: 0.16316349804401398\n",
      "Epoch 2782, Loss: 0.3415202498435974, Final Batch Loss: 0.16658692061901093\n",
      "Epoch 2783, Loss: 0.3353682905435562, Final Batch Loss: 0.17716297507286072\n",
      "Epoch 2784, Loss: 0.3207824379205704, Final Batch Loss: 0.1802697479724884\n",
      "Epoch 2785, Loss: 0.28332284092903137, Final Batch Loss: 0.10955063998699188\n",
      "Epoch 2786, Loss: 0.3345172554254532, Final Batch Loss: 0.1806713491678238\n",
      "Epoch 2787, Loss: 0.38621559739112854, Final Batch Loss: 0.2355230748653412\n",
      "Epoch 2788, Loss: 0.3034493774175644, Final Batch Loss: 0.1491507887840271\n",
      "Epoch 2789, Loss: 0.33265312016010284, Final Batch Loss: 0.15599755942821503\n",
      "Epoch 2790, Loss: 0.32901252806186676, Final Batch Loss: 0.16129830479621887\n",
      "Epoch 2791, Loss: 0.34020964801311493, Final Batch Loss: 0.1516621708869934\n",
      "Epoch 2792, Loss: 0.31075771152973175, Final Batch Loss: 0.17949926853179932\n",
      "Epoch 2793, Loss: 0.3337330222129822, Final Batch Loss: 0.15538448095321655\n",
      "Epoch 2794, Loss: 0.34908196330070496, Final Batch Loss: 0.1509716808795929\n",
      "Epoch 2795, Loss: 0.34452158212661743, Final Batch Loss: 0.15727485716342926\n",
      "Epoch 2796, Loss: 0.31942714750766754, Final Batch Loss: 0.17179720103740692\n",
      "Epoch 2797, Loss: 0.325461283326149, Final Batch Loss: 0.17852821946144104\n",
      "Epoch 2798, Loss: 0.3384939730167389, Final Batch Loss: 0.16348481178283691\n",
      "Epoch 2799, Loss: 0.3208049535751343, Final Batch Loss: 0.15117651224136353\n",
      "Epoch 2800, Loss: 0.294320210814476, Final Batch Loss: 0.15650711953639984\n",
      "Epoch 2801, Loss: 0.3224688917398453, Final Batch Loss: 0.16140976548194885\n",
      "Epoch 2802, Loss: 0.33557747304439545, Final Batch Loss: 0.1829647421836853\n",
      "Epoch 2803, Loss: 0.3130693584680557, Final Batch Loss: 0.17312073707580566\n",
      "Epoch 2804, Loss: 0.32230284810066223, Final Batch Loss: 0.1372392177581787\n",
      "Epoch 2805, Loss: 0.3134009689092636, Final Batch Loss: 0.15288668870925903\n",
      "Epoch 2806, Loss: 0.3647604435682297, Final Batch Loss: 0.1704602837562561\n",
      "Epoch 2807, Loss: 0.33059677481651306, Final Batch Loss: 0.13868574798107147\n",
      "Epoch 2808, Loss: 0.2834053039550781, Final Batch Loss: 0.152368426322937\n",
      "Epoch 2809, Loss: 0.34419044852256775, Final Batch Loss: 0.17708681523799896\n",
      "Epoch 2810, Loss: 0.31898778676986694, Final Batch Loss: 0.1632119119167328\n",
      "Epoch 2811, Loss: 0.2958596795797348, Final Batch Loss: 0.12483716011047363\n",
      "Epoch 2812, Loss: 0.3654443323612213, Final Batch Loss: 0.1914009004831314\n",
      "Epoch 2813, Loss: 0.2925436645746231, Final Batch Loss: 0.1520877629518509\n",
      "Epoch 2814, Loss: 0.2911698669195175, Final Batch Loss: 0.1597394049167633\n",
      "Epoch 2815, Loss: 0.33109836280345917, Final Batch Loss: 0.16572163999080658\n",
      "Epoch 2816, Loss: 0.35132093727588654, Final Batch Loss: 0.14161039888858795\n",
      "Epoch 2817, Loss: 0.30396315455436707, Final Batch Loss: 0.14738185703754425\n",
      "Epoch 2818, Loss: 0.34063005447387695, Final Batch Loss: 0.1799677461385727\n",
      "Epoch 2819, Loss: 0.3058452904224396, Final Batch Loss: 0.16002237796783447\n",
      "Epoch 2820, Loss: 0.2948492020368576, Final Batch Loss: 0.1674617975950241\n",
      "Epoch 2821, Loss: 0.3073141872882843, Final Batch Loss: 0.1199336051940918\n",
      "Epoch 2822, Loss: 0.3097352683544159, Final Batch Loss: 0.13070815801620483\n",
      "Epoch 2823, Loss: 0.31173090636730194, Final Batch Loss: 0.1559814065694809\n",
      "Epoch 2824, Loss: 0.3432089388370514, Final Batch Loss: 0.1638408750295639\n",
      "Epoch 2825, Loss: 0.29897674918174744, Final Batch Loss: 0.1418955773115158\n",
      "Epoch 2826, Loss: 0.3346807211637497, Final Batch Loss: 0.140403613448143\n",
      "Epoch 2827, Loss: 0.33323174715042114, Final Batch Loss: 0.1763281673192978\n",
      "Epoch 2828, Loss: 0.29631562530994415, Final Batch Loss: 0.1304924488067627\n",
      "Epoch 2829, Loss: 0.3417298197746277, Final Batch Loss: 0.1737784892320633\n",
      "Epoch 2830, Loss: 0.2834346741437912, Final Batch Loss: 0.14645516872406006\n",
      "Epoch 2831, Loss: 0.35986556112766266, Final Batch Loss: 0.1833474338054657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2832, Loss: 0.3134167939424515, Final Batch Loss: 0.12938521802425385\n",
      "Epoch 2833, Loss: 0.30628761649131775, Final Batch Loss: 0.14018581807613373\n",
      "Epoch 2834, Loss: 0.30606289207935333, Final Batch Loss: 0.16940833628177643\n",
      "Epoch 2835, Loss: 0.3431972712278366, Final Batch Loss: 0.20000234246253967\n",
      "Epoch 2836, Loss: 0.35095736384391785, Final Batch Loss: 0.19876264035701752\n",
      "Epoch 2837, Loss: 0.34360435605049133, Final Batch Loss: 0.15713299810886383\n",
      "Epoch 2838, Loss: 0.31312157213687897, Final Batch Loss: 0.15956242382526398\n",
      "Epoch 2839, Loss: 0.37038587033748627, Final Batch Loss: 0.14282946288585663\n",
      "Epoch 2840, Loss: 0.3170025199651718, Final Batch Loss: 0.1384163498878479\n",
      "Epoch 2841, Loss: 0.29913298785686493, Final Batch Loss: 0.1356532722711563\n",
      "Epoch 2842, Loss: 0.3076372742652893, Final Batch Loss: 0.1450105458498001\n",
      "Epoch 2843, Loss: 0.3098810315132141, Final Batch Loss: 0.1540336161851883\n",
      "Epoch 2844, Loss: 0.33678922057151794, Final Batch Loss: 0.13168178498744965\n",
      "Epoch 2845, Loss: 0.33591116964817047, Final Batch Loss: 0.20285779237747192\n",
      "Epoch 2846, Loss: 0.34941549599170685, Final Batch Loss: 0.19056323170661926\n",
      "Epoch 2847, Loss: 0.3000403493642807, Final Batch Loss: 0.16172023117542267\n",
      "Epoch 2848, Loss: 0.3116641864180565, Final Batch Loss: 0.12157610803842545\n",
      "Epoch 2849, Loss: 0.3135300427675247, Final Batch Loss: 0.19357502460479736\n",
      "Epoch 2850, Loss: 0.3550323098897934, Final Batch Loss: 0.18606600165367126\n",
      "Epoch 2851, Loss: 0.3280656486749649, Final Batch Loss: 0.13279803097248077\n",
      "Epoch 2852, Loss: 0.3072149008512497, Final Batch Loss: 0.17554844915866852\n",
      "Epoch 2853, Loss: 0.358756422996521, Final Batch Loss: 0.18600714206695557\n",
      "Epoch 2854, Loss: 0.30928872525691986, Final Batch Loss: 0.16888970136642456\n",
      "Epoch 2855, Loss: 0.29580287635326385, Final Batch Loss: 0.14528468251228333\n",
      "Epoch 2856, Loss: 0.3367506265640259, Final Batch Loss: 0.1902797967195511\n",
      "Epoch 2857, Loss: 0.34362274408340454, Final Batch Loss: 0.18274180591106415\n",
      "Epoch 2858, Loss: 0.37275704741477966, Final Batch Loss: 0.1546625941991806\n",
      "Epoch 2859, Loss: 0.3433118611574173, Final Batch Loss: 0.1654975265264511\n",
      "Epoch 2860, Loss: 0.3049323409795761, Final Batch Loss: 0.1389511674642563\n",
      "Epoch 2861, Loss: 0.32042260468006134, Final Batch Loss: 0.1629379689693451\n",
      "Epoch 2862, Loss: 0.28605684638023376, Final Batch Loss: 0.14571116864681244\n",
      "Epoch 2863, Loss: 0.28838277608156204, Final Batch Loss: 0.11379183083772659\n",
      "Epoch 2864, Loss: 0.3342895805835724, Final Batch Loss: 0.15814363956451416\n",
      "Epoch 2865, Loss: 0.2969917505979538, Final Batch Loss: 0.14695514738559723\n",
      "Epoch 2866, Loss: 0.3354097157716751, Final Batch Loss: 0.18998029828071594\n",
      "Epoch 2867, Loss: 0.3045089989900589, Final Batch Loss: 0.14582711458206177\n",
      "Epoch 2868, Loss: 0.3068487346172333, Final Batch Loss: 0.14672483503818512\n",
      "Epoch 2869, Loss: 0.32200978696346283, Final Batch Loss: 0.16773085296154022\n",
      "Epoch 2870, Loss: 0.3024226725101471, Final Batch Loss: 0.1470918208360672\n",
      "Epoch 2871, Loss: 0.3078377991914749, Final Batch Loss: 0.16847914457321167\n",
      "Epoch 2872, Loss: 0.32753345370292664, Final Batch Loss: 0.16239316761493683\n",
      "Epoch 2873, Loss: 0.3357101231813431, Final Batch Loss: 0.18907040357589722\n",
      "Epoch 2874, Loss: 0.2942954897880554, Final Batch Loss: 0.15203659236431122\n",
      "Epoch 2875, Loss: 0.29610593616962433, Final Batch Loss: 0.12052497267723083\n",
      "Epoch 2876, Loss: 0.29832543432712555, Final Batch Loss: 0.14963020384311676\n",
      "Epoch 2877, Loss: 0.3825588971376419, Final Batch Loss: 0.19548848271369934\n",
      "Epoch 2878, Loss: 0.3233271688222885, Final Batch Loss: 0.18946221470832825\n",
      "Epoch 2879, Loss: 0.2987831234931946, Final Batch Loss: 0.16196435689926147\n",
      "Epoch 2880, Loss: 0.34363827109336853, Final Batch Loss: 0.15807229280471802\n",
      "Epoch 2881, Loss: 0.3423888683319092, Final Batch Loss: 0.19462373852729797\n",
      "Epoch 2882, Loss: 0.33056730031967163, Final Batch Loss: 0.15909923613071442\n",
      "Epoch 2883, Loss: 0.2956139147281647, Final Batch Loss: 0.16854539513587952\n",
      "Epoch 2884, Loss: 0.3186434209346771, Final Batch Loss: 0.16646894812583923\n",
      "Epoch 2885, Loss: 0.29650965332984924, Final Batch Loss: 0.1339302808046341\n",
      "Epoch 2886, Loss: 0.3680907338857651, Final Batch Loss: 0.23135609924793243\n",
      "Epoch 2887, Loss: 0.31844694912433624, Final Batch Loss: 0.15564541518688202\n",
      "Epoch 2888, Loss: 0.2940434366464615, Final Batch Loss: 0.1443750411272049\n",
      "Epoch 2889, Loss: 0.3863895833492279, Final Batch Loss: 0.21567055583000183\n",
      "Epoch 2890, Loss: 0.29806873202323914, Final Batch Loss: 0.14268648624420166\n",
      "Epoch 2891, Loss: 0.3478504717350006, Final Batch Loss: 0.14687328040599823\n",
      "Epoch 2892, Loss: 0.31243982911109924, Final Batch Loss: 0.16163022816181183\n",
      "Epoch 2893, Loss: 0.29455456882715225, Final Batch Loss: 0.11553608626127243\n",
      "Epoch 2894, Loss: 0.32190537452697754, Final Batch Loss: 0.160618856549263\n",
      "Epoch 2895, Loss: 0.3003953546285629, Final Batch Loss: 0.14695969223976135\n",
      "Epoch 2896, Loss: 0.312207892537117, Final Batch Loss: 0.15554854273796082\n",
      "Epoch 2897, Loss: 0.299933522939682, Final Batch Loss: 0.14313456416130066\n",
      "Epoch 2898, Loss: 0.2717302516102791, Final Batch Loss: 0.15473106503486633\n",
      "Epoch 2899, Loss: 0.28674033284187317, Final Batch Loss: 0.1406303197145462\n",
      "Epoch 2900, Loss: 0.31517983973026276, Final Batch Loss: 0.17511186003684998\n",
      "Epoch 2901, Loss: 0.3219223916530609, Final Batch Loss: 0.18210375308990479\n",
      "Epoch 2902, Loss: 0.3326970934867859, Final Batch Loss: 0.18341565132141113\n",
      "Epoch 2903, Loss: 0.3065880239009857, Final Batch Loss: 0.15579116344451904\n",
      "Epoch 2904, Loss: 0.3247968405485153, Final Batch Loss: 0.172332301735878\n",
      "Epoch 2905, Loss: 0.2836950719356537, Final Batch Loss: 0.13200892508029938\n",
      "Epoch 2906, Loss: 0.32810911536216736, Final Batch Loss: 0.16671453416347504\n",
      "Epoch 2907, Loss: 0.3260442465543747, Final Batch Loss: 0.13599486649036407\n",
      "Epoch 2908, Loss: 0.33891761302948, Final Batch Loss: 0.14447693526744843\n",
      "Epoch 2909, Loss: 0.30161651968955994, Final Batch Loss: 0.14406681060791016\n",
      "Epoch 2910, Loss: 0.34558209776878357, Final Batch Loss: 0.21490740776062012\n",
      "Epoch 2911, Loss: 0.38202840089797974, Final Batch Loss: 0.17614206671714783\n",
      "Epoch 2912, Loss: 0.34727999567985535, Final Batch Loss: 0.17077720165252686\n",
      "Epoch 2913, Loss: 0.31444045901298523, Final Batch Loss: 0.16035713255405426\n",
      "Epoch 2914, Loss: 0.31985990703105927, Final Batch Loss: 0.17171022295951843\n",
      "Epoch 2915, Loss: 0.3312195837497711, Final Batch Loss: 0.14160069823265076\n",
      "Epoch 2916, Loss: 0.3122493475675583, Final Batch Loss: 0.16248683631420135\n",
      "Epoch 2917, Loss: 0.3054095357656479, Final Batch Loss: 0.12944085896015167\n",
      "Epoch 2918, Loss: 0.3066868185997009, Final Batch Loss: 0.14037057757377625\n",
      "Epoch 2919, Loss: 0.28412698209285736, Final Batch Loss: 0.14061105251312256\n",
      "Epoch 2920, Loss: 0.3678938001394272, Final Batch Loss: 0.22053375840187073\n",
      "Epoch 2921, Loss: 0.3396873325109482, Final Batch Loss: 0.18690454959869385\n",
      "Epoch 2922, Loss: 0.3091910034418106, Final Batch Loss: 0.16354477405548096\n",
      "Epoch 2923, Loss: 0.30721575021743774, Final Batch Loss: 0.17210206389427185\n",
      "Epoch 2924, Loss: 0.335909828543663, Final Batch Loss: 0.1858549863100052\n",
      "Epoch 2925, Loss: 0.36663420498371124, Final Batch Loss: 0.204341858625412\n",
      "Epoch 2926, Loss: 0.3336615413427353, Final Batch Loss: 0.16436441242694855\n",
      "Epoch 2927, Loss: 0.3097955882549286, Final Batch Loss: 0.15648122131824493\n",
      "Epoch 2928, Loss: 0.3088316023349762, Final Batch Loss: 0.1591474860906601\n",
      "Epoch 2929, Loss: 0.31438152492046356, Final Batch Loss: 0.16398577392101288\n",
      "Epoch 2930, Loss: 0.336589053273201, Final Batch Loss: 0.16630250215530396\n",
      "Epoch 2931, Loss: 0.3604057878255844, Final Batch Loss: 0.19565272331237793\n",
      "Epoch 2932, Loss: 0.3291289657354355, Final Batch Loss: 0.15925313532352448\n",
      "Epoch 2933, Loss: 0.3060374855995178, Final Batch Loss: 0.16331106424331665\n",
      "Epoch 2934, Loss: 0.34834662079811096, Final Batch Loss: 0.18533487617969513\n",
      "Epoch 2935, Loss: 0.32467786967754364, Final Batch Loss: 0.18794667720794678\n",
      "Epoch 2936, Loss: 0.338316410779953, Final Batch Loss: 0.16978691518306732\n",
      "Epoch 2937, Loss: 0.3307213932275772, Final Batch Loss: 0.18329547345638275\n",
      "Epoch 2938, Loss: 0.3415892869234085, Final Batch Loss: 0.18498924374580383\n",
      "Epoch 2939, Loss: 0.3370979577302933, Final Batch Loss: 0.1428382247686386\n",
      "Epoch 2940, Loss: 0.35482022166252136, Final Batch Loss: 0.1771058589220047\n",
      "Epoch 2941, Loss: 0.31119318306446075, Final Batch Loss: 0.16554668545722961\n",
      "Epoch 2942, Loss: 0.3093777447938919, Final Batch Loss: 0.1569894254207611\n",
      "Epoch 2943, Loss: 0.29528871178627014, Final Batch Loss: 0.14119766652584076\n",
      "Epoch 2944, Loss: 0.36080309748649597, Final Batch Loss: 0.2109893560409546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2945, Loss: 0.3006020486354828, Final Batch Loss: 0.15681040287017822\n",
      "Epoch 2946, Loss: 0.33027268946170807, Final Batch Loss: 0.16922487318515778\n",
      "Epoch 2947, Loss: 0.3213518261909485, Final Batch Loss: 0.1763516068458557\n",
      "Epoch 2948, Loss: 0.2942720353603363, Final Batch Loss: 0.13759757578372955\n",
      "Epoch 2949, Loss: 0.2994900643825531, Final Batch Loss: 0.161468967795372\n",
      "Epoch 2950, Loss: 0.3130187839269638, Final Batch Loss: 0.1479322761297226\n",
      "Epoch 2951, Loss: 0.3542003780603409, Final Batch Loss: 0.19975416362285614\n",
      "Epoch 2952, Loss: 0.29366759955883026, Final Batch Loss: 0.1476377248764038\n",
      "Epoch 2953, Loss: 0.3200222998857498, Final Batch Loss: 0.16715247929096222\n",
      "Epoch 2954, Loss: 0.3402228206396103, Final Batch Loss: 0.21077942848205566\n",
      "Epoch 2955, Loss: 0.36133643984794617, Final Batch Loss: 0.18546506762504578\n",
      "Epoch 2956, Loss: 0.3780137151479721, Final Batch Loss: 0.17554602026939392\n",
      "Epoch 2957, Loss: 0.281260684132576, Final Batch Loss: 0.13225804269313812\n",
      "Epoch 2958, Loss: 0.2904585599899292, Final Batch Loss: 0.13176554441452026\n",
      "Epoch 2959, Loss: 0.34527698159217834, Final Batch Loss: 0.14405985176563263\n",
      "Epoch 2960, Loss: 0.33157244324684143, Final Batch Loss: 0.16556808352470398\n",
      "Epoch 2961, Loss: 0.3563913404941559, Final Batch Loss: 0.17307522892951965\n",
      "Epoch 2962, Loss: 0.33230072259902954, Final Batch Loss: 0.17118486762046814\n",
      "Epoch 2963, Loss: 0.30961766839027405, Final Batch Loss: 0.1524311751127243\n",
      "Epoch 2964, Loss: 0.3152722716331482, Final Batch Loss: 0.14758983254432678\n",
      "Epoch 2965, Loss: 0.3138437122106552, Final Batch Loss: 0.1268407553434372\n",
      "Epoch 2966, Loss: 0.29739388823509216, Final Batch Loss: 0.15679511427879333\n",
      "Epoch 2967, Loss: 0.3002585470676422, Final Batch Loss: 0.17445488274097443\n",
      "Epoch 2968, Loss: 0.2961770296096802, Final Batch Loss: 0.12925250828266144\n",
      "Epoch 2969, Loss: 0.33706969022750854, Final Batch Loss: 0.13986487686634064\n",
      "Epoch 2970, Loss: 0.3283253312110901, Final Batch Loss: 0.1823909729719162\n",
      "Epoch 2971, Loss: 0.3001023232936859, Final Batch Loss: 0.13846264779567719\n",
      "Epoch 2972, Loss: 0.3687066286802292, Final Batch Loss: 0.17911310493946075\n",
      "Epoch 2973, Loss: 0.30312030017375946, Final Batch Loss: 0.1527307778596878\n",
      "Epoch 2974, Loss: 0.4154188334941864, Final Batch Loss: 0.17753174901008606\n",
      "Epoch 2975, Loss: 0.3251275420188904, Final Batch Loss: 0.18073996901512146\n",
      "Epoch 2976, Loss: 0.36038754880428314, Final Batch Loss: 0.14323462545871735\n",
      "Epoch 2977, Loss: 0.3445023000240326, Final Batch Loss: 0.1951398402452469\n",
      "Epoch 2978, Loss: 0.3196326047182083, Final Batch Loss: 0.15195776522159576\n",
      "Epoch 2979, Loss: 0.3287077099084854, Final Batch Loss: 0.12576888501644135\n",
      "Epoch 2980, Loss: 0.2836402654647827, Final Batch Loss: 0.14280451834201813\n",
      "Epoch 2981, Loss: 0.33472707867622375, Final Batch Loss: 0.1789027452468872\n",
      "Epoch 2982, Loss: 0.2975835055112839, Final Batch Loss: 0.14731527864933014\n",
      "Epoch 2983, Loss: 0.33081117272377014, Final Batch Loss: 0.15864896774291992\n",
      "Epoch 2984, Loss: 0.32969026267528534, Final Batch Loss: 0.17138142883777618\n",
      "Epoch 2985, Loss: 0.30996352434158325, Final Batch Loss: 0.15709953010082245\n",
      "Epoch 2986, Loss: 0.3494757413864136, Final Batch Loss: 0.17765603959560394\n",
      "Epoch 2987, Loss: 0.29610268771648407, Final Batch Loss: 0.13467220962047577\n",
      "Epoch 2988, Loss: 0.2877012938261032, Final Batch Loss: 0.1288377344608307\n",
      "Epoch 2989, Loss: 0.29850997030735016, Final Batch Loss: 0.139621764421463\n",
      "Epoch 2990, Loss: 0.2885764539241791, Final Batch Loss: 0.14727458357810974\n",
      "Epoch 2991, Loss: 0.326748862862587, Final Batch Loss: 0.16666792333126068\n",
      "Epoch 2992, Loss: 0.3081779032945633, Final Batch Loss: 0.14340415596961975\n",
      "Epoch 2993, Loss: 0.3015277087688446, Final Batch Loss: 0.11842991411685944\n",
      "Epoch 2994, Loss: 0.32095371186733246, Final Batch Loss: 0.15907375514507294\n",
      "Epoch 2995, Loss: 0.3281382769346237, Final Batch Loss: 0.13558419048786163\n",
      "Epoch 2996, Loss: 0.29947949945926666, Final Batch Loss: 0.1455594003200531\n",
      "Epoch 2997, Loss: 0.28685450553894043, Final Batch Loss: 0.1553124040365219\n",
      "Epoch 2998, Loss: 0.36200882494449615, Final Batch Loss: 0.17509061098098755\n",
      "Epoch 2999, Loss: 0.2943619638681412, Final Batch Loss: 0.1344955563545227\n",
      "Epoch 3000, Loss: 0.2816154658794403, Final Batch Loss: 0.12857216596603394\n",
      "Epoch 3001, Loss: 0.30256378650665283, Final Batch Loss: 0.16520582139492035\n",
      "Epoch 3002, Loss: 0.268330454826355, Final Batch Loss: 0.1407451033592224\n",
      "Epoch 3003, Loss: 0.3130347728729248, Final Batch Loss: 0.14267925918102264\n",
      "Epoch 3004, Loss: 0.3255268782377243, Final Batch Loss: 0.15932968258857727\n",
      "Epoch 3005, Loss: 0.2918331027030945, Final Batch Loss: 0.15242615342140198\n",
      "Epoch 3006, Loss: 0.29627376794815063, Final Batch Loss: 0.10921525955200195\n",
      "Epoch 3007, Loss: 0.3270472288131714, Final Batch Loss: 0.19581353664398193\n",
      "Epoch 3008, Loss: 0.2925948053598404, Final Batch Loss: 0.15694722533226013\n",
      "Epoch 3009, Loss: 0.2963588833808899, Final Batch Loss: 0.14823223650455475\n",
      "Epoch 3010, Loss: 0.34088633954524994, Final Batch Loss: 0.1812913715839386\n",
      "Epoch 3011, Loss: 0.3342588245868683, Final Batch Loss: 0.13149522244930267\n",
      "Epoch 3012, Loss: 0.30990253388881683, Final Batch Loss: 0.15011897683143616\n",
      "Epoch 3013, Loss: 0.2935466468334198, Final Batch Loss: 0.13669638335704803\n",
      "Epoch 3014, Loss: 0.27650681883096695, Final Batch Loss: 0.16294698417186737\n",
      "Epoch 3015, Loss: 0.3030777871608734, Final Batch Loss: 0.14575867354869843\n",
      "Epoch 3016, Loss: 0.2621777802705765, Final Batch Loss: 0.12313741445541382\n",
      "Epoch 3017, Loss: 0.34748315811157227, Final Batch Loss: 0.16004778444766998\n",
      "Epoch 3018, Loss: 0.3787586838006973, Final Batch Loss: 0.19721321761608124\n",
      "Epoch 3019, Loss: 0.39105452597141266, Final Batch Loss: 0.20082306861877441\n",
      "Epoch 3020, Loss: 0.39484576880931854, Final Batch Loss: 0.19850394129753113\n",
      "Epoch 3021, Loss: 0.2869233936071396, Final Batch Loss: 0.14444273710250854\n",
      "Epoch 3022, Loss: 0.3532983809709549, Final Batch Loss: 0.1776791512966156\n",
      "Epoch 3023, Loss: 0.31166425347328186, Final Batch Loss: 0.17208486795425415\n",
      "Epoch 3024, Loss: 0.3376869112253189, Final Batch Loss: 0.19785107672214508\n",
      "Epoch 3025, Loss: 0.31660254299640656, Final Batch Loss: 0.17866526544094086\n",
      "Epoch 3026, Loss: 0.3010742962360382, Final Batch Loss: 0.1478353887796402\n",
      "Epoch 3027, Loss: 0.3171272426843643, Final Batch Loss: 0.16556060314178467\n",
      "Epoch 3028, Loss: 0.30054304003715515, Final Batch Loss: 0.13573139905929565\n",
      "Epoch 3029, Loss: 0.2760288268327713, Final Batch Loss: 0.1365860551595688\n",
      "Epoch 3030, Loss: 0.3474739193916321, Final Batch Loss: 0.15715031325817108\n",
      "Epoch 3031, Loss: 0.3112200200557709, Final Batch Loss: 0.1643349826335907\n",
      "Epoch 3032, Loss: 0.37093907594680786, Final Batch Loss: 0.2212548851966858\n",
      "Epoch 3033, Loss: 0.32569874823093414, Final Batch Loss: 0.16555368900299072\n",
      "Epoch 3034, Loss: 0.33693796396255493, Final Batch Loss: 0.1939159333705902\n",
      "Epoch 3035, Loss: 0.29579295217990875, Final Batch Loss: 0.1478981077671051\n",
      "Epoch 3036, Loss: 0.3121376931667328, Final Batch Loss: 0.16811127960681915\n",
      "Epoch 3037, Loss: 0.3709863871335983, Final Batch Loss: 0.1903572380542755\n",
      "Epoch 3038, Loss: 0.329519122838974, Final Batch Loss: 0.13129112124443054\n",
      "Epoch 3039, Loss: 0.30973197519779205, Final Batch Loss: 0.14810404181480408\n",
      "Epoch 3040, Loss: 0.3394699692726135, Final Batch Loss: 0.1950068473815918\n",
      "Epoch 3041, Loss: 0.3182479739189148, Final Batch Loss: 0.15321595966815948\n",
      "Epoch 3042, Loss: 0.2992568165063858, Final Batch Loss: 0.16642029583454132\n",
      "Epoch 3043, Loss: 0.2710531875491142, Final Batch Loss: 0.14636331796646118\n",
      "Epoch 3044, Loss: 0.30087222158908844, Final Batch Loss: 0.16806252300739288\n",
      "Epoch 3045, Loss: 0.29032230377197266, Final Batch Loss: 0.15621687471866608\n",
      "Epoch 3046, Loss: 0.3563506454229355, Final Batch Loss: 0.14222444593906403\n",
      "Epoch 3047, Loss: 0.2956536263227463, Final Batch Loss: 0.12861046195030212\n",
      "Epoch 3048, Loss: 0.31307484209537506, Final Batch Loss: 0.14700886607170105\n",
      "Epoch 3049, Loss: 0.28698860108852386, Final Batch Loss: 0.14641891419887543\n",
      "Epoch 3050, Loss: 0.2882537990808487, Final Batch Loss: 0.14862452447414398\n",
      "Epoch 3051, Loss: 0.3418964445590973, Final Batch Loss: 0.1722383350133896\n",
      "Epoch 3052, Loss: 0.296073853969574, Final Batch Loss: 0.1687505841255188\n",
      "Epoch 3053, Loss: 0.300248421728611, Final Batch Loss: 0.18951447308063507\n",
      "Epoch 3054, Loss: 0.27924303710460663, Final Batch Loss: 0.13477852940559387\n",
      "Epoch 3055, Loss: 0.28319449722766876, Final Batch Loss: 0.1619465947151184\n",
      "Epoch 3056, Loss: 0.30992694199085236, Final Batch Loss: 0.12949153780937195\n",
      "Epoch 3057, Loss: 0.2809508740901947, Final Batch Loss: 0.13133186101913452\n",
      "Epoch 3058, Loss: 0.30448129773139954, Final Batch Loss: 0.13324476778507233\n",
      "Epoch 3059, Loss: 0.30840761959552765, Final Batch Loss: 0.17506663501262665\n",
      "Epoch 3060, Loss: 0.29231028258800507, Final Batch Loss: 0.15134261548519135\n",
      "Epoch 3061, Loss: 0.32293473184108734, Final Batch Loss: 0.18609987199306488\n",
      "Epoch 3062, Loss: 0.3018667697906494, Final Batch Loss: 0.1534673571586609\n",
      "Epoch 3063, Loss: 0.3054630160331726, Final Batch Loss: 0.15926867723464966\n",
      "Epoch 3064, Loss: 0.3321540653705597, Final Batch Loss: 0.19532042741775513\n",
      "Epoch 3065, Loss: 0.3398527055978775, Final Batch Loss: 0.16657233238220215\n",
      "Epoch 3066, Loss: 0.3028791695833206, Final Batch Loss: 0.1403781920671463\n",
      "Epoch 3067, Loss: 0.2944779470562935, Final Batch Loss: 0.18197453022003174\n",
      "Epoch 3068, Loss: 0.27814429998397827, Final Batch Loss: 0.131833016872406\n",
      "Epoch 3069, Loss: 0.28444746136665344, Final Batch Loss: 0.1489473134279251\n",
      "Epoch 3070, Loss: 0.2849341630935669, Final Batch Loss: 0.13640598952770233\n",
      "Epoch 3071, Loss: 0.3163779228925705, Final Batch Loss: 0.1395348161458969\n",
      "Epoch 3072, Loss: 0.3075163662433624, Final Batch Loss: 0.15664070844650269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3073, Loss: 0.29289205372333527, Final Batch Loss: 0.1461409032344818\n",
      "Epoch 3074, Loss: 0.36982351541519165, Final Batch Loss: 0.15365402400493622\n",
      "Epoch 3075, Loss: 0.3262646943330765, Final Batch Loss: 0.158892422914505\n",
      "Epoch 3076, Loss: 0.35312721133232117, Final Batch Loss: 0.1582411229610443\n",
      "Epoch 3077, Loss: 0.2861139625310898, Final Batch Loss: 0.1520368903875351\n",
      "Epoch 3078, Loss: 0.2961730509996414, Final Batch Loss: 0.14543290436267853\n",
      "Epoch 3079, Loss: 0.34728723764419556, Final Batch Loss: 0.18090049922466278\n",
      "Epoch 3080, Loss: 0.3308895081281662, Final Batch Loss: 0.1663428395986557\n",
      "Epoch 3081, Loss: 0.37004803121089935, Final Batch Loss: 0.21471141278743744\n",
      "Epoch 3082, Loss: 0.27722474932670593, Final Batch Loss: 0.1276671290397644\n",
      "Epoch 3083, Loss: 0.3164428770542145, Final Batch Loss: 0.1667391061782837\n",
      "Epoch 3084, Loss: 0.3134242743253708, Final Batch Loss: 0.1766800880432129\n",
      "Epoch 3085, Loss: 0.29495082795619965, Final Batch Loss: 0.14371982216835022\n",
      "Epoch 3086, Loss: 0.276160791516304, Final Batch Loss: 0.15035203099250793\n",
      "Epoch 3087, Loss: 0.310153990983963, Final Batch Loss: 0.136135071516037\n",
      "Epoch 3088, Loss: 0.3216106444597244, Final Batch Loss: 0.13872304558753967\n",
      "Epoch 3089, Loss: 0.3023049086332321, Final Batch Loss: 0.14710570871829987\n",
      "Epoch 3090, Loss: 0.29625943303108215, Final Batch Loss: 0.16404461860656738\n",
      "Epoch 3091, Loss: 0.3013278692960739, Final Batch Loss: 0.1551286280155182\n",
      "Epoch 3092, Loss: 0.29621829092502594, Final Batch Loss: 0.14969293773174286\n",
      "Epoch 3093, Loss: 0.3554925322532654, Final Batch Loss: 0.18324501812458038\n",
      "Epoch 3094, Loss: 0.31715600192546844, Final Batch Loss: 0.17967106401920319\n",
      "Epoch 3095, Loss: 0.33023957908153534, Final Batch Loss: 0.1761910766363144\n",
      "Epoch 3096, Loss: 0.3201689124107361, Final Batch Loss: 0.15062366425991058\n",
      "Epoch 3097, Loss: 0.382526695728302, Final Batch Loss: 0.17286185920238495\n",
      "Epoch 3098, Loss: 0.3126954883337021, Final Batch Loss: 0.17380009591579437\n",
      "Epoch 3099, Loss: 0.28259608149528503, Final Batch Loss: 0.13336488604545593\n",
      "Epoch 3100, Loss: 0.2897634655237198, Final Batch Loss: 0.14183227717876434\n",
      "Epoch 3101, Loss: 0.31309284269809723, Final Batch Loss: 0.15845368802547455\n",
      "Epoch 3102, Loss: 0.31505313515663147, Final Batch Loss: 0.13017569482326508\n",
      "Epoch 3103, Loss: 0.29196274280548096, Final Batch Loss: 0.14046244323253632\n",
      "Epoch 3104, Loss: 0.3092354089021683, Final Batch Loss: 0.16205176711082458\n",
      "Epoch 3105, Loss: 0.29978521168231964, Final Batch Loss: 0.15565255284309387\n",
      "Epoch 3106, Loss: 0.3417462706565857, Final Batch Loss: 0.15667136013507843\n",
      "Epoch 3107, Loss: 0.29354919493198395, Final Batch Loss: 0.14062879979610443\n",
      "Epoch 3108, Loss: 0.2866232097148895, Final Batch Loss: 0.14238542318344116\n",
      "Epoch 3109, Loss: 0.3205295205116272, Final Batch Loss: 0.1533505767583847\n",
      "Epoch 3110, Loss: 0.30834510922431946, Final Batch Loss: 0.1438841074705124\n",
      "Epoch 3111, Loss: 0.309562012553215, Final Batch Loss: 0.14230355620384216\n",
      "Epoch 3112, Loss: 0.3267751634120941, Final Batch Loss: 0.17655344307422638\n",
      "Epoch 3113, Loss: 0.2832045257091522, Final Batch Loss: 0.13972283899784088\n",
      "Epoch 3114, Loss: 0.31852875649929047, Final Batch Loss: 0.1427968144416809\n",
      "Epoch 3115, Loss: 0.31715939939022064, Final Batch Loss: 0.16076403856277466\n",
      "Epoch 3116, Loss: 0.3602941334247589, Final Batch Loss: 0.16891434788703918\n",
      "Epoch 3117, Loss: 0.27663014829158783, Final Batch Loss: 0.1336490511894226\n",
      "Epoch 3118, Loss: 0.30508826673030853, Final Batch Loss: 0.1532466560602188\n",
      "Epoch 3119, Loss: 0.31402094662189484, Final Batch Loss: 0.17192581295967102\n",
      "Epoch 3120, Loss: 0.310580238699913, Final Batch Loss: 0.11505749821662903\n",
      "Epoch 3121, Loss: 0.3088403195142746, Final Batch Loss: 0.17791810631752014\n",
      "Epoch 3122, Loss: 0.3879586309194565, Final Batch Loss: 0.22728930413722992\n",
      "Epoch 3123, Loss: 0.33627021312713623, Final Batch Loss: 0.1733333319425583\n",
      "Epoch 3124, Loss: 0.32746200263500214, Final Batch Loss: 0.1415502429008484\n",
      "Epoch 3125, Loss: 0.3932436853647232, Final Batch Loss: 0.18582797050476074\n",
      "Epoch 3126, Loss: 0.36304739117622375, Final Batch Loss: 0.18305350840091705\n",
      "Epoch 3127, Loss: 0.3041665405035019, Final Batch Loss: 0.14916329085826874\n",
      "Epoch 3128, Loss: 0.3318250924348831, Final Batch Loss: 0.16590650379657745\n",
      "Epoch 3129, Loss: 0.32808808982372284, Final Batch Loss: 0.19560179114341736\n",
      "Epoch 3130, Loss: 0.30022120475769043, Final Batch Loss: 0.1300133764743805\n",
      "Epoch 3131, Loss: 0.2821633070707321, Final Batch Loss: 0.1401258409023285\n",
      "Epoch 3132, Loss: 0.3327236622571945, Final Batch Loss: 0.15720956027507782\n",
      "Epoch 3133, Loss: 0.30295035243034363, Final Batch Loss: 0.1329105794429779\n",
      "Epoch 3134, Loss: 0.33296191692352295, Final Batch Loss: 0.16660434007644653\n",
      "Epoch 3135, Loss: 0.30441147089004517, Final Batch Loss: 0.15761613845825195\n",
      "Epoch 3136, Loss: 0.31151898205280304, Final Batch Loss: 0.14091770350933075\n",
      "Epoch 3137, Loss: 0.3331497460603714, Final Batch Loss: 0.1391901969909668\n",
      "Epoch 3138, Loss: 0.33217544853687286, Final Batch Loss: 0.1641272008419037\n",
      "Epoch 3139, Loss: 0.3643813729286194, Final Batch Loss: 0.17215923964977264\n",
      "Epoch 3140, Loss: 0.31254787743091583, Final Batch Loss: 0.16055844724178314\n",
      "Epoch 3141, Loss: 0.3031463325023651, Final Batch Loss: 0.13066057860851288\n",
      "Epoch 3142, Loss: 0.30830956995487213, Final Batch Loss: 0.13912640511989594\n",
      "Epoch 3143, Loss: 0.2855651080608368, Final Batch Loss: 0.14246873557567596\n",
      "Epoch 3144, Loss: 0.32381048798561096, Final Batch Loss: 0.15622550249099731\n",
      "Epoch 3145, Loss: 0.33331678807735443, Final Batch Loss: 0.1884240359067917\n",
      "Epoch 3146, Loss: 0.2979189306497574, Final Batch Loss: 0.15314124524593353\n",
      "Epoch 3147, Loss: 0.31988514959812164, Final Batch Loss: 0.1345944106578827\n",
      "Epoch 3148, Loss: 0.34670647978782654, Final Batch Loss: 0.19102324545383453\n",
      "Epoch 3149, Loss: 0.33719879388809204, Final Batch Loss: 0.17225927114486694\n",
      "Epoch 3150, Loss: 0.30307096242904663, Final Batch Loss: 0.12804441154003143\n",
      "Epoch 3151, Loss: 0.30832673609256744, Final Batch Loss: 0.12791594862937927\n",
      "Epoch 3152, Loss: 0.36632947623729706, Final Batch Loss: 0.206511989235878\n",
      "Epoch 3153, Loss: 0.3020678907632828, Final Batch Loss: 0.1532638967037201\n",
      "Epoch 3154, Loss: 0.2867041230201721, Final Batch Loss: 0.1413913369178772\n",
      "Epoch 3155, Loss: 0.28301727771759033, Final Batch Loss: 0.13884495198726654\n",
      "Epoch 3156, Loss: 0.3166602849960327, Final Batch Loss: 0.17114214599132538\n",
      "Epoch 3157, Loss: 0.30534040927886963, Final Batch Loss: 0.15380790829658508\n",
      "Epoch 3158, Loss: 0.3223515450954437, Final Batch Loss: 0.1640096753835678\n",
      "Epoch 3159, Loss: 0.32029271125793457, Final Batch Loss: 0.147293820977211\n",
      "Epoch 3160, Loss: 0.3362547308206558, Final Batch Loss: 0.16642248630523682\n",
      "Epoch 3161, Loss: 0.3174199312925339, Final Batch Loss: 0.1397080272436142\n",
      "Epoch 3162, Loss: 0.3159980922937393, Final Batch Loss: 0.19073759019374847\n",
      "Epoch 3163, Loss: 0.3989967405796051, Final Batch Loss: 0.17195439338684082\n",
      "Epoch 3164, Loss: 0.30983132123947144, Final Batch Loss: 0.144820436835289\n",
      "Epoch 3165, Loss: 0.30935316532850266, Final Batch Loss: 0.12482716888189316\n",
      "Epoch 3166, Loss: 0.35631120204925537, Final Batch Loss: 0.20499597489833832\n",
      "Epoch 3167, Loss: 0.32062752544879913, Final Batch Loss: 0.1436874270439148\n",
      "Epoch 3168, Loss: 0.31492920964956284, Final Batch Loss: 0.19476574659347534\n",
      "Epoch 3169, Loss: 0.3725597411394119, Final Batch Loss: 0.18612366914749146\n",
      "Epoch 3170, Loss: 0.32474610209465027, Final Batch Loss: 0.15372680127620697\n",
      "Epoch 3171, Loss: 0.3141740560531616, Final Batch Loss: 0.1396002322435379\n",
      "Epoch 3172, Loss: 0.3196412920951843, Final Batch Loss: 0.16246014833450317\n",
      "Epoch 3173, Loss: 0.2903108149766922, Final Batch Loss: 0.16396935284137726\n",
      "Epoch 3174, Loss: 0.3170859217643738, Final Batch Loss: 0.14636720716953278\n",
      "Epoch 3175, Loss: 0.3147698640823364, Final Batch Loss: 0.17717580497264862\n",
      "Epoch 3176, Loss: 0.30877865850925446, Final Batch Loss: 0.17392267286777496\n",
      "Epoch 3177, Loss: 0.34449203312397003, Final Batch Loss: 0.1835329383611679\n",
      "Epoch 3178, Loss: 0.30579161643981934, Final Batch Loss: 0.15589748322963715\n",
      "Epoch 3179, Loss: 0.29583433270454407, Final Batch Loss: 0.13470928370952606\n",
      "Epoch 3180, Loss: 0.2915945202112198, Final Batch Loss: 0.15465795993804932\n",
      "Epoch 3181, Loss: 0.29478806257247925, Final Batch Loss: 0.14372371137142181\n",
      "Epoch 3182, Loss: 0.31125858426094055, Final Batch Loss: 0.17475369572639465\n",
      "Epoch 3183, Loss: 0.32058747112751007, Final Batch Loss: 0.16702818870544434\n",
      "Epoch 3184, Loss: 0.2879253178834915, Final Batch Loss: 0.13996024429798126\n",
      "Epoch 3185, Loss: 0.34552910923957825, Final Batch Loss: 0.192849799990654\n",
      "Epoch 3186, Loss: 0.29273464530706406, Final Batch Loss: 0.16927014291286469\n",
      "Epoch 3187, Loss: 0.3393245190382004, Final Batch Loss: 0.144919753074646\n",
      "Epoch 3188, Loss: 0.28520871698856354, Final Batch Loss: 0.15111422538757324\n",
      "Epoch 3189, Loss: 0.3073465973138809, Final Batch Loss: 0.16739754378795624\n",
      "Epoch 3190, Loss: 0.2997606247663498, Final Batch Loss: 0.14593088626861572\n",
      "Epoch 3191, Loss: 0.3815908879041672, Final Batch Loss: 0.18430723249912262\n",
      "Epoch 3192, Loss: 0.3038409501314163, Final Batch Loss: 0.17729878425598145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3193, Loss: 0.30768050253391266, Final Batch Loss: 0.15579771995544434\n",
      "Epoch 3194, Loss: 0.31264515221118927, Final Batch Loss: 0.16751384735107422\n",
      "Epoch 3195, Loss: 0.2861874923110008, Final Batch Loss: 0.12385468930006027\n",
      "Epoch 3196, Loss: 0.29121002554893494, Final Batch Loss: 0.16675731539726257\n",
      "Epoch 3197, Loss: 0.3357063978910446, Final Batch Loss: 0.12836408615112305\n",
      "Epoch 3198, Loss: 0.3643391281366348, Final Batch Loss: 0.1826471984386444\n",
      "Epoch 3199, Loss: 0.3078756183385849, Final Batch Loss: 0.14750032126903534\n",
      "Epoch 3200, Loss: 0.309369757771492, Final Batch Loss: 0.14495058357715607\n",
      "Epoch 3201, Loss: 0.32413700222969055, Final Batch Loss: 0.176095649600029\n",
      "Epoch 3202, Loss: 0.3125615417957306, Final Batch Loss: 0.14372636377811432\n",
      "Epoch 3203, Loss: 0.298812597990036, Final Batch Loss: 0.1535571962594986\n",
      "Epoch 3204, Loss: 0.322415366768837, Final Batch Loss: 0.13905833661556244\n",
      "Epoch 3205, Loss: 0.35032425820827484, Final Batch Loss: 0.20239044725894928\n",
      "Epoch 3206, Loss: 0.2836649939417839, Final Batch Loss: 0.11898844689130783\n",
      "Epoch 3207, Loss: 0.3044101297855377, Final Batch Loss: 0.14984294772148132\n",
      "Epoch 3208, Loss: 0.334869384765625, Final Batch Loss: 0.12493595480918884\n",
      "Epoch 3209, Loss: 0.31213583052158356, Final Batch Loss: 0.15894560515880585\n",
      "Epoch 3210, Loss: 0.3240787535905838, Final Batch Loss: 0.16138678789138794\n",
      "Epoch 3211, Loss: 0.3351084887981415, Final Batch Loss: 0.2025301605463028\n",
      "Epoch 3212, Loss: 0.28608398884534836, Final Batch Loss: 0.16150373220443726\n",
      "Epoch 3213, Loss: 0.27904223650693893, Final Batch Loss: 0.15443669259548187\n",
      "Epoch 3214, Loss: 0.2969031482934952, Final Batch Loss: 0.14733345806598663\n",
      "Epoch 3215, Loss: 0.27637842297554016, Final Batch Loss: 0.1346074640750885\n",
      "Epoch 3216, Loss: 0.28665104508399963, Final Batch Loss: 0.1464824229478836\n",
      "Epoch 3217, Loss: 0.32807113230228424, Final Batch Loss: 0.17573346197605133\n",
      "Epoch 3218, Loss: 0.31294894218444824, Final Batch Loss: 0.17250175774097443\n",
      "Epoch 3219, Loss: 0.30304762721061707, Final Batch Loss: 0.14198291301727295\n",
      "Epoch 3220, Loss: 0.2824906259775162, Final Batch Loss: 0.1546107977628708\n",
      "Epoch 3221, Loss: 0.2841225266456604, Final Batch Loss: 0.12774015963077545\n",
      "Epoch 3222, Loss: 0.28438808023929596, Final Batch Loss: 0.16149486601352692\n",
      "Epoch 3223, Loss: 0.3273203819990158, Final Batch Loss: 0.14802920818328857\n",
      "Epoch 3224, Loss: 0.2813694328069687, Final Batch Loss: 0.14297184348106384\n",
      "Epoch 3225, Loss: 0.3311266303062439, Final Batch Loss: 0.18912285566329956\n",
      "Epoch 3226, Loss: 0.27281518280506134, Final Batch Loss: 0.1301541030406952\n",
      "Epoch 3227, Loss: 0.316096767783165, Final Batch Loss: 0.15631483495235443\n",
      "Epoch 3228, Loss: 0.33705833554267883, Final Batch Loss: 0.1920389086008072\n",
      "Epoch 3229, Loss: 0.32047751545906067, Final Batch Loss: 0.16423237323760986\n",
      "Epoch 3230, Loss: 0.31486064195632935, Final Batch Loss: 0.16366353631019592\n",
      "Epoch 3231, Loss: 0.29831646382808685, Final Batch Loss: 0.13836930692195892\n",
      "Epoch 3232, Loss: 0.28242193162441254, Final Batch Loss: 0.12869751453399658\n",
      "Epoch 3233, Loss: 0.28756536543369293, Final Batch Loss: 0.12476685643196106\n",
      "Epoch 3234, Loss: 0.26925477385520935, Final Batch Loss: 0.14907245337963104\n",
      "Epoch 3235, Loss: 0.3136327564716339, Final Batch Loss: 0.12379144132137299\n",
      "Epoch 3236, Loss: 0.32419897615909576, Final Batch Loss: 0.14354278147220612\n",
      "Epoch 3237, Loss: 0.3083480894565582, Final Batch Loss: 0.16630077362060547\n",
      "Epoch 3238, Loss: 0.31321151554584503, Final Batch Loss: 0.14483213424682617\n",
      "Epoch 3239, Loss: 0.3120213821530342, Final Batch Loss: 0.12419009953737259\n",
      "Epoch 3240, Loss: 0.2947012782096863, Final Batch Loss: 0.15952126681804657\n",
      "Epoch 3241, Loss: 0.29851897805929184, Final Batch Loss: 0.12141638249158859\n",
      "Epoch 3242, Loss: 0.3347843587398529, Final Batch Loss: 0.18817217648029327\n",
      "Epoch 3243, Loss: 0.2912893444299698, Final Batch Loss: 0.13466891646385193\n",
      "Epoch 3244, Loss: 0.3137376308441162, Final Batch Loss: 0.185258686542511\n",
      "Epoch 3245, Loss: 0.3138143718242645, Final Batch Loss: 0.15477780997753143\n",
      "Epoch 3246, Loss: 0.27602849155664444, Final Batch Loss: 0.15854698419570923\n",
      "Epoch 3247, Loss: 0.3029876947402954, Final Batch Loss: 0.14889396727085114\n",
      "Epoch 3248, Loss: 0.33173321187496185, Final Batch Loss: 0.15249040722846985\n",
      "Epoch 3249, Loss: 0.34639178216457367, Final Batch Loss: 0.17667973041534424\n",
      "Epoch 3250, Loss: 0.30653199553489685, Final Batch Loss: 0.15363414585590363\n",
      "Epoch 3251, Loss: 0.32571062445640564, Final Batch Loss: 0.17150388658046722\n",
      "Epoch 3252, Loss: 0.3374880701303482, Final Batch Loss: 0.1386817842721939\n",
      "Epoch 3253, Loss: 0.3058704137802124, Final Batch Loss: 0.17006346583366394\n",
      "Epoch 3254, Loss: 0.38842184841632843, Final Batch Loss: 0.15821518003940582\n",
      "Epoch 3255, Loss: 0.3044314384460449, Final Batch Loss: 0.1665540486574173\n",
      "Epoch 3256, Loss: 0.3288763463497162, Final Batch Loss: 0.17375551164150238\n",
      "Epoch 3257, Loss: 0.2902991324663162, Final Batch Loss: 0.14121000468730927\n",
      "Epoch 3258, Loss: 0.34775158762931824, Final Batch Loss: 0.18414832651615143\n",
      "Epoch 3259, Loss: 0.33682969212532043, Final Batch Loss: 0.14319457113742828\n",
      "Epoch 3260, Loss: 0.29111145436763763, Final Batch Loss: 0.12640248239040375\n",
      "Epoch 3261, Loss: 0.2935124635696411, Final Batch Loss: 0.13011004030704498\n",
      "Epoch 3262, Loss: 0.31653597950935364, Final Batch Loss: 0.12606316804885864\n",
      "Epoch 3263, Loss: 0.32346148788928986, Final Batch Loss: 0.14025431871414185\n",
      "Epoch 3264, Loss: 0.3214111775159836, Final Batch Loss: 0.17059136927127838\n",
      "Epoch 3265, Loss: 0.2983749508857727, Final Batch Loss: 0.1397085040807724\n",
      "Epoch 3266, Loss: 0.2790200412273407, Final Batch Loss: 0.13177751004695892\n",
      "Epoch 3267, Loss: 0.32354864478111267, Final Batch Loss: 0.17771713435649872\n",
      "Epoch 3268, Loss: 0.2787144333124161, Final Batch Loss: 0.14317353069782257\n",
      "Epoch 3269, Loss: 0.2814110368490219, Final Batch Loss: 0.1572519838809967\n",
      "Epoch 3270, Loss: 0.31294186413288116, Final Batch Loss: 0.1625375896692276\n",
      "Epoch 3271, Loss: 0.3015257865190506, Final Batch Loss: 0.16253376007080078\n",
      "Epoch 3272, Loss: 0.26174381375312805, Final Batch Loss: 0.13997666537761688\n",
      "Epoch 3273, Loss: 0.2879103422164917, Final Batch Loss: 0.13524751365184784\n",
      "Epoch 3274, Loss: 0.3346013128757477, Final Batch Loss: 0.20101535320281982\n",
      "Epoch 3275, Loss: 0.2683965787291527, Final Batch Loss: 0.1520884782075882\n",
      "Epoch 3276, Loss: 0.37797610461711884, Final Batch Loss: 0.1703074723482132\n",
      "Epoch 3277, Loss: 0.31307902932167053, Final Batch Loss: 0.15825200080871582\n",
      "Epoch 3278, Loss: 0.32327498495578766, Final Batch Loss: 0.18221525847911835\n",
      "Epoch 3279, Loss: 0.33012399822473526, Final Batch Loss: 0.12476823478937149\n",
      "Epoch 3280, Loss: 0.31324532628059387, Final Batch Loss: 0.13443022966384888\n",
      "Epoch 3281, Loss: 0.2824539691209793, Final Batch Loss: 0.12390799820423126\n",
      "Epoch 3282, Loss: 0.2995084524154663, Final Batch Loss: 0.13471341133117676\n",
      "Epoch 3283, Loss: 0.2866438627243042, Final Batch Loss: 0.11675195395946503\n",
      "Epoch 3284, Loss: 0.261604942381382, Final Batch Loss: 0.13963039219379425\n",
      "Epoch 3285, Loss: 0.25409627705812454, Final Batch Loss: 0.1301659345626831\n",
      "Epoch 3286, Loss: 0.29788926243782043, Final Batch Loss: 0.1696464717388153\n",
      "Epoch 3287, Loss: 0.3318611681461334, Final Batch Loss: 0.13885922729969025\n",
      "Epoch 3288, Loss: 0.2746165096759796, Final Batch Loss: 0.15996843576431274\n",
      "Epoch 3289, Loss: 0.2699529826641083, Final Batch Loss: 0.1433052122592926\n",
      "Epoch 3290, Loss: 0.30113843083381653, Final Batch Loss: 0.1758444905281067\n",
      "Epoch 3291, Loss: 0.29993686079978943, Final Batch Loss: 0.14997945725917816\n",
      "Epoch 3292, Loss: 0.34642066061496735, Final Batch Loss: 0.18906007707118988\n",
      "Epoch 3293, Loss: 0.2874525338411331, Final Batch Loss: 0.1608145833015442\n",
      "Epoch 3294, Loss: 0.2985044866800308, Final Batch Loss: 0.13822390139102936\n",
      "Epoch 3295, Loss: 0.3031782805919647, Final Batch Loss: 0.15156184136867523\n",
      "Epoch 3296, Loss: 0.2821388766169548, Final Batch Loss: 0.11508048325777054\n",
      "Epoch 3297, Loss: 0.3135969787836075, Final Batch Loss: 0.17463381588459015\n",
      "Epoch 3298, Loss: 0.30693961679935455, Final Batch Loss: 0.12164328992366791\n",
      "Epoch 3299, Loss: 0.3640447109937668, Final Batch Loss: 0.20211274921894073\n",
      "Epoch 3300, Loss: 0.2845330685377121, Final Batch Loss: 0.1481536626815796\n",
      "Epoch 3301, Loss: 0.3255520612001419, Final Batch Loss: 0.14686109125614166\n",
      "Epoch 3302, Loss: 0.3045744001865387, Final Batch Loss: 0.1570218801498413\n",
      "Epoch 3303, Loss: 0.3578338623046875, Final Batch Loss: 0.14722755551338196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3304, Loss: 0.30218879878520966, Final Batch Loss: 0.14315426349639893\n",
      "Epoch 3305, Loss: 0.3074926435947418, Final Batch Loss: 0.13617195188999176\n",
      "Epoch 3306, Loss: 0.35952815413475037, Final Batch Loss: 0.16629967093467712\n",
      "Epoch 3307, Loss: 0.3096248358488083, Final Batch Loss: 0.1584821343421936\n",
      "Epoch 3308, Loss: 0.3313712924718857, Final Batch Loss: 0.13271959125995636\n",
      "Epoch 3309, Loss: 0.3470713198184967, Final Batch Loss: 0.18461641669273376\n",
      "Epoch 3310, Loss: 0.2950544208288193, Final Batch Loss: 0.1547144651412964\n",
      "Epoch 3311, Loss: 0.33566658198833466, Final Batch Loss: 0.1552816480398178\n",
      "Epoch 3312, Loss: 0.3291727602481842, Final Batch Loss: 0.1477668583393097\n",
      "Epoch 3313, Loss: 0.3512563556432724, Final Batch Loss: 0.17225337028503418\n",
      "Epoch 3314, Loss: 0.26552368700504303, Final Batch Loss: 0.129014253616333\n",
      "Epoch 3315, Loss: 0.303005188703537, Final Batch Loss: 0.17217490077018738\n",
      "Epoch 3316, Loss: 0.33482038974761963, Final Batch Loss: 0.15114890038967133\n",
      "Epoch 3317, Loss: 0.31234341859817505, Final Batch Loss: 0.1805754005908966\n",
      "Epoch 3318, Loss: 0.2735218107700348, Final Batch Loss: 0.13448366522789001\n",
      "Epoch 3319, Loss: 0.31123027205467224, Final Batch Loss: 0.1440347284078598\n",
      "Epoch 3320, Loss: 0.28014419227838516, Final Batch Loss: 0.17166130244731903\n",
      "Epoch 3321, Loss: 0.2958715558052063, Final Batch Loss: 0.1356286257505417\n",
      "Epoch 3322, Loss: 0.3219779133796692, Final Batch Loss: 0.1691506803035736\n",
      "Epoch 3323, Loss: 0.2991066724061966, Final Batch Loss: 0.142412006855011\n",
      "Epoch 3324, Loss: 0.33020487427711487, Final Batch Loss: 0.12973472476005554\n",
      "Epoch 3325, Loss: 0.29640090465545654, Final Batch Loss: 0.1402473896741867\n",
      "Epoch 3326, Loss: 0.28642968088388443, Final Batch Loss: 0.1700735241174698\n",
      "Epoch 3327, Loss: 0.35839149355888367, Final Batch Loss: 0.16418617963790894\n",
      "Epoch 3328, Loss: 0.2871222570538521, Final Batch Loss: 0.16449753940105438\n",
      "Epoch 3329, Loss: 0.2839846760034561, Final Batch Loss: 0.1322878748178482\n",
      "Epoch 3330, Loss: 0.3130076229572296, Final Batch Loss: 0.17487260699272156\n",
      "Epoch 3331, Loss: 0.3102591186761856, Final Batch Loss: 0.13855138421058655\n",
      "Epoch 3332, Loss: 0.2972963750362396, Final Batch Loss: 0.15057587623596191\n",
      "Epoch 3333, Loss: 0.30132168531417847, Final Batch Loss: 0.14013135433197021\n",
      "Epoch 3334, Loss: 0.29653871059417725, Final Batch Loss: 0.1524655520915985\n",
      "Epoch 3335, Loss: 0.35545799136161804, Final Batch Loss: 0.16310635209083557\n",
      "Epoch 3336, Loss: 0.31628286838531494, Final Batch Loss: 0.1603451520204544\n",
      "Epoch 3337, Loss: 0.3150962144136429, Final Batch Loss: 0.15959639847278595\n",
      "Epoch 3338, Loss: 0.30268721282482147, Final Batch Loss: 0.16472934186458588\n",
      "Epoch 3339, Loss: 0.2883910685777664, Final Batch Loss: 0.15516212582588196\n",
      "Epoch 3340, Loss: 0.3016768544912338, Final Batch Loss: 0.14776459336280823\n",
      "Epoch 3341, Loss: 0.3139204680919647, Final Batch Loss: 0.15817414224147797\n",
      "Epoch 3342, Loss: 0.31094610691070557, Final Batch Loss: 0.1550918072462082\n",
      "Epoch 3343, Loss: 0.2832226902246475, Final Batch Loss: 0.14068789780139923\n",
      "Epoch 3344, Loss: 0.2822340428829193, Final Batch Loss: 0.14059467613697052\n",
      "Epoch 3345, Loss: 0.2707904130220413, Final Batch Loss: 0.13254320621490479\n",
      "Epoch 3346, Loss: 0.32447806000709534, Final Batch Loss: 0.17771734297275543\n",
      "Epoch 3347, Loss: 0.32866470515727997, Final Batch Loss: 0.1794995665550232\n",
      "Epoch 3348, Loss: 0.38107791543006897, Final Batch Loss: 0.23540912568569183\n",
      "Epoch 3349, Loss: 0.3592476397752762, Final Batch Loss: 0.17811232805252075\n",
      "Epoch 3350, Loss: 0.3309657871723175, Final Batch Loss: 0.1660882979631424\n",
      "Epoch 3351, Loss: 0.2904697209596634, Final Batch Loss: 0.12166197597980499\n",
      "Epoch 3352, Loss: 0.32378268241882324, Final Batch Loss: 0.1390054076910019\n",
      "Epoch 3353, Loss: 0.3043689727783203, Final Batch Loss: 0.14743687212467194\n",
      "Epoch 3354, Loss: 0.29880617558956146, Final Batch Loss: 0.15482161939144135\n",
      "Epoch 3355, Loss: 0.29725033044815063, Final Batch Loss: 0.14852742850780487\n",
      "Epoch 3356, Loss: 0.2800210267305374, Final Batch Loss: 0.15279845893383026\n",
      "Epoch 3357, Loss: 0.2697775512933731, Final Batch Loss: 0.1483077108860016\n",
      "Epoch 3358, Loss: 0.2911837249994278, Final Batch Loss: 0.13951772451400757\n",
      "Epoch 3359, Loss: 0.2867371439933777, Final Batch Loss: 0.14278580248355865\n",
      "Epoch 3360, Loss: 0.2935449331998825, Final Batch Loss: 0.15797626972198486\n",
      "Epoch 3361, Loss: 0.31562796235084534, Final Batch Loss: 0.16001440584659576\n",
      "Epoch 3362, Loss: 0.2861548364162445, Final Batch Loss: 0.15925396978855133\n",
      "Epoch 3363, Loss: 0.2777465879917145, Final Batch Loss: 0.15716741979122162\n",
      "Epoch 3364, Loss: 0.31555356085300446, Final Batch Loss: 0.1558806598186493\n",
      "Epoch 3365, Loss: 0.27570727467536926, Final Batch Loss: 0.137681245803833\n",
      "Epoch 3366, Loss: 0.2923280596733093, Final Batch Loss: 0.14584258198738098\n",
      "Epoch 3367, Loss: 0.2869509160518646, Final Batch Loss: 0.13833028078079224\n",
      "Epoch 3368, Loss: 0.2994769215583801, Final Batch Loss: 0.1432572305202484\n",
      "Epoch 3369, Loss: 0.274213582277298, Final Batch Loss: 0.1345241516828537\n",
      "Epoch 3370, Loss: 0.287107452750206, Final Batch Loss: 0.13383111357688904\n",
      "Epoch 3371, Loss: 0.32101815938949585, Final Batch Loss: 0.1901310384273529\n",
      "Epoch 3372, Loss: 0.2907135933637619, Final Batch Loss: 0.14677943289279938\n",
      "Epoch 3373, Loss: 0.3611515909433365, Final Batch Loss: 0.19544529914855957\n",
      "Epoch 3374, Loss: 0.2956344485282898, Final Batch Loss: 0.12759137153625488\n",
      "Epoch 3375, Loss: 0.2748006135225296, Final Batch Loss: 0.14507775008678436\n",
      "Epoch 3376, Loss: 0.32026442885398865, Final Batch Loss: 0.16021211445331573\n",
      "Epoch 3377, Loss: 0.31956347823143005, Final Batch Loss: 0.14141945540905\n",
      "Epoch 3378, Loss: 0.3250424563884735, Final Batch Loss: 0.169867143034935\n",
      "Epoch 3379, Loss: 0.2964460849761963, Final Batch Loss: 0.15549662709236145\n",
      "Epoch 3380, Loss: 0.3208819031715393, Final Batch Loss: 0.12323935329914093\n",
      "Epoch 3381, Loss: 0.36608587205410004, Final Batch Loss: 0.20589600503444672\n",
      "Epoch 3382, Loss: 0.2723294347524643, Final Batch Loss: 0.12507043778896332\n",
      "Epoch 3383, Loss: 0.3219973146915436, Final Batch Loss: 0.1795080602169037\n",
      "Epoch 3384, Loss: 0.3180450201034546, Final Batch Loss: 0.1105111688375473\n",
      "Epoch 3385, Loss: 0.2873827815055847, Final Batch Loss: 0.13300658762454987\n",
      "Epoch 3386, Loss: 0.304385170340538, Final Batch Loss: 0.15257352590560913\n",
      "Epoch 3387, Loss: 0.27121271193027496, Final Batch Loss: 0.11640101671218872\n",
      "Epoch 3388, Loss: 0.36372148990631104, Final Batch Loss: 0.18715332448482513\n",
      "Epoch 3389, Loss: 0.2938564568758011, Final Batch Loss: 0.13120229542255402\n",
      "Epoch 3390, Loss: 0.32810549437999725, Final Batch Loss: 0.12542419135570526\n",
      "Epoch 3391, Loss: 0.2721802741289139, Final Batch Loss: 0.143113374710083\n",
      "Epoch 3392, Loss: 0.30462026596069336, Final Batch Loss: 0.157399982213974\n",
      "Epoch 3393, Loss: 0.29016144573688507, Final Batch Loss: 0.15238548815250397\n",
      "Epoch 3394, Loss: 0.34064172208309174, Final Batch Loss: 0.1593788117170334\n",
      "Epoch 3395, Loss: 0.2858349680900574, Final Batch Loss: 0.1444782316684723\n",
      "Epoch 3396, Loss: 0.28756554424762726, Final Batch Loss: 0.1364383101463318\n",
      "Epoch 3397, Loss: 0.3510998785495758, Final Batch Loss: 0.1646910160779953\n",
      "Epoch 3398, Loss: 0.35588034987449646, Final Batch Loss: 0.1916273534297943\n",
      "Epoch 3399, Loss: 0.31681180000305176, Final Batch Loss: 0.17225004732608795\n",
      "Epoch 3400, Loss: 0.3011404424905777, Final Batch Loss: 0.13743528723716736\n",
      "Epoch 3401, Loss: 0.329876184463501, Final Batch Loss: 0.14447645843029022\n",
      "Epoch 3402, Loss: 0.29646380990743637, Final Batch Loss: 0.10786590725183487\n",
      "Epoch 3403, Loss: 0.3502548336982727, Final Batch Loss: 0.17683365941047668\n",
      "Epoch 3404, Loss: 0.3260868191719055, Final Batch Loss: 0.18357715010643005\n",
      "Epoch 3405, Loss: 0.30284614861011505, Final Batch Loss: 0.13297972083091736\n",
      "Epoch 3406, Loss: 0.29725276678800583, Final Batch Loss: 0.17404507100582123\n",
      "Epoch 3407, Loss: 0.28679364174604416, Final Batch Loss: 0.17522436380386353\n",
      "Epoch 3408, Loss: 0.32841338217258453, Final Batch Loss: 0.17733906209468842\n",
      "Epoch 3409, Loss: 0.30163440108299255, Final Batch Loss: 0.1580410748720169\n",
      "Epoch 3410, Loss: 0.3348532095551491, Final Batch Loss: 0.21098820865154266\n",
      "Epoch 3411, Loss: 0.3801184743642807, Final Batch Loss: 0.1697997897863388\n",
      "Epoch 3412, Loss: 0.2684575319290161, Final Batch Loss: 0.1408480852842331\n",
      "Epoch 3413, Loss: 0.31153111159801483, Final Batch Loss: 0.16711390018463135\n",
      "Epoch 3414, Loss: 0.2664778381586075, Final Batch Loss: 0.13658301532268524\n",
      "Epoch 3415, Loss: 0.2956177890300751, Final Batch Loss: 0.11704471707344055\n",
      "Epoch 3416, Loss: 0.2847672551870346, Final Batch Loss: 0.1628350466489792\n",
      "Epoch 3417, Loss: 0.32433976233005524, Final Batch Loss: 0.1512552797794342\n",
      "Epoch 3418, Loss: 0.32171809673309326, Final Batch Loss: 0.16823501884937286\n",
      "Epoch 3419, Loss: 0.36661824584007263, Final Batch Loss: 0.19632038474082947\n",
      "Epoch 3420, Loss: 0.3168342113494873, Final Batch Loss: 0.1407158225774765\n",
      "Epoch 3421, Loss: 0.2850809842348099, Final Batch Loss: 0.13952301442623138\n",
      "Epoch 3422, Loss: 0.32973046600818634, Final Batch Loss: 0.17560990154743195\n",
      "Epoch 3423, Loss: 0.28773562610149384, Final Batch Loss: 0.16230320930480957\n",
      "Epoch 3424, Loss: 0.34241607785224915, Final Batch Loss: 0.16703630983829498\n",
      "Epoch 3425, Loss: 0.35836996138095856, Final Batch Loss: 0.2000717967748642\n",
      "Epoch 3426, Loss: 0.3105114847421646, Final Batch Loss: 0.16086912155151367\n",
      "Epoch 3427, Loss: 0.2904762104153633, Final Batch Loss: 0.16993698477745056\n",
      "Epoch 3428, Loss: 0.31871287524700165, Final Batch Loss: 0.1575697809457779\n",
      "Epoch 3429, Loss: 0.29378339648246765, Final Batch Loss: 0.14775818586349487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3430, Loss: 0.3167493939399719, Final Batch Loss: 0.15691864490509033\n",
      "Epoch 3431, Loss: 0.2909713089466095, Final Batch Loss: 0.12170998752117157\n",
      "Epoch 3432, Loss: 0.3711809813976288, Final Batch Loss: 0.18328379094600677\n",
      "Epoch 3433, Loss: 0.34096065163612366, Final Batch Loss: 0.20110157132148743\n",
      "Epoch 3434, Loss: 0.29919709265232086, Final Batch Loss: 0.1482035219669342\n",
      "Epoch 3435, Loss: 0.32197242975234985, Final Batch Loss: 0.175779327750206\n",
      "Epoch 3436, Loss: 0.2737453579902649, Final Batch Loss: 0.1407925933599472\n",
      "Epoch 3437, Loss: 0.26689060032367706, Final Batch Loss: 0.13013805449008942\n",
      "Epoch 3438, Loss: 0.321749210357666, Final Batch Loss: 0.13778603076934814\n",
      "Epoch 3439, Loss: 0.2850096970796585, Final Batch Loss: 0.1399276852607727\n",
      "Epoch 3440, Loss: 0.2982286512851715, Final Batch Loss: 0.15658850967884064\n",
      "Epoch 3441, Loss: 0.29867345094680786, Final Batch Loss: 0.13564342260360718\n",
      "Epoch 3442, Loss: 0.29572079330682755, Final Batch Loss: 0.12107501178979874\n",
      "Epoch 3443, Loss: 0.27637018263339996, Final Batch Loss: 0.1450570821762085\n",
      "Epoch 3444, Loss: 0.31230291724205017, Final Batch Loss: 0.1259494423866272\n",
      "Epoch 3445, Loss: 0.30493681132793427, Final Batch Loss: 0.16120180487632751\n",
      "Epoch 3446, Loss: 0.33669133484363556, Final Batch Loss: 0.15221025049686432\n",
      "Epoch 3447, Loss: 0.2722534090280533, Final Batch Loss: 0.11440135538578033\n",
      "Epoch 3448, Loss: 0.29240964353084564, Final Batch Loss: 0.14273987710475922\n",
      "Epoch 3449, Loss: 0.37174805998802185, Final Batch Loss: 0.22688962519168854\n",
      "Epoch 3450, Loss: 0.294340118765831, Final Batch Loss: 0.13647794723510742\n",
      "Epoch 3451, Loss: 0.36721745133399963, Final Batch Loss: 0.1870066225528717\n",
      "Epoch 3452, Loss: 0.2967956066131592, Final Batch Loss: 0.1503862887620926\n",
      "Epoch 3453, Loss: 0.27272479236125946, Final Batch Loss: 0.1350528597831726\n",
      "Epoch 3454, Loss: 0.26458291709423065, Final Batch Loss: 0.13269619643688202\n",
      "Epoch 3455, Loss: 0.30871911346912384, Final Batch Loss: 0.13953347504138947\n",
      "Epoch 3456, Loss: 0.29523730278015137, Final Batch Loss: 0.14153985679149628\n",
      "Epoch 3457, Loss: 0.3230847716331482, Final Batch Loss: 0.16640546917915344\n",
      "Epoch 3458, Loss: 0.3638787418603897, Final Batch Loss: 0.152046337723732\n",
      "Epoch 3459, Loss: 0.2634114846587181, Final Batch Loss: 0.14186599850654602\n",
      "Epoch 3460, Loss: 0.2713499665260315, Final Batch Loss: 0.1303434818983078\n",
      "Epoch 3461, Loss: 0.2779890075325966, Final Batch Loss: 0.15537697076797485\n",
      "Epoch 3462, Loss: 0.3396521210670471, Final Batch Loss: 0.1591840386390686\n",
      "Epoch 3463, Loss: 0.263106070458889, Final Batch Loss: 0.14097312092781067\n",
      "Epoch 3464, Loss: 0.3191263824701309, Final Batch Loss: 0.14161920547485352\n",
      "Epoch 3465, Loss: 0.3253869190812111, Final Batch Loss: 0.12279892712831497\n",
      "Epoch 3466, Loss: 0.28571684658527374, Final Batch Loss: 0.12965363264083862\n",
      "Epoch 3467, Loss: 0.30910391360521317, Final Batch Loss: 0.12149892002344131\n",
      "Epoch 3468, Loss: 0.30554597079753876, Final Batch Loss: 0.1344216912984848\n",
      "Epoch 3469, Loss: 0.29928267002105713, Final Batch Loss: 0.152769073843956\n",
      "Epoch 3470, Loss: 0.29005980491638184, Final Batch Loss: 0.1608095020055771\n",
      "Epoch 3471, Loss: 0.29745516180992126, Final Batch Loss: 0.14687049388885498\n",
      "Epoch 3472, Loss: 0.3030268996953964, Final Batch Loss: 0.1511445790529251\n",
      "Epoch 3473, Loss: 0.26997582614421844, Final Batch Loss: 0.13691633939743042\n",
      "Epoch 3474, Loss: 0.3012092262506485, Final Batch Loss: 0.18139566481113434\n",
      "Epoch 3475, Loss: 0.2866770848631859, Final Batch Loss: 0.1643730252981186\n",
      "Epoch 3476, Loss: 0.30230851471424103, Final Batch Loss: 0.1588333547115326\n",
      "Epoch 3477, Loss: 0.2868253141641617, Final Batch Loss: 0.15176519751548767\n",
      "Epoch 3478, Loss: 0.28985561430454254, Final Batch Loss: 0.12818491458892822\n",
      "Epoch 3479, Loss: 0.28533047437667847, Final Batch Loss: 0.1404794305562973\n",
      "Epoch 3480, Loss: 0.3634175956249237, Final Batch Loss: 0.21294496953487396\n",
      "Epoch 3481, Loss: 0.30466265976428986, Final Batch Loss: 0.1711009293794632\n",
      "Epoch 3482, Loss: 0.33559344708919525, Final Batch Loss: 0.1859143227338791\n",
      "Epoch 3483, Loss: 0.34451600909233093, Final Batch Loss: 0.16759657859802246\n",
      "Epoch 3484, Loss: 0.28465554118156433, Final Batch Loss: 0.13607577979564667\n",
      "Epoch 3485, Loss: 0.322943776845932, Final Batch Loss: 0.17038314044475555\n",
      "Epoch 3486, Loss: 0.3240526616573334, Final Batch Loss: 0.15010568499565125\n",
      "Epoch 3487, Loss: 0.3047712743282318, Final Batch Loss: 0.15414854884147644\n",
      "Epoch 3488, Loss: 0.28629910945892334, Final Batch Loss: 0.13484448194503784\n",
      "Epoch 3489, Loss: 0.27829670906066895, Final Batch Loss: 0.10721901059150696\n",
      "Epoch 3490, Loss: 0.31096190214157104, Final Batch Loss: 0.14121483266353607\n",
      "Epoch 3491, Loss: 0.29819372296333313, Final Batch Loss: 0.14526624977588654\n",
      "Epoch 3492, Loss: 0.25452032685279846, Final Batch Loss: 0.11972737312316895\n",
      "Epoch 3493, Loss: 0.33461281657218933, Final Batch Loss: 0.14570321142673492\n",
      "Epoch 3494, Loss: 0.27242638170719147, Final Batch Loss: 0.14274710416793823\n",
      "Epoch 3495, Loss: 0.30844174325466156, Final Batch Loss: 0.15821848809719086\n",
      "Epoch 3496, Loss: 0.30201275646686554, Final Batch Loss: 0.17374487221240997\n",
      "Epoch 3497, Loss: 0.26778559386730194, Final Batch Loss: 0.16241337358951569\n",
      "Epoch 3498, Loss: 0.290000855922699, Final Batch Loss: 0.14077678322792053\n",
      "Epoch 3499, Loss: 0.3127550631761551, Final Batch Loss: 0.13131219148635864\n",
      "Epoch 3500, Loss: 0.27057357132434845, Final Batch Loss: 0.14962807297706604\n",
      "Epoch 3501, Loss: 0.30108994245529175, Final Batch Loss: 0.13524441421031952\n",
      "Epoch 3502, Loss: 0.30417679250240326, Final Batch Loss: 0.15055415034294128\n",
      "Epoch 3503, Loss: 0.29876498878002167, Final Batch Loss: 0.1393561214208603\n",
      "Epoch 3504, Loss: 0.3316335082054138, Final Batch Loss: 0.1846970170736313\n",
      "Epoch 3505, Loss: 0.2822403833270073, Final Batch Loss: 0.10910926014184952\n",
      "Epoch 3506, Loss: 0.3109712600708008, Final Batch Loss: 0.14539913833141327\n",
      "Epoch 3507, Loss: 0.2961118370294571, Final Batch Loss: 0.16971732676029205\n",
      "Epoch 3508, Loss: 0.3106023073196411, Final Batch Loss: 0.17543691396713257\n",
      "Epoch 3509, Loss: 0.31907224655151367, Final Batch Loss: 0.17669810354709625\n",
      "Epoch 3510, Loss: 0.2915394529700279, Final Batch Loss: 0.16846325993537903\n",
      "Epoch 3511, Loss: 0.3073180019855499, Final Batch Loss: 0.15917940437793732\n",
      "Epoch 3512, Loss: 0.40408873558044434, Final Batch Loss: 0.13107803463935852\n",
      "Epoch 3513, Loss: 0.32008107006549835, Final Batch Loss: 0.17631201446056366\n",
      "Epoch 3514, Loss: 0.328127458691597, Final Batch Loss: 0.1950826644897461\n",
      "Epoch 3515, Loss: 0.28569495677948, Final Batch Loss: 0.13644757866859436\n",
      "Epoch 3516, Loss: 0.28177201747894287, Final Batch Loss: 0.14817677438259125\n",
      "Epoch 3517, Loss: 0.279008150100708, Final Batch Loss: 0.13030116260051727\n",
      "Epoch 3518, Loss: 0.30205532908439636, Final Batch Loss: 0.14410226047039032\n",
      "Epoch 3519, Loss: 0.3208864778280258, Final Batch Loss: 0.1481500267982483\n",
      "Epoch 3520, Loss: 0.31346839666366577, Final Batch Loss: 0.14415331184864044\n",
      "Epoch 3521, Loss: 0.29619312286376953, Final Batch Loss: 0.1686927080154419\n",
      "Epoch 3522, Loss: 0.2847597748041153, Final Batch Loss: 0.1447465419769287\n",
      "Epoch 3523, Loss: 0.29389430582523346, Final Batch Loss: 0.16514621675014496\n",
      "Epoch 3524, Loss: 0.2965031564235687, Final Batch Loss: 0.1372566819190979\n",
      "Epoch 3525, Loss: 0.2815157026052475, Final Batch Loss: 0.13673226535320282\n",
      "Epoch 3526, Loss: 0.2685493230819702, Final Batch Loss: 0.16114848852157593\n",
      "Epoch 3527, Loss: 0.2904418855905533, Final Batch Loss: 0.13257770240306854\n",
      "Epoch 3528, Loss: 0.31434038281440735, Final Batch Loss: 0.1652398407459259\n",
      "Epoch 3529, Loss: 0.3216082751750946, Final Batch Loss: 0.14086702466011047\n",
      "Epoch 3530, Loss: 0.3168424218893051, Final Batch Loss: 0.16576451063156128\n",
      "Epoch 3531, Loss: 0.2556236460804939, Final Batch Loss: 0.10779527574777603\n",
      "Epoch 3532, Loss: 0.2783922776579857, Final Batch Loss: 0.17150373756885529\n",
      "Epoch 3533, Loss: 0.32385192066431046, Final Batch Loss: 0.12094631046056747\n",
      "Epoch 3534, Loss: 0.2950953170657158, Final Batch Loss: 0.11979957669973373\n",
      "Epoch 3535, Loss: 0.262420229613781, Final Batch Loss: 0.10262850672006607\n",
      "Epoch 3536, Loss: 0.28611382842063904, Final Batch Loss: 0.14545302093029022\n",
      "Epoch 3537, Loss: 0.3155860900878906, Final Batch Loss: 0.1696571260690689\n",
      "Epoch 3538, Loss: 0.2759022191166878, Final Batch Loss: 0.15581853687763214\n",
      "Epoch 3539, Loss: 0.2756877839565277, Final Batch Loss: 0.1522160917520523\n",
      "Epoch 3540, Loss: 0.2826429456472397, Final Batch Loss: 0.13710057735443115\n",
      "Epoch 3541, Loss: 0.26810072362422943, Final Batch Loss: 0.14718350768089294\n",
      "Epoch 3542, Loss: 0.32795748114585876, Final Batch Loss: 0.14064379036426544\n",
      "Epoch 3543, Loss: 0.3001818060874939, Final Batch Loss: 0.1639929711818695\n",
      "Epoch 3544, Loss: 0.2786529064178467, Final Batch Loss: 0.11755262315273285\n",
      "Epoch 3545, Loss: 0.28161758184432983, Final Batch Loss: 0.1517913043498993\n",
      "Epoch 3546, Loss: 0.28676117956638336, Final Batch Loss: 0.14130909740924835\n",
      "Epoch 3547, Loss: 0.2791093736886978, Final Batch Loss: 0.1465538591146469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3548, Loss: 0.31912869215011597, Final Batch Loss: 0.1514725238084793\n",
      "Epoch 3549, Loss: 0.27677638828754425, Final Batch Loss: 0.12893758714199066\n",
      "Epoch 3550, Loss: 0.317470520734787, Final Batch Loss: 0.16606897115707397\n",
      "Epoch 3551, Loss: 0.289318710565567, Final Batch Loss: 0.12593919038772583\n",
      "Epoch 3552, Loss: 0.2663184851408005, Final Batch Loss: 0.13303202390670776\n",
      "Epoch 3553, Loss: 0.28870590031147003, Final Batch Loss: 0.12952081859111786\n",
      "Epoch 3554, Loss: 0.27196717262268066, Final Batch Loss: 0.11644198000431061\n",
      "Epoch 3555, Loss: 0.269104927778244, Final Batch Loss: 0.12697023153305054\n",
      "Epoch 3556, Loss: 0.30718669295310974, Final Batch Loss: 0.15633630752563477\n",
      "Epoch 3557, Loss: 0.2793452888727188, Final Batch Loss: 0.12890829145908356\n",
      "Epoch 3558, Loss: 0.28236301243305206, Final Batch Loss: 0.13936299085617065\n",
      "Epoch 3559, Loss: 0.2536775842308998, Final Batch Loss: 0.1299360990524292\n",
      "Epoch 3560, Loss: 0.35748566687107086, Final Batch Loss: 0.20289269089698792\n",
      "Epoch 3561, Loss: 0.26859482377767563, Final Batch Loss: 0.14446380734443665\n",
      "Epoch 3562, Loss: 0.34103168547153473, Final Batch Loss: 0.15572082996368408\n",
      "Epoch 3563, Loss: 0.30316218733787537, Final Batch Loss: 0.12634319067001343\n",
      "Epoch 3564, Loss: 0.300074927508831, Final Batch Loss: 0.18189150094985962\n",
      "Epoch 3565, Loss: 0.36596277356147766, Final Batch Loss: 0.17130902409553528\n",
      "Epoch 3566, Loss: 0.2831566110253334, Final Batch Loss: 0.12494837492704391\n",
      "Epoch 3567, Loss: 0.3105757385492325, Final Batch Loss: 0.13934259116649628\n",
      "Epoch 3568, Loss: 0.35447491705417633, Final Batch Loss: 0.13642454147338867\n",
      "Epoch 3569, Loss: 0.290486142039299, Final Batch Loss: 0.12663857638835907\n",
      "Epoch 3570, Loss: 0.32117633521556854, Final Batch Loss: 0.1606776863336563\n",
      "Epoch 3571, Loss: 0.2740347385406494, Final Batch Loss: 0.13405336439609528\n",
      "Epoch 3572, Loss: 0.3110753148794174, Final Batch Loss: 0.1638229787349701\n",
      "Epoch 3573, Loss: 0.2602473497390747, Final Batch Loss: 0.1189286857843399\n",
      "Epoch 3574, Loss: 0.2862936407327652, Final Batch Loss: 0.12787681818008423\n",
      "Epoch 3575, Loss: 0.2805546969175339, Final Batch Loss: 0.1488039344549179\n",
      "Epoch 3576, Loss: 0.3113100230693817, Final Batch Loss: 0.1401434689760208\n",
      "Epoch 3577, Loss: 0.26289037615060806, Final Batch Loss: 0.11102145165205002\n",
      "Epoch 3578, Loss: 0.27957211434841156, Final Batch Loss: 0.11933726072311401\n",
      "Epoch 3579, Loss: 0.31482575833797455, Final Batch Loss: 0.16984324157238007\n",
      "Epoch 3580, Loss: 0.32327041029930115, Final Batch Loss: 0.15156593918800354\n",
      "Epoch 3581, Loss: 0.3011291176080704, Final Batch Loss: 0.14565157890319824\n",
      "Epoch 3582, Loss: 0.3248387426137924, Final Batch Loss: 0.19031070172786713\n",
      "Epoch 3583, Loss: 0.2787076532840729, Final Batch Loss: 0.13466985523700714\n",
      "Epoch 3584, Loss: 0.30459655821323395, Final Batch Loss: 0.1710251271724701\n",
      "Epoch 3585, Loss: 0.30279114842414856, Final Batch Loss: 0.14212848246097565\n",
      "Epoch 3586, Loss: 0.281871497631073, Final Batch Loss: 0.13208726048469543\n",
      "Epoch 3587, Loss: 0.3338383287191391, Final Batch Loss: 0.14109168946743011\n",
      "Epoch 3588, Loss: 0.29716698080301285, Final Batch Loss: 0.12069890648126602\n",
      "Epoch 3589, Loss: 0.33847595751285553, Final Batch Loss: 0.16896188259124756\n",
      "Epoch 3590, Loss: 0.29881635308265686, Final Batch Loss: 0.11664627492427826\n",
      "Epoch 3591, Loss: 0.3479032963514328, Final Batch Loss: 0.14587509632110596\n",
      "Epoch 3592, Loss: 0.28939515352249146, Final Batch Loss: 0.12628400325775146\n",
      "Epoch 3593, Loss: 0.31697776913642883, Final Batch Loss: 0.17157945036888123\n",
      "Epoch 3594, Loss: 0.3013889491558075, Final Batch Loss: 0.14781342446804047\n",
      "Epoch 3595, Loss: 0.2673982232809067, Final Batch Loss: 0.13377976417541504\n",
      "Epoch 3596, Loss: 0.2828962281346321, Final Batch Loss: 0.15860016644001007\n",
      "Epoch 3597, Loss: 0.30459366738796234, Final Batch Loss: 0.15535816550254822\n",
      "Epoch 3598, Loss: 0.2968145161867142, Final Batch Loss: 0.1658799946308136\n",
      "Epoch 3599, Loss: 0.3165717273950577, Final Batch Loss: 0.1785326898097992\n",
      "Epoch 3600, Loss: 0.2890500873327255, Final Batch Loss: 0.13299888372421265\n",
      "Epoch 3601, Loss: 0.29154662787914276, Final Batch Loss: 0.15328720211982727\n",
      "Epoch 3602, Loss: 0.2764349579811096, Final Batch Loss: 0.12971945106983185\n",
      "Epoch 3603, Loss: 0.3412743955850601, Final Batch Loss: 0.19428622722625732\n",
      "Epoch 3604, Loss: 0.28789184987545013, Final Batch Loss: 0.1549035757780075\n",
      "Epoch 3605, Loss: 0.31287990510463715, Final Batch Loss: 0.15082688629627228\n",
      "Epoch 3606, Loss: 0.3245719224214554, Final Batch Loss: 0.17728754878044128\n",
      "Epoch 3607, Loss: 0.29198886454105377, Final Batch Loss: 0.1209411472082138\n",
      "Epoch 3608, Loss: 0.2640225738286972, Final Batch Loss: 0.1580081433057785\n",
      "Epoch 3609, Loss: 0.2813352942466736, Final Batch Loss: 0.14956343173980713\n",
      "Epoch 3610, Loss: 0.3821467012166977, Final Batch Loss: 0.23787224292755127\n",
      "Epoch 3611, Loss: 0.322651207447052, Final Batch Loss: 0.1497296392917633\n",
      "Epoch 3612, Loss: 0.2961059510707855, Final Batch Loss: 0.15753397345542908\n",
      "Epoch 3613, Loss: 0.31203295290470123, Final Batch Loss: 0.1378285139799118\n",
      "Epoch 3614, Loss: 0.31786905229091644, Final Batch Loss: 0.14270831644535065\n",
      "Epoch 3615, Loss: 0.2530619651079178, Final Batch Loss: 0.11833129823207855\n",
      "Epoch 3616, Loss: 0.33188416063785553, Final Batch Loss: 0.1752135455608368\n",
      "Epoch 3617, Loss: 0.26183662563562393, Final Batch Loss: 0.11069934815168381\n",
      "Epoch 3618, Loss: 0.2955816984176636, Final Batch Loss: 0.16355419158935547\n",
      "Epoch 3619, Loss: 0.28051650524139404, Final Batch Loss: 0.12835386395454407\n",
      "Epoch 3620, Loss: 0.26470866799354553, Final Batch Loss: 0.11480019986629486\n",
      "Epoch 3621, Loss: 0.3686980754137039, Final Batch Loss: 0.19682377576828003\n",
      "Epoch 3622, Loss: 0.27981534600257874, Final Batch Loss: 0.13050930202007294\n",
      "Epoch 3623, Loss: 0.28011585772037506, Final Batch Loss: 0.15815699100494385\n",
      "Epoch 3624, Loss: 0.2830450013279915, Final Batch Loss: 0.1686715930700302\n",
      "Epoch 3625, Loss: 0.2646038830280304, Final Batch Loss: 0.15592065453529358\n",
      "Epoch 3626, Loss: 0.3038804531097412, Final Batch Loss: 0.14717483520507812\n",
      "Epoch 3627, Loss: 0.2651442885398865, Final Batch Loss: 0.13955660164356232\n",
      "Epoch 3628, Loss: 0.2892586290836334, Final Batch Loss: 0.14136579632759094\n",
      "Epoch 3629, Loss: 0.2857906073331833, Final Batch Loss: 0.15499642491340637\n",
      "Epoch 3630, Loss: 0.26109252870082855, Final Batch Loss: 0.11917370557785034\n",
      "Epoch 3631, Loss: 0.3024538457393646, Final Batch Loss: 0.15449510514736176\n",
      "Epoch 3632, Loss: 0.2983880937099457, Final Batch Loss: 0.15755721926689148\n",
      "Epoch 3633, Loss: 0.26139314472675323, Final Batch Loss: 0.1359456479549408\n",
      "Epoch 3634, Loss: 0.31453393399715424, Final Batch Loss: 0.15186499059200287\n",
      "Epoch 3635, Loss: 0.28648515045642853, Final Batch Loss: 0.15054216980934143\n",
      "Epoch 3636, Loss: 0.30072806775569916, Final Batch Loss: 0.12505954504013062\n",
      "Epoch 3637, Loss: 0.2676430195569992, Final Batch Loss: 0.10672050714492798\n",
      "Epoch 3638, Loss: 0.3648316413164139, Final Batch Loss: 0.22464872896671295\n",
      "Epoch 3639, Loss: 0.2742397040128708, Final Batch Loss: 0.13525116443634033\n",
      "Epoch 3640, Loss: 0.3083408325910568, Final Batch Loss: 0.13549081981182098\n",
      "Epoch 3641, Loss: 0.3049093633890152, Final Batch Loss: 0.1765524446964264\n",
      "Epoch 3642, Loss: 0.31441178917884827, Final Batch Loss: 0.17811404168605804\n",
      "Epoch 3643, Loss: 0.2507040649652481, Final Batch Loss: 0.12539763748645782\n",
      "Epoch 3644, Loss: 0.2744484096765518, Final Batch Loss: 0.1604788899421692\n",
      "Epoch 3645, Loss: 0.28659190237522125, Final Batch Loss: 0.16058416664600372\n",
      "Epoch 3646, Loss: 0.3235613703727722, Final Batch Loss: 0.20729126036167145\n",
      "Epoch 3647, Loss: 0.26186443865299225, Final Batch Loss: 0.14736036956310272\n",
      "Epoch 3648, Loss: 0.27846983075141907, Final Batch Loss: 0.12892799079418182\n",
      "Epoch 3649, Loss: 0.3032260537147522, Final Batch Loss: 0.16445718705654144\n",
      "Epoch 3650, Loss: 0.29616688191890717, Final Batch Loss: 0.16534322500228882\n",
      "Epoch 3651, Loss: 0.32536110281944275, Final Batch Loss: 0.17625831067562103\n",
      "Epoch 3652, Loss: 0.311592698097229, Final Batch Loss: 0.14433322846889496\n",
      "Epoch 3653, Loss: 0.36429475247859955, Final Batch Loss: 0.2179235816001892\n",
      "Epoch 3654, Loss: 0.2846861183643341, Final Batch Loss: 0.1489248126745224\n",
      "Epoch 3655, Loss: 0.304344579577446, Final Batch Loss: 0.15252883732318878\n",
      "Epoch 3656, Loss: 0.3366711884737015, Final Batch Loss: 0.18908409774303436\n",
      "Epoch 3657, Loss: 0.25622083991765976, Final Batch Loss: 0.0996517762541771\n",
      "Epoch 3658, Loss: 0.3687531501054764, Final Batch Loss: 0.18899370729923248\n",
      "Epoch 3659, Loss: 0.2892064154148102, Final Batch Loss: 0.1263473480939865\n",
      "Epoch 3660, Loss: 0.2625148966908455, Final Batch Loss: 0.11328480392694473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3661, Loss: 0.294742614030838, Final Batch Loss: 0.13918419182300568\n",
      "Epoch 3662, Loss: 0.3051249533891678, Final Batch Loss: 0.13706688582897186\n",
      "Epoch 3663, Loss: 0.31158968806266785, Final Batch Loss: 0.13892929255962372\n",
      "Epoch 3664, Loss: 0.2503805384039879, Final Batch Loss: 0.13490378856658936\n",
      "Epoch 3665, Loss: 0.28063562512397766, Final Batch Loss: 0.12703022360801697\n",
      "Epoch 3666, Loss: 0.27859485149383545, Final Batch Loss: 0.1553567498922348\n",
      "Epoch 3667, Loss: 0.28259502351284027, Final Batch Loss: 0.13681454956531525\n",
      "Epoch 3668, Loss: 0.2825569361448288, Final Batch Loss: 0.17668141424655914\n",
      "Epoch 3669, Loss: 0.2905982732772827, Final Batch Loss: 0.1557602882385254\n",
      "Epoch 3670, Loss: 0.26364631950855255, Final Batch Loss: 0.13694174587726593\n",
      "Epoch 3671, Loss: 0.27527448534965515, Final Batch Loss: 0.12783442437648773\n",
      "Epoch 3672, Loss: 0.27681665122509, Final Batch Loss: 0.1300060749053955\n",
      "Epoch 3673, Loss: 0.28921516239643097, Final Batch Loss: 0.13414296507835388\n",
      "Epoch 3674, Loss: 0.2986504137516022, Final Batch Loss: 0.135615274310112\n",
      "Epoch 3675, Loss: 0.2781469225883484, Final Batch Loss: 0.1612163782119751\n",
      "Epoch 3676, Loss: 0.3401351720094681, Final Batch Loss: 0.1477966010570526\n",
      "Epoch 3677, Loss: 0.3443099930882454, Final Batch Loss: 0.12322983890771866\n",
      "Epoch 3678, Loss: 0.2878882437944412, Final Batch Loss: 0.13046175241470337\n",
      "Epoch 3679, Loss: 0.3195531368255615, Final Batch Loss: 0.14995574951171875\n",
      "Epoch 3680, Loss: 0.28695322573184967, Final Batch Loss: 0.15644901990890503\n",
      "Epoch 3681, Loss: 0.3116849809885025, Final Batch Loss: 0.156614288687706\n",
      "Epoch 3682, Loss: 0.2910546809434891, Final Batch Loss: 0.12776005268096924\n",
      "Epoch 3683, Loss: 0.2743086665868759, Final Batch Loss: 0.1286398470401764\n",
      "Epoch 3684, Loss: 0.3579711318016052, Final Batch Loss: 0.16599147021770477\n",
      "Epoch 3685, Loss: 0.27572932839393616, Final Batch Loss: 0.1341022402048111\n",
      "Epoch 3686, Loss: 0.282091349363327, Final Batch Loss: 0.13318729400634766\n",
      "Epoch 3687, Loss: 0.292135551571846, Final Batch Loss: 0.16693459451198578\n",
      "Epoch 3688, Loss: 0.34417425096035004, Final Batch Loss: 0.13887092471122742\n",
      "Epoch 3689, Loss: 0.2796969562768936, Final Batch Loss: 0.1428220123052597\n",
      "Epoch 3690, Loss: 0.2933962196111679, Final Batch Loss: 0.13113756477832794\n",
      "Epoch 3691, Loss: 0.2971218079328537, Final Batch Loss: 0.13476544618606567\n",
      "Epoch 3692, Loss: 0.2862294614315033, Final Batch Loss: 0.1291145235300064\n",
      "Epoch 3693, Loss: 0.2829147279262543, Final Batch Loss: 0.13856416940689087\n",
      "Epoch 3694, Loss: 0.23683876544237137, Final Batch Loss: 0.10626745969057083\n",
      "Epoch 3695, Loss: 0.27220238745212555, Final Batch Loss: 0.1456427425146103\n",
      "Epoch 3696, Loss: 0.3202413320541382, Final Batch Loss: 0.16767598688602448\n",
      "Epoch 3697, Loss: 0.2973564863204956, Final Batch Loss: 0.13467133045196533\n",
      "Epoch 3698, Loss: 0.2969140261411667, Final Batch Loss: 0.16718195378780365\n",
      "Epoch 3699, Loss: 0.27253440022468567, Final Batch Loss: 0.1316332221031189\n",
      "Epoch 3700, Loss: 0.3171272277832031, Final Batch Loss: 0.14975805580615997\n",
      "Epoch 3701, Loss: 0.26875917613506317, Final Batch Loss: 0.1398988664150238\n",
      "Epoch 3702, Loss: 0.27308395504951477, Final Batch Loss: 0.14306765794754028\n",
      "Epoch 3703, Loss: 0.30856718122959137, Final Batch Loss: 0.14499472081661224\n",
      "Epoch 3704, Loss: 0.3352906405925751, Final Batch Loss: 0.18978026509284973\n",
      "Epoch 3705, Loss: 0.26626987755298615, Final Batch Loss: 0.12723010778427124\n",
      "Epoch 3706, Loss: 0.28031058609485626, Final Batch Loss: 0.13045527040958405\n",
      "Epoch 3707, Loss: 0.29317308962345123, Final Batch Loss: 0.1350896954536438\n",
      "Epoch 3708, Loss: 0.26104723662137985, Final Batch Loss: 0.12454473227262497\n",
      "Epoch 3709, Loss: 0.27520234137773514, Final Batch Loss: 0.11956200748682022\n",
      "Epoch 3710, Loss: 0.3375491499900818, Final Batch Loss: 0.18204830586910248\n",
      "Epoch 3711, Loss: 0.2966929152607918, Final Batch Loss: 0.12273239344358444\n",
      "Epoch 3712, Loss: 0.3277892768383026, Final Batch Loss: 0.14126898348331451\n",
      "Epoch 3713, Loss: 0.3003670871257782, Final Batch Loss: 0.14351357519626617\n",
      "Epoch 3714, Loss: 0.2945011854171753, Final Batch Loss: 0.14056366682052612\n",
      "Epoch 3715, Loss: 0.26779547333717346, Final Batch Loss: 0.14443105459213257\n",
      "Epoch 3716, Loss: 0.31001342833042145, Final Batch Loss: 0.15269286930561066\n",
      "Epoch 3717, Loss: 0.2846161872148514, Final Batch Loss: 0.13517937064170837\n",
      "Epoch 3718, Loss: 0.292398139834404, Final Batch Loss: 0.1369311511516571\n",
      "Epoch 3719, Loss: 0.3004194051027298, Final Batch Loss: 0.16163761913776398\n",
      "Epoch 3720, Loss: 0.3434257209300995, Final Batch Loss: 0.1671219915151596\n",
      "Epoch 3721, Loss: 0.2971978709101677, Final Batch Loss: 0.12107909470796585\n",
      "Epoch 3722, Loss: 0.25572773069143295, Final Batch Loss: 0.1388469636440277\n",
      "Epoch 3723, Loss: 0.309104785323143, Final Batch Loss: 0.1527196615934372\n",
      "Epoch 3724, Loss: 0.3576476275920868, Final Batch Loss: 0.190714493393898\n",
      "Epoch 3725, Loss: 0.28897979855537415, Final Batch Loss: 0.1447068452835083\n",
      "Epoch 3726, Loss: 0.30018264055252075, Final Batch Loss: 0.16235163807868958\n",
      "Epoch 3727, Loss: 0.3009312003850937, Final Batch Loss: 0.14286018908023834\n",
      "Epoch 3728, Loss: 0.3111928850412369, Final Batch Loss: 0.16103315353393555\n",
      "Epoch 3729, Loss: 0.2583501413464546, Final Batch Loss: 0.12329231947660446\n",
      "Epoch 3730, Loss: 0.2742578685283661, Final Batch Loss: 0.14344552159309387\n",
      "Epoch 3731, Loss: 0.3519528955221176, Final Batch Loss: 0.1899145543575287\n",
      "Epoch 3732, Loss: 0.28987638652324677, Final Batch Loss: 0.1502240151166916\n",
      "Epoch 3733, Loss: 0.3118675500154495, Final Batch Loss: 0.14945745468139648\n",
      "Epoch 3734, Loss: 0.3844752162694931, Final Batch Loss: 0.20045122504234314\n",
      "Epoch 3735, Loss: 0.35185229778289795, Final Batch Loss: 0.15072965621948242\n",
      "Epoch 3736, Loss: 0.34175126254558563, Final Batch Loss: 0.15842393040657043\n",
      "Epoch 3737, Loss: 0.3124340623617172, Final Batch Loss: 0.13677507638931274\n",
      "Epoch 3738, Loss: 0.2749614119529724, Final Batch Loss: 0.13349781930446625\n",
      "Epoch 3739, Loss: 0.27167344093322754, Final Batch Loss: 0.17414522171020508\n",
      "Epoch 3740, Loss: 0.26173800230026245, Final Batch Loss: 0.14624027907848358\n",
      "Epoch 3741, Loss: 0.31325551867485046, Final Batch Loss: 0.14108102023601532\n",
      "Epoch 3742, Loss: 0.2733972817659378, Final Batch Loss: 0.1455034613609314\n",
      "Epoch 3743, Loss: 0.2808632552623749, Final Batch Loss: 0.12530583143234253\n",
      "Epoch 3744, Loss: 0.2709420174360275, Final Batch Loss: 0.13463722169399261\n",
      "Epoch 3745, Loss: 0.31564846634864807, Final Batch Loss: 0.13113033771514893\n",
      "Epoch 3746, Loss: 0.2935743182897568, Final Batch Loss: 0.14577600359916687\n",
      "Epoch 3747, Loss: 0.3115852028131485, Final Batch Loss: 0.17218953371047974\n",
      "Epoch 3748, Loss: 0.31368695199489594, Final Batch Loss: 0.13775084912776947\n",
      "Epoch 3749, Loss: 0.3078000992536545, Final Batch Loss: 0.17593511939048767\n",
      "Epoch 3750, Loss: 0.28946514427661896, Final Batch Loss: 0.1349443644285202\n",
      "Epoch 3751, Loss: 0.28254324197769165, Final Batch Loss: 0.1464768499135971\n",
      "Epoch 3752, Loss: 0.29884789884090424, Final Batch Loss: 0.1637282371520996\n",
      "Epoch 3753, Loss: 0.27631109952926636, Final Batch Loss: 0.1273220032453537\n",
      "Epoch 3754, Loss: 0.3038226366043091, Final Batch Loss: 0.15376947820186615\n",
      "Epoch 3755, Loss: 0.292510524392128, Final Batch Loss: 0.12354223430156708\n",
      "Epoch 3756, Loss: 0.3157184422016144, Final Batch Loss: 0.16352379322052002\n",
      "Epoch 3757, Loss: 0.29861880838871, Final Batch Loss: 0.16122537851333618\n",
      "Epoch 3758, Loss: 0.3632694035768509, Final Batch Loss: 0.1360243409872055\n",
      "Epoch 3759, Loss: 0.26698730885982513, Final Batch Loss: 0.13909438252449036\n",
      "Epoch 3760, Loss: 0.33022816479206085, Final Batch Loss: 0.16651368141174316\n",
      "Epoch 3761, Loss: 0.2696993425488472, Final Batch Loss: 0.14619368314743042\n",
      "Epoch 3762, Loss: 0.26605209708213806, Final Batch Loss: 0.13319024443626404\n",
      "Epoch 3763, Loss: 0.263608880341053, Final Batch Loss: 0.12258490175008774\n",
      "Epoch 3764, Loss: 0.3094332069158554, Final Batch Loss: 0.129581481218338\n",
      "Epoch 3765, Loss: 0.28717190772295, Final Batch Loss: 0.1802181452512741\n",
      "Epoch 3766, Loss: 0.30193737149238586, Final Batch Loss: 0.15390682220458984\n",
      "Epoch 3767, Loss: 0.2677164450287819, Final Batch Loss: 0.1434394121170044\n",
      "Epoch 3768, Loss: 0.27456556260585785, Final Batch Loss: 0.11628718674182892\n",
      "Epoch 3769, Loss: 0.27978330105543137, Final Batch Loss: 0.10476691275835037\n",
      "Epoch 3770, Loss: 0.30912964046001434, Final Batch Loss: 0.1541205793619156\n",
      "Epoch 3771, Loss: 0.29521726071834564, Final Batch Loss: 0.14104799926280975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3772, Loss: 0.33689136803150177, Final Batch Loss: 0.14262156188488007\n",
      "Epoch 3773, Loss: 0.32348257303237915, Final Batch Loss: 0.19253593683242798\n",
      "Epoch 3774, Loss: 0.28072378039360046, Final Batch Loss: 0.12243923544883728\n",
      "Epoch 3775, Loss: 0.33881881833076477, Final Batch Loss: 0.18353231251239777\n",
      "Epoch 3776, Loss: 0.29001812636852264, Final Batch Loss: 0.1339794248342514\n",
      "Epoch 3777, Loss: 0.26533734798431396, Final Batch Loss: 0.13396437466144562\n",
      "Epoch 3778, Loss: 0.28701692074537277, Final Batch Loss: 0.11195643991231918\n",
      "Epoch 3779, Loss: 0.27394023537635803, Final Batch Loss: 0.12805546820163727\n",
      "Epoch 3780, Loss: 0.31300532817840576, Final Batch Loss: 0.18355682492256165\n",
      "Epoch 3781, Loss: 0.28088295459747314, Final Batch Loss: 0.14844809472560883\n",
      "Epoch 3782, Loss: 0.24761798232793808, Final Batch Loss: 0.12933766841888428\n",
      "Epoch 3783, Loss: 0.29969681799411774, Final Batch Loss: 0.13447025418281555\n",
      "Epoch 3784, Loss: 0.3464115113019943, Final Batch Loss: 0.19024571776390076\n",
      "Epoch 3785, Loss: 0.24323133379220963, Final Batch Loss: 0.12803839147090912\n",
      "Epoch 3786, Loss: 0.3083856552839279, Final Batch Loss: 0.1734912395477295\n",
      "Epoch 3787, Loss: 0.30406592786312103, Final Batch Loss: 0.13880707323551178\n",
      "Epoch 3788, Loss: 0.2833459824323654, Final Batch Loss: 0.1410914808511734\n",
      "Epoch 3789, Loss: 0.2660796046257019, Final Batch Loss: 0.14385047554969788\n",
      "Epoch 3790, Loss: 0.26693515479564667, Final Batch Loss: 0.13828404247760773\n",
      "Epoch 3791, Loss: 0.29534897208213806, Final Batch Loss: 0.16540664434432983\n",
      "Epoch 3792, Loss: 0.29460762441158295, Final Batch Loss: 0.15306413173675537\n",
      "Epoch 3793, Loss: 0.3460405319929123, Final Batch Loss: 0.1974540799856186\n",
      "Epoch 3794, Loss: 0.2567763924598694, Final Batch Loss: 0.12744148075580597\n",
      "Epoch 3795, Loss: 0.2972419112920761, Final Batch Loss: 0.1446574628353119\n",
      "Epoch 3796, Loss: 0.25981399416923523, Final Batch Loss: 0.1427973508834839\n",
      "Epoch 3797, Loss: 0.28679925203323364, Final Batch Loss: 0.14584630727767944\n",
      "Epoch 3798, Loss: 0.3260405361652374, Final Batch Loss: 0.14565813541412354\n",
      "Epoch 3799, Loss: 0.2724258154630661, Final Batch Loss: 0.1425386220216751\n",
      "Epoch 3800, Loss: 0.2986641377210617, Final Batch Loss: 0.1683875024318695\n",
      "Epoch 3801, Loss: 0.28874294459819794, Final Batch Loss: 0.13479268550872803\n",
      "Epoch 3802, Loss: 0.28497734665870667, Final Batch Loss: 0.1403176635503769\n",
      "Epoch 3803, Loss: 0.2704665884375572, Final Batch Loss: 0.1514672487974167\n",
      "Epoch 3804, Loss: 0.2953876554965973, Final Batch Loss: 0.15453530848026276\n",
      "Epoch 3805, Loss: 0.3237780034542084, Final Batch Loss: 0.15427826344966888\n",
      "Epoch 3806, Loss: 0.25594648718833923, Final Batch Loss: 0.11628584563732147\n",
      "Epoch 3807, Loss: 0.26891927421092987, Final Batch Loss: 0.14192230999469757\n",
      "Epoch 3808, Loss: 0.317609041929245, Final Batch Loss: 0.17984357476234436\n",
      "Epoch 3809, Loss: 0.2939665764570236, Final Batch Loss: 0.13414080440998077\n",
      "Epoch 3810, Loss: 0.3217338025569916, Final Batch Loss: 0.18409201502799988\n",
      "Epoch 3811, Loss: 0.2991560399532318, Final Batch Loss: 0.13951432704925537\n",
      "Epoch 3812, Loss: 0.3630656898021698, Final Batch Loss: 0.21083560585975647\n",
      "Epoch 3813, Loss: 0.2837631329894066, Final Batch Loss: 0.1215493455529213\n",
      "Epoch 3814, Loss: 0.3272618502378464, Final Batch Loss: 0.1629357486963272\n",
      "Epoch 3815, Loss: 0.25993313640356064, Final Batch Loss: 0.11283541470766068\n",
      "Epoch 3816, Loss: 0.3105296939611435, Final Batch Loss: 0.13634034991264343\n",
      "Epoch 3817, Loss: 0.2894334942102432, Final Batch Loss: 0.14401227235794067\n",
      "Epoch 3818, Loss: 0.2836022526025772, Final Batch Loss: 0.16732025146484375\n",
      "Epoch 3819, Loss: 0.3123210519552231, Final Batch Loss: 0.1547294408082962\n",
      "Epoch 3820, Loss: 0.2899060845375061, Final Batch Loss: 0.15441051125526428\n",
      "Epoch 3821, Loss: 0.32198381423950195, Final Batch Loss: 0.18363019824028015\n",
      "Epoch 3822, Loss: 0.3172229826450348, Final Batch Loss: 0.15202800929546356\n",
      "Epoch 3823, Loss: 0.27374017238616943, Final Batch Loss: 0.14540863037109375\n",
      "Epoch 3824, Loss: 0.2854335308074951, Final Batch Loss: 0.14555425941944122\n",
      "Epoch 3825, Loss: 0.31967075169086456, Final Batch Loss: 0.14515909552574158\n",
      "Epoch 3826, Loss: 0.3508676812052727, Final Batch Loss: 0.23243361711502075\n",
      "Epoch 3827, Loss: 0.2918974906206131, Final Batch Loss: 0.1489647626876831\n",
      "Epoch 3828, Loss: 0.2819115221500397, Final Batch Loss: 0.1347714364528656\n",
      "Epoch 3829, Loss: 0.3271341919898987, Final Batch Loss: 0.1329670250415802\n",
      "Epoch 3830, Loss: 0.31204529106616974, Final Batch Loss: 0.13480807840824127\n",
      "Epoch 3831, Loss: 0.2303037866950035, Final Batch Loss: 0.11803655326366425\n",
      "Epoch 3832, Loss: 0.28727759420871735, Final Batch Loss: 0.1475261151790619\n",
      "Epoch 3833, Loss: 0.2982165291905403, Final Batch Loss: 0.11568320542573929\n",
      "Epoch 3834, Loss: 0.2594633623957634, Final Batch Loss: 0.1388745754957199\n",
      "Epoch 3835, Loss: 0.274726927280426, Final Batch Loss: 0.1405402570962906\n",
      "Epoch 3836, Loss: 0.3145401030778885, Final Batch Loss: 0.14596393704414368\n",
      "Epoch 3837, Loss: 0.30461108684539795, Final Batch Loss: 0.1197551041841507\n",
      "Epoch 3838, Loss: 0.29865916073322296, Final Batch Loss: 0.15131931006908417\n",
      "Epoch 3839, Loss: 0.34762491285800934, Final Batch Loss: 0.18020795285701752\n",
      "Epoch 3840, Loss: 0.27449508011341095, Final Batch Loss: 0.14408382773399353\n",
      "Epoch 3841, Loss: 0.25336921215057373, Final Batch Loss: 0.11881014704704285\n",
      "Epoch 3842, Loss: 0.29436106979846954, Final Batch Loss: 0.16690608859062195\n",
      "Epoch 3843, Loss: 0.2753690183162689, Final Batch Loss: 0.14296409487724304\n",
      "Epoch 3844, Loss: 0.27710556983947754, Final Batch Loss: 0.1457643359899521\n",
      "Epoch 3845, Loss: 0.2810087278485298, Final Batch Loss: 0.11949682980775833\n",
      "Epoch 3846, Loss: 0.2739819809794426, Final Batch Loss: 0.16702789068222046\n",
      "Epoch 3847, Loss: 0.32077594101428986, Final Batch Loss: 0.15455010533332825\n",
      "Epoch 3848, Loss: 0.31674760580062866, Final Batch Loss: 0.13940374553203583\n",
      "Epoch 3849, Loss: 0.274690143764019, Final Batch Loss: 0.1537388563156128\n",
      "Epoch 3850, Loss: 0.26550596207380295, Final Batch Loss: 0.15714697539806366\n",
      "Epoch 3851, Loss: 0.278091162443161, Final Batch Loss: 0.13874374330043793\n",
      "Epoch 3852, Loss: 0.29373180866241455, Final Batch Loss: 0.1162923276424408\n",
      "Epoch 3853, Loss: 0.3040517419576645, Final Batch Loss: 0.16918545961380005\n",
      "Epoch 3854, Loss: 0.2885352224111557, Final Batch Loss: 0.1487939953804016\n",
      "Epoch 3855, Loss: 0.25074502825737, Final Batch Loss: 0.1321617215871811\n",
      "Epoch 3856, Loss: 0.2839285805821419, Final Batch Loss: 0.1654013991355896\n",
      "Epoch 3857, Loss: 0.3134494721889496, Final Batch Loss: 0.12578630447387695\n",
      "Epoch 3858, Loss: 0.3347317576408386, Final Batch Loss: 0.15040825307369232\n",
      "Epoch 3859, Loss: 0.28752976655960083, Final Batch Loss: 0.14740410447120667\n",
      "Epoch 3860, Loss: 0.3350662440061569, Final Batch Loss: 0.17603272199630737\n",
      "Epoch 3861, Loss: 0.2835504412651062, Final Batch Loss: 0.1439124047756195\n",
      "Epoch 3862, Loss: 0.29644569754600525, Final Batch Loss: 0.13799472153186798\n",
      "Epoch 3863, Loss: 0.2991505265235901, Final Batch Loss: 0.16537010669708252\n",
      "Epoch 3864, Loss: 0.2431384101510048, Final Batch Loss: 0.12338629364967346\n",
      "Epoch 3865, Loss: 0.2841983288526535, Final Batch Loss: 0.16019846498966217\n",
      "Epoch 3866, Loss: 0.36309999227523804, Final Batch Loss: 0.17305304110050201\n",
      "Epoch 3867, Loss: 0.2626781165599823, Final Batch Loss: 0.12174221873283386\n",
      "Epoch 3868, Loss: 0.32880617678165436, Final Batch Loss: 0.19381073117256165\n",
      "Epoch 3869, Loss: 0.28134481608867645, Final Batch Loss: 0.13861997425556183\n",
      "Epoch 3870, Loss: 0.3304905444383621, Final Batch Loss: 0.18643051385879517\n",
      "Epoch 3871, Loss: 0.2680809646844864, Final Batch Loss: 0.1226709634065628\n",
      "Epoch 3872, Loss: 0.25264956802129745, Final Batch Loss: 0.1440199911594391\n",
      "Epoch 3873, Loss: 0.2931363210082054, Final Batch Loss: 0.169514998793602\n",
      "Epoch 3874, Loss: 0.2728823572397232, Final Batch Loss: 0.13022001087665558\n",
      "Epoch 3875, Loss: 0.28467290103435516, Final Batch Loss: 0.15663732588291168\n",
      "Epoch 3876, Loss: 0.34385861456394196, Final Batch Loss: 0.17761358618736267\n",
      "Epoch 3877, Loss: 0.2879743054509163, Final Batch Loss: 0.1650545597076416\n",
      "Epoch 3878, Loss: 0.2770974040031433, Final Batch Loss: 0.1276269406080246\n",
      "Epoch 3879, Loss: 0.35043781995773315, Final Batch Loss: 0.20836597681045532\n",
      "Epoch 3880, Loss: 0.3261284828186035, Final Batch Loss: 0.17609870433807373\n",
      "Epoch 3881, Loss: 0.31693918257951736, Final Batch Loss: 0.19785384833812714\n",
      "Epoch 3882, Loss: 0.3590037375688553, Final Batch Loss: 0.133076012134552\n",
      "Epoch 3883, Loss: 0.2942003607749939, Final Batch Loss: 0.1427920162677765\n",
      "Epoch 3884, Loss: 0.3189888298511505, Final Batch Loss: 0.1851988583803177\n",
      "Epoch 3885, Loss: 0.2548287510871887, Final Batch Loss: 0.12617191672325134\n",
      "Epoch 3886, Loss: 0.3147897720336914, Final Batch Loss: 0.16898593306541443\n",
      "Epoch 3887, Loss: 0.31658199429512024, Final Batch Loss: 0.17343081533908844\n",
      "Epoch 3888, Loss: 0.35395458340644836, Final Batch Loss: 0.137539342045784\n",
      "Epoch 3889, Loss: 0.32356932759284973, Final Batch Loss: 0.16347543895244598\n",
      "Epoch 3890, Loss: 0.27556490153074265, Final Batch Loss: 0.12296891957521439\n",
      "Epoch 3891, Loss: 0.30864933133125305, Final Batch Loss: 0.18204942345619202\n",
      "Epoch 3892, Loss: 0.3417251259088516, Final Batch Loss: 0.16521503031253815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3893, Loss: 0.315332293510437, Final Batch Loss: 0.1490701138973236\n",
      "Epoch 3894, Loss: 0.2716071456670761, Final Batch Loss: 0.1373208910226822\n",
      "Epoch 3895, Loss: 0.3124057799577713, Final Batch Loss: 0.15654487907886505\n",
      "Epoch 3896, Loss: 0.31664395332336426, Final Batch Loss: 0.1570833921432495\n",
      "Epoch 3897, Loss: 0.2674415186047554, Final Batch Loss: 0.12370870262384415\n",
      "Epoch 3898, Loss: 0.3210315555334091, Final Batch Loss: 0.1793213039636612\n",
      "Epoch 3899, Loss: 0.3425452709197998, Final Batch Loss: 0.1477735936641693\n",
      "Epoch 3900, Loss: 0.29589763283729553, Final Batch Loss: 0.13483966886997223\n",
      "Epoch 3901, Loss: 0.26741793751716614, Final Batch Loss: 0.15513131022453308\n",
      "Epoch 3902, Loss: 0.28299789875745773, Final Batch Loss: 0.11807096749544144\n",
      "Epoch 3903, Loss: 0.2770608812570572, Final Batch Loss: 0.13099247217178345\n",
      "Epoch 3904, Loss: 0.27277256548404694, Final Batch Loss: 0.14700597524642944\n",
      "Epoch 3905, Loss: 0.29327940940856934, Final Batch Loss: 0.13782179355621338\n",
      "Epoch 3906, Loss: 0.2523307800292969, Final Batch Loss: 0.12699683010578156\n",
      "Epoch 3907, Loss: 0.27531301975250244, Final Batch Loss: 0.1433286815881729\n",
      "Epoch 3908, Loss: 0.27213820070028305, Final Batch Loss: 0.15971730649471283\n",
      "Epoch 3909, Loss: 0.2591991722583771, Final Batch Loss: 0.1466260850429535\n",
      "Epoch 3910, Loss: 0.2807614430785179, Final Batch Loss: 0.1620302051305771\n",
      "Epoch 3911, Loss: 0.2608053535223007, Final Batch Loss: 0.12701481580734253\n",
      "Epoch 3912, Loss: 0.30561573803424835, Final Batch Loss: 0.15755240619182587\n",
      "Epoch 3913, Loss: 0.3264421969652176, Final Batch Loss: 0.16512566804885864\n",
      "Epoch 3914, Loss: 0.3044465333223343, Final Batch Loss: 0.19034446775913239\n",
      "Epoch 3915, Loss: 0.3179679363965988, Final Batch Loss: 0.17391856014728546\n",
      "Epoch 3916, Loss: 0.24691705405712128, Final Batch Loss: 0.12034843862056732\n",
      "Epoch 3917, Loss: 0.2676217257976532, Final Batch Loss: 0.11993467807769775\n",
      "Epoch 3918, Loss: 0.2795086205005646, Final Batch Loss: 0.12967190146446228\n",
      "Epoch 3919, Loss: 0.26355622708797455, Final Batch Loss: 0.12519073486328125\n",
      "Epoch 3920, Loss: 0.2600606679916382, Final Batch Loss: 0.13081993162631989\n",
      "Epoch 3921, Loss: 0.24958481639623642, Final Batch Loss: 0.11895052343606949\n",
      "Epoch 3922, Loss: 0.2494860365986824, Final Batch Loss: 0.13526996970176697\n",
      "Epoch 3923, Loss: 0.2564207389950752, Final Batch Loss: 0.13707461953163147\n",
      "Epoch 3924, Loss: 0.28119857609272003, Final Batch Loss: 0.14636069536209106\n",
      "Epoch 3925, Loss: 0.2932615280151367, Final Batch Loss: 0.14748328924179077\n",
      "Epoch 3926, Loss: 0.30008216202259064, Final Batch Loss: 0.15348845720291138\n",
      "Epoch 3927, Loss: 0.30990660190582275, Final Batch Loss: 0.11915203928947449\n",
      "Epoch 3928, Loss: 0.3234569877386093, Final Batch Loss: 0.15192174911499023\n",
      "Epoch 3929, Loss: 0.25834721326828003, Final Batch Loss: 0.12202547490596771\n",
      "Epoch 3930, Loss: 0.2994174659252167, Final Batch Loss: 0.18173150718212128\n",
      "Epoch 3931, Loss: 0.29330742359161377, Final Batch Loss: 0.12257519364356995\n",
      "Epoch 3932, Loss: 0.2895887792110443, Final Batch Loss: 0.16071616113185883\n",
      "Epoch 3933, Loss: 0.2516390308737755, Final Batch Loss: 0.12306142598390579\n",
      "Epoch 3934, Loss: 0.25792400538921356, Final Batch Loss: 0.11858673393726349\n",
      "Epoch 3935, Loss: 0.26168590784072876, Final Batch Loss: 0.13356554508209229\n",
      "Epoch 3936, Loss: 0.2800598740577698, Final Batch Loss: 0.12992918491363525\n",
      "Epoch 3937, Loss: 0.2991154193878174, Final Batch Loss: 0.15261046588420868\n",
      "Epoch 3938, Loss: 0.2788624167442322, Final Batch Loss: 0.1505187451839447\n",
      "Epoch 3939, Loss: 0.28095220029354095, Final Batch Loss: 0.1169104129076004\n",
      "Epoch 3940, Loss: 0.3582294285297394, Final Batch Loss: 0.1901775300502777\n",
      "Epoch 3941, Loss: 0.2844140827655792, Final Batch Loss: 0.1534097045660019\n",
      "Epoch 3942, Loss: 0.27343807369470596, Final Batch Loss: 0.11541933566331863\n",
      "Epoch 3943, Loss: 0.2785494104027748, Final Batch Loss: 0.1187172457575798\n",
      "Epoch 3944, Loss: 0.27857905626296997, Final Batch Loss: 0.12682317197322845\n",
      "Epoch 3945, Loss: 0.3082360178232193, Final Batch Loss: 0.14346548914909363\n",
      "Epoch 3946, Loss: 0.28585590422153473, Final Batch Loss: 0.12210173904895782\n",
      "Epoch 3947, Loss: 0.2507501393556595, Final Batch Loss: 0.14476896822452545\n",
      "Epoch 3948, Loss: 0.26686176657676697, Final Batch Loss: 0.11270807683467865\n",
      "Epoch 3949, Loss: 0.2926555424928665, Final Batch Loss: 0.1102323830127716\n",
      "Epoch 3950, Loss: 0.39114271104335785, Final Batch Loss: 0.13942821323871613\n",
      "Epoch 3951, Loss: 0.3031010329723358, Final Batch Loss: 0.1698027402162552\n",
      "Epoch 3952, Loss: 0.25919296592473984, Final Batch Loss: 0.12205339223146439\n",
      "Epoch 3953, Loss: 0.2994800955057144, Final Batch Loss: 0.15931494534015656\n",
      "Epoch 3954, Loss: 0.2842509001493454, Final Batch Loss: 0.15503057837486267\n",
      "Epoch 3955, Loss: 0.2915436327457428, Final Batch Loss: 0.13103561103343964\n",
      "Epoch 3956, Loss: 0.25143441557884216, Final Batch Loss: 0.12686121463775635\n",
      "Epoch 3957, Loss: 0.2547355890274048, Final Batch Loss: 0.12758220732212067\n",
      "Epoch 3958, Loss: 0.24811667948961258, Final Batch Loss: 0.14794903993606567\n",
      "Epoch 3959, Loss: 0.32821305096149445, Final Batch Loss: 0.19876199960708618\n",
      "Epoch 3960, Loss: 0.37791505455970764, Final Batch Loss: 0.17440061271190643\n",
      "Epoch 3961, Loss: 0.2533652260899544, Final Batch Loss: 0.13587616384029388\n",
      "Epoch 3962, Loss: 0.28955262899398804, Final Batch Loss: 0.12411226332187653\n",
      "Epoch 3963, Loss: 0.30519331991672516, Final Batch Loss: 0.1459854543209076\n",
      "Epoch 3964, Loss: 0.26059599965810776, Final Batch Loss: 0.13769817352294922\n",
      "Epoch 3965, Loss: 0.26897822320461273, Final Batch Loss: 0.1352476179599762\n",
      "Epoch 3966, Loss: 0.2812376022338867, Final Batch Loss: 0.13568182289600372\n",
      "Epoch 3967, Loss: 0.279547780752182, Final Batch Loss: 0.14278927445411682\n",
      "Epoch 3968, Loss: 0.2591572776436806, Final Batch Loss: 0.14514900743961334\n",
      "Epoch 3969, Loss: 0.2804165184497833, Final Batch Loss: 0.1275598555803299\n",
      "Epoch 3970, Loss: 0.2601992264389992, Final Batch Loss: 0.1217169538140297\n",
      "Epoch 3971, Loss: 0.2893044501543045, Final Batch Loss: 0.15201708674430847\n",
      "Epoch 3972, Loss: 0.26893505454063416, Final Batch Loss: 0.09788501262664795\n",
      "Epoch 3973, Loss: 0.2374144047498703, Final Batch Loss: 0.11491493880748749\n",
      "Epoch 3974, Loss: 0.23720291256904602, Final Batch Loss: 0.11342915892601013\n",
      "Epoch 3975, Loss: 0.3111110180616379, Final Batch Loss: 0.1355830430984497\n",
      "Epoch 3976, Loss: 0.3379315435886383, Final Batch Loss: 0.20275402069091797\n",
      "Epoch 3977, Loss: 0.2660958766937256, Final Batch Loss: 0.13806161284446716\n",
      "Epoch 3978, Loss: 0.2671383321285248, Final Batch Loss: 0.1289672702550888\n",
      "Epoch 3979, Loss: 0.2610577717423439, Final Batch Loss: 0.13625317811965942\n",
      "Epoch 3980, Loss: 0.2500869333744049, Final Batch Loss: 0.14253199100494385\n",
      "Epoch 3981, Loss: 0.29276348650455475, Final Batch Loss: 0.15352487564086914\n",
      "Epoch 3982, Loss: 0.28740203380584717, Final Batch Loss: 0.1447611153125763\n",
      "Epoch 3983, Loss: 0.27677762508392334, Final Batch Loss: 0.15191814303398132\n",
      "Epoch 3984, Loss: 0.2685181498527527, Final Batch Loss: 0.12415255606174469\n",
      "Epoch 3985, Loss: 0.31353315711021423, Final Batch Loss: 0.1470271348953247\n",
      "Epoch 3986, Loss: 0.2739839553833008, Final Batch Loss: 0.14395500719547272\n",
      "Epoch 3987, Loss: 0.30636101961135864, Final Batch Loss: 0.16289208829402924\n",
      "Epoch 3988, Loss: 0.3203880041837692, Final Batch Loss: 0.20187804102897644\n",
      "Epoch 3989, Loss: 0.26915690302848816, Final Batch Loss: 0.14853814244270325\n",
      "Epoch 3990, Loss: 0.2814522385597229, Final Batch Loss: 0.14212742447853088\n",
      "Epoch 3991, Loss: 0.2618447244167328, Final Batch Loss: 0.1276387721300125\n",
      "Epoch 3992, Loss: 0.24171484261751175, Final Batch Loss: 0.12099725008010864\n",
      "Epoch 3993, Loss: 0.2678910493850708, Final Batch Loss: 0.15181685984134674\n",
      "Epoch 3994, Loss: 0.3244769424200058, Final Batch Loss: 0.1497214436531067\n",
      "Epoch 3995, Loss: 0.2815396338701248, Final Batch Loss: 0.11886253952980042\n",
      "Epoch 3996, Loss: 0.30852075666189194, Final Batch Loss: 0.1973455548286438\n",
      "Epoch 3997, Loss: 0.2512340471148491, Final Batch Loss: 0.11940007656812668\n",
      "Epoch 3998, Loss: 0.2784109562635422, Final Batch Loss: 0.175304114818573\n",
      "Epoch 3999, Loss: 0.289403073489666, Final Batch Loss: 0.12427248805761337\n",
      "Epoch 4000, Loss: 0.2955634146928787, Final Batch Loss: 0.14574141800403595\n",
      "Epoch 4001, Loss: 0.25321706384420395, Final Batch Loss: 0.15325064957141876\n",
      "Epoch 4002, Loss: 0.2709599658846855, Final Batch Loss: 0.12443674355745316\n",
      "Epoch 4003, Loss: 0.2681199461221695, Final Batch Loss: 0.11343060433864594\n",
      "Epoch 4004, Loss: 0.32137973606586456, Final Batch Loss: 0.12933063507080078\n",
      "Epoch 4005, Loss: 0.2896689772605896, Final Batch Loss: 0.1351073682308197\n",
      "Epoch 4006, Loss: 0.2483157142996788, Final Batch Loss: 0.11740726977586746\n",
      "Epoch 4007, Loss: 0.32735575735569, Final Batch Loss: 0.17831942439079285\n",
      "Epoch 4008, Loss: 0.26959409564733505, Final Batch Loss: 0.12445085495710373\n",
      "Epoch 4009, Loss: 0.29536721110343933, Final Batch Loss: 0.12395769357681274\n",
      "Epoch 4010, Loss: 0.3146241754293442, Final Batch Loss: 0.18701273202896118\n",
      "Epoch 4011, Loss: 0.3602837473154068, Final Batch Loss: 0.17371854186058044\n",
      "Epoch 4012, Loss: 0.2775324434041977, Final Batch Loss: 0.1453791856765747\n",
      "Epoch 4013, Loss: 0.2610500752925873, Final Batch Loss: 0.1285606324672699\n",
      "Epoch 4014, Loss: 0.26076190918684006, Final Batch Loss: 0.13917633891105652\n",
      "Epoch 4015, Loss: 0.36295507848262787, Final Batch Loss: 0.13200245797634125\n",
      "Epoch 4016, Loss: 0.2814556136727333, Final Batch Loss: 0.12430641800165176\n",
      "Epoch 4017, Loss: 0.23385069519281387, Final Batch Loss: 0.11557900905609131\n",
      "Epoch 4018, Loss: 0.24502865225076675, Final Batch Loss: 0.09597598761320114\n",
      "Epoch 4019, Loss: 0.3296000361442566, Final Batch Loss: 0.1458735167980194\n",
      "Epoch 4020, Loss: 0.31934666633605957, Final Batch Loss: 0.17751137912273407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4021, Loss: 0.27284133434295654, Final Batch Loss: 0.13602107763290405\n",
      "Epoch 4022, Loss: 0.3251754492521286, Final Batch Loss: 0.15173177421092987\n",
      "Epoch 4023, Loss: 0.2792428731918335, Final Batch Loss: 0.12588833272457123\n",
      "Epoch 4024, Loss: 0.23855220526456833, Final Batch Loss: 0.10762516409158707\n",
      "Epoch 4025, Loss: 0.3229020833969116, Final Batch Loss: 0.1582704335451126\n",
      "Epoch 4026, Loss: 0.3104337006807327, Final Batch Loss: 0.13058651983737946\n",
      "Epoch 4027, Loss: 0.28708982467651367, Final Batch Loss: 0.1518651396036148\n",
      "Epoch 4028, Loss: 0.2466122731566429, Final Batch Loss: 0.1286868155002594\n",
      "Epoch 4029, Loss: 0.27160783112049103, Final Batch Loss: 0.12049128115177155\n",
      "Epoch 4030, Loss: 0.27495647221803665, Final Batch Loss: 0.12236399203538895\n",
      "Epoch 4031, Loss: 0.25347066670656204, Final Batch Loss: 0.12152164429426193\n",
      "Epoch 4032, Loss: 0.2582395449280739, Final Batch Loss: 0.14564241468906403\n",
      "Epoch 4033, Loss: 0.25687018036842346, Final Batch Loss: 0.10619775950908661\n",
      "Epoch 4034, Loss: 0.2731541842222214, Final Batch Loss: 0.11044977605342865\n",
      "Epoch 4035, Loss: 0.297234907746315, Final Batch Loss: 0.12911908328533173\n",
      "Epoch 4036, Loss: 0.2852507382631302, Final Batch Loss: 0.1366458237171173\n",
      "Epoch 4037, Loss: 0.29772262275218964, Final Batch Loss: 0.1611633151769638\n",
      "Epoch 4038, Loss: 0.315618060529232, Final Batch Loss: 0.20198778808116913\n",
      "Epoch 4039, Loss: 0.34009651839733124, Final Batch Loss: 0.1798231303691864\n",
      "Epoch 4040, Loss: 0.30112532526254654, Final Batch Loss: 0.17642433941364288\n",
      "Epoch 4041, Loss: 0.267263725399971, Final Batch Loss: 0.11462250351905823\n",
      "Epoch 4042, Loss: 0.2960224449634552, Final Batch Loss: 0.13537606596946716\n",
      "Epoch 4043, Loss: 0.2681114226579666, Final Batch Loss: 0.13327719271183014\n",
      "Epoch 4044, Loss: 0.2941579669713974, Final Batch Loss: 0.16337677836418152\n",
      "Epoch 4045, Loss: 0.25936321914196014, Final Batch Loss: 0.13669121265411377\n",
      "Epoch 4046, Loss: 0.3087327629327774, Final Batch Loss: 0.15753816068172455\n",
      "Epoch 4047, Loss: 0.2428903505206108, Final Batch Loss: 0.1292642205953598\n",
      "Epoch 4048, Loss: 0.2536606341600418, Final Batch Loss: 0.13485786318778992\n",
      "Epoch 4049, Loss: 0.24916505068540573, Final Batch Loss: 0.12848436832427979\n",
      "Epoch 4050, Loss: 0.26721737533807755, Final Batch Loss: 0.10767392069101334\n",
      "Epoch 4051, Loss: 0.255938783288002, Final Batch Loss: 0.13810791075229645\n",
      "Epoch 4052, Loss: 0.2771903872489929, Final Batch Loss: 0.15171518921852112\n",
      "Epoch 4053, Loss: 0.26054084300994873, Final Batch Loss: 0.13095460832118988\n",
      "Epoch 4054, Loss: 0.2672729790210724, Final Batch Loss: 0.13032905757427216\n",
      "Epoch 4055, Loss: 0.2558179348707199, Final Batch Loss: 0.1576002538204193\n",
      "Epoch 4056, Loss: 0.28912384808063507, Final Batch Loss: 0.12881715595722198\n",
      "Epoch 4057, Loss: 0.27427005767822266, Final Batch Loss: 0.1314883530139923\n",
      "Epoch 4058, Loss: 0.2875254154205322, Final Batch Loss: 0.16803139448165894\n",
      "Epoch 4059, Loss: 0.3107113242149353, Final Batch Loss: 0.134100079536438\n",
      "Epoch 4060, Loss: 0.28900571912527084, Final Batch Loss: 0.16508349776268005\n",
      "Epoch 4061, Loss: 0.2715657651424408, Final Batch Loss: 0.13821618258953094\n",
      "Epoch 4062, Loss: 0.2937132865190506, Final Batch Loss: 0.1304093897342682\n",
      "Epoch 4063, Loss: 0.2921701967716217, Final Batch Loss: 0.13126637041568756\n",
      "Epoch 4064, Loss: 0.28192485868930817, Final Batch Loss: 0.14310553669929504\n",
      "Epoch 4065, Loss: 0.30161604285240173, Final Batch Loss: 0.1315290480852127\n",
      "Epoch 4066, Loss: 0.31908103823661804, Final Batch Loss: 0.12896837294101715\n",
      "Epoch 4067, Loss: 0.27855123579502106, Final Batch Loss: 0.1413305699825287\n",
      "Epoch 4068, Loss: 0.2743147313594818, Final Batch Loss: 0.14501526951789856\n",
      "Epoch 4069, Loss: 0.28279441595077515, Final Batch Loss: 0.1442243605852127\n",
      "Epoch 4070, Loss: 0.2796905189752579, Final Batch Loss: 0.15488867461681366\n",
      "Epoch 4071, Loss: 0.26802460849285126, Final Batch Loss: 0.14205507934093475\n",
      "Epoch 4072, Loss: 0.2793598771095276, Final Batch Loss: 0.130521759390831\n",
      "Epoch 4073, Loss: 0.26818057894706726, Final Batch Loss: 0.12361845374107361\n",
      "Epoch 4074, Loss: 0.30160389840602875, Final Batch Loss: 0.14436212182044983\n",
      "Epoch 4075, Loss: 0.31574389338493347, Final Batch Loss: 0.14970989525318146\n",
      "Epoch 4076, Loss: 0.2903877943754196, Final Batch Loss: 0.14985468983650208\n",
      "Epoch 4077, Loss: 0.2693757489323616, Final Batch Loss: 0.14546658098697662\n",
      "Epoch 4078, Loss: 0.27190257608890533, Final Batch Loss: 0.12474319338798523\n",
      "Epoch 4079, Loss: 0.31076034903526306, Final Batch Loss: 0.1464187651872635\n",
      "Epoch 4080, Loss: 0.2941490709781647, Final Batch Loss: 0.153365358710289\n",
      "Epoch 4081, Loss: 0.27063988149166107, Final Batch Loss: 0.13644419610500336\n",
      "Epoch 4082, Loss: 0.2783482000231743, Final Batch Loss: 0.17155037820339203\n",
      "Epoch 4083, Loss: 0.338346928358078, Final Batch Loss: 0.19537605345249176\n",
      "Epoch 4084, Loss: 0.2708856612443924, Final Batch Loss: 0.12937529385089874\n",
      "Epoch 4085, Loss: 0.2569609582424164, Final Batch Loss: 0.12103128433227539\n",
      "Epoch 4086, Loss: 0.29535043239593506, Final Batch Loss: 0.15543608367443085\n",
      "Epoch 4087, Loss: 0.26273347437381744, Final Batch Loss: 0.1352095603942871\n",
      "Epoch 4088, Loss: 0.24873407185077667, Final Batch Loss: 0.11135609447956085\n",
      "Epoch 4089, Loss: 0.2552945837378502, Final Batch Loss: 0.1088070347905159\n",
      "Epoch 4090, Loss: 0.2803920954465866, Final Batch Loss: 0.1457708775997162\n",
      "Epoch 4091, Loss: 0.33270497620105743, Final Batch Loss: 0.15035882592201233\n",
      "Epoch 4092, Loss: 0.29934054613113403, Final Batch Loss: 0.1452479511499405\n",
      "Epoch 4093, Loss: 0.27223459631204605, Final Batch Loss: 0.14735932648181915\n",
      "Epoch 4094, Loss: 0.32181087136268616, Final Batch Loss: 0.15067754685878754\n",
      "Epoch 4095, Loss: 0.2844235673546791, Final Batch Loss: 0.11377158015966415\n",
      "Epoch 4096, Loss: 0.2933397740125656, Final Batch Loss: 0.17702578008174896\n",
      "Epoch 4097, Loss: 0.2997641861438751, Final Batch Loss: 0.17100642621517181\n",
      "Epoch 4098, Loss: 0.24485208094120026, Final Batch Loss: 0.12079265713691711\n",
      "Epoch 4099, Loss: 0.2739601880311966, Final Batch Loss: 0.1372007578611374\n",
      "Epoch 4100, Loss: 0.25433114916086197, Final Batch Loss: 0.1149330660700798\n",
      "Epoch 4101, Loss: 0.29716795682907104, Final Batch Loss: 0.14461514353752136\n",
      "Epoch 4102, Loss: 0.2666887938976288, Final Batch Loss: 0.13933992385864258\n",
      "Epoch 4103, Loss: 0.28168754279613495, Final Batch Loss: 0.15000566840171814\n",
      "Epoch 4104, Loss: 0.27409136295318604, Final Batch Loss: 0.11237795650959015\n",
      "Epoch 4105, Loss: 0.31418783962726593, Final Batch Loss: 0.17536509037017822\n",
      "Epoch 4106, Loss: 0.27349017560482025, Final Batch Loss: 0.1387358456850052\n",
      "Epoch 4107, Loss: 0.27698785811662674, Final Batch Loss: 0.15361997485160828\n",
      "Epoch 4108, Loss: 0.2857344150543213, Final Batch Loss: 0.159926176071167\n",
      "Epoch 4109, Loss: 0.24458729475736618, Final Batch Loss: 0.15578033030033112\n",
      "Epoch 4110, Loss: 0.30264776945114136, Final Batch Loss: 0.15864083170890808\n",
      "Epoch 4111, Loss: 0.2733079567551613, Final Batch Loss: 0.11938097327947617\n",
      "Epoch 4112, Loss: 0.26912884414196014, Final Batch Loss: 0.13987186551094055\n",
      "Epoch 4113, Loss: 0.2934330850839615, Final Batch Loss: 0.16357159614562988\n",
      "Epoch 4114, Loss: 0.2422376275062561, Final Batch Loss: 0.12098761647939682\n",
      "Epoch 4115, Loss: 0.2813306748867035, Final Batch Loss: 0.13273151218891144\n",
      "Epoch 4116, Loss: 0.28757408261299133, Final Batch Loss: 0.12040457129478455\n",
      "Epoch 4117, Loss: 0.25777095556259155, Final Batch Loss: 0.12008841335773468\n",
      "Epoch 4118, Loss: 0.24103599786758423, Final Batch Loss: 0.14467139542102814\n",
      "Epoch 4119, Loss: 0.25329530984163284, Final Batch Loss: 0.12462634593248367\n",
      "Epoch 4120, Loss: 0.2564174681901932, Final Batch Loss: 0.11679443717002869\n",
      "Epoch 4121, Loss: 0.2593921944499016, Final Batch Loss: 0.14598175883293152\n",
      "Epoch 4122, Loss: 0.283724308013916, Final Batch Loss: 0.13103099167346954\n",
      "Epoch 4123, Loss: 0.24811168015003204, Final Batch Loss: 0.1090930700302124\n",
      "Epoch 4124, Loss: 0.23975656926631927, Final Batch Loss: 0.13453923165798187\n",
      "Epoch 4125, Loss: 0.2616605907678604, Final Batch Loss: 0.12837110459804535\n",
      "Epoch 4126, Loss: 0.2779913991689682, Final Batch Loss: 0.12344267964363098\n",
      "Epoch 4127, Loss: 0.24989935755729675, Final Batch Loss: 0.12021854519844055\n",
      "Epoch 4128, Loss: 0.2615341618657112, Final Batch Loss: 0.11953402310609818\n",
      "Epoch 4129, Loss: 0.24300233274698257, Final Batch Loss: 0.1424318552017212\n",
      "Epoch 4130, Loss: 0.2797869145870209, Final Batch Loss: 0.13056744635105133\n",
      "Epoch 4131, Loss: 0.2975124642252922, Final Batch Loss: 0.11559449881315231\n",
      "Epoch 4132, Loss: 0.3508191555738449, Final Batch Loss: 0.18297816812992096\n",
      "Epoch 4133, Loss: 0.2768295183777809, Final Batch Loss: 0.11884816735982895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4134, Loss: 0.26551133394241333, Final Batch Loss: 0.12330429255962372\n",
      "Epoch 4135, Loss: 0.3089963346719742, Final Batch Loss: 0.1630130261182785\n",
      "Epoch 4136, Loss: 0.2835674434900284, Final Batch Loss: 0.14549201726913452\n",
      "Epoch 4137, Loss: 0.28330903500318527, Final Batch Loss: 0.12169449776411057\n",
      "Epoch 4138, Loss: 0.26033642143011093, Final Batch Loss: 0.11568110436201096\n",
      "Epoch 4139, Loss: 0.2977936118841171, Final Batch Loss: 0.1666272133588791\n",
      "Epoch 4140, Loss: 0.2884488105773926, Final Batch Loss: 0.16468195617198944\n",
      "Epoch 4141, Loss: 0.27896101772785187, Final Batch Loss: 0.11848649382591248\n",
      "Epoch 4142, Loss: 0.27332618832588196, Final Batch Loss: 0.12654133141040802\n",
      "Epoch 4143, Loss: 0.36090368032455444, Final Batch Loss: 0.2134946584701538\n",
      "Epoch 4144, Loss: 0.23771370947360992, Final Batch Loss: 0.12362854182720184\n",
      "Epoch 4145, Loss: 0.26107578724622726, Final Batch Loss: 0.11428860574960709\n",
      "Epoch 4146, Loss: 0.273297980427742, Final Batch Loss: 0.15780742466449738\n",
      "Epoch 4147, Loss: 0.2316935509443283, Final Batch Loss: 0.1378437876701355\n",
      "Epoch 4148, Loss: 0.24624227732419968, Final Batch Loss: 0.1191188171505928\n",
      "Epoch 4149, Loss: 0.24645567685365677, Final Batch Loss: 0.13335992395877838\n",
      "Epoch 4150, Loss: 0.2815428227186203, Final Batch Loss: 0.11975529789924622\n",
      "Epoch 4151, Loss: 0.2784552127122879, Final Batch Loss: 0.12727583944797516\n",
      "Epoch 4152, Loss: 0.24289380759000778, Final Batch Loss: 0.12603545188903809\n",
      "Epoch 4153, Loss: 0.2936602979898453, Final Batch Loss: 0.15509003400802612\n",
      "Epoch 4154, Loss: 0.2624610736966133, Final Batch Loss: 0.14915476739406586\n",
      "Epoch 4155, Loss: 0.31219322979450226, Final Batch Loss: 0.18316446244716644\n",
      "Epoch 4156, Loss: 0.27582167088985443, Final Batch Loss: 0.1330483853816986\n",
      "Epoch 4157, Loss: 0.28969932347536087, Final Batch Loss: 0.18222729861736298\n",
      "Epoch 4158, Loss: 0.27630625665187836, Final Batch Loss: 0.1299840658903122\n",
      "Epoch 4159, Loss: 0.31061604619026184, Final Batch Loss: 0.17584975063800812\n",
      "Epoch 4160, Loss: 0.256224125623703, Final Batch Loss: 0.13582175970077515\n",
      "Epoch 4161, Loss: 0.2692524492740631, Final Batch Loss: 0.13366596400737762\n",
      "Epoch 4162, Loss: 0.26297158747911453, Final Batch Loss: 0.14694294333457947\n",
      "Epoch 4163, Loss: 0.25880108028650284, Final Batch Loss: 0.1148391142487526\n",
      "Epoch 4164, Loss: 0.2394903376698494, Final Batch Loss: 0.11544527113437653\n",
      "Epoch 4165, Loss: 0.2301112562417984, Final Batch Loss: 0.10831863433122635\n",
      "Epoch 4166, Loss: 0.2884962558746338, Final Batch Loss: 0.14457345008850098\n",
      "Epoch 4167, Loss: 0.32373835146427155, Final Batch Loss: 0.16290974617004395\n",
      "Epoch 4168, Loss: 0.28254052996635437, Final Batch Loss: 0.17853526771068573\n",
      "Epoch 4169, Loss: 0.2390659600496292, Final Batch Loss: 0.10702507197856903\n",
      "Epoch 4170, Loss: 0.27961306273937225, Final Batch Loss: 0.15579411387443542\n",
      "Epoch 4171, Loss: 0.2696022316813469, Final Batch Loss: 0.14541317522525787\n",
      "Epoch 4172, Loss: 0.2892308309674263, Final Batch Loss: 0.1151934340596199\n",
      "Epoch 4173, Loss: 0.2422615960240364, Final Batch Loss: 0.09673827141523361\n",
      "Epoch 4174, Loss: 0.2446342185139656, Final Batch Loss: 0.12333599478006363\n",
      "Epoch 4175, Loss: 0.2557177171111107, Final Batch Loss: 0.13135381042957306\n",
      "Epoch 4176, Loss: 0.2977384775876999, Final Batch Loss: 0.15772579610347748\n",
      "Epoch 4177, Loss: 0.2820517271757126, Final Batch Loss: 0.1445194035768509\n",
      "Epoch 4178, Loss: 0.23869936913251877, Final Batch Loss: 0.13595937192440033\n",
      "Epoch 4179, Loss: 0.2788343206048012, Final Batch Loss: 0.15686193108558655\n",
      "Epoch 4180, Loss: 0.24236523360013962, Final Batch Loss: 0.10519614070653915\n",
      "Epoch 4181, Loss: 0.2604832872748375, Final Batch Loss: 0.12475463002920151\n",
      "Epoch 4182, Loss: 0.323013499379158, Final Batch Loss: 0.15635579824447632\n",
      "Epoch 4183, Loss: 0.2742994576692581, Final Batch Loss: 0.15743039548397064\n",
      "Epoch 4184, Loss: 0.2776605933904648, Final Batch Loss: 0.14787763357162476\n",
      "Epoch 4185, Loss: 0.29112474620342255, Final Batch Loss: 0.11622768640518188\n",
      "Epoch 4186, Loss: 0.3057895004749298, Final Batch Loss: 0.17299218475818634\n",
      "Epoch 4187, Loss: 0.27272559702396393, Final Batch Loss: 0.13268046081066132\n",
      "Epoch 4188, Loss: 0.2793637216091156, Final Batch Loss: 0.1261567920446396\n",
      "Epoch 4189, Loss: 0.2547123432159424, Final Batch Loss: 0.13372810184955597\n",
      "Epoch 4190, Loss: 0.2741008326411247, Final Batch Loss: 0.15211263298988342\n",
      "Epoch 4191, Loss: 0.2774372547864914, Final Batch Loss: 0.12774525582790375\n",
      "Epoch 4192, Loss: 0.27555230259895325, Final Batch Loss: 0.12359175086021423\n",
      "Epoch 4193, Loss: 0.24120283871889114, Final Batch Loss: 0.1349327713251114\n",
      "Epoch 4194, Loss: 0.29281073808670044, Final Batch Loss: 0.12773497402668\n",
      "Epoch 4195, Loss: 0.2675251215696335, Final Batch Loss: 0.1380578875541687\n",
      "Epoch 4196, Loss: 0.2621600776910782, Final Batch Loss: 0.12473249435424805\n",
      "Epoch 4197, Loss: 0.2405802235007286, Final Batch Loss: 0.11627820134162903\n",
      "Epoch 4198, Loss: 0.2919570952653885, Final Batch Loss: 0.1332075297832489\n",
      "Epoch 4199, Loss: 0.24579595774412155, Final Batch Loss: 0.11994113773107529\n",
      "Epoch 4200, Loss: 0.25999800860881805, Final Batch Loss: 0.13966213166713715\n",
      "Epoch 4201, Loss: 0.24420181661844254, Final Batch Loss: 0.12356872856616974\n",
      "Epoch 4202, Loss: 0.2579905539751053, Final Batch Loss: 0.14177092909812927\n",
      "Epoch 4203, Loss: 0.29512956738471985, Final Batch Loss: 0.15394838154315948\n",
      "Epoch 4204, Loss: 0.2689862474799156, Final Batch Loss: 0.1528901308774948\n",
      "Epoch 4205, Loss: 0.2632317319512367, Final Batch Loss: 0.11963994055986404\n",
      "Epoch 4206, Loss: 0.22833209484815598, Final Batch Loss: 0.10475773364305496\n",
      "Epoch 4207, Loss: 0.313070684671402, Final Batch Loss: 0.16188794374465942\n",
      "Epoch 4208, Loss: 0.2511749267578125, Final Batch Loss: 0.1250436007976532\n",
      "Epoch 4209, Loss: 0.25311971455812454, Final Batch Loss: 0.1436886489391327\n",
      "Epoch 4210, Loss: 0.2521289959549904, Final Batch Loss: 0.11678219586610794\n",
      "Epoch 4211, Loss: 0.2884947806596756, Final Batch Loss: 0.16283227503299713\n",
      "Epoch 4212, Loss: 0.3492233008146286, Final Batch Loss: 0.1797676980495453\n",
      "Epoch 4213, Loss: 0.26335951685905457, Final Batch Loss: 0.12085290253162384\n",
      "Epoch 4214, Loss: 0.2741110995411873, Final Batch Loss: 0.11894205957651138\n",
      "Epoch 4215, Loss: 0.24738136678934097, Final Batch Loss: 0.14814995229244232\n",
      "Epoch 4216, Loss: 0.24995937198400497, Final Batch Loss: 0.1164257600903511\n",
      "Epoch 4217, Loss: 0.27731582522392273, Final Batch Loss: 0.13634447753429413\n",
      "Epoch 4218, Loss: 0.29303502291440964, Final Batch Loss: 0.17539598047733307\n",
      "Epoch 4219, Loss: 0.2685773968696594, Final Batch Loss: 0.15068009495735168\n",
      "Epoch 4220, Loss: 0.2812507450580597, Final Batch Loss: 0.14432166516780853\n",
      "Epoch 4221, Loss: 0.25635725259780884, Final Batch Loss: 0.1499895602464676\n",
      "Epoch 4222, Loss: 0.23582182824611664, Final Batch Loss: 0.10805359482765198\n",
      "Epoch 4223, Loss: 0.26951563358306885, Final Batch Loss: 0.13955551385879517\n",
      "Epoch 4224, Loss: 0.28467386215925217, Final Batch Loss: 0.1644776463508606\n",
      "Epoch 4225, Loss: 0.26697400212287903, Final Batch Loss: 0.1394939124584198\n",
      "Epoch 4226, Loss: 0.27821315079927444, Final Batch Loss: 0.11615093797445297\n",
      "Epoch 4227, Loss: 0.2703764736652374, Final Batch Loss: 0.1364384889602661\n",
      "Epoch 4228, Loss: 0.278414249420166, Final Batch Loss: 0.11436302959918976\n",
      "Epoch 4229, Loss: 0.30472443997859955, Final Batch Loss: 0.13668183982372284\n",
      "Epoch 4230, Loss: 0.2719701901078224, Final Batch Loss: 0.1721789836883545\n",
      "Epoch 4231, Loss: 0.3182423412799835, Final Batch Loss: 0.222407728433609\n",
      "Epoch 4232, Loss: 0.248487651348114, Final Batch Loss: 0.12277375161647797\n",
      "Epoch 4233, Loss: 0.28200508654117584, Final Batch Loss: 0.15216802060604095\n",
      "Epoch 4234, Loss: 0.2564257010817528, Final Batch Loss: 0.11614922434091568\n",
      "Epoch 4235, Loss: 0.24957899749279022, Final Batch Loss: 0.134918674826622\n",
      "Epoch 4236, Loss: 0.2917949706315994, Final Batch Loss: 0.16438744962215424\n",
      "Epoch 4237, Loss: 0.2785303294658661, Final Batch Loss: 0.12149982154369354\n",
      "Epoch 4238, Loss: 0.2634277567267418, Final Batch Loss: 0.13847412168979645\n",
      "Epoch 4239, Loss: 0.2494017630815506, Final Batch Loss: 0.12101128697395325\n",
      "Epoch 4240, Loss: 0.2927626669406891, Final Batch Loss: 0.13202598690986633\n",
      "Epoch 4241, Loss: 0.24606013298034668, Final Batch Loss: 0.07921157777309418\n",
      "Epoch 4242, Loss: 0.2902291715145111, Final Batch Loss: 0.1394737809896469\n",
      "Epoch 4243, Loss: 0.2622443735599518, Final Batch Loss: 0.12995697557926178\n",
      "Epoch 4244, Loss: 0.24098079651594162, Final Batch Loss: 0.12313009053468704\n",
      "Epoch 4245, Loss: 0.2756926715373993, Final Batch Loss: 0.123994842171669\n",
      "Epoch 4246, Loss: 0.3202376812696457, Final Batch Loss: 0.1624831259250641\n",
      "Epoch 4247, Loss: 0.28820962458848953, Final Batch Loss: 0.10822845250368118\n",
      "Epoch 4248, Loss: 0.25386112183332443, Final Batch Loss: 0.13676133751869202\n",
      "Epoch 4249, Loss: 0.2743181139230728, Final Batch Loss: 0.1384701430797577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4250, Loss: 0.279069185256958, Final Batch Loss: 0.1424107402563095\n",
      "Epoch 4251, Loss: 0.2951865717768669, Final Batch Loss: 0.1847851276397705\n",
      "Epoch 4252, Loss: 0.2440265342593193, Final Batch Loss: 0.13332414627075195\n",
      "Epoch 4253, Loss: 0.2480529546737671, Final Batch Loss: 0.12326089292764664\n",
      "Epoch 4254, Loss: 0.2202277109026909, Final Batch Loss: 0.11350899934768677\n",
      "Epoch 4255, Loss: 0.3294960707426071, Final Batch Loss: 0.14987888932228088\n",
      "Epoch 4256, Loss: 0.2617083340883255, Final Batch Loss: 0.15488207340240479\n",
      "Epoch 4257, Loss: 0.27437011897563934, Final Batch Loss: 0.12700960040092468\n",
      "Epoch 4258, Loss: 0.23311848938465118, Final Batch Loss: 0.13596665859222412\n",
      "Epoch 4259, Loss: 0.25519099086523056, Final Batch Loss: 0.1162799671292305\n",
      "Epoch 4260, Loss: 0.2692049741744995, Final Batch Loss: 0.14036217331886292\n",
      "Epoch 4261, Loss: 0.2803514897823334, Final Batch Loss: 0.12804967164993286\n",
      "Epoch 4262, Loss: 0.29138700664043427, Final Batch Loss: 0.14094944298267365\n",
      "Epoch 4263, Loss: 0.28564199805259705, Final Batch Loss: 0.144150510430336\n",
      "Epoch 4264, Loss: 0.2697249948978424, Final Batch Loss: 0.13658136129379272\n",
      "Epoch 4265, Loss: 0.2324870452284813, Final Batch Loss: 0.12320675700902939\n",
      "Epoch 4266, Loss: 0.29633111506700516, Final Batch Loss: 0.17971520125865936\n",
      "Epoch 4267, Loss: 0.27225179970264435, Final Batch Loss: 0.13848631083965302\n",
      "Epoch 4268, Loss: 0.27200011163949966, Final Batch Loss: 0.14855535328388214\n",
      "Epoch 4269, Loss: 0.2899482473731041, Final Batch Loss: 0.11448074132204056\n",
      "Epoch 4270, Loss: 0.25238437950611115, Final Batch Loss: 0.13918203115463257\n",
      "Epoch 4271, Loss: 0.25286438316106796, Final Batch Loss: 0.12902872264385223\n",
      "Epoch 4272, Loss: 0.27085448801517487, Final Batch Loss: 0.12363645434379578\n",
      "Epoch 4273, Loss: 0.2423817664384842, Final Batch Loss: 0.12407711893320084\n",
      "Epoch 4274, Loss: 0.2641565054655075, Final Batch Loss: 0.10586802661418915\n",
      "Epoch 4275, Loss: 0.2510475814342499, Final Batch Loss: 0.1468631625175476\n",
      "Epoch 4276, Loss: 0.25885171443223953, Final Batch Loss: 0.12411627918481827\n",
      "Epoch 4277, Loss: 0.3161398023366928, Final Batch Loss: 0.1624215990304947\n",
      "Epoch 4278, Loss: 0.26195941865444183, Final Batch Loss: 0.1489533632993698\n",
      "Epoch 4279, Loss: 0.33199407160282135, Final Batch Loss: 0.17879821360111237\n",
      "Epoch 4280, Loss: 0.2878747284412384, Final Batch Loss: 0.1390613466501236\n",
      "Epoch 4281, Loss: 0.2849210053682327, Final Batch Loss: 0.15642257034778595\n",
      "Epoch 4282, Loss: 0.2960551083087921, Final Batch Loss: 0.11933627724647522\n",
      "Epoch 4283, Loss: 0.2754363864660263, Final Batch Loss: 0.13606007397174835\n",
      "Epoch 4284, Loss: 0.23632393032312393, Final Batch Loss: 0.1367088109254837\n",
      "Epoch 4285, Loss: 0.29903288185596466, Final Batch Loss: 0.15780238807201385\n",
      "Epoch 4286, Loss: 0.30909208953380585, Final Batch Loss: 0.12374630570411682\n",
      "Epoch 4287, Loss: 0.2995210140943527, Final Batch Loss: 0.16463398933410645\n",
      "Epoch 4288, Loss: 0.25236518681049347, Final Batch Loss: 0.1354152411222458\n",
      "Epoch 4289, Loss: 0.23596378415822983, Final Batch Loss: 0.1311066746711731\n",
      "Epoch 4290, Loss: 0.29292114078998566, Final Batch Loss: 0.13742466270923615\n",
      "Epoch 4291, Loss: 0.30247268080711365, Final Batch Loss: 0.1439773291349411\n",
      "Epoch 4292, Loss: 0.2572099566459656, Final Batch Loss: 0.13206353783607483\n",
      "Epoch 4293, Loss: 0.25849079340696335, Final Batch Loss: 0.12109985202550888\n",
      "Epoch 4294, Loss: 0.2851896733045578, Final Batch Loss: 0.13376541435718536\n",
      "Epoch 4295, Loss: 0.2804795801639557, Final Batch Loss: 0.12124903500080109\n",
      "Epoch 4296, Loss: 0.26457925140857697, Final Batch Loss: 0.11004270613193512\n",
      "Epoch 4297, Loss: 0.2693566605448723, Final Batch Loss: 0.150934100151062\n",
      "Epoch 4298, Loss: 0.24511603266000748, Final Batch Loss: 0.12394823133945465\n",
      "Epoch 4299, Loss: 0.2712275832891464, Final Batch Loss: 0.12643669545650482\n",
      "Epoch 4300, Loss: 0.27831506729125977, Final Batch Loss: 0.14351440966129303\n",
      "Epoch 4301, Loss: 0.2544766962528229, Final Batch Loss: 0.11705672740936279\n",
      "Epoch 4302, Loss: 0.25996221601963043, Final Batch Loss: 0.13255241513252258\n",
      "Epoch 4303, Loss: 0.24749629944562912, Final Batch Loss: 0.09447538107633591\n",
      "Epoch 4304, Loss: 0.2786005288362503, Final Batch Loss: 0.106907919049263\n",
      "Epoch 4305, Loss: 0.24269495159387589, Final Batch Loss: 0.09658054262399673\n",
      "Epoch 4306, Loss: 0.27083054929971695, Final Batch Loss: 0.1626063734292984\n",
      "Epoch 4307, Loss: 0.2771589457988739, Final Batch Loss: 0.1575910598039627\n",
      "Epoch 4308, Loss: 0.2789144814014435, Final Batch Loss: 0.13343104720115662\n",
      "Epoch 4309, Loss: 0.3018558770418167, Final Batch Loss: 0.15692028403282166\n",
      "Epoch 4310, Loss: 0.2833752781152725, Final Batch Loss: 0.1268979012966156\n",
      "Epoch 4311, Loss: 0.30640721321105957, Final Batch Loss: 0.14273419976234436\n",
      "Epoch 4312, Loss: 0.3058781623840332, Final Batch Loss: 0.19341818988323212\n",
      "Epoch 4313, Loss: 0.2849601209163666, Final Batch Loss: 0.12887915968894958\n",
      "Epoch 4314, Loss: 0.33508962392807007, Final Batch Loss: 0.14804255962371826\n",
      "Epoch 4315, Loss: 0.2603398710489273, Final Batch Loss: 0.12021492421627045\n",
      "Epoch 4316, Loss: 0.3102722615003586, Final Batch Loss: 0.13501212000846863\n",
      "Epoch 4317, Loss: 0.3033882826566696, Final Batch Loss: 0.1420532912015915\n",
      "Epoch 4318, Loss: 0.2916528433561325, Final Batch Loss: 0.16258881986141205\n",
      "Epoch 4319, Loss: 0.27033831179142, Final Batch Loss: 0.12579625844955444\n",
      "Epoch 4320, Loss: 0.3105424791574478, Final Batch Loss: 0.15312963724136353\n",
      "Epoch 4321, Loss: 0.29226547479629517, Final Batch Loss: 0.14006787538528442\n",
      "Epoch 4322, Loss: 0.2457176223397255, Final Batch Loss: 0.1305287778377533\n",
      "Epoch 4323, Loss: 0.2306676059961319, Final Batch Loss: 0.12997470796108246\n",
      "Epoch 4324, Loss: 0.27212584018707275, Final Batch Loss: 0.13100996613502502\n",
      "Epoch 4325, Loss: 0.33050864934921265, Final Batch Loss: 0.14580675959587097\n",
      "Epoch 4326, Loss: 0.2984919473528862, Final Batch Loss: 0.11251837760210037\n",
      "Epoch 4327, Loss: 0.29243330657482147, Final Batch Loss: 0.14229218661785126\n",
      "Epoch 4328, Loss: 0.2708532363176346, Final Batch Loss: 0.11785739660263062\n",
      "Epoch 4329, Loss: 0.29796605557203293, Final Batch Loss: 0.17721335589885712\n",
      "Epoch 4330, Loss: 0.3180249482393265, Final Batch Loss: 0.12950950860977173\n",
      "Epoch 4331, Loss: 0.3124898672103882, Final Batch Loss: 0.16598211228847504\n",
      "Epoch 4332, Loss: 0.33747655153274536, Final Batch Loss: 0.18010565638542175\n",
      "Epoch 4333, Loss: 0.2632185071706772, Final Batch Loss: 0.13956215977668762\n",
      "Epoch 4334, Loss: 0.31551432609558105, Final Batch Loss: 0.14328667521476746\n",
      "Epoch 4335, Loss: 0.29447364807128906, Final Batch Loss: 0.15670078992843628\n",
      "Epoch 4336, Loss: 0.2960314452648163, Final Batch Loss: 0.14636074006557465\n",
      "Epoch 4337, Loss: 0.27985260635614395, Final Batch Loss: 0.11900907009840012\n",
      "Epoch 4338, Loss: 0.2808823883533478, Final Batch Loss: 0.1567428708076477\n",
      "Epoch 4339, Loss: 0.2697616368532181, Final Batch Loss: 0.15284520387649536\n",
      "Epoch 4340, Loss: 0.2683383673429489, Final Batch Loss: 0.12707164883613586\n",
      "Epoch 4341, Loss: 0.2955233007669449, Final Batch Loss: 0.13654066622257233\n",
      "Epoch 4342, Loss: 0.2922050356864929, Final Batch Loss: 0.11768509447574615\n",
      "Epoch 4343, Loss: 0.2501377686858177, Final Batch Loss: 0.11915489286184311\n",
      "Epoch 4344, Loss: 0.2933846712112427, Final Batch Loss: 0.16958698630332947\n",
      "Epoch 4345, Loss: 0.275032602250576, Final Batch Loss: 0.1197543665766716\n",
      "Epoch 4346, Loss: 0.26067306101322174, Final Batch Loss: 0.12099090218544006\n",
      "Epoch 4347, Loss: 0.2669423967599869, Final Batch Loss: 0.12790000438690186\n",
      "Epoch 4348, Loss: 0.23786887526512146, Final Batch Loss: 0.1343059241771698\n",
      "Epoch 4349, Loss: 0.2603110894560814, Final Batch Loss: 0.10191533714532852\n",
      "Epoch 4350, Loss: 0.2909531742334366, Final Batch Loss: 0.13719424605369568\n",
      "Epoch 4351, Loss: 0.27093931287527084, Final Batch Loss: 0.11434874683618546\n",
      "Epoch 4352, Loss: 0.31452466547489166, Final Batch Loss: 0.15260186791419983\n",
      "Epoch 4353, Loss: 0.2555025890469551, Final Batch Loss: 0.13695062696933746\n",
      "Epoch 4354, Loss: 0.2729848176240921, Final Batch Loss: 0.12680619955062866\n",
      "Epoch 4355, Loss: 0.31905849277973175, Final Batch Loss: 0.14653876423835754\n",
      "Epoch 4356, Loss: 0.2748800814151764, Final Batch Loss: 0.13582076132297516\n",
      "Epoch 4357, Loss: 0.26583653688430786, Final Batch Loss: 0.11524887382984161\n",
      "Epoch 4358, Loss: 0.2862963080406189, Final Batch Loss: 0.14423203468322754\n",
      "Epoch 4359, Loss: 0.2599925175309181, Final Batch Loss: 0.14712825417518616\n",
      "Epoch 4360, Loss: 0.2868950292468071, Final Batch Loss: 0.16423411667346954\n",
      "Epoch 4361, Loss: 0.24865444749593735, Final Batch Loss: 0.13902409374713898\n",
      "Epoch 4362, Loss: 0.2913094013929367, Final Batch Loss: 0.1552276611328125\n",
      "Epoch 4363, Loss: 0.261604480445385, Final Batch Loss: 0.1083965077996254\n",
      "Epoch 4364, Loss: 0.24660324305295944, Final Batch Loss: 0.10536334663629532\n",
      "Epoch 4365, Loss: 0.2722228989005089, Final Batch Loss: 0.12394993752241135\n",
      "Epoch 4366, Loss: 0.3050488233566284, Final Batch Loss: 0.16965048015117645\n",
      "Epoch 4367, Loss: 0.26300953328609467, Final Batch Loss: 0.12846490740776062\n",
      "Epoch 4368, Loss: 0.30111780017614365, Final Batch Loss: 0.18371684849262238\n",
      "Epoch 4369, Loss: 0.24543392658233643, Final Batch Loss: 0.10141991078853607\n",
      "Epoch 4370, Loss: 0.27340932190418243, Final Batch Loss: 0.14192788302898407\n",
      "Epoch 4371, Loss: 0.29146696627140045, Final Batch Loss: 0.12191194295883179\n",
      "Epoch 4372, Loss: 0.24487266689538956, Final Batch Loss: 0.13409580290317535\n",
      "Epoch 4373, Loss: 0.24588564783334732, Final Batch Loss: 0.1266922801733017\n",
      "Epoch 4374, Loss: 0.2541106045246124, Final Batch Loss: 0.13060562312602997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4375, Loss: 0.26431548595428467, Final Batch Loss: 0.12789286673069\n",
      "Epoch 4376, Loss: 0.31807270646095276, Final Batch Loss: 0.1289796084165573\n",
      "Epoch 4377, Loss: 0.2827553153038025, Final Batch Loss: 0.13055047392845154\n",
      "Epoch 4378, Loss: 0.2786960154771805, Final Batch Loss: 0.13818255066871643\n",
      "Epoch 4379, Loss: 0.27386637032032013, Final Batch Loss: 0.14662772417068481\n",
      "Epoch 4380, Loss: 0.2408735752105713, Final Batch Loss: 0.12346545606851578\n",
      "Epoch 4381, Loss: 0.24186234176158905, Final Batch Loss: 0.11199399828910828\n",
      "Epoch 4382, Loss: 0.2608790472149849, Final Batch Loss: 0.13771957159042358\n",
      "Epoch 4383, Loss: 0.3116862326860428, Final Batch Loss: 0.14154259860515594\n",
      "Epoch 4384, Loss: 0.30672501027584076, Final Batch Loss: 0.14347785711288452\n",
      "Epoch 4385, Loss: 0.23901747167110443, Final Batch Loss: 0.13026773929595947\n",
      "Epoch 4386, Loss: 0.29858632385730743, Final Batch Loss: 0.15385548770427704\n",
      "Epoch 4387, Loss: 0.3179010599851608, Final Batch Loss: 0.17397017776966095\n",
      "Epoch 4388, Loss: 0.2674219086766243, Final Batch Loss: 0.11953676491975784\n",
      "Epoch 4389, Loss: 0.21781424432992935, Final Batch Loss: 0.1179838627576828\n",
      "Epoch 4390, Loss: 0.26984763890504837, Final Batch Loss: 0.12034640461206436\n",
      "Epoch 4391, Loss: 0.2804016172885895, Final Batch Loss: 0.14338746666908264\n",
      "Epoch 4392, Loss: 0.25996866077184677, Final Batch Loss: 0.14520493149757385\n",
      "Epoch 4393, Loss: 0.2497301548719406, Final Batch Loss: 0.11372299492359161\n",
      "Epoch 4394, Loss: 0.24005497992038727, Final Batch Loss: 0.11987388134002686\n",
      "Epoch 4395, Loss: 0.2517774626612663, Final Batch Loss: 0.11899048835039139\n",
      "Epoch 4396, Loss: 0.27104875445365906, Final Batch Loss: 0.14515337347984314\n",
      "Epoch 4397, Loss: 0.22306697815656662, Final Batch Loss: 0.11957621574401855\n",
      "Epoch 4398, Loss: 0.2944975271821022, Final Batch Loss: 0.1249043419957161\n",
      "Epoch 4399, Loss: 0.25709882378578186, Final Batch Loss: 0.13178226351737976\n",
      "Epoch 4400, Loss: 0.31001420319080353, Final Batch Loss: 0.1720922887325287\n",
      "Epoch 4401, Loss: 0.2715204358100891, Final Batch Loss: 0.11557851731777191\n",
      "Epoch 4402, Loss: 0.31017495691776276, Final Batch Loss: 0.16610735654830933\n",
      "Epoch 4403, Loss: 0.284383624792099, Final Batch Loss: 0.16942475736141205\n",
      "Epoch 4404, Loss: 0.24031903594732285, Final Batch Loss: 0.1189899891614914\n",
      "Epoch 4405, Loss: 0.28816211223602295, Final Batch Loss: 0.12645885348320007\n",
      "Epoch 4406, Loss: 0.3083511143922806, Final Batch Loss: 0.16352704167366028\n",
      "Epoch 4407, Loss: 0.2826705127954483, Final Batch Loss: 0.13857372105121613\n",
      "Epoch 4408, Loss: 0.22711792588233948, Final Batch Loss: 0.10116450488567352\n",
      "Epoch 4409, Loss: 0.23845480382442474, Final Batch Loss: 0.0948430746793747\n",
      "Epoch 4410, Loss: 0.29677820205688477, Final Batch Loss: 0.17068490386009216\n",
      "Epoch 4411, Loss: 0.3237009346485138, Final Batch Loss: 0.1945200264453888\n",
      "Epoch 4412, Loss: 0.27698951959609985, Final Batch Loss: 0.1331610083580017\n",
      "Epoch 4413, Loss: 0.2662082090973854, Final Batch Loss: 0.11750508099794388\n",
      "Epoch 4414, Loss: 0.27851053327322006, Final Batch Loss: 0.11185670644044876\n",
      "Epoch 4415, Loss: 0.24630208313465118, Final Batch Loss: 0.12013635039329529\n",
      "Epoch 4416, Loss: 0.2612428814172745, Final Batch Loss: 0.13085509836673737\n",
      "Epoch 4417, Loss: 0.24661723524332047, Final Batch Loss: 0.11462118476629257\n",
      "Epoch 4418, Loss: 0.2574704736471176, Final Batch Loss: 0.11733391880989075\n",
      "Epoch 4419, Loss: 0.2531418353319168, Final Batch Loss: 0.1046249121427536\n",
      "Epoch 4420, Loss: 0.256479375064373, Final Batch Loss: 0.11093110591173172\n",
      "Epoch 4421, Loss: 0.24732854962348938, Final Batch Loss: 0.1404227465391159\n",
      "Epoch 4422, Loss: 0.24141156673431396, Final Batch Loss: 0.1154528558254242\n",
      "Epoch 4423, Loss: 0.2513292729854584, Final Batch Loss: 0.12659026682376862\n",
      "Epoch 4424, Loss: 0.30066531896591187, Final Batch Loss: 0.14623935520648956\n",
      "Epoch 4425, Loss: 0.24987128376960754, Final Batch Loss: 0.1468433439731598\n",
      "Epoch 4426, Loss: 0.28791631758213043, Final Batch Loss: 0.14368118345737457\n",
      "Epoch 4427, Loss: 0.31032349169254303, Final Batch Loss: 0.15151673555374146\n",
      "Epoch 4428, Loss: 0.2628515362739563, Final Batch Loss: 0.11528415977954865\n",
      "Epoch 4429, Loss: 0.23433658480644226, Final Batch Loss: 0.12004537135362625\n",
      "Epoch 4430, Loss: 0.2502635568380356, Final Batch Loss: 0.11768655478954315\n",
      "Epoch 4431, Loss: 0.2654358595609665, Final Batch Loss: 0.10156340897083282\n",
      "Epoch 4432, Loss: 0.2738984003663063, Final Batch Loss: 0.1614815890789032\n",
      "Epoch 4433, Loss: 0.2700375020503998, Final Batch Loss: 0.14338356256484985\n",
      "Epoch 4434, Loss: 0.2660699635744095, Final Batch Loss: 0.13695313036441803\n",
      "Epoch 4435, Loss: 0.27889569103717804, Final Batch Loss: 0.1469811648130417\n",
      "Epoch 4436, Loss: 0.25670236349105835, Final Batch Loss: 0.1252744346857071\n",
      "Epoch 4437, Loss: 0.25792166590690613, Final Batch Loss: 0.13430742919445038\n",
      "Epoch 4438, Loss: 0.2710777074098587, Final Batch Loss: 0.13958550989627838\n",
      "Epoch 4439, Loss: 0.2949873059988022, Final Batch Loss: 0.16731253266334534\n",
      "Epoch 4440, Loss: 0.33075375854969025, Final Batch Loss: 0.13375437259674072\n",
      "Epoch 4441, Loss: 0.262099027633667, Final Batch Loss: 0.15363241732120514\n",
      "Epoch 4442, Loss: 0.2554019168019295, Final Batch Loss: 0.15813645720481873\n",
      "Epoch 4443, Loss: 0.2740294933319092, Final Batch Loss: 0.14856116473674774\n",
      "Epoch 4444, Loss: 0.2604721933603287, Final Batch Loss: 0.1338663250207901\n",
      "Epoch 4445, Loss: 0.2517591044306755, Final Batch Loss: 0.12084987014532089\n",
      "Epoch 4446, Loss: 0.3624478876590729, Final Batch Loss: 0.1543944776058197\n",
      "Epoch 4447, Loss: 0.30991727113723755, Final Batch Loss: 0.14853206276893616\n",
      "Epoch 4448, Loss: 0.2939072996377945, Final Batch Loss: 0.1478068083524704\n",
      "Epoch 4449, Loss: 0.2865743264555931, Final Batch Loss: 0.1691228300333023\n",
      "Epoch 4450, Loss: 0.2596350610256195, Final Batch Loss: 0.12831231951713562\n",
      "Epoch 4451, Loss: 0.27466754615306854, Final Batch Loss: 0.1360139101743698\n",
      "Epoch 4452, Loss: 0.2989683747291565, Final Batch Loss: 0.12376980483531952\n",
      "Epoch 4453, Loss: 0.283356249332428, Final Batch Loss: 0.10971516370773315\n",
      "Epoch 4454, Loss: 0.24525415152311325, Final Batch Loss: 0.1002291664481163\n",
      "Epoch 4455, Loss: 0.258897602558136, Final Batch Loss: 0.12378284335136414\n",
      "Epoch 4456, Loss: 0.24095694720745087, Final Batch Loss: 0.148167222738266\n",
      "Epoch 4457, Loss: 0.2716715931892395, Final Batch Loss: 0.1393689662218094\n",
      "Epoch 4458, Loss: 0.23734639585018158, Final Batch Loss: 0.13735520839691162\n",
      "Epoch 4459, Loss: 0.2592964693903923, Final Batch Loss: 0.11437400430440903\n",
      "Epoch 4460, Loss: 0.2740383893251419, Final Batch Loss: 0.14539943635463715\n",
      "Epoch 4461, Loss: 0.2554533928632736, Final Batch Loss: 0.1294127106666565\n",
      "Epoch 4462, Loss: 0.269320085644722, Final Batch Loss: 0.1377457231283188\n",
      "Epoch 4463, Loss: 0.24843130260705948, Final Batch Loss: 0.14093077182769775\n",
      "Epoch 4464, Loss: 0.27096497267484665, Final Batch Loss: 0.1489361971616745\n",
      "Epoch 4465, Loss: 0.28797850012779236, Final Batch Loss: 0.13745653629302979\n",
      "Epoch 4466, Loss: 0.2579610124230385, Final Batch Loss: 0.11386043578386307\n",
      "Epoch 4467, Loss: 0.23491507768630981, Final Batch Loss: 0.1293392777442932\n",
      "Epoch 4468, Loss: 0.2750360071659088, Final Batch Loss: 0.14184053242206573\n",
      "Epoch 4469, Loss: 0.27950023114681244, Final Batch Loss: 0.14866815507411957\n",
      "Epoch 4470, Loss: 0.2792535424232483, Final Batch Loss: 0.13786228001117706\n",
      "Epoch 4471, Loss: 0.24973464012145996, Final Batch Loss: 0.12222856283187866\n",
      "Epoch 4472, Loss: 0.2966316193342209, Final Batch Loss: 0.1338013857603073\n",
      "Epoch 4473, Loss: 0.270387627184391, Final Batch Loss: 0.11834598332643509\n",
      "Epoch 4474, Loss: 0.24578256160020828, Final Batch Loss: 0.09948588162660599\n",
      "Epoch 4475, Loss: 0.2607942447066307, Final Batch Loss: 0.14680100977420807\n",
      "Epoch 4476, Loss: 0.26660116761922836, Final Batch Loss: 0.11218170076608658\n",
      "Epoch 4477, Loss: 0.27401357889175415, Final Batch Loss: 0.12554416060447693\n",
      "Epoch 4478, Loss: 0.27291861921548843, Final Batch Loss: 0.15259025990962982\n",
      "Epoch 4479, Loss: 0.269423708319664, Final Batch Loss: 0.13213106989860535\n",
      "Epoch 4480, Loss: 0.2675742506980896, Final Batch Loss: 0.13207677006721497\n",
      "Epoch 4481, Loss: 0.23896164447069168, Final Batch Loss: 0.1267056167125702\n",
      "Epoch 4482, Loss: 0.34693366289138794, Final Batch Loss: 0.14242498576641083\n",
      "Epoch 4483, Loss: 0.2981973886489868, Final Batch Loss: 0.12572602927684784\n",
      "Epoch 4484, Loss: 0.26079128682613373, Final Batch Loss: 0.14584162831306458\n",
      "Epoch 4485, Loss: 0.2981268912553787, Final Batch Loss: 0.1301449090242386\n",
      "Epoch 4486, Loss: 0.31348249316215515, Final Batch Loss: 0.16222399473190308\n",
      "Epoch 4487, Loss: 0.28310954570770264, Final Batch Loss: 0.12945523858070374\n",
      "Epoch 4488, Loss: 0.2709738314151764, Final Batch Loss: 0.1331450492143631\n",
      "Epoch 4489, Loss: 0.30821867287158966, Final Batch Loss: 0.153642475605011\n",
      "Epoch 4490, Loss: 0.24578692018985748, Final Batch Loss: 0.14166276156902313\n",
      "Epoch 4491, Loss: 0.24720635265111923, Final Batch Loss: 0.1524471491575241\n",
      "Epoch 4492, Loss: 0.3020506948232651, Final Batch Loss: 0.14159679412841797\n",
      "Epoch 4493, Loss: 0.26517079770565033, Final Batch Loss: 0.1389278769493103\n",
      "Epoch 4494, Loss: 0.2464185208082199, Final Batch Loss: 0.10647009313106537\n",
      "Epoch 4495, Loss: 0.32882530987262726, Final Batch Loss: 0.1844760924577713\n",
      "Epoch 4496, Loss: 0.2684524208307266, Final Batch Loss: 0.11780214309692383\n",
      "Epoch 4497, Loss: 0.22868990153074265, Final Batch Loss: 0.1050468161702156\n",
      "Epoch 4498, Loss: 0.2581242620944977, Final Batch Loss: 0.13619978725910187\n",
      "Epoch 4499, Loss: 0.27604883909225464, Final Batch Loss: 0.13429990410804749\n",
      "Epoch 4500, Loss: 0.2657480686903, Final Batch Loss: 0.14320659637451172\n",
      "Epoch 4501, Loss: 0.28487029671669006, Final Batch Loss: 0.14911559224128723\n",
      "Epoch 4502, Loss: 0.2500612288713455, Final Batch Loss: 0.09807135164737701\n",
      "Epoch 4503, Loss: 0.2741275131702423, Final Batch Loss: 0.14935420453548431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4504, Loss: 0.28706932067871094, Final Batch Loss: 0.1465466469526291\n",
      "Epoch 4505, Loss: 0.2727941647171974, Final Batch Loss: 0.11313840001821518\n",
      "Epoch 4506, Loss: 0.3323659673333168, Final Batch Loss: 0.2118019014596939\n",
      "Epoch 4507, Loss: 0.26818282902240753, Final Batch Loss: 0.13773228228092194\n",
      "Epoch 4508, Loss: 0.32644154131412506, Final Batch Loss: 0.11905373632907867\n",
      "Epoch 4509, Loss: 0.25116128474473953, Final Batch Loss: 0.11828992515802383\n",
      "Epoch 4510, Loss: 0.28203389048576355, Final Batch Loss: 0.1512383073568344\n",
      "Epoch 4511, Loss: 0.2777627110481262, Final Batch Loss: 0.13942687213420868\n",
      "Epoch 4512, Loss: 0.3495994210243225, Final Batch Loss: 0.13935256004333496\n",
      "Epoch 4513, Loss: 0.2949339598417282, Final Batch Loss: 0.16369298100471497\n",
      "Epoch 4514, Loss: 0.2559630200266838, Final Batch Loss: 0.11128824204206467\n",
      "Epoch 4515, Loss: 0.2716275602579117, Final Batch Loss: 0.1315038651227951\n",
      "Epoch 4516, Loss: 0.2556762546300888, Final Batch Loss: 0.14445239305496216\n",
      "Epoch 4517, Loss: 0.25195693224668503, Final Batch Loss: 0.12081205099821091\n",
      "Epoch 4518, Loss: 0.28414806723594666, Final Batch Loss: 0.1433202177286148\n",
      "Epoch 4519, Loss: 0.3317101448774338, Final Batch Loss: 0.16516754031181335\n",
      "Epoch 4520, Loss: 0.2631480172276497, Final Batch Loss: 0.10540559142827988\n",
      "Epoch 4521, Loss: 0.2784690111875534, Final Batch Loss: 0.12003743648529053\n",
      "Epoch 4522, Loss: 0.2822824567556381, Final Batch Loss: 0.1258975714445114\n",
      "Epoch 4523, Loss: 0.3015436828136444, Final Batch Loss: 0.13609111309051514\n",
      "Epoch 4524, Loss: 0.2698913738131523, Final Batch Loss: 0.14749880135059357\n",
      "Epoch 4525, Loss: 0.24900612980127335, Final Batch Loss: 0.10734381526708603\n",
      "Epoch 4526, Loss: 0.2639147713780403, Final Batch Loss: 0.16363278031349182\n",
      "Epoch 4527, Loss: 0.2448713555932045, Final Batch Loss: 0.13829873502254486\n",
      "Epoch 4528, Loss: 0.24932152032852173, Final Batch Loss: 0.12617366015911102\n",
      "Epoch 4529, Loss: 0.2554498538374901, Final Batch Loss: 0.13714943826198578\n",
      "Epoch 4530, Loss: 0.27231210470199585, Final Batch Loss: 0.12709952890872955\n",
      "Epoch 4531, Loss: 0.2886297255754471, Final Batch Loss: 0.12699782848358154\n",
      "Epoch 4532, Loss: 0.25345609337091446, Final Batch Loss: 0.13463334739208221\n",
      "Epoch 4533, Loss: 0.2617621570825577, Final Batch Loss: 0.1424470841884613\n",
      "Epoch 4534, Loss: 0.36972369253635406, Final Batch Loss: 0.21411582827568054\n",
      "Epoch 4535, Loss: 0.2802066057920456, Final Batch Loss: 0.13366255164146423\n",
      "Epoch 4536, Loss: 0.24999909847974777, Final Batch Loss: 0.12360351532697678\n",
      "Epoch 4537, Loss: 0.27289020270109177, Final Batch Loss: 0.12411516159772873\n",
      "Epoch 4538, Loss: 0.24464954435825348, Final Batch Loss: 0.11836189031600952\n",
      "Epoch 4539, Loss: 0.24598369002342224, Final Batch Loss: 0.13013793528079987\n",
      "Epoch 4540, Loss: 0.26781437546014786, Final Batch Loss: 0.10894311219453812\n",
      "Epoch 4541, Loss: 0.3057651147246361, Final Batch Loss: 0.11845246702432632\n",
      "Epoch 4542, Loss: 0.27324196696281433, Final Batch Loss: 0.11862984299659729\n",
      "Epoch 4543, Loss: 0.2760276645421982, Final Batch Loss: 0.14738969504833221\n",
      "Epoch 4544, Loss: 0.271882563829422, Final Batch Loss: 0.12194459140300751\n",
      "Epoch 4545, Loss: 0.27641561627388, Final Batch Loss: 0.13537447154521942\n",
      "Epoch 4546, Loss: 0.26779936254024506, Final Batch Loss: 0.16838766634464264\n",
      "Epoch 4547, Loss: 0.23487811535596848, Final Batch Loss: 0.11218556761741638\n",
      "Epoch 4548, Loss: 0.29706159234046936, Final Batch Loss: 0.16083329916000366\n",
      "Epoch 4549, Loss: 0.2619451433420181, Final Batch Loss: 0.15218612551689148\n",
      "Epoch 4550, Loss: 0.2852710261940956, Final Batch Loss: 0.11865175515413284\n",
      "Epoch 4551, Loss: 0.2897087186574936, Final Batch Loss: 0.1490316540002823\n",
      "Epoch 4552, Loss: 0.2676725760102272, Final Batch Loss: 0.10135417431592941\n",
      "Epoch 4553, Loss: 0.24273204058408737, Final Batch Loss: 0.1399213820695877\n",
      "Epoch 4554, Loss: 0.27056391537189484, Final Batch Loss: 0.09814965724945068\n",
      "Epoch 4555, Loss: 0.2616230621933937, Final Batch Loss: 0.15688814222812653\n",
      "Epoch 4556, Loss: 0.2747611254453659, Final Batch Loss: 0.13004881143569946\n",
      "Epoch 4557, Loss: 0.2650538235902786, Final Batch Loss: 0.11647409200668335\n",
      "Epoch 4558, Loss: 0.2688092589378357, Final Batch Loss: 0.14158235490322113\n",
      "Epoch 4559, Loss: 0.2603546231985092, Final Batch Loss: 0.1317276805639267\n",
      "Epoch 4560, Loss: 0.27744536101818085, Final Batch Loss: 0.19039519131183624\n",
      "Epoch 4561, Loss: 0.2887973338365555, Final Batch Loss: 0.15484169125556946\n",
      "Epoch 4562, Loss: 0.26676560938358307, Final Batch Loss: 0.1341530829668045\n",
      "Epoch 4563, Loss: 0.251688152551651, Final Batch Loss: 0.12940137088298798\n",
      "Epoch 4564, Loss: 0.2576225847005844, Final Batch Loss: 0.12647658586502075\n",
      "Epoch 4565, Loss: 0.23689612746238708, Final Batch Loss: 0.11234520375728607\n",
      "Epoch 4566, Loss: 0.2470041662454605, Final Batch Loss: 0.11359672248363495\n",
      "Epoch 4567, Loss: 0.26508932560682297, Final Batch Loss: 0.11382826417684555\n",
      "Epoch 4568, Loss: 0.2347673997282982, Final Batch Loss: 0.12593740224838257\n",
      "Epoch 4569, Loss: 0.2674383968114853, Final Batch Loss: 0.1652141958475113\n",
      "Epoch 4570, Loss: 0.2617117762565613, Final Batch Loss: 0.1260492205619812\n",
      "Epoch 4571, Loss: 0.30159667879343033, Final Batch Loss: 0.17989712953567505\n",
      "Epoch 4572, Loss: 0.23915594071149826, Final Batch Loss: 0.1341233253479004\n",
      "Epoch 4573, Loss: 0.2641969621181488, Final Batch Loss: 0.1274663656949997\n",
      "Epoch 4574, Loss: 0.23173855990171432, Final Batch Loss: 0.11502642184495926\n",
      "Epoch 4575, Loss: 0.23647315055131912, Final Batch Loss: 0.13544875383377075\n",
      "Epoch 4576, Loss: 0.24248376488685608, Final Batch Loss: 0.1260397732257843\n",
      "Epoch 4577, Loss: 0.24552056193351746, Final Batch Loss: 0.09262765944004059\n",
      "Epoch 4578, Loss: 0.29504385590553284, Final Batch Loss: 0.14747370779514313\n",
      "Epoch 4579, Loss: 0.25673845410346985, Final Batch Loss: 0.1223267912864685\n",
      "Epoch 4580, Loss: 0.2449304312467575, Final Batch Loss: 0.11579792201519012\n",
      "Epoch 4581, Loss: 0.29594311118125916, Final Batch Loss: 0.14945748448371887\n",
      "Epoch 4582, Loss: 0.22327286005020142, Final Batch Loss: 0.12481246143579483\n",
      "Epoch 4583, Loss: 0.2528035119175911, Final Batch Loss: 0.12899504601955414\n",
      "Epoch 4584, Loss: 0.25183960795402527, Final Batch Loss: 0.13666167855262756\n",
      "Epoch 4585, Loss: 0.21097010374069214, Final Batch Loss: 0.10284575819969177\n",
      "Epoch 4586, Loss: 0.28203655779361725, Final Batch Loss: 0.13332141935825348\n",
      "Epoch 4587, Loss: 0.2505418509244919, Final Batch Loss: 0.10897311568260193\n",
      "Epoch 4588, Loss: 0.218524768948555, Final Batch Loss: 0.10885635763406754\n",
      "Epoch 4589, Loss: 0.26530200242996216, Final Batch Loss: 0.15801435708999634\n",
      "Epoch 4590, Loss: 0.2836291491985321, Final Batch Loss: 0.1639569103717804\n",
      "Epoch 4591, Loss: 0.274461030960083, Final Batch Loss: 0.09927491843700409\n",
      "Epoch 4592, Loss: 0.2894035428762436, Final Batch Loss: 0.17376631498336792\n",
      "Epoch 4593, Loss: 0.2935978174209595, Final Batch Loss: 0.13525517284870148\n",
      "Epoch 4594, Loss: 0.2455940544605255, Final Batch Loss: 0.13034102320671082\n",
      "Epoch 4595, Loss: 0.29341816902160645, Final Batch Loss: 0.10864585638046265\n",
      "Epoch 4596, Loss: 0.23156209290027618, Final Batch Loss: 0.11012434959411621\n",
      "Epoch 4597, Loss: 0.3007262945175171, Final Batch Loss: 0.1493765413761139\n",
      "Epoch 4598, Loss: 0.3063405603170395, Final Batch Loss: 0.14825531840324402\n",
      "Epoch 4599, Loss: 0.2773559167981148, Final Batch Loss: 0.15811526775360107\n",
      "Epoch 4600, Loss: 0.24451115727424622, Final Batch Loss: 0.13819783926010132\n",
      "Epoch 4601, Loss: 0.263502761721611, Final Batch Loss: 0.14686495065689087\n",
      "Epoch 4602, Loss: 0.2508213296532631, Final Batch Loss: 0.10684823244810104\n",
      "Epoch 4603, Loss: 0.27879995107650757, Final Batch Loss: 0.15122996270656586\n",
      "Epoch 4604, Loss: 0.27425678074359894, Final Batch Loss: 0.14187069237232208\n",
      "Epoch 4605, Loss: 0.2609136775135994, Final Batch Loss: 0.11745459586381912\n",
      "Epoch 4606, Loss: 0.3611224293708801, Final Batch Loss: 0.13165625929832458\n",
      "Epoch 4607, Loss: 0.24305861443281174, Final Batch Loss: 0.09552090615034103\n",
      "Epoch 4608, Loss: 0.3144013434648514, Final Batch Loss: 0.18809092044830322\n",
      "Epoch 4609, Loss: 0.22730641067028046, Final Batch Loss: 0.1276724934577942\n",
      "Epoch 4610, Loss: 0.24645545333623886, Final Batch Loss: 0.14385132491588593\n",
      "Epoch 4611, Loss: 0.2511395737528801, Final Batch Loss: 0.112615205347538\n",
      "Epoch 4612, Loss: 0.27392059564590454, Final Batch Loss: 0.1431095004081726\n",
      "Epoch 4613, Loss: 0.25286703556776047, Final Batch Loss: 0.12800785899162292\n",
      "Epoch 4614, Loss: 0.3295547813177109, Final Batch Loss: 0.21020294725894928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4615, Loss: 0.25121647864580154, Final Batch Loss: 0.12802791595458984\n",
      "Epoch 4616, Loss: 0.27686989307403564, Final Batch Loss: 0.14008952677249908\n",
      "Epoch 4617, Loss: 0.28923071920871735, Final Batch Loss: 0.15209610760211945\n",
      "Epoch 4618, Loss: 0.30320771038532257, Final Batch Loss: 0.125981405377388\n",
      "Epoch 4619, Loss: 0.27440887689590454, Final Batch Loss: 0.11632546782493591\n",
      "Epoch 4620, Loss: 0.24993164092302322, Final Batch Loss: 0.13599540293216705\n",
      "Epoch 4621, Loss: 0.23498667031526566, Final Batch Loss: 0.1192501038312912\n",
      "Epoch 4622, Loss: 0.27348238229751587, Final Batch Loss: 0.1269986629486084\n",
      "Epoch 4623, Loss: 0.26217515021562576, Final Batch Loss: 0.12033679336309433\n",
      "Epoch 4624, Loss: 0.2729981392621994, Final Batch Loss: 0.11935311555862427\n",
      "Epoch 4625, Loss: 0.26947037875652313, Final Batch Loss: 0.14403414726257324\n",
      "Epoch 4626, Loss: 0.3172641098499298, Final Batch Loss: 0.16636903584003448\n",
      "Epoch 4627, Loss: 0.2878086417913437, Final Batch Loss: 0.13376668095588684\n",
      "Epoch 4628, Loss: 0.26718316972255707, Final Batch Loss: 0.14366145431995392\n",
      "Epoch 4629, Loss: 0.24410490691661835, Final Batch Loss: 0.11050009727478027\n",
      "Epoch 4630, Loss: 0.31180690228939056, Final Batch Loss: 0.1486753225326538\n",
      "Epoch 4631, Loss: 0.30271556228399277, Final Batch Loss: 0.11500044912099838\n",
      "Epoch 4632, Loss: 0.24491194635629654, Final Batch Loss: 0.1249777227640152\n",
      "Epoch 4633, Loss: 0.22238410264253616, Final Batch Loss: 0.10724054276943207\n",
      "Epoch 4634, Loss: 0.32516542077064514, Final Batch Loss: 0.1410137563943863\n",
      "Epoch 4635, Loss: 0.2539169117808342, Final Batch Loss: 0.14670835435390472\n",
      "Epoch 4636, Loss: 0.2700463533401489, Final Batch Loss: 0.1410476118326187\n",
      "Epoch 4637, Loss: 0.228486567735672, Final Batch Loss: 0.11808473616838455\n",
      "Epoch 4638, Loss: 0.2505677863955498, Final Batch Loss: 0.1316511034965515\n",
      "Epoch 4639, Loss: 0.24344991147518158, Final Batch Loss: 0.11715370416641235\n",
      "Epoch 4640, Loss: 0.255524642765522, Final Batch Loss: 0.14961564540863037\n",
      "Epoch 4641, Loss: 0.22306762635707855, Final Batch Loss: 0.10891115665435791\n",
      "Epoch 4642, Loss: 0.2214835360646248, Final Batch Loss: 0.1145685538649559\n",
      "Epoch 4643, Loss: 0.2603699564933777, Final Batch Loss: 0.13225600123405457\n",
      "Epoch 4644, Loss: 0.2671211436390877, Final Batch Loss: 0.10831189900636673\n",
      "Epoch 4645, Loss: 0.30308014154434204, Final Batch Loss: 0.13222581148147583\n",
      "Epoch 4646, Loss: 0.2541622146964073, Final Batch Loss: 0.13815809786319733\n",
      "Epoch 4647, Loss: 0.25111478567123413, Final Batch Loss: 0.1696190983057022\n",
      "Epoch 4648, Loss: 0.27998189628124237, Final Batch Loss: 0.1421802043914795\n",
      "Epoch 4649, Loss: 0.3270784318447113, Final Batch Loss: 0.19320537149906158\n",
      "Epoch 4650, Loss: 0.27027594298124313, Final Batch Loss: 0.15500515699386597\n",
      "Epoch 4651, Loss: 0.2434692680835724, Final Batch Loss: 0.10443887114524841\n",
      "Epoch 4652, Loss: 0.2680254876613617, Final Batch Loss: 0.125709667801857\n",
      "Epoch 4653, Loss: 0.23991923034191132, Final Batch Loss: 0.13422459363937378\n",
      "Epoch 4654, Loss: 0.28425025939941406, Final Batch Loss: 0.16640032827854156\n",
      "Epoch 4655, Loss: 0.2868802100419998, Final Batch Loss: 0.10731615126132965\n",
      "Epoch 4656, Loss: 0.2924058586359024, Final Batch Loss: 0.1582060307264328\n",
      "Epoch 4657, Loss: 0.2915043234825134, Final Batch Loss: 0.13925407826900482\n",
      "Epoch 4658, Loss: 0.27271369844675064, Final Batch Loss: 0.12168354541063309\n",
      "Epoch 4659, Loss: 0.2612558603286743, Final Batch Loss: 0.14130182564258575\n",
      "Epoch 4660, Loss: 0.264439657330513, Final Batch Loss: 0.12361221015453339\n",
      "Epoch 4661, Loss: 0.3048614263534546, Final Batch Loss: 0.1562427580356598\n",
      "Epoch 4662, Loss: 0.2461428865790367, Final Batch Loss: 0.13236328959465027\n",
      "Epoch 4663, Loss: 0.3707698583602905, Final Batch Loss: 0.22914789617061615\n",
      "Epoch 4664, Loss: 0.283801794052124, Final Batch Loss: 0.14409133791923523\n",
      "Epoch 4665, Loss: 0.25689616799354553, Final Batch Loss: 0.12403616309165955\n",
      "Epoch 4666, Loss: 0.25483760982751846, Final Batch Loss: 0.12333113700151443\n",
      "Epoch 4667, Loss: 0.3099523112177849, Final Batch Loss: 0.20083463191986084\n",
      "Epoch 4668, Loss: 0.2778746262192726, Final Batch Loss: 0.1657947152853012\n",
      "Epoch 4669, Loss: 0.2726302146911621, Final Batch Loss: 0.1096360832452774\n",
      "Epoch 4670, Loss: 0.24238009750843048, Final Batch Loss: 0.1106686145067215\n",
      "Epoch 4671, Loss: 0.2502724975347519, Final Batch Loss: 0.1403222233057022\n",
      "Epoch 4672, Loss: 0.2470107451081276, Final Batch Loss: 0.12001524120569229\n",
      "Epoch 4673, Loss: 0.28866204619407654, Final Batch Loss: 0.15608631074428558\n",
      "Epoch 4674, Loss: 0.2733544409275055, Final Batch Loss: 0.13638794422149658\n",
      "Epoch 4675, Loss: 0.27161168307065964, Final Batch Loss: 0.149376779794693\n",
      "Epoch 4676, Loss: 0.2707738131284714, Final Batch Loss: 0.14114193618297577\n",
      "Epoch 4677, Loss: 0.27610278129577637, Final Batch Loss: 0.13749851286411285\n",
      "Epoch 4678, Loss: 0.2462933138012886, Final Batch Loss: 0.10934542864561081\n",
      "Epoch 4679, Loss: 0.2480171099305153, Final Batch Loss: 0.1069398745894432\n",
      "Epoch 4680, Loss: 0.2404637709259987, Final Batch Loss: 0.13938936591148376\n",
      "Epoch 4681, Loss: 0.23999172449111938, Final Batch Loss: 0.11398054659366608\n",
      "Epoch 4682, Loss: 0.2689596265554428, Final Batch Loss: 0.14304304122924805\n",
      "Epoch 4683, Loss: 0.2395751252770424, Final Batch Loss: 0.10792828351259232\n",
      "Epoch 4684, Loss: 0.23353447020053864, Final Batch Loss: 0.1284869760274887\n",
      "Epoch 4685, Loss: 0.2382349967956543, Final Batch Loss: 0.14032289385795593\n",
      "Epoch 4686, Loss: 0.2545613422989845, Final Batch Loss: 0.1210634782910347\n",
      "Epoch 4687, Loss: 0.2778700143098831, Final Batch Loss: 0.11276715993881226\n",
      "Epoch 4688, Loss: 0.24041932821273804, Final Batch Loss: 0.12135287374258041\n",
      "Epoch 4689, Loss: 0.24679938703775406, Final Batch Loss: 0.1367892324924469\n",
      "Epoch 4690, Loss: 0.22345761954784393, Final Batch Loss: 0.12429004162549973\n",
      "Epoch 4691, Loss: 0.22714326530694962, Final Batch Loss: 0.09543336182832718\n",
      "Epoch 4692, Loss: 0.28112325072288513, Final Batch Loss: 0.13209295272827148\n",
      "Epoch 4693, Loss: 0.2430466115474701, Final Batch Loss: 0.11327378451824188\n",
      "Epoch 4694, Loss: 0.23821291327476501, Final Batch Loss: 0.14444248378276825\n",
      "Epoch 4695, Loss: 0.23159566521644592, Final Batch Loss: 0.11203382909297943\n",
      "Epoch 4696, Loss: 0.3712153136730194, Final Batch Loss: 0.1388673037290573\n",
      "Epoch 4697, Loss: 0.2649877443909645, Final Batch Loss: 0.1487630158662796\n",
      "Epoch 4698, Loss: 0.2706541568040848, Final Batch Loss: 0.14279016852378845\n",
      "Epoch 4699, Loss: 0.25188739597797394, Final Batch Loss: 0.10455401241779327\n",
      "Epoch 4700, Loss: 0.27437491714954376, Final Batch Loss: 0.13379846513271332\n",
      "Epoch 4701, Loss: 0.2871258109807968, Final Batch Loss: 0.13108453154563904\n",
      "Epoch 4702, Loss: 0.2566654458642006, Final Batch Loss: 0.13530156016349792\n",
      "Epoch 4703, Loss: 0.2881408631801605, Final Batch Loss: 0.1273345798254013\n",
      "Epoch 4704, Loss: 0.24046798795461655, Final Batch Loss: 0.11567286401987076\n",
      "Epoch 4705, Loss: 0.388569712638855, Final Batch Loss: 0.15913055837154388\n",
      "Epoch 4706, Loss: 0.2941589057445526, Final Batch Loss: 0.10584390163421631\n",
      "Epoch 4707, Loss: 0.2799438312649727, Final Batch Loss: 0.11276903003454208\n",
      "Epoch 4708, Loss: 0.2579844295978546, Final Batch Loss: 0.1280018538236618\n",
      "Epoch 4709, Loss: 0.2648681253194809, Final Batch Loss: 0.11530269682407379\n",
      "Epoch 4710, Loss: 0.2933450862765312, Final Batch Loss: 0.12127784639596939\n",
      "Epoch 4711, Loss: 0.2699492275714874, Final Batch Loss: 0.12394669651985168\n",
      "Epoch 4712, Loss: 0.2694113254547119, Final Batch Loss: 0.14191317558288574\n",
      "Epoch 4713, Loss: 0.2850220054388046, Final Batch Loss: 0.14022187888622284\n",
      "Epoch 4714, Loss: 0.24818724393844604, Final Batch Loss: 0.15096688270568848\n",
      "Epoch 4715, Loss: 0.307864174246788, Final Batch Loss: 0.1359400749206543\n",
      "Epoch 4716, Loss: 0.28434794396162033, Final Batch Loss: 0.182677760720253\n",
      "Epoch 4717, Loss: 0.2810145765542984, Final Batch Loss: 0.14571678638458252\n",
      "Epoch 4718, Loss: 0.2476348727941513, Final Batch Loss: 0.11352735757827759\n",
      "Epoch 4719, Loss: 0.2770535349845886, Final Batch Loss: 0.12401324510574341\n",
      "Epoch 4720, Loss: 0.25111062824726105, Final Batch Loss: 0.12031508982181549\n",
      "Epoch 4721, Loss: 0.2583478093147278, Final Batch Loss: 0.12935703992843628\n",
      "Epoch 4722, Loss: 0.2959808185696602, Final Batch Loss: 0.11245932430028915\n",
      "Epoch 4723, Loss: 0.39256130158901215, Final Batch Loss: 0.15551559627056122\n",
      "Epoch 4724, Loss: 0.25975994765758514, Final Batch Loss: 0.1392461657524109\n",
      "Epoch 4725, Loss: 0.27064647525548935, Final Batch Loss: 0.12162172049283981\n",
      "Epoch 4726, Loss: 0.35321471095085144, Final Batch Loss: 0.19806690514087677\n",
      "Epoch 4727, Loss: 0.23876111209392548, Final Batch Loss: 0.1263924092054367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4728, Loss: 0.2549392133951187, Final Batch Loss: 0.1282728612422943\n",
      "Epoch 4729, Loss: 0.29637742042541504, Final Batch Loss: 0.14801180362701416\n",
      "Epoch 4730, Loss: 0.27073366940021515, Final Batch Loss: 0.14872165024280548\n",
      "Epoch 4731, Loss: 0.37040895223617554, Final Batch Loss: 0.1445566862821579\n",
      "Epoch 4732, Loss: 0.23584553599357605, Final Batch Loss: 0.13487893342971802\n",
      "Epoch 4733, Loss: 0.2775426506996155, Final Batch Loss: 0.1369563788175583\n",
      "Epoch 4734, Loss: 0.23317991197109222, Final Batch Loss: 0.11853980273008347\n",
      "Epoch 4735, Loss: 0.25910231471061707, Final Batch Loss: 0.12909206748008728\n",
      "Epoch 4736, Loss: 0.23526304960250854, Final Batch Loss: 0.08210872113704681\n",
      "Epoch 4737, Loss: 0.28430475294589996, Final Batch Loss: 0.13573673367500305\n",
      "Epoch 4738, Loss: 0.2690410017967224, Final Batch Loss: 0.11736786365509033\n",
      "Epoch 4739, Loss: 0.2899235114455223, Final Batch Loss: 0.17264145612716675\n",
      "Epoch 4740, Loss: 0.2657451927661896, Final Batch Loss: 0.13559682667255402\n",
      "Epoch 4741, Loss: 0.2657427489757538, Final Batch Loss: 0.14880076050758362\n",
      "Epoch 4742, Loss: 0.3186233341693878, Final Batch Loss: 0.18738149106502533\n",
      "Epoch 4743, Loss: 0.24211585521697998, Final Batch Loss: 0.10651177167892456\n",
      "Epoch 4744, Loss: 0.2573832869529724, Final Batch Loss: 0.12812170386314392\n",
      "Epoch 4745, Loss: 0.2201809138059616, Final Batch Loss: 0.09904135018587112\n",
      "Epoch 4746, Loss: 0.27173835039138794, Final Batch Loss: 0.1422976702451706\n",
      "Epoch 4747, Loss: 0.2504521757364273, Final Batch Loss: 0.13498422503471375\n",
      "Epoch 4748, Loss: 0.4026159644126892, Final Batch Loss: 0.14089322090148926\n",
      "Epoch 4749, Loss: 0.25468047708272934, Final Batch Loss: 0.13231731951236725\n",
      "Epoch 4750, Loss: 0.26961512863636017, Final Batch Loss: 0.16374658048152924\n",
      "Epoch 4751, Loss: 0.2659853398799896, Final Batch Loss: 0.11891603469848633\n",
      "Epoch 4752, Loss: 0.2877091020345688, Final Batch Loss: 0.14228883385658264\n",
      "Epoch 4753, Loss: 0.2615545690059662, Final Batch Loss: 0.13704730570316315\n",
      "Epoch 4754, Loss: 0.2449076920747757, Final Batch Loss: 0.11493347585201263\n",
      "Epoch 4755, Loss: 0.2666381821036339, Final Batch Loss: 0.1161622628569603\n",
      "Epoch 4756, Loss: 0.23452410846948624, Final Batch Loss: 0.12471536546945572\n",
      "Epoch 4757, Loss: 0.274588942527771, Final Batch Loss: 0.1353679746389389\n",
      "Epoch 4758, Loss: 0.30730633437633514, Final Batch Loss: 0.1827879101037979\n",
      "Epoch 4759, Loss: 0.28911593556404114, Final Batch Loss: 0.14338791370391846\n",
      "Epoch 4760, Loss: 0.2761552035808563, Final Batch Loss: 0.17317742109298706\n",
      "Epoch 4761, Loss: 0.2114916741847992, Final Batch Loss: 0.12195774912834167\n",
      "Epoch 4762, Loss: 0.2642887681722641, Final Batch Loss: 0.14885461330413818\n",
      "Epoch 4763, Loss: 0.25695085525512695, Final Batch Loss: 0.14021623134613037\n",
      "Epoch 4764, Loss: 0.25889983773231506, Final Batch Loss: 0.11978542804718018\n",
      "Epoch 4765, Loss: 0.26011502742767334, Final Batch Loss: 0.1529364287853241\n",
      "Epoch 4766, Loss: 0.3144533783197403, Final Batch Loss: 0.17362067103385925\n",
      "Epoch 4767, Loss: 0.2907222807407379, Final Batch Loss: 0.15584951639175415\n",
      "Epoch 4768, Loss: 0.28497885167598724, Final Batch Loss: 0.13467799127101898\n",
      "Epoch 4769, Loss: 0.23870684951543808, Final Batch Loss: 0.1078069731593132\n",
      "Epoch 4770, Loss: 0.2696153298020363, Final Batch Loss: 0.1622745841741562\n",
      "Epoch 4771, Loss: 0.3029612749814987, Final Batch Loss: 0.13389328122138977\n",
      "Epoch 4772, Loss: 0.24343053251504898, Final Batch Loss: 0.09816376119852066\n",
      "Epoch 4773, Loss: 0.391061432659626, Final Batch Loss: 0.26890531182289124\n",
      "Epoch 4774, Loss: 0.23780269920825958, Final Batch Loss: 0.12504182755947113\n",
      "Epoch 4775, Loss: 0.3221399709582329, Final Batch Loss: 0.20248670876026154\n",
      "Epoch 4776, Loss: 0.30950626730918884, Final Batch Loss: 0.14078612625598907\n",
      "Epoch 4777, Loss: 0.277531698346138, Final Batch Loss: 0.1344493329524994\n",
      "Epoch 4778, Loss: 0.22806232422590256, Final Batch Loss: 0.107053242623806\n",
      "Epoch 4779, Loss: 0.24470407515764236, Final Batch Loss: 0.12877078354358673\n",
      "Epoch 4780, Loss: 0.2907250374555588, Final Batch Loss: 0.1348293125629425\n",
      "Epoch 4781, Loss: 0.23474113643169403, Final Batch Loss: 0.11281973868608475\n",
      "Epoch 4782, Loss: 0.2552080377936363, Final Batch Loss: 0.15155793726444244\n",
      "Epoch 4783, Loss: 0.2693183571100235, Final Batch Loss: 0.11484542489051819\n",
      "Epoch 4784, Loss: 0.24960937350988388, Final Batch Loss: 0.113074891269207\n",
      "Epoch 4785, Loss: 0.2640114575624466, Final Batch Loss: 0.1337336003780365\n",
      "Epoch 4786, Loss: 0.22873739153146744, Final Batch Loss: 0.09089917689561844\n",
      "Epoch 4787, Loss: 0.25898856669664383, Final Batch Loss: 0.14059771597385406\n",
      "Epoch 4788, Loss: 0.26559098809957504, Final Batch Loss: 0.14850357174873352\n",
      "Epoch 4789, Loss: 0.27969644218683243, Final Batch Loss: 0.12133749574422836\n",
      "Epoch 4790, Loss: 0.2475838139653206, Final Batch Loss: 0.14855794608592987\n",
      "Epoch 4791, Loss: 0.28993842005729675, Final Batch Loss: 0.1538861095905304\n",
      "Epoch 4792, Loss: 0.32084161043167114, Final Batch Loss: 0.11650337278842926\n",
      "Epoch 4793, Loss: 0.2866138219833374, Final Batch Loss: 0.13300098478794098\n",
      "Epoch 4794, Loss: 0.23506591469049454, Final Batch Loss: 0.12099859118461609\n",
      "Epoch 4795, Loss: 0.2573847845196724, Final Batch Loss: 0.11875427514314651\n",
      "Epoch 4796, Loss: 0.3011748194694519, Final Batch Loss: 0.17604093253612518\n",
      "Epoch 4797, Loss: 0.29725244641304016, Final Batch Loss: 0.16934321820735931\n",
      "Epoch 4798, Loss: 0.21769927442073822, Final Batch Loss: 0.12359701097011566\n",
      "Epoch 4799, Loss: 0.271819531917572, Final Batch Loss: 0.13124831020832062\n",
      "Epoch 4800, Loss: 0.34670020639896393, Final Batch Loss: 0.18941064178943634\n",
      "Epoch 4801, Loss: 0.26876266300678253, Final Batch Loss: 0.11683449149131775\n",
      "Epoch 4802, Loss: 0.27686598896980286, Final Batch Loss: 0.1377343237400055\n",
      "Epoch 4803, Loss: 0.24172940850257874, Final Batch Loss: 0.1059434562921524\n",
      "Epoch 4804, Loss: 0.25095319747924805, Final Batch Loss: 0.13010986149311066\n",
      "Epoch 4805, Loss: 0.29220178723335266, Final Batch Loss: 0.1347348839044571\n",
      "Epoch 4806, Loss: 0.26257000863552094, Final Batch Loss: 0.1290687769651413\n",
      "Epoch 4807, Loss: 0.23330343514680862, Final Batch Loss: 0.10282967239618301\n",
      "Epoch 4808, Loss: 0.2481597661972046, Final Batch Loss: 0.10305483639240265\n",
      "Epoch 4809, Loss: 0.24447207897901535, Final Batch Loss: 0.11630698293447495\n",
      "Epoch 4810, Loss: 0.2658182680606842, Final Batch Loss: 0.14435911178588867\n",
      "Epoch 4811, Loss: 0.2559455558657646, Final Batch Loss: 0.11874751001596451\n",
      "Epoch 4812, Loss: 0.2504909858107567, Final Batch Loss: 0.1393209844827652\n",
      "Epoch 4813, Loss: 0.24061405658721924, Final Batch Loss: 0.12547048926353455\n",
      "Epoch 4814, Loss: 0.21865513920783997, Final Batch Loss: 0.09908268600702286\n",
      "Epoch 4815, Loss: 0.2493901476264, Final Batch Loss: 0.1072506234049797\n",
      "Epoch 4816, Loss: 0.24785641580820084, Final Batch Loss: 0.10810130089521408\n",
      "Epoch 4817, Loss: 0.24573758989572525, Final Batch Loss: 0.10295230895280838\n",
      "Epoch 4818, Loss: 0.25622134655714035, Final Batch Loss: 0.09315510839223862\n",
      "Epoch 4819, Loss: 0.21964940428733826, Final Batch Loss: 0.09730774909257889\n",
      "Epoch 4820, Loss: 0.2908100336790085, Final Batch Loss: 0.15208929777145386\n",
      "Epoch 4821, Loss: 0.24638648331165314, Final Batch Loss: 0.12010963261127472\n",
      "Epoch 4822, Loss: 0.2745327949523926, Final Batch Loss: 0.14814633131027222\n",
      "Epoch 4823, Loss: 0.2691977918148041, Final Batch Loss: 0.1426820456981659\n",
      "Epoch 4824, Loss: 0.2317240759730339, Final Batch Loss: 0.11624778062105179\n",
      "Epoch 4825, Loss: 0.24471425265073776, Final Batch Loss: 0.1292351335287094\n",
      "Epoch 4826, Loss: 0.27063651382923126, Final Batch Loss: 0.13044896721839905\n",
      "Epoch 4827, Loss: 0.3222770094871521, Final Batch Loss: 0.14617803692817688\n",
      "Epoch 4828, Loss: 0.2963034212589264, Final Batch Loss: 0.1794959008693695\n",
      "Epoch 4829, Loss: 0.30073001980781555, Final Batch Loss: 0.16358062624931335\n",
      "Epoch 4830, Loss: 0.3090805336833, Final Batch Loss: 0.11581232398748398\n",
      "Epoch 4831, Loss: 0.27519475668668747, Final Batch Loss: 0.1595262736082077\n",
      "Epoch 4832, Loss: 0.3016625940799713, Final Batch Loss: 0.13601510226726532\n",
      "Epoch 4833, Loss: 0.2604449838399887, Final Batch Loss: 0.16498741507530212\n",
      "Epoch 4834, Loss: 0.25650840252637863, Final Batch Loss: 0.17036214470863342\n",
      "Epoch 4835, Loss: 0.27902667224407196, Final Batch Loss: 0.12352035939693451\n",
      "Epoch 4836, Loss: 0.2503613159060478, Final Batch Loss: 0.11964557319879532\n",
      "Epoch 4837, Loss: 0.25884316116571426, Final Batch Loss: 0.11009608954191208\n",
      "Epoch 4838, Loss: 0.21608754992485046, Final Batch Loss: 0.10299953818321228\n",
      "Epoch 4839, Loss: 0.30410154163837433, Final Batch Loss: 0.14905449748039246\n",
      "Epoch 4840, Loss: 0.26766641438007355, Final Batch Loss: 0.14214132726192474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4841, Loss: 0.2511315271258354, Final Batch Loss: 0.14425183832645416\n",
      "Epoch 4842, Loss: 0.2904979735612869, Final Batch Loss: 0.14128130674362183\n",
      "Epoch 4843, Loss: 0.21504785120487213, Final Batch Loss: 0.10574931651353836\n",
      "Epoch 4844, Loss: 0.2798169404268265, Final Batch Loss: 0.12163394689559937\n",
      "Epoch 4845, Loss: 0.24069586396217346, Final Batch Loss: 0.14535574615001678\n",
      "Epoch 4846, Loss: 0.27466626465320587, Final Batch Loss: 0.12561510503292084\n",
      "Epoch 4847, Loss: 0.2617490589618683, Final Batch Loss: 0.1342986524105072\n",
      "Epoch 4848, Loss: 0.30897940695285797, Final Batch Loss: 0.16709810495376587\n",
      "Epoch 4849, Loss: 0.2733786404132843, Final Batch Loss: 0.09802024066448212\n",
      "Epoch 4850, Loss: 0.2757861241698265, Final Batch Loss: 0.10531390458345413\n",
      "Epoch 4851, Loss: 0.22776900976896286, Final Batch Loss: 0.12410177290439606\n",
      "Epoch 4852, Loss: 0.2619127631187439, Final Batch Loss: 0.13108234107494354\n",
      "Epoch 4853, Loss: 0.270820677280426, Final Batch Loss: 0.14130571484565735\n",
      "Epoch 4854, Loss: 0.26414327323436737, Final Batch Loss: 0.13346503674983978\n",
      "Epoch 4855, Loss: 0.23365908861160278, Final Batch Loss: 0.09467577934265137\n",
      "Epoch 4856, Loss: 0.2725229263305664, Final Batch Loss: 0.14711013436317444\n",
      "Epoch 4857, Loss: 0.30727730691432953, Final Batch Loss: 0.1778796911239624\n",
      "Epoch 4858, Loss: 0.2540597692131996, Final Batch Loss: 0.1333589106798172\n",
      "Epoch 4859, Loss: 0.28366298973560333, Final Batch Loss: 0.15554291009902954\n",
      "Epoch 4860, Loss: 0.25160088390111923, Final Batch Loss: 0.11459245532751083\n",
      "Epoch 4861, Loss: 0.24551987648010254, Final Batch Loss: 0.1269431859254837\n",
      "Epoch 4862, Loss: 0.2486746534705162, Final Batch Loss: 0.13755910098552704\n",
      "Epoch 4863, Loss: 0.3568694591522217, Final Batch Loss: 0.14840909838676453\n",
      "Epoch 4864, Loss: 0.29552850127220154, Final Batch Loss: 0.15628491342067719\n",
      "Epoch 4865, Loss: 0.2695528715848923, Final Batch Loss: 0.14180146157741547\n",
      "Epoch 4866, Loss: 0.2441193014383316, Final Batch Loss: 0.1182427704334259\n",
      "Epoch 4867, Loss: 0.277781218290329, Final Batch Loss: 0.1277954876422882\n",
      "Epoch 4868, Loss: 0.22383254766464233, Final Batch Loss: 0.128634974360466\n",
      "Epoch 4869, Loss: 0.35932986438274384, Final Batch Loss: 0.23571982979774475\n",
      "Epoch 4870, Loss: 0.27013588696718216, Final Batch Loss: 0.15868842601776123\n",
      "Epoch 4871, Loss: 0.2523597180843353, Final Batch Loss: 0.14705973863601685\n",
      "Epoch 4872, Loss: 0.3384458124637604, Final Batch Loss: 0.20753784477710724\n",
      "Epoch 4873, Loss: 0.25396446138620377, Final Batch Loss: 0.11936336010694504\n",
      "Epoch 4874, Loss: 0.2768818065524101, Final Batch Loss: 0.1037265881896019\n",
      "Epoch 4875, Loss: 0.2396865114569664, Final Batch Loss: 0.13224320113658905\n",
      "Epoch 4876, Loss: 0.27347182482481003, Final Batch Loss: 0.1554744392633438\n",
      "Epoch 4877, Loss: 0.277405321598053, Final Batch Loss: 0.1326250582933426\n",
      "Epoch 4878, Loss: 0.2828756421804428, Final Batch Loss: 0.15875378251075745\n",
      "Epoch 4879, Loss: 0.315162755548954, Final Batch Loss: 0.19417834281921387\n",
      "Epoch 4880, Loss: 0.26255981624126434, Final Batch Loss: 0.15394409000873566\n",
      "Epoch 4881, Loss: 0.26401711255311966, Final Batch Loss: 0.14230573177337646\n",
      "Epoch 4882, Loss: 0.24169518053531647, Final Batch Loss: 0.10966208577156067\n",
      "Epoch 4883, Loss: 0.25369255244731903, Final Batch Loss: 0.1229749321937561\n",
      "Epoch 4884, Loss: 0.23755910992622375, Final Batch Loss: 0.12321938574314117\n",
      "Epoch 4885, Loss: 0.24863024055957794, Final Batch Loss: 0.13924835622310638\n",
      "Epoch 4886, Loss: 0.2722528725862503, Final Batch Loss: 0.10664205253124237\n",
      "Epoch 4887, Loss: 0.2534080296754837, Final Batch Loss: 0.09811420738697052\n",
      "Epoch 4888, Loss: 0.26983940601348877, Final Batch Loss: 0.14483903348445892\n",
      "Epoch 4889, Loss: 0.2396642565727234, Final Batch Loss: 0.12342935055494308\n",
      "Epoch 4890, Loss: 0.256708025932312, Final Batch Loss: 0.15662817656993866\n",
      "Epoch 4891, Loss: 0.28991761803627014, Final Batch Loss: 0.13583578169345856\n",
      "Epoch 4892, Loss: 0.2690574377775192, Final Batch Loss: 0.12763723731040955\n",
      "Epoch 4893, Loss: 0.2702327221632004, Final Batch Loss: 0.14338280260562897\n",
      "Epoch 4894, Loss: 0.22158099710941315, Final Batch Loss: 0.11238498240709305\n",
      "Epoch 4895, Loss: 0.2430163025856018, Final Batch Loss: 0.13467614352703094\n",
      "Epoch 4896, Loss: 0.25172296166419983, Final Batch Loss: 0.12566307187080383\n",
      "Epoch 4897, Loss: 0.2600153386592865, Final Batch Loss: 0.13509945571422577\n",
      "Epoch 4898, Loss: 0.23126617819070816, Final Batch Loss: 0.10625424236059189\n",
      "Epoch 4899, Loss: 0.268476739525795, Final Batch Loss: 0.15009506046772003\n",
      "Epoch 4900, Loss: 0.28448648750782013, Final Batch Loss: 0.13841871917247772\n",
      "Epoch 4901, Loss: 0.267806239426136, Final Batch Loss: 0.15520451962947845\n",
      "Epoch 4902, Loss: 0.23260045796632767, Final Batch Loss: 0.1083817109465599\n",
      "Epoch 4903, Loss: 0.252789169549942, Final Batch Loss: 0.10381726920604706\n",
      "Epoch 4904, Loss: 0.2879606634378433, Final Batch Loss: 0.14852863550186157\n",
      "Epoch 4905, Loss: 0.24152818322181702, Final Batch Loss: 0.10445638000965118\n",
      "Epoch 4906, Loss: 0.2935776561498642, Final Batch Loss: 0.15184704959392548\n",
      "Epoch 4907, Loss: 0.2615377902984619, Final Batch Loss: 0.12685233354568481\n",
      "Epoch 4908, Loss: 0.2529894486069679, Final Batch Loss: 0.1206040009856224\n",
      "Epoch 4909, Loss: 0.25018811970949173, Final Batch Loss: 0.11394738405942917\n",
      "Epoch 4910, Loss: 0.2895837426185608, Final Batch Loss: 0.14689306914806366\n",
      "Epoch 4911, Loss: 0.29870671033859253, Final Batch Loss: 0.17087675631046295\n",
      "Epoch 4912, Loss: 0.27859506011009216, Final Batch Loss: 0.12016423046588898\n",
      "Epoch 4913, Loss: 0.2317478507757187, Final Batch Loss: 0.13424713909626007\n",
      "Epoch 4914, Loss: 0.2025817111134529, Final Batch Loss: 0.12247728556394577\n",
      "Epoch 4915, Loss: 0.25949637591838837, Final Batch Loss: 0.13538911938667297\n",
      "Epoch 4916, Loss: 0.2287716567516327, Final Batch Loss: 0.11590428650379181\n",
      "Epoch 4917, Loss: 0.26606500893831253, Final Batch Loss: 0.14567092061042786\n",
      "Epoch 4918, Loss: 0.3367079645395279, Final Batch Loss: 0.15311266481876373\n",
      "Epoch 4919, Loss: 0.27410081028938293, Final Batch Loss: 0.14695116877555847\n",
      "Epoch 4920, Loss: 0.22344163805246353, Final Batch Loss: 0.10638140887022018\n",
      "Epoch 4921, Loss: 0.2609077915549278, Final Batch Loss: 0.14907953143119812\n",
      "Epoch 4922, Loss: 0.24769418686628342, Final Batch Loss: 0.09950091689825058\n",
      "Epoch 4923, Loss: 0.2684098780155182, Final Batch Loss: 0.12871693074703217\n",
      "Epoch 4924, Loss: 0.23614409565925598, Final Batch Loss: 0.1282588243484497\n",
      "Epoch 4925, Loss: 0.25209737569093704, Final Batch Loss: 0.14530308544635773\n",
      "Epoch 4926, Loss: 0.2362195998430252, Final Batch Loss: 0.12653587758541107\n",
      "Epoch 4927, Loss: 0.28967227041721344, Final Batch Loss: 0.16983239352703094\n",
      "Epoch 4928, Loss: 0.2473926767706871, Final Batch Loss: 0.12400524318218231\n",
      "Epoch 4929, Loss: 0.2675860971212387, Final Batch Loss: 0.12817813456058502\n",
      "Epoch 4930, Loss: 0.28165730834007263, Final Batch Loss: 0.126285582780838\n",
      "Epoch 4931, Loss: 0.2584986612200737, Final Batch Loss: 0.14667484164237976\n",
      "Epoch 4932, Loss: 0.23487897217273712, Final Batch Loss: 0.09933333098888397\n",
      "Epoch 4933, Loss: 0.28982238471508026, Final Batch Loss: 0.10110801458358765\n",
      "Epoch 4934, Loss: 0.3021679073572159, Final Batch Loss: 0.12374284863471985\n",
      "Epoch 4935, Loss: 0.2619102746248245, Final Batch Loss: 0.10653960704803467\n",
      "Epoch 4936, Loss: 0.24377289414405823, Final Batch Loss: 0.123833067715168\n",
      "Epoch 4937, Loss: 0.2910096198320389, Final Batch Loss: 0.15762972831726074\n",
      "Epoch 4938, Loss: 0.23773431032896042, Final Batch Loss: 0.12596653401851654\n",
      "Epoch 4939, Loss: 0.2714412286877632, Final Batch Loss: 0.14851228892803192\n",
      "Epoch 4940, Loss: 0.285519003868103, Final Batch Loss: 0.16582517325878143\n",
      "Epoch 4941, Loss: 0.26478273421525955, Final Batch Loss: 0.11778552085161209\n",
      "Epoch 4942, Loss: 0.21377220004796982, Final Batch Loss: 0.11014410108327866\n",
      "Epoch 4943, Loss: 0.28476421535015106, Final Batch Loss: 0.10780341923236847\n",
      "Epoch 4944, Loss: 0.24667024612426758, Final Batch Loss: 0.13185082376003265\n",
      "Epoch 4945, Loss: 0.25929026305675507, Final Batch Loss: 0.1331719309091568\n",
      "Epoch 4946, Loss: 0.25059080868959427, Final Batch Loss: 0.10117032378911972\n",
      "Epoch 4947, Loss: 0.24889502674341202, Final Batch Loss: 0.09928012639284134\n",
      "Epoch 4948, Loss: 0.21694505214691162, Final Batch Loss: 0.11285754293203354\n",
      "Epoch 4949, Loss: 0.2582871541380882, Final Batch Loss: 0.11560369282960892\n",
      "Epoch 4950, Loss: 0.27845966815948486, Final Batch Loss: 0.1386559009552002\n",
      "Epoch 4951, Loss: 0.23204616457223892, Final Batch Loss: 0.11002064496278763\n",
      "Epoch 4952, Loss: 0.3038490414619446, Final Batch Loss: 0.1761627048254013\n",
      "Epoch 4953, Loss: 0.25950881838798523, Final Batch Loss: 0.11543227732181549\n",
      "Epoch 4954, Loss: 0.29470790177583694, Final Batch Loss: 0.11281340569257736\n",
      "Epoch 4955, Loss: 0.2468819096684456, Final Batch Loss: 0.11640005558729172\n",
      "Epoch 4956, Loss: 0.2925747409462929, Final Batch Loss: 0.1164981946349144\n",
      "Epoch 4957, Loss: 0.21741439402103424, Final Batch Loss: 0.11389100551605225\n",
      "Epoch 4958, Loss: 0.26090389490127563, Final Batch Loss: 0.1710725873708725\n",
      "Epoch 4959, Loss: 0.2629868984222412, Final Batch Loss: 0.12666676938533783\n",
      "Epoch 4960, Loss: 0.32913656532764435, Final Batch Loss: 0.1749853938817978\n",
      "Epoch 4961, Loss: 0.26289208233356476, Final Batch Loss: 0.15168218314647675\n",
      "Epoch 4962, Loss: 0.24688297510147095, Final Batch Loss: 0.14181186258792877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4963, Loss: 0.26286157220602036, Final Batch Loss: 0.14943668246269226\n",
      "Epoch 4964, Loss: 0.25064025074243546, Final Batch Loss: 0.12036911398172379\n",
      "Epoch 4965, Loss: 0.26538845896720886, Final Batch Loss: 0.14262928068637848\n",
      "Epoch 4966, Loss: 0.2497604936361313, Final Batch Loss: 0.12439297139644623\n",
      "Epoch 4967, Loss: 0.2573613375425339, Final Batch Loss: 0.12553168833255768\n",
      "Epoch 4968, Loss: 0.24052844941616058, Final Batch Loss: 0.12840373814105988\n",
      "Epoch 4969, Loss: 0.21852248907089233, Final Batch Loss: 0.11583758145570755\n",
      "Epoch 4970, Loss: 0.29862524569034576, Final Batch Loss: 0.1751428097486496\n",
      "Epoch 4971, Loss: 0.21840111911296844, Final Batch Loss: 0.08902603387832642\n",
      "Epoch 4972, Loss: 0.27792317420244217, Final Batch Loss: 0.15859897434711456\n",
      "Epoch 4973, Loss: 0.27024471014738083, Final Batch Loss: 0.16239576041698456\n",
      "Epoch 4974, Loss: 0.26397348940372467, Final Batch Loss: 0.12262731790542603\n",
      "Epoch 4975, Loss: 0.23206942528486252, Final Batch Loss: 0.11626560986042023\n",
      "Epoch 4976, Loss: 0.2888159230351448, Final Batch Loss: 0.1783379465341568\n",
      "Epoch 4977, Loss: 0.24221710860729218, Final Batch Loss: 0.11312688887119293\n",
      "Epoch 4978, Loss: 0.2607990875840187, Final Batch Loss: 0.09811251610517502\n",
      "Epoch 4979, Loss: 0.21540573239326477, Final Batch Loss: 0.10602489858865738\n",
      "Epoch 4980, Loss: 0.265689454972744, Final Batch Loss: 0.10608457773923874\n",
      "Epoch 4981, Loss: 0.24112878739833832, Final Batch Loss: 0.1429685652256012\n",
      "Epoch 4982, Loss: 0.252422034740448, Final Batch Loss: 0.08927413821220398\n",
      "Epoch 4983, Loss: 0.3080562502145767, Final Batch Loss: 0.15479861199855804\n",
      "Epoch 4984, Loss: 0.23855354636907578, Final Batch Loss: 0.10783060640096664\n",
      "Epoch 4985, Loss: 0.22053493559360504, Final Batch Loss: 0.14059396088123322\n",
      "Epoch 4986, Loss: 0.2736610323190689, Final Batch Loss: 0.16730055212974548\n",
      "Epoch 4987, Loss: 0.27408140897750854, Final Batch Loss: 0.13579484820365906\n",
      "Epoch 4988, Loss: 0.24232523888349533, Final Batch Loss: 0.1250302642583847\n",
      "Epoch 4989, Loss: 0.25011854618787766, Final Batch Loss: 0.12979668378829956\n",
      "Epoch 4990, Loss: 0.2296624705195427, Final Batch Loss: 0.11062056571245193\n",
      "Epoch 4991, Loss: 0.2490113228559494, Final Batch Loss: 0.14979401230812073\n",
      "Epoch 4992, Loss: 0.2545374259352684, Final Batch Loss: 0.1232622042298317\n",
      "Epoch 4993, Loss: 0.20073244720697403, Final Batch Loss: 0.09482632577419281\n",
      "Epoch 4994, Loss: 0.2622688263654709, Final Batch Loss: 0.12894576787948608\n",
      "Epoch 4995, Loss: 0.2741677910089493, Final Batch Loss: 0.1418636590242386\n",
      "Epoch 4996, Loss: 0.3042033016681671, Final Batch Loss: 0.15434537827968597\n",
      "Epoch 4997, Loss: 0.299653097987175, Final Batch Loss: 0.11323763430118561\n",
      "Epoch 4998, Loss: 0.2552099823951721, Final Batch Loss: 0.1384608894586563\n",
      "Epoch 4999, Loss: 0.2819022685289383, Final Batch Loss: 0.13624714314937592\n",
      "Epoch 5000, Loss: 0.23155710101127625, Final Batch Loss: 0.1110420674085617\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29  1  0  0  0  0  5  0  0]\n",
      " [ 2 33  0  0  0  0  0  0  0]\n",
      " [33  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0 35  0  0  0  0  0]\n",
      " [ 0  0  0  0 35  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0 21]\n",
      " [ 0  0  0  0  0  0 35  0  0]\n",
      " [ 0 10  0  0  1  0 12 12  0]\n",
      " [ 0  0  0  0  0 24  0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.453     0.829     0.586        35\n",
      "         1.0      0.750     0.943     0.835        35\n",
      "         2.0      1.000     0.057     0.108        35\n",
      "         3.0      1.000     1.000     1.000        35\n",
      "         4.0      0.972     1.000     0.986        35\n",
      "         5.0      0.368     0.400     0.384        35\n",
      "         6.0      0.673     1.000     0.805        35\n",
      "         7.0      1.000     0.343     0.511        35\n",
      "         8.0      0.344     0.314     0.328        35\n",
      "\n",
      "    accuracy                          0.654       315\n",
      "   macro avg      0.729     0.654     0.616       315\n",
      "weighted avg      0.729     0.654     0.616       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7871666666666667"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET RID OF 2, 5, 8 for JUST DYNAMIC\n",
    "(0.586 + 0.835 + 1 + 0.986 + 0.805 + 0.511) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6158888888888889"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.586+0.835+0.108+1+0.986+0.384+0.805+0.511+0.328)/9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
