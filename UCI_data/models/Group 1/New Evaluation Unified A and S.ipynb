{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.452085971832275, Final Batch Loss: 2.2119810581207275\n",
      "Epoch 2, Loss: 4.451161623001099, Final Batch Loss: 2.2264087200164795\n",
      "Epoch 3, Loss: 4.443710803985596, Final Batch Loss: 2.2184813022613525\n",
      "Epoch 4, Loss: 4.442732334136963, Final Batch Loss: 2.2178618907928467\n",
      "Epoch 5, Loss: 4.442281723022461, Final Batch Loss: 2.2302377223968506\n",
      "Epoch 6, Loss: 4.439357280731201, Final Batch Loss: 2.230043411254883\n",
      "Epoch 7, Loss: 4.420681476593018, Final Batch Loss: 2.195150375366211\n",
      "Epoch 8, Loss: 4.412769317626953, Final Batch Loss: 2.188819408416748\n",
      "Epoch 9, Loss: 4.4177327156066895, Final Batch Loss: 2.2168095111846924\n",
      "Epoch 10, Loss: 4.420544624328613, Final Batch Loss: 2.2267842292785645\n",
      "Epoch 11, Loss: 4.404106616973877, Final Batch Loss: 2.203859806060791\n",
      "Epoch 12, Loss: 4.3958775997161865, Final Batch Loss: 2.2021379470825195\n",
      "Epoch 13, Loss: 4.384618520736694, Final Batch Loss: 2.188055992126465\n",
      "Epoch 14, Loss: 4.378288745880127, Final Batch Loss: 2.19391131401062\n",
      "Epoch 15, Loss: 4.3675830364227295, Final Batch Loss: 2.181424856185913\n",
      "Epoch 16, Loss: 4.338922500610352, Final Batch Loss: 2.1604397296905518\n",
      "Epoch 17, Loss: 4.342382192611694, Final Batch Loss: 2.1643807888031006\n",
      "Epoch 18, Loss: 4.333442211151123, Final Batch Loss: 2.1701109409332275\n",
      "Epoch 19, Loss: 4.2941977977752686, Final Batch Loss: 2.13472580909729\n",
      "Epoch 20, Loss: 4.270154237747192, Final Batch Loss: 2.1272873878479004\n",
      "Epoch 21, Loss: 4.263235807418823, Final Batch Loss: 2.1254398822784424\n",
      "Epoch 22, Loss: 4.23350715637207, Final Batch Loss: 2.126842737197876\n",
      "Epoch 23, Loss: 4.2126195430755615, Final Batch Loss: 2.1092331409454346\n",
      "Epoch 24, Loss: 4.189568758010864, Final Batch Loss: 2.082714319229126\n",
      "Epoch 25, Loss: 4.1716883182525635, Final Batch Loss: 2.075876235961914\n",
      "Epoch 26, Loss: 4.154684066772461, Final Batch Loss: 2.088320732116699\n",
      "Epoch 27, Loss: 4.072684288024902, Final Batch Loss: 2.0397186279296875\n",
      "Epoch 28, Loss: 4.05541205406189, Final Batch Loss: 2.0094988346099854\n",
      "Epoch 29, Loss: 4.038761615753174, Final Batch Loss: 2.018634796142578\n",
      "Epoch 30, Loss: 4.022523999214172, Final Batch Loss: 1.9892059564590454\n",
      "Epoch 31, Loss: 3.9457281827926636, Final Batch Loss: 1.9504895210266113\n",
      "Epoch 32, Loss: 3.960820198059082, Final Batch Loss: 1.9679679870605469\n",
      "Epoch 33, Loss: 3.9703716039657593, Final Batch Loss: 2.001555919647217\n",
      "Epoch 34, Loss: 3.913622736930847, Final Batch Loss: 1.940629243850708\n",
      "Epoch 35, Loss: 3.884237289428711, Final Batch Loss: 1.9427293539047241\n",
      "Epoch 36, Loss: 3.847641110420227, Final Batch Loss: 1.9288302659988403\n",
      "Epoch 37, Loss: 3.8482978343963623, Final Batch Loss: 1.9312125444412231\n",
      "Epoch 38, Loss: 3.7936742305755615, Final Batch Loss: 1.9065898656845093\n",
      "Epoch 39, Loss: 3.78713059425354, Final Batch Loss: 1.914779543876648\n",
      "Epoch 40, Loss: 3.72163987159729, Final Batch Loss: 1.8728063106536865\n",
      "Epoch 41, Loss: 3.671365737915039, Final Batch Loss: 1.8292182683944702\n",
      "Epoch 42, Loss: 3.662636399269104, Final Batch Loss: 1.8105599880218506\n",
      "Epoch 43, Loss: 3.643541693687439, Final Batch Loss: 1.8326307535171509\n",
      "Epoch 44, Loss: 3.582138776779175, Final Batch Loss: 1.7893682718276978\n",
      "Epoch 45, Loss: 3.5705976486206055, Final Batch Loss: 1.801405906677246\n",
      "Epoch 46, Loss: 3.5141656398773193, Final Batch Loss: 1.7574362754821777\n",
      "Epoch 47, Loss: 3.4741764068603516, Final Batch Loss: 1.7053074836730957\n",
      "Epoch 48, Loss: 3.4416279792785645, Final Batch Loss: 1.7081471681594849\n",
      "Epoch 49, Loss: 3.4247500896453857, Final Batch Loss: 1.726417064666748\n",
      "Epoch 50, Loss: 3.3661290407180786, Final Batch Loss: 1.686787486076355\n",
      "Epoch 51, Loss: 3.350219964981079, Final Batch Loss: 1.6726078987121582\n",
      "Epoch 52, Loss: 3.2500988245010376, Final Batch Loss: 1.5982404947280884\n",
      "Epoch 53, Loss: 3.1912801265716553, Final Batch Loss: 1.5771677494049072\n",
      "Epoch 54, Loss: 3.1649240255355835, Final Batch Loss: 1.5495456457138062\n",
      "Epoch 55, Loss: 3.165231943130493, Final Batch Loss: 1.5663728713989258\n",
      "Epoch 56, Loss: 3.137902855873108, Final Batch Loss: 1.5845284461975098\n",
      "Epoch 57, Loss: 3.121566414833069, Final Batch Loss: 1.5782943964004517\n",
      "Epoch 58, Loss: 3.0226701498031616, Final Batch Loss: 1.4452799558639526\n",
      "Epoch 59, Loss: 3.0100191831588745, Final Batch Loss: 1.5200761556625366\n",
      "Epoch 60, Loss: 2.991967797279358, Final Batch Loss: 1.4661651849746704\n",
      "Epoch 61, Loss: 2.918639302253723, Final Batch Loss: 1.4831863641738892\n",
      "Epoch 62, Loss: 2.919221520423889, Final Batch Loss: 1.4500929117202759\n",
      "Epoch 63, Loss: 2.893422484397888, Final Batch Loss: 1.4738249778747559\n",
      "Epoch 64, Loss: 2.9295814037323, Final Batch Loss: 1.4496084451675415\n",
      "Epoch 65, Loss: 2.7952674627304077, Final Batch Loss: 1.3782216310501099\n",
      "Epoch 66, Loss: 2.7587262392044067, Final Batch Loss: 1.3579225540161133\n",
      "Epoch 67, Loss: 2.863922119140625, Final Batch Loss: 1.4487709999084473\n",
      "Epoch 68, Loss: 2.8221116065979004, Final Batch Loss: 1.4262826442718506\n",
      "Epoch 69, Loss: 2.7187070846557617, Final Batch Loss: 1.3611541986465454\n",
      "Epoch 70, Loss: 2.7219479084014893, Final Batch Loss: 1.3090565204620361\n",
      "Epoch 71, Loss: 2.6860666275024414, Final Batch Loss: 1.3333860635757446\n",
      "Epoch 72, Loss: 2.730469584465027, Final Batch Loss: 1.408129096031189\n",
      "Epoch 73, Loss: 2.6478614807128906, Final Batch Loss: 1.3606388568878174\n",
      "Epoch 74, Loss: 2.6734068393707275, Final Batch Loss: 1.335920810699463\n",
      "Epoch 75, Loss: 2.6658235788345337, Final Batch Loss: 1.3693320751190186\n",
      "Epoch 76, Loss: 2.6095385551452637, Final Batch Loss: 1.3139978647232056\n",
      "Epoch 77, Loss: 2.6078572273254395, Final Batch Loss: 1.3364381790161133\n",
      "Epoch 78, Loss: 2.55807101726532, Final Batch Loss: 1.2632161378860474\n",
      "Epoch 79, Loss: 2.6222939491271973, Final Batch Loss: 1.2654224634170532\n",
      "Epoch 80, Loss: 2.566467523574829, Final Batch Loss: 1.3263028860092163\n",
      "Epoch 81, Loss: 2.5202975273132324, Final Batch Loss: 1.2342287302017212\n",
      "Epoch 82, Loss: 2.493026375770569, Final Batch Loss: 1.2263712882995605\n",
      "Epoch 83, Loss: 2.509113073348999, Final Batch Loss: 1.2381001710891724\n",
      "Epoch 84, Loss: 2.496490955352783, Final Batch Loss: 1.2136037349700928\n",
      "Epoch 85, Loss: 2.5264073610305786, Final Batch Loss: 1.2888011932373047\n",
      "Epoch 86, Loss: 2.5001370906829834, Final Batch Loss: 1.2359414100646973\n",
      "Epoch 87, Loss: 2.558710813522339, Final Batch Loss: 1.2206319570541382\n",
      "Epoch 88, Loss: 2.4986488819122314, Final Batch Loss: 1.2381987571716309\n",
      "Epoch 89, Loss: 2.4607601165771484, Final Batch Loss: 1.2711033821105957\n",
      "Epoch 90, Loss: 2.429614543914795, Final Batch Loss: 1.2176055908203125\n",
      "Epoch 91, Loss: 2.4562606811523438, Final Batch Loss: 1.2418910264968872\n",
      "Epoch 92, Loss: 2.420990467071533, Final Batch Loss: 1.2174956798553467\n",
      "Epoch 93, Loss: 2.4675540924072266, Final Batch Loss: 1.2343636751174927\n",
      "Epoch 94, Loss: 2.4832526445388794, Final Batch Loss: 1.2840917110443115\n",
      "Epoch 95, Loss: 2.40942120552063, Final Batch Loss: 1.1727880239486694\n",
      "Epoch 96, Loss: 2.3873562812805176, Final Batch Loss: 1.191473364830017\n",
      "Epoch 97, Loss: 2.4717601537704468, Final Batch Loss: 1.278649091720581\n",
      "Epoch 98, Loss: 2.3193706274032593, Final Batch Loss: 1.184432029724121\n",
      "Epoch 99, Loss: 2.320152521133423, Final Batch Loss: 1.1684068441390991\n",
      "Epoch 100, Loss: 2.3568732738494873, Final Batch Loss: 1.1688170433044434\n",
      "Epoch 101, Loss: 2.2883130311965942, Final Batch Loss: 1.1357718706130981\n",
      "Epoch 102, Loss: 2.347231388092041, Final Batch Loss: 1.160735011100769\n",
      "Epoch 103, Loss: 2.2755271196365356, Final Batch Loss: 1.1659274101257324\n",
      "Epoch 104, Loss: 2.2780452966690063, Final Batch Loss: 1.1196521520614624\n",
      "Epoch 105, Loss: 2.2416937351226807, Final Batch Loss: 1.1040912866592407\n",
      "Epoch 106, Loss: 2.2898001670837402, Final Batch Loss: 1.1469731330871582\n",
      "Epoch 107, Loss: 2.264464259147644, Final Batch Loss: 1.0552401542663574\n",
      "Epoch 108, Loss: 2.225891351699829, Final Batch Loss: 1.0967895984649658\n",
      "Epoch 109, Loss: 2.2403193712234497, Final Batch Loss: 1.1111204624176025\n",
      "Epoch 110, Loss: 2.2752610445022583, Final Batch Loss: 1.136582612991333\n",
      "Epoch 111, Loss: 2.1981379985809326, Final Batch Loss: 1.084725260734558\n",
      "Epoch 112, Loss: 2.2273716926574707, Final Batch Loss: 1.1277203559875488\n",
      "Epoch 113, Loss: 2.1714863777160645, Final Batch Loss: 1.0391995906829834\n",
      "Epoch 114, Loss: 2.2140473127365112, Final Batch Loss: 1.0937200784683228\n",
      "Epoch 115, Loss: 2.1394094228744507, Final Batch Loss: 1.0716005563735962\n",
      "Epoch 116, Loss: 2.197856068611145, Final Batch Loss: 1.1112128496170044\n",
      "Epoch 117, Loss: 2.2412513494491577, Final Batch Loss: 1.1597514152526855\n",
      "Epoch 118, Loss: 2.1285295486450195, Final Batch Loss: 1.0907909870147705\n",
      "Epoch 119, Loss: 2.1559101343154907, Final Batch Loss: 1.0377328395843506\n",
      "Epoch 120, Loss: 2.131793975830078, Final Batch Loss: 1.077068567276001\n",
      "Epoch 121, Loss: 2.1368802785873413, Final Batch Loss: 1.0724393129348755\n",
      "Epoch 122, Loss: 2.1223117113113403, Final Batch Loss: 1.0614943504333496\n",
      "Epoch 123, Loss: 2.0323355197906494, Final Batch Loss: 1.0585376024246216\n",
      "Epoch 124, Loss: 2.0375296473503113, Final Batch Loss: 1.0436795949935913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 2.0532474517822266, Final Batch Loss: 0.9920840263366699\n",
      "Epoch 126, Loss: 2.020070195198059, Final Batch Loss: 1.0039656162261963\n",
      "Epoch 127, Loss: 1.934305489063263, Final Batch Loss: 0.9073588252067566\n",
      "Epoch 128, Loss: 2.0967047214508057, Final Batch Loss: 1.1043118238449097\n",
      "Epoch 129, Loss: 2.0154263973236084, Final Batch Loss: 0.9715622663497925\n",
      "Epoch 130, Loss: 1.8752815127372742, Final Batch Loss: 0.8571147322654724\n",
      "Epoch 131, Loss: 1.8935051560401917, Final Batch Loss: 0.8968578577041626\n",
      "Epoch 132, Loss: 1.9150832891464233, Final Batch Loss: 0.969791829586029\n",
      "Epoch 133, Loss: 1.9160397052764893, Final Batch Loss: 0.983177661895752\n",
      "Epoch 134, Loss: 1.9315056800842285, Final Batch Loss: 0.9388752579689026\n",
      "Epoch 135, Loss: 1.907244861125946, Final Batch Loss: 1.0310388803482056\n",
      "Epoch 136, Loss: 1.9086102843284607, Final Batch Loss: 0.9589402675628662\n",
      "Epoch 137, Loss: 1.8526723384857178, Final Batch Loss: 0.9814061522483826\n",
      "Epoch 138, Loss: 1.9305467009544373, Final Batch Loss: 1.0411590337753296\n",
      "Epoch 139, Loss: 1.7566089034080505, Final Batch Loss: 0.8659502267837524\n",
      "Epoch 140, Loss: 1.7174755334854126, Final Batch Loss: 0.8055471777915955\n",
      "Epoch 141, Loss: 1.8042177557945251, Final Batch Loss: 0.9043512940406799\n",
      "Epoch 142, Loss: 1.7553492784500122, Final Batch Loss: 0.9138987064361572\n",
      "Epoch 143, Loss: 1.7118395566940308, Final Batch Loss: 0.8627602458000183\n",
      "Epoch 144, Loss: 1.740891695022583, Final Batch Loss: 0.8705382943153381\n",
      "Epoch 145, Loss: 1.7273532152175903, Final Batch Loss: 0.8766769170761108\n",
      "Epoch 146, Loss: 1.70521080493927, Final Batch Loss: 0.7889944911003113\n",
      "Epoch 147, Loss: 1.6843770742416382, Final Batch Loss: 0.8042124509811401\n",
      "Epoch 148, Loss: 1.6735035181045532, Final Batch Loss: 0.8510910272598267\n",
      "Epoch 149, Loss: 1.6614272594451904, Final Batch Loss: 0.8422869443893433\n",
      "Epoch 150, Loss: 1.6635367274284363, Final Batch Loss: 0.8144150972366333\n",
      "Epoch 151, Loss: 1.747353196144104, Final Batch Loss: 0.9691163301467896\n",
      "Epoch 152, Loss: 1.6731889843940735, Final Batch Loss: 0.8306522965431213\n",
      "Epoch 153, Loss: 1.589490294456482, Final Batch Loss: 0.8383564352989197\n",
      "Epoch 154, Loss: 1.6592909097671509, Final Batch Loss: 0.7626691460609436\n",
      "Epoch 155, Loss: 1.6386717557907104, Final Batch Loss: 0.8601693511009216\n",
      "Epoch 156, Loss: 1.5230100750923157, Final Batch Loss: 0.7817485332489014\n",
      "Epoch 157, Loss: 1.6406011581420898, Final Batch Loss: 0.8497004508972168\n",
      "Epoch 158, Loss: 1.5986741781234741, Final Batch Loss: 0.8034204840660095\n",
      "Epoch 159, Loss: 1.626589298248291, Final Batch Loss: 0.8675100207328796\n",
      "Epoch 160, Loss: 1.532065510749817, Final Batch Loss: 0.7588393688201904\n",
      "Epoch 161, Loss: 1.6626019477844238, Final Batch Loss: 0.8768436908721924\n",
      "Epoch 162, Loss: 1.529297649860382, Final Batch Loss: 0.7061191201210022\n",
      "Epoch 163, Loss: 1.5472072958946228, Final Batch Loss: 0.7659083008766174\n",
      "Epoch 164, Loss: 1.5738674402236938, Final Batch Loss: 0.8090724945068359\n",
      "Epoch 165, Loss: 1.5260211825370789, Final Batch Loss: 0.7683348059654236\n",
      "Epoch 166, Loss: 1.5657658576965332, Final Batch Loss: 0.8013796210289001\n",
      "Epoch 167, Loss: 1.592758297920227, Final Batch Loss: 0.8984983563423157\n",
      "Epoch 168, Loss: 1.5492825508117676, Final Batch Loss: 0.764667272567749\n",
      "Epoch 169, Loss: 1.4865835309028625, Final Batch Loss: 0.7216567993164062\n",
      "Epoch 170, Loss: 1.4940093755722046, Final Batch Loss: 0.7622649073600769\n",
      "Epoch 171, Loss: 1.5301163792610168, Final Batch Loss: 0.7728267908096313\n",
      "Epoch 172, Loss: 1.5313847661018372, Final Batch Loss: 0.765601396560669\n",
      "Epoch 173, Loss: 1.5164216756820679, Final Batch Loss: 0.724911093711853\n",
      "Epoch 174, Loss: 1.5511425733566284, Final Batch Loss: 0.8103232979774475\n",
      "Epoch 175, Loss: 1.5001365542411804, Final Batch Loss: 0.7510167956352234\n",
      "Epoch 176, Loss: 1.4982596635818481, Final Batch Loss: 0.7078133821487427\n",
      "Epoch 177, Loss: 1.4953963160514832, Final Batch Loss: 0.7417590618133545\n",
      "Epoch 178, Loss: 1.5030624866485596, Final Batch Loss: 0.8183218240737915\n",
      "Epoch 179, Loss: 1.505463719367981, Final Batch Loss: 0.7768608927726746\n",
      "Epoch 180, Loss: 1.4435411095619202, Final Batch Loss: 0.7424396872520447\n",
      "Epoch 181, Loss: 1.4119962453842163, Final Batch Loss: 0.7561067938804626\n",
      "Epoch 182, Loss: 1.3574705719947815, Final Batch Loss: 0.6614639759063721\n",
      "Epoch 183, Loss: 1.366976797580719, Final Batch Loss: 0.6568681597709656\n",
      "Epoch 184, Loss: 1.3804149627685547, Final Batch Loss: 0.6565420031547546\n",
      "Epoch 185, Loss: 1.3504904508590698, Final Batch Loss: 0.6454245448112488\n",
      "Epoch 186, Loss: 1.3789712190628052, Final Batch Loss: 0.6767343282699585\n",
      "Epoch 187, Loss: 1.3043721914291382, Final Batch Loss: 0.6768308877944946\n",
      "Epoch 188, Loss: 1.4161270260810852, Final Batch Loss: 0.7385355234146118\n",
      "Epoch 189, Loss: 1.3494946360588074, Final Batch Loss: 0.639534592628479\n",
      "Epoch 190, Loss: 1.3952940702438354, Final Batch Loss: 0.7341318130493164\n",
      "Epoch 191, Loss: 1.3207714557647705, Final Batch Loss: 0.6533976793289185\n",
      "Epoch 192, Loss: 1.3937662243843079, Final Batch Loss: 0.6896792054176331\n",
      "Epoch 193, Loss: 1.429499626159668, Final Batch Loss: 0.7601630091667175\n",
      "Epoch 194, Loss: 1.338994026184082, Final Batch Loss: 0.6598958373069763\n",
      "Epoch 195, Loss: 1.4009402394294739, Final Batch Loss: 0.756659984588623\n",
      "Epoch 196, Loss: 1.2811747193336487, Final Batch Loss: 0.6585615277290344\n",
      "Epoch 197, Loss: 1.3785197734832764, Final Batch Loss: 0.7499668598175049\n",
      "Epoch 198, Loss: 1.3427409529685974, Final Batch Loss: 0.6392456293106079\n",
      "Epoch 199, Loss: 1.3119990825653076, Final Batch Loss: 0.6405653953552246\n",
      "Epoch 200, Loss: 1.4343597888946533, Final Batch Loss: 0.730771005153656\n",
      "Epoch 201, Loss: 1.2229586243629456, Final Batch Loss: 0.5792688727378845\n",
      "Epoch 202, Loss: 1.3507394194602966, Final Batch Loss: 0.7141733765602112\n",
      "Epoch 203, Loss: 1.1816400289535522, Final Batch Loss: 0.5504760146141052\n",
      "Epoch 204, Loss: 1.288375437259674, Final Batch Loss: 0.6364520788192749\n",
      "Epoch 205, Loss: 1.294557273387909, Final Batch Loss: 0.6622969508171082\n",
      "Epoch 206, Loss: 1.338548183441162, Final Batch Loss: 0.742941677570343\n",
      "Epoch 207, Loss: 1.3079591989517212, Final Batch Loss: 0.6715162396430969\n",
      "Epoch 208, Loss: 1.2374196648597717, Final Batch Loss: 0.6255698800086975\n",
      "Epoch 209, Loss: 1.290940523147583, Final Batch Loss: 0.6469752788543701\n",
      "Epoch 210, Loss: 1.3193677067756653, Final Batch Loss: 0.6741599440574646\n",
      "Epoch 211, Loss: 1.3340802788734436, Final Batch Loss: 0.7196717262268066\n",
      "Epoch 212, Loss: 1.2259960770606995, Final Batch Loss: 0.5891985297203064\n",
      "Epoch 213, Loss: 1.2044516801834106, Final Batch Loss: 0.6213414072990417\n",
      "Epoch 214, Loss: 1.2785733938217163, Final Batch Loss: 0.6184245944023132\n",
      "Epoch 215, Loss: 1.2099709510803223, Final Batch Loss: 0.5548485517501831\n",
      "Epoch 216, Loss: 1.2239060997962952, Final Batch Loss: 0.5715472102165222\n",
      "Epoch 217, Loss: 1.217361867427826, Final Batch Loss: 0.6263287663459778\n",
      "Epoch 218, Loss: 1.2663986682891846, Final Batch Loss: 0.6330119371414185\n",
      "Epoch 219, Loss: 1.2347314953804016, Final Batch Loss: 0.6078540682792664\n",
      "Epoch 220, Loss: 1.1756568551063538, Final Batch Loss: 0.6093740463256836\n",
      "Epoch 221, Loss: 1.1543066501617432, Final Batch Loss: 0.5484085083007812\n",
      "Epoch 222, Loss: 1.1677694916725159, Final Batch Loss: 0.5864536166191101\n",
      "Epoch 223, Loss: 1.1499683260917664, Final Batch Loss: 0.6125282049179077\n",
      "Epoch 224, Loss: 1.2100647687911987, Final Batch Loss: 0.6080538034439087\n",
      "Epoch 225, Loss: 1.1865977048873901, Final Batch Loss: 0.6247791647911072\n",
      "Epoch 226, Loss: 1.208467185497284, Final Batch Loss: 0.6288511753082275\n",
      "Epoch 227, Loss: 1.1988375782966614, Final Batch Loss: 0.5956072807312012\n",
      "Epoch 228, Loss: 1.225210428237915, Final Batch Loss: 0.6311554312705994\n",
      "Epoch 229, Loss: 1.1874744892120361, Final Batch Loss: 0.5320789813995361\n",
      "Epoch 230, Loss: 1.110142469406128, Final Batch Loss: 0.5701904296875\n",
      "Epoch 231, Loss: 1.1346379518508911, Final Batch Loss: 0.5204836130142212\n",
      "Epoch 232, Loss: 1.219005048274994, Final Batch Loss: 0.6459633708000183\n",
      "Epoch 233, Loss: 1.182125747203827, Final Batch Loss: 0.5796669721603394\n",
      "Epoch 234, Loss: 1.0848663449287415, Final Batch Loss: 0.5945541262626648\n",
      "Epoch 235, Loss: 1.1692975163459778, Final Batch Loss: 0.5911686420440674\n",
      "Epoch 236, Loss: 1.0869494676589966, Final Batch Loss: 0.5320993661880493\n",
      "Epoch 237, Loss: 1.1187517046928406, Final Batch Loss: 0.5776498317718506\n",
      "Epoch 238, Loss: 1.24616277217865, Final Batch Loss: 0.6222058534622192\n",
      "Epoch 239, Loss: 1.2494321465492249, Final Batch Loss: 0.6373606324195862\n",
      "Epoch 240, Loss: 1.1861048936843872, Final Batch Loss: 0.6273040771484375\n",
      "Epoch 241, Loss: 1.2005115151405334, Final Batch Loss: 0.600803792476654\n",
      "Epoch 242, Loss: 1.1022471189498901, Final Batch Loss: 0.5570078492164612\n",
      "Epoch 243, Loss: 1.1003503203392029, Final Batch Loss: 0.5388282537460327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244, Loss: 1.12730073928833, Final Batch Loss: 0.5555812120437622\n",
      "Epoch 245, Loss: 1.0885522961616516, Final Batch Loss: 0.509425699710846\n",
      "Epoch 246, Loss: 1.0819045305252075, Final Batch Loss: 0.5658247470855713\n",
      "Epoch 247, Loss: 1.1220088601112366, Final Batch Loss: 0.5802668929100037\n",
      "Epoch 248, Loss: 1.0327616333961487, Final Batch Loss: 0.5115811228752136\n",
      "Epoch 249, Loss: 1.1485384106636047, Final Batch Loss: 0.5937907695770264\n",
      "Epoch 250, Loss: 1.0358011722564697, Final Batch Loss: 0.5186923146247864\n",
      "Epoch 251, Loss: 1.2267341017723083, Final Batch Loss: 0.543980598449707\n",
      "Epoch 252, Loss: 1.14228755235672, Final Batch Loss: 0.6066933870315552\n",
      "Epoch 253, Loss: 1.0621596574783325, Final Batch Loss: 0.5413356423377991\n",
      "Epoch 254, Loss: 1.1309849619865417, Final Batch Loss: 0.5884868502616882\n",
      "Epoch 255, Loss: 1.0735015869140625, Final Batch Loss: 0.4852635860443115\n",
      "Epoch 256, Loss: 1.0189102292060852, Final Batch Loss: 0.5033032894134521\n",
      "Epoch 257, Loss: 1.1265581846237183, Final Batch Loss: 0.5678369402885437\n",
      "Epoch 258, Loss: 1.1452922821044922, Final Batch Loss: 0.5867512226104736\n",
      "Epoch 259, Loss: 1.1099354028701782, Final Batch Loss: 0.5431988835334778\n",
      "Epoch 260, Loss: 1.1349308788776398, Final Batch Loss: 0.6883618831634521\n",
      "Epoch 261, Loss: 0.9934928715229034, Final Batch Loss: 0.46170297265052795\n",
      "Epoch 262, Loss: 1.0325030088424683, Final Batch Loss: 0.5080752372741699\n",
      "Epoch 263, Loss: 1.1115078926086426, Final Batch Loss: 0.5531431436538696\n",
      "Epoch 264, Loss: 1.1169154047966003, Final Batch Loss: 0.5413697957992554\n",
      "Epoch 265, Loss: 0.9982030093669891, Final Batch Loss: 0.5196362733840942\n",
      "Epoch 266, Loss: 1.03037890791893, Final Batch Loss: 0.44513604044914246\n",
      "Epoch 267, Loss: 1.0534220337867737, Final Batch Loss: 0.49939292669296265\n",
      "Epoch 268, Loss: 1.0250775814056396, Final Batch Loss: 0.48238980770111084\n",
      "Epoch 269, Loss: 1.058252990245819, Final Batch Loss: 0.5269597768783569\n",
      "Epoch 270, Loss: 1.051636964082718, Final Batch Loss: 0.4175242483615875\n",
      "Epoch 271, Loss: 1.1007354855537415, Final Batch Loss: 0.558441698551178\n",
      "Epoch 272, Loss: 1.0439351201057434, Final Batch Loss: 0.5394496321678162\n",
      "Epoch 273, Loss: 1.0320183634757996, Final Batch Loss: 0.5916035175323486\n",
      "Epoch 274, Loss: 1.0436026453971863, Final Batch Loss: 0.5326213836669922\n",
      "Epoch 275, Loss: 1.062671422958374, Final Batch Loss: 0.5140765905380249\n",
      "Epoch 276, Loss: 0.9779441058635712, Final Batch Loss: 0.4553814232349396\n",
      "Epoch 277, Loss: 1.1021503806114197, Final Batch Loss: 0.5930247902870178\n",
      "Epoch 278, Loss: 1.027617871761322, Final Batch Loss: 0.4453354477882385\n",
      "Epoch 279, Loss: 1.0584521293640137, Final Batch Loss: 0.5294616222381592\n",
      "Epoch 280, Loss: 1.0398061871528625, Final Batch Loss: 0.5389107465744019\n",
      "Epoch 281, Loss: 1.0263381898403168, Final Batch Loss: 0.4277501404285431\n",
      "Epoch 282, Loss: 1.022408902645111, Final Batch Loss: 0.5294104218482971\n",
      "Epoch 283, Loss: 1.0395544469356537, Final Batch Loss: 0.5514605641365051\n",
      "Epoch 284, Loss: 0.961080014705658, Final Batch Loss: 0.5258780717849731\n",
      "Epoch 285, Loss: 0.9996324777603149, Final Batch Loss: 0.4476000666618347\n",
      "Epoch 286, Loss: 0.9339410662651062, Final Batch Loss: 0.5047461986541748\n",
      "Epoch 287, Loss: 0.9177384972572327, Final Batch Loss: 0.42221400141716003\n",
      "Epoch 288, Loss: 1.066409558057785, Final Batch Loss: 0.5937592387199402\n",
      "Epoch 289, Loss: 1.0611849427223206, Final Batch Loss: 0.5338495969772339\n",
      "Epoch 290, Loss: 0.9526155292987823, Final Batch Loss: 0.5042690634727478\n",
      "Epoch 291, Loss: 0.9786355495452881, Final Batch Loss: 0.4867219030857086\n",
      "Epoch 292, Loss: 1.0252078175544739, Final Batch Loss: 0.504948079586029\n",
      "Epoch 293, Loss: 1.0142292380332947, Final Batch Loss: 0.5347346663475037\n",
      "Epoch 294, Loss: 0.9494310021400452, Final Batch Loss: 0.4457593560218811\n",
      "Epoch 295, Loss: 1.0074607133865356, Final Batch Loss: 0.48999983072280884\n",
      "Epoch 296, Loss: 1.0161059200763702, Final Batch Loss: 0.518742561340332\n",
      "Epoch 297, Loss: 1.047605276107788, Final Batch Loss: 0.560110867023468\n",
      "Epoch 298, Loss: 0.9495569467544556, Final Batch Loss: 0.4760299623012543\n",
      "Epoch 299, Loss: 0.8716694116592407, Final Batch Loss: 0.43493035435676575\n",
      "Epoch 300, Loss: 0.9409032166004181, Final Batch Loss: 0.4697379171848297\n",
      "Epoch 301, Loss: 1.0414767265319824, Final Batch Loss: 0.5837436318397522\n",
      "Epoch 302, Loss: 0.9408390522003174, Final Batch Loss: 0.48361942172050476\n",
      "Epoch 303, Loss: 0.9827501773834229, Final Batch Loss: 0.48165446519851685\n",
      "Epoch 304, Loss: 0.9875768423080444, Final Batch Loss: 0.4869661331176758\n",
      "Epoch 305, Loss: 0.9841963052749634, Final Batch Loss: 0.5515256524085999\n",
      "Epoch 306, Loss: 0.9668418765068054, Final Batch Loss: 0.5133242607116699\n",
      "Epoch 307, Loss: 0.8976580500602722, Final Batch Loss: 0.39434266090393066\n",
      "Epoch 308, Loss: 1.0208690166473389, Final Batch Loss: 0.5005039572715759\n",
      "Epoch 309, Loss: 0.9422400295734406, Final Batch Loss: 0.483262300491333\n",
      "Epoch 310, Loss: 0.9324641823768616, Final Batch Loss: 0.4868474006652832\n",
      "Epoch 311, Loss: 1.0156314671039581, Final Batch Loss: 0.5561834573745728\n",
      "Epoch 312, Loss: 0.9152578711509705, Final Batch Loss: 0.47068002820014954\n",
      "Epoch 313, Loss: 0.9140260517597198, Final Batch Loss: 0.4616338908672333\n",
      "Epoch 314, Loss: 0.9497524201869965, Final Batch Loss: 0.4729379713535309\n",
      "Epoch 315, Loss: 0.9956099092960358, Final Batch Loss: 0.5290769338607788\n",
      "Epoch 316, Loss: 0.9352821111679077, Final Batch Loss: 0.4523780345916748\n",
      "Epoch 317, Loss: 0.8742877840995789, Final Batch Loss: 0.4038153290748596\n",
      "Epoch 318, Loss: 1.0052868127822876, Final Batch Loss: 0.4870873689651489\n",
      "Epoch 319, Loss: 0.9206362664699554, Final Batch Loss: 0.4335612654685974\n",
      "Epoch 320, Loss: 0.9317147135734558, Final Batch Loss: 0.45473816990852356\n",
      "Epoch 321, Loss: 0.9225554466247559, Final Batch Loss: 0.47011005878448486\n",
      "Epoch 322, Loss: 0.8661299049854279, Final Batch Loss: 0.4309719204902649\n",
      "Epoch 323, Loss: 0.9306128323078156, Final Batch Loss: 0.45126742124557495\n",
      "Epoch 324, Loss: 0.9264991581439972, Final Batch Loss: 0.4619589149951935\n",
      "Epoch 325, Loss: 0.9839333891868591, Final Batch Loss: 0.526980459690094\n",
      "Epoch 326, Loss: 1.0078165233135223, Final Batch Loss: 0.5322335362434387\n",
      "Epoch 327, Loss: 0.8930994868278503, Final Batch Loss: 0.4049190282821655\n",
      "Epoch 328, Loss: 0.9445297420024872, Final Batch Loss: 0.5375522375106812\n",
      "Epoch 329, Loss: 0.953542560338974, Final Batch Loss: 0.5218303203582764\n",
      "Epoch 330, Loss: 0.943231999874115, Final Batch Loss: 0.4175598621368408\n",
      "Epoch 331, Loss: 1.0354560911655426, Final Batch Loss: 0.485458642244339\n",
      "Epoch 332, Loss: 0.9485172033309937, Final Batch Loss: 0.4682614207267761\n",
      "Epoch 333, Loss: 0.8802657425403595, Final Batch Loss: 0.4353136420249939\n",
      "Epoch 334, Loss: 0.9016875624656677, Final Batch Loss: 0.44858962297439575\n",
      "Epoch 335, Loss: 0.9430446028709412, Final Batch Loss: 0.4374345541000366\n",
      "Epoch 336, Loss: 0.9198925197124481, Final Batch Loss: 0.46117717027664185\n",
      "Epoch 337, Loss: 0.904651939868927, Final Batch Loss: 0.42303285002708435\n",
      "Epoch 338, Loss: 0.8540683090686798, Final Batch Loss: 0.3315627872943878\n",
      "Epoch 339, Loss: 0.9514844119548798, Final Batch Loss: 0.4666188955307007\n",
      "Epoch 340, Loss: 0.898922473192215, Final Batch Loss: 0.4177060127258301\n",
      "Epoch 341, Loss: 0.9687484800815582, Final Batch Loss: 0.4778481423854828\n",
      "Epoch 342, Loss: 0.9601198732852936, Final Batch Loss: 0.4635331630706787\n",
      "Epoch 343, Loss: 0.8717542290687561, Final Batch Loss: 0.4209462106227875\n",
      "Epoch 344, Loss: 0.9099449217319489, Final Batch Loss: 0.45249757170677185\n",
      "Epoch 345, Loss: 0.9038021564483643, Final Batch Loss: 0.4727672040462494\n",
      "Epoch 346, Loss: 0.9233345985412598, Final Batch Loss: 0.5158580541610718\n",
      "Epoch 347, Loss: 0.90976682305336, Final Batch Loss: 0.41974470019340515\n",
      "Epoch 348, Loss: 0.9872040450572968, Final Batch Loss: 0.48510000109672546\n",
      "Epoch 349, Loss: 0.9110553562641144, Final Batch Loss: 0.4884139597415924\n",
      "Epoch 350, Loss: 0.9030247330665588, Final Batch Loss: 0.48261353373527527\n",
      "Epoch 351, Loss: 0.9433763921260834, Final Batch Loss: 0.5146290063858032\n",
      "Epoch 352, Loss: 0.9309835135936737, Final Batch Loss: 0.4695269465446472\n",
      "Epoch 353, Loss: 0.9026249051094055, Final Batch Loss: 0.5126634836196899\n",
      "Epoch 354, Loss: 0.9255884885787964, Final Batch Loss: 0.5051424503326416\n",
      "Epoch 355, Loss: 0.8893010318279266, Final Batch Loss: 0.40544983744621277\n",
      "Epoch 356, Loss: 0.8792106509208679, Final Batch Loss: 0.4035334289073944\n",
      "Epoch 357, Loss: 0.9281328022480011, Final Batch Loss: 0.4854504466056824\n",
      "Epoch 358, Loss: 0.9100057482719421, Final Batch Loss: 0.43397873640060425\n",
      "Epoch 359, Loss: 0.8480077683925629, Final Batch Loss: 0.42649203538894653\n",
      "Epoch 360, Loss: 0.9045030772686005, Final Batch Loss: 0.39368554949760437\n",
      "Epoch 361, Loss: 0.8917687833309174, Final Batch Loss: 0.47685718536376953\n",
      "Epoch 362, Loss: 0.9034477174282074, Final Batch Loss: 0.4829739034175873\n",
      "Epoch 363, Loss: 0.9898950457572937, Final Batch Loss: 0.5242760181427002\n",
      "Epoch 364, Loss: 0.8910379707813263, Final Batch Loss: 0.4075016379356384\n",
      "Epoch 365, Loss: 0.8392207324504852, Final Batch Loss: 0.41333135962486267\n",
      "Epoch 366, Loss: 0.8742086291313171, Final Batch Loss: 0.43576309084892273\n",
      "Epoch 367, Loss: 0.8733466565608978, Final Batch Loss: 0.4289671778678894\n",
      "Epoch 368, Loss: 0.9361275434494019, Final Batch Loss: 0.4769706428050995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 369, Loss: 0.911255270242691, Final Batch Loss: 0.46673843264579773\n",
      "Epoch 370, Loss: 0.9503285884857178, Final Batch Loss: 0.5461252927780151\n",
      "Epoch 371, Loss: 0.9270475804805756, Final Batch Loss: 0.4909852147102356\n",
      "Epoch 372, Loss: 0.9125403761863708, Final Batch Loss: 0.5033171772956848\n",
      "Epoch 373, Loss: 0.8852497637271881, Final Batch Loss: 0.3642512261867523\n",
      "Epoch 374, Loss: 0.8929900825023651, Final Batch Loss: 0.4815034866333008\n",
      "Epoch 375, Loss: 0.9288673102855682, Final Batch Loss: 0.48586538434028625\n",
      "Epoch 376, Loss: 0.9032082855701447, Final Batch Loss: 0.4820229113101959\n",
      "Epoch 377, Loss: 0.8832864761352539, Final Batch Loss: 0.39674755930900574\n",
      "Epoch 378, Loss: 0.8555938899517059, Final Batch Loss: 0.45116081833839417\n",
      "Epoch 379, Loss: 0.8440591990947723, Final Batch Loss: 0.42449888586997986\n",
      "Epoch 380, Loss: 0.8295158743858337, Final Batch Loss: 0.33106571435928345\n",
      "Epoch 381, Loss: 0.9004021286964417, Final Batch Loss: 0.3983585238456726\n",
      "Epoch 382, Loss: 0.9155882596969604, Final Batch Loss: 0.4378277063369751\n",
      "Epoch 383, Loss: 0.8742362260818481, Final Batch Loss: 0.4335182309150696\n",
      "Epoch 384, Loss: 0.8550650179386139, Final Batch Loss: 0.4492076337337494\n",
      "Epoch 385, Loss: 0.8501037359237671, Final Batch Loss: 0.371228963136673\n",
      "Epoch 386, Loss: 0.812003880739212, Final Batch Loss: 0.3642065227031708\n",
      "Epoch 387, Loss: 0.9187079071998596, Final Batch Loss: 0.517450213432312\n",
      "Epoch 388, Loss: 0.8592408001422882, Final Batch Loss: 0.4301656186580658\n",
      "Epoch 389, Loss: 0.7997910380363464, Final Batch Loss: 0.3724183440208435\n",
      "Epoch 390, Loss: 0.8552675247192383, Final Batch Loss: 0.432613343000412\n",
      "Epoch 391, Loss: 0.9077793955802917, Final Batch Loss: 0.49761852622032166\n",
      "Epoch 392, Loss: 0.8930357694625854, Final Batch Loss: 0.4550858438014984\n",
      "Epoch 393, Loss: 0.8830384612083435, Final Batch Loss: 0.42934146523475647\n",
      "Epoch 394, Loss: 0.9219633638858795, Final Batch Loss: 0.5595266819000244\n",
      "Epoch 395, Loss: 0.8684444427490234, Final Batch Loss: 0.46995633840560913\n",
      "Epoch 396, Loss: 0.8498255014419556, Final Batch Loss: 0.4691985249519348\n",
      "Epoch 397, Loss: 0.8528775274753571, Final Batch Loss: 0.37224963307380676\n",
      "Epoch 398, Loss: 0.910466343164444, Final Batch Loss: 0.4481717646121979\n",
      "Epoch 399, Loss: 0.9351508319377899, Final Batch Loss: 0.5165302157402039\n",
      "Epoch 400, Loss: 0.8279358148574829, Final Batch Loss: 0.4055899679660797\n",
      "Epoch 401, Loss: 0.9273843169212341, Final Batch Loss: 0.44115421175956726\n",
      "Epoch 402, Loss: 0.8295720815658569, Final Batch Loss: 0.4407168924808502\n",
      "Epoch 403, Loss: 0.8434129953384399, Final Batch Loss: 0.4185822010040283\n",
      "Epoch 404, Loss: 0.8198102414608002, Final Batch Loss: 0.40386298298835754\n",
      "Epoch 405, Loss: 0.8406901061534882, Final Batch Loss: 0.39163678884506226\n",
      "Epoch 406, Loss: 0.8007915914058685, Final Batch Loss: 0.3904939293861389\n",
      "Epoch 407, Loss: 0.8765095472335815, Final Batch Loss: 0.4174734950065613\n",
      "Epoch 408, Loss: 0.8190611302852631, Final Batch Loss: 0.40462324023246765\n",
      "Epoch 409, Loss: 0.9494548141956329, Final Batch Loss: 0.557231605052948\n",
      "Epoch 410, Loss: 0.8450418710708618, Final Batch Loss: 0.44318950176239014\n",
      "Epoch 411, Loss: 0.8876789212226868, Final Batch Loss: 0.4805498719215393\n",
      "Epoch 412, Loss: 0.8153016567230225, Final Batch Loss: 0.4094924330711365\n",
      "Epoch 413, Loss: 0.8398711979389191, Final Batch Loss: 0.4214398264884949\n",
      "Epoch 414, Loss: 0.8299654126167297, Final Batch Loss: 0.39843183755874634\n",
      "Epoch 415, Loss: 0.8891558945178986, Final Batch Loss: 0.4618726074695587\n",
      "Epoch 416, Loss: 0.8340975046157837, Final Batch Loss: 0.43870046734809875\n",
      "Epoch 417, Loss: 0.8303830623626709, Final Batch Loss: 0.4190986156463623\n",
      "Epoch 418, Loss: 0.832085520029068, Final Batch Loss: 0.4313886761665344\n",
      "Epoch 419, Loss: 0.8451166749000549, Final Batch Loss: 0.4254084825515747\n",
      "Epoch 420, Loss: 0.747485339641571, Final Batch Loss: 0.3483160734176636\n",
      "Epoch 421, Loss: 0.87384432554245, Final Batch Loss: 0.4619753658771515\n",
      "Epoch 422, Loss: 0.9116317629814148, Final Batch Loss: 0.44599026441574097\n",
      "Epoch 423, Loss: 0.7885197401046753, Final Batch Loss: 0.34118449687957764\n",
      "Epoch 424, Loss: 0.905284583568573, Final Batch Loss: 0.4572148323059082\n",
      "Epoch 425, Loss: 0.7946760952472687, Final Batch Loss: 0.35049447417259216\n",
      "Epoch 426, Loss: 0.9071650207042694, Final Batch Loss: 0.5003443956375122\n",
      "Epoch 427, Loss: 0.8343239724636078, Final Batch Loss: 0.41448667645454407\n",
      "Epoch 428, Loss: 0.8256831169128418, Final Batch Loss: 0.43874597549438477\n",
      "Epoch 429, Loss: 0.8258392512798309, Final Batch Loss: 0.3824843764305115\n",
      "Epoch 430, Loss: 0.8243896067142487, Final Batch Loss: 0.4143790602684021\n",
      "Epoch 431, Loss: 0.8789206445217133, Final Batch Loss: 0.4550928771495819\n",
      "Epoch 432, Loss: 0.8432855606079102, Final Batch Loss: 0.4538530111312866\n",
      "Epoch 433, Loss: 0.8101931810379028, Final Batch Loss: 0.4262857437133789\n",
      "Epoch 434, Loss: 0.8033522963523865, Final Batch Loss: 0.39774224162101746\n",
      "Epoch 435, Loss: 0.9030665159225464, Final Batch Loss: 0.4513927698135376\n",
      "Epoch 436, Loss: 0.8185856342315674, Final Batch Loss: 0.4358961582183838\n",
      "Epoch 437, Loss: 0.8522534370422363, Final Batch Loss: 0.45211923122406006\n",
      "Epoch 438, Loss: 0.8641690313816071, Final Batch Loss: 0.41302207112312317\n",
      "Epoch 439, Loss: 0.8554443717002869, Final Batch Loss: 0.44113340973854065\n",
      "Epoch 440, Loss: 0.9036580920219421, Final Batch Loss: 0.4679165780544281\n",
      "Epoch 441, Loss: 0.7768049240112305, Final Batch Loss: 0.3647420406341553\n",
      "Epoch 442, Loss: 0.8440216779708862, Final Batch Loss: 0.4093712866306305\n",
      "Epoch 443, Loss: 0.8507539629936218, Final Batch Loss: 0.4229187071323395\n",
      "Epoch 444, Loss: 0.8288770914077759, Final Batch Loss: 0.3844469487667084\n",
      "Epoch 445, Loss: 0.862257331609726, Final Batch Loss: 0.44847381114959717\n",
      "Epoch 446, Loss: 0.8197444975376129, Final Batch Loss: 0.4058071970939636\n",
      "Epoch 447, Loss: 0.8385518789291382, Final Batch Loss: 0.4240964651107788\n",
      "Epoch 448, Loss: 0.893130898475647, Final Batch Loss: 0.4798523783683777\n",
      "Epoch 449, Loss: 0.820296972990036, Final Batch Loss: 0.4308847188949585\n",
      "Epoch 450, Loss: 0.8821541368961334, Final Batch Loss: 0.39983105659484863\n",
      "Epoch 451, Loss: 0.8570841252803802, Final Batch Loss: 0.48097291588783264\n",
      "Epoch 452, Loss: 0.867764949798584, Final Batch Loss: 0.41455015540122986\n",
      "Epoch 453, Loss: 0.7684330940246582, Final Batch Loss: 0.36391115188598633\n",
      "Epoch 454, Loss: 0.7874495685100555, Final Batch Loss: 0.4151391386985779\n",
      "Epoch 455, Loss: 0.81870436668396, Final Batch Loss: 0.38209268450737\n",
      "Epoch 456, Loss: 0.8519093990325928, Final Batch Loss: 0.4157286286354065\n",
      "Epoch 457, Loss: 0.8192008137702942, Final Batch Loss: 0.3959445655345917\n",
      "Epoch 458, Loss: 0.8729967474937439, Final Batch Loss: 0.4024810492992401\n",
      "Epoch 459, Loss: 0.7923079133033752, Final Batch Loss: 0.3573744297027588\n",
      "Epoch 460, Loss: 0.7953857779502869, Final Batch Loss: 0.42048394680023193\n",
      "Epoch 461, Loss: 0.8254079818725586, Final Batch Loss: 0.41668927669525146\n",
      "Epoch 462, Loss: 0.8409442603588104, Final Batch Loss: 0.42949992418289185\n",
      "Epoch 463, Loss: 0.8666125535964966, Final Batch Loss: 0.4533461034297943\n",
      "Epoch 464, Loss: 0.8557942807674408, Final Batch Loss: 0.4926260709762573\n",
      "Epoch 465, Loss: 0.7653763890266418, Final Batch Loss: 0.3407819867134094\n",
      "Epoch 466, Loss: 0.7536900341510773, Final Batch Loss: 0.35635849833488464\n",
      "Epoch 467, Loss: 0.8131651878356934, Final Batch Loss: 0.3748926818370819\n",
      "Epoch 468, Loss: 0.8068244755268097, Final Batch Loss: 0.38239097595214844\n",
      "Epoch 469, Loss: 0.8337886333465576, Final Batch Loss: 0.4195100665092468\n",
      "Epoch 470, Loss: 0.8540099263191223, Final Batch Loss: 0.42802295088768005\n",
      "Epoch 471, Loss: 0.8314982652664185, Final Batch Loss: 0.3928920030593872\n",
      "Epoch 472, Loss: 0.7564412653446198, Final Batch Loss: 0.3798624873161316\n",
      "Epoch 473, Loss: 0.7475888133049011, Final Batch Loss: 0.3510904312133789\n",
      "Epoch 474, Loss: 0.7605032324790955, Final Batch Loss: 0.3647964894771576\n",
      "Epoch 475, Loss: 0.8051449060440063, Final Batch Loss: 0.3870947062969208\n",
      "Epoch 476, Loss: 0.7632523775100708, Final Batch Loss: 0.3682935833930969\n",
      "Epoch 477, Loss: 0.8029274642467499, Final Batch Loss: 0.4245971143245697\n",
      "Epoch 478, Loss: 0.8105538785457611, Final Batch Loss: 0.4269750118255615\n",
      "Epoch 479, Loss: 0.7648373246192932, Final Batch Loss: 0.368027925491333\n",
      "Epoch 480, Loss: 0.8056703805923462, Final Batch Loss: 0.46176981925964355\n",
      "Epoch 481, Loss: 0.878300279378891, Final Batch Loss: 0.4599626064300537\n",
      "Epoch 482, Loss: 0.810271292924881, Final Batch Loss: 0.432119220495224\n",
      "Epoch 483, Loss: 0.8020448386669159, Final Batch Loss: 0.3924033045768738\n",
      "Epoch 484, Loss: 0.8728322088718414, Final Batch Loss: 0.4582449495792389\n",
      "Epoch 485, Loss: 0.7718238234519958, Final Batch Loss: 0.376464307308197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486, Loss: 0.8636487424373627, Final Batch Loss: 0.4521697759628296\n",
      "Epoch 487, Loss: 0.8343479335308075, Final Batch Loss: 0.45585906505584717\n",
      "Epoch 488, Loss: 0.8109950721263885, Final Batch Loss: 0.39529648423194885\n",
      "Epoch 489, Loss: 0.8389362692832947, Final Batch Loss: 0.3829059600830078\n",
      "Epoch 490, Loss: 0.7704326510429382, Final Batch Loss: 0.38216185569763184\n",
      "Epoch 491, Loss: 0.727622002363205, Final Batch Loss: 0.37106141448020935\n",
      "Epoch 492, Loss: 0.7441194951534271, Final Batch Loss: 0.35205936431884766\n",
      "Epoch 493, Loss: 0.8418553173542023, Final Batch Loss: 0.49109789729118347\n",
      "Epoch 494, Loss: 0.7780665457248688, Final Batch Loss: 0.3630353808403015\n",
      "Epoch 495, Loss: 0.7771190106868744, Final Batch Loss: 0.3900231719017029\n",
      "Epoch 496, Loss: 0.776375949382782, Final Batch Loss: 0.44187963008880615\n",
      "Epoch 497, Loss: 0.8295967876911163, Final Batch Loss: 0.42291295528411865\n",
      "Epoch 498, Loss: 0.7851683497428894, Final Batch Loss: 0.38400837779045105\n",
      "Epoch 499, Loss: 0.8418990671634674, Final Batch Loss: 0.416355699300766\n",
      "Epoch 500, Loss: 0.7875492870807648, Final Batch Loss: 0.43995416164398193\n",
      "Epoch 501, Loss: 0.8091672658920288, Final Batch Loss: 0.4007309675216675\n",
      "Epoch 502, Loss: 0.8123204410076141, Final Batch Loss: 0.3609089255332947\n",
      "Epoch 503, Loss: 0.794389009475708, Final Batch Loss: 0.4288983941078186\n",
      "Epoch 504, Loss: 0.7668421864509583, Final Batch Loss: 0.42774295806884766\n",
      "Epoch 505, Loss: 0.78246009349823, Final Batch Loss: 0.37147825956344604\n",
      "Epoch 506, Loss: 0.7521848678588867, Final Batch Loss: 0.3694212734699249\n",
      "Epoch 507, Loss: 0.7735892832279205, Final Batch Loss: 0.38605546951293945\n",
      "Epoch 508, Loss: 0.8521438539028168, Final Batch Loss: 0.46709877252578735\n",
      "Epoch 509, Loss: 0.7435473799705505, Final Batch Loss: 0.30987221002578735\n",
      "Epoch 510, Loss: 0.8228869736194611, Final Batch Loss: 0.427386075258255\n",
      "Epoch 511, Loss: 0.7850744724273682, Final Batch Loss: 0.3766326606273651\n",
      "Epoch 512, Loss: 0.7993509769439697, Final Batch Loss: 0.4461253583431244\n",
      "Epoch 513, Loss: 0.798217386007309, Final Batch Loss: 0.36892572045326233\n",
      "Epoch 514, Loss: 0.8098980784416199, Final Batch Loss: 0.42431265115737915\n",
      "Epoch 515, Loss: 0.7956365346908569, Final Batch Loss: 0.4029712677001953\n",
      "Epoch 516, Loss: 0.8117255866527557, Final Batch Loss: 0.411869078874588\n",
      "Epoch 517, Loss: 0.8267225623130798, Final Batch Loss: 0.37555527687072754\n",
      "Epoch 518, Loss: 0.7540969848632812, Final Batch Loss: 0.38632404804229736\n",
      "Epoch 519, Loss: 0.8261795341968536, Final Batch Loss: 0.3963795602321625\n",
      "Epoch 520, Loss: 0.8712532222270966, Final Batch Loss: 0.4212741255760193\n",
      "Epoch 521, Loss: 0.7930630743503571, Final Batch Loss: 0.46660804748535156\n",
      "Epoch 522, Loss: 0.8212900757789612, Final Batch Loss: 0.4322330057621002\n",
      "Epoch 523, Loss: 0.820111870765686, Final Batch Loss: 0.4069678783416748\n",
      "Epoch 524, Loss: 0.8220131397247314, Final Batch Loss: 0.46503713726997375\n",
      "Epoch 525, Loss: 0.8190292418003082, Final Batch Loss: 0.40162065625190735\n",
      "Epoch 526, Loss: 0.7255170941352844, Final Batch Loss: 0.3579409420490265\n",
      "Epoch 527, Loss: 0.7535533308982849, Final Batch Loss: 0.34155699610710144\n",
      "Epoch 528, Loss: 0.7842319011688232, Final Batch Loss: 0.4283798933029175\n",
      "Epoch 529, Loss: 0.7286586761474609, Final Batch Loss: 0.33128494024276733\n",
      "Epoch 530, Loss: 0.8085964024066925, Final Batch Loss: 0.4279753863811493\n",
      "Epoch 531, Loss: 0.7627397179603577, Final Batch Loss: 0.3606977164745331\n",
      "Epoch 532, Loss: 0.7916093468666077, Final Batch Loss: 0.39883795380592346\n",
      "Epoch 533, Loss: 0.777361273765564, Final Batch Loss: 0.41065457463264465\n",
      "Epoch 534, Loss: 0.7932361364364624, Final Batch Loss: 0.4155290424823761\n",
      "Epoch 535, Loss: 0.8844732642173767, Final Batch Loss: 0.5072757601737976\n",
      "Epoch 536, Loss: 0.7870041728019714, Final Batch Loss: 0.36307886242866516\n",
      "Epoch 537, Loss: 0.8005653619766235, Final Batch Loss: 0.3563222289085388\n",
      "Epoch 538, Loss: 0.8239892423152924, Final Batch Loss: 0.4402565062046051\n",
      "Epoch 539, Loss: 0.7767052948474884, Final Batch Loss: 0.36671924591064453\n",
      "Epoch 540, Loss: 0.8422240614891052, Final Batch Loss: 0.4172721803188324\n",
      "Epoch 541, Loss: 0.7782617509365082, Final Batch Loss: 0.3763279318809509\n",
      "Epoch 542, Loss: 0.8083581328392029, Final Batch Loss: 0.3810778856277466\n",
      "Epoch 543, Loss: 0.8242838978767395, Final Batch Loss: 0.45353949069976807\n",
      "Epoch 544, Loss: 0.7859439849853516, Final Batch Loss: 0.40611782670021057\n",
      "Epoch 545, Loss: 0.731401801109314, Final Batch Loss: 0.39122119545936584\n",
      "Epoch 546, Loss: 0.8157932758331299, Final Batch Loss: 0.4503794014453888\n",
      "Epoch 547, Loss: 0.7368530333042145, Final Batch Loss: 0.3787475526332855\n",
      "Epoch 548, Loss: 0.6926939487457275, Final Batch Loss: 0.33430546522140503\n",
      "Epoch 549, Loss: 0.7592204213142395, Final Batch Loss: 0.32821452617645264\n",
      "Epoch 550, Loss: 0.7601424157619476, Final Batch Loss: 0.3299034833908081\n",
      "Epoch 551, Loss: 0.8489323258399963, Final Batch Loss: 0.4468037486076355\n",
      "Epoch 552, Loss: 0.7477835416793823, Final Batch Loss: 0.34146520495414734\n",
      "Epoch 553, Loss: 0.717460423707962, Final Batch Loss: 0.3756750822067261\n",
      "Epoch 554, Loss: 0.8431207239627838, Final Batch Loss: 0.411502867937088\n",
      "Epoch 555, Loss: 0.7625800967216492, Final Batch Loss: 0.3486576974391937\n",
      "Epoch 556, Loss: 0.7658965885639191, Final Batch Loss: 0.3599072992801666\n",
      "Epoch 557, Loss: 0.8590488433837891, Final Batch Loss: 0.5016982555389404\n",
      "Epoch 558, Loss: 0.7473363280296326, Final Batch Loss: 0.35487452149391174\n",
      "Epoch 559, Loss: 0.7548856437206268, Final Batch Loss: 0.3584915101528168\n",
      "Epoch 560, Loss: 0.8570493161678314, Final Batch Loss: 0.49040377140045166\n",
      "Epoch 561, Loss: 0.75506392121315, Final Batch Loss: 0.42088475823402405\n",
      "Epoch 562, Loss: 0.7841129899024963, Final Batch Loss: 0.40096813440322876\n",
      "Epoch 563, Loss: 0.7317163646221161, Final Batch Loss: 0.358980655670166\n",
      "Epoch 564, Loss: 0.7431805431842804, Final Batch Loss: 0.34771719574928284\n",
      "Epoch 565, Loss: 0.7974864542484283, Final Batch Loss: 0.40266910195350647\n",
      "Epoch 566, Loss: 0.6956423223018646, Final Batch Loss: 0.3409213125705719\n",
      "Epoch 567, Loss: 0.898493230342865, Final Batch Loss: 0.502356231212616\n",
      "Epoch 568, Loss: 0.7264963090419769, Final Batch Loss: 0.37910589575767517\n",
      "Epoch 569, Loss: 0.7129855751991272, Final Batch Loss: 0.3208271861076355\n",
      "Epoch 570, Loss: 0.7498603463172913, Final Batch Loss: 0.3723711371421814\n",
      "Epoch 571, Loss: 0.7662537097930908, Final Batch Loss: 0.37483271956443787\n",
      "Epoch 572, Loss: 0.7823764681816101, Final Batch Loss: 0.32468926906585693\n",
      "Epoch 573, Loss: 0.747090220451355, Final Batch Loss: 0.38672882318496704\n",
      "Epoch 574, Loss: 0.7401392161846161, Final Batch Loss: 0.3968694508075714\n",
      "Epoch 575, Loss: 0.747707188129425, Final Batch Loss: 0.38020095229148865\n",
      "Epoch 576, Loss: 0.6876251399517059, Final Batch Loss: 0.336475670337677\n",
      "Epoch 577, Loss: 0.7636169493198395, Final Batch Loss: 0.3902745246887207\n",
      "Epoch 578, Loss: 0.701252818107605, Final Batch Loss: 0.28533342480659485\n",
      "Epoch 579, Loss: 0.7935513854026794, Final Batch Loss: 0.3827718198299408\n",
      "Epoch 580, Loss: 0.8231308460235596, Final Batch Loss: 0.4121432900428772\n",
      "Epoch 581, Loss: 0.8280076384544373, Final Batch Loss: 0.4669567942619324\n",
      "Epoch 582, Loss: 0.7885302305221558, Final Batch Loss: 0.2742229104042053\n",
      "Epoch 583, Loss: 0.7752642035484314, Final Batch Loss: 0.3912127912044525\n",
      "Epoch 584, Loss: 0.7516887784004211, Final Batch Loss: 0.38796326518058777\n",
      "Epoch 585, Loss: 0.7717565298080444, Final Batch Loss: 0.3816843330860138\n",
      "Epoch 586, Loss: 0.7958466708660126, Final Batch Loss: 0.415396124124527\n",
      "Epoch 587, Loss: 0.7615146040916443, Final Batch Loss: 0.3738127052783966\n",
      "Epoch 588, Loss: 0.7417658269405365, Final Batch Loss: 0.3517006039619446\n",
      "Epoch 589, Loss: 0.771061897277832, Final Batch Loss: 0.3883843421936035\n",
      "Epoch 590, Loss: 0.7783442735671997, Final Batch Loss: 0.3860586881637573\n",
      "Epoch 591, Loss: 0.7405814826488495, Final Batch Loss: 0.3073608875274658\n",
      "Epoch 592, Loss: 0.7665233016014099, Final Batch Loss: 0.391671746969223\n",
      "Epoch 593, Loss: 0.7507317662239075, Final Batch Loss: 0.39723122119903564\n",
      "Epoch 594, Loss: 0.8026255667209625, Final Batch Loss: 0.45774218440055847\n",
      "Epoch 595, Loss: 0.8133252263069153, Final Batch Loss: 0.4880295693874359\n",
      "Epoch 596, Loss: 0.7636915743350983, Final Batch Loss: 0.42234891653060913\n",
      "Epoch 597, Loss: 0.7139694392681122, Final Batch Loss: 0.35932520031929016\n",
      "Epoch 598, Loss: 0.7518963813781738, Final Batch Loss: 0.3895127773284912\n",
      "Epoch 599, Loss: 0.7095658779144287, Final Batch Loss: 0.3154348134994507\n",
      "Epoch 600, Loss: 0.7668658792972565, Final Batch Loss: 0.38397616147994995\n",
      "Epoch 601, Loss: 0.7577865123748779, Final Batch Loss: 0.37332576513290405\n",
      "Epoch 602, Loss: 0.7518941164016724, Final Batch Loss: 0.4316045939922333\n",
      "Epoch 603, Loss: 0.7729953229427338, Final Batch Loss: 0.3531653881072998\n",
      "Epoch 604, Loss: 0.7615666687488556, Final Batch Loss: 0.39150381088256836\n",
      "Epoch 605, Loss: 0.7742168307304382, Final Batch Loss: 0.39615586400032043\n",
      "Epoch 606, Loss: 0.8314275443553925, Final Batch Loss: 0.4834012985229492\n",
      "Epoch 607, Loss: 0.72312131524086, Final Batch Loss: 0.3391801416873932\n",
      "Epoch 608, Loss: 0.800971120595932, Final Batch Loss: 0.44386452436447144\n",
      "Epoch 609, Loss: 0.6981233060359955, Final Batch Loss: 0.2950069010257721\n",
      "Epoch 610, Loss: 0.7385939359664917, Final Batch Loss: 0.3637852668762207\n",
      "Epoch 611, Loss: 0.8474639654159546, Final Batch Loss: 0.490278035402298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 612, Loss: 0.7388001084327698, Final Batch Loss: 0.3806711435317993\n",
      "Epoch 613, Loss: 0.8531128764152527, Final Batch Loss: 0.470645010471344\n",
      "Epoch 614, Loss: 0.7433187067508698, Final Batch Loss: 0.37526392936706543\n",
      "Epoch 615, Loss: 0.8045766651630402, Final Batch Loss: 0.44633617997169495\n",
      "Epoch 616, Loss: 0.7288953363895416, Final Batch Loss: 0.3371173143386841\n",
      "Epoch 617, Loss: 0.7287055552005768, Final Batch Loss: 0.3714139759540558\n",
      "Epoch 618, Loss: 0.8248583674430847, Final Batch Loss: 0.43836989998817444\n",
      "Epoch 619, Loss: 0.6888741850852966, Final Batch Loss: 0.3328301012516022\n",
      "Epoch 620, Loss: 0.7783516943454742, Final Batch Loss: 0.3894549310207367\n",
      "Epoch 621, Loss: 0.7239126265048981, Final Batch Loss: 0.41241613030433655\n",
      "Epoch 622, Loss: 0.6993787288665771, Final Batch Loss: 0.32948169112205505\n",
      "Epoch 623, Loss: 0.7834537029266357, Final Batch Loss: 0.3298822045326233\n",
      "Epoch 624, Loss: 0.7381303608417511, Final Batch Loss: 0.4032812714576721\n",
      "Epoch 625, Loss: 0.7217798233032227, Final Batch Loss: 0.3646256625652313\n",
      "Epoch 626, Loss: 0.7002751529216766, Final Batch Loss: 0.31062087416648865\n",
      "Epoch 627, Loss: 0.7189431190490723, Final Batch Loss: 0.36233168840408325\n",
      "Epoch 628, Loss: 0.6827305555343628, Final Batch Loss: 0.3488864302635193\n",
      "Epoch 629, Loss: 0.7553427219390869, Final Batch Loss: 0.3820115923881531\n",
      "Epoch 630, Loss: 0.7608333826065063, Final Batch Loss: 0.3954228460788727\n",
      "Epoch 631, Loss: 0.8568565845489502, Final Batch Loss: 0.49194449186325073\n",
      "Epoch 632, Loss: 0.7272834479808807, Final Batch Loss: 0.3460105061531067\n",
      "Epoch 633, Loss: 0.7438821196556091, Final Batch Loss: 0.4159933924674988\n",
      "Epoch 634, Loss: 0.792577475309372, Final Batch Loss: 0.4194299280643463\n",
      "Epoch 635, Loss: 0.7811079025268555, Final Batch Loss: 0.39406636357307434\n",
      "Epoch 636, Loss: 0.7285438776016235, Final Batch Loss: 0.39339300990104675\n",
      "Epoch 637, Loss: 0.7529151737689972, Final Batch Loss: 0.38337597250938416\n",
      "Epoch 638, Loss: 0.8057160675525665, Final Batch Loss: 0.3994203507900238\n",
      "Epoch 639, Loss: 0.8149205446243286, Final Batch Loss: 0.40270358324050903\n",
      "Epoch 640, Loss: 0.688871443271637, Final Batch Loss: 0.31853988766670227\n",
      "Epoch 641, Loss: 0.7720844745635986, Final Batch Loss: 0.4285071790218353\n",
      "Epoch 642, Loss: 0.803679347038269, Final Batch Loss: 0.4641319811344147\n",
      "Epoch 643, Loss: 0.753198117017746, Final Batch Loss: 0.39733409881591797\n",
      "Epoch 644, Loss: 0.7493484020233154, Final Batch Loss: 0.4009901285171509\n",
      "Epoch 645, Loss: 0.7287502288818359, Final Batch Loss: 0.35563740134239197\n",
      "Epoch 646, Loss: 0.7463117837905884, Final Batch Loss: 0.4034804403781891\n",
      "Epoch 647, Loss: 0.7496656775474548, Final Batch Loss: 0.416227787733078\n",
      "Epoch 648, Loss: 0.741064190864563, Final Batch Loss: 0.4069512188434601\n",
      "Epoch 649, Loss: 0.7352583706378937, Final Batch Loss: 0.4447799026966095\n",
      "Epoch 650, Loss: 0.7222926914691925, Final Batch Loss: 0.3434095084667206\n",
      "Epoch 651, Loss: 0.7049458026885986, Final Batch Loss: 0.3438727855682373\n",
      "Epoch 652, Loss: 0.7270227372646332, Final Batch Loss: 0.39926356077194214\n",
      "Epoch 653, Loss: 0.7704006135463715, Final Batch Loss: 0.4239037036895752\n",
      "Epoch 654, Loss: 0.7747092247009277, Final Batch Loss: 0.3745367228984833\n",
      "Epoch 655, Loss: 0.7561792135238647, Final Batch Loss: 0.40287166833877563\n",
      "Epoch 656, Loss: 0.7849815785884857, Final Batch Loss: 0.4091137945652008\n",
      "Epoch 657, Loss: 0.8048849105834961, Final Batch Loss: 0.3801024258136749\n",
      "Epoch 658, Loss: 0.7098324298858643, Final Batch Loss: 0.4257192313671112\n",
      "Epoch 659, Loss: 0.7503622770309448, Final Batch Loss: 0.3693574070930481\n",
      "Epoch 660, Loss: 0.7627151012420654, Final Batch Loss: 0.38825279474258423\n",
      "Epoch 661, Loss: 0.7042758464813232, Final Batch Loss: 0.36226513981819153\n",
      "Epoch 662, Loss: 0.7871092259883881, Final Batch Loss: 0.46333152055740356\n",
      "Epoch 663, Loss: 0.7555128633975983, Final Batch Loss: 0.32886043190956116\n",
      "Epoch 664, Loss: 0.7632123827934265, Final Batch Loss: 0.38800668716430664\n",
      "Epoch 665, Loss: 0.7125257253646851, Final Batch Loss: 0.38293004035949707\n",
      "Epoch 666, Loss: 0.7347419559955597, Final Batch Loss: 0.3920121490955353\n",
      "Epoch 667, Loss: 0.7559971809387207, Final Batch Loss: 0.3529894948005676\n",
      "Epoch 668, Loss: 0.7616626024246216, Final Batch Loss: 0.44498729705810547\n",
      "Epoch 669, Loss: 0.7500274777412415, Final Batch Loss: 0.39396315813064575\n",
      "Epoch 670, Loss: 0.7551523447036743, Final Batch Loss: 0.4352646768093109\n",
      "Epoch 671, Loss: 0.7175662815570831, Final Batch Loss: 0.3425435423851013\n",
      "Epoch 672, Loss: 0.7058085799217224, Final Batch Loss: 0.3511010408401489\n",
      "Epoch 673, Loss: 0.7077430784702301, Final Batch Loss: 0.37093430757522583\n",
      "Epoch 674, Loss: 0.7592670321464539, Final Batch Loss: 0.39375731348991394\n",
      "Epoch 675, Loss: 0.7153677642345428, Final Batch Loss: 0.3023254871368408\n",
      "Epoch 676, Loss: 0.7182935476303101, Final Batch Loss: 0.33872973918914795\n",
      "Epoch 677, Loss: 0.7283205091953278, Final Batch Loss: 0.3735521137714386\n",
      "Epoch 678, Loss: 0.6929051578044891, Final Batch Loss: 0.29944348335266113\n",
      "Epoch 679, Loss: 0.7593305110931396, Final Batch Loss: 0.4634653329849243\n",
      "Epoch 680, Loss: 0.6865755319595337, Final Batch Loss: 0.3969862461090088\n",
      "Epoch 681, Loss: 0.7243844270706177, Final Batch Loss: 0.38126692175865173\n",
      "Epoch 682, Loss: 0.7254162430763245, Final Batch Loss: 0.3501490652561188\n",
      "Epoch 683, Loss: 0.7075648903846741, Final Batch Loss: 0.3395947217941284\n",
      "Epoch 684, Loss: 0.7499755918979645, Final Batch Loss: 0.46625280380249023\n",
      "Epoch 685, Loss: 0.7378813326358795, Final Batch Loss: 0.33194699883461\n",
      "Epoch 686, Loss: 0.6797161996364594, Final Batch Loss: 0.28335466980934143\n",
      "Epoch 687, Loss: 0.6442016363143921, Final Batch Loss: 0.310965895652771\n",
      "Epoch 688, Loss: 0.672797679901123, Final Batch Loss: 0.3182713985443115\n",
      "Epoch 689, Loss: 0.8217847347259521, Final Batch Loss: 0.4172917604446411\n",
      "Epoch 690, Loss: 0.7593411207199097, Final Batch Loss: 0.4246683716773987\n",
      "Epoch 691, Loss: 0.6880157291889191, Final Batch Loss: 0.2986755073070526\n",
      "Epoch 692, Loss: 0.6959425508975983, Final Batch Loss: 0.3362880051136017\n",
      "Epoch 693, Loss: 0.7581039667129517, Final Batch Loss: 0.40729913115501404\n",
      "Epoch 694, Loss: 0.6661046147346497, Final Batch Loss: 0.2976105213165283\n",
      "Epoch 695, Loss: 0.6879990994930267, Final Batch Loss: 0.34698936343193054\n",
      "Epoch 696, Loss: 0.7193666696548462, Final Batch Loss: 0.3765183389186859\n",
      "Epoch 697, Loss: 0.7651297152042389, Final Batch Loss: 0.40577980875968933\n",
      "Epoch 698, Loss: 0.7621656060218811, Final Batch Loss: 0.3796537220478058\n",
      "Epoch 699, Loss: 0.726631373167038, Final Batch Loss: 0.3446328639984131\n",
      "Epoch 700, Loss: 0.7321320176124573, Final Batch Loss: 0.32276928424835205\n",
      "Epoch 701, Loss: 0.7289510071277618, Final Batch Loss: 0.3517589569091797\n",
      "Epoch 702, Loss: 0.7345942854881287, Final Batch Loss: 0.38127753138542175\n",
      "Epoch 703, Loss: 0.7224145531654358, Final Batch Loss: 0.382320761680603\n",
      "Epoch 704, Loss: 0.7373605668544769, Final Batch Loss: 0.40331757068634033\n",
      "Epoch 705, Loss: 0.7872726619243622, Final Batch Loss: 0.40182462334632874\n",
      "Epoch 706, Loss: 0.6779791116714478, Final Batch Loss: 0.3190874755382538\n",
      "Epoch 707, Loss: 0.6920484602451324, Final Batch Loss: 0.3298979103565216\n",
      "Epoch 708, Loss: 0.6741713583469391, Final Batch Loss: 0.30514442920684814\n",
      "Epoch 709, Loss: 0.7056877017021179, Final Batch Loss: 0.39648136496543884\n",
      "Epoch 710, Loss: 0.8001461625099182, Final Batch Loss: 0.44931843876838684\n",
      "Epoch 711, Loss: 0.6862271428108215, Final Batch Loss: 0.35386204719543457\n",
      "Epoch 712, Loss: 0.6687286198139191, Final Batch Loss: 0.3410564363002777\n",
      "Epoch 713, Loss: 0.7662326097488403, Final Batch Loss: 0.36183029413223267\n",
      "Epoch 714, Loss: 0.7213088572025299, Final Batch Loss: 0.303469717502594\n",
      "Epoch 715, Loss: 0.7013241946697235, Final Batch Loss: 0.35226163268089294\n",
      "Epoch 716, Loss: 0.7852094173431396, Final Batch Loss: 0.4111471474170685\n",
      "Epoch 717, Loss: 0.7203070223331451, Final Batch Loss: 0.424791157245636\n",
      "Epoch 718, Loss: 0.7260017395019531, Final Batch Loss: 0.37827643752098083\n",
      "Epoch 719, Loss: 0.7025941908359528, Final Batch Loss: 0.33403849601745605\n",
      "Epoch 720, Loss: 0.7221289575099945, Final Batch Loss: 0.3412654399871826\n",
      "Epoch 721, Loss: 0.691301017999649, Final Batch Loss: 0.3517625629901886\n",
      "Epoch 722, Loss: 0.6865666806697845, Final Batch Loss: 0.323577344417572\n",
      "Epoch 723, Loss: 0.6908734738826752, Final Batch Loss: 0.3502914309501648\n",
      "Epoch 724, Loss: 0.7291758954524994, Final Batch Loss: 0.37627971172332764\n",
      "Epoch 725, Loss: 0.6519359350204468, Final Batch Loss: 0.307451069355011\n",
      "Epoch 726, Loss: 0.6894994974136353, Final Batch Loss: 0.2899348735809326\n",
      "Epoch 727, Loss: 0.7594631612300873, Final Batch Loss: 0.40076741576194763\n",
      "Epoch 728, Loss: 0.6955953538417816, Final Batch Loss: 0.32487812638282776\n",
      "Epoch 729, Loss: 0.7357463538646698, Final Batch Loss: 0.3803635537624359\n",
      "Epoch 730, Loss: 0.732731819152832, Final Batch Loss: 0.3940679132938385\n",
      "Epoch 731, Loss: 0.7249589562416077, Final Batch Loss: 0.3577018678188324\n",
      "Epoch 732, Loss: 0.7022563219070435, Final Batch Loss: 0.3112441599369049\n",
      "Epoch 733, Loss: 0.6926614046096802, Final Batch Loss: 0.3277694582939148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 734, Loss: 0.7402344942092896, Final Batch Loss: 0.35182619094848633\n",
      "Epoch 735, Loss: 0.7411428391933441, Final Batch Loss: 0.3310668468475342\n",
      "Epoch 736, Loss: 0.6822068691253662, Final Batch Loss: 0.33305925130844116\n",
      "Epoch 737, Loss: 0.7373572587966919, Final Batch Loss: 0.4105183184146881\n",
      "Epoch 738, Loss: 0.7104690372943878, Final Batch Loss: 0.2958134114742279\n",
      "Epoch 739, Loss: 0.7297219038009644, Final Batch Loss: 0.3405463397502899\n",
      "Epoch 740, Loss: 0.6882911324501038, Final Batch Loss: 0.3011975586414337\n",
      "Epoch 741, Loss: 0.6701237261295319, Final Batch Loss: 0.3361404240131378\n",
      "Epoch 742, Loss: 0.7284726202487946, Final Batch Loss: 0.37170642614364624\n",
      "Epoch 743, Loss: 0.6748412251472473, Final Batch Loss: 0.31319695711135864\n",
      "Epoch 744, Loss: 0.7529401481151581, Final Batch Loss: 0.3593892455101013\n",
      "Epoch 745, Loss: 0.7561075985431671, Final Batch Loss: 0.32261034846305847\n",
      "Epoch 746, Loss: 0.7381057143211365, Final Batch Loss: 0.4231759309768677\n",
      "Epoch 747, Loss: 0.6974693536758423, Final Batch Loss: 0.39667239785194397\n",
      "Epoch 748, Loss: 0.7134036123752594, Final Batch Loss: 0.3578457534313202\n",
      "Epoch 749, Loss: 0.7360784411430359, Final Batch Loss: 0.4106738865375519\n",
      "Epoch 750, Loss: 0.7184090316295624, Final Batch Loss: 0.3073706030845642\n",
      "Epoch 751, Loss: 0.7622389495372772, Final Batch Loss: 0.3514951765537262\n",
      "Epoch 752, Loss: 0.7175473570823669, Final Batch Loss: 0.33051949739456177\n",
      "Epoch 753, Loss: 0.7155502438545227, Final Batch Loss: 0.3666665256023407\n",
      "Epoch 754, Loss: 0.7373531460762024, Final Batch Loss: 0.3870905935764313\n",
      "Epoch 755, Loss: 0.684470534324646, Final Batch Loss: 0.3022605776786804\n",
      "Epoch 756, Loss: 0.6782059073448181, Final Batch Loss: 0.3210645020008087\n",
      "Epoch 757, Loss: 0.6371033787727356, Final Batch Loss: 0.29822656512260437\n",
      "Epoch 758, Loss: 0.7227833867073059, Final Batch Loss: 0.40212398767471313\n",
      "Epoch 759, Loss: 0.7075500190258026, Final Batch Loss: 0.3343609869480133\n",
      "Epoch 760, Loss: 0.6920993030071259, Final Batch Loss: 0.3768291473388672\n",
      "Epoch 761, Loss: 0.7725148499011993, Final Batch Loss: 0.4110158681869507\n",
      "Epoch 762, Loss: 0.7584794759750366, Final Batch Loss: 0.42836010456085205\n",
      "Epoch 763, Loss: 0.7202604711055756, Final Batch Loss: 0.3524147570133209\n",
      "Epoch 764, Loss: 0.6842503845691681, Final Batch Loss: 0.3825584650039673\n",
      "Epoch 765, Loss: 0.698110044002533, Final Batch Loss: 0.3726454973220825\n",
      "Epoch 766, Loss: 0.7024480998516083, Final Batch Loss: 0.36057084798812866\n",
      "Epoch 767, Loss: 0.7626321613788605, Final Batch Loss: 0.431101530790329\n",
      "Epoch 768, Loss: 0.6646614968776703, Final Batch Loss: 0.3480570912361145\n",
      "Epoch 769, Loss: 0.7102483212947845, Final Batch Loss: 0.3777732253074646\n",
      "Epoch 770, Loss: 0.6748687624931335, Final Batch Loss: 0.3588789999485016\n",
      "Epoch 771, Loss: 0.7257083654403687, Final Batch Loss: 0.40293070673942566\n",
      "Epoch 772, Loss: 0.6338787078857422, Final Batch Loss: 0.2935962677001953\n",
      "Epoch 773, Loss: 0.7807801067829132, Final Batch Loss: 0.4432315528392792\n",
      "Epoch 774, Loss: 0.7348864078521729, Final Batch Loss: 0.40675824880599976\n",
      "Epoch 775, Loss: 0.6819059550762177, Final Batch Loss: 0.33320268988609314\n",
      "Epoch 776, Loss: 0.7145176231861115, Final Batch Loss: 0.344047486782074\n",
      "Epoch 777, Loss: 0.6984553337097168, Final Batch Loss: 0.34627923369407654\n",
      "Epoch 778, Loss: 0.6715725064277649, Final Batch Loss: 0.33785736560821533\n",
      "Epoch 779, Loss: 0.6740192472934723, Final Batch Loss: 0.36947596073150635\n",
      "Epoch 780, Loss: 0.718593418598175, Final Batch Loss: 0.39664483070373535\n",
      "Epoch 781, Loss: 0.6709253787994385, Final Batch Loss: 0.3135716915130615\n",
      "Epoch 782, Loss: 0.7373808324337006, Final Batch Loss: 0.3531642556190491\n",
      "Epoch 783, Loss: 0.7640283405780792, Final Batch Loss: 0.43664005398750305\n",
      "Epoch 784, Loss: 0.728128582239151, Final Batch Loss: 0.3604150116443634\n",
      "Epoch 785, Loss: 0.719090074300766, Final Batch Loss: 0.36948102712631226\n",
      "Epoch 786, Loss: 0.7455496490001678, Final Batch Loss: 0.41451191902160645\n",
      "Epoch 787, Loss: 0.7152740955352783, Final Batch Loss: 0.32574212551116943\n",
      "Epoch 788, Loss: 0.705550342798233, Final Batch Loss: 0.34667423367500305\n",
      "Epoch 789, Loss: 0.6760563850402832, Final Batch Loss: 0.3435076177120209\n",
      "Epoch 790, Loss: 0.6839070320129395, Final Batch Loss: 0.352016419172287\n",
      "Epoch 791, Loss: 0.7433474063873291, Final Batch Loss: 0.39939427375793457\n",
      "Epoch 792, Loss: 0.6469670832157135, Final Batch Loss: 0.308630108833313\n",
      "Epoch 793, Loss: 0.7394961714744568, Final Batch Loss: 0.369473397731781\n",
      "Epoch 794, Loss: 0.7126717269420624, Final Batch Loss: 0.3793960511684418\n",
      "Epoch 795, Loss: 0.6631625890731812, Final Batch Loss: 0.34246206283569336\n",
      "Epoch 796, Loss: 0.7261953949928284, Final Batch Loss: 0.31112220883369446\n",
      "Epoch 797, Loss: 0.669817715883255, Final Batch Loss: 0.31877872347831726\n",
      "Epoch 798, Loss: 0.7281241118907928, Final Batch Loss: 0.39416012167930603\n",
      "Epoch 799, Loss: 0.6810555160045624, Final Batch Loss: 0.3667060434818268\n",
      "Epoch 800, Loss: 0.6937964558601379, Final Batch Loss: 0.3344562351703644\n",
      "Epoch 801, Loss: 0.6881862282752991, Final Batch Loss: 0.33856555819511414\n",
      "Epoch 802, Loss: 0.7621756494045258, Final Batch Loss: 0.402118980884552\n",
      "Epoch 803, Loss: 0.7729018330574036, Final Batch Loss: 0.45228374004364014\n",
      "Epoch 804, Loss: 0.7323473691940308, Final Batch Loss: 0.3566150963306427\n",
      "Epoch 805, Loss: 0.7049332857131958, Final Batch Loss: 0.3516975939273834\n",
      "Epoch 806, Loss: 0.6937040388584137, Final Batch Loss: 0.31561601161956787\n",
      "Epoch 807, Loss: 0.7158377766609192, Final Batch Loss: 0.30749836564064026\n",
      "Epoch 808, Loss: 0.6734621822834015, Final Batch Loss: 0.30527377128601074\n",
      "Epoch 809, Loss: 0.6970110833644867, Final Batch Loss: 0.3237120509147644\n",
      "Epoch 810, Loss: 0.6850586533546448, Final Batch Loss: 0.3714839816093445\n",
      "Epoch 811, Loss: 0.7074714303016663, Final Batch Loss: 0.3485093414783478\n",
      "Epoch 812, Loss: 0.6897847056388855, Final Batch Loss: 0.3280709385871887\n",
      "Epoch 813, Loss: 0.6818821430206299, Final Batch Loss: 0.29002270102500916\n",
      "Epoch 814, Loss: 0.6900086104869843, Final Batch Loss: 0.3307323157787323\n",
      "Epoch 815, Loss: 0.6372767686843872, Final Batch Loss: 0.29350051283836365\n",
      "Epoch 816, Loss: 0.7120507657527924, Final Batch Loss: 0.36078205704689026\n",
      "Epoch 817, Loss: 0.6751893758773804, Final Batch Loss: 0.3546938896179199\n",
      "Epoch 818, Loss: 0.7321136295795441, Final Batch Loss: 0.32267460227012634\n",
      "Epoch 819, Loss: 0.6712817251682281, Final Batch Loss: 0.332166463136673\n",
      "Epoch 820, Loss: 0.6886046528816223, Final Batch Loss: 0.3709663152694702\n",
      "Epoch 821, Loss: 0.6383149027824402, Final Batch Loss: 0.32168129086494446\n",
      "Epoch 822, Loss: 0.6783429682254791, Final Batch Loss: 0.3439510762691498\n",
      "Epoch 823, Loss: 0.689969539642334, Final Batch Loss: 0.3298434019088745\n",
      "Epoch 824, Loss: 0.7111673951148987, Final Batch Loss: 0.34360358119010925\n",
      "Epoch 825, Loss: 0.6923956573009491, Final Batch Loss: 0.396914541721344\n",
      "Epoch 826, Loss: 0.6808181703090668, Final Batch Loss: 0.2879962921142578\n",
      "Epoch 827, Loss: 0.6572510898113251, Final Batch Loss: 0.33881956338882446\n",
      "Epoch 828, Loss: 0.6601869463920593, Final Batch Loss: 0.32915860414505005\n",
      "Epoch 829, Loss: 0.6822441220283508, Final Batch Loss: 0.32239487767219543\n",
      "Epoch 830, Loss: 0.6977939903736115, Final Batch Loss: 0.36225366592407227\n",
      "Epoch 831, Loss: 0.6573192179203033, Final Batch Loss: 0.3747267723083496\n",
      "Epoch 832, Loss: 0.6727344393730164, Final Batch Loss: 0.31586599349975586\n",
      "Epoch 833, Loss: 0.6433342695236206, Final Batch Loss: 0.31930962204933167\n",
      "Epoch 834, Loss: 0.6910436749458313, Final Batch Loss: 0.34231072664260864\n",
      "Epoch 835, Loss: 0.6976475119590759, Final Batch Loss: 0.37055617570877075\n",
      "Epoch 836, Loss: 0.6593181788921356, Final Batch Loss: 0.3430261015892029\n",
      "Epoch 837, Loss: 0.6799325346946716, Final Batch Loss: 0.30792251229286194\n",
      "Epoch 838, Loss: 0.6620601713657379, Final Batch Loss: 0.3537321090698242\n",
      "Epoch 839, Loss: 0.6389943957328796, Final Batch Loss: 0.2796919047832489\n",
      "Epoch 840, Loss: 0.6835501194000244, Final Batch Loss: 0.3002651631832123\n",
      "Epoch 841, Loss: 0.6893430352210999, Final Batch Loss: 0.3094475269317627\n",
      "Epoch 842, Loss: 0.614349901676178, Final Batch Loss: 0.2517351806163788\n",
      "Epoch 843, Loss: 0.6839919984340668, Final Batch Loss: 0.3527149260044098\n",
      "Epoch 844, Loss: 0.6380096971988678, Final Batch Loss: 0.3429301381111145\n",
      "Epoch 845, Loss: 0.7290451228618622, Final Batch Loss: 0.3916361927986145\n",
      "Epoch 846, Loss: 0.703812450170517, Final Batch Loss: 0.3998604416847229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 847, Loss: 0.6555833220481873, Final Batch Loss: 0.2926190197467804\n",
      "Epoch 848, Loss: 0.7009904384613037, Final Batch Loss: 0.32576942443847656\n",
      "Epoch 849, Loss: 0.6704086065292358, Final Batch Loss: 0.3185577392578125\n",
      "Epoch 850, Loss: 0.6337036192417145, Final Batch Loss: 0.3174779713153839\n",
      "Epoch 851, Loss: 0.6775229871273041, Final Batch Loss: 0.35064423084259033\n",
      "Epoch 852, Loss: 0.6437520086765289, Final Batch Loss: 0.3001191318035126\n",
      "Epoch 853, Loss: 0.644602507352829, Final Batch Loss: 0.3094402551651001\n",
      "Epoch 854, Loss: 0.6793707311153412, Final Batch Loss: 0.3405386507511139\n",
      "Epoch 855, Loss: 0.710184782743454, Final Batch Loss: 0.3448275327682495\n",
      "Epoch 856, Loss: 0.7428337037563324, Final Batch Loss: 0.3967684209346771\n",
      "Epoch 857, Loss: 0.7113038301467896, Final Batch Loss: 0.3354337215423584\n",
      "Epoch 858, Loss: 0.7394604086875916, Final Batch Loss: 0.37254461646080017\n",
      "Epoch 859, Loss: 0.602488100528717, Final Batch Loss: 0.32251501083374023\n",
      "Epoch 860, Loss: 0.6596297919750214, Final Batch Loss: 0.3462715446949005\n",
      "Epoch 861, Loss: 0.6839452087879181, Final Batch Loss: 0.3494008481502533\n",
      "Epoch 862, Loss: 0.6687572300434113, Final Batch Loss: 0.32722708582878113\n",
      "Epoch 863, Loss: 0.6486351191997528, Final Batch Loss: 0.3347123861312866\n",
      "Epoch 864, Loss: 0.6135294735431671, Final Batch Loss: 0.26958656311035156\n",
      "Epoch 865, Loss: 0.6659186780452728, Final Batch Loss: 0.34857335686683655\n",
      "Epoch 866, Loss: 0.7131437659263611, Final Batch Loss: 0.38093358278274536\n",
      "Epoch 867, Loss: 0.6270013153553009, Final Batch Loss: 0.28891876339912415\n",
      "Epoch 868, Loss: 0.7491832673549652, Final Batch Loss: 0.4184662699699402\n",
      "Epoch 869, Loss: 0.6663578152656555, Final Batch Loss: 0.32272830605506897\n",
      "Epoch 870, Loss: 0.7010007202625275, Final Batch Loss: 0.3517841100692749\n",
      "Epoch 871, Loss: 0.636009693145752, Final Batch Loss: 0.3250281512737274\n",
      "Epoch 872, Loss: 0.6027607917785645, Final Batch Loss: 0.30320680141448975\n",
      "Epoch 873, Loss: 0.6150070428848267, Final Batch Loss: 0.26547694206237793\n",
      "Epoch 874, Loss: 0.6202968060970306, Final Batch Loss: 0.2705940902233124\n",
      "Epoch 875, Loss: 0.718029797077179, Final Batch Loss: 0.3981279134750366\n",
      "Epoch 876, Loss: 0.6465837061405182, Final Batch Loss: 0.31619617342948914\n",
      "Epoch 877, Loss: 0.6237392723560333, Final Batch Loss: 0.29342350363731384\n",
      "Epoch 878, Loss: 0.6407406032085419, Final Batch Loss: 0.2711140811443329\n",
      "Epoch 879, Loss: 0.6890340745449066, Final Batch Loss: 0.3113342225551605\n",
      "Epoch 880, Loss: 0.6463679373264313, Final Batch Loss: 0.3268323242664337\n",
      "Epoch 881, Loss: 0.6428353786468506, Final Batch Loss: 0.34764155745506287\n",
      "Epoch 882, Loss: 0.688698798418045, Final Batch Loss: 0.36235377192497253\n",
      "Epoch 883, Loss: 0.6348582208156586, Final Batch Loss: 0.33872494101524353\n",
      "Epoch 884, Loss: 0.6626766622066498, Final Batch Loss: 0.3375280499458313\n",
      "Epoch 885, Loss: 0.6657579839229584, Final Batch Loss: 0.3152715861797333\n",
      "Epoch 886, Loss: 0.6231953799724579, Final Batch Loss: 0.27076172828674316\n",
      "Epoch 887, Loss: 0.6638023555278778, Final Batch Loss: 0.34675076603889465\n",
      "Epoch 888, Loss: 0.6453114151954651, Final Batch Loss: 0.3026643395423889\n",
      "Epoch 889, Loss: 0.6899473965167999, Final Batch Loss: 0.3457310199737549\n",
      "Epoch 890, Loss: 0.6480473279953003, Final Batch Loss: 0.35465219616889954\n",
      "Epoch 891, Loss: 0.6307113766670227, Final Batch Loss: 0.283657044172287\n",
      "Epoch 892, Loss: 0.7154284715652466, Final Batch Loss: 0.33127570152282715\n",
      "Epoch 893, Loss: 0.6790664196014404, Final Batch Loss: 0.33022770285606384\n",
      "Epoch 894, Loss: 0.6718966066837311, Final Batch Loss: 0.3391972482204437\n",
      "Epoch 895, Loss: 0.6532940566539764, Final Batch Loss: 0.3850368857383728\n",
      "Epoch 896, Loss: 0.6150241792201996, Final Batch Loss: 0.28404632210731506\n",
      "Epoch 897, Loss: 0.6446983814239502, Final Batch Loss: 0.2835191488265991\n",
      "Epoch 898, Loss: 0.6175109446048737, Final Batch Loss: 0.27354714274406433\n",
      "Epoch 899, Loss: 0.7434249520301819, Final Batch Loss: 0.3248615264892578\n",
      "Epoch 900, Loss: 0.7266006767749786, Final Batch Loss: 0.40347597002983093\n",
      "Epoch 901, Loss: 0.6536233127117157, Final Batch Loss: 0.3462151885032654\n",
      "Epoch 902, Loss: 0.6800591051578522, Final Batch Loss: 0.3954804837703705\n",
      "Epoch 903, Loss: 0.6254924833774567, Final Batch Loss: 0.32425910234451294\n",
      "Epoch 904, Loss: 0.7408912181854248, Final Batch Loss: 0.37918925285339355\n",
      "Epoch 905, Loss: 0.6041211187839508, Final Batch Loss: 0.3301650583744049\n",
      "Epoch 906, Loss: 0.6618939936161041, Final Batch Loss: 0.30198001861572266\n",
      "Epoch 907, Loss: 0.625244677066803, Final Batch Loss: 0.30586495995521545\n",
      "Epoch 908, Loss: 0.5829838067293167, Final Batch Loss: 0.21456818282604218\n",
      "Epoch 909, Loss: 0.6334418952465057, Final Batch Loss: 0.33349907398223877\n",
      "Epoch 910, Loss: 0.6284872591495514, Final Batch Loss: 0.3408823013305664\n",
      "Epoch 911, Loss: 0.7649500668048859, Final Batch Loss: 0.40179574489593506\n",
      "Epoch 912, Loss: 0.7076344788074493, Final Batch Loss: 0.3822683095932007\n",
      "Epoch 913, Loss: 0.6471245586872101, Final Batch Loss: 0.29297998547554016\n",
      "Epoch 914, Loss: 0.7043362855911255, Final Batch Loss: 0.3694969117641449\n",
      "Epoch 915, Loss: 0.5881681442260742, Final Batch Loss: 0.24672222137451172\n",
      "Epoch 916, Loss: 0.630174309015274, Final Batch Loss: 0.3239401876926422\n",
      "Epoch 917, Loss: 0.6737327575683594, Final Batch Loss: 0.3228667080402374\n",
      "Epoch 918, Loss: 0.6162424087524414, Final Batch Loss: 0.31705164909362793\n",
      "Epoch 919, Loss: 0.6183347404003143, Final Batch Loss: 0.34993913769721985\n",
      "Epoch 920, Loss: 0.6690123081207275, Final Batch Loss: 0.30663198232650757\n",
      "Epoch 921, Loss: 0.6141784489154816, Final Batch Loss: 0.307026207447052\n",
      "Epoch 922, Loss: 0.6855356991291046, Final Batch Loss: 0.3741986155509949\n",
      "Epoch 923, Loss: 0.7438703179359436, Final Batch Loss: 0.35751283168792725\n",
      "Epoch 924, Loss: 0.604799747467041, Final Batch Loss: 0.26102879643440247\n",
      "Epoch 925, Loss: 0.6490536034107208, Final Batch Loss: 0.3399102985858917\n",
      "Epoch 926, Loss: 0.7433676719665527, Final Batch Loss: 0.40334561467170715\n",
      "Epoch 927, Loss: 0.5845797955989838, Final Batch Loss: 0.2254934310913086\n",
      "Epoch 928, Loss: 0.6036686301231384, Final Batch Loss: 0.27423664927482605\n",
      "Epoch 929, Loss: 0.6114532351493835, Final Batch Loss: 0.292938232421875\n",
      "Epoch 930, Loss: 0.6400465965270996, Final Batch Loss: 0.33943989872932434\n",
      "Epoch 931, Loss: 0.6246274709701538, Final Batch Loss: 0.323788583278656\n",
      "Epoch 932, Loss: 0.580491840839386, Final Batch Loss: 0.2465606927871704\n",
      "Epoch 933, Loss: 0.7092102468013763, Final Batch Loss: 0.3694498836994171\n",
      "Epoch 934, Loss: 0.6732860505580902, Final Batch Loss: 0.32333043217658997\n",
      "Epoch 935, Loss: 0.57064089179039, Final Batch Loss: 0.3011011779308319\n",
      "Epoch 936, Loss: 0.6345875263214111, Final Batch Loss: 0.2827490270137787\n",
      "Epoch 937, Loss: 0.6003461182117462, Final Batch Loss: 0.29832205176353455\n",
      "Epoch 938, Loss: 0.6490255892276764, Final Batch Loss: 0.35503697395324707\n",
      "Epoch 939, Loss: 0.6799275875091553, Final Batch Loss: 0.3065636157989502\n",
      "Epoch 940, Loss: 0.7497429847717285, Final Batch Loss: 0.3491462469100952\n",
      "Epoch 941, Loss: 0.6507441103458405, Final Batch Loss: 0.32694149017333984\n",
      "Epoch 942, Loss: 0.6594001054763794, Final Batch Loss: 0.3282705247402191\n",
      "Epoch 943, Loss: 0.6523368060588837, Final Batch Loss: 0.2985991835594177\n",
      "Epoch 944, Loss: 0.7255997657775879, Final Batch Loss: 0.42697396874427795\n",
      "Epoch 945, Loss: 0.7123619019985199, Final Batch Loss: 0.392790287733078\n",
      "Epoch 946, Loss: 0.6780523359775543, Final Batch Loss: 0.3141016662120819\n",
      "Epoch 947, Loss: 0.6495265066623688, Final Batch Loss: 0.3036493957042694\n",
      "Epoch 948, Loss: 0.6100008487701416, Final Batch Loss: 0.26903244853019714\n",
      "Epoch 949, Loss: 0.6037431061267853, Final Batch Loss: 0.27233877778053284\n",
      "Epoch 950, Loss: 0.6424417793750763, Final Batch Loss: 0.3114747405052185\n",
      "Epoch 951, Loss: 0.6233786344528198, Final Batch Loss: 0.3354852795600891\n",
      "Epoch 952, Loss: 0.6068470776081085, Final Batch Loss: 0.2644954323768616\n",
      "Epoch 953, Loss: 0.6779629290103912, Final Batch Loss: 0.39517953991889954\n",
      "Epoch 954, Loss: 0.6180174350738525, Final Batch Loss: 0.28809458017349243\n",
      "Epoch 955, Loss: 0.6084717512130737, Final Batch Loss: 0.2822175621986389\n",
      "Epoch 956, Loss: 0.6878700852394104, Final Batch Loss: 0.35770145058631897\n",
      "Epoch 957, Loss: 0.6230661571025848, Final Batch Loss: 0.33338698744773865\n",
      "Epoch 958, Loss: 0.6323018670082092, Final Batch Loss: 0.3135961890220642\n",
      "Epoch 959, Loss: 0.6447251439094543, Final Batch Loss: 0.2870660424232483\n",
      "Epoch 960, Loss: 0.6240625977516174, Final Batch Loss: 0.30776524543762207\n",
      "Epoch 961, Loss: 0.6798195242881775, Final Batch Loss: 0.3418544828891754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 962, Loss: 0.6042178571224213, Final Batch Loss: 0.29804283380508423\n",
      "Epoch 963, Loss: 0.582259327173233, Final Batch Loss: 0.29253584146499634\n",
      "Epoch 964, Loss: 0.5687118470668793, Final Batch Loss: 0.2933015823364258\n",
      "Epoch 965, Loss: 0.629827618598938, Final Batch Loss: 0.3148079812526703\n",
      "Epoch 966, Loss: 0.6501573920249939, Final Batch Loss: 0.3187862038612366\n",
      "Epoch 967, Loss: 0.6492637991905212, Final Batch Loss: 0.35088348388671875\n",
      "Epoch 968, Loss: 0.589074969291687, Final Batch Loss: 0.3015265166759491\n",
      "Epoch 969, Loss: 0.6449180245399475, Final Batch Loss: 0.34144869446754456\n",
      "Epoch 970, Loss: 0.6337379515171051, Final Batch Loss: 0.2971338629722595\n",
      "Epoch 971, Loss: 0.6205388307571411, Final Batch Loss: 0.2947293221950531\n",
      "Epoch 972, Loss: 0.6181294322013855, Final Batch Loss: 0.32470592856407166\n",
      "Epoch 973, Loss: 0.6695027947425842, Final Batch Loss: 0.37417659163475037\n",
      "Epoch 974, Loss: 0.6282570064067841, Final Batch Loss: 0.2706161439418793\n",
      "Epoch 975, Loss: 0.5865755081176758, Final Batch Loss: 0.31575703620910645\n",
      "Epoch 976, Loss: 0.6666229963302612, Final Batch Loss: 0.38943958282470703\n",
      "Epoch 977, Loss: 0.6491712331771851, Final Batch Loss: 0.33688223361968994\n",
      "Epoch 978, Loss: 0.6558928489685059, Final Batch Loss: 0.3121591806411743\n",
      "Epoch 979, Loss: 0.5536278039216995, Final Batch Loss: 0.24906544387340546\n",
      "Epoch 980, Loss: 0.6023904979228973, Final Batch Loss: 0.31406205892562866\n",
      "Epoch 981, Loss: 0.6420759558677673, Final Batch Loss: 0.3305599093437195\n",
      "Epoch 982, Loss: 0.6460761427879333, Final Batch Loss: 0.35168567299842834\n",
      "Epoch 983, Loss: 0.5867183655500412, Final Batch Loss: 0.23332099616527557\n",
      "Epoch 984, Loss: 0.5866174697875977, Final Batch Loss: 0.2748769223690033\n",
      "Epoch 985, Loss: 0.620555967092514, Final Batch Loss: 0.2564760446548462\n",
      "Epoch 986, Loss: 0.6432160437107086, Final Batch Loss: 0.29948124289512634\n",
      "Epoch 987, Loss: 0.6477763056755066, Final Batch Loss: 0.3793533444404602\n",
      "Epoch 988, Loss: 0.5827438533306122, Final Batch Loss: 0.28456342220306396\n",
      "Epoch 989, Loss: 0.5933041870594025, Final Batch Loss: 0.3501720428466797\n",
      "Epoch 990, Loss: 0.6340612471103668, Final Batch Loss: 0.3139546811580658\n",
      "Epoch 991, Loss: 0.66728475689888, Final Batch Loss: 0.3971889913082123\n",
      "Epoch 992, Loss: 0.5741767585277557, Final Batch Loss: 0.3122704029083252\n",
      "Epoch 993, Loss: 0.5652875304222107, Final Batch Loss: 0.25434088706970215\n",
      "Epoch 994, Loss: 0.6311706900596619, Final Batch Loss: 0.3444928824901581\n",
      "Epoch 995, Loss: 0.6013549864292145, Final Batch Loss: 0.3253537118434906\n",
      "Epoch 996, Loss: 0.6300361156463623, Final Batch Loss: 0.3373675048351288\n",
      "Epoch 997, Loss: 0.5427020192146301, Final Batch Loss: 0.2613956928253174\n",
      "Epoch 998, Loss: 0.6352635025978088, Final Batch Loss: 0.3167952299118042\n",
      "Epoch 999, Loss: 0.5273554921150208, Final Batch Loss: 0.2023569643497467\n",
      "Epoch 1000, Loss: 0.5924616456031799, Final Batch Loss: 0.29775717854499817\n",
      "Epoch 1001, Loss: 0.5451916754245758, Final Batch Loss: 0.22062784433364868\n",
      "Epoch 1002, Loss: 0.6023796796798706, Final Batch Loss: 0.2942813038825989\n",
      "Epoch 1003, Loss: 0.6126952767372131, Final Batch Loss: 0.2936707139015198\n",
      "Epoch 1004, Loss: 0.5938388705253601, Final Batch Loss: 0.29563915729522705\n",
      "Epoch 1005, Loss: 0.6946104466915131, Final Batch Loss: 0.3259009122848511\n",
      "Epoch 1006, Loss: 0.7318103611469269, Final Batch Loss: 0.4462131857872009\n",
      "Epoch 1007, Loss: 0.6187937259674072, Final Batch Loss: 0.2996712625026703\n",
      "Epoch 1008, Loss: 0.6107370853424072, Final Batch Loss: 0.32033273577690125\n",
      "Epoch 1009, Loss: 0.628451019525528, Final Batch Loss: 0.37119805812835693\n",
      "Epoch 1010, Loss: 0.5504339933395386, Final Batch Loss: 0.21278312802314758\n",
      "Epoch 1011, Loss: 0.6582868993282318, Final Batch Loss: 0.36068791151046753\n",
      "Epoch 1012, Loss: 0.5599064826965332, Final Batch Loss: 0.26023101806640625\n",
      "Epoch 1013, Loss: 0.5803044140338898, Final Batch Loss: 0.29878488183021545\n",
      "Epoch 1014, Loss: 0.6137138307094574, Final Batch Loss: 0.3359454572200775\n",
      "Epoch 1015, Loss: 0.614248126745224, Final Batch Loss: 0.29278531670570374\n",
      "Epoch 1016, Loss: 0.6626867353916168, Final Batch Loss: 0.3576219379901886\n",
      "Epoch 1017, Loss: 0.5809278190135956, Final Batch Loss: 0.291820228099823\n",
      "Epoch 1018, Loss: 0.5982444584369659, Final Batch Loss: 0.31675276160240173\n",
      "Epoch 1019, Loss: 0.6151244938373566, Final Batch Loss: 0.32513555884361267\n",
      "Epoch 1020, Loss: 0.664099782705307, Final Batch Loss: 0.32533931732177734\n",
      "Epoch 1021, Loss: 0.5719083845615387, Final Batch Loss: 0.2920902669429779\n",
      "Epoch 1022, Loss: 0.6075096130371094, Final Batch Loss: 0.3094468414783478\n",
      "Epoch 1023, Loss: 0.6276749968528748, Final Batch Loss: 0.31634852290153503\n",
      "Epoch 1024, Loss: 0.5532150566577911, Final Batch Loss: 0.2532596290111542\n",
      "Epoch 1025, Loss: 0.6154833734035492, Final Batch Loss: 0.34328562021255493\n",
      "Epoch 1026, Loss: 0.530491054058075, Final Batch Loss: 0.2592889070510864\n",
      "Epoch 1027, Loss: 0.5715321600437164, Final Batch Loss: 0.3050984740257263\n",
      "Epoch 1028, Loss: 0.5811381042003632, Final Batch Loss: 0.2985566258430481\n",
      "Epoch 1029, Loss: 0.673363208770752, Final Batch Loss: 0.3909425735473633\n",
      "Epoch 1030, Loss: 0.6162087619304657, Final Batch Loss: 0.3566201627254486\n",
      "Epoch 1031, Loss: 0.5763963907957077, Final Batch Loss: 0.2383417934179306\n",
      "Epoch 1032, Loss: 0.5698414146900177, Final Batch Loss: 0.25110292434692383\n",
      "Epoch 1033, Loss: 0.6037083864212036, Final Batch Loss: 0.32567062973976135\n",
      "Epoch 1034, Loss: 0.5720046758651733, Final Batch Loss: 0.28733527660369873\n",
      "Epoch 1035, Loss: 0.581999272108078, Final Batch Loss: 0.28590402007102966\n",
      "Epoch 1036, Loss: 0.5596959590911865, Final Batch Loss: 0.28591856360435486\n",
      "Epoch 1037, Loss: 0.5742672979831696, Final Batch Loss: 0.2290155291557312\n",
      "Epoch 1038, Loss: 0.5721059441566467, Final Batch Loss: 0.2874704897403717\n",
      "Epoch 1039, Loss: 0.5902222990989685, Final Batch Loss: 0.31805509328842163\n",
      "Epoch 1040, Loss: 0.5562838762998581, Final Batch Loss: 0.24169711768627167\n",
      "Epoch 1041, Loss: 0.5746156871318817, Final Batch Loss: 0.31141653656959534\n",
      "Epoch 1042, Loss: 0.5796706676483154, Final Batch Loss: 0.27695271372795105\n",
      "Epoch 1043, Loss: 0.622800886631012, Final Batch Loss: 0.34318602085113525\n",
      "Epoch 1044, Loss: 0.5504166781902313, Final Batch Loss: 0.2675975561141968\n",
      "Epoch 1045, Loss: 0.5777856409549713, Final Batch Loss: 0.26557818055152893\n",
      "Epoch 1046, Loss: 0.5430812239646912, Final Batch Loss: 0.2792721390724182\n",
      "Epoch 1047, Loss: 0.6145533621311188, Final Batch Loss: 0.2977624833583832\n",
      "Epoch 1048, Loss: 0.5588095784187317, Final Batch Loss: 0.2673642933368683\n",
      "Epoch 1049, Loss: 0.5689161419868469, Final Batch Loss: 0.27194932103157043\n",
      "Epoch 1050, Loss: 0.611232340335846, Final Batch Loss: 0.3043826222419739\n",
      "Epoch 1051, Loss: 0.6465553939342499, Final Batch Loss: 0.36154818534851074\n",
      "Epoch 1052, Loss: 0.6142627596855164, Final Batch Loss: 0.3117294907569885\n",
      "Epoch 1053, Loss: 0.6108847558498383, Final Batch Loss: 0.35220199823379517\n",
      "Epoch 1054, Loss: 0.5504075586795807, Final Batch Loss: 0.28923308849334717\n",
      "Epoch 1055, Loss: 0.6229195594787598, Final Batch Loss: 0.3553072214126587\n",
      "Epoch 1056, Loss: 0.6185498833656311, Final Batch Loss: 0.29238227009773254\n",
      "Epoch 1057, Loss: 0.6610987186431885, Final Batch Loss: 0.3572804629802704\n",
      "Epoch 1058, Loss: 0.5964809954166412, Final Batch Loss: 0.26446282863616943\n",
      "Epoch 1059, Loss: 0.5134239792823792, Final Batch Loss: 0.24918082356452942\n",
      "Epoch 1060, Loss: 0.663811981678009, Final Batch Loss: 0.34667474031448364\n",
      "Epoch 1061, Loss: 0.581620454788208, Final Batch Loss: 0.29455769062042236\n",
      "Epoch 1062, Loss: 0.5609842538833618, Final Batch Loss: 0.27941805124282837\n",
      "Epoch 1063, Loss: 0.5138590484857559, Final Batch Loss: 0.22711734473705292\n",
      "Epoch 1064, Loss: 0.5771828293800354, Final Batch Loss: 0.27562814950942993\n",
      "Epoch 1065, Loss: 0.5808863937854767, Final Batch Loss: 0.2886272966861725\n",
      "Epoch 1066, Loss: 0.5666523277759552, Final Batch Loss: 0.30221235752105713\n",
      "Epoch 1067, Loss: 0.5596180558204651, Final Batch Loss: 0.24241596460342407\n",
      "Epoch 1068, Loss: 0.5294803977012634, Final Batch Loss: 0.25250956416130066\n",
      "Epoch 1069, Loss: 0.6025932431221008, Final Batch Loss: 0.3675864338874817\n",
      "Epoch 1070, Loss: 0.5761550068855286, Final Batch Loss: 0.31746000051498413\n",
      "Epoch 1071, Loss: 0.5210923850536346, Final Batch Loss: 0.25479623675346375\n",
      "Epoch 1072, Loss: 0.571753203868866, Final Batch Loss: 0.281008243560791\n",
      "Epoch 1073, Loss: 0.5676845163106918, Final Batch Loss: 0.3316778242588043\n",
      "Epoch 1074, Loss: 0.4976643770933151, Final Batch Loss: 0.24508215487003326\n",
      "Epoch 1075, Loss: 0.5774981379508972, Final Batch Loss: 0.2802526652812958\n",
      "Epoch 1076, Loss: 0.5505562722682953, Final Batch Loss: 0.2929021418094635\n",
      "Epoch 1077, Loss: 0.535623162984848, Final Batch Loss: 0.24352383613586426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1078, Loss: 0.5769913792610168, Final Batch Loss: 0.22896459698677063\n",
      "Epoch 1079, Loss: 0.4975152909755707, Final Batch Loss: 0.23697435855865479\n",
      "Epoch 1080, Loss: 0.5115790367126465, Final Batch Loss: 0.23443347215652466\n",
      "Epoch 1081, Loss: 0.5707210302352905, Final Batch Loss: 0.28639376163482666\n",
      "Epoch 1082, Loss: 0.5992015302181244, Final Batch Loss: 0.31307414174079895\n",
      "Epoch 1083, Loss: 0.588234156370163, Final Batch Loss: 0.35549482703208923\n",
      "Epoch 1084, Loss: 0.5575561076402664, Final Batch Loss: 0.31859222054481506\n",
      "Epoch 1085, Loss: 0.6079256236553192, Final Batch Loss: 0.3142208755016327\n",
      "Epoch 1086, Loss: 0.6148209273815155, Final Batch Loss: 0.3330635130405426\n",
      "Epoch 1087, Loss: 0.542559340596199, Final Batch Loss: 0.2387654036283493\n",
      "Epoch 1088, Loss: 0.6142610311508179, Final Batch Loss: 0.3399181365966797\n",
      "Epoch 1089, Loss: 0.5380213856697083, Final Batch Loss: 0.27488040924072266\n",
      "Epoch 1090, Loss: 0.5479540228843689, Final Batch Loss: 0.2826659679412842\n",
      "Epoch 1091, Loss: 0.5272587984800339, Final Batch Loss: 0.24023811519145966\n",
      "Epoch 1092, Loss: 0.6229116022586823, Final Batch Loss: 0.2726227045059204\n",
      "Epoch 1093, Loss: 0.5708426088094711, Final Batch Loss: 0.33558976650238037\n",
      "Epoch 1094, Loss: 0.5299922227859497, Final Batch Loss: 0.25456711649894714\n",
      "Epoch 1095, Loss: 0.5441609919071198, Final Batch Loss: 0.2697792053222656\n",
      "Epoch 1096, Loss: 0.5636698305606842, Final Batch Loss: 0.29269111156463623\n",
      "Epoch 1097, Loss: 0.5568127930164337, Final Batch Loss: 0.256820946931839\n",
      "Epoch 1098, Loss: 0.4886152744293213, Final Batch Loss: 0.22615793347358704\n",
      "Epoch 1099, Loss: 0.5913122743368149, Final Batch Loss: 0.3561669588088989\n",
      "Epoch 1100, Loss: 0.5310636758804321, Final Batch Loss: 0.2317761480808258\n",
      "Epoch 1101, Loss: 0.5054749250411987, Final Batch Loss: 0.2714068293571472\n",
      "Epoch 1102, Loss: 0.5714830458164215, Final Batch Loss: 0.2967584431171417\n",
      "Epoch 1103, Loss: 0.5549625754356384, Final Batch Loss: 0.2747519314289093\n",
      "Epoch 1104, Loss: 0.5337673723697662, Final Batch Loss: 0.26140424609184265\n",
      "Epoch 1105, Loss: 0.5814972817897797, Final Batch Loss: 0.25571879744529724\n",
      "Epoch 1106, Loss: 0.586456298828125, Final Batch Loss: 0.3300473392009735\n",
      "Epoch 1107, Loss: 0.5522975027561188, Final Batch Loss: 0.2782757878303528\n",
      "Epoch 1108, Loss: 0.5666482150554657, Final Batch Loss: 0.3163527548313141\n",
      "Epoch 1109, Loss: 0.5489549040794373, Final Batch Loss: 0.2643264830112457\n",
      "Epoch 1110, Loss: 0.5200907588005066, Final Batch Loss: 0.24286431074142456\n",
      "Epoch 1111, Loss: 0.5441093742847443, Final Batch Loss: 0.2721240818500519\n",
      "Epoch 1112, Loss: 0.5534177124500275, Final Batch Loss: 0.25100457668304443\n",
      "Epoch 1113, Loss: 0.599215179681778, Final Batch Loss: 0.3160359263420105\n",
      "Epoch 1114, Loss: 0.5202385932207108, Final Batch Loss: 0.2331467717885971\n",
      "Epoch 1115, Loss: 0.5003835409879684, Final Batch Loss: 0.27446702122688293\n",
      "Epoch 1116, Loss: 0.6332173347473145, Final Batch Loss: 0.41495680809020996\n",
      "Epoch 1117, Loss: 0.5213861763477325, Final Batch Loss: 0.255702942609787\n",
      "Epoch 1118, Loss: 0.6187803745269775, Final Batch Loss: 0.28422898054122925\n",
      "Epoch 1119, Loss: 0.5232708603143692, Final Batch Loss: 0.22675015032291412\n",
      "Epoch 1120, Loss: 0.5397311449050903, Final Batch Loss: 0.2585676610469818\n",
      "Epoch 1121, Loss: 0.6042320728302002, Final Batch Loss: 0.33253219723701477\n",
      "Epoch 1122, Loss: 0.5485148280858994, Final Batch Loss: 0.3145468831062317\n",
      "Epoch 1123, Loss: 0.48567377030849457, Final Batch Loss: 0.24284160137176514\n",
      "Epoch 1124, Loss: 0.5615057647228241, Final Batch Loss: 0.2504351735115051\n",
      "Epoch 1125, Loss: 0.5879900902509689, Final Batch Loss: 0.3414708375930786\n",
      "Epoch 1126, Loss: 0.5411490201950073, Final Batch Loss: 0.2880510985851288\n",
      "Epoch 1127, Loss: 0.520498663187027, Final Batch Loss: 0.2326260507106781\n",
      "Epoch 1128, Loss: 0.5868915915489197, Final Batch Loss: 0.3104563355445862\n",
      "Epoch 1129, Loss: 0.5268153697252274, Final Batch Loss: 0.24935536086559296\n",
      "Epoch 1130, Loss: 0.4716929495334625, Final Batch Loss: 0.2001897692680359\n",
      "Epoch 1131, Loss: 0.499557301402092, Final Batch Loss: 0.2221955507993698\n",
      "Epoch 1132, Loss: 0.5304237604141235, Final Batch Loss: 0.2681991457939148\n",
      "Epoch 1133, Loss: 0.5224729031324387, Final Batch Loss: 0.23834635317325592\n",
      "Epoch 1134, Loss: 0.53940449655056, Final Batch Loss: 0.23731963336467743\n",
      "Epoch 1135, Loss: 0.5493943691253662, Final Batch Loss: 0.2959955036640167\n",
      "Epoch 1136, Loss: 0.4847458750009537, Final Batch Loss: 0.24352365732192993\n",
      "Epoch 1137, Loss: 0.5080949068069458, Final Batch Loss: 0.2692147195339203\n",
      "Epoch 1138, Loss: 0.47705723345279694, Final Batch Loss: 0.2229500561952591\n",
      "Epoch 1139, Loss: 0.5434187948703766, Final Batch Loss: 0.29110807180404663\n",
      "Epoch 1140, Loss: 0.5065516382455826, Final Batch Loss: 0.25693613290786743\n",
      "Epoch 1141, Loss: 0.6105413138866425, Final Batch Loss: 0.2968332767486572\n",
      "Epoch 1142, Loss: 0.5292623341083527, Final Batch Loss: 0.2741709351539612\n",
      "Epoch 1143, Loss: 0.5581976473331451, Final Batch Loss: 0.26448604464530945\n",
      "Epoch 1144, Loss: 0.5449116230010986, Final Batch Loss: 0.2629927694797516\n",
      "Epoch 1145, Loss: 0.5281475782394409, Final Batch Loss: 0.262972891330719\n",
      "Epoch 1146, Loss: 0.48453278839588165, Final Batch Loss: 0.22760777175426483\n",
      "Epoch 1147, Loss: 0.5131901502609253, Final Batch Loss: 0.24766606092453003\n",
      "Epoch 1148, Loss: 0.5047403424978256, Final Batch Loss: 0.26187726855278015\n",
      "Epoch 1149, Loss: 0.558742344379425, Final Batch Loss: 0.2975425124168396\n",
      "Epoch 1150, Loss: 0.5354801714420319, Final Batch Loss: 0.25943854451179504\n",
      "Epoch 1151, Loss: 0.5155051797628403, Final Batch Loss: 0.24222232401371002\n",
      "Epoch 1152, Loss: 0.492703914642334, Final Batch Loss: 0.24540375173091888\n",
      "Epoch 1153, Loss: 0.5009659379720688, Final Batch Loss: 0.2668379843235016\n",
      "Epoch 1154, Loss: 0.526273712515831, Final Batch Loss: 0.282095730304718\n",
      "Epoch 1155, Loss: 0.4987719655036926, Final Batch Loss: 0.26412099599838257\n",
      "Epoch 1156, Loss: 0.5137565732002258, Final Batch Loss: 0.22841224074363708\n",
      "Epoch 1157, Loss: 0.4890696108341217, Final Batch Loss: 0.22536388039588928\n",
      "Epoch 1158, Loss: 0.5648367404937744, Final Batch Loss: 0.2680152654647827\n",
      "Epoch 1159, Loss: 0.5149012804031372, Final Batch Loss: 0.25215983390808105\n",
      "Epoch 1160, Loss: 0.5076257735490799, Final Batch Loss: 0.27043652534484863\n",
      "Epoch 1161, Loss: 0.5336773991584778, Final Batch Loss: 0.24593430757522583\n",
      "Epoch 1162, Loss: 0.5388534665107727, Final Batch Loss: 0.2563457489013672\n",
      "Epoch 1163, Loss: 0.6038148105144501, Final Batch Loss: 0.2500227689743042\n",
      "Epoch 1164, Loss: 0.5243076384067535, Final Batch Loss: 0.2448016107082367\n",
      "Epoch 1165, Loss: 0.5378928184509277, Final Batch Loss: 0.2526768445968628\n",
      "Epoch 1166, Loss: 0.5400122702121735, Final Batch Loss: 0.2621533274650574\n",
      "Epoch 1167, Loss: 0.5318177938461304, Final Batch Loss: 0.29627716541290283\n",
      "Epoch 1168, Loss: 0.5347572565078735, Final Batch Loss: 0.2537359595298767\n",
      "Epoch 1169, Loss: 0.5486100614070892, Final Batch Loss: 0.2699095606803894\n",
      "Epoch 1170, Loss: 0.5138189941644669, Final Batch Loss: 0.2645404636859894\n",
      "Epoch 1171, Loss: 0.5126697421073914, Final Batch Loss: 0.26070666313171387\n",
      "Epoch 1172, Loss: 0.45031076669692993, Final Batch Loss: 0.22200901806354523\n",
      "Epoch 1173, Loss: 0.49004775285720825, Final Batch Loss: 0.23646119236946106\n",
      "Epoch 1174, Loss: 0.46968354284763336, Final Batch Loss: 0.25312289595603943\n",
      "Epoch 1175, Loss: 0.4846097528934479, Final Batch Loss: 0.21828576922416687\n",
      "Epoch 1176, Loss: 0.5443993210792542, Final Batch Loss: 0.2585033178329468\n",
      "Epoch 1177, Loss: 0.4955890476703644, Final Batch Loss: 0.24559849500656128\n",
      "Epoch 1178, Loss: 0.6140581965446472, Final Batch Loss: 0.3240770697593689\n",
      "Epoch 1179, Loss: 0.5792112052440643, Final Batch Loss: 0.2835685908794403\n",
      "Epoch 1180, Loss: 0.47557008266448975, Final Batch Loss: 0.24518567323684692\n",
      "Epoch 1181, Loss: 0.526315301656723, Final Batch Loss: 0.24760174751281738\n",
      "Epoch 1182, Loss: 0.520579069852829, Final Batch Loss: 0.26686975359916687\n",
      "Epoch 1183, Loss: 0.4556477516889572, Final Batch Loss: 0.19240622222423553\n",
      "Epoch 1184, Loss: 0.5117601454257965, Final Batch Loss: 0.2527143955230713\n",
      "Epoch 1185, Loss: 0.5034293979406357, Final Batch Loss: 0.2587694227695465\n",
      "Epoch 1186, Loss: 0.5037706047296524, Final Batch Loss: 0.22932182252407074\n",
      "Epoch 1187, Loss: 0.48339781165122986, Final Batch Loss: 0.2292361557483673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1188, Loss: 0.5339965373277664, Final Batch Loss: 0.30955711007118225\n",
      "Epoch 1189, Loss: 0.48948438465595245, Final Batch Loss: 0.2526382505893707\n",
      "Epoch 1190, Loss: 0.534508228302002, Final Batch Loss: 0.26676610112190247\n",
      "Epoch 1191, Loss: 0.5189217627048492, Final Batch Loss: 0.26816508173942566\n",
      "Epoch 1192, Loss: 0.5101785063743591, Final Batch Loss: 0.25373604893684387\n",
      "Epoch 1193, Loss: 0.5179580301046371, Final Batch Loss: 0.2310875803232193\n",
      "Epoch 1194, Loss: 0.5181102007627487, Final Batch Loss: 0.2725951671600342\n",
      "Epoch 1195, Loss: 0.5767851173877716, Final Batch Loss: 0.3257195055484772\n",
      "Epoch 1196, Loss: 0.5851108729839325, Final Batch Loss: 0.3035000264644623\n",
      "Epoch 1197, Loss: 0.5537412166595459, Final Batch Loss: 0.23818707466125488\n",
      "Epoch 1198, Loss: 0.5311859548091888, Final Batch Loss: 0.24264442920684814\n",
      "Epoch 1199, Loss: 0.5365204513072968, Final Batch Loss: 0.26537972688674927\n",
      "Epoch 1200, Loss: 0.5696089565753937, Final Batch Loss: 0.2461167871952057\n",
      "Epoch 1201, Loss: 0.5574618130922318, Final Batch Loss: 0.3320678770542145\n",
      "Epoch 1202, Loss: 0.5466697514057159, Final Batch Loss: 0.2796489894390106\n",
      "Epoch 1203, Loss: 0.5452286601066589, Final Batch Loss: 0.28824833035469055\n",
      "Epoch 1204, Loss: 0.4706922620534897, Final Batch Loss: 0.24277208745479584\n",
      "Epoch 1205, Loss: 0.49678783118724823, Final Batch Loss: 0.24258045852184296\n",
      "Epoch 1206, Loss: 0.49825792014598846, Final Batch Loss: 0.2449978142976761\n",
      "Epoch 1207, Loss: 0.5064424872398376, Final Batch Loss: 0.2617237865924835\n",
      "Epoch 1208, Loss: 0.4732457250356674, Final Batch Loss: 0.21953387558460236\n",
      "Epoch 1209, Loss: 0.530880481004715, Final Batch Loss: 0.26360762119293213\n",
      "Epoch 1210, Loss: 0.4842582643032074, Final Batch Loss: 0.2531455457210541\n",
      "Epoch 1211, Loss: 0.5577855706214905, Final Batch Loss: 0.2944222390651703\n",
      "Epoch 1212, Loss: 0.5671352744102478, Final Batch Loss: 0.27720990777015686\n",
      "Epoch 1213, Loss: 0.47628752887248993, Final Batch Loss: 0.23235076665878296\n",
      "Epoch 1214, Loss: 0.5474726259708405, Final Batch Loss: 0.2531979978084564\n",
      "Epoch 1215, Loss: 0.5206128358840942, Final Batch Loss: 0.2410714030265808\n",
      "Epoch 1216, Loss: 0.5001062154769897, Final Batch Loss: 0.2512228190898895\n",
      "Epoch 1217, Loss: 0.5824386179447174, Final Batch Loss: 0.2631286084651947\n",
      "Epoch 1218, Loss: 0.5079026818275452, Final Batch Loss: 0.26917052268981934\n",
      "Epoch 1219, Loss: 0.5731030404567719, Final Batch Loss: 0.31308290362358093\n",
      "Epoch 1220, Loss: 0.5041581988334656, Final Batch Loss: 0.2557101845741272\n",
      "Epoch 1221, Loss: 0.5266380310058594, Final Batch Loss: 0.29491856694221497\n",
      "Epoch 1222, Loss: 0.5034284442663193, Final Batch Loss: 0.23333169519901276\n",
      "Epoch 1223, Loss: 0.5405266135931015, Final Batch Loss: 0.3057248890399933\n",
      "Epoch 1224, Loss: 0.4540863037109375, Final Batch Loss: 0.22227142751216888\n",
      "Epoch 1225, Loss: 0.4470924586057663, Final Batch Loss: 0.20926204323768616\n",
      "Epoch 1226, Loss: 0.5776859819889069, Final Batch Loss: 0.31127533316612244\n",
      "Epoch 1227, Loss: 0.5417965203523636, Final Batch Loss: 0.23739512264728546\n",
      "Epoch 1228, Loss: 0.502961277961731, Final Batch Loss: 0.272596150636673\n",
      "Epoch 1229, Loss: 0.5963755249977112, Final Batch Loss: 0.3434813916683197\n",
      "Epoch 1230, Loss: 0.47769181430339813, Final Batch Loss: 0.23642632365226746\n",
      "Epoch 1231, Loss: 0.5656597912311554, Final Batch Loss: 0.2937486171722412\n",
      "Epoch 1232, Loss: 0.5071881860494614, Final Batch Loss: 0.2702339291572571\n",
      "Epoch 1233, Loss: 0.6713077127933502, Final Batch Loss: 0.3823019564151764\n",
      "Epoch 1234, Loss: 0.47818784415721893, Final Batch Loss: 0.20912288129329681\n",
      "Epoch 1235, Loss: 0.530484139919281, Final Batch Loss: 0.2447252869606018\n",
      "Epoch 1236, Loss: 0.5282131880521774, Final Batch Loss: 0.2915480434894562\n",
      "Epoch 1237, Loss: 0.5108939409255981, Final Batch Loss: 0.255617618560791\n",
      "Epoch 1238, Loss: 0.5604121685028076, Final Batch Loss: 0.29226839542388916\n",
      "Epoch 1239, Loss: 0.4708469361066818, Final Batch Loss: 0.2063065618276596\n",
      "Epoch 1240, Loss: 0.5690892934799194, Final Batch Loss: 0.2550356984138489\n",
      "Epoch 1241, Loss: 0.49922020733356476, Final Batch Loss: 0.2714311480522156\n",
      "Epoch 1242, Loss: 0.5411368757486343, Final Batch Loss: 0.30069729685783386\n",
      "Epoch 1243, Loss: 0.5619767904281616, Final Batch Loss: 0.29717084765434265\n",
      "Epoch 1244, Loss: 0.4481531232595444, Final Batch Loss: 0.21568500995635986\n",
      "Epoch 1245, Loss: 0.5038797557353973, Final Batch Loss: 0.23929333686828613\n",
      "Epoch 1246, Loss: 0.4588443785905838, Final Batch Loss: 0.2171170562505722\n",
      "Epoch 1247, Loss: 0.5139381438493729, Final Batch Loss: 0.2677452266216278\n",
      "Epoch 1248, Loss: 0.5333480536937714, Final Batch Loss: 0.23939216136932373\n",
      "Epoch 1249, Loss: 0.5104534476995468, Final Batch Loss: 0.26362791657447815\n",
      "Epoch 1250, Loss: 0.48094145953655243, Final Batch Loss: 0.2067710906267166\n",
      "Epoch 1251, Loss: 0.4828481823205948, Final Batch Loss: 0.2257940024137497\n",
      "Epoch 1252, Loss: 0.5414186120033264, Final Batch Loss: 0.26500749588012695\n",
      "Epoch 1253, Loss: 0.46109539270401, Final Batch Loss: 0.21804021298885345\n",
      "Epoch 1254, Loss: 0.5679904818534851, Final Batch Loss: 0.321388304233551\n",
      "Epoch 1255, Loss: 0.5336727499961853, Final Batch Loss: 0.2808578908443451\n",
      "Epoch 1256, Loss: 0.48177269101142883, Final Batch Loss: 0.23512348532676697\n",
      "Epoch 1257, Loss: 0.5404777675867081, Final Batch Loss: 0.29188787937164307\n",
      "Epoch 1258, Loss: 0.49458645284175873, Final Batch Loss: 0.25723037123680115\n",
      "Epoch 1259, Loss: 0.49739375710487366, Final Batch Loss: 0.2651374340057373\n",
      "Epoch 1260, Loss: 0.48322826623916626, Final Batch Loss: 0.247963085770607\n",
      "Epoch 1261, Loss: 0.4678664803504944, Final Batch Loss: 0.2417353093624115\n",
      "Epoch 1262, Loss: 0.4507441520690918, Final Batch Loss: 0.20039799809455872\n",
      "Epoch 1263, Loss: 0.4663582444190979, Final Batch Loss: 0.20591703057289124\n",
      "Epoch 1264, Loss: 0.5144636332988739, Final Batch Loss: 0.22292587161064148\n",
      "Epoch 1265, Loss: 0.4613020569086075, Final Batch Loss: 0.17872022092342377\n",
      "Epoch 1266, Loss: 0.5246401429176331, Final Batch Loss: 0.263385146856308\n",
      "Epoch 1267, Loss: 0.5119057446718216, Final Batch Loss: 0.23811064660549164\n",
      "Epoch 1268, Loss: 0.45499755442142487, Final Batch Loss: 0.21633698046207428\n",
      "Epoch 1269, Loss: 0.46145355701446533, Final Batch Loss: 0.21748419106006622\n",
      "Epoch 1270, Loss: 0.47423155605793, Final Batch Loss: 0.2838287055492401\n",
      "Epoch 1271, Loss: 0.5304841846227646, Final Batch Loss: 0.23933781683444977\n",
      "Epoch 1272, Loss: 0.4395301192998886, Final Batch Loss: 0.1821613758802414\n",
      "Epoch 1273, Loss: 0.5225700736045837, Final Batch Loss: 0.2721197307109833\n",
      "Epoch 1274, Loss: 0.4761744737625122, Final Batch Loss: 0.22471418976783752\n",
      "Epoch 1275, Loss: 0.4656312018632889, Final Batch Loss: 0.21470732986927032\n",
      "Epoch 1276, Loss: 0.47216610610485077, Final Batch Loss: 0.1888497918844223\n",
      "Epoch 1277, Loss: 0.5016676634550095, Final Batch Loss: 0.2369496077299118\n",
      "Epoch 1278, Loss: 0.4764687865972519, Final Batch Loss: 0.2562936246395111\n",
      "Epoch 1279, Loss: 0.44785410165786743, Final Batch Loss: 0.2195885330438614\n",
      "Epoch 1280, Loss: 0.4758874922990799, Final Batch Loss: 0.23018872737884521\n",
      "Epoch 1281, Loss: 0.5004149824380875, Final Batch Loss: 0.2717018127441406\n",
      "Epoch 1282, Loss: 0.4745132178068161, Final Batch Loss: 0.2521284818649292\n",
      "Epoch 1283, Loss: 0.525641456246376, Final Batch Loss: 0.27609020471572876\n",
      "Epoch 1284, Loss: 0.5185128599405289, Final Batch Loss: 0.2700739800930023\n",
      "Epoch 1285, Loss: 0.4856407642364502, Final Batch Loss: 0.24090628325939178\n",
      "Epoch 1286, Loss: 0.45474866032600403, Final Batch Loss: 0.2137034386396408\n",
      "Epoch 1287, Loss: 0.5490216314792633, Final Batch Loss: 0.2619490325450897\n",
      "Epoch 1288, Loss: 0.46474553644657135, Final Batch Loss: 0.20144103467464447\n",
      "Epoch 1289, Loss: 0.5103440880775452, Final Batch Loss: 0.25512778759002686\n",
      "Epoch 1290, Loss: 0.518330454826355, Final Batch Loss: 0.2682836949825287\n",
      "Epoch 1291, Loss: 0.551646500825882, Final Batch Loss: 0.3257583975791931\n",
      "Epoch 1292, Loss: 0.48175640404224396, Final Batch Loss: 0.2482759952545166\n",
      "Epoch 1293, Loss: 0.537202775478363, Final Batch Loss: 0.2820523977279663\n",
      "Epoch 1294, Loss: 0.5757277607917786, Final Batch Loss: 0.3213930130004883\n",
      "Epoch 1295, Loss: 0.440717950463295, Final Batch Loss: 0.22701847553253174\n",
      "Epoch 1296, Loss: 0.5756136476993561, Final Batch Loss: 0.3000491261482239\n",
      "Epoch 1297, Loss: 0.5094304978847504, Final Batch Loss: 0.28823983669281006\n",
      "Epoch 1298, Loss: 0.515411302447319, Final Batch Loss: 0.2691514194011688\n",
      "Epoch 1299, Loss: 0.5253665000200272, Final Batch Loss: 0.29728952050209045\n",
      "Epoch 1300, Loss: 0.5353772938251495, Final Batch Loss: 0.2854844629764557\n",
      "Epoch 1301, Loss: 0.4634842276573181, Final Batch Loss: 0.2235204428434372\n",
      "Epoch 1302, Loss: 0.4582696259021759, Final Batch Loss: 0.20320230722427368\n",
      "Epoch 1303, Loss: 0.5106059908866882, Final Batch Loss: 0.2685069739818573\n",
      "Epoch 1304, Loss: 0.6023340821266174, Final Batch Loss: 0.31042414903640747\n",
      "Epoch 1305, Loss: 0.4878564327955246, Final Batch Loss: 0.2663848400115967\n",
      "Epoch 1306, Loss: 0.48169927299022675, Final Batch Loss: 0.24403326213359833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1307, Loss: 0.5113189071416855, Final Batch Loss: 0.21687139570713043\n",
      "Epoch 1308, Loss: 0.4733027517795563, Final Batch Loss: 0.21867325901985168\n",
      "Epoch 1309, Loss: 0.5141393542289734, Final Batch Loss: 0.26294657588005066\n",
      "Epoch 1310, Loss: 0.46429844200611115, Final Batch Loss: 0.1832929402589798\n",
      "Epoch 1311, Loss: 0.47267545759677887, Final Batch Loss: 0.2094113677740097\n",
      "Epoch 1312, Loss: 0.4677761048078537, Final Batch Loss: 0.2215796262025833\n",
      "Epoch 1313, Loss: 0.49855050444602966, Final Batch Loss: 0.2535664737224579\n",
      "Epoch 1314, Loss: 0.4617668092250824, Final Batch Loss: 0.22752141952514648\n",
      "Epoch 1315, Loss: 0.511786162853241, Final Batch Loss: 0.23565909266471863\n",
      "Epoch 1316, Loss: 0.5126748383045197, Final Batch Loss: 0.2573918104171753\n",
      "Epoch 1317, Loss: 0.5346689075231552, Final Batch Loss: 0.29371377825737\n",
      "Epoch 1318, Loss: 0.48189079761505127, Final Batch Loss: 0.24994856119155884\n",
      "Epoch 1319, Loss: 0.5300653874874115, Final Batch Loss: 0.2552333176136017\n",
      "Epoch 1320, Loss: 0.49430719017982483, Final Batch Loss: 0.23956623673439026\n",
      "Epoch 1321, Loss: 0.5164242386817932, Final Batch Loss: 0.30926549434661865\n",
      "Epoch 1322, Loss: 0.4773491472005844, Final Batch Loss: 0.2127075046300888\n",
      "Epoch 1323, Loss: 0.4368393123149872, Final Batch Loss: 0.18376028537750244\n",
      "Epoch 1324, Loss: 0.49771976470947266, Final Batch Loss: 0.2669585645198822\n",
      "Epoch 1325, Loss: 0.45892056822776794, Final Batch Loss: 0.22353248298168182\n",
      "Epoch 1326, Loss: 0.46070989966392517, Final Batch Loss: 0.23843908309936523\n",
      "Epoch 1327, Loss: 0.46994490921497345, Final Batch Loss: 0.24829553067684174\n",
      "Epoch 1328, Loss: 0.43082864582538605, Final Batch Loss: 0.19344383478164673\n",
      "Epoch 1329, Loss: 0.469295009970665, Final Batch Loss: 0.25608065724372864\n",
      "Epoch 1330, Loss: 0.4967041164636612, Final Batch Loss: 0.24918362498283386\n",
      "Epoch 1331, Loss: 0.4759170562028885, Final Batch Loss: 0.3054122030735016\n",
      "Epoch 1332, Loss: 0.4879121631383896, Final Batch Loss: 0.24399657547473907\n",
      "Epoch 1333, Loss: 0.5044878572225571, Final Batch Loss: 0.2633236050605774\n",
      "Epoch 1334, Loss: 0.5120820850133896, Final Batch Loss: 0.26529303193092346\n",
      "Epoch 1335, Loss: 0.5726214051246643, Final Batch Loss: 0.3070123493671417\n",
      "Epoch 1336, Loss: 0.49712666869163513, Final Batch Loss: 0.23354744911193848\n",
      "Epoch 1337, Loss: 0.5082636028528214, Final Batch Loss: 0.27571573853492737\n",
      "Epoch 1338, Loss: 0.5106657594442368, Final Batch Loss: 0.2834528386592865\n",
      "Epoch 1339, Loss: 0.4529200345277786, Final Batch Loss: 0.2160593867301941\n",
      "Epoch 1340, Loss: 0.45236679911613464, Final Batch Loss: 0.20519553124904633\n",
      "Epoch 1341, Loss: 0.5409446954727173, Final Batch Loss: 0.2602405548095703\n",
      "Epoch 1342, Loss: 0.5302504301071167, Final Batch Loss: 0.3001770079135895\n",
      "Epoch 1343, Loss: 0.4941241294145584, Final Batch Loss: 0.26312485337257385\n",
      "Epoch 1344, Loss: 0.5326545238494873, Final Batch Loss: 0.2708583176136017\n",
      "Epoch 1345, Loss: 0.4711263030767441, Final Batch Loss: 0.21956242620944977\n",
      "Epoch 1346, Loss: 0.4733087569475174, Final Batch Loss: 0.20663337409496307\n",
      "Epoch 1347, Loss: 0.5891850292682648, Final Batch Loss: 0.3423648178577423\n",
      "Epoch 1348, Loss: 0.4853782206773758, Final Batch Loss: 0.29974254965782166\n",
      "Epoch 1349, Loss: 0.48893409967422485, Final Batch Loss: 0.22129297256469727\n",
      "Epoch 1350, Loss: 0.457208976149559, Final Batch Loss: 0.18104858696460724\n",
      "Epoch 1351, Loss: 0.447170689702034, Final Batch Loss: 0.20871829986572266\n",
      "Epoch 1352, Loss: 0.49952419102191925, Final Batch Loss: 0.2630005180835724\n",
      "Epoch 1353, Loss: 0.5725946426391602, Final Batch Loss: 0.2975270450115204\n",
      "Epoch 1354, Loss: 0.4867544025182724, Final Batch Loss: 0.25528568029403687\n",
      "Epoch 1355, Loss: 0.5495940446853638, Final Batch Loss: 0.2902355194091797\n",
      "Epoch 1356, Loss: 0.5104253739118576, Final Batch Loss: 0.26473599672317505\n",
      "Epoch 1357, Loss: 0.45736342668533325, Final Batch Loss: 0.2295302003622055\n",
      "Epoch 1358, Loss: 0.45311519503593445, Final Batch Loss: 0.2196381688117981\n",
      "Epoch 1359, Loss: 0.5241544991731644, Final Batch Loss: 0.29936152696609497\n",
      "Epoch 1360, Loss: 0.42973336577415466, Final Batch Loss: 0.20128297805786133\n",
      "Epoch 1361, Loss: 0.45605482161045074, Final Batch Loss: 0.20289616286754608\n",
      "Epoch 1362, Loss: 0.46752235293388367, Final Batch Loss: 0.25613999366760254\n",
      "Epoch 1363, Loss: 0.44514669477939606, Final Batch Loss: 0.23837433755397797\n",
      "Epoch 1364, Loss: 0.4624277949333191, Final Batch Loss: 0.21134522557258606\n",
      "Epoch 1365, Loss: 0.47845010459423065, Final Batch Loss: 0.17788736522197723\n",
      "Epoch 1366, Loss: 0.4600709229707718, Final Batch Loss: 0.21998834609985352\n",
      "Epoch 1367, Loss: 0.5209503620862961, Final Batch Loss: 0.28378045558929443\n",
      "Epoch 1368, Loss: 0.4644591063261032, Final Batch Loss: 0.228305846452713\n",
      "Epoch 1369, Loss: 0.46673691272735596, Final Batch Loss: 0.24904276430606842\n",
      "Epoch 1370, Loss: 0.4995989054441452, Final Batch Loss: 0.21748514473438263\n",
      "Epoch 1371, Loss: 0.5181248188018799, Final Batch Loss: 0.23496311902999878\n",
      "Epoch 1372, Loss: 0.5078841894865036, Final Batch Loss: 0.22614876925945282\n",
      "Epoch 1373, Loss: 0.5721203982830048, Final Batch Loss: 0.3147836923599243\n",
      "Epoch 1374, Loss: 0.48863083124160767, Final Batch Loss: 0.24717466533184052\n",
      "Epoch 1375, Loss: 0.5398919880390167, Final Batch Loss: 0.2624122202396393\n",
      "Epoch 1376, Loss: 0.46011781692504883, Final Batch Loss: 0.23213350772857666\n",
      "Epoch 1377, Loss: 0.4890873581171036, Final Batch Loss: 0.26275962591171265\n",
      "Epoch 1378, Loss: 0.48733551800251007, Final Batch Loss: 0.24844829738140106\n",
      "Epoch 1379, Loss: 0.4379691630601883, Final Batch Loss: 0.19804435968399048\n",
      "Epoch 1380, Loss: 0.4906047433614731, Final Batch Loss: 0.21980895102024078\n",
      "Epoch 1381, Loss: 0.4422135353088379, Final Batch Loss: 0.23087891936302185\n",
      "Epoch 1382, Loss: 0.4506329596042633, Final Batch Loss: 0.22050291299819946\n",
      "Epoch 1383, Loss: 0.5403043180704117, Final Batch Loss: 0.32888251543045044\n",
      "Epoch 1384, Loss: 0.4398447722196579, Final Batch Loss: 0.18948359787464142\n",
      "Epoch 1385, Loss: 0.49856774508953094, Final Batch Loss: 0.24952445924282074\n",
      "Epoch 1386, Loss: 0.5128178745508194, Final Batch Loss: 0.29141226410865784\n",
      "Epoch 1387, Loss: 0.4963018000125885, Final Batch Loss: 0.2241339087486267\n",
      "Epoch 1388, Loss: 0.5320659726858139, Final Batch Loss: 0.24528367817401886\n",
      "Epoch 1389, Loss: 0.44204625487327576, Final Batch Loss: 0.22240136563777924\n",
      "Epoch 1390, Loss: 0.4339865446090698, Final Batch Loss: 0.18997706472873688\n",
      "Epoch 1391, Loss: 0.4433920681476593, Final Batch Loss: 0.19509448111057281\n",
      "Epoch 1392, Loss: 0.5205540359020233, Final Batch Loss: 0.25957098603248596\n",
      "Epoch 1393, Loss: 0.4300534278154373, Final Batch Loss: 0.1888691633939743\n",
      "Epoch 1394, Loss: 0.4478108584880829, Final Batch Loss: 0.21534773707389832\n",
      "Epoch 1395, Loss: 0.5308594256639481, Final Batch Loss: 0.28657180070877075\n",
      "Epoch 1396, Loss: 0.4393727034330368, Final Batch Loss: 0.20137952268123627\n",
      "Epoch 1397, Loss: 0.5071360021829605, Final Batch Loss: 0.21797995269298553\n",
      "Epoch 1398, Loss: 0.4635944813489914, Final Batch Loss: 0.2107081562280655\n",
      "Epoch 1399, Loss: 0.440651535987854, Final Batch Loss: 0.21791203320026398\n",
      "Epoch 1400, Loss: 0.5406421720981598, Final Batch Loss: 0.2634700834751129\n",
      "Epoch 1401, Loss: 0.4623854011297226, Final Batch Loss: 0.20483486354351044\n",
      "Epoch 1402, Loss: 0.4651493579149246, Final Batch Loss: 0.21404890716075897\n",
      "Epoch 1403, Loss: 0.4786512404680252, Final Batch Loss: 0.2470402866601944\n",
      "Epoch 1404, Loss: 0.4932128041982651, Final Batch Loss: 0.22481565177440643\n",
      "Epoch 1405, Loss: 0.519083634018898, Final Batch Loss: 0.245789036154747\n",
      "Epoch 1406, Loss: 0.4520060867071152, Final Batch Loss: 0.18848343193531036\n",
      "Epoch 1407, Loss: 0.436721533536911, Final Batch Loss: 0.21563056111335754\n",
      "Epoch 1408, Loss: 0.48218195140361786, Final Batch Loss: 0.2051744908094406\n",
      "Epoch 1409, Loss: 0.5034080147743225, Final Batch Loss: 0.2854604125022888\n",
      "Epoch 1410, Loss: 0.6802708208560944, Final Batch Loss: 0.47695574164390564\n",
      "Epoch 1411, Loss: 0.4373292028903961, Final Batch Loss: 0.20764511823654175\n",
      "Epoch 1412, Loss: 0.4370037764310837, Final Batch Loss: 0.2241976410150528\n",
      "Epoch 1413, Loss: 0.4230378270149231, Final Batch Loss: 0.19021563231945038\n",
      "Epoch 1414, Loss: 0.518756628036499, Final Batch Loss: 0.25118470191955566\n",
      "Epoch 1415, Loss: 0.4638332724571228, Final Batch Loss: 0.22162039577960968\n",
      "Epoch 1416, Loss: 0.46852461993694305, Final Batch Loss: 0.2511969208717346\n",
      "Epoch 1417, Loss: 0.4945468455553055, Final Batch Loss: 0.23977364599704742\n",
      "Epoch 1418, Loss: 0.4791213423013687, Final Batch Loss: 0.2946350872516632\n",
      "Epoch 1419, Loss: 0.441994771361351, Final Batch Loss: 0.20646162331104279\n",
      "Epoch 1420, Loss: 0.50577512383461, Final Batch Loss: 0.254635751247406\n",
      "Epoch 1421, Loss: 0.5237750113010406, Final Batch Loss: 0.2507743239402771\n",
      "Epoch 1422, Loss: 0.46345365047454834, Final Batch Loss: 0.19922444224357605\n",
      "Epoch 1423, Loss: 0.41172297298908234, Final Batch Loss: 0.15732266008853912\n",
      "Epoch 1424, Loss: 0.4509231150150299, Final Batch Loss: 0.24212905764579773\n",
      "Epoch 1425, Loss: 0.45443426072597504, Final Batch Loss: 0.2478312999010086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1426, Loss: 0.491867333650589, Final Batch Loss: 0.27983176708221436\n",
      "Epoch 1427, Loss: 0.502001166343689, Final Batch Loss: 0.2584643065929413\n",
      "Epoch 1428, Loss: 0.4569356292486191, Final Batch Loss: 0.20290519297122955\n",
      "Epoch 1429, Loss: 0.50956991314888, Final Batch Loss: 0.2882494032382965\n",
      "Epoch 1430, Loss: 0.45840510725975037, Final Batch Loss: 0.21299725770950317\n",
      "Epoch 1431, Loss: 0.46424221992492676, Final Batch Loss: 0.2207399159669876\n",
      "Epoch 1432, Loss: 0.5016297250986099, Final Batch Loss: 0.27428218722343445\n",
      "Epoch 1433, Loss: 0.4627727121114731, Final Batch Loss: 0.21063874661922455\n",
      "Epoch 1434, Loss: 0.4304669499397278, Final Batch Loss: 0.1848578006029129\n",
      "Epoch 1435, Loss: 0.47728846967220306, Final Batch Loss: 0.20033518970012665\n",
      "Epoch 1436, Loss: 0.4939933121204376, Final Batch Loss: 0.2664273679256439\n",
      "Epoch 1437, Loss: 0.45704659819602966, Final Batch Loss: 0.23670171201229095\n",
      "Epoch 1438, Loss: 0.5002439171075821, Final Batch Loss: 0.2962871789932251\n",
      "Epoch 1439, Loss: 0.5182227194309235, Final Batch Loss: 0.27651458978652954\n",
      "Epoch 1440, Loss: 0.42708511650562286, Final Batch Loss: 0.18579739332199097\n",
      "Epoch 1441, Loss: 0.48871974647045135, Final Batch Loss: 0.22449035942554474\n",
      "Epoch 1442, Loss: 0.43548738956451416, Final Batch Loss: 0.2117183953523636\n",
      "Epoch 1443, Loss: 0.4370132386684418, Final Batch Loss: 0.19942522048950195\n",
      "Epoch 1444, Loss: 0.47745761275291443, Final Batch Loss: 0.2585977017879486\n",
      "Epoch 1445, Loss: 0.5376153588294983, Final Batch Loss: 0.2941322922706604\n",
      "Epoch 1446, Loss: 0.46000656485557556, Final Batch Loss: 0.1792755126953125\n",
      "Epoch 1447, Loss: 0.44962671399116516, Final Batch Loss: 0.19753822684288025\n",
      "Epoch 1448, Loss: 0.46671396493911743, Final Batch Loss: 0.20976144075393677\n",
      "Epoch 1449, Loss: 0.4455902427434921, Final Batch Loss: 0.21351641416549683\n",
      "Epoch 1450, Loss: 0.4754140079021454, Final Batch Loss: 0.21777227520942688\n",
      "Epoch 1451, Loss: 0.4780292361974716, Final Batch Loss: 0.256363183259964\n",
      "Epoch 1452, Loss: 0.48381151258945465, Final Batch Loss: 0.22575165331363678\n",
      "Epoch 1453, Loss: 0.4875747561454773, Final Batch Loss: 0.22853490710258484\n",
      "Epoch 1454, Loss: 0.4662780612707138, Final Batch Loss: 0.21503888070583344\n",
      "Epoch 1455, Loss: 0.4871140867471695, Final Batch Loss: 0.24325406551361084\n",
      "Epoch 1456, Loss: 0.46263761818408966, Final Batch Loss: 0.23526760935783386\n",
      "Epoch 1457, Loss: 0.4795082211494446, Final Batch Loss: 0.24005526304244995\n",
      "Epoch 1458, Loss: 0.46303944289684296, Final Batch Loss: 0.23818816244602203\n",
      "Epoch 1459, Loss: 0.47459550201892853, Final Batch Loss: 0.2483511120080948\n",
      "Epoch 1460, Loss: 0.42084434628486633, Final Batch Loss: 0.17562155425548553\n",
      "Epoch 1461, Loss: 0.4649832397699356, Final Batch Loss: 0.2461039423942566\n",
      "Epoch 1462, Loss: 0.5769041478633881, Final Batch Loss: 0.3202356994152069\n",
      "Epoch 1463, Loss: 0.4432218670845032, Final Batch Loss: 0.164899080991745\n",
      "Epoch 1464, Loss: 0.4494016468524933, Final Batch Loss: 0.23816289007663727\n",
      "Epoch 1465, Loss: 0.5095471441745758, Final Batch Loss: 0.2501968443393707\n",
      "Epoch 1466, Loss: 0.49731703102588654, Final Batch Loss: 0.2717333436012268\n",
      "Epoch 1467, Loss: 0.516141414642334, Final Batch Loss: 0.3080767095088959\n",
      "Epoch 1468, Loss: 0.4560406059026718, Final Batch Loss: 0.22725002467632294\n",
      "Epoch 1469, Loss: 0.4501142203807831, Final Batch Loss: 0.23338331282138824\n",
      "Epoch 1470, Loss: 0.45035989582538605, Final Batch Loss: 0.20927423238754272\n",
      "Epoch 1471, Loss: 0.4812210351228714, Final Batch Loss: 0.2619299292564392\n",
      "Epoch 1472, Loss: 0.47113069891929626, Final Batch Loss: 0.2471097856760025\n",
      "Epoch 1473, Loss: 0.4958973824977875, Final Batch Loss: 0.22302591800689697\n",
      "Epoch 1474, Loss: 0.4236913025379181, Final Batch Loss: 0.20872457325458527\n",
      "Epoch 1475, Loss: 0.5019574463367462, Final Batch Loss: 0.2909955382347107\n",
      "Epoch 1476, Loss: 0.4508556127548218, Final Batch Loss: 0.21003128588199615\n",
      "Epoch 1477, Loss: 0.4804028272628784, Final Batch Loss: 0.19216501712799072\n",
      "Epoch 1478, Loss: 0.4531978964805603, Final Batch Loss: 0.19434332847595215\n",
      "Epoch 1479, Loss: 0.44404198229312897, Final Batch Loss: 0.2508210241794586\n",
      "Epoch 1480, Loss: 0.4716036319732666, Final Batch Loss: 0.27022963762283325\n",
      "Epoch 1481, Loss: 0.5223900377750397, Final Batch Loss: 0.1905674934387207\n",
      "Epoch 1482, Loss: 0.5607331842184067, Final Batch Loss: 0.374721497297287\n",
      "Epoch 1483, Loss: 0.4892819672822952, Final Batch Loss: 0.3097037672996521\n",
      "Epoch 1484, Loss: 0.44719523191452026, Final Batch Loss: 0.19779908657073975\n",
      "Epoch 1485, Loss: 0.47028666734695435, Final Batch Loss: 0.20891034603118896\n",
      "Epoch 1486, Loss: 0.43919211626052856, Final Batch Loss: 0.19654248654842377\n",
      "Epoch 1487, Loss: 0.4280489534139633, Final Batch Loss: 0.23904478549957275\n",
      "Epoch 1488, Loss: 0.42580799758434296, Final Batch Loss: 0.16608081758022308\n",
      "Epoch 1489, Loss: 0.4561444818973541, Final Batch Loss: 0.20636539161205292\n",
      "Epoch 1490, Loss: 0.43305322527885437, Final Batch Loss: 0.23221024870872498\n",
      "Epoch 1491, Loss: 0.504271537065506, Final Batch Loss: 0.24643489718437195\n",
      "Epoch 1492, Loss: 0.358929306268692, Final Batch Loss: 0.14185839891433716\n",
      "Epoch 1493, Loss: 0.45203328132629395, Final Batch Loss: 0.19269320368766785\n",
      "Epoch 1494, Loss: 0.47621333599090576, Final Batch Loss: 0.25265753269195557\n",
      "Epoch 1495, Loss: 0.4807108938694, Final Batch Loss: 0.24415059387683868\n",
      "Epoch 1496, Loss: 0.47906023263931274, Final Batch Loss: 0.22299093008041382\n",
      "Epoch 1497, Loss: 0.4771818518638611, Final Batch Loss: 0.2691652774810791\n",
      "Epoch 1498, Loss: 0.46408870816230774, Final Batch Loss: 0.21426039934158325\n",
      "Epoch 1499, Loss: 0.4491335302591324, Final Batch Loss: 0.20244576036930084\n",
      "Epoch 1500, Loss: 0.5084586292505264, Final Batch Loss: 0.29569053649902344\n",
      "Epoch 1501, Loss: 0.4534410089254379, Final Batch Loss: 0.22329586744308472\n",
      "Epoch 1502, Loss: 0.45145492255687714, Final Batch Loss: 0.28116992115974426\n",
      "Epoch 1503, Loss: 0.4281782805919647, Final Batch Loss: 0.21321773529052734\n",
      "Epoch 1504, Loss: 0.5887691080570221, Final Batch Loss: 0.34954559803009033\n",
      "Epoch 1505, Loss: 0.5013706088066101, Final Batch Loss: 0.18863895535469055\n",
      "Epoch 1506, Loss: 0.44617803394794464, Final Batch Loss: 0.23532089591026306\n",
      "Epoch 1507, Loss: 0.5269081145524979, Final Batch Loss: 0.27768346667289734\n",
      "Epoch 1508, Loss: 0.4803618788719177, Final Batch Loss: 0.2530023157596588\n",
      "Epoch 1509, Loss: 0.471482515335083, Final Batch Loss: 0.2447764277458191\n",
      "Epoch 1510, Loss: 0.46951235830783844, Final Batch Loss: 0.2607884407043457\n",
      "Epoch 1511, Loss: 0.42939843237400055, Final Batch Loss: 0.1791577786207199\n",
      "Epoch 1512, Loss: 0.442618191242218, Final Batch Loss: 0.208330899477005\n",
      "Epoch 1513, Loss: 0.4436056911945343, Final Batch Loss: 0.22287578880786896\n",
      "Epoch 1514, Loss: 0.4256182909011841, Final Batch Loss: 0.19774051010608673\n",
      "Epoch 1515, Loss: 0.4337741881608963, Final Batch Loss: 0.17769671976566315\n",
      "Epoch 1516, Loss: 0.4451875686645508, Final Batch Loss: 0.2458263635635376\n",
      "Epoch 1517, Loss: 0.4479980021715164, Final Batch Loss: 0.20124387741088867\n",
      "Epoch 1518, Loss: 0.41924403607845306, Final Batch Loss: 0.18993723392486572\n",
      "Epoch 1519, Loss: 0.41696758568286896, Final Batch Loss: 0.1603430062532425\n",
      "Epoch 1520, Loss: 0.4401799589395523, Final Batch Loss: 0.23009736835956573\n",
      "Epoch 1521, Loss: 0.5047538727521896, Final Batch Loss: 0.23364807665348053\n",
      "Epoch 1522, Loss: 0.4679133743047714, Final Batch Loss: 0.2409648299217224\n",
      "Epoch 1523, Loss: 0.41217102110385895, Final Batch Loss: 0.22281041741371155\n",
      "Epoch 1524, Loss: 0.43712665140628815, Final Batch Loss: 0.2241666167974472\n",
      "Epoch 1525, Loss: 0.4402657598257065, Final Batch Loss: 0.24544091522693634\n",
      "Epoch 1526, Loss: 0.4190777987241745, Final Batch Loss: 0.1782839298248291\n",
      "Epoch 1527, Loss: 0.42729465663433075, Final Batch Loss: 0.16570807993412018\n",
      "Epoch 1528, Loss: 0.4671488255262375, Final Batch Loss: 0.2645927369594574\n",
      "Epoch 1529, Loss: 0.48220354318618774, Final Batch Loss: 0.23642857372760773\n",
      "Epoch 1530, Loss: 0.42685072124004364, Final Batch Loss: 0.1987394541501999\n",
      "Epoch 1531, Loss: 0.45124800503253937, Final Batch Loss: 0.24832306802272797\n",
      "Epoch 1532, Loss: 0.48998716473579407, Final Batch Loss: 0.3102755844593048\n",
      "Epoch 1533, Loss: 0.46622857451438904, Final Batch Loss: 0.21590453386306763\n",
      "Epoch 1534, Loss: 0.48963505029678345, Final Batch Loss: 0.3050357699394226\n",
      "Epoch 1535, Loss: 0.5160183161497116, Final Batch Loss: 0.2812556326389313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1536, Loss: 0.48572951555252075, Final Batch Loss: 0.23525959253311157\n",
      "Epoch 1537, Loss: 0.43634776771068573, Final Batch Loss: 0.20433862507343292\n",
      "Epoch 1538, Loss: 0.41733285784721375, Final Batch Loss: 0.19887378811836243\n",
      "Epoch 1539, Loss: 0.5035942643880844, Final Batch Loss: 0.2576260268688202\n",
      "Epoch 1540, Loss: 0.49052204191684723, Final Batch Loss: 0.25818783044815063\n",
      "Epoch 1541, Loss: 0.4808661937713623, Final Batch Loss: 0.206802636384964\n",
      "Epoch 1542, Loss: 0.5301612913608551, Final Batch Loss: 0.24660176038742065\n",
      "Epoch 1543, Loss: 0.4071205407381058, Final Batch Loss: 0.19703568518161774\n",
      "Epoch 1544, Loss: 0.5442819744348526, Final Batch Loss: 0.3232431411743164\n",
      "Epoch 1545, Loss: 0.477863609790802, Final Batch Loss: 0.261596143245697\n",
      "Epoch 1546, Loss: 0.43558093905448914, Final Batch Loss: 0.19241784512996674\n",
      "Epoch 1547, Loss: 0.4289660304784775, Final Batch Loss: 0.1808134913444519\n",
      "Epoch 1548, Loss: 0.4500104486942291, Final Batch Loss: 0.2705952227115631\n",
      "Epoch 1549, Loss: 0.4787685126066208, Final Batch Loss: 0.2560577988624573\n",
      "Epoch 1550, Loss: 0.40265658497810364, Final Batch Loss: 0.2088467925786972\n",
      "Epoch 1551, Loss: 0.41455069184303284, Final Batch Loss: 0.20442935824394226\n",
      "Epoch 1552, Loss: 0.4275086522102356, Final Batch Loss: 0.20759809017181396\n",
      "Epoch 1553, Loss: 0.46926961839199066, Final Batch Loss: 0.2639831602573395\n",
      "Epoch 1554, Loss: 0.4779940992593765, Final Batch Loss: 0.2330588847398758\n",
      "Epoch 1555, Loss: 0.37834396958351135, Final Batch Loss: 0.1693265736103058\n",
      "Epoch 1556, Loss: 0.4626609981060028, Final Batch Loss: 0.25473925471305847\n",
      "Epoch 1557, Loss: 0.49168306589126587, Final Batch Loss: 0.23657122254371643\n",
      "Epoch 1558, Loss: 0.45815490186214447, Final Batch Loss: 0.22401392459869385\n",
      "Epoch 1559, Loss: 0.4815194457769394, Final Batch Loss: 0.22086583077907562\n",
      "Epoch 1560, Loss: 0.4878492057323456, Final Batch Loss: 0.2296023666858673\n",
      "Epoch 1561, Loss: 0.4704461544752121, Final Batch Loss: 0.26287534832954407\n",
      "Epoch 1562, Loss: 0.46840883791446686, Final Batch Loss: 0.21620909869670868\n",
      "Epoch 1563, Loss: 0.4317515939474106, Final Batch Loss: 0.25099843740463257\n",
      "Epoch 1564, Loss: 0.4482976049184799, Final Batch Loss: 0.21015408635139465\n",
      "Epoch 1565, Loss: 0.4466773569583893, Final Batch Loss: 0.20619769394397736\n",
      "Epoch 1566, Loss: 0.4974638819694519, Final Batch Loss: 0.2561342120170593\n",
      "Epoch 1567, Loss: 0.48657770454883575, Final Batch Loss: 0.26897135376930237\n",
      "Epoch 1568, Loss: 0.4256819486618042, Final Batch Loss: 0.19336916506290436\n",
      "Epoch 1569, Loss: 0.413031741976738, Final Batch Loss: 0.199322909116745\n",
      "Epoch 1570, Loss: 0.4759627878665924, Final Batch Loss: 0.2676634192466736\n",
      "Epoch 1571, Loss: 0.4179278016090393, Final Batch Loss: 0.1834268867969513\n",
      "Epoch 1572, Loss: 0.43081924319267273, Final Batch Loss: 0.21948231756687164\n",
      "Epoch 1573, Loss: 0.409345418214798, Final Batch Loss: 0.21398138999938965\n",
      "Epoch 1574, Loss: 0.4167112708091736, Final Batch Loss: 0.18876786530017853\n",
      "Epoch 1575, Loss: 0.4217996299266815, Final Batch Loss: 0.15609341859817505\n",
      "Epoch 1576, Loss: 0.5557983815670013, Final Batch Loss: 0.2609824538230896\n",
      "Epoch 1577, Loss: 0.4831632822751999, Final Batch Loss: 0.253520667552948\n",
      "Epoch 1578, Loss: 0.41365452110767365, Final Batch Loss: 0.16955320537090302\n",
      "Epoch 1579, Loss: 0.4364788979291916, Final Batch Loss: 0.18339817225933075\n",
      "Epoch 1580, Loss: 0.4642895460128784, Final Batch Loss: 0.17787784337997437\n",
      "Epoch 1581, Loss: 0.4188065230846405, Final Batch Loss: 0.19673193991184235\n",
      "Epoch 1582, Loss: 0.5153986215591431, Final Batch Loss: 0.2602613866329193\n",
      "Epoch 1583, Loss: 0.44337956607341766, Final Batch Loss: 0.19604605436325073\n",
      "Epoch 1584, Loss: 0.4563847631216049, Final Batch Loss: 0.21708348393440247\n",
      "Epoch 1585, Loss: 0.48751217126846313, Final Batch Loss: 0.26504844427108765\n",
      "Epoch 1586, Loss: 0.44598250091075897, Final Batch Loss: 0.23429735004901886\n",
      "Epoch 1587, Loss: 0.40741999447345734, Final Batch Loss: 0.18793843686580658\n",
      "Epoch 1588, Loss: 0.40098175406455994, Final Batch Loss: 0.18475016951560974\n",
      "Epoch 1589, Loss: 0.4695417433977127, Final Batch Loss: 0.22935514152050018\n",
      "Epoch 1590, Loss: 0.4581529200077057, Final Batch Loss: 0.2170887440443039\n",
      "Epoch 1591, Loss: 0.39025963842868805, Final Batch Loss: 0.17547550797462463\n",
      "Epoch 1592, Loss: 0.42301835119724274, Final Batch Loss: 0.20298759639263153\n",
      "Epoch 1593, Loss: 0.41078750789165497, Final Batch Loss: 0.18292412161827087\n",
      "Epoch 1594, Loss: 0.4593936949968338, Final Batch Loss: 0.21022404730319977\n",
      "Epoch 1595, Loss: 0.46223485469818115, Final Batch Loss: 0.22778068482875824\n",
      "Epoch 1596, Loss: 0.4488662779331207, Final Batch Loss: 0.20614276826381683\n",
      "Epoch 1597, Loss: 0.4797336906194687, Final Batch Loss: 0.24682001769542694\n",
      "Epoch 1598, Loss: 0.47265760600566864, Final Batch Loss: 0.24261519312858582\n",
      "Epoch 1599, Loss: 0.4468662738800049, Final Batch Loss: 0.24703019857406616\n",
      "Epoch 1600, Loss: 0.46977710723876953, Final Batch Loss: 0.22693398594856262\n",
      "Epoch 1601, Loss: 0.43949735164642334, Final Batch Loss: 0.2125435769557953\n",
      "Epoch 1602, Loss: 0.4465564489364624, Final Batch Loss: 0.24189506471157074\n",
      "Epoch 1603, Loss: 0.46270276606082916, Final Batch Loss: 0.21798384189605713\n",
      "Epoch 1604, Loss: 0.4878540486097336, Final Batch Loss: 0.25535184144973755\n",
      "Epoch 1605, Loss: 0.42688751220703125, Final Batch Loss: 0.22765658795833588\n",
      "Epoch 1606, Loss: 0.49192818999290466, Final Batch Loss: 0.2531953752040863\n",
      "Epoch 1607, Loss: 0.5834574550390244, Final Batch Loss: 0.35555222630500793\n",
      "Epoch 1608, Loss: 0.4122820943593979, Final Batch Loss: 0.20235417783260345\n",
      "Epoch 1609, Loss: 0.47438476979732513, Final Batch Loss: 0.2779971659183502\n",
      "Epoch 1610, Loss: 0.5195223689079285, Final Batch Loss: 0.26761677861213684\n",
      "Epoch 1611, Loss: 0.4418793171644211, Final Batch Loss: 0.22837500274181366\n",
      "Epoch 1612, Loss: 0.5605677962303162, Final Batch Loss: 0.3054204285144806\n",
      "Epoch 1613, Loss: 0.46618181467056274, Final Batch Loss: 0.19521355628967285\n",
      "Epoch 1614, Loss: 0.420483261346817, Final Batch Loss: 0.16657188534736633\n",
      "Epoch 1615, Loss: 0.5090926587581635, Final Batch Loss: 0.25302231311798096\n",
      "Epoch 1616, Loss: 0.44802258908748627, Final Batch Loss: 0.22464853525161743\n",
      "Epoch 1617, Loss: 0.42992129921913147, Final Batch Loss: 0.21767565608024597\n",
      "Epoch 1618, Loss: 0.4506754130125046, Final Batch Loss: 0.2073538899421692\n",
      "Epoch 1619, Loss: 0.390631839632988, Final Batch Loss: 0.19312481582164764\n",
      "Epoch 1620, Loss: 0.4366869628429413, Final Batch Loss: 0.21861186623573303\n",
      "Epoch 1621, Loss: 0.38386328518390656, Final Batch Loss: 0.16845732927322388\n",
      "Epoch 1622, Loss: 0.42283689975738525, Final Batch Loss: 0.21880900859832764\n",
      "Epoch 1623, Loss: 0.4448748379945755, Final Batch Loss: 0.17432047426700592\n",
      "Epoch 1624, Loss: 0.44769586622714996, Final Batch Loss: 0.2616492509841919\n",
      "Epoch 1625, Loss: 0.41619208455085754, Final Batch Loss: 0.19319850206375122\n",
      "Epoch 1626, Loss: 0.4367952048778534, Final Batch Loss: 0.2345384657382965\n",
      "Epoch 1627, Loss: 0.42277655005455017, Final Batch Loss: 0.22502541542053223\n",
      "Epoch 1628, Loss: 0.4431305378675461, Final Batch Loss: 0.22838613390922546\n",
      "Epoch 1629, Loss: 0.46132853627204895, Final Batch Loss: 0.21223267912864685\n",
      "Epoch 1630, Loss: 0.4842107743024826, Final Batch Loss: 0.27257290482521057\n",
      "Epoch 1631, Loss: 0.4833303540945053, Final Batch Loss: 0.25951895117759705\n",
      "Epoch 1632, Loss: 0.43770046532154083, Final Batch Loss: 0.22445237636566162\n",
      "Epoch 1633, Loss: 0.42532049119472504, Final Batch Loss: 0.21657955646514893\n",
      "Epoch 1634, Loss: 0.3791595697402954, Final Batch Loss: 0.20392978191375732\n",
      "Epoch 1635, Loss: 0.4354341924190521, Final Batch Loss: 0.2619951069355011\n",
      "Epoch 1636, Loss: 0.461692214012146, Final Batch Loss: 0.20520836114883423\n",
      "Epoch 1637, Loss: 0.49716997146606445, Final Batch Loss: 0.24706870317459106\n",
      "Epoch 1638, Loss: 0.4538734406232834, Final Batch Loss: 0.2267608940601349\n",
      "Epoch 1639, Loss: 0.45522281527519226, Final Batch Loss: 0.28598976135253906\n",
      "Epoch 1640, Loss: 0.4194781631231308, Final Batch Loss: 0.22228103876113892\n",
      "Epoch 1641, Loss: 0.41499654948711395, Final Batch Loss: 0.22167359292507172\n",
      "Epoch 1642, Loss: 0.44869300723075867, Final Batch Loss: 0.23647990822792053\n",
      "Epoch 1643, Loss: 0.38591788709163666, Final Batch Loss: 0.20089615881443024\n",
      "Epoch 1644, Loss: 0.43864738941192627, Final Batch Loss: 0.2135065346956253\n",
      "Epoch 1645, Loss: 0.46790122985839844, Final Batch Loss: 0.24369220435619354\n",
      "Epoch 1646, Loss: 0.5226671397686005, Final Batch Loss: 0.3287851810455322\n",
      "Epoch 1647, Loss: 0.45985883474349976, Final Batch Loss: 0.21208980679512024\n",
      "Epoch 1648, Loss: 0.46118323504924774, Final Batch Loss: 0.2419118583202362\n",
      "Epoch 1649, Loss: 0.4367504268884659, Final Batch Loss: 0.2107810378074646\n",
      "Epoch 1650, Loss: 0.4494023770093918, Final Batch Loss: 0.22808179259300232\n",
      "Epoch 1651, Loss: 0.46433742344379425, Final Batch Loss: 0.20478571951389313\n",
      "Epoch 1652, Loss: 0.45487283170223236, Final Batch Loss: 0.22636795043945312\n",
      "Epoch 1653, Loss: 0.4032638818025589, Final Batch Loss: 0.1553005427122116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1654, Loss: 0.43658657371997833, Final Batch Loss: 0.18438036739826202\n",
      "Epoch 1655, Loss: 0.4312174618244171, Final Batch Loss: 0.17364642024040222\n",
      "Epoch 1656, Loss: 0.45676475763320923, Final Batch Loss: 0.23987774550914764\n",
      "Epoch 1657, Loss: 0.46191267669200897, Final Batch Loss: 0.2025643140077591\n",
      "Epoch 1658, Loss: 0.4493096321821213, Final Batch Loss: 0.1936260610818863\n",
      "Epoch 1659, Loss: 0.41967883706092834, Final Batch Loss: 0.18781454861164093\n",
      "Epoch 1660, Loss: 0.4681684374809265, Final Batch Loss: 0.24691037833690643\n",
      "Epoch 1661, Loss: 0.3790592551231384, Final Batch Loss: 0.16080394387245178\n",
      "Epoch 1662, Loss: 0.48605599999427795, Final Batch Loss: 0.205750972032547\n",
      "Epoch 1663, Loss: 0.4695942550897598, Final Batch Loss: 0.2322758585214615\n",
      "Epoch 1664, Loss: 0.47441744804382324, Final Batch Loss: 0.2297619879245758\n",
      "Epoch 1665, Loss: 0.43823422491550446, Final Batch Loss: 0.20930832624435425\n",
      "Epoch 1666, Loss: 0.42805148661136627, Final Batch Loss: 0.22927765548229218\n",
      "Epoch 1667, Loss: 0.47467395663261414, Final Batch Loss: 0.26093676686286926\n",
      "Epoch 1668, Loss: 0.39139510691165924, Final Batch Loss: 0.19008731842041016\n",
      "Epoch 1669, Loss: 0.48677174746990204, Final Batch Loss: 0.2629196047782898\n",
      "Epoch 1670, Loss: 0.4286016374826431, Final Batch Loss: 0.20583467185497284\n",
      "Epoch 1671, Loss: 0.44819872081279755, Final Batch Loss: 0.18787609040737152\n",
      "Epoch 1672, Loss: 0.45589369535446167, Final Batch Loss: 0.17574095726013184\n",
      "Epoch 1673, Loss: 0.4310689717531204, Final Batch Loss: 0.23603956401348114\n",
      "Epoch 1674, Loss: 0.4238789975643158, Final Batch Loss: 0.21737565100193024\n",
      "Epoch 1675, Loss: 0.4295264631509781, Final Batch Loss: 0.22020070254802704\n",
      "Epoch 1676, Loss: 0.425484374165535, Final Batch Loss: 0.20810477435588837\n",
      "Epoch 1677, Loss: 0.39066411554813385, Final Batch Loss: 0.2063334733247757\n",
      "Epoch 1678, Loss: 0.4337911903858185, Final Batch Loss: 0.21791720390319824\n",
      "Epoch 1679, Loss: 0.3983418494462967, Final Batch Loss: 0.19094540178775787\n",
      "Epoch 1680, Loss: 0.3933790922164917, Final Batch Loss: 0.18147815763950348\n",
      "Epoch 1681, Loss: 0.46131761372089386, Final Batch Loss: 0.21495071053504944\n",
      "Epoch 1682, Loss: 0.3944573700428009, Final Batch Loss: 0.14324727654457092\n",
      "Epoch 1683, Loss: 0.39854638278484344, Final Batch Loss: 0.21207019686698914\n",
      "Epoch 1684, Loss: 0.40354880690574646, Final Batch Loss: 0.20200756192207336\n",
      "Epoch 1685, Loss: 0.46252477169036865, Final Batch Loss: 0.2505333125591278\n",
      "Epoch 1686, Loss: 0.45687755942344666, Final Batch Loss: 0.20403385162353516\n",
      "Epoch 1687, Loss: 0.47620053589344025, Final Batch Loss: 0.2777771055698395\n",
      "Epoch 1688, Loss: 0.45024634897708893, Final Batch Loss: 0.24289856851100922\n",
      "Epoch 1689, Loss: 0.4878070205450058, Final Batch Loss: 0.2832457721233368\n",
      "Epoch 1690, Loss: 0.4357838034629822, Final Batch Loss: 0.1745779812335968\n",
      "Epoch 1691, Loss: 0.4359671026468277, Final Batch Loss: 0.24416834115982056\n",
      "Epoch 1692, Loss: 0.4325963705778122, Final Batch Loss: 0.2347063571214676\n",
      "Epoch 1693, Loss: 0.4357374459505081, Final Batch Loss: 0.2181609570980072\n",
      "Epoch 1694, Loss: 0.4193774461746216, Final Batch Loss: 0.22451162338256836\n",
      "Epoch 1695, Loss: 0.42827314138412476, Final Batch Loss: 0.19611074030399323\n",
      "Epoch 1696, Loss: 0.3904537558555603, Final Batch Loss: 0.18575580418109894\n",
      "Epoch 1697, Loss: 0.4111558645963669, Final Batch Loss: 0.17354175448417664\n",
      "Epoch 1698, Loss: 0.4385502189397812, Final Batch Loss: 0.2350415140390396\n",
      "Epoch 1699, Loss: 0.4654851257801056, Final Batch Loss: 0.2316180169582367\n",
      "Epoch 1700, Loss: 0.3862510323524475, Final Batch Loss: 0.17469511926174164\n",
      "Epoch 1701, Loss: 0.3976319134235382, Final Batch Loss: 0.19018682837486267\n",
      "Epoch 1702, Loss: 0.3863152414560318, Final Batch Loss: 0.1884281188249588\n",
      "Epoch 1703, Loss: 0.4375219941139221, Final Batch Loss: 0.2201123684644699\n",
      "Epoch 1704, Loss: 0.4549035429954529, Final Batch Loss: 0.2643875479698181\n",
      "Epoch 1705, Loss: 0.4479881227016449, Final Batch Loss: 0.21896032989025116\n",
      "Epoch 1706, Loss: 0.41231711208820343, Final Batch Loss: 0.17988702654838562\n",
      "Epoch 1707, Loss: 0.45574356615543365, Final Batch Loss: 0.16177232563495636\n",
      "Epoch 1708, Loss: 0.42059525847435, Final Batch Loss: 0.2000977098941803\n",
      "Epoch 1709, Loss: 0.38610048592090607, Final Batch Loss: 0.17658624053001404\n",
      "Epoch 1710, Loss: 0.4168907403945923, Final Batch Loss: 0.23971757292747498\n",
      "Epoch 1711, Loss: 0.441806897521019, Final Batch Loss: 0.23544535040855408\n",
      "Epoch 1712, Loss: 0.4428883343935013, Final Batch Loss: 0.20356737077236176\n",
      "Epoch 1713, Loss: 0.46697793900966644, Final Batch Loss: 0.3046579957008362\n",
      "Epoch 1714, Loss: 0.4209234118461609, Final Batch Loss: 0.15630164742469788\n",
      "Epoch 1715, Loss: 0.40589532256126404, Final Batch Loss: 0.2148093283176422\n",
      "Epoch 1716, Loss: 0.5076686590909958, Final Batch Loss: 0.22123531997203827\n",
      "Epoch 1717, Loss: 0.44085070490837097, Final Batch Loss: 0.1952986717224121\n",
      "Epoch 1718, Loss: 0.3956421911716461, Final Batch Loss: 0.1634872704744339\n",
      "Epoch 1719, Loss: 0.48456378281116486, Final Batch Loss: 0.26967158913612366\n",
      "Epoch 1720, Loss: 0.47231943905353546, Final Batch Loss: 0.25781288743019104\n",
      "Epoch 1721, Loss: 0.49886301159858704, Final Batch Loss: 0.26010221242904663\n",
      "Epoch 1722, Loss: 0.4022706151008606, Final Batch Loss: 0.1601915955543518\n",
      "Epoch 1723, Loss: 0.41105231642723083, Final Batch Loss: 0.19695574045181274\n",
      "Epoch 1724, Loss: 0.4081351161003113, Final Batch Loss: 0.1942398101091385\n",
      "Epoch 1725, Loss: 0.4332660734653473, Final Batch Loss: 0.2116321176290512\n",
      "Epoch 1726, Loss: 0.40721262991428375, Final Batch Loss: 0.17694516479969025\n",
      "Epoch 1727, Loss: 0.4800902307033539, Final Batch Loss: 0.2691558301448822\n",
      "Epoch 1728, Loss: 0.41132165491580963, Final Batch Loss: 0.1543300300836563\n",
      "Epoch 1729, Loss: 0.4746825844049454, Final Batch Loss: 0.29165610671043396\n",
      "Epoch 1730, Loss: 0.4163891226053238, Final Batch Loss: 0.19616547226905823\n",
      "Epoch 1731, Loss: 0.4594193547964096, Final Batch Loss: 0.20510666072368622\n",
      "Epoch 1732, Loss: 0.41047315299510956, Final Batch Loss: 0.22685527801513672\n",
      "Epoch 1733, Loss: 0.4137981981039047, Final Batch Loss: 0.17359738051891327\n",
      "Epoch 1734, Loss: 0.4629739075899124, Final Batch Loss: 0.24046359956264496\n",
      "Epoch 1735, Loss: 0.4106895327568054, Final Batch Loss: 0.16460444033145905\n",
      "Epoch 1736, Loss: 0.42405934631824493, Final Batch Loss: 0.16470958292484283\n",
      "Epoch 1737, Loss: 0.4024171084165573, Final Batch Loss: 0.20557700097560883\n",
      "Epoch 1738, Loss: 0.5708782523870468, Final Batch Loss: 0.38727009296417236\n",
      "Epoch 1739, Loss: 0.3901986628770828, Final Batch Loss: 0.17540636658668518\n",
      "Epoch 1740, Loss: 0.47017620503902435, Final Batch Loss: 0.27078402042388916\n",
      "Epoch 1741, Loss: 0.4234820753335953, Final Batch Loss: 0.20751626789569855\n",
      "Epoch 1742, Loss: 0.42140039801597595, Final Batch Loss: 0.18487326800823212\n",
      "Epoch 1743, Loss: 0.44279614090919495, Final Batch Loss: 0.2380094677209854\n",
      "Epoch 1744, Loss: 0.4187091141939163, Final Batch Loss: 0.200098916888237\n",
      "Epoch 1745, Loss: 0.47736993432044983, Final Batch Loss: 0.245794415473938\n",
      "Epoch 1746, Loss: 0.46294350922107697, Final Batch Loss: 0.2431393265724182\n",
      "Epoch 1747, Loss: 0.4065452665090561, Final Batch Loss: 0.17017249763011932\n",
      "Epoch 1748, Loss: 0.4264792650938034, Final Batch Loss: 0.22795717418193817\n",
      "Epoch 1749, Loss: 0.41081516444683075, Final Batch Loss: 0.19856101274490356\n",
      "Epoch 1750, Loss: 0.38622745871543884, Final Batch Loss: 0.18856535851955414\n",
      "Epoch 1751, Loss: 0.37323781847953796, Final Batch Loss: 0.20186704397201538\n",
      "Epoch 1752, Loss: 0.43179234862327576, Final Batch Loss: 0.18430064618587494\n",
      "Epoch 1753, Loss: 0.41338208317756653, Final Batch Loss: 0.1972983330488205\n",
      "Epoch 1754, Loss: 0.4057401269674301, Final Batch Loss: 0.223425030708313\n",
      "Epoch 1755, Loss: 0.41122619807720184, Final Batch Loss: 0.22584328055381775\n",
      "Epoch 1756, Loss: 0.4631453603506088, Final Batch Loss: 0.1761332005262375\n",
      "Epoch 1757, Loss: 0.412842258810997, Final Batch Loss: 0.24436405301094055\n",
      "Epoch 1758, Loss: 0.438425675034523, Final Batch Loss: 0.1794111579656601\n",
      "Epoch 1759, Loss: 0.4812629222869873, Final Batch Loss: 0.24664247035980225\n",
      "Epoch 1760, Loss: 0.4275146722793579, Final Batch Loss: 0.21978874504566193\n",
      "Epoch 1761, Loss: 0.4140862077474594, Final Batch Loss: 0.19908548891544342\n",
      "Epoch 1762, Loss: 0.39900967478752136, Final Batch Loss: 0.17749197781085968\n",
      "Epoch 1763, Loss: 0.3908233195543289, Final Batch Loss: 0.21490003168582916\n",
      "Epoch 1764, Loss: 0.4619915634393692, Final Batch Loss: 0.25448310375213623\n",
      "Epoch 1765, Loss: 0.4654874950647354, Final Batch Loss: 0.26405829191207886\n",
      "Epoch 1766, Loss: 0.41518068313598633, Final Batch Loss: 0.231583833694458\n",
      "Epoch 1767, Loss: 0.39366069436073303, Final Batch Loss: 0.1967065930366516\n",
      "Epoch 1768, Loss: 0.4275081753730774, Final Batch Loss: 0.18886809051036835\n",
      "Epoch 1769, Loss: 0.4107412248849869, Final Batch Loss: 0.1773025244474411\n",
      "Epoch 1770, Loss: 0.44219520688056946, Final Batch Loss: 0.20910826325416565\n",
      "Epoch 1771, Loss: 0.39681294560432434, Final Batch Loss: 0.1544526368379593\n",
      "Epoch 1772, Loss: 0.43346618115901947, Final Batch Loss: 0.1997365802526474\n",
      "Epoch 1773, Loss: 0.4109541028738022, Final Batch Loss: 0.21683284640312195\n",
      "Epoch 1774, Loss: 0.517909586429596, Final Batch Loss: 0.3206189274787903\n",
      "Epoch 1775, Loss: 0.44357143342494965, Final Batch Loss: 0.19977454841136932\n",
      "Epoch 1776, Loss: 0.40440836548805237, Final Batch Loss: 0.2260759323835373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1777, Loss: 0.4238532930612564, Final Batch Loss: 0.22991281747817993\n",
      "Epoch 1778, Loss: 0.42143702507019043, Final Batch Loss: 0.2353099137544632\n",
      "Epoch 1779, Loss: 0.44081243872642517, Final Batch Loss: 0.1522178053855896\n",
      "Epoch 1780, Loss: 0.42771418392658234, Final Batch Loss: 0.2593457102775574\n",
      "Epoch 1781, Loss: 0.4346282631158829, Final Batch Loss: 0.17142222821712494\n",
      "Epoch 1782, Loss: 0.45045986771583557, Final Batch Loss: 0.22663329541683197\n",
      "Epoch 1783, Loss: 0.42264871299266815, Final Batch Loss: 0.18140897154808044\n",
      "Epoch 1784, Loss: 0.472665473818779, Final Batch Loss: 0.2126946896314621\n",
      "Epoch 1785, Loss: 0.45161911845207214, Final Batch Loss: 0.2277553379535675\n",
      "Epoch 1786, Loss: 0.38838355243206024, Final Batch Loss: 0.1862972378730774\n",
      "Epoch 1787, Loss: 0.40321142971515656, Final Batch Loss: 0.18635736405849457\n",
      "Epoch 1788, Loss: 0.42682093381881714, Final Batch Loss: 0.2211254984140396\n",
      "Epoch 1789, Loss: 0.3672574311494827, Final Batch Loss: 0.1726178377866745\n",
      "Epoch 1790, Loss: 0.3735096603631973, Final Batch Loss: 0.1789446622133255\n",
      "Epoch 1791, Loss: 0.4586346298456192, Final Batch Loss: 0.21440936625003815\n",
      "Epoch 1792, Loss: 0.4147030711174011, Final Batch Loss: 0.20969776809215546\n",
      "Epoch 1793, Loss: 0.4572359621524811, Final Batch Loss: 0.2370389848947525\n",
      "Epoch 1794, Loss: 0.4305576980113983, Final Batch Loss: 0.2483118325471878\n",
      "Epoch 1795, Loss: 0.41090303659439087, Final Batch Loss: 0.16408482193946838\n",
      "Epoch 1796, Loss: 0.49360017478466034, Final Batch Loss: 0.27683964371681213\n",
      "Epoch 1797, Loss: 0.4814060777425766, Final Batch Loss: 0.24422158300876617\n",
      "Epoch 1798, Loss: 0.40339405834674835, Final Batch Loss: 0.15962503850460052\n",
      "Epoch 1799, Loss: 0.4386149048805237, Final Batch Loss: 0.22215652465820312\n",
      "Epoch 1800, Loss: 0.38606731593608856, Final Batch Loss: 0.18993224203586578\n",
      "Epoch 1801, Loss: 0.4136095643043518, Final Batch Loss: 0.20369845628738403\n",
      "Epoch 1802, Loss: 0.411466047167778, Final Batch Loss: 0.2057121992111206\n",
      "Epoch 1803, Loss: 0.4623226523399353, Final Batch Loss: 0.18571600317955017\n",
      "Epoch 1804, Loss: 0.39126192033290863, Final Batch Loss: 0.19147829711437225\n",
      "Epoch 1805, Loss: 0.3852110207080841, Final Batch Loss: 0.1451142579317093\n",
      "Epoch 1806, Loss: 0.4011925756931305, Final Batch Loss: 0.1812683492898941\n",
      "Epoch 1807, Loss: 0.40322159230709076, Final Batch Loss: 0.19975480437278748\n",
      "Epoch 1808, Loss: 0.4154457151889801, Final Batch Loss: 0.21820800006389618\n",
      "Epoch 1809, Loss: 0.4469136893749237, Final Batch Loss: 0.22859059274196625\n",
      "Epoch 1810, Loss: 0.3906058520078659, Final Batch Loss: 0.17489637434482574\n",
      "Epoch 1811, Loss: 0.3958243131637573, Final Batch Loss: 0.20246176421642303\n",
      "Epoch 1812, Loss: 0.40462785959243774, Final Batch Loss: 0.18778449296951294\n",
      "Epoch 1813, Loss: 0.4130164086818695, Final Batch Loss: 0.21554887294769287\n",
      "Epoch 1814, Loss: 0.3759075403213501, Final Batch Loss: 0.20290330052375793\n",
      "Epoch 1815, Loss: 0.43035997450351715, Final Batch Loss: 0.1939060389995575\n",
      "Epoch 1816, Loss: 0.4200062155723572, Final Batch Loss: 0.19701920449733734\n",
      "Epoch 1817, Loss: 0.41069909930229187, Final Batch Loss: 0.20199958980083466\n",
      "Epoch 1818, Loss: 0.40535399317741394, Final Batch Loss: 0.2346489578485489\n",
      "Epoch 1819, Loss: 0.43476034700870514, Final Batch Loss: 0.202024906873703\n",
      "Epoch 1820, Loss: 0.46542370319366455, Final Batch Loss: 0.2526281774044037\n",
      "Epoch 1821, Loss: 0.45409829914569855, Final Batch Loss: 0.24546022713184357\n",
      "Epoch 1822, Loss: 0.4036208838224411, Final Batch Loss: 0.1960451751947403\n",
      "Epoch 1823, Loss: 0.3697597235441208, Final Batch Loss: 0.1558769792318344\n",
      "Epoch 1824, Loss: 0.38774894177913666, Final Batch Loss: 0.22094129025936127\n",
      "Epoch 1825, Loss: 0.4741130769252777, Final Batch Loss: 0.27825698256492615\n",
      "Epoch 1826, Loss: 0.3764377534389496, Final Batch Loss: 0.1711786389350891\n",
      "Epoch 1827, Loss: 0.4142977297306061, Final Batch Loss: 0.2022029310464859\n",
      "Epoch 1828, Loss: 0.44230233132839203, Final Batch Loss: 0.19968529045581818\n",
      "Epoch 1829, Loss: 0.4016057401895523, Final Batch Loss: 0.2054385095834732\n",
      "Epoch 1830, Loss: 0.4689970910549164, Final Batch Loss: 0.2562299370765686\n",
      "Epoch 1831, Loss: 0.40865279734134674, Final Batch Loss: 0.198306143283844\n",
      "Epoch 1832, Loss: 0.3999786972999573, Final Batch Loss: 0.17118415236473083\n",
      "Epoch 1833, Loss: 0.44149790704250336, Final Batch Loss: 0.19755743443965912\n",
      "Epoch 1834, Loss: 0.45619307458400726, Final Batch Loss: 0.22566449642181396\n",
      "Epoch 1835, Loss: 0.3684423863887787, Final Batch Loss: 0.1705719530582428\n",
      "Epoch 1836, Loss: 0.44002842903137207, Final Batch Loss: 0.2319631278514862\n",
      "Epoch 1837, Loss: 0.44839631021022797, Final Batch Loss: 0.18775077164173126\n",
      "Epoch 1838, Loss: 0.47070951759815216, Final Batch Loss: 0.26292797923088074\n",
      "Epoch 1839, Loss: 0.4191098064184189, Final Batch Loss: 0.21848317980766296\n",
      "Epoch 1840, Loss: 0.42750731110572815, Final Batch Loss: 0.19468718767166138\n",
      "Epoch 1841, Loss: 0.42072610557079315, Final Batch Loss: 0.21432487666606903\n",
      "Epoch 1842, Loss: 0.3961953818798065, Final Batch Loss: 0.20024625957012177\n",
      "Epoch 1843, Loss: 0.42477940022945404, Final Batch Loss: 0.2288135588169098\n",
      "Epoch 1844, Loss: 0.3691946566104889, Final Batch Loss: 0.18095552921295166\n",
      "Epoch 1845, Loss: 0.5627739876508713, Final Batch Loss: 0.34110334515571594\n",
      "Epoch 1846, Loss: 0.44007980823516846, Final Batch Loss: 0.2676449120044708\n",
      "Epoch 1847, Loss: 0.47898875176906586, Final Batch Loss: 0.23702950775623322\n",
      "Epoch 1848, Loss: 0.44211409986019135, Final Batch Loss: 0.22067005932331085\n",
      "Epoch 1849, Loss: 0.42677827179431915, Final Batch Loss: 0.21653899550437927\n",
      "Epoch 1850, Loss: 0.388203963637352, Final Batch Loss: 0.1825934499502182\n",
      "Epoch 1851, Loss: 0.40239232778549194, Final Batch Loss: 0.23054540157318115\n",
      "Epoch 1852, Loss: 0.37831270694732666, Final Batch Loss: 0.18251465260982513\n",
      "Epoch 1853, Loss: 0.40596865117549896, Final Batch Loss: 0.21365101635456085\n",
      "Epoch 1854, Loss: 0.40611037611961365, Final Batch Loss: 0.23269422352313995\n",
      "Epoch 1855, Loss: 0.41750368475914, Final Batch Loss: 0.22611522674560547\n",
      "Epoch 1856, Loss: 0.4318285435438156, Final Batch Loss: 0.2405264973640442\n",
      "Epoch 1857, Loss: 0.41628308594226837, Final Batch Loss: 0.2397642880678177\n",
      "Epoch 1858, Loss: 0.4517499804496765, Final Batch Loss: 0.21459779143333435\n",
      "Epoch 1859, Loss: 0.4641226977109909, Final Batch Loss: 0.22371935844421387\n",
      "Epoch 1860, Loss: 0.41718678176403046, Final Batch Loss: 0.21029847860336304\n",
      "Epoch 1861, Loss: 0.4272844046354294, Final Batch Loss: 0.21966518461704254\n",
      "Epoch 1862, Loss: 0.387698158621788, Final Batch Loss: 0.17341580986976624\n",
      "Epoch 1863, Loss: 0.4263942092657089, Final Batch Loss: 0.24249684810638428\n",
      "Epoch 1864, Loss: 0.4146074652671814, Final Batch Loss: 0.1834380030632019\n",
      "Epoch 1865, Loss: 0.4263010323047638, Final Batch Loss: 0.21162815392017365\n",
      "Epoch 1866, Loss: 0.3526616245508194, Final Batch Loss: 0.17485184967517853\n",
      "Epoch 1867, Loss: 0.4036198705434799, Final Batch Loss: 0.208232119679451\n",
      "Epoch 1868, Loss: 0.4751560091972351, Final Batch Loss: 0.27129966020584106\n",
      "Epoch 1869, Loss: 0.43205344676971436, Final Batch Loss: 0.23853257298469543\n",
      "Epoch 1870, Loss: 0.4627281427383423, Final Batch Loss: 0.2576514184474945\n",
      "Epoch 1871, Loss: 0.42314428091049194, Final Batch Loss: 0.21194873750209808\n",
      "Epoch 1872, Loss: 0.44201208651065826, Final Batch Loss: 0.20126467943191528\n",
      "Epoch 1873, Loss: 0.4297816902399063, Final Batch Loss: 0.18280158936977386\n",
      "Epoch 1874, Loss: 0.4484405964612961, Final Batch Loss: 0.24409723281860352\n",
      "Epoch 1875, Loss: 0.36345407366752625, Final Batch Loss: 0.16845989227294922\n",
      "Epoch 1876, Loss: 0.4930083751678467, Final Batch Loss: 0.23994219303131104\n",
      "Epoch 1877, Loss: 0.3964201658964157, Final Batch Loss: 0.20871883630752563\n",
      "Epoch 1878, Loss: 0.3897341936826706, Final Batch Loss: 0.17737118899822235\n",
      "Epoch 1879, Loss: 0.37337012588977814, Final Batch Loss: 0.16789843142032623\n",
      "Epoch 1880, Loss: 0.4245479255914688, Final Batch Loss: 0.22685296833515167\n",
      "Epoch 1881, Loss: 0.43252629041671753, Final Batch Loss: 0.23432235419750214\n",
      "Epoch 1882, Loss: 0.4556550532579422, Final Batch Loss: 0.24307075142860413\n",
      "Epoch 1883, Loss: 0.4027905911207199, Final Batch Loss: 0.20485413074493408\n",
      "Epoch 1884, Loss: 0.39095039665699005, Final Batch Loss: 0.17715883255004883\n",
      "Epoch 1885, Loss: 0.40589022636413574, Final Batch Loss: 0.19046197831630707\n",
      "Epoch 1886, Loss: 0.3820594698190689, Final Batch Loss: 0.20295363664627075\n",
      "Epoch 1887, Loss: 0.39003077149391174, Final Batch Loss: 0.15250533819198608\n",
      "Epoch 1888, Loss: 0.4098537415266037, Final Batch Loss: 0.2315448820590973\n",
      "Epoch 1889, Loss: 0.5221937000751495, Final Batch Loss: 0.26073846220970154\n",
      "Epoch 1890, Loss: 0.3649648576974869, Final Batch Loss: 0.14921724796295166\n",
      "Epoch 1891, Loss: 0.37486933171749115, Final Batch Loss: 0.19904236495494843\n",
      "Epoch 1892, Loss: 0.36262066662311554, Final Batch Loss: 0.1869739145040512\n",
      "Epoch 1893, Loss: 0.36979541182518005, Final Batch Loss: 0.20167672634124756\n",
      "Epoch 1894, Loss: 0.4724271893501282, Final Batch Loss: 0.2191586196422577\n",
      "Epoch 1895, Loss: 0.37535472214221954, Final Batch Loss: 0.23113730549812317\n",
      "Epoch 1896, Loss: 0.45660264790058136, Final Batch Loss: 0.21604959666728973\n",
      "Epoch 1897, Loss: 0.3590371310710907, Final Batch Loss: 0.1753217726945877\n",
      "Epoch 1898, Loss: 0.3440226763486862, Final Batch Loss: 0.1519981026649475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1899, Loss: 0.3912908434867859, Final Batch Loss: 0.19874005019664764\n",
      "Epoch 1900, Loss: 0.3895137906074524, Final Batch Loss: 0.16005849838256836\n",
      "Epoch 1901, Loss: 0.3485911935567856, Final Batch Loss: 0.1403319090604782\n",
      "Epoch 1902, Loss: 0.3496548682451248, Final Batch Loss: 0.14624319970607758\n",
      "Epoch 1903, Loss: 0.3995978534221649, Final Batch Loss: 0.18578672409057617\n",
      "Epoch 1904, Loss: 0.4082953929901123, Final Batch Loss: 0.19817300140857697\n",
      "Epoch 1905, Loss: 0.34795787930488586, Final Batch Loss: 0.14029821753501892\n",
      "Epoch 1906, Loss: 0.46616126596927643, Final Batch Loss: 0.28871554136276245\n",
      "Epoch 1907, Loss: 0.4566420912742615, Final Batch Loss: 0.2234673798084259\n",
      "Epoch 1908, Loss: 0.3353662043809891, Final Batch Loss: 0.1300174444913864\n",
      "Epoch 1909, Loss: 0.36539046466350555, Final Batch Loss: 0.1821843683719635\n",
      "Epoch 1910, Loss: 0.3804817348718643, Final Batch Loss: 0.18246892094612122\n",
      "Epoch 1911, Loss: 0.39459389448165894, Final Batch Loss: 0.20456621050834656\n",
      "Epoch 1912, Loss: 0.3677535206079483, Final Batch Loss: 0.1543254256248474\n",
      "Epoch 1913, Loss: 0.3808474838733673, Final Batch Loss: 0.19061699509620667\n",
      "Epoch 1914, Loss: 0.41896122694015503, Final Batch Loss: 0.20319099724292755\n",
      "Epoch 1915, Loss: 0.46885867416858673, Final Batch Loss: 0.22759200632572174\n",
      "Epoch 1916, Loss: 0.4996589422225952, Final Batch Loss: 0.2664299011230469\n",
      "Epoch 1917, Loss: 0.4537164270877838, Final Batch Loss: 0.22916646301746368\n",
      "Epoch 1918, Loss: 0.38609813153743744, Final Batch Loss: 0.19321216642856598\n",
      "Epoch 1919, Loss: 0.4280974417924881, Final Batch Loss: 0.23416028916835785\n",
      "Epoch 1920, Loss: 0.3833758980035782, Final Batch Loss: 0.19614389538764954\n",
      "Epoch 1921, Loss: 0.44074150919914246, Final Batch Loss: 0.22211067378520966\n",
      "Epoch 1922, Loss: 0.39753173291683197, Final Batch Loss: 0.207368865609169\n",
      "Epoch 1923, Loss: 0.4278261512517929, Final Batch Loss: 0.23762263357639313\n",
      "Epoch 1924, Loss: 0.3739110231399536, Final Batch Loss: 0.15259893238544464\n",
      "Epoch 1925, Loss: 0.39723920822143555, Final Batch Loss: 0.17760159075260162\n",
      "Epoch 1926, Loss: 0.38009145855903625, Final Batch Loss: 0.19237078726291656\n",
      "Epoch 1927, Loss: 0.4115588665008545, Final Batch Loss: 0.22054646909236908\n",
      "Epoch 1928, Loss: 0.42113570868968964, Final Batch Loss: 0.18199372291564941\n",
      "Epoch 1929, Loss: 0.38026295602321625, Final Batch Loss: 0.15731079876422882\n",
      "Epoch 1930, Loss: 0.4038507789373398, Final Batch Loss: 0.15226884186267853\n",
      "Epoch 1931, Loss: 0.4163948893547058, Final Batch Loss: 0.2278262823820114\n",
      "Epoch 1932, Loss: 0.39740563929080963, Final Batch Loss: 0.2115834355354309\n",
      "Epoch 1933, Loss: 0.4023914635181427, Final Batch Loss: 0.1725080907344818\n",
      "Epoch 1934, Loss: 0.4291255623102188, Final Batch Loss: 0.2229379415512085\n",
      "Epoch 1935, Loss: 0.35507962107658386, Final Batch Loss: 0.16279850900173187\n",
      "Epoch 1936, Loss: 0.37661823630332947, Final Batch Loss: 0.18360048532485962\n",
      "Epoch 1937, Loss: 0.39636722207069397, Final Batch Loss: 0.19390869140625\n",
      "Epoch 1938, Loss: 0.3558489978313446, Final Batch Loss: 0.1597909927368164\n",
      "Epoch 1939, Loss: 0.41595035791397095, Final Batch Loss: 0.20310166478157043\n",
      "Epoch 1940, Loss: 0.4780055582523346, Final Batch Loss: 0.28572139143943787\n",
      "Epoch 1941, Loss: 0.35991670191287994, Final Batch Loss: 0.1364869475364685\n",
      "Epoch 1942, Loss: 0.3856095224618912, Final Batch Loss: 0.18949590623378754\n",
      "Epoch 1943, Loss: 0.3775731027126312, Final Batch Loss: 0.1952931433916092\n",
      "Epoch 1944, Loss: 0.3670927882194519, Final Batch Loss: 0.17218017578125\n",
      "Epoch 1945, Loss: 0.35134220123291016, Final Batch Loss: 0.17254889011383057\n",
      "Epoch 1946, Loss: 0.4121326059103012, Final Batch Loss: 0.21672634780406952\n",
      "Epoch 1947, Loss: 0.4273148477077484, Final Batch Loss: 0.216404989361763\n",
      "Epoch 1948, Loss: 0.42635130882263184, Final Batch Loss: 0.20683147013187408\n",
      "Epoch 1949, Loss: 0.3590007722377777, Final Batch Loss: 0.16658948361873627\n",
      "Epoch 1950, Loss: 0.42580270767211914, Final Batch Loss: 0.23044201731681824\n",
      "Epoch 1951, Loss: 0.4653501957654953, Final Batch Loss: 0.2363959401845932\n",
      "Epoch 1952, Loss: 0.3863769620656967, Final Batch Loss: 0.18606753647327423\n",
      "Epoch 1953, Loss: 0.39274071156978607, Final Batch Loss: 0.1752922534942627\n",
      "Epoch 1954, Loss: 0.4193068593740463, Final Batch Loss: 0.25462064146995544\n",
      "Epoch 1955, Loss: 0.4101816266775131, Final Batch Loss: 0.17059379816055298\n",
      "Epoch 1956, Loss: 0.4609505832195282, Final Batch Loss: 0.20622038841247559\n",
      "Epoch 1957, Loss: 0.3933793306350708, Final Batch Loss: 0.18423672020435333\n",
      "Epoch 1958, Loss: 0.41248616576194763, Final Batch Loss: 0.21195663511753082\n",
      "Epoch 1959, Loss: 0.37146328389644623, Final Batch Loss: 0.1267847865819931\n",
      "Epoch 1960, Loss: 0.37918712198734283, Final Batch Loss: 0.13535906374454498\n",
      "Epoch 1961, Loss: 0.42726172506809235, Final Batch Loss: 0.2078714668750763\n",
      "Epoch 1962, Loss: 0.43592964112758636, Final Batch Loss: 0.22682875394821167\n",
      "Epoch 1963, Loss: 0.4865942746400833, Final Batch Loss: 0.28076255321502686\n",
      "Epoch 1964, Loss: 0.36931464076042175, Final Batch Loss: 0.1700562834739685\n",
      "Epoch 1965, Loss: 0.4621612876653671, Final Batch Loss: 0.26134395599365234\n",
      "Epoch 1966, Loss: 0.40906694531440735, Final Batch Loss: 0.22491098940372467\n",
      "Epoch 1967, Loss: 0.41203898191452026, Final Batch Loss: 0.19616082310676575\n",
      "Epoch 1968, Loss: 0.4470887780189514, Final Batch Loss: 0.210816890001297\n",
      "Epoch 1969, Loss: 0.3931850343942642, Final Batch Loss: 0.17633056640625\n",
      "Epoch 1970, Loss: 0.38402897119522095, Final Batch Loss: 0.16820229589939117\n",
      "Epoch 1971, Loss: 0.3833793103694916, Final Batch Loss: 0.19389639794826508\n",
      "Epoch 1972, Loss: 0.3751258850097656, Final Batch Loss: 0.1714709848165512\n",
      "Epoch 1973, Loss: 0.3886670768260956, Final Batch Loss: 0.2027294933795929\n",
      "Epoch 1974, Loss: 0.4258466958999634, Final Batch Loss: 0.2007075697183609\n",
      "Epoch 1975, Loss: 0.4028977155685425, Final Batch Loss: 0.20478519797325134\n",
      "Epoch 1976, Loss: 0.36605218052864075, Final Batch Loss: 0.17828206717967987\n",
      "Epoch 1977, Loss: 0.4137157052755356, Final Batch Loss: 0.2055169641971588\n",
      "Epoch 1978, Loss: 0.36198949813842773, Final Batch Loss: 0.18651384115219116\n",
      "Epoch 1979, Loss: 0.4162571132183075, Final Batch Loss: 0.18535572290420532\n",
      "Epoch 1980, Loss: 0.38895347714424133, Final Batch Loss: 0.1957852691411972\n",
      "Epoch 1981, Loss: 0.39959149062633514, Final Batch Loss: 0.18902935087680817\n",
      "Epoch 1982, Loss: 0.4320579767227173, Final Batch Loss: 0.2334662228822708\n",
      "Epoch 1983, Loss: 0.3899856507778168, Final Batch Loss: 0.19822517037391663\n",
      "Epoch 1984, Loss: 0.4335778206586838, Final Batch Loss: 0.22022397816181183\n",
      "Epoch 1985, Loss: 0.4244622737169266, Final Batch Loss: 0.24862177670001984\n",
      "Epoch 1986, Loss: 0.40655314922332764, Final Batch Loss: 0.1990182250738144\n",
      "Epoch 1987, Loss: 0.384516179561615, Final Batch Loss: 0.2002183496952057\n",
      "Epoch 1988, Loss: 0.37618501484394073, Final Batch Loss: 0.1688629388809204\n",
      "Epoch 1989, Loss: 0.45750708878040314, Final Batch Loss: 0.24951374530792236\n",
      "Epoch 1990, Loss: 0.401596263051033, Final Batch Loss: 0.21752236783504486\n",
      "Epoch 1991, Loss: 0.431942418217659, Final Batch Loss: 0.2290707528591156\n",
      "Epoch 1992, Loss: 0.41385868191719055, Final Batch Loss: 0.18611295521259308\n",
      "Epoch 1993, Loss: 0.37558817863464355, Final Batch Loss: 0.18017061054706573\n",
      "Epoch 1994, Loss: 0.39177122712135315, Final Batch Loss: 0.19866943359375\n",
      "Epoch 1995, Loss: 0.3500513583421707, Final Batch Loss: 0.16797877848148346\n",
      "Epoch 1996, Loss: 0.4425520598888397, Final Batch Loss: 0.22163614630699158\n",
      "Epoch 1997, Loss: 0.4098706692457199, Final Batch Loss: 0.23248784244060516\n",
      "Epoch 1998, Loss: 0.44484272599220276, Final Batch Loss: 0.28470301628112793\n",
      "Epoch 1999, Loss: 0.4072767049074173, Final Batch Loss: 0.21002963185310364\n",
      "Epoch 2000, Loss: 0.4124383330345154, Final Batch Loss: 0.22411878407001495\n",
      "Epoch 2001, Loss: 0.40148472785949707, Final Batch Loss: 0.21478669345378876\n",
      "Epoch 2002, Loss: 0.3988031595945358, Final Batch Loss: 0.19957669079303741\n",
      "Epoch 2003, Loss: 0.35148778557777405, Final Batch Loss: 0.16004584729671478\n",
      "Epoch 2004, Loss: 0.41069750487804413, Final Batch Loss: 0.21574056148529053\n",
      "Epoch 2005, Loss: 0.3938865065574646, Final Batch Loss: 0.19172562658786774\n",
      "Epoch 2006, Loss: 0.42097175121307373, Final Batch Loss: 0.21333076059818268\n",
      "Epoch 2007, Loss: 0.4410349875688553, Final Batch Loss: 0.20559261739253998\n",
      "Epoch 2008, Loss: 0.386276513338089, Final Batch Loss: 0.19229789078235626\n",
      "Epoch 2009, Loss: 0.3833184838294983, Final Batch Loss: 0.20224998891353607\n",
      "Epoch 2010, Loss: 0.38869720697402954, Final Batch Loss: 0.186982199549675\n",
      "Epoch 2011, Loss: 0.39016926288604736, Final Batch Loss: 0.1923186331987381\n",
      "Epoch 2012, Loss: 0.38702189922332764, Final Batch Loss: 0.17700116336345673\n",
      "Epoch 2013, Loss: 0.3794657588005066, Final Batch Loss: 0.22506830096244812\n",
      "Epoch 2014, Loss: 0.354735866189003, Final Batch Loss: 0.1502654105424881\n",
      "Epoch 2015, Loss: 0.42400315403938293, Final Batch Loss: 0.23671284317970276\n",
      "Epoch 2016, Loss: 0.3731585294008255, Final Batch Loss: 0.18472537398338318\n",
      "Epoch 2017, Loss: 0.5737525522708893, Final Batch Loss: 0.2580360472202301\n",
      "Epoch 2018, Loss: 0.35331013798713684, Final Batch Loss: 0.1776985377073288\n",
      "Epoch 2019, Loss: 0.3754519373178482, Final Batch Loss: 0.19620592892169952\n",
      "Epoch 2020, Loss: 0.5196338295936584, Final Batch Loss: 0.3220147490501404\n",
      "Epoch 2021, Loss: 0.4121319055557251, Final Batch Loss: 0.19850164651870728\n",
      "Epoch 2022, Loss: 0.3893421143293381, Final Batch Loss: 0.1967906653881073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2023, Loss: 0.4278320074081421, Final Batch Loss: 0.19659161567687988\n",
      "Epoch 2024, Loss: 0.38751986622810364, Final Batch Loss: 0.184788778424263\n",
      "Epoch 2025, Loss: 0.3546747863292694, Final Batch Loss: 0.16114917397499084\n",
      "Epoch 2026, Loss: 0.3628826141357422, Final Batch Loss: 0.15185175836086273\n",
      "Epoch 2027, Loss: 0.3784022778272629, Final Batch Loss: 0.1753782033920288\n",
      "Epoch 2028, Loss: 0.35952869057655334, Final Batch Loss: 0.18613767623901367\n",
      "Epoch 2029, Loss: 0.3796234279870987, Final Batch Loss: 0.19313837587833405\n",
      "Epoch 2030, Loss: 0.40151649713516235, Final Batch Loss: 0.20081281661987305\n",
      "Epoch 2031, Loss: 0.4161186069250107, Final Batch Loss: 0.21303151547908783\n",
      "Epoch 2032, Loss: 0.36153358221054077, Final Batch Loss: 0.1502547562122345\n",
      "Epoch 2033, Loss: 0.42659173905849457, Final Batch Loss: 0.2096138894557953\n",
      "Epoch 2034, Loss: 0.37236298620700836, Final Batch Loss: 0.1908421814441681\n",
      "Epoch 2035, Loss: 0.40971411764621735, Final Batch Loss: 0.24179843068122864\n",
      "Epoch 2036, Loss: 0.4472209960222244, Final Batch Loss: 0.25778672099113464\n",
      "Epoch 2037, Loss: 0.44140511751174927, Final Batch Loss: 0.2145998179912567\n",
      "Epoch 2038, Loss: 0.3597830832004547, Final Batch Loss: 0.20767228305339813\n",
      "Epoch 2039, Loss: 0.37735024094581604, Final Batch Loss: 0.17305171489715576\n",
      "Epoch 2040, Loss: 0.39800454676151276, Final Batch Loss: 0.19032837450504303\n",
      "Epoch 2041, Loss: 0.4142792224884033, Final Batch Loss: 0.2247045934200287\n",
      "Epoch 2042, Loss: 0.3762361705303192, Final Batch Loss: 0.20344766974449158\n",
      "Epoch 2043, Loss: 0.4250672906637192, Final Batch Loss: 0.23925165832042694\n",
      "Epoch 2044, Loss: 0.3997681438922882, Final Batch Loss: 0.201407328248024\n",
      "Epoch 2045, Loss: 0.32511673867702484, Final Batch Loss: 0.1493934988975525\n",
      "Epoch 2046, Loss: 0.3854125589132309, Final Batch Loss: 0.1809418797492981\n",
      "Epoch 2047, Loss: 0.43080222606658936, Final Batch Loss: 0.2551884949207306\n",
      "Epoch 2048, Loss: 0.3949332982301712, Final Batch Loss: 0.19931833446025848\n",
      "Epoch 2049, Loss: 0.40531234443187714, Final Batch Loss: 0.19847159087657928\n",
      "Epoch 2050, Loss: 0.4447220712900162, Final Batch Loss: 0.24941368401050568\n",
      "Epoch 2051, Loss: 0.39368514716625214, Final Batch Loss: 0.22267965972423553\n",
      "Epoch 2052, Loss: 0.3957911729812622, Final Batch Loss: 0.21932587027549744\n",
      "Epoch 2053, Loss: 0.4320050776004791, Final Batch Loss: 0.21157924830913544\n",
      "Epoch 2054, Loss: 0.39379602670669556, Final Batch Loss: 0.17998264729976654\n",
      "Epoch 2055, Loss: 0.41442684829235077, Final Batch Loss: 0.2139115333557129\n",
      "Epoch 2056, Loss: 0.3826923817396164, Final Batch Loss: 0.1619432270526886\n",
      "Epoch 2057, Loss: 0.3589727580547333, Final Batch Loss: 0.19608864188194275\n",
      "Epoch 2058, Loss: 0.3679010570049286, Final Batch Loss: 0.18070366978645325\n",
      "Epoch 2059, Loss: 0.4674096554517746, Final Batch Loss: 0.1953096240758896\n",
      "Epoch 2060, Loss: 0.3779361844062805, Final Batch Loss: 0.17303800582885742\n",
      "Epoch 2061, Loss: 0.3656264990568161, Final Batch Loss: 0.1530766487121582\n",
      "Epoch 2062, Loss: 0.37712380290031433, Final Batch Loss: 0.18109475076198578\n",
      "Epoch 2063, Loss: 0.40398798882961273, Final Batch Loss: 0.17564792931079865\n",
      "Epoch 2064, Loss: 0.39548635482788086, Final Batch Loss: 0.1737745702266693\n",
      "Epoch 2065, Loss: 0.3712501674890518, Final Batch Loss: 0.20021918416023254\n",
      "Epoch 2066, Loss: 0.3913982957601547, Final Batch Loss: 0.21127425134181976\n",
      "Epoch 2067, Loss: 0.3971162587404251, Final Batch Loss: 0.23139187693595886\n",
      "Epoch 2068, Loss: 0.32759664952754974, Final Batch Loss: 0.16205470263957977\n",
      "Epoch 2069, Loss: 0.36644741892814636, Final Batch Loss: 0.18228794634342194\n",
      "Epoch 2070, Loss: 0.4242265969514847, Final Batch Loss: 0.24855923652648926\n",
      "Epoch 2071, Loss: 0.39927273988723755, Final Batch Loss: 0.22491896152496338\n",
      "Epoch 2072, Loss: 0.33479398488998413, Final Batch Loss: 0.15879513323307037\n",
      "Epoch 2073, Loss: 0.38541147112846375, Final Batch Loss: 0.20107002556324005\n",
      "Epoch 2074, Loss: 0.3836749643087387, Final Batch Loss: 0.22714237868785858\n",
      "Epoch 2075, Loss: 0.3915723115205765, Final Batch Loss: 0.2414667308330536\n",
      "Epoch 2076, Loss: 0.37087109684944153, Final Batch Loss: 0.18088525533676147\n",
      "Epoch 2077, Loss: 0.4517115503549576, Final Batch Loss: 0.27817943692207336\n",
      "Epoch 2078, Loss: 0.3688598871231079, Final Batch Loss: 0.14312608540058136\n",
      "Epoch 2079, Loss: 0.3544951230287552, Final Batch Loss: 0.17625144124031067\n",
      "Epoch 2080, Loss: 0.384809672832489, Final Batch Loss: 0.21568188071250916\n",
      "Epoch 2081, Loss: 0.3799078017473221, Final Batch Loss: 0.18662893772125244\n",
      "Epoch 2082, Loss: 0.3622222989797592, Final Batch Loss: 0.15947794914245605\n",
      "Epoch 2083, Loss: 0.3806980103254318, Final Batch Loss: 0.1572396457195282\n",
      "Epoch 2084, Loss: 0.4258406311273575, Final Batch Loss: 0.19987711310386658\n",
      "Epoch 2085, Loss: 0.45164406299591064, Final Batch Loss: 0.24862034618854523\n",
      "Epoch 2086, Loss: 0.4655485898256302, Final Batch Loss: 0.20996113121509552\n",
      "Epoch 2087, Loss: 0.3599284589290619, Final Batch Loss: 0.1600130945444107\n",
      "Epoch 2088, Loss: 0.331311360001564, Final Batch Loss: 0.14593863487243652\n",
      "Epoch 2089, Loss: 0.39049161970615387, Final Batch Loss: 0.2028372883796692\n",
      "Epoch 2090, Loss: 0.40764401853084564, Final Batch Loss: 0.23288758099079132\n",
      "Epoch 2091, Loss: 0.40128204226493835, Final Batch Loss: 0.21622604131698608\n",
      "Epoch 2092, Loss: 0.3628671020269394, Final Batch Loss: 0.1697726547718048\n",
      "Epoch 2093, Loss: 0.3843309283256531, Final Batch Loss: 0.1822495460510254\n",
      "Epoch 2094, Loss: 0.4490625113248825, Final Batch Loss: 0.28010129928588867\n",
      "Epoch 2095, Loss: 0.36067357659339905, Final Batch Loss: 0.17033660411834717\n",
      "Epoch 2096, Loss: 0.4297218322753906, Final Batch Loss: 0.2646588683128357\n",
      "Epoch 2097, Loss: 0.3691103458404541, Final Batch Loss: 0.19482091069221497\n",
      "Epoch 2098, Loss: 0.40916572511196136, Final Batch Loss: 0.22178170084953308\n",
      "Epoch 2099, Loss: 0.419534295797348, Final Batch Loss: 0.23606711626052856\n",
      "Epoch 2100, Loss: 0.3411503881216049, Final Batch Loss: 0.18323804438114166\n",
      "Epoch 2101, Loss: 0.3514408767223358, Final Batch Loss: 0.15273341536521912\n",
      "Epoch 2102, Loss: 0.3633250892162323, Final Batch Loss: 0.15391327440738678\n",
      "Epoch 2103, Loss: 0.3970743715763092, Final Batch Loss: 0.20313042402267456\n",
      "Epoch 2104, Loss: 0.3522701561450958, Final Batch Loss: 0.18503764271736145\n",
      "Epoch 2105, Loss: 0.37336473166942596, Final Batch Loss: 0.14036336541175842\n",
      "Epoch 2106, Loss: 0.3795039802789688, Final Batch Loss: 0.16115900874137878\n",
      "Epoch 2107, Loss: 0.4101848751306534, Final Batch Loss: 0.19861780107021332\n",
      "Epoch 2108, Loss: 0.40405233204364777, Final Batch Loss: 0.23377645015716553\n",
      "Epoch 2109, Loss: 0.39675478637218475, Final Batch Loss: 0.1864444464445114\n",
      "Epoch 2110, Loss: 0.3840907961130142, Final Batch Loss: 0.15115590393543243\n",
      "Epoch 2111, Loss: 0.3635648936033249, Final Batch Loss: 0.19417189061641693\n",
      "Epoch 2112, Loss: 0.36786049604415894, Final Batch Loss: 0.21391020715236664\n",
      "Epoch 2113, Loss: 0.3910652995109558, Final Batch Loss: 0.18181155622005463\n",
      "Epoch 2114, Loss: 0.38853858411312103, Final Batch Loss: 0.19846470654010773\n",
      "Epoch 2115, Loss: 0.3839924782514572, Final Batch Loss: 0.1848227083683014\n",
      "Epoch 2116, Loss: 0.42258304357528687, Final Batch Loss: 0.25108227133750916\n",
      "Epoch 2117, Loss: 0.353286936879158, Final Batch Loss: 0.1871694177389145\n",
      "Epoch 2118, Loss: 0.39961546659469604, Final Batch Loss: 0.20683664083480835\n",
      "Epoch 2119, Loss: 0.3571035861968994, Final Batch Loss: 0.16625827550888062\n",
      "Epoch 2120, Loss: 0.33989258110523224, Final Batch Loss: 0.17620262503623962\n",
      "Epoch 2121, Loss: 0.35465507209300995, Final Batch Loss: 0.1503763645887375\n",
      "Epoch 2122, Loss: 0.35276201367378235, Final Batch Loss: 0.18005025386810303\n",
      "Epoch 2123, Loss: 0.37326033413410187, Final Batch Loss: 0.16874685883522034\n",
      "Epoch 2124, Loss: 0.36551401019096375, Final Batch Loss: 0.1761249303817749\n",
      "Epoch 2125, Loss: 0.34678739309310913, Final Batch Loss: 0.17241406440734863\n",
      "Epoch 2126, Loss: 0.3776009976863861, Final Batch Loss: 0.17681469023227692\n",
      "Epoch 2127, Loss: 0.41808144748210907, Final Batch Loss: 0.151980921626091\n",
      "Epoch 2128, Loss: 0.3615598529577255, Final Batch Loss: 0.17123132944107056\n",
      "Epoch 2129, Loss: 0.3467777520418167, Final Batch Loss: 0.18387480080127716\n",
      "Epoch 2130, Loss: 0.395418256521225, Final Batch Loss: 0.1728438138961792\n",
      "Epoch 2131, Loss: 0.37213318049907684, Final Batch Loss: 0.18292205035686493\n",
      "Epoch 2132, Loss: 0.3767797499895096, Final Batch Loss: 0.20872555673122406\n",
      "Epoch 2133, Loss: 0.38804854452610016, Final Batch Loss: 0.1824418008327484\n",
      "Epoch 2134, Loss: 0.34979447722435, Final Batch Loss: 0.17810635268688202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2135, Loss: 0.3496187776327133, Final Batch Loss: 0.17974433302879333\n",
      "Epoch 2136, Loss: 0.3664983808994293, Final Batch Loss: 0.1927463710308075\n",
      "Epoch 2137, Loss: 0.38214200735092163, Final Batch Loss: 0.16124428808689117\n",
      "Epoch 2138, Loss: 0.3874617964029312, Final Batch Loss: 0.23250626027584076\n",
      "Epoch 2139, Loss: 0.40660667419433594, Final Batch Loss: 0.2180667221546173\n",
      "Epoch 2140, Loss: 0.39833344519138336, Final Batch Loss: 0.18094126880168915\n",
      "Epoch 2141, Loss: 0.3917354792356491, Final Batch Loss: 0.22225724160671234\n",
      "Epoch 2142, Loss: 0.35741935670375824, Final Batch Loss: 0.17132969200611115\n",
      "Epoch 2143, Loss: 0.41978752613067627, Final Batch Loss: 0.19721917808055878\n",
      "Epoch 2144, Loss: 0.35261236131191254, Final Batch Loss: 0.12645624577999115\n",
      "Epoch 2145, Loss: 0.35531124472618103, Final Batch Loss: 0.19954736530780792\n",
      "Epoch 2146, Loss: 0.38629262149333954, Final Batch Loss: 0.20311246812343597\n",
      "Epoch 2147, Loss: 0.39152923226356506, Final Batch Loss: 0.21885806322097778\n",
      "Epoch 2148, Loss: 0.3758929967880249, Final Batch Loss: 0.1880655288696289\n",
      "Epoch 2149, Loss: 0.38940463960170746, Final Batch Loss: 0.18100321292877197\n",
      "Epoch 2150, Loss: 0.3635251671075821, Final Batch Loss: 0.17007116973400116\n",
      "Epoch 2151, Loss: 0.47297391295433044, Final Batch Loss: 0.18230703473091125\n",
      "Epoch 2152, Loss: 0.3992091566324234, Final Batch Loss: 0.1718631088733673\n",
      "Epoch 2153, Loss: 0.3856057673692703, Final Batch Loss: 0.22251181304454803\n",
      "Epoch 2154, Loss: 0.356727734208107, Final Batch Loss: 0.17313046753406525\n",
      "Epoch 2155, Loss: 0.37358643114566803, Final Batch Loss: 0.14738446474075317\n",
      "Epoch 2156, Loss: 0.40333081781864166, Final Batch Loss: 0.18315520882606506\n",
      "Epoch 2157, Loss: 0.42039740085601807, Final Batch Loss: 0.22958128154277802\n",
      "Epoch 2158, Loss: 0.42921192944049835, Final Batch Loss: 0.2486211359500885\n",
      "Epoch 2159, Loss: 0.37016020715236664, Final Batch Loss: 0.17852577567100525\n",
      "Epoch 2160, Loss: 0.3830178678035736, Final Batch Loss: 0.18408198654651642\n",
      "Epoch 2161, Loss: 0.335980549454689, Final Batch Loss: 0.13240015506744385\n",
      "Epoch 2162, Loss: 0.34521880745887756, Final Batch Loss: 0.16448886692523956\n",
      "Epoch 2163, Loss: 0.3880702257156372, Final Batch Loss: 0.18604016304016113\n",
      "Epoch 2164, Loss: 0.3749466836452484, Final Batch Loss: 0.18327155709266663\n",
      "Epoch 2165, Loss: 0.4367985427379608, Final Batch Loss: 0.2129320204257965\n",
      "Epoch 2166, Loss: 0.3822551369667053, Final Batch Loss: 0.19121411442756653\n",
      "Epoch 2167, Loss: 0.34987691044807434, Final Batch Loss: 0.1686096340417862\n",
      "Epoch 2168, Loss: 0.3606410026550293, Final Batch Loss: 0.18221542239189148\n",
      "Epoch 2169, Loss: 0.3615809977054596, Final Batch Loss: 0.14331379532814026\n",
      "Epoch 2170, Loss: 0.37283846735954285, Final Batch Loss: 0.14800985157489777\n",
      "Epoch 2171, Loss: 0.3820071369409561, Final Batch Loss: 0.20061159133911133\n",
      "Epoch 2172, Loss: 0.39815187454223633, Final Batch Loss: 0.21794667840003967\n",
      "Epoch 2173, Loss: 0.36931346356868744, Final Batch Loss: 0.16161848604679108\n",
      "Epoch 2174, Loss: 0.3714810162782669, Final Batch Loss: 0.1810680478811264\n",
      "Epoch 2175, Loss: 0.3251805901527405, Final Batch Loss: 0.15345114469528198\n",
      "Epoch 2176, Loss: 0.3792666494846344, Final Batch Loss: 0.1911463737487793\n",
      "Epoch 2177, Loss: 0.38272295892238617, Final Batch Loss: 0.2134893834590912\n",
      "Epoch 2178, Loss: 0.32588669657707214, Final Batch Loss: 0.1840716302394867\n",
      "Epoch 2179, Loss: 0.3417636454105377, Final Batch Loss: 0.18179447948932648\n",
      "Epoch 2180, Loss: 0.42253564298152924, Final Batch Loss: 0.21206173300743103\n",
      "Epoch 2181, Loss: 0.3301260843873024, Final Batch Loss: 0.12381426244974136\n",
      "Epoch 2182, Loss: 0.3833453804254532, Final Batch Loss: 0.20175887644290924\n",
      "Epoch 2183, Loss: 0.44788503646850586, Final Batch Loss: 0.27517232298851013\n",
      "Epoch 2184, Loss: 0.37349389493465424, Final Batch Loss: 0.17923054099082947\n",
      "Epoch 2185, Loss: 0.40547314286231995, Final Batch Loss: 0.2022198885679245\n",
      "Epoch 2186, Loss: 0.3926851600408554, Final Batch Loss: 0.20868393778800964\n",
      "Epoch 2187, Loss: 0.3744577616453171, Final Batch Loss: 0.2060469686985016\n",
      "Epoch 2188, Loss: 0.3377244472503662, Final Batch Loss: 0.14277209341526031\n",
      "Epoch 2189, Loss: 0.3779081702232361, Final Batch Loss: 0.1671280860900879\n",
      "Epoch 2190, Loss: 0.41320858895778656, Final Batch Loss: 0.20438456535339355\n",
      "Epoch 2191, Loss: 0.32282622158527374, Final Batch Loss: 0.14815644919872284\n",
      "Epoch 2192, Loss: 0.37040044367313385, Final Batch Loss: 0.17480330169200897\n",
      "Epoch 2193, Loss: 0.36543916165828705, Final Batch Loss: 0.18820102512836456\n",
      "Epoch 2194, Loss: 0.44520774483680725, Final Batch Loss: 0.2975163757801056\n",
      "Epoch 2195, Loss: 0.35369668900966644, Final Batch Loss: 0.1736670434474945\n",
      "Epoch 2196, Loss: 0.4066939204931259, Final Batch Loss: 0.229105606675148\n",
      "Epoch 2197, Loss: 0.4082418233156204, Final Batch Loss: 0.22733807563781738\n",
      "Epoch 2198, Loss: 0.42563602328300476, Final Batch Loss: 0.24671369791030884\n",
      "Epoch 2199, Loss: 0.4406648725271225, Final Batch Loss: 0.29297709465026855\n",
      "Epoch 2200, Loss: 0.3275042325258255, Final Batch Loss: 0.1530894786119461\n",
      "Epoch 2201, Loss: 0.3484686315059662, Final Batch Loss: 0.1572938710451126\n",
      "Epoch 2202, Loss: 0.36477285623550415, Final Batch Loss: 0.18372322618961334\n",
      "Epoch 2203, Loss: 0.3617944121360779, Final Batch Loss: 0.15966054797172546\n",
      "Epoch 2204, Loss: 0.4142761081457138, Final Batch Loss: 0.2215198576450348\n",
      "Epoch 2205, Loss: 0.40818846225738525, Final Batch Loss: 0.23900975286960602\n",
      "Epoch 2206, Loss: 0.34436921775341034, Final Batch Loss: 0.17428138852119446\n",
      "Epoch 2207, Loss: 0.3740445673465729, Final Batch Loss: 0.17131181061267853\n",
      "Epoch 2208, Loss: 0.3643554002046585, Final Batch Loss: 0.18408529460430145\n",
      "Epoch 2209, Loss: 0.4079657346010208, Final Batch Loss: 0.19602727890014648\n",
      "Epoch 2210, Loss: 0.3811006397008896, Final Batch Loss: 0.17571286857128143\n",
      "Epoch 2211, Loss: 0.35594864189624786, Final Batch Loss: 0.20122870802879333\n",
      "Epoch 2212, Loss: 0.36179088056087494, Final Batch Loss: 0.1452089548110962\n",
      "Epoch 2213, Loss: 0.40161149203777313, Final Batch Loss: 0.20264123380184174\n",
      "Epoch 2214, Loss: 0.3487449288368225, Final Batch Loss: 0.1871993988752365\n",
      "Epoch 2215, Loss: 0.3456469923257828, Final Batch Loss: 0.18053743243217468\n",
      "Epoch 2216, Loss: 0.39523084461688995, Final Batch Loss: 0.2012684941291809\n",
      "Epoch 2217, Loss: 0.37994229793548584, Final Batch Loss: 0.1988247185945511\n",
      "Epoch 2218, Loss: 0.3647047430276871, Final Batch Loss: 0.18632124364376068\n",
      "Epoch 2219, Loss: 0.39582592248916626, Final Batch Loss: 0.17199410498142242\n",
      "Epoch 2220, Loss: 0.3692602217197418, Final Batch Loss: 0.19413672387599945\n",
      "Epoch 2221, Loss: 0.3262437731027603, Final Batch Loss: 0.15030942857265472\n",
      "Epoch 2222, Loss: 0.3555242270231247, Final Batch Loss: 0.17244639992713928\n",
      "Epoch 2223, Loss: 0.40251119434833527, Final Batch Loss: 0.19954688847064972\n",
      "Epoch 2224, Loss: 0.3103950321674347, Final Batch Loss: 0.1502896398305893\n",
      "Epoch 2225, Loss: 0.31728437542915344, Final Batch Loss: 0.1339016854763031\n",
      "Epoch 2226, Loss: 0.3713148683309555, Final Batch Loss: 0.18760262429714203\n",
      "Epoch 2227, Loss: 0.37395907938480377, Final Batch Loss: 0.20019303262233734\n",
      "Epoch 2228, Loss: 0.31941738724708557, Final Batch Loss: 0.15626102685928345\n",
      "Epoch 2229, Loss: 0.3453933000564575, Final Batch Loss: 0.18213479220867157\n",
      "Epoch 2230, Loss: 0.3901633024215698, Final Batch Loss: 0.16541104018688202\n",
      "Epoch 2231, Loss: 0.3385249525308609, Final Batch Loss: 0.1682891845703125\n",
      "Epoch 2232, Loss: 0.3336789309978485, Final Batch Loss: 0.18613606691360474\n",
      "Epoch 2233, Loss: 0.36563457548618317, Final Batch Loss: 0.20732596516609192\n",
      "Epoch 2234, Loss: 0.3735511600971222, Final Batch Loss: 0.1888357400894165\n",
      "Epoch 2235, Loss: 0.3298069089651108, Final Batch Loss: 0.1645813137292862\n",
      "Epoch 2236, Loss: 0.33460554480552673, Final Batch Loss: 0.1495884507894516\n",
      "Epoch 2237, Loss: 0.37955403327941895, Final Batch Loss: 0.21852216124534607\n",
      "Epoch 2238, Loss: 0.34596869349479675, Final Batch Loss: 0.17295224964618683\n",
      "Epoch 2239, Loss: 0.3433966040611267, Final Batch Loss: 0.14699462056159973\n",
      "Epoch 2240, Loss: 0.42624785006046295, Final Batch Loss: 0.2399524748325348\n",
      "Epoch 2241, Loss: 0.33497199416160583, Final Batch Loss: 0.1723785400390625\n",
      "Epoch 2242, Loss: 0.329276442527771, Final Batch Loss: 0.16781087219715118\n",
      "Epoch 2243, Loss: 0.36538180708885193, Final Batch Loss: 0.17252875864505768\n",
      "Epoch 2244, Loss: 0.38744017481803894, Final Batch Loss: 0.19780541956424713\n",
      "Epoch 2245, Loss: 0.3900500237941742, Final Batch Loss: 0.24076002836227417\n",
      "Epoch 2246, Loss: 0.30525273084640503, Final Batch Loss: 0.15459047257900238\n",
      "Epoch 2247, Loss: 0.3284561336040497, Final Batch Loss: 0.16951918601989746\n",
      "Epoch 2248, Loss: 0.4869193583726883, Final Batch Loss: 0.23556046187877655\n",
      "Epoch 2249, Loss: 0.39199690520763397, Final Batch Loss: 0.20453402400016785\n",
      "Epoch 2250, Loss: 0.3721971660852432, Final Batch Loss: 0.19204452633857727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2251, Loss: 0.3529282808303833, Final Batch Loss: 0.19367487728595734\n",
      "Epoch 2252, Loss: 0.40047837793827057, Final Batch Loss: 0.2127552032470703\n",
      "Epoch 2253, Loss: 0.3824850916862488, Final Batch Loss: 0.22030022740364075\n",
      "Epoch 2254, Loss: 0.34923167526721954, Final Batch Loss: 0.179356187582016\n",
      "Epoch 2255, Loss: 0.3304353952407837, Final Batch Loss: 0.13419930636882782\n",
      "Epoch 2256, Loss: 0.3362976759672165, Final Batch Loss: 0.1392054557800293\n",
      "Epoch 2257, Loss: 0.2971000075340271, Final Batch Loss: 0.1361503303050995\n",
      "Epoch 2258, Loss: 0.3905683904886246, Final Batch Loss: 0.14178234338760376\n",
      "Epoch 2259, Loss: 0.34602969884872437, Final Batch Loss: 0.18399660289287567\n",
      "Epoch 2260, Loss: 0.39570020139217377, Final Batch Loss: 0.2097165733575821\n",
      "Epoch 2261, Loss: 0.3653483986854553, Final Batch Loss: 0.22165179252624512\n",
      "Epoch 2262, Loss: 0.38408589363098145, Final Batch Loss: 0.18945391476154327\n",
      "Epoch 2263, Loss: 0.3850824385881424, Final Batch Loss: 0.18203915655612946\n",
      "Epoch 2264, Loss: 0.30831289291381836, Final Batch Loss: 0.13074372708797455\n",
      "Epoch 2265, Loss: 0.3542443662881851, Final Batch Loss: 0.1820664405822754\n",
      "Epoch 2266, Loss: 0.39480987191200256, Final Batch Loss: 0.236257404088974\n",
      "Epoch 2267, Loss: 0.417137011885643, Final Batch Loss: 0.20915256440639496\n",
      "Epoch 2268, Loss: 0.3668021708726883, Final Batch Loss: 0.1367308795452118\n",
      "Epoch 2269, Loss: 0.34770017862319946, Final Batch Loss: 0.18114598095417023\n",
      "Epoch 2270, Loss: 0.4057034105062485, Final Batch Loss: 0.18927225470542908\n",
      "Epoch 2271, Loss: 0.3714379668235779, Final Batch Loss: 0.17969144880771637\n",
      "Epoch 2272, Loss: 0.3908130079507828, Final Batch Loss: 0.19545355439186096\n",
      "Epoch 2273, Loss: 0.38987065851688385, Final Batch Loss: 0.19467096030712128\n",
      "Epoch 2274, Loss: 0.37761084735393524, Final Batch Loss: 0.19014205038547516\n",
      "Epoch 2275, Loss: 0.33687180280685425, Final Batch Loss: 0.17639008164405823\n",
      "Epoch 2276, Loss: 0.4370855242013931, Final Batch Loss: 0.228113055229187\n",
      "Epoch 2277, Loss: 0.4389951229095459, Final Batch Loss: 0.22271469235420227\n",
      "Epoch 2278, Loss: 0.3799173980951309, Final Batch Loss: 0.17445100843906403\n",
      "Epoch 2279, Loss: 0.36268164217472076, Final Batch Loss: 0.1852387934923172\n",
      "Epoch 2280, Loss: 0.3186119794845581, Final Batch Loss: 0.1469784826040268\n",
      "Epoch 2281, Loss: 0.34367071837186813, Final Batch Loss: 0.1183476373553276\n",
      "Epoch 2282, Loss: 0.37709611654281616, Final Batch Loss: 0.2179339975118637\n",
      "Epoch 2283, Loss: 0.35573603212833405, Final Batch Loss: 0.14776766300201416\n",
      "Epoch 2284, Loss: 0.34454046189785004, Final Batch Loss: 0.14352615177631378\n",
      "Epoch 2285, Loss: 0.39970579743385315, Final Batch Loss: 0.16890062391757965\n",
      "Epoch 2286, Loss: 0.4285645931959152, Final Batch Loss: 0.20409081876277924\n",
      "Epoch 2287, Loss: 0.35234320163726807, Final Batch Loss: 0.16293984651565552\n",
      "Epoch 2288, Loss: 0.3872433602809906, Final Batch Loss: 0.23466718196868896\n",
      "Epoch 2289, Loss: 0.3425524979829788, Final Batch Loss: 0.16812728345394135\n",
      "Epoch 2290, Loss: 0.32712222635746, Final Batch Loss: 0.1498347967863083\n",
      "Epoch 2291, Loss: 0.3015395998954773, Final Batch Loss: 0.1317589282989502\n",
      "Epoch 2292, Loss: 0.3361535221338272, Final Batch Loss: 0.1620323807001114\n",
      "Epoch 2293, Loss: 0.3254070430994034, Final Batch Loss: 0.16937929391860962\n",
      "Epoch 2294, Loss: 0.3344515711069107, Final Batch Loss: 0.19147692620754242\n",
      "Epoch 2295, Loss: 0.3582577109336853, Final Batch Loss: 0.14388325810432434\n",
      "Epoch 2296, Loss: 0.3306451141834259, Final Batch Loss: 0.17078199982643127\n",
      "Epoch 2297, Loss: 0.35744160413742065, Final Batch Loss: 0.1614963412284851\n",
      "Epoch 2298, Loss: 0.36952629685401917, Final Batch Loss: 0.17877431213855743\n",
      "Epoch 2299, Loss: 0.3387245833873749, Final Batch Loss: 0.15825693309307098\n",
      "Epoch 2300, Loss: 0.36646343767642975, Final Batch Loss: 0.18075725436210632\n",
      "Epoch 2301, Loss: 0.30995242297649384, Final Batch Loss: 0.13493996858596802\n",
      "Epoch 2302, Loss: 0.3548479229211807, Final Batch Loss: 0.1789046972990036\n",
      "Epoch 2303, Loss: 0.3460256904363632, Final Batch Loss: 0.17667047679424286\n",
      "Epoch 2304, Loss: 0.31447506695985794, Final Batch Loss: 0.12039860337972641\n",
      "Epoch 2305, Loss: 0.33721503615379333, Final Batch Loss: 0.16219381988048553\n",
      "Epoch 2306, Loss: 0.3681035190820694, Final Batch Loss: 0.2016345113515854\n",
      "Epoch 2307, Loss: 0.3226446211338043, Final Batch Loss: 0.13682901859283447\n",
      "Epoch 2308, Loss: 0.4057939797639847, Final Batch Loss: 0.22751614451408386\n",
      "Epoch 2309, Loss: 0.3886399120092392, Final Batch Loss: 0.18551504611968994\n",
      "Epoch 2310, Loss: 0.36705367267131805, Final Batch Loss: 0.17388801276683807\n",
      "Epoch 2311, Loss: 0.35002847015857697, Final Batch Loss: 0.18800130486488342\n",
      "Epoch 2312, Loss: 0.26069237291812897, Final Batch Loss: 0.12534989416599274\n",
      "Epoch 2313, Loss: 0.3571345806121826, Final Batch Loss: 0.17089523375034332\n",
      "Epoch 2314, Loss: 0.35165639221668243, Final Batch Loss: 0.14858472347259521\n",
      "Epoch 2315, Loss: 0.3449142426252365, Final Batch Loss: 0.20776402950286865\n",
      "Epoch 2316, Loss: 0.3788285106420517, Final Batch Loss: 0.21676944196224213\n",
      "Epoch 2317, Loss: 0.3375125974416733, Final Batch Loss: 0.15670190751552582\n",
      "Epoch 2318, Loss: 0.35324321687221527, Final Batch Loss: 0.15566419064998627\n",
      "Epoch 2319, Loss: 0.3468770980834961, Final Batch Loss: 0.1514207422733307\n",
      "Epoch 2320, Loss: 0.3962552100419998, Final Batch Loss: 0.24241630733013153\n",
      "Epoch 2321, Loss: 0.33482465893030167, Final Batch Loss: 0.11374606937170029\n",
      "Epoch 2322, Loss: 0.4023478180170059, Final Batch Loss: 0.19807100296020508\n",
      "Epoch 2323, Loss: 0.3365222364664078, Final Batch Loss: 0.12972736358642578\n",
      "Epoch 2324, Loss: 0.3313829302787781, Final Batch Loss: 0.1372639536857605\n",
      "Epoch 2325, Loss: 0.3608573228120804, Final Batch Loss: 0.18266622722148895\n",
      "Epoch 2326, Loss: 0.3187217563390732, Final Batch Loss: 0.16368524730205536\n",
      "Epoch 2327, Loss: 0.2990909516811371, Final Batch Loss: 0.14043429493904114\n",
      "Epoch 2328, Loss: 0.3408554494380951, Final Batch Loss: 0.15165255963802338\n",
      "Epoch 2329, Loss: 0.3100045695900917, Final Batch Loss: 0.11010656505823135\n",
      "Epoch 2330, Loss: 0.4248952567577362, Final Batch Loss: 0.21699541807174683\n",
      "Epoch 2331, Loss: 0.3866010755300522, Final Batch Loss: 0.16499793529510498\n",
      "Epoch 2332, Loss: 0.33082106709480286, Final Batch Loss: 0.15452425181865692\n",
      "Epoch 2333, Loss: 0.35742148756980896, Final Batch Loss: 0.20120853185653687\n",
      "Epoch 2334, Loss: 0.3710820972919464, Final Batch Loss: 0.2130948305130005\n",
      "Epoch 2335, Loss: 0.37407994270324707, Final Batch Loss: 0.21625404059886932\n",
      "Epoch 2336, Loss: 0.4122030586004257, Final Batch Loss: 0.214304119348526\n",
      "Epoch 2337, Loss: 0.33250318467617035, Final Batch Loss: 0.1630488485097885\n",
      "Epoch 2338, Loss: 0.4324839413166046, Final Batch Loss: 0.21817722916603088\n",
      "Epoch 2339, Loss: 0.34331050515174866, Final Batch Loss: 0.16274766623973846\n",
      "Epoch 2340, Loss: 0.34865227341651917, Final Batch Loss: 0.1543772965669632\n",
      "Epoch 2341, Loss: 0.3876020014286041, Final Batch Loss: 0.19971424341201782\n",
      "Epoch 2342, Loss: 0.42134641110897064, Final Batch Loss: 0.1926199197769165\n",
      "Epoch 2343, Loss: 0.32484690845012665, Final Batch Loss: 0.14733362197875977\n",
      "Epoch 2344, Loss: 0.4493483006954193, Final Batch Loss: 0.2489013969898224\n",
      "Epoch 2345, Loss: 0.35917724668979645, Final Batch Loss: 0.14578041434288025\n",
      "Epoch 2346, Loss: 0.5380444079637527, Final Batch Loss: 0.33400121331214905\n",
      "Epoch 2347, Loss: 0.32806265354156494, Final Batch Loss: 0.13222812116146088\n",
      "Epoch 2348, Loss: 0.3463895320892334, Final Batch Loss: 0.16792115569114685\n",
      "Epoch 2349, Loss: 0.3642096519470215, Final Batch Loss: 0.18058502674102783\n",
      "Epoch 2350, Loss: 0.32055430114269257, Final Batch Loss: 0.17442560195922852\n",
      "Epoch 2351, Loss: 0.320216566324234, Final Batch Loss: 0.16513875126838684\n",
      "Epoch 2352, Loss: 0.37171144783496857, Final Batch Loss: 0.18723085522651672\n",
      "Epoch 2353, Loss: 0.4632635563611984, Final Batch Loss: 0.2769835889339447\n",
      "Epoch 2354, Loss: 0.392472043633461, Final Batch Loss: 0.18281632661819458\n",
      "Epoch 2355, Loss: 0.33166538923978806, Final Batch Loss: 0.11700113862752914\n",
      "Epoch 2356, Loss: 0.34076322615146637, Final Batch Loss: 0.1640910804271698\n",
      "Epoch 2357, Loss: 0.3867727518081665, Final Batch Loss: 0.1860465556383133\n",
      "Epoch 2358, Loss: 0.3401808589696884, Final Batch Loss: 0.16488446295261383\n",
      "Epoch 2359, Loss: 0.34437815845012665, Final Batch Loss: 0.16734911501407623\n",
      "Epoch 2360, Loss: 0.3393905758857727, Final Batch Loss: 0.17551401257514954\n",
      "Epoch 2361, Loss: 0.4062202423810959, Final Batch Loss: 0.18691137433052063\n",
      "Epoch 2362, Loss: 0.4001620262861252, Final Batch Loss: 0.21280352771282196\n",
      "Epoch 2363, Loss: 0.39956575632095337, Final Batch Loss: 0.2069569081068039\n",
      "Epoch 2364, Loss: 0.366840124130249, Final Batch Loss: 0.19922278821468353\n",
      "Epoch 2365, Loss: 0.3736514002084732, Final Batch Loss: 0.16121523082256317\n",
      "Epoch 2366, Loss: 0.3632718473672867, Final Batch Loss: 0.18039141595363617\n",
      "Epoch 2367, Loss: 0.3541341572999954, Final Batch Loss: 0.1622644066810608\n",
      "Epoch 2368, Loss: 0.3755943179130554, Final Batch Loss: 0.19393283128738403\n",
      "Epoch 2369, Loss: 0.34965045750141144, Final Batch Loss: 0.13943643867969513\n",
      "Epoch 2370, Loss: 0.4189688414335251, Final Batch Loss: 0.23246918618679047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2371, Loss: 0.3165375366806984, Final Batch Loss: 0.1201961562037468\n",
      "Epoch 2372, Loss: 0.3675556629896164, Final Batch Loss: 0.1750369668006897\n",
      "Epoch 2373, Loss: 0.355090469121933, Final Batch Loss: 0.1617841124534607\n",
      "Epoch 2374, Loss: 0.34422533214092255, Final Batch Loss: 0.16409257054328918\n",
      "Epoch 2375, Loss: 0.30898450314998627, Final Batch Loss: 0.13731051981449127\n",
      "Epoch 2376, Loss: 0.354784831404686, Final Batch Loss: 0.19364413619041443\n",
      "Epoch 2377, Loss: 0.34561698138713837, Final Batch Loss: 0.18570104241371155\n",
      "Epoch 2378, Loss: 0.33474311232566833, Final Batch Loss: 0.15960942208766937\n",
      "Epoch 2379, Loss: 0.3886472284793854, Final Batch Loss: 0.15312594175338745\n",
      "Epoch 2380, Loss: 0.4082813411951065, Final Batch Loss: 0.18637003004550934\n",
      "Epoch 2381, Loss: 0.3642643541097641, Final Batch Loss: 0.18705244362354279\n",
      "Epoch 2382, Loss: 0.3502229303121567, Final Batch Loss: 0.1952715814113617\n",
      "Epoch 2383, Loss: 0.40443915128707886, Final Batch Loss: 0.21637074649333954\n",
      "Epoch 2384, Loss: 0.3032207638025284, Final Batch Loss: 0.14457890391349792\n",
      "Epoch 2385, Loss: 0.37269093096256256, Final Batch Loss: 0.23270924389362335\n",
      "Epoch 2386, Loss: 0.32734315097332, Final Batch Loss: 0.19421491026878357\n",
      "Epoch 2387, Loss: 0.42464645206928253, Final Batch Loss: 0.2621530294418335\n",
      "Epoch 2388, Loss: 0.3460926413536072, Final Batch Loss: 0.1973925232887268\n",
      "Epoch 2389, Loss: 0.33577296137809753, Final Batch Loss: 0.14751017093658447\n",
      "Epoch 2390, Loss: 0.3397436738014221, Final Batch Loss: 0.16633957624435425\n",
      "Epoch 2391, Loss: 0.34354327619075775, Final Batch Loss: 0.18131588399410248\n",
      "Epoch 2392, Loss: 0.3170322924852371, Final Batch Loss: 0.1517540067434311\n",
      "Epoch 2393, Loss: 0.40250515937805176, Final Batch Loss: 0.20155556499958038\n",
      "Epoch 2394, Loss: 0.33087220788002014, Final Batch Loss: 0.15482942759990692\n",
      "Epoch 2395, Loss: 0.34783774614334106, Final Batch Loss: 0.14543509483337402\n",
      "Epoch 2396, Loss: 0.33265888690948486, Final Batch Loss: 0.138113871216774\n",
      "Epoch 2397, Loss: 0.35699689388275146, Final Batch Loss: 0.16739888489246368\n",
      "Epoch 2398, Loss: 0.37180598080158234, Final Batch Loss: 0.21798169612884521\n",
      "Epoch 2399, Loss: 0.31968219578266144, Final Batch Loss: 0.16178256273269653\n",
      "Epoch 2400, Loss: 0.36977553367614746, Final Batch Loss: 0.19558069109916687\n",
      "Epoch 2401, Loss: 0.3982675224542618, Final Batch Loss: 0.20347046852111816\n",
      "Epoch 2402, Loss: 0.312498040497303, Final Batch Loss: 0.12278745323419571\n",
      "Epoch 2403, Loss: 0.3714097589254379, Final Batch Loss: 0.17417068779468536\n",
      "Epoch 2404, Loss: 0.37198781967163086, Final Batch Loss: 0.1677648425102234\n",
      "Epoch 2405, Loss: 0.3591928333044052, Final Batch Loss: 0.18154743313789368\n",
      "Epoch 2406, Loss: 0.3764205276966095, Final Batch Loss: 0.1865009069442749\n",
      "Epoch 2407, Loss: 0.3509185165166855, Final Batch Loss: 0.19387443363666534\n",
      "Epoch 2408, Loss: 0.44241032004356384, Final Batch Loss: 0.23169200122356415\n",
      "Epoch 2409, Loss: 0.33951620757579803, Final Batch Loss: 0.17116612195968628\n",
      "Epoch 2410, Loss: 0.3896748870611191, Final Batch Loss: 0.20942586660385132\n",
      "Epoch 2411, Loss: 0.3310811072587967, Final Batch Loss: 0.15730822086334229\n",
      "Epoch 2412, Loss: 0.3072199672460556, Final Batch Loss: 0.1523916870355606\n",
      "Epoch 2413, Loss: 0.3221415728330612, Final Batch Loss: 0.14353980123996735\n",
      "Epoch 2414, Loss: 0.3356877863407135, Final Batch Loss: 0.14659371972084045\n",
      "Epoch 2415, Loss: 0.3369588553905487, Final Batch Loss: 0.18473918735980988\n",
      "Epoch 2416, Loss: 0.389098197221756, Final Batch Loss: 0.1951613873243332\n",
      "Epoch 2417, Loss: 0.3590534031391144, Final Batch Loss: 0.183071106672287\n",
      "Epoch 2418, Loss: 0.33531586825847626, Final Batch Loss: 0.17563538253307343\n",
      "Epoch 2419, Loss: 0.35851454734802246, Final Batch Loss: 0.183820903301239\n",
      "Epoch 2420, Loss: 0.3061680346727371, Final Batch Loss: 0.14762648940086365\n",
      "Epoch 2421, Loss: 0.3375949561595917, Final Batch Loss: 0.150698721408844\n",
      "Epoch 2422, Loss: 0.3223927915096283, Final Batch Loss: 0.1498049646615982\n",
      "Epoch 2423, Loss: 0.341703861951828, Final Batch Loss: 0.1592099964618683\n",
      "Epoch 2424, Loss: 0.4647569954395294, Final Batch Loss: 0.2642415761947632\n",
      "Epoch 2425, Loss: 0.3374055474996567, Final Batch Loss: 0.17217613756656647\n",
      "Epoch 2426, Loss: 0.4098729342222214, Final Batch Loss: 0.22455786168575287\n",
      "Epoch 2427, Loss: 0.3354075402021408, Final Batch Loss: 0.1878441423177719\n",
      "Epoch 2428, Loss: 0.3259538859128952, Final Batch Loss: 0.16185501217842102\n",
      "Epoch 2429, Loss: 0.3390631228685379, Final Batch Loss: 0.15564963221549988\n",
      "Epoch 2430, Loss: 0.3593797981739044, Final Batch Loss: 0.19792483747005463\n",
      "Epoch 2431, Loss: 0.3032374680042267, Final Batch Loss: 0.15525540709495544\n",
      "Epoch 2432, Loss: 0.36122138798236847, Final Batch Loss: 0.21555635333061218\n",
      "Epoch 2433, Loss: 0.36460715532302856, Final Batch Loss: 0.20930947363376617\n",
      "Epoch 2434, Loss: 0.3811660706996918, Final Batch Loss: 0.1875261664390564\n",
      "Epoch 2435, Loss: 0.3454219400882721, Final Batch Loss: 0.17077259719371796\n",
      "Epoch 2436, Loss: 0.37444251775741577, Final Batch Loss: 0.17043337225914001\n",
      "Epoch 2437, Loss: 0.31585200130939484, Final Batch Loss: 0.14697465300559998\n",
      "Epoch 2438, Loss: 0.335692822933197, Final Batch Loss: 0.14748147130012512\n",
      "Epoch 2439, Loss: 0.355100080370903, Final Batch Loss: 0.18677367269992828\n",
      "Epoch 2440, Loss: 0.3667166084051132, Final Batch Loss: 0.2109273076057434\n",
      "Epoch 2441, Loss: 0.32363905012607574, Final Batch Loss: 0.1564444601535797\n",
      "Epoch 2442, Loss: 0.3759417086839676, Final Batch Loss: 0.19865255057811737\n",
      "Epoch 2443, Loss: 0.34126730263233185, Final Batch Loss: 0.15001215040683746\n",
      "Epoch 2444, Loss: 0.37423716485500336, Final Batch Loss: 0.1961713582277298\n",
      "Epoch 2445, Loss: 0.40645627677440643, Final Batch Loss: 0.22629357874393463\n",
      "Epoch 2446, Loss: 0.3843267261981964, Final Batch Loss: 0.19387394189834595\n",
      "Epoch 2447, Loss: 0.3556460738182068, Final Batch Loss: 0.18357731401920319\n",
      "Epoch 2448, Loss: 0.32280486822128296, Final Batch Loss: 0.14007270336151123\n",
      "Epoch 2449, Loss: 0.3524521738290787, Final Batch Loss: 0.1605038344860077\n",
      "Epoch 2450, Loss: 0.2674848735332489, Final Batch Loss: 0.11293159425258636\n",
      "Epoch 2451, Loss: 0.3563694953918457, Final Batch Loss: 0.14086584746837616\n",
      "Epoch 2452, Loss: 0.30538976192474365, Final Batch Loss: 0.13176852464675903\n",
      "Epoch 2453, Loss: 0.3606734871864319, Final Batch Loss: 0.2202463001012802\n",
      "Epoch 2454, Loss: 0.2771255820989609, Final Batch Loss: 0.1420038640499115\n",
      "Epoch 2455, Loss: 0.31914740800857544, Final Batch Loss: 0.1728108674287796\n",
      "Epoch 2456, Loss: 0.400325670838356, Final Batch Loss: 0.17258377373218536\n",
      "Epoch 2457, Loss: 0.4126940965652466, Final Batch Loss: 0.23532885313034058\n",
      "Epoch 2458, Loss: 0.3740956634283066, Final Batch Loss: 0.21607083082199097\n",
      "Epoch 2459, Loss: 0.36990924179553986, Final Batch Loss: 0.1424720138311386\n",
      "Epoch 2460, Loss: 0.3354586809873581, Final Batch Loss: 0.14701391756534576\n",
      "Epoch 2461, Loss: 0.3364325016736984, Final Batch Loss: 0.16800497472286224\n",
      "Epoch 2462, Loss: 0.3533121198415756, Final Batch Loss: 0.19462408125400543\n",
      "Epoch 2463, Loss: 0.2985546439886093, Final Batch Loss: 0.13124823570251465\n",
      "Epoch 2464, Loss: 0.3410882353782654, Final Batch Loss: 0.20230349898338318\n",
      "Epoch 2465, Loss: 0.3482372760772705, Final Batch Loss: 0.14352905750274658\n",
      "Epoch 2466, Loss: 0.41842518746852875, Final Batch Loss: 0.22780312597751617\n",
      "Epoch 2467, Loss: 0.3812190592288971, Final Batch Loss: 0.19379206001758575\n",
      "Epoch 2468, Loss: 0.356109619140625, Final Batch Loss: 0.18425999581813812\n",
      "Epoch 2469, Loss: 0.34238289296627045, Final Batch Loss: 0.18736553192138672\n",
      "Epoch 2470, Loss: 0.33470599353313446, Final Batch Loss: 0.15791572630405426\n",
      "Epoch 2471, Loss: 0.32567261159420013, Final Batch Loss: 0.15380209684371948\n",
      "Epoch 2472, Loss: 0.3347046971321106, Final Batch Loss: 0.13721580803394318\n",
      "Epoch 2473, Loss: 0.34661558270454407, Final Batch Loss: 0.1731385588645935\n",
      "Epoch 2474, Loss: 0.28868700563907623, Final Batch Loss: 0.12525874376296997\n",
      "Epoch 2475, Loss: 0.28051242232322693, Final Batch Loss: 0.13032206892967224\n",
      "Epoch 2476, Loss: 0.34852322936058044, Final Batch Loss: 0.17265072464942932\n",
      "Epoch 2477, Loss: 0.3945588022470474, Final Batch Loss: 0.2478025257587433\n",
      "Epoch 2478, Loss: 0.26393306255340576, Final Batch Loss: 0.10699394345283508\n",
      "Epoch 2479, Loss: 0.3690339922904968, Final Batch Loss: 0.19347169995307922\n",
      "Epoch 2480, Loss: 0.4008726328611374, Final Batch Loss: 0.21714474260807037\n",
      "Epoch 2481, Loss: 0.35002388060092926, Final Batch Loss: 0.1846582591533661\n",
      "Epoch 2482, Loss: 0.38355953991413116, Final Batch Loss: 0.2781614065170288\n",
      "Epoch 2483, Loss: 0.38364043831825256, Final Batch Loss: 0.2332145720720291\n",
      "Epoch 2484, Loss: 0.3802486062049866, Final Batch Loss: 0.18023216724395752\n",
      "Epoch 2485, Loss: 0.37494610249996185, Final Batch Loss: 0.1962464451789856\n",
      "Epoch 2486, Loss: 0.3044451028108597, Final Batch Loss: 0.12993675470352173\n",
      "Epoch 2487, Loss: 0.3799401670694351, Final Batch Loss: 0.23684637248516083\n",
      "Epoch 2488, Loss: 0.3315514475107193, Final Batch Loss: 0.15773218870162964\n",
      "Epoch 2489, Loss: 0.31801924109458923, Final Batch Loss: 0.14234435558319092\n",
      "Epoch 2490, Loss: 0.3659146875143051, Final Batch Loss: 0.17209720611572266\n",
      "Epoch 2491, Loss: 0.402772456407547, Final Batch Loss: 0.21685048937797546\n",
      "Epoch 2492, Loss: 0.3749202489852905, Final Batch Loss: 0.15190260112285614\n",
      "Epoch 2493, Loss: 0.4176982343196869, Final Batch Loss: 0.24639566242694855\n",
      "Epoch 2494, Loss: 0.37410779297351837, Final Batch Loss: 0.17902912199497223\n",
      "Epoch 2495, Loss: 0.3124978095293045, Final Batch Loss: 0.1347407102584839\n",
      "Epoch 2496, Loss: 0.3364211320877075, Final Batch Loss: 0.17794905602931976\n",
      "Epoch 2497, Loss: 0.34850411117076874, Final Batch Loss: 0.20111705362796783\n",
      "Epoch 2498, Loss: 0.350588321685791, Final Batch Loss: 0.1642269790172577\n",
      "Epoch 2499, Loss: 0.37335681915283203, Final Batch Loss: 0.19900286197662354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500, Loss: 0.3948451578617096, Final Batch Loss: 0.21255333721637726\n",
      "Epoch 2501, Loss: 0.36577053368091583, Final Batch Loss: 0.18215353786945343\n",
      "Epoch 2502, Loss: 0.33100810647010803, Final Batch Loss: 0.14261195063591003\n",
      "Epoch 2503, Loss: 0.33775436878204346, Final Batch Loss: 0.14945974946022034\n",
      "Epoch 2504, Loss: 0.3639846593141556, Final Batch Loss: 0.14935974776744843\n",
      "Epoch 2505, Loss: 0.360133096575737, Final Batch Loss: 0.2006453424692154\n",
      "Epoch 2506, Loss: 0.36456456780433655, Final Batch Loss: 0.1892651617527008\n",
      "Epoch 2507, Loss: 0.32723478972911835, Final Batch Loss: 0.1382884532213211\n",
      "Epoch 2508, Loss: 0.35630784183740616, Final Batch Loss: 0.11969564110040665\n",
      "Epoch 2509, Loss: 0.37192797660827637, Final Batch Loss: 0.18448221683502197\n",
      "Epoch 2510, Loss: 0.31715692579746246, Final Batch Loss: 0.14916887879371643\n",
      "Epoch 2511, Loss: 0.4154272824525833, Final Batch Loss: 0.25426122546195984\n",
      "Epoch 2512, Loss: 0.3530581146478653, Final Batch Loss: 0.18473467230796814\n",
      "Epoch 2513, Loss: 0.31973837316036224, Final Batch Loss: 0.14009611308574677\n",
      "Epoch 2514, Loss: 0.35255347192287445, Final Batch Loss: 0.2135954052209854\n",
      "Epoch 2515, Loss: 0.354435533285141, Final Batch Loss: 0.2121599167585373\n",
      "Epoch 2516, Loss: 0.35616347193717957, Final Batch Loss: 0.17070123553276062\n",
      "Epoch 2517, Loss: 0.3488757014274597, Final Batch Loss: 0.19587770104408264\n",
      "Epoch 2518, Loss: 0.3792343884706497, Final Batch Loss: 0.24438025057315826\n",
      "Epoch 2519, Loss: 0.31436987221241, Final Batch Loss: 0.1291821449995041\n",
      "Epoch 2520, Loss: 0.3144809752702713, Final Batch Loss: 0.17680293321609497\n",
      "Epoch 2521, Loss: 0.381645530462265, Final Batch Loss: 0.2063221037387848\n",
      "Epoch 2522, Loss: 0.3082505613565445, Final Batch Loss: 0.1378457099199295\n",
      "Epoch 2523, Loss: 0.38585934042930603, Final Batch Loss: 0.219003826379776\n",
      "Epoch 2524, Loss: 0.3180122673511505, Final Batch Loss: 0.16615080833435059\n",
      "Epoch 2525, Loss: 0.3065049946308136, Final Batch Loss: 0.14931780099868774\n",
      "Epoch 2526, Loss: 0.3503316342830658, Final Batch Loss: 0.17801730334758759\n",
      "Epoch 2527, Loss: 0.31651002168655396, Final Batch Loss: 0.1497594714164734\n",
      "Epoch 2528, Loss: 0.32756467163562775, Final Batch Loss: 0.15249884128570557\n",
      "Epoch 2529, Loss: 0.31575386226177216, Final Batch Loss: 0.1746215969324112\n",
      "Epoch 2530, Loss: 0.26311226189136505, Final Batch Loss: 0.10238710045814514\n",
      "Epoch 2531, Loss: 0.3391326516866684, Final Batch Loss: 0.20586225390434265\n",
      "Epoch 2532, Loss: 0.3401816785335541, Final Batch Loss: 0.1841767579317093\n",
      "Epoch 2533, Loss: 0.3426619619131088, Final Batch Loss: 0.21282899379730225\n",
      "Epoch 2534, Loss: 0.3912917375564575, Final Batch Loss: 0.20913751423358917\n",
      "Epoch 2535, Loss: 0.34864482283592224, Final Batch Loss: 0.1839919090270996\n",
      "Epoch 2536, Loss: 0.3145240694284439, Final Batch Loss: 0.14932894706726074\n",
      "Epoch 2537, Loss: 0.2985910400748253, Final Batch Loss: 0.11741461604833603\n",
      "Epoch 2538, Loss: 0.31724484264850616, Final Batch Loss: 0.1692570596933365\n",
      "Epoch 2539, Loss: 0.37830664217472076, Final Batch Loss: 0.14831888675689697\n",
      "Epoch 2540, Loss: 0.3020753711462021, Final Batch Loss: 0.1588563323020935\n",
      "Epoch 2541, Loss: 0.3248964101076126, Final Batch Loss: 0.1365494281053543\n",
      "Epoch 2542, Loss: 0.44271400570869446, Final Batch Loss: 0.17976069450378418\n",
      "Epoch 2543, Loss: 0.3142494410276413, Final Batch Loss: 0.13826186954975128\n",
      "Epoch 2544, Loss: 0.41654549539089203, Final Batch Loss: 0.28031402826309204\n",
      "Epoch 2545, Loss: 0.31875796616077423, Final Batch Loss: 0.17232146859169006\n",
      "Epoch 2546, Loss: 0.3227608650922775, Final Batch Loss: 0.13905979692935944\n",
      "Epoch 2547, Loss: 0.31674517691135406, Final Batch Loss: 0.18339593708515167\n",
      "Epoch 2548, Loss: 0.3399432748556137, Final Batch Loss: 0.18295419216156006\n",
      "Epoch 2549, Loss: 0.3246094137430191, Final Batch Loss: 0.13698141276836395\n",
      "Epoch 2550, Loss: 0.35073427855968475, Final Batch Loss: 0.16625675559043884\n",
      "Epoch 2551, Loss: 0.2861892879009247, Final Batch Loss: 0.13322493433952332\n",
      "Epoch 2552, Loss: 0.32539764046669006, Final Batch Loss: 0.16265641152858734\n",
      "Epoch 2553, Loss: 0.31696319580078125, Final Batch Loss: 0.1360950767993927\n",
      "Epoch 2554, Loss: 0.3433179706335068, Final Batch Loss: 0.18442697823047638\n",
      "Epoch 2555, Loss: 0.33549804985523224, Final Batch Loss: 0.17885419726371765\n",
      "Epoch 2556, Loss: 0.33946725726127625, Final Batch Loss: 0.16467465460300446\n",
      "Epoch 2557, Loss: 0.3257972002029419, Final Batch Loss: 0.17016473412513733\n",
      "Epoch 2558, Loss: 0.30706489086151123, Final Batch Loss: 0.17732153832912445\n",
      "Epoch 2559, Loss: 0.2964630424976349, Final Batch Loss: 0.1566520780324936\n",
      "Epoch 2560, Loss: 0.32447029650211334, Final Batch Loss: 0.18564365804195404\n",
      "Epoch 2561, Loss: 0.3001563996076584, Final Batch Loss: 0.16930213570594788\n",
      "Epoch 2562, Loss: 0.3233366906642914, Final Batch Loss: 0.21233609318733215\n",
      "Epoch 2563, Loss: 0.30623553693294525, Final Batch Loss: 0.1565667986869812\n",
      "Epoch 2564, Loss: 0.35014501959085464, Final Batch Loss: 0.24743057787418365\n",
      "Epoch 2565, Loss: 0.4087638407945633, Final Batch Loss: 0.24344037473201752\n",
      "Epoch 2566, Loss: 0.2962140589952469, Final Batch Loss: 0.167210653424263\n",
      "Epoch 2567, Loss: 0.3309347778558731, Final Batch Loss: 0.18351422250270844\n",
      "Epoch 2568, Loss: 0.2982287034392357, Final Batch Loss: 0.11546079069375992\n",
      "Epoch 2569, Loss: 0.3292485773563385, Final Batch Loss: 0.1828940510749817\n",
      "Epoch 2570, Loss: 0.35799068212509155, Final Batch Loss: 0.19085808098316193\n",
      "Epoch 2571, Loss: 0.3202979117631912, Final Batch Loss: 0.16110435128211975\n",
      "Epoch 2572, Loss: 0.4550597816705704, Final Batch Loss: 0.23494853079319\n",
      "Epoch 2573, Loss: 0.311205193400383, Final Batch Loss: 0.13128069043159485\n",
      "Epoch 2574, Loss: 0.31937481462955475, Final Batch Loss: 0.1350114643573761\n",
      "Epoch 2575, Loss: 0.3192198723554611, Final Batch Loss: 0.16451098024845123\n",
      "Epoch 2576, Loss: 0.3302602916955948, Final Batch Loss: 0.15087798237800598\n",
      "Epoch 2577, Loss: 0.3021302670240402, Final Batch Loss: 0.16217876970767975\n",
      "Epoch 2578, Loss: 0.2851749435067177, Final Batch Loss: 0.08598717302083969\n",
      "Epoch 2579, Loss: 0.4136781841516495, Final Batch Loss: 0.1924133002758026\n",
      "Epoch 2580, Loss: 0.27303293347358704, Final Batch Loss: 0.1339653879404068\n",
      "Epoch 2581, Loss: 0.3029080033302307, Final Batch Loss: 0.1338968575000763\n",
      "Epoch 2582, Loss: 0.342163547873497, Final Batch Loss: 0.16468201577663422\n",
      "Epoch 2583, Loss: 0.2839432954788208, Final Batch Loss: 0.1376236081123352\n",
      "Epoch 2584, Loss: 0.28413116931915283, Final Batch Loss: 0.14438548684120178\n",
      "Epoch 2585, Loss: 0.3773450553417206, Final Batch Loss: 0.22635336220264435\n",
      "Epoch 2586, Loss: 0.29861532151699066, Final Batch Loss: 0.15237806737422943\n",
      "Epoch 2587, Loss: 0.3910430073738098, Final Batch Loss: 0.16327711939811707\n",
      "Epoch 2588, Loss: 0.3224633038043976, Final Batch Loss: 0.13895559310913086\n",
      "Epoch 2589, Loss: 0.31081661581993103, Final Batch Loss: 0.1278606802225113\n",
      "Epoch 2590, Loss: 0.3600959926843643, Final Batch Loss: 0.22573408484458923\n",
      "Epoch 2591, Loss: 0.3076224625110626, Final Batch Loss: 0.14412502944469452\n",
      "Epoch 2592, Loss: 0.3877835124731064, Final Batch Loss: 0.19184374809265137\n",
      "Epoch 2593, Loss: 0.36258529126644135, Final Batch Loss: 0.21262586116790771\n",
      "Epoch 2594, Loss: 0.3616229444742203, Final Batch Loss: 0.20009845495224\n",
      "Epoch 2595, Loss: 0.3291631191968918, Final Batch Loss: 0.1315728724002838\n",
      "Epoch 2596, Loss: 0.34808050096035004, Final Batch Loss: 0.17757683992385864\n",
      "Epoch 2597, Loss: 0.3255268782377243, Final Batch Loss: 0.11594690382480621\n",
      "Epoch 2598, Loss: 0.31690989434719086, Final Batch Loss: 0.1380615234375\n",
      "Epoch 2599, Loss: 0.3872245103120804, Final Batch Loss: 0.18593601882457733\n",
      "Epoch 2600, Loss: 0.3227929174900055, Final Batch Loss: 0.13285242021083832\n",
      "Epoch 2601, Loss: 0.3454672396183014, Final Batch Loss: 0.18019047379493713\n",
      "Epoch 2602, Loss: 0.2978084981441498, Final Batch Loss: 0.13603369891643524\n",
      "Epoch 2603, Loss: 0.3512905091047287, Final Batch Loss: 0.17350278794765472\n",
      "Epoch 2604, Loss: 0.35451845824718475, Final Batch Loss: 0.17544656991958618\n",
      "Epoch 2605, Loss: 0.3327634483575821, Final Batch Loss: 0.147322416305542\n",
      "Epoch 2606, Loss: 0.3389226198196411, Final Batch Loss: 0.1729165017604828\n",
      "Epoch 2607, Loss: 0.30654649436473846, Final Batch Loss: 0.12367619574069977\n",
      "Epoch 2608, Loss: 0.39258040487766266, Final Batch Loss: 0.19446095824241638\n",
      "Epoch 2609, Loss: 0.2947711795568466, Final Batch Loss: 0.13836096227169037\n",
      "Epoch 2610, Loss: 0.34371986985206604, Final Batch Loss: 0.1744677722454071\n",
      "Epoch 2611, Loss: 0.26910610496997833, Final Batch Loss: 0.13428552448749542\n",
      "Epoch 2612, Loss: 0.33290357887744904, Final Batch Loss: 0.1789618730545044\n",
      "Epoch 2613, Loss: 0.3916926383972168, Final Batch Loss: 0.2110014408826828\n",
      "Epoch 2614, Loss: 0.38102880120277405, Final Batch Loss: 0.21083426475524902\n",
      "Epoch 2615, Loss: 0.3486015647649765, Final Batch Loss: 0.22893500328063965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2616, Loss: 0.3433566391468048, Final Batch Loss: 0.1451490819454193\n",
      "Epoch 2617, Loss: 0.3180885314941406, Final Batch Loss: 0.1691131293773651\n",
      "Epoch 2618, Loss: 0.3137129545211792, Final Batch Loss: 0.17394334077835083\n",
      "Epoch 2619, Loss: 0.3048071563243866, Final Batch Loss: 0.14520706236362457\n",
      "Epoch 2620, Loss: 0.31583403050899506, Final Batch Loss: 0.15960663557052612\n",
      "Epoch 2621, Loss: 0.37878261506557465, Final Batch Loss: 0.2066681981086731\n",
      "Epoch 2622, Loss: 0.30105258524417877, Final Batch Loss: 0.1358489990234375\n",
      "Epoch 2623, Loss: 0.29021888226270676, Final Batch Loss: 0.11160475760698318\n",
      "Epoch 2624, Loss: 0.29392896592617035, Final Batch Loss: 0.12509438395500183\n",
      "Epoch 2625, Loss: 0.3331030458211899, Final Batch Loss: 0.18619339168071747\n",
      "Epoch 2626, Loss: 0.35130660235881805, Final Batch Loss: 0.18194261193275452\n",
      "Epoch 2627, Loss: 0.3318338841199875, Final Batch Loss: 0.17901983857154846\n",
      "Epoch 2628, Loss: 0.33879660069942474, Final Batch Loss: 0.1308882087469101\n",
      "Epoch 2629, Loss: 0.3212917149066925, Final Batch Loss: 0.17342761158943176\n",
      "Epoch 2630, Loss: 0.3561461865901947, Final Batch Loss: 0.12923671305179596\n",
      "Epoch 2631, Loss: 0.318756639957428, Final Batch Loss: 0.1658608764410019\n",
      "Epoch 2632, Loss: 0.30890579521656036, Final Batch Loss: 0.14367978274822235\n",
      "Epoch 2633, Loss: 0.3062122240662575, Final Batch Loss: 0.1812773197889328\n",
      "Epoch 2634, Loss: 0.34964022040367126, Final Batch Loss: 0.2211073487997055\n",
      "Epoch 2635, Loss: 0.44304148852825165, Final Batch Loss: 0.2651440501213074\n",
      "Epoch 2636, Loss: 0.36173297464847565, Final Batch Loss: 0.17683959007263184\n",
      "Epoch 2637, Loss: 0.3644885718822479, Final Batch Loss: 0.15824927389621735\n",
      "Epoch 2638, Loss: 0.28653407096862793, Final Batch Loss: 0.15970253944396973\n",
      "Epoch 2639, Loss: 0.2984437048435211, Final Batch Loss: 0.13842332363128662\n",
      "Epoch 2640, Loss: 0.3212977945804596, Final Batch Loss: 0.12808553874492645\n",
      "Epoch 2641, Loss: 0.3431772440671921, Final Batch Loss: 0.16904892027378082\n",
      "Epoch 2642, Loss: 0.27234312891960144, Final Batch Loss: 0.12720844149589539\n",
      "Epoch 2643, Loss: 0.3115382045507431, Final Batch Loss: 0.17027491331100464\n",
      "Epoch 2644, Loss: 0.3170495480298996, Final Batch Loss: 0.1537182778120041\n",
      "Epoch 2645, Loss: 0.3405902534723282, Final Batch Loss: 0.15910710394382477\n",
      "Epoch 2646, Loss: 0.32266494631767273, Final Batch Loss: 0.1958669126033783\n",
      "Epoch 2647, Loss: 0.28862862288951874, Final Batch Loss: 0.13352178037166595\n",
      "Epoch 2648, Loss: 0.349964901804924, Final Batch Loss: 0.14793039858341217\n",
      "Epoch 2649, Loss: 0.3316603899002075, Final Batch Loss: 0.19253525137901306\n",
      "Epoch 2650, Loss: 0.3952208459377289, Final Batch Loss: 0.1713017374277115\n",
      "Epoch 2651, Loss: 0.3134918659925461, Final Batch Loss: 0.14187787473201752\n",
      "Epoch 2652, Loss: 0.3166239261627197, Final Batch Loss: 0.12969942390918732\n",
      "Epoch 2653, Loss: 0.3317098468542099, Final Batch Loss: 0.1547367125749588\n",
      "Epoch 2654, Loss: 0.3601654917001724, Final Batch Loss: 0.23793792724609375\n",
      "Epoch 2655, Loss: 0.3426292836666107, Final Batch Loss: 0.14716240763664246\n",
      "Epoch 2656, Loss: 0.3411918431520462, Final Batch Loss: 0.198836550116539\n",
      "Epoch 2657, Loss: 0.3290860205888748, Final Batch Loss: 0.15798501670360565\n",
      "Epoch 2658, Loss: 0.33760949969291687, Final Batch Loss: 0.18205863237380981\n",
      "Epoch 2659, Loss: 0.25293560326099396, Final Batch Loss: 0.11786161363124847\n",
      "Epoch 2660, Loss: 0.2952900528907776, Final Batch Loss: 0.142638698220253\n",
      "Epoch 2661, Loss: 0.3170759826898575, Final Batch Loss: 0.15344710648059845\n",
      "Epoch 2662, Loss: 0.2972511798143387, Final Batch Loss: 0.127008855342865\n",
      "Epoch 2663, Loss: 0.328855961561203, Final Batch Loss: 0.1576230376958847\n",
      "Epoch 2664, Loss: 0.3130338788032532, Final Batch Loss: 0.17858891189098358\n",
      "Epoch 2665, Loss: 0.29954369366168976, Final Batch Loss: 0.16878876090049744\n",
      "Epoch 2666, Loss: 0.3619479387998581, Final Batch Loss: 0.17761309444904327\n",
      "Epoch 2667, Loss: 0.3293260931968689, Final Batch Loss: 0.1611858308315277\n",
      "Epoch 2668, Loss: 0.3217783421278, Final Batch Loss: 0.18532846868038177\n",
      "Epoch 2669, Loss: 0.35327064990997314, Final Batch Loss: 0.18543294072151184\n",
      "Epoch 2670, Loss: 0.34875763952732086, Final Batch Loss: 0.16488614678382874\n",
      "Epoch 2671, Loss: 0.2959985285997391, Final Batch Loss: 0.13735057413578033\n",
      "Epoch 2672, Loss: 0.27692461758852005, Final Batch Loss: 0.10991664975881577\n",
      "Epoch 2673, Loss: 0.27956423163414, Final Batch Loss: 0.12645307183265686\n",
      "Epoch 2674, Loss: 0.3472437113523483, Final Batch Loss: 0.18509531021118164\n",
      "Epoch 2675, Loss: 0.270604208111763, Final Batch Loss: 0.14046978950500488\n",
      "Epoch 2676, Loss: 0.3969041258096695, Final Batch Loss: 0.25075915455818176\n",
      "Epoch 2677, Loss: 0.33394765853881836, Final Batch Loss: 0.18614523112773895\n",
      "Epoch 2678, Loss: 0.341043084859848, Final Batch Loss: 0.17619599401950836\n",
      "Epoch 2679, Loss: 0.35346420109272003, Final Batch Loss: 0.18629491329193115\n",
      "Epoch 2680, Loss: 0.31737013161182404, Final Batch Loss: 0.1541278064250946\n",
      "Epoch 2681, Loss: 0.3867298811674118, Final Batch Loss: 0.22286011278629303\n",
      "Epoch 2682, Loss: 0.30379312485456467, Final Batch Loss: 0.17895759642124176\n",
      "Epoch 2683, Loss: 0.36972130835056305, Final Batch Loss: 0.2172970473766327\n",
      "Epoch 2684, Loss: 0.40485066175460815, Final Batch Loss: 0.254991739988327\n",
      "Epoch 2685, Loss: 0.3207987844944, Final Batch Loss: 0.17807725071907043\n",
      "Epoch 2686, Loss: 0.34730198979377747, Final Batch Loss: 0.13762187957763672\n",
      "Epoch 2687, Loss: 0.32383477687835693, Final Batch Loss: 0.17644496262073517\n",
      "Epoch 2688, Loss: 0.2914675921201706, Final Batch Loss: 0.14884211122989655\n",
      "Epoch 2689, Loss: 0.3203740417957306, Final Batch Loss: 0.13112400472164154\n",
      "Epoch 2690, Loss: 0.30984337627887726, Final Batch Loss: 0.13960112631320953\n",
      "Epoch 2691, Loss: 0.2777941972017288, Final Batch Loss: 0.12779544293880463\n",
      "Epoch 2692, Loss: 0.31753990799188614, Final Batch Loss: 0.12206793576478958\n",
      "Epoch 2693, Loss: 0.30589255690574646, Final Batch Loss: 0.1693657636642456\n",
      "Epoch 2694, Loss: 0.26265912503004074, Final Batch Loss: 0.11490296572446823\n",
      "Epoch 2695, Loss: 0.32233133912086487, Final Batch Loss: 0.1369968205690384\n",
      "Epoch 2696, Loss: 0.298634871840477, Final Batch Loss: 0.15152034163475037\n",
      "Epoch 2697, Loss: 0.34033484756946564, Final Batch Loss: 0.17504732310771942\n",
      "Epoch 2698, Loss: 0.2967512458562851, Final Batch Loss: 0.12881821393966675\n",
      "Epoch 2699, Loss: 0.3006044179201126, Final Batch Loss: 0.1488860696554184\n",
      "Epoch 2700, Loss: 0.31288833916187286, Final Batch Loss: 0.188826784491539\n",
      "Epoch 2701, Loss: 0.41239288449287415, Final Batch Loss: 0.24374906718730927\n",
      "Epoch 2702, Loss: 0.3866768926382065, Final Batch Loss: 0.19334065914154053\n",
      "Epoch 2703, Loss: 0.31387875974178314, Final Batch Loss: 0.12225842475891113\n",
      "Epoch 2704, Loss: 0.38822783529758453, Final Batch Loss: 0.1829511821269989\n",
      "Epoch 2705, Loss: 0.2857263684272766, Final Batch Loss: 0.1427287608385086\n",
      "Epoch 2706, Loss: 0.2915898486971855, Final Batch Loss: 0.16841892898082733\n",
      "Epoch 2707, Loss: 0.2860872894525528, Final Batch Loss: 0.131296768784523\n",
      "Epoch 2708, Loss: 0.34148409962654114, Final Batch Loss: 0.20278199017047882\n",
      "Epoch 2709, Loss: 0.3257947415113449, Final Batch Loss: 0.18885959684848785\n",
      "Epoch 2710, Loss: 0.3203226327896118, Final Batch Loss: 0.1222362369298935\n",
      "Epoch 2711, Loss: 0.34435710310935974, Final Batch Loss: 0.20432570576667786\n",
      "Epoch 2712, Loss: 0.31029917299747467, Final Batch Loss: 0.18253102898597717\n",
      "Epoch 2713, Loss: 0.26987324655056, Final Batch Loss: 0.15683849155902863\n",
      "Epoch 2714, Loss: 0.3068613037467003, Final Batch Loss: 0.18540464341640472\n",
      "Epoch 2715, Loss: 0.34024304151535034, Final Batch Loss: 0.17880450189113617\n",
      "Epoch 2716, Loss: 0.3297109976410866, Final Batch Loss: 0.20873264968395233\n",
      "Epoch 2717, Loss: 0.2728985920548439, Final Batch Loss: 0.10592121630907059\n",
      "Epoch 2718, Loss: 0.365007221698761, Final Batch Loss: 0.16395124793052673\n",
      "Epoch 2719, Loss: 0.35099880397319794, Final Batch Loss: 0.18125180900096893\n",
      "Epoch 2720, Loss: 0.24067529290914536, Final Batch Loss: 0.13836029171943665\n",
      "Epoch 2721, Loss: 0.275520995259285, Final Batch Loss: 0.1259448081254959\n",
      "Epoch 2722, Loss: 0.3712403327226639, Final Batch Loss: 0.2386084794998169\n",
      "Epoch 2723, Loss: 0.26226213574409485, Final Batch Loss: 0.112596794962883\n",
      "Epoch 2724, Loss: 0.30060969293117523, Final Batch Loss: 0.1461307555437088\n",
      "Epoch 2725, Loss: 0.33055637776851654, Final Batch Loss: 0.1784704625606537\n",
      "Epoch 2726, Loss: 0.365037277340889, Final Batch Loss: 0.21496018767356873\n",
      "Epoch 2727, Loss: 0.2919309139251709, Final Batch Loss: 0.11777105927467346\n",
      "Epoch 2728, Loss: 0.2789659798145294, Final Batch Loss: 0.1403420865535736\n",
      "Epoch 2729, Loss: 0.29060880839824677, Final Batch Loss: 0.1506018489599228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2730, Loss: 0.3289557844400406, Final Batch Loss: 0.14845138788223267\n",
      "Epoch 2731, Loss: 0.2886977791786194, Final Batch Loss: 0.1468982845544815\n",
      "Epoch 2732, Loss: 0.33120594918727875, Final Batch Loss: 0.16650541126728058\n",
      "Epoch 2733, Loss: 0.27459628880023956, Final Batch Loss: 0.1424945741891861\n",
      "Epoch 2734, Loss: 0.2701359689235687, Final Batch Loss: 0.11983904242515564\n",
      "Epoch 2735, Loss: 0.275224968791008, Final Batch Loss: 0.1400763988494873\n",
      "Epoch 2736, Loss: 0.3087294101715088, Final Batch Loss: 0.1574392467737198\n",
      "Epoch 2737, Loss: 0.2923135757446289, Final Batch Loss: 0.1277584433555603\n",
      "Epoch 2738, Loss: 0.335592657327652, Final Batch Loss: 0.16372956335544586\n",
      "Epoch 2739, Loss: 0.31981199979782104, Final Batch Loss: 0.16106747090816498\n",
      "Epoch 2740, Loss: 0.3036591559648514, Final Batch Loss: 0.18921159207820892\n",
      "Epoch 2741, Loss: 0.3389018177986145, Final Batch Loss: 0.20145735144615173\n",
      "Epoch 2742, Loss: 0.3126848489046097, Final Batch Loss: 0.20402851700782776\n",
      "Epoch 2743, Loss: 0.29754407703876495, Final Batch Loss: 0.16306595504283905\n",
      "Epoch 2744, Loss: 0.3086424916982651, Final Batch Loss: 0.13190409541130066\n",
      "Epoch 2745, Loss: 0.3448784798383713, Final Batch Loss: 0.1866658627986908\n",
      "Epoch 2746, Loss: 0.2930590659379959, Final Batch Loss: 0.12750259041786194\n",
      "Epoch 2747, Loss: 0.28051187098026276, Final Batch Loss: 0.1323796957731247\n",
      "Epoch 2748, Loss: 0.2409459576010704, Final Batch Loss: 0.11049213260412216\n",
      "Epoch 2749, Loss: 0.2988852560520172, Final Batch Loss: 0.1693921834230423\n",
      "Epoch 2750, Loss: 0.3230498433113098, Final Batch Loss: 0.15293949842453003\n",
      "Epoch 2751, Loss: 0.3585330545902252, Final Batch Loss: 0.214643195271492\n",
      "Epoch 2752, Loss: 0.3110816031694412, Final Batch Loss: 0.19654971361160278\n",
      "Epoch 2753, Loss: 0.3099816143512726, Final Batch Loss: 0.1526312232017517\n",
      "Epoch 2754, Loss: 0.257411852478981, Final Batch Loss: 0.14950300753116608\n",
      "Epoch 2755, Loss: 0.24472055584192276, Final Batch Loss: 0.10390400141477585\n",
      "Epoch 2756, Loss: 0.3724163621664047, Final Batch Loss: 0.24259917438030243\n",
      "Epoch 2757, Loss: 0.33651024103164673, Final Batch Loss: 0.1601075679063797\n",
      "Epoch 2758, Loss: 0.26166389882564545, Final Batch Loss: 0.1285255402326584\n",
      "Epoch 2759, Loss: 0.31344160437583923, Final Batch Loss: 0.18329423666000366\n",
      "Epoch 2760, Loss: 0.3383445292711258, Final Batch Loss: 0.183532252907753\n",
      "Epoch 2761, Loss: 0.2755274325609207, Final Batch Loss: 0.11608345806598663\n",
      "Epoch 2762, Loss: 0.3439241796731949, Final Batch Loss: 0.15122614800930023\n",
      "Epoch 2763, Loss: 0.2823397144675255, Final Batch Loss: 0.10816078633069992\n",
      "Epoch 2764, Loss: 0.3136277198791504, Final Batch Loss: 0.1368800401687622\n",
      "Epoch 2765, Loss: 0.29639995098114014, Final Batch Loss: 0.11454901099205017\n",
      "Epoch 2766, Loss: 0.3012446165084839, Final Batch Loss: 0.12956100702285767\n",
      "Epoch 2767, Loss: 0.2847958654165268, Final Batch Loss: 0.15633952617645264\n",
      "Epoch 2768, Loss: 0.2977410852909088, Final Batch Loss: 0.12798726558685303\n",
      "Epoch 2769, Loss: 0.3458164185285568, Final Batch Loss: 0.17062832415103912\n",
      "Epoch 2770, Loss: 0.2895033359527588, Final Batch Loss: 0.1396399438381195\n",
      "Epoch 2771, Loss: 0.26739107072353363, Final Batch Loss: 0.13365116715431213\n",
      "Epoch 2772, Loss: 0.29200348258018494, Final Batch Loss: 0.1378672868013382\n",
      "Epoch 2773, Loss: 0.28579435497522354, Final Batch Loss: 0.16659671068191528\n",
      "Epoch 2774, Loss: 0.36633290350437164, Final Batch Loss: 0.20181094110012054\n",
      "Epoch 2775, Loss: 0.325157955288887, Final Batch Loss: 0.13729502260684967\n",
      "Epoch 2776, Loss: 0.2966699004173279, Final Batch Loss: 0.13636787235736847\n",
      "Epoch 2777, Loss: 0.34940995275974274, Final Batch Loss: 0.19897769391536713\n",
      "Epoch 2778, Loss: 0.2945414334535599, Final Batch Loss: 0.16312003135681152\n",
      "Epoch 2779, Loss: 0.29844288527965546, Final Batch Loss: 0.1517619639635086\n",
      "Epoch 2780, Loss: 0.3338956832885742, Final Batch Loss: 0.17463365197181702\n",
      "Epoch 2781, Loss: 0.2834707498550415, Final Batch Loss: 0.1461499035358429\n",
      "Epoch 2782, Loss: 0.29701830446720123, Final Batch Loss: 0.15943709015846252\n",
      "Epoch 2783, Loss: 0.3203696608543396, Final Batch Loss: 0.1856258064508438\n",
      "Epoch 2784, Loss: 0.3040279597043991, Final Batch Loss: 0.1505614072084427\n",
      "Epoch 2785, Loss: 0.2826578691601753, Final Batch Loss: 0.08478569239377975\n",
      "Epoch 2786, Loss: 0.3147321492433548, Final Batch Loss: 0.1395593136548996\n",
      "Epoch 2787, Loss: 0.25771720707416534, Final Batch Loss: 0.09289456903934479\n",
      "Epoch 2788, Loss: 0.2770857959985733, Final Batch Loss: 0.10688133537769318\n",
      "Epoch 2789, Loss: 0.30162525177001953, Final Batch Loss: 0.17031052708625793\n",
      "Epoch 2790, Loss: 0.3050026148557663, Final Batch Loss: 0.13940414786338806\n",
      "Epoch 2791, Loss: 0.3148217499256134, Final Batch Loss: 0.17366689443588257\n",
      "Epoch 2792, Loss: 0.2818451076745987, Final Batch Loss: 0.09701378643512726\n",
      "Epoch 2793, Loss: 0.3210024833679199, Final Batch Loss: 0.18476484715938568\n",
      "Epoch 2794, Loss: 0.3347263038158417, Final Batch Loss: 0.2023165374994278\n",
      "Epoch 2795, Loss: 0.276242658495903, Final Batch Loss: 0.13807016611099243\n",
      "Epoch 2796, Loss: 0.25803106278181076, Final Batch Loss: 0.10898617655038834\n",
      "Epoch 2797, Loss: 0.30618447065353394, Final Batch Loss: 0.1318385750055313\n",
      "Epoch 2798, Loss: 0.31827835738658905, Final Batch Loss: 0.1600278913974762\n",
      "Epoch 2799, Loss: 0.25054778903722763, Final Batch Loss: 0.10517477244138718\n",
      "Epoch 2800, Loss: 0.3331770896911621, Final Batch Loss: 0.19482949376106262\n",
      "Epoch 2801, Loss: 0.2692098617553711, Final Batch Loss: 0.14065246284008026\n",
      "Epoch 2802, Loss: 0.2759930342435837, Final Batch Loss: 0.1312316507101059\n",
      "Epoch 2803, Loss: 0.35621732473373413, Final Batch Loss: 0.18893203139305115\n",
      "Epoch 2804, Loss: 0.3009985089302063, Final Batch Loss: 0.12652723491191864\n",
      "Epoch 2805, Loss: 0.3263206034898758, Final Batch Loss: 0.17034947872161865\n",
      "Epoch 2806, Loss: 0.2512528747320175, Final Batch Loss: 0.14825661480426788\n",
      "Epoch 2807, Loss: 0.46376433968544006, Final Batch Loss: 0.2756456434726715\n",
      "Epoch 2808, Loss: 0.3376075178384781, Final Batch Loss: 0.17673484981060028\n",
      "Epoch 2809, Loss: 0.2844362407922745, Final Batch Loss: 0.1246403306722641\n",
      "Epoch 2810, Loss: 0.33506081998348236, Final Batch Loss: 0.2051180601119995\n",
      "Epoch 2811, Loss: 0.2672384977340698, Final Batch Loss: 0.14753681421279907\n",
      "Epoch 2812, Loss: 0.4601050168275833, Final Batch Loss: 0.27326712012290955\n",
      "Epoch 2813, Loss: 0.2988230139017105, Final Batch Loss: 0.1397586315870285\n",
      "Epoch 2814, Loss: 0.33532819151878357, Final Batch Loss: 0.1381942480802536\n",
      "Epoch 2815, Loss: 0.3348757475614548, Final Batch Loss: 0.13425469398498535\n",
      "Epoch 2816, Loss: 0.299540713429451, Final Batch Loss: 0.1309783160686493\n",
      "Epoch 2817, Loss: 0.3047337681055069, Final Batch Loss: 0.17409150302410126\n",
      "Epoch 2818, Loss: 0.33874915540218353, Final Batch Loss: 0.19264470040798187\n",
      "Epoch 2819, Loss: 0.2607356458902359, Final Batch Loss: 0.11339740455150604\n",
      "Epoch 2820, Loss: 0.3066568523645401, Final Batch Loss: 0.1473267823457718\n",
      "Epoch 2821, Loss: 0.2547883167862892, Final Batch Loss: 0.14605960249900818\n",
      "Epoch 2822, Loss: 0.3691781014204025, Final Batch Loss: 0.15932606160640717\n",
      "Epoch 2823, Loss: 0.3411930948495865, Final Batch Loss: 0.13591589033603668\n",
      "Epoch 2824, Loss: 0.26944995671510696, Final Batch Loss: 0.11465638130903244\n",
      "Epoch 2825, Loss: 0.3427596688270569, Final Batch Loss: 0.17499832808971405\n",
      "Epoch 2826, Loss: 0.2732221782207489, Final Batch Loss: 0.1395760327577591\n",
      "Epoch 2827, Loss: 0.3182695060968399, Final Batch Loss: 0.17135179042816162\n",
      "Epoch 2828, Loss: 0.3639150410890579, Final Batch Loss: 0.2050079107284546\n",
      "Epoch 2829, Loss: 0.3151029646396637, Final Batch Loss: 0.12828253209590912\n",
      "Epoch 2830, Loss: 0.31717056035995483, Final Batch Loss: 0.14295713603496552\n",
      "Epoch 2831, Loss: 0.34362223744392395, Final Batch Loss: 0.17328809201717377\n",
      "Epoch 2832, Loss: 0.3258958011865616, Final Batch Loss: 0.20847977697849274\n",
      "Epoch 2833, Loss: 0.3192485570907593, Final Batch Loss: 0.1355654001235962\n",
      "Epoch 2834, Loss: 0.29022523760795593, Final Batch Loss: 0.14442090690135956\n",
      "Epoch 2835, Loss: 0.28686754405498505, Final Batch Loss: 0.15233924984931946\n",
      "Epoch 2836, Loss: 0.30326832830905914, Final Batch Loss: 0.10316339135169983\n",
      "Epoch 2837, Loss: 0.3218661770224571, Final Batch Loss: 0.20201748609542847\n",
      "Epoch 2838, Loss: 0.348746582865715, Final Batch Loss: 0.188233882188797\n",
      "Epoch 2839, Loss: 0.2694245055317879, Final Batch Loss: 0.12375757843255997\n",
      "Epoch 2840, Loss: 0.29942725598812103, Final Batch Loss: 0.16512182354927063\n",
      "Epoch 2841, Loss: 0.3890729546546936, Final Batch Loss: 0.19649763405323029\n",
      "Epoch 2842, Loss: 0.369797483086586, Final Batch Loss: 0.23221643269062042\n",
      "Epoch 2843, Loss: 0.3015487790107727, Final Batch Loss: 0.1753402054309845\n",
      "Epoch 2844, Loss: 0.3361135348677635, Final Batch Loss: 0.12418562918901443\n",
      "Epoch 2845, Loss: 0.33297084271907806, Final Batch Loss: 0.18125629425048828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2846, Loss: 0.34209710359573364, Final Batch Loss: 0.17330150306224823\n",
      "Epoch 2847, Loss: 0.29282285273075104, Final Batch Loss: 0.17234571278095245\n",
      "Epoch 2848, Loss: 0.2616283968091011, Final Batch Loss: 0.11322527378797531\n",
      "Epoch 2849, Loss: 0.3248720020055771, Final Batch Loss: 0.1746463179588318\n",
      "Epoch 2850, Loss: 0.24876776337623596, Final Batch Loss: 0.10202336311340332\n",
      "Epoch 2851, Loss: 0.2919240891933441, Final Batch Loss: 0.12816764414310455\n",
      "Epoch 2852, Loss: 0.27592049539089203, Final Batch Loss: 0.14386649429798126\n",
      "Epoch 2853, Loss: 0.2859603017568588, Final Batch Loss: 0.1337004452943802\n",
      "Epoch 2854, Loss: 0.28498387336730957, Final Batch Loss: 0.12130007147789001\n",
      "Epoch 2855, Loss: 0.33379003405570984, Final Batch Loss: 0.16002978384494781\n",
      "Epoch 2856, Loss: 0.31626658141613007, Final Batch Loss: 0.14047712087631226\n",
      "Epoch 2857, Loss: 0.29476191103458405, Final Batch Loss: 0.11551055312156677\n",
      "Epoch 2858, Loss: 0.3075520247220993, Final Batch Loss: 0.13031651079654694\n",
      "Epoch 2859, Loss: 0.31214411556720734, Final Batch Loss: 0.15234898030757904\n",
      "Epoch 2860, Loss: 0.30609942972660065, Final Batch Loss: 0.17571301758289337\n",
      "Epoch 2861, Loss: 0.2880360037088394, Final Batch Loss: 0.17233207821846008\n",
      "Epoch 2862, Loss: 0.2671974152326584, Final Batch Loss: 0.12878656387329102\n",
      "Epoch 2863, Loss: 0.31198587268590927, Final Batch Loss: 0.19227361679077148\n",
      "Epoch 2864, Loss: 0.30105166137218475, Final Batch Loss: 0.16223934292793274\n",
      "Epoch 2865, Loss: 0.33210310339927673, Final Batch Loss: 0.14000055193901062\n",
      "Epoch 2866, Loss: 0.3023401200771332, Final Batch Loss: 0.16955064237117767\n",
      "Epoch 2867, Loss: 0.29785698652267456, Final Batch Loss: 0.1652338206768036\n",
      "Epoch 2868, Loss: 0.30000242590904236, Final Batch Loss: 0.1687251180410385\n",
      "Epoch 2869, Loss: 0.2544432654976845, Final Batch Loss: 0.09354046732187271\n",
      "Epoch 2870, Loss: 0.25116387754678726, Final Batch Loss: 0.10750868171453476\n",
      "Epoch 2871, Loss: 0.26541582494974136, Final Batch Loss: 0.1135784313082695\n",
      "Epoch 2872, Loss: 0.2883898541331291, Final Batch Loss: 0.11061512678861618\n",
      "Epoch 2873, Loss: 0.2452666312456131, Final Batch Loss: 0.10447365045547485\n",
      "Epoch 2874, Loss: 0.2989168018102646, Final Batch Loss: 0.1555018126964569\n",
      "Epoch 2875, Loss: 0.29117827117443085, Final Batch Loss: 0.12834303081035614\n",
      "Epoch 2876, Loss: 0.274424709379673, Final Batch Loss: 0.1127697303891182\n",
      "Epoch 2877, Loss: 0.33935634791851044, Final Batch Loss: 0.1383545845746994\n",
      "Epoch 2878, Loss: 0.2920479476451874, Final Batch Loss: 0.12536364793777466\n",
      "Epoch 2879, Loss: 0.3176994025707245, Final Batch Loss: 0.16197694838047028\n",
      "Epoch 2880, Loss: 0.28129349648952484, Final Batch Loss: 0.13630171120166779\n",
      "Epoch 2881, Loss: 0.21883545070886612, Final Batch Loss: 0.08302690833806992\n",
      "Epoch 2882, Loss: 0.23367108404636383, Final Batch Loss: 0.11447358876466751\n",
      "Epoch 2883, Loss: 0.2354843020439148, Final Batch Loss: 0.11234692484140396\n",
      "Epoch 2884, Loss: 0.29954229295253754, Final Batch Loss: 0.1875557005405426\n",
      "Epoch 2885, Loss: 0.26302454620599747, Final Batch Loss: 0.10862594097852707\n",
      "Epoch 2886, Loss: 0.2940192222595215, Final Batch Loss: 0.1548767238855362\n",
      "Epoch 2887, Loss: 0.3539779335260391, Final Batch Loss: 0.1671997606754303\n",
      "Epoch 2888, Loss: 0.29907120764255524, Final Batch Loss: 0.12881779670715332\n",
      "Epoch 2889, Loss: 0.341124951839447, Final Batch Loss: 0.14779894053936005\n",
      "Epoch 2890, Loss: 0.2968651205301285, Final Batch Loss: 0.13171285390853882\n",
      "Epoch 2891, Loss: 0.2934531792998314, Final Batch Loss: 0.18511617183685303\n",
      "Epoch 2892, Loss: 0.32053282856941223, Final Batch Loss: 0.15532207489013672\n",
      "Epoch 2893, Loss: 0.2958632633090019, Final Batch Loss: 0.17606504261493683\n",
      "Epoch 2894, Loss: 0.3227521628141403, Final Batch Loss: 0.18930169939994812\n",
      "Epoch 2895, Loss: 0.27324922382831573, Final Batch Loss: 0.13008293509483337\n",
      "Epoch 2896, Loss: 0.3426029831171036, Final Batch Loss: 0.13686947524547577\n",
      "Epoch 2897, Loss: 0.2626819312572479, Final Batch Loss: 0.15757927298545837\n",
      "Epoch 2898, Loss: 0.2717249020934105, Final Batch Loss: 0.10388980060815811\n",
      "Epoch 2899, Loss: 0.2705533355474472, Final Batch Loss: 0.14318859577178955\n",
      "Epoch 2900, Loss: 0.29045288264751434, Final Batch Loss: 0.12862882018089294\n",
      "Epoch 2901, Loss: 0.3373133987188339, Final Batch Loss: 0.18755847215652466\n",
      "Epoch 2902, Loss: 0.33395765721797943, Final Batch Loss: 0.1817668229341507\n",
      "Epoch 2903, Loss: 0.2810487225651741, Final Batch Loss: 0.12301868945360184\n",
      "Epoch 2904, Loss: 0.30731290578842163, Final Batch Loss: 0.15808075666427612\n",
      "Epoch 2905, Loss: 0.23731273412704468, Final Batch Loss: 0.07388286292552948\n",
      "Epoch 2906, Loss: 0.2921307235956192, Final Batch Loss: 0.1192808449268341\n",
      "Epoch 2907, Loss: 0.2897193357348442, Final Batch Loss: 0.11524296551942825\n",
      "Epoch 2908, Loss: 0.32570119202136993, Final Batch Loss: 0.18231947720050812\n",
      "Epoch 2909, Loss: 0.29365939646959305, Final Batch Loss: 0.18944036960601807\n",
      "Epoch 2910, Loss: 0.29773804545402527, Final Batch Loss: 0.16903235018253326\n",
      "Epoch 2911, Loss: 0.28571319580078125, Final Batch Loss: 0.14114898443222046\n",
      "Epoch 2912, Loss: 0.2984687238931656, Final Batch Loss: 0.18425312638282776\n",
      "Epoch 2913, Loss: 0.3027888089418411, Final Batch Loss: 0.16119727492332458\n",
      "Epoch 2914, Loss: 0.2798394113779068, Final Batch Loss: 0.13906414806842804\n",
      "Epoch 2915, Loss: 0.2901533544063568, Final Batch Loss: 0.14308114349842072\n",
      "Epoch 2916, Loss: 0.2722171023488045, Final Batch Loss: 0.12092266231775284\n",
      "Epoch 2917, Loss: 0.3416467010974884, Final Batch Loss: 0.18484662473201752\n",
      "Epoch 2918, Loss: 0.40787462890148163, Final Batch Loss: 0.23356831073760986\n",
      "Epoch 2919, Loss: 0.2642935514450073, Final Batch Loss: 0.13222219049930573\n",
      "Epoch 2920, Loss: 0.2715260237455368, Final Batch Loss: 0.13117574155330658\n",
      "Epoch 2921, Loss: 0.2837286815047264, Final Batch Loss: 0.12256448715925217\n",
      "Epoch 2922, Loss: 0.3212161511182785, Final Batch Loss: 0.14861960709095\n",
      "Epoch 2923, Loss: 0.3195211887359619, Final Batch Loss: 0.09579053521156311\n",
      "Epoch 2924, Loss: 0.2736740857362747, Final Batch Loss: 0.11489015817642212\n",
      "Epoch 2925, Loss: 0.26290668547153473, Final Batch Loss: 0.15580487251281738\n",
      "Epoch 2926, Loss: 0.2522619292140007, Final Batch Loss: 0.11441836506128311\n",
      "Epoch 2927, Loss: 0.25936130434274673, Final Batch Loss: 0.12416040152311325\n",
      "Epoch 2928, Loss: 0.28607259690761566, Final Batch Loss: 0.12030944228172302\n",
      "Epoch 2929, Loss: 0.30638378858566284, Final Batch Loss: 0.16351763904094696\n",
      "Epoch 2930, Loss: 0.2803310602903366, Final Batch Loss: 0.11983156204223633\n",
      "Epoch 2931, Loss: 0.26856356859207153, Final Batch Loss: 0.14373473823070526\n",
      "Epoch 2932, Loss: 0.3007970601320267, Final Batch Loss: 0.1460397094488144\n",
      "Epoch 2933, Loss: 0.30980803072452545, Final Batch Loss: 0.1452203392982483\n",
      "Epoch 2934, Loss: 0.2590836435556412, Final Batch Loss: 0.12637481093406677\n",
      "Epoch 2935, Loss: 0.3558182716369629, Final Batch Loss: 0.20074746012687683\n",
      "Epoch 2936, Loss: 0.2791762724518776, Final Batch Loss: 0.16743846237659454\n",
      "Epoch 2937, Loss: 0.26957087963819504, Final Batch Loss: 0.12388774007558823\n",
      "Epoch 2938, Loss: 0.22915903478860855, Final Batch Loss: 0.07567157596349716\n",
      "Epoch 2939, Loss: 0.3121934235095978, Final Batch Loss: 0.16430659592151642\n",
      "Epoch 2940, Loss: 0.26280196011066437, Final Batch Loss: 0.16696086525917053\n",
      "Epoch 2941, Loss: 0.3541652113199234, Final Batch Loss: 0.17842607200145721\n",
      "Epoch 2942, Loss: 0.25967177003622055, Final Batch Loss: 0.11837836354970932\n",
      "Epoch 2943, Loss: 0.27297887206077576, Final Batch Loss: 0.14205782115459442\n",
      "Epoch 2944, Loss: 0.2700408697128296, Final Batch Loss: 0.12603837251663208\n",
      "Epoch 2945, Loss: 0.25049012154340744, Final Batch Loss: 0.09982352703809738\n",
      "Epoch 2946, Loss: 0.3150889575481415, Final Batch Loss: 0.157938152551651\n",
      "Epoch 2947, Loss: 0.3003991097211838, Final Batch Loss: 0.15781676769256592\n",
      "Epoch 2948, Loss: 0.29179391264915466, Final Batch Loss: 0.14311544597148895\n",
      "Epoch 2949, Loss: 0.2871672958135605, Final Batch Loss: 0.16670650243759155\n",
      "Epoch 2950, Loss: 0.2862388342618942, Final Batch Loss: 0.13247136771678925\n",
      "Epoch 2951, Loss: 0.2509187310934067, Final Batch Loss: 0.10505111515522003\n",
      "Epoch 2952, Loss: 0.28502097725868225, Final Batch Loss: 0.15101651847362518\n",
      "Epoch 2953, Loss: 0.29458416998386383, Final Batch Loss: 0.17012542486190796\n",
      "Epoch 2954, Loss: 0.2728338837623596, Final Batch Loss: 0.12732894718647003\n",
      "Epoch 2955, Loss: 0.2999868094921112, Final Batch Loss: 0.16522002220153809\n",
      "Epoch 2956, Loss: 0.25945664942264557, Final Batch Loss: 0.15197177231311798\n",
      "Epoch 2957, Loss: 0.3440661281347275, Final Batch Loss: 0.18215124309062958\n",
      "Epoch 2958, Loss: 0.31162095069885254, Final Batch Loss: 0.1915998011827469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2959, Loss: 0.3143477290868759, Final Batch Loss: 0.16580289602279663\n",
      "Epoch 2960, Loss: 0.23163218051195145, Final Batch Loss: 0.10832709819078445\n",
      "Epoch 2961, Loss: 0.3095101863145828, Final Batch Loss: 0.1691693663597107\n",
      "Epoch 2962, Loss: 0.2384650632739067, Final Batch Loss: 0.09822555631399155\n",
      "Epoch 2963, Loss: 0.26096274703741074, Final Batch Loss: 0.1614392250776291\n",
      "Epoch 2964, Loss: 0.3390984684228897, Final Batch Loss: 0.165390744805336\n",
      "Epoch 2965, Loss: 0.30870120227336884, Final Batch Loss: 0.13562509417533875\n",
      "Epoch 2966, Loss: 0.30805082619190216, Final Batch Loss: 0.1774401217699051\n",
      "Epoch 2967, Loss: 0.23847321420907974, Final Batch Loss: 0.09681185334920883\n",
      "Epoch 2968, Loss: 0.30268795788288116, Final Batch Loss: 0.17515936493873596\n",
      "Epoch 2969, Loss: 0.3717540577054024, Final Batch Loss: 0.1238023117184639\n",
      "Epoch 2970, Loss: 0.31530818343162537, Final Batch Loss: 0.16179272532463074\n",
      "Epoch 2971, Loss: 0.3282182663679123, Final Batch Loss: 0.17873090505599976\n",
      "Epoch 2972, Loss: 0.3220871686935425, Final Batch Loss: 0.1965278685092926\n",
      "Epoch 2973, Loss: 0.2624540850520134, Final Batch Loss: 0.10702183097600937\n",
      "Epoch 2974, Loss: 0.33150774240493774, Final Batch Loss: 0.19550897181034088\n",
      "Epoch 2975, Loss: 0.24369272589683533, Final Batch Loss: 0.13894157111644745\n",
      "Epoch 2976, Loss: 0.2740570604801178, Final Batch Loss: 0.14494509994983673\n",
      "Epoch 2977, Loss: 0.34918468445539474, Final Batch Loss: 0.2559015452861786\n",
      "Epoch 2978, Loss: 0.40059587359428406, Final Batch Loss: 0.2834421396255493\n",
      "Epoch 2979, Loss: 0.31211356818675995, Final Batch Loss: 0.1473826766014099\n",
      "Epoch 2980, Loss: 0.3428978994488716, Final Batch Loss: 0.22107400000095367\n",
      "Epoch 2981, Loss: 0.30812056362628937, Final Batch Loss: 0.13060548901557922\n",
      "Epoch 2982, Loss: 0.2565188929438591, Final Batch Loss: 0.13222475349903107\n",
      "Epoch 2983, Loss: 0.3228573501110077, Final Batch Loss: 0.1488148421049118\n",
      "Epoch 2984, Loss: 0.35949312150478363, Final Batch Loss: 0.2078315168619156\n",
      "Epoch 2985, Loss: 0.28418000042438507, Final Batch Loss: 0.1306457817554474\n",
      "Epoch 2986, Loss: 0.32001541554927826, Final Batch Loss: 0.15562744438648224\n",
      "Epoch 2987, Loss: 0.2776148021221161, Final Batch Loss: 0.15283021330833435\n",
      "Epoch 2988, Loss: 0.25548288971185684, Final Batch Loss: 0.10772448033094406\n",
      "Epoch 2989, Loss: 0.3023796081542969, Final Batch Loss: 0.16888977587223053\n",
      "Epoch 2990, Loss: 0.31714367866516113, Final Batch Loss: 0.1479736864566803\n",
      "Epoch 2991, Loss: 0.3258377015590668, Final Batch Loss: 0.16124221682548523\n",
      "Epoch 2992, Loss: 0.26684246212244034, Final Batch Loss: 0.09520354121923447\n",
      "Epoch 2993, Loss: 0.3149278610944748, Final Batch Loss: 0.15208205580711365\n",
      "Epoch 2994, Loss: 0.30646221339702606, Final Batch Loss: 0.11740052700042725\n",
      "Epoch 2995, Loss: 0.2731185853481293, Final Batch Loss: 0.11630481481552124\n",
      "Epoch 2996, Loss: 0.34434060752391815, Final Batch Loss: 0.1670396625995636\n",
      "Epoch 2997, Loss: 0.2684158980846405, Final Batch Loss: 0.12744081020355225\n",
      "Epoch 2998, Loss: 0.29629142582416534, Final Batch Loss: 0.12742990255355835\n",
      "Epoch 2999, Loss: 0.33163218200206757, Final Batch Loss: 0.13416661322116852\n",
      "Epoch 3000, Loss: 0.290026031434536, Final Batch Loss: 0.11203429847955704\n",
      "Epoch 3001, Loss: 0.27627672255039215, Final Batch Loss: 0.13586851954460144\n",
      "Epoch 3002, Loss: 0.29127512872219086, Final Batch Loss: 0.13951775431632996\n",
      "Epoch 3003, Loss: 0.3606879860162735, Final Batch Loss: 0.18363617360591888\n",
      "Epoch 3004, Loss: 0.2782518118619919, Final Batch Loss: 0.11723259091377258\n",
      "Epoch 3005, Loss: 0.34181270003318787, Final Batch Loss: 0.1518295258283615\n",
      "Epoch 3006, Loss: 0.2691652625799179, Final Batch Loss: 0.13154838979244232\n",
      "Epoch 3007, Loss: 0.2736435756087303, Final Batch Loss: 0.1130872592329979\n",
      "Epoch 3008, Loss: 0.2712816447019577, Final Batch Loss: 0.13216303288936615\n",
      "Epoch 3009, Loss: 0.23075495660305023, Final Batch Loss: 0.08511054515838623\n",
      "Epoch 3010, Loss: 0.2995438128709793, Final Batch Loss: 0.14391347765922546\n",
      "Epoch 3011, Loss: 0.23158514499664307, Final Batch Loss: 0.10739286243915558\n",
      "Epoch 3012, Loss: 0.27204348146915436, Final Batch Loss: 0.14959833025932312\n",
      "Epoch 3013, Loss: 0.24219150096178055, Final Batch Loss: 0.1371334195137024\n",
      "Epoch 3014, Loss: 0.22606537491083145, Final Batch Loss: 0.115244559943676\n",
      "Epoch 3015, Loss: 0.2994772791862488, Final Batch Loss: 0.13514544069766998\n",
      "Epoch 3016, Loss: 0.28384605050086975, Final Batch Loss: 0.15754805505275726\n",
      "Epoch 3017, Loss: 0.35469451546669006, Final Batch Loss: 0.11865022778511047\n",
      "Epoch 3018, Loss: 0.30695852637290955, Final Batch Loss: 0.15921162068843842\n",
      "Epoch 3019, Loss: 0.311252161860466, Final Batch Loss: 0.17097719013690948\n",
      "Epoch 3020, Loss: 0.26970067620277405, Final Batch Loss: 0.14318019151687622\n",
      "Epoch 3021, Loss: 0.2527233809232712, Final Batch Loss: 0.1056414395570755\n",
      "Epoch 3022, Loss: 0.2921605110168457, Final Batch Loss: 0.16488783061504364\n",
      "Epoch 3023, Loss: 0.2670954614877701, Final Batch Loss: 0.1282075047492981\n",
      "Epoch 3024, Loss: 0.2725508511066437, Final Batch Loss: 0.1557541787624359\n",
      "Epoch 3025, Loss: 0.24830582737922668, Final Batch Loss: 0.12490935623645782\n",
      "Epoch 3026, Loss: 0.3510926589369774, Final Batch Loss: 0.23256707191467285\n",
      "Epoch 3027, Loss: 0.26800893247127533, Final Batch Loss: 0.13253068923950195\n",
      "Epoch 3028, Loss: 0.2726529687643051, Final Batch Loss: 0.11222277581691742\n",
      "Epoch 3029, Loss: 0.25716739147901535, Final Batch Loss: 0.10946657508611679\n",
      "Epoch 3030, Loss: 0.22242379188537598, Final Batch Loss: 0.09687311947345734\n",
      "Epoch 3031, Loss: 0.2519991099834442, Final Batch Loss: 0.10854718089103699\n",
      "Epoch 3032, Loss: 0.3300837278366089, Final Batch Loss: 0.18207216262817383\n",
      "Epoch 3033, Loss: 0.3186522126197815, Final Batch Loss: 0.16215261816978455\n",
      "Epoch 3034, Loss: 0.2889668941497803, Final Batch Loss: 0.12652401626110077\n",
      "Epoch 3035, Loss: 0.3247302919626236, Final Batch Loss: 0.1490674763917923\n",
      "Epoch 3036, Loss: 0.3549301475286484, Final Batch Loss: 0.1899888813495636\n",
      "Epoch 3037, Loss: 0.29260285198688507, Final Batch Loss: 0.16285638511180878\n",
      "Epoch 3038, Loss: 0.2662185952067375, Final Batch Loss: 0.11905325204133987\n",
      "Epoch 3039, Loss: 0.3666646182537079, Final Batch Loss: 0.21703042089939117\n",
      "Epoch 3040, Loss: 0.2868308275938034, Final Batch Loss: 0.13534654676914215\n",
      "Epoch 3041, Loss: 0.3298606425523758, Final Batch Loss: 0.15887074172496796\n",
      "Epoch 3042, Loss: 0.2699763551354408, Final Batch Loss: 0.17035093903541565\n",
      "Epoch 3043, Loss: 0.28109627217054367, Final Batch Loss: 0.12256067246198654\n",
      "Epoch 3044, Loss: 0.2966940179467201, Final Batch Loss: 0.10411480814218521\n",
      "Epoch 3045, Loss: 0.31416909396648407, Final Batch Loss: 0.15407145023345947\n",
      "Epoch 3046, Loss: 0.33300814032554626, Final Batch Loss: 0.1454099416732788\n",
      "Epoch 3047, Loss: 0.2830258011817932, Final Batch Loss: 0.15634708106517792\n",
      "Epoch 3048, Loss: 0.338890016078949, Final Batch Loss: 0.19187042117118835\n",
      "Epoch 3049, Loss: 0.25061848759651184, Final Batch Loss: 0.12964174151420593\n",
      "Epoch 3050, Loss: 0.27379828691482544, Final Batch Loss: 0.14293614029884338\n",
      "Epoch 3051, Loss: 0.2775246351957321, Final Batch Loss: 0.13480879366397858\n",
      "Epoch 3052, Loss: 0.2672882452607155, Final Batch Loss: 0.15441031754016876\n",
      "Epoch 3053, Loss: 0.2896495908498764, Final Batch Loss: 0.13227131962776184\n",
      "Epoch 3054, Loss: 0.28545621037483215, Final Batch Loss: 0.1479426473379135\n",
      "Epoch 3055, Loss: 0.2629005089402199, Final Batch Loss: 0.12305707484483719\n",
      "Epoch 3056, Loss: 0.26268406212329865, Final Batch Loss: 0.127023383975029\n",
      "Epoch 3057, Loss: 0.30138827860355377, Final Batch Loss: 0.18430261313915253\n",
      "Epoch 3058, Loss: 0.27404847741127014, Final Batch Loss: 0.1527874767780304\n",
      "Epoch 3059, Loss: 0.25476865470409393, Final Batch Loss: 0.13186800479888916\n",
      "Epoch 3060, Loss: 0.3003685772418976, Final Batch Loss: 0.1504058986902237\n",
      "Epoch 3061, Loss: 0.26238008588552475, Final Batch Loss: 0.12492895871400833\n",
      "Epoch 3062, Loss: 0.2884353697299957, Final Batch Loss: 0.15969793498516083\n",
      "Epoch 3063, Loss: 0.3458520323038101, Final Batch Loss: 0.20328755676746368\n",
      "Epoch 3064, Loss: 0.39967772364616394, Final Batch Loss: 0.16774670779705048\n",
      "Epoch 3065, Loss: 0.3315622955560684, Final Batch Loss: 0.17927725613117218\n",
      "Epoch 3066, Loss: 0.35405613481998444, Final Batch Loss: 0.1832345575094223\n",
      "Epoch 3067, Loss: 0.2886899635195732, Final Batch Loss: 0.16777271032333374\n",
      "Epoch 3068, Loss: 0.2941083312034607, Final Batch Loss: 0.13571694493293762\n",
      "Epoch 3069, Loss: 0.32001493871212006, Final Batch Loss: 0.17231956124305725\n",
      "Epoch 3070, Loss: 0.3289410471916199, Final Batch Loss: 0.19101852178573608\n",
      "Epoch 3071, Loss: 0.2887962907552719, Final Batch Loss: 0.14184711873531342\n",
      "Epoch 3072, Loss: 0.2278224155306816, Final Batch Loss: 0.1179201677441597\n",
      "Epoch 3073, Loss: 0.30021022260189056, Final Batch Loss: 0.16977672278881073\n",
      "Epoch 3074, Loss: 0.28127630054950714, Final Batch Loss: 0.14866463840007782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3075, Loss: 0.27031461894512177, Final Batch Loss: 0.13426001369953156\n",
      "Epoch 3076, Loss: 0.2753375545144081, Final Batch Loss: 0.15542416274547577\n",
      "Epoch 3077, Loss: 0.3019886612892151, Final Batch Loss: 0.14514075219631195\n",
      "Epoch 3078, Loss: 0.23892714083194733, Final Batch Loss: 0.10723106563091278\n",
      "Epoch 3079, Loss: 0.35412634909152985, Final Batch Loss: 0.21952560544013977\n",
      "Epoch 3080, Loss: 0.2627236992120743, Final Batch Loss: 0.14178770780563354\n",
      "Epoch 3081, Loss: 0.2999202013015747, Final Batch Loss: 0.1296035647392273\n",
      "Epoch 3082, Loss: 0.2906268388032913, Final Batch Loss: 0.14329257607460022\n",
      "Epoch 3083, Loss: 0.3309953212738037, Final Batch Loss: 0.14706948399543762\n",
      "Epoch 3084, Loss: 0.23930057138204575, Final Batch Loss: 0.1124647781252861\n",
      "Epoch 3085, Loss: 0.33507344126701355, Final Batch Loss: 0.158718079328537\n",
      "Epoch 3086, Loss: 0.2735278755426407, Final Batch Loss: 0.12697501480579376\n",
      "Epoch 3087, Loss: 0.3069872111082077, Final Batch Loss: 0.1277942955493927\n",
      "Epoch 3088, Loss: 0.25492551177740097, Final Batch Loss: 0.1176222488284111\n",
      "Epoch 3089, Loss: 0.22664029896259308, Final Batch Loss: 0.1266535222530365\n",
      "Epoch 3090, Loss: 0.24031922221183777, Final Batch Loss: 0.1403118520975113\n",
      "Epoch 3091, Loss: 0.26457512378692627, Final Batch Loss: 0.11773158609867096\n",
      "Epoch 3092, Loss: 0.2877192199230194, Final Batch Loss: 0.1339886486530304\n",
      "Epoch 3093, Loss: 0.3164481669664383, Final Batch Loss: 0.16247786581516266\n",
      "Epoch 3094, Loss: 0.2652345225214958, Final Batch Loss: 0.1534954309463501\n",
      "Epoch 3095, Loss: 0.2700238525867462, Final Batch Loss: 0.1485481858253479\n",
      "Epoch 3096, Loss: 0.32935893535614014, Final Batch Loss: 0.195185124874115\n",
      "Epoch 3097, Loss: 0.2886423021554947, Final Batch Loss: 0.12277373671531677\n",
      "Epoch 3098, Loss: 0.24070598930120468, Final Batch Loss: 0.12222389876842499\n",
      "Epoch 3099, Loss: 0.36320264637470245, Final Batch Loss: 0.20436784625053406\n",
      "Epoch 3100, Loss: 0.3343893140554428, Final Batch Loss: 0.2009911686182022\n",
      "Epoch 3101, Loss: 0.22340135276317596, Final Batch Loss: 0.08988618850708008\n",
      "Epoch 3102, Loss: 0.30101484805345535, Final Batch Loss: 0.1963590383529663\n",
      "Epoch 3103, Loss: 0.2812877893447876, Final Batch Loss: 0.1471204161643982\n",
      "Epoch 3104, Loss: 0.2598968893289566, Final Batch Loss: 0.11660504341125488\n",
      "Epoch 3105, Loss: 0.2934176027774811, Final Batch Loss: 0.11544285714626312\n",
      "Epoch 3106, Loss: 0.2590320184826851, Final Batch Loss: 0.14844746887683868\n",
      "Epoch 3107, Loss: 0.33657142519950867, Final Batch Loss: 0.18268927931785583\n",
      "Epoch 3108, Loss: 0.3036688268184662, Final Batch Loss: 0.14440229535102844\n",
      "Epoch 3109, Loss: 0.3028675243258476, Final Batch Loss: 0.1962055265903473\n",
      "Epoch 3110, Loss: 0.2737533152103424, Final Batch Loss: 0.13426019251346588\n",
      "Epoch 3111, Loss: 0.20185089111328125, Final Batch Loss: 0.11481659859418869\n",
      "Epoch 3112, Loss: 0.24830880761146545, Final Batch Loss: 0.11226633191108704\n",
      "Epoch 3113, Loss: 0.25397633761167526, Final Batch Loss: 0.10962911695241928\n",
      "Epoch 3114, Loss: 0.32869309186935425, Final Batch Loss: 0.190588116645813\n",
      "Epoch 3115, Loss: 0.3297208249568939, Final Batch Loss: 0.1945628523826599\n",
      "Epoch 3116, Loss: 0.3067779093980789, Final Batch Loss: 0.15394656360149384\n",
      "Epoch 3117, Loss: 0.26320023834705353, Final Batch Loss: 0.11835220456123352\n",
      "Epoch 3118, Loss: 0.24820248037576675, Final Batch Loss: 0.10509071499109268\n",
      "Epoch 3119, Loss: 0.2833060249686241, Final Batch Loss: 0.12009244412183762\n",
      "Epoch 3120, Loss: 0.29128001630306244, Final Batch Loss: 0.14428256452083588\n",
      "Epoch 3121, Loss: 0.25870106369256973, Final Batch Loss: 0.11998762935400009\n",
      "Epoch 3122, Loss: 0.283602237701416, Final Batch Loss: 0.13186736404895782\n",
      "Epoch 3123, Loss: 0.2763325572013855, Final Batch Loss: 0.12667463719844818\n",
      "Epoch 3124, Loss: 0.24874495714902878, Final Batch Loss: 0.1345389038324356\n",
      "Epoch 3125, Loss: 0.22373495250940323, Final Batch Loss: 0.10536455363035202\n",
      "Epoch 3126, Loss: 0.223036527633667, Final Batch Loss: 0.10441914200782776\n",
      "Epoch 3127, Loss: 0.24634843319654465, Final Batch Loss: 0.09514769166707993\n",
      "Epoch 3128, Loss: 0.2745904475450516, Final Batch Loss: 0.13847306370735168\n",
      "Epoch 3129, Loss: 0.3726080358028412, Final Batch Loss: 0.1808176040649414\n",
      "Epoch 3130, Loss: 0.2618608623743057, Final Batch Loss: 0.16508285701274872\n",
      "Epoch 3131, Loss: 0.26923054456710815, Final Batch Loss: 0.13555100560188293\n",
      "Epoch 3132, Loss: 0.24338718503713608, Final Batch Loss: 0.14052540063858032\n",
      "Epoch 3133, Loss: 0.26127272844314575, Final Batch Loss: 0.12946757674217224\n",
      "Epoch 3134, Loss: 0.2380121573805809, Final Batch Loss: 0.1108168289065361\n",
      "Epoch 3135, Loss: 0.28578267991542816, Final Batch Loss: 0.15647253394126892\n",
      "Epoch 3136, Loss: 0.3552049547433853, Final Batch Loss: 0.1696442812681198\n",
      "Epoch 3137, Loss: 0.334809347987175, Final Batch Loss: 0.18005628883838654\n",
      "Epoch 3138, Loss: 0.2857021540403366, Final Batch Loss: 0.15935327112674713\n",
      "Epoch 3139, Loss: 0.29383009672164917, Final Batch Loss: 0.13623575866222382\n",
      "Epoch 3140, Loss: 0.27709270268678665, Final Batch Loss: 0.15495680272579193\n",
      "Epoch 3141, Loss: 0.30235157907009125, Final Batch Loss: 0.11198021471500397\n",
      "Epoch 3142, Loss: 0.2922884002327919, Final Batch Loss: 0.12288597971200943\n",
      "Epoch 3143, Loss: 0.29312409460544586, Final Batch Loss: 0.15397261083126068\n",
      "Epoch 3144, Loss: 0.2597266063094139, Final Batch Loss: 0.13488559424877167\n",
      "Epoch 3145, Loss: 0.2936256378889084, Final Batch Loss: 0.19868505001068115\n",
      "Epoch 3146, Loss: 0.23457257449626923, Final Batch Loss: 0.11380092054605484\n",
      "Epoch 3147, Loss: 0.23976349830627441, Final Batch Loss: 0.14359237253665924\n",
      "Epoch 3148, Loss: 0.291954904794693, Final Batch Loss: 0.1277182698249817\n",
      "Epoch 3149, Loss: 0.26838693022727966, Final Batch Loss: 0.10932470858097076\n",
      "Epoch 3150, Loss: 0.3148888349533081, Final Batch Loss: 0.16111797094345093\n",
      "Epoch 3151, Loss: 0.37935996800661087, Final Batch Loss: 0.2573024332523346\n",
      "Epoch 3152, Loss: 0.22610479593276978, Final Batch Loss: 0.10558860003948212\n",
      "Epoch 3153, Loss: 0.3033537119626999, Final Batch Loss: 0.12790915369987488\n",
      "Epoch 3154, Loss: 0.2128489911556244, Final Batch Loss: 0.1009620875120163\n",
      "Epoch 3155, Loss: 0.2893627807497978, Final Batch Loss: 0.16586695611476898\n",
      "Epoch 3156, Loss: 0.26363280415534973, Final Batch Loss: 0.11652767658233643\n",
      "Epoch 3157, Loss: 0.32920151948928833, Final Batch Loss: 0.1661759912967682\n",
      "Epoch 3158, Loss: 0.26741375029087067, Final Batch Loss: 0.1376914381980896\n",
      "Epoch 3159, Loss: 0.24752407521009445, Final Batch Loss: 0.14408136904239655\n",
      "Epoch 3160, Loss: 0.2757625877857208, Final Batch Loss: 0.14319968223571777\n",
      "Epoch 3161, Loss: 0.24919585138559341, Final Batch Loss: 0.10170047730207443\n",
      "Epoch 3162, Loss: 0.28740617632865906, Final Batch Loss: 0.14462026953697205\n",
      "Epoch 3163, Loss: 0.31001465022563934, Final Batch Loss: 0.15253686904907227\n",
      "Epoch 3164, Loss: 0.2637151628732681, Final Batch Loss: 0.12201480567455292\n",
      "Epoch 3165, Loss: 0.3071351796388626, Final Batch Loss: 0.13407698273658752\n",
      "Epoch 3166, Loss: 0.2791099101305008, Final Batch Loss: 0.18059778213500977\n",
      "Epoch 3167, Loss: 0.3172977566719055, Final Batch Loss: 0.17291760444641113\n",
      "Epoch 3168, Loss: 0.2789660394191742, Final Batch Loss: 0.1369786113500595\n",
      "Epoch 3169, Loss: 0.2649708166718483, Final Batch Loss: 0.10835481435060501\n",
      "Epoch 3170, Loss: 0.2517569661140442, Final Batch Loss: 0.12127061188220978\n",
      "Epoch 3171, Loss: 0.31722167134284973, Final Batch Loss: 0.21040739119052887\n",
      "Epoch 3172, Loss: 0.2946799099445343, Final Batch Loss: 0.17190058529376984\n",
      "Epoch 3173, Loss: 0.2692469209432602, Final Batch Loss: 0.1362684816122055\n",
      "Epoch 3174, Loss: 0.25995439291000366, Final Batch Loss: 0.13059891760349274\n",
      "Epoch 3175, Loss: 0.2706202119588852, Final Batch Loss: 0.12078757584095001\n",
      "Epoch 3176, Loss: 0.3074803948402405, Final Batch Loss: 0.14844432473182678\n",
      "Epoch 3177, Loss: 0.3034153878688812, Final Batch Loss: 0.14826737344264984\n",
      "Epoch 3178, Loss: 0.3004623204469681, Final Batch Loss: 0.1535002738237381\n",
      "Epoch 3179, Loss: 0.3130760043859482, Final Batch Loss: 0.177362859249115\n",
      "Epoch 3180, Loss: 0.2777373343706131, Final Batch Loss: 0.11848297715187073\n",
      "Epoch 3181, Loss: 0.288260355591774, Final Batch Loss: 0.14943765103816986\n",
      "Epoch 3182, Loss: 0.3302529901266098, Final Batch Loss: 0.16030962765216827\n",
      "Epoch 3183, Loss: 0.28128574788570404, Final Batch Loss: 0.12919530272483826\n",
      "Epoch 3184, Loss: 0.26851756125688553, Final Batch Loss: 0.09237577766180038\n",
      "Epoch 3185, Loss: 0.22502686083316803, Final Batch Loss: 0.09556412696838379\n",
      "Epoch 3186, Loss: 0.2336951345205307, Final Batch Loss: 0.07796527445316315\n",
      "Epoch 3187, Loss: 0.2819681912660599, Final Batch Loss: 0.1441684365272522\n",
      "Epoch 3188, Loss: 0.33683915436267853, Final Batch Loss: 0.1772075891494751\n",
      "Epoch 3189, Loss: 0.27170033752918243, Final Batch Loss: 0.13259252905845642\n",
      "Epoch 3190, Loss: 0.2818047106266022, Final Batch Loss: 0.0992869883775711\n",
      "Epoch 3191, Loss: 0.34334293007850647, Final Batch Loss: 0.13830356299877167\n",
      "Epoch 3192, Loss: 0.2986494451761246, Final Batch Loss: 0.14584343135356903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3193, Loss: 0.34547531604766846, Final Batch Loss: 0.17275112867355347\n",
      "Epoch 3194, Loss: 0.3470298945903778, Final Batch Loss: 0.12784871459007263\n",
      "Epoch 3195, Loss: 0.29241184145212173, Final Batch Loss: 0.10585299879312515\n",
      "Epoch 3196, Loss: 0.2914871275424957, Final Batch Loss: 0.13648399710655212\n",
      "Epoch 3197, Loss: 0.31149742007255554, Final Batch Loss: 0.15175186097621918\n",
      "Epoch 3198, Loss: 0.27439816296100616, Final Batch Loss: 0.12460440397262573\n",
      "Epoch 3199, Loss: 0.3085368201136589, Final Batch Loss: 0.18363375961780548\n",
      "Epoch 3200, Loss: 0.25979815423488617, Final Batch Loss: 0.1170484721660614\n",
      "Epoch 3201, Loss: 0.27018001675605774, Final Batch Loss: 0.1550486534833908\n",
      "Epoch 3202, Loss: 0.28141041845083237, Final Batch Loss: 0.12242414802312851\n",
      "Epoch 3203, Loss: 0.2878524214029312, Final Batch Loss: 0.12012642621994019\n",
      "Epoch 3204, Loss: 0.315628245472908, Final Batch Loss: 0.16805222630500793\n",
      "Epoch 3205, Loss: 0.3261774927377701, Final Batch Loss: 0.18199659883975983\n",
      "Epoch 3206, Loss: 0.28537343442440033, Final Batch Loss: 0.1472410410642624\n",
      "Epoch 3207, Loss: 0.27263666689395905, Final Batch Loss: 0.15550369024276733\n",
      "Epoch 3208, Loss: 0.23178286105394363, Final Batch Loss: 0.11020135134458542\n",
      "Epoch 3209, Loss: 0.3027571737766266, Final Batch Loss: 0.16619981825351715\n",
      "Epoch 3210, Loss: 0.2674815505743027, Final Batch Loss: 0.1362440288066864\n",
      "Epoch 3211, Loss: 0.3003416061401367, Final Batch Loss: 0.15182730555534363\n",
      "Epoch 3212, Loss: 0.3053550720214844, Final Batch Loss: 0.133666530251503\n",
      "Epoch 3213, Loss: 0.2267293781042099, Final Batch Loss: 0.12554067373275757\n",
      "Epoch 3214, Loss: 0.270144447684288, Final Batch Loss: 0.12147243320941925\n",
      "Epoch 3215, Loss: 0.26930948346853256, Final Batch Loss: 0.16051587462425232\n",
      "Epoch 3216, Loss: 0.23744896799325943, Final Batch Loss: 0.11619597673416138\n",
      "Epoch 3217, Loss: 0.2572115883231163, Final Batch Loss: 0.13929307460784912\n",
      "Epoch 3218, Loss: 0.264578253030777, Final Batch Loss: 0.1250510960817337\n",
      "Epoch 3219, Loss: 0.26773491501808167, Final Batch Loss: 0.12747293710708618\n",
      "Epoch 3220, Loss: 0.25705404579639435, Final Batch Loss: 0.14145515859127045\n",
      "Epoch 3221, Loss: 0.26900386065244675, Final Batch Loss: 0.1552964597940445\n",
      "Epoch 3222, Loss: 0.2537572309374809, Final Batch Loss: 0.13828429579734802\n",
      "Epoch 3223, Loss: 0.28428635001182556, Final Batch Loss: 0.14467833936214447\n",
      "Epoch 3224, Loss: 0.2677939161658287, Final Batch Loss: 0.11326663941144943\n",
      "Epoch 3225, Loss: 0.30219490826129913, Final Batch Loss: 0.14541852474212646\n",
      "Epoch 3226, Loss: 0.23914168030023575, Final Batch Loss: 0.13618655502796173\n",
      "Epoch 3227, Loss: 0.20599832385778427, Final Batch Loss: 0.08621735125780106\n",
      "Epoch 3228, Loss: 0.27235518395900726, Final Batch Loss: 0.17548245191574097\n",
      "Epoch 3229, Loss: 0.264502614736557, Final Batch Loss: 0.13678745925426483\n",
      "Epoch 3230, Loss: 0.2814457044005394, Final Batch Loss: 0.1610410362482071\n",
      "Epoch 3231, Loss: 0.25792156159877777, Final Batch Loss: 0.12652231752872467\n",
      "Epoch 3232, Loss: 0.25915632396936417, Final Batch Loss: 0.09069576114416122\n",
      "Epoch 3233, Loss: 0.25534767657518387, Final Batch Loss: 0.12449682503938675\n",
      "Epoch 3234, Loss: 0.25452104210853577, Final Batch Loss: 0.10982973873615265\n",
      "Epoch 3235, Loss: 0.35316338390111923, Final Batch Loss: 0.24038930237293243\n",
      "Epoch 3236, Loss: 0.253485307097435, Final Batch Loss: 0.13922545313835144\n",
      "Epoch 3237, Loss: 0.2411603331565857, Final Batch Loss: 0.10220430791378021\n",
      "Epoch 3238, Loss: 0.2571946233510971, Final Batch Loss: 0.12116530537605286\n",
      "Epoch 3239, Loss: 0.3034452348947525, Final Batch Loss: 0.1773921698331833\n",
      "Epoch 3240, Loss: 0.22176272422075272, Final Batch Loss: 0.11417295783758163\n",
      "Epoch 3241, Loss: 0.32184165716171265, Final Batch Loss: 0.17977085709571838\n",
      "Epoch 3242, Loss: 0.2377494052052498, Final Batch Loss: 0.08122976869344711\n",
      "Epoch 3243, Loss: 0.2768514007329941, Final Batch Loss: 0.12146739661693573\n",
      "Epoch 3244, Loss: 0.24457573145627975, Final Batch Loss: 0.11164439469575882\n",
      "Epoch 3245, Loss: 0.25983743369579315, Final Batch Loss: 0.1142624020576477\n",
      "Epoch 3246, Loss: 0.25396444648504257, Final Batch Loss: 0.08522961288690567\n",
      "Epoch 3247, Loss: 0.3107888847589493, Final Batch Loss: 0.16438548266887665\n",
      "Epoch 3248, Loss: 0.2986118793487549, Final Batch Loss: 0.1465359777212143\n",
      "Epoch 3249, Loss: 0.30960245430469513, Final Batch Loss: 0.17461803555488586\n",
      "Epoch 3250, Loss: 0.23637168109416962, Final Batch Loss: 0.10755975544452667\n",
      "Epoch 3251, Loss: 0.2749248072504997, Final Batch Loss: 0.158830925822258\n",
      "Epoch 3252, Loss: 0.1978796198964119, Final Batch Loss: 0.08328215777873993\n",
      "Epoch 3253, Loss: 0.23467402160167694, Final Batch Loss: 0.1257402002811432\n",
      "Epoch 3254, Loss: 0.240119069814682, Final Batch Loss: 0.1337929666042328\n",
      "Epoch 3255, Loss: 0.29271338880062103, Final Batch Loss: 0.14840640127658844\n",
      "Epoch 3256, Loss: 0.23231250047683716, Final Batch Loss: 0.09910281002521515\n",
      "Epoch 3257, Loss: 0.18539652973413467, Final Batch Loss: 0.0877959132194519\n",
      "Epoch 3258, Loss: 0.25350262969732285, Final Batch Loss: 0.08587025851011276\n",
      "Epoch 3259, Loss: 0.2695346772670746, Final Batch Loss: 0.09753414988517761\n",
      "Epoch 3260, Loss: 0.2440958097577095, Final Batch Loss: 0.13100530207157135\n",
      "Epoch 3261, Loss: 0.2217389941215515, Final Batch Loss: 0.09104984998703003\n",
      "Epoch 3262, Loss: 0.24742037057876587, Final Batch Loss: 0.11917254328727722\n",
      "Epoch 3263, Loss: 0.314082995057106, Final Batch Loss: 0.1828068196773529\n",
      "Epoch 3264, Loss: 0.32052290439605713, Final Batch Loss: 0.17597757279872894\n",
      "Epoch 3265, Loss: 0.24801896512508392, Final Batch Loss: 0.12028244137763977\n",
      "Epoch 3266, Loss: 0.2794369161128998, Final Batch Loss: 0.15300065279006958\n",
      "Epoch 3267, Loss: 0.23344305902719498, Final Batch Loss: 0.12320509552955627\n",
      "Epoch 3268, Loss: 0.2561129257082939, Final Batch Loss: 0.11892665177583694\n",
      "Epoch 3269, Loss: 0.2831210345029831, Final Batch Loss: 0.16326144337654114\n",
      "Epoch 3270, Loss: 0.23672199994325638, Final Batch Loss: 0.1208525002002716\n",
      "Epoch 3271, Loss: 0.2402355745434761, Final Batch Loss: 0.12302973121404648\n",
      "Epoch 3272, Loss: 0.26577962189912796, Final Batch Loss: 0.09141240268945694\n",
      "Epoch 3273, Loss: 0.29698795080184937, Final Batch Loss: 0.18365174531936646\n",
      "Epoch 3274, Loss: 0.2624910995364189, Final Batch Loss: 0.13987763226032257\n",
      "Epoch 3275, Loss: 0.2512637674808502, Final Batch Loss: 0.13865125179290771\n",
      "Epoch 3276, Loss: 0.27435339242219925, Final Batch Loss: 0.15568003058433533\n",
      "Epoch 3277, Loss: 0.277042992413044, Final Batch Loss: 0.12323061376810074\n",
      "Epoch 3278, Loss: 0.2418321743607521, Final Batch Loss: 0.12036377936601639\n",
      "Epoch 3279, Loss: 0.2214001938700676, Final Batch Loss: 0.08780144900083542\n",
      "Epoch 3280, Loss: 0.2953067943453789, Final Batch Loss: 0.12394159287214279\n",
      "Epoch 3281, Loss: 0.3214121162891388, Final Batch Loss: 0.14974240958690643\n",
      "Epoch 3282, Loss: 0.25496481359004974, Final Batch Loss: 0.10524527728557587\n",
      "Epoch 3283, Loss: 0.3400133401155472, Final Batch Loss: 0.16784830391407013\n",
      "Epoch 3284, Loss: 0.23297687619924545, Final Batch Loss: 0.1278824508190155\n",
      "Epoch 3285, Loss: 0.33721332252025604, Final Batch Loss: 0.1428094208240509\n",
      "Epoch 3286, Loss: 0.2680440694093704, Final Batch Loss: 0.13231617212295532\n",
      "Epoch 3287, Loss: 0.2872421070933342, Final Batch Loss: 0.12108998745679855\n",
      "Epoch 3288, Loss: 0.27003785967826843, Final Batch Loss: 0.1547628492116928\n",
      "Epoch 3289, Loss: 0.31581152230501175, Final Batch Loss: 0.1238764300942421\n",
      "Epoch 3290, Loss: 0.2767932265996933, Final Batch Loss: 0.15665043890476227\n",
      "Epoch 3291, Loss: 0.2295944094657898, Final Batch Loss: 0.09561459720134735\n",
      "Epoch 3292, Loss: 0.27327118068933487, Final Batch Loss: 0.18111994862556458\n",
      "Epoch 3293, Loss: 0.22281090915203094, Final Batch Loss: 0.0859609991312027\n",
      "Epoch 3294, Loss: 0.28370020538568497, Final Batch Loss: 0.1668611466884613\n",
      "Epoch 3295, Loss: 0.25806406140327454, Final Batch Loss: 0.13557001948356628\n",
      "Epoch 3296, Loss: 0.23096958547830582, Final Batch Loss: 0.12323085963726044\n",
      "Epoch 3297, Loss: 0.27137745916843414, Final Batch Loss: 0.11968755722045898\n",
      "Epoch 3298, Loss: 0.2672678604722023, Final Batch Loss: 0.10863681882619858\n",
      "Epoch 3299, Loss: 0.2758485972881317, Final Batch Loss: 0.13886213302612305\n",
      "Epoch 3300, Loss: 0.2700736075639725, Final Batch Loss: 0.12639863789081573\n",
      "Epoch 3301, Loss: 0.312288299202919, Final Batch Loss: 0.16815170645713806\n",
      "Epoch 3302, Loss: 0.27776603400707245, Final Batch Loss: 0.11418856680393219\n",
      "Epoch 3303, Loss: 0.2238580361008644, Final Batch Loss: 0.11100265383720398\n",
      "Epoch 3304, Loss: 0.24017668515443802, Final Batch Loss: 0.11415254324674606\n",
      "Epoch 3305, Loss: 0.2537767291069031, Final Batch Loss: 0.13018923997879028\n",
      "Epoch 3306, Loss: 0.2401657998561859, Final Batch Loss: 0.13453085720539093\n",
      "Epoch 3307, Loss: 0.23613936454057693, Final Batch Loss: 0.12110456079244614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3308, Loss: 0.3409993499517441, Final Batch Loss: 0.16907130181789398\n",
      "Epoch 3309, Loss: 0.2559213414788246, Final Batch Loss: 0.11795852333307266\n",
      "Epoch 3310, Loss: 0.2478460669517517, Final Batch Loss: 0.1529289036989212\n",
      "Epoch 3311, Loss: 0.1848193034529686, Final Batch Loss: 0.07821930944919586\n",
      "Epoch 3312, Loss: 0.3100525587797165, Final Batch Loss: 0.15574492514133453\n",
      "Epoch 3313, Loss: 0.2219233736395836, Final Batch Loss: 0.09563872963190079\n",
      "Epoch 3314, Loss: 0.294413685798645, Final Batch Loss: 0.156650573015213\n",
      "Epoch 3315, Loss: 0.22989298403263092, Final Batch Loss: 0.09283746778964996\n",
      "Epoch 3316, Loss: 0.26057717949151993, Final Batch Loss: 0.1457541435956955\n",
      "Epoch 3317, Loss: 0.26217474043369293, Final Batch Loss: 0.14684200286865234\n",
      "Epoch 3318, Loss: 0.26261308789253235, Final Batch Loss: 0.12419793009757996\n",
      "Epoch 3319, Loss: 0.2583785578608513, Final Batch Loss: 0.09561989456415176\n",
      "Epoch 3320, Loss: 0.20430297404527664, Final Batch Loss: 0.07769737392663956\n",
      "Epoch 3321, Loss: 0.203515887260437, Final Batch Loss: 0.10144753754138947\n",
      "Epoch 3322, Loss: 0.2900959253311157, Final Batch Loss: 0.14730742573738098\n",
      "Epoch 3323, Loss: 0.3707786723971367, Final Batch Loss: 0.2572598159313202\n",
      "Epoch 3324, Loss: 0.25112778693437576, Final Batch Loss: 0.1382371038198471\n",
      "Epoch 3325, Loss: 0.23143509030342102, Final Batch Loss: 0.13490386307239532\n",
      "Epoch 3326, Loss: 0.27053923159837723, Final Batch Loss: 0.16911852359771729\n",
      "Epoch 3327, Loss: 0.23219463974237442, Final Batch Loss: 0.11560861766338348\n",
      "Epoch 3328, Loss: 0.2957543283700943, Final Batch Loss: 0.15252751111984253\n",
      "Epoch 3329, Loss: 0.24845116585493088, Final Batch Loss: 0.10018391162157059\n",
      "Epoch 3330, Loss: 0.23961137980222702, Final Batch Loss: 0.1303005814552307\n",
      "Epoch 3331, Loss: 0.22936709225177765, Final Batch Loss: 0.10425631701946259\n",
      "Epoch 3332, Loss: 0.25705958157777786, Final Batch Loss: 0.15366563200950623\n",
      "Epoch 3333, Loss: 0.2681991904973984, Final Batch Loss: 0.141781285405159\n",
      "Epoch 3334, Loss: 0.2699587047100067, Final Batch Loss: 0.13166135549545288\n",
      "Epoch 3335, Loss: 0.2470235601067543, Final Batch Loss: 0.10498439520597458\n",
      "Epoch 3336, Loss: 0.2635047659277916, Final Batch Loss: 0.11581415683031082\n",
      "Epoch 3337, Loss: 0.24952387809753418, Final Batch Loss: 0.1275661438703537\n",
      "Epoch 3338, Loss: 0.26551588624715805, Final Batch Loss: 0.1718074232339859\n",
      "Epoch 3339, Loss: 0.3535173386335373, Final Batch Loss: 0.1568043828010559\n",
      "Epoch 3340, Loss: 0.2289685681462288, Final Batch Loss: 0.06572670489549637\n",
      "Epoch 3341, Loss: 0.23288611322641373, Final Batch Loss: 0.12948818504810333\n",
      "Epoch 3342, Loss: 0.283482626080513, Final Batch Loss: 0.1515568643808365\n",
      "Epoch 3343, Loss: 0.23249517381191254, Final Batch Loss: 0.0915481299161911\n",
      "Epoch 3344, Loss: 0.23284032940864563, Final Batch Loss: 0.10439398884773254\n",
      "Epoch 3345, Loss: 0.20762338489294052, Final Batch Loss: 0.0789470449090004\n",
      "Epoch 3346, Loss: 0.273820236325264, Final Batch Loss: 0.1268301159143448\n",
      "Epoch 3347, Loss: 0.23560145497322083, Final Batch Loss: 0.10285776853561401\n",
      "Epoch 3348, Loss: 0.2435266524553299, Final Batch Loss: 0.1090414971113205\n",
      "Epoch 3349, Loss: 0.19439810514450073, Final Batch Loss: 0.08790058642625809\n",
      "Epoch 3350, Loss: 0.2813161462545395, Final Batch Loss: 0.14961864054203033\n",
      "Epoch 3351, Loss: 0.20705849677324295, Final Batch Loss: 0.0794447585940361\n",
      "Epoch 3352, Loss: 0.21392398327589035, Final Batch Loss: 0.10855672508478165\n",
      "Epoch 3353, Loss: 0.2666349783539772, Final Batch Loss: 0.10222335904836655\n",
      "Epoch 3354, Loss: 0.2988346368074417, Final Batch Loss: 0.165804922580719\n",
      "Epoch 3355, Loss: 0.19410602748394012, Final Batch Loss: 0.09511420875787735\n",
      "Epoch 3356, Loss: 0.21342964470386505, Final Batch Loss: 0.10747724026441574\n",
      "Epoch 3357, Loss: 0.2883182466030121, Final Batch Loss: 0.14765390753746033\n",
      "Epoch 3358, Loss: 0.3055880665779114, Final Batch Loss: 0.18282493948936462\n",
      "Epoch 3359, Loss: 0.24001716077327728, Final Batch Loss: 0.14210520684719086\n",
      "Epoch 3360, Loss: 0.2555895820260048, Final Batch Loss: 0.13376197218894958\n",
      "Epoch 3361, Loss: 0.2045387104153633, Final Batch Loss: 0.09694208949804306\n",
      "Epoch 3362, Loss: 0.23088901489973068, Final Batch Loss: 0.12979264557361603\n",
      "Epoch 3363, Loss: 0.3094407171010971, Final Batch Loss: 0.1776142120361328\n",
      "Epoch 3364, Loss: 0.3002184331417084, Final Batch Loss: 0.18992093205451965\n",
      "Epoch 3365, Loss: 0.23811225593090057, Final Batch Loss: 0.14300008118152618\n",
      "Epoch 3366, Loss: 0.23287994414567947, Final Batch Loss: 0.1224403902888298\n",
      "Epoch 3367, Loss: 0.2274753451347351, Final Batch Loss: 0.1119512990117073\n",
      "Epoch 3368, Loss: 0.28898004442453384, Final Batch Loss: 0.1806132197380066\n",
      "Epoch 3369, Loss: 0.2958497703075409, Final Batch Loss: 0.13344179093837738\n",
      "Epoch 3370, Loss: 0.31608815491199493, Final Batch Loss: 0.17964249849319458\n",
      "Epoch 3371, Loss: 0.24893132597208023, Final Batch Loss: 0.11778894811868668\n",
      "Epoch 3372, Loss: 0.24135030806064606, Final Batch Loss: 0.1311361938714981\n",
      "Epoch 3373, Loss: 0.31351901590824127, Final Batch Loss: 0.1894153356552124\n",
      "Epoch 3374, Loss: 0.26054850965738297, Final Batch Loss: 0.10024640709161758\n",
      "Epoch 3375, Loss: 0.2722649797797203, Final Batch Loss: 0.09682508558034897\n",
      "Epoch 3376, Loss: 0.25660473108291626, Final Batch Loss: 0.08955971896648407\n",
      "Epoch 3377, Loss: 0.22779451310634613, Final Batch Loss: 0.12822900712490082\n",
      "Epoch 3378, Loss: 0.2561611905694008, Final Batch Loss: 0.07967419177293777\n",
      "Epoch 3379, Loss: 0.21965346485376358, Final Batch Loss: 0.09402293711900711\n",
      "Epoch 3380, Loss: 0.22299101203680038, Final Batch Loss: 0.09239178150892258\n",
      "Epoch 3381, Loss: 0.2699674442410469, Final Batch Loss: 0.12390031665563583\n",
      "Epoch 3382, Loss: 0.2808784395456314, Final Batch Loss: 0.16226883232593536\n",
      "Epoch 3383, Loss: 0.27401578426361084, Final Batch Loss: 0.1071930080652237\n",
      "Epoch 3384, Loss: 0.3056422024965286, Final Batch Loss: 0.1634741723537445\n",
      "Epoch 3385, Loss: 0.2987138479948044, Final Batch Loss: 0.17583350837230682\n",
      "Epoch 3386, Loss: 0.23017771542072296, Final Batch Loss: 0.0875474363565445\n",
      "Epoch 3387, Loss: 0.27816468477249146, Final Batch Loss: 0.14087921380996704\n",
      "Epoch 3388, Loss: 0.24104975908994675, Final Batch Loss: 0.12974049150943756\n",
      "Epoch 3389, Loss: 0.25805220007896423, Final Batch Loss: 0.17516016960144043\n",
      "Epoch 3390, Loss: 0.2583164572715759, Final Batch Loss: 0.14859354496002197\n",
      "Epoch 3391, Loss: 0.2307957261800766, Final Batch Loss: 0.08967128396034241\n",
      "Epoch 3392, Loss: 0.1967063918709755, Final Batch Loss: 0.061288975179195404\n",
      "Epoch 3393, Loss: 0.3088836520910263, Final Batch Loss: 0.14278876781463623\n",
      "Epoch 3394, Loss: 0.23121877014636993, Final Batch Loss: 0.13916242122650146\n",
      "Epoch 3395, Loss: 0.2797992676496506, Final Batch Loss: 0.1258355975151062\n",
      "Epoch 3396, Loss: 0.24979816377162933, Final Batch Loss: 0.11899860203266144\n",
      "Epoch 3397, Loss: 0.3077319711446762, Final Batch Loss: 0.16166536509990692\n",
      "Epoch 3398, Loss: 0.19628293067216873, Final Batch Loss: 0.09090911597013474\n",
      "Epoch 3399, Loss: 0.25049101561307907, Final Batch Loss: 0.12836922705173492\n",
      "Epoch 3400, Loss: 0.3246854171156883, Final Batch Loss: 0.2077329009771347\n",
      "Epoch 3401, Loss: 0.2693898528814316, Final Batch Loss: 0.11388616263866425\n",
      "Epoch 3402, Loss: 0.250218465924263, Final Batch Loss: 0.11542068421840668\n",
      "Epoch 3403, Loss: 0.24936506152153015, Final Batch Loss: 0.14360107481479645\n",
      "Epoch 3404, Loss: 0.20443863421678543, Final Batch Loss: 0.10642483830451965\n",
      "Epoch 3405, Loss: 0.21993734687566757, Final Batch Loss: 0.10540740936994553\n",
      "Epoch 3406, Loss: 0.27366718649864197, Final Batch Loss: 0.12236607074737549\n",
      "Epoch 3407, Loss: 0.31492069363594055, Final Batch Loss: 0.13909690082073212\n",
      "Epoch 3408, Loss: 0.29444199800491333, Final Batch Loss: 0.1736859679222107\n",
      "Epoch 3409, Loss: 0.24880830943584442, Final Batch Loss: 0.12214653193950653\n",
      "Epoch 3410, Loss: 0.24636269360780716, Final Batch Loss: 0.15362034738063812\n",
      "Epoch 3411, Loss: 0.25469911098480225, Final Batch Loss: 0.12670201063156128\n",
      "Epoch 3412, Loss: 0.30128873884677887, Final Batch Loss: 0.16592822968959808\n",
      "Epoch 3413, Loss: 0.25424274057149887, Final Batch Loss: 0.1063474640250206\n",
      "Epoch 3414, Loss: 0.22093592584133148, Final Batch Loss: 0.0662720650434494\n",
      "Epoch 3415, Loss: 0.22285179793834686, Final Batch Loss: 0.12521743774414062\n",
      "Epoch 3416, Loss: 0.19538919627666473, Final Batch Loss: 0.07884787023067474\n",
      "Epoch 3417, Loss: 0.2147502899169922, Final Batch Loss: 0.11513803154230118\n",
      "Epoch 3418, Loss: 0.1781843677163124, Final Batch Loss: 0.08265370875597\n",
      "Epoch 3419, Loss: 0.24359267204999924, Final Batch Loss: 0.1201261654496193\n",
      "Epoch 3420, Loss: 0.274319127202034, Final Batch Loss: 0.13523980975151062\n",
      "Epoch 3421, Loss: 0.28480806201696396, Final Batch Loss: 0.17132553458213806\n",
      "Epoch 3422, Loss: 0.2444966584444046, Final Batch Loss: 0.12658795714378357\n",
      "Epoch 3423, Loss: 0.2366480678319931, Final Batch Loss: 0.0867958813905716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3424, Loss: 0.2299715057015419, Final Batch Loss: 0.12189464271068573\n",
      "Epoch 3425, Loss: 0.24558722972869873, Final Batch Loss: 0.11279366910457611\n",
      "Epoch 3426, Loss: 0.2618393078446388, Final Batch Loss: 0.15927070379257202\n",
      "Epoch 3427, Loss: 0.24614299833774567, Final Batch Loss: 0.11545373499393463\n",
      "Epoch 3428, Loss: 0.22653795778751373, Final Batch Loss: 0.0822051614522934\n",
      "Epoch 3429, Loss: 0.24999625980854034, Final Batch Loss: 0.1212950199842453\n",
      "Epoch 3430, Loss: 0.2849240303039551, Final Batch Loss: 0.1475151777267456\n",
      "Epoch 3431, Loss: 0.20263861864805222, Final Batch Loss: 0.12799549102783203\n",
      "Epoch 3432, Loss: 0.3134623169898987, Final Batch Loss: 0.20127755403518677\n",
      "Epoch 3433, Loss: 0.26087289303541183, Final Batch Loss: 0.13982349634170532\n",
      "Epoch 3434, Loss: 0.24208932369947433, Final Batch Loss: 0.09147212654352188\n",
      "Epoch 3435, Loss: 0.24528927356004715, Final Batch Loss: 0.13372695446014404\n",
      "Epoch 3436, Loss: 0.2666298598051071, Final Batch Loss: 0.1444343626499176\n",
      "Epoch 3437, Loss: 0.2929456830024719, Final Batch Loss: 0.14083540439605713\n",
      "Epoch 3438, Loss: 0.2082638517022133, Final Batch Loss: 0.1058872863650322\n",
      "Epoch 3439, Loss: 0.25339390337467194, Final Batch Loss: 0.14754849672317505\n",
      "Epoch 3440, Loss: 0.21545413881540298, Final Batch Loss: 0.10946167260408401\n",
      "Epoch 3441, Loss: 0.2648588865995407, Final Batch Loss: 0.16752676665782928\n",
      "Epoch 3442, Loss: 0.2049405872821808, Final Batch Loss: 0.08719456195831299\n",
      "Epoch 3443, Loss: 0.22510755062103271, Final Batch Loss: 0.10231204330921173\n",
      "Epoch 3444, Loss: 0.2347717061638832, Final Batch Loss: 0.10521545261144638\n",
      "Epoch 3445, Loss: 0.43619637191295624, Final Batch Loss: 0.33384382724761963\n",
      "Epoch 3446, Loss: 0.21935301274061203, Final Batch Loss: 0.10745973140001297\n",
      "Epoch 3447, Loss: 0.3154372125864029, Final Batch Loss: 0.16856098175048828\n",
      "Epoch 3448, Loss: 0.24979614466428757, Final Batch Loss: 0.11559102684259415\n",
      "Epoch 3449, Loss: 0.2170405015349388, Final Batch Loss: 0.10870981961488724\n",
      "Epoch 3450, Loss: 0.2672525644302368, Final Batch Loss: 0.13711510598659515\n",
      "Epoch 3451, Loss: 0.22417479753494263, Final Batch Loss: 0.09359422326087952\n",
      "Epoch 3452, Loss: 0.24457232654094696, Final Batch Loss: 0.14886780083179474\n",
      "Epoch 3453, Loss: 0.32449832558631897, Final Batch Loss: 0.2206151783466339\n",
      "Epoch 3454, Loss: 0.3141332119703293, Final Batch Loss: 0.19802601635456085\n",
      "Epoch 3455, Loss: 0.23426102846860886, Final Batch Loss: 0.09674305468797684\n",
      "Epoch 3456, Loss: 0.26881739497184753, Final Batch Loss: 0.15217478573322296\n",
      "Epoch 3457, Loss: 0.2583429515361786, Final Batch Loss: 0.11250917613506317\n",
      "Epoch 3458, Loss: 0.2083674967288971, Final Batch Loss: 0.04803457856178284\n",
      "Epoch 3459, Loss: 0.26774459332227707, Final Batch Loss: 0.1444786936044693\n",
      "Epoch 3460, Loss: 0.29369987547397614, Final Batch Loss: 0.1567733883857727\n",
      "Epoch 3461, Loss: 0.3400225043296814, Final Batch Loss: 0.16581599414348602\n",
      "Epoch 3462, Loss: 0.2720728814601898, Final Batch Loss: 0.1250481754541397\n",
      "Epoch 3463, Loss: 0.23464488238096237, Final Batch Loss: 0.10199413448572159\n",
      "Epoch 3464, Loss: 0.20374591648578644, Final Batch Loss: 0.10780040919780731\n",
      "Epoch 3465, Loss: 0.3168885335326195, Final Batch Loss: 0.20922274887561798\n",
      "Epoch 3466, Loss: 0.23284215480089188, Final Batch Loss: 0.10152819007635117\n",
      "Epoch 3467, Loss: 0.25511110574007034, Final Batch Loss: 0.1560293734073639\n",
      "Epoch 3468, Loss: 0.30666425824165344, Final Batch Loss: 0.1546109914779663\n",
      "Epoch 3469, Loss: 0.2844369411468506, Final Batch Loss: 0.14457441866397858\n",
      "Epoch 3470, Loss: 0.30154453217983246, Final Batch Loss: 0.14126579463481903\n",
      "Epoch 3471, Loss: 0.25658145546913147, Final Batch Loss: 0.12944602966308594\n",
      "Epoch 3472, Loss: 0.20566938817501068, Final Batch Loss: 0.09974145889282227\n",
      "Epoch 3473, Loss: 0.19802506268024445, Final Batch Loss: 0.09693428128957748\n",
      "Epoch 3474, Loss: 0.29063792526721954, Final Batch Loss: 0.15837185084819794\n",
      "Epoch 3475, Loss: 0.3038456067442894, Final Batch Loss: 0.18497222661972046\n",
      "Epoch 3476, Loss: 0.19043248891830444, Final Batch Loss: 0.07549751549959183\n",
      "Epoch 3477, Loss: 0.28719669580459595, Final Batch Loss: 0.14083202183246613\n",
      "Epoch 3478, Loss: 0.28127843141555786, Final Batch Loss: 0.14820827543735504\n",
      "Epoch 3479, Loss: 0.2207830250263214, Final Batch Loss: 0.10205580294132233\n",
      "Epoch 3480, Loss: 0.22174251079559326, Final Batch Loss: 0.1223612055182457\n",
      "Epoch 3481, Loss: 0.213037371635437, Final Batch Loss: 0.10178342461585999\n",
      "Epoch 3482, Loss: 0.242849700152874, Final Batch Loss: 0.12852346897125244\n",
      "Epoch 3483, Loss: 0.23678600788116455, Final Batch Loss: 0.12426808476448059\n",
      "Epoch 3484, Loss: 0.255318284034729, Final Batch Loss: 0.12644493579864502\n",
      "Epoch 3485, Loss: 0.24357298016548157, Final Batch Loss: 0.09727101027965546\n",
      "Epoch 3486, Loss: 0.2450847551226616, Final Batch Loss: 0.11019619554281235\n",
      "Epoch 3487, Loss: 0.2921912223100662, Final Batch Loss: 0.1488891839981079\n",
      "Epoch 3488, Loss: 0.32198747992515564, Final Batch Loss: 0.1451604813337326\n",
      "Epoch 3489, Loss: 0.2882142663002014, Final Batch Loss: 0.1372031420469284\n",
      "Epoch 3490, Loss: 0.2834530621767044, Final Batch Loss: 0.16539181768894196\n",
      "Epoch 3491, Loss: 0.2801368683576584, Final Batch Loss: 0.16140833497047424\n",
      "Epoch 3492, Loss: 0.2725084722042084, Final Batch Loss: 0.14556406438350677\n",
      "Epoch 3493, Loss: 0.2655750662088394, Final Batch Loss: 0.11086273193359375\n",
      "Epoch 3494, Loss: 0.3270026743412018, Final Batch Loss: 0.14342205226421356\n",
      "Epoch 3495, Loss: 0.25095630437135696, Final Batch Loss: 0.11169197410345078\n",
      "Epoch 3496, Loss: 0.23003392666578293, Final Batch Loss: 0.10144466906785965\n",
      "Epoch 3497, Loss: 0.2286008819937706, Final Batch Loss: 0.10433115065097809\n",
      "Epoch 3498, Loss: 0.24111972749233246, Final Batch Loss: 0.1085103303194046\n",
      "Epoch 3499, Loss: 0.29835473746061325, Final Batch Loss: 0.1203322634100914\n",
      "Epoch 3500, Loss: 0.2820952832698822, Final Batch Loss: 0.16833752393722534\n",
      "Epoch 3501, Loss: 0.28298474848270416, Final Batch Loss: 0.1585993617773056\n",
      "Epoch 3502, Loss: 0.20009101182222366, Final Batch Loss: 0.06682001799345016\n",
      "Epoch 3503, Loss: 0.2827828973531723, Final Batch Loss: 0.10896894335746765\n",
      "Epoch 3504, Loss: 0.2679131478071213, Final Batch Loss: 0.13307376205921173\n",
      "Epoch 3505, Loss: 0.29496969282627106, Final Batch Loss: 0.1465321034193039\n",
      "Epoch 3506, Loss: 0.23789339512586594, Final Batch Loss: 0.13089601695537567\n",
      "Epoch 3507, Loss: 0.3088555186986923, Final Batch Loss: 0.16130784153938293\n",
      "Epoch 3508, Loss: 0.26528047025203705, Final Batch Loss: 0.13053280115127563\n",
      "Epoch 3509, Loss: 0.2674238905310631, Final Batch Loss: 0.153181254863739\n",
      "Epoch 3510, Loss: 0.25683553516864777, Final Batch Loss: 0.1502140909433365\n",
      "Epoch 3511, Loss: 0.21877634525299072, Final Batch Loss: 0.11144284904003143\n",
      "Epoch 3512, Loss: 0.2129313051700592, Final Batch Loss: 0.12400411814451218\n",
      "Epoch 3513, Loss: 0.2211337313055992, Final Batch Loss: 0.11693557351827621\n",
      "Epoch 3514, Loss: 0.25261756032705307, Final Batch Loss: 0.16345541179180145\n",
      "Epoch 3515, Loss: 0.21487100422382355, Final Batch Loss: 0.10453379154205322\n",
      "Epoch 3516, Loss: 0.32058168947696686, Final Batch Loss: 0.1777629405260086\n",
      "Epoch 3517, Loss: 0.28555913269519806, Final Batch Loss: 0.1616196483373642\n",
      "Epoch 3518, Loss: 0.19371988624334335, Final Batch Loss: 0.10421796888113022\n",
      "Epoch 3519, Loss: 0.2747540846467018, Final Batch Loss: 0.11412756890058517\n",
      "Epoch 3520, Loss: 0.2099619284272194, Final Batch Loss: 0.10021302849054337\n",
      "Epoch 3521, Loss: 0.22663193941116333, Final Batch Loss: 0.10344833135604858\n",
      "Epoch 3522, Loss: 0.3110426217317581, Final Batch Loss: 0.1612175703048706\n",
      "Epoch 3523, Loss: 0.2687057554721832, Final Batch Loss: 0.14688412845134735\n",
      "Epoch 3524, Loss: 0.19720401614904404, Final Batch Loss: 0.10421932488679886\n",
      "Epoch 3525, Loss: 0.2358284667134285, Final Batch Loss: 0.1262560337781906\n",
      "Epoch 3526, Loss: 0.2601632922887802, Final Batch Loss: 0.12192697823047638\n",
      "Epoch 3527, Loss: 0.2600574344396591, Final Batch Loss: 0.12345072627067566\n",
      "Epoch 3528, Loss: 0.23056337237358093, Final Batch Loss: 0.12121962010860443\n",
      "Epoch 3529, Loss: 0.2230604737997055, Final Batch Loss: 0.12371367961168289\n",
      "Epoch 3530, Loss: 0.2723742425441742, Final Batch Loss: 0.13406093418598175\n",
      "Epoch 3531, Loss: 0.20161806792020798, Final Batch Loss: 0.0989205539226532\n",
      "Epoch 3532, Loss: 0.2990047484636307, Final Batch Loss: 0.15651732683181763\n",
      "Epoch 3533, Loss: 0.19633664935827255, Final Batch Loss: 0.09709331393241882\n",
      "Epoch 3534, Loss: 0.2807891368865967, Final Batch Loss: 0.16410832107067108\n",
      "Epoch 3535, Loss: 0.23686432093381882, Final Batch Loss: 0.11022522300481796\n",
      "Epoch 3536, Loss: 0.24939410388469696, Final Batch Loss: 0.12340618669986725\n",
      "Epoch 3537, Loss: 0.2868882045149803, Final Batch Loss: 0.11534426361322403\n",
      "Epoch 3538, Loss: 0.24093153327703476, Final Batch Loss: 0.09650594741106033\n",
      "Epoch 3539, Loss: 0.3553232401609421, Final Batch Loss: 0.1851348578929901\n",
      "Epoch 3540, Loss: 0.26517488807439804, Final Batch Loss: 0.07648009806871414\n",
      "Epoch 3541, Loss: 0.19629023224115372, Final Batch Loss: 0.08536406606435776\n",
      "Epoch 3542, Loss: 0.21508272737264633, Final Batch Loss: 0.08926936239004135\n",
      "Epoch 3543, Loss: 0.25026893615722656, Final Batch Loss: 0.12748363614082336\n",
      "Epoch 3544, Loss: 0.21755150705575943, Final Batch Loss: 0.10015410929918289\n",
      "Epoch 3545, Loss: 0.2815815806388855, Final Batch Loss: 0.1316744089126587\n",
      "Epoch 3546, Loss: 0.20758841931819916, Final Batch Loss: 0.09655559808015823\n",
      "Epoch 3547, Loss: 0.2303370013833046, Final Batch Loss: 0.13106489181518555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3548, Loss: 0.22277569770812988, Final Batch Loss: 0.12481416016817093\n",
      "Epoch 3549, Loss: 0.1840457320213318, Final Batch Loss: 0.07671036571264267\n",
      "Epoch 3550, Loss: 0.23041150718927383, Final Batch Loss: 0.09438740462064743\n",
      "Epoch 3551, Loss: 0.25764328241348267, Final Batch Loss: 0.1304919272661209\n",
      "Epoch 3552, Loss: 0.15868350118398666, Final Batch Loss: 0.07420390844345093\n",
      "Epoch 3553, Loss: 0.24731702357530594, Final Batch Loss: 0.12441924214363098\n",
      "Epoch 3554, Loss: 0.32813723385334015, Final Batch Loss: 0.19460831582546234\n",
      "Epoch 3555, Loss: 0.2520676553249359, Final Batch Loss: 0.1584911346435547\n",
      "Epoch 3556, Loss: 0.2060507833957672, Final Batch Loss: 0.10890225321054459\n",
      "Epoch 3557, Loss: 0.2615528702735901, Final Batch Loss: 0.15374711155891418\n",
      "Epoch 3558, Loss: 0.24017030745744705, Final Batch Loss: 0.12826204299926758\n",
      "Epoch 3559, Loss: 0.29107168316841125, Final Batch Loss: 0.16939647495746613\n",
      "Epoch 3560, Loss: 0.22546449303627014, Final Batch Loss: 0.11633159220218658\n",
      "Epoch 3561, Loss: 0.2851425111293793, Final Batch Loss: 0.15677379071712494\n",
      "Epoch 3562, Loss: 0.2436980903148651, Final Batch Loss: 0.09631577134132385\n",
      "Epoch 3563, Loss: 0.2721816673874855, Final Batch Loss: 0.09348072856664658\n",
      "Epoch 3564, Loss: 0.28370580077171326, Final Batch Loss: 0.18780535459518433\n",
      "Epoch 3565, Loss: 0.2300036996603012, Final Batch Loss: 0.1188676506280899\n",
      "Epoch 3566, Loss: 0.2965599000453949, Final Batch Loss: 0.11813513934612274\n",
      "Epoch 3567, Loss: 0.23209811002016068, Final Batch Loss: 0.11421000212430954\n",
      "Epoch 3568, Loss: 0.3411436378955841, Final Batch Loss: 0.15727384388446808\n",
      "Epoch 3569, Loss: 0.21423999220132828, Final Batch Loss: 0.08667924255132675\n",
      "Epoch 3570, Loss: 0.18986988067626953, Final Batch Loss: 0.11187877506017685\n",
      "Epoch 3571, Loss: 0.21672658622264862, Final Batch Loss: 0.09312612563371658\n",
      "Epoch 3572, Loss: 0.23224182426929474, Final Batch Loss: 0.14853452146053314\n",
      "Epoch 3573, Loss: 0.25147221237421036, Final Batch Loss: 0.10558637231588364\n",
      "Epoch 3574, Loss: 0.3315698504447937, Final Batch Loss: 0.1791074275970459\n",
      "Epoch 3575, Loss: 0.2494928389787674, Final Batch Loss: 0.1222175806760788\n",
      "Epoch 3576, Loss: 0.2551020681858063, Final Batch Loss: 0.13720910251140594\n",
      "Epoch 3577, Loss: 0.21610378473997116, Final Batch Loss: 0.10061739385128021\n",
      "Epoch 3578, Loss: 0.2770173251628876, Final Batch Loss: 0.1211053878068924\n",
      "Epoch 3579, Loss: 0.25248411297798157, Final Batch Loss: 0.1460282802581787\n",
      "Epoch 3580, Loss: 0.20357444882392883, Final Batch Loss: 0.07910440117120743\n",
      "Epoch 3581, Loss: 0.26760467141866684, Final Batch Loss: 0.1590932011604309\n",
      "Epoch 3582, Loss: 0.26314380019903183, Final Batch Loss: 0.15970267355442047\n",
      "Epoch 3583, Loss: 0.2140740230679512, Final Batch Loss: 0.0984499454498291\n",
      "Epoch 3584, Loss: 0.3316461890935898, Final Batch Loss: 0.1948656588792801\n",
      "Epoch 3585, Loss: 0.255646675825119, Final Batch Loss: 0.1292625516653061\n",
      "Epoch 3586, Loss: 0.2061494141817093, Final Batch Loss: 0.08379317820072174\n",
      "Epoch 3587, Loss: 0.2640523314476013, Final Batch Loss: 0.13244202733039856\n",
      "Epoch 3588, Loss: 0.18897490203380585, Final Batch Loss: 0.0679183229804039\n",
      "Epoch 3589, Loss: 0.33533236384391785, Final Batch Loss: 0.15269353985786438\n",
      "Epoch 3590, Loss: 0.2942700535058975, Final Batch Loss: 0.1273055374622345\n",
      "Epoch 3591, Loss: 0.2736654058098793, Final Batch Loss: 0.09323529154062271\n",
      "Epoch 3592, Loss: 0.19737427681684494, Final Batch Loss: 0.08779342472553253\n",
      "Epoch 3593, Loss: 0.32326143980026245, Final Batch Loss: 0.16465601325035095\n",
      "Epoch 3594, Loss: 0.22047992795705795, Final Batch Loss: 0.1043347492814064\n",
      "Epoch 3595, Loss: 0.22812288999557495, Final Batch Loss: 0.11184024065732956\n",
      "Epoch 3596, Loss: 0.2499445229768753, Final Batch Loss: 0.14747437834739685\n",
      "Epoch 3597, Loss: 0.24867995828390121, Final Batch Loss: 0.12802192568778992\n",
      "Epoch 3598, Loss: 0.26358000934123993, Final Batch Loss: 0.1298762559890747\n",
      "Epoch 3599, Loss: 0.21805954724550247, Final Batch Loss: 0.11116040498018265\n",
      "Epoch 3600, Loss: 0.268020361661911, Final Batch Loss: 0.13070279359817505\n",
      "Epoch 3601, Loss: 0.303986057639122, Final Batch Loss: 0.14977429807186127\n",
      "Epoch 3602, Loss: 0.22076895087957382, Final Batch Loss: 0.10083289444446564\n",
      "Epoch 3603, Loss: 0.17319784313440323, Final Batch Loss: 0.08091188222169876\n",
      "Epoch 3604, Loss: 0.3619331568479538, Final Batch Loss: 0.21541449427604675\n",
      "Epoch 3605, Loss: 0.17888668179512024, Final Batch Loss: 0.07446911185979843\n",
      "Epoch 3606, Loss: 0.3094107657670975, Final Batch Loss: 0.1781388372182846\n",
      "Epoch 3607, Loss: 0.210531584918499, Final Batch Loss: 0.08038734644651413\n",
      "Epoch 3608, Loss: 0.20606519281864166, Final Batch Loss: 0.0914737805724144\n",
      "Epoch 3609, Loss: 0.2138717621564865, Final Batch Loss: 0.10982496291399002\n",
      "Epoch 3610, Loss: 0.31441959738731384, Final Batch Loss: 0.19192783534526825\n",
      "Epoch 3611, Loss: 0.24990855902433395, Final Batch Loss: 0.14306288957595825\n",
      "Epoch 3612, Loss: 0.25832828879356384, Final Batch Loss: 0.14306330680847168\n",
      "Epoch 3613, Loss: 0.20753102004528046, Final Batch Loss: 0.09778386354446411\n",
      "Epoch 3614, Loss: 0.16377993673086166, Final Batch Loss: 0.04946715384721756\n",
      "Epoch 3615, Loss: 0.23373964428901672, Final Batch Loss: 0.125875785946846\n",
      "Epoch 3616, Loss: 0.2566901296377182, Final Batch Loss: 0.11716197431087494\n",
      "Epoch 3617, Loss: 0.3053247630596161, Final Batch Loss: 0.17595946788787842\n",
      "Epoch 3618, Loss: 0.22144874185323715, Final Batch Loss: 0.11151017993688583\n",
      "Epoch 3619, Loss: 0.25139449536800385, Final Batch Loss: 0.09894397854804993\n",
      "Epoch 3620, Loss: 0.2385142743587494, Final Batch Loss: 0.13223783671855927\n",
      "Epoch 3621, Loss: 0.21705611050128937, Final Batch Loss: 0.09982696920633316\n",
      "Epoch 3622, Loss: 0.22429868578910828, Final Batch Loss: 0.09358786046504974\n",
      "Epoch 3623, Loss: 0.26602674275636673, Final Batch Loss: 0.11738432198762894\n",
      "Epoch 3624, Loss: 0.19088923186063766, Final Batch Loss: 0.06580208986997604\n",
      "Epoch 3625, Loss: 0.1781991384923458, Final Batch Loss: 0.0596141554415226\n",
      "Epoch 3626, Loss: 0.23076848685741425, Final Batch Loss: 0.1362065225839615\n",
      "Epoch 3627, Loss: 0.22560105472803116, Final Batch Loss: 0.11082059144973755\n",
      "Epoch 3628, Loss: 0.23827575892210007, Final Batch Loss: 0.12213896214962006\n",
      "Epoch 3629, Loss: 0.23567475378513336, Final Batch Loss: 0.1356448084115982\n",
      "Epoch 3630, Loss: 0.2488638013601303, Final Batch Loss: 0.13647407293319702\n",
      "Epoch 3631, Loss: 0.19556470960378647, Final Batch Loss: 0.07384087145328522\n",
      "Epoch 3632, Loss: 0.18750938773155212, Final Batch Loss: 0.09585843980312347\n",
      "Epoch 3633, Loss: 0.17904986441135406, Final Batch Loss: 0.07192827016115189\n",
      "Epoch 3634, Loss: 0.17795779556035995, Final Batch Loss: 0.07761205732822418\n",
      "Epoch 3635, Loss: 0.31045272946357727, Final Batch Loss: 0.18770423531532288\n",
      "Epoch 3636, Loss: 0.2440711036324501, Final Batch Loss: 0.11626457422971725\n",
      "Epoch 3637, Loss: 0.25665878504514694, Final Batch Loss: 0.09611079841852188\n",
      "Epoch 3638, Loss: 0.20432457327842712, Final Batch Loss: 0.06866240501403809\n",
      "Epoch 3639, Loss: 0.26236703246831894, Final Batch Loss: 0.16707544028759003\n",
      "Epoch 3640, Loss: 0.31117427349090576, Final Batch Loss: 0.1982526332139969\n",
      "Epoch 3641, Loss: 0.29187096655368805, Final Batch Loss: 0.1705225557088852\n",
      "Epoch 3642, Loss: 0.24205619096755981, Final Batch Loss: 0.10921910405158997\n",
      "Epoch 3643, Loss: 0.2596764788031578, Final Batch Loss: 0.13975439965724945\n",
      "Epoch 3644, Loss: 0.27309589087963104, Final Batch Loss: 0.12779539823532104\n",
      "Epoch 3645, Loss: 0.2131279855966568, Final Batch Loss: 0.0974772572517395\n",
      "Epoch 3646, Loss: 0.28001822531223297, Final Batch Loss: 0.14667505025863647\n",
      "Epoch 3647, Loss: 0.21253594011068344, Final Batch Loss: 0.11348241567611694\n",
      "Epoch 3648, Loss: 0.24175763130187988, Final Batch Loss: 0.1240849643945694\n",
      "Epoch 3649, Loss: 0.2752469778060913, Final Batch Loss: 0.17395685613155365\n",
      "Epoch 3650, Loss: 0.24577971547842026, Final Batch Loss: 0.13211090862751007\n",
      "Epoch 3651, Loss: 0.2588222473859787, Final Batch Loss: 0.11444434523582458\n",
      "Epoch 3652, Loss: 0.23327220231294632, Final Batch Loss: 0.12962938845157623\n",
      "Epoch 3653, Loss: 0.21969012171030045, Final Batch Loss: 0.11915209144353867\n",
      "Epoch 3654, Loss: 0.24630091339349747, Final Batch Loss: 0.10269322246313095\n",
      "Epoch 3655, Loss: 0.1882794424891472, Final Batch Loss: 0.07309368997812271\n",
      "Epoch 3656, Loss: 0.344759076833725, Final Batch Loss: 0.12613950669765472\n",
      "Epoch 3657, Loss: 0.20690691471099854, Final Batch Loss: 0.09557043015956879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3658, Loss: 0.26121992617845535, Final Batch Loss: 0.15068887174129486\n",
      "Epoch 3659, Loss: 0.266486719250679, Final Batch Loss: 0.1383078694343567\n",
      "Epoch 3660, Loss: 0.28680409491062164, Final Batch Loss: 0.14053139090538025\n",
      "Epoch 3661, Loss: 0.2369092032313347, Final Batch Loss: 0.12300463020801544\n",
      "Epoch 3662, Loss: 0.22111745178699493, Final Batch Loss: 0.09540565311908722\n",
      "Epoch 3663, Loss: 0.2189928963780403, Final Batch Loss: 0.11187038570642471\n",
      "Epoch 3664, Loss: 0.1780613660812378, Final Batch Loss: 0.07358759641647339\n",
      "Epoch 3665, Loss: 0.2751188725233078, Final Batch Loss: 0.11735723912715912\n",
      "Epoch 3666, Loss: 0.19565751403570175, Final Batch Loss: 0.0899457111954689\n",
      "Epoch 3667, Loss: 0.277692973613739, Final Batch Loss: 0.12525136768817902\n",
      "Epoch 3668, Loss: 0.20407484471797943, Final Batch Loss: 0.08265653997659683\n",
      "Epoch 3669, Loss: 0.2938520908355713, Final Batch Loss: 0.15599125623703003\n",
      "Epoch 3670, Loss: 0.29358746111392975, Final Batch Loss: 0.12822768092155457\n",
      "Epoch 3671, Loss: 0.2478761300444603, Final Batch Loss: 0.12712305784225464\n",
      "Epoch 3672, Loss: 0.2594844475388527, Final Batch Loss: 0.13501058518886566\n",
      "Epoch 3673, Loss: 0.25982118397951126, Final Batch Loss: 0.14729970693588257\n",
      "Epoch 3674, Loss: 0.24934417009353638, Final Batch Loss: 0.12917298078536987\n",
      "Epoch 3675, Loss: 0.2261996939778328, Final Batch Loss: 0.11066744476556778\n",
      "Epoch 3676, Loss: 0.262061282992363, Final Batch Loss: 0.10203966498374939\n",
      "Epoch 3677, Loss: 0.23395892977714539, Final Batch Loss: 0.13165564835071564\n",
      "Epoch 3678, Loss: 0.1771933063864708, Final Batch Loss: 0.1094643697142601\n",
      "Epoch 3679, Loss: 0.23195524513721466, Final Batch Loss: 0.13509079813957214\n",
      "Epoch 3680, Loss: 0.3058081865310669, Final Batch Loss: 0.18028123676776886\n",
      "Epoch 3681, Loss: 0.24413780868053436, Final Batch Loss: 0.12793557345867157\n",
      "Epoch 3682, Loss: 0.2131810262799263, Final Batch Loss: 0.09972383826971054\n",
      "Epoch 3683, Loss: 0.2481524497270584, Final Batch Loss: 0.09592236578464508\n",
      "Epoch 3684, Loss: 0.20825332403182983, Final Batch Loss: 0.09685413539409637\n",
      "Epoch 3685, Loss: 0.22045491635799408, Final Batch Loss: 0.10365821421146393\n",
      "Epoch 3686, Loss: 0.22500834614038467, Final Batch Loss: 0.08349878340959549\n",
      "Epoch 3687, Loss: 0.2945525422692299, Final Batch Loss: 0.1871665120124817\n",
      "Epoch 3688, Loss: 0.21119388192892075, Final Batch Loss: 0.09162979573011398\n",
      "Epoch 3689, Loss: 0.27167195081710815, Final Batch Loss: 0.1539153754711151\n",
      "Epoch 3690, Loss: 0.3640699237585068, Final Batch Loss: 0.22920437157154083\n",
      "Epoch 3691, Loss: 0.21337149292230606, Final Batch Loss: 0.07771269232034683\n",
      "Epoch 3692, Loss: 0.17650505155324936, Final Batch Loss: 0.06876011192798615\n",
      "Epoch 3693, Loss: 0.21304920315742493, Final Batch Loss: 0.10779570788145065\n",
      "Epoch 3694, Loss: 0.173025693744421, Final Batch Loss: 0.05679470673203468\n",
      "Epoch 3695, Loss: 0.2400556355714798, Final Batch Loss: 0.11935219913721085\n",
      "Epoch 3696, Loss: 0.25398868322372437, Final Batch Loss: 0.12568718194961548\n",
      "Epoch 3697, Loss: 0.24791429936885834, Final Batch Loss: 0.13539093732833862\n",
      "Epoch 3698, Loss: 0.18229391425848007, Final Batch Loss: 0.07420863211154938\n",
      "Epoch 3699, Loss: 0.21091999113559723, Final Batch Loss: 0.07634471356868744\n",
      "Epoch 3700, Loss: 0.23402650654315948, Final Batch Loss: 0.1306663155555725\n",
      "Epoch 3701, Loss: 0.1845877766609192, Final Batch Loss: 0.09853964298963547\n",
      "Epoch 3702, Loss: 0.2585008963942528, Final Batch Loss: 0.15172280371189117\n",
      "Epoch 3703, Loss: 0.20591779053211212, Final Batch Loss: 0.07356482744216919\n",
      "Epoch 3704, Loss: 0.31849054992198944, Final Batch Loss: 0.18099603056907654\n",
      "Epoch 3705, Loss: 0.20644806325435638, Final Batch Loss: 0.0829755887389183\n",
      "Epoch 3706, Loss: 0.25508809834718704, Final Batch Loss: 0.13947881758213043\n",
      "Epoch 3707, Loss: 0.2285955846309662, Final Batch Loss: 0.12305258214473724\n",
      "Epoch 3708, Loss: 0.25647660344839096, Final Batch Loss: 0.1415168195962906\n",
      "Epoch 3709, Loss: 0.2070738449692726, Final Batch Loss: 0.10000333935022354\n",
      "Epoch 3710, Loss: 0.2091364711523056, Final Batch Loss: 0.09190525114536285\n",
      "Epoch 3711, Loss: 0.2836366668343544, Final Batch Loss: 0.17190992832183838\n",
      "Epoch 3712, Loss: 0.24610216915607452, Final Batch Loss: 0.0991458147764206\n",
      "Epoch 3713, Loss: 0.17836157232522964, Final Batch Loss: 0.10454651713371277\n",
      "Epoch 3714, Loss: 0.2543995901942253, Final Batch Loss: 0.1313847005367279\n",
      "Epoch 3715, Loss: 0.232866533100605, Final Batch Loss: 0.12251999229192734\n",
      "Epoch 3716, Loss: 0.17441700398921967, Final Batch Loss: 0.0875643789768219\n",
      "Epoch 3717, Loss: 0.3503880351781845, Final Batch Loss: 0.21448731422424316\n",
      "Epoch 3718, Loss: 0.22210927307605743, Final Batch Loss: 0.10700325667858124\n",
      "Epoch 3719, Loss: 0.2301117554306984, Final Batch Loss: 0.08860436826944351\n",
      "Epoch 3720, Loss: 0.21723590046167374, Final Batch Loss: 0.10696770250797272\n",
      "Epoch 3721, Loss: 0.2599281594157219, Final Batch Loss: 0.1371793895959854\n",
      "Epoch 3722, Loss: 0.27827195823192596, Final Batch Loss: 0.11374053359031677\n",
      "Epoch 3723, Loss: 0.27182769775390625, Final Batch Loss: 0.12972739338874817\n",
      "Epoch 3724, Loss: 0.2678031772375107, Final Batch Loss: 0.14659039676189423\n",
      "Epoch 3725, Loss: 0.387699618935585, Final Batch Loss: 0.2801319360733032\n",
      "Epoch 3726, Loss: 0.2537597045302391, Final Batch Loss: 0.07656151801347733\n",
      "Epoch 3727, Loss: 0.2193080484867096, Final Batch Loss: 0.13783442974090576\n",
      "Epoch 3728, Loss: 0.2513108626008034, Final Batch Loss: 0.1533651053905487\n",
      "Epoch 3729, Loss: 0.3300039768218994, Final Batch Loss: 0.18979758024215698\n",
      "Epoch 3730, Loss: 0.3185828924179077, Final Batch Loss: 0.20966117084026337\n",
      "Epoch 3731, Loss: 0.27588946372270584, Final Batch Loss: 0.11601222306489944\n",
      "Epoch 3732, Loss: 0.2620840221643448, Final Batch Loss: 0.1278291493654251\n",
      "Epoch 3733, Loss: 0.24618878960609436, Final Batch Loss: 0.136463925242424\n",
      "Epoch 3734, Loss: 0.2422950640320778, Final Batch Loss: 0.13159754872322083\n",
      "Epoch 3735, Loss: 0.293221190571785, Final Batch Loss: 0.17444230616092682\n",
      "Epoch 3736, Loss: 0.19543805718421936, Final Batch Loss: 0.07199726998806\n",
      "Epoch 3737, Loss: 0.24801091849803925, Final Batch Loss: 0.15721459686756134\n",
      "Epoch 3738, Loss: 0.18159926682710648, Final Batch Loss: 0.09161894768476486\n",
      "Epoch 3739, Loss: 0.246927872300148, Final Batch Loss: 0.11697760224342346\n",
      "Epoch 3740, Loss: 0.2474100962281227, Final Batch Loss: 0.14750084280967712\n",
      "Epoch 3741, Loss: 0.23786940425634384, Final Batch Loss: 0.09491089731454849\n",
      "Epoch 3742, Loss: 0.23091498017311096, Final Batch Loss: 0.105872243642807\n",
      "Epoch 3743, Loss: 0.17503392696380615, Final Batch Loss: 0.07370144128799438\n",
      "Epoch 3744, Loss: 0.2257620170712471, Final Batch Loss: 0.13795660436153412\n",
      "Epoch 3745, Loss: 0.20378337055444717, Final Batch Loss: 0.07248545438051224\n",
      "Epoch 3746, Loss: 0.18907785415649414, Final Batch Loss: 0.10016344487667084\n",
      "Epoch 3747, Loss: 0.2333763986825943, Final Batch Loss: 0.13073723018169403\n",
      "Epoch 3748, Loss: 0.20323185622692108, Final Batch Loss: 0.10137873888015747\n",
      "Epoch 3749, Loss: 0.25353699177503586, Final Batch Loss: 0.12373947352170944\n",
      "Epoch 3750, Loss: 0.22903089225292206, Final Batch Loss: 0.10469047725200653\n",
      "Epoch 3751, Loss: 0.29146724939346313, Final Batch Loss: 0.14605072140693665\n",
      "Epoch 3752, Loss: 0.210431307554245, Final Batch Loss: 0.12350360304117203\n",
      "Epoch 3753, Loss: 0.1912968009710312, Final Batch Loss: 0.09350437670946121\n",
      "Epoch 3754, Loss: 0.2199341505765915, Final Batch Loss: 0.0901346504688263\n",
      "Epoch 3755, Loss: 0.2552701532840729, Final Batch Loss: 0.13602854311466217\n",
      "Epoch 3756, Loss: 0.2918844148516655, Final Batch Loss: 0.19394956529140472\n",
      "Epoch 3757, Loss: 0.3432604968547821, Final Batch Loss: 0.2116227000951767\n",
      "Epoch 3758, Loss: 0.23153820633888245, Final Batch Loss: 0.11419214308261871\n",
      "Epoch 3759, Loss: 0.21678125858306885, Final Batch Loss: 0.08741496503353119\n",
      "Epoch 3760, Loss: 0.22223864495754242, Final Batch Loss: 0.08833853900432587\n",
      "Epoch 3761, Loss: 0.2224082574248314, Final Batch Loss: 0.1177937388420105\n",
      "Epoch 3762, Loss: 0.3106837868690491, Final Batch Loss: 0.15434882044792175\n",
      "Epoch 3763, Loss: 0.25705962628126144, Final Batch Loss: 0.16334590315818787\n",
      "Epoch 3764, Loss: 0.20095130801200867, Final Batch Loss: 0.09424425661563873\n",
      "Epoch 3765, Loss: 0.2441670522093773, Final Batch Loss: 0.12424395233392715\n",
      "Epoch 3766, Loss: 0.24839521199464798, Final Batch Loss: 0.12763848900794983\n",
      "Epoch 3767, Loss: 0.31506070494651794, Final Batch Loss: 0.1711507886648178\n",
      "Epoch 3768, Loss: 0.2177308276295662, Final Batch Loss: 0.11478529125452042\n",
      "Epoch 3769, Loss: 0.2912445291876793, Final Batch Loss: 0.09398286789655685\n",
      "Epoch 3770, Loss: 0.24245046079158783, Final Batch Loss: 0.13226453959941864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3771, Loss: 0.24841484427452087, Final Batch Loss: 0.12637247145175934\n",
      "Epoch 3772, Loss: 0.18328650668263435, Final Batch Loss: 0.04957840219140053\n",
      "Epoch 3773, Loss: 0.24109061807394028, Final Batch Loss: 0.12544682621955872\n",
      "Epoch 3774, Loss: 0.2739686295390129, Final Batch Loss: 0.09811874479055405\n",
      "Epoch 3775, Loss: 0.25273969024419785, Final Batch Loss: 0.1293209344148636\n",
      "Epoch 3776, Loss: 0.21504800766706467, Final Batch Loss: 0.0946987196803093\n",
      "Epoch 3777, Loss: 0.223761148750782, Final Batch Loss: 0.09912526607513428\n",
      "Epoch 3778, Loss: 0.16021154075860977, Final Batch Loss: 0.08067577332258224\n",
      "Epoch 3779, Loss: 0.435310997068882, Final Batch Loss: 0.31870096921920776\n",
      "Epoch 3780, Loss: 0.2619955539703369, Final Batch Loss: 0.15934708714485168\n",
      "Epoch 3781, Loss: 0.3256145566701889, Final Batch Loss: 0.19315986335277557\n",
      "Epoch 3782, Loss: 0.19487299770116806, Final Batch Loss: 0.09040624648332596\n",
      "Epoch 3783, Loss: 0.1836465373635292, Final Batch Loss: 0.08555572479963303\n",
      "Epoch 3784, Loss: 0.279587559401989, Final Batch Loss: 0.1553650051355362\n",
      "Epoch 3785, Loss: 0.2188621684908867, Final Batch Loss: 0.10649733245372772\n",
      "Epoch 3786, Loss: 0.2601243928074837, Final Batch Loss: 0.11426802724599838\n",
      "Epoch 3787, Loss: 0.2675948143005371, Final Batch Loss: 0.136627659201622\n",
      "Epoch 3788, Loss: 0.21762149035930634, Final Batch Loss: 0.12280772626399994\n",
      "Epoch 3789, Loss: 0.25811755657196045, Final Batch Loss: 0.1401732712984085\n",
      "Epoch 3790, Loss: 0.2061811089515686, Final Batch Loss: 0.08962777256965637\n",
      "Epoch 3791, Loss: 0.2294391170144081, Final Batch Loss: 0.0859658345580101\n",
      "Epoch 3792, Loss: 0.25380484014749527, Final Batch Loss: 0.1304159015417099\n",
      "Epoch 3793, Loss: 0.21506988257169724, Final Batch Loss: 0.07249941676855087\n",
      "Epoch 3794, Loss: 0.18079600483179092, Final Batch Loss: 0.07831256836652756\n",
      "Epoch 3795, Loss: 0.17663133889436722, Final Batch Loss: 0.10035388171672821\n",
      "Epoch 3796, Loss: 0.22387883067131042, Final Batch Loss: 0.12312112003564835\n",
      "Epoch 3797, Loss: 0.18162447214126587, Final Batch Loss: 0.0802919939160347\n",
      "Epoch 3798, Loss: 0.1899031326174736, Final Batch Loss: 0.08936798572540283\n",
      "Epoch 3799, Loss: 0.2133108377456665, Final Batch Loss: 0.10158756375312805\n",
      "Epoch 3800, Loss: 0.2457544207572937, Final Batch Loss: 0.09663638472557068\n",
      "Epoch 3801, Loss: 0.1943899169564247, Final Batch Loss: 0.07741512358188629\n",
      "Epoch 3802, Loss: 0.1499822959303856, Final Batch Loss: 0.0697767361998558\n",
      "Epoch 3803, Loss: 0.2892039194703102, Final Batch Loss: 0.2031329870223999\n",
      "Epoch 3804, Loss: 0.1892598420381546, Final Batch Loss: 0.06708799302577972\n",
      "Epoch 3805, Loss: 0.2156657874584198, Final Batch Loss: 0.12343955785036087\n",
      "Epoch 3806, Loss: 0.2975916191935539, Final Batch Loss: 0.19601339101791382\n",
      "Epoch 3807, Loss: 0.16224615275859833, Final Batch Loss: 0.070546455681324\n",
      "Epoch 3808, Loss: 0.2469184696674347, Final Batch Loss: 0.10315290093421936\n",
      "Epoch 3809, Loss: 0.2898431122303009, Final Batch Loss: 0.15960945188999176\n",
      "Epoch 3810, Loss: 0.2746753394603729, Final Batch Loss: 0.13578492403030396\n",
      "Epoch 3811, Loss: 0.19437412172555923, Final Batch Loss: 0.08202484995126724\n",
      "Epoch 3812, Loss: 0.18811407685279846, Final Batch Loss: 0.08828253298997879\n",
      "Epoch 3813, Loss: 0.23366215080022812, Final Batch Loss: 0.12495075911283493\n",
      "Epoch 3814, Loss: 0.2344750389456749, Final Batch Loss: 0.10082892328500748\n",
      "Epoch 3815, Loss: 0.19898905605077744, Final Batch Loss: 0.08974391967058182\n",
      "Epoch 3816, Loss: 0.21220160275697708, Final Batch Loss: 0.08491017669439316\n",
      "Epoch 3817, Loss: 0.23804937303066254, Final Batch Loss: 0.11876972764730453\n",
      "Epoch 3818, Loss: 0.23608245700597763, Final Batch Loss: 0.12061131000518799\n",
      "Epoch 3819, Loss: 0.23478911817073822, Final Batch Loss: 0.08711647987365723\n",
      "Epoch 3820, Loss: 0.19917310774326324, Final Batch Loss: 0.09890386462211609\n",
      "Epoch 3821, Loss: 0.21811678260564804, Final Batch Loss: 0.11866199970245361\n",
      "Epoch 3822, Loss: 0.26767999678850174, Final Batch Loss: 0.1560502052307129\n",
      "Epoch 3823, Loss: 0.23361852765083313, Final Batch Loss: 0.1312885284423828\n",
      "Epoch 3824, Loss: 0.2129959687590599, Final Batch Loss: 0.11504989117383957\n",
      "Epoch 3825, Loss: 0.21121802181005478, Final Batch Loss: 0.10804550349712372\n",
      "Epoch 3826, Loss: 0.2291397675871849, Final Batch Loss: 0.14135617017745972\n",
      "Epoch 3827, Loss: 0.2195688635110855, Final Batch Loss: 0.09756066650152206\n",
      "Epoch 3828, Loss: 0.23070410639047623, Final Batch Loss: 0.1183023527264595\n",
      "Epoch 3829, Loss: 0.2511838600039482, Final Batch Loss: 0.11797968298196793\n",
      "Epoch 3830, Loss: 0.20716159045696259, Final Batch Loss: 0.11376732587814331\n",
      "Epoch 3831, Loss: 0.21628088504076004, Final Batch Loss: 0.11905206739902496\n",
      "Epoch 3832, Loss: 0.2467961087822914, Final Batch Loss: 0.1495320200920105\n",
      "Epoch 3833, Loss: 0.25163260102272034, Final Batch Loss: 0.15236330032348633\n",
      "Epoch 3834, Loss: 0.250652976334095, Final Batch Loss: 0.13052108883857727\n",
      "Epoch 3835, Loss: 0.2623824402689934, Final Batch Loss: 0.1680583506822586\n",
      "Epoch 3836, Loss: 0.23030190914869308, Final Batch Loss: 0.11236962676048279\n",
      "Epoch 3837, Loss: 0.28118880093097687, Final Batch Loss: 0.11490090191364288\n",
      "Epoch 3838, Loss: 0.2624504119157791, Final Batch Loss: 0.13576266169548035\n",
      "Epoch 3839, Loss: 0.19880864024162292, Final Batch Loss: 0.12934991717338562\n",
      "Epoch 3840, Loss: 0.2511185109615326, Final Batch Loss: 0.09942677617073059\n",
      "Epoch 3841, Loss: 0.2864479273557663, Final Batch Loss: 0.16291938722133636\n",
      "Epoch 3842, Loss: 0.28071513772010803, Final Batch Loss: 0.11938194930553436\n",
      "Epoch 3843, Loss: 0.2141062393784523, Final Batch Loss: 0.11271679401397705\n",
      "Epoch 3844, Loss: 0.168307863175869, Final Batch Loss: 0.07444322109222412\n",
      "Epoch 3845, Loss: 0.22721079736948013, Final Batch Loss: 0.13686035573482513\n",
      "Epoch 3846, Loss: 0.24830017983913422, Final Batch Loss: 0.12739309668540955\n",
      "Epoch 3847, Loss: 0.34244251251220703, Final Batch Loss: 0.163363978266716\n",
      "Epoch 3848, Loss: 0.22765767574310303, Final Batch Loss: 0.09154161810874939\n",
      "Epoch 3849, Loss: 0.22201096266508102, Final Batch Loss: 0.11835236847400665\n",
      "Epoch 3850, Loss: 0.20526383817195892, Final Batch Loss: 0.0854724645614624\n",
      "Epoch 3851, Loss: 0.19416256248950958, Final Batch Loss: 0.07580544799566269\n",
      "Epoch 3852, Loss: 0.22376234829425812, Final Batch Loss: 0.10887786746025085\n",
      "Epoch 3853, Loss: 0.15826871246099472, Final Batch Loss: 0.08176838606595993\n",
      "Epoch 3854, Loss: 0.1936357021331787, Final Batch Loss: 0.10070735216140747\n",
      "Epoch 3855, Loss: 0.1975172832608223, Final Batch Loss: 0.09531300514936447\n",
      "Epoch 3856, Loss: 0.22008055448532104, Final Batch Loss: 0.11743300408124924\n",
      "Epoch 3857, Loss: 0.2260274738073349, Final Batch Loss: 0.10632592439651489\n",
      "Epoch 3858, Loss: 0.23998641222715378, Final Batch Loss: 0.11154352873563766\n",
      "Epoch 3859, Loss: 0.29543063044548035, Final Batch Loss: 0.18196609616279602\n",
      "Epoch 3860, Loss: 0.2854740843176842, Final Batch Loss: 0.16082651913166046\n",
      "Epoch 3861, Loss: 0.1889980584383011, Final Batch Loss: 0.12196005135774612\n",
      "Epoch 3862, Loss: 0.21722593903541565, Final Batch Loss: 0.061609312891960144\n",
      "Epoch 3863, Loss: 0.23164156079292297, Final Batch Loss: 0.11849762499332428\n",
      "Epoch 3864, Loss: 0.2330026477575302, Final Batch Loss: 0.1082988753914833\n",
      "Epoch 3865, Loss: 0.19860462099313736, Final Batch Loss: 0.06304600089788437\n",
      "Epoch 3866, Loss: 0.2954186797142029, Final Batch Loss: 0.17054873704910278\n",
      "Epoch 3867, Loss: 0.21623662114143372, Final Batch Loss: 0.09900013357400894\n",
      "Epoch 3868, Loss: 0.21191765367984772, Final Batch Loss: 0.0987733006477356\n",
      "Epoch 3869, Loss: 0.18753056973218918, Final Batch Loss: 0.10633112490177155\n",
      "Epoch 3870, Loss: 0.24697043001651764, Final Batch Loss: 0.10674075782299042\n",
      "Epoch 3871, Loss: 0.2581338807940483, Final Batch Loss: 0.12179198116064072\n",
      "Epoch 3872, Loss: 0.1842651218175888, Final Batch Loss: 0.05612097680568695\n",
      "Epoch 3873, Loss: 0.17616671323776245, Final Batch Loss: 0.08400994539260864\n",
      "Epoch 3874, Loss: 0.2757067605853081, Final Batch Loss: 0.18322062492370605\n",
      "Epoch 3875, Loss: 0.2471192479133606, Final Batch Loss: 0.14078526198863983\n",
      "Epoch 3876, Loss: 0.19679179787635803, Final Batch Loss: 0.08782695233821869\n",
      "Epoch 3877, Loss: 0.23950152844190598, Final Batch Loss: 0.10301902145147324\n",
      "Epoch 3878, Loss: 0.29442035406827927, Final Batch Loss: 0.19261525571346283\n",
      "Epoch 3879, Loss: 0.21907299011945724, Final Batch Loss: 0.06999840587377548\n",
      "Epoch 3880, Loss: 0.23103000223636627, Final Batch Loss: 0.12138361483812332\n",
      "Epoch 3881, Loss: 0.21476636081933975, Final Batch Loss: 0.08190814405679703\n",
      "Epoch 3882, Loss: 0.23981823772192, Final Batch Loss: 0.1146412268280983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3883, Loss: 0.21908030658960342, Final Batch Loss: 0.09642721712589264\n",
      "Epoch 3884, Loss: 0.19965588301420212, Final Batch Loss: 0.10697294771671295\n",
      "Epoch 3885, Loss: 0.1878330409526825, Final Batch Loss: 0.10375278443098068\n",
      "Epoch 3886, Loss: 0.2093547210097313, Final Batch Loss: 0.13511385023593903\n",
      "Epoch 3887, Loss: 0.2810598686337471, Final Batch Loss: 0.11100590974092484\n",
      "Epoch 3888, Loss: 0.22343578934669495, Final Batch Loss: 0.0871935784816742\n",
      "Epoch 3889, Loss: 0.2930036410689354, Final Batch Loss: 0.1174183115363121\n",
      "Epoch 3890, Loss: 0.2150629758834839, Final Batch Loss: 0.12215742468833923\n",
      "Epoch 3891, Loss: 0.20550885796546936, Final Batch Loss: 0.08748336881399155\n",
      "Epoch 3892, Loss: 0.20949596166610718, Final Batch Loss: 0.06341968476772308\n",
      "Epoch 3893, Loss: 0.1953534260392189, Final Batch Loss: 0.07935196906328201\n",
      "Epoch 3894, Loss: 0.20918381214141846, Final Batch Loss: 0.11812794208526611\n",
      "Epoch 3895, Loss: 0.2631828933954239, Final Batch Loss: 0.18585632741451263\n",
      "Epoch 3896, Loss: 0.23626557737588882, Final Batch Loss: 0.08572535961866379\n",
      "Epoch 3897, Loss: 0.19454731047153473, Final Batch Loss: 0.09629256278276443\n",
      "Epoch 3898, Loss: 0.22006730735301971, Final Batch Loss: 0.08122898638248444\n",
      "Epoch 3899, Loss: 0.3057437539100647, Final Batch Loss: 0.16801804304122925\n",
      "Epoch 3900, Loss: 0.18775572627782822, Final Batch Loss: 0.1028289869427681\n",
      "Epoch 3901, Loss: 0.20722927153110504, Final Batch Loss: 0.11529380083084106\n",
      "Epoch 3902, Loss: 0.23688727617263794, Final Batch Loss: 0.1407320648431778\n",
      "Epoch 3903, Loss: 0.23378175497055054, Final Batch Loss: 0.1419861912727356\n",
      "Epoch 3904, Loss: 0.182315431535244, Final Batch Loss: 0.09958741813898087\n",
      "Epoch 3905, Loss: 0.23666256293654442, Final Batch Loss: 0.17654582858085632\n",
      "Epoch 3906, Loss: 0.23044230788946152, Final Batch Loss: 0.13553564250469208\n",
      "Epoch 3907, Loss: 0.2514065131545067, Final Batch Loss: 0.14113716781139374\n",
      "Epoch 3908, Loss: 0.16483110189437866, Final Batch Loss: 0.07484176009893417\n",
      "Epoch 3909, Loss: 0.21735049784183502, Final Batch Loss: 0.10765018314123154\n",
      "Epoch 3910, Loss: 0.32876458019018173, Final Batch Loss: 0.21870386600494385\n",
      "Epoch 3911, Loss: 0.2411375418305397, Final Batch Loss: 0.13285115361213684\n",
      "Epoch 3912, Loss: 0.19428588449954987, Final Batch Loss: 0.06703196465969086\n",
      "Epoch 3913, Loss: 0.2277841940522194, Final Batch Loss: 0.09237050265073776\n",
      "Epoch 3914, Loss: 0.1834363490343094, Final Batch Loss: 0.08012472838163376\n",
      "Epoch 3915, Loss: 0.2590426728129387, Final Batch Loss: 0.14730550348758698\n",
      "Epoch 3916, Loss: 0.20783887058496475, Final Batch Loss: 0.07959330826997757\n",
      "Epoch 3917, Loss: 0.21494752913713455, Final Batch Loss: 0.07806720584630966\n",
      "Epoch 3918, Loss: 0.22436806559562683, Final Batch Loss: 0.11121638119220734\n",
      "Epoch 3919, Loss: 0.22860580682754517, Final Batch Loss: 0.1048274114727974\n",
      "Epoch 3920, Loss: 0.1663946658372879, Final Batch Loss: 0.05904223769903183\n",
      "Epoch 3921, Loss: 0.19009190797805786, Final Batch Loss: 0.10478872805833817\n",
      "Epoch 3922, Loss: 0.2583988755941391, Final Batch Loss: 0.1318029761314392\n",
      "Epoch 3923, Loss: 0.22904399037361145, Final Batch Loss: 0.10299527645111084\n",
      "Epoch 3924, Loss: 0.2543725147843361, Final Batch Loss: 0.12061502784490585\n",
      "Epoch 3925, Loss: 0.20610033720731735, Final Batch Loss: 0.12899424135684967\n",
      "Epoch 3926, Loss: 0.24002275615930557, Final Batch Loss: 0.13394278287887573\n",
      "Epoch 3927, Loss: 0.22937944531440735, Final Batch Loss: 0.11590032279491425\n",
      "Epoch 3928, Loss: 0.19242648780345917, Final Batch Loss: 0.12311594188213348\n",
      "Epoch 3929, Loss: 0.22142694145441055, Final Batch Loss: 0.13629823923110962\n",
      "Epoch 3930, Loss: 0.24410168081521988, Final Batch Loss: 0.15951207280158997\n",
      "Epoch 3931, Loss: 0.22019998729228973, Final Batch Loss: 0.12095138430595398\n",
      "Epoch 3932, Loss: 0.24344348907470703, Final Batch Loss: 0.12777839601039886\n",
      "Epoch 3933, Loss: 0.36110909283161163, Final Batch Loss: 0.22435586154460907\n",
      "Epoch 3934, Loss: 0.22003932297229767, Final Batch Loss: 0.12332729250192642\n",
      "Epoch 3935, Loss: 0.21700390428304672, Final Batch Loss: 0.1291189193725586\n",
      "Epoch 3936, Loss: 0.25341854989528656, Final Batch Loss: 0.11853839457035065\n",
      "Epoch 3937, Loss: 0.22443408519029617, Final Batch Loss: 0.11229006201028824\n",
      "Epoch 3938, Loss: 0.2765376716852188, Final Batch Loss: 0.17404431104660034\n",
      "Epoch 3939, Loss: 0.24151354283094406, Final Batch Loss: 0.09545794874429703\n",
      "Epoch 3940, Loss: 0.2297416776418686, Final Batch Loss: 0.1360781490802765\n",
      "Epoch 3941, Loss: 0.23356060683727264, Final Batch Loss: 0.09120827913284302\n",
      "Epoch 3942, Loss: 0.2196260765194893, Final Batch Loss: 0.09161252528429031\n",
      "Epoch 3943, Loss: 0.2234121486544609, Final Batch Loss: 0.12726472318172455\n",
      "Epoch 3944, Loss: 0.17216821759939194, Final Batch Loss: 0.08812364190816879\n",
      "Epoch 3945, Loss: 0.22484775632619858, Final Batch Loss: 0.14030514657497406\n",
      "Epoch 3946, Loss: 0.1976340338587761, Final Batch Loss: 0.08371888101100922\n",
      "Epoch 3947, Loss: 0.21684576570987701, Final Batch Loss: 0.13271324336528778\n",
      "Epoch 3948, Loss: 0.2221589908003807, Final Batch Loss: 0.11200819164514542\n",
      "Epoch 3949, Loss: 0.26448553800582886, Final Batch Loss: 0.13949759304523468\n",
      "Epoch 3950, Loss: 0.20566421747207642, Final Batch Loss: 0.10893364995718002\n",
      "Epoch 3951, Loss: 0.21870841830968857, Final Batch Loss: 0.10780743509531021\n",
      "Epoch 3952, Loss: 0.2459060251712799, Final Batch Loss: 0.11669440567493439\n",
      "Epoch 3953, Loss: 0.18706495314836502, Final Batch Loss: 0.11337639391422272\n",
      "Epoch 3954, Loss: 0.1813882477581501, Final Batch Loss: 0.06085680052638054\n",
      "Epoch 3955, Loss: 0.1953365057706833, Final Batch Loss: 0.07012863457202911\n",
      "Epoch 3956, Loss: 0.19942881166934967, Final Batch Loss: 0.11076953262090683\n",
      "Epoch 3957, Loss: 0.22916246205568314, Final Batch Loss: 0.0961567834019661\n",
      "Epoch 3958, Loss: 0.27716299891471863, Final Batch Loss: 0.11129690706729889\n",
      "Epoch 3959, Loss: 0.15714943781495094, Final Batch Loss: 0.06187146529555321\n",
      "Epoch 3960, Loss: 0.2055269479751587, Final Batch Loss: 0.1077997162938118\n",
      "Epoch 3961, Loss: 0.1866900622844696, Final Batch Loss: 0.08781842142343521\n",
      "Epoch 3962, Loss: 0.2026970088481903, Final Batch Loss: 0.10543935000896454\n",
      "Epoch 3963, Loss: 0.25165847688913345, Final Batch Loss: 0.10259876400232315\n",
      "Epoch 3964, Loss: 0.2396426424384117, Final Batch Loss: 0.1374664455652237\n",
      "Epoch 3965, Loss: 0.21225199103355408, Final Batch Loss: 0.09698939323425293\n",
      "Epoch 3966, Loss: 0.2302362620830536, Final Batch Loss: 0.11809763312339783\n",
      "Epoch 3967, Loss: 0.20116792619228363, Final Batch Loss: 0.08375468850135803\n",
      "Epoch 3968, Loss: 0.3072183132171631, Final Batch Loss: 0.16832002997398376\n",
      "Epoch 3969, Loss: 0.21968961507081985, Final Batch Loss: 0.11916431784629822\n",
      "Epoch 3970, Loss: 0.2115204930305481, Final Batch Loss: 0.08710450679063797\n",
      "Epoch 3971, Loss: 0.16452885419130325, Final Batch Loss: 0.06954742968082428\n",
      "Epoch 3972, Loss: 0.22148945927619934, Final Batch Loss: 0.1217479333281517\n",
      "Epoch 3973, Loss: 0.22374114394187927, Final Batch Loss: 0.06501233577728271\n",
      "Epoch 3974, Loss: 0.21782457828521729, Final Batch Loss: 0.09477891027927399\n",
      "Epoch 3975, Loss: 0.28779061138629913, Final Batch Loss: 0.15192067623138428\n",
      "Epoch 3976, Loss: 0.40569861233234406, Final Batch Loss: 0.24717938899993896\n",
      "Epoch 3977, Loss: 0.17646856606006622, Final Batch Loss: 0.08808336406946182\n",
      "Epoch 3978, Loss: 0.20068760216236115, Final Batch Loss: 0.10071757435798645\n",
      "Epoch 3979, Loss: 0.17974510043859482, Final Batch Loss: 0.07736098766326904\n",
      "Epoch 3980, Loss: 0.24036266654729843, Final Batch Loss: 0.11000258475542068\n",
      "Epoch 3981, Loss: 0.1836622729897499, Final Batch Loss: 0.09539251774549484\n",
      "Epoch 3982, Loss: 0.26263173669576645, Final Batch Loss: 0.11088304966688156\n",
      "Epoch 3983, Loss: 0.20591894537210464, Final Batch Loss: 0.14910034835338593\n",
      "Epoch 3984, Loss: 0.22175948321819305, Final Batch Loss: 0.13024455308914185\n",
      "Epoch 3985, Loss: 0.24398273229599, Final Batch Loss: 0.14135953783988953\n",
      "Epoch 3986, Loss: 0.28788408637046814, Final Batch Loss: 0.14806081354618073\n",
      "Epoch 3987, Loss: 0.21743527799844742, Final Batch Loss: 0.11320021748542786\n",
      "Epoch 3988, Loss: 0.2810171768069267, Final Batch Loss: 0.11285126954317093\n",
      "Epoch 3989, Loss: 0.16245640814304352, Final Batch Loss: 0.0757579579949379\n",
      "Epoch 3990, Loss: 0.2407452017068863, Final Batch Loss: 0.15643440186977386\n",
      "Epoch 3991, Loss: 0.2314913645386696, Final Batch Loss: 0.1038435623049736\n",
      "Epoch 3992, Loss: 0.23880600929260254, Final Batch Loss: 0.14232231676578522\n",
      "Epoch 3993, Loss: 0.22627434879541397, Final Batch Loss: 0.055084891617298126\n",
      "Epoch 3994, Loss: 0.22148627787828445, Final Batch Loss: 0.08911431580781937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3995, Loss: 0.2552417293190956, Final Batch Loss: 0.13456088304519653\n",
      "Epoch 3996, Loss: 0.22780870646238327, Final Batch Loss: 0.14151136577129364\n",
      "Epoch 3997, Loss: 0.23527763783931732, Final Batch Loss: 0.10807134211063385\n",
      "Epoch 3998, Loss: 0.278532013297081, Final Batch Loss: 0.1304597109556198\n",
      "Epoch 3999, Loss: 0.2522125095129013, Final Batch Loss: 0.13918328285217285\n",
      "Epoch 4000, Loss: 0.24055420607328415, Final Batch Loss: 0.1027059331536293\n",
      "Epoch 4001, Loss: 0.2966403365135193, Final Batch Loss: 0.1887427419424057\n",
      "Epoch 4002, Loss: 0.2276948243379593, Final Batch Loss: 0.1382479965686798\n",
      "Epoch 4003, Loss: 0.30623912811279297, Final Batch Loss: 0.17536349594593048\n",
      "Epoch 4004, Loss: 0.21064206212759018, Final Batch Loss: 0.08667753636837006\n",
      "Epoch 4005, Loss: 0.24652926623821259, Final Batch Loss: 0.15733453631401062\n",
      "Epoch 4006, Loss: 0.3084639310836792, Final Batch Loss: 0.18930210173130035\n",
      "Epoch 4007, Loss: 0.26423005759716034, Final Batch Loss: 0.15175913274288177\n",
      "Epoch 4008, Loss: 0.19517695903778076, Final Batch Loss: 0.08108886331319809\n",
      "Epoch 4009, Loss: 0.1814306601881981, Final Batch Loss: 0.060823582112789154\n",
      "Epoch 4010, Loss: 0.24878918379545212, Final Batch Loss: 0.14373628795146942\n",
      "Epoch 4011, Loss: 0.2749118357896805, Final Batch Loss: 0.15269078314304352\n",
      "Epoch 4012, Loss: 0.21433603018522263, Final Batch Loss: 0.12956207990646362\n",
      "Epoch 4013, Loss: 0.25096240639686584, Final Batch Loss: 0.11618511378765106\n",
      "Epoch 4014, Loss: 0.19694148749113083, Final Batch Loss: 0.10825635492801666\n",
      "Epoch 4015, Loss: 0.21236015856266022, Final Batch Loss: 0.0840851217508316\n",
      "Epoch 4016, Loss: 0.20178431272506714, Final Batch Loss: 0.12096977233886719\n",
      "Epoch 4017, Loss: 0.22550438344478607, Final Batch Loss: 0.13474832475185394\n",
      "Epoch 4018, Loss: 0.25715775042772293, Final Batch Loss: 0.09426335245370865\n",
      "Epoch 4019, Loss: 0.26009100675582886, Final Batch Loss: 0.13217464089393616\n",
      "Epoch 4020, Loss: 0.2440558522939682, Final Batch Loss: 0.10204572975635529\n",
      "Epoch 4021, Loss: 0.23535262793302536, Final Batch Loss: 0.08694364875555038\n",
      "Epoch 4022, Loss: 0.1497015729546547, Final Batch Loss: 0.08103060722351074\n",
      "Epoch 4023, Loss: 0.21887072920799255, Final Batch Loss: 0.1522747427225113\n",
      "Epoch 4024, Loss: 0.34159349650144577, Final Batch Loss: 0.22958098351955414\n",
      "Epoch 4025, Loss: 0.22214952111244202, Final Batch Loss: 0.10287228971719742\n",
      "Epoch 4026, Loss: 0.24613883346319199, Final Batch Loss: 0.1016818955540657\n",
      "Epoch 4027, Loss: 0.19196268916130066, Final Batch Loss: 0.09186087548732758\n",
      "Epoch 4028, Loss: 0.20383348315954208, Final Batch Loss: 0.09362909942865372\n",
      "Epoch 4029, Loss: 0.2721168175339699, Final Batch Loss: 0.10803083330392838\n",
      "Epoch 4030, Loss: 0.1981237307190895, Final Batch Loss: 0.12274220585823059\n",
      "Epoch 4031, Loss: 0.1830328106880188, Final Batch Loss: 0.1079670712351799\n",
      "Epoch 4032, Loss: 0.19375557452440262, Final Batch Loss: 0.10352592170238495\n",
      "Epoch 4033, Loss: 0.17334654182195663, Final Batch Loss: 0.08009812980890274\n",
      "Epoch 4034, Loss: 0.20125354826450348, Final Batch Loss: 0.13398593664169312\n",
      "Epoch 4035, Loss: 0.19741328805685043, Final Batch Loss: 0.07508046925067902\n",
      "Epoch 4036, Loss: 0.20886404067277908, Final Batch Loss: 0.09047837555408478\n",
      "Epoch 4037, Loss: 0.1682724878191948, Final Batch Loss: 0.055658891797065735\n",
      "Epoch 4038, Loss: 0.2184952273964882, Final Batch Loss: 0.11475808173418045\n",
      "Epoch 4039, Loss: 0.26967377960681915, Final Batch Loss: 0.13221178948879242\n",
      "Epoch 4040, Loss: 0.21132271736860275, Final Batch Loss: 0.11690393835306168\n",
      "Epoch 4041, Loss: 0.22321924567222595, Final Batch Loss: 0.1214451789855957\n",
      "Epoch 4042, Loss: 0.17327431589365005, Final Batch Loss: 0.0955946296453476\n",
      "Epoch 4043, Loss: 0.20677021145820618, Final Batch Loss: 0.09345490485429764\n",
      "Epoch 4044, Loss: 0.2567405104637146, Final Batch Loss: 0.15194246172904968\n",
      "Epoch 4045, Loss: 0.21774549782276154, Final Batch Loss: 0.1027403175830841\n",
      "Epoch 4046, Loss: 0.2401977777481079, Final Batch Loss: 0.1355958729982376\n",
      "Epoch 4047, Loss: 0.22357524186372757, Final Batch Loss: 0.12459690123796463\n",
      "Epoch 4048, Loss: 0.21665333956480026, Final Batch Loss: 0.09732455760240555\n",
      "Epoch 4049, Loss: 0.19871849566698074, Final Batch Loss: 0.06587778776884079\n",
      "Epoch 4050, Loss: 0.2745174691081047, Final Batch Loss: 0.17710717022418976\n",
      "Epoch 4051, Loss: 0.23967143148183823, Final Batch Loss: 0.12399071455001831\n",
      "Epoch 4052, Loss: 0.2459055408835411, Final Batch Loss: 0.11612743884325027\n",
      "Epoch 4053, Loss: 0.16586482524871826, Final Batch Loss: 0.0794711485505104\n",
      "Epoch 4054, Loss: 0.21249928325414658, Final Batch Loss: 0.1284821480512619\n",
      "Epoch 4055, Loss: 0.24648884683847427, Final Batch Loss: 0.13092222809791565\n",
      "Epoch 4056, Loss: 0.21146011352539062, Final Batch Loss: 0.08562429249286652\n",
      "Epoch 4057, Loss: 0.18852034956216812, Final Batch Loss: 0.10774614661931992\n",
      "Epoch 4058, Loss: 0.19611234217882156, Final Batch Loss: 0.08873909711837769\n",
      "Epoch 4059, Loss: 0.35836105793714523, Final Batch Loss: 0.24528563022613525\n",
      "Epoch 4060, Loss: 0.2165306657552719, Final Batch Loss: 0.12817294895648956\n",
      "Epoch 4061, Loss: 0.21113645285367966, Final Batch Loss: 0.09008082002401352\n",
      "Epoch 4062, Loss: 0.2544446885585785, Final Batch Loss: 0.12087289988994598\n",
      "Epoch 4063, Loss: 0.1780199185013771, Final Batch Loss: 0.06416066735982895\n",
      "Epoch 4064, Loss: 0.23940186947584152, Final Batch Loss: 0.14444169402122498\n",
      "Epoch 4065, Loss: 0.1782773807644844, Final Batch Loss: 0.1047879233956337\n",
      "Epoch 4066, Loss: 0.22844529896974564, Final Batch Loss: 0.1386798471212387\n",
      "Epoch 4067, Loss: 0.19213741272687912, Final Batch Loss: 0.08376413583755493\n",
      "Epoch 4068, Loss: 0.1859310194849968, Final Batch Loss: 0.06641868501901627\n",
      "Epoch 4069, Loss: 0.224125437438488, Final Batch Loss: 0.1273002326488495\n",
      "Epoch 4070, Loss: 0.20099187642335892, Final Batch Loss: 0.10280636698007584\n",
      "Epoch 4071, Loss: 0.2227068990468979, Final Batch Loss: 0.13177837431430817\n",
      "Epoch 4072, Loss: 0.1378619372844696, Final Batch Loss: 0.06177324801683426\n",
      "Epoch 4073, Loss: 0.219697043299675, Final Batch Loss: 0.11764214932918549\n",
      "Epoch 4074, Loss: 0.17669078707695007, Final Batch Loss: 0.10916700959205627\n",
      "Epoch 4075, Loss: 0.3091468811035156, Final Batch Loss: 0.1884448230266571\n",
      "Epoch 4076, Loss: 0.24440058320760727, Final Batch Loss: 0.09293738752603531\n",
      "Epoch 4077, Loss: 0.17698298394680023, Final Batch Loss: 0.0865466296672821\n",
      "Epoch 4078, Loss: 0.2219151258468628, Final Batch Loss: 0.1564306765794754\n",
      "Epoch 4079, Loss: 0.24477515369653702, Final Batch Loss: 0.10518433898687363\n",
      "Epoch 4080, Loss: 0.1588211953639984, Final Batch Loss: 0.09303168952465057\n",
      "Epoch 4081, Loss: 0.18466555327177048, Final Batch Loss: 0.09790229797363281\n",
      "Epoch 4082, Loss: 0.24220901727676392, Final Batch Loss: 0.14959993958473206\n",
      "Epoch 4083, Loss: 0.2475854977965355, Final Batch Loss: 0.10449383407831192\n",
      "Epoch 4084, Loss: 0.26147107779979706, Final Batch Loss: 0.12786120176315308\n",
      "Epoch 4085, Loss: 0.19290170818567276, Final Batch Loss: 0.08094490319490433\n",
      "Epoch 4086, Loss: 0.22344151884317398, Final Batch Loss: 0.12784206867218018\n",
      "Epoch 4087, Loss: 0.22861475497484207, Final Batch Loss: 0.1387171894311905\n",
      "Epoch 4088, Loss: 0.16814759373664856, Final Batch Loss: 0.07825177162885666\n",
      "Epoch 4089, Loss: 0.18573562055826187, Final Batch Loss: 0.09487941116094589\n",
      "Epoch 4090, Loss: 0.2185162603855133, Final Batch Loss: 0.08558937907218933\n",
      "Epoch 4091, Loss: 0.20867744833230972, Final Batch Loss: 0.10949937999248505\n",
      "Epoch 4092, Loss: 0.18019956350326538, Final Batch Loss: 0.06271424144506454\n",
      "Epoch 4093, Loss: 0.196148581802845, Final Batch Loss: 0.08280956000089645\n",
      "Epoch 4094, Loss: 0.1961834728717804, Final Batch Loss: 0.07971754670143127\n",
      "Epoch 4095, Loss: 0.206645667552948, Final Batch Loss: 0.11926016211509705\n",
      "Epoch 4096, Loss: 0.19274942576885223, Final Batch Loss: 0.06209650635719299\n",
      "Epoch 4097, Loss: 0.23385076969861984, Final Batch Loss: 0.09937398880720139\n",
      "Epoch 4098, Loss: 0.1944665014743805, Final Batch Loss: 0.0831526666879654\n",
      "Epoch 4099, Loss: 0.21187423169612885, Final Batch Loss: 0.12312019616365433\n",
      "Epoch 4100, Loss: 0.21679440885782242, Final Batch Loss: 0.11296329647302628\n",
      "Epoch 4101, Loss: 0.18256405740976334, Final Batch Loss: 0.07576128095388412\n",
      "Epoch 4102, Loss: 0.20678751915693283, Final Batch Loss: 0.10334604978561401\n",
      "Epoch 4103, Loss: 0.2285946160554886, Final Batch Loss: 0.07019409537315369\n",
      "Epoch 4104, Loss: 0.16094472631812096, Final Batch Loss: 0.055427584797143936\n",
      "Epoch 4105, Loss: 0.20982675999403, Final Batch Loss: 0.07019063085317612\n",
      "Epoch 4106, Loss: 0.22761369496583939, Final Batch Loss: 0.14590249955654144\n",
      "Epoch 4107, Loss: 0.27604401111602783, Final Batch Loss: 0.17945906519889832\n",
      "Epoch 4108, Loss: 0.1881949007511139, Final Batch Loss: 0.08340676873922348\n",
      "Epoch 4109, Loss: 0.20577404648065567, Final Batch Loss: 0.0970325618982315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4110, Loss: 0.3328167498111725, Final Batch Loss: 0.17254026234149933\n",
      "Epoch 4111, Loss: 0.19270580261945724, Final Batch Loss: 0.09523864090442657\n",
      "Epoch 4112, Loss: 0.20784325152635574, Final Batch Loss: 0.10050308704376221\n",
      "Epoch 4113, Loss: 0.2356022447347641, Final Batch Loss: 0.16349443793296814\n",
      "Epoch 4114, Loss: 0.22048934549093246, Final Batch Loss: 0.12684223055839539\n",
      "Epoch 4115, Loss: 0.18945523351430893, Final Batch Loss: 0.05363360792398453\n",
      "Epoch 4116, Loss: 0.16622334718704224, Final Batch Loss: 0.08380936831235886\n",
      "Epoch 4117, Loss: 0.20923079550266266, Final Batch Loss: 0.0861673578619957\n",
      "Epoch 4118, Loss: 0.2175236940383911, Final Batch Loss: 0.1078038290143013\n",
      "Epoch 4119, Loss: 0.21801266074180603, Final Batch Loss: 0.08768357336521149\n",
      "Epoch 4120, Loss: 0.151121586561203, Final Batch Loss: 0.06598594784736633\n",
      "Epoch 4121, Loss: 0.2259514406323433, Final Batch Loss: 0.11178800463676453\n",
      "Epoch 4122, Loss: 0.244065523147583, Final Batch Loss: 0.14068643748760223\n",
      "Epoch 4123, Loss: 0.19612717628479004, Final Batch Loss: 0.09489050507545471\n",
      "Epoch 4124, Loss: 0.25924236327409744, Final Batch Loss: 0.1673058420419693\n",
      "Epoch 4125, Loss: 0.21446079015731812, Final Batch Loss: 0.11012348532676697\n",
      "Epoch 4126, Loss: 0.2972458004951477, Final Batch Loss: 0.17288365960121155\n",
      "Epoch 4127, Loss: 0.3240273892879486, Final Batch Loss: 0.21603868901729584\n",
      "Epoch 4128, Loss: 0.2557065784931183, Final Batch Loss: 0.1637001931667328\n",
      "Epoch 4129, Loss: 0.39156773686408997, Final Batch Loss: 0.212760329246521\n",
      "Epoch 4130, Loss: 0.27612172812223434, Final Batch Loss: 0.11043151468038559\n",
      "Epoch 4131, Loss: 0.2094012200832367, Final Batch Loss: 0.12345463037490845\n",
      "Epoch 4132, Loss: 0.18455207347869873, Final Batch Loss: 0.096690833568573\n",
      "Epoch 4133, Loss: 0.1918240189552307, Final Batch Loss: 0.07430515438318253\n",
      "Epoch 4134, Loss: 0.1868441253900528, Final Batch Loss: 0.07796206325292587\n",
      "Epoch 4135, Loss: 0.15699324011802673, Final Batch Loss: 0.07500484585762024\n",
      "Epoch 4136, Loss: 0.18348827213048935, Final Batch Loss: 0.10362284630537033\n",
      "Epoch 4137, Loss: 0.2947799116373062, Final Batch Loss: 0.16508741676807404\n",
      "Epoch 4138, Loss: 0.1774323582649231, Final Batch Loss: 0.0963236466050148\n",
      "Epoch 4139, Loss: 0.24310177564620972, Final Batch Loss: 0.09166999161243439\n",
      "Epoch 4140, Loss: 0.24045882374048233, Final Batch Loss: 0.08864637464284897\n",
      "Epoch 4141, Loss: 0.29269856214523315, Final Batch Loss: 0.15270361304283142\n",
      "Epoch 4142, Loss: 0.22600668668746948, Final Batch Loss: 0.095590740442276\n",
      "Epoch 4143, Loss: 0.24422341585159302, Final Batch Loss: 0.10558483004570007\n",
      "Epoch 4144, Loss: 0.21485155820846558, Final Batch Loss: 0.11767343431711197\n",
      "Epoch 4145, Loss: 0.15389597415924072, Final Batch Loss: 0.07658997923135757\n",
      "Epoch 4146, Loss: 0.17725517600774765, Final Batch Loss: 0.07777373492717743\n",
      "Epoch 4147, Loss: 0.2075594887137413, Final Batch Loss: 0.090753473341465\n",
      "Epoch 4148, Loss: 0.26958129554986954, Final Batch Loss: 0.1504225879907608\n",
      "Epoch 4149, Loss: 0.20869234204292297, Final Batch Loss: 0.12712422013282776\n",
      "Epoch 4150, Loss: 0.28393593430519104, Final Batch Loss: 0.18777480721473694\n",
      "Epoch 4151, Loss: 0.2355305328965187, Final Batch Loss: 0.1339033991098404\n",
      "Epoch 4152, Loss: 0.2568874880671501, Final Batch Loss: 0.11325038224458694\n",
      "Epoch 4153, Loss: 0.21008265763521194, Final Batch Loss: 0.11505942046642303\n",
      "Epoch 4154, Loss: 0.24138093739748, Final Batch Loss: 0.15316924452781677\n",
      "Epoch 4155, Loss: 0.19425801187753677, Final Batch Loss: 0.10189713537693024\n",
      "Epoch 4156, Loss: 0.22209309041500092, Final Batch Loss: 0.08905021846294403\n",
      "Epoch 4157, Loss: 0.20147493481636047, Final Batch Loss: 0.08655249327421188\n",
      "Epoch 4158, Loss: 0.1571945995092392, Final Batch Loss: 0.08602552115917206\n",
      "Epoch 4159, Loss: 0.1744517982006073, Final Batch Loss: 0.0943579152226448\n",
      "Epoch 4160, Loss: 0.18419693410396576, Final Batch Loss: 0.09558951109647751\n",
      "Epoch 4161, Loss: 0.23783326148986816, Final Batch Loss: 0.13544709980487823\n",
      "Epoch 4162, Loss: 0.2067498117685318, Final Batch Loss: 0.1017502173781395\n",
      "Epoch 4163, Loss: 0.16298986226320267, Final Batch Loss: 0.06827301532030106\n",
      "Epoch 4164, Loss: 0.1560100018978119, Final Batch Loss: 0.06634301692247391\n",
      "Epoch 4165, Loss: 0.1797747015953064, Final Batch Loss: 0.07724293321371078\n",
      "Epoch 4166, Loss: 0.24164347350597382, Final Batch Loss: 0.1403772085905075\n",
      "Epoch 4167, Loss: 0.2570212781429291, Final Batch Loss: 0.11410674452781677\n",
      "Epoch 4168, Loss: 0.23131323605775833, Final Batch Loss: 0.14680209755897522\n",
      "Epoch 4169, Loss: 0.23226111382246017, Final Batch Loss: 0.0944836214184761\n",
      "Epoch 4170, Loss: 0.20885376632213593, Final Batch Loss: 0.10156533867120743\n",
      "Epoch 4171, Loss: 0.17832861095666885, Final Batch Loss: 0.04836484044790268\n",
      "Epoch 4172, Loss: 0.19589205086231232, Final Batch Loss: 0.0976187065243721\n",
      "Epoch 4173, Loss: 0.20355486124753952, Final Batch Loss: 0.11540041863918304\n",
      "Epoch 4174, Loss: 0.20151878893375397, Final Batch Loss: 0.11358031630516052\n",
      "Epoch 4175, Loss: 0.25630851835012436, Final Batch Loss: 0.14204370975494385\n",
      "Epoch 4176, Loss: 0.1870632842183113, Final Batch Loss: 0.08443820476531982\n",
      "Epoch 4177, Loss: 0.2861458733677864, Final Batch Loss: 0.12117644399404526\n",
      "Epoch 4178, Loss: 0.19328726083040237, Final Batch Loss: 0.10394130647182465\n",
      "Epoch 4179, Loss: 0.19522088766098022, Final Batch Loss: 0.07287494838237762\n",
      "Epoch 4180, Loss: 0.3031765967607498, Final Batch Loss: 0.16708379983901978\n",
      "Epoch 4181, Loss: 0.20511803776025772, Final Batch Loss: 0.08247236162424088\n",
      "Epoch 4182, Loss: 0.24373408406972885, Final Batch Loss: 0.119110107421875\n",
      "Epoch 4183, Loss: 0.19566130638122559, Final Batch Loss: 0.11274794489145279\n",
      "Epoch 4184, Loss: 0.15211115777492523, Final Batch Loss: 0.07869627326726913\n",
      "Epoch 4185, Loss: 0.2264890894293785, Final Batch Loss: 0.11184361577033997\n",
      "Epoch 4186, Loss: 0.20250633358955383, Final Batch Loss: 0.10687724500894547\n",
      "Epoch 4187, Loss: 0.16514549776911736, Final Batch Loss: 0.1054564118385315\n",
      "Epoch 4188, Loss: 0.20623444765806198, Final Batch Loss: 0.10600905865430832\n",
      "Epoch 4189, Loss: 0.2515002191066742, Final Batch Loss: 0.10315917432308197\n",
      "Epoch 4190, Loss: 0.24158307909965515, Final Batch Loss: 0.12188015133142471\n",
      "Epoch 4191, Loss: 0.20953483134508133, Final Batch Loss: 0.0949721559882164\n",
      "Epoch 4192, Loss: 0.17713584750890732, Final Batch Loss: 0.08649294078350067\n",
      "Epoch 4193, Loss: 0.175973080098629, Final Batch Loss: 0.07169225066900253\n",
      "Epoch 4194, Loss: 0.22135958075523376, Final Batch Loss: 0.13005660474300385\n",
      "Epoch 4195, Loss: 0.18741148710250854, Final Batch Loss: 0.09142235666513443\n",
      "Epoch 4196, Loss: 0.2150908038020134, Final Batch Loss: 0.1154000535607338\n",
      "Epoch 4197, Loss: 0.1623029038310051, Final Batch Loss: 0.07762248814105988\n",
      "Epoch 4198, Loss: 0.34059280157089233, Final Batch Loss: 0.20751412212848663\n",
      "Epoch 4199, Loss: 0.21825777739286423, Final Batch Loss: 0.13716328144073486\n",
      "Epoch 4200, Loss: 0.20546699315309525, Final Batch Loss: 0.0741516575217247\n",
      "Epoch 4201, Loss: 0.20704208314418793, Final Batch Loss: 0.14357343316078186\n",
      "Epoch 4202, Loss: 0.22887077927589417, Final Batch Loss: 0.10869838297367096\n",
      "Epoch 4203, Loss: 0.20899195224046707, Final Batch Loss: 0.1096334233880043\n",
      "Epoch 4204, Loss: 0.2522977218031883, Final Batch Loss: 0.12130130082368851\n",
      "Epoch 4205, Loss: 0.26215507090091705, Final Batch Loss: 0.11938458681106567\n",
      "Epoch 4206, Loss: 0.22470199316740036, Final Batch Loss: 0.10199925303459167\n",
      "Epoch 4207, Loss: 0.19640137255191803, Final Batch Loss: 0.08182357251644135\n",
      "Epoch 4208, Loss: 0.22515976428985596, Final Batch Loss: 0.12628890573978424\n",
      "Epoch 4209, Loss: 0.17198415100574493, Final Batch Loss: 0.07038331031799316\n",
      "Epoch 4210, Loss: 0.23265337944030762, Final Batch Loss: 0.1420278251171112\n",
      "Epoch 4211, Loss: 0.20420270413160324, Final Batch Loss: 0.12855319678783417\n",
      "Epoch 4212, Loss: 0.16129136085510254, Final Batch Loss: 0.08184217661619186\n",
      "Epoch 4213, Loss: 0.20765143632888794, Final Batch Loss: 0.09860754013061523\n",
      "Epoch 4214, Loss: 0.2542501613497734, Final Batch Loss: 0.17542420327663422\n",
      "Epoch 4215, Loss: 0.1667257770895958, Final Batch Loss: 0.05464398115873337\n",
      "Epoch 4216, Loss: 0.22214879095554352, Final Batch Loss: 0.11607102304697037\n",
      "Epoch 4217, Loss: 0.206293024122715, Final Batch Loss: 0.07505423575639725\n",
      "Epoch 4218, Loss: 0.2773952782154083, Final Batch Loss: 0.14913269877433777\n",
      "Epoch 4219, Loss: 0.17931609600782394, Final Batch Loss: 0.0947546437382698\n",
      "Epoch 4220, Loss: 0.2240249067544937, Final Batch Loss: 0.12465360015630722\n",
      "Epoch 4221, Loss: 0.19027619808912277, Final Batch Loss: 0.07312936335802078\n",
      "Epoch 4222, Loss: 0.225830078125, Final Batch Loss: 0.10593143105506897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4223, Loss: 0.2249317169189453, Final Batch Loss: 0.1434738039970398\n",
      "Epoch 4224, Loss: 0.24211354553699493, Final Batch Loss: 0.09701010584831238\n",
      "Epoch 4225, Loss: 0.24326545000076294, Final Batch Loss: 0.1226256862282753\n",
      "Epoch 4226, Loss: 0.2732270583510399, Final Batch Loss: 0.11873861402273178\n",
      "Epoch 4227, Loss: 0.166649729013443, Final Batch Loss: 0.06863430142402649\n",
      "Epoch 4228, Loss: 0.20206185430288315, Final Batch Loss: 0.09536095708608627\n",
      "Epoch 4229, Loss: 0.325541116297245, Final Batch Loss: 0.22120246291160583\n",
      "Epoch 4230, Loss: 0.1762462966144085, Final Batch Loss: 0.06068749353289604\n",
      "Epoch 4231, Loss: 0.18678981810808182, Final Batch Loss: 0.10554342716932297\n",
      "Epoch 4232, Loss: 0.2804693952202797, Final Batch Loss: 0.17077192664146423\n",
      "Epoch 4233, Loss: 0.21793480962514877, Final Batch Loss: 0.1283939629793167\n",
      "Epoch 4234, Loss: 0.3019024580717087, Final Batch Loss: 0.15494489669799805\n",
      "Epoch 4235, Loss: 0.257171630859375, Final Batch Loss: 0.14616508781909943\n",
      "Epoch 4236, Loss: 0.2027440071105957, Final Batch Loss: 0.11130259186029434\n",
      "Epoch 4237, Loss: 0.3063742443919182, Final Batch Loss: 0.20814034342765808\n",
      "Epoch 4238, Loss: 0.2192404940724373, Final Batch Loss: 0.1057557463645935\n",
      "Epoch 4239, Loss: 0.15799568593502045, Final Batch Loss: 0.07674558460712433\n",
      "Epoch 4240, Loss: 0.28601592034101486, Final Batch Loss: 0.16102969646453857\n",
      "Epoch 4241, Loss: 0.2509574294090271, Final Batch Loss: 0.14989310503005981\n",
      "Epoch 4242, Loss: 0.2201540693640709, Final Batch Loss: 0.12071877717971802\n",
      "Epoch 4243, Loss: 0.27699898928403854, Final Batch Loss: 0.10437691956758499\n",
      "Epoch 4244, Loss: 0.2598838582634926, Final Batch Loss: 0.09402980655431747\n",
      "Epoch 4245, Loss: 0.339176282286644, Final Batch Loss: 0.2014359086751938\n",
      "Epoch 4246, Loss: 0.18458600342273712, Final Batch Loss: 0.08708710223436356\n",
      "Epoch 4247, Loss: 0.2235414981842041, Final Batch Loss: 0.1049516424536705\n",
      "Epoch 4248, Loss: 0.18584677577018738, Final Batch Loss: 0.08546265959739685\n",
      "Epoch 4249, Loss: 0.24042392522096634, Final Batch Loss: 0.12814153730869293\n",
      "Epoch 4250, Loss: 0.24621094018220901, Final Batch Loss: 0.12627549469470978\n",
      "Epoch 4251, Loss: 0.24080431461334229, Final Batch Loss: 0.08757080137729645\n",
      "Epoch 4252, Loss: 0.26496966928243637, Final Batch Loss: 0.06343289464712143\n",
      "Epoch 4253, Loss: 0.25633352249860764, Final Batch Loss: 0.14903375506401062\n",
      "Epoch 4254, Loss: 0.1650589108467102, Final Batch Loss: 0.07877937704324722\n",
      "Epoch 4255, Loss: 0.2590112015604973, Final Batch Loss: 0.10667753964662552\n",
      "Epoch 4256, Loss: 0.20589280873537064, Final Batch Loss: 0.10411074012517929\n",
      "Epoch 4257, Loss: 0.19648629426956177, Final Batch Loss: 0.09870270639657974\n",
      "Epoch 4258, Loss: 0.1849987953901291, Final Batch Loss: 0.06701561063528061\n",
      "Epoch 4259, Loss: 0.2007291316986084, Final Batch Loss: 0.07085908949375153\n",
      "Epoch 4260, Loss: 0.2605714797973633, Final Batch Loss: 0.12310236692428589\n",
      "Epoch 4261, Loss: 0.17447619885206223, Final Batch Loss: 0.08399759978055954\n",
      "Epoch 4262, Loss: 0.14510053396224976, Final Batch Loss: 0.06351865082979202\n",
      "Epoch 4263, Loss: 0.17912615090608597, Final Batch Loss: 0.08955075591802597\n",
      "Epoch 4264, Loss: 0.19578971713781357, Final Batch Loss: 0.08881418406963348\n",
      "Epoch 4265, Loss: 0.18542777001857758, Final Batch Loss: 0.11419247090816498\n",
      "Epoch 4266, Loss: 0.1965157687664032, Final Batch Loss: 0.09652206301689148\n",
      "Epoch 4267, Loss: 0.2669057622551918, Final Batch Loss: 0.174345925450325\n",
      "Epoch 4268, Loss: 0.18591640144586563, Final Batch Loss: 0.10292563587427139\n",
      "Epoch 4269, Loss: 0.13674598932266235, Final Batch Loss: 0.05567770451307297\n",
      "Epoch 4270, Loss: 0.15669502317905426, Final Batch Loss: 0.08033674955368042\n",
      "Epoch 4271, Loss: 0.12679290771484375, Final Batch Loss: 0.06538128107786179\n",
      "Epoch 4272, Loss: 0.17255407944321632, Final Batch Loss: 0.11331365257501602\n",
      "Epoch 4273, Loss: 0.18823350593447685, Final Batch Loss: 0.13460494577884674\n",
      "Epoch 4274, Loss: 0.1656460016965866, Final Batch Loss: 0.07416131347417831\n",
      "Epoch 4275, Loss: 0.2281993180513382, Final Batch Loss: 0.10279183089733124\n",
      "Epoch 4276, Loss: 0.1802518218755722, Final Batch Loss: 0.08352939039468765\n",
      "Epoch 4277, Loss: 0.3032984882593155, Final Batch Loss: 0.15889112651348114\n",
      "Epoch 4278, Loss: 0.16877955198287964, Final Batch Loss: 0.09723170846700668\n",
      "Epoch 4279, Loss: 0.24035429954528809, Final Batch Loss: 0.13851036131381989\n",
      "Epoch 4280, Loss: 0.20135387778282166, Final Batch Loss: 0.09876784682273865\n",
      "Epoch 4281, Loss: 0.1833895593881607, Final Batch Loss: 0.10059911012649536\n",
      "Epoch 4282, Loss: 0.17895977199077606, Final Batch Loss: 0.11586883664131165\n",
      "Epoch 4283, Loss: 0.2176371067762375, Final Batch Loss: 0.1390392780303955\n",
      "Epoch 4284, Loss: 0.2099354863166809, Final Batch Loss: 0.08299840986728668\n",
      "Epoch 4285, Loss: 0.20110469311475754, Final Batch Loss: 0.10324189811944962\n",
      "Epoch 4286, Loss: 0.2351769283413887, Final Batch Loss: 0.1457349807024002\n",
      "Epoch 4287, Loss: 0.26119810342788696, Final Batch Loss: 0.12102861702442169\n",
      "Epoch 4288, Loss: 0.24452583491802216, Final Batch Loss: 0.15608114004135132\n",
      "Epoch 4289, Loss: 0.2706376761198044, Final Batch Loss: 0.11165076494216919\n",
      "Epoch 4290, Loss: 0.25432946532964706, Final Batch Loss: 0.11801622062921524\n",
      "Epoch 4291, Loss: 0.20218396931886673, Final Batch Loss: 0.11939990520477295\n",
      "Epoch 4292, Loss: 0.2765910401940346, Final Batch Loss: 0.1210179254412651\n",
      "Epoch 4293, Loss: 0.3035195544362068, Final Batch Loss: 0.1171538308262825\n",
      "Epoch 4294, Loss: 0.19910907745361328, Final Batch Loss: 0.10388962924480438\n",
      "Epoch 4295, Loss: 0.25559137016534805, Final Batch Loss: 0.11330380290746689\n",
      "Epoch 4296, Loss: 0.19886021316051483, Final Batch Loss: 0.10325796902179718\n",
      "Epoch 4297, Loss: 0.19510242342948914, Final Batch Loss: 0.10448846966028214\n",
      "Epoch 4298, Loss: 0.2327192947268486, Final Batch Loss: 0.09358217567205429\n",
      "Epoch 4299, Loss: 0.1850123628973961, Final Batch Loss: 0.08510380983352661\n",
      "Epoch 4300, Loss: 0.15332689136266708, Final Batch Loss: 0.06800337880849838\n",
      "Epoch 4301, Loss: 0.21257555484771729, Final Batch Loss: 0.10611779242753983\n",
      "Epoch 4302, Loss: 0.1785467192530632, Final Batch Loss: 0.09098753333091736\n",
      "Epoch 4303, Loss: 0.2387809306383133, Final Batch Loss: 0.1155826523900032\n",
      "Epoch 4304, Loss: 0.24116771668195724, Final Batch Loss: 0.12136220932006836\n",
      "Epoch 4305, Loss: 0.258511945605278, Final Batch Loss: 0.18061819672584534\n",
      "Epoch 4306, Loss: 0.25038956850767136, Final Batch Loss: 0.1364709436893463\n",
      "Epoch 4307, Loss: 0.18868112564086914, Final Batch Loss: 0.07266596704721451\n",
      "Epoch 4308, Loss: 0.20536989718675613, Final Batch Loss: 0.09048919379711151\n",
      "Epoch 4309, Loss: 0.21933526545763016, Final Batch Loss: 0.0816464051604271\n",
      "Epoch 4310, Loss: 0.20755641162395477, Final Batch Loss: 0.11720003187656403\n",
      "Epoch 4311, Loss: 0.23255502432584763, Final Batch Loss: 0.14006082713603973\n",
      "Epoch 4312, Loss: 0.20482537895441055, Final Batch Loss: 0.08500174432992935\n",
      "Epoch 4313, Loss: 0.2728506624698639, Final Batch Loss: 0.13395866751670837\n",
      "Epoch 4314, Loss: 0.2505817860364914, Final Batch Loss: 0.13503271341323853\n",
      "Epoch 4315, Loss: 0.18719901889562607, Final Batch Loss: 0.08969255536794662\n",
      "Epoch 4316, Loss: 0.2077678218483925, Final Batch Loss: 0.10503935813903809\n",
      "Epoch 4317, Loss: 0.23611138761043549, Final Batch Loss: 0.1082497388124466\n",
      "Epoch 4318, Loss: 0.18097221851348877, Final Batch Loss: 0.10498607903718948\n",
      "Epoch 4319, Loss: 0.23651251941919327, Final Batch Loss: 0.11110449582338333\n",
      "Epoch 4320, Loss: 0.20622336119413376, Final Batch Loss: 0.09616708755493164\n",
      "Epoch 4321, Loss: 0.1772063449025154, Final Batch Loss: 0.08800649642944336\n",
      "Epoch 4322, Loss: 0.18212288618087769, Final Batch Loss: 0.10284841805696487\n",
      "Epoch 4323, Loss: 0.2020886391401291, Final Batch Loss: 0.08570291846990585\n",
      "Epoch 4324, Loss: 0.2166515812277794, Final Batch Loss: 0.1351214498281479\n",
      "Epoch 4325, Loss: 0.21488898247480392, Final Batch Loss: 0.07513288408517838\n",
      "Epoch 4326, Loss: 0.20338881760835648, Final Batch Loss: 0.10125468671321869\n",
      "Epoch 4327, Loss: 0.23011188209056854, Final Batch Loss: 0.10105475783348083\n",
      "Epoch 4328, Loss: 0.20639584958553314, Final Batch Loss: 0.0903005376458168\n",
      "Epoch 4329, Loss: 0.22724216431379318, Final Batch Loss: 0.099791519343853\n",
      "Epoch 4330, Loss: 0.18774447590112686, Final Batch Loss: 0.08475486934185028\n",
      "Epoch 4331, Loss: 0.22539423406124115, Final Batch Loss: 0.10854356735944748\n",
      "Epoch 4332, Loss: 0.2771490514278412, Final Batch Loss: 0.1251261681318283\n",
      "Epoch 4333, Loss: 0.16036423295736313, Final Batch Loss: 0.09482116997241974\n",
      "Epoch 4334, Loss: 0.17148639261722565, Final Batch Loss: 0.07347096502780914\n",
      "Epoch 4335, Loss: 0.2523720785975456, Final Batch Loss: 0.15030115842819214\n",
      "Epoch 4336, Loss: 0.19488475471735, Final Batch Loss: 0.08109699189662933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4337, Loss: 0.2075548842549324, Final Batch Loss: 0.08010151237249374\n",
      "Epoch 4338, Loss: 0.21651417762041092, Final Batch Loss: 0.09554290026426315\n",
      "Epoch 4339, Loss: 0.311959907412529, Final Batch Loss: 0.1615809053182602\n",
      "Epoch 4340, Loss: 0.1803993061184883, Final Batch Loss: 0.08713194727897644\n",
      "Epoch 4341, Loss: 0.21199370920658112, Final Batch Loss: 0.11434230953454971\n",
      "Epoch 4342, Loss: 0.17499911040067673, Final Batch Loss: 0.09786199033260345\n",
      "Epoch 4343, Loss: 0.2760689854621887, Final Batch Loss: 0.15352636575698853\n",
      "Epoch 4344, Loss: 0.29399100691080093, Final Batch Loss: 0.1155344769358635\n",
      "Epoch 4345, Loss: 0.21849096566438675, Final Batch Loss: 0.1332160383462906\n",
      "Epoch 4346, Loss: 0.19785889238119125, Final Batch Loss: 0.07774395495653152\n",
      "Epoch 4347, Loss: 0.24187052994966507, Final Batch Loss: 0.12425863742828369\n",
      "Epoch 4348, Loss: 0.13028960302472115, Final Batch Loss: 0.07323098182678223\n",
      "Epoch 4349, Loss: 0.22467408329248428, Final Batch Loss: 0.09790881723165512\n",
      "Epoch 4350, Loss: 0.24131402373313904, Final Batch Loss: 0.13958624005317688\n",
      "Epoch 4351, Loss: 0.21487706154584885, Final Batch Loss: 0.07350114732980728\n",
      "Epoch 4352, Loss: 0.1820155307650566, Final Batch Loss: 0.06780797243118286\n",
      "Epoch 4353, Loss: 0.16237106919288635, Final Batch Loss: 0.1158999502658844\n",
      "Epoch 4354, Loss: 0.23926515877246857, Final Batch Loss: 0.14386145770549774\n",
      "Epoch 4355, Loss: 0.23437943309545517, Final Batch Loss: 0.08642127364873886\n",
      "Epoch 4356, Loss: 0.20177137851715088, Final Batch Loss: 0.0894610583782196\n",
      "Epoch 4357, Loss: 0.17214015126228333, Final Batch Loss: 0.0695970207452774\n",
      "Epoch 4358, Loss: 0.12734494730830193, Final Batch Loss: 0.06556107103824615\n",
      "Epoch 4359, Loss: 0.1848057024180889, Final Batch Loss: 0.12496115267276764\n",
      "Epoch 4360, Loss: 0.20517770946025848, Final Batch Loss: 0.11384274810552597\n",
      "Epoch 4361, Loss: 0.20634066313505173, Final Batch Loss: 0.09954186528921127\n",
      "Epoch 4362, Loss: 0.17948903888463974, Final Batch Loss: 0.06395351886749268\n",
      "Epoch 4363, Loss: 0.16076166927814484, Final Batch Loss: 0.06834398955106735\n",
      "Epoch 4364, Loss: 0.21452467143535614, Final Batch Loss: 0.11616919934749603\n",
      "Epoch 4365, Loss: 0.17640822380781174, Final Batch Loss: 0.08328664302825928\n",
      "Epoch 4366, Loss: 0.20148808509111404, Final Batch Loss: 0.09698796272277832\n",
      "Epoch 4367, Loss: 0.20935837924480438, Final Batch Loss: 0.08561822026968002\n",
      "Epoch 4368, Loss: 0.260739304125309, Final Batch Loss: 0.11319417506456375\n",
      "Epoch 4369, Loss: 0.16749024391174316, Final Batch Loss: 0.08560197055339813\n",
      "Epoch 4370, Loss: 0.1804983913898468, Final Batch Loss: 0.08528122305870056\n",
      "Epoch 4371, Loss: 0.208816297352314, Final Batch Loss: 0.09833388775587082\n",
      "Epoch 4372, Loss: 0.23005402088165283, Final Batch Loss: 0.14754228293895721\n",
      "Epoch 4373, Loss: 0.22189831733703613, Final Batch Loss: 0.1259939968585968\n",
      "Epoch 4374, Loss: 0.252082459628582, Final Batch Loss: 0.14548859000205994\n",
      "Epoch 4375, Loss: 0.2004460021853447, Final Batch Loss: 0.12011629343032837\n",
      "Epoch 4376, Loss: 0.2005862072110176, Final Batch Loss: 0.09257946163415909\n",
      "Epoch 4377, Loss: 0.25398218631744385, Final Batch Loss: 0.1154051423072815\n",
      "Epoch 4378, Loss: 0.18250222504138947, Final Batch Loss: 0.11471765488386154\n",
      "Epoch 4379, Loss: 0.1758878231048584, Final Batch Loss: 0.06092517077922821\n",
      "Epoch 4380, Loss: 0.2112184688448906, Final Batch Loss: 0.10426051914691925\n",
      "Epoch 4381, Loss: 0.23774022608995438, Final Batch Loss: 0.13401366770267487\n",
      "Epoch 4382, Loss: 0.17876968532800674, Final Batch Loss: 0.10022944211959839\n",
      "Epoch 4383, Loss: 0.1959773227572441, Final Batch Loss: 0.09942233562469482\n",
      "Epoch 4384, Loss: 0.19984714686870575, Final Batch Loss: 0.09974520653486252\n",
      "Epoch 4385, Loss: 0.22796698659658432, Final Batch Loss: 0.125271737575531\n",
      "Epoch 4386, Loss: 0.22796285897493362, Final Batch Loss: 0.129653662443161\n",
      "Epoch 4387, Loss: 0.2904164493083954, Final Batch Loss: 0.1442021280527115\n",
      "Epoch 4388, Loss: 0.21710039675235748, Final Batch Loss: 0.112285315990448\n",
      "Epoch 4389, Loss: 0.1764008104801178, Final Batch Loss: 0.08579494059085846\n",
      "Epoch 4390, Loss: 0.18549010902643204, Final Batch Loss: 0.10815216600894928\n",
      "Epoch 4391, Loss: 0.18600793182849884, Final Batch Loss: 0.06904679536819458\n",
      "Epoch 4392, Loss: 0.258758045732975, Final Batch Loss: 0.0641980692744255\n",
      "Epoch 4393, Loss: 0.21811818331480026, Final Batch Loss: 0.07830960303544998\n",
      "Epoch 4394, Loss: 0.24686894565820694, Final Batch Loss: 0.16499201953411102\n",
      "Epoch 4395, Loss: 0.1898517608642578, Final Batch Loss: 0.07866473495960236\n",
      "Epoch 4396, Loss: 0.1928354874253273, Final Batch Loss: 0.05005911737680435\n",
      "Epoch 4397, Loss: 0.1806727573275566, Final Batch Loss: 0.08738227933645248\n",
      "Epoch 4398, Loss: 0.18147173523902893, Final Batch Loss: 0.07693395018577576\n",
      "Epoch 4399, Loss: 0.1822994127869606, Final Batch Loss: 0.1024765819311142\n",
      "Epoch 4400, Loss: 0.2268921285867691, Final Batch Loss: 0.09173190593719482\n",
      "Epoch 4401, Loss: 0.20279592275619507, Final Batch Loss: 0.09024493396282196\n",
      "Epoch 4402, Loss: 0.14670686423778534, Final Batch Loss: 0.08048395067453384\n",
      "Epoch 4403, Loss: 0.18543179333209991, Final Batch Loss: 0.11227875202894211\n",
      "Epoch 4404, Loss: 0.22479314357042313, Final Batch Loss: 0.1378674954175949\n",
      "Epoch 4405, Loss: 0.1468280926346779, Final Batch Loss: 0.06404126435518265\n",
      "Epoch 4406, Loss: 0.23563750088214874, Final Batch Loss: 0.11781366914510727\n",
      "Epoch 4407, Loss: 0.2010464295744896, Final Batch Loss: 0.12123434990644455\n",
      "Epoch 4408, Loss: 0.23710621148347855, Final Batch Loss: 0.10262105613946915\n",
      "Epoch 4409, Loss: 0.19349367916584015, Final Batch Loss: 0.07240056991577148\n",
      "Epoch 4410, Loss: 0.19113857299089432, Final Batch Loss: 0.08513928949832916\n",
      "Epoch 4411, Loss: 0.16220443323254585, Final Batch Loss: 0.052416395395994186\n",
      "Epoch 4412, Loss: 0.2200651839375496, Final Batch Loss: 0.10488804429769516\n",
      "Epoch 4413, Loss: 0.16886306554079056, Final Batch Loss: 0.08220280706882477\n",
      "Epoch 4414, Loss: 0.1858656257390976, Final Batch Loss: 0.11098005622625351\n",
      "Epoch 4415, Loss: 0.18376683816313744, Final Batch Loss: 0.05362189933657646\n",
      "Epoch 4416, Loss: 0.2138846442103386, Final Batch Loss: 0.09695099294185638\n",
      "Epoch 4417, Loss: 0.18433479964733124, Final Batch Loss: 0.07361403107643127\n",
      "Epoch 4418, Loss: 0.19994457811117172, Final Batch Loss: 0.10457421839237213\n",
      "Epoch 4419, Loss: 0.257387213408947, Final Batch Loss: 0.13704726099967957\n",
      "Epoch 4420, Loss: 0.18134894967079163, Final Batch Loss: 0.08982405066490173\n",
      "Epoch 4421, Loss: 0.23333711922168732, Final Batch Loss: 0.1350439488887787\n",
      "Epoch 4422, Loss: 0.16392434388399124, Final Batch Loss: 0.08214791119098663\n",
      "Epoch 4423, Loss: 0.15090136975049973, Final Batch Loss: 0.07216991484165192\n",
      "Epoch 4424, Loss: 0.19404933601617813, Final Batch Loss: 0.0834137499332428\n",
      "Epoch 4425, Loss: 0.18053345382213593, Final Batch Loss: 0.10002610832452774\n",
      "Epoch 4426, Loss: 0.20910701155662537, Final Batch Loss: 0.08637846261262894\n",
      "Epoch 4427, Loss: 0.2190207988023758, Final Batch Loss: 0.1238693818449974\n",
      "Epoch 4428, Loss: 0.20999301224946976, Final Batch Loss: 0.11068467050790787\n",
      "Epoch 4429, Loss: 0.25227541476488113, Final Batch Loss: 0.10579667240381241\n",
      "Epoch 4430, Loss: 0.24617072939872742, Final Batch Loss: 0.10045963525772095\n",
      "Epoch 4431, Loss: 0.2302505299448967, Final Batch Loss: 0.12411577999591827\n",
      "Epoch 4432, Loss: 0.2393115684390068, Final Batch Loss: 0.1468169242143631\n",
      "Epoch 4433, Loss: 0.18213725835084915, Final Batch Loss: 0.057617947459220886\n",
      "Epoch 4434, Loss: 0.2592054605484009, Final Batch Loss: 0.13635188341140747\n",
      "Epoch 4435, Loss: 0.19247552752494812, Final Batch Loss: 0.07742869108915329\n",
      "Epoch 4436, Loss: 0.28795015811920166, Final Batch Loss: 0.15276461839675903\n",
      "Epoch 4437, Loss: 0.21239450573921204, Final Batch Loss: 0.07007817924022675\n",
      "Epoch 4438, Loss: 0.23910564929246902, Final Batch Loss: 0.13750281929969788\n",
      "Epoch 4439, Loss: 0.29280997812747955, Final Batch Loss: 0.1605646312236786\n",
      "Epoch 4440, Loss: 0.20976129174232483, Final Batch Loss: 0.11215843260288239\n",
      "Epoch 4441, Loss: 0.1929890215396881, Final Batch Loss: 0.07668755203485489\n",
      "Epoch 4442, Loss: 0.2401992306113243, Final Batch Loss: 0.10557819157838821\n",
      "Epoch 4443, Loss: 0.20535188913345337, Final Batch Loss: 0.10404878109693527\n",
      "Epoch 4444, Loss: 0.21949458867311478, Final Batch Loss: 0.08103183656930923\n",
      "Epoch 4445, Loss: 0.1726435050368309, Final Batch Loss: 0.09073417633771896\n",
      "Epoch 4446, Loss: 0.18158796429634094, Final Batch Loss: 0.11891528218984604\n",
      "Epoch 4447, Loss: 0.2798589617013931, Final Batch Loss: 0.0956476479768753\n",
      "Epoch 4448, Loss: 0.24718033522367477, Final Batch Loss: 0.15417969226837158\n",
      "Epoch 4449, Loss: 0.24004565924406052, Final Batch Loss: 0.1519855260848999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4450, Loss: 0.18286597728729248, Final Batch Loss: 0.07551492005586624\n",
      "Epoch 4451, Loss: 0.27844327688217163, Final Batch Loss: 0.13677754998207092\n",
      "Epoch 4452, Loss: 0.2211771234869957, Final Batch Loss: 0.11370570957660675\n",
      "Epoch 4453, Loss: 0.17841382324695587, Final Batch Loss: 0.06839116662740707\n",
      "Epoch 4454, Loss: 0.2089892029762268, Final Batch Loss: 0.12137484550476074\n",
      "Epoch 4455, Loss: 0.14507203549146652, Final Batch Loss: 0.0627676472067833\n",
      "Epoch 4456, Loss: 0.21507905423641205, Final Batch Loss: 0.12862151861190796\n",
      "Epoch 4457, Loss: 0.1761307492852211, Final Batch Loss: 0.07703227549791336\n",
      "Epoch 4458, Loss: 0.21796602755784988, Final Batch Loss: 0.07797285169363022\n",
      "Epoch 4459, Loss: 0.14531387388706207, Final Batch Loss: 0.06835572421550751\n",
      "Epoch 4460, Loss: 0.17441987991333008, Final Batch Loss: 0.07933668047189713\n",
      "Epoch 4461, Loss: 0.20844805240631104, Final Batch Loss: 0.09088282287120819\n",
      "Epoch 4462, Loss: 0.17581962049007416, Final Batch Loss: 0.07527457177639008\n",
      "Epoch 4463, Loss: 0.16170509532094002, Final Batch Loss: 0.060641463845968246\n",
      "Epoch 4464, Loss: 0.19854093343019485, Final Batch Loss: 0.07514563947916031\n",
      "Epoch 4465, Loss: 0.16897661238908768, Final Batch Loss: 0.0734754353761673\n",
      "Epoch 4466, Loss: 0.21521077305078506, Final Batch Loss: 0.10512776672840118\n",
      "Epoch 4467, Loss: 0.2844705432653427, Final Batch Loss: 0.17870593070983887\n",
      "Epoch 4468, Loss: 0.19293710589408875, Final Batch Loss: 0.10830206423997879\n",
      "Epoch 4469, Loss: 0.16518626362085342, Final Batch Loss: 0.08751803636550903\n",
      "Epoch 4470, Loss: 0.24753279983997345, Final Batch Loss: 0.15672172605991364\n",
      "Epoch 4471, Loss: 0.20278020203113556, Final Batch Loss: 0.09473878145217896\n",
      "Epoch 4472, Loss: 0.14717086032032967, Final Batch Loss: 0.06166444346308708\n",
      "Epoch 4473, Loss: 0.23878128081560135, Final Batch Loss: 0.1423989087343216\n",
      "Epoch 4474, Loss: 0.33650195598602295, Final Batch Loss: 0.1859458088874817\n",
      "Epoch 4475, Loss: 0.2615398019552231, Final Batch Loss: 0.1581529974937439\n",
      "Epoch 4476, Loss: 0.1841745674610138, Final Batch Loss: 0.10431896150112152\n",
      "Epoch 4477, Loss: 0.19015651941299438, Final Batch Loss: 0.10478334873914719\n",
      "Epoch 4478, Loss: 0.258407287299633, Final Batch Loss: 0.17203912138938904\n",
      "Epoch 4479, Loss: 0.2095397785305977, Final Batch Loss: 0.12016002088785172\n",
      "Epoch 4480, Loss: 0.19594267010688782, Final Batch Loss: 0.11021802574396133\n",
      "Epoch 4481, Loss: 0.17964081466197968, Final Batch Loss: 0.09309106320142746\n",
      "Epoch 4482, Loss: 0.1848532035946846, Final Batch Loss: 0.09899730980396271\n",
      "Epoch 4483, Loss: 0.2908274531364441, Final Batch Loss: 0.20356670022010803\n",
      "Epoch 4484, Loss: 0.19662530347704887, Final Batch Loss: 0.05397367104887962\n",
      "Epoch 4485, Loss: 0.19882533699274063, Final Batch Loss: 0.12199770659208298\n",
      "Epoch 4486, Loss: 0.23114978522062302, Final Batch Loss: 0.13693779706954956\n",
      "Epoch 4487, Loss: 0.21722105145454407, Final Batch Loss: 0.08042643964290619\n",
      "Epoch 4488, Loss: 0.23488877713680267, Final Batch Loss: 0.15844908356666565\n",
      "Epoch 4489, Loss: 0.17280656844377518, Final Batch Loss: 0.09785232692956924\n",
      "Epoch 4490, Loss: 0.3088562786579132, Final Batch Loss: 0.13939447700977325\n",
      "Epoch 4491, Loss: 0.23659129440784454, Final Batch Loss: 0.10231451690196991\n",
      "Epoch 4492, Loss: 0.21072959154844284, Final Batch Loss: 0.09893255680799484\n",
      "Epoch 4493, Loss: 0.16054119169712067, Final Batch Loss: 0.07624179124832153\n",
      "Epoch 4494, Loss: 0.17873010784387589, Final Batch Loss: 0.09156731516122818\n",
      "Epoch 4495, Loss: 0.13122916594147682, Final Batch Loss: 0.07894540578126907\n",
      "Epoch 4496, Loss: 0.22744014859199524, Final Batch Loss: 0.10475219041109085\n",
      "Epoch 4497, Loss: 0.1807909831404686, Final Batch Loss: 0.07550105452537537\n",
      "Epoch 4498, Loss: 0.21646487712860107, Final Batch Loss: 0.12297338247299194\n",
      "Epoch 4499, Loss: 0.20868760347366333, Final Batch Loss: 0.11342377215623856\n",
      "Epoch 4500, Loss: 0.20520826429128647, Final Batch Loss: 0.0964580550789833\n",
      "Epoch 4501, Loss: 0.21424169838428497, Final Batch Loss: 0.09687841683626175\n",
      "Epoch 4502, Loss: 0.2755657881498337, Final Batch Loss: 0.17495079338550568\n",
      "Epoch 4503, Loss: 0.19104920327663422, Final Batch Loss: 0.09409110993146896\n",
      "Epoch 4504, Loss: 0.242644764482975, Final Batch Loss: 0.11488527804613113\n",
      "Epoch 4505, Loss: 0.16761194169521332, Final Batch Loss: 0.08010851591825485\n",
      "Epoch 4506, Loss: 0.1642913743853569, Final Batch Loss: 0.09686611592769623\n",
      "Epoch 4507, Loss: 0.19261323660612106, Final Batch Loss: 0.10701770335435867\n",
      "Epoch 4508, Loss: 0.17020587250590324, Final Batch Loss: 0.054639752954244614\n",
      "Epoch 4509, Loss: 0.231211818754673, Final Batch Loss: 0.11238744109869003\n",
      "Epoch 4510, Loss: 0.2055165395140648, Final Batch Loss: 0.07148716598749161\n",
      "Epoch 4511, Loss: 0.1924528181552887, Final Batch Loss: 0.09718874841928482\n",
      "Epoch 4512, Loss: 0.26914335042238235, Final Batch Loss: 0.19208092987537384\n",
      "Epoch 4513, Loss: 0.2726496458053589, Final Batch Loss: 0.15103285014629364\n",
      "Epoch 4514, Loss: 0.18905072659254074, Final Batch Loss: 0.08878219872713089\n",
      "Epoch 4515, Loss: 0.17046089470386505, Final Batch Loss: 0.052046336233615875\n",
      "Epoch 4516, Loss: 0.21009323745965958, Final Batch Loss: 0.10948067158460617\n",
      "Epoch 4517, Loss: 0.17457273229956627, Final Batch Loss: 0.05510331317782402\n",
      "Epoch 4518, Loss: 0.23706429451704025, Final Batch Loss: 0.16937829554080963\n",
      "Epoch 4519, Loss: 0.2437475323677063, Final Batch Loss: 0.13415762782096863\n",
      "Epoch 4520, Loss: 0.16978193819522858, Final Batch Loss: 0.0797080546617508\n",
      "Epoch 4521, Loss: 0.1817273572087288, Final Batch Loss: 0.09748663008213043\n",
      "Epoch 4522, Loss: 0.2618727535009384, Final Batch Loss: 0.16524392366409302\n",
      "Epoch 4523, Loss: 0.1762731969356537, Final Batch Loss: 0.11188605427742004\n",
      "Epoch 4524, Loss: 0.2095131129026413, Final Batch Loss: 0.11414151638746262\n",
      "Epoch 4525, Loss: 0.22022615373134613, Final Batch Loss: 0.11285274475812912\n",
      "Epoch 4526, Loss: 0.18566104769706726, Final Batch Loss: 0.08316005021333694\n",
      "Epoch 4527, Loss: 0.21455668658018112, Final Batch Loss: 0.09543406218290329\n",
      "Epoch 4528, Loss: 0.19317077845335007, Final Batch Loss: 0.09986680746078491\n",
      "Epoch 4529, Loss: 0.21360258013010025, Final Batch Loss: 0.06818496435880661\n",
      "Epoch 4530, Loss: 0.14302539080381393, Final Batch Loss: 0.07603976875543594\n",
      "Epoch 4531, Loss: 0.23801682889461517, Final Batch Loss: 0.0888945609331131\n",
      "Epoch 4532, Loss: 0.1610296443104744, Final Batch Loss: 0.08361548185348511\n",
      "Epoch 4533, Loss: 0.23896898329257965, Final Batch Loss: 0.1227506771683693\n",
      "Epoch 4534, Loss: 0.1691589206457138, Final Batch Loss: 0.11891824007034302\n",
      "Epoch 4535, Loss: 0.2263823226094246, Final Batch Loss: 0.13561615347862244\n",
      "Epoch 4536, Loss: 0.15925658494234085, Final Batch Loss: 0.05896923691034317\n",
      "Epoch 4537, Loss: 0.20787034183740616, Final Batch Loss: 0.10233105719089508\n",
      "Epoch 4538, Loss: 0.17955905944108963, Final Batch Loss: 0.08382758498191833\n",
      "Epoch 4539, Loss: 0.14610228687524796, Final Batch Loss: 0.0676819309592247\n",
      "Epoch 4540, Loss: 0.2172594889998436, Final Batch Loss: 0.09699924290180206\n",
      "Epoch 4541, Loss: 0.15732856467366219, Final Batch Loss: 0.059559691697359085\n",
      "Epoch 4542, Loss: 0.18309804052114487, Final Batch Loss: 0.06950672715902328\n",
      "Epoch 4543, Loss: 0.1920257806777954, Final Batch Loss: 0.08596408367156982\n",
      "Epoch 4544, Loss: 0.24208679050207138, Final Batch Loss: 0.14394503831863403\n",
      "Epoch 4545, Loss: 0.23827067762613297, Final Batch Loss: 0.13724668323993683\n",
      "Epoch 4546, Loss: 0.21317964792251587, Final Batch Loss: 0.11180134117603302\n",
      "Epoch 4547, Loss: 0.22571051865816116, Final Batch Loss: 0.09305869787931442\n",
      "Epoch 4548, Loss: 0.19830497354269028, Final Batch Loss: 0.08467047661542892\n",
      "Epoch 4549, Loss: 0.2634877488017082, Final Batch Loss: 0.10573969036340714\n",
      "Epoch 4550, Loss: 0.24326161295175552, Final Batch Loss: 0.09944909065961838\n",
      "Epoch 4551, Loss: 0.24464207142591476, Final Batch Loss: 0.10585307329893112\n",
      "Epoch 4552, Loss: 0.16026314347982407, Final Batch Loss: 0.07878640294075012\n",
      "Epoch 4553, Loss: 0.2212689071893692, Final Batch Loss: 0.1489153653383255\n",
      "Epoch 4554, Loss: 0.24385440349578857, Final Batch Loss: 0.11710812151432037\n",
      "Epoch 4555, Loss: 0.21968860179185867, Final Batch Loss: 0.1257559210062027\n",
      "Epoch 4556, Loss: 0.2066548466682434, Final Batch Loss: 0.12328595668077469\n",
      "Epoch 4557, Loss: 0.14374969527125359, Final Batch Loss: 0.0615551732480526\n",
      "Epoch 4558, Loss: 0.15483754128217697, Final Batch Loss: 0.07073373347520828\n",
      "Epoch 4559, Loss: 0.23478130251169205, Final Batch Loss: 0.09436359256505966\n",
      "Epoch 4560, Loss: 0.17342598736286163, Final Batch Loss: 0.08555919677019119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4561, Loss: 0.18154702335596085, Final Batch Loss: 0.06359310448169708\n",
      "Epoch 4562, Loss: 0.20515019446611404, Final Batch Loss: 0.08159332722425461\n",
      "Epoch 4563, Loss: 0.2317010909318924, Final Batch Loss: 0.12609738111495972\n",
      "Epoch 4564, Loss: 0.17368608713150024, Final Batch Loss: 0.10055797547101974\n",
      "Epoch 4565, Loss: 0.16246102005243301, Final Batch Loss: 0.09463146328926086\n",
      "Epoch 4566, Loss: 0.2129187509417534, Final Batch Loss: 0.12176142632961273\n",
      "Epoch 4567, Loss: 0.2571898326277733, Final Batch Loss: 0.09424792975187302\n",
      "Epoch 4568, Loss: 0.1969277337193489, Final Batch Loss: 0.09879229962825775\n",
      "Epoch 4569, Loss: 0.2000475972890854, Final Batch Loss: 0.09165756404399872\n",
      "Epoch 4570, Loss: 0.20821496099233627, Final Batch Loss: 0.12447801232337952\n",
      "Epoch 4571, Loss: 0.21478526294231415, Final Batch Loss: 0.10727684199810028\n",
      "Epoch 4572, Loss: 0.22511909902095795, Final Batch Loss: 0.12166096270084381\n",
      "Epoch 4573, Loss: 0.22881296277046204, Final Batch Loss: 0.10945528745651245\n",
      "Epoch 4574, Loss: 0.19711200147867203, Final Batch Loss: 0.11642734706401825\n",
      "Epoch 4575, Loss: 0.19712676107883453, Final Batch Loss: 0.08887217938899994\n",
      "Epoch 4576, Loss: 0.15815730392932892, Final Batch Loss: 0.07869816571474075\n",
      "Epoch 4577, Loss: 0.2172614485025406, Final Batch Loss: 0.08697187900543213\n",
      "Epoch 4578, Loss: 0.14417899027466774, Final Batch Loss: 0.08873485028743744\n",
      "Epoch 4579, Loss: 0.17649507522583008, Final Batch Loss: 0.07482937723398209\n",
      "Epoch 4580, Loss: 0.22977153211832047, Final Batch Loss: 0.1084858849644661\n",
      "Epoch 4581, Loss: 0.17456229776144028, Final Batch Loss: 0.07158050686120987\n",
      "Epoch 4582, Loss: 0.16345816105604172, Final Batch Loss: 0.07069099694490433\n",
      "Epoch 4583, Loss: 0.20408156514167786, Final Batch Loss: 0.10580957680940628\n",
      "Epoch 4584, Loss: 0.1636207178235054, Final Batch Loss: 0.0680658221244812\n",
      "Epoch 4585, Loss: 0.23081818222999573, Final Batch Loss: 0.1080392450094223\n",
      "Epoch 4586, Loss: 0.17319270968437195, Final Batch Loss: 0.10070209950208664\n",
      "Epoch 4587, Loss: 0.20721664279699326, Final Batch Loss: 0.08140606433153152\n",
      "Epoch 4588, Loss: 0.22190216183662415, Final Batch Loss: 0.1217842623591423\n",
      "Epoch 4589, Loss: 0.2391654998064041, Final Batch Loss: 0.12885484099388123\n",
      "Epoch 4590, Loss: 0.32466641068458557, Final Batch Loss: 0.1832837611436844\n",
      "Epoch 4591, Loss: 0.20636947453022003, Final Batch Loss: 0.11192885041236877\n",
      "Epoch 4592, Loss: 0.20672542601823807, Final Batch Loss: 0.10059049725532532\n",
      "Epoch 4593, Loss: 0.20974702388048172, Final Batch Loss: 0.13186471164226532\n",
      "Epoch 4594, Loss: 0.2058078721165657, Final Batch Loss: 0.11808434128761292\n",
      "Epoch 4595, Loss: 0.2550700902938843, Final Batch Loss: 0.09509731829166412\n",
      "Epoch 4596, Loss: 0.13430415093898773, Final Batch Loss: 0.06359431892633438\n",
      "Epoch 4597, Loss: 0.2004585638642311, Final Batch Loss: 0.10875886678695679\n",
      "Epoch 4598, Loss: 0.19180669635534286, Final Batch Loss: 0.08424841612577438\n",
      "Epoch 4599, Loss: 0.23000800609588623, Final Batch Loss: 0.13467532396316528\n",
      "Epoch 4600, Loss: 0.1456015668809414, Final Batch Loss: 0.05740726366639137\n",
      "Epoch 4601, Loss: 0.22486337274312973, Final Batch Loss: 0.13466030359268188\n",
      "Epoch 4602, Loss: 0.18863502889871597, Final Batch Loss: 0.1022961214184761\n",
      "Epoch 4603, Loss: 0.4121043086051941, Final Batch Loss: 0.15960904955863953\n",
      "Epoch 4604, Loss: 0.17352578416466713, Final Batch Loss: 0.05964311584830284\n",
      "Epoch 4605, Loss: 0.18355702608823776, Final Batch Loss: 0.08620788902044296\n",
      "Epoch 4606, Loss: 0.20444820821285248, Final Batch Loss: 0.09719466418027878\n",
      "Epoch 4607, Loss: 0.1870870590209961, Final Batch Loss: 0.08098823577165604\n",
      "Epoch 4608, Loss: 0.20535966008901596, Final Batch Loss: 0.12339971214532852\n",
      "Epoch 4609, Loss: 0.19872862100601196, Final Batch Loss: 0.10201821476221085\n",
      "Epoch 4610, Loss: 0.14421585947275162, Final Batch Loss: 0.03988029062747955\n",
      "Epoch 4611, Loss: 0.18876010924577713, Final Batch Loss: 0.09922192990779877\n",
      "Epoch 4612, Loss: 0.23739256709814072, Final Batch Loss: 0.1207718774676323\n",
      "Epoch 4613, Loss: 0.2526469975709915, Final Batch Loss: 0.1260693520307541\n",
      "Epoch 4614, Loss: 0.19887813180685043, Final Batch Loss: 0.11660122871398926\n",
      "Epoch 4615, Loss: 0.22350049763917923, Final Batch Loss: 0.1186283528804779\n",
      "Epoch 4616, Loss: 0.2017717957496643, Final Batch Loss: 0.11964026093482971\n",
      "Epoch 4617, Loss: 0.23527111113071442, Final Batch Loss: 0.08013719320297241\n",
      "Epoch 4618, Loss: 0.2557365670800209, Final Batch Loss: 0.13523446023464203\n",
      "Epoch 4619, Loss: 0.1577313169836998, Final Batch Loss: 0.0831337571144104\n",
      "Epoch 4620, Loss: 0.26608172059059143, Final Batch Loss: 0.13736401498317719\n",
      "Epoch 4621, Loss: 0.2180037498474121, Final Batch Loss: 0.11592508107423782\n",
      "Epoch 4622, Loss: 0.1854773759841919, Final Batch Loss: 0.10955753177404404\n",
      "Epoch 4623, Loss: 0.2345341369509697, Final Batch Loss: 0.13654214143753052\n",
      "Epoch 4624, Loss: 0.13099976629018784, Final Batch Loss: 0.066661536693573\n",
      "Epoch 4625, Loss: 0.23423396050930023, Final Batch Loss: 0.12126772105693817\n",
      "Epoch 4626, Loss: 0.20911101251840591, Final Batch Loss: 0.1274290382862091\n",
      "Epoch 4627, Loss: 0.2633301615715027, Final Batch Loss: 0.15880592167377472\n",
      "Epoch 4628, Loss: 0.1611490473151207, Final Batch Loss: 0.06015487015247345\n",
      "Epoch 4629, Loss: 0.22321198135614395, Final Batch Loss: 0.09964444488286972\n",
      "Epoch 4630, Loss: 0.1950722113251686, Final Batch Loss: 0.12175065279006958\n",
      "Epoch 4631, Loss: 0.3093385770916939, Final Batch Loss: 0.19400209188461304\n",
      "Epoch 4632, Loss: 0.17138167470693588, Final Batch Loss: 0.06690790504217148\n",
      "Epoch 4633, Loss: 0.28767017275094986, Final Batch Loss: 0.10364719480276108\n",
      "Epoch 4634, Loss: 0.15948092192411423, Final Batch Loss: 0.06892147660255432\n",
      "Epoch 4635, Loss: 0.19502262026071548, Final Batch Loss: 0.12194189429283142\n",
      "Epoch 4636, Loss: 0.22302661836147308, Final Batch Loss: 0.10578545928001404\n",
      "Epoch 4637, Loss: 0.22261999547481537, Final Batch Loss: 0.13128826022148132\n",
      "Epoch 4638, Loss: 0.23993724584579468, Final Batch Loss: 0.12742005288600922\n",
      "Epoch 4639, Loss: 0.21309709548950195, Final Batch Loss: 0.10564172267913818\n",
      "Epoch 4640, Loss: 0.20346509665250778, Final Batch Loss: 0.12274115532636642\n",
      "Epoch 4641, Loss: 0.1646866872906685, Final Batch Loss: 0.06662685424089432\n",
      "Epoch 4642, Loss: 0.160772442817688, Final Batch Loss: 0.06032031029462814\n",
      "Epoch 4643, Loss: 0.21499241888523102, Final Batch Loss: 0.08181633055210114\n",
      "Epoch 4644, Loss: 0.19779199361801147, Final Batch Loss: 0.11222430318593979\n",
      "Epoch 4645, Loss: 0.16336285322904587, Final Batch Loss: 0.07022732496261597\n",
      "Epoch 4646, Loss: 0.2200143113732338, Final Batch Loss: 0.14582879841327667\n",
      "Epoch 4647, Loss: 0.14919790625572205, Final Batch Loss: 0.06839751452207565\n",
      "Epoch 4648, Loss: 0.21020042151212692, Final Batch Loss: 0.1102348119020462\n",
      "Epoch 4649, Loss: 0.21262219548225403, Final Batch Loss: 0.11364089697599411\n",
      "Epoch 4650, Loss: 0.21499435976147652, Final Batch Loss: 0.1626761555671692\n",
      "Epoch 4651, Loss: 0.18370848149061203, Final Batch Loss: 0.09239033609628677\n",
      "Epoch 4652, Loss: 0.20848239958286285, Final Batch Loss: 0.10372637212276459\n",
      "Epoch 4653, Loss: 0.14554047584533691, Final Batch Loss: 0.06656157225370407\n",
      "Epoch 4654, Loss: 0.23160415142774582, Final Batch Loss: 0.11856037378311157\n",
      "Epoch 4655, Loss: 0.16846508532762527, Final Batch Loss: 0.06930956989526749\n",
      "Epoch 4656, Loss: 0.15474778786301613, Final Batch Loss: 0.050764914602041245\n",
      "Epoch 4657, Loss: 0.11234571039676666, Final Batch Loss: 0.055129874497652054\n",
      "Epoch 4658, Loss: 0.22684380412101746, Final Batch Loss: 0.1482764333486557\n",
      "Epoch 4659, Loss: 0.1858340948820114, Final Batch Loss: 0.08344120532274246\n",
      "Epoch 4660, Loss: 0.18910929560661316, Final Batch Loss: 0.09603417664766312\n",
      "Epoch 4661, Loss: 0.2821217104792595, Final Batch Loss: 0.12220077961683273\n",
      "Epoch 4662, Loss: 0.2058195397257805, Final Batch Loss: 0.11423829942941666\n",
      "Epoch 4663, Loss: 0.21371132880449295, Final Batch Loss: 0.10181447863578796\n",
      "Epoch 4664, Loss: 0.21558164805173874, Final Batch Loss: 0.12584157288074493\n",
      "Epoch 4665, Loss: 0.21077806130051613, Final Batch Loss: 0.055583033710718155\n",
      "Epoch 4666, Loss: 0.19946837425231934, Final Batch Loss: 0.07963484525680542\n",
      "Epoch 4667, Loss: 0.2181359976530075, Final Batch Loss: 0.1185932531952858\n",
      "Epoch 4668, Loss: 0.17813287675380707, Final Batch Loss: 0.06749692559242249\n",
      "Epoch 4669, Loss: 0.20874172449111938, Final Batch Loss: 0.08522544056177139\n",
      "Epoch 4670, Loss: 0.23519770801067352, Final Batch Loss: 0.14816288650035858\n",
      "Epoch 4671, Loss: 0.27338317036628723, Final Batch Loss: 0.1672135293483734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4672, Loss: 0.21754104644060135, Final Batch Loss: 0.1304243505001068\n",
      "Epoch 4673, Loss: 0.23417479544878006, Final Batch Loss: 0.15166904032230377\n",
      "Epoch 4674, Loss: 0.23818986117839813, Final Batch Loss: 0.09916380047798157\n",
      "Epoch 4675, Loss: 0.16400843113660812, Final Batch Loss: 0.091618113219738\n",
      "Epoch 4676, Loss: 0.20455146580934525, Final Batch Loss: 0.09217371791601181\n",
      "Epoch 4677, Loss: 0.21625296771526337, Final Batch Loss: 0.14682511985301971\n",
      "Epoch 4678, Loss: 0.2504638582468033, Final Batch Loss: 0.1373957097530365\n",
      "Epoch 4679, Loss: 0.22915935516357422, Final Batch Loss: 0.09336104989051819\n",
      "Epoch 4680, Loss: 0.15808677673339844, Final Batch Loss: 0.07114493101835251\n",
      "Epoch 4681, Loss: 0.18245181441307068, Final Batch Loss: 0.10363525152206421\n",
      "Epoch 4682, Loss: 0.24583376944065094, Final Batch Loss: 0.10021248459815979\n",
      "Epoch 4683, Loss: 0.28540556132793427, Final Batch Loss: 0.14305591583251953\n",
      "Epoch 4684, Loss: 0.17736519128084183, Final Batch Loss: 0.10539324581623077\n",
      "Epoch 4685, Loss: 0.20730333775281906, Final Batch Loss: 0.11547616869211197\n",
      "Epoch 4686, Loss: 0.1722203493118286, Final Batch Loss: 0.09052419662475586\n",
      "Epoch 4687, Loss: 0.19065553694963455, Final Batch Loss: 0.12812575697898865\n",
      "Epoch 4688, Loss: 0.25158871710300446, Final Batch Loss: 0.138387069106102\n",
      "Epoch 4689, Loss: 0.24982668459415436, Final Batch Loss: 0.12763285636901855\n",
      "Epoch 4690, Loss: 0.26768428087234497, Final Batch Loss: 0.18144744634628296\n",
      "Epoch 4691, Loss: 0.2595840394496918, Final Batch Loss: 0.1594809591770172\n",
      "Epoch 4692, Loss: 0.15695930272340775, Final Batch Loss: 0.07195400446653366\n",
      "Epoch 4693, Loss: 0.17519684880971909, Final Batch Loss: 0.06157810986042023\n",
      "Epoch 4694, Loss: 0.18523554503917694, Final Batch Loss: 0.09774851053953171\n",
      "Epoch 4695, Loss: 0.2054470106959343, Final Batch Loss: 0.1071770042181015\n",
      "Epoch 4696, Loss: 0.18560408800840378, Final Batch Loss: 0.10499189049005508\n",
      "Epoch 4697, Loss: 0.2283003106713295, Final Batch Loss: 0.10019520670175552\n",
      "Epoch 4698, Loss: 0.2631787583231926, Final Batch Loss: 0.15575474500656128\n",
      "Epoch 4699, Loss: 0.1974383369088173, Final Batch Loss: 0.12924276292324066\n",
      "Epoch 4700, Loss: 0.17269278317689896, Final Batch Loss: 0.09093105047941208\n",
      "Epoch 4701, Loss: 0.22049978375434875, Final Batch Loss: 0.1384519785642624\n",
      "Epoch 4702, Loss: 0.1597166806459427, Final Batch Loss: 0.06772032380104065\n",
      "Epoch 4703, Loss: 0.22764992713928223, Final Batch Loss: 0.12243885546922684\n",
      "Epoch 4704, Loss: 0.2733222469687462, Final Batch Loss: 0.18228355050086975\n",
      "Epoch 4705, Loss: 0.17494824528694153, Final Batch Loss: 0.08311755955219269\n",
      "Epoch 4706, Loss: 0.17536872625350952, Final Batch Loss: 0.08490443229675293\n",
      "Epoch 4707, Loss: 0.18868529051542282, Final Batch Loss: 0.10390100628137589\n",
      "Epoch 4708, Loss: 0.1725628450512886, Final Batch Loss: 0.06994914263486862\n",
      "Epoch 4709, Loss: 0.1956721618771553, Final Batch Loss: 0.10893847793340683\n",
      "Epoch 4710, Loss: 0.20286355912685394, Final Batch Loss: 0.1019837036728859\n",
      "Epoch 4711, Loss: 0.16494449600577354, Final Batch Loss: 0.05136162415146828\n",
      "Epoch 4712, Loss: 0.19634758681058884, Final Batch Loss: 0.10565364360809326\n",
      "Epoch 4713, Loss: 0.20116467028856277, Final Batch Loss: 0.07002244144678116\n",
      "Epoch 4714, Loss: 0.15855009108781815, Final Batch Loss: 0.058862797915935516\n",
      "Epoch 4715, Loss: 0.3193460702896118, Final Batch Loss: 0.22400854527950287\n",
      "Epoch 4716, Loss: 0.18510320037603378, Final Batch Loss: 0.0764559954404831\n",
      "Epoch 4717, Loss: 0.23574308305978775, Final Batch Loss: 0.13175886869430542\n",
      "Epoch 4718, Loss: 0.16536839306354523, Final Batch Loss: 0.06409855931997299\n",
      "Epoch 4719, Loss: 0.18518910557031631, Final Batch Loss: 0.11044547706842422\n",
      "Epoch 4720, Loss: 0.20793335139751434, Final Batch Loss: 0.08204373717308044\n",
      "Epoch 4721, Loss: 0.21681377291679382, Final Batch Loss: 0.0919189453125\n",
      "Epoch 4722, Loss: 0.15227016061544418, Final Batch Loss: 0.06719491630792618\n",
      "Epoch 4723, Loss: 0.1812882423400879, Final Batch Loss: 0.10463710129261017\n",
      "Epoch 4724, Loss: 0.22561369836330414, Final Batch Loss: 0.10019724071025848\n",
      "Epoch 4725, Loss: 0.12848373875021935, Final Batch Loss: 0.0719129741191864\n",
      "Epoch 4726, Loss: 0.17970743030309677, Final Batch Loss: 0.08909525722265244\n",
      "Epoch 4727, Loss: 0.14001747965812683, Final Batch Loss: 0.06436166912317276\n",
      "Epoch 4728, Loss: 0.16711091995239258, Final Batch Loss: 0.07550043612718582\n",
      "Epoch 4729, Loss: 0.17405477166175842, Final Batch Loss: 0.0790126696228981\n",
      "Epoch 4730, Loss: 0.2756180018186569, Final Batch Loss: 0.19118313491344452\n",
      "Epoch 4731, Loss: 0.23319874703884125, Final Batch Loss: 0.07451383769512177\n",
      "Epoch 4732, Loss: 0.1257607266306877, Final Batch Loss: 0.06666845828294754\n",
      "Epoch 4733, Loss: 0.24245528876781464, Final Batch Loss: 0.14334145188331604\n",
      "Epoch 4734, Loss: 0.17581961303949356, Final Batch Loss: 0.07596153765916824\n",
      "Epoch 4735, Loss: 0.2067478448152542, Final Batch Loss: 0.09802770614624023\n",
      "Epoch 4736, Loss: 0.25216124951839447, Final Batch Loss: 0.14344507455825806\n",
      "Epoch 4737, Loss: 0.20946592092514038, Final Batch Loss: 0.10829757899045944\n",
      "Epoch 4738, Loss: 0.21871211379766464, Final Batch Loss: 0.09465260058641434\n",
      "Epoch 4739, Loss: 0.16042578592896461, Final Batch Loss: 0.056756798177957535\n",
      "Epoch 4740, Loss: 0.24882277101278305, Final Batch Loss: 0.11741841584444046\n",
      "Epoch 4741, Loss: 0.1797669231891632, Final Batch Loss: 0.09957868605852127\n",
      "Epoch 4742, Loss: 0.15824246406555176, Final Batch Loss: 0.06276235729455948\n",
      "Epoch 4743, Loss: 0.17620892077684402, Final Batch Loss: 0.08230012655258179\n",
      "Epoch 4744, Loss: 0.19255805760622025, Final Batch Loss: 0.0785614624619484\n",
      "Epoch 4745, Loss: 0.15705111622810364, Final Batch Loss: 0.06967376172542572\n",
      "Epoch 4746, Loss: 0.19756916910409927, Final Batch Loss: 0.1110212653875351\n",
      "Epoch 4747, Loss: 0.17323992401361465, Final Batch Loss: 0.04212891310453415\n",
      "Epoch 4748, Loss: 0.17729786783456802, Final Batch Loss: 0.10405942797660828\n",
      "Epoch 4749, Loss: 0.17842399328947067, Final Batch Loss: 0.08180252462625504\n",
      "Epoch 4750, Loss: 0.18835262954235077, Final Batch Loss: 0.09790872037410736\n",
      "Epoch 4751, Loss: 0.22338544577360153, Final Batch Loss: 0.08538895100355148\n",
      "Epoch 4752, Loss: 0.18802690505981445, Final Batch Loss: 0.10929931700229645\n",
      "Epoch 4753, Loss: 0.17280172556638718, Final Batch Loss: 0.10073499381542206\n",
      "Epoch 4754, Loss: 0.17285479605197906, Final Batch Loss: 0.09595566242933273\n",
      "Epoch 4755, Loss: 0.20972856879234314, Final Batch Loss: 0.11806786805391312\n",
      "Epoch 4756, Loss: 0.17617590725421906, Final Batch Loss: 0.09474349766969681\n",
      "Epoch 4757, Loss: 0.13822093605995178, Final Batch Loss: 0.05224023014307022\n",
      "Epoch 4758, Loss: 0.20365144312381744, Final Batch Loss: 0.11011718958616257\n",
      "Epoch 4759, Loss: 0.17048044502735138, Final Batch Loss: 0.10450747609138489\n",
      "Epoch 4760, Loss: 0.169976606965065, Final Batch Loss: 0.09654035419225693\n",
      "Epoch 4761, Loss: 0.1870291605591774, Final Batch Loss: 0.09364651143550873\n",
      "Epoch 4762, Loss: 0.23767033964395523, Final Batch Loss: 0.13346277177333832\n",
      "Epoch 4763, Loss: 0.17625834792852402, Final Batch Loss: 0.07169992476701736\n",
      "Epoch 4764, Loss: 0.22995689511299133, Final Batch Loss: 0.0956973284482956\n",
      "Epoch 4765, Loss: 0.19620513170957565, Final Batch Loss: 0.10875842720270157\n",
      "Epoch 4766, Loss: 0.2205994948744774, Final Batch Loss: 0.08568144589662552\n",
      "Epoch 4767, Loss: 0.19539551436901093, Final Batch Loss: 0.10044717788696289\n",
      "Epoch 4768, Loss: 0.16278798133134842, Final Batch Loss: 0.09341133385896683\n",
      "Epoch 4769, Loss: 0.15136003494262695, Final Batch Loss: 0.08005344867706299\n",
      "Epoch 4770, Loss: 0.20012201368808746, Final Batch Loss: 0.1251850128173828\n",
      "Epoch 4771, Loss: 0.14219214022159576, Final Batch Loss: 0.08817983418703079\n",
      "Epoch 4772, Loss: 0.17678432166576385, Final Batch Loss: 0.07809633761644363\n",
      "Epoch 4773, Loss: 0.17013796418905258, Final Batch Loss: 0.09706705808639526\n",
      "Epoch 4774, Loss: 0.25996536016464233, Final Batch Loss: 0.1429070681333542\n",
      "Epoch 4775, Loss: 0.22831546515226364, Final Batch Loss: 0.09935758262872696\n",
      "Epoch 4776, Loss: 0.24703772366046906, Final Batch Loss: 0.11913427710533142\n",
      "Epoch 4777, Loss: 0.27131427079439163, Final Batch Loss: 0.18480588495731354\n",
      "Epoch 4778, Loss: 0.11938067525625229, Final Batch Loss: 0.03585801273584366\n",
      "Epoch 4779, Loss: 0.17684566974639893, Final Batch Loss: 0.07315216213464737\n",
      "Epoch 4780, Loss: 0.16590287536382675, Final Batch Loss: 0.08579406887292862\n",
      "Epoch 4781, Loss: 0.23504367470741272, Final Batch Loss: 0.1415959596633911\n",
      "Epoch 4782, Loss: 0.1355857215821743, Final Batch Loss: 0.0578526146709919\n",
      "Epoch 4783, Loss: 0.2226036861538887, Final Batch Loss: 0.1256749927997589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4784, Loss: 0.19919109344482422, Final Batch Loss: 0.07493841648101807\n",
      "Epoch 4785, Loss: 0.19797784835100174, Final Batch Loss: 0.10192470252513885\n",
      "Epoch 4786, Loss: 0.20949847996234894, Final Batch Loss: 0.1044020876288414\n",
      "Epoch 4787, Loss: 0.1867453157901764, Final Batch Loss: 0.09250257909297943\n",
      "Epoch 4788, Loss: 0.15220796316862106, Final Batch Loss: 0.06684347987174988\n",
      "Epoch 4789, Loss: 0.21382589638233185, Final Batch Loss: 0.1008482426404953\n",
      "Epoch 4790, Loss: 0.21625323593616486, Final Batch Loss: 0.0838804543018341\n",
      "Epoch 4791, Loss: 0.23402804136276245, Final Batch Loss: 0.11028219759464264\n",
      "Epoch 4792, Loss: 0.14107517898082733, Final Batch Loss: 0.0625104084610939\n",
      "Epoch 4793, Loss: 0.24429992586374283, Final Batch Loss: 0.09181607514619827\n",
      "Epoch 4794, Loss: 0.22761959582567215, Final Batch Loss: 0.08858095854520798\n",
      "Epoch 4795, Loss: 0.21310542523860931, Final Batch Loss: 0.10143326967954636\n",
      "Epoch 4796, Loss: 0.17421536147594452, Final Batch Loss: 0.09902460873126984\n",
      "Epoch 4797, Loss: 0.15608221292495728, Final Batch Loss: 0.08130604773759842\n",
      "Epoch 4798, Loss: 0.22754579037427902, Final Batch Loss: 0.10563043504953384\n",
      "Epoch 4799, Loss: 0.21173422038555145, Final Batch Loss: 0.10198476910591125\n",
      "Epoch 4800, Loss: 0.20296064019203186, Final Batch Loss: 0.11744356155395508\n",
      "Epoch 4801, Loss: 0.17111804336309433, Final Batch Loss: 0.07317177951335907\n",
      "Epoch 4802, Loss: 0.22434106469154358, Final Batch Loss: 0.13447153568267822\n",
      "Epoch 4803, Loss: 0.13968933373689651, Final Batch Loss: 0.05428352952003479\n",
      "Epoch 4804, Loss: 0.1726963073015213, Final Batch Loss: 0.07240895181894302\n",
      "Epoch 4805, Loss: 0.1899043247103691, Final Batch Loss: 0.08515426516532898\n",
      "Epoch 4806, Loss: 0.18104790151119232, Final Batch Loss: 0.0806046649813652\n",
      "Epoch 4807, Loss: 0.24380258470773697, Final Batch Loss: 0.16393135488033295\n",
      "Epoch 4808, Loss: 0.2262100875377655, Final Batch Loss: 0.09898748993873596\n",
      "Epoch 4809, Loss: 0.25737909972667694, Final Batch Loss: 0.13347359001636505\n",
      "Epoch 4810, Loss: 0.1428987681865692, Final Batch Loss: 0.08441271632909775\n",
      "Epoch 4811, Loss: 0.19553211331367493, Final Batch Loss: 0.061626747250556946\n",
      "Epoch 4812, Loss: 0.25673626363277435, Final Batch Loss: 0.11113980412483215\n",
      "Epoch 4813, Loss: 0.1823406144976616, Final Batch Loss: 0.110414519906044\n",
      "Epoch 4814, Loss: 0.12682972848415375, Final Batch Loss: 0.055091261863708496\n",
      "Epoch 4815, Loss: 0.20279987156391144, Final Batch Loss: 0.11276283115148544\n",
      "Epoch 4816, Loss: 0.18006718158721924, Final Batch Loss: 0.08640942722558975\n",
      "Epoch 4817, Loss: 0.18670886009931564, Final Batch Loss: 0.10396482050418854\n",
      "Epoch 4818, Loss: 0.1539585441350937, Final Batch Loss: 0.05566149204969406\n",
      "Epoch 4819, Loss: 0.2501742020249367, Final Batch Loss: 0.15129506587982178\n",
      "Epoch 4820, Loss: 0.19430626183748245, Final Batch Loss: 0.07226521521806717\n",
      "Epoch 4821, Loss: 0.2469891756772995, Final Batch Loss: 0.0844460278749466\n",
      "Epoch 4822, Loss: 0.2048334926366806, Final Batch Loss: 0.12375949323177338\n",
      "Epoch 4823, Loss: 0.17150325328111649, Final Batch Loss: 0.07831786572933197\n",
      "Epoch 4824, Loss: 0.18666306883096695, Final Batch Loss: 0.07332780212163925\n",
      "Epoch 4825, Loss: 0.24841797351837158, Final Batch Loss: 0.11809900403022766\n",
      "Epoch 4826, Loss: 0.23286647349596024, Final Batch Loss: 0.10943076759576797\n",
      "Epoch 4827, Loss: 0.13880135118961334, Final Batch Loss: 0.06521947681903839\n",
      "Epoch 4828, Loss: 0.2816615104675293, Final Batch Loss: 0.186705082654953\n",
      "Epoch 4829, Loss: 0.1890290305018425, Final Batch Loss: 0.08319847285747528\n",
      "Epoch 4830, Loss: 0.22929352521896362, Final Batch Loss: 0.14239460229873657\n",
      "Epoch 4831, Loss: 0.12645051255822182, Final Batch Loss: 0.06921319663524628\n",
      "Epoch 4832, Loss: 0.1907552108168602, Final Batch Loss: 0.057921670377254486\n",
      "Epoch 4833, Loss: 0.1421765387058258, Final Batch Loss: 0.06481064110994339\n",
      "Epoch 4834, Loss: 0.222092904150486, Final Batch Loss: 0.12727050483226776\n",
      "Epoch 4835, Loss: 0.2555263265967369, Final Batch Loss: 0.16515158116817474\n",
      "Epoch 4836, Loss: 0.20049162209033966, Final Batch Loss: 0.07335866987705231\n",
      "Epoch 4837, Loss: 0.19500237703323364, Final Batch Loss: 0.0944814682006836\n",
      "Epoch 4838, Loss: 0.1769511178135872, Final Batch Loss: 0.09380562603473663\n",
      "Epoch 4839, Loss: 0.18459227681159973, Final Batch Loss: 0.08715599030256271\n",
      "Epoch 4840, Loss: 0.20691604912281036, Final Batch Loss: 0.13290449976921082\n",
      "Epoch 4841, Loss: 0.19966216385364532, Final Batch Loss: 0.07923369854688644\n",
      "Epoch 4842, Loss: 0.1583409383893013, Final Batch Loss: 0.06496671587228775\n",
      "Epoch 4843, Loss: 0.3378775864839554, Final Batch Loss: 0.19905713200569153\n",
      "Epoch 4844, Loss: 0.17381034046411514, Final Batch Loss: 0.07758519798517227\n",
      "Epoch 4845, Loss: 0.1862085461616516, Final Batch Loss: 0.11343373358249664\n",
      "Epoch 4846, Loss: 0.19571909308433533, Final Batch Loss: 0.10998325049877167\n",
      "Epoch 4847, Loss: 0.1638810932636261, Final Batch Loss: 0.09279253333806992\n",
      "Epoch 4848, Loss: 0.1764489859342575, Final Batch Loss: 0.09319393336772919\n",
      "Epoch 4849, Loss: 0.1787213310599327, Final Batch Loss: 0.08431870490312576\n",
      "Epoch 4850, Loss: 0.16000211238861084, Final Batch Loss: 0.07780788838863373\n",
      "Epoch 4851, Loss: 0.20995866507291794, Final Batch Loss: 0.08722930401563644\n",
      "Epoch 4852, Loss: 0.24137946218252182, Final Batch Loss: 0.14497070014476776\n",
      "Epoch 4853, Loss: 0.2074558064341545, Final Batch Loss: 0.1087225154042244\n",
      "Epoch 4854, Loss: 0.16485798358917236, Final Batch Loss: 0.08329986035823822\n",
      "Epoch 4855, Loss: 0.14754483103752136, Final Batch Loss: 0.08401841670274734\n",
      "Epoch 4856, Loss: 0.19618352502584457, Final Batch Loss: 0.07819411158561707\n",
      "Epoch 4857, Loss: 0.12111269682645798, Final Batch Loss: 0.05179377645254135\n",
      "Epoch 4858, Loss: 0.29178281873464584, Final Batch Loss: 0.21088211238384247\n",
      "Epoch 4859, Loss: 0.22540128976106644, Final Batch Loss: 0.11159899085760117\n",
      "Epoch 4860, Loss: 0.19758029282093048, Final Batch Loss: 0.10776448249816895\n",
      "Epoch 4861, Loss: 0.14722222462296486, Final Batch Loss: 0.04618387296795845\n",
      "Epoch 4862, Loss: 0.27813395112752914, Final Batch Loss: 0.08738353103399277\n",
      "Epoch 4863, Loss: 0.19648896902799606, Final Batch Loss: 0.09368223696947098\n",
      "Epoch 4864, Loss: 0.21255245804786682, Final Batch Loss: 0.10557565838098526\n",
      "Epoch 4865, Loss: 0.15666871517896652, Final Batch Loss: 0.08270949870347977\n",
      "Epoch 4866, Loss: 0.26814791560173035, Final Batch Loss: 0.13909989595413208\n",
      "Epoch 4867, Loss: 0.15810827910900116, Final Batch Loss: 0.06906461715698242\n",
      "Epoch 4868, Loss: 0.20941761136054993, Final Batch Loss: 0.12609639763832092\n",
      "Epoch 4869, Loss: 0.17269764840602875, Final Batch Loss: 0.07326070964336395\n",
      "Epoch 4870, Loss: 0.19940347224473953, Final Batch Loss: 0.09847914427518845\n",
      "Epoch 4871, Loss: 0.23640407621860504, Final Batch Loss: 0.12984995543956757\n",
      "Epoch 4872, Loss: 0.20948362350463867, Final Batch Loss: 0.11218128353357315\n",
      "Epoch 4873, Loss: 0.25229326635599136, Final Batch Loss: 0.1449262648820877\n",
      "Epoch 4874, Loss: 0.16343629360198975, Final Batch Loss: 0.07382091879844666\n",
      "Epoch 4875, Loss: 0.15748688951134682, Final Batch Loss: 0.04392499849200249\n",
      "Epoch 4876, Loss: 0.14430364221334457, Final Batch Loss: 0.03856096416711807\n",
      "Epoch 4877, Loss: 0.20008975267410278, Final Batch Loss: 0.10971318185329437\n",
      "Epoch 4878, Loss: 0.2479368969798088, Final Batch Loss: 0.14706753194332123\n",
      "Epoch 4879, Loss: 0.15608837828040123, Final Batch Loss: 0.04690002277493477\n",
      "Epoch 4880, Loss: 0.21086269617080688, Final Batch Loss: 0.12760519981384277\n",
      "Epoch 4881, Loss: 0.20384501665830612, Final Batch Loss: 0.10514198243618011\n",
      "Epoch 4882, Loss: 0.24064715951681137, Final Batch Loss: 0.136210098862648\n",
      "Epoch 4883, Loss: 0.2018764540553093, Final Batch Loss: 0.10640568286180496\n",
      "Epoch 4884, Loss: 0.18934251368045807, Final Batch Loss: 0.10406602174043655\n",
      "Epoch 4885, Loss: 0.15000823885202408, Final Batch Loss: 0.07986757904291153\n",
      "Epoch 4886, Loss: 0.15576734021306038, Final Batch Loss: 0.05364145711064339\n",
      "Epoch 4887, Loss: 0.22014614194631577, Final Batch Loss: 0.14311636984348297\n",
      "Epoch 4888, Loss: 0.2070542275905609, Final Batch Loss: 0.08623798191547394\n",
      "Epoch 4889, Loss: 0.19280653446912766, Final Batch Loss: 0.09020340442657471\n",
      "Epoch 4890, Loss: 0.17669281736016273, Final Batch Loss: 0.053050342947244644\n",
      "Epoch 4891, Loss: 0.1445927917957306, Final Batch Loss: 0.06516706943511963\n",
      "Epoch 4892, Loss: 0.1846095398068428, Final Batch Loss: 0.11786959320306778\n",
      "Epoch 4893, Loss: 0.16780687868595123, Final Batch Loss: 0.08617884665727615\n",
      "Epoch 4894, Loss: 0.21214452385902405, Final Batch Loss: 0.12889991700649261\n",
      "Epoch 4895, Loss: 0.13727643340826035, Final Batch Loss: 0.06370913982391357\n",
      "Epoch 4896, Loss: 0.15259931609034538, Final Batch Loss: 0.11117058247327805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4897, Loss: 0.16347705572843552, Final Batch Loss: 0.07901841402053833\n",
      "Epoch 4898, Loss: 0.17880918085575104, Final Batch Loss: 0.08442215621471405\n",
      "Epoch 4899, Loss: 0.2553791478276253, Final Batch Loss: 0.1805746704339981\n",
      "Epoch 4900, Loss: 0.16907446458935738, Final Batch Loss: 0.11507628858089447\n",
      "Epoch 4901, Loss: 0.17990579456090927, Final Batch Loss: 0.0640285462141037\n",
      "Epoch 4902, Loss: 0.2142663449048996, Final Batch Loss: 0.13544540107250214\n",
      "Epoch 4903, Loss: 0.21623771637678146, Final Batch Loss: 0.10612701624631882\n",
      "Epoch 4904, Loss: 0.25547612458467484, Final Batch Loss: 0.15996429324150085\n",
      "Epoch 4905, Loss: 0.15684397518634796, Final Batch Loss: 0.07867634296417236\n",
      "Epoch 4906, Loss: 0.17463793605566025, Final Batch Loss: 0.11192311346530914\n",
      "Epoch 4907, Loss: 0.14774642139673233, Final Batch Loss: 0.05794867128133774\n",
      "Epoch 4908, Loss: 0.13298435509204865, Final Batch Loss: 0.06449439376592636\n",
      "Epoch 4909, Loss: 0.22617466747760773, Final Batch Loss: 0.0908125787973404\n",
      "Epoch 4910, Loss: 0.16149736940860748, Final Batch Loss: 0.08531077951192856\n",
      "Epoch 4911, Loss: 0.15837864577770233, Final Batch Loss: 0.06248041242361069\n",
      "Epoch 4912, Loss: 0.1704966425895691, Final Batch Loss: 0.08060434460639954\n",
      "Epoch 4913, Loss: 0.2501521222293377, Final Batch Loss: 0.19952817261219025\n",
      "Epoch 4914, Loss: 0.22462400794029236, Final Batch Loss: 0.09777678549289703\n",
      "Epoch 4915, Loss: 0.20571601763367653, Final Batch Loss: 0.1437245011329651\n",
      "Epoch 4916, Loss: 0.13059178739786148, Final Batch Loss: 0.06421057879924774\n",
      "Epoch 4917, Loss: 0.11891202256083488, Final Batch Loss: 0.05265655741095543\n",
      "Epoch 4918, Loss: 0.20447438210248947, Final Batch Loss: 0.10531814396381378\n",
      "Epoch 4919, Loss: 0.18104711174964905, Final Batch Loss: 0.09405405819416046\n",
      "Epoch 4920, Loss: 0.17378421872854233, Final Batch Loss: 0.07700540125370026\n",
      "Epoch 4921, Loss: 0.12440260127186775, Final Batch Loss: 0.06999039649963379\n",
      "Epoch 4922, Loss: 0.20408190786838531, Final Batch Loss: 0.09254026412963867\n",
      "Epoch 4923, Loss: 0.3132553994655609, Final Batch Loss: 0.19353824853897095\n",
      "Epoch 4924, Loss: 0.21366136521100998, Final Batch Loss: 0.13167546689510345\n",
      "Epoch 4925, Loss: 0.18613532185554504, Final Batch Loss: 0.10314648598432541\n",
      "Epoch 4926, Loss: 0.1872197687625885, Final Batch Loss: 0.07799728214740753\n",
      "Epoch 4927, Loss: 0.1558387577533722, Final Batch Loss: 0.07284671813249588\n",
      "Epoch 4928, Loss: 0.22276495397090912, Final Batch Loss: 0.13526993989944458\n",
      "Epoch 4929, Loss: 0.18935371190309525, Final Batch Loss: 0.07626037299633026\n",
      "Epoch 4930, Loss: 0.21727563440799713, Final Batch Loss: 0.11260540038347244\n",
      "Epoch 4931, Loss: 0.19592783600091934, Final Batch Loss: 0.07897412776947021\n",
      "Epoch 4932, Loss: 0.20219377428293228, Final Batch Loss: 0.0568096861243248\n",
      "Epoch 4933, Loss: 0.16913101077079773, Final Batch Loss: 0.08238078653812408\n",
      "Epoch 4934, Loss: 0.17439143359661102, Final Batch Loss: 0.07739342749118805\n",
      "Epoch 4935, Loss: 0.1842735931277275, Final Batch Loss: 0.06765081733465195\n",
      "Epoch 4936, Loss: 0.21508559584617615, Final Batch Loss: 0.10565438866615295\n",
      "Epoch 4937, Loss: 0.1904599964618683, Final Batch Loss: 0.07719454169273376\n",
      "Epoch 4938, Loss: 0.20589791238307953, Final Batch Loss: 0.09594814479351044\n",
      "Epoch 4939, Loss: 0.20374362915754318, Final Batch Loss: 0.10446590185165405\n",
      "Epoch 4940, Loss: 0.1569819152355194, Final Batch Loss: 0.08657319098711014\n",
      "Epoch 4941, Loss: 0.1653258055448532, Final Batch Loss: 0.06828111410140991\n",
      "Epoch 4942, Loss: 0.20125923305749893, Final Batch Loss: 0.08989045768976212\n",
      "Epoch 4943, Loss: 0.17277714982628822, Final Batch Loss: 0.0520591102540493\n",
      "Epoch 4944, Loss: 0.21388022229075432, Final Batch Loss: 0.05855707451701164\n",
      "Epoch 4945, Loss: 0.1784069836139679, Final Batch Loss: 0.10237529873847961\n",
      "Epoch 4946, Loss: 0.1831226870417595, Final Batch Loss: 0.11521689593791962\n",
      "Epoch 4947, Loss: 0.23367232084274292, Final Batch Loss: 0.12782125174999237\n",
      "Epoch 4948, Loss: 0.29489074647426605, Final Batch Loss: 0.13455456495285034\n",
      "Epoch 4949, Loss: 0.22362226992845535, Final Batch Loss: 0.1505412608385086\n",
      "Epoch 4950, Loss: 0.1878252997994423, Final Batch Loss: 0.12741024792194366\n",
      "Epoch 4951, Loss: 0.2100072130560875, Final Batch Loss: 0.08883979171514511\n",
      "Epoch 4952, Loss: 0.1624908596277237, Final Batch Loss: 0.06411755830049515\n",
      "Epoch 4953, Loss: 0.30879272520542145, Final Batch Loss: 0.2171034812927246\n",
      "Epoch 4954, Loss: 0.1805461198091507, Final Batch Loss: 0.09116854518651962\n",
      "Epoch 4955, Loss: 0.14656442031264305, Final Batch Loss: 0.04644666239619255\n",
      "Epoch 4956, Loss: 0.2048730105161667, Final Batch Loss: 0.11109359562397003\n",
      "Epoch 4957, Loss: 0.25850966572761536, Final Batch Loss: 0.1754630208015442\n",
      "Epoch 4958, Loss: 0.1710735410451889, Final Batch Loss: 0.07817396521568298\n",
      "Epoch 4959, Loss: 0.247979074716568, Final Batch Loss: 0.136440709233284\n",
      "Epoch 4960, Loss: 0.14679521322250366, Final Batch Loss: 0.06431890279054642\n",
      "Epoch 4961, Loss: 0.16749407351016998, Final Batch Loss: 0.07318376004695892\n",
      "Epoch 4962, Loss: 0.20719079673290253, Final Batch Loss: 0.10514713823795319\n",
      "Epoch 4963, Loss: 0.17629674822092056, Final Batch Loss: 0.07794734835624695\n",
      "Epoch 4964, Loss: 0.1820371374487877, Final Batch Loss: 0.08020729571580887\n",
      "Epoch 4965, Loss: 0.2078544721007347, Final Batch Loss: 0.10645557940006256\n",
      "Epoch 4966, Loss: 0.1492113694548607, Final Batch Loss: 0.08536560088396072\n",
      "Epoch 4967, Loss: 0.1594911441206932, Final Batch Loss: 0.06697937846183777\n",
      "Epoch 4968, Loss: 0.25989679992198944, Final Batch Loss: 0.14317207038402557\n",
      "Epoch 4969, Loss: 0.1697625070810318, Final Batch Loss: 0.10001285374164581\n",
      "Epoch 4970, Loss: 0.18172775208950043, Final Batch Loss: 0.11794465035200119\n",
      "Epoch 4971, Loss: 0.25631464272737503, Final Batch Loss: 0.16097410023212433\n",
      "Epoch 4972, Loss: 0.1397547349333763, Final Batch Loss: 0.061042092740535736\n",
      "Epoch 4973, Loss: 0.22302799671888351, Final Batch Loss: 0.11037273705005646\n",
      "Epoch 4974, Loss: 0.27882716432213783, Final Batch Loss: 0.05183127894997597\n",
      "Epoch 4975, Loss: 0.17339202016592026, Final Batch Loss: 0.0815853551030159\n",
      "Epoch 4976, Loss: 0.19375617057085037, Final Batch Loss: 0.10165639221668243\n",
      "Epoch 4977, Loss: 0.20590580254793167, Final Batch Loss: 0.08828304708003998\n",
      "Epoch 4978, Loss: 0.18374035507440567, Final Batch Loss: 0.10241099447011948\n",
      "Epoch 4979, Loss: 0.1488110050559044, Final Batch Loss: 0.08002633601427078\n",
      "Epoch 4980, Loss: 0.16871746629476547, Final Batch Loss: 0.09108110517263412\n",
      "Epoch 4981, Loss: 0.2038923129439354, Final Batch Loss: 0.09335671365261078\n",
      "Epoch 4982, Loss: 0.1897432655096054, Final Batch Loss: 0.08449887484312057\n",
      "Epoch 4983, Loss: 0.1484747938811779, Final Batch Loss: 0.0942930057644844\n",
      "Epoch 4984, Loss: 0.21480142325162888, Final Batch Loss: 0.09881705790758133\n",
      "Epoch 4985, Loss: 0.1591389663517475, Final Batch Loss: 0.056488510221242905\n",
      "Epoch 4986, Loss: 0.19985642284154892, Final Batch Loss: 0.0714658722281456\n",
      "Epoch 4987, Loss: 0.19199217855930328, Final Batch Loss: 0.11125093698501587\n",
      "Epoch 4988, Loss: 0.13635515421628952, Final Batch Loss: 0.056308649480342865\n",
      "Epoch 4989, Loss: 0.16357460618019104, Final Batch Loss: 0.07656365633010864\n",
      "Epoch 4990, Loss: 0.13144133612513542, Final Batch Loss: 0.052164193242788315\n",
      "Epoch 4991, Loss: 0.1634494736790657, Final Batch Loss: 0.07017546892166138\n",
      "Epoch 4992, Loss: 0.21248941868543625, Final Batch Loss: 0.12606148421764374\n",
      "Epoch 4993, Loss: 0.21357093006372452, Final Batch Loss: 0.1078837439417839\n",
      "Epoch 4994, Loss: 0.15249218046665192, Final Batch Loss: 0.0703592449426651\n",
      "Epoch 4995, Loss: 0.16675038635730743, Final Batch Loss: 0.06684025377035141\n",
      "Epoch 4996, Loss: 0.19582749903202057, Final Batch Loss: 0.07252209633588791\n",
      "Epoch 4997, Loss: 0.16770783066749573, Final Batch Loss: 0.04114671051502228\n",
      "Epoch 4998, Loss: 0.185538150370121, Final Batch Loss: 0.10529807209968567\n",
      "Epoch 4999, Loss: 0.20022710412740707, Final Batch Loss: 0.10604558140039444\n",
      "Epoch 5000, Loss: 0.16051864624023438, Final Batch Loss: 0.07097740471363068\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  1 14  0  0  0  0]\n",
      " [ 0  0  1  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  1  0  0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        18\n",
      "           1    1.00000   1.00000   1.00000        11\n",
      "           2    0.90909   1.00000   0.95238        10\n",
      "           3    0.90909   1.00000   0.95238        10\n",
      "           4    1.00000   0.93333   0.96552        15\n",
      "           5    0.85714   0.85714   0.85714         7\n",
      "           6    1.00000   1.00000   1.00000        11\n",
      "           7    1.00000   1.00000   1.00000        10\n",
      "           8    1.00000   0.87500   0.93333         8\n",
      "\n",
      "    accuracy                        0.97000       100\n",
      "   macro avg    0.96392   0.96283   0.96231       100\n",
      "weighted avg    0.97182   0.97000   0.96997       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_8_19_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 3)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "\n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0 15  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  0  1]\n",
      " [ 0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0 16  0]\n",
      " [ 0  0  0  0  0  0  2  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        13\n",
      "           1    1.00000   1.00000   1.00000         5\n",
      "           2    1.00000   1.00000   1.00000        14\n",
      "           3    1.00000   1.00000   1.00000        15\n",
      "           4    1.00000   1.00000   1.00000        10\n",
      "           5    1.00000   0.92857   0.96296        14\n",
      "           6    0.84615   1.00000   0.91667        11\n",
      "           7    1.00000   1.00000   1.00000        16\n",
      "           8    0.00000   0.00000   0.00000         2\n",
      "\n",
      "    accuracy                        0.97000       100\n",
      "   macro avg    0.87179   0.88095   0.87551       100\n",
      "weighted avg    0.96308   0.97000   0.96565       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
